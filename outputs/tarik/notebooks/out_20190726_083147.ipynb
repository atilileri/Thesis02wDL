{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf36.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 08:31:47 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '3', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001AF91055E10>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001AF8D807EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0967, Accuracy:0.3634, Validation Loss:1.0841, Validation Accuracy:0.3957\n",
    "Epoch #2: Loss:1.0795, Accuracy:0.3947, Validation Loss:1.0764, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0748, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0757, Accuracy:0.3926, Validation Loss:1.0748, Validation Accuracy:0.3744\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3897, Validation Loss:1.0743, Validation Accuracy:0.3760\n",
    "Epoch #7: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #8: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #10: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #11: Loss:1.0741, Accuracy:0.3938, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #19: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #20: Loss:1.0738, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #21: Loss:1.0738, Accuracy:0.3938, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0738, Accuracy:0.3938, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #25: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #26: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #27: Loss:1.0737, Accuracy:0.3930, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #30: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #31: Loss:1.0739, Accuracy:0.3979, Validation Loss:1.0738, Validation Accuracy:0.3793\n",
    "Epoch #32: Loss:1.0739, Accuracy:0.3967, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #33: Loss:1.0739, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #35: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #36: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #37: Loss:1.0738, Accuracy:0.3914, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0738, Accuracy:0.3926, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0738, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0740, Accuracy:0.3967, Validation Loss:1.0735, Validation Accuracy:0.3974\n",
    "Epoch #42: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #43: Loss:1.0739, Accuracy:0.3984, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #45: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #46: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #47: Loss:1.0740, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0739, Accuracy:0.3963, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #49: Loss:1.0738, Accuracy:0.3930, Validation Loss:1.0737, Validation Accuracy:0.4089\n",
    "Epoch #50: Loss:1.0739, Accuracy:0.3934, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #51: Loss:1.0740, Accuracy:0.3914, Validation Loss:1.0738, Validation Accuracy:0.4072\n",
    "Epoch #52: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0735, Validation Accuracy:0.3974\n",
    "Epoch #53: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #54: Loss:1.0737, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.4072\n",
    "Epoch #55: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0738, Validation Accuracy:0.4072\n",
    "Epoch #56: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #57: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0734, Validation Accuracy:0.3908\n",
    "Epoch #58: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0735, Validation Accuracy:0.4056\n",
    "Epoch #59: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #61: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0735, Validation Accuracy:0.4072\n",
    "Epoch #62: Loss:1.0735, Accuracy:0.3934, Validation Loss:1.0738, Validation Accuracy:0.4039\n",
    "Epoch #63: Loss:1.0736, Accuracy:0.4037, Validation Loss:1.0739, Validation Accuracy:0.4056\n",
    "Epoch #64: Loss:1.0737, Accuracy:0.4025, Validation Loss:1.0739, Validation Accuracy:0.3810\n",
    "Epoch #65: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0739, Validation Accuracy:0.4056\n",
    "Epoch #66: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3826\n",
    "Epoch #67: Loss:1.0736, Accuracy:0.4045, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #68: Loss:1.0738, Accuracy:0.3901, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #69: Loss:1.0735, Accuracy:0.3910, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #70: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #71: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #72: Loss:1.0734, Accuracy:0.4062, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #73: Loss:1.0733, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #74: Loss:1.0736, Accuracy:0.3955, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #75: Loss:1.0736, Accuracy:0.3910, Validation Loss:1.0751, Validation Accuracy:0.3727\n",
    "Epoch #76: Loss:1.0738, Accuracy:0.4029, Validation Loss:1.0750, Validation Accuracy:0.3793\n",
    "Epoch #77: Loss:1.0748, Accuracy:0.3910, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #78: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #79: Loss:1.0739, Accuracy:0.3934, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #80: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #81: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #82: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #83: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #84: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #85: Loss:1.0736, Accuracy:0.3955, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #86: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #87: Loss:1.0734, Accuracy:0.3971, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #88: Loss:1.0739, Accuracy:0.4090, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #89: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #90: Loss:1.0733, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #91: Loss:1.0733, Accuracy:0.3934, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #92: Loss:1.0730, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #93: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #94: Loss:1.0730, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #95: Loss:1.0734, Accuracy:0.3930, Validation Loss:1.0747, Validation Accuracy:0.4039\n",
    "Epoch #96: Loss:1.0728, Accuracy:0.3996, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #97: Loss:1.0729, Accuracy:0.4086, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #98: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #99: Loss:1.0725, Accuracy:0.3901, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #100: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.4056\n",
    "Epoch #101: Loss:1.0730, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #102: Loss:1.0727, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.4039\n",
    "Epoch #103: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0743, Validation Accuracy:0.4105\n",
    "Epoch #104: Loss:1.0727, Accuracy:0.4074, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #105: Loss:1.0723, Accuracy:0.4086, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #106: Loss:1.0726, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #107: Loss:1.0737, Accuracy:0.3975, Validation Loss:1.0758, Validation Accuracy:0.3810\n",
    "Epoch #108: Loss:1.0725, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #109: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0754, Validation Accuracy:0.3826\n",
    "Epoch #110: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #111: Loss:1.0723, Accuracy:0.3951, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #112: Loss:1.0724, Accuracy:0.4037, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #113: Loss:1.0731, Accuracy:0.3889, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #114: Loss:1.0723, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #115: Loss:1.0722, Accuracy:0.3914, Validation Loss:1.0749, Validation Accuracy:0.3711\n",
    "Epoch #116: Loss:1.0724, Accuracy:0.3996, Validation Loss:1.0751, Validation Accuracy:0.3924\n",
    "Epoch #117: Loss:1.0720, Accuracy:0.3996, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #118: Loss:1.0726, Accuracy:0.4029, Validation Loss:1.0751, Validation Accuracy:0.3744\n",
    "Epoch #119: Loss:1.0718, Accuracy:0.3992, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #120: Loss:1.0720, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3744\n",
    "Epoch #121: Loss:1.0722, Accuracy:0.3975, Validation Loss:1.0752, Validation Accuracy:0.3744\n",
    "Epoch #122: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.3744\n",
    "Epoch #123: Loss:1.0715, Accuracy:0.3975, Validation Loss:1.0756, Validation Accuracy:0.4023\n",
    "Epoch #124: Loss:1.0716, Accuracy:0.3967, Validation Loss:1.0754, Validation Accuracy:0.3777\n",
    "Epoch #125: Loss:1.0719, Accuracy:0.4025, Validation Loss:1.0754, Validation Accuracy:0.3777\n",
    "Epoch #126: Loss:1.0712, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3810\n",
    "Epoch #127: Loss:1.0709, Accuracy:0.3988, Validation Loss:1.0758, Validation Accuracy:0.3727\n",
    "Epoch #128: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #129: Loss:1.0709, Accuracy:0.4099, Validation Loss:1.0756, Validation Accuracy:0.3842\n",
    "Epoch #130: Loss:1.0706, Accuracy:0.4037, Validation Loss:1.0762, Validation Accuracy:0.3777\n",
    "Epoch #131: Loss:1.0705, Accuracy:0.4025, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #132: Loss:1.0705, Accuracy:0.4082, Validation Loss:1.0757, Validation Accuracy:0.3859\n",
    "Epoch #133: Loss:1.0702, Accuracy:0.4078, Validation Loss:1.0763, Validation Accuracy:0.3777\n",
    "Epoch #134: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0761, Validation Accuracy:0.3892\n",
    "Epoch #135: Loss:1.0708, Accuracy:0.4066, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #136: Loss:1.0712, Accuracy:0.4070, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #137: Loss:1.0722, Accuracy:0.3963, Validation Loss:1.0859, Validation Accuracy:0.3924\n",
    "Epoch #138: Loss:1.0762, Accuracy:0.3984, Validation Loss:1.0785, Validation Accuracy:0.3629\n",
    "Epoch #139: Loss:1.0753, Accuracy:0.3791, Validation Loss:1.0762, Validation Accuracy:0.3892\n",
    "Epoch #140: Loss:1.0731, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #141: Loss:1.0741, Accuracy:0.3881, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #142: Loss:1.0725, Accuracy:0.3996, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #143: Loss:1.0731, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #144: Loss:1.0725, Accuracy:0.3910, Validation Loss:1.0744, Validation Accuracy:0.4072\n",
    "Epoch #145: Loss:1.0723, Accuracy:0.3873, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #146: Loss:1.0723, Accuracy:0.3906, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #147: Loss:1.0725, Accuracy:0.3947, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #148: Loss:1.0725, Accuracy:0.3922, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #149: Loss:1.0724, Accuracy:0.3906, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #150: Loss:1.0725, Accuracy:0.3959, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #151: Loss:1.0724, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #152: Loss:1.0723, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #153: Loss:1.0722, Accuracy:0.3881, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #154: Loss:1.0724, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #155: Loss:1.0724, Accuracy:0.3864, Validation Loss:1.0751, Validation Accuracy:0.3810\n",
    "Epoch #156: Loss:1.0719, Accuracy:0.3877, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #157: Loss:1.0720, Accuracy:0.3996, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #158: Loss:1.0716, Accuracy:0.3901, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #159: Loss:1.0721, Accuracy:0.3893, Validation Loss:1.0748, Validation Accuracy:0.3662\n",
    "Epoch #160: Loss:1.0716, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3678\n",
    "Epoch #161: Loss:1.0716, Accuracy:0.3938, Validation Loss:1.0747, Validation Accuracy:0.3678\n",
    "Epoch #162: Loss:1.0714, Accuracy:0.3930, Validation Loss:1.0746, Validation Accuracy:0.3662\n",
    "Epoch #163: Loss:1.0715, Accuracy:0.3914, Validation Loss:1.0746, Validation Accuracy:0.3695\n",
    "Epoch #164: Loss:1.0713, Accuracy:0.3947, Validation Loss:1.0743, Validation Accuracy:0.3662\n",
    "Epoch #165: Loss:1.0713, Accuracy:0.3926, Validation Loss:1.0744, Validation Accuracy:0.3793\n",
    "Epoch #166: Loss:1.0709, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3662\n",
    "Epoch #167: Loss:1.0710, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3596\n",
    "Epoch #168: Loss:1.0710, Accuracy:0.3955, Validation Loss:1.0740, Validation Accuracy:0.3793\n",
    "Epoch #169: Loss:1.0705, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3695\n",
    "Epoch #170: Loss:1.0702, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3826\n",
    "Epoch #171: Loss:1.0702, Accuracy:0.4033, Validation Loss:1.0739, Validation Accuracy:0.3612\n",
    "Epoch #172: Loss:1.0701, Accuracy:0.4160, Validation Loss:1.0742, Validation Accuracy:0.3793\n",
    "Epoch #173: Loss:1.0700, Accuracy:0.3938, Validation Loss:1.0739, Validation Accuracy:0.3727\n",
    "Epoch #174: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0744, Validation Accuracy:0.3727\n",
    "Epoch #175: Loss:1.0698, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3645\n",
    "Epoch #176: Loss:1.0698, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3596\n",
    "Epoch #177: Loss:1.0696, Accuracy:0.4029, Validation Loss:1.0744, Validation Accuracy:0.3596\n",
    "Epoch #178: Loss:1.0703, Accuracy:0.4025, Validation Loss:1.0745, Validation Accuracy:0.3777\n",
    "Epoch #179: Loss:1.0694, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3662\n",
    "Epoch #180: Loss:1.0696, Accuracy:0.3996, Validation Loss:1.0749, Validation Accuracy:0.3727\n",
    "Epoch #181: Loss:1.0693, Accuracy:0.4033, Validation Loss:1.0751, Validation Accuracy:0.3596\n",
    "Epoch #182: Loss:1.0688, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.3777\n",
    "Epoch #183: Loss:1.0698, Accuracy:0.4000, Validation Loss:1.0749, Validation Accuracy:0.3777\n",
    "Epoch #184: Loss:1.0699, Accuracy:0.4037, Validation Loss:1.0758, Validation Accuracy:0.3645\n",
    "Epoch #185: Loss:1.0703, Accuracy:0.3984, Validation Loss:1.0752, Validation Accuracy:0.3645\n",
    "Epoch #186: Loss:1.0688, Accuracy:0.4021, Validation Loss:1.0764, Validation Accuracy:0.3612\n",
    "Epoch #187: Loss:1.0689, Accuracy:0.4041, Validation Loss:1.0753, Validation Accuracy:0.3662\n",
    "Epoch #188: Loss:1.0695, Accuracy:0.3992, Validation Loss:1.0757, Validation Accuracy:0.3662\n",
    "Epoch #189: Loss:1.0693, Accuracy:0.4082, Validation Loss:1.0769, Validation Accuracy:0.3760\n",
    "Epoch #190: Loss:1.0686, Accuracy:0.4016, Validation Loss:1.0757, Validation Accuracy:0.3777\n",
    "Epoch #191: Loss:1.0684, Accuracy:0.4004, Validation Loss:1.0759, Validation Accuracy:0.3793\n",
    "Epoch #192: Loss:1.0680, Accuracy:0.3996, Validation Loss:1.0765, Validation Accuracy:0.3777\n",
    "Epoch #193: Loss:1.0688, Accuracy:0.4016, Validation Loss:1.0760, Validation Accuracy:0.3662\n",
    "Epoch #194: Loss:1.0687, Accuracy:0.4000, Validation Loss:1.0764, Validation Accuracy:0.3793\n",
    "Epoch #195: Loss:1.0682, Accuracy:0.4021, Validation Loss:1.0766, Validation Accuracy:0.3612\n",
    "Epoch #196: Loss:1.0681, Accuracy:0.4078, Validation Loss:1.0765, Validation Accuracy:0.3777\n",
    "Epoch #197: Loss:1.0679, Accuracy:0.4012, Validation Loss:1.0764, Validation Accuracy:0.3711\n",
    "Epoch #198: Loss:1.0680, Accuracy:0.4029, Validation Loss:1.0764, Validation Accuracy:0.3662\n",
    "Epoch #199: Loss:1.0681, Accuracy:0.3934, Validation Loss:1.0772, Validation Accuracy:0.3793\n",
    "Epoch #200: Loss:1.0681, Accuracy:0.4016, Validation Loss:1.0770, Validation Accuracy:0.3645\n",
    "Epoch #201: Loss:1.0681, Accuracy:0.4045, Validation Loss:1.0767, Validation Accuracy:0.3711\n",
    "Epoch #202: Loss:1.0678, Accuracy:0.4041, Validation Loss:1.0783, Validation Accuracy:0.3711\n",
    "Epoch #203: Loss:1.0680, Accuracy:0.3988, Validation Loss:1.0809, Validation Accuracy:0.3695\n",
    "Epoch #204: Loss:1.0682, Accuracy:0.4033, Validation Loss:1.0792, Validation Accuracy:0.3678\n",
    "Epoch #205: Loss:1.0685, Accuracy:0.3984, Validation Loss:1.0790, Validation Accuracy:0.3826\n",
    "Epoch #206: Loss:1.0679, Accuracy:0.3984, Validation Loss:1.0803, Validation Accuracy:0.3760\n",
    "Epoch #207: Loss:1.0680, Accuracy:0.3979, Validation Loss:1.0811, Validation Accuracy:0.3842\n",
    "Epoch #208: Loss:1.0682, Accuracy:0.4016, Validation Loss:1.0802, Validation Accuracy:0.3695\n",
    "Epoch #209: Loss:1.0683, Accuracy:0.4000, Validation Loss:1.0791, Validation Accuracy:0.3760\n",
    "Epoch #210: Loss:1.0679, Accuracy:0.4045, Validation Loss:1.0784, Validation Accuracy:0.3744\n",
    "Epoch #211: Loss:1.0679, Accuracy:0.3975, Validation Loss:1.0789, Validation Accuracy:0.3760\n",
    "Epoch #212: Loss:1.0680, Accuracy:0.4090, Validation Loss:1.0784, Validation Accuracy:0.3760\n",
    "Epoch #213: Loss:1.0674, Accuracy:0.4025, Validation Loss:1.0776, Validation Accuracy:0.3810\n",
    "Epoch #214: Loss:1.0674, Accuracy:0.3975, Validation Loss:1.0779, Validation Accuracy:0.3826\n",
    "Epoch #215: Loss:1.0674, Accuracy:0.4062, Validation Loss:1.0778, Validation Accuracy:0.3711\n",
    "Epoch #216: Loss:1.0678, Accuracy:0.3967, Validation Loss:1.0778, Validation Accuracy:0.3760\n",
    "Epoch #217: Loss:1.0672, Accuracy:0.4041, Validation Loss:1.0785, Validation Accuracy:0.3760\n",
    "Epoch #218: Loss:1.0669, Accuracy:0.4078, Validation Loss:1.0780, Validation Accuracy:0.3777\n",
    "Epoch #219: Loss:1.0664, Accuracy:0.4049, Validation Loss:1.0787, Validation Accuracy:0.3678\n",
    "Epoch #220: Loss:1.0670, Accuracy:0.4086, Validation Loss:1.0797, Validation Accuracy:0.3695\n",
    "Epoch #221: Loss:1.0663, Accuracy:0.4074, Validation Loss:1.0764, Validation Accuracy:0.3744\n",
    "Epoch #222: Loss:1.0671, Accuracy:0.4099, Validation Loss:1.0755, Validation Accuracy:0.3662\n",
    "Epoch #223: Loss:1.0687, Accuracy:0.3979, Validation Loss:1.0760, Validation Accuracy:0.3678\n",
    "Epoch #224: Loss:1.0700, Accuracy:0.4057, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #225: Loss:1.0683, Accuracy:0.4070, Validation Loss:1.0793, Validation Accuracy:0.3662\n",
    "Epoch #226: Loss:1.0692, Accuracy:0.4008, Validation Loss:1.0804, Validation Accuracy:0.3711\n",
    "Epoch #227: Loss:1.0856, Accuracy:0.3836, Validation Loss:1.0783, Validation Accuracy:0.3645\n",
    "Epoch #228: Loss:1.0807, Accuracy:0.3930, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #229: Loss:1.0767, Accuracy:0.3955, Validation Loss:1.0783, Validation Accuracy:0.3924\n",
    "Epoch #230: Loss:1.0752, Accuracy:0.3984, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #231: Loss:1.0727, Accuracy:0.4000, Validation Loss:1.0762, Validation Accuracy:0.3760\n",
    "Epoch #232: Loss:1.0730, Accuracy:0.3971, Validation Loss:1.0762, Validation Accuracy:0.3760\n",
    "Epoch #233: Loss:1.0729, Accuracy:0.3922, Validation Loss:1.0765, Validation Accuracy:0.3826\n",
    "Epoch #234: Loss:1.0722, Accuracy:0.3979, Validation Loss:1.0785, Validation Accuracy:0.3908\n",
    "Epoch #235: Loss:1.0719, Accuracy:0.3897, Validation Loss:1.0780, Validation Accuracy:0.4007\n",
    "Epoch #236: Loss:1.0722, Accuracy:0.3897, Validation Loss:1.0793, Validation Accuracy:0.3908\n",
    "Epoch #237: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0790, Validation Accuracy:0.3957\n",
    "Epoch #238: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0779, Validation Accuracy:0.3892\n",
    "Epoch #239: Loss:1.0717, Accuracy:0.3988, Validation Loss:1.0768, Validation Accuracy:0.3974\n",
    "Epoch #240: Loss:1.0713, Accuracy:0.4000, Validation Loss:1.0780, Validation Accuracy:0.3941\n",
    "Epoch #241: Loss:1.0710, Accuracy:0.3988, Validation Loss:1.0782, Validation Accuracy:0.3957\n",
    "Epoch #242: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0783, Validation Accuracy:0.3842\n",
    "Epoch #243: Loss:1.0706, Accuracy:0.4025, Validation Loss:1.0777, Validation Accuracy:0.3826\n",
    "Epoch #244: Loss:1.0704, Accuracy:0.3996, Validation Loss:1.0771, Validation Accuracy:0.3727\n",
    "Epoch #245: Loss:1.0701, Accuracy:0.4053, Validation Loss:1.0776, Validation Accuracy:0.3793\n",
    "Epoch #246: Loss:1.0702, Accuracy:0.4049, Validation Loss:1.0772, Validation Accuracy:0.3859\n",
    "Epoch #247: Loss:1.0699, Accuracy:0.3996, Validation Loss:1.0782, Validation Accuracy:0.3629\n",
    "Epoch #248: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0778, Validation Accuracy:0.3941\n",
    "Epoch #249: Loss:1.0708, Accuracy:0.4021, Validation Loss:1.0786, Validation Accuracy:0.3777\n",
    "Epoch #250: Loss:1.0704, Accuracy:0.3930, Validation Loss:1.0788, Validation Accuracy:0.3892\n",
    "Epoch #251: Loss:1.0703, Accuracy:0.4033, Validation Loss:1.0788, Validation Accuracy:0.3810\n",
    "Epoch #252: Loss:1.0698, Accuracy:0.3992, Validation Loss:1.0756, Validation Accuracy:0.3744\n",
    "Epoch #253: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0762, Validation Accuracy:0.3760\n",
    "Epoch #254: Loss:1.0705, Accuracy:0.3951, Validation Loss:1.0773, Validation Accuracy:0.3875\n",
    "Epoch #255: Loss:1.0704, Accuracy:0.3967, Validation Loss:1.0820, Validation Accuracy:0.3826\n",
    "Epoch #256: Loss:1.0706, Accuracy:0.3918, Validation Loss:1.0803, Validation Accuracy:0.3793\n",
    "Epoch #257: Loss:1.0707, Accuracy:0.3893, Validation Loss:1.0794, Validation Accuracy:0.3727\n",
    "Epoch #258: Loss:1.0704, Accuracy:0.3963, Validation Loss:1.0784, Validation Accuracy:0.3645\n",
    "Epoch #259: Loss:1.0705, Accuracy:0.3914, Validation Loss:1.0788, Validation Accuracy:0.3711\n",
    "Epoch #260: Loss:1.0702, Accuracy:0.3967, Validation Loss:1.0785, Validation Accuracy:0.3810\n",
    "Epoch #261: Loss:1.0704, Accuracy:0.3967, Validation Loss:1.0784, Validation Accuracy:0.3810\n",
    "Epoch #262: Loss:1.0704, Accuracy:0.4033, Validation Loss:1.0772, Validation Accuracy:0.3777\n",
    "Epoch #263: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0770, Validation Accuracy:0.3777\n",
    "Epoch #264: Loss:1.0697, Accuracy:0.4041, Validation Loss:1.0779, Validation Accuracy:0.3810\n",
    "Epoch #265: Loss:1.0697, Accuracy:0.4033, Validation Loss:1.0781, Validation Accuracy:0.3744\n",
    "Epoch #266: Loss:1.0697, Accuracy:0.4033, Validation Loss:1.0782, Validation Accuracy:0.3744\n",
    "Epoch #267: Loss:1.0699, Accuracy:0.4021, Validation Loss:1.0774, Validation Accuracy:0.3810\n",
    "Epoch #268: Loss:1.0697, Accuracy:0.4033, Validation Loss:1.0777, Validation Accuracy:0.3744\n",
    "Epoch #269: Loss:1.0697, Accuracy:0.4025, Validation Loss:1.0778, Validation Accuracy:0.3711\n",
    "Epoch #270: Loss:1.0696, Accuracy:0.4025, Validation Loss:1.0782, Validation Accuracy:0.3826\n",
    "Epoch #271: Loss:1.0696, Accuracy:0.4053, Validation Loss:1.0773, Validation Accuracy:0.3810\n",
    "Epoch #272: Loss:1.0693, Accuracy:0.4057, Validation Loss:1.0773, Validation Accuracy:0.3793\n",
    "Epoch #273: Loss:1.0695, Accuracy:0.4066, Validation Loss:1.0782, Validation Accuracy:0.3826\n",
    "Epoch #274: Loss:1.0693, Accuracy:0.4029, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #275: Loss:1.0692, Accuracy:0.4033, Validation Loss:1.0776, Validation Accuracy:0.3826\n",
    "Epoch #276: Loss:1.0693, Accuracy:0.4049, Validation Loss:1.0778, Validation Accuracy:0.3711\n",
    "Epoch #277: Loss:1.0691, Accuracy:0.4041, Validation Loss:1.0779, Validation Accuracy:0.3711\n",
    "Epoch #278: Loss:1.0693, Accuracy:0.3996, Validation Loss:1.0774, Validation Accuracy:0.3793\n",
    "Epoch #279: Loss:1.0693, Accuracy:0.4033, Validation Loss:1.0779, Validation Accuracy:0.3793\n",
    "Epoch #280: Loss:1.0688, Accuracy:0.4025, Validation Loss:1.0778, Validation Accuracy:0.3711\n",
    "Epoch #281: Loss:1.0688, Accuracy:0.4041, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #282: Loss:1.0691, Accuracy:0.4021, Validation Loss:1.0772, Validation Accuracy:0.3678\n",
    "Epoch #283: Loss:1.0693, Accuracy:0.4016, Validation Loss:1.0774, Validation Accuracy:0.3793\n",
    "Epoch #284: Loss:1.0689, Accuracy:0.4025, Validation Loss:1.0776, Validation Accuracy:0.3711\n",
    "Epoch #285: Loss:1.0687, Accuracy:0.4033, Validation Loss:1.0776, Validation Accuracy:0.3695\n",
    "Epoch #286: Loss:1.0688, Accuracy:0.4045, Validation Loss:1.0776, Validation Accuracy:0.3695\n",
    "Epoch #287: Loss:1.0683, Accuracy:0.4082, Validation Loss:1.0769, Validation Accuracy:0.3793\n",
    "Epoch #288: Loss:1.0687, Accuracy:0.4045, Validation Loss:1.0766, Validation Accuracy:0.3793\n",
    "Epoch #289: Loss:1.0687, Accuracy:0.4037, Validation Loss:1.0778, Validation Accuracy:0.3678\n",
    "Epoch #290: Loss:1.0685, Accuracy:0.4049, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #291: Loss:1.0685, Accuracy:0.4025, Validation Loss:1.0775, Validation Accuracy:0.3678\n",
    "Epoch #292: Loss:1.0683, Accuracy:0.4037, Validation Loss:1.0779, Validation Accuracy:0.3695\n",
    "Epoch #293: Loss:1.0684, Accuracy:0.4033, Validation Loss:1.0778, Validation Accuracy:0.3810\n",
    "Epoch #294: Loss:1.0683, Accuracy:0.4033, Validation Loss:1.0771, Validation Accuracy:0.3810\n",
    "Epoch #295: Loss:1.0684, Accuracy:0.4021, Validation Loss:1.0780, Validation Accuracy:0.3678\n",
    "Epoch #296: Loss:1.0683, Accuracy:0.4078, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #297: Loss:1.0684, Accuracy:0.4041, Validation Loss:1.0768, Validation Accuracy:0.3777\n",
    "Epoch #298: Loss:1.0687, Accuracy:0.3992, Validation Loss:1.0776, Validation Accuracy:0.3678\n",
    "Epoch #299: Loss:1.0693, Accuracy:0.4041, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #300: Loss:1.0685, Accuracy:0.4066, Validation Loss:1.0773, Validation Accuracy:0.3793\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07730091, Accuracy:0.3793\n",
    "Labels: ['03', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03  02   01\n",
    "t:03   0  27  115\n",
    "t:02   0  45  182\n",
    "t:01   0  54  186\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       142\n",
    "          02       0.36      0.20      0.25       227\n",
    "          01       0.39      0.78      0.51       240\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.25      0.32      0.26       609\n",
    "weighted avg       0.28      0.38      0.30       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 08:47:33 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0840764139673393, 1.0763747930918226, 1.074961123403853, 1.0748516426885069, 1.07481144097051, 1.074310394930722, 1.0742700354414816, 1.074338348236773, 1.0743273404627207, 1.074267804524777, 1.0743104311437246, 1.074234889058644, 1.0741929418739231, 1.0741109842150083, 1.0740188904388002, 1.0739705693741346, 1.073825084125663, 1.0739023456432548, 1.0740092890994695, 1.0740261475245159, 1.0740767006803615, 1.074010402893981, 1.074037365333983, 1.0739343655716218, 1.0739755628535705, 1.0735497433563759, 1.0735955451705381, 1.0736583449766162, 1.0740603858418456, 1.074165364987353, 1.0737907498928125, 1.0736280487871719, 1.0736457428517208, 1.0736370168883225, 1.07370322326134, 1.0737234886448175, 1.0737596031871728, 1.0735067281816981, 1.0730667053576566, 1.0733378683209223, 1.073483592575211, 1.0733079747809173, 1.0731365265713144, 1.0729076331863654, 1.0729606112431618, 1.0739110373511103, 1.073649150005898, 1.0737853433894016, 1.0737450459516302, 1.0740607094099173, 1.073803854106095, 1.0735210258580976, 1.0738640726102004, 1.073819671162635, 1.0738456172896136, 1.0734853388249188, 1.0734360954052904, 1.0735047644582287, 1.0735984357511272, 1.0736793109348841, 1.0735067546074026, 1.073797819062407, 1.0738944778301445, 1.0739495327515751, 1.0738715125226426, 1.0737447871754713, 1.0735901380798891, 1.0736822606307532, 1.0738376719611031, 1.0739857627840466, 1.0742536710792379, 1.0738875186697798, 1.0741157077607655, 1.0743859509137659, 1.0751278897615881, 1.0750201633215342, 1.0744615967442053, 1.0744504057519346, 1.0743140044862218, 1.0743563746779619, 1.0742718100743536, 1.0743126963159721, 1.074562644332109, 1.0746097695847059, 1.0747913104559987, 1.0746349979112497, 1.0744370188814862, 1.0745899109613328, 1.0747871485054004, 1.074642631612192, 1.0746750921647146, 1.0746113320289574, 1.0745555708561036, 1.0746290368595341, 1.074733467329116, 1.0747836822359433, 1.0745688845175632, 1.0743509777661027, 1.0741534612840424, 1.0741878779259417, 1.0746569897740932, 1.074454923568688, 1.0742956088681526, 1.0741876890310904, 1.074203990363135, 1.0744231494972467, 1.075769393119123, 1.075242587498256, 1.0754000112928193, 1.0749723805582583, 1.0748063196689623, 1.0751706569261348, 1.0750847395221979, 1.0750017906057423, 1.07489185282358, 1.0751450459162395, 1.0751965077248309, 1.0751089754167253, 1.0753008647896778, 1.0752041242001287, 1.0751870019095284, 1.0752285274574518, 1.075550401347807, 1.0753595549093287, 1.0754344576881045, 1.0751446602966985, 1.075786697061974, 1.0760044509358397, 1.075624123582699, 1.0761533010573614, 1.0755720093528234, 1.0757180720518766, 1.0762881414447902, 1.0760696173105726, 1.0757351904275578, 1.0755249508495988, 1.0858902371379933, 1.0785058357053985, 1.0762092748103276, 1.0750406641874015, 1.0754111429740643, 1.0750371626836717, 1.0754838948962333, 1.074438379511653, 1.0746101283870504, 1.0747970019655275, 1.0748920342800847, 1.0749073782186398, 1.0751458903643103, 1.0752710638375118, 1.0749876773220368, 1.07529116009648, 1.0749462510173153, 1.0749211820279827, 1.0751176127072037, 1.0752834017053614, 1.0750707524946366, 1.0749352702562054, 1.074848678507437, 1.074898586876091, 1.0746699483523816, 1.0745658655276245, 1.0746001750964838, 1.0742743649506217, 1.0743613153059886, 1.073955755515639, 1.0739258927077495, 1.074032377922672, 1.0746812552262606, 1.0740806639488107, 1.0739428699505935, 1.0741805470439991, 1.0739276540298963, 1.0743795470846893, 1.0743600685999704, 1.0741807255643145, 1.0743830043498324, 1.0745022946782103, 1.0743567411143988, 1.0749366986144744, 1.0750957854667125, 1.075043919246968, 1.0749147848542688, 1.075760236514613, 1.075154472650174, 1.0763530962181405, 1.0753311196767246, 1.0757009162887172, 1.0769094956919478, 1.0756961177722575, 1.0758956872379446, 1.0765298097983174, 1.0760380074699916, 1.0763784459071795, 1.0766124827129695, 1.076468561866209, 1.076414737599628, 1.0763690555819934, 1.077242311194221, 1.0770190613610404, 1.076689703515402, 1.0783419550346036, 1.0809243780443039, 1.0792106162934076, 1.0789718835420405, 1.0802610766124257, 1.081102465173881, 1.080221516941177, 1.0790856902430994, 1.078438338583522, 1.078866369227079, 1.0783808871443048, 1.0776024138790437, 1.0779468621722192, 1.077846835790988, 1.0777900731818038, 1.07852914454706, 1.0779980229235244, 1.0786650349158176, 1.0797250854166467, 1.076356000109455, 1.075537723860717, 1.076022632603575, 1.0742740977573864, 1.0793096545490333, 1.080382788514073, 1.07828148303948, 1.0751504197300752, 1.078334762349309, 1.075881853675216, 1.0761926084120677, 1.076204713342225, 1.076536364156037, 1.0784890244551284, 1.078007654603479, 1.0792614487787382, 1.079022886522102, 1.077917555282856, 1.0768228994410223, 1.0780285790636035, 1.0782256934834622, 1.0783013099520078, 1.07765749855386, 1.0771018239273422, 1.0776199985216013, 1.0772499296269784, 1.0781948537074875, 1.0778158576226196, 1.0786356246725874, 1.0788316444810389, 1.0788390562060628, 1.0755678732406917, 1.076195897922923, 1.0772866715351348, 1.0819985206882745, 1.0802682843701592, 1.0794250162559973, 1.0784385650615973, 1.078781076644246, 1.078523620400327, 1.078374411476461, 1.0771860753374147, 1.0769893075836507, 1.0779283242468372, 1.0781073942168788, 1.0782291346974364, 1.0774433849676097, 1.0776878246924364, 1.0778317183305086, 1.078215967845447, 1.0772792188992053, 1.0772681199075358, 1.0782376148039092, 1.077986166199244, 1.077566936685534, 1.0777781485336755, 1.0778982786122213, 1.0774050618235897, 1.0778943634972784, 1.0777929475154784, 1.078024514985985, 1.077246597443504, 1.0774346427572967, 1.0775695878688143, 1.077625986782005, 1.0776247026885084, 1.076931562134002, 1.076579983207001, 1.0777525792176696, 1.076804581925591, 1.0775433291355376, 1.0778579146208238, 1.077795820870423, 1.077075646820131, 1.0779807614575465, 1.0767096819353026, 1.0767563489466074, 1.077606396526343, 1.07684733832411, 1.0773008715343007], 'val_acc': [0.39573070465637544, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.37438423513191676, 0.37602627135458444, 0.39573070465637544, 0.3940886685315807, 0.39573070465637544, 0.39573070465637544, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.39244663240678596, 0.3908045962819912, 0.3908045962819912, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.39244663230891297, 0.39244663240678596, 0.3908045962819912, 0.3940886685315807, 0.3940886685315807, 0.3973727407811702, 0.3858784882990989, 0.379310343506301, 0.385878487907607, 0.3940886685315807, 0.3940886685315807, 0.3908045963798642, 0.3875205241302747, 0.39408866882519966, 0.3940886685315807, 0.3940886685315807, 0.3940886686294537, 0.39737274146628104, 0.39244663260253193, 0.3940886687273267, 0.3973727412705351, 0.39737274107478915, 0.39573070465637544, 0.3940886684337077, 0.39573070455850246, 0.40886699443771723, 0.4022988498406653, 0.4072249583129225, 0.39737274097691616, 0.4006568139116165, 0.4072249583129225, 0.4072249583129225, 0.39244663260253193, 0.3908045962819912, 0.40558292209025476, 0.3940886685315807, 0.39737273750242535, 0.4072249583129225, 0.403940886063333, 0.40558291822427206, 0.3809523804140796, 0.40558292199238183, 0.38259441644100134, 0.3957307055372323, 0.3990147771995839, 0.39573070475424843, 0.40065681420523547, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.37274220018159776, 0.37931034438715777, 0.39408866911881857, 0.39408866911881857, 0.39408866921669156, 0.38752052481538557, 0.3891625609401803, 0.3924466332876428, 0.3990147774932028, 0.39080459696710207, 0.3940886685315807, 0.3940886685315807, 0.3908045968692291, 0.3940886696081835, 0.39244663240678596, 0.39244663240678596, 0.3957307054393593, 0.38916256064656135, 0.3908045967713561, 0.3957307050478674, 0.4039408857697141, 0.38259441683249323, 0.384236452957288, 0.3957307051457404, 0.3957307051457404, 0.4055829226774927, 0.3875205207536569, 0.4039408857697141, 0.41050903075825795, 0.3924466336791347, 0.3908045976522129, 0.38587848849484485, 0.38095237963109574, 0.38423645178281224, 0.38259441644100134, 0.39573070475424843, 0.3760262718439494, 0.3825944166367473, 0.39573070465637544, 0.3825944166367473, 0.3711001632738192, 0.3924466327982779, 0.3809523805119525, 0.37438423571915463, 0.3924466327982779, 0.37438423562128165, 0.37438423571915463, 0.37438423562128165, 0.40229884964491935, 0.3776683081644901, 0.3776683081644901, 0.38095238002258763, 0.3727421994964869, 0.38259441673462025, 0.384236452859415, 0.37766830777299815, 0.38587848898420973, 0.38587848898420973, 0.37766830787087113, 0.38916255687845164, 0.3908045930032464, 0.38916255687845164, 0.39244663260253193, 0.3628899823562265, 0.38916256025506946, 0.3940886685315807, 0.38752052491325856, 0.3940886685315807, 0.3940886685315807, 0.4072249584107955, 0.38423645266366907, 0.39737274156415403, 0.3842364518806852, 0.40558292228600074, 0.3825944155601445, 0.39244663289615084, 0.4006568138137435, 0.3809523804140796, 0.3825944156580175, 0.3825944156580175, 0.3809523805119525, 0.39737274146628104, 0.3825944156580175, 0.38423645266366907, 0.366174054507943, 0.36781609063273774, 0.36781609063273774, 0.36617405441007006, 0.36945812695327845, 0.366174054605816, 0.37931034428928484, 0.366174054507943, 0.35960590991089103, 0.379310343310555, 0.36945812705115144, 0.3825944166367473, 0.3612479460356858, 0.379310343408428, 0.37274219930074093, 0.3727421993986139, 0.36453201838314825, 0.359605910008764, 0.359605910008764, 0.37766830777299815, 0.366174054605816, 0.37274219930074093, 0.359605910008764, 0.37766830777299815, 0.37766830777299815, 0.36453201838314825, 0.36453201848102124, 0.36124794613355876, 0.366174054605816, 0.366174054605816, 0.37602627135458444, 0.37766830777299815, 0.3793103437999199, 0.37766830777299815, 0.366174054605816, 0.3793103437999199, 0.36124794613355876, 0.37766830777299815, 0.3711001630780732, 0.366174054605816, 0.3793103437999199, 0.36453201848102124, 0.3711001630780732, 0.3711001631759462, 0.36945812695327845, 0.36781609063273774, 0.3825944159516364, 0.37602627135458444, 0.38423645266366907, 0.36945812685540547, 0.3760262716482034, 0.37438423542553567, 0.37602627145245743, 0.3760262715503304, 0.38095237982684166, 0.38259441585376347, 0.3711001630780732, 0.3760262715503304, 0.37602627145245743, 0.37766830747937924, 0.3678160907306107, 0.36945812685540547, 0.37438423542553567, 0.366174054507943, 0.3678160905348648, 0.38916256025506946, 0.36617405470368897, 0.37110016376318405, 0.3645320151044034, 0.3842364522721771, 0.3924466329940238, 0.3940886690209456, 0.3760262718439494, 0.37602627233331426, 0.3825944161473824, 0.39080459667348316, 0.40065681371587053, 0.3908045963798642, 0.39573070455850246, 0.38916256015719647, 0.3973727407811702, 0.3940886685315807, 0.39573070465637544, 0.38423645178281224, 0.3825944156580175, 0.372742199007122, 0.379310343506301, 0.385878487907607, 0.3628899823562265, 0.39408866882519966, 0.37766830738150625, 0.3891625599614505, 0.38095237943534976, 0.3743842353276627, 0.37602627135458444, 0.3875205243260207, 0.38259441585376347, 0.3793103437999199, 0.37274219910499495, 0.36453201828527526, 0.3711001629802002, 0.38095237992471465, 0.38095237992471465, 0.37766830747937924, 0.37766830747937924, 0.38095237992471465, 0.3743842353276627, 0.3743842353276627, 0.38095237992471465, 0.3743842353276627, 0.3711001632738192, 0.3825944161473824, 0.38095238002258763, 0.3793103437999199, 0.3825944161473824, 0.3711001632738192, 0.3825944161473824, 0.3711001632738192, 0.3711001632738192, 0.379310343604174, 0.379310343604174, 0.3711001632738192, 0.3711001631759462, 0.3678160911221027, 0.379310343604174, 0.3711001632738192, 0.36945812714902443, 0.36945812695327845, 0.379310343604174, 0.379310343604174, 0.3678160908284837, 0.3793103437020469, 0.3678160911221027, 0.36945812695327845, 0.3809523797289687, 0.38095237982684166, 0.3678160911221027, 0.3793103437020469, 0.37766830747937924, 0.3678160911221027, 0.3793103437020469, 0.3793103437020469], 'loss': [1.0966719873632005, 1.0795025347684197, 1.0748845370643192, 1.0748499751335787, 1.075675181001119, 1.074457072379408, 1.0746574822392552, 1.074513885470631, 1.0741684223347376, 1.0742778293895525, 1.0741387569928806, 1.0741810008485704, 1.074017116666085, 1.074021740615735, 1.0740958055186811, 1.0739407367530056, 1.073900978521155, 1.073871225693877, 1.0739081406740192, 1.0737901063425586, 1.0737570075283795, 1.073752835350115, 1.0737570292651042, 1.0737643777467387, 1.0736703789454467, 1.0736309326160125, 1.0736925366967611, 1.073696130599819, 1.073838386643349, 1.0738936354247453, 1.073942640821547, 1.0738994062803608, 1.0738522738157112, 1.0739513506879552, 1.0738301675177697, 1.0738465656245268, 1.0737612770812957, 1.0737908440693693, 1.0738136682667037, 1.0737686185621382, 1.0739582657569242, 1.0737725295570106, 1.0738569542123062, 1.0734124581671838, 1.073829084989716, 1.073568111474509, 1.0739685261274021, 1.0739358848125293, 1.0738469513534765, 1.0739371939606246, 1.0739705818144938, 1.073713001186598, 1.073489009085622, 1.0737226219392655, 1.073629685642783, 1.073573866664017, 1.073551176167122, 1.0733867677085454, 1.0735340868912684, 1.0735622740868915, 1.0736319263368173, 1.073543481170764, 1.0736496674206712, 1.073673950622214, 1.0735674776825328, 1.0736281946699233, 1.0735733780772778, 1.073829296531129, 1.073525888865978, 1.0735304773710592, 1.073390375270491, 1.073408504775907, 1.0733349983697065, 1.0736174455658367, 1.0735927381065102, 1.0737928525133544, 1.0748452541519729, 1.0739889673628602, 1.0738544882690149, 1.0734905734444056, 1.0736644604857208, 1.0736341151858257, 1.0737183199770886, 1.0737375305908172, 1.0736143715817337, 1.073979013656444, 1.0734265963399678, 1.0738871112251673, 1.07350809691623, 1.0733098976910727, 1.073267209799138, 1.0729913116725318, 1.0729713832573236, 1.0730210451130016, 1.0734150975147068, 1.0728451161413957, 1.072907504996235, 1.0728213656854335, 1.0724720738506905, 1.072766650139184, 1.0730159381576632, 1.0726605245954446, 1.0727861589474845, 1.0727052443815697, 1.0722988558011377, 1.0725670380758798, 1.0737048357663947, 1.072514286080425, 1.073007008378266, 1.0733654194543985, 1.072292297576732, 1.0723601153254265, 1.0730758843725468, 1.0723232553235313, 1.072238971516337, 1.0723902022814114, 1.0720053419195406, 1.0725709735980025, 1.0718304134247485, 1.071960880183586, 1.0722414451458127, 1.0718218633036838, 1.0715062347036122, 1.0716144570090198, 1.0718553623379623, 1.0711556608427233, 1.070876063556397, 1.0711269735066062, 1.0708630606379108, 1.0705669464761471, 1.0705100518722064, 1.0704571810101582, 1.0701598139024613, 1.0707592550978768, 1.0707809805625272, 1.0712048205017308, 1.0721717732153389, 1.0761946909726279, 1.0753014998269521, 1.0730555811457076, 1.0741321304250793, 1.0725420659572438, 1.0731400857471098, 1.0725027004551348, 1.0723079403812636, 1.072335728238006, 1.0724974199486954, 1.0725448106103854, 1.0724022796266623, 1.072463062852315, 1.0723789126966035, 1.0722764216898892, 1.0721765850603704, 1.072437549959218, 1.0723922271748099, 1.0718823219471643, 1.071998849590701, 1.0715597341192822, 1.0721362311247682, 1.0715900977778974, 1.0715503922233347, 1.071392362954925, 1.071464751975982, 1.0712975949966932, 1.0712750429490263, 1.0708531404667567, 1.0710407681044123, 1.070983469853411, 1.0704682448561431, 1.0702029690850197, 1.0701755136435036, 1.0700576866921458, 1.069965719931914, 1.0700742783242916, 1.06981444368617, 1.0697783747737657, 1.069603318061672, 1.0702880227835028, 1.069402056406166, 1.0695975670824305, 1.0692881235107015, 1.0688301084467517, 1.069844704927605, 1.0699029989555875, 1.0702795312634727, 1.0687692695574593, 1.0688900766920995, 1.0695410309386204, 1.0693129947787683, 1.068606479114086, 1.06840040889364, 1.06796757504191, 1.0688493574424447, 1.0686592049667234, 1.0682140383632277, 1.0681214485814683, 1.067929765478052, 1.0680175899235376, 1.0680680410083558, 1.068134590047096, 1.0680569901358665, 1.0677948991376027, 1.0680478262950264, 1.068162877946419, 1.0685387570755194, 1.0679250909072908, 1.067970460983762, 1.068205729891877, 1.0683226337667853, 1.067888108024362, 1.0678903407874294, 1.0680040267458686, 1.0673889803445804, 1.0674365808342028, 1.0673909197108211, 1.0677521928869478, 1.0671830490629286, 1.066896645929779, 1.0664078698755535, 1.0669904983998324, 1.0662747358149816, 1.0670944633914705, 1.0687026667643866, 1.0699691024404288, 1.0682821134767004, 1.0692026851113572, 1.085639049580944, 1.0806774384676798, 1.0767124790920124, 1.075237378251626, 1.0726768790818828, 1.0729512605823774, 1.0728766578423659, 1.072242724626216, 1.071913092092322, 1.0721960942113669, 1.0725573721362824, 1.0722021301179452, 1.0717281110477643, 1.0712759391483095, 1.070990242556625, 1.070925688302982, 1.0705601161510303, 1.0703702347479318, 1.0700953099272335, 1.0702001799303404, 1.0699398247368281, 1.070518594747696, 1.0707865043832046, 1.070423586020969, 1.0702715386600221, 1.0698159058235999, 1.0709104623637895, 1.0705312766578408, 1.0704107681094255, 1.0706023052242992, 1.0707430599651298, 1.0704432968731044, 1.070493571861079, 1.0702165225203277, 1.0704150382498208, 1.070431604326628, 1.0700882033890522, 1.069733051108139, 1.0697005983740397, 1.0696509326018346, 1.069867010478856, 1.0696694964990479, 1.06966176463838, 1.0695811820470822, 1.069623927071354, 1.0692924582737917, 1.0695377435527542, 1.069251464524553, 1.0692108878854363, 1.0692692250931288, 1.0690540151184833, 1.069331806885878, 1.0692922815405124, 1.0688130644557412, 1.06881097763226, 1.0690502426707524, 1.0692862043635312, 1.0688531002224837, 1.0687288181492924, 1.0688236146002579, 1.0683173715211527, 1.0686944840135515, 1.0686563462936902, 1.068521592112782, 1.0684812398906607, 1.0682930360829317, 1.068415608934798, 1.0683497624230824, 1.068359165514764, 1.0682766049794348, 1.0684060328794946, 1.068661717125033, 1.0692967466749939, 1.0685236020744213], 'acc': [0.36344969000904465, 0.39466118898235064, 0.39425051367747954, 0.394250514301676, 0.3926078044168758, 0.38973305733296904, 0.3942505121475862, 0.3942505129308916, 0.39466119191974586, 0.3942505119150424, 0.3938398342969726, 0.39425051171921605, 0.39466119012059125, 0.39466119309470393, 0.3950718701252947, 0.3942505146933287, 0.3946611921155722, 0.3938398374669116, 0.393839835080278, 0.3938398354719307, 0.39383983511699544, 0.39425051547663414, 0.3942505156724605, 0.39383983648777987, 0.39466119309470393, 0.39630390121706704, 0.3930184824265983, 0.39425051312671794, 0.3942505121475862, 0.39507186797120486, 0.39794661071021453, 0.39671457821094036, 0.39630390023793527, 0.39425051312671794, 0.3942505153175252, 0.3954825485266699, 0.3913757699960556, 0.3926078040252231, 0.3942505156724605, 0.39589322536143434, 0.3967145770359823, 0.39507187090860013, 0.39835729048237417, 0.3958932216040163, 0.394661189178177, 0.3975359344262117, 0.39630390376280955, 0.3963039023920251, 0.39301848203494566, 0.39342916004466816, 0.3913757701918819, 0.39260780226278597, 0.3975359350504082, 0.40164270966449556, 0.39794661403926246, 0.3975359358337136, 0.3999999981763671, 0.39630390337115684, 0.39466119332724775, 0.4000000013095887, 0.397125256649033, 0.3934291578905783, 0.40369609693482184, 0.4024640646680914, 0.3971252570406857, 0.39425051211086876, 0.40451745475342143, 0.3901437373376725, 0.3909650901871785, 0.3963039004337616, 0.3991786433318802, 0.40616016463822163, 0.3975359344262117, 0.39548254441431663, 0.3909650905788312, 0.4028747452602739, 0.3909650933571175, 0.39383983449279897, 0.3934291578905783, 0.3958932255572607, 0.39835728989489516, 0.3967145809892267, 0.39671458118505304, 0.39589322496978163, 0.39548254735171184, 0.3942505156724605, 0.3971252570406857, 0.40903490972714746, 0.394250513714197, 0.39342915965301545, 0.3934291586738837, 0.3958932245781289, 0.40082135380415945, 0.39794661126097614, 0.393018478314245, 0.39958932372823636, 0.4086242287433123, 0.3950718669553557, 0.39014373714184614, 0.40082135497911753, 0.40123203083475023, 0.4028747444769685, 0.39548254696005913, 0.40739219647658187, 0.4086242309341196, 0.40451745612420587, 0.39753593642119267, 0.4000000003671744, 0.40164271201441176, 0.399178646734363, 0.3950718685586839, 0.4036960977181272, 0.3889117050709421, 0.3983572918531586, 0.39137577195431905, 0.39958932353241, 0.3995893231407573, 0.4028747456519266, 0.3991786457185138, 0.39794661364760975, 0.3975359322721219, 0.40041067681028614, 0.3975359322721219, 0.39671457684015593, 0.40246406352985076, 0.4012320326339048, 0.3987679667296596, 0.4053388105770401, 0.4098562649632871, 0.40369609892980274, 0.40246406368895965, 0.4082135531202234, 0.4078028766771117, 0.4004106785360058, 0.40657084359035844, 0.40698152046184033, 0.39630390219619877, 0.39835728848739327, 0.379055443608051, 0.3983572916573323, 0.38809034826819166, 0.3995893221249081, 0.3946611907447878, 0.3909650912030277, 0.3872689951861419, 0.39055441315658773, 0.39466119012059125, 0.3921971275821114, 0.3905544161307004, 0.3958932261447397, 0.39425051312671794, 0.3942505129308916, 0.38809034881895327, 0.39425051508498143, 0.38644763995000225, 0.38767966986681646, 0.399589324119889, 0.39014373910010963, 0.38932238210153286, 0.3963039025878515, 0.39383983648777987, 0.3930184822674894, 0.3913757717584927, 0.39466119309470393, 0.39260780461270217, 0.395071866563703, 0.3979466138434361, 0.3954825455892747, 0.3975359338754501, 0.3991786466976456, 0.4032854232699964, 0.41601642488943724, 0.39383983609612716, 0.4028747416986822, 0.4016427126018908, 0.4016427086486464, 0.4028747427145314, 0.4024640676422041, 0.4004106785727233, 0.3995893201666446, 0.40328542189921196, 0.40451745612420587, 0.40000000095465343, 0.4036960988930853, 0.3983572916573323, 0.40205338607088986, 0.40410677733117795, 0.39917864689347193, 0.4082135527285707, 0.40164271201441176, 0.4004106760269807, 0.3995893201666446, 0.4016427127977171, 0.4000000011504798, 0.4020533898283079, 0.40780287687293804, 0.4012320314222293, 0.4028747452602739, 0.39342915710727294, 0.4016427082569937, 0.4045174527584405, 0.40410677494454433, 0.3987679668887685, 0.40328541912092564, 0.39835728911158974, 0.39835728871993703, 0.39794661149351995, 0.4016427127977171, 0.3999999987638462, 0.4045174523667878, 0.3975359350504082, 0.40903490816053667, 0.402464066467246, 0.39753593602953996, 0.40616016283906703, 0.39671458138087934, 0.404106777918657, 0.4078028739355428, 0.4049281317840122, 0.4086242309341196, 0.40739219749243105, 0.4098562624175446, 0.3979466142350888, 0.40574948858676263, 0.40698151787938036, 0.4008213563866194, 0.38357289584020815, 0.3930184824265983, 0.3954825467642328, 0.3983572893074161, 0.39999999935132524, 0.3971252578239911, 0.39219712304138793, 0.3979466138434361, 0.3897330618736925, 0.3897330606987344, 0.4024640652922879, 0.40246406349313335, 0.3987679657138104, 0.3999999985680198, 0.3987679666929421, 0.3999999987638462, 0.4024640654513968, 0.39958932372823636, 0.4053388070154484, 0.40492813217566487, 0.39958932392406266, 0.4028747448686212, 0.402053387833327, 0.3930184806641612, 0.4032854220950383, 0.39917864493520844, 0.40000000173795885, 0.39507186715118203, 0.39671457981426855, 0.3917864483974308, 0.3893223815140538, 0.39630390297950413, 0.39137577234597176, 0.3967145772318086, 0.39671457981426855, 0.40328542307417004, 0.4008213545874649, 0.404106774748718, 0.403285422486691, 0.40328542287834374, 0.40205339002413426, 0.40328542189921196, 0.40246406352985076, 0.4024640660755933, 0.4053388074071011, 0.40574948659178167, 0.4065708414362686, 0.40287474506444754, 0.4032854201000574, 0.40492813374227565, 0.40410677494454433, 0.3995893221616255, 0.40328542307417004, 0.4024640676422041, 0.404106778506136, 0.4020533858750635, 0.4016427088811902, 0.4024640648639178, 0.403285419316752, 0.4045174531500932, 0.4082135507335898, 0.40451745573255316, 0.40369609932145545, 0.4049281329589703, 0.40246406368895965, 0.40369609987221705, 0.40328541935346945, 0.40328542150755925, 0.4020533876742181, 0.4078028764812853, 0.40410677494454433, 0.3991786466976456, 0.4041067763520462, 0.4065708428437705]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
