{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf46.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 16:58:11 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '0Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['by', 'ib', 'mb', 'ds', 'yd', 'my', 'eo', 'ce', 'ek', 'eg', 'sg', 'eb', 'ck', 'aa', 'sk'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001850354E278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001855C427EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7061, Accuracy:0.0665, Validation Loss:2.7013, Validation Accuracy:0.0657\n",
    "Epoch #2: Loss:2.6990, Accuracy:0.0665, Validation Loss:2.6955, Validation Accuracy:0.0657\n",
    "Epoch #3: Loss:2.6937, Accuracy:0.0768, Validation Loss:2.6906, Validation Accuracy:0.0788\n",
    "Epoch #4: Loss:2.6891, Accuracy:0.0784, Validation Loss:2.6864, Validation Accuracy:0.0788\n",
    "Epoch #5: Loss:2.6847, Accuracy:0.0965, Validation Loss:2.6825, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6810, Accuracy:0.1023, Validation Loss:2.6787, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6773, Accuracy:0.1023, Validation Loss:2.6751, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6739, Accuracy:0.1023, Validation Loss:2.6716, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6704, Accuracy:0.1023, Validation Loss:2.6677, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6666, Accuracy:0.1023, Validation Loss:2.6636, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6620, Accuracy:0.1027, Validation Loss:2.6584, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6565, Accuracy:0.1146, Validation Loss:2.6512, Validation Accuracy:0.1264\n",
    "Epoch #13: Loss:2.6484, Accuracy:0.1339, Validation Loss:2.6402, Validation Accuracy:0.1396\n",
    "Epoch #14: Loss:2.6354, Accuracy:0.1446, Validation Loss:2.6216, Validation Accuracy:0.1445\n",
    "Epoch #15: Loss:2.6141, Accuracy:0.1491, Validation Loss:2.5917, Validation Accuracy:0.1691\n",
    "Epoch #16: Loss:2.5808, Accuracy:0.1663, Validation Loss:2.5496, Validation Accuracy:0.1757\n",
    "Epoch #17: Loss:2.5377, Accuracy:0.1663, Validation Loss:2.5055, Validation Accuracy:0.1609\n",
    "Epoch #18: Loss:2.4967, Accuracy:0.1630, Validation Loss:2.4673, Validation Accuracy:0.1642\n",
    "Epoch #19: Loss:2.4628, Accuracy:0.1659, Validation Loss:2.4317, Validation Accuracy:0.1741\n",
    "Epoch #20: Loss:2.4284, Accuracy:0.1713, Validation Loss:2.3966, Validation Accuracy:0.1921\n",
    "Epoch #21: Loss:2.3948, Accuracy:0.1815, Validation Loss:2.3660, Validation Accuracy:0.1970\n",
    "Epoch #22: Loss:2.3635, Accuracy:0.1704, Validation Loss:2.3360, Validation Accuracy:0.2299\n",
    "Epoch #23: Loss:2.3340, Accuracy:0.2246, Validation Loss:2.3109, Validation Accuracy:0.2381\n",
    "Epoch #24: Loss:2.3050, Accuracy:0.2308, Validation Loss:2.2829, Validation Accuracy:0.2381\n",
    "Epoch #25: Loss:2.2801, Accuracy:0.2423, Validation Loss:2.2605, Validation Accuracy:0.2430\n",
    "Epoch #26: Loss:2.2550, Accuracy:0.2546, Validation Loss:2.2424, Validation Accuracy:0.2463\n",
    "Epoch #27: Loss:2.2347, Accuracy:0.2571, Validation Loss:2.2217, Validation Accuracy:0.2545\n",
    "Epoch #28: Loss:2.2158, Accuracy:0.2554, Validation Loss:2.2132, Validation Accuracy:0.2447\n",
    "Epoch #29: Loss:2.2004, Accuracy:0.2583, Validation Loss:2.1913, Validation Accuracy:0.2512\n",
    "Epoch #30: Loss:2.1871, Accuracy:0.2595, Validation Loss:2.1773, Validation Accuracy:0.2545\n",
    "Epoch #31: Loss:2.1743, Accuracy:0.2616, Validation Loss:2.1695, Validation Accuracy:0.2644\n",
    "Epoch #32: Loss:2.1605, Accuracy:0.2653, Validation Loss:2.1574, Validation Accuracy:0.2611\n",
    "Epoch #33: Loss:2.1465, Accuracy:0.2682, Validation Loss:2.1467, Validation Accuracy:0.2627\n",
    "Epoch #34: Loss:2.1381, Accuracy:0.2674, Validation Loss:2.1460, Validation Accuracy:0.2677\n",
    "Epoch #35: Loss:2.1279, Accuracy:0.2764, Validation Loss:2.1274, Validation Accuracy:0.2660\n",
    "Epoch #36: Loss:2.1152, Accuracy:0.2825, Validation Loss:2.1206, Validation Accuracy:0.2742\n",
    "Epoch #37: Loss:2.1076, Accuracy:0.2825, Validation Loss:2.1101, Validation Accuracy:0.2759\n",
    "Epoch #38: Loss:2.0993, Accuracy:0.2879, Validation Loss:2.1102, Validation Accuracy:0.2775\n",
    "Epoch #39: Loss:2.0876, Accuracy:0.2932, Validation Loss:2.0982, Validation Accuracy:0.2824\n",
    "Epoch #40: Loss:2.0812, Accuracy:0.2932, Validation Loss:2.0837, Validation Accuracy:0.2808\n",
    "Epoch #41: Loss:2.0757, Accuracy:0.2982, Validation Loss:2.0921, Validation Accuracy:0.2824\n",
    "Epoch #42: Loss:2.0731, Accuracy:0.2965, Validation Loss:2.0784, Validation Accuracy:0.2874\n",
    "Epoch #43: Loss:2.0645, Accuracy:0.2945, Validation Loss:2.0803, Validation Accuracy:0.2956\n",
    "Epoch #44: Loss:2.0560, Accuracy:0.3018, Validation Loss:2.0603, Validation Accuracy:0.2939\n",
    "Epoch #45: Loss:2.0524, Accuracy:0.3023, Validation Loss:2.0601, Validation Accuracy:0.2972\n",
    "Epoch #46: Loss:2.0508, Accuracy:0.3138, Validation Loss:2.0739, Validation Accuracy:0.2939\n",
    "Epoch #47: Loss:2.0372, Accuracy:0.2986, Validation Loss:2.0628, Validation Accuracy:0.2857\n",
    "Epoch #48: Loss:2.0315, Accuracy:0.3158, Validation Loss:2.0374, Validation Accuracy:0.3169\n",
    "Epoch #49: Loss:2.0202, Accuracy:0.3051, Validation Loss:2.0266, Validation Accuracy:0.3186\n",
    "Epoch #50: Loss:2.0137, Accuracy:0.3228, Validation Loss:2.0196, Validation Accuracy:0.3284\n",
    "Epoch #51: Loss:2.0023, Accuracy:0.3265, Validation Loss:2.0159, Validation Accuracy:0.3268\n",
    "Epoch #52: Loss:1.9932, Accuracy:0.3335, Validation Loss:2.0103, Validation Accuracy:0.3235\n",
    "Epoch #53: Loss:1.9865, Accuracy:0.3322, Validation Loss:2.0038, Validation Accuracy:0.3268\n",
    "Epoch #54: Loss:1.9797, Accuracy:0.3322, Validation Loss:2.0054, Validation Accuracy:0.3284\n",
    "Epoch #55: Loss:1.9771, Accuracy:0.3355, Validation Loss:1.9983, Validation Accuracy:0.3218\n",
    "Epoch #56: Loss:1.9685, Accuracy:0.3396, Validation Loss:1.9928, Validation Accuracy:0.3350\n",
    "Epoch #57: Loss:1.9602, Accuracy:0.3359, Validation Loss:1.9915, Validation Accuracy:0.3235\n",
    "Epoch #58: Loss:1.9621, Accuracy:0.3351, Validation Loss:1.9878, Validation Accuracy:0.3251\n",
    "Epoch #59: Loss:1.9645, Accuracy:0.3474, Validation Loss:1.9772, Validation Accuracy:0.3317\n",
    "Epoch #60: Loss:1.9520, Accuracy:0.3322, Validation Loss:1.9806, Validation Accuracy:0.3235\n",
    "Epoch #61: Loss:1.9439, Accuracy:0.3405, Validation Loss:1.9765, Validation Accuracy:0.3300\n",
    "Epoch #62: Loss:1.9428, Accuracy:0.3515, Validation Loss:1.9651, Validation Accuracy:0.3268\n",
    "Epoch #63: Loss:1.9311, Accuracy:0.3450, Validation Loss:1.9598, Validation Accuracy:0.3333\n",
    "Epoch #64: Loss:1.9266, Accuracy:0.3556, Validation Loss:1.9601, Validation Accuracy:0.3235\n",
    "Epoch #65: Loss:1.9248, Accuracy:0.3528, Validation Loss:1.9570, Validation Accuracy:0.3202\n",
    "Epoch #66: Loss:1.9190, Accuracy:0.3462, Validation Loss:1.9478, Validation Accuracy:0.3235\n",
    "Epoch #67: Loss:1.9155, Accuracy:0.3507, Validation Loss:1.9515, Validation Accuracy:0.3153\n",
    "Epoch #68: Loss:1.9136, Accuracy:0.3515, Validation Loss:1.9444, Validation Accuracy:0.3169\n",
    "Epoch #69: Loss:1.9055, Accuracy:0.3470, Validation Loss:1.9429, Validation Accuracy:0.3136\n",
    "Epoch #70: Loss:1.9017, Accuracy:0.3540, Validation Loss:1.9393, Validation Accuracy:0.3186\n",
    "Epoch #71: Loss:1.8963, Accuracy:0.3556, Validation Loss:1.9354, Validation Accuracy:0.3153\n",
    "Epoch #72: Loss:1.8971, Accuracy:0.3417, Validation Loss:1.9355, Validation Accuracy:0.3103\n",
    "Epoch #73: Loss:1.8918, Accuracy:0.3515, Validation Loss:1.9327, Validation Accuracy:0.3169\n",
    "Epoch #74: Loss:1.8912, Accuracy:0.3561, Validation Loss:1.9361, Validation Accuracy:0.3120\n",
    "Epoch #75: Loss:1.8872, Accuracy:0.3622, Validation Loss:1.9276, Validation Accuracy:0.3136\n",
    "Epoch #76: Loss:1.8795, Accuracy:0.3511, Validation Loss:1.9183, Validation Accuracy:0.3071\n",
    "Epoch #77: Loss:1.8724, Accuracy:0.3610, Validation Loss:1.9228, Validation Accuracy:0.3136\n",
    "Epoch #78: Loss:1.8783, Accuracy:0.3569, Validation Loss:1.9119, Validation Accuracy:0.3120\n",
    "Epoch #79: Loss:1.8706, Accuracy:0.3598, Validation Loss:1.9234, Validation Accuracy:0.3087\n",
    "Epoch #80: Loss:1.8722, Accuracy:0.3524, Validation Loss:1.9100, Validation Accuracy:0.3136\n",
    "Epoch #81: Loss:1.8662, Accuracy:0.3634, Validation Loss:1.9193, Validation Accuracy:0.3153\n",
    "Epoch #82: Loss:1.8634, Accuracy:0.3577, Validation Loss:1.9061, Validation Accuracy:0.3136\n",
    "Epoch #83: Loss:1.8622, Accuracy:0.3602, Validation Loss:1.9039, Validation Accuracy:0.3153\n",
    "Epoch #84: Loss:1.8520, Accuracy:0.3717, Validation Loss:1.8990, Validation Accuracy:0.3186\n",
    "Epoch #85: Loss:1.8513, Accuracy:0.3708, Validation Loss:1.9046, Validation Accuracy:0.3268\n",
    "Epoch #86: Loss:1.8524, Accuracy:0.3684, Validation Loss:1.8903, Validation Accuracy:0.3153\n",
    "Epoch #87: Loss:1.8457, Accuracy:0.3721, Validation Loss:1.8935, Validation Accuracy:0.3202\n",
    "Epoch #88: Loss:1.8427, Accuracy:0.3704, Validation Loss:1.8993, Validation Accuracy:0.3284\n",
    "Epoch #89: Loss:1.8348, Accuracy:0.3778, Validation Loss:1.8857, Validation Accuracy:0.3169\n",
    "Epoch #90: Loss:1.8314, Accuracy:0.3754, Validation Loss:1.8801, Validation Accuracy:0.3186\n",
    "Epoch #91: Loss:1.8303, Accuracy:0.3754, Validation Loss:1.8872, Validation Accuracy:0.3235\n",
    "Epoch #92: Loss:1.8296, Accuracy:0.3700, Validation Loss:1.8766, Validation Accuracy:0.3235\n",
    "Epoch #93: Loss:1.8167, Accuracy:0.3725, Validation Loss:1.8778, Validation Accuracy:0.3268\n",
    "Epoch #94: Loss:1.8180, Accuracy:0.3828, Validation Loss:1.8672, Validation Accuracy:0.3218\n",
    "Epoch #95: Loss:1.8108, Accuracy:0.3877, Validation Loss:1.8700, Validation Accuracy:0.3202\n",
    "Epoch #96: Loss:1.8044, Accuracy:0.3795, Validation Loss:1.8641, Validation Accuracy:0.3284\n",
    "Epoch #97: Loss:1.8008, Accuracy:0.3832, Validation Loss:1.8598, Validation Accuracy:0.3317\n",
    "Epoch #98: Loss:1.7975, Accuracy:0.3877, Validation Loss:1.8609, Validation Accuracy:0.3300\n",
    "Epoch #99: Loss:1.7920, Accuracy:0.3844, Validation Loss:1.8537, Validation Accuracy:0.3300\n",
    "Epoch #100: Loss:1.7881, Accuracy:0.3856, Validation Loss:1.8517, Validation Accuracy:0.3284\n",
    "Epoch #101: Loss:1.7837, Accuracy:0.3881, Validation Loss:1.8633, Validation Accuracy:0.3383\n",
    "Epoch #102: Loss:1.7927, Accuracy:0.3852, Validation Loss:1.8534, Validation Accuracy:0.3383\n",
    "Epoch #103: Loss:1.7878, Accuracy:0.3889, Validation Loss:1.8750, Validation Accuracy:0.3235\n",
    "Epoch #104: Loss:1.7872, Accuracy:0.3832, Validation Loss:1.8408, Validation Accuracy:0.3399\n",
    "Epoch #105: Loss:1.7744, Accuracy:0.3873, Validation Loss:1.8451, Validation Accuracy:0.3514\n",
    "Epoch #106: Loss:1.7719, Accuracy:0.3906, Validation Loss:1.8330, Validation Accuracy:0.3481\n",
    "Epoch #107: Loss:1.7604, Accuracy:0.3918, Validation Loss:1.8370, Validation Accuracy:0.3415\n",
    "Epoch #108: Loss:1.7551, Accuracy:0.3959, Validation Loss:1.8421, Validation Accuracy:0.3563\n",
    "Epoch #109: Loss:1.7578, Accuracy:0.4000, Validation Loss:1.8336, Validation Accuracy:0.3514\n",
    "Epoch #110: Loss:1.7575, Accuracy:0.3922, Validation Loss:1.8461, Validation Accuracy:0.3333\n",
    "Epoch #111: Loss:1.7482, Accuracy:0.3881, Validation Loss:1.8292, Validation Accuracy:0.3695\n",
    "Epoch #112: Loss:1.7426, Accuracy:0.4111, Validation Loss:1.8231, Validation Accuracy:0.3465\n",
    "Epoch #113: Loss:1.7407, Accuracy:0.4045, Validation Loss:1.8246, Validation Accuracy:0.3415\n",
    "Epoch #114: Loss:1.7378, Accuracy:0.3947, Validation Loss:1.8255, Validation Accuracy:0.3415\n",
    "Epoch #115: Loss:1.7303, Accuracy:0.4016, Validation Loss:1.8251, Validation Accuracy:0.3727\n",
    "Epoch #116: Loss:1.7360, Accuracy:0.4049, Validation Loss:1.8140, Validation Accuracy:0.3514\n",
    "Epoch #117: Loss:1.7224, Accuracy:0.4140, Validation Loss:1.8140, Validation Accuracy:0.3580\n",
    "Epoch #118: Loss:1.7241, Accuracy:0.4103, Validation Loss:1.8135, Validation Accuracy:0.3563\n",
    "Epoch #119: Loss:1.7174, Accuracy:0.4103, Validation Loss:1.8084, Validation Accuracy:0.3629\n",
    "Epoch #120: Loss:1.7158, Accuracy:0.4049, Validation Loss:1.8115, Validation Accuracy:0.3612\n",
    "Epoch #121: Loss:1.7138, Accuracy:0.4057, Validation Loss:1.8057, Validation Accuracy:0.3596\n",
    "Epoch #122: Loss:1.7057, Accuracy:0.4119, Validation Loss:1.8090, Validation Accuracy:0.3695\n",
    "Epoch #123: Loss:1.7064, Accuracy:0.4140, Validation Loss:1.8018, Validation Accuracy:0.3596\n",
    "Epoch #124: Loss:1.7021, Accuracy:0.4131, Validation Loss:1.8088, Validation Accuracy:0.3580\n",
    "Epoch #125: Loss:1.6963, Accuracy:0.4193, Validation Loss:1.8018, Validation Accuracy:0.3629\n",
    "Epoch #126: Loss:1.6937, Accuracy:0.4214, Validation Loss:1.8031, Validation Accuracy:0.3612\n",
    "Epoch #127: Loss:1.7033, Accuracy:0.4115, Validation Loss:1.7939, Validation Accuracy:0.3612\n",
    "Epoch #128: Loss:1.6888, Accuracy:0.4131, Validation Loss:1.7917, Validation Accuracy:0.3629\n",
    "Epoch #129: Loss:1.6864, Accuracy:0.4201, Validation Loss:1.7940, Validation Accuracy:0.3612\n",
    "Epoch #130: Loss:1.6831, Accuracy:0.4218, Validation Loss:1.7877, Validation Accuracy:0.3695\n",
    "Epoch #131: Loss:1.6766, Accuracy:0.4205, Validation Loss:1.7918, Validation Accuracy:0.3662\n",
    "Epoch #132: Loss:1.6779, Accuracy:0.4267, Validation Loss:1.8003, Validation Accuracy:0.3645\n",
    "Epoch #133: Loss:1.6845, Accuracy:0.4287, Validation Loss:1.7898, Validation Accuracy:0.3727\n",
    "Epoch #134: Loss:1.6721, Accuracy:0.4300, Validation Loss:1.7855, Validation Accuracy:0.3662\n",
    "Epoch #135: Loss:1.6742, Accuracy:0.4185, Validation Loss:1.7838, Validation Accuracy:0.3777\n",
    "Epoch #136: Loss:1.6707, Accuracy:0.4279, Validation Loss:1.7815, Validation Accuracy:0.3629\n",
    "Epoch #137: Loss:1.6645, Accuracy:0.4308, Validation Loss:1.7870, Validation Accuracy:0.3629\n",
    "Epoch #138: Loss:1.6656, Accuracy:0.4304, Validation Loss:1.7860, Validation Accuracy:0.3662\n",
    "Epoch #139: Loss:1.6608, Accuracy:0.4271, Validation Loss:1.7824, Validation Accuracy:0.3547\n",
    "Epoch #140: Loss:1.6609, Accuracy:0.4251, Validation Loss:1.7861, Validation Accuracy:0.3645\n",
    "Epoch #141: Loss:1.6669, Accuracy:0.4275, Validation Loss:1.7812, Validation Accuracy:0.3777\n",
    "Epoch #142: Loss:1.6597, Accuracy:0.4193, Validation Loss:1.7725, Validation Accuracy:0.3793\n",
    "Epoch #143: Loss:1.6583, Accuracy:0.4292, Validation Loss:1.7703, Validation Accuracy:0.3662\n",
    "Epoch #144: Loss:1.6472, Accuracy:0.4366, Validation Loss:1.7739, Validation Accuracy:0.3612\n",
    "Epoch #145: Loss:1.6508, Accuracy:0.4329, Validation Loss:1.7666, Validation Accuracy:0.3744\n",
    "Epoch #146: Loss:1.6420, Accuracy:0.4366, Validation Loss:1.7675, Validation Accuracy:0.3645\n",
    "Epoch #147: Loss:1.6439, Accuracy:0.4370, Validation Loss:1.7644, Validation Accuracy:0.3727\n",
    "Epoch #148: Loss:1.6388, Accuracy:0.4366, Validation Loss:1.7612, Validation Accuracy:0.3711\n",
    "Epoch #149: Loss:1.6357, Accuracy:0.4345, Validation Loss:1.7774, Validation Accuracy:0.3563\n",
    "Epoch #150: Loss:1.6381, Accuracy:0.4341, Validation Loss:1.7671, Validation Accuracy:0.3645\n",
    "Epoch #151: Loss:1.6404, Accuracy:0.4337, Validation Loss:1.7640, Validation Accuracy:0.3760\n",
    "Epoch #152: Loss:1.6284, Accuracy:0.4480, Validation Loss:1.7681, Validation Accuracy:0.3744\n",
    "Epoch #153: Loss:1.6360, Accuracy:0.4320, Validation Loss:1.7589, Validation Accuracy:0.3662\n",
    "Epoch #154: Loss:1.6331, Accuracy:0.4366, Validation Loss:1.7719, Validation Accuracy:0.3678\n",
    "Epoch #155: Loss:1.6329, Accuracy:0.4341, Validation Loss:1.7725, Validation Accuracy:0.3875\n",
    "Epoch #156: Loss:1.6438, Accuracy:0.4316, Validation Loss:1.7542, Validation Accuracy:0.3842\n",
    "Epoch #157: Loss:1.6302, Accuracy:0.4431, Validation Loss:1.7713, Validation Accuracy:0.3859\n",
    "Epoch #158: Loss:1.6348, Accuracy:0.4361, Validation Loss:1.7587, Validation Accuracy:0.3826\n",
    "Epoch #159: Loss:1.6288, Accuracy:0.4398, Validation Loss:1.7556, Validation Accuracy:0.3645\n",
    "Epoch #160: Loss:1.6213, Accuracy:0.4423, Validation Loss:1.7636, Validation Accuracy:0.3793\n",
    "Epoch #161: Loss:1.6315, Accuracy:0.4361, Validation Loss:1.7864, Validation Accuracy:0.3514\n",
    "Epoch #162: Loss:1.6363, Accuracy:0.4390, Validation Loss:1.7616, Validation Accuracy:0.3645\n",
    "Epoch #163: Loss:1.6294, Accuracy:0.4407, Validation Loss:1.7494, Validation Accuracy:0.3727\n",
    "Epoch #164: Loss:1.6148, Accuracy:0.4456, Validation Loss:1.7440, Validation Accuracy:0.3744\n",
    "Epoch #165: Loss:1.6140, Accuracy:0.4534, Validation Loss:1.7458, Validation Accuracy:0.3924\n",
    "Epoch #166: Loss:1.6123, Accuracy:0.4398, Validation Loss:1.7423, Validation Accuracy:0.3842\n",
    "Epoch #167: Loss:1.6075, Accuracy:0.4509, Validation Loss:1.7472, Validation Accuracy:0.3826\n",
    "Epoch #168: Loss:1.6081, Accuracy:0.4468, Validation Loss:1.7484, Validation Accuracy:0.3645\n",
    "Epoch #169: Loss:1.6105, Accuracy:0.4427, Validation Loss:1.7533, Validation Accuracy:0.3744\n",
    "Epoch #170: Loss:1.6107, Accuracy:0.4427, Validation Loss:1.7562, Validation Accuracy:0.3875\n",
    "Epoch #171: Loss:1.6183, Accuracy:0.4378, Validation Loss:1.7487, Validation Accuracy:0.3777\n",
    "Epoch #172: Loss:1.6003, Accuracy:0.4501, Validation Loss:1.7362, Validation Accuracy:0.3908\n",
    "Epoch #173: Loss:1.5925, Accuracy:0.4480, Validation Loss:1.7460, Validation Accuracy:0.3744\n",
    "Epoch #174: Loss:1.5999, Accuracy:0.4522, Validation Loss:1.7347, Validation Accuracy:0.3727\n",
    "Epoch #175: Loss:1.6042, Accuracy:0.4411, Validation Loss:1.7395, Validation Accuracy:0.3842\n",
    "Epoch #176: Loss:1.6060, Accuracy:0.4464, Validation Loss:1.7330, Validation Accuracy:0.3826\n",
    "Epoch #177: Loss:1.6046, Accuracy:0.4464, Validation Loss:1.7324, Validation Accuracy:0.3957\n",
    "Epoch #178: Loss:1.6026, Accuracy:0.4480, Validation Loss:1.7539, Validation Accuracy:0.3662\n",
    "Epoch #179: Loss:1.5958, Accuracy:0.4534, Validation Loss:1.7311, Validation Accuracy:0.3892\n",
    "Epoch #180: Loss:1.5848, Accuracy:0.4550, Validation Loss:1.7368, Validation Accuracy:0.3892\n",
    "Epoch #181: Loss:1.5863, Accuracy:0.4587, Validation Loss:1.7302, Validation Accuracy:0.3826\n",
    "Epoch #182: Loss:1.5880, Accuracy:0.4493, Validation Loss:1.7372, Validation Accuracy:0.3760\n",
    "Epoch #183: Loss:1.6075, Accuracy:0.4472, Validation Loss:1.7305, Validation Accuracy:0.3842\n",
    "Epoch #184: Loss:1.5950, Accuracy:0.4530, Validation Loss:1.7500, Validation Accuracy:0.3924\n",
    "Epoch #185: Loss:1.5930, Accuracy:0.4517, Validation Loss:1.7360, Validation Accuracy:0.3695\n",
    "Epoch #186: Loss:1.5915, Accuracy:0.4517, Validation Loss:1.7277, Validation Accuracy:0.3892\n",
    "Epoch #187: Loss:1.5924, Accuracy:0.4505, Validation Loss:1.7499, Validation Accuracy:0.3810\n",
    "Epoch #188: Loss:1.5895, Accuracy:0.4563, Validation Loss:1.7535, Validation Accuracy:0.3695\n",
    "Epoch #189: Loss:1.5959, Accuracy:0.4559, Validation Loss:1.7299, Validation Accuracy:0.3842\n",
    "Epoch #190: Loss:1.5885, Accuracy:0.4563, Validation Loss:1.7332, Validation Accuracy:0.3974\n",
    "Epoch #191: Loss:1.5820, Accuracy:0.4583, Validation Loss:1.7303, Validation Accuracy:0.3760\n",
    "Epoch #192: Loss:1.5849, Accuracy:0.4505, Validation Loss:1.7377, Validation Accuracy:0.3711\n",
    "Epoch #193: Loss:1.5894, Accuracy:0.4509, Validation Loss:1.7265, Validation Accuracy:0.3810\n",
    "Epoch #194: Loss:1.5783, Accuracy:0.4608, Validation Loss:1.7270, Validation Accuracy:0.3957\n",
    "Epoch #195: Loss:1.5739, Accuracy:0.4632, Validation Loss:1.7282, Validation Accuracy:0.3826\n",
    "Epoch #196: Loss:1.5720, Accuracy:0.4600, Validation Loss:1.7288, Validation Accuracy:0.3826\n",
    "Epoch #197: Loss:1.5674, Accuracy:0.4567, Validation Loss:1.7214, Validation Accuracy:0.3793\n",
    "Epoch #198: Loss:1.5665, Accuracy:0.4649, Validation Loss:1.7267, Validation Accuracy:0.3941\n",
    "Epoch #199: Loss:1.5688, Accuracy:0.4600, Validation Loss:1.7180, Validation Accuracy:0.3924\n",
    "Epoch #200: Loss:1.5708, Accuracy:0.4575, Validation Loss:1.7436, Validation Accuracy:0.3793\n",
    "Epoch #201: Loss:1.5719, Accuracy:0.4501, Validation Loss:1.7263, Validation Accuracy:0.3810\n",
    "Epoch #202: Loss:1.5640, Accuracy:0.4649, Validation Loss:1.7201, Validation Accuracy:0.3810\n",
    "Epoch #203: Loss:1.5650, Accuracy:0.4587, Validation Loss:1.7229, Validation Accuracy:0.3892\n",
    "Epoch #204: Loss:1.5622, Accuracy:0.4595, Validation Loss:1.7206, Validation Accuracy:0.3990\n",
    "Epoch #205: Loss:1.5620, Accuracy:0.4600, Validation Loss:1.7255, Validation Accuracy:0.3842\n",
    "Epoch #206: Loss:1.5579, Accuracy:0.4649, Validation Loss:1.7214, Validation Accuracy:0.3990\n",
    "Epoch #207: Loss:1.5591, Accuracy:0.4604, Validation Loss:1.7137, Validation Accuracy:0.3908\n",
    "Epoch #208: Loss:1.5566, Accuracy:0.4612, Validation Loss:1.7226, Validation Accuracy:0.3777\n",
    "Epoch #209: Loss:1.5594, Accuracy:0.4657, Validation Loss:1.7236, Validation Accuracy:0.3908\n",
    "Epoch #210: Loss:1.5656, Accuracy:0.4620, Validation Loss:1.7340, Validation Accuracy:0.3875\n",
    "Epoch #211: Loss:1.5618, Accuracy:0.4616, Validation Loss:1.7314, Validation Accuracy:0.3760\n",
    "Epoch #212: Loss:1.5599, Accuracy:0.4600, Validation Loss:1.7141, Validation Accuracy:0.3941\n",
    "Epoch #213: Loss:1.5445, Accuracy:0.4723, Validation Loss:1.7112, Validation Accuracy:0.3826\n",
    "Epoch #214: Loss:1.5482, Accuracy:0.4710, Validation Loss:1.7166, Validation Accuracy:0.3826\n",
    "Epoch #215: Loss:1.5470, Accuracy:0.4694, Validation Loss:1.7097, Validation Accuracy:0.3924\n",
    "Epoch #216: Loss:1.5512, Accuracy:0.4690, Validation Loss:1.7217, Validation Accuracy:0.3941\n",
    "Epoch #217: Loss:1.5566, Accuracy:0.4645, Validation Loss:1.7276, Validation Accuracy:0.3859\n",
    "Epoch #218: Loss:1.5615, Accuracy:0.4682, Validation Loss:1.7066, Validation Accuracy:0.4023\n",
    "Epoch #219: Loss:1.5502, Accuracy:0.4604, Validation Loss:1.7188, Validation Accuracy:0.3908\n",
    "Epoch #220: Loss:1.5554, Accuracy:0.4678, Validation Loss:1.7168, Validation Accuracy:0.3859\n",
    "Epoch #221: Loss:1.5414, Accuracy:0.4637, Validation Loss:1.7081, Validation Accuracy:0.3892\n",
    "Epoch #222: Loss:1.5379, Accuracy:0.4702, Validation Loss:1.7079, Validation Accuracy:0.4007\n",
    "Epoch #223: Loss:1.5432, Accuracy:0.4645, Validation Loss:1.7132, Validation Accuracy:0.3810\n",
    "Epoch #224: Loss:1.5481, Accuracy:0.4674, Validation Loss:1.7154, Validation Accuracy:0.3777\n",
    "Epoch #225: Loss:1.5497, Accuracy:0.4678, Validation Loss:1.7501, Validation Accuracy:0.3826\n",
    "Epoch #226: Loss:1.5684, Accuracy:0.4608, Validation Loss:1.7208, Validation Accuracy:0.3990\n",
    "Epoch #227: Loss:1.5507, Accuracy:0.4620, Validation Loss:1.7291, Validation Accuracy:0.4089\n",
    "Epoch #228: Loss:1.5641, Accuracy:0.4661, Validation Loss:1.7032, Validation Accuracy:0.3974\n",
    "Epoch #229: Loss:1.5496, Accuracy:0.4752, Validation Loss:1.7143, Validation Accuracy:0.3744\n",
    "Epoch #230: Loss:1.5348, Accuracy:0.4756, Validation Loss:1.7029, Validation Accuracy:0.3892\n",
    "Epoch #231: Loss:1.5255, Accuracy:0.4739, Validation Loss:1.7068, Validation Accuracy:0.3941\n",
    "Epoch #232: Loss:1.5296, Accuracy:0.4813, Validation Loss:1.7028, Validation Accuracy:0.3974\n",
    "Epoch #233: Loss:1.5289, Accuracy:0.4727, Validation Loss:1.7025, Validation Accuracy:0.3974\n",
    "Epoch #234: Loss:1.5289, Accuracy:0.4797, Validation Loss:1.7013, Validation Accuracy:0.3842\n",
    "Epoch #235: Loss:1.5236, Accuracy:0.4780, Validation Loss:1.7076, Validation Accuracy:0.4023\n",
    "Epoch #236: Loss:1.5259, Accuracy:0.4739, Validation Loss:1.7002, Validation Accuracy:0.4007\n",
    "Epoch #237: Loss:1.5237, Accuracy:0.4768, Validation Loss:1.7022, Validation Accuracy:0.3924\n",
    "Epoch #238: Loss:1.5239, Accuracy:0.4760, Validation Loss:1.7031, Validation Accuracy:0.4023\n",
    "Epoch #239: Loss:1.5222, Accuracy:0.4801, Validation Loss:1.6992, Validation Accuracy:0.3974\n",
    "Epoch #240: Loss:1.5186, Accuracy:0.4801, Validation Loss:1.7032, Validation Accuracy:0.3990\n",
    "Epoch #241: Loss:1.5215, Accuracy:0.4805, Validation Loss:1.7046, Validation Accuracy:0.3859\n",
    "Epoch #242: Loss:1.5252, Accuracy:0.4764, Validation Loss:1.7011, Validation Accuracy:0.4007\n",
    "Epoch #243: Loss:1.5225, Accuracy:0.4743, Validation Loss:1.7023, Validation Accuracy:0.4039\n",
    "Epoch #244: Loss:1.5256, Accuracy:0.4752, Validation Loss:1.7065, Validation Accuracy:0.3826\n",
    "Epoch #245: Loss:1.5221, Accuracy:0.4752, Validation Loss:1.7236, Validation Accuracy:0.3744\n",
    "Epoch #246: Loss:1.5365, Accuracy:0.4743, Validation Loss:1.7070, Validation Accuracy:0.3974\n",
    "Epoch #247: Loss:1.5260, Accuracy:0.4752, Validation Loss:1.7057, Validation Accuracy:0.3941\n",
    "Epoch #248: Loss:1.5248, Accuracy:0.4809, Validation Loss:1.7204, Validation Accuracy:0.3941\n",
    "Epoch #249: Loss:1.5319, Accuracy:0.4747, Validation Loss:1.7086, Validation Accuracy:0.3941\n",
    "Epoch #250: Loss:1.5232, Accuracy:0.4772, Validation Loss:1.6982, Validation Accuracy:0.3974\n",
    "Epoch #251: Loss:1.5213, Accuracy:0.4776, Validation Loss:1.7062, Validation Accuracy:0.3727\n",
    "Epoch #252: Loss:1.5196, Accuracy:0.4756, Validation Loss:1.7110, Validation Accuracy:0.3777\n",
    "Epoch #253: Loss:1.5285, Accuracy:0.4760, Validation Loss:1.6998, Validation Accuracy:0.4039\n",
    "Epoch #254: Loss:1.5219, Accuracy:0.4789, Validation Loss:1.7041, Validation Accuracy:0.3990\n",
    "Epoch #255: Loss:1.5207, Accuracy:0.4747, Validation Loss:1.7049, Validation Accuracy:0.3908\n",
    "Epoch #256: Loss:1.5218, Accuracy:0.4719, Validation Loss:1.7213, Validation Accuracy:0.3826\n",
    "Epoch #257: Loss:1.5150, Accuracy:0.4825, Validation Loss:1.6954, Validation Accuracy:0.4023\n",
    "Epoch #258: Loss:1.5108, Accuracy:0.4858, Validation Loss:1.7169, Validation Accuracy:0.4007\n",
    "Epoch #259: Loss:1.5214, Accuracy:0.4768, Validation Loss:1.6934, Validation Accuracy:0.3892\n",
    "Epoch #260: Loss:1.5077, Accuracy:0.4813, Validation Loss:1.7011, Validation Accuracy:0.3892\n",
    "Epoch #261: Loss:1.5144, Accuracy:0.4813, Validation Loss:1.6942, Validation Accuracy:0.3826\n",
    "Epoch #262: Loss:1.5154, Accuracy:0.4813, Validation Loss:1.7026, Validation Accuracy:0.3974\n",
    "Epoch #263: Loss:1.5044, Accuracy:0.4887, Validation Loss:1.7010, Validation Accuracy:0.3941\n",
    "Epoch #264: Loss:1.5129, Accuracy:0.4715, Validation Loss:1.6964, Validation Accuracy:0.3908\n",
    "Epoch #265: Loss:1.4987, Accuracy:0.4846, Validation Loss:1.6915, Validation Accuracy:0.3875\n",
    "Epoch #266: Loss:1.5009, Accuracy:0.4842, Validation Loss:1.6971, Validation Accuracy:0.4007\n",
    "Epoch #267: Loss:1.4967, Accuracy:0.4879, Validation Loss:1.6866, Validation Accuracy:0.3957\n",
    "Epoch #268: Loss:1.4988, Accuracy:0.4817, Validation Loss:1.6946, Validation Accuracy:0.4007\n",
    "Epoch #269: Loss:1.4998, Accuracy:0.4809, Validation Loss:1.6911, Validation Accuracy:0.3908\n",
    "Epoch #270: Loss:1.4980, Accuracy:0.4887, Validation Loss:1.6926, Validation Accuracy:0.3974\n",
    "Epoch #271: Loss:1.4893, Accuracy:0.4879, Validation Loss:1.6866, Validation Accuracy:0.3957\n",
    "Epoch #272: Loss:1.4891, Accuracy:0.4797, Validation Loss:1.6855, Validation Accuracy:0.4039\n",
    "Epoch #273: Loss:1.4880, Accuracy:0.4867, Validation Loss:1.6860, Validation Accuracy:0.4007\n",
    "Epoch #274: Loss:1.4902, Accuracy:0.4846, Validation Loss:1.7014, Validation Accuracy:0.4023\n",
    "Epoch #275: Loss:1.4928, Accuracy:0.4879, Validation Loss:1.6988, Validation Accuracy:0.3924\n",
    "Epoch #276: Loss:1.4906, Accuracy:0.4871, Validation Loss:1.6899, Validation Accuracy:0.4039\n",
    "Epoch #277: Loss:1.4909, Accuracy:0.4899, Validation Loss:1.6874, Validation Accuracy:0.4039\n",
    "Epoch #278: Loss:1.4903, Accuracy:0.4887, Validation Loss:1.7033, Validation Accuracy:0.3760\n",
    "Epoch #279: Loss:1.4977, Accuracy:0.4821, Validation Loss:1.6898, Validation Accuracy:0.3941\n",
    "Epoch #280: Loss:1.4849, Accuracy:0.4887, Validation Loss:1.6832, Validation Accuracy:0.4007\n",
    "Epoch #281: Loss:1.4812, Accuracy:0.4903, Validation Loss:1.6861, Validation Accuracy:0.3924\n",
    "Epoch #282: Loss:1.4795, Accuracy:0.4883, Validation Loss:1.6823, Validation Accuracy:0.4023\n",
    "Epoch #283: Loss:1.4787, Accuracy:0.4916, Validation Loss:1.6924, Validation Accuracy:0.3941\n",
    "Epoch #284: Loss:1.4826, Accuracy:0.4895, Validation Loss:1.6794, Validation Accuracy:0.4089\n",
    "Epoch #285: Loss:1.4822, Accuracy:0.4879, Validation Loss:1.6827, Validation Accuracy:0.3908\n",
    "Epoch #286: Loss:1.4759, Accuracy:0.4899, Validation Loss:1.6918, Validation Accuracy:0.3777\n",
    "Epoch #287: Loss:1.4786, Accuracy:0.4920, Validation Loss:1.6864, Validation Accuracy:0.3875\n",
    "Epoch #288: Loss:1.4795, Accuracy:0.4879, Validation Loss:1.6837, Validation Accuracy:0.3957\n",
    "Epoch #289: Loss:1.4754, Accuracy:0.4903, Validation Loss:1.6824, Validation Accuracy:0.3957\n",
    "Epoch #290: Loss:1.4693, Accuracy:0.4977, Validation Loss:1.6854, Validation Accuracy:0.3957\n",
    "Epoch #291: Loss:1.4784, Accuracy:0.4854, Validation Loss:1.6843, Validation Accuracy:0.3908\n",
    "Epoch #292: Loss:1.4719, Accuracy:0.4936, Validation Loss:1.6798, Validation Accuracy:0.4056\n",
    "Epoch #293: Loss:1.4704, Accuracy:0.4936, Validation Loss:1.6806, Validation Accuracy:0.3990\n",
    "Epoch #294: Loss:1.4679, Accuracy:0.4936, Validation Loss:1.6803, Validation Accuracy:0.4023\n",
    "Epoch #295: Loss:1.4679, Accuracy:0.4965, Validation Loss:1.6756, Validation Accuracy:0.3941\n",
    "Epoch #296: Loss:1.4676, Accuracy:0.4891, Validation Loss:1.6938, Validation Accuracy:0.3957\n",
    "Epoch #297: Loss:1.4763, Accuracy:0.4846, Validation Loss:1.6828, Validation Accuracy:0.4138\n",
    "Epoch #298: Loss:1.4733, Accuracy:0.4916, Validation Loss:1.6833, Validation Accuracy:0.3908\n",
    "Epoch #299: Loss:1.4652, Accuracy:0.4957, Validation Loss:1.6836, Validation Accuracy:0.3924\n",
    "Epoch #300: Loss:1.4669, Accuracy:0.4945, Validation Loss:1.6832, Validation Accuracy:0.4007\n",
    "\n",
    "Test:\n",
    "Test Loss:1.68317378, Accuracy:0.4007\n",
    "Labels: ['by', 'ib', 'mb', 'ds', 'yd', 'my', 'eo', 'ce', 'ek', 'eg', 'sg', 'eb', 'ck', 'aa', 'sk']\n",
    "Confusion Matrix:\n",
    "      by  ib  mb  ds  yd  my  eo  ce  ek  eg  sg  eb  ck  aa  sk\n",
    "t:by  26   0   0   0   2   0   4   0   0   6   2   0   0   0   0\n",
    "t:ib   4  25   3   0   3   0   1   3   0   2  10   1   0   2   0\n",
    "t:mb   0   7  24   0   6   2   0   0   4   0   0   5   2   1   1\n",
    "t:ds   0   0   5   1   1   0   0   1   4   3   0   9   1   2   4\n",
    "t:yd   3   3   8   0  32   0   0   0   3   0   8   1   0   4   0\n",
    "t:my   0   3   5   0   1   0   0   1   1   1   0   7   1   0   0\n",
    "t:eo   6   0   0   0   0   0  25   0   0   2   1   0   0   0   0\n",
    "t:ce   2   7   2   0   0   0   1   1   1   6   1   0   6   0   0\n",
    "t:ek   0   0   9   0   3   1   0   0   8   0   0  26   0   0   1\n",
    "t:eg   7   3   3   1   0   0   0   2   3  15   1   0   2  11   2\n",
    "t:sg   0   9   3   0  11   0   0   0   0   1  22   0   1   4   0\n",
    "t:eb   0   0   2   1   2   0   0   0   9   0   0  36   0   0   0\n",
    "t:ck   0   1   7   0   0   0   0   1   2   0   0   3   7   1   1\n",
    "t:aa   0   1   2   0   4   0   0   0   1   5   2   0   0  18   1\n",
    "t:sk   0   0   9   0   0   1   0   1   4   2   0  11   1   0   4\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          by       0.54      0.65      0.59        40\n",
    "          ib       0.42      0.46      0.44        54\n",
    "          mb       0.29      0.46      0.36        52\n",
    "          ds       0.33      0.03      0.06        31\n",
    "          yd       0.49      0.52      0.50        62\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eo       0.81      0.74      0.77        34\n",
    "          ce       0.10      0.04      0.05        27\n",
    "          ek       0.20      0.17      0.18        48\n",
    "          eg       0.35      0.30      0.32        50\n",
    "          sg       0.47      0.43      0.45        51\n",
    "          eb       0.36      0.72      0.48        50\n",
    "          ck       0.33      0.30      0.32        23\n",
    "          aa       0.42      0.53      0.47        34\n",
    "          sk       0.29      0.12      0.17        33\n",
    "\n",
    "    accuracy                           0.40       609\n",
    "   macro avg       0.36      0.36      0.34       609\n",
    "weighted avg       0.38      0.40      0.37       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 17:39:00 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.701344890938995, 2.6954521666998152, 2.6906492377345392, 2.6864186100576117, 2.6825071371639106, 2.678718251352044, 2.6751407445553683, 2.671569818737863, 2.6677455471458495, 2.6635710292653303, 2.6583703523394706, 2.65120808909875, 2.6401656903265343, 2.6215840168970166, 2.591703513181464, 2.5496499397484538, 2.5054598845088814, 2.4672924865447046, 2.431683723953949, 2.3966147754775675, 2.366032926907093, 2.3360096186839887, 2.31086941971176, 2.282855505622275, 2.2604678988652473, 2.242425668415765, 2.2217216025823836, 2.2131654762086415, 2.1912838535747308, 2.177318669305059, 2.1694790573151437, 2.1574458097197935, 2.1467033199880317, 2.145972534157764, 2.127438217156822, 2.1206452736909362, 2.1101416656732166, 2.110211638589994, 2.098196712424994, 2.0837357795884457, 2.0920886339616698, 2.0783852845772928, 2.080324977098036, 2.0602574747771465, 2.0600725065898424, 2.073864103733808, 2.0627834432818033, 2.037388119595783, 2.0266148856120743, 2.0196202400282686, 2.01586520065032, 2.0103489663604837, 2.0037820374437154, 2.005436281852534, 1.9983091041176582, 1.9928230366291866, 1.991538458857043, 1.9877767163544453, 1.9771751069474495, 1.980591556317309, 1.976499451792299, 1.9651018428097804, 1.9598488361377435, 1.960085084677134, 1.9569889203276736, 1.9477817084401698, 1.9514948953744422, 1.9444311099686646, 1.9429293520540636, 1.9392531865531784, 1.9354331880954687, 1.9354881400545243, 1.9326868435040678, 1.936117873598985, 1.9275954259048738, 1.91833105972052, 1.922834258165657, 1.911947640879401, 1.9234311696148074, 1.9100406162061518, 1.9192940210082456, 1.9061106635236191, 1.903925134630626, 1.8990477227616585, 1.904573751396342, 1.8902584441581187, 1.8935134718179312, 1.8992805101209869, 1.8856857864336036, 1.880070482177296, 1.887196987720546, 1.8765718782280858, 1.8778211406886285, 1.8671506218526555, 1.8699846870597752, 1.8640554697055536, 1.8597706979131463, 1.8608784029636476, 1.8536785570858734, 1.8517464755595416, 1.8633477499919573, 1.853371526220162, 1.875011277511985, 1.8408444356448546, 1.845142501524125, 1.8329757872864922, 1.8370180985414728, 1.8421068819872852, 1.8335662399019514, 1.8461234771167898, 1.8292171097741339, 1.8230972910553755, 1.8245998881329064, 1.8255475712527196, 1.825131361707678, 1.8139672496635926, 1.8139728331213514, 1.813498176573141, 1.8084160831369986, 1.8114933149372219, 1.8057096082784467, 1.8090157532339612, 1.8018130590567252, 1.8088468595091345, 1.801813411399453, 1.8030567362977954, 1.7939155784929524, 1.791658015673971, 1.793998551486161, 1.787710335650076, 1.791777189924995, 1.8002788648621006, 1.789750299038754, 1.7855373734519595, 1.783763119739852, 1.781505062466576, 1.787035945405318, 1.7859797217380042, 1.7823633695470875, 1.7861043445778206, 1.7811545393932824, 1.7724828657454066, 1.7703240540227279, 1.7738761226531907, 1.766602991249761, 1.7674768665936975, 1.7643930996188586, 1.7611654442910882, 1.7774378605468324, 1.7671410164418087, 1.7639812794812206, 1.7681327772453697, 1.7589217902012841, 1.7719168892047676, 1.772517083117919, 1.754210376974397, 1.7713292496544975, 1.7586534152477247, 1.7555518107265478, 1.7635531656456307, 1.7864155708666898, 1.7615770669210524, 1.7493946595340724, 1.7439647186761615, 1.745832012791939, 1.7423275578002428, 1.7472297201798663, 1.7483583130859977, 1.753276122418922, 1.7562162412211226, 1.7486759136463035, 1.7361784903286712, 1.746026142281656, 1.7346607814672936, 1.7395375234935866, 1.7329648836884397, 1.732354199357808, 1.7538943525605601, 1.731066243793381, 1.7368467762356712, 1.7302234624994213, 1.7371848535850913, 1.7305411560390578, 1.7499708432477878, 1.7359733839927636, 1.7277095726949632, 1.7499275614671128, 1.753533797898316, 1.729891914452238, 1.733199105278416, 1.7303119864565595, 1.7377281071517268, 1.7265163765752256, 1.7270449184627563, 1.7282153534380282, 1.7288377044236132, 1.7214172585257168, 1.726679055365827, 1.7179982578030164, 1.7435702929160082, 1.726280279543208, 1.7201011182835144, 1.722890517198785, 1.720600475622907, 1.725466304616192, 1.7213883683795022, 1.7136760337403647, 1.7226422631681846, 1.7236387976284684, 1.7340456711247636, 1.7313747036045994, 1.7140517686975414, 1.7111707109535856, 1.716568312817215, 1.7096905720057747, 1.7216524553220651, 1.7276241473963696, 1.7066421602747124, 1.7187951313842498, 1.7167764986285632, 1.708133559508864, 1.7079072178682475, 1.7132161550333935, 1.7153646695398541, 1.7500544766878652, 1.7207933403979774, 1.7290794179944569, 1.7032303586969235, 1.71434313381834, 1.7029198228040547, 1.7067614834884117, 1.702768115965995, 1.702463788743481, 1.701333176326282, 1.707636896612609, 1.7001968694633647, 1.7021865723364067, 1.7031137258156963, 1.6992216352954483, 1.7032262039889257, 1.7046270789379752, 1.7011167705548416, 1.702348910724784, 1.706497238383113, 1.7235727314095584, 1.7069574909648677, 1.705650372653955, 1.7204359863779228, 1.7086075499335729, 1.6982291020783298, 1.7061689342379767, 1.711041252209831, 1.6998372181687254, 1.7040729211469001, 1.7048724901500008, 1.7213221986109792, 1.695350887544441, 1.716935348040952, 1.6934263486971801, 1.7011482018751072, 1.6942334263195544, 1.702576380645113, 1.7009791022255307, 1.696413843502552, 1.6915446891769008, 1.697138640289432, 1.686632962649679, 1.694553637543727, 1.6910633424232746, 1.6926154867181638, 1.68660577040392, 1.685498902167397, 1.6860432669838465, 1.7014380818713084, 1.6987628985704069, 1.6899468898773193, 1.6874053084791587, 1.7033417260118306, 1.689813582180756, 1.6832044312519392, 1.6861377500352406, 1.6823241851599933, 1.6924340601625114, 1.6794333863140913, 1.6826687986627589, 1.6918316110601566, 1.6864277171383937, 1.6837457300994196, 1.6823910664650803, 1.6854083947164475, 1.6842895063077679, 1.679755517414638, 1.6806367796238615, 1.680265637258395, 1.675610626663872, 1.6937962087308636, 1.6827703054706842, 1.683274336832106, 1.6836083606741894, 1.6831737134256974], 'val_acc': [0.06568144458806378, 0.06568144458806378, 0.07881773368429472, 0.07881773368429472, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.10180623952929414, 0.12643678060599736, 0.13957306980010128, 0.1444991782723585, 0.16912972024215267, 0.1756978644477127, 0.16091953912881404, 0.16420361137840353, 0.17405582822504498, 0.19211822569566014, 0.19704433426579035, 0.2298850570553042, 0.238095237385659, 0.23809523797289686, 0.24302134634728112, 0.24630541820537868, 0.25451559892722536, 0.24466338208058394, 0.2512315265797629, 0.25451559892722536, 0.2643678155781209, 0.2610837435242773, 0.26272577964907207, 0.26765188802345635, 0.2660098518007886, 0.27422003271838125, 0.27586206874530306, 0.2775041048700978, 0.28243021344022795, 0.2807881771196873, 0.28243021304873606, 0.28735632171673925, 0.2955665003587851, 0.2939244663137912, 0.29720853648357987, 0.2939244641361174, 0.28571428361001666, 0.3169129700789898, 0.3185550059101656, 0.3284072226589341, 0.3267651867298853, 0.3234811165600966, 0.32676518653413933, 0.32840722275680706, 0.3218390783555011, 0.334975367353859, 0.3234811144802958, 0.3251231505072176, 0.33169129490852356, 0.3234811144802958, 0.3300492589794748, 0.3267651866320123, 0.3333333333088651, 0.32348111438242283, 0.3201970442126341, 0.32348111646222366, 0.3152709357403769, 0.31691297196304463, 0.31362889971345514, 0.3185550059101656, 0.3152709359361229, 0.31034482746386566, 0.31691296988324386, 0.31198686160673256, 0.31362889961558216, 0.30706075531214916, 0.31362889971345514, 0.3119868634907874, 0.30870278935714307, 0.3136288975357814, 0.3152709336605761, 0.31362889981132813, 0.31527093395419503, 0.3185550080878394, 0.32676518653413933, 0.3152709337584491, 0.32019704203496036, 0.32840722324617194, 0.3169129720609176, 0.3185550059101656, 0.3234811144802958, 0.3234811148717877, 0.3267651867298853, 0.3218390782576281, 0.32019704252432524, 0.32840722285468005, 0.33169129539788844, 0.3300492590773478, 0.3300492588816018, 0.32840722305042597, 0.3382594399949404, 0.3382594397013215, 0.32348111467604174, 0.33990147572824325, 0.3513957289932984, 0.3481116570373278, 0.34154351204878397, 0.3563218374655556, 0.35139572909117134, 0.3333333315226832, 0.36945812705115144, 0.3464696205210412, 0.34154351204878397, 0.3415435122445299, 0.3727421994964869, 0.3513957292869173, 0.35796387359035037, 0.3563218374655556, 0.3628899820626076, 0.36124794593781284, 0.3596059097151451, 0.36945812665965955, 0.3596059098130181, 0.3579638734924774, 0.3628899820626076, 0.36124794593781284, 0.36124794593781284, 0.3628899819647346, 0.36124794574206687, 0.3694581267575325, 0.3661740542143241, 0.36453201838314825, 0.37274219890924903, 0.366174054605816, 0.3776683071857603, 0.3628899822583535, 0.3628899821604805, 0.36617405441007006, 0.3546798012428879, 0.36453201818740233, 0.37766830777299815, 0.3793103437020469, 0.3661740543121971, 0.36124794574206687, 0.3743842352297897, 0.36453201818740233, 0.37274219910499495, 0.3711001631759462, 0.3563218370740637, 0.36453201828527526, 0.37602627106096553, 0.37438423513191676, 0.3661740541164511, 0.36781609024124584, 0.3875205243260207, 0.3842364519785582, 0.38587848849484485, 0.3825944159516364, 0.36453201808952934, 0.3793103437020469, 0.35139572860180646, 0.36453201799165635, 0.37274219910499495, 0.37438423513191676, 0.3924466327982779, 0.3842364520764312, 0.3825944157558905, 0.36453201808952934, 0.3743842347404248, 0.3875205243260207, 0.3776683071857603, 0.3908045968692291, 0.3743842349361708, 0.37274219881137605, 0.3842364520764312, 0.3825944159516364, 0.3957307052436133, 0.3661740540185781, 0.38916256074443434, 0.38916256035294244, 0.38259441585376347, 0.3760262712567115, 0.3842364522721771, 0.3924466327982779, 0.3694581263660406, 0.38916256045081543, 0.3809523793374768, 0.3694581262681676, 0.38423645217430413, 0.39737274136840806, 0.37602627135458444, 0.3711001625887083, 0.38095237992471465, 0.3957307051457404, 0.3825944156580175, 0.3825944157558905, 0.379310343408428, 0.39408866892307265, 0.3924466329940238, 0.379310343506301, 0.38095237943534976, 0.3809523797289687, 0.38916256045081543, 0.3990147771995839, 0.3842364519785582, 0.3990147775910758, 0.3908045964777372, 0.37766830738150625, 0.3908045965756102, 0.3875205244238936, 0.3760262711588385, 0.39408866882519966, 0.3825944159516364, 0.3825944157558905, 0.3924466329940238, 0.3940886690209456, 0.385878487907607, 0.40229884964491935, 0.3908045965756102, 0.38587848800547997, 0.38916256035294244, 0.40065681361799754, 0.38095237953322275, 0.37766830738150625, 0.3825944155601445, 0.3990147771995839, 0.40886699414409833, 0.39737274156415403, 0.3743842349361708, 0.38916256054868836, 0.39408866882519966, 0.3973727412705351, 0.39737274107478915, 0.3842364520764312, 0.40229884954704637, 0.4006568134222516, 0.39244663250465894, 0.4022988498406653, 0.39737274107478915, 0.3990147775910758, 0.385878487809734, 0.40065681361799754, 0.403940886063333, 0.3825944155601445, 0.3743842347404248, 0.3973727412705351, 0.39408866892307265, 0.39408866892307265, 0.39408866892307265, 0.39737274117266214, 0.37274219871350306, 0.3776683071857603, 0.403940886063333, 0.3990147774932028, 0.3908045964777372, 0.3825944154622715, 0.4022988498406653, 0.40065681332437864, 0.38916256015719647, 0.38916256015719647, 0.38259441526652554, 0.39737274097691616, 0.3940886684337077, 0.3908045963798642, 0.38752052383665575, 0.4006568134222516, 0.3957307048521214, 0.40065681322650565, 0.39080459598837225, 0.39737274117266214, 0.3957307049499944, 0.40394088537822215, 0.40065681312863266, 0.4022988493513004, 0.39244663201529406, 0.4039408858675871, 0.4039408856718411, 0.37602627106096553, 0.3940886686294537, 0.40065681322650565, 0.3924466327004049, 0.4022988493513004, 0.3940886690209456, 0.40886699404622534, 0.3908045964777372, 0.3776683069900143, 0.38752052373878276, 0.39573070465637544, 0.3957307051457404, 0.39573070475424843, 0.3908045961841182, 0.4055829216008899, 0.3990147771017109, 0.4022988494491734, 0.3940886685315807, 0.3957307051457404, 0.41379310281210147, 0.3908045962819912, 0.39244663230891297, 0.40065681312863266], 'loss': [2.7060896675689508, 2.699025189754165, 2.69368796847929, 2.689086476684351, 2.6846929818452994, 2.6809548643335424, 2.677317015297359, 2.6738798693709795, 2.670367743689911, 2.666643570482853, 2.6619950821267504, 2.6564580946732352, 2.648372165965838, 2.6354221700398095, 2.6140552167285396, 2.580776279222304, 2.5377366607438856, 2.4967211828094733, 2.462842573130645, 2.428443903502008, 2.3947948830818, 2.3634613465969077, 2.3339967230262206, 2.30498565256718, 2.2801200222430533, 2.2550154389297203, 2.2346762088289984, 2.2157821703250895, 2.2004102316235614, 2.187088686240038, 2.1743063584000666, 2.1605424223004914, 2.146545644658302, 2.138111084736348, 2.1278875793519694, 2.1152118676986538, 2.1076343587291806, 2.09929714300794, 2.0875529126709735, 2.08123377392669, 2.0757480223810405, 2.0730511735841723, 2.064514144196403, 2.055984461429917, 2.0524379064415026, 2.0507687890798896, 2.037220170071972, 2.031523089683031, 2.0202255186359004, 2.013678445463553, 2.002336043743627, 1.9932301017537988, 1.9865185751317707, 1.9796949676419675, 1.977147979266345, 1.9685097612149907, 1.9602115494025072, 1.9621001565236085, 1.9645241580215078, 1.9520360087956736, 1.9439018021374024, 1.9428367097274968, 1.9311492111648623, 1.926569848530591, 1.9248153650295563, 1.918987270694004, 1.915537121310616, 1.9135675264824588, 1.9054977774864839, 1.9016911351950017, 1.8962730367570442, 1.8971064022679103, 1.8918498668337749, 1.8912071860546449, 1.8871934265081887, 1.87947912740022, 1.8723662612374559, 1.8783210716208394, 1.8706382330438194, 1.8721793470931005, 1.8661915580350026, 1.8634212694128927, 1.8622096952226863, 1.8519638836995778, 1.8512975485173093, 1.852381289959933, 1.845749248567303, 1.842730617816933, 1.8347527451583736, 1.8314292644328405, 1.8303400149825173, 1.8296177904708675, 1.8167161954746598, 1.8180364732624814, 1.8107636172668644, 1.8044047015403575, 1.8007795258469161, 1.7974666336968204, 1.791980125233378, 1.7880542778626114, 1.7837103718360101, 1.7926546170726205, 1.7878450829390382, 1.7871925198811525, 1.7744187937135325, 1.7719053118135895, 1.7604413956342537, 1.7550894568832993, 1.7578203782408635, 1.7575114767164666, 1.7482496896563615, 1.7425681523473846, 1.7407001930095822, 1.7378382900657106, 1.730274772888826, 1.7359819338796565, 1.72242307173398, 1.7241398499486873, 1.7173511475753, 1.7157628959698843, 1.7137568424369765, 1.7056653739000982, 1.7063954593219797, 1.702119478210042, 1.6963189209755931, 1.693739921307417, 1.7032696464957642, 1.688787328878712, 1.6863755162002125, 1.6830959155574228, 1.6766281552872864, 1.6779337890339094, 1.684450655061851, 1.6720547470958327, 1.67421538908134, 1.6707164131395627, 1.6644858417569734, 1.6656482197176015, 1.6607956239085422, 1.6608764907907412, 1.6668992001907537, 1.6596960985195466, 1.6582591476871247, 1.6471799255641335, 1.6507795679006243, 1.6420049996346664, 1.6439155302987696, 1.6387920397263043, 1.6356928169849716, 1.6380827494470491, 1.6404325946890108, 1.6283835021377344, 1.6360031808915814, 1.6330578022668985, 1.6329268054061357, 1.6438189374837542, 1.6301916748591272, 1.6347515327974511, 1.6288202186874297, 1.6212971798937914, 1.6314875981156587, 1.6363368156264695, 1.6293642676586488, 1.614808377152351, 1.6139523815080614, 1.6122921298416732, 1.6075452644966957, 1.6080855582529026, 1.6104826626591613, 1.6106909009710229, 1.618308707967676, 1.6003429229744162, 1.5925028046543348, 1.599851858248701, 1.6042119943630524, 1.6059528002748744, 1.6046471485122273, 1.6025735593674364, 1.5957739395282597, 1.584761584955564, 1.586332326992826, 1.5879662824117673, 1.60748045767602, 1.5949993793479715, 1.5929774994722872, 1.59154879106878, 1.592428684968968, 1.5895295326714642, 1.5959089913652174, 1.5885392098945759, 1.5820393978447884, 1.584920939723569, 1.5894389370383668, 1.5783442971642747, 1.5739324957927885, 1.572023322352149, 1.5674431926660715, 1.5664677331090218, 1.5688250044777652, 1.5708440724339574, 1.5719479853612441, 1.564041374645194, 1.5649771698691273, 1.562227183788464, 1.561982518985286, 1.55792755326696, 1.5590976165794983, 1.5566414291119428, 1.5593871634109309, 1.5656070289180999, 1.5617769538009925, 1.5599118269934058, 1.5444634032200495, 1.5481861971242228, 1.5470492119661836, 1.5511794390864442, 1.5566291772854157, 1.5614558104861689, 1.5502069155538352, 1.555449996836621, 1.5413896295324243, 1.5379473190288036, 1.5432453167756726, 1.5480869858661472, 1.5497241076502712, 1.5684240377414398, 1.5507295228617393, 1.5640712738526674, 1.5496000978246607, 1.534762850385427, 1.525532169900146, 1.529634990829217, 1.5289258514831199, 1.5288905351313722, 1.5235899793049148, 1.5259433691996078, 1.5237020960089118, 1.5239271878706602, 1.5222409147011915, 1.5185874318684884, 1.5214610379824158, 1.5252020856927797, 1.5225300172515963, 1.5255848130161513, 1.5221039040132716, 1.5365009045943587, 1.5260414540155713, 1.5247668395541776, 1.5318911016844137, 1.5231788292557797, 1.521258629812597, 1.519599827310143, 1.528475151904065, 1.521946164616814, 1.5206923161199206, 1.5217768915869616, 1.5150229226392398, 1.5108376469700244, 1.5214079809874235, 1.5077382780932793, 1.514392112608563, 1.5154374541688014, 1.504399223151393, 1.5129259215243298, 1.4986502167135782, 1.500932402189752, 1.4966967830422968, 1.4988436639676104, 1.499776806283046, 1.497990298515962, 1.489320853848232, 1.4890808405083062, 1.4880432662043483, 1.4901882789462988, 1.492826320503282, 1.4905584977637572, 1.4909180176576304, 1.4903155066883784, 1.4976982942107278, 1.4848954212004644, 1.481160266981967, 1.479533024343377, 1.4787465015720782, 1.4825804605621087, 1.4822485860613093, 1.475925551645565, 1.478626785239155, 1.4795205323358336, 1.4754388029815235, 1.4693441805653504, 1.4784344837161305, 1.4718816633341982, 1.4703728067311908, 1.4678973623369753, 1.467864895258596, 1.4675542656156317, 1.476345474910932, 1.4732782391796857, 1.465201059797706, 1.466854471935139], 'acc': [0.06652977429559832, 0.06652977448224531, 0.07679671413232658, 0.07843942538791124, 0.0965092403779774, 0.10225872644347576, 0.10225872722678116, 0.10225872742260751, 0.10225872703095482, 0.10225872743178686, 0.10266940485403034, 0.1145790563471753, 0.13388090379061884, 0.14455852130049307, 0.14907597428841757, 0.16632443561813426, 0.16632443481647013, 0.16303901367608525, 0.16591375721675905, 0.17125256703497202, 0.1815195062934006, 0.1704312117988324, 0.22464065695567786, 0.2308008208350724, 0.2422997945143212, 0.2546201220772839, 0.25708419091892437, 0.25544147966333974, 0.258316221619044, 0.25954825408160076, 0.26160164471769237, 0.26529774190953626, 0.2681724848076548, 0.2673511307831907, 0.27638603759742125, 0.28254620104844563, 0.2825462030434266, 0.2878850104750059, 0.29322381794330277, 0.2932238193140872, 0.298151950730925, 0.2965092388878613, 0.29445585099333854, 0.3018480495260971, 0.30225872847823393, 0.3137576992384462, 0.29856262854482113, 0.3158110882712096, 0.3051334692589801, 0.32279260801094023, 0.32648870696522125, 0.3334702248690799, 0.3322381949889831, 0.3322381949889831, 0.33552361377945183, 0.3396303921142398, 0.3359342935516115, 0.335112936161382, 0.3474332636876273, 0.3322381953806358, 0.340451743397135, 0.35154004045580445, 0.34496919958253663, 0.3556468185947661, 0.35277207569664754, 0.3462012332200514, 0.3507186872146458, 0.351540041080001, 0.34702258923949647, 0.35400410757172524, 0.355646815498262, 0.34168378000876254, 0.35154004123910987, 0.35605749468294257, 0.36221765813396695, 0.3511293616627766, 0.3609856274338474, 0.356878851485693, 0.359753595754596, 0.35236139471281236, 0.36344969235896085, 0.35770020413937265, 0.36016427023944425, 0.37166324628696795, 0.3708418874892366, 0.36837782260084057, 0.3720739225342533, 0.37043121147449504, 0.37782340762061994, 0.37535934426211726, 0.3753593430871591, 0.3700205358146887, 0.3724845979982333, 0.3827515402124158, 0.387679673000038, 0.3794661206386417, 0.38316221743883294, 0.38767967182507995, 0.3843942520554795, 0.38562628118898834, 0.3880903504222815, 0.3852156047458766, 0.38891170565842115, 0.3831622172430066, 0.38726899499031553, 0.390554412764935, 0.3917864485932571, 0.3958932227789744, 0.4000000015421325, 0.39219712343304064, 0.3880903476439952, 0.4110882942559048, 0.4045174515834824, 0.3946611893372859, 0.4016427094319518, 0.40492813315479664, 0.41396303954065705, 0.4102669403905497, 0.4102669388239389, 0.4049281310007068, 0.4057494864326728, 0.4119096526619835, 0.4139630407156151, 0.4131416815629485, 0.41930184857556463, 0.42135523490347654, 0.4114989728531064, 0.4131416823462539, 0.4201232046317271, 0.4217659129131991, 0.42053388224979693, 0.42669404589664767, 0.42874743535778115, 0.4299794654704217, 0.4184804917728142, 0.42792607581346187, 0.43080082011908233, 0.4303901425010125, 0.4271047223397594, 0.42505133581602106, 0.4275154001536555, 0.4193018468131275, 0.4291581110175875, 0.43655030638040704, 0.4328542099718685, 0.43655030931780225, 0.4369609839984768, 0.43655030892614954, 0.43449692161910586, 0.4340862430219043, 0.43367556364139737, 0.44804927945381806, 0.4320328535607708, 0.43655030775119147, 0.4340862412594672, 0.4316221765668975, 0.44312115003196123, 0.43613962837068454, 0.4398357283040973, 0.44229979280084064, 0.436139630328948, 0.4390143756504176, 0.44065708373606327, 0.4455852179311874, 0.4533880920503174, 0.4398357284632062, 0.45092402653772484, 0.44681724702797876, 0.4427104704556279, 0.4427104724138914, 0.4377823404095746, 0.45010266875584265, 0.4480492826237571, 0.4521560590002816, 0.4410677615499594, 0.4464065717598251, 0.44640656960573527, 0.4480492810571463, 0.4533880910344682, 0.45503080252259664, 0.458726897915286, 0.4492813125405713, 0.4472279252335276, 0.45297741184978757, 0.45174538138221176, 0.4517453792281219, 0.45051334754887057, 0.4562628322435845, 0.45585215697543086, 0.4562628356093499, 0.45831622346715517, 0.450513348332176, 0.4509240237594385, 0.4607802866298315, 0.46324435547147197, 0.4599589327644763, 0.45667351240739684, 0.46488706140302777, 0.45995893139369187, 0.4574948662360346, 0.4501026679725373, 0.4648870629696386, 0.45872690108522496, 0.45954825495058016, 0.4599589317486271, 0.4648870631654649, 0.46036960803262994, 0.46119096620616484, 0.4657084188299747, 0.4620123212464781, 0.46160164085012195, 0.4599589335477817, 0.4722792620898762, 0.471047228844014, 0.4694045170009503, 0.4689938375837259, 0.464476386954897, 0.46817248688830976, 0.4603696103458287, 0.4677618059044746, 0.4636550299196028, 0.4702258720045462, 0.464476387542376, 0.46735113047721205, 0.46776180531699557, 0.46078028643400515, 0.46201231866401815, 0.4661190972313499, 0.47515400302973126, 0.4755646830344347, 0.47392196919639007, 0.48131416632164675, 0.4726899361463543, 0.47967145839511, 0.4780287453770882, 0.4739219705671745, 0.4767967149095124, 0.4759753616316363, 0.4800821376165081, 0.4800821350340481, 0.4804928126521179, 0.476386038074748, 0.47433265096353067, 0.47515400639549665, 0.47515400557547377, 0.4743326493969199, 0.4751540044005157, 0.4809034885077506, 0.4747433283857741, 0.4772073911200803, 0.47761807073313106, 0.47556468362191373, 0.47597536143580993, 0.4788501012007069, 0.4747433254116615, 0.47186858286847816, 0.48254620254162156, 0.48583162521189976, 0.4767967166352321, 0.4813141676924312, 0.4813141669091258, 0.48131416675001687, 0.4887063661884723, 0.47145790763704193, 0.484599588245337, 0.4841889114105726, 0.4878850101690273, 0.48172484628963275, 0.480903492460995, 0.48870636383855615, 0.4878850113439854, 0.47967145565354113, 0.4866529808396921, 0.4845995872662053, 0.4878850086024165, 0.48706365767445653, 0.4899383984552027, 0.4887063636427298, 0.4821355235527673, 0.48870636442603516, 0.490349078031536, 0.48829568602465995, 0.4915811077525239, 0.4895277224404611, 0.48788500801493745, 0.4899384004501836, 0.4919917867413781, 0.4878850103648536, 0.49034907411500905, 0.4977412743734873, 0.4854209460272192, 0.4936344972136574, 0.4936344960386993, 0.49363449662617837, 0.4965092381902299, 0.4891170416524523, 0.4845995904361443, 0.4915811111182892, 0.49568788843722805, 0.4944558520581443]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
