{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf12.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 07:51:37 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '3', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001CE04284E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001CE7A316EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0846, Accuracy:0.3897, Validation Loss:1.0759, Validation Accuracy:0.3974\n",
    "Epoch #2: Loss:1.0753, Accuracy:0.3922, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #3: Loss:1.0743, Accuracy:0.3975, Validation Loss:1.0746, Validation Accuracy:0.4039\n",
    "Epoch #4: Loss:1.0747, Accuracy:0.3869, Validation Loss:1.0746, Validation Accuracy:0.3810\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0739, Accuracy:0.3967, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #7: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #8: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #9: Loss:1.0744, Accuracy:0.3922, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #10: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #11: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #12: Loss:1.0738, Accuracy:0.3967, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #13: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #14: Loss:1.0738, Accuracy:0.4016, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #15: Loss:1.0736, Accuracy:0.4041, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #16: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #17: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #18: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #19: Loss:1.0739, Accuracy:0.4021, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #21: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #22: Loss:1.0736, Accuracy:0.4021, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #23: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #24: Loss:1.0735, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #25: Loss:1.0733, Accuracy:0.4049, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #26: Loss:1.0734, Accuracy:0.4016, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #28: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #29: Loss:1.0733, Accuracy:0.3971, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #30: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #31: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #32: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #33: Loss:1.0731, Accuracy:0.4000, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #34: Loss:1.0728, Accuracy:0.4033, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #35: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0724, Accuracy:0.3955, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #37: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #38: Loss:1.0729, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #39: Loss:1.0720, Accuracy:0.4074, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #40: Loss:1.0723, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.4023\n",
    "Epoch #41: Loss:1.0724, Accuracy:0.4012, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #42: Loss:1.0738, Accuracy:0.4033, Validation Loss:1.0740, Validation Accuracy:0.3826\n",
    "Epoch #43: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #44: Loss:1.0722, Accuracy:0.3988, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #45: Loss:1.0724, Accuracy:0.4070, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #46: Loss:1.0726, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #47: Loss:1.0718, Accuracy:0.4000, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #48: Loss:1.0715, Accuracy:0.4136, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #49: Loss:1.0719, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #50: Loss:1.0712, Accuracy:0.4115, Validation Loss:1.0738, Validation Accuracy:0.3810\n",
    "Epoch #51: Loss:1.0712, Accuracy:0.4111, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #52: Loss:1.0707, Accuracy:0.4160, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #53: Loss:1.0707, Accuracy:0.4127, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #54: Loss:1.0710, Accuracy:0.4144, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #55: Loss:1.0698, Accuracy:0.4205, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #56: Loss:1.0708, Accuracy:0.4197, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #57: Loss:1.0699, Accuracy:0.4136, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #58: Loss:1.0686, Accuracy:0.4242, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #59: Loss:1.0691, Accuracy:0.4119, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #60: Loss:1.0678, Accuracy:0.4181, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0667, Accuracy:0.4345, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0658, Accuracy:0.4329, Validation Loss:1.0753, Validation Accuracy:0.3908\n",
    "Epoch #63: Loss:1.0644, Accuracy:0.4333, Validation Loss:1.0758, Validation Accuracy:0.3842\n",
    "Epoch #64: Loss:1.0681, Accuracy:0.4259, Validation Loss:1.0790, Validation Accuracy:0.3842\n",
    "Epoch #65: Loss:1.0636, Accuracy:0.4341, Validation Loss:1.0795, Validation Accuracy:0.3810\n",
    "Epoch #66: Loss:1.0617, Accuracy:0.4439, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #67: Loss:1.0584, Accuracy:0.4472, Validation Loss:1.0816, Validation Accuracy:0.3990\n",
    "Epoch #68: Loss:1.0555, Accuracy:0.4489, Validation Loss:1.0869, Validation Accuracy:0.3974\n",
    "Epoch #69: Loss:1.0536, Accuracy:0.4559, Validation Loss:1.0866, Validation Accuracy:0.3892\n",
    "Epoch #70: Loss:1.0526, Accuracy:0.4620, Validation Loss:1.1020, Validation Accuracy:0.3892\n",
    "Epoch #71: Loss:1.0566, Accuracy:0.4472, Validation Loss:1.0935, Validation Accuracy:0.3695\n",
    "Epoch #72: Loss:1.0468, Accuracy:0.4641, Validation Loss:1.1003, Validation Accuracy:0.3793\n",
    "Epoch #73: Loss:1.0487, Accuracy:0.4620, Validation Loss:1.1042, Validation Accuracy:0.3695\n",
    "Epoch #74: Loss:1.0525, Accuracy:0.4513, Validation Loss:1.0960, Validation Accuracy:0.3941\n",
    "Epoch #75: Loss:1.0496, Accuracy:0.4583, Validation Loss:1.0955, Validation Accuracy:0.3908\n",
    "Epoch #76: Loss:1.0469, Accuracy:0.4595, Validation Loss:1.0987, Validation Accuracy:0.3957\n",
    "Epoch #77: Loss:1.0457, Accuracy:0.4661, Validation Loss:1.0983, Validation Accuracy:0.3842\n",
    "Epoch #78: Loss:1.0434, Accuracy:0.4645, Validation Loss:1.0994, Validation Accuracy:0.3810\n",
    "Epoch #79: Loss:1.0436, Accuracy:0.4678, Validation Loss:1.1122, Validation Accuracy:0.3760\n",
    "Epoch #80: Loss:1.0470, Accuracy:0.4641, Validation Loss:1.1064, Validation Accuracy:0.3875\n",
    "Epoch #81: Loss:1.0426, Accuracy:0.4657, Validation Loss:1.1050, Validation Accuracy:0.3941\n",
    "Epoch #82: Loss:1.0386, Accuracy:0.4793, Validation Loss:1.1067, Validation Accuracy:0.3711\n",
    "Epoch #83: Loss:1.0351, Accuracy:0.4797, Validation Loss:1.0973, Validation Accuracy:0.3875\n",
    "Epoch #84: Loss:1.0337, Accuracy:0.4780, Validation Loss:1.1136, Validation Accuracy:0.3695\n",
    "Epoch #85: Loss:1.0326, Accuracy:0.4825, Validation Loss:1.1144, Validation Accuracy:0.3727\n",
    "Epoch #86: Loss:1.0297, Accuracy:0.4727, Validation Loss:1.1187, Validation Accuracy:0.3662\n",
    "Epoch #87: Loss:1.0303, Accuracy:0.4797, Validation Loss:1.1184, Validation Accuracy:0.3777\n",
    "Epoch #88: Loss:1.0294, Accuracy:0.4789, Validation Loss:1.1201, Validation Accuracy:0.3793\n",
    "Epoch #89: Loss:1.0283, Accuracy:0.4764, Validation Loss:1.1224, Validation Accuracy:0.3760\n",
    "Epoch #90: Loss:1.0311, Accuracy:0.4850, Validation Loss:1.1272, Validation Accuracy:0.3810\n",
    "Epoch #91: Loss:1.0274, Accuracy:0.4887, Validation Loss:1.1108, Validation Accuracy:0.3810\n",
    "Epoch #92: Loss:1.0274, Accuracy:0.4850, Validation Loss:1.1228, Validation Accuracy:0.3826\n",
    "Epoch #93: Loss:1.0227, Accuracy:0.4928, Validation Loss:1.1200, Validation Accuracy:0.3941\n",
    "Epoch #94: Loss:1.0219, Accuracy:0.4871, Validation Loss:1.1236, Validation Accuracy:0.3793\n",
    "Epoch #95: Loss:1.0204, Accuracy:0.4936, Validation Loss:1.1342, Validation Accuracy:0.3727\n",
    "Epoch #96: Loss:1.0302, Accuracy:0.4743, Validation Loss:1.1142, Validation Accuracy:0.3875\n",
    "Epoch #97: Loss:1.0205, Accuracy:0.4969, Validation Loss:1.1396, Validation Accuracy:0.3645\n",
    "Epoch #98: Loss:1.0221, Accuracy:0.4846, Validation Loss:1.1268, Validation Accuracy:0.3810\n",
    "Epoch #99: Loss:1.0209, Accuracy:0.4949, Validation Loss:1.1194, Validation Accuracy:0.3810\n",
    "Epoch #100: Loss:1.0222, Accuracy:0.4899, Validation Loss:1.1327, Validation Accuracy:0.3662\n",
    "Epoch #101: Loss:1.0221, Accuracy:0.4899, Validation Loss:1.1258, Validation Accuracy:0.3744\n",
    "Epoch #102: Loss:1.0146, Accuracy:0.4961, Validation Loss:1.1352, Validation Accuracy:0.3793\n",
    "Epoch #103: Loss:1.0175, Accuracy:0.4982, Validation Loss:1.1377, Validation Accuracy:0.3859\n",
    "Epoch #104: Loss:1.0133, Accuracy:0.4920, Validation Loss:1.1414, Validation Accuracy:0.3695\n",
    "Epoch #105: Loss:1.0117, Accuracy:0.5035, Validation Loss:1.1420, Validation Accuracy:0.3760\n",
    "Epoch #106: Loss:1.0121, Accuracy:0.4965, Validation Loss:1.1437, Validation Accuracy:0.3744\n",
    "Epoch #107: Loss:1.0158, Accuracy:0.4903, Validation Loss:1.1466, Validation Accuracy:0.3727\n",
    "Epoch #108: Loss:1.0156, Accuracy:0.4973, Validation Loss:1.1498, Validation Accuracy:0.3826\n",
    "Epoch #109: Loss:1.0288, Accuracy:0.4842, Validation Loss:1.1481, Validation Accuracy:0.3810\n",
    "Epoch #110: Loss:1.0173, Accuracy:0.4940, Validation Loss:1.1343, Validation Accuracy:0.3530\n",
    "Epoch #111: Loss:1.0123, Accuracy:0.5031, Validation Loss:1.1297, Validation Accuracy:0.3842\n",
    "Epoch #112: Loss:1.0053, Accuracy:0.5047, Validation Loss:1.1460, Validation Accuracy:0.3695\n",
    "Epoch #113: Loss:1.0094, Accuracy:0.5027, Validation Loss:1.1435, Validation Accuracy:0.3711\n",
    "Epoch #114: Loss:1.0087, Accuracy:0.4994, Validation Loss:1.1516, Validation Accuracy:0.3892\n",
    "Epoch #115: Loss:1.0041, Accuracy:0.5039, Validation Loss:1.1542, Validation Accuracy:0.3711\n",
    "Epoch #116: Loss:1.0058, Accuracy:0.5047, Validation Loss:1.1472, Validation Accuracy:0.3842\n",
    "Epoch #117: Loss:1.0094, Accuracy:0.5014, Validation Loss:1.1617, Validation Accuracy:0.3629\n",
    "Epoch #118: Loss:1.0135, Accuracy:0.4990, Validation Loss:1.1530, Validation Accuracy:0.3678\n",
    "Epoch #119: Loss:1.0073, Accuracy:0.5018, Validation Loss:1.1412, Validation Accuracy:0.3695\n",
    "Epoch #120: Loss:1.0011, Accuracy:0.5121, Validation Loss:1.1602, Validation Accuracy:0.3580\n",
    "Epoch #121: Loss:1.0001, Accuracy:0.5109, Validation Loss:1.1421, Validation Accuracy:0.3826\n",
    "Epoch #122: Loss:1.0001, Accuracy:0.5105, Validation Loss:1.1574, Validation Accuracy:0.3662\n",
    "Epoch #123: Loss:1.0060, Accuracy:0.5051, Validation Loss:1.1743, Validation Accuracy:0.3695\n",
    "Epoch #124: Loss:1.0026, Accuracy:0.5002, Validation Loss:1.1433, Validation Accuracy:0.3612\n",
    "Epoch #125: Loss:1.0002, Accuracy:0.5179, Validation Loss:1.1807, Validation Accuracy:0.3810\n",
    "Epoch #126: Loss:1.0023, Accuracy:0.5031, Validation Loss:1.1600, Validation Accuracy:0.3432\n",
    "Epoch #127: Loss:0.9948, Accuracy:0.5133, Validation Loss:1.1642, Validation Accuracy:0.3695\n",
    "Epoch #128: Loss:0.9997, Accuracy:0.5133, Validation Loss:1.1541, Validation Accuracy:0.3432\n",
    "Epoch #129: Loss:0.9952, Accuracy:0.5055, Validation Loss:1.1680, Validation Accuracy:0.3596\n",
    "Epoch #130: Loss:0.9924, Accuracy:0.5121, Validation Loss:1.1631, Validation Accuracy:0.3777\n",
    "Epoch #131: Loss:0.9898, Accuracy:0.5187, Validation Loss:1.1635, Validation Accuracy:0.3777\n",
    "Epoch #132: Loss:0.9888, Accuracy:0.5175, Validation Loss:1.1731, Validation Accuracy:0.3744\n",
    "Epoch #133: Loss:0.9884, Accuracy:0.5203, Validation Loss:1.1840, Validation Accuracy:0.3760\n",
    "Epoch #134: Loss:0.9942, Accuracy:0.5195, Validation Loss:1.1626, Validation Accuracy:0.3760\n",
    "Epoch #135: Loss:0.9838, Accuracy:0.5240, Validation Loss:1.1937, Validation Accuracy:0.3777\n",
    "Epoch #136: Loss:0.9878, Accuracy:0.5265, Validation Loss:1.1622, Validation Accuracy:0.3530\n",
    "Epoch #137: Loss:0.9891, Accuracy:0.5113, Validation Loss:1.1690, Validation Accuracy:0.3727\n",
    "Epoch #138: Loss:0.9828, Accuracy:0.5257, Validation Loss:1.1868, Validation Accuracy:0.3826\n",
    "Epoch #139: Loss:0.9849, Accuracy:0.5199, Validation Loss:1.1604, Validation Accuracy:0.3662\n",
    "Epoch #140: Loss:0.9864, Accuracy:0.5220, Validation Loss:1.1807, Validation Accuracy:0.3777\n",
    "Epoch #141: Loss:0.9821, Accuracy:0.5294, Validation Loss:1.1946, Validation Accuracy:0.3678\n",
    "Epoch #142: Loss:0.9860, Accuracy:0.5285, Validation Loss:1.1741, Validation Accuracy:0.3481\n",
    "Epoch #143: Loss:0.9874, Accuracy:0.5207, Validation Loss:1.1646, Validation Accuracy:0.3842\n",
    "Epoch #144: Loss:0.9823, Accuracy:0.5253, Validation Loss:1.1884, Validation Accuracy:0.3530\n",
    "Epoch #145: Loss:0.9880, Accuracy:0.5211, Validation Loss:1.1805, Validation Accuracy:0.3596\n",
    "Epoch #146: Loss:0.9784, Accuracy:0.5331, Validation Loss:1.1681, Validation Accuracy:0.3695\n",
    "Epoch #147: Loss:0.9768, Accuracy:0.5326, Validation Loss:1.2039, Validation Accuracy:0.3842\n",
    "Epoch #148: Loss:0.9751, Accuracy:0.5372, Validation Loss:1.1625, Validation Accuracy:0.3563\n",
    "Epoch #149: Loss:0.9792, Accuracy:0.5310, Validation Loss:1.2101, Validation Accuracy:0.3612\n",
    "Epoch #150: Loss:0.9777, Accuracy:0.5290, Validation Loss:1.2028, Validation Accuracy:0.3760\n",
    "Epoch #151: Loss:0.9731, Accuracy:0.5425, Validation Loss:1.1743, Validation Accuracy:0.3695\n",
    "Epoch #152: Loss:0.9715, Accuracy:0.5359, Validation Loss:1.2081, Validation Accuracy:0.3612\n",
    "Epoch #153: Loss:0.9705, Accuracy:0.5380, Validation Loss:1.1977, Validation Accuracy:0.3892\n",
    "Epoch #154: Loss:0.9726, Accuracy:0.5388, Validation Loss:1.1853, Validation Accuracy:0.3842\n",
    "Epoch #155: Loss:0.9780, Accuracy:0.5322, Validation Loss:1.1881, Validation Accuracy:0.3777\n",
    "Epoch #156: Loss:0.9729, Accuracy:0.5355, Validation Loss:1.2012, Validation Accuracy:0.3563\n",
    "Epoch #157: Loss:0.9694, Accuracy:0.5322, Validation Loss:1.1912, Validation Accuracy:0.3826\n",
    "Epoch #158: Loss:0.9698, Accuracy:0.5417, Validation Loss:1.1966, Validation Accuracy:0.3300\n",
    "Epoch #159: Loss:0.9682, Accuracy:0.5372, Validation Loss:1.1921, Validation Accuracy:0.3826\n",
    "Epoch #160: Loss:0.9664, Accuracy:0.5429, Validation Loss:1.2076, Validation Accuracy:0.3859\n",
    "Epoch #161: Loss:0.9653, Accuracy:0.5380, Validation Loss:1.1827, Validation Accuracy:0.3892\n",
    "Epoch #162: Loss:0.9699, Accuracy:0.5450, Validation Loss:1.2331, Validation Accuracy:0.3448\n",
    "Epoch #163: Loss:0.9742, Accuracy:0.5322, Validation Loss:1.1905, Validation Accuracy:0.3826\n",
    "Epoch #164: Loss:0.9642, Accuracy:0.5433, Validation Loss:1.1971, Validation Accuracy:0.3645\n",
    "Epoch #165: Loss:0.9643, Accuracy:0.5441, Validation Loss:1.1990, Validation Accuracy:0.3678\n",
    "Epoch #166: Loss:0.9626, Accuracy:0.5359, Validation Loss:1.1997, Validation Accuracy:0.3875\n",
    "Epoch #167: Loss:0.9723, Accuracy:0.5363, Validation Loss:1.2283, Validation Accuracy:0.3366\n",
    "Epoch #168: Loss:0.9772, Accuracy:0.5240, Validation Loss:1.2102, Validation Accuracy:0.3842\n",
    "Epoch #169: Loss:0.9695, Accuracy:0.5388, Validation Loss:1.2131, Validation Accuracy:0.3645\n",
    "Epoch #170: Loss:0.9639, Accuracy:0.5450, Validation Loss:1.1702, Validation Accuracy:0.3580\n",
    "Epoch #171: Loss:0.9634, Accuracy:0.5392, Validation Loss:1.1803, Validation Accuracy:0.3760\n",
    "Epoch #172: Loss:0.9577, Accuracy:0.5413, Validation Loss:1.2397, Validation Accuracy:0.3580\n",
    "Epoch #173: Loss:0.9665, Accuracy:0.5310, Validation Loss:1.1867, Validation Accuracy:0.3333\n",
    "Epoch #174: Loss:0.9601, Accuracy:0.5425, Validation Loss:1.2286, Validation Accuracy:0.3892\n",
    "Epoch #175: Loss:0.9594, Accuracy:0.5454, Validation Loss:1.2175, Validation Accuracy:0.3383\n",
    "Epoch #176: Loss:0.9574, Accuracy:0.5511, Validation Loss:1.2379, Validation Accuracy:0.3744\n",
    "Epoch #177: Loss:0.9666, Accuracy:0.5417, Validation Loss:1.2282, Validation Accuracy:0.3875\n",
    "Epoch #178: Loss:0.9600, Accuracy:0.5417, Validation Loss:1.1635, Validation Accuracy:0.3415\n",
    "Epoch #179: Loss:0.9619, Accuracy:0.5441, Validation Loss:1.2449, Validation Accuracy:0.3957\n",
    "Epoch #180: Loss:0.9604, Accuracy:0.5507, Validation Loss:1.2278, Validation Accuracy:0.3842\n",
    "Epoch #181: Loss:0.9545, Accuracy:0.5524, Validation Loss:1.1952, Validation Accuracy:0.3793\n",
    "Epoch #182: Loss:0.9557, Accuracy:0.5507, Validation Loss:1.2036, Validation Accuracy:0.3810\n",
    "Epoch #183: Loss:0.9536, Accuracy:0.5532, Validation Loss:1.2121, Validation Accuracy:0.3465\n",
    "Epoch #184: Loss:0.9560, Accuracy:0.5433, Validation Loss:1.2153, Validation Accuracy:0.3645\n",
    "Epoch #185: Loss:0.9528, Accuracy:0.5462, Validation Loss:1.2202, Validation Accuracy:0.3481\n",
    "Epoch #186: Loss:0.9605, Accuracy:0.5409, Validation Loss:1.2185, Validation Accuracy:0.3596\n",
    "Epoch #187: Loss:0.9568, Accuracy:0.5458, Validation Loss:1.2218, Validation Accuracy:0.3711\n",
    "Epoch #188: Loss:0.9576, Accuracy:0.5421, Validation Loss:1.2163, Validation Accuracy:0.3333\n",
    "Epoch #189: Loss:0.9601, Accuracy:0.5318, Validation Loss:1.2160, Validation Accuracy:0.3826\n",
    "Epoch #190: Loss:0.9468, Accuracy:0.5466, Validation Loss:1.2367, Validation Accuracy:0.3481\n",
    "Epoch #191: Loss:0.9506, Accuracy:0.5409, Validation Loss:1.2118, Validation Accuracy:0.3777\n",
    "Epoch #192: Loss:0.9541, Accuracy:0.5384, Validation Loss:1.2165, Validation Accuracy:0.3580\n",
    "Epoch #193: Loss:0.9475, Accuracy:0.5495, Validation Loss:1.2440, Validation Accuracy:0.3678\n",
    "Epoch #194: Loss:0.9428, Accuracy:0.5503, Validation Loss:1.2182, Validation Accuracy:0.3842\n",
    "Epoch #195: Loss:0.9433, Accuracy:0.5474, Validation Loss:1.2414, Validation Accuracy:0.3432\n",
    "Epoch #196: Loss:0.9425, Accuracy:0.5532, Validation Loss:1.2680, Validation Accuracy:0.3810\n",
    "Epoch #197: Loss:0.9465, Accuracy:0.5548, Validation Loss:1.2312, Validation Accuracy:0.3744\n",
    "Epoch #198: Loss:0.9432, Accuracy:0.5524, Validation Loss:1.2424, Validation Accuracy:0.3629\n",
    "Epoch #199: Loss:0.9406, Accuracy:0.5577, Validation Loss:1.2305, Validation Accuracy:0.3547\n",
    "Epoch #200: Loss:0.9435, Accuracy:0.5622, Validation Loss:1.2361, Validation Accuracy:0.3120\n",
    "Epoch #201: Loss:0.9525, Accuracy:0.5433, Validation Loss:1.2282, Validation Accuracy:0.3793\n",
    "Epoch #202: Loss:0.9419, Accuracy:0.5536, Validation Loss:1.2261, Validation Accuracy:0.3563\n",
    "Epoch #203: Loss:0.9355, Accuracy:0.5606, Validation Loss:1.2242, Validation Accuracy:0.3514\n",
    "Epoch #204: Loss:0.9431, Accuracy:0.5585, Validation Loss:1.2738, Validation Accuracy:0.3760\n",
    "Epoch #205: Loss:0.9363, Accuracy:0.5598, Validation Loss:1.2587, Validation Accuracy:0.3793\n",
    "Epoch #206: Loss:0.9335, Accuracy:0.5626, Validation Loss:1.2221, Validation Accuracy:0.3530\n",
    "Epoch #207: Loss:0.9339, Accuracy:0.5585, Validation Loss:1.2557, Validation Accuracy:0.3662\n",
    "Epoch #208: Loss:0.9354, Accuracy:0.5536, Validation Loss:1.2311, Validation Accuracy:0.3760\n",
    "Epoch #209: Loss:0.9367, Accuracy:0.5602, Validation Loss:1.2484, Validation Accuracy:0.3514\n",
    "Epoch #210: Loss:0.9325, Accuracy:0.5655, Validation Loss:1.3187, Validation Accuracy:0.3596\n",
    "Epoch #211: Loss:0.9439, Accuracy:0.5446, Validation Loss:1.2230, Validation Accuracy:0.3744\n",
    "Epoch #212: Loss:0.9352, Accuracy:0.5524, Validation Loss:1.2501, Validation Accuracy:0.3563\n",
    "Epoch #213: Loss:0.9389, Accuracy:0.5507, Validation Loss:1.2295, Validation Accuracy:0.3727\n",
    "Epoch #214: Loss:0.9406, Accuracy:0.5540, Validation Loss:1.2729, Validation Accuracy:0.3333\n",
    "Epoch #215: Loss:0.9380, Accuracy:0.5610, Validation Loss:1.2563, Validation Accuracy:0.3777\n",
    "Epoch #216: Loss:0.9430, Accuracy:0.5446, Validation Loss:1.3059, Validation Accuracy:0.3974\n",
    "Epoch #217: Loss:0.9445, Accuracy:0.5540, Validation Loss:1.1675, Validation Accuracy:0.3481\n",
    "Epoch #218: Loss:0.9409, Accuracy:0.5593, Validation Loss:1.2915, Validation Accuracy:0.3777\n",
    "Epoch #219: Loss:0.9366, Accuracy:0.5569, Validation Loss:1.2053, Validation Accuracy:0.3727\n",
    "Epoch #220: Loss:0.9369, Accuracy:0.5561, Validation Loss:1.2867, Validation Accuracy:0.3612\n",
    "Epoch #221: Loss:0.9362, Accuracy:0.5581, Validation Loss:1.2721, Validation Accuracy:0.3744\n",
    "Epoch #222: Loss:0.9402, Accuracy:0.5593, Validation Loss:1.2177, Validation Accuracy:0.3251\n",
    "Epoch #223: Loss:0.9279, Accuracy:0.5634, Validation Loss:1.2738, Validation Accuracy:0.3810\n",
    "Epoch #224: Loss:0.9237, Accuracy:0.5602, Validation Loss:1.2354, Validation Accuracy:0.3366\n",
    "Epoch #225: Loss:0.9287, Accuracy:0.5593, Validation Loss:1.2437, Validation Accuracy:0.3432\n",
    "Epoch #226: Loss:0.9264, Accuracy:0.5630, Validation Loss:1.2633, Validation Accuracy:0.3744\n",
    "Epoch #227: Loss:0.9226, Accuracy:0.5754, Validation Loss:1.3149, Validation Accuracy:0.3563\n",
    "Epoch #228: Loss:0.9304, Accuracy:0.5630, Validation Loss:1.2087, Validation Accuracy:0.3465\n",
    "Epoch #229: Loss:0.9270, Accuracy:0.5634, Validation Loss:1.2957, Validation Accuracy:0.3448\n",
    "Epoch #230: Loss:0.9322, Accuracy:0.5585, Validation Loss:1.2624, Validation Accuracy:0.3612\n",
    "Epoch #231: Loss:0.9266, Accuracy:0.5602, Validation Loss:1.2238, Validation Accuracy:0.3612\n",
    "Epoch #232: Loss:0.9247, Accuracy:0.5643, Validation Loss:1.3120, Validation Accuracy:0.3662\n",
    "Epoch #233: Loss:0.9350, Accuracy:0.5556, Validation Loss:1.2291, Validation Accuracy:0.3629\n",
    "Epoch #234: Loss:0.9237, Accuracy:0.5643, Validation Loss:1.2396, Validation Accuracy:0.3530\n",
    "Epoch #235: Loss:0.9149, Accuracy:0.5749, Validation Loss:1.2982, Validation Accuracy:0.3810\n",
    "Epoch #236: Loss:0.9203, Accuracy:0.5708, Validation Loss:1.2387, Validation Accuracy:0.3498\n",
    "Epoch #237: Loss:0.9216, Accuracy:0.5606, Validation Loss:1.2667, Validation Accuracy:0.3547\n",
    "Epoch #238: Loss:0.9216, Accuracy:0.5622, Validation Loss:1.3243, Validation Accuracy:0.3892\n",
    "Epoch #239: Loss:0.9240, Accuracy:0.5696, Validation Loss:1.2733, Validation Accuracy:0.3514\n",
    "Epoch #240: Loss:0.9178, Accuracy:0.5671, Validation Loss:1.2688, Validation Accuracy:0.3612\n",
    "Epoch #241: Loss:0.9125, Accuracy:0.5737, Validation Loss:1.2853, Validation Accuracy:0.3383\n",
    "Epoch #242: Loss:0.9139, Accuracy:0.5778, Validation Loss:1.2437, Validation Accuracy:0.3498\n",
    "Epoch #243: Loss:0.9176, Accuracy:0.5762, Validation Loss:1.3261, Validation Accuracy:0.3760\n",
    "Epoch #244: Loss:0.9148, Accuracy:0.5634, Validation Loss:1.2808, Validation Accuracy:0.3629\n",
    "Epoch #245: Loss:0.9130, Accuracy:0.5717, Validation Loss:1.2598, Validation Accuracy:0.3350\n",
    "Epoch #246: Loss:0.9108, Accuracy:0.5758, Validation Loss:1.2736, Validation Accuracy:0.3432\n",
    "Epoch #247: Loss:0.9045, Accuracy:0.5766, Validation Loss:1.3196, Validation Accuracy:0.3612\n",
    "Epoch #248: Loss:0.9106, Accuracy:0.5737, Validation Loss:1.2879, Validation Accuracy:0.3530\n",
    "Epoch #249: Loss:0.9170, Accuracy:0.5651, Validation Loss:1.2199, Validation Accuracy:0.3350\n",
    "Epoch #250: Loss:0.9173, Accuracy:0.5749, Validation Loss:1.3303, Validation Accuracy:0.3744\n",
    "Epoch #251: Loss:0.9162, Accuracy:0.5717, Validation Loss:1.2663, Validation Accuracy:0.3695\n",
    "Epoch #252: Loss:0.9077, Accuracy:0.5758, Validation Loss:1.2610, Validation Accuracy:0.3186\n",
    "Epoch #253: Loss:0.9078, Accuracy:0.5729, Validation Loss:1.2671, Validation Accuracy:0.3662\n",
    "Epoch #254: Loss:0.9065, Accuracy:0.5770, Validation Loss:1.3181, Validation Accuracy:0.3530\n",
    "Epoch #255: Loss:0.9102, Accuracy:0.5762, Validation Loss:1.2333, Validation Accuracy:0.3366\n",
    "Epoch #256: Loss:0.9044, Accuracy:0.5823, Validation Loss:1.3195, Validation Accuracy:0.3612\n",
    "Epoch #257: Loss:0.9016, Accuracy:0.5782, Validation Loss:1.2855, Validation Accuracy:0.3530\n",
    "Epoch #258: Loss:0.8982, Accuracy:0.5799, Validation Loss:1.2675, Validation Accuracy:0.3530\n",
    "Epoch #259: Loss:0.8983, Accuracy:0.5823, Validation Loss:1.3132, Validation Accuracy:0.3563\n",
    "Epoch #260: Loss:0.9078, Accuracy:0.5786, Validation Loss:1.2562, Validation Accuracy:0.3383\n",
    "Epoch #261: Loss:0.9020, Accuracy:0.5754, Validation Loss:1.3100, Validation Accuracy:0.3678\n",
    "Epoch #262: Loss:0.9072, Accuracy:0.5721, Validation Loss:1.2872, Validation Accuracy:0.3580\n",
    "Epoch #263: Loss:0.9026, Accuracy:0.5885, Validation Loss:1.3180, Validation Accuracy:0.3744\n",
    "Epoch #264: Loss:0.9035, Accuracy:0.5754, Validation Loss:1.2920, Validation Accuracy:0.3071\n",
    "Epoch #265: Loss:0.9174, Accuracy:0.5639, Validation Loss:1.2647, Validation Accuracy:0.3760\n",
    "Epoch #266: Loss:0.9050, Accuracy:0.5795, Validation Loss:1.3234, Validation Accuracy:0.3350\n",
    "Epoch #267: Loss:0.8986, Accuracy:0.5729, Validation Loss:1.2932, Validation Accuracy:0.3678\n",
    "Epoch #268: Loss:0.8959, Accuracy:0.5844, Validation Loss:1.2882, Validation Accuracy:0.3415\n",
    "Epoch #269: Loss:0.9061, Accuracy:0.5815, Validation Loss:1.2841, Validation Accuracy:0.3727\n",
    "Epoch #270: Loss:0.9028, Accuracy:0.5799, Validation Loss:1.3282, Validation Accuracy:0.3218\n",
    "Epoch #271: Loss:0.8973, Accuracy:0.5860, Validation Loss:1.2245, Validation Accuracy:0.3415\n",
    "Epoch #272: Loss:0.8970, Accuracy:0.5791, Validation Loss:1.3443, Validation Accuracy:0.3481\n",
    "Epoch #273: Loss:0.8997, Accuracy:0.5807, Validation Loss:1.3023, Validation Accuracy:0.3711\n",
    "Epoch #274: Loss:0.8975, Accuracy:0.5770, Validation Loss:1.2768, Validation Accuracy:0.3317\n",
    "Epoch #275: Loss:0.9056, Accuracy:0.5803, Validation Loss:1.2929, Validation Accuracy:0.3645\n",
    "Epoch #276: Loss:0.9004, Accuracy:0.5774, Validation Loss:1.3265, Validation Accuracy:0.3645\n",
    "Epoch #277: Loss:0.8990, Accuracy:0.5741, Validation Loss:1.2966, Validation Accuracy:0.3481\n",
    "Epoch #278: Loss:0.8995, Accuracy:0.5795, Validation Loss:1.2974, Validation Accuracy:0.3465\n",
    "Epoch #279: Loss:0.9096, Accuracy:0.5676, Validation Loss:1.2909, Validation Accuracy:0.3645\n",
    "Epoch #280: Loss:0.8974, Accuracy:0.5795, Validation Loss:1.3058, Validation Accuracy:0.3120\n",
    "Epoch #281: Loss:0.9070, Accuracy:0.5680, Validation Loss:1.2477, Validation Accuracy:0.3399\n",
    "Epoch #282: Loss:0.8956, Accuracy:0.5873, Validation Loss:1.3237, Validation Accuracy:0.3498\n",
    "Epoch #283: Loss:0.8861, Accuracy:0.5889, Validation Loss:1.2821, Validation Accuracy:0.3498\n",
    "Epoch #284: Loss:0.8867, Accuracy:0.5893, Validation Loss:1.3106, Validation Accuracy:0.3432\n",
    "Epoch #285: Loss:0.8907, Accuracy:0.5832, Validation Loss:1.3235, Validation Accuracy:0.3777\n",
    "Epoch #286: Loss:0.8885, Accuracy:0.5889, Validation Loss:1.3283, Validation Accuracy:0.3563\n",
    "Epoch #287: Loss:0.8980, Accuracy:0.5795, Validation Loss:1.2619, Validation Accuracy:0.3251\n",
    "Epoch #288: Loss:0.8928, Accuracy:0.5922, Validation Loss:1.3385, Validation Accuracy:0.3448\n",
    "Epoch #289: Loss:0.8832, Accuracy:0.5869, Validation Loss:1.3168, Validation Accuracy:0.3727\n",
    "Epoch #290: Loss:0.8910, Accuracy:0.5860, Validation Loss:1.3117, Validation Accuracy:0.3498\n",
    "Epoch #291: Loss:0.8803, Accuracy:0.5930, Validation Loss:1.3230, Validation Accuracy:0.3465\n",
    "Epoch #292: Loss:0.8810, Accuracy:0.5901, Validation Loss:1.3033, Validation Accuracy:0.3563\n",
    "Epoch #293: Loss:0.8770, Accuracy:0.5979, Validation Loss:1.3456, Validation Accuracy:0.3498\n",
    "Epoch #294: Loss:0.8809, Accuracy:0.5951, Validation Loss:1.2579, Validation Accuracy:0.3432\n",
    "Epoch #295: Loss:0.8859, Accuracy:0.5914, Validation Loss:1.3284, Validation Accuracy:0.3268\n",
    "Epoch #296: Loss:0.8863, Accuracy:0.5926, Validation Loss:1.3210, Validation Accuracy:0.3629\n",
    "Epoch #297: Loss:0.8815, Accuracy:0.5860, Validation Loss:1.3551, Validation Accuracy:0.3924\n",
    "Epoch #298: Loss:0.8783, Accuracy:0.5938, Validation Loss:1.2820, Validation Accuracy:0.3432\n",
    "Epoch #299: Loss:0.8770, Accuracy:0.5893, Validation Loss:1.3281, Validation Accuracy:0.3432\n",
    "Epoch #300: Loss:0.8659, Accuracy:0.6012, Validation Loss:1.3138, Validation Accuracy:0.3645\n",
    "\n",
    "Test:\n",
    "Test Loss:1.31381893, Accuracy:0.3645\n",
    "Labels: ['03', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03   02  01\n",
    "t:03  26   54  62\n",
    "t:02  30  102  95\n",
    "t:01  39  107  94\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.27      0.18      0.22       142\n",
    "          02       0.39      0.45      0.42       227\n",
    "          01       0.37      0.39      0.38       240\n",
    "\n",
    "    accuracy                           0.36       609\n",
    "   macro avg       0.35      0.34      0.34       609\n",
    "weighted avg       0.36      0.36      0.36       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 08:07:11 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 33 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.07589532021427, 1.0744076646216005, 1.0745676751990232, 1.0745550300100166, 1.0740809105887201, 1.0740809172440828, 1.074090296803241, 1.0740275541549833, 1.0741969343085203, 1.0742156213923237, 1.0740762510518918, 1.0740124272986977, 1.0741137959099756, 1.0740432520022338, 1.073986018232524, 1.0740117069535655, 1.074011724570702, 1.0740863949990234, 1.0739757715187637, 1.073960284685658, 1.0740490319889362, 1.0739583769455332, 1.0741909914611791, 1.0739303159792044, 1.0740407650498138, 1.073905890406842, 1.0738944304596223, 1.0738934348956706, 1.0738982698208788, 1.0738716096126388, 1.0739156970836845, 1.073901394122144, 1.0738825099221592, 1.0738231699259215, 1.073945525048793, 1.0737896192641485, 1.0738612113915054, 1.0738745961087481, 1.0737085704537253, 1.0737336907284991, 1.0737255155942318, 1.0740074731641998, 1.0746421093619711, 1.073634284078977, 1.0742454233232195, 1.0743099480426956, 1.0735671582871862, 1.0736168593608688, 1.0735549425648154, 1.0737648010253906, 1.0736786111430778, 1.0736125635200338, 1.073610646188357, 1.0738885629744757, 1.0739007559903149, 1.0738432448485802, 1.0737612697682748, 1.074374384480744, 1.0740182153109847, 1.074423382826431, 1.0746669839755656, 1.075342693939585, 1.0757993215019088, 1.079048124440198, 1.0794654589372707, 1.0770564693926983, 1.0815965765215494, 1.086932357504646, 1.0866063234254058, 1.102034978482915, 1.093489882002519, 1.1002566372036737, 1.1041734872388917, 1.0960404044888876, 1.0955188174553105, 1.098704168753475, 1.098301751468765, 1.099444986955677, 1.112183787552594, 1.1063931730188956, 1.105022365823755, 1.1067085735903586, 1.0972880185727023, 1.1136442732145437, 1.1144444474641522, 1.1187057814183101, 1.118378693834314, 1.1200951848711287, 1.122402690901545, 1.1272052872944347, 1.1107987825114936, 1.1227598113966692, 1.1200321989106428, 1.1236006219203054, 1.1341680946021244, 1.1141914599047507, 1.139567446238889, 1.126814243241484, 1.1193668705293502, 1.1327083032510943, 1.1258190469005815, 1.1352339458387277, 1.1376988533486678, 1.1413985167818117, 1.141991971748803, 1.1436691338988556, 1.14659913224344, 1.1498148055695157, 1.1481482978720579, 1.1343047000308735, 1.129738196950828, 1.1459765491031466, 1.1434987413472142, 1.1516430033447316, 1.1541994381420717, 1.1471533011920347, 1.1616781433227614, 1.1529896642969943, 1.141180596328134, 1.1602232937742336, 1.142124530521324, 1.1573725463134314, 1.1742615558830976, 1.14326153561008, 1.180669132516106, 1.1600461767616335, 1.1642470453760307, 1.1540978281760255, 1.1680370510505338, 1.1630629808053203, 1.1634529141956949, 1.173147947525939, 1.1839503795642572, 1.162580952268516, 1.193723070993408, 1.1622035192151374, 1.1690168919038695, 1.1868097639240458, 1.1604475740141469, 1.1806541144945744, 1.1945976754910448, 1.1740905414465417, 1.1646312837334494, 1.1884491091291305, 1.1804672475714597, 1.16808221943077, 1.2038541613345468, 1.1625499801682722, 1.210089054209454, 1.2028195446935193, 1.1742666654398877, 1.2081110162296513, 1.1976606023722682, 1.1853330697136364, 1.1881040720321079, 1.2011780265125342, 1.1912365028227883, 1.1966353482605006, 1.1921350024212365, 1.2075795937445755, 1.1826832094803232, 1.233143755955062, 1.1904677254421565, 1.197051245590736, 1.199032817372352, 1.1997002662696274, 1.228304527467499, 1.210164730967755, 1.2130606953537915, 1.1702195404002624, 1.1803084300656623, 1.2396738889377887, 1.1867275077525423, 1.2286149737087182, 1.2175265971467217, 1.2379050164778636, 1.2282031285155974, 1.163503568356456, 1.244904433173695, 1.227769702526149, 1.195150374583227, 1.203619449205195, 1.2120744117179332, 1.2152699993553224, 1.220188577382631, 1.2184853483303426, 1.2217604242913633, 1.2162872357125745, 1.2159885980421294, 1.236706530323561, 1.2118135252218136, 1.2165346711335707, 1.244037366069988, 1.2182386219012131, 1.2414421588916498, 1.2679648802589705, 1.2312362838065487, 1.2423882668437238, 1.230470798285724, 1.2360513167232519, 1.2281624928288075, 1.2260971145676862, 1.2241982021942515, 1.2738488787304982, 1.2586501298475343, 1.2221071309056775, 1.2557214586605578, 1.2310652942297298, 1.248402281152008, 1.3187366819929802, 1.2230399937073781, 1.250101757754246, 1.2295294904160774, 1.272856583344721, 1.2562719624618004, 1.3058818228334825, 1.1675433304118015, 1.291549660497894, 1.2053270265582356, 1.2867441412263316, 1.2720611187429067, 1.217710510459048, 1.2738093163188065, 1.235377102258366, 1.2437074231396754, 1.2633407393895542, 1.3148999891453383, 1.208650841697292, 1.2957264887679778, 1.262382559196898, 1.2238465555391485, 1.3120406507858502, 1.2291352189037403, 1.2395694619916342, 1.2982067774082053, 1.23866362974953, 1.2667057886107997, 1.3242740141738616, 1.2733263910697599, 1.2688327324997224, 1.285252620433939, 1.2437455235247934, 1.3261039993054369, 1.2808321481463554, 1.259753260119208, 1.2736116606613685, 1.3195673736249676, 1.287877370767014, 1.2199449026330156, 1.3302746425904273, 1.2663297236259348, 1.2609953308731856, 1.267114983207878, 1.318060942471321, 1.2333377080989394, 1.3195394602510926, 1.2855340077959259, 1.2674579183847838, 1.3132104476292927, 1.2561832077201756, 1.3100105390955858, 1.2872015237808228, 1.318016007420269, 1.2919967744150773, 1.2646855111975583, 1.3233899066013655, 1.2931964810454395, 1.2881544356667154, 1.2841373832746483, 1.328169793722469, 1.224457718860144, 1.3443326118152912, 1.3023221387064516, 1.2767848160075046, 1.2928551748664117, 1.3264522705172084, 1.2966049846952967, 1.2973592672833472, 1.2909374489572836, 1.3057860045988963, 1.2477454874903111, 1.3237062972558935, 1.2820649873251202, 1.3106346375053544, 1.3235391611340401, 1.32830645670053, 1.2619076614896652, 1.3385413607157315, 1.3168045615132022, 1.311702588313123, 1.3230403994496036, 1.303270165947662, 1.3456471680811866, 1.2578917258282991, 1.3283771145324206, 1.3209956295188816, 1.3550941145478799, 1.282043012883667, 1.3280900035585677, 1.313819091895531], 'val_acc': [0.3973727408790432, 0.3908045962819912, 0.4039408858675871, 0.38095238002258763, 0.3940886686294537, 0.385878487809734, 0.3875205240324017, 0.3990147771017109, 0.38916256074443434, 0.39244663211316705, 0.3875205240324017, 0.39244663250465894, 0.39737274117266214, 0.39901477739532987, 0.3990147771017109, 0.3990147771017109, 0.3990147770038379, 0.40229884954704637, 0.3973727408790432, 0.4022988494491734, 0.40229884954704637, 0.40229884964491935, 0.3875205240324017, 0.4006568134222516, 0.3990147774932028, 0.3940886686294537, 0.40065681332437864, 0.39573070475424843, 0.4022988494491734, 0.4022988494491734, 0.4022988494491734, 0.39901477739532987, 0.39737274117266214, 0.40229884964491935, 0.3940886687273267, 0.40065681332437864, 0.40065681332437864, 0.3957307050478674, 0.3990147770038379, 0.4022988493513004, 0.3990147770038379, 0.38259441585376347, 0.3908045961841182, 0.3990147770038379, 0.3875205244238936, 0.39244663230891297, 0.3973727406832972, 0.38752052383665575, 0.40065681312863266, 0.3809523792396038, 0.39901477680809194, 0.3842364513913203, 0.3858784875161151, 0.3842364514891933, 0.3973727407811702, 0.3924466319174211, 0.3842364513913203, 0.38916256035294244, 0.3908045958904993, 0.3940886681400888, 0.3940886682379618, 0.3908045958904993, 0.3842364510977014, 0.38423645129344736, 0.38095238002258763, 0.3891625597657046, 0.3990147771017109, 0.39737273990031335, 0.3891625594720856, 0.38916256054868836, 0.36945812646391357, 0.37931034321268203, 0.36945812646391357, 0.3940886680422158, 0.39080459579262633, 0.39573070416701056, 0.38423645158706626, 0.3809523788481119, 0.3760262718439494, 0.3875205232494179, 0.3940886683358347, 0.37110016239296234, 0.38752052383665575, 0.3694581263660406, 0.3727421982241382, 0.3661740539207051, 0.3776683067942683, 0.37931034311480905, 0.3760262706694736, 0.3809523792396038, 0.3809523792396038, 0.3825944150707796, 0.3940886686294537, 0.37931034272331715, 0.3727421983220111, 0.38752052364090983, 0.3645320176001644, 0.38095237943534976, 0.3809523792396038, 0.3661740539207051, 0.3743842346425518, 0.379310343506301, 0.38587848849484485, 0.3694581262681676, 0.37602627086521956, 0.3743842344468059, 0.3727421994964869, 0.3825944150707796, 0.38095237904385787, 0.3530377648244742, 0.38423645178281224, 0.3694581262681676, 0.37110016239296234, 0.38916256035294244, 0.3711001629802002, 0.38423645129344736, 0.36288998127962374, 0.3678160907306107, 0.3694581261702946, 0.3579638731988584, 0.3825944160495094, 0.36617405441007006, 0.36945812695327845, 0.3612479456441939, 0.3809523791417308, 0.3431855499597606, 0.36945812714902443, 0.34318554797783274, 0.35960590942152615, 0.3776683071857603, 0.37766830767512516, 0.3743842352297897, 0.3760262703758547, 0.37602627145245743, 0.37766830728363326, 0.3530377648244742, 0.372742199007122, 0.38259441585376347, 0.3661740539207051, 0.3776683071857603, 0.36781609004549987, 0.3481116584320178, 0.3842364523700501, 0.3530377649223472, 0.35960590932365316, 0.3694581263660406, 0.3842364524679231, 0.35632183726980965, 0.361247945252702, 0.37602627106096553, 0.36945812646391357, 0.3612479453505749, 0.38916256074443434, 0.3842364524679231, 0.37766830757725217, 0.3563218368783177, 0.3825944161473824, 0.3300492592730937, 0.3825944160495094, 0.38587848839697186, 0.3891625608423073, 0.3448275839068815, 0.38259441624525536, 0.36453201828527526, 0.3678160905348648, 0.38752052481538557, 0.3366174054605816, 0.3842364522721771, 0.36453201838314825, 0.3579638738839693, 0.37602627135458444, 0.35796387300311244, 0.3333333315226832, 0.3891625608423073, 0.3382594397991945, 0.3743842346425518, 0.3875205247175126, 0.34154351214665696, 0.3957307051457404, 0.3842364520764312, 0.3793103437999199, 0.38095237992471465, 0.3464696204231682, 0.36453201818740233, 0.3481116572330738, 0.35960591020451, 0.37110016219721637, 0.3333333312290643, 0.3825944160495094, 0.34811165654796294, 0.37766830787087113, 0.35796387300311244, 0.3678160905348648, 0.3842364525657961, 0.34318554836932463, 0.38095237953322275, 0.37438423552340866, 0.3628899824540995, 0.3546798016343798, 0.3119868634907874, 0.3793103438977929, 0.3563218378570475, 0.3513957292869173, 0.37602627135458444, 0.3793103437020469, 0.35303776550958504, 0.366174054605816, 0.3760262716482034, 0.3513957294826633, 0.359605911305581, 0.37438423562128165, 0.35632183766130154, 0.3727421994964869, 0.3333333314248102, 0.3776683071857603, 0.39737274136840806, 0.3481116570373278, 0.37766830738150625, 0.3727421993986139, 0.36124794583993985, 0.3743842353276627, 0.32512315089870947, 0.38095237982684166, 0.33661740377227267, 0.3431855484671976, 0.3743842353276627, 0.35632183756342856, 0.34646962120615205, 0.3448275844941194, 0.36124794623143175, 0.36124794613355876, 0.3661740543121971, 0.3628899822583535, 0.35303776550958504, 0.38095237982684166, 0.34975369325999556, 0.3546798014386338, 0.38916256074443434, 0.3513957294826633, 0.3612479460356858, 0.3382594399949404, 0.3497536929663766, 0.3760262715503304, 0.3628899821604805, 0.334975367549605, 0.3431855485650706, 0.36124794613355876, 0.35303776550958504, 0.33497536813684287, 0.3743842352297897, 0.36945812685540547, 0.3185550063995305, 0.366174054605816, 0.35303776521596614, 0.3366174043595106, 0.36124794623143175, 0.35303776541171206, 0.35303776541171206, 0.35632183775917453, 0.3382594399949404, 0.3678160907306107, 0.35796387398184226, 0.3743842353276627, 0.3070607532323483, 0.3760262715503304, 0.334975367549605, 0.3678160910242297, 0.3415435123424029, 0.37274219930074093, 0.3218390783555011, 0.34154351263602184, 0.34811165693945484, 0.37110016337169216, 0.33169129549576143, 0.3645320187746402, 0.36453201848102124, 0.3481116571352008, 0.34646962101040607, 0.3645320187746402, 0.3119868634907874, 0.3399014765112271, 0.3497536930642496, 0.34975369345574153, 0.3431855486629436, 0.37766830777299815, 0.35632183775917453, 0.32512315099658246, 0.34482758478773834, 0.37274219910499495, 0.34975369325999556, 0.3464696206189142, 0.3563218378570475, 0.34975369286850366, 0.3431855490544355, 0.3267651871213772, 0.36288998264984546, 0.3924466330918968, 0.3431855490544355, 0.3431855486629436, 0.3645320186767672], 'loss': [1.0846197395598864, 1.075329316910777, 1.0742795469824538, 1.0747262602224488, 1.0744082662359156, 1.0738718560099356, 1.0738704783226185, 1.0738306269753395, 1.074405997000191, 1.0738868638964894, 1.0738456281548407, 1.0737870479756066, 1.0736542848590953, 1.0737802914770231, 1.0735885969177654, 1.0736154340865431, 1.0737161729859621, 1.0736791834449377, 1.0739236302444333, 1.0734205542648598, 1.0734721080478458, 1.073638455529967, 1.073629232941222, 1.0734694088753733, 1.0733397168055696, 1.0733609479066037, 1.0731936996232803, 1.0732527254543265, 1.0732563237634773, 1.0729869465563577, 1.0731264251458326, 1.0729373807045468, 1.0730804253407817, 1.072770694785539, 1.0733694197460857, 1.0723873746958112, 1.0728172600881274, 1.0729080995250286, 1.0720485069913297, 1.0723387245769618, 1.0723961124185175, 1.073812128092474, 1.0722902087460309, 1.0722045059321597, 1.0723784013940079, 1.0726055475231069, 1.071762171809923, 1.0715094904635232, 1.0719203993035538, 1.071159761152718, 1.071225959956034, 1.0706848994907168, 1.0707073316926583, 1.0710318917856079, 1.069846360884163, 1.0708063656789322, 1.0699445926678008, 1.0686152251104555, 1.0691431903251631, 1.0677922852964616, 1.0666866688757706, 1.0658497019225321, 1.0644422043030757, 1.0681192948832894, 1.0636173898434491, 1.0616595740680577, 1.0583503395624965, 1.0554541476208572, 1.053569475923965, 1.0525900556811072, 1.056639661093757, 1.046782144628756, 1.0486735396316653, 1.0525204856292913, 1.0496045673652352, 1.0469476871177157, 1.04572720698997, 1.0434019774137335, 1.043624277819843, 1.0469819922222003, 1.042649231505345, 1.0386468598974803, 1.035109110681428, 1.0336970149124427, 1.032637516331134, 1.0297201976639045, 1.0303179210705924, 1.0294360089840586, 1.0282716826981342, 1.0310913052647022, 1.027422737536734, 1.0273623326965426, 1.0227455466190158, 1.0218616327465926, 1.0204475500255639, 1.0301674626446358, 1.0204607588554555, 1.0221442394432836, 1.0209246853293825, 1.0222046241623175, 1.022139212531965, 1.0145652699519476, 1.0174527754284273, 1.0132522860591662, 1.011704238435326, 1.0121347616830156, 1.0157780581676006, 1.0156331483343544, 1.0288092131976965, 1.0173405363084844, 1.0123278184103526, 1.005346290751894, 1.0094084835884751, 1.0086845955564745, 1.0040985862332448, 1.0057629507914705, 1.0094451124418442, 1.0135440497917316, 1.0072782519416887, 1.0011294226382057, 1.0000847512936446, 1.0001258257722954, 1.006005333850516, 1.0026125569118367, 1.0001607818525184, 1.002269300494106, 0.9947651985979178, 0.9997159653130988, 0.99524690443975, 0.9923820894601654, 0.9898138608041486, 0.9887703193776172, 0.9884179024725724, 0.9941691427015426, 0.9837910811759119, 0.987757087732977, 0.9890718088257728, 0.9828147924901034, 0.9849145535326101, 0.9864462614549014, 0.9821391108834033, 0.9859814361135573, 0.9874207172060894, 0.9823410977817904, 0.9879538382348094, 0.9784103575428409, 0.9767678874474042, 0.9751148423864612, 0.9791883151878811, 0.9777294840656022, 0.973133687390439, 0.9715494686817976, 0.9704862467072582, 0.9726038973923826, 0.9780482977567512, 0.972928902182491, 0.9694134811111544, 0.9697787423887781, 0.968200456533099, 0.966431422155251, 0.9652780866476055, 0.969922404857142, 0.9741693452153607, 0.9641889966488864, 0.9643178731998624, 0.9626244720492275, 0.9722729939699662, 0.9772495271488871, 0.9695262079121396, 0.96388237703997, 0.963412196606826, 0.9577393835574939, 0.966522252314879, 0.9600513235989048, 0.9593708649797851, 0.9573938014570937, 0.9666142718258335, 0.9599539955538646, 0.9618894869786758, 0.9603862525256508, 0.9545253383305528, 0.9557393006230771, 0.9535988651017144, 0.9559524096502661, 0.9527760626843823, 0.9604513076296577, 0.9568093141491163, 0.9576086899583099, 0.9601448303130617, 0.946804317948265, 0.9505948478681107, 0.9540887505856382, 0.9474660255336174, 0.9428116030761593, 0.9433078303474176, 0.9425229197410098, 0.9464858376025175, 0.9432225493435008, 0.9406451037532251, 0.9435383344577812, 0.9525063687526225, 0.9419057126407506, 0.93546134789132, 0.9430525545711634, 0.9362972187310519, 0.9334873360530063, 0.9339059297064246, 0.9353633574391782, 0.9366817575215803, 0.9324569161667716, 0.9439461613827418, 0.9351764659617227, 0.938900275225512, 0.9405872395395988, 0.9380392134801563, 0.9430294593501629, 0.944480441458661, 0.9408573727343361, 0.9366039323366153, 0.936867063099354, 0.9361677155357612, 0.940212395573054, 0.9278780430249365, 0.9236963787852371, 0.9287239261965977, 0.926353629389338, 0.9225693750430426, 0.9304199505390818, 0.9269782043580402, 0.9321826991359311, 0.9265592162613996, 0.9246725939382517, 0.934967717962833, 0.9237388363119513, 0.9148807483287318, 0.9202902049499371, 0.9216482944557065, 0.9215503359722161, 0.9239926215804333, 0.917761656586884, 0.9125419748637221, 0.9138748153279205, 0.9176137676963572, 0.914771961039831, 0.9130059649567339, 0.910789235930668, 0.904516020668116, 0.9106104527655569, 0.916961532329387, 0.9172715333942515, 0.9161801068934572, 0.9076781766615364, 0.9078082285132986, 0.9064578560588297, 0.9101906446460826, 0.9043808482021277, 0.9015972575612626, 0.8981710169349608, 0.8983216960327336, 0.9078429521231681, 0.9019628391618356, 0.9071950550441624, 0.902611517245275, 0.9035496973159132, 0.9174495243193922, 0.9049778586785162, 0.8985508238510429, 0.8959380029163322, 0.9060603410556332, 0.9027557271951523, 0.8972527394794096, 0.8969760865891004, 0.8996626239782486, 0.8975029474166385, 0.9055965721484817, 0.9004435219069526, 0.8990357229597025, 0.8994998866282939, 0.9095673958378896, 0.8974431917407919, 0.906994552607409, 0.8956326611967302, 0.8861317476942309, 0.8867454555245151, 0.8907376827645351, 0.8885386155860869, 0.8979797118743098, 0.8927778777645354, 0.8831573252805205, 0.8910222340658215, 0.8802657161160417, 0.8809509242829356, 0.8770321035776785, 0.8808563174897885, 0.8858933322483509, 0.886325008325753, 0.8814779229722228, 0.8782654710129301, 0.8770371979511739, 0.8658977499487953], 'acc': [0.3897330608945608, 0.39219712699463233, 0.39753593270049203, 0.3868583152181559, 0.3963038992220861, 0.3967145772318086, 0.3958932243823026, 0.3950718687545103, 0.39219712699463233, 0.3942505141058497, 0.39507186953781565, 0.3967145794226159, 0.398767968100444, 0.4016427122102381, 0.4041067741612389, 0.3963039016087197, 0.3946611905489614, 0.398767964930505, 0.40205339002413426, 0.3963039016087197, 0.39835728852411073, 0.4020533868541952, 0.398767968100444, 0.39507186992946836, 0.4049281319798385, 0.4016427126018908, 0.3971252588031228, 0.39548254578510106, 0.3971252584114701, 0.4004106771652214, 0.4016427108394537, 0.3995893236915189, 0.39999999977969536, 0.40328542287834374, 0.4020533866583689, 0.39548254398594646, 0.40082135736575114, 0.398357289466525, 0.40739219706406093, 0.39876796610546306, 0.4012320324380785, 0.403285418729273, 0.39917864532686115, 0.3987679661421805, 0.40698152183262476, 0.40123203361303655, 0.4000000015421325, 0.41355236329337164, 0.39425051547663414, 0.4114989740647819, 0.41108829703419114, 0.41601642704352704, 0.4127310065273387, 0.414373718174576, 0.42053387888403154, 0.4197125266220046, 0.4135523603559764, 0.4242299796007497, 0.4119096522703308, 0.41806981415474437, 0.4344969192691897, 0.43285421157519677, 0.433264887981591, 0.4258726920680099, 0.4340862445885151, 0.4439425046806218, 0.44722792598011557, 0.4488706378598967, 0.4558521567796045, 0.46201232026734634, 0.4472279240218521, 0.4640657097284799, 0.46201232183395713, 0.4513347012183994, 0.45831622307550246, 0.45954825455892745, 0.4661190987979607, 0.4644763861715916, 0.467761807275259, 0.46406570812515163, 0.4657084176182992, 0.47926077803547135, 0.4796714582360011, 0.47802874909778886, 0.4825461993716825, 0.4726899383004441, 0.47967145643684655, 0.4788501047622986, 0.47638603686307246, 0.4850102664876033, 0.48870636383855615, 0.4850102684458668, 0.49281314276082316, 0.48706365414958225, 0.4936344960386993, 0.4743326474019389, 0.4969199187824124, 0.4845995898119478, 0.494866527913777, 0.4899383961052865, 0.4899383988468554, 0.4960985609638128, 0.49815194924998824, 0.49199178654555176, 0.5034907620423139, 0.49650924233930066, 0.4903490760732725, 0.4973305949929803, 0.48418891238970435, 0.49404517678999066, 0.5030800816459577, 0.5047227942723268, 0.5026694077485886, 0.49938398249585036, 0.5039014364904447, 0.5047227923140634, 0.5014373727402893, 0.4989733038986488, 0.5018480505541855, 0.5121149890476673, 0.5108829606974639, 0.5104722830793941, 0.5051334722820493, 0.5002053389069481, 0.5178644764839502, 0.5030800798835207, 0.5133470250350983, 0.5133470264058828, 0.5055441461794186, 0.5121149884601883, 0.5186858321117425, 0.5174538023907546, 0.5203285433673271, 0.5195071863687504, 0.5240246448673507, 0.5264887070508953, 0.5112936353781384, 0.5256673557312826, 0.5199178633993411, 0.5219712550145644, 0.5293634529231266, 0.5285420941621126, 0.5207392219645287, 0.5252566740008595, 0.5211498966452033, 0.5330595522690602, 0.5326488730843797, 0.5371663209349223, 0.531006161828795, 0.5289527673985679, 0.5425051378763187, 0.5359342905040639, 0.5379876832942454, 0.5388090367679479, 0.5322381940955254, 0.535523615627563, 0.5322381956621361, 0.5416837781361731, 0.537166326491495, 0.5429158113820353, 0.537987683685898, 0.5449692024097795, 0.532238197228747, 0.5433264931124583, 0.5441478448237237, 0.5359342953997226, 0.5363449692970919, 0.5240246438882189, 0.5388090353971634, 0.5449692008431687, 0.5392197153651495, 0.5412731044346302, 0.5310061637870584, 0.5425051376804924, 0.545379876502975, 0.5511293605367751, 0.5416837824443528, 0.5416837806819156, 0.5441478473694662, 0.5507186894544096, 0.5523613993392098, 0.5507186884752778, 0.5531827543795231, 0.5433264909583685, 0.5462012346765099, 0.5408624193017243, 0.545790555296003, 0.542094456929201, 0.5318275180440664, 0.5466119128820588, 0.540862426620734, 0.5383983577790936, 0.5494866575793319, 0.5503080110530344, 0.5474332677265457, 0.5531827478438187, 0.5548254652434551, 0.5523613916285474, 0.5577002077866384, 0.5622176568121391, 0.5433264848143168, 0.5535934248744095, 0.5605749513089534, 0.5585215622394726, 0.5597535895371094, 0.5626283395951289, 0.5585215643935625, 0.5535934314101139, 0.5601642725159255, 0.5655030831174439, 0.5445585257708414, 0.5523613918243737, 0.5507186829187053, 0.5540041088323574, 0.5609856302978077, 0.5445585196267898, 0.5540041096156627, 0.5593429128981714, 0.5568788545087623, 0.5560574913661338, 0.5581108796523092, 0.5593429200213548, 0.5634496958104003, 0.5601642746700154, 0.5593429202171811, 0.5630390100900152, 0.5753593413736786, 0.5630390170173724, 0.5634496882955641, 0.5585215616519936, 0.5601642750616681, 0.5642710433358774, 0.5556468218503792, 0.5642710504590608, 0.5749486637556088, 0.5708418850291681, 0.5605749439899437, 0.5622176552455284, 0.5696098602772738, 0.5671457880331505, 0.5737166326638364, 0.5778234074737502, 0.5761807019705645, 0.5634496902538276, 0.5716632416360922, 0.5757700251358001, 0.576591379980287, 0.5737166297264412, 0.5650924021703262, 0.5749486617973453, 0.5716632408527867, 0.5757700178167903, 0.5728952739028226, 0.5770020508668261, 0.5761807025580435, 0.5823408593142547, 0.5782340821544247, 0.5798767938016621, 0.5823408604892127, 0.5786447603599736, 0.5753593471260776, 0.5720739202332937, 0.5885010265226971, 0.5753593386321098, 0.5638603722535119, 0.5794661175543767, 0.5728952814176587, 0.5843942497545199, 0.5815195117887775, 0.5798767975223628, 0.5860369574852303, 0.5790554443179214, 0.5806981561609852, 0.5770020577941832, 0.580287478934568, 0.5774127340414685, 0.5741273081278164, 0.5794661169668976, 0.5675564656512203, 0.5794661155961133, 0.5679671501966472, 0.5872689903394398, 0.5889117089140342, 0.589322386532104, 0.5831622214777513, 0.5889117037491143, 0.5794661161835923, 0.5921971215604512, 0.5868583131130226, 0.586036957876883, 0.5930184781673753, 0.5901437375824554, 0.5979466107591711, 0.5950718664535507, 0.5913757669117906, 0.5926078057142253, 0.5860369584643621, 0.5938398337951676, 0.5893223811713577, 0.6012320301371189]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
