{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf20.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 04:17:37 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '2', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '05', '01', '04', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000147BE94CE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000147BA116EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6082, Accuracy:0.2329, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6064, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6011, Accuracy:0.2329, Validation Loss:1.5997, Validation Accuracy:0.2332\n",
    "Epoch #28: Loss:1.5988, Accuracy:0.2333, Validation Loss:1.5961, Validation Accuracy:0.2332\n",
    "Epoch #29: Loss:1.5938, Accuracy:0.2423, Validation Loss:1.5894, Validation Accuracy:0.2496\n",
    "Epoch #30: Loss:1.5852, Accuracy:0.2600, Validation Loss:1.5769, Validation Accuracy:0.2660\n",
    "Epoch #31: Loss:1.5694, Accuracy:0.2752, Validation Loss:1.5551, Validation Accuracy:0.2824\n",
    "Epoch #32: Loss:1.5438, Accuracy:0.2969, Validation Loss:1.5214, Validation Accuracy:0.3005\n",
    "Epoch #33: Loss:1.5109, Accuracy:0.3088, Validation Loss:1.4845, Validation Accuracy:0.3448\n",
    "Epoch #34: Loss:1.4843, Accuracy:0.3269, Validation Loss:1.4608, Validation Accuracy:0.3678\n",
    "Epoch #35: Loss:1.4727, Accuracy:0.3232, Validation Loss:1.4653, Validation Accuracy:0.3580\n",
    "Epoch #36: Loss:1.4770, Accuracy:0.3306, Validation Loss:1.4491, Validation Accuracy:0.3760\n",
    "Epoch #37: Loss:1.4673, Accuracy:0.3302, Validation Loss:1.4457, Validation Accuracy:0.3777\n",
    "Epoch #38: Loss:1.4613, Accuracy:0.3343, Validation Loss:1.4447, Validation Accuracy:0.3727\n",
    "Epoch #39: Loss:1.4588, Accuracy:0.3339, Validation Loss:1.4439, Validation Accuracy:0.3777\n",
    "Epoch #40: Loss:1.4547, Accuracy:0.3318, Validation Loss:1.4414, Validation Accuracy:0.3777\n",
    "Epoch #41: Loss:1.4518, Accuracy:0.3359, Validation Loss:1.4393, Validation Accuracy:0.3596\n",
    "Epoch #42: Loss:1.4492, Accuracy:0.3425, Validation Loss:1.4377, Validation Accuracy:0.3744\n",
    "Epoch #43: Loss:1.4465, Accuracy:0.3441, Validation Loss:1.4348, Validation Accuracy:0.3448\n",
    "Epoch #44: Loss:1.4481, Accuracy:0.3405, Validation Loss:1.4360, Validation Accuracy:0.3481\n",
    "Epoch #45: Loss:1.4430, Accuracy:0.3413, Validation Loss:1.4391, Validation Accuracy:0.3333\n",
    "Epoch #46: Loss:1.4416, Accuracy:0.3351, Validation Loss:1.4333, Validation Accuracy:0.3465\n",
    "Epoch #47: Loss:1.4404, Accuracy:0.3421, Validation Loss:1.4302, Validation Accuracy:0.3350\n",
    "Epoch #48: Loss:1.4396, Accuracy:0.3351, Validation Loss:1.4365, Validation Accuracy:0.3415\n",
    "Epoch #49: Loss:1.4469, Accuracy:0.3417, Validation Loss:1.4392, Validation Accuracy:0.3432\n",
    "Epoch #50: Loss:1.4408, Accuracy:0.3446, Validation Loss:1.4295, Validation Accuracy:0.3498\n",
    "Epoch #51: Loss:1.4378, Accuracy:0.3450, Validation Loss:1.4306, Validation Accuracy:0.3612\n",
    "Epoch #52: Loss:1.4409, Accuracy:0.3474, Validation Loss:1.4273, Validation Accuracy:0.3383\n",
    "Epoch #53: Loss:1.4368, Accuracy:0.3437, Validation Loss:1.4335, Validation Accuracy:0.3432\n",
    "Epoch #54: Loss:1.4349, Accuracy:0.3520, Validation Loss:1.4253, Validation Accuracy:0.3563\n",
    "Epoch #55: Loss:1.4309, Accuracy:0.3520, Validation Loss:1.4249, Validation Accuracy:0.3399\n",
    "Epoch #56: Loss:1.4303, Accuracy:0.3491, Validation Loss:1.4235, Validation Accuracy:0.3481\n",
    "Epoch #57: Loss:1.4315, Accuracy:0.3524, Validation Loss:1.4392, Validation Accuracy:0.3284\n",
    "Epoch #58: Loss:1.4350, Accuracy:0.3511, Validation Loss:1.4236, Validation Accuracy:0.3563\n",
    "Epoch #59: Loss:1.4285, Accuracy:0.3569, Validation Loss:1.4222, Validation Accuracy:0.3530\n",
    "Epoch #60: Loss:1.4285, Accuracy:0.3524, Validation Loss:1.4281, Validation Accuracy:0.3547\n",
    "Epoch #61: Loss:1.4270, Accuracy:0.3577, Validation Loss:1.4278, Validation Accuracy:0.3678\n",
    "Epoch #62: Loss:1.4392, Accuracy:0.3593, Validation Loss:1.4257, Validation Accuracy:0.3596\n",
    "Epoch #63: Loss:1.4328, Accuracy:0.3474, Validation Loss:1.4237, Validation Accuracy:0.3498\n",
    "Epoch #64: Loss:1.4324, Accuracy:0.3491, Validation Loss:1.4214, Validation Accuracy:0.3777\n",
    "Epoch #65: Loss:1.4317, Accuracy:0.3478, Validation Loss:1.4219, Validation Accuracy:0.3530\n",
    "Epoch #66: Loss:1.4259, Accuracy:0.3569, Validation Loss:1.4177, Validation Accuracy:0.3596\n",
    "Epoch #67: Loss:1.4232, Accuracy:0.3577, Validation Loss:1.4158, Validation Accuracy:0.3662\n",
    "Epoch #68: Loss:1.4225, Accuracy:0.3610, Validation Loss:1.4170, Validation Accuracy:0.3596\n",
    "Epoch #69: Loss:1.4206, Accuracy:0.3614, Validation Loss:1.4156, Validation Accuracy:0.3727\n",
    "Epoch #70: Loss:1.4205, Accuracy:0.3639, Validation Loss:1.4161, Validation Accuracy:0.3744\n",
    "Epoch #71: Loss:1.4243, Accuracy:0.3626, Validation Loss:1.4259, Validation Accuracy:0.3530\n",
    "Epoch #72: Loss:1.4240, Accuracy:0.3622, Validation Loss:1.4144, Validation Accuracy:0.3826\n",
    "Epoch #73: Loss:1.4202, Accuracy:0.3659, Validation Loss:1.4136, Validation Accuracy:0.3727\n",
    "Epoch #74: Loss:1.4178, Accuracy:0.3626, Validation Loss:1.4182, Validation Accuracy:0.3612\n",
    "Epoch #75: Loss:1.4215, Accuracy:0.3606, Validation Loss:1.4122, Validation Accuracy:0.3645\n",
    "Epoch #76: Loss:1.4170, Accuracy:0.3680, Validation Loss:1.4127, Validation Accuracy:0.3760\n",
    "Epoch #77: Loss:1.4153, Accuracy:0.3688, Validation Loss:1.4124, Validation Accuracy:0.3826\n",
    "Epoch #78: Loss:1.4155, Accuracy:0.3704, Validation Loss:1.4126, Validation Accuracy:0.3842\n",
    "Epoch #79: Loss:1.4175, Accuracy:0.3684, Validation Loss:1.4160, Validation Accuracy:0.3612\n",
    "Epoch #80: Loss:1.4176, Accuracy:0.3745, Validation Loss:1.4129, Validation Accuracy:0.3580\n",
    "Epoch #81: Loss:1.4184, Accuracy:0.3671, Validation Loss:1.4132, Validation Accuracy:0.3892\n",
    "Epoch #82: Loss:1.4164, Accuracy:0.3667, Validation Loss:1.4096, Validation Accuracy:0.3727\n",
    "Epoch #83: Loss:1.4160, Accuracy:0.3622, Validation Loss:1.4224, Validation Accuracy:0.3580\n",
    "Epoch #84: Loss:1.4184, Accuracy:0.3684, Validation Loss:1.4132, Validation Accuracy:0.3793\n",
    "Epoch #85: Loss:1.4154, Accuracy:0.3663, Validation Loss:1.4121, Validation Accuracy:0.3662\n",
    "Epoch #86: Loss:1.4164, Accuracy:0.3671, Validation Loss:1.4120, Validation Accuracy:0.3596\n",
    "Epoch #87: Loss:1.4129, Accuracy:0.3680, Validation Loss:1.4125, Validation Accuracy:0.3826\n",
    "Epoch #88: Loss:1.4155, Accuracy:0.3667, Validation Loss:1.4193, Validation Accuracy:0.3547\n",
    "Epoch #89: Loss:1.4163, Accuracy:0.3688, Validation Loss:1.4049, Validation Accuracy:0.3908\n",
    "Epoch #90: Loss:1.4220, Accuracy:0.3585, Validation Loss:1.4073, Validation Accuracy:0.3875\n",
    "Epoch #91: Loss:1.4141, Accuracy:0.3708, Validation Loss:1.4120, Validation Accuracy:0.3678\n",
    "Epoch #92: Loss:1.4120, Accuracy:0.3676, Validation Loss:1.4051, Validation Accuracy:0.3924\n",
    "Epoch #93: Loss:1.4070, Accuracy:0.3717, Validation Loss:1.4146, Validation Accuracy:0.3547\n",
    "Epoch #94: Loss:1.4099, Accuracy:0.3680, Validation Loss:1.4035, Validation Accuracy:0.3859\n",
    "Epoch #95: Loss:1.4132, Accuracy:0.3671, Validation Loss:1.4018, Validation Accuracy:0.3859\n",
    "Epoch #96: Loss:1.4063, Accuracy:0.3659, Validation Loss:1.4023, Validation Accuracy:0.3760\n",
    "Epoch #97: Loss:1.4060, Accuracy:0.3725, Validation Loss:1.4019, Validation Accuracy:0.3793\n",
    "Epoch #98: Loss:1.4039, Accuracy:0.3721, Validation Loss:1.4007, Validation Accuracy:0.3842\n",
    "Epoch #99: Loss:1.4045, Accuracy:0.3729, Validation Loss:1.4022, Validation Accuracy:0.3695\n",
    "Epoch #100: Loss:1.4035, Accuracy:0.3688, Validation Loss:1.3986, Validation Accuracy:0.3793\n",
    "Epoch #101: Loss:1.4029, Accuracy:0.3704, Validation Loss:1.3993, Validation Accuracy:0.3793\n",
    "Epoch #102: Loss:1.4023, Accuracy:0.3721, Validation Loss:1.3993, Validation Accuracy:0.3678\n",
    "Epoch #103: Loss:1.4021, Accuracy:0.3663, Validation Loss:1.3984, Validation Accuracy:0.3777\n",
    "Epoch #104: Loss:1.4007, Accuracy:0.3717, Validation Loss:1.3972, Validation Accuracy:0.3777\n",
    "Epoch #105: Loss:1.4012, Accuracy:0.3717, Validation Loss:1.3983, Validation Accuracy:0.3711\n",
    "Epoch #106: Loss:1.4015, Accuracy:0.3696, Validation Loss:1.4004, Validation Accuracy:0.3793\n",
    "Epoch #107: Loss:1.4009, Accuracy:0.3721, Validation Loss:1.3965, Validation Accuracy:0.3892\n",
    "Epoch #108: Loss:1.3995, Accuracy:0.3692, Validation Loss:1.3948, Validation Accuracy:0.3859\n",
    "Epoch #109: Loss:1.4016, Accuracy:0.3680, Validation Loss:1.3947, Validation Accuracy:0.3859\n",
    "Epoch #110: Loss:1.4003, Accuracy:0.3741, Validation Loss:1.3942, Validation Accuracy:0.3777\n",
    "Epoch #111: Loss:1.3987, Accuracy:0.3782, Validation Loss:1.3972, Validation Accuracy:0.3695\n",
    "Epoch #112: Loss:1.3977, Accuracy:0.3754, Validation Loss:1.3930, Validation Accuracy:0.3842\n",
    "Epoch #113: Loss:1.4002, Accuracy:0.3770, Validation Loss:1.4123, Validation Accuracy:0.3678\n",
    "Epoch #114: Loss:1.4097, Accuracy:0.3651, Validation Loss:1.3992, Validation Accuracy:0.3760\n",
    "Epoch #115: Loss:1.4116, Accuracy:0.3692, Validation Loss:1.4115, Validation Accuracy:0.3842\n",
    "Epoch #116: Loss:1.4112, Accuracy:0.3671, Validation Loss:1.4241, Validation Accuracy:0.3481\n",
    "Epoch #117: Loss:1.4108, Accuracy:0.3741, Validation Loss:1.4000, Validation Accuracy:0.3892\n",
    "Epoch #118: Loss:1.3980, Accuracy:0.3754, Validation Loss:1.3963, Validation Accuracy:0.3711\n",
    "Epoch #119: Loss:1.3984, Accuracy:0.3700, Validation Loss:1.3897, Validation Accuracy:0.3842\n",
    "Epoch #120: Loss:1.3982, Accuracy:0.3708, Validation Loss:1.3895, Validation Accuracy:0.3777\n",
    "Epoch #121: Loss:1.3989, Accuracy:0.3749, Validation Loss:1.3930, Validation Accuracy:0.3727\n",
    "Epoch #122: Loss:1.3950, Accuracy:0.3758, Validation Loss:1.3906, Validation Accuracy:0.3760\n",
    "Epoch #123: Loss:1.3929, Accuracy:0.3745, Validation Loss:1.3882, Validation Accuracy:0.3727\n",
    "Epoch #124: Loss:1.3936, Accuracy:0.3680, Validation Loss:1.3904, Validation Accuracy:0.3875\n",
    "Epoch #125: Loss:1.3962, Accuracy:0.3770, Validation Loss:1.3876, Validation Accuracy:0.3727\n",
    "Epoch #126: Loss:1.3919, Accuracy:0.3766, Validation Loss:1.3921, Validation Accuracy:0.3826\n",
    "Epoch #127: Loss:1.3927, Accuracy:0.3725, Validation Loss:1.3887, Validation Accuracy:0.3810\n",
    "Epoch #128: Loss:1.3931, Accuracy:0.3749, Validation Loss:1.3870, Validation Accuracy:0.3711\n",
    "Epoch #129: Loss:1.3903, Accuracy:0.3795, Validation Loss:1.3865, Validation Accuracy:0.3760\n",
    "Epoch #130: Loss:1.3901, Accuracy:0.3741, Validation Loss:1.3865, Validation Accuracy:0.3727\n",
    "Epoch #131: Loss:1.3919, Accuracy:0.3733, Validation Loss:1.3931, Validation Accuracy:0.3777\n",
    "Epoch #132: Loss:1.3918, Accuracy:0.3737, Validation Loss:1.3898, Validation Accuracy:0.3727\n",
    "Epoch #133: Loss:1.3913, Accuracy:0.3778, Validation Loss:1.3877, Validation Accuracy:0.3826\n",
    "Epoch #134: Loss:1.3891, Accuracy:0.3782, Validation Loss:1.3862, Validation Accuracy:0.3760\n",
    "Epoch #135: Loss:1.3885, Accuracy:0.3737, Validation Loss:1.3879, Validation Accuracy:0.3727\n",
    "Epoch #136: Loss:1.3908, Accuracy:0.3729, Validation Loss:1.3860, Validation Accuracy:0.3842\n",
    "Epoch #137: Loss:1.3888, Accuracy:0.3725, Validation Loss:1.3862, Validation Accuracy:0.3744\n",
    "Epoch #138: Loss:1.3907, Accuracy:0.3803, Validation Loss:1.3895, Validation Accuracy:0.3678\n",
    "Epoch #139: Loss:1.3886, Accuracy:0.3758, Validation Loss:1.3857, Validation Accuracy:0.3777\n",
    "Epoch #140: Loss:1.3877, Accuracy:0.3758, Validation Loss:1.3843, Validation Accuracy:0.3777\n",
    "Epoch #141: Loss:1.3862, Accuracy:0.3725, Validation Loss:1.3878, Validation Accuracy:0.3760\n",
    "Epoch #142: Loss:1.3860, Accuracy:0.3791, Validation Loss:1.3893, Validation Accuracy:0.3612\n",
    "Epoch #143: Loss:1.3865, Accuracy:0.3754, Validation Loss:1.3833, Validation Accuracy:0.3875\n",
    "Epoch #144: Loss:1.3844, Accuracy:0.3745, Validation Loss:1.3835, Validation Accuracy:0.3810\n",
    "Epoch #145: Loss:1.3851, Accuracy:0.3770, Validation Loss:1.3961, Validation Accuracy:0.3580\n",
    "Epoch #146: Loss:1.3866, Accuracy:0.3729, Validation Loss:1.3837, Validation Accuracy:0.3744\n",
    "Epoch #147: Loss:1.3874, Accuracy:0.3737, Validation Loss:1.3909, Validation Accuracy:0.3760\n",
    "Epoch #148: Loss:1.3903, Accuracy:0.3733, Validation Loss:1.4058, Validation Accuracy:0.3547\n",
    "Epoch #149: Loss:1.3972, Accuracy:0.3754, Validation Loss:1.3886, Validation Accuracy:0.3760\n",
    "Epoch #150: Loss:1.3856, Accuracy:0.3692, Validation Loss:1.3885, Validation Accuracy:0.3892\n",
    "Epoch #151: Loss:1.3902, Accuracy:0.3770, Validation Loss:1.3852, Validation Accuracy:0.3793\n",
    "Epoch #152: Loss:1.3878, Accuracy:0.3799, Validation Loss:1.3905, Validation Accuracy:0.3744\n",
    "Epoch #153: Loss:1.3870, Accuracy:0.3811, Validation Loss:1.3814, Validation Accuracy:0.3760\n",
    "Epoch #154: Loss:1.3846, Accuracy:0.3766, Validation Loss:1.3818, Validation Accuracy:0.3711\n",
    "Epoch #155: Loss:1.3858, Accuracy:0.3786, Validation Loss:1.3877, Validation Accuracy:0.3744\n",
    "Epoch #156: Loss:1.3827, Accuracy:0.3811, Validation Loss:1.3811, Validation Accuracy:0.3793\n",
    "Epoch #157: Loss:1.3875, Accuracy:0.3799, Validation Loss:1.3892, Validation Accuracy:0.3826\n",
    "Epoch #158: Loss:1.3869, Accuracy:0.3807, Validation Loss:1.3864, Validation Accuracy:0.3695\n",
    "Epoch #159: Loss:1.3868, Accuracy:0.3795, Validation Loss:1.3969, Validation Accuracy:0.3629\n",
    "Epoch #160: Loss:1.3892, Accuracy:0.3758, Validation Loss:1.3828, Validation Accuracy:0.3810\n",
    "Epoch #161: Loss:1.3794, Accuracy:0.3791, Validation Loss:1.3839, Validation Accuracy:0.3678\n",
    "Epoch #162: Loss:1.3813, Accuracy:0.3754, Validation Loss:1.3800, Validation Accuracy:0.3678\n",
    "Epoch #163: Loss:1.3810, Accuracy:0.3795, Validation Loss:1.3852, Validation Accuracy:0.3777\n",
    "Epoch #164: Loss:1.3931, Accuracy:0.3836, Validation Loss:1.4025, Validation Accuracy:0.3547\n",
    "Epoch #165: Loss:1.3888, Accuracy:0.3803, Validation Loss:1.3812, Validation Accuracy:0.3744\n",
    "Epoch #166: Loss:1.3961, Accuracy:0.3758, Validation Loss:1.3918, Validation Accuracy:0.3793\n",
    "Epoch #167: Loss:1.3874, Accuracy:0.3786, Validation Loss:1.3876, Validation Accuracy:0.3727\n",
    "Epoch #168: Loss:1.3919, Accuracy:0.3786, Validation Loss:1.3801, Validation Accuracy:0.3596\n",
    "Epoch #169: Loss:1.3900, Accuracy:0.3774, Validation Loss:1.3845, Validation Accuracy:0.3826\n",
    "Epoch #170: Loss:1.3831, Accuracy:0.3791, Validation Loss:1.3834, Validation Accuracy:0.3760\n",
    "Epoch #171: Loss:1.3813, Accuracy:0.3786, Validation Loss:1.3808, Validation Accuracy:0.3760\n",
    "Epoch #172: Loss:1.3815, Accuracy:0.3811, Validation Loss:1.3806, Validation Accuracy:0.3760\n",
    "Epoch #173: Loss:1.3817, Accuracy:0.3782, Validation Loss:1.3785, Validation Accuracy:0.3662\n",
    "Epoch #174: Loss:1.3861, Accuracy:0.3774, Validation Loss:1.3966, Validation Accuracy:0.3612\n",
    "Epoch #175: Loss:1.3834, Accuracy:0.3832, Validation Loss:1.3828, Validation Accuracy:0.3777\n",
    "Epoch #176: Loss:1.3839, Accuracy:0.3799, Validation Loss:1.3782, Validation Accuracy:0.3645\n",
    "Epoch #177: Loss:1.3804, Accuracy:0.3770, Validation Loss:1.3793, Validation Accuracy:0.3695\n",
    "Epoch #178: Loss:1.3782, Accuracy:0.3782, Validation Loss:1.3790, Validation Accuracy:0.3826\n",
    "Epoch #179: Loss:1.3782, Accuracy:0.3754, Validation Loss:1.3775, Validation Accuracy:0.3727\n",
    "Epoch #180: Loss:1.3781, Accuracy:0.3791, Validation Loss:1.3803, Validation Accuracy:0.3744\n",
    "Epoch #181: Loss:1.3782, Accuracy:0.3860, Validation Loss:1.3779, Validation Accuracy:0.3744\n",
    "Epoch #182: Loss:1.3793, Accuracy:0.3840, Validation Loss:1.3790, Validation Accuracy:0.3678\n",
    "Epoch #183: Loss:1.3788, Accuracy:0.3770, Validation Loss:1.3798, Validation Accuracy:0.3711\n",
    "Epoch #184: Loss:1.3782, Accuracy:0.3840, Validation Loss:1.3814, Validation Accuracy:0.3777\n",
    "Epoch #185: Loss:1.3765, Accuracy:0.3754, Validation Loss:1.3790, Validation Accuracy:0.3810\n",
    "Epoch #186: Loss:1.3782, Accuracy:0.3758, Validation Loss:1.3767, Validation Accuracy:0.3744\n",
    "Epoch #187: Loss:1.3782, Accuracy:0.3758, Validation Loss:1.3776, Validation Accuracy:0.3793\n",
    "Epoch #188: Loss:1.3753, Accuracy:0.3778, Validation Loss:1.3768, Validation Accuracy:0.3744\n",
    "Epoch #189: Loss:1.3749, Accuracy:0.3762, Validation Loss:1.3768, Validation Accuracy:0.3760\n",
    "Epoch #190: Loss:1.3741, Accuracy:0.3807, Validation Loss:1.3771, Validation Accuracy:0.3678\n",
    "Epoch #191: Loss:1.3749, Accuracy:0.3791, Validation Loss:1.3775, Validation Accuracy:0.3695\n",
    "Epoch #192: Loss:1.3776, Accuracy:0.3774, Validation Loss:1.3765, Validation Accuracy:0.3629\n",
    "Epoch #193: Loss:1.3732, Accuracy:0.3811, Validation Loss:1.3757, Validation Accuracy:0.3711\n",
    "Epoch #194: Loss:1.3725, Accuracy:0.3823, Validation Loss:1.3759, Validation Accuracy:0.3744\n",
    "Epoch #195: Loss:1.3739, Accuracy:0.3733, Validation Loss:1.3771, Validation Accuracy:0.3777\n",
    "Epoch #196: Loss:1.3728, Accuracy:0.3807, Validation Loss:1.3780, Validation Accuracy:0.3695\n",
    "Epoch #197: Loss:1.3754, Accuracy:0.3815, Validation Loss:1.3801, Validation Accuracy:0.3678\n",
    "Epoch #198: Loss:1.3742, Accuracy:0.3799, Validation Loss:1.3881, Validation Accuracy:0.3629\n",
    "Epoch #199: Loss:1.3856, Accuracy:0.3819, Validation Loss:1.3770, Validation Accuracy:0.3777\n",
    "Epoch #200: Loss:1.3817, Accuracy:0.3741, Validation Loss:1.3907, Validation Accuracy:0.3727\n",
    "Epoch #201: Loss:1.3853, Accuracy:0.3791, Validation Loss:1.3781, Validation Accuracy:0.3727\n",
    "Epoch #202: Loss:1.3771, Accuracy:0.3786, Validation Loss:1.3784, Validation Accuracy:0.3695\n",
    "Epoch #203: Loss:1.3726, Accuracy:0.3791, Validation Loss:1.3756, Validation Accuracy:0.3711\n",
    "Epoch #204: Loss:1.3714, Accuracy:0.3791, Validation Loss:1.3751, Validation Accuracy:0.3695\n",
    "Epoch #205: Loss:1.3734, Accuracy:0.3848, Validation Loss:1.3736, Validation Accuracy:0.3727\n",
    "Epoch #206: Loss:1.3697, Accuracy:0.3795, Validation Loss:1.3818, Validation Accuracy:0.3662\n",
    "Epoch #207: Loss:1.3726, Accuracy:0.3737, Validation Loss:1.3739, Validation Accuracy:0.3678\n",
    "Epoch #208: Loss:1.3706, Accuracy:0.3815, Validation Loss:1.3769, Validation Accuracy:0.3711\n",
    "Epoch #209: Loss:1.3713, Accuracy:0.3786, Validation Loss:1.3750, Validation Accuracy:0.3711\n",
    "Epoch #210: Loss:1.3691, Accuracy:0.3762, Validation Loss:1.3749, Validation Accuracy:0.3678\n",
    "Epoch #211: Loss:1.3694, Accuracy:0.3823, Validation Loss:1.3754, Validation Accuracy:0.3580\n",
    "Epoch #212: Loss:1.3690, Accuracy:0.3864, Validation Loss:1.3771, Validation Accuracy:0.3612\n",
    "Epoch #213: Loss:1.3717, Accuracy:0.3828, Validation Loss:1.3751, Validation Accuracy:0.3711\n",
    "Epoch #214: Loss:1.3717, Accuracy:0.3795, Validation Loss:1.3809, Validation Accuracy:0.3645\n",
    "Epoch #215: Loss:1.3714, Accuracy:0.3786, Validation Loss:1.3851, Validation Accuracy:0.3695\n",
    "Epoch #216: Loss:1.3730, Accuracy:0.3733, Validation Loss:1.3775, Validation Accuracy:0.3514\n",
    "Epoch #217: Loss:1.3707, Accuracy:0.3815, Validation Loss:1.3771, Validation Accuracy:0.3612\n",
    "Epoch #218: Loss:1.3698, Accuracy:0.3877, Validation Loss:1.3733, Validation Accuracy:0.3662\n",
    "Epoch #219: Loss:1.3671, Accuracy:0.3795, Validation Loss:1.3753, Validation Accuracy:0.3645\n",
    "Epoch #220: Loss:1.3676, Accuracy:0.3856, Validation Loss:1.3737, Validation Accuracy:0.3678\n",
    "Epoch #221: Loss:1.3670, Accuracy:0.3799, Validation Loss:1.3786, Validation Accuracy:0.3629\n",
    "Epoch #222: Loss:1.3691, Accuracy:0.3791, Validation Loss:1.3829, Validation Accuracy:0.3695\n",
    "Epoch #223: Loss:1.3702, Accuracy:0.3832, Validation Loss:1.3735, Validation Accuracy:0.3695\n",
    "Epoch #224: Loss:1.3670, Accuracy:0.3815, Validation Loss:1.3812, Validation Accuracy:0.3760\n",
    "Epoch #225: Loss:1.3733, Accuracy:0.3758, Validation Loss:1.3783, Validation Accuracy:0.3645\n",
    "Epoch #226: Loss:1.3714, Accuracy:0.3708, Validation Loss:1.3874, Validation Accuracy:0.3826\n",
    "Epoch #227: Loss:1.3718, Accuracy:0.3852, Validation Loss:1.3733, Validation Accuracy:0.3629\n",
    "Epoch #228: Loss:1.3670, Accuracy:0.3791, Validation Loss:1.3803, Validation Accuracy:0.3711\n",
    "Epoch #229: Loss:1.3712, Accuracy:0.3774, Validation Loss:1.3725, Validation Accuracy:0.3629\n",
    "Epoch #230: Loss:1.3664, Accuracy:0.3844, Validation Loss:1.3774, Validation Accuracy:0.3645\n",
    "Epoch #231: Loss:1.3682, Accuracy:0.3836, Validation Loss:1.3799, Validation Accuracy:0.3563\n",
    "Epoch #232: Loss:1.3669, Accuracy:0.3807, Validation Loss:1.3732, Validation Accuracy:0.3727\n",
    "Epoch #233: Loss:1.3670, Accuracy:0.3832, Validation Loss:1.3733, Validation Accuracy:0.3727\n",
    "Epoch #234: Loss:1.3669, Accuracy:0.3819, Validation Loss:1.3735, Validation Accuracy:0.3596\n",
    "Epoch #235: Loss:1.3667, Accuracy:0.3811, Validation Loss:1.3929, Validation Accuracy:0.3596\n",
    "Epoch #236: Loss:1.3747, Accuracy:0.3836, Validation Loss:1.3724, Validation Accuracy:0.3662\n",
    "Epoch #237: Loss:1.3701, Accuracy:0.3860, Validation Loss:1.3729, Validation Accuracy:0.3695\n",
    "Epoch #238: Loss:1.3633, Accuracy:0.3807, Validation Loss:1.3716, Validation Accuracy:0.3596\n",
    "Epoch #239: Loss:1.3632, Accuracy:0.3795, Validation Loss:1.3712, Validation Accuracy:0.3695\n",
    "Epoch #240: Loss:1.3627, Accuracy:0.3844, Validation Loss:1.3735, Validation Accuracy:0.3596\n",
    "Epoch #241: Loss:1.3629, Accuracy:0.3840, Validation Loss:1.3735, Validation Accuracy:0.3563\n",
    "Epoch #242: Loss:1.3639, Accuracy:0.3852, Validation Loss:1.3713, Validation Accuracy:0.3662\n",
    "Epoch #243: Loss:1.3616, Accuracy:0.3852, Validation Loss:1.3725, Validation Accuracy:0.3662\n",
    "Epoch #244: Loss:1.3632, Accuracy:0.3864, Validation Loss:1.3919, Validation Accuracy:0.3695\n",
    "Epoch #245: Loss:1.3801, Accuracy:0.3680, Validation Loss:1.3744, Validation Accuracy:0.3629\n",
    "Epoch #246: Loss:1.3696, Accuracy:0.3725, Validation Loss:1.3838, Validation Accuracy:0.3612\n",
    "Epoch #247: Loss:1.3644, Accuracy:0.3832, Validation Loss:1.3720, Validation Accuracy:0.3645\n",
    "Epoch #248: Loss:1.3614, Accuracy:0.3799, Validation Loss:1.3755, Validation Accuracy:0.3612\n",
    "Epoch #249: Loss:1.3638, Accuracy:0.3836, Validation Loss:1.3791, Validation Accuracy:0.3662\n",
    "Epoch #250: Loss:1.3655, Accuracy:0.3815, Validation Loss:1.3826, Validation Accuracy:0.3580\n",
    "Epoch #251: Loss:1.3688, Accuracy:0.3745, Validation Loss:1.3799, Validation Accuracy:0.3662\n",
    "Epoch #252: Loss:1.3666, Accuracy:0.3815, Validation Loss:1.3707, Validation Accuracy:0.3760\n",
    "Epoch #253: Loss:1.3670, Accuracy:0.3807, Validation Loss:1.3796, Validation Accuracy:0.3563\n",
    "Epoch #254: Loss:1.3634, Accuracy:0.3852, Validation Loss:1.3725, Validation Accuracy:0.3612\n",
    "Epoch #255: Loss:1.3600, Accuracy:0.3799, Validation Loss:1.3708, Validation Accuracy:0.3678\n",
    "Epoch #256: Loss:1.3592, Accuracy:0.3782, Validation Loss:1.3693, Validation Accuracy:0.3629\n",
    "Epoch #257: Loss:1.3612, Accuracy:0.3774, Validation Loss:1.3727, Validation Accuracy:0.3563\n",
    "Epoch #258: Loss:1.3585, Accuracy:0.3877, Validation Loss:1.3754, Validation Accuracy:0.3415\n",
    "Epoch #259: Loss:1.3608, Accuracy:0.3803, Validation Loss:1.3694, Validation Accuracy:0.3629\n",
    "Epoch #260: Loss:1.3644, Accuracy:0.3803, Validation Loss:1.3720, Validation Accuracy:0.3563\n",
    "Epoch #261: Loss:1.3626, Accuracy:0.3807, Validation Loss:1.4058, Validation Accuracy:0.3448\n",
    "Epoch #262: Loss:1.3812, Accuracy:0.3696, Validation Loss:1.3713, Validation Accuracy:0.3612\n",
    "Epoch #263: Loss:1.3621, Accuracy:0.3844, Validation Loss:1.3753, Validation Accuracy:0.3662\n",
    "Epoch #264: Loss:1.3627, Accuracy:0.3848, Validation Loss:1.3825, Validation Accuracy:0.3596\n",
    "Epoch #265: Loss:1.3625, Accuracy:0.3840, Validation Loss:1.3684, Validation Accuracy:0.3596\n",
    "Epoch #266: Loss:1.3584, Accuracy:0.3852, Validation Loss:1.3835, Validation Accuracy:0.3744\n",
    "Epoch #267: Loss:1.3696, Accuracy:0.3782, Validation Loss:1.3830, Validation Accuracy:0.3563\n",
    "Epoch #268: Loss:1.3673, Accuracy:0.3828, Validation Loss:1.3785, Validation Accuracy:0.3498\n",
    "Epoch #269: Loss:1.3610, Accuracy:0.3823, Validation Loss:1.3688, Validation Accuracy:0.3662\n",
    "Epoch #270: Loss:1.3566, Accuracy:0.3799, Validation Loss:1.3688, Validation Accuracy:0.3645\n",
    "Epoch #271: Loss:1.3562, Accuracy:0.3823, Validation Loss:1.3675, Validation Accuracy:0.3645\n",
    "Epoch #272: Loss:1.3558, Accuracy:0.3852, Validation Loss:1.3694, Validation Accuracy:0.3596\n",
    "Epoch #273: Loss:1.3612, Accuracy:0.3848, Validation Loss:1.3888, Validation Accuracy:0.3695\n",
    "Epoch #274: Loss:1.3665, Accuracy:0.3803, Validation Loss:1.3729, Validation Accuracy:0.3629\n",
    "Epoch #275: Loss:1.3708, Accuracy:0.3828, Validation Loss:1.3752, Validation Accuracy:0.3727\n",
    "Epoch #276: Loss:1.3637, Accuracy:0.3897, Validation Loss:1.3772, Validation Accuracy:0.3629\n",
    "Epoch #277: Loss:1.3567, Accuracy:0.3799, Validation Loss:1.3692, Validation Accuracy:0.3612\n",
    "Epoch #278: Loss:1.3537, Accuracy:0.3823, Validation Loss:1.3674, Validation Accuracy:0.3596\n",
    "Epoch #279: Loss:1.3557, Accuracy:0.3914, Validation Loss:1.3737, Validation Accuracy:0.3662\n",
    "Epoch #280: Loss:1.3563, Accuracy:0.3840, Validation Loss:1.3712, Validation Accuracy:0.3580\n",
    "Epoch #281: Loss:1.3528, Accuracy:0.3852, Validation Loss:1.3662, Validation Accuracy:0.3645\n",
    "Epoch #282: Loss:1.3527, Accuracy:0.3856, Validation Loss:1.3664, Validation Accuracy:0.3645\n",
    "Epoch #283: Loss:1.3521, Accuracy:0.3852, Validation Loss:1.3674, Validation Accuracy:0.3629\n",
    "Epoch #284: Loss:1.3546, Accuracy:0.3799, Validation Loss:1.3669, Validation Accuracy:0.3760\n",
    "Epoch #285: Loss:1.3512, Accuracy:0.3832, Validation Loss:1.3676, Validation Accuracy:0.3596\n",
    "Epoch #286: Loss:1.3538, Accuracy:0.3836, Validation Loss:1.3655, Validation Accuracy:0.3563\n",
    "Epoch #287: Loss:1.3504, Accuracy:0.3881, Validation Loss:1.3698, Validation Accuracy:0.3596\n",
    "Epoch #288: Loss:1.3558, Accuracy:0.3815, Validation Loss:1.3683, Validation Accuracy:0.3695\n",
    "Epoch #289: Loss:1.3570, Accuracy:0.3832, Validation Loss:1.3664, Validation Accuracy:0.3596\n",
    "Epoch #290: Loss:1.3509, Accuracy:0.3823, Validation Loss:1.3674, Validation Accuracy:0.3612\n",
    "Epoch #291: Loss:1.3486, Accuracy:0.3828, Validation Loss:1.3652, Validation Accuracy:0.3612\n",
    "Epoch #292: Loss:1.3509, Accuracy:0.3860, Validation Loss:1.3671, Validation Accuracy:0.3645\n",
    "Epoch #293: Loss:1.3532, Accuracy:0.3840, Validation Loss:1.3848, Validation Accuracy:0.3645\n",
    "Epoch #294: Loss:1.3631, Accuracy:0.3737, Validation Loss:1.3696, Validation Accuracy:0.3645\n",
    "Epoch #295: Loss:1.3652, Accuracy:0.3848, Validation Loss:1.3732, Validation Accuracy:0.3530\n",
    "Epoch #296: Loss:1.3614, Accuracy:0.3836, Validation Loss:1.3868, Validation Accuracy:0.3596\n",
    "Epoch #297: Loss:1.3552, Accuracy:0.3844, Validation Loss:1.3676, Validation Accuracy:0.3629\n",
    "Epoch #298: Loss:1.3527, Accuracy:0.3840, Validation Loss:1.3643, Validation Accuracy:0.3563\n",
    "Epoch #299: Loss:1.3487, Accuracy:0.3864, Validation Loss:1.3637, Validation Accuracy:0.3612\n",
    "Epoch #300: Loss:1.3485, Accuracy:0.3893, Validation Loss:1.3755, Validation Accuracy:0.3645\n",
    "\n",
    "Test:\n",
    "Test Loss:1.37553072, Accuracy:0.3645\n",
    "Labels: ['02', '05', '01', '04', '03']\n",
    "Confusion Matrix:\n",
    "      02  05  01  04  03\n",
    "t:02  26  14  29  35  10\n",
    "t:05  11  77  43   6   5\n",
    "t:01  19  35  44  17  11\n",
    "t:04  17   3  10  48  34\n",
    "t:03  17   4  26  41  27\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.29      0.23      0.25       114\n",
    "          05       0.58      0.54      0.56       142\n",
    "          01       0.29      0.35      0.32       126\n",
    "          04       0.33      0.43      0.37       112\n",
    "          03       0.31      0.23      0.27       115\n",
    "\n",
    "    accuracy                           0.36       609\n",
    "   macro avg       0.36      0.36      0.35       609\n",
    "weighted avg       0.37      0.36      0.36       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 04:33:22 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6064515231278143, 1.6057495452304584, 1.6053967836063678, 1.6053758894868673, 1.60534262343972, 1.6053195680890764, 1.6053082774621121, 1.6053241311231465, 1.605325483727729, 1.605310267024048, 1.6052959249133156, 1.6052974985150867, 1.6052907139601182, 1.6052316959659845, 1.605210844714849, 1.6051568147192643, 1.6051179718697208, 1.60507667828076, 1.6049781129473732, 1.6048378834779236, 1.6046483052775191, 1.6044649698072662, 1.6041269877861286, 1.6036983824324333, 1.60297410907025, 1.6017918600433174, 1.5997353261719, 1.5960718215196983, 1.5893953253678696, 1.5768555612203914, 1.5551129327031779, 1.5214138009473803, 1.484537105842177, 1.4607920300197133, 1.4653440778478612, 1.4491181500831065, 1.4457480621651084, 1.4447095989202239, 1.443879271571468, 1.4413998800349745, 1.439275680308663, 1.4376688952907943, 1.4347899210668353, 1.4359611000725006, 1.4390773109614556, 1.4333366735032431, 1.4301719569611824, 1.4365240965766468, 1.439238892008714, 1.4294535088030185, 1.4306055499219346, 1.4272641256720757, 1.4335068568024534, 1.4252597631883543, 1.4249177159151225, 1.4235186559226125, 1.4391956803050927, 1.4236429195685927, 1.4221974679793434, 1.4280705861074388, 1.4277911213622696, 1.4257321676792964, 1.423660353878253, 1.4214108798695706, 1.4218786808070292, 1.4177414441147853, 1.4157940347010671, 1.4169615446444608, 1.4156040569831585, 1.4161270250044824, 1.4258730325401319, 1.4144100647646023, 1.4135843395991083, 1.4181583657836287, 1.4122340217208236, 1.4126816680670176, 1.4124339380483517, 1.412599339078017, 1.4159942973031983, 1.4128525157280156, 1.4132287531650711, 1.4096462111950703, 1.4223841849610528, 1.4132004001457703, 1.4121148874019753, 1.4119887269776443, 1.4125174511046636, 1.4193261648438051, 1.4049292037443966, 1.4072975162997818, 1.4120109631314457, 1.4051292594430482, 1.4145929308360434, 1.4035453541916971, 1.4017740354945116, 1.402278331308725, 1.4019279000402867, 1.4007477820996188, 1.4022193056804988, 1.3985951669110452, 1.3993476697768288, 1.3993158679094613, 1.3983942481684568, 1.397182291755927, 1.398318340821415, 1.4004116678864302, 1.3965108643220172, 1.3948123513771395, 1.3947192248452474, 1.3941927807671683, 1.3971940740967423, 1.392954540370133, 1.412253946897823, 1.399155255608958, 1.4114894733836107, 1.4240894196264458, 1.4000093345767368, 1.3962612482910282, 1.3896786031268893, 1.3894792885224416, 1.392959124544767, 1.3905556057082804, 1.3881768992381731, 1.3903559303440287, 1.3876356129184342, 1.3921078416122787, 1.3887456661374697, 1.3870428371899233, 1.3865310750375632, 1.3865451434954439, 1.393127757927467, 1.3897588861791175, 1.3877412891153045, 1.386219571180923, 1.3879439417756054, 1.3860417228614168, 1.3862152827784346, 1.3894587999885697, 1.3856621628324386, 1.3842826985764778, 1.3877711523146856, 1.3893058129719325, 1.3833047147650632, 1.3835451929831544, 1.3961215275653283, 1.3836816737217268, 1.3908785620737938, 1.4058364625830564, 1.3885734633271918, 1.3884537959920948, 1.38517501573453, 1.390450461353183, 1.381392075118956, 1.3817658522250422, 1.3876721203229305, 1.3810602604657753, 1.389180327283925, 1.3864316815030202, 1.3968604746318998, 1.3828036771423515, 1.383881323051766, 1.3800347980803065, 1.385214392774798, 1.402510746359238, 1.3812157336518487, 1.3917727102395545, 1.3875528969396707, 1.3800963846529255, 1.384516175548823, 1.3834170044349332, 1.3808385472383797, 1.3805817171857862, 1.3785139063896217, 1.396618791205934, 1.3827822590109162, 1.3781849036271545, 1.3793202433092842, 1.3790255187963225, 1.3775427357120857, 1.380333393861116, 1.3778947407780413, 1.3790163923366903, 1.3797591392238349, 1.3813696238403446, 1.3789977966662503, 1.3766862159879336, 1.3776374977014727, 1.3767884280685525, 1.3767573079843631, 1.3771142401718741, 1.377533870181818, 1.3764582357578872, 1.3756875970289233, 1.3758514113418379, 1.3770648716705773, 1.377954086255166, 1.3801369304923197, 1.3880610818346146, 1.3769728867291229, 1.3906929827676031, 1.3780589996300308, 1.3784305381853201, 1.3755875264091053, 1.3751331213464095, 1.373612120429479, 1.381756052007816, 1.3739197494948439, 1.3769034129645437, 1.3749563188975669, 1.3749313058915789, 1.3754127076498197, 1.377085995400089, 1.3750711630522128, 1.3809270717827558, 1.3850928036058674, 1.3775450051907443, 1.3770892565277801, 1.37333348368972, 1.375320777908726, 1.3737390589440006, 1.3786383616708966, 1.3829018654690195, 1.3734889709695024, 1.381185580552701, 1.3782757567654689, 1.3874473053050551, 1.3732624359318775, 1.38028537659418, 1.372548676672436, 1.3773887850380884, 1.379850041494385, 1.3731557864860948, 1.3732889346497008, 1.3735055391032904, 1.3929105542954945, 1.372355200191241, 1.372907056988558, 1.3716095415829437, 1.3711871885509521, 1.373458459850994, 1.3735297035505423, 1.3712736926055307, 1.3724678680423055, 1.3918680933308718, 1.3743846663113297, 1.3838301084703217, 1.371954039204101, 1.3755335598352116, 1.3790912307150454, 1.382615313349882, 1.3799475766167852, 1.3707311791543695, 1.379582351260193, 1.3725406109601601, 1.3708323618069853, 1.3692972755980217, 1.3727410712657109, 1.3754138433678789, 1.3693800289642635, 1.3720441986187337, 1.405841550020553, 1.3713009521878998, 1.375272405362873, 1.3824519472952155, 1.3684389442450111, 1.3835171095060401, 1.3830297811473728, 1.3784613233481722, 1.3687594095474394, 1.3688318146077674, 1.3674851771450198, 1.3694421933789558, 1.3888399638174398, 1.3729056660177672, 1.3752056589267525, 1.37715511975813, 1.3692362555142106, 1.3674109418599671, 1.3737364504333396, 1.3711818944057221, 1.3661599531158046, 1.366409811676038, 1.3674146473309872, 1.3669456225897878, 1.367570254798789, 1.3654836525862244, 1.369831963908692, 1.3682905680244584, 1.3664061502478588, 1.3673919834722634, 1.3651728389298388, 1.3670845973276349, 1.3847658039118074, 1.3695965255618292, 1.3731555993529572, 1.386758516183236, 1.3676197059048807, 1.3643494347241907, 1.3636518213745017, 1.375530881051751], 'val_acc': [0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.24958949094433308, 0.26600984991673376, 0.2824302112625542, 0.30049260873316935, 0.3448275844941194, 0.36781609024124584, 0.35796387359035037, 0.37602627145245743, 0.37766830767512516, 0.37274219920286794, 0.37766830777299815, 0.37766830747937924, 0.3596059097151451, 0.37438423562128165, 0.34482758459199236, 0.3481116570373278, 0.33333333191417513, 0.3464696208146601, 0.334975367451732, 0.3415435123424029, 0.3431855484671976, 0.3497536929663766, 0.36124794574206687, 0.3382594398970674, 0.3431855485650706, 0.35632183766130154, 0.33990147602186216, 0.3481116570373278, 0.3284072236376639, 0.35632183756342856, 0.35303776531383907, 0.3546798016343798, 0.36781609033911883, 0.359605910008764, 0.3497536929663766, 0.37766830738150625, 0.35303776521596614, 0.3596059097151451, 0.366174054507943, 0.3596059098130181, 0.372742199007122, 0.37438423513191676, 0.35303776560745803, 0.3825944156580175, 0.37274219881137605, 0.36124794593781284, 0.36453201828527526, 0.37602627135458444, 0.38259441585376347, 0.3842364519785582, 0.36124794593781284, 0.3579638736882233, 0.38916256045081543, 0.37274219910499495, 0.3579638737860963, 0.379310343408428, 0.366174054605816, 0.35960590991089103, 0.38259441585376347, 0.3546798015365068, 0.39080459667348316, 0.3875205243260207, 0.36781609063273774, 0.3924466327982779, 0.3546798014386338, 0.38587848810335296, 0.38587848820122594, 0.3760262712567115, 0.379310343604174, 0.3842364519785582, 0.3694581267575325, 0.379310343604174, 0.379310343506301, 0.36781609063273774, 0.37766830747937924, 0.37766830728363326, 0.3711001629802002, 0.37931034311480905, 0.38916256025506946, 0.38587848810335296, 0.38587848800547997, 0.37766830738150625, 0.36945812646391357, 0.3842364519785582, 0.36781609063273774, 0.37602627135458444, 0.3842364518806852, 0.3481116572330738, 0.38916256035294244, 0.3711001628823273, 0.3842364520764312, 0.37766830738150625, 0.37274219920286794, 0.37602627106096553, 0.372742199007122, 0.3875205243260207, 0.37274219910499495, 0.38259441585376347, 0.3809523797289687, 0.3711001628823273, 0.3760262712567115, 0.372742199007122, 0.3776683071857603, 0.37274219881137605, 0.3825944159516364, 0.37602627106096553, 0.37274219881137605, 0.3842364518806852, 0.3743842352297897, 0.36781609024124584, 0.37766830728363326, 0.37766830728363326, 0.37602627145245743, 0.3612479456441939, 0.3875205242281477, 0.38095237963109574, 0.3579638734924774, 0.3743842350340438, 0.37602627145245743, 0.3546798014386338, 0.37602627106096553, 0.38916256035294244, 0.379310343310555, 0.3743842350340438, 0.3760262711588385, 0.3711001627844543, 0.3743842349361708, 0.379310343408428, 0.3825944159516364, 0.36945812656178656, 0.3628899821604805, 0.38095237963109574, 0.36781609033911883, 0.3678160904369918, 0.37766830747937924, 0.3546798013407609, 0.3743842348382978, 0.3793103437020469, 0.37274219881137605, 0.3596059096172721, 0.3825944159516364, 0.3760262712567115, 0.37602627106096553, 0.3760262711588385, 0.3661740543121971, 0.36124794613355876, 0.37766830728363326, 0.36453201808952934, 0.36945812656178656, 0.3825944154622715, 0.37274219881137605, 0.3743842350340438, 0.3743842350340438, 0.3678160904369918, 0.37110016249083533, 0.3776683070878873, 0.38095237953322275, 0.3743842349361708, 0.37931034321268203, 0.3743842349361708, 0.37602627096309255, 0.36781609033911883, 0.36945812656178656, 0.3628899819647346, 0.3711001626865813, 0.3743842349361708, 0.3776683071857603, 0.3694581263660406, 0.3678160904369918, 0.3628899819647346, 0.37766830728363326, 0.372742199007122, 0.37274219881137605, 0.36945812665965955, 0.3711001627844543, 0.36945812665965955, 0.37274219881137605, 0.3661740542143241, 0.3678160904369918, 0.37110016249083533, 0.3711001627844543, 0.3678160904369918, 0.35796387359035037, 0.3612479456441939, 0.3711001627844543, 0.36453201808952934, 0.36945812646391357, 0.35139572909117134, 0.36124794574206687, 0.3661740542143241, 0.36453201799165635, 0.3678160905348648, 0.3628899819647346, 0.36945812665965955, 0.3694581263660406, 0.37602627086521956, 0.36453201808952934, 0.3825944155601445, 0.3628899821604805, 0.37110016249083533, 0.3628899819647346, 0.36453201808952934, 0.35632183756342856, 0.37274219871350306, 0.37274219871350306, 0.35960590991089103, 0.35960590942152615, 0.36617405441007006, 0.36945812646391357, 0.3596059097151451, 0.36945812646391357, 0.3596059097151451, 0.35632183756342856, 0.3661740543121971, 0.3661740543121971, 0.36945812656178656, 0.3628899819647346, 0.36124794593781284, 0.36453201789378337, 0.3612479460356858, 0.3661740543121971, 0.3579638733946044, 0.3661740542143241, 0.37602627086521956, 0.35632183736768264, 0.36124794574206687, 0.3678160904369918, 0.3628899817689886, 0.35632183736768264, 0.3415435123424029, 0.3628899819647346, 0.35632183736768264, 0.34482758478773834, 0.3612479455463209, 0.36617405441007006, 0.3596059096172721, 0.35960590991089103, 0.3743842349361708, 0.35632183726980965, 0.3497536929663766, 0.3661740543121971, 0.36453201808952934, 0.36453201808952934, 0.35960590991089103, 0.3694581267575325, 0.3628899818668616, 0.37274219871350306, 0.36288998167111564, 0.3612479456441939, 0.3596059096172721, 0.3661740540185781, 0.3579638734924774, 0.36453201818740233, 0.36453201808952934, 0.3628899819647346, 0.37602627096309255, 0.3596059097151451, 0.3563218374655556, 0.3596059096172721, 0.3694581263660406, 0.3596059096172721, 0.36124794583993985, 0.36124794574206687, 0.36453201799165635, 0.36453201828527526, 0.36453201818740233, 0.35303776502022016, 0.3596059096172721, 0.3628899819647346, 0.3563218374655556, 0.36124794583993985, 0.36453201808952934], 'loss': [1.6082487668834429, 1.6063712488209687, 1.605885986283085, 1.605659531714735, 1.6055632685733772, 1.6055488908560125, 1.6055599412878925, 1.605516998821705, 1.6055232683980734, 1.6055169354229248, 1.6054992269931143, 1.6054859025767207, 1.6055304815637013, 1.6054198220035623, 1.6054274369559005, 1.605359660232826, 1.6053317215897953, 1.605266972488936, 1.6051617421163915, 1.605211841643958, 1.6049320483844138, 1.6048042739441262, 1.60442465815456, 1.6041850622674523, 1.6035636068614356, 1.6026671970159856, 1.601107023286134, 1.5987981649394887, 1.5937606059060694, 1.5851616689556678, 1.5693677215850328, 1.5438268945936795, 1.5109403499587606, 1.4843482572684787, 1.472659410784132, 1.4769800618933457, 1.4672554538480065, 1.4613185935441473, 1.4587770572188454, 1.4546626284381936, 1.451792256788062, 1.4491912475601603, 1.446493717189687, 1.4481486726835278, 1.4430286786394688, 1.4415664676278523, 1.4403748502966316, 1.4396204681122327, 1.446930646945319, 1.44079651568215, 1.4378432666007008, 1.440864829214202, 1.4368413550653008, 1.434900275931466, 1.4309375852040442, 1.4302766389180992, 1.4315327127366584, 1.4350147876406598, 1.4285031980557608, 1.4285261705915542, 1.4269868188325385, 1.4391585858695561, 1.432773983062415, 1.432413633305434, 1.4317203584882512, 1.42593716488237, 1.4232019226164299, 1.42245948569241, 1.4206184864533755, 1.4205101665285333, 1.4243289715945109, 1.423964612733657, 1.4202479648394262, 1.4177516291518475, 1.421510888712607, 1.4170199562147168, 1.4152887497594469, 1.4155147808533184, 1.4175192369817464, 1.417607658256985, 1.41835949391555, 1.4163684580115568, 1.4159807637976425, 1.4183637585728075, 1.4153597241309634, 1.4163533606813183, 1.412933417071552, 1.4154504482261454, 1.41631612963745, 1.4219528839573479, 1.414066613087664, 1.4120151364093443, 1.4070012725599004, 1.4098631897011822, 1.4132153442997708, 1.4062995517523138, 1.4059610448089224, 1.4039446215365212, 1.4045304066836222, 1.4035005848510553, 1.4029204405798315, 1.4023249812194698, 1.4020521556572259, 1.4007356962873705, 1.401207138772373, 1.4014565041912166, 1.4009174105567854, 1.3994525412024903, 1.401625509722277, 1.4003268897900591, 1.3986730246573258, 1.3977197450289247, 1.4002258718380938, 1.4096869480928111, 1.4115710326533544, 1.411238963452208, 1.4107760754454062, 1.3980060007048338, 1.398373765867104, 1.3981885477747515, 1.398893805597842, 1.3949719836334917, 1.392905891455664, 1.3935791674580662, 1.3962004024145296, 1.3919214334331254, 1.3927478116151, 1.3931056765315468, 1.390306698908796, 1.3901347548075524, 1.3919067602627575, 1.3917998542041505, 1.391315340457266, 1.389078174038834, 1.3885080248423425, 1.3907724703117073, 1.388763291977759, 1.3907263343094312, 1.3885819452744, 1.3876537286770172, 1.386205179343723, 1.3860104373348323, 1.386498167284705, 1.384373691194601, 1.3850794975272929, 1.3866201226960952, 1.3874211932109857, 1.3903095161155998, 1.3972367969136952, 1.385639258625571, 1.3901705394780122, 1.3877605160648574, 1.3870154052299641, 1.384558683107521, 1.3857932689008772, 1.3827083197462486, 1.3875090455617258, 1.3869170539432971, 1.386827385792742, 1.389196318522616, 1.3793892662627987, 1.3812524576206717, 1.3810031254923074, 1.3931285213396045, 1.3888422556236784, 1.3960510360142044, 1.387417956984753, 1.391876103794795, 1.3899943473647507, 1.3830801057619726, 1.3812690301597486, 1.3814612859083641, 1.3816781801854316, 1.3860984380240313, 1.3834006441692062, 1.3839472804470963, 1.3803926039525372, 1.3782387450000833, 1.3781973119633888, 1.378090594142859, 1.378157564697814, 1.3793099685860855, 1.3788087894294785, 1.3781820712882635, 1.3765410943687328, 1.3781848789974893, 1.3782207687287849, 1.3753235809122513, 1.374890381795425, 1.374140245126258, 1.3748676758282483, 1.3776214207956679, 1.373183399353184, 1.3725329686484053, 1.3739035706255716, 1.372842772100006, 1.3754362495038543, 1.3741874847568771, 1.385613222386558, 1.38174695924567, 1.385302575021309, 1.3770621024118066, 1.3726012743474032, 1.371394114376828, 1.373426487607388, 1.3697333028429097, 1.3725823553680638, 1.370641581674376, 1.371329259921393, 1.3691478220589106, 1.369374317359141, 1.3690182457224789, 1.3716840758215965, 1.3717469969324507, 1.371398116333039, 1.3729709539570114, 1.3706797440683574, 1.36983526918188, 1.3671385139410501, 1.3676123314080053, 1.3670493069125886, 1.3691324303527144, 1.3702088064236808, 1.3670431127293643, 1.3732668327844608, 1.371413191779683, 1.3717664857664638, 1.3669923420559453, 1.3711931544407683, 1.3664117158071216, 1.3681809790080577, 1.3668557078931365, 1.3670320181875992, 1.3669012565632375, 1.3667143547559422, 1.3747031746458958, 1.3701067885823808, 1.3632711192665647, 1.3632486101048682, 1.362684232400428, 1.362923908772165, 1.363933632457036, 1.3616385636633181, 1.3631765085079341, 1.380140110305692, 1.3695649936213876, 1.3644321608592354, 1.3613920666598687, 1.3638355974788783, 1.3655393511852445, 1.3688332692308838, 1.3666022956248915, 1.3670113088169138, 1.3634133184225408, 1.3600213713714475, 1.3592329321455907, 1.3612305155035407, 1.358512090412743, 1.3608411796283917, 1.3643859993016205, 1.3626231289986956, 1.381169268042155, 1.3621242636772641, 1.3627428585498975, 1.3625340078890447, 1.3583744276720395, 1.3695966445445036, 1.3673049600706944, 1.3610109262642673, 1.3566435017869702, 1.3561710997039043, 1.3558313052512292, 1.3612232745305712, 1.3665344632871341, 1.3707993685097666, 1.3637332357665106, 1.3566790326664824, 1.353738484930943, 1.3557006350777723, 1.3563285976464743, 1.352840251599494, 1.3527431167371464, 1.3521481095887797, 1.3546475360525707, 1.3512493860060675, 1.3538357561373857, 1.3503531579364252, 1.3558228000723116, 1.3570210444119433, 1.350890289733542, 1.348565482701609, 1.35090677547259, 1.353236638887707, 1.3631258339852523, 1.3652148972791323, 1.3614328997825451, 1.3551508894201667, 1.352725334676629, 1.3486543717080808, 1.3485476816948925], 'acc': [0.2328542087112364, 0.23285421049203225, 0.23285421010037957, 0.23285421029620593, 0.2328542094945418, 0.23285420990455322, 0.2328542094945418, 0.23285420931707418, 0.23285421010037957, 0.2328542085337688, 0.2328542091028891, 0.23285420970872686, 0.23285420951290053, 0.23285420872959514, 0.23285420990455322, 0.2328542083195837, 0.2328542083195837, 0.2328542091028891, 0.23285421029620593, 0.2328542098861945, 0.23285421051039099, 0.23285420872959514, 0.23285420931707418, 0.23285420749956087, 0.2328542087112364, 0.23285420890706277, 0.23285420890706277, 0.23326488713097035, 0.2422997945143212, 0.25995893365793404, 0.27515400294405723, 0.29691991748506286, 0.30882956860491384, 0.3268993836041593, 0.32320328324237646, 0.3305954843208775, 0.33018480689863405, 0.3342915797502843, 0.3338809017038443, 0.3318275173709133, 0.3359342939432642, 0.3425051332866387, 0.34414784415057065, 0.34045174320130867, 0.34127310297817176, 0.3351129369446874, 0.3420944552401987, 0.33511293674886106, 0.34168377961710983, 0.34455852313942487, 0.3449692015408001, 0.3474332664659136, 0.34373716672832716, 0.3519507196772025, 0.3519507200688552, 0.34907597376825383, 0.3523613984702304, 0.35112936205442924, 0.3568788508982139, 0.3523613974910987, 0.35770020511850437, 0.35934291578661, 0.3474332642751063, 0.3490759733766011, 0.3478439448672888, 0.3568788514489756, 0.35770020770096433, 0.3609856258672366, 0.3613963062635927, 0.363860369585378, 0.3626283353603841, 0.36221765754648794, 0.36591375845903246, 0.3626283391178021, 0.3605749482858842, 0.3679671455702498, 0.3687885011980421, 0.3704312097120579, 0.368377823971625, 0.37453798648023506, 0.36714579232909106, 0.3667351151026739, 0.362217658721446, 0.36837782377579864, 0.36632443353135974, 0.36714578994245745, 0.3679671449827707, 0.36673511154108224, 0.3687884988481259, 0.3585215601588177, 0.3708418890925648, 0.3675564665813955, 0.3716632437412254, 0.36796714635355515, 0.36714579134995934, 0.36591375650076896, 0.37248459737403683, 0.37207392273007966, 0.37289527718291393, 0.36878849982725764, 0.3704312132369321, 0.37207392233842695, 0.3663244350979705, 0.3716632421746146, 0.3716632437412254, 0.36960985565087634, 0.37207392390503774, 0.36919917822863285, 0.36796714658609897, 0.37412731242375696, 0.37823408523868973, 0.3753593432829855, 0.3770020547343965, 0.36509240463039466, 0.3691991807743754, 0.3671457929165701, 0.3741273114079078, 0.375359341324722, 0.37002053170233534, 0.3708418881134331, 0.3749486646857839, 0.375770021292708, 0.3745379868718878, 0.3679671465493815, 0.3770020514053486, 0.3765913743747578, 0.37248460015232315, 0.37494866351082584, 0.37946611805618174, 0.3741273114079078, 0.37330595558428914, 0.3737166308157253, 0.3778234107538415, 0.37823408406373166, 0.3737166302649637, 0.3728952763996085, 0.3724845979982333, 0.38028747587478134, 0.375770019958641, 0.37577001894279183, 0.37248460171893394, 0.37905544384059475, 0.37535934445794356, 0.37453798765519314, 0.377002052776133, 0.37289527914117737, 0.3737166315990307, 0.3733059548009837, 0.37535934387046455, 0.36919917901193827, 0.3770020519561102, 0.3798767947318373, 0.3811088295443102, 0.37659137731215303, 0.37864476207345416, 0.3811088318942264, 0.3798767962617306, 0.38069815212206676, 0.3794661188762046, 0.37577002188018704, 0.37905544027900306, 0.37535934230385376, 0.37946611946368364, 0.38357289544855544, 0.3802874748956496, 0.37577001831859536, 0.37864476344423864, 0.378644764227544, 0.3774127319608136, 0.37905544384059475, 0.37864476559832844, 0.38110883013178926, 0.3782340852019723, 0.37741273000255016, 0.383162218026312, 0.37987679590679535, 0.37700205453857016, 0.37823408758860594, 0.3753593450454226, 0.37905544384059475, 0.3860369593945372, 0.3839835715000145, 0.37700205532187553, 0.3839835710716443, 0.3753593430871591, 0.3757700195302709, 0.3757700228960362, 0.37782340918723073, 0.3761806971483407, 0.38069815016380326, 0.3790554422372665, 0.37741273019837646, 0.3811088303276156, 0.3823408618110406, 0.37330595323437293, 0.38069815270954577, 0.38151950814151175, 0.37987679731429724, 0.38193018615123425, 0.37412731003712335, 0.37905544384059475, 0.3786447626609332, 0.3790554422372665, 0.3790554400831767, 0.38480492928189663, 0.3794661171137675, 0.3737166332023589, 0.38151950853316446, 0.3786447622692805, 0.3761806969525143, 0.382340862594346, 0.38644764053748126, 0.38275153942911044, 0.37946611946368364, 0.37864476598998115, 0.373305953821852, 0.3815195057915956, 0.38767967041757806, 0.37946612024698906, 0.3856262815439236, 0.37987679590679535, 0.37905543988735035, 0.38316221548056945, 0.38151950853316446, 0.3757700216843607, 0.37084189147919844, 0.3852156065450312, 0.379055443608051, 0.3774127309816819, 0.3843942514680005, 0.3835728976026453, 0.3806981523178931, 0.3831622182221383, 0.3819301857595816, 0.3811088303276156, 0.3835728964276872, 0.3860369613528007, 0.380698154312874, 0.37946611789707285, 0.38439425127217414, 0.3839835712674707, 0.38521560376674485, 0.3852156045133328, 0.38644764053748126, 0.36796714596190244, 0.3724846019514777, 0.38316221841796466, 0.37987679864836427, 0.38357289681933987, 0.3815195067707273, 0.3745379872635405, 0.3815195087289908, 0.3806981525137194, 0.38521560807492455, 0.37987679669010077, 0.37823408402701425, 0.3774127309816819, 0.3876796696709901, 0.38028747430817056, 0.3802874754831287, 0.3806981507512823, 0.36960985604252905, 0.38439425264295857, 0.384804930065202, 0.3839835716591234, 0.38521560470915917, 0.37823408719695323, 0.38275154236650566, 0.38234086396513045, 0.37987679590679535, 0.38234086455260946, 0.38521560807492455, 0.3848049288902439, 0.38028747473654073, 0.38275154040824216, 0.38973305952377635, 0.37987679669010077, 0.38234086396513045, 0.3913757717584927, 0.38398357146329704, 0.38521560513752934, 0.38562628216812006, 0.3852156079158156, 0.3798767984525379, 0.3831622172062891, 0.3835728964276872, 0.3880903496389761, 0.38151950618324826, 0.38316221544385204, 0.38234086200686696, 0.38275153844997867, 0.38603696315195524, 0.383983574045757, 0.3737166306566164, 0.38480492673615413, 0.38357289740681894, 0.38439424931391064, 0.38398357287079893, 0.38644764034165496, 0.38932238010655196]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
