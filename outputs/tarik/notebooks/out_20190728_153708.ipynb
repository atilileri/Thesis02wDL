{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf61.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 15:37:08 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'All', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eo', 'yd', 'ce', 'ek', 'ib', 'by', 'eg', 'sk', 'eb', 'sg', 'mb', 'my', 'ds', 'aa', 'ck'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000014F036A3DD8>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000014F009D7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7055, Accuracy:0.0604, Validation Loss:2.6993, Validation Accuracy:0.0936\n",
    "Epoch #2: Loss:2.6964, Accuracy:0.0846, Validation Loss:2.6908, Validation Accuracy:0.1018\n",
    "Epoch #3: Loss:2.6885, Accuracy:0.0994, Validation Loss:2.6834, Validation Accuracy:0.1133\n",
    "Epoch #4: Loss:2.6820, Accuracy:0.1088, Validation Loss:2.6776, Validation Accuracy:0.1084\n",
    "Epoch #5: Loss:2.6758, Accuracy:0.1109, Validation Loss:2.6721, Validation Accuracy:0.1199\n",
    "Epoch #6: Loss:2.6707, Accuracy:0.1207, Validation Loss:2.6666, Validation Accuracy:0.1281\n",
    "Epoch #7: Loss:2.6654, Accuracy:0.1195, Validation Loss:2.6599, Validation Accuracy:0.1199\n",
    "Epoch #8: Loss:2.6595, Accuracy:0.1133, Validation Loss:2.6533, Validation Accuracy:0.1166\n",
    "Epoch #9: Loss:2.6537, Accuracy:0.1154, Validation Loss:2.6478, Validation Accuracy:0.1330\n",
    "Epoch #10: Loss:2.6476, Accuracy:0.1409, Validation Loss:2.6391, Validation Accuracy:0.1593\n",
    "Epoch #11: Loss:2.6395, Accuracy:0.1618, Validation Loss:2.6284, Validation Accuracy:0.1691\n",
    "Epoch #12: Loss:2.6295, Accuracy:0.1634, Validation Loss:2.6160, Validation Accuracy:0.1691\n",
    "Epoch #13: Loss:2.6180, Accuracy:0.1598, Validation Loss:2.6015, Validation Accuracy:0.1708\n",
    "Epoch #14: Loss:2.6050, Accuracy:0.1634, Validation Loss:2.5842, Validation Accuracy:0.1724\n",
    "Epoch #15: Loss:2.5909, Accuracy:0.1606, Validation Loss:2.5687, Validation Accuracy:0.1708\n",
    "Epoch #16: Loss:2.5766, Accuracy:0.1614, Validation Loss:2.5528, Validation Accuracy:0.1708\n",
    "Epoch #17: Loss:2.5636, Accuracy:0.1610, Validation Loss:2.5348, Validation Accuracy:0.1724\n",
    "Epoch #18: Loss:2.5526, Accuracy:0.1573, Validation Loss:2.5233, Validation Accuracy:0.1675\n",
    "Epoch #19: Loss:2.5421, Accuracy:0.1602, Validation Loss:2.5105, Validation Accuracy:0.1741\n",
    "Epoch #20: Loss:2.5331, Accuracy:0.1598, Validation Loss:2.4978, Validation Accuracy:0.1724\n",
    "Epoch #21: Loss:2.5250, Accuracy:0.1643, Validation Loss:2.4969, Validation Accuracy:0.1724\n",
    "Epoch #22: Loss:2.5192, Accuracy:0.1630, Validation Loss:2.4869, Validation Accuracy:0.1741\n",
    "Epoch #23: Loss:2.5132, Accuracy:0.1684, Validation Loss:2.4830, Validation Accuracy:0.1757\n",
    "Epoch #24: Loss:2.5077, Accuracy:0.1688, Validation Loss:2.4751, Validation Accuracy:0.1741\n",
    "Epoch #25: Loss:2.5044, Accuracy:0.1692, Validation Loss:2.4725, Validation Accuracy:0.1691\n",
    "Epoch #26: Loss:2.5009, Accuracy:0.1696, Validation Loss:2.4630, Validation Accuracy:0.1741\n",
    "Epoch #27: Loss:2.4952, Accuracy:0.1721, Validation Loss:2.4848, Validation Accuracy:0.1708\n",
    "Epoch #28: Loss:2.5173, Accuracy:0.1602, Validation Loss:2.5342, Validation Accuracy:0.1790\n",
    "Epoch #29: Loss:2.5372, Accuracy:0.1548, Validation Loss:2.4574, Validation Accuracy:0.1790\n",
    "Epoch #30: Loss:2.5124, Accuracy:0.1598, Validation Loss:2.4756, Validation Accuracy:0.1642\n",
    "Epoch #31: Loss:2.5080, Accuracy:0.1606, Validation Loss:2.4555, Validation Accuracy:0.1741\n",
    "Epoch #32: Loss:2.4930, Accuracy:0.1745, Validation Loss:2.4697, Validation Accuracy:0.1823\n",
    "Epoch #33: Loss:2.4900, Accuracy:0.1749, Validation Loss:2.4507, Validation Accuracy:0.1741\n",
    "Epoch #34: Loss:2.4868, Accuracy:0.1680, Validation Loss:2.4473, Validation Accuracy:0.1757\n",
    "Epoch #35: Loss:2.4849, Accuracy:0.1676, Validation Loss:2.4512, Validation Accuracy:0.1757\n",
    "Epoch #36: Loss:2.4818, Accuracy:0.1762, Validation Loss:2.4471, Validation Accuracy:0.1757\n",
    "Epoch #37: Loss:2.4799, Accuracy:0.1721, Validation Loss:2.4419, Validation Accuracy:0.1741\n",
    "Epoch #38: Loss:2.4776, Accuracy:0.1713, Validation Loss:2.4412, Validation Accuracy:0.1741\n",
    "Epoch #39: Loss:2.4753, Accuracy:0.1733, Validation Loss:2.4425, Validation Accuracy:0.1741\n",
    "Epoch #40: Loss:2.4730, Accuracy:0.1737, Validation Loss:2.4385, Validation Accuracy:0.1773\n",
    "Epoch #41: Loss:2.4703, Accuracy:0.1725, Validation Loss:2.4377, Validation Accuracy:0.1823\n",
    "Epoch #42: Loss:2.4693, Accuracy:0.1762, Validation Loss:2.4365, Validation Accuracy:0.1790\n",
    "Epoch #43: Loss:2.4676, Accuracy:0.1737, Validation Loss:2.4337, Validation Accuracy:0.1806\n",
    "Epoch #44: Loss:2.4675, Accuracy:0.1745, Validation Loss:2.4340, Validation Accuracy:0.1724\n",
    "Epoch #45: Loss:2.4655, Accuracy:0.1729, Validation Loss:2.4341, Validation Accuracy:0.1741\n",
    "Epoch #46: Loss:2.4629, Accuracy:0.1770, Validation Loss:2.4301, Validation Accuracy:0.1724\n",
    "Epoch #47: Loss:2.4630, Accuracy:0.1733, Validation Loss:2.4332, Validation Accuracy:0.1757\n",
    "Epoch #48: Loss:2.4615, Accuracy:0.1749, Validation Loss:2.4291, Validation Accuracy:0.1773\n",
    "Epoch #49: Loss:2.4601, Accuracy:0.1749, Validation Loss:2.4294, Validation Accuracy:0.1757\n",
    "Epoch #50: Loss:2.4613, Accuracy:0.1762, Validation Loss:2.4273, Validation Accuracy:0.1790\n",
    "Epoch #51: Loss:2.4601, Accuracy:0.1696, Validation Loss:2.4257, Validation Accuracy:0.1757\n",
    "Epoch #52: Loss:2.4591, Accuracy:0.1713, Validation Loss:2.4291, Validation Accuracy:0.1757\n",
    "Epoch #53: Loss:2.4582, Accuracy:0.1696, Validation Loss:2.4237, Validation Accuracy:0.1757\n",
    "Epoch #54: Loss:2.4593, Accuracy:0.1725, Validation Loss:2.4265, Validation Accuracy:0.1741\n",
    "Epoch #55: Loss:2.4582, Accuracy:0.1725, Validation Loss:2.4257, Validation Accuracy:0.1757\n",
    "Epoch #56: Loss:2.4571, Accuracy:0.1692, Validation Loss:2.4274, Validation Accuracy:0.1806\n",
    "Epoch #57: Loss:2.4565, Accuracy:0.1692, Validation Loss:2.4269, Validation Accuracy:0.1741\n",
    "Epoch #58: Loss:2.4550, Accuracy:0.1745, Validation Loss:2.4270, Validation Accuracy:0.1691\n",
    "Epoch #59: Loss:2.4545, Accuracy:0.1733, Validation Loss:2.4260, Validation Accuracy:0.1724\n",
    "Epoch #60: Loss:2.4543, Accuracy:0.1737, Validation Loss:2.4258, Validation Accuracy:0.1741\n",
    "Epoch #61: Loss:2.4512, Accuracy:0.1782, Validation Loss:2.4264, Validation Accuracy:0.1856\n",
    "Epoch #62: Loss:2.4532, Accuracy:0.1729, Validation Loss:2.4277, Validation Accuracy:0.1806\n",
    "Epoch #63: Loss:2.4530, Accuracy:0.1721, Validation Loss:2.4313, Validation Accuracy:0.1790\n",
    "Epoch #64: Loss:2.4542, Accuracy:0.1745, Validation Loss:2.4250, Validation Accuracy:0.1839\n",
    "Epoch #65: Loss:2.4530, Accuracy:0.1754, Validation Loss:2.4315, Validation Accuracy:0.1757\n",
    "Epoch #66: Loss:2.4509, Accuracy:0.1749, Validation Loss:2.4248, Validation Accuracy:0.1741\n",
    "Epoch #67: Loss:2.4508, Accuracy:0.1733, Validation Loss:2.4238, Validation Accuracy:0.1773\n",
    "Epoch #68: Loss:2.4512, Accuracy:0.1733, Validation Loss:2.4236, Validation Accuracy:0.1741\n",
    "Epoch #69: Loss:2.4504, Accuracy:0.1708, Validation Loss:2.4229, Validation Accuracy:0.1839\n",
    "Epoch #70: Loss:2.4500, Accuracy:0.1725, Validation Loss:2.4233, Validation Accuracy:0.1773\n",
    "Epoch #71: Loss:2.4500, Accuracy:0.1754, Validation Loss:2.4233, Validation Accuracy:0.1757\n",
    "Epoch #72: Loss:2.4492, Accuracy:0.1713, Validation Loss:2.4220, Validation Accuracy:0.1806\n",
    "Epoch #73: Loss:2.4508, Accuracy:0.1749, Validation Loss:2.4249, Validation Accuracy:0.1839\n",
    "Epoch #74: Loss:2.4508, Accuracy:0.1721, Validation Loss:2.4323, Validation Accuracy:0.1708\n",
    "Epoch #75: Loss:2.4503, Accuracy:0.1762, Validation Loss:2.4276, Validation Accuracy:0.1757\n",
    "Epoch #76: Loss:2.4517, Accuracy:0.1721, Validation Loss:2.4263, Validation Accuracy:0.1806\n",
    "Epoch #77: Loss:2.4507, Accuracy:0.1778, Validation Loss:2.4303, Validation Accuracy:0.1790\n",
    "Epoch #78: Loss:2.4516, Accuracy:0.1696, Validation Loss:2.4266, Validation Accuracy:0.1839\n",
    "Epoch #79: Loss:2.4482, Accuracy:0.1745, Validation Loss:2.4290, Validation Accuracy:0.1790\n",
    "Epoch #80: Loss:2.4485, Accuracy:0.1729, Validation Loss:2.4265, Validation Accuracy:0.1757\n",
    "Epoch #81: Loss:2.4477, Accuracy:0.1741, Validation Loss:2.4297, Validation Accuracy:0.1773\n",
    "Epoch #82: Loss:2.4477, Accuracy:0.1708, Validation Loss:2.4305, Validation Accuracy:0.1757\n",
    "Epoch #83: Loss:2.4483, Accuracy:0.1745, Validation Loss:2.4317, Validation Accuracy:0.1741\n",
    "Epoch #84: Loss:2.4487, Accuracy:0.1749, Validation Loss:2.4332, Validation Accuracy:0.1724\n",
    "Epoch #85: Loss:2.4476, Accuracy:0.1729, Validation Loss:2.4310, Validation Accuracy:0.1708\n",
    "Epoch #86: Loss:2.4478, Accuracy:0.1729, Validation Loss:2.4311, Validation Accuracy:0.1708\n",
    "Epoch #87: Loss:2.4468, Accuracy:0.1754, Validation Loss:2.4320, Validation Accuracy:0.1790\n",
    "Epoch #88: Loss:2.4451, Accuracy:0.1778, Validation Loss:2.4271, Validation Accuracy:0.1741\n",
    "Epoch #89: Loss:2.4454, Accuracy:0.1721, Validation Loss:2.4260, Validation Accuracy:0.1724\n",
    "Epoch #90: Loss:2.4451, Accuracy:0.1704, Validation Loss:2.4306, Validation Accuracy:0.1724\n",
    "Epoch #91: Loss:2.4464, Accuracy:0.1729, Validation Loss:2.4260, Validation Accuracy:0.1741\n",
    "Epoch #92: Loss:2.4457, Accuracy:0.1733, Validation Loss:2.4320, Validation Accuracy:0.1806\n",
    "Epoch #93: Loss:2.4646, Accuracy:0.1688, Validation Loss:2.4425, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4665, Accuracy:0.1745, Validation Loss:2.4584, Validation Accuracy:0.1806\n",
    "Epoch #95: Loss:2.4555, Accuracy:0.1758, Validation Loss:2.4400, Validation Accuracy:0.1823\n",
    "Epoch #96: Loss:2.4540, Accuracy:0.1717, Validation Loss:2.4379, Validation Accuracy:0.1839\n",
    "Epoch #97: Loss:2.4509, Accuracy:0.1745, Validation Loss:2.4920, Validation Accuracy:0.1445\n",
    "Epoch #98: Loss:2.4666, Accuracy:0.1663, Validation Loss:2.4393, Validation Accuracy:0.1691\n",
    "Epoch #99: Loss:2.4649, Accuracy:0.1741, Validation Loss:2.4323, Validation Accuracy:0.1741\n",
    "Epoch #100: Loss:2.4514, Accuracy:0.1717, Validation Loss:2.4465, Validation Accuracy:0.1626\n",
    "Epoch #101: Loss:2.4528, Accuracy:0.1745, Validation Loss:2.4325, Validation Accuracy:0.1741\n",
    "Epoch #102: Loss:2.4510, Accuracy:0.1676, Validation Loss:2.4303, Validation Accuracy:0.1757\n",
    "Epoch #103: Loss:2.4480, Accuracy:0.1729, Validation Loss:2.4329, Validation Accuracy:0.1741\n",
    "Epoch #104: Loss:2.4467, Accuracy:0.1758, Validation Loss:2.4340, Validation Accuracy:0.1658\n",
    "Epoch #105: Loss:2.4449, Accuracy:0.1782, Validation Loss:2.4271, Validation Accuracy:0.1741\n",
    "Epoch #106: Loss:2.4446, Accuracy:0.1733, Validation Loss:2.4276, Validation Accuracy:0.1741\n",
    "Epoch #107: Loss:2.4430, Accuracy:0.1758, Validation Loss:2.4295, Validation Accuracy:0.1773\n",
    "Epoch #108: Loss:2.4441, Accuracy:0.1729, Validation Loss:2.4290, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.4434, Accuracy:0.1704, Validation Loss:2.4254, Validation Accuracy:0.1823\n",
    "Epoch #110: Loss:2.4444, Accuracy:0.1713, Validation Loss:2.4266, Validation Accuracy:0.1757\n",
    "Epoch #111: Loss:2.4437, Accuracy:0.1770, Validation Loss:2.4278, Validation Accuracy:0.1708\n",
    "Epoch #112: Loss:2.4440, Accuracy:0.1725, Validation Loss:2.4261, Validation Accuracy:0.1806\n",
    "Epoch #113: Loss:2.4430, Accuracy:0.1745, Validation Loss:2.4269, Validation Accuracy:0.1806\n",
    "Epoch #114: Loss:2.4435, Accuracy:0.1754, Validation Loss:2.4291, Validation Accuracy:0.1691\n",
    "Epoch #115: Loss:2.4417, Accuracy:0.1762, Validation Loss:2.4246, Validation Accuracy:0.1806\n",
    "Epoch #116: Loss:2.4414, Accuracy:0.1745, Validation Loss:2.4264, Validation Accuracy:0.1872\n",
    "Epoch #117: Loss:2.4411, Accuracy:0.1741, Validation Loss:2.4285, Validation Accuracy:0.1757\n",
    "Epoch #118: Loss:2.4402, Accuracy:0.1774, Validation Loss:2.4261, Validation Accuracy:0.1773\n",
    "Epoch #119: Loss:2.4402, Accuracy:0.1754, Validation Loss:2.4281, Validation Accuracy:0.1790\n",
    "Epoch #120: Loss:2.4401, Accuracy:0.1737, Validation Loss:2.4289, Validation Accuracy:0.1823\n",
    "Epoch #121: Loss:2.4402, Accuracy:0.1770, Validation Loss:2.4279, Validation Accuracy:0.1806\n",
    "Epoch #122: Loss:2.4396, Accuracy:0.1745, Validation Loss:2.4277, Validation Accuracy:0.1790\n",
    "Epoch #123: Loss:2.4398, Accuracy:0.1766, Validation Loss:2.4285, Validation Accuracy:0.1741\n",
    "Epoch #124: Loss:2.4399, Accuracy:0.1774, Validation Loss:2.4299, Validation Accuracy:0.1806\n",
    "Epoch #125: Loss:2.4397, Accuracy:0.1786, Validation Loss:2.4286, Validation Accuracy:0.1806\n",
    "Epoch #126: Loss:2.4394, Accuracy:0.1754, Validation Loss:2.4281, Validation Accuracy:0.1790\n",
    "Epoch #127: Loss:2.4391, Accuracy:0.1729, Validation Loss:2.4296, Validation Accuracy:0.1823\n",
    "Epoch #128: Loss:2.4390, Accuracy:0.1770, Validation Loss:2.4282, Validation Accuracy:0.1823\n",
    "Epoch #129: Loss:2.4399, Accuracy:0.1721, Validation Loss:2.4272, Validation Accuracy:0.1773\n",
    "Epoch #130: Loss:2.4389, Accuracy:0.1766, Validation Loss:2.4304, Validation Accuracy:0.1773\n",
    "Epoch #131: Loss:2.4391, Accuracy:0.1758, Validation Loss:2.4283, Validation Accuracy:0.1757\n",
    "Epoch #132: Loss:2.4391, Accuracy:0.1770, Validation Loss:2.4295, Validation Accuracy:0.1888\n",
    "Epoch #133: Loss:2.4390, Accuracy:0.1791, Validation Loss:2.4269, Validation Accuracy:0.1806\n",
    "Epoch #134: Loss:2.4377, Accuracy:0.1754, Validation Loss:2.4295, Validation Accuracy:0.1806\n",
    "Epoch #135: Loss:2.4378, Accuracy:0.1758, Validation Loss:2.4300, Validation Accuracy:0.1658\n",
    "Epoch #136: Loss:2.4384, Accuracy:0.1811, Validation Loss:2.4280, Validation Accuracy:0.1593\n",
    "Epoch #137: Loss:2.4375, Accuracy:0.1762, Validation Loss:2.4294, Validation Accuracy:0.1626\n",
    "Epoch #138: Loss:2.4375, Accuracy:0.1766, Validation Loss:2.4297, Validation Accuracy:0.1839\n",
    "Epoch #139: Loss:2.4374, Accuracy:0.1774, Validation Loss:2.4281, Validation Accuracy:0.1790\n",
    "Epoch #140: Loss:2.4368, Accuracy:0.1774, Validation Loss:2.4296, Validation Accuracy:0.1757\n",
    "Epoch #141: Loss:2.4377, Accuracy:0.1729, Validation Loss:2.4283, Validation Accuracy:0.1658\n",
    "Epoch #142: Loss:2.4370, Accuracy:0.1766, Validation Loss:2.4311, Validation Accuracy:0.1741\n",
    "Epoch #143: Loss:2.4371, Accuracy:0.1786, Validation Loss:2.4288, Validation Accuracy:0.1675\n",
    "Epoch #144: Loss:2.4370, Accuracy:0.1803, Validation Loss:2.4301, Validation Accuracy:0.1675\n",
    "Epoch #145: Loss:2.4369, Accuracy:0.1774, Validation Loss:2.4304, Validation Accuracy:0.1691\n",
    "Epoch #146: Loss:2.4376, Accuracy:0.1721, Validation Loss:2.4285, Validation Accuracy:0.1642\n",
    "Epoch #147: Loss:2.4360, Accuracy:0.1749, Validation Loss:2.4313, Validation Accuracy:0.1691\n",
    "Epoch #148: Loss:2.4373, Accuracy:0.1795, Validation Loss:2.4292, Validation Accuracy:0.1658\n",
    "Epoch #149: Loss:2.4374, Accuracy:0.1807, Validation Loss:2.4285, Validation Accuracy:0.1675\n",
    "Epoch #150: Loss:2.4369, Accuracy:0.1745, Validation Loss:2.4308, Validation Accuracy:0.1724\n",
    "Epoch #151: Loss:2.4372, Accuracy:0.1840, Validation Loss:2.4279, Validation Accuracy:0.1741\n",
    "Epoch #152: Loss:2.4362, Accuracy:0.1803, Validation Loss:2.4287, Validation Accuracy:0.1708\n",
    "Epoch #153: Loss:2.4366, Accuracy:0.1778, Validation Loss:2.4282, Validation Accuracy:0.1642\n",
    "Epoch #154: Loss:2.4362, Accuracy:0.1786, Validation Loss:2.4278, Validation Accuracy:0.1708\n",
    "Epoch #155: Loss:2.4367, Accuracy:0.1832, Validation Loss:2.4294, Validation Accuracy:0.1773\n",
    "Epoch #156: Loss:2.4357, Accuracy:0.1795, Validation Loss:2.4273, Validation Accuracy:0.1741\n",
    "Epoch #157: Loss:2.4362, Accuracy:0.1799, Validation Loss:2.4284, Validation Accuracy:0.1741\n",
    "Epoch #158: Loss:2.4357, Accuracy:0.1786, Validation Loss:2.4289, Validation Accuracy:0.1708\n",
    "Epoch #159: Loss:2.4359, Accuracy:0.1803, Validation Loss:2.4274, Validation Accuracy:0.1724\n",
    "Epoch #160: Loss:2.4359, Accuracy:0.1799, Validation Loss:2.4288, Validation Accuracy:0.1708\n",
    "Epoch #161: Loss:2.4363, Accuracy:0.1782, Validation Loss:2.4308, Validation Accuracy:0.1757\n",
    "Epoch #162: Loss:2.4361, Accuracy:0.1873, Validation Loss:2.4270, Validation Accuracy:0.1708\n",
    "Epoch #163: Loss:2.4358, Accuracy:0.1815, Validation Loss:2.4298, Validation Accuracy:0.1757\n",
    "Epoch #164: Loss:2.4361, Accuracy:0.1828, Validation Loss:2.4290, Validation Accuracy:0.1708\n",
    "Epoch #165: Loss:2.4353, Accuracy:0.1786, Validation Loss:2.4274, Validation Accuracy:0.1724\n",
    "Epoch #166: Loss:2.4351, Accuracy:0.1811, Validation Loss:2.4296, Validation Accuracy:0.1806\n",
    "Epoch #167: Loss:2.4354, Accuracy:0.1799, Validation Loss:2.4282, Validation Accuracy:0.1757\n",
    "Epoch #168: Loss:2.4350, Accuracy:0.1815, Validation Loss:2.4282, Validation Accuracy:0.1757\n",
    "Epoch #169: Loss:2.4350, Accuracy:0.1803, Validation Loss:2.4294, Validation Accuracy:0.1724\n",
    "Epoch #170: Loss:2.4354, Accuracy:0.1803, Validation Loss:2.4294, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4342, Accuracy:0.1819, Validation Loss:2.4294, Validation Accuracy:0.1724\n",
    "Epoch #172: Loss:2.4349, Accuracy:0.1803, Validation Loss:2.4272, Validation Accuracy:0.1741\n",
    "Epoch #173: Loss:2.4353, Accuracy:0.1828, Validation Loss:2.4290, Validation Accuracy:0.1790\n",
    "Epoch #174: Loss:2.4346, Accuracy:0.1832, Validation Loss:2.4285, Validation Accuracy:0.1741\n",
    "Epoch #175: Loss:2.4343, Accuracy:0.1815, Validation Loss:2.4276, Validation Accuracy:0.1708\n",
    "Epoch #176: Loss:2.4340, Accuracy:0.1811, Validation Loss:2.4296, Validation Accuracy:0.1757\n",
    "Epoch #177: Loss:2.4344, Accuracy:0.1823, Validation Loss:2.4289, Validation Accuracy:0.1708\n",
    "Epoch #178: Loss:2.4338, Accuracy:0.1815, Validation Loss:2.4281, Validation Accuracy:0.1724\n",
    "Epoch #179: Loss:2.4341, Accuracy:0.1823, Validation Loss:2.4290, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.4356, Accuracy:0.1823, Validation Loss:2.4303, Validation Accuracy:0.1757\n",
    "Epoch #181: Loss:2.4351, Accuracy:0.1799, Validation Loss:2.4272, Validation Accuracy:0.1708\n",
    "Epoch #182: Loss:2.4342, Accuracy:0.1828, Validation Loss:2.4311, Validation Accuracy:0.1773\n",
    "Epoch #183: Loss:2.4343, Accuracy:0.1819, Validation Loss:2.4286, Validation Accuracy:0.1691\n",
    "Epoch #184: Loss:2.4344, Accuracy:0.1819, Validation Loss:2.4276, Validation Accuracy:0.1724\n",
    "Epoch #185: Loss:2.4332, Accuracy:0.1807, Validation Loss:2.4307, Validation Accuracy:0.1757\n",
    "Epoch #186: Loss:2.4339, Accuracy:0.1823, Validation Loss:2.4285, Validation Accuracy:0.1724\n",
    "Epoch #187: Loss:2.4334, Accuracy:0.1832, Validation Loss:2.4291, Validation Accuracy:0.1724\n",
    "Epoch #188: Loss:2.4342, Accuracy:0.1819, Validation Loss:2.4281, Validation Accuracy:0.1741\n",
    "Epoch #189: Loss:2.4331, Accuracy:0.1832, Validation Loss:2.4317, Validation Accuracy:0.1757\n",
    "Epoch #190: Loss:2.4330, Accuracy:0.1864, Validation Loss:2.4284, Validation Accuracy:0.1724\n",
    "Epoch #191: Loss:2.4344, Accuracy:0.1828, Validation Loss:2.4285, Validation Accuracy:0.1724\n",
    "Epoch #192: Loss:2.4338, Accuracy:0.1819, Validation Loss:2.4334, Validation Accuracy:0.1741\n",
    "Epoch #193: Loss:2.4340, Accuracy:0.1762, Validation Loss:2.4274, Validation Accuracy:0.1708\n",
    "Epoch #194: Loss:2.4329, Accuracy:0.1811, Validation Loss:2.4302, Validation Accuracy:0.1790\n",
    "Epoch #195: Loss:2.4329, Accuracy:0.1836, Validation Loss:2.4300, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4334, Accuracy:0.1807, Validation Loss:2.4290, Validation Accuracy:0.1773\n",
    "Epoch #197: Loss:2.4327, Accuracy:0.1819, Validation Loss:2.4296, Validation Accuracy:0.1773\n",
    "Epoch #198: Loss:2.4342, Accuracy:0.1848, Validation Loss:2.4322, Validation Accuracy:0.1741\n",
    "Epoch #199: Loss:2.4331, Accuracy:0.1828, Validation Loss:2.4277, Validation Accuracy:0.1675\n",
    "Epoch #200: Loss:2.4342, Accuracy:0.1807, Validation Loss:2.4322, Validation Accuracy:0.1757\n",
    "Epoch #201: Loss:2.4325, Accuracy:0.1823, Validation Loss:2.4291, Validation Accuracy:0.1724\n",
    "Epoch #202: Loss:2.4328, Accuracy:0.1815, Validation Loss:2.4285, Validation Accuracy:0.1724\n",
    "Epoch #203: Loss:2.4326, Accuracy:0.1811, Validation Loss:2.4307, Validation Accuracy:0.1757\n",
    "Epoch #204: Loss:2.4318, Accuracy:0.1782, Validation Loss:2.4284, Validation Accuracy:0.1724\n",
    "Epoch #205: Loss:2.4324, Accuracy:0.1828, Validation Loss:2.4286, Validation Accuracy:0.1724\n",
    "Epoch #206: Loss:2.4317, Accuracy:0.1832, Validation Loss:2.4307, Validation Accuracy:0.1790\n",
    "Epoch #207: Loss:2.4322, Accuracy:0.1840, Validation Loss:2.4291, Validation Accuracy:0.1691\n",
    "Epoch #208: Loss:2.4325, Accuracy:0.1799, Validation Loss:2.4299, Validation Accuracy:0.1757\n",
    "Epoch #209: Loss:2.4338, Accuracy:0.1803, Validation Loss:2.4330, Validation Accuracy:0.1757\n",
    "Epoch #210: Loss:2.4322, Accuracy:0.1807, Validation Loss:2.4278, Validation Accuracy:0.1757\n",
    "Epoch #211: Loss:2.4332, Accuracy:0.1819, Validation Loss:2.4311, Validation Accuracy:0.1790\n",
    "Epoch #212: Loss:2.4315, Accuracy:0.1848, Validation Loss:2.4296, Validation Accuracy:0.1773\n",
    "Epoch #213: Loss:2.4315, Accuracy:0.1844, Validation Loss:2.4305, Validation Accuracy:0.1790\n",
    "Epoch #214: Loss:2.4316, Accuracy:0.1811, Validation Loss:2.4301, Validation Accuracy:0.1658\n",
    "Epoch #215: Loss:2.4312, Accuracy:0.1832, Validation Loss:2.4302, Validation Accuracy:0.1757\n",
    "Epoch #216: Loss:2.4312, Accuracy:0.1832, Validation Loss:2.4305, Validation Accuracy:0.1773\n",
    "Epoch #217: Loss:2.4318, Accuracy:0.1856, Validation Loss:2.4287, Validation Accuracy:0.1773\n",
    "Epoch #218: Loss:2.4308, Accuracy:0.1836, Validation Loss:2.4319, Validation Accuracy:0.1790\n",
    "Epoch #219: Loss:2.4310, Accuracy:0.1828, Validation Loss:2.4306, Validation Accuracy:0.1773\n",
    "Epoch #220: Loss:2.4315, Accuracy:0.1807, Validation Loss:2.4288, Validation Accuracy:0.1724\n",
    "Epoch #221: Loss:2.4308, Accuracy:0.1819, Validation Loss:2.4317, Validation Accuracy:0.1773\n",
    "Epoch #222: Loss:2.4315, Accuracy:0.1828, Validation Loss:2.4326, Validation Accuracy:0.1823\n",
    "Epoch #223: Loss:2.4309, Accuracy:0.1852, Validation Loss:2.4292, Validation Accuracy:0.1724\n",
    "Epoch #224: Loss:2.4307, Accuracy:0.1823, Validation Loss:2.4309, Validation Accuracy:0.1724\n",
    "Epoch #225: Loss:2.4307, Accuracy:0.1819, Validation Loss:2.4316, Validation Accuracy:0.1708\n",
    "Epoch #226: Loss:2.4305, Accuracy:0.1819, Validation Loss:2.4297, Validation Accuracy:0.1757\n",
    "Epoch #227: Loss:2.4304, Accuracy:0.1860, Validation Loss:2.4316, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.4307, Accuracy:0.1836, Validation Loss:2.4309, Validation Accuracy:0.1642\n",
    "Epoch #229: Loss:2.4305, Accuracy:0.1840, Validation Loss:2.4305, Validation Accuracy:0.1741\n",
    "Epoch #230: Loss:2.4327, Accuracy:0.1815, Validation Loss:2.4344, Validation Accuracy:0.1741\n",
    "Epoch #231: Loss:2.4306, Accuracy:0.1823, Validation Loss:2.4290, Validation Accuracy:0.1724\n",
    "Epoch #232: Loss:2.4304, Accuracy:0.1852, Validation Loss:2.4324, Validation Accuracy:0.1708\n",
    "Epoch #233: Loss:2.4304, Accuracy:0.1803, Validation Loss:2.4323, Validation Accuracy:0.1691\n",
    "Epoch #234: Loss:2.4300, Accuracy:0.1844, Validation Loss:2.4295, Validation Accuracy:0.1757\n",
    "Epoch #235: Loss:2.4302, Accuracy:0.1848, Validation Loss:2.4315, Validation Accuracy:0.1823\n",
    "Epoch #236: Loss:2.4312, Accuracy:0.1864, Validation Loss:2.4319, Validation Accuracy:0.1675\n",
    "Epoch #237: Loss:2.4297, Accuracy:0.1803, Validation Loss:2.4320, Validation Accuracy:0.1806\n",
    "Epoch #238: Loss:2.4295, Accuracy:0.1832, Validation Loss:2.4305, Validation Accuracy:0.1757\n",
    "Epoch #239: Loss:2.4295, Accuracy:0.1836, Validation Loss:2.4310, Validation Accuracy:0.1757\n",
    "Epoch #240: Loss:2.4295, Accuracy:0.1828, Validation Loss:2.4324, Validation Accuracy:0.1658\n",
    "Epoch #241: Loss:2.4297, Accuracy:0.1807, Validation Loss:2.4324, Validation Accuracy:0.1757\n",
    "Epoch #242: Loss:2.4291, Accuracy:0.1864, Validation Loss:2.4306, Validation Accuracy:0.1741\n",
    "Epoch #243: Loss:2.4297, Accuracy:0.1836, Validation Loss:2.4318, Validation Accuracy:0.1675\n",
    "Epoch #244: Loss:2.4294, Accuracy:0.1856, Validation Loss:2.4310, Validation Accuracy:0.1757\n",
    "Epoch #245: Loss:2.4293, Accuracy:0.1848, Validation Loss:2.4309, Validation Accuracy:0.1757\n",
    "Epoch #246: Loss:2.4293, Accuracy:0.1832, Validation Loss:2.4325, Validation Accuracy:0.1741\n",
    "Epoch #247: Loss:2.4290, Accuracy:0.1848, Validation Loss:2.4323, Validation Accuracy:0.1708\n",
    "Epoch #248: Loss:2.4290, Accuracy:0.1852, Validation Loss:2.4321, Validation Accuracy:0.1757\n",
    "Epoch #249: Loss:2.4293, Accuracy:0.1836, Validation Loss:2.4305, Validation Accuracy:0.1724\n",
    "Epoch #250: Loss:2.4288, Accuracy:0.1819, Validation Loss:2.4330, Validation Accuracy:0.1691\n",
    "Epoch #251: Loss:2.4291, Accuracy:0.1848, Validation Loss:2.4316, Validation Accuracy:0.1658\n",
    "Epoch #252: Loss:2.4293, Accuracy:0.1860, Validation Loss:2.4310, Validation Accuracy:0.1724\n",
    "Epoch #253: Loss:2.4291, Accuracy:0.1832, Validation Loss:2.4339, Validation Accuracy:0.1790\n",
    "Epoch #254: Loss:2.4292, Accuracy:0.1819, Validation Loss:2.4312, Validation Accuracy:0.1757\n",
    "Epoch #255: Loss:2.4289, Accuracy:0.1828, Validation Loss:2.4346, Validation Accuracy:0.1806\n",
    "Epoch #256: Loss:2.4292, Accuracy:0.1844, Validation Loss:2.4335, Validation Accuracy:0.1675\n",
    "Epoch #257: Loss:2.4280, Accuracy:0.1832, Validation Loss:2.4317, Validation Accuracy:0.1724\n",
    "Epoch #258: Loss:2.4284, Accuracy:0.1864, Validation Loss:2.4327, Validation Accuracy:0.1773\n",
    "Epoch #259: Loss:2.4304, Accuracy:0.1856, Validation Loss:2.4315, Validation Accuracy:0.1724\n",
    "Epoch #260: Loss:2.4287, Accuracy:0.1823, Validation Loss:2.4358, Validation Accuracy:0.1741\n",
    "Epoch #261: Loss:2.4284, Accuracy:0.1832, Validation Loss:2.4306, Validation Accuracy:0.1691\n",
    "Epoch #262: Loss:2.4290, Accuracy:0.1828, Validation Loss:2.4318, Validation Accuracy:0.1642\n",
    "Epoch #263: Loss:2.4288, Accuracy:0.1840, Validation Loss:2.4346, Validation Accuracy:0.1806\n",
    "Epoch #264: Loss:2.4280, Accuracy:0.1819, Validation Loss:2.4320, Validation Accuracy:0.1658\n",
    "Epoch #265: Loss:2.4279, Accuracy:0.1856, Validation Loss:2.4343, Validation Accuracy:0.1741\n",
    "Epoch #266: Loss:2.4279, Accuracy:0.1848, Validation Loss:2.4324, Validation Accuracy:0.1790\n",
    "Epoch #267: Loss:2.4279, Accuracy:0.1873, Validation Loss:2.4330, Validation Accuracy:0.1790\n",
    "Epoch #268: Loss:2.4282, Accuracy:0.1803, Validation Loss:2.4332, Validation Accuracy:0.1609\n",
    "Epoch #269: Loss:2.4277, Accuracy:0.1844, Validation Loss:2.4328, Validation Accuracy:0.1757\n",
    "Epoch #270: Loss:2.4283, Accuracy:0.1864, Validation Loss:2.4338, Validation Accuracy:0.1823\n",
    "Epoch #271: Loss:2.4278, Accuracy:0.1840, Validation Loss:2.4328, Validation Accuracy:0.1675\n",
    "Epoch #272: Loss:2.4285, Accuracy:0.1823, Validation Loss:2.4316, Validation Accuracy:0.1724\n",
    "Epoch #273: Loss:2.4275, Accuracy:0.1869, Validation Loss:2.4362, Validation Accuracy:0.1790\n",
    "Epoch #274: Loss:2.4280, Accuracy:0.1873, Validation Loss:2.4323, Validation Accuracy:0.1642\n",
    "Epoch #275: Loss:2.4271, Accuracy:0.1864, Validation Loss:2.4340, Validation Accuracy:0.1757\n",
    "Epoch #276: Loss:2.4280, Accuracy:0.1864, Validation Loss:2.4340, Validation Accuracy:0.1757\n",
    "Epoch #277: Loss:2.4275, Accuracy:0.1860, Validation Loss:2.4327, Validation Accuracy:0.1658\n",
    "Epoch #278: Loss:2.4281, Accuracy:0.1848, Validation Loss:2.4335, Validation Accuracy:0.1691\n",
    "Epoch #279: Loss:2.4278, Accuracy:0.1873, Validation Loss:2.4380, Validation Accuracy:0.1790\n",
    "Epoch #280: Loss:2.4265, Accuracy:0.1922, Validation Loss:2.4310, Validation Accuracy:0.1691\n",
    "Epoch #281: Loss:2.4278, Accuracy:0.1840, Validation Loss:2.4334, Validation Accuracy:0.1741\n",
    "Epoch #282: Loss:2.4279, Accuracy:0.1860, Validation Loss:2.4367, Validation Accuracy:0.1708\n",
    "Epoch #283: Loss:2.4276, Accuracy:0.1840, Validation Loss:2.4317, Validation Accuracy:0.1691\n",
    "Epoch #284: Loss:2.4266, Accuracy:0.1852, Validation Loss:2.4344, Validation Accuracy:0.1724\n",
    "Epoch #285: Loss:2.4271, Accuracy:0.1860, Validation Loss:2.4348, Validation Accuracy:0.1757\n",
    "Epoch #286: Loss:2.4276, Accuracy:0.1819, Validation Loss:2.4328, Validation Accuracy:0.1642\n",
    "Epoch #287: Loss:2.4268, Accuracy:0.1856, Validation Loss:2.4354, Validation Accuracy:0.1724\n",
    "Epoch #288: Loss:2.4266, Accuracy:0.1852, Validation Loss:2.4334, Validation Accuracy:0.1724\n",
    "Epoch #289: Loss:2.4276, Accuracy:0.1828, Validation Loss:2.4324, Validation Accuracy:0.1691\n",
    "Epoch #290: Loss:2.4263, Accuracy:0.1852, Validation Loss:2.4371, Validation Accuracy:0.1823\n",
    "Epoch #291: Loss:2.4266, Accuracy:0.1840, Validation Loss:2.4323, Validation Accuracy:0.1691\n",
    "Epoch #292: Loss:2.4274, Accuracy:0.1828, Validation Loss:2.4332, Validation Accuracy:0.1642\n",
    "Epoch #293: Loss:2.4256, Accuracy:0.1852, Validation Loss:2.4356, Validation Accuracy:0.1724\n",
    "Epoch #294: Loss:2.4265, Accuracy:0.1864, Validation Loss:2.4333, Validation Accuracy:0.1642\n",
    "Epoch #295: Loss:2.4259, Accuracy:0.1852, Validation Loss:2.4339, Validation Accuracy:0.1757\n",
    "Epoch #296: Loss:2.4261, Accuracy:0.1877, Validation Loss:2.4337, Validation Accuracy:0.1790\n",
    "Epoch #297: Loss:2.4263, Accuracy:0.1844, Validation Loss:2.4338, Validation Accuracy:0.1757\n",
    "Epoch #298: Loss:2.4253, Accuracy:0.1848, Validation Loss:2.4331, Validation Accuracy:0.1691\n",
    "Epoch #299: Loss:2.4266, Accuracy:0.1860, Validation Loss:2.4350, Validation Accuracy:0.1724\n",
    "Epoch #300: Loss:2.4274, Accuracy:0.1856, Validation Loss:2.4376, Validation Accuracy:0.1741\n",
    "\n",
    "Test:\n",
    "Test Loss:2.43763447, Accuracy:0.1741\n",
    "Labels: ['eo', 'yd', 'ce', 'ek', 'ib', 'by', 'eg', 'sk', 'eb', 'sg', 'mb', 'my', 'ds', 'aa', 'ck']\n",
    "Confusion Matrix:\n",
    "      eo  yd  ce  ek  ib  by  eg  sk  eb  sg  mb  my  ds  aa  ck\n",
    "t:eo   5   4   0   0   1   8   4   0   0  12   0   0   0   0   0\n",
    "t:yd   8  25   0   0  14   1   2   0   0  12   0   0   0   0   0\n",
    "t:ce   4   1   0   0   2   1   9   0   2   7   0   0   1   0   0\n",
    "t:ek   5   4   0   0   0   4  16   0   2  16   0   0   1   0   0\n",
    "t:ib   2  17   0   0   9   2   4   0   1  19   0   0   0   0   0\n",
    "t:by   5   0   0   0   1   8  10   0   2  13   0   0   1   0   0\n",
    "t:eg   1   0   0   0   0   6  28   0   3   4   0   0   8   0   0\n",
    "t:sk   0   2   0   0   1   5  11   0   1  10   0   0   3   0   0\n",
    "t:eb   5   4   0   0   1   6  19   0   2  10   0   0   3   0   0\n",
    "t:sg   6   4   0   0   8   3   7   0   1  22   0   0   0   0   0\n",
    "t:mb   5   7   0   0   2  10  11   0   3  13   0   0   1   0   0\n",
    "t:my   2   3   0   0   2   2   8   0   0   3   0   0   0   0   0\n",
    "t:ds   0   0   0   0   0   2  17   0   3   2   0   0   7   0   0\n",
    "t:aa   1   1   0   0   2   5  18   0   1   1   0   0   5   0   0\n",
    "t:ck   0   1   0   0   0   6  15   0   1   0   0   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eo       0.10      0.15      0.12        34\n",
    "          yd       0.34      0.40      0.37        62\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          ib       0.21      0.17      0.19        54\n",
    "          by       0.12      0.20      0.15        40\n",
    "          eg       0.16      0.56      0.24        50\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eb       0.09      0.04      0.06        50\n",
    "          sg       0.15      0.43      0.23        51\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ds       0.23      0.23      0.23        31\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ck       0.00      0.00      0.00        23\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.09      0.14      0.11       609\n",
    "weighted avg       0.11      0.17      0.13       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 16:17:44 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6992943220342127, 2.6908180373055592, 2.6833536237331446, 2.6775957105194994, 2.6720945345748626, 2.6665526238959805, 2.6598926725841703, 2.6532900274680755, 2.6478201473875, 2.639068637966914, 2.6284283381964775, 2.615997264733651, 2.6015186885307573, 2.584229329145209, 2.568698236703481, 2.552784736912043, 2.534750647928523, 2.52329241779246, 2.510507482221757, 2.497756091636194, 2.4969425925676068, 2.4869385971420113, 2.4829604856682135, 2.475075169737116, 2.4724741886402, 2.4630433965003355, 2.4848059159390052, 2.5342475851181105, 2.4573778464093388, 2.4755544670305425, 2.4555486712745456, 2.4697183298164207, 2.4507081904043315, 2.4472772092459043, 2.4512267652990785, 2.447089695186646, 2.441859352960571, 2.44119396781295, 2.4425252799330086, 2.4384646748478582, 2.4377279371659353, 2.436513008155259, 2.4336806118781933, 2.4339980296117725, 2.4341348626930723, 2.4300974867809777, 2.4331935246785483, 2.4291365913961127, 2.429366140725773, 2.4273403250720897, 2.4257450898488364, 2.429139251192215, 2.423660582118042, 2.42651034575965, 2.42570804414295, 2.427383136670969, 2.4268619810614878, 2.4270405522708236, 2.4259934816845923, 2.4258400689204924, 2.426421749963745, 2.427701453269996, 2.431250497821125, 2.425018586548678, 2.431528594497781, 2.424814473232025, 2.423786729818886, 2.423591684238077, 2.4228515742447576, 2.4233346153754125, 2.4232666962252463, 2.422024248856042, 2.4249043989259818, 2.4323127802173885, 2.4275893370310464, 2.4263238965584137, 2.4303063614223586, 2.426628658141213, 2.4289997399146923, 2.4265104127047685, 2.42969260857806, 2.4304771074912037, 2.431694727617336, 2.433162258176381, 2.4310241082227484, 2.431117690059743, 2.432009219340307, 2.4271355082444566, 2.425982541442896, 2.430559505578528, 2.426040788785186, 2.4319572460475225, 2.442494117567692, 2.4583504567984096, 2.4399534673330625, 2.4378992238851214, 2.4920044743956016, 2.439265651655902, 2.4322828569239974, 2.4464897244238895, 2.4325109557760958, 2.4303264747112254, 2.4328539144425165, 2.433998445767683, 2.427124369516357, 2.427614044477591, 2.429512128062632, 2.4290458181221495, 2.42539820843338, 2.4266248755462847, 2.427801231641096, 2.4261285483543507, 2.4269205783975534, 2.4290975424260735, 2.4246085320395987, 2.4263986750385054, 2.428505817266129, 2.4261167617071244, 2.428129978367848, 2.428883876315087, 2.4278778614864756, 2.4277064784602773, 2.4284972322398217, 2.429924565974519, 2.4285790853703944, 2.428094491191294, 2.429613773850189, 2.4281727938816466, 2.42723319291677, 2.4304235087239685, 2.428280800238423, 2.429530318734681, 2.426935014270601, 2.4294664930240275, 2.429985516960006, 2.4279547775124484, 2.429421563845354, 2.4297106399128983, 2.428129835864789, 2.4296383564108113, 2.4282866812300408, 2.431089056732228, 2.4287726150944904, 2.430120299015139, 2.4304363668845794, 2.42850749167707, 2.4313496501966454, 2.4291830340825475, 2.42845106751265, 2.430825986689926, 2.427855414905767, 2.428735779228273, 2.428216107373167, 2.4277692839429883, 2.42942452822217, 2.427329464303253, 2.4283853430661857, 2.428903448562121, 2.4274196389860707, 2.4287513112786954, 2.4307903577932977, 2.4270100061137883, 2.429808306576583, 2.428966268921525, 2.4274435360443416, 2.4295653917127837, 2.4281991428538103, 2.4281744217050485, 2.4294091934836755, 2.4294464036161676, 2.429418263568471, 2.427201834414001, 2.4289524382949854, 2.428528676087829, 2.4275786876678467, 2.4295587484863983, 2.428885529976956, 2.4280752137376758, 2.429017342174386, 2.4302943668929227, 2.4271917014286437, 2.431070074855009, 2.4285522615185315, 2.4276059069265483, 2.430664324016602, 2.428480829119878, 2.429075495558615, 2.4280606506297544, 2.4317447896465683, 2.428362280277196, 2.42845581709262, 2.4334164139476706, 2.4273538949650106, 2.430161108524341, 2.430015688459274, 2.429013181006771, 2.429633773214907, 2.432170215694384, 2.4277349775060646, 2.4321756519511806, 2.429092380996604, 2.4285490430634598, 2.4306704622184117, 2.428356020712892, 2.4286329534840703, 2.4306807674602138, 2.429065349262532, 2.429875599339678, 2.432977889363206, 2.4277951008776335, 2.43112339135657, 2.4295839562596164, 2.430465256639302, 2.4301268541558425, 2.430184017848499, 2.4305088766689957, 2.4286527430091196, 2.4319077336729453, 2.4306088272965405, 2.428838081939271, 2.431690005833292, 2.4325625164363966, 2.429227117638674, 2.4308838319700143, 2.431572861663618, 2.4296738298851475, 2.4315966655468118, 2.4308913067252376, 2.4305144162796597, 2.434433096931094, 2.4289865243219584, 2.432421434884784, 2.432261590691427, 2.4295166368750714, 2.4315001764908213, 2.431877012910514, 2.4319854044953395, 2.4304901097208407, 2.4310478236287687, 2.4324092559626536, 2.432402086571128, 2.4305724439949823, 2.4317704973549676, 2.431034212237704, 2.4308859421114617, 2.4324761674126183, 2.4322514271696996, 2.4321058328907283, 2.4304780431568918, 2.433049879246353, 2.4315906749374565, 2.4310463563170535, 2.4339369212465334, 2.431178512635881, 2.4345995095758797, 2.4335438637506392, 2.43172381742443, 2.432746498064063, 2.4315490092354257, 2.435827998692179, 2.4305694698308686, 2.43183589099076, 2.434566163468635, 2.432032360819173, 2.4343018649246893, 2.4323749154659327, 2.432951358738791, 2.4331783445793613, 2.432825438103261, 2.4337553461196975, 2.4328160642207353, 2.4315957267491886, 2.4361948986554576, 2.43228987676561, 2.4339987010204145, 2.4340286763822307, 2.432695351602213, 2.4334790557867594, 2.4380346914426054, 2.4310363035875393, 2.4334190244157914, 2.4367017667673294, 2.431679523637142, 2.4344018077223954, 2.4348374156920585, 2.4327771049023457, 2.4354133281018737, 2.4334364407168234, 2.432423393127366, 2.43710531389772, 2.4322504085077243, 2.4331591696966264, 2.4356335669707, 2.4332870023786923, 2.4339394506758265, 2.4336587114287127, 2.4338133769669557, 2.433066373583914, 2.434994866303818, 2.437634454376396], 'val_acc': [0.09359605891143748, 0.10180623933354818, 0.11330049230498437, 0.10837438393060014, 0.11986863699990932, 0.12807881663291912, 0.1198686370977823, 0.1165845637593559, 0.13300492520304932, 0.15927750329763823, 0.1691297200464067, 0.1691297200464067, 0.17077175617120144, 0.17241379229599618, 0.17077175617120144, 0.17077175617120144, 0.17241379239386917, 0.16748768392161195, 0.17405582842079093, 0.17241379229599618, 0.17241379239386917, 0.17405582832291797, 0.17569786454558567, 0.17405582832291797, 0.16912971985066075, 0.1740558285186639, 0.1707717558775825, 0.17898193679517518, 0.1789819366973022, 0.16420361147627652, 0.17405582832291797, 0.18226600904476467, 0.17405582822504498, 0.17569786464345866, 0.1756978644477127, 0.1756978644477127, 0.17405582842079093, 0.1740558285186639, 0.17405582832291797, 0.17733990067038044, 0.18226600914263763, 0.17898193689304814, 0.18062397291996993, 0.17241379229599618, 0.1740558285186639, 0.17241379229599618, 0.17569786454558567, 0.17733990067038044, 0.1756978644477127, 0.1789819366973022, 0.1756978644477127, 0.1756978644477127, 0.1756978644477127, 0.17405582832291797, 0.1756978644477127, 0.18062397291996993, 0.17405582832291797, 0.16912971985066075, 0.17241379219812322, 0.17405582832291797, 0.18555008129435416, 0.18062397282209694, 0.17898193679517518, 0.18390804507168643, 0.1756978644477127, 0.17405582822504498, 0.17733990057250745, 0.17405582822504498, 0.18390804507168643, 0.17733990057250745, 0.1756978644477127, 0.18062397282209694, 0.18390804516955941, 0.17077175607332848, 0.1756978644477127, 0.1806239730178429, 0.17898193689304814, 0.18390804516955941, 0.17898193699092113, 0.17569786474133164, 0.1773399007682534, 0.1756978644477127, 0.17405582842079093, 0.17241379219812322, 0.1707717559754555, 0.17077175607332848, 0.17898193699092113, 0.17405582842079093, 0.17241379219812322, 0.17241379229599618, 0.17405582832291797, 0.18062397282209694, 0.17733990037676148, 0.1806239730178429, 0.18226600894689168, 0.1839080452674324, 0.1444991782723585, 0.1691297199485337, 0.17405582822504498, 0.16256157544935473, 0.17405582832291797, 0.1756978644477127, 0.17405582832291797, 0.16584564769894422, 0.17405582842079093, 0.17405582842079093, 0.17733990067038044, 0.18226600914263763, 0.18226600904476467, 0.17569786464345866, 0.17077175626907443, 0.1806239730178429, 0.1806239730178429, 0.1691297200464067, 0.1806239730178429, 0.18719211761489485, 0.17569786474133164, 0.1773399008661264, 0.17898193679517518, 0.18226600924051062, 0.1806239730178429, 0.17898193679517518, 0.17405582842079093, 0.18062397311571587, 0.1806239730178429, 0.17898193689304814, 0.1822660093383836, 0.18226600924051062, 0.1773399008661264, 0.17733990096399938, 0.17569786464345866, 0.18883415373968962, 0.1806239730178429, 0.1806239730178429, 0.16584564799256318, 0.1592775033955112, 0.16256157574297367, 0.18390804546317835, 0.17898193679517518, 0.17569786483920463, 0.16584564799256318, 0.1740558286165369, 0.16748768401948494, 0.16748768382373896, 0.1691297199485337, 0.16420361167202246, 0.16912972014427968, 0.1658456478946902, 0.16748768401948494, 0.17241379239386917, 0.1740558286165369, 0.17077175646482037, 0.1642036119656414, 0.1707717563669474, 0.17733990106187233, 0.1740558286165369, 0.1740558286165369, 0.17077175646482037, 0.17241379249174216, 0.17077175646482037, 0.17569786483920463, 0.1707717563669474, 0.17569786483920463, 0.17077175646482037, 0.17241379258961514, 0.18062397321358886, 0.17569786483920463, 0.17569786483920463, 0.17241379258961514, 0.17077175646482037, 0.17241379258961514, 0.1740558287144099, 0.17898193699092113, 0.1740558286165369, 0.1707717563669474, 0.17569786483920463, 0.17077175646482037, 0.17241379258961514, 0.17569786483920463, 0.17569786483920463, 0.17077175626907443, 0.1773399008661264, 0.16912972014427968, 0.17241379249174216, 0.17569786483920463, 0.17241379258961514, 0.17241379258961514, 0.1740558287144099, 0.17569786483920463, 0.17241379258961514, 0.17241379249174216, 0.1740558287144099, 0.17077175626907443, 0.17898193699092113, 0.17241379239386917, 0.17733990096399938, 0.17733990096399938, 0.1740558287144099, 0.16748768411735793, 0.17569786483920463, 0.17241379249174216, 0.17241379258961514, 0.17569786483920463, 0.17241379249174216, 0.17241379249174216, 0.17898193699092113, 0.16912972024215267, 0.17569786474133164, 0.17569786483920463, 0.17569786474133164, 0.17898193699092113, 0.17733990096399938, 0.17898193689304814, 0.16584564799256318, 0.17569786474133164, 0.1773399008661264, 0.1773399008661264, 0.17898193699092113, 0.1773399008661264, 0.17241379249174216, 0.17733990096399938, 0.1822660093383836, 0.17241379249174216, 0.17241379249174216, 0.1707717563669474, 0.17569786464345866, 0.17241379249174216, 0.1642036118677684, 0.1740558286165369, 0.1740558287144099, 0.17241379239386917, 0.17077175646482037, 0.16912972034002563, 0.17569786474133164, 0.1822660093383836, 0.1674876842152309, 0.18062397321358886, 0.17569786483920463, 0.17569786483920463, 0.16584564809043614, 0.17569786483920463, 0.1740558285186639, 0.16748768411735793, 0.17569786483920463, 0.17569786474133164, 0.1740558287144099, 0.17077175656269336, 0.17569786474133164, 0.17241379258961514, 0.16912972034002563, 0.1658456478946902, 0.17241379249174216, 0.17898193708879412, 0.17569786474133164, 0.18062397331146185, 0.1674876842152309, 0.17241379239386917, 0.17733990106187233, 0.17241379249174216, 0.1740558286165369, 0.16912972024215267, 0.16420361176989545, 0.18062397321358886, 0.1658456478946902, 0.17405582881228285, 0.17898193708879412, 0.17898193708879412, 0.16091953961817893, 0.1756978649370776, 0.1822660094362566, 0.16748768411735793, 0.17241379249174216, 0.17898193708879412, 0.16420361176989545, 0.17569786483920463, 0.1756978649370776, 0.1658456478946902, 0.16912972024215267, 0.17898193708879412, 0.16912972024215267, 0.1740558287144099, 0.1707717563669474, 0.16912972024215267, 0.17241379258961514, 0.17569786483920463, 0.16420361176989545, 0.17241379258961514, 0.17241379239386917, 0.16912972014427968, 0.1822660093383836, 0.16912972024215267, 0.1642036118677684, 0.17241379249174216, 0.16420361176989545, 0.17569786483920463, 0.17898193699092113, 0.17569786483920463, 0.16912972014427968, 0.17241379239386917, 0.17405582881228285], 'loss': [2.705548516729774, 2.696435544622997, 2.6884616152706577, 2.6819672289815037, 2.675844196715149, 2.6707156197001556, 2.6654180693185796, 2.659491567690025, 2.653663614300487, 2.6475905447769947, 2.6395225492590995, 2.6294601462949716, 2.618030978032451, 2.6049623008626197, 2.5909008268947717, 2.576594622472963, 2.5636334862797168, 2.5525986358125596, 2.542065001317363, 2.5330537221270175, 2.525036179211595, 2.519185875671355, 2.5131734057863144, 2.5077493120267897, 2.5044147246672144, 2.500850580654105, 2.4951618396281217, 2.517317127251282, 2.537240312525379, 2.512428998065925, 2.5080087016495347, 2.493028669141891, 2.490015238070635, 2.4868044957977546, 2.484874140702234, 2.481790425204643, 2.479921248806086, 2.4776369607913664, 2.475303788351572, 2.473021719441032, 2.47031075900585, 2.4693306544967744, 2.4676049024906983, 2.4675389299647272, 2.465537864226825, 2.4629417581969464, 2.4629747515096803, 2.4614837833498537, 2.4600907094669537, 2.4612774002968165, 2.460072064448676, 2.4590620351278316, 2.4581543388797518, 2.459311242270029, 2.458206673373432, 2.4571147757144436, 2.4565257012721693, 2.4550146240962847, 2.4545444653998656, 2.454269577002868, 2.4512196638745696, 2.453237152491262, 2.452984496063765, 2.4542193730019446, 2.4529967655636202, 2.450947358329193, 2.450802150839898, 2.4511766852784205, 2.450384090862235, 2.4500424792389603, 2.450035444471136, 2.4491572934254484, 2.4508012970370188, 2.450809291549777, 2.450286966966163, 2.451712474881746, 2.4507329493332692, 2.451638041972135, 2.448171200154988, 2.4484775977947386, 2.4476899722763155, 2.4476916424792403, 2.4483048526168605, 2.448677436233301, 2.447603606394429, 2.4478024828360554, 2.446766573007102, 2.445089824390607, 2.445355450592981, 2.4451263163368804, 2.446374500458735, 2.4457261155028607, 2.4645530977777876, 2.4665050754312126, 2.4555490263678457, 2.453991153940283, 2.4509302998959894, 2.4665795267485007, 2.464896642598773, 2.4514350633601634, 2.4528031162167965, 2.4510194002970045, 2.4479733133218127, 2.4466633871105907, 2.444900508680872, 2.444551188647135, 2.4429600203551307, 2.4440920821939893, 2.4433679877365395, 2.4444110699013275, 2.4436690692784118, 2.444028543495789, 2.4429789925992367, 2.4435279068760805, 2.4416992500822157, 2.441448946312468, 2.4411411692719196, 2.440180740513106, 2.440164725148947, 2.4400601207842816, 2.44024277830026, 2.4395946377356683, 2.4398445278222556, 2.4398571321851663, 2.4396958054458335, 2.4394413214689408, 2.43912760570064, 2.438974190933259, 2.4399041014285547, 2.4388618999927685, 2.439053718950714, 2.43914486514959, 2.439035190253287, 2.437711749184548, 2.4378445013347836, 2.4383526716878525, 2.437520827307104, 2.437465803530182, 2.4373862561259183, 2.4368394346452593, 2.4377395963766735, 2.4370030361034543, 2.437118754396693, 2.4369997539559427, 2.436906927469085, 2.4376182540486235, 2.4360452762619427, 2.437273464849108, 2.437410639149942, 2.4368988363160247, 2.437190376904466, 2.436209850879176, 2.4366000573003563, 2.436185259789657, 2.436688774958773, 2.435728248482612, 2.43616087852807, 2.4357028853477147, 2.435928459774542, 2.435891313572439, 2.436250277955919, 2.436148837165911, 2.4357897102465618, 2.4360759591175056, 2.4352936191480508, 2.4351159087931107, 2.435443211874678, 2.4350274710684587, 2.434964559797878, 2.4353578614503206, 2.434228660144845, 2.4349406153269615, 2.435282394381764, 2.434586507632747, 2.4342634730760078, 2.4339862636472165, 2.434431748125832, 2.433835476771517, 2.4340963747467104, 2.4355977003579268, 2.4350820935971926, 2.43422008363618, 2.43433957638437, 2.434382954662096, 2.43324836069064, 2.4339251889340443, 2.433402905180224, 2.4341902538001903, 2.433098445438017, 2.433009123067836, 2.43437401166442, 2.4338138562697895, 2.43403389409827, 2.4328589864335264, 2.432919518120235, 2.433400184858506, 2.4326713233023454, 2.4341839799156424, 2.4330994613851122, 2.434237067851198, 2.4325270702706714, 2.4328012365580096, 2.432579841946674, 2.43178060686319, 2.4324177282791606, 2.431668532408728, 2.432229919355263, 2.4324691553135427, 2.43376115842032, 2.432182899001198, 2.4332283734785705, 2.431469889490022, 2.4314742927433772, 2.431556090486123, 2.4311788823325533, 2.431185292708066, 2.431780802983278, 2.430830303501544, 2.431041117713192, 2.4314838988090686, 2.430844702123372, 2.4315337509100443, 2.4309276440794707, 2.43066511624158, 2.4307230039788466, 2.4304916706907678, 2.4304402288714964, 2.4306661919157118, 2.430544632905807, 2.43272514167018, 2.430550746310663, 2.430420138263115, 2.430363767690482, 2.429994200289372, 2.430210366630946, 2.431224062995989, 2.429733462793871, 2.429522661898415, 2.429465530344593, 2.4294849845173423, 2.4296757270178513, 2.4290991195663043, 2.429676862027366, 2.4294126391166047, 2.429271801981838, 2.429316275026764, 2.428962918326595, 2.4289648789399947, 2.4292962874230417, 2.4287943484602033, 2.4290613904870755, 2.4292744455396273, 2.4291251235429265, 2.429215045435473, 2.4289057466283714, 2.429163658447579, 2.4280435392744, 2.428369905131064, 2.4304201304300608, 2.4287411431267523, 2.4283514256839633, 2.429031820414737, 2.428800613728392, 2.4279669249571816, 2.427946965356627, 2.427915748580525, 2.427872878080521, 2.4281982924169583, 2.4276753188648263, 2.4282770968560565, 2.427831208191858, 2.4285253679483088, 2.427525112810076, 2.4280481838837296, 2.427093865494464, 2.4279837694500994, 2.4275244885156777, 2.428079501608314, 2.427820025430323, 2.4264979688538664, 2.4277702620386834, 2.4278798179704797, 2.4275973206428043, 2.426622269628474, 2.427129670383994, 2.427576272051927, 2.4267691135406495, 2.4266212951965644, 2.427576757407531, 2.4263048718352582, 2.426649679540364, 2.427438142901818, 2.4255826437987342, 2.42652054547774, 2.4258612901033563, 2.426117163664017, 2.4262712010612724, 2.4253396274128, 2.426571254710642, 2.4274427776219176], 'acc': [0.06036961039784508, 0.08459958908065879, 0.09938398331281341, 0.10882956892007185, 0.11088295680541523, 0.12073921924743809, 0.11950718598321723, 0.11334702271883983, 0.11540040980251907, 0.1408624233253438, 0.16180698181936629, 0.16344969307495094, 0.15975359433485498, 0.16344969189999284, 0.1605749483776778, 0.1613963043787641, 0.16098562738489078, 0.15728952824396275, 0.16016426995794386, 0.15975359294571181, 0.1642710465302947, 0.16303901348025893, 0.16837782271099286, 0.16878850052488903, 0.1691991791404493, 0.16960985636686643, 0.17207392187945897, 0.1601642705454229, 0.1548254630954848, 0.15975359294571181, 0.16057494835931907, 0.17453798756951913, 0.1749486646001099, 0.1679671456987608, 0.1675564684723437, 0.1761806992351152, 0.17207392227111165, 0.17125256760409235, 0.1733059555169738, 0.1737166327250322, 0.17248460065412816, 0.17618069823762475, 0.17371663215591188, 0.17453798678621374, 0.17289527809473035, 0.1770020532595793, 0.17330595573115887, 0.17494866479593627, 0.1749486649917626, 0.17618069903928885, 0.1696098557793874, 0.1712525660558403, 0.16960985656269278, 0.17248459889169102, 0.1724846006908456, 0.16919917931791692, 0.16919917951374328, 0.17453798874447723, 0.17330595533950618, 0.17371663213755315, 0.17823408595467985, 0.17289527829055668, 0.17207392107779484, 0.1745379869820401, 0.17535934243236478, 0.17494866579342672, 0.17330595492949477, 0.173305954323657, 0.1708418886335968, 0.1724845987142234, 0.1753593420407121, 0.17125256682078696, 0.17494866638090575, 0.1720739212736212, 0.17618069884346252, 0.17207392107779484, 0.17782340970739446, 0.16960985734599818, 0.17453798854865088, 0.172895277898904, 0.17412730957815534, 0.17084188802775907, 0.17453798659038738, 0.17494866477757753, 0.1728952771155986, 0.17289527750725128, 0.17535934300148512, 0.17782340794495732, 0.17207392186110024, 0.1704312121904851, 0.17289527770307764, 0.1733059551253211, 0.1687885007390741, 0.17453798796117184, 0.17577001944459683, 0.1716632452405209, 0.17453798838954196, 0.1663244342473498, 0.17412730955979663, 0.1716632452405209, 0.17453798876283594, 0.1675564672973856, 0.17289527750725128, 0.17577002101120762, 0.17823408513465702, 0.17330595592698522, 0.1757700196404232, 0.17289527631393448, 0.17043121158464733, 0.17125256562747015, 0.17700205445289613, 0.17248459891004975, 0.17453798717786645, 0.17535934358896416, 0.17618069786433077, 0.17453798659038738, 0.17412730936397028, 0.17741273207096594, 0.17535934319731147, 0.1737166323150208, 0.17700205427542848, 0.17453798717786645, 0.17659137685318502, 0.1774127307001815, 0.17864476355439093, 0.1753593436073229, 0.17289527748889258, 0.17700205406124342, 0.1720739212736212, 0.17659137624734725, 0.17577002161704539, 0.17700205249463263, 0.17905544079916677, 0.1753593426098324, 0.17577001944459683, 0.18110882926280983, 0.17618069884346252, 0.17659137687154375, 0.17741273011270245, 0.17741273089600784, 0.17289527631393448, 0.1765913748582041, 0.17864476316273825, 0.18028747500580194, 0.1774127303085288, 0.17207392148780626, 0.17494866440428355, 0.1794661185947042, 0.18069815105726098, 0.17453798837118326, 0.1839835731951363, 0.18028747363501751, 0.17782340970739446, 0.17864476275272684, 0.18316221754898526, 0.17946611898635692, 0.1798767960169477, 0.17864476316273825, 0.1802874740266702, 0.17987679703279688, 0.17823408634633253, 0.18726899314220435, 0.1815195062934006, 0.1827515389517837, 0.17864476316273825, 0.18110882945863618, 0.17987679742444956, 0.1815195066850533, 0.18028747343919116, 0.18028747345754986, 0.18193018567390756, 0.1802874754341721, 0.18275153994927415, 0.18316221874230207, 0.1815195062934006, 0.18110883045612663, 0.18234086250867199, 0.18151950727253235, 0.18234086350616244, 0.18234086350616244, 0.179876795625295, 0.18275153914761005, 0.18193018490896087, 0.1819301845173082, 0.18069815086143462, 0.18234086250867199, 0.18316221733480018, 0.18193018471313452, 0.18316221696150622, 0.18644763888519647, 0.1827515389517837, 0.18193018588809262, 0.1761806980417984, 0.18110882945863618, 0.18357289497122872, 0.18069815225057778, 0.18193018410729678, 0.18480492782543817, 0.18275154112423225, 0.1806981514672724, 0.18234086331033608, 0.18151950746835868, 0.18110883043776793, 0.17823408593632112, 0.1827515409100472, 0.18316221656985351, 0.18398357337260393, 0.1798767976019172, 0.18028747381248514, 0.18069815221386035, 0.18193018569226627, 0.18480492860874356, 0.18439425157815278, 0.18110883043776793, 0.18316221795899668, 0.1831622185464757, 0.18562628427325334, 0.18357289516705508, 0.1827515409284059, 0.18069815223221905, 0.18193018371564407, 0.18275153971673036, 0.1852156066368248, 0.18234086231284563, 0.1819301843031231, 0.18193018410729678, 0.18603696108965903, 0.1835728947937611, 0.18398357180599315, 0.1815195066850533, 0.1823408625270307, 0.18521560622681338, 0.18028747343919116, 0.18439424963824802, 0.18480492702377405, 0.18644763908102283, 0.18028747424085528, 0.18316221715733255, 0.18357289477540237, 0.18275154032256813, 0.18069815084307592, 0.1864476377102384, 0.1835728945612173, 0.18562628404070955, 0.18480492843127594, 0.18316221774481162, 0.18480492723795913, 0.18521560603098702, 0.18357289418792333, 0.1819301854780812, 0.18480492843127594, 0.18603696206879078, 0.18316221774481162, 0.18193018391147042, 0.18275153893342497, 0.18439425097231502, 0.18316221874230207, 0.18644763829771743, 0.18562628284739274, 0.1823408617253666, 0.18316221676567987, 0.18275154051839448, 0.18398357198346077, 0.18193018469477582, 0.18562628284739274, 0.1848049286271023, 0.1872689937113247, 0.1802874752016283, 0.18439425022572709, 0.18644763751441204, 0.18398357219764583, 0.18234086211701928, 0.18685831593414595, 0.1872689933380307, 0.18644763769187966, 0.18644763829771743, 0.1860369610713003, 0.18480492782543817, 0.18726899314220435, 0.19219712612565293, 0.18398357317677758, 0.1860369602879949, 0.18398357182435188, 0.18521560524768163, 0.18603696205043205, 0.18193018369728534, 0.18562628445072096, 0.18521560583516067, 0.18275154110587352, 0.18521560644099844, 0.1839835720018195, 0.18275154051839448, 0.18521560485602895, 0.18644763908102283, 0.1852156046602026, 0.18767967074191547, 0.18439425159651152, 0.1848049291962226, 0.18603696048382126, 0.18562628386324193]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
