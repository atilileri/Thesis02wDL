{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf66.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 18:10:18 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'Front', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '03', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F5150E4E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F50F8D6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0915, Accuracy:0.3680, Validation Loss:1.0822, Validation Accuracy:0.3990\n",
    "Epoch #2: Loss:1.0787, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #3: Loss:1.0745, Accuracy:0.3951, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3947, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0749, Accuracy:0.3955, Validation Loss:1.0749, Validation Accuracy:0.3777\n",
    "Epoch #6: Loss:1.0746, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3810\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #10: Loss:1.0741, Accuracy:0.3971, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.3979, Validation Loss:1.0741, Validation Accuracy:0.3826\n",
    "Epoch #12: Loss:1.0740, Accuracy:0.4000, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #13: Loss:1.0744, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0739, Accuracy:0.4012, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #15: Loss:1.0738, Accuracy:0.3996, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #16: Loss:1.0738, Accuracy:0.3996, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #17: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #18: Loss:1.0737, Accuracy:0.4037, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #19: Loss:1.0738, Accuracy:0.4012, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #20: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #21: Loss:1.0738, Accuracy:0.4004, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #22: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.4122\n",
    "Epoch #23: Loss:1.0739, Accuracy:0.3996, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #24: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #25: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #26: Loss:1.0736, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0737, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #28: Loss:1.0736, Accuracy:0.4066, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #29: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #30: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #31: Loss:1.0736, Accuracy:0.3996, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #32: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #33: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #34: Loss:1.0734, Accuracy:0.4029, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #35: Loss:1.0734, Accuracy:0.4025, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #36: Loss:1.0735, Accuracy:0.4012, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0734, Validation Accuracy:0.3892\n",
    "Epoch #38: Loss:1.0732, Accuracy:0.4025, Validation Loss:1.0735, Validation Accuracy:0.3892\n",
    "Epoch #39: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0735, Validation Accuracy:0.3974\n",
    "Epoch #40: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #41: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0733, Validation Accuracy:0.3859\n",
    "Epoch #42: Loss:1.0739, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.3859\n",
    "Epoch #43: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #44: Loss:1.0731, Accuracy:0.3992, Validation Loss:1.0736, Validation Accuracy:0.3892\n",
    "Epoch #45: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0730, Accuracy:0.3934, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0728, Accuracy:0.3984, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #49: Loss:1.0726, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #50: Loss:1.0727, Accuracy:0.3963, Validation Loss:1.0737, Validation Accuracy:0.3810\n",
    "Epoch #51: Loss:1.0730, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #52: Loss:1.0735, Accuracy:0.3906, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #53: Loss:1.0732, Accuracy:0.4016, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #54: Loss:1.0726, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.3875\n",
    "Epoch #55: Loss:1.0720, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #56: Loss:1.0724, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #57: Loss:1.0726, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #58: Loss:1.0721, Accuracy:0.3938, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #59: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #60: Loss:1.0720, Accuracy:0.3979, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #61: Loss:1.0728, Accuracy:0.3930, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #62: Loss:1.0716, Accuracy:0.4062, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #63: Loss:1.0721, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #64: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #65: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0760, Validation Accuracy:0.3760\n",
    "Epoch #66: Loss:1.0731, Accuracy:0.4025, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #67: Loss:1.0730, Accuracy:0.4021, Validation Loss:1.0752, Validation Accuracy:0.3777\n",
    "Epoch #68: Loss:1.0727, Accuracy:0.4029, Validation Loss:1.0748, Validation Accuracy:0.3777\n",
    "Epoch #69: Loss:1.0731, Accuracy:0.3901, Validation Loss:1.0744, Validation Accuracy:0.3760\n",
    "Epoch #70: Loss:1.0732, Accuracy:0.4021, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #71: Loss:1.0729, Accuracy:0.4037, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #72: Loss:1.0729, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #73: Loss:1.0726, Accuracy:0.3992, Validation Loss:1.0753, Validation Accuracy:0.3777\n",
    "Epoch #74: Loss:1.0727, Accuracy:0.4066, Validation Loss:1.0750, Validation Accuracy:0.3810\n",
    "Epoch #75: Loss:1.0720, Accuracy:0.3984, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0727, Accuracy:0.3963, Validation Loss:1.0764, Validation Accuracy:0.3810\n",
    "Epoch #77: Loss:1.0723, Accuracy:0.4066, Validation Loss:1.0768, Validation Accuracy:0.3678\n",
    "Epoch #78: Loss:1.0723, Accuracy:0.4033, Validation Loss:1.0761, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0762, Validation Accuracy:0.3892\n",
    "Epoch #80: Loss:1.0724, Accuracy:0.4008, Validation Loss:1.0765, Validation Accuracy:0.3612\n",
    "Epoch #81: Loss:1.0720, Accuracy:0.3984, Validation Loss:1.0764, Validation Accuracy:0.3859\n",
    "Epoch #82: Loss:1.0720, Accuracy:0.3988, Validation Loss:1.0765, Validation Accuracy:0.3695\n",
    "Epoch #83: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0763, Validation Accuracy:0.3662\n",
    "Epoch #84: Loss:1.0736, Accuracy:0.3848, Validation Loss:1.0762, Validation Accuracy:0.3859\n",
    "Epoch #85: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0762, Validation Accuracy:0.3842\n",
    "Epoch #86: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0754, Validation Accuracy:0.3859\n",
    "Epoch #87: Loss:1.0730, Accuracy:0.3832, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #88: Loss:1.0719, Accuracy:0.3988, Validation Loss:1.0757, Validation Accuracy:0.3875\n",
    "Epoch #89: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0755, Validation Accuracy:0.3744\n",
    "Epoch #90: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0755, Validation Accuracy:0.3760\n",
    "Epoch #91: Loss:1.0713, Accuracy:0.4025, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #92: Loss:1.0711, Accuracy:0.4037, Validation Loss:1.0751, Validation Accuracy:0.3744\n",
    "Epoch #93: Loss:1.0721, Accuracy:0.3955, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #94: Loss:1.0714, Accuracy:0.4062, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #95: Loss:1.0712, Accuracy:0.3975, Validation Loss:1.0769, Validation Accuracy:0.3727\n",
    "Epoch #96: Loss:1.0715, Accuracy:0.3934, Validation Loss:1.0771, Validation Accuracy:0.3711\n",
    "Epoch #97: Loss:1.0715, Accuracy:0.4033, Validation Loss:1.0779, Validation Accuracy:0.3842\n",
    "Epoch #98: Loss:1.0725, Accuracy:0.3975, Validation Loss:1.0771, Validation Accuracy:0.3744\n",
    "Epoch #99: Loss:1.0724, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3793\n",
    "Epoch #100: Loss:1.0725, Accuracy:0.4025, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #101: Loss:1.0745, Accuracy:0.3967, Validation Loss:1.0753, Validation Accuracy:0.3892\n",
    "Epoch #102: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0757, Validation Accuracy:0.3695\n",
    "Epoch #103: Loss:1.0730, Accuracy:0.3934, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #104: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #105: Loss:1.0727, Accuracy:0.3979, Validation Loss:1.0748, Validation Accuracy:0.3727\n",
    "Epoch #106: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #107: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #108: Loss:1.0735, Accuracy:0.4008, Validation Loss:1.0750, Validation Accuracy:0.3727\n",
    "Epoch #109: Loss:1.0729, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #110: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0744, Validation Accuracy:0.3744\n",
    "Epoch #111: Loss:1.0727, Accuracy:0.3988, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #112: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #113: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #114: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #115: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3744\n",
    "Epoch #116: Loss:1.0730, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #117: Loss:1.0726, Accuracy:0.3992, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #118: Loss:1.0724, Accuracy:0.3996, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #119: Loss:1.0725, Accuracy:0.3975, Validation Loss:1.0748, Validation Accuracy:0.3744\n",
    "Epoch #120: Loss:1.0730, Accuracy:0.3873, Validation Loss:1.0752, Validation Accuracy:0.3727\n",
    "Epoch #121: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #122: Loss:1.0724, Accuracy:0.3938, Validation Loss:1.0751, Validation Accuracy:0.3678\n",
    "Epoch #123: Loss:1.0733, Accuracy:0.3975, Validation Loss:1.0748, Validation Accuracy:0.3744\n",
    "Epoch #124: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0757, Validation Accuracy:0.3875\n",
    "Epoch #125: Loss:1.0725, Accuracy:0.3955, Validation Loss:1.0750, Validation Accuracy:0.3711\n",
    "Epoch #126: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0749, Validation Accuracy:0.3744\n",
    "Epoch #127: Loss:1.0727, Accuracy:0.3971, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #128: Loss:1.0723, Accuracy:0.4025, Validation Loss:1.0749, Validation Accuracy:0.3760\n",
    "Epoch #129: Loss:1.0727, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.3760\n",
    "Epoch #130: Loss:1.0722, Accuracy:0.3975, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #131: Loss:1.0722, Accuracy:0.4000, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #132: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0749, Validation Accuracy:0.3744\n",
    "Epoch #133: Loss:1.0720, Accuracy:0.3988, Validation Loss:1.0749, Validation Accuracy:0.3678\n",
    "Epoch #134: Loss:1.0722, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.3678\n",
    "Epoch #135: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3727\n",
    "Epoch #136: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0749, Validation Accuracy:0.3744\n",
    "Epoch #137: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0749, Validation Accuracy:0.3678\n",
    "Epoch #138: Loss:1.0725, Accuracy:0.3988, Validation Loss:1.0750, Validation Accuracy:0.3744\n",
    "Epoch #139: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #140: Loss:1.0726, Accuracy:0.3996, Validation Loss:1.0750, Validation Accuracy:0.3678\n",
    "Epoch #141: Loss:1.0721, Accuracy:0.4016, Validation Loss:1.0750, Validation Accuracy:0.3711\n",
    "Epoch #142: Loss:1.0722, Accuracy:0.4049, Validation Loss:1.0749, Validation Accuracy:0.3711\n",
    "Epoch #143: Loss:1.0724, Accuracy:0.4049, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #144: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #145: Loss:1.0722, Accuracy:0.4029, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #146: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #147: Loss:1.0718, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.3695\n",
    "Epoch #148: Loss:1.0722, Accuracy:0.3992, Validation Loss:1.0750, Validation Accuracy:0.3678\n",
    "Epoch #149: Loss:1.0718, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3711\n",
    "Epoch #150: Loss:1.0737, Accuracy:0.3815, Validation Loss:1.0748, Validation Accuracy:0.3711\n",
    "Epoch #151: Loss:1.0718, Accuracy:0.4033, Validation Loss:1.0760, Validation Accuracy:0.3875\n",
    "Epoch #152: Loss:1.0724, Accuracy:0.4004, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #153: Loss:1.0720, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3711\n",
    "Epoch #154: Loss:1.0725, Accuracy:0.3922, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #155: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0745, Validation Accuracy:0.3711\n",
    "Epoch #156: Loss:1.0719, Accuracy:0.4053, Validation Loss:1.0745, Validation Accuracy:0.3760\n",
    "Epoch #157: Loss:1.0744, Accuracy:0.3988, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #158: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #159: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0743, Validation Accuracy:0.3760\n",
    "Epoch #160: Loss:1.0722, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #161: Loss:1.0721, Accuracy:0.3955, Validation Loss:1.0743, Validation Accuracy:0.3695\n",
    "Epoch #162: Loss:1.0717, Accuracy:0.4021, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #163: Loss:1.0718, Accuracy:0.3996, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #164: Loss:1.0718, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #165: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.3727\n",
    "Epoch #166: Loss:1.0719, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #167: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #168: Loss:1.0718, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #169: Loss:1.0718, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3744\n",
    "Epoch #170: Loss:1.0715, Accuracy:0.3992, Validation Loss:1.0740, Validation Accuracy:0.3760\n",
    "Epoch #171: Loss:1.0718, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3760\n",
    "Epoch #172: Loss:1.0717, Accuracy:0.3984, Validation Loss:1.0741, Validation Accuracy:0.3678\n",
    "Epoch #173: Loss:1.0715, Accuracy:0.4025, Validation Loss:1.0740, Validation Accuracy:0.3678\n",
    "Epoch #174: Loss:1.0717, Accuracy:0.4066, Validation Loss:1.0739, Validation Accuracy:0.3711\n",
    "Epoch #175: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0741, Validation Accuracy:0.3678\n",
    "Epoch #176: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3678\n",
    "Epoch #177: Loss:1.0716, Accuracy:0.4004, Validation Loss:1.0737, Validation Accuracy:0.3727\n",
    "Epoch #178: Loss:1.0716, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3744\n",
    "Epoch #179: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0738, Validation Accuracy:0.3678\n",
    "Epoch #180: Loss:1.0714, Accuracy:0.4045, Validation Loss:1.0737, Validation Accuracy:0.3760\n",
    "Epoch #181: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0737, Validation Accuracy:0.3727\n",
    "Epoch #182: Loss:1.0714, Accuracy:0.4037, Validation Loss:1.0737, Validation Accuracy:0.3695\n",
    "Epoch #183: Loss:1.0714, Accuracy:0.4008, Validation Loss:1.0736, Validation Accuracy:0.3695\n",
    "Epoch #184: Loss:1.0714, Accuracy:0.4021, Validation Loss:1.0736, Validation Accuracy:0.3695\n",
    "Epoch #185: Loss:1.0714, Accuracy:0.4029, Validation Loss:1.0734, Validation Accuracy:0.3760\n",
    "Epoch #186: Loss:1.0727, Accuracy:0.4033, Validation Loss:1.0734, Validation Accuracy:0.3744\n",
    "Epoch #187: Loss:1.0712, Accuracy:0.3996, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #188: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0735, Validation Accuracy:0.3760\n",
    "Epoch #189: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0733, Validation Accuracy:0.3727\n",
    "Epoch #190: Loss:1.0711, Accuracy:0.4062, Validation Loss:1.0736, Validation Accuracy:0.3678\n",
    "Epoch #191: Loss:1.0715, Accuracy:0.3992, Validation Loss:1.0736, Validation Accuracy:0.3678\n",
    "Epoch #192: Loss:1.0715, Accuracy:0.3996, Validation Loss:1.0732, Validation Accuracy:0.3744\n",
    "Epoch #193: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0733, Validation Accuracy:0.3678\n",
    "Epoch #194: Loss:1.0715, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.3727\n",
    "Epoch #195: Loss:1.0713, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #196: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0729, Validation Accuracy:0.3744\n",
    "Epoch #197: Loss:1.0711, Accuracy:0.4041, Validation Loss:1.0731, Validation Accuracy:0.3678\n",
    "Epoch #198: Loss:1.0712, Accuracy:0.4029, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #199: Loss:1.0712, Accuracy:0.3975, Validation Loss:1.0728, Validation Accuracy:0.3711\n",
    "Epoch #200: Loss:1.0717, Accuracy:0.3979, Validation Loss:1.0728, Validation Accuracy:0.3744\n",
    "Epoch #201: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0731, Validation Accuracy:0.3842\n",
    "Epoch #202: Loss:1.0724, Accuracy:0.3979, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #203: Loss:1.0724, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.3826\n",
    "Epoch #204: Loss:1.0714, Accuracy:0.4037, Validation Loss:1.0730, Validation Accuracy:0.3678\n",
    "Epoch #205: Loss:1.0714, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #206: Loss:1.0711, Accuracy:0.4094, Validation Loss:1.0725, Validation Accuracy:0.3744\n",
    "Epoch #207: Loss:1.0711, Accuracy:0.4029, Validation Loss:1.0725, Validation Accuracy:0.3744\n",
    "Epoch #208: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0726, Validation Accuracy:0.3711\n",
    "Epoch #209: Loss:1.0721, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #210: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0726, Validation Accuracy:0.3744\n",
    "Epoch #211: Loss:1.0713, Accuracy:0.4078, Validation Loss:1.0727, Validation Accuracy:0.3678\n",
    "Epoch #212: Loss:1.0710, Accuracy:0.4037, Validation Loss:1.0726, Validation Accuracy:0.3760\n",
    "Epoch #213: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0726, Validation Accuracy:0.3711\n",
    "Epoch #214: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0728, Validation Accuracy:0.3744\n",
    "Epoch #215: Loss:1.0711, Accuracy:0.4016, Validation Loss:1.0724, Validation Accuracy:0.3760\n",
    "Epoch #216: Loss:1.0711, Accuracy:0.3975, Validation Loss:1.0726, Validation Accuracy:0.3678\n",
    "Epoch #217: Loss:1.0710, Accuracy:0.4025, Validation Loss:1.0725, Validation Accuracy:0.3727\n",
    "Epoch #218: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0725, Validation Accuracy:0.3744\n",
    "Epoch #219: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0727, Validation Accuracy:0.3760\n",
    "Epoch #220: Loss:1.0711, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3678\n",
    "Epoch #221: Loss:1.0712, Accuracy:0.4062, Validation Loss:1.0725, Validation Accuracy:0.3727\n",
    "Epoch #222: Loss:1.0710, Accuracy:0.4049, Validation Loss:1.0726, Validation Accuracy:0.3760\n",
    "Epoch #223: Loss:1.0711, Accuracy:0.4004, Validation Loss:1.0728, Validation Accuracy:0.3760\n",
    "Epoch #224: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0725, Validation Accuracy:0.3777\n",
    "Epoch #225: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0724, Validation Accuracy:0.3727\n",
    "Epoch #226: Loss:1.0711, Accuracy:0.4062, Validation Loss:1.0725, Validation Accuracy:0.3760\n",
    "Epoch #227: Loss:1.0713, Accuracy:0.3988, Validation Loss:1.0730, Validation Accuracy:0.3678\n",
    "Epoch #228: Loss:1.0707, Accuracy:0.3992, Validation Loss:1.0723, Validation Accuracy:0.3727\n",
    "Epoch #229: Loss:1.0709, Accuracy:0.4057, Validation Loss:1.0723, Validation Accuracy:0.3744\n",
    "Epoch #230: Loss:1.0709, Accuracy:0.4049, Validation Loss:1.0724, Validation Accuracy:0.3760\n",
    "Epoch #231: Loss:1.0711, Accuracy:0.4021, Validation Loss:1.0727, Validation Accuracy:0.3678\n",
    "Epoch #232: Loss:1.0710, Accuracy:0.4037, Validation Loss:1.0721, Validation Accuracy:0.3744\n",
    "Epoch #233: Loss:1.0710, Accuracy:0.4021, Validation Loss:1.0724, Validation Accuracy:0.3695\n",
    "Epoch #234: Loss:1.0708, Accuracy:0.3996, Validation Loss:1.0724, Validation Accuracy:0.3760\n",
    "Epoch #235: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0723, Validation Accuracy:0.3727\n",
    "Epoch #236: Loss:1.0706, Accuracy:0.4045, Validation Loss:1.0730, Validation Accuracy:0.3678\n",
    "Epoch #237: Loss:1.0719, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #238: Loss:1.0726, Accuracy:0.3975, Validation Loss:1.0727, Validation Accuracy:0.3826\n",
    "Epoch #239: Loss:1.0720, Accuracy:0.4033, Validation Loss:1.0726, Validation Accuracy:0.3842\n",
    "Epoch #240: Loss:1.0710, Accuracy:0.4025, Validation Loss:1.0724, Validation Accuracy:0.3678\n",
    "Epoch #241: Loss:1.0713, Accuracy:0.3984, Validation Loss:1.0721, Validation Accuracy:0.3727\n",
    "Epoch #242: Loss:1.0709, Accuracy:0.4045, Validation Loss:1.0725, Validation Accuracy:0.3695\n",
    "Epoch #243: Loss:1.0710, Accuracy:0.4057, Validation Loss:1.0725, Validation Accuracy:0.3777\n",
    "Epoch #244: Loss:1.0710, Accuracy:0.4004, Validation Loss:1.0728, Validation Accuracy:0.3842\n",
    "Epoch #245: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0722, Validation Accuracy:0.3727\n",
    "Epoch #246: Loss:1.0709, Accuracy:0.4033, Validation Loss:1.0723, Validation Accuracy:0.3777\n",
    "Epoch #247: Loss:1.0709, Accuracy:0.3959, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #248: Loss:1.0708, Accuracy:0.4025, Validation Loss:1.0722, Validation Accuracy:0.3744\n",
    "Epoch #249: Loss:1.0708, Accuracy:0.4033, Validation Loss:1.0722, Validation Accuracy:0.3727\n",
    "Epoch #250: Loss:1.0713, Accuracy:0.4016, Validation Loss:1.0725, Validation Accuracy:0.3760\n",
    "Epoch #251: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0722, Validation Accuracy:0.3727\n",
    "Epoch #252: Loss:1.0710, Accuracy:0.4000, Validation Loss:1.0727, Validation Accuracy:0.3678\n",
    "Epoch #253: Loss:1.0709, Accuracy:0.4016, Validation Loss:1.0724, Validation Accuracy:0.3777\n",
    "Epoch #254: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0725, Validation Accuracy:0.3777\n",
    "Epoch #255: Loss:1.0712, Accuracy:0.4066, Validation Loss:1.0722, Validation Accuracy:0.3727\n",
    "Epoch #256: Loss:1.0707, Accuracy:0.4000, Validation Loss:1.0725, Validation Accuracy:0.3695\n",
    "Epoch #257: Loss:1.0707, Accuracy:0.4016, Validation Loss:1.0724, Validation Accuracy:0.3760\n",
    "Epoch #258: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0723, Validation Accuracy:0.3744\n",
    "Epoch #259: Loss:1.0710, Accuracy:0.4053, Validation Loss:1.0723, Validation Accuracy:0.3744\n",
    "Epoch #260: Loss:1.0710, Accuracy:0.4053, Validation Loss:1.0727, Validation Accuracy:0.3760\n",
    "Epoch #261: Loss:1.0709, Accuracy:0.4029, Validation Loss:1.0723, Validation Accuracy:0.3760\n",
    "Epoch #262: Loss:1.0712, Accuracy:0.4070, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #263: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0723, Validation Accuracy:0.3842\n",
    "Epoch #264: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0720, Validation Accuracy:0.3760\n",
    "Epoch #265: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0721, Validation Accuracy:0.3744\n",
    "Epoch #266: Loss:1.0708, Accuracy:0.4049, Validation Loss:1.0722, Validation Accuracy:0.3727\n",
    "Epoch #267: Loss:1.0717, Accuracy:0.4033, Validation Loss:1.0728, Validation Accuracy:0.3842\n",
    "Epoch #268: Loss:1.0710, Accuracy:0.4000, Validation Loss:1.0721, Validation Accuracy:0.3744\n",
    "Epoch #269: Loss:1.0711, Accuracy:0.4045, Validation Loss:1.0722, Validation Accuracy:0.3727\n",
    "Epoch #270: Loss:1.0719, Accuracy:0.3934, Validation Loss:1.0731, Validation Accuracy:0.3711\n",
    "Epoch #271: Loss:1.0709, Accuracy:0.4021, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #272: Loss:1.0714, Accuracy:0.4049, Validation Loss:1.0720, Validation Accuracy:0.3744\n",
    "Epoch #273: Loss:1.0708, Accuracy:0.4045, Validation Loss:1.0722, Validation Accuracy:0.3760\n",
    "Epoch #274: Loss:1.0707, Accuracy:0.3992, Validation Loss:1.0725, Validation Accuracy:0.3842\n",
    "Epoch #275: Loss:1.0710, Accuracy:0.4025, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #276: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0720, Validation Accuracy:0.3744\n",
    "Epoch #277: Loss:1.0708, Accuracy:0.4053, Validation Loss:1.0721, Validation Accuracy:0.3777\n",
    "Epoch #278: Loss:1.0708, Accuracy:0.3988, Validation Loss:1.0722, Validation Accuracy:0.3941\n",
    "Epoch #279: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #280: Loss:1.0708, Accuracy:0.4016, Validation Loss:1.0724, Validation Accuracy:0.3777\n",
    "Epoch #281: Loss:1.0707, Accuracy:0.4012, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #282: Loss:1.0706, Accuracy:0.4062, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #283: Loss:1.0708, Accuracy:0.4053, Validation Loss:1.0723, Validation Accuracy:0.3695\n",
    "Epoch #284: Loss:1.0708, Accuracy:0.4033, Validation Loss:1.0724, Validation Accuracy:0.3777\n",
    "Epoch #285: Loss:1.0707, Accuracy:0.4016, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #286: Loss:1.0708, Accuracy:0.4066, Validation Loss:1.0721, Validation Accuracy:0.3744\n",
    "Epoch #287: Loss:1.0711, Accuracy:0.4016, Validation Loss:1.0724, Validation Accuracy:0.3777\n",
    "Epoch #288: Loss:1.0708, Accuracy:0.4012, Validation Loss:1.0721, Validation Accuracy:0.3777\n",
    "Epoch #289: Loss:1.0707, Accuracy:0.4057, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #290: Loss:1.0707, Accuracy:0.4062, Validation Loss:1.0720, Validation Accuracy:0.3727\n",
    "Epoch #291: Loss:1.0708, Accuracy:0.4062, Validation Loss:1.0722, Validation Accuracy:0.3760\n",
    "Epoch #292: Loss:1.0707, Accuracy:0.4021, Validation Loss:1.0722, Validation Accuracy:0.3760\n",
    "Epoch #293: Loss:1.0706, Accuracy:0.4029, Validation Loss:1.0723, Validation Accuracy:0.3777\n",
    "Epoch #294: Loss:1.0711, Accuracy:0.4041, Validation Loss:1.0721, Validation Accuracy:0.3727\n",
    "Epoch #295: Loss:1.0708, Accuracy:0.4049, Validation Loss:1.0725, Validation Accuracy:0.3777\n",
    "Epoch #296: Loss:1.0710, Accuracy:0.4029, Validation Loss:1.0721, Validation Accuracy:0.3777\n",
    "Epoch #297: Loss:1.0711, Accuracy:0.4041, Validation Loss:1.0722, Validation Accuracy:0.3727\n",
    "Epoch #298: Loss:1.0713, Accuracy:0.4053, Validation Loss:1.0721, Validation Accuracy:0.3760\n",
    "Epoch #299: Loss:1.0707, Accuracy:0.4062, Validation Loss:1.0720, Validation Accuracy:0.3760\n",
    "Epoch #300: Loss:1.0708, Accuracy:0.4037, Validation Loss:1.0723, Validation Accuracy:0.3777\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07232344, Accuracy:0.3777\n",
    "Labels: ['01', '03', '02']\n",
    "Confusion Matrix:\n",
    "       01  03  02\n",
    "t:01  166   0  74\n",
    "t:03  103   0  39\n",
    "t:02  163   0  64\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.38      0.69      0.49       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "          02       0.36      0.28      0.32       227\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.25      0.32      0.27       609\n",
    "weighted avg       0.29      0.38      0.31       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 18:25:50 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 31 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0821549651658007, 1.0752342896312719, 1.0748955038772232, 1.075779139701956, 1.074888717756287, 1.0743883014312519, 1.0743969029002198, 1.0741600035055126, 1.0742980185009183, 1.074145443333781, 1.0741146963413908, 1.074084962921581, 1.0741350607723241, 1.0740881612148192, 1.0740593429073715, 1.0741103591981584, 1.0740321032910902, 1.073972249657454, 1.073943052973066, 1.0739169015085757, 1.0739168513976098, 1.0739060563994158, 1.0738515918478002, 1.073929928402204, 1.073948596890141, 1.0739468847002303, 1.073759894065669, 1.0736631331185402, 1.0738136116507018, 1.0736940588269914, 1.0736638834128043, 1.0735792825961936, 1.0734624289135235, 1.0734755106160205, 1.0735783253984499, 1.073486658739926, 1.0734255948090201, 1.0734625318758986, 1.073464012302593, 1.0734108334104415, 1.0733339066184409, 1.0734358556164896, 1.0737011916140227, 1.0735982196475877, 1.07354508225358, 1.0734383572498565, 1.0733296467948625, 1.0733844705403144, 1.0735982065326084, 1.0737075468981012, 1.074159665256494, 1.0747330666371362, 1.0746919576366156, 1.0738673625125479, 1.074509060245821, 1.0746610342771157, 1.0745444282130852, 1.0748940259952264, 1.0751468097830836, 1.0750504949409974, 1.0753257534969811, 1.07550992832591, 1.074737396733514, 1.0744184308451386, 1.0759859715384998, 1.0754811004269103, 1.07517776915984, 1.0748342271704587, 1.0744421765917824, 1.0742114153988842, 1.074780031378046, 1.075084958561927, 1.0752533515685885, 1.0750231404218376, 1.0749380866490756, 1.0763997878934362, 1.0767714198195484, 1.0760565267995073, 1.0761823090426441, 1.07653493618926, 1.0764315249688912, 1.0765434558364166, 1.0762715711577968, 1.0761686722046049, 1.0762114544415904, 1.0753931198605566, 1.0745109049557464, 1.0756906523493124, 1.075529299541843, 1.0755463065380728, 1.0752233566321763, 1.0750728113506423, 1.0752737666977255, 1.0752551860997241, 1.0768547126616554, 1.0771102858294408, 1.0779364443764898, 1.0770526149590027, 1.0740542157334452, 1.0750357492020957, 1.0753088950719347, 1.0756639725664763, 1.0749568046607407, 1.0756241795660435, 1.0747641618615889, 1.074610645939368, 1.0758666023244998, 1.0750245860057512, 1.074453845399941, 1.074437423879877, 1.074457054654953, 1.0746160056594949, 1.074807023962926, 1.0747439769297007, 1.0747970565786502, 1.0747242559157373, 1.0746903225706128, 1.0751700859351698, 1.0748363093202338, 1.0751831392545026, 1.0751298883278382, 1.0751029006366073, 1.074842435973031, 1.0756623880029312, 1.0749984451115424, 1.074864012071456, 1.0752880230717274, 1.0748938991518444, 1.074880355684628, 1.0751138532102988, 1.0749685903292376, 1.074868351367894, 1.074863368262994, 1.0750390839302677, 1.0750726408559113, 1.074918630479396, 1.0749187385311658, 1.0749747684632225, 1.0753255176230996, 1.0749632570348153, 1.0749832430888084, 1.0749455328253885, 1.0750014386545066, 1.075113648068533, 1.0747698889968822, 1.0747862995551725, 1.0748821074152228, 1.074958119877845, 1.0748129100439388, 1.0747968459559976, 1.0760477730401827, 1.0746076938945477, 1.0747400837383052, 1.074709327741601, 1.0744853828145169, 1.0745320705748935, 1.0755598584223656, 1.0750881543104676, 1.0742857260461316, 1.075034790829876, 1.0742555500447064, 1.0741655059244442, 1.0741648858012434, 1.0741883389076772, 1.0742421700253666, 1.074175180472764, 1.0743650771518451, 1.0741549103913832, 1.074241305806954, 1.0739829786892594, 1.0739632864499522, 1.074052661510524, 1.07397664767768, 1.0738962559864438, 1.0740839213573288, 1.0740776412396986, 1.0737146787064025, 1.0737577257876718, 1.0737822999312177, 1.07374556389544, 1.0737013615215159, 1.073749094369572, 1.0735902549402272, 1.073580007443483, 1.0733621952373211, 1.073404839081913, 1.0749572398040095, 1.0734612830166745, 1.0733020712785142, 1.073554058380315, 1.0735925557382393, 1.0731947286962875, 1.0733012518859262, 1.072982248805818, 1.073559654561561, 1.0729252650036991, 1.0730973517366231, 1.073453417161024, 1.072752248477466, 1.0727778859130659, 1.0731249900874247, 1.0733789671426532, 1.0732235624676658, 1.0729516030140893, 1.0737804487616753, 1.0724841000019818, 1.0724891126645217, 1.0726039922491866, 1.0733115978428882, 1.0725527684676823, 1.0727271941690806, 1.072625882715623, 1.0725500701096258, 1.072820475340281, 1.0724465886164574, 1.0725869633294092, 1.0724778962252763, 1.0724701239361942, 1.0727279994679593, 1.0731073835213196, 1.072536606898253, 1.072640426444694, 1.0727878591697204, 1.0724954665783786, 1.0723860291228897, 1.0725121948323617, 1.0730101817542892, 1.072317869009447, 1.0723185625373828, 1.0723692247237282, 1.0726987143063975, 1.0721139359748226, 1.0723515402507313, 1.0723872259136882, 1.0722545123061131, 1.0730129304190574, 1.0731644458175684, 1.0726999206887482, 1.072624054252612, 1.0723870560061952, 1.072147928630973, 1.0725143353144329, 1.0724722068689532, 1.0727732156102097, 1.0721648581118028, 1.0723383203515866, 1.072329331892856, 1.0722487671621914, 1.0722449508989582, 1.0725119016049138, 1.0722464751727476, 1.0727466981007743, 1.0723807005263706, 1.072457941294891, 1.0721626025311073, 1.0725263687973148, 1.0724391802191147, 1.0722863795526314, 1.072261539586072, 1.072710765993654, 1.0723003150990051, 1.0720364753835894, 1.0722596985953194, 1.0719892074321877, 1.0720514910561698, 1.0722215733504648, 1.072838651527129, 1.0721160279118955, 1.0721873395352919, 1.0730898219767855, 1.071990927451937, 1.0720034318995986, 1.0722276246410676, 1.0725074430991863, 1.0724636578599025, 1.0720126681727142, 1.0721431845318898, 1.0721882366390258, 1.0720130608391096, 1.0723859020837618, 1.0720295126997974, 1.0720061196873731, 1.0722882307221737, 1.0723992871924966, 1.072009255341904, 1.0721139813878853, 1.0723741422537316, 1.0720946279848347, 1.0720034162399217, 1.0720168530255898, 1.0722030765317343, 1.0722218364330347, 1.0723035327710932, 1.072108170473321, 1.072499570196681, 1.0721318122788603, 1.0721535950850187, 1.0721214350025445, 1.0720424634482473, 1.0723235667828463], 'val_acc': [0.39901477739532987, 0.38916256045081543, 0.39408866892307265, 0.39408866892307265, 0.37766830767512516, 0.3957307052436133, 0.38095237992471465, 0.3842364520764312, 0.38916256045081543, 0.3908045965756102, 0.3825944160495094, 0.3908045965756102, 0.39408866892307265, 0.3924466327004049, 0.3875205243260207, 0.38587848820122594, 0.38587848820122594, 0.3924466327004049, 0.3908045965756102, 0.3908045965756102, 0.39244663289615084, 0.4121510666873067, 0.38916256045081543, 0.38916256045081543, 0.3908045965756102, 0.39408866892307265, 0.38916256054868836, 0.3990147771995839, 0.3908045967713561, 0.39080459667348316, 0.39080459667348316, 0.3875205244238936, 0.39080459667348316, 0.39080459667348316, 0.3875205244238936, 0.39408866892307265, 0.38916256054868836, 0.38916256054868836, 0.39737274097691616, 0.3990147771017109, 0.3858784882990989, 0.38587848810335296, 0.3908045963798642, 0.38916256035294244, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3957307048521214, 0.3924466327004049, 0.38095237963109574, 0.38587848800547997, 0.3990147774932028, 0.38587848820122594, 0.3875205242281477, 0.39573070465637544, 0.3908045964777372, 0.39244663260253193, 0.3990147770038379, 0.3908045964777372, 0.3842364519785582, 0.3842364519785582, 0.38095237992471465, 0.38587848800547997, 0.38587848800547997, 0.3760262715503304, 0.3842364520764312, 0.37766830728363326, 0.37766830767512516, 0.37602627145245743, 0.39737274117266214, 0.38916256045081543, 0.3875205243260207, 0.37766830747937924, 0.38095237953322275, 0.39408866892307265, 0.38095237982684166, 0.3678160907306107, 0.39080459667348316, 0.38916256054868836, 0.36124794613355876, 0.38587848820122594, 0.36945812695327845, 0.366174054507943, 0.3858784882990989, 0.3842364520764312, 0.38587848820122594, 0.3875205243260207, 0.3875205244238936, 0.37438423542553567, 0.3760262715503304, 0.3825944159516364, 0.3743842350340438, 0.3825944159516364, 0.3825944159516364, 0.37274219920286794, 0.3711001629802002, 0.38423645217430413, 0.37438423552340866, 0.379310343604174, 0.39080459667348316, 0.38916256054868836, 0.3694581260724216, 0.38423645217430413, 0.38916256054868836, 0.37274219910499495, 0.37438423542553567, 0.3875205244238936, 0.37274219910499495, 0.38423645217430413, 0.37438423542553567, 0.38423645217430413, 0.38423645217430413, 0.3875205244238936, 0.37438423542553567, 0.37438423542553567, 0.37438423542553567, 0.37438423542553567, 0.38423645217430413, 0.37438423542553567, 0.37274219910499495, 0.38423645217430413, 0.3678160908284837, 0.37438423542553567, 0.3875205244238936, 0.3711001630780732, 0.37438423542553567, 0.38423645217430413, 0.37602627145245743, 0.37602627145245743, 0.38423645217430413, 0.38423645217430413, 0.37438423542553567, 0.3678160908284837, 0.3678160908284837, 0.37274219910499495, 0.37438423542553567, 0.3678160908284837, 0.37438423542553567, 0.38423645217430413, 0.3678160908284837, 0.3711001630780732, 0.3711001630780732, 0.38423645217430413, 0.38423645217430413, 0.37602627145245743, 0.37602627145245743, 0.36945812685540547, 0.3678160908284837, 0.3711001630780732, 0.3711001630780732, 0.3875205244238936, 0.37438423542553567, 0.3711001630780732, 0.38423645217430413, 0.3711001630780732, 0.37602627145245743, 0.3875205244238936, 0.38259441585376347, 0.37602627145245743, 0.38916256054868836, 0.36945812685540547, 0.37602627145245743, 0.37602627145245743, 0.37602627145245743, 0.37274219920286794, 0.3908045967713561, 0.38423645217430413, 0.38423645217430413, 0.37438423542553567, 0.37602627145245743, 0.37602627145245743, 0.3678160908284837, 0.3678160908284837, 0.3711001630780732, 0.3678160908284837, 0.3678160908284837, 0.37274219930074093, 0.37438423542553567, 0.3678160908284837, 0.37602627145245743, 0.37274219920286794, 0.36945812685540547, 0.36945812685540547, 0.36945812685540547, 0.37602627145245743, 0.3743842352297897, 0.3875205244238936, 0.37602627145245743, 0.37274219920286794, 0.3678160908284837, 0.3678160908284837, 0.3743842352297897, 0.3678160908284837, 0.37274219930074093, 0.38423645217430413, 0.37438423542553567, 0.3678160908284837, 0.38423645217430413, 0.3711001630780732, 0.3743842352297897, 0.38423645217430413, 0.38423645217430413, 0.3825944157558905, 0.3678160908284837, 0.3875205244238936, 0.37438423542553567, 0.37438423542553567, 0.3711001630780732, 0.38423645217430413, 0.3743842352297897, 0.3678160908284837, 0.37602627145245743, 0.3711001630780732, 0.37438423542553567, 0.37602627145245743, 0.3678160908284837, 0.37274219920286794, 0.37438423542553567, 0.37602627145245743, 0.3678160908284837, 0.37274219920286794, 0.37602627145245743, 0.37602627145245743, 0.37766830757725217, 0.37274219920286794, 0.37602627145245743, 0.3678160908284837, 0.37274219920286794, 0.3743842352297897, 0.37602627145245743, 0.3678160908284837, 0.37438423542553567, 0.36945812685540547, 0.37602627145245743, 0.37274219920286794, 0.3678160908284837, 0.38423645217430413, 0.38259441585376347, 0.38423645217430413, 0.3678160908284837, 0.37274219920286794, 0.36945812685540547, 0.37766830757725217, 0.38423645217430413, 0.37274219920286794, 0.37766830757725217, 0.39244663289615084, 0.37438423542553567, 0.37274219920286794, 0.37602627145245743, 0.37274219920286794, 0.3678160908284837, 0.37766830757725217, 0.37766830757725217, 0.37274219920286794, 0.36945812685540547, 0.37602627145245743, 0.37438423542553567, 0.3743842352297897, 0.37602627145245743, 0.37602627135458444, 0.37274219920286794, 0.38423645217430413, 0.37602627135458444, 0.37438423542553567, 0.37274219920286794, 0.38423645217430413, 0.3743842352297897, 0.37274219920286794, 0.3711001630780732, 0.37274219920286794, 0.3743842352297897, 0.37602627145245743, 0.38423645217430413, 0.3908045967713561, 0.3743842352297897, 0.37766830757725217, 0.39408866892307265, 0.37274219920286794, 0.37766830757725217, 0.37274219920286794, 0.37274219920286794, 0.36945812685540547, 0.37766830757725217, 0.37274219920286794, 0.3743842352297897, 0.37766830757725217, 0.37766830757725217, 0.37274219920286794, 0.37274219920286794, 0.37602627135458444, 0.37602627135458444, 0.37766830757725217, 0.37274219920286794, 0.37766830757725217, 0.37766830757725217, 0.37274219920286794, 0.37602627135458444, 0.37602627135458444, 0.37766830757725217], 'loss': [1.0915176748495081, 1.0786567430476632, 1.0744930332446245, 1.074586942259536, 1.0749216461083728, 1.0745948626520208, 1.0742509783171041, 1.074113441688569, 1.0741691652509466, 1.074082271176442, 1.0740040276819185, 1.074049355068246, 1.0743524943533864, 1.0738962462305779, 1.0737677559960304, 1.0738300633381523, 1.074020529625597, 1.0737249455168016, 1.0738040792379047, 1.073907429334809, 1.0737806218850294, 1.0740753107736734, 1.073927041588378, 1.0737038879668688, 1.0735892136728495, 1.073560293890857, 1.073721925433901, 1.0736426864071793, 1.0735985189003132, 1.073539049033022, 1.0735674000863422, 1.0735448628235646, 1.0735951211662997, 1.0734288584280307, 1.0733987400908735, 1.0735023790316416, 1.073405463396891, 1.073233298942049, 1.0731938203991807, 1.0731957585904632, 1.07312231548513, 1.0738834039876104, 1.0731747836302927, 1.0730882566322781, 1.072965091993187, 1.072995544018442, 1.0727647266348774, 1.072805067694897, 1.072565395582383, 1.0726866982066412, 1.0729513453751864, 1.0734788823666268, 1.0732177181165565, 1.072638503922574, 1.0720433856917113, 1.07236609263097, 1.0725539799343633, 1.0720938586111677, 1.0721028350951491, 1.0719526615475727, 1.0728009571529757, 1.0716253472059905, 1.0721428310112298, 1.0731967744396453, 1.072586184948132, 1.073119512867389, 1.0730029327423911, 1.0727257548905986, 1.0730696797126127, 1.073152729666943, 1.0728851148969583, 1.0729285423760542, 1.072624662622534, 1.0726861723639391, 1.0720081612804342, 1.0726737722479098, 1.0723036223123696, 1.0722766994695643, 1.0722795318529101, 1.0724252553446336, 1.0719911876400394, 1.0720226184053832, 1.0717729631145876, 1.0735646576852034, 1.0726129904909545, 1.0723808055051298, 1.0729842409705723, 1.0718987650450251, 1.0715934728939676, 1.0719258764686037, 1.0713036865179544, 1.0710548937932667, 1.0721427474912921, 1.0713862137628043, 1.0711650483662098, 1.0714800091005203, 1.071465145342159, 1.0725327565684701, 1.0723765835380163, 1.0725295481985355, 1.0744754290923446, 1.0724031683355877, 1.072970963601459, 1.0734755070057738, 1.0727007627487182, 1.0731548616283972, 1.0735202462766205, 1.0735064850695568, 1.0728931755500652, 1.072939334366111, 1.0726686617676973, 1.072802874978318, 1.0722854609362154, 1.0726398371083536, 1.0723200221326072, 1.0730283551637152, 1.072574719315437, 1.072391640038461, 1.072523133759626, 1.0730150780393848, 1.0721898834808161, 1.0724176903280145, 1.073270406713231, 1.072524634621716, 1.0725438502290165, 1.0725596505758455, 1.072712274694345, 1.072272226355159, 1.072720665412762, 1.0722072937650113, 1.072151392881875, 1.0720327218210428, 1.0720193141294945, 1.0722441464723749, 1.0720161262234134, 1.07237500228921, 1.0718910502212493, 1.0725427949697819, 1.0718676553859359, 1.0726267656506454, 1.07207695845461, 1.0721936951427733, 1.0723946121439063, 1.0722347294770227, 1.072201793639322, 1.0719384866573483, 1.0718152619974814, 1.0721582994323982, 1.071805476065289, 1.0737254104575094, 1.0717768900693074, 1.0724125181135455, 1.0719900501337385, 1.0725307231076688, 1.0723018605606267, 1.0719413258946162, 1.0744099722261056, 1.0718632691205159, 1.0729014925888187, 1.072163663703558, 1.072129338477916, 1.0716913597294926, 1.0718467951310489, 1.0717529885333177, 1.0716248911753816, 1.0719311563875642, 1.0715898722838573, 1.0717563065660074, 1.07179246658662, 1.0715142020454642, 1.0717586533489658, 1.071711881547493, 1.071466458798434, 1.0717300504629617, 1.0713420212880786, 1.0714661090525759, 1.0715818973047777, 1.0715815636656367, 1.0716766347630557, 1.0713954845248306, 1.0714929550335393, 1.0714472191534492, 1.0714199740294315, 1.0714416322766878, 1.0713556287225021, 1.0727292601332772, 1.0711803076937947, 1.071927278781084, 1.0717342254317517, 1.0711139945768477, 1.0714744831257532, 1.0715037805588583, 1.0715966855231251, 1.0714524864416102, 1.0713176335152659, 1.0718511195153426, 1.0710955280053298, 1.071194965051185, 1.071174607629404, 1.0716546502201465, 1.0710505816970763, 1.0724479840766232, 1.0723712012997888, 1.071404260531588, 1.0714406292541316, 1.0711458364306534, 1.0710785023730394, 1.0711979929671396, 1.0720932764683906, 1.0718026286033144, 1.07131234664447, 1.0710165730247263, 1.0709696129362196, 1.0712160811042395, 1.071056276669982, 1.0710669495486627, 1.0709814250346816, 1.0708775136015498, 1.0710127575931119, 1.071083761877103, 1.0712279832338651, 1.0709527120942697, 1.0710595881914455, 1.0708417650610516, 1.070967328181257, 1.07111059131074, 1.0712545464415815, 1.0706943034636167, 1.0708915512175041, 1.070872451294619, 1.071088581359362, 1.0710444488564557, 1.0710373698318763, 1.0707905257751809, 1.0716404741060073, 1.0706488163319456, 1.0718718485665761, 1.072573654362798, 1.0719548132385317, 1.0709844125124952, 1.0712645631061688, 1.0708921087840744, 1.0709737263665797, 1.0710354920040654, 1.0711209177726104, 1.070885933596006, 1.0709067782826982, 1.0708342972722142, 1.0708206351532827, 1.0713379457011605, 1.0710561657833122, 1.0709918290927425, 1.070878329169334, 1.0708819354584085, 1.0711759140359303, 1.0707386916667774, 1.0707449758322087, 1.0708601503156783, 1.0710373805044122, 1.0709616476505444, 1.0709345255544298, 1.0711816469012345, 1.070873893849414, 1.070923937370645, 1.0708689800278117, 1.0708296812045746, 1.071687405946563, 1.0710061941303513, 1.0710819513645995, 1.071864591829586, 1.0708914568292043, 1.0713700861901474, 1.0707970732780943, 1.0707307950182372, 1.0709509207237917, 1.0710035743165065, 1.0707696299778118, 1.0707633002827543, 1.0708179964421956, 1.0707758728238836, 1.070696250467085, 1.0706232549718273, 1.0707859682106629, 1.0708315025364838, 1.0707085270656453, 1.0707889268530468, 1.0711456440802227, 1.0707521310821941, 1.07068271578215, 1.0706511066190025, 1.0707899136709726, 1.0707216190361635, 1.07056644653148, 1.071101316828013, 1.0708018047853662, 1.0709532994754016, 1.0711394450013398, 1.0712688795105387, 1.070650108394192, 1.0708398037622597], 'acc': [0.3679671475652307, 0.3999999997062605, 0.39507186797120486, 0.3946611897289386, 0.39548254817173467, 0.39794660969436535, 0.40246406447226507, 0.3987679692754021, 0.3991786429402275, 0.39712525821564376, 0.3979466142350888, 0.3999999999755217, 0.39917864630599287, 0.4012320329888401, 0.39958932392406266, 0.39958932153742904, 0.39383983449279897, 0.40369610088806623, 0.401232034983821, 0.3975359362253663, 0.4004106770061125, 0.3946611909406141, 0.39958932294493094, 0.398767968100444, 0.39671457883513683, 0.40287474545610025, 0.40287474310618404, 0.40657084202374766, 0.4008213575615775, 0.39630389941791244, 0.39958932372823636, 0.40041067818107057, 0.40000000232543786, 0.40287474428114217, 0.4024640648639178, 0.4012320343963419, 0.40041067681028614, 0.40246406803385676, 0.40123203181388195, 0.3983572873124352, 0.398767964930505, 0.3983572902865478, 0.4012320330255575, 0.3991786466976456, 0.4016427127977171, 0.393429159616298, 0.39835729048237417, 0.39548254793919085, 0.4036960967389955, 0.3963039007886969, 0.3954825465684064, 0.3905544149557423, 0.4016427082569937, 0.3963038996137388, 0.4049281319798385, 0.3975359362253663, 0.39425051113173704, 0.39383983511699544, 0.3979466098901917, 0.39794661126097614, 0.39301848203494566, 0.40616016283906703, 0.4000000019337852, 0.3971252564532067, 0.402464066467246, 0.4024640672505514, 0.40205339002413426, 0.4028747428736403, 0.3901437353426916, 0.4020533898283079, 0.4036961001047608, 0.40123203083475023, 0.3991786457185138, 0.4065708442145549, 0.39835728793663167, 0.39630390082541433, 0.40657084006548416, 0.40328542307417004, 0.39917864352770654, 0.4008213575615775, 0.39835728848739327, 0.3987679667296596, 0.4004106771652214, 0.3848049263445014, 0.4024640642764387, 0.39835728911158974, 0.38316221743883294, 0.3987679677087913, 0.401232034983821, 0.4020533868541952, 0.40246406368895965, 0.40369609634734277, 0.39548254696005913, 0.4061601662048324, 0.3975359344629292, 0.3934291602404945, 0.4032854217033856, 0.3975359350504082, 0.39630390121706704, 0.40246406368895965, 0.3967145770359823, 0.39835728770408785, 0.39342915726638183, 0.39835729146150595, 0.39794661126097614, 0.4012320316180556, 0.3963039025878515, 0.40082135736575114, 0.40287474306946663, 0.3987679657138104, 0.39876796829627037, 0.3967145778560051, 0.4008213543916385, 0.39876796590963676, 0.4008213559949667, 0.39876796947122845, 0.39917864532686115, 0.3995893213416027, 0.3975359362253663, 0.3872689933869873, 0.40041067579443695, 0.3938398372710853, 0.3975359322721219, 0.40205339002413426, 0.3954825477433645, 0.4024640648639178, 0.3971252556331838, 0.40246406803385676, 0.4004106770061125, 0.39753593325125364, 0.399999999583869, 0.4020533898283079, 0.3987679651630488, 0.4012320324380785, 0.4004106775568741, 0.4020533888491762, 0.40205338865334983, 0.3987679684920967, 0.40041067974768135, 0.39958932137832015, 0.4016427126018908, 0.4049281292015522, 0.4049281304132277, 0.4004106779852442, 0.40287474248198757, 0.40041067622280707, 0.40082135677827213, 0.39917864708929823, 0.4020533894366552, 0.3815195049715728, 0.40328542131173295, 0.40041067935602864, 0.39999999977969536, 0.39219712761882886, 0.4004106755986106, 0.4053388105770401, 0.39876796829627037, 0.4008213545874649, 0.4045174523667878, 0.40451745612420587, 0.3954825485266699, 0.4020533902199606, 0.3995893247073681, 0.40123203361303655, 0.40287474150285585, 0.403696097950671, 0.4028747456519266, 0.3991786451310348, 0.39876796551798405, 0.39917864591434016, 0.4028747436936631, 0.39835728989489516, 0.4024640672505514, 0.4065708424521178, 0.4012320329888401, 0.4020533868541952, 0.4004106767735687, 0.40082135537077024, 0.4045174559283795, 0.40451745416594237, 0.40205339002413426, 0.40369609634734277, 0.4008213544283559, 0.402053388420806, 0.4028747452602739, 0.40328542049171007, 0.3995893247073681, 0.4016427100194308, 0.404928133350623, 0.4061601662048324, 0.3991786466976456, 0.39958932294493094, 0.4028747452602739, 0.4004106755986106, 0.4036960965431691, 0.4008213569740985, 0.40410677811448337, 0.40287474150285585, 0.3975359322721219, 0.39794661403926246, 0.4032854232699964, 0.3979466138434361, 0.3963039031753305, 0.4036960973264745, 0.4016427124060645, 0.40944558652519447, 0.40287474291035774, 0.40492813174729475, 0.39671457824765777, 0.400410679551855, 0.4078028770687644, 0.40369609971310816, 0.40164271021525716, 0.4016427100194308, 0.40164270923612544, 0.39753593524623454, 0.4024640662347022, 0.4061601634265461, 0.4016427108394537, 0.40041067579443695, 0.40616016323071974, 0.4049281303765103, 0.4004106799435077, 0.4004106783768969, 0.40328542131173295, 0.40616016205576166, 0.39876796692548594, 0.3991786447393821, 0.40574948721597814, 0.4049281331180792, 0.4020533864625426, 0.4036960967389955, 0.402053387833327, 0.39958932176997286, 0.4012320338088629, 0.40451745217096147, 0.39630390039704416, 0.39753593524623454, 0.4032854206875364, 0.4024640660755933, 0.3983572912656796, 0.4045174559283795, 0.4057494889784153, 0.40041067935602864, 0.4053388089737119, 0.4032854220950383, 0.3958932247739553, 0.4024640672505514, 0.4032854226825174, 0.40164270884447273, 0.403285422486691, 0.4000000007588271, 0.40164270884447273, 0.4008213539999858, 0.40657084065296323, 0.40000000134630614, 0.4016427118185854, 0.3999999986047373, 0.40533880936536454, 0.40533881100541025, 0.4028747426778139, 0.40698152183262476, 0.401232034983821, 0.3999999985680198, 0.40123203181388195, 0.4049281295932049, 0.40328542052842753, 0.40000000212961156, 0.40451745416594237, 0.39342915926136274, 0.4020533882616971, 0.40492813315479664, 0.404517453970116, 0.3991786447393821, 0.40246406666307233, 0.40328542131173295, 0.4053388085820592, 0.39876796790461766, 0.4004106765777423, 0.4016427114269327, 0.4012320338088629, 0.4061601642098515, 0.4053388109686928, 0.40328541951257835, 0.4016427128344346, 0.4065708404571369, 0.4016427108394537, 0.4012320326339048, 0.4057494889784153, 0.406160162251588, 0.40616016283906703, 0.4020533866583689, 0.40287474150285585, 0.4041067767436989, 0.40492813276314393, 0.4028747446360774, 0.4041067741979564, 0.4053388083862328, 0.40616016264324073, 0.4036960967389955]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
