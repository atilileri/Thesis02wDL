{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf30.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 14:54:22 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '3', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002BD02235E10>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002BD66676EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1063, Accuracy:0.3943, Validation Loss:1.0984, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0944, Accuracy:0.3943, Validation Loss:1.0894, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0869, Accuracy:0.3943, Validation Loss:1.0837, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0817, Accuracy:0.3943, Validation Loss:1.0796, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0780, Accuracy:0.3943, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0755, Accuracy:0.3943, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0740, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #10: Loss:1.0742, Accuracy:0.4037, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #11: Loss:1.0744, Accuracy:0.3963, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0742, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #14: Loss:1.0740, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0739, Accuracy:0.3922, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #19: Loss:1.0741, Accuracy:0.3938, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0738, Accuracy:0.3918, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #22: Loss:1.0738, Accuracy:0.3938, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #23: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0738, Accuracy:0.3959, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #26: Loss:1.0739, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #27: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0737, Accuracy:0.3996, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #29: Loss:1.0739, Accuracy:0.3926, Validation Loss:1.0736, Validation Accuracy:0.4089\n",
    "Epoch #30: Loss:1.0740, Accuracy:0.3906, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0743, Accuracy:0.3910, Validation Loss:1.0744, Validation Accuracy:0.3727\n",
    "Epoch #33: Loss:1.0745, Accuracy:0.3733, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #35: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.4072\n",
    "Epoch #38: Loss:1.0742, Accuracy:0.3889, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #39: Loss:1.0741, Accuracy:0.3934, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #43: Loss:1.0743, Accuracy:0.3959, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #45: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0748, Accuracy:0.3774, Validation Loss:1.0743, Validation Accuracy:0.3662\n",
    "Epoch #48: Loss:1.0742, Accuracy:0.3856, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #49: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #50: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #51: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0740, Accuracy:0.3934, Validation Loss:1.0740, Validation Accuracy:0.4089\n",
    "Epoch #54: Loss:1.0740, Accuracy:0.3955, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #55: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0742, Accuracy:0.3938, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #57: Loss:1.0742, Accuracy:0.3955, Validation Loss:1.0737, Validation Accuracy:0.4039\n",
    "Epoch #58: Loss:1.0745, Accuracy:0.3967, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #59: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #61: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #64: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #65: Loss:1.0741, Accuracy:0.3988, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #66: Loss:1.0741, Accuracy:0.3955, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #67: Loss:1.0742, Accuracy:0.3938, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #68: Loss:1.0740, Accuracy:0.3934, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #69: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #70: Loss:1.0740, Accuracy:0.3959, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #72: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #73: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #74: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #75: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0742, Accuracy:0.3881, Validation Loss:1.0736, Validation Accuracy:0.4154\n",
    "Epoch #77: Loss:1.0741, Accuracy:0.3914, Validation Loss:1.0735, Validation Accuracy:0.4072\n",
    "Epoch #78: Loss:1.0738, Accuracy:0.3992, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #79: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #80: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #81: Loss:1.0741, Accuracy:0.3971, Validation Loss:1.0735, Validation Accuracy:0.4138\n",
    "Epoch #82: Loss:1.0739, Accuracy:0.3992, Validation Loss:1.0734, Validation Accuracy:0.3908\n",
    "Epoch #83: Loss:1.0738, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #84: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #85: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #86: Loss:1.0740, Accuracy:0.3926, Validation Loss:1.0734, Validation Accuracy:0.3908\n",
    "Epoch #87: Loss:1.0740, Accuracy:0.3975, Validation Loss:1.0734, Validation Accuracy:0.3908\n",
    "Epoch #88: Loss:1.0739, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #89: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #90: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #91: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #92: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #93: Loss:1.0739, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #94: Loss:1.0739, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.4072\n",
    "Epoch #95: Loss:1.0740, Accuracy:0.3906, Validation Loss:1.0732, Validation Accuracy:0.4089\n",
    "Epoch #96: Loss:1.0737, Accuracy:0.4000, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #97: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #98: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #99: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.3908\n",
    "Epoch #100: Loss:1.0740, Accuracy:0.3934, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #101: Loss:1.0738, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.4138\n",
    "Epoch #102: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.4171\n",
    "Epoch #103: Loss:1.0738, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.3908\n",
    "Epoch #104: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #105: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #106: Loss:1.0738, Accuracy:0.3967, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #107: Loss:1.0738, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.4171\n",
    "Epoch #108: Loss:1.0738, Accuracy:0.4045, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #109: Loss:1.0739, Accuracy:0.3914, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #110: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #111: Loss:1.0740, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.4072\n",
    "Epoch #112: Loss:1.0740, Accuracy:0.3975, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #113: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #114: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #115: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0730, Validation Accuracy:0.4171\n",
    "Epoch #116: Loss:1.0738, Accuracy:0.4000, Validation Loss:1.0730, Validation Accuracy:0.4171\n",
    "Epoch #117: Loss:1.0738, Accuracy:0.3984, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #118: Loss:1.0739, Accuracy:0.3992, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #119: Loss:1.0737, Accuracy:0.3971, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #120: Loss:1.0738, Accuracy:0.3938, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #121: Loss:1.0737, Accuracy:0.3971, Validation Loss:1.0729, Validation Accuracy:0.3908\n",
    "Epoch #122: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0729, Validation Accuracy:0.3908\n",
    "Epoch #123: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0729, Validation Accuracy:0.4072\n",
    "Epoch #124: Loss:1.0737, Accuracy:0.4066, Validation Loss:1.0729, Validation Accuracy:0.3908\n",
    "Epoch #125: Loss:1.0738, Accuracy:0.3984, Validation Loss:1.0729, Validation Accuracy:0.3908\n",
    "Epoch #126: Loss:1.0737, Accuracy:0.4008, Validation Loss:1.0729, Validation Accuracy:0.4072\n",
    "Epoch #127: Loss:1.0739, Accuracy:0.3992, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #128: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #129: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0728, Validation Accuracy:0.3957\n",
    "Epoch #130: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #131: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #132: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #133: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #134: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #135: Loss:1.0737, Accuracy:0.3996, Validation Loss:1.0727, Validation Accuracy:0.4089\n",
    "Epoch #136: Loss:1.0737, Accuracy:0.4045, Validation Loss:1.0727, Validation Accuracy:0.4072\n",
    "Epoch #137: Loss:1.0740, Accuracy:0.3901, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #138: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #139: Loss:1.0742, Accuracy:0.3910, Validation Loss:1.0730, Validation Accuracy:0.4236\n",
    "Epoch #140: Loss:1.0739, Accuracy:0.4016, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #141: Loss:1.0735, Accuracy:0.3938, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #142: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #143: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #144: Loss:1.0736, Accuracy:0.4045, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #145: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #146: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #147: Loss:1.0738, Accuracy:0.4021, Validation Loss:1.0726, Validation Accuracy:0.4072\n",
    "Epoch #148: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #149: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #150: Loss:1.0736, Accuracy:0.3992, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #151: Loss:1.0736, Accuracy:0.4053, Validation Loss:1.0725, Validation Accuracy:0.4089\n",
    "Epoch #152: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #153: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #154: Loss:1.0738, Accuracy:0.4053, Validation Loss:1.0724, Validation Accuracy:0.4089\n",
    "Epoch #155: Loss:1.0734, Accuracy:0.4037, Validation Loss:1.0724, Validation Accuracy:0.3908\n",
    "Epoch #156: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0724, Validation Accuracy:0.3908\n",
    "Epoch #157: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0724, Validation Accuracy:0.3908\n",
    "Epoch #158: Loss:1.0735, Accuracy:0.3996, Validation Loss:1.0723, Validation Accuracy:0.3859\n",
    "Epoch #159: Loss:1.0735, Accuracy:0.4008, Validation Loss:1.0723, Validation Accuracy:0.3908\n",
    "Epoch #160: Loss:1.0733, Accuracy:0.4033, Validation Loss:1.0723, Validation Accuracy:0.4089\n",
    "Epoch #161: Loss:1.0733, Accuracy:0.4107, Validation Loss:1.0722, Validation Accuracy:0.3941\n",
    "Epoch #162: Loss:1.0733, Accuracy:0.4053, Validation Loss:1.0722, Validation Accuracy:0.3810\n",
    "Epoch #163: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0723, Validation Accuracy:0.3908\n",
    "Epoch #164: Loss:1.0737, Accuracy:0.3992, Validation Loss:1.0722, Validation Accuracy:0.3859\n",
    "Epoch #165: Loss:1.0734, Accuracy:0.4082, Validation Loss:1.0723, Validation Accuracy:0.3908\n",
    "Epoch #166: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0722, Validation Accuracy:0.3810\n",
    "Epoch #167: Loss:1.0733, Accuracy:0.4103, Validation Loss:1.0722, Validation Accuracy:0.4089\n",
    "Epoch #168: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0722, Validation Accuracy:0.3908\n",
    "Epoch #169: Loss:1.0735, Accuracy:0.4049, Validation Loss:1.0722, Validation Accuracy:0.4154\n",
    "Epoch #170: Loss:1.0738, Accuracy:0.3869, Validation Loss:1.0721, Validation Accuracy:0.3957\n",
    "Epoch #171: Loss:1.0732, Accuracy:0.4016, Validation Loss:1.0723, Validation Accuracy:0.3908\n",
    "Epoch #172: Loss:1.0739, Accuracy:0.3955, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #173: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0722, Validation Accuracy:0.4351\n",
    "Epoch #174: Loss:1.0734, Accuracy:0.4062, Validation Loss:1.0721, Validation Accuracy:0.3810\n",
    "Epoch #175: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0721, Validation Accuracy:0.3810\n",
    "Epoch #176: Loss:1.0732, Accuracy:0.3951, Validation Loss:1.0721, Validation Accuracy:0.3810\n",
    "Epoch #177: Loss:1.0732, Accuracy:0.4136, Validation Loss:1.0721, Validation Accuracy:0.4154\n",
    "Epoch #178: Loss:1.0733, Accuracy:0.4107, Validation Loss:1.0720, Validation Accuracy:0.3859\n",
    "Epoch #179: Loss:1.0732, Accuracy:0.4094, Validation Loss:1.0720, Validation Accuracy:0.3859\n",
    "Epoch #180: Loss:1.0732, Accuracy:0.4029, Validation Loss:1.0720, Validation Accuracy:0.3810\n",
    "Epoch #181: Loss:1.0731, Accuracy:0.4086, Validation Loss:1.0720, Validation Accuracy:0.3957\n",
    "Epoch #182: Loss:1.0735, Accuracy:0.3996, Validation Loss:1.0720, Validation Accuracy:0.3810\n",
    "Epoch #183: Loss:1.0730, Accuracy:0.4107, Validation Loss:1.0721, Validation Accuracy:0.4023\n",
    "Epoch #184: Loss:1.0733, Accuracy:0.4066, Validation Loss:1.0720, Validation Accuracy:0.3810\n",
    "Epoch #185: Loss:1.0732, Accuracy:0.4037, Validation Loss:1.0721, Validation Accuracy:0.3908\n",
    "Epoch #186: Loss:1.0730, Accuracy:0.4049, Validation Loss:1.0720, Validation Accuracy:0.4023\n",
    "Epoch #187: Loss:1.0731, Accuracy:0.4062, Validation Loss:1.0721, Validation Accuracy:0.4351\n",
    "Epoch #188: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0721, Validation Accuracy:0.3810\n",
    "Epoch #189: Loss:1.0729, Accuracy:0.4078, Validation Loss:1.0720, Validation Accuracy:0.3810\n",
    "Epoch #190: Loss:1.0730, Accuracy:0.4074, Validation Loss:1.0719, Validation Accuracy:0.3875\n",
    "Epoch #191: Loss:1.0730, Accuracy:0.4074, Validation Loss:1.0719, Validation Accuracy:0.3875\n",
    "Epoch #192: Loss:1.0734, Accuracy:0.4066, Validation Loss:1.0720, Validation Accuracy:0.3810\n",
    "Epoch #193: Loss:1.0728, Accuracy:0.4062, Validation Loss:1.0721, Validation Accuracy:0.4335\n",
    "Epoch #194: Loss:1.0731, Accuracy:0.3979, Validation Loss:1.0720, Validation Accuracy:0.3941\n",
    "Epoch #195: Loss:1.0728, Accuracy:0.4094, Validation Loss:1.0719, Validation Accuracy:0.3875\n",
    "Epoch #196: Loss:1.0728, Accuracy:0.4094, Validation Loss:1.0719, Validation Accuracy:0.3810\n",
    "Epoch #197: Loss:1.0729, Accuracy:0.4078, Validation Loss:1.0720, Validation Accuracy:0.3810\n",
    "Epoch #198: Loss:1.0729, Accuracy:0.4078, Validation Loss:1.0720, Validation Accuracy:0.3810\n",
    "Epoch #199: Loss:1.0732, Accuracy:0.3955, Validation Loss:1.0721, Validation Accuracy:0.4089\n",
    "Epoch #200: Loss:1.0726, Accuracy:0.4127, Validation Loss:1.0721, Validation Accuracy:0.3810\n",
    "Epoch #201: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0725, Validation Accuracy:0.3941\n",
    "Epoch #202: Loss:1.0726, Accuracy:0.4086, Validation Loss:1.0724, Validation Accuracy:0.4138\n",
    "Epoch #203: Loss:1.0729, Accuracy:0.4021, Validation Loss:1.0722, Validation Accuracy:0.3941\n",
    "Epoch #204: Loss:1.0726, Accuracy:0.4082, Validation Loss:1.0724, Validation Accuracy:0.3810\n",
    "Epoch #205: Loss:1.0725, Accuracy:0.4078, Validation Loss:1.0721, Validation Accuracy:0.3941\n",
    "Epoch #206: Loss:1.0725, Accuracy:0.4082, Validation Loss:1.0722, Validation Accuracy:0.4072\n",
    "Epoch #207: Loss:1.0725, Accuracy:0.4111, Validation Loss:1.0721, Validation Accuracy:0.3875\n",
    "Epoch #208: Loss:1.0724, Accuracy:0.4099, Validation Loss:1.0721, Validation Accuracy:0.3941\n",
    "Epoch #209: Loss:1.0724, Accuracy:0.4082, Validation Loss:1.0723, Validation Accuracy:0.3875\n",
    "Epoch #210: Loss:1.0725, Accuracy:0.4119, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #211: Loss:1.0722, Accuracy:0.4099, Validation Loss:1.0724, Validation Accuracy:0.3875\n",
    "Epoch #212: Loss:1.0724, Accuracy:0.4053, Validation Loss:1.0725, Validation Accuracy:0.3875\n",
    "Epoch #213: Loss:1.0733, Accuracy:0.3922, Validation Loss:1.0730, Validation Accuracy:0.3645\n",
    "Epoch #214: Loss:1.0719, Accuracy:0.4041, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #215: Loss:1.0726, Accuracy:0.3984, Validation Loss:1.0726, Validation Accuracy:0.3810\n",
    "Epoch #216: Loss:1.0723, Accuracy:0.4086, Validation Loss:1.0726, Validation Accuracy:0.3875\n",
    "Epoch #217: Loss:1.0721, Accuracy:0.4123, Validation Loss:1.0726, Validation Accuracy:0.3924\n",
    "Epoch #218: Loss:1.0719, Accuracy:0.4111, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #219: Loss:1.0717, Accuracy:0.4099, Validation Loss:1.0726, Validation Accuracy:0.3875\n",
    "Epoch #220: Loss:1.0718, Accuracy:0.4094, Validation Loss:1.0729, Validation Accuracy:0.3875\n",
    "Epoch #221: Loss:1.0717, Accuracy:0.4099, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #222: Loss:1.0719, Accuracy:0.4111, Validation Loss:1.0729, Validation Accuracy:0.3875\n",
    "Epoch #223: Loss:1.0715, Accuracy:0.4115, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #224: Loss:1.0713, Accuracy:0.4107, Validation Loss:1.0727, Validation Accuracy:0.3892\n",
    "Epoch #225: Loss:1.0716, Accuracy:0.4099, Validation Loss:1.0729, Validation Accuracy:0.3875\n",
    "Epoch #226: Loss:1.0713, Accuracy:0.4111, Validation Loss:1.0730, Validation Accuracy:0.3875\n",
    "Epoch #227: Loss:1.0714, Accuracy:0.4160, Validation Loss:1.0727, Validation Accuracy:0.3924\n",
    "Epoch #228: Loss:1.0720, Accuracy:0.4070, Validation Loss:1.0733, Validation Accuracy:0.3875\n",
    "Epoch #229: Loss:1.0705, Accuracy:0.4070, Validation Loss:1.0744, Validation Accuracy:0.3695\n",
    "Epoch #230: Loss:1.0719, Accuracy:0.3934, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #231: Loss:1.0715, Accuracy:0.3975, Validation Loss:1.0734, Validation Accuracy:0.3924\n",
    "Epoch #232: Loss:1.0708, Accuracy:0.4094, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #233: Loss:1.0706, Accuracy:0.4123, Validation Loss:1.0736, Validation Accuracy:0.3892\n",
    "Epoch #234: Loss:1.0708, Accuracy:0.4123, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #235: Loss:1.0707, Accuracy:0.4111, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #236: Loss:1.0704, Accuracy:0.4103, Validation Loss:1.0734, Validation Accuracy:0.3924\n",
    "Epoch #237: Loss:1.0709, Accuracy:0.4099, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #238: Loss:1.0703, Accuracy:0.4099, Validation Loss:1.0742, Validation Accuracy:0.3530\n",
    "Epoch #239: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #240: Loss:1.0724, Accuracy:0.3975, Validation Loss:1.0745, Validation Accuracy:0.3678\n",
    "Epoch #241: Loss:1.0716, Accuracy:0.4115, Validation Loss:1.0752, Validation Accuracy:0.3908\n",
    "Epoch #242: Loss:1.0729, Accuracy:0.4000, Validation Loss:1.0758, Validation Accuracy:0.3695\n",
    "Epoch #243: Loss:1.0708, Accuracy:0.3971, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #244: Loss:1.0711, Accuracy:0.4082, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #245: Loss:1.0702, Accuracy:0.3975, Validation Loss:1.0753, Validation Accuracy:0.3530\n",
    "Epoch #246: Loss:1.0703, Accuracy:0.4057, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #247: Loss:1.0702, Accuracy:0.4107, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #248: Loss:1.0696, Accuracy:0.4136, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #249: Loss:1.0704, Accuracy:0.3955, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #250: Loss:1.0701, Accuracy:0.4107, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #251: Loss:1.0698, Accuracy:0.4094, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #252: Loss:1.0698, Accuracy:0.4140, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #253: Loss:1.0699, Accuracy:0.4103, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #254: Loss:1.0697, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3990\n",
    "Epoch #255: Loss:1.0691, Accuracy:0.4107, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #256: Loss:1.0695, Accuracy:0.4123, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #257: Loss:1.0701, Accuracy:0.4107, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #258: Loss:1.0702, Accuracy:0.4123, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #259: Loss:1.0686, Accuracy:0.4111, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #260: Loss:1.0700, Accuracy:0.3984, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #261: Loss:1.0691, Accuracy:0.4078, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #262: Loss:1.0691, Accuracy:0.4094, Validation Loss:1.0757, Validation Accuracy:0.3448\n",
    "Epoch #263: Loss:1.0691, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #264: Loss:1.0688, Accuracy:0.4103, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #265: Loss:1.0688, Accuracy:0.4131, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #266: Loss:1.0689, Accuracy:0.4103, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #267: Loss:1.0687, Accuracy:0.4111, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #268: Loss:1.0707, Accuracy:0.3873, Validation Loss:1.0767, Validation Accuracy:0.3563\n",
    "Epoch #269: Loss:1.0698, Accuracy:0.4148, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #270: Loss:1.0691, Accuracy:0.4082, Validation Loss:1.0765, Validation Accuracy:0.3563\n",
    "Epoch #271: Loss:1.0692, Accuracy:0.3979, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #272: Loss:1.0686, Accuracy:0.4136, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #273: Loss:1.0687, Accuracy:0.4103, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #274: Loss:1.0682, Accuracy:0.4140, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #275: Loss:1.0687, Accuracy:0.4086, Validation Loss:1.0747, Validation Accuracy:0.3826\n",
    "Epoch #276: Loss:1.0683, Accuracy:0.4136, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #277: Loss:1.0687, Accuracy:0.4144, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #278: Loss:1.0687, Accuracy:0.3979, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #279: Loss:1.0691, Accuracy:0.4136, Validation Loss:1.0751, Validation Accuracy:0.3924\n",
    "Epoch #280: Loss:1.0697, Accuracy:0.4016, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #281: Loss:1.0689, Accuracy:0.4164, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #282: Loss:1.0683, Accuracy:0.4123, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #283: Loss:1.0687, Accuracy:0.4008, Validation Loss:1.0761, Validation Accuracy:0.3826\n",
    "Epoch #284: Loss:1.0694, Accuracy:0.4107, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #285: Loss:1.0686, Accuracy:0.4103, Validation Loss:1.0774, Validation Accuracy:0.3612\n",
    "Epoch #286: Loss:1.0685, Accuracy:0.4078, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #287: Loss:1.0683, Accuracy:0.4136, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #288: Loss:1.0680, Accuracy:0.4136, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #289: Loss:1.0683, Accuracy:0.4090, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #290: Loss:1.0682, Accuracy:0.4127, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #291: Loss:1.0687, Accuracy:0.4107, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #292: Loss:1.0681, Accuracy:0.4115, Validation Loss:1.0763, Validation Accuracy:0.3842\n",
    "Epoch #293: Loss:1.0683, Accuracy:0.4136, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #294: Loss:1.0705, Accuracy:0.4078, Validation Loss:1.0753, Validation Accuracy:0.3875\n",
    "Epoch #295: Loss:1.0693, Accuracy:0.4033, Validation Loss:1.0784, Validation Accuracy:0.3612\n",
    "Epoch #296: Loss:1.0699, Accuracy:0.4066, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #297: Loss:1.0682, Accuracy:0.4148, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #298: Loss:1.0684, Accuracy:0.4111, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #299: Loss:1.0678, Accuracy:0.4140, Validation Loss:1.0745, Validation Accuracy:0.3859\n",
    "Epoch #300: Loss:1.0683, Accuracy:0.4160, Validation Loss:1.0737, Validation Accuracy:0.3859\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07371855, Accuracy:0.3859\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  56  171   0\n",
    "t:01  61  179   0\n",
    "t:03  33  109   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.37      0.25      0.30       227\n",
    "          01       0.39      0.75      0.51       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.25      0.33      0.27       609\n",
    "weighted avg       0.29      0.39      0.31       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 15:10:11 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.098430534497466, 1.0894184523615345, 1.0837110322097252, 1.0796054439200164, 1.076476846226722, 1.0751025353746462, 1.074460238267244, 1.0742747895236087, 1.0744901919012586, 1.0742931679160332, 1.0741853598499143, 1.0740595838706481, 1.0741410308283539, 1.0741084630070452, 1.0740778718284394, 1.0742553259155825, 1.07413267286736, 1.0742039003199937, 1.074067707523728, 1.074012650253346, 1.0741019448623281, 1.0741047185825792, 1.0741655166904718, 1.074114764460985, 1.0738281579244704, 1.073759338147143, 1.0736629720196151, 1.0736636792497682, 1.073584946701288, 1.0735968889665526, 1.0735837620467388, 1.0744376417451305, 1.0738029955643151, 1.0739784779024046, 1.0739069538946417, 1.07383805777639, 1.0739379348034537, 1.0738811461600568, 1.0739183400456345, 1.0742666940579468, 1.0741216558932476, 1.0741618239429394, 1.0741298952321896, 1.0740586202132878, 1.074197608653352, 1.0739325467001628, 1.074346149300511, 1.0739439363745829, 1.0739538653926506, 1.0739351131254424, 1.0740063544760392, 1.073938196320056, 1.0739586637133645, 1.073895216575397, 1.0738937627701532, 1.0736985827118697, 1.0736567099106136, 1.0738879698642174, 1.0737308436035131, 1.073732088156326, 1.0737285320394732, 1.0737133779744992, 1.0735913166663134, 1.0736723312211938, 1.0736250059162258, 1.073696753270129, 1.0736368023507505, 1.0736219209598985, 1.0735846061033176, 1.0736546866803724, 1.0735895199141479, 1.073555140268235, 1.0735316356806137, 1.0735463136914132, 1.073534856680383, 1.0736279424971156, 1.0735240158776345, 1.0735294182703805, 1.0737492270853328, 1.0734720586360187, 1.0735246761287571, 1.0734268941707017, 1.0734278815133231, 1.0734902542017168, 1.0734929400320319, 1.0734005166196274, 1.0733697792187895, 1.0733441838685711, 1.073394872872113, 1.0734271200615393, 1.0733125339000684, 1.0732891465642769, 1.0732733649377557, 1.0732717680422152, 1.0732318444792273, 1.0732447742828595, 1.0733410975420221, 1.0733034943516422, 1.0732314159913212, 1.0732367326473367, 1.0732343492445295, 1.0732057856025758, 1.073150020123311, 1.073290866779772, 1.0731197975343476, 1.0731360726364336, 1.0731368231264438, 1.0730885343598615, 1.0731631912817117, 1.0730942875293676, 1.0732015800006285, 1.0730453351839815, 1.073138153415987, 1.0730510233658288, 1.0730344869428863, 1.0730130222239127, 1.0730097957432563, 1.0729894091930297, 1.0729861014778, 1.0729673319849475, 1.072942116969129, 1.0729140289898575, 1.0729149818811903, 1.0728823821532902, 1.0728864192179663, 1.0728641677959798, 1.0728324933592321, 1.072806316252021, 1.0727803004394807, 1.072757860905627, 1.0727419305121761, 1.072751814704419, 1.0728058803257683, 1.0726953770335281, 1.07268727412952, 1.0726542071560137, 1.0730756852035648, 1.0726968744901209, 1.073003113759171, 1.072638359367358, 1.072803614762029, 1.0727121961136366, 1.0726450830453331, 1.072630953514713, 1.072612216124198, 1.0726166483999668, 1.07257531585756, 1.0726250370930763, 1.072617540414306, 1.0725575321413614, 1.0725197882096364, 1.0724977346868154, 1.072465459309971, 1.0724420715826877, 1.0724376743454456, 1.0723993102905198, 1.0724184209685803, 1.0723113175879166, 1.0723164232297875, 1.072262778266506, 1.0722047532916266, 1.0722087942712217, 1.0722654750585947, 1.0722176799633232, 1.0722855014362553, 1.0721552043125546, 1.0721523276299287, 1.0722194311066802, 1.0721541846718499, 1.0721310314482264, 1.072252737477495, 1.0724348971017672, 1.0722464119468025, 1.0720865718641108, 1.0720631792431785, 1.0720868263338588, 1.0721062625374504, 1.072029193046645, 1.0719698285821624, 1.0720130714093914, 1.0719644629896568, 1.072006403323269, 1.072061568449675, 1.071966676093479, 1.0720953647726275, 1.0720101405051345, 1.0721344309683112, 1.0720859098512747, 1.0720335435006028, 1.0719156312237819, 1.0719222797353083, 1.0720486004755807, 1.0720958946569408, 1.0719630898317485, 1.0718918433917568, 1.0719180854866266, 1.0719646516887622, 1.0719909726692538, 1.0721185679114706, 1.0721354502175242, 1.0724741226346621, 1.0723560445610134, 1.0721776338633646, 1.072351180469657, 1.0721355813673172, 1.0721885838923588, 1.0721070852577197, 1.0720576613603163, 1.072304758532294, 1.0722963608348703, 1.0723814891868428, 1.0725028784991486, 1.0730167220182998, 1.0730928736562995, 1.0725999835676747, 1.0726106284287176, 1.072556207136959, 1.072557104044947, 1.0726096332562576, 1.0729441865911624, 1.0729803011335175, 1.0728523672507901, 1.0729183657415982, 1.0727386926782543, 1.0729048984195604, 1.0729774964854049, 1.0727305596293684, 1.0733076623703655, 1.0744011779919829, 1.0735511296292635, 1.073402059880775, 1.0736043887772584, 1.0735934892507217, 1.073502169454039, 1.0733738115855627, 1.07336203492138, 1.0736198098397216, 1.074183785073667, 1.0738593774476075, 1.0744748963119557, 1.0751516864021815, 1.075788476392749, 1.0756834263872044, 1.07480399127077, 1.0752714377122952, 1.0734507074497017, 1.0730424770972216, 1.0733722947501196, 1.0731777647641687, 1.0733841555850652, 1.0741349493928731, 1.0742623377316103, 1.0739602915367665, 1.074138685596009, 1.0739825364991362, 1.0745654198140737, 1.0743441916451666, 1.0746659359516963, 1.075307231818514, 1.0746980570807245, 1.0739097835982374, 1.0756647964612212, 1.0746431207813456, 1.0751228267923365, 1.0740942688802584, 1.0741771232514155, 1.074125646761877, 1.0767060645499644, 1.075827464290049, 1.0764942828853339, 1.0747059476003662, 1.0742807781755044, 1.073310735973427, 1.0751113065553612, 1.0746937025161016, 1.074248399640539, 1.0760237687131258, 1.075763357683943, 1.0751284908974308, 1.0749110112636548, 1.0749347440910653, 1.0760010001303135, 1.0761006185769644, 1.0746491708974728, 1.0774208784886377, 1.074108415636523, 1.0742208391966295, 1.0745170040083636, 1.0745377853781914, 1.0749323344582995, 1.0747857902241849, 1.0762539144807262, 1.0745928444102872, 1.075288152264061, 1.0783835637745598, 1.0754286268074524, 1.07500725250526, 1.0751944110898548, 1.0744724682790696, 1.0737186460855168], 'val_acc': [0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.4022988498406653, 0.39080459726072103, 0.3940886695103105, 0.3940886695103105, 0.39080459726072103, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.4022988503300302, 0.3940886695103105, 0.39408866941243753, 0.3891625611359263, 0.39080459716284804, 0.3940886695103105, 0.3940886695103105, 0.4022988502321572, 0.40065681430310846, 0.3940886695103105, 0.39080459726072103, 0.4088669948292092, 0.3940886695103105, 0.3940886695103105, 0.3727421998879788, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.40722495860654145, 0.3973727417599, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.36617405519305385, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.4088669947313362, 0.3940886695103105, 0.3940886695103105, 0.39080459726072103, 0.403940886259079, 0.3940886695103105, 0.3940886695103105, 0.390804597358594, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.39080459726072103, 0.39080459726072103, 0.3940886695103105, 0.39080459726072103, 0.3940886695103105, 0.39080459726072103, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.41543513942626115, 0.40722495850866847, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.4137931033014664, 0.39080459726072103, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.39080459726072103, 0.39080459726072103, 0.39080459726072103, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.39080459726072103, 0.39080459726072103, 0.40722495860654145, 0.4088669947313362, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.39080459726072103, 0.3940886695103105, 0.4137931033014664, 0.4170771755510559, 0.39080459726072103, 0.3940886695103105, 0.39080459726072103, 0.39080459726072103, 0.4170771754531829, 0.39080459726072103, 0.3940886695103105, 0.3940886695103105, 0.4072249583129225, 0.39573070573297825, 0.3940886695103105, 0.3940886695103105, 0.4170771754531829, 0.4170771754531829, 0.390804597358594, 0.39080459726072103, 0.39080459726072103, 0.39080459726072103, 0.39080459726072103, 0.39080459726072103, 0.4072249582150495, 0.390804597358594, 0.39080459726072103, 0.40722495811717657, 0.39080459726072103, 0.390804597358594, 0.39573070573297825, 0.390804597358594, 0.390804597358594, 0.390804597358594, 0.39080459726072103, 0.390804597358594, 0.40886699443771723, 0.40722495811717657, 0.3940886695103105, 0.39080459726072103, 0.42364532005023486, 0.390804597358594, 0.3940886695103105, 0.39080459726072103, 0.390804597358594, 0.390804597358594, 0.390804597358594, 0.39080459726072103, 0.40722495811717657, 0.39080459726072103, 0.3940886695103105, 0.390804597358594, 0.4088669943398443, 0.390804597358594, 0.390804597358594, 0.4088669943398443, 0.390804597358594, 0.390804597358594, 0.390804597358594, 0.38587848839697186, 0.390804597358594, 0.4088669943398443, 0.39408866911881857, 0.38095238002258763, 0.390804597358594, 0.38587848839697186, 0.390804597358594, 0.38095238002258763, 0.4088669943398443, 0.390804597358594, 0.41543513893689626, 0.3957307053414863, 0.390804597358594, 0.3940886695103105, 0.43513957292379807, 0.38095238002258763, 0.38095238002258763, 0.38095238002258763, 0.41543513893689626, 0.38587848839697186, 0.38587848839697186, 0.38095238002258763, 0.3957307053414863, 0.38095238002258763, 0.40229884993853826, 0.38095238002258763, 0.390804597358594, 0.40229884993853826, 0.43513957292379807, 0.38095238002258763, 0.38095238002258763, 0.3875205246196396, 0.3875205246196396, 0.38095238002258763, 0.43349753670113034, 0.39408866921669156, 0.3875205246196396, 0.38095238002258763, 0.38095238002258763, 0.38095238002258763, 0.40886699443771723, 0.38095238002258763, 0.3940886696081835, 0.41379310300784744, 0.39408866921669156, 0.38095238002258763, 0.39408866921669156, 0.4072249583129225, 0.3875205246196396, 0.39408866921669156, 0.3875205246196396, 0.39408866921669156, 0.3875205246196396, 0.3875205246196396, 0.3645320190682591, 0.3940886696081835, 0.38095238002258763, 0.3875205247175126, 0.3924466329940238, 0.39408866921669156, 0.3875205246196396, 0.3875205246196396, 0.3891625608423073, 0.3875205246196396, 0.39408866921669156, 0.3891625608423073, 0.3875205247175126, 0.3875205247175126, 0.3924466329940238, 0.3875205246196396, 0.3694581274426434, 0.3924466329940238, 0.3924466329940238, 0.3875205246196396, 0.38916256074443434, 0.3924466329940238, 0.3875205246196396, 0.3924466329940238, 0.3908045968692291, 0.353037765901077, 0.3924466329940238, 0.36781609121997566, 0.3908045967713561, 0.3694581272468974, 0.3908045968692291, 0.3924466329940238, 0.353037765901077, 0.3924466329940238, 0.3924466329940238, 0.3957307052436133, 0.3924466329940238, 0.3875205246196396, 0.3924466329940238, 0.3924466329940238, 0.3924466329940238, 0.3990147774932028, 0.3875205246196396, 0.38916256074443434, 0.38916256074443434, 0.3957307052436133, 0.3990147774932028, 0.3990147774932028, 0.3957307052436133, 0.3448275851792303, 0.38916256074443434, 0.3875205246196396, 0.3875205246196396, 0.39408866911881857, 0.3957307052436133, 0.3563218380527935, 0.39408866911881857, 0.3563218380527935, 0.3924466329940238, 0.3875205246196396, 0.39080459667348316, 0.39408866892307265, 0.3825944160495094, 0.39080459667348316, 0.3825944160495094, 0.39408866911881857, 0.3924466329940238, 0.39408866892307265, 0.3875205244238936, 0.3825944160495094, 0.3825944160495094, 0.3875205244238936, 0.3612479464271777, 0.3875205244238936, 0.38916256054868836, 0.3825944160495094, 0.38916256054868836, 0.38423645217430413, 0.3875205244238936, 0.3842364522721771, 0.38916256054868836, 0.3875205244238936, 0.36124794623143175, 0.3875205244238936, 0.3825944160495094, 0.3842364522721771, 0.3858784882990989, 0.3858784882990989], 'loss': [1.1062900828629794, 1.0944293719297562, 1.0869164382163015, 1.0817292538022114, 1.0779604510849752, 1.075514174633692, 1.0745562024185056, 1.073981670530425, 1.0739615997005048, 1.0742094556898552, 1.0743639185198524, 1.0740607702266998, 1.0741566678581786, 1.0740490917797205, 1.0739113048361557, 1.0741480253560343, 1.0740261844051446, 1.0738780345760086, 1.0741220278906383, 1.0738059375809939, 1.0738989028108192, 1.073844117648303, 1.0739401821238304, 1.0739133444165303, 1.0738257671528528, 1.0738647037952587, 1.073919474026016, 1.0737184175965233, 1.0738627485670837, 1.0740291739880916, 1.0743377918090664, 1.0743373322046268, 1.07447341002478, 1.0742197697167524, 1.0741867376793581, 1.0741829664555418, 1.0740925824617704, 1.0742187039808082, 1.0740883916310462, 1.0741839920470846, 1.07426598272774, 1.0740760407653434, 1.07428222946073, 1.0739081132583306, 1.0742613697443655, 1.0741863938572471, 1.074769938408227, 1.0742382599832585, 1.0739340098242005, 1.0740312024063643, 1.074109516643156, 1.0739124924250452, 1.0740193433095786, 1.0739901074638603, 1.0740043813443036, 1.0742482580443427, 1.0742086750281175, 1.0745322201531036, 1.07433999808662, 1.0742244941742758, 1.0740771473800377, 1.0740444423236886, 1.0741075239142353, 1.0740568450833738, 1.0740646915024556, 1.0741430058371604, 1.0742291100950456, 1.0740071433770337, 1.0741646844993138, 1.0740033371492579, 1.0740646663387698, 1.0739701468352174, 1.073960567450866, 1.0739860695245573, 1.0739630085242113, 1.0742017592247997, 1.074081719631532, 1.073785315744686, 1.074174727257762, 1.0740000259704903, 1.0741435631099912, 1.07393105127973, 1.073809131998301, 1.073922880963868, 1.074005821255443, 1.0739863063275692, 1.0739524799695495, 1.073880095158759, 1.0738561270418108, 1.0739129997132006, 1.0739620655713875, 1.0738030315669411, 1.0738528712819, 1.0738557315215438, 1.0740145902124518, 1.073730576405535, 1.0738008586288232, 1.073916176799876, 1.0738668317912297, 1.074019980430603, 1.073779321891816, 1.07385315630715, 1.0738401112859988, 1.0741054217183859, 1.0738820097529669, 1.0738366131420254, 1.0738467912164802, 1.0737656887062277, 1.0739424660954877, 1.0738142772376904, 1.0739625075514556, 1.0739984895659178, 1.0739267647878346, 1.0737651209077306, 1.073711214320126, 1.073751065079926, 1.0738008833518997, 1.073921186331606, 1.073695797303374, 1.0737695455551148, 1.0737387776130034, 1.0737198570670534, 1.0737659310902903, 1.0737349985071767, 1.0737975701659122, 1.0737083538356993, 1.0738834515244564, 1.0735919806012382, 1.073581445486394, 1.0736671178982242, 1.0736850310644819, 1.0735731399034818, 1.0736324570751778, 1.0736757283827607, 1.0736539782439904, 1.0737351363689258, 1.074015842570906, 1.0736673186691879, 1.07420513218678, 1.0738535807118035, 1.0735236868476477, 1.0736294106536333, 1.0735721964121354, 1.073644276027562, 1.0736167927297477, 1.0736168960281467, 1.073772403004233, 1.0735604300880823, 1.0734955824376131, 1.0736041834222219, 1.0736085787446101, 1.073502777244521, 1.0734980887211323, 1.073778363174971, 1.073439609245598, 1.0734893935417003, 1.0733772176491896, 1.0734932530342431, 1.0734866268091379, 1.0733149887355202, 1.0733340148808286, 1.0733317765856671, 1.0732996673799393, 1.073737866041352, 1.0733563290484387, 1.073688942793703, 1.0732903515288963, 1.0736334909403837, 1.0734747565012939, 1.0738359356807732, 1.0732367378975087, 1.0738717972620313, 1.0737107813970264, 1.0734258660545584, 1.0732860112337117, 1.0732183433411302, 1.0731542676381263, 1.0732688384379205, 1.0731686660641273, 1.073224733448616, 1.0730706467520774, 1.0734617955875592, 1.0730113854398473, 1.0732592092157633, 1.0731524407741226, 1.0730261913804793, 1.0730702058000976, 1.073314283515883, 1.0729435710202007, 1.0730378718346785, 1.0730006802008627, 1.0733723672263675, 1.0728422069941213, 1.0731029637785174, 1.0728055926074238, 1.0727679807302644, 1.0728837940482387, 1.0729341593121602, 1.0732120389076718, 1.072550341872464, 1.0734449234341694, 1.0726081316475995, 1.0728532590905253, 1.0726114885518194, 1.0725181308859917, 1.072480119376212, 1.0724614333812705, 1.0723810854388949, 1.0723658272373113, 1.072517524754487, 1.0722151592282054, 1.0724236175020128, 1.0732777848136008, 1.0719449286588163, 1.0725577481228712, 1.0723268322876103, 1.0721230059433766, 1.0718970191062598, 1.0716967735936753, 1.0718239099827636, 1.0717258993849863, 1.071935334587489, 1.0714733803296725, 1.0713148599777378, 1.0715675796081887, 1.071309134651748, 1.0713955605054537, 1.071984207409853, 1.0705471317870905, 1.071851437928985, 1.0714863598469102, 1.0707780558470583, 1.070580680120652, 1.0707616937233926, 1.0707064843030925, 1.0704205498313513, 1.070877123368594, 1.070347708059777, 1.0708652695101635, 1.0723612525869444, 1.0715642751854304, 1.0729087069294045, 1.070755930211265, 1.0710525912181064, 1.0701526093042362, 1.0703217782523842, 1.0702472314697515, 1.0696453078326749, 1.0703804697099408, 1.0701143947714897, 1.0697892004459546, 1.0698339863724287, 1.0698772179762197, 1.069716261740338, 1.0691155112499573, 1.0694593553915162, 1.0701486272243994, 1.070243786198892, 1.0685743513538117, 1.0699785723578514, 1.0690991723806706, 1.0691397471104804, 1.0691072240257655, 1.0688231988609205, 1.0687789244818247, 1.0688678930427504, 1.0687441145859704, 1.0706553149272284, 1.069810290894714, 1.0691011527235748, 1.0691866441429028, 1.0686446694133218, 1.0687339070396502, 1.0682293750911767, 1.0687316843126833, 1.0683048358443337, 1.068665156276319, 1.0686891364855444, 1.0691200927542466, 1.0696766778918507, 1.0689049294352286, 1.0683477739533849, 1.068658626055081, 1.0693858512862753, 1.0685542299027806, 1.068541661669831, 1.0682566186485838, 1.067968407352847, 1.0682623111736602, 1.0682393494083162, 1.0687451035090296, 1.068052227981771, 1.068313104513979, 1.0704543502913364, 1.0693161242306843, 1.0699029929339274, 1.0681796710349207, 1.0683558544339096, 1.067809430578651, 1.0683249819205283], 'acc': [0.39425051211086876, 0.3942505113642808, 0.39425051156010715, 0.3942505115233897, 0.39425051211086876, 0.39425051391002336, 0.3942505121475862, 0.39425051156010715, 0.395071869146163, 0.4036960977181272, 0.3963039020003724, 0.39425051449750237, 0.39630389980956515, 0.39630390020121786, 0.3942505134816532, 0.3942505115233897, 0.3942505113275634, 0.39219712621132696, 0.39383983550864815, 0.3917864472224727, 0.3958932247739553, 0.3938398339053199, 0.3942505148891551, 0.39425051449750237, 0.3958932255572607, 0.400821356349902, 0.3958932235989972, 0.39958932431571537, 0.39260780285026503, 0.390554412764935, 0.39425051195175986, 0.3909650917905067, 0.37330595558428914, 0.39425051195175986, 0.39425051351837065, 0.3942505117559335, 0.39425051351837065, 0.3889117054625948, 0.3934291594571891, 0.39425051547663414, 0.39425051508498143, 0.3942505148891551, 0.3958932249330642, 0.3942505113275634, 0.3942505146933287, 0.394250513714197, 0.37741273215663995, 0.38562628373473085, 0.3942505119150424, 0.3942505148891551, 0.3942505148891551, 0.3942505156724605, 0.39342915965301545, 0.3954825461767537, 0.3942505148891551, 0.3938398349211691, 0.3954825441817728, 0.3967145790309632, 0.3942505146933287, 0.3942505146933287, 0.39466118914145953, 0.3942505119150424, 0.39425051492587254, 0.3946611921155722, 0.39876796790461766, 0.39548254441431663, 0.3938398335136672, 0.3934291562872501, 0.39466119309470393, 0.3958932231706271, 0.39425051250252147, 0.39425051171921605, 0.3942505152808078, 0.39425051351837065, 0.3942505121475862, 0.38809035022645516, 0.3913757711710137, 0.3991786429402275, 0.39425051195175986, 0.3942505113275634, 0.39712525860729647, 0.39917864313605383, 0.3958932261447397, 0.39425051309000053, 0.3942505146933287, 0.3926078006594578, 0.3975359334837974, 0.3971252588031228, 0.3946611909038967, 0.394250513714197, 0.394250514301676, 0.3963038992220861, 0.39671457961844225, 0.3967145770359823, 0.3905544137807842, 0.39999999837219347, 0.394250514301676, 0.39425051547663414, 0.3975359362253663, 0.39342915769475195, 0.3975359354420609, 0.3975359344629292, 0.3971252554373575, 0.3950718667962468, 0.3942505129308916, 0.39671458020592126, 0.400410679551855, 0.4045174523667878, 0.39137576839272736, 0.394250513714197, 0.4012320314222293, 0.3975359354420609, 0.3950718693419893, 0.3942505129308916, 0.39876796551798405, 0.39999999778471446, 0.39835728809574056, 0.3991786445435557, 0.3971252546540521, 0.3938398374669116, 0.3971252564532067, 0.3967145784434841, 0.39548254637258007, 0.40657084264794413, 0.3983572896990688, 0.4008213536083331, 0.39917864395607666, 0.39835729146150595, 0.3979466128643044, 0.3979466134517834, 0.39835728989489516, 0.39835728927069863, 0.40000000232543786, 0.39835728852411073, 0.3995893227123872, 0.4045174515834824, 0.39014373518358264, 0.3942505133225443, 0.3909650925738121, 0.40164270884447273, 0.3938398374669116, 0.3950718705169474, 0.39753593563788725, 0.4045174545575951, 0.3983572873124352, 0.3979466118851726, 0.4020533858750635, 0.39876796907957573, 0.3958932243823026, 0.3991786429402275, 0.4053388070154484, 0.400410679551855, 0.3946611893740033, 0.40533880819040646, 0.40369609892980274, 0.3983572906782005, 0.39835728950324245, 0.39958932036247097, 0.40082135579914036, 0.40328542107918913, 0.4106776164420087, 0.4053388085820592, 0.3987679677087913, 0.39917864313605383, 0.4082135501461107, 0.39876796947122845, 0.41026693999889696, 0.4000000003671744, 0.4049281313923595, 0.3868583147897857, 0.4016427108027362, 0.3954825459809274, 0.3987679684920967, 0.40616016659648513, 0.39835728809574056, 0.3950718701252947, 0.4135523603559764, 0.410677619220295, 0.4094455853869538, 0.4028747413070295, 0.40862422933079134, 0.39958932310403983, 0.4106776162461823, 0.4065708424521178, 0.403696097950671, 0.4049281311598157, 0.4061601652257007, 0.39876796649711577, 0.40780287389882536, 0.4073921971007784, 0.4073921976882574, 0.40657084343124955, 0.40616016323071974, 0.3979466114568025, 0.4094455830003202, 0.4094455871126735, 0.40780287687293804, 0.40780287687293804, 0.3954825461767537, 0.4127310060989685, 0.39753593602953996, 0.408624232304904, 0.40205338904500254, 0.4082135505377634, 0.4078028770687644, 0.4082135527285707, 0.4110882944517312, 0.4098562610100427, 0.4082135501461107, 0.4119096487087391, 0.40985626061839, 0.4053388079945801, 0.3921971244488898, 0.4041067771353516, 0.39835728989489516, 0.4086242303466405, 0.4123203276975933, 0.4110882948433839, 0.40985626414326426, 0.4094455863660855, 0.40985626320084995, 0.4110882972300175, 0.41149897304893274, 0.4106776164420087, 0.4098562610100427, 0.4110882944517312, 0.4160164276310061, 0.40698152144097205, 0.4069815183077505, 0.39342915667890277, 0.3975359354420609, 0.40944558535023634, 0.4123203267184616, 0.412320330671706, 0.41108829386425216, 0.41026693960724425, 0.4098562639474379, 0.4098562633966763, 0.4036960996763907, 0.39753593407127646, 0.411498974652261, 0.399999999583869, 0.39712525684485933, 0.4082135515168952, 0.3975359328963184, 0.4057494889784153, 0.41067761980777406, 0.41355236313426275, 0.3954825445734255, 0.4106776197710566, 0.4094455869535646, 0.41396304012813606, 0.4102669384322862, 0.4028747448686212, 0.41067761941612135, 0.412320327501767, 0.41067761702948774, 0.4123203271101143, 0.4110882942926223, 0.3983572886832196, 0.4078028725280409, 0.40944558358779926, 0.40205338583834604, 0.4102669412105725, 0.41314168273790663, 0.41026693866482994, 0.41108829703419114, 0.3872689940111838, 0.4147843941893176, 0.40821355030521966, 0.3979466132559571, 0.4135523629384364, 0.41026694160222515, 0.41396303918572175, 0.40862423171742496, 0.41355236016015007, 0.414373715004637, 0.3979466122768253, 0.41355236172676085, 0.4016427124060645, 0.41642710485742324, 0.4123203271101143, 0.4008213554074877, 0.4106776178127931, 0.4102669388239389, 0.4078028727238673, 0.4135523637217418, 0.4135523605518028, 0.4090349075363402, 0.4127310043365314, 0.4106776203952531, 0.41149897504391364, 0.41355236211841356, 0.4078028760896326, 0.40328542307417004, 0.40657084124044224, 0.4147843928185332, 0.41108829781749656, 0.41396304055650623, 0.416016429038508]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
