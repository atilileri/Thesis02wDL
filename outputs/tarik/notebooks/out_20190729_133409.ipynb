{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf4.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 13:34:09 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '1', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ek', 'ds', 'by', 'eo', 'ck', 'mb', 'sk', 'eb', 'ib', 'my', 'ce', 'yd', 'aa', 'sg', 'eg'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001930525D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001937A3F6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7153, Accuracy:0.0637, Validation Loss:2.7052, Validation Accuracy:0.0788\n",
    "Epoch #2: Loss:2.7012, Accuracy:0.0747, Validation Loss:2.6938, Validation Accuracy:0.0870\n",
    "Epoch #3: Loss:2.6903, Accuracy:0.0809, Validation Loss:2.6841, Validation Accuracy:0.0821\n",
    "Epoch #4: Loss:2.6810, Accuracy:0.0813, Validation Loss:2.6767, Validation Accuracy:0.0821\n",
    "Epoch #5: Loss:2.6744, Accuracy:0.0813, Validation Loss:2.6702, Validation Accuracy:0.0821\n",
    "Epoch #6: Loss:2.6686, Accuracy:0.0813, Validation Loss:2.6638, Validation Accuracy:0.0821\n",
    "Epoch #7: Loss:2.6633, Accuracy:0.0953, Validation Loss:2.6600, Validation Accuracy:0.1363\n",
    "Epoch #8: Loss:2.6579, Accuracy:0.1470, Validation Loss:2.6554, Validation Accuracy:0.1330\n",
    "Epoch #9: Loss:2.6529, Accuracy:0.1363, Validation Loss:2.6500, Validation Accuracy:0.1232\n",
    "Epoch #10: Loss:2.6461, Accuracy:0.1351, Validation Loss:2.6436, Validation Accuracy:0.1379\n",
    "Epoch #11: Loss:2.6379, Accuracy:0.1441, Validation Loss:2.6332, Validation Accuracy:0.1593\n",
    "Epoch #12: Loss:2.6265, Accuracy:0.1540, Validation Loss:2.6206, Validation Accuracy:0.1445\n",
    "Epoch #13: Loss:2.6115, Accuracy:0.1380, Validation Loss:2.6023, Validation Accuracy:0.1314\n",
    "Epoch #14: Loss:2.5939, Accuracy:0.1236, Validation Loss:2.5837, Validation Accuracy:0.1346\n",
    "Epoch #15: Loss:2.5761, Accuracy:0.1400, Validation Loss:2.5656, Validation Accuracy:0.1527\n",
    "Epoch #16: Loss:2.5570, Accuracy:0.1548, Validation Loss:2.5490, Validation Accuracy:0.1511\n",
    "Epoch #17: Loss:2.5579, Accuracy:0.1450, Validation Loss:2.5551, Validation Accuracy:0.1642\n",
    "Epoch #18: Loss:2.5405, Accuracy:0.1602, Validation Loss:2.5270, Validation Accuracy:0.1642\n",
    "Epoch #19: Loss:2.5330, Accuracy:0.1573, Validation Loss:2.5157, Validation Accuracy:0.1741\n",
    "Epoch #20: Loss:2.5207, Accuracy:0.1618, Validation Loss:2.5158, Validation Accuracy:0.1839\n",
    "Epoch #21: Loss:2.5146, Accuracy:0.1585, Validation Loss:2.5229, Validation Accuracy:0.1724\n",
    "Epoch #22: Loss:2.5124, Accuracy:0.1565, Validation Loss:2.5056, Validation Accuracy:0.1823\n",
    "Epoch #23: Loss:2.5089, Accuracy:0.1651, Validation Loss:2.4967, Validation Accuracy:0.1839\n",
    "Epoch #24: Loss:2.5037, Accuracy:0.1622, Validation Loss:2.4978, Validation Accuracy:0.1741\n",
    "Epoch #25: Loss:2.5001, Accuracy:0.1639, Validation Loss:2.4907, Validation Accuracy:0.1806\n",
    "Epoch #26: Loss:2.4973, Accuracy:0.1643, Validation Loss:2.4886, Validation Accuracy:0.1790\n",
    "Epoch #27: Loss:2.4939, Accuracy:0.1647, Validation Loss:2.4867, Validation Accuracy:0.1642\n",
    "Epoch #28: Loss:2.4900, Accuracy:0.1634, Validation Loss:2.4957, Validation Accuracy:0.1691\n",
    "Epoch #29: Loss:2.4945, Accuracy:0.1626, Validation Loss:2.4905, Validation Accuracy:0.1626\n",
    "Epoch #30: Loss:2.4886, Accuracy:0.1614, Validation Loss:2.4772, Validation Accuracy:0.1839\n",
    "Epoch #31: Loss:2.4857, Accuracy:0.1598, Validation Loss:2.4761, Validation Accuracy:0.1773\n",
    "Epoch #32: Loss:2.4810, Accuracy:0.1593, Validation Loss:2.4730, Validation Accuracy:0.1823\n",
    "Epoch #33: Loss:2.4813, Accuracy:0.1667, Validation Loss:2.4682, Validation Accuracy:0.1806\n",
    "Epoch #34: Loss:2.4798, Accuracy:0.1622, Validation Loss:2.4710, Validation Accuracy:0.1724\n",
    "Epoch #35: Loss:2.4785, Accuracy:0.1680, Validation Loss:2.4676, Validation Accuracy:0.1773\n",
    "Epoch #36: Loss:2.4763, Accuracy:0.1643, Validation Loss:2.4634, Validation Accuracy:0.1806\n",
    "Epoch #37: Loss:2.4740, Accuracy:0.1643, Validation Loss:2.4652, Validation Accuracy:0.1839\n",
    "Epoch #38: Loss:2.4756, Accuracy:0.1696, Validation Loss:2.4657, Validation Accuracy:0.1839\n",
    "Epoch #39: Loss:2.4767, Accuracy:0.1663, Validation Loss:2.4623, Validation Accuracy:0.1790\n",
    "Epoch #40: Loss:2.4741, Accuracy:0.1643, Validation Loss:2.4610, Validation Accuracy:0.1806\n",
    "Epoch #41: Loss:2.4734, Accuracy:0.1643, Validation Loss:2.4575, Validation Accuracy:0.1856\n",
    "Epoch #42: Loss:2.4700, Accuracy:0.1659, Validation Loss:2.4570, Validation Accuracy:0.1856\n",
    "Epoch #43: Loss:2.4678, Accuracy:0.1708, Validation Loss:2.4536, Validation Accuracy:0.1938\n",
    "Epoch #44: Loss:2.4695, Accuracy:0.1659, Validation Loss:2.4482, Validation Accuracy:0.1856\n",
    "Epoch #45: Loss:2.4675, Accuracy:0.1708, Validation Loss:2.4490, Validation Accuracy:0.1921\n",
    "Epoch #46: Loss:2.4681, Accuracy:0.1717, Validation Loss:2.4484, Validation Accuracy:0.1806\n",
    "Epoch #47: Loss:2.4684, Accuracy:0.1704, Validation Loss:2.4471, Validation Accuracy:0.1938\n",
    "Epoch #48: Loss:2.4678, Accuracy:0.1729, Validation Loss:2.4471, Validation Accuracy:0.1905\n",
    "Epoch #49: Loss:2.4652, Accuracy:0.1758, Validation Loss:2.4469, Validation Accuracy:0.1938\n",
    "Epoch #50: Loss:2.4658, Accuracy:0.1725, Validation Loss:2.4473, Validation Accuracy:0.2003\n",
    "Epoch #51: Loss:2.4607, Accuracy:0.1717, Validation Loss:2.4444, Validation Accuracy:0.1954\n",
    "Epoch #52: Loss:2.4583, Accuracy:0.1704, Validation Loss:2.4444, Validation Accuracy:0.1839\n",
    "Epoch #53: Loss:2.4562, Accuracy:0.1713, Validation Loss:2.4401, Validation Accuracy:0.1938\n",
    "Epoch #54: Loss:2.4550, Accuracy:0.1700, Validation Loss:2.4409, Validation Accuracy:0.1987\n",
    "Epoch #55: Loss:2.4562, Accuracy:0.1671, Validation Loss:2.4419, Validation Accuracy:0.2069\n",
    "Epoch #56: Loss:2.4567, Accuracy:0.1717, Validation Loss:2.4420, Validation Accuracy:0.2053\n",
    "Epoch #57: Loss:2.4553, Accuracy:0.1758, Validation Loss:2.4416, Validation Accuracy:0.1856\n",
    "Epoch #58: Loss:2.4547, Accuracy:0.1729, Validation Loss:2.4429, Validation Accuracy:0.1856\n",
    "Epoch #59: Loss:2.4519, Accuracy:0.1741, Validation Loss:2.4391, Validation Accuracy:0.1938\n",
    "Epoch #60: Loss:2.4527, Accuracy:0.1708, Validation Loss:2.4336, Validation Accuracy:0.2020\n",
    "Epoch #61: Loss:2.4499, Accuracy:0.1762, Validation Loss:2.4369, Validation Accuracy:0.1938\n",
    "Epoch #62: Loss:2.4528, Accuracy:0.1671, Validation Loss:2.4394, Validation Accuracy:0.1921\n",
    "Epoch #63: Loss:2.4562, Accuracy:0.1692, Validation Loss:2.4396, Validation Accuracy:0.2036\n",
    "Epoch #64: Loss:2.4558, Accuracy:0.1745, Validation Loss:2.4369, Validation Accuracy:0.1921\n",
    "Epoch #65: Loss:2.4578, Accuracy:0.1717, Validation Loss:2.4393, Validation Accuracy:0.1888\n",
    "Epoch #66: Loss:2.4576, Accuracy:0.1688, Validation Loss:2.4391, Validation Accuracy:0.1921\n",
    "Epoch #67: Loss:2.4582, Accuracy:0.1708, Validation Loss:2.4392, Validation Accuracy:0.1921\n",
    "Epoch #68: Loss:2.4562, Accuracy:0.1696, Validation Loss:2.4406, Validation Accuracy:0.1921\n",
    "Epoch #69: Loss:2.4559, Accuracy:0.1688, Validation Loss:2.4404, Validation Accuracy:0.1938\n",
    "Epoch #70: Loss:2.4544, Accuracy:0.1696, Validation Loss:2.4384, Validation Accuracy:0.1921\n",
    "Epoch #71: Loss:2.4547, Accuracy:0.1713, Validation Loss:2.4384, Validation Accuracy:0.1921\n",
    "Epoch #72: Loss:2.4518, Accuracy:0.1708, Validation Loss:2.4378, Validation Accuracy:0.1954\n",
    "Epoch #73: Loss:2.4542, Accuracy:0.1733, Validation Loss:2.4366, Validation Accuracy:0.1938\n",
    "Epoch #74: Loss:2.4525, Accuracy:0.1684, Validation Loss:2.4362, Validation Accuracy:0.1905\n",
    "Epoch #75: Loss:2.4508, Accuracy:0.1717, Validation Loss:2.4387, Validation Accuracy:0.1970\n",
    "Epoch #76: Loss:2.4514, Accuracy:0.1696, Validation Loss:2.4388, Validation Accuracy:0.1856\n",
    "Epoch #77: Loss:2.4494, Accuracy:0.1713, Validation Loss:2.4359, Validation Accuracy:0.1888\n",
    "Epoch #78: Loss:2.4503, Accuracy:0.1684, Validation Loss:2.4342, Validation Accuracy:0.1938\n",
    "Epoch #79: Loss:2.4481, Accuracy:0.1708, Validation Loss:2.4332, Validation Accuracy:0.1970\n",
    "Epoch #80: Loss:2.4518, Accuracy:0.1704, Validation Loss:2.4320, Validation Accuracy:0.1970\n",
    "Epoch #81: Loss:2.4493, Accuracy:0.1671, Validation Loss:2.4323, Validation Accuracy:0.1970\n",
    "Epoch #82: Loss:2.4476, Accuracy:0.1721, Validation Loss:2.4344, Validation Accuracy:0.1970\n",
    "Epoch #83: Loss:2.4478, Accuracy:0.1717, Validation Loss:2.4323, Validation Accuracy:0.1921\n",
    "Epoch #84: Loss:2.4474, Accuracy:0.1704, Validation Loss:2.4305, Validation Accuracy:0.2003\n",
    "Epoch #85: Loss:2.4484, Accuracy:0.1680, Validation Loss:2.4344, Validation Accuracy:0.1987\n",
    "Epoch #86: Loss:2.4475, Accuracy:0.1733, Validation Loss:2.4352, Validation Accuracy:0.2003\n",
    "Epoch #87: Loss:2.4463, Accuracy:0.1676, Validation Loss:2.4328, Validation Accuracy:0.1954\n",
    "Epoch #88: Loss:2.4456, Accuracy:0.1700, Validation Loss:2.4303, Validation Accuracy:0.1970\n",
    "Epoch #89: Loss:2.4431, Accuracy:0.1708, Validation Loss:2.4281, Validation Accuracy:0.1921\n",
    "Epoch #90: Loss:2.4447, Accuracy:0.1680, Validation Loss:2.4289, Validation Accuracy:0.1987\n",
    "Epoch #91: Loss:2.4431, Accuracy:0.1704, Validation Loss:2.4291, Validation Accuracy:0.1987\n",
    "Epoch #92: Loss:2.4433, Accuracy:0.1713, Validation Loss:2.4329, Validation Accuracy:0.1954\n",
    "Epoch #93: Loss:2.4423, Accuracy:0.1729, Validation Loss:2.4311, Validation Accuracy:0.1970\n",
    "Epoch #94: Loss:2.4430, Accuracy:0.1659, Validation Loss:2.4296, Validation Accuracy:0.1954\n",
    "Epoch #95: Loss:2.4433, Accuracy:0.1729, Validation Loss:2.4305, Validation Accuracy:0.1970\n",
    "Epoch #96: Loss:2.4439, Accuracy:0.1708, Validation Loss:2.4346, Validation Accuracy:0.1987\n",
    "Epoch #97: Loss:2.4408, Accuracy:0.1713, Validation Loss:2.4340, Validation Accuracy:0.1905\n",
    "Epoch #98: Loss:2.4418, Accuracy:0.1651, Validation Loss:2.4318, Validation Accuracy:0.1970\n",
    "Epoch #99: Loss:2.4408, Accuracy:0.1659, Validation Loss:2.4322, Validation Accuracy:0.1970\n",
    "Epoch #100: Loss:2.4423, Accuracy:0.1667, Validation Loss:2.4314, Validation Accuracy:0.1970\n",
    "Epoch #101: Loss:2.4430, Accuracy:0.1733, Validation Loss:2.4334, Validation Accuracy:0.1970\n",
    "Epoch #102: Loss:2.4419, Accuracy:0.1737, Validation Loss:2.4351, Validation Accuracy:0.1938\n",
    "Epoch #103: Loss:2.4415, Accuracy:0.1733, Validation Loss:2.4370, Validation Accuracy:0.1888\n",
    "Epoch #104: Loss:2.4413, Accuracy:0.1729, Validation Loss:2.4338, Validation Accuracy:0.1921\n",
    "Epoch #105: Loss:2.4421, Accuracy:0.1725, Validation Loss:2.4331, Validation Accuracy:0.1970\n",
    "Epoch #106: Loss:2.4403, Accuracy:0.1733, Validation Loss:2.4358, Validation Accuracy:0.1970\n",
    "Epoch #107: Loss:2.4409, Accuracy:0.1737, Validation Loss:2.4384, Validation Accuracy:0.1905\n",
    "Epoch #108: Loss:2.4410, Accuracy:0.1725, Validation Loss:2.4377, Validation Accuracy:0.1872\n",
    "Epoch #109: Loss:2.4410, Accuracy:0.1725, Validation Loss:2.4388, Validation Accuracy:0.1921\n",
    "Epoch #110: Loss:2.4412, Accuracy:0.1704, Validation Loss:2.4326, Validation Accuracy:0.1905\n",
    "Epoch #111: Loss:2.4397, Accuracy:0.1733, Validation Loss:2.4298, Validation Accuracy:0.1954\n",
    "Epoch #112: Loss:2.4371, Accuracy:0.1733, Validation Loss:2.4302, Validation Accuracy:0.1954\n",
    "Epoch #113: Loss:2.4371, Accuracy:0.1708, Validation Loss:2.4299, Validation Accuracy:0.1954\n",
    "Epoch #114: Loss:2.4372, Accuracy:0.1704, Validation Loss:2.4298, Validation Accuracy:0.1921\n",
    "Epoch #115: Loss:2.4392, Accuracy:0.1696, Validation Loss:2.4322, Validation Accuracy:0.1970\n",
    "Epoch #116: Loss:2.4381, Accuracy:0.1688, Validation Loss:2.4323, Validation Accuracy:0.1921\n",
    "Epoch #117: Loss:2.4393, Accuracy:0.1704, Validation Loss:2.4338, Validation Accuracy:0.1987\n",
    "Epoch #118: Loss:2.4389, Accuracy:0.1737, Validation Loss:2.4318, Validation Accuracy:0.1938\n",
    "Epoch #119: Loss:2.4392, Accuracy:0.1688, Validation Loss:2.4329, Validation Accuracy:0.1970\n",
    "Epoch #120: Loss:2.4379, Accuracy:0.1684, Validation Loss:2.4299, Validation Accuracy:0.1905\n",
    "Epoch #121: Loss:2.4391, Accuracy:0.1655, Validation Loss:2.4344, Validation Accuracy:0.2053\n",
    "Epoch #122: Loss:2.4382, Accuracy:0.1745, Validation Loss:2.4366, Validation Accuracy:0.1921\n",
    "Epoch #123: Loss:2.4378, Accuracy:0.1704, Validation Loss:2.4360, Validation Accuracy:0.1872\n",
    "Epoch #124: Loss:2.4367, Accuracy:0.1741, Validation Loss:2.4380, Validation Accuracy:0.1921\n",
    "Epoch #125: Loss:2.4354, Accuracy:0.1700, Validation Loss:2.4361, Validation Accuracy:0.1823\n",
    "Epoch #126: Loss:2.4363, Accuracy:0.1758, Validation Loss:2.4356, Validation Accuracy:0.1773\n",
    "Epoch #127: Loss:2.4335, Accuracy:0.1713, Validation Loss:2.4310, Validation Accuracy:0.1823\n",
    "Epoch #128: Loss:2.4348, Accuracy:0.1667, Validation Loss:2.4336, Validation Accuracy:0.1938\n",
    "Epoch #129: Loss:2.4365, Accuracy:0.1729, Validation Loss:2.4338, Validation Accuracy:0.1970\n",
    "Epoch #130: Loss:2.4358, Accuracy:0.1713, Validation Loss:2.4320, Validation Accuracy:0.1888\n",
    "Epoch #131: Loss:2.4368, Accuracy:0.1671, Validation Loss:2.4315, Validation Accuracy:0.1839\n",
    "Epoch #132: Loss:2.4370, Accuracy:0.1676, Validation Loss:2.4333, Validation Accuracy:0.1905\n",
    "Epoch #133: Loss:2.4367, Accuracy:0.1708, Validation Loss:2.4316, Validation Accuracy:0.1888\n",
    "Epoch #134: Loss:2.4365, Accuracy:0.1634, Validation Loss:2.4356, Validation Accuracy:0.2036\n",
    "Epoch #135: Loss:2.4394, Accuracy:0.1700, Validation Loss:2.4327, Validation Accuracy:0.1872\n",
    "Epoch #136: Loss:2.4362, Accuracy:0.1655, Validation Loss:2.4330, Validation Accuracy:0.2036\n",
    "Epoch #137: Loss:2.4366, Accuracy:0.1758, Validation Loss:2.4359, Validation Accuracy:0.2003\n",
    "Epoch #138: Loss:2.4371, Accuracy:0.1676, Validation Loss:2.4336, Validation Accuracy:0.1888\n",
    "Epoch #139: Loss:2.4370, Accuracy:0.1643, Validation Loss:2.4354, Validation Accuracy:0.1970\n",
    "Epoch #140: Loss:2.4375, Accuracy:0.1626, Validation Loss:2.4317, Validation Accuracy:0.1888\n",
    "Epoch #141: Loss:2.4323, Accuracy:0.1745, Validation Loss:2.4389, Validation Accuracy:0.1757\n",
    "Epoch #142: Loss:2.4318, Accuracy:0.1704, Validation Loss:2.4318, Validation Accuracy:0.2003\n",
    "Epoch #143: Loss:2.4287, Accuracy:0.1754, Validation Loss:2.4349, Validation Accuracy:0.1872\n",
    "Epoch #144: Loss:2.4306, Accuracy:0.1729, Validation Loss:2.4336, Validation Accuracy:0.2003\n",
    "Epoch #145: Loss:2.4290, Accuracy:0.1745, Validation Loss:2.4331, Validation Accuracy:0.1938\n",
    "Epoch #146: Loss:2.4307, Accuracy:0.1819, Validation Loss:2.4335, Validation Accuracy:0.1938\n",
    "Epoch #147: Loss:2.4333, Accuracy:0.1749, Validation Loss:2.4262, Validation Accuracy:0.2102\n",
    "Epoch #148: Loss:2.4310, Accuracy:0.1762, Validation Loss:2.4308, Validation Accuracy:0.1905\n",
    "Epoch #149: Loss:2.4283, Accuracy:0.1807, Validation Loss:2.4308, Validation Accuracy:0.2102\n",
    "Epoch #150: Loss:2.4294, Accuracy:0.1778, Validation Loss:2.4351, Validation Accuracy:0.2151\n",
    "Epoch #151: Loss:2.4291, Accuracy:0.1832, Validation Loss:2.4288, Validation Accuracy:0.2069\n",
    "Epoch #152: Loss:2.4273, Accuracy:0.1828, Validation Loss:2.4306, Validation Accuracy:0.1856\n",
    "Epoch #153: Loss:2.4267, Accuracy:0.1819, Validation Loss:2.4294, Validation Accuracy:0.2036\n",
    "Epoch #154: Loss:2.4278, Accuracy:0.1815, Validation Loss:2.4327, Validation Accuracy:0.1888\n",
    "Epoch #155: Loss:2.4273, Accuracy:0.1815, Validation Loss:2.4326, Validation Accuracy:0.2118\n",
    "Epoch #156: Loss:2.4265, Accuracy:0.1815, Validation Loss:2.4314, Validation Accuracy:0.2102\n",
    "Epoch #157: Loss:2.4252, Accuracy:0.1807, Validation Loss:2.4317, Validation Accuracy:0.2118\n",
    "Epoch #158: Loss:2.4256, Accuracy:0.1786, Validation Loss:2.4296, Validation Accuracy:0.2036\n",
    "Epoch #159: Loss:2.4251, Accuracy:0.1766, Validation Loss:2.4306, Validation Accuracy:0.2102\n",
    "Epoch #160: Loss:2.4271, Accuracy:0.1815, Validation Loss:2.4301, Validation Accuracy:0.2085\n",
    "Epoch #161: Loss:2.4255, Accuracy:0.1786, Validation Loss:2.4317, Validation Accuracy:0.2102\n",
    "Epoch #162: Loss:2.4268, Accuracy:0.1807, Validation Loss:2.4335, Validation Accuracy:0.2151\n",
    "Epoch #163: Loss:2.4248, Accuracy:0.1836, Validation Loss:2.4298, Validation Accuracy:0.2102\n",
    "Epoch #164: Loss:2.4238, Accuracy:0.1836, Validation Loss:2.4321, Validation Accuracy:0.2135\n",
    "Epoch #165: Loss:2.4234, Accuracy:0.1848, Validation Loss:2.4295, Validation Accuracy:0.2118\n",
    "Epoch #166: Loss:2.4242, Accuracy:0.1836, Validation Loss:2.4358, Validation Accuracy:0.2167\n",
    "Epoch #167: Loss:2.4246, Accuracy:0.1856, Validation Loss:2.4342, Validation Accuracy:0.2151\n",
    "Epoch #168: Loss:2.4270, Accuracy:0.1828, Validation Loss:2.4352, Validation Accuracy:0.1905\n",
    "Epoch #169: Loss:2.4243, Accuracy:0.1864, Validation Loss:2.4414, Validation Accuracy:0.1773\n",
    "Epoch #170: Loss:2.4225, Accuracy:0.1791, Validation Loss:2.4393, Validation Accuracy:0.2053\n",
    "Epoch #171: Loss:2.4229, Accuracy:0.1799, Validation Loss:2.4353, Validation Accuracy:0.2118\n",
    "Epoch #172: Loss:2.4222, Accuracy:0.1848, Validation Loss:2.4357, Validation Accuracy:0.2135\n",
    "Epoch #173: Loss:2.4242, Accuracy:0.1791, Validation Loss:2.4355, Validation Accuracy:0.2085\n",
    "Epoch #174: Loss:2.4239, Accuracy:0.1782, Validation Loss:2.4400, Validation Accuracy:0.1970\n",
    "Epoch #175: Loss:2.4249, Accuracy:0.1807, Validation Loss:2.4353, Validation Accuracy:0.2085\n",
    "Epoch #176: Loss:2.4238, Accuracy:0.1815, Validation Loss:2.4372, Validation Accuracy:0.1987\n",
    "Epoch #177: Loss:2.4231, Accuracy:0.1791, Validation Loss:2.4337, Validation Accuracy:0.2069\n",
    "Epoch #178: Loss:2.4221, Accuracy:0.1836, Validation Loss:2.4357, Validation Accuracy:0.1987\n",
    "Epoch #179: Loss:2.4231, Accuracy:0.1770, Validation Loss:2.4313, Validation Accuracy:0.2069\n",
    "Epoch #180: Loss:2.4218, Accuracy:0.1811, Validation Loss:2.4369, Validation Accuracy:0.2053\n",
    "Epoch #181: Loss:2.4208, Accuracy:0.1828, Validation Loss:2.4347, Validation Accuracy:0.1954\n",
    "Epoch #182: Loss:2.4216, Accuracy:0.1778, Validation Loss:2.4360, Validation Accuracy:0.2085\n",
    "Epoch #183: Loss:2.4244, Accuracy:0.1869, Validation Loss:2.4419, Validation Accuracy:0.1987\n",
    "Epoch #184: Loss:2.4208, Accuracy:0.1832, Validation Loss:2.4414, Validation Accuracy:0.1970\n",
    "Epoch #185: Loss:2.4206, Accuracy:0.1791, Validation Loss:2.4466, Validation Accuracy:0.2036\n",
    "Epoch #186: Loss:2.4196, Accuracy:0.1840, Validation Loss:2.4371, Validation Accuracy:0.1970\n",
    "Epoch #187: Loss:2.4191, Accuracy:0.1840, Validation Loss:2.4426, Validation Accuracy:0.1987\n",
    "Epoch #188: Loss:2.4167, Accuracy:0.1877, Validation Loss:2.4446, Validation Accuracy:0.1987\n",
    "Epoch #189: Loss:2.4150, Accuracy:0.1877, Validation Loss:2.4419, Validation Accuracy:0.1987\n",
    "Epoch #190: Loss:2.4146, Accuracy:0.1836, Validation Loss:2.4458, Validation Accuracy:0.1938\n",
    "Epoch #191: Loss:2.4155, Accuracy:0.1885, Validation Loss:2.4425, Validation Accuracy:0.2053\n",
    "Epoch #192: Loss:2.4145, Accuracy:0.1848, Validation Loss:2.4444, Validation Accuracy:0.2003\n",
    "Epoch #193: Loss:2.4169, Accuracy:0.1910, Validation Loss:2.4381, Validation Accuracy:0.2053\n",
    "Epoch #194: Loss:2.4148, Accuracy:0.1869, Validation Loss:2.4404, Validation Accuracy:0.2003\n",
    "Epoch #195: Loss:2.4161, Accuracy:0.1918, Validation Loss:2.4421, Validation Accuracy:0.2036\n",
    "Epoch #196: Loss:2.4157, Accuracy:0.1869, Validation Loss:2.4484, Validation Accuracy:0.1970\n",
    "Epoch #197: Loss:2.4115, Accuracy:0.1914, Validation Loss:2.4438, Validation Accuracy:0.1987\n",
    "Epoch #198: Loss:2.4129, Accuracy:0.1856, Validation Loss:2.4464, Validation Accuracy:0.1938\n",
    "Epoch #199: Loss:2.4150, Accuracy:0.1897, Validation Loss:2.4471, Validation Accuracy:0.1921\n",
    "Epoch #200: Loss:2.4143, Accuracy:0.1873, Validation Loss:2.4486, Validation Accuracy:0.1938\n",
    "Epoch #201: Loss:2.4164, Accuracy:0.1897, Validation Loss:2.4490, Validation Accuracy:0.1938\n",
    "Epoch #202: Loss:2.4124, Accuracy:0.1815, Validation Loss:2.4447, Validation Accuracy:0.2069\n",
    "Epoch #203: Loss:2.4154, Accuracy:0.1910, Validation Loss:2.4478, Validation Accuracy:0.2020\n",
    "Epoch #204: Loss:2.4134, Accuracy:0.1852, Validation Loss:2.4470, Validation Accuracy:0.1839\n",
    "Epoch #205: Loss:2.4125, Accuracy:0.1906, Validation Loss:2.4475, Validation Accuracy:0.1938\n",
    "Epoch #206: Loss:2.4075, Accuracy:0.1864, Validation Loss:2.4498, Validation Accuracy:0.1905\n",
    "Epoch #207: Loss:2.4074, Accuracy:0.1881, Validation Loss:2.4494, Validation Accuracy:0.1905\n",
    "Epoch #208: Loss:2.4083, Accuracy:0.1934, Validation Loss:2.4429, Validation Accuracy:0.1938\n",
    "Epoch #209: Loss:2.4064, Accuracy:0.1893, Validation Loss:2.4444, Validation Accuracy:0.1773\n",
    "Epoch #210: Loss:2.4039, Accuracy:0.1943, Validation Loss:2.4419, Validation Accuracy:0.1970\n",
    "Epoch #211: Loss:2.4061, Accuracy:0.1885, Validation Loss:2.4455, Validation Accuracy:0.1806\n",
    "Epoch #212: Loss:2.4065, Accuracy:0.1897, Validation Loss:2.4454, Validation Accuracy:0.1872\n",
    "Epoch #213: Loss:2.4045, Accuracy:0.1910, Validation Loss:2.4436, Validation Accuracy:0.1806\n",
    "Epoch #214: Loss:2.4047, Accuracy:0.1934, Validation Loss:2.4437, Validation Accuracy:0.1872\n",
    "Epoch #215: Loss:2.4056, Accuracy:0.1930, Validation Loss:2.4458, Validation Accuracy:0.1905\n",
    "Epoch #216: Loss:2.4074, Accuracy:0.1897, Validation Loss:2.4503, Validation Accuracy:0.1888\n",
    "Epoch #217: Loss:2.4060, Accuracy:0.1930, Validation Loss:2.4484, Validation Accuracy:0.1872\n",
    "Epoch #218: Loss:2.4075, Accuracy:0.1971, Validation Loss:2.4465, Validation Accuracy:0.1888\n",
    "Epoch #219: Loss:2.4111, Accuracy:0.1918, Validation Loss:2.4530, Validation Accuracy:0.1872\n",
    "Epoch #220: Loss:2.4083, Accuracy:0.1926, Validation Loss:2.4516, Validation Accuracy:0.1856\n",
    "Epoch #221: Loss:2.4049, Accuracy:0.1934, Validation Loss:2.4547, Validation Accuracy:0.1790\n",
    "Epoch #222: Loss:2.4034, Accuracy:0.1938, Validation Loss:2.4461, Validation Accuracy:0.1823\n",
    "Epoch #223: Loss:2.4029, Accuracy:0.1971, Validation Loss:2.4504, Validation Accuracy:0.1806\n",
    "Epoch #224: Loss:2.3995, Accuracy:0.1889, Validation Loss:2.4451, Validation Accuracy:0.1905\n",
    "Epoch #225: Loss:2.3980, Accuracy:0.1988, Validation Loss:2.4507, Validation Accuracy:0.1872\n",
    "Epoch #226: Loss:2.4068, Accuracy:0.1975, Validation Loss:2.4472, Validation Accuracy:0.1905\n",
    "Epoch #227: Loss:2.4097, Accuracy:0.1885, Validation Loss:2.4499, Validation Accuracy:0.2020\n",
    "Epoch #228: Loss:2.4120, Accuracy:0.1864, Validation Loss:2.4389, Validation Accuracy:0.1872\n",
    "Epoch #229: Loss:2.4113, Accuracy:0.1893, Validation Loss:2.4399, Validation Accuracy:0.1970\n",
    "Epoch #230: Loss:2.4132, Accuracy:0.1864, Validation Loss:2.4419, Validation Accuracy:0.1905\n",
    "Epoch #231: Loss:2.4095, Accuracy:0.1877, Validation Loss:2.4489, Validation Accuracy:0.1888\n",
    "Epoch #232: Loss:2.4108, Accuracy:0.1918, Validation Loss:2.4498, Validation Accuracy:0.1938\n",
    "Epoch #233: Loss:2.4092, Accuracy:0.1832, Validation Loss:2.4535, Validation Accuracy:0.1921\n",
    "Epoch #234: Loss:2.4085, Accuracy:0.1877, Validation Loss:2.4440, Validation Accuracy:0.1970\n",
    "Epoch #235: Loss:2.4081, Accuracy:0.1840, Validation Loss:2.4453, Validation Accuracy:0.2003\n",
    "Epoch #236: Loss:2.4054, Accuracy:0.1823, Validation Loss:2.4435, Validation Accuracy:0.2053\n",
    "Epoch #237: Loss:2.4078, Accuracy:0.1877, Validation Loss:2.4542, Validation Accuracy:0.1856\n",
    "Epoch #238: Loss:2.4097, Accuracy:0.1864, Validation Loss:2.4428, Validation Accuracy:0.1938\n",
    "Epoch #239: Loss:2.4140, Accuracy:0.1840, Validation Loss:2.4465, Validation Accuracy:0.1921\n",
    "Epoch #240: Loss:2.4109, Accuracy:0.1881, Validation Loss:2.4466, Validation Accuracy:0.1970\n",
    "Epoch #241: Loss:2.4115, Accuracy:0.1856, Validation Loss:2.4460, Validation Accuracy:0.1921\n",
    "Epoch #242: Loss:2.4042, Accuracy:0.1885, Validation Loss:2.4391, Validation Accuracy:0.1905\n",
    "Epoch #243: Loss:2.4035, Accuracy:0.1881, Validation Loss:2.4400, Validation Accuracy:0.1970\n",
    "Epoch #244: Loss:2.4005, Accuracy:0.1918, Validation Loss:2.4432, Validation Accuracy:0.1970\n",
    "Epoch #245: Loss:2.3963, Accuracy:0.1914, Validation Loss:2.4510, Validation Accuracy:0.1905\n",
    "Epoch #246: Loss:2.3965, Accuracy:0.1893, Validation Loss:2.4521, Validation Accuracy:0.1905\n",
    "Epoch #247: Loss:2.4003, Accuracy:0.1947, Validation Loss:2.4466, Validation Accuracy:0.1987\n",
    "Epoch #248: Loss:2.4013, Accuracy:0.1901, Validation Loss:2.4489, Validation Accuracy:0.1839\n",
    "Epoch #249: Loss:2.4027, Accuracy:0.1918, Validation Loss:2.4312, Validation Accuracy:0.2069\n",
    "Epoch #250: Loss:2.4085, Accuracy:0.1938, Validation Loss:2.4408, Validation Accuracy:0.1938\n",
    "Epoch #251: Loss:2.4058, Accuracy:0.1828, Validation Loss:2.4501, Validation Accuracy:0.2020\n",
    "Epoch #252: Loss:2.4098, Accuracy:0.1852, Validation Loss:2.4394, Validation Accuracy:0.2053\n",
    "Epoch #253: Loss:2.4087, Accuracy:0.1856, Validation Loss:2.4637, Validation Accuracy:0.1823\n",
    "Epoch #254: Loss:2.4162, Accuracy:0.1869, Validation Loss:2.4565, Validation Accuracy:0.1921\n",
    "Epoch #255: Loss:2.4170, Accuracy:0.1799, Validation Loss:2.4518, Validation Accuracy:0.2020\n",
    "Epoch #256: Loss:2.4122, Accuracy:0.1893, Validation Loss:2.4530, Validation Accuracy:0.1675\n",
    "Epoch #257: Loss:2.4147, Accuracy:0.1828, Validation Loss:2.4413, Validation Accuracy:0.1872\n",
    "Epoch #258: Loss:2.4075, Accuracy:0.1955, Validation Loss:2.4420, Validation Accuracy:0.1954\n",
    "Epoch #259: Loss:2.4075, Accuracy:0.1979, Validation Loss:2.4332, Validation Accuracy:0.1954\n",
    "Epoch #260: Loss:2.4047, Accuracy:0.1938, Validation Loss:2.4467, Validation Accuracy:0.1806\n",
    "Epoch #261: Loss:2.4087, Accuracy:0.1926, Validation Loss:2.4401, Validation Accuracy:0.1823\n",
    "Epoch #262: Loss:2.4025, Accuracy:0.1951, Validation Loss:2.4405, Validation Accuracy:0.1970\n",
    "Epoch #263: Loss:2.3990, Accuracy:0.1967, Validation Loss:2.4460, Validation Accuracy:0.1888\n",
    "Epoch #264: Loss:2.3934, Accuracy:0.2000, Validation Loss:2.4388, Validation Accuracy:0.1987\n",
    "Epoch #265: Loss:2.3926, Accuracy:0.2000, Validation Loss:2.4418, Validation Accuracy:0.1724\n",
    "Epoch #266: Loss:2.4024, Accuracy:0.1959, Validation Loss:2.4345, Validation Accuracy:0.1970\n",
    "Epoch #267: Loss:2.4067, Accuracy:0.1943, Validation Loss:2.4345, Validation Accuracy:0.1888\n",
    "Epoch #268: Loss:2.3971, Accuracy:0.1996, Validation Loss:2.4364, Validation Accuracy:0.1675\n",
    "Epoch #269: Loss:2.3936, Accuracy:0.1910, Validation Loss:2.4303, Validation Accuracy:0.1921\n",
    "Epoch #270: Loss:2.3984, Accuracy:0.1943, Validation Loss:2.4351, Validation Accuracy:0.1741\n",
    "Epoch #271: Loss:2.4157, Accuracy:0.1828, Validation Loss:2.4508, Validation Accuracy:0.1839\n",
    "Epoch #272: Loss:2.4147, Accuracy:0.1856, Validation Loss:2.4528, Validation Accuracy:0.1609\n",
    "Epoch #273: Loss:2.4166, Accuracy:0.1848, Validation Loss:2.4586, Validation Accuracy:0.1987\n",
    "Epoch #274: Loss:2.4155, Accuracy:0.1832, Validation Loss:2.4601, Validation Accuracy:0.1658\n",
    "Epoch #275: Loss:2.4151, Accuracy:0.1864, Validation Loss:2.4525, Validation Accuracy:0.1970\n",
    "Epoch #276: Loss:2.4152, Accuracy:0.1860, Validation Loss:2.4598, Validation Accuracy:0.1741\n",
    "Epoch #277: Loss:2.4131, Accuracy:0.1930, Validation Loss:2.4520, Validation Accuracy:0.1872\n",
    "Epoch #278: Loss:2.4119, Accuracy:0.1860, Validation Loss:2.4589, Validation Accuracy:0.1741\n",
    "Epoch #279: Loss:2.4097, Accuracy:0.1860, Validation Loss:2.4567, Validation Accuracy:0.1823\n",
    "Epoch #280: Loss:2.4104, Accuracy:0.1869, Validation Loss:2.4512, Validation Accuracy:0.1856\n",
    "Epoch #281: Loss:2.4094, Accuracy:0.1901, Validation Loss:2.4511, Validation Accuracy:0.1888\n",
    "Epoch #282: Loss:2.4104, Accuracy:0.1930, Validation Loss:2.4514, Validation Accuracy:0.1921\n",
    "Epoch #283: Loss:2.4116, Accuracy:0.1881, Validation Loss:2.4563, Validation Accuracy:0.1724\n",
    "Epoch #284: Loss:2.4084, Accuracy:0.1877, Validation Loss:2.4569, Validation Accuracy:0.1856\n",
    "Epoch #285: Loss:2.4055, Accuracy:0.1959, Validation Loss:2.4543, Validation Accuracy:0.1806\n",
    "Epoch #286: Loss:2.4104, Accuracy:0.1947, Validation Loss:2.4489, Validation Accuracy:0.1954\n",
    "Epoch #287: Loss:2.4108, Accuracy:0.1910, Validation Loss:2.4538, Validation Accuracy:0.1872\n",
    "Epoch #288: Loss:2.4089, Accuracy:0.1881, Validation Loss:2.4529, Validation Accuracy:0.1872\n",
    "Epoch #289: Loss:2.4078, Accuracy:0.1864, Validation Loss:2.4511, Validation Accuracy:0.1823\n",
    "Epoch #290: Loss:2.4064, Accuracy:0.1877, Validation Loss:2.4482, Validation Accuracy:0.1872\n",
    "Epoch #291: Loss:2.4063, Accuracy:0.1943, Validation Loss:2.4558, Validation Accuracy:0.1790\n",
    "Epoch #292: Loss:2.4047, Accuracy:0.1889, Validation Loss:2.4557, Validation Accuracy:0.1773\n",
    "Epoch #293: Loss:2.4039, Accuracy:0.1848, Validation Loss:2.4550, Validation Accuracy:0.1938\n",
    "Epoch #294: Loss:2.4048, Accuracy:0.1873, Validation Loss:2.4599, Validation Accuracy:0.1872\n",
    "Epoch #295: Loss:2.4023, Accuracy:0.1910, Validation Loss:2.4541, Validation Accuracy:0.1888\n",
    "Epoch #296: Loss:2.4045, Accuracy:0.1901, Validation Loss:2.4658, Validation Accuracy:0.1856\n",
    "Epoch #297: Loss:2.4042, Accuracy:0.1852, Validation Loss:2.4556, Validation Accuracy:0.1905\n",
    "Epoch #298: Loss:2.4014, Accuracy:0.1893, Validation Loss:2.4640, Validation Accuracy:0.1839\n",
    "Epoch #299: Loss:2.3964, Accuracy:0.1934, Validation Loss:2.4555, Validation Accuracy:0.1970\n",
    "Epoch #300: Loss:2.3974, Accuracy:0.1906, Validation Loss:2.4589, Validation Accuracy:0.1823\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45893717, Accuracy:0.1823\n",
    "Labels: ['ek', 'ds', 'by', 'eo', 'ck', 'mb', 'sk', 'eb', 'ib', 'my', 'ce', 'yd', 'aa', 'sg', 'eg']\n",
    "Confusion Matrix:\n",
    "      ek  ds  by  eo  ck  mb  sk  eb  ib  my  ce  yd  aa  sg  eg\n",
    "t:ek  10   1   3   0   0   0   0   2   4   0   0   1   1   9  17\n",
    "t:ds   5   8   1   0   0   0   0   2   0   0   0   0   1   2  12\n",
    "t:by   8   1   2   0   0   0   0   4   0   0   0   1   1  10  13\n",
    "t:eo   8   0   1   0   0   0   0   7   3   0   0   1   0   6   8\n",
    "t:ck   3   0   0   0   0   0   0   0   1   0   0   2   0   1  16\n",
    "t:mb   6   2   2   0   0   0   0   5   6   0   0   5   0  13  13\n",
    "t:sk   4   1   5   0   0   0   0   3   1   0   0   4   0   5  10\n",
    "t:eb   5   1   5   0   0   0   0   3   7   0   0   6   1   5  17\n",
    "t:ib   4   0   1   0   0   0   0   1  10   0   0  20   1  12   5\n",
    "t:my   3   1   0   0   0   0   0   1   2   0   0   3   0   2   8\n",
    "t:ce   8   0   4   0   0   0   0   1   2   0   0   2   2   3   5\n",
    "t:yd   4   0   1   0   0   0   0   1  17   0   0  27   0  11   1\n",
    "t:aa   1   7   2   0   0   0   0   3   1   0   0   1   0   3  16\n",
    "t:sg  11   0   1   0   0   0   0   2   3   0   0   8   1  22   3\n",
    "t:eg   3   6   1   1   0   0   0   4   0   0   0   0   3   3  29\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ek       0.12      0.21      0.15        48\n",
    "          ds       0.29      0.26      0.27        31\n",
    "          by       0.07      0.05      0.06        40\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eb       0.08      0.06      0.07        50\n",
    "          ib       0.18      0.19      0.18        54\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          yd       0.33      0.44      0.38        62\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          sg       0.21      0.43      0.28        51\n",
    "          eg       0.17      0.58      0.26        50\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.10      0.15      0.11       609\n",
    "weighted avg       0.12      0.18      0.13       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 13:49:42 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 33 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.705226705970827, 2.6937845614547604, 2.68407797500222, 2.6767321134044226, 2.6702227537659393, 2.663822152931702, 2.6599835758334507, 2.6554009064860726, 2.649993711308697, 2.6436408689652366, 2.633205849157374, 2.620615036812518, 2.60229258271078, 2.583689555354502, 2.5655892459042553, 2.5490491722996405, 2.5551165908036757, 2.527026105201107, 2.5156739564560513, 2.515777659925138, 2.5228979990791607, 2.5056022677711276, 2.496707695066831, 2.497832812699191, 2.4906952475092092, 2.48856100347046, 2.4866719582593695, 2.495700687806203, 2.4904598430263976, 2.4771758723141524, 2.4761431017532725, 2.472997268040975, 2.468247243727761, 2.4710405804645057, 2.467592605424828, 2.4633887467908937, 2.4651690555127774, 2.4657294092507196, 2.4623137189836926, 2.4609849777910706, 2.457489850681599, 2.4569772217661288, 2.453562558382407, 2.4481678725463416, 2.4489676874063675, 2.448402819375099, 2.447108458611374, 2.447147596841571, 2.446892113912673, 2.4473115563979877, 2.4444088513040776, 2.4444427169211003, 2.4400953561410135, 2.440857220948819, 2.441859971517804, 2.4419792649781176, 2.4416482284151275, 2.442943280162091, 2.4390539508343525, 2.4335594964144853, 2.4368559116213193, 2.4394295137308304, 2.4396299297977944, 2.4369188028407605, 2.439323097614232, 2.4391476634296487, 2.4392077433456145, 2.440556369978806, 2.4403844553065808, 2.4384030074321577, 2.438367388714319, 2.4378371422709697, 2.436624018821027, 2.4362410485059365, 2.4387184395187202, 2.4387967829242325, 2.435944564628288, 2.43415391973674, 2.433240539334678, 2.432017951176084, 2.4323287511302527, 2.4344102623818933, 2.432316068749514, 2.4305317546737997, 2.4344313222982223, 2.435227558922102, 2.4328018460171954, 2.4303256016842445, 2.428135567697985, 2.428869205155396, 2.4290705290921215, 2.432855522299831, 2.4310810182286406, 2.4295929421736493, 2.430542772039404, 2.4345662437244786, 2.4340093057535355, 2.4318178342089474, 2.432244812913716, 2.4314326997265243, 2.4334299944108735, 2.435138162133729, 2.436994438296664, 2.433805325935627, 2.4330671835806963, 2.4358417858631154, 2.438426712659388, 2.4376969141717417, 2.4388359141075746, 2.4325706997919943, 2.4298293089436, 2.4302201588165584, 2.429858393269807, 2.42982369886439, 2.432153695126864, 2.4322677762637586, 2.4337686427512586, 2.4317729527922882, 2.4328624031618116, 2.4299164691386355, 2.434379381107775, 2.4365946357864856, 2.4360216634809873, 2.438040133571781, 2.436136446954386, 2.435572404188084, 2.4309507469434064, 2.433558102703251, 2.433793140358134, 2.4320060443408384, 2.431518053186351, 2.433254873419826, 2.4316176111475, 2.4355793057991364, 2.432693829873121, 2.4330127133524475, 2.435908327353216, 2.4335713570536845, 2.4354023307023573, 2.431687442735694, 2.4389166534436355, 2.4317534424009777, 2.4349366432340274, 2.433627906887011, 2.433136220440293, 2.433513937716805, 2.426191642366607, 2.4307581043400006, 2.4307970734456883, 2.435120121403085, 2.4287781182964054, 2.4305611204826967, 2.4293929590967487, 2.4326938314390887, 2.4325824782178906, 2.4313769892518744, 2.4316890736910315, 2.4295771094574325, 2.430583091400723, 2.4301452507526418, 2.431668427973154, 2.433474020026196, 2.4297699431088953, 2.432117140743337, 2.4294770030161037, 2.435810559293124, 2.434168956158392, 2.4352070925075235, 2.4413513647903167, 2.4392840169333474, 2.4352511447442966, 2.435726460173408, 2.43554979122331, 2.4400286220368885, 2.435293055911761, 2.4372006257375083, 2.433723066827934, 2.435726300444705, 2.431306912981231, 2.4369051750070354, 2.4347160524139655, 2.4359815492614345, 2.4419170396864316, 2.441395283136853, 2.446628900192837, 2.437142979726807, 2.442613012489231, 2.4446325345188136, 2.441861967343611, 2.4458070619548677, 2.442476333655747, 2.4443863898466764, 2.438056104093154, 2.44041787971221, 2.4420792774613855, 2.448429437302212, 2.4438125770080266, 2.446437369817975, 2.447095490833026, 2.448638774687042, 2.449028111052239, 2.4446860749537525, 2.447847370639419, 2.446993053840299, 2.4475256817289957, 2.4497941797002785, 2.449368471386789, 2.4428847411583208, 2.444432937452946, 2.441942995600708, 2.4454506355749173, 2.445386919090509, 2.4435654128909308, 2.4436534977898807, 2.445778533547187, 2.450268205946498, 2.448412558519586, 2.446479227351046, 2.4530403833279664, 2.451626027550408, 2.454733830563149, 2.446130444850828, 2.450437249025492, 2.4451100744050125, 2.450715729755721, 2.44716933599638, 2.4499324417270856, 2.438867204882241, 2.4399152502833523, 2.44194393204938, 2.4489036663412462, 2.449827663221187, 2.4535012667989498, 2.443956786188586, 2.445334416892141, 2.4435394311381873, 2.454177472391739, 2.442757133192617, 2.446519269536086, 2.4465798435148542, 2.4460077567640783, 2.4391463581955883, 2.440031608337252, 2.443177097928152, 2.450989335237074, 2.452124459402902, 2.4465904709545065, 2.4488769744221606, 2.4311926768135357, 2.4408012994600243, 2.4500514697558775, 2.4393961006784672, 2.4637314286725274, 2.4565326043929177, 2.451758955499809, 2.4529641500639014, 2.4412615874717973, 2.441995071855868, 2.433246325585251, 2.446675187065488, 2.440096095277758, 2.440478849097817, 2.44597597819048, 2.4387620129608756, 2.441815689083782, 2.434509525941119, 2.434525229856494, 2.4363915011996315, 2.430283345220907, 2.4350768470607562, 2.450788389090051, 2.4527662555963925, 2.4585723293630166, 2.460098977159397, 2.452481762724753, 2.459839291564741, 2.4520208635940928, 2.4588829136051373, 2.456677333866238, 2.451209083566525, 2.451096014044751, 2.4514226917367066, 2.456303573398559, 2.4568805475344604, 2.45431329816433, 2.448925694808584, 2.453751856078851, 2.452932288885508, 2.451136852524355, 2.448150820724287, 2.4557754277008508, 2.455742245237228, 2.455019153006167, 2.4599393259715563, 2.4541074156956917, 2.465787946688522, 2.455562416947338, 2.4640264933919673, 2.455466496729107, 2.458937163423435], 'val_acc': [0.07881773388004068, 0.08702791440614142, 0.0821018060317572, 0.0821018060317572, 0.0821018060317572, 0.0821018060317572, 0.13628899774625775, 0.13300492549666826, 0.12315270865002681, 0.13793103396892548, 0.15927750368913016, 0.14449917817448552, 0.1313628888825086, 0.13464696122997108, 0.15270935918995115, 0.1510673231630294, 0.16420361235713332, 0.1642036120635144, 0.17405582900802882, 0.18390804367699645, 0.17241379307898003, 0.18226600982774852, 0.18390804585467027, 0.17405582910590178, 0.18062397360508078, 0.17898193738241305, 0.16420361225926033, 0.1691297206336446, 0.16256157593871964, 0.18390804585467027, 0.1773399012576183, 0.18226600953412955, 0.18062397360508078, 0.17241379298110704, 0.1773399013554913, 0.18062397360508078, 0.1839080456589243, 0.18390804585467027, 0.178981937578159, 0.1806239735072078, 0.18555008197946501, 0.18555008178371907, 0.19376026250556577, 0.18555008188159203, 0.19211822628289804, 0.1806239735072078, 0.19376026240769278, 0.1904761900602303, 0.19376026240769278, 0.20032840700474475, 0.19540229872823348, 0.18390804575679728, 0.19376026260343873, 0.198686370977823, 0.2068965516996697, 0.20525451547700196, 0.18555008188159203, 0.18555008188159203, 0.19376026250556577, 0.20197044322741248, 0.19376026260343873, 0.19211822638077103, 0.20361247915646125, 0.192118226478644, 0.1888341542290545, 0.19211822628289804, 0.19211822628289804, 0.19211822628289804, 0.1937602623098198, 0.19211822628289804, 0.19211822628289804, 0.19540229853248753, 0.19376026240769278, 0.1904761901581033, 0.19704433475515526, 0.18555008178371907, 0.18883415403330855, 0.19376026240769278, 0.19704433465728227, 0.19704433465728227, 0.19704433465728227, 0.19704433455940928, 0.19211822618502505, 0.20032840690687176, 0.19868637078207702, 0.20032840690687176, 0.19540229853248753, 0.19704433465728227, 0.19211822618502505, 0.19868637078207702, 0.19868637087995, 0.19540229853248753, 0.19704433465728227, 0.19540229853248753, 0.19704433465728227, 0.19868637078207702, 0.19047619025597626, 0.19704433475515526, 0.19704433475515526, 0.19704433465728227, 0.19704433465728227, 0.1937602623098198, 0.18883415393543557, 0.19211822618502505, 0.19704433455940928, 0.19704433465728227, 0.1904761900602303, 0.18719211781064082, 0.19211822618502505, 0.1904761901581033, 0.19540229853248753, 0.19540229853248753, 0.19540229853248753, 0.19211822628289804, 0.19704433465728227, 0.19211822628289804, 0.19868637078207702, 0.19376026240769278, 0.19704433465728227, 0.1904761901581033, 0.205254515281256, 0.19211822618502505, 0.1871921179085138, 0.19211822618502505, 0.1822660094362566, 0.17733990106187233, 0.18226600953412955, 0.1937602623098198, 0.19704433455940928, 0.18883415383756258, 0.18390804546317835, 0.19047618996235732, 0.18883415403330855, 0.20361247925433423, 0.18719211781064082, 0.20361247925433423, 0.20032840700474475, 0.18883415393543557, 0.19704433455940928, 0.18883415393543557, 0.17569786434983972, 0.20032840690687176, 0.1871921179085138, 0.20032840700474475, 0.1937602623098198, 0.19376026201620086, 0.2101806237535132, 0.1904761896687384, 0.2101806237535132, 0.21510673212789747, 0.2068965516017967, 0.18555008139222715, 0.20361247935220722, 0.18883415373968962, 0.21182265987830795, 0.2101806237535132, 0.211822659780435, 0.20361247915646125, 0.2101806237535132, 0.20853858772659145, 0.2101806237535132, 0.21510673222577043, 0.2101806237535132, 0.21346469610097568, 0.21182265987830795, 0.21674876835056517, 0.21510673222577043, 0.19047618996235732, 0.1773399007682534, 0.205254515281256, 0.21182266007405393, 0.21346469610097568, 0.20853858772659145, 0.19704433446153632, 0.20853858782446444, 0.19868637068420403, 0.20689655150392372, 0.19868637048845808, 0.20689655140605073, 0.205254515281256, 0.1954022982388686, 0.2085385875308455, 0.19868637058633107, 0.19704433446153632, 0.20361247905858829, 0.19704433436366334, 0.19868637068420403, 0.19868637068420403, 0.19868637068420403, 0.19376026240769278, 0.20525451518338303, 0.20032840690687176, 0.20525451518338303, 0.20032840690687176, 0.20361247915646125, 0.19704433455940928, 0.19868637078207702, 0.1937602622119468, 0.19211822589140612, 0.19376026211407385, 0.19376026201620086, 0.2068965516017967, 0.20197044104973866, 0.18390804536530536, 0.1937602623098198, 0.19047618996235732, 0.19047619035384925, 0.19376026260343873, 0.17733990096399938, 0.19704433465728227, 0.18062397311571587, 0.18719211771276784, 0.18062397311571587, 0.18719211781064082, 0.19047618996235732, 0.18883415373968962, 0.18719211761489485, 0.18883415383756258, 0.18719211771276784, 0.1855500815879731, 0.17898193728454007, 0.18226600924051062, 0.1806239730178429, 0.1904761900602303, 0.18719211781064082, 0.19047618976661138, 0.20197044293379354, 0.18719211771276784, 0.19704433455940928, 0.19047618996235732, 0.18883415393543557, 0.1937602623098198, 0.19211822608715207, 0.19704433465728227, 0.20032840690687176, 0.205254515281256, 0.1855500814901001, 0.1937602623098198, 0.19211822628289804, 0.19704433446153632, 0.192118226478644, 0.1904761901581033, 0.19704433465728227, 0.19704433446153632, 0.1904761900602303, 0.1904761900602303, 0.19868637087995, 0.18390804546317835, 0.20689655130817777, 0.1937602622119468, 0.20197044104973866, 0.20525451547700196, 0.1822660094362566, 0.19211822608715207, 0.2019704431295395, 0.16748768401948494, 0.1871921179085138, 0.19540229853248753, 0.1954022981409956, 0.18062397321358886, 0.18226600904476467, 0.19704433455940928, 0.18883415403330855, 0.198686370977823, 0.17241379229599618, 0.1970443349509012, 0.1888341542290545, 0.16748768362799302, 0.19211822628289804, 0.1740558285186639, 0.18390804556105134, 0.160919539226687, 0.198686370977823, 0.16584564799256318, 0.1970443349509012, 0.1740558287144099, 0.18719211781064082, 0.1740558286165369, 0.1822660093383836, 0.18555008168584608, 0.18883415383756258, 0.19211822608715207, 0.17241379229599618, 0.1855500814901001, 0.18062397321358886, 0.19540229853248753, 0.18719211781064082, 0.18719211781064082, 0.1822660094362566, 0.1871921179085138, 0.17898193708879412, 0.1773399008661264, 0.19376026260343873, 0.18719211800638677, 0.18883415393543557, 0.18555008178371907, 0.19047619035384925, 0.18390804536530536, 0.19704433485302825, 0.1822660093383836], 'loss': [2.7153060202236294, 2.7011902134521297, 2.690344415210356, 2.681025655166814, 2.6743808651362113, 2.66855743719567, 2.6633049162995888, 2.657881068400044, 2.652897795416736, 2.646130061883946, 2.6378533458318065, 2.6264648582411496, 2.6115383632863574, 2.5939497859571015, 2.576111602783203, 2.5570231008823403, 2.557874033730133, 2.540532822187921, 2.5330473559103464, 2.520742911626671, 2.514579138863503, 2.5123606132530822, 2.508879460642225, 2.5036979801600965, 2.500055828231561, 2.4973377250303233, 2.493851834988447, 2.489985759938767, 2.49448334480458, 2.4886158552502704, 2.4856993578297892, 2.481034090093029, 2.4812733561106532, 2.479816133530478, 2.478470708408395, 2.4762786186696077, 2.473966257674983, 2.475610848569772, 2.476721321239119, 2.4740579151764543, 2.473399415495949, 2.4700141520960375, 2.4678091465325327, 2.469455103609841, 2.4674675598771176, 2.468142438865051, 2.468363990333291, 2.4677725680310134, 2.4652394999713625, 2.4657831863700976, 2.4606610758348655, 2.458265398072511, 2.4561651345395945, 2.45504667940081, 2.4562367492143133, 2.4567144587788983, 2.455303652722243, 2.4547025413239028, 2.45191738150203, 2.4526915769557442, 2.4498817821792507, 2.45276806565036, 2.4562449789145155, 2.455758229809865, 2.457841265128134, 2.4575744068842895, 2.4582121345786345, 2.456154577932808, 2.4559027036357466, 2.4543534443363764, 2.4547179379747144, 2.451752016627568, 2.4542039206385367, 2.4524811026986373, 2.4507808138947222, 2.4514088038301565, 2.4494347030866805, 2.450331000185111, 2.4481230120884074, 2.4518459875236056, 2.449285289345336, 2.4476353144009257, 2.4478216393527554, 2.44742934042913, 2.448359915754878, 2.4474908891399783, 2.44627156091177, 2.4456411781741854, 2.4430960944056266, 2.4447389972772933, 2.443119946936073, 2.443262303389563, 2.442341485894926, 2.4430412032031423, 2.443323388579445, 2.4438506868585668, 2.440838226694346, 2.441780457800174, 2.440845489991519, 2.442344021258658, 2.442985699064188, 2.4418856423983093, 2.441536935007303, 2.4412905380711174, 2.442101465162555, 2.4403241707803778, 2.440893026837578, 2.441022930987317, 2.4410428835381226, 2.4412121037438177, 2.439658073233383, 2.437109014189953, 2.437053463052675, 2.437211394750607, 2.4391631220400454, 2.4380587353598657, 2.4392648432533846, 2.4389366441683604, 2.4391838448737926, 2.4378883535122724, 2.4391042033749684, 2.43820370374519, 2.4377651639542783, 2.4367485426289837, 2.4353759134085027, 2.436318209920331, 2.4335439444077824, 2.4347936906364174, 2.43651663576553, 2.435793411952025, 2.436772559555649, 2.4369717013419776, 2.4366628882821337, 2.4364925287587442, 2.4393691467308654, 2.4361933107983162, 2.4366420884886315, 2.4371110553858952, 2.4370053568415084, 2.43753219179549, 2.4323237612996502, 2.4317684697419466, 2.428747040339319, 2.4306477169726173, 2.428951767876408, 2.430685880394687, 2.433316487110616, 2.4309928054927066, 2.428282400029396, 2.4294003131208477, 2.429051013061398, 2.427273603043762, 2.4266958577432183, 2.4278480096029793, 2.427268305157734, 2.426528547921465, 2.425162678332789, 2.4255696944877108, 2.42510841563497, 2.4271243649586514, 2.4254953596871003, 2.4268303438378553, 2.424845135236423, 2.423779602168277, 2.4233540912917997, 2.4241664648545598, 2.424582165712204, 2.4269666022588585, 2.424259233768471, 2.422524400707143, 2.422873002645661, 2.4222237338275634, 2.424233145978172, 2.4239482690666247, 2.4249446582990015, 2.4237601084386053, 2.42306574433736, 2.4220776243621076, 2.423057413982415, 2.4218211958295757, 2.420763072830451, 2.421640552779243, 2.4244208677593444, 2.4208240853687575, 2.4205817240219587, 2.4195573149765295, 2.4191300226677614, 2.4166973986420053, 2.414978971912141, 2.414620019571982, 2.4154856362626784, 2.4145477130427744, 2.416886854661319, 2.414781570238744, 2.416132789177082, 2.415668586343221, 2.4115195666984857, 2.4129025727571647, 2.415000098246079, 2.4142793480130926, 2.4163758288663515, 2.412379971666747, 2.415405836731991, 2.4133955342568902, 2.412522062971362, 2.407528839072163, 2.4074480361272665, 2.4082708568298843, 2.4063880806830875, 2.4039382813157975, 2.4061099646762165, 2.4064913160257517, 2.404511574700138, 2.404677313896665, 2.4055752372350048, 2.407352866578151, 2.4060251202671434, 2.407490050964042, 2.411057059035409, 2.408250230734353, 2.404875026933956, 2.4033693793373185, 2.4028647618616876, 2.3994803116306875, 2.3980076320851853, 2.4067842095784338, 2.4097251808863644, 2.4120132620574513, 2.4112591053181363, 2.41317810238754, 2.4095398643423156, 2.410833911680343, 2.4091618990261696, 2.4084674354451394, 2.4081456843342868, 2.405403630787342, 2.4077694831198, 2.409655759368834, 2.4139939112340154, 2.41091554443939, 2.4114831097072154, 2.404215300988857, 2.403476365146206, 2.400457369571349, 2.396272884991624, 2.396456066343084, 2.400344992761005, 2.401301458973659, 2.4027112979418934, 2.408493670888505, 2.40578165445974, 2.4097692610057226, 2.4087278361193207, 2.4162139977762584, 2.41695751037441, 2.412241240056878, 2.4147278229558737, 2.4075489203787925, 2.407483370739821, 2.404696835629504, 2.408665166302628, 2.402527693801347, 2.399044471795554, 2.393352908422325, 2.39260523568923, 2.402416355751868, 2.40670423987465, 2.397050217583439, 2.393628236155735, 2.3983981419392926, 2.4156767231238208, 2.4146627781572283, 2.416633702009855, 2.4154543357708125, 2.4150671319550314, 2.4151645770063146, 2.4131052859265214, 2.411934199421313, 2.409700144879382, 2.4103781225255383, 2.4094231813105713, 2.4104494347464622, 2.411595585801518, 2.408357843089642, 2.405469008686606, 2.4104410935231546, 2.4108379040900196, 2.4088572296518564, 2.4078451322089474, 2.4064106626921857, 2.406322363514675, 2.404659082757374, 2.403922560474466, 2.404821180367127, 2.4023074732668834, 2.4045375597795178, 2.4041590235316534, 2.4014081416433597, 2.39639863860191, 2.3974356618015675], 'acc': [0.0636550313424036, 0.0747433264336302, 0.08090349070467744, 0.08131416832274725, 0.08131416794027391, 0.08131416834569564, 0.0952772077195943, 0.1470225873730266, 0.13634496912574376, 0.13511293666318702, 0.1441478436732439, 0.15400410648856075, 0.13798767957966432, 0.12361396258310615, 0.14004106806166608, 0.15482546113722134, 0.1449691981077194, 0.16016427115126067, 0.15728952821642467, 0.1618069810360609, 0.15852156087480776, 0.1564681724111647, 0.16509240295975114, 0.1622176586541307, 0.16386036990971536, 0.16427104791943786, 0.16468172553750768, 0.16344969189999284, 0.16262833764298495, 0.1613963028121533, 0.15975359274988546, 0.1593429165026001, 0.166735112648725, 0.16221765966080054, 0.1679671448970967, 0.1642710467444798, 0.16427104752778518, 0.1696098555652023, 0.16632443599142824, 0.1642710467444798, 0.16427104733195882, 0.16591375661092128, 0.1708418902002076, 0.16591375800006444, 0.17084188902524952, 0.1716632432638987, 0.17043121080134194, 0.17289527629557577, 0.1757700200320759, 0.17248460065412816, 0.17166324424303042, 0.17043121060551558, 0.17125256742662473, 0.17002053337909848, 0.1671457900709685, 0.17166324543634723, 0.17577001944459683, 0.1728952767239459, 0.17412730955979663, 0.17084188802775907, 0.17618069864763616, 0.16714578966095708, 0.16919917876715532, 0.17453798659038738, 0.17166324424303042, 0.16878850093490044, 0.1708418901818489, 0.16960985654433405, 0.1687885016998471, 0.16960985519190833, 0.17125256703497202, 0.1708418882235854, 0.17330595530278875, 0.1683778232984719, 0.1716632448488682, 0.16960985615268134, 0.17125256603748157, 0.16837782310264557, 0.17084188824194413, 0.1704312121904851, 0.16714578948348943, 0.17207392187945897, 0.17166324443885678, 0.17043121080134194, 0.1679671460720548, 0.17330595571280016, 0.16755646749321196, 0.17002053298744577, 0.17084188804611777, 0.1679671464820662, 0.1704312126004965, 0.1712525666249606, 0.17289527690141354, 0.16591375880172854, 0.172895277898904, 0.17084188800940034, 0.1712525668391457, 0.16509240176643433, 0.16591375662927998, 0.16673511403786817, 0.17330595414618938, 0.17371663135424775, 0.17330595412783065, 0.17289527609974942, 0.17248459987082276, 0.17330595473366842, 0.17371663270667348, 0.17248460065412816, 0.17248460065412816, 0.1704312125821378, 0.1733059539320043, 0.17330595471530968, 0.17084188938018477, 0.17043121240467016, 0.16960985656269278, 0.16878850111236807, 0.17043121040968925, 0.17371663313504362, 0.16878850111236807, 0.16837782449178873, 0.16550307959868923, 0.17453798678621374, 0.1704312121904851, 0.17412730975562296, 0.17002053478660034, 0.1757700204420873, 0.17125256742662473, 0.16673511206124597, 0.17289527770307764, 0.1712525678182774, 0.16714578946513073, 0.16755646708320054, 0.17084188959436986, 0.16344969268329823, 0.17002053377075116, 0.16550308118365872, 0.17577001944459683, 0.1675564676890383, 0.1642710465302947, 0.1626283362538418, 0.17453798715950772, 0.1704312117988324, 0.17535934398061687, 0.172895277898904, 0.1745379869820401, 0.18193018569226627, 0.17494866579342672, 0.17618069747267806, 0.18069815103890224, 0.17782340952992684, 0.18316221717569128, 0.18275154073257954, 0.18193018530061358, 0.18151950647086823, 0.1815195062934006, 0.18151950609757425, 0.18069815125308733, 0.17864476496189283, 0.17659137507238917, 0.18151950666669459, 0.1786447637502173, 0.18069815223221905, 0.18357289618290426, 0.18357289536288143, 0.18480492782543817, 0.18357289636037188, 0.1856262842548946, 0.18275154110587352, 0.18644763790606472, 0.17905544236577756, 0.1798767968002531, 0.18480492743378546, 0.17905544216995123, 0.1782340865421589, 0.18069815144891366, 0.1815195082700228, 0.1790554425616039, 0.18357289634201315, 0.17700205251299136, 0.18110883063359426, 0.18275154110587352, 0.17782340933410049, 0.18685831552413454, 0.18316221835064936, 0.1790554409766344, 0.18398357258929854, 0.18398357337260393, 0.18767967033190405, 0.18767967193523227, 0.1835728955770665, 0.18850102617388143, 0.18480492860874356, 0.19096509346726984, 0.18685831530994948, 0.1917864485075831, 0.18685831570160216, 0.1913757700878492, 0.1856262826699251, 0.18973306002558135, 0.18726899472717387, 0.18973306022140768, 0.18151950725417362, 0.19096509186394162, 0.18521560563933434, 0.1905544154575473, 0.18644763868937012, 0.18809035015913986, 0.19342915760907795, 0.18932238281752295, 0.19425051341433788, 0.1885010262105989, 0.18973306043559277, 0.19096509149064764, 0.19342915880239475, 0.1930184811659662, 0.1897330606130604, 0.19301848155761892, 0.197125257150838, 0.19178644751009266, 0.19260780296041735, 0.19342915915733, 0.19383983540461538, 0.19712525775667578, 0.18891170537692076, 0.19876796625233284, 0.19753593320229704, 0.18850102619224016, 0.18644763829771743, 0.1893223822300439, 0.18644763788770602, 0.1876796707786329, 0.19178644651260218, 0.18316221653313608, 0.18767967117028558, 0.18398357378261535, 0.18234086309615102, 0.18767967076027417, 0.18644763810189108, 0.1839835727851249, 0.1880903499633135, 0.18562628345323048, 0.18850102758138332, 0.1880903487699967, 0.19178644831175676, 0.19137576910871745, 0.18932238221168518, 0.19466119007163468, 0.1901437378394775, 0.1917864467635047, 0.19383983542297412, 0.18275154032256813, 0.1852156066368248, 0.1856262838448832, 0.18685831710910406, 0.17987679599858897, 0.1893223830133493, 0.18275153953926274, 0.1954825456810683, 0.197946612582804, 0.1938398362062795, 0.19260780178545928, 0.1950718688646626, 0.19671457990606217, 0.20000000108316449, 0.20000000008567403, 0.19589322429662856, 0.19425051241684743, 0.199589323073442, 0.1909650917048327, 0.19425051245356487, 0.18275154071422084, 0.1856262822782724, 0.18480492802126453, 0.18316221754898526, 0.18644763949103424, 0.1860369618546057, 0.1930184803826608, 0.18603696206879078, 0.18603696126712665, 0.18685831630743993, 0.19014373605868165, 0.19301848157597762, 0.1880903499633135, 0.18767967232688496, 0.19589322427826986, 0.19466119146077784, 0.1909650917048327, 0.18809034896582305, 0.18644763908102283, 0.1876796717210472, 0.19425051245356487, 0.1889117051627357, 0.1848049284129172, 0.1872689933380307, 0.19096509307561713, 0.19014373725199846, 0.18521560485602895, 0.18932238281752295, 0.19342915897986238, 0.19055441584920002]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
