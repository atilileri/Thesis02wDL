{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf5.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 13:14:39 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'Front', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '04', '05', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000016CA72EBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000016CA3A86EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6129, Accuracy:0.1860, Validation Loss:1.6078, Validation Accuracy:0.2430\n",
    "Epoch #2: Loss:1.6084, Accuracy:0.2263, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6071, Accuracy:0.2324, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6053, Accuracy:0.2324, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6052, Accuracy:0.2324, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6050, Accuracy:0.2324, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6040, Accuracy:0.2337, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6035, Accuracy:0.2337, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6028, Accuracy:0.2337, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6041, Accuracy:0.2337, Validation Loss:1.6050, Validation Accuracy:0.2299\n",
    "Epoch #20: Loss:1.6044, Accuracy:0.2320, Validation Loss:1.6042, Validation Accuracy:0.2315\n",
    "Epoch #21: Loss:1.6043, Accuracy:0.2349, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6035, Accuracy:0.2357, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6031, Accuracy:0.2333, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2315\n",
    "Epoch #25: Loss:1.6031, Accuracy:0.2333, Validation Loss:1.6027, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6033, Accuracy:0.2312, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6034, Accuracy:0.2324, Validation Loss:1.6016, Validation Accuracy:0.2332\n",
    "Epoch #28: Loss:1.6033, Accuracy:0.2324, Validation Loss:1.6005, Validation Accuracy:0.2332\n",
    "Epoch #29: Loss:1.6025, Accuracy:0.2324, Validation Loss:1.6002, Validation Accuracy:0.2348\n",
    "Epoch #30: Loss:1.6021, Accuracy:0.2324, Validation Loss:1.5996, Validation Accuracy:0.2332\n",
    "Epoch #31: Loss:1.6020, Accuracy:0.2320, Validation Loss:1.5991, Validation Accuracy:0.2332\n",
    "Epoch #32: Loss:1.6017, Accuracy:0.2333, Validation Loss:1.5994, Validation Accuracy:0.2332\n",
    "Epoch #33: Loss:1.6015, Accuracy:0.2333, Validation Loss:1.5993, Validation Accuracy:0.2315\n",
    "Epoch #34: Loss:1.6013, Accuracy:0.2390, Validation Loss:1.6004, Validation Accuracy:0.2299\n",
    "Epoch #35: Loss:1.6028, Accuracy:0.2345, Validation Loss:1.6012, Validation Accuracy:0.2282\n",
    "Epoch #36: Loss:1.6021, Accuracy:0.2337, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #37: Loss:1.6028, Accuracy:0.2337, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #38: Loss:1.6023, Accuracy:0.2353, Validation Loss:1.6002, Validation Accuracy:0.2397\n",
    "Epoch #39: Loss:1.6014, Accuracy:0.2370, Validation Loss:1.5997, Validation Accuracy:0.2430\n",
    "Epoch #40: Loss:1.6020, Accuracy:0.2378, Validation Loss:1.6029, Validation Accuracy:0.2233\n",
    "Epoch #41: Loss:1.6021, Accuracy:0.2382, Validation Loss:1.5997, Validation Accuracy:0.2447\n",
    "Epoch #42: Loss:1.6013, Accuracy:0.2423, Validation Loss:1.5976, Validation Accuracy:0.2529\n",
    "Epoch #43: Loss:1.6008, Accuracy:0.2415, Validation Loss:1.5988, Validation Accuracy:0.2381\n",
    "Epoch #44: Loss:1.6003, Accuracy:0.2378, Validation Loss:1.5987, Validation Accuracy:0.2512\n",
    "Epoch #45: Loss:1.6000, Accuracy:0.2390, Validation Loss:1.5976, Validation Accuracy:0.2611\n",
    "Epoch #46: Loss:1.6002, Accuracy:0.2431, Validation Loss:1.5982, Validation Accuracy:0.2545\n",
    "Epoch #47: Loss:1.6005, Accuracy:0.2456, Validation Loss:1.5960, Validation Accuracy:0.2430\n",
    "Epoch #48: Loss:1.5996, Accuracy:0.2456, Validation Loss:1.5958, Validation Accuracy:0.2447\n",
    "Epoch #49: Loss:1.5989, Accuracy:0.2423, Validation Loss:1.6023, Validation Accuracy:0.2447\n",
    "Epoch #50: Loss:1.6067, Accuracy:0.2341, Validation Loss:1.6103, Validation Accuracy:0.2332\n",
    "Epoch #51: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6009, Validation Accuracy:0.2463\n",
    "Epoch #52: Loss:1.6024, Accuracy:0.2415, Validation Loss:1.6000, Validation Accuracy:0.2447\n",
    "Epoch #53: Loss:1.6034, Accuracy:0.2353, Validation Loss:1.6001, Validation Accuracy:0.2397\n",
    "Epoch #54: Loss:1.6013, Accuracy:0.2501, Validation Loss:1.5995, Validation Accuracy:0.2545\n",
    "Epoch #55: Loss:1.6004, Accuracy:0.2390, Validation Loss:1.5995, Validation Accuracy:0.2332\n",
    "Epoch #56: Loss:1.6007, Accuracy:0.2345, Validation Loss:1.5985, Validation Accuracy:0.2315\n",
    "Epoch #57: Loss:1.6015, Accuracy:0.2353, Validation Loss:1.5985, Validation Accuracy:0.2332\n",
    "Epoch #58: Loss:1.6013, Accuracy:0.2333, Validation Loss:1.5980, Validation Accuracy:0.2332\n",
    "Epoch #59: Loss:1.6004, Accuracy:0.2353, Validation Loss:1.5973, Validation Accuracy:0.2397\n",
    "Epoch #60: Loss:1.5997, Accuracy:0.2402, Validation Loss:1.5968, Validation Accuracy:0.2299\n",
    "Epoch #61: Loss:1.5993, Accuracy:0.2386, Validation Loss:1.5967, Validation Accuracy:0.2348\n",
    "Epoch #62: Loss:1.5981, Accuracy:0.2402, Validation Loss:1.5960, Validation Accuracy:0.2365\n",
    "Epoch #63: Loss:1.5994, Accuracy:0.2407, Validation Loss:1.5970, Validation Accuracy:0.2348\n",
    "Epoch #64: Loss:1.6007, Accuracy:0.2349, Validation Loss:1.5980, Validation Accuracy:0.2414\n",
    "Epoch #65: Loss:1.5997, Accuracy:0.2353, Validation Loss:1.5995, Validation Accuracy:0.2348\n",
    "Epoch #66: Loss:1.5997, Accuracy:0.2472, Validation Loss:1.5969, Validation Accuracy:0.2299\n",
    "Epoch #67: Loss:1.5999, Accuracy:0.2448, Validation Loss:1.5977, Validation Accuracy:0.2512\n",
    "Epoch #68: Loss:1.5985, Accuracy:0.2476, Validation Loss:1.5974, Validation Accuracy:0.2578\n",
    "Epoch #69: Loss:1.5985, Accuracy:0.2439, Validation Loss:1.5979, Validation Accuracy:0.2529\n",
    "Epoch #70: Loss:1.5977, Accuracy:0.2423, Validation Loss:1.5982, Validation Accuracy:0.2512\n",
    "Epoch #71: Loss:1.5994, Accuracy:0.2419, Validation Loss:1.5978, Validation Accuracy:0.2479\n",
    "Epoch #72: Loss:1.6002, Accuracy:0.2431, Validation Loss:1.5982, Validation Accuracy:0.2496\n",
    "Epoch #73: Loss:1.5997, Accuracy:0.2415, Validation Loss:1.5949, Validation Accuracy:0.2545\n",
    "Epoch #74: Loss:1.6001, Accuracy:0.2361, Validation Loss:1.5970, Validation Accuracy:0.2529\n",
    "Epoch #75: Loss:1.5994, Accuracy:0.2419, Validation Loss:1.5982, Validation Accuracy:0.2594\n",
    "Epoch #76: Loss:1.5980, Accuracy:0.2448, Validation Loss:1.5955, Validation Accuracy:0.2479\n",
    "Epoch #77: Loss:1.5975, Accuracy:0.2419, Validation Loss:1.5940, Validation Accuracy:0.2545\n",
    "Epoch #78: Loss:1.5971, Accuracy:0.2439, Validation Loss:1.5949, Validation Accuracy:0.2496\n",
    "Epoch #79: Loss:1.5986, Accuracy:0.2448, Validation Loss:1.5949, Validation Accuracy:0.2529\n",
    "Epoch #80: Loss:1.6022, Accuracy:0.2402, Validation Loss:1.5980, Validation Accuracy:0.2512\n",
    "Epoch #81: Loss:1.5997, Accuracy:0.2415, Validation Loss:1.5999, Validation Accuracy:0.2447\n",
    "Epoch #82: Loss:1.6015, Accuracy:0.2468, Validation Loss:1.5970, Validation Accuracy:0.2512\n",
    "Epoch #83: Loss:1.6000, Accuracy:0.2464, Validation Loss:1.5960, Validation Accuracy:0.2562\n",
    "Epoch #84: Loss:1.6001, Accuracy:0.2464, Validation Loss:1.5971, Validation Accuracy:0.2562\n",
    "Epoch #85: Loss:1.5990, Accuracy:0.2509, Validation Loss:1.5966, Validation Accuracy:0.2562\n",
    "Epoch #86: Loss:1.5978, Accuracy:0.2439, Validation Loss:1.5955, Validation Accuracy:0.2562\n",
    "Epoch #87: Loss:1.5989, Accuracy:0.2501, Validation Loss:1.6008, Validation Accuracy:0.2381\n",
    "Epoch #88: Loss:1.6005, Accuracy:0.2431, Validation Loss:1.5982, Validation Accuracy:0.2447\n",
    "Epoch #89: Loss:1.5990, Accuracy:0.2423, Validation Loss:1.5959, Validation Accuracy:0.2529\n",
    "Epoch #90: Loss:1.5994, Accuracy:0.2472, Validation Loss:1.5982, Validation Accuracy:0.2365\n",
    "Epoch #91: Loss:1.5984, Accuracy:0.2402, Validation Loss:1.5972, Validation Accuracy:0.2562\n",
    "Epoch #92: Loss:1.5966, Accuracy:0.2472, Validation Loss:1.5941, Validation Accuracy:0.2463\n",
    "Epoch #93: Loss:1.5965, Accuracy:0.2485, Validation Loss:1.5963, Validation Accuracy:0.2447\n",
    "Epoch #94: Loss:1.5978, Accuracy:0.2423, Validation Loss:1.5963, Validation Accuracy:0.2447\n",
    "Epoch #95: Loss:1.5967, Accuracy:0.2489, Validation Loss:1.5975, Validation Accuracy:0.2447\n",
    "Epoch #96: Loss:1.5971, Accuracy:0.2431, Validation Loss:1.5965, Validation Accuracy:0.2430\n",
    "Epoch #97: Loss:1.5976, Accuracy:0.2439, Validation Loss:1.5955, Validation Accuracy:0.2447\n",
    "Epoch #98: Loss:1.5972, Accuracy:0.2452, Validation Loss:1.5965, Validation Accuracy:0.2397\n",
    "Epoch #99: Loss:1.5976, Accuracy:0.2431, Validation Loss:1.5966, Validation Accuracy:0.2430\n",
    "Epoch #100: Loss:1.5955, Accuracy:0.2460, Validation Loss:1.5967, Validation Accuracy:0.2381\n",
    "Epoch #101: Loss:1.5949, Accuracy:0.2522, Validation Loss:1.5976, Validation Accuracy:0.2545\n",
    "Epoch #102: Loss:1.5952, Accuracy:0.2501, Validation Loss:1.5977, Validation Accuracy:0.2479\n",
    "Epoch #103: Loss:1.5949, Accuracy:0.2522, Validation Loss:1.5980, Validation Accuracy:0.2463\n",
    "Epoch #104: Loss:1.5949, Accuracy:0.2530, Validation Loss:1.5997, Validation Accuracy:0.2512\n",
    "Epoch #105: Loss:1.5948, Accuracy:0.2542, Validation Loss:1.6006, Validation Accuracy:0.2447\n",
    "Epoch #106: Loss:1.5946, Accuracy:0.2538, Validation Loss:1.6004, Validation Accuracy:0.2529\n",
    "Epoch #107: Loss:1.5933, Accuracy:0.2530, Validation Loss:1.5970, Validation Accuracy:0.2463\n",
    "Epoch #108: Loss:1.5964, Accuracy:0.2567, Validation Loss:1.5989, Validation Accuracy:0.2299\n",
    "Epoch #109: Loss:1.5971, Accuracy:0.2472, Validation Loss:1.5972, Validation Accuracy:0.2447\n",
    "Epoch #110: Loss:1.5977, Accuracy:0.2468, Validation Loss:1.5993, Validation Accuracy:0.2611\n",
    "Epoch #111: Loss:1.5966, Accuracy:0.2489, Validation Loss:1.5980, Validation Accuracy:0.2365\n",
    "Epoch #112: Loss:1.5983, Accuracy:0.2435, Validation Loss:1.5954, Validation Accuracy:0.2397\n",
    "Epoch #113: Loss:1.5962, Accuracy:0.2439, Validation Loss:1.5962, Validation Accuracy:0.2578\n",
    "Epoch #114: Loss:1.5951, Accuracy:0.2485, Validation Loss:1.5985, Validation Accuracy:0.2397\n",
    "Epoch #115: Loss:1.5968, Accuracy:0.2476, Validation Loss:1.5943, Validation Accuracy:0.2611\n",
    "Epoch #116: Loss:1.5950, Accuracy:0.2497, Validation Loss:1.5948, Validation Accuracy:0.2463\n",
    "Epoch #117: Loss:1.5958, Accuracy:0.2554, Validation Loss:1.5967, Validation Accuracy:0.2512\n",
    "Epoch #118: Loss:1.5947, Accuracy:0.2546, Validation Loss:1.5977, Validation Accuracy:0.2529\n",
    "Epoch #119: Loss:1.5944, Accuracy:0.2485, Validation Loss:1.5982, Validation Accuracy:0.2545\n",
    "Epoch #120: Loss:1.5945, Accuracy:0.2571, Validation Loss:1.5995, Validation Accuracy:0.2578\n",
    "Epoch #121: Loss:1.5951, Accuracy:0.2501, Validation Loss:1.5972, Validation Accuracy:0.2594\n",
    "Epoch #122: Loss:1.5938, Accuracy:0.2530, Validation Loss:1.5981, Validation Accuracy:0.2430\n",
    "Epoch #123: Loss:1.5930, Accuracy:0.2554, Validation Loss:1.5974, Validation Accuracy:0.2644\n",
    "Epoch #124: Loss:1.5928, Accuracy:0.2530, Validation Loss:1.5977, Validation Accuracy:0.2496\n",
    "Epoch #125: Loss:1.5944, Accuracy:0.2464, Validation Loss:1.5994, Validation Accuracy:0.2348\n",
    "Epoch #126: Loss:1.5916, Accuracy:0.2534, Validation Loss:1.6017, Validation Accuracy:0.2545\n",
    "Epoch #127: Loss:1.5923, Accuracy:0.2546, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #128: Loss:1.5926, Accuracy:0.2480, Validation Loss:1.6013, Validation Accuracy:0.2479\n",
    "Epoch #129: Loss:1.5960, Accuracy:0.2435, Validation Loss:1.6011, Validation Accuracy:0.2348\n",
    "Epoch #130: Loss:1.5959, Accuracy:0.2485, Validation Loss:1.5988, Validation Accuracy:0.2315\n",
    "Epoch #131: Loss:1.5960, Accuracy:0.2517, Validation Loss:1.5979, Validation Accuracy:0.2529\n",
    "Epoch #132: Loss:1.5957, Accuracy:0.2464, Validation Loss:1.5969, Validation Accuracy:0.2381\n",
    "Epoch #133: Loss:1.5940, Accuracy:0.2538, Validation Loss:1.5976, Validation Accuracy:0.2529\n",
    "Epoch #134: Loss:1.5928, Accuracy:0.2546, Validation Loss:1.5967, Validation Accuracy:0.2594\n",
    "Epoch #135: Loss:1.5934, Accuracy:0.2509, Validation Loss:1.5961, Validation Accuracy:0.2447\n",
    "Epoch #136: Loss:1.5923, Accuracy:0.2542, Validation Loss:1.5975, Validation Accuracy:0.2512\n",
    "Epoch #137: Loss:1.5931, Accuracy:0.2550, Validation Loss:1.5993, Validation Accuracy:0.2217\n",
    "Epoch #138: Loss:1.5928, Accuracy:0.2472, Validation Loss:1.5973, Validation Accuracy:0.2512\n",
    "Epoch #139: Loss:1.5925, Accuracy:0.2559, Validation Loss:1.5997, Validation Accuracy:0.2562\n",
    "Epoch #140: Loss:1.5912, Accuracy:0.2559, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #141: Loss:1.6002, Accuracy:0.2386, Validation Loss:1.6061, Validation Accuracy:0.2381\n",
    "Epoch #142: Loss:1.5997, Accuracy:0.2439, Validation Loss:1.6009, Validation Accuracy:0.2578\n",
    "Epoch #143: Loss:1.5938, Accuracy:0.2476, Validation Loss:1.5992, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5928, Accuracy:0.2538, Validation Loss:1.5956, Validation Accuracy:0.2381\n",
    "Epoch #145: Loss:1.5941, Accuracy:0.2509, Validation Loss:1.5949, Validation Accuracy:0.2397\n",
    "Epoch #146: Loss:1.5950, Accuracy:0.2419, Validation Loss:1.5934, Validation Accuracy:0.2562\n",
    "Epoch #147: Loss:1.5946, Accuracy:0.2522, Validation Loss:1.5932, Validation Accuracy:0.2447\n",
    "Epoch #148: Loss:1.5963, Accuracy:0.2505, Validation Loss:1.5954, Validation Accuracy:0.2414\n",
    "Epoch #149: Loss:1.5934, Accuracy:0.2448, Validation Loss:1.5926, Validation Accuracy:0.2496\n",
    "Epoch #150: Loss:1.5953, Accuracy:0.2427, Validation Loss:1.5921, Validation Accuracy:0.2315\n",
    "Epoch #151: Loss:1.5939, Accuracy:0.2567, Validation Loss:1.5918, Validation Accuracy:0.2594\n",
    "Epoch #152: Loss:1.5931, Accuracy:0.2612, Validation Loss:1.5913, Validation Accuracy:0.2496\n",
    "Epoch #153: Loss:1.5924, Accuracy:0.2608, Validation Loss:1.5926, Validation Accuracy:0.2611\n",
    "Epoch #154: Loss:1.5928, Accuracy:0.2595, Validation Loss:1.5932, Validation Accuracy:0.2677\n",
    "Epoch #155: Loss:1.5926, Accuracy:0.2563, Validation Loss:1.5937, Validation Accuracy:0.2611\n",
    "Epoch #156: Loss:1.5928, Accuracy:0.2641, Validation Loss:1.5919, Validation Accuracy:0.2644\n",
    "Epoch #157: Loss:1.5923, Accuracy:0.2620, Validation Loss:1.5926, Validation Accuracy:0.2545\n",
    "Epoch #158: Loss:1.5917, Accuracy:0.2608, Validation Loss:1.5905, Validation Accuracy:0.2562\n",
    "Epoch #159: Loss:1.5903, Accuracy:0.2645, Validation Loss:1.5916, Validation Accuracy:0.2545\n",
    "Epoch #160: Loss:1.5908, Accuracy:0.2620, Validation Loss:1.5928, Validation Accuracy:0.2414\n",
    "Epoch #161: Loss:1.5899, Accuracy:0.2616, Validation Loss:1.5918, Validation Accuracy:0.2529\n",
    "Epoch #162: Loss:1.5887, Accuracy:0.2690, Validation Loss:1.5916, Validation Accuracy:0.2496\n",
    "Epoch #163: Loss:1.5890, Accuracy:0.2653, Validation Loss:1.5947, Validation Accuracy:0.2496\n",
    "Epoch #164: Loss:1.5900, Accuracy:0.2612, Validation Loss:1.5957, Validation Accuracy:0.2496\n",
    "Epoch #165: Loss:1.5885, Accuracy:0.2661, Validation Loss:1.5965, Validation Accuracy:0.2365\n",
    "Epoch #166: Loss:1.5877, Accuracy:0.2661, Validation Loss:1.5991, Validation Accuracy:0.2282\n",
    "Epoch #167: Loss:1.5880, Accuracy:0.2493, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #168: Loss:1.5866, Accuracy:0.2587, Validation Loss:1.6031, Validation Accuracy:0.2397\n",
    "Epoch #169: Loss:1.5873, Accuracy:0.2550, Validation Loss:1.6044, Validation Accuracy:0.2381\n",
    "Epoch #170: Loss:1.5849, Accuracy:0.2513, Validation Loss:1.5998, Validation Accuracy:0.2545\n",
    "Epoch #171: Loss:1.5860, Accuracy:0.2624, Validation Loss:1.5966, Validation Accuracy:0.2414\n",
    "Epoch #172: Loss:1.5867, Accuracy:0.2674, Validation Loss:1.6005, Validation Accuracy:0.2365\n",
    "Epoch #173: Loss:1.5880, Accuracy:0.2674, Validation Loss:1.5988, Validation Accuracy:0.2447\n",
    "Epoch #174: Loss:1.5857, Accuracy:0.2669, Validation Loss:1.5958, Validation Accuracy:0.2447\n",
    "Epoch #175: Loss:1.5861, Accuracy:0.2645, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #176: Loss:1.5846, Accuracy:0.2674, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #177: Loss:1.5846, Accuracy:0.2628, Validation Loss:1.5993, Validation Accuracy:0.2365\n",
    "Epoch #178: Loss:1.5863, Accuracy:0.2600, Validation Loss:1.6010, Validation Accuracy:0.2299\n",
    "Epoch #179: Loss:1.5850, Accuracy:0.2624, Validation Loss:1.6020, Validation Accuracy:0.2348\n",
    "Epoch #180: Loss:1.5838, Accuracy:0.2632, Validation Loss:1.6026, Validation Accuracy:0.2381\n",
    "Epoch #181: Loss:1.5828, Accuracy:0.2665, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #182: Loss:1.5832, Accuracy:0.2657, Validation Loss:1.6004, Validation Accuracy:0.2479\n",
    "Epoch #183: Loss:1.5817, Accuracy:0.2710, Validation Loss:1.6010, Validation Accuracy:0.2447\n",
    "Epoch #184: Loss:1.5839, Accuracy:0.2641, Validation Loss:1.6045, Validation Accuracy:0.2348\n",
    "Epoch #185: Loss:1.5819, Accuracy:0.2678, Validation Loss:1.6014, Validation Accuracy:0.2496\n",
    "Epoch #186: Loss:1.5822, Accuracy:0.2665, Validation Loss:1.6021, Validation Accuracy:0.2299\n",
    "Epoch #187: Loss:1.5843, Accuracy:0.2612, Validation Loss:1.6011, Validation Accuracy:0.2447\n",
    "Epoch #188: Loss:1.5841, Accuracy:0.2587, Validation Loss:1.6031, Validation Accuracy:0.2299\n",
    "Epoch #189: Loss:1.5858, Accuracy:0.2637, Validation Loss:1.6062, Validation Accuracy:0.2315\n",
    "Epoch #190: Loss:1.5883, Accuracy:0.2587, Validation Loss:1.6104, Validation Accuracy:0.2365\n",
    "Epoch #191: Loss:1.5904, Accuracy:0.2538, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #192: Loss:1.5840, Accuracy:0.2637, Validation Loss:1.6015, Validation Accuracy:0.2250\n",
    "Epoch #193: Loss:1.5827, Accuracy:0.2612, Validation Loss:1.5980, Validation Accuracy:0.2348\n",
    "Epoch #194: Loss:1.5827, Accuracy:0.2624, Validation Loss:1.6022, Validation Accuracy:0.2200\n",
    "Epoch #195: Loss:1.5819, Accuracy:0.2665, Validation Loss:1.5988, Validation Accuracy:0.2397\n",
    "Epoch #196: Loss:1.5805, Accuracy:0.2628, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #197: Loss:1.5810, Accuracy:0.2632, Validation Loss:1.6053, Validation Accuracy:0.2463\n",
    "Epoch #198: Loss:1.5789, Accuracy:0.2690, Validation Loss:1.6051, Validation Accuracy:0.2299\n",
    "Epoch #199: Loss:1.5773, Accuracy:0.2678, Validation Loss:1.6100, Validation Accuracy:0.2332\n",
    "Epoch #200: Loss:1.5802, Accuracy:0.2637, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #201: Loss:1.5800, Accuracy:0.2694, Validation Loss:1.6028, Validation Accuracy:0.2529\n",
    "Epoch #202: Loss:1.5790, Accuracy:0.2710, Validation Loss:1.6006, Validation Accuracy:0.2562\n",
    "Epoch #203: Loss:1.5765, Accuracy:0.2821, Validation Loss:1.6034, Validation Accuracy:0.2496\n",
    "Epoch #204: Loss:1.5768, Accuracy:0.2809, Validation Loss:1.6035, Validation Accuracy:0.2381\n",
    "Epoch #205: Loss:1.5768, Accuracy:0.2719, Validation Loss:1.6070, Validation Accuracy:0.2397\n",
    "Epoch #206: Loss:1.5769, Accuracy:0.2739, Validation Loss:1.6060, Validation Accuracy:0.2266\n",
    "Epoch #207: Loss:1.5769, Accuracy:0.2731, Validation Loss:1.6003, Validation Accuracy:0.2644\n",
    "Epoch #208: Loss:1.5777, Accuracy:0.2735, Validation Loss:1.6028, Validation Accuracy:0.2562\n",
    "Epoch #209: Loss:1.5761, Accuracy:0.2690, Validation Loss:1.6018, Validation Accuracy:0.2594\n",
    "Epoch #210: Loss:1.5766, Accuracy:0.2583, Validation Loss:1.6012, Validation Accuracy:0.2447\n",
    "Epoch #211: Loss:1.5787, Accuracy:0.2669, Validation Loss:1.6011, Validation Accuracy:0.2644\n",
    "Epoch #212: Loss:1.5797, Accuracy:0.2674, Validation Loss:1.6047, Validation Accuracy:0.2381\n",
    "Epoch #213: Loss:1.5794, Accuracy:0.2674, Validation Loss:1.6058, Validation Accuracy:0.2479\n",
    "Epoch #214: Loss:1.5795, Accuracy:0.2542, Validation Loss:1.6019, Validation Accuracy:0.2545\n",
    "Epoch #215: Loss:1.5765, Accuracy:0.2752, Validation Loss:1.6102, Validation Accuracy:0.2315\n",
    "Epoch #216: Loss:1.5745, Accuracy:0.2698, Validation Loss:1.6103, Validation Accuracy:0.2414\n",
    "Epoch #217: Loss:1.5740, Accuracy:0.2698, Validation Loss:1.6125, Validation Accuracy:0.2397\n",
    "Epoch #218: Loss:1.5748, Accuracy:0.2682, Validation Loss:1.6117, Validation Accuracy:0.2282\n",
    "Epoch #219: Loss:1.5811, Accuracy:0.2587, Validation Loss:1.6176, Validation Accuracy:0.2348\n",
    "Epoch #220: Loss:1.5770, Accuracy:0.2641, Validation Loss:1.6071, Validation Accuracy:0.2217\n",
    "Epoch #221: Loss:1.5846, Accuracy:0.2546, Validation Loss:1.6017, Validation Accuracy:0.2430\n",
    "Epoch #222: Loss:1.5822, Accuracy:0.2657, Validation Loss:1.6059, Validation Accuracy:0.2496\n",
    "Epoch #223: Loss:1.5730, Accuracy:0.2682, Validation Loss:1.6042, Validation Accuracy:0.2233\n",
    "Epoch #224: Loss:1.5749, Accuracy:0.2678, Validation Loss:1.6054, Validation Accuracy:0.2496\n",
    "Epoch #225: Loss:1.5756, Accuracy:0.2698, Validation Loss:1.6079, Validation Accuracy:0.2266\n",
    "Epoch #226: Loss:1.5698, Accuracy:0.2772, Validation Loss:1.6116, Validation Accuracy:0.2332\n",
    "Epoch #227: Loss:1.5696, Accuracy:0.2821, Validation Loss:1.6100, Validation Accuracy:0.2381\n",
    "Epoch #228: Loss:1.5699, Accuracy:0.2678, Validation Loss:1.6078, Validation Accuracy:0.2414\n",
    "Epoch #229: Loss:1.5669, Accuracy:0.2719, Validation Loss:1.6072, Validation Accuracy:0.2299\n",
    "Epoch #230: Loss:1.5654, Accuracy:0.2772, Validation Loss:1.6083, Validation Accuracy:0.2496\n",
    "Epoch #231: Loss:1.5657, Accuracy:0.2891, Validation Loss:1.6086, Validation Accuracy:0.2447\n",
    "Epoch #232: Loss:1.5664, Accuracy:0.2735, Validation Loss:1.6097, Validation Accuracy:0.2479\n",
    "Epoch #233: Loss:1.5690, Accuracy:0.2854, Validation Loss:1.6113, Validation Accuracy:0.2365\n",
    "Epoch #234: Loss:1.5679, Accuracy:0.2653, Validation Loss:1.6166, Validation Accuracy:0.2266\n",
    "Epoch #235: Loss:1.5683, Accuracy:0.2735, Validation Loss:1.6139, Validation Accuracy:0.2381\n",
    "Epoch #236: Loss:1.5680, Accuracy:0.2760, Validation Loss:1.6102, Validation Accuracy:0.2479\n",
    "Epoch #237: Loss:1.5676, Accuracy:0.2686, Validation Loss:1.6167, Validation Accuracy:0.2135\n",
    "Epoch #238: Loss:1.5667, Accuracy:0.2682, Validation Loss:1.6172, Validation Accuracy:0.2479\n",
    "Epoch #239: Loss:1.5670, Accuracy:0.2764, Validation Loss:1.6154, Validation Accuracy:0.2365\n",
    "Epoch #240: Loss:1.5687, Accuracy:0.2719, Validation Loss:1.6134, Validation Accuracy:0.2463\n",
    "Epoch #241: Loss:1.5692, Accuracy:0.2743, Validation Loss:1.6125, Validation Accuracy:0.2397\n",
    "Epoch #242: Loss:1.5680, Accuracy:0.2776, Validation Loss:1.6099, Validation Accuracy:0.2397\n",
    "Epoch #243: Loss:1.5625, Accuracy:0.2797, Validation Loss:1.6122, Validation Accuracy:0.2562\n",
    "Epoch #244: Loss:1.5616, Accuracy:0.2784, Validation Loss:1.6154, Validation Accuracy:0.2365\n",
    "Epoch #245: Loss:1.5609, Accuracy:0.2805, Validation Loss:1.6207, Validation Accuracy:0.2447\n",
    "Epoch #246: Loss:1.5582, Accuracy:0.2813, Validation Loss:1.6187, Validation Accuracy:0.2348\n",
    "Epoch #247: Loss:1.5589, Accuracy:0.2887, Validation Loss:1.6244, Validation Accuracy:0.2282\n",
    "Epoch #248: Loss:1.5571, Accuracy:0.2780, Validation Loss:1.6208, Validation Accuracy:0.2266\n",
    "Epoch #249: Loss:1.5570, Accuracy:0.2883, Validation Loss:1.6178, Validation Accuracy:0.2348\n",
    "Epoch #250: Loss:1.5580, Accuracy:0.2813, Validation Loss:1.6194, Validation Accuracy:0.2381\n",
    "Epoch #251: Loss:1.5590, Accuracy:0.2756, Validation Loss:1.6150, Validation Accuracy:0.2562\n",
    "Epoch #252: Loss:1.5574, Accuracy:0.2776, Validation Loss:1.6234, Validation Accuracy:0.2562\n",
    "Epoch #253: Loss:1.5585, Accuracy:0.2825, Validation Loss:1.6178, Validation Accuracy:0.2414\n",
    "Epoch #254: Loss:1.5533, Accuracy:0.2858, Validation Loss:1.6190, Validation Accuracy:0.2529\n",
    "Epoch #255: Loss:1.5582, Accuracy:0.2715, Validation Loss:1.6324, Validation Accuracy:0.2332\n",
    "Epoch #256: Loss:1.5619, Accuracy:0.2858, Validation Loss:1.6144, Validation Accuracy:0.2594\n",
    "Epoch #257: Loss:1.5604, Accuracy:0.2747, Validation Loss:1.6257, Validation Accuracy:0.2282\n",
    "Epoch #258: Loss:1.5557, Accuracy:0.2838, Validation Loss:1.6239, Validation Accuracy:0.2397\n",
    "Epoch #259: Loss:1.5576, Accuracy:0.2830, Validation Loss:1.6223, Validation Accuracy:0.2463\n",
    "Epoch #260: Loss:1.5584, Accuracy:0.2862, Validation Loss:1.6247, Validation Accuracy:0.2299\n",
    "Epoch #261: Loss:1.5544, Accuracy:0.2821, Validation Loss:1.6222, Validation Accuracy:0.2397\n",
    "Epoch #262: Loss:1.5545, Accuracy:0.2858, Validation Loss:1.6239, Validation Accuracy:0.2315\n",
    "Epoch #263: Loss:1.5551, Accuracy:0.2928, Validation Loss:1.6139, Validation Accuracy:0.2479\n",
    "Epoch #264: Loss:1.5538, Accuracy:0.2875, Validation Loss:1.6220, Validation Accuracy:0.2299\n",
    "Epoch #265: Loss:1.5547, Accuracy:0.2854, Validation Loss:1.6133, Validation Accuracy:0.2414\n",
    "Epoch #266: Loss:1.5583, Accuracy:0.2797, Validation Loss:1.6208, Validation Accuracy:0.2479\n",
    "Epoch #267: Loss:1.5621, Accuracy:0.2883, Validation Loss:1.6142, Validation Accuracy:0.2430\n",
    "Epoch #268: Loss:1.5619, Accuracy:0.2789, Validation Loss:1.6121, Validation Accuracy:0.2463\n",
    "Epoch #269: Loss:1.5536, Accuracy:0.2887, Validation Loss:1.6115, Validation Accuracy:0.2233\n",
    "Epoch #270: Loss:1.5517, Accuracy:0.2920, Validation Loss:1.6131, Validation Accuracy:0.2447\n",
    "Epoch #271: Loss:1.5551, Accuracy:0.2912, Validation Loss:1.6206, Validation Accuracy:0.2299\n",
    "Epoch #272: Loss:1.5542, Accuracy:0.2924, Validation Loss:1.6179, Validation Accuracy:0.2332\n",
    "Epoch #273: Loss:1.5598, Accuracy:0.2867, Validation Loss:1.6281, Validation Accuracy:0.2348\n",
    "Epoch #274: Loss:1.5637, Accuracy:0.2756, Validation Loss:1.6208, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5677, Accuracy:0.2760, Validation Loss:1.6183, Validation Accuracy:0.2282\n",
    "Epoch #276: Loss:1.5665, Accuracy:0.2752, Validation Loss:1.6136, Validation Accuracy:0.2381\n",
    "Epoch #277: Loss:1.5687, Accuracy:0.2567, Validation Loss:1.6097, Validation Accuracy:0.2414\n",
    "Epoch #278: Loss:1.5645, Accuracy:0.2678, Validation Loss:1.6135, Validation Accuracy:0.2397\n",
    "Epoch #279: Loss:1.5615, Accuracy:0.2760, Validation Loss:1.6152, Validation Accuracy:0.2414\n",
    "Epoch #280: Loss:1.5574, Accuracy:0.2793, Validation Loss:1.6186, Validation Accuracy:0.2365\n",
    "Epoch #281: Loss:1.5590, Accuracy:0.2776, Validation Loss:1.6174, Validation Accuracy:0.2365\n",
    "Epoch #282: Loss:1.5589, Accuracy:0.2756, Validation Loss:1.6125, Validation Accuracy:0.2332\n",
    "Epoch #283: Loss:1.5596, Accuracy:0.2723, Validation Loss:1.6167, Validation Accuracy:0.2315\n",
    "Epoch #284: Loss:1.5599, Accuracy:0.2760, Validation Loss:1.6261, Validation Accuracy:0.2184\n",
    "Epoch #285: Loss:1.5585, Accuracy:0.2768, Validation Loss:1.6239, Validation Accuracy:0.2299\n",
    "Epoch #286: Loss:1.5648, Accuracy:0.2731, Validation Loss:1.6212, Validation Accuracy:0.2414\n",
    "Epoch #287: Loss:1.5682, Accuracy:0.2727, Validation Loss:1.6217, Validation Accuracy:0.2348\n",
    "Epoch #288: Loss:1.5723, Accuracy:0.2645, Validation Loss:1.6171, Validation Accuracy:0.2266\n",
    "Epoch #289: Loss:1.5726, Accuracy:0.2595, Validation Loss:1.6154, Validation Accuracy:0.2430\n",
    "Epoch #290: Loss:1.5704, Accuracy:0.2575, Validation Loss:1.6168, Validation Accuracy:0.2299\n",
    "Epoch #291: Loss:1.5821, Accuracy:0.2579, Validation Loss:1.6062, Validation Accuracy:0.2545\n",
    "Epoch #292: Loss:1.5736, Accuracy:0.2571, Validation Loss:1.6259, Validation Accuracy:0.2118\n",
    "Epoch #293: Loss:1.6019, Accuracy:0.2460, Validation Loss:1.6168, Validation Accuracy:0.2200\n",
    "Epoch #294: Loss:1.5903, Accuracy:0.2538, Validation Loss:1.6051, Validation Accuracy:0.2414\n",
    "Epoch #295: Loss:1.5874, Accuracy:0.2579, Validation Loss:1.6027, Validation Accuracy:0.2447\n",
    "Epoch #296: Loss:1.5858, Accuracy:0.2485, Validation Loss:1.6084, Validation Accuracy:0.2545\n",
    "Epoch #297: Loss:1.5830, Accuracy:0.2526, Validation Loss:1.6119, Validation Accuracy:0.2381\n",
    "Epoch #298: Loss:1.5835, Accuracy:0.2538, Validation Loss:1.6145, Validation Accuracy:0.2512\n",
    "Epoch #299: Loss:1.5813, Accuracy:0.2567, Validation Loss:1.6097, Validation Accuracy:0.2447\n",
    "Epoch #300: Loss:1.5775, Accuracy:0.2608, Validation Loss:1.6108, Validation Accuracy:0.2282\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61080205, Accuracy:0.2282\n",
    "Labels: ['02', '04', '05', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  04   05  03  01\n",
    "t:02   4   9   87   0  14\n",
    "t:04   8  15   67   2  20\n",
    "t:05   3   5  101   2  31\n",
    "t:03   3   5   88   2  17\n",
    "t:01   2   8   99   0  17\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.20      0.04      0.06       114\n",
    "          04       0.36      0.13      0.19       112\n",
    "          05       0.23      0.71      0.35       142\n",
    "          03       0.33      0.02      0.03       115\n",
    "          01       0.17      0.13      0.15       126\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.26      0.21      0.16       609\n",
    "weighted avg       0.25      0.23      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 13:30:20 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 40 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6078275694635702, 1.6059115299058861, 1.6050994353145605, 1.6047606832288168, 1.6049787166279132, 1.6051399966178856, 1.6050361473180585, 1.60493759177197, 1.605008709606866, 1.604737248913995, 1.604577046309786, 1.6042101455635234, 1.6043033274915222, 1.604864453838768, 1.6044110815317565, 1.6040774419389923, 1.6039578701279238, 1.603174626925113, 1.6050029157222003, 1.6042038069178515, 1.6039114728545516, 1.602845666639519, 1.6028677113537717, 1.6024609111212744, 1.6026688829822884, 1.6025725098079062, 1.6015517494361389, 1.6004712920275033, 1.6002421966327236, 1.599646158797792, 1.5991189935915968, 1.5994088224981022, 1.5992949719499485, 1.600410633094988, 1.6011841913749432, 1.6014487520227292, 1.6022595445119296, 1.600177135960809, 1.5997425257083036, 1.602938496616282, 1.599662218970814, 1.5976417916161674, 1.5988379104188315, 1.5986661834669817, 1.5975708859698918, 1.5982423707573676, 1.5960445844481144, 1.5957614034658973, 1.602330265961257, 1.6102600604638286, 1.600947954971802, 1.6000286223265925, 1.6000509254255122, 1.5995060869038398, 1.5994730416581353, 1.598508591526639, 1.5985362210688725, 1.598029848194279, 1.5973075947346553, 1.596837117166942, 1.5966958549418082, 1.5960020026549917, 1.597046419708991, 1.5980155021686273, 1.5994906407858938, 1.5968907529301635, 1.5977177561210294, 1.5974238836902312, 1.5979079080528422, 1.5982189908599227, 1.5977710741885582, 1.5981759351658311, 1.5949387869419918, 1.596978280148874, 1.5982274061744828, 1.5955054045506494, 1.5939689921628077, 1.5948733938934376, 1.5948679286662386, 1.5979988111063765, 1.5998703619137968, 1.5969737922812526, 1.595988739887482, 1.5970610491747927, 1.5965896386818346, 1.5955008514996232, 1.6008259978004669, 1.5981657054819693, 1.5958828677489056, 1.5981648085739812, 1.5972415075709276, 1.5940797321119136, 1.5963114417832474, 1.5963308662420814, 1.597481262311951, 1.5965078147174103, 1.5954596659624323, 1.5965238840904925, 1.5965947985453364, 1.596731843815257, 1.5976317281206254, 1.5976519322356175, 1.5980315778055803, 1.5997486611696692, 1.6005690737898126, 1.6004022788531675, 1.5970093949479227, 1.5989479605591748, 1.5971715283902799, 1.5993173997390446, 1.5980415757262256, 1.5953977068852516, 1.5962474465565923, 1.5985283982773328, 1.5942978113155646, 1.594752101866874, 1.596698410209568, 1.5977046313544212, 1.5981799769284102, 1.5994944930663837, 1.597224313050068, 1.5980691925449715, 1.5974039453982525, 1.5976902313028847, 1.599402573895572, 1.6017333952272663, 1.6036084031040836, 1.6013017299727266, 1.6011414843044063, 1.5988314858406831, 1.5978874561234648, 1.5969097676927038, 1.5975767575657034, 1.596651676448891, 1.5960901400138592, 1.5974683374019678, 1.59925806444071, 1.5973201549699154, 1.5996544727159447, 1.6007117148494876, 1.6060998739280137, 1.600932196834796, 1.5992465733698829, 1.5955786689357414, 1.5948565912559898, 1.5934197143185118, 1.5931654015589622, 1.5954175173551186, 1.5925542803233481, 1.5921087910976317, 1.5917581349170853, 1.5912976946149553, 1.5925967014090376, 1.5931946510947592, 1.593672883921656, 1.591867276796175, 1.5925797819112517, 1.5905495797863538, 1.5915842823598576, 1.5928455027453419, 1.5918062208908532, 1.5915835261932147, 1.594718235075376, 1.5956958330715036, 1.5965468243425116, 1.5991168607436181, 1.6004446279043438, 1.6031414511168531, 1.604405929498093, 1.5998244338434906, 1.5966298311997713, 1.6004691993074465, 1.5988310551995715, 1.5957812654169519, 1.599432812535704, 1.5994415821504515, 1.5993290348788984, 1.6009895770224836, 1.602013046518335, 1.6026161501951797, 1.6051987740402347, 1.600355800345222, 1.6010290871700044, 1.604490374305174, 1.60138574296422, 1.6021308388028825, 1.601116613801477, 1.6031090541817676, 1.6062418447535223, 1.6104261697023765, 1.602151734488351, 1.6014905874365069, 1.597974497500703, 1.6022053545918957, 1.5987890999892662, 1.6052750762068775, 1.6053102987348935, 1.6050967594672894, 1.610043124612329, 1.6071380883797832, 1.602774411978197, 1.6006163251028076, 1.6033902806405755, 1.6035082029004402, 1.607004522690045, 1.6059645412394958, 1.6003335870936979, 1.602811950376664, 1.6017508743627513, 1.6012060076536607, 1.6010807582310267, 1.6046757882060285, 1.605813580389289, 1.601908901250617, 1.6102220499261064, 1.6102980451630842, 1.6125262860202634, 1.6116623980266902, 1.6175815307448063, 1.6071371701354855, 1.6016973830600483, 1.6059174870426822, 1.6041519829792341, 1.6054285379074673, 1.6078615719070184, 1.6116063318816312, 1.6099653523935278, 1.6078313900332146, 1.607194228908307, 1.6083497290540798, 1.6086112390010816, 1.6097329737517634, 1.6112756535337476, 1.6166130357187958, 1.613901148288708, 1.6101639237505658, 1.616747122875771, 1.6171859162194389, 1.6154116452816867, 1.6134160394934793, 1.6125364773379172, 1.6099466191136778, 1.6122297768913858, 1.615353143861141, 1.6207332573892252, 1.6187037143409742, 1.624426699820019, 1.6207642713791044, 1.6178374366807233, 1.6193547959398167, 1.6149755528407732, 1.623351601935764, 1.6177775507490035, 1.6189837442047295, 1.6323898629406206, 1.6143504757012053, 1.6256950798097307, 1.6238774689547535, 1.6223466441353358, 1.6247479001485263, 1.62223212022108, 1.6238826333204122, 1.6138723748070853, 1.6219534067489048, 1.613262931505839, 1.6207561363727587, 1.614178819805139, 1.6120508999268606, 1.611510235687782, 1.6130971336991133, 1.6205673268667387, 1.617891486838142, 1.6281071008719834, 1.6207669716946205, 1.6182790376086933, 1.613581807938311, 1.6097159043125722, 1.613549340730426, 1.6152166909184948, 1.6186159635803774, 1.6173692820303154, 1.612540139744826, 1.616673356793784, 1.6260777660974337, 1.62394130034204, 1.6212452591346402, 1.6217025913824197, 1.617059739743939, 1.615388541777537, 1.6167730850539184, 1.6062446728911501, 1.6259061329078988, 1.6167840491766217, 1.6051122819261598, 1.6027115268268803, 1.6083881197304561, 1.6118679265866334, 1.6144706496268462, 1.6096750975438134, 1.610802001945295], 'val_acc': [0.24302134446322624, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.22988505725105016, 0.2315270933758449, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.2315270933758449, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23481116572330737, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.2315270934737179, 0.22988505695743122, 0.22824302093050947, 0.23316912959851263, 0.23316912959851263, 0.2397372741955646, 0.24302134644515408, 0.22331691275187118, 0.24466338256994882, 0.25287356319392257, 0.23809523797289686, 0.2512315270691278, 0.2610837415423495, 0.2545155970431705, 0.24302134615153514, 0.2446633821784569, 0.24466338256994882, 0.23316912959851263, 0.24630541869474357, 0.24466338237420288, 0.2397372740976916, 0.2545155993187173, 0.23316912959851263, 0.2315270934737179, 0.23316912959851263, 0.23316912959851263, 0.2397372719200178, 0.22988505725105016, 0.23481116552756143, 0.23645320184810212, 0.23481116572330737, 0.24137930804481256, 0.23481116354563358, 0.2298850566638123, 0.2512315266776359, 0.2577996713725608, 0.25287356091837576, 0.251231524891454, 0.2479474548195383, 0.24958949094433308, 0.25451559912297134, 0.25287356101624875, 0.25944170759522855, 0.24794745472166535, 0.2545155992208443, 0.2495894908464601, 0.25287356319392257, 0.251231524891454, 0.24466338256994882, 0.2512315270691278, 0.25615763316796525, 0.25615763544351206, 0.25615763326583824, 0.25615763316796525, 0.23809523589309606, 0.24466338256994882, 0.25287356101624875, 0.23645319967042833, 0.25615763326583824, 0.2463054164191968, 0.24466338256994882, 0.24466338247207586, 0.24466338256994882, 0.24302134644515408, 0.24466338247207586, 0.2397372741955646, 0.24302134436535328, 0.23809523807076985, 0.2545155971410435, 0.2479474527397375, 0.24630541651706978, 0.251231524989327, 0.24466338039227503, 0.2528735613098677, 0.2463054164191968, 0.2298850553669953, 0.24466338029440204, 0.26108374193384143, 0.2364532019459751, 0.23973727231150974, 0.25779966948850597, 0.2397372720178908, 0.26108374173809545, 0.24630541671281572, 0.2512315250872, 0.2528735612119947, 0.25451559753253544, 0.25779966978212493, 0.2594417058090467, 0.24302134456109922, 0.2643678142813039, 0.24958948876665926, 0.23481116374137953, 0.25451559743466246, 0.23481116383925252, 0.2479474527397375, 0.23481116374137953, 0.23152709158966303, 0.2528735612119947, 0.23809523589309606, 0.2528735612119947, 0.2594417057111737, 0.24466338039227503, 0.2512315250872, 0.22167487493876753, 0.251231524891454, 0.2561576335594572, 0.24466338068589397, 0.23809523638246094, 0.257799669390633, 0.22988505517124935, 0.23809523589309606, 0.2397372720178908, 0.2561576333637112, 0.24466338039227503, 0.2413793082405585, 0.24958948876665926, 0.23152709357159088, 0.2594417058090467, 0.24958948876665926, 0.26108374173809545, 0.2676518864330204, 0.26108374173809545, 0.2643678141834309, 0.2545155970431705, 0.25615763326583824, 0.2545155971410435, 0.24137930814268554, 0.2528735613098677, 0.24958948886453225, 0.24958948896240524, 0.2495894890602782, 0.23645320184810212, 0.2282430212241284, 0.24137930814268554, 0.23973727231150974, 0.23809523589309606, 0.25451559753253544, 0.2413793083384315, 0.23645320184810212, 0.24466338039227503, 0.24466338058802098, 0.24302134446322624, 0.2430213442674803, 0.23645319976830131, 0.22988505734892314, 0.23481116552756143, 0.23809523807076985, 0.2282430212241284, 0.2479474527397375, 0.24466338049014802, 0.23481116572330737, 0.24958948886453225, 0.2298850553669953, 0.24466338058802098, 0.22988505744679613, 0.2315270933758449, 0.23645319986617427, 0.2430213442674803, 0.22495894907241187, 0.23481116364350654, 0.22003284069802764, 0.23973727399981865, 0.2364532019459751, 0.24630541661494276, 0.22988505734892314, 0.23316912742083884, 0.2282430213220014, 0.2528735613098677, 0.2561576338530761, 0.2495894895496431, 0.23809523807076985, 0.2397372741955646, 0.22660098341102475, 0.26436781408555793, 0.2561576334615842, 0.2594417056133007, 0.24466338058802098, 0.2643678145749228, 0.238095236088842, 0.24794745303335644, 0.25451559733678947, 0.2315270912960441, 0.24137930872992341, 0.23973727211576376, 0.22824301934007354, 0.23481116364350654, 0.22167487474302158, 0.24302134644515408, 0.24958948876665926, 0.22331691086781633, 0.24958948886453225, 0.22660098500146067, 0.2331691275187118, 0.23809523579522307, 0.24137930804481256, 0.2298850552691223, 0.24958948876665926, 0.24466338256994882, 0.24794745472166535, 0.2364532019459751, 0.22660098490358768, 0.23809523797289686, 0.24794745264186452, 0.21346469580735675, 0.24794745264186452, 0.2364532019459751, 0.24630541869474357, 0.23973727211576376, 0.2397372741955646, 0.25615763544351206, 0.23645319976830131, 0.24466338256994882, 0.23481116354563358, 0.2282430213220014, 0.22660098519720664, 0.23481116354563358, 0.238095236186715, 0.2561576337552032, 0.2561576339509491, 0.24137930804481256, 0.25287356101624875, 0.23316912969638562, 0.2594417058090467, 0.22824301904645458, 0.2397372740976916, 0.2463054169085617, 0.22988505725105016, 0.23973727240938272, 0.2315270912960441, 0.2479474532291024, 0.22988505734892314, 0.2413793083384315, 0.2479474548195383, 0.24302134436535328, 0.2463054185968706, 0.22331691275187118, 0.2446633821784569, 0.22988505734892314, 0.23316912910914772, 0.23481116572330737, 0.23481116552756143, 0.22824302102838243, 0.2380952378750239, 0.2413793101246134, 0.2397372741955646, 0.24137931022248635, 0.23645320184810212, 0.23645320165235617, 0.23316912940276668, 0.2315270933758449, 0.21839080408386802, 0.22988505725105016, 0.24137931032035934, 0.2348111651360695, 0.22660098500146067, 0.24302134624940813, 0.22988505725105016, 0.2545155993187173, 0.21182266007405393, 0.22003284011078977, 0.24137931022248635, 0.24466338049014802, 0.2545155970431705, 0.23809523797289686, 0.251231524793581, 0.24466338247207586, 0.22824302102838243], 'loss': [1.6128783958403727, 1.6084154731683908, 1.6070716302742458, 1.6058969675882642, 1.6053344901827082, 1.6052648717617841, 1.6052002404504733, 1.6049936792444155, 1.604883298149344, 1.6046968215300073, 1.604518274506994, 1.6044497212834916, 1.6044455076879545, 1.6042285921637283, 1.6040018211399996, 1.6035236868770215, 1.6028185663282015, 1.602844555666804, 1.6040671383330953, 1.6044317054063142, 1.6042545117391942, 1.6035463050650376, 1.6031383335223188, 1.6028307328233973, 1.6030851462048916, 1.6032710743146266, 1.60340368032945, 1.6032778327714736, 1.6024715911191592, 1.6021230403892313, 1.6020452363779902, 1.6017217085346793, 1.6014607380058241, 1.601264012716634, 1.6027927020247223, 1.6021044810939373, 1.6027918323109527, 1.6022772558905505, 1.601431988201102, 1.6019748065995485, 1.6020639323111188, 1.6012659279472774, 1.600768740759738, 1.6003391212506461, 1.599991328515556, 1.600223616995606, 1.6005156352534675, 1.5995976678155042, 1.5989065778818463, 1.6066722073349375, 1.603319142290699, 1.6024311728546017, 1.6034206250854586, 1.601270112217819, 1.6004194308110575, 1.6006590075561398, 1.601509980401464, 1.601332991030182, 1.6004410509700893, 1.5996739255329422, 1.5993317647146739, 1.5980933795964203, 1.5993926808574606, 1.6006949294029564, 1.599733419878527, 1.5996539599596842, 1.5998968500865802, 1.598533754231259, 1.598468774740701, 1.5976905572585747, 1.59937948793846, 1.6002276951282666, 1.5997433981121933, 1.6000714786243635, 1.5994375207340938, 1.5979539834987946, 1.5975048960601526, 1.5971470420120677, 1.5986308359267531, 1.6021610454856983, 1.5997442706654448, 1.6014744188751284, 1.6000389083454987, 1.6001366760696474, 1.5990174267571076, 1.5978344915828666, 1.5988794148090684, 1.600537478213927, 1.5989780361402697, 1.599393842597272, 1.5983767146692138, 1.5965948602746889, 1.5964794691583215, 1.5978420332471939, 1.5967397007364512, 1.5971140083591062, 1.5975854567678558, 1.5971852590416002, 1.5975532243383983, 1.5954976631630617, 1.5949459845035718, 1.5951538755174046, 1.5949307468147982, 1.5948646501838795, 1.594803095157631, 1.5945981063881938, 1.5933325476714963, 1.596432886916754, 1.5971102912323185, 1.5976849193690494, 1.596580898737271, 1.5982506521428634, 1.5961654651825923, 1.5950832825177015, 1.5968010326185755, 1.5950099905413524, 1.595768575697709, 1.5947208890679925, 1.594396719159042, 1.5945402510601883, 1.5951054362056192, 1.593770824810318, 1.5930153249959926, 1.592786769210925, 1.594372729207456, 1.5916365230352727, 1.59234291253883, 1.5926424750067616, 1.5959514234100278, 1.5959070639443838, 1.5959527593373763, 1.5956681020940353, 1.5939617124671075, 1.5928151448894086, 1.5934278288416304, 1.5922706236829502, 1.5930784429612834, 1.5927950722970512, 1.592545841068213, 1.59119175645605, 1.6002410269860614, 1.5997160543406523, 1.5938466728100786, 1.5927837411481007, 1.5940796065379461, 1.5950309942390395, 1.594615678278083, 1.5962528134763119, 1.593389726370512, 1.595253421246883, 1.5939465416040755, 1.5930833327452016, 1.5924244185492733, 1.592750787294376, 1.5925797363571073, 1.5927630613471937, 1.5923220334356571, 1.591663767278072, 1.5903218456362307, 1.5908156157519049, 1.5899188135194093, 1.588681372382068, 1.5890122974677743, 1.590032921385716, 1.5884704795950981, 1.5876785376723053, 1.5880034943625667, 1.5865955239203922, 1.5873118849505634, 1.5849022384541724, 1.5860421427955862, 1.5866955649926187, 1.587970489642948, 1.5856514707972627, 1.5861011347976308, 1.5846448832713602, 1.584570467349685, 1.586294139924725, 1.5849663966490257, 1.583771020137309, 1.582761535017887, 1.5832020178957398, 1.5816558572056356, 1.5839090868677692, 1.581949862513454, 1.582219751414822, 1.5843478707562237, 1.584088920714674, 1.5857616188589798, 1.5883006485580664, 1.5904376061300478, 1.5840277057408796, 1.5827382335917417, 1.5826916986422372, 1.5819127482800024, 1.5804975926753677, 1.5810052554465417, 1.5788666627735084, 1.5772565718793772, 1.5801802224447106, 1.5800190407637453, 1.5790312063522651, 1.5764682807961528, 1.5768093303978077, 1.576759811348494, 1.5769142745701439, 1.5769318625177935, 1.577709214349547, 1.5761358240546632, 1.5766036062025193, 1.578725946169859, 1.5797242401072131, 1.5793621915077036, 1.579541339179084, 1.5764666541645904, 1.5744786625280518, 1.5739883624552702, 1.5747621530869658, 1.581110959718849, 1.5770435321991938, 1.584553971623493, 1.582155272113714, 1.5729913374236966, 1.5749448394873304, 1.575598775532701, 1.5698431602004128, 1.569567027522798, 1.5698665459787577, 1.5668822323761926, 1.5653660111848333, 1.5656984764447692, 1.5663998385474422, 1.5690381518134835, 1.5678777420055694, 1.5682726214798568, 1.5680239282349542, 1.5676030325938544, 1.5666715156371098, 1.5670231735926634, 1.5686993132871279, 1.5692115592760716, 1.567990131446713, 1.5624659683670106, 1.5615908090094033, 1.5609106122590677, 1.5581995342791204, 1.5588706569260395, 1.5570577878482044, 1.5570183852859592, 1.5579826976729125, 1.5589882128537313, 1.557402465132962, 1.5584940816342707, 1.5532792714586983, 1.558237382175986, 1.5619109579180301, 1.5604254855267565, 1.555652055945974, 1.5576068336713975, 1.5583532443036778, 1.554391144482262, 1.5545389057429664, 1.5550615630845024, 1.553776332338243, 1.5547189869675058, 1.558306190218524, 1.5621107366295566, 1.5619211528825074, 1.553584004182835, 1.55166708306855, 1.5551459024574232, 1.5542104217306054, 1.5597785655967509, 1.5637199757280291, 1.5677441402627212, 1.566485307250914, 1.5687467883499742, 1.564468971904543, 1.561465685127697, 1.5574140374420604, 1.5590162526900273, 1.5589369089941225, 1.5595571313305803, 1.5599312688780516, 1.5585035311367967, 1.5648326452263082, 1.5682423689526943, 1.572297813613312, 1.5726499414541883, 1.5703500264479149, 1.5820865438214562, 1.5735844764376568, 1.601920115580549, 1.5903191893007722, 1.5873773242903442, 1.5858416653266922, 1.5829690273292745, 1.5834720763337686, 1.58128039557831, 1.577516705397463], 'acc': [0.18603696206879078, 0.22628336721377207, 0.23244353289232117, 0.23285421029620593, 0.2328542091028891, 0.23244353209065705, 0.23244353287396244, 0.23244353128899295, 0.23285421051039099, 0.23285420933543288, 0.23285420969036816, 0.2328542091028891, 0.23285420990455322, 0.23285420931707418, 0.2336755641432024, 0.23367556572817189, 0.23285420931707418, 0.23367556435738746, 0.23367556514069285, 0.2320328542767609, 0.2349075972115969, 0.23572895381852096, 0.23326488732679668, 0.2328542085337688, 0.23326488652513258, 0.23121149864896856, 0.2324435322864834, 0.23244353248230976, 0.23244353287396244, 0.23244353246395102, 0.2320328530834441, 0.23326488673931764, 0.23326488691678526, 0.23901437454889443, 0.23449692016264742, 0.23367556553234556, 0.2336755641615611, 0.2353182756129721, 0.23696098608525137, 0.23778234286964306, 0.2381930189211021, 0.24229979570763802, 0.24147843870906124, 0.2377823387205723, 0.23901437300064235, 0.24312114957299322, 0.24558521585053242, 0.24558521405137784, 0.24229979492433262, 0.2340862425445776, 0.2328542106694999, 0.24147843986566062, 0.23531827482966672, 0.2501026706284321, 0.23901437396141537, 0.23449691937934203, 0.23531827463384036, 0.23326488713097035, 0.23531827463384036, 0.2402464056039493, 0.23860369556004016, 0.24024640644233083, 0.24065708466623842, 0.23490759640993278, 0.2353182744196553, 0.24722792749776978, 0.24476386002691375, 0.24763860272920596, 0.24394250480913285, 0.24229979471014756, 0.24188911650459868, 0.24312114896715545, 0.24147843927818158, 0.23613963084911174, 0.24188911826703582, 0.2447638606143928, 0.24188911630877236, 0.24394250480913285, 0.24476386082857787, 0.24024640565902544, 0.24147843986566062, 0.24681724772561012, 0.246406570499193, 0.24640657108667205, 0.25092402488544, 0.2439425071406903, 0.25010267025513816, 0.24312115033793988, 0.2422997953159853, 0.24722792555786502, 0.24024640799058292, 0.24722792749776978, 0.2484599597828589, 0.2422997941226685, 0.24887063816587537, 0.24312114935880813, 0.24394250400746872, 0.2451745384466477, 0.24312115094377765, 0.24599589307694955, 0.25215605754382314, 0.2501026696493004, 0.2521560583271285, 0.2529774118191897, 0.2542094464174776, 0.2537987695827132, 0.25297741098080817, 0.2566735117342438, 0.24722792573533264, 0.24681724890056822, 0.24887063718674365, 0.24353182736853066, 0.2439425071406903, 0.24845995878536845, 0.24763860452836053, 0.24969199320618865, 0.2554414787025667, 0.2546201224689366, 0.24845996015615288, 0.25708418954813994, 0.25010266984512675, 0.25297741156828724, 0.25544147809672896, 0.25297741238831006, 0.24640657030336666, 0.2533880905938589, 0.25462012227311026, 0.24804928040235194, 0.24353182658522526, 0.24845995800206302, 0.25174537972992694, 0.2464065708908457, 0.25379876703697063, 0.254620123252242, 0.2509240228904591, 0.25420944624000996, 0.25503080263274897, 0.24722792828107518, 0.25585215430729685, 0.2558521559289838, 0.2386036971633684, 0.24394250500495918, 0.24763860374505514, 0.25379876879940777, 0.2509240249037987, 0.24188911712879518, 0.2521560567605177, 0.2505133474815553, 0.24476386041856643, 0.24271047389482817, 0.25667351251754916, 0.2611909667079698, 0.260780285332482, 0.25954825408160076, 0.256262831533714, 0.26406571062193757, 0.2620123211791628, 0.26078028654415747, 0.264476384874242, 0.2620123215524568, 0.26160164354273424, 0.2689938402763382, 0.2652977421053626, 0.26119096555137045, 0.2661190971640346, 0.2661190955790651, 0.24928131304237633, 0.2587269009995509, 0.2550308000870064, 0.2513347001168762, 0.26242299819139486, 0.26735112741742534, 0.2673511283965571, 0.2669404509743136, 0.2644763849109595, 0.2673511309790171, 0.26283367737607544, 0.2599589338537604, 0.26242299701643673, 0.2632443534458932, 0.2665297729645911, 0.2657084207025642, 0.2710472265675327, 0.2640657078987274, 0.26776180523132154, 0.2665297739804403, 0.2611909639296835, 0.2587268986496348, 0.26365503065395157, 0.25872689822126466, 0.2537987686219401, 0.2636550318289097, 0.26119096514135903, 0.2624229972122631, 0.26652977394372285, 0.2628336766111288, 0.26324435538579793, 0.2689938382813573, 0.2677618066388234, 0.26365502924644973, 0.2694045176985817, 0.27104722934581904, 0.2821355228428968, 0.28090349038034007, 0.2718685837986533, 0.27392197149734965, 0.27310061446205547, 0.2735112927043218, 0.2689938389055538, 0.2583162206031948, 0.26694045038683456, 0.2673511299998853, 0.2673511294124063, 0.25420944622165126, 0.2751540049390382, 0.2698151964916096, 0.2698151947291725, 0.2681724868026357, 0.2587268970463065, 0.2640657090553268, 0.2546201220772839, 0.2657084191359534, 0.26817248523602494, 0.26776180683464973, 0.26981519351749694, 0.27720739263773453, 0.2821355242503987, 0.2677618089887396, 0.27186858337028313, 0.2772073906427536, 0.28911704156677825, 0.27351129208012526, 0.28542094597826256, 0.2652977409304045, 0.27351129290014814, 0.27597535997935146, 0.26858316304992114, 0.2681724838285231, 0.276386035406614, 0.27186858082454063, 0.2743326467287859, 0.27761806845664977, 0.27967145576369346, 0.27843942568777036, 0.28049281358229305, 0.28131416940591175, 0.28870636751030015, 0.27802874627054597, 0.28829568871727224, 0.28131416604014636, 0.27556467977882165, 0.27761806806499706, 0.2825462000693139, 0.28583162437963777, 0.2714579043814289, 0.2858316208180461, 0.2747433279084474, 0.283778235310157, 0.28295687690407834, 0.28624229745698415, 0.28213552542535675, 0.2858316227763096, 0.2928131421243875, 0.28747433406861167, 0.2854209455866099, 0.2796714599127642, 0.2882956859389859, 0.2788501020941646, 0.2887063643403611, 0.2919917864598778, 0.29117043004878, 0.2924024625113368, 0.28665297922412475, 0.2755646805621271, 0.2759753613501359, 0.27515400415573277, 0.25667351036345937, 0.26776180800960786, 0.27597535997935146, 0.2792607824905208, 0.27761806806499706, 0.27556468314458704, 0.27227926161254945, 0.27597535919604604, 0.2767967165862755, 0.2731006168486891, 0.272689937039812, 0.26447638804418105, 0.2595482542957858, 0.25749486540377264, 0.2579055447842796, 0.25708418839154057, 0.24599589348696096, 0.25379876823028746, 0.2579055447842796, 0.24845995819788938, 0.25256673437858757, 0.25379876641277416, 0.25667351234008157, 0.26078028811076825]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
