{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf25.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 03:08:59 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '3Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ce', 'ib', 'my', 'sk', 'mb', 'aa', 'eg', 'by', 'ck', 'ds', 'yd', 'sg', 'eb', 'ek', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001C08075D278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001C0C3017EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7122, Accuracy:0.0924, Validation Loss:2.7037, Validation Accuracy:0.1117\n",
    "Epoch #2: Loss:2.6980, Accuracy:0.1055, Validation Loss:2.6921, Validation Accuracy:0.0788\n",
    "Epoch #3: Loss:2.6886, Accuracy:0.0899, Validation Loss:2.6842, Validation Accuracy:0.0936\n",
    "Epoch #4: Loss:2.6817, Accuracy:0.0871, Validation Loss:2.6778, Validation Accuracy:0.0887\n",
    "Epoch #5: Loss:2.6751, Accuracy:0.0891, Validation Loss:2.6698, Validation Accuracy:0.1067\n",
    "Epoch #6: Loss:2.6669, Accuracy:0.1121, Validation Loss:2.6601, Validation Accuracy:0.1248\n",
    "Epoch #7: Loss:2.6593, Accuracy:0.1166, Validation Loss:2.6531, Validation Accuracy:0.1363\n",
    "Epoch #8: Loss:2.6503, Accuracy:0.1211, Validation Loss:2.6428, Validation Accuracy:0.1346\n",
    "Epoch #9: Loss:2.6413, Accuracy:0.1203, Validation Loss:2.6320, Validation Accuracy:0.1297\n",
    "Epoch #10: Loss:2.6314, Accuracy:0.1166, Validation Loss:2.6240, Validation Accuracy:0.1149\n",
    "Epoch #11: Loss:2.6200, Accuracy:0.1285, Validation Loss:2.6072, Validation Accuracy:0.1511\n",
    "Epoch #12: Loss:2.6058, Accuracy:0.1585, Validation Loss:2.5894, Validation Accuracy:0.1642\n",
    "Epoch #13: Loss:2.5890, Accuracy:0.1643, Validation Loss:2.5797, Validation Accuracy:0.1626\n",
    "Epoch #14: Loss:2.5786, Accuracy:0.1593, Validation Loss:2.5622, Validation Accuracy:0.1642\n",
    "Epoch #15: Loss:2.5640, Accuracy:0.1602, Validation Loss:2.5477, Validation Accuracy:0.1658\n",
    "Epoch #16: Loss:2.5531, Accuracy:0.1659, Validation Loss:2.5465, Validation Accuracy:0.1560\n",
    "Epoch #17: Loss:2.5429, Accuracy:0.1598, Validation Loss:2.5335, Validation Accuracy:0.1560\n",
    "Epoch #18: Loss:2.5372, Accuracy:0.1589, Validation Loss:2.5302, Validation Accuracy:0.1626\n",
    "Epoch #19: Loss:2.5289, Accuracy:0.1573, Validation Loss:2.5207, Validation Accuracy:0.1658\n",
    "Epoch #20: Loss:2.5245, Accuracy:0.1593, Validation Loss:2.5290, Validation Accuracy:0.1511\n",
    "Epoch #21: Loss:2.5163, Accuracy:0.1598, Validation Loss:2.5264, Validation Accuracy:0.1544\n",
    "Epoch #22: Loss:2.5080, Accuracy:0.1606, Validation Loss:2.5231, Validation Accuracy:0.1626\n",
    "Epoch #23: Loss:2.5011, Accuracy:0.1593, Validation Loss:2.5077, Validation Accuracy:0.1642\n",
    "Epoch #24: Loss:2.4948, Accuracy:0.1614, Validation Loss:2.4980, Validation Accuracy:0.1609\n",
    "Epoch #25: Loss:2.4847, Accuracy:0.1643, Validation Loss:2.4945, Validation Accuracy:0.1708\n",
    "Epoch #26: Loss:2.4797, Accuracy:0.1713, Validation Loss:2.4950, Validation Accuracy:0.1741\n",
    "Epoch #27: Loss:2.4750, Accuracy:0.1680, Validation Loss:2.4873, Validation Accuracy:0.1790\n",
    "Epoch #28: Loss:2.4708, Accuracy:0.1749, Validation Loss:2.4848, Validation Accuracy:0.1856\n",
    "Epoch #29: Loss:2.4686, Accuracy:0.1725, Validation Loss:2.4802, Validation Accuracy:0.1773\n",
    "Epoch #30: Loss:2.4641, Accuracy:0.1737, Validation Loss:2.4779, Validation Accuracy:0.1790\n",
    "Epoch #31: Loss:2.4608, Accuracy:0.1749, Validation Loss:2.4782, Validation Accuracy:0.1757\n",
    "Epoch #32: Loss:2.4599, Accuracy:0.1754, Validation Loss:2.4791, Validation Accuracy:0.1724\n",
    "Epoch #33: Loss:2.4630, Accuracy:0.1741, Validation Loss:2.4825, Validation Accuracy:0.1724\n",
    "Epoch #34: Loss:2.4604, Accuracy:0.1733, Validation Loss:2.4746, Validation Accuracy:0.1806\n",
    "Epoch #35: Loss:2.4543, Accuracy:0.1766, Validation Loss:2.4728, Validation Accuracy:0.1773\n",
    "Epoch #36: Loss:2.4633, Accuracy:0.1770, Validation Loss:2.4810, Validation Accuracy:0.1757\n",
    "Epoch #37: Loss:2.4759, Accuracy:0.1704, Validation Loss:2.4757, Validation Accuracy:0.1872\n",
    "Epoch #38: Loss:2.4572, Accuracy:0.1758, Validation Loss:2.4769, Validation Accuracy:0.1806\n",
    "Epoch #39: Loss:2.4546, Accuracy:0.1741, Validation Loss:2.4757, Validation Accuracy:0.1741\n",
    "Epoch #40: Loss:2.4533, Accuracy:0.1782, Validation Loss:2.4734, Validation Accuracy:0.1708\n",
    "Epoch #41: Loss:2.4538, Accuracy:0.1692, Validation Loss:2.4733, Validation Accuracy:0.1790\n",
    "Epoch #42: Loss:2.4529, Accuracy:0.1749, Validation Loss:2.4751, Validation Accuracy:0.1823\n",
    "Epoch #43: Loss:2.4500, Accuracy:0.1799, Validation Loss:2.4739, Validation Accuracy:0.1708\n",
    "Epoch #44: Loss:2.4495, Accuracy:0.1782, Validation Loss:2.4728, Validation Accuracy:0.1757\n",
    "Epoch #45: Loss:2.4479, Accuracy:0.1778, Validation Loss:2.4719, Validation Accuracy:0.1773\n",
    "Epoch #46: Loss:2.4471, Accuracy:0.1782, Validation Loss:2.4698, Validation Accuracy:0.1741\n",
    "Epoch #47: Loss:2.4459, Accuracy:0.1799, Validation Loss:2.4671, Validation Accuracy:0.1790\n",
    "Epoch #48: Loss:2.4466, Accuracy:0.1836, Validation Loss:2.4642, Validation Accuracy:0.1724\n",
    "Epoch #49: Loss:2.4482, Accuracy:0.1729, Validation Loss:2.4609, Validation Accuracy:0.1741\n",
    "Epoch #50: Loss:2.4469, Accuracy:0.1782, Validation Loss:2.4630, Validation Accuracy:0.1806\n",
    "Epoch #51: Loss:2.4462, Accuracy:0.1680, Validation Loss:2.4637, Validation Accuracy:0.1658\n",
    "Epoch #52: Loss:2.4463, Accuracy:0.1704, Validation Loss:2.4664, Validation Accuracy:0.1675\n",
    "Epoch #53: Loss:2.4487, Accuracy:0.1721, Validation Loss:2.4684, Validation Accuracy:0.1691\n",
    "Epoch #54: Loss:2.4463, Accuracy:0.1782, Validation Loss:2.4647, Validation Accuracy:0.1691\n",
    "Epoch #55: Loss:2.4461, Accuracy:0.1799, Validation Loss:2.4684, Validation Accuracy:0.1724\n",
    "Epoch #56: Loss:2.4412, Accuracy:0.1799, Validation Loss:2.4644, Validation Accuracy:0.1691\n",
    "Epoch #57: Loss:2.4412, Accuracy:0.1762, Validation Loss:2.4670, Validation Accuracy:0.1708\n",
    "Epoch #58: Loss:2.4390, Accuracy:0.1807, Validation Loss:2.4690, Validation Accuracy:0.1708\n",
    "Epoch #59: Loss:2.4392, Accuracy:0.1782, Validation Loss:2.4674, Validation Accuracy:0.1576\n",
    "Epoch #60: Loss:2.4407, Accuracy:0.1811, Validation Loss:2.4687, Validation Accuracy:0.1642\n",
    "Epoch #61: Loss:2.4433, Accuracy:0.1770, Validation Loss:2.4674, Validation Accuracy:0.1790\n",
    "Epoch #62: Loss:2.4417, Accuracy:0.1766, Validation Loss:2.4671, Validation Accuracy:0.1658\n",
    "Epoch #63: Loss:2.4372, Accuracy:0.1799, Validation Loss:2.4644, Validation Accuracy:0.1724\n",
    "Epoch #64: Loss:2.4385, Accuracy:0.1762, Validation Loss:2.4673, Validation Accuracy:0.1757\n",
    "Epoch #65: Loss:2.4370, Accuracy:0.1795, Validation Loss:2.4628, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4368, Accuracy:0.1704, Validation Loss:2.4683, Validation Accuracy:0.1675\n",
    "Epoch #67: Loss:2.4388, Accuracy:0.1758, Validation Loss:2.4644, Validation Accuracy:0.1741\n",
    "Epoch #68: Loss:2.4365, Accuracy:0.1762, Validation Loss:2.4654, Validation Accuracy:0.1724\n",
    "Epoch #69: Loss:2.4383, Accuracy:0.1811, Validation Loss:2.4627, Validation Accuracy:0.1757\n",
    "Epoch #70: Loss:2.4374, Accuracy:0.1749, Validation Loss:2.4639, Validation Accuracy:0.1691\n",
    "Epoch #71: Loss:2.4379, Accuracy:0.1799, Validation Loss:2.4657, Validation Accuracy:0.1708\n",
    "Epoch #72: Loss:2.4380, Accuracy:0.1778, Validation Loss:2.4640, Validation Accuracy:0.1773\n",
    "Epoch #73: Loss:2.4372, Accuracy:0.1778, Validation Loss:2.4646, Validation Accuracy:0.1675\n",
    "Epoch #74: Loss:2.4354, Accuracy:0.1774, Validation Loss:2.4661, Validation Accuracy:0.1741\n",
    "Epoch #75: Loss:2.4366, Accuracy:0.1766, Validation Loss:2.4648, Validation Accuracy:0.1626\n",
    "Epoch #76: Loss:2.4361, Accuracy:0.1717, Validation Loss:2.4631, Validation Accuracy:0.1642\n",
    "Epoch #77: Loss:2.4351, Accuracy:0.1758, Validation Loss:2.4622, Validation Accuracy:0.1757\n",
    "Epoch #78: Loss:2.4351, Accuracy:0.1754, Validation Loss:2.4613, Validation Accuracy:0.1708\n",
    "Epoch #79: Loss:2.4343, Accuracy:0.1749, Validation Loss:2.4637, Validation Accuracy:0.1675\n",
    "Epoch #80: Loss:2.4354, Accuracy:0.1766, Validation Loss:2.4644, Validation Accuracy:0.1658\n",
    "Epoch #81: Loss:2.4351, Accuracy:0.1758, Validation Loss:2.4604, Validation Accuracy:0.1658\n",
    "Epoch #82: Loss:2.4338, Accuracy:0.1770, Validation Loss:2.4612, Validation Accuracy:0.1675\n",
    "Epoch #83: Loss:2.4337, Accuracy:0.1770, Validation Loss:2.4577, Validation Accuracy:0.1823\n",
    "Epoch #84: Loss:2.4336, Accuracy:0.1786, Validation Loss:2.4596, Validation Accuracy:0.1741\n",
    "Epoch #85: Loss:2.4341, Accuracy:0.1803, Validation Loss:2.4627, Validation Accuracy:0.1658\n",
    "Epoch #86: Loss:2.4342, Accuracy:0.1782, Validation Loss:2.4612, Validation Accuracy:0.1576\n",
    "Epoch #87: Loss:2.4366, Accuracy:0.1741, Validation Loss:2.4613, Validation Accuracy:0.1609\n",
    "Epoch #88: Loss:2.4351, Accuracy:0.1696, Validation Loss:2.4613, Validation Accuracy:0.1626\n",
    "Epoch #89: Loss:2.4343, Accuracy:0.1749, Validation Loss:2.4662, Validation Accuracy:0.1560\n",
    "Epoch #90: Loss:2.4328, Accuracy:0.1786, Validation Loss:2.4656, Validation Accuracy:0.1757\n",
    "Epoch #91: Loss:2.4341, Accuracy:0.1815, Validation Loss:2.4645, Validation Accuracy:0.1658\n",
    "Epoch #92: Loss:2.4367, Accuracy:0.1766, Validation Loss:2.4619, Validation Accuracy:0.1691\n",
    "Epoch #93: Loss:2.4369, Accuracy:0.1737, Validation Loss:2.4651, Validation Accuracy:0.1609\n",
    "Epoch #94: Loss:2.4367, Accuracy:0.1704, Validation Loss:2.4603, Validation Accuracy:0.1790\n",
    "Epoch #95: Loss:2.4372, Accuracy:0.1725, Validation Loss:2.4637, Validation Accuracy:0.1675\n",
    "Epoch #96: Loss:2.4390, Accuracy:0.1745, Validation Loss:2.4585, Validation Accuracy:0.1675\n",
    "Epoch #97: Loss:2.4384, Accuracy:0.1754, Validation Loss:2.4612, Validation Accuracy:0.1642\n",
    "Epoch #98: Loss:2.4371, Accuracy:0.1749, Validation Loss:2.4603, Validation Accuracy:0.1675\n",
    "Epoch #99: Loss:2.4368, Accuracy:0.1721, Validation Loss:2.4569, Validation Accuracy:0.1593\n",
    "Epoch #100: Loss:2.4367, Accuracy:0.1733, Validation Loss:2.4583, Validation Accuracy:0.1675\n",
    "Epoch #101: Loss:2.4370, Accuracy:0.1696, Validation Loss:2.4601, Validation Accuracy:0.1757\n",
    "Epoch #102: Loss:2.4357, Accuracy:0.1741, Validation Loss:2.4585, Validation Accuracy:0.1773\n",
    "Epoch #103: Loss:2.4354, Accuracy:0.1749, Validation Loss:2.4576, Validation Accuracy:0.1757\n",
    "Epoch #104: Loss:2.4357, Accuracy:0.1741, Validation Loss:2.4606, Validation Accuracy:0.1626\n",
    "Epoch #105: Loss:2.4363, Accuracy:0.1762, Validation Loss:2.4614, Validation Accuracy:0.1741\n",
    "Epoch #106: Loss:2.4360, Accuracy:0.1770, Validation Loss:2.4594, Validation Accuracy:0.1790\n",
    "Epoch #107: Loss:2.4364, Accuracy:0.1782, Validation Loss:2.4597, Validation Accuracy:0.1773\n",
    "Epoch #108: Loss:2.4348, Accuracy:0.1766, Validation Loss:2.4617, Validation Accuracy:0.1757\n",
    "Epoch #109: Loss:2.4358, Accuracy:0.1770, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #110: Loss:2.4353, Accuracy:0.1778, Validation Loss:2.4639, Validation Accuracy:0.1741\n",
    "Epoch #111: Loss:2.4354, Accuracy:0.1803, Validation Loss:2.4610, Validation Accuracy:0.1593\n",
    "Epoch #112: Loss:2.4340, Accuracy:0.1844, Validation Loss:2.4557, Validation Accuracy:0.1576\n",
    "Epoch #113: Loss:2.4336, Accuracy:0.1807, Validation Loss:2.4543, Validation Accuracy:0.1560\n",
    "Epoch #114: Loss:2.4320, Accuracy:0.1799, Validation Loss:2.4537, Validation Accuracy:0.1626\n",
    "Epoch #115: Loss:2.4342, Accuracy:0.1836, Validation Loss:2.4552, Validation Accuracy:0.1609\n",
    "Epoch #116: Loss:2.4334, Accuracy:0.1823, Validation Loss:2.4539, Validation Accuracy:0.1609\n",
    "Epoch #117: Loss:2.4318, Accuracy:0.1832, Validation Loss:2.4551, Validation Accuracy:0.1626\n",
    "Epoch #118: Loss:2.4322, Accuracy:0.1832, Validation Loss:2.4527, Validation Accuracy:0.1708\n",
    "Epoch #119: Loss:2.4326, Accuracy:0.1836, Validation Loss:2.4559, Validation Accuracy:0.1675\n",
    "Epoch #120: Loss:2.4323, Accuracy:0.1811, Validation Loss:2.4526, Validation Accuracy:0.1757\n",
    "Epoch #121: Loss:2.4339, Accuracy:0.1811, Validation Loss:2.4535, Validation Accuracy:0.1708\n",
    "Epoch #122: Loss:2.4487, Accuracy:0.1840, Validation Loss:2.4651, Validation Accuracy:0.1708\n",
    "Epoch #123: Loss:2.4365, Accuracy:0.1823, Validation Loss:2.4551, Validation Accuracy:0.1741\n",
    "Epoch #124: Loss:2.4373, Accuracy:0.1758, Validation Loss:2.4521, Validation Accuracy:0.1675\n",
    "Epoch #125: Loss:2.4322, Accuracy:0.1869, Validation Loss:2.4588, Validation Accuracy:0.1757\n",
    "Epoch #126: Loss:2.4331, Accuracy:0.1799, Validation Loss:2.4550, Validation Accuracy:0.1773\n",
    "Epoch #127: Loss:2.4319, Accuracy:0.1811, Validation Loss:2.4546, Validation Accuracy:0.1675\n",
    "Epoch #128: Loss:2.4305, Accuracy:0.1848, Validation Loss:2.4535, Validation Accuracy:0.1691\n",
    "Epoch #129: Loss:2.4306, Accuracy:0.1852, Validation Loss:2.4538, Validation Accuracy:0.1675\n",
    "Epoch #130: Loss:2.4308, Accuracy:0.1819, Validation Loss:2.4533, Validation Accuracy:0.1658\n",
    "Epoch #131: Loss:2.4299, Accuracy:0.1832, Validation Loss:2.4543, Validation Accuracy:0.1658\n",
    "Epoch #132: Loss:2.4313, Accuracy:0.1836, Validation Loss:2.4556, Validation Accuracy:0.1642\n",
    "Epoch #133: Loss:2.4304, Accuracy:0.1856, Validation Loss:2.4539, Validation Accuracy:0.1691\n",
    "Epoch #134: Loss:2.4310, Accuracy:0.1844, Validation Loss:2.4539, Validation Accuracy:0.1626\n",
    "Epoch #135: Loss:2.4313, Accuracy:0.1828, Validation Loss:2.4565, Validation Accuracy:0.1658\n",
    "Epoch #136: Loss:2.4304, Accuracy:0.1811, Validation Loss:2.4536, Validation Accuracy:0.1593\n",
    "Epoch #137: Loss:2.4311, Accuracy:0.1819, Validation Loss:2.4528, Validation Accuracy:0.1658\n",
    "Epoch #138: Loss:2.4308, Accuracy:0.1844, Validation Loss:2.4524, Validation Accuracy:0.1576\n",
    "Epoch #139: Loss:2.4320, Accuracy:0.1832, Validation Loss:2.4537, Validation Accuracy:0.1626\n",
    "Epoch #140: Loss:2.4312, Accuracy:0.1844, Validation Loss:2.4542, Validation Accuracy:0.1741\n",
    "Epoch #141: Loss:2.4307, Accuracy:0.1840, Validation Loss:2.4543, Validation Accuracy:0.1741\n",
    "Epoch #142: Loss:2.4305, Accuracy:0.1852, Validation Loss:2.4554, Validation Accuracy:0.1708\n",
    "Epoch #143: Loss:2.4299, Accuracy:0.1848, Validation Loss:2.4547, Validation Accuracy:0.1642\n",
    "Epoch #144: Loss:2.4285, Accuracy:0.1848, Validation Loss:2.4551, Validation Accuracy:0.1593\n",
    "Epoch #145: Loss:2.4295, Accuracy:0.1864, Validation Loss:2.4590, Validation Accuracy:0.1527\n",
    "Epoch #146: Loss:2.4310, Accuracy:0.1852, Validation Loss:2.4568, Validation Accuracy:0.1560\n",
    "Epoch #147: Loss:2.4345, Accuracy:0.1836, Validation Loss:2.4575, Validation Accuracy:0.1593\n",
    "Epoch #148: Loss:2.4330, Accuracy:0.1848, Validation Loss:2.4575, Validation Accuracy:0.1675\n",
    "Epoch #149: Loss:2.4321, Accuracy:0.1856, Validation Loss:2.4548, Validation Accuracy:0.1576\n",
    "Epoch #150: Loss:2.4320, Accuracy:0.1860, Validation Loss:2.4551, Validation Accuracy:0.1560\n",
    "Epoch #151: Loss:2.4311, Accuracy:0.1877, Validation Loss:2.4562, Validation Accuracy:0.1593\n",
    "Epoch #152: Loss:2.4319, Accuracy:0.1869, Validation Loss:2.4561, Validation Accuracy:0.1609\n",
    "Epoch #153: Loss:2.4319, Accuracy:0.1869, Validation Loss:2.4564, Validation Accuracy:0.1724\n",
    "Epoch #154: Loss:2.4319, Accuracy:0.1864, Validation Loss:2.4566, Validation Accuracy:0.1658\n",
    "Epoch #155: Loss:2.4332, Accuracy:0.1852, Validation Loss:2.4547, Validation Accuracy:0.1593\n",
    "Epoch #156: Loss:2.4315, Accuracy:0.1864, Validation Loss:2.4543, Validation Accuracy:0.1609\n",
    "Epoch #157: Loss:2.4321, Accuracy:0.1864, Validation Loss:2.4543, Validation Accuracy:0.1626\n",
    "Epoch #158: Loss:2.4313, Accuracy:0.1864, Validation Loss:2.4545, Validation Accuracy:0.1658\n",
    "Epoch #159: Loss:2.4313, Accuracy:0.1840, Validation Loss:2.4546, Validation Accuracy:0.1642\n",
    "Epoch #160: Loss:2.4315, Accuracy:0.1844, Validation Loss:2.4552, Validation Accuracy:0.1658\n",
    "Epoch #161: Loss:2.4312, Accuracy:0.1848, Validation Loss:2.4547, Validation Accuracy:0.1691\n",
    "Epoch #162: Loss:2.4351, Accuracy:0.1897, Validation Loss:2.4543, Validation Accuracy:0.1708\n",
    "Epoch #163: Loss:2.4312, Accuracy:0.1914, Validation Loss:2.4578, Validation Accuracy:0.1708\n",
    "Epoch #164: Loss:2.4315, Accuracy:0.1869, Validation Loss:2.4560, Validation Accuracy:0.1691\n",
    "Epoch #165: Loss:2.4315, Accuracy:0.1877, Validation Loss:2.4560, Validation Accuracy:0.1691\n",
    "Epoch #166: Loss:2.4350, Accuracy:0.1832, Validation Loss:2.4556, Validation Accuracy:0.1691\n",
    "Epoch #167: Loss:2.4324, Accuracy:0.1844, Validation Loss:2.4566, Validation Accuracy:0.1658\n",
    "Epoch #168: Loss:2.4312, Accuracy:0.1856, Validation Loss:2.4563, Validation Accuracy:0.1724\n",
    "Epoch #169: Loss:2.4303, Accuracy:0.1836, Validation Loss:2.4538, Validation Accuracy:0.1675\n",
    "Epoch #170: Loss:2.4318, Accuracy:0.1844, Validation Loss:2.4545, Validation Accuracy:0.1675\n",
    "Epoch #171: Loss:2.4303, Accuracy:0.1840, Validation Loss:2.4546, Validation Accuracy:0.1724\n",
    "Epoch #172: Loss:2.4276, Accuracy:0.1873, Validation Loss:2.4527, Validation Accuracy:0.1724\n",
    "Epoch #173: Loss:2.4265, Accuracy:0.1901, Validation Loss:2.4532, Validation Accuracy:0.1691\n",
    "Epoch #174: Loss:2.4254, Accuracy:0.1897, Validation Loss:2.4531, Validation Accuracy:0.1691\n",
    "Epoch #175: Loss:2.4260, Accuracy:0.1893, Validation Loss:2.4534, Validation Accuracy:0.1675\n",
    "Epoch #176: Loss:2.4277, Accuracy:0.1918, Validation Loss:2.4553, Validation Accuracy:0.1642\n",
    "Epoch #177: Loss:2.4289, Accuracy:0.1844, Validation Loss:2.4525, Validation Accuracy:0.1724\n",
    "Epoch #178: Loss:2.4295, Accuracy:0.1836, Validation Loss:2.4513, Validation Accuracy:0.1708\n",
    "Epoch #179: Loss:2.4287, Accuracy:0.1889, Validation Loss:2.4517, Validation Accuracy:0.1626\n",
    "Epoch #180: Loss:2.4297, Accuracy:0.1881, Validation Loss:2.4492, Validation Accuracy:0.1773\n",
    "Epoch #181: Loss:2.4315, Accuracy:0.1786, Validation Loss:2.4518, Validation Accuracy:0.1626\n",
    "Epoch #182: Loss:2.4384, Accuracy:0.1832, Validation Loss:2.4548, Validation Accuracy:0.1691\n",
    "Epoch #183: Loss:2.4296, Accuracy:0.1828, Validation Loss:2.4578, Validation Accuracy:0.1560\n",
    "Epoch #184: Loss:2.4259, Accuracy:0.1869, Validation Loss:2.4538, Validation Accuracy:0.1642\n",
    "Epoch #185: Loss:2.4254, Accuracy:0.1856, Validation Loss:2.4566, Validation Accuracy:0.1544\n",
    "Epoch #186: Loss:2.4260, Accuracy:0.1860, Validation Loss:2.4600, Validation Accuracy:0.1691\n",
    "Epoch #187: Loss:2.4262, Accuracy:0.1832, Validation Loss:2.4591, Validation Accuracy:0.1544\n",
    "Epoch #188: Loss:2.4252, Accuracy:0.1836, Validation Loss:2.4570, Validation Accuracy:0.1691\n",
    "Epoch #189: Loss:2.4235, Accuracy:0.1881, Validation Loss:2.4576, Validation Accuracy:0.1691\n",
    "Epoch #190: Loss:2.4249, Accuracy:0.1844, Validation Loss:2.4571, Validation Accuracy:0.1626\n",
    "Epoch #191: Loss:2.4241, Accuracy:0.1869, Validation Loss:2.4565, Validation Accuracy:0.1609\n",
    "Epoch #192: Loss:2.4247, Accuracy:0.1832, Validation Loss:2.4548, Validation Accuracy:0.1675\n",
    "Epoch #193: Loss:2.4236, Accuracy:0.1819, Validation Loss:2.4577, Validation Accuracy:0.1626\n",
    "Epoch #194: Loss:2.4240, Accuracy:0.1840, Validation Loss:2.4613, Validation Accuracy:0.1593\n",
    "Epoch #195: Loss:2.4252, Accuracy:0.1901, Validation Loss:2.4581, Validation Accuracy:0.1609\n",
    "Epoch #196: Loss:2.4282, Accuracy:0.1889, Validation Loss:2.4577, Validation Accuracy:0.1708\n",
    "Epoch #197: Loss:2.4305, Accuracy:0.1873, Validation Loss:2.4526, Validation Accuracy:0.1757\n",
    "Epoch #198: Loss:2.4257, Accuracy:0.1840, Validation Loss:2.4518, Validation Accuracy:0.1642\n",
    "Epoch #199: Loss:2.4265, Accuracy:0.1893, Validation Loss:2.4534, Validation Accuracy:0.1658\n",
    "Epoch #200: Loss:2.4257, Accuracy:0.1873, Validation Loss:2.4540, Validation Accuracy:0.1626\n",
    "Epoch #201: Loss:2.4238, Accuracy:0.1918, Validation Loss:2.4555, Validation Accuracy:0.1642\n",
    "Epoch #202: Loss:2.4239, Accuracy:0.1885, Validation Loss:2.4557, Validation Accuracy:0.1675\n",
    "Epoch #203: Loss:2.4236, Accuracy:0.1877, Validation Loss:2.4543, Validation Accuracy:0.1658\n",
    "Epoch #204: Loss:2.4243, Accuracy:0.1881, Validation Loss:2.4505, Validation Accuracy:0.1691\n",
    "Epoch #205: Loss:2.4224, Accuracy:0.1893, Validation Loss:2.4491, Validation Accuracy:0.1658\n",
    "Epoch #206: Loss:2.4239, Accuracy:0.1906, Validation Loss:2.4529, Validation Accuracy:0.1691\n",
    "Epoch #207: Loss:2.4238, Accuracy:0.1901, Validation Loss:2.4519, Validation Accuracy:0.1691\n",
    "Epoch #208: Loss:2.4246, Accuracy:0.1881, Validation Loss:2.4556, Validation Accuracy:0.1642\n",
    "Epoch #209: Loss:2.4252, Accuracy:0.1869, Validation Loss:2.4585, Validation Accuracy:0.1675\n",
    "Epoch #210: Loss:2.4247, Accuracy:0.1873, Validation Loss:2.4576, Validation Accuracy:0.1626\n",
    "Epoch #211: Loss:2.4241, Accuracy:0.1881, Validation Loss:2.4557, Validation Accuracy:0.1642\n",
    "Epoch #212: Loss:2.4226, Accuracy:0.1897, Validation Loss:2.4543, Validation Accuracy:0.1593\n",
    "Epoch #213: Loss:2.4223, Accuracy:0.1930, Validation Loss:2.4539, Validation Accuracy:0.1609\n",
    "Epoch #214: Loss:2.4222, Accuracy:0.1918, Validation Loss:2.4514, Validation Accuracy:0.1593\n",
    "Epoch #215: Loss:2.4233, Accuracy:0.1893, Validation Loss:2.4508, Validation Accuracy:0.1658\n",
    "Epoch #216: Loss:2.4242, Accuracy:0.1873, Validation Loss:2.4519, Validation Accuracy:0.1658\n",
    "Epoch #217: Loss:2.4236, Accuracy:0.1848, Validation Loss:2.4536, Validation Accuracy:0.1708\n",
    "Epoch #218: Loss:2.4232, Accuracy:0.1864, Validation Loss:2.4526, Validation Accuracy:0.1658\n",
    "Epoch #219: Loss:2.4231, Accuracy:0.1869, Validation Loss:2.4544, Validation Accuracy:0.1658\n",
    "Epoch #220: Loss:2.4231, Accuracy:0.1893, Validation Loss:2.4540, Validation Accuracy:0.1675\n",
    "Epoch #221: Loss:2.4237, Accuracy:0.1897, Validation Loss:2.4536, Validation Accuracy:0.1658\n",
    "Epoch #222: Loss:2.4235, Accuracy:0.1873, Validation Loss:2.4524, Validation Accuracy:0.1658\n",
    "Epoch #223: Loss:2.4232, Accuracy:0.1901, Validation Loss:2.4529, Validation Accuracy:0.1675\n",
    "Epoch #224: Loss:2.4230, Accuracy:0.1885, Validation Loss:2.4510, Validation Accuracy:0.1691\n",
    "Epoch #225: Loss:2.4223, Accuracy:0.1856, Validation Loss:2.4543, Validation Accuracy:0.1626\n",
    "Epoch #226: Loss:2.4228, Accuracy:0.1885, Validation Loss:2.4555, Validation Accuracy:0.1642\n",
    "Epoch #227: Loss:2.4217, Accuracy:0.1860, Validation Loss:2.4537, Validation Accuracy:0.1642\n",
    "Epoch #228: Loss:2.4210, Accuracy:0.1869, Validation Loss:2.4529, Validation Accuracy:0.1626\n",
    "Epoch #229: Loss:2.4219, Accuracy:0.1943, Validation Loss:2.4506, Validation Accuracy:0.1576\n",
    "Epoch #230: Loss:2.4225, Accuracy:0.1897, Validation Loss:2.4522, Validation Accuracy:0.1576\n",
    "Epoch #231: Loss:2.4213, Accuracy:0.1906, Validation Loss:2.4525, Validation Accuracy:0.1593\n",
    "Epoch #232: Loss:2.4218, Accuracy:0.1889, Validation Loss:2.4554, Validation Accuracy:0.1593\n",
    "Epoch #233: Loss:2.4210, Accuracy:0.1893, Validation Loss:2.4548, Validation Accuracy:0.1593\n",
    "Epoch #234: Loss:2.4212, Accuracy:0.1914, Validation Loss:2.4544, Validation Accuracy:0.1593\n",
    "Epoch #235: Loss:2.4205, Accuracy:0.1877, Validation Loss:2.4541, Validation Accuracy:0.1609\n",
    "Epoch #236: Loss:2.4205, Accuracy:0.1897, Validation Loss:2.4544, Validation Accuracy:0.1593\n",
    "Epoch #237: Loss:2.4209, Accuracy:0.1906, Validation Loss:2.4533, Validation Accuracy:0.1626\n",
    "Epoch #238: Loss:2.4207, Accuracy:0.1889, Validation Loss:2.4544, Validation Accuracy:0.1560\n",
    "Epoch #239: Loss:2.4212, Accuracy:0.1918, Validation Loss:2.4547, Validation Accuracy:0.1544\n",
    "Epoch #240: Loss:2.4204, Accuracy:0.1918, Validation Loss:2.4550, Validation Accuracy:0.1560\n",
    "Epoch #241: Loss:2.4198, Accuracy:0.1869, Validation Loss:2.4554, Validation Accuracy:0.1609\n",
    "Epoch #242: Loss:2.4196, Accuracy:0.1881, Validation Loss:2.4550, Validation Accuracy:0.1576\n",
    "Epoch #243: Loss:2.4210, Accuracy:0.1918, Validation Loss:2.4554, Validation Accuracy:0.1576\n",
    "Epoch #244: Loss:2.4220, Accuracy:0.1873, Validation Loss:2.4512, Validation Accuracy:0.1708\n",
    "Epoch #245: Loss:2.4206, Accuracy:0.1934, Validation Loss:2.4545, Validation Accuracy:0.1593\n",
    "Epoch #246: Loss:2.4216, Accuracy:0.1906, Validation Loss:2.4565, Validation Accuracy:0.1544\n",
    "Epoch #247: Loss:2.4216, Accuracy:0.1901, Validation Loss:2.4529, Validation Accuracy:0.1609\n",
    "Epoch #248: Loss:2.4205, Accuracy:0.1877, Validation Loss:2.4542, Validation Accuracy:0.1576\n",
    "Epoch #249: Loss:2.4199, Accuracy:0.1930, Validation Loss:2.4532, Validation Accuracy:0.1642\n",
    "Epoch #250: Loss:2.4202, Accuracy:0.1881, Validation Loss:2.4559, Validation Accuracy:0.1642\n",
    "Epoch #251: Loss:2.4210, Accuracy:0.1922, Validation Loss:2.4561, Validation Accuracy:0.1724\n",
    "Epoch #252: Loss:2.4206, Accuracy:0.1906, Validation Loss:2.4546, Validation Accuracy:0.1724\n",
    "Epoch #253: Loss:2.4219, Accuracy:0.1901, Validation Loss:2.4530, Validation Accuracy:0.1675\n",
    "Epoch #254: Loss:2.4201, Accuracy:0.1901, Validation Loss:2.4549, Validation Accuracy:0.1691\n",
    "Epoch #255: Loss:2.4198, Accuracy:0.1885, Validation Loss:2.4564, Validation Accuracy:0.1708\n",
    "Epoch #256: Loss:2.4202, Accuracy:0.1885, Validation Loss:2.4588, Validation Accuracy:0.1708\n",
    "Epoch #257: Loss:2.4207, Accuracy:0.1922, Validation Loss:2.4540, Validation Accuracy:0.1658\n",
    "Epoch #258: Loss:2.4210, Accuracy:0.1906, Validation Loss:2.4509, Validation Accuracy:0.1691\n",
    "Epoch #259: Loss:2.4214, Accuracy:0.1910, Validation Loss:2.4490, Validation Accuracy:0.1691\n",
    "Epoch #260: Loss:2.4226, Accuracy:0.1918, Validation Loss:2.4495, Validation Accuracy:0.1642\n",
    "Epoch #261: Loss:2.4226, Accuracy:0.1885, Validation Loss:2.4508, Validation Accuracy:0.1609\n",
    "Epoch #262: Loss:2.4223, Accuracy:0.1869, Validation Loss:2.4523, Validation Accuracy:0.1675\n",
    "Epoch #263: Loss:2.4232, Accuracy:0.1873, Validation Loss:2.4537, Validation Accuracy:0.1741\n",
    "Epoch #264: Loss:2.4242, Accuracy:0.1836, Validation Loss:2.4502, Validation Accuracy:0.1757\n",
    "Epoch #265: Loss:2.4236, Accuracy:0.1836, Validation Loss:2.4514, Validation Accuracy:0.1790\n",
    "Epoch #266: Loss:2.4251, Accuracy:0.1864, Validation Loss:2.4505, Validation Accuracy:0.1658\n",
    "Epoch #267: Loss:2.4218, Accuracy:0.1844, Validation Loss:2.4486, Validation Accuracy:0.1790\n",
    "Epoch #268: Loss:2.4222, Accuracy:0.1844, Validation Loss:2.4461, Validation Accuracy:0.1757\n",
    "Epoch #269: Loss:2.4214, Accuracy:0.1877, Validation Loss:2.4488, Validation Accuracy:0.1872\n",
    "Epoch #270: Loss:2.4214, Accuracy:0.1873, Validation Loss:2.4454, Validation Accuracy:0.1790\n",
    "Epoch #271: Loss:2.4214, Accuracy:0.1856, Validation Loss:2.4455, Validation Accuracy:0.1724\n",
    "Epoch #272: Loss:2.4211, Accuracy:0.1828, Validation Loss:2.4475, Validation Accuracy:0.1658\n",
    "Epoch #273: Loss:2.4218, Accuracy:0.1860, Validation Loss:2.4478, Validation Accuracy:0.1823\n",
    "Epoch #274: Loss:2.4214, Accuracy:0.1815, Validation Loss:2.4478, Validation Accuracy:0.1806\n",
    "Epoch #275: Loss:2.4208, Accuracy:0.1889, Validation Loss:2.4460, Validation Accuracy:0.1626\n",
    "Epoch #276: Loss:2.4211, Accuracy:0.1864, Validation Loss:2.4475, Validation Accuracy:0.1708\n",
    "Epoch #277: Loss:2.4197, Accuracy:0.1889, Validation Loss:2.4496, Validation Accuracy:0.1773\n",
    "Epoch #278: Loss:2.4202, Accuracy:0.1852, Validation Loss:2.4486, Validation Accuracy:0.1773\n",
    "Epoch #279: Loss:2.4202, Accuracy:0.1910, Validation Loss:2.4469, Validation Accuracy:0.1609\n",
    "Epoch #280: Loss:2.4200, Accuracy:0.1914, Validation Loss:2.4498, Validation Accuracy:0.1642\n",
    "Epoch #281: Loss:2.4203, Accuracy:0.1893, Validation Loss:2.4506, Validation Accuracy:0.1724\n",
    "Epoch #282: Loss:2.4205, Accuracy:0.1852, Validation Loss:2.4491, Validation Accuracy:0.1544\n",
    "Epoch #283: Loss:2.4197, Accuracy:0.1910, Validation Loss:2.4506, Validation Accuracy:0.1609\n",
    "Epoch #284: Loss:2.4192, Accuracy:0.1967, Validation Loss:2.4543, Validation Accuracy:0.1544\n",
    "Epoch #285: Loss:2.4201, Accuracy:0.1906, Validation Loss:2.4577, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.4194, Accuracy:0.1910, Validation Loss:2.4582, Validation Accuracy:0.1560\n",
    "Epoch #287: Loss:2.4208, Accuracy:0.1897, Validation Loss:2.4558, Validation Accuracy:0.1511\n",
    "Epoch #288: Loss:2.4200, Accuracy:0.1922, Validation Loss:2.4579, Validation Accuracy:0.1609\n",
    "Epoch #289: Loss:2.4185, Accuracy:0.1881, Validation Loss:2.4550, Validation Accuracy:0.1593\n",
    "Epoch #290: Loss:2.4198, Accuracy:0.1930, Validation Loss:2.4597, Validation Accuracy:0.1527\n",
    "Epoch #291: Loss:2.4190, Accuracy:0.1885, Validation Loss:2.4560, Validation Accuracy:0.1593\n",
    "Epoch #292: Loss:2.4196, Accuracy:0.1906, Validation Loss:2.4566, Validation Accuracy:0.1626\n",
    "Epoch #293: Loss:2.4202, Accuracy:0.1885, Validation Loss:2.4575, Validation Accuracy:0.1658\n",
    "Epoch #294: Loss:2.4186, Accuracy:0.1889, Validation Loss:2.4554, Validation Accuracy:0.1544\n",
    "Epoch #295: Loss:2.4203, Accuracy:0.1910, Validation Loss:2.4592, Validation Accuracy:0.1593\n",
    "Epoch #296: Loss:2.4186, Accuracy:0.1901, Validation Loss:2.4565, Validation Accuracy:0.1642\n",
    "Epoch #297: Loss:2.4196, Accuracy:0.1885, Validation Loss:2.4580, Validation Accuracy:0.1560\n",
    "Epoch #298: Loss:2.4184, Accuracy:0.1967, Validation Loss:2.4570, Validation Accuracy:0.1544\n",
    "Epoch #299: Loss:2.4188, Accuracy:0.1906, Validation Loss:2.4570, Validation Accuracy:0.1593\n",
    "Epoch #300: Loss:2.4184, Accuracy:0.1910, Validation Loss:2.4555, Validation Accuracy:0.1560\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45554256, Accuracy:0.1560\n",
    "Labels: ['ce', 'ib', 'my', 'sk', 'mb', 'aa', 'eg', 'by', 'ck', 'ds', 'yd', 'sg', 'eb', 'ek', 'eo']\n",
    "Confusion Matrix:\n",
    "      ce  ib  my  sk  mb  aa  eg  by  ck  ds  yd  sg  eb  ek  eo\n",
    "t:ce   0   0   0   0   0   2   9   3   0   2   1   8   0   0   2\n",
    "t:ib   0   3   0   0   0   0   3   1   0   1  26  19   0   0   1\n",
    "t:my   0   0   0   0   1   1   6   2   0   3   3   1   0   0   3\n",
    "t:sk   0   0   0   0   2   1  15   5   0   3   1   4   0   0   2\n",
    "t:mb   0   2   0   0   0   1   7   6   0   1   6  27   0   0   2\n",
    "t:aa   0   0   0   0   2   4  12   0   0   5   4   6   0   0   1\n",
    "t:eg   0   0   0   0   2   2  19  10   0  11   0   2   0   0   4\n",
    "t:by   0   1   0   0   4   2   9   5   0   1   0  13   0   0   5\n",
    "t:ck   0   0   0   0   2   1   8   1   0   1   0   6   1   0   3\n",
    "t:ds   0   0   0   0   2   1   8   2   0   7   1   6   0   0   4\n",
    "t:yd   0   4   0   0   0   0   4   1   0   0  28  21   0   0   4\n",
    "t:sg   0   2   0   0   3   0   3   2   0   1   7  23   1   0   9\n",
    "t:eb   0   2   0   0   4   1  17   7   0   1   3   8   2   0   5\n",
    "t:ek   0   2   0   0   3   2  15   3   0   1   3  10   0   0   9\n",
    "t:eo   0   0   0   0   2   0   6   5   0   0   1  16   0   0   4\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ib       0.19      0.06      0.09        54\n",
    "          my       0.00      0.00      0.00        20\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          aa       0.22      0.12      0.15        34\n",
    "          eg       0.13      0.38      0.20        50\n",
    "          by       0.09      0.12      0.11        40\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ds       0.18      0.23      0.20        31\n",
    "          yd       0.33      0.45      0.38        62\n",
    "          sg       0.14      0.45      0.21        51\n",
    "          eb       0.50      0.04      0.07        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          eo       0.07      0.12      0.09        34\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.12      0.13      0.10       609\n",
    "weighted avg       0.15      0.16      0.12       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 03:49:54 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 54 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7036556211011162, 2.692091367906342, 2.6842276678100987, 2.6778437296549478, 2.669772094889423, 2.660095824005177, 2.6530738064808213, 2.642773567162124, 2.631983508030182, 2.6240276628722894, 2.607168035554181, 2.5893975983699553, 2.5796636615089206, 2.5622482750020397, 2.5476721836428338, 2.546546326873729, 2.533537380018062, 2.5301523881984265, 2.5206807117744034, 2.5290368421520113, 2.5264177228429636, 2.523130968873724, 2.507681717817811, 2.4979501557467607, 2.4944984521380396, 2.494965036905849, 2.487333901801524, 2.484829419156405, 2.4802369468513574, 2.4779245540230534, 2.4782497479606342, 2.479110842267868, 2.482457712953314, 2.474637273888674, 2.4727726687351472, 2.481026073590484, 2.475706086370158, 2.4769200018082542, 2.475724947276374, 2.4733698426796296, 2.4733161432989714, 2.475081849763742, 2.4738832530129717, 2.4728170725316643, 2.471873319794979, 2.4697541920422332, 2.4670504807251428, 2.4642076175201115, 2.4608796132217683, 2.4629557289317714, 2.4636578626429113, 2.466400653857903, 2.468423288248247, 2.464674517830409, 2.4684328610086674, 2.464393070374412, 2.4669945275255025, 2.468996788676345, 2.467414276940482, 2.468679491130785, 2.467402409254428, 2.467115351719222, 2.464449790115231, 2.4673327471822355, 2.462770641730924, 2.4683119311121295, 2.4644212804991623, 2.4654383706341823, 2.46265643022722, 2.463877328315196, 2.4657157078165137, 2.464008576764262, 2.4645779731825654, 2.466087549973787, 2.4648133091542914, 2.4630533662335625, 2.462218639690105, 2.4613192594305833, 2.463692384400391, 2.464416670681808, 2.4604211957583875, 2.4612075499517383, 2.4577212208401784, 2.4596033295974356, 2.462654948430304, 2.4611744179905735, 2.4613027028458068, 2.4612573304982805, 2.466181509600484, 2.4655966617790934, 2.464539506752503, 2.461876015357783, 2.4651148550224615, 2.460267878518316, 2.4636799161657326, 2.4584930726068555, 2.4611984195771868, 2.460314041288028, 2.4568738087840463, 2.4583266129830394, 2.4600593068916807, 2.4584705763066736, 2.4575983393564207, 2.4605617499703842, 2.461408189169096, 2.459403511338633, 2.4597229460385828, 2.4617418921835514, 2.4615013047392145, 2.463912937637229, 2.460995926645589, 2.455718184926827, 2.4542654069577923, 2.4536752552038736, 2.455215237606531, 2.4539063383988755, 2.455097106485727, 2.4527396157457324, 2.4559259066245045, 2.452561524896982, 2.453544314467457, 2.4651281066324517, 2.4550910395354473, 2.4521379909296144, 2.4587680537908145, 2.455030405266923, 2.4545612797165544, 2.4535104692080143, 2.4538026971770037, 2.4532715424724008, 2.4543036302713728, 2.4555630225853378, 2.453852705571843, 2.453949584162294, 2.4564713907163522, 2.453600365540077, 2.452781061037812, 2.4524453026907787, 2.453725966718201, 2.454245224374856, 2.4543019315879335, 2.455445737087081, 2.4547163872491744, 2.455148092827382, 2.4590224362359256, 2.456762208531447, 2.457524371656095, 2.457509998421755, 2.454781955490363, 2.455137284126971, 2.4562047381314933, 2.4560885335424265, 2.4563808245416148, 2.4565743033717613, 2.454722133567572, 2.45430329515429, 2.454334908126806, 2.4544846788415766, 2.454637525508361, 2.4552241867203235, 2.4546537876912136, 2.454306618529196, 2.4577696429097595, 2.4559652730944905, 2.455957180173526, 2.4555816055322905, 2.4566105238126807, 2.456342748820488, 2.453761971447072, 2.4545131316913174, 2.4545793333664316, 2.452721679934923, 2.4531979157615376, 2.453143299115311, 2.453362919035412, 2.4552734133057994, 2.45254360278839, 2.4513276715583987, 2.451662901782833, 2.4492314312062633, 2.4518477247266346, 2.4547598213202066, 2.457772756836489, 2.4538493125113754, 2.4566127780231546, 2.4600416932787215, 2.459066799708775, 2.4569683067121333, 2.4576321697391705, 2.4570850711346455, 2.4564650642069297, 2.454820257885311, 2.4576952809770707, 2.461273544919119, 2.458115582395657, 2.4577020768853046, 2.4526203316812247, 2.4517884982630536, 2.4534232581190287, 2.4539906446178166, 2.4554884112722966, 2.455670820276921, 2.4542518073114854, 2.450514201851705, 2.4490835153802077, 2.452855552945818, 2.4518803720208027, 2.4556005427794307, 2.458531915652145, 2.457569236238602, 2.455685761566037, 2.4543421323272003, 2.4538760251795324, 2.4514252912430536, 2.450818278323645, 2.4519347083588148, 2.4535792699979835, 2.4526007903620526, 2.454437859148423, 2.4539992496102117, 2.4535598844925954, 2.4523695376510495, 2.452944790005488, 2.4509907673145164, 2.454277208873204, 2.4554730471719073, 2.453652169708352, 2.4529086063648093, 2.4506244776871404, 2.452155303485288, 2.4524545473809702, 2.455386030654406, 2.45482606566794, 2.4544199033715257, 2.4541357726299116, 2.4543932239801816, 2.4532674137986157, 2.4544014312167866, 2.4547052226826085, 2.455043560178409, 2.4554360500110195, 2.455035520108854, 2.455409825533286, 2.4511714564951377, 2.4545485883315012, 2.456520032021408, 2.452895720799764, 2.454153924544261, 2.453236669546669, 2.4558826479418525, 2.4561154286458184, 2.454621633285372, 2.4530397201406546, 2.4549472316341054, 2.4564099891236655, 2.4587944423036623, 2.4540191953405373, 2.450918489684808, 2.4489554395816597, 2.4495240447947935, 2.4507709258099886, 2.452303164893966, 2.4537364403015287, 2.450218121210734, 2.451375956214316, 2.4505430461933657, 2.4485779110042527, 2.4460722973389775, 2.448757830120268, 2.4453538818703886, 2.4454646854369315, 2.4475000761999874, 2.4477953041715574, 2.4477786191774316, 2.4459850001217696, 2.4474946732200036, 2.4495768347397227, 2.4486271779133966, 2.446855881726996, 2.4498368088639233, 2.450582515625727, 2.4490881753085283, 2.4505584345662537, 2.454260725888908, 2.457682593898429, 2.4582479184092754, 2.4557655640619336, 2.4579212563769963, 2.4550062098918093, 2.4597210891924077, 2.4559884752546037, 2.456557970328871, 2.457533622218666, 2.4554178519006236, 2.45919917369711, 2.456534084623866, 2.458007613230613, 2.4570385752053094, 2.4569529633608163, 2.4555428055511124], 'val_acc': [0.11165845577646359, 0.07881773398403072, 0.09359605910718344, 0.0886699507327992, 0.1067323478058054, 0.12479474508067462, 0.13628899785636486, 0.13464696173157012, 0.12972085335718586, 0.11494252823403316, 0.15106732346888246, 0.16420361176989545, 0.16256157544935473, 0.1642036118677684, 0.1658456477968172, 0.15599343095017576, 0.15599343095017576, 0.1625615756451007, 0.1658456477968172, 0.15106732257579153, 0.15435139502112696, 0.16256157544935473, 0.16420361167202246, 0.16091953932456, 0.17077175617120144, 0.1740558285186639, 0.17898193679517518, 0.1855500814901001, 0.17733990067038044, 0.1789819366973022, 0.17569786434983972, 0.17241379200237725, 0.17241379200237725, 0.18062397272422395, 0.17733990037676148, 0.17569786425196673, 0.18719211722340295, 0.18062397272422395, 0.174055828127172, 0.1707717558775825, 0.1789819366973022, 0.1822660088490187, 0.1707717558775825, 0.17569786534080364, 0.1773399014655984, 0.17405582822504498, 0.1789819365994292, 0.17241379200237725, 0.174055828127172, 0.180623972626351, 0.16584564750319827, 0.16748768362799302, 0.16912971985066075, 0.16912971975278776, 0.17241379210025023, 0.16912971975278776, 0.1707717558775825, 0.1707717559754555, 0.15763546787018845, 0.16420361246724044, 0.17898193650155622, 0.16584564750319827, 0.17241379210025023, 0.17569786425196673, 0.1707717569664194, 0.16748768362799302, 0.174055828127172, 0.17241379200237725, 0.17569786434983972, 0.16912971985066075, 0.1707717558775825, 0.17733990047463447, 0.167487683725866, 0.17405582822504498, 0.1625615752536088, 0.16420361157414948, 0.17569786434983972, 0.17077175617120144, 0.16748768392161195, 0.16584564760107126, 0.16584564750319827, 0.16748768382373896, 0.1822660088490187, 0.174055828127172, 0.16584564760107126, 0.15763546687922456, 0.16091953912881404, 0.1625615752536088, 0.15599343075442978, 0.17569786434983972, 0.16584564760107126, 0.16912971985066075, 0.160919539226687, 0.1789819366973022, 0.16748768362799302, 0.16748768362799302, 0.16420361137840353, 0.16748768362799302, 0.1592775030040193, 0.167487683725866, 0.1756978644477127, 0.17733990047463447, 0.1756978644477127, 0.1625615752536088, 0.174055828127172, 0.17898193650155622, 0.17733990047463447, 0.17569786434983972, 0.17241379200237725, 0.174055828127172, 0.1592775030040193, 0.15763546687922456, 0.15599343075442978, 0.16256157535148175, 0.160919539226687, 0.16091953912881404, 0.16256157535148175, 0.1707717559754555, 0.167487683725866, 0.1756978644477127, 0.1707717558775825, 0.1707717558775825, 0.17405582822504498, 0.16748768382373896, 0.17569786425196673, 0.17733990047463447, 0.167487683725866, 0.16912971975278776, 0.16748768362799302, 0.16584564750319827, 0.16584564750319827, 0.16420361246724044, 0.16912971975278776, 0.1625615752536088, 0.16584564760107126, 0.1592775030040193, 0.16584564859203518, 0.15763546796806144, 0.1625615752536088, 0.1740558292160089, 0.174055828127172, 0.1707717569664194, 0.16420361246724044, 0.15927750409285618, 0.1527093595936772, 0.1559934318432667, 0.15927750409285618, 0.16748768471682993, 0.15763546796806144, 0.1559934318432667, 0.15927750409285618, 0.16091954021765092, 0.17241379309121416, 0.16584564859203518, 0.15927750409285618, 0.16091954021765092, 0.16256157634244567, 0.16584564859203518, 0.16420361246724044, 0.16584564859203518, 0.16912972084162467, 0.1707717569664194, 0.1707717569664194, 0.16912972084162467, 0.16912972084162467, 0.16912972084162467, 0.16584564859203518, 0.17241379200237725, 0.16748768471682993, 0.16748768471682993, 0.17241379309121416, 0.17241379309121416, 0.16912972084162467, 0.16912972084162467, 0.16748768471682993, 0.16420361137840353, 0.17241379200237725, 0.1707717559754555, 0.1625615752536088, 0.1773399014655984, 0.1625615752536088, 0.16912971975278776, 0.15599343085230277, 0.16420361147627652, 0.15435139571847195, 0.16912971985066075, 0.15435139571847195, 0.16912972084162467, 0.16912972074375168, 0.1625615762445727, 0.16091954021765092, 0.16748768362799302, 0.1625615752536088, 0.1592775030040193, 0.16091954021765092, 0.1707717559754555, 0.17569786425196673, 0.16420361246724044, 0.16584564750319827, 0.1625615752536088, 0.16420361147627652, 0.16748768362799302, 0.16584564750319827, 0.16912971975278776, 0.16584564859203518, 0.16912972084162467, 0.16912972084162467, 0.16420361137840353, 0.16748768362799302, 0.1625615752536088, 0.16420361137840353, 0.1592775030040193, 0.16091953912881404, 0.1592775030040193, 0.16584564750319827, 0.16584564750319827, 0.1707717558775825, 0.16584564750319827, 0.16584564750319827, 0.16748768362799302, 0.16584564859203518, 0.16584564750319827, 0.16748768362799302, 0.16912971975278776, 0.1625615752536088, 0.16420361137840353, 0.16420361137840353, 0.1625615752536088, 0.15763546687922456, 0.15763546697709752, 0.15927750310189226, 0.1592775030040193, 0.15927750310189226, 0.15927750310189226, 0.16091953912881404, 0.15927750310189226, 0.1625615752536088, 0.15599343085230277, 0.15435139472750803, 0.15599343075442978, 0.16091953912881404, 0.15763546687922456, 0.15763546796806144, 0.1707717558775825, 0.15927750409285618, 0.15435139571847195, 0.16091954021765092, 0.15763546796806144, 0.16420361246724044, 0.16420361137840353, 0.17241379309121416, 0.17241379200237725, 0.16748768471682993, 0.16912972084162467, 0.1707717569664194, 0.1707717569664194, 0.16584564859203518, 0.16912972084162467, 0.16912971985066075, 0.16420361137840353, 0.16091954021765092, 0.16748768471682993, 0.1740558292160089, 0.17569786534080364, 0.17898193650155622, 0.16584564859203518, 0.17898193650155622, 0.17569786425196673, 0.18719211712552997, 0.17898193749252014, 0.17241379309121416, 0.16584564750319827, 0.18226600983998262, 0.18062397371518787, 0.16256157634244567, 0.1707717558775825, 0.1773399014655984, 0.1773399013677254, 0.16091953912881404, 0.16420361137840353, 0.17241379200237725, 0.15435139571847195, 0.16091953912881404, 0.15435139472750803, 0.16912971975278776, 0.15599343085230277, 0.15106732346888246, 0.16091954021765092, 0.1592775030040193, 0.15270935860271329, 0.15927750409285618, 0.16256157634244567, 0.16584564750319827, 0.15435139571847195, 0.1592775030040193, 0.16420361137840353, 0.15599343075442978, 0.15435139462963504, 0.15927750409285618, 0.15599343075442978], 'loss': [2.712244849332304, 2.6979931388792315, 2.6885549150697994, 2.681677175302525, 2.675076095083656, 2.666881964485748, 2.659306729158092, 2.650263678513513, 2.641295957271568, 2.6313617462495023, 2.6200463403666534, 2.6057608757175705, 2.589027426717707, 2.5785548436323475, 2.564002122036975, 2.5530956627651897, 2.5429077375596063, 2.537227107171405, 2.5289357413501463, 2.5245134398677753, 2.5162929919710884, 2.5079771713554493, 2.501140100216719, 2.494796052263013, 2.484660005569458, 2.479703205468963, 2.474970498760623, 2.470844321084463, 2.4685800725674483, 2.4641059833385617, 2.4607812510378797, 2.459864524257746, 2.463022269360583, 2.4603765925342787, 2.4543433143372897, 2.463296549618856, 2.475903922968087, 2.457160747712153, 2.4546495790109497, 2.453275077298926, 2.4537923276302016, 2.4529074404518707, 2.4500317893723444, 2.4494929344012752, 2.4478610315851608, 2.447067777038355, 2.445929024302739, 2.446575909032959, 2.448156949088314, 2.4469045623371977, 2.446210046764272, 2.446318598498554, 2.448738702170903, 2.4463315401723498, 2.4461093386585464, 2.4412135626501126, 2.441181968810377, 2.4390292059959084, 2.439179510942964, 2.4406500216137457, 2.4433285777818496, 2.441681983583517, 2.4372438613883767, 2.438487734588999, 2.436978218834503, 2.4368116107564686, 2.4387809481219342, 2.4365184810372105, 2.4383364702886623, 2.437360935191599, 2.4378760730461417, 2.4380023757535083, 2.4371701260122185, 2.4354302702008823, 2.4365606435270526, 2.436053740170457, 2.435060363628536, 2.43510063649203, 2.434320140325558, 2.4353765585584073, 2.4350904097547277, 2.433779133320834, 2.433715728176203, 2.433646662279321, 2.434099267493528, 2.434179331096046, 2.4366439535387734, 2.4350665993269462, 2.4342708171515492, 2.4327926854088564, 2.434139856322835, 2.436732199941083, 2.436906283200399, 2.436730742209746, 2.4371520981406776, 2.4390191033145974, 2.4383915829707465, 2.4370697127230603, 2.4368296269763423, 2.43672795687368, 2.4370397893799893, 2.4356893278000538, 2.435366607936256, 2.435701913951114, 2.436261807134264, 2.4360135165572903, 2.436373673523231, 2.4347910939790385, 2.435821608498356, 2.4353232437580274, 2.435448409987181, 2.434019485097646, 2.4335870746714376, 2.4319920661268295, 2.4341880140363314, 2.4334437630749335, 2.4317566728689832, 2.4321805328314308, 2.4325697134162856, 2.4323371974349755, 2.433940990945397, 2.4486969799966047, 2.4365162520438006, 2.437291486747945, 2.4321535065433575, 2.433096625721675, 2.43188853048446, 2.430543708017964, 2.4306081124644505, 2.4307572558675212, 2.4299480872966917, 2.4313169308022062, 2.4303856036011933, 2.4310078611119326, 2.4312598068856115, 2.430353681556498, 2.431079820538938, 2.4308184298646522, 2.4320161075317883, 2.4312337227181, 2.4306634663066826, 2.4304906460294, 2.4298799440356493, 2.428535557380692, 2.4294760044105734, 2.431037336698058, 2.434524268242368, 2.4330034801357825, 2.4321333606140323, 2.4320459623356374, 2.4310512608326436, 2.431889143323017, 2.4318840265763613, 2.4319091659796555, 2.433152808692666, 2.431501067688333, 2.4320689808416662, 2.431330248560504, 2.4313306174973444, 2.4314930367518746, 2.4311606967228885, 2.4351190051014173, 2.431239358110839, 2.431460388876819, 2.431534030354243, 2.4350403795496884, 2.432407687968542, 2.4312373277832595, 2.4302818472625294, 2.4317877228989495, 2.430258907576606, 2.4276049423021946, 2.4264716418616827, 2.425438389004623, 2.4260072901018837, 2.427718596879462, 2.428896989861553, 2.429528229339412, 2.4287123286503784, 2.429695016304815, 2.4314730252573376, 2.4384130703105575, 2.4296362327599184, 2.4258675529237155, 2.4253668031163773, 2.425997297670807, 2.4262197365261446, 2.4252235682838017, 2.423475069676581, 2.424936078461289, 2.4241293023988697, 2.424733782793707, 2.4235623759165925, 2.4239830148293495, 2.4252098136369207, 2.4282046485485727, 2.43047898868271, 2.425745045428893, 2.4264660651189347, 2.4256882424716832, 2.423757774580186, 2.4239128070690303, 2.423639646741644, 2.424294685681008, 2.422356809482927, 2.423873840856846, 2.423780546638755, 2.4246249946970226, 2.4252116647833915, 2.424650465340585, 2.424091227784049, 2.4226232248654846, 2.4223368492948936, 2.422163740075834, 2.423274983615601, 2.4241731733756877, 2.423594228247108, 2.423224640382144, 2.4231422580977484, 2.4230764625008834, 2.4236750088678005, 2.423480320858025, 2.423246132128048, 2.423030797950541, 2.422297494073668, 2.4227902206796883, 2.4216879021215734, 2.420956533596501, 2.42191223647805, 2.422505857420653, 2.4212980553354817, 2.421822243598452, 2.420966501647197, 2.421213698142363, 2.4204823408773057, 2.420515987270911, 2.420856736768687, 2.420693795734852, 2.4212328305724222, 2.4203918674398497, 2.4197716860800553, 2.4196015477425266, 2.421043420376474, 2.421954411990344, 2.420552848154025, 2.4216087672255124, 2.4215516673955584, 2.4205346536342613, 2.4199418713179948, 2.4202436342376457, 2.42103512830558, 2.4205570319839573, 2.4219012323101445, 2.420133449507445, 2.419821623855058, 2.420155836277674, 2.4207444951764368, 2.4210069317592486, 2.4214420783691093, 2.4225853056388713, 2.4226363156610446, 2.4222601103342045, 2.423162337054462, 2.424213232298896, 2.4235940768733406, 2.425083486745, 2.421786233482909, 2.4221574361319416, 2.4213694348227564, 2.421442007773711, 2.4214495600126607, 2.421095186777918, 2.421763217914276, 2.421398484584487, 2.420829447485828, 2.4210528588148112, 2.4196694049991865, 2.4202048322258545, 2.4201760063915527, 2.420048639367983, 2.420280664118898, 2.4205334304049764, 2.419691734294382, 2.4192377986359643, 2.4201140213796, 2.4194455068459013, 2.4208168686782554, 2.420047690883065, 2.4184934035463743, 2.419832832857324, 2.4190416835416757, 2.419616407535404, 2.420249816475463, 2.4186028060482267, 2.4202937776303144, 2.418601968107282, 2.4195650290659567, 2.4183642407951904, 2.4187558839942884, 2.4183743842083816], 'acc': [0.0924024642064586, 0.10554414818051903, 0.08993839890805114, 0.08706365478907767, 0.08911704326190009, 0.11211498965962467, 0.11663244366339834, 0.12114989665132284, 0.12032854262685874, 0.11663244345839263, 0.1285420933665681, 0.15852156105227538, 0.1642710479377966, 0.15934291474016296, 0.16016427134708702, 0.16591375778587936, 0.15975359335572323, 0.15893223790539854, 0.15728952803895704, 0.15934291691261152, 0.15975359232151534, 0.1605749483960365, 0.15934291491763058, 0.1613963028121533, 0.16427104672612106, 0.17125256564582889, 0.16796714470127036, 0.17494866598925307, 0.17248459949752878, 0.1737166325475646, 0.1749486642084572, 0.17535934184488575, 0.17412730916814392, 0.17330595451948333, 0.17659137624734725, 0.17700205306375297, 0.1704312109971683, 0.17577001985460824, 0.1741273101656344, 0.17823408513465702, 0.16919917776966487, 0.1749486642084572, 0.1798767976019172, 0.17823408515301573, 0.17782340931574178, 0.17823408554466844, 0.1798767976019172, 0.18357289536288143, 0.1728952767055872, 0.17823408673798524, 0.16796714511128177, 0.1704312121904851, 0.17207392109615358, 0.17823408652380018, 0.17987679719190577, 0.179876795625295, 0.17618069806015713, 0.18069815105726098, 0.1782340861505062, 0.18110883026030028, 0.17700205407960215, 0.17659137606987962, 0.17987679640860038, 0.17618069763178698, 0.17946611996548867, 0.1704312121904851, 0.17577002161704539, 0.17618069862927743, 0.18110882965446254, 0.17494866501012132, 0.17987679758355848, 0.17782340812242498, 0.17782340872826272, 0.17741273091436657, 0.17659137646153233, 0.1716632440472041, 0.1757700208153813, 0.17535934321567018, 0.17494866501012132, 0.17659137647989104, 0.1757700200320759, 0.1770020531004704, 0.17700205229880628, 0.1786447629669119, 0.18028747384920257, 0.1782340867196265, 0.17412730936397028, 0.1696098567401604, 0.1749486646001099, 0.17864476453352268, 0.18151950727253235, 0.17659137624734725, 0.17371663156843284, 0.1704312125821378, 0.1724846006908456, 0.17453798837118326, 0.1753593426098324, 0.17494866516923024, 0.1720739212736212, 0.17330595569444143, 0.16960985517354962, 0.1741273109489398, 0.17494866518758895, 0.17412730936397028, 0.17618069903928885, 0.177002052690459, 0.1782340857404948, 0.1765913764431736, 0.17700205228044757, 0.1778234083182513, 0.18028747500580194, 0.18439424981571564, 0.18069815223221905, 0.17987679660442674, 0.18357289497122872, 0.1823408617253666, 0.183162218528117, 0.1831622181364643, 0.18357289538124014, 0.181108830651953, 0.18110882885279841, 0.1839835735684303, 0.1823408617253666, 0.17577001944459683, 0.18685831710910406, 0.17987679699607942, 0.1811088296361038, 0.18480492921458133, 0.18521560581680196, 0.18193018410729678, 0.18316221656985351, 0.18357289595036047, 0.1856262826699251, 0.18439424961988932, 0.18275154032256813, 0.1811088286753308, 0.18193018489060217, 0.1843942512048588, 0.18316221676567987, 0.1843942504031947, 0.18398357337260393, 0.1852156066368248, 0.18480492743378546, 0.18480492782543817, 0.18644763966850186, 0.18521560503349657, 0.18357289497122872, 0.18480492843127594, 0.18562628364905684, 0.18603696165877937, 0.18767967076027417, 0.18685831630743993, 0.18685831669909264, 0.18644763829771743, 0.18521560483767022, 0.1864476374960533, 0.18644763888519647, 0.18644763790606472, 0.18398357396008297, 0.18439425099067375, 0.18480492702377405, 0.189733059829755, 0.1913757693229025, 0.18685831570160216, 0.18767967056444784, 0.18316221713897385, 0.18439425159651152, 0.18562628325740416, 0.1835728941695646, 0.1843942507948474, 0.18398357376425664, 0.18726899273219294, 0.19014373645033436, 0.18973305963392864, 0.18932238141002106, 0.19178644713679868, 0.18439425038483598, 0.18357289498958745, 0.1889117046119741, 0.1880903495533021, 0.1786447649435341, 0.18316221774481162, 0.18275154071422084, 0.18685831709074532, 0.1856262838448832, 0.18603696025127747, 0.18316221756734397, 0.18357289497122872, 0.18809034994495477, 0.1843942511865001, 0.18685831609325487, 0.18316221833229065, 0.18193018567390756, 0.18398357258929854, 0.19014373686034577, 0.1889117032044722, 0.18726899392550975, 0.18398357180599315, 0.1893223826216966, 0.1872689933380307, 0.19178644772427772, 0.1885010263880665, 0.18767967154357956, 0.18809034818251766, 0.18932238123255343, 0.1905544154575473, 0.19014373646869306, 0.18809034896582305, 0.18685831709074532, 0.1872689941213361, 0.1880903487699967, 0.18973305924227596, 0.1930184811476075, 0.19178644752845136, 0.1893223826033379, 0.1872689945129888, 0.18480492843127594, 0.18644763966850186, 0.1868583157199609, 0.18932238242587024, 0.18973306022140768, 0.1872689933380307, 0.19014373643197563, 0.18850102658389287, 0.1856262842548946, 0.18850102736719826, 0.1860369618546057, 0.18685831709074532, 0.19425051384270803, 0.18973306022140768, 0.19055441506589463, 0.18891170341865726, 0.18932238201585883, 0.1913757708895133, 0.1876796715619383, 0.18973305963392864, 0.19055441563501496, 0.18891170480780042, 0.19178644813428913, 0.19178644811593043, 0.18685831630743993, 0.18809034898418175, 0.19178644774263645, 0.18726899453134752, 0.1934291570215989, 0.1905544154575473, 0.1901437374478248, 0.18767967154357956, 0.19301848134343383, 0.18809035014078113, 0.1921971255381739, 0.19055441367675147, 0.1901437366461607, 0.1901437374478248, 0.18850102560476112, 0.18850102738555696, 0.19219712495069485, 0.19055441408676288, 0.1909650920964854, 0.19178644792010408, 0.18850102777720967, 0.18685831728657168, 0.1872689933380307, 0.18357289516705508, 0.18357289497122872, 0.18644763888519647, 0.18439424942406296, 0.18439424981571564, 0.18767967154357956, 0.18726899331967198, 0.18562628382652446, 0.1827515401267418, 0.18603696108965903, 0.18151950746835868, 0.1889117046119741, 0.1864476377102384, 0.18891170322283093, 0.18521560485602895, 0.1909650924881381, 0.1913757701062079, 0.18932238240751154, 0.18521560622681338, 0.1909650920964854, 0.19671457990606217, 0.19055441428258924, 0.19096509268396444, 0.18973306041723403, 0.19219712612565293, 0.18809034796833257, 0.19301848016847575, 0.18850102578222874, 0.1905544144784156, 0.18850102775885094, 0.18891170322283093, 0.19096509229231173, 0.1901437366461607, 0.1885010267797192, 0.19671457972859455, 0.19055441406840418, 0.19096509229231173]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
