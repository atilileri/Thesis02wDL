{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf23.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 23:05:05 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '04', '05', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023D8692BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023DC9606EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6106, Accuracy:0.2078, Validation Loss:1.6092, Validation Accuracy:0.2069\n",
    "Epoch #2: Loss:1.6091, Accuracy:0.1988, Validation Loss:1.6091, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6084, Accuracy:0.2329, Validation Loss:1.6076, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6073, Accuracy:0.2329, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6067, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6061, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #22: Loss:1.6031, Accuracy:0.2386, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #23: Loss:1.6029, Accuracy:0.2370, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #25: Loss:1.6024, Accuracy:0.2378, Validation Loss:1.6005, Validation Accuracy:0.2463\n",
    "Epoch #26: Loss:1.6019, Accuracy:0.2382, Validation Loss:1.6005, Validation Accuracy:0.2430\n",
    "Epoch #27: Loss:1.6015, Accuracy:0.2398, Validation Loss:1.6002, Validation Accuracy:0.2479\n",
    "Epoch #28: Loss:1.6014, Accuracy:0.2435, Validation Loss:1.6004, Validation Accuracy:0.2447\n",
    "Epoch #29: Loss:1.6016, Accuracy:0.2431, Validation Loss:1.6000, Validation Accuracy:0.2479\n",
    "Epoch #30: Loss:1.6010, Accuracy:0.2411, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #31: Loss:1.6009, Accuracy:0.2411, Validation Loss:1.5999, Validation Accuracy:0.2479\n",
    "Epoch #32: Loss:1.6010, Accuracy:0.2415, Validation Loss:1.5996, Validation Accuracy:0.2512\n",
    "Epoch #33: Loss:1.6011, Accuracy:0.2419, Validation Loss:1.5998, Validation Accuracy:0.2496\n",
    "Epoch #34: Loss:1.6006, Accuracy:0.2423, Validation Loss:1.5995, Validation Accuracy:0.2512\n",
    "Epoch #35: Loss:1.6003, Accuracy:0.2415, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #36: Loss:1.6003, Accuracy:0.2415, Validation Loss:1.5998, Validation Accuracy:0.2414\n",
    "Epoch #37: Loss:1.6000, Accuracy:0.2435, Validation Loss:1.5989, Validation Accuracy:0.2496\n",
    "Epoch #38: Loss:1.5998, Accuracy:0.2374, Validation Loss:1.6002, Validation Accuracy:0.2512\n",
    "Epoch #39: Loss:1.5998, Accuracy:0.2357, Validation Loss:1.6002, Validation Accuracy:0.2496\n",
    "Epoch #40: Loss:1.5997, Accuracy:0.2366, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #41: Loss:1.6063, Accuracy:0.2324, Validation Loss:1.6030, Validation Accuracy:0.2348\n",
    "Epoch #42: Loss:1.6033, Accuracy:0.2341, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #43: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2348\n",
    "Epoch #44: Loss:1.6043, Accuracy:0.2349, Validation Loss:1.6049, Validation Accuracy:0.2397\n",
    "Epoch #45: Loss:1.6025, Accuracy:0.2329, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #46: Loss:1.6028, Accuracy:0.2333, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #47: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #48: Loss:1.6017, Accuracy:0.2357, Validation Loss:1.6016, Validation Accuracy:0.2479\n",
    "Epoch #49: Loss:1.6013, Accuracy:0.2366, Validation Loss:1.6015, Validation Accuracy:0.2447\n",
    "Epoch #50: Loss:1.6011, Accuracy:0.2394, Validation Loss:1.6008, Validation Accuracy:0.2512\n",
    "Epoch #51: Loss:1.6010, Accuracy:0.2361, Validation Loss:1.6005, Validation Accuracy:0.2397\n",
    "Epoch #52: Loss:1.6008, Accuracy:0.2394, Validation Loss:1.6002, Validation Accuracy:0.2496\n",
    "Epoch #53: Loss:1.6008, Accuracy:0.2407, Validation Loss:1.6003, Validation Accuracy:0.2479\n",
    "Epoch #54: Loss:1.6009, Accuracy:0.2407, Validation Loss:1.6002, Validation Accuracy:0.2479\n",
    "Epoch #55: Loss:1.6010, Accuracy:0.2390, Validation Loss:1.6002, Validation Accuracy:0.2479\n",
    "Epoch #56: Loss:1.6004, Accuracy:0.2427, Validation Loss:1.6002, Validation Accuracy:0.2463\n",
    "Epoch #57: Loss:1.6005, Accuracy:0.2382, Validation Loss:1.5998, Validation Accuracy:0.2529\n",
    "Epoch #58: Loss:1.6004, Accuracy:0.2423, Validation Loss:1.5995, Validation Accuracy:0.2479\n",
    "Epoch #59: Loss:1.6002, Accuracy:0.2411, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #60: Loss:1.6002, Accuracy:0.2398, Validation Loss:1.5995, Validation Accuracy:0.2578\n",
    "Epoch #61: Loss:1.6005, Accuracy:0.2374, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #62: Loss:1.6001, Accuracy:0.2386, Validation Loss:1.5998, Validation Accuracy:0.2529\n",
    "Epoch #63: Loss:1.5999, Accuracy:0.2349, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #64: Loss:1.5999, Accuracy:0.2402, Validation Loss:1.5993, Validation Accuracy:0.2496\n",
    "Epoch #65: Loss:1.6003, Accuracy:0.2337, Validation Loss:1.5995, Validation Accuracy:0.2512\n",
    "Epoch #66: Loss:1.5999, Accuracy:0.2349, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #67: Loss:1.5993, Accuracy:0.2402, Validation Loss:1.5998, Validation Accuracy:0.2545\n",
    "Epoch #68: Loss:1.5998, Accuracy:0.2407, Validation Loss:1.6010, Validation Accuracy:0.2545\n",
    "Epoch #69: Loss:1.5997, Accuracy:0.2419, Validation Loss:1.6010, Validation Accuracy:0.2529\n",
    "Epoch #70: Loss:1.5989, Accuracy:0.2427, Validation Loss:1.6005, Validation Accuracy:0.2512\n",
    "Epoch #71: Loss:1.5996, Accuracy:0.2349, Validation Loss:1.6005, Validation Accuracy:0.2496\n",
    "Epoch #72: Loss:1.5997, Accuracy:0.2398, Validation Loss:1.5996, Validation Accuracy:0.2512\n",
    "Epoch #73: Loss:1.6003, Accuracy:0.2402, Validation Loss:1.6000, Validation Accuracy:0.2496\n",
    "Epoch #74: Loss:1.5999, Accuracy:0.2357, Validation Loss:1.6001, Validation Accuracy:0.2562\n",
    "Epoch #75: Loss:1.5999, Accuracy:0.2370, Validation Loss:1.6001, Validation Accuracy:0.2545\n",
    "Epoch #76: Loss:1.6002, Accuracy:0.2386, Validation Loss:1.6002, Validation Accuracy:0.2529\n",
    "Epoch #77: Loss:1.5998, Accuracy:0.2374, Validation Loss:1.6002, Validation Accuracy:0.2562\n",
    "Epoch #78: Loss:1.5996, Accuracy:0.2411, Validation Loss:1.6001, Validation Accuracy:0.2529\n",
    "Epoch #79: Loss:1.5991, Accuracy:0.2366, Validation Loss:1.6006, Validation Accuracy:0.2512\n",
    "Epoch #80: Loss:1.5993, Accuracy:0.2394, Validation Loss:1.6001, Validation Accuracy:0.2545\n",
    "Epoch #81: Loss:1.5995, Accuracy:0.2394, Validation Loss:1.5994, Validation Accuracy:0.2562\n",
    "Epoch #82: Loss:1.6001, Accuracy:0.2423, Validation Loss:1.5990, Validation Accuracy:0.2529\n",
    "Epoch #83: Loss:1.6000, Accuracy:0.2382, Validation Loss:1.5989, Validation Accuracy:0.2496\n",
    "Epoch #84: Loss:1.5996, Accuracy:0.2407, Validation Loss:1.5984, Validation Accuracy:0.2496\n",
    "Epoch #85: Loss:1.5995, Accuracy:0.2427, Validation Loss:1.5986, Validation Accuracy:0.2578\n",
    "Epoch #86: Loss:1.5995, Accuracy:0.2398, Validation Loss:1.5988, Validation Accuracy:0.2578\n",
    "Epoch #87: Loss:1.5991, Accuracy:0.2370, Validation Loss:1.5992, Validation Accuracy:0.2578\n",
    "Epoch #88: Loss:1.5987, Accuracy:0.2329, Validation Loss:1.5984, Validation Accuracy:0.2594\n",
    "Epoch #89: Loss:1.5996, Accuracy:0.2304, Validation Loss:1.5972, Validation Accuracy:0.2611\n",
    "Epoch #90: Loss:1.5999, Accuracy:0.2415, Validation Loss:1.5975, Validation Accuracy:0.2512\n",
    "Epoch #91: Loss:1.5992, Accuracy:0.2423, Validation Loss:1.5992, Validation Accuracy:0.2463\n",
    "Epoch #92: Loss:1.5990, Accuracy:0.2402, Validation Loss:1.5985, Validation Accuracy:0.2562\n",
    "Epoch #93: Loss:1.6000, Accuracy:0.2345, Validation Loss:1.5993, Validation Accuracy:0.2545\n",
    "Epoch #94: Loss:1.5986, Accuracy:0.2423, Validation Loss:1.5998, Validation Accuracy:0.2562\n",
    "Epoch #95: Loss:1.5983, Accuracy:0.2411, Validation Loss:1.5987, Validation Accuracy:0.2496\n",
    "Epoch #96: Loss:1.5985, Accuracy:0.2407, Validation Loss:1.5984, Validation Accuracy:0.2529\n",
    "Epoch #97: Loss:1.5975, Accuracy:0.2402, Validation Loss:1.5996, Validation Accuracy:0.2562\n",
    "Epoch #98: Loss:1.5981, Accuracy:0.2361, Validation Loss:1.5994, Validation Accuracy:0.2578\n",
    "Epoch #99: Loss:1.5973, Accuracy:0.2423, Validation Loss:1.5995, Validation Accuracy:0.2414\n",
    "Epoch #100: Loss:1.5975, Accuracy:0.2419, Validation Loss:1.5980, Validation Accuracy:0.2578\n",
    "Epoch #101: Loss:1.5973, Accuracy:0.2407, Validation Loss:1.5977, Validation Accuracy:0.2529\n",
    "Epoch #102: Loss:1.5967, Accuracy:0.2402, Validation Loss:1.5978, Validation Accuracy:0.2578\n",
    "Epoch #103: Loss:1.5967, Accuracy:0.2423, Validation Loss:1.5979, Validation Accuracy:0.2315\n",
    "Epoch #104: Loss:1.5960, Accuracy:0.2398, Validation Loss:1.5990, Validation Accuracy:0.2594\n",
    "Epoch #105: Loss:1.5961, Accuracy:0.2419, Validation Loss:1.5992, Validation Accuracy:0.2594\n",
    "Epoch #106: Loss:1.5957, Accuracy:0.2444, Validation Loss:1.5983, Validation Accuracy:0.2545\n",
    "Epoch #107: Loss:1.5957, Accuracy:0.2415, Validation Loss:1.5974, Validation Accuracy:0.2578\n",
    "Epoch #108: Loss:1.5965, Accuracy:0.2370, Validation Loss:1.5976, Validation Accuracy:0.2545\n",
    "Epoch #109: Loss:1.5978, Accuracy:0.2296, Validation Loss:1.5991, Validation Accuracy:0.2463\n",
    "Epoch #110: Loss:1.6004, Accuracy:0.2374, Validation Loss:1.5987, Validation Accuracy:0.2512\n",
    "Epoch #111: Loss:1.5960, Accuracy:0.2452, Validation Loss:1.6003, Validation Accuracy:0.2627\n",
    "Epoch #112: Loss:1.5982, Accuracy:0.2390, Validation Loss:1.5996, Validation Accuracy:0.2644\n",
    "Epoch #113: Loss:1.5976, Accuracy:0.2456, Validation Loss:1.6006, Validation Accuracy:0.2479\n",
    "Epoch #114: Loss:1.5965, Accuracy:0.2402, Validation Loss:1.5990, Validation Accuracy:0.2529\n",
    "Epoch #115: Loss:1.5970, Accuracy:0.2464, Validation Loss:1.5991, Validation Accuracy:0.2529\n",
    "Epoch #116: Loss:1.5957, Accuracy:0.2489, Validation Loss:1.5990, Validation Accuracy:0.2545\n",
    "Epoch #117: Loss:1.5962, Accuracy:0.2448, Validation Loss:1.5983, Validation Accuracy:0.2594\n",
    "Epoch #118: Loss:1.5948, Accuracy:0.2423, Validation Loss:1.5985, Validation Accuracy:0.2562\n",
    "Epoch #119: Loss:1.5942, Accuracy:0.2431, Validation Loss:1.5985, Validation Accuracy:0.2447\n",
    "Epoch #120: Loss:1.5933, Accuracy:0.2456, Validation Loss:1.5985, Validation Accuracy:0.2397\n",
    "Epoch #121: Loss:1.5939, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2529\n",
    "Epoch #122: Loss:1.5944, Accuracy:0.2402, Validation Loss:1.5982, Validation Accuracy:0.2414\n",
    "Epoch #123: Loss:1.5939, Accuracy:0.2394, Validation Loss:1.5981, Validation Accuracy:0.2479\n",
    "Epoch #124: Loss:1.5927, Accuracy:0.2468, Validation Loss:1.5989, Validation Accuracy:0.2594\n",
    "Epoch #125: Loss:1.5933, Accuracy:0.2464, Validation Loss:1.5975, Validation Accuracy:0.2447\n",
    "Epoch #126: Loss:1.5924, Accuracy:0.2431, Validation Loss:1.5971, Validation Accuracy:0.2447\n",
    "Epoch #127: Loss:1.5925, Accuracy:0.2415, Validation Loss:1.5967, Validation Accuracy:0.2447\n",
    "Epoch #128: Loss:1.5931, Accuracy:0.2427, Validation Loss:1.5976, Validation Accuracy:0.2447\n",
    "Epoch #129: Loss:1.5930, Accuracy:0.2419, Validation Loss:1.5974, Validation Accuracy:0.2397\n",
    "Epoch #130: Loss:1.5940, Accuracy:0.2472, Validation Loss:1.5981, Validation Accuracy:0.2496\n",
    "Epoch #131: Loss:1.5933, Accuracy:0.2423, Validation Loss:1.5977, Validation Accuracy:0.2397\n",
    "Epoch #132: Loss:1.5949, Accuracy:0.2419, Validation Loss:1.5963, Validation Accuracy:0.2299\n",
    "Epoch #133: Loss:1.5965, Accuracy:0.2419, Validation Loss:1.5971, Validation Accuracy:0.2348\n",
    "Epoch #134: Loss:1.5950, Accuracy:0.2407, Validation Loss:1.5964, Validation Accuracy:0.2250\n",
    "Epoch #135: Loss:1.5937, Accuracy:0.2460, Validation Loss:1.5963, Validation Accuracy:0.2266\n",
    "Epoch #136: Loss:1.5940, Accuracy:0.2419, Validation Loss:1.5969, Validation Accuracy:0.2266\n",
    "Epoch #137: Loss:1.5938, Accuracy:0.2435, Validation Loss:1.5964, Validation Accuracy:0.2282\n",
    "Epoch #138: Loss:1.5943, Accuracy:0.2448, Validation Loss:1.5957, Validation Accuracy:0.2299\n",
    "Epoch #139: Loss:1.5937, Accuracy:0.2448, Validation Loss:1.5970, Validation Accuracy:0.2184\n",
    "Epoch #140: Loss:1.5936, Accuracy:0.2444, Validation Loss:1.5981, Validation Accuracy:0.2332\n",
    "Epoch #141: Loss:1.5975, Accuracy:0.2402, Validation Loss:1.5960, Validation Accuracy:0.2233\n",
    "Epoch #142: Loss:1.5954, Accuracy:0.2460, Validation Loss:1.5982, Validation Accuracy:0.2430\n",
    "Epoch #143: Loss:1.5930, Accuracy:0.2493, Validation Loss:1.5987, Validation Accuracy:0.2250\n",
    "Epoch #144: Loss:1.5959, Accuracy:0.2505, Validation Loss:1.5978, Validation Accuracy:0.2332\n",
    "Epoch #145: Loss:1.5961, Accuracy:0.2472, Validation Loss:1.5941, Validation Accuracy:0.2479\n",
    "Epoch #146: Loss:1.5949, Accuracy:0.2378, Validation Loss:1.5957, Validation Accuracy:0.2397\n",
    "Epoch #147: Loss:1.5965, Accuracy:0.2439, Validation Loss:1.5944, Validation Accuracy:0.2397\n",
    "Epoch #148: Loss:1.5974, Accuracy:0.2444, Validation Loss:1.5970, Validation Accuracy:0.2397\n",
    "Epoch #149: Loss:1.5965, Accuracy:0.2476, Validation Loss:1.5972, Validation Accuracy:0.2463\n",
    "Epoch #150: Loss:1.5965, Accuracy:0.2378, Validation Loss:1.5961, Validation Accuracy:0.2562\n",
    "Epoch #151: Loss:1.5963, Accuracy:0.2374, Validation Loss:1.5961, Validation Accuracy:0.2545\n",
    "Epoch #152: Loss:1.5960, Accuracy:0.2480, Validation Loss:1.5971, Validation Accuracy:0.2447\n",
    "Epoch #153: Loss:1.5956, Accuracy:0.2472, Validation Loss:1.5970, Validation Accuracy:0.2496\n",
    "Epoch #154: Loss:1.5956, Accuracy:0.2448, Validation Loss:1.5961, Validation Accuracy:0.2545\n",
    "Epoch #155: Loss:1.5953, Accuracy:0.2501, Validation Loss:1.5957, Validation Accuracy:0.2496\n",
    "Epoch #156: Loss:1.5950, Accuracy:0.2493, Validation Loss:1.5971, Validation Accuracy:0.2512\n",
    "Epoch #157: Loss:1.5957, Accuracy:0.2509, Validation Loss:1.5963, Validation Accuracy:0.2512\n",
    "Epoch #158: Loss:1.5944, Accuracy:0.2530, Validation Loss:1.5980, Validation Accuracy:0.2512\n",
    "Epoch #159: Loss:1.5952, Accuracy:0.2476, Validation Loss:1.5971, Validation Accuracy:0.2562\n",
    "Epoch #160: Loss:1.5956, Accuracy:0.2501, Validation Loss:1.5960, Validation Accuracy:0.2496\n",
    "Epoch #161: Loss:1.5941, Accuracy:0.2517, Validation Loss:1.5983, Validation Accuracy:0.2545\n",
    "Epoch #162: Loss:1.5940, Accuracy:0.2563, Validation Loss:1.5981, Validation Accuracy:0.2496\n",
    "Epoch #163: Loss:1.5932, Accuracy:0.2526, Validation Loss:1.5976, Validation Accuracy:0.2545\n",
    "Epoch #164: Loss:1.5971, Accuracy:0.2415, Validation Loss:1.5999, Validation Accuracy:0.2594\n",
    "Epoch #165: Loss:1.5964, Accuracy:0.2485, Validation Loss:1.6010, Validation Accuracy:0.2496\n",
    "Epoch #166: Loss:1.5978, Accuracy:0.2431, Validation Loss:1.5983, Validation Accuracy:0.2578\n",
    "Epoch #167: Loss:1.5957, Accuracy:0.2559, Validation Loss:1.5991, Validation Accuracy:0.2545\n",
    "Epoch #168: Loss:1.5949, Accuracy:0.2493, Validation Loss:1.5972, Validation Accuracy:0.2562\n",
    "Epoch #169: Loss:1.5955, Accuracy:0.2501, Validation Loss:1.5971, Validation Accuracy:0.2512\n",
    "Epoch #170: Loss:1.5955, Accuracy:0.2493, Validation Loss:1.5989, Validation Accuracy:0.2397\n",
    "Epoch #171: Loss:1.5948, Accuracy:0.2448, Validation Loss:1.5981, Validation Accuracy:0.2512\n",
    "Epoch #172: Loss:1.5947, Accuracy:0.2497, Validation Loss:1.5986, Validation Accuracy:0.2512\n",
    "Epoch #173: Loss:1.5936, Accuracy:0.2501, Validation Loss:1.5992, Validation Accuracy:0.2430\n",
    "Epoch #174: Loss:1.5940, Accuracy:0.2534, Validation Loss:1.5995, Validation Accuracy:0.2463\n",
    "Epoch #175: Loss:1.5936, Accuracy:0.2517, Validation Loss:1.5990, Validation Accuracy:0.2512\n",
    "Epoch #176: Loss:1.5933, Accuracy:0.2485, Validation Loss:1.5994, Validation Accuracy:0.2512\n",
    "Epoch #177: Loss:1.5929, Accuracy:0.2550, Validation Loss:1.5995, Validation Accuracy:0.2496\n",
    "Epoch #178: Loss:1.5929, Accuracy:0.2526, Validation Loss:1.6001, Validation Accuracy:0.2381\n",
    "Epoch #179: Loss:1.5926, Accuracy:0.2530, Validation Loss:1.5996, Validation Accuracy:0.2414\n",
    "Epoch #180: Loss:1.5923, Accuracy:0.2468, Validation Loss:1.6002, Validation Accuracy:0.2414\n",
    "Epoch #181: Loss:1.5930, Accuracy:0.2476, Validation Loss:1.6000, Validation Accuracy:0.2496\n",
    "Epoch #182: Loss:1.5925, Accuracy:0.2505, Validation Loss:1.5992, Validation Accuracy:0.2463\n",
    "Epoch #183: Loss:1.5919, Accuracy:0.2501, Validation Loss:1.5991, Validation Accuracy:0.2479\n",
    "Epoch #184: Loss:1.5923, Accuracy:0.2554, Validation Loss:1.5986, Validation Accuracy:0.2496\n",
    "Epoch #185: Loss:1.5915, Accuracy:0.2575, Validation Loss:1.5985, Validation Accuracy:0.2496\n",
    "Epoch #186: Loss:1.5917, Accuracy:0.2509, Validation Loss:1.5983, Validation Accuracy:0.2479\n",
    "Epoch #187: Loss:1.5914, Accuracy:0.2587, Validation Loss:1.5986, Validation Accuracy:0.2463\n",
    "Epoch #188: Loss:1.5913, Accuracy:0.2534, Validation Loss:1.5989, Validation Accuracy:0.2348\n",
    "Epoch #189: Loss:1.5911, Accuracy:0.2583, Validation Loss:1.6007, Validation Accuracy:0.2512\n",
    "Epoch #190: Loss:1.5917, Accuracy:0.2571, Validation Loss:1.6004, Validation Accuracy:0.2463\n",
    "Epoch #191: Loss:1.5914, Accuracy:0.2595, Validation Loss:1.6009, Validation Accuracy:0.2463\n",
    "Epoch #192: Loss:1.5904, Accuracy:0.2559, Validation Loss:1.5998, Validation Accuracy:0.2496\n",
    "Epoch #193: Loss:1.5906, Accuracy:0.2583, Validation Loss:1.6025, Validation Accuracy:0.2447\n",
    "Epoch #194: Loss:1.5906, Accuracy:0.2587, Validation Loss:1.6009, Validation Accuracy:0.2496\n",
    "Epoch #195: Loss:1.5897, Accuracy:0.2579, Validation Loss:1.6017, Validation Accuracy:0.2397\n",
    "Epoch #196: Loss:1.5897, Accuracy:0.2624, Validation Loss:1.6028, Validation Accuracy:0.2479\n",
    "Epoch #197: Loss:1.5896, Accuracy:0.2649, Validation Loss:1.6039, Validation Accuracy:0.2397\n",
    "Epoch #198: Loss:1.5894, Accuracy:0.2575, Validation Loss:1.6041, Validation Accuracy:0.2496\n",
    "Epoch #199: Loss:1.5892, Accuracy:0.2604, Validation Loss:1.6042, Validation Accuracy:0.2430\n",
    "Epoch #200: Loss:1.5903, Accuracy:0.2571, Validation Loss:1.6047, Validation Accuracy:0.2348\n",
    "Epoch #201: Loss:1.5903, Accuracy:0.2591, Validation Loss:1.6043, Validation Accuracy:0.2397\n",
    "Epoch #202: Loss:1.5895, Accuracy:0.2530, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #203: Loss:1.5915, Accuracy:0.2522, Validation Loss:1.6040, Validation Accuracy:0.2397\n",
    "Epoch #204: Loss:1.5895, Accuracy:0.2554, Validation Loss:1.6037, Validation Accuracy:0.2299\n",
    "Epoch #205: Loss:1.5900, Accuracy:0.2624, Validation Loss:1.6043, Validation Accuracy:0.2365\n",
    "Epoch #206: Loss:1.5892, Accuracy:0.2571, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #207: Loss:1.5895, Accuracy:0.2579, Validation Loss:1.6032, Validation Accuracy:0.2365\n",
    "Epoch #208: Loss:1.5898, Accuracy:0.2583, Validation Loss:1.6030, Validation Accuracy:0.2397\n",
    "Epoch #209: Loss:1.5895, Accuracy:0.2628, Validation Loss:1.6040, Validation Accuracy:0.2365\n",
    "Epoch #210: Loss:1.5895, Accuracy:0.2604, Validation Loss:1.6047, Validation Accuracy:0.2381\n",
    "Epoch #211: Loss:1.5908, Accuracy:0.2600, Validation Loss:1.6045, Validation Accuracy:0.2282\n",
    "Epoch #212: Loss:1.5884, Accuracy:0.2595, Validation Loss:1.6064, Validation Accuracy:0.2348\n",
    "Epoch #213: Loss:1.5887, Accuracy:0.2604, Validation Loss:1.6038, Validation Accuracy:0.2348\n",
    "Epoch #214: Loss:1.5908, Accuracy:0.2579, Validation Loss:1.6033, Validation Accuracy:0.2381\n",
    "Epoch #215: Loss:1.5884, Accuracy:0.2579, Validation Loss:1.6060, Validation Accuracy:0.2381\n",
    "Epoch #216: Loss:1.5875, Accuracy:0.2632, Validation Loss:1.6045, Validation Accuracy:0.2282\n",
    "Epoch #217: Loss:1.5890, Accuracy:0.2600, Validation Loss:1.6063, Validation Accuracy:0.2250\n",
    "Epoch #218: Loss:1.5901, Accuracy:0.2653, Validation Loss:1.6072, Validation Accuracy:0.2414\n",
    "Epoch #219: Loss:1.5927, Accuracy:0.2649, Validation Loss:1.6065, Validation Accuracy:0.2299\n",
    "Epoch #220: Loss:1.5907, Accuracy:0.2674, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #221: Loss:1.5905, Accuracy:0.2637, Validation Loss:1.6038, Validation Accuracy:0.2266\n",
    "Epoch #222: Loss:1.5904, Accuracy:0.2571, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #223: Loss:1.5896, Accuracy:0.2608, Validation Loss:1.6030, Validation Accuracy:0.2365\n",
    "Epoch #224: Loss:1.5894, Accuracy:0.2612, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #225: Loss:1.5892, Accuracy:0.2641, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #226: Loss:1.5896, Accuracy:0.2579, Validation Loss:1.6029, Validation Accuracy:0.2282\n",
    "Epoch #227: Loss:1.5900, Accuracy:0.2579, Validation Loss:1.6050, Validation Accuracy:0.2348\n",
    "Epoch #228: Loss:1.5909, Accuracy:0.2522, Validation Loss:1.6038, Validation Accuracy:0.2299\n",
    "Epoch #229: Loss:1.5891, Accuracy:0.2563, Validation Loss:1.6037, Validation Accuracy:0.2397\n",
    "Epoch #230: Loss:1.5887, Accuracy:0.2559, Validation Loss:1.6047, Validation Accuracy:0.2266\n",
    "Epoch #231: Loss:1.5894, Accuracy:0.2563, Validation Loss:1.6058, Validation Accuracy:0.2282\n",
    "Epoch #232: Loss:1.5891, Accuracy:0.2538, Validation Loss:1.6043, Validation Accuracy:0.2299\n",
    "Epoch #233: Loss:1.5894, Accuracy:0.2534, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #234: Loss:1.5898, Accuracy:0.2575, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #235: Loss:1.5886, Accuracy:0.2579, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #236: Loss:1.5869, Accuracy:0.2604, Validation Loss:1.6040, Validation Accuracy:0.2381\n",
    "Epoch #237: Loss:1.5867, Accuracy:0.2571, Validation Loss:1.6023, Validation Accuracy:0.2414\n",
    "Epoch #238: Loss:1.5872, Accuracy:0.2559, Validation Loss:1.6020, Validation Accuracy:0.2397\n",
    "Epoch #239: Loss:1.5884, Accuracy:0.2604, Validation Loss:1.6004, Validation Accuracy:0.2447\n",
    "Epoch #240: Loss:1.5895, Accuracy:0.2579, Validation Loss:1.6012, Validation Accuracy:0.2397\n",
    "Epoch #241: Loss:1.5908, Accuracy:0.2550, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #242: Loss:1.5892, Accuracy:0.2550, Validation Loss:1.6042, Validation Accuracy:0.2348\n",
    "Epoch #243: Loss:1.5885, Accuracy:0.2591, Validation Loss:1.6030, Validation Accuracy:0.2397\n",
    "Epoch #244: Loss:1.5893, Accuracy:0.2522, Validation Loss:1.6024, Validation Accuracy:0.2365\n",
    "Epoch #245: Loss:1.5875, Accuracy:0.2591, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #246: Loss:1.5875, Accuracy:0.2563, Validation Loss:1.6070, Validation Accuracy:0.2479\n",
    "Epoch #247: Loss:1.5846, Accuracy:0.2632, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #248: Loss:1.5848, Accuracy:0.2595, Validation Loss:1.6073, Validation Accuracy:0.2414\n",
    "Epoch #249: Loss:1.5848, Accuracy:0.2608, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #250: Loss:1.5846, Accuracy:0.2608, Validation Loss:1.6076, Validation Accuracy:0.2381\n",
    "Epoch #251: Loss:1.5860, Accuracy:0.2559, Validation Loss:1.6099, Validation Accuracy:0.2102\n",
    "Epoch #252: Loss:1.5854, Accuracy:0.2587, Validation Loss:1.6083, Validation Accuracy:0.2266\n",
    "Epoch #253: Loss:1.5853, Accuracy:0.2501, Validation Loss:1.6097, Validation Accuracy:0.2332\n",
    "Epoch #254: Loss:1.5848, Accuracy:0.2489, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #255: Loss:1.5847, Accuracy:0.2616, Validation Loss:1.6059, Validation Accuracy:0.2315\n",
    "Epoch #256: Loss:1.5849, Accuracy:0.2608, Validation Loss:1.6038, Validation Accuracy:0.2299\n",
    "Epoch #257: Loss:1.5870, Accuracy:0.2563, Validation Loss:1.6087, Validation Accuracy:0.2266\n",
    "Epoch #258: Loss:1.5877, Accuracy:0.2657, Validation Loss:1.6079, Validation Accuracy:0.2348\n",
    "Epoch #259: Loss:1.5856, Accuracy:0.2587, Validation Loss:1.6072, Validation Accuracy:0.2266\n",
    "Epoch #260: Loss:1.5860, Accuracy:0.2546, Validation Loss:1.6050, Validation Accuracy:0.2397\n",
    "Epoch #261: Loss:1.5870, Accuracy:0.2645, Validation Loss:1.6027, Validation Accuracy:0.2447\n",
    "Epoch #262: Loss:1.5868, Accuracy:0.2480, Validation Loss:1.6028, Validation Accuracy:0.2365\n",
    "Epoch #263: Loss:1.5867, Accuracy:0.2468, Validation Loss:1.6017, Validation Accuracy:0.2447\n",
    "Epoch #264: Loss:1.5857, Accuracy:0.2669, Validation Loss:1.5998, Validation Accuracy:0.2447\n",
    "Epoch #265: Loss:1.5861, Accuracy:0.2608, Validation Loss:1.6017, Validation Accuracy:0.2463\n",
    "Epoch #266: Loss:1.5854, Accuracy:0.2571, Validation Loss:1.6033, Validation Accuracy:0.2447\n",
    "Epoch #267: Loss:1.5848, Accuracy:0.2628, Validation Loss:1.6023, Validation Accuracy:0.2447\n",
    "Epoch #268: Loss:1.5838, Accuracy:0.2612, Validation Loss:1.6025, Validation Accuracy:0.2381\n",
    "Epoch #269: Loss:1.5824, Accuracy:0.2690, Validation Loss:1.6063, Validation Accuracy:0.2430\n",
    "Epoch #270: Loss:1.5818, Accuracy:0.2661, Validation Loss:1.6026, Validation Accuracy:0.2463\n",
    "Epoch #271: Loss:1.5829, Accuracy:0.2657, Validation Loss:1.6038, Validation Accuracy:0.2447\n",
    "Epoch #272: Loss:1.5810, Accuracy:0.2702, Validation Loss:1.6065, Validation Accuracy:0.2365\n",
    "Epoch #273: Loss:1.5808, Accuracy:0.2657, Validation Loss:1.6015, Validation Accuracy:0.2397\n",
    "Epoch #274: Loss:1.5823, Accuracy:0.2571, Validation Loss:1.6026, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5839, Accuracy:0.2678, Validation Loss:1.6066, Validation Accuracy:0.2299\n",
    "Epoch #276: Loss:1.5808, Accuracy:0.2645, Validation Loss:1.6068, Validation Accuracy:0.2381\n",
    "Epoch #277: Loss:1.5821, Accuracy:0.2641, Validation Loss:1.6092, Validation Accuracy:0.2414\n",
    "Epoch #278: Loss:1.5837, Accuracy:0.2657, Validation Loss:1.6042, Validation Accuracy:0.2447\n",
    "Epoch #279: Loss:1.5826, Accuracy:0.2632, Validation Loss:1.6048, Validation Accuracy:0.2447\n",
    "Epoch #280: Loss:1.5823, Accuracy:0.2669, Validation Loss:1.6060, Validation Accuracy:0.2365\n",
    "Epoch #281: Loss:1.5803, Accuracy:0.2661, Validation Loss:1.6054, Validation Accuracy:0.2414\n",
    "Epoch #282: Loss:1.5800, Accuracy:0.2653, Validation Loss:1.6057, Validation Accuracy:0.2414\n",
    "Epoch #283: Loss:1.5813, Accuracy:0.2591, Validation Loss:1.6082, Validation Accuracy:0.2397\n",
    "Epoch #284: Loss:1.5863, Accuracy:0.2538, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #285: Loss:1.5862, Accuracy:0.2587, Validation Loss:1.6057, Validation Accuracy:0.2233\n",
    "Epoch #286: Loss:1.5808, Accuracy:0.2637, Validation Loss:1.6038, Validation Accuracy:0.2184\n",
    "Epoch #287: Loss:1.5841, Accuracy:0.2620, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #288: Loss:1.5834, Accuracy:0.2641, Validation Loss:1.6008, Validation Accuracy:0.2266\n",
    "Epoch #289: Loss:1.5822, Accuracy:0.2637, Validation Loss:1.5990, Validation Accuracy:0.2463\n",
    "Epoch #290: Loss:1.5824, Accuracy:0.2637, Validation Loss:1.5991, Validation Accuracy:0.2397\n",
    "Epoch #291: Loss:1.5831, Accuracy:0.2616, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #292: Loss:1.5821, Accuracy:0.2620, Validation Loss:1.6019, Validation Accuracy:0.2282\n",
    "Epoch #293: Loss:1.5823, Accuracy:0.2616, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #294: Loss:1.5824, Accuracy:0.2591, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #295: Loss:1.5834, Accuracy:0.2698, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #296: Loss:1.5825, Accuracy:0.2649, Validation Loss:1.6026, Validation Accuracy:0.2365\n",
    "Epoch #297: Loss:1.5815, Accuracy:0.2616, Validation Loss:1.6013, Validation Accuracy:0.2463\n",
    "Epoch #298: Loss:1.5819, Accuracy:0.2653, Validation Loss:1.5996, Validation Accuracy:0.2447\n",
    "Epoch #299: Loss:1.5809, Accuracy:0.2645, Validation Loss:1.6003, Validation Accuracy:0.2529\n",
    "Epoch #300: Loss:1.5800, Accuracy:0.2678, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60100973, Accuracy:0.2365\n",
    "Labels: ['03', '04', '05', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03  04  05  01  02\n",
    "t:03  27  10  69   9   0\n",
    "t:04  17  15  62  18   0\n",
    "t:05  35   9  90   8   0\n",
    "t:01  27  10  77  12   0\n",
    "t:02  17  12  69  16   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.22      0.23      0.23       115\n",
    "          04       0.27      0.13      0.18       112\n",
    "          05       0.25      0.63      0.35       142\n",
    "          01       0.19      0.10      0.13       126\n",
    "          02       0.00      0.00      0.00       114\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.18      0.22      0.18       609\n",
    "weighted avg       0.19      0.24      0.18       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 23:45:42 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6092353452406885, 1.6091318792114508, 1.6075952851713584, 1.606894110028184, 1.6059715174297589, 1.6054915849406928, 1.6048951391711808, 1.6048754424297165, 1.604326020320648, 1.6038689656406397, 1.6039309274582636, 1.603803754831574, 1.6039627395044211, 1.6037126391979273, 1.6033916526240082, 1.6032962648347877, 1.6028316361563546, 1.602608849849607, 1.602574695507294, 1.6012150804789, 1.601144728010707, 1.6013996141101732, 1.601218795932964, 1.6011598100411677, 1.600452121255433, 1.6005173032898425, 1.6002011798285498, 1.600388609325553, 1.6000346088252828, 1.5997933096486359, 1.5998831353164071, 1.5995946834827293, 1.599796030125986, 1.5995213482375998, 1.5994019408531377, 1.5997940411512879, 1.598887894345426, 1.6002166533509303, 1.6002409311350931, 1.604156275492388, 1.602999966328563, 1.603857100890775, 1.6050779394719792, 1.6049167736019017, 1.6027753104521527, 1.6043971307171976, 1.6024784223590969, 1.6016239417206086, 1.6014682633927695, 1.6007547357007983, 1.6004554293621545, 1.6002058004119322, 1.6003070652778513, 1.6001745243182128, 1.6001634488160583, 1.6002257353762297, 1.5998482874461584, 1.5995237046274646, 1.5993708937821913, 1.5994946948804682, 1.5994060274415416, 1.5997876020879385, 1.5994163888624344, 1.5992576876297373, 1.599505588730372, 1.599821647204006, 1.599847972490909, 1.6009711336424002, 1.6009536776049385, 1.6004744096734058, 1.600462482089088, 1.599610534599066, 1.599964840267288, 1.6000591146534886, 1.6000910825134302, 1.6002155250712178, 1.600232345521548, 1.6001282217858852, 1.6005752092511782, 1.6000937477904196, 1.599396483260031, 1.5989639651403442, 1.598897782452588, 1.5984181960423787, 1.598633504657714, 1.5987629970697739, 1.599229653872097, 1.5983575474843994, 1.5971818289341793, 1.5974794853301275, 1.5991583485125713, 1.5985087665235271, 1.5992579804656932, 1.5997530709346528, 1.5987392773573426, 1.5984420210661363, 1.5995511867729901, 1.5994161007243817, 1.5995326324049475, 1.5979975332767504, 1.5976727094947802, 1.5978378613398385, 1.5979164200659064, 1.5990346537043505, 1.599227137557783, 1.5983219228941818, 1.5973785863134073, 1.597603557732305, 1.5991379140045843, 1.5986519298334232, 1.600272800339071, 1.599583172445814, 1.6005580164920326, 1.599003114136569, 1.5991477749030578, 1.5989835465874382, 1.5982759225936163, 1.5984691242474836, 1.5984715268333949, 1.5985153578772333, 1.5982985643330465, 1.5981797077777156, 1.5981285699286876, 1.5988778048156713, 1.597524306457031, 1.597113817587666, 1.5966664503752108, 1.597649409657433, 1.5974124760071828, 1.5981409563415352, 1.5976581459953672, 1.5963487558568443, 1.5971093829629457, 1.5963678007642623, 1.596254806017445, 1.5968611845241978, 1.596446917562062, 1.595684016866637, 1.597008271561859, 1.5980889853977023, 1.5959721206639983, 1.5982020928941925, 1.5987237096811555, 1.5978384524926372, 1.5940771294736313, 1.5957086205678228, 1.594373760356496, 1.596975720574703, 1.597235059503264, 1.5960731713838374, 1.5961253378778844, 1.5971127726957324, 1.5969601061152316, 1.596089199650268, 1.5957462231709647, 1.597085743506358, 1.5962919055534701, 1.5979935463230401, 1.5970695016810852, 1.5959547409674608, 1.5983051363079028, 1.598058410466011, 1.5975851969570165, 1.5999397111839457, 1.6009927418431624, 1.5982903621858369, 1.5991493257982978, 1.5972179088294995, 1.597114306365328, 1.5989316510057998, 1.5980680335331432, 1.5985816540976463, 1.5991731537582448, 1.599547944241165, 1.5990079674618976, 1.59938187023689, 1.5994767259885916, 1.6000554442209955, 1.5996094803113263, 1.600238117482666, 1.599981114194898, 1.5992319486019841, 1.5991263642099691, 1.5985952116585718, 1.5985488292618926, 1.5982521639277392, 1.5985582023614342, 1.598863072191749, 1.6007349872824006, 1.60043452525961, 1.6009001540041519, 1.599767881661213, 1.602514927610388, 1.6009123804925502, 1.6016926066628818, 1.6028436328389961, 1.6039173669611488, 1.6041016987783372, 1.6041845854475776, 1.604697561029143, 1.604272371638193, 1.6030745052155995, 1.6040381639461798, 1.603677735148588, 1.6042962825944271, 1.602174480168886, 1.6031826106198315, 1.6030081690630107, 1.6039812339742, 1.6046509815162822, 1.604488662115263, 1.6063746593660126, 1.603806369018868, 1.6032819368177642, 1.6060368539077308, 1.604502378230416, 1.606260529488374, 1.6071720479548663, 1.606515992059692, 1.6039903829446176, 1.603755688628148, 1.603042629160513, 1.60297846911576, 1.6032497904375074, 1.602607630939515, 1.602878666472161, 1.6050242500743648, 1.6037841675121014, 1.603653535858555, 1.604738776515466, 1.6057568600612322, 1.6042759907852449, 1.605207223218846, 1.6035550047807114, 1.6035957350127998, 1.6040489397612698, 1.6022601448647886, 1.6019843650373136, 1.600374269759518, 1.6011806517007512, 1.6049266245173313, 1.6042258649428294, 1.602970199436194, 1.6023792406216826, 1.6032740474725984, 1.6070286448561695, 1.6067986212340482, 1.6072934208245113, 1.6075531590748302, 1.607648018741451, 1.6098528497520534, 1.6082917046664384, 1.6097202892178188, 1.606666381331696, 1.6059018537915986, 1.603810556807933, 1.608742741332657, 1.6078671870756227, 1.6071532996025775, 1.6049869107495387, 1.6027264091964621, 1.6028005648129091, 1.6016931447685254, 1.5997695987447729, 1.6017262215293295, 1.6033043260449062, 1.602327847324177, 1.6025149297635934, 1.6062881153792583, 1.6026117948475729, 1.6038203454761475, 1.6064582841932675, 1.6014640601397736, 1.602620385942005, 1.6066117069404113, 1.6068158049888799, 1.609168641868679, 1.6042307611365232, 1.604752364417015, 1.606036482577645, 1.6053793352029986, 1.6057210410952765, 1.6082273154031663, 1.6028401019733722, 1.6056640661017256, 1.6037967565220173, 1.6008030510888311, 1.600797546908186, 1.599007414283815, 1.5991121240828816, 1.6003783350116123, 1.6018503407147913, 1.601443467469051, 1.6007742094876143, 1.5999478798585964, 1.6025996131849993, 1.601287456568826, 1.599550070042289, 1.6002974193084416, 1.6010095942000842], 'val_acc': [0.20689655091668585, 0.23316912959851263, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23645320184810212, 0.2397372740976916, 0.23316912969638562, 0.23645320184810212, 0.2463054185968706, 0.24302134634728112, 0.2479474548195383, 0.24466338256994882, 0.2479474548195383, 0.2479474548195383, 0.2479474548195383, 0.2512315270691278, 0.24958949094433308, 0.2512315270691278, 0.2479474548195383, 0.24137931022248635, 0.24958949094433308, 0.251231524793581, 0.24958948866878627, 0.23316912969638562, 0.23481116582118036, 0.23316912969638562, 0.23481116572330737, 0.2397372741955646, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.24794745472166535, 0.24466338247207586, 0.2512315270691278, 0.2397372740976916, 0.24958949094433308, 0.2479474548195383, 0.2479474548195383, 0.2479474548195383, 0.2463054185968706, 0.2528735630960496, 0.2479474548195383, 0.2479474548195383, 0.2577996714704338, 0.2479474548195383, 0.2528735630960496, 0.2479474548195383, 0.24958949094433308, 0.25123152697125484, 0.24794745462379236, 0.2545155993187173, 0.2545155992208443, 0.25287356319392257, 0.25123152697125484, 0.2495894908464601, 0.2512315270691278, 0.24958949094433308, 0.25615763534563907, 0.2545155993187173, 0.25287356319392257, 0.25615763544351206, 0.25287356319392257, 0.2512315270691278, 0.2545155993187173, 0.25615763534563907, 0.25287356319392257, 0.2495894908464601, 0.2495894908464601, 0.2577996714704338, 0.2577996714704338, 0.2577996714704338, 0.25944170749735557, 0.2610837436221503, 0.25123152677550886, 0.2463054185968706, 0.2561576350520201, 0.25451559892722536, 0.25615763534563907, 0.24958949065071412, 0.2528735628024306, 0.2561576351498931, 0.25779967127468784, 0.2413793099288674, 0.25779967127468784, 0.2528735630960496, 0.2577996713725608, 0.23152709318009895, 0.25944170749735557, 0.25944170749735557, 0.25451559902509835, 0.2577996713725608, 0.25451559902509835, 0.2463054185968706, 0.25123152697125484, 0.26272577994269103, 0.2643678159696128, 0.24794745472166535, 0.2528735629003036, 0.2528735629003036, 0.2545155992208443, 0.25944170769310154, 0.25615763534563907, 0.2446633822763299, 0.23973727380407267, 0.2528735629003036, 0.2413793101246134, 0.24794745472166535, 0.25944170769310154, 0.2446633822763299, 0.24466338237420288, 0.2446633822763299, 0.24466338247207586, 0.2397372741955646, 0.2495894907485871, 0.23973727399981865, 0.2298850571531772, 0.23481116552756143, 0.22495894877879294, 0.22660098490358768, 0.22660098490358768, 0.2282430211262554, 0.22988505725105016, 0.21839080427961396, 0.23316912940276668, 0.22331691275187118, 0.24302134615153514, 0.22495894887666593, 0.2331691292070207, 0.24794745462379236, 0.23973727399981865, 0.23973727390194566, 0.23973727399981865, 0.24630541840112463, 0.2561576352477661, 0.2545155992208443, 0.24466338247207586, 0.2495894908464601, 0.2545155992208443, 0.2495894908464601, 0.25123152697125484, 0.2512315270691278, 0.2512315270691278, 0.25615763534563907, 0.24958949094433308, 0.2545155992208443, 0.2495894907485871, 0.2545155970431705, 0.25944170749735557, 0.24958949094433308, 0.2577996715683068, 0.25451559912297134, 0.25615763326583824, 0.251231524793581, 0.23973727380407267, 0.2512315270691278, 0.2512315270691278, 0.24302134605366216, 0.2463054185968706, 0.2512315270691278, 0.2512315270691278, 0.24958949055284116, 0.23809523758140494, 0.24137931032035934, 0.24137930983099445, 0.2495894908464601, 0.24630541849899762, 0.24794745452591938, 0.2495894907485871, 0.24958949094433308, 0.24794745462379236, 0.24630541849899762, 0.23481116572330737, 0.25123152697125484, 0.2463054185968706, 0.2463054164191968, 0.24958948866878627, 0.24466338256994882, 0.2495894908464601, 0.2397372740976916, 0.24794745472166535, 0.23973727399981865, 0.24958949094433308, 0.24302134644515408, 0.2348111656254344, 0.2397372740976916, 0.23316912959851263, 0.2397372740976916, 0.22988505734892314, 0.23645320175022916, 0.24302134634728112, 0.2364532019459751, 0.23973727399981865, 0.23645320175022916, 0.2380952378750239, 0.2282430211262554, 0.2348111656254344, 0.2348111656254344, 0.23809523579522307, 0.23809523797289686, 0.22824301904645458, 0.22495894887666593, 0.2413793101246134, 0.2298850570553042, 0.23316912959851263, 0.22660098480571472, 0.23316912950063964, 0.23645320184810212, 0.23316912950063964, 0.23316912969638562, 0.2282430212241284, 0.23481116552756143, 0.22988505734892314, 0.2397372719200178, 0.22660098519720664, 0.22824302093050947, 0.22988505744679613, 0.2282430213220014, 0.23481116582118036, 0.23481116582118036, 0.23809523797289686, 0.24137931022248635, 0.2397372741955646, 0.24466338029440204, 0.2397372741955646, 0.2331691293048937, 0.23481116572330737, 0.2397372740976916, 0.2364532019459751, 0.23316912959851263, 0.2479474548195383, 0.23316912969638562, 0.24137931032035934, 0.23645320175022916, 0.2380952378750239, 0.2101806216737124, 0.22660098480571472, 0.23316912940276668, 0.22824302093050947, 0.2315270934737179, 0.2298850553669953, 0.22660098490358768, 0.23481116572330737, 0.2266009832152788, 0.2397372741955646, 0.24466338256994882, 0.23645319996404726, 0.24466338256994882, 0.24466338029440204, 0.2463054164191968, 0.24466338256994882, 0.24466338029440204, 0.23809523807076985, 0.24302134634728112, 0.24630541869474357, 0.24466338247207586, 0.23645320184810212, 0.23973727399981865, 0.23481116552756143, 0.2298850571531772, 0.23809523807076985, 0.24137931022248635, 0.24466338247207586, 0.24466338237420288, 0.23645320184810212, 0.24137931022248635, 0.24137931022248635, 0.2397372740976916, 0.24302134644515408, 0.22331691245825225, 0.21839080408386802, 0.24137931022248635, 0.22660098460996875, 0.24630541869474357, 0.2397372740976916, 0.24137931032035934, 0.22824302093050947, 0.23645320184810212, 0.23973727399981865, 0.23809523797289686, 0.23645320184810212, 0.2463054164191968, 0.24466338247207586, 0.25287356101624875, 0.23645320165235617], 'loss': [1.6105930945711704, 1.6091070405756425, 1.6084231598421288, 1.6072912606370522, 1.6066993004487524, 1.606125232032682, 1.6052617793210477, 1.6048675551306786, 1.604662033321921, 1.60429595221729, 1.603915823264778, 1.604195289347451, 1.6040846743867627, 1.603740917143146, 1.603749158592929, 1.6035319640161565, 1.60342341091109, 1.60298991678187, 1.6028468169226049, 1.6029356040014624, 1.6028639156960365, 1.603055400084666, 1.6029244303948091, 1.602393199971569, 1.6023611910778883, 1.6019238907208921, 1.6015349227544953, 1.6013965183704542, 1.6016277793986107, 1.6010004687358221, 1.6008516511388382, 1.601000276434348, 1.601062240541838, 1.6006111615492333, 1.6002907146418608, 1.600267558763649, 1.5999558289682596, 1.5997854696406966, 1.5997520497202629, 1.5997486989356164, 1.6063105363865409, 1.6032753434269336, 1.6028984769903414, 1.604263635435633, 1.6025385086541304, 1.6028069885849219, 1.6030692354120024, 1.6016823642307727, 1.6012910932486062, 1.601093891367041, 1.601010760291646, 1.6008373542977554, 1.6007769220418753, 1.6008945419068699, 1.601025232348354, 1.6003962117298918, 1.6004833466708048, 1.6004388965375613, 1.6002273875340298, 1.6002064741122894, 1.60047610706862, 1.6000897078053906, 1.5999005090039857, 1.5998644457705458, 1.6003015409994419, 1.5999216403314955, 1.5993300227903489, 1.5997836385664264, 1.599733501782897, 1.5989240724692844, 1.5996381561858943, 1.599663199389495, 1.6003150238393513, 1.5999467546200605, 1.5998870055533532, 1.6002032312279608, 1.5998150083808194, 1.5995923798676634, 1.5991016971012406, 1.5992909458873208, 1.599521553442953, 1.6000609829685282, 1.6000283664256885, 1.599631401449748, 1.5994860240321385, 1.5994522630311625, 1.5991214253329644, 1.5987285097032111, 1.5995606418507788, 1.5998700877234677, 1.5992373712253767, 1.5990388503064854, 1.5999902909296495, 1.5986256941143246, 1.5982688065182258, 1.5984818887416832, 1.5975446352478904, 1.598069867852777, 1.5972808983781255, 1.5974516652203195, 1.5973255514853788, 1.5967339859361276, 1.5966980549833858, 1.5959893472875168, 1.5961364882682627, 1.5957189833603844, 1.595675646793671, 1.5964842174087461, 1.5978446209455173, 1.600443714404253, 1.5960457813078863, 1.5982156860754966, 1.5976415430985438, 1.5965409803684243, 1.597016520960375, 1.5956730143979834, 1.5961578383338035, 1.594845555499349, 1.5941702303210812, 1.5933335912301065, 1.593875959423778, 1.594421579607703, 1.5939281697145968, 1.5927347450040938, 1.5933032249278356, 1.5923740688045902, 1.5924793718776664, 1.5930757378650642, 1.5929723504632405, 1.5940246965361327, 1.5932949836249224, 1.5948945988375058, 1.59645818924757, 1.5949846589834538, 1.5936805585571383, 1.593977626784871, 1.5937786438626675, 1.5942536477925107, 1.5936509649856379, 1.5936195665316415, 1.5974734637281978, 1.5954170992731804, 1.5929989068659913, 1.5959223959725006, 1.5961136311721018, 1.5948889319656812, 1.5965140278579273, 1.5973818885227493, 1.596518760099548, 1.596530401584304, 1.596286727172883, 1.5959844364033098, 1.595571100834214, 1.5956351746279112, 1.5953130716660673, 1.595035778570469, 1.5957065316441124, 1.5944119294321268, 1.595195051140364, 1.5956447227290034, 1.5941206674066657, 1.5940089765270633, 1.5931640331260477, 1.5970561934692413, 1.5963859193378895, 1.597810235160577, 1.595746918576454, 1.5949061283095907, 1.595466882100585, 1.5954665500770115, 1.5947950956513015, 1.5947088881439742, 1.5936057399185777, 1.5939912346109473, 1.5936238003462493, 1.5933356642967866, 1.5929332516766181, 1.59287211708954, 1.5926117725196072, 1.592332753557444, 1.5930223613793846, 1.5924827974190212, 1.5918897245454102, 1.5923268418537273, 1.591544994093799, 1.591668865127485, 1.5913759266816125, 1.591289760444688, 1.5910758319087097, 1.591680746597431, 1.5914257233637314, 1.5904260196724955, 1.5905522295092165, 1.5906225919233945, 1.5896993433915125, 1.5897464255287908, 1.5895729375815735, 1.589365073444907, 1.5892369354530036, 1.5902746855111092, 1.5903322981613128, 1.589484159265945, 1.5914692683386362, 1.5894588172313369, 1.590033148103671, 1.5891595997604746, 1.589452130251107, 1.5897500600168593, 1.589453195791225, 1.5894667427642635, 1.590793713553975, 1.5884463064969199, 1.5886572419740337, 1.590750122119269, 1.5883963204996787, 1.5875257218398109, 1.5890278654176841, 1.590081041598467, 1.5926806497867592, 1.5907301570845336, 1.5905310330204896, 1.5903530384725615, 1.5896421234220939, 1.58943094394535, 1.5891651645578153, 1.589635146814695, 1.5900418108738423, 1.5908800503556488, 1.5890565627899014, 1.5887299312458392, 1.5894307153180884, 1.5890553146417137, 1.5893635150098702, 1.5898098568651955, 1.5885955935386171, 1.586914146901156, 1.5867208464679288, 1.5871983270625558, 1.5884412178023886, 1.5895349906944887, 1.5908299338891032, 1.5891845118093784, 1.5884934975626044, 1.5893448320502372, 1.5874602766741963, 1.5874662621554898, 1.5845854046897967, 1.5847900323554476, 1.5847884934540892, 1.5846045478903048, 1.5860037328281442, 1.5854055289615105, 1.5853318299600965, 1.5848440765110618, 1.5847136914118114, 1.5849066641786016, 1.5870466271465073, 1.5876726225416273, 1.5856234021255369, 1.5859519715181856, 1.5870478061190376, 1.5867709340011316, 1.586732663266223, 1.5856788647003488, 1.5860569344408948, 1.585357529769443, 1.5848351850157156, 1.5838364835637306, 1.5823782626608314, 1.5818281852244351, 1.5828841427268434, 1.5809898688807869, 1.5807873320041006, 1.582314559762238, 1.5839220353465306, 1.5808238792713174, 1.5821170941025815, 1.583742897319598, 1.582595989346749, 1.5823290751455257, 1.5803052984468746, 1.5800041090046846, 1.5812862189153871, 1.5863146321729469, 1.5861706094330585, 1.580816258052536, 1.5840532760110968, 1.5834285758114448, 1.582162436518581, 1.5823973411407315, 1.5830886681221839, 1.582144602955734, 1.5822928066371158, 1.582427722766414, 1.5834150083745528, 1.5825217714055118, 1.581464996719752, 1.5818662852477243, 1.5809030916656557, 1.5800436830128977], 'acc': [0.20780287578977355, 0.19876796662562682, 0.23285420931707418, 0.23285420929871545, 0.2328542083195837, 0.23285420851541005, 0.23285421029620593, 0.2328542094945418, 0.2328542091028891, 0.2328542091028891, 0.23285420970872686, 0.23285420892542147, 0.23285420990455322, 0.23285420931707418, 0.23285420929871545, 0.23285421029620593, 0.23285421010037957, 0.23285421086532623, 0.23285420851541005, 0.23285421051039099, 0.2328542094945418, 0.23860369614751922, 0.23696098628107773, 0.23285420969036816, 0.23778233993224784, 0.2381930185478081, 0.23983572941174008, 0.24353182854348873, 0.24312115016047225, 0.24106776244341716, 0.24106776207012318, 0.24147843888652887, 0.2418891168962514, 0.24229979251934028, 0.24147843927818158, 0.24147843988401935, 0.24353182817019475, 0.2373716638807888, 0.2357289532126832, 0.23655030766551743, 0.2324435330514301, 0.23408624313205664, 0.23285420951290053, 0.23490759819072865, 0.2328542087112364, 0.23326488632930623, 0.23285420951290053, 0.23572895203772512, 0.2365503090546606, 0.23942505218532295, 0.2361396298516213, 0.23942505157948518, 0.24065708404204195, 0.24065708445205336, 0.23901437533219982, 0.242710471544912, 0.2381930189211021, 0.24229979431849485, 0.2410677630308962, 0.23983572939338135, 0.23737166233253676, 0.23860369516838748, 0.23490759797654356, 0.2402464058364931, 0.2336755637515497, 0.23490759758489088, 0.24024640603231942, 0.2406570850211737, 0.2418891173062628, 0.24271047271987006, 0.23490759738906453, 0.23983572923427246, 0.2402464070298099, 0.23572895344522699, 0.23696098489193457, 0.23860369636170428, 0.23737166290165707, 0.24106776285342857, 0.23655030846718156, 0.23942505295026964, 0.23942505236279057, 0.24229979666841103, 0.23819301950858116, 0.24065708322201912, 0.242710471544912, 0.23983572978503406, 0.2369609854794136, 0.23285420990455322, 0.2303901437861229, 0.24147843869070254, 0.24229979472850627, 0.24024640683398354, 0.2344969195935271, 0.24229979512015898, 0.24106776128681778, 0.24065708327709526, 0.24024640603231942, 0.23613963004744762, 0.24229979512015898, 0.24188911709207775, 0.24065708327709526, 0.24024640622814578, 0.2422997945326799, 0.2398357286284347, 0.24188911650459868, 0.2443531837979871, 0.241478440648966, 0.2369609866727304, 0.22956878758921026, 0.2373716637033212, 0.24517453883830037, 0.23901437591967886, 0.24558521528141208, 0.2402464066381572, 0.24640656991171397, 0.24887063601178555, 0.24476386041856643, 0.2422997945143212, 0.24312114957299322, 0.24558521526305338, 0.24188911750208916, 0.24024640622814578, 0.23942505179367027, 0.24681724792143647, 0.24640657030336666, 0.24312115033793988, 0.24147843849487619, 0.24271047215074973, 0.24188911670042504, 0.24722792573533264, 0.24229979373101582, 0.2418891180895682, 0.24188911490127046, 0.240657084256227, 0.24599589348696096, 0.24188911789374185, 0.24353182817019475, 0.2447638594394347, 0.24476385924360836, 0.24435318240884393, 0.24024640544484038, 0.2459958928811232, 0.249281313434029, 0.2505133490298073, 0.24722792534367993, 0.2377823387205723, 0.24394250617991728, 0.244353183583802, 0.24763860335340246, 0.2377823409113796, 0.23737166485992056, 0.24804927897649134, 0.24722792432783075, 0.24476386041856643, 0.25010266745849313, 0.24928131460898711, 0.25092402429796096, 0.2529774127799628, 0.2476386031575761, 0.25010266827851596, 0.2517453809232438, 0.25626283372452124, 0.252566735749372, 0.2414784389048876, 0.24845995917702113, 0.24312115033793988, 0.25585215571479875, 0.24928131304237633, 0.2501026692576477, 0.24928131306073503, 0.2447638610244042, 0.24969199124792518, 0.25010266984512675, 0.25338809039803256, 0.2517453811007114, 0.24845995819788938, 0.25503080184944354, 0.25256673437858757, 0.2529774125841364, 0.24681724911475328, 0.24763860531166593, 0.2505133472673702, 0.2501026692576477, 0.2554414779009026, 0.25749486794951515, 0.2509240235146556, 0.2587269011953773, 0.2533880898105535, 0.25831622142321764, 0.25708418995815135, 0.25954825564821155, 0.25585215649810417, 0.2583162225981757, 0.25872689825798206, 0.25790554537175864, 0.2624229976039158, 0.2648870648789455, 0.25749486599125165, 0.2603696118634829, 0.25708418798152916, 0.2591375764635309, 0.2529774125841364, 0.2521560583454872, 0.25544147850674037, 0.2624229961964139, 0.2570841887648346, 0.2579055432176688, 0.25831622142321764, 0.26283367659277007, 0.26036960928102293, 0.2599589334621077, 0.25954825408160076, 0.26036960928102293, 0.25790554380514785, 0.2579055447842796, 0.26324435518997163, 0.2599589338537604, 0.2652977409304045, 0.26488706448729277, 0.2673511270257727, 0.26365503202473606, 0.25708418892394347, 0.2607802888940737, 0.2611909629505518, 0.2640657078803687, 0.2579055445884532, 0.257905545194291, 0.25215605854131357, 0.2562628333328686, 0.25585215806471495, 0.25626283372452124, 0.2537987686219401, 0.2533880913771643, 0.2574948673620361, 0.2579055436093215, 0.26036961010104576, 0.25708418954813994, 0.2558521566939305, 0.2603696083018912, 0.25790554302184243, 0.2550308000870064, 0.2550308014577909, 0.2591375784217944, 0.25215605969791294, 0.25913757763848905, 0.2562628327453895, 0.2632443526442291, 0.2595482558440379, 0.2607802863116137, 0.2607802886982473, 0.25585215610645146, 0.2587269000204192, 0.25010266945347404, 0.2488706367583735, 0.2616016427594289, 0.2607802873458216, 0.25626283352869494, 0.2657084191543121, 0.2587269000204192, 0.25462012184474014, 0.26447638569426485, 0.24804928116729863, 0.246817250467179, 0.26694045296929453, 0.2607802873274629, 0.25708418954813994, 0.2628336746345066, 0.26119096353803084, 0.2689938382813573, 0.26611909554234764, 0.2657084193317797, 0.2702258745013321, 0.26570841991925875, 0.2570841893523136, 0.26776180761795515, 0.2644763872608757, 0.26406570964280585, 0.26570841991925875, 0.2632443508450745, 0.2669404535567736, 0.26611909694984953, 0.2652977424970153, 0.2591375790092735, 0.2537987686035814, 0.25872689825798206, 0.26365503261221507, 0.26201231994912855, 0.26406570862695666, 0.263655031437257, 0.2636550302622989, 0.26160164295525523, 0.2620123192025406, 0.26160164393438695, 0.25913757861762077, 0.2698151945333461, 0.2648870652705982, 0.26160164432603966, 0.26529774034292547, 0.2644763882400074, 0.26776180820543416]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
