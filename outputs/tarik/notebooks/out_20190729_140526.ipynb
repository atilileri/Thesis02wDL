{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf6.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 14:05:26 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '1', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F3BAB34E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F3B33A6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1041, Accuracy:0.3035, Validation Loss:1.0846, Validation Accuracy:0.3777\n",
    "Epoch #2: Loss:1.0788, Accuracy:0.3873, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #3: Loss:1.0736, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0748, Accuracy:0.3959, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0754, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0746, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0740, Accuracy:0.3967, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #8: Loss:1.0739, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #9: Loss:1.0739, Accuracy:0.4000, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #12: Loss:1.0734, Accuracy:0.3979, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #13: Loss:1.0734, Accuracy:0.4000, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0733, Accuracy:0.4012, Validation Loss:1.0733, Validation Accuracy:0.3892\n",
    "Epoch #15: Loss:1.0732, Accuracy:0.4000, Validation Loss:1.0732, Validation Accuracy:0.3892\n",
    "Epoch #16: Loss:1.0732, Accuracy:0.4016, Validation Loss:1.0732, Validation Accuracy:0.3892\n",
    "Epoch #17: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0729, Validation Accuracy:0.3908\n",
    "Epoch #18: Loss:1.0733, Accuracy:0.4021, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #19: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #20: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #21: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0733, Validation Accuracy:0.3859\n",
    "Epoch #22: Loss:1.0731, Accuracy:0.4021, Validation Loss:1.0732, Validation Accuracy:0.3859\n",
    "Epoch #23: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #24: Loss:1.0729, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #25: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #26: Loss:1.0732, Accuracy:0.3955, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #27: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #28: Loss:1.0726, Accuracy:0.4045, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #29: Loss:1.0727, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0729, Accuracy:0.4016, Validation Loss:1.0736, Validation Accuracy:0.3892\n",
    "Epoch #31: Loss:1.0724, Accuracy:0.4041, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #32: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #33: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0728, Validation Accuracy:0.3924\n",
    "Epoch #34: Loss:1.0730, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3793\n",
    "Epoch #35: Loss:1.0759, Accuracy:0.3762, Validation Loss:1.0770, Validation Accuracy:0.3892\n",
    "Epoch #36: Loss:1.0744, Accuracy:0.3832, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #37: Loss:1.0741, Accuracy:0.4000, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0745, Accuracy:0.3963, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #39: Loss:1.0733, Accuracy:0.4025, Validation Loss:1.0724, Validation Accuracy:0.3957\n",
    "Epoch #40: Loss:1.0737, Accuracy:0.3975, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #41: Loss:1.0731, Accuracy:0.3951, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #42: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #43: Loss:1.0725, Accuracy:0.4029, Validation Loss:1.0736, Validation Accuracy:0.3810\n",
    "Epoch #44: Loss:1.0729, Accuracy:0.3938, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #45: Loss:1.0733, Accuracy:0.3914, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0733, Accuracy:0.3897, Validation Loss:1.0747, Validation Accuracy:0.3695\n",
    "Epoch #47: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #48: Loss:1.0727, Accuracy:0.3967, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #49: Loss:1.0726, Accuracy:0.3979, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #50: Loss:1.0720, Accuracy:0.4016, Validation Loss:1.0755, Validation Accuracy:0.3793\n",
    "Epoch #51: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.3777\n",
    "Epoch #52: Loss:1.0722, Accuracy:0.4037, Validation Loss:1.0748, Validation Accuracy:0.3629\n",
    "Epoch #53: Loss:1.0717, Accuracy:0.4123, Validation Loss:1.0742, Validation Accuracy:0.4072\n",
    "Epoch #54: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0739, Validation Accuracy:0.4039\n",
    "Epoch #55: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #56: Loss:1.0716, Accuracy:0.4094, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #57: Loss:1.0715, Accuracy:0.4025, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #58: Loss:1.0723, Accuracy:0.4099, Validation Loss:1.0734, Validation Accuracy:0.4105\n",
    "Epoch #59: Loss:1.0719, Accuracy:0.4012, Validation Loss:1.0737, Validation Accuracy:0.4122\n",
    "Epoch #60: Loss:1.0713, Accuracy:0.4062, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0721, Accuracy:0.4074, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #62: Loss:1.0730, Accuracy:0.4086, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #63: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.4039\n",
    "Epoch #64: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #65: Loss:1.0724, Accuracy:0.4074, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #66: Loss:1.0719, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #67: Loss:1.0720, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #68: Loss:1.0719, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #69: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #70: Loss:1.0715, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0720, Accuracy:0.4053, Validation Loss:1.0752, Validation Accuracy:0.3957\n",
    "Epoch #72: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0751, Validation Accuracy:0.4023\n",
    "Epoch #73: Loss:1.0717, Accuracy:0.4074, Validation Loss:1.0760, Validation Accuracy:0.3908\n",
    "Epoch #74: Loss:1.0726, Accuracy:0.3951, Validation Loss:1.0738, Validation Accuracy:0.3826\n",
    "Epoch #75: Loss:1.0719, Accuracy:0.4086, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #76: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #77: Loss:1.0709, Accuracy:0.4111, Validation Loss:1.0752, Validation Accuracy:0.3793\n",
    "Epoch #78: Loss:1.0707, Accuracy:0.4152, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0710, Accuracy:0.4131, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #80: Loss:1.0709, Accuracy:0.4111, Validation Loss:1.0755, Validation Accuracy:0.3826\n",
    "Epoch #81: Loss:1.0707, Accuracy:0.4041, Validation Loss:1.0753, Validation Accuracy:0.4072\n",
    "Epoch #82: Loss:1.0702, Accuracy:0.4152, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #83: Loss:1.0704, Accuracy:0.4090, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #84: Loss:1.0705, Accuracy:0.4115, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #85: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.3974\n",
    "Epoch #86: Loss:1.0724, Accuracy:0.3979, Validation Loss:1.0741, Validation Accuracy:0.4089\n",
    "Epoch #87: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #88: Loss:1.0732, Accuracy:0.3856, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #89: Loss:1.0721, Accuracy:0.3926, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #90: Loss:1.0711, Accuracy:0.4062, Validation Loss:1.0765, Validation Accuracy:0.4105\n",
    "Epoch #91: Loss:1.0713, Accuracy:0.4066, Validation Loss:1.0754, Validation Accuracy:0.4056\n",
    "Epoch #92: Loss:1.0712, Accuracy:0.3906, Validation Loss:1.0758, Validation Accuracy:0.3924\n",
    "Epoch #93: Loss:1.0716, Accuracy:0.3971, Validation Loss:1.0775, Validation Accuracy:0.4089\n",
    "Epoch #94: Loss:1.0735, Accuracy:0.4103, Validation Loss:1.0770, Validation Accuracy:0.4007\n",
    "Epoch #95: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #96: Loss:1.0723, Accuracy:0.3963, Validation Loss:1.0744, Validation Accuracy:0.4122\n",
    "Epoch #97: Loss:1.0724, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #98: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #99: Loss:1.0715, Accuracy:0.4016, Validation Loss:1.0769, Validation Accuracy:0.3810\n",
    "Epoch #100: Loss:1.0716, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #101: Loss:1.0716, Accuracy:0.4057, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #102: Loss:1.0716, Accuracy:0.3963, Validation Loss:1.0731, Validation Accuracy:0.4056\n",
    "Epoch #103: Loss:1.0712, Accuracy:0.4025, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #104: Loss:1.0715, Accuracy:0.4066, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #105: Loss:1.0716, Accuracy:0.4041, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #106: Loss:1.0719, Accuracy:0.4062, Validation Loss:1.0742, Validation Accuracy:0.4056\n",
    "Epoch #107: Loss:1.0711, Accuracy:0.4029, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #108: Loss:1.0706, Accuracy:0.4115, Validation Loss:1.0758, Validation Accuracy:0.3924\n",
    "Epoch #109: Loss:1.0708, Accuracy:0.4152, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #110: Loss:1.0713, Accuracy:0.4119, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #111: Loss:1.0708, Accuracy:0.4057, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #112: Loss:1.0702, Accuracy:0.4119, Validation Loss:1.0760, Validation Accuracy:0.4007\n",
    "Epoch #113: Loss:1.0705, Accuracy:0.4004, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #114: Loss:1.0699, Accuracy:0.4037, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #115: Loss:1.0703, Accuracy:0.4057, Validation Loss:1.0756, Validation Accuracy:0.3957\n",
    "Epoch #116: Loss:1.0707, Accuracy:0.3988, Validation Loss:1.0757, Validation Accuracy:0.4023\n",
    "Epoch #117: Loss:1.0700, Accuracy:0.3914, Validation Loss:1.0752, Validation Accuracy:0.4072\n",
    "Epoch #118: Loss:1.0693, Accuracy:0.3988, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #119: Loss:1.0693, Accuracy:0.4016, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #120: Loss:1.0690, Accuracy:0.3959, Validation Loss:1.0765, Validation Accuracy:0.3974\n",
    "Epoch #121: Loss:1.0699, Accuracy:0.4021, Validation Loss:1.0768, Validation Accuracy:0.3941\n",
    "Epoch #122: Loss:1.0700, Accuracy:0.4111, Validation Loss:1.0747, Validation Accuracy:0.4007\n",
    "Epoch #123: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0751, Validation Accuracy:0.4023\n",
    "Epoch #124: Loss:1.0703, Accuracy:0.4070, Validation Loss:1.0750, Validation Accuracy:0.4056\n",
    "Epoch #125: Loss:1.0692, Accuracy:0.4090, Validation Loss:1.0755, Validation Accuracy:0.3924\n",
    "Epoch #126: Loss:1.0705, Accuracy:0.4090, Validation Loss:1.0760, Validation Accuracy:0.4089\n",
    "Epoch #127: Loss:1.0698, Accuracy:0.4094, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #128: Loss:1.0696, Accuracy:0.4070, Validation Loss:1.0771, Validation Accuracy:0.3957\n",
    "Epoch #129: Loss:1.0700, Accuracy:0.4160, Validation Loss:1.0766, Validation Accuracy:0.4007\n",
    "Epoch #130: Loss:1.0711, Accuracy:0.4066, Validation Loss:1.0760, Validation Accuracy:0.3974\n",
    "Epoch #131: Loss:1.0722, Accuracy:0.4049, Validation Loss:1.0770, Validation Accuracy:0.3793\n",
    "Epoch #132: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0780, Validation Accuracy:0.3875\n",
    "Epoch #133: Loss:1.0707, Accuracy:0.3971, Validation Loss:1.0772, Validation Accuracy:0.3859\n",
    "Epoch #134: Loss:1.0692, Accuracy:0.4053, Validation Loss:1.0777, Validation Accuracy:0.3875\n",
    "Epoch #135: Loss:1.0683, Accuracy:0.4070, Validation Loss:1.0770, Validation Accuracy:0.4039\n",
    "Epoch #136: Loss:1.0678, Accuracy:0.4082, Validation Loss:1.0776, Validation Accuracy:0.3744\n",
    "Epoch #137: Loss:1.0682, Accuracy:0.4099, Validation Loss:1.0769, Validation Accuracy:0.3957\n",
    "Epoch #138: Loss:1.0688, Accuracy:0.4086, Validation Loss:1.0774, Validation Accuracy:0.4039\n",
    "Epoch #139: Loss:1.0680, Accuracy:0.4025, Validation Loss:1.0774, Validation Accuracy:0.4007\n",
    "Epoch #140: Loss:1.0681, Accuracy:0.4082, Validation Loss:1.0746, Validation Accuracy:0.4072\n",
    "Epoch #141: Loss:1.0697, Accuracy:0.3951, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #142: Loss:1.0698, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #143: Loss:1.0697, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #144: Loss:1.0699, Accuracy:0.3992, Validation Loss:1.0753, Validation Accuracy:0.4089\n",
    "Epoch #145: Loss:1.0704, Accuracy:0.3951, Validation Loss:1.0754, Validation Accuracy:0.3727\n",
    "Epoch #146: Loss:1.0698, Accuracy:0.3979, Validation Loss:1.0768, Validation Accuracy:0.3826\n",
    "Epoch #147: Loss:1.0715, Accuracy:0.4000, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #148: Loss:1.0698, Accuracy:0.4053, Validation Loss:1.0756, Validation Accuracy:0.3826\n",
    "Epoch #149: Loss:1.0696, Accuracy:0.3984, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #150: Loss:1.0701, Accuracy:0.4053, Validation Loss:1.0749, Validation Accuracy:0.3957\n",
    "Epoch #151: Loss:1.0696, Accuracy:0.3979, Validation Loss:1.0762, Validation Accuracy:0.3892\n",
    "Epoch #152: Loss:1.0706, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #153: Loss:1.0688, Accuracy:0.4103, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #154: Loss:1.0689, Accuracy:0.4086, Validation Loss:1.0757, Validation Accuracy:0.3924\n",
    "Epoch #155: Loss:1.0684, Accuracy:0.4103, Validation Loss:1.0760, Validation Accuracy:0.4105\n",
    "Epoch #156: Loss:1.0683, Accuracy:0.4131, Validation Loss:1.0756, Validation Accuracy:0.3957\n",
    "Epoch #157: Loss:1.0696, Accuracy:0.4119, Validation Loss:1.0752, Validation Accuracy:0.3957\n",
    "Epoch #158: Loss:1.0696, Accuracy:0.4074, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #159: Loss:1.0673, Accuracy:0.4152, Validation Loss:1.0759, Validation Accuracy:0.3892\n",
    "Epoch #160: Loss:1.0689, Accuracy:0.3996, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #161: Loss:1.0672, Accuracy:0.4144, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #162: Loss:1.0681, Accuracy:0.4045, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #163: Loss:1.0676, Accuracy:0.4004, Validation Loss:1.0746, Validation Accuracy:0.4023\n",
    "Epoch #164: Loss:1.0666, Accuracy:0.4148, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #165: Loss:1.0659, Accuracy:0.4156, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #166: Loss:1.0662, Accuracy:0.4123, Validation Loss:1.0759, Validation Accuracy:0.3892\n",
    "Epoch #167: Loss:1.0676, Accuracy:0.4136, Validation Loss:1.0773, Validation Accuracy:0.4122\n",
    "Epoch #168: Loss:1.0689, Accuracy:0.3955, Validation Loss:1.0766, Validation Accuracy:0.3892\n",
    "Epoch #169: Loss:1.0684, Accuracy:0.3979, Validation Loss:1.0761, Validation Accuracy:0.4056\n",
    "Epoch #170: Loss:1.0659, Accuracy:0.4099, Validation Loss:1.0768, Validation Accuracy:0.3924\n",
    "Epoch #171: Loss:1.0668, Accuracy:0.4094, Validation Loss:1.0760, Validation Accuracy:0.3924\n",
    "Epoch #172: Loss:1.0654, Accuracy:0.4164, Validation Loss:1.0754, Validation Accuracy:0.4072\n",
    "Epoch #173: Loss:1.0641, Accuracy:0.4086, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #174: Loss:1.0649, Accuracy:0.3967, Validation Loss:1.0779, Validation Accuracy:0.3908\n",
    "Epoch #175: Loss:1.0657, Accuracy:0.4107, Validation Loss:1.0775, Validation Accuracy:0.4056\n",
    "Epoch #176: Loss:1.0660, Accuracy:0.4004, Validation Loss:1.0790, Validation Accuracy:0.3941\n",
    "Epoch #177: Loss:1.0717, Accuracy:0.4041, Validation Loss:1.0804, Validation Accuracy:0.3810\n",
    "Epoch #178: Loss:1.0669, Accuracy:0.4033, Validation Loss:1.0822, Validation Accuracy:0.3957\n",
    "Epoch #179: Loss:1.0659, Accuracy:0.4070, Validation Loss:1.0827, Validation Accuracy:0.3941\n",
    "Epoch #180: Loss:1.0722, Accuracy:0.4144, Validation Loss:1.0994, Validation Accuracy:0.3941\n",
    "Epoch #181: Loss:1.0758, Accuracy:0.4053, Validation Loss:1.0963, Validation Accuracy:0.3612\n",
    "Epoch #182: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0803, Validation Accuracy:0.4056\n",
    "Epoch #183: Loss:1.0705, Accuracy:0.4082, Validation Loss:1.0798, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0673, Accuracy:0.4185, Validation Loss:1.0777, Validation Accuracy:0.3859\n",
    "Epoch #185: Loss:1.0667, Accuracy:0.3967, Validation Loss:1.0762, Validation Accuracy:0.3908\n",
    "Epoch #186: Loss:1.0666, Accuracy:0.4168, Validation Loss:1.0731, Validation Accuracy:0.3924\n",
    "Epoch #187: Loss:1.0666, Accuracy:0.4103, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #188: Loss:1.0653, Accuracy:0.4160, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #189: Loss:1.0655, Accuracy:0.4070, Validation Loss:1.0762, Validation Accuracy:0.3941\n",
    "Epoch #190: Loss:1.0655, Accuracy:0.4127, Validation Loss:1.0756, Validation Accuracy:0.4056\n",
    "Epoch #191: Loss:1.0644, Accuracy:0.4160, Validation Loss:1.0764, Validation Accuracy:0.4154\n",
    "Epoch #192: Loss:1.0649, Accuracy:0.3992, Validation Loss:1.0812, Validation Accuracy:0.4056\n",
    "Epoch #193: Loss:1.0652, Accuracy:0.4074, Validation Loss:1.0803, Validation Accuracy:0.3941\n",
    "Epoch #194: Loss:1.0651, Accuracy:0.4090, Validation Loss:1.0873, Validation Accuracy:0.4007\n",
    "Epoch #195: Loss:1.0638, Accuracy:0.4041, Validation Loss:1.0825, Validation Accuracy:0.3908\n",
    "Epoch #196: Loss:1.0646, Accuracy:0.4074, Validation Loss:1.0784, Validation Accuracy:0.3941\n",
    "Epoch #197: Loss:1.0656, Accuracy:0.4037, Validation Loss:1.0796, Validation Accuracy:0.3990\n",
    "Epoch #198: Loss:1.0645, Accuracy:0.4086, Validation Loss:1.0787, Validation Accuracy:0.3974\n",
    "Epoch #199: Loss:1.0631, Accuracy:0.4127, Validation Loss:1.0824, Validation Accuracy:0.3990\n",
    "Epoch #200: Loss:1.0621, Accuracy:0.4168, Validation Loss:1.0827, Validation Accuracy:0.3744\n",
    "Epoch #201: Loss:1.0619, Accuracy:0.4156, Validation Loss:1.0837, Validation Accuracy:0.3859\n",
    "Epoch #202: Loss:1.0622, Accuracy:0.4086, Validation Loss:1.0814, Validation Accuracy:0.3908\n",
    "Epoch #203: Loss:1.0611, Accuracy:0.4251, Validation Loss:1.0836, Validation Accuracy:0.3941\n",
    "Epoch #204: Loss:1.0623, Accuracy:0.4033, Validation Loss:1.0812, Validation Accuracy:0.3892\n",
    "Epoch #205: Loss:1.0605, Accuracy:0.4160, Validation Loss:1.0810, Validation Accuracy:0.3859\n",
    "Epoch #206: Loss:1.0603, Accuracy:0.4090, Validation Loss:1.0817, Validation Accuracy:0.3957\n",
    "Epoch #207: Loss:1.0598, Accuracy:0.4185, Validation Loss:1.0813, Validation Accuracy:0.3875\n",
    "Epoch #208: Loss:1.0588, Accuracy:0.4152, Validation Loss:1.0833, Validation Accuracy:0.3957\n",
    "Epoch #209: Loss:1.0586, Accuracy:0.4222, Validation Loss:1.0838, Validation Accuracy:0.3957\n",
    "Epoch #210: Loss:1.0592, Accuracy:0.4160, Validation Loss:1.0855, Validation Accuracy:0.3990\n",
    "Epoch #211: Loss:1.0582, Accuracy:0.4201, Validation Loss:1.0847, Validation Accuracy:0.3924\n",
    "Epoch #212: Loss:1.0597, Accuracy:0.4189, Validation Loss:1.0855, Validation Accuracy:0.3974\n",
    "Epoch #213: Loss:1.0562, Accuracy:0.4136, Validation Loss:1.0829, Validation Accuracy:0.3892\n",
    "Epoch #214: Loss:1.0578, Accuracy:0.4123, Validation Loss:1.0862, Validation Accuracy:0.4007\n",
    "Epoch #215: Loss:1.0591, Accuracy:0.4049, Validation Loss:1.0852, Validation Accuracy:0.3924\n",
    "Epoch #216: Loss:1.0584, Accuracy:0.4185, Validation Loss:1.0846, Validation Accuracy:0.4072\n",
    "Epoch #217: Loss:1.0562, Accuracy:0.4246, Validation Loss:1.0844, Validation Accuracy:0.3941\n",
    "Epoch #218: Loss:1.0573, Accuracy:0.4185, Validation Loss:1.0846, Validation Accuracy:0.3777\n",
    "Epoch #219: Loss:1.0560, Accuracy:0.4251, Validation Loss:1.0874, Validation Accuracy:0.3810\n",
    "Epoch #220: Loss:1.0561, Accuracy:0.4296, Validation Loss:1.0854, Validation Accuracy:0.3974\n",
    "Epoch #221: Loss:1.0570, Accuracy:0.4156, Validation Loss:1.0868, Validation Accuracy:0.3842\n",
    "Epoch #222: Loss:1.0559, Accuracy:0.4251, Validation Loss:1.0861, Validation Accuracy:0.3793\n",
    "Epoch #223: Loss:1.0545, Accuracy:0.4246, Validation Loss:1.0883, Validation Accuracy:0.3875\n",
    "Epoch #224: Loss:1.0558, Accuracy:0.4222, Validation Loss:1.0892, Validation Accuracy:0.3875\n",
    "Epoch #225: Loss:1.0558, Accuracy:0.4205, Validation Loss:1.0904, Validation Accuracy:0.3941\n",
    "Epoch #226: Loss:1.0551, Accuracy:0.4201, Validation Loss:1.0902, Validation Accuracy:0.3892\n",
    "Epoch #227: Loss:1.0560, Accuracy:0.4156, Validation Loss:1.0920, Validation Accuracy:0.3777\n",
    "Epoch #228: Loss:1.0577, Accuracy:0.4214, Validation Loss:1.0822, Validation Accuracy:0.3892\n",
    "Epoch #229: Loss:1.0591, Accuracy:0.4234, Validation Loss:1.0881, Validation Accuracy:0.3842\n",
    "Epoch #230: Loss:1.0671, Accuracy:0.4000, Validation Loss:1.0939, Validation Accuracy:0.3826\n",
    "Epoch #231: Loss:1.0650, Accuracy:0.4049, Validation Loss:1.0846, Validation Accuracy:0.3974\n",
    "Epoch #232: Loss:1.0614, Accuracy:0.4086, Validation Loss:1.0887, Validation Accuracy:0.3842\n",
    "Epoch #233: Loss:1.0574, Accuracy:0.4271, Validation Loss:1.0856, Validation Accuracy:0.3777\n",
    "Epoch #234: Loss:1.0582, Accuracy:0.4136, Validation Loss:1.0843, Validation Accuracy:0.3908\n",
    "Epoch #235: Loss:1.0580, Accuracy:0.4115, Validation Loss:1.0856, Validation Accuracy:0.3957\n",
    "Epoch #236: Loss:1.0580, Accuracy:0.4209, Validation Loss:1.0885, Validation Accuracy:0.3908\n",
    "Epoch #237: Loss:1.0548, Accuracy:0.4193, Validation Loss:1.0860, Validation Accuracy:0.3892\n",
    "Epoch #238: Loss:1.0546, Accuracy:0.4242, Validation Loss:1.0836, Validation Accuracy:0.4072\n",
    "Epoch #239: Loss:1.0541, Accuracy:0.4234, Validation Loss:1.0878, Validation Accuracy:0.3957\n",
    "Epoch #240: Loss:1.0534, Accuracy:0.4234, Validation Loss:1.0895, Validation Accuracy:0.3941\n",
    "Epoch #241: Loss:1.0538, Accuracy:0.4181, Validation Loss:1.0918, Validation Accuracy:0.3990\n",
    "Epoch #242: Loss:1.0572, Accuracy:0.4193, Validation Loss:1.0901, Validation Accuracy:0.3990\n",
    "Epoch #243: Loss:1.0570, Accuracy:0.4181, Validation Loss:1.0873, Validation Accuracy:0.4023\n",
    "Epoch #244: Loss:1.0596, Accuracy:0.4201, Validation Loss:1.0870, Validation Accuracy:0.3974\n",
    "Epoch #245: Loss:1.0615, Accuracy:0.4131, Validation Loss:1.0878, Validation Accuracy:0.4039\n",
    "Epoch #246: Loss:1.0600, Accuracy:0.4181, Validation Loss:1.0847, Validation Accuracy:0.3760\n",
    "Epoch #247: Loss:1.0628, Accuracy:0.4119, Validation Loss:1.0833, Validation Accuracy:0.3810\n",
    "Epoch #248: Loss:1.0576, Accuracy:0.4271, Validation Loss:1.0981, Validation Accuracy:0.4007\n",
    "Epoch #249: Loss:1.0619, Accuracy:0.4099, Validation Loss:1.0851, Validation Accuracy:0.3826\n",
    "Epoch #250: Loss:1.0577, Accuracy:0.4300, Validation Loss:1.0855, Validation Accuracy:0.3793\n",
    "Epoch #251: Loss:1.0567, Accuracy:0.4218, Validation Loss:1.0867, Validation Accuracy:0.3875\n",
    "Epoch #252: Loss:1.0555, Accuracy:0.4320, Validation Loss:1.0897, Validation Accuracy:0.3760\n",
    "Epoch #253: Loss:1.0557, Accuracy:0.4279, Validation Loss:1.0895, Validation Accuracy:0.3892\n",
    "Epoch #254: Loss:1.0545, Accuracy:0.4275, Validation Loss:1.0949, Validation Accuracy:0.3793\n",
    "Epoch #255: Loss:1.0540, Accuracy:0.4209, Validation Loss:1.0994, Validation Accuracy:0.3760\n",
    "Epoch #256: Loss:1.0550, Accuracy:0.4292, Validation Loss:1.1008, Validation Accuracy:0.3810\n",
    "Epoch #257: Loss:1.0587, Accuracy:0.4099, Validation Loss:1.1008, Validation Accuracy:0.3924\n",
    "Epoch #258: Loss:1.0582, Accuracy:0.4099, Validation Loss:1.0933, Validation Accuracy:0.3875\n",
    "Epoch #259: Loss:1.0570, Accuracy:0.4168, Validation Loss:1.0950, Validation Accuracy:0.3760\n",
    "Epoch #260: Loss:1.0541, Accuracy:0.4275, Validation Loss:1.0854, Validation Accuracy:0.3810\n",
    "Epoch #261: Loss:1.0555, Accuracy:0.4177, Validation Loss:1.0898, Validation Accuracy:0.3711\n",
    "Epoch #262: Loss:1.0541, Accuracy:0.4263, Validation Loss:1.0892, Validation Accuracy:0.3842\n",
    "Epoch #263: Loss:1.0536, Accuracy:0.4201, Validation Loss:1.0879, Validation Accuracy:0.3924\n",
    "Epoch #264: Loss:1.0527, Accuracy:0.4279, Validation Loss:1.0935, Validation Accuracy:0.3695\n",
    "Epoch #265: Loss:1.0526, Accuracy:0.4296, Validation Loss:1.0942, Validation Accuracy:0.3793\n",
    "Epoch #266: Loss:1.0507, Accuracy:0.4251, Validation Loss:1.1016, Validation Accuracy:0.3711\n",
    "Epoch #267: Loss:1.0551, Accuracy:0.4226, Validation Loss:1.1000, Validation Accuracy:0.3908\n",
    "Epoch #268: Loss:1.0524, Accuracy:0.4205, Validation Loss:1.1035, Validation Accuracy:0.3924\n",
    "Epoch #269: Loss:1.0519, Accuracy:0.4312, Validation Loss:1.0912, Validation Accuracy:0.3892\n",
    "Epoch #270: Loss:1.0543, Accuracy:0.4259, Validation Loss:1.0909, Validation Accuracy:0.3711\n",
    "Epoch #271: Loss:1.0547, Accuracy:0.4251, Validation Loss:1.0853, Validation Accuracy:0.3777\n",
    "Epoch #272: Loss:1.0498, Accuracy:0.4329, Validation Loss:1.0923, Validation Accuracy:0.3777\n",
    "Epoch #273: Loss:1.0495, Accuracy:0.4382, Validation Loss:1.0872, Validation Accuracy:0.3744\n",
    "Epoch #274: Loss:1.0491, Accuracy:0.4300, Validation Loss:1.0871, Validation Accuracy:0.3711\n",
    "Epoch #275: Loss:1.0487, Accuracy:0.4329, Validation Loss:1.0868, Validation Accuracy:0.3842\n",
    "Epoch #276: Loss:1.0496, Accuracy:0.4366, Validation Loss:1.0873, Validation Accuracy:0.3859\n",
    "Epoch #277: Loss:1.0495, Accuracy:0.4349, Validation Loss:1.0933, Validation Accuracy:0.3711\n",
    "Epoch #278: Loss:1.0499, Accuracy:0.4398, Validation Loss:1.1124, Validation Accuracy:0.3760\n",
    "Epoch #279: Loss:1.0484, Accuracy:0.4448, Validation Loss:1.1056, Validation Accuracy:0.3777\n",
    "Epoch #280: Loss:1.0501, Accuracy:0.4386, Validation Loss:1.1112, Validation Accuracy:0.3744\n",
    "Epoch #281: Loss:1.0529, Accuracy:0.4353, Validation Loss:1.1031, Validation Accuracy:0.3596\n",
    "Epoch #282: Loss:1.0516, Accuracy:0.4349, Validation Loss:1.0985, Validation Accuracy:0.3777\n",
    "Epoch #283: Loss:1.0513, Accuracy:0.4357, Validation Loss:1.0945, Validation Accuracy:0.3744\n",
    "Epoch #284: Loss:1.0536, Accuracy:0.4234, Validation Loss:1.1094, Validation Accuracy:0.3777\n",
    "Epoch #285: Loss:1.0499, Accuracy:0.4415, Validation Loss:1.0931, Validation Accuracy:0.3678\n",
    "Epoch #286: Loss:1.0512, Accuracy:0.4296, Validation Loss:1.0956, Validation Accuracy:0.3563\n",
    "Epoch #287: Loss:1.0530, Accuracy:0.4218, Validation Loss:1.0909, Validation Accuracy:0.3711\n",
    "Epoch #288: Loss:1.0490, Accuracy:0.4283, Validation Loss:1.0953, Validation Accuracy:0.3465\n",
    "Epoch #289: Loss:1.0504, Accuracy:0.4201, Validation Loss:1.0866, Validation Accuracy:0.3859\n",
    "Epoch #290: Loss:1.0487, Accuracy:0.4361, Validation Loss:1.1015, Validation Accuracy:0.3563\n",
    "Epoch #291: Loss:1.0493, Accuracy:0.4308, Validation Loss:1.0938, Validation Accuracy:0.3711\n",
    "Epoch #292: Loss:1.0504, Accuracy:0.4259, Validation Loss:1.1054, Validation Accuracy:0.3662\n",
    "Epoch #293: Loss:1.0504, Accuracy:0.4283, Validation Loss:1.1109, Validation Accuracy:0.3711\n",
    "Epoch #294: Loss:1.0543, Accuracy:0.4259, Validation Loss:1.1066, Validation Accuracy:0.3695\n",
    "Epoch #295: Loss:1.0529, Accuracy:0.4218, Validation Loss:1.0967, Validation Accuracy:0.3777\n",
    "Epoch #296: Loss:1.0518, Accuracy:0.4230, Validation Loss:1.0898, Validation Accuracy:0.3875\n",
    "Epoch #297: Loss:1.0492, Accuracy:0.4296, Validation Loss:1.0943, Validation Accuracy:0.3842\n",
    "Epoch #298: Loss:1.0464, Accuracy:0.4333, Validation Loss:1.1031, Validation Accuracy:0.3793\n",
    "Epoch #299: Loss:1.0476, Accuracy:0.4316, Validation Loss:1.0935, Validation Accuracy:0.3941\n",
    "Epoch #300: Loss:1.0489, Accuracy:0.4287, Validation Loss:1.1089, Validation Accuracy:0.3645\n",
    "\n",
    "Test:\n",
    "Test Loss:1.10885024, Accuracy:0.3645\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "      01   02  03\n",
    "t:01  72  168   0\n",
    "t:02  77  150   0\n",
    "t:03  54   88   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.35      0.30      0.33       240\n",
    "          02       0.37      0.66      0.47       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.36       609\n",
    "   macro avg       0.24      0.32      0.27       609\n",
    "weighted avg       0.28      0.36      0.30       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 14:21:00 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 33 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.084638369102979, 1.0747455583613104, 1.0733860713507741, 1.0754102842365383, 1.0748550435788133, 1.0739882244852377, 1.0735401167658163, 1.0737739310084502, 1.073534661717407, 1.073530190488192, 1.0733711044189378, 1.0733505984636755, 1.073216048563251, 1.073287371735659, 1.073221567424843, 1.0732358649054967, 1.0728938448409533, 1.07300338173539, 1.0730648526221465, 1.0729756646947124, 1.0732906796466346, 1.0732067247916912, 1.073942331257712, 1.073569758185025, 1.0729114447516956, 1.0731102455229986, 1.0734074808693872, 1.0737191041310628, 1.0735987755661136, 1.0736316671512398, 1.0735838105917368, 1.0728946336971714, 1.072826789126216, 1.0737663534865982, 1.0770314064714905, 1.0742849332749942, 1.0746042425017834, 1.0729011516461426, 1.0724140499612969, 1.0734527713951023, 1.0736576168016456, 1.0734944646973132, 1.0735945844493673, 1.0761066352205324, 1.075203395829412, 1.0747301609841082, 1.0740610854379062, 1.0745241340549512, 1.0744653493900018, 1.0755276650630783, 1.0745619008889535, 1.0748393177398907, 1.074169728360544, 1.0738680239381462, 1.0742583541055815, 1.0750727117159489, 1.0746109769457863, 1.073404463641162, 1.073705153707996, 1.074430871675363, 1.0745466007974935, 1.073265723406975, 1.0743733630783256, 1.0744882808334526, 1.0750302644003005, 1.074838170277074, 1.0742587569507667, 1.0745027155320241, 1.075008565373413, 1.0752727967764943, 1.075238565114527, 1.0750777797745954, 1.0759837416005251, 1.0737637142438214, 1.073840247782189, 1.0747403458421454, 1.0751595364024096, 1.074591796386418, 1.074976921473035, 1.075486953622602, 1.075290502781547, 1.0748809564289787, 1.0744324165024781, 1.0740203035288844, 1.0734867158977466, 1.0740557701521123, 1.074312536000031, 1.0743385864596062, 1.0759083907592473, 1.0765095033081882, 1.075402455181128, 1.075796736480763, 1.0775430627252864, 1.0769869545214674, 1.076523600344979, 1.074361839317923, 1.0747119720737726, 1.0744944450694744, 1.0769354891894487, 1.0739554049346247, 1.0736748025139369, 1.0731271305694956, 1.0731679224615613, 1.073411703892725, 1.0733948928382009, 1.074243889653624, 1.0748069364644819, 1.0757691269046177, 1.0746677732232757, 1.0753071926693218, 1.0758830796321626, 1.0760377837323594, 1.075429935760686, 1.0755945684874586, 1.075626610730865, 1.0756808178765433, 1.0752280102966258, 1.0755704741172603, 1.0754564000272202, 1.0764859470436334, 1.0767560308594226, 1.0746579240695597, 1.0750550832262964, 1.0750454820827116, 1.075464938661735, 1.0760292022294795, 1.0758777886188675, 1.0771424782099983, 1.076605499280106, 1.0759571428565164, 1.0769745299381575, 1.077975883272481, 1.0771672455548065, 1.0776781597356686, 1.0769961430325687, 1.077564156701412, 1.0769383046035892, 1.0773501764181603, 1.0774031726793312, 1.0746038768483304, 1.0746163742491373, 1.07417067479226, 1.074154423375435, 1.0752706147962798, 1.0753864919023561, 1.0768269114502154, 1.0766579552824274, 1.075592708313602, 1.0755205252291926, 1.074896021625287, 1.0762311390468053, 1.0748497641145303, 1.0756248977579703, 1.0756685195493776, 1.0759652255986907, 1.0756266412672346, 1.0752398182801621, 1.076487596203345, 1.075943039164363, 1.074875587704538, 1.0743162457774622, 1.0740912201369337, 1.0746476501471107, 1.073462149779785, 1.0747299311783514, 1.075903609272686, 1.0772595063023183, 1.076621301068461, 1.0761390229555579, 1.0768154667711807, 1.0760030957865598, 1.075398076735498, 1.0767026041528862, 1.0778631633529914, 1.0774707167802382, 1.0790346075944321, 1.0804070625790625, 1.082163193151477, 1.0827094841081717, 1.0993608089503397, 1.0963404423301835, 1.0803366776170402, 1.0797733898429056, 1.0776554627958776, 1.076245582945437, 1.0731351524346764, 1.0736191903037586, 1.076115320468771, 1.076174473527617, 1.0755722148860813, 1.076373103216951, 1.0812189995948904, 1.0803101714608705, 1.0873057871616532, 1.0824934030792788, 1.078422740175219, 1.0795825445789031, 1.0786523245434063, 1.0824007060140224, 1.0826620902921178, 1.0837005588221433, 1.08141630584579, 1.0835819598684953, 1.0812274133434827, 1.081020703652418, 1.0817128327875498, 1.0812763964209846, 1.0833287930057944, 1.083778918866062, 1.0855448226427602, 1.084678863461186, 1.0854916971892559, 1.0828790097009569, 1.0861722140868113, 1.0852057548187832, 1.0845793372108823, 1.0843826221127815, 1.0846206028081709, 1.0874243510767745, 1.0854366074250446, 1.0868295135560686, 1.0861421952693922, 1.088349296345891, 1.0892110295679378, 1.0904447860122706, 1.0902278626885125, 1.0920140130570761, 1.0822165439085811, 1.0881005576482938, 1.0938734781174433, 1.0845945662465588, 1.0886950498731265, 1.085636912308303, 1.0842801978435423, 1.0856053989704801, 1.0885004433505054, 1.0859709993763313, 1.0836131852640112, 1.0877866108820748, 1.0894559318404675, 1.0918312423139174, 1.0901217223779713, 1.0873305836726097, 1.086950070556553, 1.0877530623735074, 1.0846636248339574, 1.083291876296496, 1.098070297335169, 1.085054153096304, 1.0855248649719313, 1.0867100728947932, 1.0897295431941991, 1.0894800148574002, 1.0949050759642778, 1.0994479174684422, 1.100750100632215, 1.1008371490563078, 1.0933432698445562, 1.0950321581563338, 1.085376837570679, 1.0898139421967254, 1.0892358550492962, 1.0879163383850323, 1.0934700975668645, 1.0941598697248938, 1.1016235445520561, 1.1000313529827324, 1.1035241464088703, 1.091183335127306, 1.0908784508117901, 1.0852529176546044, 1.09229033510086, 1.087202592044824, 1.087149338573462, 1.0867825265001194, 1.0872781118148653, 1.093341004085071, 1.112446172679782, 1.1055642693305054, 1.1111791315924358, 1.1030938102693981, 1.0984716472171603, 1.0944791566366436, 1.1093600279787688, 1.0930712031221939, 1.0955616041944531, 1.0908703598482856, 1.0953315247847333, 1.0865870549761016, 1.1015463309922242, 1.0937621098238064, 1.105403785243606, 1.1109148543847998, 1.1066305596252968, 1.0966712594619525, 1.0898045236841212, 1.0943222611604262, 1.1031390103604797, 1.093492971852495, 1.1088503197887651], 'val_acc': [0.37766830777299815, 0.39737274156415403, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.3924466330918968, 0.3924466330918968, 0.39408866931456454, 0.39408866931456454, 0.3957307054393593, 0.3957307054393593, 0.39408866931456454, 0.3891625609401803, 0.3891625608423073, 0.3891625609401803, 0.39080459696710207, 0.39080459696710207, 0.3891625608423073, 0.3891625608423073, 0.38587848859271784, 0.38587848859271784, 0.3891625609401803, 0.3924466332876428, 0.3973727417599, 0.39080459716284804, 0.3990147780804407, 0.38752052481538557, 0.39408866931456454, 0.3891625609401803, 0.39737274156415403, 0.4022988503300302, 0.3924466332876428, 0.37931034282119014, 0.38916256035294244, 0.3990147772974569, 0.39408866921669156, 0.40229884974279234, 0.39573070475424843, 0.3990147770038379, 0.3908045963798642, 0.3990147775910758, 0.3809523792396038, 0.3924466327004049, 0.3940886685315807, 0.3694581262681676, 0.3908045967713561, 0.3908045968692291, 0.3990147774932028, 0.379310343604174, 0.3776683070878873, 0.36288998167111564, 0.40722495811717657, 0.40394088596546, 0.38916256035294244, 0.385878487809734, 0.3973727412705351, 0.4105090301710201, 0.4121510667851797, 0.3940886687273267, 0.3825944155601445, 0.39573070465637544, 0.4039408857697141, 0.3809523793374768, 0.39901477680809194, 0.38916256035294244, 0.38423645168493925, 0.38423645168493925, 0.4006568135201246, 0.3940886687273267, 0.39573070475424843, 0.4022988494491734, 0.3908045963798642, 0.3825944157558905, 0.3924466327004049, 0.40558292209025476, 0.379310343310555, 0.3908045961841182, 0.3940886685315807, 0.3825944155601445, 0.40722495762781163, 0.38587848810335296, 0.3842364518806852, 0.39244663230891297, 0.3973727405854242, 0.4088669943398443, 0.3973727408790432, 0.4022988492534274, 0.3990147771995839, 0.41050903046463905, 0.40558292189450884, 0.39244663250465894, 0.4088669945355902, 0.40065681332437864, 0.3908045958904993, 0.4121510665894338, 0.3973727417599, 0.40229884964491935, 0.3809523792396038, 0.3990147775910758, 0.39408866941243753, 0.40558292209025476, 0.3940886684337077, 0.4022988494491734, 0.40229884974279234, 0.40558292199238183, 0.38423645158706626, 0.39244663250465894, 0.3908045962819912, 0.38423645158706626, 0.38752052383665575, 0.4006568135201246, 0.3940886685315807, 0.3940886684337077, 0.39573070455850246, 0.4022988491555544, 0.4072249579214306, 0.3973727406832972, 0.3973727406832972, 0.3973727408790432, 0.3940886683358347, 0.40065681322650565, 0.40229884964491935, 0.4055829214051439, 0.3924466327982779, 0.40886699385047937, 0.39901477690596493, 0.3957307049499944, 0.40065681322650565, 0.3973727407811702, 0.3793103438977929, 0.38752052364090983, 0.38587848800547997, 0.3875205242281477, 0.40394088547609514, 0.3743842349361708, 0.39573070426488355, 0.40394088537822215, 0.4006568129328867, 0.4072249579214306, 0.3891625599614505, 0.4006568130307597, 0.38752052383665575, 0.4088669936547334, 0.372742199007122, 0.3825944157558905, 0.3875205245217666, 0.3825944157558905, 0.38095237992471465, 0.3957307050478674, 0.38916256054868836, 0.38587848859271784, 0.39244663250465894, 0.3924466327982779, 0.41050903026889307, 0.3957307051457404, 0.3957307049499944, 0.3908045967713561, 0.38916256045081543, 0.3793103439956659, 0.39737274146628104, 0.40394088635695197, 0.40229884974279234, 0.40065681361799754, 0.3924466329940238, 0.38916256035294244, 0.4121510667851797, 0.38916256045081543, 0.40558292199238183, 0.39244663289615084, 0.39244663260253193, 0.4072249578235576, 0.3875205242281477, 0.3908045964777372, 0.4055829216008899, 0.39408866911881857, 0.38095237992471465, 0.3957307052436133, 0.39408866882519966, 0.39408866921669156, 0.36124794623143175, 0.40558292209025476, 0.3957307053414863, 0.38587848810335296, 0.39080459667348316, 0.3924466327982779, 0.3990147771995839, 0.3990147774932028, 0.39408866882519966, 0.40558292189450884, 0.4154351388390233, 0.40558292189450884, 0.39408866911881857, 0.40065681332437864, 0.3908045968692291, 0.39408866911881857, 0.3990147771995839, 0.39737274136840806, 0.39901477690596493, 0.3743842353276627, 0.38587848849484485, 0.3908045967713561, 0.3940886686294537, 0.38916256054868836, 0.38587848849484485, 0.3957307049499944, 0.3875205246196396, 0.3957307049499944, 0.3957307052436133, 0.3990147771995839, 0.3924466329940238, 0.39737274097691616, 0.38916256074443434, 0.4006568134222516, 0.39244663289615084, 0.4072249578235576, 0.39408866911881857, 0.37766830757725217, 0.38095237992471465, 0.39737274136840806, 0.38423645217430413, 0.379310343408428, 0.3875205246196396, 0.3875205247175126, 0.3940886687273267, 0.3891625608423073, 0.37766830747937924, 0.38916256045081543, 0.38423645217430413, 0.3825944159516364, 0.39737274156415403, 0.38423645168493925, 0.37766830757725217, 0.39080459667348316, 0.3957307050478674, 0.3908045968692291, 0.38916256074443434, 0.40722495762781163, 0.39573070475424843, 0.3940886687273267, 0.3990147772974569, 0.3990147770038379, 0.40229884954704637, 0.39737274136840806, 0.4039408855739681, 0.3760262711588385, 0.38095238002258763, 0.40065681371587053, 0.3825944160495094, 0.37931034311480905, 0.3875205242281477, 0.3760262711588385, 0.38916256054868836, 0.37931034321268203, 0.37602627096309255, 0.38095237982684166, 0.39244663289615084, 0.3875205244238936, 0.37602627106096553, 0.38095238002258763, 0.3711001629802002, 0.3842364523700501, 0.3924466330918968, 0.3694581267575325, 0.379310343604174, 0.3711001629802002, 0.3908045968692291, 0.3924466330918968, 0.38916256074443434, 0.3711001632738192, 0.37766830767512516, 0.37766830757725217, 0.37438423542553567, 0.3711001628823273, 0.3842364519785582, 0.38587848810335296, 0.3711001625887083, 0.37602627106096553, 0.37766830728363326, 0.3743842352297897, 0.3596059097151451, 0.37766830757725217, 0.37438423562128165, 0.37766830757725217, 0.3678160911221027, 0.3563218383464124, 0.37110016346956515, 0.34646962140189796, 0.3858784887884638, 0.3563218385421584, 0.3711001632738192, 0.366174054605816, 0.37110016337169216, 0.36945812705115144, 0.37766830787087113, 0.3875205245217666, 0.3842364522721771, 0.3793103437020469, 0.39408866921669156, 0.3645320185788942], 'loss': [1.104081817671993, 1.078762340937307, 1.073621050777866, 1.074759799352172, 1.075353563933402, 1.0745549126572187, 1.0740258224690964, 1.073910991660868, 1.073917023112397, 1.07387092240782, 1.0736358366462975, 1.0733926578713637, 1.073389815892527, 1.0732889424114502, 1.0732334070381933, 1.0731702771274951, 1.0733470718963436, 1.0732736471986868, 1.0732164373143254, 1.0731059242812515, 1.0730840704524296, 1.0731341035459077, 1.073203644370641, 1.0728872300173469, 1.0733378827939044, 1.0731535868967828, 1.0727545347057084, 1.0726347558552234, 1.0726661997409328, 1.0728530918058674, 1.072430724872456, 1.072967348500199, 1.0733765339214945, 1.0729893121876022, 1.0759402550711035, 1.0744286150413371, 1.074134197127403, 1.0745235703564278, 1.0733055320853326, 1.073749186566723, 1.0730996977377232, 1.0730263296338811, 1.072535326789292, 1.0728633750880279, 1.0733261786936734, 1.073323265776742, 1.0733827374064702, 1.0727485086394042, 1.072646239454986, 1.0720351857081576, 1.0719684683077144, 1.072183058296141, 1.0716585158322627, 1.0715658057641688, 1.071073666588237, 1.0715969285925802, 1.0714952463486846, 1.072344223625606, 1.071852161752125, 1.0713090190162893, 1.072149025928803, 1.073034181144448, 1.0728924228914953, 1.0728443286257359, 1.072371750690609, 1.0719302192116176, 1.0719988845946609, 1.0718902032233362, 1.0712927813892248, 1.0715427937693665, 1.0720297740470213, 1.071926347724711, 1.071747688248417, 1.07259499551824, 1.0718888148145265, 1.0715999227774462, 1.0709023961296316, 1.0707147436709865, 1.0710089960137432, 1.0708627614152504, 1.0706828649039142, 1.070239938260104, 1.070359852965118, 1.0705425321688642, 1.0719972558090085, 1.0723700945871812, 1.0721822608912506, 1.073162549770833, 1.0720509636328694, 1.071095210326036, 1.071325822189848, 1.0712459490284538, 1.0715785903362767, 1.0734611394224225, 1.0733770992232055, 1.0722918847747898, 1.0723558857210853, 1.071066131533049, 1.071543218859412, 1.0715600360835114, 1.071625443062988, 1.0715973525076676, 1.0711813287813317, 1.0714911987159776, 1.0715869113894703, 1.071946091818369, 1.07107931412221, 1.070624886450092, 1.0707725900399367, 1.0712902170921499, 1.0708311061839548, 1.070186792408906, 1.0704686409149327, 1.0698990308773346, 1.070278289528598, 1.0706570084824454, 1.070024603984684, 1.0692905087245808, 1.0692683762348654, 1.0690023036952871, 1.0698892773054465, 1.0699715406742918, 1.070869182610169, 1.0702697055785317, 1.0691948763398909, 1.0704508073520855, 1.069786594731607, 1.0696129738182991, 1.0700134860906267, 1.0710964128466847, 1.0721652687942222, 1.0718811002844904, 1.0706654626975558, 1.0692366316088415, 1.0683339707905262, 1.0677919733940453, 1.0682001911394405, 1.0688104416555448, 1.0679513072575877, 1.068099522247941, 1.0696867327915325, 1.0698286847167435, 1.0697255317680154, 1.0698910619199153, 1.070386887869551, 1.0697920171631925, 1.0715023996404065, 1.0697554938847034, 1.069602695235971, 1.0701276927513264, 1.0696181333040555, 1.070615968126536, 1.0688387074754468, 1.068929863122944, 1.068378739337412, 1.0682603853194377, 1.0695501617337644, 1.0696333628170789, 1.0673148780877586, 1.0688935721434607, 1.0672226772171272, 1.0680541045367107, 1.067647224236318, 1.0665729547672937, 1.0659121511898002, 1.0661816551455237, 1.0676322613408677, 1.0689494547168332, 1.0683821929308912, 1.0659111710299702, 1.0668476547303876, 1.0653557849860533, 1.064057515827782, 1.0649170105951768, 1.065738347031987, 1.0659655945991344, 1.0717032963735122, 1.0669367866104877, 1.0658635912000276, 1.072161781518611, 1.075788731349812, 1.0740745889577532, 1.070546944185449, 1.0673176948539531, 1.066740625545964, 1.066556282993215, 1.066597129676866, 1.0652801816713149, 1.065493334734954, 1.0655219510840195, 1.0644118856845206, 1.0649026554467986, 1.0652003620194703, 1.0650636676400593, 1.063791552118697, 1.0646318151231173, 1.0656313601950111, 1.06448113981948, 1.0630773821405806, 1.0621434188721361, 1.0618914238970871, 1.0622405964735842, 1.06109871776197, 1.0623080500831839, 1.060457637814281, 1.0602843346781798, 1.059782976242551, 1.05884660964629, 1.0586239975825473, 1.0592226584099647, 1.0581905576972257, 1.0596792370386927, 1.0561875567543924, 1.0577993197607554, 1.059136008824656, 1.0583647811682073, 1.0561632677759722, 1.0572571230130519, 1.0559622293135469, 1.0560528761552344, 1.0569530376908225, 1.0558807186032713, 1.054503385438077, 1.0558221697562529, 1.055761263943306, 1.0550849615426035, 1.055955698602743, 1.05769166819124, 1.0590924965527513, 1.0671320748280206, 1.0649749920353508, 1.0613831762415673, 1.0574015062692474, 1.0581639644301648, 1.0580307976176362, 1.0580032237011794, 1.054791632765862, 1.0546244379920882, 1.0540637472082213, 1.0533895833291558, 1.0537678914882809, 1.0571646081838275, 1.056982023515251, 1.0596391171155768, 1.0614630826934408, 1.060017018837116, 1.0628198330896836, 1.0575626108925444, 1.0618547848852264, 1.0577117549320512, 1.056677545708063, 1.0555206569068485, 1.055715927255227, 1.0545037726846809, 1.0539793975544172, 1.054972541748376, 1.058728890350467, 1.0582346829056006, 1.0570276693152207, 1.05407769396075, 1.0555179817720606, 1.0541214157178906, 1.0536189333369355, 1.0527010415368991, 1.0525958645270346, 1.0507385806626117, 1.055088051339684, 1.0523831852162888, 1.0518917931668323, 1.0543371943232949, 1.0547060949356895, 1.0498422250610602, 1.049521166098436, 1.0490795166340696, 1.0487037279277855, 1.049649027045013, 1.049495296899298, 1.049939889290984, 1.0484044866150655, 1.0501370784438366, 1.0528564877089044, 1.0515910545658527, 1.0513493957950348, 1.0535868034225715, 1.0499243181588958, 1.051155231571785, 1.0530361111893547, 1.0490428894697028, 1.050442595305629, 1.0487294964721805, 1.0492973536191779, 1.0503816024968267, 1.0503693797015556, 1.054291488894202, 1.0529399861055724, 1.051827828007802, 1.049160109114598, 1.0464182992735438, 1.0475844106635028, 1.0489402093436928], 'acc': [0.30349076090407323, 0.38726899381535745, 0.3958932231706271, 0.39589322418647627, 0.39630390141289334, 0.39589322418647627, 0.39671458138087934, 0.3987679692754021, 0.3999999991922163, 0.39753593520951713, 0.3987679673171386, 0.3979466128643044, 0.4000000019337852, 0.4012320306389239, 0.4000000019337852, 0.40164270845282, 0.40082135736575114, 0.4020533866583689, 0.40123203338049274, 0.4016427127977171, 0.4012320339679718, 0.40205339041578697, 0.40082135341250674, 0.4036961006922399, 0.39876796496722244, 0.39548254833084356, 0.40082135619079307, 0.4045174555367268, 0.4036960977181272, 0.4016427126018908, 0.4041067767436989, 0.3991786441151856, 0.40041067778941786, 0.40287474310618404, 0.3761806989107778, 0.3831622176346593, 0.4000000007588271, 0.39630390020121786, 0.4024640672505514, 0.39753593579699614, 0.39507186836285757, 0.4008213555665966, 0.40287474365694564, 0.39383983394203737, 0.3913757715626664, 0.3897330606987344, 0.4020533892408289, 0.3967145794226159, 0.3979466124726517, 0.40164271005614827, 0.40205338724584794, 0.40369609892980274, 0.41232033086753234, 0.40369609634734277, 0.4053388070154484, 0.40944558574188905, 0.40246406349313335, 0.40985626320084995, 0.4012320314222293, 0.4061601652257007, 0.4073921966724082, 0.4086242285474859, 0.40082135619079307, 0.40082135380415945, 0.4073921953016238, 0.4024640662714196, 0.4000000007588271, 0.404517453382637, 0.4012320310305766, 0.4053388095611909, 0.4053388089737119, 0.4016427090402991, 0.4073921980799101, 0.3950718681670312, 0.4086242309341196, 0.40287474404859835, 0.4110882956266893, 0.41519507380236836, 0.41314168215042757, 0.41108829585923307, 0.4041067765478726, 0.41519507141573475, 0.4090349071446875, 0.4114989748113699, 0.40041067876854963, 0.3979466114568025, 0.40246406705472504, 0.38562628134809723, 0.3926078010511105, 0.406160162251588, 0.40657084065296323, 0.390554412764935, 0.397125256649033, 0.41026694003561437, 0.3975359358337136, 0.39630390020121786, 0.3975359362253663, 0.40492813315479664, 0.4016427098603219, 0.39999999935132524, 0.4057494844376918, 0.39630390023793527, 0.40246406368895965, 0.40657084186463877, 0.40410677631532876, 0.4061601630348934, 0.4028747416986822, 0.4114989712864956, 0.415195073606542, 0.4119096506670026, 0.40574948467023564, 0.4119096504711762, 0.40041067778941786, 0.40369609892980274, 0.4057494881951099, 0.3987679653221577, 0.39137576901692384, 0.398767968687923, 0.4016427114269327, 0.3958932259489134, 0.40205338865334983, 0.41108829785421397, 0.40000000232543786, 0.40698151909105584, 0.40903490969043005, 0.4090349099229738, 0.4094455861702592, 0.40698152065766663, 0.41601642884268164, 0.4065708404571369, 0.4049281310007068, 0.4016427124060645, 0.39712525582901015, 0.4053388105770401, 0.40698151909105584, 0.4082135501461107, 0.4098562614016954, 0.4086242293675088, 0.40246406705472504, 0.4082135515168952, 0.3950718667962468, 0.3975359350504082, 0.40164271162275905, 0.39917864532686115, 0.3950718687545103, 0.39794661071021453, 0.3999999991554989, 0.4053388082271239, 0.39835728809574056, 0.40533880681962203, 0.3979466116893463, 0.402464066467246, 0.4102669401947233, 0.40862422933079134, 0.4102669421897042, 0.41314168489199643, 0.4119096508628289, 0.40739219588910286, 0.4151950718073874, 0.39958932353241, 0.414373715004637, 0.40451745177930876, 0.40041067974768135, 0.4147843937976649, 0.4156057488379782, 0.41232032969257426, 0.41355235976849736, 0.39548254696005913, 0.39794661110186724, 0.40985626277247983, 0.40944558754104365, 0.4164271060690988, 0.40862422972244405, 0.39671457766017876, 0.4106776162461823, 0.4004106775935915, 0.40410677592367605, 0.4032854232699964, 0.40698151807520666, 0.41437371797874967, 0.4053388070154484, 0.3946611887498068, 0.40821355370770246, 0.418480493339425, 0.39671457824765777, 0.4168377807130559, 0.4102669421897042, 0.41601642845102893, 0.40698151889522954, 0.4127310071148177, 0.4160164260643953, 0.3991786433318802, 0.40739219788408376, 0.4090349075363402, 0.4041067770986342, 0.40739219788408376, 0.4036960975590183, 0.4086242319132513, 0.4127310059031422, 0.41683778466630034, 0.41560574903380454, 0.4086242305057494, 0.42505133421269287, 0.4032854232699964, 0.4160164282552026, 0.4090349089438421, 0.4184804909895088, 0.41519507145245216, 0.42217658994378987, 0.416016426456048, 0.42012320185344076, 0.41889117256082303, 0.4135523603559764, 0.4123203276975933, 0.4049281335464493, 0.4184804927886634, 0.42464065843049503, 0.4184804909895088, 0.42505133522854205, 0.4295687898106154, 0.41560575181209086, 0.42505133444523663, 0.4246406554563824, 0.42217659252624984, 0.4205338826414496, 0.4201232022450935, 0.4156057502087626, 0.42135523686174003, 0.4234086222105202, 0.40000000173795885, 0.40492813217566487, 0.4086242279600069, 0.4271047237105438, 0.41355236313426275, 0.4114989716781483, 0.42094455806871217, 0.41930184798808556, 0.4242299782299653, 0.423408623777131, 0.4234086224063466, 0.41806981673720434, 0.41930184700895384, 0.4180698173246834, 0.4201232056108588, 0.4131416819546012, 0.4180698137630917, 0.41190965184196066, 0.42710472155645396, 0.40985626320084995, 0.42997946801616427, 0.42176591588731177, 0.43203285453990253, 0.42792607937505356, 0.42751540152443995, 0.4209445574812331, 0.4291581125841983, 0.40985626218500076, 0.4098562643758081, 0.41683778423793016, 0.4275153997620028, 0.41765913869076443, 0.4262833663203143, 0.42012320482755344, 0.42792607640094094, 0.4295687894189627, 0.42505133483688934, 0.4225872691651879, 0.420533880254816, 0.43121150129874386, 0.4258726879189391, 0.425051334053584, 0.4328542101676949, 0.4381930180276444, 0.429979466412836, 0.43285421079189135, 0.4365503101378251, 0.43490759551647507, 0.43983572869575, 0.4447638589376297, 0.43860369623319323, 0.43531827450532934, 0.4349075953206487, 0.43572895134009376, 0.4234086257353945, 0.4414784417504892, 0.4295687900431592, 0.42176591369650446, 0.4283367546064898, 0.42012320126596175, 0.4361396315039061, 0.4308008197274296, 0.4258726908563344, 0.42833675343153166, 0.4258726908563344, 0.4217659125215464, 0.4229979467465403, 0.4295687874606992, 0.4332648870024593, 0.43162217731348546, 0.42874743339951765]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
