{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf13.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 02:26:31 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '0', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eb', 'yd', 'sg', 'sk', 'my', 'by', 'ib', 'aa', 'eg', 'ek', 'mb', 'eo', 'ce', 'ds', 'ck'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000228A2CCD240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002289D4B6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7036, Accuracy:0.1023, Validation Loss:2.6963, Validation Accuracy:0.1018\n",
    "Epoch #2: Loss:2.6924, Accuracy:0.1023, Validation Loss:2.6862, Validation Accuracy:0.1018\n",
    "Epoch #3: Loss:2.6830, Accuracy:0.1023, Validation Loss:2.6778, Validation Accuracy:0.1018\n",
    "Epoch #4: Loss:2.6753, Accuracy:0.0821, Validation Loss:2.6714, Validation Accuracy:0.0854\n",
    "Epoch #5: Loss:2.6705, Accuracy:0.0891, Validation Loss:2.6668, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6662, Accuracy:0.1023, Validation Loss:2.6642, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6637, Accuracy:0.1023, Validation Loss:2.6627, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6625, Accuracy:0.1023, Validation Loss:2.6619, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6618, Accuracy:0.1023, Validation Loss:2.6613, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6615, Accuracy:0.1023, Validation Loss:2.6609, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6611, Accuracy:0.1023, Validation Loss:2.6606, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6606, Accuracy:0.1023, Validation Loss:2.6602, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6603, Accuracy:0.1023, Validation Loss:2.6596, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6596, Accuracy:0.1023, Validation Loss:2.6589, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6587, Accuracy:0.1023, Validation Loss:2.6578, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6574, Accuracy:0.1023, Validation Loss:2.6561, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6554, Accuracy:0.1027, Validation Loss:2.6534, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6523, Accuracy:0.1035, Validation Loss:2.6492, Validation Accuracy:0.1051\n",
    "Epoch #19: Loss:2.6473, Accuracy:0.1080, Validation Loss:2.6423, Validation Accuracy:0.1182\n",
    "Epoch #20: Loss:2.6386, Accuracy:0.1191, Validation Loss:2.6304, Validation Accuracy:0.1232\n",
    "Epoch #21: Loss:2.6237, Accuracy:0.1281, Validation Loss:2.6105, Validation Accuracy:0.1396\n",
    "Epoch #22: Loss:2.5984, Accuracy:0.1429, Validation Loss:2.5797, Validation Accuracy:0.1609\n",
    "Epoch #23: Loss:2.5621, Accuracy:0.1548, Validation Loss:2.5395, Validation Accuracy:0.1576\n",
    "Epoch #24: Loss:2.5181, Accuracy:0.1552, Validation Loss:2.4987, Validation Accuracy:0.1658\n",
    "Epoch #25: Loss:2.4757, Accuracy:0.1602, Validation Loss:2.4657, Validation Accuracy:0.1905\n",
    "Epoch #26: Loss:2.4393, Accuracy:0.1844, Validation Loss:2.4369, Validation Accuracy:0.2020\n",
    "Epoch #27: Loss:2.4068, Accuracy:0.1836, Validation Loss:2.4131, Validation Accuracy:0.1839\n",
    "Epoch #28: Loss:2.3822, Accuracy:0.1828, Validation Loss:2.3803, Validation Accuracy:0.2053\n",
    "Epoch #29: Loss:2.3618, Accuracy:0.1897, Validation Loss:2.3528, Validation Accuracy:0.2496\n",
    "Epoch #30: Loss:2.3254, Accuracy:0.2337, Validation Loss:2.3305, Validation Accuracy:0.2463\n",
    "Epoch #31: Loss:2.3044, Accuracy:0.2341, Validation Loss:2.3034, Validation Accuracy:0.2578\n",
    "Epoch #32: Loss:2.2853, Accuracy:0.2275, Validation Loss:2.2845, Validation Accuracy:0.2562\n",
    "Epoch #33: Loss:2.2625, Accuracy:0.2353, Validation Loss:2.2587, Validation Accuracy:0.2627\n",
    "Epoch #34: Loss:2.2477, Accuracy:0.2349, Validation Loss:2.2481, Validation Accuracy:0.2562\n",
    "Epoch #35: Loss:2.2322, Accuracy:0.2382, Validation Loss:2.2239, Validation Accuracy:0.2578\n",
    "Epoch #36: Loss:2.2138, Accuracy:0.2411, Validation Loss:2.2094, Validation Accuracy:0.2562\n",
    "Epoch #37: Loss:2.1965, Accuracy:0.2415, Validation Loss:2.1901, Validation Accuracy:0.2726\n",
    "Epoch #38: Loss:2.1822, Accuracy:0.2419, Validation Loss:2.1721, Validation Accuracy:0.2677\n",
    "Epoch #39: Loss:2.1642, Accuracy:0.2468, Validation Loss:2.1589, Validation Accuracy:0.2677\n",
    "Epoch #40: Loss:2.1573, Accuracy:0.2563, Validation Loss:2.1495, Validation Accuracy:0.2742\n",
    "Epoch #41: Loss:2.1397, Accuracy:0.2563, Validation Loss:2.1456, Validation Accuracy:0.2644\n",
    "Epoch #42: Loss:2.1372, Accuracy:0.2620, Validation Loss:2.1217, Validation Accuracy:0.2939\n",
    "Epoch #43: Loss:2.1189, Accuracy:0.2731, Validation Loss:2.1099, Validation Accuracy:0.2890\n",
    "Epoch #44: Loss:2.1048, Accuracy:0.2723, Validation Loss:2.0922, Validation Accuracy:0.2989\n",
    "Epoch #45: Loss:2.0921, Accuracy:0.2903, Validation Loss:2.0846, Validation Accuracy:0.3169\n",
    "Epoch #46: Loss:2.0830, Accuracy:0.2879, Validation Loss:2.0822, Validation Accuracy:0.2956\n",
    "Epoch #47: Loss:2.0725, Accuracy:0.2982, Validation Loss:2.0679, Validation Accuracy:0.3153\n",
    "Epoch #48: Loss:2.0649, Accuracy:0.3031, Validation Loss:2.0655, Validation Accuracy:0.3169\n",
    "Epoch #49: Loss:2.0579, Accuracy:0.2945, Validation Loss:2.0575, Validation Accuracy:0.3021\n",
    "Epoch #50: Loss:2.0526, Accuracy:0.3064, Validation Loss:2.0489, Validation Accuracy:0.3153\n",
    "Epoch #51: Loss:2.0432, Accuracy:0.3133, Validation Loss:2.0427, Validation Accuracy:0.3169\n",
    "Epoch #52: Loss:2.0335, Accuracy:0.3080, Validation Loss:2.0393, Validation Accuracy:0.3071\n",
    "Epoch #53: Loss:2.0297, Accuracy:0.3109, Validation Loss:2.0228, Validation Accuracy:0.3235\n",
    "Epoch #54: Loss:2.0225, Accuracy:0.3133, Validation Loss:2.0205, Validation Accuracy:0.3284\n",
    "Epoch #55: Loss:2.0141, Accuracy:0.3195, Validation Loss:2.0191, Validation Accuracy:0.3120\n",
    "Epoch #56: Loss:2.0110, Accuracy:0.3170, Validation Loss:2.0132, Validation Accuracy:0.3251\n",
    "Epoch #57: Loss:2.0051, Accuracy:0.3191, Validation Loss:2.0041, Validation Accuracy:0.3284\n",
    "Epoch #58: Loss:1.9959, Accuracy:0.3232, Validation Loss:1.9992, Validation Accuracy:0.3333\n",
    "Epoch #59: Loss:1.9914, Accuracy:0.3265, Validation Loss:1.9962, Validation Accuracy:0.3251\n",
    "Epoch #60: Loss:1.9874, Accuracy:0.3236, Validation Loss:1.9925, Validation Accuracy:0.3366\n",
    "Epoch #61: Loss:1.9810, Accuracy:0.3240, Validation Loss:1.9900, Validation Accuracy:0.3300\n",
    "Epoch #62: Loss:1.9798, Accuracy:0.3261, Validation Loss:1.9873, Validation Accuracy:0.3415\n",
    "Epoch #63: Loss:1.9766, Accuracy:0.3265, Validation Loss:1.9846, Validation Accuracy:0.3186\n",
    "Epoch #64: Loss:1.9699, Accuracy:0.3302, Validation Loss:1.9752, Validation Accuracy:0.3366\n",
    "Epoch #65: Loss:1.9663, Accuracy:0.3306, Validation Loss:1.9736, Validation Accuracy:0.3300\n",
    "Epoch #66: Loss:1.9631, Accuracy:0.3294, Validation Loss:1.9716, Validation Accuracy:0.3399\n",
    "Epoch #67: Loss:1.9598, Accuracy:0.3269, Validation Loss:1.9667, Validation Accuracy:0.3465\n",
    "Epoch #68: Loss:1.9574, Accuracy:0.3298, Validation Loss:1.9766, Validation Accuracy:0.3350\n",
    "Epoch #69: Loss:1.9565, Accuracy:0.3285, Validation Loss:1.9639, Validation Accuracy:0.3383\n",
    "Epoch #70: Loss:1.9487, Accuracy:0.3339, Validation Loss:1.9595, Validation Accuracy:0.3415\n",
    "Epoch #71: Loss:1.9444, Accuracy:0.3314, Validation Loss:1.9558, Validation Accuracy:0.3448\n",
    "Epoch #72: Loss:1.9426, Accuracy:0.3372, Validation Loss:1.9529, Validation Accuracy:0.3465\n",
    "Epoch #73: Loss:1.9397, Accuracy:0.3351, Validation Loss:1.9554, Validation Accuracy:0.3350\n",
    "Epoch #74: Loss:1.9415, Accuracy:0.3368, Validation Loss:1.9497, Validation Accuracy:0.3432\n",
    "Epoch #75: Loss:1.9370, Accuracy:0.3306, Validation Loss:1.9514, Validation Accuracy:0.3399\n",
    "Epoch #76: Loss:1.9310, Accuracy:0.3351, Validation Loss:1.9435, Validation Accuracy:0.3432\n",
    "Epoch #77: Loss:1.9301, Accuracy:0.3335, Validation Loss:1.9465, Validation Accuracy:0.3448\n",
    "Epoch #78: Loss:1.9311, Accuracy:0.3363, Validation Loss:1.9439, Validation Accuracy:0.3481\n",
    "Epoch #79: Loss:1.9247, Accuracy:0.3425, Validation Loss:1.9571, Validation Accuracy:0.3399\n",
    "Epoch #80: Loss:1.9348, Accuracy:0.3285, Validation Loss:1.9379, Validation Accuracy:0.3530\n",
    "Epoch #81: Loss:1.9246, Accuracy:0.3285, Validation Loss:1.9397, Validation Accuracy:0.3432\n",
    "Epoch #82: Loss:1.9174, Accuracy:0.3372, Validation Loss:1.9319, Validation Accuracy:0.3580\n",
    "Epoch #83: Loss:1.9179, Accuracy:0.3351, Validation Loss:1.9485, Validation Accuracy:0.3415\n",
    "Epoch #84: Loss:1.9200, Accuracy:0.3359, Validation Loss:1.9319, Validation Accuracy:0.3547\n",
    "Epoch #85: Loss:1.9147, Accuracy:0.3417, Validation Loss:1.9441, Validation Accuracy:0.3383\n",
    "Epoch #86: Loss:1.9124, Accuracy:0.3425, Validation Loss:1.9277, Validation Accuracy:0.3580\n",
    "Epoch #87: Loss:1.9109, Accuracy:0.3413, Validation Loss:1.9334, Validation Accuracy:0.3415\n",
    "Epoch #88: Loss:1.9081, Accuracy:0.3421, Validation Loss:1.9234, Validation Accuracy:0.3481\n",
    "Epoch #89: Loss:1.9076, Accuracy:0.3417, Validation Loss:1.9246, Validation Accuracy:0.3612\n",
    "Epoch #90: Loss:1.8988, Accuracy:0.3462, Validation Loss:1.9243, Validation Accuracy:0.3530\n",
    "Epoch #91: Loss:1.9019, Accuracy:0.3446, Validation Loss:1.9200, Validation Accuracy:0.3498\n",
    "Epoch #92: Loss:1.8942, Accuracy:0.3421, Validation Loss:1.9235, Validation Accuracy:0.3481\n",
    "Epoch #93: Loss:1.8970, Accuracy:0.3425, Validation Loss:1.9183, Validation Accuracy:0.3563\n",
    "Epoch #94: Loss:1.8921, Accuracy:0.3409, Validation Loss:1.9130, Validation Accuracy:0.3612\n",
    "Epoch #95: Loss:1.8927, Accuracy:0.3400, Validation Loss:1.9333, Validation Accuracy:0.3383\n",
    "Epoch #96: Loss:1.8971, Accuracy:0.3507, Validation Loss:1.9096, Validation Accuracy:0.3612\n",
    "Epoch #97: Loss:1.8882, Accuracy:0.3470, Validation Loss:1.9117, Validation Accuracy:0.3530\n",
    "Epoch #98: Loss:1.8837, Accuracy:0.3544, Validation Loss:1.9069, Validation Accuracy:0.3596\n",
    "Epoch #99: Loss:1.8818, Accuracy:0.3536, Validation Loss:1.9086, Validation Accuracy:0.3415\n",
    "Epoch #100: Loss:1.8818, Accuracy:0.3487, Validation Loss:1.9100, Validation Accuracy:0.3415\n",
    "Epoch #101: Loss:1.8800, Accuracy:0.3552, Validation Loss:1.9040, Validation Accuracy:0.3596\n",
    "Epoch #102: Loss:1.8775, Accuracy:0.3520, Validation Loss:1.9035, Validation Accuracy:0.3432\n",
    "Epoch #103: Loss:1.8767, Accuracy:0.3552, Validation Loss:1.9138, Validation Accuracy:0.3350\n",
    "Epoch #104: Loss:1.8808, Accuracy:0.3515, Validation Loss:1.9049, Validation Accuracy:0.3563\n",
    "Epoch #105: Loss:1.8776, Accuracy:0.3524, Validation Loss:1.9111, Validation Accuracy:0.3563\n",
    "Epoch #106: Loss:1.8777, Accuracy:0.3544, Validation Loss:1.9031, Validation Accuracy:0.3415\n",
    "Epoch #107: Loss:1.8763, Accuracy:0.3520, Validation Loss:1.9131, Validation Accuracy:0.3366\n",
    "Epoch #108: Loss:1.8797, Accuracy:0.3532, Validation Loss:1.8992, Validation Accuracy:0.3415\n",
    "Epoch #109: Loss:1.8740, Accuracy:0.3548, Validation Loss:1.9070, Validation Accuracy:0.3629\n",
    "Epoch #110: Loss:1.8724, Accuracy:0.3552, Validation Loss:1.8954, Validation Accuracy:0.3514\n",
    "Epoch #111: Loss:1.8667, Accuracy:0.3556, Validation Loss:1.9009, Validation Accuracy:0.3317\n",
    "Epoch #112: Loss:1.8682, Accuracy:0.3548, Validation Loss:1.8955, Validation Accuracy:0.3695\n",
    "Epoch #113: Loss:1.8607, Accuracy:0.3548, Validation Loss:1.8914, Validation Accuracy:0.3514\n",
    "Epoch #114: Loss:1.8627, Accuracy:0.3610, Validation Loss:1.8920, Validation Accuracy:0.3514\n",
    "Epoch #115: Loss:1.8636, Accuracy:0.3610, Validation Loss:1.8889, Validation Accuracy:0.3563\n",
    "Epoch #116: Loss:1.8581, Accuracy:0.3577, Validation Loss:1.8875, Validation Accuracy:0.3629\n",
    "Epoch #117: Loss:1.8558, Accuracy:0.3565, Validation Loss:1.8869, Validation Accuracy:0.3563\n",
    "Epoch #118: Loss:1.8538, Accuracy:0.3610, Validation Loss:1.8843, Validation Accuracy:0.3645\n",
    "Epoch #119: Loss:1.8574, Accuracy:0.3593, Validation Loss:1.8851, Validation Accuracy:0.3530\n",
    "Epoch #120: Loss:1.8544, Accuracy:0.3593, Validation Loss:1.8848, Validation Accuracy:0.3514\n",
    "Epoch #121: Loss:1.8524, Accuracy:0.3598, Validation Loss:1.8880, Validation Accuracy:0.3498\n",
    "Epoch #122: Loss:1.8542, Accuracy:0.3593, Validation Loss:1.8824, Validation Accuracy:0.3580\n",
    "Epoch #123: Loss:1.8489, Accuracy:0.3622, Validation Loss:1.8788, Validation Accuracy:0.3629\n",
    "Epoch #124: Loss:1.8476, Accuracy:0.3630, Validation Loss:1.8795, Validation Accuracy:0.3662\n",
    "Epoch #125: Loss:1.8455, Accuracy:0.3643, Validation Loss:1.8767, Validation Accuracy:0.3678\n",
    "Epoch #126: Loss:1.8447, Accuracy:0.3663, Validation Loss:1.8769, Validation Accuracy:0.3629\n",
    "Epoch #127: Loss:1.8443, Accuracy:0.3622, Validation Loss:1.8791, Validation Accuracy:0.3580\n",
    "Epoch #128: Loss:1.8453, Accuracy:0.3671, Validation Loss:1.8815, Validation Accuracy:0.3580\n",
    "Epoch #129: Loss:1.8438, Accuracy:0.3676, Validation Loss:1.8738, Validation Accuracy:0.3612\n",
    "Epoch #130: Loss:1.8418, Accuracy:0.3684, Validation Loss:1.8760, Validation Accuracy:0.3596\n",
    "Epoch #131: Loss:1.8409, Accuracy:0.3634, Validation Loss:1.8788, Validation Accuracy:0.3514\n",
    "Epoch #132: Loss:1.8439, Accuracy:0.3639, Validation Loss:1.8747, Validation Accuracy:0.3612\n",
    "Epoch #133: Loss:1.8400, Accuracy:0.3721, Validation Loss:1.8714, Validation Accuracy:0.3629\n",
    "Epoch #134: Loss:1.8399, Accuracy:0.3634, Validation Loss:1.8768, Validation Accuracy:0.3612\n",
    "Epoch #135: Loss:1.8371, Accuracy:0.3655, Validation Loss:1.8729, Validation Accuracy:0.3695\n",
    "Epoch #136: Loss:1.8408, Accuracy:0.3713, Validation Loss:1.8713, Validation Accuracy:0.3612\n",
    "Epoch #137: Loss:1.8352, Accuracy:0.3659, Validation Loss:1.8728, Validation Accuracy:0.3645\n",
    "Epoch #138: Loss:1.8325, Accuracy:0.3680, Validation Loss:1.8689, Validation Accuracy:0.3645\n",
    "Epoch #139: Loss:1.8372, Accuracy:0.3708, Validation Loss:1.8807, Validation Accuracy:0.3481\n",
    "Epoch #140: Loss:1.8392, Accuracy:0.3749, Validation Loss:1.8664, Validation Accuracy:0.3645\n",
    "Epoch #141: Loss:1.8319, Accuracy:0.3745, Validation Loss:1.8761, Validation Accuracy:0.3481\n",
    "Epoch #142: Loss:1.8305, Accuracy:0.3733, Validation Loss:1.8644, Validation Accuracy:0.3711\n",
    "Epoch #143: Loss:1.8290, Accuracy:0.3754, Validation Loss:1.8675, Validation Accuracy:0.3695\n",
    "Epoch #144: Loss:1.8294, Accuracy:0.3725, Validation Loss:1.8658, Validation Accuracy:0.3777\n",
    "Epoch #145: Loss:1.8315, Accuracy:0.3688, Validation Loss:1.8691, Validation Accuracy:0.3645\n",
    "Epoch #146: Loss:1.8302, Accuracy:0.3733, Validation Loss:1.8664, Validation Accuracy:0.3695\n",
    "Epoch #147: Loss:1.8276, Accuracy:0.3766, Validation Loss:1.8735, Validation Accuracy:0.3481\n",
    "Epoch #148: Loss:1.8278, Accuracy:0.3713, Validation Loss:1.8615, Validation Accuracy:0.3678\n",
    "Epoch #149: Loss:1.8266, Accuracy:0.3749, Validation Loss:1.8731, Validation Accuracy:0.3580\n",
    "Epoch #150: Loss:1.8260, Accuracy:0.3721, Validation Loss:1.8778, Validation Accuracy:0.3596\n",
    "Epoch #151: Loss:1.8395, Accuracy:0.3602, Validation Loss:1.8951, Validation Accuracy:0.3448\n",
    "Epoch #152: Loss:1.8471, Accuracy:0.3639, Validation Loss:1.8663, Validation Accuracy:0.3678\n",
    "Epoch #153: Loss:1.8391, Accuracy:0.3692, Validation Loss:1.8719, Validation Accuracy:0.3547\n",
    "Epoch #154: Loss:1.8301, Accuracy:0.3663, Validation Loss:1.8740, Validation Accuracy:0.3563\n",
    "Epoch #155: Loss:1.8268, Accuracy:0.3770, Validation Loss:1.8717, Validation Accuracy:0.3596\n",
    "Epoch #156: Loss:1.8248, Accuracy:0.3655, Validation Loss:1.8684, Validation Accuracy:0.3629\n",
    "Epoch #157: Loss:1.8351, Accuracy:0.3680, Validation Loss:1.8647, Validation Accuracy:0.3629\n",
    "Epoch #158: Loss:1.8318, Accuracy:0.3667, Validation Loss:1.8872, Validation Accuracy:0.3415\n",
    "Epoch #159: Loss:1.8305, Accuracy:0.3717, Validation Loss:1.8608, Validation Accuracy:0.3645\n",
    "Epoch #160: Loss:1.8258, Accuracy:0.3782, Validation Loss:1.8914, Validation Accuracy:0.3432\n",
    "Epoch #161: Loss:1.8256, Accuracy:0.3717, Validation Loss:1.8665, Validation Accuracy:0.3530\n",
    "Epoch #162: Loss:1.8202, Accuracy:0.3811, Validation Loss:1.8590, Validation Accuracy:0.3629\n",
    "Epoch #163: Loss:1.8141, Accuracy:0.3770, Validation Loss:1.8584, Validation Accuracy:0.3596\n",
    "Epoch #164: Loss:1.8161, Accuracy:0.3877, Validation Loss:1.8623, Validation Accuracy:0.3645\n",
    "Epoch #165: Loss:1.8190, Accuracy:0.3766, Validation Loss:1.8662, Validation Accuracy:0.3678\n",
    "Epoch #166: Loss:1.8222, Accuracy:0.3733, Validation Loss:1.8544, Validation Accuracy:0.3760\n",
    "Epoch #167: Loss:1.8138, Accuracy:0.3754, Validation Loss:1.8644, Validation Accuracy:0.3563\n",
    "Epoch #168: Loss:1.8165, Accuracy:0.3815, Validation Loss:1.8586, Validation Accuracy:0.3727\n",
    "Epoch #169: Loss:1.8282, Accuracy:0.3749, Validation Loss:1.8906, Validation Accuracy:0.3514\n",
    "Epoch #170: Loss:1.8232, Accuracy:0.3737, Validation Loss:1.8558, Validation Accuracy:0.3596\n",
    "Epoch #171: Loss:1.8115, Accuracy:0.3766, Validation Loss:1.8550, Validation Accuracy:0.3727\n",
    "Epoch #172: Loss:1.8112, Accuracy:0.3708, Validation Loss:1.8583, Validation Accuracy:0.3711\n",
    "Epoch #173: Loss:1.8092, Accuracy:0.3766, Validation Loss:1.8534, Validation Accuracy:0.3695\n",
    "Epoch #174: Loss:1.8064, Accuracy:0.3815, Validation Loss:1.8512, Validation Accuracy:0.3678\n",
    "Epoch #175: Loss:1.8084, Accuracy:0.3852, Validation Loss:1.8697, Validation Accuracy:0.3580\n",
    "Epoch #176: Loss:1.8093, Accuracy:0.3786, Validation Loss:1.8531, Validation Accuracy:0.3596\n",
    "Epoch #177: Loss:1.8066, Accuracy:0.3770, Validation Loss:1.8501, Validation Accuracy:0.3662\n",
    "Epoch #178: Loss:1.8041, Accuracy:0.3819, Validation Loss:1.8551, Validation Accuracy:0.3695\n",
    "Epoch #179: Loss:1.8013, Accuracy:0.3840, Validation Loss:1.8460, Validation Accuracy:0.3760\n",
    "Epoch #180: Loss:1.8002, Accuracy:0.3848, Validation Loss:1.8534, Validation Accuracy:0.3580\n",
    "Epoch #181: Loss:1.8025, Accuracy:0.3828, Validation Loss:1.8512, Validation Accuracy:0.3744\n",
    "Epoch #182: Loss:1.7991, Accuracy:0.3856, Validation Loss:1.8467, Validation Accuracy:0.3678\n",
    "Epoch #183: Loss:1.7994, Accuracy:0.3832, Validation Loss:1.8666, Validation Accuracy:0.3547\n",
    "Epoch #184: Loss:1.8009, Accuracy:0.3815, Validation Loss:1.8422, Validation Accuracy:0.3777\n",
    "Epoch #185: Loss:1.7998, Accuracy:0.3885, Validation Loss:1.8500, Validation Accuracy:0.3645\n",
    "Epoch #186: Loss:1.7978, Accuracy:0.3836, Validation Loss:1.8444, Validation Accuracy:0.3678\n",
    "Epoch #187: Loss:1.7961, Accuracy:0.3848, Validation Loss:1.8447, Validation Accuracy:0.3727\n",
    "Epoch #188: Loss:1.7967, Accuracy:0.3860, Validation Loss:1.8458, Validation Accuracy:0.3793\n",
    "Epoch #189: Loss:1.7949, Accuracy:0.3856, Validation Loss:1.8465, Validation Accuracy:0.3678\n",
    "Epoch #190: Loss:1.7930, Accuracy:0.3877, Validation Loss:1.8421, Validation Accuracy:0.3711\n",
    "Epoch #191: Loss:1.7921, Accuracy:0.3848, Validation Loss:1.8422, Validation Accuracy:0.3711\n",
    "Epoch #192: Loss:1.7916, Accuracy:0.3869, Validation Loss:1.8463, Validation Accuracy:0.3662\n",
    "Epoch #193: Loss:1.7964, Accuracy:0.3881, Validation Loss:1.8457, Validation Accuracy:0.3777\n",
    "Epoch #194: Loss:1.7943, Accuracy:0.3840, Validation Loss:1.8479, Validation Accuracy:0.3645\n",
    "Epoch #195: Loss:1.7896, Accuracy:0.3844, Validation Loss:1.8394, Validation Accuracy:0.3777\n",
    "Epoch #196: Loss:1.7903, Accuracy:0.3852, Validation Loss:1.8499, Validation Accuracy:0.3596\n",
    "Epoch #197: Loss:1.7982, Accuracy:0.3815, Validation Loss:1.8404, Validation Accuracy:0.3744\n",
    "Epoch #198: Loss:1.7940, Accuracy:0.3930, Validation Loss:1.8432, Validation Accuracy:0.3695\n",
    "Epoch #199: Loss:1.7867, Accuracy:0.3877, Validation Loss:1.8444, Validation Accuracy:0.3744\n",
    "Epoch #200: Loss:1.7871, Accuracy:0.3786, Validation Loss:1.8448, Validation Accuracy:0.3596\n",
    "Epoch #201: Loss:1.7946, Accuracy:0.3823, Validation Loss:1.8467, Validation Accuracy:0.3645\n",
    "Epoch #202: Loss:1.7912, Accuracy:0.3947, Validation Loss:1.8464, Validation Accuracy:0.3678\n",
    "Epoch #203: Loss:1.7900, Accuracy:0.3840, Validation Loss:1.8372, Validation Accuracy:0.3760\n",
    "Epoch #204: Loss:1.7855, Accuracy:0.3934, Validation Loss:1.8385, Validation Accuracy:0.3711\n",
    "Epoch #205: Loss:1.7820, Accuracy:0.3901, Validation Loss:1.8380, Validation Accuracy:0.3695\n",
    "Epoch #206: Loss:1.7850, Accuracy:0.3906, Validation Loss:1.8450, Validation Accuracy:0.3629\n",
    "Epoch #207: Loss:1.7866, Accuracy:0.3869, Validation Loss:1.8355, Validation Accuracy:0.3711\n",
    "Epoch #208: Loss:1.7852, Accuracy:0.3877, Validation Loss:1.8377, Validation Accuracy:0.3678\n",
    "Epoch #209: Loss:1.7824, Accuracy:0.3918, Validation Loss:1.8359, Validation Accuracy:0.3744\n",
    "Epoch #210: Loss:1.7852, Accuracy:0.3914, Validation Loss:1.8589, Validation Accuracy:0.3580\n",
    "Epoch #211: Loss:1.7850, Accuracy:0.3873, Validation Loss:1.8377, Validation Accuracy:0.3727\n",
    "Epoch #212: Loss:1.7799, Accuracy:0.3885, Validation Loss:1.8355, Validation Accuracy:0.3711\n",
    "Epoch #213: Loss:1.7781, Accuracy:0.3926, Validation Loss:1.8315, Validation Accuracy:0.3760\n",
    "Epoch #214: Loss:1.7758, Accuracy:0.3930, Validation Loss:1.8311, Validation Accuracy:0.3777\n",
    "Epoch #215: Loss:1.7743, Accuracy:0.3947, Validation Loss:1.8307, Validation Accuracy:0.3744\n",
    "Epoch #216: Loss:1.7750, Accuracy:0.3893, Validation Loss:1.8277, Validation Accuracy:0.3777\n",
    "Epoch #217: Loss:1.7719, Accuracy:0.3967, Validation Loss:1.8322, Validation Accuracy:0.3760\n",
    "Epoch #218: Loss:1.7750, Accuracy:0.3967, Validation Loss:1.8302, Validation Accuracy:0.3645\n",
    "Epoch #219: Loss:1.7775, Accuracy:0.3934, Validation Loss:1.8333, Validation Accuracy:0.3727\n",
    "Epoch #220: Loss:1.7852, Accuracy:0.3836, Validation Loss:1.8616, Validation Accuracy:0.3514\n",
    "Epoch #221: Loss:1.7865, Accuracy:0.3873, Validation Loss:1.8319, Validation Accuracy:0.3645\n",
    "Epoch #222: Loss:1.7828, Accuracy:0.4004, Validation Loss:1.8567, Validation Accuracy:0.3662\n",
    "Epoch #223: Loss:1.7772, Accuracy:0.3955, Validation Loss:1.8304, Validation Accuracy:0.3744\n",
    "Epoch #224: Loss:1.7767, Accuracy:0.3889, Validation Loss:1.8295, Validation Accuracy:0.3695\n",
    "Epoch #225: Loss:1.7718, Accuracy:0.3906, Validation Loss:1.8270, Validation Accuracy:0.3744\n",
    "Epoch #226: Loss:1.7723, Accuracy:0.3943, Validation Loss:1.8322, Validation Accuracy:0.3711\n",
    "Epoch #227: Loss:1.7711, Accuracy:0.3984, Validation Loss:1.8308, Validation Accuracy:0.3711\n",
    "Epoch #228: Loss:1.7677, Accuracy:0.3910, Validation Loss:1.8398, Validation Accuracy:0.3695\n",
    "Epoch #229: Loss:1.7712, Accuracy:0.3922, Validation Loss:1.8247, Validation Accuracy:0.3777\n",
    "Epoch #230: Loss:1.7647, Accuracy:0.3963, Validation Loss:1.8266, Validation Accuracy:0.3793\n",
    "Epoch #231: Loss:1.7642, Accuracy:0.4029, Validation Loss:1.8247, Validation Accuracy:0.3727\n",
    "Epoch #232: Loss:1.7641, Accuracy:0.3971, Validation Loss:1.8311, Validation Accuracy:0.3744\n",
    "Epoch #233: Loss:1.7644, Accuracy:0.3984, Validation Loss:1.8232, Validation Accuracy:0.3793\n",
    "Epoch #234: Loss:1.7630, Accuracy:0.3992, Validation Loss:1.8206, Validation Accuracy:0.3826\n",
    "Epoch #235: Loss:1.7660, Accuracy:0.3947, Validation Loss:1.8252, Validation Accuracy:0.3695\n",
    "Epoch #236: Loss:1.7618, Accuracy:0.3926, Validation Loss:1.8315, Validation Accuracy:0.3760\n",
    "Epoch #237: Loss:1.7685, Accuracy:0.3959, Validation Loss:1.8376, Validation Accuracy:0.3645\n",
    "Epoch #238: Loss:1.7699, Accuracy:0.3930, Validation Loss:1.8268, Validation Accuracy:0.3793\n",
    "Epoch #239: Loss:1.7720, Accuracy:0.3934, Validation Loss:1.8315, Validation Accuracy:0.3629\n",
    "Epoch #240: Loss:1.7776, Accuracy:0.3897, Validation Loss:1.8289, Validation Accuracy:0.3662\n",
    "Epoch #241: Loss:1.7675, Accuracy:0.4004, Validation Loss:1.8174, Validation Accuracy:0.3760\n",
    "Epoch #242: Loss:1.7656, Accuracy:0.3979, Validation Loss:1.8341, Validation Accuracy:0.3760\n",
    "Epoch #243: Loss:1.7558, Accuracy:0.3975, Validation Loss:1.8208, Validation Accuracy:0.3695\n",
    "Epoch #244: Loss:1.7563, Accuracy:0.3979, Validation Loss:1.8218, Validation Accuracy:0.3727\n",
    "Epoch #245: Loss:1.7530, Accuracy:0.4033, Validation Loss:1.8136, Validation Accuracy:0.3744\n",
    "Epoch #246: Loss:1.7587, Accuracy:0.4004, Validation Loss:1.8545, Validation Accuracy:0.3612\n",
    "Epoch #247: Loss:1.7691, Accuracy:0.3988, Validation Loss:1.8155, Validation Accuracy:0.3695\n",
    "Epoch #248: Loss:1.7593, Accuracy:0.4000, Validation Loss:1.8235, Validation Accuracy:0.3629\n",
    "Epoch #249: Loss:1.7526, Accuracy:0.3996, Validation Loss:1.8118, Validation Accuracy:0.3760\n",
    "Epoch #250: Loss:1.7495, Accuracy:0.4057, Validation Loss:1.8397, Validation Accuracy:0.3777\n",
    "Epoch #251: Loss:1.7517, Accuracy:0.4004, Validation Loss:1.8169, Validation Accuracy:0.3793\n",
    "Epoch #252: Loss:1.7489, Accuracy:0.4029, Validation Loss:1.8123, Validation Accuracy:0.3777\n",
    "Epoch #253: Loss:1.7494, Accuracy:0.4094, Validation Loss:1.8147, Validation Accuracy:0.3760\n",
    "Epoch #254: Loss:1.7447, Accuracy:0.4070, Validation Loss:1.8214, Validation Accuracy:0.3760\n",
    "Epoch #255: Loss:1.7508, Accuracy:0.3971, Validation Loss:1.8101, Validation Accuracy:0.3826\n",
    "Epoch #256: Loss:1.7431, Accuracy:0.4053, Validation Loss:1.8114, Validation Accuracy:0.3826\n",
    "Epoch #257: Loss:1.7446, Accuracy:0.4066, Validation Loss:1.8135, Validation Accuracy:0.3727\n",
    "Epoch #258: Loss:1.7450, Accuracy:0.4029, Validation Loss:1.8052, Validation Accuracy:0.3842\n",
    "Epoch #259: Loss:1.7410, Accuracy:0.4029, Validation Loss:1.8226, Validation Accuracy:0.3727\n",
    "Epoch #260: Loss:1.7443, Accuracy:0.4070, Validation Loss:1.8166, Validation Accuracy:0.3744\n",
    "Epoch #261: Loss:1.7419, Accuracy:0.4041, Validation Loss:1.8173, Validation Accuracy:0.3810\n",
    "Epoch #262: Loss:1.7363, Accuracy:0.4037, Validation Loss:1.8077, Validation Accuracy:0.3777\n",
    "Epoch #263: Loss:1.7389, Accuracy:0.4144, Validation Loss:1.8083, Validation Accuracy:0.3793\n",
    "Epoch #264: Loss:1.7355, Accuracy:0.4090, Validation Loss:1.8072, Validation Accuracy:0.3826\n",
    "Epoch #265: Loss:1.7356, Accuracy:0.4062, Validation Loss:1.8043, Validation Accuracy:0.3859\n",
    "Epoch #266: Loss:1.7344, Accuracy:0.4086, Validation Loss:1.8079, Validation Accuracy:0.3810\n",
    "Epoch #267: Loss:1.7370, Accuracy:0.4131, Validation Loss:1.8078, Validation Accuracy:0.3777\n",
    "Epoch #268: Loss:1.7319, Accuracy:0.4131, Validation Loss:1.8105, Validation Accuracy:0.3777\n",
    "Epoch #269: Loss:1.7335, Accuracy:0.4094, Validation Loss:1.8032, Validation Accuracy:0.3793\n",
    "Epoch #270: Loss:1.7312, Accuracy:0.4090, Validation Loss:1.8086, Validation Accuracy:0.3744\n",
    "Epoch #271: Loss:1.7293, Accuracy:0.4107, Validation Loss:1.8027, Validation Accuracy:0.3760\n",
    "Epoch #272: Loss:1.7242, Accuracy:0.4144, Validation Loss:1.8103, Validation Accuracy:0.3777\n",
    "Epoch #273: Loss:1.7298, Accuracy:0.4062, Validation Loss:1.8056, Validation Accuracy:0.3810\n",
    "Epoch #274: Loss:1.7296, Accuracy:0.4099, Validation Loss:1.8061, Validation Accuracy:0.3793\n",
    "Epoch #275: Loss:1.7268, Accuracy:0.4144, Validation Loss:1.7971, Validation Accuracy:0.3760\n",
    "Epoch #276: Loss:1.7217, Accuracy:0.4185, Validation Loss:1.8028, Validation Accuracy:0.3760\n",
    "Epoch #277: Loss:1.7191, Accuracy:0.4152, Validation Loss:1.7950, Validation Accuracy:0.3908\n",
    "Epoch #278: Loss:1.7195, Accuracy:0.4205, Validation Loss:1.7977, Validation Accuracy:0.3810\n",
    "Epoch #279: Loss:1.7197, Accuracy:0.4189, Validation Loss:1.8117, Validation Accuracy:0.3810\n",
    "Epoch #280: Loss:1.7262, Accuracy:0.4140, Validation Loss:1.7971, Validation Accuracy:0.3826\n",
    "Epoch #281: Loss:1.7249, Accuracy:0.4144, Validation Loss:1.7967, Validation Accuracy:0.3842\n",
    "Epoch #282: Loss:1.7153, Accuracy:0.4111, Validation Loss:1.7921, Validation Accuracy:0.3875\n",
    "Epoch #283: Loss:1.7200, Accuracy:0.4152, Validation Loss:1.7925, Validation Accuracy:0.3859\n",
    "Epoch #284: Loss:1.7161, Accuracy:0.4152, Validation Loss:1.7959, Validation Accuracy:0.3793\n",
    "Epoch #285: Loss:1.7147, Accuracy:0.4209, Validation Loss:1.8034, Validation Accuracy:0.3744\n",
    "Epoch #286: Loss:1.7132, Accuracy:0.4201, Validation Loss:1.7986, Validation Accuracy:0.3760\n",
    "Epoch #287: Loss:1.7116, Accuracy:0.4201, Validation Loss:1.7979, Validation Accuracy:0.3777\n",
    "Epoch #288: Loss:1.7079, Accuracy:0.4177, Validation Loss:1.7917, Validation Accuracy:0.3842\n",
    "Epoch #289: Loss:1.7049, Accuracy:0.4181, Validation Loss:1.7848, Validation Accuracy:0.3842\n",
    "Epoch #290: Loss:1.7077, Accuracy:0.4218, Validation Loss:1.7944, Validation Accuracy:0.3842\n",
    "Epoch #291: Loss:1.7089, Accuracy:0.4222, Validation Loss:1.7892, Validation Accuracy:0.3826\n",
    "Epoch #292: Loss:1.7039, Accuracy:0.4177, Validation Loss:1.7883, Validation Accuracy:0.3875\n",
    "Epoch #293: Loss:1.7020, Accuracy:0.4251, Validation Loss:1.7964, Validation Accuracy:0.3826\n",
    "Epoch #294: Loss:1.7036, Accuracy:0.4205, Validation Loss:1.7854, Validation Accuracy:0.3842\n",
    "Epoch #295: Loss:1.6974, Accuracy:0.4271, Validation Loss:1.7855, Validation Accuracy:0.3826\n",
    "Epoch #296: Loss:1.6976, Accuracy:0.4230, Validation Loss:1.7861, Validation Accuracy:0.3826\n",
    "Epoch #297: Loss:1.6986, Accuracy:0.4226, Validation Loss:1.7979, Validation Accuracy:0.3842\n",
    "Epoch #298: Loss:1.6986, Accuracy:0.4230, Validation Loss:1.7760, Validation Accuracy:0.3859\n",
    "Epoch #299: Loss:1.6945, Accuracy:0.4242, Validation Loss:1.7810, Validation Accuracy:0.3793\n",
    "Epoch #300: Loss:1.6967, Accuracy:0.4251, Validation Loss:1.7750, Validation Accuracy:0.3924\n",
    "\n",
    "Test:\n",
    "Test Loss:1.77499318, Accuracy:0.3924\n",
    "Labels: ['eb', 'yd', 'sg', 'sk', 'my', 'by', 'ib', 'aa', 'eg', 'ek', 'mb', 'eo', 'ce', 'ds', 'ck']\n",
    "Confusion Matrix:\n",
    "      eb  yd  sg  sk  my  by  ib  aa  eg  ek  mb  eo  ce  ds  ck\n",
    "t:eb  45   2   0   0   0   0   0   0   0   2   1   0   0   0   0\n",
    "t:yd   1  51   5   0   0   0   1   0   0   1   3   0   0   0   0\n",
    "t:sg   1  16  14   0   0   1  11   0   3   1   3   0   0   0   1\n",
    "t:sk  10   3   0   0   0   0   0   2   2   5   9   0   0   1   1\n",
    "t:my   4   1   1   1   0   0   0   1   3   2   5   0   0   2   0\n",
    "t:by   0   4  10   0   0  14   2   0   6   0   3   1   0   0   0\n",
    "t:ib   0   6   5   0   0   3  28   0   5   0   3   3   0   0   1\n",
    "t:aa   0   5   2   1   0   0   0  13   7   0   5   0   0   1   0\n",
    "t:eg   2   2   1   0   0   4   0   8  19   3   6   1   0   3   1\n",
    "t:ek  33   3   0   0   0   0   0   0   0   8   4   0   0   0   0\n",
    "t:mb   6   9   2   2   0   0   2   4   1   2  22   0   0   1   1\n",
    "t:eo   0   0   0   0   0   6   2   0   4   0   0  22   0   0   0\n",
    "t:ce   0   0   6   0   0   3   5   0   8   0   1   2   2   0   0\n",
    "t:ds  14   1   0   0   0   0   0   4   2   2   7   0   0   1   0\n",
    "t:ck   2   1   0   0   0   0   0   1   4   3  11   0   1   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eb       0.38      0.90      0.54        50\n",
    "          yd       0.49      0.82      0.61        62\n",
    "          sg       0.30      0.27      0.29        51\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          my       0.00      0.00      0.00        20\n",
    "          by       0.45      0.35      0.39        40\n",
    "          ib       0.55      0.52      0.53        54\n",
    "          aa       0.39      0.38      0.39        34\n",
    "          eg       0.30      0.38      0.33        50\n",
    "          ek       0.28      0.17      0.21        48\n",
    "          mb       0.27      0.42      0.33        52\n",
    "          eo       0.76      0.65      0.70        34\n",
    "          ce       0.67      0.07      0.13        27\n",
    "          ds       0.11      0.03      0.05        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.33      0.33      0.30       609\n",
    "weighted avg       0.35      0.39      0.34       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 02:42:16 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.696263604563445, 2.686153231778951, 2.6777968735530457, 2.6713628749346303, 2.6668034132282528, 2.664162380158999, 2.662659501794524, 2.661874118501134, 2.661338160582168, 2.6609441959994964, 2.6605568926518384, 2.660163199373067, 2.6596031220284195, 2.6588582143016244, 2.657754196909261, 2.6560705686829165, 2.6534431759751294, 2.6492398873534304, 2.6423492040148706, 2.6303872995580164, 2.6104680959422795, 2.5797416298651736, 2.5395467367469777, 2.4987324137601554, 2.465721008617107, 2.436852956640309, 2.413116459776028, 2.380266477321756, 2.352832223785726, 2.330512602340999, 2.3034196635968187, 2.2845343239788938, 2.2586868275171037, 2.248145666811462, 2.2238908957182284, 2.209429704105521, 2.190063769398456, 2.172091773773845, 2.158875481835727, 2.1495090599717765, 2.145559227525307, 2.121672915707668, 2.1099289156533225, 2.09222943755402, 2.0846153534887657, 2.0822000139452554, 2.067934737025419, 2.065484540607346, 2.057461260574792, 2.0488505660998215, 2.0426901056261486, 2.0392541854056625, 2.022847456297851, 2.02053389956407, 2.0191166256057413, 2.0131697541191462, 2.0040830481424314, 1.9992286509089479, 1.9961929078564071, 1.9924629368805533, 1.9900397537964318, 1.98728751431545, 1.9846094825193408, 1.975151041654139, 1.9736093366870349, 1.9716137289413678, 1.9667029075434643, 1.9766253501127897, 1.963941967937551, 1.9595053904553743, 1.9558436823595922, 1.9529291982525478, 1.9553742389177846, 1.9497095448238704, 1.9514079790788723, 1.9435179237465945, 1.9464821423998804, 1.9439026771116334, 1.9570716535320813, 1.9378564064138628, 1.9396872483255045, 1.9318573065774978, 1.94853915193398, 1.93189538445183, 1.944131042569729, 1.9277213710086492, 1.9334005256396014, 1.9233845928423903, 1.9245535368206856, 1.924323529445479, 1.9199765995022502, 1.9234848390463342, 1.9183422432744444, 1.9130262263694224, 1.9332597559113025, 1.9096397648891206, 1.9117334720927899, 1.9068959851570317, 1.908599911652175, 1.9099938671772898, 1.9040416302939354, 1.9034740194702775, 1.9137681458383946, 1.9048663113504796, 1.9110658125728612, 1.9030984281906353, 1.9130780148780209, 1.8992392323874487, 1.9070396067082196, 1.8954172901723576, 1.900940974553426, 1.8954956793824245, 1.891426180579588, 1.8920483548065712, 1.888909346168656, 1.8874722223955227, 1.8868581134893232, 1.884272609633961, 1.8850712065626247, 1.8847569258537982, 1.8879868911796407, 1.8824270556517226, 1.878799060882606, 1.8795497775665058, 1.8767365767255009, 1.8769372435430391, 1.8791242548201863, 1.8815449427305575, 1.8737707337722402, 1.8759863081041033, 1.8787961986851809, 1.8746647083113346, 1.8714266068046708, 1.8768367467842666, 1.8729281679945822, 1.8713376228445269, 1.8728368765810637, 1.8689461648953567, 1.8806598853986642, 1.866354379160651, 1.876085761341164, 1.8644233963563916, 1.8675430946553673, 1.8658156535895587, 1.8691005334869786, 1.8664024659174026, 1.8734771228580445, 1.8614916873878642, 1.8731193372181483, 1.8778164948540172, 1.8950971607699967, 1.8663421522807606, 1.871917227610383, 1.874036333831073, 1.8716905032864148, 1.8684238410739868, 1.8646576561168302, 1.887235979532765, 1.8608438097588926, 1.891409530232497, 1.8664922354060833, 1.8590069676463437, 1.8583958133296623, 1.8622686610433268, 1.866154017902556, 1.8544273995022076, 1.8644123093052254, 1.8586112642523103, 1.8905904197144783, 1.855838270805935, 1.8550420220458057, 1.8583383111922416, 1.8534010466683675, 1.8512324719201951, 1.8697360738353386, 1.8531236621155136, 1.8501443798318873, 1.8550576302414066, 1.8460305796076708, 1.8533875102480057, 1.8512353029940125, 1.8467438622257002, 1.8665922903662244, 1.842215300193561, 1.8500296081032463, 1.8444412795976661, 1.8446880013288927, 1.845779070321758, 1.8464710046896597, 1.8420574698346392, 1.842235842949064, 1.8463180583881822, 1.8456589707795819, 1.8479233839241742, 1.839449117923605, 1.8498529632299012, 1.8404427496670501, 1.8431687676064878, 1.8443869011742728, 1.8447545821639313, 1.8467293803523523, 1.8464166752027564, 1.8371681577857883, 1.8385053573570815, 1.8379999471611186, 1.8450048498332208, 1.8354832168870372, 1.83773882224642, 1.8359174370178448, 1.8589221434835925, 1.837695637947233, 1.8354916163461745, 1.8314955794361034, 1.8310574774671657, 1.8307401409681598, 1.8276731924861913, 1.8321756932927273, 1.8301926195523617, 1.8332654014596799, 1.8616149020312456, 1.8319028839101932, 1.8566935543943508, 1.8304211050027306, 1.8294730773700283, 1.8269589225255405, 1.8321922252135128, 1.8307533510800065, 1.8397697130056045, 1.8246569132374229, 1.8265812310874951, 1.8246953614631114, 1.831131251574737, 1.8232473477549938, 1.8206036159361916, 1.8251658400095547, 1.8314835849066673, 1.8376188152920827, 1.8268271218771222, 1.8315198000624457, 1.82893725764771, 1.8173571382641596, 1.8340690879790458, 1.8207796093669824, 1.821833137612429, 1.8136452688959432, 1.8544669531053315, 1.815512766783265, 1.823468380373687, 1.8118304002461174, 1.8397409643837188, 1.8168619009857303, 1.8122573578103227, 1.8146923533801376, 1.8213577464296313, 1.8101261948129814, 1.8114499967477984, 1.8135322567277354, 1.805173460290154, 1.8226170800198083, 1.8166272857506287, 1.817337077630956, 1.807708267703628, 1.808298238392534, 1.8071534340017534, 1.8042508025083244, 1.8079344572496336, 1.8077912514628645, 1.8105476536774283, 1.8031697059891298, 1.8086207363210092, 1.8026817355837141, 1.8103175691783135, 1.8055830076214519, 1.806085847672962, 1.7971222247983434, 1.8027690975928345, 1.7949721178984994, 1.7976632425546255, 1.8117087795620872, 1.797083678112437, 1.796665408536914, 1.7921492158877244, 1.7924981083971723, 1.795916857586314, 1.8033900258967834, 1.7986048767327871, 1.7978807278650344, 1.791687685084852, 1.7848086967844095, 1.7944019396708322, 1.7891798622306736, 1.7883398059162208, 1.79643702702765, 1.7854105662829771, 1.785465590668038, 1.786092314031128, 1.7978986106286887, 1.7760382338697687, 1.7809698102117955, 1.7749933491786714], 'val_acc': [0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.08538587798772775, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10509031158313767, 0.11822660058149563, 0.12315270895587987, 0.13957307020382734, 0.16091954021765092, 0.15763546796806144, 0.16584564750319827, 0.19047618937511945, 0.20197044333751957, 0.18390804487594048, 0.20525451558710905, 0.2495894907485871, 0.24630541820537868, 0.2577996715683068, 0.2561576349541472, 0.26272577964907207, 0.2561576350520201, 0.25779967127468784, 0.2561576352477661, 0.2725779965935865, 0.2676518881213293, 0.26765188831707526, 0.27422003063858047, 0.2643678159696128, 0.29392446442973635, 0.288998357939407, 0.2988505725105016, 0.3169129699811168, 0.29556650243858595, 0.31527093395419503, 0.3169129699811168, 0.3021346449558371, 0.31527093395419503, 0.3169129699811168, 0.3070607532323483, 0.32348111457816875, 0.32840722305042597, 0.31198686141098664, 0.3251231505072176, 0.32840722305042597, 0.3333333314248102, 0.3251231508008365, 0.33661740367439974, 0.3300492591752207, 0.34154351204878397, 0.3185550062037845, 0.33661740367439974, 0.3300492591752207, 0.3399014759239892, 0.3464696205210412, 0.334975367549605, 0.3382594398970674, 0.3415435122445299, 0.34482758439624644, 0.3464696206189142, 0.33497536764747793, 0.3431855482714517, 0.33990147602186216, 0.3431855482714517, 0.3448275844941194, 0.34811165664583593, 0.33990147582611624, 0.35303776511809315, 0.34318554836932463, 0.3579638733946044, 0.34154351204878397, 0.3546798012428879, 0.3382594397013215, 0.3579638734924774, 0.34154351214665696, 0.34811165664583593, 0.36124794574206687, 0.35303776511809315, 0.3497536927706307, 0.34811165664583593, 0.3563218374655556, 0.36124794574206687, 0.3382594398970674, 0.3612479456441939, 0.35303776521596614, 0.3596059096172721, 0.34154351204878397, 0.34154351214665696, 0.3596059097151451, 0.3431855481735787, 0.33497536764747793, 0.35632183756342856, 0.3563218374655556, 0.3415435122445299, 0.33661740357652675, 0.34154351214665696, 0.3628899819647346, 0.3513957289932984, 0.33169129530001545, 0.36945812646391357, 0.3513957289932984, 0.3513957289932984, 0.35632183736768264, 0.3628899819647346, 0.35632183726980965, 0.36453201808952934, 0.35303776511809315, 0.3513957288954254, 0.3497536929663766, 0.3579638733946044, 0.3628899819647346, 0.3661740541164511, 0.36781609024124584, 0.3628899818668616, 0.3579638733946044, 0.3579638734924774, 0.36124794574206687, 0.35960590951939914, 0.3513957289932984, 0.3612479456441939, 0.3628899817689886, 0.36124794574206687, 0.36945812646391357, 0.36124794574206687, 0.36453201789378337, 0.36453201799165635, 0.34811165684158185, 0.36453201789378337, 0.34811165664583593, 0.3711001625887083, 0.3694581263660406, 0.3776683069900143, 0.36453201789378337, 0.3694581262681676, 0.3481116567437089, 0.36781609014337285, 0.3579638734924774, 0.3596059097151451, 0.34482758439624644, 0.36781609024124584, 0.3546798013407609, 0.35632183717193666, 0.35960590942152615, 0.3628899818668616, 0.3628899817689886, 0.34154351204878397, 0.36453201799165635, 0.3431855482714517, 0.35303776511809315, 0.3628899818668616, 0.35960590951939914, 0.36453201808952934, 0.36781609033911883, 0.37602627076734657, 0.35632183736768264, 0.37274219871350306, 0.35139572879755243, 0.3596059096172721, 0.37274219861563007, 0.3711001625887083, 0.3694581262681676, 0.36781609014337285, 0.35796387359035037, 0.3596059096172721, 0.3661740541164511, 0.3694581263660406, 0.37602627096309255, 0.3579638734924774, 0.3743842347404248, 0.36781609033911883, 0.3546798012428879, 0.3776683068921413, 0.36453201808952934, 0.36781609014337285, 0.3727421985177571, 0.379310343408428, 0.36781609024124584, 0.37110016249083533, 0.37110016249083533, 0.3661740542143241, 0.3776683068921413, 0.36453201818740233, 0.3776683070878873, 0.3596059096172721, 0.3743842349361708, 0.36945812646391357, 0.3743842349361708, 0.3596059096172721, 0.36453201789378337, 0.3678160904369918, 0.37602627076734657, 0.37110016239296234, 0.3694581262681676, 0.3628899818668616, 0.3711001625887083, 0.36781609024124584, 0.3743842348382978, 0.3579638733946044, 0.37274219871350306, 0.37110016239296234, 0.37602627106096553, 0.3776683069900143, 0.3743842348382978, 0.3776683070878873, 0.37602627096309255, 0.3645320177959104, 0.37274219861563007, 0.35139572869967944, 0.36453201799165635, 0.3661740540185781, 0.3743842346425518, 0.3694581261702946, 0.3743842347404248, 0.3711001626865813, 0.37110016249083533, 0.36945812646391357, 0.3776683070878873, 0.37931034321268203, 0.37274219861563007, 0.3743842348382978, 0.37931034311480905, 0.3825944155601445, 0.3694581260724216, 0.37602627096309255, 0.36453201808952934, 0.379310343408428, 0.36288998157324265, 0.3661740540185781, 0.37602627096309255, 0.37602627106096553, 0.36945812646391357, 0.3727421985177571, 0.3743842347404248, 0.36124794574206687, 0.3694581263660406, 0.36288998157324265, 0.37602627096309255, 0.3776683071857603, 0.37931034311480905, 0.3776683067942683, 0.37602627096309255, 0.3760262711588385, 0.3825944155601445, 0.3825944155601445, 0.37274219861563007, 0.38423645168493925, 0.37274219881137605, 0.3743842349361708, 0.38095237963109574, 0.3776683070878873, 0.379310343408428, 0.3825944155601445, 0.385878487907607, 0.3809523792396038, 0.37766830728363326, 0.3776683069900143, 0.379310343310555, 0.3743842347404248, 0.37602627086521956, 0.37766830728363326, 0.38095237953322275, 0.379310343310555, 0.3760262711588385, 0.3760262711588385, 0.3908045962819912, 0.3809523793374768, 0.38095237943534976, 0.3825944156580175, 0.38423645158706626, 0.38752052393452874, 0.385878487809734, 0.379310343310555, 0.3743842348382978, 0.3760262711588385, 0.37766830728363326, 0.38423645168493925, 0.38423645168493925, 0.3842364518806852, 0.3825944156580175, 0.3875205241302747, 0.3825944155601445, 0.3842364518806852, 0.3825944157558905, 0.3825944156580175, 0.38423645178281224, 0.38587848800547997, 0.379310343506301, 0.39244663230891297], 'loss': [2.703642024739322, 2.692413638112971, 2.682951332754178, 2.6752768870007086, 2.670535426462945, 2.6661839111140133, 2.663681819257795, 2.6625453771751766, 2.6618479383065226, 2.6614652034438366, 2.661080005281515, 2.660624443579014, 2.6602852392490397, 2.659613739930139, 2.6587257481208817, 2.6574106609062493, 2.6554064824106267, 2.652324854911475, 2.647315730304444, 2.6385877401677003, 2.6237113984948066, 2.5984019472368933, 2.562074217218638, 2.5180814138917706, 2.4756929058803423, 2.439283436475593, 2.4068257512008384, 2.3821887246392346, 2.361824693816888, 2.3253985204246255, 2.304365573135, 2.2852598361655674, 2.2624525691939086, 2.247651493867565, 2.232184500762814, 2.2137907018406926, 2.196499181919764, 2.1821642240214887, 2.1642234105104294, 2.157291102947885, 2.1396937438839516, 2.137218898816275, 2.118863106557231, 2.1048330946379865, 2.092051588974939, 2.082980049513204, 2.072535949859776, 2.064907390821641, 2.0578936791273112, 2.0525822197876917, 2.043153009081768, 2.033491782926681, 2.029748831245689, 2.022532432280037, 2.0141299614916104, 2.010985537280292, 2.0051103250201967, 1.995894133530603, 1.991402355209758, 1.9873694851657937, 1.9810130690157537, 1.9798187888868046, 1.9765657756852417, 1.9699143299087116, 1.9663092136872622, 1.9630968645123241, 1.959826252886402, 1.957445244622671, 1.9564927307732052, 1.9486760455724885, 1.944356049257627, 1.9426302703743843, 1.939744569143965, 1.9415190462703822, 1.937031541223154, 1.930960243242722, 1.930086150551234, 1.9310754179709746, 1.9246780630009865, 1.9348424044477865, 1.9245999427791494, 1.9174492813478505, 1.9179007897876372, 1.919981513473777, 1.914656832967206, 1.9123945682200563, 1.9108617994085229, 1.9081269980945625, 1.9075767624794335, 1.8987997296409684, 1.9018990671854978, 1.8942492256419126, 1.8969730528962685, 1.892114710269278, 1.8927151438636702, 1.8971436314024719, 1.8882312099057301, 1.8837056916352415, 1.8817745087817463, 1.88176515195404, 1.8800093970504386, 1.8775018467795432, 1.8767472438009367, 1.8807926743427097, 1.8776223363817595, 1.8776732739481838, 1.876262432398003, 1.879747426583292, 1.8739967471030703, 1.8723531721553763, 1.8667278405822034, 1.8682196635729968, 1.8607429192051506, 1.8626930852200705, 1.8635955688644974, 1.8580988967198366, 1.8558425242895953, 1.8537758672996223, 1.857371387687307, 1.8543992639811866, 1.8523913234166296, 1.8542239834395766, 1.848860911862806, 1.8476156992589179, 1.8455410783540542, 1.844727014809908, 1.8443180886626978, 1.8453302952298394, 1.8438128943316012, 1.8417786820958038, 1.8408605263708064, 1.8438606471251657, 1.840006099442437, 1.8398584655667722, 1.8370993991162499, 1.8408489335978546, 1.8351583224302446, 1.8325093355022173, 1.8372269835070663, 1.839247910100087, 1.8318927793287398, 1.8305456346554922, 1.8289690213036978, 1.8293587189190685, 1.8314732604937387, 1.8302211996466227, 1.8276191238994717, 1.8277658259844143, 1.826611249158025, 1.8259575282278981, 1.8394513245725534, 1.8471441893117384, 1.8391125640340409, 1.8300783462837735, 1.8267823987917733, 1.8248394542161444, 1.8351416376337133, 1.8318345265711602, 1.8305042123402904, 1.8257782514579977, 1.8256388929100742, 1.8202123592032056, 1.8140560151125615, 1.8161124947624285, 1.8189773961014326, 1.8221642404611105, 1.8138099673837118, 1.8165118243415252, 1.828215157716426, 1.8232271246841556, 1.8114562596138988, 1.8112104861888063, 1.8092314691758988, 1.8064286485100185, 1.8083709686443792, 1.809334818097845, 1.8066087586679007, 1.8040535100921224, 1.8013309671648718, 1.800183555526655, 1.8024527020522947, 1.799122821232132, 1.7993627010674447, 1.8008793778488035, 1.7997805482308233, 1.7978317419851095, 1.7960644478670627, 1.796678315491647, 1.7949311793462452, 1.7929762732566505, 1.792057857180523, 1.791639043760985, 1.7963520729566256, 1.7942729912744166, 1.7896306855477837, 1.7903487509036211, 1.7982329622675997, 1.7940149269064838, 1.7867353774683676, 1.7871287535837788, 1.7946159812214437, 1.7912205320119368, 1.7899833992521377, 1.7855025653721615, 1.7819621931111298, 1.7850406258992346, 1.7865715427809916, 1.7852348059844187, 1.7823664472333214, 1.7852339767087901, 1.7849618967553673, 1.7799278721917091, 1.7781115304273256, 1.7757667128799877, 1.7742750200647592, 1.7750482431916974, 1.7719056990112367, 1.775008540476617, 1.7775194813828203, 1.7852139991411684, 1.7865125297765712, 1.7827902900609638, 1.777162582086097, 1.7767242981423097, 1.7717643580152758, 1.7722603830223944, 1.7711344352248268, 1.7676522141854132, 1.7712140858785328, 1.7646762694666274, 1.7642040480823242, 1.764133258766707, 1.7643837470538317, 1.7630042316977248, 1.766045823038481, 1.761828163321258, 1.7685197884542008, 1.7698834238600683, 1.7719908798499764, 1.7776311037232009, 1.7674693969240913, 1.7656336949836058, 1.7557809001856026, 1.7562729259291223, 1.7529511378775877, 1.7586981215271371, 1.7690732646037421, 1.759273124573412, 1.752553923223053, 1.749545058136848, 1.751713223966485, 1.7488818399225663, 1.7494005198840978, 1.7447213010376728, 1.7508276164898882, 1.743052799354099, 1.7445805116355786, 1.7450405779315705, 1.7409599359520163, 1.7443294903580902, 1.7418898942289411, 1.7362772951870238, 1.7388830565818771, 1.7355356638926012, 1.7356369949219408, 1.7343600361743747, 1.7369590850336596, 1.7318860449095772, 1.7334574671986167, 1.731235817962114, 1.7292708995161115, 1.7241700279639243, 1.729772344898639, 1.729599962782811, 1.7267598190346782, 1.7216738206405169, 1.719122829623291, 1.719500612674063, 1.7197268578061333, 1.72624912188528, 1.7249218406618498, 1.7152715267341974, 1.7200455988701853, 1.716108347356197, 1.714723341371979, 1.7132393142770692, 1.7116052700019226, 1.707911532419663, 1.7049091369464413, 1.7076545185621759, 1.7088830571889388, 1.7039025468747964, 1.7019757902842527, 1.70361680612427, 1.6973606410212585, 1.6975864259614102, 1.6986052913587442, 1.6985843176714448, 1.6945444881548872, 1.696709721582871], 'acc': [0.10225872664848147, 0.10225872684430783, 0.10225872663930212, 0.08213552377766163, 0.0891170428702474, 0.10225872704931353, 0.10225872644347576, 0.10225872722678116, 0.10225872723596052, 0.1022587272176018, 0.10225872683512846, 0.10225872664848147, 0.10225872664848147, 0.1022587272176018, 0.10225872682594911, 0.10225872663930212, 0.10266940484485097, 0.10349075969851727, 0.1080082126772624, 0.1190965087843382, 0.12813141713764142, 0.14291581158398114, 0.15482546289965846, 0.15523613914694384, 0.16016427036795527, 0.18439425140068516, 0.18357289616454553, 0.18275153953926274, 0.18973305941974358, 0.23367556396573477, 0.23408624138797823, 0.22751540143876595, 0.23531827424218768, 0.23490759817236992, 0.2381930185294494, 0.2410677626392435, 0.24147843910071395, 0.24188911807120947, 0.24681724890056822, 0.25626283254956317, 0.2562628348994794, 0.2620123217482831, 0.2731006138745764, 0.27227926259168117, 0.29034907461681403, 0.28788500969170056, 0.29815194896848785, 0.30308008175611006, 0.2944558535757985, 0.3063655046589321, 0.31334702338281356, 0.30800821160633707, 0.3108829590451791, 0.3133470209961799, 0.3195071864789027, 0.31704312312039995, 0.3190965098399646, 0.3232032840623993, 0.32648870418693493, 0.32361396086044625, 0.3240246420401078, 0.3260780267646915, 0.3264887077485267, 0.330184805723676, 0.3305954803676331, 0.32936345029171, 0.3268993845832911, 0.3297741290847379, 0.32854209466391765, 0.333880905265436, 0.33141683916536446, 0.33716632620999454, 0.3351129347538801, 0.3367556468294876, 0.33059548236261405, 0.33511293494970645, 0.3334702276473662, 0.3363449709738549, 0.3425051356365548, 0.32854209227728404, 0.3285420958388757, 0.33716632620999454, 0.33511293714051377, 0.3359342929641324, 0.3416837794212835, 0.34250513387411774, 0.3412731027823454, 0.3420944570393533, 0.34168378000876254, 0.3462012316167232, 0.3445585215360967, 0.3420944578226587, 0.3425051346574231, 0.3408624218352277, 0.34004106636654424, 0.3507186860396877, 0.3470225886520174, 0.35441478299898777, 0.35359342917035, 0.3486652992834056, 0.35523613745182203, 0.35195072026468155, 0.3552361416008928, 0.35154004045580445, 0.35236139572866154, 0.35441478299898777, 0.35195071729056887, 0.353182753118891, 0.35482546280786487, 0.3552361386267801, 0.35564681781146074, 0.35482546159618933, 0.3548254641419319, 0.36098562469227846, 0.3609856270789121, 0.3577002031602409, 0.356468173439253, 0.36098562469227846, 0.35934291382834654, 0.35934291441582555, 0.35975359125059, 0.35934291539495733, 0.36221766149973234, 0.3630390145450647, 0.36427104939425503, 0.3663244362729286, 0.3622176611080796, 0.3671457905299365, 0.3675564701062698, 0.36837782220918786, 0.3634496897765009, 0.36386036883879, 0.372073922925906, 0.3634496929831573, 0.3655030814284417, 0.3712525685097892, 0.36591375650076896, 0.367967144003639, 0.3708418877217804, 0.37494866390247855, 0.3745379890259776, 0.37330595578011544, 0.37535934152054834, 0.37248460015232315, 0.3687885004147367, 0.3733059558168329, 0.3765913765288476, 0.37125256671063467, 0.37494866586074205, 0.3720739205759898, 0.36016427180605504, 0.3638603709561624, 0.3691991784244592, 0.36632443568544953, 0.37700205516276664, 0.3655030782952201, 0.36796714439529166, 0.3667351149068476, 0.3716632441328781, 0.37823408719695323, 0.37166324432870446, 0.38110882915265754, 0.3770020557502457, 0.38767967143342724, 0.37659137457058417, 0.3733059533934818, 0.37535934488631373, 0.38151950716238003, 0.37494866605656835, 0.37371663418149065, 0.376591374142214, 0.37084188952093494, 0.37659137617391236, 0.3815195069665537, 0.38521560533335564, 0.37864476344423864, 0.3770020515644575, 0.3819301838013181, 0.3839835724791462, 0.38480492810693856, 0.38275154158320024, 0.3856262829514255, 0.3831622168513539, 0.38151950837405557, 0.38850102741615483, 0.3835728930619218, 0.38480492732363314, 0.38603696115697433, 0.38562628451803627, 0.38767967221673266, 0.3848049283027649, 0.3868583185472038, 0.3880903476439952, 0.3839835715000145, 0.3843942504888687, 0.38521560513752934, 0.38151950618324826, 0.39301848203494566, 0.3876796706501219, 0.3786447657941548, 0.3823408604035387, 0.3946611928988776, 0.3839835726749726, 0.39342915926136274, 0.39014373514686523, 0.39055441652235307, 0.38685831835137746, 0.3876796706501219, 0.39178644780995175, 0.3913757711710137, 0.3872689947944892, 0.38850102725704594, 0.3926078026544387, 0.39301848062744377, 0.3946611927030512, 0.38932237971489925, 0.39671458079340033, 0.3967145794226159, 0.39342915609142376, 0.38357289544855544, 0.3872689947944892, 0.4004106785727233, 0.3954825459809274, 0.3889117034676139, 0.3905544133891315, 0.39425051312671794, 0.3983572916573323, 0.39096509038300487, 0.39219712425306347, 0.3963039031753305, 0.40287474428114217, 0.397125256649033, 0.39835728750826155, 0.3991786461101665, 0.3946611907080703, 0.39260780461270217, 0.39589322179984265, 0.3930184810558139, 0.39342915609142376, 0.3897330583121008, 0.4004106766144598, 0.39794661149351995, 0.3975359348545819, 0.3979466103185618, 0.40328542150755925, 0.4004106755986106, 0.3987679663012894, 0.39999999977969536, 0.3995893245115417, 0.40574948799928356, 0.4004106775935915, 0.4028747440853158, 0.4094455865619119, 0.40698151987436126, 0.3971252583747527, 0.40533881116451914, 0.4065708416688124, 0.4028747444769685, 0.40287474291035774, 0.40698151987436126, 0.4041067763520462, 0.40369609638406023, 0.41437371755037955, 0.4090349063613821, 0.406160165421527, 0.40862422776418056, 0.4131416839495821, 0.41314168571201926, 0.40944558754104365, 0.40903490734051384, 0.41067762000360036, 0.4143737169629005, 0.4061601634265461, 0.40985626515911344, 0.41437371520046334, 0.4184804923602933, 0.4151950720399312, 0.42053388299638483, 0.418891168803405, 0.41396303797404627, 0.414373718921164, 0.41108829703419114, 0.41519507102408204, 0.415195070436603, 0.4209445570895804, 0.4201232054150325, 0.42012320126596175, 0.4176591397066136, 0.4180698169330307, 0.42176591432070093, 0.42217659350538156, 0.4176591397066136, 0.42505133683187024, 0.4205338820172531, 0.42710472096897495, 0.42299794498410315, 0.42258727112345135, 0.4229979451799295, 0.424229978621618, 0.4250513360118474]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
