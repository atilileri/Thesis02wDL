{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf20.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 12:17:14 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '0', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '03', '05', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000026905AABE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000026902246EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6164, Accuracy:0.1893, Validation Loss:1.6099, Validation Accuracy:0.1987\n",
    "Epoch #2: Loss:1.6089, Accuracy:0.2111, Validation Loss:1.6078, Validation Accuracy:0.2315\n",
    "Epoch #3: Loss:1.6078, Accuracy:0.2333, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6070, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6023, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6019, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2348\n",
    "Epoch #17: Loss:1.6014, Accuracy:0.2361, Validation Loss:1.6051, Validation Accuracy:0.2496\n",
    "Epoch #18: Loss:1.6006, Accuracy:0.2357, Validation Loss:1.6049, Validation Accuracy:0.2496\n",
    "Epoch #19: Loss:1.6005, Accuracy:0.2370, Validation Loss:1.6053, Validation Accuracy:0.2479\n",
    "Epoch #20: Loss:1.6014, Accuracy:0.2337, Validation Loss:1.6055, Validation Accuracy:0.2512\n",
    "Epoch #21: Loss:1.6004, Accuracy:0.2382, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6005, Accuracy:0.2460, Validation Loss:1.6060, Validation Accuracy:0.2315\n",
    "Epoch #23: Loss:1.6011, Accuracy:0.2407, Validation Loss:1.6061, Validation Accuracy:0.2414\n",
    "Epoch #24: Loss:1.6002, Accuracy:0.2415, Validation Loss:1.6067, Validation Accuracy:0.2397\n",
    "Epoch #25: Loss:1.5999, Accuracy:0.2394, Validation Loss:1.6070, Validation Accuracy:0.2365\n",
    "Epoch #26: Loss:1.6003, Accuracy:0.2407, Validation Loss:1.6067, Validation Accuracy:0.2348\n",
    "Epoch #27: Loss:1.5997, Accuracy:0.2407, Validation Loss:1.6064, Validation Accuracy:0.2365\n",
    "Epoch #28: Loss:1.5996, Accuracy:0.2415, Validation Loss:1.6065, Validation Accuracy:0.2381\n",
    "Epoch #29: Loss:1.5995, Accuracy:0.2411, Validation Loss:1.6067, Validation Accuracy:0.2381\n",
    "Epoch #30: Loss:1.5996, Accuracy:0.2394, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #31: Loss:1.5993, Accuracy:0.2394, Validation Loss:1.6072, Validation Accuracy:0.2430\n",
    "Epoch #32: Loss:1.5992, Accuracy:0.2419, Validation Loss:1.6072, Validation Accuracy:0.2414\n",
    "Epoch #33: Loss:1.5989, Accuracy:0.2415, Validation Loss:1.6070, Validation Accuracy:0.2397\n",
    "Epoch #34: Loss:1.5993, Accuracy:0.2435, Validation Loss:1.6069, Validation Accuracy:0.2348\n",
    "Epoch #35: Loss:1.5996, Accuracy:0.2439, Validation Loss:1.6076, Validation Accuracy:0.2430\n",
    "Epoch #36: Loss:1.5996, Accuracy:0.2427, Validation Loss:1.6069, Validation Accuracy:0.2430\n",
    "Epoch #37: Loss:1.5995, Accuracy:0.2407, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #38: Loss:1.5996, Accuracy:0.2431, Validation Loss:1.6075, Validation Accuracy:0.2348\n",
    "Epoch #39: Loss:1.5993, Accuracy:0.2435, Validation Loss:1.6069, Validation Accuracy:0.2430\n",
    "Epoch #40: Loss:1.5993, Accuracy:0.2427, Validation Loss:1.6066, Validation Accuracy:0.2463\n",
    "Epoch #41: Loss:1.5991, Accuracy:0.2439, Validation Loss:1.6068, Validation Accuracy:0.2348\n",
    "Epoch #42: Loss:1.5990, Accuracy:0.2468, Validation Loss:1.6070, Validation Accuracy:0.2315\n",
    "Epoch #43: Loss:1.5993, Accuracy:0.2452, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #44: Loss:1.5989, Accuracy:0.2407, Validation Loss:1.6055, Validation Accuracy:0.2414\n",
    "Epoch #45: Loss:1.5987, Accuracy:0.2419, Validation Loss:1.6065, Validation Accuracy:0.2381\n",
    "Epoch #46: Loss:1.5984, Accuracy:0.2423, Validation Loss:1.6065, Validation Accuracy:0.2348\n",
    "Epoch #47: Loss:1.5984, Accuracy:0.2452, Validation Loss:1.6064, Validation Accuracy:0.2348\n",
    "Epoch #48: Loss:1.5983, Accuracy:0.2431, Validation Loss:1.6064, Validation Accuracy:0.2414\n",
    "Epoch #49: Loss:1.5982, Accuracy:0.2415, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #50: Loss:1.5982, Accuracy:0.2448, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #51: Loss:1.5980, Accuracy:0.2456, Validation Loss:1.6074, Validation Accuracy:0.2348\n",
    "Epoch #52: Loss:1.5979, Accuracy:0.2439, Validation Loss:1.6074, Validation Accuracy:0.2282\n",
    "Epoch #53: Loss:1.5978, Accuracy:0.2431, Validation Loss:1.6078, Validation Accuracy:0.2266\n",
    "Epoch #54: Loss:1.5975, Accuracy:0.2435, Validation Loss:1.6078, Validation Accuracy:0.2381\n",
    "Epoch #55: Loss:1.5975, Accuracy:0.2411, Validation Loss:1.6075, Validation Accuracy:0.2332\n",
    "Epoch #56: Loss:1.5976, Accuracy:0.2407, Validation Loss:1.6081, Validation Accuracy:0.2315\n",
    "Epoch #57: Loss:1.5975, Accuracy:0.2427, Validation Loss:1.6078, Validation Accuracy:0.2332\n",
    "Epoch #58: Loss:1.5973, Accuracy:0.2439, Validation Loss:1.6081, Validation Accuracy:0.2430\n",
    "Epoch #59: Loss:1.5994, Accuracy:0.2411, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #60: Loss:1.5982, Accuracy:0.2382, Validation Loss:1.6060, Validation Accuracy:0.2529\n",
    "Epoch #61: Loss:1.5986, Accuracy:0.2431, Validation Loss:1.6069, Validation Accuracy:0.2365\n",
    "Epoch #62: Loss:1.5970, Accuracy:0.2444, Validation Loss:1.6061, Validation Accuracy:0.2479\n",
    "Epoch #63: Loss:1.5977, Accuracy:0.2423, Validation Loss:1.6067, Validation Accuracy:0.2414\n",
    "Epoch #64: Loss:1.5971, Accuracy:0.2374, Validation Loss:1.6075, Validation Accuracy:0.2348\n",
    "Epoch #65: Loss:1.5975, Accuracy:0.2394, Validation Loss:1.6074, Validation Accuracy:0.2381\n",
    "Epoch #66: Loss:1.5970, Accuracy:0.2398, Validation Loss:1.6076, Validation Accuracy:0.2430\n",
    "Epoch #67: Loss:1.5968, Accuracy:0.2415, Validation Loss:1.6069, Validation Accuracy:0.2414\n",
    "Epoch #68: Loss:1.5974, Accuracy:0.2382, Validation Loss:1.6081, Validation Accuracy:0.2348\n",
    "Epoch #69: Loss:1.5966, Accuracy:0.2448, Validation Loss:1.6080, Validation Accuracy:0.2496\n",
    "Epoch #70: Loss:1.5966, Accuracy:0.2394, Validation Loss:1.6086, Validation Accuracy:0.2381\n",
    "Epoch #71: Loss:1.5969, Accuracy:0.2419, Validation Loss:1.6086, Validation Accuracy:0.2365\n",
    "Epoch #72: Loss:1.5973, Accuracy:0.2394, Validation Loss:1.6089, Validation Accuracy:0.2479\n",
    "Epoch #73: Loss:1.5976, Accuracy:0.2390, Validation Loss:1.6086, Validation Accuracy:0.2332\n",
    "Epoch #74: Loss:1.5975, Accuracy:0.2378, Validation Loss:1.6087, Validation Accuracy:0.2463\n",
    "Epoch #75: Loss:1.5975, Accuracy:0.2374, Validation Loss:1.6090, Validation Accuracy:0.2250\n",
    "Epoch #76: Loss:1.5972, Accuracy:0.2402, Validation Loss:1.6081, Validation Accuracy:0.2463\n",
    "Epoch #77: Loss:1.5971, Accuracy:0.2402, Validation Loss:1.6081, Validation Accuracy:0.2447\n",
    "Epoch #78: Loss:1.5970, Accuracy:0.2382, Validation Loss:1.6077, Validation Accuracy:0.2447\n",
    "Epoch #79: Loss:1.5967, Accuracy:0.2398, Validation Loss:1.6077, Validation Accuracy:0.2447\n",
    "Epoch #80: Loss:1.5965, Accuracy:0.2423, Validation Loss:1.6084, Validation Accuracy:0.2397\n",
    "Epoch #81: Loss:1.5970, Accuracy:0.2370, Validation Loss:1.6098, Validation Accuracy:0.2233\n",
    "Epoch #82: Loss:1.5967, Accuracy:0.2415, Validation Loss:1.6096, Validation Accuracy:0.2447\n",
    "Epoch #83: Loss:1.5970, Accuracy:0.2411, Validation Loss:1.6096, Validation Accuracy:0.2447\n",
    "Epoch #84: Loss:1.5964, Accuracy:0.2415, Validation Loss:1.6106, Validation Accuracy:0.2184\n",
    "Epoch #85: Loss:1.5966, Accuracy:0.2435, Validation Loss:1.6092, Validation Accuracy:0.2397\n",
    "Epoch #86: Loss:1.5957, Accuracy:0.2394, Validation Loss:1.6089, Validation Accuracy:0.2447\n",
    "Epoch #87: Loss:1.5961, Accuracy:0.2431, Validation Loss:1.6093, Validation Accuracy:0.2447\n",
    "Epoch #88: Loss:1.5964, Accuracy:0.2398, Validation Loss:1.6091, Validation Accuracy:0.2447\n",
    "Epoch #89: Loss:1.5956, Accuracy:0.2427, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #90: Loss:1.5958, Accuracy:0.2452, Validation Loss:1.6096, Validation Accuracy:0.2365\n",
    "Epoch #91: Loss:1.5957, Accuracy:0.2431, Validation Loss:1.6098, Validation Accuracy:0.2414\n",
    "Epoch #92: Loss:1.5953, Accuracy:0.2452, Validation Loss:1.6096, Validation Accuracy:0.2430\n",
    "Epoch #93: Loss:1.5959, Accuracy:0.2407, Validation Loss:1.6097, Validation Accuracy:0.2282\n",
    "Epoch #94: Loss:1.5967, Accuracy:0.2489, Validation Loss:1.6102, Validation Accuracy:0.2184\n",
    "Epoch #95: Loss:1.5945, Accuracy:0.2464, Validation Loss:1.6107, Validation Accuracy:0.2512\n",
    "Epoch #96: Loss:1.5962, Accuracy:0.2407, Validation Loss:1.6100, Validation Accuracy:0.2414\n",
    "Epoch #97: Loss:1.5971, Accuracy:0.2366, Validation Loss:1.6112, Validation Accuracy:0.2184\n",
    "Epoch #98: Loss:1.5950, Accuracy:0.2423, Validation Loss:1.6103, Validation Accuracy:0.2348\n",
    "Epoch #99: Loss:1.5953, Accuracy:0.2374, Validation Loss:1.6092, Validation Accuracy:0.2414\n",
    "Epoch #100: Loss:1.5949, Accuracy:0.2489, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #101: Loss:1.5945, Accuracy:0.2464, Validation Loss:1.6091, Validation Accuracy:0.2430\n",
    "Epoch #102: Loss:1.5946, Accuracy:0.2435, Validation Loss:1.6089, Validation Accuracy:0.2381\n",
    "Epoch #103: Loss:1.5940, Accuracy:0.2550, Validation Loss:1.6093, Validation Accuracy:0.2447\n",
    "Epoch #104: Loss:1.5941, Accuracy:0.2493, Validation Loss:1.6095, Validation Accuracy:0.2430\n",
    "Epoch #105: Loss:1.5943, Accuracy:0.2460, Validation Loss:1.6100, Validation Accuracy:0.2430\n",
    "Epoch #106: Loss:1.5940, Accuracy:0.2452, Validation Loss:1.6097, Validation Accuracy:0.2282\n",
    "Epoch #107: Loss:1.5946, Accuracy:0.2534, Validation Loss:1.6098, Validation Accuracy:0.2365\n",
    "Epoch #108: Loss:1.5940, Accuracy:0.2501, Validation Loss:1.6101, Validation Accuracy:0.2430\n",
    "Epoch #109: Loss:1.5940, Accuracy:0.2460, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #110: Loss:1.5934, Accuracy:0.2513, Validation Loss:1.6096, Validation Accuracy:0.2348\n",
    "Epoch #111: Loss:1.5933, Accuracy:0.2497, Validation Loss:1.6096, Validation Accuracy:0.2545\n",
    "Epoch #112: Loss:1.5931, Accuracy:0.2472, Validation Loss:1.6095, Validation Accuracy:0.2479\n",
    "Epoch #113: Loss:1.5948, Accuracy:0.2435, Validation Loss:1.6097, Validation Accuracy:0.2348\n",
    "Epoch #114: Loss:1.5937, Accuracy:0.2468, Validation Loss:1.6103, Validation Accuracy:0.2463\n",
    "Epoch #115: Loss:1.5931, Accuracy:0.2505, Validation Loss:1.6097, Validation Accuracy:0.2381\n",
    "Epoch #116: Loss:1.5944, Accuracy:0.2415, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #117: Loss:1.5939, Accuracy:0.2439, Validation Loss:1.6103, Validation Accuracy:0.2463\n",
    "Epoch #118: Loss:1.5932, Accuracy:0.2480, Validation Loss:1.6099, Validation Accuracy:0.2381\n",
    "Epoch #119: Loss:1.5935, Accuracy:0.2448, Validation Loss:1.6107, Validation Accuracy:0.2036\n",
    "Epoch #120: Loss:1.5935, Accuracy:0.2505, Validation Loss:1.6116, Validation Accuracy:0.2496\n",
    "Epoch #121: Loss:1.5932, Accuracy:0.2567, Validation Loss:1.6114, Validation Accuracy:0.2365\n",
    "Epoch #122: Loss:1.5925, Accuracy:0.2480, Validation Loss:1.6096, Validation Accuracy:0.2447\n",
    "Epoch #123: Loss:1.5925, Accuracy:0.2464, Validation Loss:1.6109, Validation Accuracy:0.2299\n",
    "Epoch #124: Loss:1.5929, Accuracy:0.2517, Validation Loss:1.6107, Validation Accuracy:0.2365\n",
    "Epoch #125: Loss:1.5932, Accuracy:0.2452, Validation Loss:1.6095, Validation Accuracy:0.2529\n",
    "Epoch #126: Loss:1.5929, Accuracy:0.2513, Validation Loss:1.6082, Validation Accuracy:0.2479\n",
    "Epoch #127: Loss:1.5924, Accuracy:0.2448, Validation Loss:1.6075, Validation Accuracy:0.2200\n",
    "Epoch #128: Loss:1.5920, Accuracy:0.2423, Validation Loss:1.6083, Validation Accuracy:0.2348\n",
    "Epoch #129: Loss:1.5917, Accuracy:0.2415, Validation Loss:1.6085, Validation Accuracy:0.2365\n",
    "Epoch #130: Loss:1.5908, Accuracy:0.2435, Validation Loss:1.6080, Validation Accuracy:0.2463\n",
    "Epoch #131: Loss:1.5925, Accuracy:0.2489, Validation Loss:1.6084, Validation Accuracy:0.2266\n",
    "Epoch #132: Loss:1.5928, Accuracy:0.2505, Validation Loss:1.6088, Validation Accuracy:0.2463\n",
    "Epoch #133: Loss:1.5921, Accuracy:0.2530, Validation Loss:1.6085, Validation Accuracy:0.2512\n",
    "Epoch #134: Loss:1.5922, Accuracy:0.2464, Validation Loss:1.6094, Validation Accuracy:0.2545\n",
    "Epoch #135: Loss:1.5911, Accuracy:0.2517, Validation Loss:1.6088, Validation Accuracy:0.2233\n",
    "Epoch #136: Loss:1.5919, Accuracy:0.2480, Validation Loss:1.6072, Validation Accuracy:0.2578\n",
    "Epoch #137: Loss:1.5916, Accuracy:0.2530, Validation Loss:1.6071, Validation Accuracy:0.2512\n",
    "Epoch #138: Loss:1.5918, Accuracy:0.2472, Validation Loss:1.6068, Validation Accuracy:0.2315\n",
    "Epoch #139: Loss:1.5929, Accuracy:0.2382, Validation Loss:1.6072, Validation Accuracy:0.2512\n",
    "Epoch #140: Loss:1.5929, Accuracy:0.2448, Validation Loss:1.6066, Validation Accuracy:0.2512\n",
    "Epoch #141: Loss:1.5921, Accuracy:0.2575, Validation Loss:1.6084, Validation Accuracy:0.2348\n",
    "Epoch #142: Loss:1.5923, Accuracy:0.2575, Validation Loss:1.6060, Validation Accuracy:0.2414\n",
    "Epoch #143: Loss:1.5919, Accuracy:0.2501, Validation Loss:1.6070, Validation Accuracy:0.2414\n",
    "Epoch #144: Loss:1.5920, Accuracy:0.2485, Validation Loss:1.6071, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5924, Accuracy:0.2480, Validation Loss:1.6069, Validation Accuracy:0.2250\n",
    "Epoch #146: Loss:1.5913, Accuracy:0.2554, Validation Loss:1.6053, Validation Accuracy:0.2463\n",
    "Epoch #147: Loss:1.5898, Accuracy:0.2559, Validation Loss:1.6050, Validation Accuracy:0.2496\n",
    "Epoch #148: Loss:1.5907, Accuracy:0.2538, Validation Loss:1.6057, Validation Accuracy:0.2447\n",
    "Epoch #149: Loss:1.5917, Accuracy:0.2591, Validation Loss:1.6046, Validation Accuracy:0.2348\n",
    "Epoch #150: Loss:1.5900, Accuracy:0.2571, Validation Loss:1.6024, Validation Accuracy:0.2594\n",
    "Epoch #151: Loss:1.5914, Accuracy:0.2489, Validation Loss:1.6052, Validation Accuracy:0.2578\n",
    "Epoch #152: Loss:1.5908, Accuracy:0.2452, Validation Loss:1.6046, Validation Accuracy:0.2250\n",
    "Epoch #153: Loss:1.5921, Accuracy:0.2448, Validation Loss:1.6058, Validation Accuracy:0.2184\n",
    "Epoch #154: Loss:1.5922, Accuracy:0.2411, Validation Loss:1.6066, Validation Accuracy:0.2447\n",
    "Epoch #155: Loss:1.5922, Accuracy:0.2431, Validation Loss:1.6025, Validation Accuracy:0.2496\n",
    "Epoch #156: Loss:1.5910, Accuracy:0.2435, Validation Loss:1.6047, Validation Accuracy:0.2397\n",
    "Epoch #157: Loss:1.5912, Accuracy:0.2415, Validation Loss:1.6036, Validation Accuracy:0.2250\n",
    "Epoch #158: Loss:1.5913, Accuracy:0.2407, Validation Loss:1.6020, Validation Accuracy:0.2578\n",
    "Epoch #159: Loss:1.5908, Accuracy:0.2493, Validation Loss:1.6020, Validation Accuracy:0.2430\n",
    "Epoch #160: Loss:1.5908, Accuracy:0.2480, Validation Loss:1.6046, Validation Accuracy:0.2299\n",
    "Epoch #161: Loss:1.5902, Accuracy:0.2517, Validation Loss:1.6048, Validation Accuracy:0.2414\n",
    "Epoch #162: Loss:1.5903, Accuracy:0.2509, Validation Loss:1.6043, Validation Accuracy:0.2496\n",
    "Epoch #163: Loss:1.5901, Accuracy:0.2448, Validation Loss:1.6042, Validation Accuracy:0.2266\n",
    "Epoch #164: Loss:1.5902, Accuracy:0.2472, Validation Loss:1.6042, Validation Accuracy:0.2299\n",
    "Epoch #165: Loss:1.5902, Accuracy:0.2423, Validation Loss:1.6047, Validation Accuracy:0.2381\n",
    "Epoch #166: Loss:1.5904, Accuracy:0.2456, Validation Loss:1.6059, Validation Accuracy:0.2365\n",
    "Epoch #167: Loss:1.5891, Accuracy:0.2534, Validation Loss:1.6059, Validation Accuracy:0.2512\n",
    "Epoch #168: Loss:1.5897, Accuracy:0.2464, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #169: Loss:1.5886, Accuracy:0.2493, Validation Loss:1.6066, Validation Accuracy:0.2529\n",
    "Epoch #170: Loss:1.5895, Accuracy:0.2439, Validation Loss:1.6069, Validation Accuracy:0.2381\n",
    "Epoch #171: Loss:1.5899, Accuracy:0.2485, Validation Loss:1.6080, Validation Accuracy:0.2545\n",
    "Epoch #172: Loss:1.5898, Accuracy:0.2522, Validation Loss:1.6075, Validation Accuracy:0.2430\n",
    "Epoch #173: Loss:1.5895, Accuracy:0.2493, Validation Loss:1.6079, Validation Accuracy:0.2447\n",
    "Epoch #174: Loss:1.5894, Accuracy:0.2505, Validation Loss:1.6083, Validation Accuracy:0.2430\n",
    "Epoch #175: Loss:1.5893, Accuracy:0.2497, Validation Loss:1.6083, Validation Accuracy:0.2496\n",
    "Epoch #176: Loss:1.5893, Accuracy:0.2526, Validation Loss:1.6087, Validation Accuracy:0.2479\n",
    "Epoch #177: Loss:1.5892, Accuracy:0.2423, Validation Loss:1.6090, Validation Accuracy:0.2250\n",
    "Epoch #178: Loss:1.5884, Accuracy:0.2497, Validation Loss:1.6087, Validation Accuracy:0.2266\n",
    "Epoch #179: Loss:1.5879, Accuracy:0.2468, Validation Loss:1.6079, Validation Accuracy:0.2348\n",
    "Epoch #180: Loss:1.5880, Accuracy:0.2456, Validation Loss:1.6089, Validation Accuracy:0.2151\n",
    "Epoch #181: Loss:1.5887, Accuracy:0.2456, Validation Loss:1.6084, Validation Accuracy:0.2233\n",
    "Epoch #182: Loss:1.5894, Accuracy:0.2472, Validation Loss:1.6098, Validation Accuracy:0.2545\n",
    "Epoch #183: Loss:1.5878, Accuracy:0.2472, Validation Loss:1.6090, Validation Accuracy:0.2233\n",
    "Epoch #184: Loss:1.5889, Accuracy:0.2423, Validation Loss:1.6102, Validation Accuracy:0.2233\n",
    "Epoch #185: Loss:1.5876, Accuracy:0.2435, Validation Loss:1.6109, Validation Accuracy:0.2594\n",
    "Epoch #186: Loss:1.5888, Accuracy:0.2456, Validation Loss:1.6104, Validation Accuracy:0.2562\n",
    "Epoch #187: Loss:1.5881, Accuracy:0.2439, Validation Loss:1.6114, Validation Accuracy:0.2200\n",
    "Epoch #188: Loss:1.5894, Accuracy:0.2419, Validation Loss:1.6106, Validation Accuracy:0.2479\n",
    "Epoch #189: Loss:1.5875, Accuracy:0.2460, Validation Loss:1.6099, Validation Accuracy:0.2414\n",
    "Epoch #190: Loss:1.5873, Accuracy:0.2489, Validation Loss:1.6088, Validation Accuracy:0.2332\n",
    "Epoch #191: Loss:1.5868, Accuracy:0.2522, Validation Loss:1.6077, Validation Accuracy:0.2348\n",
    "Epoch #192: Loss:1.5867, Accuracy:0.2435, Validation Loss:1.6076, Validation Accuracy:0.2578\n",
    "Epoch #193: Loss:1.5875, Accuracy:0.2505, Validation Loss:1.6081, Validation Accuracy:0.2479\n",
    "Epoch #194: Loss:1.5866, Accuracy:0.2616, Validation Loss:1.6081, Validation Accuracy:0.2282\n",
    "Epoch #195: Loss:1.5872, Accuracy:0.2526, Validation Loss:1.6082, Validation Accuracy:0.2496\n",
    "Epoch #196: Loss:1.5862, Accuracy:0.2526, Validation Loss:1.6081, Validation Accuracy:0.2397\n",
    "Epoch #197: Loss:1.5866, Accuracy:0.2579, Validation Loss:1.6087, Validation Accuracy:0.2430\n",
    "Epoch #198: Loss:1.5861, Accuracy:0.2591, Validation Loss:1.6082, Validation Accuracy:0.2594\n",
    "Epoch #199: Loss:1.5866, Accuracy:0.2538, Validation Loss:1.6099, Validation Accuracy:0.2578\n",
    "Epoch #200: Loss:1.5853, Accuracy:0.2554, Validation Loss:1.6104, Validation Accuracy:0.2332\n",
    "Epoch #201: Loss:1.5860, Accuracy:0.2587, Validation Loss:1.6106, Validation Accuracy:0.2414\n",
    "Epoch #202: Loss:1.5855, Accuracy:0.2563, Validation Loss:1.6103, Validation Accuracy:0.2299\n",
    "Epoch #203: Loss:1.5858, Accuracy:0.2563, Validation Loss:1.6097, Validation Accuracy:0.2463\n",
    "Epoch #204: Loss:1.5863, Accuracy:0.2567, Validation Loss:1.6107, Validation Accuracy:0.2315\n",
    "Epoch #205: Loss:1.5865, Accuracy:0.2546, Validation Loss:1.6101, Validation Accuracy:0.2594\n",
    "Epoch #206: Loss:1.5852, Accuracy:0.2427, Validation Loss:1.6109, Validation Accuracy:0.2135\n",
    "Epoch #207: Loss:1.5852, Accuracy:0.2649, Validation Loss:1.6129, Validation Accuracy:0.2365\n",
    "Epoch #208: Loss:1.5861, Accuracy:0.2550, Validation Loss:1.6133, Validation Accuracy:0.2430\n",
    "Epoch #209: Loss:1.5852, Accuracy:0.2612, Validation Loss:1.6124, Validation Accuracy:0.2233\n",
    "Epoch #210: Loss:1.5857, Accuracy:0.2542, Validation Loss:1.6132, Validation Accuracy:0.2184\n",
    "Epoch #211: Loss:1.5842, Accuracy:0.2641, Validation Loss:1.6119, Validation Accuracy:0.2512\n",
    "Epoch #212: Loss:1.5844, Accuracy:0.2563, Validation Loss:1.6121, Validation Accuracy:0.2250\n",
    "Epoch #213: Loss:1.5841, Accuracy:0.2546, Validation Loss:1.6126, Validation Accuracy:0.2315\n",
    "Epoch #214: Loss:1.5850, Accuracy:0.2538, Validation Loss:1.6129, Validation Accuracy:0.2167\n",
    "Epoch #215: Loss:1.5844, Accuracy:0.2493, Validation Loss:1.6134, Validation Accuracy:0.2233\n",
    "Epoch #216: Loss:1.5839, Accuracy:0.2538, Validation Loss:1.6142, Validation Accuracy:0.2299\n",
    "Epoch #217: Loss:1.5838, Accuracy:0.2608, Validation Loss:1.6139, Validation Accuracy:0.2299\n",
    "Epoch #218: Loss:1.5848, Accuracy:0.2542, Validation Loss:1.6127, Validation Accuracy:0.2233\n",
    "Epoch #219: Loss:1.5843, Accuracy:0.2579, Validation Loss:1.6136, Validation Accuracy:0.2299\n",
    "Epoch #220: Loss:1.5838, Accuracy:0.2624, Validation Loss:1.6151, Validation Accuracy:0.2282\n",
    "Epoch #221: Loss:1.5842, Accuracy:0.2600, Validation Loss:1.6141, Validation Accuracy:0.2167\n",
    "Epoch #222: Loss:1.5847, Accuracy:0.2587, Validation Loss:1.6134, Validation Accuracy:0.2348\n",
    "Epoch #223: Loss:1.5833, Accuracy:0.2632, Validation Loss:1.6135, Validation Accuracy:0.2414\n",
    "Epoch #224: Loss:1.5838, Accuracy:0.2604, Validation Loss:1.6144, Validation Accuracy:0.2381\n",
    "Epoch #225: Loss:1.5841, Accuracy:0.2669, Validation Loss:1.6114, Validation Accuracy:0.2430\n",
    "Epoch #226: Loss:1.5861, Accuracy:0.2579, Validation Loss:1.6124, Validation Accuracy:0.2463\n",
    "Epoch #227: Loss:1.5874, Accuracy:0.2530, Validation Loss:1.6137, Validation Accuracy:0.2315\n",
    "Epoch #228: Loss:1.5866, Accuracy:0.2628, Validation Loss:1.6122, Validation Accuracy:0.2430\n",
    "Epoch #229: Loss:1.5863, Accuracy:0.2530, Validation Loss:1.6127, Validation Accuracy:0.2118\n",
    "Epoch #230: Loss:1.5843, Accuracy:0.2612, Validation Loss:1.6127, Validation Accuracy:0.2463\n",
    "Epoch #231: Loss:1.5842, Accuracy:0.2628, Validation Loss:1.6151, Validation Accuracy:0.2381\n",
    "Epoch #232: Loss:1.5840, Accuracy:0.2628, Validation Loss:1.6142, Validation Accuracy:0.2397\n",
    "Epoch #233: Loss:1.5838, Accuracy:0.2632, Validation Loss:1.6136, Validation Accuracy:0.2381\n",
    "Epoch #234: Loss:1.5830, Accuracy:0.2641, Validation Loss:1.6142, Validation Accuracy:0.2348\n",
    "Epoch #235: Loss:1.5833, Accuracy:0.2641, Validation Loss:1.6150, Validation Accuracy:0.2348\n",
    "Epoch #236: Loss:1.5830, Accuracy:0.2649, Validation Loss:1.6142, Validation Accuracy:0.2414\n",
    "Epoch #237: Loss:1.5831, Accuracy:0.2641, Validation Loss:1.6150, Validation Accuracy:0.2414\n",
    "Epoch #238: Loss:1.5835, Accuracy:0.2591, Validation Loss:1.6130, Validation Accuracy:0.2381\n",
    "Epoch #239: Loss:1.5838, Accuracy:0.2571, Validation Loss:1.6123, Validation Accuracy:0.2332\n",
    "Epoch #240: Loss:1.5833, Accuracy:0.2637, Validation Loss:1.6123, Validation Accuracy:0.2397\n",
    "Epoch #241: Loss:1.5836, Accuracy:0.2624, Validation Loss:1.6121, Validation Accuracy:0.2381\n",
    "Epoch #242: Loss:1.5829, Accuracy:0.2686, Validation Loss:1.6123, Validation Accuracy:0.2365\n",
    "Epoch #243: Loss:1.5838, Accuracy:0.2591, Validation Loss:1.6137, Validation Accuracy:0.2332\n",
    "Epoch #244: Loss:1.5835, Accuracy:0.2649, Validation Loss:1.6151, Validation Accuracy:0.2217\n",
    "Epoch #245: Loss:1.5837, Accuracy:0.2600, Validation Loss:1.6150, Validation Accuracy:0.2332\n",
    "Epoch #246: Loss:1.5844, Accuracy:0.2686, Validation Loss:1.6141, Validation Accuracy:0.2332\n",
    "Epoch #247: Loss:1.5833, Accuracy:0.2694, Validation Loss:1.6156, Validation Accuracy:0.2250\n",
    "Epoch #248: Loss:1.5830, Accuracy:0.2674, Validation Loss:1.6145, Validation Accuracy:0.2282\n",
    "Epoch #249: Loss:1.5826, Accuracy:0.2645, Validation Loss:1.6136, Validation Accuracy:0.2348\n",
    "Epoch #250: Loss:1.5827, Accuracy:0.2678, Validation Loss:1.6143, Validation Accuracy:0.2332\n",
    "Epoch #251: Loss:1.5825, Accuracy:0.2682, Validation Loss:1.6149, Validation Accuracy:0.2332\n",
    "Epoch #252: Loss:1.5828, Accuracy:0.2682, Validation Loss:1.6151, Validation Accuracy:0.2332\n",
    "Epoch #253: Loss:1.5837, Accuracy:0.2571, Validation Loss:1.6151, Validation Accuracy:0.2282\n",
    "Epoch #254: Loss:1.5829, Accuracy:0.2641, Validation Loss:1.6158, Validation Accuracy:0.2365\n",
    "Epoch #255: Loss:1.5825, Accuracy:0.2669, Validation Loss:1.6153, Validation Accuracy:0.2151\n",
    "Epoch #256: Loss:1.5857, Accuracy:0.2517, Validation Loss:1.6160, Validation Accuracy:0.2348\n",
    "Epoch #257: Loss:1.5823, Accuracy:0.2669, Validation Loss:1.6161, Validation Accuracy:0.2200\n",
    "Epoch #258: Loss:1.5829, Accuracy:0.2591, Validation Loss:1.6143, Validation Accuracy:0.2463\n",
    "Epoch #259: Loss:1.5840, Accuracy:0.2645, Validation Loss:1.6144, Validation Accuracy:0.2332\n",
    "Epoch #260: Loss:1.5821, Accuracy:0.2616, Validation Loss:1.6141, Validation Accuracy:0.2266\n",
    "Epoch #261: Loss:1.5828, Accuracy:0.2538, Validation Loss:1.6160, Validation Accuracy:0.2250\n",
    "Epoch #262: Loss:1.5819, Accuracy:0.2715, Validation Loss:1.6160, Validation Accuracy:0.2282\n",
    "Epoch #263: Loss:1.5821, Accuracy:0.2719, Validation Loss:1.6143, Validation Accuracy:0.2365\n",
    "Epoch #264: Loss:1.5808, Accuracy:0.2665, Validation Loss:1.6166, Validation Accuracy:0.2299\n",
    "Epoch #265: Loss:1.5828, Accuracy:0.2554, Validation Loss:1.6158, Validation Accuracy:0.2282\n",
    "Epoch #266: Loss:1.5817, Accuracy:0.2637, Validation Loss:1.6167, Validation Accuracy:0.2299\n",
    "Epoch #267: Loss:1.5822, Accuracy:0.2665, Validation Loss:1.6163, Validation Accuracy:0.2397\n",
    "Epoch #268: Loss:1.5816, Accuracy:0.2739, Validation Loss:1.6168, Validation Accuracy:0.2414\n",
    "Epoch #269: Loss:1.5800, Accuracy:0.2694, Validation Loss:1.6172, Validation Accuracy:0.2365\n",
    "Epoch #270: Loss:1.5804, Accuracy:0.2637, Validation Loss:1.6186, Validation Accuracy:0.2332\n",
    "Epoch #271: Loss:1.5810, Accuracy:0.2743, Validation Loss:1.6178, Validation Accuracy:0.2447\n",
    "Epoch #272: Loss:1.5800, Accuracy:0.2702, Validation Loss:1.6184, Validation Accuracy:0.2299\n",
    "Epoch #273: Loss:1.5794, Accuracy:0.2686, Validation Loss:1.6165, Validation Accuracy:0.2332\n",
    "Epoch #274: Loss:1.5789, Accuracy:0.2698, Validation Loss:1.6181, Validation Accuracy:0.2233\n",
    "Epoch #275: Loss:1.5794, Accuracy:0.2686, Validation Loss:1.6191, Validation Accuracy:0.2348\n",
    "Epoch #276: Loss:1.5786, Accuracy:0.2653, Validation Loss:1.6188, Validation Accuracy:0.2282\n",
    "Epoch #277: Loss:1.5814, Accuracy:0.2719, Validation Loss:1.6173, Validation Accuracy:0.2414\n",
    "Epoch #278: Loss:1.5824, Accuracy:0.2710, Validation Loss:1.6205, Validation Accuracy:0.2381\n",
    "Epoch #279: Loss:1.5797, Accuracy:0.2784, Validation Loss:1.6212, Validation Accuracy:0.2348\n",
    "Epoch #280: Loss:1.5805, Accuracy:0.2665, Validation Loss:1.6173, Validation Accuracy:0.2332\n",
    "Epoch #281: Loss:1.5794, Accuracy:0.2719, Validation Loss:1.6175, Validation Accuracy:0.2365\n",
    "Epoch #282: Loss:1.5800, Accuracy:0.2727, Validation Loss:1.6210, Validation Accuracy:0.2365\n",
    "Epoch #283: Loss:1.5792, Accuracy:0.2715, Validation Loss:1.6178, Validation Accuracy:0.2151\n",
    "Epoch #284: Loss:1.5808, Accuracy:0.2706, Validation Loss:1.6176, Validation Accuracy:0.2315\n",
    "Epoch #285: Loss:1.5796, Accuracy:0.2694, Validation Loss:1.6198, Validation Accuracy:0.2381\n",
    "Epoch #286: Loss:1.5797, Accuracy:0.2669, Validation Loss:1.6195, Validation Accuracy:0.2332\n",
    "Epoch #287: Loss:1.5776, Accuracy:0.2735, Validation Loss:1.6176, Validation Accuracy:0.2414\n",
    "Epoch #288: Loss:1.5790, Accuracy:0.2690, Validation Loss:1.6158, Validation Accuracy:0.2430\n",
    "Epoch #289: Loss:1.5814, Accuracy:0.2665, Validation Loss:1.6211, Validation Accuracy:0.2299\n",
    "Epoch #290: Loss:1.5822, Accuracy:0.2571, Validation Loss:1.6194, Validation Accuracy:0.2250\n",
    "Epoch #291: Loss:1.5814, Accuracy:0.2661, Validation Loss:1.6153, Validation Accuracy:0.2562\n",
    "Epoch #292: Loss:1.5833, Accuracy:0.2612, Validation Loss:1.6146, Validation Accuracy:0.2447\n",
    "Epoch #293: Loss:1.5837, Accuracy:0.2526, Validation Loss:1.6128, Validation Accuracy:0.2381\n",
    "Epoch #294: Loss:1.5836, Accuracy:0.2624, Validation Loss:1.6136, Validation Accuracy:0.2397\n",
    "Epoch #295: Loss:1.5846, Accuracy:0.2534, Validation Loss:1.6146, Validation Accuracy:0.2463\n",
    "Epoch #296: Loss:1.5849, Accuracy:0.2579, Validation Loss:1.6133, Validation Accuracy:0.2496\n",
    "Epoch #297: Loss:1.5946, Accuracy:0.2460, Validation Loss:1.6162, Validation Accuracy:0.2217\n",
    "Epoch #298: Loss:1.5900, Accuracy:0.2431, Validation Loss:1.6108, Validation Accuracy:0.2644\n",
    "Epoch #299: Loss:1.5867, Accuracy:0.2497, Validation Loss:1.6111, Validation Accuracy:0.2627\n",
    "Epoch #300: Loss:1.5856, Accuracy:0.2591, Validation Loss:1.6094, Validation Accuracy:0.2381\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60944772, Accuracy:0.2381\n",
    "Labels: ['04', '03', '05', '02', '01']\n",
    "Confusion Matrix:\n",
    "      04  03  05  02  01\n",
    "t:04  21  17  38   6  30\n",
    "t:03  17  15  44   5  34\n",
    "t:05  15  15  62   5  45\n",
    "t:02  23  18  38   3  32\n",
    "t:01  25  14  41   2  44\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.21      0.19      0.20       112\n",
    "          03       0.19      0.13      0.15       115\n",
    "          05       0.28      0.44      0.34       142\n",
    "          02       0.14      0.03      0.04       114\n",
    "          01       0.24      0.35      0.28       126\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.21      0.23      0.20       609\n",
    "weighted avg       0.21      0.24      0.21       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 12:32:50 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6099268994699363, 1.6077924287573653, 1.6067961685371712, 1.605941155469672, 1.605682704053293, 1.6051648222949901, 1.6051096788963857, 1.6046787455359899, 1.6051914433540382, 1.6048705027804195, 1.6041939313384308, 1.604668559307731, 1.6046322069340346, 1.6045324109457983, 1.6044939178942852, 1.604622877681588, 1.605102012701614, 1.6048793964981054, 1.605255579517784, 1.605500228495042, 1.6049974080181277, 1.6060261608931818, 1.6061321185727424, 1.606708239452005, 1.6070018673960995, 1.606743092215903, 1.6063717824876407, 1.6065149653721325, 1.6067453225453694, 1.6075890886372532, 1.6071979335963433, 1.6072257487057464, 1.606962014693149, 1.6068559487660725, 1.6076007511815413, 1.6068925211582277, 1.6072187682090722, 1.6074815904369886, 1.6069192062262048, 1.6065728954102214, 1.6068296356154192, 1.6070356646977817, 1.6063688926900352, 1.6054720547790402, 1.6064541155873064, 1.606518766758673, 1.6064015834398067, 1.6064357689057274, 1.606017885928475, 1.606007416651558, 1.6073597154789565, 1.6074049408212672, 1.607783624495583, 1.6077963055061002, 1.607453299077665, 1.6080543331324761, 1.6078008591443642, 1.608065932842311, 1.6068425728573978, 1.6060499626231703, 1.6068684307029486, 1.6061323237145084, 1.6067149419894164, 1.6075422497609957, 1.6074042766552252, 1.607572308314845, 1.6068711770187654, 1.6080812159038724, 1.6079637213489302, 1.6085917607121083, 1.6085986353102184, 1.6088998675933612, 1.6085722949508767, 1.608656536965143, 1.6089750365866424, 1.608074362837818, 1.6081099972153337, 1.6076836634935026, 1.607662403329057, 1.6083821087635208, 1.6097748904001146, 1.6095603056533387, 1.6096003145615652, 1.61059110133323, 1.6091801906845644, 1.608909849071346, 1.6092660049303804, 1.6091429946457811, 1.6097230269208134, 1.6095896160661294, 1.6097779591095271, 1.609628262973967, 1.6097300579199454, 1.6102301925665443, 1.6107279455720498, 1.6099932902356477, 1.6111933227830333, 1.6103224470501853, 1.6091572191131918, 1.6093696193350555, 1.6090909209353192, 1.6088907520955027, 1.609336505382519, 1.6095359513325056, 1.6100135648191856, 1.6096947880214072, 1.6097597451437087, 1.6100519938617701, 1.6096687534172547, 1.6095711080898791, 1.6096301321521378, 1.6094957856317655, 1.6096584804735357, 1.6103464543134316, 1.6097421704841952, 1.6100469590799367, 1.6102610327340112, 1.6098635852434757, 1.6107054435952348, 1.6116415009710001, 1.6113792348573557, 1.6095641477550389, 1.610935179666541, 1.6107050679587378, 1.6094758970592604, 1.6082333277403231, 1.6074649943115285, 1.6083490958158997, 1.6085487181329963, 1.608005862909389, 1.608356417497782, 1.6087959082843049, 1.608540654965418, 1.6093962210152537, 1.608800573889258, 1.6072214305498722, 1.6071488246541892, 1.606797939059378, 1.607223055241339, 1.606598066970437, 1.6084046436256572, 1.605969665085741, 1.6070181313406657, 1.607125109639661, 1.6068543914112159, 1.605348815080176, 1.6049772532311175, 1.6056899709263066, 1.6046496866567577, 1.6023630520393108, 1.605209367615836, 1.6046195707493422, 1.6057908677898214, 1.6065518906942533, 1.6025159638894995, 1.6046978712864892, 1.6035768909407366, 1.6019991158656104, 1.6020115884066803, 1.60455836177068, 1.6047955847334587, 1.6043413955785566, 1.604163808384161, 1.604236798529163, 1.6047046832458922, 1.6058604337507476, 1.6059044145402455, 1.6062150453699047, 1.6066256906402914, 1.6068580604734874, 1.6079827873969117, 1.6075274145661904, 1.6079336565312101, 1.6082552536367782, 1.6083324069068545, 1.6086631142251402, 1.6090224764029968, 1.6086508393874897, 1.6078892666326563, 1.6088717295031243, 1.608405093058381, 1.6098040786674261, 1.6089578197507435, 1.6101661108201752, 1.6108610907994663, 1.6103754480092591, 1.6113582651799143, 1.6105511499743157, 1.609875567636662, 1.608759104911917, 1.607715656408927, 1.607562107992877, 1.6081087951393942, 1.6081370379537197, 1.608175559779889, 1.6080795892549462, 1.6086887356095714, 1.6081646128828302, 1.6099214438342893, 1.6104032854337018, 1.6105531849493142, 1.6102653493239178, 1.609707777919049, 1.61069880663272, 1.6100914362811887, 1.6108670379532186, 1.6128767474335794, 1.6133464962390844, 1.6124165641458947, 1.613178428757954, 1.6119167270331547, 1.6121159122495228, 1.6125938690746164, 1.6129187948402317, 1.613358902030782, 1.6141755735541408, 1.6138526882444109, 1.6126688427134297, 1.613557507056125, 1.6151341346684347, 1.6140734522996474, 1.6133777690051225, 1.613525068035658, 1.6143719719352785, 1.611404748777255, 1.61243425742746, 1.6136893214067607, 1.61221660162232, 1.6126626132940032, 1.6127425304970326, 1.615126941787394, 1.6142208118156847, 1.6135879504465314, 1.614192151083735, 1.6150363456635248, 1.614189140902364, 1.6150042873689499, 1.6130192266113457, 1.6123054859477703, 1.6123171490792962, 1.6120686382299965, 1.6122765819035922, 1.6136564871752008, 1.6150625646603713, 1.6149926978379048, 1.6140805418268214, 1.6156333160322092, 1.6145010252891503, 1.6136008403179876, 1.6142718670599174, 1.6148893653074117, 1.6150876927649838, 1.6150651843285522, 1.6158402960484446, 1.6152998792322595, 1.6160096969510533, 1.6160538617417535, 1.6143245642212616, 1.6144076245171683, 1.614066867209812, 1.6159573754262062, 1.6160193356778625, 1.614311222372384, 1.6166404078550918, 1.6157564349558162, 1.6166957867360858, 1.6163146078880197, 1.6167633067602398, 1.617224677442917, 1.6185738464881634, 1.61780464864521, 1.6183852440813689, 1.6164725584349608, 1.6181068927392193, 1.6190512775396086, 1.618775541167737, 1.6172618396176492, 1.6204604057255636, 1.6211566302576677, 1.6173394143287771, 1.6174676441793958, 1.6209914925063185, 1.617779211653473, 1.6175731056429483, 1.6198078556405304, 1.6194755706098083, 1.617630801764615, 1.6157872580933845, 1.621123292567499, 1.6194303953784637, 1.6153474222067346, 1.6146438119838196, 1.6127719313444566, 1.6136140684384626, 1.6145951628489252, 1.613290251377964, 1.6161702515064984, 1.61079900409592, 1.6110837872588184, 1.6094477345007785], 'val_acc': [0.19868636999909317, 0.23152709357159088, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23481116582118036, 0.24958948866878627, 0.24958948866878627, 0.24794745254399153, 0.251231524793581, 0.23316912959851263, 0.2315270934737179, 0.24137931022248635, 0.2397372740976916, 0.23645320184810212, 0.23481116572330737, 0.23645320184810212, 0.23809523797289686, 0.23809523797289686, 0.23645320184810212, 0.24302134644515408, 0.24137931022248635, 0.2397372740976916, 0.2348111656254344, 0.24302134644515408, 0.24302134644515408, 0.23316912950063964, 0.23481116572330737, 0.24302134644515408, 0.24630541869474357, 0.2348111656254344, 0.2315270933758449, 0.23316912950063964, 0.24137931032035934, 0.23809523797289686, 0.23481116572330737, 0.23481116572330737, 0.24137931032035934, 0.23481116572330737, 0.23481116572330737, 0.2348111656254344, 0.2282430212241284, 0.22660098500146067, 0.23809523797289686, 0.23316912950063964, 0.2315270934737179, 0.23316912950063964, 0.24302134644515408, 0.23316912969638562, 0.25287356111412174, 0.23645320184810212, 0.2479474548195383, 0.24137931032035934, 0.23481116572330737, 0.2380952378750239, 0.24302134634728112, 0.24137931022248635, 0.2348111656254344, 0.24958948876665926, 0.23809523807076985, 0.2364532019459751, 0.24794745254399153, 0.23316912969638562, 0.2463054164191968, 0.2249589489745389, 0.2463054164191968, 0.24466338039227503, 0.24466338039227503, 0.24466338039227503, 0.2397372720178908, 0.2233169126539982, 0.24466338029440204, 0.24466338029440204, 0.21839080408386802, 0.2397372719200178, 0.24466338029440204, 0.24466338029440204, 0.24466338256994882, 0.22988505734892314, 0.23645320184810212, 0.24137931022248635, 0.24302134634728112, 0.2282430211262554, 0.21839080437748695, 0.2512315270691278, 0.24137931022248635, 0.21839080408386802, 0.23481116572330737, 0.2413793101246134, 0.23316912959851263, 0.24302134634728112, 0.23809523579522307, 0.24466338049014802, 0.24302134624940813, 0.24302134634728112, 0.22824301914432757, 0.2364532019459751, 0.24302134634728112, 0.22988505744679613, 0.23481116572330737, 0.2545155972389165, 0.24794745264186452, 0.23481116582118036, 0.24630541869474357, 0.23809523579522307, 0.2282430213220014, 0.2463054185968706, 0.23809523589309606, 0.20361247905858829, 0.24958948866878627, 0.23645320184810212, 0.24466338029440204, 0.22988505734892314, 0.23645319986617427, 0.25287356101624875, 0.2479474548195383, 0.2200328404044087, 0.23481116552756143, 0.23645320165235617, 0.24630541849899762, 0.22660098519720664, 0.24630541869474357, 0.251231524793581, 0.2545155970431705, 0.2233169126539982, 0.2577996715683068, 0.251231524793581, 0.23152709318009895, 0.25123152697125484, 0.25123152697125484, 0.2348111656254344, 0.24137931022248635, 0.24137931022248635, 0.2397372741955646, 0.22495894877879294, 0.2463054164191968, 0.24958948876665926, 0.24466338029440204, 0.23481116572330737, 0.25944170769310154, 0.25779966929276, 0.22495894887666593, 0.21839080437748695, 0.24466338256994882, 0.24958948866878627, 0.23973727370619968, 0.22495894868091998, 0.2577996713725608, 0.24302134605366216, 0.22988505685955823, 0.2413793100267404, 0.2495894907485871, 0.22660098470784173, 0.22988505685955823, 0.23809523797289686, 0.23645320175022916, 0.25123152687338185, 0.23316912969638562, 0.25287356319392257, 0.23809523589309606, 0.2545155993187173, 0.24302134644515408, 0.24466338237420288, 0.24302134644515408, 0.24958949065071412, 0.24794745452591938, 0.22495894689473808, 0.22660098480571472, 0.23481116552756143, 0.21510673212789747, 0.22331691284974417, 0.2545155970431705, 0.22331691284974417, 0.22331691284974417, 0.25944170541755474, 0.25615763544351206, 0.2200328404044087, 0.24794745452591938, 0.2413793101246134, 0.2331691292070207, 0.23481116552756143, 0.2577996715683068, 0.24794745472166535, 0.22824301904645458, 0.24958948866878627, 0.2397372741955646, 0.24302134624940813, 0.25944170541755474, 0.25779966929276, 0.23316912959851263, 0.2413793099288674, 0.22988505744679613, 0.24630541869474357, 0.23152709327797194, 0.25944170541755474, 0.21346469619884867, 0.23645320184810212, 0.24302134615153514, 0.22331691236037926, 0.2183908045732329, 0.251231524793581, 0.2249589489745389, 0.2315270933758449, 0.2167487678612003, 0.22331691284974417, 0.22988505734892314, 0.2298850570553042, 0.2233169125561252, 0.2298850552691223, 0.2282430213220014, 0.2167487682526922, 0.2348111656254344, 0.2413793083384315, 0.238095236186715, 0.24302134436535328, 0.24630541651706978, 0.23152709139391706, 0.24302134446322624, 0.21182265779850715, 0.24630541661494276, 0.23809523599096902, 0.2397372720178908, 0.23809523579522307, 0.23481116354563358, 0.23481116354563358, 0.24137930804481256, 0.24137930804481256, 0.23809523589309606, 0.2331691275187118, 0.2397372741955646, 0.23809523807076985, 0.23645320175022916, 0.23316912950063964, 0.22167487652920345, 0.23316912950063964, 0.23316912950063964, 0.22495894877879294, 0.2282430211262554, 0.2348111656254344, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.2282430213220014, 0.23645320175022916, 0.21510673203002448, 0.23481116364350654, 0.22003284030653572, 0.24630541830325164, 0.23316912910914772, 0.22660098490358768, 0.22495894868091998, 0.2282430212241284, 0.23645320155448318, 0.22988505744679613, 0.22824301904645458, 0.2298850571531772, 0.2397372719200178, 0.2413793101246134, 0.23645319996404726, 0.23316912742083884, 0.24466338029440204, 0.22988505734892314, 0.23316912742083884, 0.22331691294761713, 0.23481116383925252, 0.2282430212241284, 0.24137930814268554, 0.23809523599096902, 0.23481116582118036, 0.23316912940276668, 0.23645320184810212, 0.23645319986617427, 0.2151067319321515, 0.23152709318009895, 0.23809523589309606, 0.2331691276165848, 0.2413793083384315, 0.24302134446322624, 0.2298850553669953, 0.22495894709048403, 0.2561576336573302, 0.24466338049014802, 0.238095236088842, 0.23973727211576376, 0.2463054168106887, 0.24958948915815118, 0.2216748745472756, 0.26436781408555793, 0.2627257778628902, 0.23809523628458798], 'loss': [1.6163712771766239, 1.6088605960046976, 1.607789933167444, 1.6070301430426095, 1.6057530800175128, 1.6054878993200816, 1.604994560707766, 1.604630726955265, 1.604271583586503, 1.604287329248824, 1.6039345221353019, 1.6037347510610027, 1.6033144545016593, 1.6027091999073537, 1.602252783667625, 1.6018903696561495, 1.6014453225067264, 1.6005602236401129, 1.6005207733941518, 1.6014236093301792, 1.6003908775180762, 1.6005232483454555, 1.6010590486702734, 1.6002238877255324, 1.5999390315226216, 1.6002865011442369, 1.5997019495562605, 1.5995969621063013, 1.5995045264888348, 1.5995799424956711, 1.599297355283702, 1.5992222001175616, 1.5988875978046864, 1.5993392596254603, 1.5995691183411365, 1.5996283679037857, 1.59951134550498, 1.5995925650214757, 1.5993052769001015, 1.599303253671227, 1.59909973090679, 1.5990053608676982, 1.5992782595221267, 1.5989121927617755, 1.5987372868359702, 1.5983812245500162, 1.5983763913599127, 1.5983197143680017, 1.5982226289518071, 1.5982430870772877, 1.598040752880872, 1.5978645706078844, 1.5977651566695383, 1.5975376688234615, 1.5974632492300422, 1.5975826650674338, 1.5974701860846925, 1.5973267083784883, 1.599376986354773, 1.5982172297256438, 1.5986112054613337, 1.596994094486354, 1.5976813590012537, 1.5971249849154965, 1.5974869735431867, 1.5970037071611847, 1.5968112561247432, 1.5973829546503462, 1.5966439655429283, 1.5965659397583478, 1.5968629361178106, 1.59726150740833, 1.5976129128458074, 1.597452490922117, 1.5975092303826335, 1.5971743329104946, 1.5970779630927334, 1.597040565351686, 1.596693049221313, 1.5964874249464187, 1.5969667218793833, 1.596707527789247, 1.5970343262263147, 1.5963532028746557, 1.596623573557797, 1.595706906455743, 1.5960991615142666, 1.5964110765613815, 1.5956132609741398, 1.5958411335700347, 1.5957400955458687, 1.595292800603706, 1.5959138544677953, 1.5967010976842297, 1.5945426553671365, 1.5962096752327326, 1.5970604359981215, 1.5950421897782436, 1.5953258329348399, 1.5949386726904209, 1.5944590876968978, 1.5945764379579674, 1.593952135677455, 1.5941152626973647, 1.5942760018107827, 1.5939794949682342, 1.5945696574216996, 1.594007251345891, 1.5939738954606733, 1.593410858481327, 1.5932740932617344, 1.5931029505798215, 1.5947822591852114, 1.593662644754445, 1.5930818781470861, 1.5944351215871697, 1.5939159538711611, 1.5931567345311755, 1.5935080735835208, 1.593462663950127, 1.5932376112046918, 1.5925476412019202, 1.5925308793477209, 1.592940505867866, 1.593182817181033, 1.5929438137175855, 1.59238033549252, 1.5920459855998077, 1.591749678549091, 1.590760832058086, 1.592525879411482, 1.5927887959157172, 1.5920928228072806, 1.5921738872782651, 1.5911291487652663, 1.591887880105992, 1.5915751739693862, 1.5917827380511305, 1.5928594860452892, 1.592912926174532, 1.5920530965930382, 1.5922970438395192, 1.5918980649364558, 1.5919624052008565, 1.5924346727022645, 1.5912840957269532, 1.589817607623106, 1.5907489965583754, 1.5917048136556418, 1.589981660118338, 1.591382055752576, 1.5908300197100003, 1.5920658484131893, 1.5922150934011785, 1.5921583045924224, 1.5910407844265384, 1.5912016413784615, 1.5913424162894059, 1.5907529080917704, 1.5907607839827176, 1.5901906522147709, 1.5902602461574014, 1.5901178374672327, 1.5901709608473573, 1.5902456640462856, 1.5903793215996431, 1.5890501855090413, 1.5897383960120732, 1.5886062109984411, 1.5894760893600433, 1.5899457648549482, 1.5898367155748716, 1.589513023726994, 1.5894295934289389, 1.5893096809759277, 1.589301758772049, 1.5891936941558087, 1.5883987927094132, 1.5879011751934733, 1.5879851146889907, 1.5886537706093131, 1.5894265016735947, 1.5877628853678458, 1.5888746592053642, 1.5875830196991594, 1.5888366316867806, 1.5881399613875873, 1.5894417485172498, 1.5874738221295805, 1.5872519805935619, 1.586808838834508, 1.5867476588646734, 1.5874749066158975, 1.5865727228305668, 1.5872387213873422, 1.5861569264096647, 1.5865612064298908, 1.5860679488407268, 1.5866265255812502, 1.5853459209387308, 1.58599133966395, 1.5854844529525944, 1.5857877514445562, 1.5862649273333853, 1.5864659582564964, 1.5851782637699918, 1.5852091602231442, 1.5861139576048333, 1.5852308291429367, 1.5856849683628436, 1.584191973253442, 1.584437229109496, 1.584082992561544, 1.5850313636066977, 1.5843830706892073, 1.5839286827208814, 1.5838097646251106, 1.584798061627382, 1.5842834258715965, 1.5838207740313708, 1.5841512081804217, 1.5847355367711438, 1.5832827353624348, 1.5838275045340067, 1.5841247509147598, 1.5860740927455361, 1.5873643260716903, 1.5865560675548578, 1.5862530793007883, 1.584267458778632, 1.5842397132693375, 1.583982167939141, 1.583779292772438, 1.583025301506387, 1.583304236899656, 1.5829674828468652, 1.583144623298175, 1.5835132052521441, 1.5838420301492209, 1.583272135673852, 1.5836423114584701, 1.582870171985587, 1.5837951987186252, 1.5834997750895223, 1.58369784090798, 1.5843673377066423, 1.5833146128076792, 1.5830077806292617, 1.5825979606326845, 1.582696515431884, 1.5824813569595682, 1.58284716077409, 1.5837023678256745, 1.582922317751624, 1.5825477885024992, 1.5857427156436614, 1.58234474502305, 1.5829102385949794, 1.5839848770987572, 1.5821396539343455, 1.5828337283105087, 1.5818850140796794, 1.582098772344648, 1.5807781194514563, 1.582838341930319, 1.5817308052854127, 1.5821849700606578, 1.5815548763138068, 1.5800019978987363, 1.5804317337776357, 1.5809781710469992, 1.5800031246835446, 1.5794083919368485, 1.5789303316962302, 1.5793997963840711, 1.5785668598308211, 1.5814104387157997, 1.5823763783707512, 1.5797350908941312, 1.5804895536610724, 1.5794434918025682, 1.5800444042902952, 1.57915432360138, 1.5808450886845833, 1.5795517378029638, 1.5796899651110294, 1.5775516236342444, 1.5789874392123682, 1.5813765245786193, 1.582207383708053, 1.5813810504682255, 1.583327617782342, 1.5837354384408595, 1.5835602882216844, 1.5845914197408688, 1.5849341464483273, 1.5945743924048892, 1.5899537638717118, 1.5866848937784621, 1.5855673124168443], 'acc': [0.18932238084090075, 0.211088296146853, 0.23326488652513258, 0.23285421047367355, 0.2328542083195837, 0.2328542106694999, 0.23285420912124782, 0.2328542083195837, 0.23285421031456463, 0.23285420851541005, 0.23285420851541005, 0.23285420929871545, 0.23285420912124782, 0.23285420931707418, 0.23285420969036816, 0.23285420851541005, 0.23613963043910033, 0.23572895419181494, 0.23696098606689264, 0.2336755653365192, 0.23819301776450272, 0.24599589307694955, 0.2406570842378683, 0.24147843869070254, 0.23942505218532295, 0.24065708482534734, 0.24065708541282638, 0.24147843908235522, 0.24106776322672255, 0.23942505198949662, 0.23942505120619123, 0.24188911670042504, 0.24147843947400793, 0.24353182756435698, 0.24394250479077412, 0.24271047232821738, 0.2406570830629102, 0.24312114935880813, 0.24353182736853066, 0.24271047134908563, 0.24394250420329508, 0.24681724948804726, 0.24517453727168959, 0.24065708523535875, 0.24188911787538314, 0.24229979353518946, 0.24517453864247402, 0.24312114957299322, 0.24147844047149838, 0.24476386063275152, 0.24558521604635877, 0.2439425067490376, 0.24312114935880813, 0.24353182818855348, 0.24106776226594953, 0.24065708365038924, 0.24271047170402088, 0.24394250598409092, 0.2410677626576022, 0.2381930181377967, 0.24312115033793988, 0.24435318456293376, 0.24229979472850627, 0.23737166368496246, 0.23942505140201756, 0.23983572960756644, 0.24147843829904983, 0.23819301970440748, 0.2447638614160569, 0.23942505157948518, 0.24188911924616757, 0.23942505020870075, 0.23901437298228365, 0.23778233993224784, 0.23737166329330978, 0.24024640603231942, 0.24024640722563625, 0.23819301893946082, 0.23983572880590232, 0.24229979570763802, 0.23696098606689264, 0.24147843947400793, 0.24106776228430823, 0.24147843829904983, 0.24353182893514144, 0.2394250504045271, 0.24312115016047225, 0.2398357284142496, 0.24271047271987006, 0.24517453903412673, 0.2431211507479513, 0.24517453881994164, 0.24065708541282638, 0.24887063759675507, 0.2464065703217254, 0.2406570842378683, 0.23655030768387617, 0.24229979533434404, 0.23737166290165707, 0.248870637009276, 0.246406570499193, 0.2435318291309678, 0.2550308014577909, 0.24928131402150805, 0.24599589427026636, 0.24517453842828896, 0.2533880906122176, 0.25010266945347404, 0.24599589386025494, 0.2513347028951625, 0.24969199042790235, 0.24722792651863804, 0.24353182697687795, 0.24681724631810825, 0.2505133476590229, 0.24147844007984567, 0.24394250498660047, 0.24804928177313637, 0.24476386122023056, 0.2505133460924121, 0.25667351153841744, 0.24804928253808306, 0.24640657265328283, 0.2517453795341006, 0.24517453803663625, 0.25133470269933617, 0.2447638610244042, 0.24229979433685359, 0.24147843888652887, 0.2435318291309678, 0.24887063757839634, 0.25051334785484924, 0.2529774133674418, 0.24640657071337807, 0.251745380904885, 0.24804928214643035, 0.25297741317161543, 0.2472279267328231, 0.23819301833362305, 0.24476385924360836, 0.25749486873282057, 0.2574948641920971, 0.25010267043260576, 0.24845995919537986, 0.2480492823422567, 0.25544147770507625, 0.25585215469894956, 0.2537987680161024, 0.25913757665935727, 0.2570841887648346, 0.24887063601178555, 0.2451745374491572, 0.24476386218100357, 0.2410677614642854, 0.24312115096213635, 0.24353182856184746, 0.24147844007984567, 0.2406570834545629, 0.24928131558811886, 0.24804928057981956, 0.251745380317406, 0.2509240249037987, 0.24476386002691375, 0.24722792573533264, 0.24229979570763802, 0.24558521565470606, 0.25338809100387033, 0.2464065728491092, 0.24928131501899853, 0.24394250537825315, 0.24845995976450017, 0.2521560569563441, 0.24928131460898711, 0.2505133465024235, 0.2496919928512534, 0.2525667341827612, 0.24229979373101582, 0.2496919926187096, 0.246817249506406, 0.24558521682966417, 0.24558521444303055, 0.2472279271244758, 0.24722792471948346, 0.24229979627675835, 0.24353182893514144, 0.24558521545887974, 0.24394250655321126, 0.24188911711043645, 0.24599589348696096, 0.24887063601178555, 0.25215605813130215, 0.24353182756435698, 0.2505133464840648, 0.26160164393438695, 0.25256673516189293, 0.25256673516189293, 0.25790554439262686, 0.25913757822596806, 0.25379876840775506, 0.25544147668922706, 0.2587268984538084, 0.25626283352869494, 0.25626283374288, 0.2566735117342438, 0.2546201230564157, 0.24271046994158374, 0.2648870619048328, 0.25503079985446264, 0.2611909669037962, 0.2542094446550405, 0.26406571042611127, 0.25626283372452124, 0.25462012227311026, 0.2537987658252951, 0.24928131323820266, 0.2537987686035814, 0.2607802877191156, 0.2542094444592141, 0.2579055418101669, 0.2624229991705266, 0.259958933070455, 0.2587269008037246, 0.26324435538579793, 0.26036960970939305, 0.2669404539484263, 0.25790554537175864, 0.25297741340415925, 0.2628336764153024, 0.2529774121924837, 0.2611909667079698, 0.26283367659277007, 0.262833676005291, 0.2632443544066662, 0.2640657082720214, 0.2640657088595005, 0.2648870637039874, 0.26406570608121416, 0.2591375770693687, 0.2570841887648346, 0.2636550318289097, 0.26242299737137204, 0.26858316246244207, 0.25913757705101, 0.2648870642914664, 0.2599589318954969, 0.26858316304992114, 0.26940451791276676, 0.26735112780907805, 0.2644763872608757, 0.2677618074404875, 0.26817248464854593, 0.2681724842568932, 0.2570841889606609, 0.2640657086636741, 0.26694045140268374, 0.2517453811007114, 0.26694045336094724, 0.2591375784217944, 0.2644763862817439, 0.2616016433469079, 0.25379876705532933, 0.27145790379394985, 0.2718685849736114, 0.26652977633035646, 0.2554414804466451, 0.26365503124143064, 0.2665297755470511, 0.2739219700898478, 0.26940451828606077, 0.26365503202473606, 0.2743326465329595, 0.2702258707439141, 0.26858316207078936, 0.26981519533501025, 0.2685831634415738, 0.2652977389354236, 0.27186858281952153, 0.27104722637170636, 0.27843942388861576, 0.2665297725729384, 0.2718685830153479, 0.27268993864314023, 0.2714579044181463, 0.2706365505527911, 0.2694045153119481, 0.26694045019100826, 0.2735112916884726, 0.26899383769387825, 0.26652977178963305, 0.2570841867698536, 0.2661190975373286, 0.26119096594302316, 0.25256673614102465, 0.26242299897470023, 0.25338808899053067, 0.257905542985125, 0.24599589427026636, 0.24312114935880813, 0.249691990232076, 0.2591375764818897]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
