{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf40.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 12:51:57 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'AllShfUni', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ek', 'eg', 'eo', 'yd', 'sg', 'ib', 'mb', 'my', 'eb', 'ck', 'ds', 'by', 'sk', 'aa', 'ce'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000019D00ABD278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000019D7D5A7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7080, Accuracy:0.0850, Validation Loss:2.7023, Validation Accuracy:0.0854\n",
    "Epoch #2: Loss:2.6991, Accuracy:0.0850, Validation Loss:2.6947, Validation Accuracy:0.0854\n",
    "Epoch #3: Loss:2.6924, Accuracy:0.0850, Validation Loss:2.6879, Validation Accuracy:0.0854\n",
    "Epoch #4: Loss:2.6858, Accuracy:0.0850, Validation Loss:2.6821, Validation Accuracy:0.0854\n",
    "Epoch #5: Loss:2.6804, Accuracy:0.0850, Validation Loss:2.6771, Validation Accuracy:0.0854\n",
    "Epoch #6: Loss:2.6757, Accuracy:0.0850, Validation Loss:2.6726, Validation Accuracy:0.0854\n",
    "Epoch #7: Loss:2.6714, Accuracy:0.0850, Validation Loss:2.6687, Validation Accuracy:0.0854\n",
    "Epoch #8: Loss:2.6675, Accuracy:0.0850, Validation Loss:2.6649, Validation Accuracy:0.0673\n",
    "Epoch #9: Loss:2.6634, Accuracy:0.0949, Validation Loss:2.6607, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6589, Accuracy:0.1023, Validation Loss:2.6556, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6531, Accuracy:0.1023, Validation Loss:2.6484, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6448, Accuracy:0.1023, Validation Loss:2.6379, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6329, Accuracy:0.1047, Validation Loss:2.6213, Validation Accuracy:0.1084\n",
    "Epoch #14: Loss:2.6138, Accuracy:0.1150, Validation Loss:2.5968, Validation Accuracy:0.1314\n",
    "Epoch #15: Loss:2.5871, Accuracy:0.1392, Validation Loss:2.5631, Validation Accuracy:0.1429\n",
    "Epoch #16: Loss:2.5518, Accuracy:0.1569, Validation Loss:2.5250, Validation Accuracy:0.1494\n",
    "Epoch #17: Loss:2.5173, Accuracy:0.1618, Validation Loss:2.4897, Validation Accuracy:0.1609\n",
    "Epoch #18: Loss:2.4843, Accuracy:0.1659, Validation Loss:2.4608, Validation Accuracy:0.1609\n",
    "Epoch #19: Loss:2.4555, Accuracy:0.1676, Validation Loss:2.4347, Validation Accuracy:0.1626\n",
    "Epoch #20: Loss:2.4283, Accuracy:0.1684, Validation Loss:2.4095, Validation Accuracy:0.1576\n",
    "Epoch #21: Loss:2.3986, Accuracy:0.1856, Validation Loss:2.3865, Validation Accuracy:0.1773\n",
    "Epoch #22: Loss:2.3713, Accuracy:0.1984, Validation Loss:2.3591, Validation Accuracy:0.1905\n",
    "Epoch #23: Loss:2.3425, Accuracy:0.2074, Validation Loss:2.3301, Validation Accuracy:0.1905\n",
    "Epoch #24: Loss:2.3149, Accuracy:0.2021, Validation Loss:2.3020, Validation Accuracy:0.2003\n",
    "Epoch #25: Loss:2.2852, Accuracy:0.2131, Validation Loss:2.2764, Validation Accuracy:0.2118\n",
    "Epoch #26: Loss:2.2594, Accuracy:0.2209, Validation Loss:2.2603, Validation Accuracy:0.2250\n",
    "Epoch #27: Loss:2.2407, Accuracy:0.2168, Validation Loss:2.2331, Validation Accuracy:0.2430\n",
    "Epoch #28: Loss:2.2113, Accuracy:0.2292, Validation Loss:2.2102, Validation Accuracy:0.2365\n",
    "Epoch #29: Loss:2.1976, Accuracy:0.2431, Validation Loss:2.1979, Validation Accuracy:0.2611\n",
    "Epoch #30: Loss:2.1784, Accuracy:0.2616, Validation Loss:2.1810, Validation Accuracy:0.2709\n",
    "Epoch #31: Loss:2.1627, Accuracy:0.2727, Validation Loss:2.1646, Validation Accuracy:0.2923\n",
    "Epoch #32: Loss:2.1501, Accuracy:0.2842, Validation Loss:2.1518, Validation Accuracy:0.2824\n",
    "Epoch #33: Loss:2.1379, Accuracy:0.2887, Validation Loss:2.1482, Validation Accuracy:0.2545\n",
    "Epoch #34: Loss:2.1334, Accuracy:0.2813, Validation Loss:2.1318, Validation Accuracy:0.3038\n",
    "Epoch #35: Loss:2.1208, Accuracy:0.2924, Validation Loss:2.1136, Validation Accuracy:0.3169\n",
    "Epoch #36: Loss:2.1071, Accuracy:0.3035, Validation Loss:2.1179, Validation Accuracy:0.2644\n",
    "Epoch #37: Loss:2.1037, Accuracy:0.3039, Validation Loss:2.0933, Validation Accuracy:0.3054\n",
    "Epoch #38: Loss:2.0895, Accuracy:0.2957, Validation Loss:2.0878, Validation Accuracy:0.3103\n",
    "Epoch #39: Loss:2.0802, Accuracy:0.3101, Validation Loss:2.0790, Validation Accuracy:0.2989\n",
    "Epoch #40: Loss:2.0723, Accuracy:0.2998, Validation Loss:2.0662, Validation Accuracy:0.2939\n",
    "Epoch #41: Loss:2.0663, Accuracy:0.3055, Validation Loss:2.0695, Validation Accuracy:0.3235\n",
    "Epoch #42: Loss:2.0637, Accuracy:0.3023, Validation Loss:2.0642, Validation Accuracy:0.2956\n",
    "Epoch #43: Loss:2.0519, Accuracy:0.3105, Validation Loss:2.0484, Validation Accuracy:0.3071\n",
    "Epoch #44: Loss:2.0486, Accuracy:0.3113, Validation Loss:2.0438, Validation Accuracy:0.3186\n",
    "Epoch #45: Loss:2.0394, Accuracy:0.3228, Validation Loss:2.0427, Validation Accuracy:0.2939\n",
    "Epoch #46: Loss:2.0373, Accuracy:0.3064, Validation Loss:2.0285, Validation Accuracy:0.3300\n",
    "Epoch #47: Loss:2.0302, Accuracy:0.3335, Validation Loss:2.0288, Validation Accuracy:0.3021\n",
    "Epoch #48: Loss:2.0234, Accuracy:0.3138, Validation Loss:2.0234, Validation Accuracy:0.3300\n",
    "Epoch #49: Loss:2.0207, Accuracy:0.3269, Validation Loss:2.0167, Validation Accuracy:0.3169\n",
    "Epoch #50: Loss:2.0211, Accuracy:0.3322, Validation Loss:2.0103, Validation Accuracy:0.3054\n",
    "Epoch #51: Loss:2.0151, Accuracy:0.3203, Validation Loss:2.0176, Validation Accuracy:0.3153\n",
    "Epoch #52: Loss:2.0068, Accuracy:0.3220, Validation Loss:1.9997, Validation Accuracy:0.3103\n",
    "Epoch #53: Loss:2.0001, Accuracy:0.3302, Validation Loss:1.9981, Validation Accuracy:0.3153\n",
    "Epoch #54: Loss:1.9929, Accuracy:0.3273, Validation Loss:1.9912, Validation Accuracy:0.3218\n",
    "Epoch #55: Loss:1.9935, Accuracy:0.3285, Validation Loss:1.9977, Validation Accuracy:0.3021\n",
    "Epoch #56: Loss:1.9892, Accuracy:0.3191, Validation Loss:1.9824, Validation Accuracy:0.3300\n",
    "Epoch #57: Loss:1.9834, Accuracy:0.3343, Validation Loss:1.9853, Validation Accuracy:0.3087\n",
    "Epoch #58: Loss:1.9826, Accuracy:0.3248, Validation Loss:1.9793, Validation Accuracy:0.3317\n",
    "Epoch #59: Loss:1.9835, Accuracy:0.3290, Validation Loss:1.9863, Validation Accuracy:0.3186\n",
    "Epoch #60: Loss:1.9741, Accuracy:0.3269, Validation Loss:1.9704, Validation Accuracy:0.3235\n",
    "Epoch #61: Loss:1.9689, Accuracy:0.3331, Validation Loss:1.9664, Validation Accuracy:0.3317\n",
    "Epoch #62: Loss:1.9615, Accuracy:0.3331, Validation Loss:1.9593, Validation Accuracy:0.3268\n",
    "Epoch #63: Loss:1.9617, Accuracy:0.3326, Validation Loss:1.9628, Validation Accuracy:0.3202\n",
    "Epoch #64: Loss:1.9576, Accuracy:0.3314, Validation Loss:1.9532, Validation Accuracy:0.3317\n",
    "Epoch #65: Loss:1.9507, Accuracy:0.3331, Validation Loss:1.9565, Validation Accuracy:0.3136\n",
    "Epoch #66: Loss:1.9497, Accuracy:0.3351, Validation Loss:1.9476, Validation Accuracy:0.3399\n",
    "Epoch #67: Loss:1.9444, Accuracy:0.3376, Validation Loss:1.9499, Validation Accuracy:0.3120\n",
    "Epoch #68: Loss:1.9423, Accuracy:0.3351, Validation Loss:1.9400, Validation Accuracy:0.3415\n",
    "Epoch #69: Loss:1.9370, Accuracy:0.3384, Validation Loss:1.9409, Validation Accuracy:0.3251\n",
    "Epoch #70: Loss:1.9395, Accuracy:0.3363, Validation Loss:1.9368, Validation Accuracy:0.3317\n",
    "Epoch #71: Loss:1.9354, Accuracy:0.3405, Validation Loss:1.9384, Validation Accuracy:0.3218\n",
    "Epoch #72: Loss:1.9317, Accuracy:0.3372, Validation Loss:1.9339, Validation Accuracy:0.3399\n",
    "Epoch #73: Loss:1.9277, Accuracy:0.3384, Validation Loss:1.9259, Validation Accuracy:0.3268\n",
    "Epoch #74: Loss:1.9249, Accuracy:0.3400, Validation Loss:1.9257, Validation Accuracy:0.3235\n",
    "Epoch #75: Loss:1.9190, Accuracy:0.3392, Validation Loss:1.9322, Validation Accuracy:0.3481\n",
    "Epoch #76: Loss:1.9204, Accuracy:0.3392, Validation Loss:1.9224, Validation Accuracy:0.3153\n",
    "Epoch #77: Loss:1.9132, Accuracy:0.3446, Validation Loss:1.9137, Validation Accuracy:0.3333\n",
    "Epoch #78: Loss:1.9083, Accuracy:0.3446, Validation Loss:1.9218, Validation Accuracy:0.3333\n",
    "Epoch #79: Loss:1.9049, Accuracy:0.3483, Validation Loss:1.9134, Validation Accuracy:0.3268\n",
    "Epoch #80: Loss:1.9057, Accuracy:0.3462, Validation Loss:1.9096, Validation Accuracy:0.3317\n",
    "Epoch #81: Loss:1.8959, Accuracy:0.3544, Validation Loss:1.9047, Validation Accuracy:0.3432\n",
    "Epoch #82: Loss:1.8951, Accuracy:0.3466, Validation Loss:1.9030, Validation Accuracy:0.3448\n",
    "Epoch #83: Loss:1.8922, Accuracy:0.3499, Validation Loss:1.8975, Validation Accuracy:0.3366\n",
    "Epoch #84: Loss:1.8872, Accuracy:0.3589, Validation Loss:1.8969, Validation Accuracy:0.3366\n",
    "Epoch #85: Loss:1.8824, Accuracy:0.3507, Validation Loss:1.8953, Validation Accuracy:0.3448\n",
    "Epoch #86: Loss:1.8806, Accuracy:0.3606, Validation Loss:1.8957, Validation Accuracy:0.3300\n",
    "Epoch #87: Loss:1.8757, Accuracy:0.3540, Validation Loss:1.8886, Validation Accuracy:0.3580\n",
    "Epoch #88: Loss:1.8750, Accuracy:0.3659, Validation Loss:1.8955, Validation Accuracy:0.3300\n",
    "Epoch #89: Loss:1.8705, Accuracy:0.3639, Validation Loss:1.8821, Validation Accuracy:0.3465\n",
    "Epoch #90: Loss:1.8639, Accuracy:0.3692, Validation Loss:1.8749, Validation Accuracy:0.3563\n",
    "Epoch #91: Loss:1.8644, Accuracy:0.3618, Validation Loss:1.8899, Validation Accuracy:0.3432\n",
    "Epoch #92: Loss:1.8671, Accuracy:0.3721, Validation Loss:1.8814, Validation Accuracy:0.3514\n",
    "Epoch #93: Loss:1.8629, Accuracy:0.3684, Validation Loss:1.8936, Validation Accuracy:0.3645\n",
    "Epoch #94: Loss:1.8615, Accuracy:0.3749, Validation Loss:1.8695, Validation Accuracy:0.3530\n",
    "Epoch #95: Loss:1.8534, Accuracy:0.3688, Validation Loss:1.8694, Validation Accuracy:0.3530\n",
    "Epoch #96: Loss:1.8524, Accuracy:0.3700, Validation Loss:1.8624, Validation Accuracy:0.3612\n",
    "Epoch #97: Loss:1.8488, Accuracy:0.3680, Validation Loss:1.8734, Validation Accuracy:0.3448\n",
    "Epoch #98: Loss:1.8340, Accuracy:0.3778, Validation Loss:1.8552, Validation Accuracy:0.3596\n",
    "Epoch #99: Loss:1.8317, Accuracy:0.3803, Validation Loss:1.8523, Validation Accuracy:0.3563\n",
    "Epoch #100: Loss:1.8248, Accuracy:0.3811, Validation Loss:1.8456, Validation Accuracy:0.3678\n",
    "Epoch #101: Loss:1.8208, Accuracy:0.3791, Validation Loss:1.8499, Validation Accuracy:0.3777\n",
    "Epoch #102: Loss:1.8228, Accuracy:0.3856, Validation Loss:1.8422, Validation Accuracy:0.3629\n",
    "Epoch #103: Loss:1.8155, Accuracy:0.3914, Validation Loss:1.8289, Validation Accuracy:0.3711\n",
    "Epoch #104: Loss:1.8061, Accuracy:0.3852, Validation Loss:1.8308, Validation Accuracy:0.3645\n",
    "Epoch #105: Loss:1.8027, Accuracy:0.3926, Validation Loss:1.8259, Validation Accuracy:0.3793\n",
    "Epoch #106: Loss:1.7939, Accuracy:0.3996, Validation Loss:1.8177, Validation Accuracy:0.3826\n",
    "Epoch #107: Loss:1.7844, Accuracy:0.3979, Validation Loss:1.8175, Validation Accuracy:0.3842\n",
    "Epoch #108: Loss:1.7800, Accuracy:0.3914, Validation Loss:1.8087, Validation Accuracy:0.3777\n",
    "Epoch #109: Loss:1.7728, Accuracy:0.3992, Validation Loss:1.8073, Validation Accuracy:0.3711\n",
    "Epoch #110: Loss:1.7706, Accuracy:0.3996, Validation Loss:1.8131, Validation Accuracy:0.3662\n",
    "Epoch #111: Loss:1.7684, Accuracy:0.3959, Validation Loss:1.7923, Validation Accuracy:0.3908\n",
    "Epoch #112: Loss:1.7651, Accuracy:0.4021, Validation Loss:1.8013, Validation Accuracy:0.4039\n",
    "Epoch #113: Loss:1.7596, Accuracy:0.4016, Validation Loss:1.7944, Validation Accuracy:0.3957\n",
    "Epoch #114: Loss:1.7483, Accuracy:0.4025, Validation Loss:1.7842, Validation Accuracy:0.3695\n",
    "Epoch #115: Loss:1.7414, Accuracy:0.4057, Validation Loss:1.7865, Validation Accuracy:0.3875\n",
    "Epoch #116: Loss:1.7444, Accuracy:0.4021, Validation Loss:1.7887, Validation Accuracy:0.3810\n",
    "Epoch #117: Loss:1.7446, Accuracy:0.4021, Validation Loss:1.7750, Validation Accuracy:0.3875\n",
    "Epoch #118: Loss:1.7298, Accuracy:0.4074, Validation Loss:1.7696, Validation Accuracy:0.4089\n",
    "Epoch #119: Loss:1.7275, Accuracy:0.4078, Validation Loss:1.7651, Validation Accuracy:0.4089\n",
    "Epoch #120: Loss:1.7238, Accuracy:0.4136, Validation Loss:1.7546, Validation Accuracy:0.4039\n",
    "Epoch #121: Loss:1.7099, Accuracy:0.4140, Validation Loss:1.7488, Validation Accuracy:0.3974\n",
    "Epoch #122: Loss:1.7053, Accuracy:0.4197, Validation Loss:1.7537, Validation Accuracy:0.3924\n",
    "Epoch #123: Loss:1.7055, Accuracy:0.4160, Validation Loss:1.7357, Validation Accuracy:0.4089\n",
    "Epoch #124: Loss:1.7029, Accuracy:0.4148, Validation Loss:1.7409, Validation Accuracy:0.4204\n",
    "Epoch #125: Loss:1.6961, Accuracy:0.4144, Validation Loss:1.7402, Validation Accuracy:0.4187\n",
    "Epoch #126: Loss:1.6866, Accuracy:0.4259, Validation Loss:1.7265, Validation Accuracy:0.4122\n",
    "Epoch #127: Loss:1.6823, Accuracy:0.4275, Validation Loss:1.7290, Validation Accuracy:0.4105\n",
    "Epoch #128: Loss:1.6816, Accuracy:0.4230, Validation Loss:1.7218, Validation Accuracy:0.4138\n",
    "Epoch #129: Loss:1.6782, Accuracy:0.4259, Validation Loss:1.7164, Validation Accuracy:0.4187\n",
    "Epoch #130: Loss:1.6753, Accuracy:0.4214, Validation Loss:1.7224, Validation Accuracy:0.4072\n",
    "Epoch #131: Loss:1.6773, Accuracy:0.4222, Validation Loss:1.7261, Validation Accuracy:0.4105\n",
    "Epoch #132: Loss:1.6784, Accuracy:0.4246, Validation Loss:1.7149, Validation Accuracy:0.4171\n",
    "Epoch #133: Loss:1.6651, Accuracy:0.4218, Validation Loss:1.7039, Validation Accuracy:0.4253\n",
    "Epoch #134: Loss:1.6585, Accuracy:0.4267, Validation Loss:1.6992, Validation Accuracy:0.4204\n",
    "Epoch #135: Loss:1.6512, Accuracy:0.4337, Validation Loss:1.6979, Validation Accuracy:0.4204\n",
    "Epoch #136: Loss:1.6465, Accuracy:0.4370, Validation Loss:1.6965, Validation Accuracy:0.4220\n",
    "Epoch #137: Loss:1.6458, Accuracy:0.4316, Validation Loss:1.6933, Validation Accuracy:0.4335\n",
    "Epoch #138: Loss:1.6393, Accuracy:0.4366, Validation Loss:1.6906, Validation Accuracy:0.4335\n",
    "Epoch #139: Loss:1.6430, Accuracy:0.4341, Validation Loss:1.6948, Validation Accuracy:0.4351\n",
    "Epoch #140: Loss:1.6385, Accuracy:0.4345, Validation Loss:1.6818, Validation Accuracy:0.4236\n",
    "Epoch #141: Loss:1.6341, Accuracy:0.4353, Validation Loss:1.6863, Validation Accuracy:0.4335\n",
    "Epoch #142: Loss:1.6288, Accuracy:0.4366, Validation Loss:1.6848, Validation Accuracy:0.4286\n",
    "Epoch #143: Loss:1.6280, Accuracy:0.4398, Validation Loss:1.6773, Validation Accuracy:0.4319\n",
    "Epoch #144: Loss:1.6266, Accuracy:0.4394, Validation Loss:1.6791, Validation Accuracy:0.4105\n",
    "Epoch #145: Loss:1.6235, Accuracy:0.4398, Validation Loss:1.6758, Validation Accuracy:0.4335\n",
    "Epoch #146: Loss:1.6131, Accuracy:0.4386, Validation Loss:1.6690, Validation Accuracy:0.4368\n",
    "Epoch #147: Loss:1.6124, Accuracy:0.4402, Validation Loss:1.6730, Validation Accuracy:0.4319\n",
    "Epoch #148: Loss:1.6136, Accuracy:0.4370, Validation Loss:1.6708, Validation Accuracy:0.4466\n",
    "Epoch #149: Loss:1.6039, Accuracy:0.4353, Validation Loss:1.6736, Validation Accuracy:0.4319\n",
    "Epoch #150: Loss:1.6076, Accuracy:0.4353, Validation Loss:1.6685, Validation Accuracy:0.4499\n",
    "Epoch #151: Loss:1.6056, Accuracy:0.4390, Validation Loss:1.6532, Validation Accuracy:0.4351\n",
    "Epoch #152: Loss:1.6080, Accuracy:0.4435, Validation Loss:1.6585, Validation Accuracy:0.4335\n",
    "Epoch #153: Loss:1.6096, Accuracy:0.4444, Validation Loss:1.7151, Validation Accuracy:0.3990\n",
    "Epoch #154: Loss:1.6236, Accuracy:0.4324, Validation Loss:1.6551, Validation Accuracy:0.4302\n",
    "Epoch #155: Loss:1.5979, Accuracy:0.4402, Validation Loss:1.6529, Validation Accuracy:0.4401\n",
    "Epoch #156: Loss:1.5979, Accuracy:0.4497, Validation Loss:1.6523, Validation Accuracy:0.4433\n",
    "Epoch #157: Loss:1.5917, Accuracy:0.4402, Validation Loss:1.6743, Validation Accuracy:0.4417\n",
    "Epoch #158: Loss:1.5863, Accuracy:0.4444, Validation Loss:1.6463, Validation Accuracy:0.4433\n",
    "Epoch #159: Loss:1.5753, Accuracy:0.4480, Validation Loss:1.6461, Validation Accuracy:0.4384\n",
    "Epoch #160: Loss:1.5734, Accuracy:0.4579, Validation Loss:1.6496, Validation Accuracy:0.4417\n",
    "Epoch #161: Loss:1.5700, Accuracy:0.4517, Validation Loss:1.6428, Validation Accuracy:0.4516\n",
    "Epoch #162: Loss:1.5674, Accuracy:0.4452, Validation Loss:1.6402, Validation Accuracy:0.4516\n",
    "Epoch #163: Loss:1.5597, Accuracy:0.4530, Validation Loss:1.6459, Validation Accuracy:0.4450\n",
    "Epoch #164: Loss:1.5612, Accuracy:0.4628, Validation Loss:1.6430, Validation Accuracy:0.4466\n",
    "Epoch #165: Loss:1.5576, Accuracy:0.4571, Validation Loss:1.6282, Validation Accuracy:0.4516\n",
    "Epoch #166: Loss:1.5582, Accuracy:0.4559, Validation Loss:1.6507, Validation Accuracy:0.4384\n",
    "Epoch #167: Loss:1.5717, Accuracy:0.4501, Validation Loss:1.6330, Validation Accuracy:0.4516\n",
    "Epoch #168: Loss:1.5542, Accuracy:0.4567, Validation Loss:1.6295, Validation Accuracy:0.4516\n",
    "Epoch #169: Loss:1.5498, Accuracy:0.4608, Validation Loss:1.6357, Validation Accuracy:0.4351\n",
    "Epoch #170: Loss:1.5466, Accuracy:0.4587, Validation Loss:1.6221, Validation Accuracy:0.4516\n",
    "Epoch #171: Loss:1.5434, Accuracy:0.4538, Validation Loss:1.6236, Validation Accuracy:0.4532\n",
    "Epoch #172: Loss:1.5423, Accuracy:0.4624, Validation Loss:1.6289, Validation Accuracy:0.4401\n",
    "Epoch #173: Loss:1.5415, Accuracy:0.4616, Validation Loss:1.6215, Validation Accuracy:0.4499\n",
    "Epoch #174: Loss:1.5458, Accuracy:0.4554, Validation Loss:1.6187, Validation Accuracy:0.4598\n",
    "Epoch #175: Loss:1.5446, Accuracy:0.4554, Validation Loss:1.6251, Validation Accuracy:0.4466\n",
    "Epoch #176: Loss:1.5365, Accuracy:0.4645, Validation Loss:1.6219, Validation Accuracy:0.4466\n",
    "Epoch #177: Loss:1.5371, Accuracy:0.4641, Validation Loss:1.6049, Validation Accuracy:0.4581\n",
    "Epoch #178: Loss:1.5269, Accuracy:0.4665, Validation Loss:1.6378, Validation Accuracy:0.4269\n",
    "Epoch #179: Loss:1.5312, Accuracy:0.4698, Validation Loss:1.6019, Validation Accuracy:0.4614\n",
    "Epoch #180: Loss:1.5244, Accuracy:0.4694, Validation Loss:1.6033, Validation Accuracy:0.4548\n",
    "Epoch #181: Loss:1.5155, Accuracy:0.4698, Validation Loss:1.6057, Validation Accuracy:0.4581\n",
    "Epoch #182: Loss:1.5134, Accuracy:0.4686, Validation Loss:1.5993, Validation Accuracy:0.4581\n",
    "Epoch #183: Loss:1.5079, Accuracy:0.4727, Validation Loss:1.5937, Validation Accuracy:0.4598\n",
    "Epoch #184: Loss:1.5034, Accuracy:0.4723, Validation Loss:1.5888, Validation Accuracy:0.4680\n",
    "Epoch #185: Loss:1.5126, Accuracy:0.4661, Validation Loss:1.6139, Validation Accuracy:0.4532\n",
    "Epoch #186: Loss:1.5087, Accuracy:0.4637, Validation Loss:1.5884, Validation Accuracy:0.4598\n",
    "Epoch #187: Loss:1.5090, Accuracy:0.4686, Validation Loss:1.5961, Validation Accuracy:0.4548\n",
    "Epoch #188: Loss:1.5130, Accuracy:0.4731, Validation Loss:1.6116, Validation Accuracy:0.4532\n",
    "Epoch #189: Loss:1.5114, Accuracy:0.4731, Validation Loss:1.5820, Validation Accuracy:0.4614\n",
    "Epoch #190: Loss:1.4991, Accuracy:0.4735, Validation Loss:1.5897, Validation Accuracy:0.4532\n",
    "Epoch #191: Loss:1.4954, Accuracy:0.4719, Validation Loss:1.5951, Validation Accuracy:0.4450\n",
    "Epoch #192: Loss:1.4999, Accuracy:0.4764, Validation Loss:1.5907, Validation Accuracy:0.4565\n",
    "Epoch #193: Loss:1.4947, Accuracy:0.4768, Validation Loss:1.5855, Validation Accuracy:0.4598\n",
    "Epoch #194: Loss:1.4926, Accuracy:0.4768, Validation Loss:1.5966, Validation Accuracy:0.4565\n",
    "Epoch #195: Loss:1.5016, Accuracy:0.4727, Validation Loss:1.6069, Validation Accuracy:0.4335\n",
    "Epoch #196: Loss:1.5169, Accuracy:0.4735, Validation Loss:1.5932, Validation Accuracy:0.4499\n",
    "Epoch #197: Loss:1.4896, Accuracy:0.4830, Validation Loss:1.5763, Validation Accuracy:0.4696\n",
    "Epoch #198: Loss:1.4899, Accuracy:0.4858, Validation Loss:1.5946, Validation Accuracy:0.4450\n",
    "Epoch #199: Loss:1.4895, Accuracy:0.4793, Validation Loss:1.5683, Validation Accuracy:0.4647\n",
    "Epoch #200: Loss:1.4747, Accuracy:0.4871, Validation Loss:1.5723, Validation Accuracy:0.4663\n",
    "Epoch #201: Loss:1.4757, Accuracy:0.4867, Validation Loss:1.5809, Validation Accuracy:0.4631\n",
    "Epoch #202: Loss:1.4756, Accuracy:0.4862, Validation Loss:1.5595, Validation Accuracy:0.4713\n",
    "Epoch #203: Loss:1.4736, Accuracy:0.4846, Validation Loss:1.5742, Validation Accuracy:0.4581\n",
    "Epoch #204: Loss:1.4729, Accuracy:0.4850, Validation Loss:1.5745, Validation Accuracy:0.4631\n",
    "Epoch #205: Loss:1.4767, Accuracy:0.4821, Validation Loss:1.5618, Validation Accuracy:0.4631\n",
    "Epoch #206: Loss:1.4593, Accuracy:0.4895, Validation Loss:1.5689, Validation Accuracy:0.4745\n",
    "Epoch #207: Loss:1.4631, Accuracy:0.4875, Validation Loss:1.5474, Validation Accuracy:0.4663\n",
    "Epoch #208: Loss:1.4601, Accuracy:0.4916, Validation Loss:1.5585, Validation Accuracy:0.4663\n",
    "Epoch #209: Loss:1.4526, Accuracy:0.4867, Validation Loss:1.5509, Validation Accuracy:0.4745\n",
    "Epoch #210: Loss:1.4546, Accuracy:0.4887, Validation Loss:1.5552, Validation Accuracy:0.4713\n",
    "Epoch #211: Loss:1.4542, Accuracy:0.4916, Validation Loss:1.5482, Validation Accuracy:0.4745\n",
    "Epoch #212: Loss:1.4538, Accuracy:0.4879, Validation Loss:1.5522, Validation Accuracy:0.4713\n",
    "Epoch #213: Loss:1.4526, Accuracy:0.4965, Validation Loss:1.5638, Validation Accuracy:0.4647\n",
    "Epoch #214: Loss:1.4477, Accuracy:0.4945, Validation Loss:1.5465, Validation Accuracy:0.4663\n",
    "Epoch #215: Loss:1.4405, Accuracy:0.4949, Validation Loss:1.5502, Validation Accuracy:0.4647\n",
    "Epoch #216: Loss:1.4405, Accuracy:0.4953, Validation Loss:1.5395, Validation Accuracy:0.4811\n",
    "Epoch #217: Loss:1.4442, Accuracy:0.4957, Validation Loss:1.5494, Validation Accuracy:0.4713\n",
    "Epoch #218: Loss:1.4377, Accuracy:0.4969, Validation Loss:1.5300, Validation Accuracy:0.4860\n",
    "Epoch #219: Loss:1.4331, Accuracy:0.4969, Validation Loss:1.5352, Validation Accuracy:0.4877\n",
    "Epoch #220: Loss:1.4283, Accuracy:0.5035, Validation Loss:1.5351, Validation Accuracy:0.4762\n",
    "Epoch #221: Loss:1.4321, Accuracy:0.5002, Validation Loss:1.5499, Validation Accuracy:0.4713\n",
    "Epoch #222: Loss:1.4392, Accuracy:0.5006, Validation Loss:1.5284, Validation Accuracy:0.4795\n",
    "Epoch #223: Loss:1.4299, Accuracy:0.4973, Validation Loss:1.5465, Validation Accuracy:0.4762\n",
    "Epoch #224: Loss:1.4318, Accuracy:0.4957, Validation Loss:1.5391, Validation Accuracy:0.4943\n",
    "Epoch #225: Loss:1.4437, Accuracy:0.4945, Validation Loss:1.5330, Validation Accuracy:0.4729\n",
    "Epoch #226: Loss:1.4283, Accuracy:0.5018, Validation Loss:1.5350, Validation Accuracy:0.4696\n",
    "Epoch #227: Loss:1.4231, Accuracy:0.5039, Validation Loss:1.5218, Validation Accuracy:0.4926\n",
    "Epoch #228: Loss:1.4191, Accuracy:0.5072, Validation Loss:1.5268, Validation Accuracy:0.4844\n",
    "Epoch #229: Loss:1.4095, Accuracy:0.5068, Validation Loss:1.5280, Validation Accuracy:0.4811\n",
    "Epoch #230: Loss:1.4216, Accuracy:0.5113, Validation Loss:1.5201, Validation Accuracy:0.4877\n",
    "Epoch #231: Loss:1.4229, Accuracy:0.5055, Validation Loss:1.5360, Validation Accuracy:0.4762\n",
    "Epoch #232: Loss:1.4137, Accuracy:0.5023, Validation Loss:1.5254, Validation Accuracy:0.4926\n",
    "Epoch #233: Loss:1.4190, Accuracy:0.5080, Validation Loss:1.5193, Validation Accuracy:0.4959\n",
    "Epoch #234: Loss:1.4136, Accuracy:0.5043, Validation Loss:1.5092, Validation Accuracy:0.4844\n",
    "Epoch #235: Loss:1.4020, Accuracy:0.5002, Validation Loss:1.5162, Validation Accuracy:0.4860\n",
    "Epoch #236: Loss:1.4003, Accuracy:0.5097, Validation Loss:1.5033, Validation Accuracy:0.5008\n",
    "Epoch #237: Loss:1.3990, Accuracy:0.5055, Validation Loss:1.5129, Validation Accuracy:0.4992\n",
    "Epoch #238: Loss:1.4029, Accuracy:0.5113, Validation Loss:1.5175, Validation Accuracy:0.4778\n",
    "Epoch #239: Loss:1.3942, Accuracy:0.5142, Validation Loss:1.5023, Validation Accuracy:0.5041\n",
    "Epoch #240: Loss:1.3958, Accuracy:0.5068, Validation Loss:1.5111, Validation Accuracy:0.4860\n",
    "Epoch #241: Loss:1.3962, Accuracy:0.5055, Validation Loss:1.4984, Validation Accuracy:0.4910\n",
    "Epoch #242: Loss:1.3847, Accuracy:0.5080, Validation Loss:1.5042, Validation Accuracy:0.4975\n",
    "Epoch #243: Loss:1.3825, Accuracy:0.5138, Validation Loss:1.4978, Validation Accuracy:0.5025\n",
    "Epoch #244: Loss:1.3834, Accuracy:0.5138, Validation Loss:1.4928, Validation Accuracy:0.5025\n",
    "Epoch #245: Loss:1.3860, Accuracy:0.5166, Validation Loss:1.5087, Validation Accuracy:0.4926\n",
    "Epoch #246: Loss:1.3886, Accuracy:0.5133, Validation Loss:1.4901, Validation Accuracy:0.5041\n",
    "Epoch #247: Loss:1.3816, Accuracy:0.5146, Validation Loss:1.5369, Validation Accuracy:0.4745\n",
    "Epoch #248: Loss:1.3935, Accuracy:0.5002, Validation Loss:1.5261, Validation Accuracy:0.4762\n",
    "Epoch #249: Loss:1.3982, Accuracy:0.4998, Validation Loss:1.4927, Validation Accuracy:0.4959\n",
    "Epoch #250: Loss:1.3857, Accuracy:0.5179, Validation Loss:1.4897, Validation Accuracy:0.5008\n",
    "Epoch #251: Loss:1.3756, Accuracy:0.5162, Validation Loss:1.4903, Validation Accuracy:0.4926\n",
    "Epoch #252: Loss:1.3779, Accuracy:0.5117, Validation Loss:1.4886, Validation Accuracy:0.5074\n",
    "Epoch #253: Loss:1.3743, Accuracy:0.5170, Validation Loss:1.4851, Validation Accuracy:0.4877\n",
    "Epoch #254: Loss:1.3693, Accuracy:0.5236, Validation Loss:1.4932, Validation Accuracy:0.4943\n",
    "Epoch #255: Loss:1.3710, Accuracy:0.5216, Validation Loss:1.5139, Validation Accuracy:0.4926\n",
    "Epoch #256: Loss:1.3773, Accuracy:0.5183, Validation Loss:1.4778, Validation Accuracy:0.4992\n",
    "Epoch #257: Loss:1.3661, Accuracy:0.5281, Validation Loss:1.4748, Validation Accuracy:0.5008\n",
    "Epoch #258: Loss:1.3608, Accuracy:0.5244, Validation Loss:1.4815, Validation Accuracy:0.5057\n",
    "Epoch #259: Loss:1.3545, Accuracy:0.5265, Validation Loss:1.4717, Validation Accuracy:0.5090\n",
    "Epoch #260: Loss:1.3507, Accuracy:0.5281, Validation Loss:1.4807, Validation Accuracy:0.5074\n",
    "Epoch #261: Loss:1.3514, Accuracy:0.5298, Validation Loss:1.4761, Validation Accuracy:0.5008\n",
    "Epoch #262: Loss:1.3550, Accuracy:0.5269, Validation Loss:1.4692, Validation Accuracy:0.5025\n",
    "Epoch #263: Loss:1.3477, Accuracy:0.5322, Validation Loss:1.4699, Validation Accuracy:0.5008\n",
    "Epoch #264: Loss:1.3480, Accuracy:0.5232, Validation Loss:1.4792, Validation Accuracy:0.5008\n",
    "Epoch #265: Loss:1.3466, Accuracy:0.5339, Validation Loss:1.4610, Validation Accuracy:0.5140\n",
    "Epoch #266: Loss:1.3409, Accuracy:0.5331, Validation Loss:1.4713, Validation Accuracy:0.5107\n",
    "Epoch #267: Loss:1.3414, Accuracy:0.5351, Validation Loss:1.4678, Validation Accuracy:0.5025\n",
    "Epoch #268: Loss:1.3525, Accuracy:0.5277, Validation Loss:1.4842, Validation Accuracy:0.5107\n",
    "Epoch #269: Loss:1.3440, Accuracy:0.5339, Validation Loss:1.4545, Validation Accuracy:0.5140\n",
    "Epoch #270: Loss:1.3450, Accuracy:0.5277, Validation Loss:1.4624, Validation Accuracy:0.5123\n",
    "Epoch #271: Loss:1.3446, Accuracy:0.5318, Validation Loss:1.4783, Validation Accuracy:0.5074\n",
    "Epoch #272: Loss:1.3449, Accuracy:0.5302, Validation Loss:1.4695, Validation Accuracy:0.5090\n",
    "Epoch #273: Loss:1.3483, Accuracy:0.5392, Validation Loss:1.4520, Validation Accuracy:0.5156\n",
    "Epoch #274: Loss:1.3442, Accuracy:0.5322, Validation Loss:1.4736, Validation Accuracy:0.5025\n",
    "Epoch #275: Loss:1.3300, Accuracy:0.5322, Validation Loss:1.4661, Validation Accuracy:0.4992\n",
    "Epoch #276: Loss:1.3268, Accuracy:0.5380, Validation Loss:1.4611, Validation Accuracy:0.5057\n",
    "Epoch #277: Loss:1.3168, Accuracy:0.5372, Validation Loss:1.4444, Validation Accuracy:0.5123\n",
    "Epoch #278: Loss:1.3181, Accuracy:0.5413, Validation Loss:1.4471, Validation Accuracy:0.5189\n",
    "Epoch #279: Loss:1.3148, Accuracy:0.5405, Validation Loss:1.4457, Validation Accuracy:0.5107\n",
    "Epoch #280: Loss:1.3151, Accuracy:0.5454, Validation Loss:1.4552, Validation Accuracy:0.5090\n",
    "Epoch #281: Loss:1.3117, Accuracy:0.5409, Validation Loss:1.4419, Validation Accuracy:0.5205\n",
    "Epoch #282: Loss:1.3086, Accuracy:0.5446, Validation Loss:1.4431, Validation Accuracy:0.5107\n",
    "Epoch #283: Loss:1.3067, Accuracy:0.5462, Validation Loss:1.4531, Validation Accuracy:0.5041\n",
    "Epoch #284: Loss:1.3075, Accuracy:0.5454, Validation Loss:1.4440, Validation Accuracy:0.5041\n",
    "Epoch #285: Loss:1.3039, Accuracy:0.5491, Validation Loss:1.4540, Validation Accuracy:0.5057\n",
    "Epoch #286: Loss:1.3043, Accuracy:0.5474, Validation Loss:1.4551, Validation Accuracy:0.5057\n",
    "Epoch #287: Loss:1.3109, Accuracy:0.5454, Validation Loss:1.4429, Validation Accuracy:0.5140\n",
    "Epoch #288: Loss:1.3114, Accuracy:0.5470, Validation Loss:1.4393, Validation Accuracy:0.5090\n",
    "Epoch #289: Loss:1.2987, Accuracy:0.5532, Validation Loss:1.4483, Validation Accuracy:0.5090\n",
    "Epoch #290: Loss:1.2986, Accuracy:0.5524, Validation Loss:1.4525, Validation Accuracy:0.5123\n",
    "Epoch #291: Loss:1.3035, Accuracy:0.5503, Validation Loss:1.4447, Validation Accuracy:0.5156\n",
    "Epoch #292: Loss:1.3031, Accuracy:0.5454, Validation Loss:1.4323, Validation Accuracy:0.5189\n",
    "Epoch #293: Loss:1.2918, Accuracy:0.5495, Validation Loss:1.4365, Validation Accuracy:0.5172\n",
    "Epoch #294: Loss:1.2865, Accuracy:0.5552, Validation Loss:1.4326, Validation Accuracy:0.5123\n",
    "Epoch #295: Loss:1.2838, Accuracy:0.5532, Validation Loss:1.4276, Validation Accuracy:0.5140\n",
    "Epoch #296: Loss:1.2926, Accuracy:0.5536, Validation Loss:1.4303, Validation Accuracy:0.5222\n",
    "Epoch #297: Loss:1.2802, Accuracy:0.5536, Validation Loss:1.4155, Validation Accuracy:0.5222\n",
    "Epoch #298: Loss:1.2751, Accuracy:0.5643, Validation Loss:1.4319, Validation Accuracy:0.5172\n",
    "Epoch #299: Loss:1.2904, Accuracy:0.5483, Validation Loss:1.4254, Validation Accuracy:0.5172\n",
    "Epoch #300: Loss:1.2823, Accuracy:0.5552, Validation Loss:1.4219, Validation Accuracy:0.5205\n",
    "\n",
    "Test:\n",
    "Test Loss:1.42189085, Accuracy:0.5205\n",
    "Labels: ['ek', 'eg', 'eo', 'yd', 'sg', 'ib', 'mb', 'my', 'eb', 'ck', 'ds', 'by', 'sk', 'aa', 'ce']\n",
    "Confusion Matrix:\n",
    "      ek  eg  eo  yd  sg  ib  mb  my  eb  ck  ds  by  sk  aa  ce\n",
    "t:ek  32   0   0   0   0   0   2   0  12   0   1   0   0   1   0\n",
    "t:eg   1  35   1   1   2   1   0   0   2   1   2   1   1   1   1\n",
    "t:eo   0   2  20   0   1   1   0   0   0   0   0   8   0   0   2\n",
    "t:yd   7   0   0  41   4   1   3   0   1   0   2   0   0   3   0\n",
    "t:sg   1   3   0   8  30   1   6   0   0   0   0   0   0   2   0\n",
    "t:ib   1   1   0   4  13  18   3   0   1   0   0   2   0   5   6\n",
    "t:mb   2   2   0  16   6   5  14   0   2   1   0   0   2   2   0\n",
    "t:my   1   2   0   0   0   1   0   1   4   6   1   0   4   0   0\n",
    "t:eb   6   0   0   0   0   0   0   1  41   0   2   0   0   0   0\n",
    "t:ck   0   1   0   0   0   0   1   0   1  13   0   0   4   2   1\n",
    "t:ds   6   7   0   2   0   0   3   0   4   1   5   0   3   0   0\n",
    "t:by   0   1   5   4   4   3   0   0   0   0   0  19   0   1   3\n",
    "t:sk   0   0   0   1   0   1   1   1   6   7   2   0  12   2   0\n",
    "t:aa   2   0   0   3   0   0   0   0   1   0   0   0   0  28   0\n",
    "t:ce   0   2   0   0   2  10   0   0   0   4   0   1   0   0   8\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ek       0.54      0.67      0.60        48\n",
    "          eg       0.62      0.70      0.66        50\n",
    "          eo       0.77      0.59      0.67        34\n",
    "          yd       0.51      0.66      0.58        62\n",
    "          sg       0.48      0.59      0.53        51\n",
    "          ib       0.43      0.33      0.38        54\n",
    "          mb       0.42      0.27      0.33        52\n",
    "          my       0.33      0.05      0.09        20\n",
    "          eb       0.55      0.82      0.66        50\n",
    "          ck       0.39      0.57      0.46        23\n",
    "          ds       0.33      0.16      0.22        31\n",
    "          by       0.61      0.47      0.54        40\n",
    "          sk       0.46      0.36      0.41        33\n",
    "          aa       0.60      0.82      0.69        34\n",
    "          ce       0.38      0.30      0.33        27\n",
    "\n",
    "    accuracy                           0.52       609\n",
    "   macro avg       0.50      0.49      0.48       609\n",
    "weighted avg       0.51      0.52      0.50       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 13:32:49 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 52 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.702309595931731, 2.6946964455747056, 2.6878893132671737, 2.6821115498472317, 2.6770921379866075, 2.6726396866815625, 2.6686692398365692, 2.6648883788260727, 2.6607369206026075, 2.655634977547406, 2.648438885881396, 2.637910885959619, 2.621289010901365, 2.5968387029049627, 2.5630941668950475, 2.5249553454920575, 2.489745429388212, 2.4607718096578064, 2.434736790523936, 2.4095076700345244, 2.3865257112067715, 2.359119466568645, 2.330092220666569, 2.3019697075015415, 2.2764099595581957, 2.2603318338910934, 2.2331314184787043, 2.210245542729821, 2.1979335528876396, 2.1809720112185174, 2.16464384201125, 2.1517730702711835, 2.1482187534983717, 2.1317713949676413, 2.1135747573645833, 2.1178717155174667, 2.0933433706537254, 2.087814616452297, 2.0789833863576255, 2.066238921264122, 2.0695373871056315, 2.0641651051776555, 2.048377543638884, 2.043811657550104, 2.042693608695846, 2.028515831786032, 2.0288205624409694, 2.0234444644455056, 2.0166630739061704, 2.010318174542269, 2.0176182942241674, 1.9997358940700787, 1.998073891661633, 1.9912089031122393, 1.9976520178157513, 1.9824373610501218, 1.9853401027485262, 1.9792783062642039, 1.9862615441649614, 1.9704475081808657, 1.9664057383592102, 1.9593236728254797, 1.9628431016001209, 1.953236041397884, 1.956538308038696, 1.9475773659049975, 1.9498846671851398, 1.939994840199137, 1.940932377022867, 1.9368228344690233, 1.9383643869500247, 1.9338840723820703, 1.925913944620217, 1.925675323444047, 1.9321509270832455, 1.9223982757339728, 1.9136677933443944, 1.9218207985309546, 1.9133607990831774, 1.9095785696126752, 1.9047481036930054, 1.9030458065872318, 1.8975236488289042, 1.896869148526873, 1.8953057611712878, 1.8956987662072644, 1.8886457110077681, 1.8954892144806084, 1.8821366034900808, 1.8749063332092586, 1.889868272544911, 1.8814183818099925, 1.8935993293236042, 1.8694955155571498, 1.8693520983647438, 1.8624137549956248, 1.8734021184871155, 1.8551668087249906, 1.8522822160047459, 1.8456329788480486, 1.849905686229712, 1.842220357877672, 1.8289011217690454, 1.8307785103082266, 1.8258835568608125, 1.8177149822363516, 1.8175302591229894, 1.8087459700839665, 1.8073240720188284, 1.8131033947510868, 1.7922735075253766, 1.8013294469351055, 1.7944313843653512, 1.7841802616229003, 1.786521950378794, 1.7887313248488703, 1.7750300437163054, 1.7696412901572993, 1.7651005104453301, 1.7546416440816544, 1.7487573790041293, 1.753680042836858, 1.7356944469787021, 1.7409045923323858, 1.7401994024395746, 1.7265163810773827, 1.729009853795244, 1.721801123399844, 1.7163953350486818, 1.722447068233208, 1.7261472850401804, 1.7149202864745567, 1.703915040089775, 1.6992392436232668, 1.6979196075539675, 1.69646559521091, 1.693289295793167, 1.690589646987727, 1.694818851395781, 1.6817737526102803, 1.6863377098183718, 1.684762067591224, 1.6773006430596162, 1.6790730101721627, 1.6758195803866207, 1.6690060734161603, 1.6730393367056384, 1.6707571921090187, 1.6736476742379576, 1.668480249461282, 1.6532223962602162, 1.6584712225815346, 1.7150937105438784, 1.6551407097987159, 1.6528767312101542, 1.6523337845731838, 1.6743200813803962, 1.646319183810004, 1.6460698762746475, 1.6496410238723254, 1.642753215258932, 1.640187109828191, 1.6458908828412762, 1.6430365831785405, 1.628224355638125, 1.6506630973079912, 1.633006907253234, 1.6294593924567813, 1.6357332926078383, 1.6220636166179514, 1.6236238452209824, 1.6289002370756052, 1.6215454807813923, 1.6186873979364906, 1.6250539673568776, 1.6218682721330615, 1.6049220603087853, 1.6378325355072523, 1.6019247355328012, 1.6033498884617596, 1.6057208120725033, 1.599310762189292, 1.5936691896081558, 1.588807609672421, 1.6138925497559296, 1.5883595191786442, 1.5960831689129908, 1.6116258538219532, 1.5819644481677728, 1.5896806959643937, 1.5950883086679017, 1.590718856977516, 1.5854529993874686, 1.5966090048083728, 1.6068692277804972, 1.5932188229803577, 1.576290127679045, 1.5946389237060923, 1.5682837037225859, 1.5722890833915748, 1.580943582680425, 1.5595024600992062, 1.5742036190330493, 1.5744856139904955, 1.5618032269877167, 1.5688500913297405, 1.5473799145671925, 1.558506059333413, 1.550881091597045, 1.5551885863634558, 1.5481549834187198, 1.5522098085171678, 1.5638064206723117, 1.5464890451462594, 1.550242036042738, 1.5394728927580985, 1.54940620823251, 1.5300102337632078, 1.535240645087607, 1.5351250367407336, 1.5499345283398682, 1.5284126670098266, 1.5464511876818778, 1.5391424060455097, 1.53300121891479, 1.5349785312643192, 1.521779760351322, 1.5267988104734123, 1.5279515782013315, 1.5200932108122727, 1.53601041998965, 1.5253650762373199, 1.5193215044848438, 1.5091622443426222, 1.5161950138010611, 1.5032595916726124, 1.512939126229247, 1.5175409170207133, 1.502301339836935, 1.511148940753467, 1.498372290130515, 1.5041532489075058, 1.497797464502269, 1.4928426811064797, 1.5086997228694472, 1.4900911318257524, 1.536908931920094, 1.5261144495166972, 1.492653562126097, 1.48966055296129, 1.4903123762415744, 1.4885755747997116, 1.485141581502454, 1.493192717946809, 1.5138736540460822, 1.477823350034128, 1.4748410667691911, 1.4815444010623375, 1.4717306382158903, 1.480729234238172, 1.476067884410739, 1.469207608053837, 1.4699264482911585, 1.4791745920290893, 1.460990324591964, 1.4713184669100006, 1.4678024862004422, 1.484153292644983, 1.454510295723851, 1.4623834301881211, 1.4782961434722925, 1.46953386097706, 1.451995361614697, 1.4736379439803375, 1.4660875480163273, 1.4611126270591723, 1.4444120155375189, 1.4470850885012272, 1.4457375451261774, 1.4551520979854664, 1.4418646931061017, 1.4431300112375094, 1.453052257865129, 1.4440304414783596, 1.4539991161114671, 1.4551384613431733, 1.4429283097068273, 1.439339143693545, 1.4482827513480225, 1.4525240416988754, 1.4447107959067684, 1.4322750002684068, 1.4365200755631395, 1.4326150885160724, 1.4275655045689424, 1.4303108285409085, 1.4154662772743964, 1.4319378008396166, 1.4254073683655712, 1.421890840937547], 'val_acc': [0.08538587798772775, 0.08538587798772775, 0.08538587798772775, 0.08538587798772775, 0.08538587798772775, 0.08538587798772775, 0.08538587798772775, 0.06732348091472154, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10837438383272716, 0.13136288997134551, 0.14285714175607184, 0.1494252863531238, 0.16091953932456, 0.16091953932456, 0.16256157554722772, 0.1576354670749705, 0.1773399008661264, 0.1904761900602303, 0.19047619035384925, 0.20032840700474475, 0.211822659682562, 0.224958948583047, 0.24302134634728112, 0.23645319967042833, 0.26108374164022247, 0.270935958388991, 0.2922824283049416, 0.2824302113604272, 0.2545155972389165, 0.30377668088488585, 0.31691297017686276, 0.26436781388981195, 0.30541871759691847, 0.31034482597130275, 0.2988505729998665, 0.29392446442973635, 0.3234811150675337, 0.2955665006524041, 0.3070607536238403, 0.3185550063016575, 0.2939244645276093, 0.3300492591752207, 0.30213464515158306, 0.3300492593709667, 0.31691297017686276, 0.30541871720542657, 0.315270934052068, 0.3103448256776838, 0.315270934052068, 0.32183907874699297, 0.3021346449558371, 0.3300492592730937, 0.30870278955288905, 0.33169129549576143, 0.3185550063016575, 0.3234811147739147, 0.33169129539788844, 0.3267651869256312, 0.32019704252432524, 0.33169129549576143, 0.31362889802514626, 0.3399014759239892, 0.31198686170460554, 0.34154351214665696, 0.32512315109445544, 0.33169129549576143, 0.32183907884486596, 0.33990147602186216, 0.3267651872192502, 0.3234811147739147, 0.34811165684158185, 0.315270934345687, 0.3333333316205562, 0.33333333191417513, 0.3267651872192502, 0.3316912955936344, 0.3431855485650706, 0.34482758468986535, 0.3366174041637646, 0.33661740406589163, 0.34482758478773834, 0.3300492594688397, 0.3579638737860963, 0.33004925966458565, 0.3464696209125331, 0.35632183766130154, 0.3431855487608166, 0.3513957294826633, 0.36453201838314825, 0.35303776560745803, 0.35303776560745803, 0.36124794632930474, 0.3448275851792303, 0.35960591040025597, 0.3563218381506665, 0.36781609121997566, 0.37766830787087113, 0.36288998255197247, 0.3711001631759462, 0.3645320187746402, 0.37931034428928484, 0.3825944166367473, 0.3842364525657961, 0.3776683081644901, 0.37110016356743813, 0.3661740550951809, 0.39080459696710207, 0.403940886063333, 0.39573070573297825, 0.36945812714902443, 0.3875205246196396, 0.3809523803162066, 0.38752052491325856, 0.4088669945355902, 0.4088669946334632, 0.40394088596546, 0.397372741662027, 0.3924466330918968, 0.4088669945355902, 0.4203612475070264, 0.41871921148010466, 0.4121510669809257, 0.41050903066038497, 0.41379310290997445, 0.41871921138223167, 0.40722495850866847, 0.41050903095400393, 0.41707717515956394, 0.4252873561750296, 0.4203612478006454, 0.4203612477027724, 0.42200328382756713, 0.43349753670113034, 0.4334975367990033, 0.43513957272805215, 0.4236453199523619, 0.4334975367990033, 0.4285714282288731, 0.4318555006742086, 0.41050903095400393, 0.4334975367990033, 0.4367816091464658, 0.4318555006742086, 0.4466338212462677, 0.4318555006742086, 0.44991789814482375, 0.43513957292379807, 0.4334975368968763, 0.3990147780804407, 0.4302134599983203, 0.4400656812981823, 0.4433497535477718, 0.4417077173251041, 0.4433497490945512, 0.43842364091591296, 0.44170771296975647, 0.45155993417174556, 0.45155992962065195, 0.44499178967256653, 0.4466338213441407, 0.4515599298163979, 0.438423640720167, 0.4515599299142709, 0.45155992962065195, 0.4351395728259251, 0.4515599298163979, 0.45320196574544674, 0.44006567674708874, 0.4499178934958572, 0.4597701102446257, 0.4466338215398867, 0.4466338215398867, 0.4581280742177039, 0.42692939220195136, 0.46141214636942046, 0.4548440021638604, 0.4581280743155769, 0.4581280787687975, 0.4597701103424987, 0.4679802910643454, 0.45320196594119266, 0.4597701102446257, 0.4548440021638604, 0.45320196574544674, 0.4614121465651664, 0.4532019701986673, 0.44499178512147297, 0.45648603819078215, 0.45977011489359226, 0.4564860384844011, 0.43349753224790977, 0.4499178934958572, 0.46962233154448774, 0.44499178521934596, 0.46469621871688294, 0.4663382548416777, 0.4630541826899611, 0.4712643632160619, 0.4581280742177039, 0.4630541869474358, 0.4630541826899611, 0.4745484355635244, 0.4663382551352966, 0.4663382547438047, 0.4745484354656514, 0.4712643632160619, 0.47454844001674495, 0.4712643634118079, 0.46469621871688294, 0.46633825939277124, 0.4646962232679765, 0.48111657996483037, 0.4712643632160619, 0.4860426884370876, 0.48768472456188233, 0.4761904717861921, 0.47126436776715547, 0.4794745441336546, 0.4761904717861921, 0.4942528689631883, 0.47290639924298367, 0.46962232718914015, 0.4926108328383936, 0.48440065211654687, 0.48111657996483037, 0.48768472436613636, 0.47619047159044614, 0.4926108333277585, 0.4958949096390766, 0.48440065221441986, 0.4860426885349606, 0.5008210136581133, 0.49917897743544554, 0.47783250781311387, 0.5041050857119568, 0.4860426882413416, 0.49096879671359883, 0.4975369414085238, 0.502463049880781, 0.502463049880781, 0.49261083293626656, 0.5041050861034487, 0.47454843991887197, 0.47619047159044614, 0.49589490508798306, 0.5008210136581133, 0.49261083293626656, 0.5073891580594193, 0.48768472436613636, 0.4942528689631883, 0.4926108328383936, 0.49917897733757255, 0.5008210134623673, 0.5057471219346246, 0.50903119437996, 0.5073891584509111, 0.5008210133644944, 0.502463049587162, 0.5008210133644944, 0.5008210137559863, 0.5139573028522172, 0.5106732302111358, 0.5024630501744, 0.5106732347622294, 0.5139573026564712, 0.5123152667274224, 0.5073891583530382, 0.509031194282087, 0.515599338879139, 0.502463049587162, 0.49917898188866616, 0.5057471220324975, 0.5123152666295495, 0.5188834111287285, 0.5106732304068817, 0.509031194184214, 0.5205254471556502, 0.5106732304068817, 0.5041050859077028, 0.5041050859077028, 0.5057471219346246, 0.5057471217388786, 0.5139573023628523, 0.509031194184214, 0.509031194282087, 0.5123152664338035, 0.51559933858552, 0.5188834109329825, 0.5172413750039337, 0.5123152664338035, 0.5139573023628523, 0.522167483182572, 0.522167483182572, 0.5172413748081878, 0.5172413747103147, 0.5205254468620313], 'loss': [2.708048759495698, 2.6991254193582086, 2.6923846303559915, 2.6857891174802058, 2.680374340844595, 2.6757105528696363, 2.6713976505600696, 2.6675164149282407, 2.6634466857636, 2.6588789414086627, 2.6531379245389903, 2.64484274382464, 2.632888913791038, 2.6138189476373506, 2.587104567463148, 2.551783173284981, 2.517260636049619, 2.4843359055215575, 2.4554597438483268, 2.4282794866228983, 2.3985514533103616, 2.3712539522065272, 2.342489935681071, 2.314934646618195, 2.2851557757085845, 2.259420646338492, 2.2407102408595154, 2.211332180955327, 2.197562805681013, 2.1783680866875934, 2.1626736782904277, 2.1501141198606706, 2.137889221218822, 2.133445074230249, 2.120800412508986, 2.107122152395072, 2.1037184512590725, 2.0895270293742967, 2.0802285791667336, 2.07229383261052, 2.0663230688420167, 2.063721953378321, 2.0519021384769887, 2.048601090785659, 2.039352990077996, 2.0372884273529053, 2.0302163290536868, 2.023384458573202, 2.020695350596058, 2.021142494654019, 2.0150635791755067, 2.006804234291249, 2.0001439476404834, 1.992891105538276, 1.9935346054590213, 1.9891922711346919, 1.9834424288121093, 1.9826248070053007, 1.9834897321352478, 1.974092871452504, 1.9688889713013196, 1.9615390738912186, 1.9616710905176904, 1.9575998832557726, 1.9507482862570447, 1.9497415657160952, 1.944409226294171, 1.9422548499195482, 1.9370173013185819, 1.9395304844854304, 1.935369058849875, 1.9316630147076241, 1.9277228989395518, 1.9248740496821473, 1.918969718484663, 1.9204474063869375, 1.9131874250435486, 1.9083329878303794, 1.904854670099654, 1.9057382821547177, 1.8959151392844669, 1.895092343990318, 1.8921519747504953, 1.887160384679477, 1.8824082195391645, 1.8805500837322133, 1.8757013984284607, 1.875012712948621, 1.8705102656656223, 1.8639011511322898, 1.8644280097812598, 1.8671041648245934, 1.8628707244900462, 1.8614777170901915, 1.8534055418547175, 1.8524418651200907, 1.8488292372936586, 1.8340305033650486, 1.8316972523499317, 1.8247869990934336, 1.8207990251282646, 1.8227983301425128, 1.815511420130485, 1.8061259685845346, 1.8026560731492247, 1.7938930236828157, 1.7844451416199703, 1.7800201516376628, 1.7728029574212107, 1.7706024053894764, 1.768433890205634, 1.7650827253623664, 1.7596081912395156, 1.7483271771632671, 1.7414498848591986, 1.7443934603148663, 1.7445884018218494, 1.7297747238950318, 1.7275231905296842, 1.7237537880452996, 1.7098675515862216, 1.7053297369387115, 1.7055033875197112, 1.7028881275188752, 1.6960738536513562, 1.6866099566160042, 1.6822556377681128, 1.6815826402797347, 1.6781760180510534, 1.6752849970020554, 1.6772931431842781, 1.6783900401920264, 1.6651101173071892, 1.6585152202073554, 1.6511576106171344, 1.6464651204232563, 1.645838837555057, 1.63930211091678, 1.6430148674966862, 1.6385193951565626, 1.6341325641412754, 1.6287627227497297, 1.6280330333866377, 1.6266471983226174, 1.6234793382993224, 1.613134172564904, 1.6124037510560523, 1.61356434327621, 1.603947911810826, 1.6075874666413732, 1.6056110749254482, 1.608006753030499, 1.6095859968197175, 1.6235587136211826, 1.597908733806571, 1.5979269941729441, 1.5916778282463184, 1.586272973201603, 1.5753268532194886, 1.573438555797757, 1.5699815123477756, 1.567388602546598, 1.559673556997546, 1.561176901186761, 1.5576382897962535, 1.5582269384630896, 1.5716755517454364, 1.5541990935190502, 1.5498475946195316, 1.5466068231104826, 1.5434483753337507, 1.5423356908057995, 1.541480479005426, 1.5457924418380862, 1.5445935378084437, 1.5365067678310542, 1.537148270861569, 1.526909236447767, 1.5312143027660048, 1.5244420389375157, 1.515525156271776, 1.5134182082554153, 1.5079422084702603, 1.5034269857700355, 1.5126169365779085, 1.5086927861403636, 1.5090153561480482, 1.513006648831299, 1.5113519952527306, 1.499098503075586, 1.4954488336183207, 1.4998778855776151, 1.4946905346132158, 1.4925754998988439, 1.5015799022063583, 1.5169145376530515, 1.4896090820829482, 1.4899267604463644, 1.4895372135193685, 1.4747287949987016, 1.4756569282230165, 1.4756301364859516, 1.473570150173665, 1.47285093091107, 1.4767077570823184, 1.4593363992976947, 1.4630567943780575, 1.4600538051593475, 1.4526030072930902, 1.45462628238255, 1.4541651979364163, 1.4538373752296339, 1.4526492586370856, 1.4477372903843435, 1.4404623448726332, 1.4405472180192231, 1.4442013291607647, 1.4377095055531182, 1.4330988726821523, 1.4283146247237126, 1.4320728805765235, 1.4392047904110543, 1.429894740087051, 1.43177697957174, 1.4437482014329037, 1.42829088273724, 1.4230953114723033, 1.419076727841669, 1.4095104786894406, 1.4216236398450157, 1.422932608171655, 1.413717360369234, 1.4189669970369436, 1.4136086010100661, 1.401980265650661, 1.4003452250600106, 1.3990384612974445, 1.4028500444835217, 1.3942280000729728, 1.395788708800408, 1.396214532705303, 1.384685022532328, 1.382518609887031, 1.3833859457372395, 1.3860287340759496, 1.388565094123386, 1.381587003143906, 1.3935483935432513, 1.3982462802217237, 1.3857469041244694, 1.3755801238073706, 1.3779058727150826, 1.3743031975669782, 1.3692706910491723, 1.3709775270622613, 1.3773194825135218, 1.36614566182209, 1.3608256329256407, 1.3544562922855667, 1.350711206880683, 1.351381721437834, 1.355000244616483, 1.3477233980225831, 1.3479999851152393, 1.3465555377564635, 1.340875002392998, 1.341379545650443, 1.3525337323026245, 1.344005774276702, 1.3450099967098823, 1.3446241468374733, 1.3448997830463385, 1.3482795782402555, 1.344191878434324, 1.3299974111560924, 1.3268236423664759, 1.3168119855974734, 1.3181052566308995, 1.3148044157321939, 1.3150936832173403, 1.3117209280785593, 1.3086105650700093, 1.3066986477105769, 1.3075340224976901, 1.303873822723326, 1.3042943468328865, 1.310903133748738, 1.311432115008454, 1.298672068779963, 1.2986008972602703, 1.3034883306746121, 1.3030760385662135, 1.2918160495327238, 1.2865446614044158, 1.2838012602294984, 1.2926095041651011, 1.2801619153247967, 1.2751400360091756, 1.2904272523993585, 1.2823171807510407], 'acc': [0.085010267482034, 0.08501026668954924, 0.08501026728620764, 0.08501026728620764, 0.08501026667119052, 0.08501026649372288, 0.08501026708120193, 0.08501026689455495, 0.09486653031570956, 0.10225872644347576, 0.10225872663930212, 0.10225872722678116, 0.10472279294437942, 0.11498973376941876, 0.1392197126297001, 0.15687885001087581, 0.16180698084023454, 0.1659137586059022, 0.16755646866817003, 0.16837782429596237, 0.18562628423653588, 0.19835729037834143, 0.20739219699674563, 0.20205338915515483, 0.21314168288477636, 0.2209445579830381, 0.2168377831914831, 0.22915811055861948, 0.2431211507295926, 0.26160164115610063, 0.272689937039812, 0.2841889093666351, 0.28870636751030015, 0.2813141674109308, 0.29240246172803136, 0.30349075976583256, 0.3039014393788833, 0.2956878830642426, 0.31006160263408133, 0.2997946598324198, 0.3055441476603553, 0.30225872890660405, 0.3104722814638267, 0.3112936340807889, 0.32279260879424565, 0.306365504071453, 0.33347022408577454, 0.3137576992017288, 0.3268993836041593, 0.3322381944015041, 0.32032854030754043, 0.32197125297062695, 0.33018480650698134, 0.32731006278883995, 0.32854209623052844, 0.3190965116024017, 0.33429158033776335, 0.32484599512215756, 0.3289527738485983, 0.32689938477911745, 0.333059549245991, 0.3330595496376437, 0.332648870024593, 0.3314168356037727, 0.3330595500292964, 0.33511293436222744, 0.3375770044155434, 0.33511293792381913, 0.3383983576567021, 0.3363449674122632, 0.3404517453921159, 0.3371663264058209, 0.33839835926003037, 0.3400410677373287, 0.3392197130886681, 0.33921971312538557, 0.34455852075279125, 0.3445585235310776, 0.34825461990289863, 0.3462012324000286, 0.3544147837822932, 0.3466119106422949, 0.3498973292002198, 0.3589322387560192, 0.3507186858071439, 0.3605749476616877, 0.35400410698424617, 0.365913759670708, 0.3638603691937253, 0.36919918018689635, 0.36180698008752704, 0.37207392077181617, 0.3683778229924932, 0.37494866488161027, 0.3687885004147367, 0.37002053503138327, 0.3679671455702498, 0.3778234093830571, 0.38028747430817056, 0.3811088315025737, 0.37905544106230843, 0.3856262811522709, 0.3913757705835346, 0.38521560611666106, 0.3926078030460914, 0.3995893211824938, 0.3979466114568025, 0.39137576980022926, 0.3991786433318802, 0.3995893217332554, 0.395893225165608, 0.4020533902199606, 0.4016427086486464, 0.40246406666307233, 0.40574948858676263, 0.4020533862667162, 0.4020533884575235, 0.40739219549745015, 0.4078028741313692, 0.4135523629384364, 0.41396304114398524, 0.41971252482285, 0.41601642743517975, 0.41478439634340747, 0.4143737161795951, 0.4258726922271188, 0.4275153995661765, 0.4229979461590612, 0.42587268909389725, 0.42135523686174003, 0.4221765903354425, 0.42464065721881955, 0.42176591310902545, 0.42669404472168954, 0.43367556364139737, 0.43696098376593306, 0.43162217750931176, 0.4365503085344969, 0.4340862430219043, 0.4344969177025789, 0.4353182756802874, 0.4365503085344969, 0.43983572709242175, 0.43942505088185385, 0.4398357300665344, 0.43860369466658244, 0.44024640650964614, 0.4369609853692613, 0.43531827489698205, 0.4353182766961366, 0.439014375846244, 0.44353182941246816, 0.4443531822986916, 0.4324435315704933, 0.4402464059221671, 0.4496919940751681, 0.44024640870045345, 0.4443531850402605, 0.4480492818404517, 0.4579055444783009, 0.45174538099055905, 0.4451745371064611, 0.45297741443224754, 0.4628336751118333, 0.4570841880304857, 0.45585215693871345, 0.4501026697349744, 0.45667351319070226, 0.46078028522232967, 0.4587269008893986, 0.4537987664984482, 0.462423000039506, 0.4616016406542956, 0.4554414783782293, 0.4554414772032712, 0.4644763877382024, 0.46406570812515163, 0.46652977657513944, 0.4698151959898046, 0.46940451543433953, 0.4698151963814573, 0.46858316391890054, 0.472689937125486, 0.47227926009489524, 0.46611909464888995, 0.46365503093545196, 0.4685831625481161, 0.4731006177176685, 0.4731006181093212, 0.4735112921657993, 0.4718685813018674, 0.47638603885805336, 0.4767967149095124, 0.4767967140894896, 0.47268993951211963, 0.47351129572739104, 0.48295687777305774, 0.4858316201204147, 0.47926077862295036, 0.4870636572828038, 0.48665297574820704, 0.4862423014959026, 0.48459958785368434, 0.48501026468844877, 0.48213552429935524, 0.4895277224404611, 0.4874743331384365, 0.4915811087316556, 0.4866529782939496, 0.48870636344690344, 0.49158110657756576, 0.4878850082107638, 0.4965092426942359, 0.4944558510790126, 0.49486653006786685, 0.49527720572767314, 0.49568788393322205, 0.4969199175707369, 0.4969199177665632, 0.5034907590682012, 0.5002053379278163, 0.5006160157417126, 0.49733059577628574, 0.49568788549983284, 0.4944558515073827, 0.5018480527082753, 0.5039014402111452, 0.5071868566516978, 0.5067761794252806, 0.5112936336157013, 0.5055441510750772, 0.5022587275847762, 0.5080082146661238, 0.504312116654257, 0.5002053389069481, 0.5096509262766437, 0.5055441491168138, 0.511293636589814, 0.5141683787413446, 0.5067761811877178, 0.5055441495084665, 0.5080082136502746, 0.5137577012823837, 0.5137577016740364, 0.5166324436297407, 0.5133470238601403, 0.5145790582809605, 0.5002053387111217, 0.49979466109305193, 0.5178644776956257, 0.5162217656200182, 0.5117043122129029, 0.5170431249685111, 0.52361396568267, 0.5215605777881474, 0.5182751582143733, 0.5281314179148272, 0.5244353207229834, 0.5264887099882906, 0.5281314181106536, 0.5297741285829328, 0.5268993864314022, 0.5322381976203997, 0.5232032863021632, 0.5338809067218945, 0.5330595514857549, 0.5351129384011458, 0.5277207375551886, 0.5338809057427628, 0.527720742255021, 0.5318275186315454, 0.5301848012319091, 0.5392197136027123, 0.5322381942913518, 0.5322381974245733, 0.5379876815318082, 0.5371663274706266, 0.541273102084714, 0.5404517497859697, 0.5453798721213605, 0.540862426620734, 0.5445585240084043, 0.5462012275533265, 0.5453798721213605, 0.549075971858947, 0.5474332677265457, 0.5453798776779332, 0.5470225889335179, 0.5531827486271241, 0.5523614009058206, 0.5503080118363398, 0.5453798788528912, 0.5494866552294158, 0.555236143449004, 0.553182747452166, 0.5535934246785832, 0.553593432585072, 0.5642710454899672, 0.5482546181894181, 0.5552361418823931]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
