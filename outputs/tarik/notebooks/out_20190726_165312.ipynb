{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf10.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 16:53:12 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'AllShfUni', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['by', 'mb', 'eb', 'yd', 'ib', 'sg', 'sk', 'eg', 'ek', 'aa', 'eo', 'ce', 'ck', 'my', 'ds'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002BE1502E278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002BE11E06EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7165, Accuracy:0.0476, Validation Loss:2.7091, Validation Accuracy:0.0361\n",
    "Epoch #2: Loss:2.7046, Accuracy:0.0563, Validation Loss:2.6975, Validation Accuracy:0.0772\n",
    "Epoch #3: Loss:2.6941, Accuracy:0.0858, Validation Loss:2.6885, Validation Accuracy:0.1346\n",
    "Epoch #4: Loss:2.6856, Accuracy:0.1142, Validation Loss:2.6811, Validation Accuracy:0.1100\n",
    "Epoch #5: Loss:2.6786, Accuracy:0.0924, Validation Loss:2.6745, Validation Accuracy:0.0837\n",
    "Epoch #6: Loss:2.6729, Accuracy:0.0830, Validation Loss:2.6689, Validation Accuracy:0.0837\n",
    "Epoch #7: Loss:2.6675, Accuracy:0.0830, Validation Loss:2.6640, Validation Accuracy:0.0837\n",
    "Epoch #8: Loss:2.6623, Accuracy:0.0830, Validation Loss:2.6583, Validation Accuracy:0.0837\n",
    "Epoch #9: Loss:2.6567, Accuracy:0.0879, Validation Loss:2.6520, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6503, Accuracy:0.0961, Validation Loss:2.6445, Validation Accuracy:0.1215\n",
    "Epoch #11: Loss:2.6426, Accuracy:0.1109, Validation Loss:2.6354, Validation Accuracy:0.1232\n",
    "Epoch #12: Loss:2.6335, Accuracy:0.1150, Validation Loss:2.6244, Validation Accuracy:0.1396\n",
    "Epoch #13: Loss:2.6240, Accuracy:0.1211, Validation Loss:2.6217, Validation Accuracy:0.1264\n",
    "Epoch #14: Loss:2.6236, Accuracy:0.1088, Validation Loss:2.6042, Validation Accuracy:0.1445\n",
    "Epoch #15: Loss:2.6038, Accuracy:0.1273, Validation Loss:2.5944, Validation Accuracy:0.1445\n",
    "Epoch #16: Loss:2.5928, Accuracy:0.1199, Validation Loss:2.5761, Validation Accuracy:0.1396\n",
    "Epoch #17: Loss:2.5750, Accuracy:0.1158, Validation Loss:2.5615, Validation Accuracy:0.1642\n",
    "Epoch #18: Loss:2.5594, Accuracy:0.1565, Validation Loss:2.5447, Validation Accuracy:0.1675\n",
    "Epoch #19: Loss:2.5445, Accuracy:0.1606, Validation Loss:2.5302, Validation Accuracy:0.1675\n",
    "Epoch #20: Loss:2.5288, Accuracy:0.1622, Validation Loss:2.5189, Validation Accuracy:0.1642\n",
    "Epoch #21: Loss:2.5161, Accuracy:0.1659, Validation Loss:2.5097, Validation Accuracy:0.1494\n",
    "Epoch #22: Loss:2.5045, Accuracy:0.1602, Validation Loss:2.5016, Validation Accuracy:0.1511\n",
    "Epoch #23: Loss:2.4974, Accuracy:0.1618, Validation Loss:2.4965, Validation Accuracy:0.1527\n",
    "Epoch #24: Loss:2.4892, Accuracy:0.1659, Validation Loss:2.4941, Validation Accuracy:0.1544\n",
    "Epoch #25: Loss:2.4822, Accuracy:0.1684, Validation Loss:2.4870, Validation Accuracy:0.1593\n",
    "Epoch #26: Loss:2.4787, Accuracy:0.1713, Validation Loss:2.4820, Validation Accuracy:0.1593\n",
    "Epoch #27: Loss:2.4731, Accuracy:0.1684, Validation Loss:2.4791, Validation Accuracy:0.1642\n",
    "Epoch #28: Loss:2.4697, Accuracy:0.1667, Validation Loss:2.4731, Validation Accuracy:0.1708\n",
    "Epoch #29: Loss:2.4677, Accuracy:0.1737, Validation Loss:2.5528, Validation Accuracy:0.1626\n",
    "Epoch #30: Loss:2.5119, Accuracy:0.1565, Validation Loss:2.5235, Validation Accuracy:0.1429\n",
    "Epoch #31: Loss:2.4990, Accuracy:0.1577, Validation Loss:2.4971, Validation Accuracy:0.1576\n",
    "Epoch #32: Loss:2.4922, Accuracy:0.1540, Validation Loss:2.4878, Validation Accuracy:0.1757\n",
    "Epoch #33: Loss:2.4707, Accuracy:0.1676, Validation Loss:2.4974, Validation Accuracy:0.1609\n",
    "Epoch #34: Loss:2.4682, Accuracy:0.1688, Validation Loss:2.4836, Validation Accuracy:0.1708\n",
    "Epoch #35: Loss:2.4649, Accuracy:0.1786, Validation Loss:2.4820, Validation Accuracy:0.1675\n",
    "Epoch #36: Loss:2.4599, Accuracy:0.1684, Validation Loss:2.4822, Validation Accuracy:0.1642\n",
    "Epoch #37: Loss:2.4618, Accuracy:0.1704, Validation Loss:2.4841, Validation Accuracy:0.1642\n",
    "Epoch #38: Loss:2.4614, Accuracy:0.1733, Validation Loss:2.4775, Validation Accuracy:0.1658\n",
    "Epoch #39: Loss:2.4587, Accuracy:0.1770, Validation Loss:2.4766, Validation Accuracy:0.1675\n",
    "Epoch #40: Loss:2.4563, Accuracy:0.1774, Validation Loss:2.4756, Validation Accuracy:0.1642\n",
    "Epoch #41: Loss:2.5056, Accuracy:0.1696, Validation Loss:2.5040, Validation Accuracy:0.1478\n",
    "Epoch #42: Loss:2.4693, Accuracy:0.1778, Validation Loss:2.4733, Validation Accuracy:0.1675\n",
    "Epoch #43: Loss:2.4607, Accuracy:0.1749, Validation Loss:2.4766, Validation Accuracy:0.1724\n",
    "Epoch #44: Loss:2.4597, Accuracy:0.1741, Validation Loss:2.4724, Validation Accuracy:0.1675\n",
    "Epoch #45: Loss:2.4571, Accuracy:0.1782, Validation Loss:2.4785, Validation Accuracy:0.1675\n",
    "Epoch #46: Loss:2.4538, Accuracy:0.1791, Validation Loss:2.4748, Validation Accuracy:0.1658\n",
    "Epoch #47: Loss:2.4537, Accuracy:0.1766, Validation Loss:2.4756, Validation Accuracy:0.1642\n",
    "Epoch #48: Loss:2.4535, Accuracy:0.1791, Validation Loss:2.4776, Validation Accuracy:0.1675\n",
    "Epoch #49: Loss:2.4526, Accuracy:0.1786, Validation Loss:2.4778, Validation Accuracy:0.1675\n",
    "Epoch #50: Loss:2.4520, Accuracy:0.1786, Validation Loss:2.4787, Validation Accuracy:0.1675\n",
    "Epoch #51: Loss:2.4523, Accuracy:0.1786, Validation Loss:2.4792, Validation Accuracy:0.1658\n",
    "Epoch #52: Loss:2.4522, Accuracy:0.1791, Validation Loss:2.4814, Validation Accuracy:0.1642\n",
    "Epoch #53: Loss:2.4517, Accuracy:0.1791, Validation Loss:2.4784, Validation Accuracy:0.1658\n",
    "Epoch #54: Loss:2.4514, Accuracy:0.1799, Validation Loss:2.4793, Validation Accuracy:0.1642\n",
    "Epoch #55: Loss:2.4512, Accuracy:0.1795, Validation Loss:2.4791, Validation Accuracy:0.1691\n",
    "Epoch #56: Loss:2.4506, Accuracy:0.1795, Validation Loss:2.4777, Validation Accuracy:0.1658\n",
    "Epoch #57: Loss:2.4504, Accuracy:0.1791, Validation Loss:2.4770, Validation Accuracy:0.1642\n",
    "Epoch #58: Loss:2.4500, Accuracy:0.1782, Validation Loss:2.4763, Validation Accuracy:0.1626\n",
    "Epoch #59: Loss:2.4492, Accuracy:0.1782, Validation Loss:2.4747, Validation Accuracy:0.1642\n",
    "Epoch #60: Loss:2.4489, Accuracy:0.1791, Validation Loss:2.4755, Validation Accuracy:0.1642\n",
    "Epoch #61: Loss:2.4479, Accuracy:0.1799, Validation Loss:2.4723, Validation Accuracy:0.1642\n",
    "Epoch #62: Loss:2.4455, Accuracy:0.1799, Validation Loss:2.4706, Validation Accuracy:0.1642\n",
    "Epoch #63: Loss:2.4456, Accuracy:0.1803, Validation Loss:2.4690, Validation Accuracy:0.1642\n",
    "Epoch #64: Loss:2.4454, Accuracy:0.1799, Validation Loss:2.4684, Validation Accuracy:0.1642\n",
    "Epoch #65: Loss:2.4452, Accuracy:0.1803, Validation Loss:2.4694, Validation Accuracy:0.1626\n",
    "Epoch #66: Loss:2.4450, Accuracy:0.1799, Validation Loss:2.4693, Validation Accuracy:0.1642\n",
    "Epoch #67: Loss:2.4443, Accuracy:0.1811, Validation Loss:2.4689, Validation Accuracy:0.1658\n",
    "Epoch #68: Loss:2.4447, Accuracy:0.1786, Validation Loss:2.4756, Validation Accuracy:0.1609\n",
    "Epoch #69: Loss:2.4447, Accuracy:0.1791, Validation Loss:2.4741, Validation Accuracy:0.1593\n",
    "Epoch #70: Loss:2.4443, Accuracy:0.1778, Validation Loss:2.4756, Validation Accuracy:0.1626\n",
    "Epoch #71: Loss:2.4437, Accuracy:0.1799, Validation Loss:2.4777, Validation Accuracy:0.1658\n",
    "Epoch #72: Loss:2.4431, Accuracy:0.1803, Validation Loss:2.4762, Validation Accuracy:0.1675\n",
    "Epoch #73: Loss:2.4419, Accuracy:0.1807, Validation Loss:2.4746, Validation Accuracy:0.1658\n",
    "Epoch #74: Loss:2.4419, Accuracy:0.1807, Validation Loss:2.4769, Validation Accuracy:0.1675\n",
    "Epoch #75: Loss:2.4416, Accuracy:0.1778, Validation Loss:2.4775, Validation Accuracy:0.1675\n",
    "Epoch #76: Loss:2.4411, Accuracy:0.1819, Validation Loss:2.4759, Validation Accuracy:0.1675\n",
    "Epoch #77: Loss:2.4401, Accuracy:0.1815, Validation Loss:2.4769, Validation Accuracy:0.1691\n",
    "Epoch #78: Loss:2.4398, Accuracy:0.1803, Validation Loss:2.4753, Validation Accuracy:0.1658\n",
    "Epoch #79: Loss:2.4393, Accuracy:0.1819, Validation Loss:2.4764, Validation Accuracy:0.1691\n",
    "Epoch #80: Loss:2.4391, Accuracy:0.1828, Validation Loss:2.4775, Validation Accuracy:0.1675\n",
    "Epoch #81: Loss:2.4398, Accuracy:0.1836, Validation Loss:2.4751, Validation Accuracy:0.1626\n",
    "Epoch #82: Loss:2.4395, Accuracy:0.1799, Validation Loss:2.4748, Validation Accuracy:0.1691\n",
    "Epoch #83: Loss:2.4396, Accuracy:0.1815, Validation Loss:2.4773, Validation Accuracy:0.1642\n",
    "Epoch #84: Loss:2.4391, Accuracy:0.1823, Validation Loss:2.4765, Validation Accuracy:0.1642\n",
    "Epoch #85: Loss:2.4388, Accuracy:0.1832, Validation Loss:2.4728, Validation Accuracy:0.1691\n",
    "Epoch #86: Loss:2.4384, Accuracy:0.1832, Validation Loss:2.4730, Validation Accuracy:0.1691\n",
    "Epoch #87: Loss:2.4384, Accuracy:0.1819, Validation Loss:2.4700, Validation Accuracy:0.1675\n",
    "Epoch #88: Loss:2.4365, Accuracy:0.1811, Validation Loss:2.4700, Validation Accuracy:0.1708\n",
    "Epoch #89: Loss:2.4365, Accuracy:0.1832, Validation Loss:2.4704, Validation Accuracy:0.1658\n",
    "Epoch #90: Loss:2.4364, Accuracy:0.1819, Validation Loss:2.4692, Validation Accuracy:0.1691\n",
    "Epoch #91: Loss:2.4367, Accuracy:0.1828, Validation Loss:2.4688, Validation Accuracy:0.1691\n",
    "Epoch #92: Loss:2.4359, Accuracy:0.1836, Validation Loss:2.4733, Validation Accuracy:0.1658\n",
    "Epoch #93: Loss:2.4356, Accuracy:0.1832, Validation Loss:2.4690, Validation Accuracy:0.1658\n",
    "Epoch #94: Loss:2.4364, Accuracy:0.1836, Validation Loss:2.4683, Validation Accuracy:0.1691\n",
    "Epoch #95: Loss:2.4354, Accuracy:0.1832, Validation Loss:2.4708, Validation Accuracy:0.1691\n",
    "Epoch #96: Loss:2.4352, Accuracy:0.1828, Validation Loss:2.4712, Validation Accuracy:0.1675\n",
    "Epoch #97: Loss:2.4355, Accuracy:0.1819, Validation Loss:2.4709, Validation Accuracy:0.1658\n",
    "Epoch #98: Loss:2.4357, Accuracy:0.1819, Validation Loss:2.4711, Validation Accuracy:0.1658\n",
    "Epoch #99: Loss:2.4352, Accuracy:0.1823, Validation Loss:2.4711, Validation Accuracy:0.1658\n",
    "Epoch #100: Loss:2.4346, Accuracy:0.1836, Validation Loss:2.4727, Validation Accuracy:0.1691\n",
    "Epoch #101: Loss:2.4363, Accuracy:0.1823, Validation Loss:2.4754, Validation Accuracy:0.1658\n",
    "Epoch #102: Loss:2.4393, Accuracy:0.1815, Validation Loss:2.4775, Validation Accuracy:0.1658\n",
    "Epoch #103: Loss:2.4386, Accuracy:0.1848, Validation Loss:2.4759, Validation Accuracy:0.1642\n",
    "Epoch #104: Loss:2.4392, Accuracy:0.1807, Validation Loss:2.4757, Validation Accuracy:0.1675\n",
    "Epoch #105: Loss:2.4391, Accuracy:0.1836, Validation Loss:2.4766, Validation Accuracy:0.1658\n",
    "Epoch #106: Loss:2.4396, Accuracy:0.1840, Validation Loss:2.4759, Validation Accuracy:0.1642\n",
    "Epoch #107: Loss:2.4392, Accuracy:0.1807, Validation Loss:2.4746, Validation Accuracy:0.1593\n",
    "Epoch #108: Loss:2.4381, Accuracy:0.1807, Validation Loss:2.4739, Validation Accuracy:0.1593\n",
    "Epoch #109: Loss:2.4367, Accuracy:0.1828, Validation Loss:2.4709, Validation Accuracy:0.1593\n",
    "Epoch #110: Loss:2.4362, Accuracy:0.1791, Validation Loss:2.4720, Validation Accuracy:0.1593\n",
    "Epoch #111: Loss:2.4354, Accuracy:0.1778, Validation Loss:2.4713, Validation Accuracy:0.1675\n",
    "Epoch #112: Loss:2.4355, Accuracy:0.1844, Validation Loss:2.4723, Validation Accuracy:0.1675\n",
    "Epoch #113: Loss:2.4351, Accuracy:0.1828, Validation Loss:2.4710, Validation Accuracy:0.1675\n",
    "Epoch #114: Loss:2.4346, Accuracy:0.1836, Validation Loss:2.4720, Validation Accuracy:0.1642\n",
    "Epoch #115: Loss:2.4356, Accuracy:0.1836, Validation Loss:2.4711, Validation Accuracy:0.1642\n",
    "Epoch #116: Loss:2.4359, Accuracy:0.1799, Validation Loss:2.4662, Validation Accuracy:0.1691\n",
    "Epoch #117: Loss:2.4352, Accuracy:0.1836, Validation Loss:2.4681, Validation Accuracy:0.1626\n",
    "Epoch #118: Loss:2.4336, Accuracy:0.1836, Validation Loss:2.4647, Validation Accuracy:0.1642\n",
    "Epoch #119: Loss:2.4308, Accuracy:0.1836, Validation Loss:2.4660, Validation Accuracy:0.1642\n",
    "Epoch #120: Loss:2.4305, Accuracy:0.1852, Validation Loss:2.4658, Validation Accuracy:0.1642\n",
    "Epoch #121: Loss:2.4308, Accuracy:0.1860, Validation Loss:2.4650, Validation Accuracy:0.1642\n",
    "Epoch #122: Loss:2.4299, Accuracy:0.1844, Validation Loss:2.4665, Validation Accuracy:0.1675\n",
    "Epoch #123: Loss:2.4314, Accuracy:0.1828, Validation Loss:2.4664, Validation Accuracy:0.1658\n",
    "Epoch #124: Loss:2.4332, Accuracy:0.1819, Validation Loss:2.4670, Validation Accuracy:0.1642\n",
    "Epoch #125: Loss:2.4346, Accuracy:0.1811, Validation Loss:2.4676, Validation Accuracy:0.1642\n",
    "Epoch #126: Loss:2.4337, Accuracy:0.1819, Validation Loss:2.4706, Validation Accuracy:0.1609\n",
    "Epoch #127: Loss:2.4322, Accuracy:0.1828, Validation Loss:2.4646, Validation Accuracy:0.1642\n",
    "Epoch #128: Loss:2.4313, Accuracy:0.1823, Validation Loss:2.4660, Validation Accuracy:0.1609\n",
    "Epoch #129: Loss:2.4309, Accuracy:0.1832, Validation Loss:2.4668, Validation Accuracy:0.1609\n",
    "Epoch #130: Loss:2.4312, Accuracy:0.1807, Validation Loss:2.4665, Validation Accuracy:0.1593\n",
    "Epoch #131: Loss:2.4297, Accuracy:0.1832, Validation Loss:2.4663, Validation Accuracy:0.1642\n",
    "Epoch #132: Loss:2.4283, Accuracy:0.1840, Validation Loss:2.4641, Validation Accuracy:0.1708\n",
    "Epoch #133: Loss:2.4282, Accuracy:0.1852, Validation Loss:2.4648, Validation Accuracy:0.1658\n",
    "Epoch #134: Loss:2.4277, Accuracy:0.1848, Validation Loss:2.4686, Validation Accuracy:0.1658\n",
    "Epoch #135: Loss:2.4283, Accuracy:0.1832, Validation Loss:2.4677, Validation Accuracy:0.1741\n",
    "Epoch #136: Loss:2.4287, Accuracy:0.1856, Validation Loss:2.4682, Validation Accuracy:0.1675\n",
    "Epoch #137: Loss:2.4305, Accuracy:0.1836, Validation Loss:2.4689, Validation Accuracy:0.1773\n",
    "Epoch #138: Loss:2.4291, Accuracy:0.1864, Validation Loss:2.4680, Validation Accuracy:0.1724\n",
    "Epoch #139: Loss:2.4269, Accuracy:0.1852, Validation Loss:2.4696, Validation Accuracy:0.1724\n",
    "Epoch #140: Loss:2.4289, Accuracy:0.1881, Validation Loss:2.4658, Validation Accuracy:0.1675\n",
    "Epoch #141: Loss:2.4282, Accuracy:0.1860, Validation Loss:2.4626, Validation Accuracy:0.1658\n",
    "Epoch #142: Loss:2.4274, Accuracy:0.1840, Validation Loss:2.4637, Validation Accuracy:0.1658\n",
    "Epoch #143: Loss:2.4295, Accuracy:0.1828, Validation Loss:2.4614, Validation Accuracy:0.1675\n",
    "Epoch #144: Loss:2.4299, Accuracy:0.1832, Validation Loss:2.4624, Validation Accuracy:0.1675\n",
    "Epoch #145: Loss:2.4298, Accuracy:0.1836, Validation Loss:2.4620, Validation Accuracy:0.1626\n",
    "Epoch #146: Loss:2.4280, Accuracy:0.1860, Validation Loss:2.4617, Validation Accuracy:0.1642\n",
    "Epoch #147: Loss:2.4275, Accuracy:0.1848, Validation Loss:2.4615, Validation Accuracy:0.1642\n",
    "Epoch #148: Loss:2.4274, Accuracy:0.1873, Validation Loss:2.4638, Validation Accuracy:0.1675\n",
    "Epoch #149: Loss:2.4292, Accuracy:0.1885, Validation Loss:2.4634, Validation Accuracy:0.1708\n",
    "Epoch #150: Loss:2.4305, Accuracy:0.1864, Validation Loss:2.4629, Validation Accuracy:0.1675\n",
    "Epoch #151: Loss:2.4318, Accuracy:0.1840, Validation Loss:2.4627, Validation Accuracy:0.1708\n",
    "Epoch #152: Loss:2.4285, Accuracy:0.1811, Validation Loss:2.4615, Validation Accuracy:0.1708\n",
    "Epoch #153: Loss:2.4272, Accuracy:0.1873, Validation Loss:2.4602, Validation Accuracy:0.1691\n",
    "Epoch #154: Loss:2.4276, Accuracy:0.1881, Validation Loss:2.4556, Validation Accuracy:0.1691\n",
    "Epoch #155: Loss:2.4271, Accuracy:0.1873, Validation Loss:2.4556, Validation Accuracy:0.1691\n",
    "Epoch #156: Loss:2.4254, Accuracy:0.1885, Validation Loss:2.4602, Validation Accuracy:0.1708\n",
    "Epoch #157: Loss:2.4265, Accuracy:0.1864, Validation Loss:2.4577, Validation Accuracy:0.1708\n",
    "Epoch #158: Loss:2.4256, Accuracy:0.1864, Validation Loss:2.4615, Validation Accuracy:0.1691\n",
    "Epoch #159: Loss:2.4256, Accuracy:0.1832, Validation Loss:2.4578, Validation Accuracy:0.1658\n",
    "Epoch #160: Loss:2.4239, Accuracy:0.1860, Validation Loss:2.4688, Validation Accuracy:0.1658\n",
    "Epoch #161: Loss:2.4357, Accuracy:0.1840, Validation Loss:2.4721, Validation Accuracy:0.1658\n",
    "Epoch #162: Loss:2.4293, Accuracy:0.1836, Validation Loss:2.4646, Validation Accuracy:0.1658\n",
    "Epoch #163: Loss:2.4278, Accuracy:0.1873, Validation Loss:2.4633, Validation Accuracy:0.1642\n",
    "Epoch #164: Loss:2.4292, Accuracy:0.1844, Validation Loss:2.4619, Validation Accuracy:0.1675\n",
    "Epoch #165: Loss:2.4242, Accuracy:0.1832, Validation Loss:2.4608, Validation Accuracy:0.1675\n",
    "Epoch #166: Loss:2.4258, Accuracy:0.1869, Validation Loss:2.4627, Validation Accuracy:0.1675\n",
    "Epoch #167: Loss:2.4249, Accuracy:0.1844, Validation Loss:2.4608, Validation Accuracy:0.1675\n",
    "Epoch #168: Loss:2.4250, Accuracy:0.1869, Validation Loss:2.4585, Validation Accuracy:0.1691\n",
    "Epoch #169: Loss:2.4249, Accuracy:0.1852, Validation Loss:2.4606, Validation Accuracy:0.1658\n",
    "Epoch #170: Loss:2.4256, Accuracy:0.1860, Validation Loss:2.4598, Validation Accuracy:0.1675\n",
    "Epoch #171: Loss:2.4256, Accuracy:0.1856, Validation Loss:2.4638, Validation Accuracy:0.1658\n",
    "Epoch #172: Loss:2.4257, Accuracy:0.1819, Validation Loss:2.4631, Validation Accuracy:0.1691\n",
    "Epoch #173: Loss:2.4256, Accuracy:0.1856, Validation Loss:2.4637, Validation Accuracy:0.1675\n",
    "Epoch #174: Loss:2.4260, Accuracy:0.1828, Validation Loss:2.4643, Validation Accuracy:0.1658\n",
    "Epoch #175: Loss:2.4273, Accuracy:0.1852, Validation Loss:2.4617, Validation Accuracy:0.1691\n",
    "Epoch #176: Loss:2.4247, Accuracy:0.1869, Validation Loss:2.4661, Validation Accuracy:0.1642\n",
    "Epoch #177: Loss:2.4251, Accuracy:0.1877, Validation Loss:2.4619, Validation Accuracy:0.1675\n",
    "Epoch #178: Loss:2.4244, Accuracy:0.1869, Validation Loss:2.4633, Validation Accuracy:0.1658\n",
    "Epoch #179: Loss:2.4243, Accuracy:0.1869, Validation Loss:2.4635, Validation Accuracy:0.1658\n",
    "Epoch #180: Loss:2.4238, Accuracy:0.1873, Validation Loss:2.4662, Validation Accuracy:0.1675\n",
    "Epoch #181: Loss:2.4244, Accuracy:0.1864, Validation Loss:2.4654, Validation Accuracy:0.1658\n",
    "Epoch #182: Loss:2.4239, Accuracy:0.1844, Validation Loss:2.4661, Validation Accuracy:0.1675\n",
    "Epoch #183: Loss:2.4228, Accuracy:0.1885, Validation Loss:2.4635, Validation Accuracy:0.1691\n",
    "Epoch #184: Loss:2.4219, Accuracy:0.1901, Validation Loss:2.4650, Validation Accuracy:0.1675\n",
    "Epoch #185: Loss:2.4226, Accuracy:0.1893, Validation Loss:2.4630, Validation Accuracy:0.1675\n",
    "Epoch #186: Loss:2.4224, Accuracy:0.1869, Validation Loss:2.4614, Validation Accuracy:0.1675\n",
    "Epoch #187: Loss:2.4223, Accuracy:0.1881, Validation Loss:2.4613, Validation Accuracy:0.1691\n",
    "Epoch #188: Loss:2.4215, Accuracy:0.1889, Validation Loss:2.4606, Validation Accuracy:0.1691\n",
    "Epoch #189: Loss:2.4228, Accuracy:0.1815, Validation Loss:2.4609, Validation Accuracy:0.1691\n",
    "Epoch #190: Loss:2.4257, Accuracy:0.1803, Validation Loss:2.4594, Validation Accuracy:0.1691\n",
    "Epoch #191: Loss:2.4245, Accuracy:0.1848, Validation Loss:2.4633, Validation Accuracy:0.1708\n",
    "Epoch #192: Loss:2.4210, Accuracy:0.1860, Validation Loss:2.4614, Validation Accuracy:0.1691\n",
    "Epoch #193: Loss:2.4224, Accuracy:0.1852, Validation Loss:2.4612, Validation Accuracy:0.1691\n",
    "Epoch #194: Loss:2.4214, Accuracy:0.1885, Validation Loss:2.4618, Validation Accuracy:0.1691\n",
    "Epoch #195: Loss:2.4213, Accuracy:0.1889, Validation Loss:2.4615, Validation Accuracy:0.1708\n",
    "Epoch #196: Loss:2.4204, Accuracy:0.1889, Validation Loss:2.4650, Validation Accuracy:0.1691\n",
    "Epoch #197: Loss:2.4204, Accuracy:0.1873, Validation Loss:2.4668, Validation Accuracy:0.1642\n",
    "Epoch #198: Loss:2.4208, Accuracy:0.1852, Validation Loss:2.4647, Validation Accuracy:0.1675\n",
    "Epoch #199: Loss:2.4211, Accuracy:0.1873, Validation Loss:2.4667, Validation Accuracy:0.1658\n",
    "Epoch #200: Loss:2.4205, Accuracy:0.1869, Validation Loss:2.4657, Validation Accuracy:0.1691\n",
    "Epoch #201: Loss:2.4216, Accuracy:0.1852, Validation Loss:2.4674, Validation Accuracy:0.1675\n",
    "Epoch #202: Loss:2.4216, Accuracy:0.1864, Validation Loss:2.4657, Validation Accuracy:0.1724\n",
    "Epoch #203: Loss:2.4214, Accuracy:0.1848, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #204: Loss:2.4191, Accuracy:0.1836, Validation Loss:2.4644, Validation Accuracy:0.1691\n",
    "Epoch #205: Loss:2.4194, Accuracy:0.1836, Validation Loss:2.4650, Validation Accuracy:0.1691\n",
    "Epoch #206: Loss:2.4196, Accuracy:0.1823, Validation Loss:2.4664, Validation Accuracy:0.1691\n",
    "Epoch #207: Loss:2.4200, Accuracy:0.1856, Validation Loss:2.4657, Validation Accuracy:0.1691\n",
    "Epoch #208: Loss:2.4204, Accuracy:0.1869, Validation Loss:2.4667, Validation Accuracy:0.1675\n",
    "Epoch #209: Loss:2.4210, Accuracy:0.1860, Validation Loss:2.4651, Validation Accuracy:0.1658\n",
    "Epoch #210: Loss:2.4189, Accuracy:0.1885, Validation Loss:2.4660, Validation Accuracy:0.1675\n",
    "Epoch #211: Loss:2.4188, Accuracy:0.1873, Validation Loss:2.4664, Validation Accuracy:0.1658\n",
    "Epoch #212: Loss:2.4185, Accuracy:0.1864, Validation Loss:2.4674, Validation Accuracy:0.1691\n",
    "Epoch #213: Loss:2.4178, Accuracy:0.1860, Validation Loss:2.4673, Validation Accuracy:0.1691\n",
    "Epoch #214: Loss:2.4176, Accuracy:0.1873, Validation Loss:2.4659, Validation Accuracy:0.1675\n",
    "Epoch #215: Loss:2.4171, Accuracy:0.1877, Validation Loss:2.4657, Validation Accuracy:0.1675\n",
    "Epoch #216: Loss:2.4173, Accuracy:0.1881, Validation Loss:2.4655, Validation Accuracy:0.1708\n",
    "Epoch #217: Loss:2.4170, Accuracy:0.1897, Validation Loss:2.4657, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.4161, Accuracy:0.1869, Validation Loss:2.4647, Validation Accuracy:0.1724\n",
    "Epoch #219: Loss:2.4161, Accuracy:0.1893, Validation Loss:2.4641, Validation Accuracy:0.1708\n",
    "Epoch #220: Loss:2.4162, Accuracy:0.1877, Validation Loss:2.4657, Validation Accuracy:0.1741\n",
    "Epoch #221: Loss:2.4160, Accuracy:0.1901, Validation Loss:2.4651, Validation Accuracy:0.1691\n",
    "Epoch #222: Loss:2.4157, Accuracy:0.1906, Validation Loss:2.4655, Validation Accuracy:0.1675\n",
    "Epoch #223: Loss:2.4150, Accuracy:0.1893, Validation Loss:2.4658, Validation Accuracy:0.1675\n",
    "Epoch #224: Loss:2.4153, Accuracy:0.1877, Validation Loss:2.4660, Validation Accuracy:0.1691\n",
    "Epoch #225: Loss:2.4161, Accuracy:0.1881, Validation Loss:2.4678, Validation Accuracy:0.1658\n",
    "Epoch #226: Loss:2.4152, Accuracy:0.1873, Validation Loss:2.4679, Validation Accuracy:0.1675\n",
    "Epoch #227: Loss:2.4180, Accuracy:0.1897, Validation Loss:2.4665, Validation Accuracy:0.1658\n",
    "Epoch #228: Loss:2.4192, Accuracy:0.1885, Validation Loss:2.4658, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4196, Accuracy:0.1860, Validation Loss:2.4666, Validation Accuracy:0.1691\n",
    "Epoch #230: Loss:2.4148, Accuracy:0.1914, Validation Loss:2.4641, Validation Accuracy:0.1708\n",
    "Epoch #231: Loss:2.4163, Accuracy:0.1901, Validation Loss:2.4661, Validation Accuracy:0.1757\n",
    "Epoch #232: Loss:2.4154, Accuracy:0.1914, Validation Loss:2.4703, Validation Accuracy:0.1675\n",
    "Epoch #233: Loss:2.4166, Accuracy:0.1840, Validation Loss:2.4655, Validation Accuracy:0.1724\n",
    "Epoch #234: Loss:2.4165, Accuracy:0.1881, Validation Loss:2.4664, Validation Accuracy:0.1724\n",
    "Epoch #235: Loss:2.4155, Accuracy:0.1873, Validation Loss:2.4662, Validation Accuracy:0.1741\n",
    "Epoch #236: Loss:2.4163, Accuracy:0.1877, Validation Loss:2.4665, Validation Accuracy:0.1675\n",
    "Epoch #237: Loss:2.4168, Accuracy:0.1906, Validation Loss:2.4729, Validation Accuracy:0.1675\n",
    "Epoch #238: Loss:2.4149, Accuracy:0.1934, Validation Loss:2.4732, Validation Accuracy:0.1626\n",
    "Epoch #239: Loss:2.4160, Accuracy:0.1873, Validation Loss:2.4720, Validation Accuracy:0.1658\n",
    "Epoch #240: Loss:2.4172, Accuracy:0.1848, Validation Loss:2.4719, Validation Accuracy:0.1675\n",
    "Epoch #241: Loss:2.4144, Accuracy:0.1873, Validation Loss:2.4707, Validation Accuracy:0.1675\n",
    "Epoch #242: Loss:2.4151, Accuracy:0.1893, Validation Loss:2.4705, Validation Accuracy:0.1691\n",
    "Epoch #243: Loss:2.4149, Accuracy:0.1848, Validation Loss:2.4707, Validation Accuracy:0.1675\n",
    "Epoch #244: Loss:2.4149, Accuracy:0.1881, Validation Loss:2.4716, Validation Accuracy:0.1691\n",
    "Epoch #245: Loss:2.4152, Accuracy:0.1889, Validation Loss:2.4730, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4151, Accuracy:0.1864, Validation Loss:2.4725, Validation Accuracy:0.1675\n",
    "Epoch #247: Loss:2.4123, Accuracy:0.1897, Validation Loss:2.4735, Validation Accuracy:0.1675\n",
    "Epoch #248: Loss:2.4133, Accuracy:0.1910, Validation Loss:2.4715, Validation Accuracy:0.1675\n",
    "Epoch #249: Loss:2.4142, Accuracy:0.1881, Validation Loss:2.4706, Validation Accuracy:0.1675\n",
    "Epoch #250: Loss:2.4136, Accuracy:0.1893, Validation Loss:2.4727, Validation Accuracy:0.1658\n",
    "Epoch #251: Loss:2.4130, Accuracy:0.1922, Validation Loss:2.4742, Validation Accuracy:0.1675\n",
    "Epoch #252: Loss:2.4127, Accuracy:0.1881, Validation Loss:2.4726, Validation Accuracy:0.1675\n",
    "Epoch #253: Loss:2.4150, Accuracy:0.1889, Validation Loss:2.4719, Validation Accuracy:0.1675\n",
    "Epoch #254: Loss:2.4128, Accuracy:0.1889, Validation Loss:2.4723, Validation Accuracy:0.1658\n",
    "Epoch #255: Loss:2.4136, Accuracy:0.1881, Validation Loss:2.4680, Validation Accuracy:0.1708\n",
    "Epoch #256: Loss:2.4162, Accuracy:0.1881, Validation Loss:2.4663, Validation Accuracy:0.1708\n",
    "Epoch #257: Loss:2.4147, Accuracy:0.1869, Validation Loss:2.4659, Validation Accuracy:0.1675\n",
    "Epoch #258: Loss:2.4148, Accuracy:0.1885, Validation Loss:2.4671, Validation Accuracy:0.1658\n",
    "Epoch #259: Loss:2.4129, Accuracy:0.1910, Validation Loss:2.4683, Validation Accuracy:0.1658\n",
    "Epoch #260: Loss:2.4133, Accuracy:0.1885, Validation Loss:2.4697, Validation Accuracy:0.1658\n",
    "Epoch #261: Loss:2.4145, Accuracy:0.1869, Validation Loss:2.4707, Validation Accuracy:0.1658\n",
    "Epoch #262: Loss:2.4133, Accuracy:0.1893, Validation Loss:2.4680, Validation Accuracy:0.1658\n",
    "Epoch #263: Loss:2.4150, Accuracy:0.1877, Validation Loss:2.4697, Validation Accuracy:0.1560\n",
    "Epoch #264: Loss:2.4141, Accuracy:0.1889, Validation Loss:2.4656, Validation Accuracy:0.1593\n",
    "Epoch #265: Loss:2.4141, Accuracy:0.1848, Validation Loss:2.4679, Validation Accuracy:0.1560\n",
    "Epoch #266: Loss:2.4147, Accuracy:0.1848, Validation Loss:2.4668, Validation Accuracy:0.1626\n",
    "Epoch #267: Loss:2.4154, Accuracy:0.1910, Validation Loss:2.4645, Validation Accuracy:0.1609\n",
    "Epoch #268: Loss:2.4138, Accuracy:0.1864, Validation Loss:2.4663, Validation Accuracy:0.1593\n",
    "Epoch #269: Loss:2.4146, Accuracy:0.1873, Validation Loss:2.4654, Validation Accuracy:0.1626\n",
    "Epoch #270: Loss:2.4151, Accuracy:0.1893, Validation Loss:2.4620, Validation Accuracy:0.1626\n",
    "Epoch #271: Loss:2.4143, Accuracy:0.1897, Validation Loss:2.4630, Validation Accuracy:0.1626\n",
    "Epoch #272: Loss:2.4149, Accuracy:0.1901, Validation Loss:2.4626, Validation Accuracy:0.1576\n",
    "Epoch #273: Loss:2.4150, Accuracy:0.1856, Validation Loss:2.4623, Validation Accuracy:0.1609\n",
    "Epoch #274: Loss:2.4136, Accuracy:0.1897, Validation Loss:2.4635, Validation Accuracy:0.1691\n",
    "Epoch #275: Loss:2.4140, Accuracy:0.1910, Validation Loss:2.4661, Validation Accuracy:0.1626\n",
    "Epoch #276: Loss:2.4146, Accuracy:0.1910, Validation Loss:2.4675, Validation Accuracy:0.1675\n",
    "Epoch #277: Loss:2.4132, Accuracy:0.1897, Validation Loss:2.4638, Validation Accuracy:0.1626\n",
    "Epoch #278: Loss:2.4128, Accuracy:0.1889, Validation Loss:2.4637, Validation Accuracy:0.1691\n",
    "Epoch #279: Loss:2.4128, Accuracy:0.1844, Validation Loss:2.4649, Validation Accuracy:0.1691\n",
    "Epoch #280: Loss:2.4114, Accuracy:0.1864, Validation Loss:2.4657, Validation Accuracy:0.1675\n",
    "Epoch #281: Loss:2.4099, Accuracy:0.1897, Validation Loss:2.4669, Validation Accuracy:0.1658\n",
    "Epoch #282: Loss:2.4091, Accuracy:0.1893, Validation Loss:2.4673, Validation Accuracy:0.1609\n",
    "Epoch #283: Loss:2.4112, Accuracy:0.1885, Validation Loss:2.4672, Validation Accuracy:0.1658\n",
    "Epoch #284: Loss:2.4096, Accuracy:0.1901, Validation Loss:2.4701, Validation Accuracy:0.1658\n",
    "Epoch #285: Loss:2.4090, Accuracy:0.1897, Validation Loss:2.4679, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.4114, Accuracy:0.1910, Validation Loss:2.4653, Validation Accuracy:0.1642\n",
    "Epoch #287: Loss:2.4105, Accuracy:0.1877, Validation Loss:2.4678, Validation Accuracy:0.1675\n",
    "Epoch #288: Loss:2.4120, Accuracy:0.1901, Validation Loss:2.4684, Validation Accuracy:0.1675\n",
    "Epoch #289: Loss:2.4113, Accuracy:0.1877, Validation Loss:2.4704, Validation Accuracy:0.1642\n",
    "Epoch #290: Loss:2.4092, Accuracy:0.1910, Validation Loss:2.4691, Validation Accuracy:0.1675\n",
    "Epoch #291: Loss:2.4096, Accuracy:0.1914, Validation Loss:2.4731, Validation Accuracy:0.1675\n",
    "Epoch #292: Loss:2.4080, Accuracy:0.1906, Validation Loss:2.4720, Validation Accuracy:0.1675\n",
    "Epoch #293: Loss:2.4085, Accuracy:0.1906, Validation Loss:2.4733, Validation Accuracy:0.1675\n",
    "Epoch #294: Loss:2.4088, Accuracy:0.1906, Validation Loss:2.4709, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4095, Accuracy:0.1901, Validation Loss:2.4719, Validation Accuracy:0.1691\n",
    "Epoch #296: Loss:2.4109, Accuracy:0.1901, Validation Loss:2.4749, Validation Accuracy:0.1675\n",
    "Epoch #297: Loss:2.4074, Accuracy:0.1926, Validation Loss:2.4744, Validation Accuracy:0.1741\n",
    "Epoch #298: Loss:2.4088, Accuracy:0.1893, Validation Loss:2.4726, Validation Accuracy:0.1626\n",
    "Epoch #299: Loss:2.4078, Accuracy:0.1918, Validation Loss:2.4733, Validation Accuracy:0.1675\n",
    "Epoch #300: Loss:2.4093, Accuracy:0.1918, Validation Loss:2.4755, Validation Accuracy:0.1675\n",
    "\n",
    "Test:\n",
    "Test Loss:2.47546935, Accuracy:0.1675\n",
    "Labels: ['by', 'mb', 'eb', 'yd', 'ib', 'sg', 'sk', 'eg', 'ek', 'aa', 'eo', 'ce', 'ck', 'my', 'ds']\n",
    "Confusion Matrix:\n",
    "      by  mb  eb  yd  ib  sg  sk  eg  ek  aa  eo  ce  ck  my  ds\n",
    "t:by   3   0   0   1   2   8   0  15   9   0   0   0   0   0   2\n",
    "t:mb   0   0   0   6   4  19   0  13   8   2   0   0   0   0   0\n",
    "t:eb   3   0   0   4   2   7   0  24   8   0   0   0   0   0   2\n",
    "t:yd   2   0   0  26   3  23   0   2   6   0   0   0   0   0   0\n",
    "t:ib   0   0   0  23   5  16   0   4   5   1   0   0   0   0   0\n",
    "t:sg   0   0   0  14   3  24   0   3   7   0   0   0   0   0   0\n",
    "t:sk   2   0   0   0   2   4   0  15   4   2   0   0   0   0   4\n",
    "t:eg   1   0   0   0   0   2   0  30   5   3   1   0   0   0   8\n",
    "t:ek   2   0   0   5   1   8   0  20   8   1   2   0   0   0   1\n",
    "t:aa   1   0   0   2   0   4   0  17   6   0   0   0   0   0   4\n",
    "t:eo   1   0   0   3   2  17   0   4   7   0   0   0   0   0   0\n",
    "t:ce   2   0   0   3   2   7   0   8   3   0   1   0   0   0   1\n",
    "t:ck   4   0   0   0   1   3   0   8   5   0   0   0   0   0   2\n",
    "t:my   0   0   0   2   1   7   0   5   1   1   0   0   0   0   3\n",
    "t:ds   1   0   0   1   0   7   0  11   5   0   0   0   0   0   6\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          by       0.14      0.07      0.10        40\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          yd       0.29      0.42      0.34        62\n",
    "          ib       0.18      0.09      0.12        54\n",
    "          sg       0.15      0.47      0.23        51\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eg       0.17      0.60      0.26        50\n",
    "          ek       0.09      0.17      0.12        48\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ds       0.18      0.19      0.19        31\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.08      0.13      0.09       609\n",
    "weighted avg       0.10      0.17      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 17:34:15 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 41 minutes, 3 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7091292090016634, 2.697486107768292, 2.6884799739605882, 2.681066564346965, 2.674516054209817, 2.668892964157956, 2.6639770138244128, 2.6582580094266994, 2.651981436756053, 2.644467650963168, 2.6353692699144236, 2.6244424759656533, 2.6216513476348275, 2.6041987447315837, 2.5943844357539083, 2.5760778977561665, 2.5614650515695705, 2.5447247658652823, 2.5301682502765375, 2.5188895982670276, 2.5097328273729347, 2.5016411901107563, 2.4964777303642434, 2.494118430540088, 2.486985967664296, 2.4819588469362808, 2.4790563994440538, 2.4731189781809086, 2.5528211758054535, 2.5234634457354868, 2.4971349924460227, 2.4877972082159987, 2.4973725403470945, 2.4835936784352772, 2.482038740258303, 2.4821566133859316, 2.4840589192113267, 2.4774636364922733, 2.4765800713318322, 2.4755604913082028, 2.503973841471429, 2.4733252552734024, 2.476571822988576, 2.472423658778123, 2.478466524474922, 2.4748131465442076, 2.475560666696583, 2.4775907633144083, 2.4777812374440713, 2.4786832590995753, 2.4791556551734413, 2.4814177949244556, 2.478413275701463, 2.479275010097986, 2.4790779917893935, 2.477661927149605, 2.477029410489087, 2.4762955721963213, 2.474667919093165, 2.475508865660243, 2.4722770239136294, 2.4706024665550643, 2.4690358411698115, 2.46840975789601, 2.469377505172454, 2.469337010814247, 2.468912470321154, 2.475623490579414, 2.4740990625422183, 2.4756487707786374, 2.4776638990944044, 2.4761693974825354, 2.4745603461179435, 2.4768899841653105, 2.477503729179771, 2.4758504028195034, 2.4768989571601105, 2.4752720737300677, 2.4764054526249177, 2.477502304932167, 2.4750809054852314, 2.474783328953635, 2.4773273546315964, 2.4764707045406347, 2.4728273922586674, 2.4729842752071436, 2.4699801455186114, 2.470023006445473, 2.47043667207602, 2.4691700422509353, 2.468815110978626, 2.4732981667730023, 2.4689915732209906, 2.4683386187248044, 2.4708136141985313, 2.471235000441227, 2.470880628610871, 2.471064616110916, 2.471106503005881, 2.4727363179274184, 2.4753716955044, 2.4774853568554707, 2.475864311352935, 2.475696218816322, 2.4766497114804773, 2.4759033834209974, 2.4745538332583674, 2.4739401262186234, 2.4709176014992598, 2.472023305438814, 2.4713437040451125, 2.4723274433749847, 2.4710373565285466, 2.4720188240308087, 2.471143589035435, 2.4661592701190016, 2.4680758262502738, 2.4647307020102818, 2.4659898754802634, 2.4657690853908143, 2.4649596120336374, 2.4665238332670114, 2.4663856483641124, 2.467024442989055, 2.467641785422765, 2.4705838475908553, 2.4645718776533756, 2.4660270515529588, 2.466765162980028, 2.4665454322677136, 2.4662634508167387, 2.4641455840594664, 2.464805244420745, 2.4685555420485623, 2.467728758484663, 2.4681634934273453, 2.468859842845372, 2.4679929673769596, 2.4696282847174285, 2.4657953157409267, 2.4625841616018262, 2.4637366777961867, 2.4614392199931276, 2.4624284954102365, 2.4620115940989726, 2.461694505218606, 2.461515467350902, 2.4638050138852474, 2.463401841804116, 2.4629046744705225, 2.4627184441132695, 2.461458273513368, 2.46015548157966, 2.455586606841565, 2.4556491410203756, 2.460193785540576, 2.4576585030516576, 2.461476684595368, 2.457782775897698, 2.46879568514957, 2.4721370240541907, 2.4646195437520597, 2.4633239384355217, 2.461906861789121, 2.4607794437502406, 2.462651983661996, 2.460798432282822, 2.458517445327809, 2.460632704748896, 2.459817022721364, 2.463784318447896, 2.4631456514493193, 2.4637264152270038, 2.464272891750868, 2.461696967311289, 2.466094152093521, 2.4619268897327493, 2.463273229270146, 2.463529344067002, 2.4662223480800884, 2.4654460736291943, 2.466051381601293, 2.463469503744091, 2.4650159606401165, 2.4630060094134953, 2.4613525346777907, 2.4612605920174633, 2.460622465277736, 2.4609119199179665, 2.4594068491987406, 2.463285033534509, 2.4614416002639996, 2.4612461412677233, 2.4617755726248958, 2.461541279196152, 2.465040449242678, 2.466821620029769, 2.464716941069304, 2.466737152124665, 2.4656504539433373, 2.4673590429114984, 2.465706449032613, 2.46614489453571, 2.4643713899433908, 2.464992944438665, 2.4664427239710864, 2.465665281308304, 2.4667482845888937, 2.465127262184381, 2.4659854625833444, 2.466354675480885, 2.46742348404745, 2.4673464775868434, 2.4658610664173497, 2.4657471450091584, 2.465493479385752, 2.4657172205412916, 2.4647235259633935, 2.4641130667406155, 2.4657378419866705, 2.465065905613265, 2.4655492117839493, 2.4657893736765693, 2.4660432229096862, 2.4678020124952194, 2.4679316397762454, 2.4665248859887834, 2.4657968218103417, 2.466550926465315, 2.464099280744155, 2.466055774532124, 2.4702647656251253, 2.465511377221845, 2.466434775119149, 2.4661992661079943, 2.466450025686881, 2.4729426416074505, 2.473185976150588, 2.4720334043643746, 2.4719041877583723, 2.4707300753037527, 2.470543354015632, 2.470668711685782, 2.471599090275506, 2.4730041211070293, 2.472483529637404, 2.473472642193874, 2.471475503323309, 2.4705502074731784, 2.4727347147680074, 2.4741784684568007, 2.47264891657336, 2.4718507945243946, 2.472282618333162, 2.467997408461297, 2.466300067447481, 2.465912133797832, 2.4671360475480655, 2.4683185547639193, 2.4696626130778996, 2.4707226502680033, 2.468017517835244, 2.4697267104839455, 2.46563037941217, 2.46788679985773, 2.466803080538419, 2.4645454045782733, 2.466332185836065, 2.465423496681677, 2.4620474657205915, 2.4630312453741317, 2.462618025261389, 2.462278668320629, 2.4635465239069143, 2.4661118197323653, 2.4674617891828414, 2.4637765692568374, 2.4637314247576083, 2.4649175470098488, 2.4656876041775657, 2.4669139463521774, 2.467297424432288, 2.467226300529267, 2.4701339615193887, 2.4678945220358464, 2.4653368505155315, 2.467824009643204, 2.4683835220650105, 2.470409157632411, 2.469058735225784, 2.473079146618522, 2.4720038976183862, 2.473267893485835, 2.470860734557479, 2.4718886020735567, 2.4749244860631885, 2.474427239648227, 2.4725639835758555, 2.473327340750859, 2.475469278388814], 'val_acc': [0.03612479454362138, 0.07717569736375401, 0.13464696202518905, 0.11001641985964893, 0.08374384226054198, 0.08374384226054198, 0.08374384226054198, 0.08374384226054198, 0.10180623973115716, 0.12151067273321214, 0.12315270895587987, 0.13957307039957328, 0.1264367814012153, 0.1444991789697035, 0.1444991789697035, 0.13957307059531923, 0.16420361167202246, 0.16748768392161195, 0.16748768392161195, 0.16420361176989545, 0.14942528664674273, 0.15106732277153748, 0.15270935918995115, 0.1543513953147459, 0.15927750368913016, 0.15927750368913016, 0.1642036118677684, 0.1707717563669474, 0.16256157535148175, 0.14285714195181778, 0.1576354671728435, 0.1756978649370776, 0.16091953942243298, 0.17077175656269336, 0.16748768431310387, 0.1642036118677684, 0.1642036118677684, 0.1658456478946902, 0.16748768392161195, 0.16420361176989545, 0.147783250521948, 0.16748768392161195, 0.17241379229599618, 0.16748768392161195, 0.16748768392161195, 0.16584564769894422, 0.16420361157414948, 0.16748768392161195, 0.16748768392161195, 0.16748768392161195, 0.1658456477968172, 0.16420361167202246, 0.1658456477968172, 0.16420361167202246, 0.1691297200464067, 0.1658456477968172, 0.16420361167202246, 0.16256157554722772, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16256157554722772, 0.16420361167202246, 0.1658456477968172, 0.16091953942243298, 0.15927750319976525, 0.16256157554722772, 0.1658456477968172, 0.16748768392161195, 0.1658456477968172, 0.16748768392161195, 0.16748768392161195, 0.16748768401948494, 0.16912972014427968, 0.1658456477968172, 0.16912972014427968, 0.16748768401948494, 0.1625615756451007, 0.16912972014427968, 0.16420361176989545, 0.16420361176989545, 0.16912972014427968, 0.16912972014427968, 0.16748768401948494, 0.17077175626907443, 0.1658456478946902, 0.16912972014427968, 0.16912972014427968, 0.1658456478946902, 0.1658456478946902, 0.16912972014427968, 0.16912972014427968, 0.16748768392161195, 0.1658456478946902, 0.1658456477968172, 0.1658456477968172, 0.16912972014427968, 0.1658456478946902, 0.1658456477968172, 0.16420361167202246, 0.16748768411735793, 0.16584564799256318, 0.1642036118677684, 0.15927750329763823, 0.15927750329763823, 0.15927750329763823, 0.15927750319976525, 0.16748768411735793, 0.16748768411735793, 0.16748768411735793, 0.1642036118677684, 0.1642036118677684, 0.16912972024215267, 0.16256157574297367, 0.1642036118677684, 0.1642036118677684, 0.1642036118677684, 0.16420361176989545, 0.16748768411735793, 0.16584564799256318, 0.1642036118677684, 0.1642036119656414, 0.16091953961817893, 0.1642036118677684, 0.1609195397160519, 0.1609195397160519, 0.15927750359125717, 0.1642036119656414, 0.17077175656269336, 0.16584564809043614, 0.16584564809043614, 0.17405582891015584, 0.1674876842152309, 0.17733990115974532, 0.1724137926874881, 0.1724137927853611, 0.16748768431310387, 0.16584564818830913, 0.16584564818830913, 0.16748768431310387, 0.16748768431310387, 0.16256157584084666, 0.1642036119656414, 0.1642036119656414, 0.16748768431310387, 0.17077175666056635, 0.1674876842152309, 0.17077175666056635, 0.17077175656269336, 0.16912972043789862, 0.16912972043789862, 0.16912972043789862, 0.17077175656269336, 0.1707717563669474, 0.16912972034002563, 0.16584564818830913, 0.16584564799256318, 0.16584564809043614, 0.16584564799256318, 0.16420361216138737, 0.16748768441097686, 0.1674876842152309, 0.1674876842152309, 0.1674876842152309, 0.16912972034002563, 0.16584564809043614, 0.1674876842152309, 0.16584564828618212, 0.16912972034002563, 0.1674876842152309, 0.16584564838405508, 0.16912972043789862, 0.16420361216138737, 0.1674876842152309, 0.16584564809043614, 0.16584564809043614, 0.1674876842152309, 0.16584564809043614, 0.16748768441097686, 0.16912972043789862, 0.1674876842152309, 0.1674876842152309, 0.1674876842152309, 0.16912972043789862, 0.16912972043789862, 0.16912972043789862, 0.16912972043789862, 0.17077175675843934, 0.16912972043789862, 0.16912972043789862, 0.16912972043789862, 0.17077175656269336, 0.16912972043789862, 0.16420361225926033, 0.16748768441097686, 0.16584564828618212, 0.16912972043789862, 0.16748768450884982, 0.17241379288323408, 0.1691297206336446, 0.1691297206336446, 0.1691297206336446, 0.1691297205357716, 0.1691297206336446, 0.16748768441097686, 0.16584564809043614, 0.16748768441097686, 0.16584564809043614, 0.1691297205357716, 0.1691297205357716, 0.1674876842152309, 0.1674876842152309, 0.17077175666056635, 0.16912972034002563, 0.17241379288323408, 0.17077175675843934, 0.17405582900802882, 0.1691297205357716, 0.16748768441097686, 0.16748768441097686, 0.1691297206336446, 0.16584564828618212, 0.16748768441097686, 0.16584564828618212, 0.1691297205357716, 0.16912972043789862, 0.17077175666056635, 0.17569786513282357, 0.16748768450884982, 0.17241379288323408, 0.17241379288323408, 0.17405582881228285, 0.16748768441097686, 0.1674876842152309, 0.16256157603659263, 0.16584564809043614, 0.16748768441097686, 0.16748768441097686, 0.16912972034002563, 0.16748768441097686, 0.1691297205357716, 0.1691297205357716, 0.16748768441097686, 0.1674876842152309, 0.16748768441097686, 0.16748768441097686, 0.16584564809043614, 0.16748768441097686, 0.16748768441097686, 0.16748768441097686, 0.16584564809043614, 0.17077175666056635, 0.17077175666056635, 0.16748768441097686, 0.16584564809043614, 0.16584564828618212, 0.16584564828618212, 0.16584564828618212, 0.16584564828618212, 0.15599343134166768, 0.15927750378700312, 0.15599343143954067, 0.16256157574297367, 0.1609195398139249, 0.15927750368913016, 0.16256157593871964, 0.16256157593871964, 0.16256157593871964, 0.15763546746646243, 0.1609195398139249, 0.16912972014427968, 0.16256157593871964, 0.16748768431310387, 0.16256157593871964, 0.16912972043789862, 0.16912972034002563, 0.16748768431310387, 0.16584564818830913, 0.1609195398139249, 0.16584564818830913, 0.16584564818830913, 0.16912972043789862, 0.1642036120635144, 0.16748768431310387, 0.16748768431310387, 0.1642036120635144, 0.16748768431310387, 0.16748768431310387, 0.16748768431310387, 0.16748768431310387, 0.16912972043789862, 0.16912972043789862, 0.16748768431310387, 0.17405582881228285, 0.16256157593871964, 0.16748768431310387, 0.16748768431310387], 'loss': [2.716545870857317, 2.704617066353988, 2.6941330112715764, 2.6856461889200385, 2.6786341495827237, 2.672897719651522, 2.667477047467868, 2.662286798077687, 2.6567351728004596, 2.65031662829358, 2.642576056780022, 2.6335349819253846, 2.624040972182883, 2.623550850850601, 2.603771507862412, 2.592796216020839, 2.575025885991247, 2.559374962205515, 2.5445133844685017, 2.528784596895535, 2.5161498715990134, 2.504513196827695, 2.497377793353196, 2.4892295522121923, 2.4822283493175155, 2.4786576717541204, 2.473067272762009, 2.4697421286385164, 2.4676892692793078, 2.5118581477131934, 2.4990067081529745, 2.4921843494477947, 2.4706681164383153, 2.468223029925838, 2.4648969750139993, 2.459850012252463, 2.461757176025203, 2.461395187201686, 2.4586838009421097, 2.4562977062358504, 2.505596715175151, 2.4692501990457334, 2.460706790810493, 2.459685097191123, 2.457085753270488, 2.453780954427543, 2.453725006546083, 2.453515859502052, 2.4525563732554536, 2.4520162005688864, 2.4523277647930986, 2.4521972171579787, 2.4517243078846707, 2.451354195792572, 2.451243295121242, 2.4505565653101864, 2.450418430187374, 2.449961571429055, 2.449210689200023, 2.4489408883715558, 2.447894058775853, 2.4455340723237464, 2.445631249974151, 2.4454137645462946, 2.4451553797085426, 2.445000143951949, 2.444282162116049, 2.4446912845791733, 2.444733947013683, 2.4443170199893585, 2.443670737620986, 2.443061787979314, 2.441896599469978, 2.4419370092160895, 2.441590569396283, 2.4410982180914593, 2.4401394935114427, 2.439777013849184, 2.43934692602138, 2.4391048806403943, 2.439772595125547, 2.439476424223099, 2.439610598758016, 2.439136090170921, 2.4387760319014595, 2.4384256129881683, 2.4384490168804507, 2.4364763535023717, 2.4364522457122804, 2.4364092778865807, 2.4366664590776823, 2.435916476279069, 2.435600178638278, 2.4363554439505513, 2.4353972175527647, 2.435209640291437, 2.4354763259143555, 2.43566602534582, 2.4352232070429367, 2.4346002935139306, 2.4362948456828843, 2.4392693316912015, 2.4385796916068703, 2.439197818552444, 2.439142927252047, 2.4396404741236317, 2.4391787909873948, 2.4381341171460473, 2.436746656331683, 2.436168738214387, 2.4353767763173066, 2.4355381115750854, 2.4350799419551903, 2.4345872888819637, 2.4355707099060746, 2.4358620015013144, 2.435170600546459, 2.4335502684238755, 2.4307503034446762, 2.430506340825827, 2.4308414408313666, 2.4298762934408638, 2.4314412387244757, 2.4332493871144445, 2.4346399399289362, 2.4336597282049346, 2.4321974524237535, 2.4313077315657536, 2.4308541639629575, 2.431189570338819, 2.4297138825089535, 2.428309823453304, 2.428247853030414, 2.427700066615424, 2.4283365749970107, 2.428680647570005, 2.430535807208114, 2.4290960906222616, 2.426907231822396, 2.428870483884087, 2.4281836539078543, 2.4273672160671476, 2.4294618456760224, 2.4299261837279773, 2.429831295679237, 2.4280305904529422, 2.4275178419735886, 2.4274182083670364, 2.4292464532401774, 2.430533463264638, 2.4317830834790177, 2.4285335805137054, 2.4272002915337345, 2.4276323626907943, 2.4270603738036733, 2.4253972545051967, 2.4264776695925105, 2.4256082504436955, 2.425590094207983, 2.423901150701472, 2.435726585329436, 2.4292728227266784, 2.427805822243191, 2.4291698853338035, 2.4242013110762013, 2.4258117910772867, 2.424926418837091, 2.42498709151877, 2.4249162459520344, 2.4255614186704038, 2.425621508377044, 2.4257341901869256, 2.42564210686106, 2.4259528662389798, 2.4272829575705086, 2.424709490633109, 2.425134738123148, 2.424406489993023, 2.4242655483848994, 2.4237731683915156, 2.4244161231806634, 2.423863834424185, 2.42283814163913, 2.421906533916873, 2.4226053304495996, 2.422417831714638, 2.422277295271229, 2.4214634978550906, 2.4227699563733363, 2.4256787925775045, 2.4245351992103843, 2.4209749225718284, 2.4223545006902802, 2.421395226915269, 2.4213070962463314, 2.4204461399289863, 2.4204403290758387, 2.4207625454700947, 2.4211034567204344, 2.4204972512912946, 2.421583397236693, 2.421646114149622, 2.4214050291010487, 2.4191231884261177, 2.419447945079764, 2.4195864661763093, 2.419984248825167, 2.420443326687666, 2.4210116629238247, 2.418887629009615, 2.4188080441046056, 2.4184958978844864, 2.417822984111872, 2.4175743719880343, 2.4171273150238415, 2.417320533356872, 2.4170214003850794, 2.4160540644393076, 2.4160976551886213, 2.4161538537278067, 2.4160301917387477, 2.4157238644985695, 2.4149840816090484, 2.4153053683177155, 2.416100805447087, 2.4152051236350434, 2.418026669559048, 2.419249675503991, 2.4196385041888977, 2.4148201399025733, 2.416328186567804, 2.415386030757207, 2.4166177202299144, 2.416522179687782, 2.415526078124311, 2.416253734565124, 2.416766581251391, 2.4149419860918173, 2.416028075443401, 2.417176511694029, 2.4143551867600586, 2.4151377147228077, 2.41486398708649, 2.4148547372289264, 2.4152445052928257, 2.4150778550142133, 2.4123264833642226, 2.4132709591295685, 2.4141647541547457, 2.413614379992475, 2.4129935335084887, 2.4126723771222562, 2.414988448438703, 2.4128037431156857, 2.413639797371271, 2.416217372745459, 2.4147041677204735, 2.4147789945347844, 2.412941158576668, 2.4132636873139495, 2.4144545764648937, 2.4132555251738372, 2.4149930937334254, 2.414066943887323, 2.4141272570318266, 2.414737385254376, 2.4153852565577387, 2.4138429595704443, 2.414558198662509, 2.415106994464412, 2.414329685273846, 2.4149178938699207, 2.414964234412818, 2.4136290470922264, 2.4140481553283317, 2.414587258803037, 2.413231141856074, 2.4128254706365126, 2.412784264758382, 2.4113661940337696, 2.409858407328016, 2.4091317225775435, 2.4112021376709674, 2.4095877440803104, 2.4089714004273777, 2.411392051923936, 2.4105101827233724, 2.411987662364325, 2.411302676582728, 2.409179076325967, 2.409562659802133, 2.408016347395566, 2.4085494089420325, 2.408843966724936, 2.409488535808587, 2.410895450012395, 2.4074115863815715, 2.4088149158861603, 2.407761454826997, 2.4093278107457095], 'acc': [0.04763860347273413, 0.056262833443020896, 0.08583162172986251, 0.11416837816916452, 0.09240246442064368, 0.08295687919585856, 0.08295687939168492, 0.08295687899085286, 0.08788501039851128, 0.09609856275990758, 0.11088295680541523, 0.11498973337776607, 0.1211498972571606, 0.10882956832341345, 0.12731006231151323, 0.11991786401129846, 0.11581108802642666, 0.1564681731761114, 0.1605749491609832, 0.16221765804829294, 0.1659137587650111, 0.1601642705454229, 0.16180698221101897, 0.16591375700257396, 0.16837782370848334, 0.1712525668391457, 0.1683778236901246, 0.1667351126303663, 0.1737166335266963, 0.1564681728028174, 0.15770020644033225, 0.15400410647020202, 0.1675564692556491, 0.16878850054324776, 0.17864476474770774, 0.16837782449178873, 0.17043121138882097, 0.17330595373617794, 0.17700205327793803, 0.17741273071854022, 0.16960985693598674, 0.17782340911991543, 0.1749486646001099, 0.17412730914978522, 0.17823408534884208, 0.17905544037079665, 0.17659137646153233, 0.17905544056662298, 0.1786447645518814, 0.17864476396440235, 0.17864476433769633, 0.17905544056662298, 0.17905544119081948, 0.17987679582112134, 0.17946611978802104, 0.17946611839887788, 0.17905544117246075, 0.17823408691545287, 0.17823408732546428, 0.1790554409766344, 0.17987679740609086, 0.17987679738773213, 0.1802874740266702, 0.17987679560693628, 0.18028747461414923, 0.17987679564365372, 0.18110882847950444, 0.1786447645518814, 0.17905544236577756, 0.177823408709904, 0.17987679758355848, 0.18028747363501751, 0.18069815105726098, 0.18069815143055495, 0.17782340814078368, 0.18193018369728534, 0.1815195066850533, 0.18028747463250797, 0.18193018410729678, 0.18275153893342497, 0.18357289497122872, 0.17987679621277403, 0.181519507076706, 0.1823408625270307, 0.18316221656985351, 0.18316221815482303, 0.18193018469477582, 0.18110883026030028, 0.183162218528117, 0.18193018471313452, 0.18275154053675322, 0.18357289497122872, 0.18316221794063794, 0.18357289477540237, 0.18316221835064936, 0.18275154010838307, 0.18193018369728534, 0.18193018410729678, 0.18234086233120433, 0.18357289596871917, 0.1823408626861396, 0.1815195066850533, 0.18480492780707944, 0.1806981526422305, 0.18357289616454553, 0.18398357217928712, 0.18069815223221905, 0.1806981526422305, 0.18275153893342497, 0.1790554421515925, 0.17782340931574178, 0.1843942512048588, 0.18275153953926274, 0.18357289497122872, 0.183572894579576, 0.1798767968002531, 0.18357289477540237, 0.18357289577289285, 0.18357289555870776, 0.18521560604934575, 0.1860369618546057, 0.18439424942406296, 0.1827515393434364, 0.18193018469477582, 0.1811088286753308, 0.18193018371564407, 0.18275154112423225, 0.18234086133371388, 0.18316221717569128, 0.18069815087979335, 0.18316221835064936, 0.18398357180599315, 0.18521560583516067, 0.18480492803962323, 0.18316221676567987, 0.1856262838448832, 0.18357289573617538, 0.1864476377102384, 0.1852156046602026, 0.18809034816415893, 0.18603696128548539, 0.18398357198346077, 0.18275154053675322, 0.18316221874230207, 0.1835728941695646, 0.18603696066128889, 0.18480492743378546, 0.18726899275055167, 0.1885010255864024, 0.1864476394726755, 0.18398357299930995, 0.1811088290853422, 0.1872689945129888, 0.18809034818251766, 0.1872689929280193, 0.18850102775885094, 0.18644763751441204, 0.18644763831607614, 0.1831622181364643, 0.18603696148131174, 0.1839835720018195, 0.18357289573617538, 0.18726899372968342, 0.18439424940570423, 0.1831622185464757, 0.1868583161116136, 0.18439424942406296, 0.18685831530994948, 0.18521560622681338, 0.18603696046546256, 0.18562628405906825, 0.18193018448059073, 0.18562628245574003, 0.1827515397350891, 0.18521560661846606, 0.1868583169132777, 0.18767967035026276, 0.18685831630743993, 0.18685831709074532, 0.18726899312384565, 0.1864476396868606, 0.18439425081320612, 0.1885010271897306, 0.19014373824948894, 0.18932238123255343, 0.186858316894919, 0.18809034796833257, 0.18891170519945313, 0.18151950670341202, 0.18028747363501751, 0.18480492901875498, 0.1860369618546057, 0.1852156066368248, 0.18850102560476112, 0.18891170418360395, 0.18891170537692076, 0.18726899294637803, 0.18521560644099844, 0.18726899275055167, 0.18685831630743993, 0.18521560583516067, 0.18644763847518506, 0.18480492821709085, 0.18357289614618683, 0.18357289516705508, 0.18234086272285704, 0.18562628404070955, 0.18685831530994948, 0.18603696148131174, 0.18850102617388143, 0.1872689941213361, 0.18644763927684918, 0.18603696067964762, 0.1872689929280193, 0.18767967154357956, 0.1880903487883554, 0.18973305902809087, 0.186858316894919, 0.18932238141002106, 0.18767967193523227, 0.190143736254508, 0.19055441367675147, 0.18932238258497916, 0.18767967213105863, 0.1880903495533021, 0.18726899492300023, 0.1897330590464496, 0.18850102738555696, 0.1860369602696362, 0.1913757693045438, 0.1901437362728667, 0.19137577069368694, 0.18398357337260393, 0.18809034796833257, 0.18726899312384565, 0.18767967115192688, 0.19055441526172098, 0.19342915759071921, 0.1872689945129888, 0.18480492762961181, 0.18726899470881514, 0.18932238242587024, 0.18480492921458133, 0.18809034835998528, 0.1889117051810944, 0.18644763927684918, 0.18973306002558135, 0.1909650924881381, 0.18809034875163796, 0.18932238141002106, 0.19219712434485708, 0.18809034818251766, 0.18891170381030997, 0.188911704397789, 0.1880903499633135, 0.18809035015913986, 0.1868583161116136, 0.18850102581894618, 0.19096509129482128, 0.1885010271713719, 0.18685831630743993, 0.18932238299499057, 0.18767967195359098, 0.1889117032044722, 0.18480492723795913, 0.18480492702377405, 0.19096509229231173, 0.18644763888519647, 0.1872689949046415, 0.18932238199750012, 0.18973305885062325, 0.19014373668287815, 0.18562628247409876, 0.1897330606130604, 0.19096509246977939, 0.19096509286143207, 0.18973306063141912, 0.18891170537692076, 0.18439425157815278, 0.18644763847518506, 0.1897330590464496, 0.18932238082254202, 0.18850102580058747, 0.19014373687870448, 0.18973305885062325, 0.19096509150900634, 0.18767967035026276, 0.19014373764365117, 0.1876796705460891, 0.19096509307561713, 0.19137577030203426, 0.19055441465588321, 0.19055441428258924, 0.1905544144784156, 0.1901437382311302, 0.1901437378394775, 0.19260780335207006, 0.18932238201585883, 0.19178644811593043, 0.19178644692261362]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
