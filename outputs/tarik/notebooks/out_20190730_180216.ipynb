{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf36.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 18:02:16 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': 'Split', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F60297BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F600066EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0788, Accuracy:0.3934, Validation Loss:1.0751, Validation Accuracy:0.3760\n",
    "Epoch #2: Loss:1.0743, Accuracy:0.3907, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0742, Accuracy:0.3931, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #4: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.4011\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3823, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0751, Accuracy:0.3951, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0750, Accuracy:0.3939, Validation Loss:1.0744, Validation Accuracy:0.3953\n",
    "Epoch #11: Loss:1.0748, Accuracy:0.3765, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0743, Accuracy:0.3944, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0742, Accuracy:0.3954, Validation Loss:1.0741, Validation Accuracy:0.3933\n",
    "Epoch #14: Loss:1.0742, Accuracy:0.3993, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0760, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0746, Accuracy:0.3936, Validation Loss:1.0742, Validation Accuracy:0.3970\n",
    "Epoch #17: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0742, Accuracy:0.3997, Validation Loss:1.0739, Validation Accuracy:0.3986\n",
    "Epoch #20: Loss:1.0745, Accuracy:0.3802, Validation Loss:1.0738, Validation Accuracy:0.3953\n",
    "Epoch #21: Loss:1.0743, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3953\n",
    "Epoch #22: Loss:1.0740, Accuracy:0.3973, Validation Loss:1.0736, Validation Accuracy:0.3949\n",
    "Epoch #23: Loss:1.0743, Accuracy:0.3956, Validation Loss:1.0738, Validation Accuracy:0.4011\n",
    "Epoch #24: Loss:1.0747, Accuracy:0.3906, Validation Loss:1.0738, Validation Accuracy:0.3953\n",
    "Epoch #25: Loss:1.0749, Accuracy:0.3941, Validation Loss:1.0739, Validation Accuracy:0.3929\n",
    "Epoch #26: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #27: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3961\n",
    "Epoch #28: Loss:1.0740, Accuracy:0.3972, Validation Loss:1.0741, Validation Accuracy:0.4011\n",
    "Epoch #29: Loss:1.0739, Accuracy:0.3929, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0754, Accuracy:0.3960, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0741, Accuracy:0.3932, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0741, Accuracy:0.3929, Validation Loss:1.0740, Validation Accuracy:0.3961\n",
    "Epoch #35: Loss:1.0746, Accuracy:0.3792, Validation Loss:1.0740, Validation Accuracy:0.3949\n",
    "Epoch #36: Loss:1.0742, Accuracy:0.3932, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3961\n",
    "Epoch #38: Loss:1.0743, Accuracy:0.3944, Validation Loss:1.0740, Validation Accuracy:0.3961\n",
    "Epoch #39: Loss:1.0743, Accuracy:0.3961, Validation Loss:1.0740, Validation Accuracy:0.3953\n",
    "Epoch #40: Loss:1.0740, Accuracy:0.3936, Validation Loss:1.0739, Validation Accuracy:0.3937\n",
    "Epoch #41: Loss:1.0747, Accuracy:0.3941, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0744, Accuracy:0.3986, Validation Loss:1.0739, Validation Accuracy:0.3982\n",
    "Epoch #43: Loss:1.0742, Accuracy:0.3968, Validation Loss:1.0741, Validation Accuracy:0.3982\n",
    "Epoch #44: Loss:1.0739, Accuracy:0.3960, Validation Loss:1.0739, Validation Accuracy:0.3953\n",
    "Epoch #45: Loss:1.0741, Accuracy:0.3919, Validation Loss:1.0738, Validation Accuracy:0.3961\n",
    "Epoch #46: Loss:1.0750, Accuracy:0.3972, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #47: Loss:1.0741, Accuracy:0.3980, Validation Loss:1.0735, Validation Accuracy:0.3986\n",
    "Epoch #48: Loss:1.0739, Accuracy:0.3950, Validation Loss:1.0737, Validation Accuracy:0.3945\n",
    "Epoch #49: Loss:1.0743, Accuracy:0.3934, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #50: Loss:1.0738, Accuracy:0.4010, Validation Loss:1.0734, Validation Accuracy:0.3986\n",
    "Epoch #51: Loss:1.0739, Accuracy:0.3940, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #52: Loss:1.0738, Accuracy:0.3953, Validation Loss:1.0734, Validation Accuracy:0.3961\n",
    "Epoch #53: Loss:1.0737, Accuracy:0.3944, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #54: Loss:1.0736, Accuracy:0.3952, Validation Loss:1.0735, Validation Accuracy:0.3953\n",
    "Epoch #55: Loss:1.0740, Accuracy:0.3936, Validation Loss:1.0736, Validation Accuracy:0.3920\n",
    "Epoch #56: Loss:1.0737, Accuracy:0.3935, Validation Loss:1.0736, Validation Accuracy:0.3920\n",
    "Epoch #57: Loss:1.0737, Accuracy:0.3939, Validation Loss:1.0735, Validation Accuracy:0.3953\n",
    "Epoch #58: Loss:1.0732, Accuracy:0.4016, Validation Loss:1.0733, Validation Accuracy:0.4019\n",
    "Epoch #59: Loss:1.0741, Accuracy:0.3913, Validation Loss:1.0732, Validation Accuracy:0.3953\n",
    "Epoch #60: Loss:1.0739, Accuracy:0.3996, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #61: Loss:1.0738, Accuracy:0.3968, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #62: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0736, Validation Accuracy:0.3966\n",
    "Epoch #63: Loss:1.0742, Accuracy:0.3851, Validation Loss:1.0734, Validation Accuracy:0.3953\n",
    "Epoch #64: Loss:1.0736, Accuracy:0.3968, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #65: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.3953\n",
    "Epoch #66: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #67: Loss:1.0738, Accuracy:0.3969, Validation Loss:1.0736, Validation Accuracy:0.3953\n",
    "Epoch #68: Loss:1.0733, Accuracy:0.4010, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #69: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0731, Validation Accuracy:0.4007\n",
    "Epoch #70: Loss:1.0729, Accuracy:0.4002, Validation Loss:1.0730, Validation Accuracy:0.3953\n",
    "Epoch #71: Loss:1.0730, Accuracy:0.4001, Validation Loss:1.0731, Validation Accuracy:0.3966\n",
    "Epoch #72: Loss:1.0734, Accuracy:0.3941, Validation Loss:1.0734, Validation Accuracy:0.4002\n",
    "Epoch #73: Loss:1.0729, Accuracy:0.3970, Validation Loss:1.0731, Validation Accuracy:0.3974\n",
    "Epoch #74: Loss:1.0732, Accuracy:0.4000, Validation Loss:1.0730, Validation Accuracy:0.4015\n",
    "Epoch #75: Loss:1.0727, Accuracy:0.3961, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #76: Loss:1.0735, Accuracy:0.3949, Validation Loss:1.0732, Validation Accuracy:0.3966\n",
    "Epoch #77: Loss:1.0731, Accuracy:0.3991, Validation Loss:1.0733, Validation Accuracy:0.3937\n",
    "Epoch #78: Loss:1.0730, Accuracy:0.3954, Validation Loss:1.0733, Validation Accuracy:0.3953\n",
    "Epoch #79: Loss:1.0731, Accuracy:0.3940, Validation Loss:1.0731, Validation Accuracy:0.3945\n",
    "Epoch #80: Loss:1.0726, Accuracy:0.3978, Validation Loss:1.0729, Validation Accuracy:0.3953\n",
    "Epoch #81: Loss:1.0726, Accuracy:0.3969, Validation Loss:1.0733, Validation Accuracy:0.3961\n",
    "Epoch #82: Loss:1.0731, Accuracy:0.3896, Validation Loss:1.0732, Validation Accuracy:0.3937\n",
    "Epoch #83: Loss:1.0725, Accuracy:0.3998, Validation Loss:1.0730, Validation Accuracy:0.3966\n",
    "Epoch #84: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0733, Validation Accuracy:0.3929\n",
    "Epoch #85: Loss:1.0727, Accuracy:0.3946, Validation Loss:1.0731, Validation Accuracy:0.3961\n",
    "Epoch #86: Loss:1.0730, Accuracy:0.3955, Validation Loss:1.0735, Validation Accuracy:0.4002\n",
    "Epoch #87: Loss:1.0726, Accuracy:0.3976, Validation Loss:1.0732, Validation Accuracy:0.3961\n",
    "Epoch #88: Loss:1.0723, Accuracy:0.3998, Validation Loss:1.0732, Validation Accuracy:0.3982\n",
    "Epoch #89: Loss:1.0731, Accuracy:0.3980, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #90: Loss:1.0726, Accuracy:0.3996, Validation Loss:1.0731, Validation Accuracy:0.3961\n",
    "Epoch #91: Loss:1.0726, Accuracy:0.3974, Validation Loss:1.0730, Validation Accuracy:0.4015\n",
    "Epoch #92: Loss:1.0724, Accuracy:0.3994, Validation Loss:1.0730, Validation Accuracy:0.3961\n",
    "Epoch #93: Loss:1.0722, Accuracy:0.4005, Validation Loss:1.0730, Validation Accuracy:0.4011\n",
    "Epoch #94: Loss:1.0735, Accuracy:0.3987, Validation Loss:1.0730, Validation Accuracy:0.4007\n",
    "Epoch #95: Loss:1.0724, Accuracy:0.3990, Validation Loss:1.0732, Validation Accuracy:0.4015\n",
    "Epoch #96: Loss:1.0728, Accuracy:0.3979, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #97: Loss:1.0726, Accuracy:0.4031, Validation Loss:1.0728, Validation Accuracy:0.4019\n",
    "Epoch #98: Loss:1.0747, Accuracy:0.3721, Validation Loss:1.0735, Validation Accuracy:0.3929\n",
    "Epoch #99: Loss:1.0722, Accuracy:0.3959, Validation Loss:1.0731, Validation Accuracy:0.3961\n",
    "Epoch #100: Loss:1.0723, Accuracy:0.3970, Validation Loss:1.0731, Validation Accuracy:0.3982\n",
    "Epoch #101: Loss:1.0726, Accuracy:0.3975, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #102: Loss:1.0724, Accuracy:0.3973, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #103: Loss:1.0731, Accuracy:0.3742, Validation Loss:1.0732, Validation Accuracy:0.4023\n",
    "Epoch #104: Loss:1.0721, Accuracy:0.4027, Validation Loss:1.0729, Validation Accuracy:0.4035\n",
    "Epoch #105: Loss:1.0722, Accuracy:0.4022, Validation Loss:1.0729, Validation Accuracy:0.4035\n",
    "Epoch #106: Loss:1.0720, Accuracy:0.4024, Validation Loss:1.0728, Validation Accuracy:0.3953\n",
    "Epoch #107: Loss:1.0721, Accuracy:0.4012, Validation Loss:1.0729, Validation Accuracy:0.4007\n",
    "Epoch #108: Loss:1.0723, Accuracy:0.4003, Validation Loss:1.0730, Validation Accuracy:0.4019\n",
    "Epoch #109: Loss:1.0724, Accuracy:0.3964, Validation Loss:1.0729, Validation Accuracy:0.4019\n",
    "Epoch #110: Loss:1.0722, Accuracy:0.4011, Validation Loss:1.0728, Validation Accuracy:0.4019\n",
    "Epoch #111: Loss:1.0718, Accuracy:0.4013, Validation Loss:1.0729, Validation Accuracy:0.4019\n",
    "Epoch #112: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0731, Validation Accuracy:0.4035\n",
    "Epoch #113: Loss:1.0721, Accuracy:0.3991, Validation Loss:1.0728, Validation Accuracy:0.4015\n",
    "Epoch #114: Loss:1.0720, Accuracy:0.3968, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #115: Loss:1.0723, Accuracy:0.3920, Validation Loss:1.0727, Validation Accuracy:0.4019\n",
    "Epoch #116: Loss:1.0727, Accuracy:0.3999, Validation Loss:1.0729, Validation Accuracy:0.4015\n",
    "Epoch #117: Loss:1.0720, Accuracy:0.4001, Validation Loss:1.0731, Validation Accuracy:0.4015\n",
    "Epoch #118: Loss:1.0721, Accuracy:0.3954, Validation Loss:1.0732, Validation Accuracy:0.4015\n",
    "Epoch #119: Loss:1.0719, Accuracy:0.3985, Validation Loss:1.0729, Validation Accuracy:0.4019\n",
    "Epoch #120: Loss:1.0725, Accuracy:0.4022, Validation Loss:1.0729, Validation Accuracy:0.4011\n",
    "Epoch #121: Loss:1.0723, Accuracy:0.4013, Validation Loss:1.0725, Validation Accuracy:0.4019\n",
    "Epoch #122: Loss:1.0719, Accuracy:0.3948, Validation Loss:1.0726, Validation Accuracy:0.3961\n",
    "Epoch #123: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0726, Validation Accuracy:0.4019\n",
    "Epoch #124: Loss:1.0718, Accuracy:0.4013, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #125: Loss:1.0718, Accuracy:0.4009, Validation Loss:1.0731, Validation Accuracy:0.3961\n",
    "Epoch #126: Loss:1.0724, Accuracy:0.3999, Validation Loss:1.0729, Validation Accuracy:0.4019\n",
    "Epoch #127: Loss:1.0717, Accuracy:0.4006, Validation Loss:1.0726, Validation Accuracy:0.4023\n",
    "Epoch #128: Loss:1.0715, Accuracy:0.4010, Validation Loss:1.0726, Validation Accuracy:0.4023\n",
    "Epoch #129: Loss:1.0719, Accuracy:0.3954, Validation Loss:1.0725, Validation Accuracy:0.4023\n",
    "Epoch #130: Loss:1.0715, Accuracy:0.3991, Validation Loss:1.0724, Validation Accuracy:0.3974\n",
    "Epoch #131: Loss:1.0714, Accuracy:0.3995, Validation Loss:1.0726, Validation Accuracy:0.4015\n",
    "Epoch #132: Loss:1.0721, Accuracy:0.3936, Validation Loss:1.0728, Validation Accuracy:0.3945\n",
    "Epoch #133: Loss:1.0715, Accuracy:0.3968, Validation Loss:1.0725, Validation Accuracy:0.3949\n",
    "Epoch #134: Loss:1.0711, Accuracy:0.3996, Validation Loss:1.0723, Validation Accuracy:0.4015\n",
    "Epoch #135: Loss:1.0713, Accuracy:0.3979, Validation Loss:1.0722, Validation Accuracy:0.3966\n",
    "Epoch #136: Loss:1.0710, Accuracy:0.3999, Validation Loss:1.0719, Validation Accuracy:0.4019\n",
    "Epoch #137: Loss:1.0712, Accuracy:0.4001, Validation Loss:1.0720, Validation Accuracy:0.4015\n",
    "Epoch #138: Loss:1.0715, Accuracy:0.3952, Validation Loss:1.0720, Validation Accuracy:0.4027\n",
    "Epoch #139: Loss:1.0711, Accuracy:0.4013, Validation Loss:1.0716, Validation Accuracy:0.4019\n",
    "Epoch #140: Loss:1.0712, Accuracy:0.4011, Validation Loss:1.0719, Validation Accuracy:0.4019\n",
    "Epoch #141: Loss:1.0716, Accuracy:0.4017, Validation Loss:1.0722, Validation Accuracy:0.4007\n",
    "Epoch #142: Loss:1.0712, Accuracy:0.3988, Validation Loss:1.0722, Validation Accuracy:0.3949\n",
    "Epoch #143: Loss:1.0713, Accuracy:0.3980, Validation Loss:1.0716, Validation Accuracy:0.4027\n",
    "Epoch #144: Loss:1.0713, Accuracy:0.4013, Validation Loss:1.0718, Validation Accuracy:0.4039\n",
    "Epoch #145: Loss:1.0718, Accuracy:0.4024, Validation Loss:1.0719, Validation Accuracy:0.4019\n",
    "Epoch #146: Loss:1.0714, Accuracy:0.4015, Validation Loss:1.0717, Validation Accuracy:0.4023\n",
    "Epoch #147: Loss:1.0709, Accuracy:0.4018, Validation Loss:1.0720, Validation Accuracy:0.3957\n",
    "Epoch #148: Loss:1.0710, Accuracy:0.3986, Validation Loss:1.0722, Validation Accuracy:0.3953\n",
    "Epoch #149: Loss:1.0713, Accuracy:0.3954, Validation Loss:1.0722, Validation Accuracy:0.4023\n",
    "Epoch #150: Loss:1.0711, Accuracy:0.4002, Validation Loss:1.0720, Validation Accuracy:0.4011\n",
    "Epoch #151: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0719, Validation Accuracy:0.4019\n",
    "Epoch #152: Loss:1.0709, Accuracy:0.4021, Validation Loss:1.0719, Validation Accuracy:0.4027\n",
    "Epoch #153: Loss:1.0710, Accuracy:0.4026, Validation Loss:1.0715, Validation Accuracy:0.4019\n",
    "Epoch #154: Loss:1.0707, Accuracy:0.4014, Validation Loss:1.0716, Validation Accuracy:0.4023\n",
    "Epoch #155: Loss:1.0711, Accuracy:0.4014, Validation Loss:1.0717, Validation Accuracy:0.4019\n",
    "Epoch #156: Loss:1.0713, Accuracy:0.3998, Validation Loss:1.0720, Validation Accuracy:0.4007\n",
    "Epoch #157: Loss:1.0707, Accuracy:0.3999, Validation Loss:1.0726, Validation Accuracy:0.3953\n",
    "Epoch #158: Loss:1.0710, Accuracy:0.3996, Validation Loss:1.0720, Validation Accuracy:0.4027\n",
    "Epoch #159: Loss:1.0709, Accuracy:0.4024, Validation Loss:1.0719, Validation Accuracy:0.4019\n",
    "Epoch #160: Loss:1.0718, Accuracy:0.3966, Validation Loss:1.0722, Validation Accuracy:0.4023\n",
    "Epoch #161: Loss:1.0718, Accuracy:0.4013, Validation Loss:1.0719, Validation Accuracy:0.4048\n",
    "Epoch #162: Loss:1.0711, Accuracy:0.4018, Validation Loss:1.0718, Validation Accuracy:0.4019\n",
    "Epoch #163: Loss:1.0708, Accuracy:0.4013, Validation Loss:1.0717, Validation Accuracy:0.4019\n",
    "Epoch #164: Loss:1.0709, Accuracy:0.4013, Validation Loss:1.0717, Validation Accuracy:0.4019\n",
    "Epoch #165: Loss:1.0710, Accuracy:0.4025, Validation Loss:1.0716, Validation Accuracy:0.4019\n",
    "Epoch #166: Loss:1.0713, Accuracy:0.3922, Validation Loss:1.0720, Validation Accuracy:0.3986\n",
    "Epoch #167: Loss:1.0709, Accuracy:0.4010, Validation Loss:1.0716, Validation Accuracy:0.4039\n",
    "Epoch #168: Loss:1.0705, Accuracy:0.4026, Validation Loss:1.0719, Validation Accuracy:0.4027\n",
    "Epoch #169: Loss:1.0712, Accuracy:0.3965, Validation Loss:1.0723, Validation Accuracy:0.3974\n",
    "Epoch #170: Loss:1.0714, Accuracy:0.3992, Validation Loss:1.0720, Validation Accuracy:0.3982\n",
    "Epoch #171: Loss:1.0709, Accuracy:0.3991, Validation Loss:1.0716, Validation Accuracy:0.4068\n",
    "Epoch #172: Loss:1.0708, Accuracy:0.4028, Validation Loss:1.0714, Validation Accuracy:0.4048\n",
    "Epoch #173: Loss:1.0710, Accuracy:0.4031, Validation Loss:1.0715, Validation Accuracy:0.4039\n",
    "Epoch #174: Loss:1.0707, Accuracy:0.4017, Validation Loss:1.0715, Validation Accuracy:0.4019\n",
    "Epoch #175: Loss:1.0707, Accuracy:0.4013, Validation Loss:1.0715, Validation Accuracy:0.4048\n",
    "Epoch #176: Loss:1.0710, Accuracy:0.4037, Validation Loss:1.0718, Validation Accuracy:0.4060\n",
    "Epoch #177: Loss:1.0711, Accuracy:0.4044, Validation Loss:1.0720, Validation Accuracy:0.4048\n",
    "Epoch #178: Loss:1.0709, Accuracy:0.4032, Validation Loss:1.0718, Validation Accuracy:0.4035\n",
    "Epoch #179: Loss:1.0710, Accuracy:0.4003, Validation Loss:1.0721, Validation Accuracy:0.4011\n",
    "Epoch #180: Loss:1.0707, Accuracy:0.4005, Validation Loss:1.0719, Validation Accuracy:0.4007\n",
    "Epoch #181: Loss:1.0708, Accuracy:0.4018, Validation Loss:1.0717, Validation Accuracy:0.4019\n",
    "Epoch #182: Loss:1.0714, Accuracy:0.3934, Validation Loss:1.0721, Validation Accuracy:0.4019\n",
    "Epoch #183: Loss:1.0709, Accuracy:0.4015, Validation Loss:1.0715, Validation Accuracy:0.4019\n",
    "Epoch #184: Loss:1.0714, Accuracy:0.4000, Validation Loss:1.0721, Validation Accuracy:0.4064\n",
    "Epoch #185: Loss:1.0709, Accuracy:0.4022, Validation Loss:1.0718, Validation Accuracy:0.4019\n",
    "Epoch #186: Loss:1.0709, Accuracy:0.4004, Validation Loss:1.0715, Validation Accuracy:0.4015\n",
    "Epoch #187: Loss:1.0706, Accuracy:0.4028, Validation Loss:1.0715, Validation Accuracy:0.4064\n",
    "Epoch #188: Loss:1.0704, Accuracy:0.4031, Validation Loss:1.0719, Validation Accuracy:0.4039\n",
    "Epoch #189: Loss:1.0705, Accuracy:0.4018, Validation Loss:1.0716, Validation Accuracy:0.4019\n",
    "Epoch #190: Loss:1.0715, Accuracy:0.4006, Validation Loss:1.0722, Validation Accuracy:0.4007\n",
    "Epoch #191: Loss:1.0716, Accuracy:0.3920, Validation Loss:1.0720, Validation Accuracy:0.4019\n",
    "Epoch #192: Loss:1.0713, Accuracy:0.4013, Validation Loss:1.0716, Validation Accuracy:0.4019\n",
    "Epoch #193: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0719, Validation Accuracy:0.4019\n",
    "Epoch #194: Loss:1.0709, Accuracy:0.4006, Validation Loss:1.0721, Validation Accuracy:0.4039\n",
    "Epoch #195: Loss:1.0709, Accuracy:0.4018, Validation Loss:1.0719, Validation Accuracy:0.4019\n",
    "Epoch #196: Loss:1.0707, Accuracy:0.4022, Validation Loss:1.0715, Validation Accuracy:0.4035\n",
    "Epoch #197: Loss:1.0703, Accuracy:0.4022, Validation Loss:1.0715, Validation Accuracy:0.4068\n",
    "Epoch #198: Loss:1.0707, Accuracy:0.4030, Validation Loss:1.0716, Validation Accuracy:0.4068\n",
    "Epoch #199: Loss:1.0706, Accuracy:0.4020, Validation Loss:1.0715, Validation Accuracy:0.3986\n",
    "Epoch #200: Loss:1.0706, Accuracy:0.3997, Validation Loss:1.0719, Validation Accuracy:0.4007\n",
    "Epoch #201: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0720, Validation Accuracy:0.4035\n",
    "Epoch #202: Loss:1.0706, Accuracy:0.4016, Validation Loss:1.0719, Validation Accuracy:0.4035\n",
    "Epoch #203: Loss:1.0705, Accuracy:0.3995, Validation Loss:1.0717, Validation Accuracy:0.3986\n",
    "Epoch #204: Loss:1.0707, Accuracy:0.4009, Validation Loss:1.0715, Validation Accuracy:0.4035\n",
    "Epoch #205: Loss:1.0705, Accuracy:0.4026, Validation Loss:1.0719, Validation Accuracy:0.4064\n",
    "Epoch #206: Loss:1.0704, Accuracy:0.4039, Validation Loss:1.0716, Validation Accuracy:0.4064\n",
    "Epoch #207: Loss:1.0707, Accuracy:0.4023, Validation Loss:1.0718, Validation Accuracy:0.4039\n",
    "Epoch #208: Loss:1.0705, Accuracy:0.4038, Validation Loss:1.0718, Validation Accuracy:0.4064\n",
    "Epoch #209: Loss:1.0706, Accuracy:0.4021, Validation Loss:1.0714, Validation Accuracy:0.4039\n",
    "Epoch #210: Loss:1.0708, Accuracy:0.4018, Validation Loss:1.0718, Validation Accuracy:0.4039\n",
    "Epoch #211: Loss:1.0709, Accuracy:0.3995, Validation Loss:1.0722, Validation Accuracy:0.4060\n",
    "Epoch #212: Loss:1.0705, Accuracy:0.4040, Validation Loss:1.0716, Validation Accuracy:0.4064\n",
    "Epoch #213: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0714, Validation Accuracy:0.4044\n",
    "Epoch #214: Loss:1.0703, Accuracy:0.4038, Validation Loss:1.0714, Validation Accuracy:0.4056\n",
    "Epoch #215: Loss:1.0704, Accuracy:0.4018, Validation Loss:1.0714, Validation Accuracy:0.4035\n",
    "Epoch #216: Loss:1.0708, Accuracy:0.4017, Validation Loss:1.0715, Validation Accuracy:0.4035\n",
    "Epoch #217: Loss:1.0710, Accuracy:0.4036, Validation Loss:1.0714, Validation Accuracy:0.4048\n",
    "Epoch #218: Loss:1.0703, Accuracy:0.4027, Validation Loss:1.0712, Validation Accuracy:0.4048\n",
    "Epoch #219: Loss:1.0707, Accuracy:0.4039, Validation Loss:1.0716, Validation Accuracy:0.4064\n",
    "Epoch #220: Loss:1.0707, Accuracy:0.4040, Validation Loss:1.0715, Validation Accuracy:0.4068\n",
    "Epoch #221: Loss:1.0708, Accuracy:0.4028, Validation Loss:1.0714, Validation Accuracy:0.4039\n",
    "Epoch #222: Loss:1.0704, Accuracy:0.4036, Validation Loss:1.0714, Validation Accuracy:0.4064\n",
    "Epoch #223: Loss:1.0703, Accuracy:0.4039, Validation Loss:1.0714, Validation Accuracy:0.4064\n",
    "Epoch #224: Loss:1.0704, Accuracy:0.4039, Validation Loss:1.0715, Validation Accuracy:0.4064\n",
    "Epoch #225: Loss:1.0705, Accuracy:0.4037, Validation Loss:1.0717, Validation Accuracy:0.4048\n",
    "Epoch #226: Loss:1.0703, Accuracy:0.4036, Validation Loss:1.0717, Validation Accuracy:0.4068\n",
    "Epoch #227: Loss:1.0713, Accuracy:0.3988, Validation Loss:1.0724, Validation Accuracy:0.3970\n",
    "Epoch #228: Loss:1.0708, Accuracy:0.3986, Validation Loss:1.0716, Validation Accuracy:0.4011\n",
    "Epoch #229: Loss:1.0707, Accuracy:0.4035, Validation Loss:1.0715, Validation Accuracy:0.4068\n",
    "Epoch #230: Loss:1.0707, Accuracy:0.4018, Validation Loss:1.0717, Validation Accuracy:0.4019\n",
    "Epoch #231: Loss:1.0704, Accuracy:0.4013, Validation Loss:1.0716, Validation Accuracy:0.4039\n",
    "Epoch #232: Loss:1.0705, Accuracy:0.4027, Validation Loss:1.0716, Validation Accuracy:0.4068\n",
    "Epoch #233: Loss:1.0703, Accuracy:0.4041, Validation Loss:1.0713, Validation Accuracy:0.4064\n",
    "Epoch #234: Loss:1.0706, Accuracy:0.4039, Validation Loss:1.0713, Validation Accuracy:0.4068\n",
    "Epoch #235: Loss:1.0703, Accuracy:0.4042, Validation Loss:1.0718, Validation Accuracy:0.4068\n",
    "Epoch #236: Loss:1.0708, Accuracy:0.4026, Validation Loss:1.0720, Validation Accuracy:0.4048\n",
    "Epoch #237: Loss:1.0705, Accuracy:0.4022, Validation Loss:1.0716, Validation Accuracy:0.4064\n",
    "Epoch #238: Loss:1.0706, Accuracy:0.4038, Validation Loss:1.0715, Validation Accuracy:0.4064\n",
    "Epoch #239: Loss:1.0704, Accuracy:0.4031, Validation Loss:1.0716, Validation Accuracy:0.4007\n",
    "Epoch #240: Loss:1.0710, Accuracy:0.4002, Validation Loss:1.0716, Validation Accuracy:0.4007\n",
    "Epoch #241: Loss:1.0713, Accuracy:0.4023, Validation Loss:1.0718, Validation Accuracy:0.4048\n",
    "Epoch #242: Loss:1.0703, Accuracy:0.4027, Validation Loss:1.0720, Validation Accuracy:0.4048\n",
    "Epoch #243: Loss:1.0708, Accuracy:0.4027, Validation Loss:1.0717, Validation Accuracy:0.4048\n",
    "Epoch #244: Loss:1.0702, Accuracy:0.4027, Validation Loss:1.0716, Validation Accuracy:0.4048\n",
    "Epoch #245: Loss:1.0707, Accuracy:0.4027, Validation Loss:1.0718, Validation Accuracy:0.4048\n",
    "Epoch #246: Loss:1.0704, Accuracy:0.4038, Validation Loss:1.0716, Validation Accuracy:0.4068\n",
    "Epoch #247: Loss:1.0702, Accuracy:0.4042, Validation Loss:1.0714, Validation Accuracy:0.4064\n",
    "Epoch #248: Loss:1.0705, Accuracy:0.4039, Validation Loss:1.0715, Validation Accuracy:0.4064\n",
    "Epoch #249: Loss:1.0704, Accuracy:0.4039, Validation Loss:1.0713, Validation Accuracy:0.4064\n",
    "Epoch #250: Loss:1.0708, Accuracy:0.4031, Validation Loss:1.0714, Validation Accuracy:0.4056\n",
    "Epoch #251: Loss:1.0705, Accuracy:0.4041, Validation Loss:1.0713, Validation Accuracy:0.4068\n",
    "Epoch #252: Loss:1.0704, Accuracy:0.4017, Validation Loss:1.0713, Validation Accuracy:0.4035\n",
    "Epoch #253: Loss:1.0703, Accuracy:0.4031, Validation Loss:1.0713, Validation Accuracy:0.4064\n",
    "Epoch #254: Loss:1.0702, Accuracy:0.4039, Validation Loss:1.0712, Validation Accuracy:0.4064\n",
    "Epoch #255: Loss:1.0701, Accuracy:0.4039, Validation Loss:1.0713, Validation Accuracy:0.4035\n",
    "Epoch #256: Loss:1.0702, Accuracy:0.4036, Validation Loss:1.0714, Validation Accuracy:0.3990\n",
    "Epoch #257: Loss:1.0702, Accuracy:0.3999, Validation Loss:1.0713, Validation Accuracy:0.4035\n",
    "Epoch #258: Loss:1.0704, Accuracy:0.4030, Validation Loss:1.0712, Validation Accuracy:0.4064\n",
    "Epoch #259: Loss:1.0705, Accuracy:0.4041, Validation Loss:1.0717, Validation Accuracy:0.4007\n",
    "Epoch #260: Loss:1.0705, Accuracy:0.4020, Validation Loss:1.0717, Validation Accuracy:0.4052\n",
    "Epoch #261: Loss:1.0705, Accuracy:0.4041, Validation Loss:1.0716, Validation Accuracy:0.4068\n",
    "Epoch #262: Loss:1.0702, Accuracy:0.4039, Validation Loss:1.0713, Validation Accuracy:0.4064\n",
    "Epoch #263: Loss:1.0703, Accuracy:0.4039, Validation Loss:1.0716, Validation Accuracy:0.4064\n",
    "Epoch #264: Loss:1.0705, Accuracy:0.4021, Validation Loss:1.0716, Validation Accuracy:0.4039\n",
    "Epoch #265: Loss:1.0704, Accuracy:0.4041, Validation Loss:1.0720, Validation Accuracy:0.4056\n",
    "Epoch #266: Loss:1.0705, Accuracy:0.4038, Validation Loss:1.0715, Validation Accuracy:0.4064\n",
    "Epoch #267: Loss:1.0701, Accuracy:0.4039, Validation Loss:1.0711, Validation Accuracy:0.4064\n",
    "Epoch #268: Loss:1.0706, Accuracy:0.4036, Validation Loss:1.0716, Validation Accuracy:0.4068\n",
    "Epoch #269: Loss:1.0704, Accuracy:0.4028, Validation Loss:1.0713, Validation Accuracy:0.4068\n",
    "Epoch #270: Loss:1.0713, Accuracy:0.4023, Validation Loss:1.0714, Validation Accuracy:0.4068\n",
    "Epoch #271: Loss:1.0706, Accuracy:0.3995, Validation Loss:1.0720, Validation Accuracy:0.4035\n",
    "Epoch #272: Loss:1.0704, Accuracy:0.4042, Validation Loss:1.0719, Validation Accuracy:0.3986\n",
    "Epoch #273: Loss:1.0703, Accuracy:0.4011, Validation Loss:1.0718, Validation Accuracy:0.4023\n",
    "Epoch #274: Loss:1.0706, Accuracy:0.4013, Validation Loss:1.0713, Validation Accuracy:0.4064\n",
    "Epoch #275: Loss:1.0701, Accuracy:0.4040, Validation Loss:1.0711, Validation Accuracy:0.4056\n",
    "Epoch #276: Loss:1.0703, Accuracy:0.4041, Validation Loss:1.0713, Validation Accuracy:0.4064\n",
    "Epoch #277: Loss:1.0703, Accuracy:0.4040, Validation Loss:1.0713, Validation Accuracy:0.4068\n",
    "Epoch #278: Loss:1.0707, Accuracy:0.4040, Validation Loss:1.0716, Validation Accuracy:0.4064\n",
    "Epoch #279: Loss:1.0701, Accuracy:0.4039, Validation Loss:1.0712, Validation Accuracy:0.4064\n",
    "Epoch #280: Loss:1.0702, Accuracy:0.4038, Validation Loss:1.0714, Validation Accuracy:0.4064\n",
    "Epoch #281: Loss:1.0703, Accuracy:0.4042, Validation Loss:1.0713, Validation Accuracy:0.4068\n",
    "Epoch #282: Loss:1.0705, Accuracy:0.4044, Validation Loss:1.0719, Validation Accuracy:0.4068\n",
    "Epoch #283: Loss:1.0701, Accuracy:0.4036, Validation Loss:1.0714, Validation Accuracy:0.4068\n",
    "Epoch #284: Loss:1.0704, Accuracy:0.4042, Validation Loss:1.0714, Validation Accuracy:0.4064\n",
    "Epoch #285: Loss:1.0702, Accuracy:0.4039, Validation Loss:1.0718, Validation Accuracy:0.4064\n",
    "Epoch #286: Loss:1.0703, Accuracy:0.4042, Validation Loss:1.0719, Validation Accuracy:0.4068\n",
    "Epoch #287: Loss:1.0702, Accuracy:0.4028, Validation Loss:1.0717, Validation Accuracy:0.4064\n",
    "Epoch #288: Loss:1.0705, Accuracy:0.4041, Validation Loss:1.0715, Validation Accuracy:0.4002\n",
    "Epoch #289: Loss:1.0701, Accuracy:0.4034, Validation Loss:1.0718, Validation Accuracy:0.4064\n",
    "Epoch #290: Loss:1.0704, Accuracy:0.4039, Validation Loss:1.0718, Validation Accuracy:0.4064\n",
    "Epoch #291: Loss:1.0704, Accuracy:0.4014, Validation Loss:1.0716, Validation Accuracy:0.4056\n",
    "Epoch #292: Loss:1.0700, Accuracy:0.4039, Validation Loss:1.0717, Validation Accuracy:0.4064\n",
    "Epoch #293: Loss:1.0706, Accuracy:0.4039, Validation Loss:1.0721, Validation Accuracy:0.4064\n",
    "Epoch #294: Loss:1.0711, Accuracy:0.4039, Validation Loss:1.0720, Validation Accuracy:0.4064\n",
    "Epoch #295: Loss:1.0702, Accuracy:0.4039, Validation Loss:1.0720, Validation Accuracy:0.4064\n",
    "Epoch #296: Loss:1.0704, Accuracy:0.4039, Validation Loss:1.0717, Validation Accuracy:0.4064\n",
    "Epoch #297: Loss:1.0702, Accuracy:0.4039, Validation Loss:1.0714, Validation Accuracy:0.4064\n",
    "Epoch #298: Loss:1.0705, Accuracy:0.4040, Validation Loss:1.0715, Validation Accuracy:0.4068\n",
    "Epoch #299: Loss:1.0701, Accuracy:0.4039, Validation Loss:1.0715, Validation Accuracy:0.4064\n",
    "Epoch #300: Loss:1.0706, Accuracy:0.4041, Validation Loss:1.0713, Validation Accuracy:0.4068\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07125854, Accuracy:0.4068\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  786  174   0\n",
    "t:02  703  205   0\n",
    "t:03  471   97   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.40      0.82      0.54       960\n",
    "          02       0.43      0.23      0.30       908\n",
    "          03       0.00      0.00      0.00       568\n",
    "\n",
    "    accuracy                           0.41      2436\n",
    "   macro avg       0.28      0.35      0.28      2436\n",
    "weighted avg       0.32      0.41      0.32      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 19:04:33 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 17 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0750958972376556, 1.0744222844958502, 1.0738434480328864, 1.0737869144464753, 1.0742580086139624, 1.0740995414934331, 1.0747183847114175, 1.0744745459266876, 1.074492886540142, 1.0744022962886517, 1.0741891759174016, 1.0740897586975975, 1.0740573670476528, 1.074301613962709, 1.0740086920742917, 1.0742265477360566, 1.073978801861968, 1.0740191274871576, 1.0738874703205277, 1.0737562946889592, 1.0736967354572464, 1.07360850551054, 1.07377898242869, 1.0737722681465212, 1.0738614386525647, 1.0738080254524995, 1.0737206393666259, 1.0740624531149277, 1.073985224286911, 1.0740617627189273, 1.0741348615029371, 1.0741171686128639, 1.0739656359887084, 1.0739993328727133, 1.0739603486945868, 1.0740866234345585, 1.074166491505352, 1.0739839879554285, 1.0739858583080748, 1.0739189783732097, 1.0744404217292522, 1.0738838772076886, 1.074082744532618, 1.0738718464652501, 1.0738428492459953, 1.0742178867603172, 1.0735221870231315, 1.0737488958831687, 1.0735208999934456, 1.0734194679604767, 1.0735292013838569, 1.0734190332087, 1.073497747161314, 1.0734874806772117, 1.0735651918232734, 1.0735994722259847, 1.07347917008674, 1.0733214507157776, 1.0731878405917064, 1.0735652323426872, 1.0734145743114802, 1.0735534936532207, 1.073416593626802, 1.073549049632694, 1.073372868090036, 1.0734912607274423, 1.0735503211984494, 1.073541873194314, 1.0730979538511956, 1.0730266054275588, 1.0731303588118655, 1.0734169678930774, 1.073145343948076, 1.072977598860542, 1.0735197102495015, 1.0732400499541184, 1.0732594111870075, 1.0732551750486903, 1.0731167779571709, 1.0729252908421658, 1.0732679517789818, 1.0732420960866367, 1.0729524768240541, 1.0732591150233703, 1.0731215784310904, 1.0734533312285475, 1.0731751470534476, 1.07317286348108, 1.0731847804951158, 1.0731320246099838, 1.0730336961292086, 1.073020427293574, 1.0729833733663574, 1.0729745957260257, 1.0732178265238044, 1.0728803786933911, 1.0728498380172429, 1.07349567029668, 1.073115758903704, 1.0731351328600804, 1.072966218386182, 1.0735024294046738, 1.0732360958857294, 1.0728629152175828, 1.072865181564306, 1.0728223151565577, 1.0728591784272092, 1.072998749015758, 1.07291867913088, 1.0728242225051905, 1.072859115201264, 1.0731197092529197, 1.0728258720563941, 1.073243460631723, 1.072713296010185, 1.0729192428792442, 1.0730931316494745, 1.0732295920304673, 1.0728702919040798, 1.0728542628546653, 1.0725406698013957, 1.0726027265558102, 1.0726123394441527, 1.0728705121182847, 1.0731195835840135, 1.072902958772844, 1.0725749613616267, 1.0726209088107832, 1.072529766751432, 1.0724145739732314, 1.0726127980769367, 1.0728173222643598, 1.0725453383425383, 1.0723215020544619, 1.07220691060785, 1.0718637405357925, 1.0719775940201357, 1.0719704418542546, 1.0715855731948452, 1.0718915519260226, 1.0722338767670254, 1.0722288484448086, 1.071642654674198, 1.0718257566195208, 1.071852280392827, 1.0717213741076992, 1.0720192103941844, 1.0721909682738957, 1.0722026805376577, 1.0719807837005515, 1.0718587482308324, 1.0718750082604795, 1.0714653701030563, 1.0716245628538585, 1.0717363314479833, 1.0720083006888579, 1.0726405548540439, 1.0720077962515193, 1.0718535948269472, 1.0721970050792975, 1.0719395349374155, 1.0717988433117545, 1.071663922864228, 1.0716645535577107, 1.0715787751334054, 1.0719654425024399, 1.0715828562409224, 1.0719383493041366, 1.072305649959395, 1.0720127832713386, 1.0716188627315077, 1.0713795341294388, 1.0714691920429225, 1.0715460908432508, 1.0715148086814066, 1.071754715712787, 1.0720261525246506, 1.0718310129857807, 1.0721069443206286, 1.0718872118466005, 1.071708385776025, 1.0721409543981693, 1.0714860904001446, 1.0720668543735747, 1.0718038957107243, 1.071536080003372, 1.0715300408490185, 1.071935338339782, 1.0715698751518488, 1.072161048895424, 1.0720130997925557, 1.0716170148896467, 1.071882919529193, 1.0721141896615867, 1.0718614473718728, 1.0714510140943605, 1.0714770958732893, 1.071568991554586, 1.0715080466372235, 1.0718925707837434, 1.0719610648397937, 1.0718737411968813, 1.0717438242118347, 1.0715223349178171, 1.0719197045014606, 1.071620923936465, 1.0717713278894159, 1.071811927950441, 1.0714126907545944, 1.0718335625769078, 1.0722189765845613, 1.0715690275718426, 1.0714200220280288, 1.0713511768037267, 1.0713949869027475, 1.0714666119154255, 1.0713946987646945, 1.0712499951298404, 1.0715665598025268, 1.071517137471091, 1.071432347172391, 1.0713912767338243, 1.0713851308979228, 1.071525953086139, 1.0717398499816118, 1.0716952287113333, 1.0723998057235442, 1.071642805790079, 1.0714889246058974, 1.071651836334191, 1.0716000094593843, 1.07163367384956, 1.0712589416989355, 1.0713142093962245, 1.0717691829051879, 1.0720440327436074, 1.0715843574166886, 1.0715434701963402, 1.0715882637230634, 1.0716466666833913, 1.07175474996833, 1.0720387491686592, 1.0717391773984937, 1.0715948431362659, 1.0717719583871526, 1.0715984773557565, 1.0714288501708182, 1.0714926085448617, 1.0712895199583081, 1.071422635040847, 1.071313524872603, 1.0713020731467136, 1.0712792423166861, 1.0712143450926481, 1.0713061313519532, 1.0714329500699473, 1.0713478925780122, 1.0712093148129718, 1.0716705741161978, 1.0717073849269323, 1.071580717716311, 1.0712935803167534, 1.0716027017493162, 1.0716221547870606, 1.0719886887053942, 1.0714873864341448, 1.0711092438016618, 1.0716204349630571, 1.0713265024382492, 1.0713865044473232, 1.071988416814256, 1.0718716508257762, 1.0717590520730356, 1.0713215663123796, 1.0710963785941965, 1.0712820553818752, 1.0713117897804147, 1.0715608222926976, 1.0712360511663903, 1.0714011035725008, 1.0712591390108632, 1.0718564215943536, 1.0714436017820987, 1.071374857366966, 1.0717763383987502, 1.0718921939727708, 1.071654809911067, 1.0714739756826892, 1.0717600239517262, 1.0717532454648824, 1.0715583881916866, 1.0717000008020887, 1.0721470062760101, 1.0720012755621047, 1.0720431518867881, 1.071653464157593, 1.07141031302842, 1.071458169979415, 1.0714771677120565, 1.071258691339853], 'val_acc': [0.3760262735567265, 0.3940886690698821, 0.39244663534297536, 0.40106732504708426, 0.3940886690698821, 0.3940886690698821, 0.3940886690698821, 0.3940886690698821, 0.3940886690698821, 0.39532019861030265, 0.3940886690698821, 0.3940886690698821, 0.39326765100748473, 0.3940886690698821, 0.3940886690698821, 0.3969622323372094, 0.3940886690698821, 0.3940886690698821, 0.39860426855987713, 0.39532019861030265, 0.39532019861030265, 0.39490968718121594, 0.40106732514495724, 0.39532019861030265, 0.39285714437417407, 0.39244663534297536, 0.396141214274812, 0.40106732514495724, 0.3940886691677551, 0.3940886690698821, 0.3940886690698821, 0.3940886690698821, 0.3940886690698821, 0.396141214274812, 0.3949096895301675, 0.3940886690698821, 0.39614121662376356, 0.396141214274812, 0.3953201985613662, 0.39367816243657144, 0.3940886690698821, 0.3981937595286784, 0.3981937619265664, 0.39532019861030265, 0.396141214274812, 0.3990147775910758, 0.3986042709577651, 0.3944991781500173, 0.396141214274812, 0.3986042709577651, 0.396141214274812, 0.396141214274812, 0.396141214274812, 0.3953201985613662, 0.39203612391388865, 0.39203612391388865, 0.39532019621241465, 0.4018883408094666, 0.39532019621241465, 0.3957307052436133, 0.3957307052436133, 0.3965517256549622, 0.39532019621241465, 0.39614121662376356, 0.39532019621241465, 0.3940886690698821, 0.39532019621241465, 0.40065681611375853, 0.40065681611375853, 0.39532019621241465, 0.3965517256549622, 0.40024630443998943, 0.3973727437662961, 0.40147783417615596, 0.3961412142258755, 0.3965517233060107, 0.3936781600876199, 0.39532019861030265, 0.3944991781500173, 0.39532019621241465, 0.39614121398119306, 0.3936781600876199, 0.3965517233060107, 0.39285714432523755, 0.396141214274812, 0.40024630443998943, 0.396141214274812, 0.3981937594308054, 0.40394088836334807, 0.396141214274812, 0.40147783417615596, 0.396141214274812, 0.40106732514495724, 0.40065681611375853, 0.40147783417615596, 0.4022988498406653, 0.4018883432073546, 0.3928571417805401, 0.3961412142258755, 0.3981937594308054, 0.40394088836334807, 0.4006568134711881, 0.4022988498406653, 0.40353037933214936, 0.40353037933214936, 0.39532019621241465, 0.40065681611375853, 0.40188834291373565, 0.4018883432073546, 0.4018883432073546, 0.4018883432073546, 0.40353037933214936, 0.40147783417615596, 0.4006568134711881, 0.4018883432073546, 0.40147783417615596, 0.401477833882537, 0.401477833882537, 0.4018883432073546, 0.40106732514495724, 0.4018883432073546, 0.396141214274812, 0.4018883432073546, 0.4022988498406653, 0.396141214274812, 0.4018883432073546, 0.4022988498406653, 0.4022988498406653, 0.4022988498406653, 0.3973727413194716, 0.40147783153358546, 0.3944991804500323, 0.394909689481231, 0.40147783153358546, 0.3965517253613433, 0.4018883432073546, 0.40147783153358546, 0.402709358871864, 0.4018883432073546, 0.4018883432073546, 0.40065681366693406, 0.394909689481231, 0.4027093612208155, 0.40394088591652355, 0.4018883432073546, 0.4022988498406653, 0.3957307075436284, 0.39532019851242967, 0.4022988498406653, 0.40106732514495724, 0.4018883432073546, 0.4027093612208155, 0.4018883432073546, 0.4022988498406653, 0.4018883432073546, 0.40065681366693406, 0.39532019851242967, 0.4027093612208155, 0.4018883432073546, 0.4022988498406653, 0.40476190642574544, 0.4018883432073546, 0.4018883432073546, 0.4018883432073546, 0.4018883432073546, 0.3986042709577651, 0.40394088836334807, 0.40270935862718155, 0.3973727434237407, 0.3981937617308205, 0.40681445158173885, 0.40476190642574544, 0.40394088836334807, 0.4018883432073546, 0.40476190642574544, 0.4059934335193415, 0.40476190642574544, 0.40353037693426136, 0.4010673226981327, 0.40065681366693406, 0.4018883432073546, 0.4018883432073546, 0.4018883432073546, 0.40640394015265213, 0.40188834315841815, 0.40147783412721944, 0.40640394015265213, 0.40394088836334807, 0.4018883432073546, 0.40065681376480705, 0.4018883432073546, 0.4018883432073546, 0.4018883432073546, 0.40394088836334807, 0.4018883432073546, 0.40353037688532484, 0.40681445158173885, 0.40681445158173885, 0.39860427076201915, 0.40065681366693406, 0.40353037688532484, 0.40353037688532484, 0.39860427076201915, 0.40353037688532484, 0.40640394015265213, 0.40640394015265213, 0.40394088836334807, 0.40640394015265213, 0.40394088836334807, 0.40394088836334807, 0.40599343322572257, 0.40640394015265213, 0.40435139734561026, 0.4055829244392063, 0.40353037688532484, 0.40353037688532484, 0.40476190642574544, 0.40476190642574544, 0.40640394015265213, 0.40681445158173885, 0.40394088836334807, 0.40640394015265213, 0.40640394015265213, 0.40640394015265213, 0.40476190642574544, 0.40681445158173885, 0.3969622322882729, 0.4010673249981478, 0.40681445158173885, 0.4018883432073546, 0.40394088836334807, 0.40681444918385085, 0.40640394015265213, 0.40681445158173885, 0.40681445158173885, 0.40476190642574544, 0.40640394015265213, 0.40640394015265213, 0.40065681376480705, 0.40065681376480705, 0.40476190642574544, 0.40476190642574544, 0.40476190642574544, 0.40476190642574544, 0.40476190642574544, 0.40681445158173885, 0.40640394015265213, 0.40640394015265213, 0.40640394015265213, 0.40558292209025476, 0.40681445158173885, 0.40353037688532484, 0.40640394015265213, 0.40640394015265213, 0.40353037688532484, 0.39901477979321787, 0.40353037688532484, 0.40640394015265213, 0.4006568159669491, 0.4051724130590561, 0.40681445158173885, 0.40640394015265213, 0.40640394015265213, 0.40394088836334807, 0.40558292209025476, 0.40640394015265213, 0.40640394015265213, 0.40681445158173885, 0.40681445158173885, 0.40681445158173885, 0.40353037693426136, 0.39860427076201915, 0.4022988498406653, 0.40640394015265213, 0.40558292209025476, 0.40640394015265213, 0.40681445158173885, 0.40640394015265213, 0.40640394015265213, 0.40640394015265213, 0.40681445158173885, 0.40681445158173885, 0.40681445158173885, 0.40640394015265213, 0.40640394015265213, 0.40681445158173885, 0.40640394015265213, 0.4002463069357504, 0.40640394015265213, 0.40640394015265213, 0.40558292209025476, 0.40640394015265213, 0.40640394015265213, 0.40640394015265213, 0.40640394015265213, 0.40640394015265213, 0.40640394015265213, 0.40681445158173885, 0.40640394015265213, 0.40681445158173885], 'loss': [1.0788422208057538, 1.0742930048545039, 1.0742356826637316, 1.0739192992510003, 1.0744186963878373, 1.0747395349479065, 1.0750754868959744, 1.0745754692833527, 1.0745163913624978, 1.0750191513762581, 1.0748206878834436, 1.074270807056701, 1.0742293996732581, 1.074210572438563, 1.0760414568550534, 1.0745948963341527, 1.0744805732546892, 1.0747193663516819, 1.0742232581673217, 1.074484863956851, 1.074301540092766, 1.074025801589112, 1.0743025940791293, 1.0746660750015071, 1.0749342827336743, 1.0742064319352105, 1.0739794298853473, 1.0740382838298164, 1.0739263169084976, 1.0753805260883464, 1.0740775517124905, 1.0741909795228461, 1.0741730691471139, 1.074080024411791, 1.0745664666565538, 1.074222062647465, 1.0739974031458155, 1.0742862749393471, 1.0742835134451394, 1.0740421395282236, 1.0747445720666733, 1.074407874436349, 1.0742127604063532, 1.0739085679181548, 1.0740556331630604, 1.0750129147476728, 1.0741290877242353, 1.073851460744713, 1.0742956256474803, 1.0737582848546932, 1.0739059839405318, 1.0738165330103535, 1.073708845066094, 1.0736357022115093, 1.0739807618716903, 1.0737217104165706, 1.073719081692627, 1.0732304034781408, 1.0741474595647573, 1.0738921851348093, 1.0737748519595889, 1.0737255938488846, 1.0742379123425336, 1.0735682816965624, 1.0735676078580978, 1.073452626655234, 1.073792243982977, 1.073253871772813, 1.0731429842708047, 1.0728900218646384, 1.0730240794912256, 1.0733516614784695, 1.0729196905845, 1.073229885737754, 1.0727469031570873, 1.0735491984678736, 1.0731202926723864, 1.0729934770713352, 1.0731416430071883, 1.0726337639947692, 1.0725947299287548, 1.073133854748532, 1.072534134990136, 1.0731545273528207, 1.072709877584015, 1.073019045970768, 1.0726465817839212, 1.0723220694481226, 1.0731343161643654, 1.0725675139339064, 1.0726119085014234, 1.0723856660130087, 1.0721534924340688, 1.0734502209285446, 1.0723768247471208, 1.0727711754413112, 1.0726292084864277, 1.0747444172904232, 1.0722410492828494, 1.07232735313674, 1.072588820134345, 1.0723538695909158, 1.0730513685293022, 1.0720987212731363, 1.0721807783878805, 1.0719979491811513, 1.072082006270391, 1.0723092586842407, 1.0724345299742306, 1.0721666101067953, 1.0718019018427791, 1.0724860727909409, 1.0721217103072995, 1.0720472162509112, 1.0722940480195031, 1.0727425255080267, 1.0720441709064115, 1.0720790384731254, 1.0719016451120866, 1.0725377330545038, 1.0723030082498977, 1.0718590187830601, 1.0718678217404187, 1.0717860982158591, 1.0718225676910589, 1.0723660731462483, 1.0717271314264567, 1.071540695490044, 1.0718949763437071, 1.0714614074577786, 1.0714365250765665, 1.0721058694733732, 1.0714788053559572, 1.0711359636494755, 1.071349741839775, 1.0710366144806942, 1.0712318563852956, 1.071540747775679, 1.0711361648610485, 1.0712262298536985, 1.0716497184804332, 1.0711640915097151, 1.0712796158859128, 1.0712584182222276, 1.0718204706356511, 1.0713519952135655, 1.0709047658732296, 1.070998798650393, 1.0713487406286126, 1.0711168919745413, 1.0708521833654792, 1.0709046673236198, 1.071008933055572, 1.0707398549487215, 1.0710555822207943, 1.071293135981785, 1.070726058737698, 1.0709711464033969, 1.0708708346012437, 1.0718340136921627, 1.071806370232874, 1.0711412059208205, 1.070766679266395, 1.0708647271690916, 1.0710364027924117, 1.0713481307274506, 1.0709007566224868, 1.0704591938602361, 1.0711902212558095, 1.07138613019391, 1.0708766955369797, 1.0708372065663583, 1.071045540637304, 1.070667509670375, 1.070669005245154, 1.0709734808003388, 1.071054157486197, 1.0708636371751585, 1.0709918356774035, 1.070693362860709, 1.0708069034670413, 1.0714422364744072, 1.0709358872084647, 1.071386384033814, 1.070898410231181, 1.070861075399348, 1.0705706402995994, 1.0704318137629076, 1.0704768254771615, 1.0715439105425528, 1.0716110809627744, 1.0712574681217422, 1.0709407907736619, 1.0709402476003282, 1.0708664603791442, 1.0706505101809023, 1.070349316308141, 1.0707497730392206, 1.070621298715564, 1.0705742688149642, 1.0711602581110333, 1.0706029419047143, 1.070518901118018, 1.0706569898789424, 1.0704846081547668, 1.0703835978400291, 1.0706852832369247, 1.0705271401199716, 1.0705895277508966, 1.070779499968464, 1.0709079018363719, 1.070545782592507, 1.0709138824709632, 1.0702923937744673, 1.0704281679169108, 1.0707870743357915, 1.0709581732994722, 1.0703219261012773, 1.0706958702702296, 1.0706837785317422, 1.0707724953578972, 1.0704465710896487, 1.070340645484611, 1.070405696745526, 1.070453947574451, 1.0702984016289212, 1.0713018126066705, 1.07081777116846, 1.0706522529864457, 1.0706964356208974, 1.0703966660176458, 1.0704638884052848, 1.0703227175824206, 1.070592348923184, 1.0703231437739895, 1.0707642313880843, 1.070459492715722, 1.070571511531023, 1.0703954062667471, 1.0709710552462317, 1.0713158701479557, 1.0702977253915837, 1.0708432418854574, 1.0701734014115538, 1.0706781945434194, 1.0704320272625838, 1.0701858458822513, 1.0704844851758202, 1.070413321048572, 1.070793045717588, 1.0705016922901789, 1.070374399974361, 1.0702617739749885, 1.0702147075038182, 1.0701403527778766, 1.0702478853828854, 1.0701672107532039, 1.070377607805773, 1.0704816549955207, 1.070493351360611, 1.0704508293825499, 1.0701993562357626, 1.0702557323894462, 1.070480380410776, 1.0704473875141731, 1.0705029474391585, 1.0701212384127983, 1.0705677708071606, 1.0704140620554743, 1.0712711295552813, 1.070579571767999, 1.0703573562281332, 1.070302411809839, 1.0706052364999508, 1.0700520177151878, 1.0702731471286906, 1.0703145333139314, 1.070652410528743, 1.0700896248190799, 1.0702371399015862, 1.070253973526142, 1.0704879241802365, 1.0700728802710344, 1.0703835605351097, 1.0701980584945523, 1.070261699463063, 1.0701652262489898, 1.0704598926665603, 1.0701111167852884, 1.0703714092654124, 1.070411044420403, 1.0699783616487006, 1.070596400080765, 1.0710976849591218, 1.0702432962168904, 1.0703698986854397, 1.070188529829225, 1.0705189748955948, 1.0700851556456799, 1.0705628591886047], 'acc': [0.3934291581231221, 0.39065708417667255, 0.39312114988509145, 0.3942505133470226, 0.382340862422998, 0.39425051333478345, 0.3950718685709231, 0.3942505133470226, 0.3942505133470226, 0.3939425051395898, 0.37648870637774223, 0.3943531827637792, 0.3953798768089537, 0.3992813141683778, 0.39425051333478345, 0.393634496932157, 0.3942505133225443, 0.39425051335926176, 0.3996919917986868, 0.3801848049281314, 0.3946611909650924, 0.3973305954580679, 0.39558521561798865, 0.3905544147966334, 0.3941478439425051, 0.3942505133470226, 0.3946611909650924, 0.39722792607802876, 0.3929158110882957, 0.39599589323605844, 0.3932238193018481, 0.3942505133225443, 0.39425051335926176, 0.39291581109441526, 0.379158110895196, 0.3932238193018481, 0.3942505133470226, 0.39435318275154, 0.39609856262833676, 0.39363449691991786, 0.394147843930266, 0.39856262833675565, 0.39681724844771976, 0.3959958932238193, 0.39188911704618096, 0.3972279260902679, 0.3980492813141684, 0.39496919917864476, 0.39342915811088297, 0.40102669404517455, 0.39404517453798765, 0.395277207379958, 0.39435318273930087, 0.39517453798767965, 0.3936344969443962, 0.3935318275398787, 0.3939425051212311, 0.4016427104722793, 0.391273100622136, 0.3995893224064085, 0.39681724847219807, 0.39876796715191015, 0.38511293635720834, 0.39681724847219807, 0.3983572895154816, 0.39466119097733154, 0.3969199178644764, 0.4010266940329354, 0.4008213552239005, 0.4002053388090349, 0.4001026694075772, 0.3941478439425051, 0.39702258724451556, 0.4000000000122391, 0.39609856265281507, 0.3948665297618882, 0.3990759753348646, 0.39537987682119285, 0.3940451745502268, 0.3978439425173726, 0.39691991787671554, 0.38963039013149803, 0.3997946611970847, 0.39794661189741176, 0.39455852156363475, 0.3954825462073516, 0.3976386036716203, 0.3997946612032042, 0.39804928130192924, 0.39958932239416933, 0.39743326488706365, 0.39938398356065613, 0.40051334703482644, 0.3986652977657514, 0.3989733059670646, 0.3979466118851726, 0.4030800821385834, 0.3720739219590134, 0.395893223831541, 0.39702258726899387, 0.3975359342977007, 0.39733059549478533, 0.3742299794661191, 0.4026694044929755, 0.4021560575071057, 0.40236139632837975, 0.4012320328419703, 0.40030800820131324, 0.39640657082965, 0.4011293634619311, 0.4013347022342486, 0.4020533880903491, 0.3990759753593429, 0.39681724843548066, 0.39199178643539945, 0.3998973305832434, 0.40010266940451744, 0.39537987678447545, 0.3984599589077599, 0.40215605747038824, 0.40133470227096607, 0.39476386034513156, 0.4004106776180698, 0.40133470224648776, 0.4009240246406571, 0.39989733057100424, 0.40061601641486555, 0.40102669404517455, 0.3953798767967146, 0.39907597537158207, 0.39948665298965186, 0.39363449691991786, 0.39681724847219807, 0.399589322369691, 0.39794661189741176, 0.39989733057100424, 0.4001026693922783, 0.39517453798767965, 0.4013347022587269, 0.40112936345581157, 0.40174537986455755, 0.3987679671580297, 0.39804928130192924, 0.40133470228320517, 0.402361396310021, 0.4015400410677618, 0.40184804929355333, 0.3985626283428752, 0.39537987678447545, 0.40020533879679576, 0.40082135524837875, 0.40205338806587076, 0.4025667351251755, 0.4014373716387661, 0.40143737166936394, 0.3997946611787259, 0.39989733057100424, 0.39958932235745187, 0.4023613963161406, 0.3966119096264457, 0.4013347022342486, 0.40184804929355333, 0.4013347022342486, 0.40133470224648776, 0.40246406571453847, 0.3921971252566735, 0.4010266940329354, 0.4025667351159961, 0.39650924023416745, 0.3991786447393821, 0.3990759753348646, 0.40277207392197123, 0.40308008211104535, 0.4017453798829163, 0.4013347022587269, 0.40369609856262834, 0.40441478440648965, 0.4031827515278019, 0.4003080081890741, 0.400513346998109, 0.401848049269075, 0.39342915808640466, 0.40154004105552266, 0.4, 0.4021560575071057, 0.40041067760583066, 0.40277207390973213, 0.40308008211104535, 0.40184804929355333, 0.40061601643934386, 0.39199178645987776, 0.4013347022587269, 0.40123203286032905, 0.40061601643934386, 0.40184804925683587, 0.40215605749486655, 0.4021560575193448, 0.40297741273712573, 0.40195071869807075, 0.39969199178644765, 0.40164271046004013, 0.40164271046004013, 0.3994866529774127, 0.4009240246406571, 0.4025667351006972, 0.403901437374723, 0.40225872689938397, 0.40379876797938496, 0.4020533881025882, 0.4018480492813142, 0.3994866529835323, 0.4040041067761807, 0.4036960985748675, 0.40379876795490666, 0.4018480492813142, 0.4017453798767967, 0.40359342915811086, 0.4026694044929755, 0.4039014373594241, 0.40400410678841986, 0.4027720739342104, 0.40359342914587176, 0.403901437347185, 0.403901437347185, 0.4036960985748675, 0.40359342915811086, 0.39876796714579055, 0.39856262836123396, 0.403490759759713, 0.4018480492813142, 0.40133470227096607, 0.4026694045174538, 0.404106776168459, 0.40390143737778283, 0.40420944559745475, 0.40256673511905594, 0.40215605749486655, 0.40379876797326536, 0.40308008212328444, 0.40020533882127407, 0.40225872689938397, 0.40266940450521466, 0.40266940452969296, 0.4026694044929755, 0.4026694045174538, 0.4037987679671458, 0.4042094455852156, 0.4039014373839024, 0.40390143737166323, 0.4030800821355236, 0.4041067761806982, 0.4017453798767967, 0.4030800821355236, 0.4039014373839024, 0.4039014373839024, 0.40359342914587176, 0.3998973306077217, 0.40297741274324533, 0.4041067761806982, 0.40195071869807075, 0.4041067761806982, 0.40390143739614154, 0.4039014373839024, 0.4020533880781099, 0.40410677620517643, 0.40379876797938496, 0.40390143737166323, 0.40359342915811086, 0.40277207392197123, 0.4022587269116231, 0.39948665295293445, 0.40420944557297644, 0.40112936344969197, 0.40133470224648776, 0.4040041067761807, 0.40410677615621987, 0.40400410678841986, 0.4040041067517024, 0.403901437347185, 0.4037987679426675, 0.4042094455852156, 0.4044147843820114, 0.40359342914587176, 0.40420944557297644, 0.40390143737166323, 0.4042094455852156, 0.40277207390973213, 0.4041067761806982, 0.4033880903368368, 0.4039014373594241, 0.4014373716510052, 0.40390143737166323, 0.4039014373839024, 0.403901437347185, 0.403901437347185, 0.403901437347185, 0.4039014373594241, 0.4040041067761807, 0.4039014373594241, 0.4041067761929373]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
