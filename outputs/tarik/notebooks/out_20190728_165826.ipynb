{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf63.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 16:58:26 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'All', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000025B82F99550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000025BD4447EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0957, Accuracy:0.3852, Validation Loss:1.0880, Validation Accuracy:0.3760\n",
    "Epoch #2: Loss:1.0826, Accuracy:0.3963, Validation Loss:1.0787, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0760, Accuracy:0.3951, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0742, Accuracy:0.3975, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0742, Accuracy:0.3963, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0748, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.3957\n",
    "Epoch #7: Loss:1.0746, Accuracy:0.3906, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #9: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #10: Loss:1.0739, Accuracy:0.3963, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.3975, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #12: Loss:1.0739, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #13: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.4105\n",
    "Epoch #14: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #15: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #16: Loss:1.0738, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #17: Loss:1.0738, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #18: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #19: Loss:1.0737, Accuracy:0.3922, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #20: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0742, Validation Accuracy:0.4072\n",
    "Epoch #21: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #22: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0740, Validation Accuracy:0.4089\n",
    "Epoch #23: Loss:1.0734, Accuracy:0.3996, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #24: Loss:1.0734, Accuracy:0.3938, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #25: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #26: Loss:1.0732, Accuracy:0.4004, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #27: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #28: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #29: Loss:1.0729, Accuracy:0.3963, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #30: Loss:1.0729, Accuracy:0.4053, Validation Loss:1.0736, Validation Accuracy:0.4105\n",
    "Epoch #31: Loss:1.0729, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.3990\n",
    "Epoch #32: Loss:1.0728, Accuracy:0.3971, Validation Loss:1.0736, Validation Accuracy:0.4023\n",
    "Epoch #33: Loss:1.0730, Accuracy:0.3967, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0731, Accuracy:0.3889, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #35: Loss:1.0728, Accuracy:0.4037, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #36: Loss:1.0731, Accuracy:0.4037, Validation Loss:1.0731, Validation Accuracy:0.3974\n",
    "Epoch #37: Loss:1.0722, Accuracy:0.4103, Validation Loss:1.0732, Validation Accuracy:0.3810\n",
    "Epoch #38: Loss:1.0724, Accuracy:0.4004, Validation Loss:1.0729, Validation Accuracy:0.3875\n",
    "Epoch #39: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.4007\n",
    "Epoch #40: Loss:1.0723, Accuracy:0.4049, Validation Loss:1.0729, Validation Accuracy:0.3924\n",
    "Epoch #41: Loss:1.0726, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #42: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #43: Loss:1.0727, Accuracy:0.4000, Validation Loss:1.0758, Validation Accuracy:0.3777\n",
    "Epoch #44: Loss:1.0738, Accuracy:0.3979, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #45: Loss:1.0728, Accuracy:0.3984, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #46: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #47: Loss:1.0729, Accuracy:0.4000, Validation Loss:1.0785, Validation Accuracy:0.4023\n",
    "Epoch #48: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #49: Loss:1.0736, Accuracy:0.3918, Validation Loss:1.0760, Validation Accuracy:0.3859\n",
    "Epoch #50: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #51: Loss:1.0728, Accuracy:0.3988, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #52: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #53: Loss:1.0727, Accuracy:0.3967, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #54: Loss:1.0727, Accuracy:0.3947, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #55: Loss:1.0729, Accuracy:0.3938, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0722, Accuracy:0.4053, Validation Loss:1.0745, Validation Accuracy:0.4039\n",
    "Epoch #57: Loss:1.0721, Accuracy:0.4033, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #58: Loss:1.0722, Accuracy:0.3984, Validation Loss:1.0746, Validation Accuracy:0.4072\n",
    "Epoch #59: Loss:1.0715, Accuracy:0.4029, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0716, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.4056\n",
    "Epoch #61: Loss:1.0715, Accuracy:0.4057, Validation Loss:1.0750, Validation Accuracy:0.4023\n",
    "Epoch #62: Loss:1.0716, Accuracy:0.4053, Validation Loss:1.0754, Validation Accuracy:0.4039\n",
    "Epoch #63: Loss:1.0714, Accuracy:0.4012, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #64: Loss:1.0716, Accuracy:0.4086, Validation Loss:1.0754, Validation Accuracy:0.3990\n",
    "Epoch #65: Loss:1.0718, Accuracy:0.3926, Validation Loss:1.0760, Validation Accuracy:0.3908\n",
    "Epoch #66: Loss:1.0712, Accuracy:0.4066, Validation Loss:1.0764, Validation Accuracy:0.3875\n",
    "Epoch #67: Loss:1.0718, Accuracy:0.4033, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #68: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #69: Loss:1.0711, Accuracy:0.4062, Validation Loss:1.0761, Validation Accuracy:0.3908\n",
    "Epoch #70: Loss:1.0709, Accuracy:0.4045, Validation Loss:1.0762, Validation Accuracy:0.3974\n",
    "Epoch #71: Loss:1.0705, Accuracy:0.4025, Validation Loss:1.0763, Validation Accuracy:0.3990\n",
    "Epoch #72: Loss:1.0704, Accuracy:0.3963, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #73: Loss:1.0706, Accuracy:0.4086, Validation Loss:1.0768, Validation Accuracy:0.3941\n",
    "Epoch #74: Loss:1.0707, Accuracy:0.4008, Validation Loss:1.0767, Validation Accuracy:0.3941\n",
    "Epoch #75: Loss:1.0706, Accuracy:0.4057, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0710, Accuracy:0.4057, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #77: Loss:1.0707, Accuracy:0.3996, Validation Loss:1.0773, Validation Accuracy:0.3892\n",
    "Epoch #78: Loss:1.0703, Accuracy:0.4045, Validation Loss:1.0774, Validation Accuracy:0.3990\n",
    "Epoch #79: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0775, Validation Accuracy:0.4007\n",
    "Epoch #80: Loss:1.0712, Accuracy:0.4000, Validation Loss:1.0778, Validation Accuracy:0.3908\n",
    "Epoch #81: Loss:1.0710, Accuracy:0.4021, Validation Loss:1.0782, Validation Accuracy:0.3892\n",
    "Epoch #82: Loss:1.0706, Accuracy:0.4016, Validation Loss:1.0771, Validation Accuracy:0.3941\n",
    "Epoch #83: Loss:1.0719, Accuracy:0.4049, Validation Loss:1.0766, Validation Accuracy:0.4072\n",
    "Epoch #84: Loss:1.0717, Accuracy:0.4012, Validation Loss:1.0764, Validation Accuracy:0.4072\n",
    "Epoch #85: Loss:1.0713, Accuracy:0.3996, Validation Loss:1.0764, Validation Accuracy:0.3990\n",
    "Epoch #86: Loss:1.0718, Accuracy:0.3996, Validation Loss:1.0767, Validation Accuracy:0.4023\n",
    "Epoch #87: Loss:1.0719, Accuracy:0.4062, Validation Loss:1.0771, Validation Accuracy:0.3941\n",
    "Epoch #88: Loss:1.0725, Accuracy:0.3947, Validation Loss:1.0766, Validation Accuracy:0.3941\n",
    "Epoch #89: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0772, Validation Accuracy:0.3974\n",
    "Epoch #90: Loss:1.0723, Accuracy:0.4016, Validation Loss:1.0768, Validation Accuracy:0.3941\n",
    "Epoch #91: Loss:1.0721, Accuracy:0.3959, Validation Loss:1.0769, Validation Accuracy:0.4007\n",
    "Epoch #92: Loss:1.0714, Accuracy:0.3975, Validation Loss:1.0762, Validation Accuracy:0.4072\n",
    "Epoch #93: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0760, Validation Accuracy:0.4072\n",
    "Epoch #94: Loss:1.0716, Accuracy:0.3996, Validation Loss:1.0759, Validation Accuracy:0.4072\n",
    "Epoch #95: Loss:1.0712, Accuracy:0.4004, Validation Loss:1.0762, Validation Accuracy:0.4039\n",
    "Epoch #96: Loss:1.0716, Accuracy:0.3996, Validation Loss:1.0755, Validation Accuracy:0.4072\n",
    "Epoch #97: Loss:1.0720, Accuracy:0.3959, Validation Loss:1.0756, Validation Accuracy:0.4023\n",
    "Epoch #98: Loss:1.0719, Accuracy:0.4025, Validation Loss:1.0759, Validation Accuracy:0.4105\n",
    "Epoch #99: Loss:1.0721, Accuracy:0.4000, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #100: Loss:1.0725, Accuracy:0.4070, Validation Loss:1.0758, Validation Accuracy:0.3974\n",
    "Epoch #101: Loss:1.0713, Accuracy:0.4008, Validation Loss:1.0763, Validation Accuracy:0.3859\n",
    "Epoch #102: Loss:1.0719, Accuracy:0.3967, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #103: Loss:1.0719, Accuracy:0.4099, Validation Loss:1.0766, Validation Accuracy:0.3974\n",
    "Epoch #104: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0764, Validation Accuracy:0.3990\n",
    "Epoch #105: Loss:1.0720, Accuracy:0.3975, Validation Loss:1.0763, Validation Accuracy:0.4089\n",
    "Epoch #106: Loss:1.0713, Accuracy:0.4000, Validation Loss:1.0761, Validation Accuracy:0.3974\n",
    "Epoch #107: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0760, Validation Accuracy:0.4105\n",
    "Epoch #108: Loss:1.0712, Accuracy:0.3996, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #109: Loss:1.0713, Accuracy:0.3975, Validation Loss:1.0761, Validation Accuracy:0.4072\n",
    "Epoch #110: Loss:1.0711, Accuracy:0.4029, Validation Loss:1.0763, Validation Accuracy:0.3974\n",
    "Epoch #111: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0762, Validation Accuracy:0.4072\n",
    "Epoch #112: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0763, Validation Accuracy:0.3924\n",
    "Epoch #113: Loss:1.0708, Accuracy:0.4016, Validation Loss:1.0762, Validation Accuracy:0.4089\n",
    "Epoch #114: Loss:1.0719, Accuracy:0.4041, Validation Loss:1.0764, Validation Accuracy:0.3974\n",
    "Epoch #115: Loss:1.0706, Accuracy:0.4029, Validation Loss:1.0769, Validation Accuracy:0.3924\n",
    "Epoch #116: Loss:1.0714, Accuracy:0.3984, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #117: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0760, Validation Accuracy:0.4072\n",
    "Epoch #118: Loss:1.0707, Accuracy:0.4041, Validation Loss:1.0761, Validation Accuracy:0.4072\n",
    "Epoch #119: Loss:1.0708, Accuracy:0.4029, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #120: Loss:1.0712, Accuracy:0.4066, Validation Loss:1.0763, Validation Accuracy:0.4105\n",
    "Epoch #121: Loss:1.0705, Accuracy:0.4021, Validation Loss:1.0762, Validation Accuracy:0.3974\n",
    "Epoch #122: Loss:1.0710, Accuracy:0.4066, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #123: Loss:1.0706, Accuracy:0.4045, Validation Loss:1.0761, Validation Accuracy:0.4072\n",
    "Epoch #124: Loss:1.0707, Accuracy:0.4066, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #125: Loss:1.0710, Accuracy:0.4074, Validation Loss:1.0763, Validation Accuracy:0.4089\n",
    "Epoch #126: Loss:1.0707, Accuracy:0.4021, Validation Loss:1.0770, Validation Accuracy:0.3990\n",
    "Epoch #127: Loss:1.0708, Accuracy:0.4041, Validation Loss:1.0764, Validation Accuracy:0.3974\n",
    "Epoch #128: Loss:1.0709, Accuracy:0.4049, Validation Loss:1.0762, Validation Accuracy:0.3974\n",
    "Epoch #129: Loss:1.0708, Accuracy:0.4049, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #130: Loss:1.0708, Accuracy:0.4082, Validation Loss:1.0766, Validation Accuracy:0.3924\n",
    "Epoch #131: Loss:1.0706, Accuracy:0.4041, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #132: Loss:1.0705, Accuracy:0.4045, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #133: Loss:1.0704, Accuracy:0.4074, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #134: Loss:1.0704, Accuracy:0.3996, Validation Loss:1.0762, Validation Accuracy:0.3974\n",
    "Epoch #135: Loss:1.0704, Accuracy:0.4049, Validation Loss:1.0761, Validation Accuracy:0.4007\n",
    "Epoch #136: Loss:1.0704, Accuracy:0.4053, Validation Loss:1.0763, Validation Accuracy:0.4072\n",
    "Epoch #137: Loss:1.0703, Accuracy:0.4082, Validation Loss:1.0763, Validation Accuracy:0.4072\n",
    "Epoch #138: Loss:1.0703, Accuracy:0.4037, Validation Loss:1.0763, Validation Accuracy:0.4056\n",
    "Epoch #139: Loss:1.0705, Accuracy:0.4062, Validation Loss:1.0764, Validation Accuracy:0.4089\n",
    "Epoch #140: Loss:1.0719, Accuracy:0.3984, Validation Loss:1.0766, Validation Accuracy:0.3892\n",
    "Epoch #141: Loss:1.0709, Accuracy:0.4041, Validation Loss:1.0764, Validation Accuracy:0.3974\n",
    "Epoch #142: Loss:1.0707, Accuracy:0.4045, Validation Loss:1.0763, Validation Accuracy:0.3957\n",
    "Epoch #143: Loss:1.0703, Accuracy:0.4029, Validation Loss:1.0759, Validation Accuracy:0.4039\n",
    "Epoch #144: Loss:1.0702, Accuracy:0.4025, Validation Loss:1.0758, Validation Accuracy:0.3974\n",
    "Epoch #145: Loss:1.0701, Accuracy:0.4037, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "Epoch #146: Loss:1.0700, Accuracy:0.4062, Validation Loss:1.0761, Validation Accuracy:0.4072\n",
    "Epoch #147: Loss:1.0701, Accuracy:0.4045, Validation Loss:1.0760, Validation Accuracy:0.4089\n",
    "Epoch #148: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0761, Validation Accuracy:0.4007\n",
    "Epoch #149: Loss:1.0699, Accuracy:0.4041, Validation Loss:1.0761, Validation Accuracy:0.4089\n",
    "Epoch #150: Loss:1.0700, Accuracy:0.4066, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #151: Loss:1.0704, Accuracy:0.4029, Validation Loss:1.0760, Validation Accuracy:0.4105\n",
    "Epoch #152: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0760, Validation Accuracy:0.4105\n",
    "Epoch #153: Loss:1.0697, Accuracy:0.4033, Validation Loss:1.0762, Validation Accuracy:0.4105\n",
    "Epoch #154: Loss:1.0698, Accuracy:0.4000, Validation Loss:1.0762, Validation Accuracy:0.4105\n",
    "Epoch #155: Loss:1.0699, Accuracy:0.4066, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #156: Loss:1.0699, Accuracy:0.4082, Validation Loss:1.0761, Validation Accuracy:0.3974\n",
    "Epoch #157: Loss:1.0698, Accuracy:0.4057, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #158: Loss:1.0703, Accuracy:0.4053, Validation Loss:1.0763, Validation Accuracy:0.3957\n",
    "Epoch #159: Loss:1.0712, Accuracy:0.4086, Validation Loss:1.0768, Validation Accuracy:0.3974\n",
    "Epoch #160: Loss:1.0696, Accuracy:0.4033, Validation Loss:1.0770, Validation Accuracy:0.4007\n",
    "Epoch #161: Loss:1.0708, Accuracy:0.4099, Validation Loss:1.0762, Validation Accuracy:0.4056\n",
    "Epoch #162: Loss:1.0694, Accuracy:0.4066, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #163: Loss:1.0698, Accuracy:0.4049, Validation Loss:1.0757, Validation Accuracy:0.4072\n",
    "Epoch #164: Loss:1.0699, Accuracy:0.4090, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #165: Loss:1.0696, Accuracy:0.4086, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #166: Loss:1.0697, Accuracy:0.4062, Validation Loss:1.0756, Validation Accuracy:0.4056\n",
    "Epoch #167: Loss:1.0698, Accuracy:0.4074, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #168: Loss:1.0705, Accuracy:0.4119, Validation Loss:1.0755, Validation Accuracy:0.4056\n",
    "Epoch #169: Loss:1.0695, Accuracy:0.4094, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #170: Loss:1.0697, Accuracy:0.4025, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #171: Loss:1.0693, Accuracy:0.4062, Validation Loss:1.0756, Validation Accuracy:0.4089\n",
    "Epoch #172: Loss:1.0693, Accuracy:0.4070, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #173: Loss:1.0693, Accuracy:0.4053, Validation Loss:1.0757, Validation Accuracy:0.4023\n",
    "Epoch #174: Loss:1.0699, Accuracy:0.4066, Validation Loss:1.0760, Validation Accuracy:0.3974\n",
    "Epoch #175: Loss:1.0691, Accuracy:0.4078, Validation Loss:1.0758, Validation Accuracy:0.4023\n",
    "Epoch #176: Loss:1.0693, Accuracy:0.4082, Validation Loss:1.0760, Validation Accuracy:0.4089\n",
    "Epoch #177: Loss:1.0692, Accuracy:0.4066, Validation Loss:1.0760, Validation Accuracy:0.4072\n",
    "Epoch #178: Loss:1.0689, Accuracy:0.4078, Validation Loss:1.0760, Validation Accuracy:0.3924\n",
    "Epoch #179: Loss:1.0692, Accuracy:0.4082, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #180: Loss:1.0696, Accuracy:0.4156, Validation Loss:1.0763, Validation Accuracy:0.4056\n",
    "Epoch #181: Loss:1.0695, Accuracy:0.4090, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #182: Loss:1.0695, Accuracy:0.4086, Validation Loss:1.0756, Validation Accuracy:0.3957\n",
    "Epoch #183: Loss:1.0699, Accuracy:0.4029, Validation Loss:1.0765, Validation Accuracy:0.3990\n",
    "Epoch #184: Loss:1.0694, Accuracy:0.4049, Validation Loss:1.0753, Validation Accuracy:0.4089\n",
    "Epoch #185: Loss:1.0690, Accuracy:0.4090, Validation Loss:1.0754, Validation Accuracy:0.4023\n",
    "Epoch #186: Loss:1.0687, Accuracy:0.4078, Validation Loss:1.0754, Validation Accuracy:0.4089\n",
    "Epoch #187: Loss:1.0698, Accuracy:0.4074, Validation Loss:1.0762, Validation Accuracy:0.4007\n",
    "Epoch #188: Loss:1.0686, Accuracy:0.4033, Validation Loss:1.0755, Validation Accuracy:0.3957\n",
    "Epoch #189: Loss:1.0690, Accuracy:0.4078, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #190: Loss:1.0688, Accuracy:0.4086, Validation Loss:1.0754, Validation Accuracy:0.4089\n",
    "Epoch #191: Loss:1.0686, Accuracy:0.4062, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #192: Loss:1.0687, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #193: Loss:1.0688, Accuracy:0.4057, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #194: Loss:1.0684, Accuracy:0.4090, Validation Loss:1.0756, Validation Accuracy:0.3957\n",
    "Epoch #195: Loss:1.0686, Accuracy:0.4078, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #196: Loss:1.0691, Accuracy:0.4041, Validation Loss:1.0762, Validation Accuracy:0.4007\n",
    "Epoch #197: Loss:1.0689, Accuracy:0.4029, Validation Loss:1.0753, Validation Accuracy:0.4089\n",
    "Epoch #198: Loss:1.0688, Accuracy:0.4074, Validation Loss:1.0755, Validation Accuracy:0.4105\n",
    "Epoch #199: Loss:1.0683, Accuracy:0.4057, Validation Loss:1.0755, Validation Accuracy:0.3990\n",
    "Epoch #200: Loss:1.0692, Accuracy:0.4045, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #201: Loss:1.0689, Accuracy:0.4066, Validation Loss:1.0754, Validation Accuracy:0.3957\n",
    "Epoch #202: Loss:1.0683, Accuracy:0.4037, Validation Loss:1.0754, Validation Accuracy:0.4072\n",
    "Epoch #203: Loss:1.0686, Accuracy:0.4021, Validation Loss:1.0755, Validation Accuracy:0.3990\n",
    "Epoch #204: Loss:1.0683, Accuracy:0.4070, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #205: Loss:1.0685, Accuracy:0.4070, Validation Loss:1.0750, Validation Accuracy:0.4122\n",
    "Epoch #206: Loss:1.0682, Accuracy:0.4082, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #207: Loss:1.0688, Accuracy:0.4115, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #208: Loss:1.0683, Accuracy:0.4078, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #209: Loss:1.0681, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.4138\n",
    "Epoch #210: Loss:1.0685, Accuracy:0.4086, Validation Loss:1.0750, Validation Accuracy:0.4122\n",
    "Epoch #211: Loss:1.0681, Accuracy:0.4111, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #212: Loss:1.0685, Accuracy:0.4057, Validation Loss:1.0754, Validation Accuracy:0.3924\n",
    "Epoch #213: Loss:1.0682, Accuracy:0.4082, Validation Loss:1.0753, Validation Accuracy:0.4105\n",
    "Epoch #214: Loss:1.0682, Accuracy:0.4074, Validation Loss:1.0750, Validation Accuracy:0.4122\n",
    "Epoch #215: Loss:1.0683, Accuracy:0.4107, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #216: Loss:1.0678, Accuracy:0.4074, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #217: Loss:1.0681, Accuracy:0.4066, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #218: Loss:1.0679, Accuracy:0.4041, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #219: Loss:1.0681, Accuracy:0.4074, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #220: Loss:1.0683, Accuracy:0.4090, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #221: Loss:1.0685, Accuracy:0.4045, Validation Loss:1.0759, Validation Accuracy:0.3859\n",
    "Epoch #222: Loss:1.0688, Accuracy:0.4062, Validation Loss:1.0748, Validation Accuracy:0.4089\n",
    "Epoch #223: Loss:1.0682, Accuracy:0.4062, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #224: Loss:1.0682, Accuracy:0.4053, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #225: Loss:1.0683, Accuracy:0.4066, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #226: Loss:1.0679, Accuracy:0.4041, Validation Loss:1.0748, Validation Accuracy:0.4089\n",
    "Epoch #227: Loss:1.0683, Accuracy:0.4078, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #228: Loss:1.0677, Accuracy:0.4086, Validation Loss:1.0749, Validation Accuracy:0.4023\n",
    "Epoch #229: Loss:1.0678, Accuracy:0.4066, Validation Loss:1.0744, Validation Accuracy:0.4072\n",
    "Epoch #230: Loss:1.0675, Accuracy:0.4049, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #231: Loss:1.0680, Accuracy:0.4082, Validation Loss:1.0750, Validation Accuracy:0.3810\n",
    "Epoch #232: Loss:1.0675, Accuracy:0.4078, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #233: Loss:1.0672, Accuracy:0.4033, Validation Loss:1.0749, Validation Accuracy:0.4105\n",
    "Epoch #234: Loss:1.0678, Accuracy:0.4078, Validation Loss:1.0746, Validation Accuracy:0.4187\n",
    "Epoch #235: Loss:1.0676, Accuracy:0.4099, Validation Loss:1.0746, Validation Accuracy:0.4122\n",
    "Epoch #236: Loss:1.0682, Accuracy:0.4033, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #237: Loss:1.0681, Accuracy:0.4074, Validation Loss:1.0749, Validation Accuracy:0.3957\n",
    "Epoch #238: Loss:1.0680, Accuracy:0.4078, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #239: Loss:1.0676, Accuracy:0.4090, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #240: Loss:1.0684, Accuracy:0.4090, Validation Loss:1.0752, Validation Accuracy:0.4056\n",
    "Epoch #241: Loss:1.0676, Accuracy:0.4045, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #242: Loss:1.0682, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #243: Loss:1.0674, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #244: Loss:1.0681, Accuracy:0.4078, Validation Loss:1.0756, Validation Accuracy:0.4056\n",
    "Epoch #245: Loss:1.0678, Accuracy:0.4053, Validation Loss:1.0755, Validation Accuracy:0.3875\n",
    "Epoch #246: Loss:1.0674, Accuracy:0.4082, Validation Loss:1.0747, Validation Accuracy:0.3793\n",
    "Epoch #247: Loss:1.0674, Accuracy:0.4078, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #248: Loss:1.0672, Accuracy:0.4070, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #249: Loss:1.0677, Accuracy:0.4045, Validation Loss:1.0747, Validation Accuracy:0.4089\n",
    "Epoch #250: Loss:1.0674, Accuracy:0.4070, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #251: Loss:1.0672, Accuracy:0.4078, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #252: Loss:1.0675, Accuracy:0.4094, Validation Loss:1.0746, Validation Accuracy:0.3777\n",
    "Epoch #253: Loss:1.0674, Accuracy:0.4090, Validation Loss:1.0744, Validation Accuracy:0.4105\n",
    "Epoch #254: Loss:1.0676, Accuracy:0.4103, Validation Loss:1.0744, Validation Accuracy:0.4056\n",
    "Epoch #255: Loss:1.0672, Accuracy:0.4078, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #256: Loss:1.0669, Accuracy:0.4119, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #257: Loss:1.0673, Accuracy:0.4004, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #258: Loss:1.0668, Accuracy:0.4078, Validation Loss:1.0744, Validation Accuracy:0.3744\n",
    "Epoch #259: Loss:1.0680, Accuracy:0.4070, Validation Loss:1.0747, Validation Accuracy:0.3793\n",
    "Epoch #260: Loss:1.0669, Accuracy:0.4074, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #261: Loss:1.0676, Accuracy:0.4094, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #262: Loss:1.0670, Accuracy:0.4103, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #263: Loss:1.0670, Accuracy:0.4049, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #264: Loss:1.0669, Accuracy:0.4037, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #265: Loss:1.0671, Accuracy:0.4090, Validation Loss:1.0749, Validation Accuracy:0.3810\n",
    "Epoch #266: Loss:1.0668, Accuracy:0.4094, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #267: Loss:1.0671, Accuracy:0.4090, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #268: Loss:1.0667, Accuracy:0.4082, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #269: Loss:1.0672, Accuracy:0.4053, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #270: Loss:1.0667, Accuracy:0.4127, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #271: Loss:1.0669, Accuracy:0.4090, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #272: Loss:1.0672, Accuracy:0.4062, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #273: Loss:1.0668, Accuracy:0.4066, Validation Loss:1.0748, Validation Accuracy:0.3744\n",
    "Epoch #274: Loss:1.0675, Accuracy:0.4103, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #275: Loss:1.0671, Accuracy:0.4086, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #276: Loss:1.0668, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #277: Loss:1.0670, Accuracy:0.4111, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #278: Loss:1.0670, Accuracy:0.4078, Validation Loss:1.0745, Validation Accuracy:0.3760\n",
    "Epoch #279: Loss:1.0669, Accuracy:0.4066, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #280: Loss:1.0670, Accuracy:0.4025, Validation Loss:1.0746, Validation Accuracy:0.3760\n",
    "Epoch #281: Loss:1.0669, Accuracy:0.4078, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #282: Loss:1.0668, Accuracy:0.4111, Validation Loss:1.0741, Validation Accuracy:0.3744\n",
    "Epoch #283: Loss:1.0669, Accuracy:0.4115, Validation Loss:1.0742, Validation Accuracy:0.3793\n",
    "Epoch #284: Loss:1.0668, Accuracy:0.4090, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #285: Loss:1.0668, Accuracy:0.4074, Validation Loss:1.0744, Validation Accuracy:0.3810\n",
    "Epoch #286: Loss:1.0668, Accuracy:0.4094, Validation Loss:1.0745, Validation Accuracy:0.3760\n",
    "Epoch #287: Loss:1.0671, Accuracy:0.4033, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #288: Loss:1.0666, Accuracy:0.4074, Validation Loss:1.0743, Validation Accuracy:0.4056\n",
    "Epoch #289: Loss:1.0668, Accuracy:0.4111, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #290: Loss:1.0669, Accuracy:0.4086, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #291: Loss:1.0669, Accuracy:0.4070, Validation Loss:1.0745, Validation Accuracy:0.3777\n",
    "Epoch #292: Loss:1.0667, Accuracy:0.4066, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #293: Loss:1.0670, Accuracy:0.4111, Validation Loss:1.0742, Validation Accuracy:0.3826\n",
    "Epoch #294: Loss:1.0665, Accuracy:0.4090, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #295: Loss:1.0667, Accuracy:0.4111, Validation Loss:1.0746, Validation Accuracy:0.3760\n",
    "Epoch #296: Loss:1.0670, Accuracy:0.4119, Validation Loss:1.0741, Validation Accuracy:0.3810\n",
    "Epoch #297: Loss:1.0668, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.3744\n",
    "Epoch #298: Loss:1.0666, Accuracy:0.4086, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #299: Loss:1.0666, Accuracy:0.4090, Validation Loss:1.0743, Validation Accuracy:0.3826\n",
    "Epoch #300: Loss:1.0667, Accuracy:0.4082, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07422757, Accuracy:0.3892\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  45  176   6\n",
    "t:01  38  190  12\n",
    "t:03  19  121   2\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.44      0.20      0.27       227\n",
    "          01       0.39      0.79      0.52       240\n",
    "          03       0.10      0.01      0.02       142\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.31      0.33      0.27       609\n",
    "weighted avg       0.34      0.39      0.31       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 17:38:57 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 31 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0879789025129747, 1.0787447389514966, 1.0753637260599873, 1.0752120544562003, 1.075444957305645, 1.0754978139999465, 1.074950600492543, 1.0746355389530826, 1.0746811510894099, 1.0745203436301847, 1.0745043701726227, 1.0743430805910985, 1.0741345874585932, 1.0742387004282283, 1.0741743552077971, 1.0742031946558084, 1.0742270603947255, 1.0742239016421715, 1.074124925829507, 1.074189187662159, 1.0740810754068184, 1.0739599497643206, 1.073837648080096, 1.0739011398481422, 1.0738759751390354, 1.0738556075761667, 1.0741450261991403, 1.0738901633934435, 1.0737600007472172, 1.073556129176824, 1.07332025823139, 1.0735945249425953, 1.0738713040531953, 1.073543150240956, 1.0738237463977733, 1.0730784747792386, 1.0731722899454177, 1.0729397893539203, 1.0730320191735705, 1.0728930624443518, 1.0738917465867668, 1.0744510016026363, 1.0757507847251955, 1.0748494652104494, 1.0744474836562459, 1.0747697417959203, 1.0784842511898973, 1.0755552634817038, 1.0760310336286798, 1.0750199080688025, 1.075350236031418, 1.0751231885308703, 1.0745374404738102, 1.074336353976934, 1.0745746795767046, 1.0744555399727156, 1.0747649583518994, 1.0745967832104912, 1.0747956965357213, 1.0748095925414112, 1.0750125055438389, 1.0753522271593217, 1.0755496581003976, 1.0754230345411253, 1.0760240286637606, 1.0764118264657132, 1.075924950280213, 1.0760279764682787, 1.0760546983364963, 1.0761564230096752, 1.0763299034538332, 1.0767499092960202, 1.076800925978299, 1.0767467718797756, 1.0768548878542896, 1.07690993357566, 1.0772660888474563, 1.0773579974479863, 1.0775395629832702, 1.0778010091170889, 1.0782060366741737, 1.0771016121302137, 1.076580164859252, 1.0764073677642396, 1.0764493967707718, 1.07674230358675, 1.0771334991470738, 1.0765500311389542, 1.0771600446481815, 1.0768027554200397, 1.0769135838463193, 1.0762484874239893, 1.0760129836979757, 1.075892063588736, 1.0761958374374214, 1.0754636467384, 1.075599724827533, 1.0758708660629974, 1.0758650923401656, 1.075821355058642, 1.07627399351405, 1.076111142075512, 1.0765808170847901, 1.076387999484496, 1.0763055874992082, 1.0760790137038834, 1.0759532557332456, 1.076007765697924, 1.0760515061113831, 1.0762806570980152, 1.0762197573979695, 1.0762914360450406, 1.07616573151305, 1.076408677304711, 1.0769116594677879, 1.0760750792100904, 1.0760131459713764, 1.0760862978025414, 1.076079098461884, 1.076300941664597, 1.0761769011690112, 1.0761084337344116, 1.076126954434149, 1.0762012787836135, 1.0762623828424414, 1.0770440718223309, 1.07638184562301, 1.0762397914097226, 1.0760923792380221, 1.0765961109123794, 1.076140982372616, 1.0761619694714475, 1.0762134181650598, 1.0762162116556528, 1.076129323351755, 1.0763341628859195, 1.076254836835689, 1.0762699791559054, 1.0763734485128242, 1.0766395383280487, 1.076369119395176, 1.0762564006501623, 1.075906553683414, 1.0757530140759322, 1.075829603989136, 1.0761309776008618, 1.0760109240589861, 1.076107786794014, 1.0761247237131906, 1.076068201480045, 1.0760407246196604, 1.0760425849892628, 1.0761978363951634, 1.0761669061845551, 1.0760952347800845, 1.0761199633671927, 1.0760570247772292, 1.0763053067995019, 1.0768016801874822, 1.076981402187316, 1.0761541597948874, 1.075558997140142, 1.0756624310670424, 1.0758257926195518, 1.075567627775258, 1.075561587642175, 1.0759651455385932, 1.0755113461138972, 1.0760112536951827, 1.0754511469886416, 1.0755783055216221, 1.075517524052136, 1.0757362408003783, 1.0759802156285503, 1.0758051496421175, 1.0759920672634355, 1.0760381059302093, 1.0760110133191438, 1.0758686071546206, 1.076337340038594, 1.0754870004058863, 1.075595058635342, 1.076545135728244, 1.0753212734591981, 1.075440940011311, 1.075350757694401, 1.0761889818266694, 1.0754836431669288, 1.075269097765091, 1.0753543312326441, 1.075464719230514, 1.0752947048600672, 1.075589418019763, 1.0755890582386887, 1.0757343512646278, 1.0761648858905035, 1.07532264837882, 1.075493150352453, 1.0754695991772931, 1.075701814762673, 1.0753886092864038, 1.0754114943380622, 1.0754854536213114, 1.0751378035114707, 1.0750210308676282, 1.0759656309885737, 1.0754671451101945, 1.0751144208735826, 1.0749549730657944, 1.075047053922769, 1.0753201596646864, 1.0754329868529622, 1.0752952793744592, 1.074956857316404, 1.075341504195641, 1.0751041708321407, 1.0752547695523216, 1.0749359148476512, 1.0753424273335874, 1.07503933624681, 1.0758718710227553, 1.0748088694558355, 1.0748165675571986, 1.0760182649239727, 1.074997632765809, 1.0747748217950706, 1.0746607032706976, 1.074897133266593, 1.0743813048834088, 1.0747960277378852, 1.0750253531341678, 1.0749700179045227, 1.074877571981333, 1.0746375500470742, 1.0746431450538447, 1.0749023487219473, 1.0749086863888895, 1.074652641864833, 1.0746243937653666, 1.0751829900960812, 1.0746830897573962, 1.0753399538876387, 1.0750477034078638, 1.0755654211310526, 1.0754861673110812, 1.074655562394554, 1.0746534037081088, 1.07443838401381, 1.0746838735242195, 1.074308143656438, 1.0745562193624687, 1.0746371309549743, 1.0744171721986167, 1.074435454871267, 1.0748914337314799, 1.0746272763203712, 1.074591791884261, 1.0744419898501367, 1.0746799402049023, 1.0747694982879463, 1.0746125842158627, 1.074297122767406, 1.0743998741281444, 1.074716586784776, 1.0749485161895627, 1.0747128654583333, 1.0746563903999642, 1.0748152771998314, 1.0744330542428153, 1.074574892744055, 1.0747327597074712, 1.0749409212463203, 1.0747726715257016, 1.0746923281837175, 1.0745794031224618, 1.0744765147395519, 1.0744023636252618, 1.0745457718133535, 1.0746463774068797, 1.0745694615766528, 1.0742158271213276, 1.0741233422446916, 1.0741624207723708, 1.0742130690607532, 1.074376086883357, 1.0745086323451527, 1.074585536822114, 1.0743051702753077, 1.0741934267366657, 1.0744861507259176, 1.0744589659185049, 1.0743352985147185, 1.074198412973501, 1.074700868188454, 1.0745510279838675, 1.0741198644262229, 1.074380581993579, 1.074361044980818, 1.0743379422596522, 1.0742277384587304], 'val_acc': [0.37602627106096553, 0.39408866525283587, 0.3940886699018024, 0.39408866525283587, 0.39408866525283587, 0.39573070602659716, 0.39244663377700767, 0.3990147736272201, 0.3973727420535189, 0.39901477817831366, 0.3973727420535189, 0.3973727420535189, 0.4105090310518769, 0.3973727421513919, 0.39573070602659716, 0.39573070602659716, 0.39573070602659716, 0.39901477798256774, 0.4022988504279032, 0.40722495880228743, 0.40065681430310846, 0.4088669949270822, 0.4006568139116165, 0.3957307054393593, 0.4022988502321572, 0.4022988503300302, 0.4022988502321572, 0.397372741955646, 0.4022988498406653, 0.41050903066038497, 0.39901477778682176, 0.40229885013428424, 0.39408866525283587, 0.3924466332876428, 0.3957307054393593, 0.39737274136840806, 0.3809523804140796, 0.38752052501113154, 0.4006568138137435, 0.3924466327982779, 0.38423645217430413, 0.3990147775910758, 0.377668308360236, 0.39737274156415403, 0.38587848859271784, 0.3891625614295452, 0.4022988504279032, 0.38916256074443434, 0.3858784882990989, 0.38423645276154206, 0.38752052510900453, 0.3842364525657961, 0.38752052481538557, 0.3924466332876428, 0.39408866882519966, 0.403940886259079, 0.3891625611359263, 0.40722495860654145, 0.39408866921669156, 0.4055829223838737, 0.40229884993853826, 0.403940886259079, 0.3908045968692291, 0.3990147775910758, 0.39080459716284804, 0.3875205244238936, 0.39408866911881857, 0.39408866911881857, 0.39080459667348316, 0.39737274136840806, 0.3990147774932028, 0.3875205244238936, 0.39408866911881857, 0.39408866911881857, 0.3940886690209456, 0.39408866911881857, 0.38916256054868836, 0.3990147774932028, 0.40065681361799754, 0.39080459667348316, 0.3891625608423073, 0.3940886690209456, 0.40722495870441444, 0.40722495850866847, 0.39901477788469475, 0.40229885013428424, 0.3940886697060565, 0.3940886697060565, 0.39737274156415403, 0.3940886697060565, 0.40065681420523547, 0.40722495850866847, 0.4072249584107955, 0.4072249584107955, 0.40394088645482495, 0.40722495870441444, 0.4022988502321572, 0.41050903095400393, 0.4022988502321572, 0.39737274156415403, 0.3858784887884638, 0.3940886696081835, 0.39737274156415403, 0.39901477798256774, 0.4088669948292092, 0.39737274156415403, 0.41050903095400393, 0.39901477798256774, 0.40722495870441444, 0.39737274156415403, 0.40722495870441444, 0.39244663348338876, 0.4088669948292092, 0.39737274156415403, 0.39244663348338876, 0.41050903095400393, 0.40722495870441444, 0.40722495850866847, 0.41050903095400393, 0.41050903095400393, 0.39737274156415403, 0.41050903095400393, 0.40722495870441444, 0.4022988502321572, 0.4088669948292092, 0.39901477817831366, 0.39737274156415403, 0.39737274156415403, 0.41050903095400393, 0.39244663348338876, 0.41050903095400393, 0.4022988502321572, 0.4022988502321572, 0.39737274156415403, 0.4006568140094895, 0.40722495870441444, 0.40722495850866847, 0.4055829223838737, 0.4088669948292092, 0.3891625612337993, 0.39737274156415403, 0.39573070573297825, 0.40394088645482495, 0.39737274156415403, 0.4006568140094895, 0.40722495870441444, 0.4088669948292092, 0.40065681420523547, 0.4088669948292092, 0.41050903095400393, 0.41050903095400393, 0.41050903095400393, 0.41050903095400393, 0.41050903095400393, 0.41050903095400393, 0.39737274156415403, 0.41050903095400393, 0.39573070592872417, 0.39737274156415403, 0.4006568144009814, 0.4055829226774927, 0.39737274156415403, 0.40722495850866847, 0.3940886697060565, 0.39737274156415403, 0.4055829226774927, 0.39901477817831366, 0.4055829226774927, 0.40229885052577613, 0.4088669949270822, 0.4088669949270822, 0.4088669949270822, 0.40229885003641125, 0.3973727420535189, 0.4022988502321572, 0.4088669949270822, 0.40722495880228743, 0.3924466332876428, 0.40229885003641125, 0.4055829227753656, 0.4088669949270822, 0.3957307055372323, 0.39901477788469475, 0.4088669949270822, 0.40229885003641125, 0.4088669949270822, 0.4006568144009814, 0.3957307055372323, 0.39901477798256774, 0.4088669949270822, 0.4088669949270822, 0.39901477798256774, 0.3891625610380533, 0.3957307055372323, 0.39901477798256774, 0.4006568141073625, 0.4088669949270822, 0.4105090310518769, 0.39901477798256774, 0.3891625609401803, 0.3957307055372323, 0.40722495870441444, 0.39901477788469475, 0.3957307055372323, 0.41215106727454465, 0.39901477798256774, 0.38587848849484485, 0.3957307055372323, 0.4137931033993394, 0.41215106727454465, 0.3924466329940238, 0.3924466329940238, 0.4105090311497499, 0.41215106727454465, 0.3957307055372323, 0.4055829225796197, 0.39573070563510526, 0.39901477798256774, 0.39737274156415403, 0.3891625609401803, 0.3858784886905908, 0.4088669949270822, 0.39901477798256774, 0.38259441634312835, 0.3957307055372323, 0.4088669949270822, 0.39901477798256774, 0.4022988502321572, 0.40722495880228743, 0.3957307055372323, 0.38095238002258763, 0.3842364524679231, 0.4105090310518769, 0.4187192071247571, 0.41215106727454465, 0.3924466332876428, 0.3957307055372323, 0.3924466332876428, 0.3957307055372323, 0.4055829226774927, 0.3842364524679231, 0.3973727417599, 0.39080459696710207, 0.4055829226774927, 0.38752052491325856, 0.3793103438977929, 0.3957307055372323, 0.3891625609401803, 0.4088669949270822, 0.3957307055372323, 0.37438423542553567, 0.37766830787087113, 0.4105090310518769, 0.4055829225796197, 0.3842364525657961, 0.37438423542553567, 0.39080459706497506, 0.37438423542553567, 0.37931034409353886, 0.39737274156415403, 0.37438423542553567, 0.39080459706497506, 0.3891625609401803, 0.37438423542553567, 0.3809523802183336, 0.38587848849484485, 0.37438423542553567, 0.3891625610380533, 0.3891625609401803, 0.3924466331897698, 0.4055829225796197, 0.39080459716284804, 0.37438423542553567, 0.38587848839697186, 0.39901477798256774, 0.3809523802183336, 0.39901477788469475, 0.3760262715503304, 0.3924466332876428, 0.3760262715503304, 0.3891625609401803, 0.37438423542553567, 0.3793103438977929, 0.3891625609401803, 0.38095238002258763, 0.3760262715503304, 0.38752052481538557, 0.4055829225796197, 0.3924466331897698, 0.38259441624525536, 0.37766830777299815, 0.3891625609401803, 0.38259441624525536, 0.37438423542553567, 0.3760262715503304, 0.3809523801204606, 0.37438423542553567, 0.3891625610380533, 0.38259441624525536, 0.3891625609401803], 'loss': [1.0957351873053174, 1.082621694100711, 1.0759534932749473, 1.0741953165868958, 1.0742411384347528, 1.0747931762397656, 1.0745710904103773, 1.0740796425504116, 1.0739393219565954, 1.073937374120865, 1.0740163932835542, 1.0738631051668641, 1.07376918959177, 1.0737496873436523, 1.0737292862525956, 1.0737502879430625, 1.07376422926141, 1.0738727337525855, 1.0736992562331213, 1.0735401145241834, 1.073444390541719, 1.0733137864596545, 1.0734247419134058, 1.0734242022159897, 1.0733216223530702, 1.0732364304501418, 1.073177072350739, 1.0729368972092928, 1.0729230836676376, 1.0729381837884013, 1.0728938257914549, 1.0728263739442923, 1.0729827282120314, 1.0731334316657064, 1.0728117653966194, 1.07312191756599, 1.0722206003122505, 1.0724309662773868, 1.0722141084240202, 1.072275398986785, 1.0726398074896184, 1.0725630592271778, 1.0727399554340746, 1.073839199175825, 1.0727887269163034, 1.072577622098355, 1.0729286332884365, 1.073367728149132, 1.0735512522456583, 1.0731582666079855, 1.0727775346571904, 1.0728460410781955, 1.0726761991238447, 1.0727343278254327, 1.0728551416671253, 1.072247644175739, 1.0720961438556962, 1.0721573199579604, 1.0715125537261336, 1.0715707741723657, 1.0715041842059188, 1.0716313955964982, 1.0713897433858632, 1.0716044934623294, 1.071780152634184, 1.0711987494932798, 1.0718433236684153, 1.071295834126169, 1.0710686030084346, 1.0708810832221405, 1.07054272544947, 1.0704397047814402, 1.0706066108092636, 1.0707344759171504, 1.0705915384468847, 1.0710494805655195, 1.0706825169204932, 1.0703275198809175, 1.0711789580096454, 1.0712018344926149, 1.0710136145781688, 1.0706159983327501, 1.0719281718471456, 1.0717285469572158, 1.0713201410716564, 1.0718097951133148, 1.0718952729716682, 1.0724962468999122, 1.0733756461427442, 1.0723342493084667, 1.0720583358094922, 1.0713610740168138, 1.0715617513264963, 1.0715948992441322, 1.0711675437813666, 1.0715693772940664, 1.0719526087723719, 1.0718518445134408, 1.0721438092617528, 1.072460363190277, 1.0712873752601828, 1.0718678594369908, 1.0719298278037037, 1.0710903611271287, 1.0719597896756088, 1.0712741964406791, 1.071174243490309, 1.0711791158456823, 1.07128607066505, 1.0711427408566954, 1.070935140057511, 1.0710825082947342, 1.07077813887743, 1.0719037486297638, 1.0705992060765104, 1.0713950572806952, 1.070961257270719, 1.0707233312927966, 1.0707605006513654, 1.0711989076720125, 1.0705360837051265, 1.07104536684142, 1.0706485125563228, 1.070725122859101, 1.0710412578171529, 1.070669571379127, 1.0707601941341738, 1.0709172706584422, 1.0707913022756086, 1.0708351056923366, 1.0705729472808525, 1.070480127403134, 1.0703651266176353, 1.0704335713533406, 1.0703857603014373, 1.0703544141330759, 1.0703136843577548, 1.070266111430691, 1.0704642306607852, 1.07194554370042, 1.0708615849884628, 1.070705425372114, 1.070282178884659, 1.0702205603617172, 1.0701321305680325, 1.069987638334474, 1.0700674684140716, 1.0699706597494638, 1.0698857136086026, 1.0699590417638696, 1.0704359541193906, 1.070081529039622, 1.069710076760952, 1.0698157620381037, 1.0698725029673175, 1.069922217304457, 1.0697857968371507, 1.0703200418601535, 1.0712174554625087, 1.069591165910756, 1.070769212622907, 1.0694354421059453, 1.0697792685252197, 1.0698707750445764, 1.0695945222274967, 1.0696506060124422, 1.0697597196214743, 1.0705446060188497, 1.0695240624876239, 1.0696585492186967, 1.0693411690498524, 1.069338399233025, 1.0693117832011512, 1.0699332547628415, 1.0691382361633333, 1.0693207884716058, 1.069215434581592, 1.0689486150134515, 1.0691872597719854, 1.0696356997597634, 1.0695058780039606, 1.06949270961221, 1.0698744065952497, 1.0694302563305018, 1.068998749300195, 1.0687443899178162, 1.0698277220833718, 1.068594367646094, 1.0690286205045008, 1.0687650754466438, 1.0686043019167453, 1.0686614122234086, 1.0687871155063229, 1.0684277202559203, 1.068586271891114, 1.0690656200816255, 1.0688839923674567, 1.0688282136310052, 1.0682629294463986, 1.0691751539340009, 1.0688808987517622, 1.0683119952556288, 1.0686441664333461, 1.0682714359961007, 1.068549833160651, 1.0682337381022178, 1.0687650215454414, 1.0682930001977533, 1.068126242753172, 1.0685444988509223, 1.0681383643551774, 1.0685362974965842, 1.06817167427016, 1.068244212412981, 1.0682978272682833, 1.0678279881604642, 1.068061848440699, 1.0678734970778165, 1.0680641627654404, 1.0683059642447095, 1.0684550792529597, 1.0687654470760963, 1.0681761248155786, 1.068213059818965, 1.0682546687077203, 1.0678742837612145, 1.068274624098008, 1.0676999977237145, 1.067797648343707, 1.067512257339039, 1.0679704327847679, 1.0674822630089167, 1.0672213637118957, 1.0678295543795984, 1.0675851840502917, 1.0681957388315848, 1.0680615981746258, 1.0680109521936343, 1.0675572608285862, 1.0683946481230813, 1.0675996883693906, 1.0682489764274268, 1.0674231679042996, 1.0681128854379516, 1.0678473632193688, 1.0674111595878366, 1.0673784184015263, 1.0671861537428118, 1.0677135773997533, 1.0674400562133632, 1.0671941450733913, 1.0675484038965903, 1.067350352716152, 1.0676322720134037, 1.0671941910436267, 1.0669421363415414, 1.0672822519004712, 1.066833168867922, 1.0680217803136525, 1.0669054434774348, 1.0675605735739644, 1.0670488409927494, 1.0670182733809923, 1.0668566046309422, 1.0670856418550871, 1.0668303304629159, 1.0671209605077943, 1.0667220043695438, 1.0671675292373437, 1.0667304640188355, 1.0669064107127257, 1.0671604216710742, 1.066795178115735, 1.0674795765162004, 1.0670964287047024, 1.0668467244083633, 1.066969553792746, 1.0669916129944506, 1.0668781890516654, 1.0670316459706677, 1.0668595872130973, 1.0667998880331522, 1.0668659174956336, 1.0667896747099546, 1.0667526529065392, 1.0667755364882139, 1.067118343484475, 1.066604651415862, 1.066764890682526, 1.0668720510216465, 1.0669027497391437, 1.0667168497794464, 1.0670118136082831, 1.066544973238293, 1.0667161408391086, 1.0669863797311177, 1.0667993627289727, 1.066641170190345, 1.0666474988083574, 1.0667249341275413], 'acc': [0.38521560412168016, 0.39630390337115684, 0.3950718681670312, 0.3975359350504082, 0.39630390297950413, 0.39466119309470393, 0.39055441593487406, 0.3934291578905783, 0.39466119035313507, 0.39630390219619877, 0.39753593524623454, 0.4000000011504798, 0.3942505123066951, 0.39835729106985324, 0.3987679692754021, 0.39917864708929823, 0.399178646146884, 0.3938398342969726, 0.3921971249996514, 0.3967145772318086, 0.40041067583115436, 0.4008213532166804, 0.39958932294493094, 0.39383983410114626, 0.396714580597574, 0.40041067618608966, 0.3971252570406857, 0.4008213530208541, 0.3963038996137388, 0.4053388082271239, 0.39630390219619877, 0.3971252584114701, 0.396714580597574, 0.3889117042876367, 0.4036961006922399, 0.40369610026386976, 0.4102669388239389, 0.40041067638191596, 0.40041067540278424, 0.40492813076816303, 0.3979466110651498, 0.40246406666307233, 0.39999999798054076, 0.3979466098901917, 0.3983572883282844, 0.39876796947122845, 0.4000000019337852, 0.3975359350504082, 0.39178644780995175, 0.3983572896990688, 0.39876796751296495, 0.39917864630599287, 0.3967145804017476, 0.3946611893740033, 0.39383983410114626, 0.4053388070154484, 0.40328542131173295, 0.3983572893074161, 0.4028747427145314, 0.4008213565824458, 0.40574948858676263, 0.4053388085820592, 0.40123203338049274, 0.4086242319132513, 0.3926078024586123, 0.40657084068968063, 0.4032854197084047, 0.4004106791602023, 0.4061601642465689, 0.40451745295426683, 0.40246406447226507, 0.3963039016087197, 0.4086242285474859, 0.4008213559949667, 0.4057494883909363, 0.4057494873750871, 0.39958932235745187, 0.4045174543617687, 0.40328542189921196, 0.399999999583869, 0.40205338963248155, 0.40164271103528, 0.404928133350623, 0.4012320306389239, 0.399589324119889, 0.3995893201666446, 0.4061601624474144, 0.3946611901573087, 0.39671458079340033, 0.4016427122102381, 0.39589322496978163, 0.39753593524623454, 0.40369609830560627, 0.3995893205582973, 0.4004106785727233, 0.3995893245115417, 0.3958932245781289, 0.40246406368895965, 0.4000000007588271, 0.40698151964181745, 0.40082135482000864, 0.3967145794226159, 0.40985626081421633, 0.40328542229086467, 0.3975359324679482, 0.3999999981763671, 0.40492813018068397, 0.3995893225532782, 0.3975359338754501, 0.4028747448686212, 0.4012320306389239, 0.39794661149351995, 0.4016427102519746, 0.40410677478543544, 0.40287474111120314, 0.39835728907487233, 0.40328541990423106, 0.40410677831030967, 0.4028747426778139, 0.40657084303959684, 0.40205338607088986, 0.4065708404571369, 0.40451745295426683, 0.4065708408487896, 0.4073921976882574, 0.4020533884575235, 0.4041067767436989, 0.4049281323714912, 0.4049281305723367, 0.40821355292439704, 0.4041067745528916, 0.40451745573255316, 0.40739219749243105, 0.3995893247073681, 0.4049281300215751, 0.40533881014866996, 0.4082135501461107, 0.4036960967389955, 0.4061601652257007, 0.3983572918531586, 0.40410677533619704, 0.4045174539333986, 0.40287474428114217, 0.4024640662714196, 0.403696098697259, 0.40616016362237245, 0.40451745416594237, 0.40205338802915336, 0.4041067763520462, 0.40657083967383145, 0.40287474346111934, 0.4028747413070295, 0.40328541951257835, 0.4000000013095887, 0.4065708398696578, 0.40821355092941614, 0.40574948855004517, 0.40533880819040646, 0.4086242287433123, 0.40328541951257835, 0.409856263788329, 0.4065708408487896, 0.4049281325673176, 0.4090349083196456, 0.4086242319132513, 0.40616016205576166, 0.4073921966724082, 0.4119096500795235, 0.409445584566931, 0.402464063884786, 0.40616016283906703, 0.40698151807520666, 0.4053388097937347, 0.4065708442145549, 0.4078028725280409, 0.4082135535118761, 0.40657083967383145, 0.4078028755021536, 0.40821355092941614, 0.4156057511878944, 0.4090349095313211, 0.408624232304904, 0.40287474306946663, 0.4049281317840122, 0.40903490773216655, 0.40780287569797996, 0.4073921984715628, 0.4032854211159066, 0.40780287589380626, 0.40862422776418056, 0.406160162251588, 0.4053388070154484, 0.40574948482934453, 0.40903490874801574, 0.4078028733113463, 0.4041067767436989, 0.4028747411479206, 0.4073921961216466, 0.4057494881951099, 0.40451745295426683, 0.40657084401872856, 0.40369609971310816, 0.40205339002413426, 0.4069815172919013, 0.4069815212451457, 0.40821355331604975, 0.4114989732447591, 0.40780287291969364, 0.4045174523667878, 0.4086242315215986, 0.4110882960550594, 0.40574948522099724, 0.40821355429518147, 0.40739219706406093, 0.41067762000360036, 0.40739219886321554, 0.4065708436270759, 0.4041067775270043, 0.4073921982757365, 0.4090349099229738, 0.40451745334591954, 0.40616016584989717, 0.40616016561735335, 0.4053388083862328, 0.4065708442145549, 0.40410677576456716, 0.4078028725280409, 0.40862422815583327, 0.40657084186463877, 0.4049281299848576, 0.40821355190854786, 0.4078028747188482, 0.4032854189250993, 0.4078028764812853, 0.40985626061839, 0.40328542189921196, 0.40739219690495204, 0.4078028760896326, 0.40903490734051384, 0.40903490890712463, 0.40451745416594237, 0.4053388090104293, 0.40451745217096147, 0.40780287295641104, 0.4053388079945801, 0.4082135515168952, 0.4078028727238673, 0.4069815216367984, 0.4045174529909843, 0.40698151807520666, 0.40780287389882536, 0.4094455849953011, 0.40903490694886113, 0.4102669423855306, 0.40780287530632725, 0.41190965285780984, 0.4004106799435077, 0.4078028770687644, 0.40698151905433844, 0.40739219628075557, 0.4094455873452173, 0.4102669412105725, 0.40492813217566487, 0.4036960977181272, 0.409034907768884, 0.4094455839794519, 0.40903490577390306, 0.4082135539035288, 0.4053388069787309, 0.4127310045323577, 0.40903490972714746, 0.4061601638549162, 0.40657084401872856, 0.4102669427771833, 0.40862422776418056, 0.40451745377428966, 0.4110882962508858, 0.4078028740946517, 0.40657084343124955, 0.4024640678380304, 0.40780287373971646, 0.41108829543086295, 0.4114989726939975, 0.409034908356363, 0.4073921984715628, 0.40944558754104365, 0.40328542147084184, 0.40739219749243105, 0.4110882968383648, 0.4086242287433123, 0.4069815200701876, 0.4065708424521178, 0.41108829664253843, 0.40903490577390306, 0.41108829464755753, 0.4119096530536362, 0.40451745612420587, 0.4086242299549878, 0.4090349055780767, 0.4082135531202234]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
