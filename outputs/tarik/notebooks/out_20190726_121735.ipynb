{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf3.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 12:17:35 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'All', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000025080959550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000250DDE26EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0940, Accuracy:0.3589, Validation Loss:1.0830, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0797, Accuracy:0.3943, Validation Loss:1.0774, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0751, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0751, Validation Accuracy:0.3727\n",
    "Epoch #5: Loss:1.0743, Accuracy:0.3774, Validation Loss:1.0751, Validation Accuracy:0.3727\n",
    "Epoch #6: Loss:1.0742, Accuracy:0.3864, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #7: Loss:1.0736, Accuracy:0.3947, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #11: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0735, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #15: Loss:1.0734, Accuracy:0.3951, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #16: Loss:1.0734, Accuracy:0.3955, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0732, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #18: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #19: Loss:1.0733, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #20: Loss:1.0728, Accuracy:0.3996, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #21: Loss:1.0730, Accuracy:0.3959, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #22: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #23: Loss:1.0731, Accuracy:0.3996, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #24: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0728, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #26: Loss:1.0728, Accuracy:0.3971, Validation Loss:1.0747, Validation Accuracy:0.4122\n",
    "Epoch #27: Loss:1.0725, Accuracy:0.4136, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #28: Loss:1.0721, Accuracy:0.4074, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #29: Loss:1.0721, Accuracy:0.4000, Validation Loss:1.0751, Validation Accuracy:0.3924\n",
    "Epoch #30: Loss:1.0721, Accuracy:0.3971, Validation Loss:1.0750, Validation Accuracy:0.4039\n",
    "Epoch #31: Loss:1.0718, Accuracy:0.4107, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #32: Loss:1.0719, Accuracy:0.4049, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #33: Loss:1.0712, Accuracy:0.4086, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.4105\n",
    "Epoch #35: Loss:1.0709, Accuracy:0.4111, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #36: Loss:1.0710, Accuracy:0.4107, Validation Loss:1.0766, Validation Accuracy:0.3826\n",
    "Epoch #37: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0766, Validation Accuracy:0.3826\n",
    "Epoch #38: Loss:1.0702, Accuracy:0.4111, Validation Loss:1.0765, Validation Accuracy:0.3727\n",
    "Epoch #39: Loss:1.0710, Accuracy:0.4066, Validation Loss:1.0770, Validation Accuracy:0.3711\n",
    "Epoch #40: Loss:1.0702, Accuracy:0.4107, Validation Loss:1.0782, Validation Accuracy:0.3727\n",
    "Epoch #41: Loss:1.0707, Accuracy:0.4140, Validation Loss:1.0787, Validation Accuracy:0.3662\n",
    "Epoch #42: Loss:1.0715, Accuracy:0.4111, Validation Loss:1.0788, Validation Accuracy:0.3826\n",
    "Epoch #43: Loss:1.0712, Accuracy:0.4062, Validation Loss:1.0777, Validation Accuracy:0.3711\n",
    "Epoch #44: Loss:1.0710, Accuracy:0.4123, Validation Loss:1.0781, Validation Accuracy:0.3465\n",
    "Epoch #45: Loss:1.0698, Accuracy:0.4201, Validation Loss:1.0770, Validation Accuracy:0.3826\n",
    "Epoch #46: Loss:1.0718, Accuracy:0.4012, Validation Loss:1.0772, Validation Accuracy:0.3859\n",
    "Epoch #47: Loss:1.0708, Accuracy:0.4062, Validation Loss:1.0778, Validation Accuracy:0.3629\n",
    "Epoch #48: Loss:1.0707, Accuracy:0.4053, Validation Loss:1.0794, Validation Accuracy:0.3826\n",
    "Epoch #49: Loss:1.0688, Accuracy:0.4041, Validation Loss:1.0759, Validation Accuracy:0.3842\n",
    "Epoch #50: Loss:1.0707, Accuracy:0.4144, Validation Loss:1.0763, Validation Accuracy:0.3793\n",
    "Epoch #51: Loss:1.0705, Accuracy:0.4070, Validation Loss:1.0770, Validation Accuracy:0.3645\n",
    "Epoch #52: Loss:1.0696, Accuracy:0.4062, Validation Loss:1.0785, Validation Accuracy:0.3810\n",
    "Epoch #53: Loss:1.0702, Accuracy:0.4021, Validation Loss:1.0774, Validation Accuracy:0.3777\n",
    "Epoch #54: Loss:1.0704, Accuracy:0.4086, Validation Loss:1.0778, Validation Accuracy:0.3760\n",
    "Epoch #55: Loss:1.0689, Accuracy:0.4152, Validation Loss:1.0792, Validation Accuracy:0.3859\n",
    "Epoch #56: Loss:1.0699, Accuracy:0.4144, Validation Loss:1.0785, Validation Accuracy:0.3448\n",
    "Epoch #57: Loss:1.0689, Accuracy:0.4066, Validation Loss:1.0774, Validation Accuracy:0.3760\n",
    "Epoch #58: Loss:1.0691, Accuracy:0.4094, Validation Loss:1.0802, Validation Accuracy:0.3662\n",
    "Epoch #59: Loss:1.0713, Accuracy:0.3979, Validation Loss:1.0787, Validation Accuracy:0.3842\n",
    "Epoch #60: Loss:1.0702, Accuracy:0.4033, Validation Loss:1.0773, Validation Accuracy:0.3826\n",
    "Epoch #61: Loss:1.0680, Accuracy:0.4066, Validation Loss:1.0768, Validation Accuracy:0.3777\n",
    "Epoch #62: Loss:1.0681, Accuracy:0.4086, Validation Loss:1.0803, Validation Accuracy:0.3498\n",
    "Epoch #63: Loss:1.0697, Accuracy:0.4115, Validation Loss:1.0810, Validation Accuracy:0.3678\n",
    "Epoch #64: Loss:1.0699, Accuracy:0.4074, Validation Loss:1.0800, Validation Accuracy:0.3908\n",
    "Epoch #65: Loss:1.0702, Accuracy:0.4057, Validation Loss:1.0776, Validation Accuracy:0.3711\n",
    "Epoch #66: Loss:1.0682, Accuracy:0.4099, Validation Loss:1.0783, Validation Accuracy:0.3580\n",
    "Epoch #67: Loss:1.0704, Accuracy:0.4074, Validation Loss:1.0789, Validation Accuracy:0.3596\n",
    "Epoch #68: Loss:1.0702, Accuracy:0.4057, Validation Loss:1.0799, Validation Accuracy:0.3645\n",
    "Epoch #69: Loss:1.0701, Accuracy:0.3975, Validation Loss:1.0795, Validation Accuracy:0.3662\n",
    "Epoch #70: Loss:1.0700, Accuracy:0.4066, Validation Loss:1.0826, Validation Accuracy:0.3596\n",
    "Epoch #71: Loss:1.0700, Accuracy:0.4090, Validation Loss:1.0814, Validation Accuracy:0.3777\n",
    "Epoch #72: Loss:1.0706, Accuracy:0.3996, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #73: Loss:1.0701, Accuracy:0.4136, Validation Loss:1.0749, Validation Accuracy:0.3810\n",
    "Epoch #74: Loss:1.0702, Accuracy:0.4066, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #75: Loss:1.0693, Accuracy:0.4074, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #76: Loss:1.0700, Accuracy:0.4074, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #77: Loss:1.0697, Accuracy:0.4000, Validation Loss:1.0741, Validation Accuracy:0.3727\n",
    "Epoch #78: Loss:1.0690, Accuracy:0.4062, Validation Loss:1.0764, Validation Accuracy:0.3580\n",
    "Epoch #79: Loss:1.0689, Accuracy:0.4099, Validation Loss:1.0769, Validation Accuracy:0.3760\n",
    "Epoch #80: Loss:1.0695, Accuracy:0.4164, Validation Loss:1.0777, Validation Accuracy:0.3695\n",
    "Epoch #81: Loss:1.0703, Accuracy:0.4049, Validation Loss:1.0776, Validation Accuracy:0.3645\n",
    "Epoch #82: Loss:1.0672, Accuracy:0.4094, Validation Loss:1.0778, Validation Accuracy:0.3563\n",
    "Epoch #83: Loss:1.0683, Accuracy:0.4053, Validation Loss:1.0789, Validation Accuracy:0.3563\n",
    "Epoch #84: Loss:1.0684, Accuracy:0.4094, Validation Loss:1.0777, Validation Accuracy:0.3612\n",
    "Epoch #85: Loss:1.0677, Accuracy:0.4115, Validation Loss:1.0780, Validation Accuracy:0.3810\n",
    "Epoch #86: Loss:1.0685, Accuracy:0.4115, Validation Loss:1.0767, Validation Accuracy:0.3760\n",
    "Epoch #87: Loss:1.0703, Accuracy:0.4074, Validation Loss:1.0760, Validation Accuracy:0.3810\n",
    "Epoch #88: Loss:1.0683, Accuracy:0.4160, Validation Loss:1.0759, Validation Accuracy:0.3777\n",
    "Epoch #89: Loss:1.0706, Accuracy:0.3881, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #90: Loss:1.0694, Accuracy:0.4127, Validation Loss:1.0768, Validation Accuracy:0.3695\n",
    "Epoch #91: Loss:1.0671, Accuracy:0.4111, Validation Loss:1.0781, Validation Accuracy:0.3727\n",
    "Epoch #92: Loss:1.0668, Accuracy:0.4094, Validation Loss:1.0773, Validation Accuracy:0.3842\n",
    "Epoch #93: Loss:1.0685, Accuracy:0.4049, Validation Loss:1.0762, Validation Accuracy:0.3711\n",
    "Epoch #94: Loss:1.0685, Accuracy:0.4057, Validation Loss:1.0759, Validation Accuracy:0.3629\n",
    "Epoch #95: Loss:1.0689, Accuracy:0.3951, Validation Loss:1.0767, Validation Accuracy:0.3596\n",
    "Epoch #96: Loss:1.0697, Accuracy:0.4049, Validation Loss:1.0818, Validation Accuracy:0.3695\n",
    "Epoch #97: Loss:1.0706, Accuracy:0.3934, Validation Loss:1.0802, Validation Accuracy:0.3366\n",
    "Epoch #98: Loss:1.0676, Accuracy:0.4107, Validation Loss:1.0803, Validation Accuracy:0.3563\n",
    "Epoch #99: Loss:1.0686, Accuracy:0.3996, Validation Loss:1.0786, Validation Accuracy:0.3498\n",
    "Epoch #100: Loss:1.0683, Accuracy:0.4094, Validation Loss:1.0780, Validation Accuracy:0.3498\n",
    "Epoch #101: Loss:1.0674, Accuracy:0.4148, Validation Loss:1.0804, Validation Accuracy:0.3629\n",
    "Epoch #102: Loss:1.0670, Accuracy:0.4152, Validation Loss:1.0797, Validation Accuracy:0.3678\n",
    "Epoch #103: Loss:1.0663, Accuracy:0.4111, Validation Loss:1.0795, Validation Accuracy:0.3744\n",
    "Epoch #104: Loss:1.0669, Accuracy:0.4012, Validation Loss:1.0802, Validation Accuracy:0.3465\n",
    "Epoch #105: Loss:1.0683, Accuracy:0.4082, Validation Loss:1.0791, Validation Accuracy:0.3448\n",
    "Epoch #106: Loss:1.0667, Accuracy:0.4049, Validation Loss:1.0794, Validation Accuracy:0.3875\n",
    "Epoch #107: Loss:1.0648, Accuracy:0.4086, Validation Loss:1.0796, Validation Accuracy:0.3744\n",
    "Epoch #108: Loss:1.0650, Accuracy:0.4168, Validation Loss:1.0815, Validation Accuracy:0.3810\n",
    "Epoch #109: Loss:1.0673, Accuracy:0.4103, Validation Loss:1.0793, Validation Accuracy:0.3481\n",
    "Epoch #110: Loss:1.0665, Accuracy:0.4123, Validation Loss:1.0801, Validation Accuracy:0.3727\n",
    "Epoch #111: Loss:1.0653, Accuracy:0.4136, Validation Loss:1.0823, Validation Accuracy:0.3727\n",
    "Epoch #112: Loss:1.0665, Accuracy:0.4086, Validation Loss:1.0792, Validation Accuracy:0.3744\n",
    "Epoch #113: Loss:1.0648, Accuracy:0.4136, Validation Loss:1.0786, Validation Accuracy:0.3711\n",
    "Epoch #114: Loss:1.0642, Accuracy:0.4037, Validation Loss:1.0793, Validation Accuracy:0.3448\n",
    "Epoch #115: Loss:1.0646, Accuracy:0.4123, Validation Loss:1.0807, Validation Accuracy:0.3514\n",
    "Epoch #116: Loss:1.0639, Accuracy:0.4115, Validation Loss:1.0830, Validation Accuracy:0.3662\n",
    "Epoch #117: Loss:1.0637, Accuracy:0.4197, Validation Loss:1.0811, Validation Accuracy:0.3744\n",
    "Epoch #118: Loss:1.0638, Accuracy:0.4103, Validation Loss:1.0824, Validation Accuracy:0.3612\n",
    "Epoch #119: Loss:1.0639, Accuracy:0.4111, Validation Loss:1.0801, Validation Accuracy:0.3481\n",
    "Epoch #120: Loss:1.0638, Accuracy:0.4103, Validation Loss:1.0798, Validation Accuracy:0.3498\n",
    "Epoch #121: Loss:1.0638, Accuracy:0.4049, Validation Loss:1.0787, Validation Accuracy:0.3415\n",
    "Epoch #122: Loss:1.0630, Accuracy:0.4160, Validation Loss:1.0776, Validation Accuracy:0.3596\n",
    "Epoch #123: Loss:1.0638, Accuracy:0.4099, Validation Loss:1.0788, Validation Accuracy:0.3760\n",
    "Epoch #124: Loss:1.0671, Accuracy:0.4049, Validation Loss:1.0788, Validation Accuracy:0.3777\n",
    "Epoch #125: Loss:1.0645, Accuracy:0.4012, Validation Loss:1.0793, Validation Accuracy:0.3826\n",
    "Epoch #126: Loss:1.0633, Accuracy:0.4082, Validation Loss:1.0786, Validation Accuracy:0.3760\n",
    "Epoch #127: Loss:1.0611, Accuracy:0.4152, Validation Loss:1.0814, Validation Accuracy:0.3695\n",
    "Epoch #128: Loss:1.0633, Accuracy:0.4099, Validation Loss:1.0817, Validation Accuracy:0.3465\n",
    "Epoch #129: Loss:1.0614, Accuracy:0.4185, Validation Loss:1.0827, Validation Accuracy:0.3711\n",
    "Epoch #130: Loss:1.0621, Accuracy:0.4078, Validation Loss:1.0828, Validation Accuracy:0.3547\n",
    "Epoch #131: Loss:1.0608, Accuracy:0.4160, Validation Loss:1.0825, Validation Accuracy:0.3662\n",
    "Epoch #132: Loss:1.0622, Accuracy:0.4205, Validation Loss:1.0840, Validation Accuracy:0.3530\n",
    "Epoch #133: Loss:1.0630, Accuracy:0.4115, Validation Loss:1.0878, Validation Accuracy:0.3563\n",
    "Epoch #134: Loss:1.0683, Accuracy:0.4029, Validation Loss:1.0892, Validation Accuracy:0.3645\n",
    "Epoch #135: Loss:1.0651, Accuracy:0.4144, Validation Loss:1.0869, Validation Accuracy:0.3612\n",
    "Epoch #136: Loss:1.0651, Accuracy:0.4066, Validation Loss:1.0836, Validation Accuracy:0.3826\n",
    "Epoch #137: Loss:1.0654, Accuracy:0.4107, Validation Loss:1.0844, Validation Accuracy:0.3727\n",
    "Epoch #138: Loss:1.0643, Accuracy:0.4119, Validation Loss:1.0831, Validation Accuracy:0.3580\n",
    "Epoch #139: Loss:1.0638, Accuracy:0.4082, Validation Loss:1.0827, Validation Accuracy:0.3859\n",
    "Epoch #140: Loss:1.0631, Accuracy:0.4115, Validation Loss:1.0831, Validation Accuracy:0.3612\n",
    "Epoch #141: Loss:1.0621, Accuracy:0.4172, Validation Loss:1.0836, Validation Accuracy:0.3645\n",
    "Epoch #142: Loss:1.0614, Accuracy:0.4127, Validation Loss:1.0829, Validation Accuracy:0.3596\n",
    "Epoch #143: Loss:1.0608, Accuracy:0.4074, Validation Loss:1.0827, Validation Accuracy:0.3645\n",
    "Epoch #144: Loss:1.0592, Accuracy:0.4251, Validation Loss:1.0836, Validation Accuracy:0.3662\n",
    "Epoch #145: Loss:1.0603, Accuracy:0.4127, Validation Loss:1.0806, Validation Accuracy:0.3777\n",
    "Epoch #146: Loss:1.0592, Accuracy:0.4127, Validation Loss:1.0848, Validation Accuracy:0.3645\n",
    "Epoch #147: Loss:1.0584, Accuracy:0.4156, Validation Loss:1.0853, Validation Accuracy:0.3596\n",
    "Epoch #148: Loss:1.0566, Accuracy:0.4164, Validation Loss:1.0890, Validation Accuracy:0.3810\n",
    "Epoch #149: Loss:1.0594, Accuracy:0.4070, Validation Loss:1.0836, Validation Accuracy:0.3678\n",
    "Epoch #150: Loss:1.0588, Accuracy:0.4230, Validation Loss:1.0851, Validation Accuracy:0.3498\n",
    "Epoch #151: Loss:1.0594, Accuracy:0.4136, Validation Loss:1.0862, Validation Accuracy:0.3645\n",
    "Epoch #152: Loss:1.0612, Accuracy:0.4103, Validation Loss:1.0823, Validation Accuracy:0.3777\n",
    "Epoch #153: Loss:1.0598, Accuracy:0.4123, Validation Loss:1.0857, Validation Accuracy:0.3826\n",
    "Epoch #154: Loss:1.0594, Accuracy:0.4193, Validation Loss:1.0805, Validation Accuracy:0.3514\n",
    "Epoch #155: Loss:1.0576, Accuracy:0.4181, Validation Loss:1.0819, Validation Accuracy:0.3530\n",
    "Epoch #156: Loss:1.0566, Accuracy:0.4103, Validation Loss:1.0852, Validation Accuracy:0.3727\n",
    "Epoch #157: Loss:1.0569, Accuracy:0.4107, Validation Loss:1.0814, Validation Accuracy:0.3530\n",
    "Epoch #158: Loss:1.0550, Accuracy:0.4226, Validation Loss:1.0820, Validation Accuracy:0.3481\n",
    "Epoch #159: Loss:1.0554, Accuracy:0.4160, Validation Loss:1.0821, Validation Accuracy:0.3498\n",
    "Epoch #160: Loss:1.0554, Accuracy:0.4218, Validation Loss:1.0852, Validation Accuracy:0.3547\n",
    "Epoch #161: Loss:1.0552, Accuracy:0.4193, Validation Loss:1.0853, Validation Accuracy:0.3530\n",
    "Epoch #162: Loss:1.0573, Accuracy:0.4119, Validation Loss:1.0831, Validation Accuracy:0.3875\n",
    "Epoch #163: Loss:1.0569, Accuracy:0.4181, Validation Loss:1.0850, Validation Accuracy:0.3793\n",
    "Epoch #164: Loss:1.0568, Accuracy:0.4152, Validation Loss:1.0840, Validation Accuracy:0.4007\n",
    "Epoch #165: Loss:1.0592, Accuracy:0.4144, Validation Loss:1.0883, Validation Accuracy:0.3777\n",
    "Epoch #166: Loss:1.0558, Accuracy:0.4185, Validation Loss:1.0886, Validation Accuracy:0.3859\n",
    "Epoch #167: Loss:1.0558, Accuracy:0.4160, Validation Loss:1.0919, Validation Accuracy:0.3842\n",
    "Epoch #168: Loss:1.0556, Accuracy:0.4181, Validation Loss:1.0886, Validation Accuracy:0.3859\n",
    "Epoch #169: Loss:1.0582, Accuracy:0.4172, Validation Loss:1.0864, Validation Accuracy:0.3727\n",
    "Epoch #170: Loss:1.0567, Accuracy:0.4123, Validation Loss:1.0917, Validation Accuracy:0.3695\n",
    "Epoch #171: Loss:1.0582, Accuracy:0.4082, Validation Loss:1.0857, Validation Accuracy:0.3596\n",
    "Epoch #172: Loss:1.0625, Accuracy:0.4103, Validation Loss:1.0876, Validation Accuracy:0.3678\n",
    "Epoch #173: Loss:1.0625, Accuracy:0.4053, Validation Loss:1.0901, Validation Accuracy:0.3662\n",
    "Epoch #174: Loss:1.0619, Accuracy:0.4115, Validation Loss:1.0860, Validation Accuracy:0.3662\n",
    "Epoch #175: Loss:1.0599, Accuracy:0.4094, Validation Loss:1.0883, Validation Accuracy:0.3662\n",
    "Epoch #176: Loss:1.0585, Accuracy:0.4090, Validation Loss:1.0891, Validation Accuracy:0.3629\n",
    "Epoch #177: Loss:1.0577, Accuracy:0.4189, Validation Loss:1.0933, Validation Accuracy:0.3678\n",
    "Epoch #178: Loss:1.0556, Accuracy:0.4185, Validation Loss:1.0966, Validation Accuracy:0.3695\n",
    "Epoch #179: Loss:1.0549, Accuracy:0.4160, Validation Loss:1.0904, Validation Accuracy:0.3563\n",
    "Epoch #180: Loss:1.0542, Accuracy:0.4259, Validation Loss:1.0900, Validation Accuracy:0.3826\n",
    "Epoch #181: Loss:1.0539, Accuracy:0.4181, Validation Loss:1.0882, Validation Accuracy:0.3498\n",
    "Epoch #182: Loss:1.0549, Accuracy:0.4193, Validation Loss:1.0882, Validation Accuracy:0.3612\n",
    "Epoch #183: Loss:1.0556, Accuracy:0.4164, Validation Loss:1.0929, Validation Accuracy:0.3777\n",
    "Epoch #184: Loss:1.0558, Accuracy:0.4181, Validation Loss:1.0991, Validation Accuracy:0.3777\n",
    "Epoch #185: Loss:1.0568, Accuracy:0.4193, Validation Loss:1.1032, Validation Accuracy:0.3580\n",
    "Epoch #186: Loss:1.0565, Accuracy:0.4181, Validation Loss:1.1122, Validation Accuracy:0.3662\n",
    "Epoch #187: Loss:1.0576, Accuracy:0.4074, Validation Loss:1.1092, Validation Accuracy:0.3645\n",
    "Epoch #188: Loss:1.0593, Accuracy:0.4164, Validation Loss:1.1024, Validation Accuracy:0.3547\n",
    "Epoch #189: Loss:1.0568, Accuracy:0.4181, Validation Loss:1.0908, Validation Accuracy:0.3596\n",
    "Epoch #190: Loss:1.0552, Accuracy:0.4131, Validation Loss:1.0918, Validation Accuracy:0.3514\n",
    "Epoch #191: Loss:1.0610, Accuracy:0.4045, Validation Loss:1.0892, Validation Accuracy:0.3662\n",
    "Epoch #192: Loss:1.0613, Accuracy:0.4094, Validation Loss:1.0838, Validation Accuracy:0.3744\n",
    "Epoch #193: Loss:1.0606, Accuracy:0.4111, Validation Loss:1.0828, Validation Accuracy:0.3711\n",
    "Epoch #194: Loss:1.0592, Accuracy:0.4152, Validation Loss:1.0826, Validation Accuracy:0.3645\n",
    "Epoch #195: Loss:1.0576, Accuracy:0.4127, Validation Loss:1.0866, Validation Accuracy:0.3859\n",
    "Epoch #196: Loss:1.0559, Accuracy:0.4160, Validation Loss:1.0894, Validation Accuracy:0.3908\n",
    "Epoch #197: Loss:1.0548, Accuracy:0.4148, Validation Loss:1.0895, Validation Accuracy:0.3810\n",
    "Epoch #198: Loss:1.0562, Accuracy:0.4193, Validation Loss:1.0936, Validation Accuracy:0.3810\n",
    "Epoch #199: Loss:1.0572, Accuracy:0.4144, Validation Loss:1.0939, Validation Accuracy:0.3793\n",
    "Epoch #200: Loss:1.0542, Accuracy:0.4136, Validation Loss:1.1171, Validation Accuracy:0.3760\n",
    "Epoch #201: Loss:1.0595, Accuracy:0.4148, Validation Loss:1.1022, Validation Accuracy:0.3727\n",
    "Epoch #202: Loss:1.0619, Accuracy:0.4090, Validation Loss:1.0826, Validation Accuracy:0.3810\n",
    "Epoch #203: Loss:1.0581, Accuracy:0.4099, Validation Loss:1.0927, Validation Accuracy:0.3793\n",
    "Epoch #204: Loss:1.0567, Accuracy:0.4123, Validation Loss:1.0855, Validation Accuracy:0.3892\n",
    "Epoch #205: Loss:1.0586, Accuracy:0.4103, Validation Loss:1.0839, Validation Accuracy:0.3908\n",
    "Epoch #206: Loss:1.0573, Accuracy:0.4131, Validation Loss:1.0841, Validation Accuracy:0.3629\n",
    "Epoch #207: Loss:1.0547, Accuracy:0.4209, Validation Loss:1.0824, Validation Accuracy:0.3760\n",
    "Epoch #208: Loss:1.0551, Accuracy:0.4218, Validation Loss:1.0836, Validation Accuracy:0.3727\n",
    "Epoch #209: Loss:1.0527, Accuracy:0.4152, Validation Loss:1.0884, Validation Accuracy:0.3645\n",
    "Epoch #210: Loss:1.0516, Accuracy:0.4279, Validation Loss:1.0917, Validation Accuracy:0.3744\n",
    "Epoch #211: Loss:1.0504, Accuracy:0.4222, Validation Loss:1.0926, Validation Accuracy:0.3580\n",
    "Epoch #212: Loss:1.0494, Accuracy:0.4214, Validation Loss:1.0943, Validation Accuracy:0.3777\n",
    "Epoch #213: Loss:1.0510, Accuracy:0.4308, Validation Loss:1.1000, Validation Accuracy:0.3760\n",
    "Epoch #214: Loss:1.0540, Accuracy:0.4279, Validation Loss:1.1023, Validation Accuracy:0.3596\n",
    "Epoch #215: Loss:1.0518, Accuracy:0.4103, Validation Loss:1.0956, Validation Accuracy:0.3810\n",
    "Epoch #216: Loss:1.0533, Accuracy:0.4234, Validation Loss:1.0963, Validation Accuracy:0.3826\n",
    "Epoch #217: Loss:1.0547, Accuracy:0.4156, Validation Loss:1.0945, Validation Accuracy:0.3580\n",
    "Epoch #218: Loss:1.0524, Accuracy:0.4074, Validation Loss:1.0970, Validation Accuracy:0.3629\n",
    "Epoch #219: Loss:1.0534, Accuracy:0.4164, Validation Loss:1.0978, Validation Accuracy:0.3645\n",
    "Epoch #220: Loss:1.0562, Accuracy:0.4049, Validation Loss:1.1016, Validation Accuracy:0.3547\n",
    "Epoch #221: Loss:1.0555, Accuracy:0.4160, Validation Loss:1.1049, Validation Accuracy:0.3596\n",
    "Epoch #222: Loss:1.0569, Accuracy:0.3951, Validation Loss:1.1226, Validation Accuracy:0.3727\n",
    "Epoch #223: Loss:1.0590, Accuracy:0.4156, Validation Loss:1.1139, Validation Accuracy:0.3629\n",
    "Epoch #224: Loss:1.0566, Accuracy:0.4160, Validation Loss:1.1113, Validation Accuracy:0.3645\n",
    "Epoch #225: Loss:1.0705, Accuracy:0.4160, Validation Loss:1.0973, Validation Accuracy:0.3924\n",
    "Epoch #226: Loss:1.0622, Accuracy:0.4177, Validation Loss:1.1082, Validation Accuracy:0.3678\n",
    "Epoch #227: Loss:1.0619, Accuracy:0.4099, Validation Loss:1.0826, Validation Accuracy:0.3711\n",
    "Epoch #228: Loss:1.0586, Accuracy:0.4144, Validation Loss:1.0779, Validation Accuracy:0.3744\n",
    "Epoch #229: Loss:1.0595, Accuracy:0.4148, Validation Loss:1.0781, Validation Accuracy:0.3695\n",
    "Epoch #230: Loss:1.0592, Accuracy:0.4164, Validation Loss:1.0821, Validation Accuracy:0.3629\n",
    "Epoch #231: Loss:1.0584, Accuracy:0.4156, Validation Loss:1.0820, Validation Accuracy:0.3695\n",
    "Epoch #232: Loss:1.0570, Accuracy:0.4226, Validation Loss:1.0834, Validation Accuracy:0.3695\n",
    "Epoch #233: Loss:1.0547, Accuracy:0.4205, Validation Loss:1.0893, Validation Accuracy:0.3596\n",
    "Epoch #234: Loss:1.0535, Accuracy:0.4181, Validation Loss:1.0983, Validation Accuracy:0.3530\n",
    "Epoch #235: Loss:1.0542, Accuracy:0.4209, Validation Loss:1.0935, Validation Accuracy:0.3678\n",
    "Epoch #236: Loss:1.0541, Accuracy:0.4205, Validation Loss:1.0942, Validation Accuracy:0.3777\n",
    "Epoch #237: Loss:1.0528, Accuracy:0.4259, Validation Loss:1.0920, Validation Accuracy:0.3612\n",
    "Epoch #238: Loss:1.0534, Accuracy:0.4218, Validation Loss:1.0979, Validation Accuracy:0.3563\n",
    "Epoch #239: Loss:1.0547, Accuracy:0.4287, Validation Loss:1.0957, Validation Accuracy:0.3662\n",
    "Epoch #240: Loss:1.0603, Accuracy:0.4255, Validation Loss:1.1099, Validation Accuracy:0.3432\n",
    "Epoch #241: Loss:1.0613, Accuracy:0.4160, Validation Loss:1.1056, Validation Accuracy:0.3777\n",
    "Epoch #242: Loss:1.0649, Accuracy:0.4140, Validation Loss:1.0938, Validation Accuracy:0.3941\n",
    "Epoch #243: Loss:1.0591, Accuracy:0.4144, Validation Loss:1.0890, Validation Accuracy:0.3563\n",
    "Epoch #244: Loss:1.0618, Accuracy:0.4144, Validation Loss:1.0899, Validation Accuracy:0.3612\n",
    "Epoch #245: Loss:1.0614, Accuracy:0.4168, Validation Loss:1.0910, Validation Accuracy:0.3695\n",
    "Epoch #246: Loss:1.0651, Accuracy:0.4140, Validation Loss:1.0944, Validation Accuracy:0.3596\n",
    "Epoch #247: Loss:1.0635, Accuracy:0.4205, Validation Loss:1.0910, Validation Accuracy:0.3777\n",
    "Epoch #248: Loss:1.0601, Accuracy:0.4181, Validation Loss:1.0893, Validation Accuracy:0.3727\n",
    "Epoch #249: Loss:1.0623, Accuracy:0.4152, Validation Loss:1.0905, Validation Accuracy:0.3810\n",
    "Epoch #250: Loss:1.0588, Accuracy:0.4131, Validation Loss:1.0863, Validation Accuracy:0.3744\n",
    "Epoch #251: Loss:1.0573, Accuracy:0.4189, Validation Loss:1.0849, Validation Accuracy:0.3695\n",
    "Epoch #252: Loss:1.0576, Accuracy:0.4197, Validation Loss:1.0886, Validation Accuracy:0.3744\n",
    "Epoch #253: Loss:1.0546, Accuracy:0.4238, Validation Loss:1.0972, Validation Accuracy:0.3563\n",
    "Epoch #254: Loss:1.0552, Accuracy:0.4193, Validation Loss:1.1032, Validation Accuracy:0.3793\n",
    "Epoch #255: Loss:1.0547, Accuracy:0.4222, Validation Loss:1.1000, Validation Accuracy:0.3662\n",
    "Epoch #256: Loss:1.0530, Accuracy:0.4263, Validation Loss:1.0966, Validation Accuracy:0.3547\n",
    "Epoch #257: Loss:1.0520, Accuracy:0.4246, Validation Loss:1.0900, Validation Accuracy:0.3596\n",
    "Epoch #258: Loss:1.0527, Accuracy:0.4320, Validation Loss:1.0938, Validation Accuracy:0.3793\n",
    "Epoch #259: Loss:1.0517, Accuracy:0.4177, Validation Loss:1.0948, Validation Accuracy:0.3612\n",
    "Epoch #260: Loss:1.0521, Accuracy:0.4271, Validation Loss:1.1006, Validation Accuracy:0.3530\n",
    "Epoch #261: Loss:1.0530, Accuracy:0.4168, Validation Loss:1.1058, Validation Accuracy:0.3826\n",
    "Epoch #262: Loss:1.0544, Accuracy:0.4168, Validation Loss:1.1081, Validation Accuracy:0.3612\n",
    "Epoch #263: Loss:1.0522, Accuracy:0.4152, Validation Loss:1.1132, Validation Accuracy:0.3810\n",
    "Epoch #264: Loss:1.0547, Accuracy:0.4115, Validation Loss:1.1147, Validation Accuracy:0.3727\n",
    "Epoch #265: Loss:1.0561, Accuracy:0.4177, Validation Loss:1.1083, Validation Accuracy:0.3678\n",
    "Epoch #266: Loss:1.0551, Accuracy:0.4234, Validation Loss:1.1003, Validation Accuracy:0.3612\n",
    "Epoch #267: Loss:1.0543, Accuracy:0.4246, Validation Loss:1.0985, Validation Accuracy:0.3810\n",
    "Epoch #268: Loss:1.0558, Accuracy:0.4209, Validation Loss:1.0924, Validation Accuracy:0.3711\n",
    "Epoch #269: Loss:1.0558, Accuracy:0.4193, Validation Loss:1.0939, Validation Accuracy:0.3563\n",
    "Epoch #270: Loss:1.0547, Accuracy:0.4222, Validation Loss:1.0993, Validation Accuracy:0.3744\n",
    "Epoch #271: Loss:1.0555, Accuracy:0.4283, Validation Loss:1.1079, Validation Accuracy:0.3727\n",
    "Epoch #272: Loss:1.0564, Accuracy:0.4160, Validation Loss:1.0974, Validation Accuracy:0.3744\n",
    "Epoch #273: Loss:1.0547, Accuracy:0.4201, Validation Loss:1.0992, Validation Accuracy:0.3645\n",
    "Epoch #274: Loss:1.0551, Accuracy:0.4144, Validation Loss:1.0997, Validation Accuracy:0.3662\n",
    "Epoch #275: Loss:1.0535, Accuracy:0.4234, Validation Loss:1.0940, Validation Accuracy:0.3711\n",
    "Epoch #276: Loss:1.0538, Accuracy:0.4222, Validation Loss:1.0952, Validation Accuracy:0.3662\n",
    "Epoch #277: Loss:1.0539, Accuracy:0.4263, Validation Loss:1.0976, Validation Accuracy:0.3612\n",
    "Epoch #278: Loss:1.0524, Accuracy:0.4287, Validation Loss:1.1007, Validation Accuracy:0.3547\n",
    "Epoch #279: Loss:1.0513, Accuracy:0.4209, Validation Loss:1.1046, Validation Accuracy:0.3563\n",
    "Epoch #280: Loss:1.0495, Accuracy:0.4287, Validation Loss:1.1021, Validation Accuracy:0.3612\n",
    "Epoch #281: Loss:1.0509, Accuracy:0.4287, Validation Loss:1.1026, Validation Accuracy:0.3695\n",
    "Epoch #282: Loss:1.0504, Accuracy:0.4324, Validation Loss:1.1159, Validation Accuracy:0.3711\n",
    "Epoch #283: Loss:1.0539, Accuracy:0.4209, Validation Loss:1.1196, Validation Accuracy:0.3662\n",
    "Epoch #284: Loss:1.0529, Accuracy:0.4214, Validation Loss:1.1136, Validation Accuracy:0.3678\n",
    "Epoch #285: Loss:1.0528, Accuracy:0.4238, Validation Loss:1.1108, Validation Accuracy:0.3662\n",
    "Epoch #286: Loss:1.0527, Accuracy:0.4209, Validation Loss:1.1113, Validation Accuracy:0.3678\n",
    "Epoch #287: Loss:1.0523, Accuracy:0.4185, Validation Loss:1.1145, Validation Accuracy:0.3563\n",
    "Epoch #288: Loss:1.0516, Accuracy:0.4255, Validation Loss:1.1118, Validation Accuracy:0.3547\n",
    "Epoch #289: Loss:1.0522, Accuracy:0.4234, Validation Loss:1.1139, Validation Accuracy:0.3612\n",
    "Epoch #290: Loss:1.0514, Accuracy:0.4218, Validation Loss:1.1104, Validation Accuracy:0.3580\n",
    "Epoch #291: Loss:1.0526, Accuracy:0.4246, Validation Loss:1.1172, Validation Accuracy:0.3530\n",
    "Epoch #292: Loss:1.0528, Accuracy:0.4119, Validation Loss:1.1231, Validation Accuracy:0.3645\n",
    "Epoch #293: Loss:1.0551, Accuracy:0.4251, Validation Loss:1.1208, Validation Accuracy:0.3662\n",
    "Epoch #294: Loss:1.0603, Accuracy:0.4090, Validation Loss:1.1107, Validation Accuracy:0.3563\n",
    "Epoch #295: Loss:1.0597, Accuracy:0.4218, Validation Loss:1.1093, Validation Accuracy:0.3645\n",
    "Epoch #296: Loss:1.0579, Accuracy:0.4205, Validation Loss:1.1094, Validation Accuracy:0.3695\n",
    "Epoch #297: Loss:1.0550, Accuracy:0.4222, Validation Loss:1.1054, Validation Accuracy:0.3678\n",
    "Epoch #298: Loss:1.0551, Accuracy:0.4242, Validation Loss:1.1029, Validation Accuracy:0.3662\n",
    "Epoch #299: Loss:1.0538, Accuracy:0.4251, Validation Loss:1.0973, Validation Accuracy:0.3629\n",
    "Epoch #300: Loss:1.0567, Accuracy:0.4263, Validation Loss:1.0861, Validation Accuracy:0.3612\n",
    "\n",
    "Test:\n",
    "Test Loss:1.08610058, Accuracy:0.3612\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03   01  02\n",
    "t:03   0  108  34\n",
    "t:01   0  161  79\n",
    "t:02   0  168  59\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.37      0.67      0.48       240\n",
    "          02       0.34      0.26      0.30       227\n",
    "\n",
    "    accuracy                           0.36       609\n",
    "   macro avg       0.24      0.31      0.26       609\n",
    "weighted avg       0.27      0.36      0.30       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 12:58:32 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 56 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0829961906708716, 1.0774058267988007, 1.074968402804608, 1.0750827509389917, 1.0750926047906109, 1.0742897063444792, 1.075279605995454, 1.0747591399989889, 1.0748700890047798, 1.0746695034218148, 1.0742379419126338, 1.0742809664831177, 1.0742007184694162, 1.074468399699294, 1.0743084803394887, 1.0745507400415606, 1.0744855259048136, 1.0745285939309006, 1.0744974345017733, 1.074306723323753, 1.0743814642206202, 1.074291765004739, 1.0746772964599685, 1.0746267628787187, 1.0741393452598935, 1.0746601806289848, 1.074899297433925, 1.0749552594421337, 1.0751491802666575, 1.0749510027504907, 1.0750562097442953, 1.074825927346015, 1.0750281528886316, 1.0745245335724554, 1.0748068967280522, 1.0766453799747286, 1.0765559068454311, 1.0765324325984336, 1.0770224066594942, 1.078203929468916, 1.078690660215168, 1.078760920095522, 1.0776555146685571, 1.078077412003954, 1.0770245105370708, 1.0772454196400634, 1.0777861631562557, 1.0794261259398437, 1.0759050170776292, 1.0762519585870953, 1.0769827886559498, 1.0784843678544895, 1.077439351230615, 1.0777809122708826, 1.079206372716744, 1.0784631525158686, 1.0773934409731911, 1.0802139335469463, 1.078697505842876, 1.077324145728927, 1.0767683383866484, 1.0803499194397324, 1.080954226562738, 1.0800142454592074, 1.0775613592959, 1.0782642521098722, 1.078917223440211, 1.079932279970454, 1.0794872928331247, 1.0826034888453868, 1.0814038650155655, 1.07800459685584, 1.0749076808418938, 1.0753503196149428, 1.074733692045478, 1.0746609996300809, 1.0740911528003236, 1.076441018452198, 1.0768561735137538, 1.0776783932605987, 1.0776219845601098, 1.0778405826863005, 1.0788858261797425, 1.0777108917878375, 1.0780250231424968, 1.0767141698029241, 1.0760171742274844, 1.075893730952822, 1.074883988338151, 1.076764103618553, 1.078058255521339, 1.077294384904683, 1.0761984181521562, 1.0759393935916068, 1.076703946187187, 1.0818331909101389, 1.0802242881167308, 1.080345060242025, 1.0785550597461768, 1.0780444141287717, 1.0804243651516918, 1.0797345211548954, 1.0795095456253327, 1.0801581351823604, 1.0790670423084878, 1.079380970087349, 1.0795545587790227, 1.0814578010530895, 1.079276939722509, 1.0800541625625786, 1.0823149998199764, 1.079151762921626, 1.0786222638363516, 1.079298749933102, 1.080747989793912, 1.0829832401181676, 1.0810573876197702, 1.0824248252439577, 1.0800541784180013, 1.0797608063138764, 1.078691123154363, 1.0776205248824873, 1.0787578329859893, 1.0788070384309014, 1.0793324278297487, 1.0785976894970597, 1.0814062597716383, 1.0817072319084005, 1.0827098401700725, 1.082822215576673, 1.0824898420687772, 1.0839694910644506, 1.0878344349477482, 1.0892012199549057, 1.0869328357120258, 1.083552565284942, 1.084412884633921, 1.0830572295462948, 1.0826934278500686, 1.0830570165746904, 1.0835606668187283, 1.0829076113176268, 1.0827252535984433, 1.0835826627922371, 1.0806255322959035, 1.084844523071264, 1.08528791781521, 1.0890142441970374, 1.083594477431136, 1.0851005043693756, 1.0861881660123176, 1.0822976868728111, 1.0856599328161656, 1.0805392398427076, 1.081887799922273, 1.0852317700440857, 1.0813685430486017, 1.0819919101514643, 1.0821156732750252, 1.0851579329063152, 1.0852828186329557, 1.0831385555329973, 1.0850114524853836, 1.083993102529366, 1.088305774189177, 1.0885611889984808, 1.0919238820256074, 1.0885540730456023, 1.0864123687368308, 1.0916530023067457, 1.0856715393771093, 1.0876351156453976, 1.0900783517286303, 1.0860312652509592, 1.0883367457021829, 1.0891178965764288, 1.0933426524617988, 1.0965689033123072, 1.0903963013039826, 1.0900120032440461, 1.088165283594617, 1.0882064215655398, 1.0929200690368126, 1.099136554940385, 1.1031761664670872, 1.1122136190411296, 1.109208857288893, 1.1023568727308501, 1.0907908642820536, 1.091773719427425, 1.089184284405951, 1.083807251919275, 1.0827539992841397, 1.0825697214928363, 1.086638497210097, 1.089364655107896, 1.0894563111961377, 1.0936234320325804, 1.0939187915454358, 1.117072621394065, 1.102195431446207, 1.0825916039336883, 1.0926976896859155, 1.0854959566213422, 1.0839321644631121, 1.0840891722975106, 1.0823840541009637, 1.083621723311288, 1.0884228471073218, 1.0917312597797812, 1.0925944967222918, 1.0943151209350486, 1.100046738810923, 1.10227019191767, 1.0955638638858138, 1.096265479653144, 1.0945004786568127, 1.0969830747504148, 1.0978425862558174, 1.1016208902368405, 1.1049465855158414, 1.1226132781243285, 1.1138969963211536, 1.1112791703056624, 1.0972690361082456, 1.1082382912706272, 1.0825610258700618, 1.0778670725955557, 1.078123463981453, 1.0820895333595464, 1.08199109349932, 1.0834123747689384, 1.0892815840459613, 1.0982904622120222, 1.0935431415420056, 1.0942066306942593, 1.0919594676623792, 1.0979411224230562, 1.0956685337527046, 1.1098979280891481, 1.1056385925054941, 1.0937546986860203, 1.0890383873079799, 1.0898582534054033, 1.0909832013260163, 1.0944473101391972, 1.091019317630085, 1.0892552562143611, 1.0905407917714862, 1.0862543165977365, 1.0848995077198949, 1.0885632300416042, 1.0972029757617143, 1.103198401054921, 1.0999839031833343, 1.0965525861248397, 1.0900440004658816, 1.0938381659377776, 1.0948425425684511, 1.1006034182014528, 1.1057506190927942, 1.1080781501306494, 1.1132077947625973, 1.1146524692403859, 1.1082808110122806, 1.100312239430808, 1.0984561126220402, 1.09244491646834, 1.0939404067930525, 1.099307114267584, 1.1079108991059177, 1.0973954378873452, 1.0992325710741366, 1.0997100776835225, 1.0940289237033363, 1.0951831888878483, 1.0975551382074216, 1.1007059468032887, 1.1045594119477546, 1.1020938481016112, 1.1025529258161146, 1.1159152616616737, 1.1196037595495214, 1.1136004605708256, 1.1108243471295962, 1.1112953720030134, 1.1144634614437086, 1.1117587878394792, 1.113895278258864, 1.1103957815123309, 1.1171856844562225, 1.1231485143279403, 1.1207605406568555, 1.11069345337221, 1.1093076629983185, 1.1094139250628468, 1.1053985457114985, 1.1028516382615163, 1.0972504801742353, 1.0861004947245807], 'val_acc': [0.39408866931456454, 0.39408866931456454, 0.39408866931456454, 0.3727421994964869, 0.3727421994964869, 0.39573070563510526, 0.39408866931456454, 0.39408866931456454, 0.39408866931456454, 0.3957307055372323, 0.3940886695103105, 0.39408866931456454, 0.39408866931456454, 0.39573070563510526, 0.3924466333855158, 0.39408866931456454, 0.3957307055372323, 0.3957307054393593, 0.4006568139116165, 0.39573070563510526, 0.39573070563510526, 0.3924466335812617, 0.39573070573297825, 0.39408866931456454, 0.3891625609401803, 0.4121510667851797, 0.4055829225796197, 0.39244663348338876, 0.3924466332876428, 0.40394088645482495, 0.3891625612337993, 0.3924466333855158, 0.3940886695103105, 0.4105090310518769, 0.38423645266366907, 0.38259441673462025, 0.38259441634312835, 0.3727421997901058, 0.37110016366531107, 0.3727421995943599, 0.36617405489943494, 0.38259441673462025, 0.37110016356743813, 0.34646962130402503, 0.38259441673462025, 0.38587848898420973, 0.3628899823562265, 0.38259441683249323, 0.3842364525657961, 0.37931034438715777, 0.3645320186767672, 0.3809523801204606, 0.37766830787087113, 0.37602627233331426, 0.3858784886905908, 0.3448275853749762, 0.3760262722354413, 0.366174054507943, 0.38423645266366907, 0.3825944161473824, 0.377668308360236, 0.3497536935536145, 0.3678160907306107, 0.390804597358594, 0.3711001630780732, 0.35796387447120714, 0.3596059106938749, 0.3645320192640051, 0.3661740550951809, 0.35960591049812896, 0.37766830826236303, 0.37110016376318405, 0.3809523801204606, 0.3891625612337993, 0.37438423562128165, 0.3940886695103105, 0.3727421994964869, 0.3579638746669531, 0.3760262716482034, 0.3694581278341353, 0.36453201936187807, 0.3563218379549205, 0.3563218385421584, 0.3612479469165426, 0.3809523801204606, 0.3760262721375683, 0.3809523807076985, 0.377668308360236, 0.3825944166367473, 0.3694581276383893, 0.3727421993986139, 0.384236452957288, 0.37110016366531107, 0.3628899823562265, 0.35960591020451, 0.3694581273447704, 0.3366174043595106, 0.35632183775917453, 0.3497536935536145, 0.3497536935536145, 0.36288998274771844, 0.3678160913178486, 0.37438423562128165, 0.34646962130402503, 0.3448275851792303, 0.3875205244238936, 0.3743842362085195, 0.3809523802183336, 0.3481116577224387, 0.3727422000837248, 0.37274219910499495, 0.37438423611064653, 0.3711001632738192, 0.3448275851792303, 0.35139572967840926, 0.36617405489943494, 0.37438423601277354, 0.3612479466229237, 0.3481116573309468, 0.34975369335786854, 0.3415435128317678, 0.359605910302383, 0.3760262718439494, 0.3776683080666171, 0.38259441634312835, 0.3760262722354413, 0.36945812695327845, 0.34646962130402503, 0.37110016376318405, 0.35467980192799875, 0.3661740553887998, 0.353037765803204, 0.3563218380527935, 0.3645320192640051, 0.36124794623143175, 0.3825944165388743, 0.3727421998879788, 0.35796387398184226, 0.3858784888863368, 0.3612479468186696, 0.36453201848102124, 0.359605910106637, 0.3645320191661321, 0.366174054605816, 0.3776683081644901, 0.3645320185788942, 0.359605910106637, 0.38095237992471465, 0.3678160908284837, 0.34975369345574153, 0.36453201838314825, 0.3776683081644901, 0.38259441585376347, 0.35139572967840926, 0.353037765705331, 0.37274219890924903, 0.35303776550958504, 0.3481116570373278, 0.34975369316212257, 0.3546798016343798, 0.35303776541171206, 0.38752052481538557, 0.3793103439956659, 0.4006568141073625, 0.3776683080666171, 0.3858784887884638, 0.38423645217430413, 0.38587848898420973, 0.3727421997901058, 0.36945812665965955, 0.359605910302383, 0.3678160908284837, 0.366174054605816, 0.36617405480156195, 0.36617405470368897, 0.36288998255197247, 0.36781609063273774, 0.36945812705115144, 0.3563218381506665, 0.38259441624525536, 0.34975369365148745, 0.3612479466229237, 0.37766830787087113, 0.37766830787087113, 0.35796387447120714, 0.36617405480156195, 0.3645320187746402, 0.35467980222161766, 0.35960591079174786, 0.3513957295805363, 0.366174054507943, 0.37438423542553567, 0.3711001632738192, 0.3645320185788942, 0.38587848849484485, 0.3908045968692291, 0.38095238002258763, 0.3809523802183336, 0.37931034419141185, 0.37602627106096553, 0.3727421995943599, 0.3809523803162066, 0.379310343310555, 0.3891625609401803, 0.39080459706497506, 0.3628899821604805, 0.3760262715503304, 0.37274219920286794, 0.36453201818740233, 0.37438423552340866, 0.35796387359035037, 0.37766830777299815, 0.3760262716482034, 0.3596059097151451, 0.38095238002258763, 0.3825944161473824, 0.3579638738839693, 0.3628899820626076, 0.36453201848102124, 0.35467980192799875, 0.359605910302383, 0.3727421995943599, 0.3628899819647346, 0.3645320186767672, 0.3924466331897698, 0.3678160910242297, 0.3711001630780732, 0.3743842353276627, 0.36945812685540547, 0.3628899820626076, 0.3694581273447704, 0.3694581273447704, 0.359605910008764, 0.35303776550958504, 0.3678160913178486, 0.377668308360236, 0.3612479464271777, 0.3563218384442854, 0.366174054605816, 0.34318554836932463, 0.3776683081644901, 0.3940886696081835, 0.3563218379549205, 0.3612479465250507, 0.3694581273447704, 0.35960591049812896, 0.3776683081644901, 0.3727421997901058, 0.3809523807076985, 0.3743842358170276, 0.3694581273447704, 0.3743842358170276, 0.3563218379549205, 0.3793103439956659, 0.36617405499730793, 0.35467980202587174, 0.35960591020451, 0.37931034428928484, 0.3612479464271777, 0.3530377659989499, 0.38259441673462025, 0.3612479468186696, 0.3809523802183336, 0.3727421995943599, 0.3678160914157216, 0.3612479469165426, 0.3809523802183336, 0.37110016356743813, 0.3563218383464124, 0.37438423562128165, 0.3727421996922329, 0.37438423601277354, 0.3645320189703862, 0.3661740550951809, 0.37110016366531107, 0.366174054605816, 0.36124794613355876, 0.3546798016343798, 0.35632183766130154, 0.36124794623143175, 0.36945812705115144, 0.37110016356743813, 0.36617405519305385, 0.3678160913178486, 0.36617405519305385, 0.36781609151359457, 0.3563218378570475, 0.35467980212374467, 0.3612479466229237, 0.35796387447120714, 0.35303776560745803, 0.3645320189703862, 0.36617405499730793, 0.35632183766130154, 0.3645320189703862, 0.36945812714902443, 0.3678160911221027, 0.36617405499730793, 0.36288998255197247, 0.3612479460356858], 'loss': [1.0940279518554343, 1.0797488636549493, 1.0750547812459894, 1.0737759654771621, 1.074337135007494, 1.0741793215397202, 1.0735897753028165, 1.074061657417971, 1.0737195727271955, 1.0737071121008244, 1.0738401526053583, 1.0739238562280393, 1.0737787576671498, 1.0734866553997846, 1.07336148143549, 1.0734152284735772, 1.0732396128241286, 1.0732656906272842, 1.0732656487204455, 1.0728402625363955, 1.0730467104569108, 1.0730900318470824, 1.0731324772080846, 1.0731146094735398, 1.072815190742148, 1.072750632816761, 1.0724685737484534, 1.0721475801918297, 1.0721195739887088, 1.0720681879309903, 1.071776188717241, 1.071905560023486, 1.0711592890643487, 1.0712085142762264, 1.0709493884315726, 1.0709849578888755, 1.0713016255925079, 1.0702296897860768, 1.0709883005467284, 1.070183304741642, 1.070705361581681, 1.0714861253938146, 1.0711696002028073, 1.071023015907413, 1.0697674906963686, 1.0718412904034404, 1.0708236231696189, 1.0706925378932601, 1.0687561503670788, 1.0707015642640036, 1.0704568073245289, 1.06964412562901, 1.0702448176162689, 1.0703565597044613, 1.0688522904805333, 1.0699122479808894, 1.0689461621415688, 1.069104799352877, 1.0713256098651298, 1.0702256362785794, 1.0680027804090746, 1.068089498925258, 1.0696612059947646, 1.0698743468682135, 1.0702499308870068, 1.0682282359693085, 1.0703914873409075, 1.070242182625882, 1.0700891295008101, 1.0699746724271677, 1.0699768272513481, 1.0706260673808856, 1.0700778341880814, 1.0702399298885275, 1.0693326666125038, 1.0699800350827602, 1.0697076340720395, 1.068957164693907, 1.0689078780904688, 1.0695383762187292, 1.0703258924660497, 1.0672147862475512, 1.068337185426904, 1.0683620510649632, 1.067745097217129, 1.0685417172355574, 1.0703221757798713, 1.068324378824332, 1.0706270035287437, 1.0693743850171444, 1.067075273878031, 1.066780843235384, 1.0684949882221417, 1.0685393366725537, 1.0689430382707035, 1.069668465473324, 1.0706461946577508, 1.067583042982912, 1.068593375393987, 1.0683382239429857, 1.067430713436197, 1.0670157836937562, 1.0663441841607222, 1.0669414953039902, 1.0683454655523907, 1.0666856940522087, 1.0648491687598414, 1.0650190419484946, 1.067257807974453, 1.0665182638462074, 1.065256390042863, 1.066500556150746, 1.0647898622606815, 1.064234188205163, 1.0645827970465596, 1.063941430947619, 1.0637476964652905, 1.063809793783654, 1.0638896432990166, 1.063766680410021, 1.0637829623427968, 1.063013844911078, 1.063791680776608, 1.0671164198332987, 1.0645039149623143, 1.0633431593740255, 1.0610919077048802, 1.0633267773739856, 1.0614095235996912, 1.062086536996908, 1.060805553136665, 1.0622192828807009, 1.0629533954714359, 1.0682736987205992, 1.0650664666839693, 1.0650582694419846, 1.0654248367834385, 1.0642501046770163, 1.0638327359174067, 1.063085693255587, 1.062111687562304, 1.0614334839814987, 1.0608384224423637, 1.0591757420396903, 1.060297035632437, 1.0592302106489146, 1.05841116121907, 1.0565799323929899, 1.0593597912935262, 1.058792272240719, 1.0594448602664641, 1.0611731725062188, 1.05975611043417, 1.059447929648648, 1.0576467281983863, 1.0566155147748317, 1.0568692789430245, 1.0550031776545719, 1.0553657042661977, 1.0553814053780244, 1.0551932378471265, 1.0572708371238786, 1.0569273184457109, 1.0567604452677577, 1.059244404632208, 1.0558246391265054, 1.0558297676227422, 1.0556111569766882, 1.058247237626532, 1.0566698842959237, 1.0582270769612745, 1.062474344936974, 1.0624832277670044, 1.0618718603553223, 1.0598720028659891, 1.0585453168567447, 1.05773703082631, 1.0555614357366698, 1.0549446901991137, 1.054181981918993, 1.0538561153705606, 1.0549166080153698, 1.055581948106049, 1.0557510431787072, 1.0568166382748487, 1.0565389182778109, 1.0575901603307079, 1.059252602804368, 1.0567678006033143, 1.0551993253539476, 1.06095848558375, 1.061322742810729, 1.0606171081198315, 1.059163527224343, 1.0575920833454484, 1.0559056380446197, 1.0548432980229967, 1.0562468308932482, 1.057244683584883, 1.0542203280470455, 1.0595101754523402, 1.0619082700055726, 1.058127470280845, 1.056703546012941, 1.0585535141966427, 1.057304314470389, 1.0547227030172486, 1.0550521004616114, 1.0527324205061739, 1.051611575796374, 1.0503667460819535, 1.0493908012672126, 1.0510252239767777, 1.0539516924832635, 1.0518256517895928, 1.0532754329195748, 1.0547073350060403, 1.052387571775448, 1.0533802485808699, 1.0561853289359404, 1.0555444788883843, 1.0568856764623027, 1.0590048745427534, 1.056563714691256, 1.0704873264692647, 1.0622042916393868, 1.0619420892159308, 1.0586363914810901, 1.0594627617320975, 1.0591784801326494, 1.0583615309403906, 1.0570466295649628, 1.0546547654228289, 1.053526781324978, 1.0542269750787003, 1.0541140632707726, 1.0528373659024248, 1.0533804347627707, 1.0547395941168376, 1.0602520596564917, 1.0612576603155117, 1.064898062878321, 1.0590603388310458, 1.0617545684505048, 1.0614241062493295, 1.0650971214384515, 1.0635242284445792, 1.060072048195089, 1.0623496609302028, 1.058753017380497, 1.0572711754138955, 1.0576270435869817, 1.0545958613957713, 1.0552385946563627, 1.0547494418811993, 1.0529902888519318, 1.051963073517018, 1.0526767085954638, 1.05167380074456, 1.0520801259262116, 1.0529891627525156, 1.0544324066604678, 1.0521775218250815, 1.0547234535217285, 1.0560771799675004, 1.0550884563085725, 1.0543414000368216, 1.0558462366675938, 1.0558116311654908, 1.054718482714659, 1.0554880549530719, 1.0564177471019893, 1.0547440395707712, 1.0551288403524755, 1.053468507516066, 1.053820320859827, 1.053892096748587, 1.0523916753655342, 1.0512523295208658, 1.0495401515608207, 1.0508896808604684, 1.0504480093656379, 1.0538992374095095, 1.0528753971906657, 1.0528073295675509, 1.0526750948394838, 1.0523022906736181, 1.0516404844652212, 1.052248648206801, 1.0513955324827033, 1.052570260280946, 1.0528166572171316, 1.0551474259864133, 1.0602767255516756, 1.0597128970422294, 1.0579195404933952, 1.0549510039832803, 1.0551459718289071, 1.0538155899889905, 1.0566737305212315], 'acc': [0.35893223695686466, 0.3942505115233897, 0.394250514301676, 0.3954825471558855, 0.37741273000255016, 0.3864476401458286, 0.3946611928988776, 0.3942505133225443, 0.3942505141058497, 0.3946611921155722, 0.3963039005928705, 0.39383983805439066, 0.3946611917239195, 0.3950718701252947, 0.3950718687545103, 0.395482548367561, 0.3946611901573087, 0.39548254735171184, 0.4045174525626141, 0.399589324119889, 0.39589322199566895, 0.4012320343963419, 0.3995893211457764, 0.4028747452602739, 0.3942505113275634, 0.3971252564164892, 0.4135523605518028, 0.4073921986673892, 0.40000000212961156, 0.3971252570406857, 0.41067761702948774, 0.40492813096398933, 0.4086242279600069, 0.4049281335464493, 0.41108829781749656, 0.4106776168336614, 0.4032854201367748, 0.41108829347259945, 0.4065708408487896, 0.4106776162094649, 0.41396303816987257, 0.4110882962141683, 0.4061601634265461, 0.4123203287134425, 0.42012320283257254, 0.401232034983821, 0.4061601664373762, 0.4053388090104293, 0.4041067759603935, 0.41437371895788144, 0.406981520266014, 0.40616016558063595, 0.40205338865334983, 0.4086242279600069, 0.4151950732148893, 0.41437371895788144, 0.40657084303959684, 0.4094455861702592, 0.39794661110186724, 0.40328542029588377, 0.40657084124044224, 0.4086242311299459, 0.41149897246145367, 0.40739219886321554, 0.4057494881951099, 0.40985626218500076, 0.4073921956932765, 0.4057494850251709, 0.39753593661701897, 0.4065708418279213, 0.40903490913966845, 0.399589324119889, 0.41355236074762913, 0.40657084202374766, 0.4073921967091257, 0.4073921982757365, 0.3999999995471516, 0.4061601622883054, 0.40985626081421633, 0.4164271060690988, 0.404928133350623, 0.40944558754104365, 0.4053388070154484, 0.40944558378362556, 0.411498974652261, 0.41149897304893274, 0.40739219804319265, 0.41601642923433435, 0.3880903472523425, 0.41273100731064405, 0.41108829546758036, 0.4094455857786065, 0.4049281313923595, 0.40574948522099724, 0.3950718707494912, 0.40492813374227565, 0.3934291573030993, 0.4106776158545296, 0.39958932098666744, 0.4094455839794519, 0.41478439500934045, 0.41519507063242933, 0.4110882976216702, 0.40123203361303655, 0.4082135505744809, 0.404928133350623, 0.40862423132577225, 0.4168377807130559, 0.4102669388239389, 0.4123203298884006, 0.4135523623509573, 0.40862422893913863, 0.41355235976849736, 0.4036960967389955, 0.41232032789341966, 0.4114989716781483, 0.41971252740530995, 0.4102669401947233, 0.41108829464755753, 0.41026693960724425, 0.40492813315479664, 0.4160164294301607, 0.4098562628091973, 0.40492813276314393, 0.401232034983821, 0.40821355132106885, 0.41519507239486647, 0.40985626218500076, 0.41848049059785614, 0.4078028760896326, 0.41601642704352704, 0.4205338826414496, 0.4114989736731292, 0.4028747444769685, 0.41437371461298433, 0.4065708418279213, 0.41067761745785786, 0.4119096492962181, 0.40821355429518147, 0.41149897148232195, 0.417248457939473, 0.4127310060989685, 0.40739219647658187, 0.42505133683187024, 0.412731006135686, 0.4127310065273387, 0.41560574785884646, 0.41642710567744606, 0.4069815216367984, 0.42299794795821577, 0.4135523630975453, 0.41026694101474614, 0.41232032730594065, 0.41930184583399577, 0.4180698163455517, 0.4102669382364598, 0.41067761663783503, 0.42258726795351237, 0.4160164284143115, 0.42176591408815717, 0.41930184978724017, 0.41190964949204445, 0.41806981552552885, 0.415195070436603, 0.41437371895788144, 0.41848049157698786, 0.41601642923433435, 0.41806981556224626, 0.41724846110941205, 0.41232032969257426, 0.4082135501461107, 0.41026694199387786, 0.4053388103812137, 0.4114989740280645, 0.4094455861702592, 0.40903490616555577, 0.41889116942760146, 0.4184804923602933, 0.4160164282552026, 0.42587269007302897, 0.41806981556224626, 0.4193018468131275, 0.41642710681568673, 0.4180698163455517, 0.4193018468131275, 0.4180698169330307, 0.40739219905904184, 0.41642710528579335, 0.4180698169330307, 0.41314168175877486, 0.4045174551450741, 0.4094455830003202, 0.41108829488010135, 0.4151950739981947, 0.41273100492401044, 0.41601642766772357, 0.4147843928185332, 0.41930184939558746, 0.41437371461298433, 0.4135523625467837, 0.4147843937976649, 0.4090349079647103, 0.40985626198917446, 0.4123203286767251, 0.4102669427771833, 0.41314168175877486, 0.4209445584603648, 0.42176591173824096, 0.41519506961658015, 0.4279260761683971, 0.4221765928811851, 0.42135523607843467, 0.4308008220773458, 0.42792607777172537, 0.4102669384322862, 0.423408623189652, 0.41560574864215183, 0.4073921984715628, 0.4164271027033334, 0.4049281295932049, 0.4160164282552026, 0.3950718673470084, 0.4156057502454801, 0.4160164266518744, 0.4160164250852636, 0.41765913575336916, 0.40985626355578525, 0.414373715004637, 0.41478439579264587, 0.41642710626492513, 0.4156057515795471, 0.42258726756185966, 0.4205338800589896, 0.4180698153297025, 0.42094455826453847, 0.4205338786882052, 0.4258726905013991, 0.4217659129131991, 0.4287474353944986, 0.4254620110841747, 0.416016429038508, 0.4139630401648535, 0.41437371856622873, 0.4143737148088107, 0.41683778032140323, 0.4139630399323097, 0.4205338826414496, 0.41806981517059355, 0.415195070436603, 0.41314168610367197, 0.418891169390884, 0.41971252501867634, 0.4238193031943554, 0.41930184896721734, 0.4221765924895324, 0.4262833669077934, 0.42464065843049503, 0.43203285336494446, 0.4176591353617165, 0.42710472312306474, 0.4168377829038632, 0.416837782475493, 0.4151950725906928, 0.4114989756313927, 0.4176591365366746, 0.42340862577211197, 0.42464065764718967, 0.42094456025951943, 0.41930184978724017, 0.42217659013961617, 0.4283367575806024, 0.4160164266518744, 0.4201232010701354, 0.4143737161795951, 0.42340862538045926, 0.42217658916048445, 0.42628336612448803, 0.42874743339951765, 0.42094455650210133, 0.42874743320369135, 0.42874743320369135, 0.4324435331371041, 0.4209445574812331, 0.4213552372901102, 0.4238193039776608, 0.42094455826453847, 0.41848049337614246, 0.4254620134340909, 0.4234086222105202, 0.42176591271737274, 0.4246406566313405, 0.4119096510953727, 0.42505133366193126, 0.40903490773216655, 0.4217659152998327, 0.42053388142977405, 0.42217659151040066, 0.4242299803840551, 0.4250513330744522, 0.42628336553700896]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
