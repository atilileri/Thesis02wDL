{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf14.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 19:37:56 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '02', '01', '05', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000019E00B3BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000019E754A6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6120, Accuracy:0.1803, Validation Loss:1.6086, Validation Accuracy:0.1741\n",
    "Epoch #2: Loss:1.6083, Accuracy:0.2074, Validation Loss:1.6071, Validation Accuracy:0.2200\n",
    "Epoch #3: Loss:1.6066, Accuracy:0.2251, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #4: Loss:1.6060, Accuracy:0.2324, Validation Loss:1.6059, Validation Accuracy:0.2282\n",
    "Epoch #5: Loss:1.6048, Accuracy:0.2341, Validation Loss:1.6053, Validation Accuracy:0.2299\n",
    "Epoch #6: Loss:1.6052, Accuracy:0.2324, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6026, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6026, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6024, Accuracy:0.2324, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #28: Loss:1.6022, Accuracy:0.2333, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #29: Loss:1.6018, Accuracy:0.2345, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #30: Loss:1.6016, Accuracy:0.2374, Validation Loss:1.6039, Validation Accuracy:0.2315\n",
    "Epoch #31: Loss:1.6014, Accuracy:0.2370, Validation Loss:1.6039, Validation Accuracy:0.2381\n",
    "Epoch #32: Loss:1.6013, Accuracy:0.2402, Validation Loss:1.6037, Validation Accuracy:0.2397\n",
    "Epoch #33: Loss:1.6013, Accuracy:0.2386, Validation Loss:1.6037, Validation Accuracy:0.2397\n",
    "Epoch #34: Loss:1.6014, Accuracy:0.2378, Validation Loss:1.6035, Validation Accuracy:0.2365\n",
    "Epoch #35: Loss:1.6014, Accuracy:0.2394, Validation Loss:1.6030, Validation Accuracy:0.2299\n",
    "Epoch #36: Loss:1.6013, Accuracy:0.2431, Validation Loss:1.6028, Validation Accuracy:0.2233\n",
    "Epoch #37: Loss:1.6012, Accuracy:0.2419, Validation Loss:1.6030, Validation Accuracy:0.2217\n",
    "Epoch #38: Loss:1.6018, Accuracy:0.2419, Validation Loss:1.6025, Validation Accuracy:0.2348\n",
    "Epoch #39: Loss:1.6017, Accuracy:0.2415, Validation Loss:1.6025, Validation Accuracy:0.2381\n",
    "Epoch #40: Loss:1.6009, Accuracy:0.2407, Validation Loss:1.6025, Validation Accuracy:0.2397\n",
    "Epoch #41: Loss:1.6007, Accuracy:0.2439, Validation Loss:1.6024, Validation Accuracy:0.2430\n",
    "Epoch #42: Loss:1.6004, Accuracy:0.2419, Validation Loss:1.6026, Validation Accuracy:0.2414\n",
    "Epoch #43: Loss:1.6001, Accuracy:0.2431, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #44: Loss:1.6003, Accuracy:0.2431, Validation Loss:1.6020, Validation Accuracy:0.2430\n",
    "Epoch #45: Loss:1.6006, Accuracy:0.2444, Validation Loss:1.6023, Validation Accuracy:0.2430\n",
    "Epoch #46: Loss:1.6008, Accuracy:0.2423, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #47: Loss:1.6008, Accuracy:0.2423, Validation Loss:1.6023, Validation Accuracy:0.2430\n",
    "Epoch #48: Loss:1.6008, Accuracy:0.2427, Validation Loss:1.6024, Validation Accuracy:0.2430\n",
    "Epoch #49: Loss:1.6006, Accuracy:0.2427, Validation Loss:1.6022, Validation Accuracy:0.2447\n",
    "Epoch #50: Loss:1.6006, Accuracy:0.2411, Validation Loss:1.6018, Validation Accuracy:0.2447\n",
    "Epoch #51: Loss:1.6006, Accuracy:0.2402, Validation Loss:1.6017, Validation Accuracy:0.2430\n",
    "Epoch #52: Loss:1.6007, Accuracy:0.2419, Validation Loss:1.6010, Validation Accuracy:0.2447\n",
    "Epoch #53: Loss:1.6004, Accuracy:0.2431, Validation Loss:1.6010, Validation Accuracy:0.2430\n",
    "Epoch #54: Loss:1.6000, Accuracy:0.2452, Validation Loss:1.6025, Validation Accuracy:0.2414\n",
    "Epoch #55: Loss:1.6002, Accuracy:0.2394, Validation Loss:1.6018, Validation Accuracy:0.2430\n",
    "Epoch #56: Loss:1.6003, Accuracy:0.2435, Validation Loss:1.6025, Validation Accuracy:0.2381\n",
    "Epoch #57: Loss:1.5998, Accuracy:0.2390, Validation Loss:1.6025, Validation Accuracy:0.2430\n",
    "Epoch #58: Loss:1.5996, Accuracy:0.2394, Validation Loss:1.6029, Validation Accuracy:0.2397\n",
    "Epoch #59: Loss:1.6000, Accuracy:0.2394, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #60: Loss:1.5991, Accuracy:0.2390, Validation Loss:1.6016, Validation Accuracy:0.2414\n",
    "Epoch #61: Loss:1.5999, Accuracy:0.2390, Validation Loss:1.6024, Validation Accuracy:0.2315\n",
    "Epoch #62: Loss:1.5991, Accuracy:0.2476, Validation Loss:1.6023, Validation Accuracy:0.2414\n",
    "Epoch #63: Loss:1.5992, Accuracy:0.2419, Validation Loss:1.6020, Validation Accuracy:0.2397\n",
    "Epoch #64: Loss:1.5994, Accuracy:0.2427, Validation Loss:1.6028, Validation Accuracy:0.2381\n",
    "Epoch #65: Loss:1.5993, Accuracy:0.2415, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #66: Loss:1.6006, Accuracy:0.2398, Validation Loss:1.6029, Validation Accuracy:0.2512\n",
    "Epoch #67: Loss:1.6007, Accuracy:0.2382, Validation Loss:1.6027, Validation Accuracy:0.2414\n",
    "Epoch #68: Loss:1.5992, Accuracy:0.2402, Validation Loss:1.6019, Validation Accuracy:0.2365\n",
    "Epoch #69: Loss:1.5996, Accuracy:0.2427, Validation Loss:1.6015, Validation Accuracy:0.2397\n",
    "Epoch #70: Loss:1.5992, Accuracy:0.2423, Validation Loss:1.6013, Validation Accuracy:0.2430\n",
    "Epoch #71: Loss:1.5991, Accuracy:0.2407, Validation Loss:1.6021, Validation Accuracy:0.2430\n",
    "Epoch #72: Loss:1.5989, Accuracy:0.2407, Validation Loss:1.6023, Validation Accuracy:0.2430\n",
    "Epoch #73: Loss:1.5986, Accuracy:0.2402, Validation Loss:1.6025, Validation Accuracy:0.2430\n",
    "Epoch #74: Loss:1.5990, Accuracy:0.2402, Validation Loss:1.6023, Validation Accuracy:0.2430\n",
    "Epoch #75: Loss:1.5984, Accuracy:0.2402, Validation Loss:1.6021, Validation Accuracy:0.2430\n",
    "Epoch #76: Loss:1.5981, Accuracy:0.2407, Validation Loss:1.6025, Validation Accuracy:0.2381\n",
    "Epoch #77: Loss:1.5981, Accuracy:0.2415, Validation Loss:1.6027, Validation Accuracy:0.2430\n",
    "Epoch #78: Loss:1.5980, Accuracy:0.2439, Validation Loss:1.6026, Validation Accuracy:0.2430\n",
    "Epoch #79: Loss:1.5983, Accuracy:0.2402, Validation Loss:1.6036, Validation Accuracy:0.2414\n",
    "Epoch #80: Loss:1.5986, Accuracy:0.2398, Validation Loss:1.6035, Validation Accuracy:0.2414\n",
    "Epoch #81: Loss:1.5984, Accuracy:0.2402, Validation Loss:1.6024, Validation Accuracy:0.2365\n",
    "Epoch #82: Loss:1.5983, Accuracy:0.2435, Validation Loss:1.6029, Validation Accuracy:0.2381\n",
    "Epoch #83: Loss:1.5981, Accuracy:0.2423, Validation Loss:1.6038, Validation Accuracy:0.2414\n",
    "Epoch #84: Loss:1.5979, Accuracy:0.2382, Validation Loss:1.6040, Validation Accuracy:0.2447\n",
    "Epoch #85: Loss:1.5977, Accuracy:0.2411, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #86: Loss:1.5981, Accuracy:0.2411, Validation Loss:1.6048, Validation Accuracy:0.2348\n",
    "Epoch #87: Loss:1.5981, Accuracy:0.2407, Validation Loss:1.6041, Validation Accuracy:0.2299\n",
    "Epoch #88: Loss:1.5979, Accuracy:0.2444, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #89: Loss:1.5979, Accuracy:0.2411, Validation Loss:1.6044, Validation Accuracy:0.2381\n",
    "Epoch #90: Loss:1.5983, Accuracy:0.2423, Validation Loss:1.6039, Validation Accuracy:0.2381\n",
    "Epoch #91: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6036, Validation Accuracy:0.2365\n",
    "Epoch #92: Loss:1.5985, Accuracy:0.2444, Validation Loss:1.6041, Validation Accuracy:0.2397\n",
    "Epoch #93: Loss:1.5987, Accuracy:0.2489, Validation Loss:1.6047, Validation Accuracy:0.2233\n",
    "Epoch #94: Loss:1.5976, Accuracy:0.2460, Validation Loss:1.6045, Validation Accuracy:0.2266\n",
    "Epoch #95: Loss:1.5971, Accuracy:0.2456, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #96: Loss:1.5976, Accuracy:0.2448, Validation Loss:1.6029, Validation Accuracy:0.2233\n",
    "Epoch #97: Loss:1.5985, Accuracy:0.2489, Validation Loss:1.6045, Validation Accuracy:0.2217\n",
    "Epoch #98: Loss:1.6008, Accuracy:0.2444, Validation Loss:1.6047, Validation Accuracy:0.2151\n",
    "Epoch #99: Loss:1.5993, Accuracy:0.2468, Validation Loss:1.6052, Validation Accuracy:0.2299\n",
    "Epoch #100: Loss:1.5992, Accuracy:0.2431, Validation Loss:1.6044, Validation Accuracy:0.2282\n",
    "Epoch #101: Loss:1.5993, Accuracy:0.2402, Validation Loss:1.6039, Validation Accuracy:0.2069\n",
    "Epoch #102: Loss:1.5983, Accuracy:0.2460, Validation Loss:1.6040, Validation Accuracy:0.2299\n",
    "Epoch #103: Loss:1.5981, Accuracy:0.2460, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #104: Loss:1.5974, Accuracy:0.2468, Validation Loss:1.6032, Validation Accuracy:0.2266\n",
    "Epoch #105: Loss:1.5978, Accuracy:0.2468, Validation Loss:1.6036, Validation Accuracy:0.2020\n",
    "Epoch #106: Loss:1.5980, Accuracy:0.2435, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #107: Loss:1.5979, Accuracy:0.2468, Validation Loss:1.6025, Validation Accuracy:0.2315\n",
    "Epoch #108: Loss:1.5975, Accuracy:0.2485, Validation Loss:1.6021, Validation Accuracy:0.2250\n",
    "Epoch #109: Loss:1.5970, Accuracy:0.2485, Validation Loss:1.6036, Validation Accuracy:0.2315\n",
    "Epoch #110: Loss:1.5973, Accuracy:0.2448, Validation Loss:1.6051, Validation Accuracy:0.2414\n",
    "Epoch #111: Loss:1.5977, Accuracy:0.2456, Validation Loss:1.6057, Validation Accuracy:0.2282\n",
    "Epoch #112: Loss:1.5980, Accuracy:0.2501, Validation Loss:1.6056, Validation Accuracy:0.2200\n",
    "Epoch #113: Loss:1.5973, Accuracy:0.2472, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #114: Loss:1.5967, Accuracy:0.2444, Validation Loss:1.6059, Validation Accuracy:0.2348\n",
    "Epoch #115: Loss:1.5965, Accuracy:0.2460, Validation Loss:1.6059, Validation Accuracy:0.2184\n",
    "Epoch #116: Loss:1.5965, Accuracy:0.2546, Validation Loss:1.6059, Validation Accuracy:0.2151\n",
    "Epoch #117: Loss:1.5974, Accuracy:0.2431, Validation Loss:1.6064, Validation Accuracy:0.2348\n",
    "Epoch #118: Loss:1.5970, Accuracy:0.2456, Validation Loss:1.6055, Validation Accuracy:0.2266\n",
    "Epoch #119: Loss:1.5976, Accuracy:0.2435, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #120: Loss:1.5968, Accuracy:0.2493, Validation Loss:1.6077, Validation Accuracy:0.2414\n",
    "Epoch #121: Loss:1.5963, Accuracy:0.2435, Validation Loss:1.6079, Validation Accuracy:0.2315\n",
    "Epoch #122: Loss:1.5954, Accuracy:0.2546, Validation Loss:1.6075, Validation Accuracy:0.2069\n",
    "Epoch #123: Loss:1.5964, Accuracy:0.2550, Validation Loss:1.6074, Validation Accuracy:0.2118\n",
    "Epoch #124: Loss:1.5952, Accuracy:0.2591, Validation Loss:1.6072, Validation Accuracy:0.2217\n",
    "Epoch #125: Loss:1.5947, Accuracy:0.2591, Validation Loss:1.6068, Validation Accuracy:0.2233\n",
    "Epoch #126: Loss:1.5949, Accuracy:0.2571, Validation Loss:1.6073, Validation Accuracy:0.2020\n",
    "Epoch #127: Loss:1.5943, Accuracy:0.2559, Validation Loss:1.6057, Validation Accuracy:0.2233\n",
    "Epoch #128: Loss:1.5946, Accuracy:0.2497, Validation Loss:1.6056, Validation Accuracy:0.2102\n",
    "Epoch #129: Loss:1.5947, Accuracy:0.2604, Validation Loss:1.6072, Validation Accuracy:0.2053\n",
    "Epoch #130: Loss:1.5948, Accuracy:0.2554, Validation Loss:1.6063, Validation Accuracy:0.2151\n",
    "Epoch #131: Loss:1.5955, Accuracy:0.2530, Validation Loss:1.6060, Validation Accuracy:0.2167\n",
    "Epoch #132: Loss:1.5955, Accuracy:0.2448, Validation Loss:1.6065, Validation Accuracy:0.2151\n",
    "Epoch #133: Loss:1.5964, Accuracy:0.2468, Validation Loss:1.6070, Validation Accuracy:0.2184\n",
    "Epoch #134: Loss:1.5972, Accuracy:0.2472, Validation Loss:1.6085, Validation Accuracy:0.2118\n",
    "Epoch #135: Loss:1.5966, Accuracy:0.2505, Validation Loss:1.6073, Validation Accuracy:0.1970\n",
    "Epoch #136: Loss:1.5955, Accuracy:0.2534, Validation Loss:1.6039, Validation Accuracy:0.2118\n",
    "Epoch #137: Loss:1.5976, Accuracy:0.2456, Validation Loss:1.6056, Validation Accuracy:0.2315\n",
    "Epoch #138: Loss:1.5982, Accuracy:0.2394, Validation Loss:1.6057, Validation Accuracy:0.2118\n",
    "Epoch #139: Loss:1.5977, Accuracy:0.2427, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #140: Loss:1.5978, Accuracy:0.2448, Validation Loss:1.6053, Validation Accuracy:0.2233\n",
    "Epoch #141: Loss:1.5977, Accuracy:0.2394, Validation Loss:1.6041, Validation Accuracy:0.2217\n",
    "Epoch #142: Loss:1.5971, Accuracy:0.2431, Validation Loss:1.6051, Validation Accuracy:0.2233\n",
    "Epoch #143: Loss:1.5987, Accuracy:0.2513, Validation Loss:1.6050, Validation Accuracy:0.2266\n",
    "Epoch #144: Loss:1.5990, Accuracy:0.2431, Validation Loss:1.6058, Validation Accuracy:0.2266\n",
    "Epoch #145: Loss:1.5979, Accuracy:0.2522, Validation Loss:1.6066, Validation Accuracy:0.2135\n",
    "Epoch #146: Loss:1.5973, Accuracy:0.2480, Validation Loss:1.6069, Validation Accuracy:0.2299\n",
    "Epoch #147: Loss:1.5977, Accuracy:0.2468, Validation Loss:1.6060, Validation Accuracy:0.2102\n",
    "Epoch #148: Loss:1.5972, Accuracy:0.2522, Validation Loss:1.6057, Validation Accuracy:0.2250\n",
    "Epoch #149: Loss:1.5976, Accuracy:0.2386, Validation Loss:1.6055, Validation Accuracy:0.2315\n",
    "Epoch #150: Loss:1.5982, Accuracy:0.2444, Validation Loss:1.6066, Validation Accuracy:0.2315\n",
    "Epoch #151: Loss:1.6002, Accuracy:0.2448, Validation Loss:1.6054, Validation Accuracy:0.2397\n",
    "Epoch #152: Loss:1.6000, Accuracy:0.2435, Validation Loss:1.6054, Validation Accuracy:0.2299\n",
    "Epoch #153: Loss:1.5997, Accuracy:0.2390, Validation Loss:1.6040, Validation Accuracy:0.2365\n",
    "Epoch #154: Loss:1.5986, Accuracy:0.2517, Validation Loss:1.6019, Validation Accuracy:0.2381\n",
    "Epoch #155: Loss:1.5999, Accuracy:0.2497, Validation Loss:1.6037, Validation Accuracy:0.2479\n",
    "Epoch #156: Loss:1.6046, Accuracy:0.2431, Validation Loss:1.6165, Validation Accuracy:0.1921\n",
    "Epoch #157: Loss:1.6049, Accuracy:0.2283, Validation Loss:1.6080, Validation Accuracy:0.2315\n",
    "Epoch #158: Loss:1.6038, Accuracy:0.2407, Validation Loss:1.6124, Validation Accuracy:0.2381\n",
    "Epoch #159: Loss:1.6012, Accuracy:0.2427, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #160: Loss:1.5990, Accuracy:0.2423, Validation Loss:1.6053, Validation Accuracy:0.2167\n",
    "Epoch #161: Loss:1.6041, Accuracy:0.2251, Validation Loss:1.6042, Validation Accuracy:0.2282\n",
    "Epoch #162: Loss:1.5992, Accuracy:0.2419, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #163: Loss:1.5999, Accuracy:0.2448, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #164: Loss:1.5986, Accuracy:0.2452, Validation Loss:1.6056, Validation Accuracy:0.2250\n",
    "Epoch #165: Loss:1.5977, Accuracy:0.2534, Validation Loss:1.6051, Validation Accuracy:0.2348\n",
    "Epoch #166: Loss:1.5974, Accuracy:0.2501, Validation Loss:1.6046, Validation Accuracy:0.2217\n",
    "Epoch #167: Loss:1.5967, Accuracy:0.2452, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #168: Loss:1.5970, Accuracy:0.2448, Validation Loss:1.6069, Validation Accuracy:0.2315\n",
    "Epoch #169: Loss:1.5969, Accuracy:0.2435, Validation Loss:1.6051, Validation Accuracy:0.2315\n",
    "Epoch #170: Loss:1.5968, Accuracy:0.2439, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #171: Loss:1.5960, Accuracy:0.2460, Validation Loss:1.6048, Validation Accuracy:0.2299\n",
    "Epoch #172: Loss:1.5964, Accuracy:0.2423, Validation Loss:1.6048, Validation Accuracy:0.2348\n",
    "Epoch #173: Loss:1.5960, Accuracy:0.2452, Validation Loss:1.6049, Validation Accuracy:0.2299\n",
    "Epoch #174: Loss:1.5962, Accuracy:0.2563, Validation Loss:1.6046, Validation Accuracy:0.2266\n",
    "Epoch #175: Loss:1.5962, Accuracy:0.2534, Validation Loss:1.6040, Validation Accuracy:0.2282\n",
    "Epoch #176: Loss:1.5961, Accuracy:0.2513, Validation Loss:1.6038, Validation Accuracy:0.2266\n",
    "Epoch #177: Loss:1.5960, Accuracy:0.2530, Validation Loss:1.6051, Validation Accuracy:0.2233\n",
    "Epoch #178: Loss:1.5960, Accuracy:0.2415, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #179: Loss:1.5960, Accuracy:0.2513, Validation Loss:1.6044, Validation Accuracy:0.2250\n",
    "Epoch #180: Loss:1.5958, Accuracy:0.2517, Validation Loss:1.6040, Validation Accuracy:0.2250\n",
    "Epoch #181: Loss:1.5957, Accuracy:0.2526, Validation Loss:1.6042, Validation Accuracy:0.2250\n",
    "Epoch #182: Loss:1.5953, Accuracy:0.2526, Validation Loss:1.6041, Validation Accuracy:0.2200\n",
    "Epoch #183: Loss:1.5950, Accuracy:0.2476, Validation Loss:1.6055, Validation Accuracy:0.2299\n",
    "Epoch #184: Loss:1.5954, Accuracy:0.2464, Validation Loss:1.6053, Validation Accuracy:0.2233\n",
    "Epoch #185: Loss:1.5951, Accuracy:0.2563, Validation Loss:1.6048, Validation Accuracy:0.2233\n",
    "Epoch #186: Loss:1.5952, Accuracy:0.2571, Validation Loss:1.6043, Validation Accuracy:0.2266\n",
    "Epoch #187: Loss:1.5949, Accuracy:0.2583, Validation Loss:1.6038, Validation Accuracy:0.2250\n",
    "Epoch #188: Loss:1.5955, Accuracy:0.2460, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #189: Loss:1.5952, Accuracy:0.2509, Validation Loss:1.6041, Validation Accuracy:0.2250\n",
    "Epoch #190: Loss:1.5945, Accuracy:0.2616, Validation Loss:1.6046, Validation Accuracy:0.2233\n",
    "Epoch #191: Loss:1.5935, Accuracy:0.2624, Validation Loss:1.6057, Validation Accuracy:0.2266\n",
    "Epoch #192: Loss:1.5944, Accuracy:0.2591, Validation Loss:1.6045, Validation Accuracy:0.2299\n",
    "Epoch #193: Loss:1.5942, Accuracy:0.2612, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #194: Loss:1.5939, Accuracy:0.2620, Validation Loss:1.6050, Validation Accuracy:0.2315\n",
    "Epoch #195: Loss:1.5944, Accuracy:0.2579, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #196: Loss:1.5945, Accuracy:0.2575, Validation Loss:1.6053, Validation Accuracy:0.2348\n",
    "Epoch #197: Loss:1.5936, Accuracy:0.2509, Validation Loss:1.6048, Validation Accuracy:0.2299\n",
    "Epoch #198: Loss:1.5939, Accuracy:0.2637, Validation Loss:1.6050, Validation Accuracy:0.2282\n",
    "Epoch #199: Loss:1.5945, Accuracy:0.2587, Validation Loss:1.6043, Validation Accuracy:0.2266\n",
    "Epoch #200: Loss:1.5954, Accuracy:0.2550, Validation Loss:1.6057, Validation Accuracy:0.2085\n",
    "Epoch #201: Loss:1.5963, Accuracy:0.2435, Validation Loss:1.6074, Validation Accuracy:0.2348\n",
    "Epoch #202: Loss:1.5953, Accuracy:0.2460, Validation Loss:1.6053, Validation Accuracy:0.2348\n",
    "Epoch #203: Loss:1.5942, Accuracy:0.2493, Validation Loss:1.6064, Validation Accuracy:0.2069\n",
    "Epoch #204: Loss:1.5946, Accuracy:0.2542, Validation Loss:1.6050, Validation Accuracy:0.2184\n",
    "Epoch #205: Loss:1.5936, Accuracy:0.2567, Validation Loss:1.6051, Validation Accuracy:0.2266\n",
    "Epoch #206: Loss:1.5946, Accuracy:0.2468, Validation Loss:1.6057, Validation Accuracy:0.2266\n",
    "Epoch #207: Loss:1.5952, Accuracy:0.2554, Validation Loss:1.6079, Validation Accuracy:0.2315\n",
    "Epoch #208: Loss:1.5938, Accuracy:0.2542, Validation Loss:1.6076, Validation Accuracy:0.2250\n",
    "Epoch #209: Loss:1.5935, Accuracy:0.2530, Validation Loss:1.6062, Validation Accuracy:0.2299\n",
    "Epoch #210: Loss:1.5939, Accuracy:0.2563, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #211: Loss:1.5935, Accuracy:0.2583, Validation Loss:1.6078, Validation Accuracy:0.2315\n",
    "Epoch #212: Loss:1.5932, Accuracy:0.2591, Validation Loss:1.6071, Validation Accuracy:0.2217\n",
    "Epoch #213: Loss:1.5934, Accuracy:0.2530, Validation Loss:1.6070, Validation Accuracy:0.2282\n",
    "Epoch #214: Loss:1.5925, Accuracy:0.2579, Validation Loss:1.6062, Validation Accuracy:0.2200\n",
    "Epoch #215: Loss:1.5930, Accuracy:0.2567, Validation Loss:1.6062, Validation Accuracy:0.2217\n",
    "Epoch #216: Loss:1.5925, Accuracy:0.2591, Validation Loss:1.6088, Validation Accuracy:0.2332\n",
    "Epoch #217: Loss:1.5924, Accuracy:0.2546, Validation Loss:1.6078, Validation Accuracy:0.2315\n",
    "Epoch #218: Loss:1.5926, Accuracy:0.2567, Validation Loss:1.6075, Validation Accuracy:0.2282\n",
    "Epoch #219: Loss:1.5921, Accuracy:0.2608, Validation Loss:1.6070, Validation Accuracy:0.2332\n",
    "Epoch #220: Loss:1.5931, Accuracy:0.2534, Validation Loss:1.6080, Validation Accuracy:0.2315\n",
    "Epoch #221: Loss:1.5944, Accuracy:0.2554, Validation Loss:1.6084, Validation Accuracy:0.2266\n",
    "Epoch #222: Loss:1.5935, Accuracy:0.2530, Validation Loss:1.6095, Validation Accuracy:0.2250\n",
    "Epoch #223: Loss:1.5931, Accuracy:0.2563, Validation Loss:1.6075, Validation Accuracy:0.2250\n",
    "Epoch #224: Loss:1.5932, Accuracy:0.2538, Validation Loss:1.6083, Validation Accuracy:0.2250\n",
    "Epoch #225: Loss:1.5915, Accuracy:0.2637, Validation Loss:1.6090, Validation Accuracy:0.2315\n",
    "Epoch #226: Loss:1.5932, Accuracy:0.2612, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #227: Loss:1.5924, Accuracy:0.2595, Validation Loss:1.6089, Validation Accuracy:0.2250\n",
    "Epoch #228: Loss:1.5924, Accuracy:0.2571, Validation Loss:1.6092, Validation Accuracy:0.2200\n",
    "Epoch #229: Loss:1.5915, Accuracy:0.2632, Validation Loss:1.6098, Validation Accuracy:0.2184\n",
    "Epoch #230: Loss:1.5916, Accuracy:0.2604, Validation Loss:1.6091, Validation Accuracy:0.2167\n",
    "Epoch #231: Loss:1.5910, Accuracy:0.2563, Validation Loss:1.6088, Validation Accuracy:0.2233\n",
    "Epoch #232: Loss:1.5918, Accuracy:0.2637, Validation Loss:1.6080, Validation Accuracy:0.2167\n",
    "Epoch #233: Loss:1.5903, Accuracy:0.2682, Validation Loss:1.6091, Validation Accuracy:0.2233\n",
    "Epoch #234: Loss:1.5910, Accuracy:0.2604, Validation Loss:1.6081, Validation Accuracy:0.2348\n",
    "Epoch #235: Loss:1.5910, Accuracy:0.2608, Validation Loss:1.6072, Validation Accuracy:0.2315\n",
    "Epoch #236: Loss:1.5906, Accuracy:0.2678, Validation Loss:1.6065, Validation Accuracy:0.2266\n",
    "Epoch #237: Loss:1.5909, Accuracy:0.2649, Validation Loss:1.6076, Validation Accuracy:0.2250\n",
    "Epoch #238: Loss:1.5910, Accuracy:0.2661, Validation Loss:1.6079, Validation Accuracy:0.2250\n",
    "Epoch #239: Loss:1.5915, Accuracy:0.2637, Validation Loss:1.6097, Validation Accuracy:0.2282\n",
    "Epoch #240: Loss:1.5909, Accuracy:0.2632, Validation Loss:1.6096, Validation Accuracy:0.2250\n",
    "Epoch #241: Loss:1.5908, Accuracy:0.2637, Validation Loss:1.6094, Validation Accuracy:0.2315\n",
    "Epoch #242: Loss:1.5905, Accuracy:0.2669, Validation Loss:1.6089, Validation Accuracy:0.2299\n",
    "Epoch #243: Loss:1.5908, Accuracy:0.2653, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #244: Loss:1.5901, Accuracy:0.2694, Validation Loss:1.6087, Validation Accuracy:0.2348\n",
    "Epoch #245: Loss:1.5899, Accuracy:0.2678, Validation Loss:1.6103, Validation Accuracy:0.2282\n",
    "Epoch #246: Loss:1.5894, Accuracy:0.2706, Validation Loss:1.6104, Validation Accuracy:0.2282\n",
    "Epoch #247: Loss:1.5896, Accuracy:0.2645, Validation Loss:1.6101, Validation Accuracy:0.2299\n",
    "Epoch #248: Loss:1.5892, Accuracy:0.2641, Validation Loss:1.6090, Validation Accuracy:0.2348\n",
    "Epoch #249: Loss:1.5896, Accuracy:0.2649, Validation Loss:1.6101, Validation Accuracy:0.2332\n",
    "Epoch #250: Loss:1.5883, Accuracy:0.2674, Validation Loss:1.6109, Validation Accuracy:0.2266\n",
    "Epoch #251: Loss:1.5884, Accuracy:0.2657, Validation Loss:1.6109, Validation Accuracy:0.2332\n",
    "Epoch #252: Loss:1.5893, Accuracy:0.2653, Validation Loss:1.6091, Validation Accuracy:0.2414\n",
    "Epoch #253: Loss:1.5894, Accuracy:0.2657, Validation Loss:1.6120, Validation Accuracy:0.2381\n",
    "Epoch #254: Loss:1.5907, Accuracy:0.2608, Validation Loss:1.6103, Validation Accuracy:0.2332\n",
    "Epoch #255: Loss:1.5904, Accuracy:0.2616, Validation Loss:1.6103, Validation Accuracy:0.2299\n",
    "Epoch #256: Loss:1.5910, Accuracy:0.2600, Validation Loss:1.6092, Validation Accuracy:0.2348\n",
    "Epoch #257: Loss:1.5904, Accuracy:0.2649, Validation Loss:1.6120, Validation Accuracy:0.2217\n",
    "Epoch #258: Loss:1.5914, Accuracy:0.2583, Validation Loss:1.6121, Validation Accuracy:0.2167\n",
    "Epoch #259: Loss:1.5923, Accuracy:0.2600, Validation Loss:1.6096, Validation Accuracy:0.2315\n",
    "Epoch #260: Loss:1.5956, Accuracy:0.2534, Validation Loss:1.6097, Validation Accuracy:0.2200\n",
    "Epoch #261: Loss:1.5960, Accuracy:0.2497, Validation Loss:1.6143, Validation Accuracy:0.2217\n",
    "Epoch #262: Loss:1.6046, Accuracy:0.2444, Validation Loss:1.6260, Validation Accuracy:0.1806\n",
    "Epoch #263: Loss:1.6049, Accuracy:0.2333, Validation Loss:1.6128, Validation Accuracy:0.2217\n",
    "Epoch #264: Loss:1.5993, Accuracy:0.2415, Validation Loss:1.6113, Validation Accuracy:0.2332\n",
    "Epoch #265: Loss:1.5968, Accuracy:0.2546, Validation Loss:1.6069, Validation Accuracy:0.2282\n",
    "Epoch #266: Loss:1.5927, Accuracy:0.2616, Validation Loss:1.6067, Validation Accuracy:0.2266\n",
    "Epoch #267: Loss:1.5927, Accuracy:0.2513, Validation Loss:1.6062, Validation Accuracy:0.2118\n",
    "Epoch #268: Loss:1.5916, Accuracy:0.2526, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #269: Loss:1.5903, Accuracy:0.2632, Validation Loss:1.6037, Validation Accuracy:0.2282\n",
    "Epoch #270: Loss:1.5910, Accuracy:0.2591, Validation Loss:1.6035, Validation Accuracy:0.2266\n",
    "Epoch #271: Loss:1.5899, Accuracy:0.2645, Validation Loss:1.6050, Validation Accuracy:0.2266\n",
    "Epoch #272: Loss:1.5916, Accuracy:0.2608, Validation Loss:1.6048, Validation Accuracy:0.2250\n",
    "Epoch #273: Loss:1.5909, Accuracy:0.2616, Validation Loss:1.6062, Validation Accuracy:0.2266\n",
    "Epoch #274: Loss:1.5906, Accuracy:0.2595, Validation Loss:1.6078, Validation Accuracy:0.2217\n",
    "Epoch #275: Loss:1.5898, Accuracy:0.2575, Validation Loss:1.6076, Validation Accuracy:0.2282\n",
    "Epoch #276: Loss:1.5905, Accuracy:0.2608, Validation Loss:1.6095, Validation Accuracy:0.2233\n",
    "Epoch #277: Loss:1.5889, Accuracy:0.2674, Validation Loss:1.6089, Validation Accuracy:0.2233\n",
    "Epoch #278: Loss:1.5897, Accuracy:0.2649, Validation Loss:1.6102, Validation Accuracy:0.2217\n",
    "Epoch #279: Loss:1.5883, Accuracy:0.2686, Validation Loss:1.6098, Validation Accuracy:0.2217\n",
    "Epoch #280: Loss:1.5883, Accuracy:0.2620, Validation Loss:1.6092, Validation Accuracy:0.2217\n",
    "Epoch #281: Loss:1.5889, Accuracy:0.2669, Validation Loss:1.6120, Validation Accuracy:0.2151\n",
    "Epoch #282: Loss:1.5889, Accuracy:0.2678, Validation Loss:1.6113, Validation Accuracy:0.2266\n",
    "Epoch #283: Loss:1.5887, Accuracy:0.2624, Validation Loss:1.6094, Validation Accuracy:0.2233\n",
    "Epoch #284: Loss:1.5904, Accuracy:0.2632, Validation Loss:1.6099, Validation Accuracy:0.2200\n",
    "Epoch #285: Loss:1.5879, Accuracy:0.2653, Validation Loss:1.6097, Validation Accuracy:0.2217\n",
    "Epoch #286: Loss:1.5887, Accuracy:0.2682, Validation Loss:1.6108, Validation Accuracy:0.2250\n",
    "Epoch #287: Loss:1.5906, Accuracy:0.2624, Validation Loss:1.6102, Validation Accuracy:0.2200\n",
    "Epoch #288: Loss:1.5886, Accuracy:0.2637, Validation Loss:1.6105, Validation Accuracy:0.2233\n",
    "Epoch #289: Loss:1.5876, Accuracy:0.2616, Validation Loss:1.6110, Validation Accuracy:0.2250\n",
    "Epoch #290: Loss:1.5872, Accuracy:0.2632, Validation Loss:1.6110, Validation Accuracy:0.2233\n",
    "Epoch #291: Loss:1.5879, Accuracy:0.2674, Validation Loss:1.6132, Validation Accuracy:0.2233\n",
    "Epoch #292: Loss:1.5889, Accuracy:0.2641, Validation Loss:1.6127, Validation Accuracy:0.2200\n",
    "Epoch #293: Loss:1.5887, Accuracy:0.2669, Validation Loss:1.6128, Validation Accuracy:0.2250\n",
    "Epoch #294: Loss:1.5891, Accuracy:0.2669, Validation Loss:1.6121, Validation Accuracy:0.2200\n",
    "Epoch #295: Loss:1.5880, Accuracy:0.2665, Validation Loss:1.6127, Validation Accuracy:0.2250\n",
    "Epoch #296: Loss:1.5885, Accuracy:0.2637, Validation Loss:1.6147, Validation Accuracy:0.2200\n",
    "Epoch #297: Loss:1.5878, Accuracy:0.2674, Validation Loss:1.6143, Validation Accuracy:0.2184\n",
    "Epoch #298: Loss:1.5886, Accuracy:0.2620, Validation Loss:1.6151, Validation Accuracy:0.2233\n",
    "Epoch #299: Loss:1.5874, Accuracy:0.2624, Validation Loss:1.6133, Validation Accuracy:0.2184\n",
    "Epoch #300: Loss:1.5875, Accuracy:0.2649, Validation Loss:1.6135, Validation Accuracy:0.2250\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61346912, Accuracy:0.2250\n",
    "Labels: ['03', '02', '01', '05', '04']\n",
    "Confusion Matrix:\n",
    "      03  02  01  05  04\n",
    "t:03   1  10  41  52  11\n",
    "t:02   1   8  31  69   5\n",
    "t:01   1   9  47  59  10\n",
    "t:05   0  14  46  73   9\n",
    "t:04   0  14  37  53   8\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.33      0.01      0.02       115\n",
    "          02       0.15      0.07      0.09       114\n",
    "          01       0.23      0.37      0.29       126\n",
    "          05       0.24      0.51      0.33       142\n",
    "          04       0.19      0.07      0.10       112\n",
    "\n",
    "    accuracy                           0.22       609\n",
    "   macro avg       0.23      0.21      0.17       609\n",
    "weighted avg       0.23      0.22      0.18       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 20:19:05 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 41 minutes, 9 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6086034479203875, 1.6070591199574211, 1.6061811593953024, 1.60594104898387, 1.6052673716458976, 1.6062533597053565, 1.6067385648075974, 1.6064128127982855, 1.6055597309604264, 1.605557649985127, 1.60522111902879, 1.6050890842682035, 1.6046393185805021, 1.6051797007496524, 1.6050895323307055, 1.6046857091985116, 1.6047656015418041, 1.6048081807902295, 1.6051551287593122, 1.6047740877164016, 1.6044800843315563, 1.604645404322394, 1.6044403210845095, 1.6045095268728697, 1.60455089171336, 1.6043546009925003, 1.604343348731744, 1.6041123164306916, 1.604256959971536, 1.6039175353026742, 1.6039453803612094, 1.60374143303713, 1.603737434534408, 1.6034892188700158, 1.6030303069523402, 1.6027627130251605, 1.6030077268728873, 1.6024962714544462, 1.6024698758947438, 1.6025177103349533, 1.6023688267408724, 1.6025552796612819, 1.6019331643538326, 1.6020221387224245, 1.6022696130968668, 1.6019382891788077, 1.6023244820596354, 1.6023627562671656, 1.6022322119163175, 1.6017734225356128, 1.6017199901524435, 1.6009964768718226, 1.6009778847248095, 1.6025263579999676, 1.6018148047974936, 1.6025102678778136, 1.6024590513389099, 1.6029086246083326, 1.6022152794992983, 1.601643301191784, 1.6023994968051003, 1.602290588059449, 1.601990121143009, 1.6027758683281383, 1.6028121964293356, 1.6028877647443749, 1.60272996844525, 1.601901849893905, 1.6015466336154782, 1.601274940180661, 1.6021170999811984, 1.6022689074326815, 1.6024671849750338, 1.6023429710485273, 1.6021405859729536, 1.6024765964407834, 1.602699676366471, 1.6026082318796118, 1.6036319141513218, 1.603467150274756, 1.6023852140053936, 1.602919797983467, 1.6038232033671613, 1.6040170940468073, 1.6044325296123236, 1.6047857130689573, 1.604112601632555, 1.6041427422039614, 1.6044120056484328, 1.6038983245984282, 1.603596947659021, 1.604072391693228, 1.6046832296844382, 1.6044556254823807, 1.6043822853436023, 1.6028715015827923, 1.604472143505203, 1.6046691798224237, 1.6052107225693701, 1.6044320999499417, 1.6038937124321222, 1.6040004290187693, 1.6037611000252083, 1.6032428516347224, 1.6036239564908157, 1.6027915512986959, 1.602523650441851, 1.6020598961606205, 1.6036244992943625, 1.6050770515683053, 1.6057168302081881, 1.605622688733494, 1.6056209785010427, 1.6059178750111747, 1.6059123729837352, 1.6058653945406083, 1.6063805397703925, 1.6054982708396974, 1.606145275050196, 1.6076513847889766, 1.6079122115825784, 1.6075062927941384, 1.6073945089318287, 1.6071981958959294, 1.6068274542224428, 1.6072527514694164, 1.6056632623688145, 1.6056328665446766, 1.6071571708704255, 1.6063200270601095, 1.6060115682276208, 1.6065178798337287, 1.607022397232369, 1.6084711312856188, 1.6072655454253524, 1.6038894465404192, 1.605617529652976, 1.60566160753247, 1.6054739701532574, 1.6053342940576363, 1.6041121662935405, 1.605076575122639, 1.6050414483143973, 1.605847272379645, 1.6066224085677825, 1.6069493317251722, 1.6060376374788081, 1.6056640339993882, 1.6054857164768164, 1.6066345594982403, 1.6053703733657185, 1.6054438949610017, 1.603960204398495, 1.6019347579216918, 1.603698008949142, 1.6165372098020732, 1.6080156355264348, 1.6124313592127784, 1.60637790953193, 1.6053146139545784, 1.6041544223653859, 1.6062145984818783, 1.6067886395603175, 1.6056094543491481, 1.6050876277225163, 1.6046454552163436, 1.6057849456915518, 1.606883289387269, 1.6051226592024754, 1.604080685449547, 1.6047674736561641, 1.604792283086354, 1.604910915512561, 1.6046156143320018, 1.604045881230647, 1.603840767064901, 1.6050893532231523, 1.605630055632693, 1.6043630489966356, 1.6039625701841658, 1.6042348438100078, 1.6040876099628767, 1.605517952117231, 1.6052616836597962, 1.6047969907766884, 1.6042888604948673, 1.603788723107825, 1.6052445504074222, 1.6041276386414451, 1.6046120707428906, 1.6056722065889582, 1.6044502232853806, 1.605211751605881, 1.6050291088805801, 1.6051530732309878, 1.6052663576818256, 1.6048075123177765, 1.6049716494157789, 1.604285512260224, 1.6056713666430444, 1.6074489824877585, 1.6053464490987592, 1.6064334408990268, 1.6050194241534705, 1.6050537534926717, 1.6056680970982768, 1.6079393883644066, 1.6075854937627005, 1.6061669703579105, 1.6067284116604057, 1.6078437617651151, 1.6071488438372934, 1.6069634074256534, 1.6062343729344886, 1.606245127804761, 1.608814751377638, 1.6077536270144734, 1.6074959810927192, 1.6070261179715737, 1.6079553318728368, 1.608362063789994, 1.609493579183306, 1.6075060085710047, 1.608337509025298, 1.6089718566935247, 1.6100236590468433, 1.608909586380268, 1.6092301477939623, 1.6097681019302268, 1.6091294390423152, 1.6088490629039571, 1.6080117799182636, 1.6090622834971386, 1.6081383171535673, 1.6072041943351232, 1.606531955729956, 1.607595477981129, 1.607894075719398, 1.6096552496864682, 1.6095690351401644, 1.6093935144358669, 1.608949148987706, 1.6097027687799363, 1.6087118048581779, 1.6103019798526232, 1.6103938585040214, 1.6100828913828031, 1.6089883723673954, 1.6101232909999654, 1.6108745367851947, 1.6108752007554905, 1.6090811471437978, 1.6120302332641652, 1.6103498557909761, 1.610330469306858, 1.6091602788182902, 1.6119726334495106, 1.6120930516661094, 1.6096342811835027, 1.6096904289546272, 1.6143462319287956, 1.6259618733316807, 1.6128284919438103, 1.6112711265169342, 1.60691597289444, 1.6066570101896138, 1.606213467853214, 1.603831356773627, 1.6037027904357033, 1.6035413884959981, 1.6049570148605823, 1.6047565643423296, 1.6061828884193659, 1.6078082554054574, 1.6076154787160688, 1.6094775971129218, 1.6088724796016425, 1.6101544040373001, 1.6097513785698927, 1.6092130223714267, 1.6119704152562935, 1.6112744594833925, 1.6094070618180023, 1.6099464124059442, 1.6097073856441455, 1.61083283252121, 1.6101833603456495, 1.6105111165978443, 1.6109670752962235, 1.6109653016420813, 1.6131955909807303, 1.6126976888168034, 1.6127784704339916, 1.6121195721117343, 1.612700207480069, 1.6146997595068269, 1.6142615414605352, 1.6151468724452804, 1.6132764360196092, 1.6134691222743645], 'val_acc': [0.1740558286165369, 0.22003284011078977, 0.2348111652339425, 0.22824302093050947, 0.2298850570553042, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.23152709318009895, 0.23809523767927793, 0.23973727380407267, 0.23973727380407267, 0.23645320155448318, 0.2298850570553042, 0.22331691245825225, 0.22167487643133046, 0.23481116542968844, 0.23809523758140494, 0.23973727370619968, 0.2430213459557892, 0.24137930983099445, 0.23973727380407267, 0.2430213459557892, 0.24302134605366216, 0.23973727370619968, 0.2430213459557892, 0.24302134615153514, 0.2446633822763299, 0.2446633822763299, 0.24302134624940813, 0.2446633821784569, 0.24302134615153514, 0.2413793100267404, 0.24302134624940813, 0.2380952378750239, 0.24302134615153514, 0.23973727390194566, 0.24302134624940813, 0.2413793100267404, 0.23152709298435298, 0.2413793100267404, 0.23973727399981865, 0.23809523767927793, 0.24302134605366216, 0.25123152687338185, 0.2413793100267404, 0.23645320165235617, 0.23973727380407267, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.23809523777715091, 0.24302134624940813, 0.24302134624940813, 0.2413793100267404, 0.2413793100267404, 0.23645320155448318, 0.23809523767927793, 0.2413793099288674, 0.2446633821784569, 0.23645320155448318, 0.23481116533181545, 0.22988505685955823, 0.2331691292070207, 0.23809523767927793, 0.23809523767927793, 0.2364532014566102, 0.23973727380407267, 0.22331691295985126, 0.22660098470784173, 0.23152709327797194, 0.2233169125561252, 0.2216748763334575, 0.21510673124704063, 0.2298850570553042, 0.22824302093050947, 0.20689655072093988, 0.22988505695743122, 0.23152709308222597, 0.22660098460996875, 0.20197044323964658, 0.2331691292070207, 0.23152709308222597, 0.22495894828942806, 0.23152709288648002, 0.2413793099288674, 0.22824302093050947, 0.22003283981717084, 0.23481116542968844, 0.23481116542968844, 0.21839080379024906, 0.21510673144278658, 0.23481116542968844, 0.22660098480571472, 0.2331691292070207, 0.2413793099288674, 0.23152709318009895, 0.2068965517119038, 0.21182265998841507, 0.22167487613771153, 0.22331691236037926, 0.20197044254230162, 0.22331691245825225, 0.2101806229705294, 0.20525451449827217, 0.21510673163853256, 0.2167487678612003, 0.21510673144278658, 0.2183908035945031, 0.21182265948681606, 0.1970443340700444, 0.21182265948681606, 0.23152709308222597, 0.21182266008628806, 0.22824302083263648, 0.2233169125561252, 0.22167487643133046, 0.2233169125561252, 0.22660098470784173, 0.22660098470784173, 0.21346469531799184, 0.22988505695743122, 0.21018062316627534, 0.2249589489745389, 0.23152709318009895, 0.23152709278860703, 0.23973727341258075, 0.22988505695743122, 0.23645320116299126, 0.23809523758140494, 0.24794745413442745, 0.19211822639300513, 0.23152709308222597, 0.23809523758140494, 0.2331691293048937, 0.2167487677633273, 0.22824302083263648, 0.23481116533181545, 0.2331691292070207, 0.224958948485174, 0.2348111651360695, 0.22167487623558452, 0.2331691292070207, 0.23152709308222597, 0.23152709308222597, 0.23152709308222597, 0.22988505676168527, 0.23481116533181545, 0.22988505676168527, 0.2266009844142228, 0.22824302063689053, 0.2266009844142228, 0.22331691236037926, 0.2331691292070207, 0.224958948485174, 0.22495894828942806, 0.22495894828942806, 0.2200328399150438, 0.22988505695743122, 0.2233169121646333, 0.2233169121646333, 0.2266009844142228, 0.22495894809368208, 0.22824302083263648, 0.22495894828942806, 0.2233169121646333, 0.2266009844142228, 0.2298850566638123, 0.22824302044114456, 0.23152709278860703, 0.2331691292070207, 0.2348111651360695, 0.2298850565659393, 0.22824302063689053, 0.22660098421847683, 0.20853858783669854, 0.23481116572330737, 0.23481116552756143, 0.20689655141828486, 0.21839080379024906, 0.22660098451209576, 0.22660098412060387, 0.23152709288648002, 0.22495894819155507, 0.22988505676168527, 0.22824302053901754, 0.23152709269073404, 0.22167487603983857, 0.22824302053901754, 0.22003283971929785, 0.2216748758440926, 0.23316912910914772, 0.23152709288648002, 0.22824302053901754, 0.23316912910914772, 0.23152709288648002, 0.22660098451209576, 0.22495894819155507, 0.22495894819155507, 0.22495894838730102, 0.23152709288648002, 0.22824302053901754, 0.22495894809368208, 0.22003283971929785, 0.21839080388812204, 0.2167487676654543, 0.22331691206676033, 0.2167487677633273, 0.22331691226250627, 0.2348111651360695, 0.23152709288648002, 0.22660098451209576, 0.22495894838730102, 0.22495894838730102, 0.22824302044114456, 0.22495894838730102, 0.23152709288648002, 0.22988505676168527, 0.22988505676168527, 0.2348111651360695, 0.22824302063689053, 0.22824302063689053, 0.22988505676168527, 0.2348111652339425, 0.23316912910914772, 0.22660098460996875, 0.23316912910914772, 0.24137930973312147, 0.238095237287786, 0.2331691288155288, 0.2298850564680663, 0.23481116503819652, 0.22167487594196558, 0.2167487683627993, 0.23152709358382498, 0.22003283971929785, 0.22167487603983857, 0.18062397311571587, 0.2216748763334575, 0.2331691292070207, 0.2282430202453986, 0.22660098451209576, 0.21182265958468902, 0.23152709269073404, 0.22824302063689053, 0.22660098520944075, 0.22660098451209576, 0.22495894828942806, 0.2266009844142228, 0.2216748757462196, 0.22824302063689053, 0.22331691295985126, 0.22331691295985126, 0.22167487683505652, 0.22167487613771153, 0.22167487683505652, 0.21510673223800456, 0.22660098451209576, 0.22331691295985126, 0.22003284061238879, 0.22167487613771153, 0.22495894838730102, 0.22003284061238879, 0.22331691295985126, 0.22495894838730102, 0.22331691295985126, 0.22331691295985126, 0.22003284071026177, 0.224958949084646, 0.22003284071026177, 0.224958949084646, 0.22003284071026177, 0.21839080458546703, 0.22331691295985126, 0.21839080448759404, 0.224958949084646], 'loss': [1.6120111579522949, 1.6082650579711006, 1.606556888429536, 1.6060148021768494, 1.6048183723152052, 1.6052074060792552, 1.6047249389135372, 1.604555004186454, 1.6046912348001154, 1.6046169321639827, 1.6045065805897332, 1.6040618104856361, 1.604273767050287, 1.6041463131777316, 1.6044942932207238, 1.6045777801615502, 1.6044737953424943, 1.6044892861857796, 1.6041906905125298, 1.6042066606897594, 1.603802503403697, 1.6035353084364465, 1.6031043021341125, 1.6029134531530267, 1.6026467722788973, 1.602558447595005, 1.6024031274862114, 1.60224839862612, 1.6017675795839064, 1.6015907169612282, 1.6014212624982642, 1.6013484580316093, 1.6013458821807798, 1.601357705049691, 1.6013937903135953, 1.6013347022097704, 1.601227967254435, 1.6018340877438961, 1.6016667234334612, 1.6008741239257906, 1.600669110188494, 1.6003820685635357, 1.6000642621296877, 1.6003372684396513, 1.600570238542263, 1.6008208998419666, 1.6007715177242272, 1.6008217049819977, 1.6006142332323767, 1.6005797989314587, 1.6005572931477665, 1.6006765313217037, 1.6004299088425216, 1.5999827236610271, 1.6001511906206731, 1.60034902306308, 1.5998448262714018, 1.5996272726470195, 1.5999610769675252, 1.599118328388222, 1.5998835936708862, 1.5990529126944728, 1.5991981775608886, 1.599415207692485, 1.5993338240735095, 1.6005881676194114, 1.600703592271041, 1.5991998785575068, 1.5995838795844044, 1.5991520270674626, 1.599057995122561, 1.5988567547142138, 1.598553181771625, 1.598969077917095, 1.5983960731318354, 1.598144308350659, 1.5980596515432275, 1.597984885288215, 1.5983073404437462, 1.5986408347711425, 1.5983658416070488, 1.598346737473897, 1.5980844202472444, 1.5978609086551705, 1.5977139233563715, 1.5981163399909801, 1.5981152576587527, 1.5978951177068315, 1.5978758487858078, 1.5982615899745933, 1.5983722092434611, 1.5984681341927154, 1.5987311706895455, 1.5975839118448372, 1.5970508906875547, 1.5976147073005504, 1.5985063546981655, 1.600777995219221, 1.5992785106694185, 1.5991839681073137, 1.5992875979421564, 1.5982868787444349, 1.5980891912624822, 1.5974495528904564, 1.5977742649936089, 1.5979628742108354, 1.5978793420341226, 1.597533433589113, 1.5969663897578965, 1.5973226719078832, 1.597671961882276, 1.5979871208907643, 1.5972589984811552, 1.596742809234948, 1.5964820256223424, 1.596468335647113, 1.597424420879607, 1.597041206046541, 1.5975917505287782, 1.5967556445260802, 1.5962931297642984, 1.5954136921884587, 1.5963576410830143, 1.595170169493501, 1.5947111013244064, 1.5948896994091402, 1.5942501398082631, 1.594564737578437, 1.5947142335178917, 1.5947975206179297, 1.595481548368074, 1.5954527236108174, 1.5964197466260843, 1.597230837624176, 1.596622438793065, 1.5954994316708135, 1.5975957406374952, 1.5982365909298342, 1.5976633464041676, 1.5978398013653452, 1.5976606573657088, 1.5971490402730828, 1.5986546897300704, 1.5989873488580912, 1.5978872081337523, 1.5972873616267524, 1.597715438415872, 1.5972386548161752, 1.5976209096105682, 1.5981821298599244, 1.6001992741159834, 1.5999955637988614, 1.5996809180022755, 1.5985715809299226, 1.599890688998009, 1.6046368126996489, 1.604941865159256, 1.6037908207464513, 1.6011549087520498, 1.59903511966768, 1.6041241044136534, 1.5992345834904382, 1.5998862792334272, 1.5985616235028057, 1.5977484606129921, 1.5973813239553871, 1.5966530365131228, 1.5970390893105852, 1.5968993697078322, 1.596818744377434, 1.5960360579422121, 1.5963614809439657, 1.5959906132069457, 1.5961633165758984, 1.5961616800061487, 1.5961134299115722, 1.5960371060537852, 1.5959732364580126, 1.5959512466277919, 1.595814698334837, 1.5957388425999353, 1.59534593460741, 1.5950385349242349, 1.5953920015808982, 1.5951278721282613, 1.595174980310444, 1.5948941557314362, 1.5955451127684825, 1.5952133139545668, 1.5944883788146034, 1.5935430282929595, 1.5943698698490307, 1.5941660072279662, 1.5939447645289209, 1.5944311918419245, 1.5945475246382446, 1.5936065933298036, 1.5939079894667043, 1.5944521461424153, 1.5953551369770842, 1.5962514911588948, 1.5952533395383393, 1.5941767111451228, 1.59463939294678, 1.5935611726322214, 1.5945559212804086, 1.595163462490027, 1.5938022109762109, 1.5935237342572066, 1.5939136729348122, 1.5935180333605536, 1.5932140140807605, 1.5934235122903906, 1.5925228527194422, 1.5929522998523908, 1.592463744935069, 1.5923738308755768, 1.592628607563904, 1.5921053073244662, 1.5930776164272238, 1.5944398155447395, 1.593525692031124, 1.5930981211593753, 1.593226654769459, 1.5915242901572946, 1.5932365207456711, 1.5924105608487766, 1.5923665125511999, 1.5914851038362945, 1.5916369210033692, 1.5909983734330602, 1.5917572772967987, 1.5903197217036567, 1.5909937119337079, 1.5909835580927636, 1.5906091359116947, 1.5908539596279543, 1.5910150901982427, 1.5915250892756656, 1.590931144191499, 1.5908173363801146, 1.5905340398851116, 1.5907665980669996, 1.590112007667886, 1.58986144036483, 1.5893664919620176, 1.589594287549201, 1.5891590727428146, 1.5895645615990892, 1.5883064214209022, 1.5884370250623574, 1.5893241941072123, 1.589408380882451, 1.590655470971454, 1.5903859687781676, 1.5910332248441004, 1.5903724812873825, 1.5914223681729922, 1.5922852246423522, 1.5956103133959447, 1.595973262160221, 1.6046471890482814, 1.6049019383209198, 1.5993310682093094, 1.596836821107649, 1.5926570143787768, 1.5927271938911454, 1.591606977881837, 1.5902979397431047, 1.5910496685783966, 1.589917741272239, 1.591561806618066, 1.590885855774615, 1.590555911778914, 1.589768301828686, 1.5905296251759147, 1.5888509501177184, 1.589666109701936, 1.5882870320177176, 1.5882676413906183, 1.5888575117201287, 1.5888575840779644, 1.5887375426243462, 1.5904231785748772, 1.5879216880034617, 1.5886780745684488, 1.5905826360048454, 1.5885781876115583, 1.587645932928003, 1.5871918952930144, 1.5878945386385281, 1.5889152440691876, 1.5887463226945004, 1.589146379181002, 1.5879618417555792, 1.588492847859737, 1.5877566004680657, 1.5885505729142646, 1.5873810317237274, 1.587457820669092], 'acc': [0.1802874732433648, 0.20739219817170373, 0.22505133516122672, 0.2324435326781361, 0.2340862421529249, 0.23244353071987262, 0.23285420851541005, 0.23285420969036816, 0.23285420929871545, 0.23285421051039099, 0.23285421008202084, 0.23285420990455322, 0.23285421051039099, 0.2328542087112364, 0.23285420892542147, 0.23285421031456463, 0.23285420892542147, 0.2328542094945418, 0.23285420912124782, 0.23285420990455322, 0.23285420990455322, 0.23285420990455322, 0.23285421010037957, 0.2328542083195837, 0.23285420833794243, 0.23285420872959514, 0.23244353246395102, 0.23326488811010207, 0.23449691918351567, 0.23737166525157324, 0.23696098606689264, 0.2402464068156248, 0.23860369830160905, 0.23778234286964306, 0.23942505099200614, 0.24312115016047225, 0.2418891180895682, 0.24188911671878377, 0.24147843929654028, 0.24065708365038924, 0.24394250617991728, 0.24188911748373043, 0.24312115112124527, 0.24312114935880813, 0.24435318242720266, 0.2422997941226685, 0.2422997949059739, 0.2427104725240437, 0.24271047134908563, 0.24106776126845905, 0.2402464083822356, 0.2418891180895682, 0.24312114755965356, 0.24517453960324703, 0.2394250511878325, 0.24353182893514144, 0.23901437337393633, 0.2394250504045271, 0.2394250507961798, 0.23901437376558904, 0.2390143739797741, 0.2476386051158396, 0.2418891180895682, 0.24271047350317546, 0.241478437479027, 0.23983572880590232, 0.23819301952693986, 0.24024640722563625, 0.24271047213239103, 0.24229979568927928, 0.24065708523535875, 0.240657084256227, 0.2402464087738883, 0.24024640661979849, 0.24024640761728894, 0.2406570842378683, 0.24147843986566062, 0.24394250694486394, 0.24024640642397213, 0.23983572882426105, 0.2402464048206439, 0.2435318271727043, 0.24229979472850627, 0.2381930193311135, 0.2410677626576022, 0.24106776148264414, 0.240657084256227, 0.2443531822313763, 0.24106776246177586, 0.24229979551181166, 0.24435318240884393, 0.24435318242720266, 0.24887063538758908, 0.24599589503521302, 0.24558521624218513, 0.2447638598310874, 0.2488706365992646, 0.24435318201719122, 0.24681724792143647, 0.2431211507479513, 0.2402464058364931, 0.24599589387861365, 0.24599589327277588, 0.24681724852727424, 0.24681724792143647, 0.24353182697687795, 0.2468172487231006, 0.24845995996032652, 0.24845995679038752, 0.24476386043692516, 0.24558521624218513, 0.2501026706284321, 0.247227925931159, 0.24435318321050806, 0.2459958928811232, 0.2546201244272001, 0.24312115055212494, 0.24558521665219654, 0.24353182756435698, 0.2492813153922925, 0.24353182777854207, 0.2546201244272001, 0.25503080224109625, 0.25913757822596806, 0.2591375764635309, 0.2570841885690082, 0.25585215767306224, 0.2496919928328947, 0.2603696083018912, 0.2554414782925553, 0.2529774125841364, 0.24476385945779341, 0.24681724792143647, 0.2472279245236571, 0.25051334667989117, 0.25338809157299064, 0.2455852160647175, 0.23942505099200614, 0.24271047311152277, 0.24476386218100357, 0.2394250515978439, 0.24312114916298178, 0.25133470132855174, 0.2431211507479513, 0.2521560591104339, 0.2480492815589513, 0.24681725027135265, 0.2521560583454872, 0.2386036965391719, 0.2443531843671074, 0.2447638610060455, 0.24353182776018334, 0.23901437259063094, 0.251745380904885, 0.24969199105209883, 0.2431211503562986, 0.22833675571413256, 0.2406570838462156, 0.24271047136744434, 0.24229979551181166, 0.22505133435956262, 0.24188911846286218, 0.24476386159352453, 0.24517453727168959, 0.2533880882072253, 0.2501026692576477, 0.24517453604165534, 0.24476385965361977, 0.24353182658522526, 0.24394250459494776, 0.24599589248947049, 0.24229979431849485, 0.24517453686167817, 0.256262834330359, 0.2533880925521224, 0.2513347019160308, 0.25297741395492085, 0.241478439688193, 0.2513347019160308, 0.25174537972992694, 0.2525667347702402, 0.25256673596355705, 0.24763860550749228, 0.24640657012589903, 0.2562628347220117, 0.2570841893706723, 0.25831622220652306, 0.24599589248947049, 0.25092402586457174, 0.261601644521866, 0.26242299779974215, 0.2591375758760519, 0.2611909672954489, 0.26201232194410945, 0.25790554341349514, 0.2574948660096104, 0.25092402529545144, 0.2636550298706462, 0.2587268996287665, 0.25503080067448547, 0.24353182640775764, 0.24599589386025494, 0.24928131402150805, 0.2542094464174776, 0.25667351153841744, 0.24681724987969997, 0.2554414779009026, 0.2542094464174776, 0.2529774107849818, 0.2562628354869584, 0.25831622144157634, 0.2591375774426627, 0.2529774127799628, 0.2579055432176688, 0.2566735109509384, 0.259137576677716, 0.25462012243221915, 0.2566735113425911, 0.2607802877191156, 0.25338808941890084, 0.25544148064247146, 0.2529774107849818, 0.25626283352869494, 0.2537987686035814, 0.26365503065395157, 0.2611909653371854, 0.2595482558440379, 0.2570841911147507, 0.26324435460249257, 0.2603696114718302, 0.25626283372452124, 0.2636550328080414, 0.26817248562767765, 0.26036960912191404, 0.26078028850242096, 0.26776180800960786, 0.2648870627248556, 0.2661190981248076, 0.263655031437257, 0.2632443549941453, 0.26365503202473606, 0.26694044940770284, 0.2652977407345782, 0.2694045153119481, 0.2677618087929132, 0.27063655076697624, 0.26447638706504933, 0.26406570725617223, 0.2648870648789455, 0.2673511299998853, 0.26570841895848574, 0.2652977407345782, 0.26570841952760604, 0.2607802892857264, 0.2616016416028295, 0.2599589318954969, 0.26488706252902927, 0.2583162229898284, 0.25995893068382137, 0.25338809157299064, 0.24969199163957786, 0.24435318360216074, 0.23326488711261162, 0.24147844027567203, 0.2546201248188528, 0.2616016427594289, 0.2513347030909889, 0.25256673653267736, 0.26324435225257636, 0.25913757726519504, 0.26447638569426485, 0.26078028948155274, 0.26160164432603966, 0.2595482554523852, 0.2574948673620361, 0.26078028654415747, 0.267351127221599, 0.2648870648789455, 0.26858316304992114, 0.26201232194410945, 0.2669404535567736, 0.26776180523132154, 0.2624229979955685, 0.2632443534275345, 0.2652977389354236, 0.26817248562767765, 0.2624230001496583, 0.26365502865897067, 0.26160164393438695, 0.2632443520200326, 0.26735112820073076, 0.26406570862695666, 0.2669404539484263, 0.26694045336094724, 0.2665297745679193, 0.2636550314556157, 0.2673511309790171, 0.26201232137498914, 0.26242299897470023, 0.26488706546642454]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
