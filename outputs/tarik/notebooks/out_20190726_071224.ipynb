{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf31.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 07:12:24 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '2', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ds', 'my', 'mb', 'eg', 'ib', 'sk', 'eb', 'sg', 'ek', 'aa', 'ce', 'yd', 'by', 'ck', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001CAD522D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001CAD09E7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7058, Accuracy:0.0665, Validation Loss:2.7001, Validation Accuracy:0.0821\n",
    "Epoch #2: Loss:2.6971, Accuracy:0.0817, Validation Loss:2.6930, Validation Accuracy:0.0821\n",
    "Epoch #3: Loss:2.6901, Accuracy:0.0875, Validation Loss:2.6862, Validation Accuracy:0.0969\n",
    "Epoch #4: Loss:2.6837, Accuracy:0.1006, Validation Loss:2.6799, Validation Accuracy:0.1166\n",
    "Epoch #5: Loss:2.6780, Accuracy:0.1006, Validation Loss:2.6748, Validation Accuracy:0.1034\n",
    "Epoch #6: Loss:2.6737, Accuracy:0.0908, Validation Loss:2.6700, Validation Accuracy:0.0985\n",
    "Epoch #7: Loss:2.6684, Accuracy:0.0924, Validation Loss:2.6651, Validation Accuracy:0.1100\n",
    "Epoch #8: Loss:2.6633, Accuracy:0.1047, Validation Loss:2.6601, Validation Accuracy:0.1100\n",
    "Epoch #9: Loss:2.6577, Accuracy:0.1146, Validation Loss:2.6539, Validation Accuracy:0.1264\n",
    "Epoch #10: Loss:2.6507, Accuracy:0.1232, Validation Loss:2.6463, Validation Accuracy:0.1379\n",
    "Epoch #11: Loss:2.6420, Accuracy:0.1290, Validation Loss:2.6371, Validation Accuracy:0.1494\n",
    "Epoch #12: Loss:2.6364, Accuracy:0.1216, Validation Loss:2.6316, Validation Accuracy:0.1346\n",
    "Epoch #13: Loss:2.6253, Accuracy:0.1384, Validation Loss:2.6194, Validation Accuracy:0.1511\n",
    "Epoch #14: Loss:2.6133, Accuracy:0.1400, Validation Loss:2.6051, Validation Accuracy:0.1593\n",
    "Epoch #15: Loss:2.5969, Accuracy:0.1540, Validation Loss:2.5902, Validation Accuracy:0.1527\n",
    "Epoch #16: Loss:2.5861, Accuracy:0.1507, Validation Loss:2.5755, Validation Accuracy:0.1642\n",
    "Epoch #17: Loss:2.5644, Accuracy:0.1651, Validation Loss:2.5608, Validation Accuracy:0.1773\n",
    "Epoch #18: Loss:2.5506, Accuracy:0.1598, Validation Loss:2.5470, Validation Accuracy:0.1724\n",
    "Epoch #19: Loss:2.5380, Accuracy:0.1671, Validation Loss:2.5433, Validation Accuracy:0.1609\n",
    "Epoch #20: Loss:2.5240, Accuracy:0.1713, Validation Loss:2.5166, Validation Accuracy:0.1609\n",
    "Epoch #21: Loss:2.5175, Accuracy:0.1630, Validation Loss:2.5143, Validation Accuracy:0.1708\n",
    "Epoch #22: Loss:2.5043, Accuracy:0.1741, Validation Loss:2.5029, Validation Accuracy:0.1724\n",
    "Epoch #23: Loss:2.4928, Accuracy:0.1754, Validation Loss:2.4901, Validation Accuracy:0.1708\n",
    "Epoch #24: Loss:2.4845, Accuracy:0.1778, Validation Loss:2.5049, Validation Accuracy:0.1708\n",
    "Epoch #25: Loss:2.5029, Accuracy:0.1696, Validation Loss:2.4910, Validation Accuracy:0.1708\n",
    "Epoch #26: Loss:2.5011, Accuracy:0.1598, Validation Loss:2.4866, Validation Accuracy:0.1691\n",
    "Epoch #27: Loss:2.4957, Accuracy:0.1684, Validation Loss:2.4874, Validation Accuracy:0.1642\n",
    "Epoch #28: Loss:2.5035, Accuracy:0.1659, Validation Loss:2.4834, Validation Accuracy:0.1642\n",
    "Epoch #29: Loss:2.4920, Accuracy:0.1696, Validation Loss:2.4767, Validation Accuracy:0.1642\n",
    "Epoch #30: Loss:2.4802, Accuracy:0.1647, Validation Loss:2.4722, Validation Accuracy:0.1724\n",
    "Epoch #31: Loss:2.4692, Accuracy:0.1795, Validation Loss:2.4584, Validation Accuracy:0.1544\n",
    "Epoch #32: Loss:2.4834, Accuracy:0.1704, Validation Loss:2.4725, Validation Accuracy:0.1708\n",
    "Epoch #33: Loss:2.4811, Accuracy:0.1754, Validation Loss:2.4714, Validation Accuracy:0.1773\n",
    "Epoch #34: Loss:2.4781, Accuracy:0.1725, Validation Loss:2.4715, Validation Accuracy:0.1741\n",
    "Epoch #35: Loss:2.4790, Accuracy:0.1745, Validation Loss:2.4692, Validation Accuracy:0.1626\n",
    "Epoch #36: Loss:2.4799, Accuracy:0.1717, Validation Loss:2.4656, Validation Accuracy:0.1757\n",
    "Epoch #37: Loss:2.4801, Accuracy:0.1749, Validation Loss:2.4656, Validation Accuracy:0.1741\n",
    "Epoch #38: Loss:2.4789, Accuracy:0.1745, Validation Loss:2.4659, Validation Accuracy:0.1708\n",
    "Epoch #39: Loss:2.4783, Accuracy:0.1762, Validation Loss:2.4679, Validation Accuracy:0.1708\n",
    "Epoch #40: Loss:2.4775, Accuracy:0.1774, Validation Loss:2.4672, Validation Accuracy:0.1708\n",
    "Epoch #41: Loss:2.4767, Accuracy:0.1770, Validation Loss:2.4660, Validation Accuracy:0.1691\n",
    "Epoch #42: Loss:2.4762, Accuracy:0.1774, Validation Loss:2.4654, Validation Accuracy:0.1691\n",
    "Epoch #43: Loss:2.4755, Accuracy:0.1774, Validation Loss:2.4644, Validation Accuracy:0.1708\n",
    "Epoch #44: Loss:2.4758, Accuracy:0.1766, Validation Loss:2.4650, Validation Accuracy:0.1691\n",
    "Epoch #45: Loss:2.4742, Accuracy:0.1774, Validation Loss:2.4628, Validation Accuracy:0.1708\n",
    "Epoch #46: Loss:2.4746, Accuracy:0.1774, Validation Loss:2.4633, Validation Accuracy:0.1691\n",
    "Epoch #47: Loss:2.4739, Accuracy:0.1782, Validation Loss:2.4639, Validation Accuracy:0.1724\n",
    "Epoch #48: Loss:2.4730, Accuracy:0.1774, Validation Loss:2.4613, Validation Accuracy:0.1708\n",
    "Epoch #49: Loss:2.4740, Accuracy:0.1754, Validation Loss:2.4641, Validation Accuracy:0.1708\n",
    "Epoch #50: Loss:2.4728, Accuracy:0.1774, Validation Loss:2.4611, Validation Accuracy:0.1708\n",
    "Epoch #51: Loss:2.4728, Accuracy:0.1766, Validation Loss:2.4615, Validation Accuracy:0.1708\n",
    "Epoch #52: Loss:2.4723, Accuracy:0.1766, Validation Loss:2.4616, Validation Accuracy:0.1691\n",
    "Epoch #53: Loss:2.4721, Accuracy:0.1778, Validation Loss:2.4615, Validation Accuracy:0.1691\n",
    "Epoch #54: Loss:2.4705, Accuracy:0.1774, Validation Loss:2.4616, Validation Accuracy:0.1708\n",
    "Epoch #55: Loss:2.4699, Accuracy:0.1770, Validation Loss:2.4602, Validation Accuracy:0.1708\n",
    "Epoch #56: Loss:2.4706, Accuracy:0.1778, Validation Loss:2.4620, Validation Accuracy:0.1708\n",
    "Epoch #57: Loss:2.4700, Accuracy:0.1766, Validation Loss:2.4590, Validation Accuracy:0.1708\n",
    "Epoch #58: Loss:2.4696, Accuracy:0.1766, Validation Loss:2.4594, Validation Accuracy:0.1708\n",
    "Epoch #59: Loss:2.4700, Accuracy:0.1758, Validation Loss:2.4581, Validation Accuracy:0.1691\n",
    "Epoch #60: Loss:2.4714, Accuracy:0.1778, Validation Loss:2.4594, Validation Accuracy:0.1708\n",
    "Epoch #61: Loss:2.4691, Accuracy:0.1774, Validation Loss:2.4577, Validation Accuracy:0.1724\n",
    "Epoch #62: Loss:2.4688, Accuracy:0.1782, Validation Loss:2.4609, Validation Accuracy:0.1724\n",
    "Epoch #63: Loss:2.4673, Accuracy:0.1745, Validation Loss:2.4576, Validation Accuracy:0.1691\n",
    "Epoch #64: Loss:2.4667, Accuracy:0.1758, Validation Loss:2.4603, Validation Accuracy:0.1724\n",
    "Epoch #65: Loss:2.4680, Accuracy:0.1762, Validation Loss:2.4576, Validation Accuracy:0.1691\n",
    "Epoch #66: Loss:2.4661, Accuracy:0.1774, Validation Loss:2.4584, Validation Accuracy:0.1708\n",
    "Epoch #67: Loss:2.4663, Accuracy:0.1766, Validation Loss:2.4592, Validation Accuracy:0.1724\n",
    "Epoch #68: Loss:2.4660, Accuracy:0.1782, Validation Loss:2.4575, Validation Accuracy:0.1708\n",
    "Epoch #69: Loss:2.4656, Accuracy:0.1778, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #70: Loss:2.4670, Accuracy:0.1766, Validation Loss:2.4574, Validation Accuracy:0.1724\n",
    "Epoch #71: Loss:2.4638, Accuracy:0.1782, Validation Loss:2.4636, Validation Accuracy:0.1724\n",
    "Epoch #72: Loss:2.4648, Accuracy:0.1778, Validation Loss:2.4581, Validation Accuracy:0.1724\n",
    "Epoch #73: Loss:2.4648, Accuracy:0.1774, Validation Loss:2.4585, Validation Accuracy:0.1741\n",
    "Epoch #74: Loss:2.4638, Accuracy:0.1791, Validation Loss:2.4584, Validation Accuracy:0.1708\n",
    "Epoch #75: Loss:2.4635, Accuracy:0.1754, Validation Loss:2.4596, Validation Accuracy:0.1708\n",
    "Epoch #76: Loss:2.4632, Accuracy:0.1754, Validation Loss:2.4584, Validation Accuracy:0.1724\n",
    "Epoch #77: Loss:2.4634, Accuracy:0.1782, Validation Loss:2.4586, Validation Accuracy:0.1741\n",
    "Epoch #78: Loss:2.4628, Accuracy:0.1791, Validation Loss:2.4602, Validation Accuracy:0.1741\n",
    "Epoch #79: Loss:2.4624, Accuracy:0.1782, Validation Loss:2.4579, Validation Accuracy:0.1724\n",
    "Epoch #80: Loss:2.4623, Accuracy:0.1786, Validation Loss:2.4611, Validation Accuracy:0.1741\n",
    "Epoch #81: Loss:2.4632, Accuracy:0.1786, Validation Loss:2.4589, Validation Accuracy:0.1724\n",
    "Epoch #82: Loss:2.4611, Accuracy:0.1786, Validation Loss:2.4577, Validation Accuracy:0.1724\n",
    "Epoch #83: Loss:2.4617, Accuracy:0.1795, Validation Loss:2.4628, Validation Accuracy:0.1741\n",
    "Epoch #84: Loss:2.4623, Accuracy:0.1795, Validation Loss:2.4593, Validation Accuracy:0.1741\n",
    "Epoch #85: Loss:2.4612, Accuracy:0.1778, Validation Loss:2.4582, Validation Accuracy:0.1724\n",
    "Epoch #86: Loss:2.4609, Accuracy:0.1782, Validation Loss:2.4596, Validation Accuracy:0.1741\n",
    "Epoch #87: Loss:2.4602, Accuracy:0.1795, Validation Loss:2.4576, Validation Accuracy:0.1724\n",
    "Epoch #88: Loss:2.4612, Accuracy:0.1786, Validation Loss:2.4606, Validation Accuracy:0.1741\n",
    "Epoch #89: Loss:2.4630, Accuracy:0.1778, Validation Loss:2.4593, Validation Accuracy:0.1741\n",
    "Epoch #90: Loss:2.4616, Accuracy:0.1782, Validation Loss:2.4578, Validation Accuracy:0.1724\n",
    "Epoch #91: Loss:2.4621, Accuracy:0.1795, Validation Loss:2.4622, Validation Accuracy:0.1741\n",
    "Epoch #92: Loss:2.4608, Accuracy:0.1795, Validation Loss:2.4572, Validation Accuracy:0.1741\n",
    "Epoch #93: Loss:2.4606, Accuracy:0.1762, Validation Loss:2.4618, Validation Accuracy:0.1691\n",
    "Epoch #94: Loss:2.4604, Accuracy:0.1786, Validation Loss:2.4569, Validation Accuracy:0.1741\n",
    "Epoch #95: Loss:2.4599, Accuracy:0.1791, Validation Loss:2.4587, Validation Accuracy:0.1741\n",
    "Epoch #96: Loss:2.4592, Accuracy:0.1799, Validation Loss:2.4590, Validation Accuracy:0.1741\n",
    "Epoch #97: Loss:2.4591, Accuracy:0.1799, Validation Loss:2.4575, Validation Accuracy:0.1724\n",
    "Epoch #98: Loss:2.4596, Accuracy:0.1786, Validation Loss:2.4607, Validation Accuracy:0.1741\n",
    "Epoch #99: Loss:2.4594, Accuracy:0.1782, Validation Loss:2.4575, Validation Accuracy:0.1724\n",
    "Epoch #100: Loss:2.4593, Accuracy:0.1791, Validation Loss:2.4596, Validation Accuracy:0.1741\n",
    "Epoch #101: Loss:2.4585, Accuracy:0.1774, Validation Loss:2.4572, Validation Accuracy:0.1724\n",
    "Epoch #102: Loss:2.4586, Accuracy:0.1778, Validation Loss:2.4588, Validation Accuracy:0.1724\n",
    "Epoch #103: Loss:2.4596, Accuracy:0.1795, Validation Loss:2.4581, Validation Accuracy:0.1724\n",
    "Epoch #104: Loss:2.4582, Accuracy:0.1778, Validation Loss:2.4571, Validation Accuracy:0.1724\n",
    "Epoch #105: Loss:2.4584, Accuracy:0.1774, Validation Loss:2.4608, Validation Accuracy:0.1741\n",
    "Epoch #106: Loss:2.4587, Accuracy:0.1791, Validation Loss:2.4596, Validation Accuracy:0.1741\n",
    "Epoch #107: Loss:2.4583, Accuracy:0.1791, Validation Loss:2.4575, Validation Accuracy:0.1724\n",
    "Epoch #108: Loss:2.4585, Accuracy:0.1782, Validation Loss:2.4588, Validation Accuracy:0.1675\n",
    "Epoch #109: Loss:2.4580, Accuracy:0.1770, Validation Loss:2.4582, Validation Accuracy:0.1724\n",
    "Epoch #110: Loss:2.4579, Accuracy:0.1774, Validation Loss:2.4581, Validation Accuracy:0.1741\n",
    "Epoch #111: Loss:2.4577, Accuracy:0.1782, Validation Loss:2.4589, Validation Accuracy:0.1741\n",
    "Epoch #112: Loss:2.4577, Accuracy:0.1791, Validation Loss:2.4584, Validation Accuracy:0.1741\n",
    "Epoch #113: Loss:2.4579, Accuracy:0.1782, Validation Loss:2.4575, Validation Accuracy:0.1724\n",
    "Epoch #114: Loss:2.4584, Accuracy:0.1803, Validation Loss:2.4601, Validation Accuracy:0.1691\n",
    "Epoch #115: Loss:2.4586, Accuracy:0.1778, Validation Loss:2.4576, Validation Accuracy:0.1724\n",
    "Epoch #116: Loss:2.4567, Accuracy:0.1782, Validation Loss:2.4617, Validation Accuracy:0.1691\n",
    "Epoch #117: Loss:2.4574, Accuracy:0.1803, Validation Loss:2.4576, Validation Accuracy:0.1658\n",
    "Epoch #118: Loss:2.4566, Accuracy:0.1791, Validation Loss:2.4593, Validation Accuracy:0.1675\n",
    "Epoch #119: Loss:2.4566, Accuracy:0.1791, Validation Loss:2.4583, Validation Accuracy:0.1675\n",
    "Epoch #120: Loss:2.4571, Accuracy:0.1807, Validation Loss:2.4578, Validation Accuracy:0.1691\n",
    "Epoch #121: Loss:2.4566, Accuracy:0.1766, Validation Loss:2.4579, Validation Accuracy:0.1741\n",
    "Epoch #122: Loss:2.4563, Accuracy:0.1799, Validation Loss:2.4598, Validation Accuracy:0.1675\n",
    "Epoch #123: Loss:2.4561, Accuracy:0.1799, Validation Loss:2.4578, Validation Accuracy:0.1658\n",
    "Epoch #124: Loss:2.4577, Accuracy:0.1786, Validation Loss:2.4593, Validation Accuracy:0.1675\n",
    "Epoch #125: Loss:2.4576, Accuracy:0.1803, Validation Loss:2.4610, Validation Accuracy:0.1691\n",
    "Epoch #126: Loss:2.4568, Accuracy:0.1733, Validation Loss:2.4572, Validation Accuracy:0.1724\n",
    "Epoch #127: Loss:2.4561, Accuracy:0.1778, Validation Loss:2.4637, Validation Accuracy:0.1691\n",
    "Epoch #128: Loss:2.4614, Accuracy:0.1782, Validation Loss:2.4571, Validation Accuracy:0.1675\n",
    "Epoch #129: Loss:2.4579, Accuracy:0.1799, Validation Loss:2.4560, Validation Accuracy:0.1708\n",
    "Epoch #130: Loss:2.4552, Accuracy:0.1823, Validation Loss:2.4604, Validation Accuracy:0.1691\n",
    "Epoch #131: Loss:2.4566, Accuracy:0.1807, Validation Loss:2.4555, Validation Accuracy:0.1691\n",
    "Epoch #132: Loss:2.4558, Accuracy:0.1811, Validation Loss:2.4567, Validation Accuracy:0.1691\n",
    "Epoch #133: Loss:2.4553, Accuracy:0.1799, Validation Loss:2.4571, Validation Accuracy:0.1691\n",
    "Epoch #134: Loss:2.4554, Accuracy:0.1811, Validation Loss:2.4561, Validation Accuracy:0.1691\n",
    "Epoch #135: Loss:2.4553, Accuracy:0.1811, Validation Loss:2.4569, Validation Accuracy:0.1691\n",
    "Epoch #136: Loss:2.4555, Accuracy:0.1811, Validation Loss:2.4554, Validation Accuracy:0.1708\n",
    "Epoch #137: Loss:2.4556, Accuracy:0.1807, Validation Loss:2.4564, Validation Accuracy:0.1691\n",
    "Epoch #138: Loss:2.4550, Accuracy:0.1815, Validation Loss:2.4554, Validation Accuracy:0.1675\n",
    "Epoch #139: Loss:2.4551, Accuracy:0.1803, Validation Loss:2.4552, Validation Accuracy:0.1675\n",
    "Epoch #140: Loss:2.4551, Accuracy:0.1807, Validation Loss:2.4544, Validation Accuracy:0.1691\n",
    "Epoch #141: Loss:2.4547, Accuracy:0.1799, Validation Loss:2.4585, Validation Accuracy:0.1658\n",
    "Epoch #142: Loss:2.4550, Accuracy:0.1795, Validation Loss:2.4524, Validation Accuracy:0.1691\n",
    "Epoch #143: Loss:2.4546, Accuracy:0.1795, Validation Loss:2.4524, Validation Accuracy:0.1675\n",
    "Epoch #144: Loss:2.4551, Accuracy:0.1782, Validation Loss:2.4512, Validation Accuracy:0.1691\n",
    "Epoch #145: Loss:2.4557, Accuracy:0.1786, Validation Loss:2.4509, Validation Accuracy:0.1691\n",
    "Epoch #146: Loss:2.4544, Accuracy:0.1786, Validation Loss:2.4559, Validation Accuracy:0.1675\n",
    "Epoch #147: Loss:2.4543, Accuracy:0.1795, Validation Loss:2.4517, Validation Accuracy:0.1708\n",
    "Epoch #148: Loss:2.4543, Accuracy:0.1786, Validation Loss:2.4523, Validation Accuracy:0.1691\n",
    "Epoch #149: Loss:2.4548, Accuracy:0.1782, Validation Loss:2.4497, Validation Accuracy:0.1724\n",
    "Epoch #150: Loss:2.4548, Accuracy:0.1782, Validation Loss:2.4448, Validation Accuracy:0.1790\n",
    "Epoch #151: Loss:2.4520, Accuracy:0.1791, Validation Loss:2.4486, Validation Accuracy:0.1691\n",
    "Epoch #152: Loss:2.4528, Accuracy:0.1803, Validation Loss:2.4452, Validation Accuracy:0.1757\n",
    "Epoch #153: Loss:2.4500, Accuracy:0.1745, Validation Loss:2.4474, Validation Accuracy:0.1773\n",
    "Epoch #154: Loss:2.4500, Accuracy:0.1786, Validation Loss:2.4608, Validation Accuracy:0.1593\n",
    "Epoch #155: Loss:2.4545, Accuracy:0.1786, Validation Loss:2.4389, Validation Accuracy:0.1823\n",
    "Epoch #156: Loss:2.4465, Accuracy:0.1811, Validation Loss:2.4429, Validation Accuracy:0.1806\n",
    "Epoch #157: Loss:2.4509, Accuracy:0.1774, Validation Loss:2.4600, Validation Accuracy:0.1626\n",
    "Epoch #158: Loss:2.4521, Accuracy:0.1795, Validation Loss:2.4480, Validation Accuracy:0.1691\n",
    "Epoch #159: Loss:2.4536, Accuracy:0.1778, Validation Loss:2.4438, Validation Accuracy:0.1757\n",
    "Epoch #160: Loss:2.4498, Accuracy:0.1795, Validation Loss:2.4555, Validation Accuracy:0.1642\n",
    "Epoch #161: Loss:2.4521, Accuracy:0.1795, Validation Loss:2.4472, Validation Accuracy:0.1757\n",
    "Epoch #162: Loss:2.4533, Accuracy:0.1729, Validation Loss:2.4438, Validation Accuracy:0.1773\n",
    "Epoch #163: Loss:2.4503, Accuracy:0.1836, Validation Loss:2.4552, Validation Accuracy:0.1658\n",
    "Epoch #164: Loss:2.4508, Accuracy:0.1803, Validation Loss:2.4477, Validation Accuracy:0.1741\n",
    "Epoch #165: Loss:2.4495, Accuracy:0.1782, Validation Loss:2.4453, Validation Accuracy:0.1675\n",
    "Epoch #166: Loss:2.4488, Accuracy:0.1823, Validation Loss:2.4577, Validation Accuracy:0.1609\n",
    "Epoch #167: Loss:2.4477, Accuracy:0.1795, Validation Loss:2.4507, Validation Accuracy:0.1691\n",
    "Epoch #168: Loss:2.4490, Accuracy:0.1823, Validation Loss:2.4522, Validation Accuracy:0.1691\n",
    "Epoch #169: Loss:2.4469, Accuracy:0.1828, Validation Loss:2.4523, Validation Accuracy:0.1658\n",
    "Epoch #170: Loss:2.4478, Accuracy:0.1828, Validation Loss:2.4522, Validation Accuracy:0.1675\n",
    "Epoch #171: Loss:2.4484, Accuracy:0.1811, Validation Loss:2.4479, Validation Accuracy:0.1675\n",
    "Epoch #172: Loss:2.4487, Accuracy:0.1815, Validation Loss:2.4525, Validation Accuracy:0.1658\n",
    "Epoch #173: Loss:2.4477, Accuracy:0.1807, Validation Loss:2.4489, Validation Accuracy:0.1658\n",
    "Epoch #174: Loss:2.4460, Accuracy:0.1811, Validation Loss:2.4463, Validation Accuracy:0.1691\n",
    "Epoch #175: Loss:2.4462, Accuracy:0.1795, Validation Loss:2.4477, Validation Accuracy:0.1675\n",
    "Epoch #176: Loss:2.4459, Accuracy:0.1803, Validation Loss:2.4480, Validation Accuracy:0.1675\n",
    "Epoch #177: Loss:2.4458, Accuracy:0.1815, Validation Loss:2.4464, Validation Accuracy:0.1675\n",
    "Epoch #178: Loss:2.4458, Accuracy:0.1803, Validation Loss:2.4472, Validation Accuracy:0.1675\n",
    "Epoch #179: Loss:2.4452, Accuracy:0.1807, Validation Loss:2.4478, Validation Accuracy:0.1675\n",
    "Epoch #180: Loss:2.4454, Accuracy:0.1799, Validation Loss:2.4481, Validation Accuracy:0.1658\n",
    "Epoch #181: Loss:2.4454, Accuracy:0.1807, Validation Loss:2.4476, Validation Accuracy:0.1675\n",
    "Epoch #182: Loss:2.4440, Accuracy:0.1811, Validation Loss:2.4461, Validation Accuracy:0.1593\n",
    "Epoch #183: Loss:2.4430, Accuracy:0.1840, Validation Loss:2.4461, Validation Accuracy:0.1741\n",
    "Epoch #184: Loss:2.4531, Accuracy:0.1721, Validation Loss:2.4454, Validation Accuracy:0.1741\n",
    "Epoch #185: Loss:2.4424, Accuracy:0.1778, Validation Loss:2.4449, Validation Accuracy:0.1609\n",
    "Epoch #186: Loss:2.4442, Accuracy:0.1782, Validation Loss:2.4466, Validation Accuracy:0.1708\n",
    "Epoch #187: Loss:2.4446, Accuracy:0.1795, Validation Loss:2.4454, Validation Accuracy:0.1724\n",
    "Epoch #188: Loss:2.4439, Accuracy:0.1811, Validation Loss:2.4461, Validation Accuracy:0.1773\n",
    "Epoch #189: Loss:2.4427, Accuracy:0.1799, Validation Loss:2.4460, Validation Accuracy:0.1708\n",
    "Epoch #190: Loss:2.4456, Accuracy:0.1795, Validation Loss:2.4431, Validation Accuracy:0.1741\n",
    "Epoch #191: Loss:2.4507, Accuracy:0.1741, Validation Loss:2.4576, Validation Accuracy:0.1773\n",
    "Epoch #192: Loss:2.4504, Accuracy:0.1758, Validation Loss:2.4646, Validation Accuracy:0.1560\n",
    "Epoch #193: Loss:2.4557, Accuracy:0.1778, Validation Loss:2.4557, Validation Accuracy:0.1626\n",
    "Epoch #194: Loss:2.4471, Accuracy:0.1786, Validation Loss:2.4486, Validation Accuracy:0.1757\n",
    "Epoch #195: Loss:2.4480, Accuracy:0.1745, Validation Loss:2.4472, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4455, Accuracy:0.1786, Validation Loss:2.4538, Validation Accuracy:0.1626\n",
    "Epoch #197: Loss:2.4447, Accuracy:0.1782, Validation Loss:2.4453, Validation Accuracy:0.1691\n",
    "Epoch #198: Loss:2.4447, Accuracy:0.1807, Validation Loss:2.4443, Validation Accuracy:0.1741\n",
    "Epoch #199: Loss:2.4422, Accuracy:0.1799, Validation Loss:2.4491, Validation Accuracy:0.1675\n",
    "Epoch #200: Loss:2.4433, Accuracy:0.1807, Validation Loss:2.4476, Validation Accuracy:0.1741\n",
    "Epoch #201: Loss:2.4438, Accuracy:0.1795, Validation Loss:2.4447, Validation Accuracy:0.1741\n",
    "Epoch #202: Loss:2.4427, Accuracy:0.1791, Validation Loss:2.4505, Validation Accuracy:0.1741\n",
    "Epoch #203: Loss:2.4468, Accuracy:0.1758, Validation Loss:2.4488, Validation Accuracy:0.1757\n",
    "Epoch #204: Loss:2.4425, Accuracy:0.1840, Validation Loss:2.4467, Validation Accuracy:0.1773\n",
    "Epoch #205: Loss:2.4439, Accuracy:0.1832, Validation Loss:2.4511, Validation Accuracy:0.1773\n",
    "Epoch #206: Loss:2.4417, Accuracy:0.1795, Validation Loss:2.4542, Validation Accuracy:0.1757\n",
    "Epoch #207: Loss:2.4410, Accuracy:0.1823, Validation Loss:2.4505, Validation Accuracy:0.1790\n",
    "Epoch #208: Loss:2.4422, Accuracy:0.1795, Validation Loss:2.4490, Validation Accuracy:0.1741\n",
    "Epoch #209: Loss:2.4409, Accuracy:0.1799, Validation Loss:2.4520, Validation Accuracy:0.1708\n",
    "Epoch #210: Loss:2.4437, Accuracy:0.1819, Validation Loss:2.4536, Validation Accuracy:0.1658\n",
    "Epoch #211: Loss:2.4422, Accuracy:0.1807, Validation Loss:2.4488, Validation Accuracy:0.1708\n",
    "Epoch #212: Loss:2.4429, Accuracy:0.1795, Validation Loss:2.4490, Validation Accuracy:0.1708\n",
    "Epoch #213: Loss:2.4404, Accuracy:0.1832, Validation Loss:2.4545, Validation Accuracy:0.1675\n",
    "Epoch #214: Loss:2.4400, Accuracy:0.1799, Validation Loss:2.4503, Validation Accuracy:0.1724\n",
    "Epoch #215: Loss:2.4397, Accuracy:0.1811, Validation Loss:2.4485, Validation Accuracy:0.1757\n",
    "Epoch #216: Loss:2.4387, Accuracy:0.1758, Validation Loss:2.4515, Validation Accuracy:0.1741\n",
    "Epoch #217: Loss:2.4375, Accuracy:0.1782, Validation Loss:2.4524, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.4394, Accuracy:0.1840, Validation Loss:2.4435, Validation Accuracy:0.1790\n",
    "Epoch #219: Loss:2.4404, Accuracy:0.1754, Validation Loss:2.4436, Validation Accuracy:0.1741\n",
    "Epoch #220: Loss:2.4416, Accuracy:0.1733, Validation Loss:2.4518, Validation Accuracy:0.1724\n",
    "Epoch #221: Loss:2.4405, Accuracy:0.1799, Validation Loss:2.4424, Validation Accuracy:0.1757\n",
    "Epoch #222: Loss:2.4387, Accuracy:0.1799, Validation Loss:2.4440, Validation Accuracy:0.1773\n",
    "Epoch #223: Loss:2.4375, Accuracy:0.1799, Validation Loss:2.4459, Validation Accuracy:0.1691\n",
    "Epoch #224: Loss:2.4384, Accuracy:0.1840, Validation Loss:2.4460, Validation Accuracy:0.1675\n",
    "Epoch #225: Loss:2.4375, Accuracy:0.1860, Validation Loss:2.4459, Validation Accuracy:0.1773\n",
    "Epoch #226: Loss:2.4392, Accuracy:0.1836, Validation Loss:2.4481, Validation Accuracy:0.1675\n",
    "Epoch #227: Loss:2.4384, Accuracy:0.1877, Validation Loss:2.4460, Validation Accuracy:0.1757\n",
    "Epoch #228: Loss:2.4388, Accuracy:0.1836, Validation Loss:2.4470, Validation Accuracy:0.1757\n",
    "Epoch #229: Loss:2.4388, Accuracy:0.1848, Validation Loss:2.4492, Validation Accuracy:0.1724\n",
    "Epoch #230: Loss:2.4395, Accuracy:0.1873, Validation Loss:2.4453, Validation Accuracy:0.1757\n",
    "Epoch #231: Loss:2.4405, Accuracy:0.1852, Validation Loss:2.4501, Validation Accuracy:0.1724\n",
    "Epoch #232: Loss:2.4397, Accuracy:0.1877, Validation Loss:2.4456, Validation Accuracy:0.1708\n",
    "Epoch #233: Loss:2.4384, Accuracy:0.1844, Validation Loss:2.4469, Validation Accuracy:0.1691\n",
    "Epoch #234: Loss:2.4375, Accuracy:0.1856, Validation Loss:2.4476, Validation Accuracy:0.1691\n",
    "Epoch #235: Loss:2.4373, Accuracy:0.1885, Validation Loss:2.4480, Validation Accuracy:0.1691\n",
    "Epoch #236: Loss:2.4377, Accuracy:0.1848, Validation Loss:2.4481, Validation Accuracy:0.1773\n",
    "Epoch #237: Loss:2.4376, Accuracy:0.1856, Validation Loss:2.4476, Validation Accuracy:0.1691\n",
    "Epoch #238: Loss:2.4372, Accuracy:0.1877, Validation Loss:2.4473, Validation Accuracy:0.1757\n",
    "Epoch #239: Loss:2.4371, Accuracy:0.1885, Validation Loss:2.4458, Validation Accuracy:0.1773\n",
    "Epoch #240: Loss:2.4374, Accuracy:0.1840, Validation Loss:2.4481, Validation Accuracy:0.1691\n",
    "Epoch #241: Loss:2.4377, Accuracy:0.1823, Validation Loss:2.4465, Validation Accuracy:0.1708\n",
    "Epoch #242: Loss:2.4368, Accuracy:0.1844, Validation Loss:2.4467, Validation Accuracy:0.1691\n",
    "Epoch #243: Loss:2.4363, Accuracy:0.1848, Validation Loss:2.4454, Validation Accuracy:0.1724\n",
    "Epoch #244: Loss:2.4357, Accuracy:0.1877, Validation Loss:2.4440, Validation Accuracy:0.1708\n",
    "Epoch #245: Loss:2.4347, Accuracy:0.1864, Validation Loss:2.4444, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4353, Accuracy:0.1844, Validation Loss:2.4446, Validation Accuracy:0.1708\n",
    "Epoch #247: Loss:2.4347, Accuracy:0.1848, Validation Loss:2.4391, Validation Accuracy:0.1708\n",
    "Epoch #248: Loss:2.4377, Accuracy:0.1860, Validation Loss:2.4585, Validation Accuracy:0.1757\n",
    "Epoch #249: Loss:2.4411, Accuracy:0.1832, Validation Loss:2.4434, Validation Accuracy:0.1839\n",
    "Epoch #250: Loss:2.4413, Accuracy:0.1770, Validation Loss:2.4440, Validation Accuracy:0.1823\n",
    "Epoch #251: Loss:2.4395, Accuracy:0.1786, Validation Loss:2.4516, Validation Accuracy:0.1741\n",
    "Epoch #252: Loss:2.4383, Accuracy:0.1844, Validation Loss:2.4442, Validation Accuracy:0.1724\n",
    "Epoch #253: Loss:2.4379, Accuracy:0.1864, Validation Loss:2.4459, Validation Accuracy:0.1724\n",
    "Epoch #254: Loss:2.4383, Accuracy:0.1844, Validation Loss:2.4489, Validation Accuracy:0.1757\n",
    "Epoch #255: Loss:2.4374, Accuracy:0.1836, Validation Loss:2.4455, Validation Accuracy:0.1724\n",
    "Epoch #256: Loss:2.4385, Accuracy:0.1782, Validation Loss:2.4449, Validation Accuracy:0.1773\n",
    "Epoch #257: Loss:2.4354, Accuracy:0.1844, Validation Loss:2.4493, Validation Accuracy:0.1741\n",
    "Epoch #258: Loss:2.4349, Accuracy:0.1852, Validation Loss:2.4479, Validation Accuracy:0.1708\n",
    "Epoch #259: Loss:2.4352, Accuracy:0.1823, Validation Loss:2.4463, Validation Accuracy:0.1708\n",
    "Epoch #260: Loss:2.4352, Accuracy:0.1864, Validation Loss:2.4519, Validation Accuracy:0.1708\n",
    "Epoch #261: Loss:2.4344, Accuracy:0.1881, Validation Loss:2.4493, Validation Accuracy:0.1773\n",
    "Epoch #262: Loss:2.4348, Accuracy:0.1860, Validation Loss:2.4500, Validation Accuracy:0.1773\n",
    "Epoch #263: Loss:2.4339, Accuracy:0.1873, Validation Loss:2.4455, Validation Accuracy:0.1773\n",
    "Epoch #264: Loss:2.4347, Accuracy:0.1885, Validation Loss:2.4459, Validation Accuracy:0.1741\n",
    "Epoch #265: Loss:2.4345, Accuracy:0.1877, Validation Loss:2.4496, Validation Accuracy:0.1741\n",
    "Epoch #266: Loss:2.4344, Accuracy:0.1873, Validation Loss:2.4473, Validation Accuracy:0.1806\n",
    "Epoch #267: Loss:2.4340, Accuracy:0.1860, Validation Loss:2.4484, Validation Accuracy:0.1724\n",
    "Epoch #268: Loss:2.4346, Accuracy:0.1877, Validation Loss:2.4494, Validation Accuracy:0.1724\n",
    "Epoch #269: Loss:2.4338, Accuracy:0.1889, Validation Loss:2.4466, Validation Accuracy:0.1741\n",
    "Epoch #270: Loss:2.4343, Accuracy:0.1881, Validation Loss:2.4492, Validation Accuracy:0.1724\n",
    "Epoch #271: Loss:2.4338, Accuracy:0.1860, Validation Loss:2.4459, Validation Accuracy:0.1757\n",
    "Epoch #272: Loss:2.4322, Accuracy:0.1869, Validation Loss:2.4430, Validation Accuracy:0.1757\n",
    "Epoch #273: Loss:2.4327, Accuracy:0.1877, Validation Loss:2.4449, Validation Accuracy:0.1757\n",
    "Epoch #274: Loss:2.4330, Accuracy:0.1864, Validation Loss:2.4438, Validation Accuracy:0.1757\n",
    "Epoch #275: Loss:2.4325, Accuracy:0.1860, Validation Loss:2.4447, Validation Accuracy:0.1757\n",
    "Epoch #276: Loss:2.4332, Accuracy:0.1864, Validation Loss:2.4460, Validation Accuracy:0.1757\n",
    "Epoch #277: Loss:2.4329, Accuracy:0.1860, Validation Loss:2.4447, Validation Accuracy:0.1757\n",
    "Epoch #278: Loss:2.4332, Accuracy:0.1864, Validation Loss:2.4469, Validation Accuracy:0.1757\n",
    "Epoch #279: Loss:2.4324, Accuracy:0.1864, Validation Loss:2.4453, Validation Accuracy:0.1757\n",
    "Epoch #280: Loss:2.4321, Accuracy:0.1840, Validation Loss:2.4440, Validation Accuracy:0.1806\n",
    "Epoch #281: Loss:2.4324, Accuracy:0.1881, Validation Loss:2.4474, Validation Accuracy:0.1757\n",
    "Epoch #282: Loss:2.4323, Accuracy:0.1873, Validation Loss:2.4463, Validation Accuracy:0.1806\n",
    "Epoch #283: Loss:2.4315, Accuracy:0.1844, Validation Loss:2.4457, Validation Accuracy:0.1773\n",
    "Epoch #284: Loss:2.4316, Accuracy:0.1877, Validation Loss:2.4468, Validation Accuracy:0.1757\n",
    "Epoch #285: Loss:2.4328, Accuracy:0.1836, Validation Loss:2.4462, Validation Accuracy:0.1773\n",
    "Epoch #286: Loss:2.4318, Accuracy:0.1848, Validation Loss:2.4467, Validation Accuracy:0.1790\n",
    "Epoch #287: Loss:2.4329, Accuracy:0.1836, Validation Loss:2.4495, Validation Accuracy:0.1757\n",
    "Epoch #288: Loss:2.4317, Accuracy:0.1869, Validation Loss:2.4432, Validation Accuracy:0.1741\n",
    "Epoch #289: Loss:2.4313, Accuracy:0.1828, Validation Loss:2.4452, Validation Accuracy:0.1773\n",
    "Epoch #290: Loss:2.4307, Accuracy:0.1836, Validation Loss:2.4458, Validation Accuracy:0.1741\n",
    "Epoch #291: Loss:2.4304, Accuracy:0.1856, Validation Loss:2.4448, Validation Accuracy:0.1757\n",
    "Epoch #292: Loss:2.4293, Accuracy:0.1864, Validation Loss:2.4439, Validation Accuracy:0.1773\n",
    "Epoch #293: Loss:2.4304, Accuracy:0.1848, Validation Loss:2.4433, Validation Accuracy:0.1741\n",
    "Epoch #294: Loss:2.4302, Accuracy:0.1840, Validation Loss:2.4469, Validation Accuracy:0.1724\n",
    "Epoch #295: Loss:2.4309, Accuracy:0.1860, Validation Loss:2.4447, Validation Accuracy:0.1741\n",
    "Epoch #296: Loss:2.4310, Accuracy:0.1864, Validation Loss:2.4423, Validation Accuracy:0.1773\n",
    "Epoch #297: Loss:2.4300, Accuracy:0.1844, Validation Loss:2.4451, Validation Accuracy:0.1741\n",
    "Epoch #298: Loss:2.4310, Accuracy:0.1848, Validation Loss:2.4455, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.4302, Accuracy:0.1844, Validation Loss:2.4444, Validation Accuracy:0.1773\n",
    "Epoch #300: Loss:2.4296, Accuracy:0.1844, Validation Loss:2.4470, Validation Accuracy:0.1773\n",
    "\n",
    "Test:\n",
    "Test Loss:2.44696164, Accuracy:0.1773\n",
    "Labels: ['ds', 'my', 'mb', 'eg', 'ib', 'sk', 'eb', 'sg', 'ek', 'aa', 'ce', 'yd', 'by', 'ck', 'eo']\n",
    "Confusion Matrix:\n",
    "      ds  my  mb  eg  ib  sk  eb  sg  ek  aa  ce  yd  by  ck  eo\n",
    "t:ds   8   0   0  13   0   0   1   3   0   0   0   0   6   0   0\n",
    "t:my   2   0   0   5   0   0   0   4   0   0   0   5   4   0   0\n",
    "t:mb   0   0   0  13   0   0   1  21   0   0   0   5  12   0   0\n",
    "t:eg   8   0   0  31   0   0   1   4   0   0   0   0   6   0   0\n",
    "t:ib   0   0   0   3   0   0   0  15   0   0   0  30   6   0   0\n",
    "t:sk   5   0   0  11   0   0   0   9   0   0   0   4   4   0   0\n",
    "t:eb   1   0   0  20   0   0   0   9   0   0   0   7  13   0   0\n",
    "t:sg   0   0   0   4   0   0   0  25   0   0   0   8  14   0   0\n",
    "t:ek   0   0   0  17   0   0   0  12   0   0   0   1  18   0   0\n",
    "t:aa   5   0   0  15   0   0   1   5   0   0   0   2   6   0   0\n",
    "t:ce   2   0   0   7   0   0   1   8   0   0   0   1   8   0   0\n",
    "t:yd   0   0   0   2   0   0   0  25   0   0   0  30   5   0   0\n",
    "t:by   0   0   0  12   0   0   0   9   0   0   0   5  14   0   0\n",
    "t:ck   1   0   0  13   0   0   0   3   0   0   0   0   6   0   0\n",
    "t:eo   0   0   0   1   0   0   0  11   0   0   0   5  17   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ds       0.25      0.26      0.25        31\n",
    "          my       0.00      0.00      0.00        20\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eg       0.19      0.62      0.29        50\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          sg       0.15      0.49      0.23        51\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          yd       0.29      0.48      0.36        62\n",
    "          by       0.10      0.35      0.16        40\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eo       0.00      0.00      0.00        34\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.07      0.15      0.09       609\n",
    "weighted avg       0.08      0.18      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 07:28:10 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7000888223914288, 2.6929769719567007, 2.6862002628777413, 2.6799230160580088, 2.6747791892397776, 2.669978745465208, 2.6651027030349757, 2.660102664151998, 2.6538770465036525, 2.6463183131319745, 2.637091585372273, 2.6315818150055232, 2.619427905294108, 2.6051124003524655, 2.590229214901603, 2.5754935518274165, 2.5607599252941964, 2.5469654177992997, 2.5433243894811923, 2.5165963376488394, 2.514344664434298, 2.5028752712976363, 2.4901337709724416, 2.504916095577046, 2.491002034671201, 2.4865739693978344, 2.4874045398630726, 2.48340053393923, 2.4767112403080382, 2.472161152484186, 2.4583735047107065, 2.472511415606845, 2.4713520424315103, 2.4715278872911175, 2.4692457881075605, 2.465625163173832, 2.465631165919437, 2.465920416983869, 2.467921358023958, 2.467158843731058, 2.465958746000268, 2.4654241269836676, 2.464435387127505, 2.465042277901435, 2.462823623506894, 2.4633310843375322, 2.463937803638001, 2.4613048454810835, 2.464055231247825, 2.461051666873625, 2.4614619042094312, 2.4615996972289187, 2.461527864725523, 2.4615786608021053, 2.460174351881682, 2.462011489962122, 2.4589683751167333, 2.459363448404522, 2.4581490674825335, 2.459372229568281, 2.457674781677171, 2.460887391383229, 2.4576363234684386, 2.4603180713058497, 2.4576220261835307, 2.4583735963198157, 2.4591950736022348, 2.457462625550519, 2.4614620106952336, 2.4573522905998044, 2.463638416064784, 2.4581414349560666, 2.4584960373751636, 2.458390443783088, 2.459555886257654, 2.458418402570026, 2.458618432625957, 2.460181909633192, 2.457924943447896, 2.4610866470681425, 2.458900171351942, 2.457703051308693, 2.4628210365282883, 2.459285357119806, 2.4582381534263225, 2.459629968665112, 2.457596041298852, 2.4605536899347413, 2.4593031465126374, 2.4578269530203936, 2.4621812980163273, 2.4571765671026924, 2.461756765744565, 2.4569478066292496, 2.4587110466949262, 2.459039149808962, 2.4574926075676977, 2.4607136840695034, 2.4575040919831626, 2.4595648995761215, 2.4571965069606385, 2.4587620952837965, 2.4581233390250623, 2.457101267155364, 2.4607821951554523, 2.459592908082533, 2.4575112124381984, 2.4587962408175414, 2.4582204666043737, 2.458074259640548, 2.458862425658503, 2.458437370353536, 2.4574562446237196, 2.460054538128607, 2.4575828875618417, 2.4617496327617876, 2.4576352053675157, 2.4592715730808052, 2.458258906804478, 2.4578111825513917, 2.457943631706175, 2.459773009242291, 2.4577775459571427, 2.4593314960085113, 2.4610238490237784, 2.45718558864249, 2.463725526148854, 2.4571218705921143, 2.4559916545604836, 2.460410192486492, 2.4555221500459368, 2.456677003447058, 2.4571496559481316, 2.4561085681414174, 2.456891948170654, 2.4554149509454986, 2.4564307217527492, 2.455424828678125, 2.4551678036625555, 2.4543899933888604, 2.458538202229392, 2.45237810701768, 2.4524004357593205, 2.4511904880918305, 2.4509206867374616, 2.455928029293693, 2.4517353144772533, 2.4523125309466534, 2.4496892077973715, 2.4447526121374423, 2.448588314119035, 2.445179918129456, 2.447408299140742, 2.46082436196714, 2.438907966237937, 2.4428530310958085, 2.4599558214835935, 2.447965828655976, 2.443829464794967, 2.4554821305674284, 2.4472027549211224, 2.4438057822742683, 2.455197274391287, 2.447662270127846, 2.445255112373966, 2.4576791973145333, 2.4507093828887188, 2.4521871078973527, 2.452291770913135, 2.4521825023864094, 2.447905174029872, 2.452533433785775, 2.4489089820185317, 2.4462519240105287, 2.4477264771516296, 2.4480484052636156, 2.4464437068976794, 2.447235780787977, 2.4478043065282513, 2.4481000050731088, 2.44764415149031, 2.4461225167479617, 2.44612616271221, 2.445352621657899, 2.44485515130956, 2.446600118489884, 2.445390680153382, 2.4460875749196522, 2.446044876853429, 2.443118963335535, 2.457600953347969, 2.4645733692375895, 2.455722618181326, 2.4486244911043515, 2.447218807264306, 2.4537782269745625, 2.4452981737446904, 2.4442629062483463, 2.4490599871073253, 2.4475671313274865, 2.4446742699064057, 2.4505350906860652, 2.448846420435287, 2.446690008166584, 2.4510513315059868, 2.4541630153781284, 2.450515494558024, 2.449016863489386, 2.4519527992004244, 2.4536341371990384, 2.448776753665191, 2.449040876429265, 2.454505984223339, 2.450326589136484, 2.448510822208448, 2.451537549985062, 2.4524382949854155, 2.44349262084084, 2.443633794001562, 2.451777720490504, 2.44239241775425, 2.444048539758316, 2.445913801835284, 2.4460318957643556, 2.445891807036251, 2.448086055824518, 2.44596473532553, 2.446955089302877, 2.449151955997611, 2.4452972369045263, 2.450088044105492, 2.4455866774510477, 2.4468725471465262, 2.447622799912501, 2.448012128056368, 2.448071669670944, 2.4475775215230358, 2.447324225468001, 2.4458151807142987, 2.4480882665793886, 2.446536260285401, 2.4467130149722296, 2.4454356316470944, 2.443950521534887, 2.44440699798133, 2.4446124263193414, 2.439074197621964, 2.4585028340663815, 2.443446194597066, 2.443952091417485, 2.4515847384636036, 2.4442198069029057, 2.445860876825643, 2.448926358387388, 2.4455258497855152, 2.444942012013277, 2.4492682318382077, 2.4478882607959567, 2.446330229832817, 2.4518702570440736, 2.4492972774067145, 2.449979070763674, 2.4454531540424367, 2.445931652300855, 2.4496217502161786, 2.4473164265574687, 2.4484370028835603, 2.449420748477303, 2.4466190506476293, 2.449153871567574, 2.4458531299835355, 2.4430175904178464, 2.4448727539607455, 2.443754839779708, 2.444720274904874, 2.4459683801153025, 2.444712099770607, 2.4468959928146137, 2.4453192309205756, 2.4439813679662246, 2.4474007946321334, 2.446313963735045, 2.4457090171099884, 2.446844424715966, 2.446172671952271, 2.4467400423998904, 2.449540074822938, 2.4431540284837996, 2.4451859114792547, 2.445797612514402, 2.4448045822982913, 2.4439498943648315, 2.4432644068901173, 2.446904538691729, 2.444706034777787, 2.4422870105123287, 2.445097755328775, 2.4455015275670196, 2.4444215971065075, 2.4469617078652717], 'val_acc': [0.0821018060317572, 0.0821018060317572, 0.09688013135065586, 0.11658456405297485, 0.10344827485887093, 0.0985221674754506, 0.11001642034901382, 0.11001641985964893, 0.12643678120546936, 0.13793103437265153, 0.14942528724621473, 0.13464696212306201, 0.15106732337100948, 0.15927750389711023, 0.15270935949580422, 0.16420361236936745, 0.17733990047463447, 0.17241379219812322, 0.16091953942243298, 0.16091953932456, 0.17077175617120144, 0.17241379229599618, 0.17077175607332848, 0.1707717559754555, 0.1707717559754555, 0.1691297200464067, 0.16420361157414948, 0.16420361157414948, 0.16420361147627652, 0.17241379210025023, 0.15435139482538102, 0.17077175617120144, 0.1773399007682534, 0.1740558285186639, 0.1625615752536088, 0.1756978644477127, 0.17405582832291797, 0.17077175607332848, 0.17077175607332848, 0.17077175607332848, 0.1691297199485337, 0.1691297199485337, 0.17077175617120144, 0.1691297199485337, 0.17077175617120144, 0.1691297199485337, 0.17241379239386917, 0.17077175617120144, 0.17077175626907443, 0.17077175617120144, 0.17077175617120144, 0.1691297200464067, 0.1691297200464067, 0.17077175626907443, 0.17077175617120144, 0.17077175626907443, 0.17077175617120144, 0.17077175626907443, 0.1691297200464067, 0.17077175626907443, 0.17241379219812322, 0.17241379229599618, 0.1691297200464067, 0.17241379239386917, 0.1691297200464067, 0.17077175617120144, 0.17241379239386917, 0.17077175607332848, 0.17241379229599618, 0.17241379219812322, 0.17241379229599618, 0.17241379219812322, 0.17405582842079093, 0.17077175617120144, 0.17077175617120144, 0.17241379219812322, 0.17405582842079093, 0.17405582842079093, 0.17241379219812322, 0.17405582842079093, 0.17241379219812322, 0.17241379219812322, 0.17405582842079093, 0.17405582842079093, 0.17241379219812322, 0.17405582842079093, 0.17241379219812322, 0.17405582842079093, 0.17405582842079093, 0.17241379219812322, 0.17405582842079093, 0.17405582832291797, 0.1691297199485337, 0.17405582832291797, 0.17405582842079093, 0.17405582842079093, 0.17241379219812322, 0.17405582842079093, 0.17241379219812322, 0.17405582842079093, 0.17241379219812322, 0.17241379219812322, 0.17241379219812322, 0.17241379219812322, 0.17405582842079093, 0.17405582842079093, 0.17241379219812322, 0.167487683725866, 0.17241379219812322, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17241379219812322, 0.1691297199485337, 0.17241379219812322, 0.1691297199485337, 0.16584564760107126, 0.16748768382373896, 0.16748768382373896, 0.1691297199485337, 0.17405582842079093, 0.16748768382373896, 0.16584564760107126, 0.16748768382373896, 0.1691297199485337, 0.17241379219812322, 0.1691297199485337, 0.167487683725866, 0.17077175607332848, 0.1691297199485337, 0.16912971985066075, 0.1691297199485337, 0.1691297199485337, 0.16912971985066075, 0.16912971985066075, 0.1707717559754555, 0.16912971985066075, 0.167487683725866, 0.167487683725866, 0.16912971985066075, 0.16584564760107126, 0.16912971985066075, 0.167487683725866, 0.16912971985066075, 0.16912971985066075, 0.167487683725866, 0.1707717559754555, 0.16912971985066075, 0.17241379219812322, 0.1789819366973022, 0.16912971985066075, 0.1756978644477127, 0.17733990047463447, 0.1592775033955112, 0.18226600894689168, 0.18062397282209694, 0.16256157535148175, 0.16912971975278776, 0.17569786434983972, 0.16420361147627652, 0.1756978644477127, 0.17733990057250745, 0.1658456477968172, 0.17405582822504498, 0.167487683725866, 0.160919539226687, 0.16912971985066075, 0.16912971985066075, 0.16584564760107126, 0.167487683725866, 0.167487683725866, 0.16584564760107126, 0.16584564760107126, 0.16912971985066075, 0.167487683725866, 0.167487683725866, 0.167487683725866, 0.167487683725866, 0.167487683725866, 0.16584564760107126, 0.167487683725866, 0.1592775030040193, 0.17405582822504498, 0.17405582822504498, 0.160919539226687, 0.17077175607332848, 0.17241379219812322, 0.17733990057250745, 0.1707717559754555, 0.17405582832291797, 0.1773399014655984, 0.1559934312437947, 0.1625615756451007, 0.17569786425196673, 0.17241379219812322, 0.16256157554722772, 0.16912971985066075, 0.17405582822504498, 0.167487683725866, 0.17405582822504498, 0.17405582822504498, 0.17405582832291797, 0.17569786503495058, 0.17733990057250745, 0.17733990067038044, 0.17569786483920463, 0.17898193679517518, 0.17405582822504498, 0.17077175607332848, 0.16584564799256318, 0.17077175607332848, 0.17077175607332848, 0.1674876842152309, 0.17241379229599618, 0.17569786454558567, 0.17405582891015584, 0.16912972034002563, 0.17898193679517518, 0.17405582832291797, 0.17241379258961514, 0.17569786454558567, 0.17733990067038044, 0.16912972034002563, 0.1674876842152309, 0.17733990057250745, 0.16748768411735793, 0.1756978644477127, 0.1756978644477127, 0.17241379258961514, 0.1756978644477127, 0.17241379258961514, 0.17077175607332848, 0.16912972024215267, 0.16912972034002563, 0.16912972034002563, 0.17733990057250745, 0.16912972034002563, 0.17569786483920463, 0.17733990057250745, 0.16912972034002563, 0.17077175607332848, 0.16912972024215267, 0.17241379249174216, 0.17077175646482037, 0.16912972024215267, 0.1707717563669474, 0.17077175607332848, 0.17569786503495058, 0.18390804516955941, 0.18226600904476467, 0.17405582881228285, 0.17241379258961514, 0.17241379258961514, 0.1756978649370776, 0.17241379258961514, 0.17733990096399938, 0.17405582881228285, 0.17077175656269336, 0.17077175646482037, 0.17077175646482037, 0.17733990096399938, 0.17733990106187233, 0.17733990106187233, 0.17405582881228285, 0.17405582881228285, 0.18062397331146185, 0.1724137926874881, 0.1724137926874881, 0.17405582881228285, 0.1724137926874881, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.18062397331146185, 0.1756978649370776, 0.18062397331146185, 0.17733990106187233, 0.1756978649370776, 0.17733990106187233, 0.1789819371866671, 0.1756978649370776, 0.17405582881228285, 0.17733990106187233, 0.17405582881228285, 0.1756978649370776, 0.17733990106187233, 0.17405582881228285, 0.1724137926874881, 0.17405582881228285, 0.17733990106187233, 0.17405582881228285, 0.17405582881228285, 0.17733990106187233, 0.17733990106187233], 'loss': [2.7058446340737157, 2.697088651833348, 2.690099556881789, 2.6836677885153457, 2.6780081346539255, 2.673687205226514, 2.6684312231976395, 2.663265998162773, 2.6577249327234664, 2.6507346744654847, 2.642002008680935, 2.6363835633413015, 2.625308903139964, 2.613308289506352, 2.5969048179884955, 2.5861299914256257, 2.5643753554052395, 2.5506282824020854, 2.5379748825175072, 2.5239952446743694, 2.5175083656820183, 2.5042874093418, 2.4928303809626144, 2.484479186275412, 2.5028574155341428, 2.5011391516338874, 2.495675454149501, 2.5034527181355126, 2.4920362897477357, 2.480220536382781, 2.4692327727527346, 2.483445527372419, 2.48111498850327, 2.4780511166770354, 2.479016983337716, 2.4799296344819743, 2.480111511923694, 2.4788795571062843, 2.4782867330790057, 2.4775017598326445, 2.476743959939945, 2.4762135148293183, 2.475458860495252, 2.475751588770496, 2.474196508043356, 2.4745679740788264, 2.473918278457203, 2.4729947847017764, 2.4739605287751623, 2.472780066300222, 2.4728360061528014, 2.4722955524554244, 2.472123864152348, 2.4705410135844894, 2.4699028413643336, 2.4706211487615377, 2.4699852434271903, 2.4696160819741, 2.469970180366563, 2.4714225478240843, 2.469075268935374, 2.4687781090119536, 2.467272735963857, 2.4666686277369942, 2.4679670133140297, 2.466095798754839, 2.466252739669361, 2.466017567989028, 2.4655635034768735, 2.467042011891547, 2.463797231572365, 2.4648163753368526, 2.4647872332429985, 2.4638111874308186, 2.4635190551530655, 2.463174328128415, 2.463442017361369, 2.4628032057681857, 2.462423816598661, 2.46226879223661, 2.463212955365191, 2.46114130617412, 2.4616589244631037, 2.4623358354431404, 2.461239861071232, 2.4609499683615117, 2.4602095298453768, 2.461158784850667, 2.46304316246534, 2.461554316475651, 2.4620534664796363, 2.4607577369932767, 2.460618892587431, 2.4603616041814034, 2.459851393415698, 2.4591958446424353, 2.459139827634275, 2.4595735390328284, 2.4593732036849065, 2.459301588618535, 2.458513587505176, 2.458551806835668, 2.459648614840341, 2.4582298796769284, 2.4583929052098332, 2.458657056839804, 2.4583173866389467, 2.4584906144308603, 2.4579932599586627, 2.457944574101505, 2.457687425907143, 2.457712320233762, 2.4579472969689653, 2.4584138360111623, 2.4586119473592456, 2.4567194453010326, 2.457420520273322, 2.456614144382046, 2.4566047989612243, 2.4570934237885522, 2.456623814482953, 2.45627295221881, 2.4561139254599382, 2.457679154593842, 2.457610587760408, 2.456845252176085, 2.456066864080253, 2.4613659617837205, 2.457909940351451, 2.455190347695008, 2.456617955358611, 2.455839286937361, 2.455261532432979, 2.4554250058207425, 2.4552798438610726, 2.4555449263515903, 2.4555763716570405, 2.4549726554745277, 2.4551278630321276, 2.4551455090422896, 2.454702592042927, 2.4549569364935464, 2.4546348950701327, 2.455101152122388, 2.455683446273177, 2.454401389186632, 2.4543251008223703, 2.454345044463077, 2.4548035693119683, 2.4548175692313507, 2.451982435504514, 2.452848930182643, 2.4500305466583376, 2.450021423892074, 2.4544660483052843, 2.4465215827894897, 2.4509280195960765, 2.452142762354512, 2.453645002866428, 2.4498422270193236, 2.452104945936732, 2.4533385811400366, 2.4503412924263266, 2.450781423697971, 2.449460365346325, 2.4487522492418545, 2.4477010021953856, 2.448966124415153, 2.4469347044183, 2.4477836721486868, 2.448354049384961, 2.4487026599398383, 2.4477244007024432, 2.4459802958510006, 2.4462079562200905, 2.445893137459882, 2.4458340029941694, 2.445820548940733, 2.445205223878551, 2.4454083135240623, 2.4454387592339173, 2.444026612843821, 2.4430316043830262, 2.453109065707949, 2.442375679721088, 2.444241439928999, 2.4446128077575557, 2.443920585800735, 2.442663960779962, 2.4455505898845757, 2.450688522405448, 2.4503802369997, 2.455676458601589, 2.4471132140384806, 2.4480005831199505, 2.44554887924351, 2.444659000941126, 2.444701065615707, 2.442240936702282, 2.4433292331147243, 2.443781396740516, 2.44265083385444, 2.4467907641213045, 2.442542831119326, 2.44389318098033, 2.441730571331674, 2.440966117063832, 2.442176635456281, 2.4408993150174494, 2.443667384584337, 2.44223573555447, 2.4428968864299923, 2.4403651250216507, 2.440004365400122, 2.4397204290425263, 2.4387472665774994, 2.4374527944921223, 2.4394109477252686, 2.4404467200841258, 2.4415605219972205, 2.440492217986246, 2.438662962355408, 2.437519462789109, 2.4383908162126797, 2.437510689083311, 2.4391721978079857, 2.438419857553878, 2.4387700712411555, 2.4388049601529413, 2.4394574793946817, 2.4405443976791976, 2.439720365398963, 2.438414086551392, 2.4374750222513564, 2.437326571975645, 2.4377451869251794, 2.43764220067363, 2.4371581655263412, 2.4370514292981347, 2.4373685201335493, 2.4377324196347465, 2.436827654809188, 2.436333346024186, 2.435680549002771, 2.434723987716424, 2.435276768486603, 2.4346833029322066, 2.437733257575691, 2.4411320407287787, 2.441321989838837, 2.4394682820572746, 2.438284652678629, 2.437938646222532, 2.438309381140331, 2.4373674456343757, 2.438516023271627, 2.435362226811278, 2.4349405787074345, 2.4351534349962427, 2.435186525243019, 2.4344301121925183, 2.4347524216042897, 2.433931370486469, 2.4346905267703702, 2.4345019231831513, 2.4344065155091963, 2.434032470636544, 2.4346454265915636, 2.4337690645174814, 2.4343166368942732, 2.433833281362326, 2.4322077853968502, 2.432706036851636, 2.4329503986624967, 2.432546541578226, 2.4332007348415052, 2.4328911977137384, 2.4332295789855705, 2.432388498259276, 2.4321373181666193, 2.432395423560172, 2.4322891376346534, 2.4314937833398274, 2.431639339889589, 2.4328463982262893, 2.431753452210945, 2.4328708487124904, 2.4316549385352793, 2.431315391900848, 2.4306527709569283, 2.4303624336234844, 2.4293161629161797, 2.4304470397118916, 2.4301554724910663, 2.4308743779419384, 2.431006644149091, 2.4300411117639875, 2.4310131178744276, 2.4302011584843943, 2.429580426754648], 'acc': [0.06652977447306596, 0.08172484614582277, 0.08747433318127353, 0.10061601676368126, 0.10061601598037587, 0.09075975355671172, 0.09240246361897957, 0.10472279216107402, 0.11457905497639086, 0.12320328493749826, 0.12895277100299662, 0.12156057508023613, 0.13839835661025507, 0.14004106786583972, 0.15400410688021346, 0.15071868652313397, 0.16509240254973973, 0.15975359294571181, 0.1671457900709685, 0.17125256603748157, 0.16303901389027034, 0.17412730955979663, 0.17535934262819114, 0.17782340773077226, 0.1696098567585191, 0.15975359351483215, 0.1683778229068192, 0.16591375680674764, 0.1696098571318131, 0.16468172439008769, 0.17946611879053057, 0.17043121197630004, 0.1753593420407121, 0.17248459910587607, 0.17453798657202868, 0.1716632440472041, 0.17494866518758895, 0.1745379885670096, 0.17618069704430794, 0.1774127303268875, 0.177002052690459, 0.1774127303268875, 0.1774127299168761, 0.17659137546404186, 0.17741273089600784, 0.17741273148348688, 0.17823408572213606, 0.1774127308776491, 0.17535934380314924, 0.17741273011270245, 0.17659137468073646, 0.17659137487656282, 0.1778234075349459, 0.17741273130601926, 0.1770020527088177, 0.17782340929738305, 0.17659137587405327, 0.17659137546404186, 0.17577002161704539, 0.17782340812242498, 0.17741273050435516, 0.17823408711127922, 0.17453798876283594, 0.1757700200504346, 0.17618069784597204, 0.17741273068182278, 0.17659137626570598, 0.1782340865421589, 0.17782340792659862, 0.17659137567822694, 0.1782340867196265, 0.17782340792659862, 0.17741273169767197, 0.17905544076244934, 0.1753593428240175, 0.17535934241400608, 0.1782340857404948, 0.17905544236577756, 0.17823408711127922, 0.17864476357274967, 0.17864476316273825, 0.17864476355439093, 0.17946611939636833, 0.17946611939636833, 0.17782340773077226, 0.1782340857588535, 0.17946611916382454, 0.17864476394604364, 0.17782340794495732, 0.1782340865421589, 0.17946611902307436, 0.1794661195921947, 0.1761806980417984, 0.17864476335856458, 0.1790554417599398, 0.17987679583948007, 0.17987679640860038, 0.17864476376857602, 0.17823408632797383, 0.1790554409766344, 0.17741273148348688, 0.1778234075349459, 0.17946611920054198, 0.17782340849571893, 0.17741273128766055, 0.1790554421515925, 0.17905544156411346, 0.17823408634633253, 0.1770020528679266, 0.17741273208932465, 0.17823408691545287, 0.1790554423841363, 0.1782340865421589, 0.18028747404502893, 0.17782340794495732, 0.17823408595467985, 0.1802874744183229, 0.17905544038915536, 0.17905544195576614, 0.18069815105726098, 0.17659137626570598, 0.17987679740609086, 0.1798767960169477, 0.1786447629669119, 0.18028747484669302, 0.1733059559086265, 0.17782340851407766, 0.17823408556302714, 0.17987679719190577, 0.1823408615111815, 0.18069815225057778, 0.18110882967282124, 0.17987679621277403, 0.18110883043776793, 0.18110883006447395, 0.1811088294769949, 0.18069815125308733, 0.1815195082700228, 0.1802874740266702, 0.18069815184056637, 0.17987679762027592, 0.1794661188088893, 0.1794661185947042, 0.17823408613214747, 0.17864476277108554, 0.1786447645518814, 0.1794661193780096, 0.17864476277108554, 0.1782340861505062, 0.17823408712963792, 0.1790554421515925, 0.18028747343919116, 0.17453798678621374, 0.17864476396440235, 0.17864476314437952, 0.18110882985028887, 0.1774127303085288, 0.17946611978802104, 0.17782340794495732, 0.17946611898635692, 0.17946611838051915, 0.17289527631393448, 0.18357289614618683, 0.180287475219987, 0.17823408554466844, 0.18234086193955165, 0.1794661195921947, 0.18234086290032467, 0.18275154073257954, 0.18275154112423225, 0.18110882926280983, 0.1815195063117593, 0.1806981530522419, 0.1811088290853422, 0.1794661197696623, 0.18028747424085528, 0.1815195062934006, 0.18028747422249655, 0.1806981526422305, 0.179876795625295, 0.18069815223221905, 0.18110883004611522, 0.1839835739784417, 0.172073922466938, 0.17782340851407766, 0.17823408652380018, 0.17946611902307436, 0.18110882926280983, 0.17987679699607942, 0.1794661193780096, 0.17412731055728708, 0.17577002163540412, 0.1778234083182513, 0.17864476357274967, 0.17453798700039883, 0.17864476414187, 0.1782340867196265, 0.18069815223221905, 0.1798767960169477, 0.18069815125308733, 0.17946611800722517, 0.17905544117246075, 0.17577001942623813, 0.1839835739784417, 0.18316221713897385, 0.17946611998384737, 0.18234086209866054, 0.17946612017967373, 0.1798767968002531, 0.18193018510478723, 0.18069815164474, 0.17946611918218328, 0.18316221835064936, 0.17987679738773213, 0.18110883004611522, 0.1757700196404232, 0.17823408595467985, 0.18398357178763441, 0.17535934202235337, 0.17330595492949477, 0.17987679738773213, 0.17987679621277403, 0.17987679582112134, 0.18398357239347218, 0.18603696165877937, 0.183572894579576, 0.18767967193523227, 0.18357289616454553, 0.18480492762961181, 0.18726899372968342, 0.18521560683265115, 0.18767967115192688, 0.18439425157815278, 0.18562628245574003, 0.1885010275630246, 0.18480492702377405, 0.18562628423653588, 0.18767967035026276, 0.18850102695718682, 0.1839835728034836, 0.18234086233120433, 0.18439425140068516, 0.18480492741542676, 0.18767967113356815, 0.18644763909938153, 0.18439425038483598, 0.18480492702377405, 0.18603696011052728, 0.18316221872394334, 0.17700205228044757, 0.17864476355439093, 0.18439424960153059, 0.18644763788770602, 0.1843942504031947, 0.18357289477540237, 0.17823408632797383, 0.18439424981571564, 0.18521560485602895, 0.1823408622944869, 0.18644763788770602, 0.18809034816415893, 0.18603696205043205, 0.1872689949046415, 0.18850102715301317, 0.18767967074191547, 0.18726899273219294, 0.1860369610713003, 0.18767967193523227, 0.18891170478944172, 0.18809034796833257, 0.1860369598779835, 0.1868583173049304, 0.18767967173940592, 0.18644763769187966, 0.18603696009216858, 0.18644763888519647, 0.18603696067964762, 0.1864476377102384, 0.18644763788770602, 0.18398357258929854, 0.18809034896582305, 0.18726899472717387, 0.18439425059902104, 0.18767967117028558, 0.18357289634201315, 0.18480492803962323, 0.18357289477540237, 0.1868583157199609, 0.1827515397350891, 0.18357289616454553, 0.1856262822782724, 0.18644763888519647, 0.18480492782543817, 0.1839835720018195, 0.18603696048382126, 0.18644763966850186, 0.18439425020736835, 0.18480492762961181, 0.18439424981571564, 0.18439424961988932]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
