{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf16.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 21:00:20 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '0Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eb', 'ck', 'mb', 'yd', 'aa', 'eo', 'ek', 'sk', 'ib', 'ds', 'sg', 'my', 'by', 'ce', 'eg'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000012B0058E278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000012B67CE7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6970, Accuracy:0.0854, Validation Loss:2.6917, Validation Accuracy:0.0854\n",
    "Epoch #2: Loss:2.6890, Accuracy:0.0846, Validation Loss:2.6834, Validation Accuracy:0.0854\n",
    "Epoch #3: Loss:2.6805, Accuracy:0.0846, Validation Loss:2.6757, Validation Accuracy:0.0854\n",
    "Epoch #4: Loss:2.6751, Accuracy:0.0903, Validation Loss:2.6722, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6709, Accuracy:0.0990, Validation Loss:2.6686, Validation Accuracy:0.0854\n",
    "Epoch #6: Loss:2.6668, Accuracy:0.0903, Validation Loss:2.6656, Validation Accuracy:0.0887\n",
    "Epoch #7: Loss:2.6633, Accuracy:0.0891, Validation Loss:2.6621, Validation Accuracy:0.0870\n",
    "Epoch #8: Loss:2.6599, Accuracy:0.0887, Validation Loss:2.6584, Validation Accuracy:0.0854\n",
    "Epoch #9: Loss:2.6561, Accuracy:0.0858, Validation Loss:2.6538, Validation Accuracy:0.0805\n",
    "Epoch #10: Loss:2.6514, Accuracy:0.0871, Validation Loss:2.6494, Validation Accuracy:0.0821\n",
    "Epoch #11: Loss:2.6456, Accuracy:0.0883, Validation Loss:2.6434, Validation Accuracy:0.0936\n",
    "Epoch #12: Loss:2.6389, Accuracy:0.0969, Validation Loss:2.6361, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6294, Accuracy:0.1097, Validation Loss:2.6265, Validation Accuracy:0.1084\n",
    "Epoch #14: Loss:2.6175, Accuracy:0.1154, Validation Loss:2.6151, Validation Accuracy:0.1149\n",
    "Epoch #15: Loss:2.6033, Accuracy:0.1133, Validation Loss:2.6016, Validation Accuracy:0.1084\n",
    "Epoch #16: Loss:2.5856, Accuracy:0.1125, Validation Loss:2.5859, Validation Accuracy:0.1182\n",
    "Epoch #17: Loss:2.5660, Accuracy:0.1302, Validation Loss:2.5714, Validation Accuracy:0.1248\n",
    "Epoch #18: Loss:2.5449, Accuracy:0.1396, Validation Loss:2.5583, Validation Accuracy:0.1461\n",
    "Epoch #19: Loss:2.5274, Accuracy:0.1614, Validation Loss:2.5452, Validation Accuracy:0.1511\n",
    "Epoch #20: Loss:2.5148, Accuracy:0.1569, Validation Loss:2.5341, Validation Accuracy:0.1494\n",
    "Epoch #21: Loss:2.5009, Accuracy:0.1667, Validation Loss:2.5277, Validation Accuracy:0.1609\n",
    "Epoch #22: Loss:2.4945, Accuracy:0.1639, Validation Loss:2.5231, Validation Accuracy:0.1527\n",
    "Epoch #23: Loss:2.4871, Accuracy:0.1676, Validation Loss:2.5172, Validation Accuracy:0.1527\n",
    "Epoch #24: Loss:2.5008, Accuracy:0.1639, Validation Loss:2.5270, Validation Accuracy:0.1724\n",
    "Epoch #25: Loss:2.5095, Accuracy:0.1655, Validation Loss:2.5205, Validation Accuracy:0.1560\n",
    "Epoch #26: Loss:2.5006, Accuracy:0.1667, Validation Loss:2.5022, Validation Accuracy:0.1708\n",
    "Epoch #27: Loss:2.4847, Accuracy:0.1676, Validation Loss:2.4999, Validation Accuracy:0.1691\n",
    "Epoch #28: Loss:2.4749, Accuracy:0.1721, Validation Loss:2.5031, Validation Accuracy:0.1626\n",
    "Epoch #29: Loss:2.4720, Accuracy:0.1708, Validation Loss:2.4994, Validation Accuracy:0.1773\n",
    "Epoch #30: Loss:2.4695, Accuracy:0.1749, Validation Loss:2.4968, Validation Accuracy:0.1839\n",
    "Epoch #31: Loss:2.4668, Accuracy:0.1754, Validation Loss:2.4921, Validation Accuracy:0.1823\n",
    "Epoch #32: Loss:2.4644, Accuracy:0.1725, Validation Loss:2.4908, Validation Accuracy:0.1806\n",
    "Epoch #33: Loss:2.4610, Accuracy:0.1700, Validation Loss:2.4888, Validation Accuracy:0.1724\n",
    "Epoch #34: Loss:2.4578, Accuracy:0.1713, Validation Loss:2.4874, Validation Accuracy:0.1724\n",
    "Epoch #35: Loss:2.4551, Accuracy:0.1745, Validation Loss:2.4837, Validation Accuracy:0.1691\n",
    "Epoch #36: Loss:2.4561, Accuracy:0.1770, Validation Loss:2.4806, Validation Accuracy:0.1658\n",
    "Epoch #37: Loss:2.4594, Accuracy:0.1778, Validation Loss:2.4757, Validation Accuracy:0.1658\n",
    "Epoch #38: Loss:2.4615, Accuracy:0.1733, Validation Loss:2.4766, Validation Accuracy:0.1708\n",
    "Epoch #39: Loss:2.4598, Accuracy:0.1725, Validation Loss:2.4758, Validation Accuracy:0.1790\n",
    "Epoch #40: Loss:2.4598, Accuracy:0.1737, Validation Loss:2.4781, Validation Accuracy:0.1691\n",
    "Epoch #41: Loss:2.4576, Accuracy:0.1737, Validation Loss:2.4780, Validation Accuracy:0.1675\n",
    "Epoch #42: Loss:2.4588, Accuracy:0.1754, Validation Loss:2.4826, Validation Accuracy:0.1658\n",
    "Epoch #43: Loss:2.4993, Accuracy:0.1544, Validation Loss:2.4884, Validation Accuracy:0.1708\n",
    "Epoch #44: Loss:2.5015, Accuracy:0.1659, Validation Loss:2.4825, Validation Accuracy:0.1757\n",
    "Epoch #45: Loss:2.4832, Accuracy:0.1676, Validation Loss:2.5065, Validation Accuracy:0.1593\n",
    "Epoch #46: Loss:2.4802, Accuracy:0.1696, Validation Loss:2.4886, Validation Accuracy:0.1691\n",
    "Epoch #47: Loss:2.4746, Accuracy:0.1618, Validation Loss:2.4751, Validation Accuracy:0.1593\n",
    "Epoch #48: Loss:2.4612, Accuracy:0.1721, Validation Loss:2.4816, Validation Accuracy:0.1741\n",
    "Epoch #49: Loss:2.4596, Accuracy:0.1754, Validation Loss:2.4780, Validation Accuracy:0.1708\n",
    "Epoch #50: Loss:2.4549, Accuracy:0.1766, Validation Loss:2.4747, Validation Accuracy:0.1708\n",
    "Epoch #51: Loss:2.4546, Accuracy:0.1754, Validation Loss:2.4741, Validation Accuracy:0.1675\n",
    "Epoch #52: Loss:2.4519, Accuracy:0.1774, Validation Loss:2.4742, Validation Accuracy:0.1675\n",
    "Epoch #53: Loss:2.4509, Accuracy:0.1758, Validation Loss:2.4711, Validation Accuracy:0.1708\n",
    "Epoch #54: Loss:2.4499, Accuracy:0.1754, Validation Loss:2.4711, Validation Accuracy:0.1708\n",
    "Epoch #55: Loss:2.4491, Accuracy:0.1758, Validation Loss:2.4713, Validation Accuracy:0.1658\n",
    "Epoch #56: Loss:2.4481, Accuracy:0.1782, Validation Loss:2.4677, Validation Accuracy:0.1724\n",
    "Epoch #57: Loss:2.4478, Accuracy:0.1774, Validation Loss:2.4685, Validation Accuracy:0.1757\n",
    "Epoch #58: Loss:2.4477, Accuracy:0.1774, Validation Loss:2.4662, Validation Accuracy:0.1741\n",
    "Epoch #59: Loss:2.4478, Accuracy:0.1778, Validation Loss:2.4654, Validation Accuracy:0.1757\n",
    "Epoch #60: Loss:2.4466, Accuracy:0.1762, Validation Loss:2.4667, Validation Accuracy:0.1757\n",
    "Epoch #61: Loss:2.4460, Accuracy:0.1774, Validation Loss:2.4665, Validation Accuracy:0.1741\n",
    "Epoch #62: Loss:2.4452, Accuracy:0.1758, Validation Loss:2.4654, Validation Accuracy:0.1757\n",
    "Epoch #63: Loss:2.4433, Accuracy:0.1766, Validation Loss:2.4679, Validation Accuracy:0.1691\n",
    "Epoch #64: Loss:2.4438, Accuracy:0.1754, Validation Loss:2.4662, Validation Accuracy:0.1757\n",
    "Epoch #65: Loss:2.4427, Accuracy:0.1758, Validation Loss:2.4697, Validation Accuracy:0.1806\n",
    "Epoch #66: Loss:2.4421, Accuracy:0.1754, Validation Loss:2.4683, Validation Accuracy:0.1724\n",
    "Epoch #67: Loss:2.4409, Accuracy:0.1786, Validation Loss:2.4696, Validation Accuracy:0.1691\n",
    "Epoch #68: Loss:2.4386, Accuracy:0.1733, Validation Loss:2.4662, Validation Accuracy:0.1691\n",
    "Epoch #69: Loss:2.4390, Accuracy:0.1778, Validation Loss:2.4654, Validation Accuracy:0.1708\n",
    "Epoch #70: Loss:2.4420, Accuracy:0.1766, Validation Loss:2.4660, Validation Accuracy:0.1790\n",
    "Epoch #71: Loss:2.4418, Accuracy:0.1774, Validation Loss:2.4641, Validation Accuracy:0.1773\n",
    "Epoch #72: Loss:2.4398, Accuracy:0.1754, Validation Loss:2.4683, Validation Accuracy:0.1675\n",
    "Epoch #73: Loss:2.4417, Accuracy:0.1774, Validation Loss:2.4660, Validation Accuracy:0.1757\n",
    "Epoch #74: Loss:2.4405, Accuracy:0.1807, Validation Loss:2.4666, Validation Accuracy:0.1773\n",
    "Epoch #75: Loss:2.4401, Accuracy:0.1799, Validation Loss:2.4607, Validation Accuracy:0.1790\n",
    "Epoch #76: Loss:2.4428, Accuracy:0.1795, Validation Loss:2.4607, Validation Accuracy:0.1790\n",
    "Epoch #77: Loss:2.4462, Accuracy:0.1791, Validation Loss:2.4640, Validation Accuracy:0.1806\n",
    "Epoch #78: Loss:2.4441, Accuracy:0.1791, Validation Loss:2.4615, Validation Accuracy:0.1773\n",
    "Epoch #79: Loss:2.4426, Accuracy:0.1795, Validation Loss:2.4628, Validation Accuracy:0.1839\n",
    "Epoch #80: Loss:2.4415, Accuracy:0.1807, Validation Loss:2.4605, Validation Accuracy:0.1839\n",
    "Epoch #81: Loss:2.4419, Accuracy:0.1774, Validation Loss:2.4612, Validation Accuracy:0.1757\n",
    "Epoch #82: Loss:2.4423, Accuracy:0.1770, Validation Loss:2.4613, Validation Accuracy:0.1823\n",
    "Epoch #83: Loss:2.4410, Accuracy:0.1815, Validation Loss:2.4595, Validation Accuracy:0.1806\n",
    "Epoch #84: Loss:2.4416, Accuracy:0.1795, Validation Loss:2.4616, Validation Accuracy:0.1823\n",
    "Epoch #85: Loss:2.4403, Accuracy:0.1815, Validation Loss:2.4614, Validation Accuracy:0.1823\n",
    "Epoch #86: Loss:2.4395, Accuracy:0.1799, Validation Loss:2.4620, Validation Accuracy:0.1773\n",
    "Epoch #87: Loss:2.4393, Accuracy:0.1811, Validation Loss:2.4624, Validation Accuracy:0.1790\n",
    "Epoch #88: Loss:2.4398, Accuracy:0.1795, Validation Loss:2.4612, Validation Accuracy:0.1773\n",
    "Epoch #89: Loss:2.4431, Accuracy:0.1811, Validation Loss:2.4631, Validation Accuracy:0.1790\n",
    "Epoch #90: Loss:2.4452, Accuracy:0.1782, Validation Loss:2.4705, Validation Accuracy:0.1773\n",
    "Epoch #91: Loss:2.4437, Accuracy:0.1807, Validation Loss:2.4687, Validation Accuracy:0.1757\n",
    "Epoch #92: Loss:2.4422, Accuracy:0.1795, Validation Loss:2.4720, Validation Accuracy:0.1757\n",
    "Epoch #93: Loss:2.4426, Accuracy:0.1795, Validation Loss:2.4704, Validation Accuracy:0.1790\n",
    "Epoch #94: Loss:2.4417, Accuracy:0.1795, Validation Loss:2.4620, Validation Accuracy:0.1823\n",
    "Epoch #95: Loss:2.4394, Accuracy:0.1786, Validation Loss:2.4624, Validation Accuracy:0.1823\n",
    "Epoch #96: Loss:2.4391, Accuracy:0.1770, Validation Loss:2.4620, Validation Accuracy:0.1823\n",
    "Epoch #97: Loss:2.4377, Accuracy:0.1766, Validation Loss:2.4628, Validation Accuracy:0.1773\n",
    "Epoch #98: Loss:2.4382, Accuracy:0.1774, Validation Loss:2.4612, Validation Accuracy:0.1741\n",
    "Epoch #99: Loss:2.4366, Accuracy:0.1782, Validation Loss:2.4608, Validation Accuracy:0.1773\n",
    "Epoch #100: Loss:2.4366, Accuracy:0.1799, Validation Loss:2.4618, Validation Accuracy:0.1757\n",
    "Epoch #101: Loss:2.4356, Accuracy:0.1791, Validation Loss:2.4693, Validation Accuracy:0.1806\n",
    "Epoch #102: Loss:2.4376, Accuracy:0.1782, Validation Loss:2.4685, Validation Accuracy:0.1806\n",
    "Epoch #103: Loss:2.4369, Accuracy:0.1782, Validation Loss:2.4668, Validation Accuracy:0.1741\n",
    "Epoch #104: Loss:2.4365, Accuracy:0.1786, Validation Loss:2.4680, Validation Accuracy:0.1741\n",
    "Epoch #105: Loss:2.4356, Accuracy:0.1762, Validation Loss:2.4659, Validation Accuracy:0.1691\n",
    "Epoch #106: Loss:2.4349, Accuracy:0.1778, Validation Loss:2.4661, Validation Accuracy:0.1724\n",
    "Epoch #107: Loss:2.4352, Accuracy:0.1770, Validation Loss:2.4680, Validation Accuracy:0.1806\n",
    "Epoch #108: Loss:2.4336, Accuracy:0.1786, Validation Loss:2.4646, Validation Accuracy:0.1658\n",
    "Epoch #109: Loss:2.4339, Accuracy:0.1807, Validation Loss:2.4682, Validation Accuracy:0.1757\n",
    "Epoch #110: Loss:2.4340, Accuracy:0.1799, Validation Loss:2.4662, Validation Accuracy:0.1675\n",
    "Epoch #111: Loss:2.4343, Accuracy:0.1803, Validation Loss:2.4663, Validation Accuracy:0.1757\n",
    "Epoch #112: Loss:2.4327, Accuracy:0.1786, Validation Loss:2.4700, Validation Accuracy:0.1806\n",
    "Epoch #113: Loss:2.4340, Accuracy:0.1795, Validation Loss:2.4658, Validation Accuracy:0.1658\n",
    "Epoch #114: Loss:2.4334, Accuracy:0.1762, Validation Loss:2.4654, Validation Accuracy:0.1806\n",
    "Epoch #115: Loss:2.4330, Accuracy:0.1758, Validation Loss:2.4646, Validation Accuracy:0.1708\n",
    "Epoch #116: Loss:2.4334, Accuracy:0.1770, Validation Loss:2.4666, Validation Accuracy:0.1773\n",
    "Epoch #117: Loss:2.4313, Accuracy:0.1823, Validation Loss:2.4622, Validation Accuracy:0.1708\n",
    "Epoch #118: Loss:2.4312, Accuracy:0.1795, Validation Loss:2.4636, Validation Accuracy:0.1691\n",
    "Epoch #119: Loss:2.4320, Accuracy:0.1807, Validation Loss:2.4634, Validation Accuracy:0.1757\n",
    "Epoch #120: Loss:2.4320, Accuracy:0.1795, Validation Loss:2.4612, Validation Accuracy:0.1741\n",
    "Epoch #121: Loss:2.4308, Accuracy:0.1795, Validation Loss:2.4626, Validation Accuracy:0.1708\n",
    "Epoch #122: Loss:2.4321, Accuracy:0.1786, Validation Loss:2.4602, Validation Accuracy:0.1708\n",
    "Epoch #123: Loss:2.4320, Accuracy:0.1815, Validation Loss:2.4590, Validation Accuracy:0.1790\n",
    "Epoch #124: Loss:2.4324, Accuracy:0.1811, Validation Loss:2.4617, Validation Accuracy:0.1757\n",
    "Epoch #125: Loss:2.4318, Accuracy:0.1807, Validation Loss:2.4608, Validation Accuracy:0.1757\n",
    "Epoch #126: Loss:2.4311, Accuracy:0.1803, Validation Loss:2.4593, Validation Accuracy:0.1773\n",
    "Epoch #127: Loss:2.4314, Accuracy:0.1823, Validation Loss:2.4585, Validation Accuracy:0.1773\n",
    "Epoch #128: Loss:2.4302, Accuracy:0.1786, Validation Loss:2.4588, Validation Accuracy:0.1724\n",
    "Epoch #129: Loss:2.4305, Accuracy:0.1770, Validation Loss:2.4601, Validation Accuracy:0.1806\n",
    "Epoch #130: Loss:2.4308, Accuracy:0.1799, Validation Loss:2.4601, Validation Accuracy:0.1757\n",
    "Epoch #131: Loss:2.4318, Accuracy:0.1782, Validation Loss:2.4608, Validation Accuracy:0.1806\n",
    "Epoch #132: Loss:2.4303, Accuracy:0.1832, Validation Loss:2.4580, Validation Accuracy:0.1724\n",
    "Epoch #133: Loss:2.4317, Accuracy:0.1807, Validation Loss:2.4601, Validation Accuracy:0.1708\n",
    "Epoch #134: Loss:2.4322, Accuracy:0.1828, Validation Loss:2.4576, Validation Accuracy:0.1773\n",
    "Epoch #135: Loss:2.4301, Accuracy:0.1803, Validation Loss:2.4663, Validation Accuracy:0.1741\n",
    "Epoch #136: Loss:2.4297, Accuracy:0.1823, Validation Loss:2.4615, Validation Accuracy:0.1757\n",
    "Epoch #137: Loss:2.4314, Accuracy:0.1791, Validation Loss:2.4663, Validation Accuracy:0.1757\n",
    "Epoch #138: Loss:2.4306, Accuracy:0.1819, Validation Loss:2.4619, Validation Accuracy:0.1708\n",
    "Epoch #139: Loss:2.4297, Accuracy:0.1803, Validation Loss:2.4665, Validation Accuracy:0.1790\n",
    "Epoch #140: Loss:2.4307, Accuracy:0.1832, Validation Loss:2.4641, Validation Accuracy:0.1790\n",
    "Epoch #141: Loss:2.4284, Accuracy:0.1803, Validation Loss:2.4648, Validation Accuracy:0.1741\n",
    "Epoch #142: Loss:2.4282, Accuracy:0.1823, Validation Loss:2.4583, Validation Accuracy:0.1757\n",
    "Epoch #143: Loss:2.4279, Accuracy:0.1807, Validation Loss:2.4645, Validation Accuracy:0.1773\n",
    "Epoch #144: Loss:2.4277, Accuracy:0.1823, Validation Loss:2.4640, Validation Accuracy:0.1741\n",
    "Epoch #145: Loss:2.4290, Accuracy:0.1811, Validation Loss:2.4585, Validation Accuracy:0.1741\n",
    "Epoch #146: Loss:2.4307, Accuracy:0.1786, Validation Loss:2.4779, Validation Accuracy:0.1658\n",
    "Epoch #147: Loss:2.4437, Accuracy:0.1774, Validation Loss:2.4605, Validation Accuracy:0.1708\n",
    "Epoch #148: Loss:2.4385, Accuracy:0.1692, Validation Loss:2.4670, Validation Accuracy:0.1724\n",
    "Epoch #149: Loss:2.4393, Accuracy:0.1741, Validation Loss:2.4607, Validation Accuracy:0.1642\n",
    "Epoch #150: Loss:2.4341, Accuracy:0.1774, Validation Loss:2.4610, Validation Accuracy:0.1708\n",
    "Epoch #151: Loss:2.4326, Accuracy:0.1795, Validation Loss:2.4637, Validation Accuracy:0.1658\n",
    "Epoch #152: Loss:2.4307, Accuracy:0.1770, Validation Loss:2.4589, Validation Accuracy:0.1691\n",
    "Epoch #153: Loss:2.4320, Accuracy:0.1778, Validation Loss:2.4602, Validation Accuracy:0.1724\n",
    "Epoch #154: Loss:2.4337, Accuracy:0.1799, Validation Loss:2.4574, Validation Accuracy:0.1757\n",
    "Epoch #155: Loss:2.4335, Accuracy:0.1782, Validation Loss:2.4546, Validation Accuracy:0.1724\n",
    "Epoch #156: Loss:2.4318, Accuracy:0.1774, Validation Loss:2.4585, Validation Accuracy:0.1773\n",
    "Epoch #157: Loss:2.4313, Accuracy:0.1745, Validation Loss:2.4549, Validation Accuracy:0.1724\n",
    "Epoch #158: Loss:2.4297, Accuracy:0.1815, Validation Loss:2.4595, Validation Accuracy:0.1741\n",
    "Epoch #159: Loss:2.4272, Accuracy:0.1791, Validation Loss:2.4581, Validation Accuracy:0.1642\n",
    "Epoch #160: Loss:2.4254, Accuracy:0.1745, Validation Loss:2.4587, Validation Accuracy:0.1757\n",
    "Epoch #161: Loss:2.4254, Accuracy:0.1770, Validation Loss:2.4616, Validation Accuracy:0.1658\n",
    "Epoch #162: Loss:2.4268, Accuracy:0.1770, Validation Loss:2.4599, Validation Accuracy:0.1675\n",
    "Epoch #163: Loss:2.4262, Accuracy:0.1786, Validation Loss:2.4548, Validation Accuracy:0.1773\n",
    "Epoch #164: Loss:2.4261, Accuracy:0.1811, Validation Loss:2.4572, Validation Accuracy:0.1773\n",
    "Epoch #165: Loss:2.4245, Accuracy:0.1762, Validation Loss:2.4584, Validation Accuracy:0.1708\n",
    "Epoch #166: Loss:2.4619, Accuracy:0.1758, Validation Loss:2.4825, Validation Accuracy:0.1823\n",
    "Epoch #167: Loss:2.4592, Accuracy:0.1692, Validation Loss:2.4533, Validation Accuracy:0.1691\n",
    "Epoch #168: Loss:2.4446, Accuracy:0.1680, Validation Loss:2.4771, Validation Accuracy:0.1724\n",
    "Epoch #169: Loss:2.4361, Accuracy:0.1692, Validation Loss:2.4604, Validation Accuracy:0.1724\n",
    "Epoch #170: Loss:2.4367, Accuracy:0.1745, Validation Loss:2.4604, Validation Accuracy:0.1724\n",
    "Epoch #171: Loss:2.4319, Accuracy:0.1766, Validation Loss:2.4629, Validation Accuracy:0.1741\n",
    "Epoch #172: Loss:2.4300, Accuracy:0.1737, Validation Loss:2.4569, Validation Accuracy:0.1757\n",
    "Epoch #173: Loss:2.4327, Accuracy:0.1737, Validation Loss:2.4590, Validation Accuracy:0.1757\n",
    "Epoch #174: Loss:2.4319, Accuracy:0.1766, Validation Loss:2.4610, Validation Accuracy:0.1773\n",
    "Epoch #175: Loss:2.4301, Accuracy:0.1758, Validation Loss:2.4575, Validation Accuracy:0.1757\n",
    "Epoch #176: Loss:2.4301, Accuracy:0.1733, Validation Loss:2.4580, Validation Accuracy:0.1757\n",
    "Epoch #177: Loss:2.4305, Accuracy:0.1762, Validation Loss:2.4589, Validation Accuracy:0.1741\n",
    "Epoch #178: Loss:2.4289, Accuracy:0.1741, Validation Loss:2.4561, Validation Accuracy:0.1724\n",
    "Epoch #179: Loss:2.4282, Accuracy:0.1766, Validation Loss:2.4611, Validation Accuracy:0.1773\n",
    "Epoch #180: Loss:2.4284, Accuracy:0.1762, Validation Loss:2.4567, Validation Accuracy:0.1773\n",
    "Epoch #181: Loss:2.4288, Accuracy:0.1778, Validation Loss:2.4558, Validation Accuracy:0.1773\n",
    "Epoch #182: Loss:2.4280, Accuracy:0.1754, Validation Loss:2.4578, Validation Accuracy:0.1790\n",
    "Epoch #183: Loss:2.4281, Accuracy:0.1762, Validation Loss:2.4566, Validation Accuracy:0.1708\n",
    "Epoch #184: Loss:2.4308, Accuracy:0.1795, Validation Loss:2.4572, Validation Accuracy:0.1691\n",
    "Epoch #185: Loss:2.4298, Accuracy:0.1737, Validation Loss:2.4594, Validation Accuracy:0.1757\n",
    "Epoch #186: Loss:2.4284, Accuracy:0.1778, Validation Loss:2.4635, Validation Accuracy:0.1642\n",
    "Epoch #187: Loss:2.4285, Accuracy:0.1778, Validation Loss:2.4641, Validation Accuracy:0.1658\n",
    "Epoch #188: Loss:2.4295, Accuracy:0.1729, Validation Loss:2.4614, Validation Accuracy:0.1741\n",
    "Epoch #189: Loss:2.4280, Accuracy:0.1766, Validation Loss:2.4634, Validation Accuracy:0.1658\n",
    "Epoch #190: Loss:2.4271, Accuracy:0.1762, Validation Loss:2.4627, Validation Accuracy:0.1675\n",
    "Epoch #191: Loss:2.4258, Accuracy:0.1803, Validation Loss:2.4662, Validation Accuracy:0.1658\n",
    "Epoch #192: Loss:2.4263, Accuracy:0.1758, Validation Loss:2.4574, Validation Accuracy:0.1642\n",
    "Epoch #193: Loss:2.4348, Accuracy:0.1721, Validation Loss:2.4615, Validation Accuracy:0.1708\n",
    "Epoch #194: Loss:2.4347, Accuracy:0.1749, Validation Loss:2.4617, Validation Accuracy:0.1642\n",
    "Epoch #195: Loss:2.4290, Accuracy:0.1807, Validation Loss:2.4726, Validation Accuracy:0.1626\n",
    "Epoch #196: Loss:2.4318, Accuracy:0.1708, Validation Loss:2.4587, Validation Accuracy:0.1724\n",
    "Epoch #197: Loss:2.4284, Accuracy:0.1741, Validation Loss:2.4607, Validation Accuracy:0.1741\n",
    "Epoch #198: Loss:2.4271, Accuracy:0.1782, Validation Loss:2.4602, Validation Accuracy:0.1724\n",
    "Epoch #199: Loss:2.4269, Accuracy:0.1758, Validation Loss:2.4589, Validation Accuracy:0.1724\n",
    "Epoch #200: Loss:2.4255, Accuracy:0.1762, Validation Loss:2.4582, Validation Accuracy:0.1658\n",
    "Epoch #201: Loss:2.4231, Accuracy:0.1766, Validation Loss:2.4614, Validation Accuracy:0.1724\n",
    "Epoch #202: Loss:2.4237, Accuracy:0.1782, Validation Loss:2.4631, Validation Accuracy:0.1708\n",
    "Epoch #203: Loss:2.4244, Accuracy:0.1795, Validation Loss:2.4587, Validation Accuracy:0.1773\n",
    "Epoch #204: Loss:2.4230, Accuracy:0.1733, Validation Loss:2.4581, Validation Accuracy:0.1675\n",
    "Epoch #205: Loss:2.4233, Accuracy:0.1749, Validation Loss:2.4594, Validation Accuracy:0.1741\n",
    "Epoch #206: Loss:2.4225, Accuracy:0.1782, Validation Loss:2.4590, Validation Accuracy:0.1806\n",
    "Epoch #207: Loss:2.4206, Accuracy:0.1766, Validation Loss:2.4600, Validation Accuracy:0.1724\n",
    "Epoch #208: Loss:2.4204, Accuracy:0.1749, Validation Loss:2.4606, Validation Accuracy:0.1708\n",
    "Epoch #209: Loss:2.4202, Accuracy:0.1774, Validation Loss:2.4589, Validation Accuracy:0.1741\n",
    "Epoch #210: Loss:2.4213, Accuracy:0.1782, Validation Loss:2.4580, Validation Accuracy:0.1773\n",
    "Epoch #211: Loss:2.4214, Accuracy:0.1791, Validation Loss:2.4595, Validation Accuracy:0.1773\n",
    "Epoch #212: Loss:2.4214, Accuracy:0.1782, Validation Loss:2.4575, Validation Accuracy:0.1773\n",
    "Epoch #213: Loss:2.4206, Accuracy:0.1791, Validation Loss:2.4576, Validation Accuracy:0.1790\n",
    "Epoch #214: Loss:2.4208, Accuracy:0.1774, Validation Loss:2.4565, Validation Accuracy:0.1724\n",
    "Epoch #215: Loss:2.4200, Accuracy:0.1795, Validation Loss:2.4671, Validation Accuracy:0.1675\n",
    "Epoch #216: Loss:2.4216, Accuracy:0.1852, Validation Loss:2.4612, Validation Accuracy:0.1724\n",
    "Epoch #217: Loss:2.4197, Accuracy:0.1782, Validation Loss:2.4606, Validation Accuracy:0.1757\n",
    "Epoch #218: Loss:2.4215, Accuracy:0.1778, Validation Loss:2.4649, Validation Accuracy:0.1790\n",
    "Epoch #219: Loss:2.4208, Accuracy:0.1795, Validation Loss:2.4624, Validation Accuracy:0.1757\n",
    "Epoch #220: Loss:2.4234, Accuracy:0.1815, Validation Loss:2.4645, Validation Accuracy:0.1757\n",
    "Epoch #221: Loss:2.4208, Accuracy:0.1828, Validation Loss:2.4654, Validation Accuracy:0.1642\n",
    "Epoch #222: Loss:2.4192, Accuracy:0.1766, Validation Loss:2.4644, Validation Accuracy:0.1708\n",
    "Epoch #223: Loss:2.4205, Accuracy:0.1807, Validation Loss:2.4667, Validation Accuracy:0.1675\n",
    "Epoch #224: Loss:2.4195, Accuracy:0.1819, Validation Loss:2.4613, Validation Accuracy:0.1658\n",
    "Epoch #225: Loss:2.4200, Accuracy:0.1815, Validation Loss:2.4632, Validation Accuracy:0.1642\n",
    "Epoch #226: Loss:2.4194, Accuracy:0.1770, Validation Loss:2.4646, Validation Accuracy:0.1691\n",
    "Epoch #227: Loss:2.4194, Accuracy:0.1766, Validation Loss:2.4609, Validation Accuracy:0.1675\n",
    "Epoch #228: Loss:2.4180, Accuracy:0.1811, Validation Loss:2.4553, Validation Accuracy:0.1708\n",
    "Epoch #229: Loss:2.4185, Accuracy:0.1786, Validation Loss:2.4643, Validation Accuracy:0.1724\n",
    "Epoch #230: Loss:2.4270, Accuracy:0.1795, Validation Loss:2.4620, Validation Accuracy:0.1724\n",
    "Epoch #231: Loss:2.4616, Accuracy:0.1749, Validation Loss:2.5519, Validation Accuracy:0.1461\n",
    "Epoch #232: Loss:2.5078, Accuracy:0.1634, Validation Loss:2.5244, Validation Accuracy:0.1741\n",
    "Epoch #233: Loss:2.5555, Accuracy:0.1413, Validation Loss:2.5293, Validation Accuracy:0.1379\n",
    "Epoch #234: Loss:2.6391, Accuracy:0.1388, Validation Loss:2.8113, Validation Accuracy:0.1084\n",
    "Epoch #235: Loss:2.6156, Accuracy:0.1240, Validation Loss:2.6205, Validation Accuracy:0.1067\n",
    "Epoch #236: Loss:2.5793, Accuracy:0.1072, Validation Loss:2.5540, Validation Accuracy:0.1100\n",
    "Epoch #237: Loss:2.5257, Accuracy:0.1388, Validation Loss:2.5343, Validation Accuracy:0.1560\n",
    "Epoch #238: Loss:2.4798, Accuracy:0.1745, Validation Loss:2.5065, Validation Accuracy:0.1691\n",
    "Epoch #239: Loss:2.4662, Accuracy:0.1741, Validation Loss:2.5171, Validation Accuracy:0.1576\n",
    "Epoch #240: Loss:2.4670, Accuracy:0.1692, Validation Loss:2.5132, Validation Accuracy:0.1560\n",
    "Epoch #241: Loss:2.4593, Accuracy:0.1692, Validation Loss:2.5038, Validation Accuracy:0.1511\n",
    "Epoch #242: Loss:2.4528, Accuracy:0.1713, Validation Loss:2.4915, Validation Accuracy:0.1609\n",
    "Epoch #243: Loss:2.4503, Accuracy:0.1749, Validation Loss:2.4859, Validation Accuracy:0.1658\n",
    "Epoch #244: Loss:2.4477, Accuracy:0.1770, Validation Loss:2.4799, Validation Accuracy:0.1626\n",
    "Epoch #245: Loss:2.4442, Accuracy:0.1762, Validation Loss:2.4792, Validation Accuracy:0.1626\n",
    "Epoch #246: Loss:2.4425, Accuracy:0.1745, Validation Loss:2.4765, Validation Accuracy:0.1675\n",
    "Epoch #247: Loss:2.4426, Accuracy:0.1725, Validation Loss:2.4789, Validation Accuracy:0.1658\n",
    "Epoch #248: Loss:2.4421, Accuracy:0.1713, Validation Loss:2.4770, Validation Accuracy:0.1609\n",
    "Epoch #249: Loss:2.4411, Accuracy:0.1741, Validation Loss:2.4963, Validation Accuracy:0.1593\n",
    "Epoch #250: Loss:2.4584, Accuracy:0.1770, Validation Loss:2.4742, Validation Accuracy:0.1773\n",
    "Epoch #251: Loss:2.4513, Accuracy:0.1807, Validation Loss:2.4686, Validation Accuracy:0.1708\n",
    "Epoch #252: Loss:2.4417, Accuracy:0.1873, Validation Loss:2.4864, Validation Accuracy:0.1609\n",
    "Epoch #253: Loss:2.4454, Accuracy:0.1799, Validation Loss:2.4751, Validation Accuracy:0.1691\n",
    "Epoch #254: Loss:2.4394, Accuracy:0.1823, Validation Loss:2.4735, Validation Accuracy:0.1856\n",
    "Epoch #255: Loss:2.4408, Accuracy:0.1856, Validation Loss:2.4745, Validation Accuracy:0.1773\n",
    "Epoch #256: Loss:2.4369, Accuracy:0.1832, Validation Loss:2.4714, Validation Accuracy:0.1576\n",
    "Epoch #257: Loss:2.4352, Accuracy:0.1770, Validation Loss:2.4729, Validation Accuracy:0.1626\n",
    "Epoch #258: Loss:2.4334, Accuracy:0.1791, Validation Loss:2.4691, Validation Accuracy:0.1773\n",
    "Epoch #259: Loss:2.4314, Accuracy:0.1848, Validation Loss:2.4674, Validation Accuracy:0.1675\n",
    "Epoch #260: Loss:2.4317, Accuracy:0.1811, Validation Loss:2.4694, Validation Accuracy:0.1642\n",
    "Epoch #261: Loss:2.4304, Accuracy:0.1819, Validation Loss:2.4692, Validation Accuracy:0.1741\n",
    "Epoch #262: Loss:2.4303, Accuracy:0.1836, Validation Loss:2.4705, Validation Accuracy:0.1724\n",
    "Epoch #263: Loss:2.4280, Accuracy:0.1791, Validation Loss:2.4700, Validation Accuracy:0.1823\n",
    "Epoch #264: Loss:2.4282, Accuracy:0.1791, Validation Loss:2.4694, Validation Accuracy:0.1773\n",
    "Epoch #265: Loss:2.4273, Accuracy:0.1782, Validation Loss:2.4675, Validation Accuracy:0.1724\n",
    "Epoch #266: Loss:2.4265, Accuracy:0.1828, Validation Loss:2.4719, Validation Accuracy:0.1773\n",
    "Epoch #267: Loss:2.4264, Accuracy:0.1848, Validation Loss:2.4667, Validation Accuracy:0.1724\n",
    "Epoch #268: Loss:2.4261, Accuracy:0.1807, Validation Loss:2.4648, Validation Accuracy:0.1741\n",
    "Epoch #269: Loss:2.4269, Accuracy:0.1770, Validation Loss:2.4626, Validation Accuracy:0.1741\n",
    "Epoch #270: Loss:2.4284, Accuracy:0.1786, Validation Loss:2.4637, Validation Accuracy:0.1839\n",
    "Epoch #271: Loss:2.4275, Accuracy:0.1803, Validation Loss:2.4652, Validation Accuracy:0.1823\n",
    "Epoch #272: Loss:2.4277, Accuracy:0.1807, Validation Loss:2.4647, Validation Accuracy:0.1839\n",
    "Epoch #273: Loss:2.4278, Accuracy:0.1774, Validation Loss:2.4673, Validation Accuracy:0.1708\n",
    "Epoch #274: Loss:2.4276, Accuracy:0.1791, Validation Loss:2.4652, Validation Accuracy:0.1839\n",
    "Epoch #275: Loss:2.4279, Accuracy:0.1828, Validation Loss:2.4635, Validation Accuracy:0.1823\n",
    "Epoch #276: Loss:2.4260, Accuracy:0.1828, Validation Loss:2.4661, Validation Accuracy:0.1757\n",
    "Epoch #277: Loss:2.4273, Accuracy:0.1803, Validation Loss:2.4675, Validation Accuracy:0.1773\n",
    "Epoch #278: Loss:2.4268, Accuracy:0.1828, Validation Loss:2.4673, Validation Accuracy:0.1839\n",
    "Epoch #279: Loss:2.4261, Accuracy:0.1828, Validation Loss:2.4679, Validation Accuracy:0.1757\n",
    "Epoch #280: Loss:2.4258, Accuracy:0.1803, Validation Loss:2.4642, Validation Accuracy:0.1773\n",
    "Epoch #281: Loss:2.4253, Accuracy:0.1791, Validation Loss:2.4626, Validation Accuracy:0.1773\n",
    "Epoch #282: Loss:2.4251, Accuracy:0.1795, Validation Loss:2.4593, Validation Accuracy:0.1790\n",
    "Epoch #283: Loss:2.4257, Accuracy:0.1778, Validation Loss:2.4610, Validation Accuracy:0.1757\n",
    "Epoch #284: Loss:2.4253, Accuracy:0.1799, Validation Loss:2.4621, Validation Accuracy:0.1773\n",
    "Epoch #285: Loss:2.4248, Accuracy:0.1811, Validation Loss:2.4610, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.4257, Accuracy:0.1782, Validation Loss:2.4644, Validation Accuracy:0.1724\n",
    "Epoch #287: Loss:2.4258, Accuracy:0.1807, Validation Loss:2.4735, Validation Accuracy:0.1724\n",
    "Epoch #288: Loss:2.4248, Accuracy:0.1795, Validation Loss:2.4737, Validation Accuracy:0.1757\n",
    "Epoch #289: Loss:2.4248, Accuracy:0.1844, Validation Loss:2.4727, Validation Accuracy:0.1741\n",
    "Epoch #290: Loss:2.4261, Accuracy:0.1807, Validation Loss:2.4732, Validation Accuracy:0.1773\n",
    "Epoch #291: Loss:2.4270, Accuracy:0.1844, Validation Loss:2.4662, Validation Accuracy:0.1757\n",
    "Epoch #292: Loss:2.4267, Accuracy:0.1860, Validation Loss:2.4614, Validation Accuracy:0.1806\n",
    "Epoch #293: Loss:2.4270, Accuracy:0.1844, Validation Loss:2.4624, Validation Accuracy:0.1741\n",
    "Epoch #294: Loss:2.4257, Accuracy:0.1856, Validation Loss:2.4697, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4262, Accuracy:0.1815, Validation Loss:2.4753, Validation Accuracy:0.1724\n",
    "Epoch #296: Loss:2.4256, Accuracy:0.1836, Validation Loss:2.4742, Validation Accuracy:0.1757\n",
    "Epoch #297: Loss:2.4236, Accuracy:0.1852, Validation Loss:2.4716, Validation Accuracy:0.1724\n",
    "Epoch #298: Loss:2.4238, Accuracy:0.1848, Validation Loss:2.4717, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.4219, Accuracy:0.1832, Validation Loss:2.4739, Validation Accuracy:0.1724\n",
    "Epoch #300: Loss:2.4234, Accuracy:0.1823, Validation Loss:2.4697, Validation Accuracy:0.1675\n",
    "\n",
    "Test:\n",
    "Test Loss:2.46970773, Accuracy:0.1675\n",
    "Labels: ['eb', 'ck', 'mb', 'yd', 'aa', 'eo', 'ek', 'sk', 'ib', 'ds', 'sg', 'my', 'by', 'ce', 'eg']\n",
    "Confusion Matrix:\n",
    "      eb  ck  mb  yd  aa  eo  ek  sk  ib  ds  sg  my  by  ce  eg\n",
    "t:eb   8   0   0   7   0   0   0   0   0   0  11   0   0   0  24\n",
    "t:ck   3   0   0   0   0   0   0   0   0   0   7   0   0   0  13\n",
    "t:mb   6   0   0  16   0   0   0   0   0   0  18   0   1   0  11\n",
    "t:yd   6   0   0  39   0   0   0   0   0   0  16   0   0   0   1\n",
    "t:aa   2   0   0   1   0   0   0   0   0   0   6   0   1   0  24\n",
    "t:eo   6   0   1   6   0   0   0   0   0   0  19   0   0   0   2\n",
    "t:ek   4   0   0   8   0   0   0   0   0   0  17   0   1   0  18\n",
    "t:sk   4   0   0   3   0   0   0   0   0   0   5   0   1   0  20\n",
    "t:ib   3   0   0  33   0   0   0   0   0   0  10   0   0   0   8\n",
    "t:ds   5   0   0   2   0   0   0   0   0   0   5   0   0   0  19\n",
    "t:sg   7   0   0  14   0   0   0   0   0   0  22   0   0   0   8\n",
    "t:my   1   0   0   9   0   0   0   0   0   0   4   0   0   0   6\n",
    "t:by   4   0   0   4   0   0   0   0   0   0  15   0   0   0  17\n",
    "t:ce   1   0   0   2   0   0   0   0   0   0   7   0   0   0  17\n",
    "t:eg   3   0   0   3   0   0   0   0   0   0   9   0   2   0  33\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eb       0.13      0.16      0.14        50\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          yd       0.27      0.63      0.37        62\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          ds       0.00      0.00      0.00        31\n",
    "          sg       0.13      0.43      0.20        51\n",
    "          my       0.00      0.00      0.00        20\n",
    "          by       0.00      0.00      0.00        40\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          eg       0.15      0.66      0.24        50\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.04      0.13      0.06       609\n",
    "weighted avg       0.06      0.17      0.09       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 21:41:05 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6917375825308816, 2.683444375866544, 2.6756962625851184, 2.6721929525115415, 2.668630228449754, 2.665581674607125, 2.6621092070499666, 2.658387063955047, 2.653828604467984, 2.649445671949089, 2.6433988376987, 2.636102220695007, 2.626459400446348, 2.615064444408824, 2.6016147755245465, 2.5859051220522726, 2.5714373091367273, 2.558304172822799, 2.545179551849616, 2.534087281704732, 2.5277430404387475, 2.5231490757665025, 2.5172098254531083, 2.527003752186968, 2.520509768393631, 2.502210582222649, 2.4999361453189444, 2.50306995474842, 2.4993683884688003, 2.4967512391470925, 2.4921296621582583, 2.490842370564127, 2.48878480178382, 2.4874155407860163, 2.483731520391254, 2.4805537722576623, 2.4756956844298514, 2.4765699219038138, 2.4757788795947246, 2.4780809249000986, 2.4779956043255935, 2.4825597972118207, 2.488402310654839, 2.4825389326499603, 2.5064696643152846, 2.4886028954548203, 2.4751158163856792, 2.4815996613212796, 2.4780371353544037, 2.474697636070314, 2.4740729202777882, 2.474172177181651, 2.4711410541252548, 2.4710573128291538, 2.4713241321895705, 2.467696653798296, 2.4684578248824196, 2.466150891017444, 2.4653625425642542, 2.4666855425278738, 2.4664500147251074, 2.4654209053966603, 2.4678825611747155, 2.466244940687283, 2.4697382320911427, 2.4682986838085506, 2.469647851092083, 2.466245846599585, 2.4654297311904982, 2.4660287643301078, 2.46407606801376, 2.46825002723531, 2.4660223285944394, 2.4666399149276157, 2.4606862976437522, 2.460658541258137, 2.464013416387373, 2.4615366059571064, 2.4627889678591774, 2.460459656707563, 2.4612309815261164, 2.4613151616846594, 2.4595299432626105, 2.461606825905285, 2.4613544890054535, 2.4620412312117703, 2.462401574076886, 2.4611999945491796, 2.463053563936982, 2.4704696415680383, 2.468698152376122, 2.4720415423069095, 2.4704307816885964, 2.462034924276944, 2.462389505946969, 2.4619536078817936, 2.4628299093011563, 2.461237638846211, 2.4607671971000085, 2.4617792385552315, 2.469344134401218, 2.4685417028092007, 2.466821633340494, 2.468015462894158, 2.4658874005128206, 2.4661115656541095, 2.46798232427763, 2.4645850184711526, 2.468200307370015, 2.46619334909912, 2.4663412038524357, 2.4700412985139293, 2.465759643388695, 2.465360532253247, 2.464575147393889, 2.4666149318707595, 2.462168013129524, 2.4636061438198746, 2.463421188943296, 2.4612247344895537, 2.4625738810240145, 2.4601968513137993, 2.4590247851874443, 2.4617413656269194, 2.460846585788946, 2.459320458676819, 2.458529830957673, 2.4587591172047634, 2.460133376379906, 2.460071504213931, 2.460784834985467, 2.458029101439102, 2.460056640048724, 2.457641038988611, 2.4662702522058595, 2.4614972539723214, 2.4663217369167283, 2.461940738367917, 2.466498338921708, 2.4641000570726317, 2.4648281314298632, 2.4582732385406745, 2.4645442297110223, 2.463976362851649, 2.4585434266890602, 2.4778785509820445, 2.460501261336854, 2.4670138245537165, 2.460724494727374, 2.4609974362384315, 2.463739162595401, 2.4588641599677077, 2.4601807077529982, 2.4573788815139745, 2.4546050254151544, 2.458498857291461, 2.45485542991087, 2.459532832081486, 2.4581422649189366, 2.4587101728849614, 2.4616398267166564, 2.45985146618046, 2.454783711331623, 2.457213145367226, 2.4583677951925496, 2.4825318118034327, 2.4532852615237433, 2.477102256565063, 2.460409496022367, 2.4603819103272286, 2.462885422463879, 2.4569006984065513, 2.4590208428638127, 2.461038686567535, 2.457540516782864, 2.4579758338740305, 2.4589269991187237, 2.4561320596140592, 2.4610513129649294, 2.4567236011642932, 2.455750580491691, 2.457779192572157, 2.4566254392633295, 2.4572313642266934, 2.4594478505389836, 2.463525370424017, 2.4641202560982287, 2.4613945973526277, 2.463448031978263, 2.462659122517152, 2.466238299026865, 2.4574058658775244, 2.4615343654488497, 2.4617248207869005, 2.4726013397348336, 2.4586759752827914, 2.4606831770616604, 2.4601605353488516, 2.458942304886816, 2.458217066888543, 2.4614199170925346, 2.4631491611743797, 2.4586943409517286, 2.458064271115709, 2.459422167886067, 2.458992776025105, 2.4600317067113417, 2.4606174111170525, 2.4589142529248016, 2.4579534252680384, 2.459468915153215, 2.4574901662240864, 2.457568585187539, 2.4564798265842382, 2.4670675771772763, 2.461155120962359, 2.4606158874305013, 2.4648779945812005, 2.462389004837312, 2.4645421551953395, 2.465422638922881, 2.464392925522402, 2.4666891798792996, 2.4612897190162895, 2.4632245206284797, 2.4646100884392146, 2.4609220004434067, 2.45531777090627, 2.464290386741776, 2.4619942999434197, 2.5519280746848323, 2.524412764704286, 2.5292956089151315, 2.811293241034196, 2.620458484674714, 2.554042354201644, 2.53432274256238, 2.5065384177347316, 2.5170758189434683, 2.513180025301152, 2.5037663601497906, 2.4914755065648624, 2.4858799305651185, 2.4799303233329884, 2.4792311939308402, 2.4765490060565116, 2.4789364733327983, 2.476993938189226, 2.4963223730597788, 2.47420763891123, 2.4685588830406053, 2.486358447615149, 2.475129114583208, 2.4734571802205054, 2.4744727192645395, 2.4713745747489493, 2.472852163126903, 2.469059108709075, 2.4674256501722414, 2.469448462691409, 2.4691525108512793, 2.4705248724650866, 2.4700282430413907, 2.4694062457687553, 2.4674849012998132, 2.4718537326712524, 2.4666811069244234, 2.4648253749352564, 2.46257493844369, 2.4636945489592152, 2.465160263387245, 2.464691070500266, 2.4672882885768495, 2.46515524367785, 2.46348249618643, 2.4660852683588788, 2.4674905951582935, 2.4673111313473806, 2.467938196482917, 2.464240220575693, 2.4625564686379016, 2.459303749018702, 2.461013055591552, 2.4621368879559395, 2.4610435997911275, 2.4643828473459126, 2.4734676964764524, 2.4736828389034677, 2.4726768933689263, 2.473210261177351, 2.4662387324084203, 2.4614407374158085, 2.4624216591783346, 2.469697619893868, 2.475271731174638, 2.474194420578053, 2.471628413803276, 2.4717115447634743, 2.473870993834998, 2.4697076988533406], 'val_acc': [0.08538587798772775, 0.08538587798772775, 0.08538587798772775, 0.1018062395354112, 0.08538587838533672, 0.08866995063492622, 0.08702791451013146, 0.08538587838533672, 0.0804597699130795, 0.08210180603787426, 0.09359605910718344, 0.10180623933354818, 0.10837438383272716, 0.1149425285276521, 0.10837438413246316, 0.11822660087511458, 0.12479474537429355, 0.1461412141035343, 0.15106732257579153, 0.14942528645099679, 0.16091953942243298, 0.15270935879845923, 0.15270935860271329, 0.17241379249174216, 0.15599343104804875, 0.17077175626907443, 0.16912972034002563, 0.16256157554722772, 0.17733990096399938, 0.18390804575679728, 0.18226600953412955, 0.1806239734093348, 0.17241379258961514, 0.17241379229599618, 0.1691297200464067, 0.1658456477968172, 0.1658456477968172, 0.17077175646482037, 0.1789819371866671, 0.1691297200464067, 0.16748768401948494, 0.16584564809043614, 0.17077175626907443, 0.1756978644477127, 0.1592775030040193, 0.16912972014427968, 0.15927750319976525, 0.1740558285186639, 0.17077175626907443, 0.17077175626907443, 0.16748768392161195, 0.16748768401948494, 0.17077175626907443, 0.17077175626907443, 0.16584564769894422, 0.17241379239386917, 0.17569786474133164, 0.1740558285186639, 0.17569786464345866, 0.17569786474133164, 0.1740558286165369, 0.17569786464345866, 0.16912972014427968, 0.17569786474133164, 0.18062397311571587, 0.17241379219812322, 0.1691297200464067, 0.1691297199485337, 0.17077175617120144, 0.17898193699092113, 0.1773399008661264, 0.16748768401948494, 0.17569786474133164, 0.17733990096399938, 0.1789819371866671, 0.17898193708879412, 0.18062397321358886, 0.17733990096399938, 0.18390804546317835, 0.18390804556105134, 0.17569786474133164, 0.1822660093383836, 0.18062397331146185, 0.1822660093383836, 0.1822660094362566, 0.17733990096399938, 0.17898193708879412, 0.17733990096399938, 0.17898193708879412, 0.1773399007682534, 0.17569786483920463, 0.17569786464345866, 0.17898193708879412, 0.18226600924051062, 0.18226600924051062, 0.18226600924051062, 0.1773399008661264, 0.1740558286165369, 0.1773399008661264, 0.17569786464345866, 0.18062397311571587, 0.18062397331146185, 0.17405582842079093, 0.1740558285186639, 0.16912971985066075, 0.17241379219812322, 0.18062397291996993, 0.16584564760107126, 0.17569786454558567, 0.16748768362799302, 0.17569786434983972, 0.1806239730178429, 0.16584564760107126, 0.18062397311571587, 0.17077175607332848, 0.17733990067038044, 0.17077175607332848, 0.1691297199485337, 0.17569786474133164, 0.1740558286165369, 0.17077175617120144, 0.17077175617120144, 0.17898193699092113, 0.17569786474133164, 0.17569786454558567, 0.1773399007682534, 0.1773399008661264, 0.17241379219812322, 0.1806239730178429, 0.1756978644477127, 0.1806239730178429, 0.17241379219812322, 0.17077175626907443, 0.1773399008661264, 0.1740558285186639, 0.17569786474133164, 0.17569786474133164, 0.17077175626907443, 0.17898193699092113, 0.17898193708879412, 0.17405582842079093, 0.17569786474133164, 0.1773399008661264, 0.1740558286165369, 0.1740558285186639, 0.16584564799256318, 0.1707717563669474, 0.17241379258961514, 0.1642036118677684, 0.17077175626907443, 0.1658456478946902, 0.16912972014427968, 0.17241379239386917, 0.17569786464345866, 0.17241379229599618, 0.1773399007682534, 0.17241379229599618, 0.17405582842079093, 0.16420361147627652, 0.17569786454558567, 0.16584564769894422, 0.16748768392161195, 0.1773399007682534, 0.1773399008661264, 0.17077175626907443, 0.18226600914263763, 0.1691297200464067, 0.17241379229599618, 0.17241379258961514, 0.17241379229599618, 0.17405582842079093, 0.17569786483920463, 0.17569786474133164, 0.1773399008661264, 0.17569786474133164, 0.17569786474133164, 0.1740558286165369, 0.17241379239386917, 0.17733990106187233, 0.17733990096399938, 0.17733990096399938, 0.17898193708879412, 0.17077175617120144, 0.1691297199485337, 0.17569786474133164, 0.16420361157414948, 0.16584564769894422, 0.1740558285186639, 0.1658456478946902, 0.167487683725866, 0.1658456477968172, 0.16420361147627652, 0.17077175617120144, 0.16420361147627652, 0.16256157554722772, 0.17241379249174216, 0.1740558285186639, 0.17241379239386917, 0.17241379229599618, 0.1658456478946902, 0.17241379229599618, 0.17077175617120144, 0.1773399007682534, 0.16748768382373896, 0.1740558287144099, 0.18062397321358886, 0.17241379249174216, 0.1707717563669474, 0.1740558286165369, 0.17733990096399938, 0.17733990096399938, 0.17733990106187233, 0.17898193708879412, 0.17241379258961514, 0.1674876842152309, 0.17241379249174216, 0.17569786474133164, 0.17898193708879412, 0.17569786474133164, 0.17569786474133164, 0.1642036119656414, 0.1707717563669474, 0.1674876842152309, 0.1658456478946902, 0.16420361167202246, 0.16912972014427968, 0.16748768392161195, 0.17077175626907443, 0.17241379249174216, 0.17241379258961514, 0.1461412141035343, 0.17405582881228285, 0.1379310333816876, 0.10837438313538218, 0.10673234790367837, 0.1100164199575219, 0.15599343095017576, 0.16912972034002563, 0.15763546736858944, 0.1559934311459217, 0.15106732277153748, 0.16091953961817893, 0.16584564799256318, 0.16256157574297367, 0.16256157584084666, 0.1674876842152309, 0.16584564799256318, 0.16091953961817893, 0.15927750359125717, 0.1773399007682534, 0.17077175646482037, 0.1609195397160519, 0.1691297200464067, 0.18555008188159203, 0.17733990115974532, 0.1576354671728435, 0.16256157554722772, 0.1773399008661264, 0.16748768382373896, 0.16420361167202246, 0.17405582842079093, 0.17241379229599618, 0.18226600924051062, 0.1773399008661264, 0.17241379239386917, 0.1773399007682534, 0.17241379239386917, 0.1740558285186639, 0.17405582842079093, 0.18390804556105134, 0.1822660094362566, 0.18390804556105134, 0.17077175607332848, 0.18390804556105134, 0.1822660094362566, 0.17569786464345866, 0.1773399008661264, 0.18390804556105134, 0.17569786483920463, 0.17733990096399938, 0.17733990096399938, 0.17898193689304814, 0.1756978649370776, 0.17733990096399938, 0.16912972014427968, 0.17241379258961514, 0.17241379249174216, 0.17569786464345866, 0.1740558287144099, 0.17733990096399938, 0.17569786483920463, 0.1806239734093348, 0.17405582891015584, 0.16912972024215267, 0.17241379229599618, 0.17569786454558567, 0.17241379229599618, 0.17405582842079093, 0.17241379219812322, 0.16748768382373896], 'loss': [2.6970218782796995, 2.6889981320751275, 2.680536641330445, 2.675064852399258, 2.6708650310915845, 2.666834964301797, 2.6632911859351753, 2.6599136681527327, 2.656060403671108, 2.6514465610104665, 2.645603549358047, 2.6388794786386667, 2.629354013799397, 2.6175050100996264, 2.6032978528823696, 2.58557775240904, 2.5659738629750404, 2.544882349507765, 2.527402097439619, 2.514756737303685, 2.500859147567279, 2.494523044582265, 2.4871013873411645, 2.5007594115435463, 2.509490996072914, 2.5005586324531195, 2.484747430382323, 2.474877856840098, 2.4719509004322657, 2.4695362879265503, 2.46679089779237, 2.4644250988225918, 2.4610183536639205, 2.4578002685883695, 2.455062164956784, 2.4561086693828353, 2.459373088636927, 2.461462455314777, 2.459807878940747, 2.459761682036476, 2.457625380482762, 2.458823321438423, 2.4993152680093504, 2.501455762988488, 2.4832324835798825, 2.480235024693076, 2.4746161543123533, 2.461209731229277, 2.45959524354406, 2.454856059879248, 2.454557094926462, 2.4518653846129745, 2.4508902243275417, 2.4499467714611267, 2.449058923486322, 2.44809105234714, 2.447781990881573, 2.4477348905324448, 2.4478432943199206, 2.446588381017258, 2.4459582688137735, 2.4452052559940722, 2.4433262474483044, 2.443771858039088, 2.4427489007522927, 2.442140193054074, 2.440855889643487, 2.4386370733288523, 2.439005873873983, 2.4420455418573024, 2.4417915546918554, 2.4398278215827394, 2.441720422826998, 2.440475536518763, 2.4400789342132194, 2.442767840391312, 2.446217217337669, 2.4440931919419056, 2.442564370940598, 2.4415202088914123, 2.441899784291794, 2.442285046097679, 2.4410104285520204, 2.4416306388941145, 2.4402564956422212, 2.439461740967674, 2.439328633980095, 2.439776644422778, 2.443094229845051, 2.4452307287917243, 2.4437433559057404, 2.4422337342091898, 2.4425578387610964, 2.44169073555259, 2.4393513656984362, 2.4390819426189947, 2.437674699426921, 2.438153598342833, 2.4365521483842354, 2.436630707061266, 2.435637502552792, 2.4376201125385824, 2.4369255009128326, 2.4365011946621373, 2.4355920093505046, 2.4348686866446934, 2.4352449883180967, 2.4335895092825135, 2.4338957987282064, 2.433956790215181, 2.434258228650573, 2.432685753452215, 2.434010054492363, 2.4333913577900286, 2.433029953018596, 2.4334416283719102, 2.431335482313403, 2.4311696022198186, 2.432042403975062, 2.431952551796696, 2.4308239455095797, 2.4321279689271837, 2.4320066197452115, 2.432373058722494, 2.4318493116562863, 2.431114270505964, 2.431413382132685, 2.4302378256952495, 2.4304935057794776, 2.4307868787150606, 2.4317852612638378, 2.430340137618768, 2.431744458004679, 2.4321529939678905, 2.430134182054649, 2.4297007849573844, 2.4313958670324367, 2.430619399993082, 2.4296788173534543, 2.4307270520032063, 2.4284490859484036, 2.428152335595791, 2.4279336245887335, 2.427668514290874, 2.4290146581935685, 2.430655123223025, 2.4437053810644445, 2.438515183274506, 2.439266822470287, 2.4341272989582476, 2.4325825572748205, 2.430676704559483, 2.432003845179595, 2.43369642997914, 2.4334925872834066, 2.4318317667904332, 2.431332638718999, 2.4296846771631886, 2.4272098671484286, 2.4253722656923644, 2.4253612546705368, 2.426750236468149, 2.4262049233399376, 2.4261488052853815, 2.4245042680470115, 2.4619110152461934, 2.459225740569818, 2.444620149287355, 2.436055098324096, 2.436707606991214, 2.4318957367961658, 2.4299574249823723, 2.432707301008628, 2.4318516958420773, 2.4301471685726783, 2.4301365496931133, 2.430493562764945, 2.4288717152401653, 2.428154430252326, 2.428395197768476, 2.4287527800095887, 2.427985971713213, 2.4280854101788094, 2.430804910111476, 2.429782157169475, 2.4284260202482253, 2.4284808605358585, 2.429473323841604, 2.427966426187472, 2.4271415065201403, 2.425840210278176, 2.426268667166238, 2.434752332894954, 2.4347241164722484, 2.4290470624606466, 2.4318085624452, 2.4283605890841944, 2.4271026809112737, 2.4268715138797643, 2.425480424501078, 2.4231368550529715, 2.423690567369089, 2.424422464723215, 2.422963478922599, 2.423266114738198, 2.422539729993691, 2.4205908837994023, 2.420429312962526, 2.4201984678695334, 2.421346564361447, 2.421433476500932, 2.421449646274167, 2.42061075096992, 2.420755883163985, 2.4200257052631104, 2.4215507410390176, 2.4196580949505253, 2.4215206542788588, 2.4207604084171552, 2.423394954277994, 2.4208490965057936, 2.41915208583495, 2.4204997477834964, 2.4194692975931344, 2.41996297043207, 2.419409647516646, 2.4193549814165496, 2.4180493182470175, 2.418513033767011, 2.4269819741376373, 2.4615934556025008, 2.507792497219736, 2.5554565545225048, 2.639131931210935, 2.615646136027342, 2.5793010229937106, 2.5256928500698335, 2.4798086717633008, 2.4661773465252512, 2.4669581913605363, 2.4592501399453415, 2.452818360318883, 2.4503373383986142, 2.447716771993304, 2.444228740981962, 2.442486638650757, 2.442647054112178, 2.442128435738033, 2.441076119381789, 2.4583913227371124, 2.451340186277699, 2.4417031088404095, 2.4454116436490287, 2.4393843815312004, 2.440772044927922, 2.43694312704662, 2.4351591226746168, 2.4333658677596577, 2.431416822605799, 2.43174899559491, 2.430435123384856, 2.4302851528112894, 2.4279717969209016, 2.4281990226534114, 2.427325031351015, 2.426468040908876, 2.4263628933219206, 2.426126709513106, 2.4269193213578366, 2.428380778095316, 2.4275347672448757, 2.427679527662618, 2.427787557128029, 2.427631053885395, 2.4279264019255278, 2.4260403673262076, 2.4272982557206673, 2.426848328040121, 2.426076132462989, 2.425823294406554, 2.425269545666736, 2.4251322555346166, 2.4256511510030445, 2.425328996196175, 2.424785479089318, 2.425738701438512, 2.4257864301943926, 2.424780598705065, 2.424819884114197, 2.4261478899930293, 2.426999271185246, 2.4266554608237327, 2.427029101608715, 2.4257249604015625, 2.426179903978195, 2.425643993550013, 2.42360577769348, 2.423801676399654, 2.4218902346045086, 2.423439817311093], 'acc': [0.08542094509092445, 0.08459958909442782, 0.08459958966813783, 0.09034907592028318, 0.09897330590892867, 0.09034907631193588, 0.0891170428702474, 0.08870636583965662, 0.08583162251316791, 0.08706365558156243, 0.08829568783911347, 0.09691991780940022, 0.10965092395120578, 0.11540041038999813, 0.11334702232718713, 0.1125256674827002, 0.13018480405303243, 0.1396303890544531, 0.16139630379128506, 0.15687884940503805, 0.16673511321784534, 0.16386037010554172, 0.1675564676890383, 0.1638603704971944, 0.16550307901121017, 0.16673511362785676, 0.16755646708320054, 0.17207392088196852, 0.17084189000438127, 0.17494866598925307, 0.1753593436073229, 0.17248459867750596, 0.17002053396657751, 0.17125256744498346, 0.1745379881753569, 0.1770020538837758, 0.1778234083182513, 0.17330595373617794, 0.17248459850003833, 0.17371663135424775, 0.17371663194172682, 0.17535934241400608, 0.1544147854590563, 0.1659137578042381, 0.16755646884563768, 0.16960985636686643, 0.16180698043022312, 0.1720739224485793, 0.17535934243236478, 0.17659137685318502, 0.17535934319731147, 0.17741272993523483, 0.17577002061955493, 0.17535934358896416, 0.17577002142121906, 0.17823408572213606, 0.17741273169767197, 0.17741273169767197, 0.17782340794495732, 0.17618069706266665, 0.17741273050435516, 0.1757700208153813, 0.17659137487656282, 0.17535934399897557, 0.1757700208153813, 0.1753593428240175, 0.17864476277108554, 0.1733059559086265, 0.1778234091015567, 0.17659137546404186, 0.17741273009434372, 0.17535934321567018, 0.17741273169767197, 0.18069815086143462, 0.1798767964269591, 0.1794661195921947, 0.17905544117246075, 0.1790554413682871, 0.17946611839887788, 0.18069815303388317, 0.1774127315018456, 0.17700205327793803, 0.18151950807419645, 0.17946611879053057, 0.18151950825166407, 0.17987679623113276, 0.18110882944027745, 0.1794661181846928, 0.18110882906698347, 0.17823408556302714, 0.18069815105726098, 0.1794661185947042, 0.17946611898635692, 0.17946611800722517, 0.17864476474770774, 0.1770020527088177, 0.17659137546404186, 0.1774127318934983, 0.17823408632797383, 0.17987679758355848, 0.17905544236577756, 0.17823408613214747, 0.17823408593632112, 0.1786447645518814, 0.17618069786433077, 0.1778234085324364, 0.17700205327793803, 0.17864476314437952, 0.18069815144891366, 0.1798767976019172, 0.18028747461414923, 0.1786447641602287, 0.17946612017967373, 0.1761806980417984, 0.1757700208153813, 0.17700205407960215, 0.1823408617253666, 0.17946611800722517, 0.1806981516263813, 0.1794661195921947, 0.1794661185947042, 0.17864476355439093, 0.181519507076706, 0.18110883045612663, 0.1806981526422305, 0.18028747424085528, 0.18234086331033608, 0.17864476396440235, 0.17700205425706977, 0.1798767960169477, 0.1782340869338116, 0.18316221774481162, 0.18069815303388317, 0.1827515389517837, 0.18028747384920257, 0.18234086233120433, 0.17905544234741885, 0.1819301850864285, 0.18028747539745463, 0.1831622169431475, 0.18028747365337622, 0.1823408625270307, 0.18069815303388317, 0.1823408634878037, 0.18110882847950444, 0.17864476453352268, 0.17741273050435516, 0.16919917974628707, 0.17412731055728708, 0.1774127318934983, 0.17946611918218328, 0.17700205366959074, 0.17782340931574178, 0.17987679621277403, 0.1782340857588535, 0.17741273011270245, 0.1745379885670096, 0.1815195078600114, 0.1790554409766344, 0.17453798796117184, 0.17700205329629676, 0.17700205427542848, 0.17864476316273825, 0.18110883045612663, 0.17618069745431936, 0.17577001983624954, 0.1691991787487966, 0.1679671460720548, 0.1691991791404493, 0.1745379885670096, 0.17659137646153233, 0.17371663333086998, 0.17371663274339091, 0.17659137487656282, 0.17577001944459683, 0.17330595571280016, 0.17618069884346252, 0.17412730955979663, 0.17659137546404186, 0.176180697258493, 0.1778234075349459, 0.17535934341149653, 0.17618069884346252, 0.17946611898635692, 0.17371663313504362, 0.17782340771241353, 0.17782340851407766, 0.17289527829055668, 0.17659137526821553, 0.176180697258493, 0.18028747343919116, 0.17577001985460824, 0.17207392187945897, 0.17494866520594768, 0.18069815262387176, 0.1708418893985435, 0.17412730955979663, 0.1782340869338116, 0.17577001983624954, 0.17618069706266665, 0.17659137468073646, 0.17823408632797383, 0.1794661197696623, 0.17330595412783065, 0.17494866438592485, 0.17823408730710558, 0.17659137626570598, 0.17494866618507943, 0.17741273050435516, 0.17823408691545287, 0.1790554409766344, 0.17823408513465702, 0.1790554425616039, 0.1774127303085288, 0.17946611998384737, 0.18521560644099844, 0.1782340867196265, 0.17782340892408907, 0.17946611898635692, 0.18151950688087964, 0.18275153953926274, 0.17659137546404186, 0.18069815184056637, 0.1819301854780812, 0.1815195068625209, 0.17700205447125483, 0.17659137646153233, 0.18110882847950444, 0.17864476474770774, 0.17946611898635692, 0.17494866540177403, 0.16344969268329823, 0.14127310054258155, 0.13880903485252138, 0.12402464038782296, 0.10718685844779259, 0.13880903559910931, 0.17453798659038738, 0.17412731075311344, 0.16919917775130613, 0.1691991775554798, 0.17125256603748157, 0.17494866438592485, 0.17700205386541706, 0.17618069884346252, 0.17453798835282452, 0.17248459889169102, 0.17125256725833646, 0.1741273101656344, 0.17700205347376438, 0.18069815184056637, 0.18726899353385706, 0.17987679640860038, 0.18234086233120433, 0.18562628325740416, 0.18316221756734397, 0.17700205447125483, 0.1790554405849817, 0.18480492823544958, 0.18110882926280983, 0.18193018369728534, 0.18357289636037188, 0.17905544177829852, 0.17905544236577756, 0.17823408652380018, 0.18275154073257954, 0.18480492743378546, 0.18069815184056637, 0.1770020538837758, 0.17864476357274967, 0.18028747345754986, 0.18069815084307592, 0.17741273071854022, 0.1790554405849817, 0.18275153911089262, 0.18275154071422084, 0.18028747541581336, 0.18275153953926274, 0.18275154112423225, 0.18028747343919116, 0.17905544134992837, 0.17946611861306294, 0.1778234097257532, 0.1798767968186118, 0.18110883004611522, 0.17823408534884208, 0.18069815086143462, 0.17946611898635692, 0.1843942504215534, 0.18069815125308733, 0.1843942511865001, 0.18603695989634222, 0.1843942507948474, 0.1856262822782724, 0.1815195062934006, 0.18357289575453412, 0.18521560544350799, 0.18480492880456992, 0.18316221656985351, 0.18234086290032467]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
