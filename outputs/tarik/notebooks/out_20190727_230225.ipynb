{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf52.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 23:02:25 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '2Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ce', 'sg', 'eo', 'yd', 'my', 'eg', 'ib', 'ek', 'by', 'mb', 'aa', 'ck', 'eb', 'ds', 'sk'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002F08301F278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002F0EBEA6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7130, Accuracy:0.0830, Validation Loss:2.7064, Validation Accuracy:0.0837\n",
    "Epoch #2: Loss:2.7028, Accuracy:0.0891, Validation Loss:2.6977, Validation Accuracy:0.0887\n",
    "Epoch #3: Loss:2.6949, Accuracy:0.0891, Validation Loss:2.6905, Validation Accuracy:0.0887\n",
    "Epoch #4: Loss:2.6883, Accuracy:0.0891, Validation Loss:2.6845, Validation Accuracy:0.0887\n",
    "Epoch #5: Loss:2.6825, Accuracy:0.0891, Validation Loss:2.6797, Validation Accuracy:0.0887\n",
    "Epoch #6: Loss:2.6787, Accuracy:0.0891, Validation Loss:2.6760, Validation Accuracy:0.0887\n",
    "Epoch #7: Loss:2.6750, Accuracy:0.0891, Validation Loss:2.6733, Validation Accuracy:0.0887\n",
    "Epoch #8: Loss:2.6726, Accuracy:0.0891, Validation Loss:2.6710, Validation Accuracy:0.0887\n",
    "Epoch #9: Loss:2.6701, Accuracy:0.0891, Validation Loss:2.6689, Validation Accuracy:0.0887\n",
    "Epoch #10: Loss:2.6681, Accuracy:0.0891, Validation Loss:2.6671, Validation Accuracy:0.0887\n",
    "Epoch #11: Loss:2.6666, Accuracy:0.0895, Validation Loss:2.6655, Validation Accuracy:0.0903\n",
    "Epoch #12: Loss:2.6650, Accuracy:0.1010, Validation Loss:2.6641, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6635, Accuracy:0.1023, Validation Loss:2.6626, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6620, Accuracy:0.1023, Validation Loss:2.6608, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6597, Accuracy:0.1023, Validation Loss:2.6585, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6571, Accuracy:0.1023, Validation Loss:2.6552, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6527, Accuracy:0.1023, Validation Loss:2.6503, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6471, Accuracy:0.1023, Validation Loss:2.6431, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6380, Accuracy:0.1035, Validation Loss:2.6325, Validation Accuracy:0.1051\n",
    "Epoch #20: Loss:2.6248, Accuracy:0.1170, Validation Loss:2.6185, Validation Accuracy:0.1248\n",
    "Epoch #21: Loss:2.6083, Accuracy:0.1335, Validation Loss:2.6029, Validation Accuracy:0.1429\n",
    "Epoch #22: Loss:2.5917, Accuracy:0.1421, Validation Loss:2.5871, Validation Accuracy:0.1396\n",
    "Epoch #23: Loss:2.5743, Accuracy:0.1478, Validation Loss:2.5739, Validation Accuracy:0.1445\n",
    "Epoch #24: Loss:2.5590, Accuracy:0.1454, Validation Loss:2.5639, Validation Accuracy:0.1396\n",
    "Epoch #25: Loss:2.5484, Accuracy:0.1417, Validation Loss:2.5554, Validation Accuracy:0.1494\n",
    "Epoch #26: Loss:2.5372, Accuracy:0.1466, Validation Loss:2.5475, Validation Accuracy:0.1445\n",
    "Epoch #27: Loss:2.5258, Accuracy:0.1474, Validation Loss:2.5396, Validation Accuracy:0.1429\n",
    "Epoch #28: Loss:2.5177, Accuracy:0.1495, Validation Loss:2.5321, Validation Accuracy:0.1609\n",
    "Epoch #29: Loss:2.5086, Accuracy:0.1671, Validation Loss:2.5251, Validation Accuracy:0.1823\n",
    "Epoch #30: Loss:2.4974, Accuracy:0.1704, Validation Loss:2.5170, Validation Accuracy:0.1757\n",
    "Epoch #31: Loss:2.4900, Accuracy:0.1737, Validation Loss:2.5103, Validation Accuracy:0.1905\n",
    "Epoch #32: Loss:2.4811, Accuracy:0.1828, Validation Loss:2.5030, Validation Accuracy:0.1806\n",
    "Epoch #33: Loss:2.4740, Accuracy:0.1815, Validation Loss:2.4954, Validation Accuracy:0.1790\n",
    "Epoch #34: Loss:2.4648, Accuracy:0.1819, Validation Loss:2.4895, Validation Accuracy:0.1954\n",
    "Epoch #35: Loss:2.4596, Accuracy:0.1971, Validation Loss:2.4854, Validation Accuracy:0.1757\n",
    "Epoch #36: Loss:2.4519, Accuracy:0.1951, Validation Loss:2.4733, Validation Accuracy:0.2003\n",
    "Epoch #37: Loss:2.4413, Accuracy:0.1984, Validation Loss:2.4642, Validation Accuracy:0.1872\n",
    "Epoch #38: Loss:2.4271, Accuracy:0.2049, Validation Loss:2.4542, Validation Accuracy:0.2020\n",
    "Epoch #39: Loss:2.4188, Accuracy:0.2131, Validation Loss:2.4465, Validation Accuracy:0.1823\n",
    "Epoch #40: Loss:2.4094, Accuracy:0.2049, Validation Loss:2.4347, Validation Accuracy:0.2118\n",
    "Epoch #41: Loss:2.4001, Accuracy:0.2152, Validation Loss:2.4251, Validation Accuracy:0.2020\n",
    "Epoch #42: Loss:2.3900, Accuracy:0.2168, Validation Loss:2.4168, Validation Accuracy:0.1987\n",
    "Epoch #43: Loss:2.3800, Accuracy:0.2119, Validation Loss:2.4059, Validation Accuracy:0.2085\n",
    "Epoch #44: Loss:2.3708, Accuracy:0.2201, Validation Loss:2.3969, Validation Accuracy:0.2069\n",
    "Epoch #45: Loss:2.3573, Accuracy:0.2255, Validation Loss:2.3853, Validation Accuracy:0.1954\n",
    "Epoch #46: Loss:2.3480, Accuracy:0.2214, Validation Loss:2.3721, Validation Accuracy:0.2118\n",
    "Epoch #47: Loss:2.3342, Accuracy:0.2337, Validation Loss:2.3659, Validation Accuracy:0.1954\n",
    "Epoch #48: Loss:2.3204, Accuracy:0.2287, Validation Loss:2.3459, Validation Accuracy:0.2085\n",
    "Epoch #49: Loss:2.3023, Accuracy:0.2390, Validation Loss:2.3341, Validation Accuracy:0.2069\n",
    "Epoch #50: Loss:2.2898, Accuracy:0.2341, Validation Loss:2.3213, Validation Accuracy:0.2085\n",
    "Epoch #51: Loss:2.2747, Accuracy:0.2431, Validation Loss:2.3098, Validation Accuracy:0.2200\n",
    "Epoch #52: Loss:2.2569, Accuracy:0.2427, Validation Loss:2.2930, Validation Accuracy:0.2315\n",
    "Epoch #53: Loss:2.2383, Accuracy:0.2456, Validation Loss:2.2734, Validation Accuracy:0.2332\n",
    "Epoch #54: Loss:2.2204, Accuracy:0.2550, Validation Loss:2.2611, Validation Accuracy:0.2299\n",
    "Epoch #55: Loss:2.2055, Accuracy:0.2604, Validation Loss:2.2565, Validation Accuracy:0.2315\n",
    "Epoch #56: Loss:2.1970, Accuracy:0.2583, Validation Loss:2.2519, Validation Accuracy:0.2200\n",
    "Epoch #57: Loss:2.1861, Accuracy:0.2530, Validation Loss:2.2234, Validation Accuracy:0.2348\n",
    "Epoch #58: Loss:2.1674, Accuracy:0.2587, Validation Loss:2.2171, Validation Accuracy:0.2397\n",
    "Epoch #59: Loss:2.1615, Accuracy:0.2616, Validation Loss:2.2178, Validation Accuracy:0.2430\n",
    "Epoch #60: Loss:2.1570, Accuracy:0.2665, Validation Loss:2.2080, Validation Accuracy:0.2529\n",
    "Epoch #61: Loss:2.1575, Accuracy:0.2698, Validation Loss:2.1842, Validation Accuracy:0.2512\n",
    "Epoch #62: Loss:2.1347, Accuracy:0.2723, Validation Loss:2.1794, Validation Accuracy:0.2299\n",
    "Epoch #63: Loss:2.1232, Accuracy:0.2793, Validation Loss:2.1792, Validation Accuracy:0.2627\n",
    "Epoch #64: Loss:2.1157, Accuracy:0.2805, Validation Loss:2.1671, Validation Accuracy:0.2611\n",
    "Epoch #65: Loss:2.1046, Accuracy:0.2875, Validation Loss:2.1560, Validation Accuracy:0.2594\n",
    "Epoch #66: Loss:2.1006, Accuracy:0.2838, Validation Loss:2.1481, Validation Accuracy:0.2693\n",
    "Epoch #67: Loss:2.0902, Accuracy:0.2875, Validation Loss:2.1420, Validation Accuracy:0.2791\n",
    "Epoch #68: Loss:2.0816, Accuracy:0.2912, Validation Loss:2.1328, Validation Accuracy:0.2726\n",
    "Epoch #69: Loss:2.0761, Accuracy:0.2932, Validation Loss:2.1256, Validation Accuracy:0.2726\n",
    "Epoch #70: Loss:2.0668, Accuracy:0.2965, Validation Loss:2.1204, Validation Accuracy:0.2841\n",
    "Epoch #71: Loss:2.0600, Accuracy:0.2973, Validation Loss:2.1330, Validation Accuracy:0.2775\n",
    "Epoch #72: Loss:2.0649, Accuracy:0.2957, Validation Loss:2.1092, Validation Accuracy:0.2759\n",
    "Epoch #73: Loss:2.0519, Accuracy:0.2998, Validation Loss:2.1085, Validation Accuracy:0.2791\n",
    "Epoch #74: Loss:2.0462, Accuracy:0.3043, Validation Loss:2.1099, Validation Accuracy:0.2660\n",
    "Epoch #75: Loss:2.0452, Accuracy:0.3002, Validation Loss:2.1378, Validation Accuracy:0.2677\n",
    "Epoch #76: Loss:2.0537, Accuracy:0.3047, Validation Loss:2.1277, Validation Accuracy:0.2726\n",
    "Epoch #77: Loss:2.0521, Accuracy:0.3010, Validation Loss:2.0854, Validation Accuracy:0.2742\n",
    "Epoch #78: Loss:2.0279, Accuracy:0.3092, Validation Loss:2.0934, Validation Accuracy:0.2890\n",
    "Epoch #79: Loss:2.0239, Accuracy:0.3072, Validation Loss:2.0800, Validation Accuracy:0.2808\n",
    "Epoch #80: Loss:2.0236, Accuracy:0.3064, Validation Loss:2.0747, Validation Accuracy:0.2791\n",
    "Epoch #81: Loss:2.0060, Accuracy:0.3133, Validation Loss:2.0786, Validation Accuracy:0.2775\n",
    "Epoch #82: Loss:2.0061, Accuracy:0.3097, Validation Loss:2.0995, Validation Accuracy:0.2742\n",
    "Epoch #83: Loss:2.0101, Accuracy:0.3084, Validation Loss:2.0611, Validation Accuracy:0.2857\n",
    "Epoch #84: Loss:1.9983, Accuracy:0.3055, Validation Loss:2.0766, Validation Accuracy:0.2989\n",
    "Epoch #85: Loss:1.9917, Accuracy:0.3150, Validation Loss:2.0514, Validation Accuracy:0.2923\n",
    "Epoch #86: Loss:1.9830, Accuracy:0.3207, Validation Loss:2.0496, Validation Accuracy:0.2791\n",
    "Epoch #87: Loss:1.9764, Accuracy:0.3158, Validation Loss:2.0434, Validation Accuracy:0.2972\n",
    "Epoch #88: Loss:1.9759, Accuracy:0.3125, Validation Loss:2.0438, Validation Accuracy:0.2972\n",
    "Epoch #89: Loss:1.9692, Accuracy:0.3195, Validation Loss:2.0367, Validation Accuracy:0.2890\n",
    "Epoch #90: Loss:1.9615, Accuracy:0.3220, Validation Loss:2.0326, Validation Accuracy:0.2874\n",
    "Epoch #91: Loss:1.9584, Accuracy:0.3228, Validation Loss:2.0309, Validation Accuracy:0.2857\n",
    "Epoch #92: Loss:1.9562, Accuracy:0.3220, Validation Loss:2.0366, Validation Accuracy:0.2890\n",
    "Epoch #93: Loss:1.9518, Accuracy:0.3203, Validation Loss:2.0244, Validation Accuracy:0.3038\n",
    "Epoch #94: Loss:1.9474, Accuracy:0.3220, Validation Loss:2.0262, Validation Accuracy:0.3038\n",
    "Epoch #95: Loss:1.9416, Accuracy:0.3203, Validation Loss:2.0180, Validation Accuracy:0.2906\n",
    "Epoch #96: Loss:1.9364, Accuracy:0.3290, Validation Loss:2.0206, Validation Accuracy:0.3087\n",
    "Epoch #97: Loss:1.9402, Accuracy:0.3273, Validation Loss:2.0096, Validation Accuracy:0.3038\n",
    "Epoch #98: Loss:1.9317, Accuracy:0.3261, Validation Loss:2.0174, Validation Accuracy:0.2972\n",
    "Epoch #99: Loss:1.9308, Accuracy:0.3265, Validation Loss:2.0169, Validation Accuracy:0.2972\n",
    "Epoch #100: Loss:1.9373, Accuracy:0.3228, Validation Loss:2.0104, Validation Accuracy:0.2989\n",
    "Epoch #101: Loss:1.9531, Accuracy:0.3195, Validation Loss:2.0189, Validation Accuracy:0.3087\n",
    "Epoch #102: Loss:1.9556, Accuracy:0.3207, Validation Loss:2.0013, Validation Accuracy:0.3153\n",
    "Epoch #103: Loss:1.9453, Accuracy:0.3170, Validation Loss:2.0180, Validation Accuracy:0.2956\n",
    "Epoch #104: Loss:1.9265, Accuracy:0.3265, Validation Loss:2.0136, Validation Accuracy:0.2972\n",
    "Epoch #105: Loss:1.9131, Accuracy:0.3290, Validation Loss:2.0050, Validation Accuracy:0.3005\n",
    "Epoch #106: Loss:1.9132, Accuracy:0.3347, Validation Loss:1.9976, Validation Accuracy:0.3120\n",
    "Epoch #107: Loss:1.9089, Accuracy:0.3281, Validation Loss:2.0074, Validation Accuracy:0.3038\n",
    "Epoch #108: Loss:1.9119, Accuracy:0.3269, Validation Loss:1.9954, Validation Accuracy:0.3120\n",
    "Epoch #109: Loss:1.9192, Accuracy:0.3331, Validation Loss:1.9889, Validation Accuracy:0.3169\n",
    "Epoch #110: Loss:1.9218, Accuracy:0.3265, Validation Loss:2.0021, Validation Accuracy:0.3169\n",
    "Epoch #111: Loss:1.9099, Accuracy:0.3294, Validation Loss:1.9843, Validation Accuracy:0.3268\n",
    "Epoch #112: Loss:1.9015, Accuracy:0.3351, Validation Loss:2.0007, Validation Accuracy:0.3186\n",
    "Epoch #113: Loss:1.9063, Accuracy:0.3400, Validation Loss:1.9860, Validation Accuracy:0.3235\n",
    "Epoch #114: Loss:1.8987, Accuracy:0.3343, Validation Loss:1.9839, Validation Accuracy:0.3235\n",
    "Epoch #115: Loss:1.8946, Accuracy:0.3363, Validation Loss:1.9871, Validation Accuracy:0.3169\n",
    "Epoch #116: Loss:1.8944, Accuracy:0.3400, Validation Loss:1.9759, Validation Accuracy:0.3268\n",
    "Epoch #117: Loss:1.8917, Accuracy:0.3425, Validation Loss:2.0076, Validation Accuracy:0.3038\n",
    "Epoch #118: Loss:1.8904, Accuracy:0.3343, Validation Loss:1.9784, Validation Accuracy:0.3251\n",
    "Epoch #119: Loss:1.8861, Accuracy:0.3437, Validation Loss:1.9862, Validation Accuracy:0.3153\n",
    "Epoch #120: Loss:1.8887, Accuracy:0.3433, Validation Loss:1.9821, Validation Accuracy:0.3136\n",
    "Epoch #121: Loss:1.8948, Accuracy:0.3396, Validation Loss:1.9765, Validation Accuracy:0.3235\n",
    "Epoch #122: Loss:1.8811, Accuracy:0.3368, Validation Loss:1.9703, Validation Accuracy:0.3268\n",
    "Epoch #123: Loss:1.8784, Accuracy:0.3429, Validation Loss:1.9691, Validation Accuracy:0.3300\n",
    "Epoch #124: Loss:1.8761, Accuracy:0.3470, Validation Loss:1.9681, Validation Accuracy:0.3218\n",
    "Epoch #125: Loss:1.8751, Accuracy:0.3425, Validation Loss:1.9701, Validation Accuracy:0.3235\n",
    "Epoch #126: Loss:1.8739, Accuracy:0.3446, Validation Loss:1.9663, Validation Accuracy:0.3300\n",
    "Epoch #127: Loss:1.8692, Accuracy:0.3409, Validation Loss:1.9701, Validation Accuracy:0.3235\n",
    "Epoch #128: Loss:1.8726, Accuracy:0.3458, Validation Loss:1.9616, Validation Accuracy:0.3350\n",
    "Epoch #129: Loss:1.8706, Accuracy:0.3478, Validation Loss:1.9647, Validation Accuracy:0.3333\n",
    "Epoch #130: Loss:1.8719, Accuracy:0.3441, Validation Loss:1.9901, Validation Accuracy:0.3071\n",
    "Epoch #131: Loss:1.8730, Accuracy:0.3376, Validation Loss:1.9828, Validation Accuracy:0.3103\n",
    "Epoch #132: Loss:1.8712, Accuracy:0.3524, Validation Loss:1.9824, Validation Accuracy:0.3038\n",
    "Epoch #133: Loss:1.8653, Accuracy:0.3487, Validation Loss:1.9600, Validation Accuracy:0.3284\n",
    "Epoch #134: Loss:1.8726, Accuracy:0.3495, Validation Loss:1.9655, Validation Accuracy:0.3333\n",
    "Epoch #135: Loss:1.8781, Accuracy:0.3413, Validation Loss:1.9710, Validation Accuracy:0.3333\n",
    "Epoch #136: Loss:1.8803, Accuracy:0.3413, Validation Loss:1.9769, Validation Accuracy:0.3284\n",
    "Epoch #137: Loss:1.8671, Accuracy:0.3544, Validation Loss:1.9661, Validation Accuracy:0.3383\n",
    "Epoch #138: Loss:1.8659, Accuracy:0.3515, Validation Loss:1.9554, Validation Accuracy:0.3235\n",
    "Epoch #139: Loss:1.8659, Accuracy:0.3478, Validation Loss:1.9567, Validation Accuracy:0.3202\n",
    "Epoch #140: Loss:1.8560, Accuracy:0.3548, Validation Loss:1.9612, Validation Accuracy:0.3317\n",
    "Epoch #141: Loss:1.8520, Accuracy:0.3515, Validation Loss:1.9550, Validation Accuracy:0.3350\n",
    "Epoch #142: Loss:1.8529, Accuracy:0.3528, Validation Loss:1.9611, Validation Accuracy:0.3300\n",
    "Epoch #143: Loss:1.8519, Accuracy:0.3532, Validation Loss:1.9470, Validation Accuracy:0.3284\n",
    "Epoch #144: Loss:1.8463, Accuracy:0.3524, Validation Loss:1.9471, Validation Accuracy:0.3284\n",
    "Epoch #145: Loss:1.8476, Accuracy:0.3528, Validation Loss:1.9497, Validation Accuracy:0.3317\n",
    "Epoch #146: Loss:1.8493, Accuracy:0.3610, Validation Loss:1.9613, Validation Accuracy:0.3333\n",
    "Epoch #147: Loss:1.8651, Accuracy:0.3478, Validation Loss:1.9510, Validation Accuracy:0.3415\n",
    "Epoch #148: Loss:1.8613, Accuracy:0.3515, Validation Loss:1.9571, Validation Accuracy:0.3186\n",
    "Epoch #149: Loss:1.8573, Accuracy:0.3540, Validation Loss:2.0028, Validation Accuracy:0.3136\n",
    "Epoch #150: Loss:1.8562, Accuracy:0.3598, Validation Loss:1.9569, Validation Accuracy:0.3186\n",
    "Epoch #151: Loss:1.8534, Accuracy:0.3536, Validation Loss:1.9675, Validation Accuracy:0.3235\n",
    "Epoch #152: Loss:1.8462, Accuracy:0.3552, Validation Loss:1.9393, Validation Accuracy:0.3300\n",
    "Epoch #153: Loss:1.8436, Accuracy:0.3536, Validation Loss:1.9506, Validation Accuracy:0.3432\n",
    "Epoch #154: Loss:1.8395, Accuracy:0.3532, Validation Loss:1.9408, Validation Accuracy:0.3284\n",
    "Epoch #155: Loss:1.8348, Accuracy:0.3589, Validation Loss:1.9407, Validation Accuracy:0.3350\n",
    "Epoch #156: Loss:1.8402, Accuracy:0.3630, Validation Loss:1.9737, Validation Accuracy:0.3169\n",
    "Epoch #157: Loss:1.8436, Accuracy:0.3569, Validation Loss:1.9623, Validation Accuracy:0.3186\n",
    "Epoch #158: Loss:1.8358, Accuracy:0.3556, Validation Loss:1.9363, Validation Accuracy:0.3251\n",
    "Epoch #159: Loss:1.8284, Accuracy:0.3598, Validation Loss:1.9374, Validation Accuracy:0.3284\n",
    "Epoch #160: Loss:1.8235, Accuracy:0.3614, Validation Loss:1.9367, Validation Accuracy:0.3383\n",
    "Epoch #161: Loss:1.8232, Accuracy:0.3692, Validation Loss:1.9339, Validation Accuracy:0.3333\n",
    "Epoch #162: Loss:1.8240, Accuracy:0.3618, Validation Loss:1.9318, Validation Accuracy:0.3399\n",
    "Epoch #163: Loss:1.8227, Accuracy:0.3671, Validation Loss:1.9360, Validation Accuracy:0.3383\n",
    "Epoch #164: Loss:1.8304, Accuracy:0.3589, Validation Loss:1.9452, Validation Accuracy:0.3202\n",
    "Epoch #165: Loss:1.8246, Accuracy:0.3593, Validation Loss:1.9496, Validation Accuracy:0.3268\n",
    "Epoch #166: Loss:1.8300, Accuracy:0.3655, Validation Loss:1.9312, Validation Accuracy:0.3268\n",
    "Epoch #167: Loss:1.8235, Accuracy:0.3634, Validation Loss:1.9358, Validation Accuracy:0.3300\n",
    "Epoch #168: Loss:1.8170, Accuracy:0.3639, Validation Loss:1.9278, Validation Accuracy:0.3284\n",
    "Epoch #169: Loss:1.8166, Accuracy:0.3606, Validation Loss:1.9271, Validation Accuracy:0.3383\n",
    "Epoch #170: Loss:1.8120, Accuracy:0.3651, Validation Loss:1.9319, Validation Accuracy:0.3350\n",
    "Epoch #171: Loss:1.8109, Accuracy:0.3639, Validation Loss:1.9215, Validation Accuracy:0.3399\n",
    "Epoch #172: Loss:1.8129, Accuracy:0.3680, Validation Loss:1.9394, Validation Accuracy:0.3366\n",
    "Epoch #173: Loss:1.8127, Accuracy:0.3676, Validation Loss:1.9234, Validation Accuracy:0.3448\n",
    "Epoch #174: Loss:1.8182, Accuracy:0.3622, Validation Loss:1.9364, Validation Accuracy:0.3432\n",
    "Epoch #175: Loss:1.8169, Accuracy:0.3610, Validation Loss:1.9373, Validation Accuracy:0.3350\n",
    "Epoch #176: Loss:1.8032, Accuracy:0.3708, Validation Loss:1.9192, Validation Accuracy:0.3383\n",
    "Epoch #177: Loss:1.8039, Accuracy:0.3721, Validation Loss:1.9268, Validation Accuracy:0.3300\n",
    "Epoch #178: Loss:1.8043, Accuracy:0.3684, Validation Loss:1.9224, Validation Accuracy:0.3432\n",
    "Epoch #179: Loss:1.8076, Accuracy:0.3671, Validation Loss:1.9192, Validation Accuracy:0.3300\n",
    "Epoch #180: Loss:1.8061, Accuracy:0.3737, Validation Loss:1.9342, Validation Accuracy:0.3366\n",
    "Epoch #181: Loss:1.8004, Accuracy:0.3659, Validation Loss:1.9189, Validation Accuracy:0.3366\n",
    "Epoch #182: Loss:1.8048, Accuracy:0.3733, Validation Loss:1.9284, Validation Accuracy:0.3350\n",
    "Epoch #183: Loss:1.8037, Accuracy:0.3700, Validation Loss:1.9255, Validation Accuracy:0.3415\n",
    "Epoch #184: Loss:1.8085, Accuracy:0.3647, Validation Loss:1.9231, Validation Accuracy:0.3465\n",
    "Epoch #185: Loss:1.7997, Accuracy:0.3733, Validation Loss:1.9280, Validation Accuracy:0.3383\n",
    "Epoch #186: Loss:1.7974, Accuracy:0.3688, Validation Loss:1.9218, Validation Accuracy:0.3300\n",
    "Epoch #187: Loss:1.7975, Accuracy:0.3737, Validation Loss:1.9325, Validation Accuracy:0.3333\n",
    "Epoch #188: Loss:1.7995, Accuracy:0.3762, Validation Loss:1.9183, Validation Accuracy:0.3366\n",
    "Epoch #189: Loss:1.7911, Accuracy:0.3696, Validation Loss:1.9165, Validation Accuracy:0.3366\n",
    "Epoch #190: Loss:1.7917, Accuracy:0.3754, Validation Loss:1.9177, Validation Accuracy:0.3415\n",
    "Epoch #191: Loss:1.7946, Accuracy:0.3713, Validation Loss:1.9160, Validation Accuracy:0.3415\n",
    "Epoch #192: Loss:1.7898, Accuracy:0.3774, Validation Loss:1.9173, Validation Accuracy:0.3317\n",
    "Epoch #193: Loss:1.7890, Accuracy:0.3737, Validation Loss:1.9312, Validation Accuracy:0.3235\n",
    "Epoch #194: Loss:1.7987, Accuracy:0.3713, Validation Loss:1.9101, Validation Accuracy:0.3350\n",
    "Epoch #195: Loss:1.7952, Accuracy:0.3795, Validation Loss:1.9299, Validation Accuracy:0.3481\n",
    "Epoch #196: Loss:1.8007, Accuracy:0.3778, Validation Loss:1.9354, Validation Accuracy:0.3383\n",
    "Epoch #197: Loss:1.8045, Accuracy:0.3713, Validation Loss:1.9142, Validation Accuracy:0.3350\n",
    "Epoch #198: Loss:1.7919, Accuracy:0.3733, Validation Loss:1.9207, Validation Accuracy:0.3350\n",
    "Epoch #199: Loss:1.7855, Accuracy:0.3795, Validation Loss:1.9272, Validation Accuracy:0.3366\n",
    "Epoch #200: Loss:1.7906, Accuracy:0.3700, Validation Loss:1.9081, Validation Accuracy:0.3366\n",
    "Epoch #201: Loss:1.7844, Accuracy:0.3828, Validation Loss:1.9267, Validation Accuracy:0.3481\n",
    "Epoch #202: Loss:1.7809, Accuracy:0.3754, Validation Loss:1.9033, Validation Accuracy:0.3366\n",
    "Epoch #203: Loss:1.7817, Accuracy:0.3770, Validation Loss:1.9244, Validation Accuracy:0.3432\n",
    "Epoch #204: Loss:1.7764, Accuracy:0.3749, Validation Loss:1.9055, Validation Accuracy:0.3399\n",
    "Epoch #205: Loss:1.7808, Accuracy:0.3766, Validation Loss:1.9268, Validation Accuracy:0.3448\n",
    "Epoch #206: Loss:1.7817, Accuracy:0.3819, Validation Loss:1.9179, Validation Accuracy:0.3399\n",
    "Epoch #207: Loss:1.7834, Accuracy:0.3704, Validation Loss:1.9116, Validation Accuracy:0.3481\n",
    "Epoch #208: Loss:1.7773, Accuracy:0.3795, Validation Loss:1.9101, Validation Accuracy:0.3432\n",
    "Epoch #209: Loss:1.7728, Accuracy:0.3807, Validation Loss:1.9200, Validation Accuracy:0.3399\n",
    "Epoch #210: Loss:1.7730, Accuracy:0.3741, Validation Loss:1.9068, Validation Accuracy:0.3399\n",
    "Epoch #211: Loss:1.7750, Accuracy:0.3770, Validation Loss:1.9225, Validation Accuracy:0.3415\n",
    "Epoch #212: Loss:1.7739, Accuracy:0.3758, Validation Loss:1.9031, Validation Accuracy:0.3432\n",
    "Epoch #213: Loss:1.7684, Accuracy:0.3856, Validation Loss:1.9073, Validation Accuracy:0.3399\n",
    "Epoch #214: Loss:1.7616, Accuracy:0.3836, Validation Loss:1.9057, Validation Accuracy:0.3465\n",
    "Epoch #215: Loss:1.7645, Accuracy:0.3828, Validation Loss:1.9048, Validation Accuracy:0.3399\n",
    "Epoch #216: Loss:1.7623, Accuracy:0.3819, Validation Loss:1.9122, Validation Accuracy:0.3415\n",
    "Epoch #217: Loss:1.7611, Accuracy:0.3856, Validation Loss:1.9034, Validation Accuracy:0.3432\n",
    "Epoch #218: Loss:1.7620, Accuracy:0.3791, Validation Loss:1.9033, Validation Accuracy:0.3432\n",
    "Epoch #219: Loss:1.7648, Accuracy:0.3844, Validation Loss:1.9052, Validation Accuracy:0.3448\n",
    "Epoch #220: Loss:1.7621, Accuracy:0.3844, Validation Loss:1.9083, Validation Accuracy:0.3481\n",
    "Epoch #221: Loss:1.7701, Accuracy:0.3860, Validation Loss:1.9067, Validation Accuracy:0.3514\n",
    "Epoch #222: Loss:1.7624, Accuracy:0.3840, Validation Loss:1.9066, Validation Accuracy:0.3465\n",
    "Epoch #223: Loss:1.7551, Accuracy:0.3856, Validation Loss:1.9035, Validation Accuracy:0.3399\n",
    "Epoch #224: Loss:1.7630, Accuracy:0.3819, Validation Loss:1.9342, Validation Accuracy:0.3333\n",
    "Epoch #225: Loss:1.7680, Accuracy:0.3791, Validation Loss:1.9192, Validation Accuracy:0.3399\n",
    "Epoch #226: Loss:1.7695, Accuracy:0.3836, Validation Loss:1.9127, Validation Accuracy:0.3465\n",
    "Epoch #227: Loss:1.7661, Accuracy:0.3803, Validation Loss:1.9131, Validation Accuracy:0.3498\n",
    "Epoch #228: Loss:1.7555, Accuracy:0.3881, Validation Loss:1.9067, Validation Accuracy:0.3383\n",
    "Epoch #229: Loss:1.7585, Accuracy:0.3848, Validation Loss:1.9077, Validation Accuracy:0.3481\n",
    "Epoch #230: Loss:1.7524, Accuracy:0.3881, Validation Loss:1.9028, Validation Accuracy:0.3432\n",
    "Epoch #231: Loss:1.7483, Accuracy:0.3906, Validation Loss:1.9159, Validation Accuracy:0.3481\n",
    "Epoch #232: Loss:1.7578, Accuracy:0.3819, Validation Loss:1.9036, Validation Accuracy:0.3448\n",
    "Epoch #233: Loss:1.7561, Accuracy:0.3893, Validation Loss:1.9133, Validation Accuracy:0.3514\n",
    "Epoch #234: Loss:1.7508, Accuracy:0.3848, Validation Loss:1.9057, Validation Accuracy:0.3432\n",
    "Epoch #235: Loss:1.7602, Accuracy:0.3893, Validation Loss:1.9101, Validation Accuracy:0.3514\n",
    "Epoch #236: Loss:1.7608, Accuracy:0.3786, Validation Loss:1.9297, Validation Accuracy:0.3399\n",
    "Epoch #237: Loss:1.7671, Accuracy:0.3856, Validation Loss:1.8984, Validation Accuracy:0.3415\n",
    "Epoch #238: Loss:1.7548, Accuracy:0.3889, Validation Loss:1.9107, Validation Accuracy:0.3514\n",
    "Epoch #239: Loss:1.7527, Accuracy:0.3914, Validation Loss:1.9097, Validation Accuracy:0.3498\n",
    "Epoch #240: Loss:1.7630, Accuracy:0.3869, Validation Loss:1.9107, Validation Accuracy:0.3530\n",
    "Epoch #241: Loss:1.7565, Accuracy:0.3864, Validation Loss:1.9038, Validation Accuracy:0.3481\n",
    "Epoch #242: Loss:1.7469, Accuracy:0.3918, Validation Loss:1.9104, Validation Accuracy:0.3432\n",
    "Epoch #243: Loss:1.7507, Accuracy:0.3947, Validation Loss:1.8999, Validation Accuracy:0.3415\n",
    "Epoch #244: Loss:1.7437, Accuracy:0.3873, Validation Loss:1.9061, Validation Accuracy:0.3547\n",
    "Epoch #245: Loss:1.7396, Accuracy:0.3869, Validation Loss:1.8992, Validation Accuracy:0.3465\n",
    "Epoch #246: Loss:1.7393, Accuracy:0.3914, Validation Loss:1.9051, Validation Accuracy:0.3547\n",
    "Epoch #247: Loss:1.7418, Accuracy:0.3893, Validation Loss:1.9045, Validation Accuracy:0.3448\n",
    "Epoch #248: Loss:1.7422, Accuracy:0.3938, Validation Loss:1.9022, Validation Accuracy:0.3481\n",
    "Epoch #249: Loss:1.7355, Accuracy:0.3918, Validation Loss:1.9031, Validation Accuracy:0.3415\n",
    "Epoch #250: Loss:1.7331, Accuracy:0.3934, Validation Loss:1.8997, Validation Accuracy:0.3530\n",
    "Epoch #251: Loss:1.7334, Accuracy:0.3860, Validation Loss:1.8937, Validation Accuracy:0.3432\n",
    "Epoch #252: Loss:1.7313, Accuracy:0.3938, Validation Loss:1.9025, Validation Accuracy:0.3530\n",
    "Epoch #253: Loss:1.7344, Accuracy:0.3910, Validation Loss:1.8992, Validation Accuracy:0.3481\n",
    "Epoch #254: Loss:1.7329, Accuracy:0.3951, Validation Loss:1.8946, Validation Accuracy:0.3498\n",
    "Epoch #255: Loss:1.7316, Accuracy:0.4000, Validation Loss:1.9038, Validation Accuracy:0.3448\n",
    "Epoch #256: Loss:1.7310, Accuracy:0.3856, Validation Loss:1.8975, Validation Accuracy:0.3415\n",
    "Epoch #257: Loss:1.7319, Accuracy:0.3984, Validation Loss:1.8998, Validation Accuracy:0.3514\n",
    "Epoch #258: Loss:1.7306, Accuracy:0.3971, Validation Loss:1.8967, Validation Accuracy:0.3432\n",
    "Epoch #259: Loss:1.7255, Accuracy:0.4004, Validation Loss:1.9042, Validation Accuracy:0.3415\n",
    "Epoch #260: Loss:1.7261, Accuracy:0.3930, Validation Loss:1.8923, Validation Accuracy:0.3514\n",
    "Epoch #261: Loss:1.7237, Accuracy:0.3988, Validation Loss:1.9010, Validation Accuracy:0.3448\n",
    "Epoch #262: Loss:1.7208, Accuracy:0.3971, Validation Loss:1.8892, Validation Accuracy:0.3514\n",
    "Epoch #263: Loss:1.7215, Accuracy:0.3996, Validation Loss:1.8986, Validation Accuracy:0.3498\n",
    "Epoch #264: Loss:1.7239, Accuracy:0.3996, Validation Loss:1.9032, Validation Accuracy:0.3448\n",
    "Epoch #265: Loss:1.7254, Accuracy:0.3959, Validation Loss:1.8990, Validation Accuracy:0.3498\n",
    "Epoch #266: Loss:1.7242, Accuracy:0.3926, Validation Loss:1.8900, Validation Accuracy:0.3481\n",
    "Epoch #267: Loss:1.7189, Accuracy:0.4016, Validation Loss:1.9010, Validation Accuracy:0.3399\n",
    "Epoch #268: Loss:1.7229, Accuracy:0.3984, Validation Loss:1.8920, Validation Accuracy:0.3547\n",
    "Epoch #269: Loss:1.7200, Accuracy:0.3922, Validation Loss:1.8886, Validation Accuracy:0.3399\n",
    "Epoch #270: Loss:1.7175, Accuracy:0.3996, Validation Loss:1.9000, Validation Accuracy:0.3465\n",
    "Epoch #271: Loss:1.7210, Accuracy:0.3943, Validation Loss:1.8914, Validation Accuracy:0.3514\n",
    "Epoch #272: Loss:1.7170, Accuracy:0.3967, Validation Loss:1.8916, Validation Accuracy:0.3514\n",
    "Epoch #273: Loss:1.7145, Accuracy:0.3951, Validation Loss:1.8963, Validation Accuracy:0.3481\n",
    "Epoch #274: Loss:1.7161, Accuracy:0.3926, Validation Loss:1.8930, Validation Accuracy:0.3465\n",
    "Epoch #275: Loss:1.7221, Accuracy:0.3988, Validation Loss:1.8931, Validation Accuracy:0.3481\n",
    "Epoch #276: Loss:1.7163, Accuracy:0.4057, Validation Loss:1.9083, Validation Accuracy:0.3366\n",
    "Epoch #277: Loss:1.7220, Accuracy:0.3910, Validation Loss:1.8857, Validation Accuracy:0.3465\n",
    "Epoch #278: Loss:1.7142, Accuracy:0.4057, Validation Loss:1.8996, Validation Accuracy:0.3596\n",
    "Epoch #279: Loss:1.7104, Accuracy:0.3951, Validation Loss:1.8895, Validation Accuracy:0.3498\n",
    "Epoch #280: Loss:1.7188, Accuracy:0.4000, Validation Loss:1.8954, Validation Accuracy:0.3530\n",
    "Epoch #281: Loss:1.7120, Accuracy:0.3988, Validation Loss:1.9245, Validation Accuracy:0.3415\n",
    "Epoch #282: Loss:1.7248, Accuracy:0.4062, Validation Loss:1.9038, Validation Accuracy:0.3514\n",
    "Epoch #283: Loss:1.7129, Accuracy:0.3971, Validation Loss:1.8857, Validation Accuracy:0.3514\n",
    "Epoch #284: Loss:1.7101, Accuracy:0.4000, Validation Loss:1.9034, Validation Accuracy:0.3530\n",
    "Epoch #285: Loss:1.7079, Accuracy:0.3967, Validation Loss:1.8858, Validation Accuracy:0.3530\n",
    "Epoch #286: Loss:1.7023, Accuracy:0.4016, Validation Loss:1.8893, Validation Accuracy:0.3514\n",
    "Epoch #287: Loss:1.7011, Accuracy:0.4041, Validation Loss:1.8857, Validation Accuracy:0.3481\n",
    "Epoch #288: Loss:1.7077, Accuracy:0.4025, Validation Loss:1.8937, Validation Accuracy:0.3580\n",
    "Epoch #289: Loss:1.7012, Accuracy:0.4025, Validation Loss:1.8828, Validation Accuracy:0.3563\n",
    "Epoch #290: Loss:1.7027, Accuracy:0.4062, Validation Loss:1.9024, Validation Accuracy:0.3415\n",
    "Epoch #291: Loss:1.7069, Accuracy:0.4045, Validation Loss:1.9021, Validation Accuracy:0.3465\n",
    "Epoch #292: Loss:1.7066, Accuracy:0.4021, Validation Loss:1.8798, Validation Accuracy:0.3530\n",
    "Epoch #293: Loss:1.7054, Accuracy:0.3984, Validation Loss:1.9059, Validation Accuracy:0.3514\n",
    "Epoch #294: Loss:1.7071, Accuracy:0.4008, Validation Loss:1.8856, Validation Accuracy:0.3465\n",
    "Epoch #295: Loss:1.7067, Accuracy:0.4099, Validation Loss:1.9010, Validation Accuracy:0.3448\n",
    "Epoch #296: Loss:1.6987, Accuracy:0.4008, Validation Loss:1.8900, Validation Accuracy:0.3415\n",
    "Epoch #297: Loss:1.6992, Accuracy:0.4082, Validation Loss:1.8871, Validation Accuracy:0.3498\n",
    "Epoch #298: Loss:1.6939, Accuracy:0.4078, Validation Loss:1.8962, Validation Accuracy:0.3399\n",
    "Epoch #299: Loss:1.6950, Accuracy:0.4090, Validation Loss:1.8917, Validation Accuracy:0.3465\n",
    "Epoch #300: Loss:1.6954, Accuracy:0.4062, Validation Loss:1.8884, Validation Accuracy:0.3498\n",
    "\n",
    "Test:\n",
    "Test Loss:1.88842452, Accuracy:0.3498\n",
    "Labels: ['ce', 'sg', 'eo', 'yd', 'my', 'eg', 'ib', 'ek', 'by', 'mb', 'aa', 'ck', 'eb', 'ds', 'sk']\n",
    "Confusion Matrix:\n",
    "      ce  sg  eo  yd  my  eg  ib  ek  by  mb  aa  ck  eb  ds  sk\n",
    "t:ce   0   1   0   3   0   2   2   7   6   3   0   0   3   0   0\n",
    "t:sg   0  21   0  11   0   0   2   3   7   5   0   0   2   0   0\n",
    "t:eo   0   3  18   0   0   4   0   0   6   1   2   0   0   0   0\n",
    "t:yd   0  14   0  26   0   2  11   3   3   2   0   0   1   0   0\n",
    "t:my   0   0   0   0   5   1   0   0   0   1   0   0  12   1   0\n",
    "t:eg   0   2   1   2   0  29   2   4   3   2   0   0   1   3   1\n",
    "t:ib   0   6   1  17   0   6  15   1   5   3   0   0   0   0   0\n",
    "t:ek   0   6   0   7   0   3   4  11   0   9   1   0   4   2   1\n",
    "t:by   0   7   3   0   0   0   8   0  11  10   0   0   0   1   0\n",
    "t:mb   0   2   4   1   0   3   1   3   5  29   1   0   2   1   0\n",
    "t:aa   0   0   0   1   0   7   0   1   1   3  18   0   2   1   0\n",
    "t:ck   0   2   0   1   0   2   1   5   0   4   0   0   7   0   1\n",
    "t:eb   0   0   0   1   2   3   0   5   0   5   3   0  28   1   2\n",
    "t:ds   0   0   0   0   0   2   0   8   2   1   6   0   9   2   1\n",
    "t:sk   0   0   0   0   3   1   0   4   0   4   1   0  17   3   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sg       0.33      0.41      0.37        51\n",
    "          eo       0.67      0.53      0.59        34\n",
    "          yd       0.37      0.42      0.39        62\n",
    "          my       0.50      0.25      0.33        20\n",
    "          eg       0.45      0.58      0.50        50\n",
    "          ib       0.33      0.28      0.30        54\n",
    "          ek       0.20      0.23      0.21        48\n",
    "          by       0.22      0.28      0.25        40\n",
    "          mb       0.35      0.56      0.43        52\n",
    "          aa       0.56      0.53      0.55        34\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eb       0.32      0.56      0.41        50\n",
    "          ds       0.13      0.06      0.09        31\n",
    "          sk       0.00      0.00      0.00        33\n",
    "\n",
    "    accuracy                           0.35       609\n",
    "   macro avg       0.30      0.31      0.29       609\n",
    "weighted avg       0.31      0.35      0.32       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 23:42:51 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 25 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7064414752527997, 2.697658901339877, 2.6905143609383617, 2.684473555663536, 2.6796517830176896, 2.6760222477278686, 2.6733116203145246, 2.67096660015814, 2.6689181069435155, 2.6670513806867677, 2.6654999933415056, 2.664095586548102, 2.6625635322483108, 2.660791473827143, 2.6585042030353265, 2.655232662050595, 2.6503379994816774, 2.6430776816087795, 2.632452646891276, 2.618459829164452, 2.6029125348296267, 2.587110717895583, 2.5738957683832577, 2.5638971606694616, 2.555392491602154, 2.547474219881255, 2.539619769956091, 2.5321162013193264, 2.525090266526822, 2.5170233073492945, 2.510333240521561, 2.50301140752332, 2.495447401538467, 2.4895299964741926, 2.4853844865789556, 2.47327635362622, 2.46423430591577, 2.454191824877008, 2.4464582942780995, 2.434669579191161, 2.425149874146936, 2.4167907570774725, 2.4059084150787253, 2.396873346103236, 2.3852785753303367, 2.3720775057725327, 2.365886191037684, 2.345872412369952, 2.3341238475198227, 2.3213236034405837, 2.309751424491895, 2.2930063199135664, 2.2733853270463364, 2.2611136957146654, 2.2565052434924397, 2.2519075154083703, 2.2234352973881615, 2.2170613544132127, 2.217800488417176, 2.208004116816278, 2.184174569369537, 2.1794392984293167, 2.1792177543264306, 2.167088295242861, 2.155994348338085, 2.148063327682821, 2.1419813214068735, 2.132821584961489, 2.125646326146494, 2.120396243723351, 2.133010907321924, 2.109158120523337, 2.108453202521664, 2.109870367253747, 2.1377522941489135, 2.127705098372962, 2.0854239201506566, 2.0934072253347815, 2.079987326670554, 2.0747308531418223, 2.078648362840925, 2.099465844666429, 2.0610918149180795, 2.0765879228588786, 2.05141406889228, 2.049633996240024, 2.043377464432239, 2.043834101390369, 2.0367336194894974, 2.0326220699523274, 2.030861471282633, 2.0366404123102697, 2.0244183258470057, 2.026228241145317, 2.0180142348623042, 2.020580212666679, 2.0095799666124416, 2.0174091492576163, 2.0168992071511904, 2.010402426931071, 2.0188649836040677, 2.0013338431153196, 2.0179592979756875, 2.013583321876714, 2.004997711072023, 1.9976250812142158, 2.0074464973362014, 1.9954378691017138, 1.9888985509355668, 2.002061046794522, 1.9843342938446646, 2.000736145354648, 1.9860480978766881, 1.9839131166586539, 1.9871356326762484, 1.975924376000716, 2.007589448653223, 1.9783879896298613, 1.9862150061502442, 1.9821424887489607, 1.9765430984434431, 1.9702596108510184, 1.9691128366686441, 1.968082265117877, 1.970138181802283, 1.9663086141075798, 1.9700848124493127, 1.9616471640582156, 1.9646823770306967, 1.9900779821994075, 1.9828256519361473, 1.9823995693563827, 1.9600398023727492, 1.9655336425417946, 1.9709603222720142, 1.9769106075681488, 1.9661383977273024, 1.9553960086089637, 1.9567069983834704, 1.9611555668716556, 1.9549587227049328, 1.9611182741343682, 1.9470398285118817, 1.9471200702617126, 1.9497278488328305, 1.9613268246204394, 1.9510159606025332, 1.9570830291127923, 2.002782994694702, 1.9569185684467185, 1.9674609083260222, 1.9392740589448776, 1.9505730121593756, 1.9407993981795162, 1.9407270242427956, 1.973713106709748, 1.9622994085838055, 1.9362703843657019, 1.9374350023582847, 1.9367231958605386, 1.9339011377105964, 1.93184177139514, 1.936031255424512, 1.9452326303632388, 1.9496069526045976, 1.9312420901406575, 1.9357952621378531, 1.9277977974739764, 1.9271036983515046, 1.9318908100644943, 1.9214748230278003, 1.9394329122721856, 1.9233878287188526, 1.9364141940287574, 1.9372725228370704, 1.919193504870623, 1.9268197815602244, 1.9223642319881271, 1.9191876567643265, 1.9342013920469237, 1.9188721185834536, 1.9283878955935023, 1.9254762801434997, 1.9231038436122325, 1.9280361242482227, 1.9217635269822746, 1.9324855244609913, 1.9183017410863992, 1.916503325275991, 1.917700815866342, 1.9160076144880849, 1.917267093713256, 1.9312267129252894, 1.9100948818798722, 1.9299035250455483, 1.9353567853154023, 1.914156874999624, 1.9207155493092654, 1.9272005360310496, 1.90810166928177, 1.926682373964532, 1.9033136739715175, 1.924403853408613, 1.905454422061275, 1.9268194746305594, 1.917917890305981, 1.911603760053763, 1.9100852476552201, 1.9199770645946508, 1.9068127205023429, 1.9224847463159922, 1.9030977039305839, 1.9072662509720901, 1.9056814638851898, 1.9048244618429926, 1.912181565914248, 1.903403571869548, 1.9033146483949057, 1.9051642449227069, 1.9083382445211676, 1.906670512432731, 1.9066249255476326, 1.9035406513950117, 1.9342452369887253, 1.919196186786019, 1.9126521930318747, 1.9131012679320838, 1.9067375295855142, 1.9077044703886035, 1.9027613832054076, 1.9159331926571324, 1.9036091943875517, 1.9133216080015711, 1.9056941437212314, 1.9100771196957291, 1.9297251785525744, 1.8984431202579992, 1.9106624619714145, 1.9097230737823963, 1.9106634812206273, 1.9038200623100419, 1.9104070647792473, 1.8999432754046812, 1.9060658200816765, 1.8992003364907502, 1.9050845191592263, 1.9045251014784639, 1.902214492678838, 1.903104080550972, 1.8997317376395164, 1.8937369459759816, 1.902479141412306, 1.8992097689013177, 1.894633647060551, 1.903788831825131, 1.897549827697829, 1.8997544212685822, 1.8967288436952288, 1.9041500285341235, 1.8922588764544583, 1.9009907695851693, 1.889212805062092, 1.8986120815151821, 1.9032365943019223, 1.8989563965053589, 1.8900240610777255, 1.901000342737082, 1.892028666482183, 1.8885627785339731, 1.9000340570957202, 1.8913652299855925, 1.8916223585507748, 1.8963428193516723, 1.8930184447706626, 1.8931337577368825, 1.908287003318273, 1.8856772595438465, 1.8995710691599228, 1.8894743966351588, 1.8954091122976469, 1.9244941559135424, 1.903798876137569, 1.8857024700575078, 1.9034431900688384, 1.8858175982395415, 1.8892570448234947, 1.8856763996318449, 1.8936865797575275, 1.8827968645957107, 1.9023927643968555, 1.9021275464341363, 1.8798297422468564, 1.9058781652810735, 1.885574123346551, 1.9009860626778188, 1.889978685206772, 1.8871200828520927, 1.8961718587452554, 1.8917496413824397, 1.8884247054020171], 'val_acc': [0.083743841263461, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.09031198685759394, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10180623933354818, 0.10509031158313767, 0.12479474527642058, 0.14285714175607184, 0.13957307049744627, 0.14449917877395752, 0.1395730703017003, 0.14942528714834175, 0.14449917877395752, 0.14285714264916277, 0.16091954021765092, 0.18226600894689168, 0.17569786434983972, 0.1904761895708654, 0.18062397311571587, 0.17898193679517518, 0.1954022981409956, 0.17569786474133164, 0.2003284067111258, 0.18719211761489485, 0.20197044283592056, 0.1822660093383836, 0.211822659780435, 0.20197044283592056, 0.19868637068420403, 0.2085385875308455, 0.20689655130817777, 0.19540229833674158, 0.21182265958468902, 0.19540229843461454, 0.20853858733509953, 0.20689655130817777, 0.20853858743297252, 0.2200328404044087, 0.23152709308222597, 0.23316912940276668, 0.22988505695743122, 0.23152709327797194, 0.22003284020866276, 0.23481116542968844, 0.23973727390194566, 0.24302134615153514, 0.2528735629981766, 0.25123152697125484, 0.22988505695743122, 0.26272577994269103, 0.2610837438178963, 0.2594417073994826, 0.2692939222641962, 0.2791461392087106, 0.2725779966914595, 0.27257799441591274, 0.2840722477788408, 0.27750410298604294, 0.2758620667633752, 0.2791461391108377, 0.2660098521922805, 0.26765188623727443, 0.27257799470953165, 0.2742200305407075, 0.28899835615322506, 0.28078817523563243, 0.2791461389150917, 0.27750410279029697, 0.2742200307364534, 0.28571428361001666, 0.29885057290199357, 0.29228242840281454, 0.2791461393065836, 0.29720853667932584, 0.29720853677719883, 0.28899835615322506, 0.2873563198326844, 0.28571428390363557, 0.28899835595747914, 0.3037766814721237, 0.3037766813742508, 0.2906403923758928, 0.3087027900422539, 0.3037766814721237, 0.29720853667932584, 0.29720853658145285, 0.2988505728041206, 0.308702789748635, 0.31527093444355997, 0.2955665003587851, 0.29720853648357987, 0.30049260873316935, 0.3119868619982245, 0.30377668098275884, 0.3119868619003515, 0.3169129705683547, 0.31691297066622764, 0.32676518751286915, 0.3185550067910224, 0.3234811150675337, 0.3234811151654067, 0.31691297037260874, 0.32676518741499616, 0.3037766812763778, 0.32512315138807435, 0.3152709345414329, 0.31362889812301925, 0.3234811150675337, 0.32676518751286915, 0.33004925976245864, 0.32183907894273894, 0.3234811150675337, 0.33004925976245864, 0.3234811149696607, 0.33497536823471585, 0.33333333181630215, 0.3070607536238403, 0.31034482587342976, 0.3037766814721237, 0.3284072235397909, 0.33333333191417513, 0.3333333320120481, 0.32840722334404493, 0.33825944038643235, 0.3234811149696607, 0.3201970429158172, 0.33169129598512637, 0.33497536813684287, 0.33004925966458565, 0.3284072236376639, 0.3284072234419179, 0.3316912958872534, 0.3333333320120481, 0.34154351263602184, 0.31855500669314946, 0.3136288983187652, 0.31855500659527647, 0.3234811151654067, 0.33004925986033157, 0.3431855487608166, 0.3284072235397909, 0.33497536823471585, 0.3169129705683547, 0.31855500669314946, 0.32512315138807435, 0.3284072234419179, 0.33825944028855937, 0.33333333181630215, 0.3399014764133541, 0.3382594400928134, 0.3201970429158172, 0.3267651873171232, 0.3267651873171232, 0.3300492594688397, 0.32840722334404493, 0.33825944028855937, 0.3349753680389699, 0.3399014763154811, 0.33661740396801865, 0.34482758478773834, 0.3431855484671976, 0.33497536813684287, 0.3382594400928134, 0.3300492594688397, 0.3431855485650706, 0.3300492594688397, 0.3366174042616376, 0.3366174041637646, 0.3349753679410969, 0.34154351214665696, 0.3464696209125331, 0.33825944028855937, 0.33004925956671266, 0.33333333191417513, 0.33661740387014566, 0.3366174041637646, 0.3415435122445299, 0.3415435123424029, 0.3316912958872534, 0.3234811151654067, 0.3349753680389699, 0.3481116567437089, 0.3382594398970674, 0.33497536813684287, 0.3349753679410969, 0.3366174042616376, 0.33661740396801865, 0.34811165654796294, 0.33661740396801865, 0.3431855487608166, 0.3399014763154811, 0.34482758459199236, 0.33990147602186216, 0.3481116567437089, 0.34318554836932463, 0.3399014764133541, 0.3399014763154811, 0.34154351214665696, 0.3431855484671976, 0.3399014763154811, 0.3464696208146601, 0.3399014763154811, 0.34154351244027586, 0.3431855484671976, 0.3431855482714517, 0.34482758478773834, 0.34811165654796294, 0.3513957288954254, 0.3464696208146601, 0.3399014763154811, 0.3333333320120481, 0.3399014765112271, 0.3464696208146601, 0.3497536929663766, 0.3382594398970674, 0.34811165684158185, 0.3431855485650706, 0.34811165684158185, 0.34482758478773834, 0.35139572909117134, 0.34318554836932463, 0.35139572909117134, 0.33990147621760813, 0.3415435123424029, 0.3513957292869173, 0.34975369286850366, 0.35303776531383907, 0.3481116573309468, 0.3431855486629436, 0.34154351214665696, 0.3546798014386338, 0.34646962110827906, 0.3546798014386338, 0.34482758439624644, 0.34811165693945484, 0.34154351244027586, 0.35303776531383907, 0.3431855485650706, 0.35303776521596614, 0.34811165684158185, 0.3497536930642496, 0.34482758468986535, 0.34154351244027586, 0.3513957291890443, 0.3431855484671976, 0.34154351244027586, 0.3513957291890443, 0.34482758468986535, 0.3513957292869173, 0.34975369316212257, 0.3448275849834843, 0.34975369316212257, 0.3481116567437089, 0.33990147621760813, 0.3546798014386338, 0.3399014764133541, 0.3464696208146601, 0.35139572909117134, 0.3513957292869173, 0.34811165693945484, 0.3464696208146601, 0.3481116570373278, 0.33661740406589163, 0.34646962101040607, 0.35960590991089103, 0.34975369325999556, 0.35303776550958504, 0.34154351302751373, 0.35139572909117134, 0.3513957292869173, 0.35303776531383907, 0.35303776541171206, 0.3513957292869173, 0.3481116570373278, 0.3579638737860963, 0.35632183766130154, 0.34154351244027586, 0.34646962101040607, 0.35303776560745803, 0.3513957292869173, 0.3464696209125331, 0.34482758478773834, 0.3415435122445299, 0.3497536930642496, 0.3399014765112271, 0.34646962110827906, 0.34975369325999556], 'loss': [2.712951616389061, 2.7028383614346234, 2.694862776957988, 2.688291342204601, 2.682469897005837, 2.6787054581808603, 2.6750165973600666, 2.6725730830394268, 2.6701308214199373, 2.668146407481826, 2.6665732535493447, 2.6650476780760215, 2.663469010848529, 2.662012811069371, 2.659715983608175, 2.657089384282639, 2.6527202448560963, 2.647140832505432, 2.638040325626945, 2.6247740151211465, 2.608251634417618, 2.5917273331471784, 2.5743116818414333, 2.5590132427411403, 2.5484108853389107, 2.5371839234471567, 2.5258160559793272, 2.5176685760153394, 2.5086005872769523, 2.4973978839615776, 2.489994404986654, 2.4810908686698583, 2.473988579430864, 2.4648165444329044, 2.459644163413704, 2.451910591908794, 2.441320531520021, 2.4270594678130726, 2.4187964531430475, 2.409366417910284, 2.400063272570193, 2.389970567436923, 2.3800459307566806, 2.370784154026415, 2.3573357958078875, 2.3480154053141695, 2.3342178711900967, 2.3203836028825577, 2.3023318283856526, 2.2898211291193715, 2.2746526316695634, 2.256914588900807, 2.23830678164347, 2.2203715431127216, 2.2055115225868303, 2.196999313209581, 2.1861122714910173, 2.167362862103284, 2.161504318679872, 2.157029540338066, 2.157495442063412, 2.134737978529881, 2.1232194910304014, 2.115693902088142, 2.104576790651012, 2.1006270264698004, 2.090216767861368, 2.0815683504884004, 2.0761098800988167, 2.0667996115753047, 2.060003915019104, 2.064923424593477, 2.0519323742609985, 2.046231248099701, 2.0451875060001194, 2.0537190179805247, 2.0520666336866373, 2.0279218784347943, 2.023938724882059, 2.0236317766275738, 2.0059821662472013, 2.0061062411850727, 2.010142205921776, 1.9983265156129058, 1.9916896439186111, 1.98303291352133, 1.9764142585730895, 1.9758829558409705, 1.9692150947249645, 1.9615138789221982, 1.9583936191437425, 1.9561884608356859, 1.9517754758407937, 1.9473736636202927, 1.9416495898421051, 1.9363928985301964, 1.9402322723145846, 1.931698876778448, 1.930782963265139, 1.9372608925527617, 1.9530682434046782, 1.9555936460867065, 1.9452941840679003, 1.9264854514378542, 1.9130543259869366, 1.9132055088235123, 1.9089271407352582, 1.9118587007757575, 1.9191577220109943, 1.9217576624186867, 1.9099337497531022, 1.9015089930450157, 1.9063210919652387, 1.8987137759735453, 1.8946050100502783, 1.8943737818719915, 1.8917204839737753, 1.89037818340795, 1.886112103325141, 1.8887104993972934, 1.8948341524331722, 1.8811492126825164, 1.878408718451827, 1.876052301963007, 1.8751437462330844, 1.8739218905231547, 1.86922238940331, 1.8726012243627277, 1.870627205425709, 1.871899210745794, 1.8730269794346615, 1.8712391282498715, 1.8652807650869632, 1.8726103601514437, 1.8781109351641834, 1.8803224322242658, 1.867108769877001, 1.8659487488823014, 1.8659364399234373, 1.8560421749796465, 1.8519587910395627, 1.852878707684041, 1.8519096942408129, 1.8463018526531587, 1.8475748541418777, 1.8492547376444697, 1.8650722487506435, 1.8613471511453084, 1.8572672156093057, 1.8562294869941853, 1.853372978038122, 1.8462373271370325, 1.8436028419823618, 1.83954420917088, 1.8347840678275733, 1.8401757465985276, 1.8436439464224437, 1.8358094112584233, 1.8284466139834519, 1.8234714081644767, 1.8231668638742435, 1.8239850430028395, 1.8226788520812989, 1.8303663748245709, 1.8246486034236649, 1.8300144423204772, 1.8234980745726788, 1.8169945093640556, 1.8165587168699417, 1.812002365202385, 1.8109096739081632, 1.8128719683790109, 1.8126912496417944, 1.8181975721089012, 1.8169100878419817, 1.8032326055503234, 1.8038954462114056, 1.8043339254919752, 1.8076446369198558, 1.8061190025028018, 1.800445649119618, 1.8048022427842847, 1.803702907199977, 1.8084953388883838, 1.7997087055163217, 1.797393075592464, 1.7975289213094379, 1.7994628568449549, 1.7910897352367456, 1.791720707607465, 1.7946173338429883, 1.7897819863697342, 1.7890283565991223, 1.7987102604989398, 1.7951631513219595, 1.8006660436457922, 1.8044919337580092, 1.7918866598630587, 1.7854978137437323, 1.7905512282490974, 1.7843569735971565, 1.780861262521215, 1.781698016464343, 1.7764499995742735, 1.7807907033505137, 1.7816730275535975, 1.7834314304700378, 1.777342015076467, 1.7728377188990003, 1.772975014661127, 1.7750218637180524, 1.773861740991565, 1.7683814665130522, 1.7616490109500453, 1.764520628251579, 1.7622571653409171, 1.7610763118497155, 1.761952986805346, 1.7648218498092902, 1.7621465693753848, 1.7700843334197998, 1.7624021169830886, 1.7551472865090967, 1.7629838221861351, 1.767994917981189, 1.7695190395907454, 1.7660772448447695, 1.7555299845564292, 1.7585296109471722, 1.752360816706867, 1.7482679083606791, 1.7577876546789244, 1.756075709017885, 1.7508286921640197, 1.7601990067738527, 1.7608329991295597, 1.7670634723052352, 1.7547961331001298, 1.7527173234207185, 1.7630394674179735, 1.7565463988932741, 1.7469093968001723, 1.7507129399928225, 1.7436590047832388, 1.7395820119297725, 1.739316914703322, 1.7418340719211274, 1.7421653751475121, 1.7354934807920357, 1.733122086329137, 1.7333578512653922, 1.7312700196702866, 1.7343819345536908, 1.7328734294100219, 1.731575072129894, 1.7309709395716077, 1.7318528848996642, 1.7306331351062845, 1.7254990515033322, 1.72613292234879, 1.7237299867234435, 1.7207967376806899, 1.7214675268842945, 1.7238741428700315, 1.7254270641221159, 1.7241784122200716, 1.7188545270622144, 1.7229166314831994, 1.7199517744033, 1.71746445583367, 1.7209820820320803, 1.7170434654126177, 1.7145197432143977, 1.7161116684731517, 1.7221192656601234, 1.7163264739195179, 1.72197419010883, 1.714194641906378, 1.7103605152889934, 1.7188484814622318, 1.711998407062319, 1.7248110076974794, 1.7128951081994623, 1.7101284542612472, 1.7079122382757355, 1.7023264992653222, 1.7010999791186447, 1.7076986259993097, 1.7012058931209713, 1.702725727602197, 1.7069356441008237, 1.7066316447463614, 1.705353269537861, 1.7071362841055868, 1.7066992610876566, 1.6986952804197275, 1.6992277966387708, 1.6938524946295015, 1.6950442956458371, 1.6953619457613027], 'acc': [0.08295687860837953, 0.08911704345772643, 0.08911704365355279, 0.0891170430752531, 0.08911704366273214, 0.08911704288860611, 0.08911704365355279, 0.08911704267442104, 0.0891170434669058, 0.08911704266524167, 0.0895277204883172, 0.10102669438175108, 0.10225872663930212, 0.10225872683512846, 0.10225872723596052, 0.10225872725431924, 0.10225872703095482, 0.10225872664848147, 0.10349075928850585, 0.1170431208806361, 0.1334702267783868, 0.14209445576036245, 0.14784394143420812, 0.14537987768405272, 0.1416837775548136, 0.14661191075244723, 0.1474332638161383, 0.14948665251232515, 0.16714578948348943, 0.170431211015527, 0.1737166335083376, 0.1827515401267418, 0.1815195062934006, 0.1819301858697339, 0.19712525576169485, 0.195071869647968, 0.19835728842007797, 0.2049281307008477, 0.21314168404137573, 0.20492813208999086, 0.21519507113423436, 0.21683778279983043, 0.21190965059968725, 0.22012320217777817, 0.2254620129751229, 0.22135523503198762, 0.2336755641615611, 0.2287474331180173, 0.23901437298228365, 0.2340862413696195, 0.24312115055212494, 0.24271047134908563, 0.24558521665219654, 0.25503080184944354, 0.26036961166765654, 0.25831622142321764, 0.25297741299414783, 0.25872689880874367, 0.26160164354273424, 0.2665297761345301, 0.2698151937133233, 0.27227926200420216, 0.27926077834145, 0.28049281217479116, 0.2874743324652834, 0.2837782345268516, 0.2874743330894799, 0.2911704334145454, 0.2932238173558237, 0.29650923967116666, 0.2973305967064609, 0.2956878830642426, 0.2997946624148798, 0.3043121164094741, 0.300205338233795, 0.3047227937950001, 0.3010266919033239, 0.30924024857289983, 0.3071868589526574, 0.306365504071453, 0.3133470224036818, 0.3096509248201852, 0.30841889196597577, 0.30554414668122354, 0.31498973385509277, 0.32073921952893847, 0.3158110894828851, 0.3125256657967577, 0.31950718843716613, 0.3219712547330641, 0.3227926082067666, 0.3219712517589514, 0.32032854308582676, 0.3219712523831479, 0.3203285432816531, 0.3289527736527719, 0.32731006040220634, 0.3260780287596724, 0.3264887077852441, 0.32279260801094023, 0.3195071856955973, 0.3207392212913756, 0.3170431203788311, 0.32648870794435303, 0.3289527744360773, 0.33470226050157564, 0.3281314174375005, 0.326899383408333, 0.33305954944181737, 0.3264887057902632, 0.32936345048753635, 0.33511293534135916, 0.3400410669540233, 0.3342915822960268, 0.3363449699947232, 0.340041069928136, 0.3425051346574231, 0.33429158265096204, 0.34373716633667445, 0.3433264865277974, 0.33963039035180265, 0.33675564463868035, 0.34291581090470846, 0.3470225894353228, 0.3425051350490758, 0.34455852036113854, 0.3408624239893175, 0.34579055658111335, 0.34784394130569707, 0.3441478453255287, 0.33757700402389074, 0.3523613978827514, 0.348665298732644, 0.3494866507988446, 0.3412730982783394, 0.3412730998082327, 0.3544147841739459, 0.3515400420591327, 0.347843943300678, 0.3548254617920157, 0.35154004186330634, 0.3527720749133421, 0.35318275370637003, 0.35236139866605676, 0.35277207507245106, 0.3609856274338474, 0.3478439440839834, 0.35154004147165363, 0.3540041067517024, 0.35975359281720076, 0.3535934275670218, 0.35523614179671914, 0.3535934311286135, 0.3531827533147173, 0.3589322401635211, 0.36303901395758564, 0.3568788508614965, 0.3556468184356572, 0.3597535932088535, 0.3613963062635927, 0.3691991792077646, 0.36180698349000984, 0.3671457901382838, 0.3589322391843894, 0.35934291578661, 0.3655030779035674, 0.36344969219985196, 0.3638603686062462, 0.3605749490691896, 0.3650924024395874, 0.36386036743128813, 0.3679671449827707, 0.3675564701062698, 0.36221766087553586, 0.36098562704219467, 0.37084189007169654, 0.37207392273007966, 0.3683778249507567, 0.36714579033411016, 0.3737166310482691, 0.36591375751661814, 0.37330595261017646, 0.370020535423036, 0.3646817244298649, 0.37330595460515736, 0.36878850217717385, 0.3737166318315745, 0.3761806991066042, 0.3696098542800919, 0.37535934445794356, 0.3712525657315029, 0.377412728986701, 0.3737166332023589, 0.3712525680814191, 0.3794661186803783, 0.3778234078164463, 0.3712525665148083, 0.37330595617176815, 0.3794661188762046, 0.3700205346397306, 0.38275153942911044, 0.3753593411288957, 0.37700205218865396, 0.37494866683987377, 0.3765913749622368, 0.3819301853679289, 0.3704312093204052, 0.3794661184845519, 0.3806981540803302, 0.37412731046549347, 0.37700205414691745, 0.375770018710248, 0.38562628271888166, 0.3835728970151662, 0.38275153805832596, 0.38193018556375524, 0.385626284909689, 0.3790554430205719, 0.3843942516638268, 0.38439425264295857, 0.38603695919871084, 0.383983574045757, 0.38562628530134163, 0.3819301867387133, 0.37905543965480654, 0.38357289584020815, 0.3802874748956496, 0.3880903472523425, 0.38480492693198043, 0.38809034744816884, 0.390554412764935, 0.3819301835687743, 0.38932238112240114, 0.3848049290860703, 0.38932238327649094, 0.3786447622692805, 0.38562628314725184, 0.3889117054625948, 0.3913757721501454, 0.3868583173722457, 0.38644764034165496, 0.39178644898490983, 0.3946611887498068, 0.38726899596944725, 0.3868583168214841, 0.39137576980022926, 0.3893223810856837, 0.393839833746211, 0.39178644976821525, 0.39342916043632087, 0.3860369633110642, 0.3938398345295164, 0.3909650907746575, 0.3950718687545103, 0.40000000017134807, 0.38562628412638356, 0.39835728907487233, 0.3971252589989492, 0.40041067974768135, 0.39301848223077196, 0.39876796751296495, 0.39712525821564376, 0.39958932137832015, 0.39958932392406266, 0.3958932239906499, 0.3926078010511105, 0.4016427098603219, 0.39835729106985324, 0.39219712421634606, 0.39958932392406266, 0.3942505141058497, 0.39671457981426855, 0.3950718707127738, 0.39260780206695967, 0.3987679653221577, 0.40574948799928356, 0.3909650931245737, 0.4057494846335182, 0.39507186636787667, 0.39999999798054076, 0.398767964930505, 0.40616016323071974, 0.39712525622066286, 0.4000000015421325, 0.3967145770359823, 0.401642710447801, 0.4041067739654126, 0.402464066467246, 0.40246406803385676, 0.4061601630348934, 0.40451745534090044, 0.4020533894366552, 0.3983572873124352, 0.40082135677827213, 0.40985626316413254, 0.4008213559949667, 0.40821355429518147, 0.40780287295641104, 0.4090349095313211, 0.4061601664006588]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
