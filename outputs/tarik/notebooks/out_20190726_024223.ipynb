{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf14.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 02:42:23 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '0', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '03', '02', '01', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000027D0715BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000027D6CB46EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6069, Accuracy:0.2283, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6022, Accuracy:0.2333, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #17: Loss:1.6011, Accuracy:0.2345, Validation Loss:1.5979, Validation Accuracy:0.2414\n",
    "Epoch #18: Loss:1.5989, Accuracy:0.2435, Validation Loss:1.5946, Validation Accuracy:0.2430\n",
    "Epoch #19: Loss:1.5958, Accuracy:0.2435, Validation Loss:1.5901, Validation Accuracy:0.2611\n",
    "Epoch #20: Loss:1.5926, Accuracy:0.2456, Validation Loss:1.5848, Validation Accuracy:0.2611\n",
    "Epoch #21: Loss:1.5894, Accuracy:0.2456, Validation Loss:1.5800, Validation Accuracy:0.2611\n",
    "Epoch #22: Loss:1.5882, Accuracy:0.2501, Validation Loss:1.5767, Validation Accuracy:0.2627\n",
    "Epoch #23: Loss:1.5865, Accuracy:0.2489, Validation Loss:1.5746, Validation Accuracy:0.2660\n",
    "Epoch #24: Loss:1.5851, Accuracy:0.2485, Validation Loss:1.5739, Validation Accuracy:0.2677\n",
    "Epoch #25: Loss:1.5850, Accuracy:0.2538, Validation Loss:1.5724, Validation Accuracy:0.2660\n",
    "Epoch #26: Loss:1.5835, Accuracy:0.2501, Validation Loss:1.5712, Validation Accuracy:0.2660\n",
    "Epoch #27: Loss:1.5826, Accuracy:0.2493, Validation Loss:1.5706, Validation Accuracy:0.2677\n",
    "Epoch #28: Loss:1.5826, Accuracy:0.2509, Validation Loss:1.5703, Validation Accuracy:0.2709\n",
    "Epoch #29: Loss:1.5817, Accuracy:0.2526, Validation Loss:1.5705, Validation Accuracy:0.2709\n",
    "Epoch #30: Loss:1.5807, Accuracy:0.2534, Validation Loss:1.5685, Validation Accuracy:0.2660\n",
    "Epoch #31: Loss:1.5808, Accuracy:0.2522, Validation Loss:1.5683, Validation Accuracy:0.2693\n",
    "Epoch #32: Loss:1.5800, Accuracy:0.2526, Validation Loss:1.5680, Validation Accuracy:0.2709\n",
    "Epoch #33: Loss:1.5798, Accuracy:0.2554, Validation Loss:1.5696, Validation Accuracy:0.2693\n",
    "Epoch #34: Loss:1.5795, Accuracy:0.2559, Validation Loss:1.5664, Validation Accuracy:0.2677\n",
    "Epoch #35: Loss:1.5784, Accuracy:0.2567, Validation Loss:1.5683, Validation Accuracy:0.2742\n",
    "Epoch #36: Loss:1.5781, Accuracy:0.2579, Validation Loss:1.5681, Validation Accuracy:0.2791\n",
    "Epoch #37: Loss:1.5774, Accuracy:0.2550, Validation Loss:1.5658, Validation Accuracy:0.2742\n",
    "Epoch #38: Loss:1.5771, Accuracy:0.2550, Validation Loss:1.5683, Validation Accuracy:0.2660\n",
    "Epoch #39: Loss:1.5769, Accuracy:0.2604, Validation Loss:1.5662, Validation Accuracy:0.2775\n",
    "Epoch #40: Loss:1.5764, Accuracy:0.2567, Validation Loss:1.5667, Validation Accuracy:0.2775\n",
    "Epoch #41: Loss:1.5753, Accuracy:0.2563, Validation Loss:1.5671, Validation Accuracy:0.2742\n",
    "Epoch #42: Loss:1.5761, Accuracy:0.2579, Validation Loss:1.5661, Validation Accuracy:0.2545\n",
    "Epoch #43: Loss:1.5756, Accuracy:0.2563, Validation Loss:1.5701, Validation Accuracy:0.2545\n",
    "Epoch #44: Loss:1.5749, Accuracy:0.2489, Validation Loss:1.5640, Validation Accuracy:0.2627\n",
    "Epoch #45: Loss:1.5731, Accuracy:0.2534, Validation Loss:1.5693, Validation Accuracy:0.2627\n",
    "Epoch #46: Loss:1.5753, Accuracy:0.2534, Validation Loss:1.5671, Validation Accuracy:0.2578\n",
    "Epoch #47: Loss:1.5743, Accuracy:0.2546, Validation Loss:1.5643, Validation Accuracy:0.2562\n",
    "Epoch #48: Loss:1.5722, Accuracy:0.2559, Validation Loss:1.5688, Validation Accuracy:0.2512\n",
    "Epoch #49: Loss:1.5716, Accuracy:0.2604, Validation Loss:1.5654, Validation Accuracy:0.2693\n",
    "Epoch #50: Loss:1.5708, Accuracy:0.2571, Validation Loss:1.5670, Validation Accuracy:0.2529\n",
    "Epoch #51: Loss:1.5703, Accuracy:0.2538, Validation Loss:1.5650, Validation Accuracy:0.2644\n",
    "Epoch #52: Loss:1.5697, Accuracy:0.2563, Validation Loss:1.5654, Validation Accuracy:0.2578\n",
    "Epoch #53: Loss:1.5694, Accuracy:0.2604, Validation Loss:1.5676, Validation Accuracy:0.2529\n",
    "Epoch #54: Loss:1.5684, Accuracy:0.2575, Validation Loss:1.5653, Validation Accuracy:0.2496\n",
    "Epoch #55: Loss:1.5679, Accuracy:0.2534, Validation Loss:1.5640, Validation Accuracy:0.2529\n",
    "Epoch #56: Loss:1.5678, Accuracy:0.2579, Validation Loss:1.5659, Validation Accuracy:0.2545\n",
    "Epoch #57: Loss:1.5667, Accuracy:0.2595, Validation Loss:1.5652, Validation Accuracy:0.2611\n",
    "Epoch #58: Loss:1.5664, Accuracy:0.2534, Validation Loss:1.5649, Validation Accuracy:0.2479\n",
    "Epoch #59: Loss:1.5660, Accuracy:0.2546, Validation Loss:1.5655, Validation Accuracy:0.2529\n",
    "Epoch #60: Loss:1.5651, Accuracy:0.2522, Validation Loss:1.5638, Validation Accuracy:0.2562\n",
    "Epoch #61: Loss:1.5642, Accuracy:0.2522, Validation Loss:1.5669, Validation Accuracy:0.2430\n",
    "Epoch #62: Loss:1.5644, Accuracy:0.2517, Validation Loss:1.5633, Validation Accuracy:0.2463\n",
    "Epoch #63: Loss:1.5631, Accuracy:0.2530, Validation Loss:1.5640, Validation Accuracy:0.2545\n",
    "Epoch #64: Loss:1.5628, Accuracy:0.2575, Validation Loss:1.5642, Validation Accuracy:0.2479\n",
    "Epoch #65: Loss:1.5617, Accuracy:0.2554, Validation Loss:1.5621, Validation Accuracy:0.2512\n",
    "Epoch #66: Loss:1.5625, Accuracy:0.2480, Validation Loss:1.5638, Validation Accuracy:0.2479\n",
    "Epoch #67: Loss:1.5607, Accuracy:0.2513, Validation Loss:1.5625, Validation Accuracy:0.2594\n",
    "Epoch #68: Loss:1.5599, Accuracy:0.2546, Validation Loss:1.5636, Validation Accuracy:0.2496\n",
    "Epoch #69: Loss:1.5599, Accuracy:0.2517, Validation Loss:1.5624, Validation Accuracy:0.2578\n",
    "Epoch #70: Loss:1.5581, Accuracy:0.2595, Validation Loss:1.5619, Validation Accuracy:0.2463\n",
    "Epoch #71: Loss:1.5573, Accuracy:0.2513, Validation Loss:1.5611, Validation Accuracy:0.2562\n",
    "Epoch #72: Loss:1.5563, Accuracy:0.2493, Validation Loss:1.5603, Validation Accuracy:0.2529\n",
    "Epoch #73: Loss:1.5557, Accuracy:0.2505, Validation Loss:1.5615, Validation Accuracy:0.2545\n",
    "Epoch #74: Loss:1.5579, Accuracy:0.2583, Validation Loss:1.5611, Validation Accuracy:0.2578\n",
    "Epoch #75: Loss:1.5575, Accuracy:0.2538, Validation Loss:1.5601, Validation Accuracy:0.2627\n",
    "Epoch #76: Loss:1.5533, Accuracy:0.2550, Validation Loss:1.5600, Validation Accuracy:0.2611\n",
    "Epoch #77: Loss:1.5541, Accuracy:0.2530, Validation Loss:1.5587, Validation Accuracy:0.2562\n",
    "Epoch #78: Loss:1.5521, Accuracy:0.2509, Validation Loss:1.5592, Validation Accuracy:0.2611\n",
    "Epoch #79: Loss:1.5537, Accuracy:0.2641, Validation Loss:1.5596, Validation Accuracy:0.2611\n",
    "Epoch #80: Loss:1.5540, Accuracy:0.2571, Validation Loss:1.5627, Validation Accuracy:0.2512\n",
    "Epoch #81: Loss:1.5543, Accuracy:0.2632, Validation Loss:1.5592, Validation Accuracy:0.2463\n",
    "Epoch #82: Loss:1.5511, Accuracy:0.2554, Validation Loss:1.5581, Validation Accuracy:0.2578\n",
    "Epoch #83: Loss:1.5481, Accuracy:0.2571, Validation Loss:1.5575, Validation Accuracy:0.2594\n",
    "Epoch #84: Loss:1.5469, Accuracy:0.2608, Validation Loss:1.5574, Validation Accuracy:0.2594\n",
    "Epoch #85: Loss:1.5465, Accuracy:0.2637, Validation Loss:1.5583, Validation Accuracy:0.2545\n",
    "Epoch #86: Loss:1.5466, Accuracy:0.2620, Validation Loss:1.5588, Validation Accuracy:0.2578\n",
    "Epoch #87: Loss:1.5479, Accuracy:0.2612, Validation Loss:1.5665, Validation Accuracy:0.2397\n",
    "Epoch #88: Loss:1.5465, Accuracy:0.2604, Validation Loss:1.5620, Validation Accuracy:0.2496\n",
    "Epoch #89: Loss:1.5488, Accuracy:0.2653, Validation Loss:1.5613, Validation Accuracy:0.2562\n",
    "Epoch #90: Loss:1.5451, Accuracy:0.2698, Validation Loss:1.5583, Validation Accuracy:0.2562\n",
    "Epoch #91: Loss:1.5448, Accuracy:0.2632, Validation Loss:1.5580, Validation Accuracy:0.2709\n",
    "Epoch #92: Loss:1.5435, Accuracy:0.2682, Validation Loss:1.5591, Validation Accuracy:0.2611\n",
    "Epoch #93: Loss:1.5426, Accuracy:0.2731, Validation Loss:1.5587, Validation Accuracy:0.2677\n",
    "Epoch #94: Loss:1.5462, Accuracy:0.2706, Validation Loss:1.5631, Validation Accuracy:0.2430\n",
    "Epoch #95: Loss:1.5440, Accuracy:0.2735, Validation Loss:1.5596, Validation Accuracy:0.2578\n",
    "Epoch #96: Loss:1.5453, Accuracy:0.2772, Validation Loss:1.5576, Validation Accuracy:0.2627\n",
    "Epoch #97: Loss:1.5421, Accuracy:0.2702, Validation Loss:1.5583, Validation Accuracy:0.2693\n",
    "Epoch #98: Loss:1.5424, Accuracy:0.2747, Validation Loss:1.5583, Validation Accuracy:0.2726\n",
    "Epoch #99: Loss:1.5424, Accuracy:0.2768, Validation Loss:1.5588, Validation Accuracy:0.2594\n",
    "Epoch #100: Loss:1.5414, Accuracy:0.2764, Validation Loss:1.5569, Validation Accuracy:0.2726\n",
    "Epoch #101: Loss:1.5399, Accuracy:0.2776, Validation Loss:1.5581, Validation Accuracy:0.2693\n",
    "Epoch #102: Loss:1.5398, Accuracy:0.2772, Validation Loss:1.5578, Validation Accuracy:0.2644\n",
    "Epoch #103: Loss:1.5392, Accuracy:0.2809, Validation Loss:1.5571, Validation Accuracy:0.2709\n",
    "Epoch #104: Loss:1.5384, Accuracy:0.2793, Validation Loss:1.5577, Validation Accuracy:0.2578\n",
    "Epoch #105: Loss:1.5420, Accuracy:0.2674, Validation Loss:1.5580, Validation Accuracy:0.2578\n",
    "Epoch #106: Loss:1.5384, Accuracy:0.2850, Validation Loss:1.5570, Validation Accuracy:0.2791\n",
    "Epoch #107: Loss:1.5383, Accuracy:0.2719, Validation Loss:1.5557, Validation Accuracy:0.2775\n",
    "Epoch #108: Loss:1.5401, Accuracy:0.2809, Validation Loss:1.5574, Validation Accuracy:0.2660\n",
    "Epoch #109: Loss:1.5376, Accuracy:0.2821, Validation Loss:1.5549, Validation Accuracy:0.2644\n",
    "Epoch #110: Loss:1.5357, Accuracy:0.2797, Validation Loss:1.5548, Validation Accuracy:0.2759\n",
    "Epoch #111: Loss:1.5362, Accuracy:0.2842, Validation Loss:1.5562, Validation Accuracy:0.2677\n",
    "Epoch #112: Loss:1.5349, Accuracy:0.2825, Validation Loss:1.5566, Validation Accuracy:0.2693\n",
    "Epoch #113: Loss:1.5343, Accuracy:0.2871, Validation Loss:1.5547, Validation Accuracy:0.2709\n",
    "Epoch #114: Loss:1.5337, Accuracy:0.2867, Validation Loss:1.5544, Validation Accuracy:0.2644\n",
    "Epoch #115: Loss:1.5332, Accuracy:0.2830, Validation Loss:1.5548, Validation Accuracy:0.2808\n",
    "Epoch #116: Loss:1.5322, Accuracy:0.2883, Validation Loss:1.5557, Validation Accuracy:0.2791\n",
    "Epoch #117: Loss:1.5312, Accuracy:0.2895, Validation Loss:1.5539, Validation Accuracy:0.2759\n",
    "Epoch #118: Loss:1.5329, Accuracy:0.2821, Validation Loss:1.5572, Validation Accuracy:0.2611\n",
    "Epoch #119: Loss:1.5336, Accuracy:0.2879, Validation Loss:1.5555, Validation Accuracy:0.2644\n",
    "Epoch #120: Loss:1.5309, Accuracy:0.2883, Validation Loss:1.5549, Validation Accuracy:0.2742\n",
    "Epoch #121: Loss:1.5366, Accuracy:0.2867, Validation Loss:1.5564, Validation Accuracy:0.2594\n",
    "Epoch #122: Loss:1.5315, Accuracy:0.2821, Validation Loss:1.5522, Validation Accuracy:0.2726\n",
    "Epoch #123: Loss:1.5320, Accuracy:0.2879, Validation Loss:1.5578, Validation Accuracy:0.2742\n",
    "Epoch #124: Loss:1.5383, Accuracy:0.2797, Validation Loss:1.5696, Validation Accuracy:0.2578\n",
    "Epoch #125: Loss:1.5383, Accuracy:0.2813, Validation Loss:1.5553, Validation Accuracy:0.2841\n",
    "Epoch #126: Loss:1.5321, Accuracy:0.2928, Validation Loss:1.5538, Validation Accuracy:0.2644\n",
    "Epoch #127: Loss:1.5269, Accuracy:0.2891, Validation Loss:1.5522, Validation Accuracy:0.2775\n",
    "Epoch #128: Loss:1.5326, Accuracy:0.2895, Validation Loss:1.5515, Validation Accuracy:0.2693\n",
    "Epoch #129: Loss:1.5253, Accuracy:0.2891, Validation Loss:1.5493, Validation Accuracy:0.2742\n",
    "Epoch #130: Loss:1.5321, Accuracy:0.2862, Validation Loss:1.5586, Validation Accuracy:0.2627\n",
    "Epoch #131: Loss:1.5294, Accuracy:0.2903, Validation Loss:1.5485, Validation Accuracy:0.2742\n",
    "Epoch #132: Loss:1.5257, Accuracy:0.2953, Validation Loss:1.5470, Validation Accuracy:0.2791\n",
    "Epoch #133: Loss:1.5253, Accuracy:0.2965, Validation Loss:1.5461, Validation Accuracy:0.2775\n",
    "Epoch #134: Loss:1.5230, Accuracy:0.2891, Validation Loss:1.5474, Validation Accuracy:0.2824\n",
    "Epoch #135: Loss:1.5204, Accuracy:0.2990, Validation Loss:1.5448, Validation Accuracy:0.2791\n",
    "Epoch #136: Loss:1.5213, Accuracy:0.2982, Validation Loss:1.5471, Validation Accuracy:0.2808\n",
    "Epoch #137: Loss:1.5222, Accuracy:0.3023, Validation Loss:1.5484, Validation Accuracy:0.2824\n",
    "Epoch #138: Loss:1.5236, Accuracy:0.2973, Validation Loss:1.5457, Validation Accuracy:0.2989\n",
    "Epoch #139: Loss:1.5229, Accuracy:0.2949, Validation Loss:1.5458, Validation Accuracy:0.2775\n",
    "Epoch #140: Loss:1.5185, Accuracy:0.2977, Validation Loss:1.5457, Validation Accuracy:0.2824\n",
    "Epoch #141: Loss:1.5165, Accuracy:0.3023, Validation Loss:1.5433, Validation Accuracy:0.2857\n",
    "Epoch #142: Loss:1.5160, Accuracy:0.3006, Validation Loss:1.5449, Validation Accuracy:0.2857\n",
    "Epoch #143: Loss:1.5155, Accuracy:0.2994, Validation Loss:1.5443, Validation Accuracy:0.2709\n",
    "Epoch #144: Loss:1.5179, Accuracy:0.2990, Validation Loss:1.5413, Validation Accuracy:0.2874\n",
    "Epoch #145: Loss:1.5162, Accuracy:0.3035, Validation Loss:1.5432, Validation Accuracy:0.2989\n",
    "Epoch #146: Loss:1.5183, Accuracy:0.2994, Validation Loss:1.5565, Validation Accuracy:0.2742\n",
    "Epoch #147: Loss:1.5235, Accuracy:0.2957, Validation Loss:1.5439, Validation Accuracy:0.2841\n",
    "Epoch #148: Loss:1.5200, Accuracy:0.3018, Validation Loss:1.5627, Validation Accuracy:0.2627\n",
    "Epoch #149: Loss:1.5265, Accuracy:0.2920, Validation Loss:1.5474, Validation Accuracy:0.2841\n",
    "Epoch #150: Loss:1.5229, Accuracy:0.2982, Validation Loss:1.5496, Validation Accuracy:0.2693\n",
    "Epoch #151: Loss:1.5193, Accuracy:0.2969, Validation Loss:1.5368, Validation Accuracy:0.2956\n",
    "Epoch #152: Loss:1.5155, Accuracy:0.3043, Validation Loss:1.5457, Validation Accuracy:0.2742\n",
    "Epoch #153: Loss:1.5207, Accuracy:0.2924, Validation Loss:1.5368, Validation Accuracy:0.2791\n",
    "Epoch #154: Loss:1.5128, Accuracy:0.2982, Validation Loss:1.5394, Validation Accuracy:0.2857\n",
    "Epoch #155: Loss:1.5090, Accuracy:0.3027, Validation Loss:1.5365, Validation Accuracy:0.2874\n",
    "Epoch #156: Loss:1.5068, Accuracy:0.3072, Validation Loss:1.5401, Validation Accuracy:0.2890\n",
    "Epoch #157: Loss:1.5080, Accuracy:0.2998, Validation Loss:1.5360, Validation Accuracy:0.2841\n",
    "Epoch #158: Loss:1.5110, Accuracy:0.3018, Validation Loss:1.5438, Validation Accuracy:0.2824\n",
    "Epoch #159: Loss:1.5094, Accuracy:0.3043, Validation Loss:1.5379, Validation Accuracy:0.2857\n",
    "Epoch #160: Loss:1.5112, Accuracy:0.3035, Validation Loss:1.5466, Validation Accuracy:0.2759\n",
    "Epoch #161: Loss:1.5138, Accuracy:0.2969, Validation Loss:1.5367, Validation Accuracy:0.2890\n",
    "Epoch #162: Loss:1.5109, Accuracy:0.3129, Validation Loss:1.5480, Validation Accuracy:0.2841\n",
    "Epoch #163: Loss:1.5102, Accuracy:0.2986, Validation Loss:1.5374, Validation Accuracy:0.3021\n",
    "Epoch #164: Loss:1.5055, Accuracy:0.3064, Validation Loss:1.5417, Validation Accuracy:0.2874\n",
    "Epoch #165: Loss:1.5012, Accuracy:0.3097, Validation Loss:1.5342, Validation Accuracy:0.2972\n",
    "Epoch #166: Loss:1.5012, Accuracy:0.3158, Validation Loss:1.5394, Validation Accuracy:0.2791\n",
    "Epoch #167: Loss:1.5035, Accuracy:0.3097, Validation Loss:1.5376, Validation Accuracy:0.2906\n",
    "Epoch #168: Loss:1.5014, Accuracy:0.3109, Validation Loss:1.5356, Validation Accuracy:0.3021\n",
    "Epoch #169: Loss:1.4984, Accuracy:0.3179, Validation Loss:1.5396, Validation Accuracy:0.2890\n",
    "Epoch #170: Loss:1.4990, Accuracy:0.3080, Validation Loss:1.5356, Validation Accuracy:0.2874\n",
    "Epoch #171: Loss:1.4976, Accuracy:0.3076, Validation Loss:1.5363, Validation Accuracy:0.2841\n",
    "Epoch #172: Loss:1.4973, Accuracy:0.3109, Validation Loss:1.5349, Validation Accuracy:0.2972\n",
    "Epoch #173: Loss:1.4961, Accuracy:0.3125, Validation Loss:1.5445, Validation Accuracy:0.2791\n",
    "Epoch #174: Loss:1.4971, Accuracy:0.3117, Validation Loss:1.5344, Validation Accuracy:0.3005\n",
    "Epoch #175: Loss:1.4949, Accuracy:0.3175, Validation Loss:1.5343, Validation Accuracy:0.2989\n",
    "Epoch #176: Loss:1.4936, Accuracy:0.3216, Validation Loss:1.5350, Validation Accuracy:0.3021\n",
    "Epoch #177: Loss:1.4944, Accuracy:0.3158, Validation Loss:1.5397, Validation Accuracy:0.2857\n",
    "Epoch #178: Loss:1.4940, Accuracy:0.3125, Validation Loss:1.5344, Validation Accuracy:0.3071\n",
    "Epoch #179: Loss:1.4955, Accuracy:0.3084, Validation Loss:1.5347, Validation Accuracy:0.2939\n",
    "Epoch #180: Loss:1.4931, Accuracy:0.3236, Validation Loss:1.5358, Validation Accuracy:0.3005\n",
    "Epoch #181: Loss:1.4927, Accuracy:0.3191, Validation Loss:1.5366, Validation Accuracy:0.2906\n",
    "Epoch #182: Loss:1.4912, Accuracy:0.3162, Validation Loss:1.5336, Validation Accuracy:0.2989\n",
    "Epoch #183: Loss:1.4924, Accuracy:0.3216, Validation Loss:1.5377, Validation Accuracy:0.2939\n",
    "Epoch #184: Loss:1.4910, Accuracy:0.3175, Validation Loss:1.5342, Validation Accuracy:0.3071\n",
    "Epoch #185: Loss:1.4878, Accuracy:0.3261, Validation Loss:1.5344, Validation Accuracy:0.2972\n",
    "Epoch #186: Loss:1.4871, Accuracy:0.3232, Validation Loss:1.5314, Validation Accuracy:0.3103\n",
    "Epoch #187: Loss:1.4901, Accuracy:0.3220, Validation Loss:1.5371, Validation Accuracy:0.2906\n",
    "Epoch #188: Loss:1.4870, Accuracy:0.3228, Validation Loss:1.5349, Validation Accuracy:0.2857\n",
    "Epoch #189: Loss:1.4908, Accuracy:0.3216, Validation Loss:1.5325, Validation Accuracy:0.3054\n",
    "Epoch #190: Loss:1.4886, Accuracy:0.3195, Validation Loss:1.5317, Validation Accuracy:0.2989\n",
    "Epoch #191: Loss:1.4915, Accuracy:0.3269, Validation Loss:1.5508, Validation Accuracy:0.2857\n",
    "Epoch #192: Loss:1.4878, Accuracy:0.3183, Validation Loss:1.5315, Validation Accuracy:0.2989\n",
    "Epoch #193: Loss:1.4939, Accuracy:0.3150, Validation Loss:1.5321, Validation Accuracy:0.2939\n",
    "Epoch #194: Loss:1.4840, Accuracy:0.3236, Validation Loss:1.5280, Validation Accuracy:0.2874\n",
    "Epoch #195: Loss:1.4831, Accuracy:0.3232, Validation Loss:1.5320, Validation Accuracy:0.2989\n",
    "Epoch #196: Loss:1.4812, Accuracy:0.3240, Validation Loss:1.5373, Validation Accuracy:0.2857\n",
    "Epoch #197: Loss:1.4833, Accuracy:0.3248, Validation Loss:1.5333, Validation Accuracy:0.3087\n",
    "Epoch #198: Loss:1.4825, Accuracy:0.3240, Validation Loss:1.5324, Validation Accuracy:0.2956\n",
    "Epoch #199: Loss:1.4827, Accuracy:0.3273, Validation Loss:1.5400, Validation Accuracy:0.2956\n",
    "Epoch #200: Loss:1.4822, Accuracy:0.3170, Validation Loss:1.5322, Validation Accuracy:0.3038\n",
    "Epoch #201: Loss:1.4826, Accuracy:0.3285, Validation Loss:1.5509, Validation Accuracy:0.2841\n",
    "Epoch #202: Loss:1.4909, Accuracy:0.3138, Validation Loss:1.5345, Validation Accuracy:0.2857\n",
    "Epoch #203: Loss:1.4888, Accuracy:0.3351, Validation Loss:1.5350, Validation Accuracy:0.3021\n",
    "Epoch #204: Loss:1.4942, Accuracy:0.3142, Validation Loss:1.5390, Validation Accuracy:0.2906\n",
    "Epoch #205: Loss:1.4916, Accuracy:0.3253, Validation Loss:1.5355, Validation Accuracy:0.2989\n",
    "Epoch #206: Loss:1.4819, Accuracy:0.3191, Validation Loss:1.5273, Validation Accuracy:0.3054\n",
    "Epoch #207: Loss:1.4802, Accuracy:0.3372, Validation Loss:1.5333, Validation Accuracy:0.2857\n",
    "Epoch #208: Loss:1.4775, Accuracy:0.3265, Validation Loss:1.5273, Validation Accuracy:0.2923\n",
    "Epoch #209: Loss:1.4752, Accuracy:0.3310, Validation Loss:1.5356, Validation Accuracy:0.2890\n",
    "Epoch #210: Loss:1.4745, Accuracy:0.3310, Validation Loss:1.5287, Validation Accuracy:0.3103\n",
    "Epoch #211: Loss:1.4806, Accuracy:0.3265, Validation Loss:1.5380, Validation Accuracy:0.2890\n",
    "Epoch #212: Loss:1.4786, Accuracy:0.3368, Validation Loss:1.5345, Validation Accuracy:0.2939\n",
    "Epoch #213: Loss:1.4776, Accuracy:0.3380, Validation Loss:1.5300, Validation Accuracy:0.3136\n",
    "Epoch #214: Loss:1.4748, Accuracy:0.3388, Validation Loss:1.5371, Validation Accuracy:0.2989\n",
    "Epoch #215: Loss:1.4814, Accuracy:0.3310, Validation Loss:1.5282, Validation Accuracy:0.3021\n",
    "Epoch #216: Loss:1.4787, Accuracy:0.3294, Validation Loss:1.5268, Validation Accuracy:0.3005\n",
    "Epoch #217: Loss:1.4817, Accuracy:0.3298, Validation Loss:1.5429, Validation Accuracy:0.2857\n",
    "Epoch #218: Loss:1.4757, Accuracy:0.3446, Validation Loss:1.5250, Validation Accuracy:0.3103\n",
    "Epoch #219: Loss:1.4716, Accuracy:0.3421, Validation Loss:1.5270, Validation Accuracy:0.2956\n",
    "Epoch #220: Loss:1.4720, Accuracy:0.3347, Validation Loss:1.5338, Validation Accuracy:0.2923\n",
    "Epoch #221: Loss:1.4702, Accuracy:0.3310, Validation Loss:1.5271, Validation Accuracy:0.2989\n",
    "Epoch #222: Loss:1.4714, Accuracy:0.3306, Validation Loss:1.5282, Validation Accuracy:0.2956\n",
    "Epoch #223: Loss:1.4690, Accuracy:0.3363, Validation Loss:1.5381, Validation Accuracy:0.2890\n",
    "Epoch #224: Loss:1.4682, Accuracy:0.3294, Validation Loss:1.5274, Validation Accuracy:0.3005\n",
    "Epoch #225: Loss:1.4694, Accuracy:0.3322, Validation Loss:1.5342, Validation Accuracy:0.2906\n",
    "Epoch #226: Loss:1.4696, Accuracy:0.3409, Validation Loss:1.5320, Validation Accuracy:0.2890\n",
    "Epoch #227: Loss:1.4677, Accuracy:0.3335, Validation Loss:1.5280, Validation Accuracy:0.2857\n",
    "Epoch #228: Loss:1.4655, Accuracy:0.3425, Validation Loss:1.5286, Validation Accuracy:0.2890\n",
    "Epoch #229: Loss:1.4656, Accuracy:0.3363, Validation Loss:1.5321, Validation Accuracy:0.2874\n",
    "Epoch #230: Loss:1.4637, Accuracy:0.3384, Validation Loss:1.5299, Validation Accuracy:0.2939\n",
    "Epoch #231: Loss:1.4630, Accuracy:0.3359, Validation Loss:1.5272, Validation Accuracy:0.3087\n",
    "Epoch #232: Loss:1.4643, Accuracy:0.3335, Validation Loss:1.5327, Validation Accuracy:0.2874\n",
    "Epoch #233: Loss:1.4655, Accuracy:0.3310, Validation Loss:1.5357, Validation Accuracy:0.2841\n",
    "Epoch #234: Loss:1.4660, Accuracy:0.3363, Validation Loss:1.5276, Validation Accuracy:0.3005\n",
    "Epoch #235: Loss:1.4657, Accuracy:0.3347, Validation Loss:1.5277, Validation Accuracy:0.2956\n",
    "Epoch #236: Loss:1.4701, Accuracy:0.3326, Validation Loss:1.5347, Validation Accuracy:0.2890\n",
    "Epoch #237: Loss:1.4680, Accuracy:0.3306, Validation Loss:1.5283, Validation Accuracy:0.2923\n",
    "Epoch #238: Loss:1.4616, Accuracy:0.3347, Validation Loss:1.5262, Validation Accuracy:0.3005\n",
    "Epoch #239: Loss:1.4660, Accuracy:0.3359, Validation Loss:1.5330, Validation Accuracy:0.2841\n",
    "Epoch #240: Loss:1.4605, Accuracy:0.3409, Validation Loss:1.5274, Validation Accuracy:0.2857\n",
    "Epoch #241: Loss:1.4603, Accuracy:0.3433, Validation Loss:1.5360, Validation Accuracy:0.2972\n",
    "Epoch #242: Loss:1.4614, Accuracy:0.3372, Validation Loss:1.5273, Validation Accuracy:0.2890\n",
    "Epoch #243: Loss:1.4622, Accuracy:0.3384, Validation Loss:1.5285, Validation Accuracy:0.3038\n",
    "Epoch #244: Loss:1.4599, Accuracy:0.3433, Validation Loss:1.5430, Validation Accuracy:0.2906\n",
    "Epoch #245: Loss:1.4650, Accuracy:0.3359, Validation Loss:1.5228, Validation Accuracy:0.2972\n",
    "Epoch #246: Loss:1.4657, Accuracy:0.3343, Validation Loss:1.5317, Validation Accuracy:0.2874\n",
    "Epoch #247: Loss:1.4621, Accuracy:0.3396, Validation Loss:1.5330, Validation Accuracy:0.3038\n",
    "Epoch #248: Loss:1.4635, Accuracy:0.3384, Validation Loss:1.5262, Validation Accuracy:0.2989\n",
    "Epoch #249: Loss:1.4565, Accuracy:0.3388, Validation Loss:1.5257, Validation Accuracy:0.2956\n",
    "Epoch #250: Loss:1.4576, Accuracy:0.3450, Validation Loss:1.5292, Validation Accuracy:0.2972\n",
    "Epoch #251: Loss:1.4601, Accuracy:0.3425, Validation Loss:1.5245, Validation Accuracy:0.2906\n",
    "Epoch #252: Loss:1.4627, Accuracy:0.3396, Validation Loss:1.5282, Validation Accuracy:0.3021\n",
    "Epoch #253: Loss:1.4709, Accuracy:0.3392, Validation Loss:1.5389, Validation Accuracy:0.2939\n",
    "Epoch #254: Loss:1.4576, Accuracy:0.3384, Validation Loss:1.5221, Validation Accuracy:0.2890\n",
    "Epoch #255: Loss:1.4571, Accuracy:0.3478, Validation Loss:1.5274, Validation Accuracy:0.3005\n",
    "Epoch #256: Loss:1.4528, Accuracy:0.3511, Validation Loss:1.5254, Validation Accuracy:0.3005\n",
    "Epoch #257: Loss:1.4539, Accuracy:0.3458, Validation Loss:1.5251, Validation Accuracy:0.2874\n",
    "Epoch #258: Loss:1.4521, Accuracy:0.3511, Validation Loss:1.5276, Validation Accuracy:0.2906\n",
    "Epoch #259: Loss:1.4523, Accuracy:0.3499, Validation Loss:1.5243, Validation Accuracy:0.3038\n",
    "Epoch #260: Loss:1.4548, Accuracy:0.3454, Validation Loss:1.5335, Validation Accuracy:0.2989\n",
    "Epoch #261: Loss:1.4529, Accuracy:0.3524, Validation Loss:1.5253, Validation Accuracy:0.3054\n",
    "Epoch #262: Loss:1.4514, Accuracy:0.3544, Validation Loss:1.5249, Validation Accuracy:0.3005\n",
    "Epoch #263: Loss:1.4522, Accuracy:0.3450, Validation Loss:1.5250, Validation Accuracy:0.2890\n",
    "Epoch #264: Loss:1.4501, Accuracy:0.3446, Validation Loss:1.5298, Validation Accuracy:0.2989\n",
    "Epoch #265: Loss:1.4505, Accuracy:0.3458, Validation Loss:1.5280, Validation Accuracy:0.3021\n",
    "Epoch #266: Loss:1.4505, Accuracy:0.3450, Validation Loss:1.5271, Validation Accuracy:0.2939\n",
    "Epoch #267: Loss:1.4492, Accuracy:0.3462, Validation Loss:1.5283, Validation Accuracy:0.2989\n",
    "Epoch #268: Loss:1.4523, Accuracy:0.3515, Validation Loss:1.5225, Validation Accuracy:0.3087\n",
    "Epoch #269: Loss:1.4540, Accuracy:0.3392, Validation Loss:1.5317, Validation Accuracy:0.3071\n",
    "Epoch #270: Loss:1.4553, Accuracy:0.3511, Validation Loss:1.5289, Validation Accuracy:0.2989\n",
    "Epoch #271: Loss:1.4521, Accuracy:0.3511, Validation Loss:1.5213, Validation Accuracy:0.3038\n",
    "Epoch #272: Loss:1.4524, Accuracy:0.3561, Validation Loss:1.5364, Validation Accuracy:0.3005\n",
    "Epoch #273: Loss:1.4539, Accuracy:0.3524, Validation Loss:1.5241, Validation Accuracy:0.2923\n",
    "Epoch #274: Loss:1.4556, Accuracy:0.3372, Validation Loss:1.5232, Validation Accuracy:0.3038\n",
    "Epoch #275: Loss:1.4482, Accuracy:0.3495, Validation Loss:1.5341, Validation Accuracy:0.2906\n",
    "Epoch #276: Loss:1.4464, Accuracy:0.3483, Validation Loss:1.5227, Validation Accuracy:0.2906\n",
    "Epoch #277: Loss:1.4491, Accuracy:0.3499, Validation Loss:1.5217, Validation Accuracy:0.3120\n",
    "Epoch #278: Loss:1.4448, Accuracy:0.3569, Validation Loss:1.5251, Validation Accuracy:0.3186\n",
    "Epoch #279: Loss:1.4467, Accuracy:0.3437, Validation Loss:1.5312, Validation Accuracy:0.3021\n",
    "Epoch #280: Loss:1.4473, Accuracy:0.3524, Validation Loss:1.5277, Validation Accuracy:0.2939\n",
    "Epoch #281: Loss:1.4463, Accuracy:0.3548, Validation Loss:1.5217, Validation Accuracy:0.2923\n",
    "Epoch #282: Loss:1.4453, Accuracy:0.3483, Validation Loss:1.5235, Validation Accuracy:0.3054\n",
    "Epoch #283: Loss:1.4437, Accuracy:0.3581, Validation Loss:1.5264, Validation Accuracy:0.3087\n",
    "Epoch #284: Loss:1.4444, Accuracy:0.3454, Validation Loss:1.5234, Validation Accuracy:0.3054\n",
    "Epoch #285: Loss:1.4482, Accuracy:0.3593, Validation Loss:1.5259, Validation Accuracy:0.3120\n",
    "Epoch #286: Loss:1.4536, Accuracy:0.3503, Validation Loss:1.5432, Validation Accuracy:0.3136\n",
    "Epoch #287: Loss:1.4500, Accuracy:0.3450, Validation Loss:1.5279, Validation Accuracy:0.3235\n",
    "Epoch #288: Loss:1.4502, Accuracy:0.3503, Validation Loss:1.5472, Validation Accuracy:0.2857\n",
    "Epoch #289: Loss:1.4527, Accuracy:0.3458, Validation Loss:1.5194, Validation Accuracy:0.3087\n",
    "Epoch #290: Loss:1.4460, Accuracy:0.3589, Validation Loss:1.5176, Validation Accuracy:0.3136\n",
    "Epoch #291: Loss:1.4426, Accuracy:0.3548, Validation Loss:1.5239, Validation Accuracy:0.3054\n",
    "Epoch #292: Loss:1.4399, Accuracy:0.3639, Validation Loss:1.5204, Validation Accuracy:0.3054\n",
    "Epoch #293: Loss:1.4423, Accuracy:0.3507, Validation Loss:1.5308, Validation Accuracy:0.3186\n",
    "Epoch #294: Loss:1.4412, Accuracy:0.3618, Validation Loss:1.5222, Validation Accuracy:0.3218\n",
    "Epoch #295: Loss:1.4414, Accuracy:0.3618, Validation Loss:1.5184, Validation Accuracy:0.3087\n",
    "Epoch #296: Loss:1.4388, Accuracy:0.3569, Validation Loss:1.5275, Validation Accuracy:0.3103\n",
    "Epoch #297: Loss:1.4386, Accuracy:0.3618, Validation Loss:1.5247, Validation Accuracy:0.3071\n",
    "Epoch #298: Loss:1.4390, Accuracy:0.3540, Validation Loss:1.5231, Validation Accuracy:0.3054\n",
    "Epoch #299: Loss:1.4391, Accuracy:0.3589, Validation Loss:1.5249, Validation Accuracy:0.3120\n",
    "Epoch #300: Loss:1.4391, Accuracy:0.3585, Validation Loss:1.5217, Validation Accuracy:0.3087\n",
    "\n",
    "Test:\n",
    "Test Loss:1.52165115, Accuracy:0.3087\n",
    "Labels: ['05', '03', '02', '01', '04']\n",
    "Confusion Matrix:\n",
    "      05  03  02  01  04\n",
    "t:05  47  46   9  19  21\n",
    "t:03  15  46  14  16  24\n",
    "t:02  15  32  29  13  25\n",
    "t:01  27  39  18  27  15\n",
    "t:04  16  29  10  18  39\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.39      0.33      0.36       142\n",
    "          03       0.24      0.40      0.30       115\n",
    "          02       0.36      0.25      0.30       114\n",
    "          01       0.29      0.21      0.25       126\n",
    "          04       0.31      0.35      0.33       112\n",
    "\n",
    "    accuracy                           0.31       609\n",
    "   macro avg       0.32      0.31      0.31       609\n",
    "weighted avg       0.32      0.31      0.31       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 02:58:06 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 42 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.60554711435033, 1.6053968462450752, 1.6054220953206906, 1.6053464763074476, 1.6052142317071925, 1.6052319222483142, 1.6051261458295123, 1.6050235553719532, 1.6048681526544255, 1.6046714273775349, 1.6044067708142284, 1.6040924566328427, 1.6036109526952107, 1.6029309320136635, 1.6018947687838074, 1.6002971188383932, 1.5979466600762604, 1.5946459623393168, 1.5900789767454802, 1.5848009490418709, 1.5800286633236262, 1.576740884820033, 1.5745993144014983, 1.573890531395848, 1.5724491078669607, 1.5711972828960574, 1.5705794649954108, 1.5702500388344325, 1.5705358508380958, 1.5684878918141958, 1.5682501904482913, 1.5680411116438742, 1.5696468100759196, 1.5663906091148239, 1.5683310797257572, 1.5681316504141771, 1.5657601213611796, 1.5682515148654557, 1.5661827862164852, 1.5667349965310058, 1.5671401254844979, 1.5661106003916323, 1.57008395210667, 1.564030092338036, 1.56929203579187, 1.5671469343119655, 1.5643230891971558, 1.568789122531371, 1.5653720794640151, 1.5670254275520838, 1.5650210881663857, 1.5654419635121262, 1.5676271277303961, 1.5652793583220057, 1.5640162025962165, 1.5658608085807713, 1.5651635522717129, 1.5648783758551812, 1.5654794745061589, 1.5637934106128362, 1.566934766832048, 1.5633020422533033, 1.564027901940745, 1.5642191882203953, 1.5620888064451797, 1.563844875553363, 1.562545817865331, 1.5635785169789356, 1.5623638941149407, 1.561855024892121, 1.5611368979530773, 1.5602878306691086, 1.561451109955072, 1.5611493331066688, 1.5600667087902576, 1.5599946509832623, 1.5586772135325841, 1.559234175776026, 1.5596202144090374, 1.5626799240096645, 1.5591874433855706, 1.5580760628131811, 1.557492659205482, 1.5573730899391112, 1.5582563491486172, 1.5588161191721073, 1.5665100630868245, 1.5620056848807875, 1.5612567281488128, 1.5582654662124433, 1.5579766177974508, 1.5591413585227503, 1.5586537531835496, 1.5630743987063078, 1.5596190698824102, 1.5575606905180832, 1.5582981375833647, 1.5583077686760813, 1.5587948942419343, 1.5568667519072985, 1.5580676582646487, 1.5578182869161095, 1.557052907293849, 1.557749496696422, 1.5579710989358586, 1.5569623906428396, 1.5556943860938788, 1.5573838798479103, 1.5548515817018957, 1.5548277065671723, 1.5562365041382011, 1.556559340315695, 1.5547369105866782, 1.554373230057201, 1.5547712041043686, 1.5557026268030427, 1.553864949050991, 1.5572242192642638, 1.5554654057977235, 1.5549288939176913, 1.5563682028029744, 1.55215985140777, 1.5578208390519341, 1.569627388161783, 1.555300927906005, 1.5538170398358249, 1.5521891369608236, 1.5514908255810416, 1.5493428585760307, 1.5585524797048083, 1.5485016250453756, 1.5470440718536502, 1.54609079744624, 1.5474171458402486, 1.5448334700564055, 1.5471301474203225, 1.5483574233031625, 1.5456554928828148, 1.5457889626570327, 1.545677690670408, 1.5432829958660457, 1.5448926769453903, 1.5442549467869775, 1.5413205838947266, 1.5431627317015173, 1.5565217707936203, 1.5439147369810708, 1.5627254556944021, 1.5473893062626003, 1.5495528708929303, 1.5367933274881398, 1.5456574207847733, 1.5368447573901398, 1.539390553394562, 1.536501773668236, 1.5400523385782352, 1.535954095068432, 1.5438113333947945, 1.5378858193583873, 1.5466176136373886, 1.5367103836610791, 1.5479602786316269, 1.5373920433235482, 1.5416610796854806, 1.5342064203299912, 1.5394392346317936, 1.5375629299379923, 1.5355645811616494, 1.5396404330953588, 1.5355571559301542, 1.536325933310786, 1.5349250845916949, 1.544486866991704, 1.5344173618529622, 1.5343440326759576, 1.5350072150942924, 1.539687800681454, 1.534414291381836, 1.534706326541055, 1.5358402817120105, 1.5366360831926218, 1.5335572660458694, 1.5377260589443014, 1.534238884405941, 1.534354016111402, 1.531389427889744, 1.53706107644612, 1.5348941449852804, 1.5324745871163354, 1.5317007064427843, 1.5507721750215553, 1.5315079749707126, 1.5321135875235246, 1.5279698782953723, 1.5320167494524877, 1.5372717323757352, 1.5333071825735283, 1.5323599919505504, 1.5400296150169936, 1.5321890501357456, 1.5509007472318577, 1.5344570683336807, 1.5350137132729216, 1.5389722787296438, 1.5354635249609234, 1.5273082436403422, 1.533342226776975, 1.5273008518814062, 1.5356032439249099, 1.5286506952714842, 1.5379786426797877, 1.5344634905628773, 1.5299651270429488, 1.5371000616029762, 1.5282213742705597, 1.5268371692431972, 1.5429395535113581, 1.5250268354400234, 1.527047311143922, 1.5338052928154104, 1.5271250099579885, 1.528156169725365, 1.5381484476020575, 1.527368382867334, 1.5342197095232057, 1.5319895174702987, 1.5279548211246485, 1.528569738461662, 1.532137242639789, 1.5299316233602063, 1.527198134188973, 1.5326821974345617, 1.5356681121785456, 1.5275565779267861, 1.5277196309836627, 1.5347188102396447, 1.5283337428260515, 1.5262039790208313, 1.533011047906672, 1.5274049442977153, 1.53602813871819, 1.5273160860064778, 1.528521912438529, 1.5430412274863332, 1.5227962526781806, 1.531717905857293, 1.5330142411105152, 1.5261923718726498, 1.525731306357924, 1.5291986295155116, 1.5244997459875147, 1.528177997161602, 1.538943213782287, 1.5220631885606863, 1.52739466293692, 1.525439655839516, 1.5251215782463061, 1.5275822271071435, 1.5243368941574849, 1.5334661858422416, 1.5253283832656535, 1.524882898346348, 1.5250396049277144, 1.5297530036058724, 1.5280309551455118, 1.5271021137488103, 1.5282709496753362, 1.5225294834287295, 1.5317047664097376, 1.5288988876421072, 1.52133143124322, 1.5364315983501367, 1.5240511631926488, 1.523240658841501, 1.5341483663847097, 1.522705829202248, 1.5216966293911236, 1.5250988210167595, 1.5311563868436515, 1.5277391954008581, 1.5216565899465275, 1.5234674215316772, 1.5264404967109166, 1.523407889312907, 1.5259214584854828, 1.543216425601289, 1.5279208611580735, 1.5472476040005487, 1.5194458562165059, 1.5175960719683292, 1.5238502321180647, 1.5204150345916623, 1.5307547799472152, 1.5221686104835548, 1.5183645253893974, 1.527546884195362, 1.5246574052644677, 1.5230880991382942, 1.5249487255594414, 1.5216510767615683], 'val_acc': [0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23645320184810212, 0.24137931022248635, 0.24302134634728112, 0.26108374173809545, 0.26108374183596844, 0.26108374173809545, 0.2627257776671442, 0.2660098501124797, 0.2676518860415285, 0.26600984991673376, 0.26600984991673376, 0.2676518860415285, 0.270935958388991, 0.270935958388991, 0.26600984991673376, 0.2692939222641962, 0.270935958388991, 0.2692939222641962, 0.26765188613940144, 0.2742200307364534, 0.2791461392087106, 0.2742200307364534, 0.2660098500146067, 0.27750410298604294, 0.27750410288816996, 0.27422003063858047, 0.2545155993187173, 0.2545155971410435, 0.2627257776671442, 0.2627257776671442, 0.25779966929276, 0.2561576333637112, 0.251231524793581, 0.26929392236206917, 0.2528735630960496, 0.26436781388981195, 0.25779966948850597, 0.25287356101624875, 0.24958948886453225, 0.25287356111412174, 0.2545155972389165, 0.2610837438178963, 0.24794745264186452, 0.25287356091837576, 0.25615763316796525, 0.24302134624940813, 0.2463054164191968, 0.2545155971410435, 0.24794745254399153, 0.251231524891454, 0.24794745254399153, 0.2594417058090467, 0.24958948876665926, 0.25779966958637895, 0.24630541869474357, 0.25615763326583824, 0.25287356101624875, 0.2545155971410435, 0.25779966929276, 0.2627257779607632, 0.26108374183596844, 0.25615763316796525, 0.2610837420317144, 0.2610837415423495, 0.251231524891454, 0.24630541661494276, 0.25779966968425194, 0.2594417058090467, 0.25944170600479266, 0.2545155971410435, 0.25779966978212493, 0.23973727250725568, 0.24958948886453225, 0.25615763414669507, 0.25615763414669507, 0.27093595907410184, 0.2610837424232063, 0.2676518866287664, 0.24302134505046413, 0.25779966968425194, 0.26272577835225513, 0.2692939227535611, 0.2725779950031506, 0.25944170629841157, 0.2725779952967695, 0.26929392304718003, 0.2643678143791769, 0.27093595917197477, 0.25779966978212493, 0.25779967007574384, 0.27914613979594854, 0.27750410376902673, 0.26600985069971755, 0.2643678144770499, 0.27586206744848607, 0.2676518866287664, 0.269293923145053, 0.27093595926984776, 0.2643678143791769, 0.2807881757249973, 0.27914613969807556, 0.27586206744848607, 0.26108374222746034, 0.2643678144770499, 0.27422003112794535, 0.25944170639628455, 0.2725779954925155, 0.2742200309321994, 0.2577996705651088, 0.2840722477788408, 0.26436781496641476, 0.27750410318178886, 0.269293923145053, 0.27422003112794535, 0.262725778939493, 0.2742200317151832, 0.27914613999169446, 0.27750410376902673, 0.28243021224128395, 0.27914613979594854, 0.2807881762143622, 0.282430212143411, 0.29885057348923144, 0.2775041039647727, 0.282430212143411, 0.2857142842951275, 0.2857142842951275, 0.2709359595634667, 0.28735632061566824, 0.29885057329348547, 0.27422003151943725, 0.2840722478767138, 0.26272577825438215, 0.2840722477788408, 0.26929392304718003, 0.29556650114176897, 0.27422003151943725, 0.27914613960020257, 0.2857142843930005, 0.2873563203220493, 0.288998356740463, 0.2840722479745868, 0.282430212143411, 0.28571428370788965, 0.275862067742105, 0.288998356740463, 0.28407224856182467, 0.3021346458366939, 0.28735632051779525, 0.29720853707081774, 0.27914614008956745, 0.29064039296313066, 0.3021346458366939, 0.2889983568383359, 0.28735632071354117, 0.2840722484639517, 0.2972085374623097, 0.27914613999169446, 0.3004926099076451, 0.2988505736849774, 0.3021346459345669, 0.28571428449087344, 0.3070607539174592, 0.2939244651148472, 0.30049260980977216, 0.29064039306100364, 0.29885057378285035, 0.2939244652127202, 0.3070607544068241, 0.2972085376580556, 0.31034482646066763, 0.29064039315887663, 0.2857142843930005, 0.3054187179884104, 0.29885057378285035, 0.28571428400150856, 0.2988505731956125, 0.2939244650169742, 0.28735632051779525, 0.29885057329348547, 0.2857142847844924, 0.3087027902379999, 0.29556650123964195, 0.29556650133751494, 0.3037766814721237, 0.28407224826820576, 0.2857142843930005, 0.30213464554307495, 0.29064039266951175, 0.29885057339135845, 0.30541871759691847, 0.28571428400150856, 0.2922824286964335, 0.288998356544717, 0.31034482587342976, 0.28899835664259, 0.2939244650169742, 0.31362889841663816, 0.29885057358710443, 0.30213464564094794, 0.30049260912466125, 0.2857142842951275, 0.31034482616704867, 0.295566501043896, 0.2922824287943065, 0.2988505730977395, 0.295566500750277, 0.28899835664259, 0.3004926093204072, 0.29064039266951175, 0.28899835664259, 0.28571428409938154, 0.288998356446844, 0.28735632041992226, 0.29392446472335526, 0.308702789846508, 0.28735632041992226, 0.2840722481703328, 0.3004926093204072, 0.2955665005545311, 0.288998356544717, 0.2922824288921795, 0.3004926090267883, 0.2840722483660787, 0.28571428400150856, 0.29720853707081774, 0.288998356544717, 0.3037766816678697, 0.2906403924737658, 0.29720853667932584, 0.2873563201263033, 0.30377668186361567, 0.2988505730977395, 0.295566500750277, 0.2972085372665637, 0.2906403924737658, 0.30213464505371007, 0.29392446491910124, 0.28899835634897103, 0.30049260922253423, 0.3004926094182802, 0.2873563201263033, 0.2906403923758928, 0.3037766813742508, 0.2988505730977395, 0.30541871779266444, 0.30049260892891533, 0.288998356446844, 0.29885057348923144, 0.30213464544520197, 0.29392446482122825, 0.29885057339135845, 0.308702789846508, 0.30706075401533217, 0.2988505731956125, 0.3037766811785048, 0.3004926094182802, 0.2922824285985605, 0.3037766813742508, 0.2906403924737658, 0.29064039266951175, 0.3119868621939704, 0.31855500669314946, 0.30213464544520197, 0.29392446472335526, 0.29228242850068753, 0.30541871779266444, 0.3087027900422539, 0.30541871769479145, 0.3119868624875894, 0.31362889861238413, 0.3234811150675337, 0.28571428409938154, 0.308702789846508, 0.31362889822089224, 0.30541871789053743, 0.30541871789053743, 0.3185550068888954, 0.32183907913848486, 0.3087027900422539, 0.31034482636279465, 0.3070607539174592, 0.3054187179884104, 0.3119868622918434, 0.3087027900422539], 'loss': [1.6069106678208775, 1.6056091268449348, 1.6055567045231374, 1.605684374734851, 1.6055466826201954, 1.6055646032278543, 1.6054027699347149, 1.6053065789064098, 1.6052466585895608, 1.6052008481485889, 1.605091662181721, 1.604692296570576, 1.6044815426734438, 1.6039035908250594, 1.6032855991465356, 1.6022479554221372, 1.6010542577297047, 1.5989327753838571, 1.5957560661637074, 1.5925973533360132, 1.589428667072398, 1.5882337461996372, 1.586507341847038, 1.5850880662518605, 1.585025561320953, 1.5835076047164949, 1.5825803192244419, 1.582639812541938, 1.5817049368695801, 1.5806785528664715, 1.580791490327651, 1.5799920980445659, 1.579760390287552, 1.5794598025707738, 1.578404835019513, 1.5781274017122493, 1.5773876854036868, 1.5770522908263627, 1.5768606017991993, 1.576421705114768, 1.5753397466220895, 1.5761418397911275, 1.5755990058244866, 1.5748714326098714, 1.5731277710603249, 1.5752571676790836, 1.5742786259132244, 1.5721601437249468, 1.571601047946687, 1.5708468858221474, 1.5702697550246847, 1.5697140704435, 1.5694093130452431, 1.568355235622649, 1.5679074229645777, 1.5678337872640309, 1.5667376771844632, 1.566412668404393, 1.566024664735892, 1.565092511637255, 1.5642299723576227, 1.5644486415557548, 1.5631332783728409, 1.5628118982550054, 1.5616684505337317, 1.5625304969674019, 1.5607444679957396, 1.5598897811078927, 1.5598817619699716, 1.558145702595094, 1.5573042170957374, 1.5563276588060038, 1.555688604632932, 1.5578964681351208, 1.5575402879127487, 1.5533218436662177, 1.5541336404714252, 1.552131176141743, 1.5536604513622652, 1.5540035534688335, 1.5542843186635011, 1.5510994941057366, 1.5480868787491346, 1.5468565759227995, 1.5464915924738076, 1.5466274443593113, 1.547911620923381, 1.5464829879619748, 1.5487660111343102, 1.5451227187620786, 1.5447503265658933, 1.5434853886676765, 1.5425577557796815, 1.5462349927400907, 1.543964822189519, 1.5452712401717106, 1.5420607146785978, 1.5423587824529692, 1.5424457716501223, 1.5414476869532214, 1.539900532885009, 1.5398075000461366, 1.5391664548576245, 1.5383928719976845, 1.5419997132534364, 1.5383669193765221, 1.5382844167568355, 1.5401373968966443, 1.5376325215647109, 1.5356969349193377, 1.5361775247957672, 1.5349259176783003, 1.5342794713053616, 1.5336676365051427, 1.5332186846762468, 1.53221025222136, 1.5312017464784626, 1.532893956025768, 1.5335598168676638, 1.5308842183628122, 1.5365905395523478, 1.5315282968035469, 1.532020496440864, 1.5382523030960584, 1.538342468792408, 1.532097984486292, 1.5269326762741842, 1.532594598245327, 1.5252556998137332, 1.5320817275213754, 1.52937051912108, 1.525738765473728, 1.5252568533777946, 1.5230299045417832, 1.5203828427825865, 1.5212512381512526, 1.5222487899067465, 1.5235948601297775, 1.5229195502749213, 1.5185121495620915, 1.5164579407635166, 1.5159750778327488, 1.5154544192417936, 1.5178808011558267, 1.5162146064535058, 1.5183453320967344, 1.5234900706112997, 1.5200219948433753, 1.5265168865113776, 1.522885861191172, 1.5192636458535949, 1.515526919394303, 1.5207362455509037, 1.5128495446465589, 1.509010525160991, 1.5068073684429975, 1.508001644116897, 1.511024357406021, 1.5094137705816626, 1.5112175932655099, 1.513831350103296, 1.5109150682386676, 1.5102132303269247, 1.5055084516380357, 1.5011623714004454, 1.5011570875160014, 1.5035329518621707, 1.5013880168143239, 1.498394465348559, 1.4990229872952252, 1.4976087318064006, 1.4973490505982228, 1.4961082191682693, 1.4970944937249717, 1.4948658847221359, 1.4935850698600315, 1.4943656951739803, 1.494030164154648, 1.4954607596387608, 1.493109688230119, 1.4926858648871983, 1.4911718910479692, 1.492420382715104, 1.4910260824207409, 1.4878161723608843, 1.487077622345096, 1.490131177109125, 1.4869859854053913, 1.4907665626713873, 1.4886353729686699, 1.4914939557747184, 1.487790614917293, 1.4939065267417955, 1.4839989160854958, 1.4830673023415786, 1.4811645681608385, 1.483337826454664, 1.482504952370019, 1.4827271202016905, 1.4822423973612227, 1.4825545310974122, 1.4909277416597402, 1.4887689596328892, 1.4941647863485974, 1.4916222475392618, 1.4818761878924203, 1.4802119939479006, 1.4774655650038984, 1.4751934573880456, 1.4745460879876138, 1.480596692850947, 1.4786208556173273, 1.4775721952900505, 1.4747980247043242, 1.4814068829009666, 1.478705470909573, 1.4816985242420644, 1.4757451036872316, 1.4715613543375317, 1.4720188065475996, 1.4701933876934483, 1.4713665163737304, 1.4689822073100285, 1.468228653715866, 1.4694495047387157, 1.4696402383291256, 1.4677498181497781, 1.465482572265719, 1.465564213934865, 1.463663229609417, 1.4630300378407786, 1.4643486314730478, 1.4655321503077199, 1.4660160730017284, 1.4657364467820593, 1.4701306150679225, 1.4679713877809122, 1.4616330580544912, 1.4660164327836869, 1.4604553173699664, 1.4603236572943183, 1.4613667683434928, 1.4621721827763552, 1.459946482871837, 1.4650210581276206, 1.4657243287538846, 1.4621417372133698, 1.4635135891011608, 1.4565285070720884, 1.457588154236639, 1.4600509275890718, 1.4627076495599454, 1.470935715199496, 1.4576340767392388, 1.4570501961502451, 1.4527503137471005, 1.4538796758749646, 1.452115173065687, 1.452306594447189, 1.4547873961117723, 1.4528708314993544, 1.4514468268447343, 1.4521557115186656, 1.4501013344073443, 1.4505269318880243, 1.4505103219950712, 1.449239362583513, 1.4523241420546107, 1.4540216286813943, 1.455345672503634, 1.4521428216898955, 1.4523909176644358, 1.4538692304485876, 1.4556113459491142, 1.4481748333701852, 1.4463992989283567, 1.4490504835665348, 1.444800651832283, 1.4467318520164099, 1.4473211227256415, 1.446290794241355, 1.4453111757243193, 1.4436633054235877, 1.4443573268287235, 1.4481730470422356, 1.4535552769960565, 1.4500053111532631, 1.450225473429388, 1.4527008107555475, 1.445992659445416, 1.4426363752607938, 1.4398513652950342, 1.4423471749930412, 1.4411861561162271, 1.4414096047011733, 1.4387611315725275, 1.4386028103270325, 1.4389980756281828, 1.4390829999833625, 1.439079739327793], 'acc': [0.22833675590995892, 0.2328542098861945, 0.23285421049203225, 0.23285421010037957, 0.23285420970872686, 0.23285421029620593, 0.23285421029620593, 0.23285420990455322, 0.23285420951290053, 0.23285420912124782, 0.23285420890706277, 0.23285420890706277, 0.23285420951290053, 0.23285421049203225, 0.2328542106694999, 0.23326488652513258, 0.2344969195935271, 0.24353182870259765, 0.24353182776018334, 0.24558521604635877, 0.24558521526305338, 0.25010266867016867, 0.24887063679509094, 0.24845995878536845, 0.25379876938688684, 0.25010266886599497, 0.24928131441316076, 0.2509240241021346, 0.2525667329710857, 0.2533880909855116, 0.2521560583454872, 0.25256673457441386, 0.25544147786418514, 0.25585215532314604, 0.25667351036345937, 0.25790554278929867, 0.25503080126196453, 0.2550308024369226, 0.2603696118634829, 0.25667351155677615, 0.256262834703653, 0.2579055446068119, 0.2562628332961511, 0.24887063640343823, 0.2533880911996967, 0.2533880925521224, 0.2546201250146791, 0.2558521578688886, 0.26036961205930925, 0.2570841893523136, 0.25379876703697063, 0.256262831533714, 0.26036961166765654, 0.2574948677536888, 0.2533880911996967, 0.25790554200599325, 0.25954825603986426, 0.2533880913771643, 0.25462012086560837, 0.2521560573479968, 0.25215605754382314, 0.2517453804765149, 0.2529774121924837, 0.2574948681453415, 0.2554414784883816, 0.24804928136312496, 0.2513347025035098, 0.25462012462302647, 0.2517453795341006, 0.25954825424070965, 0.25133470252186857, 0.2492813142173344, 0.2505133476590229, 0.2583162235773075, 0.25379876602112145, 0.2550308010844969, 0.25297741358162684, 0.2509240256687454, 0.2640657084311303, 0.25708418915648723, 0.26324435205675006, 0.2554414788800343, 0.2570841867698536, 0.2607802883065946, 0.26365503065395157, 0.2620123181866914, 0.26119096435805367, 0.26036961051105717, 0.26529774230118897, 0.2698151964916096, 0.26324435284005543, 0.2681724848443723, 0.2731006182194735, 0.2706365519235756, 0.2735112936834535, 0.2772073916586028, 0.27022587234724227, 0.27474332434685567, 0.2767967148238384, 0.27638603481913493, 0.277618070647457, 0.27720739005527456, 0.2809034912003629, 0.27926077892892903, 0.2673511307831907, 0.2850102661693855, 0.271868584777785, 0.28090349139618925, 0.28213552303872313, 0.27967145912945884, 0.2841889103457668, 0.2825462000693139, 0.28706365700130343, 0.28665297703331744, 0.28295687886234183, 0.2882956898922303, 0.28952772153476425, 0.2821355256211831, 0.2878850114908551, 0.2882956859389859, 0.28665297765751396, 0.28213552401785486, 0.2878850124699869, 0.27967145815032707, 0.2813141664317991, 0.2928131418918437, 0.2891170407834729, 0.2895277197723271, 0.2891170453241963, 0.2862423016060549, 0.2903490765750775, 0.2952772076002626, 0.2965092410419511, 0.2891170409792992, 0.29897330678708745, 0.2981519527259059, 0.302258725149186, 0.29733059392817457, 0.2948665291988874, 0.29774127154624436, 0.30225872730327585, 0.300616018201781, 0.2993839814310446, 0.29897330776621917, 0.30349076074496434, 0.2993839829976554, 0.29568788482667974, 0.30184804694363715, 0.291991785480746, 0.29815194994761957, 0.29691991748506286, 0.30431211421866683, 0.29240246227879296, 0.2981519497517932, 0.3026694055088247, 0.30718685832846093, 0.2997946624148798, 0.3018480492935533, 0.30431211598110397, 0.30349076074496434, 0.29691991611427837, 0.312936345373091, 0.29856263030725827, 0.30636550093823145, 0.3096509222377252, 0.3158110894828851, 0.3096509232535744, 0.3108829545411731, 0.3178644756149707, 0.30800821278129514, 0.307597538137338, 0.3108829555203048, 0.31252566818339134, 0.3117043141222098, 0.3174537972135955, 0.3215605741408816, 0.3158110885037534, 0.31252566951745836, 0.30841889157432306, 0.3236139610562726, 0.31909650788170113, 0.31622176768843396, 0.3215605729659235, 0.31745379956351166, 0.3260780303262832, 0.3232032854331837, 0.3219712521873216, 0.3227926091858983, 0.3215605743734254, 0.31950718644218523, 0.32689938282085396, 0.3182751542121723, 0.31498973385509277, 0.3236139616804691, 0.32320328382985547, 0.3240246394943652, 0.32484599371465567, 0.32402464125680236, 0.3273100600105536, 0.31704312351205266, 0.32854209466391765, 0.3137577021758414, 0.33511293733634007, 0.31416837662397223, 0.3252566743068382, 0.31909650690256935, 0.3371663254266892, 0.32648870457858764, 0.33100615974814007, 0.33100616037233654, 0.3264887055944368, 0.3367556474169667, 0.33798768203361323, 0.33880903409981383, 0.3310061615472946, 0.329363450879189, 0.3297741296722169, 0.3445585229435985, 0.34209445743100597, 0.3347022591307912, 0.33100616193894733, 0.33059548490835655, 0.3363449680364597, 0.32936345048753635, 0.3322381934223723, 0.34086242121103116, 0.33347022428160084, 0.3425051318791368, 0.33634497077802855, 0.33839835530678597, 0.3359342935516115, 0.3334702278431926, 0.33100615998068383, 0.336344967644807, 0.3347022571358103, 0.33264887162792117, 0.3305954809918296, 0.33470225889824745, 0.3359342905774988, 0.3408624239893175, 0.343326488914431, 0.33716632503503646, 0.338398355894265, 0.34332648636868845, 0.3359342929641324, 0.33429158131689507, 0.3396303913309344, 0.33839835651846145, 0.3388090335123348, 0.34496919719590297, 0.34250513524490217, 0.33963038796516903, 0.33921971034709925, 0.33839835550261227, 0.34784394032656535, 0.3511293622502556, 0.34579055658111335, 0.35112936362104, 0.3498973311584833, 0.34537987896304356, 0.3523613956919441, 0.3544147841739459, 0.34496919715918556, 0.3445585235310776, 0.34579055458613245, 0.34496920056166835, 0.3462012334158778, 0.3515400414349362, 0.33921971390869093, 0.35112936542019463, 0.35112936385358384, 0.3560574926512442, 0.35236139416205076, 0.3371663258183419, 0.34948665357713093, 0.34825461974378974, 0.34989733237015885, 0.35687885207317205, 0.34373716590830433, 0.35236139432115965, 0.3548254612045366, 0.3482546213104005, 0.35811088371570593, 0.3453798759889309, 0.3593429175490471, 0.350308010184055, 0.34496919876251375, 0.35030800998822864, 0.34579055658111335, 0.35893223836436655, 0.3548254625753211, 0.363860370172857, 0.3507186846321858, 0.3618069828658133, 0.36180698251087806, 0.3568788522689984, 0.36180697973259174, 0.35400410557674433, 0.3589322377768875, 0.35852156195797225]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
