{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf19.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 19:04:33 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'Split', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 15 Label(s): ['eo', 'yd', 'my', 'ce', 'sk', 'ck', 'aa', 'sg', 'ds', 'mb', 'ek', 'ib', 'by', 'eg', 'eb'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000226851C4240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002269F636EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6913, Accuracy:0.0742, Validation Loss:2.6691, Validation Accuracy:0.1067\n",
    "Epoch #2: Loss:2.6594, Accuracy:0.0976, Validation Loss:2.6427, Validation Accuracy:0.1026\n",
    "Epoch #3: Loss:2.6252, Accuracy:0.1260, Validation Loss:2.5977, Validation Accuracy:0.1494\n",
    "Epoch #4: Loss:2.5734, Accuracy:0.1622, Validation Loss:2.5512, Validation Accuracy:0.1576\n",
    "Epoch #5: Loss:2.5373, Accuracy:0.1607, Validation Loss:2.5187, Validation Accuracy:0.1576\n",
    "Epoch #6: Loss:2.5127, Accuracy:0.1688, Validation Loss:2.5065, Validation Accuracy:0.1646\n",
    "Epoch #7: Loss:2.4988, Accuracy:0.1724, Validation Loss:2.4951, Validation Accuracy:0.1605\n",
    "Epoch #8: Loss:2.4896, Accuracy:0.1692, Validation Loss:2.4876, Validation Accuracy:0.1691\n",
    "Epoch #9: Loss:2.4837, Accuracy:0.1716, Validation Loss:2.4797, Validation Accuracy:0.1683\n",
    "Epoch #10: Loss:2.4801, Accuracy:0.1709, Validation Loss:2.4736, Validation Accuracy:0.1638\n",
    "Epoch #11: Loss:2.4734, Accuracy:0.1689, Validation Loss:2.4782, Validation Accuracy:0.1679\n",
    "Epoch #12: Loss:2.4727, Accuracy:0.1732, Validation Loss:2.4667, Validation Accuracy:0.1716\n",
    "Epoch #13: Loss:2.4687, Accuracy:0.1732, Validation Loss:2.4649, Validation Accuracy:0.1712\n",
    "Epoch #14: Loss:2.4672, Accuracy:0.1732, Validation Loss:2.4608, Validation Accuracy:0.1741\n",
    "Epoch #15: Loss:2.4594, Accuracy:0.1748, Validation Loss:2.4584, Validation Accuracy:0.1728\n",
    "Epoch #16: Loss:2.4578, Accuracy:0.1744, Validation Loss:2.4552, Validation Accuracy:0.1732\n",
    "Epoch #17: Loss:2.4569, Accuracy:0.1753, Validation Loss:2.4536, Validation Accuracy:0.1741\n",
    "Epoch #18: Loss:2.4547, Accuracy:0.1741, Validation Loss:2.4527, Validation Accuracy:0.1691\n",
    "Epoch #19: Loss:2.4530, Accuracy:0.1754, Validation Loss:2.4542, Validation Accuracy:0.1646\n",
    "Epoch #20: Loss:2.4567, Accuracy:0.1714, Validation Loss:2.4505, Validation Accuracy:0.1675\n",
    "Epoch #21: Loss:2.4508, Accuracy:0.1728, Validation Loss:2.4514, Validation Accuracy:0.1724\n",
    "Epoch #22: Loss:2.4517, Accuracy:0.1745, Validation Loss:2.4478, Validation Accuracy:0.1732\n",
    "Epoch #23: Loss:2.4509, Accuracy:0.1791, Validation Loss:2.4481, Validation Accuracy:0.1794\n",
    "Epoch #24: Loss:2.4479, Accuracy:0.1805, Validation Loss:2.4440, Validation Accuracy:0.1761\n",
    "Epoch #25: Loss:2.4465, Accuracy:0.1819, Validation Loss:2.4457, Validation Accuracy:0.1773\n",
    "Epoch #26: Loss:2.4460, Accuracy:0.1785, Validation Loss:2.4436, Validation Accuracy:0.1790\n",
    "Epoch #27: Loss:2.4440, Accuracy:0.1804, Validation Loss:2.4411, Validation Accuracy:0.1790\n",
    "Epoch #28: Loss:2.4452, Accuracy:0.1770, Validation Loss:2.4414, Validation Accuracy:0.1806\n",
    "Epoch #29: Loss:2.4428, Accuracy:0.1814, Validation Loss:2.4420, Validation Accuracy:0.1823\n",
    "Epoch #30: Loss:2.4447, Accuracy:0.1777, Validation Loss:2.4411, Validation Accuracy:0.1732\n",
    "Epoch #31: Loss:2.4470, Accuracy:0.1764, Validation Loss:2.4456, Validation Accuracy:0.1802\n",
    "Epoch #32: Loss:2.4442, Accuracy:0.1772, Validation Loss:2.4437, Validation Accuracy:0.1769\n",
    "Epoch #33: Loss:2.4435, Accuracy:0.1789, Validation Loss:2.4441, Validation Accuracy:0.1778\n",
    "Epoch #34: Loss:2.4438, Accuracy:0.1768, Validation Loss:2.4419, Validation Accuracy:0.1712\n",
    "Epoch #35: Loss:2.4415, Accuracy:0.1767, Validation Loss:2.4414, Validation Accuracy:0.1749\n",
    "Epoch #36: Loss:2.4447, Accuracy:0.1756, Validation Loss:2.4427, Validation Accuracy:0.1728\n",
    "Epoch #37: Loss:2.4416, Accuracy:0.1782, Validation Loss:2.4426, Validation Accuracy:0.1778\n",
    "Epoch #38: Loss:2.4406, Accuracy:0.1757, Validation Loss:2.4446, Validation Accuracy:0.1728\n",
    "Epoch #39: Loss:2.4386, Accuracy:0.1748, Validation Loss:2.4410, Validation Accuracy:0.1745\n",
    "Epoch #40: Loss:2.4387, Accuracy:0.1762, Validation Loss:2.4405, Validation Accuracy:0.1773\n",
    "Epoch #41: Loss:2.4384, Accuracy:0.1798, Validation Loss:2.4408, Validation Accuracy:0.1814\n",
    "Epoch #42: Loss:2.4383, Accuracy:0.1786, Validation Loss:2.4415, Validation Accuracy:0.1769\n",
    "Epoch #43: Loss:2.4389, Accuracy:0.1811, Validation Loss:2.4403, Validation Accuracy:0.1741\n",
    "Epoch #44: Loss:2.4393, Accuracy:0.1757, Validation Loss:2.4404, Validation Accuracy:0.1761\n",
    "Epoch #45: Loss:2.4373, Accuracy:0.1797, Validation Loss:2.4397, Validation Accuracy:0.1851\n",
    "Epoch #46: Loss:2.4407, Accuracy:0.1823, Validation Loss:2.4393, Validation Accuracy:0.1790\n",
    "Epoch #47: Loss:2.4397, Accuracy:0.1817, Validation Loss:2.4385, Validation Accuracy:0.1773\n",
    "Epoch #48: Loss:2.4388, Accuracy:0.1783, Validation Loss:2.4384, Validation Accuracy:0.1790\n",
    "Epoch #49: Loss:2.4362, Accuracy:0.1800, Validation Loss:2.4383, Validation Accuracy:0.1814\n",
    "Epoch #50: Loss:2.4366, Accuracy:0.1797, Validation Loss:2.4397, Validation Accuracy:0.1831\n",
    "Epoch #51: Loss:2.4351, Accuracy:0.1817, Validation Loss:2.4381, Validation Accuracy:0.1794\n",
    "Epoch #52: Loss:2.4342, Accuracy:0.1806, Validation Loss:2.4404, Validation Accuracy:0.1827\n",
    "Epoch #53: Loss:2.4410, Accuracy:0.1854, Validation Loss:2.4387, Validation Accuracy:0.1831\n",
    "Epoch #54: Loss:2.4347, Accuracy:0.1811, Validation Loss:2.4366, Validation Accuracy:0.1810\n",
    "Epoch #55: Loss:2.4343, Accuracy:0.1822, Validation Loss:2.4398, Validation Accuracy:0.1810\n",
    "Epoch #56: Loss:2.4352, Accuracy:0.1785, Validation Loss:2.4387, Validation Accuracy:0.1765\n",
    "Epoch #57: Loss:2.4344, Accuracy:0.1797, Validation Loss:2.4420, Validation Accuracy:0.1806\n",
    "Epoch #58: Loss:2.4357, Accuracy:0.1803, Validation Loss:2.4383, Validation Accuracy:0.1778\n",
    "Epoch #59: Loss:2.4350, Accuracy:0.1798, Validation Loss:2.4357, Validation Accuracy:0.1847\n",
    "Epoch #60: Loss:2.4330, Accuracy:0.1826, Validation Loss:2.4351, Validation Accuracy:0.1741\n",
    "Epoch #61: Loss:2.4320, Accuracy:0.1825, Validation Loss:2.4347, Validation Accuracy:0.1757\n",
    "Epoch #62: Loss:2.4328, Accuracy:0.1833, Validation Loss:2.4357, Validation Accuracy:0.1827\n",
    "Epoch #63: Loss:2.4400, Accuracy:0.1820, Validation Loss:2.4392, Validation Accuracy:0.1839\n",
    "Epoch #64: Loss:2.4344, Accuracy:0.1829, Validation Loss:2.4399, Validation Accuracy:0.1847\n",
    "Epoch #65: Loss:2.4366, Accuracy:0.1760, Validation Loss:2.4351, Validation Accuracy:0.1798\n",
    "Epoch #66: Loss:2.4319, Accuracy:0.1800, Validation Loss:2.4347, Validation Accuracy:0.1814\n",
    "Epoch #67: Loss:2.4329, Accuracy:0.1814, Validation Loss:2.4404, Validation Accuracy:0.1724\n",
    "Epoch #68: Loss:2.4351, Accuracy:0.1786, Validation Loss:2.4405, Validation Accuracy:0.1769\n",
    "Epoch #69: Loss:2.4354, Accuracy:0.1818, Validation Loss:2.5152, Validation Accuracy:0.1679\n",
    "Epoch #70: Loss:2.4409, Accuracy:0.1808, Validation Loss:2.4382, Validation Accuracy:0.1819\n",
    "Epoch #71: Loss:2.4334, Accuracy:0.1839, Validation Loss:2.4365, Validation Accuracy:0.1778\n",
    "Epoch #72: Loss:2.4359, Accuracy:0.1808, Validation Loss:2.4341, Validation Accuracy:0.1782\n",
    "Epoch #73: Loss:2.4341, Accuracy:0.1806, Validation Loss:2.4328, Validation Accuracy:0.1819\n",
    "Epoch #74: Loss:2.4331, Accuracy:0.1801, Validation Loss:2.4337, Validation Accuracy:0.1786\n",
    "Epoch #75: Loss:2.4323, Accuracy:0.1840, Validation Loss:2.4339, Validation Accuracy:0.1761\n",
    "Epoch #76: Loss:2.4317, Accuracy:0.1839, Validation Loss:2.4345, Validation Accuracy:0.1802\n",
    "Epoch #77: Loss:2.4301, Accuracy:0.1844, Validation Loss:2.4341, Validation Accuracy:0.1769\n",
    "Epoch #78: Loss:2.4298, Accuracy:0.1840, Validation Loss:2.4341, Validation Accuracy:0.1773\n",
    "Epoch #79: Loss:2.4305, Accuracy:0.1841, Validation Loss:2.4361, Validation Accuracy:0.1786\n",
    "Epoch #80: Loss:2.4312, Accuracy:0.1826, Validation Loss:2.4335, Validation Accuracy:0.1802\n",
    "Epoch #81: Loss:2.4300, Accuracy:0.1837, Validation Loss:2.4346, Validation Accuracy:0.1782\n",
    "Epoch #82: Loss:2.4324, Accuracy:0.1821, Validation Loss:2.4351, Validation Accuracy:0.1802\n",
    "Epoch #83: Loss:2.4289, Accuracy:0.1841, Validation Loss:2.4346, Validation Accuracy:0.1778\n",
    "Epoch #84: Loss:2.4284, Accuracy:0.1852, Validation Loss:2.4313, Validation Accuracy:0.1839\n",
    "Epoch #85: Loss:2.4299, Accuracy:0.1848, Validation Loss:2.4328, Validation Accuracy:0.1790\n",
    "Epoch #86: Loss:2.4302, Accuracy:0.1861, Validation Loss:2.4350, Validation Accuracy:0.1806\n",
    "Epoch #87: Loss:2.4294, Accuracy:0.1836, Validation Loss:2.4345, Validation Accuracy:0.1786\n",
    "Epoch #88: Loss:2.4300, Accuracy:0.1816, Validation Loss:2.4323, Validation Accuracy:0.1843\n",
    "Epoch #89: Loss:2.4293, Accuracy:0.1859, Validation Loss:2.4328, Validation Accuracy:0.1798\n",
    "Epoch #90: Loss:2.4293, Accuracy:0.1858, Validation Loss:2.4346, Validation Accuracy:0.1753\n",
    "Epoch #91: Loss:2.4281, Accuracy:0.1855, Validation Loss:2.4335, Validation Accuracy:0.1782\n",
    "Epoch #92: Loss:2.4288, Accuracy:0.1837, Validation Loss:2.4348, Validation Accuracy:0.1794\n",
    "Epoch #93: Loss:2.4303, Accuracy:0.1858, Validation Loss:2.4363, Validation Accuracy:0.1753\n",
    "Epoch #94: Loss:2.4294, Accuracy:0.1822, Validation Loss:2.4347, Validation Accuracy:0.1831\n",
    "Epoch #95: Loss:2.4297, Accuracy:0.1837, Validation Loss:2.4338, Validation Accuracy:0.1872\n",
    "Epoch #96: Loss:2.4303, Accuracy:0.1815, Validation Loss:2.4334, Validation Accuracy:0.1761\n",
    "Epoch #97: Loss:2.4283, Accuracy:0.1826, Validation Loss:2.4320, Validation Accuracy:0.1732\n",
    "Epoch #98: Loss:2.4277, Accuracy:0.1762, Validation Loss:2.4314, Validation Accuracy:0.1814\n",
    "Epoch #99: Loss:2.4278, Accuracy:0.1800, Validation Loss:2.4328, Validation Accuracy:0.1790\n",
    "Epoch #100: Loss:2.4300, Accuracy:0.1830, Validation Loss:2.4325, Validation Accuracy:0.1827\n",
    "Epoch #101: Loss:2.4280, Accuracy:0.1871, Validation Loss:2.4308, Validation Accuracy:0.1835\n",
    "Epoch #102: Loss:2.4277, Accuracy:0.1822, Validation Loss:2.4294, Validation Accuracy:0.1823\n",
    "Epoch #103: Loss:2.4260, Accuracy:0.1872, Validation Loss:2.4298, Validation Accuracy:0.1765\n",
    "Epoch #104: Loss:2.4268, Accuracy:0.1861, Validation Loss:2.4301, Validation Accuracy:0.1810\n",
    "Epoch #105: Loss:2.4277, Accuracy:0.1867, Validation Loss:2.4337, Validation Accuracy:0.1810\n",
    "Epoch #106: Loss:2.4267, Accuracy:0.1862, Validation Loss:2.4275, Validation Accuracy:0.1856\n",
    "Epoch #107: Loss:2.4261, Accuracy:0.1852, Validation Loss:2.4296, Validation Accuracy:0.1827\n",
    "Epoch #108: Loss:2.4249, Accuracy:0.1882, Validation Loss:2.4277, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.4258, Accuracy:0.1859, Validation Loss:2.4281, Validation Accuracy:0.1847\n",
    "Epoch #110: Loss:2.4246, Accuracy:0.1893, Validation Loss:2.4292, Validation Accuracy:0.1794\n",
    "Epoch #111: Loss:2.4241, Accuracy:0.1885, Validation Loss:2.4311, Validation Accuracy:0.1794\n",
    "Epoch #112: Loss:2.4257, Accuracy:0.1848, Validation Loss:2.4308, Validation Accuracy:0.1827\n",
    "Epoch #113: Loss:2.4256, Accuracy:0.1872, Validation Loss:2.4305, Validation Accuracy:0.1814\n",
    "Epoch #114: Loss:2.4241, Accuracy:0.1872, Validation Loss:2.4300, Validation Accuracy:0.1778\n",
    "Epoch #115: Loss:2.4243, Accuracy:0.1846, Validation Loss:2.4333, Validation Accuracy:0.1827\n",
    "Epoch #116: Loss:2.4289, Accuracy:0.1803, Validation Loss:2.4287, Validation Accuracy:0.1810\n",
    "Epoch #117: Loss:2.4250, Accuracy:0.1843, Validation Loss:2.4279, Validation Accuracy:0.1856\n",
    "Epoch #118: Loss:2.4241, Accuracy:0.1853, Validation Loss:2.4276, Validation Accuracy:0.1823\n",
    "Epoch #119: Loss:2.4246, Accuracy:0.1867, Validation Loss:2.4287, Validation Accuracy:0.1778\n",
    "Epoch #120: Loss:2.4272, Accuracy:0.1846, Validation Loss:2.4424, Validation Accuracy:0.1835\n",
    "Epoch #121: Loss:2.4393, Accuracy:0.1814, Validation Loss:2.4358, Validation Accuracy:0.1757\n",
    "Epoch #122: Loss:2.4262, Accuracy:0.1843, Validation Loss:2.4253, Validation Accuracy:0.1794\n",
    "Epoch #123: Loss:2.4228, Accuracy:0.1860, Validation Loss:2.4231, Validation Accuracy:0.1794\n",
    "Epoch #124: Loss:2.4242, Accuracy:0.1859, Validation Loss:2.4238, Validation Accuracy:0.1790\n",
    "Epoch #125: Loss:2.4240, Accuracy:0.1848, Validation Loss:2.4275, Validation Accuracy:0.1786\n",
    "Epoch #126: Loss:2.4239, Accuracy:0.1833, Validation Loss:2.4257, Validation Accuracy:0.1765\n",
    "Epoch #127: Loss:2.4252, Accuracy:0.1844, Validation Loss:2.4320, Validation Accuracy:0.1773\n",
    "Epoch #128: Loss:2.4248, Accuracy:0.1856, Validation Loss:2.4283, Validation Accuracy:0.1773\n",
    "Epoch #129: Loss:2.4230, Accuracy:0.1826, Validation Loss:2.4305, Validation Accuracy:0.1773\n",
    "Epoch #130: Loss:2.4245, Accuracy:0.1825, Validation Loss:2.4279, Validation Accuracy:0.1745\n",
    "Epoch #131: Loss:2.4245, Accuracy:0.1833, Validation Loss:2.4268, Validation Accuracy:0.1790\n",
    "Epoch #132: Loss:2.4230, Accuracy:0.1867, Validation Loss:2.4264, Validation Accuracy:0.1778\n",
    "Epoch #133: Loss:2.4235, Accuracy:0.1823, Validation Loss:2.4307, Validation Accuracy:0.1827\n",
    "Epoch #134: Loss:2.4241, Accuracy:0.1831, Validation Loss:2.4299, Validation Accuracy:0.1814\n",
    "Epoch #135: Loss:2.4251, Accuracy:0.1872, Validation Loss:2.4312, Validation Accuracy:0.1749\n",
    "Epoch #136: Loss:2.4248, Accuracy:0.1852, Validation Loss:2.4274, Validation Accuracy:0.1827\n",
    "Epoch #137: Loss:2.4222, Accuracy:0.1888, Validation Loss:2.4275, Validation Accuracy:0.1876\n",
    "Epoch #138: Loss:2.4239, Accuracy:0.1847, Validation Loss:2.4261, Validation Accuracy:0.1790\n",
    "Epoch #139: Loss:2.4271, Accuracy:0.1870, Validation Loss:2.4815, Validation Accuracy:0.1667\n",
    "Epoch #140: Loss:2.4592, Accuracy:0.1765, Validation Loss:2.4508, Validation Accuracy:0.1831\n",
    "Epoch #141: Loss:2.4430, Accuracy:0.1772, Validation Loss:2.4384, Validation Accuracy:0.1675\n",
    "Epoch #142: Loss:2.4340, Accuracy:0.1802, Validation Loss:2.4328, Validation Accuracy:0.1753\n",
    "Epoch #143: Loss:2.4323, Accuracy:0.1767, Validation Loss:2.4320, Validation Accuracy:0.1831\n",
    "Epoch #144: Loss:2.4369, Accuracy:0.1802, Validation Loss:2.4327, Validation Accuracy:0.1831\n",
    "Epoch #145: Loss:2.4359, Accuracy:0.1759, Validation Loss:2.4296, Validation Accuracy:0.1864\n",
    "Epoch #146: Loss:2.4301, Accuracy:0.1792, Validation Loss:2.4319, Validation Accuracy:0.1831\n",
    "Epoch #147: Loss:2.4304, Accuracy:0.1829, Validation Loss:2.4301, Validation Accuracy:0.1876\n",
    "Epoch #148: Loss:2.4293, Accuracy:0.1807, Validation Loss:2.4360, Validation Accuracy:0.1831\n",
    "Epoch #149: Loss:2.4308, Accuracy:0.1776, Validation Loss:2.4321, Validation Accuracy:0.1888\n",
    "Epoch #150: Loss:2.4286, Accuracy:0.1846, Validation Loss:2.4305, Validation Accuracy:0.1880\n",
    "Epoch #151: Loss:2.4289, Accuracy:0.1847, Validation Loss:2.4347, Validation Accuracy:0.1843\n",
    "Epoch #152: Loss:2.4315, Accuracy:0.1844, Validation Loss:2.4323, Validation Accuracy:0.1843\n",
    "Epoch #153: Loss:2.4297, Accuracy:0.1799, Validation Loss:2.4313, Validation Accuracy:0.1823\n",
    "Epoch #154: Loss:2.4283, Accuracy:0.1821, Validation Loss:2.4280, Validation Accuracy:0.1716\n",
    "Epoch #155: Loss:2.4266, Accuracy:0.1888, Validation Loss:2.4282, Validation Accuracy:0.1847\n",
    "Epoch #156: Loss:2.4268, Accuracy:0.1845, Validation Loss:2.4274, Validation Accuracy:0.1806\n",
    "Epoch #157: Loss:2.4360, Accuracy:0.1795, Validation Loss:2.4509, Validation Accuracy:0.1749\n",
    "Epoch #158: Loss:2.4377, Accuracy:0.1781, Validation Loss:2.4338, Validation Accuracy:0.1757\n",
    "Epoch #159: Loss:2.4276, Accuracy:0.1799, Validation Loss:2.4287, Validation Accuracy:0.1786\n",
    "Epoch #160: Loss:2.4288, Accuracy:0.1806, Validation Loss:2.4301, Validation Accuracy:0.1864\n",
    "Epoch #161: Loss:2.4249, Accuracy:0.1831, Validation Loss:2.4297, Validation Accuracy:0.1847\n",
    "Epoch #162: Loss:2.4255, Accuracy:0.1802, Validation Loss:2.4293, Validation Accuracy:0.1880\n",
    "Epoch #163: Loss:2.4261, Accuracy:0.1800, Validation Loss:2.4280, Validation Accuracy:0.1876\n",
    "Epoch #164: Loss:2.4282, Accuracy:0.1834, Validation Loss:2.4345, Validation Accuracy:0.1839\n",
    "Epoch #165: Loss:2.4312, Accuracy:0.1838, Validation Loss:2.4317, Validation Accuracy:0.1782\n",
    "Epoch #166: Loss:2.4318, Accuracy:0.1813, Validation Loss:2.4288, Validation Accuracy:0.1843\n",
    "Epoch #167: Loss:2.4278, Accuracy:0.1803, Validation Loss:2.4281, Validation Accuracy:0.1802\n",
    "Epoch #168: Loss:2.4303, Accuracy:0.1806, Validation Loss:2.4273, Validation Accuracy:0.1786\n",
    "Epoch #169: Loss:2.4273, Accuracy:0.1825, Validation Loss:2.4273, Validation Accuracy:0.1835\n",
    "Epoch #170: Loss:2.4246, Accuracy:0.1791, Validation Loss:2.4273, Validation Accuracy:0.1786\n",
    "Epoch #171: Loss:2.4249, Accuracy:0.1808, Validation Loss:2.4304, Validation Accuracy:0.1819\n",
    "Epoch #172: Loss:2.4255, Accuracy:0.1835, Validation Loss:2.4281, Validation Accuracy:0.1794\n",
    "Epoch #173: Loss:2.4277, Accuracy:0.1826, Validation Loss:2.4286, Validation Accuracy:0.1786\n",
    "Epoch #174: Loss:2.4242, Accuracy:0.1832, Validation Loss:2.4255, Validation Accuracy:0.1827\n",
    "Epoch #175: Loss:2.4249, Accuracy:0.1817, Validation Loss:2.4275, Validation Accuracy:0.1823\n",
    "Epoch #176: Loss:2.4246, Accuracy:0.1797, Validation Loss:2.4278, Validation Accuracy:0.1884\n",
    "Epoch #177: Loss:2.4240, Accuracy:0.1848, Validation Loss:2.4283, Validation Accuracy:0.1823\n",
    "Epoch #178: Loss:2.4238, Accuracy:0.1837, Validation Loss:2.4296, Validation Accuracy:0.1814\n",
    "Epoch #179: Loss:2.4237, Accuracy:0.1825, Validation Loss:2.4258, Validation Accuracy:0.1847\n",
    "Epoch #180: Loss:2.4225, Accuracy:0.1861, Validation Loss:2.4280, Validation Accuracy:0.1819\n",
    "Epoch #181: Loss:2.4240, Accuracy:0.1870, Validation Loss:2.4297, Validation Accuracy:0.1814\n",
    "Epoch #182: Loss:2.4223, Accuracy:0.1846, Validation Loss:2.4305, Validation Accuracy:0.1835\n",
    "Epoch #183: Loss:2.4230, Accuracy:0.1864, Validation Loss:2.4302, Validation Accuracy:0.1798\n",
    "Epoch #184: Loss:2.4247, Accuracy:0.1858, Validation Loss:2.4244, Validation Accuracy:0.1757\n",
    "Epoch #185: Loss:2.4234, Accuracy:0.1870, Validation Loss:2.4330, Validation Accuracy:0.1892\n",
    "Epoch #186: Loss:2.4273, Accuracy:0.1832, Validation Loss:2.4336, Validation Accuracy:0.1790\n",
    "Epoch #187: Loss:2.4294, Accuracy:0.1782, Validation Loss:2.4364, Validation Accuracy:0.1856\n",
    "Epoch #188: Loss:2.4256, Accuracy:0.1855, Validation Loss:2.4325, Validation Accuracy:0.1847\n",
    "Epoch #189: Loss:2.4239, Accuracy:0.1880, Validation Loss:2.4313, Validation Accuracy:0.1814\n",
    "Epoch #190: Loss:2.4238, Accuracy:0.1863, Validation Loss:2.4332, Validation Accuracy:0.1851\n",
    "Epoch #191: Loss:2.4256, Accuracy:0.1851, Validation Loss:2.4327, Validation Accuracy:0.1695\n",
    "Epoch #192: Loss:2.4252, Accuracy:0.1795, Validation Loss:2.4321, Validation Accuracy:0.1847\n",
    "Epoch #193: Loss:2.4244, Accuracy:0.1789, Validation Loss:2.4307, Validation Accuracy:0.1806\n",
    "Epoch #194: Loss:2.4246, Accuracy:0.1848, Validation Loss:2.4274, Validation Accuracy:0.1835\n",
    "Epoch #195: Loss:2.4247, Accuracy:0.1809, Validation Loss:2.4321, Validation Accuracy:0.1864\n",
    "Epoch #196: Loss:2.4240, Accuracy:0.1822, Validation Loss:2.4294, Validation Accuracy:0.1860\n",
    "Epoch #197: Loss:2.4238, Accuracy:0.1819, Validation Loss:2.4296, Validation Accuracy:0.1732\n",
    "Epoch #198: Loss:2.4256, Accuracy:0.1782, Validation Loss:2.4285, Validation Accuracy:0.1769\n",
    "Epoch #199: Loss:2.4221, Accuracy:0.1837, Validation Loss:2.4296, Validation Accuracy:0.1753\n",
    "Epoch #200: Loss:2.4225, Accuracy:0.1864, Validation Loss:2.4329, Validation Accuracy:0.1782\n",
    "Epoch #201: Loss:2.4229, Accuracy:0.1852, Validation Loss:2.4304, Validation Accuracy:0.1847\n",
    "Epoch #202: Loss:2.4227, Accuracy:0.1850, Validation Loss:2.4292, Validation Accuracy:0.1851\n",
    "Epoch #203: Loss:2.4244, Accuracy:0.1840, Validation Loss:2.4286, Validation Accuracy:0.1884\n",
    "Epoch #204: Loss:2.4252, Accuracy:0.1830, Validation Loss:2.4292, Validation Accuracy:0.1843\n",
    "Epoch #205: Loss:2.4213, Accuracy:0.1872, Validation Loss:2.4295, Validation Accuracy:0.1884\n",
    "Epoch #206: Loss:2.4231, Accuracy:0.1852, Validation Loss:2.4285, Validation Accuracy:0.1876\n",
    "Epoch #207: Loss:2.4230, Accuracy:0.1828, Validation Loss:2.4315, Validation Accuracy:0.1765\n",
    "Epoch #208: Loss:2.4232, Accuracy:0.1833, Validation Loss:2.4293, Validation Accuracy:0.1810\n",
    "Epoch #209: Loss:2.4213, Accuracy:0.1825, Validation Loss:2.4286, Validation Accuracy:0.1757\n",
    "Epoch #210: Loss:2.4219, Accuracy:0.1818, Validation Loss:2.4287, Validation Accuracy:0.1728\n",
    "Epoch #211: Loss:2.4227, Accuracy:0.1854, Validation Loss:2.4295, Validation Accuracy:0.1704\n",
    "Epoch #212: Loss:2.4215, Accuracy:0.1826, Validation Loss:2.4273, Validation Accuracy:0.1892\n",
    "Epoch #213: Loss:2.4198, Accuracy:0.1861, Validation Loss:2.4276, Validation Accuracy:0.1819\n",
    "Epoch #214: Loss:2.4194, Accuracy:0.1856, Validation Loss:2.4256, Validation Accuracy:0.1786\n",
    "Epoch #215: Loss:2.4209, Accuracy:0.1860, Validation Loss:2.4267, Validation Accuracy:0.1880\n",
    "Epoch #216: Loss:2.4230, Accuracy:0.1817, Validation Loss:2.4335, Validation Accuracy:0.1765\n",
    "Epoch #217: Loss:2.4212, Accuracy:0.1816, Validation Loss:2.4309, Validation Accuracy:0.1798\n",
    "Epoch #218: Loss:2.4253, Accuracy:0.1805, Validation Loss:2.4295, Validation Accuracy:0.1732\n",
    "Epoch #219: Loss:2.4218, Accuracy:0.1828, Validation Loss:2.4301, Validation Accuracy:0.1831\n",
    "Epoch #220: Loss:2.4188, Accuracy:0.1830, Validation Loss:2.4259, Validation Accuracy:0.1860\n",
    "Epoch #221: Loss:2.4197, Accuracy:0.1836, Validation Loss:2.4257, Validation Accuracy:0.1860\n",
    "Epoch #222: Loss:2.4191, Accuracy:0.1880, Validation Loss:2.4295, Validation Accuracy:0.1913\n",
    "Epoch #223: Loss:2.4187, Accuracy:0.1857, Validation Loss:2.4264, Validation Accuracy:0.1810\n",
    "Epoch #224: Loss:2.4200, Accuracy:0.1800, Validation Loss:2.4269, Validation Accuracy:0.1757\n",
    "Epoch #225: Loss:2.4233, Accuracy:0.1843, Validation Loss:2.4294, Validation Accuracy:0.1905\n",
    "Epoch #226: Loss:2.4307, Accuracy:0.1833, Validation Loss:2.4385, Validation Accuracy:0.1814\n",
    "Epoch #227: Loss:2.4318, Accuracy:0.1850, Validation Loss:2.4324, Validation Accuracy:0.1843\n",
    "Epoch #228: Loss:2.4296, Accuracy:0.1795, Validation Loss:2.4324, Validation Accuracy:0.1769\n",
    "Epoch #229: Loss:2.4230, Accuracy:0.1814, Validation Loss:2.4273, Validation Accuracy:0.1839\n",
    "Epoch #230: Loss:2.4205, Accuracy:0.1828, Validation Loss:2.4288, Validation Accuracy:0.1798\n",
    "Epoch #231: Loss:2.4247, Accuracy:0.1850, Validation Loss:2.4298, Validation Accuracy:0.1888\n",
    "Epoch #232: Loss:2.4232, Accuracy:0.1846, Validation Loss:2.4294, Validation Accuracy:0.1827\n",
    "Epoch #233: Loss:2.4211, Accuracy:0.1842, Validation Loss:2.4263, Validation Accuracy:0.1827\n",
    "Epoch #234: Loss:2.4198, Accuracy:0.1850, Validation Loss:2.4281, Validation Accuracy:0.1851\n",
    "Epoch #235: Loss:2.4210, Accuracy:0.1824, Validation Loss:2.4280, Validation Accuracy:0.1847\n",
    "Epoch #236: Loss:2.4213, Accuracy:0.1844, Validation Loss:2.4280, Validation Accuracy:0.1847\n",
    "Epoch #237: Loss:2.4211, Accuracy:0.1806, Validation Loss:2.4274, Validation Accuracy:0.1802\n",
    "Epoch #238: Loss:2.4211, Accuracy:0.1847, Validation Loss:2.4279, Validation Accuracy:0.1831\n",
    "Epoch #239: Loss:2.4208, Accuracy:0.1832, Validation Loss:2.4274, Validation Accuracy:0.1806\n",
    "Epoch #240: Loss:2.4196, Accuracy:0.1853, Validation Loss:2.4303, Validation Accuracy:0.1831\n",
    "Epoch #241: Loss:2.4199, Accuracy:0.1858, Validation Loss:2.4276, Validation Accuracy:0.1773\n",
    "Epoch #242: Loss:2.4190, Accuracy:0.1822, Validation Loss:2.4287, Validation Accuracy:0.1847\n",
    "Epoch #243: Loss:2.4185, Accuracy:0.1862, Validation Loss:2.4273, Validation Accuracy:0.1823\n",
    "Epoch #244: Loss:2.4209, Accuracy:0.1834, Validation Loss:2.4257, Validation Accuracy:0.1794\n",
    "Epoch #245: Loss:2.4174, Accuracy:0.1844, Validation Loss:2.4270, Validation Accuracy:0.1749\n",
    "Epoch #246: Loss:2.4186, Accuracy:0.1874, Validation Loss:2.4276, Validation Accuracy:0.1732\n",
    "Epoch #247: Loss:2.4197, Accuracy:0.1842, Validation Loss:2.4284, Validation Accuracy:0.1806\n",
    "Epoch #248: Loss:2.4187, Accuracy:0.1836, Validation Loss:2.4263, Validation Accuracy:0.1736\n",
    "Epoch #249: Loss:2.4186, Accuracy:0.1853, Validation Loss:2.4285, Validation Accuracy:0.1851\n",
    "Epoch #250: Loss:2.4165, Accuracy:0.1847, Validation Loss:2.4254, Validation Accuracy:0.1814\n",
    "Epoch #251: Loss:2.4142, Accuracy:0.1848, Validation Loss:2.4246, Validation Accuracy:0.1728\n",
    "Epoch #252: Loss:2.4149, Accuracy:0.1855, Validation Loss:2.4244, Validation Accuracy:0.1839\n",
    "Epoch #253: Loss:2.4155, Accuracy:0.1881, Validation Loss:2.4229, Validation Accuracy:0.1761\n",
    "Epoch #254: Loss:2.4159, Accuracy:0.1844, Validation Loss:2.4264, Validation Accuracy:0.1806\n",
    "Epoch #255: Loss:2.4145, Accuracy:0.1842, Validation Loss:2.4223, Validation Accuracy:0.1856\n",
    "Epoch #256: Loss:2.4155, Accuracy:0.1858, Validation Loss:2.4216, Validation Accuracy:0.1905\n",
    "Epoch #257: Loss:2.4140, Accuracy:0.1881, Validation Loss:2.4241, Validation Accuracy:0.1823\n",
    "Epoch #258: Loss:2.4155, Accuracy:0.1867, Validation Loss:2.4229, Validation Accuracy:0.1823\n",
    "Epoch #259: Loss:2.4161, Accuracy:0.1867, Validation Loss:2.4297, Validation Accuracy:0.1761\n",
    "Epoch #260: Loss:2.4161, Accuracy:0.1858, Validation Loss:2.4231, Validation Accuracy:0.1868\n",
    "Epoch #261: Loss:2.4176, Accuracy:0.1855, Validation Loss:2.4299, Validation Accuracy:0.1819\n",
    "Epoch #262: Loss:2.4154, Accuracy:0.1847, Validation Loss:2.4223, Validation Accuracy:0.1823\n",
    "Epoch #263: Loss:2.4138, Accuracy:0.1905, Validation Loss:2.4230, Validation Accuracy:0.1814\n",
    "Epoch #264: Loss:2.4155, Accuracy:0.1854, Validation Loss:2.4310, Validation Accuracy:0.1802\n",
    "Epoch #265: Loss:2.4188, Accuracy:0.1848, Validation Loss:2.4258, Validation Accuracy:0.1782\n",
    "Epoch #266: Loss:2.4165, Accuracy:0.1830, Validation Loss:2.4239, Validation Accuracy:0.1810\n",
    "Epoch #267: Loss:2.4142, Accuracy:0.1871, Validation Loss:2.4225, Validation Accuracy:0.1847\n",
    "Epoch #268: Loss:2.4139, Accuracy:0.1852, Validation Loss:2.4249, Validation Accuracy:0.1773\n",
    "Epoch #269: Loss:2.4160, Accuracy:0.1894, Validation Loss:2.4240, Validation Accuracy:0.1860\n",
    "Epoch #270: Loss:2.4159, Accuracy:0.1880, Validation Loss:2.4237, Validation Accuracy:0.1835\n",
    "Epoch #271: Loss:2.4153, Accuracy:0.1893, Validation Loss:2.4263, Validation Accuracy:0.1892\n",
    "Epoch #272: Loss:2.4157, Accuracy:0.1898, Validation Loss:2.4231, Validation Accuracy:0.1782\n",
    "Epoch #273: Loss:2.4142, Accuracy:0.1888, Validation Loss:2.4249, Validation Accuracy:0.1847\n",
    "Epoch #274: Loss:2.4149, Accuracy:0.1870, Validation Loss:2.4248, Validation Accuracy:0.1761\n",
    "Epoch #275: Loss:2.4177, Accuracy:0.1851, Validation Loss:2.4205, Validation Accuracy:0.1798\n",
    "Epoch #276: Loss:2.4143, Accuracy:0.1883, Validation Loss:2.4222, Validation Accuracy:0.1823\n",
    "Epoch #277: Loss:2.4148, Accuracy:0.1884, Validation Loss:2.4259, Validation Accuracy:0.1736\n",
    "Epoch #278: Loss:2.4145, Accuracy:0.1885, Validation Loss:2.4213, Validation Accuracy:0.1769\n",
    "Epoch #279: Loss:2.4177, Accuracy:0.1880, Validation Loss:2.4219, Validation Accuracy:0.1773\n",
    "Epoch #280: Loss:2.4158, Accuracy:0.1862, Validation Loss:2.4225, Validation Accuracy:0.1773\n",
    "Epoch #281: Loss:2.4179, Accuracy:0.1822, Validation Loss:2.4256, Validation Accuracy:0.1753\n",
    "Epoch #282: Loss:2.4170, Accuracy:0.1862, Validation Loss:2.4226, Validation Accuracy:0.1765\n",
    "Epoch #283: Loss:2.4173, Accuracy:0.1868, Validation Loss:2.4247, Validation Accuracy:0.1769\n",
    "Epoch #284: Loss:2.4171, Accuracy:0.1864, Validation Loss:2.4234, Validation Accuracy:0.1753\n",
    "Epoch #285: Loss:2.4161, Accuracy:0.1881, Validation Loss:2.4220, Validation Accuracy:0.1802\n",
    "Epoch #286: Loss:2.4142, Accuracy:0.1898, Validation Loss:2.4220, Validation Accuracy:0.1761\n",
    "Epoch #287: Loss:2.4129, Accuracy:0.1867, Validation Loss:2.4220, Validation Accuracy:0.1773\n",
    "Epoch #288: Loss:2.4135, Accuracy:0.1856, Validation Loss:2.4199, Validation Accuracy:0.1745\n",
    "Epoch #289: Loss:2.4138, Accuracy:0.1880, Validation Loss:2.4198, Validation Accuracy:0.1843\n",
    "Epoch #290: Loss:2.4147, Accuracy:0.1889, Validation Loss:2.4252, Validation Accuracy:0.1827\n",
    "Epoch #291: Loss:2.4156, Accuracy:0.1880, Validation Loss:2.4222, Validation Accuracy:0.1798\n",
    "Epoch #292: Loss:2.4170, Accuracy:0.1869, Validation Loss:2.4257, Validation Accuracy:0.1741\n",
    "Epoch #293: Loss:2.4162, Accuracy:0.1881, Validation Loss:2.4200, Validation Accuracy:0.1884\n",
    "Epoch #294: Loss:2.4156, Accuracy:0.1906, Validation Loss:2.4249, Validation Accuracy:0.1884\n",
    "Epoch #295: Loss:2.4138, Accuracy:0.1906, Validation Loss:2.4223, Validation Accuracy:0.1831\n",
    "Epoch #296: Loss:2.4134, Accuracy:0.1899, Validation Loss:2.4249, Validation Accuracy:0.1876\n",
    "Epoch #297: Loss:2.4153, Accuracy:0.1916, Validation Loss:2.4214, Validation Accuracy:0.1814\n",
    "Epoch #298: Loss:2.4142, Accuracy:0.1931, Validation Loss:2.4219, Validation Accuracy:0.1839\n",
    "Epoch #299: Loss:2.4146, Accuracy:0.1887, Validation Loss:2.4227, Validation Accuracy:0.1856\n",
    "Epoch #300: Loss:2.4146, Accuracy:0.1899, Validation Loss:2.4240, Validation Accuracy:0.1880\n",
    "\n",
    "Test:\n",
    "Test Loss:2.42402959, Accuracy:0.1880\n",
    "Labels: ['eo', 'yd', 'my', 'ce', 'sk', 'ck', 'aa', 'sg', 'ds', 'mb', 'ek', 'ib', 'by', 'eg', 'eb']\n",
    "Confusion Matrix:\n",
    "      eo   yd  my  ce  sk  ck  aa   sg  ds  mb  ek  ib  by   eg  eb\n",
    "t:eo   0    5   0   0   0   0   0   75   0   0   7  11  10   20   7\n",
    "t:yd   0  102   0   0   0   0   1   76   2   0   7  51   0    8   2\n",
    "t:my   0   14   0   0   0   0   2   18   7   0   3  16   1   19   0\n",
    "t:ce   0    9   0   0   0   0   3   47   4   0   1   6   5   31   3\n",
    "t:sk   0    7   0   0   0   0   0   41   7   0   8   5   6   49   7\n",
    "t:ck   0    3   0   0   0   0   4   23   4   0   2   5   7   39   4\n",
    "t:aa   0    2   0   0   0   0   9   17  14   0  10   5   2   72   6\n",
    "t:sg   0   23   0   0   0   0   0  110   0   0  10  40   4   13   3\n",
    "t:ds   0    1   0   0   0   0   9   33  26   0   3   3   4   45   2\n",
    "t:mb   0   22   0   0   0   0   3   77   5   0  18  29   1   39  13\n",
    "t:ek   0   10   0   0   0   0   7   70   5   0  13  17   2   63   4\n",
    "t:ib   0   74   0   0   0   0   2   58   1   0   6  62   2   10   2\n",
    "t:by   0    4   0   0   0   0   5   72   6   0  11  11  12   29  12\n",
    "t:eg   0    3   0   0   0   0  10   34  15   0  10   4   0  114   8\n",
    "t:eb   0   21   0   0   0   0   4   50   1   0  19  17   7   72  10\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eo       0.00      0.00      0.00       135\n",
    "          yd       0.34      0.41      0.37       249\n",
    "          my       0.00      0.00      0.00        80\n",
    "          ce       0.00      0.00      0.00       109\n",
    "          sk       0.00      0.00      0.00       130\n",
    "          ck       0.00      0.00      0.00        91\n",
    "          aa       0.15      0.07      0.09       137\n",
    "          sg       0.14      0.54      0.22       203\n",
    "          ds       0.27      0.21      0.23       126\n",
    "          mb       0.00      0.00      0.00       207\n",
    "          ek       0.10      0.07      0.08       191\n",
    "          ib       0.22      0.29      0.25       217\n",
    "          by       0.19      0.07      0.11       162\n",
    "          eg       0.18      0.58      0.28       198\n",
    "          eb       0.12      0.05      0.07       201\n",
    "\n",
    "    accuracy                           0.19      2436\n",
    "   macro avg       0.11      0.15      0.11      2436\n",
    "weighted avg       0.13      0.19      0.14      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 20:17:46 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 13 minutes, 12 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.669080952705421, 2.6427249035420286, 2.597684486354709, 2.551183727574466, 2.5186942273564332, 2.5065194648279148, 2.4950862224466106, 2.4876416203227927, 2.479668807122116, 2.473564292801229, 2.478158609816202, 2.466660268592521, 2.4648553197607033, 2.460812112184972, 2.458377290046078, 2.4551675405799855, 2.453589614388978, 2.452652210085263, 2.4542167343333827, 2.4505352977852906, 2.4513537343499694, 2.4478054461612295, 2.448134901488356, 2.4439534733839614, 2.4457193227432827, 2.4436078396532532, 2.441105412340712, 2.441441588801116, 2.4420107542391873, 2.441067840861178, 2.4456128807882176, 2.4436515830029015, 2.4440661835161532, 2.441948621731086, 2.441391408932816, 2.4427210082757256, 2.442562423120383, 2.4446036385002197, 2.4410436803288453, 2.4405380317143033, 2.4407670165126154, 2.441484032006099, 2.4403115896560093, 2.4403683685121083, 2.439651047654927, 2.4393219094362557, 2.438471266005818, 2.4383951208274355, 2.438280874089459, 2.439740256135687, 2.4380907683536925, 2.4404011332538524, 2.438694382340254, 2.4366055529301587, 2.439762968147917, 2.438672539048594, 2.442048947799382, 2.438348639774792, 2.4356880994461636, 2.435090681993707, 2.434662470480883, 2.4357365074220354, 2.4392021035130194, 2.439947481813102, 2.4350515163590756, 2.434712711421923, 2.4404399445882965, 2.44053969516347, 2.5151892005907883, 2.4382209973577993, 2.4364776411667246, 2.434092863831418, 2.432792385223464, 2.433748712289118, 2.4338791699245057, 2.434530989877109, 2.434079256355273, 2.4340670140114518, 2.4360959060086405, 2.4335108347518495, 2.434636098410696, 2.435074006395387, 2.4345847624667565, 2.4313200205221945, 2.432836489528662, 2.435024502633632, 2.4345145734464397, 2.4323365672664297, 2.432827392822416, 2.434616028968924, 2.433476155223126, 2.4347576178940646, 2.436296820444818, 2.4347295487064056, 2.4338387816606093, 2.4334205997792764, 2.4319822893941345, 2.4313714880074184, 2.4328003528670137, 2.432534294175397, 2.4307582276599553, 2.429439747470549, 2.429767321679001, 2.430051272334332, 2.4336713166855435, 2.4275135539826893, 2.4295828373757096, 2.4276827759734907, 2.428104709913382, 2.429238107208352, 2.4310894818924527, 2.4308261847848374, 2.4305301123651963, 2.4300161664709083, 2.433325668861126, 2.4287339039819775, 2.427883239410977, 2.427634738348975, 2.4286569542876997, 2.4424017920282672, 2.4358186396863464, 2.4253143520386544, 2.4231105927371823, 2.4238310431807695, 2.4275481806600037, 2.425740142173955, 2.4319960131433795, 2.428334598275045, 2.430489362754258, 2.4279233180047646, 2.426782148420713, 2.426433385886582, 2.430718671316388, 2.429944468249241, 2.4311893600939922, 2.4274059285475507, 2.427457650502523, 2.4261059506577616, 2.481541931139816, 2.45080529488562, 2.438398623897133, 2.432784115739644, 2.4320390768630555, 2.4326683570598733, 2.4296233747980276, 2.431894990219467, 2.4300680125288188, 2.435961052701978, 2.432059620793034, 2.4304806410972706, 2.4346744438697554, 2.4323186161874353, 2.431276149154688, 2.428043885379785, 2.4281667770423327, 2.4273844321177314, 2.4508701686201424, 2.433799649302791, 2.428663175485796, 2.430107642081375, 2.4297309090155492, 2.4292952611137104, 2.427956652367252, 2.4344698298349363, 2.4317216446442753, 2.4288161928430565, 2.428117489384117, 2.4272678751859367, 2.427259438926559, 2.427299818969125, 2.430398698706541, 2.4280940903035684, 2.4285937451768196, 2.425547143312902, 2.4274837277792947, 2.427781352464397, 2.4283496910715336, 2.429559048369209, 2.4258030635382744, 2.4280180457386087, 2.4296582371534776, 2.4305277485369854, 2.4302148353094344, 2.4244474984937896, 2.4329885982331776, 2.433625555586541, 2.4364222333153283, 2.4325028984808963, 2.431348849204178, 2.4332251811066676, 2.432658947943075, 2.4320513211643364, 2.4307015114425634, 2.4274312451555224, 2.4321447734175057, 2.429399755004983, 2.4296410815860643, 2.4284751039420445, 2.4296271761845682, 2.4328833710775393, 2.4304329906582636, 2.429223183927865, 2.428599056156202, 2.429197834434572, 2.4295356504631354, 2.428488177031719, 2.4314606150578593, 2.4293191264611353, 2.42862846894413, 2.428740591838442, 2.4295345488048734, 2.427281827957955, 2.427564566554303, 2.4256230144469413, 2.4266520650516004, 2.4334502568581615, 2.430900728369777, 2.4294505741795884, 2.430053014473375, 2.425946817413731, 2.425711341287898, 2.4294861960293623, 2.4263829335399056, 2.4268759878593906, 2.429358885206025, 2.43849902160845, 2.4324424082813985, 2.432369308127167, 2.4272872035335045, 2.428782133437534, 2.429813761624992, 2.429413325680888, 2.4263402871506163, 2.428133874495433, 2.4280188205011175, 2.427971892756194, 2.4273678500859805, 2.427919645810558, 2.4274234391981353, 2.430261585317026, 2.4276366425656724, 2.4286587477121837, 2.427328714987719, 2.425712826999733, 2.4270415188643732, 2.4276202601948005, 2.428350559792104, 2.4263064481550445, 2.4285108776906834, 2.425400996638832, 2.4246208714734157, 2.424421884743451, 2.4228975252173415, 2.4263578569164808, 2.4222636077987345, 2.4215945614186807, 2.424058335951005, 2.4229026568934247, 2.4297281333378384, 2.423121418663238, 2.429949617542461, 2.422347204242825, 2.4230078253252754, 2.4309871823133897, 2.4257988209403405, 2.423915855207271, 2.4225119655746936, 2.424877162833128, 2.4239970056098477, 2.423650076040885, 2.426321993516192, 2.4231379603713212, 2.424877643585205, 2.4248382775067108, 2.4204886241499426, 2.4222289310104546, 2.4258521425312964, 2.421284455188194, 2.4219223180623675, 2.4224557923565944, 2.4256243596131775, 2.422555309211092, 2.4246600085291368, 2.4233505490965443, 2.422038670243888, 2.4219740932602405, 2.4219605813081237, 2.419910076999508, 2.419828928554391, 2.4251970900299122, 2.4221858257926354, 2.4256877832616297, 2.4200153499597006, 2.424892672568511, 2.4222876700665954, 2.4248511427141763, 2.421439854382294, 2.4219288845563365, 2.4227229848088108, 2.424029657210427], 'val_acc': [0.10673234797708311, 0.10262725776296923, 0.149425287270683, 0.1576354668547563, 0.15763546695262928, 0.16461412036066572, 0.16050903009761536, 0.16912972084162467, 0.16830870166592216, 0.16379310234720484, 0.16789819258578698, 0.1715927740133846, 0.1711822659486816, 0.17405582805376726, 0.17282430098463944, 0.1732348099913699, 0.17405582817610848, 0.16912971975278776, 0.16461412043407045, 0.1674876835545883, 0.17241379197790901, 0.17323481006477462, 0.17939244662159182, 0.1761083732097607, 0.17733990144113015, 0.17898193759039313, 0.1789819364281515, 0.18062397364178315, 0.18226600872667748, 0.17323481112914327, 0.1802134635462158, 0.17692939127215807, 0.17775041049679707, 0.1711822648598447, 0.17487684727840627, 0.1728243010580442, 0.17775040938349193, 0.17282430207347635, 0.17446633710943418, 0.17733990035229324, 0.1814449906642801, 0.17692939132109456, 0.17405582919154067, 0.17610837437200233, 0.18513957194506828, 0.1789819375659249, 0.17733990035229324, 0.178981936477088, 0.1814449906642801, 0.18308702686247957, 0.17939244555722317, 0.1826765188711813, 0.18308702678907485, 0.18103448163308142, 0.18103448165754968, 0.1765188822654276, 0.18062397371518787, 0.1777504093590237, 0.18472906298727435, 0.17405582815164025, 0.17569786420303024, 0.18267651778234442, 0.18390804487594048, 0.18472906298727435, 0.1798029545639536, 0.1814449907621531, 0.17241379304227766, 0.1769293924343997, 0.16789819268365994, 0.1818555007843157, 0.17775040943242842, 0.17816091843915885, 0.1818554996465423, 0.1785714285347262, 0.1761083732097607, 0.18021346352174755, 0.1769293923854632, 0.17733990030335675, 0.1785714285347262, 0.18021346357068405, 0.17816091839022238, 0.18021346359515228, 0.17775040940796016, 0.18390804485147222, 0.178981936477088, 0.180623972626351, 0.17857142742142107, 0.18431855390713917, 0.17980295451501713, 0.17528735522076805, 0.17816091841469062, 0.1793924455816914, 0.17528735514736332, 0.18308702674013835, 0.1871921171010617, 0.1761083732586972, 0.17323481018711584, 0.1814449906153436, 0.1789819364281515, 0.18267651884671307, 0.18349753582027353, 0.18226600983998262, 0.1765188823143641, 0.1810344816820179, 0.18103448160861318, 0.18555008104967172, 0.18267651778234442, 0.18226600875114574, 0.18472906296280608, 0.1793924456306279, 0.17939244545935018, 0.18267651780681265, 0.1814449906642801, 0.17775040933455544, 0.18267651778234442, 0.18103448274638656, 0.18555008104967172, 0.18226600872667748, 0.17775040938349193, 0.18349753591814652, 0.1756978642274985, 0.17939244548381844, 0.17939244560615966, 0.178981936477088, 0.1785714273969528, 0.17651888228989587, 0.177339900327825, 0.17733990037676148, 0.17733990037676148, 0.17446633713390244, 0.17898193645261976, 0.17775040938349193, 0.18267651773340793, 0.1814449906153436, 0.17487684623850586, 0.18267651775787616, 0.18760262719662907, 0.17898193655049272, 0.16666666551665915, 0.1830870268135431, 0.16748768357905652, 0.17528735630960496, 0.18308702790238, 0.18308702674013835, 0.18637110012750124, 0.18308702790238, 0.1876026261322604, 0.18308702790238, 0.18883415424128863, 0.18801313627676425, 0.18431855494703958, 0.18431855385820267, 0.18226600875114574, 0.17159277393997988, 0.18472906288940136, 0.18062397371518787, 0.17487684614063287, 0.175697864178562, 0.1785714273969528, 0.18637109898972784, 0.18472906288940136, 0.18801313516345908, 0.18760262618119689, 0.18390804596477736, 0.17816091841469062, 0.1843185538337344, 0.18021346359515228, 0.17857142742142107, 0.1834975358447418, 0.17857142855919445, 0.18185549974441528, 0.17939244550828667, 0.1785714285347262, 0.18267651797809037, 0.18226600894689168, 0.18842364429253075, 0.18226600867774098, 0.1814449906642801, 0.1847290630851473, 0.1818554996465423, 0.18144499177758525, 0.1834975357958053, 0.17980295451501713, 0.17569786434983972, 0.1892446625017376, 0.17898193645261976, 0.18555008095179873, 0.1847290628649331, 0.18144499177758525, 0.18513957303390519, 0.16954022982388686, 0.18472906395377, 0.1806239725774145, 0.1834975357958053, 0.18637109903866433, 0.1859605900319339, 0.17323481008924288, 0.17692939240993147, 0.17528735519629982, 0.17816091945459103, 0.1847290628649331, 0.18513957303390519, 0.18842364530796293, 0.18431855385820267, 0.18842364528349467, 0.18760262610779216, 0.17651888335426452, 0.18103448274638656, 0.17569786434983972, 0.1728243010580442, 0.1703612467729791, 0.18924466220811867, 0.18185550071091097, 0.1785714273969528, 0.18801313518792734, 0.17651888228989587, 0.1798029545639536, 0.17323481115361153, 0.18308702676460661, 0.18596058995852915, 0.18596058995852915, 0.1912972074130486, 0.18103448170648614, 0.175697864178562, 0.19047618944852418, 0.18144499073768483, 0.1843185538337344, 0.17692939144343578, 0.18390804480253573, 0.1798029545639536, 0.1888341532503247, 0.18267651773340793, 0.18267651775787616, 0.18513957194506828, 0.18472906288940136, 0.1847290628649331, 0.1802134646839892, 0.18308702787791176, 0.18062397364178315, 0.18308702790238, 0.17733990144113015, 0.18472906288940136, 0.18226600870220924, 0.17939244548381844, 0.17487684611616464, 0.17323481013817935, 0.18062397366625138, 0.1736453190470368, 0.18513957305837342, 0.18144499063981187, 0.1728243010580442, 0.18390804487594048, 0.17610837333210191, 0.1806239725774145, 0.18555008100073522, 0.19047618937511945, 0.18226600877561397, 0.18226600887348696, 0.17610837330763368, 0.18678160814326777, 0.1818554996465423, 0.18226600872667748, 0.1814449906153436, 0.18021346352174755, 0.17816091841469062, 0.18103448160861318, 0.18472906293833785, 0.17733990042569797, 0.1859605900319339, 0.18349753582027353, 0.18924466225705514, 0.17816091848809534, 0.18472906288940136, 0.1761083743475341, 0.17980295468629484, 0.1822660088000822, 0.17364531902256858, 0.17692939141896755, 0.17733990045016623, 0.17733990047463447, 0.17528735536757753, 0.1765188824122371, 0.1769293913455628, 0.17528735522076805, 0.18021346366855703, 0.17610837340550664, 0.17733990047463447, 0.17446633723177543, 0.18431855395607566, 0.18267651773340793, 0.17980295463735835, 0.174055828127172, 0.18842364426806252, 0.18842364429253075, 0.18308702676460661, 0.18760262618119689, 0.18144499078662132, 0.1839080449493452, 0.18555008097626696, 0.18801313531026856], 'loss': [2.691285347889581, 2.659438477208727, 2.625249616713005, 2.5734038502773466, 2.5373365367952068, 2.512700472034713, 2.4987566865689947, 2.489583829296198, 2.48367111394048, 2.4801076839102367, 2.473408071999677, 2.4727025430550076, 2.468704581505464, 2.467152108989457, 2.4594406667431277, 2.4577623703151756, 2.456851225369275, 2.454715402415156, 2.4530015362851185, 2.4567318531522027, 2.4508149341880907, 2.451683123645352, 2.450884955717553, 2.4479055251918536, 2.446538731400727, 2.4459917652043965, 2.444010738572546, 2.445171025646296, 2.442846144983656, 2.444726770565495, 2.447049241584919, 2.444197268554562, 2.4434960642389694, 2.4438224138420463, 2.441509766353474, 2.444659275979232, 2.441598503496613, 2.4406138979189205, 2.43857627962649, 2.4387334630719444, 2.4384401788456973, 2.4383354330944087, 2.438885349120937, 2.4392659231377825, 2.4373388262989586, 2.44072030016529, 2.4397275496802044, 2.4388088569993602, 2.4362435780511498, 2.436570336245903, 2.4350691539795735, 2.4341885537337475, 2.4410343303327933, 2.4346989740336453, 2.434280850608246, 2.4351906506187864, 2.4343758615380193, 2.435651385270105, 2.434998627707699, 2.433042917995727, 2.4320167877346095, 2.4328488948164044, 2.4400374025779583, 2.434447932977696, 2.4365588563178844, 2.431924354564972, 2.432939203270162, 2.4351146906553107, 2.4353569591804205, 2.440921568332022, 2.433445911289975, 2.43589965273957, 2.4340572447257856, 2.4330720461859108, 2.432259376631625, 2.431652443022209, 2.43010978375617, 2.4297584377030326, 2.430491962080374, 2.431220873865993, 2.4299587528808404, 2.432369946552253, 2.42893443910493, 2.428434878355179, 2.4298964376077516, 2.4302257599527097, 2.429390327249954, 2.4299906741422306, 2.429349709732087, 2.4292575036231008, 2.428061615612962, 2.4287701662560996, 2.43025999196501, 2.4294033053474506, 2.4296611471587384, 2.430315352661164, 2.4283193390472224, 2.4277488305093815, 2.4278305589296, 2.4300277215499406, 2.4280257175590467, 2.4276602133098812, 2.4259898075088095, 2.4268063256383186, 2.427660196272989, 2.4267254423067066, 2.4260639543161253, 2.424907423583389, 2.425821292229012, 2.4245871631027, 2.4240945126731295, 2.4257387344352517, 2.4255829858094513, 2.424058985465361, 2.424292996189188, 2.428884442777849, 2.425027965275414, 2.4240642734621582, 2.4246404393252896, 2.4271635310116246, 2.439262204101688, 2.4261971764495973, 2.422817704613938, 2.4242356407079364, 2.423965661383752, 2.42392265830931, 2.425166712551391, 2.424848505603704, 2.4229917813130717, 2.424450584603531, 2.424522181458052, 2.423047903969547, 2.423523329905171, 2.4241471621534907, 2.4251029561922044, 2.4248489903718293, 2.4222047671645086, 2.423901994419294, 2.427094567923575, 2.459172781683826, 2.4429632743036476, 2.4339786726346495, 2.4323327349441497, 2.436935698472009, 2.435904706136402, 2.4300615229401012, 2.4303608348971766, 2.42931489327605, 2.43080193041776, 2.4285796544390292, 2.428906327645147, 2.431458707413879, 2.4297426180673085, 2.4283488374471176, 2.4266487467215536, 2.426754190397948, 2.435983983887784, 2.4376925402842997, 2.4275997999022874, 2.4288046585216168, 2.424859625896634, 2.4255290682555715, 2.426051190256828, 2.4282391253438083, 2.431169320229877, 2.4317895823680398, 2.4278425106032917, 2.4302510503381183, 2.4273403278366494, 2.4245830037510614, 2.4249487255143434, 2.425452351031607, 2.4276588837468895, 2.4242120520534947, 2.4249247154416, 2.4246147740303368, 2.424046469713873, 2.423791011109244, 2.4236877622545623, 2.422544755974834, 2.423976144653571, 2.4222558285910982, 2.423004099279948, 2.424658321306201, 2.423354133492378, 2.427251027790673, 2.429387831736884, 2.4256461016206528, 2.4238589806723154, 2.4237929403904284, 2.425580396103908, 2.425167726050657, 2.4244480246635924, 2.424576171957247, 2.4246855885585967, 2.4239602387563406, 2.4237911624830115, 2.4256122711502797, 2.422131526935272, 2.4224522622948554, 2.4228956489837143, 2.4226701639516155, 2.4243701851588253, 2.4252319687445794, 2.42128463134139, 2.423107293499079, 2.422976999067428, 2.4231782475046555, 2.421296488235129, 2.4219222236218148, 2.4226587180973813, 2.4215095099972013, 2.4197776358719967, 2.419420905475499, 2.420912288787184, 2.423027636823713, 2.4212483271925844, 2.425281298674597, 2.4217862532124137, 2.4188285059997434, 2.419654705098522, 2.419073927622801, 2.418667264542785, 2.4199845589161897, 2.423279646338868, 2.4306732745630786, 2.431841351217313, 2.429628946942715, 2.422994727324656, 2.420509392818631, 2.42470353506429, 2.423219055708429, 2.4211084447112663, 2.4198234470962743, 2.4210174739727983, 2.421321204751424, 2.421052448754438, 2.4211262893872583, 2.420776503188899, 2.4195967485282943, 2.419896420316285, 2.4189579758066415, 2.418533784897665, 2.4209367380984266, 2.4174453986988422, 2.4186422484121772, 2.419668628351889, 2.41866097469839, 2.418571013053095, 2.4164656101555795, 2.4142450508885314, 2.4149439346618964, 2.4155076646706894, 2.415850655107283, 2.4145196970483362, 2.415544125186834, 2.4140151209899776, 2.4155223093483236, 2.416114755332837, 2.4161101919914416, 2.417554979598498, 2.4153743637171123, 2.4137612636573995, 2.415537608379701, 2.4187813307470365, 2.4164957696652265, 2.4141971724234077, 2.41391088732459, 2.4160207894303714, 2.415867001026318, 2.415320167306512, 2.4157264262988583, 2.4142445907945262, 2.414910829532318, 2.417672060157729, 2.414257164314787, 2.414812051491081, 2.4144792125944727, 2.417726173342131, 2.4158257130969476, 2.4179304407852142, 2.417004840819498, 2.417269216181072, 2.4170872466030553, 2.4160921888919336, 2.41421522036715, 2.412907373733834, 2.4135226059743267, 2.4137850217995456, 2.4147189847252943, 2.4155549820933255, 2.4169869606010232, 2.4162029267336553, 2.415583515069323, 2.413846549556975, 2.413443626564386, 2.415331775402876, 2.4141793676959904, 2.414582327601846, 2.4145654924106794], 'acc': [0.0742299794661191, 0.09763860369609856, 0.1259753593490354, 0.16221765914369657, 0.16067761807287498, 0.1687885010389332, 0.17238193018480494, 0.16919917864782363, 0.1715605749486653, 0.17094455852462037, 0.16889117043733107, 0.17320328542400434, 0.17320328542706412, 0.17320328542706412, 0.17484599586874552, 0.17443531828127357, 0.17525667351435342, 0.17412731006160165, 0.17535934292193067, 0.17135523615186954, 0.1727926078089943, 0.1745379876674323, 0.17905544148455899, 0.18049281314168378, 0.1819301848110477, 0.17854209445585215, 0.18039014374940548, 0.17700205339420993, 0.18141683778234086, 0.17772073923195167, 0.17638603696404542, 0.17720739219712525, 0.17885010265716536, 0.176796714585175, 0.17669404518065757, 0.1755646817279058, 0.17823408624841935, 0.175667351135483, 0.17484599589322383, 0.1761806981580703, 0.17977412732230075, 0.17864476387260875, 0.1811088295687885, 0.175667351135483, 0.17967145790860392, 0.18234086242605774, 0.1817248460020128, 0.17833675565293683, 0.1799794661252161, 0.17967145791166372, 0.18172484600813238, 0.1805954825523208, 0.18542094456464114, 0.18110882957490806, 0.1822381930215403, 0.17854209445891195, 0.17967145791166372, 0.18028747433570866, 0.17977412731618117, 0.18264887062431115, 0.18254620123203286, 0.18326488706365504, 0.18203285422168472, 0.182854209448645, 0.17597535933067665, 0.1799794661252161, 0.18141683778234086, 0.17864476386648917, 0.18182751541264983, 0.18080082136747527, 0.18388090349381955, 0.180800821342997, 0.18059548255844038, 0.18008213552667374, 0.18398357290139677, 0.18388090349075975, 0.184394250513347, 0.183983572898337, 0.18408624229979467, 0.18264887062431115, 0.18367556468172486, 0.18213552361396304, 0.18408624230285445, 0.18521560575254645, 0.18480492813447663, 0.18613963039014375, 0.18357289528332696, 0.18162217659137578, 0.1859342915872284, 0.18583162217659138, 0.1855236139691586, 0.1836755646878444, 0.18583162218883054, 0.18223819302460006, 0.1836755646878444, 0.1815195071899181, 0.18264887064266988, 0.17618069816418985, 0.1799794661252161, 0.18295687885316245, 0.18706365501856168, 0.1822381930184805, 0.18716632443531828, 0.1861396303779046, 0.18665297742497014, 0.1862422997946612, 0.18521560574948664, 0.1881930184804928, 0.18593429159334798, 0.1893223819301848, 0.18850102670016475, 0.1848049281375364, 0.18716632444755743, 0.18716632444755743, 0.18459958932238194, 0.18028747433570866, 0.18429158110882957, 0.18531827516012367, 0.186652977412731, 0.1845995893254417, 0.18141683778234086, 0.18429158112106872, 0.18603696098562628, 0.1859342915841686, 0.18480492814365598, 0.18326488706671482, 0.18439425052558617, 0.18562628337367604, 0.18264887064266988, 0.182546201244272, 0.18326488706977462, 0.1866529774157908, 0.18234086242299793, 0.18305954825767992, 0.18716632442307912, 0.18521560574948664, 0.18880903491371712, 0.18470225873301896, 0.18696098562628335, 0.17648870636856287, 0.1772073922093644, 0.1801848049311912, 0.176694045174538, 0.1801848049311912, 0.17587268994145813, 0.17915811088601666, 0.1828542094517048, 0.1806981519507187, 0.17761806981825487, 0.18459958931014278, 0.18470225872689938, 0.18439425051640682, 0.17987679672069862, 0.18213552362008262, 0.18880903491065731, 0.18449691992398404, 0.1794661191026288, 0.1781314168255432, 0.17987679672069862, 0.1805954825523208, 0.18305954825462012, 0.1801848049281314, 0.1799794661252161, 0.18336755647429206, 0.18377823409236188, 0.181314168383943, 0.18028747433264888, 0.18059548254620122, 0.18254620123815243, 0.17905544147843944, 0.18080082135829592, 0.18347022587574974, 0.18264887064266988, 0.18316221766525714, 0.18172484599589322, 0.17967145790860392, 0.18480492813141683, 0.18367556468172486, 0.18254620123203286, 0.18613963039320353, 0.1869609856385225, 0.18459958932238194, 0.1864476386036961, 0.18583162218271096, 0.18696098562934316, 0.18316221766219737, 0.17823408623006065, 0.1855236139691586, 0.1879876796714579, 0.18634496920529822, 0.18511293634802897, 0.17946611909650925, 0.17885010267246432, 0.1848049281375364, 0.1809034907597536, 0.1822381930184805, 0.1819301848110477, 0.1782340862422998, 0.18367556468172486, 0.1864476386036961, 0.18521560574948664, 0.18501026694045175, 0.1839835728952772, 0.18295687886234183, 0.18716632444755743, 0.18521560574948664, 0.18275154004106775, 0.18326488706365504, 0.18254620123815243, 0.18182751540653025, 0.18542094456464114, 0.18264887064878946, 0.18613963039014375, 0.18562628336755646, 0.18603696099786543, 0.181724845998953, 0.18162217659443555, 0.18049281314780335, 0.18275154004106775, 0.18295687885622225, 0.18357289528332696, 0.1879876796714579, 0.1857289527751337, 0.1799794661252161, 0.18429158110882957, 0.18326488706365504, 0.18501026694045175, 0.17946611909650925, 0.18141683778540066, 0.18275154004718733, 0.18501026694045175, 0.18459958931014278, 0.18418891170431212, 0.18501026694351153, 0.18244353183975454, 0.1843942505194666, 0.18059548254620122, 0.18470225873301896, 0.18316221765913757, 0.18531827514176497, 0.18583162218883054, 0.18223819302460006, 0.18624229979772097, 0.18336755647123226, 0.1843942505194666, 0.18737166324435317, 0.18418891171655127, 0.18357289528944654, 0.18531827516012367, 0.18470225872689938, 0.18480492813447663, 0.1855236139660988, 0.18809034907903516, 0.18439425052558617, 0.18418891169207297, 0.18583162217965116, 0.18809034907597535, 0.1866529774188506, 0.186652977412731, 0.18583162217659138, 0.18552361397527817, 0.18470225872689938, 0.19045174538599638, 0.18542094457076072, 0.1848049281375364, 0.18295687885010267, 0.1870636550338606, 0.18521560574948664, 0.18942505132246312, 0.18798767967451768, 0.1893223819301848, 0.18983572895583187, 0.18880903491371712, 0.18696098562628335, 0.18511293634802897, 0.18829568789112985, 0.1883983572956473, 0.18850102669404517, 0.18798767967757748, 0.1862422997946612, 0.18223819302460006, 0.18624229980690032, 0.18675564682336804, 0.18644763860981567, 0.1880903490882145, 0.18983572895277206, 0.1866529774188506, 0.18562628336755646, 0.18798767967451768, 0.18891170432435414, 0.1879876796714579, 0.1868583162217659, 0.18809034908209493, 0.19055441478745402, 0.19055441478439425, 0.1899383983634091, 0.19158110882956877, 0.1931211499003904, 0.18870636550919964, 0.18993839836034931]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
