{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf82.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 05:44:07 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '2Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['yd', 'ek', 'ck', 'by', 'ib', 'mb', 'my', 'sk', 'sg', 'eo', 'ce', 'eg', 'eb', 'aa', 'ds'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001A58EF62E10>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001A5E4D87EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7111, Accuracy:0.0538, Validation Loss:2.7035, Validation Accuracy:0.0706\n",
    "Epoch #2: Loss:2.6997, Accuracy:0.0834, Validation Loss:2.6944, Validation Accuracy:0.1051\n",
    "Epoch #3: Loss:2.6909, Accuracy:0.1023, Validation Loss:2.6863, Validation Accuracy:0.1018\n",
    "Epoch #4: Loss:2.6839, Accuracy:0.1023, Validation Loss:2.6794, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6768, Accuracy:0.1023, Validation Loss:2.6730, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6703, Accuracy:0.1023, Validation Loss:2.6665, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6640, Accuracy:0.1023, Validation Loss:2.6603, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6582, Accuracy:0.1023, Validation Loss:2.6540, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6519, Accuracy:0.1023, Validation Loss:2.6503, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6476, Accuracy:0.1023, Validation Loss:2.6443, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6400, Accuracy:0.1023, Validation Loss:2.6356, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6307, Accuracy:0.1142, Validation Loss:2.6239, Validation Accuracy:0.1264\n",
    "Epoch #13: Loss:2.6178, Accuracy:0.1257, Validation Loss:2.6090, Validation Accuracy:0.1363\n",
    "Epoch #14: Loss:2.6015, Accuracy:0.1294, Validation Loss:2.5908, Validation Accuracy:0.1691\n",
    "Epoch #15: Loss:2.5837, Accuracy:0.1483, Validation Loss:2.5708, Validation Accuracy:0.1724\n",
    "Epoch #16: Loss:2.5626, Accuracy:0.1491, Validation Loss:2.5506, Validation Accuracy:0.1724\n",
    "Epoch #17: Loss:2.5450, Accuracy:0.1515, Validation Loss:2.5338, Validation Accuracy:0.1724\n",
    "Epoch #18: Loss:2.5300, Accuracy:0.1503, Validation Loss:2.5199, Validation Accuracy:0.1708\n",
    "Epoch #19: Loss:2.5191, Accuracy:0.1511, Validation Loss:2.5115, Validation Accuracy:0.1675\n",
    "Epoch #20: Loss:2.5204, Accuracy:0.1524, Validation Loss:2.5126, Validation Accuracy:0.1757\n",
    "Epoch #21: Loss:2.5134, Accuracy:0.1548, Validation Loss:2.4991, Validation Accuracy:0.1609\n",
    "Epoch #22: Loss:2.5059, Accuracy:0.1532, Validation Loss:2.4978, Validation Accuracy:0.1741\n",
    "Epoch #23: Loss:2.4954, Accuracy:0.1577, Validation Loss:2.4995, Validation Accuracy:0.1806\n",
    "Epoch #24: Loss:2.4929, Accuracy:0.1622, Validation Loss:2.4857, Validation Accuracy:0.1675\n",
    "Epoch #25: Loss:2.4896, Accuracy:0.1577, Validation Loss:2.4828, Validation Accuracy:0.1741\n",
    "Epoch #26: Loss:2.4834, Accuracy:0.1618, Validation Loss:2.4839, Validation Accuracy:0.1773\n",
    "Epoch #27: Loss:2.4813, Accuracy:0.1643, Validation Loss:2.4785, Validation Accuracy:0.1757\n",
    "Epoch #28: Loss:2.4779, Accuracy:0.1618, Validation Loss:2.4737, Validation Accuracy:0.1773\n",
    "Epoch #29: Loss:2.4742, Accuracy:0.1622, Validation Loss:2.4719, Validation Accuracy:0.1757\n",
    "Epoch #30: Loss:2.4729, Accuracy:0.1643, Validation Loss:2.4688, Validation Accuracy:0.1773\n",
    "Epoch #31: Loss:2.4704, Accuracy:0.1589, Validation Loss:2.4685, Validation Accuracy:0.1757\n",
    "Epoch #32: Loss:2.4701, Accuracy:0.1602, Validation Loss:2.4663, Validation Accuracy:0.1757\n",
    "Epoch #33: Loss:2.4658, Accuracy:0.1598, Validation Loss:2.4660, Validation Accuracy:0.1708\n",
    "Epoch #34: Loss:2.4645, Accuracy:0.1754, Validation Loss:2.4775, Validation Accuracy:0.1576\n",
    "Epoch #35: Loss:2.4731, Accuracy:0.1741, Validation Loss:2.4769, Validation Accuracy:0.1806\n",
    "Epoch #36: Loss:2.4703, Accuracy:0.1749, Validation Loss:2.4608, Validation Accuracy:0.1691\n",
    "Epoch #37: Loss:2.4633, Accuracy:0.1758, Validation Loss:2.4615, Validation Accuracy:0.1691\n",
    "Epoch #38: Loss:2.4636, Accuracy:0.1741, Validation Loss:2.4638, Validation Accuracy:0.1741\n",
    "Epoch #39: Loss:2.4611, Accuracy:0.1729, Validation Loss:2.4617, Validation Accuracy:0.1675\n",
    "Epoch #40: Loss:2.4600, Accuracy:0.1741, Validation Loss:2.4674, Validation Accuracy:0.1708\n",
    "Epoch #41: Loss:2.4589, Accuracy:0.1745, Validation Loss:2.4618, Validation Accuracy:0.1658\n",
    "Epoch #42: Loss:2.4590, Accuracy:0.1725, Validation Loss:2.4596, Validation Accuracy:0.1757\n",
    "Epoch #43: Loss:2.4560, Accuracy:0.1766, Validation Loss:2.4606, Validation Accuracy:0.1675\n",
    "Epoch #44: Loss:2.4529, Accuracy:0.1774, Validation Loss:2.4629, Validation Accuracy:0.1757\n",
    "Epoch #45: Loss:2.4532, Accuracy:0.1791, Validation Loss:2.4581, Validation Accuracy:0.1658\n",
    "Epoch #46: Loss:2.4524, Accuracy:0.1758, Validation Loss:2.4566, Validation Accuracy:0.1658\n",
    "Epoch #47: Loss:2.4506, Accuracy:0.1770, Validation Loss:2.4582, Validation Accuracy:0.1724\n",
    "Epoch #48: Loss:2.4510, Accuracy:0.1766, Validation Loss:2.4574, Validation Accuracy:0.1741\n",
    "Epoch #49: Loss:2.4508, Accuracy:0.1774, Validation Loss:2.4575, Validation Accuracy:0.1658\n",
    "Epoch #50: Loss:2.4504, Accuracy:0.1749, Validation Loss:2.4579, Validation Accuracy:0.1741\n",
    "Epoch #51: Loss:2.4500, Accuracy:0.1762, Validation Loss:2.4558, Validation Accuracy:0.1757\n",
    "Epoch #52: Loss:2.4484, Accuracy:0.1774, Validation Loss:2.4547, Validation Accuracy:0.1658\n",
    "Epoch #53: Loss:2.4480, Accuracy:0.1762, Validation Loss:2.4568, Validation Accuracy:0.1757\n",
    "Epoch #54: Loss:2.4482, Accuracy:0.1745, Validation Loss:2.4544, Validation Accuracy:0.1757\n",
    "Epoch #55: Loss:2.4491, Accuracy:0.1770, Validation Loss:2.4530, Validation Accuracy:0.1691\n",
    "Epoch #56: Loss:2.4468, Accuracy:0.1778, Validation Loss:2.4552, Validation Accuracy:0.1806\n",
    "Epoch #57: Loss:2.4461, Accuracy:0.1803, Validation Loss:2.4540, Validation Accuracy:0.1773\n",
    "Epoch #58: Loss:2.4447, Accuracy:0.1774, Validation Loss:2.4535, Validation Accuracy:0.1790\n",
    "Epoch #59: Loss:2.4440, Accuracy:0.1791, Validation Loss:2.4533, Validation Accuracy:0.1790\n",
    "Epoch #60: Loss:2.4445, Accuracy:0.1811, Validation Loss:2.4515, Validation Accuracy:0.1806\n",
    "Epoch #61: Loss:2.4437, Accuracy:0.1791, Validation Loss:2.4516, Validation Accuracy:0.1823\n",
    "Epoch #62: Loss:2.4427, Accuracy:0.1795, Validation Loss:2.4542, Validation Accuracy:0.1872\n",
    "Epoch #63: Loss:2.4435, Accuracy:0.1786, Validation Loss:2.4532, Validation Accuracy:0.1823\n",
    "Epoch #64: Loss:2.4437, Accuracy:0.1778, Validation Loss:2.4535, Validation Accuracy:0.1757\n",
    "Epoch #65: Loss:2.4435, Accuracy:0.1774, Validation Loss:2.4567, Validation Accuracy:0.1905\n",
    "Epoch #66: Loss:2.4430, Accuracy:0.1803, Validation Loss:2.4554, Validation Accuracy:0.1921\n",
    "Epoch #67: Loss:2.4426, Accuracy:0.1766, Validation Loss:2.4535, Validation Accuracy:0.1921\n",
    "Epoch #68: Loss:2.4431, Accuracy:0.1786, Validation Loss:2.4537, Validation Accuracy:0.1888\n",
    "Epoch #69: Loss:2.4440, Accuracy:0.1803, Validation Loss:2.4573, Validation Accuracy:0.1954\n",
    "Epoch #70: Loss:2.4420, Accuracy:0.1786, Validation Loss:2.4532, Validation Accuracy:0.1856\n",
    "Epoch #71: Loss:2.4428, Accuracy:0.1774, Validation Loss:2.4537, Validation Accuracy:0.1970\n",
    "Epoch #72: Loss:2.4419, Accuracy:0.1799, Validation Loss:2.4535, Validation Accuracy:0.1938\n",
    "Epoch #73: Loss:2.4421, Accuracy:0.1799, Validation Loss:2.4524, Validation Accuracy:0.1938\n",
    "Epoch #74: Loss:2.4414, Accuracy:0.1791, Validation Loss:2.4545, Validation Accuracy:0.1938\n",
    "Epoch #75: Loss:2.4408, Accuracy:0.1807, Validation Loss:2.4531, Validation Accuracy:0.1872\n",
    "Epoch #76: Loss:2.4410, Accuracy:0.1762, Validation Loss:2.4531, Validation Accuracy:0.1888\n",
    "Epoch #77: Loss:2.4413, Accuracy:0.1786, Validation Loss:2.4545, Validation Accuracy:0.1888\n",
    "Epoch #78: Loss:2.4403, Accuracy:0.1762, Validation Loss:2.4529, Validation Accuracy:0.1954\n",
    "Epoch #79: Loss:2.4406, Accuracy:0.1782, Validation Loss:2.4505, Validation Accuracy:0.1954\n",
    "Epoch #80: Loss:2.4421, Accuracy:0.1778, Validation Loss:2.4512, Validation Accuracy:0.1987\n",
    "Epoch #81: Loss:2.4411, Accuracy:0.1791, Validation Loss:2.4535, Validation Accuracy:0.1970\n",
    "Epoch #82: Loss:2.4421, Accuracy:0.1770, Validation Loss:2.4517, Validation Accuracy:0.1872\n",
    "Epoch #83: Loss:2.4410, Accuracy:0.1749, Validation Loss:2.4504, Validation Accuracy:0.1806\n",
    "Epoch #84: Loss:2.4406, Accuracy:0.1799, Validation Loss:2.4506, Validation Accuracy:0.2003\n",
    "Epoch #85: Loss:2.4402, Accuracy:0.1778, Validation Loss:2.4499, Validation Accuracy:0.1806\n",
    "Epoch #86: Loss:2.4399, Accuracy:0.1795, Validation Loss:2.4497, Validation Accuracy:0.1823\n",
    "Epoch #87: Loss:2.4402, Accuracy:0.1758, Validation Loss:2.4506, Validation Accuracy:0.1970\n",
    "Epoch #88: Loss:2.4408, Accuracy:0.1770, Validation Loss:2.4509, Validation Accuracy:0.2036\n",
    "Epoch #89: Loss:2.4406, Accuracy:0.1758, Validation Loss:2.4489, Validation Accuracy:0.1970\n",
    "Epoch #90: Loss:2.4396, Accuracy:0.1791, Validation Loss:2.4464, Validation Accuracy:0.1724\n",
    "Epoch #91: Loss:2.4393, Accuracy:0.1803, Validation Loss:2.4466, Validation Accuracy:0.1839\n",
    "Epoch #92: Loss:2.4390, Accuracy:0.1762, Validation Loss:2.4469, Validation Accuracy:0.1987\n",
    "Epoch #93: Loss:2.4383, Accuracy:0.1770, Validation Loss:2.4462, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4387, Accuracy:0.1766, Validation Loss:2.4468, Validation Accuracy:0.1839\n",
    "Epoch #95: Loss:2.4382, Accuracy:0.1774, Validation Loss:2.4468, Validation Accuracy:0.1839\n",
    "Epoch #96: Loss:2.4391, Accuracy:0.1766, Validation Loss:2.4472, Validation Accuracy:0.1839\n",
    "Epoch #97: Loss:2.4375, Accuracy:0.1745, Validation Loss:2.4489, Validation Accuracy:0.1987\n",
    "Epoch #98: Loss:2.4384, Accuracy:0.1762, Validation Loss:2.4488, Validation Accuracy:0.1987\n",
    "Epoch #99: Loss:2.4389, Accuracy:0.1762, Validation Loss:2.4482, Validation Accuracy:0.1806\n",
    "Epoch #100: Loss:2.4404, Accuracy:0.1758, Validation Loss:2.4491, Validation Accuracy:0.1905\n",
    "Epoch #101: Loss:2.4392, Accuracy:0.1745, Validation Loss:2.4498, Validation Accuracy:0.1905\n",
    "Epoch #102: Loss:2.4404, Accuracy:0.1762, Validation Loss:2.4491, Validation Accuracy:0.1938\n",
    "Epoch #103: Loss:2.4394, Accuracy:0.1733, Validation Loss:2.4553, Validation Accuracy:0.1839\n",
    "Epoch #104: Loss:2.4428, Accuracy:0.1782, Validation Loss:2.4538, Validation Accuracy:0.1724\n",
    "Epoch #105: Loss:2.4426, Accuracy:0.1782, Validation Loss:2.4543, Validation Accuracy:0.1757\n",
    "Epoch #106: Loss:2.4422, Accuracy:0.1770, Validation Loss:2.4565, Validation Accuracy:0.1773\n",
    "Epoch #107: Loss:2.4407, Accuracy:0.1770, Validation Loss:2.4544, Validation Accuracy:0.1856\n",
    "Epoch #108: Loss:2.4417, Accuracy:0.1762, Validation Loss:2.4528, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.4374, Accuracy:0.1795, Validation Loss:2.4557, Validation Accuracy:0.1741\n",
    "Epoch #110: Loss:2.4415, Accuracy:0.1758, Validation Loss:2.4562, Validation Accuracy:0.1856\n",
    "Epoch #111: Loss:2.4397, Accuracy:0.1778, Validation Loss:2.4566, Validation Accuracy:0.1806\n",
    "Epoch #112: Loss:2.4453, Accuracy:0.1733, Validation Loss:2.4581, Validation Accuracy:0.1790\n",
    "Epoch #113: Loss:2.4447, Accuracy:0.1791, Validation Loss:2.4738, Validation Accuracy:0.1724\n",
    "Epoch #114: Loss:2.4473, Accuracy:0.1778, Validation Loss:2.4600, Validation Accuracy:0.1675\n",
    "Epoch #115: Loss:2.4435, Accuracy:0.1749, Validation Loss:2.4647, Validation Accuracy:0.1741\n",
    "Epoch #116: Loss:2.4430, Accuracy:0.1795, Validation Loss:2.4619, Validation Accuracy:0.1724\n",
    "Epoch #117: Loss:2.4422, Accuracy:0.1766, Validation Loss:2.4614, Validation Accuracy:0.1724\n",
    "Epoch #118: Loss:2.4422, Accuracy:0.1774, Validation Loss:2.4610, Validation Accuracy:0.1773\n",
    "Epoch #119: Loss:2.4384, Accuracy:0.1741, Validation Loss:2.4577, Validation Accuracy:0.1658\n",
    "Epoch #120: Loss:2.4379, Accuracy:0.1733, Validation Loss:2.4557, Validation Accuracy:0.1790\n",
    "Epoch #121: Loss:2.4378, Accuracy:0.1729, Validation Loss:2.4540, Validation Accuracy:0.1724\n",
    "Epoch #122: Loss:2.4383, Accuracy:0.1733, Validation Loss:2.4547, Validation Accuracy:0.1741\n",
    "Epoch #123: Loss:2.4382, Accuracy:0.1774, Validation Loss:2.4539, Validation Accuracy:0.1741\n",
    "Epoch #124: Loss:2.4388, Accuracy:0.1782, Validation Loss:2.4535, Validation Accuracy:0.1790\n",
    "Epoch #125: Loss:2.4376, Accuracy:0.1749, Validation Loss:2.4532, Validation Accuracy:0.1741\n",
    "Epoch #126: Loss:2.4372, Accuracy:0.1791, Validation Loss:2.4526, Validation Accuracy:0.1741\n",
    "Epoch #127: Loss:2.4376, Accuracy:0.1729, Validation Loss:2.4575, Validation Accuracy:0.1773\n",
    "Epoch #128: Loss:2.4416, Accuracy:0.1741, Validation Loss:2.4592, Validation Accuracy:0.1741\n",
    "Epoch #129: Loss:2.4414, Accuracy:0.1754, Validation Loss:2.4634, Validation Accuracy:0.1741\n",
    "Epoch #130: Loss:2.4433, Accuracy:0.1778, Validation Loss:2.4625, Validation Accuracy:0.1724\n",
    "Epoch #131: Loss:2.4419, Accuracy:0.1766, Validation Loss:2.4644, Validation Accuracy:0.1806\n",
    "Epoch #132: Loss:2.4419, Accuracy:0.1791, Validation Loss:2.4647, Validation Accuracy:0.1790\n",
    "Epoch #133: Loss:2.4420, Accuracy:0.1745, Validation Loss:2.4643, Validation Accuracy:0.1724\n",
    "Epoch #134: Loss:2.4427, Accuracy:0.1799, Validation Loss:2.4627, Validation Accuracy:0.1724\n",
    "Epoch #135: Loss:2.4429, Accuracy:0.1700, Validation Loss:2.4617, Validation Accuracy:0.1741\n",
    "Epoch #136: Loss:2.4450, Accuracy:0.1791, Validation Loss:2.4638, Validation Accuracy:0.1773\n",
    "Epoch #137: Loss:2.4420, Accuracy:0.1733, Validation Loss:2.4620, Validation Accuracy:0.1708\n",
    "Epoch #138: Loss:2.4415, Accuracy:0.1778, Validation Loss:2.4638, Validation Accuracy:0.1856\n",
    "Epoch #139: Loss:2.4419, Accuracy:0.1795, Validation Loss:2.4625, Validation Accuracy:0.1823\n",
    "Epoch #140: Loss:2.4419, Accuracy:0.1782, Validation Loss:2.4618, Validation Accuracy:0.1741\n",
    "Epoch #141: Loss:2.4401, Accuracy:0.1770, Validation Loss:2.4632, Validation Accuracy:0.1691\n",
    "Epoch #142: Loss:2.4404, Accuracy:0.1774, Validation Loss:2.4630, Validation Accuracy:0.1675\n",
    "Epoch #143: Loss:2.4404, Accuracy:0.1745, Validation Loss:2.4616, Validation Accuracy:0.1691\n",
    "Epoch #144: Loss:2.4404, Accuracy:0.1774, Validation Loss:2.4621, Validation Accuracy:0.1691\n",
    "Epoch #145: Loss:2.4400, Accuracy:0.1749, Validation Loss:2.4631, Validation Accuracy:0.1691\n",
    "Epoch #146: Loss:2.4437, Accuracy:0.1733, Validation Loss:2.4624, Validation Accuracy:0.1773\n",
    "Epoch #147: Loss:2.4432, Accuracy:0.1791, Validation Loss:2.4635, Validation Accuracy:0.1773\n",
    "Epoch #148: Loss:2.4416, Accuracy:0.1754, Validation Loss:2.4613, Validation Accuracy:0.1691\n",
    "Epoch #149: Loss:2.4445, Accuracy:0.1778, Validation Loss:2.4652, Validation Accuracy:0.1839\n",
    "Epoch #150: Loss:2.4406, Accuracy:0.1778, Validation Loss:2.4615, Validation Accuracy:0.1642\n",
    "Epoch #151: Loss:2.4405, Accuracy:0.1766, Validation Loss:2.4644, Validation Accuracy:0.1823\n",
    "Epoch #152: Loss:2.4411, Accuracy:0.1795, Validation Loss:2.4619, Validation Accuracy:0.1773\n",
    "Epoch #153: Loss:2.4410, Accuracy:0.1745, Validation Loss:2.4629, Validation Accuracy:0.1708\n",
    "Epoch #154: Loss:2.4412, Accuracy:0.1791, Validation Loss:2.4666, Validation Accuracy:0.1757\n",
    "Epoch #155: Loss:2.4405, Accuracy:0.1791, Validation Loss:2.4606, Validation Accuracy:0.1691\n",
    "Epoch #156: Loss:2.4396, Accuracy:0.1778, Validation Loss:2.4621, Validation Accuracy:0.1757\n",
    "Epoch #157: Loss:2.4395, Accuracy:0.1811, Validation Loss:2.4612, Validation Accuracy:0.1757\n",
    "Epoch #158: Loss:2.4387, Accuracy:0.1795, Validation Loss:2.4612, Validation Accuracy:0.1741\n",
    "Epoch #159: Loss:2.4390, Accuracy:0.1791, Validation Loss:2.4615, Validation Accuracy:0.1757\n",
    "Epoch #160: Loss:2.4387, Accuracy:0.1786, Validation Loss:2.4603, Validation Accuracy:0.1691\n",
    "Epoch #161: Loss:2.4405, Accuracy:0.1774, Validation Loss:2.4591, Validation Accuracy:0.1691\n",
    "Epoch #162: Loss:2.4387, Accuracy:0.1819, Validation Loss:2.4622, Validation Accuracy:0.1773\n",
    "Epoch #163: Loss:2.4386, Accuracy:0.1803, Validation Loss:2.4595, Validation Accuracy:0.1675\n",
    "Epoch #164: Loss:2.4381, Accuracy:0.1795, Validation Loss:2.4605, Validation Accuracy:0.1839\n",
    "Epoch #165: Loss:2.4386, Accuracy:0.1840, Validation Loss:2.4568, Validation Accuracy:0.1691\n",
    "Epoch #166: Loss:2.4384, Accuracy:0.1786, Validation Loss:2.4568, Validation Accuracy:0.1708\n",
    "Epoch #167: Loss:2.4376, Accuracy:0.1799, Validation Loss:2.4599, Validation Accuracy:0.1839\n",
    "Epoch #168: Loss:2.4381, Accuracy:0.1811, Validation Loss:2.4578, Validation Accuracy:0.1741\n",
    "Epoch #169: Loss:2.4386, Accuracy:0.1782, Validation Loss:2.4577, Validation Accuracy:0.1790\n",
    "Epoch #170: Loss:2.4378, Accuracy:0.1811, Validation Loss:2.4574, Validation Accuracy:0.1790\n",
    "Epoch #171: Loss:2.4374, Accuracy:0.1799, Validation Loss:2.4585, Validation Accuracy:0.1790\n",
    "Epoch #172: Loss:2.4371, Accuracy:0.1823, Validation Loss:2.4591, Validation Accuracy:0.1790\n",
    "Epoch #173: Loss:2.4374, Accuracy:0.1807, Validation Loss:2.4570, Validation Accuracy:0.1708\n",
    "Epoch #174: Loss:2.4372, Accuracy:0.1782, Validation Loss:2.4596, Validation Accuracy:0.1724\n",
    "Epoch #175: Loss:2.4376, Accuracy:0.1815, Validation Loss:2.4577, Validation Accuracy:0.1773\n",
    "Epoch #176: Loss:2.4374, Accuracy:0.1791, Validation Loss:2.4590, Validation Accuracy:0.1790\n",
    "Epoch #177: Loss:2.4368, Accuracy:0.1811, Validation Loss:2.4581, Validation Accuracy:0.1806\n",
    "Epoch #178: Loss:2.4375, Accuracy:0.1799, Validation Loss:2.4590, Validation Accuracy:0.1790\n",
    "Epoch #179: Loss:2.4366, Accuracy:0.1819, Validation Loss:2.4577, Validation Accuracy:0.1790\n",
    "Epoch #180: Loss:2.4369, Accuracy:0.1807, Validation Loss:2.4583, Validation Accuracy:0.1790\n",
    "Epoch #181: Loss:2.4364, Accuracy:0.1811, Validation Loss:2.4589, Validation Accuracy:0.1790\n",
    "Epoch #182: Loss:2.4366, Accuracy:0.1819, Validation Loss:2.4582, Validation Accuracy:0.1790\n",
    "Epoch #183: Loss:2.4376, Accuracy:0.1807, Validation Loss:2.4601, Validation Accuracy:0.1839\n",
    "Epoch #184: Loss:2.4363, Accuracy:0.1815, Validation Loss:2.4571, Validation Accuracy:0.1757\n",
    "Epoch #185: Loss:2.4362, Accuracy:0.1828, Validation Loss:2.4594, Validation Accuracy:0.1790\n",
    "Epoch #186: Loss:2.4364, Accuracy:0.1828, Validation Loss:2.4579, Validation Accuracy:0.1773\n",
    "Epoch #187: Loss:2.4362, Accuracy:0.1807, Validation Loss:2.4582, Validation Accuracy:0.1773\n",
    "Epoch #188: Loss:2.4361, Accuracy:0.1811, Validation Loss:2.4583, Validation Accuracy:0.1806\n",
    "Epoch #189: Loss:2.4360, Accuracy:0.1828, Validation Loss:2.4575, Validation Accuracy:0.1806\n",
    "Epoch #190: Loss:2.4361, Accuracy:0.1828, Validation Loss:2.4581, Validation Accuracy:0.1790\n",
    "Epoch #191: Loss:2.4358, Accuracy:0.1815, Validation Loss:2.4575, Validation Accuracy:0.1790\n",
    "Epoch #192: Loss:2.4372, Accuracy:0.1832, Validation Loss:2.4589, Validation Accuracy:0.1806\n",
    "Epoch #193: Loss:2.4360, Accuracy:0.1815, Validation Loss:2.4571, Validation Accuracy:0.1790\n",
    "Epoch #194: Loss:2.4362, Accuracy:0.1815, Validation Loss:2.4589, Validation Accuracy:0.1790\n",
    "Epoch #195: Loss:2.4362, Accuracy:0.1828, Validation Loss:2.4597, Validation Accuracy:0.1839\n",
    "Epoch #196: Loss:2.4354, Accuracy:0.1828, Validation Loss:2.4572, Validation Accuracy:0.1757\n",
    "Epoch #197: Loss:2.4355, Accuracy:0.1823, Validation Loss:2.4577, Validation Accuracy:0.1790\n",
    "Epoch #198: Loss:2.4357, Accuracy:0.1823, Validation Loss:2.4591, Validation Accuracy:0.1790\n",
    "Epoch #199: Loss:2.4357, Accuracy:0.1811, Validation Loss:2.4584, Validation Accuracy:0.1790\n",
    "Epoch #200: Loss:2.4357, Accuracy:0.1828, Validation Loss:2.4583, Validation Accuracy:0.1806\n",
    "Epoch #201: Loss:2.4356, Accuracy:0.1828, Validation Loss:2.4584, Validation Accuracy:0.1839\n",
    "Epoch #202: Loss:2.4360, Accuracy:0.1823, Validation Loss:2.4579, Validation Accuracy:0.1773\n",
    "Epoch #203: Loss:2.4355, Accuracy:0.1819, Validation Loss:2.4586, Validation Accuracy:0.1790\n",
    "Epoch #204: Loss:2.4352, Accuracy:0.1819, Validation Loss:2.4575, Validation Accuracy:0.1790\n",
    "Epoch #205: Loss:2.4362, Accuracy:0.1828, Validation Loss:2.4587, Validation Accuracy:0.1790\n",
    "Epoch #206: Loss:2.4377, Accuracy:0.1819, Validation Loss:2.4576, Validation Accuracy:0.1773\n",
    "Epoch #207: Loss:2.4363, Accuracy:0.1836, Validation Loss:2.4603, Validation Accuracy:0.1806\n",
    "Epoch #208: Loss:2.4344, Accuracy:0.1844, Validation Loss:2.4584, Validation Accuracy:0.1757\n",
    "Epoch #209: Loss:2.4357, Accuracy:0.1815, Validation Loss:2.4599, Validation Accuracy:0.1839\n",
    "Epoch #210: Loss:2.4372, Accuracy:0.1782, Validation Loss:2.4591, Validation Accuracy:0.1806\n",
    "Epoch #211: Loss:2.4369, Accuracy:0.1828, Validation Loss:2.4571, Validation Accuracy:0.1773\n",
    "Epoch #212: Loss:2.4342, Accuracy:0.1823, Validation Loss:2.4603, Validation Accuracy:0.1839\n",
    "Epoch #213: Loss:2.4348, Accuracy:0.1815, Validation Loss:2.4582, Validation Accuracy:0.1790\n",
    "Epoch #214: Loss:2.4347, Accuracy:0.1815, Validation Loss:2.4584, Validation Accuracy:0.1790\n",
    "Epoch #215: Loss:2.4337, Accuracy:0.1823, Validation Loss:2.4588, Validation Accuracy:0.1806\n",
    "Epoch #216: Loss:2.4345, Accuracy:0.1848, Validation Loss:2.4570, Validation Accuracy:0.1773\n",
    "Epoch #217: Loss:2.4335, Accuracy:0.1840, Validation Loss:2.4583, Validation Accuracy:0.1839\n",
    "Epoch #218: Loss:2.4343, Accuracy:0.1795, Validation Loss:2.4587, Validation Accuracy:0.1823\n",
    "Epoch #219: Loss:2.4340, Accuracy:0.1832, Validation Loss:2.4584, Validation Accuracy:0.1790\n",
    "Epoch #220: Loss:2.4338, Accuracy:0.1840, Validation Loss:2.4573, Validation Accuracy:0.1773\n",
    "Epoch #221: Loss:2.4337, Accuracy:0.1823, Validation Loss:2.4585, Validation Accuracy:0.1839\n",
    "Epoch #222: Loss:2.4346, Accuracy:0.1819, Validation Loss:2.4588, Validation Accuracy:0.1823\n",
    "Epoch #223: Loss:2.4342, Accuracy:0.1815, Validation Loss:2.4579, Validation Accuracy:0.1806\n",
    "Epoch #224: Loss:2.4345, Accuracy:0.1815, Validation Loss:2.4575, Validation Accuracy:0.1790\n",
    "Epoch #225: Loss:2.4340, Accuracy:0.1807, Validation Loss:2.4597, Validation Accuracy:0.1806\n",
    "Epoch #226: Loss:2.4333, Accuracy:0.1823, Validation Loss:2.4580, Validation Accuracy:0.1757\n",
    "Epoch #227: Loss:2.4343, Accuracy:0.1828, Validation Loss:2.4583, Validation Accuracy:0.1806\n",
    "Epoch #228: Loss:2.4338, Accuracy:0.1840, Validation Loss:2.4575, Validation Accuracy:0.1806\n",
    "Epoch #229: Loss:2.4336, Accuracy:0.1819, Validation Loss:2.4588, Validation Accuracy:0.1839\n",
    "Epoch #230: Loss:2.4345, Accuracy:0.1770, Validation Loss:2.4579, Validation Accuracy:0.1823\n",
    "Epoch #231: Loss:2.4334, Accuracy:0.1823, Validation Loss:2.4595, Validation Accuracy:0.1806\n",
    "Epoch #232: Loss:2.4336, Accuracy:0.1823, Validation Loss:2.4576, Validation Accuracy:0.1757\n",
    "Epoch #233: Loss:2.4339, Accuracy:0.1807, Validation Loss:2.4596, Validation Accuracy:0.1839\n",
    "Epoch #234: Loss:2.4336, Accuracy:0.1832, Validation Loss:2.4576, Validation Accuracy:0.1773\n",
    "Epoch #235: Loss:2.4330, Accuracy:0.1844, Validation Loss:2.4572, Validation Accuracy:0.1806\n",
    "Epoch #236: Loss:2.4345, Accuracy:0.1840, Validation Loss:2.4580, Validation Accuracy:0.1806\n",
    "Epoch #237: Loss:2.4333, Accuracy:0.1815, Validation Loss:2.4583, Validation Accuracy:0.1839\n",
    "Epoch #238: Loss:2.4346, Accuracy:0.1815, Validation Loss:2.4591, Validation Accuracy:0.1839\n",
    "Epoch #239: Loss:2.4323, Accuracy:0.1828, Validation Loss:2.4584, Validation Accuracy:0.1790\n",
    "Epoch #240: Loss:2.4334, Accuracy:0.1832, Validation Loss:2.4582, Validation Accuracy:0.1806\n",
    "Epoch #241: Loss:2.4335, Accuracy:0.1832, Validation Loss:2.4595, Validation Accuracy:0.1839\n",
    "Epoch #242: Loss:2.4357, Accuracy:0.1836, Validation Loss:2.4588, Validation Accuracy:0.1757\n",
    "Epoch #243: Loss:2.4321, Accuracy:0.1832, Validation Loss:2.4620, Validation Accuracy:0.1806\n",
    "Epoch #244: Loss:2.4354, Accuracy:0.1782, Validation Loss:2.4580, Validation Accuracy:0.1790\n",
    "Epoch #245: Loss:2.4342, Accuracy:0.1819, Validation Loss:2.4581, Validation Accuracy:0.1806\n",
    "Epoch #246: Loss:2.4338, Accuracy:0.1856, Validation Loss:2.4585, Validation Accuracy:0.1773\n",
    "Epoch #247: Loss:2.4356, Accuracy:0.1786, Validation Loss:2.4613, Validation Accuracy:0.1839\n",
    "Epoch #248: Loss:2.4322, Accuracy:0.1840, Validation Loss:2.4591, Validation Accuracy:0.1724\n",
    "Epoch #249: Loss:2.4335, Accuracy:0.1840, Validation Loss:2.4606, Validation Accuracy:0.1806\n",
    "Epoch #250: Loss:2.4331, Accuracy:0.1823, Validation Loss:2.4582, Validation Accuracy:0.1773\n",
    "Epoch #251: Loss:2.4323, Accuracy:0.1815, Validation Loss:2.4583, Validation Accuracy:0.1823\n",
    "Epoch #252: Loss:2.4327, Accuracy:0.1828, Validation Loss:2.4587, Validation Accuracy:0.1790\n",
    "Epoch #253: Loss:2.4332, Accuracy:0.1832, Validation Loss:2.4588, Validation Accuracy:0.1839\n",
    "Epoch #254: Loss:2.4326, Accuracy:0.1823, Validation Loss:2.4582, Validation Accuracy:0.1806\n",
    "Epoch #255: Loss:2.4325, Accuracy:0.1815, Validation Loss:2.4589, Validation Accuracy:0.1839\n",
    "Epoch #256: Loss:2.4318, Accuracy:0.1807, Validation Loss:2.4589, Validation Accuracy:0.1790\n",
    "Epoch #257: Loss:2.4323, Accuracy:0.1828, Validation Loss:2.4586, Validation Accuracy:0.1806\n",
    "Epoch #258: Loss:2.4324, Accuracy:0.1823, Validation Loss:2.4589, Validation Accuracy:0.1839\n",
    "Epoch #259: Loss:2.4322, Accuracy:0.1823, Validation Loss:2.4585, Validation Accuracy:0.1806\n",
    "Epoch #260: Loss:2.4314, Accuracy:0.1840, Validation Loss:2.4591, Validation Accuracy:0.1839\n",
    "Epoch #261: Loss:2.4314, Accuracy:0.1836, Validation Loss:2.4589, Validation Accuracy:0.1839\n",
    "Epoch #262: Loss:2.4330, Accuracy:0.1832, Validation Loss:2.4592, Validation Accuracy:0.1839\n",
    "Epoch #263: Loss:2.4323, Accuracy:0.1819, Validation Loss:2.4604, Validation Accuracy:0.1839\n",
    "Epoch #264: Loss:2.4337, Accuracy:0.1791, Validation Loss:2.4583, Validation Accuracy:0.1790\n",
    "Epoch #265: Loss:2.4320, Accuracy:0.1836, Validation Loss:2.4611, Validation Accuracy:0.1806\n",
    "Epoch #266: Loss:2.4316, Accuracy:0.1828, Validation Loss:2.4580, Validation Accuracy:0.1806\n",
    "Epoch #267: Loss:2.4341, Accuracy:0.1807, Validation Loss:2.4588, Validation Accuracy:0.1839\n",
    "Epoch #268: Loss:2.4349, Accuracy:0.1791, Validation Loss:2.4629, Validation Accuracy:0.1806\n",
    "Epoch #269: Loss:2.4337, Accuracy:0.1803, Validation Loss:2.4598, Validation Accuracy:0.1724\n",
    "Epoch #270: Loss:2.4320, Accuracy:0.1803, Validation Loss:2.4622, Validation Accuracy:0.1839\n",
    "Epoch #271: Loss:2.4346, Accuracy:0.1819, Validation Loss:2.4590, Validation Accuracy:0.1806\n",
    "Epoch #272: Loss:2.4319, Accuracy:0.1840, Validation Loss:2.4587, Validation Accuracy:0.1757\n",
    "Epoch #273: Loss:2.4326, Accuracy:0.1828, Validation Loss:2.4615, Validation Accuracy:0.1839\n",
    "Epoch #274: Loss:2.4328, Accuracy:0.1860, Validation Loss:2.4583, Validation Accuracy:0.1757\n",
    "Epoch #275: Loss:2.4319, Accuracy:0.1840, Validation Loss:2.4599, Validation Accuracy:0.1839\n",
    "Epoch #276: Loss:2.4315, Accuracy:0.1815, Validation Loss:2.4592, Validation Accuracy:0.1823\n",
    "Epoch #277: Loss:2.4309, Accuracy:0.1815, Validation Loss:2.4605, Validation Accuracy:0.1806\n",
    "Epoch #278: Loss:2.4316, Accuracy:0.1832, Validation Loss:2.4588, Validation Accuracy:0.1823\n",
    "Epoch #279: Loss:2.4307, Accuracy:0.1836, Validation Loss:2.4598, Validation Accuracy:0.1839\n",
    "Epoch #280: Loss:2.4315, Accuracy:0.1836, Validation Loss:2.4585, Validation Accuracy:0.1823\n",
    "Epoch #281: Loss:2.4320, Accuracy:0.1844, Validation Loss:2.4588, Validation Accuracy:0.1839\n",
    "Epoch #282: Loss:2.4299, Accuracy:0.1844, Validation Loss:2.4620, Validation Accuracy:0.1806\n",
    "Epoch #283: Loss:2.4330, Accuracy:0.1844, Validation Loss:2.4590, Validation Accuracy:0.1856\n",
    "Epoch #284: Loss:2.4305, Accuracy:0.1844, Validation Loss:2.4598, Validation Accuracy:0.1839\n",
    "Epoch #285: Loss:2.4305, Accuracy:0.1840, Validation Loss:2.4603, Validation Accuracy:0.1839\n",
    "Epoch #286: Loss:2.4310, Accuracy:0.1852, Validation Loss:2.4594, Validation Accuracy:0.1856\n",
    "Epoch #287: Loss:2.4328, Accuracy:0.1836, Validation Loss:2.4599, Validation Accuracy:0.1839\n",
    "Epoch #288: Loss:2.4339, Accuracy:0.1860, Validation Loss:2.4594, Validation Accuracy:0.1790\n",
    "Epoch #289: Loss:2.4317, Accuracy:0.1815, Validation Loss:2.4622, Validation Accuracy:0.1839\n",
    "Epoch #290: Loss:2.4319, Accuracy:0.1803, Validation Loss:2.4597, Validation Accuracy:0.1741\n",
    "Epoch #291: Loss:2.4309, Accuracy:0.1832, Validation Loss:2.4603, Validation Accuracy:0.1823\n",
    "Epoch #292: Loss:2.4311, Accuracy:0.1856, Validation Loss:2.4598, Validation Accuracy:0.1856\n",
    "Epoch #293: Loss:2.4324, Accuracy:0.1852, Validation Loss:2.4594, Validation Accuracy:0.1856\n",
    "Epoch #294: Loss:2.4319, Accuracy:0.1823, Validation Loss:2.4616, Validation Accuracy:0.1806\n",
    "Epoch #295: Loss:2.4310, Accuracy:0.1836, Validation Loss:2.4594, Validation Accuracy:0.1806\n",
    "Epoch #296: Loss:2.4310, Accuracy:0.1836, Validation Loss:2.4610, Validation Accuracy:0.1806\n",
    "Epoch #297: Loss:2.4313, Accuracy:0.1840, Validation Loss:2.4597, Validation Accuracy:0.1856\n",
    "Epoch #298: Loss:2.4312, Accuracy:0.1844, Validation Loss:2.4589, Validation Accuracy:0.1856\n",
    "Epoch #299: Loss:2.4323, Accuracy:0.1766, Validation Loss:2.4617, Validation Accuracy:0.1823\n",
    "Epoch #300: Loss:2.4309, Accuracy:0.1819, Validation Loss:2.4592, Validation Accuracy:0.1757\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45915055, Accuracy:0.1757\n",
    "Labels: ['yd', 'ek', 'ck', 'by', 'ib', 'mb', 'my', 'sk', 'sg', 'eo', 'ce', 'eg', 'eb', 'aa', 'ds']\n",
    "Confusion Matrix:\n",
    "      yd  ek  ck  by  ib  mb  my  sk  sg  eo  ce  eg  eb  aa  ds\n",
    "t:yd  22   1   0   0   9   0   0   0  30   0   0   0   0   0   0\n",
    "t:ek   6   4   0   3   1   0   0   0  26   0   0   8   0   0   0\n",
    "t:ck   0   5   0   1   1   0   0   0   6   0   0  10   0   0   0\n",
    "t:by   1   4   0   2   2   0   0   0  25   0   0   5   0   0   1\n",
    "t:ib  26   0   0   0   7   0   0   0  18   0   0   2   1   0   0\n",
    "t:mb   7   5   0   4   1   0   0   0  27   0   0   7   1   0   0\n",
    "t:my   5   0   0   1   1   0   0   0   5   0   0   6   0   0   2\n",
    "t:sk   3   2   0   0   1   0   0   0   9   0   0  16   0   0   2\n",
    "t:sg  10   1   0   2   2   0   0   0  31   0   0   5   0   0   0\n",
    "t:eo   2   4   0   2   1   0   0   0  19   0   0   2   4   0   0\n",
    "t:ce   5   1   0   1   1   0   0   0  12   0   0   7   0   0   0\n",
    "t:eg   1   5   0   0   0   0   0   0   4   0   0  32   4   0   4\n",
    "t:eb   3   6   0   5   1   0   0   0  17   0   0  16   1   0   1\n",
    "t:aa   2   5   0   0   0   0   0   0  10   0   0  13   0   0   4\n",
    "t:ds   2   2   0   0   0   0   0   0   9   0   0  10   0   0   8\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.23      0.35      0.28        62\n",
    "          ek       0.09      0.08      0.09        48\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          by       0.10      0.05      0.07        40\n",
    "          ib       0.25      0.13      0.17        54\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          my       0.00      0.00      0.00        20\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          sg       0.12      0.61      0.21        51\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          eg       0.23      0.64      0.34        50\n",
    "          eb       0.09      0.02      0.03        50\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ds       0.36      0.26      0.30        31\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.10      0.14      0.10       609\n",
    "weighted avg       0.11      0.18      0.12       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 06:24:36 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 29 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7035263712183006, 2.694369010345885, 2.6862819535391673, 2.6794448458698192, 2.6730214523760165, 2.6665487896241187, 2.6603299279518313, 2.6540495617244826, 2.6502619114611146, 2.64430381355223, 2.6356003973480124, 2.623911346708025, 2.6090062013009105, 2.5907648580610654, 2.570755919016445, 2.550612856406101, 2.5337643024369414, 2.5198679169997793, 2.5115239256121256, 2.512644872289573, 2.499102548621167, 2.4977785241231936, 2.4994819849386984, 2.4856853614299754, 2.482807126147015, 2.483890245700705, 2.478452775278702, 2.4737132977578047, 2.4719001146764397, 2.4687576086454595, 2.468454454136991, 2.4662542993016237, 2.4659879673486467, 2.4775226985292482, 2.476878991463697, 2.4608442783355713, 2.461451189075589, 2.46384071285893, 2.461710909513026, 2.467446417252614, 2.461814497883488, 2.4596157367593547, 2.460588544068861, 2.4628854103276296, 2.4580670362231376, 2.456601981458993, 2.458245927281372, 2.4574025652091493, 2.457477954808127, 2.457914675789318, 2.4557725768566914, 2.454729358942442, 2.4568114386403503, 2.454371134831596, 2.4529988863589534, 2.4551915672220814, 2.4540448948276063, 2.453451643632159, 2.4532819005656124, 2.451474277452491, 2.451581563855627, 2.4542117725647925, 2.4531786798060624, 2.453521646693814, 2.4567285742861493, 2.4553842192212936, 2.45352048552878, 2.4537499659558626, 2.4572605162810026, 2.453184039330443, 2.4536849285777174, 2.4535127946700173, 2.4524164579576264, 2.4544841045229306, 2.4530949056246403, 2.4530837880371044, 2.4545019467671714, 2.4528924146505022, 2.4504502943192406, 2.4512342443607125, 2.4535067586475994, 2.45174963603466, 2.4503685535468493, 2.4505669487325235, 2.4499054640188986, 2.4496974428299025, 2.4505575854202797, 2.4509480731632127, 2.44889679252612, 2.4464488099948527, 2.446647656179218, 2.446932710058779, 2.4461724797297384, 2.4468432592445213, 2.446759942717153, 2.447210591414879, 2.448913129483929, 2.4487639319133288, 2.4482193271123327, 2.4491110180790594, 2.4497541527834237, 2.4491274000584395, 2.4553340573616214, 2.4537800348842476, 2.4543242137420354, 2.456516916920203, 2.454368878663663, 2.4527830106675723, 2.4556585708862455, 2.456181024291441, 2.4566217310518663, 2.45813127769821, 2.4738008968152827, 2.4600055574000566, 2.4646522188421542, 2.46190606627754, 2.4614185856285156, 2.46102431646513, 2.457661431411217, 2.4556814297079455, 2.454000833195027, 2.454722157057087, 2.453855873132966, 2.453451360583501, 2.453155356283454, 2.4526018939777745, 2.4575303563930717, 2.4592008367547846, 2.4634407001175904, 2.4624520322959413, 2.464358095269289, 2.464723877523137, 2.464300286006458, 2.462651071485823, 2.4616580999935005, 2.463784364252451, 2.461967229451648, 2.4637983193734203, 2.4625331682133167, 2.4617653539028073, 2.463196770898227, 2.4629563862467045, 2.4615836922562573, 2.4621242893544717, 2.463092385450216, 2.4623683418937894, 2.463496813045934, 2.4613181315423622, 2.465200079289955, 2.461465103481399, 2.4644392225738425, 2.4619446646403795, 2.4628623956921456, 2.466573895687736, 2.4605716680266783, 2.4620943124266876, 2.461240368719367, 2.4612191631680442, 2.461511713334884, 2.4602892755091874, 2.4591295449017303, 2.4622108020218723, 2.4595376368618167, 2.460522072264322, 2.456811250332737, 2.456778898223476, 2.4598936639200093, 2.457788981827609, 2.4577399940522042, 2.4573522213057344, 2.4585195770013115, 2.4591199169409492, 2.4570289625127133, 2.459590977635877, 2.4576873207718672, 2.4590223884739117, 2.458110502787999, 2.45897462293628, 2.4576824823232317, 2.458343086963021, 2.45893560646007, 2.4581521712304726, 2.460105767195252, 2.457148917985863, 2.459353604731693, 2.457858823594593, 2.458201303466396, 2.4582525705077574, 2.457456828729664, 2.458087906657377, 2.4574822107167864, 2.4588519712582793, 2.4570937258465144, 2.4589064144735855, 2.4596887523513318, 2.4572455726429356, 2.457730506637022, 2.4591410622025163, 2.458376436593693, 2.4583264591267153, 2.4584193351037786, 2.457880359956588, 2.4586237158094133, 2.457538405858433, 2.4586811069588745, 2.4576055784335082, 2.4602996989815495, 2.4583609096326655, 2.4598601036666845, 2.4590775258043913, 2.457149823115181, 2.460292686578284, 2.4581585893490043, 2.45835985417045, 2.4587628759186844, 2.4569552293160473, 2.458258130867493, 2.4587470807856917, 2.4584079567826245, 2.4572519289057437, 2.45854597373549, 2.458825686882282, 2.4578709661079743, 2.457536376755813, 2.459714348484534, 2.4580031788016385, 2.458266401134297, 2.4575350194533274, 2.4588218392996954, 2.4578857785962485, 2.4594830875522007, 2.457585857419545, 2.4596453492081616, 2.457635903006117, 2.4571726842858324, 2.4579655807006535, 2.4583349552843567, 2.459083276820692, 2.458377101738465, 2.4582440669117696, 2.4594949431020052, 2.4588070213305344, 2.4619862563504373, 2.457979739397422, 2.4581419196230634, 2.458515747427353, 2.4612840251578096, 2.459120112686909, 2.4606034559960825, 2.458235122104388, 2.4582687383410575, 2.4586693633757593, 2.458793487846362, 2.4582068098002465, 2.4589336740559546, 2.458874419796447, 2.4585537182286457, 2.458878154629361, 2.45845007622379, 2.4590596013468473, 2.4588899988259, 2.4592149531704255, 2.4603804802072458, 2.458327587014936, 2.4610810268101435, 2.4580304379925155, 2.458789161860649, 2.462907178452841, 2.459811178138495, 2.4622382671374994, 2.458983362210404, 2.45871920068863, 2.4615374351369925, 2.4583024152589745, 2.4598607993478256, 2.459216449452543, 2.4604648459329588, 2.4588154595473717, 2.459824526055497, 2.4585358466225107, 2.458781977200939, 2.461961746607312, 2.458996037544288, 2.4597544168995324, 2.46025432389358, 2.4593614083401283, 2.4599379350007657, 2.4594082068927183, 2.462214241278387, 2.459718771951735, 2.460316700301147, 2.459838473346629, 2.4593669651764367, 2.4615503927365507, 2.459361804529951, 2.4609570945620733, 2.459696933357978, 2.458926952922677, 2.461697269934543, 2.4591505049876194], 'val_acc': [0.07060755336005699, 0.10509031088579268, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.12643678060599736, 0.1362889974526388, 0.16912972043789862, 0.1724137927853611, 0.1724137927853611, 0.1724137927853611, 0.17077175666056635, 0.16748768431310387, 0.17569786503495058, 0.1609195398139249, 0.17405582891015584, 0.1806239734093348, 0.1674876842152309, 0.17405582891015584, 0.1773399013554913, 0.17569786513282357, 0.1773399012576183, 0.17569786503495058, 0.1773399012576183, 0.17569786503495058, 0.1756978649370776, 0.17077175656269336, 0.1576354675643354, 0.1806239734093348, 0.16912972043789862, 0.1691297205357716, 0.17405582900802882, 0.16748768431310387, 0.17077175666056635, 0.16584564818830913, 0.17569786513282357, 0.16748768431310387, 0.17569786513282357, 0.16584564818830913, 0.16584564818830913, 0.1724137927853611, 0.17405582891015584, 0.16584564818830913, 0.17405582881228285, 0.1756978649370776, 0.16584564818830913, 0.1756978649370776, 0.1756978649370776, 0.16912972034002563, 0.18062397331146185, 0.17733990115974532, 0.17898193738241305, 0.17898193748028604, 0.18062397360508078, 0.18226600972987553, 0.18719211592658597, 0.18226600972987553, 0.17569786513282357, 0.19047619025597626, 0.192118226478644, 0.192118226478644, 0.18883415413118151, 0.19540229872823348, 0.18555008188159203, 0.1970443349509012, 0.19376026260343873, 0.19376026240769278, 0.19376026260343873, 0.1871921179085138, 0.18883415403330855, 0.18883415403330855, 0.19540229853248753, 0.19540229853248753, 0.19868637087995, 0.19704433475515526, 0.1871921179085138, 0.18062397331146185, 0.20032840700474475, 0.1806239734093348, 0.1822660094362566, 0.1970443349509012, 0.20361247935220722, 0.19704433475515526, 0.17241379258961514, 0.1839080456589243, 0.19868637087995, 0.17733990106187233, 0.1839080456589243, 0.1839080456589243, 0.1839080456589243, 0.19868637087995, 0.19868637087995, 0.18062397331146185, 0.1904761900602303, 0.19047619025597626, 0.19376026250556577, 0.1839080456589243, 0.17241379258961514, 0.17569786483920463, 0.1773399012576183, 0.18555008188159203, 0.1822660094362566, 0.17405582891015584, 0.185550082077338, 0.18062397331146185, 0.17898193728454007, 0.17241379258961514, 0.16748768431310387, 0.17405582881228285, 0.17241379258961514, 0.17241379298110704, 0.17733990145336426, 0.16584564818830913, 0.17898193738241305, 0.1724137927853611, 0.17405582891015584, 0.17405582891015584, 0.17898193738241305, 0.17405582891015584, 0.17405582891015584, 0.1773399013554913, 0.17405582891015584, 0.1740558287144099, 0.1724137927853611, 0.18062397331146185, 0.1789819371866671, 0.17241379249174216, 0.1724137926874881, 0.17405582881228285, 0.17733990096399938, 0.17077175646482037, 0.18555008178371907, 0.1822660094362566, 0.1740558287144099, 0.16912972024215267, 0.1674876842152309, 0.16912972014427968, 0.16912972034002563, 0.16912972034002563, 0.17733990096399938, 0.17733990106187233, 0.16912972034002563, 0.1839080456589243, 0.1642036120635144, 0.18226600953412955, 0.17733990115974532, 0.17077175656269336, 0.17569786483920463, 0.16912972034002563, 0.17569786483920463, 0.17569786474133164, 0.1740558287144099, 0.17569786474133164, 0.16912972034002563, 0.16912972034002563, 0.17733990096399938, 0.1674876842152309, 0.18390804556105134, 0.16912972024215267, 0.17077175646482037, 0.18390804546317835, 0.17405582900802882, 0.17898193708879412, 0.17898193708879412, 0.1789819371866671, 0.17898193708879412, 0.17077175646482037, 0.17241379249174216, 0.17733990106187233, 0.1789819371866671, 0.1806239734093348, 0.17898193728454007, 0.1789819371866671, 0.17898193708879412, 0.17898193708879412, 0.17898193738241305, 0.18390804546317835, 0.17569786503495058, 0.1789819371866671, 0.17733990106187233, 0.17733990106187233, 0.1806239734093348, 0.1806239734093348, 0.17898193708879412, 0.17898193708879412, 0.1806239734093348, 0.17898193738241305, 0.17898193708879412, 0.18390804546317835, 0.17569786503495058, 0.1789819371866671, 0.17898193708879412, 0.1789819371866671, 0.1806239734093348, 0.18390804546317835, 0.17733990096399938, 0.17898193708879412, 0.17898193738241305, 0.17898193708879412, 0.17733990096399938, 0.1806239734093348, 0.17569786503495058, 0.18390804546317835, 0.1806239734093348, 0.1773399012576183, 0.18390804546317835, 0.17898193738241305, 0.17898193738241305, 0.1806239734093348, 0.17733990106187233, 0.18390804546317835, 0.1822660094362566, 0.1789819371866671, 0.17733990106187233, 0.18390804546317835, 0.18226600953412955, 0.1806239734093348, 0.17898193738241305, 0.18062397331146185, 0.17569786503495058, 0.1806239734093348, 0.1806239734093348, 0.18390804546317835, 0.18226600953412955, 0.1806239734093348, 0.17569786503495058, 0.18390804546317835, 0.17733990106187233, 0.1806239734093348, 0.1806239734093348, 0.18390804546317835, 0.18390804546317835, 0.17898193738241305, 0.1806239734093348, 0.18390804556105134, 0.17569786503495058, 0.1806239735072078, 0.17898193738241305, 0.1806239734093348, 0.17733990106187233, 0.18390804556105134, 0.1724137927853611, 0.1806239734093348, 0.17733990106187233, 0.1822660094362566, 0.17898193738241305, 0.18390804546317835, 0.1806239734093348, 0.18390804546317835, 0.17898193738241305, 0.1806239734093348, 0.18390804546317835, 0.1806239734093348, 0.18390804556105134, 0.18390804556105134, 0.18390804556105134, 0.18390804556105134, 0.17898193738241305, 0.18062397331146185, 0.1806239735072078, 0.18390804556105134, 0.1806239734093348, 0.1724137926874881, 0.18390804546317835, 0.1806239734093348, 0.17569786503495058, 0.18390804556105134, 0.17569786503495058, 0.18390804556105134, 0.1822660094362566, 0.1806239734093348, 0.18226600963200254, 0.18390804546317835, 0.18226600963200254, 0.18390804575679728, 0.1806239734093348, 0.18555008178371907, 0.18390804556105134, 0.18390804556105134, 0.18555008178371907, 0.18390804556105134, 0.1789819371866671, 0.18390804546317835, 0.17405582900802882, 0.18226600963200254, 0.18555008178371907, 0.18555008178371907, 0.1806239734093348, 0.18062397360508078, 0.1806239734093348, 0.18555008178371907, 0.18555008178371907, 0.18226600963200254, 0.17569786513282357], 'loss': [2.7111289017499107, 2.6997307468978287, 2.690939421918113, 2.6838734523961185, 2.676752750193069, 2.670323260559928, 2.664029172509603, 2.6582102805926815, 2.651869709731617, 2.6476290237732245, 2.639967435977787, 2.630741858825057, 2.6177638622769583, 2.6015060045880705, 2.583735160710141, 2.562634878922292, 2.545049114834357, 2.529965311496899, 2.519050893254838, 2.520446844708014, 2.5133663151053676, 2.5058659567235675, 2.4954205738200788, 2.4928698798224667, 2.489622190111227, 2.4833510744498253, 2.4812535520941323, 2.4778629694631213, 2.4742058303566683, 2.472908277922832, 2.4704384783210207, 2.470081446351946, 2.4658325361764897, 2.464522660684292, 2.4730990864168203, 2.470274142318193, 2.4632912463476035, 2.4636478884264186, 2.461118066188491, 2.4599768966619973, 2.4589145445970537, 2.4590000227981035, 2.4560474064805424, 2.452903301368259, 2.4532048426124837, 2.4524022895452666, 2.4506115790020515, 2.4509834138764495, 2.450771100614105, 2.4503525865151405, 2.449995705528181, 2.4484373365829124, 2.4480381969553733, 2.4482203198654204, 2.4491166483939795, 2.4468063556193327, 2.446141749485807, 2.4447266767646743, 2.443964025470021, 2.4445093242049953, 2.443740428497659, 2.442676168202864, 2.4434835701262925, 2.4436537568329295, 2.443528936139367, 2.443048845228473, 2.442640522911808, 2.443103357213234, 2.4440126416129986, 2.4419862727609747, 2.442767834516521, 2.441851554416288, 2.4420859729484854, 2.441419249246742, 2.4408338910989937, 2.440988520968866, 2.441275390119768, 2.4403286787029166, 2.4405934089018335, 2.4421450455330724, 2.4411468075041407, 2.4421436496828615, 2.4410266472818427, 2.440613007398601, 2.4401798142544786, 2.4398773283439494, 2.4402385290643274, 2.4407955855069954, 2.440618200615446, 2.4395884781157946, 2.439316852969066, 2.438991745590429, 2.4382594349447952, 2.4386658722370314, 2.438206756482134, 2.4391005178251794, 2.4374501106920183, 2.438380742219929, 2.4388806322516845, 2.4403660115275296, 2.439228610483283, 2.440353054285539, 2.4393802687862327, 2.4427608450824967, 2.44257605178645, 2.442222728376761, 2.4407287171733945, 2.44168361178169, 2.437406038307801, 2.4415309752282175, 2.439734389111247, 2.445304511068293, 2.4447185037562, 2.4472944635630145, 2.4435375105918555, 2.4430118010518975, 2.4422024226531356, 2.442157076320609, 2.4384424417170654, 2.437937487811768, 2.4378420549741273, 2.4382846931167697, 2.4382214575577565, 2.438783771056659, 2.4375686355684816, 2.4371557562747777, 2.437621888096083, 2.441624242226446, 2.4414250737098206, 2.443281534222362, 2.4418928729924825, 2.441945100956629, 2.4420388823906745, 2.4426684614569254, 2.4429097073768444, 2.4450371922408776, 2.441994454189982, 2.4415462196240436, 2.4419392627857057, 2.4419020682144947, 2.4400841933256303, 2.440357564068428, 2.440388059616089, 2.4403747656506924, 2.439966206090406, 2.4436762373060663, 2.443247416474736, 2.4415642579722943, 2.4444794093803703, 2.4405890936724215, 2.4404767584751763, 2.441106868916224, 2.4410381904617715, 2.4412213068967974, 2.4404890966121666, 2.4395630191239, 2.4394541106919245, 2.438672673286109, 2.439046255910666, 2.4386689242885833, 2.44050232477991, 2.4386883509477304, 2.438603210057566, 2.438098436410422, 2.4385543483483474, 2.438417130084498, 2.4376335732501144, 2.4380611768248635, 2.4386180111025393, 2.4378450442633346, 2.4373885005896097, 2.4371369045617888, 2.437423972966, 2.437188970283806, 2.437569277095599, 2.4374178373348543, 2.436809896884268, 2.4374712434882255, 2.43663823041583, 2.436875570285492, 2.436361925606855, 2.4366237966431727, 2.4375811717838234, 2.436265101520922, 2.4361660586245497, 2.4363672857656615, 2.436196502767794, 2.4360839925997064, 2.4359965420356766, 2.436062354669434, 2.435781979707722, 2.4371696235218088, 2.4360387421241776, 2.4362407515915514, 2.436218280322253, 2.435430428624398, 2.435533319802255, 2.4356785858436285, 2.4357169092558246, 2.4357035267769187, 2.435579947230752, 2.435976417157684, 2.435537783761778, 2.4352426244982457, 2.43621126748698, 2.437700062757645, 2.4363360541067576, 2.434382575836025, 2.4356552127939963, 2.437221870383198, 2.436940505519295, 2.434214271705988, 2.4348451570318956, 2.4347492696813, 2.433709064205569, 2.4345022222099852, 2.4335081491137434, 2.4342955994654973, 2.4339507546512986, 2.433776575926638, 2.433711551298106, 2.434610976918277, 2.434156065555079, 2.434452457447561, 2.4340464003521802, 2.433339218731044, 2.4343321216669414, 2.4337907858697787, 2.4336199311994675, 2.434511160214089, 2.4333559317265694, 2.4336413573435443, 2.433947117568531, 2.433627576651759, 2.43304170024958, 2.4344631429080845, 2.433292428375025, 2.4345579096424017, 2.4323245721186457, 2.4333553707819946, 2.4334766580828404, 2.435658599610691, 2.4320561500055833, 2.435407222125075, 2.434225041371841, 2.4337641635714613, 2.4355691860833453, 2.432186205137437, 2.4335095666027655, 2.4330593964892, 2.4323205586576364, 2.4327461126159102, 2.43323546856091, 2.4326433399129943, 2.4324624760684537, 2.431765003615581, 2.4323192724701803, 2.4323589643168986, 2.4322438177386836, 2.4313664197432185, 2.4314466530292678, 2.432973258549183, 2.432291564902241, 2.4336527738238263, 2.431952888618015, 2.4316434953247006, 2.43407235028073, 2.4349038805560164, 2.433705404309032, 2.4320493094974966, 2.4345592658377773, 2.4319080236266526, 2.43259906866712, 2.4327928877953875, 2.4319295925281375, 2.4315232816418093, 2.4308883999896977, 2.431569858057543, 2.4307097654323067, 2.4315447593861292, 2.432030447111972, 2.4299492462949344, 2.432962689115771, 2.4305114311359257, 2.4305224433327113, 2.430954931599893, 2.432830907187178, 2.433891745220709, 2.4317440190599195, 2.431929101983135, 2.4309271014446594, 2.431109418418618, 2.43243119075313, 2.4319083389070735, 2.4310452954725075, 2.430952275802957, 2.431301704030752, 2.4311970597175114, 2.4322923327373527, 2.43093297809546], 'acc': [0.053798768508728036, 0.083367556991396, 0.10225872743178686, 0.10225872744096623, 0.10225872743178686, 0.10225872684430783, 0.1022587280009072, 0.10225872742260751, 0.10225872683512846, 0.10225872643429641, 0.10225872704931353, 0.11416837795497946, 0.12566735106510793, 0.12936344981438325, 0.14825461944393065, 0.14907597644250742, 0.15154004158180598, 0.15030800732009464, 0.15112936374955108, 0.1523613972095983, 0.15482546250800577, 0.15318275205408524, 0.15770020503283036, 0.16221765924160975, 0.15770020446371005, 0.1618069826210304, 0.1642710461202833, 0.16180698238848662, 0.16221765965162116, 0.1642710467444798, 0.1589322383245893, 0.1601642709370756, 0.1597535945123226, 0.17535934202235337, 0.17412730936397028, 0.17494866618507943, 0.1757700196404232, 0.1741273109489398, 0.17289527768471893, 0.17412731053892835, 0.1745379873920515, 0.17248459988918147, 0.17659137626570598, 0.1774127318934983, 0.17905544236577756, 0.17577002063791364, 0.17700205386541706, 0.17659137585569457, 0.17741273148348688, 0.17494866479593627, 0.1761806992167565, 0.17741273091436657, 0.17618069825598345, 0.17453798717786645, 0.17700205366959074, 0.1778234094932094, 0.18028747384920257, 0.1774127299168761, 0.17905544115410205, 0.18110882906698347, 0.1790554425616039, 0.17946611957383596, 0.1786447629669119, 0.17782340929738305, 0.1774127318934983, 0.18028747482833432, 0.1765913756598682, 0.17864476376857602, 0.1802874749874432, 0.17864476453352268, 0.17741273130601926, 0.17987679719190577, 0.179876795625295, 0.1790554425432452, 0.18069815144891366, 0.17618069745431936, 0.17864476277108554, 0.1761806976501457, 0.1782340869338116, 0.17782340911991543, 0.17905544117246075, 0.17700205229880628, 0.17494866479593627, 0.1798767968002531, 0.17782340968903576, 0.17946611957383596, 0.1757700208153813, 0.17700205347376438, 0.17577002083374, 0.17905544078080807, 0.1802874740266702, 0.17618069708102538, 0.17700205347376438, 0.17659137626570598, 0.17741272988015866, 0.17659137487656282, 0.17453798837118326, 0.1761806984334511, 0.17618069708102538, 0.17577002143957776, 0.17453798815699817, 0.17618069862927743, 0.1733059539136456, 0.17823408613214747, 0.1782340857404948, 0.17700205427542848, 0.17700205427542848, 0.1761806984334511, 0.17946611877217186, 0.17577002083374, 0.1778234091015567, 0.17330595592698522, 0.17905544195576614, 0.17782340792659862, 0.17494866598925307, 0.17946611879053057, 0.17659137626570598, 0.1774127299168761, 0.17412731075311344, 0.1733059543420157, 0.17289527691977224, 0.1733059539320043, 0.1774127318751396, 0.1782340865421589, 0.17494866438592485, 0.1790554425616039, 0.1728952763322932, 0.17412730895395886, 0.17535934202235337, 0.17782340851407766, 0.1765913756598682, 0.17905544257996264, 0.17453798659038738, 0.17987679738773213, 0.1700205337891099, 0.17905544038915536, 0.1733059555169738, 0.177823408709904, 0.17946611918218328, 0.17823408732546428, 0.17700205329629676, 0.1774127318934983, 0.1745379885670096, 0.17741273013106118, 0.1749486653834153, 0.1733059543420157, 0.1790554405849817, 0.17535934321567018, 0.1778234094932094, 0.17782340812242498, 0.17659137526821553, 0.17946611839887788, 0.17453798874447723, 0.1790554405849817, 0.1790554405849817, 0.17782340952992684, 0.1811088286753308, 0.17946611877217186, 0.1790554409766344, 0.17864476294855317, 0.1774127299168761, 0.18193018528225485, 0.1802874732250061, 0.17946611879053057, 0.18398357258929854, 0.1786447637502173, 0.1798767976019172, 0.181108830651953, 0.17823408554466844, 0.18110883043776793, 0.1798767976019172, 0.18234086192119292, 0.18069815244640414, 0.17823408711127922, 0.1815195062934006, 0.1790554425616039, 0.18110883004611522, 0.17987679619441532, 0.18193018371564407, 0.18069815164474, 0.18110882985028887, 0.18193018369728534, 0.18069815184056637, 0.1815195082700228, 0.1827515401267418, 0.18275154030420943, 0.18069815125308733, 0.1811088290853422, 0.18275154034092686, 0.18275154112423225, 0.18151950807419645, 0.1831622169431475, 0.1815195066850533, 0.18151950727253235, 0.18275153953926274, 0.18275153991255672, 0.18234086233120433, 0.1823408615111815, 0.181108830651953, 0.18275154034092686, 0.1827515409100472, 0.18234086170700786, 0.1819301854964399, 0.18193018449894946, 0.1827515409284059, 0.18193018410729678, 0.18357289418792333, 0.18439425081320612, 0.181519507076706, 0.17823408613214747, 0.18275154071422084, 0.18234086311450973, 0.1815195074867174, 0.18151950768254377, 0.18234086133371388, 0.18480492725631784, 0.18398357219764583, 0.1794661188088893, 0.18316221733480018, 0.18398357219764583, 0.18234086350616244, 0.18193018449894946, 0.18151950609757425, 0.18151950688087964, 0.1806981526422305, 0.1823408617437253, 0.18275154073257954, 0.18398357258929854, 0.18193018588809262, 0.1770020538837758, 0.1823408625270307, 0.182340862135378, 0.18069815084307592, 0.18316221833229065, 0.18439424979735694, 0.18398357317677758, 0.18151950766418504, 0.1815195074867174, 0.18275154110587352, 0.18316221872394334, 0.18316221715733255, 0.18357289614618683, 0.18316221737151764, 0.17823408691545287, 0.18193018567390756, 0.18562628364905684, 0.17864476414187, 0.18398357298095122, 0.1839835739784417, 0.18234086311450973, 0.181519507076706, 0.1827515393434364, 0.18316221674732114, 0.1823408629186834, 0.18151950768254377, 0.18069815125308733, 0.18275154112423225, 0.18234086192119292, 0.1823408614928228, 0.18398357276676616, 0.18357289536288143, 0.18316221756734397, 0.18193018391147042, 0.1790554409766344, 0.18357289459793474, 0.18275154051839448, 0.18069815184056637, 0.17905544195576614, 0.1802874744183229, 0.1802874740266702, 0.18193018588809262, 0.18398357239347218, 0.18275154010838307, 0.18603696007380985, 0.183983573586789, 0.18151950768254377, 0.181519507076706, 0.1831622173531589, 0.18357289516705508, 0.1835728953445227, 0.18439425020736835, 0.18439425059902104, 0.18439424983407438, 0.18439425157815278, 0.1839835739784417, 0.18521560622681338, 0.18357289438374969, 0.18603696067964762, 0.18151950688087964, 0.18028747422249655, 0.18316221776317032, 0.1856262824373813, 0.18521560583516067, 0.18234086170700786, 0.18357289475704366, 0.18357289436539095, 0.1839835720018195, 0.18439425100903248, 0.17659137587405327, 0.18193018391147042]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
