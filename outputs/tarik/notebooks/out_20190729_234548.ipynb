{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf24.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 23:45:48 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '03', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000026206ED8550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000026242C46EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0858, Accuracy:0.3729, Validation Loss:1.0754, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0751, Accuracy:0.3721, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #3: Loss:1.0748, Accuracy:0.3914, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0752, Accuracy:0.3922, Validation Loss:1.0776, Validation Accuracy:0.3957\n",
    "Epoch #8: Loss:1.0751, Accuracy:0.4016, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #9: Loss:1.0739, Accuracy:0.3979, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #10: Loss:1.0743, Accuracy:0.3996, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #11: Loss:1.0744, Accuracy:0.3959, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #12: Loss:1.0743, Accuracy:0.3934, Validation Loss:1.0760, Validation Accuracy:0.3924\n",
    "Epoch #13: Loss:1.0746, Accuracy:0.3934, Validation Loss:1.0764, Validation Accuracy:0.3908\n",
    "Epoch #14: Loss:1.0744, Accuracy:0.3955, Validation Loss:1.0763, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3963, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #16: Loss:1.0742, Accuracy:0.3975, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #17: Loss:1.0741, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0741, Accuracy:0.3967, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #19: Loss:1.0738, Accuracy:0.3996, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #20: Loss:1.0738, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #22: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #23: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #24: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #25: Loss:1.0735, Accuracy:0.4021, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #26: Loss:1.0735, Accuracy:0.3979, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #27: Loss:1.0733, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #28: Loss:1.0733, Accuracy:0.4012, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #29: Loss:1.0733, Accuracy:0.4049, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #30: Loss:1.0733, Accuracy:0.4016, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #31: Loss:1.0732, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #32: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #33: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #34: Loss:1.0730, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #35: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #36: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #37: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #38: Loss:1.0729, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #39: Loss:1.0729, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #40: Loss:1.0729, Accuracy:0.4037, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #41: Loss:1.0726, Accuracy:0.4033, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #42: Loss:1.0725, Accuracy:0.4012, Validation Loss:1.0749, Validation Accuracy:0.3957\n",
    "Epoch #43: Loss:1.0726, Accuracy:0.4029, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0724, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #45: Loss:1.0723, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #46: Loss:1.0724, Accuracy:0.4033, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #47: Loss:1.0724, Accuracy:0.4029, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #48: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #49: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #50: Loss:1.0725, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #51: Loss:1.0724, Accuracy:0.3975, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #52: Loss:1.0725, Accuracy:0.4012, Validation Loss:1.0754, Validation Accuracy:0.3908\n",
    "Epoch #53: Loss:1.0725, Accuracy:0.4025, Validation Loss:1.0754, Validation Accuracy:0.3908\n",
    "Epoch #54: Loss:1.0720, Accuracy:0.4012, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #55: Loss:1.0729, Accuracy:0.3934, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #56: Loss:1.0726, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #57: Loss:1.0725, Accuracy:0.4025, Validation Loss:1.0755, Validation Accuracy:0.3957\n",
    "Epoch #58: Loss:1.0726, Accuracy:0.3971, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #59: Loss:1.0725, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0728, Accuracy:0.4021, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #61: Loss:1.0723, Accuracy:0.3988, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #62: Loss:1.0723, Accuracy:0.4012, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0723, Accuracy:0.4025, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #64: Loss:1.0726, Accuracy:0.3996, Validation Loss:1.0761, Validation Accuracy:0.3957\n",
    "Epoch #65: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #66: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0761, Validation Accuracy:0.3793\n",
    "Epoch #67: Loss:1.0726, Accuracy:0.4012, Validation Loss:1.0761, Validation Accuracy:0.3875\n",
    "Epoch #68: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0760, Validation Accuracy:0.3875\n",
    "Epoch #69: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0767, Validation Accuracy:0.3892\n",
    "Epoch #70: Loss:1.0730, Accuracy:0.4025, Validation Loss:1.0770, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0722, Accuracy:0.4000, Validation Loss:1.0763, Validation Accuracy:0.3859\n",
    "Epoch #72: Loss:1.0727, Accuracy:0.4008, Validation Loss:1.0762, Validation Accuracy:0.3859\n",
    "Epoch #73: Loss:1.0736, Accuracy:0.3959, Validation Loss:1.0759, Validation Accuracy:0.3957\n",
    "Epoch #74: Loss:1.0721, Accuracy:0.3975, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #75: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0764, Validation Accuracy:0.3892\n",
    "Epoch #76: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0761, Validation Accuracy:0.3875\n",
    "Epoch #77: Loss:1.0721, Accuracy:0.3955, Validation Loss:1.0764, Validation Accuracy:0.3842\n",
    "Epoch #78: Loss:1.0720, Accuracy:0.3988, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #79: Loss:1.0720, Accuracy:0.4008, Validation Loss:1.0763, Validation Accuracy:0.3859\n",
    "Epoch #80: Loss:1.0720, Accuracy:0.4016, Validation Loss:1.0761, Validation Accuracy:0.3842\n",
    "Epoch #81: Loss:1.0716, Accuracy:0.3979, Validation Loss:1.0758, Validation Accuracy:0.3859\n",
    "Epoch #82: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0760, Validation Accuracy:0.3924\n",
    "Epoch #83: Loss:1.0714, Accuracy:0.3988, Validation Loss:1.0760, Validation Accuracy:0.3859\n",
    "Epoch #84: Loss:1.0717, Accuracy:0.3975, Validation Loss:1.0759, Validation Accuracy:0.3892\n",
    "Epoch #85: Loss:1.0714, Accuracy:0.4000, Validation Loss:1.0759, Validation Accuracy:0.3908\n",
    "Epoch #86: Loss:1.0717, Accuracy:0.4029, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #87: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.3892\n",
    "Epoch #88: Loss:1.0721, Accuracy:0.4016, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #89: Loss:1.0718, Accuracy:0.3951, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #90: Loss:1.0714, Accuracy:0.4062, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #91: Loss:1.0716, Accuracy:0.3992, Validation Loss:1.0753, Validation Accuracy:0.3875\n",
    "Epoch #92: Loss:1.0708, Accuracy:0.4045, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #93: Loss:1.0711, Accuracy:0.4037, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #94: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #95: Loss:1.0714, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #96: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0756, Validation Accuracy:0.4007\n",
    "Epoch #97: Loss:1.0715, Accuracy:0.4053, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #98: Loss:1.0711, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #99: Loss:1.0711, Accuracy:0.3984, Validation Loss:1.0754, Validation Accuracy:0.3924\n",
    "Epoch #100: Loss:1.0706, Accuracy:0.4062, Validation Loss:1.0757, Validation Accuracy:0.3974\n",
    "Epoch #101: Loss:1.0711, Accuracy:0.3984, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #102: Loss:1.0706, Accuracy:0.4037, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #103: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #104: Loss:1.0708, Accuracy:0.4074, Validation Loss:1.0761, Validation Accuracy:0.3957\n",
    "Epoch #105: Loss:1.0717, Accuracy:0.3996, Validation Loss:1.0761, Validation Accuracy:0.3908\n",
    "Epoch #106: Loss:1.0706, Accuracy:0.4033, Validation Loss:1.0760, Validation Accuracy:0.4056\n",
    "Epoch #107: Loss:1.0711, Accuracy:0.3988, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #108: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0758, Validation Accuracy:0.3892\n",
    "Epoch #109: Loss:1.0707, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.4007\n",
    "Epoch #110: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0756, Validation Accuracy:0.4007\n",
    "Epoch #111: Loss:1.0713, Accuracy:0.3975, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #112: Loss:1.0710, Accuracy:0.4021, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #113: Loss:1.0708, Accuracy:0.4012, Validation Loss:1.0758, Validation Accuracy:0.4023\n",
    "Epoch #114: Loss:1.0705, Accuracy:0.4016, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #115: Loss:1.0707, Accuracy:0.4012, Validation Loss:1.0756, Validation Accuracy:0.3957\n",
    "Epoch #116: Loss:1.0703, Accuracy:0.4057, Validation Loss:1.0764, Validation Accuracy:0.3990\n",
    "Epoch #117: Loss:1.0704, Accuracy:0.4045, Validation Loss:1.0779, Validation Accuracy:0.4023\n",
    "Epoch #118: Loss:1.0704, Accuracy:0.4086, Validation Loss:1.0778, Validation Accuracy:0.3924\n",
    "Epoch #119: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0780, Validation Accuracy:0.3941\n",
    "Epoch #120: Loss:1.0702, Accuracy:0.4066, Validation Loss:1.0777, Validation Accuracy:0.4023\n",
    "Epoch #121: Loss:1.0705, Accuracy:0.4021, Validation Loss:1.0767, Validation Accuracy:0.3924\n",
    "Epoch #122: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0763, Validation Accuracy:0.3875\n",
    "Epoch #123: Loss:1.0701, Accuracy:0.4025, Validation Loss:1.0768, Validation Accuracy:0.3892\n",
    "Epoch #124: Loss:1.0702, Accuracy:0.4021, Validation Loss:1.0769, Validation Accuracy:0.3892\n",
    "Epoch #125: Loss:1.0702, Accuracy:0.4078, Validation Loss:1.0770, Validation Accuracy:0.3924\n",
    "Epoch #126: Loss:1.0702, Accuracy:0.4099, Validation Loss:1.0767, Validation Accuracy:0.3941\n",
    "Epoch #127: Loss:1.0703, Accuracy:0.4025, Validation Loss:1.0770, Validation Accuracy:0.3941\n",
    "Epoch #128: Loss:1.0696, Accuracy:0.4021, Validation Loss:1.0770, Validation Accuracy:0.3892\n",
    "Epoch #129: Loss:1.0696, Accuracy:0.4082, Validation Loss:1.0771, Validation Accuracy:0.3941\n",
    "Epoch #130: Loss:1.0694, Accuracy:0.4053, Validation Loss:1.0764, Validation Accuracy:0.3892\n",
    "Epoch #131: Loss:1.0709, Accuracy:0.4066, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #132: Loss:1.0692, Accuracy:0.4025, Validation Loss:1.0768, Validation Accuracy:0.3974\n",
    "Epoch #133: Loss:1.0701, Accuracy:0.4025, Validation Loss:1.0769, Validation Accuracy:0.3957\n",
    "Epoch #134: Loss:1.0690, Accuracy:0.4045, Validation Loss:1.0766, Validation Accuracy:0.3924\n",
    "Epoch #135: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0772, Validation Accuracy:0.3892\n",
    "Epoch #136: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0777, Validation Accuracy:0.3941\n",
    "Epoch #137: Loss:1.0701, Accuracy:0.4037, Validation Loss:1.0762, Validation Accuracy:0.3957\n",
    "Epoch #138: Loss:1.0693, Accuracy:0.4012, Validation Loss:1.0764, Validation Accuracy:0.3908\n",
    "Epoch #139: Loss:1.0703, Accuracy:0.4025, Validation Loss:1.0759, Validation Accuracy:0.3908\n",
    "Epoch #140: Loss:1.0697, Accuracy:0.4008, Validation Loss:1.0763, Validation Accuracy:0.3957\n",
    "Epoch #141: Loss:1.0696, Accuracy:0.4041, Validation Loss:1.0760, Validation Accuracy:0.3924\n",
    "Epoch #142: Loss:1.0687, Accuracy:0.4016, Validation Loss:1.0759, Validation Accuracy:0.3892\n",
    "Epoch #143: Loss:1.0692, Accuracy:0.4021, Validation Loss:1.0759, Validation Accuracy:0.3892\n",
    "Epoch #144: Loss:1.0684, Accuracy:0.4045, Validation Loss:1.0764, Validation Accuracy:0.3941\n",
    "Epoch #145: Loss:1.0685, Accuracy:0.4037, Validation Loss:1.0768, Validation Accuracy:0.3941\n",
    "Epoch #146: Loss:1.0687, Accuracy:0.4025, Validation Loss:1.0767, Validation Accuracy:0.3957\n",
    "Epoch #147: Loss:1.0687, Accuracy:0.4025, Validation Loss:1.0769, Validation Accuracy:0.3957\n",
    "Epoch #148: Loss:1.0685, Accuracy:0.4029, Validation Loss:1.0768, Validation Accuracy:0.3957\n",
    "Epoch #149: Loss:1.0687, Accuracy:0.4021, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #150: Loss:1.0683, Accuracy:0.4004, Validation Loss:1.0766, Validation Accuracy:0.3941\n",
    "Epoch #151: Loss:1.0690, Accuracy:0.4000, Validation Loss:1.0784, Validation Accuracy:0.4007\n",
    "Epoch #152: Loss:1.0678, Accuracy:0.4033, Validation Loss:1.0774, Validation Accuracy:0.3957\n",
    "Epoch #153: Loss:1.0680, Accuracy:0.4012, Validation Loss:1.0762, Validation Accuracy:0.3990\n",
    "Epoch #154: Loss:1.0681, Accuracy:0.4008, Validation Loss:1.0753, Validation Accuracy:0.4089\n",
    "Epoch #155: Loss:1.0680, Accuracy:0.4021, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #156: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0765, Validation Accuracy:0.4023\n",
    "Epoch #157: Loss:1.0684, Accuracy:0.4037, Validation Loss:1.0759, Validation Accuracy:0.4039\n",
    "Epoch #158: Loss:1.0672, Accuracy:0.4041, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #159: Loss:1.0673, Accuracy:0.4037, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #160: Loss:1.0677, Accuracy:0.4016, Validation Loss:1.0853, Validation Accuracy:0.3957\n",
    "Epoch #161: Loss:1.0752, Accuracy:0.3988, Validation Loss:1.0744, Validation Accuracy:0.3974\n",
    "Epoch #162: Loss:1.0742, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #163: Loss:1.0737, Accuracy:0.4008, Validation Loss:1.0752, Validation Accuracy:0.3957\n",
    "Epoch #164: Loss:1.0740, Accuracy:0.3988, Validation Loss:1.0753, Validation Accuracy:0.3744\n",
    "Epoch #165: Loss:1.0728, Accuracy:0.4025, Validation Loss:1.0776, Validation Accuracy:0.3596\n",
    "Epoch #166: Loss:1.0725, Accuracy:0.4000, Validation Loss:1.0733, Validation Accuracy:0.3826\n",
    "Epoch #167: Loss:1.0754, Accuracy:0.3979, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #168: Loss:1.0731, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #169: Loss:1.0729, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.4007\n",
    "Epoch #170: Loss:1.0726, Accuracy:0.4033, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #171: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #172: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #173: Loss:1.0718, Accuracy:0.4021, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #174: Loss:1.0714, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #175: Loss:1.0713, Accuracy:0.4045, Validation Loss:1.0752, Validation Accuracy:0.3924\n",
    "Epoch #176: Loss:1.0705, Accuracy:0.4062, Validation Loss:1.0759, Validation Accuracy:0.3924\n",
    "Epoch #177: Loss:1.0698, Accuracy:0.4066, Validation Loss:1.0764, Validation Accuracy:0.3941\n",
    "Epoch #178: Loss:1.0691, Accuracy:0.4066, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #179: Loss:1.0687, Accuracy:0.4078, Validation Loss:1.0771, Validation Accuracy:0.3924\n",
    "Epoch #180: Loss:1.0673, Accuracy:0.4070, Validation Loss:1.0771, Validation Accuracy:0.3908\n",
    "Epoch #181: Loss:1.0668, Accuracy:0.4062, Validation Loss:1.0795, Validation Accuracy:0.3892\n",
    "Epoch #182: Loss:1.0661, Accuracy:0.4078, Validation Loss:1.0813, Validation Accuracy:0.3859\n",
    "Epoch #183: Loss:1.0637, Accuracy:0.3832, Validation Loss:1.0812, Validation Accuracy:0.3924\n",
    "Epoch #184: Loss:1.0639, Accuracy:0.4094, Validation Loss:1.0803, Validation Accuracy:0.3908\n",
    "Epoch #185: Loss:1.0626, Accuracy:0.4082, Validation Loss:1.0861, Validation Accuracy:0.3760\n",
    "Epoch #186: Loss:1.0646, Accuracy:0.4033, Validation Loss:1.0829, Validation Accuracy:0.3908\n",
    "Epoch #187: Loss:1.0654, Accuracy:0.4086, Validation Loss:1.0778, Validation Accuracy:0.3941\n",
    "Epoch #188: Loss:1.0713, Accuracy:0.3926, Validation Loss:1.0883, Validation Accuracy:0.3924\n",
    "Epoch #189: Loss:1.0828, Accuracy:0.3951, Validation Loss:1.1203, Validation Accuracy:0.3859\n",
    "Epoch #190: Loss:1.1162, Accuracy:0.3663, Validation Loss:1.0846, Validation Accuracy:0.3727\n",
    "Epoch #191: Loss:1.0717, Accuracy:0.3988, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #192: Loss:1.0702, Accuracy:0.4041, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #193: Loss:1.0707, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #194: Loss:1.0707, Accuracy:0.4037, Validation Loss:1.0754, Validation Accuracy:0.3974\n",
    "Epoch #195: Loss:1.0697, Accuracy:0.4041, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #196: Loss:1.0691, Accuracy:0.4066, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #197: Loss:1.0689, Accuracy:0.4053, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #198: Loss:1.0688, Accuracy:0.4057, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #199: Loss:1.0683, Accuracy:0.4066, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #200: Loss:1.0678, Accuracy:0.4082, Validation Loss:1.0754, Validation Accuracy:0.3924\n",
    "Epoch #201: Loss:1.0671, Accuracy:0.4086, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #202: Loss:1.0666, Accuracy:0.4086, Validation Loss:1.0754, Validation Accuracy:0.3924\n",
    "Epoch #203: Loss:1.0660, Accuracy:0.4082, Validation Loss:1.0759, Validation Accuracy:0.3924\n",
    "Epoch #204: Loss:1.0651, Accuracy:0.4078, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #205: Loss:1.0645, Accuracy:0.4094, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #206: Loss:1.0638, Accuracy:0.4090, Validation Loss:1.0766, Validation Accuracy:0.3941\n",
    "Epoch #207: Loss:1.0628, Accuracy:0.4090, Validation Loss:1.0766, Validation Accuracy:0.3941\n",
    "Epoch #208: Loss:1.0621, Accuracy:0.4099, Validation Loss:1.0778, Validation Accuracy:0.3924\n",
    "Epoch #209: Loss:1.0613, Accuracy:0.4103, Validation Loss:1.0790, Validation Accuracy:0.3924\n",
    "Epoch #210: Loss:1.0605, Accuracy:0.4127, Validation Loss:1.0783, Validation Accuracy:0.3941\n",
    "Epoch #211: Loss:1.0606, Accuracy:0.4136, Validation Loss:1.0828, Validation Accuracy:0.3892\n",
    "Epoch #212: Loss:1.0603, Accuracy:0.3992, Validation Loss:1.0815, Validation Accuracy:0.3941\n",
    "Epoch #213: Loss:1.0612, Accuracy:0.4016, Validation Loss:1.0811, Validation Accuracy:0.3941\n",
    "Epoch #214: Loss:1.0608, Accuracy:0.4193, Validation Loss:1.0830, Validation Accuracy:0.3924\n",
    "Epoch #215: Loss:1.0603, Accuracy:0.3901, Validation Loss:1.0809, Validation Accuracy:0.3990\n",
    "Epoch #216: Loss:1.0577, Accuracy:0.4164, Validation Loss:1.0808, Validation Accuracy:0.3924\n",
    "Epoch #217: Loss:1.0595, Accuracy:0.4127, Validation Loss:1.0812, Validation Accuracy:0.3941\n",
    "Epoch #218: Loss:1.0572, Accuracy:0.4008, Validation Loss:1.0843, Validation Accuracy:0.3859\n",
    "Epoch #219: Loss:1.0596, Accuracy:0.4078, Validation Loss:1.0817, Validation Accuracy:0.3941\n",
    "Epoch #220: Loss:1.0600, Accuracy:0.4029, Validation Loss:1.0828, Validation Accuracy:0.3990\n",
    "Epoch #221: Loss:1.0573, Accuracy:0.4177, Validation Loss:1.0872, Validation Accuracy:0.3810\n",
    "Epoch #222: Loss:1.0584, Accuracy:0.3947, Validation Loss:1.0825, Validation Accuracy:0.3924\n",
    "Epoch #223: Loss:1.0562, Accuracy:0.3975, Validation Loss:1.0866, Validation Accuracy:0.3924\n",
    "Epoch #224: Loss:1.0560, Accuracy:0.4086, Validation Loss:1.0870, Validation Accuracy:0.3908\n",
    "Epoch #225: Loss:1.0537, Accuracy:0.4144, Validation Loss:1.0867, Validation Accuracy:0.3924\n",
    "Epoch #226: Loss:1.0552, Accuracy:0.4123, Validation Loss:1.0895, Validation Accuracy:0.3941\n",
    "Epoch #227: Loss:1.0536, Accuracy:0.4119, Validation Loss:1.0835, Validation Accuracy:0.3957\n",
    "Epoch #228: Loss:1.0598, Accuracy:0.4008, Validation Loss:1.0876, Validation Accuracy:0.3810\n",
    "Epoch #229: Loss:1.0668, Accuracy:0.4066, Validation Loss:1.0856, Validation Accuracy:0.3924\n",
    "Epoch #230: Loss:1.0605, Accuracy:0.3992, Validation Loss:1.0819, Validation Accuracy:0.3924\n",
    "Epoch #231: Loss:1.0549, Accuracy:0.4185, Validation Loss:1.0826, Validation Accuracy:0.3924\n",
    "Epoch #232: Loss:1.0520, Accuracy:0.4205, Validation Loss:1.0820, Validation Accuracy:0.3957\n",
    "Epoch #233: Loss:1.0524, Accuracy:0.4209, Validation Loss:1.0787, Validation Accuracy:0.3990\n",
    "Epoch #234: Loss:1.0509, Accuracy:0.4209, Validation Loss:1.0848, Validation Accuracy:0.3859\n",
    "Epoch #235: Loss:1.0515, Accuracy:0.4021, Validation Loss:1.0859, Validation Accuracy:0.3941\n",
    "Epoch #236: Loss:1.0531, Accuracy:0.4230, Validation Loss:1.0860, Validation Accuracy:0.3760\n",
    "Epoch #237: Loss:1.0529, Accuracy:0.4078, Validation Loss:1.0859, Validation Accuracy:0.3924\n",
    "Epoch #238: Loss:1.0534, Accuracy:0.4201, Validation Loss:1.1007, Validation Accuracy:0.3810\n",
    "Epoch #239: Loss:1.0572, Accuracy:0.3992, Validation Loss:1.0970, Validation Accuracy:0.3924\n",
    "Epoch #240: Loss:1.0588, Accuracy:0.4078, Validation Loss:1.0946, Validation Accuracy:0.3810\n",
    "Epoch #241: Loss:1.0473, Accuracy:0.4201, Validation Loss:1.0916, Validation Accuracy:0.3908\n",
    "Epoch #242: Loss:1.0521, Accuracy:0.4230, Validation Loss:1.0836, Validation Accuracy:0.3974\n",
    "Epoch #243: Loss:1.0519, Accuracy:0.3926, Validation Loss:1.0822, Validation Accuracy:0.3908\n",
    "Epoch #244: Loss:1.0481, Accuracy:0.4287, Validation Loss:1.0822, Validation Accuracy:0.3941\n",
    "Epoch #245: Loss:1.0431, Accuracy:0.4419, Validation Loss:1.0838, Validation Accuracy:0.3974\n",
    "Epoch #246: Loss:1.0417, Accuracy:0.4218, Validation Loss:1.0831, Validation Accuracy:0.3974\n",
    "Epoch #247: Loss:1.0397, Accuracy:0.4341, Validation Loss:1.0899, Validation Accuracy:0.3711\n",
    "Epoch #248: Loss:1.0399, Accuracy:0.4300, Validation Loss:1.0869, Validation Accuracy:0.3974\n",
    "Epoch #249: Loss:1.0372, Accuracy:0.4357, Validation Loss:1.0891, Validation Accuracy:0.3957\n",
    "Epoch #250: Loss:1.0399, Accuracy:0.4263, Validation Loss:1.0921, Validation Accuracy:0.3810\n",
    "Epoch #251: Loss:1.0382, Accuracy:0.4308, Validation Loss:1.0930, Validation Accuracy:0.3941\n",
    "Epoch #252: Loss:1.0420, Accuracy:0.4053, Validation Loss:1.1093, Validation Accuracy:0.3859\n",
    "Epoch #253: Loss:1.0455, Accuracy:0.4209, Validation Loss:1.0932, Validation Accuracy:0.3908\n",
    "Epoch #254: Loss:1.0387, Accuracy:0.4324, Validation Loss:1.0977, Validation Accuracy:0.3793\n",
    "Epoch #255: Loss:1.0400, Accuracy:0.4168, Validation Loss:1.0974, Validation Accuracy:0.3990\n",
    "Epoch #256: Loss:1.0453, Accuracy:0.4308, Validation Loss:1.1030, Validation Accuracy:0.3777\n",
    "Epoch #257: Loss:1.0478, Accuracy:0.4205, Validation Loss:1.0906, Validation Accuracy:0.4007\n",
    "Epoch #258: Loss:1.0440, Accuracy:0.3963, Validation Loss:1.0916, Validation Accuracy:0.3990\n",
    "Epoch #259: Loss:1.0426, Accuracy:0.4185, Validation Loss:1.0914, Validation Accuracy:0.3810\n",
    "Epoch #260: Loss:1.0407, Accuracy:0.4242, Validation Loss:1.0923, Validation Accuracy:0.3974\n",
    "Epoch #261: Loss:1.0367, Accuracy:0.4156, Validation Loss:1.0916, Validation Accuracy:0.3892\n",
    "Epoch #262: Loss:1.0351, Accuracy:0.4279, Validation Loss:1.0948, Validation Accuracy:0.3875\n",
    "Epoch #263: Loss:1.0330, Accuracy:0.4374, Validation Loss:1.0982, Validation Accuracy:0.3941\n",
    "Epoch #264: Loss:1.0414, Accuracy:0.4234, Validation Loss:1.1048, Validation Accuracy:0.3793\n",
    "Epoch #265: Loss:1.0480, Accuracy:0.4156, Validation Loss:1.1006, Validation Accuracy:0.3777\n",
    "Epoch #266: Loss:1.0397, Accuracy:0.4148, Validation Loss:1.0949, Validation Accuracy:0.3908\n",
    "Epoch #267: Loss:1.0407, Accuracy:0.4279, Validation Loss:1.0900, Validation Accuracy:0.3859\n",
    "Epoch #268: Loss:1.0389, Accuracy:0.4304, Validation Loss:1.0898, Validation Accuracy:0.3875\n",
    "Epoch #269: Loss:1.0381, Accuracy:0.4407, Validation Loss:1.0923, Validation Accuracy:0.3941\n",
    "Epoch #270: Loss:1.0374, Accuracy:0.4287, Validation Loss:1.0952, Validation Accuracy:0.3908\n",
    "Epoch #271: Loss:1.0349, Accuracy:0.4201, Validation Loss:1.0990, Validation Accuracy:0.3892\n",
    "Epoch #272: Loss:1.0346, Accuracy:0.4324, Validation Loss:1.0930, Validation Accuracy:0.3990\n",
    "Epoch #273: Loss:1.0324, Accuracy:0.4312, Validation Loss:1.0920, Validation Accuracy:0.3990\n",
    "Epoch #274: Loss:1.0326, Accuracy:0.4411, Validation Loss:1.0950, Validation Accuracy:0.3957\n",
    "Epoch #275: Loss:1.0303, Accuracy:0.4263, Validation Loss:1.0973, Validation Accuracy:0.3859\n",
    "Epoch #276: Loss:1.0311, Accuracy:0.4271, Validation Loss:1.0978, Validation Accuracy:0.4007\n",
    "Epoch #277: Loss:1.0302, Accuracy:0.4320, Validation Loss:1.0986, Validation Accuracy:0.3924\n",
    "Epoch #278: Loss:1.0269, Accuracy:0.4366, Validation Loss:1.1033, Validation Accuracy:0.3941\n",
    "Epoch #279: Loss:1.0238, Accuracy:0.4407, Validation Loss:1.1151, Validation Accuracy:0.3941\n",
    "Epoch #280: Loss:1.0352, Accuracy:0.4177, Validation Loss:1.1070, Validation Accuracy:0.3924\n",
    "Epoch #281: Loss:1.0283, Accuracy:0.4209, Validation Loss:1.1050, Validation Accuracy:0.3990\n",
    "Epoch #282: Loss:1.0259, Accuracy:0.4366, Validation Loss:1.1098, Validation Accuracy:0.3826\n",
    "Epoch #283: Loss:1.0349, Accuracy:0.4226, Validation Loss:1.1138, Validation Accuracy:0.3924\n",
    "Epoch #284: Loss:1.0311, Accuracy:0.4415, Validation Loss:1.1079, Validation Accuracy:0.3744\n",
    "Epoch #285: Loss:1.0290, Accuracy:0.4415, Validation Loss:1.1002, Validation Accuracy:0.4007\n",
    "Epoch #286: Loss:1.0311, Accuracy:0.4333, Validation Loss:1.0996, Validation Accuracy:0.3908\n",
    "Epoch #287: Loss:1.0248, Accuracy:0.4476, Validation Loss:1.1006, Validation Accuracy:0.3892\n",
    "Epoch #288: Loss:1.0253, Accuracy:0.4452, Validation Loss:1.1088, Validation Accuracy:0.3875\n",
    "Epoch #289: Loss:1.0335, Accuracy:0.4407, Validation Loss:1.1084, Validation Accuracy:0.3875\n",
    "Epoch #290: Loss:1.0251, Accuracy:0.4464, Validation Loss:1.1100, Validation Accuracy:0.3974\n",
    "Epoch #291: Loss:1.0270, Accuracy:0.4415, Validation Loss:1.1140, Validation Accuracy:0.3810\n",
    "Epoch #292: Loss:1.0288, Accuracy:0.4308, Validation Loss:1.1166, Validation Accuracy:0.3974\n",
    "Epoch #293: Loss:1.0213, Accuracy:0.4534, Validation Loss:1.1107, Validation Accuracy:0.3793\n",
    "Epoch #294: Loss:1.0275, Accuracy:0.4353, Validation Loss:1.1029, Validation Accuracy:0.3892\n",
    "Epoch #295: Loss:1.0239, Accuracy:0.4242, Validation Loss:1.1101, Validation Accuracy:0.3974\n",
    "Epoch #296: Loss:1.0177, Accuracy:0.4427, Validation Loss:1.1180, Validation Accuracy:0.3908\n",
    "Epoch #297: Loss:1.0297, Accuracy:0.4320, Validation Loss:1.1180, Validation Accuracy:0.3859\n",
    "Epoch #298: Loss:1.0196, Accuracy:0.4452, Validation Loss:1.1055, Validation Accuracy:0.3941\n",
    "Epoch #299: Loss:1.0202, Accuracy:0.4374, Validation Loss:1.1315, Validation Accuracy:0.3908\n",
    "Epoch #300: Loss:1.0240, Accuracy:0.4526, Validation Loss:1.1214, Validation Accuracy:0.3826\n",
    "\n",
    "Test:\n",
    "Test Loss:1.12135768, Accuracy:0.3826\n",
    "Labels: ['01', '03', '02']\n",
    "Confusion Matrix:\n",
    "      01  03   02\n",
    "t:01  16   6  218\n",
    "t:03   5   6  131\n",
    "t:02   8   8  211\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.55      0.07      0.12       240\n",
    "          03       0.30      0.04      0.07       142\n",
    "          02       0.38      0.93      0.54       227\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.41      0.35      0.24       609\n",
    "weighted avg       0.43      0.38      0.26       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 00:26:25 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 37 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.075387633297048, 1.0730496842677175, 1.074769026540183, 1.0745928729891974, 1.0738777769805958, 1.0749360835806685, 1.077632477326542, 1.0748427644347518, 1.0753951254736613, 1.0742926317678492, 1.075860618370507, 1.076038147036861, 1.0764477693388614, 1.0762657860816993, 1.075410719967045, 1.075070394671022, 1.0749460531181498, 1.0747778932449266, 1.0748901145994565, 1.074693554532156, 1.0746512990475483, 1.0747387949469054, 1.0746361056376366, 1.074655128817253, 1.0746823292843422, 1.074700702000134, 1.0746583390509945, 1.074497644537188, 1.0747088869216994, 1.0748081733832022, 1.0746618106055925, 1.0746328683909525, 1.0745945362426181, 1.0744374485438681, 1.0744016468035569, 1.0744707001058142, 1.0745516181579364, 1.0743460250018266, 1.074663889819178, 1.074589289859402, 1.0746684806491746, 1.074883706463969, 1.0749146077041751, 1.0750010391370024, 1.0751375010839628, 1.07497538486725, 1.0750223120249356, 1.0751222943633256, 1.0750657077297592, 1.0750563888518485, 1.0750473555672933, 1.0753620130870924, 1.0753615115859434, 1.0756212678448906, 1.075881799062093, 1.0754663263048445, 1.0755040545768926, 1.0756219525642583, 1.075227029022129, 1.0750185290385155, 1.0750035758088963, 1.07534200569679, 1.0757470651604664, 1.0761285143337032, 1.0760144527714044, 1.0761140880522078, 1.0761026948543604, 1.0759734700270278, 1.0766692823181403, 1.077040677391641, 1.0762946801428332, 1.076206151487792, 1.0759443003555824, 1.0757139817442996, 1.076372143670256, 1.076084573672127, 1.0763878884965368, 1.0759432123995376, 1.0762500363617695, 1.0760691326435758, 1.075779275941144, 1.075963083942144, 1.0760489645458402, 1.075868985531561, 1.0758551909222782, 1.075383927043044, 1.0752620442551737, 1.07476360946649, 1.0756007352681778, 1.0752091092624882, 1.075334238105611, 1.075104009341724, 1.074968232309877, 1.075020982909868, 1.0748826746870144, 1.0756311479264684, 1.075420256905955, 1.0753089647574965, 1.0754387813248658, 1.0756876388402603, 1.0758962656672562, 1.075830072213472, 1.0756352204211632, 1.0761354102681226, 1.0760979832491069, 1.076030691856234, 1.0755865881204214, 1.0758486306921797, 1.0751384578902146, 1.0755819912223001, 1.0756610359855865, 1.075436078268906, 1.0757867544155402, 1.0748409220737776, 1.0755788831679496, 1.0763981076101168, 1.0779238974519552, 1.0777546956229875, 1.0780039428685881, 1.0776730008508968, 1.076681161553206, 1.0762577612803292, 1.076754556500853, 1.0768959563353966, 1.0769756282687384, 1.0767447785986664, 1.0770413076936318, 1.077044958160037, 1.0770990838753962, 1.0763745916692298, 1.076938773023671, 1.0767700018358153, 1.0769014472053164, 1.0766053387684187, 1.0771881052230183, 1.0777182289336507, 1.0761926129142247, 1.076439965338934, 1.0758708502075747, 1.076328617402877, 1.0760312323108292, 1.075888922649064, 1.0758938726728968, 1.0764125755855016, 1.076799121592041, 1.076658278850499, 1.0768828472284653, 1.0768182530191732, 1.0764930256090337, 1.0766497274924969, 1.0783527322199153, 1.0774439945205287, 1.0762123082854673, 1.07530129347333, 1.0762092992785726, 1.076522371060351, 1.07589481342798, 1.075559123983524, 1.0767557231467737, 1.0853168079614248, 1.0744407341398041, 1.0744790216580595, 1.0752467600191364, 1.0752534836971115, 1.0775949845368835, 1.073283593251396, 1.0755616855151549, 1.0747375229896583, 1.0747183353834355, 1.0748321454121756, 1.0748479260599673, 1.075110316863788, 1.0750282176805444, 1.075251018668239, 1.0751847349755674, 1.0758674915983955, 1.0763845649258843, 1.0759253539083822, 1.0770609233962687, 1.0771013488518977, 1.079485525834345, 1.0813461994302684, 1.0812306963947216, 1.080299501152853, 1.0860856229252807, 1.0828781830657683, 1.0778358297786494, 1.0883276572172669, 1.1203295389811199, 1.0846285602729309, 1.0753379465128206, 1.0750064125593464, 1.0750688355544518, 1.0753502242866604, 1.0749124159366625, 1.074039784558301, 1.0740076552079425, 1.0740064907152274, 1.0748947670894304, 1.075430082178664, 1.0752979516983032, 1.0753966258664436, 1.0759384773047687, 1.075661705632515, 1.0756965511538126, 1.0765943280581771, 1.0765597360278978, 1.077777960617554, 1.07895165123963, 1.0783069977423632, 1.082768026243877, 1.0815422875540597, 1.0811241734008288, 1.0830325514616441, 1.080925727908443, 1.0808462801042253, 1.0812392806380449, 1.0843199248775863, 1.0817468234862404, 1.0827879596422068, 1.0871627080225201, 1.082532359265733, 1.0866361165477334, 1.086994839810777, 1.0866961124886825, 1.0895484803345403, 1.0835250541690145, 1.0875617866641392, 1.0855707638760896, 1.0818636094408083, 1.0826099234065791, 1.0819705628800667, 1.0787028285670164, 1.0848070208857996, 1.0859080392543123, 1.0860026424937257, 1.0858959601626217, 1.1006537047513012, 1.0970292195114988, 1.0945915901797942, 1.091580579825027, 1.0836082628403587, 1.0821683492958056, 1.0821780659295068, 1.0838080384265418, 1.0830826585124476, 1.0899007843045765, 1.0869158482903918, 1.0890557969536492, 1.0920747863052318, 1.0930233624181136, 1.1093359291064133, 1.093195105421132, 1.0977081322709132, 1.097377471931658, 1.1030240538476528, 1.0906470375890998, 1.0915540791497442, 1.0914086277653234, 1.0922789336816823, 1.0916374937458382, 1.0947801732077387, 1.0982349057894427, 1.1047988662187298, 1.100591018869372, 1.0948994907448053, 1.090000915605642, 1.089825206985223, 1.0922636032495985, 1.0951511211974672, 1.0990019842908887, 1.0929599771358696, 1.092038363267244, 1.0949872060753834, 1.0973051285313071, 1.0977879623669904, 1.0985702789084273, 1.1032750984326567, 1.1150912185412127, 1.1070141432124798, 1.105004618712051, 1.1097993482705604, 1.1137672905459977, 1.1078917005379212, 1.1002309733423694, 1.0995959434994727, 1.1005548356201849, 1.1087752406428797, 1.108443943737763, 1.1099570286880769, 1.1140286865688505, 1.1166218444827352, 1.110731426130962, 1.1028598914984216, 1.1100900621445504, 1.1180180426693118, 1.1179639987757641, 1.1054890146004939, 1.1314875682195027, 1.1213575625067274], 'val_acc': [0.37274219881137605, 0.397372741662027, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39573070563510526, 0.3924466330918968, 0.3875205247175126, 0.3842364525657961, 0.38752052491325856, 0.3924466331897698, 0.39080459716284804, 0.39408866941243753, 0.3891625610380533, 0.39080459716284804, 0.3940886695103105, 0.3858784886905908, 0.38752052501113154, 0.3940886695103105, 0.3924466333855158, 0.3924466333855158, 0.38752052481538557, 0.3891625611359263, 0.38752052501113154, 0.39080459726072103, 0.3891625609401803, 0.3891625610380533, 0.3875205246196396, 0.39080459706497506, 0.3891625609401803, 0.39080459706497506, 0.39080459706497506, 0.3891625609401803, 0.3891625609401803, 0.3891625609401803, 0.38752052481538557, 0.39080459706497506, 0.3858784882990989, 0.3875205245217666, 0.3891625609401803, 0.39573070563510526, 0.39408866931456454, 0.3875205245217666, 0.38916256054868836, 0.3875205244238936, 0.3924466331897698, 0.39080459726072103, 0.3875205244238936, 0.38916256054868836, 0.39080459706497506, 0.39080459706497506, 0.39080459706497506, 0.38916256074443434, 0.40229885003641125, 0.3973727417599, 0.39573070563510526, 0.39408866921669156, 0.39408866921669156, 0.3957307054393593, 0.39737274156415403, 0.39408866931456454, 0.39408866931456454, 0.39573070563510526, 0.39408866931456454, 0.37931034409353886, 0.3875205247175126, 0.3875205247175126, 0.3891625608423073, 0.3940886695103105, 0.38587848859271784, 0.38587848859271784, 0.39573070563510526, 0.39080459706497506, 0.3891625610380533, 0.38752052481538557, 0.3842364525657961, 0.38752052481538557, 0.38587848859271784, 0.3842364524679231, 0.38587848859271784, 0.3924466332876428, 0.38587848859271784, 0.3891625608423073, 0.39080459716284804, 0.39408866921669156, 0.3891625608423073, 0.39408866941243753, 0.3924466333855158, 0.3891625608423073, 0.3875205247175126, 0.39080459716284804, 0.3891625610380533, 0.3957307055372323, 0.3891625609401803, 0.4006568140094895, 0.3891625609401803, 0.3924466332876428, 0.3924466332876428, 0.3973727417599, 0.39901477778682176, 0.39408866941243753, 0.39408866941243753, 0.3957307054393593, 0.39080459716284804, 0.4055829223838737, 0.39737274156415403, 0.3891625610380533, 0.4006568138137435, 0.40065681371587053, 0.39901477788469475, 0.3891625610380533, 0.40229885003641125, 0.3924466332876428, 0.3957307055372323, 0.3990147775910758, 0.40229885003641125, 0.3924466332876428, 0.39408866941243753, 0.40229885003641125, 0.3924466332876428, 0.38752052491325856, 0.3891625610380533, 0.3891625610380533, 0.3924466330918968, 0.39408866941243753, 0.39408866941243753, 0.3891625610380533, 0.39408866931456454, 0.3891625610380533, 0.39408866941243753, 0.397372741662027, 0.3957307055372323, 0.3924466332876428, 0.3891625610380533, 0.39408866941243753, 0.3957307055372323, 0.39080459716284804, 0.39080459716284804, 0.3957307055372323, 0.3924466331897698, 0.3891625610380533, 0.3891625610380533, 0.39408866941243753, 0.39408866931456454, 0.3957307054393593, 0.3957307055372323, 0.3957307055372323, 0.39080459716284804, 0.39408866941243753, 0.4006568141073625, 0.39573070563510526, 0.39901477778682176, 0.4088669947313362, 0.40229885013428424, 0.40229885003641125, 0.403940886063333, 0.39737274156415403, 0.37931034409353886, 0.3957307052436133, 0.397372741662027, 0.39080459716284804, 0.3957307054393593, 0.37438423552340866, 0.35960591040025597, 0.3825944165388743, 0.3924466330918968, 0.3957307052436133, 0.40065681371587053, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866921669156, 0.39408866931456454, 0.3924466331897698, 0.3924466331897698, 0.39408866931456454, 0.39408866931456454, 0.3924466331897698, 0.39080459706497506, 0.3891625609401803, 0.38587848810335296, 0.3924466331897698, 0.39080459706497506, 0.37602627135458444, 0.3908045968692291, 0.3940886690209456, 0.39244663289615084, 0.38587848898420973, 0.3727421994964869, 0.39408866921669156, 0.4006568135201246, 0.3957307052436133, 0.39737274136840806, 0.3924466329940238, 0.3957307052436133, 0.39408866911881857, 0.3957307052436133, 0.3924466329940238, 0.3924466329940238, 0.3924466329940238, 0.3924466329940238, 0.3924466329940238, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.3924466329940238, 0.39244663289615084, 0.3940886690209456, 0.38916256045081543, 0.3940886690209456, 0.3940886690209456, 0.39244663289615084, 0.3990147772974569, 0.3924466329940238, 0.39408866911881857, 0.38587848810335296, 0.3940886686294537, 0.3990147772974569, 0.3809523797289687, 0.39244663289615084, 0.39244663289615084, 0.39080459696710207, 0.39244663260253193, 0.39408866882519966, 0.3957307051457404, 0.3809523797289687, 0.3924466330918968, 0.3924466330918968, 0.3924466330918968, 0.3957307051457404, 0.39901477739532987, 0.38587848810335296, 0.3940886690209456, 0.3760262717460764, 0.3924466329940238, 0.3809523797289687, 0.39244663260253193, 0.3809523797289687, 0.3908045968692291, 0.39737274146628104, 0.39080459667348316, 0.39408866892307265, 0.39737274136840806, 0.39737274136840806, 0.37110016356743813, 0.39737274136840806, 0.3957307049499944, 0.3809523801204606, 0.39408866892307265, 0.38587848810335296, 0.3908045965756102, 0.3793103437020469, 0.3990147772974569, 0.37766830728363326, 0.4006568134222516, 0.3990147772974569, 0.3809523797289687, 0.3973727412705351, 0.38916256054868836, 0.3875205246196396, 0.3940886687273267, 0.379310343506301, 0.37766830738150625, 0.3908045963798642, 0.38587848849484485, 0.3875205243260207, 0.3940886690209456, 0.3908045962819912, 0.38916256035294244, 0.3990147774932028, 0.3990147770038379, 0.3957307052436133, 0.38587848800547997, 0.4006568135201246, 0.3924466330918968, 0.3940886686294537, 0.39408866882519966, 0.3924466327004049, 0.3990147772974569, 0.38259441585376347, 0.3924466327982779, 0.37438423513191676, 0.40065681332437864, 0.3908045968692291, 0.38916256015719647, 0.3875205246196396, 0.3875205244238936, 0.39737274117266214, 0.38095237943534976, 0.39737274097691616, 0.379310343408428, 0.38916256035294244, 0.39737274117266214, 0.3908045967713561, 0.38587848859271784, 0.3940886690209456, 0.39080459667348316, 0.38259441585376347], 'loss': [1.0857556925172434, 1.0750517419721066, 1.0747537250146728, 1.0750329833745467, 1.074431499710318, 1.0738321028695703, 1.075202389762142, 1.0750684739627878, 1.0738819725459605, 1.0742602777676906, 1.0744087052296318, 1.0743471989151878, 1.0745908004792075, 1.074380053338084, 1.0741160196445316, 1.074191230425355, 1.0740551631798245, 1.074107828179424, 1.0737885817854802, 1.0737781356247544, 1.0738099500628713, 1.073695986177887, 1.0735591271574738, 1.0735140809777826, 1.073549708105945, 1.0735043830695339, 1.0732684607868077, 1.073309232273141, 1.0732551527708707, 1.0732741792588754, 1.073241469013128, 1.0731371650460808, 1.073055677737054, 1.073027826971097, 1.0729581768752614, 1.0729607829812615, 1.0729231415343237, 1.0728897107455275, 1.0728861357886688, 1.0728977409965939, 1.0725848875496178, 1.0724659143287298, 1.072565804761538, 1.0723706898503236, 1.072341244959978, 1.0723921141340502, 1.0723882376535718, 1.0732452625121913, 1.0728103279333094, 1.0725475216303517, 1.072430512890434, 1.0725169421711007, 1.0724769372959646, 1.0720147870159737, 1.072916573910253, 1.0726187448482005, 1.0725358907202187, 1.07264137919189, 1.0724707652900742, 1.072756191200789, 1.0722769819980285, 1.0723058373531522, 1.0723267908703376, 1.0725688767873776, 1.0727660722066734, 1.072910485913866, 1.0726285391030126, 1.0729175548533885, 1.072635509982491, 1.0730257534148513, 1.0722392759283954, 1.0727149327922405, 1.0735797721012907, 1.0721051838853275, 1.0727950461346512, 1.0721809973706944, 1.0720736773841435, 1.0720082031872729, 1.0719816692555955, 1.0720383952530503, 1.0716278426701038, 1.0716011770452072, 1.0714340518387435, 1.0717402920830665, 1.0714084851423573, 1.0717219205852406, 1.0714950596282615, 1.0721409436368845, 1.071750375475482, 1.0714062116963663, 1.071552900514074, 1.0707654093814827, 1.0710884697383434, 1.071093712401341, 1.0714066835889091, 1.0712247300196966, 1.0714967069195036, 1.0710990064197987, 1.0711142087619163, 1.0706317981410565, 1.0710934541063877, 1.0706361031385419, 1.0709499670005187, 1.070799522820929, 1.0716533710334826, 1.0706219821984764, 1.071130024187374, 1.0710096282391088, 1.0707028324843921, 1.0712113129774403, 1.071278404357252, 1.0710481748443854, 1.0707590936390525, 1.070489146038737, 1.07068704236949, 1.0702587110061175, 1.07043348357418, 1.0703921559410174, 1.071856785261166, 1.0702395186042395, 1.0704802112657676, 1.0700598790660287, 1.0700531538017477, 1.0701878311697708, 1.0702008319341672, 1.0701662187948364, 1.0702530624440563, 1.069619630102749, 1.0696053661604927, 1.0694369156502601, 1.0708844276424305, 1.069184171345689, 1.0701304247736687, 1.0690251204022636, 1.0717126417453773, 1.0709457158063227, 1.0701435241366315, 1.0692705893173844, 1.070257167258057, 1.0697302855994912, 1.0695700855960102, 1.0687134975770172, 1.0692334014042693, 1.0683918294475798, 1.0685385470517608, 1.0687372115113651, 1.068671037724865, 1.068524940106903, 1.0686525467729666, 1.0683230900911336, 1.0690269468746145, 1.0678374936203692, 1.0680199310275318, 1.068140321884312, 1.0680021825512331, 1.0706307891947533, 1.0683607902370194, 1.0672104049267466, 1.0673178475985048, 1.0677021982243908, 1.0751847767976765, 1.0741579613891226, 1.0736666795409435, 1.0739671462370386, 1.0728162522678257, 1.0725150086306938, 1.0753748652381818, 1.0730789383823622, 1.0728838482431808, 1.0726197774405353, 1.0724743552276486, 1.0719786644961065, 1.0717694934143913, 1.0714005371873139, 1.07134841217887, 1.0704548863170082, 1.069820197798633, 1.069099116374335, 1.0687328883020295, 1.0672671576544979, 1.0668257338310414, 1.0661312228110782, 1.0636797398267586, 1.0638506492794906, 1.0625785757628798, 1.0645650612989734, 1.0654474321576848, 1.0713190621174336, 1.0828475604556669, 1.116182414266363, 1.0716981741437188, 1.070207544518692, 1.0706634666395873, 1.0707412307022532, 1.0697419714878718, 1.0691305518395113, 1.0688853844480104, 1.0687764199607426, 1.068257996580684, 1.0677657667371527, 1.0671220596810875, 1.0665920912607494, 1.0659631696325063, 1.065127020499055, 1.064515381133532, 1.063826591229292, 1.0628176164822902, 1.0621492302148494, 1.0613434942840796, 1.0604957662323906, 1.060558659829643, 1.0603013064582245, 1.0612471710240328, 1.0607890832105946, 1.060293384597042, 1.0576951153224499, 1.0595278394295695, 1.0571979555995556, 1.0596303890373182, 1.059952362411076, 1.0572708208702917, 1.0583595811463968, 1.0562215076579695, 1.05603646208863, 1.0536508458350964, 1.0551713722687237, 1.0536371982073147, 1.0597595910516853, 1.0667920951725767, 1.0605378318861036, 1.0549240760000336, 1.0520082528096695, 1.0524363963266172, 1.0509312919033136, 1.0515132414486863, 1.0531135407316612, 1.0529126606927515, 1.0533882822099407, 1.0571899780747338, 1.0587764233289558, 1.0472722003102548, 1.0520907585625776, 1.0518810027433863, 1.0481264550583074, 1.043053736089436, 1.0417456722357434, 1.0396783635356832, 1.0398504841743799, 1.0372092972056333, 1.0399468993748973, 1.0381796673827592, 1.0419928235929359, 1.0454518637373218, 1.0386832631344178, 1.0400085170166204, 1.0452994319202964, 1.0477646814479475, 1.043979335173934, 1.0426164049387467, 1.0406527311160578, 1.0366554533921228, 1.0350609314270334, 1.032975323097417, 1.041420738652991, 1.0480338210197935, 1.0397459432574514, 1.0407354617265705, 1.0389258600113573, 1.0381409770409429, 1.0374104492962972, 1.034870369674244, 1.0345584819938614, 1.0324303494341809, 1.032603128392104, 1.030264364914238, 1.0310884975554762, 1.0302204393018688, 1.0268552755673073, 1.0238257106569513, 1.03524157628876, 1.0283127885090007, 1.0259422141178922, 1.0348941435314547, 1.0310554098054858, 1.0289656423690139, 1.0311084492740201, 1.0248277561865304, 1.0253314094132222, 1.0334733593635246, 1.0250676229993911, 1.027017271739012, 1.0288191006168936, 1.0213195399337236, 1.0274707379527162, 1.0238681112226764, 1.0176723761724986, 1.0296661763710162, 1.019560361814205, 1.0201641408814541, 1.0240148447867048], 'acc': [0.37289527917789483, 0.37207391995179334, 0.3913757686252711, 0.39425051211086876, 0.39425051250252147, 0.3942505123434126, 0.39219712621132696, 0.40164271162275905, 0.3979466122401079, 0.3995893201666446, 0.39589322379482356, 0.39342915769475195, 0.3934291602404945, 0.3954825475475382, 0.3963039005928705, 0.3975359338387327, 0.3963039023920251, 0.3967145772318086, 0.3995893207541237, 0.39753593266377457, 0.39630390337115684, 0.3963039005928705, 0.39671457781928765, 0.4000000001346306, 0.4020533872825654, 0.3979466118484552, 0.40041067876854963, 0.4012320306389239, 0.40492812978903125, 0.4016427118185854, 0.40000000134630614, 0.4012320316180556, 0.4012320326339048, 0.39999999895967253, 0.40041067579443695, 0.4004106755986106, 0.40082135564003146, 0.40246406744637775, 0.4028747416986822, 0.403696099284738, 0.4032854211159066, 0.4012320306389239, 0.4028747446727948, 0.4012320310305766, 0.4012320320097083, 0.4032854232699964, 0.40287474349783675, 0.4012320316180556, 0.3967145784434841, 0.4012320324380785, 0.39753593344708, 0.40123203259718737, 0.40246406666307233, 0.4012320306389239, 0.3934291588697101, 0.394250513714197, 0.4024640660755933, 0.39712525606155397, 0.3979466134517834, 0.4020533862667162, 0.39876796590963676, 0.401232034983821, 0.40246406744637775, 0.39958932372823636, 0.40000000017134807, 0.3987679663012894, 0.40123203122640294, 0.3987679688837494, 0.3987679684920967, 0.4024640648639178, 0.39999999837219347, 0.4008213563866194, 0.39589322575308705, 0.3975359324679482, 0.4016427102519746, 0.40246406803385676, 0.39548254496507823, 0.3987679677087913, 0.40082135674155467, 0.40164271021525716, 0.3979466098901917, 0.4036961004964135, 0.3987679684920967, 0.3975359362253663, 0.40000000033045696, 0.40287474388948946, 0.4020533862667162, 0.4016427100194308, 0.39507186715118203, 0.4061601624474144, 0.39917864591434016, 0.40451745373757225, 0.40369609948056434, 0.39794661208099896, 0.3991786461101665, 0.40492813076816303, 0.40533881014866996, 0.4020533884575235, 0.39835729106985324, 0.4061601644423953, 0.3983572878999142, 0.40369610088806623, 0.3995893213416027, 0.4073921982757365, 0.3995893207541237, 0.40328541912092564, 0.39876796947122845, 0.4016427098236045, 0.3991786425485748, 0.40328542150755925, 0.3975359350504082, 0.402053388420806, 0.40123203478799463, 0.4016427114269327, 0.4012320345921683, 0.4057494889784153, 0.4045174555000094, 0.40862422776418056, 0.40205338607088986, 0.4065708398696578, 0.40205338724584794, 0.4008213559949667, 0.402464063884786, 0.40205338747839175, 0.4078028733113463, 0.40985626457163443, 0.40246406509646154, 0.4020533882616971, 0.4082135535118761, 0.4053388086187766, 0.4065708442145549, 0.40246406548811425, 0.4024640668588987, 0.40451745177930876, 0.4045174543617687, 0.4008213532166804, 0.4036960973264745, 0.40123203420051556, 0.40246406505974414, 0.40082135419581216, 0.40410677831030967, 0.4016427114269327, 0.40205339041578697, 0.40451745177930876, 0.4036960967757129, 0.40246406509646154, 0.4024640678380304, 0.40287474209033486, 0.4020533892408289, 0.40041067681028614, 0.39999999935132524, 0.40328542189921196, 0.4012320322422521, 0.4008213551749439, 0.4020533888491762, 0.3979466110651498, 0.4036960965431691, 0.40410677811448337, 0.4036961001047608, 0.4016427098236045, 0.39876796653383323, 0.39507187032112107, 0.4008213554074877, 0.39876796610546306, 0.40246406744637775, 0.4000000005630007, 0.39794661364760975, 0.40041067896437593, 0.40041067696939503, 0.40328542287834374, 0.4008213544283559, 0.40041067579443695, 0.4020533872825654, 0.40533881116451914, 0.40451745557344426, 0.4061601644423953, 0.4065708422562914, 0.40657084186463877, 0.40780287350717265, 0.4069815212451457, 0.406160164834048, 0.4078028733113463, 0.38316221681463647, 0.4094455839794519, 0.40821355429518147, 0.403285419316752, 0.40862423073829324, 0.39260780363357045, 0.3950718687545103, 0.3663244376804305, 0.3987679653221577, 0.40410677811448337, 0.4012320326339048, 0.40369609634734277, 0.4041067763520462, 0.40657084147298606, 0.40533880721127474, 0.4057494850251709, 0.4065708404571369, 0.40821355132106885, 0.40862422815583327, 0.408624229134965, 0.4082135517127215, 0.40780287452302183, 0.40944558476275733, 0.40903490694886113, 0.40903490577390306, 0.4098562649632871, 0.41026694199387786, 0.4127310047281841, 0.41355236058852024, 0.3991786429402275, 0.4016427124427819, 0.41930184978724017, 0.3901437387084569, 0.4164271034866388, 0.41273100414070507, 0.4008213563866194, 0.4078028751105009, 0.4028747413070295, 0.4176591363408482, 0.39466119191974586, 0.39753593524623454, 0.40862422893913863, 0.41437371676707413, 0.4123203263268089, 0.41190965027534987, 0.400821355015835, 0.4065708404571369, 0.3991786465018192, 0.4184804945143831, 0.4205338830331023, 0.42094455928038765, 0.42094455767705946, 0.4020533870500216, 0.4229979475665631, 0.4078028752696098, 0.42012320185344076, 0.3991786427444012, 0.40780287632217643, 0.4201232036158779, 0.42299794737073676, 0.3926078030460914, 0.4287474355536075, 0.44188911717775176, 0.42176591310902545, 0.43408624165111986, 0.42997946723285885, 0.43572895173174647, 0.42628336811946893, 0.43080082289736865, 0.40533880779875375, 0.42094455630627503, 0.43244353356547427, 0.4168377801255769, 0.4308008203149087, 0.42053388299638483, 0.3963039005928705, 0.41848049357196876, 0.4242299815957306, 0.41560574985382737, 0.427926078163378, 0.4373716622040257, 0.42340862655541733, 0.4156057482504992, 0.4147843937976649, 0.4279260760092882, 0.43039014508347245, 0.4406570835402369, 0.42874743320369135, 0.42012320381170426, 0.43244353356547427, 0.4312115002828947, 0.44106776174578577, 0.42628336753198987, 0.42710472312306474, 0.4320328561432308, 0.4365503079470178, 0.4406570858901531, 0.4176591369283273, 0.42094455728540675, 0.43655030677205975, 0.42258726952012315, 0.44147843955968197, 0.44147844073464004, 0.4332648850441958, 0.4476386036349028, 0.44517453577239413, 0.4406570819369087, 0.4464065713681724, 0.44147843838472384, 0.43080082305647754, 0.4533880914628383, 0.4353182766961366, 0.4242299796007497, 0.442710471630586, 0.4320328561432308, 0.44517453714317856, 0.43737166357481017, 0.4525667370100041]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
