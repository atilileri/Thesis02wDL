{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf4.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 00:03:46 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '1', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['yd', 'ce', 'eb', 'ek', 'eo', 'mb', 'ck', 'by', 'sg', 'aa', 'sk', 'ds', 'ib', 'eg', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F762919F98>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F75C116EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7173, Accuracy:0.0821, Validation Loss:2.7086, Validation Accuracy:0.0821\n",
    "Epoch #2: Loss:2.7058, Accuracy:0.0752, Validation Loss:2.6995, Validation Accuracy:0.0788\n",
    "Epoch #3: Loss:2.6969, Accuracy:0.0834, Validation Loss:2.6923, Validation Accuracy:0.0821\n",
    "Epoch #4: Loss:2.6907, Accuracy:0.0813, Validation Loss:2.6869, Validation Accuracy:0.0821\n",
    "Epoch #5: Loss:2.6858, Accuracy:0.0813, Validation Loss:2.6827, Validation Accuracy:0.0821\n",
    "Epoch #6: Loss:2.6817, Accuracy:0.0813, Validation Loss:2.6786, Validation Accuracy:0.0821\n",
    "Epoch #7: Loss:2.6779, Accuracy:0.0813, Validation Loss:2.6750, Validation Accuracy:0.0821\n",
    "Epoch #8: Loss:2.6737, Accuracy:0.0912, Validation Loss:2.6717, Validation Accuracy:0.1067\n",
    "Epoch #9: Loss:2.6696, Accuracy:0.1150, Validation Loss:2.6681, Validation Accuracy:0.1199\n",
    "Epoch #10: Loss:2.6645, Accuracy:0.1232, Validation Loss:2.6639, Validation Accuracy:0.1199\n",
    "Epoch #11: Loss:2.6592, Accuracy:0.1441, Validation Loss:2.6598, Validation Accuracy:0.1412\n",
    "Epoch #12: Loss:2.6523, Accuracy:0.1626, Validation Loss:2.6541, Validation Accuracy:0.1461\n",
    "Epoch #13: Loss:2.6464, Accuracy:0.1511, Validation Loss:2.6495, Validation Accuracy:0.1314\n",
    "Epoch #14: Loss:2.6388, Accuracy:0.1458, Validation Loss:2.6396, Validation Accuracy:0.1396\n",
    "Epoch #15: Loss:2.6272, Accuracy:0.1478, Validation Loss:2.6290, Validation Accuracy:0.1314\n",
    "Epoch #16: Loss:2.6153, Accuracy:0.1454, Validation Loss:2.6208, Validation Accuracy:0.1314\n",
    "Epoch #17: Loss:2.6079, Accuracy:0.1478, Validation Loss:2.6104, Validation Accuracy:0.1478\n",
    "Epoch #18: Loss:2.5932, Accuracy:0.1598, Validation Loss:2.6030, Validation Accuracy:0.1560\n",
    "Epoch #19: Loss:2.5784, Accuracy:0.1585, Validation Loss:2.5857, Validation Accuracy:0.1412\n",
    "Epoch #20: Loss:2.5645, Accuracy:0.1561, Validation Loss:2.5737, Validation Accuracy:0.1199\n",
    "Epoch #21: Loss:2.5561, Accuracy:0.1536, Validation Loss:2.5719, Validation Accuracy:0.1396\n",
    "Epoch #22: Loss:2.5394, Accuracy:0.1507, Validation Loss:2.5698, Validation Accuracy:0.1346\n",
    "Epoch #23: Loss:2.5360, Accuracy:0.1487, Validation Loss:2.5568, Validation Accuracy:0.1199\n",
    "Epoch #24: Loss:2.5291, Accuracy:0.1507, Validation Loss:2.5375, Validation Accuracy:0.1445\n",
    "Epoch #25: Loss:2.5179, Accuracy:0.1589, Validation Loss:2.5320, Validation Accuracy:0.1544\n",
    "Epoch #26: Loss:2.5112, Accuracy:0.1634, Validation Loss:2.5322, Validation Accuracy:0.1527\n",
    "Epoch #27: Loss:2.5011, Accuracy:0.1598, Validation Loss:2.5215, Validation Accuracy:0.1593\n",
    "Epoch #28: Loss:2.4940, Accuracy:0.1643, Validation Loss:2.5220, Validation Accuracy:0.1576\n",
    "Epoch #29: Loss:2.4945, Accuracy:0.1602, Validation Loss:2.5150, Validation Accuracy:0.1609\n",
    "Epoch #30: Loss:2.4844, Accuracy:0.1573, Validation Loss:2.5119, Validation Accuracy:0.1544\n",
    "Epoch #31: Loss:2.4825, Accuracy:0.1610, Validation Loss:2.5114, Validation Accuracy:0.1576\n",
    "Epoch #32: Loss:2.4770, Accuracy:0.1634, Validation Loss:2.5103, Validation Accuracy:0.1609\n",
    "Epoch #33: Loss:2.4741, Accuracy:0.1643, Validation Loss:2.5103, Validation Accuracy:0.1626\n",
    "Epoch #34: Loss:2.4703, Accuracy:0.1680, Validation Loss:2.5073, Validation Accuracy:0.1593\n",
    "Epoch #35: Loss:2.4694, Accuracy:0.1671, Validation Loss:2.5082, Validation Accuracy:0.1609\n",
    "Epoch #36: Loss:2.4661, Accuracy:0.1749, Validation Loss:2.5089, Validation Accuracy:0.1658\n",
    "Epoch #37: Loss:2.4648, Accuracy:0.1708, Validation Loss:2.5069, Validation Accuracy:0.1478\n",
    "Epoch #38: Loss:2.4727, Accuracy:0.1610, Validation Loss:2.5237, Validation Accuracy:0.1560\n",
    "Epoch #39: Loss:2.4765, Accuracy:0.1680, Validation Loss:2.5105, Validation Accuracy:0.1461\n",
    "Epoch #40: Loss:2.4644, Accuracy:0.1655, Validation Loss:2.4986, Validation Accuracy:0.1642\n",
    "Epoch #41: Loss:2.4584, Accuracy:0.1741, Validation Loss:2.5024, Validation Accuracy:0.1560\n",
    "Epoch #42: Loss:2.4570, Accuracy:0.1663, Validation Loss:2.4991, Validation Accuracy:0.1576\n",
    "Epoch #43: Loss:2.4534, Accuracy:0.1762, Validation Loss:2.5008, Validation Accuracy:0.1642\n",
    "Epoch #44: Loss:2.4519, Accuracy:0.1791, Validation Loss:2.5003, Validation Accuracy:0.1626\n",
    "Epoch #45: Loss:2.4507, Accuracy:0.1799, Validation Loss:2.5017, Validation Accuracy:0.1675\n",
    "Epoch #46: Loss:2.4492, Accuracy:0.1774, Validation Loss:2.5003, Validation Accuracy:0.1658\n",
    "Epoch #47: Loss:2.4475, Accuracy:0.1791, Validation Loss:2.5011, Validation Accuracy:0.1642\n",
    "Epoch #48: Loss:2.4456, Accuracy:0.1778, Validation Loss:2.5004, Validation Accuracy:0.1609\n",
    "Epoch #49: Loss:2.4458, Accuracy:0.1778, Validation Loss:2.4984, Validation Accuracy:0.1658\n",
    "Epoch #50: Loss:2.4441, Accuracy:0.1791, Validation Loss:2.4996, Validation Accuracy:0.1626\n",
    "Epoch #51: Loss:2.4422, Accuracy:0.1786, Validation Loss:2.4954, Validation Accuracy:0.1658\n",
    "Epoch #52: Loss:2.4430, Accuracy:0.1778, Validation Loss:2.4977, Validation Accuracy:0.1626\n",
    "Epoch #53: Loss:2.4412, Accuracy:0.1762, Validation Loss:2.5012, Validation Accuracy:0.1609\n",
    "Epoch #54: Loss:2.4410, Accuracy:0.1774, Validation Loss:2.4971, Validation Accuracy:0.1609\n",
    "Epoch #55: Loss:2.4416, Accuracy:0.1832, Validation Loss:2.4982, Validation Accuracy:0.1593\n",
    "Epoch #56: Loss:2.4390, Accuracy:0.1786, Validation Loss:2.4983, Validation Accuracy:0.1609\n",
    "Epoch #57: Loss:2.4380, Accuracy:0.1807, Validation Loss:2.5003, Validation Accuracy:0.1642\n",
    "Epoch #58: Loss:2.4370, Accuracy:0.1807, Validation Loss:2.4977, Validation Accuracy:0.1609\n",
    "Epoch #59: Loss:2.4368, Accuracy:0.1811, Validation Loss:2.5034, Validation Accuracy:0.1675\n",
    "Epoch #60: Loss:2.4374, Accuracy:0.1823, Validation Loss:2.4994, Validation Accuracy:0.1642\n",
    "Epoch #61: Loss:2.4368, Accuracy:0.1815, Validation Loss:2.4977, Validation Accuracy:0.1626\n",
    "Epoch #62: Loss:2.4351, Accuracy:0.1811, Validation Loss:2.4961, Validation Accuracy:0.1626\n",
    "Epoch #63: Loss:2.4331, Accuracy:0.1860, Validation Loss:2.4985, Validation Accuracy:0.1642\n",
    "Epoch #64: Loss:2.4326, Accuracy:0.1836, Validation Loss:2.4984, Validation Accuracy:0.1626\n",
    "Epoch #65: Loss:2.4337, Accuracy:0.1778, Validation Loss:2.4997, Validation Accuracy:0.1593\n",
    "Epoch #66: Loss:2.4317, Accuracy:0.1840, Validation Loss:2.5043, Validation Accuracy:0.1642\n",
    "Epoch #67: Loss:2.4333, Accuracy:0.1832, Validation Loss:2.4992, Validation Accuracy:0.1576\n",
    "Epoch #68: Loss:2.4325, Accuracy:0.1754, Validation Loss:2.5026, Validation Accuracy:0.1675\n",
    "Epoch #69: Loss:2.4322, Accuracy:0.1807, Validation Loss:2.4996, Validation Accuracy:0.1708\n",
    "Epoch #70: Loss:2.4307, Accuracy:0.1819, Validation Loss:2.5014, Validation Accuracy:0.1708\n",
    "Epoch #71: Loss:2.4294, Accuracy:0.1819, Validation Loss:2.5022, Validation Accuracy:0.1675\n",
    "Epoch #72: Loss:2.4297, Accuracy:0.1815, Validation Loss:2.4988, Validation Accuracy:0.1609\n",
    "Epoch #73: Loss:2.4280, Accuracy:0.1791, Validation Loss:2.4968, Validation Accuracy:0.1642\n",
    "Epoch #74: Loss:2.4277, Accuracy:0.1799, Validation Loss:2.4994, Validation Accuracy:0.1675\n",
    "Epoch #75: Loss:2.4270, Accuracy:0.1782, Validation Loss:2.5037, Validation Accuracy:0.1691\n",
    "Epoch #76: Loss:2.4271, Accuracy:0.1819, Validation Loss:2.5015, Validation Accuracy:0.1675\n",
    "Epoch #77: Loss:2.4267, Accuracy:0.1864, Validation Loss:2.5009, Validation Accuracy:0.1675\n",
    "Epoch #78: Loss:2.4258, Accuracy:0.1852, Validation Loss:2.4967, Validation Accuracy:0.1708\n",
    "Epoch #79: Loss:2.4253, Accuracy:0.1856, Validation Loss:2.4991, Validation Accuracy:0.1675\n",
    "Epoch #80: Loss:2.4261, Accuracy:0.1844, Validation Loss:2.4970, Validation Accuracy:0.1642\n",
    "Epoch #81: Loss:2.4246, Accuracy:0.1840, Validation Loss:2.4969, Validation Accuracy:0.1675\n",
    "Epoch #82: Loss:2.4233, Accuracy:0.1873, Validation Loss:2.4967, Validation Accuracy:0.1642\n",
    "Epoch #83: Loss:2.4238, Accuracy:0.1840, Validation Loss:2.4980, Validation Accuracy:0.1658\n",
    "Epoch #84: Loss:2.4217, Accuracy:0.1852, Validation Loss:2.5010, Validation Accuracy:0.1691\n",
    "Epoch #85: Loss:2.4194, Accuracy:0.1840, Validation Loss:2.4983, Validation Accuracy:0.1691\n",
    "Epoch #86: Loss:2.4189, Accuracy:0.1836, Validation Loss:2.4974, Validation Accuracy:0.1658\n",
    "Epoch #87: Loss:2.4174, Accuracy:0.1791, Validation Loss:2.4994, Validation Accuracy:0.1708\n",
    "Epoch #88: Loss:2.4179, Accuracy:0.1815, Validation Loss:2.4975, Validation Accuracy:0.1675\n",
    "Epoch #89: Loss:2.4178, Accuracy:0.1877, Validation Loss:2.4993, Validation Accuracy:0.1708\n",
    "Epoch #90: Loss:2.4188, Accuracy:0.1852, Validation Loss:2.5012, Validation Accuracy:0.1708\n",
    "Epoch #91: Loss:2.4175, Accuracy:0.1873, Validation Loss:2.4985, Validation Accuracy:0.1708\n",
    "Epoch #92: Loss:2.4170, Accuracy:0.1848, Validation Loss:2.4943, Validation Accuracy:0.1724\n",
    "Epoch #93: Loss:2.4189, Accuracy:0.1881, Validation Loss:2.4940, Validation Accuracy:0.1708\n",
    "Epoch #94: Loss:2.4181, Accuracy:0.1840, Validation Loss:2.4996, Validation Accuracy:0.1691\n",
    "Epoch #95: Loss:2.4166, Accuracy:0.1897, Validation Loss:2.4978, Validation Accuracy:0.1691\n",
    "Epoch #96: Loss:2.4165, Accuracy:0.1918, Validation Loss:2.4971, Validation Accuracy:0.1691\n",
    "Epoch #97: Loss:2.4165, Accuracy:0.1922, Validation Loss:2.4950, Validation Accuracy:0.1675\n",
    "Epoch #98: Loss:2.4157, Accuracy:0.1914, Validation Loss:2.4957, Validation Accuracy:0.1675\n",
    "Epoch #99: Loss:2.4145, Accuracy:0.1893, Validation Loss:2.4970, Validation Accuracy:0.1658\n",
    "Epoch #100: Loss:2.4167, Accuracy:0.1901, Validation Loss:2.4951, Validation Accuracy:0.1609\n",
    "Epoch #101: Loss:2.4172, Accuracy:0.1869, Validation Loss:2.4970, Validation Accuracy:0.1675\n",
    "Epoch #102: Loss:2.4183, Accuracy:0.1885, Validation Loss:2.5000, Validation Accuracy:0.1642\n",
    "Epoch #103: Loss:2.4185, Accuracy:0.1889, Validation Loss:2.5024, Validation Accuracy:0.1675\n",
    "Epoch #104: Loss:2.4180, Accuracy:0.1881, Validation Loss:2.5055, Validation Accuracy:0.1675\n",
    "Epoch #105: Loss:2.4195, Accuracy:0.1901, Validation Loss:2.5053, Validation Accuracy:0.1658\n",
    "Epoch #106: Loss:2.4209, Accuracy:0.1869, Validation Loss:2.5060, Validation Accuracy:0.1658\n",
    "Epoch #107: Loss:2.4199, Accuracy:0.1881, Validation Loss:2.5071, Validation Accuracy:0.1658\n",
    "Epoch #108: Loss:2.4206, Accuracy:0.1848, Validation Loss:2.5027, Validation Accuracy:0.1626\n",
    "Epoch #109: Loss:2.4171, Accuracy:0.1848, Validation Loss:2.5049, Validation Accuracy:0.1642\n",
    "Epoch #110: Loss:2.4174, Accuracy:0.1885, Validation Loss:2.4998, Validation Accuracy:0.1593\n",
    "Epoch #111: Loss:2.4169, Accuracy:0.1889, Validation Loss:2.5018, Validation Accuracy:0.1626\n",
    "Epoch #112: Loss:2.4157, Accuracy:0.1893, Validation Loss:2.4971, Validation Accuracy:0.1642\n",
    "Epoch #113: Loss:2.4164, Accuracy:0.1893, Validation Loss:2.4966, Validation Accuracy:0.1626\n",
    "Epoch #114: Loss:2.4158, Accuracy:0.1893, Validation Loss:2.5002, Validation Accuracy:0.1675\n",
    "Epoch #115: Loss:2.4138, Accuracy:0.1877, Validation Loss:2.5044, Validation Accuracy:0.1658\n",
    "Epoch #116: Loss:2.4141, Accuracy:0.1873, Validation Loss:2.5053, Validation Accuracy:0.1658\n",
    "Epoch #117: Loss:2.4141, Accuracy:0.1864, Validation Loss:2.5052, Validation Accuracy:0.1626\n",
    "Epoch #118: Loss:2.4129, Accuracy:0.1889, Validation Loss:2.5055, Validation Accuracy:0.1658\n",
    "Epoch #119: Loss:2.4133, Accuracy:0.1881, Validation Loss:2.5046, Validation Accuracy:0.1708\n",
    "Epoch #120: Loss:2.4123, Accuracy:0.1893, Validation Loss:2.5036, Validation Accuracy:0.1675\n",
    "Epoch #121: Loss:2.4120, Accuracy:0.1897, Validation Loss:2.5031, Validation Accuracy:0.1708\n",
    "Epoch #122: Loss:2.4116, Accuracy:0.1893, Validation Loss:2.5027, Validation Accuracy:0.1724\n",
    "Epoch #123: Loss:2.4104, Accuracy:0.1918, Validation Loss:2.5064, Validation Accuracy:0.1691\n",
    "Epoch #124: Loss:2.4118, Accuracy:0.1885, Validation Loss:2.5054, Validation Accuracy:0.1675\n",
    "Epoch #125: Loss:2.4104, Accuracy:0.1885, Validation Loss:2.5067, Validation Accuracy:0.1708\n",
    "Epoch #126: Loss:2.4092, Accuracy:0.1901, Validation Loss:2.5119, Validation Accuracy:0.1691\n",
    "Epoch #127: Loss:2.4102, Accuracy:0.1889, Validation Loss:2.5066, Validation Accuracy:0.1642\n",
    "Epoch #128: Loss:2.4073, Accuracy:0.1901, Validation Loss:2.5060, Validation Accuracy:0.1773\n",
    "Epoch #129: Loss:2.4080, Accuracy:0.1889, Validation Loss:2.5098, Validation Accuracy:0.1691\n",
    "Epoch #130: Loss:2.4068, Accuracy:0.1893, Validation Loss:2.5076, Validation Accuracy:0.1691\n",
    "Epoch #131: Loss:2.4081, Accuracy:0.1906, Validation Loss:2.5093, Validation Accuracy:0.1790\n",
    "Epoch #132: Loss:2.4064, Accuracy:0.1889, Validation Loss:2.5067, Validation Accuracy:0.1724\n",
    "Epoch #133: Loss:2.4082, Accuracy:0.1864, Validation Loss:2.5081, Validation Accuracy:0.1691\n",
    "Epoch #134: Loss:2.4082, Accuracy:0.1922, Validation Loss:2.5072, Validation Accuracy:0.1708\n",
    "Epoch #135: Loss:2.4068, Accuracy:0.1914, Validation Loss:2.5019, Validation Accuracy:0.1708\n",
    "Epoch #136: Loss:2.4055, Accuracy:0.1893, Validation Loss:2.5023, Validation Accuracy:0.1790\n",
    "Epoch #137: Loss:2.4055, Accuracy:0.1864, Validation Loss:2.5068, Validation Accuracy:0.1757\n",
    "Epoch #138: Loss:2.4053, Accuracy:0.1864, Validation Loss:2.5036, Validation Accuracy:0.1757\n",
    "Epoch #139: Loss:2.4075, Accuracy:0.1873, Validation Loss:2.5077, Validation Accuracy:0.1757\n",
    "Epoch #140: Loss:2.4066, Accuracy:0.1906, Validation Loss:2.5104, Validation Accuracy:0.1741\n",
    "Epoch #141: Loss:2.4058, Accuracy:0.1901, Validation Loss:2.5105, Validation Accuracy:0.1724\n",
    "Epoch #142: Loss:2.4082, Accuracy:0.1885, Validation Loss:2.5103, Validation Accuracy:0.1708\n",
    "Epoch #143: Loss:2.4034, Accuracy:0.1922, Validation Loss:2.5084, Validation Accuracy:0.1724\n",
    "Epoch #144: Loss:2.4045, Accuracy:0.1877, Validation Loss:2.5052, Validation Accuracy:0.1724\n",
    "Epoch #145: Loss:2.4050, Accuracy:0.1877, Validation Loss:2.5053, Validation Accuracy:0.1626\n",
    "Epoch #146: Loss:2.4042, Accuracy:0.1918, Validation Loss:2.5069, Validation Accuracy:0.1658\n",
    "Epoch #147: Loss:2.4046, Accuracy:0.1959, Validation Loss:2.5076, Validation Accuracy:0.1626\n",
    "Epoch #148: Loss:2.4039, Accuracy:0.1922, Validation Loss:2.5058, Validation Accuracy:0.1708\n",
    "Epoch #149: Loss:2.4035, Accuracy:0.1889, Validation Loss:2.5049, Validation Accuracy:0.1691\n",
    "Epoch #150: Loss:2.4035, Accuracy:0.1943, Validation Loss:2.5079, Validation Accuracy:0.1691\n",
    "Epoch #151: Loss:2.4019, Accuracy:0.1988, Validation Loss:2.5088, Validation Accuracy:0.1675\n",
    "Epoch #152: Loss:2.4025, Accuracy:0.1947, Validation Loss:2.5068, Validation Accuracy:0.1724\n",
    "Epoch #153: Loss:2.4028, Accuracy:0.1996, Validation Loss:2.5069, Validation Accuracy:0.1691\n",
    "Epoch #154: Loss:2.4003, Accuracy:0.1963, Validation Loss:2.5083, Validation Accuracy:0.1839\n",
    "Epoch #155: Loss:2.4001, Accuracy:0.1947, Validation Loss:2.5054, Validation Accuracy:0.1823\n",
    "Epoch #156: Loss:2.4034, Accuracy:0.1971, Validation Loss:2.5007, Validation Accuracy:0.1757\n",
    "Epoch #157: Loss:2.4001, Accuracy:0.1959, Validation Loss:2.5005, Validation Accuracy:0.1806\n",
    "Epoch #158: Loss:2.3996, Accuracy:0.1930, Validation Loss:2.5060, Validation Accuracy:0.1724\n",
    "Epoch #159: Loss:2.3975, Accuracy:0.1947, Validation Loss:2.5047, Validation Accuracy:0.1773\n",
    "Epoch #160: Loss:2.3973, Accuracy:0.1996, Validation Loss:2.5079, Validation Accuracy:0.1675\n",
    "Epoch #161: Loss:2.3974, Accuracy:0.1988, Validation Loss:2.5060, Validation Accuracy:0.1675\n",
    "Epoch #162: Loss:2.3964, Accuracy:0.2008, Validation Loss:2.5036, Validation Accuracy:0.1691\n",
    "Epoch #163: Loss:2.3957, Accuracy:0.2016, Validation Loss:2.5077, Validation Accuracy:0.1626\n",
    "Epoch #164: Loss:2.3956, Accuracy:0.2004, Validation Loss:2.5059, Validation Accuracy:0.1741\n",
    "Epoch #165: Loss:2.3950, Accuracy:0.1992, Validation Loss:2.5051, Validation Accuracy:0.1708\n",
    "Epoch #166: Loss:2.3948, Accuracy:0.1988, Validation Loss:2.5049, Validation Accuracy:0.1708\n",
    "Epoch #167: Loss:2.3929, Accuracy:0.1971, Validation Loss:2.5046, Validation Accuracy:0.1675\n",
    "Epoch #168: Loss:2.3933, Accuracy:0.2004, Validation Loss:2.5069, Validation Accuracy:0.1708\n",
    "Epoch #169: Loss:2.3933, Accuracy:0.1955, Validation Loss:2.5074, Validation Accuracy:0.1708\n",
    "Epoch #170: Loss:2.3943, Accuracy:0.1930, Validation Loss:2.5066, Validation Accuracy:0.1658\n",
    "Epoch #171: Loss:2.3959, Accuracy:0.2004, Validation Loss:2.5027, Validation Accuracy:0.1675\n",
    "Epoch #172: Loss:2.3951, Accuracy:0.1938, Validation Loss:2.5039, Validation Accuracy:0.1658\n",
    "Epoch #173: Loss:2.3950, Accuracy:0.1951, Validation Loss:2.5028, Validation Accuracy:0.1724\n",
    "Epoch #174: Loss:2.3973, Accuracy:0.1947, Validation Loss:2.5072, Validation Accuracy:0.1708\n",
    "Epoch #175: Loss:2.3968, Accuracy:0.1967, Validation Loss:2.5153, Validation Accuracy:0.1691\n",
    "Epoch #176: Loss:2.3995, Accuracy:0.1943, Validation Loss:2.5095, Validation Accuracy:0.1626\n",
    "Epoch #177: Loss:2.3987, Accuracy:0.2016, Validation Loss:2.5111, Validation Accuracy:0.1626\n",
    "Epoch #178: Loss:2.4028, Accuracy:0.1938, Validation Loss:2.5106, Validation Accuracy:0.1642\n",
    "Epoch #179: Loss:2.4042, Accuracy:0.1959, Validation Loss:2.5133, Validation Accuracy:0.1576\n",
    "Epoch #180: Loss:2.3998, Accuracy:0.1901, Validation Loss:2.5077, Validation Accuracy:0.1757\n",
    "Epoch #181: Loss:2.3980, Accuracy:0.1889, Validation Loss:2.5104, Validation Accuracy:0.1724\n",
    "Epoch #182: Loss:2.3920, Accuracy:0.1951, Validation Loss:2.5134, Validation Accuracy:0.1626\n",
    "Epoch #183: Loss:2.3925, Accuracy:0.1984, Validation Loss:2.5184, Validation Accuracy:0.1675\n",
    "Epoch #184: Loss:2.3952, Accuracy:0.1992, Validation Loss:2.5212, Validation Accuracy:0.1626\n",
    "Epoch #185: Loss:2.3931, Accuracy:0.2000, Validation Loss:2.5210, Validation Accuracy:0.1691\n",
    "Epoch #186: Loss:2.3960, Accuracy:0.2004, Validation Loss:2.5149, Validation Accuracy:0.1691\n",
    "Epoch #187: Loss:2.3952, Accuracy:0.1979, Validation Loss:2.5114, Validation Accuracy:0.1724\n",
    "Epoch #188: Loss:2.3925, Accuracy:0.1943, Validation Loss:2.5115, Validation Accuracy:0.1724\n",
    "Epoch #189: Loss:2.3926, Accuracy:0.1959, Validation Loss:2.5110, Validation Accuracy:0.1691\n",
    "Epoch #190: Loss:2.3882, Accuracy:0.1947, Validation Loss:2.5084, Validation Accuracy:0.1757\n",
    "Epoch #191: Loss:2.3888, Accuracy:0.1947, Validation Loss:2.5077, Validation Accuracy:0.1691\n",
    "Epoch #192: Loss:2.3873, Accuracy:0.2012, Validation Loss:2.5088, Validation Accuracy:0.1691\n",
    "Epoch #193: Loss:2.3894, Accuracy:0.1951, Validation Loss:2.5105, Validation Accuracy:0.1773\n",
    "Epoch #194: Loss:2.3901, Accuracy:0.2004, Validation Loss:2.5115, Validation Accuracy:0.1708\n",
    "Epoch #195: Loss:2.3913, Accuracy:0.2004, Validation Loss:2.5112, Validation Accuracy:0.1741\n",
    "Epoch #196: Loss:2.3890, Accuracy:0.1979, Validation Loss:2.5071, Validation Accuracy:0.1691\n",
    "Epoch #197: Loss:2.3884, Accuracy:0.1992, Validation Loss:2.5095, Validation Accuracy:0.1691\n",
    "Epoch #198: Loss:2.3851, Accuracy:0.1992, Validation Loss:2.5101, Validation Accuracy:0.1708\n",
    "Epoch #199: Loss:2.3868, Accuracy:0.1992, Validation Loss:2.5114, Validation Accuracy:0.1691\n",
    "Epoch #200: Loss:2.3874, Accuracy:0.1955, Validation Loss:2.5114, Validation Accuracy:0.1741\n",
    "Epoch #201: Loss:2.3876, Accuracy:0.1901, Validation Loss:2.5112, Validation Accuracy:0.1658\n",
    "Epoch #202: Loss:2.3863, Accuracy:0.2004, Validation Loss:2.5153, Validation Accuracy:0.1626\n",
    "Epoch #203: Loss:2.3849, Accuracy:0.2004, Validation Loss:2.5167, Validation Accuracy:0.1675\n",
    "Epoch #204: Loss:2.3855, Accuracy:0.1943, Validation Loss:2.5131, Validation Accuracy:0.1626\n",
    "Epoch #205: Loss:2.3878, Accuracy:0.1967, Validation Loss:2.5178, Validation Accuracy:0.1724\n",
    "Epoch #206: Loss:2.3892, Accuracy:0.1971, Validation Loss:2.5166, Validation Accuracy:0.1609\n",
    "Epoch #207: Loss:2.3994, Accuracy:0.2004, Validation Loss:2.5202, Validation Accuracy:0.1691\n",
    "Epoch #208: Loss:2.3987, Accuracy:0.1951, Validation Loss:2.5213, Validation Accuracy:0.1609\n",
    "Epoch #209: Loss:2.3909, Accuracy:0.1938, Validation Loss:2.5167, Validation Accuracy:0.1642\n",
    "Epoch #210: Loss:2.3919, Accuracy:0.1988, Validation Loss:2.5143, Validation Accuracy:0.1626\n",
    "Epoch #211: Loss:2.3909, Accuracy:0.2012, Validation Loss:2.5213, Validation Accuracy:0.1544\n",
    "Epoch #212: Loss:2.3967, Accuracy:0.1943, Validation Loss:2.5145, Validation Accuracy:0.1560\n",
    "Epoch #213: Loss:2.3938, Accuracy:0.1979, Validation Loss:2.5107, Validation Accuracy:0.1773\n",
    "Epoch #214: Loss:2.3900, Accuracy:0.1938, Validation Loss:2.5152, Validation Accuracy:0.1658\n",
    "Epoch #215: Loss:2.3885, Accuracy:0.1992, Validation Loss:2.5133, Validation Accuracy:0.1576\n",
    "Epoch #216: Loss:2.3873, Accuracy:0.1996, Validation Loss:2.5166, Validation Accuracy:0.1691\n",
    "Epoch #217: Loss:2.3897, Accuracy:0.1979, Validation Loss:2.5126, Validation Accuracy:0.1708\n",
    "Epoch #218: Loss:2.3870, Accuracy:0.1975, Validation Loss:2.5108, Validation Accuracy:0.1773\n",
    "Epoch #219: Loss:2.3905, Accuracy:0.1951, Validation Loss:2.5185, Validation Accuracy:0.1626\n",
    "Epoch #220: Loss:2.3915, Accuracy:0.1988, Validation Loss:2.5232, Validation Accuracy:0.1724\n",
    "Epoch #221: Loss:2.3901, Accuracy:0.1967, Validation Loss:2.5298, Validation Accuracy:0.1691\n",
    "Epoch #222: Loss:2.3917, Accuracy:0.1869, Validation Loss:2.5226, Validation Accuracy:0.1691\n",
    "Epoch #223: Loss:2.3859, Accuracy:0.1951, Validation Loss:2.5208, Validation Accuracy:0.1724\n",
    "Epoch #224: Loss:2.3869, Accuracy:0.1971, Validation Loss:2.5157, Validation Accuracy:0.1741\n",
    "Epoch #225: Loss:2.3851, Accuracy:0.2000, Validation Loss:2.5147, Validation Accuracy:0.1626\n",
    "Epoch #226: Loss:2.3862, Accuracy:0.2041, Validation Loss:2.5138, Validation Accuracy:0.1642\n",
    "Epoch #227: Loss:2.3818, Accuracy:0.2053, Validation Loss:2.5180, Validation Accuracy:0.1642\n",
    "Epoch #228: Loss:2.3847, Accuracy:0.2004, Validation Loss:2.5169, Validation Accuracy:0.1724\n",
    "Epoch #229: Loss:2.3859, Accuracy:0.1984, Validation Loss:2.5265, Validation Accuracy:0.1675\n",
    "Epoch #230: Loss:2.3850, Accuracy:0.1975, Validation Loss:2.5299, Validation Accuracy:0.1708\n",
    "Epoch #231: Loss:2.3865, Accuracy:0.1992, Validation Loss:2.5341, Validation Accuracy:0.1757\n",
    "Epoch #232: Loss:2.3884, Accuracy:0.1992, Validation Loss:2.5323, Validation Accuracy:0.1708\n",
    "Epoch #233: Loss:2.3909, Accuracy:0.1959, Validation Loss:2.5340, Validation Accuracy:0.1675\n",
    "Epoch #234: Loss:2.3927, Accuracy:0.2025, Validation Loss:2.5322, Validation Accuracy:0.1757\n",
    "Epoch #235: Loss:2.3881, Accuracy:0.2078, Validation Loss:2.5321, Validation Accuracy:0.1675\n",
    "Epoch #236: Loss:2.3900, Accuracy:0.1996, Validation Loss:2.5248, Validation Accuracy:0.1658\n",
    "Epoch #237: Loss:2.3879, Accuracy:0.1996, Validation Loss:2.5254, Validation Accuracy:0.1691\n",
    "Epoch #238: Loss:2.3890, Accuracy:0.1967, Validation Loss:2.5217, Validation Accuracy:0.1658\n",
    "Epoch #239: Loss:2.3886, Accuracy:0.2016, Validation Loss:2.5261, Validation Accuracy:0.1593\n",
    "Epoch #240: Loss:2.3937, Accuracy:0.1984, Validation Loss:2.5293, Validation Accuracy:0.1675\n",
    "Epoch #241: Loss:2.3902, Accuracy:0.1984, Validation Loss:2.5261, Validation Accuracy:0.1609\n",
    "Epoch #242: Loss:2.3892, Accuracy:0.2033, Validation Loss:2.5337, Validation Accuracy:0.1609\n",
    "Epoch #243: Loss:2.3912, Accuracy:0.2025, Validation Loss:2.5267, Validation Accuracy:0.1576\n",
    "Epoch #244: Loss:2.3903, Accuracy:0.1992, Validation Loss:2.5254, Validation Accuracy:0.1724\n",
    "Epoch #245: Loss:2.3911, Accuracy:0.1988, Validation Loss:2.5288, Validation Accuracy:0.1593\n",
    "Epoch #246: Loss:2.3916, Accuracy:0.2000, Validation Loss:2.5237, Validation Accuracy:0.1642\n",
    "Epoch #247: Loss:2.3874, Accuracy:0.1988, Validation Loss:2.5242, Validation Accuracy:0.1741\n",
    "Epoch #248: Loss:2.3898, Accuracy:0.1938, Validation Loss:2.5273, Validation Accuracy:0.1642\n",
    "Epoch #249: Loss:2.3871, Accuracy:0.1992, Validation Loss:2.5242, Validation Accuracy:0.1609\n",
    "Epoch #250: Loss:2.3878, Accuracy:0.2008, Validation Loss:2.5268, Validation Accuracy:0.1675\n",
    "Epoch #251: Loss:2.3865, Accuracy:0.1967, Validation Loss:2.5248, Validation Accuracy:0.1741\n",
    "Epoch #252: Loss:2.3850, Accuracy:0.2033, Validation Loss:2.5250, Validation Accuracy:0.1626\n",
    "Epoch #253: Loss:2.3847, Accuracy:0.2021, Validation Loss:2.5253, Validation Accuracy:0.1658\n",
    "Epoch #254: Loss:2.3823, Accuracy:0.2041, Validation Loss:2.5243, Validation Accuracy:0.1708\n",
    "Epoch #255: Loss:2.3819, Accuracy:0.2041, Validation Loss:2.5236, Validation Accuracy:0.1724\n",
    "Epoch #256: Loss:2.3819, Accuracy:0.2107, Validation Loss:2.5207, Validation Accuracy:0.1790\n",
    "Epoch #257: Loss:2.3813, Accuracy:0.2012, Validation Loss:2.5291, Validation Accuracy:0.1609\n",
    "Epoch #258: Loss:2.3805, Accuracy:0.2041, Validation Loss:2.5316, Validation Accuracy:0.1691\n",
    "Epoch #259: Loss:2.3827, Accuracy:0.2004, Validation Loss:2.5314, Validation Accuracy:0.1626\n",
    "Epoch #260: Loss:2.3815, Accuracy:0.1951, Validation Loss:2.5313, Validation Accuracy:0.1724\n",
    "Epoch #261: Loss:2.3821, Accuracy:0.2004, Validation Loss:2.5323, Validation Accuracy:0.1626\n",
    "Epoch #262: Loss:2.3806, Accuracy:0.2041, Validation Loss:2.5296, Validation Accuracy:0.1658\n",
    "Epoch #263: Loss:2.3784, Accuracy:0.1984, Validation Loss:2.5288, Validation Accuracy:0.1626\n",
    "Epoch #264: Loss:2.3807, Accuracy:0.2029, Validation Loss:2.5300, Validation Accuracy:0.1576\n",
    "Epoch #265: Loss:2.3789, Accuracy:0.2045, Validation Loss:2.5340, Validation Accuracy:0.1576\n",
    "Epoch #266: Loss:2.3779, Accuracy:0.2041, Validation Loss:2.5296, Validation Accuracy:0.1675\n",
    "Epoch #267: Loss:2.3778, Accuracy:0.2000, Validation Loss:2.5309, Validation Accuracy:0.1658\n",
    "Epoch #268: Loss:2.3762, Accuracy:0.2029, Validation Loss:2.5264, Validation Accuracy:0.1691\n",
    "Epoch #269: Loss:2.3758, Accuracy:0.1984, Validation Loss:2.5286, Validation Accuracy:0.1724\n",
    "Epoch #270: Loss:2.3745, Accuracy:0.2037, Validation Loss:2.5286, Validation Accuracy:0.1741\n",
    "Epoch #271: Loss:2.3747, Accuracy:0.2049, Validation Loss:2.5265, Validation Accuracy:0.1790\n",
    "Epoch #272: Loss:2.3785, Accuracy:0.2025, Validation Loss:2.5323, Validation Accuracy:0.1642\n",
    "Epoch #273: Loss:2.3773, Accuracy:0.2053, Validation Loss:2.5276, Validation Accuracy:0.1675\n",
    "Epoch #274: Loss:2.3760, Accuracy:0.2016, Validation Loss:2.5339, Validation Accuracy:0.1642\n",
    "Epoch #275: Loss:2.3752, Accuracy:0.2053, Validation Loss:2.5330, Validation Accuracy:0.1741\n",
    "Epoch #276: Loss:2.3736, Accuracy:0.2049, Validation Loss:2.5305, Validation Accuracy:0.1609\n",
    "Epoch #277: Loss:2.3699, Accuracy:0.2103, Validation Loss:2.5292, Validation Accuracy:0.1741\n",
    "Epoch #278: Loss:2.3690, Accuracy:0.2082, Validation Loss:2.5359, Validation Accuracy:0.1675\n",
    "Epoch #279: Loss:2.3674, Accuracy:0.2062, Validation Loss:2.5309, Validation Accuracy:0.1806\n",
    "Epoch #280: Loss:2.3685, Accuracy:0.2066, Validation Loss:2.5356, Validation Accuracy:0.1708\n",
    "Epoch #281: Loss:2.3659, Accuracy:0.2086, Validation Loss:2.5349, Validation Accuracy:0.1609\n",
    "Epoch #282: Loss:2.3662, Accuracy:0.2057, Validation Loss:2.5283, Validation Accuracy:0.1642\n",
    "Epoch #283: Loss:2.3657, Accuracy:0.2115, Validation Loss:2.5311, Validation Accuracy:0.1560\n",
    "Epoch #284: Loss:2.3668, Accuracy:0.2094, Validation Loss:2.5271, Validation Accuracy:0.1658\n",
    "Epoch #285: Loss:2.3670, Accuracy:0.2074, Validation Loss:2.5251, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.3675, Accuracy:0.2045, Validation Loss:2.5228, Validation Accuracy:0.1741\n",
    "Epoch #287: Loss:2.3712, Accuracy:0.2012, Validation Loss:2.5322, Validation Accuracy:0.1708\n",
    "Epoch #288: Loss:2.3678, Accuracy:0.2099, Validation Loss:2.5401, Validation Accuracy:0.1675\n",
    "Epoch #289: Loss:2.3739, Accuracy:0.2025, Validation Loss:2.5396, Validation Accuracy:0.1724\n",
    "Epoch #290: Loss:2.3732, Accuracy:0.2045, Validation Loss:2.5346, Validation Accuracy:0.1773\n",
    "Epoch #291: Loss:2.3729, Accuracy:0.2082, Validation Loss:2.5387, Validation Accuracy:0.1675\n",
    "Epoch #292: Loss:2.3717, Accuracy:0.1992, Validation Loss:2.5421, Validation Accuracy:0.1593\n",
    "Epoch #293: Loss:2.3685, Accuracy:0.2037, Validation Loss:2.5374, Validation Accuracy:0.1724\n",
    "Epoch #294: Loss:2.3622, Accuracy:0.2041, Validation Loss:2.5325, Validation Accuracy:0.1790\n",
    "Epoch #295: Loss:2.3636, Accuracy:0.2066, Validation Loss:2.5331, Validation Accuracy:0.1856\n",
    "Epoch #296: Loss:2.3603, Accuracy:0.2070, Validation Loss:2.5328, Validation Accuracy:0.1658\n",
    "Epoch #297: Loss:2.3589, Accuracy:0.2103, Validation Loss:2.5372, Validation Accuracy:0.1675\n",
    "Epoch #298: Loss:2.3655, Accuracy:0.2045, Validation Loss:2.5330, Validation Accuracy:0.1757\n",
    "Epoch #299: Loss:2.3636, Accuracy:0.1992, Validation Loss:2.5339, Validation Accuracy:0.1773\n",
    "Epoch #300: Loss:2.3583, Accuracy:0.2066, Validation Loss:2.5384, Validation Accuracy:0.1823\n",
    "\n",
    "Test:\n",
    "Test Loss:2.53839350, Accuracy:0.1823\n",
    "Labels: ['yd', 'ce', 'eb', 'ek', 'eo', 'mb', 'ck', 'by', 'sg', 'aa', 'sk', 'ds', 'ib', 'eg', 'my']\n",
    "Confusion Matrix:\n",
    "      yd  ce  eb  ek  eo  mb  ck  by  sg  aa  sk  ds  ib  eg  my\n",
    "t:yd  28   0   0   0   0   0   0   6  19   0   0   1   4   4   0\n",
    "t:ce   2   0   0   0   0   0   0   5  11   0   0   1   0   8   0\n",
    "t:eb   3   0   4   0   0   0   0   9   7   0   0   2   2  23   0\n",
    "t:ek   6   0   4   0   0   0   0  12  12   1   0   0   2  11   0\n",
    "t:eo   5   0   2   0   2   3   0  10   7   1   0   0   0   4   0\n",
    "t:mb  12   0   5   0   3   1   0   6  17   0   0   1   1   6   0\n",
    "t:ck   0   0   2   0   0   0   0   1   8   0   0   0   1  11   0\n",
    "t:by   3   0   3   0   0   0   0  12   8   1   0   1   1  11   0\n",
    "t:sg  13   0   1   0   1   1   0   7  22   0   0   1   0   5   0\n",
    "t:aa   3   0   0   0   1   0   0   6   3   1   0   2   0  18   0\n",
    "t:sk   3   0   2   0   0   0   0   8   0   0   0   3   0  17   0\n",
    "t:ds   2   0   2   0   0   0   0   5   4   0   0   6   0  12   0\n",
    "t:ib  25   0   1   0   1   0   0   4  16   0   0   1   3   3   0\n",
    "t:eg   0   0   1   0   1   1   0   6   3   1   0   5   0  32   0\n",
    "t:my   4   0   0   0   0   0   0   3   2   0   0   3   1   7   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.26      0.45      0.33        62\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          eb       0.15      0.08      0.10        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          eo       0.22      0.06      0.09        34\n",
    "          mb       0.17      0.02      0.03        52\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          by       0.12      0.30      0.17        40\n",
    "          sg       0.16      0.43      0.23        51\n",
    "          aa       0.20      0.03      0.05        34\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ds       0.22      0.19      0.21        31\n",
    "          ib       0.20      0.06      0.09        54\n",
    "          eg       0.19      0.64      0.29        50\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.13      0.15      0.11       609\n",
    "weighted avg       0.14      0.18      0.13       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 00:19:35 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.708580518590993, 2.6995046154423106, 2.6922773222617917, 2.6868533078085615, 2.682678407440436, 2.6786182905457094, 2.6750220077965645, 2.6717361248968468, 2.6681064361421933, 2.6639416233463633, 2.659815149745722, 2.6540757932490706, 2.6494677426975546, 2.6395716119086607, 2.628991082775573, 2.620809081348488, 2.6103899819510326, 2.6029753692827398, 2.5856845750793056, 2.5737011937672283, 2.571850671361037, 2.5697767714952993, 2.5568022457836883, 2.537477751279308, 2.53198549312911, 2.532211784071523, 2.521530844307885, 2.5220449851651496, 2.514954266681264, 2.5119111158186187, 2.51138625121469, 2.5103215552707416, 2.510283883569276, 2.507265098380729, 2.5081655196172656, 2.508881709062799, 2.5069010602233837, 2.5236953103483604, 2.5104979368657707, 2.49859196799142, 2.5023926976083337, 2.4990828424838964, 2.5008243570969806, 2.500276282111608, 2.5016651689908382, 2.5002597075182034, 2.5010992631144906, 2.500398116745972, 2.498435972946618, 2.4996498081288707, 2.4954049066565505, 2.497697065224984, 2.5011899420389008, 2.4970818023963517, 2.4982314904530845, 2.4982610864592303, 2.500296978331943, 2.4977334720160576, 2.503387061245923, 2.4993886963291514, 2.497731157124336, 2.496096943008097, 2.4984722035663274, 2.498427167118868, 2.499724870049112, 2.5042832688549272, 2.4992443767478707, 2.502577981729617, 2.4996267120630673, 2.501362248203046, 2.5022026572517184, 2.498809579949465, 2.4968346030449826, 2.499449508726499, 2.503716128213065, 2.5014989227301188, 2.500851581445077, 2.496663764192553, 2.499135504019476, 2.4970018112013492, 2.4968666721055857, 2.4966844870343388, 2.4979796797183935, 2.5010313283046477, 2.498296641755378, 2.497405426451334, 2.4994093072042483, 2.4974603880019415, 2.4992967043408423, 2.501216679371049, 2.4984982585280595, 2.494348993442329, 2.4940357294380178, 2.4995928256969733, 2.49780668257101, 2.497062780195465, 2.4949832314928178, 2.4957068722040585, 2.496970388102414, 2.4950974469114406, 2.4969918614342097, 2.5000221831066463, 2.502438832190628, 2.5055271514335096, 2.505259857976378, 2.5060274123362523, 2.5071016998322335, 2.502676765319749, 2.504868122940189, 2.499771807581333, 2.501836806878276, 2.497074404373545, 2.496646338887207, 2.5001641607832634, 2.5043890448822372, 2.505294163630318, 2.505194101427576, 2.5054613866633773, 2.5046063816214628, 2.503606146779554, 2.5031128567819327, 2.502651641325802, 2.5064013908649314, 2.5053833584088605, 2.5066816896836355, 2.5118960034475344, 2.506624099264787, 2.5059644988017715, 2.5097564182845242, 2.507594224854643, 2.509276383420321, 2.5067422448707917, 2.5081131740156652, 2.5071775964132477, 2.501949502133775, 2.502314235972262, 2.5068342082801904, 2.503588430204219, 2.5077146121433804, 2.510361666358359, 2.5105032231811624, 2.5102867066175087, 2.5083944770111435, 2.5052133209403906, 2.505254360646841, 2.506917256244102, 2.5075821046563007, 2.5057799683024338, 2.5049351616250273, 2.5079280335719165, 2.508822212861285, 2.5067577377720225, 2.506920039751651, 2.5082650912806317, 2.505435956131257, 2.5007448783649013, 2.500505603592971, 2.506036774865512, 2.5047114349546886, 2.507942493717463, 2.506030221682268, 2.503594176131125, 2.507673555994269, 2.50587121332416, 2.5051483462009525, 2.5049433970490504, 2.504582205429453, 2.506923885376778, 2.507380617858937, 2.5065843373879617, 2.502730092783084, 2.5038778922827962, 2.502834181871712, 2.5071889093552513, 2.5152524050037655, 2.509518169221424, 2.511143721970431, 2.5105780705638314, 2.5133013310299326, 2.507730509064272, 2.5104469250771406, 2.5134144201263027, 2.5184302063802586, 2.521183971505251, 2.521048372797974, 2.51489292068043, 2.511423310622793, 2.511523231496952, 2.5110323190297597, 2.508444582887471, 2.5076523089448024, 2.508827440061397, 2.5105464658126455, 2.511546733148384, 2.5111577099767226, 2.5071357228290077, 2.5095280062388903, 2.510077459667312, 2.511387850459182, 2.511377776980596, 2.5111716455230964, 2.515274678153553, 2.516689098135787, 2.513090744002895, 2.517810769073286, 2.5166295839256447, 2.520200691003909, 2.5212564769832566, 2.516687280438804, 2.5142519004239237, 2.521308222819236, 2.5145267982201034, 2.510675838232432, 2.515194554634282, 2.5133352655495327, 2.516603167616871, 2.512561307556328, 2.510803490436723, 2.518474875608297, 2.5231728749517934, 2.529774342851686, 2.522643052885685, 2.520830311407205, 2.515712004381252, 2.514712047890098, 2.513817133770396, 2.5179641207646464, 2.516886215883327, 2.5265061831826645, 2.529892361222817, 2.53409066654387, 2.5322738649026904, 2.5339798050365228, 2.5321761927581186, 2.532063636873743, 2.52480387257042, 2.5253508646891425, 2.521678338888635, 2.5261242875128938, 2.5293476930001293, 2.52610877151364, 2.533727131062149, 2.5266917174672847, 2.525427440508637, 2.5287662096602967, 2.5237187372248355, 2.524238578987435, 2.527320347004532, 2.524157863532381, 2.526754681895715, 2.5247671557177465, 2.525017086508239, 2.5252857963831357, 2.524287540141389, 2.5236145130714953, 2.5207004566693736, 2.529106567254403, 2.5316156033420407, 2.531365220378381, 2.531273433531838, 2.5322813337855345, 2.5295909699939547, 2.528769654006206, 2.529966164105044, 2.5339771260573163, 2.5296427545876337, 2.5309462363301045, 2.5263552254643935, 2.52863445109726, 2.5286177367412397, 2.5264701608366567, 2.53228525849203, 2.5276392543648654, 2.5338934458339546, 2.532995893841698, 2.530521619887579, 2.5291532334827243, 2.535933133612321, 2.53091875946972, 2.5355698361576877, 2.534874291647048, 2.5282925134417655, 2.5310612911074033, 2.527090892807408, 2.525097740890553, 2.52277052030579, 2.5322032092240057, 2.5401206126158264, 2.539616682650812, 2.534617270546398, 2.5387221874274646, 2.542055055621418, 2.5374292728348906, 2.532461395404609, 2.5330661364964078, 2.532820508985097, 2.5372319064899815, 2.5330166511347727, 2.5338774780530255, 2.5383933013295894], 'val_acc': [0.0821018059338842, 0.0788177337821677, 0.0821018060317572, 0.0821018060317572, 0.0821018060317572, 0.0821018060317572, 0.0821018060317572, 0.10673234809942433, 0.1198686360089454, 0.1198686370977823, 0.14121510621851496, 0.14614121449502623, 0.1313628888825086, 0.13957306989797427, 0.13136288917612757, 0.13136288917612757, 0.147783250521948, 0.1559934311459217, 0.1412151056312771, 0.11986863699990932, 0.13957306980010128, 0.134646962220935, 0.11986863610681837, 0.14449917797873957, 0.15435139482538102, 0.15270935879845923, 0.1592775033955112, 0.15763546727071645, 0.1609195398139249, 0.15435139511899995, 0.1576354675643354, 0.16091953991179786, 0.16256157584084666, 0.1592775033955112, 0.1609195397160519, 0.1658456477968172, 0.147783250521948, 0.1559934312437947, 0.14614121449502623, 0.1642036118677684, 0.15599343104804875, 0.15763546727071645, 0.1642036119656414, 0.16256157554722772, 0.1674876842152309, 0.1658456477968172, 0.16420361176989545, 0.16091953942243298, 0.16584564828618212, 0.1625615756451007, 0.16584564809043614, 0.16256157574297367, 0.16091953961817893, 0.16091953961817893, 0.1592775033955112, 0.16091953961817893, 0.16420361176989545, 0.16091953952030597, 0.16748768401948494, 0.1642036118677684, 0.1625615756451007, 0.1625615756451007, 0.16420361167202246, 0.16256157554722772, 0.15927750319976525, 0.16420361167202246, 0.1576354670749705, 0.16748768382373896, 0.17077175607332848, 0.17077175607332848, 0.16748768392161195, 0.16091953932456, 0.16420361167202246, 0.16748768392161195, 0.1691297200464067, 0.16748768392161195, 0.16748768392161195, 0.17077175617120144, 0.16748768382373896, 0.16420361167202246, 0.16748768392161195, 0.16420361167202246, 0.1658456477968172, 0.1691297200464067, 0.1691297200464067, 0.16584564769894422, 0.17077175607332848, 0.16748768392161195, 0.17077175617120144, 0.17077175617120144, 0.17077175617120144, 0.17241379229599618, 0.17077175617120144, 0.1691297199485337, 0.1691297200464067, 0.1691297200464067, 0.16748768392161195, 0.16748768392161195, 0.1658456477968172, 0.16091953942243298, 0.16748768392161195, 0.16420361176989545, 0.16748768392161195, 0.16748768401948494, 0.1658456478946902, 0.1658456477968172, 0.16584564769894422, 0.16256157544935473, 0.16420361176989545, 0.15927750310189226, 0.16256157554722772, 0.16420361167202246, 0.16256157554722772, 0.16748768392161195, 0.1658456477968172, 0.1658456477968172, 0.16256157554722772, 0.1658456477968172, 0.17077175617120144, 0.16748768392161195, 0.17077175617120144, 0.17241379239386917, 0.1691297199485337, 0.16748768401948494, 0.17077175626907443, 0.1691297199485337, 0.16420361157414948, 0.1773399007682534, 0.1691297200464067, 0.16912972024215267, 0.17898193689304814, 0.17241379239386917, 0.1691297199485337, 0.17077175607332848, 0.17077175626907443, 0.17898193679517518, 0.17569786454558567, 0.17569786454558567, 0.17569786474133164, 0.17405582842079093, 0.17241379229599618, 0.17077175607332848, 0.17241379239386917, 0.17241379229599618, 0.16256157554722772, 0.1658456478946902, 0.1625615756451007, 0.17077175626907443, 0.1691297200464067, 0.16912972014427968, 0.16748768401948494, 0.17241379239386917, 0.16912972014427968, 0.18390804536530536, 0.18226600914263763, 0.17569786454558567, 0.18062397321358886, 0.17241379229599618, 0.1773399007682534, 0.16748768382373896, 0.16748768392161195, 0.16912972014427968, 0.16256157544935473, 0.1740558286165369, 0.17077175626907443, 0.1707717563669474, 0.16748768392161195, 0.1707717563669474, 0.17077175626907443, 0.16584564760107126, 0.16748768382373896, 0.1658456477968172, 0.17241379210025023, 0.17077175617120144, 0.16912971985066075, 0.16256157544935473, 0.1625615756451007, 0.16420361176989545, 0.1576354670749705, 0.17569786524293066, 0.17241379219812322, 0.16256157554722772, 0.16748768382373896, 0.1625615752536088, 0.16912972014427968, 0.1691297199485337, 0.17241379249174216, 0.17241379229599618, 0.1691297200464067, 0.1756978644477127, 0.1691297199485337, 0.16912972014427968, 0.17733990057250745, 0.17077175626907443, 0.1740558285186639, 0.16912972014427968, 0.16912972014427968, 0.17077175607332848, 0.1691297199485337, 0.17405582822504498, 0.16584564799256318, 0.16256157544935473, 0.167487683725866, 0.1625615756451007, 0.17241379239386917, 0.16091953932456, 0.1691297199485337, 0.16091953942243298, 0.16420361246724044, 0.16256157614669972, 0.154351394923254, 0.15599343104804875, 0.1773399007682534, 0.1658456477968172, 0.15763546727071645, 0.1691297199485337, 0.17077175626907443, 0.17733990067038044, 0.16256157554722772, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.17241379219812322, 0.17405582832291797, 0.16256157554722772, 0.16420361147627652, 0.16420361157414948, 0.17241379219812322, 0.16748768392161195, 0.17077175607332848, 0.17569786454558567, 0.17077175646482037, 0.16748768392161195, 0.17569786434983972, 0.16748768382373896, 0.16584564760107126, 0.16912971975278776, 0.16584564760107126, 0.1592775030040193, 0.167487683725866, 0.160919539826159, 0.16091953932456, 0.15763546687922456, 0.17241379299334117, 0.15927750319976525, 0.1642036119656414, 0.1740558292160089, 0.16420361246724044, 0.160919539226687, 0.167487683725866, 0.17405582822504498, 0.16256157535148175, 0.16584564769894422, 0.1707717559754555, 0.17241379219812322, 0.17898193759039313, 0.160919539226687, 0.16912972084162467, 0.16256157535148175, 0.17241379210025023, 0.16256157634244567, 0.16584564769894422, 0.16256157544935473, 0.1576354670749705, 0.15763546757656952, 0.16748768382373896, 0.1658456478946902, 0.16912972074375168, 0.17241379229599618, 0.17405582842079093, 0.17898193650155622, 0.16420361137840353, 0.167487683725866, 0.16420361157414948, 0.17405582842079093, 0.16091953932456, 0.1740558292160089, 0.16748768392161195, 0.18062397371518787, 0.1707717558775825, 0.16091953942243298, 0.16420361147627652, 0.15599343104804875, 0.16584564769894422, 0.1691297200464067, 0.1740558285186639, 0.17077175626907443, 0.16748768392161195, 0.17241379239386917, 0.17733990047463447, 0.16748768392161195, 0.15927750319976525, 0.17241379219812322, 0.17898193679517518, 0.1855500814901001, 0.16584564750319827, 0.16748768411735793, 0.17569786474133164, 0.17733990096399938, 0.1822660093383836], 'loss': [2.717316399168919, 2.7057616048769786, 2.6969308032637014, 2.690679307542053, 2.685814959606351, 2.6817119772674123, 2.6779138339863175, 2.6737148592359476, 2.66961578059735, 2.6644519874447425, 2.659181094512313, 2.6523065624785374, 2.6464152227436983, 2.638837644353784, 2.6271567727506038, 2.615253163167339, 2.60793986702357, 2.5932342049522323, 2.5783891755207855, 2.5645046259588287, 2.5560531924637435, 2.5393838323362066, 2.536006468767013, 2.5290689135478996, 2.5179404326288117, 2.5112000984332887, 2.5011174208819256, 2.4940153093553423, 2.4945191151307595, 2.484437720046151, 2.4824698836896455, 2.4769607635983695, 2.474086910988026, 2.4703249158800507, 2.4694083782681693, 2.4660924402350517, 2.464752373803078, 2.4727021406318617, 2.476490243210685, 2.4643586639506125, 2.458406771183993, 2.456998053125777, 2.453437495182672, 2.45189596358266, 2.4507268936971864, 2.4491539962972215, 2.4474508401060007, 2.445578555943295, 2.445773474146943, 2.4441047333593975, 2.4422032753789695, 2.442961768496942, 2.4412067435850107, 2.441017452745222, 2.4416318716209773, 2.4389633638902857, 2.4379793682137554, 2.4369830863921305, 2.4368358651225814, 2.437435874410234, 2.4368074360324616, 2.435108391509164, 2.4330902000717067, 2.432587324471444, 2.433667150023537, 2.431684804011664, 2.433260282892466, 2.432542595971047, 2.4322242139546044, 2.4307001491836453, 2.4293903343976155, 2.4297277699260986, 2.4280381360337966, 2.4276526409987307, 2.427005199044637, 2.4271054240467613, 2.4266580810781866, 2.4258096978894494, 2.425310447916113, 2.4261198546117826, 2.4246194493844033, 2.4233099651532495, 2.423838429186623, 2.421740656760684, 2.419443580795852, 2.4189429949930807, 2.417426672216803, 2.4179407103105737, 2.4177579800451072, 2.4188470205976733, 2.4175450392572295, 2.4170261588674307, 2.4189178259220947, 2.4181258475756007, 2.416599197407278, 2.4164752579322832, 2.416477174484754, 2.415732393617258, 2.414454074561963, 2.416670215105374, 2.4171590998921797, 2.4183230481353384, 2.4185197557022438, 2.418048707268811, 2.4195395569536964, 2.420928060543366, 2.419914867452038, 2.4206119200532195, 2.4171087499027135, 2.417350492046599, 2.4168539769840436, 2.415725324286083, 2.416398015306226, 2.4157764530769366, 2.413803686302545, 2.4140552799804498, 2.4140776799689574, 2.4129127674768593, 2.413301521845667, 2.412296934539043, 2.4120176526800075, 2.4115623733591005, 2.4104218383099756, 2.4118415272456173, 2.4103939332511635, 2.40920178248897, 2.4101989196800844, 2.407322696885534, 2.408015320384282, 2.406763129851167, 2.4081479504367898, 2.4064103092256266, 2.40816502943176, 2.4082239706168673, 2.4067664126840707, 2.405493288803884, 2.405482156173894, 2.4052663882410257, 2.4075398434358943, 2.406572179774729, 2.4058256735791907, 2.408240657371662, 2.403385821798744, 2.404471844667282, 2.4049591211322885, 2.404165598597125, 2.40457821695222, 2.403909347189525, 2.4035199854652984, 2.403527140568414, 2.4019301919721725, 2.4025183476951333, 2.40277982786206, 2.400264214000663, 2.4001251229515312, 2.4033726399439317, 2.400131877491851, 2.399612708160275, 2.3974963764879984, 2.397338425501171, 2.3973723546680237, 2.3964466646221876, 2.395748501391871, 2.3955942574957314, 2.3949749088874834, 2.3948113313201027, 2.392889668956185, 2.3933361450994286, 2.3933266607398127, 2.3942860880426804, 2.3959145623310882, 2.3950730342395006, 2.3950381973196104, 2.3973318982173284, 2.3967782586506994, 2.399499797918958, 2.398703402415438, 2.4027622922000456, 2.4041674224258203, 2.399816382053696, 2.398037620050951, 2.3920332314297403, 2.3924805511928926, 2.395233209519905, 2.39309467123764, 2.3960464596014, 2.395172578647151, 2.3925367488508598, 2.39256755654572, 2.3882485048971627, 2.388817495780804, 2.387296254434135, 2.3893582244183738, 2.3900600156255325, 2.39131271334889, 2.3890428140667677, 2.388442407741194, 2.3850533786985175, 2.386817323794355, 2.387351271846701, 2.387576439835942, 2.386286934396325, 2.3849334476909596, 2.3854981871845786, 2.3878053092369065, 2.3891514040851005, 2.3994146962919762, 2.3986689680166067, 2.390930088638525, 2.3918992049395427, 2.390921742029993, 2.3966531854390607, 2.3937874803797663, 2.3899833975876135, 2.388470331990988, 2.387251404227662, 2.389733953593448, 2.386954129354175, 2.3904507399094914, 2.391523624298754, 2.39007262821315, 2.3916723515708345, 2.385850179856318, 2.386944132517006, 2.385051704970718, 2.386224292191147, 2.3818330678606916, 2.384715831646929, 2.3859391102800624, 2.3850246956216234, 2.38651822499426, 2.3883802607808513, 2.390888094853082, 2.392725164248958, 2.3881308901236533, 2.3899604973606996, 2.387923971485553, 2.389043048372993, 2.388600025921142, 2.3937043341767863, 2.3902037508923417, 2.389161527230265, 2.391195812068681, 2.390250357759072, 2.3911013140081137, 2.391632991356037, 2.3873652798928764, 2.3898370368769526, 2.3871183057585292, 2.3878244403940942, 2.3865131635685475, 2.3849500726625417, 2.3846551617068186, 2.3823485785686014, 2.38192247747151, 2.3818635872012535, 2.3813308754985583, 2.380546444103703, 2.382707492037231, 2.3815420469953783, 2.382115405785719, 2.3805886315614044, 2.3784283692831867, 2.380701960675281, 2.3788987824559458, 2.3779385510900917, 2.377780035829642, 2.37618543959741, 2.375837516295102, 2.3744616488901253, 2.3746708721106056, 2.3785413087026295, 2.3773140982680743, 2.375995321195473, 2.375197455525643, 2.3736254336652816, 2.3699094421809703, 2.3689754790593955, 2.36737759597982, 2.3684670180021126, 2.365943219970139, 2.366199586131979, 2.3657090834278836, 2.366831081554875, 2.3669966108255562, 2.367481748377273, 2.3711692574087846, 2.3677614144476045, 2.3739084222233515, 2.3732108798604727, 2.3728842969303012, 2.371722589723873, 2.368482562697644, 2.3622068875134605, 2.363566155306368, 2.3603302961502233, 2.3588512766287804, 2.365547496043681, 2.3635872882004882, 2.3583487678112682], 'acc': [0.08213552376389259, 0.07515400407464842, 0.08336755700057537, 0.08131416793109456, 0.0813141687235793, 0.08131416813610026, 0.0813141687235793, 0.0911704317347225, 0.11498973376941876, 0.12320328593498872, 0.14414784304904743, 0.1626283374471586, 0.15112936374955108, 0.14579055332550034, 0.14784394339247162, 0.14537987731075874, 0.14784394260916622, 0.1597535925357004, 0.1585215608931665, 0.15605749418725712, 0.15359342984962268, 0.15071868476069683, 0.14866529845114362, 0.15071868515234954, 0.15893223888453029, 0.1634496920958192, 0.15975359431649624, 0.16427104752778518, 0.16016427156127208, 0.15728952743311927, 0.16098562734817332, 0.16344969148998142, 0.16427104654865343, 0.1679671448970967, 0.1671457908542739, 0.17494866540177403, 0.17084188920271715, 0.1609856267974117, 0.16796714471962906, 0.1655030811469413, 0.17412731114476612, 0.16632443599142824, 0.17618069745431936, 0.17905544038915536, 0.17987679738773213, 0.17741273128766055, 0.17905544156411346, 0.1778234094932094, 0.1778234095115681, 0.1790554405849817, 0.1786447637502173, 0.17782340792659862, 0.17618069884346252, 0.17741273207096594, 0.18316221774481162, 0.1786447629669119, 0.18069815144891366, 0.18069815205475143, 0.18110882965446254, 0.18234086309615102, 0.1815195078600114, 0.18110882887115715, 0.18603696205043205, 0.18357289634201315, 0.17782340851407766, 0.18398357299930995, 0.18316221874230207, 0.17535934300148512, 0.18069815086143462, 0.1819301854780812, 0.1819301854964399, 0.18151950727253235, 0.17905544197412487, 0.179876795625295, 0.17823408691545287, 0.18193018469477582, 0.18644763927684918, 0.18521560622681338, 0.18562628445072096, 0.1843942504031947, 0.18398357198346077, 0.1872689933380307, 0.18398357299930995, 0.18521560603098702, 0.1839835735684303, 0.18357289516705508, 0.17905544117246075, 0.18151950688087964, 0.18767967117028558, 0.18521560505185528, 0.18726899492300023, 0.18480492802126453, 0.18809034935747573, 0.1839835728034836, 0.18973306059470166, 0.19178644733262504, 0.1921971255381739, 0.19137577049786061, 0.18932238141002106, 0.1901437378394775, 0.1868583165032663, 0.18850102738555696, 0.18891170498526808, 0.18809034898418175, 0.19014373803530385, 0.186858316894919, 0.18809034896582305, 0.1848049276112531, 0.18480492704213278, 0.18850102617388143, 0.18891170322283093, 0.1893223814283798, 0.18932238082254202, 0.18932238123255343, 0.18767967154357956, 0.1872689949046415, 0.18644763808353237, 0.1889117039877776, 0.18809034935747573, 0.18932238085925945, 0.18973305963392864, 0.1893223814283798, 0.1917864465309609, 0.18850102777720967, 0.18850102679807793, 0.19014373686034577, 0.18891170361448362, 0.19014373666451942, 0.1889117046119741, 0.18932238199750012, 0.19055441508425336, 0.18891170379195124, 0.1864476384935438, 0.1921971251648799, 0.19137577028367553, 0.1893223830133493, 0.18644763927684918, 0.1864476377102384, 0.18726899353385706, 0.19055441387257782, 0.19014373803530385, 0.1885010259780551, 0.1921971247548685, 0.18767967115192688, 0.18767967195359098, 0.19178644772427772, 0.19589322312167048, 0.1921971255381739, 0.18891170498526808, 0.19425051245356487, 0.19876796822895504, 0.19466119026746104, 0.19958932188012515, 0.19630390113139298, 0.19466119044492866, 0.19712525656335897, 0.19589322488410763, 0.1930184803643021, 0.19466119202989818, 0.1995893220575928, 0.1987679664481592, 0.20082135510762858, 0.20164271036212694, 0.2004106780586791, 0.19917864504536076, 0.19876796742729094, 0.1971252561717063, 0.2004106764920683, 0.19548254669691748, 0.19301848098849858, 0.20041067652878575, 0.19383983640210584, 0.19507186945214164, 0.19466119144241914, 0.19671457892693045, 0.1942505122393798, 0.20164271014794188, 0.19383983681211725, 0.1958932239233346, 0.19014373686034577, 0.18891170459361536, 0.1950718688646626, 0.1983572902008738, 0.19917864404787028, 0.1999999991065423, 0.20041067709790608, 0.1979466113894872, 0.19425051282685885, 0.19589322390497588, 0.1946611920666156, 0.19466119163824547, 0.20123203352736252, 0.1950718684546512, 0.2004106764920683, 0.20041067729373244, 0.1979466129744567, 0.19917864387040266, 0.19917864524118709, 0.19917864404787028, 0.19548254587689465, 0.19014373762529244, 0.20041067748955876, 0.20041067868287557, 0.19425051284521758, 0.19671457931858313, 0.19712525754249072, 0.200410676883721, 0.19507186806299848, 0.19383983520878903, 0.19876796821059634, 0.2012320319240343, 0.19425051284521758, 0.19794661082036685, 0.19383983560044174, 0.19917864545537217, 0.1995893224492455, 0.19794661080200815, 0.1975359337897761, 0.19507186747551944, 0.1987679664481592, 0.19671457816198376, 0.18685831591578725, 0.19507186806299848, 0.19712525615334756, 0.19999999891071593, 0.20410677505469665, 0.2053388079089061, 0.20041067789957018, 0.19835728842007797, 0.19753593494637547, 0.19917864385204392, 0.19917864543701344, 0.19589322331749684, 0.20246406540244022, 0.2078028748290005, 0.1995893212742874, 0.19958932305508326, 0.1967145799244209, 0.20164271094960598, 0.19835728980922113, 0.19835729000504745, 0.2032854212260589, 0.2024640646007761, 0.19917864582866615, 0.19876796799641125, 0.1999999999082064, 0.19876796762311727, 0.19383983640210584, 0.19917864463534932, 0.2008213559092927, 0.19671457816198376, 0.20328542103023256, 0.2020533881760231, 0.2041067756421757, 0.2041067750914141, 0.21067761913462096, 0.20123203174656668, 0.20410677585636075, 0.20041067768538512, 0.19507186907884766, 0.20041067709790608, 0.2041067766763836, 0.1983572902008738, 0.20287474360798907, 0.20451745347443057, 0.2041067772271452, 0.19999999949819497, 0.20287474239631356, 0.19835728863426302, 0.20369609786499698, 0.2049281314841531, 0.20246406538408152, 0.2053388098855283, 0.2016427097746479, 0.20533880872892893, 0.2049281314841531, 0.21026693955828765, 0.2082135528203643, 0.20616016337758952, 0.2065708421522587, 0.20862422928183474, 0.2057494873261305, 0.21149897219831204, 0.20944558549710612, 0.20739219621344024, 0.20451745408026833, 0.2012320323340457, 0.20985626389848133, 0.2024640646007761, 0.20451745269112517, 0.2082135530345494, 0.19917864543701344, 0.20369609864830238, 0.20410677704967758, 0.20657084099565934, 0.20698152057199262, 0.21026694032323434, 0.2045174546677474, 0.19917864387040266, 0.20657084136895332]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
