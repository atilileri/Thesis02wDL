{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf31.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 15:10:15 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': 'Front', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['aa', 'yd', 'ck', 'eo', 'ek', 'ds', 'sg', 'eb', 'ib', 'ce', 'eg', 'my', 'mb', 'sk', 'by'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001AD94F6D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001AD90726EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7000, Accuracy:0.0850, Validation Loss:2.6938, Validation Accuracy:0.0854\n",
    "Epoch #2: Loss:2.6898, Accuracy:0.0850, Validation Loss:2.6847, Validation Accuracy:0.0854\n",
    "Epoch #3: Loss:2.6814, Accuracy:0.0809, Validation Loss:2.6763, Validation Accuracy:0.0772\n",
    "Epoch #4: Loss:2.6738, Accuracy:0.0809, Validation Loss:2.6691, Validation Accuracy:0.0870\n",
    "Epoch #5: Loss:2.6680, Accuracy:0.0895, Validation Loss:2.6671, Validation Accuracy:0.0936\n",
    "Epoch #6: Loss:2.6661, Accuracy:0.1080, Validation Loss:2.6629, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6618, Accuracy:0.1043, Validation Loss:2.6574, Validation Accuracy:0.1182\n",
    "Epoch #8: Loss:2.6565, Accuracy:0.1248, Validation Loss:2.6521, Validation Accuracy:0.1412\n",
    "Epoch #9: Loss:2.6512, Accuracy:0.1331, Validation Loss:2.6454, Validation Accuracy:0.1478\n",
    "Epoch #10: Loss:2.6449, Accuracy:0.1351, Validation Loss:2.6384, Validation Accuracy:0.1429\n",
    "Epoch #11: Loss:2.6377, Accuracy:0.1326, Validation Loss:2.6302, Validation Accuracy:0.1461\n",
    "Epoch #12: Loss:2.6298, Accuracy:0.1511, Validation Loss:2.6195, Validation Accuracy:0.1675\n",
    "Epoch #13: Loss:2.6192, Accuracy:0.1610, Validation Loss:2.6057, Validation Accuracy:0.1658\n",
    "Epoch #14: Loss:2.6076, Accuracy:0.1606, Validation Loss:2.5895, Validation Accuracy:0.1675\n",
    "Epoch #15: Loss:2.5948, Accuracy:0.1610, Validation Loss:2.5723, Validation Accuracy:0.1675\n",
    "Epoch #16: Loss:2.5785, Accuracy:0.1606, Validation Loss:2.5534, Validation Accuracy:0.1675\n",
    "Epoch #17: Loss:2.5673, Accuracy:0.1618, Validation Loss:2.5474, Validation Accuracy:0.1642\n",
    "Epoch #18: Loss:2.5878, Accuracy:0.1441, Validation Loss:2.5692, Validation Accuracy:0.1478\n",
    "Epoch #19: Loss:2.5797, Accuracy:0.1437, Validation Loss:2.5474, Validation Accuracy:0.1741\n",
    "Epoch #20: Loss:2.5608, Accuracy:0.1626, Validation Loss:2.5172, Validation Accuracy:0.1691\n",
    "Epoch #21: Loss:2.5392, Accuracy:0.1606, Validation Loss:2.5065, Validation Accuracy:0.1675\n",
    "Epoch #22: Loss:2.5285, Accuracy:0.1589, Validation Loss:2.4867, Validation Accuracy:0.1675\n",
    "Epoch #23: Loss:2.5170, Accuracy:0.1610, Validation Loss:2.4796, Validation Accuracy:0.1691\n",
    "Epoch #24: Loss:2.5101, Accuracy:0.1663, Validation Loss:2.4652, Validation Accuracy:0.1658\n",
    "Epoch #25: Loss:2.5016, Accuracy:0.1684, Validation Loss:2.4610, Validation Accuracy:0.1790\n",
    "Epoch #26: Loss:2.4957, Accuracy:0.1721, Validation Loss:2.4531, Validation Accuracy:0.1757\n",
    "Epoch #27: Loss:2.4902, Accuracy:0.1696, Validation Loss:2.4476, Validation Accuracy:0.1806\n",
    "Epoch #28: Loss:2.4863, Accuracy:0.1696, Validation Loss:2.4415, Validation Accuracy:0.1806\n",
    "Epoch #29: Loss:2.4840, Accuracy:0.1708, Validation Loss:2.4443, Validation Accuracy:0.1806\n",
    "Epoch #30: Loss:2.4804, Accuracy:0.1696, Validation Loss:2.4385, Validation Accuracy:0.1757\n",
    "Epoch #31: Loss:2.4784, Accuracy:0.1684, Validation Loss:2.4380, Validation Accuracy:0.1741\n",
    "Epoch #32: Loss:2.4776, Accuracy:0.1749, Validation Loss:2.4346, Validation Accuracy:0.1741\n",
    "Epoch #33: Loss:2.4752, Accuracy:0.1721, Validation Loss:2.4342, Validation Accuracy:0.1790\n",
    "Epoch #34: Loss:2.4746, Accuracy:0.1737, Validation Loss:2.4301, Validation Accuracy:0.1757\n",
    "Epoch #35: Loss:2.4715, Accuracy:0.1754, Validation Loss:2.4338, Validation Accuracy:0.1790\n",
    "Epoch #36: Loss:2.4752, Accuracy:0.1741, Validation Loss:2.4275, Validation Accuracy:0.1741\n",
    "Epoch #37: Loss:2.4738, Accuracy:0.1725, Validation Loss:2.4359, Validation Accuracy:0.1823\n",
    "Epoch #38: Loss:2.4723, Accuracy:0.1721, Validation Loss:2.4265, Validation Accuracy:0.1757\n",
    "Epoch #39: Loss:2.4680, Accuracy:0.1745, Validation Loss:2.4264, Validation Accuracy:0.1806\n",
    "Epoch #40: Loss:2.4668, Accuracy:0.1762, Validation Loss:2.4223, Validation Accuracy:0.1741\n",
    "Epoch #41: Loss:2.4653, Accuracy:0.1729, Validation Loss:2.4273, Validation Accuracy:0.1790\n",
    "Epoch #42: Loss:2.4648, Accuracy:0.1762, Validation Loss:2.4206, Validation Accuracy:0.1773\n",
    "Epoch #43: Loss:2.4641, Accuracy:0.1717, Validation Loss:2.4228, Validation Accuracy:0.1790\n",
    "Epoch #44: Loss:2.4631, Accuracy:0.1778, Validation Loss:2.4192, Validation Accuracy:0.1806\n",
    "Epoch #45: Loss:2.4627, Accuracy:0.1725, Validation Loss:2.4254, Validation Accuracy:0.1806\n",
    "Epoch #46: Loss:2.4633, Accuracy:0.1741, Validation Loss:2.4187, Validation Accuracy:0.1872\n",
    "Epoch #47: Loss:2.4643, Accuracy:0.1745, Validation Loss:2.4276, Validation Accuracy:0.1823\n",
    "Epoch #48: Loss:2.4659, Accuracy:0.1729, Validation Loss:2.4190, Validation Accuracy:0.1806\n",
    "Epoch #49: Loss:2.4636, Accuracy:0.1729, Validation Loss:2.4188, Validation Accuracy:0.1790\n",
    "Epoch #50: Loss:2.4618, Accuracy:0.1758, Validation Loss:2.4197, Validation Accuracy:0.1790\n",
    "Epoch #51: Loss:2.4604, Accuracy:0.1688, Validation Loss:2.4154, Validation Accuracy:0.1806\n",
    "Epoch #52: Loss:2.4585, Accuracy:0.1737, Validation Loss:2.4195, Validation Accuracy:0.1790\n",
    "Epoch #53: Loss:2.4597, Accuracy:0.1704, Validation Loss:2.4133, Validation Accuracy:0.1806\n",
    "Epoch #54: Loss:2.4587, Accuracy:0.1737, Validation Loss:2.4175, Validation Accuracy:0.1839\n",
    "Epoch #55: Loss:2.4573, Accuracy:0.1729, Validation Loss:2.4148, Validation Accuracy:0.1806\n",
    "Epoch #56: Loss:2.4573, Accuracy:0.1749, Validation Loss:2.4162, Validation Accuracy:0.1856\n",
    "Epoch #57: Loss:2.4558, Accuracy:0.1758, Validation Loss:2.4141, Validation Accuracy:0.1806\n",
    "Epoch #58: Loss:2.4579, Accuracy:0.1758, Validation Loss:2.4167, Validation Accuracy:0.1839\n",
    "Epoch #59: Loss:2.4581, Accuracy:0.1704, Validation Loss:2.4142, Validation Accuracy:0.1790\n",
    "Epoch #60: Loss:2.4568, Accuracy:0.1774, Validation Loss:2.4192, Validation Accuracy:0.1806\n",
    "Epoch #61: Loss:2.4552, Accuracy:0.1737, Validation Loss:2.4122, Validation Accuracy:0.1806\n",
    "Epoch #62: Loss:2.4557, Accuracy:0.1791, Validation Loss:2.4153, Validation Accuracy:0.1757\n",
    "Epoch #63: Loss:2.4557, Accuracy:0.1758, Validation Loss:2.4129, Validation Accuracy:0.1741\n",
    "Epoch #64: Loss:2.4567, Accuracy:0.1766, Validation Loss:2.4134, Validation Accuracy:0.1757\n",
    "Epoch #65: Loss:2.4582, Accuracy:0.1770, Validation Loss:2.4152, Validation Accuracy:0.1741\n",
    "Epoch #66: Loss:2.4582, Accuracy:0.1762, Validation Loss:2.4148, Validation Accuracy:0.1724\n",
    "Epoch #67: Loss:2.4556, Accuracy:0.1754, Validation Loss:2.4186, Validation Accuracy:0.1839\n",
    "Epoch #68: Loss:2.4576, Accuracy:0.1758, Validation Loss:2.4152, Validation Accuracy:0.1757\n",
    "Epoch #69: Loss:2.4574, Accuracy:0.1762, Validation Loss:2.4175, Validation Accuracy:0.1757\n",
    "Epoch #70: Loss:2.4551, Accuracy:0.1749, Validation Loss:2.4149, Validation Accuracy:0.1724\n",
    "Epoch #71: Loss:2.4548, Accuracy:0.1725, Validation Loss:2.4208, Validation Accuracy:0.1823\n",
    "Epoch #72: Loss:2.4559, Accuracy:0.1745, Validation Loss:2.4147, Validation Accuracy:0.1724\n",
    "Epoch #73: Loss:2.4551, Accuracy:0.1741, Validation Loss:2.4177, Validation Accuracy:0.1823\n",
    "Epoch #74: Loss:2.4540, Accuracy:0.1758, Validation Loss:2.4166, Validation Accuracy:0.1757\n",
    "Epoch #75: Loss:2.4548, Accuracy:0.1758, Validation Loss:2.4155, Validation Accuracy:0.1757\n",
    "Epoch #76: Loss:2.4550, Accuracy:0.1745, Validation Loss:2.4179, Validation Accuracy:0.1839\n",
    "Epoch #77: Loss:2.4543, Accuracy:0.1737, Validation Loss:2.4143, Validation Accuracy:0.1741\n",
    "Epoch #78: Loss:2.4546, Accuracy:0.1745, Validation Loss:2.4147, Validation Accuracy:0.1741\n",
    "Epoch #79: Loss:2.4548, Accuracy:0.1737, Validation Loss:2.4136, Validation Accuracy:0.1757\n",
    "Epoch #80: Loss:2.4540, Accuracy:0.1749, Validation Loss:2.4135, Validation Accuracy:0.1741\n",
    "Epoch #81: Loss:2.4534, Accuracy:0.1708, Validation Loss:2.4172, Validation Accuracy:0.1872\n",
    "Epoch #82: Loss:2.4543, Accuracy:0.1717, Validation Loss:2.4136, Validation Accuracy:0.1741\n",
    "Epoch #83: Loss:2.4530, Accuracy:0.1754, Validation Loss:2.4152, Validation Accuracy:0.1757\n",
    "Epoch #84: Loss:2.4536, Accuracy:0.1749, Validation Loss:2.4130, Validation Accuracy:0.1741\n",
    "Epoch #85: Loss:2.4527, Accuracy:0.1749, Validation Loss:2.4139, Validation Accuracy:0.1741\n",
    "Epoch #86: Loss:2.4522, Accuracy:0.1754, Validation Loss:2.4144, Validation Accuracy:0.1757\n",
    "Epoch #87: Loss:2.4523, Accuracy:0.1749, Validation Loss:2.4133, Validation Accuracy:0.1757\n",
    "Epoch #88: Loss:2.4518, Accuracy:0.1758, Validation Loss:2.4141, Validation Accuracy:0.1741\n",
    "Epoch #89: Loss:2.4529, Accuracy:0.1737, Validation Loss:2.4138, Validation Accuracy:0.1741\n",
    "Epoch #90: Loss:2.4531, Accuracy:0.1733, Validation Loss:2.4144, Validation Accuracy:0.1757\n",
    "Epoch #91: Loss:2.4510, Accuracy:0.1758, Validation Loss:2.4131, Validation Accuracy:0.1741\n",
    "Epoch #92: Loss:2.4519, Accuracy:0.1758, Validation Loss:2.4151, Validation Accuracy:0.1741\n",
    "Epoch #93: Loss:2.4518, Accuracy:0.1749, Validation Loss:2.4127, Validation Accuracy:0.1741\n",
    "Epoch #94: Loss:2.4507, Accuracy:0.1754, Validation Loss:2.4127, Validation Accuracy:0.1757\n",
    "Epoch #95: Loss:2.4510, Accuracy:0.1758, Validation Loss:2.4120, Validation Accuracy:0.1757\n",
    "Epoch #96: Loss:2.4515, Accuracy:0.1754, Validation Loss:2.4119, Validation Accuracy:0.1741\n",
    "Epoch #97: Loss:2.4519, Accuracy:0.1762, Validation Loss:2.4135, Validation Accuracy:0.1757\n",
    "Epoch #98: Loss:2.4513, Accuracy:0.1717, Validation Loss:2.4145, Validation Accuracy:0.1856\n",
    "Epoch #99: Loss:2.4506, Accuracy:0.1766, Validation Loss:2.4141, Validation Accuracy:0.1741\n",
    "Epoch #100: Loss:2.4505, Accuracy:0.1741, Validation Loss:2.4133, Validation Accuracy:0.1757\n",
    "Epoch #101: Loss:2.4520, Accuracy:0.1758, Validation Loss:2.4175, Validation Accuracy:0.1839\n",
    "Epoch #102: Loss:2.4542, Accuracy:0.1754, Validation Loss:2.4141, Validation Accuracy:0.1741\n",
    "Epoch #103: Loss:2.4538, Accuracy:0.1762, Validation Loss:2.4139, Validation Accuracy:0.1757\n",
    "Epoch #104: Loss:2.4520, Accuracy:0.1774, Validation Loss:2.4161, Validation Accuracy:0.1741\n",
    "Epoch #105: Loss:2.4512, Accuracy:0.1758, Validation Loss:2.4134, Validation Accuracy:0.1741\n",
    "Epoch #106: Loss:2.4499, Accuracy:0.1741, Validation Loss:2.4158, Validation Accuracy:0.1823\n",
    "Epoch #107: Loss:2.4507, Accuracy:0.1749, Validation Loss:2.4132, Validation Accuracy:0.1757\n",
    "Epoch #108: Loss:2.4490, Accuracy:0.1758, Validation Loss:2.4155, Validation Accuracy:0.1839\n",
    "Epoch #109: Loss:2.4504, Accuracy:0.1733, Validation Loss:2.4128, Validation Accuracy:0.1757\n",
    "Epoch #110: Loss:2.4497, Accuracy:0.1758, Validation Loss:2.4136, Validation Accuracy:0.1757\n",
    "Epoch #111: Loss:2.4499, Accuracy:0.1754, Validation Loss:2.4141, Validation Accuracy:0.1872\n",
    "Epoch #112: Loss:2.4495, Accuracy:0.1741, Validation Loss:2.4132, Validation Accuracy:0.1872\n",
    "Epoch #113: Loss:2.4494, Accuracy:0.1733, Validation Loss:2.4147, Validation Accuracy:0.1823\n",
    "Epoch #114: Loss:2.4490, Accuracy:0.1754, Validation Loss:2.4126, Validation Accuracy:0.1757\n",
    "Epoch #115: Loss:2.4506, Accuracy:0.1762, Validation Loss:2.4129, Validation Accuracy:0.1757\n",
    "Epoch #116: Loss:2.4494, Accuracy:0.1725, Validation Loss:2.4142, Validation Accuracy:0.1839\n",
    "Epoch #117: Loss:2.4515, Accuracy:0.1758, Validation Loss:2.4137, Validation Accuracy:0.1741\n",
    "Epoch #118: Loss:2.4546, Accuracy:0.1737, Validation Loss:2.4158, Validation Accuracy:0.1856\n",
    "Epoch #119: Loss:2.4512, Accuracy:0.1766, Validation Loss:2.4159, Validation Accuracy:0.1741\n",
    "Epoch #120: Loss:2.4529, Accuracy:0.1749, Validation Loss:2.4180, Validation Accuracy:0.1839\n",
    "Epoch #121: Loss:2.4494, Accuracy:0.1745, Validation Loss:2.4148, Validation Accuracy:0.1724\n",
    "Epoch #122: Loss:2.4548, Accuracy:0.1721, Validation Loss:2.4168, Validation Accuracy:0.1806\n",
    "Epoch #123: Loss:2.4505, Accuracy:0.1745, Validation Loss:2.4137, Validation Accuracy:0.1823\n",
    "Epoch #124: Loss:2.4506, Accuracy:0.1745, Validation Loss:2.4151, Validation Accuracy:0.1856\n",
    "Epoch #125: Loss:2.4496, Accuracy:0.1762, Validation Loss:2.4134, Validation Accuracy:0.1856\n",
    "Epoch #126: Loss:2.4488, Accuracy:0.1758, Validation Loss:2.4146, Validation Accuracy:0.1823\n",
    "Epoch #127: Loss:2.4490, Accuracy:0.1749, Validation Loss:2.4142, Validation Accuracy:0.1806\n",
    "Epoch #128: Loss:2.4493, Accuracy:0.1745, Validation Loss:2.4149, Validation Accuracy:0.1806\n",
    "Epoch #129: Loss:2.4497, Accuracy:0.1745, Validation Loss:2.4149, Validation Accuracy:0.1806\n",
    "Epoch #130: Loss:2.4495, Accuracy:0.1725, Validation Loss:2.4142, Validation Accuracy:0.1806\n",
    "Epoch #131: Loss:2.4489, Accuracy:0.1754, Validation Loss:2.4160, Validation Accuracy:0.1806\n",
    "Epoch #132: Loss:2.4505, Accuracy:0.1749, Validation Loss:2.4149, Validation Accuracy:0.1823\n",
    "Epoch #133: Loss:2.4490, Accuracy:0.1749, Validation Loss:2.4154, Validation Accuracy:0.1823\n",
    "Epoch #134: Loss:2.4485, Accuracy:0.1749, Validation Loss:2.4141, Validation Accuracy:0.1806\n",
    "Epoch #135: Loss:2.4484, Accuracy:0.1745, Validation Loss:2.4141, Validation Accuracy:0.1806\n",
    "Epoch #136: Loss:2.4485, Accuracy:0.1741, Validation Loss:2.4192, Validation Accuracy:0.1757\n",
    "Epoch #137: Loss:2.4481, Accuracy:0.1758, Validation Loss:2.4190, Validation Accuracy:0.1773\n",
    "Epoch #138: Loss:2.4481, Accuracy:0.1754, Validation Loss:2.4187, Validation Accuracy:0.1773\n",
    "Epoch #139: Loss:2.4509, Accuracy:0.1749, Validation Loss:2.4288, Validation Accuracy:0.1839\n",
    "Epoch #140: Loss:2.4544, Accuracy:0.1762, Validation Loss:2.4217, Validation Accuracy:0.1790\n",
    "Epoch #141: Loss:2.4518, Accuracy:0.1733, Validation Loss:2.4256, Validation Accuracy:0.1724\n",
    "Epoch #142: Loss:2.4554, Accuracy:0.1725, Validation Loss:2.4174, Validation Accuracy:0.1806\n",
    "Epoch #143: Loss:2.4690, Accuracy:0.1725, Validation Loss:2.4382, Validation Accuracy:0.1872\n",
    "Epoch #144: Loss:2.4701, Accuracy:0.1671, Validation Loss:2.4492, Validation Accuracy:0.1938\n",
    "Epoch #145: Loss:2.4615, Accuracy:0.1717, Validation Loss:2.4337, Validation Accuracy:0.1724\n",
    "Epoch #146: Loss:2.4580, Accuracy:0.1713, Validation Loss:2.4296, Validation Accuracy:0.1921\n",
    "Epoch #147: Loss:2.4552, Accuracy:0.1708, Validation Loss:2.4180, Validation Accuracy:0.1741\n",
    "Epoch #148: Loss:2.4551, Accuracy:0.1721, Validation Loss:2.4179, Validation Accuracy:0.1724\n",
    "Epoch #149: Loss:2.4551, Accuracy:0.1766, Validation Loss:2.4182, Validation Accuracy:0.1741\n",
    "Epoch #150: Loss:2.4545, Accuracy:0.1704, Validation Loss:2.4173, Validation Accuracy:0.1724\n",
    "Epoch #151: Loss:2.4501, Accuracy:0.1754, Validation Loss:2.4224, Validation Accuracy:0.1741\n",
    "Epoch #152: Loss:2.4511, Accuracy:0.1758, Validation Loss:2.4155, Validation Accuracy:0.1773\n",
    "Epoch #153: Loss:2.4511, Accuracy:0.1725, Validation Loss:2.4131, Validation Accuracy:0.1773\n",
    "Epoch #154: Loss:2.4538, Accuracy:0.1741, Validation Loss:2.4137, Validation Accuracy:0.1773\n",
    "Epoch #155: Loss:2.4563, Accuracy:0.1770, Validation Loss:2.4136, Validation Accuracy:0.1790\n",
    "Epoch #156: Loss:2.4578, Accuracy:0.1733, Validation Loss:2.4152, Validation Accuracy:0.1790\n",
    "Epoch #157: Loss:2.4539, Accuracy:0.1749, Validation Loss:2.4148, Validation Accuracy:0.1790\n",
    "Epoch #158: Loss:2.4563, Accuracy:0.1708, Validation Loss:2.4170, Validation Accuracy:0.1790\n",
    "Epoch #159: Loss:2.4548, Accuracy:0.1729, Validation Loss:2.4135, Validation Accuracy:0.1790\n",
    "Epoch #160: Loss:2.4516, Accuracy:0.1721, Validation Loss:2.4142, Validation Accuracy:0.1790\n",
    "Epoch #161: Loss:2.4557, Accuracy:0.1717, Validation Loss:2.4120, Validation Accuracy:0.1773\n",
    "Epoch #162: Loss:2.4562, Accuracy:0.1708, Validation Loss:2.4119, Validation Accuracy:0.1773\n",
    "Epoch #163: Loss:2.4532, Accuracy:0.1737, Validation Loss:2.4149, Validation Accuracy:0.1773\n",
    "Epoch #164: Loss:2.4524, Accuracy:0.1737, Validation Loss:2.4148, Validation Accuracy:0.1806\n",
    "Epoch #165: Loss:2.4526, Accuracy:0.1729, Validation Loss:2.4136, Validation Accuracy:0.1790\n",
    "Epoch #166: Loss:2.4529, Accuracy:0.1745, Validation Loss:2.4122, Validation Accuracy:0.1773\n",
    "Epoch #167: Loss:2.4519, Accuracy:0.1696, Validation Loss:2.4120, Validation Accuracy:0.1773\n",
    "Epoch #168: Loss:2.4524, Accuracy:0.1737, Validation Loss:2.4122, Validation Accuracy:0.1773\n",
    "Epoch #169: Loss:2.4529, Accuracy:0.1692, Validation Loss:2.4115, Validation Accuracy:0.1790\n",
    "Epoch #170: Loss:2.4511, Accuracy:0.1741, Validation Loss:2.4118, Validation Accuracy:0.1773\n",
    "Epoch #171: Loss:2.4514, Accuracy:0.1741, Validation Loss:2.4109, Validation Accuracy:0.1773\n",
    "Epoch #172: Loss:2.4516, Accuracy:0.1745, Validation Loss:2.4116, Validation Accuracy:0.1790\n",
    "Epoch #173: Loss:2.4515, Accuracy:0.1729, Validation Loss:2.4109, Validation Accuracy:0.1773\n",
    "Epoch #174: Loss:2.4510, Accuracy:0.1737, Validation Loss:2.4114, Validation Accuracy:0.1773\n",
    "Epoch #175: Loss:2.4508, Accuracy:0.1737, Validation Loss:2.4111, Validation Accuracy:0.1773\n",
    "Epoch #176: Loss:2.4511, Accuracy:0.1741, Validation Loss:2.4111, Validation Accuracy:0.1790\n",
    "Epoch #177: Loss:2.4505, Accuracy:0.1741, Validation Loss:2.4107, Validation Accuracy:0.1790\n",
    "Epoch #178: Loss:2.4507, Accuracy:0.1741, Validation Loss:2.4109, Validation Accuracy:0.1773\n",
    "Epoch #179: Loss:2.4503, Accuracy:0.1737, Validation Loss:2.4112, Validation Accuracy:0.1773\n",
    "Epoch #180: Loss:2.4509, Accuracy:0.1745, Validation Loss:2.4111, Validation Accuracy:0.1790\n",
    "Epoch #181: Loss:2.4510, Accuracy:0.1741, Validation Loss:2.4105, Validation Accuracy:0.1773\n",
    "Epoch #182: Loss:2.4504, Accuracy:0.1741, Validation Loss:2.4104, Validation Accuracy:0.1773\n",
    "Epoch #183: Loss:2.4503, Accuracy:0.1737, Validation Loss:2.4108, Validation Accuracy:0.1790\n",
    "Epoch #184: Loss:2.4510, Accuracy:0.1737, Validation Loss:2.4102, Validation Accuracy:0.1773\n",
    "Epoch #185: Loss:2.4505, Accuracy:0.1737, Validation Loss:2.4103, Validation Accuracy:0.1790\n",
    "Epoch #186: Loss:2.4500, Accuracy:0.1741, Validation Loss:2.4102, Validation Accuracy:0.1790\n",
    "Epoch #187: Loss:2.4502, Accuracy:0.1745, Validation Loss:2.4100, Validation Accuracy:0.1790\n",
    "Epoch #188: Loss:2.4498, Accuracy:0.1745, Validation Loss:2.4107, Validation Accuracy:0.1773\n",
    "Epoch #189: Loss:2.4500, Accuracy:0.1737, Validation Loss:2.4106, Validation Accuracy:0.1773\n",
    "Epoch #190: Loss:2.4498, Accuracy:0.1737, Validation Loss:2.4100, Validation Accuracy:0.1790\n",
    "Epoch #191: Loss:2.4500, Accuracy:0.1737, Validation Loss:2.4096, Validation Accuracy:0.1790\n",
    "Epoch #192: Loss:2.4506, Accuracy:0.1774, Validation Loss:2.4093, Validation Accuracy:0.1790\n",
    "Epoch #193: Loss:2.4502, Accuracy:0.1741, Validation Loss:2.4098, Validation Accuracy:0.1790\n",
    "Epoch #194: Loss:2.4518, Accuracy:0.1708, Validation Loss:2.4109, Validation Accuracy:0.1790\n",
    "Epoch #195: Loss:2.4494, Accuracy:0.1745, Validation Loss:2.4116, Validation Accuracy:0.1790\n",
    "Epoch #196: Loss:2.4508, Accuracy:0.1737, Validation Loss:2.4105, Validation Accuracy:0.1790\n",
    "Epoch #197: Loss:2.4498, Accuracy:0.1778, Validation Loss:2.4096, Validation Accuracy:0.1790\n",
    "Epoch #198: Loss:2.4498, Accuracy:0.1741, Validation Loss:2.4096, Validation Accuracy:0.1790\n",
    "Epoch #199: Loss:2.4494, Accuracy:0.1741, Validation Loss:2.4101, Validation Accuracy:0.1790\n",
    "Epoch #200: Loss:2.4497, Accuracy:0.1741, Validation Loss:2.4096, Validation Accuracy:0.1790\n",
    "Epoch #201: Loss:2.4496, Accuracy:0.1762, Validation Loss:2.4095, Validation Accuracy:0.1790\n",
    "Epoch #202: Loss:2.4510, Accuracy:0.1696, Validation Loss:2.4106, Validation Accuracy:0.1806\n",
    "Epoch #203: Loss:2.4569, Accuracy:0.1692, Validation Loss:2.4090, Validation Accuracy:0.1806\n",
    "Epoch #204: Loss:2.4533, Accuracy:0.1754, Validation Loss:2.4092, Validation Accuracy:0.1839\n",
    "Epoch #205: Loss:2.4530, Accuracy:0.1770, Validation Loss:2.4122, Validation Accuracy:0.1806\n",
    "Epoch #206: Loss:2.4512, Accuracy:0.1721, Validation Loss:2.4124, Validation Accuracy:0.1741\n",
    "Epoch #207: Loss:2.4496, Accuracy:0.1725, Validation Loss:2.4113, Validation Accuracy:0.1773\n",
    "Epoch #208: Loss:2.4497, Accuracy:0.1749, Validation Loss:2.4092, Validation Accuracy:0.1790\n",
    "Epoch #209: Loss:2.4507, Accuracy:0.1721, Validation Loss:2.4089, Validation Accuracy:0.1790\n",
    "Epoch #210: Loss:2.4530, Accuracy:0.1741, Validation Loss:2.4100, Validation Accuracy:0.1806\n",
    "Epoch #211: Loss:2.4519, Accuracy:0.1737, Validation Loss:2.4095, Validation Accuracy:0.1806\n",
    "Epoch #212: Loss:2.4498, Accuracy:0.1741, Validation Loss:2.4117, Validation Accuracy:0.1806\n",
    "Epoch #213: Loss:2.4491, Accuracy:0.1766, Validation Loss:2.4104, Validation Accuracy:0.1790\n",
    "Epoch #214: Loss:2.4504, Accuracy:0.1708, Validation Loss:2.4095, Validation Accuracy:0.1806\n",
    "Epoch #215: Loss:2.4503, Accuracy:0.1770, Validation Loss:2.4094, Validation Accuracy:0.1773\n",
    "Epoch #216: Loss:2.4499, Accuracy:0.1704, Validation Loss:2.4090, Validation Accuracy:0.1790\n",
    "Epoch #217: Loss:2.4496, Accuracy:0.1758, Validation Loss:2.4095, Validation Accuracy:0.1790\n",
    "Epoch #218: Loss:2.4501, Accuracy:0.1700, Validation Loss:2.4093, Validation Accuracy:0.1823\n",
    "Epoch #219: Loss:2.4495, Accuracy:0.1778, Validation Loss:2.4102, Validation Accuracy:0.1806\n",
    "Epoch #220: Loss:2.4482, Accuracy:0.1770, Validation Loss:2.4098, Validation Accuracy:0.1790\n",
    "Epoch #221: Loss:2.4493, Accuracy:0.1729, Validation Loss:2.4089, Validation Accuracy:0.1806\n",
    "Epoch #222: Loss:2.4496, Accuracy:0.1782, Validation Loss:2.4085, Validation Accuracy:0.1823\n",
    "Epoch #223: Loss:2.4489, Accuracy:0.1708, Validation Loss:2.4089, Validation Accuracy:0.1823\n",
    "Epoch #224: Loss:2.4487, Accuracy:0.1778, Validation Loss:2.4100, Validation Accuracy:0.1823\n",
    "Epoch #225: Loss:2.4492, Accuracy:0.1774, Validation Loss:2.4093, Validation Accuracy:0.1790\n",
    "Epoch #226: Loss:2.4491, Accuracy:0.1774, Validation Loss:2.4097, Validation Accuracy:0.1806\n",
    "Epoch #227: Loss:2.4484, Accuracy:0.1786, Validation Loss:2.4095, Validation Accuracy:0.1790\n",
    "Epoch #228: Loss:2.4497, Accuracy:0.1795, Validation Loss:2.4094, Validation Accuracy:0.1806\n",
    "Epoch #229: Loss:2.4481, Accuracy:0.1774, Validation Loss:2.4087, Validation Accuracy:0.1806\n",
    "Epoch #230: Loss:2.4489, Accuracy:0.1704, Validation Loss:2.4085, Validation Accuracy:0.1790\n",
    "Epoch #231: Loss:2.4489, Accuracy:0.1774, Validation Loss:2.4089, Validation Accuracy:0.1806\n",
    "Epoch #232: Loss:2.4503, Accuracy:0.1737, Validation Loss:2.4094, Validation Accuracy:0.1823\n",
    "Epoch #233: Loss:2.4485, Accuracy:0.1778, Validation Loss:2.4101, Validation Accuracy:0.1806\n",
    "Epoch #234: Loss:2.4519, Accuracy:0.1729, Validation Loss:2.4092, Validation Accuracy:0.1806\n",
    "Epoch #235: Loss:2.4492, Accuracy:0.1774, Validation Loss:2.4108, Validation Accuracy:0.1823\n",
    "Epoch #236: Loss:2.4491, Accuracy:0.1745, Validation Loss:2.4099, Validation Accuracy:0.1757\n",
    "Epoch #237: Loss:2.4495, Accuracy:0.1774, Validation Loss:2.4103, Validation Accuracy:0.1806\n",
    "Epoch #238: Loss:2.4480, Accuracy:0.1782, Validation Loss:2.4091, Validation Accuracy:0.1790\n",
    "Epoch #239: Loss:2.4480, Accuracy:0.1774, Validation Loss:2.4085, Validation Accuracy:0.1806\n",
    "Epoch #240: Loss:2.4475, Accuracy:0.1770, Validation Loss:2.4089, Validation Accuracy:0.1823\n",
    "Epoch #241: Loss:2.4476, Accuracy:0.1786, Validation Loss:2.4085, Validation Accuracy:0.1823\n",
    "Epoch #242: Loss:2.4486, Accuracy:0.1758, Validation Loss:2.4079, Validation Accuracy:0.1806\n",
    "Epoch #243: Loss:2.4481, Accuracy:0.1778, Validation Loss:2.4090, Validation Accuracy:0.1790\n",
    "Epoch #244: Loss:2.4473, Accuracy:0.1786, Validation Loss:2.4085, Validation Accuracy:0.1806\n",
    "Epoch #245: Loss:2.4475, Accuracy:0.1791, Validation Loss:2.4091, Validation Accuracy:0.1806\n",
    "Epoch #246: Loss:2.4480, Accuracy:0.1778, Validation Loss:2.4091, Validation Accuracy:0.1823\n",
    "Epoch #247: Loss:2.4496, Accuracy:0.1729, Validation Loss:2.4089, Validation Accuracy:0.1823\n",
    "Epoch #248: Loss:2.4489, Accuracy:0.1737, Validation Loss:2.4095, Validation Accuracy:0.1823\n",
    "Epoch #249: Loss:2.4481, Accuracy:0.1799, Validation Loss:2.4087, Validation Accuracy:0.1757\n",
    "Epoch #250: Loss:2.4479, Accuracy:0.1786, Validation Loss:2.4097, Validation Accuracy:0.1823\n",
    "Epoch #251: Loss:2.4474, Accuracy:0.1782, Validation Loss:2.4097, Validation Accuracy:0.1790\n",
    "Epoch #252: Loss:2.4481, Accuracy:0.1782, Validation Loss:2.4088, Validation Accuracy:0.1823\n",
    "Epoch #253: Loss:2.4471, Accuracy:0.1782, Validation Loss:2.4090, Validation Accuracy:0.1823\n",
    "Epoch #254: Loss:2.4474, Accuracy:0.1782, Validation Loss:2.4078, Validation Accuracy:0.1806\n",
    "Epoch #255: Loss:2.4470, Accuracy:0.1782, Validation Loss:2.4082, Validation Accuracy:0.1839\n",
    "Epoch #256: Loss:2.4470, Accuracy:0.1791, Validation Loss:2.4084, Validation Accuracy:0.1823\n",
    "Epoch #257: Loss:2.4471, Accuracy:0.1782, Validation Loss:2.4086, Validation Accuracy:0.1823\n",
    "Epoch #258: Loss:2.4470, Accuracy:0.1778, Validation Loss:2.4095, Validation Accuracy:0.1823\n",
    "Epoch #259: Loss:2.4471, Accuracy:0.1778, Validation Loss:2.4083, Validation Accuracy:0.1806\n",
    "Epoch #260: Loss:2.4470, Accuracy:0.1786, Validation Loss:2.4082, Validation Accuracy:0.1823\n",
    "Epoch #261: Loss:2.4470, Accuracy:0.1774, Validation Loss:2.4087, Validation Accuracy:0.1823\n",
    "Epoch #262: Loss:2.4469, Accuracy:0.1774, Validation Loss:2.4086, Validation Accuracy:0.1806\n",
    "Epoch #263: Loss:2.4467, Accuracy:0.1786, Validation Loss:2.4086, Validation Accuracy:0.1823\n",
    "Epoch #264: Loss:2.4466, Accuracy:0.1786, Validation Loss:2.4083, Validation Accuracy:0.1823\n",
    "Epoch #265: Loss:2.4468, Accuracy:0.1786, Validation Loss:2.4083, Validation Accuracy:0.1806\n",
    "Epoch #266: Loss:2.4468, Accuracy:0.1782, Validation Loss:2.4092, Validation Accuracy:0.1806\n",
    "Epoch #267: Loss:2.4475, Accuracy:0.1778, Validation Loss:2.4078, Validation Accuracy:0.1806\n",
    "Epoch #268: Loss:2.4496, Accuracy:0.1791, Validation Loss:2.4081, Validation Accuracy:0.1839\n",
    "Epoch #269: Loss:2.4480, Accuracy:0.1791, Validation Loss:2.4081, Validation Accuracy:0.1806\n",
    "Epoch #270: Loss:2.4474, Accuracy:0.1782, Validation Loss:2.4090, Validation Accuracy:0.1823\n",
    "Epoch #271: Loss:2.4476, Accuracy:0.1791, Validation Loss:2.4080, Validation Accuracy:0.1757\n",
    "Epoch #272: Loss:2.4466, Accuracy:0.1791, Validation Loss:2.4085, Validation Accuracy:0.1839\n",
    "Epoch #273: Loss:2.4465, Accuracy:0.1782, Validation Loss:2.4087, Validation Accuracy:0.1806\n",
    "Epoch #274: Loss:2.4463, Accuracy:0.1778, Validation Loss:2.4092, Validation Accuracy:0.1806\n",
    "Epoch #275: Loss:2.4470, Accuracy:0.1782, Validation Loss:2.4089, Validation Accuracy:0.1823\n",
    "Epoch #276: Loss:2.4465, Accuracy:0.1782, Validation Loss:2.4078, Validation Accuracy:0.1773\n",
    "Epoch #277: Loss:2.4467, Accuracy:0.1782, Validation Loss:2.4079, Validation Accuracy:0.1823\n",
    "Epoch #278: Loss:2.4461, Accuracy:0.1791, Validation Loss:2.4074, Validation Accuracy:0.1823\n",
    "Epoch #279: Loss:2.4467, Accuracy:0.1774, Validation Loss:2.4091, Validation Accuracy:0.1806\n",
    "Epoch #280: Loss:2.4464, Accuracy:0.1782, Validation Loss:2.4084, Validation Accuracy:0.1806\n",
    "Epoch #281: Loss:2.4459, Accuracy:0.1778, Validation Loss:2.4079, Validation Accuracy:0.1773\n",
    "Epoch #282: Loss:2.4464, Accuracy:0.1778, Validation Loss:2.4079, Validation Accuracy:0.1823\n",
    "Epoch #283: Loss:2.4459, Accuracy:0.1782, Validation Loss:2.4088, Validation Accuracy:0.1823\n",
    "Epoch #284: Loss:2.4468, Accuracy:0.1782, Validation Loss:2.4081, Validation Accuracy:0.1823\n",
    "Epoch #285: Loss:2.4479, Accuracy:0.1799, Validation Loss:2.4083, Validation Accuracy:0.1806\n",
    "Epoch #286: Loss:2.4464, Accuracy:0.1754, Validation Loss:2.4101, Validation Accuracy:0.1806\n",
    "Epoch #287: Loss:2.4470, Accuracy:0.1791, Validation Loss:2.4088, Validation Accuracy:0.1790\n",
    "Epoch #288: Loss:2.4473, Accuracy:0.1782, Validation Loss:2.4105, Validation Accuracy:0.1823\n",
    "Epoch #289: Loss:2.4458, Accuracy:0.1791, Validation Loss:2.4088, Validation Accuracy:0.1806\n",
    "Epoch #290: Loss:2.4461, Accuracy:0.1786, Validation Loss:2.4077, Validation Accuracy:0.1823\n",
    "Epoch #291: Loss:2.4456, Accuracy:0.1782, Validation Loss:2.4080, Validation Accuracy:0.1839\n",
    "Epoch #292: Loss:2.4455, Accuracy:0.1786, Validation Loss:2.4078, Validation Accuracy:0.1806\n",
    "Epoch #293: Loss:2.4455, Accuracy:0.1795, Validation Loss:2.4084, Validation Accuracy:0.1823\n",
    "Epoch #294: Loss:2.4459, Accuracy:0.1782, Validation Loss:2.4085, Validation Accuracy:0.1806\n",
    "Epoch #295: Loss:2.4458, Accuracy:0.1786, Validation Loss:2.4082, Validation Accuracy:0.1806\n",
    "Epoch #296: Loss:2.4457, Accuracy:0.1782, Validation Loss:2.4083, Validation Accuracy:0.1806\n",
    "Epoch #297: Loss:2.4453, Accuracy:0.1782, Validation Loss:2.4086, Validation Accuracy:0.1823\n",
    "Epoch #298: Loss:2.4457, Accuracy:0.1782, Validation Loss:2.4076, Validation Accuracy:0.1823\n",
    "Epoch #299: Loss:2.4457, Accuracy:0.1786, Validation Loss:2.4074, Validation Accuracy:0.1823\n",
    "Epoch #300: Loss:2.4455, Accuracy:0.1778, Validation Loss:2.4092, Validation Accuracy:0.1823\n",
    "\n",
    "Test:\n",
    "Test Loss:2.40923548, Accuracy:0.1823\n",
    "Labels: ['aa', 'yd', 'ck', 'eo', 'ek', 'ds', 'sg', 'eb', 'ib', 'ce', 'eg', 'my', 'mb', 'sk', 'by']\n",
    "Confusion Matrix:\n",
    "      aa  yd  ck  eo  ek  ds  sg  eb  ib  ce  eg  my  mb  sk  by\n",
    "t:aa   0   3   0   0   0   4   4   0   0   0  23   0   0   0   0\n",
    "t:yd   0  33   0   0   0   0  27   0   0   0   2   0   0   0   0\n",
    "t:ck   0   0   0   0   0   3  11   0   0   0   9   0   0   0   0\n",
    "t:eo   0   2   0   0   0   0  25   0   0   0   7   0   0   0   0\n",
    "t:ek   0   3   0   0   0   0  17   0   0   0  28   0   0   0   0\n",
    "t:ds   0   0   0   0   0   7   5   1   0   0  16   0   0   0   2\n",
    "t:sg   0   9   0   0   0   0  33   1   0   0   5   0   0   0   3\n",
    "t:eb   0   6   0   0   0   0  18   1   0   0  24   0   0   0   1\n",
    "t:ib   0  30   0   0   0   0  19   0   0   0   4   0   0   0   1\n",
    "t:ce   0   2   0   0   0   1  13   0   0   0  11   0   0   0   0\n",
    "t:eg   0   2   0   0   0   6   6   0   0   0  36   0   0   0   0\n",
    "t:my   0   6   0   0   0   1   3   0   0   0   9   0   0   0   1\n",
    "t:mb   0   7   0   0   0   3  30   0   0   0  12   0   0   0   0\n",
    "t:sk   0   1   0   0   0   5  13   0   0   0  13   0   0   0   1\n",
    "t:by   0   1   0   0   0   0  24   0   0   0  14   0   0   0   1\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          yd       0.31      0.53      0.40        62\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          ds       0.23      0.23      0.23        31\n",
    "          sg       0.13      0.65      0.22        51\n",
    "          eb       0.33      0.02      0.04        50\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          eg       0.17      0.72      0.27        50\n",
    "          my       0.00      0.00      0.00        20\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          by       0.10      0.03      0.04        40\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.09      0.14      0.08       609\n",
    "weighted avg       0.10      0.18      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 15:25:57 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 41 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6937760441565555, 2.6846690831708986, 2.6763348802557134, 2.6691419017334486, 2.6670621956510496, 2.6628793421245756, 2.6573665780191154, 2.652077420004483, 2.645426746659678, 2.638367217554052, 2.630158086128423, 2.6194531839273636, 2.605686020968583, 2.58948054102254, 2.5723293156459413, 2.5534403927024756, 2.5474326328691004, 2.5692083714239313, 2.5474185610835383, 2.5171559726076174, 2.506522165339177, 2.4866979764208614, 2.4795652731690305, 2.4652101331939447, 2.4609757812543847, 2.4531444418802244, 2.447620724222343, 2.441505107190613, 2.444271580926303, 2.4384754440075853, 2.4380092953617742, 2.4345795904670053, 2.4342026863192103, 2.430127339997315, 2.43382995351782, 2.4275490462486378, 2.435873334239465, 2.426543901315072, 2.426393261096748, 2.4222590868500458, 2.4272954401320033, 2.420559751576391, 2.4227688069805526, 2.4192291715462217, 2.425402000815606, 2.4187222619362063, 2.4275863495562073, 2.419008910753848, 2.418815425268339, 2.41968693795854, 2.415392035921219, 2.4195425561300445, 2.413313937696134, 2.4174835889406, 2.414845560963322, 2.416157080817888, 2.4141061204211858, 2.416650166456727, 2.414198491765165, 2.419210892396999, 2.4121676959427707, 2.4153420275263793, 2.4129326453154114, 2.4133787523153774, 2.4152226726018347, 2.4148113406546208, 2.4185830470180667, 2.4152351142150428, 2.4175093134831522, 2.41487807358427, 2.4208308378072405, 2.414737353379699, 2.4177059614403884, 2.416561583188563, 2.4154717851742147, 2.4178513634968275, 2.4142928886883364, 2.4147074390906225, 2.413584932709367, 2.413521643342643, 2.417191753638006, 2.4136070105046867, 2.4152330725846816, 2.4129675988884784, 2.413937783202123, 2.4143771279621595, 2.413252987493631, 2.414130459865326, 2.4138305954549506, 2.4144093915942464, 2.413133632569086, 2.415108279837372, 2.4126926732963723, 2.4126854475300106, 2.4120223142439117, 2.4118904116118483, 2.4134633361020894, 2.4145050530363186, 2.4141116001335856, 2.4133297253907804, 2.4175105689977388, 2.4140977448430556, 2.4138735753953555, 2.4160778279766464, 2.4134460603466565, 2.4158288516434543, 2.4131700092152815, 2.415523194327143, 2.412812662829319, 2.413554653549821, 2.4141484336508516, 2.413245911668674, 2.414716079708782, 2.4125610404022417, 2.4129416621573063, 2.4141936501845938, 2.413676555129303, 2.415770943333167, 2.415857255165213, 2.4179985006454543, 2.4147926230344474, 2.4168084795251854, 2.413726649260873, 2.4150602042381397, 2.413403886488114, 2.4146475341715443, 2.414174817857288, 2.414908339041599, 2.4148626789475114, 2.414188146199695, 2.4159644213803295, 2.414892726735333, 2.4153564003692276, 2.414080476525969, 2.414056931418934, 2.4191538610286116, 2.418986424240964, 2.4186722852522125, 2.4288117376650105, 2.421653331794175, 2.4256041789877005, 2.4174241358031976, 2.4381747829111533, 2.449182097352001, 2.433720788344961, 2.4296452725070647, 2.4179866752405275, 2.417944873690801, 2.418214903284959, 2.417253261325003, 2.4223769270923534, 2.4154778607373166, 2.4131425953850956, 2.4136566227097034, 2.4135540854950452, 2.4152437172499783, 2.414758727664039, 2.417005366683985, 2.4135024218723693, 2.414215868134021, 2.411954500796564, 2.4119293634918915, 2.4149199448195584, 2.414775483518203, 2.4135659191213024, 2.4121830749198527, 2.41201545569697, 2.4122243773173815, 2.4114820416924987, 2.4117529725010565, 2.410851625777622, 2.411554100478224, 2.4109160614326863, 2.411393057145117, 2.411061612256055, 2.4110839597893072, 2.4106927650119676, 2.410852753665843, 2.4111535553078736, 2.411122650935732, 2.410460932109939, 2.410436539422898, 2.41076826970957, 2.4101913586038672, 2.4103465397369686, 2.4101935075030148, 2.4100207119739703, 2.4106691189000173, 2.4106148968776457, 2.4099860841221803, 2.4096337821096037, 2.409328619247587, 2.4097815660028816, 2.4108772908134024, 2.4116429495694014, 2.410473444583185, 2.409573794194239, 2.4095760080810447, 2.4100889936456538, 2.409560395383287, 2.4095313995342535, 2.4105600108849785, 2.4090176172835878, 2.409222019129786, 2.4122465980072523, 2.4124377204475342, 2.4112719181918942, 2.409245530960008, 2.4088792659966227, 2.4099850631112534, 2.409538115969628, 2.4117040187854486, 2.4104265814344283, 2.409541828096011, 2.409357550500453, 2.4089919718224033, 2.4095039003588297, 2.4092509691742645, 2.4102140201136395, 2.409845603119172, 2.408897179492393, 2.4084815505298685, 2.408859152316264, 2.410018251447255, 2.409253136865024, 2.4097062155530957, 2.409468915074917, 2.409426949881568, 2.4087431896692033, 2.4084893690149967, 2.4088948908306302, 2.4093626144484346, 2.4101466087284935, 2.409202892792049, 2.4107567355746315, 2.4098699793635525, 2.4102947966414328, 2.409136460136702, 2.4084856075606322, 2.408868188341263, 2.4084595980119627, 2.4079003204853078, 2.409017938698454, 2.408546826326593, 2.4090853684837206, 2.409053818932895, 2.40894741377807, 2.4095469155335074, 2.4087336869858365, 2.4097040729178194, 2.409718464160788, 2.408800729977086, 2.409044449356781, 2.4077750614711215, 2.4082011393529834, 2.4084185974547037, 2.4085691268808147, 2.4094885174668286, 2.408332122370527, 2.408153357372691, 2.4086691025638425, 2.408564913253283, 2.408596157831903, 2.4082796816363907, 2.4082692718662457, 2.4091553206514256, 2.4077672418115172, 2.408068361736479, 2.4080958448607346, 2.4089548243286183, 2.408013567744413, 2.4085420712657357, 2.4087461501311003, 2.4091607878360843, 2.408915455118189, 2.407793232568575, 2.4079180151370947, 2.407368360090334, 2.409104477204321, 2.4083554173142256, 2.407905799023232, 2.4078781209360005, 2.4088495885601575, 2.408085091360684, 2.4082925354905904, 2.4100547999584028, 2.4087618501315564, 2.410490498754191, 2.4088115856565278, 2.4077190659903542, 2.4079782113261605, 2.4077628124719377, 2.408350939038156, 2.4084804774505164, 2.408180140900886, 2.4083309698183157, 2.408591419213707, 2.4075801337293803, 2.4074054187154537, 2.409235384272433], 'val_acc': [0.08538587808560072, 0.08538587808560072, 0.077175697761363, 0.08702791411252249, 0.09359605880744742, 0.10180623933354818, 0.11822660087511458, 0.14121510662224102, 0.14778325112142, 0.14285714274703576, 0.14614121499662525, 0.16748768362799302, 0.16584564750319827, 0.16748768362799302, 0.16748768362799302, 0.16748768362799302, 0.16420361137840353, 0.14778325013045607, 0.17405582822504498, 0.16912971975278776, 0.167487683725866, 0.167487683725866, 0.16912971985066075, 0.16584564760107126, 0.1789819366973022, 0.17569786454558567, 0.18062397291996993, 0.18062397291996993, 0.18062397282209694, 0.1756978644477127, 0.17405582822504498, 0.17405582822504498, 0.17898193679517518, 0.17569786434983972, 0.17898193679517518, 0.17405582842079093, 0.18226600904476467, 0.17569786454558567, 0.18062397291996993, 0.17405582832291797, 0.17898193679517518, 0.17733990067038044, 0.17898193679517518, 0.18062397291996993, 0.18062397291996993, 0.1871921174191489, 0.18226600904476467, 0.18062397291996993, 0.17898193679517518, 0.17898193679517518, 0.18062397291996993, 0.17898193679517518, 0.18062397291996993, 0.1839080452674324, 0.18062397291996993, 0.18555008139222715, 0.18062397291996993, 0.1839080452674324, 0.17898193679517518, 0.1806239730178429, 0.18062397291996993, 0.17569786464345866, 0.1740558285186639, 0.17569786464345866, 0.17405582842079093, 0.17241379229599618, 0.1839080452674324, 0.17569786454558567, 0.17569786454558567, 0.17241379229599618, 0.18226600914263763, 0.17241379229599618, 0.18226600914263763, 0.17569786454558567, 0.17569786454558567, 0.1839080452674324, 0.17405582842079093, 0.17405582842079093, 0.17569786454558567, 0.17405582842079093, 0.1871921175170219, 0.17405582842079093, 0.17569786454558567, 0.17405582842079093, 0.17405582842079093, 0.17569786454558567, 0.17569786454558567, 0.17405582842079093, 0.17405582842079093, 0.17569786454558567, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17569786454558567, 0.17569786454558567, 0.17405582842079093, 0.17569786454558567, 0.18555008139222715, 0.17405582842079093, 0.17569786454558567, 0.1839080452674324, 0.17405582842079093, 0.17569786454558567, 0.17405582842079093, 0.17405582842079093, 0.18226600914263763, 0.17569786454558567, 0.1839080452674324, 0.17569786454558567, 0.17569786454558567, 0.1871921175170219, 0.1871921175170219, 0.18226600914263763, 0.17569786454558567, 0.17569786454558567, 0.1839080452674324, 0.17405582842079093, 0.1855500814901001, 0.17405582842079093, 0.1839080452674324, 0.17241379229599618, 0.1806239730178429, 0.18226600914263763, 0.18555008139222715, 0.18555008139222715, 0.18226600914263763, 0.1806239730178429, 0.1806239730178429, 0.1806239730178429, 0.1806239730178429, 0.1806239730178429, 0.18226600914263763, 0.18226600914263763, 0.1806239730178429, 0.1806239730178429, 0.17569786454558567, 0.17733990067038044, 0.17733990067038044, 0.1839080452674324, 0.1789819366973022, 0.17241379229599618, 0.1806239730178429, 0.1871921175170219, 0.19376026191832788, 0.17241379229599618, 0.19211822579353313, 0.17405582842079093, 0.17241379229599618, 0.17405582842079093, 0.17241379229599618, 0.17405582842079093, 0.17733990067038044, 0.17733990067038044, 0.17733990067038044, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17733990067038044, 0.17733990067038044, 0.17733990067038044, 0.18062397291996993, 0.17898193679517518, 0.17733990067038044, 0.17733990067038044, 0.17733990067038044, 0.17898193679517518, 0.17733990067038044, 0.17733990067038044, 0.17898193679517518, 0.17733990067038044, 0.17733990067038044, 0.17733990067038044, 0.17898193679517518, 0.17898193679517518, 0.17733990067038044, 0.17733990067038044, 0.17898193679517518, 0.17733990067038044, 0.17733990067038044, 0.17898193679517518, 0.17733990067038044, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17733990067038044, 0.17733990067038044, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.17898193679517518, 0.18062397272422395, 0.18062397272422395, 0.18390804497381344, 0.18062397272422395, 0.17405582842079093, 0.17733990067038044, 0.1789819365994292, 0.17898193679517518, 0.18062397272422395, 0.18062397291996993, 0.18062397272422395, 0.17898193679517518, 0.18062397272422395, 0.17733990067038044, 0.17898193679517518, 0.17898193679517518, 0.18226600904476467, 0.18062397272422395, 0.17898193679517518, 0.18062397272422395, 0.1822660088490187, 0.1822660088490187, 0.1822660088490187, 0.1789819365994292, 0.18062397272422395, 0.1789819365994292, 0.18062397272422395, 0.18062397272422395, 0.1789819365994292, 0.18062397272422395, 0.1822660088490187, 0.18062397272422395, 0.18062397272422395, 0.1822660088490187, 0.17569786434983972, 0.18062397272422395, 0.1789819365994292, 0.18062397272422395, 0.1822660088490187, 0.1822660088490187, 0.18062397272422395, 0.1789819365994292, 0.18062397272422395, 0.18062397272422395, 0.1822660088490187, 0.1822660088490187, 0.1822660088490187, 0.17569786434983972, 0.1822660088490187, 0.1789819365994292, 0.1822660088490187, 0.1822660088490187, 0.18062397272422395, 0.18390804497381344, 0.1822660088490187, 0.1822660088490187, 0.1822660088490187, 0.18062397272422395, 0.1822660088490187, 0.1822660088490187, 0.18062397272422395, 0.1822660088490187, 0.1822660088490187, 0.18062397272422395, 0.18062397272422395, 0.18062397272422395, 0.18390804507168643, 0.18062397272422395, 0.1822660088490187, 0.17569786434983972, 0.18390804497381344, 0.18062397272422395, 0.18062397272422395, 0.1822660088490187, 0.17733990047463447, 0.18226600875114574, 0.1822660088490187, 0.18062397272422395, 0.18062397272422395, 0.17733990047463447, 0.1822660088490187, 0.1822660088490187, 0.1822660088490187, 0.18062397272422395, 0.180623972626351, 0.1789819366973022, 0.18226600875114574, 0.18062397272422395, 0.1822660088490187, 0.18390804497381344, 0.18062397272422395, 0.1822660088490187, 0.18062397272422395, 0.18062397272422395, 0.18062397272422395, 0.1822660088490187, 0.1822660088490187, 0.1822660088490187, 0.1822660088490187], 'loss': [2.699998646154541, 2.689790958643449, 2.681401909841894, 2.673796537035055, 2.667962448210197, 2.6661430046543693, 2.6617603094426023, 2.6564936748520305, 2.6512153423787144, 2.6449350603796864, 2.637720298277524, 2.629763941402553, 2.619214849942029, 2.6075708596858154, 2.5947752525674246, 2.5785456943316136, 2.567327409897007, 2.5878147663766597, 2.5797336396250636, 2.5607787847029355, 2.539161712286164, 2.528526184935834, 2.5169519845465125, 2.510088367677567, 2.5015795806594943, 2.4956818842055615, 2.4901610483134307, 2.486287985885902, 2.4839520900890815, 2.4804378550645017, 2.4784433996408137, 2.477575489725665, 2.4752005211871264, 2.4745686879637794, 2.471541595165245, 2.4751796232846237, 2.4737901640623745, 2.4723057973066638, 2.4680398011844016, 2.4667894360465925, 2.465335124425085, 2.464805666279254, 2.46410061378009, 2.463116879629648, 2.4626690182108164, 2.4632709168310774, 2.464306725319896, 2.4659381133085403, 2.4636431359167705, 2.461769507993663, 2.46037155343277, 2.4584601126167565, 2.459662673605541, 2.458662963452036, 2.457327262381019, 2.4572940247749155, 2.4558153367874804, 2.457925385567197, 2.4581020791917365, 2.456799305733714, 2.455187151221524, 2.4557044992701473, 2.4556726483104163, 2.4567367192411327, 2.4581901800950696, 2.4582153660071215, 2.4556350388810864, 2.4575617360383335, 2.4574365812650205, 2.455148865701726, 2.4547745388391324, 2.455925313941752, 2.455127111646429, 2.454007125243514, 2.45475903419009, 2.4550315457447844, 2.4542512704704333, 2.454591335946774, 2.4548378136613285, 2.453998254701587, 2.4534455026199686, 2.454285333974161, 2.453000943127109, 2.4536055710771, 2.4527496331036702, 2.452205793177078, 2.4522670553939787, 2.45176860243388, 2.452907180786133, 2.453080704080006, 2.4509951729549275, 2.4518838722847813, 2.4517859911282205, 2.450688118219865, 2.451049569108403, 2.4514817605028405, 2.4518550489962223, 2.451338802177069, 2.4506306654129184, 2.4504814363358203, 2.45195958648619, 2.454217633280666, 2.45376186135858, 2.452048410576227, 2.4512437464030614, 2.4499270235488546, 2.4507392307571316, 2.449031312950338, 2.4504387879028946, 2.449684412327635, 2.4498538071125195, 2.4495352216325013, 2.4494228533406033, 2.4490290376439967, 2.450562582662218, 2.449350487573925, 2.451485821941305, 2.4546161486138063, 2.4512107754145314, 2.452871022038391, 2.4494179536674547, 2.4548121661865734, 2.45054412642054, 2.4505747168460665, 2.449582010030257, 2.448794876917187, 2.4490267845639457, 2.4492909040783957, 2.44974368935493, 2.4495184503786374, 2.4488756358011545, 2.4504678903419133, 2.449041121108821, 2.4485210072088535, 2.4484362487675475, 2.4484672111652226, 2.4481217282508676, 2.448094740148932, 2.450938745690567, 2.4543948058964538, 2.451808964692102, 2.4553978804445364, 2.469000149654412, 2.470074658325321, 2.4614928786025154, 2.458026963533562, 2.45520730860669, 2.455081922464547, 2.4550993677527018, 2.454535408039602, 2.4501119425654165, 2.4511341646221876, 2.4511447814455756, 2.453782864811484, 2.4562608339948087, 2.4578067963617785, 2.453865149574358, 2.4562619924055724, 2.4547605312825227, 2.4516072755966345, 2.4556973395651127, 2.456176354703962, 2.4531795489959403, 2.4524211408666026, 2.4526285151926155, 2.4529284329384993, 2.45189946779725, 2.452355441910041, 2.452898618964444, 2.451091854087626, 2.4513925632656965, 2.4516177067766445, 2.4515036210876717, 2.4510286319427177, 2.4508189126940967, 2.451084782994014, 2.45053589339129, 2.4507160054584793, 2.450346852425922, 2.4509245156752257, 2.4510013958267116, 2.4503859541499393, 2.4503334594703063, 2.451004667791253, 2.4505342623536346, 2.4500162223526094, 2.4501601347443502, 2.4497515848774687, 2.4499918795709004, 2.449840169178142, 2.449961989322482, 2.4506333097540134, 2.4502308448971664, 2.4517527162171975, 2.449427550137655, 2.450844819345024, 2.4498351604297177, 2.449824862313711, 2.4493766400848327, 2.4497258990207493, 2.4496324166135377, 2.4509567671977517, 2.45691776128765, 2.453323967892531, 2.4529657965078493, 2.4511604266979368, 2.4496344526200815, 2.449685513948758, 2.450672582434433, 2.453003180834792, 2.4518780800351374, 2.449833052359078, 2.449073351775841, 2.4503551874806995, 2.4503062540990372, 2.449900393515397, 2.4496351374248215, 2.4500856396598736, 2.4495469491829374, 2.448201138087122, 2.449270078142076, 2.449553130833275, 2.4489488048475136, 2.4486802412499147, 2.449227840944482, 2.449137208887684, 2.448416189197642, 2.4496776979317167, 2.448083662546146, 2.448942558770307, 2.4488922389380985, 2.4503201781357093, 2.4485219302363466, 2.4518826833251075, 2.4491879080355288, 2.449115881342173, 2.4495035986146396, 2.4479895172667456, 2.447968666656306, 2.4474813209666855, 2.447618468047657, 2.448578125348571, 2.4481203957504807, 2.447286843568148, 2.44748112122381, 2.4479538142069166, 2.4495702586869195, 2.4488911813779044, 2.448123136144399, 2.4478689273035257, 2.447398130996516, 2.4480738607520194, 2.447142065428121, 2.447442416146061, 2.447007883647629, 2.4470048870149332, 2.4471153244590367, 2.447044365606758, 2.4471360346619844, 2.446974990989638, 2.4470004471420506, 2.446945913616392, 2.4467024844285152, 2.4466040581893136, 2.446769001155908, 2.446836224718505, 2.4475061886609213, 2.449625339155569, 2.4480174641344825, 2.4473976151899146, 2.447604253013031, 2.4465744162487053, 2.4465285811336135, 2.446296990921365, 2.4470415504071745, 2.4465102167344925, 2.4466977274148616, 2.446116345912769, 2.4467379767791937, 2.446442811053392, 2.4459177269827905, 2.446365303376372, 2.445868371497434, 2.446822460280307, 2.4478961137775523, 2.4463563979773553, 2.4469925502487277, 2.4472762415296487, 2.445819824970723, 2.446141918875598, 2.445607208569192, 2.445517246238505, 2.445546048671558, 2.445870356784954, 2.445811146142791, 2.445668386091197, 2.4452542145394203, 2.445735416275275, 2.445723999501254, 2.445537246179287], 'acc': [0.08501026687619623, 0.08501026727702828, 0.0809034905272098, 0.08090349069549807, 0.0895277212716226, 0.10800821330145889, 0.1043121153263096, 0.12484599679892068, 0.13305954838619094, 0.1351129362807137, 0.13264887115059448, 0.15112936236040794, 0.16098562521244222, 0.16057494857350413, 0.1609856255857362, 0.16057494818185145, 0.16180698062604948, 0.144147844848202, 0.14373716664265312, 0.16262833627220052, 0.16057494955263588, 0.15893223770957218, 0.16098562619157394, 0.16632443640143965, 0.16837782351265698, 0.1720739224485793, 0.16960985617104007, 0.16960985656269278, 0.17084188922107588, 0.1696098569727042, 0.16837782429596237, 0.17494866542013274, 0.172073922466938, 0.17371663348997887, 0.17535934300148512, 0.17412731055728708, 0.1724846004583018, 0.17207392287694942, 0.17453798796117184, 0.17618069864763616, 0.17289527631393448, 0.17618069745431936, 0.1716632440655628, 0.17782340773077226, 0.17248459908751737, 0.17412731034310203, 0.1745379877653455, 0.17289527691977224, 0.17289527650976083, 0.17577002102956635, 0.1687885011307268, 0.17371663211919444, 0.17043121160300606, 0.17371663137260648, 0.17289527709723987, 0.1749486646001099, 0.17577002142121906, 0.17577001983624954, 0.17043121238631145, 0.17741273169767197, 0.17371663292085854, 0.1790554413682871, 0.17577002161704539, 0.17659137468073646, 0.17700205447125483, 0.17618069706266665, 0.1753593440173343, 0.17577001946295556, 0.17618069784597204, 0.17494866618507943, 0.17248459949752878, 0.1745379881753569, 0.17412730916814392, 0.1757700200504346, 0.17577002140286033, 0.17453798835282452, 0.1737166315317154, 0.17453798778370422, 0.1737166327250322, 0.1749486653834153, 0.17084188841941175, 0.171663243833019, 0.1753593426098324, 0.17494866520594768, 0.17494866597089434, 0.17535934341149653, 0.17494866440428355, 0.17577002161704539, 0.17371663331251125, 0.17330595530278875, 0.17577002061955493, 0.17577001942623813, 0.1749486646001099, 0.1753593436073229, 0.17577002102956635, 0.17535934382150795, 0.17618069864763616, 0.17166324347808376, 0.17659137487656282, 0.17412730936397028, 0.17577002161704539, 0.1753593436073229, 0.17618069825598345, 0.17741273130601926, 0.17577002159868668, 0.17412731075311344, 0.17494866557924166, 0.17577001965878192, 0.1733059551253211, 0.17577002120703397, 0.17535934319731147, 0.17412730936397028, 0.17330595530278875, 0.17535934319731147, 0.17618069745431936, 0.17248459869586466, 0.17577001985460824, 0.1737166325475646, 0.17659137646153233, 0.17494866477757753, 0.17453798737369278, 0.1720739220569266, 0.1745379873920515, 0.17453798756951913, 0.1761806992351152, 0.17577002140286033, 0.17494866559760036, 0.17453798717786645, 0.1745379877653455, 0.17248459908751737, 0.17535934243236478, 0.1749486642084572, 0.17494866577506799, 0.17494866501012132, 0.1745379885670096, 0.17412730918650265, 0.1757700212253927, 0.17535934300148512, 0.17494866440428355, 0.17618069784597204, 0.1733059551253211, 0.17248459850003833, 0.17248460008500782, 0.1671457900709685, 0.1716632448488682, 0.17125256641077555, 0.17084188841941175, 0.17207392129197993, 0.17659137507238917, 0.1704312125821378, 0.17535934321567018, 0.17577002161704539, 0.17248459889169102, 0.17412730995144932, 0.17700205368794944, 0.1733059551069624, 0.17494866620343813, 0.17084188881106446, 0.17289527809473035, 0.17207392107779484, 0.17166324424303042, 0.17084188902524952, 0.17371663215591188, 0.17371663174590046, 0.17289527611810812, 0.1745379869820401, 0.16960985654433405, 0.17371663252920586, 0.1691991775554798, 0.17412731034310203, 0.17412730996980805, 0.1745379869820401, 0.17289527611810812, 0.17371663293921727, 0.17371663333086998, 0.17412731075311344, 0.17412730897231757, 0.17412730995144932, 0.17371663252920586, 0.17453798854865088, 0.17412730916814392, 0.1741273101656344, 0.17371663333086998, 0.17371663354505504, 0.17371663156843284, 0.17412730955979663, 0.17453798678621374, 0.17453798815699817, 0.17371663292085854, 0.1737166331166849, 0.1737166325475646, 0.17741273089600784, 0.17412730916814392, 0.1708418894169022, 0.17453798678621374, 0.17371663333086998, 0.17782340931574178, 0.17412731075311344, 0.17412731075311344, 0.17412731116312485, 0.17618069823762475, 0.16960985636686643, 0.16919917796549122, 0.17535934300148512, 0.177002052690459, 0.172073922466938, 0.17248460065412816, 0.17494866477757753, 0.17207392285859072, 0.1741273107347547, 0.17371663194172682, 0.17412730936397028, 0.17659137526821553, 0.17084188959436986, 0.17700205228044757, 0.1704312114071797, 0.1757700208153813, 0.17002053359328353, 0.17782340890573037, 0.177002052317165, 0.17289527690141354, 0.17823408711127922, 0.1708418890068908, 0.17782340812242498, 0.1774127299168761, 0.17741273011270245, 0.17864476394604364, 0.17946611800722517, 0.1774127315018456, 0.17043121039133052, 0.1774127311101929, 0.17371663213755315, 0.17782340773077226, 0.17289527693813098, 0.1774127299168761, 0.17453798797953055, 0.17741273208932465, 0.17823408515301573, 0.1774127307001815, 0.1770020528679266, 0.17864476314437952, 0.17577002159868668, 0.17782340792659862, 0.1786447629669119, 0.17905544038915536, 0.17782340851407766, 0.1728952783089154, 0.17371663333086998, 0.17987679719190577, 0.17864476316273825, 0.17823408593632112, 0.1782340857588535, 0.17823408652380018, 0.17823408673798524, 0.1782340869338116, 0.17905544197412487, 0.17823408632797383, 0.17782340968903576, 0.1778234085324364, 0.17864476496189283, 0.1774127318751396, 0.1774127298985174, 0.17864476394604364, 0.1786447641602287, 0.17864476474770774, 0.17823408652380018, 0.17782340771241353, 0.1790554405849817, 0.17905544236577756, 0.1782340865421589, 0.1790554425616039, 0.17905544056662298, 0.17823408613214747, 0.17782340812242498, 0.17823408513465702, 0.1782340869338116, 0.1782340865421589, 0.17905544158247216, 0.1774127318934983, 0.17823408634633253, 0.1778234083182513, 0.17782340771241353, 0.1782340861505062, 0.17823408554466844, 0.1798767972102645, 0.17535934341149653, 0.1790554421515925, 0.17823408593632112, 0.17905544236577756, 0.17864476355439093, 0.1782340865421589, 0.17864476474770774, 0.1794661185947042, 0.17823408556302714, 0.17864476355439093, 0.1782340855263097, 0.1782340867196265, 0.17823408593632112, 0.17864476334020588, 0.1778234085324364]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
