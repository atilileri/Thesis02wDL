{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf3.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.25 23:47:58 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '0', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000176297F4E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000017626F76EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0896, Accuracy:0.3713, Validation Loss:1.0827, Validation Accuracy:0.3760\n",
    "Epoch #2: Loss:1.0804, Accuracy:0.3749, Validation Loss:1.0776, Validation Accuracy:0.3727\n",
    "Epoch #3: Loss:1.0751, Accuracy:0.3803, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #4: Loss:1.0742, Accuracy:0.3959, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3914, Validation Loss:1.0752, Validation Accuracy:0.3990\n",
    "Epoch #7: Loss:1.0741, Accuracy:0.3938, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #11: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0749, Validation Accuracy:0.3957\n",
    "Epoch #12: Loss:1.0735, Accuracy:0.4111, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #14: Loss:1.0732, Accuracy:0.3926, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #15: Loss:1.0727, Accuracy:0.4041, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #16: Loss:1.0728, Accuracy:0.4033, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #17: Loss:1.0727, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #18: Loss:1.0725, Accuracy:0.4045, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #19: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.3893, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #21: Loss:1.0725, Accuracy:0.4074, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #22: Loss:1.0726, Accuracy:0.4037, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #23: Loss:1.0722, Accuracy:0.4066, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #24: Loss:1.0728, Accuracy:0.3971, Validation Loss:1.0730, Validation Accuracy:0.3810\n",
    "Epoch #25: Loss:1.0734, Accuracy:0.3889, Validation Loss:1.0747, Validation Accuracy:0.3744\n",
    "Epoch #26: Loss:1.0729, Accuracy:0.3963, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #27: Loss:1.0735, Accuracy:0.3926, Validation Loss:1.0747, Validation Accuracy:0.3810\n",
    "Epoch #28: Loss:1.0740, Accuracy:0.3951, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0745, Accuracy:0.3926, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #30: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #31: Loss:1.0729, Accuracy:0.3975, Validation Loss:1.0740, Validation Accuracy:0.3777\n",
    "Epoch #32: Loss:1.0730, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3777\n",
    "Epoch #33: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #34: Loss:1.0724, Accuracy:0.3926, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #35: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #36: Loss:1.0727, Accuracy:0.4049, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #37: Loss:1.0726, Accuracy:0.4021, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #38: Loss:1.0724, Accuracy:0.4029, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #39: Loss:1.0722, Accuracy:0.3971, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #40: Loss:1.0720, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #41: Loss:1.0721, Accuracy:0.3951, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #42: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.3760\n",
    "Epoch #43: Loss:1.0722, Accuracy:0.4037, Validation Loss:1.0753, Validation Accuracy:0.3859\n",
    "Epoch #44: Loss:1.0717, Accuracy:0.4057, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #45: Loss:1.0716, Accuracy:0.3988, Validation Loss:1.0752, Validation Accuracy:0.3908\n",
    "Epoch #46: Loss:1.0713, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #47: Loss:1.0733, Accuracy:0.4053, Validation Loss:1.0754, Validation Accuracy:0.3810\n",
    "Epoch #48: Loss:1.0721, Accuracy:0.3930, Validation Loss:1.0759, Validation Accuracy:0.3793\n",
    "Epoch #49: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #50: Loss:1.0729, Accuracy:0.4066, Validation Loss:1.0751, Validation Accuracy:0.3924\n",
    "Epoch #51: Loss:1.0721, Accuracy:0.4066, Validation Loss:1.0745, Validation Accuracy:0.3859\n",
    "Epoch #52: Loss:1.0720, Accuracy:0.4041, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #53: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #54: Loss:1.0718, Accuracy:0.4062, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #55: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0754, Validation Accuracy:0.3990\n",
    "Epoch #56: Loss:1.0714, Accuracy:0.3992, Validation Loss:1.0759, Validation Accuracy:0.3744\n",
    "Epoch #57: Loss:1.0718, Accuracy:0.3975, Validation Loss:1.0751, Validation Accuracy:0.3990\n",
    "Epoch #58: Loss:1.0711, Accuracy:0.4037, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #59: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.4105\n",
    "Epoch #60: Loss:1.0720, Accuracy:0.3967, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #61: Loss:1.0717, Accuracy:0.3979, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #62: Loss:1.0714, Accuracy:0.4062, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0709, Accuracy:0.4053, Validation Loss:1.0763, Validation Accuracy:0.4039\n",
    "Epoch #64: Loss:1.0707, Accuracy:0.4082, Validation Loss:1.0758, Validation Accuracy:0.3875\n",
    "Epoch #65: Loss:1.0701, Accuracy:0.4160, Validation Loss:1.0764, Validation Accuracy:0.3892\n",
    "Epoch #66: Loss:1.0706, Accuracy:0.3971, Validation Loss:1.0770, Validation Accuracy:0.3744\n",
    "Epoch #67: Loss:1.0698, Accuracy:0.4201, Validation Loss:1.0772, Validation Accuracy:0.3908\n",
    "Epoch #68: Loss:1.0697, Accuracy:0.4000, Validation Loss:1.0782, Validation Accuracy:0.3711\n",
    "Epoch #69: Loss:1.0707, Accuracy:0.4045, Validation Loss:1.0778, Validation Accuracy:0.3842\n",
    "Epoch #70: Loss:1.0684, Accuracy:0.4037, Validation Loss:1.0778, Validation Accuracy:0.4023\n",
    "Epoch #71: Loss:1.0690, Accuracy:0.4057, Validation Loss:1.0771, Validation Accuracy:0.3727\n",
    "Epoch #72: Loss:1.0687, Accuracy:0.3996, Validation Loss:1.0776, Validation Accuracy:0.3727\n",
    "Epoch #73: Loss:1.0679, Accuracy:0.4119, Validation Loss:1.0771, Validation Accuracy:0.4007\n",
    "Epoch #74: Loss:1.0668, Accuracy:0.4004, Validation Loss:1.0773, Validation Accuracy:0.3810\n",
    "Epoch #75: Loss:1.0682, Accuracy:0.4160, Validation Loss:1.0799, Validation Accuracy:0.3908\n",
    "Epoch #76: Loss:1.0671, Accuracy:0.4066, Validation Loss:1.0837, Validation Accuracy:0.3777\n",
    "Epoch #77: Loss:1.0703, Accuracy:0.4140, Validation Loss:1.0806, Validation Accuracy:0.4056\n",
    "Epoch #78: Loss:1.0664, Accuracy:0.4131, Validation Loss:1.0810, Validation Accuracy:0.3760\n",
    "Epoch #79: Loss:1.0667, Accuracy:0.4119, Validation Loss:1.0826, Validation Accuracy:0.4007\n",
    "Epoch #80: Loss:1.0666, Accuracy:0.4164, Validation Loss:1.0796, Validation Accuracy:0.3777\n",
    "Epoch #81: Loss:1.0666, Accuracy:0.4160, Validation Loss:1.0806, Validation Accuracy:0.3908\n",
    "Epoch #82: Loss:1.0706, Accuracy:0.3901, Validation Loss:1.0831, Validation Accuracy:0.3842\n",
    "Epoch #83: Loss:1.0683, Accuracy:0.4115, Validation Loss:1.0803, Validation Accuracy:0.3941\n",
    "Epoch #84: Loss:1.0651, Accuracy:0.4127, Validation Loss:1.0791, Validation Accuracy:0.3810\n",
    "Epoch #85: Loss:1.0645, Accuracy:0.4230, Validation Loss:1.0797, Validation Accuracy:0.3875\n",
    "Epoch #86: Loss:1.0641, Accuracy:0.4201, Validation Loss:1.0798, Validation Accuracy:0.3990\n",
    "Epoch #87: Loss:1.0638, Accuracy:0.4201, Validation Loss:1.0760, Validation Accuracy:0.3859\n",
    "Epoch #88: Loss:1.0647, Accuracy:0.4164, Validation Loss:1.0801, Validation Accuracy:0.3974\n",
    "Epoch #89: Loss:1.0649, Accuracy:0.4078, Validation Loss:1.0795, Validation Accuracy:0.3760\n",
    "Epoch #90: Loss:1.0661, Accuracy:0.4205, Validation Loss:1.0793, Validation Accuracy:0.3990\n",
    "Epoch #91: Loss:1.0615, Accuracy:0.4242, Validation Loss:1.0798, Validation Accuracy:0.3875\n",
    "Epoch #92: Loss:1.0631, Accuracy:0.4160, Validation Loss:1.0798, Validation Accuracy:0.3842\n",
    "Epoch #93: Loss:1.0654, Accuracy:0.4152, Validation Loss:1.0862, Validation Accuracy:0.3793\n",
    "Epoch #94: Loss:1.0679, Accuracy:0.4066, Validation Loss:1.0880, Validation Accuracy:0.3875\n",
    "Epoch #95: Loss:1.0711, Accuracy:0.4086, Validation Loss:1.0864, Validation Accuracy:0.3908\n",
    "Epoch #96: Loss:1.0696, Accuracy:0.4057, Validation Loss:1.0819, Validation Accuracy:0.3875\n",
    "Epoch #97: Loss:1.0680, Accuracy:0.4181, Validation Loss:1.0801, Validation Accuracy:0.3826\n",
    "Epoch #98: Loss:1.0665, Accuracy:0.4082, Validation Loss:1.0776, Validation Accuracy:0.3941\n",
    "Epoch #99: Loss:1.0639, Accuracy:0.4189, Validation Loss:1.0773, Validation Accuracy:0.3875\n",
    "Epoch #100: Loss:1.0641, Accuracy:0.4140, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #101: Loss:1.0622, Accuracy:0.4209, Validation Loss:1.0793, Validation Accuracy:0.4023\n",
    "Epoch #102: Loss:1.0632, Accuracy:0.4029, Validation Loss:1.0798, Validation Accuracy:0.3859\n",
    "Epoch #103: Loss:1.0619, Accuracy:0.4107, Validation Loss:1.0781, Validation Accuracy:0.3810\n",
    "Epoch #104: Loss:1.0605, Accuracy:0.4168, Validation Loss:1.0786, Validation Accuracy:0.3990\n",
    "Epoch #105: Loss:1.0604, Accuracy:0.4156, Validation Loss:1.0784, Validation Accuracy:0.3875\n",
    "Epoch #106: Loss:1.0608, Accuracy:0.4185, Validation Loss:1.0782, Validation Accuracy:0.3859\n",
    "Epoch #107: Loss:1.0602, Accuracy:0.4242, Validation Loss:1.0842, Validation Accuracy:0.3941\n",
    "Epoch #108: Loss:1.0613, Accuracy:0.4086, Validation Loss:1.0854, Validation Accuracy:0.3892\n",
    "Epoch #109: Loss:1.0566, Accuracy:0.4230, Validation Loss:1.1006, Validation Accuracy:0.3744\n",
    "Epoch #110: Loss:1.0676, Accuracy:0.4049, Validation Loss:1.0830, Validation Accuracy:0.3892\n",
    "Epoch #111: Loss:1.0598, Accuracy:0.4119, Validation Loss:1.0876, Validation Accuracy:0.3810\n",
    "Epoch #112: Loss:1.0592, Accuracy:0.4271, Validation Loss:1.0904, Validation Accuracy:0.3826\n",
    "Epoch #113: Loss:1.0612, Accuracy:0.4111, Validation Loss:1.0914, Validation Accuracy:0.3842\n",
    "Epoch #114: Loss:1.0574, Accuracy:0.4238, Validation Loss:1.0900, Validation Accuracy:0.3908\n",
    "Epoch #115: Loss:1.0582, Accuracy:0.4164, Validation Loss:1.0893, Validation Accuracy:0.3793\n",
    "Epoch #116: Loss:1.0575, Accuracy:0.4259, Validation Loss:1.0887, Validation Accuracy:0.3810\n",
    "Epoch #117: Loss:1.0566, Accuracy:0.4205, Validation Loss:1.0920, Validation Accuracy:0.3842\n",
    "Epoch #118: Loss:1.0581, Accuracy:0.4172, Validation Loss:1.0892, Validation Accuracy:0.3892\n",
    "Epoch #119: Loss:1.0583, Accuracy:0.4271, Validation Loss:1.0938, Validation Accuracy:0.3859\n",
    "Epoch #120: Loss:1.0610, Accuracy:0.4193, Validation Loss:1.0935, Validation Accuracy:0.3941\n",
    "Epoch #121: Loss:1.0620, Accuracy:0.4218, Validation Loss:1.0913, Validation Accuracy:0.3957\n",
    "Epoch #122: Loss:1.0595, Accuracy:0.4189, Validation Loss:1.0906, Validation Accuracy:0.3941\n",
    "Epoch #123: Loss:1.0612, Accuracy:0.4111, Validation Loss:1.0922, Validation Accuracy:0.4039\n",
    "Epoch #124: Loss:1.0593, Accuracy:0.4242, Validation Loss:1.0904, Validation Accuracy:0.3859\n",
    "Epoch #125: Loss:1.0604, Accuracy:0.4160, Validation Loss:1.0979, Validation Accuracy:0.3842\n",
    "Epoch #126: Loss:1.0594, Accuracy:0.4201, Validation Loss:1.0923, Validation Accuracy:0.3892\n",
    "Epoch #127: Loss:1.0587, Accuracy:0.4160, Validation Loss:1.0962, Validation Accuracy:0.3875\n",
    "Epoch #128: Loss:1.0570, Accuracy:0.4271, Validation Loss:1.0954, Validation Accuracy:0.3908\n",
    "Epoch #129: Loss:1.0559, Accuracy:0.4283, Validation Loss:1.0972, Validation Accuracy:0.3842\n",
    "Epoch #130: Loss:1.0568, Accuracy:0.4275, Validation Loss:1.0972, Validation Accuracy:0.3875\n",
    "Epoch #131: Loss:1.0562, Accuracy:0.4193, Validation Loss:1.0937, Validation Accuracy:0.3826\n",
    "Epoch #132: Loss:1.0572, Accuracy:0.4086, Validation Loss:1.0995, Validation Accuracy:0.3810\n",
    "Epoch #133: Loss:1.0538, Accuracy:0.4238, Validation Loss:1.0959, Validation Accuracy:0.3810\n",
    "Epoch #134: Loss:1.0541, Accuracy:0.4251, Validation Loss:1.0943, Validation Accuracy:0.3908\n",
    "Epoch #135: Loss:1.0578, Accuracy:0.4160, Validation Loss:1.0908, Validation Accuracy:0.3842\n",
    "Epoch #136: Loss:1.0591, Accuracy:0.4193, Validation Loss:1.0901, Validation Accuracy:0.3826\n",
    "Epoch #137: Loss:1.0614, Accuracy:0.4103, Validation Loss:1.0925, Validation Accuracy:0.3941\n",
    "Epoch #138: Loss:1.0607, Accuracy:0.4168, Validation Loss:1.0883, Validation Accuracy:0.3793\n",
    "Epoch #139: Loss:1.0600, Accuracy:0.4172, Validation Loss:1.0886, Validation Accuracy:0.3957\n",
    "Epoch #140: Loss:1.0578, Accuracy:0.4218, Validation Loss:1.0904, Validation Accuracy:0.3908\n",
    "Epoch #141: Loss:1.0575, Accuracy:0.4209, Validation Loss:1.0925, Validation Accuracy:0.3892\n",
    "Epoch #142: Loss:1.0569, Accuracy:0.4218, Validation Loss:1.0931, Validation Accuracy:0.3924\n",
    "Epoch #143: Loss:1.0549, Accuracy:0.4172, Validation Loss:1.0980, Validation Accuracy:0.3924\n",
    "Epoch #144: Loss:1.0545, Accuracy:0.4209, Validation Loss:1.0975, Validation Accuracy:0.3875\n",
    "Epoch #145: Loss:1.0555, Accuracy:0.4185, Validation Loss:1.0968, Validation Accuracy:0.3941\n",
    "Epoch #146: Loss:1.0565, Accuracy:0.4230, Validation Loss:1.0957, Validation Accuracy:0.3924\n",
    "Epoch #147: Loss:1.0566, Accuracy:0.4099, Validation Loss:1.0979, Validation Accuracy:0.3810\n",
    "Epoch #148: Loss:1.0554, Accuracy:0.4201, Validation Loss:1.0971, Validation Accuracy:0.3793\n",
    "Epoch #149: Loss:1.0549, Accuracy:0.4251, Validation Loss:1.0967, Validation Accuracy:0.3941\n",
    "Epoch #150: Loss:1.0559, Accuracy:0.4136, Validation Loss:1.0948, Validation Accuracy:0.3892\n",
    "Epoch #151: Loss:1.0562, Accuracy:0.4185, Validation Loss:1.0935, Validation Accuracy:0.3908\n",
    "Epoch #152: Loss:1.0538, Accuracy:0.4251, Validation Loss:1.0982, Validation Accuracy:0.3924\n",
    "Epoch #153: Loss:1.0539, Accuracy:0.4238, Validation Loss:1.0963, Validation Accuracy:0.3793\n",
    "Epoch #154: Loss:1.0538, Accuracy:0.4144, Validation Loss:1.1037, Validation Accuracy:0.3875\n",
    "Epoch #155: Loss:1.0540, Accuracy:0.4201, Validation Loss:1.0992, Validation Accuracy:0.3908\n",
    "Epoch #156: Loss:1.0537, Accuracy:0.4189, Validation Loss:1.1071, Validation Accuracy:0.3941\n",
    "Epoch #157: Loss:1.0522, Accuracy:0.4246, Validation Loss:1.1060, Validation Accuracy:0.3941\n",
    "Epoch #158: Loss:1.0525, Accuracy:0.4226, Validation Loss:1.1074, Validation Accuracy:0.3875\n",
    "Epoch #159: Loss:1.0506, Accuracy:0.4251, Validation Loss:1.1095, Validation Accuracy:0.3941\n",
    "Epoch #160: Loss:1.0517, Accuracy:0.4300, Validation Loss:1.1032, Validation Accuracy:0.3892\n",
    "Epoch #161: Loss:1.0531, Accuracy:0.4267, Validation Loss:1.1030, Validation Accuracy:0.3957\n",
    "Epoch #162: Loss:1.0542, Accuracy:0.4218, Validation Loss:1.0978, Validation Accuracy:0.3908\n",
    "Epoch #163: Loss:1.0527, Accuracy:0.4242, Validation Loss:1.0973, Validation Accuracy:0.3957\n",
    "Epoch #164: Loss:1.0510, Accuracy:0.4320, Validation Loss:1.1008, Validation Accuracy:0.3957\n",
    "Epoch #165: Loss:1.0505, Accuracy:0.4324, Validation Loss:1.1079, Validation Accuracy:0.3924\n",
    "Epoch #166: Loss:1.0518, Accuracy:0.4218, Validation Loss:1.1068, Validation Accuracy:0.3924\n",
    "Epoch #167: Loss:1.0518, Accuracy:0.4304, Validation Loss:1.1014, Validation Accuracy:0.3957\n",
    "Epoch #168: Loss:1.0510, Accuracy:0.4320, Validation Loss:1.0987, Validation Accuracy:0.4056\n",
    "Epoch #169: Loss:1.0507, Accuracy:0.4345, Validation Loss:1.0967, Validation Accuracy:0.3941\n",
    "Epoch #170: Loss:1.0496, Accuracy:0.4320, Validation Loss:1.0985, Validation Accuracy:0.3990\n",
    "Epoch #171: Loss:1.0484, Accuracy:0.4333, Validation Loss:1.1042, Validation Accuracy:0.4007\n",
    "Epoch #172: Loss:1.0488, Accuracy:0.4353, Validation Loss:1.1008, Validation Accuracy:0.3859\n",
    "Epoch #173: Loss:1.0487, Accuracy:0.4370, Validation Loss:1.1011, Validation Accuracy:0.4039\n",
    "Epoch #174: Loss:1.0509, Accuracy:0.4238, Validation Loss:1.1013, Validation Accuracy:0.3842\n",
    "Epoch #175: Loss:1.0520, Accuracy:0.4333, Validation Loss:1.1044, Validation Accuracy:0.3990\n",
    "Epoch #176: Loss:1.0511, Accuracy:0.4287, Validation Loss:1.0964, Validation Accuracy:0.3957\n",
    "Epoch #177: Loss:1.0501, Accuracy:0.4242, Validation Loss:1.0920, Validation Accuracy:0.3826\n",
    "Epoch #178: Loss:1.0492, Accuracy:0.4222, Validation Loss:1.0940, Validation Accuracy:0.3793\n",
    "Epoch #179: Loss:1.0472, Accuracy:0.4329, Validation Loss:1.0996, Validation Accuracy:0.3842\n",
    "Epoch #180: Loss:1.0496, Accuracy:0.4275, Validation Loss:1.0989, Validation Accuracy:0.3777\n",
    "Epoch #181: Loss:1.0467, Accuracy:0.4394, Validation Loss:1.1006, Validation Accuracy:0.3892\n",
    "Epoch #182: Loss:1.0463, Accuracy:0.4370, Validation Loss:1.1082, Validation Accuracy:0.3859\n",
    "Epoch #183: Loss:1.0497, Accuracy:0.4312, Validation Loss:1.1111, Validation Accuracy:0.3826\n",
    "Epoch #184: Loss:1.0513, Accuracy:0.4185, Validation Loss:1.1103, Validation Accuracy:0.3711\n",
    "Epoch #185: Loss:1.0520, Accuracy:0.4172, Validation Loss:1.1160, Validation Accuracy:0.3629\n",
    "Epoch #186: Loss:1.0493, Accuracy:0.4304, Validation Loss:1.1081, Validation Accuracy:0.3842\n",
    "Epoch #187: Loss:1.0523, Accuracy:0.4222, Validation Loss:1.1099, Validation Accuracy:0.3777\n",
    "Epoch #188: Loss:1.0509, Accuracy:0.4222, Validation Loss:1.1034, Validation Accuracy:0.3810\n",
    "Epoch #189: Loss:1.0499, Accuracy:0.4099, Validation Loss:1.1081, Validation Accuracy:0.3842\n",
    "Epoch #190: Loss:1.0492, Accuracy:0.4246, Validation Loss:1.1120, Validation Accuracy:0.3727\n",
    "Epoch #191: Loss:1.0469, Accuracy:0.4411, Validation Loss:1.1108, Validation Accuracy:0.3842\n",
    "Epoch #192: Loss:1.0464, Accuracy:0.4296, Validation Loss:1.0983, Validation Accuracy:0.3892\n",
    "Epoch #193: Loss:1.0451, Accuracy:0.4304, Validation Loss:1.1034, Validation Accuracy:0.3744\n",
    "Epoch #194: Loss:1.0473, Accuracy:0.4316, Validation Loss:1.1027, Validation Accuracy:0.3760\n",
    "Epoch #195: Loss:1.0443, Accuracy:0.4398, Validation Loss:1.1068, Validation Accuracy:0.3629\n",
    "Epoch #196: Loss:1.0440, Accuracy:0.4427, Validation Loss:1.1078, Validation Accuracy:0.3727\n",
    "Epoch #197: Loss:1.0414, Accuracy:0.4439, Validation Loss:1.1133, Validation Accuracy:0.3744\n",
    "Epoch #198: Loss:1.0432, Accuracy:0.4485, Validation Loss:1.1143, Validation Accuracy:0.3842\n",
    "Epoch #199: Loss:1.0437, Accuracy:0.4427, Validation Loss:1.1112, Validation Accuracy:0.3826\n",
    "Epoch #200: Loss:1.0437, Accuracy:0.4361, Validation Loss:1.1170, Validation Accuracy:0.3924\n",
    "Epoch #201: Loss:1.0491, Accuracy:0.4275, Validation Loss:1.1083, Validation Accuracy:0.3859\n",
    "Epoch #202: Loss:1.0451, Accuracy:0.4333, Validation Loss:1.1178, Validation Accuracy:0.3810\n",
    "Epoch #203: Loss:1.0461, Accuracy:0.4197, Validation Loss:1.1076, Validation Accuracy:0.3777\n",
    "Epoch #204: Loss:1.0447, Accuracy:0.4304, Validation Loss:1.1105, Validation Accuracy:0.4072\n",
    "Epoch #205: Loss:1.0457, Accuracy:0.4333, Validation Loss:1.1026, Validation Accuracy:0.3892\n",
    "Epoch #206: Loss:1.0511, Accuracy:0.4201, Validation Loss:1.0995, Validation Accuracy:0.3793\n",
    "Epoch #207: Loss:1.0483, Accuracy:0.4045, Validation Loss:1.1067, Validation Accuracy:0.3908\n",
    "Epoch #208: Loss:1.0419, Accuracy:0.4448, Validation Loss:1.1076, Validation Accuracy:0.3810\n",
    "Epoch #209: Loss:1.0456, Accuracy:0.4407, Validation Loss:1.1117, Validation Accuracy:0.3908\n",
    "Epoch #210: Loss:1.0413, Accuracy:0.4464, Validation Loss:1.1097, Validation Accuracy:0.3810\n",
    "Epoch #211: Loss:1.0396, Accuracy:0.4456, Validation Loss:1.1125, Validation Accuracy:0.3859\n",
    "Epoch #212: Loss:1.0434, Accuracy:0.4341, Validation Loss:1.1103, Validation Accuracy:0.3842\n",
    "Epoch #213: Loss:1.0441, Accuracy:0.4378, Validation Loss:1.1097, Validation Accuracy:0.3859\n",
    "Epoch #214: Loss:1.0405, Accuracy:0.4398, Validation Loss:1.1114, Validation Accuracy:0.3941\n",
    "Epoch #215: Loss:1.0411, Accuracy:0.4460, Validation Loss:1.1176, Validation Accuracy:0.3908\n",
    "Epoch #216: Loss:1.0396, Accuracy:0.4378, Validation Loss:1.1195, Validation Accuracy:0.3875\n",
    "Epoch #217: Loss:1.0378, Accuracy:0.4345, Validation Loss:1.1192, Validation Accuracy:0.3777\n",
    "Epoch #218: Loss:1.0393, Accuracy:0.4349, Validation Loss:1.1225, Validation Accuracy:0.3924\n",
    "Epoch #219: Loss:1.0380, Accuracy:0.4370, Validation Loss:1.1212, Validation Accuracy:0.3711\n",
    "Epoch #220: Loss:1.0381, Accuracy:0.4411, Validation Loss:1.1419, Validation Accuracy:0.3974\n",
    "Epoch #221: Loss:1.0387, Accuracy:0.4415, Validation Loss:1.1220, Validation Accuracy:0.3859\n",
    "Epoch #222: Loss:1.0463, Accuracy:0.4283, Validation Loss:1.1302, Validation Accuracy:0.3842\n",
    "Epoch #223: Loss:1.0394, Accuracy:0.4361, Validation Loss:1.1289, Validation Accuracy:0.3908\n",
    "Epoch #224: Loss:1.0418, Accuracy:0.4300, Validation Loss:1.1332, Validation Accuracy:0.3842\n",
    "Epoch #225: Loss:1.0334, Accuracy:0.4329, Validation Loss:1.1179, Validation Accuracy:0.3875\n",
    "Epoch #226: Loss:1.0302, Accuracy:0.4472, Validation Loss:1.1419, Validation Accuracy:0.4007\n",
    "Epoch #227: Loss:1.0346, Accuracy:0.4382, Validation Loss:1.1188, Validation Accuracy:0.3859\n",
    "Epoch #228: Loss:1.0327, Accuracy:0.4439, Validation Loss:1.1351, Validation Accuracy:0.3777\n",
    "Epoch #229: Loss:1.0305, Accuracy:0.4415, Validation Loss:1.1381, Validation Accuracy:0.3793\n",
    "Epoch #230: Loss:1.0281, Accuracy:0.4497, Validation Loss:1.1315, Validation Accuracy:0.4105\n",
    "Epoch #231: Loss:1.0293, Accuracy:0.4390, Validation Loss:1.1271, Validation Accuracy:0.3810\n",
    "Epoch #232: Loss:1.0256, Accuracy:0.4444, Validation Loss:1.1511, Validation Accuracy:0.4089\n",
    "Epoch #233: Loss:1.0291, Accuracy:0.4468, Validation Loss:1.1326, Validation Accuracy:0.3842\n",
    "Epoch #234: Loss:1.0350, Accuracy:0.4255, Validation Loss:1.1298, Validation Accuracy:0.3744\n",
    "Epoch #235: Loss:1.0324, Accuracy:0.4394, Validation Loss:1.1471, Validation Accuracy:0.3810\n",
    "Epoch #236: Loss:1.0328, Accuracy:0.4448, Validation Loss:1.1315, Validation Accuracy:0.3842\n",
    "Epoch #237: Loss:1.0344, Accuracy:0.4329, Validation Loss:1.1307, Validation Accuracy:0.3777\n",
    "Epoch #238: Loss:1.0328, Accuracy:0.4382, Validation Loss:1.1329, Validation Accuracy:0.3875\n",
    "Epoch #239: Loss:1.0324, Accuracy:0.4444, Validation Loss:1.1339, Validation Accuracy:0.3777\n",
    "Epoch #240: Loss:1.0317, Accuracy:0.4472, Validation Loss:1.1203, Validation Accuracy:0.3678\n",
    "Epoch #241: Loss:1.0338, Accuracy:0.4501, Validation Loss:1.1240, Validation Accuracy:0.3727\n",
    "Epoch #242: Loss:1.0367, Accuracy:0.4333, Validation Loss:1.1215, Validation Accuracy:0.3842\n",
    "Epoch #243: Loss:1.0323, Accuracy:0.4242, Validation Loss:1.1203, Validation Accuracy:0.3711\n",
    "Epoch #244: Loss:1.0333, Accuracy:0.4316, Validation Loss:1.1257, Validation Accuracy:0.3875\n",
    "Epoch #245: Loss:1.0289, Accuracy:0.4382, Validation Loss:1.1198, Validation Accuracy:0.3744\n",
    "Epoch #246: Loss:1.0322, Accuracy:0.4464, Validation Loss:1.1350, Validation Accuracy:0.3727\n",
    "Epoch #247: Loss:1.0291, Accuracy:0.4435, Validation Loss:1.1203, Validation Accuracy:0.3793\n",
    "Epoch #248: Loss:1.0319, Accuracy:0.4353, Validation Loss:1.1216, Validation Accuracy:0.3760\n",
    "Epoch #249: Loss:1.0301, Accuracy:0.4452, Validation Loss:1.1328, Validation Accuracy:0.3810\n",
    "Epoch #250: Loss:1.0287, Accuracy:0.4493, Validation Loss:1.1266, Validation Accuracy:0.3695\n",
    "Epoch #251: Loss:1.0283, Accuracy:0.4534, Validation Loss:1.1224, Validation Accuracy:0.3842\n",
    "Epoch #252: Loss:1.0265, Accuracy:0.4489, Validation Loss:1.1314, Validation Accuracy:0.3842\n",
    "Epoch #253: Loss:1.0303, Accuracy:0.4501, Validation Loss:1.1349, Validation Accuracy:0.3924\n",
    "Epoch #254: Loss:1.0297, Accuracy:0.4476, Validation Loss:1.1313, Validation Accuracy:0.3678\n",
    "Epoch #255: Loss:1.0297, Accuracy:0.4600, Validation Loss:1.1486, Validation Accuracy:0.3842\n",
    "Epoch #256: Loss:1.0300, Accuracy:0.4505, Validation Loss:1.1460, Validation Accuracy:0.3859\n",
    "Epoch #257: Loss:1.0277, Accuracy:0.4522, Validation Loss:1.1439, Validation Accuracy:0.3892\n",
    "Epoch #258: Loss:1.0290, Accuracy:0.4559, Validation Loss:1.1378, Validation Accuracy:0.3990\n",
    "Epoch #259: Loss:1.0280, Accuracy:0.4411, Validation Loss:1.1446, Validation Accuracy:0.4023\n",
    "Epoch #260: Loss:1.0258, Accuracy:0.4559, Validation Loss:1.1389, Validation Accuracy:0.3875\n",
    "Epoch #261: Loss:1.0277, Accuracy:0.4419, Validation Loss:1.1410, Validation Accuracy:0.3826\n",
    "Epoch #262: Loss:1.0250, Accuracy:0.4530, Validation Loss:1.1433, Validation Accuracy:0.3908\n",
    "Epoch #263: Loss:1.0289, Accuracy:0.4517, Validation Loss:1.1386, Validation Accuracy:0.3859\n",
    "Epoch #264: Loss:1.0227, Accuracy:0.4517, Validation Loss:1.1416, Validation Accuracy:0.3875\n",
    "Epoch #265: Loss:1.0213, Accuracy:0.4509, Validation Loss:1.1451, Validation Accuracy:0.3941\n",
    "Epoch #266: Loss:1.0217, Accuracy:0.4538, Validation Loss:1.1427, Validation Accuracy:0.3793\n",
    "Epoch #267: Loss:1.0235, Accuracy:0.4427, Validation Loss:1.1378, Validation Accuracy:0.3744\n",
    "Epoch #268: Loss:1.0256, Accuracy:0.4411, Validation Loss:1.1438, Validation Accuracy:0.3711\n",
    "Epoch #269: Loss:1.0334, Accuracy:0.4407, Validation Loss:1.1414, Validation Accuracy:0.3727\n",
    "Epoch #270: Loss:1.0293, Accuracy:0.4300, Validation Loss:1.1364, Validation Accuracy:0.3842\n",
    "Epoch #271: Loss:1.0296, Accuracy:0.4370, Validation Loss:1.1463, Validation Accuracy:0.3744\n",
    "Epoch #272: Loss:1.0264, Accuracy:0.4489, Validation Loss:1.1397, Validation Accuracy:0.3826\n",
    "Epoch #273: Loss:1.0262, Accuracy:0.4423, Validation Loss:1.1450, Validation Accuracy:0.3859\n",
    "Epoch #274: Loss:1.0247, Accuracy:0.4563, Validation Loss:1.1466, Validation Accuracy:0.3744\n",
    "Epoch #275: Loss:1.0217, Accuracy:0.4550, Validation Loss:1.1583, Validation Accuracy:0.3908\n",
    "Epoch #276: Loss:1.0228, Accuracy:0.4554, Validation Loss:1.1583, Validation Accuracy:0.3875\n",
    "Epoch #277: Loss:1.0205, Accuracy:0.4517, Validation Loss:1.1456, Validation Accuracy:0.3563\n",
    "Epoch #278: Loss:1.0220, Accuracy:0.4604, Validation Loss:1.1564, Validation Accuracy:0.3892\n",
    "Epoch #279: Loss:1.0209, Accuracy:0.4612, Validation Loss:1.1564, Validation Accuracy:0.3842\n",
    "Epoch #280: Loss:1.0202, Accuracy:0.4542, Validation Loss:1.1639, Validation Accuracy:0.3580\n",
    "Epoch #281: Loss:1.0236, Accuracy:0.4509, Validation Loss:1.1518, Validation Accuracy:0.3711\n",
    "Epoch #282: Loss:1.0220, Accuracy:0.4497, Validation Loss:1.1527, Validation Accuracy:0.3662\n",
    "Epoch #283: Loss:1.0196, Accuracy:0.4571, Validation Loss:1.1594, Validation Accuracy:0.3695\n",
    "Epoch #284: Loss:1.0202, Accuracy:0.4608, Validation Loss:1.1492, Validation Accuracy:0.3711\n",
    "Epoch #285: Loss:1.0174, Accuracy:0.4608, Validation Loss:1.1800, Validation Accuracy:0.3793\n",
    "Epoch #286: Loss:1.0196, Accuracy:0.4505, Validation Loss:1.1568, Validation Accuracy:0.3645\n",
    "Epoch #287: Loss:1.0191, Accuracy:0.4435, Validation Loss:1.1621, Validation Accuracy:0.3711\n",
    "Epoch #288: Loss:1.0190, Accuracy:0.4497, Validation Loss:1.1801, Validation Accuracy:0.3760\n",
    "Epoch #289: Loss:1.0248, Accuracy:0.4476, Validation Loss:1.1468, Validation Accuracy:0.3695\n",
    "Epoch #290: Loss:1.0196, Accuracy:0.4522, Validation Loss:1.1674, Validation Accuracy:0.3826\n",
    "Epoch #291: Loss:1.0240, Accuracy:0.4587, Validation Loss:1.1509, Validation Accuracy:0.3662\n",
    "Epoch #292: Loss:1.0225, Accuracy:0.4501, Validation Loss:1.1382, Validation Accuracy:0.3842\n",
    "Epoch #293: Loss:1.0248, Accuracy:0.4419, Validation Loss:1.1424, Validation Accuracy:0.3875\n",
    "Epoch #294: Loss:1.0180, Accuracy:0.4456, Validation Loss:1.1530, Validation Accuracy:0.3760\n",
    "Epoch #295: Loss:1.0136, Accuracy:0.4542, Validation Loss:1.1553, Validation Accuracy:0.3744\n",
    "Epoch #296: Loss:1.0217, Accuracy:0.4489, Validation Loss:1.1511, Validation Accuracy:0.3727\n",
    "Epoch #297: Loss:1.0220, Accuracy:0.4505, Validation Loss:1.1445, Validation Accuracy:0.3826\n",
    "Epoch #298: Loss:1.0204, Accuracy:0.4538, Validation Loss:1.1456, Validation Accuracy:0.3695\n",
    "Epoch #299: Loss:1.0225, Accuracy:0.4501, Validation Loss:1.1466, Validation Accuracy:0.3695\n",
    "Epoch #300: Loss:1.0163, Accuracy:0.4567, Validation Loss:1.1518, Validation Accuracy:0.3941\n",
    "\n",
    "Test:\n",
    "Test Loss:1.15176499, Accuracy:0.3941\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "       02  03  01\n",
    "t:02  162   0  65\n",
    "t:03   95   0  47\n",
    "t:01  162   0  78\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.39      0.71      0.50       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.41      0.33      0.36       240\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.27      0.35      0.29       609\n",
    "weighted avg       0.31      0.39      0.33       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 00:03:39 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 41 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.082670582730586, 1.077620239093386, 1.0750113690427958, 1.0754190630513458, 1.074828998795871, 1.075218930816024, 1.0748014046836565, 1.0746554617811306, 1.0749941439855666, 1.0748378065810806, 1.0748737204838268, 1.0751232360971386, 1.0749943364038452, 1.0743270634821875, 1.0741731139826658, 1.0751764517895301, 1.0744948905872789, 1.074336943563765, 1.0742325326687792, 1.0744852573413568, 1.0737435755079798, 1.0738217566400914, 1.0737666050201566, 1.0729916076159047, 1.0747073291753508, 1.075237922676287, 1.0747100406483867, 1.0750714785164017, 1.0745342507933944, 1.0738940976914906, 1.0740297506203988, 1.0741104660754526, 1.0741174316954338, 1.0745632668042613, 1.0753188475795177, 1.07499471262758, 1.0744710060567495, 1.0743549085407227, 1.0747726969726763, 1.0746074306162317, 1.074756598629192, 1.0754995191430028, 1.075339367628489, 1.0749694054154144, 1.075217195136598, 1.0745268382853868, 1.0753791032753555, 1.0758781613191752, 1.0750970515515808, 1.075116183761697, 1.0744917146090804, 1.0747337676034185, 1.074849477738191, 1.0752354684134422, 1.0753577700976669, 1.075855646227381, 1.0751268209886473, 1.075097012010897, 1.0749073130352351, 1.0749528656647906, 1.0756948671513198, 1.075221558901281, 1.07625371031769, 1.0757851798350393, 1.0764180278934672, 1.0769683341870362, 1.0771521261368675, 1.0782273937328695, 1.0778396605270837, 1.0777744961098106, 1.0771160470245311, 1.077634968193881, 1.0770679729912669, 1.0773131414783021, 1.0799297671795673, 1.0836887220639508, 1.080567918583286, 1.0809874681416403, 1.0826194617157108, 1.0796231432697065, 1.0806013396612333, 1.0830975058435024, 1.0803140464478918, 1.0791489177540996, 1.0796617103131925, 1.0797798128550864, 1.0760097722897584, 1.0801216829782245, 1.0794864109975755, 1.0792621622727618, 1.0798169493870977, 1.079814129862292, 1.0862077145740903, 1.087973640470082, 1.0863754232528762, 1.081858325082876, 1.0800959533462775, 1.077563046038836, 1.0773192751779541, 1.075332448204554, 1.0793221811159883, 1.0798372257323492, 1.078137306939988, 1.0785676603051046, 1.078355615362158, 1.0781834959396588, 1.0841886687944284, 1.0853582248703404, 1.1006007775884543, 1.0829607340306875, 1.0875578891663324, 1.0903824988648614, 1.0913725059803678, 1.0899674532252972, 1.08926428206057, 1.0886664257456713, 1.0919612417080131, 1.08920683512351, 1.0938277663464224, 1.0935074157511269, 1.0912946339311271, 1.0906486291994995, 1.0922339404940802, 1.0903858046226313, 1.0978600618678753, 1.092306775021044, 1.096240490528163, 1.0954128903121196, 1.0971765970361644, 1.0971991634134002, 1.0936658981398408, 1.099459585297871, 1.0958850906400257, 1.0942624435440464, 1.0908330927537189, 1.0901201123674515, 1.092455453669105, 1.0883181791978909, 1.0886096498257616, 1.090355001451151, 1.0925325691602108, 1.0931154238962384, 1.097955296388009, 1.0975358118173133, 1.096814083152608, 1.0957100467728864, 1.0979323696424612, 1.0970658330103056, 1.0967266101555284, 1.0947704918083103, 1.0935344455277392, 1.0981796292835855, 1.0962500846248933, 1.1036986502129065, 1.0991500154113143, 1.10710687883969, 1.1060461295257844, 1.1073966057625506, 1.1095375897261897, 1.1031537003117828, 1.1030354294283637, 1.0977873516395957, 1.0973035752871159, 1.1007907848640028, 1.1078776091777633, 1.1068156356686245, 1.101404368388046, 1.098695305767905, 1.0966832279571759, 1.0985378829520716, 1.104209805748537, 1.1008494708729886, 1.1010952761216313, 1.1012897051026669, 1.1043560739808482, 1.0963692905867628, 1.0920355762362677, 1.0939706313394757, 1.099620789142665, 1.0989082374400498, 1.1005603280560723, 1.1082299916419294, 1.1110720701014076, 1.1102842548602125, 1.11598420358448, 1.108107957933924, 1.109911683157747, 1.1034102071877967, 1.1081081256882115, 1.111986177308219, 1.1108142411571809, 1.0982959070816416, 1.1033635689511478, 1.102711660717117, 1.106751419836273, 1.1078064075635963, 1.113327042809848, 1.114314171481015, 1.111188940422484, 1.1170006418854537, 1.1082985724134398, 1.117760288500042, 1.1076297671924085, 1.110513399387228, 1.1026400570407486, 1.0994860010194074, 1.1066638025744209, 1.10757785223192, 1.1116525475027526, 1.1097297012708065, 1.1125168025200003, 1.110263369353534, 1.1096765049572648, 1.1114479600893845, 1.1175544494870064, 1.1194704346273137, 1.1191522568121723, 1.1224651432585442, 1.12118881754883, 1.1419245820914583, 1.1220350271375308, 1.1301865092247774, 1.1289025668440194, 1.133206776014494, 1.117902388909376, 1.1419330753129104, 1.1187527837424442, 1.135118134307548, 1.1380715117665934, 1.1314952864826997, 1.1270609309129136, 1.1510503797108316, 1.1325521003240826, 1.1298258895748745, 1.1471170987597437, 1.1314822120228032, 1.1307247606991546, 1.1329212732894471, 1.1338602633311832, 1.120346213209218, 1.123982178558074, 1.1215464307365355, 1.1203338271878622, 1.1257462307737378, 1.119758802485975, 1.135046680181093, 1.1203320312186806, 1.121647234816465, 1.1328003476993205, 1.1265701088803546, 1.1224365437950798, 1.131416346247756, 1.134912161991514, 1.1312613868948274, 1.148626905356722, 1.146040649249636, 1.1439070231808817, 1.1377958531058676, 1.1446397701899211, 1.1388996590925946, 1.1409989524944661, 1.1433086132964085, 1.1385864992251342, 1.1416343581696058, 1.1451070837199395, 1.1426550631452663, 1.1378283357776835, 1.1437740184990643, 1.141410929033126, 1.1364274328369617, 1.146314902062878, 1.1397016197198326, 1.144990325365552, 1.1465591626801515, 1.158334727553507, 1.158269577034197, 1.1456036295601104, 1.156420278236001, 1.1564110304138735, 1.1638814022975603, 1.1517727498350472, 1.152734841814965, 1.1594313030759689, 1.1491562460835147, 1.1799549214749891, 1.1568388754902605, 1.1620669004756634, 1.1801097017203646, 1.1468288199654941, 1.1673982996854484, 1.1509169255962903, 1.138185900028899, 1.1424107103316459, 1.1529968064798315, 1.1552525083419725, 1.1510642091628953, 1.1445461462675448, 1.1456329681603192, 1.1465868364609717, 1.1517649663884455], 'val_acc': [0.3760262717460764, 0.3727421993986139, 0.3973727408790432, 0.3940886685315807, 0.3940886685315807, 0.3990147771017109, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3990147772974569, 0.3957307049499944, 0.39408866921669156, 0.39737274097691616, 0.3924466327004049, 0.3957307051457404, 0.3842364525657961, 0.4006568135201246, 0.3957307049499944, 0.3990147771995839, 0.3891625612337993, 0.3924466329940238, 0.3842364520764312, 0.39080459716284804, 0.3809523797289687, 0.3743842353276627, 0.38916256054868836, 0.38095238002258763, 0.3940886685315807, 0.3825944161473824, 0.39737274136840806, 0.37766830767512516, 0.37766830757725217, 0.3842364520764312, 0.3875205241302747, 0.379310343310555, 0.3825944156580175, 0.3875205240324017, 0.3875205246196396, 0.38095237992471465, 0.3858784882990989, 0.3842364522721771, 0.3760262715503304, 0.38587848820122594, 0.3908045964777372, 0.3908045964777372, 0.3825944159516364, 0.38095237953322275, 0.37931034438715777, 0.3908045967713561, 0.3924466327982779, 0.38587848820122594, 0.3908045965756102, 0.37602627135458444, 0.38259441585376347, 0.39901477739532987, 0.3743842353276627, 0.39901477739532987, 0.3825944160495094, 0.410509030562512, 0.3957307053414863, 0.38916256064656135, 0.39408866931456454, 0.40394088547609514, 0.38752052481538557, 0.38916256074443434, 0.3743842353276627, 0.39080459716284804, 0.37110016346956515, 0.38423645266366907, 0.40229884993853826, 0.37274219920286794, 0.37274219910499495, 0.40065681371587053, 0.38095238002258763, 0.39080459667348316, 0.37766830747937924, 0.40558292209025476, 0.3760262715503304, 0.40065681371587053, 0.37766830777299815, 0.39080459706497506, 0.3842364518806852, 0.3940886690209456, 0.3809523802183336, 0.3875205247175126, 0.3990147776889488, 0.3858784882990989, 0.39737274136840806, 0.3760262715503304, 0.3990147775910758, 0.3875205246196396, 0.3842364523700501, 0.37931034311480905, 0.3875205241302747, 0.3908045967713561, 0.3875205245217666, 0.3825944160495094, 0.39408866911881857, 0.3875205245217666, 0.3825944159516364, 0.4022988498406653, 0.3858784882990989, 0.38095237982684166, 0.3990147774932028, 0.3875205244238936, 0.3858784882990989, 0.3940886686294537, 0.38916256035294244, 0.37438423562128165, 0.38916256074443434, 0.38095237953322275, 0.3825944161473824, 0.3842364520764312, 0.3908045967713561, 0.3793103438977929, 0.3809523797289687, 0.3842364520764312, 0.38916256064656135, 0.3858784882990989, 0.39408866921669156, 0.3957307053414863, 0.39408866931456454, 0.4039408858675871, 0.38587848859271784, 0.3842364519785582, 0.38916256074443434, 0.3875205244238936, 0.39080459706497506, 0.3842364525657961, 0.3875205244238936, 0.38259441624525536, 0.38095237963109574, 0.38095237992471465, 0.3908045965756102, 0.3842364520764312, 0.3825944159516364, 0.39408866941243753, 0.3793103437020469, 0.3957307051457404, 0.3908045968692291, 0.38916256064656135, 0.3924466327982779, 0.3924466329940238, 0.3875205247175126, 0.39408866911881857, 0.3924466330918968, 0.38095238002258763, 0.3793103438977929, 0.39408866921669156, 0.38916256064656135, 0.3908045967713561, 0.39244663289615084, 0.37931034321268203, 0.38752052383665575, 0.3908045961841182, 0.3940886690209456, 0.39408866882519966, 0.3875205243260207, 0.3940886690209456, 0.38916256074443434, 0.3957307048521214, 0.3908045968692291, 0.3957307053414863, 0.3957307053414863, 0.3924466332876428, 0.3924466331897698, 0.3957307054393593, 0.40558292228600074, 0.39408866931456454, 0.39901477778682176, 0.40065681371587053, 0.3858784887884638, 0.4039408858675871, 0.3842364522721771, 0.39901477778682176, 0.39573070563510526, 0.38259441624525536, 0.37931034409353886, 0.3842364522721771, 0.3776683079687441, 0.3891625609401803, 0.38587848859271784, 0.38259441624525536, 0.3711001632738192, 0.36288998264984546, 0.3842364522721771, 0.37766830787087113, 0.3809523803162066, 0.3842364525657961, 0.3727421995943599, 0.3842364525657961, 0.3891625609401803, 0.3743842358170276, 0.37602627203969535, 0.36288998284559143, 0.3727421996922329, 0.37438423571915463, 0.3842364524679231, 0.38259441634312835, 0.3924466327982779, 0.38587848800547997, 0.38095237982684166, 0.37766830757725217, 0.40722495860654145, 0.38916256054868836, 0.3793103438977929, 0.3908045967713561, 0.38095237982684166, 0.39080459755433994, 0.3809523804140796, 0.3858784888863368, 0.38423645266366907, 0.3858784887884638, 0.39408866525283587, 0.39080459745646695, 0.38752052520687746, 0.3776683079687441, 0.3924466335812617, 0.37110016376318405, 0.3973727420535189, 0.38587848820122594, 0.3842364524679231, 0.39080459667348316, 0.3842364525657961, 0.3875205246196396, 0.4006568138137435, 0.38587848810335296, 0.37766830738150625, 0.3793103439956659, 0.410509030562512, 0.3809523802183336, 0.40886699414409833, 0.3842364524679231, 0.37438423571915463, 0.3809523804140796, 0.3842364525657961, 0.3776683080666171, 0.3875205247175126, 0.3776683081644901, 0.3678160911221027, 0.3727421994964869, 0.38423645217430413, 0.3711001632738192, 0.3875205245217666, 0.3743842358170276, 0.3727421999858518, 0.37931034428928484, 0.37602627194182237, 0.3809523805119525, 0.3694581273447704, 0.3842364525657961, 0.38423645276154206, 0.3924466329940238, 0.36781609121997566, 0.3842364523700501, 0.3858784887884638, 0.3891625612337993, 0.3990147780804407, 0.4022988503300302, 0.3875205247175126, 0.3825944165388743, 0.39080459726072103, 0.3858784887884638, 0.3875205246196396, 0.39408866931456454, 0.3793103437020469, 0.3743842352297897, 0.3711001630780732, 0.372742199007122, 0.3842364523700501, 0.3743842350340438, 0.38259441624525536, 0.38587848859271784, 0.37438423571915463, 0.39080459696710207, 0.3875205247175126, 0.3563218379549205, 0.38916256074443434, 0.3842364523700501, 0.35796387417758824, 0.37110016337169216, 0.3661740550951809, 0.3694581274426434, 0.3711001632738192, 0.37931034409353886, 0.3645320188725132, 0.3711001632738192, 0.3760262716482034, 0.3694581267575325, 0.3825944166367473, 0.36617405480156195, 0.3842364522721771, 0.3875205247175126, 0.3760262717460764, 0.37438423552340866, 0.3727421995943599, 0.3825944165388743, 0.3694581272468974, 0.3694581273447704, 0.39408866921669156], 'loss': [1.0896254851343206, 1.080449373570311, 1.0751114216183735, 1.07416669421617, 1.074227644971264, 1.0744857457628976, 1.0741100885540062, 1.0739876755943534, 1.0738198533929593, 1.0733953285021458, 1.0734561570615984, 1.0734716091802232, 1.0731299738619606, 1.0732318138439798, 1.072671423017122, 1.072774378915587, 1.0727476452410343, 1.0725107884260174, 1.073220229589474, 1.073393665153143, 1.0725314147173746, 1.0726135243135801, 1.0722215333268872, 1.0727699254327732, 1.0733513677879036, 1.0729380178255712, 1.0734907910074787, 1.074021084254772, 1.0745217922042283, 1.0732907191928651, 1.0729201097507985, 1.0729978916826188, 1.0727799794512363, 1.072443024382699, 1.072913853833318, 1.0726627087446208, 1.0726271776203258, 1.0723505085743428, 1.0722494015703457, 1.0720448391148687, 1.0721199643195778, 1.072132654797125, 1.0722328928217018, 1.071711428650106, 1.0715710530290858, 1.0713031479954964, 1.0733083631468505, 1.0720960388927734, 1.0728403953556163, 1.072921394077904, 1.0720904378186016, 1.0719847458343976, 1.0713591454210223, 1.071842972404903, 1.0718611454816813, 1.0714387297875092, 1.071768516485696, 1.0711350840954321, 1.0708842371033938, 1.0720178836180199, 1.0717420556462032, 1.0713875103290567, 1.070917667402624, 1.07066689565686, 1.0700777410726527, 1.0706164847164428, 1.069750666765217, 1.069722162379866, 1.0707385177240234, 1.0684310071522205, 1.068975688985241, 1.06865355155796, 1.067915944201256, 1.0668123958536733, 1.068187889328238, 1.0671196323155867, 1.0702664024776012, 1.0664420363839402, 1.0666974458361553, 1.0665746249212622, 1.0665961383059774, 1.0706143176531155, 1.068286273905384, 1.0650540708761196, 1.0644906546300932, 1.0641396902425089, 1.063805445116893, 1.06471604473537, 1.0648566548094858, 1.0660628547413882, 1.0614981331130073, 1.063064680060322, 1.0654222872712529, 1.067923960744478, 1.0711174132642804, 1.0696147740499196, 1.067985047452014, 1.0664753852683662, 1.0638648610829817, 1.0640711031410484, 1.0621797296300806, 1.0631805064007487, 1.0619410907952938, 1.0604759912960828, 1.0604432099164145, 1.0608262361687066, 1.060169819199329, 1.0612874698834742, 1.0565584508790127, 1.0675784561423551, 1.0597917936176244, 1.0592453919886564, 1.0612357919465834, 1.0574281736565811, 1.0582192611890162, 1.0575471004666244, 1.0565501632142116, 1.0580708906635856, 1.058261811659811, 1.060966731047973, 1.0620146275545783, 1.0595095599701272, 1.0612433263163792, 1.059283662233999, 1.060353199020793, 1.059430048548955, 1.0586952920322301, 1.0569791716471835, 1.0559169904896855, 1.0568164515544256, 1.056211851705516, 1.0571707893445996, 1.0537514437395445, 1.054135099769373, 1.0577997599783864, 1.0591239096447673, 1.061435388539606, 1.0606679751398138, 1.0599540590016014, 1.0578446012747607, 1.057519499721958, 1.0569009240403067, 1.054877564892387, 1.0545297449374347, 1.0555374482818698, 1.056532375376817, 1.0565984599644154, 1.0553895653640466, 1.0548754976515409, 1.0558946887570484, 1.0562028431060133, 1.0537555205014206, 1.0539400057136645, 1.0537597064854427, 1.0539653480420121, 1.053689302458166, 1.0522183045224731, 1.0525279236525236, 1.0506062370061384, 1.0516845248318305, 1.0531253841133823, 1.0542072958525202, 1.0526800874811912, 1.0509972068073814, 1.0505236260944812, 1.0518322908413238, 1.0518457680512259, 1.050990591989161, 1.0507031247356344, 1.0496055887465114, 1.0483592848513406, 1.0488083292571426, 1.0486610849779978, 1.0509355611624904, 1.0520069614328154, 1.0511446601801095, 1.0501234741915912, 1.049184514266999, 1.047177232217495, 1.0496046702719812, 1.0466716072642583, 1.0463277842719452, 1.049660578302779, 1.0513088915627105, 1.0519539860484537, 1.0493460128439525, 1.052259428202494, 1.0508548259245543, 1.049938873686585, 1.0492170394078906, 1.0469285550303529, 1.0464476736174473, 1.045107955462634, 1.0473142064327576, 1.0442701886566756, 1.0439574766452797, 1.041377862767762, 1.043154207881716, 1.0437024825407495, 1.0436538852950141, 1.0490668833867725, 1.0451018052424248, 1.0460568945510675, 1.0447421183576329, 1.045745872276275, 1.0511248802991864, 1.0482983314525909, 1.0418702507900262, 1.0455911763639665, 1.0413418095214655, 1.0396394799622177, 1.0434153830491053, 1.044122302262935, 1.0404718519970622, 1.0411380167614508, 1.0395875893579127, 1.0378349140684218, 1.0393257184195077, 1.03804647834394, 1.0381445503332776, 1.0386575978884218, 1.0462729962209902, 1.0394482966565988, 1.0418128571226366, 1.0333606938317081, 1.0301694229398175, 1.0346231979511111, 1.0327164488406642, 1.0305007365694772, 1.0280864978473045, 1.0292961634649633, 1.025598739011087, 1.0290627004674329, 1.0349789287030575, 1.032388664956455, 1.0327821238084984, 1.034424217623607, 1.0327947743864274, 1.0323522105598841, 1.0316813869887553, 1.0337654663552005, 1.0366997573899537, 1.0322517593783276, 1.033336226553398, 1.0289225894567657, 1.0321545926941984, 1.0291144763664544, 1.0319139350855864, 1.0301078563353365, 1.0287025837927628, 1.0283129843843057, 1.026457953208281, 1.0302812073999361, 1.0296968047378978, 1.0296513580933244, 1.0299512581658803, 1.0276536116120263, 1.0289539554035885, 1.0280472697663356, 1.025782116580548, 1.0277266241931329, 1.025019957984987, 1.0289393563045368, 1.0227475435092463, 1.0213055323281572, 1.0217219332649967, 1.023475366010803, 1.0256271944398507, 1.0334342015597364, 1.0293216902617313, 1.0296425100224709, 1.0263668196891613, 1.026197747870882, 1.0247456006690463, 1.0216694582659116, 1.0228104850349975, 1.0205213673060924, 1.0219649907255075, 1.0209013248126364, 1.0201820548310172, 1.0236191563537722, 1.0220129402755957, 1.019598926117288, 1.0202288220795273, 1.0174100623728068, 1.0195763623689968, 1.0190802957487792, 1.018963241968801, 1.024821043406179, 1.0195795648151844, 1.0239855332541026, 1.0225041296937383, 1.024759643670225, 1.0179512244230424, 1.0136342450089033, 1.021667509059397, 1.0220142112865096, 1.020376833459435, 1.0224808069225209, 1.0162656773776735], 'acc': [0.3712525665148083, 0.3749486662523947, 0.380287475091476, 0.3958932243823026, 0.39425051391002336, 0.3913757684294448, 0.39383983707525894, 0.3942505156724605, 0.39548254735171184, 0.394250514301676, 0.3958932231706271, 0.41108829406007846, 0.39876796907957573, 0.3926078012469368, 0.4041067765478726, 0.4032854232699964, 0.4041067772944605, 0.40451745217096147, 0.3979466138434361, 0.3893223822973592, 0.40739219804319265, 0.4036961004964135, 0.40657084398201115, 0.39712525582901015, 0.38891170448346307, 0.3963039031753305, 0.3926078020302422, 0.395071869733642, 0.3926078040252231, 0.39671457938589844, 0.3975359348178644, 0.39753593207629556, 0.4016427090402991, 0.39260780382939675, 0.39425051171921605, 0.40492813135564204, 0.4020533882616971, 0.40287474189450856, 0.39712525524153114, 0.39958932176997286, 0.3950718705169474, 0.3946611889456332, 0.4036960991256291, 0.4057494846335182, 0.3987679673171386, 0.3991786461101665, 0.40533880681962203, 0.39301848164329295, 0.3950718669553557, 0.40657084006548416, 0.4065708402613105, 0.404106777918657, 0.40041067618608966, 0.40616016362237245, 0.4016427098603219, 0.3991786447393821, 0.3975359324679482, 0.4036960965431691, 0.40082135579914036, 0.39671458138087934, 0.3979466106734971, 0.406160165421527, 0.40533880721127474, 0.40821355233691803, 0.4160164260643953, 0.3971252564532067, 0.4201232022450935, 0.39999999778471446, 0.4045174543617687, 0.4036960971306482, 0.4057494858084763, 0.3995893211457764, 0.41190965027534987, 0.40041067540278424, 0.4160164276310061, 0.4065708420604651, 0.4139630393815481, 0.41314168371703835, 0.4119096500795235, 0.41642710489414064, 0.4160164272393534, 0.39014373612599695, 0.4114989736731292, 0.41273100786140565, 0.4229979461590612, 0.4201232045950096, 0.42012320302839884, 0.41642710489414064, 0.40780287350717265, 0.4205338826414496, 0.42422997744665986, 0.4160164272393534, 0.415195073606542, 0.40657084186463877, 0.40862423073829324, 0.4057494874118045, 0.41806981513387614, 0.4082135507335898, 0.418891168803405, 0.41396303679908814, 0.42094455650210133, 0.40287474209033486, 0.4106776187919249, 0.4168377801255769, 0.4156057472713674, 0.4184804925561196, 0.42422997842579163, 0.4086242279600069, 0.4229979473340193, 0.4049281292015522, 0.4119096514870254, 0.427104722143933, 0.41108829644671213, 0.423819303745117, 0.4164271040741178, 0.4258726879189391, 0.42053388084229504, 0.4172484618927174, 0.42710472335560856, 0.41930184583399577, 0.42176591212989367, 0.4188911703700158, 0.41108829386425216, 0.42422997979657606, 0.4160164250852636, 0.42012320302839884, 0.41601642782683246, 0.42710472292723844, 0.42833675636892693, 0.4275154007411346, 0.4193018483797383, 0.4086242281925507, 0.4238193031943554, 0.42505133327027855, 0.41601642547691625, 0.41930184841645574, 0.4102669402314407, 0.41683778466630034, 0.4172484609135857, 0.4217659129131991, 0.42094456022280197, 0.4217659145165273, 0.41724846169689106, 0.42094455806871217, 0.4184804917728142, 0.4229979485089774, 0.4098562649632871, 0.4201232056108588, 0.4250513370276966, 0.41355236211841356, 0.41848049059785614, 0.42505133624439123, 0.42381930257015893, 0.41437371758709696, 0.42012320126596175, 0.4188911701741894, 0.42464065643551413, 0.4225872697159495, 0.4250513360485649, 0.42997946429546363, 0.4266940437425578, 0.42176591588731177, 0.4242299811673605, 0.4320328551273816, 0.43244353137466696, 0.4217659142839835, 0.4303901448509287, 0.43203285258163904, 0.43449692161910586, 0.43203285591068696, 0.43326488758993836, 0.4353182739178503, 0.4369609859567403, 0.42381930280270275, 0.4332648870024593, 0.4287474314412542, 0.4242299782299653, 0.42217659307701144, 0.432854207426126, 0.42751539897869745, 0.43942505264429094, 0.43696098439012954, 0.4312115002828947, 0.41848049275194593, 0.4172484598977365, 0.4303901442634496, 0.42217659154711806, 0.4221765933095552, 0.40985626398415537, 0.4246406580021249, 0.44106776076665405, 0.42956879078974713, 0.4303901452792988, 0.4316221761385273, 0.43983573104566615, 0.44271047440887235, 0.4439425028814672, 0.4484599580877371, 0.44271047061473684, 0.4361396287256198, 0.42751540254028914, 0.43326488880161385, 0.4197125261936344, 0.4303901452425814, 0.4332648881774174, 0.42012320146178805, 0.40451745573255316, 0.44476385991676143, 0.4406570835035195, 0.44640657273895684, 0.4455852157403801, 0.4340862440377535, 0.43778234021374823, 0.43983572709242175, 0.4459958953534308, 0.4377823424045555, 0.4344969200524951, 0.43490759946971946, 0.4369609871316984, 0.44106776037500134, 0.44147843873965914, 0.4283367575806024, 0.43613963130807976, 0.42997946723285885, 0.4328542089927368, 0.4472279283667492, 0.43819301704851266, 0.44394250487644815, 0.4414784403797048, 0.44969198973027097, 0.4390143748303948, 0.4443531834736497, 0.44681725039374415, 0.4254620108883484, 0.43942505049020114, 0.4447638589376297, 0.43285421020441234, 0.4381930166201425, 0.4443531821028653, 0.4472279238260257, 0.4501026697349744, 0.433264887394112, 0.42422997783831257, 0.4316221767260064, 0.43819302014501677, 0.446406569997388, 0.4435318262792466, 0.4353182736853065, 0.4451745393339858, 0.449281313519703, 0.4533880914628383, 0.44887063449413134, 0.45010266832747253, 0.447638605984819, 0.4599589335477817, 0.45051334477058425, 0.45215605841280254, 0.4558521543929709, 0.4410677609624804, 0.455852158150389, 0.4418891173735781, 0.4529774150197266, 0.45174537883646926, 0.4517453792281219, 0.45092402258448044, 0.45379876966838717, 0.4427104739805022, 0.44106776174578577, 0.44065708631852324, 0.4299794654704217, 0.4369609855650876, 0.44887063766407037, 0.4422997944041689, 0.45626283201104073, 0.4550308017392912, 0.4554414807281455, 0.451745381149668, 0.46036961194915693, 0.4611909644437277, 0.45420944513236716, 0.45092402555859307, 0.4496919899260973, 0.4570841900254667, 0.46078028737641946, 0.4607802895672267, 0.45051334696139156, 0.4435318288249891, 0.44969198973027097, 0.4476386037940117, 0.45215605543868986, 0.4587269003019196, 0.45010266914749536, 0.4418891185485362, 0.445585214565422, 0.4542094459156726, 0.44887063883902845, 0.450513348919655, 0.45379876673099195, 0.45010266856001635, 0.45667351201574413]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
