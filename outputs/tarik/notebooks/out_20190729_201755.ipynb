{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf20.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 20:17:55 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'Split', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 5 Label(s): ['03', '04', '05', '02', '01'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000029B802A1E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000029B9F427EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6101, Accuracy:0.2164, Validation Loss:1.6065, Validation Accuracy:0.2323\n",
    "Epoch #2: Loss:1.6062, Accuracy:0.2330, Validation Loss:1.6057, Validation Accuracy:0.2328\n",
    "Epoch #3: Loss:1.6049, Accuracy:0.2330, Validation Loss:1.6049, Validation Accuracy:0.2328\n",
    "Epoch #4: Loss:1.6040, Accuracy:0.2330, Validation Loss:1.6043, Validation Accuracy:0.2328\n",
    "Epoch #5: Loss:1.6034, Accuracy:0.2330, Validation Loss:1.6036, Validation Accuracy:0.2344\n",
    "Epoch #6: Loss:1.6029, Accuracy:0.2400, Validation Loss:1.6046, Validation Accuracy:0.2328\n",
    "Epoch #7: Loss:1.6035, Accuracy:0.2344, Validation Loss:1.6045, Validation Accuracy:0.2438\n",
    "Epoch #8: Loss:1.6026, Accuracy:0.2390, Validation Loss:1.6037, Validation Accuracy:0.2340\n",
    "Epoch #9: Loss:1.6025, Accuracy:0.2342, Validation Loss:1.6029, Validation Accuracy:0.2418\n",
    "Epoch #10: Loss:1.6021, Accuracy:0.2376, Validation Loss:1.6025, Validation Accuracy:0.2447\n",
    "Epoch #11: Loss:1.6015, Accuracy:0.2420, Validation Loss:1.6023, Validation Accuracy:0.2438\n",
    "Epoch #12: Loss:1.6007, Accuracy:0.2439, Validation Loss:1.6022, Validation Accuracy:0.2459\n",
    "Epoch #13: Loss:1.6011, Accuracy:0.2443, Validation Loss:1.6022, Validation Accuracy:0.2467\n",
    "Epoch #14: Loss:1.6000, Accuracy:0.2448, Validation Loss:1.6027, Validation Accuracy:0.2434\n",
    "Epoch #15: Loss:1.6003, Accuracy:0.2403, Validation Loss:1.6023, Validation Accuracy:0.2434\n",
    "Epoch #16: Loss:1.5997, Accuracy:0.2437, Validation Loss:1.6021, Validation Accuracy:0.2463\n",
    "Epoch #17: Loss:1.5993, Accuracy:0.2439, Validation Loss:1.6020, Validation Accuracy:0.2455\n",
    "Epoch #18: Loss:1.5990, Accuracy:0.2432, Validation Loss:1.6023, Validation Accuracy:0.2443\n",
    "Epoch #19: Loss:1.5985, Accuracy:0.2454, Validation Loss:1.6018, Validation Accuracy:0.2430\n",
    "Epoch #20: Loss:1.5992, Accuracy:0.2448, Validation Loss:1.6022, Validation Accuracy:0.2393\n",
    "Epoch #21: Loss:1.6001, Accuracy:0.2414, Validation Loss:1.6026, Validation Accuracy:0.2406\n",
    "Epoch #22: Loss:1.6003, Accuracy:0.2421, Validation Loss:1.6018, Validation Accuracy:0.2410\n",
    "Epoch #23: Loss:1.5995, Accuracy:0.2368, Validation Loss:1.6017, Validation Accuracy:0.2471\n",
    "Epoch #24: Loss:1.5990, Accuracy:0.2399, Validation Loss:1.6014, Validation Accuracy:0.2438\n",
    "Epoch #25: Loss:1.5990, Accuracy:0.2408, Validation Loss:1.6017, Validation Accuracy:0.2356\n",
    "Epoch #26: Loss:1.5985, Accuracy:0.2399, Validation Loss:1.6007, Validation Accuracy:0.2406\n",
    "Epoch #27: Loss:1.5985, Accuracy:0.2358, Validation Loss:1.6002, Validation Accuracy:0.2348\n",
    "Epoch #28: Loss:1.5979, Accuracy:0.2381, Validation Loss:1.6000, Validation Accuracy:0.2385\n",
    "Epoch #29: Loss:1.5985, Accuracy:0.2390, Validation Loss:1.6002, Validation Accuracy:0.2369\n",
    "Epoch #30: Loss:1.5975, Accuracy:0.2406, Validation Loss:1.6003, Validation Accuracy:0.2356\n",
    "Epoch #31: Loss:1.5982, Accuracy:0.2393, Validation Loss:1.6004, Validation Accuracy:0.2369\n",
    "Epoch #32: Loss:1.6009, Accuracy:0.2325, Validation Loss:1.6034, Validation Accuracy:0.2278\n",
    "Epoch #33: Loss:1.6004, Accuracy:0.2333, Validation Loss:1.6015, Validation Accuracy:0.2295\n",
    "Epoch #34: Loss:1.6014, Accuracy:0.2298, Validation Loss:1.6016, Validation Accuracy:0.2397\n",
    "Epoch #35: Loss:1.6004, Accuracy:0.2408, Validation Loss:1.6008, Validation Accuracy:0.2385\n",
    "Epoch #36: Loss:1.6005, Accuracy:0.2457, Validation Loss:1.6012, Validation Accuracy:0.2323\n",
    "Epoch #37: Loss:1.5993, Accuracy:0.2478, Validation Loss:1.6003, Validation Accuracy:0.2295\n",
    "Epoch #38: Loss:1.5992, Accuracy:0.2374, Validation Loss:1.6003, Validation Accuracy:0.2332\n",
    "Epoch #39: Loss:1.5984, Accuracy:0.2385, Validation Loss:1.6005, Validation Accuracy:0.2319\n",
    "Epoch #40: Loss:1.5991, Accuracy:0.2389, Validation Loss:1.6014, Validation Accuracy:0.2315\n",
    "Epoch #41: Loss:1.5993, Accuracy:0.2364, Validation Loss:1.6007, Validation Accuracy:0.2352\n",
    "Epoch #42: Loss:1.5984, Accuracy:0.2386, Validation Loss:1.6004, Validation Accuracy:0.2356\n",
    "Epoch #43: Loss:1.5984, Accuracy:0.2400, Validation Loss:1.6004, Validation Accuracy:0.2291\n",
    "Epoch #44: Loss:1.5986, Accuracy:0.2417, Validation Loss:1.5999, Validation Accuracy:0.2315\n",
    "Epoch #45: Loss:1.5982, Accuracy:0.2452, Validation Loss:1.6002, Validation Accuracy:0.2254\n",
    "Epoch #46: Loss:1.5980, Accuracy:0.2380, Validation Loss:1.6004, Validation Accuracy:0.2377\n",
    "Epoch #47: Loss:1.5992, Accuracy:0.2354, Validation Loss:1.6002, Validation Accuracy:0.2344\n",
    "Epoch #48: Loss:1.5978, Accuracy:0.2388, Validation Loss:1.6005, Validation Accuracy:0.2323\n",
    "Epoch #49: Loss:1.5983, Accuracy:0.2402, Validation Loss:1.6007, Validation Accuracy:0.2299\n",
    "Epoch #50: Loss:1.5988, Accuracy:0.2417, Validation Loss:1.5998, Validation Accuracy:0.2237\n",
    "Epoch #51: Loss:1.5974, Accuracy:0.2413, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #52: Loss:1.5982, Accuracy:0.2445, Validation Loss:1.5998, Validation Accuracy:0.2389\n",
    "Epoch #53: Loss:1.5974, Accuracy:0.2451, Validation Loss:1.5998, Validation Accuracy:0.2315\n",
    "Epoch #54: Loss:1.5981, Accuracy:0.2435, Validation Loss:1.6000, Validation Accuracy:0.2323\n",
    "Epoch #55: Loss:1.5976, Accuracy:0.2448, Validation Loss:1.5998, Validation Accuracy:0.2328\n",
    "Epoch #56: Loss:1.5973, Accuracy:0.2450, Validation Loss:1.5998, Validation Accuracy:0.2418\n",
    "Epoch #57: Loss:1.5976, Accuracy:0.2472, Validation Loss:1.5996, Validation Accuracy:0.2369\n",
    "Epoch #58: Loss:1.5980, Accuracy:0.2487, Validation Loss:1.6002, Validation Accuracy:0.2328\n",
    "Epoch #59: Loss:1.5974, Accuracy:0.2424, Validation Loss:1.5998, Validation Accuracy:0.2352\n",
    "Epoch #60: Loss:1.5983, Accuracy:0.2466, Validation Loss:1.6002, Validation Accuracy:0.2430\n",
    "Epoch #61: Loss:1.5974, Accuracy:0.2482, Validation Loss:1.6003, Validation Accuracy:0.2438\n",
    "Epoch #62: Loss:1.5971, Accuracy:0.2487, Validation Loss:1.5998, Validation Accuracy:0.2393\n",
    "Epoch #63: Loss:1.5995, Accuracy:0.2373, Validation Loss:1.6006, Validation Accuracy:0.2323\n",
    "Epoch #64: Loss:1.5977, Accuracy:0.2413, Validation Loss:1.5998, Validation Accuracy:0.2344\n",
    "Epoch #65: Loss:1.5979, Accuracy:0.2497, Validation Loss:1.5998, Validation Accuracy:0.2389\n",
    "Epoch #66: Loss:1.5959, Accuracy:0.2494, Validation Loss:1.5992, Validation Accuracy:0.2492\n",
    "Epoch #67: Loss:1.5969, Accuracy:0.2444, Validation Loss:1.5990, Validation Accuracy:0.2373\n",
    "Epoch #68: Loss:1.5974, Accuracy:0.2488, Validation Loss:1.5988, Validation Accuracy:0.2406\n",
    "Epoch #69: Loss:1.5966, Accuracy:0.2459, Validation Loss:1.5986, Validation Accuracy:0.2397\n",
    "Epoch #70: Loss:1.5966, Accuracy:0.2494, Validation Loss:1.5995, Validation Accuracy:0.2410\n",
    "Epoch #71: Loss:1.5975, Accuracy:0.2396, Validation Loss:1.5989, Validation Accuracy:0.2295\n",
    "Epoch #72: Loss:1.5966, Accuracy:0.2473, Validation Loss:1.5993, Validation Accuracy:0.2328\n",
    "Epoch #73: Loss:1.5973, Accuracy:0.2432, Validation Loss:1.5992, Validation Accuracy:0.2430\n",
    "Epoch #74: Loss:1.5958, Accuracy:0.2466, Validation Loss:1.5989, Validation Accuracy:0.2414\n",
    "Epoch #75: Loss:1.5963, Accuracy:0.2470, Validation Loss:1.5989, Validation Accuracy:0.2311\n",
    "Epoch #76: Loss:1.5959, Accuracy:0.2508, Validation Loss:1.5992, Validation Accuracy:0.2401\n",
    "Epoch #77: Loss:1.5970, Accuracy:0.2479, Validation Loss:1.5990, Validation Accuracy:0.2406\n",
    "Epoch #78: Loss:1.5957, Accuracy:0.2478, Validation Loss:1.5997, Validation Accuracy:0.2323\n",
    "Epoch #79: Loss:1.5960, Accuracy:0.2518, Validation Loss:1.5987, Validation Accuracy:0.2365\n",
    "Epoch #80: Loss:1.5951, Accuracy:0.2525, Validation Loss:1.5980, Validation Accuracy:0.2422\n",
    "Epoch #81: Loss:1.5961, Accuracy:0.2503, Validation Loss:1.5991, Validation Accuracy:0.2430\n",
    "Epoch #82: Loss:1.5948, Accuracy:0.2552, Validation Loss:1.5988, Validation Accuracy:0.2418\n",
    "Epoch #83: Loss:1.5964, Accuracy:0.2509, Validation Loss:1.5991, Validation Accuracy:0.2422\n",
    "Epoch #84: Loss:1.5951, Accuracy:0.2488, Validation Loss:1.5992, Validation Accuracy:0.2393\n",
    "Epoch #85: Loss:1.5954, Accuracy:0.2475, Validation Loss:1.5985, Validation Accuracy:0.2377\n",
    "Epoch #86: Loss:1.5945, Accuracy:0.2542, Validation Loss:1.5983, Validation Accuracy:0.2418\n",
    "Epoch #87: Loss:1.5948, Accuracy:0.2537, Validation Loss:1.5983, Validation Accuracy:0.2434\n",
    "Epoch #88: Loss:1.5944, Accuracy:0.2552, Validation Loss:1.5983, Validation Accuracy:0.2418\n",
    "Epoch #89: Loss:1.5949, Accuracy:0.2531, Validation Loss:1.5984, Validation Accuracy:0.2348\n",
    "Epoch #90: Loss:1.5959, Accuracy:0.2508, Validation Loss:1.5985, Validation Accuracy:0.2307\n",
    "Epoch #91: Loss:1.5957, Accuracy:0.2449, Validation Loss:1.5980, Validation Accuracy:0.2414\n",
    "Epoch #92: Loss:1.5955, Accuracy:0.2464, Validation Loss:1.5981, Validation Accuracy:0.2389\n",
    "Epoch #93: Loss:1.5951, Accuracy:0.2465, Validation Loss:1.5980, Validation Accuracy:0.2434\n",
    "Epoch #94: Loss:1.5955, Accuracy:0.2497, Validation Loss:1.5976, Validation Accuracy:0.2270\n",
    "Epoch #95: Loss:1.5951, Accuracy:0.2474, Validation Loss:1.5974, Validation Accuracy:0.2389\n",
    "Epoch #96: Loss:1.5946, Accuracy:0.2464, Validation Loss:1.5972, Validation Accuracy:0.2422\n",
    "Epoch #97: Loss:1.5944, Accuracy:0.2488, Validation Loss:1.5976, Validation Accuracy:0.2447\n",
    "Epoch #98: Loss:1.5941, Accuracy:0.2494, Validation Loss:1.5973, Validation Accuracy:0.2369\n",
    "Epoch #99: Loss:1.5939, Accuracy:0.2491, Validation Loss:1.5985, Validation Accuracy:0.2438\n",
    "Epoch #100: Loss:1.5946, Accuracy:0.2446, Validation Loss:1.5975, Validation Accuracy:0.2373\n",
    "Epoch #101: Loss:1.5949, Accuracy:0.2497, Validation Loss:1.5986, Validation Accuracy:0.2406\n",
    "Epoch #102: Loss:1.5937, Accuracy:0.2516, Validation Loss:1.5982, Validation Accuracy:0.2430\n",
    "Epoch #103: Loss:1.5958, Accuracy:0.2459, Validation Loss:1.5985, Validation Accuracy:0.2291\n",
    "Epoch #104: Loss:1.5934, Accuracy:0.2480, Validation Loss:1.5978, Validation Accuracy:0.2443\n",
    "Epoch #105: Loss:1.5940, Accuracy:0.2472, Validation Loss:1.5985, Validation Accuracy:0.2438\n",
    "Epoch #106: Loss:1.5943, Accuracy:0.2500, Validation Loss:1.5975, Validation Accuracy:0.2348\n",
    "Epoch #107: Loss:1.5935, Accuracy:0.2458, Validation Loss:1.5973, Validation Accuracy:0.2406\n",
    "Epoch #108: Loss:1.5937, Accuracy:0.2492, Validation Loss:1.5981, Validation Accuracy:0.2393\n",
    "Epoch #109: Loss:1.5949, Accuracy:0.2493, Validation Loss:1.5986, Validation Accuracy:0.2369\n",
    "Epoch #110: Loss:1.5931, Accuracy:0.2479, Validation Loss:1.5987, Validation Accuracy:0.2274\n",
    "Epoch #111: Loss:1.5950, Accuracy:0.2444, Validation Loss:1.5976, Validation Accuracy:0.2434\n",
    "Epoch #112: Loss:1.5936, Accuracy:0.2516, Validation Loss:1.5980, Validation Accuracy:0.2438\n",
    "Epoch #113: Loss:1.5936, Accuracy:0.2518, Validation Loss:1.5988, Validation Accuracy:0.2447\n",
    "Epoch #114: Loss:1.5956, Accuracy:0.2494, Validation Loss:1.5977, Validation Accuracy:0.2262\n",
    "Epoch #115: Loss:1.5935, Accuracy:0.2413, Validation Loss:1.5972, Validation Accuracy:0.2340\n",
    "Epoch #116: Loss:1.5923, Accuracy:0.2530, Validation Loss:1.5966, Validation Accuracy:0.2422\n",
    "Epoch #117: Loss:1.5917, Accuracy:0.2532, Validation Loss:1.5964, Validation Accuracy:0.2406\n",
    "Epoch #118: Loss:1.5920, Accuracy:0.2524, Validation Loss:1.5967, Validation Accuracy:0.2418\n",
    "Epoch #119: Loss:1.5919, Accuracy:0.2503, Validation Loss:1.5975, Validation Accuracy:0.2451\n",
    "Epoch #120: Loss:1.5918, Accuracy:0.2504, Validation Loss:1.5981, Validation Accuracy:0.2258\n",
    "Epoch #121: Loss:1.5920, Accuracy:0.2528, Validation Loss:1.5969, Validation Accuracy:0.2258\n",
    "Epoch #122: Loss:1.5925, Accuracy:0.2460, Validation Loss:1.5977, Validation Accuracy:0.2307\n",
    "Epoch #123: Loss:1.5915, Accuracy:0.2522, Validation Loss:1.5963, Validation Accuracy:0.2443\n",
    "Epoch #124: Loss:1.5922, Accuracy:0.2506, Validation Loss:1.5972, Validation Accuracy:0.2459\n",
    "Epoch #125: Loss:1.5916, Accuracy:0.2488, Validation Loss:1.5959, Validation Accuracy:0.2459\n",
    "Epoch #126: Loss:1.5904, Accuracy:0.2533, Validation Loss:1.5971, Validation Accuracy:0.2245\n",
    "Epoch #127: Loss:1.5916, Accuracy:0.2482, Validation Loss:1.5968, Validation Accuracy:0.2426\n",
    "Epoch #128: Loss:1.5909, Accuracy:0.2515, Validation Loss:1.5975, Validation Accuracy:0.2422\n",
    "Epoch #129: Loss:1.5911, Accuracy:0.2506, Validation Loss:1.5965, Validation Accuracy:0.2373\n",
    "Epoch #130: Loss:1.5912, Accuracy:0.2502, Validation Loss:1.5958, Validation Accuracy:0.2430\n",
    "Epoch #131: Loss:1.5912, Accuracy:0.2476, Validation Loss:1.5965, Validation Accuracy:0.2311\n",
    "Epoch #132: Loss:1.5942, Accuracy:0.2475, Validation Loss:1.5998, Validation Accuracy:0.2484\n",
    "Epoch #133: Loss:1.5931, Accuracy:0.2464, Validation Loss:1.5976, Validation Accuracy:0.2377\n",
    "Epoch #134: Loss:1.5920, Accuracy:0.2478, Validation Loss:1.5989, Validation Accuracy:0.2282\n",
    "Epoch #135: Loss:1.5910, Accuracy:0.2469, Validation Loss:1.5979, Validation Accuracy:0.2447\n",
    "Epoch #136: Loss:1.5905, Accuracy:0.2494, Validation Loss:1.5987, Validation Accuracy:0.2323\n",
    "Epoch #137: Loss:1.5911, Accuracy:0.2433, Validation Loss:1.5987, Validation Accuracy:0.2262\n",
    "Epoch #138: Loss:1.5909, Accuracy:0.2462, Validation Loss:1.5977, Validation Accuracy:0.2323\n",
    "Epoch #139: Loss:1.5894, Accuracy:0.2477, Validation Loss:1.5980, Validation Accuracy:0.2319\n",
    "Epoch #140: Loss:1.5899, Accuracy:0.2523, Validation Loss:1.5970, Validation Accuracy:0.2414\n",
    "Epoch #141: Loss:1.5896, Accuracy:0.2514, Validation Loss:1.5974, Validation Accuracy:0.2434\n",
    "Epoch #142: Loss:1.5899, Accuracy:0.2544, Validation Loss:1.5982, Validation Accuracy:0.2356\n",
    "Epoch #143: Loss:1.5879, Accuracy:0.2534, Validation Loss:1.5976, Validation Accuracy:0.2365\n",
    "Epoch #144: Loss:1.5886, Accuracy:0.2524, Validation Loss:1.5973, Validation Accuracy:0.2455\n",
    "Epoch #145: Loss:1.5908, Accuracy:0.2511, Validation Loss:1.5982, Validation Accuracy:0.2426\n",
    "Epoch #146: Loss:1.5906, Accuracy:0.2509, Validation Loss:1.5995, Validation Accuracy:0.2303\n",
    "Epoch #147: Loss:1.5917, Accuracy:0.2471, Validation Loss:1.5973, Validation Accuracy:0.2406\n",
    "Epoch #148: Loss:1.5894, Accuracy:0.2518, Validation Loss:1.5970, Validation Accuracy:0.2455\n",
    "Epoch #149: Loss:1.5885, Accuracy:0.2577, Validation Loss:1.5976, Validation Accuracy:0.2406\n",
    "Epoch #150: Loss:1.5890, Accuracy:0.2579, Validation Loss:1.5974, Validation Accuracy:0.2500\n",
    "Epoch #151: Loss:1.5884, Accuracy:0.2553, Validation Loss:1.5970, Validation Accuracy:0.2451\n",
    "Epoch #152: Loss:1.5878, Accuracy:0.2502, Validation Loss:1.5975, Validation Accuracy:0.2434\n",
    "Epoch #153: Loss:1.5873, Accuracy:0.2550, Validation Loss:1.5974, Validation Accuracy:0.2451\n",
    "Epoch #154: Loss:1.5877, Accuracy:0.2561, Validation Loss:1.5978, Validation Accuracy:0.2414\n",
    "Epoch #155: Loss:1.5887, Accuracy:0.2530, Validation Loss:1.5984, Validation Accuracy:0.2393\n",
    "Epoch #156: Loss:1.5872, Accuracy:0.2507, Validation Loss:1.5978, Validation Accuracy:0.2443\n",
    "Epoch #157: Loss:1.5893, Accuracy:0.2536, Validation Loss:1.5996, Validation Accuracy:0.2426\n",
    "Epoch #158: Loss:1.5886, Accuracy:0.2527, Validation Loss:1.5995, Validation Accuracy:0.2356\n",
    "Epoch #159: Loss:1.5880, Accuracy:0.2569, Validation Loss:1.5984, Validation Accuracy:0.2459\n",
    "Epoch #160: Loss:1.5891, Accuracy:0.2585, Validation Loss:1.6015, Validation Accuracy:0.2373\n",
    "Epoch #161: Loss:1.5885, Accuracy:0.2517, Validation Loss:1.5977, Validation Accuracy:0.2397\n",
    "Epoch #162: Loss:1.5863, Accuracy:0.2568, Validation Loss:1.5970, Validation Accuracy:0.2426\n",
    "Epoch #163: Loss:1.5868, Accuracy:0.2568, Validation Loss:1.5974, Validation Accuracy:0.2418\n",
    "Epoch #164: Loss:1.5869, Accuracy:0.2576, Validation Loss:1.5977, Validation Accuracy:0.2381\n",
    "Epoch #165: Loss:1.5858, Accuracy:0.2578, Validation Loss:1.5975, Validation Accuracy:0.2418\n",
    "Epoch #166: Loss:1.5868, Accuracy:0.2570, Validation Loss:1.5980, Validation Accuracy:0.2438\n",
    "Epoch #167: Loss:1.5862, Accuracy:0.2572, Validation Loss:1.5971, Validation Accuracy:0.2434\n",
    "Epoch #168: Loss:1.5863, Accuracy:0.2584, Validation Loss:1.5990, Validation Accuracy:0.2262\n",
    "Epoch #169: Loss:1.5883, Accuracy:0.2492, Validation Loss:1.5965, Validation Accuracy:0.2410\n",
    "Epoch #170: Loss:1.5864, Accuracy:0.2539, Validation Loss:1.5970, Validation Accuracy:0.2414\n",
    "Epoch #171: Loss:1.5874, Accuracy:0.2539, Validation Loss:1.5994, Validation Accuracy:0.2258\n",
    "Epoch #172: Loss:1.5884, Accuracy:0.2500, Validation Loss:1.6004, Validation Accuracy:0.2373\n",
    "Epoch #173: Loss:1.5906, Accuracy:0.2557, Validation Loss:1.5957, Validation Accuracy:0.2328\n",
    "Epoch #174: Loss:1.5881, Accuracy:0.2553, Validation Loss:1.5958, Validation Accuracy:0.2397\n",
    "Epoch #175: Loss:1.5879, Accuracy:0.2541, Validation Loss:1.5975, Validation Accuracy:0.2406\n",
    "Epoch #176: Loss:1.5883, Accuracy:0.2490, Validation Loss:1.5985, Validation Accuracy:0.2315\n",
    "Epoch #177: Loss:1.5863, Accuracy:0.2522, Validation Loss:1.5968, Validation Accuracy:0.2438\n",
    "Epoch #178: Loss:1.5861, Accuracy:0.2535, Validation Loss:1.5973, Validation Accuracy:0.2426\n",
    "Epoch #179: Loss:1.5853, Accuracy:0.2570, Validation Loss:1.5954, Validation Accuracy:0.2426\n",
    "Epoch #180: Loss:1.5855, Accuracy:0.2562, Validation Loss:1.5965, Validation Accuracy:0.2438\n",
    "Epoch #181: Loss:1.5861, Accuracy:0.2546, Validation Loss:1.5952, Validation Accuracy:0.2443\n",
    "Epoch #182: Loss:1.5841, Accuracy:0.2581, Validation Loss:1.5957, Validation Accuracy:0.2430\n",
    "Epoch #183: Loss:1.5851, Accuracy:0.2573, Validation Loss:1.5955, Validation Accuracy:0.2488\n",
    "Epoch #184: Loss:1.5855, Accuracy:0.2509, Validation Loss:1.5958, Validation Accuracy:0.2479\n",
    "Epoch #185: Loss:1.5843, Accuracy:0.2582, Validation Loss:1.5954, Validation Accuracy:0.2451\n",
    "Epoch #186: Loss:1.5841, Accuracy:0.2568, Validation Loss:1.5940, Validation Accuracy:0.2451\n",
    "Epoch #187: Loss:1.5836, Accuracy:0.2569, Validation Loss:1.5946, Validation Accuracy:0.2447\n",
    "Epoch #188: Loss:1.5858, Accuracy:0.2502, Validation Loss:1.5966, Validation Accuracy:0.2328\n",
    "Epoch #189: Loss:1.5855, Accuracy:0.2541, Validation Loss:1.5946, Validation Accuracy:0.2393\n",
    "Epoch #190: Loss:1.5841, Accuracy:0.2520, Validation Loss:1.5946, Validation Accuracy:0.2463\n",
    "Epoch #191: Loss:1.5839, Accuracy:0.2545, Validation Loss:1.5933, Validation Accuracy:0.2492\n",
    "Epoch #192: Loss:1.5826, Accuracy:0.2554, Validation Loss:1.5961, Validation Accuracy:0.2455\n",
    "Epoch #193: Loss:1.5834, Accuracy:0.2555, Validation Loss:1.5951, Validation Accuracy:0.2414\n",
    "Epoch #194: Loss:1.5821, Accuracy:0.2586, Validation Loss:1.5958, Validation Accuracy:0.2479\n",
    "Epoch #195: Loss:1.5840, Accuracy:0.2533, Validation Loss:1.5973, Validation Accuracy:0.2422\n",
    "Epoch #196: Loss:1.5822, Accuracy:0.2602, Validation Loss:1.5967, Validation Accuracy:0.2451\n",
    "Epoch #197: Loss:1.5844, Accuracy:0.2562, Validation Loss:1.5994, Validation Accuracy:0.2484\n",
    "Epoch #198: Loss:1.5841, Accuracy:0.2609, Validation Loss:1.5974, Validation Accuracy:0.2500\n",
    "Epoch #199: Loss:1.5849, Accuracy:0.2587, Validation Loss:1.5977, Validation Accuracy:0.2455\n",
    "Epoch #200: Loss:1.5839, Accuracy:0.2617, Validation Loss:1.5997, Validation Accuracy:0.2344\n",
    "Epoch #201: Loss:1.5851, Accuracy:0.2570, Validation Loss:1.5985, Validation Accuracy:0.2451\n",
    "Epoch #202: Loss:1.5836, Accuracy:0.2624, Validation Loss:1.5980, Validation Accuracy:0.2459\n",
    "Epoch #203: Loss:1.5857, Accuracy:0.2540, Validation Loss:1.5994, Validation Accuracy:0.2434\n",
    "Epoch #204: Loss:1.5892, Accuracy:0.2516, Validation Loss:1.5959, Validation Accuracy:0.2430\n",
    "Epoch #205: Loss:1.5864, Accuracy:0.2607, Validation Loss:1.5992, Validation Accuracy:0.2401\n",
    "Epoch #206: Loss:1.5853, Accuracy:0.2582, Validation Loss:1.6002, Validation Accuracy:0.2401\n",
    "Epoch #207: Loss:1.5899, Accuracy:0.2487, Validation Loss:1.6030, Validation Accuracy:0.2410\n",
    "Epoch #208: Loss:1.5850, Accuracy:0.2614, Validation Loss:1.5960, Validation Accuracy:0.2484\n",
    "Epoch #209: Loss:1.5849, Accuracy:0.2619, Validation Loss:1.5979, Validation Accuracy:0.2512\n",
    "Epoch #210: Loss:1.5862, Accuracy:0.2528, Validation Loss:1.5979, Validation Accuracy:0.2512\n",
    "Epoch #211: Loss:1.5860, Accuracy:0.2586, Validation Loss:1.5958, Validation Accuracy:0.2488\n",
    "Epoch #212: Loss:1.5849, Accuracy:0.2597, Validation Loss:1.5967, Validation Accuracy:0.2459\n",
    "Epoch #213: Loss:1.5857, Accuracy:0.2573, Validation Loss:1.5970, Validation Accuracy:0.2447\n",
    "Epoch #214: Loss:1.5859, Accuracy:0.2557, Validation Loss:1.5956, Validation Accuracy:0.2484\n",
    "Epoch #215: Loss:1.5863, Accuracy:0.2565, Validation Loss:1.5945, Validation Accuracy:0.2434\n",
    "Epoch #216: Loss:1.5860, Accuracy:0.2562, Validation Loss:1.5951, Validation Accuracy:0.2484\n",
    "Epoch #217: Loss:1.5858, Accuracy:0.2580, Validation Loss:1.5947, Validation Accuracy:0.2455\n",
    "Epoch #218: Loss:1.5875, Accuracy:0.2548, Validation Loss:1.5968, Validation Accuracy:0.2422\n",
    "Epoch #219: Loss:1.5872, Accuracy:0.2577, Validation Loss:1.5944, Validation Accuracy:0.2443\n",
    "Epoch #220: Loss:1.5858, Accuracy:0.2592, Validation Loss:1.5956, Validation Accuracy:0.2475\n",
    "Epoch #221: Loss:1.5845, Accuracy:0.2593, Validation Loss:1.5956, Validation Accuracy:0.2467\n",
    "Epoch #222: Loss:1.5865, Accuracy:0.2575, Validation Loss:1.5963, Validation Accuracy:0.2459\n",
    "Epoch #223: Loss:1.5832, Accuracy:0.2562, Validation Loss:1.5969, Validation Accuracy:0.2438\n",
    "Epoch #224: Loss:1.5836, Accuracy:0.2543, Validation Loss:1.5949, Validation Accuracy:0.2508\n",
    "Epoch #225: Loss:1.5834, Accuracy:0.2552, Validation Loss:1.5954, Validation Accuracy:0.2475\n",
    "Epoch #226: Loss:1.5834, Accuracy:0.2600, Validation Loss:1.5976, Validation Accuracy:0.2315\n",
    "Epoch #227: Loss:1.5840, Accuracy:0.2521, Validation Loss:1.5947, Validation Accuracy:0.2467\n",
    "Epoch #228: Loss:1.5826, Accuracy:0.2578, Validation Loss:1.5968, Validation Accuracy:0.2443\n",
    "Epoch #229: Loss:1.5839, Accuracy:0.2531, Validation Loss:1.5964, Validation Accuracy:0.2397\n",
    "Epoch #230: Loss:1.5806, Accuracy:0.2576, Validation Loss:1.5939, Validation Accuracy:0.2467\n",
    "Epoch #231: Loss:1.5808, Accuracy:0.2548, Validation Loss:1.5952, Validation Accuracy:0.2508\n",
    "Epoch #232: Loss:1.5810, Accuracy:0.2552, Validation Loss:1.5959, Validation Accuracy:0.2377\n",
    "Epoch #233: Loss:1.5809, Accuracy:0.2571, Validation Loss:1.5962, Validation Accuracy:0.2434\n",
    "Epoch #234: Loss:1.5806, Accuracy:0.2606, Validation Loss:1.5969, Validation Accuracy:0.2369\n",
    "Epoch #235: Loss:1.5813, Accuracy:0.2590, Validation Loss:1.6007, Validation Accuracy:0.2463\n",
    "Epoch #236: Loss:1.5805, Accuracy:0.2609, Validation Loss:1.5969, Validation Accuracy:0.2401\n",
    "Epoch #237: Loss:1.5821, Accuracy:0.2605, Validation Loss:1.5978, Validation Accuracy:0.2418\n",
    "Epoch #238: Loss:1.5804, Accuracy:0.2607, Validation Loss:1.5985, Validation Accuracy:0.2516\n",
    "Epoch #239: Loss:1.5803, Accuracy:0.2594, Validation Loss:1.5974, Validation Accuracy:0.2377\n",
    "Epoch #240: Loss:1.5795, Accuracy:0.2645, Validation Loss:1.5973, Validation Accuracy:0.2521\n",
    "Epoch #241: Loss:1.5814, Accuracy:0.2599, Validation Loss:1.5970, Validation Accuracy:0.2541\n",
    "Epoch #242: Loss:1.5808, Accuracy:0.2642, Validation Loss:1.5965, Validation Accuracy:0.2582\n",
    "Epoch #243: Loss:1.5808, Accuracy:0.2591, Validation Loss:1.5945, Validation Accuracy:0.2549\n",
    "Epoch #244: Loss:1.5834, Accuracy:0.2621, Validation Loss:1.5954, Validation Accuracy:0.2553\n",
    "Epoch #245: Loss:1.5826, Accuracy:0.2612, Validation Loss:1.5970, Validation Accuracy:0.2492\n",
    "Epoch #246: Loss:1.5806, Accuracy:0.2600, Validation Loss:1.5989, Validation Accuracy:0.2516\n",
    "Epoch #247: Loss:1.5819, Accuracy:0.2674, Validation Loss:1.5977, Validation Accuracy:0.2459\n",
    "Epoch #248: Loss:1.5813, Accuracy:0.2651, Validation Loss:1.5966, Validation Accuracy:0.2500\n",
    "Epoch #249: Loss:1.5811, Accuracy:0.2611, Validation Loss:1.5958, Validation Accuracy:0.2443\n",
    "Epoch #250: Loss:1.5821, Accuracy:0.2581, Validation Loss:1.5949, Validation Accuracy:0.2512\n",
    "Epoch #251: Loss:1.5816, Accuracy:0.2611, Validation Loss:1.5995, Validation Accuracy:0.2504\n",
    "Epoch #252: Loss:1.5794, Accuracy:0.2656, Validation Loss:1.5982, Validation Accuracy:0.2455\n",
    "Epoch #253: Loss:1.5805, Accuracy:0.2610, Validation Loss:1.5989, Validation Accuracy:0.2467\n",
    "Epoch #254: Loss:1.5797, Accuracy:0.2658, Validation Loss:1.5988, Validation Accuracy:0.2233\n",
    "Epoch #255: Loss:1.5803, Accuracy:0.2584, Validation Loss:1.5988, Validation Accuracy:0.2516\n",
    "Epoch #256: Loss:1.5798, Accuracy:0.2631, Validation Loss:1.5956, Validation Accuracy:0.2529\n",
    "Epoch #257: Loss:1.5786, Accuracy:0.2617, Validation Loss:1.5960, Validation Accuracy:0.2553\n",
    "Epoch #258: Loss:1.5785, Accuracy:0.2684, Validation Loss:1.5979, Validation Accuracy:0.2553\n",
    "Epoch #259: Loss:1.5815, Accuracy:0.2641, Validation Loss:1.6003, Validation Accuracy:0.2492\n",
    "Epoch #260: Loss:1.5839, Accuracy:0.2533, Validation Loss:1.5946, Validation Accuracy:0.2566\n",
    "Epoch #261: Loss:1.5795, Accuracy:0.2699, Validation Loss:1.5979, Validation Accuracy:0.2537\n",
    "Epoch #262: Loss:1.5804, Accuracy:0.2613, Validation Loss:1.5969, Validation Accuracy:0.2533\n",
    "Epoch #263: Loss:1.5797, Accuracy:0.2631, Validation Loss:1.5989, Validation Accuracy:0.2557\n",
    "Epoch #264: Loss:1.5787, Accuracy:0.2676, Validation Loss:1.5964, Validation Accuracy:0.2549\n",
    "Epoch #265: Loss:1.5792, Accuracy:0.2643, Validation Loss:1.5957, Validation Accuracy:0.2615\n",
    "Epoch #266: Loss:1.5793, Accuracy:0.2682, Validation Loss:1.5961, Validation Accuracy:0.2570\n",
    "Epoch #267: Loss:1.5804, Accuracy:0.2605, Validation Loss:1.5986, Validation Accuracy:0.2553\n",
    "Epoch #268: Loss:1.5800, Accuracy:0.2645, Validation Loss:1.5954, Validation Accuracy:0.2541\n",
    "Epoch #269: Loss:1.5810, Accuracy:0.2597, Validation Loss:1.5987, Validation Accuracy:0.2516\n",
    "Epoch #270: Loss:1.5783, Accuracy:0.2674, Validation Loss:1.5978, Validation Accuracy:0.2500\n",
    "Epoch #271: Loss:1.5784, Accuracy:0.2643, Validation Loss:1.5980, Validation Accuracy:0.2512\n",
    "Epoch #272: Loss:1.5787, Accuracy:0.2633, Validation Loss:1.5993, Validation Accuracy:0.2504\n",
    "Epoch #273: Loss:1.5779, Accuracy:0.2637, Validation Loss:1.5954, Validation Accuracy:0.2500\n",
    "Epoch #274: Loss:1.5775, Accuracy:0.2640, Validation Loss:1.5962, Validation Accuracy:0.2504\n",
    "Epoch #275: Loss:1.5776, Accuracy:0.2630, Validation Loss:1.5962, Validation Accuracy:0.2467\n",
    "Epoch #276: Loss:1.5773, Accuracy:0.2640, Validation Loss:1.5963, Validation Accuracy:0.2537\n",
    "Epoch #277: Loss:1.5764, Accuracy:0.2634, Validation Loss:1.5970, Validation Accuracy:0.2533\n",
    "Epoch #278: Loss:1.5770, Accuracy:0.2641, Validation Loss:1.5976, Validation Accuracy:0.2455\n",
    "Epoch #279: Loss:1.5805, Accuracy:0.2629, Validation Loss:1.5956, Validation Accuracy:0.2451\n",
    "Epoch #280: Loss:1.5790, Accuracy:0.2594, Validation Loss:1.5974, Validation Accuracy:0.2463\n",
    "Epoch #281: Loss:1.5788, Accuracy:0.2599, Validation Loss:1.5970, Validation Accuracy:0.2484\n",
    "Epoch #282: Loss:1.5777, Accuracy:0.2631, Validation Loss:1.6006, Validation Accuracy:0.2438\n",
    "Epoch #283: Loss:1.5788, Accuracy:0.2637, Validation Loss:1.5987, Validation Accuracy:0.2455\n",
    "Epoch #284: Loss:1.5771, Accuracy:0.2653, Validation Loss:1.5989, Validation Accuracy:0.2401\n",
    "Epoch #285: Loss:1.5796, Accuracy:0.2629, Validation Loss:1.5971, Validation Accuracy:0.2422\n",
    "Epoch #286: Loss:1.5772, Accuracy:0.2632, Validation Loss:1.5987, Validation Accuracy:0.2389\n",
    "Epoch #287: Loss:1.5813, Accuracy:0.2624, Validation Loss:1.6046, Validation Accuracy:0.2336\n",
    "Epoch #288: Loss:1.5828, Accuracy:0.2536, Validation Loss:1.6036, Validation Accuracy:0.2426\n",
    "Epoch #289: Loss:1.5810, Accuracy:0.2556, Validation Loss:1.6011, Validation Accuracy:0.2406\n",
    "Epoch #290: Loss:1.5807, Accuracy:0.2543, Validation Loss:1.6027, Validation Accuracy:0.2438\n",
    "Epoch #291: Loss:1.5794, Accuracy:0.2548, Validation Loss:1.6019, Validation Accuracy:0.2463\n",
    "Epoch #292: Loss:1.5788, Accuracy:0.2603, Validation Loss:1.6020, Validation Accuracy:0.2410\n",
    "Epoch #293: Loss:1.5807, Accuracy:0.2601, Validation Loss:1.6014, Validation Accuracy:0.2488\n",
    "Epoch #294: Loss:1.5792, Accuracy:0.2622, Validation Loss:1.6026, Validation Accuracy:0.2397\n",
    "Epoch #295: Loss:1.5788, Accuracy:0.2615, Validation Loss:1.6016, Validation Accuracy:0.2447\n",
    "Epoch #296: Loss:1.5781, Accuracy:0.2645, Validation Loss:1.6028, Validation Accuracy:0.2406\n",
    "Epoch #297: Loss:1.5782, Accuracy:0.2656, Validation Loss:1.6022, Validation Accuracy:0.2410\n",
    "Epoch #298: Loss:1.5781, Accuracy:0.2617, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #299: Loss:1.5769, Accuracy:0.2637, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #300: Loss:1.5770, Accuracy:0.2638, Validation Loss:1.6007, Validation Accuracy:0.2389\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60066283, Accuracy:0.2389\n",
    "Labels: ['03', '04', '05', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03  04   05  02   01\n",
    "t:03  16  39  170  28  206\n",
    "t:04  12  52  152  43  191\n",
    "t:05  21  35  249  27  235\n",
    "t:02  19  48  198  44  148\n",
    "t:01  10  30  207  35  221\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.21      0.03      0.06       459\n",
    "          04       0.25      0.12      0.16       450\n",
    "          05       0.26      0.44      0.32       567\n",
    "          02       0.25      0.10      0.14       457\n",
    "          01       0.22      0.44      0.29       503\n",
    "\n",
    "    accuracy                           0.24      2436\n",
    "   macro avg       0.24      0.23      0.19      2436\n",
    "weighted avg       0.24      0.24      0.20      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 21:21:27 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 3 minutes, 31 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.606508152042508, 1.6056703630535083, 1.6048610134077776, 1.6042730933535472, 1.6036432069314916, 1.6045573354746125, 1.6045030631455295, 1.603745464229427, 1.6028590629057735, 1.6024852597654746, 1.6022696495056152, 1.6022084325013686, 1.602226490261911, 1.602718706397196, 1.6023031336137619, 1.6020524613375735, 1.601980651932201, 1.602308077373724, 1.6017722453194103, 1.6021710054823526, 1.602566376499746, 1.6018170425653067, 1.6017185369344376, 1.6014187991716984, 1.6016559379637143, 1.6007127945842023, 1.6001512047105235, 1.599995552416897, 1.6002278752710628, 1.6003096033199666, 1.6003996550743216, 1.6033596835895907, 1.6015185432872554, 1.6015559949702622, 1.6008011222081426, 1.6011766966536323, 1.6003465589827113, 1.6002983804211044, 1.6005433236045399, 1.6014064342909062, 1.6007452402600317, 1.600431135526823, 1.6004411467581938, 1.5998734570489141, 1.6002129377011203, 1.6004237308486537, 1.6002108402831605, 1.6005186829073677, 1.6006923291483537, 1.5998498792523037, 1.599880552448467, 1.5997565967108816, 1.5997783427167995, 1.5999864136252693, 1.5997661534201335, 1.5998101665077147, 1.5995705656230157, 1.6002173417894712, 1.599797317351418, 1.6001819894818836, 1.600328267110001, 1.5997626624866854, 1.6006057501230726, 1.5997968434504493, 1.5998319269988337, 1.59916777407203, 1.5990342283483796, 1.5988383236385526, 1.598621540468902, 1.5995393361168346, 1.5988960283730418, 1.599252364514105, 1.5991574938857105, 1.5988573806822202, 1.5989143564587547, 1.5991899027612997, 1.5989848334213783, 1.5997104290475204, 1.5987243135574416, 1.5979863810421797, 1.5990570327527025, 1.5988476119800936, 1.599081761339811, 1.599219813722695, 1.5985154794354743, 1.5982908351080758, 1.598305564012825, 1.5982892023910247, 1.5984480729439772, 1.5984692996358636, 1.5980364683226411, 1.598087323514503, 1.5979841342700527, 1.5976047036291539, 1.5973615998705033, 1.5972185144674993, 1.5976360756383936, 1.5972738062415412, 1.5984895223467221, 1.5975264921564187, 1.5986042437686514, 1.5981973924464585, 1.5984578553483209, 1.5978181790835753, 1.5985151476460724, 1.5975191225167762, 1.5972860342959074, 1.598148846469685, 1.5985658305814896, 1.5986728501828824, 1.5975998287717696, 1.597969524574593, 1.5987534722671133, 1.5976584519463024, 1.5971723941746603, 1.5966128880167243, 1.5963943112268433, 1.5966731276613935, 1.5975385342521229, 1.598062345547042, 1.596944618303396, 1.5976915817542616, 1.5963227815424477, 1.597200413055608, 1.5958940792945022, 1.597142102291627, 1.5968025404048476, 1.5975116465870773, 1.596468277948439, 1.595804346801808, 1.5965030674863918, 1.5998030875508225, 1.5975995950510937, 1.5989108481039163, 1.5978519065039498, 1.598717752740105, 1.598706500479349, 1.5976986393748442, 1.5979689212855448, 1.5970421851366416, 1.5974045103210925, 1.5981768984316997, 1.597559060564965, 1.5973472990621682, 1.5981817909062201, 1.5994764112290882, 1.597283958801495, 1.5969645610975318, 1.597553370229912, 1.5974068555534375, 1.59703478202444, 1.5975393078401563, 1.5973932756774727, 1.5977851647657322, 1.5983946360586507, 1.5977684449288254, 1.5996444231183657, 1.5994931686492193, 1.5984386514951834, 1.6014989872871361, 1.5977384018389071, 1.5969592146881304, 1.5973647554911221, 1.5977022322919372, 1.5974601444548182, 1.5980439160649216, 1.5970941809402115, 1.598966760196905, 1.596474285391947, 1.5970481330733777, 1.5993996684382898, 1.6003741452650875, 1.595662167506852, 1.5957950463240174, 1.5974616996564692, 1.598515110062848, 1.5968137149544577, 1.59731266338054, 1.5954109244354449, 1.5965274276796038, 1.5951709453695513, 1.595682861965474, 1.5955465624876601, 1.5958149983182133, 1.5954136126147116, 1.5940149961825467, 1.594648987788872, 1.5966185362663958, 1.5945939943316731, 1.5945646246078566, 1.5932985086159166, 1.5961427960685517, 1.5951130423444049, 1.5958261713018558, 1.5972681624940268, 1.5966549808364392, 1.5993872530550401, 1.597384976244521, 1.597736656959421, 1.5996671114453345, 1.598543771577782, 1.5979544689698368, 1.5994146917449625, 1.5958881043448236, 1.5991513515732363, 1.6002351170885936, 1.6029898766030624, 1.5960473434874185, 1.5978971668847872, 1.597942434312479, 1.5958218419884618, 1.5967057226913903, 1.5969961398145052, 1.5956466573799772, 1.5945325817772125, 1.5950980110121478, 1.5947408178952722, 1.5967727210525613, 1.5944357131698057, 1.5956100131490547, 1.5955926780825962, 1.5962677536339596, 1.5968878803582027, 1.5948937549966897, 1.595362579881264, 1.5975864411183374, 1.594695386432466, 1.5967705420085363, 1.5964070077012913, 1.5939093274240228, 1.5951908850317518, 1.5958674830951909, 1.5962071185824516, 1.59692489983413, 1.600743517695585, 1.5969479356101777, 1.59776176196601, 1.59849219447482, 1.5973762606556583, 1.5973045191741342, 1.5970220648009201, 1.5965095141838337, 1.5945172897113369, 1.5954009852385873, 1.5970078832018748, 1.5989283006179509, 1.5977192948800198, 1.5965704917907715, 1.5957953769389437, 1.5948790963647401, 1.5994930895678516, 1.598247338006845, 1.598940810937991, 1.5988254564736277, 1.5987910337636035, 1.5955772495817864, 1.5959901895820605, 1.5978703692628833, 1.6003452375017364, 1.5946003014622454, 1.5979346939299885, 1.5968699754752549, 1.5989200969047734, 1.5964187827995062, 1.5957182912012235, 1.596050910957537, 1.5986455988218435, 1.5953985707121725, 1.5987186500395851, 1.5978113109450818, 1.5979832489110761, 1.5993026414724014, 1.595367753446983, 1.5962049354277612, 1.5961993181059513, 1.5963156328999937, 1.5969633574556248, 1.5976069578396275, 1.5956451804767102, 1.5973631589870734, 1.5969836891969829, 1.6005900244798958, 1.5987170192799935, 1.5989326172078575, 1.5971400765166885, 1.598656864784817, 1.604639480266665, 1.6036442289211479, 1.601134736745424, 1.602749150179094, 1.6019206260421202, 1.6020407555334282, 1.6013622105806724, 1.6026213679994856, 1.6016407960349899, 1.602806075844663, 1.6022167515089163, 1.600353689616537, 1.6002901289459128, 1.6006628051767209], 'val_acc': [0.23234811153611526, 0.23275862056731395, 0.23275862056731395, 0.23275862056731395, 0.23440065671657695, 0.23275862056731395, 0.2438423645320197, 0.23399014766091, 0.24178981702707475, 0.24466338264335358, 0.24384236218306818, 0.24589490978588613, 0.24671592549933197, 0.2434318531518695, 0.2434318531518695, 0.24630541646813328, 0.24548439835679942, 0.24425287121426686, 0.24302134651855883, 0.23932676283988263, 0.2405582923313667, 0.2409688013136289, 0.24712643692841865, 0.2438423645809562, 0.23563218165696745, 0.24055829238030318, 0.23481116574777564, 0.23850574477748526, 0.23686370865269052, 0.2356321838591095, 0.2368637109771738, 0.22783251219292971, 0.22947454826878796, 0.23973727426896932, 0.23850574477748526, 0.23234811160951999, 0.22947454831772446, 0.2331691296229809, 0.2319376025783213, 0.23152709354712261, 0.2352216748279108, 0.23563218383464124, 0.22906403928652577, 0.23152709349818612, 0.2253694579078646, 0.2376847267150879, 0.2344006566921087, 0.23234811153611526, 0.22988505720211366, 0.22372742178306987, 0.23809523814417458, 0.23891625603529423, 0.23152709335137667, 0.23234811143824227, 0.23275862051837745, 0.24178981935155802, 0.23686371105057852, 0.23275862064071867, 0.23522167485237905, 0.24302134651855883, 0.24384236223200467, 0.23932676513989767, 0.23234811148717877, 0.2344006566921087, 0.23891625608423073, 0.24917897968652408, 0.23727421998390424, 0.24055829225796196, 0.23973727422003285, 0.24096880126469242, 0.22947454821985147, 0.23275862049390922, 0.2430213442185438, 0.2413793103448276, 0.2311165844425192, 0.24014778090228, 0.24055829223349373, 0.23234811153611526, 0.2364532018970386, 0.24220032605827344, 0.24302134651855883, 0.24178981942496278, 0.24220032605827344, 0.23932676513989767, 0.23768472899063467, 0.24178981702707475, 0.243431853200806, 0.24178981935155802, 0.23481116574777564, 0.23070607531344753, 0.24137931027142284, 0.23891625615763548, 0.2434318531518695, 0.22701149408159585, 0.23891625380868395, 0.24220032840722497, 0.24466338024546555, 0.23686370865269052, 0.24384236450755145, 0.2372742176838892, 0.24055829223349373, 0.2430213441206708, 0.2290640392620575, 0.2442528736121549, 0.24384236223200467, 0.23481116574777564, 0.24055829228243022, 0.23932676509096118, 0.2368637108793008, 0.22742200299045331, 0.2434318553295433, 0.2438423645320197, 0.24466338024546555, 0.22619047584792076, 0.2339901476119735, 0.24220032835828847, 0.2405582899334787, 0.24178981930262153, 0.24507388927666424, 0.2257799668901268, 0.22577996686565857, 0.23070607541132052, 0.2442528735142819, 0.24589490968801314, 0.24589490966354488, 0.22454843974759425, 0.24261083508947212, 0.24220032605827344, 0.23727421995943598, 0.24302134644515408, 0.23111658436911448, 0.24835796157519022, 0.2376847290640394, 0.22824302124859663, 0.2446633825454806, 0.23234811158505175, 0.22619047589685726, 0.2323481115605835, 0.2319376025293848, 0.24137931032035934, 0.24343185550082103, 0.23563218373676825, 0.23645320184810212, 0.24548440060787796, 0.2426108373650189, 0.23029556633118534, 0.24055829223349373, 0.24548440058340973, 0.24055829213562074, 0.24999999997553177, 0.24507389157667928, 0.24343185547635277, 0.2450738916011475, 0.2413793102958911, 0.23932676501755645, 0.2442528734653454, 0.2426108373650189, 0.23563218368783176, 0.24589490963907665, 0.2372742199104995, 0.23973727417109636, 0.24261083731608243, 0.2417898192781533, 0.23809523794842863, 0.2417898192781533, 0.24384236450755145, 0.24343185535401154, 0.22619047594579375, 0.24096880126469242, 0.2413793102469546, 0.22577996703693629, 0.2372742200817772, 0.23275862051837745, 0.23973727417109636, 0.24055829225796196, 0.23152709349818612, 0.24384236450755145, 0.24261083738948716, 0.24261083738948716, 0.24384236438521023, 0.24425287348981364, 0.24302134639621759, 0.24876847275959446, 0.24794745472166535, 0.24507389155221102, 0.24507389152774278, 0.24466338247207586, 0.23275862056731395, 0.23932676501755645, 0.2463054186458071, 0.24917898193760263, 0.24548440070575095, 0.24137931027142284, 0.24794745484400657, 0.24220032835828847, 0.24507389162561577, 0.24835796372839578, 0.25, 0.24548440060787796, 0.2344006566921087, 0.24507389152774278, 0.24589490961460841, 0.2434318553784798, 0.24302134644515408, 0.24014778317782678, 0.24014778322676328, 0.24096880126469242, 0.24835796387520526, 0.251231524793581, 0.251231524793581, 0.24876847300427693, 0.24589490978588613, 0.24466338029440204, 0.2483579640709512, 0.24343185324974248, 0.2483579640709512, 0.24548440070575095, 0.24220032845616146, 0.24425287371002785, 0.24753694356172934, 0.24671592794615646, 0.2458949098348226, 0.24384236448308322, 0.2508210180379291, 0.24753694346385635, 0.23152709354712261, 0.246715927799347, 0.2442528736121549, 0.2397372719200178, 0.2467159278482835, 0.2508210158113188, 0.2376847267150879, 0.24343185554975752, 0.23686370865269052, 0.24630541876814832, 0.2401477833491045, 0.24178981702707475, 0.2516420339226527, 0.23768472903957116, 0.2520525429538514, 0.2541050904587963, 0.25821017847076816, 0.2549261085211937, 0.25533661760132886, 0.24917897968652408, 0.2516420361003265, 0.24589490758374408, 0.24999999779785795, 0.2442528735632184, 0.251231527191469, 0.25041050673118365, 0.24548439830786292, 0.24671592540145898, 0.22331691277633942, 0.2516420361737312, 0.25287356331626376, 0.2553366175523924, 0.25533661769920185, 0.24917897978439707, 0.25656814239490994, 0.2536945791765191, 0.25328407249427193, 0.25574712428357604, 0.2549261061722422, 0.2614942506714212, 0.25697865367718714, 0.2553366151545044, 0.2541050903609234, 0.25164203382477973, 0.25000000004893647, 0.251231524793581, 0.2504105090801352, 0.2499999976510485, 0.25041050912907165, 0.246715927799347, 0.25369457902970965, 0.253284072298526, 0.24548440065681446, 0.24507388932560073, 0.24630541871921183, 0.24835796387520526, 0.24384236462989267, 0.2454844006323462, 0.2401477809512165, 0.24220032845616146, 0.23891625620657195, 0.23357963865417958, 0.24261083748736015, 0.24055829223349373, 0.24384236223200467, 0.24630541651706978, 0.24096880141150187, 0.2487684706063889, 0.2397372719200178, 0.24466338034333854, 0.2405582899334787, 0.24096880136256538, 0.24302134651855883, 0.24302134646962234, 0.2389162560597625], 'loss': [1.6100712516224605, 1.606235136927031, 1.6049200326755062, 1.6039554660569961, 1.6033683079713668, 1.6029269037795018, 1.603497634825031, 1.6026379665065351, 1.60254095109826, 1.6020715193092456, 1.601465783471689, 1.6007439589843124, 1.6011478505829766, 1.6000150282524939, 1.6002757661396474, 1.5997365531980134, 1.5992764819574061, 1.5990170369158045, 1.5984530357364757, 1.59924099660752, 1.6000888508692903, 1.600260362928653, 1.5994614044988424, 1.5989780275239103, 1.5990411120518522, 1.598533201854087, 1.5984621397034098, 1.5979015710172713, 1.5985052057360232, 1.5974819538284866, 1.5981911386552532, 1.6009194457800238, 1.6003505921706527, 1.6013612531783399, 1.6004355454102188, 1.6005447867959433, 1.5992629372363707, 1.5991754839797285, 1.5984339800703453, 1.599119192227201, 1.5993462642849838, 1.5984015917631145, 1.598370036696996, 1.5985573905694166, 1.5982155895331067, 1.5979996806052676, 1.5991511532902962, 1.5978410157334877, 1.598331556183112, 1.5987690770895329, 1.5974121132915269, 1.5982202557322915, 1.5973752414910944, 1.5980874429248442, 1.5975769826274144, 1.5973115693861943, 1.5976373760117641, 1.5979728213081126, 1.5974224892974633, 1.5982766526925245, 1.5973549247522374, 1.597080459633892, 1.5995333532043552, 1.5977071825238958, 1.5978775699525398, 1.5959093091424241, 1.5969103570346714, 1.5973784235223853, 1.5966183536596121, 1.5966171754703875, 1.5975396111270974, 1.5966083514861746, 1.5972672405184172, 1.5958143195577226, 1.5962718216055962, 1.5958634890080476, 1.5970174730680806, 1.5957234745397706, 1.5959907970389302, 1.5951049992680795, 1.596080042547269, 1.5948441680207144, 1.5964279229146499, 1.595100239953466, 1.595440662470197, 1.5945175366724786, 1.5948331046153388, 1.594407387239977, 1.5949303063523843, 1.5958828133479281, 1.595701299115128, 1.5955124915258105, 1.5951311434074105, 1.5954814374324477, 1.5951112514648593, 1.5946470037867646, 1.594423424487731, 1.594117810838766, 1.5938860472222862, 1.5945791855485043, 1.5948912616627906, 1.5937218060483678, 1.5957554370226068, 1.5934480075718687, 1.5940113212048885, 1.5942862449485418, 1.5934807924764112, 1.5937084507893242, 1.5949357364211973, 1.5931120116607853, 1.5949590800479208, 1.59357723548427, 1.5935656224922479, 1.595611667828883, 1.5934996258306797, 1.5922858568677178, 1.5917117575600406, 1.5920470968164213, 1.5919121062731107, 1.5917670296447723, 1.5920226953357641, 1.592469405491494, 1.5914983428724003, 1.5921569483480904, 1.5915710629868556, 1.5904330788207006, 1.591567650810649, 1.590906429927207, 1.5910631991020219, 1.591175590011863, 1.5911649077335177, 1.5942276862612494, 1.5930847191957478, 1.591971405624609, 1.5909676084283442, 1.5904586685756394, 1.5910545864633956, 1.5908832724823845, 1.5894296570235453, 1.5899490673194432, 1.589584196617471, 1.5898810744530367, 1.5878869900713222, 1.5885687450608679, 1.5908281430571476, 1.590608105228667, 1.5916695659410292, 1.589390910969133, 1.588545973296038, 1.5890238787848847, 1.5884211792838157, 1.5878154514261829, 1.5872913509913293, 1.587715711290097, 1.5886506125177935, 1.587157123730168, 1.589281619304994, 1.5886188342585945, 1.5879651563123511, 1.5891104247780552, 1.5885333050937378, 1.5863178254642525, 1.586802505761446, 1.5869381068423543, 1.5858045504078484, 1.5868207978516877, 1.586185857696455, 1.5863478239067281, 1.5882876363378287, 1.5864231863061016, 1.5874165430695615, 1.5884070339633698, 1.5905643643295007, 1.5880749631956128, 1.5879039319878485, 1.5883076026943919, 1.5863222382641426, 1.5860925688635887, 1.5853365163783517, 1.5854697402253044, 1.5861496182682577, 1.584115095696655, 1.5850601651095757, 1.5855407053440258, 1.5843342346821967, 1.584118630753895, 1.583631690326902, 1.585812428844538, 1.5854546870539077, 1.5841100030855966, 1.5838996093620754, 1.5826199358738424, 1.5834395826230059, 1.5821382165689488, 1.5840158454201794, 1.5821644589641501, 1.5844260037067734, 1.5841467303172274, 1.584932841118846, 1.5838633611216928, 1.5851202920232221, 1.5836331084034037, 1.585697658008129, 1.589242862724915, 1.5864073802803087, 1.5853325961306843, 1.589892498815329, 1.584958850774432, 1.584923121033263, 1.5861989947559898, 1.585986782588998, 1.5848910329767811, 1.5856789790139796, 1.5859201409733517, 1.5863450319125667, 1.5859986463366593, 1.5857815007654303, 1.5875237967688935, 1.587161827479055, 1.585831851538202, 1.5845037487742837, 1.5865441719854148, 1.5832101869387303, 1.583552364793891, 1.5833666748579522, 1.5834260690383597, 1.584000983228429, 1.5825502431368192, 1.583851003597894, 1.5806117110673408, 1.5808042392593635, 1.5810335261131458, 1.5809293087013447, 1.5805829547024361, 1.581331137073603, 1.5805194670659561, 1.5821130753053043, 1.5803734350008642, 1.5802647475099663, 1.5795203982437416, 1.5814330369295282, 1.5807807507211422, 1.5808261223642244, 1.5833630185352459, 1.5825831368718548, 1.5805732977708506, 1.5819152971067958, 1.581324911802946, 1.581098183714144, 1.5821260417511331, 1.581559142439762, 1.5793991105022862, 1.5805281648400873, 1.579682628819585, 1.5802730732140355, 1.579806390092603, 1.5785651980973856, 1.5784900457707274, 1.5815070606599844, 1.5838653948762333, 1.5795232369424872, 1.5803789898600178, 1.5796895377200242, 1.578749044670951, 1.5791606098719446, 1.5792618665362286, 1.5803730947525838, 1.579993321959243, 1.5810006923499293, 1.578340784578108, 1.5784067828062869, 1.578687596908585, 1.5779215452362625, 1.5775439185528295, 1.577628323038011, 1.577271364307991, 1.576367875976484, 1.5769533320374067, 1.5805318117631288, 1.5789934600403177, 1.5787691245088833, 1.5777462042822241, 1.5788187743701974, 1.5770807344076325, 1.5796254586879723, 1.5772202839841587, 1.5813254566408035, 1.5828117999697613, 1.580979389527495, 1.5807189773485155, 1.5793888554191198, 1.5788371033247492, 1.5807448883076223, 1.5791772239262074, 1.5788290671499359, 1.5781460954423312, 1.578244119845866, 1.5780945478278754, 1.5768685770230617, 1.5770043264913853], 'acc': [0.21642710473503177, 0.23295687885622224, 0.23295687885622224, 0.23295687885010266, 0.23295687885622224, 0.24004106776486678, 0.23439425050110788, 0.23901437371663245, 0.23418891171655126, 0.23757700205950766, 0.24199178643539945, 0.24394250513653, 0.24425051334702258, 0.24476386036960984, 0.24034907597535934, 0.24373716632443532, 0.24394250514570937, 0.24322381930184805, 0.24537987679671458, 0.24476386036960984, 0.24137577003277302, 0.24209445585215605, 0.23675564682336803, 0.2399383983634091, 0.24075975360566831, 0.2399383983603493, 0.23583162218883053, 0.23809034908209492, 0.23901437372275203, 0.2405544147721551, 0.2393223819332446, 0.232546201244272, 0.23326488706365503, 0.2297741273161812, 0.24075975359648893, 0.24568788501638653, 0.24784394250819325, 0.23737166323211403, 0.23850102668180603, 0.23891170432435413, 0.2364476386036961, 0.2386036961046822, 0.24004106777404613, 0.24168377823408624, 0.24517453798767966, 0.2379876796745177, 0.2354209445462824, 0.2388090348953584, 0.24024640657390167, 0.24168377824020582, 0.24127310061601642, 0.2444558521560575, 0.24507186859540137, 0.24353182750316127, 0.24476386037266964, 0.24496919919088392, 0.24722792607802874, 0.24866529774739268, 0.24240246406570842, 0.2466119096539838, 0.24815195073092497, 0.24866529774433288, 0.2372689938459553, 0.24127310061907623, 0.24969199179868679, 0.24938398357595506, 0.24435318275154005, 0.2487679671580297, 0.24589322382542142, 0.24938398358513442, 0.23963039014373716, 0.2473305954825462, 0.2432238193140872, 0.24661190965092403, 0.2470225872567547, 0.2508213552391994, 0.24794661192189008, 0.24784394250819325, 0.25184804928437393, 0.25246406569617974, 0.25030800821355237, 0.25523613963039016, 0.2509240246528962, 0.24876796715191013, 0.2475359342915811, 0.2542094455913352, 0.25369609856568814, 0.2552361396365097, 0.25308008213552363, 0.2508213552422592, 0.2448665297802469, 0.24640657084800868, 0.24650924024946635, 0.24969199178644763, 0.24743326489012343, 0.24640657084800868, 0.24876796714579055, 0.24938398358513442, 0.24907597537158208, 0.24455852156363472, 0.24969199179868679, 0.2516427104783988, 0.245893223831541, 0.24804928132028795, 0.2472279260902679, 0.25, 0.24579055442702355, 0.24917864476997995, 0.24928131416837782, 0.24794661190965092, 0.24435318275459983, 0.2516427104845184, 0.25184804928131416, 0.2493839835728953, 0.24127310062825558, 0.2529774127371257, 0.25318275154004105, 0.2523613963039014, 0.2503080082013132, 0.25041067763030894, 0.25277207392197126, 0.24599589322993887, 0.2521560574948665, 0.2506160164301645, 0.24876796714579055, 0.2532854209476183, 0.24815195072174562, 0.25154004108000094, 0.2506160164148656, 0.25020533882127405, 0.24763860369609855, 0.2475359342915811, 0.24640657085412826, 0.24784394251125305, 0.24691991785223724, 0.2493839835728953, 0.24332648871860466, 0.24620123204509337, 0.24774127310061603, 0.252258726899384, 0.25143737166324437, 0.2544147843942505, 0.2533880903521358, 0.2523613963161406, 0.251129363449692, 0.25092402464677666, 0.2471252566735113, 0.25184804928131416, 0.25770020533880905, 0.25790554414784395, 0.2553388090379674, 0.2502053388151545, 0.25503080083359436, 0.25605749487264934, 0.25297741273100616, 0.25071868583774176, 0.25359342916423044, 0.2526694045235734, 0.25687885009043027, 0.25852156057800846, 0.2517453798829163, 0.2567761807012117, 0.25677618070427155, 0.25759753594653073, 0.25780287474332647, 0.256981519519426, 0.25718685831928156, 0.258418891173491, 0.24917864476997995, 0.2539014373777828, 0.25390143737166326, 0.24999999998776085, 0.2557494866560372, 0.25533880904714673, 0.25410677618069816, 0.24897330595482547, 0.25215605750098613, 0.253490759759713, 0.25698151951330644, 0.2561601642710472, 0.2546201232155246, 0.25811088295687884, 0.25728952773297836, 0.2509240246406571, 0.25821355236751586, 0.25677618070427155, 0.25687885010878897, 0.2502053388090349, 0.254106776168459, 0.25195071868583163, 0.2545174538110071, 0.2554414784455446, 0.2555441478439425, 0.2586242299794661, 0.2532854209567977, 0.2601642710594671, 0.25616016425880805, 0.26088295689108926, 0.25872689938704335, 0.26170431211498973, 0.25698151950718684, 0.2624229979527315, 0.25400410678841984, 0.2516427104845184, 0.26067761807593476, 0.25821355236751586, 0.2486652977412731, 0.26139630390143737, 0.26190965093014423, 0.25277207392503104, 0.2586242299794661, 0.25965092402770046, 0.2572895277207392, 0.2557494866560372, 0.25646817248765935, 0.2561601642710472, 0.2580082135646005, 0.2548254620123203, 0.2577002053418688, 0.2592402464126904, 0.25934291582332747, 0.2574948665297741, 0.25616016427410704, 0.25431211498973305, 0.2552361396426293, 0.2599589322443126, 0.25205338809034905, 0.2578028747555656, 0.2530800821477628, 0.2575975359404112, 0.25482546200008116, 0.2552361396426293, 0.25708418891782386, 0.2605749486683575, 0.25903490759753595, 0.2608829568819099, 0.2604722792607803, 0.26067761805757605, 0.25944558522172534, 0.26447638604308055, 0.2598562628214364, 0.2641683778264684, 0.25913757698981427, 0.2621149897391791, 0.261190965098522, 0.259958932238193, 0.2673511293634497, 0.26509240246406574, 0.2610882956940046, 0.25811088295687884, 0.2610882956940046, 0.26560574949889215, 0.2609856262894871, 0.26581108830792705, 0.2584188911704312, 0.2631416837782341, 0.26170431211498973, 0.2683778233963851, 0.264065708406652, 0.25328542093231937, 0.26991786447638605, 0.2612936344969199, 0.2631416837782341, 0.2675564681724846, 0.2642710472279261, 0.2681724845995893, 0.2604722792607803, 0.26447638604308055, 0.2596509240246407, 0.26735112933897137, 0.2642710472309859, 0.263347022587269, 0.26365503080388114, 0.26396303901437373, 0.26303901437371663, 0.2639630390266129, 0.263449691997906, 0.26406570841889115, 0.2629363449814383, 0.2594455852278449, 0.2598562628336756, 0.2631416837782341, 0.2636550308130605, 0.26529774127310063, 0.26293634496919915, 0.2632443531888711, 0.26242299795885105, 0.2535934291581109, 0.25564681725457955, 0.25431211498973305, 0.25482546202455947, 0.26026694045786497, 0.2600616016549496, 0.262217659137577, 0.261498973318194, 0.26447638604002077, 0.26560574949889215, 0.2617043121180495, 0.2636550308069409, 0.2637577002114584]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
