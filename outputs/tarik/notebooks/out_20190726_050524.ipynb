{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf23.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 05:05:24 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '3', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '03', '04', '05', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001C9CF1ABE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001C9C89D6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6117, Accuracy:0.2230, Validation Loss:1.6091, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6081, Accuracy:0.2329, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6068, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6061, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6026, Accuracy:0.2329, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6011, Accuracy:0.2329, Validation Loss:1.6008, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.5987, Accuracy:0.2485, Validation Loss:1.5984, Validation Accuracy:0.2759\n",
    "Epoch #28: Loss:1.5949, Accuracy:0.2587, Validation Loss:1.5938, Validation Accuracy:0.2463\n",
    "Epoch #29: Loss:1.5877, Accuracy:0.2768, Validation Loss:1.5866, Validation Accuracy:0.2989\n",
    "Epoch #30: Loss:1.5774, Accuracy:0.2887, Validation Loss:1.5773, Validation Accuracy:0.2956\n",
    "Epoch #31: Loss:1.5669, Accuracy:0.2903, Validation Loss:1.5744, Validation Accuracy:0.3005\n",
    "Epoch #32: Loss:1.5623, Accuracy:0.2928, Validation Loss:1.5803, Validation Accuracy:0.2775\n",
    "Epoch #33: Loss:1.5631, Accuracy:0.2912, Validation Loss:1.5707, Validation Accuracy:0.2841\n",
    "Epoch #34: Loss:1.5576, Accuracy:0.2887, Validation Loss:1.5656, Validation Accuracy:0.3005\n",
    "Epoch #35: Loss:1.5558, Accuracy:0.2899, Validation Loss:1.5688, Validation Accuracy:0.2857\n",
    "Epoch #36: Loss:1.5548, Accuracy:0.2932, Validation Loss:1.5621, Validation Accuracy:0.2972\n",
    "Epoch #37: Loss:1.5543, Accuracy:0.2961, Validation Loss:1.5662, Validation Accuracy:0.2841\n",
    "Epoch #38: Loss:1.5549, Accuracy:0.2887, Validation Loss:1.5587, Validation Accuracy:0.2956\n",
    "Epoch #39: Loss:1.5517, Accuracy:0.2920, Validation Loss:1.5588, Validation Accuracy:0.2857\n",
    "Epoch #40: Loss:1.5512, Accuracy:0.2949, Validation Loss:1.5558, Validation Accuracy:0.2972\n",
    "Epoch #41: Loss:1.5523, Accuracy:0.2949, Validation Loss:1.5567, Validation Accuracy:0.2890\n",
    "Epoch #42: Loss:1.5520, Accuracy:0.2940, Validation Loss:1.5557, Validation Accuracy:0.2956\n",
    "Epoch #43: Loss:1.5502, Accuracy:0.2945, Validation Loss:1.5545, Validation Accuracy:0.2972\n",
    "Epoch #44: Loss:1.5515, Accuracy:0.2936, Validation Loss:1.5567, Validation Accuracy:0.2824\n",
    "Epoch #45: Loss:1.5515, Accuracy:0.2932, Validation Loss:1.5536, Validation Accuracy:0.2972\n",
    "Epoch #46: Loss:1.5487, Accuracy:0.2973, Validation Loss:1.5538, Validation Accuracy:0.2956\n",
    "Epoch #47: Loss:1.5496, Accuracy:0.2953, Validation Loss:1.5528, Validation Accuracy:0.2923\n",
    "Epoch #48: Loss:1.5484, Accuracy:0.2953, Validation Loss:1.5530, Validation Accuracy:0.2972\n",
    "Epoch #49: Loss:1.5506, Accuracy:0.2928, Validation Loss:1.5520, Validation Accuracy:0.2989\n",
    "Epoch #50: Loss:1.5505, Accuracy:0.2961, Validation Loss:1.5539, Validation Accuracy:0.2841\n",
    "Epoch #51: Loss:1.5535, Accuracy:0.2936, Validation Loss:1.5514, Validation Accuracy:0.2923\n",
    "Epoch #52: Loss:1.5514, Accuracy:0.2969, Validation Loss:1.5513, Validation Accuracy:0.2923\n",
    "Epoch #53: Loss:1.5479, Accuracy:0.2977, Validation Loss:1.5540, Validation Accuracy:0.2824\n",
    "Epoch #54: Loss:1.5471, Accuracy:0.2957, Validation Loss:1.5501, Validation Accuracy:0.2972\n",
    "Epoch #55: Loss:1.5458, Accuracy:0.3014, Validation Loss:1.5526, Validation Accuracy:0.2857\n",
    "Epoch #56: Loss:1.5446, Accuracy:0.3027, Validation Loss:1.5495, Validation Accuracy:0.2956\n",
    "Epoch #57: Loss:1.5469, Accuracy:0.2940, Validation Loss:1.5525, Validation Accuracy:0.2857\n",
    "Epoch #58: Loss:1.5469, Accuracy:0.2973, Validation Loss:1.5495, Validation Accuracy:0.2906\n",
    "Epoch #59: Loss:1.5467, Accuracy:0.2977, Validation Loss:1.5490, Validation Accuracy:0.2956\n",
    "Epoch #60: Loss:1.5439, Accuracy:0.2977, Validation Loss:1.5533, Validation Accuracy:0.2824\n",
    "Epoch #61: Loss:1.5438, Accuracy:0.3006, Validation Loss:1.5484, Validation Accuracy:0.3005\n",
    "Epoch #62: Loss:1.5435, Accuracy:0.2990, Validation Loss:1.5518, Validation Accuracy:0.2874\n",
    "Epoch #63: Loss:1.5461, Accuracy:0.2945, Validation Loss:1.5478, Validation Accuracy:0.2939\n",
    "Epoch #64: Loss:1.5468, Accuracy:0.2957, Validation Loss:1.5500, Validation Accuracy:0.2857\n",
    "Epoch #65: Loss:1.5446, Accuracy:0.2994, Validation Loss:1.5474, Validation Accuracy:0.2956\n",
    "Epoch #66: Loss:1.5425, Accuracy:0.3014, Validation Loss:1.5473, Validation Accuracy:0.2972\n",
    "Epoch #67: Loss:1.5418, Accuracy:0.3027, Validation Loss:1.5473, Validation Accuracy:0.2972\n",
    "Epoch #68: Loss:1.5422, Accuracy:0.3006, Validation Loss:1.5462, Validation Accuracy:0.2939\n",
    "Epoch #69: Loss:1.5439, Accuracy:0.2957, Validation Loss:1.5479, Validation Accuracy:0.2890\n",
    "Epoch #70: Loss:1.5446, Accuracy:0.2977, Validation Loss:1.5457, Validation Accuracy:0.2956\n",
    "Epoch #71: Loss:1.5425, Accuracy:0.3010, Validation Loss:1.5457, Validation Accuracy:0.2939\n",
    "Epoch #72: Loss:1.5412, Accuracy:0.2998, Validation Loss:1.5458, Validation Accuracy:0.2923\n",
    "Epoch #73: Loss:1.5404, Accuracy:0.3039, Validation Loss:1.5445, Validation Accuracy:0.2939\n",
    "Epoch #74: Loss:1.5393, Accuracy:0.3035, Validation Loss:1.5455, Validation Accuracy:0.2939\n",
    "Epoch #75: Loss:1.5401, Accuracy:0.3043, Validation Loss:1.5439, Validation Accuracy:0.2972\n",
    "Epoch #76: Loss:1.5397, Accuracy:0.3014, Validation Loss:1.5447, Validation Accuracy:0.2890\n",
    "Epoch #77: Loss:1.5402, Accuracy:0.3002, Validation Loss:1.5433, Validation Accuracy:0.2939\n",
    "Epoch #78: Loss:1.5392, Accuracy:0.3010, Validation Loss:1.5429, Validation Accuracy:0.2906\n",
    "Epoch #79: Loss:1.5378, Accuracy:0.3014, Validation Loss:1.5431, Validation Accuracy:0.2890\n",
    "Epoch #80: Loss:1.5376, Accuracy:0.3010, Validation Loss:1.5420, Validation Accuracy:0.2874\n",
    "Epoch #81: Loss:1.5362, Accuracy:0.3060, Validation Loss:1.5411, Validation Accuracy:0.2972\n",
    "Epoch #82: Loss:1.5363, Accuracy:0.3027, Validation Loss:1.5405, Validation Accuracy:0.2972\n",
    "Epoch #83: Loss:1.5359, Accuracy:0.3002, Validation Loss:1.5422, Validation Accuracy:0.2906\n",
    "Epoch #84: Loss:1.5356, Accuracy:0.3010, Validation Loss:1.5402, Validation Accuracy:0.2857\n",
    "Epoch #85: Loss:1.5354, Accuracy:0.3018, Validation Loss:1.5385, Validation Accuracy:0.2956\n",
    "Epoch #86: Loss:1.5343, Accuracy:0.2986, Validation Loss:1.5423, Validation Accuracy:0.2972\n",
    "Epoch #87: Loss:1.5333, Accuracy:0.3014, Validation Loss:1.5374, Validation Accuracy:0.2972\n",
    "Epoch #88: Loss:1.5339, Accuracy:0.3006, Validation Loss:1.5368, Validation Accuracy:0.2956\n",
    "Epoch #89: Loss:1.5325, Accuracy:0.3014, Validation Loss:1.5411, Validation Accuracy:0.2923\n",
    "Epoch #90: Loss:1.5317, Accuracy:0.3039, Validation Loss:1.5355, Validation Accuracy:0.3005\n",
    "Epoch #91: Loss:1.5322, Accuracy:0.3031, Validation Loss:1.5341, Validation Accuracy:0.3038\n",
    "Epoch #92: Loss:1.5296, Accuracy:0.3064, Validation Loss:1.5388, Validation Accuracy:0.2956\n",
    "Epoch #93: Loss:1.5288, Accuracy:0.3064, Validation Loss:1.5353, Validation Accuracy:0.3054\n",
    "Epoch #94: Loss:1.5304, Accuracy:0.3027, Validation Loss:1.5322, Validation Accuracy:0.3071\n",
    "Epoch #95: Loss:1.5308, Accuracy:0.3014, Validation Loss:1.5332, Validation Accuracy:0.3054\n",
    "Epoch #96: Loss:1.5271, Accuracy:0.2998, Validation Loss:1.5309, Validation Accuracy:0.3186\n",
    "Epoch #97: Loss:1.5251, Accuracy:0.3105, Validation Loss:1.5289, Validation Accuracy:0.3169\n",
    "Epoch #98: Loss:1.5244, Accuracy:0.3142, Validation Loss:1.5301, Validation Accuracy:0.3120\n",
    "Epoch #99: Loss:1.5260, Accuracy:0.3084, Validation Loss:1.5258, Validation Accuracy:0.3300\n",
    "Epoch #100: Loss:1.5214, Accuracy:0.3125, Validation Loss:1.5310, Validation Accuracy:0.3251\n",
    "Epoch #101: Loss:1.5199, Accuracy:0.3211, Validation Loss:1.5246, Validation Accuracy:0.3333\n",
    "Epoch #102: Loss:1.5182, Accuracy:0.3179, Validation Loss:1.5232, Validation Accuracy:0.3333\n",
    "Epoch #103: Loss:1.5169, Accuracy:0.3191, Validation Loss:1.5446, Validation Accuracy:0.3005\n",
    "Epoch #104: Loss:1.5202, Accuracy:0.3150, Validation Loss:1.5175, Validation Accuracy:0.3530\n",
    "Epoch #105: Loss:1.5203, Accuracy:0.3331, Validation Loss:1.5175, Validation Accuracy:0.3498\n",
    "Epoch #106: Loss:1.5163, Accuracy:0.3216, Validation Loss:1.5148, Validation Accuracy:0.3530\n",
    "Epoch #107: Loss:1.5096, Accuracy:0.3265, Validation Loss:1.5129, Validation Accuracy:0.3580\n",
    "Epoch #108: Loss:1.5074, Accuracy:0.3310, Validation Loss:1.5103, Validation Accuracy:0.3580\n",
    "Epoch #109: Loss:1.5046, Accuracy:0.3335, Validation Loss:1.5097, Validation Accuracy:0.3498\n",
    "Epoch #110: Loss:1.5024, Accuracy:0.3425, Validation Loss:1.5098, Validation Accuracy:0.3514\n",
    "Epoch #111: Loss:1.5010, Accuracy:0.3343, Validation Loss:1.5073, Validation Accuracy:0.3662\n",
    "Epoch #112: Loss:1.5065, Accuracy:0.3318, Validation Loss:1.5127, Validation Accuracy:0.3333\n",
    "Epoch #113: Loss:1.4979, Accuracy:0.3335, Validation Loss:1.5002, Validation Accuracy:0.3498\n",
    "Epoch #114: Loss:1.4905, Accuracy:0.3359, Validation Loss:1.4935, Validation Accuracy:0.3530\n",
    "Epoch #115: Loss:1.4904, Accuracy:0.3487, Validation Loss:1.4936, Validation Accuracy:0.3563\n",
    "Epoch #116: Loss:1.5053, Accuracy:0.3351, Validation Loss:1.5461, Validation Accuracy:0.3186\n",
    "Epoch #117: Loss:1.5049, Accuracy:0.3269, Validation Loss:1.5264, Validation Accuracy:0.3218\n",
    "Epoch #118: Loss:1.4875, Accuracy:0.3421, Validation Loss:1.5017, Validation Accuracy:0.3498\n",
    "Epoch #119: Loss:1.4834, Accuracy:0.3466, Validation Loss:1.4890, Validation Accuracy:0.3415\n",
    "Epoch #120: Loss:1.4829, Accuracy:0.3409, Validation Loss:1.4875, Validation Accuracy:0.3481\n",
    "Epoch #121: Loss:1.4760, Accuracy:0.3507, Validation Loss:1.4963, Validation Accuracy:0.3383\n",
    "Epoch #122: Loss:1.4848, Accuracy:0.3499, Validation Loss:1.4894, Validation Accuracy:0.3448\n",
    "Epoch #123: Loss:1.4786, Accuracy:0.3458, Validation Loss:1.4869, Validation Accuracy:0.3547\n",
    "Epoch #124: Loss:1.4748, Accuracy:0.3589, Validation Loss:1.5008, Validation Accuracy:0.3317\n",
    "Epoch #125: Loss:1.4816, Accuracy:0.3520, Validation Loss:1.4948, Validation Accuracy:0.3432\n",
    "Epoch #126: Loss:1.4953, Accuracy:0.3228, Validation Loss:1.5026, Validation Accuracy:0.3448\n",
    "Epoch #127: Loss:1.4838, Accuracy:0.3376, Validation Loss:1.4864, Validation Accuracy:0.3563\n",
    "Epoch #128: Loss:1.4823, Accuracy:0.3446, Validation Loss:1.4831, Validation Accuracy:0.3448\n",
    "Epoch #129: Loss:1.4748, Accuracy:0.3433, Validation Loss:1.4906, Validation Accuracy:0.3678\n",
    "Epoch #130: Loss:1.4814, Accuracy:0.3462, Validation Loss:1.4973, Validation Accuracy:0.3514\n",
    "Epoch #131: Loss:1.4800, Accuracy:0.3429, Validation Loss:1.4875, Validation Accuracy:0.3645\n",
    "Epoch #132: Loss:1.4731, Accuracy:0.3503, Validation Loss:1.4833, Validation Accuracy:0.3612\n",
    "Epoch #133: Loss:1.4720, Accuracy:0.3565, Validation Loss:1.4824, Validation Accuracy:0.3629\n",
    "Epoch #134: Loss:1.4731, Accuracy:0.3454, Validation Loss:1.4942, Validation Accuracy:0.3333\n",
    "Epoch #135: Loss:1.4820, Accuracy:0.3388, Validation Loss:1.4886, Validation Accuracy:0.3383\n",
    "Epoch #136: Loss:1.4750, Accuracy:0.3478, Validation Loss:1.4800, Validation Accuracy:0.3547\n",
    "Epoch #137: Loss:1.4703, Accuracy:0.3577, Validation Loss:1.4859, Validation Accuracy:0.3350\n",
    "Epoch #138: Loss:1.4739, Accuracy:0.3499, Validation Loss:1.4802, Validation Accuracy:0.3514\n",
    "Epoch #139: Loss:1.4682, Accuracy:0.3478, Validation Loss:1.4794, Validation Accuracy:0.3612\n",
    "Epoch #140: Loss:1.4685, Accuracy:0.3499, Validation Loss:1.4791, Validation Accuracy:0.3612\n",
    "Epoch #141: Loss:1.4712, Accuracy:0.3511, Validation Loss:1.4876, Validation Accuracy:0.3596\n",
    "Epoch #142: Loss:1.4801, Accuracy:0.3462, Validation Loss:1.4810, Validation Accuracy:0.3629\n",
    "Epoch #143: Loss:1.4745, Accuracy:0.3458, Validation Loss:1.4891, Validation Accuracy:0.3399\n",
    "Epoch #144: Loss:1.4763, Accuracy:0.3413, Validation Loss:1.4781, Validation Accuracy:0.3629\n",
    "Epoch #145: Loss:1.4690, Accuracy:0.3544, Validation Loss:1.4844, Validation Accuracy:0.3333\n",
    "Epoch #146: Loss:1.4720, Accuracy:0.3450, Validation Loss:1.4885, Validation Accuracy:0.3399\n",
    "Epoch #147: Loss:1.4714, Accuracy:0.3483, Validation Loss:1.4784, Validation Accuracy:0.3596\n",
    "Epoch #148: Loss:1.4664, Accuracy:0.3520, Validation Loss:1.4803, Validation Accuracy:0.3629\n",
    "Epoch #149: Loss:1.4648, Accuracy:0.3536, Validation Loss:1.4782, Validation Accuracy:0.3530\n",
    "Epoch #150: Loss:1.4656, Accuracy:0.3483, Validation Loss:1.4780, Validation Accuracy:0.3596\n",
    "Epoch #151: Loss:1.4688, Accuracy:0.3561, Validation Loss:1.4780, Validation Accuracy:0.3596\n",
    "Epoch #152: Loss:1.4702, Accuracy:0.3536, Validation Loss:1.4776, Validation Accuracy:0.3530\n",
    "Epoch #153: Loss:1.4657, Accuracy:0.3495, Validation Loss:1.4783, Validation Accuracy:0.3448\n",
    "Epoch #154: Loss:1.4696, Accuracy:0.3548, Validation Loss:1.4788, Validation Accuracy:0.3399\n",
    "Epoch #155: Loss:1.4685, Accuracy:0.3487, Validation Loss:1.4759, Validation Accuracy:0.3580\n",
    "Epoch #156: Loss:1.4668, Accuracy:0.3524, Validation Loss:1.4768, Validation Accuracy:0.3498\n",
    "Epoch #157: Loss:1.4658, Accuracy:0.3528, Validation Loss:1.4791, Validation Accuracy:0.3662\n",
    "Epoch #158: Loss:1.4647, Accuracy:0.3569, Validation Loss:1.4784, Validation Accuracy:0.3596\n",
    "Epoch #159: Loss:1.4643, Accuracy:0.3544, Validation Loss:1.4982, Validation Accuracy:0.3481\n",
    "Epoch #160: Loss:1.4738, Accuracy:0.3474, Validation Loss:1.4935, Validation Accuracy:0.3498\n",
    "Epoch #161: Loss:1.4814, Accuracy:0.3470, Validation Loss:1.4835, Validation Accuracy:0.3514\n",
    "Epoch #162: Loss:1.4753, Accuracy:0.3409, Validation Loss:1.4768, Validation Accuracy:0.3563\n",
    "Epoch #163: Loss:1.4650, Accuracy:0.3511, Validation Loss:1.4764, Validation Accuracy:0.3580\n",
    "Epoch #164: Loss:1.4671, Accuracy:0.3491, Validation Loss:1.4762, Validation Accuracy:0.3580\n",
    "Epoch #165: Loss:1.4648, Accuracy:0.3515, Validation Loss:1.4810, Validation Accuracy:0.3678\n",
    "Epoch #166: Loss:1.4665, Accuracy:0.3561, Validation Loss:1.4765, Validation Accuracy:0.3596\n",
    "Epoch #167: Loss:1.4620, Accuracy:0.3556, Validation Loss:1.4779, Validation Accuracy:0.3498\n",
    "Epoch #168: Loss:1.4652, Accuracy:0.3532, Validation Loss:1.4850, Validation Accuracy:0.3350\n",
    "Epoch #169: Loss:1.4729, Accuracy:0.3437, Validation Loss:1.4945, Validation Accuracy:0.3333\n",
    "Epoch #170: Loss:1.4821, Accuracy:0.3462, Validation Loss:1.5108, Validation Accuracy:0.3202\n",
    "Epoch #171: Loss:1.4934, Accuracy:0.3318, Validation Loss:1.4776, Validation Accuracy:0.3612\n",
    "Epoch #172: Loss:1.4800, Accuracy:0.3347, Validation Loss:1.4995, Validation Accuracy:0.3514\n",
    "Epoch #173: Loss:1.4725, Accuracy:0.3450, Validation Loss:1.4840, Validation Accuracy:0.3530\n",
    "Epoch #174: Loss:1.4740, Accuracy:0.3487, Validation Loss:1.4801, Validation Accuracy:0.3399\n",
    "Epoch #175: Loss:1.4747, Accuracy:0.3425, Validation Loss:1.4864, Validation Accuracy:0.3415\n",
    "Epoch #176: Loss:1.4722, Accuracy:0.3454, Validation Loss:1.4772, Validation Accuracy:0.3514\n",
    "Epoch #177: Loss:1.4763, Accuracy:0.3446, Validation Loss:1.4814, Validation Accuracy:0.3629\n",
    "Epoch #178: Loss:1.4652, Accuracy:0.3499, Validation Loss:1.4915, Validation Accuracy:0.3530\n",
    "Epoch #179: Loss:1.4682, Accuracy:0.3511, Validation Loss:1.4758, Validation Accuracy:0.3596\n",
    "Epoch #180: Loss:1.4674, Accuracy:0.3491, Validation Loss:1.4761, Validation Accuracy:0.3563\n",
    "Epoch #181: Loss:1.4632, Accuracy:0.3548, Validation Loss:1.4825, Validation Accuracy:0.3350\n",
    "Epoch #182: Loss:1.4670, Accuracy:0.3487, Validation Loss:1.4756, Validation Accuracy:0.3530\n",
    "Epoch #183: Loss:1.4617, Accuracy:0.3491, Validation Loss:1.4842, Validation Accuracy:0.3399\n",
    "Epoch #184: Loss:1.4716, Accuracy:0.3503, Validation Loss:1.4746, Validation Accuracy:0.3612\n",
    "Epoch #185: Loss:1.4614, Accuracy:0.3544, Validation Loss:1.4912, Validation Accuracy:0.3481\n",
    "Epoch #186: Loss:1.4789, Accuracy:0.3409, Validation Loss:1.5403, Validation Accuracy:0.3136\n",
    "Epoch #187: Loss:1.4865, Accuracy:0.3355, Validation Loss:1.4755, Validation Accuracy:0.3465\n",
    "Epoch #188: Loss:1.4808, Accuracy:0.3376, Validation Loss:1.4931, Validation Accuracy:0.3465\n",
    "Epoch #189: Loss:1.4780, Accuracy:0.3409, Validation Loss:1.4766, Validation Accuracy:0.3530\n",
    "Epoch #190: Loss:1.4669, Accuracy:0.3495, Validation Loss:1.4972, Validation Accuracy:0.3465\n",
    "Epoch #191: Loss:1.4707, Accuracy:0.3503, Validation Loss:1.4759, Validation Accuracy:0.3563\n",
    "Epoch #192: Loss:1.4612, Accuracy:0.3606, Validation Loss:1.4762, Validation Accuracy:0.3596\n",
    "Epoch #193: Loss:1.4602, Accuracy:0.3503, Validation Loss:1.4755, Validation Accuracy:0.3629\n",
    "Epoch #194: Loss:1.4599, Accuracy:0.3548, Validation Loss:1.4772, Validation Accuracy:0.3530\n",
    "Epoch #195: Loss:1.4634, Accuracy:0.3503, Validation Loss:1.4754, Validation Accuracy:0.3596\n",
    "Epoch #196: Loss:1.4593, Accuracy:0.3520, Validation Loss:1.4794, Validation Accuracy:0.3580\n",
    "Epoch #197: Loss:1.4588, Accuracy:0.3507, Validation Loss:1.4750, Validation Accuracy:0.3580\n",
    "Epoch #198: Loss:1.4587, Accuracy:0.3602, Validation Loss:1.4800, Validation Accuracy:0.3399\n",
    "Epoch #199: Loss:1.4664, Accuracy:0.3548, Validation Loss:1.4746, Validation Accuracy:0.3563\n",
    "Epoch #200: Loss:1.4670, Accuracy:0.3462, Validation Loss:1.4820, Validation Accuracy:0.3432\n",
    "Epoch #201: Loss:1.4736, Accuracy:0.3441, Validation Loss:1.4742, Validation Accuracy:0.3563\n",
    "Epoch #202: Loss:1.4604, Accuracy:0.3581, Validation Loss:1.4790, Validation Accuracy:0.3563\n",
    "Epoch #203: Loss:1.4583, Accuracy:0.3524, Validation Loss:1.4797, Validation Accuracy:0.3629\n",
    "Epoch #204: Loss:1.4600, Accuracy:0.3536, Validation Loss:1.4893, Validation Accuracy:0.3498\n",
    "Epoch #205: Loss:1.4617, Accuracy:0.3577, Validation Loss:1.4739, Validation Accuracy:0.3612\n",
    "Epoch #206: Loss:1.4558, Accuracy:0.3548, Validation Loss:1.4746, Validation Accuracy:0.3580\n",
    "Epoch #207: Loss:1.4578, Accuracy:0.3536, Validation Loss:1.4742, Validation Accuracy:0.3580\n",
    "Epoch #208: Loss:1.4620, Accuracy:0.3429, Validation Loss:1.4762, Validation Accuracy:0.3596\n",
    "Epoch #209: Loss:1.4603, Accuracy:0.3524, Validation Loss:1.4875, Validation Accuracy:0.3498\n",
    "Epoch #210: Loss:1.4617, Accuracy:0.3511, Validation Loss:1.4877, Validation Accuracy:0.3498\n",
    "Epoch #211: Loss:1.4619, Accuracy:0.3515, Validation Loss:1.4766, Validation Accuracy:0.3596\n",
    "Epoch #212: Loss:1.4568, Accuracy:0.3569, Validation Loss:1.4934, Validation Accuracy:0.3481\n",
    "Epoch #213: Loss:1.4638, Accuracy:0.3499, Validation Loss:1.4794, Validation Accuracy:0.3563\n",
    "Epoch #214: Loss:1.4706, Accuracy:0.3495, Validation Loss:1.4766, Validation Accuracy:0.3399\n",
    "Epoch #215: Loss:1.4745, Accuracy:0.3462, Validation Loss:1.5185, Validation Accuracy:0.3120\n",
    "Epoch #216: Loss:1.4803, Accuracy:0.3335, Validation Loss:1.4886, Validation Accuracy:0.3530\n",
    "Epoch #217: Loss:1.4649, Accuracy:0.3511, Validation Loss:1.4814, Validation Accuracy:0.3514\n",
    "Epoch #218: Loss:1.4663, Accuracy:0.3454, Validation Loss:1.4784, Validation Accuracy:0.3465\n",
    "Epoch #219: Loss:1.4652, Accuracy:0.3536, Validation Loss:1.4811, Validation Accuracy:0.3448\n",
    "Epoch #220: Loss:1.4653, Accuracy:0.3483, Validation Loss:1.4765, Validation Accuracy:0.3580\n",
    "Epoch #221: Loss:1.4554, Accuracy:0.3556, Validation Loss:1.4802, Validation Accuracy:0.3629\n",
    "Epoch #222: Loss:1.4646, Accuracy:0.3450, Validation Loss:1.4775, Validation Accuracy:0.3596\n",
    "Epoch #223: Loss:1.4642, Accuracy:0.3536, Validation Loss:1.4750, Validation Accuracy:0.3530\n",
    "Epoch #224: Loss:1.4584, Accuracy:0.3610, Validation Loss:1.4765, Validation Accuracy:0.3432\n",
    "Epoch #225: Loss:1.4572, Accuracy:0.3520, Validation Loss:1.4742, Validation Accuracy:0.3678\n",
    "Epoch #226: Loss:1.4552, Accuracy:0.3544, Validation Loss:1.4933, Validation Accuracy:0.3448\n",
    "Epoch #227: Loss:1.4604, Accuracy:0.3528, Validation Loss:1.4735, Validation Accuracy:0.3662\n",
    "Epoch #228: Loss:1.4567, Accuracy:0.3515, Validation Loss:1.4730, Validation Accuracy:0.3612\n",
    "Epoch #229: Loss:1.4569, Accuracy:0.3544, Validation Loss:1.4738, Validation Accuracy:0.3612\n",
    "Epoch #230: Loss:1.4545, Accuracy:0.3499, Validation Loss:1.4732, Validation Accuracy:0.3596\n",
    "Epoch #231: Loss:1.4523, Accuracy:0.3536, Validation Loss:1.4848, Validation Accuracy:0.3530\n",
    "Epoch #232: Loss:1.4571, Accuracy:0.3483, Validation Loss:1.4763, Validation Accuracy:0.3596\n",
    "Epoch #233: Loss:1.4569, Accuracy:0.3532, Validation Loss:1.4736, Validation Accuracy:0.3580\n",
    "Epoch #234: Loss:1.4529, Accuracy:0.3548, Validation Loss:1.4848, Validation Accuracy:0.3514\n",
    "Epoch #235: Loss:1.4596, Accuracy:0.3565, Validation Loss:1.4867, Validation Accuracy:0.3514\n",
    "Epoch #236: Loss:1.4597, Accuracy:0.3622, Validation Loss:1.4774, Validation Accuracy:0.3481\n",
    "Epoch #237: Loss:1.4532, Accuracy:0.3622, Validation Loss:1.4767, Validation Accuracy:0.3530\n",
    "Epoch #238: Loss:1.4525, Accuracy:0.3573, Validation Loss:1.4793, Validation Accuracy:0.3514\n",
    "Epoch #239: Loss:1.4590, Accuracy:0.3515, Validation Loss:1.4745, Validation Accuracy:0.3645\n",
    "Epoch #240: Loss:1.4537, Accuracy:0.3618, Validation Loss:1.4737, Validation Accuracy:0.3629\n",
    "Epoch #241: Loss:1.4513, Accuracy:0.3569, Validation Loss:1.4810, Validation Accuracy:0.3514\n",
    "Epoch #242: Loss:1.4536, Accuracy:0.3593, Validation Loss:1.4814, Validation Accuracy:0.3514\n",
    "Epoch #243: Loss:1.4549, Accuracy:0.3589, Validation Loss:1.4741, Validation Accuracy:0.3596\n",
    "Epoch #244: Loss:1.4520, Accuracy:0.3610, Validation Loss:1.4732, Validation Accuracy:0.3629\n",
    "Epoch #245: Loss:1.4505, Accuracy:0.3565, Validation Loss:1.4719, Validation Accuracy:0.3678\n",
    "Epoch #246: Loss:1.4500, Accuracy:0.3581, Validation Loss:1.4833, Validation Accuracy:0.3596\n",
    "Epoch #247: Loss:1.4531, Accuracy:0.3598, Validation Loss:1.4774, Validation Accuracy:0.3596\n",
    "Epoch #248: Loss:1.4570, Accuracy:0.3585, Validation Loss:1.4808, Validation Accuracy:0.3530\n",
    "Epoch #249: Loss:1.4558, Accuracy:0.3556, Validation Loss:1.4887, Validation Accuracy:0.3432\n",
    "Epoch #250: Loss:1.4608, Accuracy:0.3561, Validation Loss:1.4731, Validation Accuracy:0.3563\n",
    "Epoch #251: Loss:1.4535, Accuracy:0.3561, Validation Loss:1.4760, Validation Accuracy:0.3580\n",
    "Epoch #252: Loss:1.4555, Accuracy:0.3610, Validation Loss:1.4853, Validation Accuracy:0.3530\n",
    "Epoch #253: Loss:1.4527, Accuracy:0.3602, Validation Loss:1.4764, Validation Accuracy:0.3612\n",
    "Epoch #254: Loss:1.4507, Accuracy:0.3602, Validation Loss:1.4714, Validation Accuracy:0.3645\n",
    "Epoch #255: Loss:1.4505, Accuracy:0.3577, Validation Loss:1.4718, Validation Accuracy:0.3629\n",
    "Epoch #256: Loss:1.4485, Accuracy:0.3610, Validation Loss:1.4741, Validation Accuracy:0.3563\n",
    "Epoch #257: Loss:1.4501, Accuracy:0.3561, Validation Loss:1.4729, Validation Accuracy:0.3563\n",
    "Epoch #258: Loss:1.4546, Accuracy:0.3491, Validation Loss:1.4744, Validation Accuracy:0.3580\n",
    "Epoch #259: Loss:1.4568, Accuracy:0.3561, Validation Loss:1.4773, Validation Accuracy:0.3547\n",
    "Epoch #260: Loss:1.4517, Accuracy:0.3593, Validation Loss:1.4719, Validation Accuracy:0.3612\n",
    "Epoch #261: Loss:1.4499, Accuracy:0.3622, Validation Loss:1.4743, Validation Accuracy:0.3514\n",
    "Epoch #262: Loss:1.4524, Accuracy:0.3643, Validation Loss:1.4712, Validation Accuracy:0.3645\n",
    "Epoch #263: Loss:1.4518, Accuracy:0.3573, Validation Loss:1.4717, Validation Accuracy:0.3612\n",
    "Epoch #264: Loss:1.4479, Accuracy:0.3602, Validation Loss:1.4776, Validation Accuracy:0.3563\n",
    "Epoch #265: Loss:1.4507, Accuracy:0.3565, Validation Loss:1.4737, Validation Accuracy:0.3563\n",
    "Epoch #266: Loss:1.4516, Accuracy:0.3602, Validation Loss:1.4759, Validation Accuracy:0.3580\n",
    "Epoch #267: Loss:1.4490, Accuracy:0.3585, Validation Loss:1.4804, Validation Accuracy:0.3498\n",
    "Epoch #268: Loss:1.4648, Accuracy:0.3441, Validation Loss:1.4919, Validation Accuracy:0.3498\n",
    "Epoch #269: Loss:1.4648, Accuracy:0.3577, Validation Loss:1.5203, Validation Accuracy:0.3103\n",
    "Epoch #270: Loss:1.4968, Accuracy:0.3277, Validation Loss:1.4754, Validation Accuracy:0.3448\n",
    "Epoch #271: Loss:1.4819, Accuracy:0.3331, Validation Loss:1.4863, Validation Accuracy:0.3432\n",
    "Epoch #272: Loss:1.4737, Accuracy:0.3483, Validation Loss:1.4838, Validation Accuracy:0.3514\n",
    "Epoch #273: Loss:1.4691, Accuracy:0.3441, Validation Loss:1.5060, Validation Accuracy:0.3300\n",
    "Epoch #274: Loss:1.4645, Accuracy:0.3396, Validation Loss:1.4762, Validation Accuracy:0.3514\n",
    "Epoch #275: Loss:1.4586, Accuracy:0.3495, Validation Loss:1.4724, Validation Accuracy:0.3580\n",
    "Epoch #276: Loss:1.4558, Accuracy:0.3552, Validation Loss:1.5046, Validation Accuracy:0.3448\n",
    "Epoch #277: Loss:1.4639, Accuracy:0.3515, Validation Loss:1.4719, Validation Accuracy:0.3612\n",
    "Epoch #278: Loss:1.4509, Accuracy:0.3589, Validation Loss:1.4804, Validation Accuracy:0.3498\n",
    "Epoch #279: Loss:1.4639, Accuracy:0.3524, Validation Loss:1.4709, Validation Accuracy:0.3612\n",
    "Epoch #280: Loss:1.4519, Accuracy:0.3577, Validation Loss:1.4910, Validation Accuracy:0.3530\n",
    "Epoch #281: Loss:1.4501, Accuracy:0.3581, Validation Loss:1.4695, Validation Accuracy:0.3678\n",
    "Epoch #282: Loss:1.4459, Accuracy:0.3618, Validation Loss:1.4702, Validation Accuracy:0.3695\n",
    "Epoch #283: Loss:1.4463, Accuracy:0.3581, Validation Loss:1.4703, Validation Accuracy:0.3612\n",
    "Epoch #284: Loss:1.4466, Accuracy:0.3622, Validation Loss:1.4877, Validation Accuracy:0.3514\n",
    "Epoch #285: Loss:1.4552, Accuracy:0.3503, Validation Loss:1.4767, Validation Accuracy:0.3563\n",
    "Epoch #286: Loss:1.4498, Accuracy:0.3643, Validation Loss:1.4708, Validation Accuracy:0.3645\n",
    "Epoch #287: Loss:1.4507, Accuracy:0.3598, Validation Loss:1.4722, Validation Accuracy:0.3563\n",
    "Epoch #288: Loss:1.4550, Accuracy:0.3651, Validation Loss:1.4812, Validation Accuracy:0.3481\n",
    "Epoch #289: Loss:1.4565, Accuracy:0.3643, Validation Loss:1.4700, Validation Accuracy:0.3662\n",
    "Epoch #290: Loss:1.4535, Accuracy:0.3515, Validation Loss:1.4760, Validation Accuracy:0.3530\n",
    "Epoch #291: Loss:1.4485, Accuracy:0.3602, Validation Loss:1.4813, Validation Accuracy:0.3530\n",
    "Epoch #292: Loss:1.4513, Accuracy:0.3483, Validation Loss:1.4711, Validation Accuracy:0.3645\n",
    "Epoch #293: Loss:1.4505, Accuracy:0.3598, Validation Loss:1.4691, Validation Accuracy:0.3612\n",
    "Epoch #294: Loss:1.4458, Accuracy:0.3630, Validation Loss:1.4693, Validation Accuracy:0.3662\n",
    "Epoch #295: Loss:1.4437, Accuracy:0.3618, Validation Loss:1.4821, Validation Accuracy:0.3465\n",
    "Epoch #296: Loss:1.4510, Accuracy:0.3618, Validation Loss:1.4715, Validation Accuracy:0.3596\n",
    "Epoch #297: Loss:1.4501, Accuracy:0.3618, Validation Loss:1.4847, Validation Accuracy:0.3399\n",
    "Epoch #298: Loss:1.4581, Accuracy:0.3511, Validation Loss:1.4736, Validation Accuracy:0.3596\n",
    "Epoch #299: Loss:1.4472, Accuracy:0.3647, Validation Loss:1.4877, Validation Accuracy:0.3563\n",
    "Epoch #300: Loss:1.4510, Accuracy:0.3602, Validation Loss:1.4692, Validation Accuracy:0.3629\n",
    "\n",
    "Test:\n",
    "Test Loss:1.46916866, Accuracy:0.3629\n",
    "Labels: ['01', '03', '04', '05', '02']\n",
    "Confusion Matrix:\n",
    "      01  03  04  05  02\n",
    "t:01  66   0  26  34   0\n",
    "t:03  42   0  42  31   0\n",
    "t:04  43   0  59  10   0\n",
    "t:05  40   0   6  96   0\n",
    "t:02  45   0  32  37   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.28      0.52      0.36       126\n",
    "          03       0.00      0.00      0.00       115\n",
    "          04       0.36      0.53      0.43       112\n",
    "          05       0.46      0.68      0.55       142\n",
    "          02       0.00      0.00      0.00       114\n",
    "\n",
    "    accuracy                           0.36       609\n",
    "   macro avg       0.22      0.35      0.27       609\n",
    "weighted avg       0.23      0.36      0.28       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 05:21:09 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6090766434207535, 1.6068818455650693, 1.6059360142020365, 1.6056839206144335, 1.605506375896911, 1.6054017541835266, 1.6053372134128814, 1.6053033746130556, 1.6052952233597955, 1.6053303102358614, 1.6052602108671943, 1.6052615669952042, 1.6051999718097631, 1.605189303850697, 1.6051307317658599, 1.6050765614204219, 1.6050093262066394, 1.6049306310456375, 1.6048089443952187, 1.6046856453853289, 1.6044563141166674, 1.6042538084615823, 1.60380363777549, 1.6032004070595176, 1.6023981271706191, 1.6007788052112597, 1.5984393880872303, 1.593809625589593, 1.5866083272767968, 1.577340245442633, 1.574352702483755, 1.5802855391807744, 1.5707473212666503, 1.5656368660026387, 1.5688497407487265, 1.5620530434625686, 1.5662264275824886, 1.5586547841775202, 1.5588237325154697, 1.5557510772557877, 1.5567024711317616, 1.555711483524742, 1.5545029941646533, 1.55674332879447, 1.5535721083971472, 1.553840906553472, 1.552766565423098, 1.5529732032753956, 1.5519603334232699, 1.5538665588657647, 1.5514413284746493, 1.551282052336068, 1.5539758332648692, 1.5500718971778606, 1.5525676049230916, 1.549522739913076, 1.5525162106468564, 1.5494554718139724, 1.5489846730271388, 1.5532636654200813, 1.5483569319808033, 1.5518325205115457, 1.547776480026433, 1.5500013143166729, 1.547440475626728, 1.5472760464757533, 1.5472772894625986, 1.5461825447520992, 1.5478785658509078, 1.5456737444318573, 1.5456685996407946, 1.5458218135269992, 1.5445123102473117, 1.5455243246895927, 1.5438632023549823, 1.5446827405779233, 1.5433347779150275, 1.5429104920874284, 1.5431409986148328, 1.5419866308594377, 1.5410679071799092, 1.5404741769940982, 1.5421535919844027, 1.540192679622882, 1.5385447967620123, 1.5423141473228317, 1.5374084681712936, 1.5367891211032085, 1.5411414017622498, 1.535536391590225, 1.5340878517169672, 1.5388161785692613, 1.5352667002450853, 1.532162499545243, 1.5331995479383296, 1.5308827815580446, 1.5288581865761668, 1.5301246838812366, 1.5258300969949106, 1.5309537698091154, 1.524571136300787, 1.5231525232443472, 1.5445551974041316, 1.5175177459842075, 1.5175305489444577, 1.5147809361785112, 1.5129192300226497, 1.5103043086814567, 1.509732533166757, 1.5098216024721394, 1.507297680687239, 1.5127100672432159, 1.5002269220273874, 1.4935140903360151, 1.4935661366420427, 1.5461158257204128, 1.5263573222951152, 1.5016586013223934, 1.489028806365378, 1.4874734896157176, 1.496334980078323, 1.4894444020510895, 1.4868501058744483, 1.500802501669071, 1.4948264174469195, 1.5026094475011715, 1.4863828146594695, 1.4831473600296747, 1.49056359758518, 1.4972999745793334, 1.4874657408161507, 1.4833145858031775, 1.4823934127544534, 1.4941546012615334, 1.4886214061715137, 1.4799619092925624, 1.4858960490704365, 1.4802160110379674, 1.4793926410878624, 1.479081250176641, 1.4875690071845094, 1.4810323206270466, 1.4891020842569411, 1.4781080785839038, 1.4844045883720536, 1.488497027818401, 1.4783641896615867, 1.4803346352428441, 1.4781957199225089, 1.477985948960378, 1.4779586868333112, 1.4775815475946186, 1.478295131857172, 1.4788362331969789, 1.4758958646229334, 1.476842452935593, 1.4790917713262373, 1.478357176083845, 1.4981937228360982, 1.4935368538294325, 1.4835058940063752, 1.4767848378527537, 1.4764336713624902, 1.4762357213031287, 1.4810173928444021, 1.4765149202252843, 1.4778675733528701, 1.484960879598345, 1.494544852347601, 1.5108162855671348, 1.477580647946187, 1.4994664875353108, 1.4840490778874489, 1.480094338481258, 1.4863540646673619, 1.47716472888815, 1.4814212547342962, 1.4914689001387171, 1.475821886939564, 1.476054003085996, 1.48248653811187, 1.4756417806904107, 1.484180645402429, 1.4745643587143746, 1.4912477224722676, 1.5402560218410148, 1.4755105533819088, 1.493116479006111, 1.4766175658832044, 1.4971673776363503, 1.4759112739406393, 1.476204841399232, 1.4755361749620861, 1.4771517505395197, 1.4753595081651936, 1.479376977104663, 1.475024699969049, 1.4799664415945168, 1.4745682679569387, 1.4819702193850564, 1.4742142523842297, 1.4789777326662161, 1.4796521953369792, 1.4893114402376373, 1.4739205682610448, 1.4745543439595765, 1.4742456972109665, 1.4762286378440794, 1.4874859427779374, 1.4876750120388462, 1.476581295722811, 1.493373775521327, 1.479426438193799, 1.4766123026658358, 1.5184739820279902, 1.4885622018272262, 1.4813760599283554, 1.478357698334066, 1.481082192391206, 1.4764629210940332, 1.4801690032329466, 1.477482181073018, 1.4750264417165997, 1.476534391858895, 1.4741919794301876, 1.4932588405405556, 1.4735110174063195, 1.4729960602884027, 1.473753898014576, 1.4732431950436045, 1.484813211978167, 1.4763210730012415, 1.473629326068709, 1.484764253760402, 1.4867086835291194, 1.4773750001769543, 1.476652307659143, 1.4793353830456537, 1.474468660276316, 1.4737392534763354, 1.4810448675515813, 1.481404994704649, 1.4741471606522358, 1.4732107804913825, 1.4719283800015504, 1.483289278003774, 1.4773701540942263, 1.4808410249515902, 1.4886654993191923, 1.4730950202456445, 1.4759826898966322, 1.485281151503765, 1.4763746623726706, 1.4713707956774482, 1.4718173511314079, 1.4740953854543626, 1.4729007596061343, 1.474443925425337, 1.4772627574861148, 1.4718995765707958, 1.4742898508441469, 1.4711519779242905, 1.4716931712646986, 1.4775596902092494, 1.473652354797902, 1.4758579611582514, 1.4804429172099323, 1.491866087482872, 1.520256674544173, 1.4753928773704617, 1.4863072798169892, 1.4837864302649286, 1.5060490618394122, 1.4761808335487479, 1.4724401936350981, 1.504598324326263, 1.471892408158, 1.480412082719098, 1.4709001957685097, 1.4910000184878145, 1.469541755803113, 1.470211157462084, 1.4703184697036868, 1.4876574622390697, 1.4767390753835292, 1.4708016464862916, 1.472247390715751, 1.4811641954631836, 1.4699954986572266, 1.4760408883024319, 1.4812830496695633, 1.471121077467068, 1.4690874030045884, 1.4693228779559457, 1.4820646041719785, 1.4714722331913037, 1.484705934579345, 1.4736441358165397, 1.4876812988118389, 1.4691684052274732], 'val_acc': [0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.275862068941049, 0.2463054185968706, 0.2988505725105016, 0.2955665003587851, 0.30049260873316935, 0.2775041050658437, 0.28407224956502275, 0.30049260883104234, 0.2857142834142707, 0.29720853658145285, 0.28407224728947594, 0.2955665004566581, 0.28571428370788965, 0.29720853648357987, 0.28899835595747914, 0.2955665004566581, 0.29720853648357987, 0.2824302113604272, 0.29720853648357987, 0.2955665004566581, 0.29228242791344966, 0.29720853658145285, 0.29885057241262863, 0.2840722474852219, 0.29228242801132265, 0.29228242801132265, 0.2824302112625542, 0.2972085363857069, 0.28571428361001666, 0.29556650026091214, 0.28571428361001666, 0.2906403917886549, 0.29556650026091214, 0.2824302112625542, 0.30049260873316935, 0.2873563197348114, 0.2939244641361174, 0.28571428370788965, 0.29556650016303915, 0.2972085362878339, 0.2972085362878339, 0.2939244641361174, 0.28899835585960615, 0.29556650026091214, 0.2939244640382444, 0.29228243018899647, 0.2939244641361174, 0.2939244641361174, 0.2972085363857069, 0.2889983556638602, 0.2939244640382444, 0.2906403917886549, 0.288998357939407, 0.28735632181461224, 0.2972085363857069, 0.2972085363857069, 0.2906403919844009, 0.2857142834142707, 0.29556650026091214, 0.29720853648357987, 0.2972085363857069, 0.2955665003587851, 0.29228242801132265, 0.30049260873316935, 0.30377668098275884, 0.2955665004566581, 0.30541871720542657, 0.30706075313447534, 0.3054187171075536, 0.31855500610591153, 0.31691297017686276, 0.31198686180247853, 0.3300492592730937, 0.32512315268489134, 0.33333333132693727, 0.3333333311311913, 0.3004926103236053, 0.35303776511809315, 0.3497536926727577, 0.3530377649223472, 0.3579638732967314, 0.3579638733946044, 0.34975369228126574, 0.35139572869967944, 0.36617405372495915, 0.3333333332109921, 0.3497536923791387, 0.3530377647266012, 0.3563218370740637, 0.31855500779422047, 0.32183908023955593, 0.34975369228126574, 0.341543511657292, 0.348111656254344, 0.33825943940770253, 0.3448275840047545, 0.35467980094926893, 0.33169129718407037, 0.34318554787995975, 0.3448275860845553, 0.35632183717193666, 0.3448275839068815, 0.36781609004549987, 0.3513957306816073, 0.3645320177959104, 0.361247945154829, 0.36288998127962374, 0.3333333310333183, 0.3382594396034485, 0.3546798010471419, 0.334975367353859, 0.35139572869967944, 0.361247945252702, 0.361247945252702, 0.3596059091279072, 0.36288998137749673, 0.33990147572824325, 0.36288998147536966, 0.33333333132693727, 0.33990147572824325, 0.35960590942152615, 0.36288998127962374, 0.3530377645308552, 0.35960590942152615, 0.35960590942152615, 0.3530377649223472, 0.34482758420050047, 0.33990147582611624, 0.3579638732967314, 0.3497536926727577, 0.36617405352921323, 0.35960590903003425, 0.348111656254344, 0.34975369228126574, 0.3513957305837343, 0.35632183717193666, 0.3579638732967314, 0.35796387310098543, 0.36781608984975395, 0.35960590932365316, 0.3497536925748847, 0.334975367353859, 0.33333333132693727, 0.32019704203496036, 0.3612479455463209, 0.35139572850393347, 0.3530377646287282, 0.33990147582611624, 0.34154351204878397, 0.3513957288954254, 0.36288998137749673, 0.3530377646287282, 0.35960590942152615, 0.3563218390559915, 0.334975367451732, 0.3530377649223472, 0.33990147582611624, 0.3612479455463209, 0.348111656254344, 0.3136288995177092, 0.3464696202274222, 0.3464696205210412, 0.3530377647266012, 0.34646962230722306, 0.35632183726980965, 0.359605911305581, 0.36288998167111564, 0.35303776502022016, 0.35960590942152615, 0.3579638750829133, 0.3579638732967314, 0.33990147582611624, 0.35632183717193666, 0.3431855480757057, 0.3563218370740637, 0.35632183895811853, 0.36288998127962374, 0.3497536923791387, 0.3612479453505749, 0.3579638729052395, 0.3579638731988584, 0.359605911305581, 0.3497536923791387, 0.3497536923791387, 0.359605911305581, 0.348111656254344, 0.35632183895811853, 0.3399014759239892, 0.31198686141098664, 0.3530377646287282, 0.3513957305837343, 0.3464696206189142, 0.34482758420050047, 0.3579638750829133, 0.36288998127962374, 0.359605911207708, 0.3530377648244742, 0.3431855482714517, 0.36781609004549987, 0.3448275840047545, 0.36617405382283214, 0.3612479455463209, 0.3612479454484479, 0.35960590942152615, 0.3530377646287282, 0.359605911207708, 0.3579638732967314, 0.3513957284060605, 0.35139572850393347, 0.34811165664583593, 0.35303776680640203, 0.3513957306816073, 0.36453201967996524, 0.36288998167111564, 0.3513957284060605, 0.3513957284060605, 0.359605911305581, 0.36288998137749673, 0.3678160899476269, 0.3596059091279072, 0.359605911305581, 0.35303776680640203, 0.3431855482714517, 0.3563218370740637, 0.3579638729052395, 0.3530377645308552, 0.361247945154829, 0.36453201789378337, 0.36288998137749673, 0.3563218390559915, 0.3563218390559915, 0.35796387518078626, 0.3546798029311968, 0.361247945252702, 0.35139572879755243, 0.3645320177959104, 0.3612479453505749, 0.35632183678044477, 0.35632183717193666, 0.3579638729052395, 0.34975369228126574, 0.3497536923791387, 0.3103448253840648, 0.34482758429837346, 0.3431855481735787, 0.3513957288954254, 0.3300492587837288, 0.35139572879755243, 0.3579638732967314, 0.3448275839068815, 0.3612479453505749, 0.3497536926727577, 0.3612479455463209, 0.3530377646287282, 0.36781609014337285, 0.3694581261702946, 0.3612479454484479, 0.35139572860180646, 0.35632183678044477, 0.3645320176980374, 0.35632183736768264, 0.34811165664583593, 0.3661740540185781, 0.3530377646287282, 0.3530377646287282, 0.36453201750229147, 0.3612479455463209, 0.3661740539207051, 0.34646962003167625, 0.3596059096172721, 0.33990147582611624, 0.3596059092257802, 0.3563218368783177, 0.36288998167111564], 'loss': [1.6117420100088726, 1.6081048715775508, 1.6068131544751554, 1.6060714486198504, 1.6057046719889867, 1.6057270369245775, 1.6055312968867026, 1.6054818516149658, 1.60557161875574, 1.605593787375417, 1.6054966067386605, 1.6054073458579532, 1.6055344481243001, 1.605352112008316, 1.6052931388545575, 1.6052622593893409, 1.6051985981038464, 1.6050673948910692, 1.604977054370747, 1.6049404143797543, 1.6048920922700385, 1.604486463740621, 1.604027620918697, 1.6035170364673623, 1.6026160352283925, 1.6011384356927578, 1.598705555183442, 1.5949262840302327, 1.587747057654285, 1.5774499059457798, 1.5669360731661441, 1.562258736798406, 1.5630569371354652, 1.5576164389537834, 1.555780684727663, 1.5548187904044588, 1.5542928915004222, 1.554862003943269, 1.5516822180464038, 1.5511732988533788, 1.5522666917934065, 1.5519858315740034, 1.5502098596561127, 1.5514581598540351, 1.5514593266363752, 1.548663209742834, 1.5496052058080874, 1.548393727523835, 1.5506403289046866, 1.550516055154115, 1.5535080000605181, 1.551417793581373, 1.5478703588430887, 1.547083722835204, 1.5458450673786766, 1.5445649393285323, 1.5468800716086823, 1.5469404654825982, 1.5467441857473072, 1.5439082864373617, 1.5437541338452567, 1.5434609603588096, 1.5460839329803748, 1.5468113595700117, 1.54460047299857, 1.5425228132114763, 1.5418284839183642, 1.542234372994738, 1.543947183522845, 1.5445514622655003, 1.5425470265519692, 1.5411749853000993, 1.5404473516730557, 1.5393298003218256, 1.540117838984887, 1.5396704261552627, 1.540153508019888, 1.5391824665010831, 1.537801214703789, 1.5376336575533576, 1.5362398197518727, 1.5362652281226563, 1.535881598039819, 1.5355620192796053, 1.5354298146108827, 1.534253633830092, 1.5332960277612204, 1.5339111749641214, 1.5324862009690772, 1.531712685228618, 1.532248088613428, 1.5296165838378655, 1.5288307195326631, 1.5304350689451307, 1.530838564778745, 1.5270956015929549, 1.5251494602501026, 1.5244088406435519, 1.5259634711659176, 1.5213643765302653, 1.5199402672064624, 1.518233738200131, 1.5169225879763186, 1.520223425594933, 1.5202913027279676, 1.516269209301692, 1.5096232936122824, 1.5073636028066553, 1.5045780604869678, 1.5024218725717533, 1.5009631338060758, 1.5065051103764246, 1.4978823852734888, 1.4905135514555037, 1.4903615496241827, 1.5053333633978998, 1.5049276089521404, 1.4874896440173078, 1.4833555279815955, 1.4828713539444691, 1.47604516840079, 1.4848390254151895, 1.4786140317055234, 1.4748301532479038, 1.4815778061104996, 1.4952527988617914, 1.4838461322706094, 1.4822542370222433, 1.4747831518889942, 1.4813956081500044, 1.4799928556967075, 1.4731186586728575, 1.4719569232184784, 1.473116632849284, 1.4820290773556217, 1.4749865207339214, 1.470277674193255, 1.4738598910690088, 1.4681787932922707, 1.4685377679566338, 1.4711939411241661, 1.4801134504087161, 1.4745286847531673, 1.4763210182072446, 1.4689640135246136, 1.4719662189973208, 1.471376470520756, 1.4664293433116937, 1.464798031891151, 1.4655755943830988, 1.4688446410138014, 1.470219801338791, 1.4657251081917075, 1.4696226072996794, 1.4684996177528429, 1.4668048456219431, 1.465763414028489, 1.464686819023665, 1.4642834284956696, 1.473820349616926, 1.4813994158954347, 1.4753380610957527, 1.4649506778442885, 1.4671354612041059, 1.4647977283113547, 1.466535168996337, 1.4619820494426594, 1.4651859494450157, 1.472858153672189, 1.4821322669728336, 1.4933982501529326, 1.4799631280820718, 1.4724706128392622, 1.4740265189744608, 1.4747174747181135, 1.4722169035514032, 1.4763392126780517, 1.4651924442706412, 1.4682372131876387, 1.4673688845957573, 1.4632446318926018, 1.4670040752853457, 1.4617298769999822, 1.4715840159010838, 1.4613508611244344, 1.4789065199955778, 1.4864596967579649, 1.4807583596916905, 1.4779885629364107, 1.4669112671082514, 1.4706766034543393, 1.4612222240201256, 1.4602031210364748, 1.4598550689293863, 1.4633842489313051, 1.4592756976337158, 1.4587994944633156, 1.4586996320826318, 1.466362590172942, 1.4669678179879941, 1.4735959710526516, 1.4603842749977503, 1.4582614085512728, 1.4600330070793262, 1.4616925054507088, 1.4557988899689192, 1.4577552222618086, 1.4620422195360157, 1.4603328185894162, 1.461706796563871, 1.4618581344949146, 1.4568199907240191, 1.4638259951338877, 1.470632966538964, 1.474530470346768, 1.480269971177808, 1.464867335178524, 1.466279115862915, 1.4652091093866242, 1.465298070604062, 1.4553591324318607, 1.4646189890847803, 1.464155772726149, 1.4583533109825495, 1.4572411723695007, 1.455208523317529, 1.4604296480116168, 1.4567211889877947, 1.4569028384386882, 1.4545109141778652, 1.4522518402252353, 1.4570601665998142, 1.456864344412786, 1.4529164813627207, 1.459572790731395, 1.4596657310912742, 1.4532089891374969, 1.4525313489490956, 1.4589668752231637, 1.4537302347668877, 1.451316607463531, 1.4536187490643417, 1.454870779166721, 1.4519650558671422, 1.4504564273039173, 1.4500291438073347, 1.4531168708566278, 1.4570012969892372, 1.4558211656076951, 1.4608291405182354, 1.4535184734411064, 1.4555153704277055, 1.4527256046232502, 1.450683292128467, 1.4504677534103394, 1.448474856421688, 1.4501497740618257, 1.4545668858522263, 1.456844364837944, 1.451695348888452, 1.4498767789629206, 1.4524040572697132, 1.451796317198438, 1.4479288661259646, 1.450667623181118, 1.4516064157720954, 1.4490174927995434, 1.464777143682053, 1.4648121256113542, 1.4967700768789962, 1.48188061317624, 1.4736990431740544, 1.4691464103467655, 1.464508248256707, 1.4585700297502522, 1.455792374924223, 1.463913384600097, 1.450880609890274, 1.4639024390332263, 1.4518976279597509, 1.4501407888146152, 1.4459047602921786, 1.4462648483272451, 1.4465699221808808, 1.4552045546028403, 1.4497997808260594, 1.4506915445445254, 1.454956999304848, 1.456460795901884, 1.4535023616324705, 1.4484792605073056, 1.4512758979562372, 1.4504645394103972, 1.4457707785483014, 1.4437436608563214, 1.4510373257513653, 1.450131851642773, 1.4580575456854254, 1.4472406894519343, 1.451006951322301], 'acc': [0.22299794705251894, 0.2328542094945418, 0.2328542083195837, 0.23285420851541005, 0.23285420890706277, 0.23285420929871545, 0.23285421049203225, 0.23285420990455322, 0.23285420990455322, 0.2328542091028891, 0.23285420969036816, 0.23285420912124782, 0.23285420931707418, 0.23285421010037957, 0.23285420851541005, 0.23285420912124782, 0.23285421049203225, 0.2328542106694999, 0.23285421125697894, 0.23285420990455322, 0.2328542094945418, 0.23285421049203225, 0.2328542087112364, 0.2328542102778472, 0.2328542085337688, 0.23285420990455322, 0.24845995679038752, 0.2587268984538084, 0.27679671501966474, 0.2887063637528821, 0.29034907461681403, 0.29281314388682467, 0.2911704310279118, 0.2887063669228211, 0.28993839938537785, 0.2932238193140872, 0.2960985614656178, 0.288706365747863, 0.2919917862640514, 0.29486653178134736, 0.2948665286114084, 0.294045176900143, 0.29445585099333854, 0.2936344987313116, 0.293223816964171, 0.29733059471147993, 0.29527720959524356, 0.2952772054461728, 0.29281314169601735, 0.29609856283640223, 0.2936344951697199, 0.2969199172892365, 0.29774127154624436, 0.29568788682166064, 0.30143737147965716, 0.3026694055455421, 0.2940451763493814, 0.29733059748976626, 0.29774127252537613, 0.2977412711545917, 0.3006160144810804, 0.2989733059670646, 0.2944558500142068, 0.295687887017487, 0.2993839857759417, 0.30143737304626794, 0.3026694065246739, 0.30061601506855945, 0.29568788482667974, 0.2977412743245307, 0.30102669503654544, 0.2997946635531204, 0.3039014350339862, 0.3034907589825272, 0.3043121130437087, 0.30143737187130987, 0.3002053400329496, 0.3010266944490664, 0.3014373732788117, 0.30102669307828195, 0.30595482347927055, 0.30266940492134564, 0.300205338233795, 0.3010266919033239, 0.3018480476902251, 0.29856262776151576, 0.3014373708921781, 0.30061601467690674, 0.30143737187130987, 0.30390143522981256, 0.303080080581152, 0.30636550172153687, 0.30636550250484224, 0.30266940590047736, 0.3014373716754835, 0.29979466221905343, 0.3104722782938877, 0.314168380185564, 0.30841889161104047, 0.31252566951745836, 0.32114989616787654, 0.3178644783565396, 0.31909651042744364, 0.31498973483422454, 0.3330595480343155, 0.3215605728068146, 0.3264887071610476, 0.331006161743121, 0.33347022803901893, 0.34250513285826856, 0.33429157951774047, 0.33182751580430253, 0.333470224709971, 0.33593429218082704, 0.34866529971177573, 0.3351129355739029, 0.3268993845832911, 0.34209445547274253, 0.34661191005481584, 0.3408624243809702, 0.35071868365305403, 0.34989733256598515, 0.3457905567769397, 0.35893224035934745, 0.35195071928554983, 0.3227926101650301, 0.3375770042197171, 0.34455852313942487, 0.3433264867236237, 0.3462012316167232, 0.342915813450451, 0.35030800642663695, 0.35646817383090573, 0.345379876417301, 0.3388090366822738, 0.3478439436923307, 0.35770020750513803, 0.34989733295763786, 0.3478439440839834, 0.3498973311952007, 0.3511293646001718, 0.3462012310659616, 0.3457905532520654, 0.3412730992207537, 0.3544147826073351, 0.34496919817503474, 0.3482546224853586, 0.3519507200688552, 0.35359342897452367, 0.34825461911959327, 0.3560574930796143, 0.35359343152026623, 0.3494866509579535, 0.3548254641786493, 0.34866529575853134, 0.35236139510446507, 0.3527720749133421, 0.35687884968653844, 0.35441478675640586, 0.34743326627008725, 0.34702258826036475, 0.3408624251642756, 0.3511293622502556, 0.3490759733766011, 0.35154003908502, 0.35605749527042163, 0.3556468160490236, 0.3531827493614729, 0.34373716790328523, 0.3462012324000286, 0.33182751678343425, 0.334702257527463, 0.34496919876251375, 0.3486652999076021, 0.3425051350490758, 0.34537987500979916, 0.3445585215360967, 0.3498973327618115, 0.35112936581184734, 0.34907597556740844, 0.35482546398282294, 0.3486652959543577, 0.34907597513903826, 0.3503080066224633, 0.35441478456559855, 0.34086242496844926, 0.3355236145627572, 0.337577003632238, 0.3408624253601019, 0.34948665295293446, 0.3503080094007496, 0.3605749480533404, 0.35030800959657593, 0.35482546237949475, 0.3503080077974214, 0.3519507174863952, 0.3507186850605559, 0.36016427340938323, 0.35482546198784204, 0.34620123005011244, 0.34414784235141604, 0.35811088156161613, 0.35236139432115965, 0.35359342776284813, 0.3577002066851152, 0.35482546159618933, 0.35359342697954277, 0.3429158098888593, 0.35236139827440405, 0.3511293646368892, 0.3515400394766727, 0.35687885027401745, 0.34989732861274075, 0.34948665216962904, 0.34620123044176515, 0.3334702282348453, 0.35112936283773466, 0.3453798763805836, 0.3535934273711955, 0.3482546224853586, 0.35564681546154453, 0.34496919700007667, 0.3535934311286135, 0.36098562469227846, 0.35195071885717966, 0.3544147822156824, 0.3527720758924739, 0.35154004186330634, 0.3544147851897951, 0.3498973292369373, 0.35359342913363256, 0.3482546224853586, 0.35318275229886814, 0.35482546002957854, 0.35646817187264224, 0.3622176573506616, 0.3622176583297933, 0.3572895283204574, 0.35154004225495905, 0.36180698012424445, 0.35687884831575395, 0.3593429164108065, 0.3589322362102767, 0.3609856276296737, 0.35646817226429495, 0.3581108823449215, 0.35975359281720076, 0.35852156094212306, 0.3556468196106153, 0.3560574972286851, 0.35605749284707056, 0.360985624068082, 0.3601642704352706, 0.3601642714144023, 0.3577002057427009, 0.360985627238021, 0.3560574932754407, 0.3490759735724275, 0.35605749703285877, 0.35934291738993823, 0.3622176593089251, 0.3642710460284897, 0.3572895292628717, 0.3601642720385988, 0.35646817304760037, 0.3601642730177306, 0.358521560354644, 0.3441478453255287, 0.3577002071134853, 0.3277207409943888, 0.33305954705518376, 0.3482546193154196, 0.34414784512970237, 0.3396303915267608, 0.34948665158215, 0.3552361405850436, 0.3515400402599781, 0.3589322381685402, 0.35236139510446507, 0.35770020492267807, 0.3581108853190342, 0.3618069799284181, 0.35811088214909514, 0.362217658721446, 0.35030800760159503, 0.3642710485742322, 0.35975359144641633, 0.3650924008729766, 0.3642710470076214, 0.3515400433931997, 0.36016426984779154, 0.34825462013544245, 0.35975359125059, 0.3630390127459101, 0.3618069827067044, 0.36180697973259174, 0.3618069803200708, 0.35112936542019463, 0.3646817254089966, 0.3601642690644862]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
