{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf23.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 13:04:23 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '1', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '03', '05', '01', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001C39AE6BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001C397606EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6091, Accuracy:0.2246, Validation Loss:1.6066, Validation Accuracy:0.2365\n",
    "Epoch #2: Loss:1.6052, Accuracy:0.2402, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6050, Accuracy:0.2316, Validation Loss:1.6055, Validation Accuracy:0.2381\n",
    "Epoch #4: Loss:1.6051, Accuracy:0.2300, Validation Loss:1.6062, Validation Accuracy:0.2430\n",
    "Epoch #5: Loss:1.6057, Accuracy:0.2300, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #6: Loss:1.6046, Accuracy:0.2292, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2299\n",
    "Epoch #8: Loss:1.6044, Accuracy:0.2324, Validation Loss:1.6057, Validation Accuracy:0.2348\n",
    "Epoch #9: Loss:1.6043, Accuracy:0.2341, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #10: Loss:1.6038, Accuracy:0.2341, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6024, Accuracy:0.2378, Validation Loss:1.6068, Validation Accuracy:0.2315\n",
    "Epoch #14: Loss:1.6020, Accuracy:0.2407, Validation Loss:1.6072, Validation Accuracy:0.2299\n",
    "Epoch #15: Loss:1.6015, Accuracy:0.2398, Validation Loss:1.6072, Validation Accuracy:0.2315\n",
    "Epoch #16: Loss:1.6014, Accuracy:0.2398, Validation Loss:1.6079, Validation Accuracy:0.2299\n",
    "Epoch #17: Loss:1.6011, Accuracy:0.2386, Validation Loss:1.6075, Validation Accuracy:0.2233\n",
    "Epoch #18: Loss:1.6022, Accuracy:0.2370, Validation Loss:1.6083, Validation Accuracy:0.2282\n",
    "Epoch #19: Loss:1.6015, Accuracy:0.2398, Validation Loss:1.6090, Validation Accuracy:0.2315\n",
    "Epoch #20: Loss:1.6015, Accuracy:0.2407, Validation Loss:1.6093, Validation Accuracy:0.2315\n",
    "Epoch #21: Loss:1.6009, Accuracy:0.2419, Validation Loss:1.6083, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6005, Accuracy:0.2370, Validation Loss:1.6089, Validation Accuracy:0.2135\n",
    "Epoch #23: Loss:1.6009, Accuracy:0.2304, Validation Loss:1.6090, Validation Accuracy:0.2167\n",
    "Epoch #24: Loss:1.6003, Accuracy:0.2374, Validation Loss:1.6097, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.5997, Accuracy:0.2419, Validation Loss:1.6105, Validation Accuracy:0.2365\n",
    "Epoch #26: Loss:1.5993, Accuracy:0.2415, Validation Loss:1.6107, Validation Accuracy:0.2233\n",
    "Epoch #27: Loss:1.5987, Accuracy:0.2382, Validation Loss:1.6114, Validation Accuracy:0.2200\n",
    "Epoch #28: Loss:1.5986, Accuracy:0.2349, Validation Loss:1.6125, Validation Accuracy:0.2266\n",
    "Epoch #29: Loss:1.5988, Accuracy:0.2378, Validation Loss:1.6127, Validation Accuracy:0.2282\n",
    "Epoch #30: Loss:1.5984, Accuracy:0.2378, Validation Loss:1.6123, Validation Accuracy:0.2414\n",
    "Epoch #31: Loss:1.5979, Accuracy:0.2427, Validation Loss:1.6120, Validation Accuracy:0.2217\n",
    "Epoch #32: Loss:1.5978, Accuracy:0.2345, Validation Loss:1.6121, Validation Accuracy:0.2200\n",
    "Epoch #33: Loss:1.5974, Accuracy:0.2320, Validation Loss:1.6126, Validation Accuracy:0.2299\n",
    "Epoch #34: Loss:1.5981, Accuracy:0.2341, Validation Loss:1.6123, Validation Accuracy:0.2233\n",
    "Epoch #35: Loss:1.5981, Accuracy:0.2394, Validation Loss:1.6135, Validation Accuracy:0.2217\n",
    "Epoch #36: Loss:1.5979, Accuracy:0.2431, Validation Loss:1.6141, Validation Accuracy:0.2282\n",
    "Epoch #37: Loss:1.5981, Accuracy:0.2402, Validation Loss:1.6134, Validation Accuracy:0.2200\n",
    "Epoch #38: Loss:1.5972, Accuracy:0.2402, Validation Loss:1.6124, Validation Accuracy:0.2217\n",
    "Epoch #39: Loss:1.5976, Accuracy:0.2444, Validation Loss:1.6126, Validation Accuracy:0.2233\n",
    "Epoch #40: Loss:1.5980, Accuracy:0.2366, Validation Loss:1.6110, Validation Accuracy:0.2430\n",
    "Epoch #41: Loss:1.6009, Accuracy:0.2345, Validation Loss:1.6098, Validation Accuracy:0.2282\n",
    "Epoch #42: Loss:1.6009, Accuracy:0.2279, Validation Loss:1.6085, Validation Accuracy:0.2250\n",
    "Epoch #43: Loss:1.5995, Accuracy:0.2300, Validation Loss:1.6116, Validation Accuracy:0.2381\n",
    "Epoch #44: Loss:1.6022, Accuracy:0.2337, Validation Loss:1.6140, Validation Accuracy:0.2463\n",
    "Epoch #45: Loss:1.5992, Accuracy:0.2394, Validation Loss:1.6130, Validation Accuracy:0.2200\n",
    "Epoch #46: Loss:1.5987, Accuracy:0.2345, Validation Loss:1.6124, Validation Accuracy:0.2184\n",
    "Epoch #47: Loss:1.5981, Accuracy:0.2431, Validation Loss:1.6106, Validation Accuracy:0.2200\n",
    "Epoch #48: Loss:1.5984, Accuracy:0.2394, Validation Loss:1.6105, Validation Accuracy:0.2250\n",
    "Epoch #49: Loss:1.5975, Accuracy:0.2423, Validation Loss:1.6118, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.5977, Accuracy:0.2456, Validation Loss:1.6121, Validation Accuracy:0.2397\n",
    "Epoch #51: Loss:1.5972, Accuracy:0.2460, Validation Loss:1.6115, Validation Accuracy:0.2414\n",
    "Epoch #52: Loss:1.5966, Accuracy:0.2460, Validation Loss:1.6111, Validation Accuracy:0.2365\n",
    "Epoch #53: Loss:1.5962, Accuracy:0.2452, Validation Loss:1.6115, Validation Accuracy:0.2381\n",
    "Epoch #54: Loss:1.5963, Accuracy:0.2464, Validation Loss:1.6114, Validation Accuracy:0.2365\n",
    "Epoch #55: Loss:1.5963, Accuracy:0.2448, Validation Loss:1.6124, Validation Accuracy:0.2348\n",
    "Epoch #56: Loss:1.5952, Accuracy:0.2444, Validation Loss:1.6116, Validation Accuracy:0.2365\n",
    "Epoch #57: Loss:1.5953, Accuracy:0.2456, Validation Loss:1.6113, Validation Accuracy:0.2348\n",
    "Epoch #58: Loss:1.5960, Accuracy:0.2460, Validation Loss:1.6122, Validation Accuracy:0.2365\n",
    "Epoch #59: Loss:1.5957, Accuracy:0.2452, Validation Loss:1.6127, Validation Accuracy:0.2365\n",
    "Epoch #60: Loss:1.5952, Accuracy:0.2480, Validation Loss:1.6125, Validation Accuracy:0.2365\n",
    "Epoch #61: Loss:1.5954, Accuracy:0.2456, Validation Loss:1.6121, Validation Accuracy:0.2365\n",
    "Epoch #62: Loss:1.5950, Accuracy:0.2460, Validation Loss:1.6129, Validation Accuracy:0.2365\n",
    "Epoch #63: Loss:1.5943, Accuracy:0.2493, Validation Loss:1.6124, Validation Accuracy:0.2282\n",
    "Epoch #64: Loss:1.5943, Accuracy:0.2460, Validation Loss:1.6129, Validation Accuracy:0.2315\n",
    "Epoch #65: Loss:1.5939, Accuracy:0.2464, Validation Loss:1.6137, Validation Accuracy:0.2365\n",
    "Epoch #66: Loss:1.5943, Accuracy:0.2464, Validation Loss:1.6147, Validation Accuracy:0.2365\n",
    "Epoch #67: Loss:1.5942, Accuracy:0.2476, Validation Loss:1.6133, Validation Accuracy:0.2315\n",
    "Epoch #68: Loss:1.5938, Accuracy:0.2460, Validation Loss:1.6129, Validation Accuracy:0.2332\n",
    "Epoch #69: Loss:1.5939, Accuracy:0.2464, Validation Loss:1.6131, Validation Accuracy:0.2250\n",
    "Epoch #70: Loss:1.5936, Accuracy:0.2468, Validation Loss:1.6137, Validation Accuracy:0.2282\n",
    "Epoch #71: Loss:1.5934, Accuracy:0.2460, Validation Loss:1.6138, Validation Accuracy:0.2299\n",
    "Epoch #72: Loss:1.5931, Accuracy:0.2456, Validation Loss:1.6141, Validation Accuracy:0.2266\n",
    "Epoch #73: Loss:1.5937, Accuracy:0.2480, Validation Loss:1.6140, Validation Accuracy:0.2266\n",
    "Epoch #74: Loss:1.5924, Accuracy:0.2489, Validation Loss:1.6152, Validation Accuracy:0.2217\n",
    "Epoch #75: Loss:1.5938, Accuracy:0.2460, Validation Loss:1.6152, Validation Accuracy:0.2250\n",
    "Epoch #76: Loss:1.5938, Accuracy:0.2402, Validation Loss:1.6155, Validation Accuracy:0.2200\n",
    "Epoch #77: Loss:1.5937, Accuracy:0.2464, Validation Loss:1.6155, Validation Accuracy:0.2282\n",
    "Epoch #78: Loss:1.5933, Accuracy:0.2476, Validation Loss:1.6107, Validation Accuracy:0.2332\n",
    "Epoch #79: Loss:1.5936, Accuracy:0.2493, Validation Loss:1.6103, Validation Accuracy:0.2348\n",
    "Epoch #80: Loss:1.5939, Accuracy:0.2444, Validation Loss:1.6123, Validation Accuracy:0.2414\n",
    "Epoch #81: Loss:1.5938, Accuracy:0.2468, Validation Loss:1.6139, Validation Accuracy:0.2348\n",
    "Epoch #82: Loss:1.5936, Accuracy:0.2444, Validation Loss:1.6134, Validation Accuracy:0.2332\n",
    "Epoch #83: Loss:1.5945, Accuracy:0.2456, Validation Loss:1.6136, Validation Accuracy:0.2315\n",
    "Epoch #84: Loss:1.5937, Accuracy:0.2456, Validation Loss:1.6160, Validation Accuracy:0.2299\n",
    "Epoch #85: Loss:1.5933, Accuracy:0.2448, Validation Loss:1.6138, Validation Accuracy:0.2299\n",
    "Epoch #86: Loss:1.5948, Accuracy:0.2407, Validation Loss:1.6183, Validation Accuracy:0.2332\n",
    "Epoch #87: Loss:1.5943, Accuracy:0.2480, Validation Loss:1.6143, Validation Accuracy:0.2003\n",
    "Epoch #88: Loss:1.6073, Accuracy:0.2386, Validation Loss:1.6101, Validation Accuracy:0.2036\n",
    "Epoch #89: Loss:1.6055, Accuracy:0.2172, Validation Loss:1.6144, Validation Accuracy:0.2069\n",
    "Epoch #90: Loss:1.6004, Accuracy:0.2337, Validation Loss:1.6126, Validation Accuracy:0.2299\n",
    "Epoch #91: Loss:1.5987, Accuracy:0.2366, Validation Loss:1.6133, Validation Accuracy:0.2266\n",
    "Epoch #92: Loss:1.5974, Accuracy:0.2378, Validation Loss:1.6160, Validation Accuracy:0.2315\n",
    "Epoch #93: Loss:1.5962, Accuracy:0.2378, Validation Loss:1.6154, Validation Accuracy:0.2250\n",
    "Epoch #94: Loss:1.5963, Accuracy:0.2448, Validation Loss:1.6161, Validation Accuracy:0.2233\n",
    "Epoch #95: Loss:1.5964, Accuracy:0.2489, Validation Loss:1.6173, Validation Accuracy:0.2200\n",
    "Epoch #96: Loss:1.5963, Accuracy:0.2468, Validation Loss:1.6158, Validation Accuracy:0.2118\n",
    "Epoch #97: Loss:1.5958, Accuracy:0.2493, Validation Loss:1.6166, Validation Accuracy:0.2085\n",
    "Epoch #98: Loss:1.5958, Accuracy:0.2460, Validation Loss:1.6190, Validation Accuracy:0.2135\n",
    "Epoch #99: Loss:1.5954, Accuracy:0.2460, Validation Loss:1.6165, Validation Accuracy:0.2135\n",
    "Epoch #100: Loss:1.5955, Accuracy:0.2485, Validation Loss:1.6163, Validation Accuracy:0.2036\n",
    "Epoch #101: Loss:1.5947, Accuracy:0.2530, Validation Loss:1.6173, Validation Accuracy:0.2085\n",
    "Epoch #102: Loss:1.5953, Accuracy:0.2493, Validation Loss:1.6185, Validation Accuracy:0.2217\n",
    "Epoch #103: Loss:1.5952, Accuracy:0.2423, Validation Loss:1.6170, Validation Accuracy:0.2184\n",
    "Epoch #104: Loss:1.5953, Accuracy:0.2476, Validation Loss:1.6154, Validation Accuracy:0.2118\n",
    "Epoch #105: Loss:1.5951, Accuracy:0.2509, Validation Loss:1.6170, Validation Accuracy:0.2069\n",
    "Epoch #106: Loss:1.5938, Accuracy:0.2497, Validation Loss:1.6193, Validation Accuracy:0.2167\n",
    "Epoch #107: Loss:1.5951, Accuracy:0.2448, Validation Loss:1.6184, Validation Accuracy:0.2167\n",
    "Epoch #108: Loss:1.5945, Accuracy:0.2468, Validation Loss:1.6162, Validation Accuracy:0.2053\n",
    "Epoch #109: Loss:1.5945, Accuracy:0.2538, Validation Loss:1.6169, Validation Accuracy:0.2036\n",
    "Epoch #110: Loss:1.5939, Accuracy:0.2480, Validation Loss:1.6191, Validation Accuracy:0.2036\n",
    "Epoch #111: Loss:1.5939, Accuracy:0.2448, Validation Loss:1.6176, Validation Accuracy:0.2003\n",
    "Epoch #112: Loss:1.5934, Accuracy:0.2497, Validation Loss:1.6178, Validation Accuracy:0.2003\n",
    "Epoch #113: Loss:1.5932, Accuracy:0.2476, Validation Loss:1.6190, Validation Accuracy:0.2003\n",
    "Epoch #114: Loss:1.5930, Accuracy:0.2460, Validation Loss:1.6184, Validation Accuracy:0.2003\n",
    "Epoch #115: Loss:1.5934, Accuracy:0.2501, Validation Loss:1.6194, Validation Accuracy:0.2053\n",
    "Epoch #116: Loss:1.5932, Accuracy:0.2493, Validation Loss:1.6186, Validation Accuracy:0.2003\n",
    "Epoch #117: Loss:1.5928, Accuracy:0.2485, Validation Loss:1.6194, Validation Accuracy:0.1987\n",
    "Epoch #118: Loss:1.5923, Accuracy:0.2509, Validation Loss:1.6187, Validation Accuracy:0.1987\n",
    "Epoch #119: Loss:1.5924, Accuracy:0.2517, Validation Loss:1.6181, Validation Accuracy:0.1987\n",
    "Epoch #120: Loss:1.5938, Accuracy:0.2439, Validation Loss:1.6194, Validation Accuracy:0.2036\n",
    "Epoch #121: Loss:1.5931, Accuracy:0.2435, Validation Loss:1.6202, Validation Accuracy:0.2036\n",
    "Epoch #122: Loss:1.5933, Accuracy:0.2542, Validation Loss:1.6206, Validation Accuracy:0.2020\n",
    "Epoch #123: Loss:1.5918, Accuracy:0.2435, Validation Loss:1.6180, Validation Accuracy:0.2003\n",
    "Epoch #124: Loss:1.5932, Accuracy:0.2517, Validation Loss:1.6184, Validation Accuracy:0.2053\n",
    "Epoch #125: Loss:1.5923, Accuracy:0.2448, Validation Loss:1.6186, Validation Accuracy:0.2102\n",
    "Epoch #126: Loss:1.5920, Accuracy:0.2464, Validation Loss:1.6191, Validation Accuracy:0.1987\n",
    "Epoch #127: Loss:1.5918, Accuracy:0.2526, Validation Loss:1.6201, Validation Accuracy:0.2085\n",
    "Epoch #128: Loss:1.5918, Accuracy:0.2538, Validation Loss:1.6186, Validation Accuracy:0.2003\n",
    "Epoch #129: Loss:1.5922, Accuracy:0.2472, Validation Loss:1.6195, Validation Accuracy:0.2020\n",
    "Epoch #130: Loss:1.5916, Accuracy:0.2472, Validation Loss:1.6202, Validation Accuracy:0.2053\n",
    "Epoch #131: Loss:1.5917, Accuracy:0.2526, Validation Loss:1.6206, Validation Accuracy:0.2069\n",
    "Epoch #132: Loss:1.5915, Accuracy:0.2546, Validation Loss:1.6183, Validation Accuracy:0.2069\n",
    "Epoch #133: Loss:1.5913, Accuracy:0.2542, Validation Loss:1.6191, Validation Accuracy:0.2069\n",
    "Epoch #134: Loss:1.5909, Accuracy:0.2538, Validation Loss:1.6197, Validation Accuracy:0.2053\n",
    "Epoch #135: Loss:1.5909, Accuracy:0.2546, Validation Loss:1.6195, Validation Accuracy:0.2069\n",
    "Epoch #136: Loss:1.5908, Accuracy:0.2579, Validation Loss:1.6192, Validation Accuracy:0.2135\n",
    "Epoch #137: Loss:1.5912, Accuracy:0.2546, Validation Loss:1.6197, Validation Accuracy:0.2069\n",
    "Epoch #138: Loss:1.5909, Accuracy:0.2493, Validation Loss:1.6195, Validation Accuracy:0.2167\n",
    "Epoch #139: Loss:1.5908, Accuracy:0.2530, Validation Loss:1.6187, Validation Accuracy:0.2135\n",
    "Epoch #140: Loss:1.5913, Accuracy:0.2501, Validation Loss:1.6192, Validation Accuracy:0.2069\n",
    "Epoch #141: Loss:1.5906, Accuracy:0.2534, Validation Loss:1.6187, Validation Accuracy:0.2151\n",
    "Epoch #142: Loss:1.5905, Accuracy:0.2509, Validation Loss:1.6208, Validation Accuracy:0.2069\n",
    "Epoch #143: Loss:1.5904, Accuracy:0.2509, Validation Loss:1.6205, Validation Accuracy:0.2069\n",
    "Epoch #144: Loss:1.5912, Accuracy:0.2550, Validation Loss:1.6208, Validation Accuracy:0.2135\n",
    "Epoch #145: Loss:1.5902, Accuracy:0.2575, Validation Loss:1.6193, Validation Accuracy:0.2151\n",
    "Epoch #146: Loss:1.5912, Accuracy:0.2472, Validation Loss:1.6195, Validation Accuracy:0.2217\n",
    "Epoch #147: Loss:1.5900, Accuracy:0.2575, Validation Loss:1.6193, Validation Accuracy:0.2184\n",
    "Epoch #148: Loss:1.5913, Accuracy:0.2530, Validation Loss:1.6181, Validation Accuracy:0.2069\n",
    "Epoch #149: Loss:1.5900, Accuracy:0.2563, Validation Loss:1.6176, Validation Accuracy:0.2184\n",
    "Epoch #150: Loss:1.5907, Accuracy:0.2517, Validation Loss:1.6198, Validation Accuracy:0.2151\n",
    "Epoch #151: Loss:1.5903, Accuracy:0.2595, Validation Loss:1.6202, Validation Accuracy:0.2135\n",
    "Epoch #152: Loss:1.5902, Accuracy:0.2571, Validation Loss:1.6200, Validation Accuracy:0.2135\n",
    "Epoch #153: Loss:1.5897, Accuracy:0.2575, Validation Loss:1.6182, Validation Accuracy:0.2118\n",
    "Epoch #154: Loss:1.5902, Accuracy:0.2534, Validation Loss:1.6197, Validation Accuracy:0.2085\n",
    "Epoch #155: Loss:1.5906, Accuracy:0.2563, Validation Loss:1.6210, Validation Accuracy:0.2135\n",
    "Epoch #156: Loss:1.5912, Accuracy:0.2534, Validation Loss:1.6197, Validation Accuracy:0.2184\n",
    "Epoch #157: Loss:1.5898, Accuracy:0.2632, Validation Loss:1.6199, Validation Accuracy:0.2135\n",
    "Epoch #158: Loss:1.5900, Accuracy:0.2575, Validation Loss:1.6196, Validation Accuracy:0.2020\n",
    "Epoch #159: Loss:1.5891, Accuracy:0.2575, Validation Loss:1.6183, Validation Accuracy:0.2135\n",
    "Epoch #160: Loss:1.5903, Accuracy:0.2485, Validation Loss:1.6196, Validation Accuracy:0.2299\n",
    "Epoch #161: Loss:1.5895, Accuracy:0.2624, Validation Loss:1.6195, Validation Accuracy:0.2135\n",
    "Epoch #162: Loss:1.5895, Accuracy:0.2604, Validation Loss:1.6189, Validation Accuracy:0.2151\n",
    "Epoch #163: Loss:1.5891, Accuracy:0.2559, Validation Loss:1.6201, Validation Accuracy:0.2217\n",
    "Epoch #164: Loss:1.5894, Accuracy:0.2591, Validation Loss:1.6195, Validation Accuracy:0.2151\n",
    "Epoch #165: Loss:1.5893, Accuracy:0.2595, Validation Loss:1.6203, Validation Accuracy:0.2151\n",
    "Epoch #166: Loss:1.5886, Accuracy:0.2616, Validation Loss:1.6195, Validation Accuracy:0.2167\n",
    "Epoch #167: Loss:1.5893, Accuracy:0.2559, Validation Loss:1.6196, Validation Accuracy:0.2184\n",
    "Epoch #168: Loss:1.5885, Accuracy:0.2632, Validation Loss:1.6204, Validation Accuracy:0.2135\n",
    "Epoch #169: Loss:1.5888, Accuracy:0.2587, Validation Loss:1.6209, Validation Accuracy:0.2151\n",
    "Epoch #170: Loss:1.5882, Accuracy:0.2595, Validation Loss:1.6196, Validation Accuracy:0.2184\n",
    "Epoch #171: Loss:1.5885, Accuracy:0.2583, Validation Loss:1.6200, Validation Accuracy:0.2151\n",
    "Epoch #172: Loss:1.5886, Accuracy:0.2567, Validation Loss:1.6201, Validation Accuracy:0.2135\n",
    "Epoch #173: Loss:1.5888, Accuracy:0.2604, Validation Loss:1.6196, Validation Accuracy:0.2151\n",
    "Epoch #174: Loss:1.5880, Accuracy:0.2600, Validation Loss:1.6200, Validation Accuracy:0.2151\n",
    "Epoch #175: Loss:1.5887, Accuracy:0.2608, Validation Loss:1.6199, Validation Accuracy:0.2151\n",
    "Epoch #176: Loss:1.5884, Accuracy:0.2522, Validation Loss:1.6194, Validation Accuracy:0.2167\n",
    "Epoch #177: Loss:1.5883, Accuracy:0.2612, Validation Loss:1.6209, Validation Accuracy:0.2135\n",
    "Epoch #178: Loss:1.5885, Accuracy:0.2587, Validation Loss:1.6198, Validation Accuracy:0.2102\n",
    "Epoch #179: Loss:1.5890, Accuracy:0.2509, Validation Loss:1.6203, Validation Accuracy:0.2266\n",
    "Epoch #180: Loss:1.5880, Accuracy:0.2608, Validation Loss:1.6214, Validation Accuracy:0.2151\n",
    "Epoch #181: Loss:1.5878, Accuracy:0.2600, Validation Loss:1.6218, Validation Accuracy:0.2135\n",
    "Epoch #182: Loss:1.5881, Accuracy:0.2567, Validation Loss:1.6213, Validation Accuracy:0.2200\n",
    "Epoch #183: Loss:1.5876, Accuracy:0.2616, Validation Loss:1.6208, Validation Accuracy:0.2167\n",
    "Epoch #184: Loss:1.5882, Accuracy:0.2616, Validation Loss:1.6215, Validation Accuracy:0.2167\n",
    "Epoch #185: Loss:1.5887, Accuracy:0.2542, Validation Loss:1.6208, Validation Accuracy:0.2102\n",
    "Epoch #186: Loss:1.5880, Accuracy:0.2509, Validation Loss:1.6211, Validation Accuracy:0.2151\n",
    "Epoch #187: Loss:1.5871, Accuracy:0.2616, Validation Loss:1.6214, Validation Accuracy:0.2151\n",
    "Epoch #188: Loss:1.5877, Accuracy:0.2587, Validation Loss:1.6207, Validation Accuracy:0.2135\n",
    "Epoch #189: Loss:1.5874, Accuracy:0.2608, Validation Loss:1.6206, Validation Accuracy:0.2167\n",
    "Epoch #190: Loss:1.5870, Accuracy:0.2624, Validation Loss:1.6202, Validation Accuracy:0.2167\n",
    "Epoch #191: Loss:1.5869, Accuracy:0.2620, Validation Loss:1.6213, Validation Accuracy:0.2167\n",
    "Epoch #192: Loss:1.5878, Accuracy:0.2616, Validation Loss:1.6207, Validation Accuracy:0.2184\n",
    "Epoch #193: Loss:1.5874, Accuracy:0.2604, Validation Loss:1.6206, Validation Accuracy:0.2151\n",
    "Epoch #194: Loss:1.5877, Accuracy:0.2624, Validation Loss:1.6200, Validation Accuracy:0.2135\n",
    "Epoch #195: Loss:1.5868, Accuracy:0.2612, Validation Loss:1.6207, Validation Accuracy:0.2151\n",
    "Epoch #196: Loss:1.5866, Accuracy:0.2632, Validation Loss:1.6216, Validation Accuracy:0.2151\n",
    "Epoch #197: Loss:1.5870, Accuracy:0.2595, Validation Loss:1.6221, Validation Accuracy:0.2135\n",
    "Epoch #198: Loss:1.5863, Accuracy:0.2604, Validation Loss:1.6214, Validation Accuracy:0.2167\n",
    "Epoch #199: Loss:1.5865, Accuracy:0.2628, Validation Loss:1.6209, Validation Accuracy:0.2167\n",
    "Epoch #200: Loss:1.5868, Accuracy:0.2608, Validation Loss:1.6209, Validation Accuracy:0.2151\n",
    "Epoch #201: Loss:1.5871, Accuracy:0.2505, Validation Loss:1.6208, Validation Accuracy:0.2299\n",
    "Epoch #202: Loss:1.5878, Accuracy:0.2579, Validation Loss:1.6230, Validation Accuracy:0.2151\n",
    "Epoch #203: Loss:1.5862, Accuracy:0.2649, Validation Loss:1.6205, Validation Accuracy:0.2167\n",
    "Epoch #204: Loss:1.5869, Accuracy:0.2554, Validation Loss:1.6222, Validation Accuracy:0.2151\n",
    "Epoch #205: Loss:1.5862, Accuracy:0.2616, Validation Loss:1.6213, Validation Accuracy:0.2151\n",
    "Epoch #206: Loss:1.5860, Accuracy:0.2620, Validation Loss:1.6198, Validation Accuracy:0.2167\n",
    "Epoch #207: Loss:1.5873, Accuracy:0.2579, Validation Loss:1.6199, Validation Accuracy:0.2102\n",
    "Epoch #208: Loss:1.5860, Accuracy:0.2587, Validation Loss:1.6243, Validation Accuracy:0.2135\n",
    "Epoch #209: Loss:1.5867, Accuracy:0.2604, Validation Loss:1.6206, Validation Accuracy:0.2151\n",
    "Epoch #210: Loss:1.5892, Accuracy:0.2600, Validation Loss:1.6204, Validation Accuracy:0.2299\n",
    "Epoch #211: Loss:1.5871, Accuracy:0.2567, Validation Loss:1.6261, Validation Accuracy:0.2151\n",
    "Epoch #212: Loss:1.5863, Accuracy:0.2591, Validation Loss:1.6205, Validation Accuracy:0.2167\n",
    "Epoch #213: Loss:1.5860, Accuracy:0.2567, Validation Loss:1.6194, Validation Accuracy:0.2266\n",
    "Epoch #214: Loss:1.5858, Accuracy:0.2591, Validation Loss:1.6225, Validation Accuracy:0.2135\n",
    "Epoch #215: Loss:1.5860, Accuracy:0.2591, Validation Loss:1.6217, Validation Accuracy:0.2151\n",
    "Epoch #216: Loss:1.5855, Accuracy:0.2620, Validation Loss:1.6211, Validation Accuracy:0.2184\n",
    "Epoch #217: Loss:1.5851, Accuracy:0.2612, Validation Loss:1.6226, Validation Accuracy:0.2151\n",
    "Epoch #218: Loss:1.5853, Accuracy:0.2604, Validation Loss:1.6224, Validation Accuracy:0.2151\n",
    "Epoch #219: Loss:1.5850, Accuracy:0.2579, Validation Loss:1.6209, Validation Accuracy:0.2167\n",
    "Epoch #220: Loss:1.5851, Accuracy:0.2616, Validation Loss:1.6225, Validation Accuracy:0.2151\n",
    "Epoch #221: Loss:1.5855, Accuracy:0.2620, Validation Loss:1.6226, Validation Accuracy:0.2151\n",
    "Epoch #222: Loss:1.5847, Accuracy:0.2604, Validation Loss:1.6212, Validation Accuracy:0.2135\n",
    "Epoch #223: Loss:1.5855, Accuracy:0.2612, Validation Loss:1.6209, Validation Accuracy:0.2167\n",
    "Epoch #224: Loss:1.5850, Accuracy:0.2559, Validation Loss:1.6227, Validation Accuracy:0.2135\n",
    "Epoch #225: Loss:1.5853, Accuracy:0.2620, Validation Loss:1.6234, Validation Accuracy:0.2151\n",
    "Epoch #226: Loss:1.5850, Accuracy:0.2624, Validation Loss:1.6216, Validation Accuracy:0.2167\n",
    "Epoch #227: Loss:1.5846, Accuracy:0.2600, Validation Loss:1.6232, Validation Accuracy:0.2151\n",
    "Epoch #228: Loss:1.5848, Accuracy:0.2550, Validation Loss:1.6212, Validation Accuracy:0.2151\n",
    "Epoch #229: Loss:1.5843, Accuracy:0.2616, Validation Loss:1.6227, Validation Accuracy:0.2151\n",
    "Epoch #230: Loss:1.5843, Accuracy:0.2612, Validation Loss:1.6210, Validation Accuracy:0.2151\n",
    "Epoch #231: Loss:1.5842, Accuracy:0.2538, Validation Loss:1.6212, Validation Accuracy:0.2151\n",
    "Epoch #232: Loss:1.5844, Accuracy:0.2632, Validation Loss:1.6225, Validation Accuracy:0.2151\n",
    "Epoch #233: Loss:1.5844, Accuracy:0.2612, Validation Loss:1.6222, Validation Accuracy:0.2167\n",
    "Epoch #234: Loss:1.5845, Accuracy:0.2546, Validation Loss:1.6224, Validation Accuracy:0.2151\n",
    "Epoch #235: Loss:1.5836, Accuracy:0.2608, Validation Loss:1.6258, Validation Accuracy:0.2151\n",
    "Epoch #236: Loss:1.5838, Accuracy:0.2616, Validation Loss:1.6240, Validation Accuracy:0.2151\n",
    "Epoch #237: Loss:1.5842, Accuracy:0.2550, Validation Loss:1.6224, Validation Accuracy:0.2184\n",
    "Epoch #238: Loss:1.5834, Accuracy:0.2608, Validation Loss:1.6248, Validation Accuracy:0.2151\n",
    "Epoch #239: Loss:1.5850, Accuracy:0.2628, Validation Loss:1.6221, Validation Accuracy:0.2151\n",
    "Epoch #240: Loss:1.5831, Accuracy:0.2624, Validation Loss:1.6239, Validation Accuracy:0.2151\n",
    "Epoch #241: Loss:1.5836, Accuracy:0.2608, Validation Loss:1.6220, Validation Accuracy:0.2200\n",
    "Epoch #242: Loss:1.5851, Accuracy:0.2468, Validation Loss:1.6229, Validation Accuracy:0.2151\n",
    "Epoch #243: Loss:1.5848, Accuracy:0.2567, Validation Loss:1.6241, Validation Accuracy:0.2151\n",
    "Epoch #244: Loss:1.5843, Accuracy:0.2604, Validation Loss:1.6209, Validation Accuracy:0.2299\n",
    "Epoch #245: Loss:1.5831, Accuracy:0.2616, Validation Loss:1.6252, Validation Accuracy:0.2184\n",
    "Epoch #246: Loss:1.5836, Accuracy:0.2628, Validation Loss:1.6236, Validation Accuracy:0.2167\n",
    "Epoch #247: Loss:1.5831, Accuracy:0.2641, Validation Loss:1.6230, Validation Accuracy:0.2167\n",
    "Epoch #248: Loss:1.5831, Accuracy:0.2628, Validation Loss:1.6251, Validation Accuracy:0.2135\n",
    "Epoch #249: Loss:1.5831, Accuracy:0.2554, Validation Loss:1.6224, Validation Accuracy:0.2200\n",
    "Epoch #250: Loss:1.5830, Accuracy:0.2628, Validation Loss:1.6244, Validation Accuracy:0.2167\n",
    "Epoch #251: Loss:1.5826, Accuracy:0.2624, Validation Loss:1.6224, Validation Accuracy:0.2200\n",
    "Epoch #252: Loss:1.5826, Accuracy:0.2628, Validation Loss:1.6244, Validation Accuracy:0.2151\n",
    "Epoch #253: Loss:1.5823, Accuracy:0.2608, Validation Loss:1.6235, Validation Accuracy:0.2151\n",
    "Epoch #254: Loss:1.5828, Accuracy:0.2620, Validation Loss:1.6220, Validation Accuracy:0.2167\n",
    "Epoch #255: Loss:1.5818, Accuracy:0.2632, Validation Loss:1.6226, Validation Accuracy:0.2167\n",
    "Epoch #256: Loss:1.5828, Accuracy:0.2612, Validation Loss:1.6238, Validation Accuracy:0.2167\n",
    "Epoch #257: Loss:1.5842, Accuracy:0.2567, Validation Loss:1.6233, Validation Accuracy:0.2151\n",
    "Epoch #258: Loss:1.5808, Accuracy:0.2665, Validation Loss:1.6259, Validation Accuracy:0.2167\n",
    "Epoch #259: Loss:1.5851, Accuracy:0.2600, Validation Loss:1.6231, Validation Accuracy:0.2167\n",
    "Epoch #260: Loss:1.5839, Accuracy:0.2571, Validation Loss:1.6223, Validation Accuracy:0.2167\n",
    "Epoch #261: Loss:1.5826, Accuracy:0.2604, Validation Loss:1.6269, Validation Accuracy:0.2151\n",
    "Epoch #262: Loss:1.5823, Accuracy:0.2616, Validation Loss:1.6221, Validation Accuracy:0.2200\n",
    "Epoch #263: Loss:1.5820, Accuracy:0.2616, Validation Loss:1.6221, Validation Accuracy:0.2200\n",
    "Epoch #264: Loss:1.5828, Accuracy:0.2542, Validation Loss:1.6258, Validation Accuracy:0.2184\n",
    "Epoch #265: Loss:1.5823, Accuracy:0.2620, Validation Loss:1.6237, Validation Accuracy:0.2200\n",
    "Epoch #266: Loss:1.5838, Accuracy:0.2513, Validation Loss:1.6245, Validation Accuracy:0.2200\n",
    "Epoch #267: Loss:1.5816, Accuracy:0.2632, Validation Loss:1.6242, Validation Accuracy:0.2167\n",
    "Epoch #268: Loss:1.5825, Accuracy:0.2628, Validation Loss:1.6224, Validation Accuracy:0.2200\n",
    "Epoch #269: Loss:1.5815, Accuracy:0.2608, Validation Loss:1.6252, Validation Accuracy:0.2184\n",
    "Epoch #270: Loss:1.5817, Accuracy:0.2612, Validation Loss:1.6222, Validation Accuracy:0.2200\n",
    "Epoch #271: Loss:1.5814, Accuracy:0.2624, Validation Loss:1.6230, Validation Accuracy:0.2200\n",
    "Epoch #272: Loss:1.5815, Accuracy:0.2641, Validation Loss:1.6242, Validation Accuracy:0.2167\n",
    "Epoch #273: Loss:1.5810, Accuracy:0.2587, Validation Loss:1.6236, Validation Accuracy:0.2167\n",
    "Epoch #274: Loss:1.5813, Accuracy:0.2653, Validation Loss:1.6240, Validation Accuracy:0.2167\n",
    "Epoch #275: Loss:1.5824, Accuracy:0.2645, Validation Loss:1.6237, Validation Accuracy:0.2167\n",
    "Epoch #276: Loss:1.5813, Accuracy:0.2641, Validation Loss:1.6254, Validation Accuracy:0.2167\n",
    "Epoch #277: Loss:1.5814, Accuracy:0.2600, Validation Loss:1.6251, Validation Accuracy:0.2167\n",
    "Epoch #278: Loss:1.5812, Accuracy:0.2567, Validation Loss:1.6230, Validation Accuracy:0.2200\n",
    "Epoch #279: Loss:1.5805, Accuracy:0.2616, Validation Loss:1.6262, Validation Accuracy:0.2167\n",
    "Epoch #280: Loss:1.5809, Accuracy:0.2637, Validation Loss:1.6229, Validation Accuracy:0.2200\n",
    "Epoch #281: Loss:1.5808, Accuracy:0.2608, Validation Loss:1.6247, Validation Accuracy:0.2200\n",
    "Epoch #282: Loss:1.5819, Accuracy:0.2624, Validation Loss:1.6257, Validation Accuracy:0.2167\n",
    "Epoch #283: Loss:1.5810, Accuracy:0.2637, Validation Loss:1.6234, Validation Accuracy:0.2217\n",
    "Epoch #284: Loss:1.5809, Accuracy:0.2554, Validation Loss:1.6236, Validation Accuracy:0.2217\n",
    "Epoch #285: Loss:1.5819, Accuracy:0.2641, Validation Loss:1.6241, Validation Accuracy:0.2200\n",
    "Epoch #286: Loss:1.5813, Accuracy:0.2637, Validation Loss:1.6266, Validation Accuracy:0.2167\n",
    "Epoch #287: Loss:1.5811, Accuracy:0.2522, Validation Loss:1.6248, Validation Accuracy:0.2200\n",
    "Epoch #288: Loss:1.5820, Accuracy:0.2649, Validation Loss:1.6249, Validation Accuracy:0.2167\n",
    "Epoch #289: Loss:1.5819, Accuracy:0.2595, Validation Loss:1.6215, Validation Accuracy:0.2299\n",
    "Epoch #290: Loss:1.5815, Accuracy:0.2604, Validation Loss:1.6278, Validation Accuracy:0.2217\n",
    "Epoch #291: Loss:1.5811, Accuracy:0.2612, Validation Loss:1.6238, Validation Accuracy:0.2167\n",
    "Epoch #292: Loss:1.5804, Accuracy:0.2575, Validation Loss:1.6241, Validation Accuracy:0.2233\n",
    "Epoch #293: Loss:1.5808, Accuracy:0.2501, Validation Loss:1.6257, Validation Accuracy:0.2200\n",
    "Epoch #294: Loss:1.5805, Accuracy:0.2641, Validation Loss:1.6251, Validation Accuracy:0.2167\n",
    "Epoch #295: Loss:1.5809, Accuracy:0.2645, Validation Loss:1.6226, Validation Accuracy:0.2250\n",
    "Epoch #296: Loss:1.5811, Accuracy:0.2485, Validation Loss:1.6230, Validation Accuracy:0.2217\n",
    "Epoch #297: Loss:1.5804, Accuracy:0.2649, Validation Loss:1.6249, Validation Accuracy:0.2167\n",
    "Epoch #298: Loss:1.5816, Accuracy:0.2583, Validation Loss:1.6285, Validation Accuracy:0.2167\n",
    "Epoch #299: Loss:1.5796, Accuracy:0.2628, Validation Loss:1.6235, Validation Accuracy:0.2299\n",
    "Epoch #300: Loss:1.5812, Accuracy:0.2587, Validation Loss:1.6237, Validation Accuracy:0.2282\n",
    "\n",
    "Test:\n",
    "Test Loss:1.62365806, Accuracy:0.2282\n",
    "Labels: ['02', '03', '05', '01', '04']\n",
    "Confusion Matrix:\n",
    "      02  03   05  01  04\n",
    "t:02   0   3   88   2  21\n",
    "t:03   0   4   92   0  19\n",
    "t:05   0   6  109   3  24\n",
    "t:01   1   4   96   1  24\n",
    "t:04   0   4   82   1  25\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.00      0.00      0.00       114\n",
    "          03       0.19      0.03      0.06       115\n",
    "          05       0.23      0.77      0.36       142\n",
    "          01       0.14      0.01      0.02       126\n",
    "          04       0.22      0.22      0.22       112\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.16      0.21      0.13       609\n",
    "weighted avg       0.16      0.23      0.14       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 13:20:18 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 54 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6065983553042358, 1.6051465306180255, 1.6055046611623027, 1.606237969375009, 1.60550744427836, 1.6055524805300734, 1.6055432436697197, 1.6056723813901002, 1.6056447594819594, 1.605718946222014, 1.6062711745451628, 1.606576934432357, 1.6067933057525083, 1.607246414781204, 1.6072218005097363, 1.6079456287456069, 1.6074687968725445, 1.6082680712779756, 1.6089898815687458, 1.609315476589798, 1.6083315360330792, 1.60887228640038, 1.6089956310190787, 1.6096967180765713, 1.6105296566764318, 1.6106730080981952, 1.6113600535150037, 1.6124954025929392, 1.612688938580906, 1.6122959728898674, 1.612035743317189, 1.61208772502705, 1.6126317300624253, 1.612287305258765, 1.613511841481151, 1.6141224884243042, 1.613415997408098, 1.612364811458807, 1.612563289640768, 1.611031847242847, 1.6098260944112768, 1.6084935297128211, 1.6116418895267306, 1.6139969440125088, 1.6129537084811232, 1.6123904837371876, 1.6106185253422052, 1.6105000726107894, 1.6117870698029968, 1.6121020814272375, 1.6115159352228952, 1.611122727002612, 1.611466958800756, 1.611412894745374, 1.6123788106226178, 1.6116248101044954, 1.6113444821196432, 1.6121853544990026, 1.6127495387896333, 1.612475139949905, 1.6120536562257213, 1.6128757583292443, 1.6124368023206839, 1.6129140822562482, 1.6136899231298412, 1.614666174198019, 1.6132873696059429, 1.6128933347504715, 1.6131281884041522, 1.613673419200728, 1.6138434028390594, 1.6140963097511254, 1.6140337205677002, 1.6152212672632904, 1.6152109508639683, 1.6155200824753209, 1.6155020675831435, 1.6106941531640164, 1.6103034105598437, 1.612343957075736, 1.6138701020007455, 1.6133989561563251, 1.6136408995329257, 1.6159651126767614, 1.6137921117209448, 1.6183318148301349, 1.6143090306048715, 1.6100856400475714, 1.61437324271805, 1.6125650131839446, 1.6133453337038288, 1.6160313589819546, 1.6154245423957436, 1.6161303034752657, 1.6173327480043684, 1.6158352258365924, 1.616562559686858, 1.619029803033337, 1.6165166118462098, 1.6163036517909009, 1.617259510436473, 1.618462801958344, 1.616981989644431, 1.6154093401772636, 1.6170471196104153, 1.6192531008242779, 1.6183602905821526, 1.61618175158164, 1.6168857538837127, 1.619050051778408, 1.6176148880095709, 1.6178245773456368, 1.6189998544887174, 1.6184423224287863, 1.6194382375488532, 1.6186484938184615, 1.6193657218920579, 1.6187495351424945, 1.618116699611808, 1.6194127097309907, 1.620175813610722, 1.6205804187480257, 1.6179722872469422, 1.6183883274717283, 1.6186301751285548, 1.6190977838434806, 1.6201371250089949, 1.6186481680971845, 1.6195015188899926, 1.6202353904595712, 1.6206348414100058, 1.6182553952159162, 1.6191090308190959, 1.6196903009915782, 1.6194858464897168, 1.619173195366006, 1.6196778170972426, 1.6194995406813222, 1.6187011238389415, 1.6192022808666886, 1.6186667220737352, 1.6208351335697768, 1.6205372336658546, 1.6208482812386624, 1.6192698860403352, 1.6194680704076105, 1.6193032687520745, 1.6180871398186645, 1.6176130654189387, 1.61982749737738, 1.6202333607697135, 1.6200210672293978, 1.618245995299178, 1.6196647113179925, 1.6210465157169036, 1.6197306820128743, 1.6198708911247441, 1.619561035644832, 1.618276044652967, 1.6196159586334855, 1.6195449382800775, 1.618918235274567, 1.620118908889971, 1.6195090842755948, 1.6203283568712683, 1.6195350491941856, 1.6195989073986685, 1.6203877806467768, 1.620919167701834, 1.619554900574958, 1.6199668379644259, 1.6200661911753012, 1.6196014644281422, 1.619971606531754, 1.6198733770984344, 1.6194479504633812, 1.6209139082036386, 1.6198452523189226, 1.6202770749532138, 1.6213903791211508, 1.6217702135859648, 1.621269922146852, 1.620812982565468, 1.621461584258745, 1.620755721391324, 1.6211472395409896, 1.6214093563004668, 1.6207496609006609, 1.6206319978084471, 1.6202218111708442, 1.621286397692801, 1.6206808673532922, 1.6206184005110917, 1.620000319527875, 1.6206667100267458, 1.6215754657347605, 1.6221455249488843, 1.6213694877420937, 1.6209032251721336, 1.6209005207459524, 1.6207985183092566, 1.6229599340404393, 1.6204896824700492, 1.622216893338609, 1.6213175398962838, 1.6197528122681115, 1.6198700425660082, 1.6243073169038018, 1.6206259676584078, 1.6203753396208063, 1.6261260278510734, 1.620541006669231, 1.6194063820470925, 1.6224856347286056, 1.6216734847411733, 1.6211124415859604, 1.6226246141643554, 1.622350158558299, 1.6209318246355982, 1.6225215588101416, 1.622619307882876, 1.6211611468999452, 1.620919581900285, 1.6226795559446212, 1.623410608576632, 1.6216017826045872, 1.6232162839281932, 1.621210602508194, 1.6226654741760154, 1.6210251167685723, 1.6212364856049737, 1.6225035771947776, 1.6221622606412138, 1.6223988157187776, 1.6257826547904555, 1.6239840823832796, 1.6224248965189767, 1.6248007088850676, 1.6221323514415322, 1.6239371239062406, 1.6219737995630024, 1.6229467761927638, 1.6240615778172935, 1.6208625403531078, 1.6251560224492365, 1.6235609128948894, 1.623002968398221, 1.6250641077805819, 1.6223704198311115, 1.624420190875362, 1.6223993109560562, 1.624393763213322, 1.6234550507393573, 1.621980271120181, 1.622553059815969, 1.623763533844345, 1.6232788400305511, 1.6258586034398947, 1.6230789491499977, 1.6223427444843237, 1.6268590103425025, 1.6220586628749454, 1.622140987557535, 1.6257880135318525, 1.6236650119665612, 1.6244528896507175, 1.6241929936291548, 1.6223858281700874, 1.6251661356642524, 1.6222287314670232, 1.6230011675353904, 1.6242377174703162, 1.6235821076801844, 1.6240096808654334, 1.6236572451583662, 1.6254194998388807, 1.625108340690876, 1.622970861558648, 1.6261593256090663, 1.6228619907877129, 1.6246723196972375, 1.6256893078486125, 1.623425968174864, 1.6236127229355435, 1.6240551091963042, 1.6265930961114041, 1.624845700898194, 1.6249330836563862, 1.621549855703595, 1.6277931566504618, 1.6238383970824368, 1.6241261571498926, 1.6257411975578722, 1.6251036318260657, 1.6225902856081382, 1.6229646699181919, 1.6249409375715334, 1.6285221570818296, 1.6234792827189655, 1.6236582025518558], 'val_acc': [0.23645320184810212, 0.2331691293048937, 0.2380952378750239, 0.24302134624940813, 0.23645320165235617, 0.2331691293048937, 0.2298850570553042, 0.23481116542968844, 0.23481116542968844, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.23152709318009895, 0.2298850570553042, 0.23152709327797194, 0.2298850571531772, 0.22331691245825225, 0.22824302093050947, 0.23152709327797194, 0.23152709327797194, 0.2331691293048937, 0.2134646955137378, 0.2167487678612003, 0.23316912940276668, 0.23645320184810212, 0.2233169125561252, 0.22003284020866276, 0.22660098480571472, 0.22824302102838243, 0.24137931022248635, 0.22167487643133046, 0.22003284011078977, 0.2298850571531772, 0.22331691245825225, 0.22167487652920345, 0.22824302102838243, 0.22003284030653572, 0.2216748763334575, 0.2233169126539982, 0.24302134624940813, 0.2282430212241284, 0.224958948583047, 0.23809523579522307, 0.2463054185968706, 0.22003284020866276, 0.21839080408386802, 0.2200328399150438, 0.22495894868091998, 0.23645320165235617, 0.23973727390194566, 0.2413793100267404, 0.23645320165235617, 0.23809523777715091, 0.23645320165235617, 0.23481116552756143, 0.23645320165235617, 0.23481116552756143, 0.23645320165235617, 0.23645320165235617, 0.23645320175022916, 0.23645320175022916, 0.23645320175022916, 0.22824302093050947, 0.23152709318009895, 0.23645320165235617, 0.23645320165235617, 0.23152709318009895, 0.2331691293048937, 0.224958948583047, 0.22824302083263648, 0.22988505695743122, 0.22660098480571472, 0.22660098480571472, 0.2216748763334575, 0.224958948583047, 0.22003284011078977, 0.22824302083263648, 0.23316912950063964, 0.23481116542968844, 0.2413793101246134, 0.23481116542968844, 0.2331691292070207, 0.23152709308222597, 0.22988505695743122, 0.2298850570553042, 0.2331691292070207, 0.2003284062217609, 0.2036124789607153, 0.20689655101455884, 0.22988505695743122, 0.22660098480571472, 0.23152709318009895, 0.224958948583047, 0.22331691245825225, 0.22003284020866276, 0.21182265909532413, 0.2085385870414806, 0.21346469541586482, 0.21346469541586482, 0.2036124784713504, 0.2085385870414806, 0.22167487623558452, 0.21839080388812204, 0.21182265938894307, 0.20689655091668585, 0.2167487676654543, 0.2167487676654543, 0.20525451537912898, 0.2036124784713504, 0.2036124784713504, 0.2003284062217609, 0.2003284062217609, 0.2003284062217609, 0.2003284062217609, 0.20525451537912898, 0.2003284062217609, 0.19868637068420403, 0.19868637068420403, 0.19868637068420403, 0.20361247856922338, 0.20361247925433423, 0.20197044234655565, 0.20032840690687176, 0.20525451537912898, 0.21018062316627534, 0.19868637068420403, 0.20853858772659145, 0.2003284062217609, 0.20197044234655565, 0.20525451537912898, 0.20689655150392372, 0.20689655150392372, 0.2068965516017967, 0.20525451537912898, 0.20689655150392372, 0.21346469619884867, 0.20689655150392372, 0.2167487678612003, 0.21346469619884867, 0.20689655150392372, 0.21510673004809663, 0.2068965516996697, 0.20689655150392372, 0.21346469619884867, 0.21510673004809663, 0.22167487652920345, 0.21839080249343207, 0.20689655150392372, 0.21839080408386802, 0.21510673004809663, 0.21346469619884867, 0.21346469619884867, 0.21182265938894307, 0.20853858762871846, 0.21346469619884867, 0.21839080408386802, 0.21346469619884867, 0.2019704431295395, 0.21346469610097568, 0.22988505734892314, 0.21346469619884867, 0.21510673004809663, 0.2216748763334575, 0.21510673004809663, 0.21510673004809663, 0.2167487678612003, 0.21839080408386802, 0.21346469619884867, 0.21510673004809663, 0.21839080408386802, 0.2151067323236434, 0.21346469619884867, 0.21510673004809663, 0.21510673004809663, 0.2151067323236434, 0.21674876617289138, 0.21346469619884867, 0.21018062365564025, 0.22660098490358768, 0.21510673004809663, 0.21346469619884867, 0.22003283861822684, 0.21674876617289138, 0.21674876617289138, 0.21018062365564025, 0.2151067323236434, 0.2151067323236434, 0.21346469610097568, 0.21674876617289138, 0.21674876617289138, 0.21674876617289138, 0.2183908023955591, 0.2151067323236434, 0.21346469610097568, 0.2151067323236434, 0.2151067323236434, 0.21346469610097568, 0.21674876617289138, 0.21674876617289138, 0.2151067323236434, 0.22988505734892314, 0.2151067323236434, 0.2167487678612003, 0.2151067323236434, 0.2151067323236434, 0.2167487678612003, 0.21018062365564025, 0.21346469619884867, 0.2151067323236434, 0.2298850571531772, 0.21510673004809663, 0.21674876844843816, 0.22660098480571472, 0.21346469610097568, 0.2151067323236434, 0.21839080398599503, 0.2151067323236434, 0.2151067323236434, 0.2167487677633273, 0.2151067323236434, 0.2151067323236434, 0.21346469610097568, 0.21674876844843816, 0.21346469610097568, 0.2151067323236434, 0.2167487678612003, 0.2151067323236434, 0.2151067323236434, 0.2151067323236434, 0.2151067323236434, 0.2151067323236434, 0.2151067323236434, 0.21674876844843816, 0.2151067323236434, 0.2151067323236434, 0.2151067323236434, 0.21839080398599503, 0.2151067323236434, 0.2151067323236434, 0.2151067323236434, 0.22003283842248086, 0.2151067323236434, 0.2151067323236434, 0.2298850571531772, 0.2183908023955591, 0.21674876844843816, 0.21674876844843816, 0.21346469610097568, 0.22003283842248086, 0.21674876844843816, 0.22003283842248086, 0.2151067323236434, 0.2151067323236434, 0.21674876844843816, 0.21674876844843816, 0.21674876835056517, 0.21510673212789747, 0.21674876844843816, 0.21674876844843816, 0.21674876835056517, 0.21510673222577043, 0.22003283842248086, 0.22003283842248086, 0.21839080229768612, 0.22003283842248086, 0.22003283861822684, 0.21674876844843816, 0.22003283842248086, 0.21839080229768612, 0.22003283842248086, 0.22003283842248086, 0.21674876844843816, 0.21674876844843816, 0.21674876844843816, 0.21674876844843816, 0.21674876844843816, 0.21674876835056517, 0.22003284069802764, 0.21674876835056517, 0.22003284069802764, 0.22003283842248086, 0.21674876844843816, 0.2216748746451486, 0.2216748746451486, 0.22003283842248086, 0.21674876835056517, 0.22003283842248086, 0.21674876844843816, 0.2298850571531772, 0.2216748746451486, 0.21674876835056517, 0.2233169126539982, 0.22003283852035385, 0.21674876844843816, 0.22495894709048403, 0.22167487474302158, 0.21674876835056517, 0.21674876835056517, 0.2298850570553042, 0.2282430211262554], 'loss': [1.6090577424674064, 1.605234353840963, 1.6050213023622422, 1.6051072811443947, 1.605724017429156, 1.6045620544735166, 1.604666767228066, 1.6043925927160212, 1.604282797141731, 1.6037639740800955, 1.6032692487234943, 1.6028299450629546, 1.6023776857270353, 1.6019771373247464, 1.6014550388716084, 1.601368087517897, 1.6010645242687123, 1.602176413203167, 1.601469936165232, 1.6015428408949772, 1.6009370555133546, 1.6004761791327162, 1.6009245736398245, 1.6002743619178599, 1.5997183276397737, 1.5993145011533703, 1.5986665041784487, 1.5985896464490792, 1.5987661198179335, 1.5983805262822146, 1.5979332745687183, 1.5977854393835675, 1.5973641666298775, 1.5981107228100913, 1.598120274387101, 1.5978955568963742, 1.5980583519906233, 1.5972180250978567, 1.5975590535013093, 1.5979609814512656, 1.600897606638178, 1.6009178830368074, 1.5995444100985048, 1.6022296921673251, 1.5991944432013823, 1.5987108632034834, 1.5980952858190516, 1.5984409627483611, 1.5975241235149469, 1.5977251232037555, 1.597178501319102, 1.5966269021651094, 1.5961815857544572, 1.5962662992536165, 1.5963037733179832, 1.5951949890634118, 1.595261193790475, 1.595989097462053, 1.5956509649875963, 1.5951996205034198, 1.5954292501512248, 1.595009774202194, 1.5942717499311945, 1.5943336906374357, 1.593864808190285, 1.5942945491117129, 1.594152204358847, 1.5938366684825513, 1.593947535667576, 1.593588459858904, 1.5934337169972288, 1.5930973650738445, 1.5936603155469014, 1.592350831266791, 1.593817809621901, 1.5937720816238214, 1.5937001030547908, 1.593303601795643, 1.5936259672627067, 1.593856437152416, 1.5937988512325092, 1.5936098665182596, 1.5945457708174688, 1.593668918296297, 1.593329870088879, 1.5947855838759968, 1.5943263342737908, 1.6073296888163446, 1.6055204086969521, 1.6004050402181105, 1.5986686270829344, 1.597390469339594, 1.5962488528884167, 1.5963097092552108, 1.5964103335962159, 1.596272804164299, 1.595788998917143, 1.595832835281654, 1.5954118516655673, 1.5954676881218348, 1.5947135602179494, 1.5952872317429685, 1.5951855737325837, 1.5952910900605533, 1.5950994167484542, 1.5937824932211968, 1.5950551276823823, 1.5944578088039736, 1.5945470069712926, 1.5939198621244646, 1.5939331785609345, 1.5934157903679098, 1.5932012284805643, 1.5929876947305042, 1.5934280462578336, 1.593244395951226, 1.5927939110468057, 1.5922763354479654, 1.5923861992187813, 1.5937799822868017, 1.593136506158958, 1.593309427874289, 1.5917607974222798, 1.5931590757820395, 1.592298058758526, 1.5920494941225776, 1.5918397566621063, 1.5917971578711603, 1.5921541804405699, 1.5916435699443308, 1.5917222281011467, 1.591543809882914, 1.591333209611552, 1.5908510208129882, 1.5909288680039393, 1.5907972200695248, 1.5911695036310436, 1.5908856529474749, 1.5908128942062723, 1.5913329575830417, 1.5905865170382867, 1.5904617283133755, 1.5903628137811743, 1.59119293141414, 1.5902136930939599, 1.5911877015288116, 1.5899874279386454, 1.591287779954914, 1.58996093385763, 1.5906531461210467, 1.5903308085592376, 1.5902034356608772, 1.5897487437211022, 1.590225007695584, 1.5905794977407435, 1.5912456472796337, 1.5897799372428252, 1.5899755118563925, 1.5890643322981848, 1.5902570898772754, 1.5894649829218275, 1.5895401441585846, 1.5890682337465227, 1.5894310702043881, 1.5892723327789464, 1.5885635936529485, 1.589268398774478, 1.5885430606238895, 1.5887765258734232, 1.588229957302493, 1.5884965452080635, 1.5886154557155632, 1.5888392605086372, 1.587973923604836, 1.5886804312895946, 1.5884146683025164, 1.5883347617527297, 1.5884741400301579, 1.5890115744768962, 1.5880268118954293, 1.5877668261283233, 1.588060548026459, 1.5875512813885353, 1.5882265133045048, 1.588689945316902, 1.588033358615037, 1.5871056134206314, 1.587693405053454, 1.5873922636376758, 1.586973137581373, 1.586867091200435, 1.587772034864406, 1.5873780343077266, 1.587728782062413, 1.586842365186562, 1.586639450071284, 1.5869935682422083, 1.5863220351432628, 1.5864868753989374, 1.586811561408229, 1.5870668660443912, 1.587787400917351, 1.5861944326385091, 1.5869390969893282, 1.586213708070759, 1.5860356804771345, 1.5873274021324926, 1.5859714969227692, 1.586749500905219, 1.5891683733194026, 1.587145788370951, 1.586280224651282, 1.5859771873427122, 1.585784690140209, 1.5859696783813853, 1.5855277177489513, 1.585066458185106, 1.5852659306242236, 1.5850061979137162, 1.585090670203771, 1.5854697170688385, 1.5846847180223562, 1.5855249932659234, 1.5850434254327104, 1.5852996673916888, 1.5850037450907901, 1.5845572848584373, 1.584762049357749, 1.5842848424794003, 1.5843251231269915, 1.5842376205221094, 1.5844134470275786, 1.5843596099093709, 1.5844698005143623, 1.5835518430635425, 1.5838203519766336, 1.5842195152991607, 1.5833712095107875, 1.585002692132515, 1.5830776648844538, 1.5836058388990053, 1.5850757151903314, 1.584822002034902, 1.5843470932766641, 1.5831079036058586, 1.583639850361881, 1.5830610764834425, 1.5830711981109524, 1.583103695640329, 1.583026613298138, 1.58263533301422, 1.582597757364935, 1.5823406311520805, 1.5828408505637543, 1.5817984451748262, 1.582798942158599, 1.5841665324733978, 1.5808143599077418, 1.585146047740991, 1.5838556444865233, 1.5826030287164927, 1.582332865462411, 1.5819872010659877, 1.5828140752761026, 1.5823040405582842, 1.5838434767674128, 1.581590212639842, 1.5825196403742325, 1.5814758263084678, 1.581738009638855, 1.5813614839890655, 1.5814574904510372, 1.5810261371933705, 1.5812510603996763, 1.5823807463264075, 1.581260957120625, 1.5814144128157128, 1.5812484868008498, 1.580454987079456, 1.5809051403029988, 1.5808162058158577, 1.5819206514887252, 1.5809565242066275, 1.5808693289022426, 1.5819436795412882, 1.5812592465285158, 1.5811213327873903, 1.5819982961462753, 1.581914928708478, 1.5815364583561797, 1.5810634294329728, 1.5803741496691224, 1.5808216831767339, 1.5804999347584938, 1.580922972446105, 1.581079851479501, 1.580381855729669, 1.581637866051535, 1.579563450029988, 1.581158017818443], 'acc': [0.22464065811227724, 0.2402464058548518, 0.2316221768361587, 0.22997946599058547, 0.22997946698807592, 0.22915811075444584, 0.23285421049203225, 0.23244353287396244, 0.2340862421529249, 0.23408624274040393, 0.23285420872959514, 0.2328542091028891, 0.23778234190887004, 0.2406570834545629, 0.23983572782677057, 0.23983572900172867, 0.23860369614751922, 0.2369609866727304, 0.2398357299808604, 0.24065708484370604, 0.24188911709207775, 0.23696098446356442, 0.23039014400030797, 0.2373716646640942, 0.24188911728790408, 0.24147843849487619, 0.2381930173544913, 0.234907598386555, 0.2377823405380856, 0.2377823401280742, 0.2427104733073491, 0.23449692057265883, 0.2320328534934555, 0.23408624274040393, 0.2394250507961798, 0.24312114896715545, 0.2402464068156248, 0.24024640544484038, 0.24435318321050806, 0.2365503070780384, 0.2344969199851798, 0.22792607870190051, 0.22997946638223815, 0.23367556355572333, 0.23942505020870075, 0.23449691777601378, 0.2431211503562986, 0.2394250511878325, 0.24229979570763802, 0.24558521526305338, 0.24599589327277588, 0.24599589348696096, 0.24517453785916862, 0.24640657012589903, 0.24476386041856643, 0.2443531847587601, 0.24558521702549052, 0.24599589346860223, 0.2451745386241153, 0.24804928216478908, 0.24558521469393305, 0.24599589407444, 0.24928131521482488, 0.24599589346860223, 0.24640657149668346, 0.24640656971588762, 0.24763860296174975, 0.24599589250782922, 0.24640657226163015, 0.24681724890056822, 0.24599589127779498, 0.24558521526305338, 0.24804928253808306, 0.24887063718674365, 0.24599589425190763, 0.24024640644233083, 0.24640657186997744, 0.24763860474254562, 0.24928131482317217, 0.24435318240884393, 0.246817249506406, 0.24435318495458647, 0.24558521405137784, 0.2455852160647175, 0.24476386159352453, 0.24065708406040065, 0.24804928077564592, 0.23860369673499826, 0.21724845885128946, 0.23367556514069285, 0.2365503086630079, 0.23778234073391197, 0.2377823417130437, 0.2447638610244042, 0.24887063601178555, 0.2468172487231006, 0.24928131320148522, 0.24599589387861365, 0.24599589405608127, 0.2484599597828589, 0.25297741238831006, 0.24928131482317217, 0.24229979414102723, 0.2476386031575761, 0.2509240268437035, 0.24969199242288326, 0.2447638614160569, 0.2468172500755263, 0.2537987682119287, 0.24804928157731004, 0.24476386004527248, 0.24969199163957786, 0.2476386043508929, 0.24599589327277588, 0.25010267025513816, 0.24928131323820266, 0.2484599586079008, 0.25092402468961367, 0.2517453818840168, 0.24394250598409092, 0.24353182776018334, 0.25420944582999855, 0.24353182658522526, 0.25174537972992694, 0.24476385963526104, 0.24640657188833617, 0.25256673555354564, 0.253798767232797, 0.24722792632281168, 0.24722792749776978, 0.2525667329710857, 0.2546201230564157, 0.2542094456341722, 0.25379876840775506, 0.25462012423137376, 0.2579055432176688, 0.2546201228605893, 0.24928131362985537, 0.25297741395492085, 0.2501026690618213, 0.253388090808044, 0.2509240250812664, 0.25092402230298005, 0.25503080067448547, 0.25749486638290436, 0.24722792614534406, 0.2574948671662098, 0.252977411800831, 0.2562628339203476, 0.2517453789466216, 0.25954825564821155, 0.2570841877857028, 0.2574948663461869, 0.2533880911996967, 0.256262834703653, 0.2533880911996967, 0.2632443540150135, 0.2574948656179577, 0.25749486834116786, 0.24845995898119477, 0.2624229985830475, 0.26036961049269847, 0.2558521555189724, 0.2591375776017716, 0.25954825564821155, 0.26160164393438695, 0.25585215649810417, 0.26324435362336085, 0.25872690060789827, 0.2595482550790912, 0.25831622142321764, 0.2566735117342438, 0.26036961205930925, 0.2599589322871496, 0.26078028967737904, 0.2521560583271285, 0.2611909629505518, 0.2587268972421329, 0.25092402664787716, 0.26078028967737904, 0.25995893404958675, 0.2566735113425911, 0.2616016433469079, 0.2616016425636025, 0.25420944622165126, 0.25092402527709273, 0.2616016431510816, 0.2587269000204192, 0.2607802877191156, 0.2624229979955685, 0.26201232037749866, 0.2616016431694403, 0.26036960992357816, 0.2624229991705266, 0.2611909629505518, 0.26324435538579793, 0.2595482554523852, 0.2603696108476337, 0.26283367776772815, 0.2607802873458216, 0.2505133464840648, 0.25790554499846463, 0.26488706389981376, 0.25544148064247146, 0.2616016433469079, 0.26201231980837836, 0.2579055440193329, 0.2587268990596462, 0.2603696075185858, 0.2599589318954969, 0.25667351114676473, 0.25913757765684775, 0.2566735129092019, 0.25913757920509983, 0.25913757724683634, 0.26201232096497773, 0.26119096353803084, 0.26036960951356675, 0.2579055441968005, 0.26160164295525523, 0.262012320573325, 0.2603696099052194, 0.2611909657471968, 0.25585215610645146, 0.26201232233576216, 0.2624229976039158, 0.2599589323055083, 0.2550308000870064, 0.26160164354273424, 0.2611909647497064, 0.2537987689952341, 0.26324435417412245, 0.2611909649455327, 0.25462012383972105, 0.2607802883065946, 0.2616016415844708, 0.25503080067448547, 0.2607802885207797, 0.26283367659277007, 0.2624229983872212, 0.2607802877191156, 0.24681724733395743, 0.25667351091422097, 0.2603696114718302, 0.261601641351927, 0.26283367580946465, 0.2640657102302849, 0.26283367381448375, 0.2554414782925553, 0.2628336769844227, 0.2624229983872212, 0.2628336764153024, 0.2607802886982473, 0.26201232233576216, 0.2632443544066662, 0.26119096592466445, 0.25667351251754916, 0.2665297747637457, 0.2599589320546058, 0.25708418956649864, 0.26036961049269847, 0.2616016437385606, 0.2616016405686216, 0.2542094464174776, 0.2620123205916837, 0.25133470132855174, 0.2632443530358818, 0.26283367737607544, 0.26078028713163653, 0.2611909661204908, 0.2624229991705266, 0.26406571062193757, 0.2587268976337856, 0.2652977421237213, 0.26447638526589473, 0.26406570768454235, 0.25995893130801784, 0.25667351310502823, 0.26160164037279526, 0.2636550294422761, 0.2607802892857264, 0.26242299740808944, 0.2636550330038678, 0.2554414788800343, 0.2640657090553268, 0.263655031437257, 0.2521560569747028, 0.26488706210065915, 0.2595482564315169, 0.26036960992357816, 0.26119096514135903, 0.25749486640126307, 0.2501026684743423, 0.2640657082720214, 0.2644763862817439, 0.24845995819788938, 0.2648870645056515, 0.2583162220106967, 0.2628336740103101, 0.2587268990412875]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
