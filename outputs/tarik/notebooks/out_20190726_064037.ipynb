{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf29.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 06:40:37 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '1', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '01', '04', '05', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002C02166BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002C014FB6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6112, Accuracy:0.1967, Validation Loss:1.6068, Validation Accuracy:0.2217\n",
    "Epoch #2: Loss:1.6076, Accuracy:0.2271, Validation Loss:1.6064, Validation Accuracy:0.2315\n",
    "Epoch #3: Loss:1.6061, Accuracy:0.2320, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6062, Validation Accuracy:0.2299\n",
    "Epoch #8: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6042, Accuracy:0.2312, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6041, Accuracy:0.2320, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6030, Accuracy:0.2324, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6026, Accuracy:0.2333, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6023, Accuracy:0.2333, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6022, Accuracy:0.2357, Validation Loss:1.6036, Validation Accuracy:0.2397\n",
    "Epoch #24: Loss:1.6022, Accuracy:0.2357, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6022, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6021, Accuracy:0.2324, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6012, Accuracy:0.2345, Validation Loss:1.6043, Validation Accuracy:0.2365\n",
    "Epoch #28: Loss:1.6013, Accuracy:0.2316, Validation Loss:1.6039, Validation Accuracy:0.2282\n",
    "Epoch #29: Loss:1.6014, Accuracy:0.2378, Validation Loss:1.6036, Validation Accuracy:0.2496\n",
    "Epoch #30: Loss:1.6014, Accuracy:0.2448, Validation Loss:1.6034, Validation Accuracy:0.2463\n",
    "Epoch #31: Loss:1.6013, Accuracy:0.2435, Validation Loss:1.6032, Validation Accuracy:0.2463\n",
    "Epoch #32: Loss:1.6013, Accuracy:0.2435, Validation Loss:1.6032, Validation Accuracy:0.2479\n",
    "Epoch #33: Loss:1.6017, Accuracy:0.2402, Validation Loss:1.6029, Validation Accuracy:0.2414\n",
    "Epoch #34: Loss:1.6012, Accuracy:0.2419, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #35: Loss:1.6006, Accuracy:0.2456, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #36: Loss:1.6008, Accuracy:0.2427, Validation Loss:1.6033, Validation Accuracy:0.2479\n",
    "Epoch #37: Loss:1.6006, Accuracy:0.2411, Validation Loss:1.6041, Validation Accuracy:0.2463\n",
    "Epoch #38: Loss:1.6003, Accuracy:0.2419, Validation Loss:1.6040, Validation Accuracy:0.2414\n",
    "Epoch #39: Loss:1.6010, Accuracy:0.2370, Validation Loss:1.6040, Validation Accuracy:0.2447\n",
    "Epoch #40: Loss:1.6021, Accuracy:0.2337, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #41: Loss:1.6011, Accuracy:0.2345, Validation Loss:1.6041, Validation Accuracy:0.2414\n",
    "Epoch #42: Loss:1.6014, Accuracy:0.2411, Validation Loss:1.6043, Validation Accuracy:0.2463\n",
    "Epoch #43: Loss:1.6008, Accuracy:0.2452, Validation Loss:1.6035, Validation Accuracy:0.2414\n",
    "Epoch #44: Loss:1.6006, Accuracy:0.2390, Validation Loss:1.6035, Validation Accuracy:0.2414\n",
    "Epoch #45: Loss:1.6002, Accuracy:0.2407, Validation Loss:1.6029, Validation Accuracy:0.2447\n",
    "Epoch #46: Loss:1.6000, Accuracy:0.2456, Validation Loss:1.6028, Validation Accuracy:0.2447\n",
    "Epoch #47: Loss:1.5997, Accuracy:0.2464, Validation Loss:1.6026, Validation Accuracy:0.2463\n",
    "Epoch #48: Loss:1.5994, Accuracy:0.2460, Validation Loss:1.6027, Validation Accuracy:0.2479\n",
    "Epoch #49: Loss:1.5991, Accuracy:0.2452, Validation Loss:1.6029, Validation Accuracy:0.2430\n",
    "Epoch #50: Loss:1.5992, Accuracy:0.2444, Validation Loss:1.6030, Validation Accuracy:0.2365\n",
    "Epoch #51: Loss:1.5990, Accuracy:0.2431, Validation Loss:1.6028, Validation Accuracy:0.2381\n",
    "Epoch #52: Loss:1.5989, Accuracy:0.2448, Validation Loss:1.6023, Validation Accuracy:0.2430\n",
    "Epoch #53: Loss:1.5992, Accuracy:0.2444, Validation Loss:1.6021, Validation Accuracy:0.2430\n",
    "Epoch #54: Loss:1.5991, Accuracy:0.2431, Validation Loss:1.6021, Validation Accuracy:0.2397\n",
    "Epoch #55: Loss:1.5986, Accuracy:0.2431, Validation Loss:1.6033, Validation Accuracy:0.2430\n",
    "Epoch #56: Loss:1.5987, Accuracy:0.2435, Validation Loss:1.6034, Validation Accuracy:0.2447\n",
    "Epoch #57: Loss:1.5989, Accuracy:0.2423, Validation Loss:1.6039, Validation Accuracy:0.2381\n",
    "Epoch #58: Loss:1.5982, Accuracy:0.2431, Validation Loss:1.6030, Validation Accuracy:0.2463\n",
    "Epoch #59: Loss:1.5986, Accuracy:0.2435, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #60: Loss:1.5984, Accuracy:0.2456, Validation Loss:1.6027, Validation Accuracy:0.2463\n",
    "Epoch #61: Loss:1.5979, Accuracy:0.2435, Validation Loss:1.6031, Validation Accuracy:0.2447\n",
    "Epoch #62: Loss:1.5979, Accuracy:0.2435, Validation Loss:1.6020, Validation Accuracy:0.2463\n",
    "Epoch #63: Loss:1.5976, Accuracy:0.2468, Validation Loss:1.6028, Validation Accuracy:0.2414\n",
    "Epoch #64: Loss:1.5979, Accuracy:0.2452, Validation Loss:1.6037, Validation Accuracy:0.2414\n",
    "Epoch #65: Loss:1.5984, Accuracy:0.2464, Validation Loss:1.6031, Validation Accuracy:0.2414\n",
    "Epoch #66: Loss:1.5990, Accuracy:0.2460, Validation Loss:1.6033, Validation Accuracy:0.2430\n",
    "Epoch #67: Loss:1.5989, Accuracy:0.2468, Validation Loss:1.6022, Validation Accuracy:0.2332\n",
    "Epoch #68: Loss:1.5984, Accuracy:0.2472, Validation Loss:1.6026, Validation Accuracy:0.2381\n",
    "Epoch #69: Loss:1.5982, Accuracy:0.2464, Validation Loss:1.6025, Validation Accuracy:0.2348\n",
    "Epoch #70: Loss:1.5988, Accuracy:0.2448, Validation Loss:1.6029, Validation Accuracy:0.2348\n",
    "Epoch #71: Loss:1.5982, Accuracy:0.2427, Validation Loss:1.6041, Validation Accuracy:0.2414\n",
    "Epoch #72: Loss:1.5977, Accuracy:0.2468, Validation Loss:1.6031, Validation Accuracy:0.2348\n",
    "Epoch #73: Loss:1.5978, Accuracy:0.2493, Validation Loss:1.6013, Validation Accuracy:0.2348\n",
    "Epoch #74: Loss:1.5979, Accuracy:0.2464, Validation Loss:1.6019, Validation Accuracy:0.2381\n",
    "Epoch #75: Loss:1.5975, Accuracy:0.2472, Validation Loss:1.6010, Validation Accuracy:0.2430\n",
    "Epoch #76: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #77: Loss:1.5975, Accuracy:0.2476, Validation Loss:1.6013, Validation Accuracy:0.2365\n",
    "Epoch #78: Loss:1.5971, Accuracy:0.2452, Validation Loss:1.6003, Validation Accuracy:0.2496\n",
    "Epoch #79: Loss:1.5999, Accuracy:0.2411, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #80: Loss:1.6004, Accuracy:0.2427, Validation Loss:1.6025, Validation Accuracy:0.2562\n",
    "Epoch #81: Loss:1.6021, Accuracy:0.2415, Validation Loss:1.5996, Validation Accuracy:0.2414\n",
    "Epoch #82: Loss:1.5991, Accuracy:0.2480, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #83: Loss:1.5993, Accuracy:0.2439, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #84: Loss:1.5986, Accuracy:0.2448, Validation Loss:1.6009, Validation Accuracy:0.2496\n",
    "Epoch #85: Loss:1.5980, Accuracy:0.2448, Validation Loss:1.6005, Validation Accuracy:0.2447\n",
    "Epoch #86: Loss:1.5981, Accuracy:0.2452, Validation Loss:1.6011, Validation Accuracy:0.2447\n",
    "Epoch #87: Loss:1.5977, Accuracy:0.2460, Validation Loss:1.6013, Validation Accuracy:0.2496\n",
    "Epoch #88: Loss:1.5978, Accuracy:0.2431, Validation Loss:1.6001, Validation Accuracy:0.2463\n",
    "Epoch #89: Loss:1.5974, Accuracy:0.2456, Validation Loss:1.5979, Validation Accuracy:0.2463\n",
    "Epoch #90: Loss:1.5977, Accuracy:0.2468, Validation Loss:1.5988, Validation Accuracy:0.2447\n",
    "Epoch #91: Loss:1.5979, Accuracy:0.2468, Validation Loss:1.5997, Validation Accuracy:0.2447\n",
    "Epoch #92: Loss:1.5975, Accuracy:0.2460, Validation Loss:1.6005, Validation Accuracy:0.2414\n",
    "Epoch #93: Loss:1.5975, Accuracy:0.2431, Validation Loss:1.6008, Validation Accuracy:0.2463\n",
    "Epoch #94: Loss:1.5981, Accuracy:0.2452, Validation Loss:1.6016, Validation Accuracy:0.2414\n",
    "Epoch #95: Loss:1.5970, Accuracy:0.2444, Validation Loss:1.6008, Validation Accuracy:0.2463\n",
    "Epoch #96: Loss:1.5970, Accuracy:0.2452, Validation Loss:1.6006, Validation Accuracy:0.2463\n",
    "Epoch #97: Loss:1.6001, Accuracy:0.2275, Validation Loss:1.5999, Validation Accuracy:0.2430\n",
    "Epoch #98: Loss:1.6005, Accuracy:0.2382, Validation Loss:1.6013, Validation Accuracy:0.2463\n",
    "Epoch #99: Loss:1.5991, Accuracy:0.2460, Validation Loss:1.5994, Validation Accuracy:0.2463\n",
    "Epoch #100: Loss:1.5991, Accuracy:0.2423, Validation Loss:1.5993, Validation Accuracy:0.2512\n",
    "Epoch #101: Loss:1.5989, Accuracy:0.2439, Validation Loss:1.6003, Validation Accuracy:0.2463\n",
    "Epoch #102: Loss:1.5979, Accuracy:0.2427, Validation Loss:1.5974, Validation Accuracy:0.2529\n",
    "Epoch #103: Loss:1.5970, Accuracy:0.2468, Validation Loss:1.5983, Validation Accuracy:0.2512\n",
    "Epoch #104: Loss:1.5973, Accuracy:0.2448, Validation Loss:1.5986, Validation Accuracy:0.2479\n",
    "Epoch #105: Loss:1.5968, Accuracy:0.2468, Validation Loss:1.5981, Validation Accuracy:0.2479\n",
    "Epoch #106: Loss:1.5967, Accuracy:0.2456, Validation Loss:1.5985, Validation Accuracy:0.2545\n",
    "Epoch #107: Loss:1.5966, Accuracy:0.2460, Validation Loss:1.5993, Validation Accuracy:0.2496\n",
    "Epoch #108: Loss:1.5957, Accuracy:0.2534, Validation Loss:1.5989, Validation Accuracy:0.2496\n",
    "Epoch #109: Loss:1.5959, Accuracy:0.2497, Validation Loss:1.5986, Validation Accuracy:0.2479\n",
    "Epoch #110: Loss:1.5955, Accuracy:0.2448, Validation Loss:1.5992, Validation Accuracy:0.2479\n",
    "Epoch #111: Loss:1.5962, Accuracy:0.2427, Validation Loss:1.5995, Validation Accuracy:0.2594\n",
    "Epoch #112: Loss:1.5957, Accuracy:0.2493, Validation Loss:1.6001, Validation Accuracy:0.2447\n",
    "Epoch #113: Loss:1.5965, Accuracy:0.2505, Validation Loss:1.6009, Validation Accuracy:0.2512\n",
    "Epoch #114: Loss:1.5964, Accuracy:0.2522, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #115: Loss:1.5969, Accuracy:0.2480, Validation Loss:1.5992, Validation Accuracy:0.2479\n",
    "Epoch #116: Loss:1.5967, Accuracy:0.2431, Validation Loss:1.5998, Validation Accuracy:0.2545\n",
    "Epoch #117: Loss:1.5971, Accuracy:0.2439, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #118: Loss:1.5963, Accuracy:0.2448, Validation Loss:1.6000, Validation Accuracy:0.2348\n",
    "Epoch #119: Loss:1.5962, Accuracy:0.2509, Validation Loss:1.6000, Validation Accuracy:0.2479\n",
    "Epoch #120: Loss:1.5961, Accuracy:0.2485, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #121: Loss:1.5960, Accuracy:0.2501, Validation Loss:1.5999, Validation Accuracy:0.2430\n",
    "Epoch #122: Loss:1.5957, Accuracy:0.2480, Validation Loss:1.6002, Validation Accuracy:0.2479\n",
    "Epoch #123: Loss:1.5965, Accuracy:0.2472, Validation Loss:1.6003, Validation Accuracy:0.2479\n",
    "Epoch #124: Loss:1.5955, Accuracy:0.2517, Validation Loss:1.6021, Validation Accuracy:0.2414\n",
    "Epoch #125: Loss:1.5950, Accuracy:0.2485, Validation Loss:1.6002, Validation Accuracy:0.2496\n",
    "Epoch #126: Loss:1.5952, Accuracy:0.2493, Validation Loss:1.6035, Validation Accuracy:0.2512\n",
    "Epoch #127: Loss:1.5939, Accuracy:0.2464, Validation Loss:1.6019, Validation Accuracy:0.2529\n",
    "Epoch #128: Loss:1.5929, Accuracy:0.2559, Validation Loss:1.6055, Validation Accuracy:0.2463\n",
    "Epoch #129: Loss:1.5960, Accuracy:0.2456, Validation Loss:1.6012, Validation Accuracy:0.2430\n",
    "Epoch #130: Loss:1.5962, Accuracy:0.2493, Validation Loss:1.5982, Validation Accuracy:0.2299\n",
    "Epoch #131: Loss:1.5962, Accuracy:0.2431, Validation Loss:1.5991, Validation Accuracy:0.2332\n",
    "Epoch #132: Loss:1.5960, Accuracy:0.2390, Validation Loss:1.5984, Validation Accuracy:0.2463\n",
    "Epoch #133: Loss:1.5962, Accuracy:0.2485, Validation Loss:1.5982, Validation Accuracy:0.2430\n",
    "Epoch #134: Loss:1.5959, Accuracy:0.2485, Validation Loss:1.5983, Validation Accuracy:0.2463\n",
    "Epoch #135: Loss:1.5967, Accuracy:0.2472, Validation Loss:1.5986, Validation Accuracy:0.2463\n",
    "Epoch #136: Loss:1.5956, Accuracy:0.2513, Validation Loss:1.5965, Validation Accuracy:0.2447\n",
    "Epoch #137: Loss:1.5954, Accuracy:0.2456, Validation Loss:1.5975, Validation Accuracy:0.2447\n",
    "Epoch #138: Loss:1.5955, Accuracy:0.2480, Validation Loss:1.5982, Validation Accuracy:0.2479\n",
    "Epoch #139: Loss:1.5967, Accuracy:0.2394, Validation Loss:1.5983, Validation Accuracy:0.2365\n",
    "Epoch #140: Loss:1.5973, Accuracy:0.2423, Validation Loss:1.5996, Validation Accuracy:0.2512\n",
    "Epoch #141: Loss:1.5962, Accuracy:0.2468, Validation Loss:1.6004, Validation Accuracy:0.2463\n",
    "Epoch #142: Loss:1.5957, Accuracy:0.2464, Validation Loss:1.5983, Validation Accuracy:0.2430\n",
    "Epoch #143: Loss:1.5966, Accuracy:0.2460, Validation Loss:1.5988, Validation Accuracy:0.2397\n",
    "Epoch #144: Loss:1.5959, Accuracy:0.2452, Validation Loss:1.5978, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5984, Accuracy:0.2300, Validation Loss:1.6019, Validation Accuracy:0.2414\n",
    "Epoch #146: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.5970, Validation Accuracy:0.2348\n",
    "Epoch #147: Loss:1.5983, Accuracy:0.2402, Validation Loss:1.6012, Validation Accuracy:0.2463\n",
    "Epoch #148: Loss:1.5961, Accuracy:0.2476, Validation Loss:1.5993, Validation Accuracy:0.2397\n",
    "Epoch #149: Loss:1.5971, Accuracy:0.2448, Validation Loss:1.6003, Validation Accuracy:0.2397\n",
    "Epoch #150: Loss:1.5962, Accuracy:0.2460, Validation Loss:1.5980, Validation Accuracy:0.2397\n",
    "Epoch #151: Loss:1.5955, Accuracy:0.2407, Validation Loss:1.5990, Validation Accuracy:0.2479\n",
    "Epoch #152: Loss:1.5958, Accuracy:0.2394, Validation Loss:1.5976, Validation Accuracy:0.2463\n",
    "Epoch #153: Loss:1.5950, Accuracy:0.2513, Validation Loss:1.5974, Validation Accuracy:0.2414\n",
    "Epoch #154: Loss:1.5948, Accuracy:0.2476, Validation Loss:1.5971, Validation Accuracy:0.2397\n",
    "Epoch #155: Loss:1.5948, Accuracy:0.2398, Validation Loss:1.5984, Validation Accuracy:0.2463\n",
    "Epoch #156: Loss:1.5944, Accuracy:0.2419, Validation Loss:1.5967, Validation Accuracy:0.2414\n",
    "Epoch #157: Loss:1.5937, Accuracy:0.2468, Validation Loss:1.5966, Validation Accuracy:0.2414\n",
    "Epoch #158: Loss:1.5941, Accuracy:0.2493, Validation Loss:1.5975, Validation Accuracy:0.2430\n",
    "Epoch #159: Loss:1.5942, Accuracy:0.2497, Validation Loss:1.5970, Validation Accuracy:0.2496\n",
    "Epoch #160: Loss:1.5939, Accuracy:0.2439, Validation Loss:1.5968, Validation Accuracy:0.2430\n",
    "Epoch #161: Loss:1.5941, Accuracy:0.2472, Validation Loss:1.5970, Validation Accuracy:0.2397\n",
    "Epoch #162: Loss:1.5931, Accuracy:0.2493, Validation Loss:1.5982, Validation Accuracy:0.2479\n",
    "Epoch #163: Loss:1.5940, Accuracy:0.2534, Validation Loss:1.5972, Validation Accuracy:0.2397\n",
    "Epoch #164: Loss:1.5936, Accuracy:0.2530, Validation Loss:1.5959, Validation Accuracy:0.2397\n",
    "Epoch #165: Loss:1.5937, Accuracy:0.2448, Validation Loss:1.5957, Validation Accuracy:0.2463\n",
    "Epoch #166: Loss:1.5933, Accuracy:0.2460, Validation Loss:1.5958, Validation Accuracy:0.2496\n",
    "Epoch #167: Loss:1.5921, Accuracy:0.2534, Validation Loss:1.5978, Validation Accuracy:0.2447\n",
    "Epoch #168: Loss:1.5956, Accuracy:0.2501, Validation Loss:1.5977, Validation Accuracy:0.2479\n",
    "Epoch #169: Loss:1.5966, Accuracy:0.2505, Validation Loss:1.5999, Validation Accuracy:0.2463\n",
    "Epoch #170: Loss:1.5969, Accuracy:0.2456, Validation Loss:1.6003, Validation Accuracy:0.2463\n",
    "Epoch #171: Loss:1.5973, Accuracy:0.2407, Validation Loss:1.6004, Validation Accuracy:0.2447\n",
    "Epoch #172: Loss:1.5962, Accuracy:0.2497, Validation Loss:1.6010, Validation Accuracy:0.2447\n",
    "Epoch #173: Loss:1.5975, Accuracy:0.2489, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #174: Loss:1.5967, Accuracy:0.2493, Validation Loss:1.6012, Validation Accuracy:0.2430\n",
    "Epoch #175: Loss:1.5967, Accuracy:0.2489, Validation Loss:1.6009, Validation Accuracy:0.2430\n",
    "Epoch #176: Loss:1.5962, Accuracy:0.2493, Validation Loss:1.6000, Validation Accuracy:0.2430\n",
    "Epoch #177: Loss:1.5962, Accuracy:0.2501, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #178: Loss:1.5964, Accuracy:0.2480, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #179: Loss:1.5958, Accuracy:0.2513, Validation Loss:1.6002, Validation Accuracy:0.2397\n",
    "Epoch #180: Loss:1.5962, Accuracy:0.2485, Validation Loss:1.6001, Validation Accuracy:0.2414\n",
    "Epoch #181: Loss:1.5953, Accuracy:0.2411, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #182: Loss:1.5954, Accuracy:0.2485, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #183: Loss:1.5953, Accuracy:0.2489, Validation Loss:1.6005, Validation Accuracy:0.2414\n",
    "Epoch #184: Loss:1.5952, Accuracy:0.2480, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #185: Loss:1.5950, Accuracy:0.2517, Validation Loss:1.6011, Validation Accuracy:0.2282\n",
    "Epoch #186: Loss:1.5960, Accuracy:0.2509, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #187: Loss:1.5954, Accuracy:0.2493, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #188: Loss:1.5959, Accuracy:0.2456, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #189: Loss:1.5955, Accuracy:0.2448, Validation Loss:1.6011, Validation Accuracy:0.2397\n",
    "Epoch #190: Loss:1.5955, Accuracy:0.2501, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #191: Loss:1.5957, Accuracy:0.2489, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #192: Loss:1.5956, Accuracy:0.2509, Validation Loss:1.6015, Validation Accuracy:0.2414\n",
    "Epoch #193: Loss:1.5961, Accuracy:0.2493, Validation Loss:1.6013, Validation Accuracy:0.2430\n",
    "Epoch #194: Loss:1.5954, Accuracy:0.2505, Validation Loss:1.6015, Validation Accuracy:0.2414\n",
    "Epoch #195: Loss:1.5958, Accuracy:0.2493, Validation Loss:1.6015, Validation Accuracy:0.2414\n",
    "Epoch #196: Loss:1.5954, Accuracy:0.2509, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #197: Loss:1.5955, Accuracy:0.2489, Validation Loss:1.6015, Validation Accuracy:0.2348\n",
    "Epoch #198: Loss:1.5952, Accuracy:0.2444, Validation Loss:1.6011, Validation Accuracy:0.2397\n",
    "Epoch #199: Loss:1.5954, Accuracy:0.2435, Validation Loss:1.6016, Validation Accuracy:0.2381\n",
    "Epoch #200: Loss:1.5949, Accuracy:0.2501, Validation Loss:1.6009, Validation Accuracy:0.2348\n",
    "Epoch #201: Loss:1.5951, Accuracy:0.2505, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #202: Loss:1.5952, Accuracy:0.2513, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #203: Loss:1.5948, Accuracy:0.2542, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #204: Loss:1.5948, Accuracy:0.2522, Validation Loss:1.6010, Validation Accuracy:0.2348\n",
    "Epoch #205: Loss:1.5946, Accuracy:0.2534, Validation Loss:1.6015, Validation Accuracy:0.2332\n",
    "Epoch #206: Loss:1.5949, Accuracy:0.2513, Validation Loss:1.6010, Validation Accuracy:0.2430\n",
    "Epoch #207: Loss:1.5945, Accuracy:0.2530, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #208: Loss:1.5947, Accuracy:0.2530, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #209: Loss:1.5945, Accuracy:0.2530, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #210: Loss:1.5947, Accuracy:0.2517, Validation Loss:1.6012, Validation Accuracy:0.2430\n",
    "Epoch #211: Loss:1.5950, Accuracy:0.2456, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #212: Loss:1.5947, Accuracy:0.2493, Validation Loss:1.6009, Validation Accuracy:0.2447\n",
    "Epoch #213: Loss:1.5945, Accuracy:0.2526, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #214: Loss:1.5944, Accuracy:0.2517, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #215: Loss:1.5944, Accuracy:0.2522, Validation Loss:1.6010, Validation Accuracy:0.2430\n",
    "Epoch #216: Loss:1.5943, Accuracy:0.2444, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #217: Loss:1.5948, Accuracy:0.2563, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #218: Loss:1.5943, Accuracy:0.2517, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #219: Loss:1.5943, Accuracy:0.2509, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #220: Loss:1.5944, Accuracy:0.2526, Validation Loss:1.6009, Validation Accuracy:0.2414\n",
    "Epoch #221: Loss:1.5940, Accuracy:0.2522, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #222: Loss:1.5939, Accuracy:0.2509, Validation Loss:1.6009, Validation Accuracy:0.2430\n",
    "Epoch #223: Loss:1.5940, Accuracy:0.2526, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #224: Loss:1.5941, Accuracy:0.2522, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #225: Loss:1.5944, Accuracy:0.2526, Validation Loss:1.6014, Validation Accuracy:0.2414\n",
    "Epoch #226: Loss:1.5940, Accuracy:0.2517, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #227: Loss:1.5940, Accuracy:0.2530, Validation Loss:1.6008, Validation Accuracy:0.2447\n",
    "Epoch #228: Loss:1.5940, Accuracy:0.2509, Validation Loss:1.6014, Validation Accuracy:0.2414\n",
    "Epoch #229: Loss:1.5941, Accuracy:0.2513, Validation Loss:1.6014, Validation Accuracy:0.2414\n",
    "Epoch #230: Loss:1.5940, Accuracy:0.2522, Validation Loss:1.6006, Validation Accuracy:0.2447\n",
    "Epoch #231: Loss:1.5943, Accuracy:0.2550, Validation Loss:1.6005, Validation Accuracy:0.2447\n",
    "Epoch #232: Loss:1.5940, Accuracy:0.2538, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #233: Loss:1.5939, Accuracy:0.2522, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #234: Loss:1.5938, Accuracy:0.2513, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #235: Loss:1.5938, Accuracy:0.2493, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #236: Loss:1.5936, Accuracy:0.2530, Validation Loss:1.6007, Validation Accuracy:0.2414\n",
    "Epoch #237: Loss:1.5937, Accuracy:0.2513, Validation Loss:1.6006, Validation Accuracy:0.2414\n",
    "Epoch #238: Loss:1.5936, Accuracy:0.2538, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #239: Loss:1.5938, Accuracy:0.2534, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #240: Loss:1.5936, Accuracy:0.2513, Validation Loss:1.6020, Validation Accuracy:0.2397\n",
    "Epoch #241: Loss:1.5937, Accuracy:0.2497, Validation Loss:1.6011, Validation Accuracy:0.2430\n",
    "Epoch #242: Loss:1.5939, Accuracy:0.2509, Validation Loss:1.6009, Validation Accuracy:0.2447\n",
    "Epoch #243: Loss:1.5933, Accuracy:0.2542, Validation Loss:1.6010, Validation Accuracy:0.2397\n",
    "Epoch #244: Loss:1.5934, Accuracy:0.2505, Validation Loss:1.6009, Validation Accuracy:0.2414\n",
    "Epoch #245: Loss:1.5939, Accuracy:0.2513, Validation Loss:1.6011, Validation Accuracy:0.2447\n",
    "Epoch #246: Loss:1.5932, Accuracy:0.2522, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #247: Loss:1.5933, Accuracy:0.2517, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #248: Loss:1.5933, Accuracy:0.2554, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #249: Loss:1.5932, Accuracy:0.2517, Validation Loss:1.6005, Validation Accuracy:0.2414\n",
    "Epoch #250: Loss:1.5944, Accuracy:0.2460, Validation Loss:1.6006, Validation Accuracy:0.2365\n",
    "Epoch #251: Loss:1.5933, Accuracy:0.2489, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #252: Loss:1.5933, Accuracy:0.2517, Validation Loss:1.6002, Validation Accuracy:0.2447\n",
    "Epoch #253: Loss:1.5941, Accuracy:0.2472, Validation Loss:1.6015, Validation Accuracy:0.2447\n",
    "Epoch #254: Loss:1.5937, Accuracy:0.2480, Validation Loss:1.6012, Validation Accuracy:0.2430\n",
    "Epoch #255: Loss:1.5934, Accuracy:0.2501, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #256: Loss:1.5927, Accuracy:0.2526, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #257: Loss:1.5930, Accuracy:0.2542, Validation Loss:1.6002, Validation Accuracy:0.2381\n",
    "Epoch #258: Loss:1.5927, Accuracy:0.2559, Validation Loss:1.6003, Validation Accuracy:0.2447\n",
    "Epoch #259: Loss:1.5927, Accuracy:0.2595, Validation Loss:1.6003, Validation Accuracy:0.2397\n",
    "Epoch #260: Loss:1.5925, Accuracy:0.2522, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #261: Loss:1.5928, Accuracy:0.2534, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #262: Loss:1.5930, Accuracy:0.2534, Validation Loss:1.6006, Validation Accuracy:0.2447\n",
    "Epoch #263: Loss:1.5925, Accuracy:0.2517, Validation Loss:1.6004, Validation Accuracy:0.2365\n",
    "Epoch #264: Loss:1.5926, Accuracy:0.2513, Validation Loss:1.6006, Validation Accuracy:0.2365\n",
    "Epoch #265: Loss:1.5934, Accuracy:0.2509, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #266: Loss:1.5926, Accuracy:0.2497, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #267: Loss:1.5924, Accuracy:0.2546, Validation Loss:1.6005, Validation Accuracy:0.2397\n",
    "Epoch #268: Loss:1.5922, Accuracy:0.2530, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #269: Loss:1.5922, Accuracy:0.2505, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #270: Loss:1.5920, Accuracy:0.2530, Validation Loss:1.6008, Validation Accuracy:0.2299\n",
    "Epoch #271: Loss:1.5929, Accuracy:0.2452, Validation Loss:1.6007, Validation Accuracy:0.2414\n",
    "Epoch #272: Loss:1.5922, Accuracy:0.2567, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #273: Loss:1.5922, Accuracy:0.2526, Validation Loss:1.6001, Validation Accuracy:0.2414\n",
    "Epoch #274: Loss:1.5921, Accuracy:0.2513, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #275: Loss:1.5925, Accuracy:0.2513, Validation Loss:1.6003, Validation Accuracy:0.2397\n",
    "Epoch #276: Loss:1.5921, Accuracy:0.2513, Validation Loss:1.6008, Validation Accuracy:0.2447\n",
    "Epoch #277: Loss:1.5917, Accuracy:0.2542, Validation Loss:1.6000, Validation Accuracy:0.2381\n",
    "Epoch #278: Loss:1.5922, Accuracy:0.2550, Validation Loss:1.6000, Validation Accuracy:0.2365\n",
    "Epoch #279: Loss:1.5919, Accuracy:0.2460, Validation Loss:1.6004, Validation Accuracy:0.2463\n",
    "Epoch #280: Loss:1.5919, Accuracy:0.2513, Validation Loss:1.6002, Validation Accuracy:0.2430\n",
    "Epoch #281: Loss:1.5916, Accuracy:0.2575, Validation Loss:1.6001, Validation Accuracy:0.2496\n",
    "Epoch #282: Loss:1.5918, Accuracy:0.2563, Validation Loss:1.6001, Validation Accuracy:0.2381\n",
    "Epoch #283: Loss:1.5930, Accuracy:0.2464, Validation Loss:1.6005, Validation Accuracy:0.2430\n",
    "Epoch #284: Loss:1.5915, Accuracy:0.2497, Validation Loss:1.6004, Validation Accuracy:0.2348\n",
    "Epoch #285: Loss:1.5921, Accuracy:0.2526, Validation Loss:1.5999, Validation Accuracy:0.2365\n",
    "Epoch #286: Loss:1.5917, Accuracy:0.2509, Validation Loss:1.5997, Validation Accuracy:0.2365\n",
    "Epoch #287: Loss:1.5913, Accuracy:0.2546, Validation Loss:1.6000, Validation Accuracy:0.2512\n",
    "Epoch #288: Loss:1.5915, Accuracy:0.2546, Validation Loss:1.6002, Validation Accuracy:0.2414\n",
    "Epoch #289: Loss:1.5913, Accuracy:0.2546, Validation Loss:1.5999, Validation Accuracy:0.2365\n",
    "Epoch #290: Loss:1.5915, Accuracy:0.2534, Validation Loss:1.6002, Validation Accuracy:0.2365\n",
    "Epoch #291: Loss:1.5918, Accuracy:0.2542, Validation Loss:1.6003, Validation Accuracy:0.2397\n",
    "Epoch #292: Loss:1.5910, Accuracy:0.2546, Validation Loss:1.5999, Validation Accuracy:0.2496\n",
    "Epoch #293: Loss:1.5918, Accuracy:0.2509, Validation Loss:1.5999, Validation Accuracy:0.2348\n",
    "Epoch #294: Loss:1.5922, Accuracy:0.2534, Validation Loss:1.5997, Validation Accuracy:0.2348\n",
    "Epoch #295: Loss:1.5906, Accuracy:0.2583, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #296: Loss:1.5919, Accuracy:0.2513, Validation Loss:1.5999, Validation Accuracy:0.2348\n",
    "Epoch #297: Loss:1.5913, Accuracy:0.2480, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #298: Loss:1.5912, Accuracy:0.2489, Validation Loss:1.5991, Validation Accuracy:0.2365\n",
    "Epoch #299: Loss:1.5925, Accuracy:0.2546, Validation Loss:1.5999, Validation Accuracy:0.2315\n",
    "Epoch #300: Loss:1.5910, Accuracy:0.2530, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60035229, Accuracy:0.2414\n",
    "Labels: ['03', '01', '04', '05', '02']\n",
    "Confusion Matrix:\n",
    "      03  01  04   05  02\n",
    "t:03   0   6  13   95   1\n",
    "t:01   0   6  16  104   0\n",
    "t:04   0   7  17   87   1\n",
    "t:05   0   4  14  123   1\n",
    "t:02   0   4  12   97   1\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       115\n",
    "          01       0.22      0.05      0.08       126\n",
    "          04       0.24      0.15      0.18       112\n",
    "          05       0.24      0.87      0.38       142\n",
    "          02       0.25      0.01      0.02       114\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.19      0.21      0.13       609\n",
    "weighted avg       0.19      0.24      0.14       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 06:56:24 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 46 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6067948110389396, 1.606389368304674, 1.6064049322616878, 1.6060745420518572, 1.6061634684626889, 1.6060457482126547, 1.6062443411017482, 1.6056102833332881, 1.6053697968938667, 1.6056290033024128, 1.6058059660671966, 1.605627080881341, 1.6055490320734986, 1.6056316049619652, 1.6052169118608748, 1.6050727220591654, 1.604715240216999, 1.6045712790465707, 1.6042804434185935, 1.6042339206720613, 1.6043171050709064, 1.6039494620559642, 1.6035872130166917, 1.604649251121997, 1.6049992613408757, 1.6044290478789356, 1.6042566158501386, 1.603879525547936, 1.6036208316023126, 1.6034229371348039, 1.603200907581937, 1.6031834942170944, 1.6029046773910522, 1.602618119399536, 1.602569229301365, 1.6033340160091132, 1.6041115763152174, 1.6039530659348311, 1.604022063645236, 1.6037277685989104, 1.6040641877843045, 1.60432772761691, 1.6034513251926317, 1.6034968177281772, 1.6029383676196947, 1.6028117444519143, 1.6025577378390459, 1.6026861319205248, 1.6029018431852995, 1.6030305455666654, 1.6027731323868575, 1.602303942240322, 1.6021219977408598, 1.6021095028847505, 1.6032883750981297, 1.6034475185209502, 1.6038532141589963, 1.6029877985639525, 1.602607889128436, 1.6027140909032085, 1.6031256569625905, 1.6020481065772045, 1.602751533973393, 1.6036747762526589, 1.603122329477019, 1.6032683964824832, 1.602208347351876, 1.6026408713439415, 1.6024716362381608, 1.602878997870071, 1.6041022781863785, 1.6031062716529483, 1.6013497509588357, 1.6018618950115635, 1.6010363659835214, 1.6012749969469895, 1.6012967028249856, 1.600301586544181, 1.6037880072648498, 1.6025447056602766, 1.5995879437535854, 1.6002506039216993, 1.5995888886193337, 1.6008998049891054, 1.60051467305138, 1.601086149857745, 1.6013180099684616, 1.6001404054059183, 1.5978878587729042, 1.5988000887759606, 1.5996662791335132, 1.600497622403801, 1.6008137883419669, 1.601607518438831, 1.600803899843313, 1.6006122884296237, 1.5999061104112071, 1.6013251198532155, 1.5994272513929846, 1.5993485738491189, 1.6002871228752074, 1.597436868302732, 1.5982793585224495, 1.598648775778772, 1.598139815925573, 1.598479040150572, 1.5993204927209563, 1.5988719097303443, 1.598610281357037, 1.5991690064885933, 1.5994903097794757, 1.6001032185671953, 1.6008698188612613, 1.5996389136525797, 1.5992474779119632, 1.5997967925564995, 1.5992157801814464, 1.5999556404029207, 1.599960549124356, 1.599764762057851, 1.5998775105562508, 1.6002491732144786, 1.6002662085938728, 1.6021130730953124, 1.6001827370357045, 1.6034849398633333, 1.6019368962505571, 1.6055261842135726, 1.6011813509053197, 1.5981793799032327, 1.5990928358632355, 1.5984357092376609, 1.598191450969339, 1.5983419191269648, 1.5986029502793486, 1.5964759584326658, 1.5975318078337044, 1.5982484705929685, 1.5983197164457223, 1.5995988550248796, 1.6004282905550427, 1.598266870321703, 1.5987519066909264, 1.5978410786204347, 1.6019299556860587, 1.5969796830601684, 1.6012231436464783, 1.5992717977815074, 1.6003130619553314, 1.5980361144139457, 1.599006195765215, 1.5976102262099192, 1.5974463527817249, 1.5971072831960342, 1.598381572560528, 1.5966868721597105, 1.5966449192983567, 1.5975274732351694, 1.5970324003833465, 1.59682455751892, 1.596959177300652, 1.598236182248847, 1.5971529810690919, 1.5958963021856223, 1.595709366751422, 1.5958184364002521, 1.5977517431005468, 1.5977214856688025, 1.5998553366496646, 1.6002890543005932, 1.6004117519788945, 1.6010289432967237, 1.6007130607987077, 1.601167960315698, 1.6009014827277273, 1.6000006733269527, 1.6007864761039345, 1.6011370573137782, 1.6002445990228888, 1.6000831448190123, 1.6007853452795244, 1.6007463350671853, 1.6004516103584778, 1.6011626626470405, 1.601104259295221, 1.599924707060377, 1.6006641544536222, 1.6004939081242127, 1.6010529675898686, 1.600825362800573, 1.6013729816978592, 1.601482097738482, 1.6013343755051812, 1.6015119636783068, 1.6014565127627995, 1.6012937065415782, 1.6014581670119061, 1.6010572730222554, 1.6016372138839245, 1.6008561070525196, 1.6007008168889187, 1.6010255911471614, 1.600699431203269, 1.6010084218775307, 1.6015332304981151, 1.6010051830648788, 1.6006636363140663, 1.6009642173504006, 1.6013022420441576, 1.6011983451780623, 1.6010563749397917, 1.600869888351077, 1.6011641841803865, 1.601159208318087, 1.6009915804823827, 1.6008288805512176, 1.6006353216609737, 1.6013007782558697, 1.6012385533556759, 1.6009134453505718, 1.6007662627888821, 1.600910874814627, 1.6011625663400284, 1.6012892926659295, 1.6014023524004055, 1.600722362255228, 1.600819921845873, 1.6014086532671072, 1.6013934719934448, 1.6006225222241506, 1.6004940811636412, 1.6011615946570836, 1.601138384862878, 1.6006982060293062, 1.6009554990210948, 1.6007425518850191, 1.6005835079011463, 1.6007564723589542, 1.6010113369263648, 1.6020144701787011, 1.6010685853770215, 1.6009357944497922, 1.6009814320330942, 1.600899748418523, 1.6010965490576081, 1.600715048011692, 1.6002662863050188, 1.6008451096530032, 1.600535644099043, 1.6006203095118205, 1.6008135256508889, 1.6002144631493855, 1.60153293374724, 1.601238357609716, 1.6009696538029436, 1.6007767111209814, 1.6002235259915807, 1.6002533461268509, 1.600335199844661, 1.6006473392884328, 1.600689684228944, 1.6005800792149134, 1.600380954875539, 1.600571949102217, 1.6005557025790411, 1.600532269830187, 1.6005495115258228, 1.60080579583868, 1.60045631961478, 1.6008365344139939, 1.600733864679321, 1.6002869439634, 1.600138086795024, 1.6004075166235612, 1.6002517034267556, 1.6007841439865689, 1.599976507705225, 1.5999985819771176, 1.6003904467928782, 1.6002151688135708, 1.6001480712091982, 1.6000552287046936, 1.6005403414148416, 1.6003880062322506, 1.5998522362294063, 1.5996981657588816, 1.600021779047836, 1.6002333855198325, 1.5999261714359028, 1.6001520047242614, 1.6003437308450834, 1.599913999364881, 1.5999410183754657, 1.5997037916935135, 1.5997658128221635, 1.5998671865228362, 1.6006904268891158, 1.5990978203383572, 1.599883224185073, 1.6003522716328036], 'val_acc': [0.22167487623558452, 0.23152709327797194, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.2298850571531772, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23973727399981865, 0.23316912940276668, 0.23316912940276668, 0.2331691293048937, 0.23645320155448318, 0.22824302093050947, 0.2495894907485871, 0.24630541849899762, 0.24630541849899762, 0.24794745462379236, 0.2413793100267404, 0.24466338237420288, 0.24466338237420288, 0.24794745462379236, 0.24630541849899762, 0.2413793100267404, 0.24466338247207586, 0.23316912940276668, 0.24137931022248635, 0.24630541849899762, 0.2413793101246134, 0.24137931022248635, 0.24466338237420288, 0.24466338237420288, 0.24630541849899762, 0.24794745462379236, 0.24302134624940813, 0.23645320135873724, 0.23809523748353198, 0.24302134615153514, 0.24302134615153514, 0.23973727380407267, 0.24302134624940813, 0.24466338237420288, 0.23809523758140494, 0.24630541849899762, 0.24466338237420288, 0.24630541849899762, 0.24466338237420288, 0.24630541849899762, 0.2413793100267404, 0.2413793099288674, 0.2413793099288674, 0.24302134605366216, 0.2331691293048937, 0.23809523767927793, 0.2348111656254344, 0.2348111656254344, 0.2413793101246134, 0.23481116552756143, 0.23481116552756143, 0.23809523777715091, 0.24302134624940813, 0.2413793099288674, 0.23645320165235617, 0.2495894907485871, 0.23152709318009895, 0.2561576346605282, 0.2413793100267404, 0.24302134624940813, 0.24794745452591938, 0.24958949065071412, 0.2446633822763299, 0.2446633822763299, 0.24958949065071412, 0.24630541840112463, 0.24630541849899762, 0.24466338237420288, 0.2446633822763299, 0.24137930983099445, 0.24630541849899762, 0.24137930973312147, 0.24630541849899762, 0.24630541840112463, 0.2430213459557892, 0.24630541840112463, 0.24630541849899762, 0.25123152687338185, 0.24630541849899762, 0.2528735628024306, 0.25123152677550886, 0.24794745462379236, 0.24794745462379236, 0.2545155992208443, 0.24958949055284116, 0.24958949065071412, 0.24794745462379236, 0.2479474548195383, 0.25944170769310154, 0.24466338247207586, 0.25123152687338185, 0.24794745462379236, 0.24794745462379236, 0.2545155992208443, 0.24958949094433308, 0.23481116552756143, 0.24794745462379236, 0.24794745462379236, 0.24302134605366216, 0.24794745472166535, 0.24794745462379236, 0.24137930983099445, 0.2495894908464601, 0.2512315270691278, 0.2528735629003036, 0.2463054181075057, 0.24302134624940813, 0.2298850571531772, 0.2331691292070207, 0.24630541849899762, 0.24302134624940813, 0.24630541849899762, 0.24630541840112463, 0.2446633822763299, 0.2446633822763299, 0.24794745452591938, 0.23645320135873724, 0.25123152677550886, 0.24630541840112463, 0.24302134615153514, 0.23973727380407267, 0.23973727380407267, 0.24137931022248635, 0.2348111651360695, 0.2463054180096327, 0.23973727390194566, 0.23973727390194566, 0.23973727390194566, 0.24794745433017343, 0.24630541840112463, 0.2413793101246134, 0.23973727399981865, 0.2463054181075057, 0.24137930983099445, 0.2413793101246134, 0.24302134624940813, 0.24958949035709518, 0.24302134615153514, 0.23973727360832672, 0.2479474540365545, 0.23973727370619968, 0.23973727390194566, 0.2463054181075057, 0.24958949065071412, 0.2446633822763299, 0.24794745462379236, 0.24630541849899762, 0.2463054185968706, 0.2446633821784569, 0.2446633821784569, 0.2446633822763299, 0.24302134605366216, 0.24302134605366216, 0.24302134605366216, 0.23973727380407267, 0.2413793099288674, 0.23973727390194566, 0.2413793100267404, 0.2413793100267404, 0.24466338208058394, 0.2413793100267404, 0.2413793100267404, 0.22824302044114456, 0.2413793100267404, 0.24302134615153514, 0.23809523758140494, 0.23973727380407267, 0.23973727380407267, 0.23973727380407267, 0.2413793099288674, 0.24302134615153514, 0.2413793099288674, 0.2413793099288674, 0.2413793099288674, 0.2348111656254344, 0.23973727390194566, 0.23809523777715091, 0.23481116552756143, 0.24466338237420288, 0.2331691293048937, 0.23481116552756143, 0.23481116552756143, 0.2331691293048937, 0.24302134615153514, 0.23481116552756143, 0.2331691293048937, 0.2413793099288674, 0.24302134615153514, 0.23645320126086425, 0.2446633822763299, 0.2413793099288674, 0.2413793099288674, 0.24302134615153514, 0.24302134624940813, 0.24302134615153514, 0.2413793099288674, 0.2413793099288674, 0.2413793099288674, 0.24302134615153514, 0.24302134615153514, 0.2413793099288674, 0.2413793099288674, 0.2413793099288674, 0.24466338256994882, 0.24466338247207586, 0.2413793099288674, 0.2413793099288674, 0.24466338247207586, 0.24466338247207586, 0.2413793099288674, 0.2413793099288674, 0.2446633822763299, 0.2413793099288674, 0.2413793099288674, 0.2413793099288674, 0.2413793099288674, 0.2413793099288674, 0.23973727370619968, 0.24302134624940813, 0.24466338256994882, 0.23973727370619968, 0.2413793099288674, 0.24466338256994882, 0.23645320155448318, 0.24302134624940813, 0.2413793100267404, 0.2413793099288674, 0.23645320155448318, 0.24302134644515408, 0.24466338247207586, 0.244663381786965, 0.24302134624940813, 0.2413793099288674, 0.2348111651360695, 0.23809523767927793, 0.24466338247207586, 0.23973727370619968, 0.24302134605366216, 0.23973727370619968, 0.24466338256994882, 0.2364532014566102, 0.2364532014566102, 0.24302134634728112, 0.23481116533181545, 0.23973727370619968, 0.24137931022248635, 0.23809523758140494, 0.22988505676168527, 0.24137930983099445, 0.23809523767927793, 0.2413793099288674, 0.24137930983099445, 0.23973727370619968, 0.24466338256994882, 0.23809523748353198, 0.23645320126086425, 0.2463054185968706, 0.24302134634728112, 0.24958949065071412, 0.238095237385659, 0.24302134644515408, 0.2348111651360695, 0.2364532014566102, 0.23645320165235617, 0.25123152677550886, 0.2413793100267404, 0.2364532014566102, 0.2364532014566102, 0.2397372740976916, 0.24958949065071412, 0.23481116533181545, 0.23481116552756143, 0.24794745452591938, 0.2348111651360695, 0.24466338237420288, 0.23645320135873724, 0.23152709327797194, 0.24137931032035934], 'loss': [1.6111767561283934, 1.6076154074384936, 1.60613817840631, 1.605837974705001, 1.605701000147042, 1.605269505943361, 1.6051950127681913, 1.6048714808614837, 1.6048565388704963, 1.6047227756198672, 1.60458674318247, 1.604372806911351, 1.604229273688377, 1.6040858900277766, 1.603952918747857, 1.6037144759352446, 1.6036138311793426, 1.603543454763581, 1.6031354605539623, 1.6029633840251507, 1.6026311040658971, 1.6023092375154122, 1.6022232650486596, 1.6022474908730822, 1.6021873051625748, 1.6020863830676069, 1.6012395709447058, 1.6013041013564906, 1.601364131531921, 1.6014425167068074, 1.601284851871232, 1.6012760784591737, 1.601741145229927, 1.6011651509596336, 1.6006212100845587, 1.6008065960000917, 1.6006340929125369, 1.600263347518028, 1.6009579391205335, 1.6021163590879657, 1.6011405415113946, 1.6013807545452392, 1.6007641580315342, 1.6006251225481287, 1.6002322130379492, 1.5999809955914162, 1.5996571568737774, 1.5993569120489353, 1.5990872845267858, 1.5991966359179612, 1.5989595164998112, 1.5988645626044615, 1.5991713556176093, 1.5991213868041303, 1.5985638300251421, 1.5987153297087495, 1.5988760257893275, 1.5982075378880118, 1.5986213653728947, 1.5984129532161924, 1.5979469424155703, 1.5979100229803787, 1.5976019273793183, 1.5978939664926861, 1.598420863327794, 1.5989646343235118, 1.5989007661474803, 1.5984189474117585, 1.5982499364465168, 1.5988292840961558, 1.5982317433464943, 1.5977309902101082, 1.597774712799511, 1.5979374455720248, 1.597501889587183, 1.597492614957586, 1.5974685005093991, 1.5970509036120937, 1.5999040020075177, 1.6003621071515877, 1.6020772001826542, 1.5990672447353418, 1.5993260881984013, 1.5985993806831156, 1.5979865594566236, 1.5981165900122702, 1.5977131160622504, 1.5977728882364668, 1.5973706025117722, 1.5976646296052717, 1.5978977072165488, 1.5975036987778588, 1.5975144861659965, 1.5980697389011265, 1.5970359524662245, 1.5970144571954465, 1.6000917928664347, 1.6004640838693545, 1.5990596396232777, 1.5991271078219405, 1.598924275002685, 1.597917864944411, 1.5970162169889257, 1.5973359398283753, 1.596774707731525, 1.5967243978375036, 1.596588870827912, 1.5956943363624432, 1.5958638661695947, 1.595539492699155, 1.596218158381186, 1.595729150909173, 1.5965367132633375, 1.596399045039496, 1.596881651290878, 1.5967034171983692, 1.5971408244765515, 1.5963451526003452, 1.5961515072679617, 1.5960877308855312, 1.5959526531994954, 1.5957437547570137, 1.5965080899134798, 1.5955499943766507, 1.595042542118801, 1.595249156295886, 1.5938776563569994, 1.5928801952201483, 1.5959627801143168, 1.5962384837364025, 1.5962445381485706, 1.5960145956192173, 1.5962300051409117, 1.5958574842378588, 1.5967142004741535, 1.595615039420079, 1.5953789661552382, 1.5955118197924791, 1.596699154401462, 1.5973216715289826, 1.5962019671160093, 1.5956607356453334, 1.5965850323867015, 1.595920085466373, 1.598415890905157, 1.6036423026169104, 1.5982557448028785, 1.5960609301404542, 1.5970867998056588, 1.5962201982553, 1.595469539464132, 1.595820849632091, 1.5950149178749726, 1.5948058708982056, 1.5947739531127334, 1.5944441467829553, 1.5937161938120943, 1.5940758972442126, 1.5941928926679387, 1.5939228065694382, 1.5940776114101527, 1.593140427875323, 1.5940248944676143, 1.593602985767858, 1.5936725154304896, 1.5933374647242333, 1.5920500337220804, 1.5956487868111235, 1.5966493509143775, 1.5969203600893276, 1.5973178788621811, 1.5962216855564157, 1.5975329246364336, 1.5967235962223467, 1.5967222529515104, 1.5962048423853255, 1.5961636675456712, 1.596442206194758, 1.5958024928701977, 1.5961673619566024, 1.5953315039679745, 1.5954305595930598, 1.5952970579174754, 1.5951953234858582, 1.595020926267949, 1.5959619996973622, 1.5953569503780263, 1.5958872085234468, 1.5954829882792134, 1.595540302490062, 1.5956614074276214, 1.5955604064146351, 1.5961315323929521, 1.5954139250749435, 1.5958482753080019, 1.595397954652931, 1.5954755076147937, 1.595155595705005, 1.595427507394638, 1.5948681505798559, 1.5950972525735656, 1.5952156731235418, 1.5947706778191444, 1.5948461767094826, 1.5946143645280686, 1.594869391237686, 1.5944885590238003, 1.5947141338422803, 1.594535745095913, 1.5946890949468593, 1.5949591929907672, 1.5946945623695483, 1.5945486618018494, 1.5943790432364056, 1.5943891021015708, 1.5943291653353087, 1.5948375965780301, 1.5942522764205933, 1.594253046507708, 1.594401079232688, 1.5940309129456476, 1.5939284747141342, 1.593998300417248, 1.5940835581667858, 1.594361110585426, 1.5939931359379198, 1.594016683273002, 1.5939597044637315, 1.594125836487913, 1.5940372035243917, 1.594276810965254, 1.5940286598166402, 1.5939335753051163, 1.593806375027682, 1.5938149640202768, 1.5935792758479501, 1.5937075147393793, 1.5936230211532092, 1.593750017085849, 1.5935621477984794, 1.5936799207507217, 1.593872066983452, 1.5933142056944924, 1.593404088617595, 1.5939108642954112, 1.5932279507971887, 1.5933245527181292, 1.5932718404754231, 1.5932442562291265, 1.594440026743456, 1.5933316754609408, 1.5933450620522, 1.5940723898964007, 1.5936865662157658, 1.5933560850194348, 1.5927318038881682, 1.5929784283256139, 1.5927140831212978, 1.5926586786579546, 1.5924890676318253, 1.592802387684033, 1.59302354945784, 1.592477765514131, 1.592649835580673, 1.593408937375893, 1.5925507832846357, 1.592361784617759, 1.5922452242222656, 1.5921838900391816, 1.592012184943996, 1.5929462449506568, 1.592188723327198, 1.5922326983857205, 1.5920845475774525, 1.5924927705612026, 1.5920619781501975, 1.5917146408092804, 1.5922308807255552, 1.5918720753530702, 1.591889520543312, 1.5915751513514431, 1.5918379653895416, 1.5929890354066414, 1.5914593170310927, 1.592095515811223, 1.5917148398667635, 1.5913436400572132, 1.591539774244571, 1.5913416835072105, 1.5915335323286741, 1.5918116175908084, 1.5910470530727316, 1.5917805728481536, 1.5922309331091034, 1.5906195883388636, 1.5919488196010707, 1.5913198921959504, 1.591208874126724, 1.5924601341419886, 1.5910198423162378], 'acc': [0.19671457792943997, 0.22710472305574947, 0.23203285367092313, 0.23285420851541005, 0.23285420992291195, 0.2328542087112364, 0.2328542102778472, 0.2328542087112364, 0.23285420872959514, 0.23285421029620593, 0.23285420970872686, 0.23285420912124782, 0.2312114992180889, 0.23203285367092313, 0.23285421029620593, 0.2328542102778472, 0.23285421029620593, 0.2328542098861945, 0.2328542087112364, 0.2324435326781361, 0.23326488691678526, 0.23326488652513258, 0.23572895477929398, 0.2357289524293778, 0.23285421010037957, 0.2324435314848193, 0.23449691978935344, 0.2316221758753857, 0.23778234190887004, 0.24476386122023056, 0.24353182738688936, 0.2435318287393151, 0.24024640761728894, 0.24188911529292317, 0.24558521365972516, 0.2427104725240437, 0.24106776107263272, 0.24188911728790408, 0.23696098645854535, 0.2336755653365192, 0.23449691777601378, 0.24106776283506984, 0.24517453883830037, 0.23901437356976268, 0.24065708365038924, 0.2455852174171432, 0.24640657108667205, 0.2459958928811232, 0.24517453784080992, 0.244353183583802, 0.24312114976881954, 0.24476386120187182, 0.24435318280049662, 0.2431211507479513, 0.24312114975046084, 0.2435318263893989, 0.24229979549345296, 0.24312115053376623, 0.24353182719106303, 0.24558521624218513, 0.24353182660358397, 0.2435318283476624, 0.246817249506406, 0.24517453903412673, 0.2464065708908457, 0.2459958936644286, 0.24681724792143647, 0.24722792635952912, 0.24640657071337807, 0.2447638602410988, 0.2427104725240437, 0.24681724670976093, 0.249281313434029, 0.24640656991171397, 0.24722792730194343, 0.24681724948804726, 0.24763860276592342, 0.24517453864247402, 0.2410677626576022, 0.2427104733073491, 0.24147844027567203, 0.2480492835172148, 0.24394250655321126, 0.24476386063275152, 0.24476386041856643, 0.2451745376633423, 0.24599589405608127, 0.24312114975046084, 0.24558521326807245, 0.24681724811726283, 0.2468172485089155, 0.24599589387861365, 0.24312114957299322, 0.24517453764498356, 0.24435318299632297, 0.24517453805499498, 0.2275154008696456, 0.23819301893946082, 0.24599589346860223, 0.24229979510180025, 0.24394250420329508, 0.24271047033323645, 0.2468172479397952, 0.24476386065111022, 0.24681724811726283, 0.24558521467557434, 0.24599589464356034, 0.25338809100387033, 0.24969199320618865, 0.24476386120187182, 0.24271047271987006, 0.24928131202652715, 0.2505133480506756, 0.2521560579354758, 0.24804928057981956, 0.24312115014211352, 0.24394250400746872, 0.24476386257265628, 0.25092402488544, 0.2484599578062367, 0.25010266945347404, 0.24804928196896273, 0.24722792692864945, 0.25174538051323236, 0.2484599585895421, 0.24928131441316076, 0.24640657226163015, 0.2558521568897568, 0.2455852156730648, 0.24928131323820266, 0.24312115014211352, 0.23901437298228365, 0.2484599585895421, 0.24845995819788938, 0.24722792651863804, 0.25133470132855174, 0.24558521545887974, 0.24804928116729863, 0.2394250504045271, 0.24229979512015898, 0.2468172461222819, 0.24640657108667205, 0.24599589229364416, 0.24517453803663625, 0.22997946520728008, 0.23285421029620593, 0.24024640622814578, 0.24763860374505514, 0.24476386082857787, 0.24599589483938666, 0.2406570838462156, 0.2394250523811493, 0.25133470252186857, 0.24763860376341387, 0.23983572980339277, 0.24188911807120947, 0.24681724733395743, 0.249281313434029, 0.24969199398949407, 0.2439425055740795, 0.24722792632281168, 0.24928131402150805, 0.253388091181338, 0.2529774133858005, 0.24476386081021914, 0.24599589307694955, 0.25338809000637985, 0.2501026692576477, 0.25051334863815466, 0.24558521667055525, 0.24065708464787972, 0.24969199242288326, 0.24887063661762332, 0.24928131382568172, 0.24887063836170173, 0.24928131402150805, 0.25010267141173753, 0.24804928195060402, 0.25133469992104984, 0.2484599597828589, 0.24106776226594953, 0.24845995737786655, 0.24887063816587537, 0.2480492835172148, 0.2517453799257533, 0.2509240249037987, 0.24928131323820266, 0.24558521565470606, 0.2447638602410988, 0.2501026716075639, 0.2488706362076119, 0.25092402568710415, 0.2492813118307008, 0.2505133446849102, 0.2492813142173344, 0.25092402527709273, 0.2488706362259706, 0.24435318360216074, 0.24353182658522526, 0.2501026706284321, 0.25051334667989117, 0.2513347025035098, 0.2542094464174776, 0.2521560567788764, 0.2533880898105535, 0.2513347019160308, 0.252977411800831, 0.25297741199665735, 0.25297741356326814, 0.25174538053159107, 0.245585214479748, 0.24928131519646615, 0.2525667373159828, 0.2517453807274174, 0.2521560579354758, 0.24435318221301758, 0.25626283568278474, 0.2517453818840168, 0.25092402529545144, 0.25256673457441386, 0.2521560567605177, 0.2509240254729191, 0.2525667327752593, 0.2521560583271285, 0.25256673437858757, 0.25174538051323236, 0.25297741317161543, 0.25092402269463276, 0.25133470031270255, 0.2521560589146076, 0.2550307988753309, 0.2537987684261138, 0.2521560579354758, 0.251334701096008, 0.24928131384404043, 0.2529774133858005, 0.2513347015243781, 0.2537987697785395, 0.25338809039803256, 0.25133470211185716, 0.249691993402015, 0.2509240250812664, 0.2542094460258249, 0.2505133468757175, 0.25133470330517393, 0.2521560593062602, 0.25174537972992694, 0.2554414779009026, 0.2517453807090587, 0.2459958936644286, 0.2488706374009287, 0.251745380317406, 0.2472279271244758, 0.24804928177313637, 0.25010267023677946, 0.2525667333627384, 0.2542094460441836, 0.2558521566939305, 0.2595482568231696, 0.25215605854131357, 0.2533880893821834, 0.25338809039803256, 0.251745379944112, 0.25133470330517393, 0.2509240254912778, 0.24969199124792518, 0.25462012462302647, 0.2529774129757891, 0.2505133484423283, 0.2529774121924837, 0.2451745384466477, 0.2566735133008546, 0.2525667353577193, 0.25133470031270255, 0.25133470367846794, 0.2513347023260422, 0.2542094460258249, 0.2550308016536172, 0.24599589407444, 0.2513347015243781, 0.25749486834116786, 0.25626283372452124, 0.2464065701075403, 0.24969199301036232, 0.2525667353577193, 0.2509240254912778, 0.25462012423137376, 0.2546201228605893, 0.25462012227311026, 0.2533880905938589, 0.2542094464358363, 0.25462012385807975, 0.2509240230862854, 0.2533880902022062, 0.2583162239689602, 0.2513347031093476, 0.24804928236061544, 0.24887063681344967, 0.2546201244272001, 0.25297741201501606]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
