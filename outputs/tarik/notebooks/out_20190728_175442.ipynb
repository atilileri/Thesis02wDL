{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf65.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 17:54:43 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'Front', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '04', '05', '03', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F1664ABE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F161C77EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6125, Accuracy:0.1930, Validation Loss:1.6103, Validation Accuracy:0.2085\n",
    "Epoch #2: Loss:1.6087, Accuracy:0.2107, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6070, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6025, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6019, Accuracy:0.2357, Validation Loss:1.6031, Validation Accuracy:0.2348\n",
    "Epoch #11: Loss:1.6016, Accuracy:0.2382, Validation Loss:1.6031, Validation Accuracy:0.2365\n",
    "Epoch #12: Loss:1.6011, Accuracy:0.2415, Validation Loss:1.6036, Validation Accuracy:0.2381\n",
    "Epoch #13: Loss:1.6010, Accuracy:0.2431, Validation Loss:1.6030, Validation Accuracy:0.2397\n",
    "Epoch #14: Loss:1.6006, Accuracy:0.2435, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #15: Loss:1.6007, Accuracy:0.2444, Validation Loss:1.6032, Validation Accuracy:0.2430\n",
    "Epoch #16: Loss:1.6009, Accuracy:0.2439, Validation Loss:1.6040, Validation Accuracy:0.2299\n",
    "Epoch #17: Loss:1.6009, Accuracy:0.2444, Validation Loss:1.6037, Validation Accuracy:0.2430\n",
    "Epoch #18: Loss:1.6003, Accuracy:0.2431, Validation Loss:1.6037, Validation Accuracy:0.2447\n",
    "Epoch #19: Loss:1.6002, Accuracy:0.2435, Validation Loss:1.6035, Validation Accuracy:0.2447\n",
    "Epoch #20: Loss:1.6000, Accuracy:0.2452, Validation Loss:1.6036, Validation Accuracy:0.2414\n",
    "Epoch #21: Loss:1.6008, Accuracy:0.2382, Validation Loss:1.6036, Validation Accuracy:0.2381\n",
    "Epoch #22: Loss:1.6011, Accuracy:0.2398, Validation Loss:1.6034, Validation Accuracy:0.2447\n",
    "Epoch #23: Loss:1.6013, Accuracy:0.2431, Validation Loss:1.6041, Validation Accuracy:0.2365\n",
    "Epoch #24: Loss:1.6006, Accuracy:0.2460, Validation Loss:1.6034, Validation Accuracy:0.2430\n",
    "Epoch #25: Loss:1.6000, Accuracy:0.2480, Validation Loss:1.6025, Validation Accuracy:0.2430\n",
    "Epoch #26: Loss:1.5995, Accuracy:0.2460, Validation Loss:1.6029, Validation Accuracy:0.2365\n",
    "Epoch #27: Loss:1.5999, Accuracy:0.2448, Validation Loss:1.6035, Validation Accuracy:0.2430\n",
    "Epoch #28: Loss:1.6002, Accuracy:0.2489, Validation Loss:1.6034, Validation Accuracy:0.2365\n",
    "Epoch #29: Loss:1.5993, Accuracy:0.2452, Validation Loss:1.6037, Validation Accuracy:0.2430\n",
    "Epoch #30: Loss:1.5997, Accuracy:0.2452, Validation Loss:1.6035, Validation Accuracy:0.2430\n",
    "Epoch #31: Loss:1.5996, Accuracy:0.2468, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #32: Loss:1.5990, Accuracy:0.2444, Validation Loss:1.6038, Validation Accuracy:0.2414\n",
    "Epoch #33: Loss:1.5992, Accuracy:0.2452, Validation Loss:1.6038, Validation Accuracy:0.2447\n",
    "Epoch #34: Loss:1.5993, Accuracy:0.2456, Validation Loss:1.6038, Validation Accuracy:0.2381\n",
    "Epoch #35: Loss:1.5988, Accuracy:0.2456, Validation Loss:1.6039, Validation Accuracy:0.2381\n",
    "Epoch #36: Loss:1.5994, Accuracy:0.2419, Validation Loss:1.6069, Validation Accuracy:0.2250\n",
    "Epoch #37: Loss:1.6012, Accuracy:0.2312, Validation Loss:1.6051, Validation Accuracy:0.2365\n",
    "Epoch #38: Loss:1.5994, Accuracy:0.2431, Validation Loss:1.6041, Validation Accuracy:0.2365\n",
    "Epoch #39: Loss:1.5994, Accuracy:0.2444, Validation Loss:1.6043, Validation Accuracy:0.2365\n",
    "Epoch #40: Loss:1.5992, Accuracy:0.2468, Validation Loss:1.6035, Validation Accuracy:0.2414\n",
    "Epoch #41: Loss:1.5989, Accuracy:0.2476, Validation Loss:1.6031, Validation Accuracy:0.2430\n",
    "Epoch #42: Loss:1.5987, Accuracy:0.2472, Validation Loss:1.6032, Validation Accuracy:0.2430\n",
    "Epoch #43: Loss:1.5985, Accuracy:0.2489, Validation Loss:1.6029, Validation Accuracy:0.2430\n",
    "Epoch #44: Loss:1.5984, Accuracy:0.2489, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #45: Loss:1.5983, Accuracy:0.2489, Validation Loss:1.6029, Validation Accuracy:0.2430\n",
    "Epoch #46: Loss:1.5980, Accuracy:0.2464, Validation Loss:1.6032, Validation Accuracy:0.2414\n",
    "Epoch #47: Loss:1.5980, Accuracy:0.2472, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #48: Loss:1.5979, Accuracy:0.2485, Validation Loss:1.6034, Validation Accuracy:0.2414\n",
    "Epoch #49: Loss:1.5980, Accuracy:0.2489, Validation Loss:1.6032, Validation Accuracy:0.2397\n",
    "Epoch #50: Loss:1.5974, Accuracy:0.2485, Validation Loss:1.6033, Validation Accuracy:0.2430\n",
    "Epoch #51: Loss:1.5976, Accuracy:0.2448, Validation Loss:1.6032, Validation Accuracy:0.2430\n",
    "Epoch #52: Loss:1.5972, Accuracy:0.2472, Validation Loss:1.6035, Validation Accuracy:0.2381\n",
    "Epoch #53: Loss:1.5973, Accuracy:0.2456, Validation Loss:1.6040, Validation Accuracy:0.2381\n",
    "Epoch #54: Loss:1.5972, Accuracy:0.2472, Validation Loss:1.6039, Validation Accuracy:0.2381\n",
    "Epoch #55: Loss:1.5971, Accuracy:0.2472, Validation Loss:1.6040, Validation Accuracy:0.2430\n",
    "Epoch #56: Loss:1.5971, Accuracy:0.2456, Validation Loss:1.6039, Validation Accuracy:0.2430\n",
    "Epoch #57: Loss:1.5963, Accuracy:0.2509, Validation Loss:1.6045, Validation Accuracy:0.2381\n",
    "Epoch #58: Loss:1.5972, Accuracy:0.2476, Validation Loss:1.6043, Validation Accuracy:0.2414\n",
    "Epoch #59: Loss:1.5965, Accuracy:0.2489, Validation Loss:1.6039, Validation Accuracy:0.2430\n",
    "Epoch #60: Loss:1.5969, Accuracy:0.2452, Validation Loss:1.6040, Validation Accuracy:0.2447\n",
    "Epoch #61: Loss:1.5963, Accuracy:0.2439, Validation Loss:1.6041, Validation Accuracy:0.2397\n",
    "Epoch #62: Loss:1.5963, Accuracy:0.2468, Validation Loss:1.6046, Validation Accuracy:0.2397\n",
    "Epoch #63: Loss:1.5958, Accuracy:0.2468, Validation Loss:1.6045, Validation Accuracy:0.2430\n",
    "Epoch #64: Loss:1.5959, Accuracy:0.2452, Validation Loss:1.6044, Validation Accuracy:0.2381\n",
    "Epoch #65: Loss:1.5955, Accuracy:0.2485, Validation Loss:1.6045, Validation Accuracy:0.2414\n",
    "Epoch #66: Loss:1.5955, Accuracy:0.2456, Validation Loss:1.6050, Validation Accuracy:0.2430\n",
    "Epoch #67: Loss:1.5962, Accuracy:0.2423, Validation Loss:1.6049, Validation Accuracy:0.2430\n",
    "Epoch #68: Loss:1.5955, Accuracy:0.2456, Validation Loss:1.6055, Validation Accuracy:0.2381\n",
    "Epoch #69: Loss:1.5956, Accuracy:0.2423, Validation Loss:1.6055, Validation Accuracy:0.2414\n",
    "Epoch #70: Loss:1.5951, Accuracy:0.2489, Validation Loss:1.6049, Validation Accuracy:0.2381\n",
    "Epoch #71: Loss:1.5953, Accuracy:0.2489, Validation Loss:1.6048, Validation Accuracy:0.2151\n",
    "Epoch #72: Loss:1.5949, Accuracy:0.2448, Validation Loss:1.6050, Validation Accuracy:0.2397\n",
    "Epoch #73: Loss:1.5959, Accuracy:0.2398, Validation Loss:1.6062, Validation Accuracy:0.2069\n",
    "Epoch #74: Loss:1.5971, Accuracy:0.2378, Validation Loss:1.6048, Validation Accuracy:0.2381\n",
    "Epoch #75: Loss:1.5955, Accuracy:0.2439, Validation Loss:1.6051, Validation Accuracy:0.2414\n",
    "Epoch #76: Loss:1.5976, Accuracy:0.2357, Validation Loss:1.6045, Validation Accuracy:0.2135\n",
    "Epoch #77: Loss:1.5953, Accuracy:0.2349, Validation Loss:1.6048, Validation Accuracy:0.2365\n",
    "Epoch #78: Loss:1.5946, Accuracy:0.2452, Validation Loss:1.6049, Validation Accuracy:0.2397\n",
    "Epoch #79: Loss:1.5939, Accuracy:0.2439, Validation Loss:1.6057, Validation Accuracy:0.2184\n",
    "Epoch #80: Loss:1.5941, Accuracy:0.2460, Validation Loss:1.6056, Validation Accuracy:0.2397\n",
    "Epoch #81: Loss:1.5949, Accuracy:0.2476, Validation Loss:1.6051, Validation Accuracy:0.2397\n",
    "Epoch #82: Loss:1.5957, Accuracy:0.2452, Validation Loss:1.6065, Validation Accuracy:0.2266\n",
    "Epoch #83: Loss:1.5968, Accuracy:0.2402, Validation Loss:1.6046, Validation Accuracy:0.2365\n",
    "Epoch #84: Loss:1.5974, Accuracy:0.2439, Validation Loss:1.6056, Validation Accuracy:0.2397\n",
    "Epoch #85: Loss:1.5948, Accuracy:0.2444, Validation Loss:1.6042, Validation Accuracy:0.2463\n",
    "Epoch #86: Loss:1.5970, Accuracy:0.2448, Validation Loss:1.6049, Validation Accuracy:0.2118\n",
    "Epoch #87: Loss:1.5952, Accuracy:0.2415, Validation Loss:1.6032, Validation Accuracy:0.2414\n",
    "Epoch #88: Loss:1.5951, Accuracy:0.2448, Validation Loss:1.6043, Validation Accuracy:0.2381\n",
    "Epoch #89: Loss:1.5963, Accuracy:0.2476, Validation Loss:1.6040, Validation Accuracy:0.2414\n",
    "Epoch #90: Loss:1.5951, Accuracy:0.2427, Validation Loss:1.6042, Validation Accuracy:0.2447\n",
    "Epoch #91: Loss:1.5950, Accuracy:0.2439, Validation Loss:1.6045, Validation Accuracy:0.2381\n",
    "Epoch #92: Loss:1.5945, Accuracy:0.2480, Validation Loss:1.6031, Validation Accuracy:0.2414\n",
    "Epoch #93: Loss:1.5955, Accuracy:0.2444, Validation Loss:1.6061, Validation Accuracy:0.2365\n",
    "Epoch #94: Loss:1.5958, Accuracy:0.2452, Validation Loss:1.6063, Validation Accuracy:0.2397\n",
    "Epoch #95: Loss:1.5990, Accuracy:0.2452, Validation Loss:1.6106, Validation Accuracy:0.2332\n",
    "Epoch #96: Loss:1.5996, Accuracy:0.2407, Validation Loss:1.6088, Validation Accuracy:0.2332\n",
    "Epoch #97: Loss:1.6014, Accuracy:0.2439, Validation Loss:1.6088, Validation Accuracy:0.2447\n",
    "Epoch #98: Loss:1.6009, Accuracy:0.2324, Validation Loss:1.6105, Validation Accuracy:0.2217\n",
    "Epoch #99: Loss:1.5989, Accuracy:0.2357, Validation Loss:1.6083, Validation Accuracy:0.2414\n",
    "Epoch #100: Loss:1.5962, Accuracy:0.2476, Validation Loss:1.6085, Validation Accuracy:0.2430\n",
    "Epoch #101: Loss:1.5963, Accuracy:0.2460, Validation Loss:1.6053, Validation Accuracy:0.2479\n",
    "Epoch #102: Loss:1.5958, Accuracy:0.2444, Validation Loss:1.6064, Validation Accuracy:0.2496\n",
    "Epoch #103: Loss:1.5961, Accuracy:0.2435, Validation Loss:1.6057, Validation Accuracy:0.2397\n",
    "Epoch #104: Loss:1.5951, Accuracy:0.2431, Validation Loss:1.6052, Validation Accuracy:0.2479\n",
    "Epoch #105: Loss:1.5949, Accuracy:0.2444, Validation Loss:1.6048, Validation Accuracy:0.2463\n",
    "Epoch #106: Loss:1.5949, Accuracy:0.2411, Validation Loss:1.6049, Validation Accuracy:0.2447\n",
    "Epoch #107: Loss:1.5940, Accuracy:0.2448, Validation Loss:1.6051, Validation Accuracy:0.2430\n",
    "Epoch #108: Loss:1.5949, Accuracy:0.2382, Validation Loss:1.6056, Validation Accuracy:0.2430\n",
    "Epoch #109: Loss:1.5941, Accuracy:0.2398, Validation Loss:1.6064, Validation Accuracy:0.2463\n",
    "Epoch #110: Loss:1.5946, Accuracy:0.2419, Validation Loss:1.6070, Validation Accuracy:0.2414\n",
    "Epoch #111: Loss:1.5956, Accuracy:0.2411, Validation Loss:1.6060, Validation Accuracy:0.2479\n",
    "Epoch #112: Loss:1.5950, Accuracy:0.2431, Validation Loss:1.6053, Validation Accuracy:0.2430\n",
    "Epoch #113: Loss:1.5935, Accuracy:0.2485, Validation Loss:1.6087, Validation Accuracy:0.2200\n",
    "Epoch #114: Loss:1.5959, Accuracy:0.2386, Validation Loss:1.6079, Validation Accuracy:0.2430\n",
    "Epoch #115: Loss:1.5960, Accuracy:0.2431, Validation Loss:1.6081, Validation Accuracy:0.2381\n",
    "Epoch #116: Loss:1.5960, Accuracy:0.2435, Validation Loss:1.6078, Validation Accuracy:0.2397\n",
    "Epoch #117: Loss:1.5956, Accuracy:0.2435, Validation Loss:1.6075, Validation Accuracy:0.2430\n",
    "Epoch #118: Loss:1.5955, Accuracy:0.2493, Validation Loss:1.6047, Validation Accuracy:0.2266\n",
    "Epoch #119: Loss:1.5959, Accuracy:0.2370, Validation Loss:1.6091, Validation Accuracy:0.2365\n",
    "Epoch #120: Loss:1.5961, Accuracy:0.2480, Validation Loss:1.6098, Validation Accuracy:0.2397\n",
    "Epoch #121: Loss:1.5953, Accuracy:0.2476, Validation Loss:1.6097, Validation Accuracy:0.2348\n",
    "Epoch #122: Loss:1.5953, Accuracy:0.2464, Validation Loss:1.6106, Validation Accuracy:0.2397\n",
    "Epoch #123: Loss:1.5951, Accuracy:0.2485, Validation Loss:1.6103, Validation Accuracy:0.2348\n",
    "Epoch #124: Loss:1.5954, Accuracy:0.2468, Validation Loss:1.6103, Validation Accuracy:0.2167\n",
    "Epoch #125: Loss:1.5956, Accuracy:0.2480, Validation Loss:1.6102, Validation Accuracy:0.2266\n",
    "Epoch #126: Loss:1.5956, Accuracy:0.2456, Validation Loss:1.6097, Validation Accuracy:0.2397\n",
    "Epoch #127: Loss:1.5955, Accuracy:0.2476, Validation Loss:1.6097, Validation Accuracy:0.2184\n",
    "Epoch #128: Loss:1.5956, Accuracy:0.2448, Validation Loss:1.6101, Validation Accuracy:0.2184\n",
    "Epoch #129: Loss:1.5954, Accuracy:0.2419, Validation Loss:1.6096, Validation Accuracy:0.2266\n",
    "Epoch #130: Loss:1.5948, Accuracy:0.2456, Validation Loss:1.6095, Validation Accuracy:0.2217\n",
    "Epoch #131: Loss:1.5952, Accuracy:0.2419, Validation Loss:1.6091, Validation Accuracy:0.2184\n",
    "Epoch #132: Loss:1.5945, Accuracy:0.2423, Validation Loss:1.6083, Validation Accuracy:0.2250\n",
    "Epoch #133: Loss:1.5939, Accuracy:0.2419, Validation Loss:1.6078, Validation Accuracy:0.2299\n",
    "Epoch #134: Loss:1.5935, Accuracy:0.2402, Validation Loss:1.6083, Validation Accuracy:0.2315\n",
    "Epoch #135: Loss:1.5936, Accuracy:0.2427, Validation Loss:1.6084, Validation Accuracy:0.2315\n",
    "Epoch #136: Loss:1.5938, Accuracy:0.2423, Validation Loss:1.6080, Validation Accuracy:0.2200\n",
    "Epoch #137: Loss:1.5941, Accuracy:0.2390, Validation Loss:1.6077, Validation Accuracy:0.2118\n",
    "Epoch #138: Loss:1.5936, Accuracy:0.2402, Validation Loss:1.6075, Validation Accuracy:0.2266\n",
    "Epoch #139: Loss:1.5932, Accuracy:0.2419, Validation Loss:1.6064, Validation Accuracy:0.2266\n",
    "Epoch #140: Loss:1.5931, Accuracy:0.2456, Validation Loss:1.6063, Validation Accuracy:0.2200\n",
    "Epoch #141: Loss:1.5935, Accuracy:0.2448, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #142: Loss:1.5942, Accuracy:0.2394, Validation Loss:1.6078, Validation Accuracy:0.2200\n",
    "Epoch #143: Loss:1.5928, Accuracy:0.2444, Validation Loss:1.6069, Validation Accuracy:0.2348\n",
    "Epoch #144: Loss:1.5940, Accuracy:0.2402, Validation Loss:1.6071, Validation Accuracy:0.2299\n",
    "Epoch #145: Loss:1.5931, Accuracy:0.2431, Validation Loss:1.6067, Validation Accuracy:0.2365\n",
    "Epoch #146: Loss:1.5936, Accuracy:0.2390, Validation Loss:1.6066, Validation Accuracy:0.2184\n",
    "Epoch #147: Loss:1.5931, Accuracy:0.2378, Validation Loss:1.6078, Validation Accuracy:0.2233\n",
    "Epoch #148: Loss:1.5928, Accuracy:0.2435, Validation Loss:1.6073, Validation Accuracy:0.2332\n",
    "Epoch #149: Loss:1.5931, Accuracy:0.2439, Validation Loss:1.6072, Validation Accuracy:0.2430\n",
    "Epoch #150: Loss:1.5935, Accuracy:0.2489, Validation Loss:1.6068, Validation Accuracy:0.2397\n",
    "Epoch #151: Loss:1.5932, Accuracy:0.2480, Validation Loss:1.6070, Validation Accuracy:0.2250\n",
    "Epoch #152: Loss:1.5938, Accuracy:0.2493, Validation Loss:1.6072, Validation Accuracy:0.2348\n",
    "Epoch #153: Loss:1.5933, Accuracy:0.2468, Validation Loss:1.6069, Validation Accuracy:0.2414\n",
    "Epoch #154: Loss:1.5944, Accuracy:0.2415, Validation Loss:1.6060, Validation Accuracy:0.2627\n",
    "Epoch #155: Loss:1.5930, Accuracy:0.2509, Validation Loss:1.6064, Validation Accuracy:0.2233\n",
    "Epoch #156: Loss:1.5931, Accuracy:0.2460, Validation Loss:1.6062, Validation Accuracy:0.2299\n",
    "Epoch #157: Loss:1.5928, Accuracy:0.2489, Validation Loss:1.6063, Validation Accuracy:0.2545\n",
    "Epoch #158: Loss:1.5929, Accuracy:0.2472, Validation Loss:1.6064, Validation Accuracy:0.2348\n",
    "Epoch #159: Loss:1.5928, Accuracy:0.2472, Validation Loss:1.6064, Validation Accuracy:0.2315\n",
    "Epoch #160: Loss:1.5927, Accuracy:0.2444, Validation Loss:1.6064, Validation Accuracy:0.2430\n",
    "Epoch #161: Loss:1.5937, Accuracy:0.2509, Validation Loss:1.6063, Validation Accuracy:0.2414\n",
    "Epoch #162: Loss:1.5931, Accuracy:0.2505, Validation Loss:1.6064, Validation Accuracy:0.2282\n",
    "Epoch #163: Loss:1.5928, Accuracy:0.2501, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #164: Loss:1.5938, Accuracy:0.2472, Validation Loss:1.6077, Validation Accuracy:0.2299\n",
    "Epoch #165: Loss:1.5924, Accuracy:0.2513, Validation Loss:1.6066, Validation Accuracy:0.2430\n",
    "Epoch #166: Loss:1.5941, Accuracy:0.2460, Validation Loss:1.6059, Validation Accuracy:0.2430\n",
    "Epoch #167: Loss:1.5923, Accuracy:0.2501, Validation Loss:1.6064, Validation Accuracy:0.2299\n",
    "Epoch #168: Loss:1.5934, Accuracy:0.2427, Validation Loss:1.6058, Validation Accuracy:0.2348\n",
    "Epoch #169: Loss:1.5921, Accuracy:0.2485, Validation Loss:1.6059, Validation Accuracy:0.2348\n",
    "Epoch #170: Loss:1.5926, Accuracy:0.2497, Validation Loss:1.6059, Validation Accuracy:0.2299\n",
    "Epoch #171: Loss:1.5927, Accuracy:0.2509, Validation Loss:1.6061, Validation Accuracy:0.2463\n",
    "Epoch #172: Loss:1.5922, Accuracy:0.2480, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #173: Loss:1.5924, Accuracy:0.2501, Validation Loss:1.6064, Validation Accuracy:0.2282\n",
    "Epoch #174: Loss:1.5927, Accuracy:0.2485, Validation Loss:1.6058, Validation Accuracy:0.2348\n",
    "Epoch #175: Loss:1.5921, Accuracy:0.2509, Validation Loss:1.6059, Validation Accuracy:0.2463\n",
    "Epoch #176: Loss:1.5925, Accuracy:0.2517, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #177: Loss:1.5923, Accuracy:0.2509, Validation Loss:1.6056, Validation Accuracy:0.2299\n",
    "Epoch #178: Loss:1.5927, Accuracy:0.2509, Validation Loss:1.6060, Validation Accuracy:0.2315\n",
    "Epoch #179: Loss:1.5925, Accuracy:0.2509, Validation Loss:1.6058, Validation Accuracy:0.2381\n",
    "Epoch #180: Loss:1.5924, Accuracy:0.2497, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #181: Loss:1.5928, Accuracy:0.2476, Validation Loss:1.6058, Validation Accuracy:0.2348\n",
    "Epoch #182: Loss:1.5909, Accuracy:0.2497, Validation Loss:1.6060, Validation Accuracy:0.2545\n",
    "Epoch #183: Loss:1.5927, Accuracy:0.2534, Validation Loss:1.6058, Validation Accuracy:0.2463\n",
    "Epoch #184: Loss:1.5920, Accuracy:0.2501, Validation Loss:1.6061, Validation Accuracy:0.2381\n",
    "Epoch #185: Loss:1.5917, Accuracy:0.2460, Validation Loss:1.6055, Validation Accuracy:0.2430\n",
    "Epoch #186: Loss:1.5920, Accuracy:0.2501, Validation Loss:1.6062, Validation Accuracy:0.2397\n",
    "Epoch #187: Loss:1.5919, Accuracy:0.2534, Validation Loss:1.6056, Validation Accuracy:0.2315\n",
    "Epoch #188: Loss:1.5924, Accuracy:0.2476, Validation Loss:1.6063, Validation Accuracy:0.2250\n",
    "Epoch #189: Loss:1.5920, Accuracy:0.2472, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #190: Loss:1.5918, Accuracy:0.2509, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #191: Loss:1.5914, Accuracy:0.2530, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #192: Loss:1.5915, Accuracy:0.2444, Validation Loss:1.6055, Validation Accuracy:0.2430\n",
    "Epoch #193: Loss:1.5916, Accuracy:0.2522, Validation Loss:1.6052, Validation Accuracy:0.2447\n",
    "Epoch #194: Loss:1.5924, Accuracy:0.2509, Validation Loss:1.6056, Validation Accuracy:0.2381\n",
    "Epoch #195: Loss:1.5913, Accuracy:0.2489, Validation Loss:1.6057, Validation Accuracy:0.2381\n",
    "Epoch #196: Loss:1.5914, Accuracy:0.2505, Validation Loss:1.6050, Validation Accuracy:0.2430\n",
    "Epoch #197: Loss:1.5913, Accuracy:0.2517, Validation Loss:1.6050, Validation Accuracy:0.2447\n",
    "Epoch #198: Loss:1.5911, Accuracy:0.2472, Validation Loss:1.6052, Validation Accuracy:0.2381\n",
    "Epoch #199: Loss:1.5918, Accuracy:0.2480, Validation Loss:1.6056, Validation Accuracy:0.2430\n",
    "Epoch #200: Loss:1.5911, Accuracy:0.2517, Validation Loss:1.6056, Validation Accuracy:0.2381\n",
    "Epoch #201: Loss:1.5907, Accuracy:0.2534, Validation Loss:1.6055, Validation Accuracy:0.2381\n",
    "Epoch #202: Loss:1.5911, Accuracy:0.2534, Validation Loss:1.6056, Validation Accuracy:0.2447\n",
    "Epoch #203: Loss:1.5910, Accuracy:0.2526, Validation Loss:1.6050, Validation Accuracy:0.2430\n",
    "Epoch #204: Loss:1.5906, Accuracy:0.2513, Validation Loss:1.6055, Validation Accuracy:0.2381\n",
    "Epoch #205: Loss:1.5910, Accuracy:0.2554, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #206: Loss:1.5901, Accuracy:0.2501, Validation Loss:1.6055, Validation Accuracy:0.2414\n",
    "Epoch #207: Loss:1.5908, Accuracy:0.2509, Validation Loss:1.6053, Validation Accuracy:0.2430\n",
    "Epoch #208: Loss:1.5908, Accuracy:0.2509, Validation Loss:1.6061, Validation Accuracy:0.2365\n",
    "Epoch #209: Loss:1.5908, Accuracy:0.2538, Validation Loss:1.6053, Validation Accuracy:0.2430\n",
    "Epoch #210: Loss:1.5903, Accuracy:0.2517, Validation Loss:1.6060, Validation Accuracy:0.2414\n",
    "Epoch #211: Loss:1.5907, Accuracy:0.2530, Validation Loss:1.6051, Validation Accuracy:0.2479\n",
    "Epoch #212: Loss:1.5902, Accuracy:0.2522, Validation Loss:1.6054, Validation Accuracy:0.2414\n",
    "Epoch #213: Loss:1.5909, Accuracy:0.2480, Validation Loss:1.6058, Validation Accuracy:0.2397\n",
    "Epoch #214: Loss:1.5911, Accuracy:0.2493, Validation Loss:1.6068, Validation Accuracy:0.2365\n",
    "Epoch #215: Loss:1.5907, Accuracy:0.2509, Validation Loss:1.6080, Validation Accuracy:0.2463\n",
    "Epoch #216: Loss:1.5897, Accuracy:0.2513, Validation Loss:1.6090, Validation Accuracy:0.2381\n",
    "Epoch #217: Loss:1.5908, Accuracy:0.2530, Validation Loss:1.6099, Validation Accuracy:0.2463\n",
    "Epoch #218: Loss:1.5900, Accuracy:0.2583, Validation Loss:1.6097, Validation Accuracy:0.2397\n",
    "Epoch #219: Loss:1.5908, Accuracy:0.2538, Validation Loss:1.6095, Validation Accuracy:0.2414\n",
    "Epoch #220: Loss:1.5902, Accuracy:0.2526, Validation Loss:1.6113, Validation Accuracy:0.2447\n",
    "Epoch #221: Loss:1.5917, Accuracy:0.2546, Validation Loss:1.6064, Validation Accuracy:0.2430\n",
    "Epoch #222: Loss:1.5918, Accuracy:0.2505, Validation Loss:1.6088, Validation Accuracy:0.2365\n",
    "Epoch #223: Loss:1.5920, Accuracy:0.2505, Validation Loss:1.6079, Validation Accuracy:0.2414\n",
    "Epoch #224: Loss:1.5922, Accuracy:0.2534, Validation Loss:1.6065, Validation Accuracy:0.2512\n",
    "Epoch #225: Loss:1.5911, Accuracy:0.2575, Validation Loss:1.6068, Validation Accuracy:0.2545\n",
    "Epoch #226: Loss:1.5923, Accuracy:0.2579, Validation Loss:1.6072, Validation Accuracy:0.2496\n",
    "Epoch #227: Loss:1.5892, Accuracy:0.2575, Validation Loss:1.6094, Validation Accuracy:0.2463\n",
    "Epoch #228: Loss:1.5901, Accuracy:0.2554, Validation Loss:1.6115, Validation Accuracy:0.2381\n",
    "Epoch #229: Loss:1.5891, Accuracy:0.2563, Validation Loss:1.6093, Validation Accuracy:0.2430\n",
    "Epoch #230: Loss:1.5900, Accuracy:0.2575, Validation Loss:1.6107, Validation Accuracy:0.2447\n",
    "Epoch #231: Loss:1.5894, Accuracy:0.2550, Validation Loss:1.6106, Validation Accuracy:0.2463\n",
    "Epoch #232: Loss:1.5901, Accuracy:0.2571, Validation Loss:1.6110, Validation Accuracy:0.2447\n",
    "Epoch #233: Loss:1.5905, Accuracy:0.2554, Validation Loss:1.6107, Validation Accuracy:0.2447\n",
    "Epoch #234: Loss:1.5912, Accuracy:0.2563, Validation Loss:1.6098, Validation Accuracy:0.2414\n",
    "Epoch #235: Loss:1.5910, Accuracy:0.2505, Validation Loss:1.6105, Validation Accuracy:0.2414\n",
    "Epoch #236: Loss:1.5902, Accuracy:0.2517, Validation Loss:1.6099, Validation Accuracy:0.2463\n",
    "Epoch #237: Loss:1.5904, Accuracy:0.2559, Validation Loss:1.6103, Validation Accuracy:0.2479\n",
    "Epoch #238: Loss:1.5905, Accuracy:0.2571, Validation Loss:1.6103, Validation Accuracy:0.2447\n",
    "Epoch #239: Loss:1.5900, Accuracy:0.2534, Validation Loss:1.6102, Validation Accuracy:0.2447\n",
    "Epoch #240: Loss:1.5900, Accuracy:0.2550, Validation Loss:1.6109, Validation Accuracy:0.2447\n",
    "Epoch #241: Loss:1.5898, Accuracy:0.2530, Validation Loss:1.6107, Validation Accuracy:0.2463\n",
    "Epoch #242: Loss:1.5901, Accuracy:0.2513, Validation Loss:1.6113, Validation Accuracy:0.2397\n",
    "Epoch #243: Loss:1.5899, Accuracy:0.2538, Validation Loss:1.6128, Validation Accuracy:0.2348\n",
    "Epoch #244: Loss:1.5898, Accuracy:0.2485, Validation Loss:1.6118, Validation Accuracy:0.2365\n",
    "Epoch #245: Loss:1.5895, Accuracy:0.2497, Validation Loss:1.6105, Validation Accuracy:0.2479\n",
    "Epoch #246: Loss:1.5897, Accuracy:0.2649, Validation Loss:1.6093, Validation Accuracy:0.2332\n",
    "Epoch #247: Loss:1.5891, Accuracy:0.2563, Validation Loss:1.6085, Validation Accuracy:0.2365\n",
    "Epoch #248: Loss:1.5892, Accuracy:0.2485, Validation Loss:1.6082, Validation Accuracy:0.2463\n",
    "Epoch #249: Loss:1.5898, Accuracy:0.2567, Validation Loss:1.6073, Validation Accuracy:0.2414\n",
    "Epoch #250: Loss:1.5897, Accuracy:0.2579, Validation Loss:1.6083, Validation Accuracy:0.2430\n",
    "Epoch #251: Loss:1.5902, Accuracy:0.2493, Validation Loss:1.6080, Validation Accuracy:0.2365\n",
    "Epoch #252: Loss:1.5897, Accuracy:0.2517, Validation Loss:1.6081, Validation Accuracy:0.2397\n",
    "Epoch #253: Loss:1.5906, Accuracy:0.2476, Validation Loss:1.6082, Validation Accuracy:0.2315\n",
    "Epoch #254: Loss:1.5895, Accuracy:0.2530, Validation Loss:1.6084, Validation Accuracy:0.2348\n",
    "Epoch #255: Loss:1.5901, Accuracy:0.2480, Validation Loss:1.6084, Validation Accuracy:0.2479\n",
    "Epoch #256: Loss:1.5889, Accuracy:0.2522, Validation Loss:1.6078, Validation Accuracy:0.2348\n",
    "Epoch #257: Loss:1.5897, Accuracy:0.2444, Validation Loss:1.6086, Validation Accuracy:0.2282\n",
    "Epoch #258: Loss:1.5886, Accuracy:0.2517, Validation Loss:1.6085, Validation Accuracy:0.2463\n",
    "Epoch #259: Loss:1.5891, Accuracy:0.2538, Validation Loss:1.6086, Validation Accuracy:0.2397\n",
    "Epoch #260: Loss:1.5887, Accuracy:0.2493, Validation Loss:1.6085, Validation Accuracy:0.2266\n",
    "Epoch #261: Loss:1.5884, Accuracy:0.2456, Validation Loss:1.6090, Validation Accuracy:0.2266\n",
    "Epoch #262: Loss:1.5884, Accuracy:0.2456, Validation Loss:1.6084, Validation Accuracy:0.2315\n",
    "Epoch #263: Loss:1.5892, Accuracy:0.2452, Validation Loss:1.6085, Validation Accuracy:0.2348\n",
    "Epoch #264: Loss:1.5885, Accuracy:0.2476, Validation Loss:1.6083, Validation Accuracy:0.2430\n",
    "Epoch #265: Loss:1.5894, Accuracy:0.2476, Validation Loss:1.6090, Validation Accuracy:0.2233\n",
    "Epoch #266: Loss:1.5900, Accuracy:0.2386, Validation Loss:1.6083, Validation Accuracy:0.2299\n",
    "Epoch #267: Loss:1.5887, Accuracy:0.2522, Validation Loss:1.6093, Validation Accuracy:0.2397\n",
    "Epoch #268: Loss:1.5904, Accuracy:0.2435, Validation Loss:1.6086, Validation Accuracy:0.2250\n",
    "Epoch #269: Loss:1.5889, Accuracy:0.2472, Validation Loss:1.6081, Validation Accuracy:0.2266\n",
    "Epoch #270: Loss:1.5897, Accuracy:0.2444, Validation Loss:1.6073, Validation Accuracy:0.2315\n",
    "Epoch #271: Loss:1.5887, Accuracy:0.2456, Validation Loss:1.6096, Validation Accuracy:0.2348\n",
    "Epoch #272: Loss:1.5883, Accuracy:0.2456, Validation Loss:1.6081, Validation Accuracy:0.2282\n",
    "Epoch #273: Loss:1.5883, Accuracy:0.2444, Validation Loss:1.6067, Validation Accuracy:0.2365\n",
    "Epoch #274: Loss:1.5879, Accuracy:0.2427, Validation Loss:1.6072, Validation Accuracy:0.2315\n",
    "Epoch #275: Loss:1.5873, Accuracy:0.2435, Validation Loss:1.6065, Validation Accuracy:0.2348\n",
    "Epoch #276: Loss:1.5880, Accuracy:0.2439, Validation Loss:1.6069, Validation Accuracy:0.2315\n",
    "Epoch #277: Loss:1.5878, Accuracy:0.2480, Validation Loss:1.6073, Validation Accuracy:0.2299\n",
    "Epoch #278: Loss:1.5877, Accuracy:0.2464, Validation Loss:1.6078, Validation Accuracy:0.2365\n",
    "Epoch #279: Loss:1.5882, Accuracy:0.2472, Validation Loss:1.6086, Validation Accuracy:0.2266\n",
    "Epoch #280: Loss:1.5879, Accuracy:0.2472, Validation Loss:1.6095, Validation Accuracy:0.2332\n",
    "Epoch #281: Loss:1.5882, Accuracy:0.2460, Validation Loss:1.6083, Validation Accuracy:0.2266\n",
    "Epoch #282: Loss:1.5881, Accuracy:0.2415, Validation Loss:1.6088, Validation Accuracy:0.2282\n",
    "Epoch #283: Loss:1.5882, Accuracy:0.2427, Validation Loss:1.6082, Validation Accuracy:0.2332\n",
    "Epoch #284: Loss:1.5893, Accuracy:0.2444, Validation Loss:1.6100, Validation Accuracy:0.2332\n",
    "Epoch #285: Loss:1.5892, Accuracy:0.2497, Validation Loss:1.6090, Validation Accuracy:0.2348\n",
    "Epoch #286: Loss:1.5877, Accuracy:0.2476, Validation Loss:1.6087, Validation Accuracy:0.2250\n",
    "Epoch #287: Loss:1.5876, Accuracy:0.2464, Validation Loss:1.6088, Validation Accuracy:0.2266\n",
    "Epoch #288: Loss:1.5926, Accuracy:0.2497, Validation Loss:1.6174, Validation Accuracy:0.2135\n",
    "Epoch #289: Loss:1.6002, Accuracy:0.2407, Validation Loss:1.6138, Validation Accuracy:0.2266\n",
    "Epoch #290: Loss:1.5963, Accuracy:0.2526, Validation Loss:1.6102, Validation Accuracy:0.2562\n",
    "Epoch #291: Loss:1.5934, Accuracy:0.2468, Validation Loss:1.6048, Validation Accuracy:0.2479\n",
    "Epoch #292: Loss:1.5942, Accuracy:0.2419, Validation Loss:1.6070, Validation Accuracy:0.2447\n",
    "Epoch #293: Loss:1.5928, Accuracy:0.2427, Validation Loss:1.6066, Validation Accuracy:0.2447\n",
    "Epoch #294: Loss:1.5907, Accuracy:0.2559, Validation Loss:1.6052, Validation Accuracy:0.2529\n",
    "Epoch #295: Loss:1.5898, Accuracy:0.2444, Validation Loss:1.6059, Validation Accuracy:0.2365\n",
    "Epoch #296: Loss:1.5899, Accuracy:0.2517, Validation Loss:1.6053, Validation Accuracy:0.2512\n",
    "Epoch #297: Loss:1.5894, Accuracy:0.2509, Validation Loss:1.6061, Validation Accuracy:0.2479\n",
    "Epoch #298: Loss:1.5889, Accuracy:0.2468, Validation Loss:1.6054, Validation Accuracy:0.2562\n",
    "Epoch #299: Loss:1.5882, Accuracy:0.2517, Validation Loss:1.6046, Validation Accuracy:0.2562\n",
    "Epoch #300: Loss:1.5886, Accuracy:0.2554, Validation Loss:1.6035, Validation Accuracy:0.2496\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60349333, Accuracy:0.2496\n",
    "Labels: ['01', '04', '05', '03', '02']\n",
    "Confusion Matrix:\n",
    "      01  04  05  03  02\n",
    "t:01  26  19  56  25   0\n",
    "t:04  17  20  61  14   0\n",
    "t:05  19  12  83  28   0\n",
    "t:03  23  19  52  21   0\n",
    "t:02  17  27  53  15   2\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.25      0.21      0.23       126\n",
    "          04       0.21      0.18      0.19       112\n",
    "          05       0.27      0.58      0.37       142\n",
    "          03       0.20      0.18      0.19       115\n",
    "          02       1.00      0.02      0.03       114\n",
    "\n",
    "    accuracy                           0.25       609\n",
    "   macro avg       0.39      0.23      0.20       609\n",
    "weighted avg       0.38      0.25      0.21       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 18:10:13 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 30 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6103270819230229, 1.6071725817540987, 1.605732389075807, 1.605177977402222, 1.604500027908676, 1.6045578040904405, 1.6040154006485086, 1.6035844300963804, 1.6032858658306703, 1.6030975977579753, 1.6030752999441964, 1.6036309336598087, 1.6029502539016147, 1.6030140220629563, 1.6031752775846835, 1.603972536002474, 1.603675452946442, 1.6037158654828376, 1.6035485189340777, 1.6036363613037836, 1.603551169334374, 1.6033810510228224, 1.6040710283226176, 1.603438071037944, 1.6025276027485262, 1.602919783498266, 1.603458319391523, 1.6034200565372585, 1.603698486569284, 1.6034942246814472, 1.6036055579365571, 1.6038436958159523, 1.6038401743461346, 1.6037738019805432, 1.6039210732151525, 1.606861783365898, 1.6051044902582279, 1.6040858631259312, 1.6043042717700327, 1.6034783608416228, 1.6031024322916918, 1.6032383541755488, 1.6029386256128697, 1.6030155643453738, 1.6028836164959936, 1.6031848088469607, 1.6030341281092226, 1.6034471763570124, 1.6032210694158018, 1.6033305117649397, 1.6032019718527206, 1.603475677360259, 1.6040342715377682, 1.603893713802344, 1.604029670333236, 1.6038619459947734, 1.604469919831099, 1.6042749298421424, 1.603852425498524, 1.6040312586159542, 1.6040687825292201, 1.6046029125724128, 1.6044771534272995, 1.604422098114377, 1.6045006390275627, 1.6049919717613308, 1.6049477656682332, 1.605455199290183, 1.6054611491843789, 1.6048926821678926, 1.6048386598064004, 1.604982014360099, 1.6062379783793233, 1.6048027298524854, 1.6050517795904125, 1.6044696052673415, 1.6048448124934105, 1.6048851484931357, 1.6056594547184033, 1.6055742318211321, 1.6051182793866237, 1.606501758979459, 1.6045505348684752, 1.6055705181287818, 1.6041644285073617, 1.6049307263739199, 1.6031585039176377, 1.604302562907803, 1.6039767707705694, 1.6042127983127712, 1.6045060463139575, 1.603103342510405, 1.6061193406679752, 1.6063441605794997, 1.610593294470964, 1.6088168574084203, 1.60878551691428, 1.6105302564420527, 1.6082794901185435, 1.6085457431858983, 1.6053043171298524, 1.606352677094721, 1.6056918109383294, 1.6052081900081416, 1.6048379776317303, 1.60487497616284, 1.6050704932956665, 1.605614413964533, 1.6064087868911292, 1.6070449399243434, 1.6059770758320349, 1.6053403447610013, 1.6086767109352575, 1.607881969614765, 1.6081118360528805, 1.607835517336778, 1.6075324008030256, 1.6047049678605179, 1.6091182132072637, 1.6097762056172187, 1.6097145824401053, 1.6105934902169239, 1.610276670487252, 1.6102502899999884, 1.6102245069293946, 1.6097333987162423, 1.6097121434454456, 1.6100673802771983, 1.6095685042771213, 1.6094767340689848, 1.609081775292583, 1.6083161284770873, 1.6078298193676326, 1.6082702119557923, 1.6083613238702659, 1.6080346598805269, 1.6076832287417258, 1.607511048637979, 1.606419187265468, 1.6062526430793975, 1.6062669832326704, 1.6077916344202603, 1.606930117888991, 1.6070959023849913, 1.6066639787457846, 1.6065761678911783, 1.60780228476219, 1.6073420447081768, 1.607213518106683, 1.60679603777887, 1.6070257675863056, 1.6071848033488483, 1.6069101719629197, 1.6059638199156336, 1.6063635498040612, 1.6061988141149135, 1.6063124495382575, 1.6063708961499343, 1.606399918816164, 1.6063501928827446, 1.606298229964496, 1.6063860748788994, 1.6062531371422002, 1.607653733740495, 1.6066376540461198, 1.6058542360421668, 1.606410611439221, 1.6057687441899466, 1.6058672907317213, 1.605874439374175, 1.6061249765856513, 1.6059670691028214, 1.6063891112902287, 1.6058054960811472, 1.605925460558611, 1.6056679626208026, 1.6056164544204186, 1.6059672742445872, 1.6058032696665998, 1.6062601470007685, 1.6057742505237973, 1.606040993347544, 1.605827370300669, 1.606064879052549, 1.6055326986391165, 1.6061890289701264, 1.605633698073514, 1.606276722181411, 1.6054919480494483, 1.605458803364796, 1.6056559935383412, 1.6054843869702569, 1.6052376047535286, 1.605565061318659, 1.605699852378106, 1.604985909900446, 1.6049698379826662, 1.6051512473126741, 1.605552183583452, 1.6056048780043528, 1.605510598920249, 1.605619768595265, 1.6050477129680965, 1.6055357642165937, 1.6054516598117372, 1.6054724820924706, 1.6052723590572089, 1.60613948312299, 1.6053002955291071, 1.605958060286511, 1.6051443130120464, 1.6053645726299441, 1.6058132300040209, 1.6067545306310669, 1.6079509009672894, 1.6089700325369247, 1.6099475704390427, 1.6097056891139114, 1.609462646623746, 1.611319236175963, 1.6064197026645803, 1.6088437166903016, 1.6078537399154187, 1.6065091832322245, 1.60675015688334, 1.6071835550768623, 1.60940726735126, 1.6115189248509398, 1.6093042184566628, 1.6107492004513544, 1.6105599967129711, 1.6109951393944877, 1.610651882215478, 1.6097675099944442, 1.6105326333852432, 1.6099399193167099, 1.6102989450072616, 1.6102558037721857, 1.6101710152351993, 1.6108708659612094, 1.6107183275943124, 1.6112729224861158, 1.6127902328087191, 1.6117720543261624, 1.6105202835768901, 1.609264407447602, 1.608472657125376, 1.608219485956264, 1.6073343556111277, 1.60825341147155, 1.6080167737891913, 1.6081422690687508, 1.608177071721683, 1.6083996162821703, 1.608352844938269, 1.607847234494189, 1.6085807603764024, 1.608482150021445, 1.6085903127792434, 1.6084689434330255, 1.6089650858407734, 1.608434571616951, 1.6085207342905756, 1.6082828798513302, 1.609039906797738, 1.608288225477748, 1.609320030228062, 1.6086316613728189, 1.6080892839650998, 1.6072818823831618, 1.6096004038217229, 1.6081170425039206, 1.6066667763470428, 1.6071634736945868, 1.6065239076348166, 1.606927532672099, 1.6073499996282392, 1.6078489766332316, 1.6086376425863682, 1.6095360368734901, 1.6082590203762837, 1.6087722326147145, 1.6082252600705877, 1.6100388371885703, 1.609010934829712, 1.6087488973473485, 1.6088216324353648, 1.6173954197925886, 1.6137571303519513, 1.6101923304042598, 1.6048215375158, 1.6070175102387352, 1.606562667879565, 1.605157142789493, 1.605939966508712, 1.6052676077155252, 1.60605219667181, 1.6053654664059969, 1.6045793306651375, 1.6034933191606369], 'val_acc': [0.20853858713935358, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2348111652339425, 0.23645320135873724, 0.238095237385659, 0.23973727351045374, 0.2430213458579162, 0.2430213458579162, 0.22988505695743122, 0.2430213459557892, 0.24466338208058394, 0.24466338208058394, 0.24137930983099445, 0.23809523758140494, 0.24466338208058394, 0.23645320155448318, 0.2430213459557892, 0.2430213459557892, 0.2364532014566102, 0.24302134615153514, 0.23645320155448318, 0.2430213458579162, 0.24302134605366216, 0.23481116542968844, 0.2413793099288674, 0.24466338208058394, 0.23809523767927793, 0.23809523777715091, 0.224958948583047, 0.2364532014566102, 0.2364532014566102, 0.23645320155448318, 0.2413793099288674, 0.24302134615153514, 0.24302134605366216, 0.24302134615153514, 0.24302134615153514, 0.24302134615153514, 0.2413793100267404, 0.24302134615153514, 0.2413793099288674, 0.23973727370619968, 0.24302134615153514, 0.2430213459557892, 0.23809523777715091, 0.23809523777715091, 0.23809523777715091, 0.2430213459557892, 0.2430213459557892, 0.23809523777715091, 0.24137931022248635, 0.2430213458579162, 0.24466338198271095, 0.23973727390194566, 0.23973727390194566, 0.2430213458579162, 0.23809523767927793, 0.2413793101246134, 0.2430213458579162, 0.2430213458579162, 0.23809523767927793, 0.24137930973312147, 0.23809523767927793, 0.2151067318342785, 0.23973727360832672, 0.20689655140605073, 0.2380952378750239, 0.2413793099288674, 0.21346469610097568, 0.2364532014566102, 0.23973727351045374, 0.21839080398599503, 0.23973727390194566, 0.23973727360832672, 0.22660098460996875, 0.23645320165235617, 0.23973727351045374, 0.24630541820537868, 0.21182265958468902, 0.2413793100267404, 0.238095237385659, 0.2413793100267404, 0.24466338208058394, 0.23809523777715091, 0.2413793100267404, 0.23645320126086425, 0.23973727380407267, 0.2331691293048937, 0.23316912969638562, 0.2446633822763299, 0.22167487643133046, 0.24137931032035934, 0.24302134615153514, 0.24794745462379236, 0.2495894908464601, 0.23973727380407267, 0.24794745462379236, 0.24630541840112463, 0.24466338237420288, 0.24302134605366216, 0.2430213458579162, 0.24630541840112463, 0.24137931022248635, 0.24794745442804642, 0.24302134624940813, 0.2200328387160998, 0.24302134634728112, 0.23809523748353198, 0.23973727370619968, 0.24302134624940813, 0.22660098490358768, 0.23645320155448318, 0.23973727380407267, 0.23481116542968844, 0.23973727370619968, 0.23481116542968844, 0.21674876835056517, 0.22660098509933366, 0.23973727370619968, 0.21839080427961396, 0.21839080437748695, 0.22660098509933366, 0.22167487652920345, 0.21839080427961396, 0.22495894907241187, 0.22988505734892314, 0.2315270934737179, 0.2315270934737179, 0.22003284069802764, 0.211822659780435, 0.22660098470784173, 0.22660098470784173, 0.22003284069802764, 0.22660098470784173, 0.2200328404044087, 0.23481116354563358, 0.22988505517124935, 0.23645320184810212, 0.21839080229768612, 0.22331691067207035, 0.23316912969638562, 0.2430213459557892, 0.2397372741955646, 0.22495894907241187, 0.23481116354563358, 0.24137931032035934, 0.26272577964907207, 0.22331691076994334, 0.22988505517124935, 0.2545155992208443, 0.23481116582118036, 0.2315270912960441, 0.2430213459557892, 0.24137931032035934, 0.22824301904645458, 0.23316912969638562, 0.2298850552691223, 0.2430213459557892, 0.24302134634728112, 0.2298850552691223, 0.23481116582118036, 0.23481116354563358, 0.22988505517124935, 0.24630541840112463, 0.23481116354563358, 0.22824301914432757, 0.23481116582118036, 0.24630541840112463, 0.2298850552691223, 0.22988505517124935, 0.2315270933758449, 0.2380952378750239, 0.2266009831174058, 0.23481116354563358, 0.25451559882935243, 0.24630541840112463, 0.23809523807076985, 0.24302134605366216, 0.23973727399981865, 0.23152709139391706, 0.22495894907241187, 0.23645320175022916, 0.23645320175022916, 0.22988505725105016, 0.24302134624940813, 0.2446633821784569, 0.2380952378750239, 0.23809523579522307, 0.24302134605366216, 0.2446633822763299, 0.23809523807076985, 0.24302134624940813, 0.2380952378750239, 0.23809523797289686, 0.2446633822763299, 0.24302134615153514, 0.23809523807076985, 0.23645320184810212, 0.2413793099288674, 0.24302134615153514, 0.23645320184810212, 0.24302134615153514, 0.24137931022248635, 0.24794745452591938, 0.2413793100267404, 0.23973727399981865, 0.23645320184810212, 0.2463054185968706, 0.2380952378750239, 0.24630541849899762, 0.2397372740976916, 0.24137931022248635, 0.2446633822763299, 0.24302134615153514, 0.23645320175022916, 0.2413793100267404, 0.25123152687338185, 0.2545155992208443, 0.24958949065071412, 0.2463054185968706, 0.23809523807076985, 0.24302134624940813, 0.24466338247207586, 0.24630541869474357, 0.24466338256994882, 0.24466338256994882, 0.24137931032035934, 0.24137931032035934, 0.24630541869474357, 0.24794745452591938, 0.24466338256994882, 0.24466338256994882, 0.24466338256994882, 0.2463054164191968, 0.2397372740976916, 0.23481116354563358, 0.2364532019459751, 0.24794745462379236, 0.23316912950063964, 0.2364532019459751, 0.24630541869474357, 0.2413793100267404, 0.24302134644515408, 0.23645320184810212, 0.23973727380407267, 0.2315270934737179, 0.2348111656254344, 0.24794745452591938, 0.23481116572330737, 0.22824301904645458, 0.24630541849899762, 0.23973727390194566, 0.22660098509933366, 0.22660098292165984, 0.2315270933758449, 0.2348111656254344, 0.24302134644515408, 0.22331691275187118, 0.22988505725105016, 0.23973727380407267, 0.22495894877879294, 0.22660098500146067, 0.23152709308222597, 0.2348111652339425, 0.2282430212241284, 0.23645320165235617, 0.2315270933758449, 0.23481116572330737, 0.2315270934737179, 0.22988505725105016, 0.23645320175022916, 0.22660098509933366, 0.23316912950063964, 0.22660098509933366, 0.2282430212241284, 0.23316912940276668, 0.23316912950063964, 0.2348111656254344, 0.2249589489745389, 0.22660098509933366, 0.21346469610097568, 0.22660098509933366, 0.2561576351498931, 0.24794745462379236, 0.2446633821784569, 0.24466338208058394, 0.2528735627045577, 0.2364532014566102, 0.2512315265797629, 0.24794745452591938, 0.2561576351498931, 0.2561576350520201, 0.2495894907485871], 'loss': [1.612500792851928, 1.608718771366613, 1.6069583537397443, 1.6056326825515934, 1.6045577406638456, 1.60413066420467, 1.6037025616643854, 1.6028452079643705, 1.602482138277324, 1.6019127771839714, 1.6016121040869054, 1.60108949952057, 1.6009950913442967, 1.6006412074306418, 1.600666468833751, 1.600918631583024, 1.6009103822512303, 1.600254068384425, 1.6001554894006718, 1.6000262267290934, 1.6008355843702626, 1.6011095577686474, 1.6012941438804171, 1.600615248591993, 1.6000302545343825, 1.5995393652690755, 1.5998965214410112, 1.600188102908203, 1.5992966212775919, 1.5997221998610291, 1.5995669434447553, 1.5989776142813588, 1.5991818471121348, 1.5992560776352147, 1.5988098544996132, 1.5993700637954462, 1.6011695924970404, 1.599366942617193, 1.5993771903568714, 1.5991862346014694, 1.5989367077727583, 1.5986595869553897, 1.5985299264136281, 1.5984308252099604, 1.598296222549689, 1.597993807185602, 1.5980032642274422, 1.597859939363703, 1.5980088666723984, 1.597417567691764, 1.597640413767993, 1.597246154182011, 1.5973134032999465, 1.5971799049534103, 1.597074816848708, 1.5971079583040744, 1.5963041031385103, 1.5971645766949507, 1.5965403538709793, 1.5969126008129708, 1.5963252245767896, 1.5962504432921047, 1.5958450173450447, 1.5958536005118056, 1.5955381678359954, 1.5955290237736164, 1.596154030980026, 1.595490698011504, 1.5955714130303698, 1.595071173007973, 1.5953195653656915, 1.5949031186054865, 1.595878939315279, 1.5971262870627996, 1.5955204041341982, 1.5975562099558618, 1.5953329317868368, 1.5946006296596489, 1.5938642493018869, 1.5940545334218708, 1.5949219525961906, 1.5956887291197415, 1.5968238126081118, 1.5974195540073717, 1.594816251555018, 1.5969575751733487, 1.5951625496944608, 1.5951230581291402, 1.5963210165133466, 1.5950645749818617, 1.5949992016845171, 1.5944600948318075, 1.595507379038378, 1.5958355478682313, 1.5989862087570912, 1.5995543053997126, 1.6014125894961662, 1.6009036118000195, 1.5989052196302942, 1.5961870862228424, 1.59630004336457, 1.5957832332019688, 1.5960573377550504, 1.5951353171522857, 1.5949403943956755, 1.5949464191891085, 1.5940348265841757, 1.5949097249542175, 1.59412764895868, 1.5945790551771128, 1.5956074328393173, 1.5949944098627298, 1.5934755225445945, 1.5959364051446778, 1.5960309871168352, 1.5960102515054189, 1.5956407104919088, 1.595486590162195, 1.5959405928421804, 1.596105230711324, 1.5952626017819196, 1.5953182737440543, 1.5951381940861258, 1.595394058100252, 1.5956216925223505, 1.595589444818438, 1.5954922132178744, 1.595623614606916, 1.5953588736865065, 1.5948157935661458, 1.5951890666871589, 1.5944951818709012, 1.593902546277526, 1.593496630275029, 1.5936096488572733, 1.5937533309572287, 1.5940843287924231, 1.5936422136530004, 1.5931784441338916, 1.5931042352496232, 1.5934781357982566, 1.5941680727553318, 1.59281251322807, 1.594007732589142, 1.593132927089746, 1.5935694708226886, 1.593098615718818, 1.5927579781358001, 1.5930997918518661, 1.593549605169825, 1.593207102977275, 1.5938168578079348, 1.593313877930142, 1.5943839556872232, 1.5930015388700263, 1.5930771092859382, 1.592757804682612, 1.592859860465267, 1.5928092844432384, 1.5926528803866502, 1.593663880076007, 1.5930757469709893, 1.592767274942731, 1.593795815434544, 1.592425291445221, 1.5940769597000655, 1.5923486802122677, 1.5934199657283525, 1.5921045883969849, 1.5926147391908712, 1.5926555122438153, 1.5921726632167181, 1.592388294169056, 1.5927065150693702, 1.592117688200557, 1.5924619116577525, 1.5923173495141878, 1.592738600188457, 1.5924558033443819, 1.5923850896177352, 1.5928040162738588, 1.5909389570263621, 1.5926661134500524, 1.5920153914046238, 1.5916852072279066, 1.591956936994862, 1.5919087112317094, 1.5923524314128399, 1.5919665154490383, 1.5917580232972726, 1.5913945419342856, 1.5915118477917305, 1.5916339210905823, 1.5924367316694474, 1.5913404713421142, 1.5913821929778895, 1.5913016299202702, 1.5910533694515971, 1.591836538305028, 1.5911254140141073, 1.5906714383581582, 1.5910950645529025, 1.5910068283825194, 1.5905831475522239, 1.5909615406999842, 1.5901350319018355, 1.5908053417225394, 1.5908182407061913, 1.590832251934545, 1.5903046686301732, 1.5906784637752744, 1.5901999891171466, 1.59091887302712, 1.5911492781962213, 1.590703999971707, 1.589739976379661, 1.5907733839395355, 1.5900232529493328, 1.5908001040531135, 1.590238362416105, 1.5917128071892677, 1.5917878524968267, 1.592041142471517, 1.592175163037968, 1.5911298226526875, 1.5923073657973836, 1.5891994030813417, 1.5900628527576675, 1.5891193199451454, 1.5900356753406093, 1.5894415115673683, 1.5900522230586966, 1.5904843767076058, 1.5912388834375621, 1.5909666468720172, 1.5902228792590039, 1.5904121608459973, 1.590541298531409, 1.589988998368046, 1.5899829667206906, 1.5897781071476869, 1.5900554224206191, 1.5898958697700891, 1.5898375362341408, 1.5894501161771144, 1.5897497017035984, 1.5890650556807155, 1.5892000844101641, 1.5898159258664266, 1.5897377423437224, 1.5902116185096256, 1.5897049075524174, 1.5905588601404146, 1.5894525202392797, 1.5900668446777781, 1.5889368876784244, 1.5896925273127624, 1.5886256835788672, 1.589121227685431, 1.5887188961373706, 1.5884492887363786, 1.5883945508659254, 1.5891835737522133, 1.5884646468583563, 1.5893698310949964, 1.5899583555589711, 1.5887093738853564, 1.5904076209548073, 1.5888799878850854, 1.5897073124468448, 1.5887032606763272, 1.5883303619263354, 1.588262126235257, 1.5879435097167625, 1.5872999040987457, 1.5880215996344722, 1.5878040191329235, 1.5877194014418052, 1.5881679492319878, 1.587928776868315, 1.588182966909859, 1.5880937548878256, 1.588227559359901, 1.5892983622129937, 1.5892273769241583, 1.587744719928295, 1.587635324378278, 1.5925634849242851, 1.6002110593372791, 1.5963352851064787, 1.593394164529914, 1.5941584740331285, 1.5927725427204578, 1.5907031078358205, 1.589826560950622, 1.589878711220665, 1.5893657437095408, 1.5888767145987164, 1.5882414629816763, 1.58861023292894], 'acc': [0.19301847979518177, 0.21067761715799876, 0.23285420969036816, 0.23285421047367355, 0.23285421029620593, 0.23285420851541005, 0.2328542091028891, 0.23285420970872686, 0.23285421010037957, 0.2357289524293778, 0.23819301774614401, 0.2414784389048876, 0.24312115131707163, 0.2435318279560097, 0.2443531818213649, 0.24394250537825315, 0.24435318338797568, 0.24312115033793988, 0.2435318267810516, 0.24517453784080992, 0.2381930181377967, 0.23983572802259692, 0.24312115094377765, 0.24599589108196862, 0.24804928097147227, 0.24599589127779498, 0.24476386082857787, 0.2488706365992646, 0.24517453686167817, 0.24517453764498356, 0.24681725027135265, 0.244353182623029, 0.24517453883830037, 0.24558521585053242, 0.24558521545887974, 0.241889116112946, 0.2312114992180889, 0.2431211499462872, 0.24435318321050806, 0.24681724792143647, 0.24763860413670785, 0.2472279267144644, 0.24887063640343823, 0.24887063816587537, 0.24887063681344967, 0.2464065708908457, 0.2472279261637028, 0.2484599589995535, 0.24887063480011, 0.24845995919537986, 0.24476385963526104, 0.24722792651863804, 0.24558521528141208, 0.24722792534367993, 0.24722792691029072, 0.24558521565470606, 0.250924024512146, 0.24763860550749228, 0.24887063797004905, 0.24517453785916862, 0.24394250655321126, 0.2468172475297838, 0.246817249506406, 0.2451745382324626, 0.24845996015615288, 0.24558521467557434, 0.24229979353518946, 0.24558521626054383, 0.24229979492433262, 0.24887063797004905, 0.24887063679509094, 0.24476386063275152, 0.23983572880590232, 0.23778233993224784, 0.24394250459494776, 0.23572895458346765, 0.2349075972115969, 0.2451745382324626, 0.2439425051824268, 0.24599589483938666, 0.24763860257009707, 0.24517453705750453, 0.24024640544484038, 0.24394250617991728, 0.24435318319214933, 0.24476386002691375, 0.241478440648966, 0.2447638614160569, 0.2476386043325342, 0.24271047312988148, 0.24394250598409092, 0.24804928273390942, 0.24435318162553854, 0.2451745384466477, 0.24517453803663625, 0.24065708462952098, 0.2439425059657322, 0.23244353128899295, 0.23572895323104193, 0.24763860415506656, 0.24599589444773398, 0.24435318299632297, 0.24353182893514144, 0.24312114935880813, 0.24435318221301758, 0.24106776207012318, 0.24476386120187182, 0.23819301794197034, 0.23983572843260834, 0.2418891176979155, 0.2410677626576022, 0.2431211517087243, 0.24845995839371574, 0.23860369516838748, 0.24312115055212494, 0.24353182797436843, 0.24353182660358397, 0.24928131441316076, 0.23696098606689264, 0.24804928116729863, 0.24763860374505514, 0.24640657245745648, 0.24845995819788938, 0.246817249506406, 0.2480492835172148, 0.24558521526305338, 0.24763860550749228, 0.2447638610244042, 0.24188911748373043, 0.2455852160647175, 0.24188911826703582, 0.24229979588510564, 0.24188911846286218, 0.24024640462481756, 0.2427104733073491, 0.24229979666841103, 0.23901437298228365, 0.24024640603231942, 0.2418891147054441, 0.24558521506722703, 0.24476386159352453, 0.23942505081453852, 0.24435318340633438, 0.24024640642397213, 0.24312114937716686, 0.23901437415724172, 0.2377823409297383, 0.24353182658522526, 0.24394250617991728, 0.24887063836170173, 0.24804928136312496, 0.24928131384404043, 0.24681724733395743, 0.24147843927818158, 0.25092402470797237, 0.24599589307694955, 0.2488706362076119, 0.24722792573533264, 0.24722792573533264, 0.24435318338797568, 0.25092402488544, 0.2505133476590229, 0.25010266945347404, 0.2472279267144644, 0.25133470252186857, 0.24599589503521302, 0.25010267043260576, 0.2427104733073491, 0.24845995996032652, 0.24969199242288326, 0.25092402586457174, 0.24804928018816688, 0.2501026696493004, 0.24845995937284748, 0.250924024512146, 0.2517453811007114, 0.2509240254912778, 0.2509240249037987, 0.2509240249221575, 0.24969199105209883, 0.2476386043325342, 0.24969199301036232, 0.2533880909855116, 0.2501026706284321, 0.24599589307694955, 0.25010266947183274, 0.25338809041639127, 0.24763860531166593, 0.2472279267144644, 0.2509240239063083, 0.2529774129757891, 0.2443531843671074, 0.2521560573479968, 0.25092402371048195, 0.2488706362076119, 0.25051334746319653, 0.25174538012157965, 0.24722792653699674, 0.24804928018816688, 0.2517453787507952, 0.2533880896147272, 0.253388090808044, 0.25256673555354564, 0.25133470173856315, 0.2554414782925553, 0.2501026692576477, 0.25092402371048195, 0.25092402470797237, 0.2537987686219401, 0.251745379944112, 0.252977411800831, 0.25215605795383456, 0.24804928116729863, 0.2492813144498782, 0.25092402527709273, 0.2513347015243781, 0.252977411800831, 0.25831622103156493, 0.25379876758773223, 0.2525667357677307, 0.2546201224689366, 0.2505133464840648, 0.2505133460924121, 0.2533880887947043, 0.2574948667745571, 0.2579055436093215, 0.2574948677536888, 0.2554414795042308, 0.25626283212119305, 0.25749486738039484, 0.25503080067448547, 0.2570841877857028, 0.25544147811508766, 0.2562628333512273, 0.2505133480690343, 0.2517453807090587, 0.2558521559106251, 0.2570841883364644, 0.2533880902022062, 0.25503079969535375, 0.2529774129757891, 0.25133470132855174, 0.25379876782027605, 0.24845995878536845, 0.24969199105209883, 0.2648870658580772, 0.25626283450782666, 0.24845995819788938, 0.2566735117526025, 0.257905545567585, 0.24928131382568172, 0.251745380317406, 0.2476386031575761, 0.2529774121924837, 0.24804928097147227, 0.2521560573663555, 0.2443531830146817, 0.2517453787507952, 0.2537987680161024, 0.2492813150006398, 0.2455852166338378, 0.24558521487140067, 0.24517453725333085, 0.24763860474254562, 0.24763860276592342, 0.2386036965391719, 0.2521560587187812, 0.24353182893514144, 0.2472279267144644, 0.24435318360216074, 0.24558521467557434, 0.2455852166338378, 0.24435318201719122, 0.24271047293405512, 0.24353182660358397, 0.24394250559243824, 0.24804928196896273, 0.24640657108667205, 0.24722792710611707, 0.2472279251478536, 0.24599589127779498, 0.24147843849487619, 0.24271047213239103, 0.2443531818213649, 0.24969199185376295, 0.2476386043508929, 0.2464065720658038, 0.24969199163957786, 0.2406570842378683, 0.25256673457441386, 0.2468172475297838, 0.24188911787538314, 0.24271047312988148, 0.2558521566939305, 0.2443531830146817, 0.2517453787507952, 0.2509240256687454, 0.24681724772561012, 0.25174538051323236, 0.2554414787025667]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
