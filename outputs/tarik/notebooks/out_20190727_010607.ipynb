{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf22.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 01:06:08 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '2Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ib', 'sg', 'ce', 'aa', 'eo', 'my', 'mb', 'yd', 'eg', 'ek', 'sk', 'ds', 'ck', 'by', 'eb'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001BF015FD2B0>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001BF4F367EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7105, Accuracy:0.0637, Validation Loss:2.7066, Validation Accuracy:0.0805\n",
    "Epoch #2: Loss:2.7038, Accuracy:0.0821, Validation Loss:2.6994, Validation Accuracy:0.0952\n",
    "Epoch #3: Loss:2.6974, Accuracy:0.1125, Validation Loss:2.6944, Validation Accuracy:0.1215\n",
    "Epoch #4: Loss:2.6922, Accuracy:0.1138, Validation Loss:2.6886, Validation Accuracy:0.1100\n",
    "Epoch #5: Loss:2.6861, Accuracy:0.1092, Validation Loss:2.6832, Validation Accuracy:0.1084\n",
    "Epoch #6: Loss:2.6808, Accuracy:0.1076, Validation Loss:2.6772, Validation Accuracy:0.1034\n",
    "Epoch #7: Loss:2.6753, Accuracy:0.1051, Validation Loss:2.6714, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6693, Accuracy:0.1023, Validation Loss:2.6656, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6637, Accuracy:0.1055, Validation Loss:2.6594, Validation Accuracy:0.1166\n",
    "Epoch #10: Loss:2.6575, Accuracy:0.1142, Validation Loss:2.6524, Validation Accuracy:0.1199\n",
    "Epoch #11: Loss:2.6509, Accuracy:0.1228, Validation Loss:2.6437, Validation Accuracy:0.1363\n",
    "Epoch #12: Loss:2.6419, Accuracy:0.1409, Validation Loss:2.6335, Validation Accuracy:0.1445\n",
    "Epoch #13: Loss:2.6307, Accuracy:0.1487, Validation Loss:2.6223, Validation Accuracy:0.1511\n",
    "Epoch #14: Loss:2.6192, Accuracy:0.1503, Validation Loss:2.6093, Validation Accuracy:0.1494\n",
    "Epoch #15: Loss:2.6064, Accuracy:0.1544, Validation Loss:2.5973, Validation Accuracy:0.1527\n",
    "Epoch #16: Loss:2.5947, Accuracy:0.1552, Validation Loss:2.5793, Validation Accuracy:0.1626\n",
    "Epoch #17: Loss:2.5779, Accuracy:0.1577, Validation Loss:2.5618, Validation Accuracy:0.1642\n",
    "Epoch #18: Loss:2.5600, Accuracy:0.1606, Validation Loss:2.5453, Validation Accuracy:0.1708\n",
    "Epoch #19: Loss:2.5421, Accuracy:0.1667, Validation Loss:2.5236, Validation Accuracy:0.1658\n",
    "Epoch #20: Loss:2.5286, Accuracy:0.1659, Validation Loss:2.5067, Validation Accuracy:0.1724\n",
    "Epoch #21: Loss:2.5172, Accuracy:0.1663, Validation Loss:2.4949, Validation Accuracy:0.1675\n",
    "Epoch #22: Loss:2.5086, Accuracy:0.1692, Validation Loss:2.4837, Validation Accuracy:0.1724\n",
    "Epoch #23: Loss:2.4976, Accuracy:0.1729, Validation Loss:2.4736, Validation Accuracy:0.1642\n",
    "Epoch #24: Loss:2.5023, Accuracy:0.1692, Validation Loss:2.4750, Validation Accuracy:0.1675\n",
    "Epoch #25: Loss:2.4951, Accuracy:0.1692, Validation Loss:2.4674, Validation Accuracy:0.1609\n",
    "Epoch #26: Loss:2.4884, Accuracy:0.1741, Validation Loss:2.4595, Validation Accuracy:0.1560\n",
    "Epoch #27: Loss:2.4877, Accuracy:0.1795, Validation Loss:2.4649, Validation Accuracy:0.1741\n",
    "Epoch #28: Loss:2.4832, Accuracy:0.1754, Validation Loss:2.4545, Validation Accuracy:0.1691\n",
    "Epoch #29: Loss:2.4766, Accuracy:0.1795, Validation Loss:2.4501, Validation Accuracy:0.1675\n",
    "Epoch #30: Loss:2.4747, Accuracy:0.1778, Validation Loss:2.4510, Validation Accuracy:0.1708\n",
    "Epoch #31: Loss:2.4736, Accuracy:0.1815, Validation Loss:2.4461, Validation Accuracy:0.1691\n",
    "Epoch #32: Loss:2.4691, Accuracy:0.1815, Validation Loss:2.4427, Validation Accuracy:0.1708\n",
    "Epoch #33: Loss:2.4681, Accuracy:0.1828, Validation Loss:2.4447, Validation Accuracy:0.1724\n",
    "Epoch #34: Loss:2.4685, Accuracy:0.1741, Validation Loss:2.4435, Validation Accuracy:0.1675\n",
    "Epoch #35: Loss:2.5522, Accuracy:0.1532, Validation Loss:2.4949, Validation Accuracy:0.1560\n",
    "Epoch #36: Loss:2.5165, Accuracy:0.1647, Validation Loss:2.4964, Validation Accuracy:0.1609\n",
    "Epoch #37: Loss:2.4914, Accuracy:0.1676, Validation Loss:2.4649, Validation Accuracy:0.1626\n",
    "Epoch #38: Loss:2.5253, Accuracy:0.1483, Validation Loss:2.4516, Validation Accuracy:0.1658\n",
    "Epoch #39: Loss:2.4792, Accuracy:0.1737, Validation Loss:2.4603, Validation Accuracy:0.1724\n",
    "Epoch #40: Loss:2.4769, Accuracy:0.1786, Validation Loss:2.4456, Validation Accuracy:0.1691\n",
    "Epoch #41: Loss:2.4683, Accuracy:0.1754, Validation Loss:2.4473, Validation Accuracy:0.1708\n",
    "Epoch #42: Loss:2.4656, Accuracy:0.1762, Validation Loss:2.4420, Validation Accuracy:0.1741\n",
    "Epoch #43: Loss:2.4653, Accuracy:0.1791, Validation Loss:2.4430, Validation Accuracy:0.1691\n",
    "Epoch #44: Loss:2.4629, Accuracy:0.1770, Validation Loss:2.4387, Validation Accuracy:0.1658\n",
    "Epoch #45: Loss:2.4619, Accuracy:0.1819, Validation Loss:2.4381, Validation Accuracy:0.1642\n",
    "Epoch #46: Loss:2.4586, Accuracy:0.1786, Validation Loss:2.4307, Validation Accuracy:0.1741\n",
    "Epoch #47: Loss:2.4581, Accuracy:0.1803, Validation Loss:2.4337, Validation Accuracy:0.1741\n",
    "Epoch #48: Loss:2.4588, Accuracy:0.1832, Validation Loss:2.4345, Validation Accuracy:0.1691\n",
    "Epoch #49: Loss:2.4541, Accuracy:0.1832, Validation Loss:2.4300, Validation Accuracy:0.1773\n",
    "Epoch #50: Loss:2.4544, Accuracy:0.1823, Validation Loss:2.4299, Validation Accuracy:0.1856\n",
    "Epoch #51: Loss:2.4538, Accuracy:0.1807, Validation Loss:2.4323, Validation Accuracy:0.1741\n",
    "Epoch #52: Loss:2.4547, Accuracy:0.1795, Validation Loss:2.4313, Validation Accuracy:0.1724\n",
    "Epoch #53: Loss:2.4546, Accuracy:0.1762, Validation Loss:2.4303, Validation Accuracy:0.1741\n",
    "Epoch #54: Loss:2.4536, Accuracy:0.1762, Validation Loss:2.4281, Validation Accuracy:0.1741\n",
    "Epoch #55: Loss:2.4538, Accuracy:0.1774, Validation Loss:2.4290, Validation Accuracy:0.1757\n",
    "Epoch #56: Loss:2.4525, Accuracy:0.1766, Validation Loss:2.4272, Validation Accuracy:0.1708\n",
    "Epoch #57: Loss:2.4517, Accuracy:0.1762, Validation Loss:2.4271, Validation Accuracy:0.1724\n",
    "Epoch #58: Loss:2.4506, Accuracy:0.1795, Validation Loss:2.4248, Validation Accuracy:0.1773\n",
    "Epoch #59: Loss:2.4500, Accuracy:0.1766, Validation Loss:2.4237, Validation Accuracy:0.1741\n",
    "Epoch #60: Loss:2.4489, Accuracy:0.1762, Validation Loss:2.4252, Validation Accuracy:0.1708\n",
    "Epoch #61: Loss:2.4489, Accuracy:0.1807, Validation Loss:2.4250, Validation Accuracy:0.1724\n",
    "Epoch #62: Loss:2.4512, Accuracy:0.1791, Validation Loss:2.4269, Validation Accuracy:0.1806\n",
    "Epoch #63: Loss:2.4510, Accuracy:0.1795, Validation Loss:2.4263, Validation Accuracy:0.1823\n",
    "Epoch #64: Loss:2.4500, Accuracy:0.1844, Validation Loss:2.4262, Validation Accuracy:0.1823\n",
    "Epoch #65: Loss:2.4496, Accuracy:0.1803, Validation Loss:2.4267, Validation Accuracy:0.1757\n",
    "Epoch #66: Loss:2.4500, Accuracy:0.1836, Validation Loss:2.4247, Validation Accuracy:0.1741\n",
    "Epoch #67: Loss:2.4501, Accuracy:0.1823, Validation Loss:2.4261, Validation Accuracy:0.1757\n",
    "Epoch #68: Loss:2.4477, Accuracy:0.1811, Validation Loss:2.4251, Validation Accuracy:0.1757\n",
    "Epoch #69: Loss:2.4473, Accuracy:0.1828, Validation Loss:2.4237, Validation Accuracy:0.1823\n",
    "Epoch #70: Loss:2.4463, Accuracy:0.1844, Validation Loss:2.4259, Validation Accuracy:0.1790\n",
    "Epoch #71: Loss:2.4459, Accuracy:0.1819, Validation Loss:2.4248, Validation Accuracy:0.1773\n",
    "Epoch #72: Loss:2.4457, Accuracy:0.1791, Validation Loss:2.4248, Validation Accuracy:0.1741\n",
    "Epoch #73: Loss:2.4478, Accuracy:0.1799, Validation Loss:2.4238, Validation Accuracy:0.1708\n",
    "Epoch #74: Loss:2.4455, Accuracy:0.1807, Validation Loss:2.4261, Validation Accuracy:0.1724\n",
    "Epoch #75: Loss:2.4460, Accuracy:0.1811, Validation Loss:2.4263, Validation Accuracy:0.1609\n",
    "Epoch #76: Loss:2.4479, Accuracy:0.1819, Validation Loss:2.4245, Validation Accuracy:0.1691\n",
    "Epoch #77: Loss:2.4456, Accuracy:0.1786, Validation Loss:2.4250, Validation Accuracy:0.1724\n",
    "Epoch #78: Loss:2.4449, Accuracy:0.1803, Validation Loss:2.4238, Validation Accuracy:0.1626\n",
    "Epoch #79: Loss:2.4452, Accuracy:0.1799, Validation Loss:2.4257, Validation Accuracy:0.1609\n",
    "Epoch #80: Loss:2.4438, Accuracy:0.1782, Validation Loss:2.4258, Validation Accuracy:0.1593\n",
    "Epoch #81: Loss:2.4443, Accuracy:0.1778, Validation Loss:2.4248, Validation Accuracy:0.1642\n",
    "Epoch #82: Loss:2.4448, Accuracy:0.1803, Validation Loss:2.4280, Validation Accuracy:0.1642\n",
    "Epoch #83: Loss:2.4434, Accuracy:0.1795, Validation Loss:2.4273, Validation Accuracy:0.1576\n",
    "Epoch #84: Loss:2.4427, Accuracy:0.1737, Validation Loss:2.4262, Validation Accuracy:0.1576\n",
    "Epoch #85: Loss:2.4444, Accuracy:0.1733, Validation Loss:2.4223, Validation Accuracy:0.1609\n",
    "Epoch #86: Loss:2.4421, Accuracy:0.1754, Validation Loss:2.4205, Validation Accuracy:0.1626\n",
    "Epoch #87: Loss:2.4409, Accuracy:0.1795, Validation Loss:2.4213, Validation Accuracy:0.1691\n",
    "Epoch #88: Loss:2.4407, Accuracy:0.1807, Validation Loss:2.4224, Validation Accuracy:0.1675\n",
    "Epoch #89: Loss:2.4406, Accuracy:0.1836, Validation Loss:2.4202, Validation Accuracy:0.1642\n",
    "Epoch #90: Loss:2.4403, Accuracy:0.1815, Validation Loss:2.4218, Validation Accuracy:0.1642\n",
    "Epoch #91: Loss:2.4409, Accuracy:0.1803, Validation Loss:2.4213, Validation Accuracy:0.1642\n",
    "Epoch #92: Loss:2.4403, Accuracy:0.1819, Validation Loss:2.4214, Validation Accuracy:0.1593\n",
    "Epoch #93: Loss:2.4403, Accuracy:0.1828, Validation Loss:2.4206, Validation Accuracy:0.1642\n",
    "Epoch #94: Loss:2.4396, Accuracy:0.1778, Validation Loss:2.4211, Validation Accuracy:0.1658\n",
    "Epoch #95: Loss:2.4410, Accuracy:0.1811, Validation Loss:2.4202, Validation Accuracy:0.1724\n",
    "Epoch #96: Loss:2.4412, Accuracy:0.1852, Validation Loss:2.4197, Validation Accuracy:0.1741\n",
    "Epoch #97: Loss:2.4418, Accuracy:0.1811, Validation Loss:2.4217, Validation Accuracy:0.1658\n",
    "Epoch #98: Loss:2.4444, Accuracy:0.1803, Validation Loss:2.4227, Validation Accuracy:0.1757\n",
    "Epoch #99: Loss:2.4438, Accuracy:0.1840, Validation Loss:2.4232, Validation Accuracy:0.1724\n",
    "Epoch #100: Loss:2.4429, Accuracy:0.1819, Validation Loss:2.4229, Validation Accuracy:0.1658\n",
    "Epoch #101: Loss:2.4447, Accuracy:0.1807, Validation Loss:2.4219, Validation Accuracy:0.1724\n",
    "Epoch #102: Loss:2.4431, Accuracy:0.1811, Validation Loss:2.4223, Validation Accuracy:0.1576\n",
    "Epoch #103: Loss:2.4428, Accuracy:0.1807, Validation Loss:2.4208, Validation Accuracy:0.1576\n",
    "Epoch #104: Loss:2.4440, Accuracy:0.1807, Validation Loss:2.4215, Validation Accuracy:0.1626\n",
    "Epoch #105: Loss:2.4460, Accuracy:0.1811, Validation Loss:2.4230, Validation Accuracy:0.1642\n",
    "Epoch #106: Loss:2.4441, Accuracy:0.1823, Validation Loss:2.4233, Validation Accuracy:0.1642\n",
    "Epoch #107: Loss:2.4444, Accuracy:0.1828, Validation Loss:2.4222, Validation Accuracy:0.1609\n",
    "Epoch #108: Loss:2.4436, Accuracy:0.1828, Validation Loss:2.4246, Validation Accuracy:0.1593\n",
    "Epoch #109: Loss:2.4433, Accuracy:0.1828, Validation Loss:2.4252, Validation Accuracy:0.1609\n",
    "Epoch #110: Loss:2.4439, Accuracy:0.1840, Validation Loss:2.4245, Validation Accuracy:0.1609\n",
    "Epoch #111: Loss:2.4441, Accuracy:0.1819, Validation Loss:2.4239, Validation Accuracy:0.1642\n",
    "Epoch #112: Loss:2.4435, Accuracy:0.1811, Validation Loss:2.4229, Validation Accuracy:0.1642\n",
    "Epoch #113: Loss:2.4438, Accuracy:0.1840, Validation Loss:2.4231, Validation Accuracy:0.1658\n",
    "Epoch #114: Loss:2.4448, Accuracy:0.1840, Validation Loss:2.4259, Validation Accuracy:0.1658\n",
    "Epoch #115: Loss:2.4449, Accuracy:0.1836, Validation Loss:2.4265, Validation Accuracy:0.1658\n",
    "Epoch #116: Loss:2.4452, Accuracy:0.1828, Validation Loss:2.4250, Validation Accuracy:0.1708\n",
    "Epoch #117: Loss:2.4435, Accuracy:0.1815, Validation Loss:2.4252, Validation Accuracy:0.1708\n",
    "Epoch #118: Loss:2.4431, Accuracy:0.1811, Validation Loss:2.4245, Validation Accuracy:0.1658\n",
    "Epoch #119: Loss:2.4456, Accuracy:0.1828, Validation Loss:2.4233, Validation Accuracy:0.1724\n",
    "Epoch #120: Loss:2.4431, Accuracy:0.1832, Validation Loss:2.4215, Validation Accuracy:0.1724\n",
    "Epoch #121: Loss:2.4434, Accuracy:0.1819, Validation Loss:2.4236, Validation Accuracy:0.1724\n",
    "Epoch #122: Loss:2.4443, Accuracy:0.1832, Validation Loss:2.4228, Validation Accuracy:0.1708\n",
    "Epoch #123: Loss:2.4454, Accuracy:0.1819, Validation Loss:2.4249, Validation Accuracy:0.1724\n",
    "Epoch #124: Loss:2.4455, Accuracy:0.1823, Validation Loss:2.4253, Validation Accuracy:0.1675\n",
    "Epoch #125: Loss:2.4465, Accuracy:0.1811, Validation Loss:2.4251, Validation Accuracy:0.1658\n",
    "Epoch #126: Loss:2.4458, Accuracy:0.1815, Validation Loss:2.4225, Validation Accuracy:0.1609\n",
    "Epoch #127: Loss:2.4442, Accuracy:0.1832, Validation Loss:2.4241, Validation Accuracy:0.1675\n",
    "Epoch #128: Loss:2.4474, Accuracy:0.1807, Validation Loss:2.4251, Validation Accuracy:0.1626\n",
    "Epoch #129: Loss:2.4458, Accuracy:0.1807, Validation Loss:2.4283, Validation Accuracy:0.1642\n",
    "Epoch #130: Loss:2.4454, Accuracy:0.1803, Validation Loss:2.4271, Validation Accuracy:0.1626\n",
    "Epoch #131: Loss:2.4455, Accuracy:0.1815, Validation Loss:2.4264, Validation Accuracy:0.1593\n",
    "Epoch #132: Loss:2.4439, Accuracy:0.1828, Validation Loss:2.4218, Validation Accuracy:0.1658\n",
    "Epoch #133: Loss:2.4432, Accuracy:0.1811, Validation Loss:2.4224, Validation Accuracy:0.1675\n",
    "Epoch #134: Loss:2.4432, Accuracy:0.1803, Validation Loss:2.4210, Validation Accuracy:0.1708\n",
    "Epoch #135: Loss:2.4437, Accuracy:0.1758, Validation Loss:2.4200, Validation Accuracy:0.1675\n",
    "Epoch #136: Loss:2.4419, Accuracy:0.1778, Validation Loss:2.4226, Validation Accuracy:0.1724\n",
    "Epoch #137: Loss:2.4410, Accuracy:0.1795, Validation Loss:2.4224, Validation Accuracy:0.1757\n",
    "Epoch #138: Loss:2.4423, Accuracy:0.1811, Validation Loss:2.4272, Validation Accuracy:0.1675\n",
    "Epoch #139: Loss:2.4545, Accuracy:0.1819, Validation Loss:2.4293, Validation Accuracy:0.1741\n",
    "Epoch #140: Loss:2.4624, Accuracy:0.1733, Validation Loss:2.4330, Validation Accuracy:0.1609\n",
    "Epoch #141: Loss:2.4513, Accuracy:0.1807, Validation Loss:2.4280, Validation Accuracy:0.1691\n",
    "Epoch #142: Loss:2.4425, Accuracy:0.1799, Validation Loss:2.4245, Validation Accuracy:0.1708\n",
    "Epoch #143: Loss:2.4388, Accuracy:0.1795, Validation Loss:2.4215, Validation Accuracy:0.1691\n",
    "Epoch #144: Loss:2.4387, Accuracy:0.1799, Validation Loss:2.4185, Validation Accuracy:0.1708\n",
    "Epoch #145: Loss:2.4411, Accuracy:0.1782, Validation Loss:2.4178, Validation Accuracy:0.1773\n",
    "Epoch #146: Loss:2.4407, Accuracy:0.1795, Validation Loss:2.4194, Validation Accuracy:0.1757\n",
    "Epoch #147: Loss:2.4435, Accuracy:0.1778, Validation Loss:2.4210, Validation Accuracy:0.1773\n",
    "Epoch #148: Loss:2.4432, Accuracy:0.1791, Validation Loss:2.4189, Validation Accuracy:0.1773\n",
    "Epoch #149: Loss:2.4409, Accuracy:0.1828, Validation Loss:2.4164, Validation Accuracy:0.1773\n",
    "Epoch #150: Loss:2.4400, Accuracy:0.1819, Validation Loss:2.4186, Validation Accuracy:0.1773\n",
    "Epoch #151: Loss:2.4412, Accuracy:0.1836, Validation Loss:2.4198, Validation Accuracy:0.1741\n",
    "Epoch #152: Loss:2.4423, Accuracy:0.1828, Validation Loss:2.4209, Validation Accuracy:0.1741\n",
    "Epoch #153: Loss:2.4404, Accuracy:0.1803, Validation Loss:2.4169, Validation Accuracy:0.1839\n",
    "Epoch #154: Loss:2.4404, Accuracy:0.1848, Validation Loss:2.4172, Validation Accuracy:0.1708\n",
    "Epoch #155: Loss:2.4398, Accuracy:0.1864, Validation Loss:2.4175, Validation Accuracy:0.1691\n",
    "Epoch #156: Loss:2.4399, Accuracy:0.1815, Validation Loss:2.4173, Validation Accuracy:0.1823\n",
    "Epoch #157: Loss:2.4385, Accuracy:0.1836, Validation Loss:2.4159, Validation Accuracy:0.1806\n",
    "Epoch #158: Loss:2.4384, Accuracy:0.1860, Validation Loss:2.4132, Validation Accuracy:0.1839\n",
    "Epoch #159: Loss:2.4368, Accuracy:0.1873, Validation Loss:2.4179, Validation Accuracy:0.1790\n",
    "Epoch #160: Loss:2.4377, Accuracy:0.1885, Validation Loss:2.4167, Validation Accuracy:0.1823\n",
    "Epoch #161: Loss:2.4398, Accuracy:0.1815, Validation Loss:2.4197, Validation Accuracy:0.1741\n",
    "Epoch #162: Loss:2.4382, Accuracy:0.1836, Validation Loss:2.4175, Validation Accuracy:0.1823\n",
    "Epoch #163: Loss:2.4379, Accuracy:0.1885, Validation Loss:2.4179, Validation Accuracy:0.1856\n",
    "Epoch #164: Loss:2.4375, Accuracy:0.1864, Validation Loss:2.4181, Validation Accuracy:0.1823\n",
    "Epoch #165: Loss:2.4379, Accuracy:0.1869, Validation Loss:2.4188, Validation Accuracy:0.1823\n",
    "Epoch #166: Loss:2.4378, Accuracy:0.1873, Validation Loss:2.4192, Validation Accuracy:0.1806\n",
    "Epoch #167: Loss:2.4370, Accuracy:0.1864, Validation Loss:2.4188, Validation Accuracy:0.1724\n",
    "Epoch #168: Loss:2.4370, Accuracy:0.1860, Validation Loss:2.4175, Validation Accuracy:0.1823\n",
    "Epoch #169: Loss:2.4374, Accuracy:0.1852, Validation Loss:2.4160, Validation Accuracy:0.1741\n",
    "Epoch #170: Loss:2.4381, Accuracy:0.1852, Validation Loss:2.4178, Validation Accuracy:0.1839\n",
    "Epoch #171: Loss:2.4371, Accuracy:0.1856, Validation Loss:2.4176, Validation Accuracy:0.1839\n",
    "Epoch #172: Loss:2.4370, Accuracy:0.1864, Validation Loss:2.4172, Validation Accuracy:0.1708\n",
    "Epoch #173: Loss:2.4363, Accuracy:0.1864, Validation Loss:2.4182, Validation Accuracy:0.1708\n",
    "Epoch #174: Loss:2.4381, Accuracy:0.1856, Validation Loss:2.4190, Validation Accuracy:0.1823\n",
    "Epoch #175: Loss:2.4379, Accuracy:0.1848, Validation Loss:2.4191, Validation Accuracy:0.1741\n",
    "Epoch #176: Loss:2.4378, Accuracy:0.1828, Validation Loss:2.4183, Validation Accuracy:0.1839\n",
    "Epoch #177: Loss:2.4353, Accuracy:0.1881, Validation Loss:2.4179, Validation Accuracy:0.1823\n",
    "Epoch #178: Loss:2.4362, Accuracy:0.1856, Validation Loss:2.4209, Validation Accuracy:0.1724\n",
    "Epoch #179: Loss:2.4370, Accuracy:0.1852, Validation Loss:2.4197, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.4366, Accuracy:0.1860, Validation Loss:2.4195, Validation Accuracy:0.1708\n",
    "Epoch #181: Loss:2.4363, Accuracy:0.1877, Validation Loss:2.4210, Validation Accuracy:0.1708\n",
    "Epoch #182: Loss:2.4364, Accuracy:0.1864, Validation Loss:2.4212, Validation Accuracy:0.1741\n",
    "Epoch #183: Loss:2.4370, Accuracy:0.1860, Validation Loss:2.4203, Validation Accuracy:0.1773\n",
    "Epoch #184: Loss:2.4358, Accuracy:0.1860, Validation Loss:2.4186, Validation Accuracy:0.1806\n",
    "Epoch #185: Loss:2.4347, Accuracy:0.1881, Validation Loss:2.4184, Validation Accuracy:0.1790\n",
    "Epoch #186: Loss:2.4350, Accuracy:0.1901, Validation Loss:2.4171, Validation Accuracy:0.1675\n",
    "Epoch #187: Loss:2.4351, Accuracy:0.1869, Validation Loss:2.4181, Validation Accuracy:0.1724\n",
    "Epoch #188: Loss:2.4351, Accuracy:0.1877, Validation Loss:2.4195, Validation Accuracy:0.1658\n",
    "Epoch #189: Loss:2.4351, Accuracy:0.1873, Validation Loss:2.4211, Validation Accuracy:0.1691\n",
    "Epoch #190: Loss:2.4353, Accuracy:0.1885, Validation Loss:2.4200, Validation Accuracy:0.1691\n",
    "Epoch #191: Loss:2.4343, Accuracy:0.1848, Validation Loss:2.4207, Validation Accuracy:0.1790\n",
    "Epoch #192: Loss:2.4345, Accuracy:0.1893, Validation Loss:2.4221, Validation Accuracy:0.1708\n",
    "Epoch #193: Loss:2.4357, Accuracy:0.1815, Validation Loss:2.4200, Validation Accuracy:0.1806\n",
    "Epoch #194: Loss:2.4339, Accuracy:0.1848, Validation Loss:2.4208, Validation Accuracy:0.1757\n",
    "Epoch #195: Loss:2.4348, Accuracy:0.1864, Validation Loss:2.4185, Validation Accuracy:0.1757\n",
    "Epoch #196: Loss:2.4354, Accuracy:0.1885, Validation Loss:2.4209, Validation Accuracy:0.1757\n",
    "Epoch #197: Loss:2.4350, Accuracy:0.1873, Validation Loss:2.4181, Validation Accuracy:0.1806\n",
    "Epoch #198: Loss:2.4349, Accuracy:0.1864, Validation Loss:2.4210, Validation Accuracy:0.1741\n",
    "Epoch #199: Loss:2.4347, Accuracy:0.1864, Validation Loss:2.4204, Validation Accuracy:0.1773\n",
    "Epoch #200: Loss:2.4351, Accuracy:0.1828, Validation Loss:2.4215, Validation Accuracy:0.1757\n",
    "Epoch #201: Loss:2.4360, Accuracy:0.1869, Validation Loss:2.4204, Validation Accuracy:0.1741\n",
    "Epoch #202: Loss:2.4370, Accuracy:0.1873, Validation Loss:2.4222, Validation Accuracy:0.1757\n",
    "Epoch #203: Loss:2.4346, Accuracy:0.1877, Validation Loss:2.4203, Validation Accuracy:0.1790\n",
    "Epoch #204: Loss:2.4351, Accuracy:0.1885, Validation Loss:2.4203, Validation Accuracy:0.1773\n",
    "Epoch #205: Loss:2.4344, Accuracy:0.1873, Validation Loss:2.4202, Validation Accuracy:0.1757\n",
    "Epoch #206: Loss:2.4344, Accuracy:0.1869, Validation Loss:2.4208, Validation Accuracy:0.1773\n",
    "Epoch #207: Loss:2.4355, Accuracy:0.1873, Validation Loss:2.4196, Validation Accuracy:0.1823\n",
    "Epoch #208: Loss:2.4350, Accuracy:0.1881, Validation Loss:2.4204, Validation Accuracy:0.1773\n",
    "Epoch #209: Loss:2.4358, Accuracy:0.1860, Validation Loss:2.4194, Validation Accuracy:0.1773\n",
    "Epoch #210: Loss:2.4335, Accuracy:0.1873, Validation Loss:2.4196, Validation Accuracy:0.1790\n",
    "Epoch #211: Loss:2.4343, Accuracy:0.1893, Validation Loss:2.4187, Validation Accuracy:0.1839\n",
    "Epoch #212: Loss:2.4337, Accuracy:0.1881, Validation Loss:2.4199, Validation Accuracy:0.1773\n",
    "Epoch #213: Loss:2.4331, Accuracy:0.1881, Validation Loss:2.4197, Validation Accuracy:0.1724\n",
    "Epoch #214: Loss:2.4330, Accuracy:0.1885, Validation Loss:2.4189, Validation Accuracy:0.1773\n",
    "Epoch #215: Loss:2.4346, Accuracy:0.1881, Validation Loss:2.4186, Validation Accuracy:0.1806\n",
    "Epoch #216: Loss:2.4338, Accuracy:0.1897, Validation Loss:2.4182, Validation Accuracy:0.1773\n",
    "Epoch #217: Loss:2.4335, Accuracy:0.1881, Validation Loss:2.4189, Validation Accuracy:0.1708\n",
    "Epoch #218: Loss:2.4361, Accuracy:0.1877, Validation Loss:2.4185, Validation Accuracy:0.1757\n",
    "Epoch #219: Loss:2.4338, Accuracy:0.1885, Validation Loss:2.4173, Validation Accuracy:0.1790\n",
    "Epoch #220: Loss:2.4334, Accuracy:0.1889, Validation Loss:2.4168, Validation Accuracy:0.1741\n",
    "Epoch #221: Loss:2.4320, Accuracy:0.1864, Validation Loss:2.4168, Validation Accuracy:0.1773\n",
    "Epoch #222: Loss:2.4318, Accuracy:0.1877, Validation Loss:2.4189, Validation Accuracy:0.1724\n",
    "Epoch #223: Loss:2.4330, Accuracy:0.1877, Validation Loss:2.4197, Validation Accuracy:0.1773\n",
    "Epoch #224: Loss:2.4322, Accuracy:0.1848, Validation Loss:2.4197, Validation Accuracy:0.1741\n",
    "Epoch #225: Loss:2.4329, Accuracy:0.1848, Validation Loss:2.4196, Validation Accuracy:0.1708\n",
    "Epoch #226: Loss:2.4316, Accuracy:0.1823, Validation Loss:2.4190, Validation Accuracy:0.1790\n",
    "Epoch #227: Loss:2.4344, Accuracy:0.1881, Validation Loss:2.4204, Validation Accuracy:0.1741\n",
    "Epoch #228: Loss:2.4320, Accuracy:0.1856, Validation Loss:2.4202, Validation Accuracy:0.1823\n",
    "Epoch #229: Loss:2.4358, Accuracy:0.1864, Validation Loss:2.4207, Validation Accuracy:0.1741\n",
    "Epoch #230: Loss:2.4331, Accuracy:0.1828, Validation Loss:2.4199, Validation Accuracy:0.1741\n",
    "Epoch #231: Loss:2.4324, Accuracy:0.1869, Validation Loss:2.4221, Validation Accuracy:0.1790\n",
    "Epoch #232: Loss:2.4303, Accuracy:0.1848, Validation Loss:2.4196, Validation Accuracy:0.1741\n",
    "Epoch #233: Loss:2.4303, Accuracy:0.1864, Validation Loss:2.4205, Validation Accuracy:0.1773\n",
    "Epoch #234: Loss:2.4307, Accuracy:0.1864, Validation Loss:2.4209, Validation Accuracy:0.1741\n",
    "Epoch #235: Loss:2.4303, Accuracy:0.1864, Validation Loss:2.4199, Validation Accuracy:0.1642\n",
    "Epoch #236: Loss:2.4302, Accuracy:0.1840, Validation Loss:2.4197, Validation Accuracy:0.1724\n",
    "Epoch #237: Loss:2.4296, Accuracy:0.1864, Validation Loss:2.4230, Validation Accuracy:0.1724\n",
    "Epoch #238: Loss:2.4310, Accuracy:0.1910, Validation Loss:2.4244, Validation Accuracy:0.1724\n",
    "Epoch #239: Loss:2.4298, Accuracy:0.1901, Validation Loss:2.4197, Validation Accuracy:0.1642\n",
    "Epoch #240: Loss:2.4303, Accuracy:0.1836, Validation Loss:2.4171, Validation Accuracy:0.1708\n",
    "Epoch #241: Loss:2.4272, Accuracy:0.1864, Validation Loss:2.4192, Validation Accuracy:0.1790\n",
    "Epoch #242: Loss:2.4287, Accuracy:0.1873, Validation Loss:2.4208, Validation Accuracy:0.1757\n",
    "Epoch #243: Loss:2.4257, Accuracy:0.1852, Validation Loss:2.4183, Validation Accuracy:0.1691\n",
    "Epoch #244: Loss:2.4253, Accuracy:0.1893, Validation Loss:2.4188, Validation Accuracy:0.1708\n",
    "Epoch #245: Loss:2.4241, Accuracy:0.1873, Validation Loss:2.4180, Validation Accuracy:0.1741\n",
    "Epoch #246: Loss:2.4256, Accuracy:0.1860, Validation Loss:2.4236, Validation Accuracy:0.1675\n",
    "Epoch #247: Loss:2.6499, Accuracy:0.1647, Validation Loss:3.4734, Validation Accuracy:0.1018\n",
    "Epoch #248: Loss:3.3824, Accuracy:0.1023, Validation Loss:3.2126, Validation Accuracy:0.1018\n",
    "Epoch #249: Loss:3.0480, Accuracy:0.1023, Validation Loss:2.8484, Validation Accuracy:0.1018\n",
    "Epoch #250: Loss:2.8160, Accuracy:0.1023, Validation Loss:2.7706, Validation Accuracy:0.1018\n",
    "Epoch #251: Loss:2.7513, Accuracy:0.1023, Validation Loss:2.7230, Validation Accuracy:0.1018\n",
    "Epoch #252: Loss:2.7117, Accuracy:0.0854, Validation Loss:2.6976, Validation Accuracy:0.0854\n",
    "Epoch #253: Loss:2.6920, Accuracy:0.0842, Validation Loss:2.6841, Validation Accuracy:0.0805\n",
    "Epoch #254: Loss:2.6811, Accuracy:0.0862, Validation Loss:2.6766, Validation Accuracy:0.1018\n",
    "Epoch #255: Loss:2.6748, Accuracy:0.1023, Validation Loss:2.6719, Validation Accuracy:0.1018\n",
    "Epoch #256: Loss:2.6710, Accuracy:0.1023, Validation Loss:2.6688, Validation Accuracy:0.1018\n",
    "Epoch #257: Loss:2.6680, Accuracy:0.1023, Validation Loss:2.6665, Validation Accuracy:0.1018\n",
    "Epoch #258: Loss:2.6659, Accuracy:0.1023, Validation Loss:2.6649, Validation Accuracy:0.1018\n",
    "Epoch #259: Loss:2.6645, Accuracy:0.1023, Validation Loss:2.6636, Validation Accuracy:0.1018\n",
    "Epoch #260: Loss:2.6635, Accuracy:0.1023, Validation Loss:2.6627, Validation Accuracy:0.1018\n",
    "Epoch #261: Loss:2.6626, Accuracy:0.1023, Validation Loss:2.6619, Validation Accuracy:0.1018\n",
    "Epoch #262: Loss:2.6620, Accuracy:0.1023, Validation Loss:2.6613, Validation Accuracy:0.1018\n",
    "Epoch #263: Loss:2.6615, Accuracy:0.1023, Validation Loss:2.6608, Validation Accuracy:0.1018\n",
    "Epoch #264: Loss:2.6609, Accuracy:0.1023, Validation Loss:2.6603, Validation Accuracy:0.1018\n",
    "Epoch #265: Loss:2.6605, Accuracy:0.1023, Validation Loss:2.6598, Validation Accuracy:0.1018\n",
    "Epoch #266: Loss:2.6600, Accuracy:0.1023, Validation Loss:2.6595, Validation Accuracy:0.1018\n",
    "Epoch #267: Loss:2.6597, Accuracy:0.1023, Validation Loss:2.6592, Validation Accuracy:0.1018\n",
    "Epoch #268: Loss:2.6594, Accuracy:0.1023, Validation Loss:2.6589, Validation Accuracy:0.1018\n",
    "Epoch #269: Loss:2.6592, Accuracy:0.1023, Validation Loss:2.6586, Validation Accuracy:0.1018\n",
    "Epoch #270: Loss:2.6590, Accuracy:0.1023, Validation Loss:2.6584, Validation Accuracy:0.1018\n",
    "Epoch #271: Loss:2.6587, Accuracy:0.1023, Validation Loss:2.6581, Validation Accuracy:0.1018\n",
    "Epoch #272: Loss:2.6585, Accuracy:0.1023, Validation Loss:2.6579, Validation Accuracy:0.1018\n",
    "Epoch #273: Loss:2.6583, Accuracy:0.1023, Validation Loss:2.6577, Validation Accuracy:0.1018\n",
    "Epoch #274: Loss:2.6581, Accuracy:0.1023, Validation Loss:2.6574, Validation Accuracy:0.1018\n",
    "Epoch #275: Loss:2.6579, Accuracy:0.1023, Validation Loss:2.6572, Validation Accuracy:0.1018\n",
    "Epoch #276: Loss:2.6577, Accuracy:0.1023, Validation Loss:2.6569, Validation Accuracy:0.1018\n",
    "Epoch #277: Loss:2.6575, Accuracy:0.1023, Validation Loss:2.6567, Validation Accuracy:0.1018\n",
    "Epoch #278: Loss:2.6572, Accuracy:0.1023, Validation Loss:2.6564, Validation Accuracy:0.1018\n",
    "Epoch #279: Loss:2.6569, Accuracy:0.1023, Validation Loss:2.6561, Validation Accuracy:0.1018\n",
    "Epoch #280: Loss:2.6566, Accuracy:0.1023, Validation Loss:2.6558, Validation Accuracy:0.1018\n",
    "Epoch #281: Loss:2.6564, Accuracy:0.1023, Validation Loss:2.6554, Validation Accuracy:0.1018\n",
    "Epoch #282: Loss:2.6560, Accuracy:0.1023, Validation Loss:2.6550, Validation Accuracy:0.1018\n",
    "Epoch #283: Loss:2.6557, Accuracy:0.1023, Validation Loss:2.6546, Validation Accuracy:0.1018\n",
    "Epoch #284: Loss:2.6553, Accuracy:0.1023, Validation Loss:2.6541, Validation Accuracy:0.1018\n",
    "Epoch #285: Loss:2.6550, Accuracy:0.1023, Validation Loss:2.6536, Validation Accuracy:0.1018\n",
    "Epoch #286: Loss:2.6545, Accuracy:0.1023, Validation Loss:2.6530, Validation Accuracy:0.1018\n",
    "Epoch #287: Loss:2.6539, Accuracy:0.1023, Validation Loss:2.6524, Validation Accuracy:0.1018\n",
    "Epoch #288: Loss:2.6533, Accuracy:0.1023, Validation Loss:2.6517, Validation Accuracy:0.1018\n",
    "Epoch #289: Loss:2.6526, Accuracy:0.1023, Validation Loss:2.6509, Validation Accuracy:0.1018\n",
    "Epoch #290: Loss:2.6517, Accuracy:0.1023, Validation Loss:2.6500, Validation Accuracy:0.1018\n",
    "Epoch #291: Loss:2.6508, Accuracy:0.1023, Validation Loss:2.6490, Validation Accuracy:0.1018\n",
    "Epoch #292: Loss:2.6498, Accuracy:0.1023, Validation Loss:2.6478, Validation Accuracy:0.1018\n",
    "Epoch #293: Loss:2.6487, Accuracy:0.1023, Validation Loss:2.6464, Validation Accuracy:0.1018\n",
    "Epoch #294: Loss:2.6473, Accuracy:0.1023, Validation Loss:2.6448, Validation Accuracy:0.1018\n",
    "Epoch #295: Loss:2.6457, Accuracy:0.1023, Validation Loss:2.6426, Validation Accuracy:0.1018\n",
    "Epoch #296: Loss:2.6437, Accuracy:0.1023, Validation Loss:2.6403, Validation Accuracy:0.1018\n",
    "Epoch #297: Loss:2.6414, Accuracy:0.1027, Validation Loss:2.6375, Validation Accuracy:0.1133\n",
    "Epoch #298: Loss:2.6385, Accuracy:0.1129, Validation Loss:2.6341, Validation Accuracy:0.1166\n",
    "Epoch #299: Loss:2.6353, Accuracy:0.1142, Validation Loss:2.6302, Validation Accuracy:0.1166\n",
    "Epoch #300: Loss:2.6314, Accuracy:0.1138, Validation Loss:2.6260, Validation Accuracy:0.1166\n",
    "\n",
    "Test:\n",
    "Test Loss:2.62602568, Accuracy:0.1166\n",
    "Labels: ['ib', 'sg', 'ce', 'aa', 'eo', 'my', 'mb', 'yd', 'eg', 'ek', 'sk', 'ds', 'ck', 'by', 'eb']\n",
    "Confusion Matrix:\n",
    "      ib  sg  ce  aa  eo  my  mb  yd  eg  ek  sk  ds  ck  by  eb\n",
    "t:ib   0   0   0   0   0   0   0  54   0   0   0   0   0   0   0\n",
    "t:sg   0   0   0   0   0   0   0  50   1   0   0   0   0   0   0\n",
    "t:ce   0   0   0   0   0   0   0  26   1   0   0   0   0   0   0\n",
    "t:aa   0   0   0   0   0   0   0  25   9   0   0   0   0   0   0\n",
    "t:eo   0   0   0   0   0   0   0  34   0   0   0   0   0   0   0\n",
    "t:my   0   0   0   0   0   0   0  15   5   0   0   0   0   0   0\n",
    "t:mb   0   0   0   0   0   0   0  52   0   0   0   0   0   0   0\n",
    "t:yd   0   0   0   0   0   0   0  62   0   0   0   0   0   0   0\n",
    "t:eg   0   0   0   0   0   0   0  41   9   0   0   0   0   0   0\n",
    "t:ek   0   0   0   0   0   0   0  44   4   0   0   0   0   0   0\n",
    "t:sk   0   0   0   0   0   0   0  29   4   0   0   0   0   0   0\n",
    "t:ds   0   0   0   0   0   0   0  17  14   0   0   0   0   0   0\n",
    "t:ck   0   0   0   0   0   0   0  19   4   0   0   0   0   0   0\n",
    "t:by   0   0   0   0   0   0   0  38   2   0   0   0   0   0   0\n",
    "t:eb   0   0   0   0   0   0   0  45   5   0   0   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          sg       0.00      0.00      0.00        51\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          my       0.00      0.00      0.00        20\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          yd       0.11      1.00      0.20        62\n",
    "          eg       0.16      0.18      0.17        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ds       0.00      0.00      0.00        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          by       0.00      0.00      0.00        40\n",
    "          eb       0.00      0.00      0.00        50\n",
    "\n",
    "    accuracy                           0.12       609\n",
    "   macro avg       0.02      0.08      0.02       609\n",
    "weighted avg       0.02      0.12      0.03       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 01:46:55 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 47 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.706628498772682, 2.6993881188002713, 2.69435092459367, 2.6885895298423828, 2.683155519425967, 2.67724425021455, 2.671389371499248, 2.665631311084641, 2.659365942521244, 2.6524033691299764, 2.6437327376335906, 2.633452885256612, 2.622337939899739, 2.609348193373782, 2.5973103982082923, 2.5793422576046146, 2.561831079288852, 2.545298773275416, 2.5235605619615327, 2.506703916637377, 2.494883812119808, 2.483706994205469, 2.473608438604571, 2.4750028192899105, 2.4674348208704604, 2.459548549307587, 2.464896119091115, 2.4545146565523446, 2.450079502143296, 2.4510210496059974, 2.4461153070327684, 2.4426595734062255, 2.444742373449266, 2.4435134685685482, 2.494905816706139, 2.496355222755269, 2.464881123384623, 2.4516099196153713, 2.4602962237077786, 2.4455975683647617, 2.447280482901337, 2.441983451984199, 2.4429841374333074, 2.438747401699448, 2.438054338464596, 2.4307366008633267, 2.433748074548781, 2.4345198076934063, 2.4300087199031033, 2.4299106092875813, 2.4323278068517427, 2.431251873132239, 2.4303430720111616, 2.4281188419886996, 2.4289801958550763, 2.4271785645257857, 2.427138003613953, 2.4248240381626074, 2.4236779737550833, 2.4252443685515956, 2.4249814222207404, 2.426868298175104, 2.426338986222967, 2.426216223361261, 2.426730975337412, 2.4247297833510024, 2.426086113370698, 2.425057536471262, 2.423736888591096, 2.4259357608989345, 2.424763585937826, 2.424773848115517, 2.4237945608317557, 2.4260641389292448, 2.426277864938495, 2.424454804907487, 2.4250092968369157, 2.4238249507835152, 2.425688946384123, 2.4258163421612067, 2.4248400587949455, 2.4280185194438313, 2.427313056876898, 2.4262425809462473, 2.422283953633802, 2.4204971379247207, 2.421340571248473, 2.422420870494373, 2.420246105475966, 2.421778885210285, 2.4212915713368184, 2.421442897840478, 2.4206315883861973, 2.4211157049451555, 2.4202291135521747, 2.41966058154803, 2.421668724473474, 2.422674372082665, 2.42320966916327, 2.4229149646163965, 2.421893415388411, 2.422334312413909, 2.4207920942008982, 2.421510499099205, 2.4230059958835346, 2.4232532852780446, 2.422227071423836, 2.4246063248081553, 2.4251630255349945, 2.4245141668272723, 2.423925002024483, 2.4229069374660748, 2.423108170576675, 2.4259259943500138, 2.4264538577820476, 2.4250254912916662, 2.425159906910362, 2.424471505952782, 2.423258851510159, 2.4214855003826723, 2.423592248377933, 2.422841431863594, 2.42488996422741, 2.425303925825849, 2.425051265945184, 2.4224872412939966, 2.424138449291486, 2.4251290765301934, 2.428305344824329, 2.4271437237023914, 2.4263947984855165, 2.4217766282593676, 2.422416208217101, 2.421009940662603, 2.419968871647501, 2.4226486819914017, 2.4224428711657846, 2.4271947292271507, 2.429326256311977, 2.4329610321126354, 2.4280498755976483, 2.424532638982012, 2.4214737344845174, 2.418496160475883, 2.417812017775913, 2.4194355793970166, 2.4210485793491108, 2.418883223838994, 2.4164139034321352, 2.41861471440796, 2.4198310030700734, 2.420928375670084, 2.4168888555567447, 2.4172258224393346, 2.417499250574848, 2.4173321786576696, 2.415872019108489, 2.413195578727033, 2.4178854808431542, 2.416682579638727, 2.419698496365978, 2.417495257748759, 2.4179171623267566, 2.4180694385898134, 2.418776833952354, 2.4191956617953547, 2.418815665252886, 2.4175387517180544, 2.4160411216942546, 2.4177873299039643, 2.4175699375728863, 2.4172102023032305, 2.418233091216565, 2.4189869464911853, 2.4190664643724564, 2.4182528674308887, 2.417911508400452, 2.4208568594921593, 2.4197374008754986, 2.4194768246367255, 2.421018897214742, 2.421210653480442, 2.420329122512016, 2.418589768934328, 2.418419341539906, 2.417128914878482, 2.41813937274889, 2.4194648011368876, 2.4211432538400532, 2.420008035324673, 2.4206796146574474, 2.4221212296258834, 2.4200142549568016, 2.4207996511694247, 2.4184886187755414, 2.4209481115607403, 2.4180652504092564, 2.4209545852711245, 2.4203620642081076, 2.421495794271209, 2.4203973810856763, 2.422215115260608, 2.4203256715107435, 2.420276132123224, 2.420240906463272, 2.4207963180072203, 2.419593910474104, 2.42035377280074, 2.4193986018107245, 2.4196035760181096, 2.418721401436967, 2.4198792008147842, 2.419724251053408, 2.4188671507467387, 2.4186103880307552, 2.418220369686634, 2.418895583630391, 2.4184690375241935, 2.4173098205541352, 2.416845909676137, 2.4167552507178147, 2.418921853521187, 2.4196601540388536, 2.419738574568274, 2.419621668425687, 2.419016695570672, 2.420394538462847, 2.4201591026606817, 2.420707056675051, 2.419914625352631, 2.4220640428352045, 2.4195775688183914, 2.4205344224406775, 2.4209247633741406, 2.41990269502787, 2.419690713898106, 2.4229947706357207, 2.4244422180507765, 2.4196930025598684, 2.4171412022438736, 2.419216547497779, 2.4207840650931174, 2.418255598870013, 2.4187872374586283, 2.417955823719795, 2.423609352268413, 3.4734481535913124, 3.2125590239056616, 2.848411462185614, 2.7705861437692625, 2.722991982117075, 2.6976308587736684, 2.684125610564534, 2.6765714431631156, 2.6719098674448447, 2.6687898005562265, 2.6664638934268545, 2.6648591054092683, 2.6636466416232105, 2.662692651764317, 2.6619480290436393, 2.661336061011003, 2.660768721882737, 2.6602729239878786, 2.659838492842926, 2.659496195014866, 2.6591877119098783, 2.658906925292242, 2.6586362740089156, 2.6583817063881257, 2.6581407114007005, 2.6579065569515885, 2.657674760066817, 2.657435914761523, 2.6571940584918745, 2.6569451581081145, 2.656677255098064, 2.6563902680314038, 2.6560866785754125, 2.6557587535902, 2.655413554807015, 2.655012610706398, 2.6546029590425038, 2.6541473466187275, 2.6536328444144215, 2.653048744734089, 2.6524203015469956, 2.651670798488047, 2.6508593453562317, 2.6500172121771453, 2.6489924985199726, 2.6477758234553344, 2.646389873939978, 2.644794137607067, 2.642617018156255, 2.640324299754376, 2.6375285228484957, 2.634102916874126, 2.6301970203913294, 2.62602561252262], 'val_acc': [0.08045976980908946, 0.09523809523197818, 0.12151067213374014, 0.11001642034901382, 0.10837438422421908, 0.10344827575196186, 0.10180623962716712, 0.10180623962716712, 0.11658456385722889, 0.11986863610681837, 0.13628899735476582, 0.14449917807661253, 0.15106732257579153, 0.14942528645099679, 0.15270935860271329, 0.16256157544935473, 0.16420361157414948, 0.17077175607332848, 0.16584564769894422, 0.17241379229599618, 0.16748768411735793, 0.17241379210025023, 0.16420361167202246, 0.16748768401948494, 0.16091953961817893, 0.15599343104804875, 0.1740558285186639, 0.1691297200464067, 0.16748768401948494, 0.17077175626907443, 0.16912972014427968, 0.17077175617120144, 0.17241379229599618, 0.16748768392161195, 0.15599343143954067, 0.16091953912881404, 0.16256157633021157, 0.1658456477968172, 0.17241379219812322, 0.1691297200464067, 0.17077175626907443, 0.17405582832291797, 0.16912972074375168, 0.1658456477968172, 0.16420361167202246, 0.1740558292160089, 0.1740558292160089, 0.16912972024215267, 0.1773399008661264, 0.1855500815879731, 0.1740558285186639, 0.17241379249174216, 0.1740558286165369, 0.1740558287144099, 0.17569786483920463, 0.17077175626907443, 0.17241379200237725, 0.1773399008661264, 0.1740558286165369, 0.1707717558775825, 0.17241379239386917, 0.18062397311571587, 0.18226600924051062, 0.18226600924051062, 0.17569786474133164, 0.1740558286165369, 0.17569786474133164, 0.17569786474133164, 0.18226600914263763, 0.17898193679517518, 0.17733990067038044, 0.17405582842079093, 0.17077175686854643, 0.17241379239386917, 0.16091954011977797, 0.16912972084162467, 0.17241379239386917, 0.1625615762445727, 0.16091954011977797, 0.1592775033955112, 0.16420361137840353, 0.1642036118677684, 0.15763546697709752, 0.15763546697709752, 0.16091953952030597, 0.1625615752536088, 0.16912972024215267, 0.16748768401948494, 0.16420361246724044, 0.16420361137840353, 0.16420361137840353, 0.1592775030040193, 0.16420361246724044, 0.1658456477968172, 0.17241379239386917, 0.1740558287144099, 0.1658456478946902, 0.17569786464345866, 0.17241379249174216, 0.1658456478946902, 0.17241379249174216, 0.1576354671728435, 0.1576354671728435, 0.1625615756451007, 0.16420361176989545, 0.16420361176989545, 0.16091953952030597, 0.1592775033955112, 0.16091953952030597, 0.16091953952030597, 0.16420361176989545, 0.16420361176989545, 0.1658456478946902, 0.1658456478946902, 0.1658456477968172, 0.17077175626907443, 0.17077175617120144, 0.1658456478946902, 0.17241379239386917, 0.17241379239386917, 0.17241379239386917, 0.17077175626907443, 0.17241379239386917, 0.16748768401948494, 0.1658456478946902, 0.16091953952030597, 0.16748768401948494, 0.1625615756451007, 0.1642036118677684, 0.16256157554722772, 0.1592775033955112, 0.1658456478946902, 0.16748768392161195, 0.17077175626907443, 0.16748768401948494, 0.17241379229599618, 0.17569786464345866, 0.1674876842152309, 0.1740558286165369, 0.16091953932456, 0.16912972024215267, 0.17077175617120144, 0.16912972014427968, 0.1707717563669474, 0.17733990096399938, 0.17569786474133164, 0.17733990096399938, 0.17733990096399938, 0.1773399008661264, 0.17733990096399938, 0.1740558286165369, 0.1740558286165369, 0.1839080452674324, 0.17077175617120144, 0.16912972034002563, 0.18226600904476467, 0.1806239730178429, 0.18390804516955941, 0.17898193679517518, 0.1822660088490187, 0.17405582842079093, 0.18226600894689168, 0.18555008129435416, 0.18226600894689168, 0.1822660088490187, 0.18062397272422395, 0.17241379219812322, 0.18226600894689168, 0.17405582832291797, 0.18390804507168643, 0.18390804507168643, 0.17077175607332848, 0.17077175607332848, 0.18226600904476467, 0.17405582842079093, 0.18390804507168643, 0.18226600894689168, 0.17241379219812322, 0.17569786434983972, 0.1707717559754555, 0.1707717559754555, 0.174055828127172, 0.17733990047463447, 0.18062397272422395, 0.1789819366973022, 0.167487683725866, 0.17241379210025023, 0.16584564760107126, 0.16912971985066075, 0.16912972084162467, 0.1789819365994292, 0.17077175607332848, 0.18062397282209694, 0.1756978644477127, 0.1756978644477127, 0.1756978644477127, 0.1806239730178429, 0.17405582842079093, 0.17733990057250745, 0.17569786454558567, 0.17405582842079093, 0.17569786454558567, 0.1789819366973022, 0.17733990067038044, 0.17569786454558567, 0.1773399007682534, 0.18226600914263763, 0.1773399007682534, 0.1773399007682534, 0.17898193689304814, 0.1839080452674324, 0.1773399007682534, 0.17241379219812322, 0.1773399007682534, 0.1806239730178429, 0.1773399007682534, 0.17077175607332848, 0.17569786464345866, 0.17898193689304814, 0.1740558285186639, 0.1773399007682534, 0.17241379239386917, 0.17733990057250745, 0.1740558285186639, 0.17077175626907443, 0.17898193679517518, 0.1740558292160089, 0.18226600904476467, 0.17405582902026293, 0.17405582892238997, 0.17898193739464718, 0.17405582902026293, 0.17733990057250745, 0.17405582902026293, 0.16420361236936745, 0.17241379289546818, 0.17241379299334117, 0.17241379289546818, 0.16420361227149446, 0.17077175617120144, 0.17898193759039313, 0.17569786434983972, 0.16912972084162467, 0.17077175617120144, 0.17405582832291797, 0.16748768382373896, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.08538587808560072, 0.0804597696133435, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.1133004916076394, 0.11658456385722889, 0.11658456385722889, 0.11658456385722889], 'loss': [2.71051088348796, 2.7037832203342194, 2.697448353796769, 2.692158315999307, 2.6860940626759304, 2.6808057973027473, 2.6752845931591684, 2.6693031140666235, 2.66365627018578, 2.6574748798072707, 2.650880293777591, 2.6419103808471553, 2.6307246386392893, 2.619198884121936, 2.6064117353310086, 2.5946694459268933, 2.5778567030199744, 2.56002564126706, 2.5420837140915573, 2.5286064673742965, 2.517240037957256, 2.5085886529338923, 2.4976276566115736, 2.5022754551693644, 2.495122992967923, 2.4884269779957298, 2.487676307161241, 2.4831895161458353, 2.4765550733836523, 2.474698803703888, 2.473616835517805, 2.4690578712330216, 2.468072509765625, 2.468503656426494, 2.5521846640036583, 2.516502260133716, 2.491438948691993, 2.5252978780186397, 2.4791666705505557, 2.4768611808087546, 2.4683309150672303, 2.4656109294852193, 2.4653263744142757, 2.462883480177768, 2.4619229805298164, 2.458647970595154, 2.458115582789239, 2.458818641776177, 2.4541050392989017, 2.454377051985974, 2.4537637169111437, 2.454666162173606, 2.454558011687512, 2.4535797338466137, 2.4537642177370294, 2.452526613133644, 2.4517084632810873, 2.4505618561464657, 2.449994854956437, 2.448943285677712, 2.4488759336530306, 2.451248785406657, 2.450974258211359, 2.4500385349046523, 2.4496121164709637, 2.449963658840015, 2.450141967346536, 2.4477062413335093, 2.447328158913207, 2.4463448756529322, 2.4458679405815547, 2.445729886726677, 2.447764965249283, 2.445530275838331, 2.446016491120356, 2.447919092237093, 2.445583374740162, 2.4448814651559756, 2.4452373584927476, 2.4438062135199012, 2.4442744714278706, 2.4448443774080375, 2.443390098881183, 2.4427189462728323, 2.444386622156695, 2.4421174388157025, 2.4409416664797177, 2.4406995950538275, 2.4405820278661206, 2.4403041931637994, 2.440853041545077, 2.440274258952366, 2.4402706384169246, 2.4395989788141583, 2.44095841423442, 2.4411862539804448, 2.441827151320064, 2.444439896323108, 2.443780321262211, 2.442858268397055, 2.4446835435636234, 2.443057565473678, 2.4427675546806698, 2.4439770547761075, 2.4459531924073454, 2.44405693126655, 2.4444387251836317, 2.443557383637164, 2.4432856078999734, 2.443944946500555, 2.4440966852881334, 2.443475242219177, 2.443811805439191, 2.444780470947955, 2.4449212644134457, 2.4451790933001947, 2.443499128040102, 2.443065694127484, 2.4455570269903855, 2.443130017208123, 2.443366614014706, 2.444261975259017, 2.4454212913278193, 2.4454574004335816, 2.446483900512758, 2.4458153187127083, 2.4441570317231163, 2.4473884550208185, 2.445814184192759, 2.445365278529925, 2.44550956367712, 2.443920050019846, 2.4431902899144857, 2.44320823367861, 2.4436570885245072, 2.4419013084082635, 2.4410317372002885, 2.442275597672198, 2.454543659476529, 2.4623682672238205, 2.451323107870208, 2.4424569656716724, 2.4387915502583466, 2.438742864107449, 2.4411427404846253, 2.4407392680033033, 2.4434925483727112, 2.443184626812318, 2.4409292190716254, 2.4400122936256614, 2.4412038561255045, 2.4422692451633714, 2.4404333894992023, 2.4404360927840276, 2.439816718816268, 2.4398843960106005, 2.4384701882544486, 2.438394704540652, 2.436832410549971, 2.4377465393019406, 2.4397513975108183, 2.4381501226209763, 2.4379200702330417, 2.437467519360646, 2.4379432462813675, 2.4378427564241067, 2.437011210482713, 2.4369674432938595, 2.4374155884650697, 2.4380932135258857, 2.4370948695548997, 2.437001444524808, 2.4363010377120187, 2.4380617126057524, 2.4379452083634643, 2.437813612469902, 2.4352644940910886, 2.4362114820147442, 2.4369985294537866, 2.436644938153653, 2.4363044281515007, 2.436382129402866, 2.4369568166791535, 2.4357510290596274, 2.434698881114043, 2.435038830367447, 2.4350516631617927, 2.4350867486832324, 2.435141903174242, 2.4352942984696533, 2.4342915860044885, 2.434529094578549, 2.435665598052728, 2.4339428493374426, 2.434811609726422, 2.4354348836738224, 2.434996882699109, 2.4348938301603407, 2.4346916422951637, 2.4350820907577106, 2.4360491755561906, 2.4369921341569025, 2.4345917722282957, 2.4350602489722095, 2.4343595248228227, 2.4344111063641933, 2.43551811584457, 2.4350065436940906, 2.4357627739406955, 2.4334915338355656, 2.434292696731536, 2.4336608698725457, 2.4330643583372145, 2.433028482558546, 2.4346105227969757, 2.433758855894116, 2.4335118945863945, 2.436115155719389, 2.433786520380259, 2.4334248621116186, 2.4319564733172343, 2.4318164517012955, 2.4329646312235806, 2.4322128170569575, 2.432869453058106, 2.4315591635890077, 2.434412341930538, 2.432048589052361, 2.4358316656500407, 2.43314362140162, 2.4324055773521596, 2.4302997148502046, 2.430260870735748, 2.4307027830480306, 2.430285375955413, 2.4302456729465933, 2.4295534355194905, 2.431012242922303, 2.4297592453888064, 2.4303435306039924, 2.427235031323756, 2.4286526933587798, 2.4257157408970826, 2.4253012329156394, 2.4240785927743147, 2.4255649412926705, 2.649875857355169, 3.3824283744765014, 3.047998654719985, 2.8160072961137526, 2.7512981041745728, 2.7117316800221283, 2.6920166480712577, 2.681146045778811, 2.674827649412214, 2.670980914564348, 2.6680362220662333, 2.6659238844681568, 2.664495442780136, 2.663546426585078, 2.662608820897598, 2.6620038166672786, 2.6614692546993313, 2.6608653751976434, 2.6604844435039734, 2.660033068764626, 2.6597326746711496, 2.659432906144943, 2.659192086441071, 2.6590239640378854, 2.6586937978771923, 2.6585075331419645, 2.6582983906019395, 2.658083571643555, 2.6579298470788912, 2.6576982685183106, 2.6574622912083807, 2.657221043232285, 2.656926087138589, 2.6566351620323605, 2.656386300132015, 2.656035499357345, 2.6556905435096065, 2.6552764339368693, 2.654976675397806, 2.6544887195132842, 2.653933387125787, 2.6532653217198177, 2.6526374863892856, 2.651710374154594, 2.650787159451714, 2.6497734399791617, 2.6487034267958185, 2.647265707638719, 2.645670847824222, 2.6437363075279845, 2.641423288361003, 2.638531768689165, 2.6353206146913877, 2.631425152862831], 'acc': [0.06365503115575662, 0.08213552395053958, 0.11252566708186813, 0.11375770033690964, 0.10924024692979437, 0.1075975364575151, 0.1051334701524378, 0.10225872663930212, 0.10554414855381301, 0.1141683783558115, 0.12279260792526621, 0.14086242292451173, 0.14866529706200046, 0.15030800851341145, 0.1544147839108042, 0.15523613854110607, 0.1577002062445059, 0.1605749483776778, 0.1667351126303663, 0.1659137578042381, 0.16632443481647013, 0.16919917892626424, 0.17289527631393448, 0.16919917814295884, 0.1691991783387852, 0.17412730916814392, 0.1794661193780096, 0.17535934341149653, 0.17946611839887788, 0.17782340890573037, 0.18151950727253235, 0.18151950766418504, 0.18275153993091545, 0.17412730955979663, 0.15318275105659476, 0.16468172375671183, 0.1675564688639964, 0.14825461903391923, 0.17371663293921727, 0.17864476396440235, 0.17535934399897557, 0.1761806976501457, 0.1790554409766344, 0.17700205347376438, 0.18193018371564407, 0.1786447629669119, 0.1802874744183229, 0.183162218528117, 0.1831622181364643, 0.1823408617253666, 0.18069815262387176, 0.17946611820305153, 0.1761806992167565, 0.17618069745431936, 0.1774127315018456, 0.17659137526821553, 0.1761806984518098, 0.17946611920054198, 0.1765913754824006, 0.1761806976501457, 0.18069815301552444, 0.1790554413682871, 0.17946611918218328, 0.18439425020736835, 0.18028747361665878, 0.18357289538124014, 0.1823408626861396, 0.18110883006447395, 0.18275153914761005, 0.18439424981571564, 0.1819301858697339, 0.17905544078080807, 0.17987679583948007, 0.18069815283805682, 0.1811088286936895, 0.1819301850864285, 0.1786447641602287, 0.18028747363501751, 0.17987679779774354, 0.17823408730710558, 0.17782340773077226, 0.18028747365337622, 0.17946611820305153, 0.1737166315317154, 0.17330595569444143, 0.17535934184488575, 0.17946611820305153, 0.18069815103890224, 0.18357289418792333, 0.18151950746835868, 0.18028747343919116, 0.18193018487224344, 0.1827515393434364, 0.17782340851407766, 0.18110882885279841, 0.18521560524768163, 0.18110882906698347, 0.180287475219987, 0.18398357258929854, 0.1819301838931117, 0.18069815144891366, 0.18110882906698347, 0.1806981526422305, 0.18069815283805682, 0.18110883024194158, 0.1823408625270307, 0.1827515401451005, 0.1827515401267418, 0.18275154114259098, 0.18398357258929854, 0.18193018530061358, 0.18110883045612663, 0.18398357239347218, 0.18398357337260393, 0.18357289516705508, 0.18275154032256813, 0.18151950729089106, 0.1811088290853422, 0.1827515409284059, 0.18316221756734397, 0.18193018588809262, 0.18316221794063794, 0.18193018391147042, 0.18234086311450973, 0.18110882906698347, 0.181519507076706, 0.18316221774481162, 0.18069815086143462, 0.18069815184056637, 0.18028747384920257, 0.1815195078600114, 0.18275153971673036, 0.18110882924445112, 0.18028747342083243, 0.17577002042372858, 0.17782340794495732, 0.17946611879053057, 0.1811088286753308, 0.18193018410729678, 0.173305954323657, 0.18069815203639272, 0.17987679779774354, 0.1794661184172366, 0.17987679560693628, 0.17823408656051762, 0.1794661181846928, 0.1778234079082399, 0.17905544038915536, 0.18275154053675322, 0.18193018449894946, 0.18357289636037188, 0.18275154051839448, 0.18028747541581336, 0.18480492843127594, 0.1864476396868606, 0.1815195082700228, 0.18357289536288143, 0.18603696124876795, 0.1872689945129888, 0.18850102734883953, 0.1815195082700228, 0.1835728955770665, 0.1885010262105989, 0.18644763966850186, 0.1868583153283082, 0.18726899492300023, 0.18644763888519647, 0.18603695989634222, 0.18521560681429242, 0.18521560563933434, 0.18562628247409876, 0.1864476384935438, 0.1864476396868606, 0.18562628404070955, 0.18480492704213278, 0.18275153993091545, 0.18809034857417034, 0.18562628345323048, 0.18521560544350799, 0.1860369610713003, 0.18767967173940592, 0.18644763829771743, 0.18603696165877937, 0.186036961462953, 0.18809034935747573, 0.1901437382311302, 0.18685831632579866, 0.1876796709377418, 0.18726899453134752, 0.18850102560476112, 0.18480492880456992, 0.18932238082254202, 0.18151950727253235, 0.18480492823544958, 0.18644763929520788, 0.18850102658389287, 0.18726899390715104, 0.18644763908102283, 0.18644763790606472, 0.1827515409100472, 0.1868583153283082, 0.18726899453134752, 0.1876796705460891, 0.1885010271713719, 0.18726899431716246, 0.18685831591578725, 0.18726899294637803, 0.18809034896582305, 0.18603696187296442, 0.18726899353385706, 0.18932238141002106, 0.1880903479866913, 0.18809034896582305, 0.18850102736719826, 0.1880903495716608, 0.1897330606130604, 0.1880903479866913, 0.18767967076027417, 0.1885010259964138, 0.18891170361448362, 0.18644763927684918, 0.18767967056444784, 0.1876796713293945, 0.1848049286271023, 0.18480492900039625, 0.1823408615111815, 0.1880903479866913, 0.1856262826699251, 0.18644763908102283, 0.18275153914761005, 0.1868583161116136, 0.18480492821709085, 0.18644763868937012, 0.18644763810189108, 0.18644763790606472, 0.18398357317677758, 0.18644763927684918, 0.19096509190065905, 0.19014373686034577, 0.18357289514869635, 0.1864476384935438, 0.18726899431716246, 0.1852156062451721, 0.18932238221168518, 0.1872689945129888, 0.18603696048382126, 0.16468172517339308, 0.10225872722678116, 0.10225872722678116, 0.10225872704013417, 0.10225872683512846, 0.0854209441117927, 0.08418891225507372, 0.086242300112879, 0.10225872645265513, 0.10225872645265513, 0.10225872663930212, 0.10225872582845864, 0.10225872682594911, 0.10225872643429641, 0.10225872683512846, 0.10225872645265513, 0.10225872684430783, 0.10225872685348718, 0.10225872663012275, 0.10225872704931353, 0.10225872663930212, 0.10225872664848147, 0.10225872683512846, 0.10225872704931353, 0.10225872702177545, 0.10225872742260751, 0.10225872684430783, 0.10225872723596052, 0.10225872703095482, 0.10225872644347576, 0.10225872723596052, 0.10225872684430783, 0.10225872743178686, 0.10225872663930212, 0.10225872685348718, 0.10225872684430783, 0.10225872724513987, 0.10225872704013417, 0.10225872743178686, 0.10225872743178686, 0.10225872683512846, 0.10225872744096623, 0.1022587274501456, 0.10225872724513987, 0.10225872644347576, 0.10225872704931353, 0.10225872663930212, 0.10225872723596052, 0.10225872722678116, 0.10225872722678116, 0.10266940486320969, 0.11293634548324334, 0.11416837836499087, 0.11375770054191535]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
