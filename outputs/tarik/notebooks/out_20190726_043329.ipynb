{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf21.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 04:33:29 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '2', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000021703285E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002177B127EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0968, Accuracy:0.3943, Validation Loss:1.0861, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0815, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0751, Accuracy:0.3770, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0748, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0756, Accuracy:0.3943, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0752, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0728, Accuracy:0.3947, Validation Loss:1.0724, Validation Accuracy:0.4007\n",
    "Epoch #18: Loss:1.0723, Accuracy:0.4107, Validation Loss:1.0717, Validation Accuracy:0.4171\n",
    "Epoch #19: Loss:1.0715, Accuracy:0.4045, Validation Loss:1.0707, Validation Accuracy:0.4023\n",
    "Epoch #20: Loss:1.0703, Accuracy:0.4099, Validation Loss:1.0691, Validation Accuracy:0.4384\n",
    "Epoch #21: Loss:1.0687, Accuracy:0.4214, Validation Loss:1.0666, Validation Accuracy:0.4384\n",
    "Epoch #22: Loss:1.0653, Accuracy:0.4390, Validation Loss:1.0624, Validation Accuracy:0.4614\n",
    "Epoch #23: Loss:1.0598, Accuracy:0.4665, Validation Loss:1.0546, Validation Accuracy:0.4828\n",
    "Epoch #24: Loss:1.0494, Accuracy:0.4809, Validation Loss:1.0405, Validation Accuracy:0.4877\n",
    "Epoch #25: Loss:1.0310, Accuracy:0.4908, Validation Loss:1.0192, Validation Accuracy:0.4729\n",
    "Epoch #26: Loss:1.0021, Accuracy:0.4994, Validation Loss:0.9980, Validation Accuracy:0.4745\n",
    "Epoch #27: Loss:0.9765, Accuracy:0.5014, Validation Loss:0.9900, Validation Accuracy:0.4811\n",
    "Epoch #28: Loss:0.9592, Accuracy:0.5064, Validation Loss:0.9920, Validation Accuracy:0.4844\n",
    "Epoch #29: Loss:0.9532, Accuracy:0.5031, Validation Loss:0.9855, Validation Accuracy:0.4860\n",
    "Epoch #30: Loss:0.9417, Accuracy:0.5179, Validation Loss:0.9655, Validation Accuracy:0.4893\n",
    "Epoch #31: Loss:0.9366, Accuracy:0.5035, Validation Loss:0.9620, Validation Accuracy:0.5255\n",
    "Epoch #32: Loss:0.9320, Accuracy:0.5343, Validation Loss:0.9640, Validation Accuracy:0.5222\n",
    "Epoch #33: Loss:0.9228, Accuracy:0.5626, Validation Loss:0.9614, Validation Accuracy:0.5189\n",
    "Epoch #34: Loss:0.9215, Accuracy:0.5302, Validation Loss:0.9681, Validation Accuracy:0.5353\n",
    "Epoch #35: Loss:0.9264, Accuracy:0.5310, Validation Loss:0.9572, Validation Accuracy:0.5304\n",
    "Epoch #36: Loss:0.9196, Accuracy:0.5474, Validation Loss:0.9558, Validation Accuracy:0.5238\n",
    "Epoch #37: Loss:0.9210, Accuracy:0.5425, Validation Loss:0.9552, Validation Accuracy:0.5238\n",
    "Epoch #38: Loss:0.9181, Accuracy:0.5454, Validation Loss:0.9544, Validation Accuracy:0.5255\n",
    "Epoch #39: Loss:0.9172, Accuracy:0.5355, Validation Loss:0.9542, Validation Accuracy:0.5238\n",
    "Epoch #40: Loss:0.9169, Accuracy:0.5474, Validation Loss:0.9538, Validation Accuracy:0.5271\n",
    "Epoch #41: Loss:0.9139, Accuracy:0.5363, Validation Loss:0.9531, Validation Accuracy:0.5320\n",
    "Epoch #42: Loss:0.9099, Accuracy:0.5478, Validation Loss:0.9521, Validation Accuracy:0.5255\n",
    "Epoch #43: Loss:0.9084, Accuracy:0.5483, Validation Loss:0.9524, Validation Accuracy:0.5337\n",
    "Epoch #44: Loss:0.9074, Accuracy:0.5499, Validation Loss:0.9541, Validation Accuracy:0.5337\n",
    "Epoch #45: Loss:0.9069, Accuracy:0.5487, Validation Loss:0.9535, Validation Accuracy:0.5287\n",
    "Epoch #46: Loss:0.9066, Accuracy:0.5450, Validation Loss:0.9519, Validation Accuracy:0.5353\n",
    "Epoch #47: Loss:0.9061, Accuracy:0.5520, Validation Loss:0.9519, Validation Accuracy:0.5255\n",
    "Epoch #48: Loss:0.9052, Accuracy:0.5405, Validation Loss:0.9527, Validation Accuracy:0.5353\n",
    "Epoch #49: Loss:0.9053, Accuracy:0.5536, Validation Loss:0.9524, Validation Accuracy:0.5304\n",
    "Epoch #50: Loss:0.9041, Accuracy:0.5503, Validation Loss:0.9525, Validation Accuracy:0.5304\n",
    "Epoch #51: Loss:0.9033, Accuracy:0.5491, Validation Loss:0.9515, Validation Accuracy:0.5337\n",
    "Epoch #52: Loss:0.9030, Accuracy:0.5536, Validation Loss:0.9515, Validation Accuracy:0.5353\n",
    "Epoch #53: Loss:0.9034, Accuracy:0.5487, Validation Loss:0.9525, Validation Accuracy:0.5402\n",
    "Epoch #54: Loss:0.9028, Accuracy:0.5565, Validation Loss:0.9573, Validation Accuracy:0.5353\n",
    "Epoch #55: Loss:0.9047, Accuracy:0.5470, Validation Loss:0.9528, Validation Accuracy:0.5369\n",
    "Epoch #56: Loss:0.9029, Accuracy:0.5495, Validation Loss:0.9510, Validation Accuracy:0.5386\n",
    "Epoch #57: Loss:0.9001, Accuracy:0.5556, Validation Loss:0.9527, Validation Accuracy:0.5369\n",
    "Epoch #58: Loss:0.9011, Accuracy:0.5552, Validation Loss:0.9516, Validation Accuracy:0.5369\n",
    "Epoch #59: Loss:0.9003, Accuracy:0.5495, Validation Loss:0.9511, Validation Accuracy:0.5435\n",
    "Epoch #60: Loss:0.9000, Accuracy:0.5450, Validation Loss:0.9499, Validation Accuracy:0.5419\n",
    "Epoch #61: Loss:0.8985, Accuracy:0.5565, Validation Loss:0.9495, Validation Accuracy:0.5452\n",
    "Epoch #62: Loss:0.9005, Accuracy:0.5511, Validation Loss:0.9493, Validation Accuracy:0.5402\n",
    "Epoch #63: Loss:0.8978, Accuracy:0.5536, Validation Loss:0.9490, Validation Accuracy:0.5435\n",
    "Epoch #64: Loss:0.9002, Accuracy:0.5585, Validation Loss:0.9634, Validation Accuracy:0.5402\n",
    "Epoch #65: Loss:0.9073, Accuracy:0.5388, Validation Loss:0.9605, Validation Accuracy:0.5369\n",
    "Epoch #66: Loss:0.9059, Accuracy:0.5388, Validation Loss:0.9471, Validation Accuracy:0.5419\n",
    "Epoch #67: Loss:0.8987, Accuracy:0.5602, Validation Loss:0.9506, Validation Accuracy:0.5435\n",
    "Epoch #68: Loss:0.9008, Accuracy:0.5507, Validation Loss:0.9484, Validation Accuracy:0.5386\n",
    "Epoch #69: Loss:0.8978, Accuracy:0.5544, Validation Loss:0.9471, Validation Accuracy:0.5435\n",
    "Epoch #70: Loss:0.8968, Accuracy:0.5573, Validation Loss:0.9479, Validation Accuracy:0.5435\n",
    "Epoch #71: Loss:0.8944, Accuracy:0.5602, Validation Loss:0.9488, Validation Accuracy:0.5419\n",
    "Epoch #72: Loss:0.8952, Accuracy:0.5556, Validation Loss:0.9502, Validation Accuracy:0.5402\n",
    "Epoch #73: Loss:0.8938, Accuracy:0.5569, Validation Loss:0.9474, Validation Accuracy:0.5452\n",
    "Epoch #74: Loss:0.8932, Accuracy:0.5602, Validation Loss:0.9460, Validation Accuracy:0.5534\n",
    "Epoch #75: Loss:0.8946, Accuracy:0.5610, Validation Loss:0.9480, Validation Accuracy:0.5386\n",
    "Epoch #76: Loss:0.8976, Accuracy:0.5561, Validation Loss:0.9692, Validation Accuracy:0.5353\n",
    "Epoch #77: Loss:0.9059, Accuracy:0.5540, Validation Loss:0.9520, Validation Accuracy:0.5353\n",
    "Epoch #78: Loss:0.8965, Accuracy:0.5470, Validation Loss:0.9468, Validation Accuracy:0.5468\n",
    "Epoch #79: Loss:0.8936, Accuracy:0.5671, Validation Loss:0.9471, Validation Accuracy:0.5435\n",
    "Epoch #80: Loss:0.8915, Accuracy:0.5585, Validation Loss:0.9466, Validation Accuracy:0.5402\n",
    "Epoch #81: Loss:0.8937, Accuracy:0.5565, Validation Loss:0.9484, Validation Accuracy:0.5304\n",
    "Epoch #82: Loss:0.8924, Accuracy:0.5643, Validation Loss:0.9467, Validation Accuracy:0.5435\n",
    "Epoch #83: Loss:0.8911, Accuracy:0.5643, Validation Loss:0.9460, Validation Accuracy:0.5468\n",
    "Epoch #84: Loss:0.8902, Accuracy:0.5598, Validation Loss:0.9471, Validation Accuracy:0.5386\n",
    "Epoch #85: Loss:0.8903, Accuracy:0.5659, Validation Loss:0.9513, Validation Accuracy:0.5435\n",
    "Epoch #86: Loss:0.8898, Accuracy:0.5598, Validation Loss:0.9445, Validation Accuracy:0.5501\n",
    "Epoch #87: Loss:0.8899, Accuracy:0.5626, Validation Loss:0.9451, Validation Accuracy:0.5435\n",
    "Epoch #88: Loss:0.8873, Accuracy:0.5651, Validation Loss:0.9458, Validation Accuracy:0.5402\n",
    "Epoch #89: Loss:0.8877, Accuracy:0.5610, Validation Loss:0.9444, Validation Accuracy:0.5534\n",
    "Epoch #90: Loss:0.8872, Accuracy:0.5634, Validation Loss:0.9449, Validation Accuracy:0.5435\n",
    "Epoch #91: Loss:0.8861, Accuracy:0.5643, Validation Loss:0.9443, Validation Accuracy:0.5534\n",
    "Epoch #92: Loss:0.8865, Accuracy:0.5671, Validation Loss:0.9455, Validation Accuracy:0.5468\n",
    "Epoch #93: Loss:0.8925, Accuracy:0.5618, Validation Loss:0.9502, Validation Accuracy:0.5435\n",
    "Epoch #94: Loss:0.8933, Accuracy:0.5581, Validation Loss:0.9497, Validation Accuracy:0.5353\n",
    "Epoch #95: Loss:0.8901, Accuracy:0.5581, Validation Loss:0.9434, Validation Accuracy:0.5369\n",
    "Epoch #96: Loss:0.8867, Accuracy:0.5651, Validation Loss:0.9478, Validation Accuracy:0.5435\n",
    "Epoch #97: Loss:0.8836, Accuracy:0.5618, Validation Loss:0.9540, Validation Accuracy:0.5353\n",
    "Epoch #98: Loss:0.8958, Accuracy:0.5495, Validation Loss:0.9438, Validation Accuracy:0.5419\n",
    "Epoch #99: Loss:0.8953, Accuracy:0.5651, Validation Loss:0.9787, Validation Accuracy:0.5484\n",
    "Epoch #100: Loss:0.9054, Accuracy:0.5565, Validation Loss:0.9487, Validation Accuracy:0.5435\n",
    "Epoch #101: Loss:0.8944, Accuracy:0.5598, Validation Loss:0.9406, Validation Accuracy:0.5468\n",
    "Epoch #102: Loss:0.8916, Accuracy:0.5708, Validation Loss:0.9475, Validation Accuracy:0.5452\n",
    "Epoch #103: Loss:0.8925, Accuracy:0.5569, Validation Loss:0.9515, Validation Accuracy:0.5304\n",
    "Epoch #104: Loss:0.8900, Accuracy:0.5614, Validation Loss:0.9467, Validation Accuracy:0.5468\n",
    "Epoch #105: Loss:0.8839, Accuracy:0.5565, Validation Loss:0.9457, Validation Accuracy:0.5484\n",
    "Epoch #106: Loss:0.8897, Accuracy:0.5622, Validation Loss:0.9445, Validation Accuracy:0.5534\n",
    "Epoch #107: Loss:0.8854, Accuracy:0.5721, Validation Loss:0.9446, Validation Accuracy:0.5501\n",
    "Epoch #108: Loss:0.8833, Accuracy:0.5634, Validation Loss:0.9435, Validation Accuracy:0.5517\n",
    "Epoch #109: Loss:0.8836, Accuracy:0.5680, Validation Loss:0.9448, Validation Accuracy:0.5402\n",
    "Epoch #110: Loss:0.8829, Accuracy:0.5643, Validation Loss:0.9437, Validation Accuracy:0.5435\n",
    "Epoch #111: Loss:0.8883, Accuracy:0.5634, Validation Loss:0.9434, Validation Accuracy:0.5484\n",
    "Epoch #112: Loss:0.8839, Accuracy:0.5639, Validation Loss:0.9535, Validation Accuracy:0.5468\n",
    "Epoch #113: Loss:0.8865, Accuracy:0.5544, Validation Loss:0.9554, Validation Accuracy:0.5337\n",
    "Epoch #114: Loss:0.8883, Accuracy:0.5618, Validation Loss:0.9470, Validation Accuracy:0.5452\n",
    "Epoch #115: Loss:0.8829, Accuracy:0.5684, Validation Loss:0.9417, Validation Accuracy:0.5435\n",
    "Epoch #116: Loss:0.8813, Accuracy:0.5614, Validation Loss:0.9448, Validation Accuracy:0.5452\n",
    "Epoch #117: Loss:0.8812, Accuracy:0.5651, Validation Loss:0.9446, Validation Accuracy:0.5435\n",
    "Epoch #118: Loss:0.8820, Accuracy:0.5622, Validation Loss:0.9449, Validation Accuracy:0.5517\n",
    "Epoch #119: Loss:0.8814, Accuracy:0.5663, Validation Loss:0.9445, Validation Accuracy:0.5468\n",
    "Epoch #120: Loss:0.8824, Accuracy:0.5647, Validation Loss:0.9497, Validation Accuracy:0.5452\n",
    "Epoch #121: Loss:0.8844, Accuracy:0.5577, Validation Loss:0.9414, Validation Accuracy:0.5452\n",
    "Epoch #122: Loss:0.8803, Accuracy:0.5692, Validation Loss:0.9420, Validation Accuracy:0.5452\n",
    "Epoch #123: Loss:0.8796, Accuracy:0.5630, Validation Loss:0.9425, Validation Accuracy:0.5435\n",
    "Epoch #124: Loss:0.8795, Accuracy:0.5659, Validation Loss:0.9430, Validation Accuracy:0.5435\n",
    "Epoch #125: Loss:0.8790, Accuracy:0.5663, Validation Loss:0.9464, Validation Accuracy:0.5419\n",
    "Epoch #126: Loss:0.8837, Accuracy:0.5593, Validation Loss:0.9543, Validation Accuracy:0.5468\n",
    "Epoch #127: Loss:0.8806, Accuracy:0.5676, Validation Loss:0.9469, Validation Accuracy:0.5517\n",
    "Epoch #128: Loss:0.8845, Accuracy:0.5618, Validation Loss:0.9418, Validation Accuracy:0.5452\n",
    "Epoch #129: Loss:0.8854, Accuracy:0.5639, Validation Loss:0.9528, Validation Accuracy:0.5517\n",
    "Epoch #130: Loss:0.8812, Accuracy:0.5643, Validation Loss:0.9438, Validation Accuracy:0.5501\n",
    "Epoch #131: Loss:0.8787, Accuracy:0.5671, Validation Loss:0.9436, Validation Accuracy:0.5419\n",
    "Epoch #132: Loss:0.8792, Accuracy:0.5659, Validation Loss:0.9476, Validation Accuracy:0.5419\n",
    "Epoch #133: Loss:0.8793, Accuracy:0.5676, Validation Loss:0.9457, Validation Accuracy:0.5501\n",
    "Epoch #134: Loss:0.8818, Accuracy:0.5585, Validation Loss:0.9418, Validation Accuracy:0.5484\n",
    "Epoch #135: Loss:0.8822, Accuracy:0.5717, Validation Loss:0.9478, Validation Accuracy:0.5435\n",
    "Epoch #136: Loss:0.8797, Accuracy:0.5663, Validation Loss:0.9426, Validation Accuracy:0.5435\n",
    "Epoch #137: Loss:0.8778, Accuracy:0.5659, Validation Loss:0.9489, Validation Accuracy:0.5468\n",
    "Epoch #138: Loss:0.8830, Accuracy:0.5630, Validation Loss:0.9640, Validation Accuracy:0.5452\n",
    "Epoch #139: Loss:0.8843, Accuracy:0.5680, Validation Loss:0.9428, Validation Accuracy:0.5468\n",
    "Epoch #140: Loss:0.8783, Accuracy:0.5639, Validation Loss:0.9445, Validation Accuracy:0.5468\n",
    "Epoch #141: Loss:0.8776, Accuracy:0.5676, Validation Loss:0.9452, Validation Accuracy:0.5402\n",
    "Epoch #142: Loss:0.8786, Accuracy:0.5643, Validation Loss:0.9434, Validation Accuracy:0.5452\n",
    "Epoch #143: Loss:0.8768, Accuracy:0.5667, Validation Loss:0.9457, Validation Accuracy:0.5501\n",
    "Epoch #144: Loss:0.8814, Accuracy:0.5667, Validation Loss:0.9454, Validation Accuracy:0.5402\n",
    "Epoch #145: Loss:0.8790, Accuracy:0.5680, Validation Loss:0.9414, Validation Accuracy:0.5452\n",
    "Epoch #146: Loss:0.8813, Accuracy:0.5676, Validation Loss:0.9511, Validation Accuracy:0.5419\n",
    "Epoch #147: Loss:0.8854, Accuracy:0.5581, Validation Loss:0.9429, Validation Accuracy:0.5501\n",
    "Epoch #148: Loss:0.8756, Accuracy:0.5659, Validation Loss:0.9420, Validation Accuracy:0.5435\n",
    "Epoch #149: Loss:0.8764, Accuracy:0.5659, Validation Loss:0.9425, Validation Accuracy:0.5484\n",
    "Epoch #150: Loss:0.8782, Accuracy:0.5663, Validation Loss:0.9440, Validation Accuracy:0.5419\n",
    "Epoch #151: Loss:0.8761, Accuracy:0.5651, Validation Loss:0.9436, Validation Accuracy:0.5468\n",
    "Epoch #152: Loss:0.8766, Accuracy:0.5602, Validation Loss:0.9434, Validation Accuracy:0.5468\n",
    "Epoch #153: Loss:0.8772, Accuracy:0.5651, Validation Loss:0.9519, Validation Accuracy:0.5484\n",
    "Epoch #154: Loss:0.8831, Accuracy:0.5598, Validation Loss:0.9409, Validation Accuracy:0.5484\n",
    "Epoch #155: Loss:0.8760, Accuracy:0.5671, Validation Loss:0.9431, Validation Accuracy:0.5550\n",
    "Epoch #156: Loss:0.8779, Accuracy:0.5647, Validation Loss:0.9427, Validation Accuracy:0.5386\n",
    "Epoch #157: Loss:0.8752, Accuracy:0.5667, Validation Loss:0.9447, Validation Accuracy:0.5452\n",
    "Epoch #158: Loss:0.8763, Accuracy:0.5651, Validation Loss:0.9518, Validation Accuracy:0.5484\n",
    "Epoch #159: Loss:0.8807, Accuracy:0.5663, Validation Loss:0.9425, Validation Accuracy:0.5435\n",
    "Epoch #160: Loss:0.8761, Accuracy:0.5634, Validation Loss:0.9444, Validation Accuracy:0.5517\n",
    "Epoch #161: Loss:0.8765, Accuracy:0.5647, Validation Loss:0.9445, Validation Accuracy:0.5468\n",
    "Epoch #162: Loss:0.8776, Accuracy:0.5630, Validation Loss:0.9426, Validation Accuracy:0.5468\n",
    "Epoch #163: Loss:0.8725, Accuracy:0.5667, Validation Loss:0.9492, Validation Accuracy:0.5452\n",
    "Epoch #164: Loss:0.8758, Accuracy:0.5676, Validation Loss:0.9436, Validation Accuracy:0.5435\n",
    "Epoch #165: Loss:0.8726, Accuracy:0.5663, Validation Loss:0.9407, Validation Accuracy:0.5468\n",
    "Epoch #166: Loss:0.8737, Accuracy:0.5700, Validation Loss:0.9434, Validation Accuracy:0.5517\n",
    "Epoch #167: Loss:0.8755, Accuracy:0.5618, Validation Loss:0.9494, Validation Accuracy:0.5435\n",
    "Epoch #168: Loss:0.8763, Accuracy:0.5667, Validation Loss:0.9449, Validation Accuracy:0.5435\n",
    "Epoch #169: Loss:0.8725, Accuracy:0.5639, Validation Loss:0.9408, Validation Accuracy:0.5583\n",
    "Epoch #170: Loss:0.8725, Accuracy:0.5655, Validation Loss:0.9406, Validation Accuracy:0.5501\n",
    "Epoch #171: Loss:0.8721, Accuracy:0.5639, Validation Loss:0.9506, Validation Accuracy:0.5337\n",
    "Epoch #172: Loss:0.8752, Accuracy:0.5671, Validation Loss:0.9431, Validation Accuracy:0.5435\n",
    "Epoch #173: Loss:0.8740, Accuracy:0.5663, Validation Loss:0.9438, Validation Accuracy:0.5484\n",
    "Epoch #174: Loss:0.8752, Accuracy:0.5614, Validation Loss:0.9414, Validation Accuracy:0.5501\n",
    "Epoch #175: Loss:0.8763, Accuracy:0.5692, Validation Loss:0.9452, Validation Accuracy:0.5435\n",
    "Epoch #176: Loss:0.8779, Accuracy:0.5713, Validation Loss:0.9462, Validation Accuracy:0.5501\n",
    "Epoch #177: Loss:0.8745, Accuracy:0.5676, Validation Loss:0.9404, Validation Accuracy:0.5550\n",
    "Epoch #178: Loss:0.8709, Accuracy:0.5659, Validation Loss:0.9435, Validation Accuracy:0.5484\n",
    "Epoch #179: Loss:0.8729, Accuracy:0.5671, Validation Loss:0.9753, Validation Accuracy:0.5468\n",
    "Epoch #180: Loss:0.8806, Accuracy:0.5622, Validation Loss:0.9430, Validation Accuracy:0.5435\n",
    "Epoch #181: Loss:0.8758, Accuracy:0.5667, Validation Loss:0.9379, Validation Accuracy:0.5550\n",
    "Epoch #182: Loss:0.8790, Accuracy:0.5651, Validation Loss:0.9445, Validation Accuracy:0.5501\n",
    "Epoch #183: Loss:0.8745, Accuracy:0.5643, Validation Loss:0.9455, Validation Accuracy:0.5468\n",
    "Epoch #184: Loss:0.8750, Accuracy:0.5626, Validation Loss:0.9479, Validation Accuracy:0.5468\n",
    "Epoch #185: Loss:0.8749, Accuracy:0.5684, Validation Loss:0.9481, Validation Accuracy:0.5435\n",
    "Epoch #186: Loss:0.8746, Accuracy:0.5643, Validation Loss:0.9410, Validation Accuracy:0.5484\n",
    "Epoch #187: Loss:0.8730, Accuracy:0.5655, Validation Loss:0.9430, Validation Accuracy:0.5452\n",
    "Epoch #188: Loss:0.8721, Accuracy:0.5639, Validation Loss:0.9456, Validation Accuracy:0.5402\n",
    "Epoch #189: Loss:0.8729, Accuracy:0.5655, Validation Loss:0.9457, Validation Accuracy:0.5468\n",
    "Epoch #190: Loss:0.8715, Accuracy:0.5655, Validation Loss:0.9400, Validation Accuracy:0.5484\n",
    "Epoch #191: Loss:0.8704, Accuracy:0.5630, Validation Loss:0.9478, Validation Accuracy:0.5402\n",
    "Epoch #192: Loss:0.8720, Accuracy:0.5622, Validation Loss:0.9437, Validation Accuracy:0.5452\n",
    "Epoch #193: Loss:0.8689, Accuracy:0.5684, Validation Loss:0.9413, Validation Accuracy:0.5402\n",
    "Epoch #194: Loss:0.8681, Accuracy:0.5659, Validation Loss:0.9448, Validation Accuracy:0.5452\n",
    "Epoch #195: Loss:0.8717, Accuracy:0.5647, Validation Loss:0.9468, Validation Accuracy:0.5386\n",
    "Epoch #196: Loss:0.8697, Accuracy:0.5630, Validation Loss:0.9417, Validation Accuracy:0.5484\n",
    "Epoch #197: Loss:0.8742, Accuracy:0.5721, Validation Loss:0.9395, Validation Accuracy:0.5468\n",
    "Epoch #198: Loss:0.8681, Accuracy:0.5659, Validation Loss:0.9403, Validation Accuracy:0.5419\n",
    "Epoch #199: Loss:0.8687, Accuracy:0.5696, Validation Loss:0.9428, Validation Accuracy:0.5435\n",
    "Epoch #200: Loss:0.8698, Accuracy:0.5667, Validation Loss:0.9446, Validation Accuracy:0.5435\n",
    "Epoch #201: Loss:0.8685, Accuracy:0.5647, Validation Loss:0.9431, Validation Accuracy:0.5419\n",
    "Epoch #202: Loss:0.8723, Accuracy:0.5676, Validation Loss:0.9457, Validation Accuracy:0.5402\n",
    "Epoch #203: Loss:0.8769, Accuracy:0.5630, Validation Loss:0.9456, Validation Accuracy:0.5435\n",
    "Epoch #204: Loss:0.8784, Accuracy:0.5692, Validation Loss:0.9510, Validation Accuracy:0.5517\n",
    "Epoch #205: Loss:0.8710, Accuracy:0.5688, Validation Loss:0.9398, Validation Accuracy:0.5435\n",
    "Epoch #206: Loss:0.8693, Accuracy:0.5667, Validation Loss:0.9450, Validation Accuracy:0.5550\n",
    "Epoch #207: Loss:0.8710, Accuracy:0.5569, Validation Loss:0.9452, Validation Accuracy:0.5386\n",
    "Epoch #208: Loss:0.8700, Accuracy:0.5667, Validation Loss:0.9375, Validation Accuracy:0.5550\n",
    "Epoch #209: Loss:0.8689, Accuracy:0.5663, Validation Loss:0.9410, Validation Accuracy:0.5419\n",
    "Epoch #210: Loss:0.8671, Accuracy:0.5729, Validation Loss:0.9514, Validation Accuracy:0.5484\n",
    "Epoch #211: Loss:0.8735, Accuracy:0.5684, Validation Loss:0.9406, Validation Accuracy:0.5452\n",
    "Epoch #212: Loss:0.8670, Accuracy:0.5634, Validation Loss:0.9433, Validation Accuracy:0.5468\n",
    "Epoch #213: Loss:0.8669, Accuracy:0.5659, Validation Loss:0.9391, Validation Accuracy:0.5435\n",
    "Epoch #214: Loss:0.8688, Accuracy:0.5671, Validation Loss:0.9393, Validation Accuracy:0.5419\n",
    "Epoch #215: Loss:0.8680, Accuracy:0.5659, Validation Loss:0.9432, Validation Accuracy:0.5567\n",
    "Epoch #216: Loss:0.8661, Accuracy:0.5700, Validation Loss:0.9417, Validation Accuracy:0.5435\n",
    "Epoch #217: Loss:0.8692, Accuracy:0.5647, Validation Loss:0.9481, Validation Accuracy:0.5353\n",
    "Epoch #218: Loss:0.8680, Accuracy:0.5725, Validation Loss:0.9541, Validation Accuracy:0.5419\n",
    "Epoch #219: Loss:0.8715, Accuracy:0.5741, Validation Loss:0.9414, Validation Accuracy:0.5484\n",
    "Epoch #220: Loss:0.8674, Accuracy:0.5622, Validation Loss:0.9381, Validation Accuracy:0.5501\n",
    "Epoch #221: Loss:0.8669, Accuracy:0.5667, Validation Loss:0.9429, Validation Accuracy:0.5468\n",
    "Epoch #222: Loss:0.8663, Accuracy:0.5696, Validation Loss:0.9402, Validation Accuracy:0.5501\n",
    "Epoch #223: Loss:0.8648, Accuracy:0.5651, Validation Loss:0.9404, Validation Accuracy:0.5468\n",
    "Epoch #224: Loss:0.8664, Accuracy:0.5639, Validation Loss:0.9411, Validation Accuracy:0.5484\n",
    "Epoch #225: Loss:0.8659, Accuracy:0.5671, Validation Loss:0.9420, Validation Accuracy:0.5484\n",
    "Epoch #226: Loss:0.8659, Accuracy:0.5680, Validation Loss:0.9374, Validation Accuracy:0.5484\n",
    "Epoch #227: Loss:0.8658, Accuracy:0.5655, Validation Loss:0.9370, Validation Accuracy:0.5452\n",
    "Epoch #228: Loss:0.8644, Accuracy:0.5647, Validation Loss:0.9403, Validation Accuracy:0.5468\n",
    "Epoch #229: Loss:0.8649, Accuracy:0.5692, Validation Loss:0.9400, Validation Accuracy:0.5419\n",
    "Epoch #230: Loss:0.8638, Accuracy:0.5700, Validation Loss:0.9380, Validation Accuracy:0.5468\n",
    "Epoch #231: Loss:0.8631, Accuracy:0.5717, Validation Loss:0.9403, Validation Accuracy:0.5452\n",
    "Epoch #232: Loss:0.8636, Accuracy:0.5671, Validation Loss:0.9380, Validation Accuracy:0.5484\n",
    "Epoch #233: Loss:0.8625, Accuracy:0.5667, Validation Loss:0.9381, Validation Accuracy:0.5550\n",
    "Epoch #234: Loss:0.8627, Accuracy:0.5643, Validation Loss:0.9408, Validation Accuracy:0.5517\n",
    "Epoch #235: Loss:0.8627, Accuracy:0.5713, Validation Loss:0.9383, Validation Accuracy:0.5402\n",
    "Epoch #236: Loss:0.8697, Accuracy:0.5680, Validation Loss:0.9367, Validation Accuracy:0.5534\n",
    "Epoch #237: Loss:0.8661, Accuracy:0.5667, Validation Loss:0.9518, Validation Accuracy:0.5501\n",
    "Epoch #238: Loss:0.8671, Accuracy:0.5774, Validation Loss:0.9384, Validation Accuracy:0.5468\n",
    "Epoch #239: Loss:0.8686, Accuracy:0.5721, Validation Loss:0.9510, Validation Accuracy:0.5238\n",
    "Epoch #240: Loss:0.8735, Accuracy:0.5626, Validation Loss:0.9383, Validation Accuracy:0.5632\n",
    "Epoch #241: Loss:0.8636, Accuracy:0.5725, Validation Loss:0.9385, Validation Accuracy:0.5501\n",
    "Epoch #242: Loss:0.8621, Accuracy:0.5684, Validation Loss:0.9380, Validation Accuracy:0.5468\n",
    "Epoch #243: Loss:0.8621, Accuracy:0.5671, Validation Loss:0.9452, Validation Accuracy:0.5419\n",
    "Epoch #244: Loss:0.8647, Accuracy:0.5696, Validation Loss:0.9376, Validation Accuracy:0.5517\n",
    "Epoch #245: Loss:0.8646, Accuracy:0.5692, Validation Loss:0.9397, Validation Accuracy:0.5501\n",
    "Epoch #246: Loss:0.8652, Accuracy:0.5676, Validation Loss:0.9417, Validation Accuracy:0.5567\n",
    "Epoch #247: Loss:0.8665, Accuracy:0.5696, Validation Loss:0.9431, Validation Accuracy:0.5484\n",
    "Epoch #248: Loss:0.8628, Accuracy:0.5721, Validation Loss:0.9361, Validation Accuracy:0.5435\n",
    "Epoch #249: Loss:0.8594, Accuracy:0.5676, Validation Loss:0.9374, Validation Accuracy:0.5484\n",
    "Epoch #250: Loss:0.8602, Accuracy:0.5684, Validation Loss:0.9365, Validation Accuracy:0.5534\n",
    "Epoch #251: Loss:0.8597, Accuracy:0.5704, Validation Loss:0.9359, Validation Accuracy:0.5435\n",
    "Epoch #252: Loss:0.8639, Accuracy:0.5708, Validation Loss:0.9374, Validation Accuracy:0.5550\n",
    "Epoch #253: Loss:0.8638, Accuracy:0.5704, Validation Loss:0.9541, Validation Accuracy:0.5435\n",
    "Epoch #254: Loss:0.8630, Accuracy:0.5684, Validation Loss:0.9354, Validation Accuracy:0.5534\n",
    "Epoch #255: Loss:0.8619, Accuracy:0.5692, Validation Loss:0.9426, Validation Accuracy:0.5402\n",
    "Epoch #256: Loss:0.8659, Accuracy:0.5692, Validation Loss:0.9678, Validation Accuracy:0.5550\n",
    "Epoch #257: Loss:0.8735, Accuracy:0.5659, Validation Loss:0.9389, Validation Accuracy:0.5435\n",
    "Epoch #258: Loss:0.8661, Accuracy:0.5655, Validation Loss:0.9340, Validation Accuracy:0.5419\n",
    "Epoch #259: Loss:0.8621, Accuracy:0.5749, Validation Loss:0.9394, Validation Accuracy:0.5484\n",
    "Epoch #260: Loss:0.8634, Accuracy:0.5725, Validation Loss:0.9366, Validation Accuracy:0.5452\n",
    "Epoch #261: Loss:0.8591, Accuracy:0.5667, Validation Loss:0.9357, Validation Accuracy:0.5517\n",
    "Epoch #262: Loss:0.8615, Accuracy:0.5696, Validation Loss:0.9347, Validation Accuracy:0.5534\n",
    "Epoch #263: Loss:0.8673, Accuracy:0.5614, Validation Loss:0.9552, Validation Accuracy:0.5287\n",
    "Epoch #264: Loss:0.8744, Accuracy:0.5622, Validation Loss:0.9417, Validation Accuracy:0.5501\n",
    "Epoch #265: Loss:0.8643, Accuracy:0.5630, Validation Loss:0.9330, Validation Accuracy:0.5452\n",
    "Epoch #266: Loss:0.8633, Accuracy:0.5643, Validation Loss:0.9358, Validation Accuracy:0.5501\n",
    "Epoch #267: Loss:0.8625, Accuracy:0.5692, Validation Loss:0.9380, Validation Accuracy:0.5468\n",
    "Epoch #268: Loss:0.8581, Accuracy:0.5704, Validation Loss:0.9368, Validation Accuracy:0.5517\n",
    "Epoch #269: Loss:0.8586, Accuracy:0.5700, Validation Loss:0.9331, Validation Accuracy:0.5501\n",
    "Epoch #270: Loss:0.8577, Accuracy:0.5676, Validation Loss:0.9318, Validation Accuracy:0.5484\n",
    "Epoch #271: Loss:0.8581, Accuracy:0.5713, Validation Loss:0.9385, Validation Accuracy:0.5501\n",
    "Epoch #272: Loss:0.8581, Accuracy:0.5713, Validation Loss:0.9354, Validation Accuracy:0.5501\n",
    "Epoch #273: Loss:0.8568, Accuracy:0.5717, Validation Loss:0.9335, Validation Accuracy:0.5468\n",
    "Epoch #274: Loss:0.8583, Accuracy:0.5680, Validation Loss:0.9426, Validation Accuracy:0.5419\n",
    "Epoch #275: Loss:0.8597, Accuracy:0.5786, Validation Loss:0.9377, Validation Accuracy:0.5501\n",
    "Epoch #276: Loss:0.8602, Accuracy:0.5737, Validation Loss:0.9344, Validation Accuracy:0.5468\n",
    "Epoch #277: Loss:0.8604, Accuracy:0.5680, Validation Loss:0.9422, Validation Accuracy:0.5452\n",
    "Epoch #278: Loss:0.8628, Accuracy:0.5721, Validation Loss:0.9315, Validation Accuracy:0.5534\n",
    "Epoch #279: Loss:0.8558, Accuracy:0.5708, Validation Loss:0.9328, Validation Accuracy:0.5484\n",
    "Epoch #280: Loss:0.8586, Accuracy:0.5766, Validation Loss:0.9350, Validation Accuracy:0.5517\n",
    "Epoch #281: Loss:0.8579, Accuracy:0.5721, Validation Loss:0.9290, Validation Accuracy:0.5452\n",
    "Epoch #282: Loss:0.8550, Accuracy:0.5704, Validation Loss:0.9316, Validation Accuracy:0.5484\n",
    "Epoch #283: Loss:0.8585, Accuracy:0.5692, Validation Loss:0.9348, Validation Accuracy:0.5452\n",
    "Epoch #284: Loss:0.8582, Accuracy:0.5733, Validation Loss:0.9326, Validation Accuracy:0.5484\n",
    "Epoch #285: Loss:0.8593, Accuracy:0.5704, Validation Loss:0.9348, Validation Accuracy:0.5550\n",
    "Epoch #286: Loss:0.8566, Accuracy:0.5717, Validation Loss:0.9304, Validation Accuracy:0.5484\n",
    "Epoch #287: Loss:0.8546, Accuracy:0.5713, Validation Loss:0.9337, Validation Accuracy:0.5468\n",
    "Epoch #288: Loss:0.8545, Accuracy:0.5696, Validation Loss:0.9335, Validation Accuracy:0.5583\n",
    "Epoch #289: Loss:0.8560, Accuracy:0.5733, Validation Loss:0.9319, Validation Accuracy:0.5468\n",
    "Epoch #290: Loss:0.8550, Accuracy:0.5745, Validation Loss:0.9291, Validation Accuracy:0.5452\n",
    "Epoch #291: Loss:0.8537, Accuracy:0.5729, Validation Loss:0.9293, Validation Accuracy:0.5484\n",
    "Epoch #292: Loss:0.8531, Accuracy:0.5770, Validation Loss:0.9300, Validation Accuracy:0.5484\n",
    "Epoch #293: Loss:0.8532, Accuracy:0.5774, Validation Loss:0.9426, Validation Accuracy:0.5386\n",
    "Epoch #294: Loss:0.8540, Accuracy:0.5754, Validation Loss:0.9299, Validation Accuracy:0.5501\n",
    "Epoch #295: Loss:0.8536, Accuracy:0.5700, Validation Loss:0.9273, Validation Accuracy:0.5484\n",
    "Epoch #296: Loss:0.8555, Accuracy:0.5733, Validation Loss:0.9290, Validation Accuracy:0.5550\n",
    "Epoch #297: Loss:0.8521, Accuracy:0.5778, Validation Loss:0.9274, Validation Accuracy:0.5468\n",
    "Epoch #298: Loss:0.8541, Accuracy:0.5729, Validation Loss:0.9284, Validation Accuracy:0.5534\n",
    "Epoch #299: Loss:0.8538, Accuracy:0.5737, Validation Loss:0.9272, Validation Accuracy:0.5468\n",
    "Epoch #300: Loss:0.8535, Accuracy:0.5725, Validation Loss:0.9297, Validation Accuracy:0.5435\n",
    "\n",
    "Test:\n",
    "Test Loss:0.92972916, Accuracy:0.5435\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03   01   02\n",
    "t:03  32   93   17\n",
    "t:01   9  154   77\n",
    "t:02   3   79  145\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.73      0.23      0.34       142\n",
    "          01       0.47      0.64      0.54       240\n",
    "          02       0.61      0.64      0.62       227\n",
    "\n",
    "    accuracy                           0.54       609\n",
    "   macro avg       0.60      0.50      0.50       609\n",
    "weighted avg       0.58      0.54      0.53       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 04:49:14 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.086116348581361, 1.0761127503243182, 1.074738509549296, 1.0755388014422262, 1.0756673940101085, 1.074640579803041, 1.0742910258679945, 1.074267688055931, 1.0742435243916628, 1.0741683201641088, 1.074045287955962, 1.0738664098169612, 1.0737144545772783, 1.073519903059272, 1.0732376571555051, 1.0728859948407252, 1.072392765915844, 1.071731278070284, 1.0707052897154208, 1.0691431400615399, 1.0666155813167053, 1.0623538819048401, 1.0545920967468487, 1.0404830848054933, 1.0191709194668799, 0.9980259408504505, 0.9900490488912085, 0.9919637924148923, 0.9854829937757921, 0.9655481240236504, 0.9619815658857473, 0.9639981995858191, 0.9613753671716587, 0.9680847408149043, 0.9572176674903907, 0.9558041918845404, 0.9551696793982156, 0.9543981730252847, 0.9541768655792637, 0.9538020267470912, 0.9531046720756882, 0.9521138132695103, 0.9524350253427752, 0.954102805682591, 0.9535009566394762, 0.9519110879874582, 0.9519266314890192, 0.9527339866791649, 0.9524318044408789, 0.95253833986464, 0.9514918373331843, 0.9514527128052046, 0.9525034191768941, 0.9572594339037176, 0.9528426388018628, 0.9510032022723619, 0.9527264321574632, 0.9516023798724896, 0.951133253734883, 0.9498548921693135, 0.949462841884256, 0.9492846734222324, 0.9489518225878134, 0.9634493953488731, 0.9605060811896238, 0.9471391358203293, 0.9505657531162005, 0.9483911334391689, 0.947080925082534, 0.947856029755572, 0.9487598798740869, 0.9502307248037242, 0.9474302280712598, 0.9459612394005599, 0.9479941050407334, 0.9692481046631223, 0.9519638013957169, 0.9468489133665714, 0.9471078143135472, 0.9466049639853742, 0.9483851517362548, 0.9466980684958459, 0.9459649717866494, 0.9471414623589351, 0.9512768965245076, 0.9445190204579647, 0.9450891043360793, 0.9458323013410583, 0.9443562786371641, 0.9449088591072947, 0.9442688390147705, 0.9454912004016695, 0.9501679404615768, 0.9496554672620175, 0.9434108267276745, 0.9477921829043546, 0.9539792904712884, 0.9437662979652142, 0.9786947796301693, 0.9487327719948366, 0.9405689230693385, 0.9475035042011092, 0.9514502791935587, 0.9467119243736142, 0.9457222429793848, 0.9444892013992974, 0.9445507153501651, 0.9434598834056572, 0.9448494332764537, 0.9437211182317123, 0.9433754208835671, 0.9535279022453258, 0.9554345377756066, 0.9469681068006994, 0.9416657959299134, 0.9447657062698076, 0.9446445191630785, 0.9449176492753679, 0.9444751568224238, 0.9496588382031922, 0.941379101503463, 0.9419942049948844, 0.942531367730233, 0.9430475886819398, 0.9463958514147791, 0.9543349948422662, 0.9468646731674182, 0.9417529340056559, 0.9528334639929785, 0.9437649615096733, 0.9435676621098824, 0.9475784571887237, 0.9457445191632351, 0.9417588568869091, 0.9478048423828163, 0.942618108735296, 0.9489099624121717, 0.9640049328553462, 0.9428220979490108, 0.9444563428952385, 0.9452314114531468, 0.943423564896012, 0.9456803556929276, 0.9454207550520184, 0.9413672465408964, 0.9511402195506103, 0.942928857012531, 0.9419853213581154, 0.9424874823668907, 0.9440170324886178, 0.9435559138288638, 0.9433759993128784, 0.951928442139148, 0.9408673168403174, 0.943142957973167, 0.9426599892685175, 0.9447370089333633, 0.951768110929843, 0.9424947590272024, 0.9444269742480248, 0.9444755499781842, 0.942559369777028, 0.9491534520839823, 0.9435934221803262, 0.9406762435322716, 0.9434017815026157, 0.9494381110460691, 0.9448644232084402, 0.9407628622939825, 0.9406083244799784, 0.9505713359867215, 0.9431363396065184, 0.9437617782888741, 0.9413660120690006, 0.9452456929805048, 0.9462265651214299, 0.9403541046018866, 0.9434776895347683, 0.9752673085882941, 0.9430070198423952, 0.9379071745966455, 0.944459246101442, 0.9454852816310814, 0.9479220876552789, 0.9481415529360716, 0.941013514995575, 0.9429715760235716, 0.9456463487473223, 0.9456803729185721, 0.9399768991031866, 0.9477732912660233, 0.9436676721463259, 0.9412810579309323, 0.9447628233233109, 0.9468495167534927, 0.9416874694119534, 0.9395398935073702, 0.9402808857277305, 0.9427813083080235, 0.9445612521594381, 0.9430776911807569, 0.9457400219976804, 0.9455870952707989, 0.951035815604606, 0.9397553693876282, 0.9449638435601797, 0.9452279758179325, 0.9374822087475819, 0.9410304941958786, 0.9513672310339014, 0.9406005049182472, 0.9432677071670006, 0.9390624874135348, 0.9393051247095631, 0.9432307985811593, 0.9416751472037805, 0.9480759228391601, 0.9540706066466709, 0.9413579266842559, 0.9380747894152436, 0.9428651920093104, 0.9401793959497036, 0.940424816459662, 0.9411247997056871, 0.9419819209571738, 0.9373657285482034, 0.9369672158081543, 0.9403381551232048, 0.9399664776861569, 0.9380140664737996, 0.9402854170509551, 0.9380457606808893, 0.9381364738608424, 0.9407940194720313, 0.9382669942328103, 0.9367011807039258, 0.9517914872059877, 0.9384038032373575, 0.9509603448689278, 0.9382579001691346, 0.9385029351574251, 0.9379627260277033, 0.945221146926504, 0.9376062968094361, 0.9397364894940544, 0.9416900832077553, 0.9430722130343244, 0.9360557741719514, 0.9374003515063444, 0.9364564027300805, 0.9358814670926049, 0.9374380857486443, 0.9540545616439606, 0.935392824006198, 0.9426493637863247, 0.9678423771717278, 0.938901478042352, 0.9340168624284428, 0.9394323962858353, 0.9366302335595067, 0.9357384843583569, 0.9347280603519997, 0.9551815007903501, 0.9416883220813544, 0.9329753665892753, 0.9357518543946528, 0.9380348378605835, 0.9368182018472644, 0.9330963227157718, 0.9318224879712699, 0.9385019566233718, 0.9354448883208539, 0.9334842660548456, 0.9426436267658603, 0.9377000959244464, 0.9344234371615944, 0.9422033316591886, 0.9314621054675974, 0.9327904304064357, 0.9349732649541646, 0.9290014463300971, 0.9316406139403532, 0.9347873672671702, 0.9325675311542693, 0.9347688012522429, 0.9304047466694624, 0.9336845407932263, 0.9335428036883938, 0.9319197149895291, 0.929104097371031, 0.9293003313255623, 0.9299559401369643, 0.9425934646125693, 0.9299211680204019, 0.9273305794679864, 0.9289848916049074, 0.9274087788044721, 0.928449195496163, 0.9271862849421885, 0.9297292683707865], 'val_acc': [0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.4006568126392678, 0.41707717437658015, 0.4022988487640625, 0.4384236441946578, 0.4384236441946578, 0.4614121501375302, 0.4827586204449727, 0.4876847289172299, 0.4729064037940772, 0.47454843991887197, 0.48111658432017795, 0.48440065666764043, 0.48604269289030816, 0.48932676513989765, 0.5254515552364155, 0.5221674832804449, 0.5188834108351095, 0.5353037725724219, 0.5303776641001646, 0.5238095193073667, 0.5238095193073667, 0.5254515554321615, 0.5238095195031127, 0.5270935916548292, 0.5320197000292134, 0.5254515554321615, 0.5336617360561352, 0.5336617362518812, 0.528735627877497, 0.5353037722788029, 0.5254515555300344, 0.5353037722788029, 0.5303776639044187, 0.5303776639044187, 0.5336617361540081, 0.5353037723766759, 0.5402298806531872, 0.535303772083057, 0.5369458084035976, 0.5385878444305194, 0.5369458084035976, 0.5369458083057247, 0.5435139530985226, 0.5418719169737278, 0.5451559889296984, 0.5402298807510602, 0.5435139528049037, 0.5402298805553142, 0.5369458084035976, 0.5418719168758549, 0.5435139527070307, 0.5385878447241383, 0.5435139530985226, 0.5435139530985226, 0.541871916777982, 0.5402298808489331, 0.5451559893211904, 0.5533661694557992, 0.5385878447241383, 0.5353037766341505, 0.5353037722788029, 0.5467980252502391, 0.5435139527070307, 0.5402298808489331, 0.5303776639044187, 0.5435139526091577, 0.5467980249566202, 0.5385878447241383, 0.5435139526091577, 0.5500820974019557, 0.5435139530006496, 0.5402298805553142, 0.5533661696515452, 0.5435139526091577, 0.5533661695536721, 0.5467980250544932, 0.5435139527070307, 0.53530377218093, 0.5369458083057247, 0.5435139526091577, 0.53530377218093, 0.541871916777982, 0.548440061081415, 0.5435139527070307, 0.5467980250544932, 0.5451559889296984, 0.5303776638065457, 0.5467980248587472, 0.5484400611792879, 0.5533661694557992, 0.5500820973040826, 0.5517241335267504, 0.5402298803595682, 0.5435139526091577, 0.5484400611792879, 0.5467980249566202, 0.5336617360561352, 0.545155993382919, 0.5435139527070307, 0.5451559889296984, 0.5435139526091577, 0.5517241334288774, 0.5467980250544932, 0.5451559889296984, 0.545155993382919, 0.5451559887339524, 0.5435139572581242, 0.5435139527070307, 0.541871916582236, 0.5467980248587472, 0.5517241333310045, 0.5451559887339524, 0.5517241332331314, 0.5500820973040826, 0.541871916484363, 0.541871916582236, 0.5500820972062097, 0.5484400609835419, 0.5435139526091577, 0.5435139572581242, 0.5467980248587472, 0.545155993285046, 0.5467980295077137, 0.5467980249566202, 0.5402298850085347, 0.5451559887339524, 0.5500820972062097, 0.5402298803595682, 0.5451559887339524, 0.5418719211333295, 0.5500820972062097, 0.5435139526091577, 0.5484400609835419, 0.5418719211333295, 0.5467980295077137, 0.5467980248587472, 0.5484400655346355, 0.5484400609835419, 0.5550082056784669, 0.53858784888374, 0.5451559888318255, 0.5484400609835419, 0.5435139526091577, 0.551724137882098, 0.5467980248587472, 0.5467980248587472, 0.545155993382919, 0.5435139572581242, 0.5467980295077137, 0.551724137882098, 0.5435139526091577, 0.5435139572581242, 0.5582922778301834, 0.5500820971083367, 0.5336617404114828, 0.5435139572581242, 0.5484400656325085, 0.5500820972062097, 0.5435139572581242, 0.5500820971083367, 0.5550082101316874, 0.5484400656325085, 0.5467980295077137, 0.5435139572581242, 0.5550082101316874, 0.5500820971083367, 0.5467980248587472, 0.5467980293119677, 0.5435139571602513, 0.5484400655346355, 0.5451559887339524, 0.5402298850085347, 0.5467980248587472, 0.5484400609835419, 0.5402298850085347, 0.545155993382919, 0.5402298849106618, 0.545155993382919, 0.53858784888374, 0.5484400609835419, 0.5467980295077137, 0.5418719211333295, 0.5435139572581242, 0.5435139572581242, 0.5418719211333295, 0.5402298848127888, 0.5435139571602513, 0.551724137784225, 0.5435139571602513, 0.5550082057763399, 0.53858784888374, 0.5550082100338145, 0.5418719210354566, 0.5484400655346355, 0.545155993382919, 0.5467980248587472, 0.5435139526091577, 0.5418719210354566, 0.5566502417053887, 0.5435139572581242, 0.5353037763405316, 0.5418719210354566, 0.5484400656325085, 0.5500820971083367, 0.5467980294098408, 0.5500820971083367, 0.5467980294098408, 0.5484400656325085, 0.548440061081415, 0.5484400609835419, 0.545155993285046, 0.5467980295077137, 0.5418719210354566, 0.5467980294098408, 0.545155993382919, 0.5484400656325085, 0.5550082054827209, 0.5517241332331314, 0.5402298849106618, 0.5533661739090198, 0.5500821017573032, 0.5467980295077137, 0.5238095236627144, 0.5632183907556612, 0.5500821017573032, 0.5467980294098408, 0.5418719210354566, 0.551724137882098, 0.5500820971083367, 0.5566502417053887, 0.5484400655346355, 0.5435139571602513, 0.5484400655346355, 0.5533661740068927, 0.5435139571602513, 0.5550082055805939, 0.5435139571602513, 0.5533661693579262, 0.5402298848127888, 0.5550082101316874, 0.5435139569645053, 0.5418719211333295, 0.5484400655346355, 0.5451559887339524, 0.551724137784225, 0.5533661739090198, 0.528735627583878, 0.5500821017573032, 0.545155993382919, 0.5500820972062097, 0.5467980294098408, 0.5517241332331314, 0.5500821016594303, 0.5484400655346355, 0.5500820971083367, 0.5500820971083367, 0.5467980295077137, 0.5418719211333295, 0.5500820971083367, 0.5467980295077137, 0.545155993285046, 0.5533661740068927, 0.5484400609835419, 0.5517241333310045, 0.545155993285046, 0.5484400609835419, 0.5451559887339524, 0.5484400609835419, 0.5550082100338145, 0.5484400609835419, 0.5467980249566202, 0.5582922823812769, 0.5467980295077137, 0.5451559887339524, 0.5484400609835419, 0.5484400609835419, 0.53858784888374, 0.5500821016594303, 0.5484400656325085, 0.5550082101316874, 0.5467980295077137, 0.5533661693579262, 0.5467980248587472, 0.5435139572581242], 'loss': [1.096788368577585, 1.0815144151632792, 1.075060842609993, 1.0747889678336267, 1.0756408101479376, 1.0752001697767442, 1.074282313861886, 1.0740822974171726, 1.0741119829291437, 1.0740994156263692, 1.0740335120802298, 1.0738658032622914, 1.073709008581095, 1.0735494261160035, 1.0733296909371441, 1.0730721862898716, 1.0728280909497145, 1.0723085545416486, 1.0714943427080001, 1.070306378566264, 1.0686669277214662, 1.0652771917946284, 1.0598189842040044, 1.0494496708777896, 1.0310021426398652, 1.0020880041915534, 0.9764854838960714, 0.9592353610042674, 0.9532086036043735, 0.9416549190358704, 0.9365615913021001, 0.9319973827142735, 0.9227867706355618, 0.9214779975233137, 0.9264253492962409, 0.9195712403839863, 0.9210488229561635, 0.9181021123206591, 0.9172223710671098, 0.9169188697480078, 0.9138654588673883, 0.9099320397239936, 0.9084149032647605, 0.9074015995070674, 0.90693175234589, 0.9066214886289358, 0.9061260634134437, 0.9052163401423539, 0.9053206005380384, 0.9041057387171829, 0.9032803994429429, 0.9029941023742394, 0.9034139852749004, 0.902822696061105, 0.90473598672624, 0.9028701926158929, 0.9000517495603777, 0.9010523256824736, 0.9002991225195617, 0.8999509303721559, 0.8984957772603515, 0.9005431403859195, 0.8977557625858691, 0.9002369114750465, 0.9072650985306538, 0.9058982141208844, 0.898722021241942, 0.9008416969673345, 0.8977532040411932, 0.8967550567777739, 0.8943524949604481, 0.8951732945148461, 0.8938176173449052, 0.8931905817691795, 0.8945727829326104, 0.8975635072289061, 0.9058742239489938, 0.8964942796763943, 0.8935790388980686, 0.891483840609478, 0.8937110373616464, 0.8923901362096015, 0.8911121664105989, 0.8901910136857317, 0.8902714567262778, 0.8898213914777219, 0.8898627650321631, 0.8873156025913951, 0.8877170548791513, 0.8872418002426257, 0.8860862146168029, 0.8865269313847505, 0.8924863605773424, 0.8932725264551213, 0.8901478115782846, 0.8867240241910398, 0.883571141652258, 0.8957616540440788, 0.8953255973312644, 0.905362845299425, 0.8943780972727515, 0.8915580590402811, 0.8925341437239911, 0.8900436837325596, 0.8838967617532311, 0.8896889980323995, 0.8854001557802518, 0.8833335795931258, 0.8835721822979514, 0.882910679522481, 0.8883438110106779, 0.8839153521114795, 0.8865215414848171, 0.8882636059725799, 0.8829396994941289, 0.8812718235981293, 0.8812147799703375, 0.8819851654021402, 0.8814184433625709, 0.882360109264601, 0.8843898739168531, 0.8803432744631288, 0.8795737759043794, 0.8794753919636689, 0.8790158746913228, 0.8837021805912073, 0.8806327166743347, 0.8844747714193449, 0.8853542237555956, 0.8812371538649839, 0.8787380623132052, 0.8792283107123091, 0.8793016389410109, 0.8818340616059744, 0.8822097300504023, 0.8797351294964001, 0.8778447975613008, 0.8830426698592654, 0.8842791092958783, 0.8782930706805517, 0.8775934186314656, 0.8786400767812005, 0.876827411411724, 0.8813671118914469, 0.8789810738524372, 0.8812533417276778, 0.8854182841107097, 0.8756383207299626, 0.8763807707498695, 0.8782448132669657, 0.8761046711669076, 0.8765542139262879, 0.8772036955831477, 0.883064525499481, 0.8760459723169064, 0.8779289140838372, 0.8751710736776035, 0.8763479998958674, 0.88070008485469, 0.8760972254819693, 0.8764561104823432, 0.8775634640296137, 0.8725475403317681, 0.8758063132023665, 0.8725824968526006, 0.8737061795512754, 0.8754823402212876, 0.876290696382033, 0.8724533196837015, 0.87247016402975, 0.8721281362999636, 0.8751837229337046, 0.8739616290989353, 0.8751651673835895, 0.8763073078171184, 0.8779089294664669, 0.8745018537284412, 0.8708529597435154, 0.8728798277569013, 0.8805684882023006, 0.8757807330184404, 0.8789943101224958, 0.8745300710323655, 0.8750257903300761, 0.8749467649254221, 0.8745598461838473, 0.8730452931637147, 0.8720936502764113, 0.8729265420343841, 0.8715381429670283, 0.8704182388601361, 0.8719656165375602, 0.8688867706537736, 0.8681131931056232, 0.8717291547777227, 0.8697171033040699, 0.8741542156472099, 0.8680738109582748, 0.8686968576981546, 0.8698284028002369, 0.8684859426114594, 0.8723335267337196, 0.8769400550844243, 0.8784045878377049, 0.8709732586843032, 0.8693436721511935, 0.8710040049631248, 0.8700208638238222, 0.8689161993150104, 0.8671260044315268, 0.8735064725121923, 0.867034961726876, 0.8668675199181637, 0.8687631835927708, 0.8680276518974461, 0.8661010351269152, 0.8691714590335039, 0.8680222068234391, 0.8715293685268817, 0.867399796739496, 0.8669274297827813, 0.8663126284092114, 0.8647687743331862, 0.8664127381185731, 0.8658974528312683, 0.865862420525639, 0.8657535140274486, 0.864375178574047, 0.8649052532791357, 0.8637804506006183, 0.863144691034509, 0.8636446275505442, 0.8625127499842791, 0.86269027528332, 0.8627284396845213, 0.8696741121016, 0.866066399687859, 0.8670668049269878, 0.868635851254943, 0.8734811858230058, 0.8635758485637406, 0.8621461371376774, 0.8620798753761902, 0.8646628996429991, 0.8645871772413626, 0.865164738073486, 0.8665415198651183, 0.862750817862869, 0.8594097165845992, 0.8601946967583173, 0.8597225356150946, 0.8639466871715914, 0.8638258787151235, 0.8629520265228694, 0.8619147237321435, 0.8659319511184458, 0.8735437911638734, 0.8661117902771404, 0.8620838222317627, 0.8633541674829361, 0.8590508759388934, 0.8614892136634498, 0.8673109144890333, 0.8744380729643961, 0.8642604324117579, 0.8633037490766396, 0.8624852733690391, 0.8581065607756315, 0.8585618369633168, 0.8577040269144751, 0.858060651736093, 0.8581394908854114, 0.856763850418694, 0.8582579285457149, 0.8597000380071527, 0.8602433617844474, 0.8603658636247843, 0.8628235235840878, 0.8557817620907966, 0.8586447684671844, 0.8578923190398873, 0.8550325464908592, 0.858517525646476, 0.8582328057876603, 0.8593060083947387, 0.8565518297698709, 0.8545565148643399, 0.8544505680855784, 0.8560133155856045, 0.8550026674534995, 0.8537014137792881, 0.8530905330939949, 0.8531931823039202, 0.8540466569042793, 0.8536129318223596, 0.8555226124777197, 0.8520762842293882, 0.8541251632956753, 0.8537943698052753, 0.8535343147401203], 'acc': [0.3942505156724605, 0.3942505141058497, 0.37700205160117495, 0.3942505148524376, 0.39425051351837065, 0.3942505128941742, 0.3942505148891551, 0.3942505133225443, 0.39425051312671794, 0.3942505152808078, 0.39425051171921605, 0.39425051351837065, 0.3942505129308916, 0.3942505133225443, 0.3942505152808078, 0.3942505125392389, 0.39466119191974586, 0.41067761800861946, 0.40451745318681065, 0.4098562625766535, 0.4213552366659137, 0.43901437580952657, 0.466529773870288, 0.48090349085766676, 0.49075975408299505, 0.4993839819083713, 0.5014373707820258, 0.5063655018072108, 0.5030800853666584, 0.51786447863804, 0.5034907627889018, 0.5342915825775272, 0.5626283403784342, 0.5301848036552601, 0.5310061602621842, 0.5474332689015038, 0.5425051345472708, 0.5453798734921449, 0.535523615627563, 0.5474332683140247, 0.5363449694929182, 0.5478439459320945, 0.5482546210044219, 0.549897333630791, 0.5486653007765815, 0.5449692022139532, 0.5519507232877509, 0.5404517474360535, 0.5535934256577149, 0.5503080124238189, 0.5490759770238669, 0.5535934312142875, 0.5486653001891024, 0.5564681768906924, 0.5470225827894661, 0.5494866567960265, 0.5556468216545528, 0.5552361365216468, 0.5494866552294158, 0.544969200451516, 0.5564681727783392, 0.5511293613200805, 0.5535934335642038, 0.5585215622394726, 0.5388090369637742, 0.5388090389220377, 0.5601642740825363, 0.5507186827228789, 0.5544147799147228, 0.5572895236328642, 0.5601642665677002, 0.555646818912984, 0.5568788547045886, 0.5601642750616681, 0.5609856219996662, 0.5560574913661338, 0.5540041111822736, 0.5470225895209968, 0.5671457947646813, 0.5585215558995946, 0.5564681689842036, 0.5642710506548871, 0.5642710447066619, 0.5597535887538041, 0.5659137603438611, 0.5597535966602929, 0.5626283322761191, 0.5650924017052386, 0.5609856289270233, 0.5634496875122588, 0.5642710496757555, 0.5671457864665398, 0.5618069857297737, 0.5581108796523092, 0.5581108796523092, 0.5650923983761907, 0.5618069855339474, 0.549486654837763, 0.5650923983761907, 0.5564681701591617, 0.5597535980310773, 0.5708418917606988, 0.5568788537254569, 0.5613963079158775, 0.5564681678092456, 0.5622176621728854, 0.5720739176875512, 0.5634496873164324, 0.5679671420943321, 0.5642710512423662, 0.5634496920162647, 0.5638603736242964, 0.5544147876253852, 0.5618069849464683, 0.5683778218664918, 0.5613963000093887, 0.565092399159496, 0.5622176548538756, 0.5663244329928373, 0.5646817215414263, 0.5577002083741174, 0.5691991828550304, 0.5630390100900152, 0.5659137547872884, 0.5663244316220528, 0.5593429186505704, 0.5675564708161404, 0.5618069786065902, 0.5638603740159491, 0.5642710512423662, 0.5671457941772022, 0.5659137611271664, 0.5675564715994457, 0.5585215645893888, 0.5716632402653077, 0.5663244324053582, 0.5659137532206776, 0.5630390185839831, 0.5679671428776375, 0.563860365913634, 0.5675564642804359, 0.5642710506548871, 0.5667351155800007, 0.5667351175382641, 0.5679671420943321, 0.5675564646720886, 0.5581108865796663, 0.5659137555705938, 0.5659137621062982, 0.5663244322095319, 0.5650923999428015, 0.5601642740825363, 0.5650924051077214, 0.5597535958769875, 0.5671457862707134, 0.5646817286646096, 0.5667351082609909, 0.5650923995511488, 0.5663244320137055, 0.5634496873164324, 0.5646817221289053, 0.5630390178006778, 0.5667351092401226, 0.5675564638887832, 0.5663244331886637, 0.5700205384828226, 0.5618069857297737, 0.5667351155800007, 0.5638603655219813, 0.565503076777566, 0.5638603668927656, 0.5671457876414978, 0.5663244326011846, 0.5613963005968677, 0.5691991820717249, 0.5712525691829423, 0.567556471207793, 0.565913753416504, 0.5671457930022441, 0.5622176629561908, 0.5667351102192544, 0.5650923977887117, 0.564271043140051, 0.5626283411617397, 0.5683778214748391, 0.5642710486966237, 0.5655030783441767, 0.5638603726451646, 0.5655030761900869, 0.5655030775608713, 0.5630390104816679, 0.5622176633478435, 0.5683778274230644, 0.5659137621062982, 0.5646817292520887, 0.5630390170173724, 0.5720739263773454, 0.5659137615188191, 0.5696098608647528, 0.5667351149925216, 0.5646817209539472, 0.5675564727744038, 0.5630390183881567, 0.569199175731847, 0.5687885052369606, 0.5667351161674797, 0.5568788537254569, 0.5667351092401226, 0.5663244395285416, 0.5728952731195172, 0.5683778218664918, 0.5634496890788695, 0.56591376093134, 0.5671457933938968, 0.565913754003983, 0.570020532338771, 0.5646817201706418, 0.5724845996138007, 0.5741273053862476, 0.5622176546580493, 0.5667351161674797, 0.5696098606689265, 0.5650924060868532, 0.563860365913634, 0.5671457862707134, 0.5679671492175156, 0.5655030769733922, 0.5646817292520887, 0.5691991812884195, 0.5700205371120383, 0.5716632481717966, 0.5671457943730286, 0.5667351111983862, 0.5642710506548871, 0.5712525651929804, 0.5679671480425574, 0.5667351165591324, 0.5774127277015906, 0.572073919841641, 0.5626283322761191, 0.5724846039954152, 0.5683778202998809, 0.5671457864665398, 0.5696098521749586, 0.5691991776901104, 0.567556471795272, 0.5696098594939685, 0.5720739180792039, 0.5675564723827511, 0.5683778264439326, 0.5704312149259344, 0.5708418869874315, 0.5704312079985774, 0.5683778214748391, 0.569199182659204, 0.5691991822675513, 0.5659137549831148, 0.5655030844882284, 0.5749486614056926, 0.5724846032121098, 0.5667351096317753, 0.5696098606689265, 0.5613962998135623, 0.5622176621728854, 0.5630390116566261, 0.5642710494799291, 0.5691991745568888, 0.570431207215272, 0.5700205376995173, 0.5675564674136575, 0.571252563234717, 0.5712525640180224, 0.5716632477801439, 0.5679671476509047, 0.5786447642765006, 0.5737166370454511, 0.5679671417026794, 0.5720739261815191, 0.5708418850291681, 0.5765913791969817, 0.5720739182750303, 0.5704312161008925, 0.5691991745568888, 0.5733059519125451, 0.5704312087818827, 0.571663240461134, 0.5712525634305433, 0.5696098541332222, 0.5733059586440759, 0.5745379843751018, 0.5728952740986489, 0.5770020508668261, 0.5774127294640277, 0.5753593465385985, 0.5700205380911699, 0.5733059586440759, 0.5778234132261492, 0.5728952739028226, 0.5737166368496247, 0.5724845951097947]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
