{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf78.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 03:01:21 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '0Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000246B4CD8550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000246B2656EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0929, Accuracy:0.3610, Validation Loss:1.0833, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0816, Accuracy:0.3943, Validation Loss:1.0767, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0757, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0741, Accuracy:0.3938, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #5: Loss:1.0749, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3926, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #7: Loss:1.0742, Accuracy:0.3860, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #8: Loss:1.0740, Accuracy:0.3885, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #14: Loss:1.0737, Accuracy:0.3906, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #15: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0737, Accuracy:0.3922, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #20: Loss:1.0738, Accuracy:0.3926, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0735, Accuracy:0.3934, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0736, Accuracy:0.3869, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #23: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0736, Accuracy:0.3938, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0738, Accuracy:0.3901, Validation Loss:1.0739, Validation Accuracy:0.4056\n",
    "Epoch #26: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #27: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #28: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0737, Validation Accuracy:0.4023\n",
    "Epoch #29: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #30: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3760\n",
    "Epoch #34: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #35: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #36: Loss:1.0735, Accuracy:0.4053, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #37: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0743, Validation Accuracy:0.3760\n",
    "Epoch #41: Loss:1.0735, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #42: Loss:1.0735, Accuracy:0.4057, Validation Loss:1.0743, Validation Accuracy:0.3826\n",
    "Epoch #43: Loss:1.0733, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.3744\n",
    "Epoch #44: Loss:1.0733, Accuracy:0.4004, Validation Loss:1.0742, Validation Accuracy:0.3744\n",
    "Epoch #45: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0743, Validation Accuracy:0.3727\n",
    "Epoch #46: Loss:1.0734, Accuracy:0.3951, Validation Loss:1.0742, Validation Accuracy:0.3777\n",
    "Epoch #47: Loss:1.0735, Accuracy:0.4057, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #48: Loss:1.0733, Accuracy:0.4041, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #49: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #50: Loss:1.0735, Accuracy:0.4012, Validation Loss:1.0746, Validation Accuracy:0.3777\n",
    "Epoch #51: Loss:1.0732, Accuracy:0.3988, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #52: Loss:1.0735, Accuracy:0.4066, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #53: Loss:1.0737, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #54: Loss:1.0735, Accuracy:0.4037, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #55: Loss:1.0735, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #56: Loss:1.0736, Accuracy:0.3955, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #57: Loss:1.0735, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3760\n",
    "Epoch #58: Loss:1.0733, Accuracy:0.4012, Validation Loss:1.0746, Validation Accuracy:0.3777\n",
    "Epoch #59: Loss:1.0730, Accuracy:0.4057, Validation Loss:1.0746, Validation Accuracy:0.3810\n",
    "Epoch #60: Loss:1.0728, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0731, Accuracy:0.4070, Validation Loss:1.0748, Validation Accuracy:0.3777\n",
    "Epoch #62: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #63: Loss:1.0732, Accuracy:0.4066, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #64: Loss:1.0734, Accuracy:0.4037, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #65: Loss:1.0733, Accuracy:0.3996, Validation Loss:1.0750, Validation Accuracy:0.3810\n",
    "Epoch #66: Loss:1.0734, Accuracy:0.4049, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #67: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #68: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #69: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #70: Loss:1.0734, Accuracy:0.4000, Validation Loss:1.0750, Validation Accuracy:0.3777\n",
    "Epoch #71: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0750, Validation Accuracy:0.3777\n",
    "Epoch #72: Loss:1.0731, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #73: Loss:1.0732, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.3793\n",
    "Epoch #74: Loss:1.0731, Accuracy:0.3959, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #75: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0757, Validation Accuracy:0.3760\n",
    "Epoch #76: Loss:1.0731, Accuracy:0.3979, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #77: Loss:1.0731, Accuracy:0.4033, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #78: Loss:1.0732, Accuracy:0.4033, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0755, Validation Accuracy:0.3875\n",
    "Epoch #80: Loss:1.0731, Accuracy:0.4053, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #81: Loss:1.0729, Accuracy:0.4037, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #82: Loss:1.0730, Accuracy:0.4029, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #83: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #84: Loss:1.0728, Accuracy:0.4029, Validation Loss:1.0758, Validation Accuracy:0.3793\n",
    "Epoch #85: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0758, Validation Accuracy:0.3875\n",
    "Epoch #86: Loss:1.0728, Accuracy:0.4057, Validation Loss:1.0759, Validation Accuracy:0.3777\n",
    "Epoch #87: Loss:1.0732, Accuracy:0.3975, Validation Loss:1.0757, Validation Accuracy:0.3777\n",
    "Epoch #88: Loss:1.0727, Accuracy:0.4029, Validation Loss:1.0759, Validation Accuracy:0.3793\n",
    "Epoch #89: Loss:1.0732, Accuracy:0.3877, Validation Loss:1.0760, Validation Accuracy:0.3793\n",
    "Epoch #90: Loss:1.0724, Accuracy:0.3975, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #91: Loss:1.0728, Accuracy:0.4012, Validation Loss:1.0760, Validation Accuracy:0.3875\n",
    "Epoch #92: Loss:1.0730, Accuracy:0.4000, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #93: Loss:1.0724, Accuracy:0.4062, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #94: Loss:1.0724, Accuracy:0.4008, Validation Loss:1.0756, Validation Accuracy:0.3892\n",
    "Epoch #95: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0757, Validation Accuracy:0.3875\n",
    "Epoch #96: Loss:1.0725, Accuracy:0.4016, Validation Loss:1.0757, Validation Accuracy:0.3859\n",
    "Epoch #97: Loss:1.0722, Accuracy:0.4057, Validation Loss:1.0758, Validation Accuracy:0.3859\n",
    "Epoch #98: Loss:1.0724, Accuracy:0.3979, Validation Loss:1.0758, Validation Accuracy:0.3908\n",
    "Epoch #99: Loss:1.0721, Accuracy:0.4029, Validation Loss:1.0760, Validation Accuracy:0.3875\n",
    "Epoch #100: Loss:1.0725, Accuracy:0.4012, Validation Loss:1.0760, Validation Accuracy:0.3793\n",
    "Epoch #101: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #102: Loss:1.0720, Accuracy:0.4049, Validation Loss:1.0761, Validation Accuracy:0.3892\n",
    "Epoch #103: Loss:1.0720, Accuracy:0.4037, Validation Loss:1.0760, Validation Accuracy:0.3892\n",
    "Epoch #104: Loss:1.0721, Accuracy:0.4029, Validation Loss:1.0760, Validation Accuracy:0.3892\n",
    "Epoch #105: Loss:1.0718, Accuracy:0.4041, Validation Loss:1.0763, Validation Accuracy:0.3842\n",
    "Epoch #106: Loss:1.0718, Accuracy:0.4033, Validation Loss:1.0763, Validation Accuracy:0.3908\n",
    "Epoch #107: Loss:1.0719, Accuracy:0.4029, Validation Loss:1.0763, Validation Accuracy:0.3892\n",
    "Epoch #108: Loss:1.0719, Accuracy:0.3971, Validation Loss:1.0765, Validation Accuracy:0.3875\n",
    "Epoch #109: Loss:1.0717, Accuracy:0.4012, Validation Loss:1.0764, Validation Accuracy:0.3908\n",
    "Epoch #110: Loss:1.0717, Accuracy:0.4041, Validation Loss:1.0764, Validation Accuracy:0.3892\n",
    "Epoch #111: Loss:1.0715, Accuracy:0.4029, Validation Loss:1.0767, Validation Accuracy:0.3842\n",
    "Epoch #112: Loss:1.0715, Accuracy:0.4053, Validation Loss:1.0766, Validation Accuracy:0.3892\n",
    "Epoch #113: Loss:1.0715, Accuracy:0.4045, Validation Loss:1.0766, Validation Accuracy:0.3859\n",
    "Epoch #114: Loss:1.0713, Accuracy:0.4057, Validation Loss:1.0769, Validation Accuracy:0.3875\n",
    "Epoch #115: Loss:1.0713, Accuracy:0.4049, Validation Loss:1.0768, Validation Accuracy:0.3892\n",
    "Epoch #116: Loss:1.0712, Accuracy:0.4029, Validation Loss:1.0768, Validation Accuracy:0.3892\n",
    "Epoch #117: Loss:1.0710, Accuracy:0.4025, Validation Loss:1.0771, Validation Accuracy:0.3793\n",
    "Epoch #118: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0769, Validation Accuracy:0.3793\n",
    "Epoch #119: Loss:1.0710, Accuracy:0.4008, Validation Loss:1.0771, Validation Accuracy:0.3842\n",
    "Epoch #120: Loss:1.0709, Accuracy:0.4016, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #121: Loss:1.0713, Accuracy:0.4045, Validation Loss:1.0772, Validation Accuracy:0.3892\n",
    "Epoch #122: Loss:1.0709, Accuracy:0.4021, Validation Loss:1.0772, Validation Accuracy:0.3793\n",
    "Epoch #123: Loss:1.0708, Accuracy:0.4041, Validation Loss:1.0775, Validation Accuracy:0.3793\n",
    "Epoch #124: Loss:1.0708, Accuracy:0.4008, Validation Loss:1.0774, Validation Accuracy:0.3892\n",
    "Epoch #125: Loss:1.0709, Accuracy:0.4041, Validation Loss:1.0776, Validation Accuracy:0.3842\n",
    "Epoch #126: Loss:1.0713, Accuracy:0.4041, Validation Loss:1.0772, Validation Accuracy:0.3793\n",
    "Epoch #127: Loss:1.0704, Accuracy:0.3971, Validation Loss:1.0780, Validation Accuracy:0.3580\n",
    "Epoch #128: Loss:1.0710, Accuracy:0.3975, Validation Loss:1.0770, Validation Accuracy:0.3892\n",
    "Epoch #129: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0773, Validation Accuracy:0.3793\n",
    "Epoch #130: Loss:1.0711, Accuracy:0.4000, Validation Loss:1.0769, Validation Accuracy:0.3892\n",
    "Epoch #131: Loss:1.0705, Accuracy:0.4012, Validation Loss:1.0771, Validation Accuracy:0.3842\n",
    "Epoch #132: Loss:1.0705, Accuracy:0.4045, Validation Loss:1.0771, Validation Accuracy:0.3842\n",
    "Epoch #133: Loss:1.0708, Accuracy:0.3984, Validation Loss:1.0782, Validation Accuracy:0.3793\n",
    "Epoch #134: Loss:1.0704, Accuracy:0.4004, Validation Loss:1.0772, Validation Accuracy:0.3908\n",
    "Epoch #135: Loss:1.0706, Accuracy:0.4049, Validation Loss:1.0771, Validation Accuracy:0.3793\n",
    "Epoch #136: Loss:1.0703, Accuracy:0.4012, Validation Loss:1.0776, Validation Accuracy:0.3793\n",
    "Epoch #137: Loss:1.0705, Accuracy:0.4053, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #138: Loss:1.0699, Accuracy:0.4004, Validation Loss:1.0782, Validation Accuracy:0.3777\n",
    "Epoch #139: Loss:1.0703, Accuracy:0.4016, Validation Loss:1.0772, Validation Accuracy:0.3810\n",
    "Epoch #140: Loss:1.0701, Accuracy:0.4053, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #141: Loss:1.0702, Accuracy:0.3947, Validation Loss:1.0775, Validation Accuracy:0.3793\n",
    "Epoch #142: Loss:1.0705, Accuracy:0.4045, Validation Loss:1.0779, Validation Accuracy:0.3892\n",
    "Epoch #143: Loss:1.0718, Accuracy:0.3885, Validation Loss:1.0781, Validation Accuracy:0.3777\n",
    "Epoch #144: Loss:1.0705, Accuracy:0.4016, Validation Loss:1.0772, Validation Accuracy:0.3892\n",
    "Epoch #145: Loss:1.0697, Accuracy:0.4033, Validation Loss:1.0779, Validation Accuracy:0.3859\n",
    "Epoch #146: Loss:1.0723, Accuracy:0.3963, Validation Loss:1.0779, Validation Accuracy:0.3908\n",
    "Epoch #147: Loss:1.0705, Accuracy:0.3967, Validation Loss:1.0775, Validation Accuracy:0.3908\n",
    "Epoch #148: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0770, Validation Accuracy:0.3842\n",
    "Epoch #149: Loss:1.0698, Accuracy:0.3971, Validation Loss:1.0768, Validation Accuracy:0.3842\n",
    "Epoch #150: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0770, Validation Accuracy:0.3892\n",
    "Epoch #151: Loss:1.0698, Accuracy:0.4004, Validation Loss:1.0772, Validation Accuracy:0.3842\n",
    "Epoch #152: Loss:1.0696, Accuracy:0.4066, Validation Loss:1.0773, Validation Accuracy:0.3793\n",
    "Epoch #153: Loss:1.0698, Accuracy:0.4029, Validation Loss:1.0784, Validation Accuracy:0.3842\n",
    "Epoch #154: Loss:1.0706, Accuracy:0.4049, Validation Loss:1.0775, Validation Accuracy:0.3892\n",
    "Epoch #155: Loss:1.0702, Accuracy:0.3873, Validation Loss:1.0785, Validation Accuracy:0.3711\n",
    "Epoch #156: Loss:1.0699, Accuracy:0.3963, Validation Loss:1.0773, Validation Accuracy:0.3892\n",
    "Epoch #157: Loss:1.0695, Accuracy:0.4049, Validation Loss:1.0776, Validation Accuracy:0.3810\n",
    "Epoch #158: Loss:1.0694, Accuracy:0.4053, Validation Loss:1.0775, Validation Accuracy:0.3859\n",
    "Epoch #159: Loss:1.0700, Accuracy:0.4016, Validation Loss:1.0777, Validation Accuracy:0.3793\n",
    "Epoch #160: Loss:1.0691, Accuracy:0.3971, Validation Loss:1.0785, Validation Accuracy:0.3826\n",
    "Epoch #161: Loss:1.0693, Accuracy:0.3963, Validation Loss:1.0778, Validation Accuracy:0.3892\n",
    "Epoch #162: Loss:1.0692, Accuracy:0.4041, Validation Loss:1.0781, Validation Accuracy:0.3793\n",
    "Epoch #163: Loss:1.0688, Accuracy:0.4029, Validation Loss:1.0779, Validation Accuracy:0.3793\n",
    "Epoch #164: Loss:1.0690, Accuracy:0.3992, Validation Loss:1.0781, Validation Accuracy:0.3793\n",
    "Epoch #165: Loss:1.0694, Accuracy:0.4029, Validation Loss:1.0778, Validation Accuracy:0.3793\n",
    "Epoch #166: Loss:1.0694, Accuracy:0.4021, Validation Loss:1.0778, Validation Accuracy:0.3793\n",
    "Epoch #167: Loss:1.0688, Accuracy:0.3955, Validation Loss:1.0780, Validation Accuracy:0.3826\n",
    "Epoch #168: Loss:1.0689, Accuracy:0.3975, Validation Loss:1.0780, Validation Accuracy:0.3793\n",
    "Epoch #169: Loss:1.0688, Accuracy:0.4070, Validation Loss:1.0787, Validation Accuracy:0.3892\n",
    "Epoch #170: Loss:1.0690, Accuracy:0.4082, Validation Loss:1.0779, Validation Accuracy:0.3793\n",
    "Epoch #171: Loss:1.0690, Accuracy:0.3934, Validation Loss:1.0789, Validation Accuracy:0.3711\n",
    "Epoch #172: Loss:1.0689, Accuracy:0.3938, Validation Loss:1.0778, Validation Accuracy:0.3810\n",
    "Epoch #173: Loss:1.0689, Accuracy:0.3996, Validation Loss:1.0781, Validation Accuracy:0.3793\n",
    "Epoch #174: Loss:1.0688, Accuracy:0.4008, Validation Loss:1.0776, Validation Accuracy:0.3842\n",
    "Epoch #175: Loss:1.0680, Accuracy:0.4029, Validation Loss:1.0784, Validation Accuracy:0.3892\n",
    "Epoch #176: Loss:1.0688, Accuracy:0.3934, Validation Loss:1.0779, Validation Accuracy:0.3875\n",
    "Epoch #177: Loss:1.0685, Accuracy:0.3996, Validation Loss:1.0777, Validation Accuracy:0.3826\n",
    "Epoch #178: Loss:1.0697, Accuracy:0.3885, Validation Loss:1.0785, Validation Accuracy:0.3810\n",
    "Epoch #179: Loss:1.0678, Accuracy:0.4078, Validation Loss:1.0778, Validation Accuracy:0.3842\n",
    "Epoch #180: Loss:1.0694, Accuracy:0.4025, Validation Loss:1.0778, Validation Accuracy:0.3859\n",
    "Epoch #181: Loss:1.0694, Accuracy:0.3967, Validation Loss:1.0785, Validation Accuracy:0.3678\n",
    "Epoch #182: Loss:1.0700, Accuracy:0.3852, Validation Loss:1.0771, Validation Accuracy:0.3842\n",
    "Epoch #183: Loss:1.0683, Accuracy:0.4008, Validation Loss:1.0779, Validation Accuracy:0.3826\n",
    "Epoch #184: Loss:1.0683, Accuracy:0.4008, Validation Loss:1.0771, Validation Accuracy:0.3826\n",
    "Epoch #185: Loss:1.0686, Accuracy:0.4033, Validation Loss:1.0772, Validation Accuracy:0.3875\n",
    "Epoch #186: Loss:1.0689, Accuracy:0.4029, Validation Loss:1.0781, Validation Accuracy:0.3826\n",
    "Epoch #187: Loss:1.0679, Accuracy:0.4025, Validation Loss:1.0772, Validation Accuracy:0.3793\n",
    "Epoch #188: Loss:1.0677, Accuracy:0.4029, Validation Loss:1.0773, Validation Accuracy:0.3826\n",
    "Epoch #189: Loss:1.0676, Accuracy:0.4012, Validation Loss:1.0775, Validation Accuracy:0.3859\n",
    "Epoch #190: Loss:1.0679, Accuracy:0.4008, Validation Loss:1.0778, Validation Accuracy:0.3826\n",
    "Epoch #191: Loss:1.0677, Accuracy:0.3996, Validation Loss:1.0774, Validation Accuracy:0.3859\n",
    "Epoch #192: Loss:1.0679, Accuracy:0.3984, Validation Loss:1.0774, Validation Accuracy:0.3842\n",
    "Epoch #193: Loss:1.0673, Accuracy:0.4021, Validation Loss:1.0777, Validation Accuracy:0.3826\n",
    "Epoch #194: Loss:1.0681, Accuracy:0.3963, Validation Loss:1.0774, Validation Accuracy:0.3859\n",
    "Epoch #195: Loss:1.0682, Accuracy:0.4012, Validation Loss:1.0773, Validation Accuracy:0.3810\n",
    "Epoch #196: Loss:1.0671, Accuracy:0.4012, Validation Loss:1.0775, Validation Accuracy:0.3793\n",
    "Epoch #197: Loss:1.0677, Accuracy:0.3938, Validation Loss:1.0775, Validation Accuracy:0.3777\n",
    "Epoch #198: Loss:1.0676, Accuracy:0.4004, Validation Loss:1.0771, Validation Accuracy:0.3810\n",
    "Epoch #199: Loss:1.0676, Accuracy:0.4025, Validation Loss:1.0778, Validation Accuracy:0.3842\n",
    "Epoch #200: Loss:1.0675, Accuracy:0.3918, Validation Loss:1.0774, Validation Accuracy:0.3859\n",
    "Epoch #201: Loss:1.0684, Accuracy:0.4053, Validation Loss:1.0771, Validation Accuracy:0.3810\n",
    "Epoch #202: Loss:1.0681, Accuracy:0.4016, Validation Loss:1.0784, Validation Accuracy:0.3859\n",
    "Epoch #203: Loss:1.0676, Accuracy:0.3906, Validation Loss:1.0771, Validation Accuracy:0.3810\n",
    "Epoch #204: Loss:1.0677, Accuracy:0.3959, Validation Loss:1.0768, Validation Accuracy:0.3859\n",
    "Epoch #205: Loss:1.0674, Accuracy:0.3988, Validation Loss:1.0769, Validation Accuracy:0.3793\n",
    "Epoch #206: Loss:1.0683, Accuracy:0.3992, Validation Loss:1.0767, Validation Accuracy:0.3810\n",
    "Epoch #207: Loss:1.0676, Accuracy:0.3951, Validation Loss:1.0786, Validation Accuracy:0.3875\n",
    "Epoch #208: Loss:1.0680, Accuracy:0.3992, Validation Loss:1.0774, Validation Accuracy:0.3744\n",
    "Epoch #209: Loss:1.0684, Accuracy:0.3996, Validation Loss:1.0771, Validation Accuracy:0.3859\n",
    "Epoch #210: Loss:1.0671, Accuracy:0.3984, Validation Loss:1.0773, Validation Accuracy:0.3875\n",
    "Epoch #211: Loss:1.0677, Accuracy:0.4094, Validation Loss:1.0774, Validation Accuracy:0.3859\n",
    "Epoch #212: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #213: Loss:1.0685, Accuracy:0.3922, Validation Loss:1.0798, Validation Accuracy:0.3777\n",
    "Epoch #214: Loss:1.0694, Accuracy:0.3881, Validation Loss:1.0776, Validation Accuracy:0.3810\n",
    "Epoch #215: Loss:1.0675, Accuracy:0.4021, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #216: Loss:1.0674, Accuracy:0.4057, Validation Loss:1.0763, Validation Accuracy:0.3875\n",
    "Epoch #217: Loss:1.0666, Accuracy:0.3996, Validation Loss:1.0773, Validation Accuracy:0.3875\n",
    "Epoch #218: Loss:1.0681, Accuracy:0.3959, Validation Loss:1.0764, Validation Accuracy:0.3842\n",
    "Epoch #219: Loss:1.0683, Accuracy:0.3955, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #220: Loss:1.0674, Accuracy:0.4053, Validation Loss:1.0779, Validation Accuracy:0.3695\n",
    "Epoch #221: Loss:1.0675, Accuracy:0.4016, Validation Loss:1.0769, Validation Accuracy:0.3826\n",
    "Epoch #222: Loss:1.0677, Accuracy:0.4029, Validation Loss:1.0766, Validation Accuracy:0.3924\n",
    "Epoch #223: Loss:1.0681, Accuracy:0.4004, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #224: Loss:1.0680, Accuracy:0.3959, Validation Loss:1.0782, Validation Accuracy:0.3892\n",
    "Epoch #225: Loss:1.0681, Accuracy:0.4066, Validation Loss:1.0770, Validation Accuracy:0.3957\n",
    "Epoch #226: Loss:1.0684, Accuracy:0.4053, Validation Loss:1.0797, Validation Accuracy:0.3662\n",
    "Epoch #227: Loss:1.0714, Accuracy:0.3807, Validation Loss:1.0803, Validation Accuracy:0.3514\n",
    "Epoch #228: Loss:1.0701, Accuracy:0.3864, Validation Loss:1.0803, Validation Accuracy:0.3941\n",
    "Epoch #229: Loss:1.0692, Accuracy:0.4012, Validation Loss:1.0804, Validation Accuracy:0.3842\n",
    "Epoch #230: Loss:1.0697, Accuracy:0.4074, Validation Loss:1.0797, Validation Accuracy:0.3875\n",
    "Epoch #231: Loss:1.0704, Accuracy:0.3951, Validation Loss:1.0794, Validation Accuracy:0.3908\n",
    "Epoch #232: Loss:1.0697, Accuracy:0.4082, Validation Loss:1.0801, Validation Accuracy:0.3826\n",
    "Epoch #233: Loss:1.0699, Accuracy:0.4016, Validation Loss:1.0794, Validation Accuracy:0.3810\n",
    "Epoch #234: Loss:1.0693, Accuracy:0.4041, Validation Loss:1.0787, Validation Accuracy:0.3793\n",
    "Epoch #235: Loss:1.0696, Accuracy:0.3971, Validation Loss:1.0785, Validation Accuracy:0.3842\n",
    "Epoch #236: Loss:1.0705, Accuracy:0.4016, Validation Loss:1.0807, Validation Accuracy:0.3645\n",
    "Epoch #237: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0789, Validation Accuracy:0.3678\n",
    "Epoch #238: Loss:1.0698, Accuracy:0.4070, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #239: Loss:1.0701, Accuracy:0.4057, Validation Loss:1.0763, Validation Accuracy:0.3842\n",
    "Epoch #240: Loss:1.0703, Accuracy:0.3992, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #241: Loss:1.0701, Accuracy:0.4000, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #242: Loss:1.0707, Accuracy:0.4000, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #243: Loss:1.0705, Accuracy:0.4053, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #244: Loss:1.0697, Accuracy:0.4037, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #245: Loss:1.0697, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #246: Loss:1.0699, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #247: Loss:1.0698, Accuracy:0.4025, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #248: Loss:1.0695, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #249: Loss:1.0695, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #250: Loss:1.0695, Accuracy:0.3975, Validation Loss:1.0754, Validation Accuracy:0.3842\n",
    "Epoch #251: Loss:1.0703, Accuracy:0.3984, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #252: Loss:1.0706, Accuracy:0.4053, Validation Loss:1.0762, Validation Accuracy:0.3826\n",
    "Epoch #253: Loss:1.0701, Accuracy:0.3930, Validation Loss:1.0750, Validation Accuracy:0.3859\n",
    "Epoch #254: Loss:1.0694, Accuracy:0.3984, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #255: Loss:1.0704, Accuracy:0.3938, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #256: Loss:1.0697, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #257: Loss:1.0693, Accuracy:0.4037, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #258: Loss:1.0689, Accuracy:0.4033, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #259: Loss:1.0696, Accuracy:0.4029, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #260: Loss:1.0689, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #261: Loss:1.0690, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #262: Loss:1.0686, Accuracy:0.4049, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #263: Loss:1.0687, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #264: Loss:1.0685, Accuracy:0.4062, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #265: Loss:1.0686, Accuracy:0.4045, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #266: Loss:1.0683, Accuracy:0.4045, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #267: Loss:1.0685, Accuracy:0.4037, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #268: Loss:1.0685, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #269: Loss:1.0682, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #270: Loss:1.0688, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #271: Loss:1.0680, Accuracy:0.4049, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #272: Loss:1.0684, Accuracy:0.4000, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #273: Loss:1.0684, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #274: Loss:1.0693, Accuracy:0.3984, Validation Loss:1.0752, Validation Accuracy:0.3908\n",
    "Epoch #275: Loss:1.0681, Accuracy:0.4062, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #276: Loss:1.0688, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #277: Loss:1.0680, Accuracy:0.4021, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #278: Loss:1.0681, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #279: Loss:1.0684, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #280: Loss:1.0685, Accuracy:0.4004, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #281: Loss:1.0680, Accuracy:0.4029, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #282: Loss:1.0679, Accuracy:0.4041, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #283: Loss:1.0678, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #284: Loss:1.0680, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #285: Loss:1.0681, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #286: Loss:1.0675, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #287: Loss:1.0684, Accuracy:0.4025, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #288: Loss:1.0684, Accuracy:0.4033, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #289: Loss:1.0677, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #290: Loss:1.0676, Accuracy:0.4037, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #291: Loss:1.0677, Accuracy:0.4033, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #292: Loss:1.0681, Accuracy:0.4045, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #293: Loss:1.0685, Accuracy:0.4045, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #294: Loss:1.0680, Accuracy:0.4016, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #295: Loss:1.0674, Accuracy:0.3971, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #296: Loss:1.0678, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #297: Loss:1.0674, Accuracy:0.4025, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #298: Loss:1.0677, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #299: Loss:1.0678, Accuracy:0.4041, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #300: Loss:1.0673, Accuracy:0.4037, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07467663, Accuracy:0.3908\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  03   01\n",
    "t:02  75   0  152\n",
    "t:03  33   0  109\n",
    "t:01  77   0  163\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.41      0.33      0.36       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.38      0.68      0.49       240\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.26      0.34      0.29       609\n",
    "weighted avg       0.30      0.39      0.33       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 03:41:51 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 29 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0833211972795684, 1.0766610163577477, 1.0738878365612186, 1.0742537166880466, 1.0745616622746283, 1.0739616582350582, 1.0740584577441412, 1.0739958887225498, 1.0739400537534691, 1.073984102271069, 1.0737408096175671, 1.0738838496075083, 1.073905753971908, 1.0737630428351792, 1.0738718088820258, 1.0738469074512351, 1.0739667570258205, 1.073927475509581, 1.0738130854855616, 1.073843152456487, 1.073867575875644, 1.0737506482010013, 1.0737496624243474, 1.07383852580498, 1.0738536820231597, 1.0738742306510412, 1.073881603226873, 1.0737466387365056, 1.0736761281055769, 1.07392401863593, 1.074917407458639, 1.0744566435884373, 1.074056307670518, 1.0739252430269088, 1.0744920902455772, 1.0741420506648047, 1.074301950450014, 1.0742707193778653, 1.0743924479179194, 1.0743449908759206, 1.0743799618703782, 1.0743442758159294, 1.0741820564410958, 1.074181016442811, 1.0743363169809477, 1.0741642075414923, 1.074263602446257, 1.0741040387568608, 1.0742198457858834, 1.0746211222631394, 1.0744549241559258, 1.07445379215704, 1.0742847727633071, 1.074504842116132, 1.074541029867476, 1.0748581788418523, 1.0744736145674105, 1.0745797452863997, 1.074574360119298, 1.0743347523834905, 1.0748314610843002, 1.0743924522243307, 1.074917503765651, 1.075016463527147, 1.0749880950439152, 1.0751419451044895, 1.0751609449903365, 1.0750617250824601, 1.0751630359486797, 1.0750390197255928, 1.0749825603269003, 1.074908829674932, 1.075096776919999, 1.0753544114884876, 1.075696173559856, 1.0755139863354035, 1.0754753333594411, 1.0754582001070672, 1.0754631119604376, 1.0755330302640917, 1.0755066198276964, 1.075685670223142, 1.0756679757671013, 1.0758417521791506, 1.0757549764291798, 1.0759048526510229, 1.0757055744553239, 1.0759246194695409, 1.0760431215289388, 1.0756805228873818, 1.0759629592519675, 1.0756719983465761, 1.0756491095757446, 1.0756261877238458, 1.0756839141861363, 1.075708557428006, 1.0758051838976606, 1.0757950695081688, 1.0759741823465758, 1.0759911034103293, 1.0758737858097345, 1.0760840637539015, 1.076014103952104, 1.076017814904011, 1.076270975894333, 1.0762756984613604, 1.076264714764061, 1.0765472337334419, 1.076366502467439, 1.0764334143088956, 1.0766519086897275, 1.0765556034391932, 1.0766385447215565, 1.0768963374527805, 1.0767551357131482, 1.0767741976504648, 1.0770863819200613, 1.0768852991423583, 1.0770727789460732, 1.077116725480028, 1.077235723363942, 1.077154698043034, 1.0775346108062318, 1.0773605889287488, 1.0775598127070711, 1.077200633747433, 1.0780070804805786, 1.0770017932397, 1.0773251082118118, 1.0769339110855203, 1.0770730780459, 1.07708596870034, 1.0781583026516417, 1.0771934837347572, 1.0770643467973606, 1.0775885576097837, 1.0770931972071456, 1.0781938839820022, 1.0772476883357383, 1.0770546722490408, 1.0775209800363175, 1.0778721811736158, 1.0780548720524228, 1.0771713503475846, 1.0778630036242882, 1.0778615883809983, 1.077487656244112, 1.0770169218576993, 1.0767846162291779, 1.077015226306195, 1.0771667350493432, 1.0773016707650547, 1.078356014683916, 1.0774533930670451, 1.0785217134431861, 1.0772725915282426, 1.077563289155318, 1.0774654643288974, 1.077716363866145, 1.0784501146604666, 1.077826918052335, 1.0780949365525019, 1.077938515955983, 1.0781189029048426, 1.0777961796727673, 1.0778250289080766, 1.0780177981591186, 1.0779651444533775, 1.0786513303496763, 1.077898241616235, 1.078866525236609, 1.0778264789941472, 1.078118168074509, 1.0776431893284488, 1.078428230457901, 1.077861809573933, 1.077718427420054, 1.0785403925014052, 1.0777894512968893, 1.0777876465191394, 1.078537803956832, 1.077132362254539, 1.0779008213522399, 1.0770582465702676, 1.0771576698581964, 1.0781292286999706, 1.077203451706271, 1.077315233806867, 1.0774783986346868, 1.0777707714556866, 1.0774020258037524, 1.0773508153329734, 1.0776865507777298, 1.0773866472181624, 1.0772978194632945, 1.0775254659464795, 1.0775423875974708, 1.0771182274387778, 1.0778452598402652, 1.0773819949239345, 1.0770729465046147, 1.0784373766878752, 1.0770831057199313, 1.0768145097691828, 1.0768741220480507, 1.076693969602851, 1.0786114835191047, 1.077416465200227, 1.0771320418184027, 1.0772523187064185, 1.0773934004537773, 1.0767384133315439, 1.07978547069631, 1.0775514770611165, 1.0767164071792452, 1.0763231784056364, 1.0773295958836873, 1.076438336536802, 1.0767231776405046, 1.077885521456526, 1.0769152790063317, 1.0766024865540378, 1.0770703072618382, 1.0781675774867117, 1.0769838636927613, 1.0796709882801976, 1.0803497099915553, 1.0802860573203301, 1.0804383136172993, 1.0797177097088793, 1.0794395204443845, 1.0801142873043692, 1.0793909171140448, 1.0786879391505801, 1.0784882549777604, 1.0807074100904668, 1.0788507978317186, 1.0757352955431383, 1.0763199221715942, 1.0753704338825394, 1.0748375867583677, 1.073394492146221, 1.0745904913481037, 1.0745791795805757, 1.07447941941385, 1.0745595012392317, 1.074185491586945, 1.0744345957422492, 1.0744758390244984, 1.0754146031753966, 1.07467372898985, 1.076243934177217, 1.0750152675193323, 1.0752845941897489, 1.0749940024612377, 1.0744494162561076, 1.0749428233098122, 1.0752565680662007, 1.0748859217209965, 1.074954891831221, 1.0748180278220592, 1.0748693165912222, 1.0742133691393096, 1.0742626154951274, 1.0743386723920825, 1.074279673581053, 1.0745580006507034, 1.0743751981966994, 1.0744377014476483, 1.0745861048768894, 1.0744370789754958, 1.0748272850399925, 1.0745193012829484, 1.0752051160449074, 1.074604498537499, 1.0745744994904216, 1.0742777266917363, 1.074378197807788, 1.0744395485065255, 1.0743193413040713, 1.0746263682548636, 1.0744496883429917, 1.0744588815519962, 1.0748180113793986, 1.0747624081735345, 1.074520873514498, 1.0745955545131014, 1.0755784315820203, 1.0746501985637622, 1.074659775434848, 1.0750157112754233, 1.0752553618795961, 1.0751161906128055, 1.0747490473372987, 1.0750246913170776, 1.0744658035206285, 1.0746152500800898, 1.074424274253532, 1.0750456850712717, 1.0746766606770908], 'val_acc': [0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3957307055372323, 0.3891625610380533, 0.397372741662027, 0.397372741662027, 0.40229885003641125, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3957307055372323, 0.397372741662027, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39737274156415403, 0.39408866941243753, 0.39408866931456454, 0.3957307054393593, 0.39408866941243753, 0.39408866931456454, 0.40558292189450884, 0.3924466331897698, 0.39080459716284804, 0.40229884964491935, 0.39901477739532987, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3760262717460764, 0.39080459716284804, 0.3875205246196396, 0.3842364523700501, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3760262717460764, 0.3842364520764312, 0.38259441624525536, 0.37438423562128165, 0.37438423562128165, 0.3727421994964869, 0.37766830787087113, 0.3842364519785582, 0.3842364522721771, 0.3760262716482034, 0.37766830787087113, 0.3842364523700501, 0.3825944157558905, 0.3875205244238936, 0.3825944155601445, 0.3842364523700501, 0.39408866941243753, 0.3760262717460764, 0.3776683068921413, 0.38095237953322275, 0.3940886685315807, 0.37766830777299815, 0.38587848839697186, 0.3875205242281477, 0.38916256064656135, 0.38095238002258763, 0.38259441624525536, 0.38916256064656135, 0.3875205242281477, 0.38916256064656135, 0.37766830787087113, 0.37766830787087113, 0.38916256064656135, 0.37931034311480905, 0.3875205242281477, 0.3760262717460764, 0.38095238002258763, 0.38916256064656135, 0.3908045967713561, 0.3875205245217666, 0.3908045967713561, 0.38916256064656135, 0.3908045967713561, 0.38916256064656135, 0.37931034311480905, 0.3875205242281477, 0.37766830787087113, 0.37766830787087113, 0.37931034311480905, 0.37931034311480905, 0.3908045967713561, 0.3875205247175126, 0.38916256064656135, 0.3875205243260207, 0.38916256064656135, 0.3875205243260207, 0.38587848839697186, 0.38587848849484485, 0.3908045967713561, 0.3875205243260207, 0.37931034311480905, 0.3875205245217666, 0.38916256064656135, 0.38916256064656135, 0.38916256064656135, 0.3842364520764312, 0.3908045967713561, 0.38916256064656135, 0.3875205242281477, 0.3908045967713561, 0.38916256064656135, 0.3842364520764312, 0.38916256064656135, 0.38587848839697186, 0.3875205242281477, 0.38916256064656135, 0.38916256064656135, 0.379310343604174, 0.3793103437020469, 0.3842364520764312, 0.38916256064656135, 0.38916256064656135, 0.3793103437020469, 0.379310343604174, 0.38916256064656135, 0.3842364520764312, 0.3793103437020469, 0.3579638734924774, 0.38916256064656135, 0.379310343604174, 0.38916256064656135, 0.3842364520764312, 0.3842364520764312, 0.379310343604174, 0.3908045967713561, 0.3793103437020469, 0.379310343604174, 0.38916256064656135, 0.37766830738150625, 0.38095237982684166, 0.38916256064656135, 0.379310343604174, 0.38916256064656135, 0.3776683068921413, 0.38916256064656135, 0.38587848820122594, 0.3908045961841182, 0.3908045967713561, 0.3842364519785582, 0.3842364519785582, 0.38916256064656135, 0.3842364519785582, 0.379310343604174, 0.3842364519785582, 0.38916256064656135, 0.37110016229508935, 0.38916256064656135, 0.38095237982684166, 0.38587848820122594, 0.379310343604174, 0.38259441585376347, 0.38916256064656135, 0.379310343604174, 0.379310343604174, 0.379310343604174, 0.3793103437020469, 0.379310343604174, 0.3825944155601445, 0.379310343604174, 0.38916256064656135, 0.379310343604174, 0.37110016229508935, 0.38095237982684166, 0.379310343604174, 0.3842364522721771, 0.3891625600593235, 0.3875205240324017, 0.3825944159516364, 0.3809523789459849, 0.3842364523700501, 0.38587848820122594, 0.36781609004549987, 0.3842364522721771, 0.38259441585376347, 0.38259441585376347, 0.3875205240324017, 0.38259441585376347, 0.379310343604174, 0.38259441585376347, 0.385878487907607, 0.3825944156580175, 0.385878487907607, 0.38423645178281224, 0.3825944156580175, 0.385878487907607, 0.38095237982684166, 0.3793103429190631, 0.3776683067942683, 0.3809523797289687, 0.38423645178281224, 0.385878487907607, 0.3809523797289687, 0.385878487907607, 0.3809523797289687, 0.385878487907607, 0.3793103429190631, 0.38095237982684166, 0.38752052354303684, 0.37438423542553567, 0.385878487907607, 0.3875205240324017, 0.38587848810335296, 0.3875205240324017, 0.3776683068921413, 0.38095237992471465, 0.379310343604174, 0.3875205240324017, 0.38752052373878276, 0.38423645129344736, 0.379310343604174, 0.3694581261702946, 0.3825944157558905, 0.39244663240678596, 0.38916256025506946, 0.3891625598635775, 0.39573070475424843, 0.36617405470368897, 0.35139572879755243, 0.39408866911881857, 0.3842364520764312, 0.38752052393452874, 0.3908045962819912, 0.38259441526652554, 0.3809523792396038, 0.37931034311480905, 0.3842364522721771, 0.36453201828527526, 0.36781609014337285, 0.38916256045081543, 0.38423645178281224, 0.3875205245217666, 0.38259441585376347, 0.3875205242281477, 0.39408866882519966, 0.3875205245217666, 0.3842364520764312, 0.3875205242281477, 0.3908045968692291, 0.3875205244238936, 0.3908045963798642, 0.3842364518806852, 0.38916256025506946, 0.38259441624525536, 0.38587848839697186, 0.3825944156580175, 0.3924466327004049, 0.3908045965756102, 0.3842364518806852, 0.3842364518806852, 0.385878487907607, 0.3875205241302747, 0.38916256035294244, 0.3908045963798642, 0.38916256035294244, 0.3908045963798642, 0.3908045963798642, 0.38916256025506946, 0.38423645217430413, 0.3908045963798642, 0.3875205242281477, 0.3940886686294537, 0.38916256035294244, 0.38916256045081543, 0.3908045963798642, 0.3908045963798642, 0.39080459667348316, 0.38916256045081543, 0.3940886686294537, 0.3940886686294537, 0.39080459667348316, 0.38916256025506946, 0.3990147770038379, 0.3940886686294537, 0.38916256025506946, 0.38916256025506946, 0.3940886686294537, 0.3940886686294537, 0.38916256035294244, 0.3940886686294537, 0.38916256035294244, 0.3940886686294537, 0.3940886686294537, 0.39573070465637544, 0.3875205245217666, 0.3940886686294537, 0.3990147770038379, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3908045963798642], 'loss': [1.09292692457626, 1.081635494055934, 1.0756614036383816, 1.0741151350969163, 1.0749135852594396, 1.0744716001976686, 1.074159222363936, 1.0739893780596692, 1.0737910841035156, 1.0739589937903309, 1.0740015760829071, 1.0738749851681124, 1.0737550935216509, 1.073713606681667, 1.0737292489476762, 1.0736421097964965, 1.0737408853899038, 1.0736305510973294, 1.073676285851418, 1.0738235544130297, 1.0735318566739436, 1.07355855529558, 1.0736964035817484, 1.0736149010961795, 1.073753177997268, 1.0736074450079665, 1.0736619298218213, 1.0737067633830546, 1.0736433548604194, 1.0736143701619925, 1.0737450623659137, 1.0738045849104927, 1.0736646178321916, 1.073656220553592, 1.0736806235519034, 1.073452948789577, 1.073495208262418, 1.0736188356881269, 1.0735881913613978, 1.0734593566193473, 1.0734761829004151, 1.0734686760442214, 1.0733041586572385, 1.0732679649544936, 1.0733534895174313, 1.0734312308642409, 1.0735345057638275, 1.0733115133563595, 1.0735635391740583, 1.0734634587407357, 1.073170026714552, 1.0734606083903224, 1.0736839143647305, 1.0735108610540935, 1.073458685228712, 1.073574969313228, 1.0735005936828237, 1.0732733614390881, 1.073041114719007, 1.0728403774864619, 1.0730704779008087, 1.0731164624314045, 1.0732209062674207, 1.0733514398519999, 1.0733005968697018, 1.0733618429798855, 1.073441231666894, 1.073211135903423, 1.0732142154196205, 1.0733844643500796, 1.0734364690232325, 1.0731410105859964, 1.0732005742051518, 1.073082048202687, 1.0734492228506036, 1.0731221951498389, 1.0730866934484524, 1.0732180937114928, 1.0731080546760952, 1.0731367740298199, 1.0729236019220685, 1.072962927034993, 1.0728779890698819, 1.0728162558416567, 1.0729977543593923, 1.0728491898679635, 1.0732428603103763, 1.0727130274997845, 1.0731748775290268, 1.0724445199574777, 1.0727822339999846, 1.0729841287620747, 1.072414390751958, 1.0724344342641028, 1.0724361728104232, 1.0724672464864209, 1.0722238561211181, 1.0724334139109148, 1.0721435696192592, 1.072457373019851, 1.0722604101933004, 1.0720313313560563, 1.071977242992644, 1.0720863775550953, 1.0718147498136674, 1.0718328105350785, 1.0718879045647027, 1.071927553623364, 1.0716502050599523, 1.0716855800616913, 1.0715004073031384, 1.071501477396219, 1.0714716061919132, 1.0713435342914026, 1.0712826955979364, 1.0711571696357804, 1.0709806044243688, 1.0711285709110863, 1.0710285615627282, 1.0708686669014806, 1.0712641953932431, 1.070893716910047, 1.07084425327959, 1.070787978074389, 1.070867544082156, 1.0713212913556265, 1.0704077774494336, 1.0709727401850895, 1.0709640426557412, 1.0710628630444254, 1.0705476285495796, 1.0704633260899257, 1.0707971914592955, 1.070394332208183, 1.0706474033958857, 1.0703334976270704, 1.0705487858343419, 1.06990969665731, 1.0702946359371992, 1.0700750735750924, 1.070186767881656, 1.0705384851236364, 1.0718062387110028, 1.0704722776060476, 1.0697039356466682, 1.0723439111357107, 1.0704984068136196, 1.0715524642619265, 1.0697560450379608, 1.069988905281991, 1.0697594054180983, 1.0695574150437936, 1.0698296927818283, 1.0705628512086809, 1.070153506629521, 1.0699415353289374, 1.06949585105849, 1.0694162865683774, 1.069959635362488, 1.06912243444572, 1.0693256925998038, 1.069210300455348, 1.0688309325329821, 1.069013083348284, 1.0694077639609147, 1.0694238621106626, 1.068776513467824, 1.0689219694607557, 1.0687996882432784, 1.0689975403662335, 1.0689992608475734, 1.0688867590021058, 1.068875161627235, 1.0688075677570132, 1.0680439934838235, 1.068821972155718, 1.0684634235605321, 1.0697175004888608, 1.0677698411980694, 1.0693530377421292, 1.0694233089501852, 1.0699810269432146, 1.0682837183715381, 1.0682584014027026, 1.0685650060309033, 1.0688942062291766, 1.067906389784764, 1.067710603727697, 1.0676419426528336, 1.0678997543068638, 1.0676571277132758, 1.0678696175620297, 1.0673313342570279, 1.0680972335764516, 1.0681644724624602, 1.06711596394956, 1.067728184282902, 1.0675698250960521, 1.0675756724708134, 1.0675394475827227, 1.0683597197033297, 1.0680639798146743, 1.0675603894482404, 1.0676556460421678, 1.06739089591792, 1.0683242781206324, 1.0676038028767956, 1.067961471771068, 1.0684169009970443, 1.0670657086910897, 1.0677032286626358, 1.0699688668613316, 1.0684728061883602, 1.0694309054948465, 1.0674532268571169, 1.0674107340571817, 1.0666467628929404, 1.0680974920672313, 1.0682810939068177, 1.067444569227387, 1.067476057712547, 1.0677149477435823, 1.068058654366088, 1.0679554145683743, 1.068146774313533, 1.0684423606253748, 1.0714464619419168, 1.0701486293295326, 1.069166699180368, 1.0696865588977351, 1.070437458261572, 1.0697144097616051, 1.0699299649781024, 1.0692637927233561, 1.0695940705540243, 1.0704924925641601, 1.0700064514207155, 1.0697840221119124, 1.0701135033209956, 1.0703048859288804, 1.070137609299693, 1.0707489167885125, 1.0705446687811944, 1.0696786244057532, 1.0696701055189912, 1.0698893194081112, 1.069779842198507, 1.069532334486317, 1.0695031136213142, 1.0694907003359628, 1.070284887456796, 1.0705993579398436, 1.0701413496808594, 1.069373929182362, 1.0704252728691337, 1.069721541708255, 1.0692923132643808, 1.0688587000237844, 1.0695766882729971, 1.068928300820337, 1.069025700390951, 1.0686334690763721, 1.0687045185472932, 1.0684705654943258, 1.0686226140302308, 1.068256675878834, 1.0684893290854578, 1.0684565638125065, 1.0681586041832363, 1.0687554208649748, 1.06799350646487, 1.0683520771884332, 1.0683539464977978, 1.0692773249115053, 1.0681450568185449, 1.0688246352961421, 1.068001628656407, 1.068108191480382, 1.0683510578633333, 1.0684508055387336, 1.0680042119486377, 1.0679190649878563, 1.0677988859662284, 1.0680300925057038, 1.068072753765255, 1.0675408348655309, 1.0683514301781782, 1.0684379055269935, 1.0676834302761227, 1.0676261612032474, 1.0677404723373036, 1.068125477170063, 1.0684611945181657, 1.0679555697607555, 1.06742512391578, 1.0677755921283543, 1.0674189460350991, 1.0676776575112, 1.0677936332671305, 1.0673175944929496], 'acc': [0.3609856268830858, 0.394250514301676, 0.3942505156724605, 0.39383983805439066, 0.39342915808640466, 0.39260780382939675, 0.3860369595536461, 0.3885010274528723, 0.3934291598488418, 0.3942505156724605, 0.3942505125392389, 0.3942505156724605, 0.39342916004466816, 0.3905544167181794, 0.3942505154399167, 0.394250514301676, 0.39425051508498143, 0.3942505148891551, 0.3921971260155006, 0.3926078010878279, 0.39342916004466816, 0.3868583167847667, 0.39260780382939675, 0.3938398374669116, 0.39014373612599695, 0.39507186895033664, 0.39260780285026503, 0.3926078048085285, 0.3963039023920251, 0.3987679688837494, 0.3942505156724605, 0.39425051547663414, 0.39425051547663414, 0.3963039035669832, 0.39630390141289334, 0.4053388076029274, 0.39589322418647627, 0.3942505156724605, 0.394250513714197, 0.39589322418647627, 0.40082135560331406, 0.40574948878258893, 0.4028747448686212, 0.40041067540278424, 0.39835729146150595, 0.3950718685586839, 0.4057494879625661, 0.404106777918657, 0.4008213554074877, 0.40123203400468926, 0.39876796512633134, 0.40657084104461594, 0.40123203204642577, 0.40369609775484466, 0.4008213569740985, 0.395482545197622, 0.39507187090860013, 0.4012320343963419, 0.4057494846335182, 0.4028747426778139, 0.40698152065766663, 0.40164270923612544, 0.4065708436270759, 0.4036960965431691, 0.3995893247073681, 0.40492813217566487, 0.40041067638191596, 0.4008213563866194, 0.3971252554373575, 0.4000000003671744, 0.3975359362253663, 0.3991786465018192, 0.39917864395607666, 0.3958932261447397, 0.401232034983821, 0.3979466116893463, 0.40328541951257835, 0.40328541896181674, 0.40287474349783675, 0.40533880779875375, 0.4036961004964135, 0.40287474388948946, 0.404517453970116, 0.4028747440853158, 0.3983572886832196, 0.40574948662849913, 0.39753593524623454, 0.4028747452602739, 0.3876796704542955, 0.3975359350504082, 0.40123203478799463, 0.4000000007588271, 0.40616016264324073, 0.40082135560331406, 0.4020533870500216, 0.4016427094319518, 0.4057494854168236, 0.3979466108693234, 0.40287474111120314, 0.4012320320097083, 0.40041067935602864, 0.40492813315479664, 0.4036960967389955, 0.4028747413070295, 0.4041067739654126, 0.403285418729273, 0.40287474150285585, 0.3971252562573803, 0.40123203400468926, 0.4041067751403707, 0.40287474150285585, 0.4053388082271239, 0.40451745534090044, 0.40574948721597814, 0.404928133350623, 0.40287474349783675, 0.40246406349313335, 0.40328542307417004, 0.4008213547832912, 0.4016427124060645, 0.40451745475342143, 0.4020533874416743, 0.40410677693952524, 0.4008213565824458, 0.40410677733117795, 0.4041067751403707, 0.3971252589989492, 0.39753593602953996, 0.4016427124060645, 0.4000000003671744, 0.40123203083475023, 0.4045174519751351, 0.39835728852411073, 0.40041067736104774, 0.4049281295932049, 0.40123203361303655, 0.4053388079945801, 0.4004106785727233, 0.40164270884447273, 0.4053388113603455, 0.3946611897289386, 0.404517453970116, 0.38850102725704594, 0.4016427100194308, 0.403285422486691, 0.3963039010212407, 0.3967145809892267, 0.4036960973264745, 0.3971252554373575, 0.403285419316752, 0.4004106771652214, 0.40657084124044224, 0.40287474506444754, 0.4049281315514684, 0.387268992444573, 0.39630390376280955, 0.40492813076816303, 0.4053388079945801, 0.40164270884447273, 0.3971252554373575, 0.39630390020121786, 0.40410677749028684, 0.40287474388948946, 0.39917864630599287, 0.40287474209033486, 0.4020533864625426, 0.39548254793919085, 0.3975359354053435, 0.40698152065766663, 0.40821355429518147, 0.39342915910225384, 0.3938398353128218, 0.39958932353241, 0.4008213536083331, 0.4028747452602739, 0.39342915808640466, 0.39958932192908175, 0.3885010264737405, 0.4078028766403942, 0.40246406587976696, 0.3967145776234613, 0.38521560431750645, 0.40082135736575114, 0.4008213565824458, 0.4032854226825174, 0.40287474150285585, 0.4024640662714196, 0.40287474388948946, 0.4012320314222293, 0.40082135579914036, 0.39958932333658365, 0.3983572900907215, 0.4020533862667162, 0.3963039023920251, 0.40123203361303655, 0.401232034983821, 0.3938398372710853, 0.40041067618608966, 0.40246406705472504, 0.3917864487890835, 0.4053388103812137, 0.4016427114269327, 0.3905544129607614, 0.3958932247739553, 0.3987679688837494, 0.39917864450683826, 0.39507186895033664, 0.39917864630599287, 0.399589324119889, 0.39835729106985324, 0.4094455833919729, 0.40328542052842753, 0.39219712343304064, 0.3880903486598444, 0.4020533894366552, 0.4057494850251709, 0.39958932235745187, 0.395893222583148, 0.3954825465684064, 0.40533880959790836, 0.4016427094319518, 0.4028747444769685, 0.4004106783768969, 0.3958932235989972, 0.4065708436270759, 0.4053388078354712, 0.38069815055545597, 0.3864476387750442, 0.40123203322138384, 0.4073921982757365, 0.395071869146163, 0.4082135515168952, 0.4016427098236045, 0.40410677811448337, 0.39712525821564376, 0.40164271123110634, 0.40205338963248155, 0.40698152065766663, 0.40574948522099724, 0.3991786437602503, 0.39999999778471446, 0.3999999991922163, 0.4053388083862328, 0.4036960987339764, 0.4049281299848576, 0.40451745612420587, 0.4024640668588987, 0.3979466108693234, 0.4008213563866194, 0.3975359328596009, 0.3983572882915669, 0.40533880881460294, 0.3930184798808558, 0.39835729106985324, 0.3938398335136672, 0.4016427102519746, 0.4036960967389955, 0.40328541951257835, 0.4028747448686212, 0.4045174519751351, 0.40041067540278424, 0.40492813217566487, 0.4012320322422521, 0.406160164834048, 0.4045174551450741, 0.4045174525626141, 0.40369609853815003, 0.40041067778941786, 0.4045174559283795, 0.3979466138434361, 0.4049281295932049, 0.40000000134630614, 0.40082135677827213, 0.39835729106985324, 0.4061601634265461, 0.40082135521166135, 0.40205339002413426, 0.40451745416594237, 0.4000000007588271, 0.40041067720193885, 0.4028747427145314, 0.4041067759603935, 0.4041067761195024, 0.4024640656839406, 0.40205339002413426, 0.40451745334591954, 0.40246406408061236, 0.4032854189250993, 0.4012320324380785, 0.40369609693482184, 0.403285419316752, 0.4045174549492478, 0.40451745279515794, 0.4016427098236045, 0.3971252584114701, 0.4045174523667878, 0.40246406705472504, 0.40164270923612544, 0.40410677435706527, 0.4036961006922399]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
