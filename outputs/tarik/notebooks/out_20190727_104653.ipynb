{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf38.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 10:46:53 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'Split', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 5 Label(s): ['04', '03', '02', '01', '05'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001E9B2F22E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001E9B05D7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6131, Accuracy:0.1960, Validation Loss:1.6060, Validation Accuracy:0.2328\n",
    "Epoch #2: Loss:1.6059, Accuracy:0.2330, Validation Loss:1.6055, Validation Accuracy:0.2328\n",
    "Epoch #3: Loss:1.6061, Accuracy:0.2330, Validation Loss:1.6056, Validation Accuracy:0.2328\n",
    "Epoch #4: Loss:1.6064, Accuracy:0.2330, Validation Loss:1.6061, Validation Accuracy:0.2328\n",
    "Epoch #5: Loss:1.6056, Accuracy:0.2330, Validation Loss:1.6049, Validation Accuracy:0.2328\n",
    "Epoch #6: Loss:1.6047, Accuracy:0.2330, Validation Loss:1.6041, Validation Accuracy:0.2328\n",
    "Epoch #7: Loss:1.6039, Accuracy:0.2330, Validation Loss:1.6023, Validation Accuracy:0.2328\n",
    "Epoch #8: Loss:1.6027, Accuracy:0.2269, Validation Loss:1.6011, Validation Accuracy:0.2422\n",
    "Epoch #9: Loss:1.5991, Accuracy:0.2357, Validation Loss:1.5945, Validation Accuracy:0.2422\n",
    "Epoch #10: Loss:1.5913, Accuracy:0.2433, Validation Loss:1.5849, Validation Accuracy:0.2406\n",
    "Epoch #11: Loss:1.5854, Accuracy:0.2406, Validation Loss:1.5825, Validation Accuracy:0.2640\n",
    "Epoch #12: Loss:1.5843, Accuracy:0.2507, Validation Loss:1.5777, Validation Accuracy:0.2672\n",
    "Epoch #13: Loss:1.5833, Accuracy:0.2556, Validation Loss:1.5815, Validation Accuracy:0.2627\n",
    "Epoch #14: Loss:1.5809, Accuracy:0.2586, Validation Loss:1.5774, Validation Accuracy:0.2648\n",
    "Epoch #15: Loss:1.5790, Accuracy:0.2615, Validation Loss:1.5774, Validation Accuracy:0.2664\n",
    "Epoch #16: Loss:1.5782, Accuracy:0.2627, Validation Loss:1.5782, Validation Accuracy:0.2681\n",
    "Epoch #17: Loss:1.5788, Accuracy:0.2634, Validation Loss:1.5750, Validation Accuracy:0.2709\n",
    "Epoch #18: Loss:1.5790, Accuracy:0.2643, Validation Loss:1.5757, Validation Accuracy:0.2705\n",
    "Epoch #19: Loss:1.5786, Accuracy:0.2619, Validation Loss:1.5762, Validation Accuracy:0.2705\n",
    "Epoch #20: Loss:1.5778, Accuracy:0.2612, Validation Loss:1.5739, Validation Accuracy:0.2701\n",
    "Epoch #21: Loss:1.5777, Accuracy:0.2605, Validation Loss:1.5751, Validation Accuracy:0.2685\n",
    "Epoch #22: Loss:1.5797, Accuracy:0.2618, Validation Loss:1.5776, Validation Accuracy:0.2627\n",
    "Epoch #23: Loss:1.5780, Accuracy:0.2634, Validation Loss:1.5738, Validation Accuracy:0.2705\n",
    "Epoch #24: Loss:1.5761, Accuracy:0.2646, Validation Loss:1.5739, Validation Accuracy:0.2730\n",
    "Epoch #25: Loss:1.5757, Accuracy:0.2628, Validation Loss:1.5752, Validation Accuracy:0.2755\n",
    "Epoch #26: Loss:1.5752, Accuracy:0.2646, Validation Loss:1.5734, Validation Accuracy:0.2705\n",
    "Epoch #27: Loss:1.5742, Accuracy:0.2619, Validation Loss:1.5737, Validation Accuracy:0.2701\n",
    "Epoch #28: Loss:1.5742, Accuracy:0.2667, Validation Loss:1.5732, Validation Accuracy:0.2730\n",
    "Epoch #29: Loss:1.5748, Accuracy:0.2705, Validation Loss:1.5727, Validation Accuracy:0.2746\n",
    "Epoch #30: Loss:1.5775, Accuracy:0.2639, Validation Loss:1.5727, Validation Accuracy:0.2701\n",
    "Epoch #31: Loss:1.5740, Accuracy:0.2662, Validation Loss:1.5737, Validation Accuracy:0.2726\n",
    "Epoch #32: Loss:1.5732, Accuracy:0.2672, Validation Loss:1.5725, Validation Accuracy:0.2656\n",
    "Epoch #33: Loss:1.5731, Accuracy:0.2649, Validation Loss:1.5729, Validation Accuracy:0.2718\n",
    "Epoch #34: Loss:1.5726, Accuracy:0.2676, Validation Loss:1.5718, Validation Accuracy:0.2689\n",
    "Epoch #35: Loss:1.5726, Accuracy:0.2663, Validation Loss:1.5718, Validation Accuracy:0.2730\n",
    "Epoch #36: Loss:1.5722, Accuracy:0.2691, Validation Loss:1.5722, Validation Accuracy:0.2738\n",
    "Epoch #37: Loss:1.5712, Accuracy:0.2701, Validation Loss:1.5713, Validation Accuracy:0.2718\n",
    "Epoch #38: Loss:1.5746, Accuracy:0.2683, Validation Loss:1.5711, Validation Accuracy:0.2718\n",
    "Epoch #39: Loss:1.5701, Accuracy:0.2709, Validation Loss:1.5726, Validation Accuracy:0.2685\n",
    "Epoch #40: Loss:1.5720, Accuracy:0.2718, Validation Loss:1.5705, Validation Accuracy:0.2672\n",
    "Epoch #41: Loss:1.5694, Accuracy:0.2694, Validation Loss:1.5700, Validation Accuracy:0.2697\n",
    "Epoch #42: Loss:1.5691, Accuracy:0.2705, Validation Loss:1.5727, Validation Accuracy:0.2722\n",
    "Epoch #43: Loss:1.5710, Accuracy:0.2700, Validation Loss:1.5694, Validation Accuracy:0.2701\n",
    "Epoch #44: Loss:1.5678, Accuracy:0.2724, Validation Loss:1.5698, Validation Accuracy:0.2681\n",
    "Epoch #45: Loss:1.5691, Accuracy:0.2715, Validation Loss:1.5728, Validation Accuracy:0.2726\n",
    "Epoch #46: Loss:1.5720, Accuracy:0.2666, Validation Loss:1.5686, Validation Accuracy:0.2648\n",
    "Epoch #47: Loss:1.5663, Accuracy:0.2731, Validation Loss:1.5673, Validation Accuracy:0.2693\n",
    "Epoch #48: Loss:1.5665, Accuracy:0.2726, Validation Loss:1.5669, Validation Accuracy:0.2644\n",
    "Epoch #49: Loss:1.5665, Accuracy:0.2752, Validation Loss:1.5672, Validation Accuracy:0.2722\n",
    "Epoch #50: Loss:1.5642, Accuracy:0.2734, Validation Loss:1.5677, Validation Accuracy:0.2718\n",
    "Epoch #51: Loss:1.5643, Accuracy:0.2751, Validation Loss:1.5664, Validation Accuracy:0.2734\n",
    "Epoch #52: Loss:1.5631, Accuracy:0.2731, Validation Loss:1.5659, Validation Accuracy:0.2689\n",
    "Epoch #53: Loss:1.5631, Accuracy:0.2751, Validation Loss:1.5647, Validation Accuracy:0.2685\n",
    "Epoch #54: Loss:1.5613, Accuracy:0.2800, Validation Loss:1.5654, Validation Accuracy:0.2693\n",
    "Epoch #55: Loss:1.5632, Accuracy:0.2779, Validation Loss:1.5644, Validation Accuracy:0.2693\n",
    "Epoch #56: Loss:1.5607, Accuracy:0.2798, Validation Loss:1.5658, Validation Accuracy:0.2722\n",
    "Epoch #57: Loss:1.5591, Accuracy:0.2802, Validation Loss:1.5649, Validation Accuracy:0.2685\n",
    "Epoch #58: Loss:1.5639, Accuracy:0.2791, Validation Loss:1.5655, Validation Accuracy:0.2730\n",
    "Epoch #59: Loss:1.5606, Accuracy:0.2806, Validation Loss:1.5637, Validation Accuracy:0.2693\n",
    "Epoch #60: Loss:1.5605, Accuracy:0.2815, Validation Loss:1.5621, Validation Accuracy:0.2775\n",
    "Epoch #61: Loss:1.5587, Accuracy:0.2836, Validation Loss:1.5619, Validation Accuracy:0.2742\n",
    "Epoch #62: Loss:1.5593, Accuracy:0.2826, Validation Loss:1.5628, Validation Accuracy:0.2775\n",
    "Epoch #63: Loss:1.5576, Accuracy:0.2805, Validation Loss:1.5628, Validation Accuracy:0.2689\n",
    "Epoch #64: Loss:1.5572, Accuracy:0.2812, Validation Loss:1.5619, Validation Accuracy:0.2767\n",
    "Epoch #65: Loss:1.5582, Accuracy:0.2842, Validation Loss:1.5666, Validation Accuracy:0.2816\n",
    "Epoch #66: Loss:1.5678, Accuracy:0.2851, Validation Loss:1.5673, Validation Accuracy:0.2664\n",
    "Epoch #67: Loss:1.5622, Accuracy:0.2806, Validation Loss:1.5629, Validation Accuracy:0.2701\n",
    "Epoch #68: Loss:1.5578, Accuracy:0.2836, Validation Loss:1.5630, Validation Accuracy:0.2800\n",
    "Epoch #69: Loss:1.5589, Accuracy:0.2822, Validation Loss:1.5599, Validation Accuracy:0.2779\n",
    "Epoch #70: Loss:1.5566, Accuracy:0.2871, Validation Loss:1.5623, Validation Accuracy:0.2726\n",
    "Epoch #71: Loss:1.5609, Accuracy:0.2815, Validation Loss:1.5664, Validation Accuracy:0.2841\n",
    "Epoch #72: Loss:1.5661, Accuracy:0.2814, Validation Loss:1.5641, Validation Accuracy:0.2730\n",
    "Epoch #73: Loss:1.5566, Accuracy:0.2876, Validation Loss:1.5612, Validation Accuracy:0.2755\n",
    "Epoch #74: Loss:1.5565, Accuracy:0.2871, Validation Loss:1.5592, Validation Accuracy:0.2779\n",
    "Epoch #75: Loss:1.5550, Accuracy:0.2860, Validation Loss:1.5589, Validation Accuracy:0.2742\n",
    "Epoch #76: Loss:1.5550, Accuracy:0.2835, Validation Loss:1.5592, Validation Accuracy:0.2709\n",
    "Epoch #77: Loss:1.5551, Accuracy:0.2879, Validation Loss:1.5586, Validation Accuracy:0.2787\n",
    "Epoch #78: Loss:1.5533, Accuracy:0.2872, Validation Loss:1.5583, Validation Accuracy:0.2767\n",
    "Epoch #79: Loss:1.5536, Accuracy:0.2874, Validation Loss:1.5585, Validation Accuracy:0.2796\n",
    "Epoch #80: Loss:1.5541, Accuracy:0.2864, Validation Loss:1.5593, Validation Accuracy:0.2713\n",
    "Epoch #81: Loss:1.5535, Accuracy:0.2862, Validation Loss:1.5594, Validation Accuracy:0.2804\n",
    "Epoch #82: Loss:1.5531, Accuracy:0.2882, Validation Loss:1.5583, Validation Accuracy:0.2796\n",
    "Epoch #83: Loss:1.5535, Accuracy:0.2816, Validation Loss:1.5591, Validation Accuracy:0.2796\n",
    "Epoch #84: Loss:1.5539, Accuracy:0.2826, Validation Loss:1.5629, Validation Accuracy:0.2713\n",
    "Epoch #85: Loss:1.5568, Accuracy:0.2815, Validation Loss:1.5586, Validation Accuracy:0.2824\n",
    "Epoch #86: Loss:1.5524, Accuracy:0.2831, Validation Loss:1.5580, Validation Accuracy:0.2763\n",
    "Epoch #87: Loss:1.5522, Accuracy:0.2814, Validation Loss:1.5608, Validation Accuracy:0.2816\n",
    "Epoch #88: Loss:1.5533, Accuracy:0.2860, Validation Loss:1.5563, Validation Accuracy:0.2746\n",
    "Epoch #89: Loss:1.5523, Accuracy:0.2808, Validation Loss:1.5564, Validation Accuracy:0.2787\n",
    "Epoch #90: Loss:1.5521, Accuracy:0.2866, Validation Loss:1.5563, Validation Accuracy:0.2812\n",
    "Epoch #91: Loss:1.5517, Accuracy:0.2852, Validation Loss:1.5582, Validation Accuracy:0.2771\n",
    "Epoch #92: Loss:1.5524, Accuracy:0.2867, Validation Loss:1.5572, Validation Accuracy:0.2771\n",
    "Epoch #93: Loss:1.5512, Accuracy:0.2863, Validation Loss:1.5585, Validation Accuracy:0.2763\n",
    "Epoch #94: Loss:1.5503, Accuracy:0.2860, Validation Loss:1.5615, Validation Accuracy:0.2771\n",
    "Epoch #95: Loss:1.5520, Accuracy:0.2872, Validation Loss:1.5574, Validation Accuracy:0.2709\n",
    "Epoch #96: Loss:1.5565, Accuracy:0.2807, Validation Loss:1.5582, Validation Accuracy:0.2816\n",
    "Epoch #97: Loss:1.5512, Accuracy:0.2863, Validation Loss:1.5596, Validation Accuracy:0.2783\n",
    "Epoch #98: Loss:1.5532, Accuracy:0.2917, Validation Loss:1.5592, Validation Accuracy:0.2718\n",
    "Epoch #99: Loss:1.5503, Accuracy:0.2873, Validation Loss:1.5572, Validation Accuracy:0.2775\n",
    "Epoch #100: Loss:1.5496, Accuracy:0.2908, Validation Loss:1.5578, Validation Accuracy:0.2779\n",
    "Epoch #101: Loss:1.5521, Accuracy:0.2855, Validation Loss:1.5560, Validation Accuracy:0.2808\n",
    "Epoch #102: Loss:1.5499, Accuracy:0.2883, Validation Loss:1.5550, Validation Accuracy:0.2808\n",
    "Epoch #103: Loss:1.5494, Accuracy:0.2868, Validation Loss:1.5614, Validation Accuracy:0.2828\n",
    "Epoch #104: Loss:1.5615, Accuracy:0.2805, Validation Loss:1.5583, Validation Accuracy:0.2816\n",
    "Epoch #105: Loss:1.5527, Accuracy:0.2938, Validation Loss:1.5567, Validation Accuracy:0.2759\n",
    "Epoch #106: Loss:1.5504, Accuracy:0.2912, Validation Loss:1.5554, Validation Accuracy:0.2763\n",
    "Epoch #107: Loss:1.5492, Accuracy:0.2879, Validation Loss:1.5546, Validation Accuracy:0.2824\n",
    "Epoch #108: Loss:1.5484, Accuracy:0.2873, Validation Loss:1.5549, Validation Accuracy:0.2841\n",
    "Epoch #109: Loss:1.5478, Accuracy:0.2877, Validation Loss:1.5562, Validation Accuracy:0.2861\n",
    "Epoch #110: Loss:1.5480, Accuracy:0.2924, Validation Loss:1.5545, Validation Accuracy:0.2853\n",
    "Epoch #111: Loss:1.5483, Accuracy:0.2877, Validation Loss:1.5540, Validation Accuracy:0.2816\n",
    "Epoch #112: Loss:1.5477, Accuracy:0.2900, Validation Loss:1.5563, Validation Accuracy:0.2902\n",
    "Epoch #113: Loss:1.5485, Accuracy:0.2922, Validation Loss:1.5575, Validation Accuracy:0.2771\n",
    "Epoch #114: Loss:1.5482, Accuracy:0.2929, Validation Loss:1.5534, Validation Accuracy:0.2804\n",
    "Epoch #115: Loss:1.5471, Accuracy:0.2903, Validation Loss:1.5541, Validation Accuracy:0.2767\n",
    "Epoch #116: Loss:1.5506, Accuracy:0.2918, Validation Loss:1.5533, Validation Accuracy:0.2837\n",
    "Epoch #117: Loss:1.5461, Accuracy:0.2924, Validation Loss:1.5531, Validation Accuracy:0.2812\n",
    "Epoch #118: Loss:1.5471, Accuracy:0.2926, Validation Loss:1.5581, Validation Accuracy:0.2894\n",
    "Epoch #119: Loss:1.5502, Accuracy:0.2938, Validation Loss:1.5530, Validation Accuracy:0.2898\n",
    "Epoch #120: Loss:1.5483, Accuracy:0.2930, Validation Loss:1.5538, Validation Accuracy:0.2861\n",
    "Epoch #121: Loss:1.5453, Accuracy:0.2936, Validation Loss:1.5547, Validation Accuracy:0.2742\n",
    "Epoch #122: Loss:1.5448, Accuracy:0.2902, Validation Loss:1.5552, Validation Accuracy:0.2878\n",
    "Epoch #123: Loss:1.5457, Accuracy:0.2922, Validation Loss:1.5545, Validation Accuracy:0.2845\n",
    "Epoch #124: Loss:1.5456, Accuracy:0.2910, Validation Loss:1.5520, Validation Accuracy:0.2828\n",
    "Epoch #125: Loss:1.5459, Accuracy:0.2893, Validation Loss:1.5520, Validation Accuracy:0.2865\n",
    "Epoch #126: Loss:1.5447, Accuracy:0.2924, Validation Loss:1.5524, Validation Accuracy:0.2911\n",
    "Epoch #127: Loss:1.5429, Accuracy:0.2999, Validation Loss:1.5538, Validation Accuracy:0.2816\n",
    "Epoch #128: Loss:1.5439, Accuracy:0.3007, Validation Loss:1.5578, Validation Accuracy:0.2845\n",
    "Epoch #129: Loss:1.5447, Accuracy:0.2913, Validation Loss:1.5510, Validation Accuracy:0.2882\n",
    "Epoch #130: Loss:1.5460, Accuracy:0.2964, Validation Loss:1.5539, Validation Accuracy:0.2853\n",
    "Epoch #131: Loss:1.5486, Accuracy:0.2937, Validation Loss:1.5523, Validation Accuracy:0.2865\n",
    "Epoch #132: Loss:1.5433, Accuracy:0.2978, Validation Loss:1.5517, Validation Accuracy:0.2853\n",
    "Epoch #133: Loss:1.5417, Accuracy:0.2999, Validation Loss:1.5541, Validation Accuracy:0.2796\n",
    "Epoch #134: Loss:1.5458, Accuracy:0.2971, Validation Loss:1.5565, Validation Accuracy:0.2713\n",
    "Epoch #135: Loss:1.5448, Accuracy:0.2961, Validation Loss:1.5515, Validation Accuracy:0.2923\n",
    "Epoch #136: Loss:1.5420, Accuracy:0.2991, Validation Loss:1.5562, Validation Accuracy:0.2837\n",
    "Epoch #137: Loss:1.5441, Accuracy:0.2956, Validation Loss:1.5489, Validation Accuracy:0.2865\n",
    "Epoch #138: Loss:1.5405, Accuracy:0.3027, Validation Loss:1.5492, Validation Accuracy:0.2791\n",
    "Epoch #139: Loss:1.5432, Accuracy:0.2980, Validation Loss:1.5505, Validation Accuracy:0.2853\n",
    "Epoch #140: Loss:1.5416, Accuracy:0.3009, Validation Loss:1.5502, Validation Accuracy:0.2861\n",
    "Epoch #141: Loss:1.5412, Accuracy:0.3034, Validation Loss:1.5515, Validation Accuracy:0.2816\n",
    "Epoch #142: Loss:1.5405, Accuracy:0.3046, Validation Loss:1.5507, Validation Accuracy:0.2882\n",
    "Epoch #143: Loss:1.5403, Accuracy:0.3067, Validation Loss:1.5534, Validation Accuracy:0.2902\n",
    "Epoch #144: Loss:1.5423, Accuracy:0.3048, Validation Loss:1.5528, Validation Accuracy:0.2857\n",
    "Epoch #145: Loss:1.5419, Accuracy:0.3060, Validation Loss:1.5537, Validation Accuracy:0.2931\n",
    "Epoch #146: Loss:1.5420, Accuracy:0.3062, Validation Loss:1.5512, Validation Accuracy:0.2898\n",
    "Epoch #147: Loss:1.5417, Accuracy:0.3013, Validation Loss:1.5518, Validation Accuracy:0.2911\n",
    "Epoch #148: Loss:1.5400, Accuracy:0.3046, Validation Loss:1.5520, Validation Accuracy:0.2861\n",
    "Epoch #149: Loss:1.5405, Accuracy:0.3075, Validation Loss:1.5489, Validation Accuracy:0.2816\n",
    "Epoch #150: Loss:1.5382, Accuracy:0.3081, Validation Loss:1.5489, Validation Accuracy:0.2886\n",
    "Epoch #151: Loss:1.5389, Accuracy:0.3055, Validation Loss:1.5491, Validation Accuracy:0.2841\n",
    "Epoch #152: Loss:1.5393, Accuracy:0.3059, Validation Loss:1.5492, Validation Accuracy:0.2919\n",
    "Epoch #153: Loss:1.5382, Accuracy:0.3107, Validation Loss:1.5513, Validation Accuracy:0.2923\n",
    "Epoch #154: Loss:1.5378, Accuracy:0.3087, Validation Loss:1.5465, Validation Accuracy:0.2911\n",
    "Epoch #155: Loss:1.5352, Accuracy:0.3109, Validation Loss:1.5475, Validation Accuracy:0.2882\n",
    "Epoch #156: Loss:1.5367, Accuracy:0.3118, Validation Loss:1.5508, Validation Accuracy:0.2956\n",
    "Epoch #157: Loss:1.5372, Accuracy:0.3140, Validation Loss:1.5479, Validation Accuracy:0.2915\n",
    "Epoch #158: Loss:1.5367, Accuracy:0.3112, Validation Loss:1.5501, Validation Accuracy:0.2886\n",
    "Epoch #159: Loss:1.5392, Accuracy:0.3099, Validation Loss:1.5505, Validation Accuracy:0.2927\n",
    "Epoch #160: Loss:1.5388, Accuracy:0.3060, Validation Loss:1.5487, Validation Accuracy:0.2874\n",
    "Epoch #161: Loss:1.5334, Accuracy:0.3130, Validation Loss:1.5487, Validation Accuracy:0.2989\n",
    "Epoch #162: Loss:1.5338, Accuracy:0.3137, Validation Loss:1.5482, Validation Accuracy:0.3013\n",
    "Epoch #163: Loss:1.5346, Accuracy:0.3110, Validation Loss:1.5517, Validation Accuracy:0.2882\n",
    "Epoch #164: Loss:1.5335, Accuracy:0.3156, Validation Loss:1.5477, Validation Accuracy:0.2882\n",
    "Epoch #165: Loss:1.5323, Accuracy:0.3164, Validation Loss:1.5509, Validation Accuracy:0.2952\n",
    "Epoch #166: Loss:1.5434, Accuracy:0.3061, Validation Loss:1.5476, Validation Accuracy:0.2915\n",
    "Epoch #167: Loss:1.5354, Accuracy:0.3146, Validation Loss:1.5479, Validation Accuracy:0.2980\n",
    "Epoch #168: Loss:1.5338, Accuracy:0.3138, Validation Loss:1.5481, Validation Accuracy:0.2857\n",
    "Epoch #169: Loss:1.5336, Accuracy:0.3153, Validation Loss:1.5478, Validation Accuracy:0.2898\n",
    "Epoch #170: Loss:1.5326, Accuracy:0.3177, Validation Loss:1.5488, Validation Accuracy:0.2882\n",
    "Epoch #171: Loss:1.5331, Accuracy:0.3120, Validation Loss:1.5502, Validation Accuracy:0.2898\n",
    "Epoch #172: Loss:1.5319, Accuracy:0.3149, Validation Loss:1.5491, Validation Accuracy:0.2952\n",
    "Epoch #173: Loss:1.5316, Accuracy:0.3208, Validation Loss:1.5458, Validation Accuracy:0.2927\n",
    "Epoch #174: Loss:1.5297, Accuracy:0.3206, Validation Loss:1.5468, Validation Accuracy:0.2989\n",
    "Epoch #175: Loss:1.5296, Accuracy:0.3176, Validation Loss:1.5482, Validation Accuracy:0.2824\n",
    "Epoch #176: Loss:1.5299, Accuracy:0.3159, Validation Loss:1.5482, Validation Accuracy:0.2919\n",
    "Epoch #177: Loss:1.5326, Accuracy:0.3134, Validation Loss:1.5481, Validation Accuracy:0.2997\n",
    "Epoch #178: Loss:1.5352, Accuracy:0.3144, Validation Loss:1.5486, Validation Accuracy:0.2984\n",
    "Epoch #179: Loss:1.5366, Accuracy:0.3059, Validation Loss:1.5489, Validation Accuracy:0.2911\n",
    "Epoch #180: Loss:1.5299, Accuracy:0.3205, Validation Loss:1.5472, Validation Accuracy:0.2993\n",
    "Epoch #181: Loss:1.5295, Accuracy:0.3193, Validation Loss:1.5470, Validation Accuracy:0.2968\n",
    "Epoch #182: Loss:1.5292, Accuracy:0.3153, Validation Loss:1.5466, Validation Accuracy:0.2890\n",
    "Epoch #183: Loss:1.5300, Accuracy:0.3134, Validation Loss:1.5498, Validation Accuracy:0.2890\n",
    "Epoch #184: Loss:1.5319, Accuracy:0.3150, Validation Loss:1.5441, Validation Accuracy:0.3009\n",
    "Epoch #185: Loss:1.5287, Accuracy:0.3170, Validation Loss:1.5453, Validation Accuracy:0.2997\n",
    "Epoch #186: Loss:1.5282, Accuracy:0.3157, Validation Loss:1.5448, Validation Accuracy:0.3038\n",
    "Epoch #187: Loss:1.5272, Accuracy:0.3207, Validation Loss:1.5485, Validation Accuracy:0.2993\n",
    "Epoch #188: Loss:1.5255, Accuracy:0.3218, Validation Loss:1.5464, Validation Accuracy:0.2993\n",
    "Epoch #189: Loss:1.5259, Accuracy:0.3227, Validation Loss:1.5485, Validation Accuracy:0.2997\n",
    "Epoch #190: Loss:1.5329, Accuracy:0.3112, Validation Loss:1.5445, Validation Accuracy:0.2952\n",
    "Epoch #191: Loss:1.5294, Accuracy:0.3150, Validation Loss:1.5432, Validation Accuracy:0.2927\n",
    "Epoch #192: Loss:1.5263, Accuracy:0.3202, Validation Loss:1.5439, Validation Accuracy:0.3021\n",
    "Epoch #193: Loss:1.5272, Accuracy:0.3195, Validation Loss:1.5437, Validation Accuracy:0.3075\n",
    "Epoch #194: Loss:1.5284, Accuracy:0.3182, Validation Loss:1.5487, Validation Accuracy:0.3083\n",
    "Epoch #195: Loss:1.5274, Accuracy:0.3202, Validation Loss:1.5420, Validation Accuracy:0.2989\n",
    "Epoch #196: Loss:1.5279, Accuracy:0.3171, Validation Loss:1.5423, Validation Accuracy:0.3013\n",
    "Epoch #197: Loss:1.5242, Accuracy:0.3200, Validation Loss:1.5427, Validation Accuracy:0.3034\n",
    "Epoch #198: Loss:1.5236, Accuracy:0.3208, Validation Loss:1.5488, Validation Accuracy:0.2935\n",
    "Epoch #199: Loss:1.5260, Accuracy:0.3199, Validation Loss:1.5456, Validation Accuracy:0.2968\n",
    "Epoch #200: Loss:1.5258, Accuracy:0.3205, Validation Loss:1.5418, Validation Accuracy:0.3009\n",
    "Epoch #201: Loss:1.5214, Accuracy:0.3259, Validation Loss:1.5437, Validation Accuracy:0.3013\n",
    "Epoch #202: Loss:1.5322, Accuracy:0.3112, Validation Loss:1.5429, Validation Accuracy:0.3030\n",
    "Epoch #203: Loss:1.5228, Accuracy:0.3241, Validation Loss:1.5445, Validation Accuracy:0.3005\n",
    "Epoch #204: Loss:1.5241, Accuracy:0.3198, Validation Loss:1.5456, Validation Accuracy:0.2960\n",
    "Epoch #205: Loss:1.5263, Accuracy:0.3210, Validation Loss:1.5393, Validation Accuracy:0.2943\n",
    "Epoch #206: Loss:1.5244, Accuracy:0.3165, Validation Loss:1.5472, Validation Accuracy:0.3095\n",
    "Epoch #207: Loss:1.5229, Accuracy:0.3193, Validation Loss:1.5423, Validation Accuracy:0.2960\n",
    "Epoch #208: Loss:1.5208, Accuracy:0.3226, Validation Loss:1.5473, Validation Accuracy:0.2906\n",
    "Epoch #209: Loss:1.5371, Accuracy:0.3130, Validation Loss:1.5457, Validation Accuracy:0.2980\n",
    "Epoch #210: Loss:1.5277, Accuracy:0.3210, Validation Loss:1.5454, Validation Accuracy:0.2923\n",
    "Epoch #211: Loss:1.5238, Accuracy:0.3217, Validation Loss:1.5397, Validation Accuracy:0.3083\n",
    "Epoch #212: Loss:1.5227, Accuracy:0.3242, Validation Loss:1.5382, Validation Accuracy:0.3021\n",
    "Epoch #213: Loss:1.5212, Accuracy:0.3283, Validation Loss:1.5415, Validation Accuracy:0.3050\n",
    "Epoch #214: Loss:1.5215, Accuracy:0.3244, Validation Loss:1.5398, Validation Accuracy:0.2993\n",
    "Epoch #215: Loss:1.5221, Accuracy:0.3180, Validation Loss:1.5409, Validation Accuracy:0.3046\n",
    "Epoch #216: Loss:1.5189, Accuracy:0.3251, Validation Loss:1.5410, Validation Accuracy:0.2964\n",
    "Epoch #217: Loss:1.5231, Accuracy:0.3234, Validation Loss:1.5384, Validation Accuracy:0.3001\n",
    "Epoch #218: Loss:1.5213, Accuracy:0.3248, Validation Loss:1.5434, Validation Accuracy:0.3025\n",
    "Epoch #219: Loss:1.5208, Accuracy:0.3249, Validation Loss:1.5425, Validation Accuracy:0.3083\n",
    "Epoch #220: Loss:1.5193, Accuracy:0.3261, Validation Loss:1.5409, Validation Accuracy:0.2997\n",
    "Epoch #221: Loss:1.5202, Accuracy:0.3252, Validation Loss:1.5399, Validation Accuracy:0.3034\n",
    "Epoch #222: Loss:1.5217, Accuracy:0.3214, Validation Loss:1.5439, Validation Accuracy:0.2997\n",
    "Epoch #223: Loss:1.5222, Accuracy:0.3235, Validation Loss:1.5399, Validation Accuracy:0.2939\n",
    "Epoch #224: Loss:1.5242, Accuracy:0.3199, Validation Loss:1.5386, Validation Accuracy:0.3034\n",
    "Epoch #225: Loss:1.5178, Accuracy:0.3252, Validation Loss:1.5376, Validation Accuracy:0.3079\n",
    "Epoch #226: Loss:1.5185, Accuracy:0.3220, Validation Loss:1.5375, Validation Accuracy:0.3124\n",
    "Epoch #227: Loss:1.5202, Accuracy:0.3235, Validation Loss:1.5402, Validation Accuracy:0.3050\n",
    "Epoch #228: Loss:1.5176, Accuracy:0.3266, Validation Loss:1.5385, Validation Accuracy:0.3025\n",
    "Epoch #229: Loss:1.5179, Accuracy:0.3220, Validation Loss:1.5378, Validation Accuracy:0.3095\n",
    "Epoch #230: Loss:1.5156, Accuracy:0.3280, Validation Loss:1.5382, Validation Accuracy:0.3108\n",
    "Epoch #231: Loss:1.5149, Accuracy:0.3314, Validation Loss:1.5378, Validation Accuracy:0.3001\n",
    "Epoch #232: Loss:1.5191, Accuracy:0.3306, Validation Loss:1.5368, Validation Accuracy:0.3046\n",
    "Epoch #233: Loss:1.5137, Accuracy:0.3280, Validation Loss:1.5416, Validation Accuracy:0.3058\n",
    "Epoch #234: Loss:1.5153, Accuracy:0.3266, Validation Loss:1.5381, Validation Accuracy:0.3095\n",
    "Epoch #235: Loss:1.5129, Accuracy:0.3312, Validation Loss:1.5398, Validation Accuracy:0.3054\n",
    "Epoch #236: Loss:1.5161, Accuracy:0.3284, Validation Loss:1.5402, Validation Accuracy:0.2989\n",
    "Epoch #237: Loss:1.5174, Accuracy:0.3259, Validation Loss:1.5411, Validation Accuracy:0.3001\n",
    "Epoch #238: Loss:1.5120, Accuracy:0.3328, Validation Loss:1.5437, Validation Accuracy:0.3009\n",
    "Epoch #239: Loss:1.5140, Accuracy:0.3322, Validation Loss:1.5386, Validation Accuracy:0.3038\n",
    "Epoch #240: Loss:1.5143, Accuracy:0.3302, Validation Loss:1.5353, Validation Accuracy:0.2968\n",
    "Epoch #241: Loss:1.5140, Accuracy:0.3289, Validation Loss:1.5376, Validation Accuracy:0.3062\n",
    "Epoch #242: Loss:1.5129, Accuracy:0.3321, Validation Loss:1.5390, Validation Accuracy:0.3058\n",
    "Epoch #243: Loss:1.5167, Accuracy:0.3245, Validation Loss:1.5357, Validation Accuracy:0.2960\n",
    "Epoch #244: Loss:1.5129, Accuracy:0.3287, Validation Loss:1.5373, Validation Accuracy:0.3042\n",
    "Epoch #245: Loss:1.5148, Accuracy:0.3292, Validation Loss:1.5343, Validation Accuracy:0.3038\n",
    "Epoch #246: Loss:1.5166, Accuracy:0.3232, Validation Loss:1.5377, Validation Accuracy:0.3083\n",
    "Epoch #247: Loss:1.5136, Accuracy:0.3285, Validation Loss:1.5374, Validation Accuracy:0.3058\n",
    "Epoch #248: Loss:1.5116, Accuracy:0.3292, Validation Loss:1.5394, Validation Accuracy:0.3091\n",
    "Epoch #249: Loss:1.5131, Accuracy:0.3293, Validation Loss:1.5343, Validation Accuracy:0.3099\n",
    "Epoch #250: Loss:1.5105, Accuracy:0.3330, Validation Loss:1.5405, Validation Accuracy:0.3054\n",
    "Epoch #251: Loss:1.5169, Accuracy:0.3268, Validation Loss:1.5330, Validation Accuracy:0.3067\n",
    "Epoch #252: Loss:1.5137, Accuracy:0.3263, Validation Loss:1.5360, Validation Accuracy:0.3079\n",
    "Epoch #253: Loss:1.5129, Accuracy:0.3317, Validation Loss:1.5313, Validation Accuracy:0.3030\n",
    "Epoch #254: Loss:1.5161, Accuracy:0.3285, Validation Loss:1.5363, Validation Accuracy:0.3025\n",
    "Epoch #255: Loss:1.5104, Accuracy:0.3307, Validation Loss:1.5361, Validation Accuracy:0.3075\n",
    "Epoch #256: Loss:1.5088, Accuracy:0.3330, Validation Loss:1.5362, Validation Accuracy:0.3058\n",
    "Epoch #257: Loss:1.5095, Accuracy:0.3293, Validation Loss:1.5353, Validation Accuracy:0.3038\n",
    "Epoch #258: Loss:1.5117, Accuracy:0.3307, Validation Loss:1.5363, Validation Accuracy:0.3079\n",
    "Epoch #259: Loss:1.5121, Accuracy:0.3343, Validation Loss:1.5333, Validation Accuracy:0.2984\n",
    "Epoch #260: Loss:1.5082, Accuracy:0.3319, Validation Loss:1.5330, Validation Accuracy:0.3030\n",
    "Epoch #261: Loss:1.5072, Accuracy:0.3371, Validation Loss:1.5366, Validation Accuracy:0.3058\n",
    "Epoch #262: Loss:1.5111, Accuracy:0.3342, Validation Loss:1.5349, Validation Accuracy:0.3116\n",
    "Epoch #263: Loss:1.5129, Accuracy:0.3302, Validation Loss:1.5302, Validation Accuracy:0.3062\n",
    "Epoch #264: Loss:1.5095, Accuracy:0.3298, Validation Loss:1.5303, Validation Accuracy:0.3001\n",
    "Epoch #265: Loss:1.5058, Accuracy:0.3356, Validation Loss:1.5342, Validation Accuracy:0.3034\n",
    "Epoch #266: Loss:1.5059, Accuracy:0.3342, Validation Loss:1.5397, Validation Accuracy:0.3099\n",
    "Epoch #267: Loss:1.5116, Accuracy:0.3315, Validation Loss:1.5342, Validation Accuracy:0.3013\n",
    "Epoch #268: Loss:1.5054, Accuracy:0.3338, Validation Loss:1.5409, Validation Accuracy:0.2976\n",
    "Epoch #269: Loss:1.5145, Accuracy:0.3277, Validation Loss:1.5396, Validation Accuracy:0.3058\n",
    "Epoch #270: Loss:1.5131, Accuracy:0.3286, Validation Loss:1.5290, Validation Accuracy:0.3058\n",
    "Epoch #271: Loss:1.5103, Accuracy:0.3308, Validation Loss:1.5322, Validation Accuracy:0.3050\n",
    "Epoch #272: Loss:1.5090, Accuracy:0.3344, Validation Loss:1.5350, Validation Accuracy:0.2989\n",
    "Epoch #273: Loss:1.5092, Accuracy:0.3325, Validation Loss:1.5306, Validation Accuracy:0.3067\n",
    "Epoch #274: Loss:1.5055, Accuracy:0.3319, Validation Loss:1.5337, Validation Accuracy:0.3046\n",
    "Epoch #275: Loss:1.5066, Accuracy:0.3336, Validation Loss:1.5365, Validation Accuracy:0.3128\n",
    "Epoch #276: Loss:1.5072, Accuracy:0.3291, Validation Loss:1.5292, Validation Accuracy:0.3103\n",
    "Epoch #277: Loss:1.5046, Accuracy:0.3313, Validation Loss:1.5273, Validation Accuracy:0.3079\n",
    "Epoch #278: Loss:1.5061, Accuracy:0.3354, Validation Loss:1.5294, Validation Accuracy:0.3071\n",
    "Epoch #279: Loss:1.5065, Accuracy:0.3383, Validation Loss:1.5289, Validation Accuracy:0.3128\n",
    "Epoch #280: Loss:1.5026, Accuracy:0.3341, Validation Loss:1.5298, Validation Accuracy:0.3042\n",
    "Epoch #281: Loss:1.5023, Accuracy:0.3372, Validation Loss:1.5305, Validation Accuracy:0.3099\n",
    "Epoch #282: Loss:1.5045, Accuracy:0.3359, Validation Loss:1.5289, Validation Accuracy:0.3091\n",
    "Epoch #283: Loss:1.5031, Accuracy:0.3344, Validation Loss:1.5344, Validation Accuracy:0.3042\n",
    "Epoch #284: Loss:1.5039, Accuracy:0.3364, Validation Loss:1.5312, Validation Accuracy:0.3030\n",
    "Epoch #285: Loss:1.4999, Accuracy:0.3375, Validation Loss:1.5277, Validation Accuracy:0.3087\n",
    "Epoch #286: Loss:1.5031, Accuracy:0.3385, Validation Loss:1.5275, Validation Accuracy:0.3075\n",
    "Epoch #287: Loss:1.5019, Accuracy:0.3341, Validation Loss:1.5346, Validation Accuracy:0.3075\n",
    "Epoch #288: Loss:1.4990, Accuracy:0.3401, Validation Loss:1.5322, Validation Accuracy:0.3112\n",
    "Epoch #289: Loss:1.5001, Accuracy:0.3407, Validation Loss:1.5294, Validation Accuracy:0.3116\n",
    "Epoch #290: Loss:1.5030, Accuracy:0.3389, Validation Loss:1.5291, Validation Accuracy:0.3030\n",
    "Epoch #291: Loss:1.5040, Accuracy:0.3377, Validation Loss:1.5255, Validation Accuracy:0.3108\n",
    "Epoch #292: Loss:1.5014, Accuracy:0.3409, Validation Loss:1.5274, Validation Accuracy:0.3153\n",
    "Epoch #293: Loss:1.4985, Accuracy:0.3395, Validation Loss:1.5295, Validation Accuracy:0.3071\n",
    "Epoch #294: Loss:1.5009, Accuracy:0.3409, Validation Loss:1.5257, Validation Accuracy:0.3128\n",
    "Epoch #295: Loss:1.5005, Accuracy:0.3398, Validation Loss:1.5271, Validation Accuracy:0.3136\n",
    "Epoch #296: Loss:1.5055, Accuracy:0.3353, Validation Loss:1.5258, Validation Accuracy:0.3083\n",
    "Epoch #297: Loss:1.5007, Accuracy:0.3390, Validation Loss:1.5250, Validation Accuracy:0.3161\n",
    "Epoch #298: Loss:1.4983, Accuracy:0.3440, Validation Loss:1.5306, Validation Accuracy:0.3112\n",
    "Epoch #299: Loss:1.5004, Accuracy:0.3412, Validation Loss:1.5234, Validation Accuracy:0.3177\n",
    "Epoch #300: Loss:1.4987, Accuracy:0.3386, Validation Loss:1.5259, Validation Accuracy:0.3116\n",
    "\n",
    "Test:\n",
    "Test Loss:1.52587926, Accuracy:0.3116\n",
    "Labels: ['04', '03', '02', '01', '05']\n",
    "Confusion Matrix:\n",
    "       04   03  02  01   05\n",
    "t:04  147   87  67  46  103\n",
    "t:03   78  137  83  49  112\n",
    "t:02   81   87  90  45  154\n",
    "t:01   52   73  95  80  203\n",
    "t:05   62   61  77  62  305\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.35      0.33      0.34       450\n",
    "          03       0.31      0.30      0.30       459\n",
    "          02       0.22      0.20      0.21       457\n",
    "          01       0.28      0.16      0.20       503\n",
    "          05       0.35      0.54      0.42       567\n",
    "\n",
    "    accuracy                           0.31      2436\n",
    "   macro avg       0.30      0.30      0.29      2436\n",
    "weighted avg       0.30      0.31      0.30      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 11:49:12 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 18 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6060127887036804, 1.6055468681019123, 1.6055544058873343, 1.6060712770092467, 1.6048969073444361, 1.604075297737748, 1.602330993353244, 1.6011451435793798, 1.594539044525823, 1.584868047820719, 1.582531705278481, 1.5776983990849336, 1.5815442782904714, 1.5773575186533686, 1.5774334768943599, 1.5781559997004242, 1.5749661076832286, 1.5756565494881867, 1.576157233202203, 1.57390950779218, 1.5750952211311102, 1.5775674080418052, 1.5738181697911229, 1.5738554901285908, 1.5752495006583203, 1.5734004719895487, 1.57374995251986, 1.573244449931804, 1.5727001373795257, 1.5727476596049292, 1.5737413244294416, 1.5724998855434225, 1.572863382267443, 1.5717790641612412, 1.5717903254263115, 1.572166436802969, 1.5713011312171548, 1.5710992114297275, 1.5726160179022302, 1.570510545191898, 1.5699849551534417, 1.5727213811013108, 1.5694100461374167, 1.5697538797882782, 1.5728097124444245, 1.5685694395810708, 1.5673151718963347, 1.566862985222602, 1.5671934476626919, 1.5676909514835902, 1.56637529904032, 1.5658699127253641, 1.5647272249356474, 1.5653850707318786, 1.5643973599122272, 1.5657834397943933, 1.5648549992854177, 1.5655286824957686, 1.5637412411825997, 1.5620837199864128, 1.5618877105524975, 1.562809509596801, 1.5627563618282574, 1.5619017443633432, 1.566618711881841, 1.5673353488026385, 1.5629008573851562, 1.562966110279603, 1.5599080298725998, 1.5623235076127577, 1.5664391742747015, 1.564061925524757, 1.5611933530453586, 1.5592344868163561, 1.5588794750924573, 1.5591880644875011, 1.558572998187812, 1.5582681332511463, 1.5584525791882293, 1.5593108071873731, 1.5594081608532684, 1.5583398616176911, 1.5590911265860246, 1.5629199847016233, 1.558616041354162, 1.5580128069190164, 1.5608427346437828, 1.5562950364866084, 1.5564182599385579, 1.556270205915855, 1.5582359048533323, 1.5572316165040867, 1.558475945970695, 1.5614935283003182, 1.557369438689722, 1.5582429286098636, 1.559554934110156, 1.5592415516795393, 1.557207884851152, 1.5578313555036272, 1.5560261165762965, 1.5549720893743981, 1.5614310328792078, 1.5583339808218193, 1.5567082567950972, 1.555396703663718, 1.5545523806745782, 1.5548988602235791, 1.5562258289365345, 1.5544511993921841, 1.554042818119569, 1.5563493391563152, 1.5574913886184567, 1.5533762890325586, 1.5540766931323975, 1.5533455835383123, 1.5530728532371458, 1.5580680840121115, 1.553029551490383, 1.5537789668551416, 1.5547401096629, 1.5551979535906186, 1.5544583445112106, 1.551950699002872, 1.5520449561634282, 1.552403421237551, 1.553827790986924, 1.5577926584848238, 1.5509776307640013, 1.5539122068236026, 1.5523493278202751, 1.5517046195141395, 1.5541390020076082, 1.55654638545658, 1.5514601232187306, 1.5561878191817962, 1.5488702622540478, 1.5492006168381138, 1.5504589808985518, 1.5502277426727495, 1.5515338158959826, 1.5507437973382634, 1.553376501612671, 1.5527760504893287, 1.5536515321246118, 1.5511577180257963, 1.5517678527017729, 1.5520497544841423, 1.5489314066365434, 1.5489021433985293, 1.5490573008463693, 1.5491793508012894, 1.551286548229274, 1.5464714301630782, 1.547544236449381, 1.5507527542818944, 1.5479441086451213, 1.5501099349242713, 1.5504685139225425, 1.5487397226011028, 1.5487409628475046, 1.5481514069442874, 1.5517350161212615, 1.5477401664104369, 1.5508681390868815, 1.5476069669613892, 1.5478731650241295, 1.548098126459983, 1.5478366110320945, 1.548768815735878, 1.5501961482960993, 1.5490698606901372, 1.5458114266591314, 1.5467702479198062, 1.5481787577442738, 1.5481620594394228, 1.5481353683033208, 1.548624243838055, 1.5489074569226093, 1.547173930897893, 1.5470156816426168, 1.5465682752809697, 1.5498397982570729, 1.5440771973191811, 1.5453088735711986, 1.5448127505423008, 1.5485335793988457, 1.5463643027056615, 1.548547849670811, 1.5444613017863633, 1.5432350238164265, 1.5438792615492747, 1.543660678691269, 1.548724602791672, 1.541954419883014, 1.5423370556682592, 1.5426737960727734, 1.548819684434211, 1.545588302690603, 1.5417640770988903, 1.5436606407165527, 1.5429474309160205, 1.5444528394927728, 1.5456227578944566, 1.5392540470132687, 1.5472149723660573, 1.5423337630254685, 1.5473209171263846, 1.5457321753838575, 1.5453939269524686, 1.539703110364466, 1.538153257471783, 1.5415196618423086, 1.5398086436667857, 1.5408601150136862, 1.5410081322361486, 1.5384133410179752, 1.5434083891619603, 1.5425385270016925, 1.5409133324677917, 1.5399068406062761, 1.5438758557653192, 1.5399218619554893, 1.538600148042826, 1.537630972212367, 1.537462879480008, 1.540200373809326, 1.5384806560961093, 1.5377603223171141, 1.5382393694472039, 1.5377597654198583, 1.5368092009195162, 1.5416404844700604, 1.5380768006658319, 1.539842046148867, 1.5401856464705443, 1.5410741469738714, 1.5436616251229849, 1.5385767477878014, 1.5352608873730613, 1.537574979080551, 1.5390491916236815, 1.535734026694337, 1.537259564023887, 1.53426336537441, 1.5377047903627794, 1.5374478293561387, 1.5393886157053054, 1.5343214420262228, 1.5404572418366356, 1.5330202272177134, 1.535950307188363, 1.5313437139655177, 1.5362813999304434, 1.5360755824494636, 1.536205866849677, 1.5353166851504096, 1.5362930658024128, 1.5333030043760152, 1.5329805414860667, 1.5365860624657868, 1.5348914342952285, 1.5302212635676067, 1.5302954168351022, 1.5341992313638697, 1.5397078274506066, 1.5341552176890507, 1.5408571821519699, 1.5396451235600488, 1.528986195038105, 1.532200202957554, 1.5350463824906373, 1.530643358606423, 1.533702883814356, 1.5365182218097506, 1.5292306347629316, 1.5272866673461714, 1.5293789477575392, 1.528901545676496, 1.52977353305065, 1.5304761377265692, 1.528944778912173, 1.5344081112903913, 1.5311581397487222, 1.527725700282894, 1.527494089356784, 1.53462705060179, 1.5322289897499022, 1.5294410632357418, 1.5290892674222172, 1.5255379097410806, 1.5274396163880923, 1.5294994761791136, 1.5256614716377948, 1.52707660570129, 1.5257579113658035, 1.52500306835707, 1.5306141605518135, 1.5234013431765177, 1.5258792795375455], 'val_acc': [0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.23275862044497272, 0.24220032830935198, 0.24220032828488372, 0.24055828998241519, 0.2639573071586283, 0.2672413770592663, 0.2627257777650172, 0.26477832292101067, 0.2664203613458204, 0.26806239517060015, 0.27093596083581545, 0.27052545195142624, 0.27052544945566526, 0.27011494052233953, 0.26847290425073533, 0.2627257780096997, 0.2705254496024747, 0.2729885061875548, 0.27545155807473193, 0.27052544955353824, 0.27011494066914904, 0.2729885061875548, 0.2746305423612861, 0.2701149406202125, 0.27257799485634115, 0.265599343577042, 0.2717569790939588, 0.2688834135266165, 0.27298850613861836, 0.2738095243967617, 0.2717569768918167, 0.2717569794365142, 0.2684729068443693, 0.2672413798486462, 0.2697044341337113, 0.2721674859719519, 0.27011494306703704, 0.268062395757838, 0.2725779949542141, 0.26477832341037555, 0.26929392505357613, 0.2643678166791919, 0.27216748822303044, 0.27175697684288025, 0.273399015463436, 0.26888341357555295, 0.2684729044954178, 0.2692939249067667, 0.2692939247599572, 0.27216748812515745, 0.26847290454435424, 0.27298850383860335, 0.26929392495570315, 0.27750410337753484, 0.2742200333300874, 0.2775041055307404, 0.2688834136244894, 0.2766830876151525, 0.28160919364058523, 0.26642036159050286, 0.27011494296916405, 0.27996715766259994, 0.27791461470874856, 0.2725779973031656, 0.28407225012779236, 0.2729885037896668, 0.27545155807473193, 0.27791461465981204, 0.2742200333790239, 0.2709359587315464, 0.27873563267327295, 0.276683085168328, 0.2795566483377823, 0.27134646786061806, 0.2803776664980527, 0.2795566508824798, 0.27955664863140123, 0.2713464675669991, 0.28243021175191907, 0.2762725762350023, 0.2816091961852827, 0.27463054255703206, 0.2787356328690189, 0.2811986847072595, 0.27709359659741467, 0.2770935941505902, 0.2762725786818268, 0.27709359669528766, 0.2709359610315614, 0.28160919603847323, 0.27832512129312276, 0.2717569790450223, 0.2775041055307404, 0.27791461475768503, 0.28078817787820287, 0.28078817787820287, 0.2828407205873718, 0.2816091934937757, 0.27586206700805765, 0.2762725785350173, 0.2824302117029826, 0.2840722478767138, 0.28612479303270727, 0.2853037747745639, 0.2816091959895367, 0.2902298834915036, 0.2770935965484782, 0.2803776687001947, 0.276683085168328, 0.2836617388944516, 0.2811986866647191, 0.2894088677291212, 0.28981937666244695, 0.2861247952837858, 0.2742200308343264, 0.2877668291085655, 0.28448275671216655, 0.2828407204894988, 0.28653530186815995, 0.29105090365817005, 0.28160919334696627, 0.2844827590611181, 0.2881773380908277, 0.28530377707457893, 0.28653530196603294, 0.28530377707457893, 0.27955665068673385, 0.2713464676159356, 0.2922824307517661, 0.2836617386497692, 0.2865353042171115, 0.27914614165553514, 0.2853037771724519, 0.2861247929348343, 0.2816091934937757, 0.28817733794401823, 0.29022988324682114, 0.28571428600790466, 0.29310344631840246, 0.28981937656457396, 0.291050901260282, 0.2861247951369763, 0.28160919574485427, 0.28858784937310494, 0.2840722498341734, 0.2918719215737579, 0.2922824283049416, 0.2910509036092335, 0.28817733794401823, 0.29556650300135556, 0.29146141264043224, 0.28858784687734396, 0.29269293983190126, 0.2873563223284454, 0.2988505728041206, 0.30131362704024917, 0.2881773380908277, 0.28817733789508176, 0.29515599162120537, 0.2914614125914957, 0.29802955728642067, 0.2857142834142707, 0.2898193739220035, 0.28817733794401823, 0.2898193764177645, 0.2951559939212204, 0.2926929372872038, 0.29885057515307206, 0.28243021131149065, 0.2918719193716159, 0.29967159096439094, 0.29844006377292187, 0.2910509036092335, 0.29926108428214376, 0.29679802999707866, 0.2889983584532401, 0.28899835835536714, 0.300903117960114, 0.29967159326440596, 0.3037766811785048, 0.29926108198212875, 0.29926108183531924, 0.29967159326440596, 0.29515599137652293, 0.29269293983190126, 0.30213464750053454, 0.30747126505292693, 0.3082922831642608, 0.2988505727062476, 0.3013136293892007, 0.3033661722941156, 0.2935139576006797, 0.29679802755025414, 0.300903118057987, 0.3013136294870737, 0.30295566321398043, 0.3004926114246763, 0.29597700948785677, 0.29433497580988655, 0.30952381025785686, 0.29597701188574477, 0.2906403919844009, 0.2980295546927867, 0.2922824282560051, 0.30829228296851485, 0.30213464515158306, 0.30500820836997383, 0.29926108428214376, 0.30459770178559964, 0.29638752106375293, 0.3000821025892236, 0.30254515423171824, 0.3082922806195633, 0.2996715909154544, 0.3033661747409401, 0.2996715933133424, 0.29392446442973635, 0.3033661721962426, 0.30788177158836466, 0.3123973709804867, 0.3050082082231644, 0.30254515662960624, 0.3095238101110474, 0.31075533725357996, 0.3000820999955896, 0.3045977015898537, 0.3058292288791957, 0.3095238076642229, 0.305418717352236, 0.29885057275518406, 0.3000820998977166, 0.3009031205048115, 0.3037766836742658, 0.29679802755025414, 0.3062397355614429, 0.3058292265302442, 0.29597701198361775, 0.30418719270546446, 0.30377668132531427, 0.30829228066849984, 0.3058292288302592, 0.3091133011287852, 0.3099343191911826, 0.30541871989693353, 0.3066502445437051, 0.3078817739373162, 0.3029556631650439, 0.3025451565806697, 0.30747126255716595, 0.3058292288791957, 0.3037766835763928, 0.30788177398625266, 0.2984400661218734, 0.30295566551399544, 0.30582922638343474, 0.3115763530159623, 0.30623973771464846, 0.3000820998977166, 0.3033661721473061, 0.30993431684223105, 0.30131362713812215, 0.2976190457105245, 0.3058292289770687, 0.30582922878132274, 0.30500821076786183, 0.2988505728041206, 0.3066502468437201, 0.3045977017366632, 0.3128078823606369, 0.31034482822238124, 0.3078817716373011, 0.3070607558749188, 0.31280788001168536, 0.30418719040544945, 0.30993431674435806, 0.3091133010309122, 0.30418719025864, 0.30295566561186843, 0.30870278965076203, 0.30747126265503893, 0.30747126495505395, 0.3111658439847636, 0.3115763530159623, 0.302955662969298, 0.3107553372046435, 0.315270934247814, 0.3070607558749188, 0.3128078799627489, 0.3136289003740978, 0.30829228057062685, 0.31609195226127484, 0.3111658438868906, 0.3177339908818306, 0.31157635526704086], 'loss': [1.6130653874340488, 1.6059443384225363, 1.6061113576869455, 1.6064094311892374, 1.6055962525843595, 1.6046989077170526, 1.6038815920847398, 1.602696032249952, 1.5990783533276474, 1.5913127877139457, 1.5854008442077794, 1.5843159182605313, 1.5832789317783145, 1.5809459621656603, 1.5789804573666144, 1.5781655018334515, 1.5787898008338725, 1.578967710686905, 1.5786020580992806, 1.5778282994362363, 1.5776765829728614, 1.5796657547079318, 1.578006593059955, 1.5760691592335945, 1.57573515521917, 1.5752012277775476, 1.5742473509766972, 1.5742027908869594, 1.5748061766125094, 1.577527413769669, 1.5740425801619857, 1.573201132261288, 1.5731494710185934, 1.5726221752362575, 1.572632319432754, 1.5721629653868, 1.5711753824163512, 1.5746418061442444, 1.5700761661882028, 1.5720259452502585, 1.5694061274890783, 1.5690976487537673, 1.5709719480185538, 1.567771800146945, 1.5691133898631258, 1.5719956182601271, 1.5662965963018993, 1.5665036661179403, 1.5665019221374386, 1.5642049073193842, 1.5642627656826982, 1.5631100489618353, 1.5630915299578125, 1.561298680305481, 1.5632393601983479, 1.5607428276563327, 1.5590820040790943, 1.5638825038130524, 1.5606130458980616, 1.5605370660582119, 1.5586944067502657, 1.5593115256797117, 1.5575909004074346, 1.5572455584880507, 1.558216376568992, 1.5677973279718012, 1.5621796624126865, 1.55778401553998, 1.5588987099806142, 1.5565911757627797, 1.5608782879380965, 1.5661187754029855, 1.5565535249161768, 1.5565222404820718, 1.5550408749609756, 1.5549838041133215, 1.555105127886825, 1.5532569560182168, 1.5535623499989755, 1.5540906602107525, 1.5534503310612828, 1.5530614992431546, 1.5535016739882483, 1.5539115471516791, 1.556791514831402, 1.5524355202484914, 1.552186144255025, 1.5532881821450266, 1.552304366139171, 1.5521127074650916, 1.5516975734268126, 1.552438436004905, 1.551244572153816, 1.5502738280462778, 1.552003996328162, 1.5565035221268264, 1.5512233134902234, 1.553233521136415, 1.5503001724670065, 1.5496390983554127, 1.552119687450495, 1.5499094756966498, 1.5494109214943292, 1.5615262929908549, 1.5527363865772068, 1.5503603924960816, 1.5492217956871956, 1.5483737532363047, 1.547838772248928, 1.5479832532224713, 1.5482556196208852, 1.547673823897109, 1.5485437475435544, 1.5481966469566926, 1.5471060408214279, 1.5506112507481349, 1.5460703544792942, 1.5470656402301983, 1.5502150756377704, 1.5482831359643956, 1.545302989105914, 1.5447780123971082, 1.5456842751473616, 1.5455995086282186, 1.5459320234322205, 1.544705762021106, 1.542946866552443, 1.543916676764126, 1.5446564465332815, 1.5460496481928738, 1.5485791666551783, 1.5433202864453044, 1.541705261559457, 1.5457960160116395, 1.5447646221830615, 1.5419773645714323, 1.5441127281658948, 1.5405474421424787, 1.5432487804542088, 1.5415596986942957, 1.5411794358944746, 1.5405093003592207, 1.5403470664543293, 1.5422586002878584, 1.5419028956297731, 1.5419989733235793, 1.5417040271191136, 1.5400376043280537, 1.5404916754003912, 1.538203967523281, 1.538860173538725, 1.5393091679598516, 1.5382254238246158, 1.5378407626670978, 1.5351762846510024, 1.536658375023327, 1.5372190512181307, 1.536728097525955, 1.539243533234332, 1.5388234809683579, 1.5334275846363827, 1.5337786610366382, 1.5345759617474535, 1.5335469648823357, 1.532331209701679, 1.54343530749883, 1.5353737120266078, 1.5338047970981323, 1.5336372313313416, 1.5325792547613688, 1.533113209031201, 1.531860599478657, 1.531550413333415, 1.5297142216312323, 1.529593414986158, 1.5298925564763972, 1.5325615520105225, 1.5351899865226823, 1.536639280975232, 1.5298562864013765, 1.5295387741476603, 1.5291947158699897, 1.5300334894681613, 1.5319358341992513, 1.528709692582947, 1.5282138276149115, 1.5272478170218653, 1.5254755562580586, 1.5258646080381326, 1.532893243560556, 1.5294081531755734, 1.5262559221510525, 1.527150376719371, 1.5283845845189183, 1.527407507142492, 1.5278859358303845, 1.524164745058612, 1.523646147539973, 1.5259866352198794, 1.5258486845654873, 1.5214456880851448, 1.5321854926722251, 1.5227823420471724, 1.524139744692025, 1.5263008106415765, 1.5244235918017628, 1.5229141739114844, 1.5207558306825235, 1.537067404664762, 1.5277322349607088, 1.523775966211511, 1.5226743096443662, 1.5211713118719614, 1.5215159910170695, 1.5220528930119666, 1.5188515872681165, 1.5231011985998133, 1.5213272499597537, 1.5207938113496533, 1.519329111336193, 1.5202389589325358, 1.5216933472200587, 1.5222471411957632, 1.524184376992729, 1.5177593286522115, 1.518463475748254, 1.5201567158317175, 1.5176489989125999, 1.5179196665663983, 1.515570954130905, 1.51493132564811, 1.5191170768326558, 1.51367024474565, 1.5152866212249536, 1.5128921100980692, 1.5161191666640295, 1.5173865080858893, 1.5120444895550456, 1.5139551801603188, 1.5143155064181382, 1.514046861503648, 1.5129222407233298, 1.516682244277343, 1.512921487477281, 1.5147527677567343, 1.5166231708115376, 1.513574323566053, 1.5116191310314673, 1.5130736692240596, 1.5105020499082562, 1.5169373994000883, 1.513689091807763, 1.512864693674953, 1.5160911473405434, 1.510359520931753, 1.5088432128424518, 1.5094519843800602, 1.5117108901178569, 1.5121059483816002, 1.5081802078341067, 1.5072291827054973, 1.5110538396502422, 1.512880609706197, 1.5095296675664442, 1.505846110751252, 1.5059436222366238, 1.5115554241184337, 1.505411122369081, 1.5144767768573957, 1.5130847906919476, 1.5103233190532583, 1.5090388321044264, 1.5091907964839584, 1.505503746910017, 1.506585014821078, 1.5071653913913077, 1.5045689972029574, 1.5061313953732562, 1.5065036451057732, 1.5026059525213693, 1.5023018242152564, 1.504525151977304, 1.5030744701929895, 1.5039290356195438, 1.4998965766640415, 1.5030537141177198, 1.5018799864535948, 1.4990471054151562, 1.5000889733097147, 1.5030389940469417, 1.504042955541513, 1.5014322541332832, 1.4985041001004604, 1.5009011104611156, 1.5005488486260603, 1.5055231180034379, 1.5006687497211433, 1.498310736366366, 1.5003549858285172, 1.498700097601027], 'acc': [0.19599589322687908, 0.23295687885316246, 0.23295687885010266, 0.23295687883786353, 0.23295687885316246, 0.23295687885010266, 0.23295687885010266, 0.22689938397133375, 0.23572895277207392, 0.24332648871860466, 0.24055441478439424, 0.2507186858438613, 0.2556468172606991, 0.2586242299855857, 0.2614989733120744, 0.2627310061479251, 0.26344969199178647, 0.2642710472279261, 0.26190965092402463, 0.261190965098522, 0.26047227926689986, 0.261806981522567, 0.26344969199178647, 0.26457905545371757, 0.26283367556468173, 0.26457905544453825, 0.26190965092402463, 0.26673511293634494, 0.27053388089125163, 0.2638603696220954, 0.2662217659198773, 0.26724845997117136, 0.2648870636611504, 0.2675564681786042, 0.266324435306036, 0.26909650925248557, 0.2701232032915405, 0.2682751539796285, 0.2709445585093214, 0.27176591376381976, 0.2694045174537988, 0.2705338809157299, 0.27002053388090347, 0.2723819301970441, 0.27145790554414784, 0.26663244351958837, 0.27310061602866625, 0.272587269006079, 0.2751540041190153, 0.2734086242177403, 0.27505133470225873, 0.2731006160225467, 0.27505133470225873, 0.2799794661221563, 0.2779260780287474, 0.2797741273223007, 0.2801848049281314, 0.2790554414784394, 0.2805954825523208, 0.28151950717461915, 0.28357289526496826, 0.28264887062431115, 0.2804928131172055, 0.28121149897330594, 0.2841889117287904, 0.2851129363449692, 0.28059548254620126, 0.2835728952894465, 0.2822381930215403, 0.28706365503080084, 0.28151950719909746, 0.2814168377823409, 0.28757700205950765, 0.28706365504304, 0.28603696097338716, 0.28347022587268994, 0.28788501027306, 0.28716632443531825, 0.2873716632565923, 0.28644763860981565, 0.28624229980690036, 0.2881930184835526, 0.28162217657913663, 0.2826488706365503, 0.28151950719909746, 0.2830595482607397, 0.28141683779458, 0.2860369609886861, 0.28080082135523615, 0.2865503080143331, 0.2852156057494867, 0.2866529774188506, 0.28634496917470037, 0.2860369609856263, 0.28716632443531825, 0.2806981519507187, 0.2863449691869395, 0.2916837782218471, 0.2872689938428955, 0.2907597536056683, 0.285523613963039, 0.2882956878972494, 0.2867556468294876, 0.28049281315392294, 0.29383983574119193, 0.291170431211499, 0.28788501026694047, 0.28726899383983573, 0.2876796714456664, 0.292402464071828, 0.28767967147014467, 0.29004106777404615, 0.2921971252566735, 0.29291581110053483, 0.2903490759814789, 0.2917864476263646, 0.29240246405346926, 0.2926078028625042, 0.29383983574119193, 0.293018480480574, 0.29363449692297766, 0.2902464065708419, 0.2921971252566735, 0.2909650924055239, 0.2893223819332446, 0.292402464071828, 0.29989733060772167, 0.3007186858316222, 0.291273100622136, 0.2964065708449489, 0.2937371663244353, 0.2978439425173726, 0.29989733059548257, 0.2971252566735113, 0.2960985626283368, 0.2990759753471038, 0.29558521561186907, 0.3026694045052147, 0.29804928131416836, 0.30092402465289625, 0.30338809036131514, 0.30462012321552456, 0.30667351131811277, 0.3048254620123203, 0.30595482546507213, 0.30616016427104725, 0.3013347022587269, 0.30462012319104625, 0.3074948665328339, 0.308110882969118, 0.3055441478470023, 0.30585215606973404, 0.3106776180698152, 0.30872689939010317, 0.31088295686661094, 0.31180698153174635, 0.3139630390266129, 0.311190965098522, 0.30985626283367557, 0.30595482544977315, 0.31303901436147746, 0.31365503080082136, 0.3109856263078459, 0.315605749486653, 0.3164271047350318, 0.3060574948665298, 0.314579055447598, 0.3137577002083986, 0.31529774126086146, 0.31765913758312164, 0.31201232031630294, 0.3148870636427917, 0.3208418891201029, 0.320636550311068, 0.3175564681724846, 0.31591375767572705, 0.3134496919795473, 0.3143737166385631, 0.3058521560574949, 0.32053388091572993, 0.3193018480492813, 0.3152977412761604, 0.3134496920040256, 0.31498973306566785, 0.31704312114989736, 0.3157084188911704, 0.3207392197002865, 0.3217659137821785, 0.32268993838611815, 0.31119096509240246, 0.3149897330717874, 0.32022587270217756, 0.31950718687055535, 0.3181724845995893, 0.3202258726899384, 0.3171457905544148, 0.32002053388702306, 0.320841889104804, 0.3199178644886252, 0.3205338809034908, 0.32587268991392004, 0.3111909650801633, 0.32412731006160167, 0.3198151950749284, 0.3210472279383172, 0.3165297741028318, 0.31930184806152045, 0.3225872689968996, 0.3130390143798362, 0.3210472279138389, 0.3216632443654219, 0.3242299794722387, 0.32833675567129555, 0.3244353182812736, 0.317967145796674, 0.3250513347053185, 0.3234086242422186, 0.324845995905463, 0.3249486653099804, 0.32607802875355285, 0.3251540041067762, 0.3213552361396304, 0.32351129363449693, 0.31991786447638604, 0.32515400409453704, 0.3219712525728547, 0.3235112936222578, 0.3265913757761401, 0.3219712525667351, 0.3280287474210257, 0.3314168377823409, 0.3305954825584404, 0.32802874743632465, 0.3265913757700205, 0.33121149896106683, 0.3284394250513347, 0.3258726899261592, 0.3327515400288286, 0.33223819300624136, 0.3301848049281314, 0.32885010265716536, 0.3321355235894847, 0.32453798768273123, 0.3287474332404088, 0.3291581108829569, 0.32320328542094456, 0.3285420944680913, 0.3291581108829569, 0.3292607802997135, 0.33295687885010267, 0.3267967145912946, 0.3262833675687073, 0.33172484599589325, 0.32854209445585214, 0.3306981519629578, 0.33295687885010267, 0.32926078028747435, 0.33069815195071867, 0.33429158110882956, 0.3319301848110477, 0.33706365504304, 0.3341889117104317, 0.3301848049281314, 0.32977412729782246, 0.3356262833797956, 0.3341889117104317, 0.33151950719909745, 0.3337782340862423, 0.3277207392197125, 0.3286447638603696, 0.3308008213613557, 0.3343942505011079, 0.332546201244272, 0.3319301848171673, 0.33357289528944656, 0.3290554414662003, 0.3313141683778234, 0.3354209445462824, 0.3382956878972494, 0.33408624229979467, 0.33716632445979655, 0.3359342915811088, 0.3343942505194666, 0.3364476386036961, 0.3374743326488706, 0.33850102670628435, 0.33408624227531636, 0.34014373717856355, 0.34065708421338997, 0.33891170431517476, 0.3376796714456664, 0.34086242301018577, 0.33952772072698056, 0.3408624229979466, 0.3398357289527721, 0.335318275141765, 0.3390143737288716, 0.3440451745502268, 0.3411704311992598, 0.33860369611080177]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
