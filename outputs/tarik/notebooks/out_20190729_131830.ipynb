{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf3.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 13:18:30 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '0', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001E2C4CD4E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001E2B85F6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0822, Accuracy:0.3708, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #2: Loss:1.0754, Accuracy:0.3770, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #3: Loss:1.0745, Accuracy:0.3901, Validation Loss:1.0756, Validation Accuracy:0.3908\n",
    "Epoch #4: Loss:1.0745, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0761, Validation Accuracy:0.3908\n",
    "Epoch #7: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0757, Validation Accuracy:0.3695\n",
    "Epoch #8: Loss:1.0739, Accuracy:0.3988, Validation Loss:1.0751, Validation Accuracy:0.3793\n",
    "Epoch #9: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0733, Accuracy:0.3947, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #14: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #15: Loss:1.0733, Accuracy:0.4021, Validation Loss:1.0745, Validation Accuracy:0.4056\n",
    "Epoch #16: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #17: Loss:1.0731, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0730, Accuracy:0.3988, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #20: Loss:1.0726, Accuracy:0.3963, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #21: Loss:1.0723, Accuracy:0.3951, Validation Loss:1.0757, Validation Accuracy:0.3924\n",
    "Epoch #22: Loss:1.0732, Accuracy:0.3951, Validation Loss:1.0752, Validation Accuracy:0.3908\n",
    "Epoch #23: Loss:1.0724, Accuracy:0.4123, Validation Loss:1.0748, Validation Accuracy:0.3695\n",
    "Epoch #24: Loss:1.0720, Accuracy:0.4115, Validation Loss:1.0760, Validation Accuracy:0.3760\n",
    "Epoch #25: Loss:1.0727, Accuracy:0.4062, Validation Loss:1.0766, Validation Accuracy:0.3957\n",
    "Epoch #26: Loss:1.0723, Accuracy:0.4078, Validation Loss:1.0759, Validation Accuracy:0.3580\n",
    "Epoch #27: Loss:1.0728, Accuracy:0.4049, Validation Loss:1.0762, Validation Accuracy:0.3826\n",
    "Epoch #28: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #29: Loss:1.0741, Accuracy:0.3959, Validation Loss:1.0762, Validation Accuracy:0.3990\n",
    "Epoch #30: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0761, Validation Accuracy:0.3711\n",
    "Epoch #31: Loss:1.0734, Accuracy:0.4029, Validation Loss:1.0755, Validation Accuracy:0.3596\n",
    "Epoch #32: Loss:1.0733, Accuracy:0.4033, Validation Loss:1.0760, Validation Accuracy:0.3383\n",
    "Epoch #33: Loss:1.0727, Accuracy:0.4062, Validation Loss:1.0753, Validation Accuracy:0.3596\n",
    "Epoch #34: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #35: Loss:1.0732, Accuracy:0.4025, Validation Loss:1.0753, Validation Accuracy:0.3777\n",
    "Epoch #36: Loss:1.0732, Accuracy:0.4037, Validation Loss:1.0756, Validation Accuracy:0.3777\n",
    "Epoch #37: Loss:1.0730, Accuracy:0.4021, Validation Loss:1.0754, Validation Accuracy:0.3760\n",
    "Epoch #38: Loss:1.0731, Accuracy:0.3984, Validation Loss:1.0758, Validation Accuracy:0.3629\n",
    "Epoch #39: Loss:1.0731, Accuracy:0.4041, Validation Loss:1.0753, Validation Accuracy:0.3695\n",
    "Epoch #40: Loss:1.0725, Accuracy:0.4111, Validation Loss:1.0759, Validation Accuracy:0.3760\n",
    "Epoch #41: Loss:1.0726, Accuracy:0.4049, Validation Loss:1.0764, Validation Accuracy:0.3711\n",
    "Epoch #42: Loss:1.0725, Accuracy:0.4078, Validation Loss:1.0760, Validation Accuracy:0.3662\n",
    "Epoch #43: Loss:1.0722, Accuracy:0.4074, Validation Loss:1.0763, Validation Accuracy:0.3678\n",
    "Epoch #44: Loss:1.0723, Accuracy:0.4045, Validation Loss:1.0763, Validation Accuracy:0.3645\n",
    "Epoch #45: Loss:1.0725, Accuracy:0.4062, Validation Loss:1.0767, Validation Accuracy:0.3711\n",
    "Epoch #46: Loss:1.0719, Accuracy:0.4074, Validation Loss:1.0764, Validation Accuracy:0.3596\n",
    "Epoch #47: Loss:1.0719, Accuracy:0.4033, Validation Loss:1.0766, Validation Accuracy:0.3645\n",
    "Epoch #48: Loss:1.0718, Accuracy:0.4115, Validation Loss:1.0759, Validation Accuracy:0.3695\n",
    "Epoch #49: Loss:1.0720, Accuracy:0.4086, Validation Loss:1.0758, Validation Accuracy:0.3678\n",
    "Epoch #50: Loss:1.0722, Accuracy:0.4049, Validation Loss:1.0766, Validation Accuracy:0.3662\n",
    "Epoch #51: Loss:1.0720, Accuracy:0.4062, Validation Loss:1.0764, Validation Accuracy:0.3645\n",
    "Epoch #52: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0755, Validation Accuracy:0.3793\n",
    "Epoch #53: Loss:1.0732, Accuracy:0.4123, Validation Loss:1.0752, Validation Accuracy:0.3695\n",
    "Epoch #54: Loss:1.0724, Accuracy:0.4099, Validation Loss:1.0754, Validation Accuracy:0.3760\n",
    "Epoch #55: Loss:1.0730, Accuracy:0.4066, Validation Loss:1.0752, Validation Accuracy:0.3629\n",
    "Epoch #56: Loss:1.0725, Accuracy:0.4074, Validation Loss:1.0759, Validation Accuracy:0.3580\n",
    "Epoch #57: Loss:1.0719, Accuracy:0.4049, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #58: Loss:1.0733, Accuracy:0.3992, Validation Loss:1.0759, Validation Accuracy:0.4007\n",
    "Epoch #59: Loss:1.0732, Accuracy:0.4033, Validation Loss:1.0757, Validation Accuracy:0.3629\n",
    "Epoch #60: Loss:1.0732, Accuracy:0.4041, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #61: Loss:1.0724, Accuracy:0.4078, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #62: Loss:1.0718, Accuracy:0.4111, Validation Loss:1.0755, Validation Accuracy:0.3744\n",
    "Epoch #63: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0757, Validation Accuracy:0.3859\n",
    "Epoch #64: Loss:1.0710, Accuracy:0.4090, Validation Loss:1.0758, Validation Accuracy:0.3859\n",
    "Epoch #65: Loss:1.0709, Accuracy:0.4107, Validation Loss:1.0754, Validation Accuracy:0.3711\n",
    "Epoch #66: Loss:1.0717, Accuracy:0.4168, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #67: Loss:1.0706, Accuracy:0.4078, Validation Loss:1.0755, Validation Accuracy:0.3744\n",
    "Epoch #68: Loss:1.0714, Accuracy:0.4008, Validation Loss:1.0760, Validation Accuracy:0.3793\n",
    "Epoch #69: Loss:1.0700, Accuracy:0.4041, Validation Loss:1.0770, Validation Accuracy:0.3695\n",
    "Epoch #70: Loss:1.0702, Accuracy:0.4016, Validation Loss:1.0766, Validation Accuracy:0.3859\n",
    "Epoch #71: Loss:1.0703, Accuracy:0.4066, Validation Loss:1.0779, Validation Accuracy:0.3678\n",
    "Epoch #72: Loss:1.0709, Accuracy:0.4045, Validation Loss:1.0781, Validation Accuracy:0.3826\n",
    "Epoch #73: Loss:1.0708, Accuracy:0.4066, Validation Loss:1.0783, Validation Accuracy:0.3842\n",
    "Epoch #74: Loss:1.0706, Accuracy:0.4000, Validation Loss:1.0770, Validation Accuracy:0.3530\n",
    "Epoch #75: Loss:1.0712, Accuracy:0.4012, Validation Loss:1.0762, Validation Accuracy:0.3711\n",
    "Epoch #76: Loss:1.0708, Accuracy:0.4086, Validation Loss:1.0770, Validation Accuracy:0.3859\n",
    "Epoch #77: Loss:1.0707, Accuracy:0.4131, Validation Loss:1.0760, Validation Accuracy:0.3727\n",
    "Epoch #78: Loss:1.0709, Accuracy:0.4115, Validation Loss:1.0759, Validation Accuracy:0.3826\n",
    "Epoch #79: Loss:1.0702, Accuracy:0.4115, Validation Loss:1.0770, Validation Accuracy:0.3678\n",
    "Epoch #80: Loss:1.0691, Accuracy:0.4012, Validation Loss:1.0758, Validation Accuracy:0.3711\n",
    "Epoch #81: Loss:1.0701, Accuracy:0.4045, Validation Loss:1.0766, Validation Accuracy:0.3859\n",
    "Epoch #82: Loss:1.0707, Accuracy:0.4066, Validation Loss:1.0761, Validation Accuracy:0.3678\n",
    "Epoch #83: Loss:1.0690, Accuracy:0.4049, Validation Loss:1.0767, Validation Accuracy:0.3760\n",
    "Epoch #84: Loss:1.0695, Accuracy:0.4090, Validation Loss:1.0785, Validation Accuracy:0.3826\n",
    "Epoch #85: Loss:1.0687, Accuracy:0.4156, Validation Loss:1.0794, Validation Accuracy:0.3777\n",
    "Epoch #86: Loss:1.0684, Accuracy:0.4111, Validation Loss:1.0800, Validation Accuracy:0.3875\n",
    "Epoch #87: Loss:1.0688, Accuracy:0.4136, Validation Loss:1.0811, Validation Accuracy:0.3580\n",
    "Epoch #88: Loss:1.0704, Accuracy:0.4037, Validation Loss:1.0815, Validation Accuracy:0.3645\n",
    "Epoch #89: Loss:1.0694, Accuracy:0.4037, Validation Loss:1.0824, Validation Accuracy:0.3399\n",
    "Epoch #90: Loss:1.0672, Accuracy:0.4107, Validation Loss:1.0810, Validation Accuracy:0.3957\n",
    "Epoch #91: Loss:1.0675, Accuracy:0.4152, Validation Loss:1.0807, Validation Accuracy:0.3678\n",
    "Epoch #92: Loss:1.0827, Accuracy:0.4103, Validation Loss:1.1089, Validation Accuracy:0.4039\n",
    "Epoch #93: Loss:1.0901, Accuracy:0.3877, Validation Loss:1.0953, Validation Accuracy:0.3727\n",
    "Epoch #94: Loss:1.0892, Accuracy:0.3717, Validation Loss:1.0772, Validation Accuracy:0.3777\n",
    "Epoch #95: Loss:1.0748, Accuracy:0.3885, Validation Loss:1.0754, Validation Accuracy:0.3957\n",
    "Epoch #96: Loss:1.0750, Accuracy:0.4016, Validation Loss:1.0788, Validation Accuracy:0.3924\n",
    "Epoch #97: Loss:1.0757, Accuracy:0.3975, Validation Loss:1.0764, Validation Accuracy:0.3941\n",
    "Epoch #98: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.4122\n",
    "Epoch #99: Loss:1.0729, Accuracy:0.3971, Validation Loss:1.0747, Validation Accuracy:0.3580\n",
    "Epoch #100: Loss:1.0735, Accuracy:0.3885, Validation Loss:1.0746, Validation Accuracy:0.3678\n",
    "Epoch #101: Loss:1.0730, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #102: Loss:1.0726, Accuracy:0.4094, Validation Loss:1.0744, Validation Accuracy:0.4089\n",
    "Epoch #103: Loss:1.0731, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3727\n",
    "Epoch #104: Loss:1.0727, Accuracy:0.4053, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #105: Loss:1.0725, Accuracy:0.4074, Validation Loss:1.0745, Validation Accuracy:0.3793\n",
    "Epoch #106: Loss:1.0730, Accuracy:0.4037, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #107: Loss:1.0728, Accuracy:0.4103, Validation Loss:1.0738, Validation Accuracy:0.3760\n",
    "Epoch #108: Loss:1.0728, Accuracy:0.4057, Validation Loss:1.0748, Validation Accuracy:0.3727\n",
    "Epoch #109: Loss:1.0725, Accuracy:0.4103, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #110: Loss:1.0723, Accuracy:0.4074, Validation Loss:1.0747, Validation Accuracy:0.3711\n",
    "Epoch #111: Loss:1.0724, Accuracy:0.4053, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #112: Loss:1.0725, Accuracy:0.4033, Validation Loss:1.0751, Validation Accuracy:0.3760\n",
    "Epoch #113: Loss:1.0721, Accuracy:0.4041, Validation Loss:1.0754, Validation Accuracy:0.3711\n",
    "Epoch #114: Loss:1.0719, Accuracy:0.4062, Validation Loss:1.0751, Validation Accuracy:0.3711\n",
    "Epoch #115: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.3727\n",
    "Epoch #116: Loss:1.0717, Accuracy:0.4041, Validation Loss:1.0759, Validation Accuracy:0.3777\n",
    "Epoch #117: Loss:1.0713, Accuracy:0.4021, Validation Loss:1.0764, Validation Accuracy:0.3744\n",
    "Epoch #118: Loss:1.0713, Accuracy:0.4041, Validation Loss:1.0767, Validation Accuracy:0.3695\n",
    "Epoch #119: Loss:1.0710, Accuracy:0.4053, Validation Loss:1.0763, Validation Accuracy:0.3760\n",
    "Epoch #120: Loss:1.0712, Accuracy:0.4021, Validation Loss:1.0768, Validation Accuracy:0.3826\n",
    "Epoch #121: Loss:1.0719, Accuracy:0.3914, Validation Loss:1.0768, Validation Accuracy:0.3645\n",
    "Epoch #122: Loss:1.0718, Accuracy:0.3955, Validation Loss:1.0766, Validation Accuracy:0.3777\n",
    "Epoch #123: Loss:1.0712, Accuracy:0.4062, Validation Loss:1.0767, Validation Accuracy:0.3727\n",
    "Epoch #124: Loss:1.0717, Accuracy:0.4062, Validation Loss:1.0768, Validation Accuracy:0.3678\n",
    "Epoch #125: Loss:1.0715, Accuracy:0.4033, Validation Loss:1.0772, Validation Accuracy:0.3662\n",
    "Epoch #126: Loss:1.0715, Accuracy:0.4000, Validation Loss:1.0767, Validation Accuracy:0.3678\n",
    "Epoch #127: Loss:1.0713, Accuracy:0.4021, Validation Loss:1.0770, Validation Accuracy:0.3777\n",
    "Epoch #128: Loss:1.0710, Accuracy:0.4045, Validation Loss:1.0765, Validation Accuracy:0.3744\n",
    "Epoch #129: Loss:1.0708, Accuracy:0.4074, Validation Loss:1.0769, Validation Accuracy:0.3760\n",
    "Epoch #130: Loss:1.0702, Accuracy:0.4000, Validation Loss:1.0772, Validation Accuracy:0.3678\n",
    "Epoch #131: Loss:1.0706, Accuracy:0.3996, Validation Loss:1.0769, Validation Accuracy:0.3662\n",
    "Epoch #132: Loss:1.0706, Accuracy:0.4111, Validation Loss:1.0770, Validation Accuracy:0.3744\n",
    "Epoch #133: Loss:1.0702, Accuracy:0.4090, Validation Loss:1.0775, Validation Accuracy:0.3826\n",
    "Epoch #134: Loss:1.0715, Accuracy:0.3963, Validation Loss:1.0869, Validation Accuracy:0.3990\n",
    "Epoch #135: Loss:1.0811, Accuracy:0.3967, Validation Loss:1.0771, Validation Accuracy:0.4007\n",
    "Epoch #136: Loss:1.0731, Accuracy:0.3864, Validation Loss:1.0778, Validation Accuracy:0.3662\n",
    "Epoch #137: Loss:1.0734, Accuracy:0.3901, Validation Loss:1.0765, Validation Accuracy:0.3645\n",
    "Epoch #138: Loss:1.0729, Accuracy:0.3975, Validation Loss:1.0761, Validation Accuracy:0.4122\n",
    "Epoch #139: Loss:1.0719, Accuracy:0.3893, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #140: Loss:1.0706, Accuracy:0.3996, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #141: Loss:1.0712, Accuracy:0.4029, Validation Loss:1.0754, Validation Accuracy:0.3645\n",
    "Epoch #142: Loss:1.0716, Accuracy:0.4025, Validation Loss:1.0764, Validation Accuracy:0.3793\n",
    "Epoch #143: Loss:1.0707, Accuracy:0.4099, Validation Loss:1.0761, Validation Accuracy:0.3629\n",
    "Epoch #144: Loss:1.0723, Accuracy:0.4078, Validation Loss:1.0770, Validation Accuracy:0.3711\n",
    "Epoch #145: Loss:1.0702, Accuracy:0.4025, Validation Loss:1.0754, Validation Accuracy:0.3793\n",
    "Epoch #146: Loss:1.0715, Accuracy:0.4099, Validation Loss:1.0764, Validation Accuracy:0.3842\n",
    "Epoch #147: Loss:1.0702, Accuracy:0.4111, Validation Loss:1.0764, Validation Accuracy:0.3580\n",
    "Epoch #148: Loss:1.0709, Accuracy:0.3963, Validation Loss:1.0771, Validation Accuracy:0.3744\n",
    "Epoch #149: Loss:1.0710, Accuracy:0.3910, Validation Loss:1.0765, Validation Accuracy:0.3826\n",
    "Epoch #150: Loss:1.0700, Accuracy:0.4103, Validation Loss:1.0764, Validation Accuracy:0.3711\n",
    "Epoch #151: Loss:1.0691, Accuracy:0.4082, Validation Loss:1.0764, Validation Accuracy:0.3760\n",
    "Epoch #152: Loss:1.0697, Accuracy:0.4045, Validation Loss:1.0765, Validation Accuracy:0.3727\n",
    "Epoch #153: Loss:1.0691, Accuracy:0.4049, Validation Loss:1.0769, Validation Accuracy:0.3711\n",
    "Epoch #154: Loss:1.0684, Accuracy:0.4029, Validation Loss:1.0760, Validation Accuracy:0.3760\n",
    "Epoch #155: Loss:1.0697, Accuracy:0.4070, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #156: Loss:1.0685, Accuracy:0.4082, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #157: Loss:1.0695, Accuracy:0.4193, Validation Loss:1.0749, Validation Accuracy:0.3760\n",
    "Epoch #158: Loss:1.0696, Accuracy:0.4156, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #159: Loss:1.0703, Accuracy:0.4103, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #160: Loss:1.0696, Accuracy:0.4136, Validation Loss:1.0762, Validation Accuracy:0.3612\n",
    "Epoch #161: Loss:1.0697, Accuracy:0.4127, Validation Loss:1.0778, Validation Accuracy:0.3826\n",
    "Epoch #162: Loss:1.0770, Accuracy:0.4012, Validation Loss:1.0802, Validation Accuracy:0.3990\n",
    "Epoch #163: Loss:1.0748, Accuracy:0.3955, Validation Loss:1.0777, Validation Accuracy:0.3842\n",
    "Epoch #164: Loss:1.0772, Accuracy:0.3762, Validation Loss:1.0789, Validation Accuracy:0.3875\n",
    "Epoch #165: Loss:1.0730, Accuracy:0.3877, Validation Loss:1.0762, Validation Accuracy:0.3957\n",
    "Epoch #166: Loss:1.0734, Accuracy:0.4008, Validation Loss:1.0768, Validation Accuracy:0.4039\n",
    "Epoch #167: Loss:1.0723, Accuracy:0.4041, Validation Loss:1.0755, Validation Accuracy:0.4039\n",
    "Epoch #168: Loss:1.0716, Accuracy:0.4094, Validation Loss:1.0770, Validation Accuracy:0.3695\n",
    "Epoch #169: Loss:1.0709, Accuracy:0.4074, Validation Loss:1.0756, Validation Accuracy:0.3842\n",
    "Epoch #170: Loss:1.0702, Accuracy:0.4148, Validation Loss:1.0759, Validation Accuracy:0.3859\n",
    "Epoch #171: Loss:1.0703, Accuracy:0.4131, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #172: Loss:1.0698, Accuracy:0.4119, Validation Loss:1.0761, Validation Accuracy:0.3629\n",
    "Epoch #173: Loss:1.0690, Accuracy:0.4115, Validation Loss:1.0774, Validation Accuracy:0.3563\n",
    "Epoch #174: Loss:1.0692, Accuracy:0.4119, Validation Loss:1.0770, Validation Accuracy:0.3580\n",
    "Epoch #175: Loss:1.0690, Accuracy:0.4115, Validation Loss:1.0769, Validation Accuracy:0.3711\n",
    "Epoch #176: Loss:1.0685, Accuracy:0.4094, Validation Loss:1.0775, Validation Accuracy:0.3695\n",
    "Epoch #177: Loss:1.0680, Accuracy:0.4086, Validation Loss:1.0782, Validation Accuracy:0.3612\n",
    "Epoch #178: Loss:1.0680, Accuracy:0.4099, Validation Loss:1.0786, Validation Accuracy:0.3826\n",
    "Epoch #179: Loss:1.0679, Accuracy:0.4172, Validation Loss:1.0798, Validation Accuracy:0.3596\n",
    "Epoch #180: Loss:1.0693, Accuracy:0.4057, Validation Loss:1.0797, Validation Accuracy:0.3744\n",
    "Epoch #181: Loss:1.0679, Accuracy:0.4074, Validation Loss:1.0791, Validation Accuracy:0.4089\n",
    "Epoch #182: Loss:1.0691, Accuracy:0.4074, Validation Loss:1.0794, Validation Accuracy:0.3629\n",
    "Epoch #183: Loss:1.0672, Accuracy:0.4053, Validation Loss:1.0771, Validation Accuracy:0.4105\n",
    "Epoch #184: Loss:1.0681, Accuracy:0.4078, Validation Loss:1.0775, Validation Accuracy:0.3892\n",
    "Epoch #185: Loss:1.0673, Accuracy:0.4148, Validation Loss:1.0784, Validation Accuracy:0.3826\n",
    "Epoch #186: Loss:1.0666, Accuracy:0.4090, Validation Loss:1.0806, Validation Accuracy:0.3744\n",
    "Epoch #187: Loss:1.0663, Accuracy:0.4152, Validation Loss:1.0795, Validation Accuracy:0.3793\n",
    "Epoch #188: Loss:1.0656, Accuracy:0.4168, Validation Loss:1.0802, Validation Accuracy:0.3744\n",
    "Epoch #189: Loss:1.0652, Accuracy:0.4160, Validation Loss:1.0807, Validation Accuracy:0.3810\n",
    "Epoch #190: Loss:1.0659, Accuracy:0.4115, Validation Loss:1.0829, Validation Accuracy:0.3760\n",
    "Epoch #191: Loss:1.0678, Accuracy:0.4070, Validation Loss:1.0835, Validation Accuracy:0.3826\n",
    "Epoch #192: Loss:1.0690, Accuracy:0.3967, Validation Loss:1.0854, Validation Accuracy:0.3810\n",
    "Epoch #193: Loss:1.0643, Accuracy:0.4181, Validation Loss:1.0832, Validation Accuracy:0.3974\n",
    "Epoch #194: Loss:1.0648, Accuracy:0.4070, Validation Loss:1.0843, Validation Accuracy:0.3612\n",
    "Epoch #195: Loss:1.0669, Accuracy:0.3906, Validation Loss:1.0798, Validation Accuracy:0.3793\n",
    "Epoch #196: Loss:1.0691, Accuracy:0.4099, Validation Loss:1.0809, Validation Accuracy:0.3957\n",
    "Epoch #197: Loss:1.0647, Accuracy:0.4152, Validation Loss:1.0853, Validation Accuracy:0.3580\n",
    "Epoch #198: Loss:1.0669, Accuracy:0.3979, Validation Loss:1.0803, Validation Accuracy:0.3941\n",
    "Epoch #199: Loss:1.0647, Accuracy:0.4140, Validation Loss:1.0803, Validation Accuracy:0.3727\n",
    "Epoch #200: Loss:1.0628, Accuracy:0.4168, Validation Loss:1.0853, Validation Accuracy:0.3514\n",
    "Epoch #201: Loss:1.0631, Accuracy:0.4131, Validation Loss:1.0818, Validation Accuracy:0.3711\n",
    "Epoch #202: Loss:1.0643, Accuracy:0.4226, Validation Loss:1.0835, Validation Accuracy:0.3695\n",
    "Epoch #203: Loss:1.0633, Accuracy:0.4123, Validation Loss:1.0850, Validation Accuracy:0.3645\n",
    "Epoch #204: Loss:1.0635, Accuracy:0.4144, Validation Loss:1.0902, Validation Accuracy:0.3711\n",
    "Epoch #205: Loss:1.0642, Accuracy:0.4160, Validation Loss:1.0908, Validation Accuracy:0.3695\n",
    "Epoch #206: Loss:1.0638, Accuracy:0.4181, Validation Loss:1.0882, Validation Accuracy:0.3793\n",
    "Epoch #207: Loss:1.0647, Accuracy:0.4127, Validation Loss:1.0832, Validation Accuracy:0.3810\n",
    "Epoch #208: Loss:1.0660, Accuracy:0.4078, Validation Loss:1.0839, Validation Accuracy:0.3810\n",
    "Epoch #209: Loss:1.0646, Accuracy:0.4156, Validation Loss:1.0839, Validation Accuracy:0.3645\n",
    "Epoch #210: Loss:1.0637, Accuracy:0.4127, Validation Loss:1.0824, Validation Accuracy:0.3842\n",
    "Epoch #211: Loss:1.0639, Accuracy:0.4185, Validation Loss:1.0834, Validation Accuracy:0.3810\n",
    "Epoch #212: Loss:1.0628, Accuracy:0.4148, Validation Loss:1.0811, Validation Accuracy:0.3842\n",
    "Epoch #213: Loss:1.0623, Accuracy:0.4226, Validation Loss:1.0801, Validation Accuracy:0.3810\n",
    "Epoch #214: Loss:1.0639, Accuracy:0.4107, Validation Loss:1.0767, Validation Accuracy:0.3974\n",
    "Epoch #215: Loss:1.0654, Accuracy:0.4115, Validation Loss:1.0761, Validation Accuracy:0.4138\n",
    "Epoch #216: Loss:1.0674, Accuracy:0.4025, Validation Loss:1.0752, Validation Accuracy:0.4122\n",
    "Epoch #217: Loss:1.0699, Accuracy:0.3918, Validation Loss:1.0744, Validation Accuracy:0.4056\n",
    "Epoch #218: Loss:1.0779, Accuracy:0.4008, Validation Loss:1.0869, Validation Accuracy:0.4007\n",
    "Epoch #219: Loss:1.0808, Accuracy:0.4053, Validation Loss:1.0809, Validation Accuracy:0.4023\n",
    "Epoch #220: Loss:1.0716, Accuracy:0.4045, Validation Loss:1.0761, Validation Accuracy:0.4056\n",
    "Epoch #221: Loss:1.0702, Accuracy:0.4099, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #222: Loss:1.0703, Accuracy:0.4090, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #223: Loss:1.0706, Accuracy:0.4053, Validation Loss:1.0737, Validation Accuracy:0.4089\n",
    "Epoch #224: Loss:1.0711, Accuracy:0.4062, Validation Loss:1.0734, Validation Accuracy:0.4089\n",
    "Epoch #225: Loss:1.0699, Accuracy:0.4123, Validation Loss:1.0748, Validation Accuracy:0.4122\n",
    "Epoch #226: Loss:1.0706, Accuracy:0.4218, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #227: Loss:1.0709, Accuracy:0.4053, Validation Loss:1.0725, Validation Accuracy:0.4007\n",
    "Epoch #228: Loss:1.0695, Accuracy:0.4099, Validation Loss:1.0720, Validation Accuracy:0.4154\n",
    "Epoch #229: Loss:1.0678, Accuracy:0.4168, Validation Loss:1.0714, Validation Accuracy:0.3941\n",
    "Epoch #230: Loss:1.0679, Accuracy:0.4168, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #231: Loss:1.0669, Accuracy:0.4193, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #232: Loss:1.0661, Accuracy:0.4181, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #233: Loss:1.0645, Accuracy:0.4209, Validation Loss:1.0737, Validation Accuracy:0.3842\n",
    "Epoch #234: Loss:1.0646, Accuracy:0.4201, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #235: Loss:1.0640, Accuracy:0.4197, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #236: Loss:1.0627, Accuracy:0.4193, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #237: Loss:1.0632, Accuracy:0.4185, Validation Loss:1.0784, Validation Accuracy:0.3826\n",
    "Epoch #238: Loss:1.0623, Accuracy:0.4218, Validation Loss:1.0787, Validation Accuracy:0.3842\n",
    "Epoch #239: Loss:1.0637, Accuracy:0.4123, Validation Loss:1.0794, Validation Accuracy:0.4007\n",
    "Epoch #240: Loss:1.0646, Accuracy:0.4156, Validation Loss:1.0813, Validation Accuracy:0.3777\n",
    "Epoch #241: Loss:1.0617, Accuracy:0.4209, Validation Loss:1.0820, Validation Accuracy:0.3777\n",
    "Epoch #242: Loss:1.0626, Accuracy:0.4119, Validation Loss:1.0789, Validation Accuracy:0.3941\n",
    "Epoch #243: Loss:1.0648, Accuracy:0.4107, Validation Loss:1.0782, Validation Accuracy:0.3826\n",
    "Epoch #244: Loss:1.0642, Accuracy:0.3959, Validation Loss:1.0796, Validation Accuracy:0.3793\n",
    "Epoch #245: Loss:1.0605, Accuracy:0.4189, Validation Loss:1.0791, Validation Accuracy:0.3990\n",
    "Epoch #246: Loss:1.0617, Accuracy:0.4156, Validation Loss:1.0794, Validation Accuracy:0.3810\n",
    "Epoch #247: Loss:1.0616, Accuracy:0.4062, Validation Loss:1.0845, Validation Accuracy:0.3760\n",
    "Epoch #248: Loss:1.0635, Accuracy:0.4107, Validation Loss:1.0817, Validation Accuracy:0.4007\n",
    "Epoch #249: Loss:1.0621, Accuracy:0.4111, Validation Loss:1.0808, Validation Accuracy:0.3875\n",
    "Epoch #250: Loss:1.0631, Accuracy:0.4103, Validation Loss:1.0793, Validation Accuracy:0.4039\n",
    "Epoch #251: Loss:1.0622, Accuracy:0.4115, Validation Loss:1.0815, Validation Accuracy:0.3924\n",
    "Epoch #252: Loss:1.0607, Accuracy:0.4201, Validation Loss:1.0765, Validation Accuracy:0.3826\n",
    "Epoch #253: Loss:1.0609, Accuracy:0.4209, Validation Loss:1.0768, Validation Accuracy:0.3892\n",
    "Epoch #254: Loss:1.0602, Accuracy:0.4214, Validation Loss:1.0755, Validation Accuracy:0.3875\n",
    "Epoch #255: Loss:1.0597, Accuracy:0.4214, Validation Loss:1.0765, Validation Accuracy:0.3859\n",
    "Epoch #256: Loss:1.0579, Accuracy:0.4226, Validation Loss:1.0797, Validation Accuracy:0.3826\n",
    "Epoch #257: Loss:1.0575, Accuracy:0.4214, Validation Loss:1.0819, Validation Accuracy:0.3810\n",
    "Epoch #258: Loss:1.0589, Accuracy:0.4193, Validation Loss:1.0819, Validation Accuracy:0.3826\n",
    "Epoch #259: Loss:1.0589, Accuracy:0.3938, Validation Loss:1.0820, Validation Accuracy:0.4023\n",
    "Epoch #260: Loss:1.0594, Accuracy:0.4172, Validation Loss:1.0872, Validation Accuracy:0.3810\n",
    "Epoch #261: Loss:1.0605, Accuracy:0.4177, Validation Loss:1.0809, Validation Accuracy:0.4072\n",
    "Epoch #262: Loss:1.0606, Accuracy:0.4201, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #263: Loss:1.0605, Accuracy:0.3943, Validation Loss:1.0773, Validation Accuracy:0.3924\n",
    "Epoch #264: Loss:1.0596, Accuracy:0.4214, Validation Loss:1.0752, Validation Accuracy:0.4105\n",
    "Epoch #265: Loss:1.0582, Accuracy:0.4136, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #266: Loss:1.0583, Accuracy:0.4205, Validation Loss:1.0758, Validation Accuracy:0.3859\n",
    "Epoch #267: Loss:1.0590, Accuracy:0.4177, Validation Loss:1.0768, Validation Accuracy:0.3859\n",
    "Epoch #268: Loss:1.0591, Accuracy:0.4123, Validation Loss:1.0796, Validation Accuracy:0.3859\n",
    "Epoch #269: Loss:1.0573, Accuracy:0.4201, Validation Loss:1.0815, Validation Accuracy:0.3842\n",
    "Epoch #270: Loss:1.0570, Accuracy:0.4242, Validation Loss:1.0831, Validation Accuracy:0.3842\n",
    "Epoch #271: Loss:1.0553, Accuracy:0.4136, Validation Loss:1.0851, Validation Accuracy:0.3974\n",
    "Epoch #272: Loss:1.0602, Accuracy:0.4148, Validation Loss:1.0874, Validation Accuracy:0.3711\n",
    "Epoch #273: Loss:1.0576, Accuracy:0.4152, Validation Loss:1.0863, Validation Accuracy:0.3941\n",
    "Epoch #274: Loss:1.0581, Accuracy:0.4000, Validation Loss:1.0876, Validation Accuracy:0.3957\n",
    "Epoch #275: Loss:1.0552, Accuracy:0.4189, Validation Loss:1.0907, Validation Accuracy:0.3678\n",
    "Epoch #276: Loss:1.0565, Accuracy:0.3951, Validation Loss:1.0881, Validation Accuracy:0.3941\n",
    "Epoch #277: Loss:1.0534, Accuracy:0.4255, Validation Loss:1.0857, Validation Accuracy:0.3678\n",
    "Epoch #278: Loss:1.0532, Accuracy:0.4156, Validation Loss:1.0824, Validation Accuracy:0.4007\n",
    "Epoch #279: Loss:1.0513, Accuracy:0.4234, Validation Loss:1.0963, Validation Accuracy:0.3810\n",
    "Epoch #280: Loss:1.0586, Accuracy:0.4177, Validation Loss:1.0905, Validation Accuracy:0.4056\n",
    "Epoch #281: Loss:1.0557, Accuracy:0.4279, Validation Loss:1.0886, Validation Accuracy:0.3744\n",
    "Epoch #282: Loss:1.0583, Accuracy:0.4099, Validation Loss:1.0860, Validation Accuracy:0.3957\n",
    "Epoch #283: Loss:1.0573, Accuracy:0.4123, Validation Loss:1.0876, Validation Accuracy:0.3793\n",
    "Epoch #284: Loss:1.0562, Accuracy:0.4177, Validation Loss:1.0916, Validation Accuracy:0.3760\n",
    "Epoch #285: Loss:1.0578, Accuracy:0.4115, Validation Loss:1.0935, Validation Accuracy:0.3760\n",
    "Epoch #286: Loss:1.0571, Accuracy:0.4172, Validation Loss:1.0950, Validation Accuracy:0.3596\n",
    "Epoch #287: Loss:1.0571, Accuracy:0.4214, Validation Loss:1.0927, Validation Accuracy:0.3941\n",
    "Epoch #288: Loss:1.0567, Accuracy:0.4131, Validation Loss:1.0940, Validation Accuracy:0.3908\n",
    "Epoch #289: Loss:1.0569, Accuracy:0.4008, Validation Loss:1.0937, Validation Accuracy:0.3810\n",
    "Epoch #290: Loss:1.0563, Accuracy:0.4160, Validation Loss:1.0917, Validation Accuracy:0.3842\n",
    "Epoch #291: Loss:1.0548, Accuracy:0.4041, Validation Loss:1.0927, Validation Accuracy:0.4056\n",
    "Epoch #292: Loss:1.0558, Accuracy:0.4168, Validation Loss:1.0958, Validation Accuracy:0.4039\n",
    "Epoch #293: Loss:1.0589, Accuracy:0.4181, Validation Loss:1.0950, Validation Accuracy:0.3859\n",
    "Epoch #294: Loss:1.0587, Accuracy:0.4222, Validation Loss:1.0911, Validation Accuracy:0.4007\n",
    "Epoch #295: Loss:1.0564, Accuracy:0.4156, Validation Loss:1.0911, Validation Accuracy:0.3711\n",
    "Epoch #296: Loss:1.0576, Accuracy:0.4214, Validation Loss:1.0853, Validation Accuracy:0.4007\n",
    "Epoch #297: Loss:1.0559, Accuracy:0.4209, Validation Loss:1.0862, Validation Accuracy:0.3711\n",
    "Epoch #298: Loss:1.0560, Accuracy:0.4119, Validation Loss:1.0868, Validation Accuracy:0.3826\n",
    "Epoch #299: Loss:1.0558, Accuracy:0.4209, Validation Loss:1.0881, Validation Accuracy:0.3810\n",
    "Epoch #300: Loss:1.0544, Accuracy:0.4197, Validation Loss:1.0916, Validation Accuracy:0.3695\n",
    "\n",
    "Test:\n",
    "Test Loss:1.09155703, Accuracy:0.3695\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  140  100   0\n",
    "t:02  142   85   0\n",
    "t:03   91   51   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.38      0.58      0.46       240\n",
    "          02       0.36      0.37      0.37       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.37       609\n",
    "   macro avg       0.25      0.32      0.27       609\n",
    "weighted avg       0.28      0.37      0.32       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 13:34:04 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0752709624411045, 1.0744649532001789, 1.0755895624802814, 1.07546055512671, 1.07589507651055, 1.0761048353364315, 1.0756555345453849, 1.0751339864652536, 1.0751211576665367, 1.074690687832574, 1.074774863293214, 1.0745604727264304, 1.0740896404670377, 1.074076303120317, 1.0744624991330802, 1.0747858397479129, 1.074619315919422, 1.0747979317588368, 1.0749747261820952, 1.0753150117416883, 1.0756613732558753, 1.075202182204461, 1.074802046730405, 1.076032470991263, 1.0766187403198142, 1.0758681696624004, 1.0761945782036617, 1.075904189463711, 1.0761788703733672, 1.076054046110958, 1.0754944156543376, 1.0760298546507636, 1.0753234354733245, 1.075199115452508, 1.0753139202622162, 1.0756469841661125, 1.0753923163233916, 1.075820042190489, 1.075327406571612, 1.0759144201263027, 1.076360085719129, 1.0760242860696978, 1.0763123031515989, 1.076347277473738, 1.0766776565260487, 1.0764366308065079, 1.0765520269647608, 1.0758555951376854, 1.075796619424679, 1.0766387670889668, 1.0764482271886615, 1.075469235285554, 1.0752228429947777, 1.0754213710919585, 1.0751917988600206, 1.075932546397931, 1.0757278922351905, 1.0759481561594997, 1.075698030797523, 1.0753474554600582, 1.0752775056413046, 1.0754833951567977, 1.0756886367531637, 1.0757650056691788, 1.0753742421201884, 1.0752935908698096, 1.0755027886877702, 1.0759826937724022, 1.0770235273051145, 1.076633616621271, 1.0778711288433356, 1.0781482342624509, 1.0782628807136774, 1.076962913002678, 1.076194941508163, 1.0770005134526144, 1.0760261360647643, 1.0758676857783878, 1.0769617414631083, 1.0758319016552127, 1.0766337379837663, 1.076089542683318, 1.0766697413424162, 1.0784957175967338, 1.0793759544886197, 1.0799911517423557, 1.0810638503683807, 1.0814712523239587, 1.0823650784876155, 1.0809893756860192, 1.0806659495302022, 1.108882149452059, 1.0953289791085254, 1.0772250665623957, 1.0754498816867573, 1.0787632428170817, 1.076427269255978, 1.0747259573396204, 1.0747378976474256, 1.0746123371844614, 1.074122292067617, 1.0744469165802002, 1.075156086379867, 1.0744125978112808, 1.0744855556581996, 1.0741185200429706, 1.0738056036834842, 1.074780051344134, 1.0745705516859032, 1.0746772007401941, 1.074798827100857, 1.075067054657709, 1.075411681666946, 1.0751222976910069, 1.0753338468094373, 1.0759173659072525, 1.0763501056113658, 1.0767028893547497, 1.076330319022506, 1.0768028926379576, 1.076795615194662, 1.0766022794548122, 1.076676948512912, 1.0768247365168555, 1.07723590951835, 1.0767489021830567, 1.0769706876407117, 1.0765069819044792, 1.0769482021065573, 1.0772128731550645, 1.0768680388508562, 1.0770303541412103, 1.0775476635383268, 1.0869259769693385, 1.0770692367271837, 1.077837576811341, 1.0765494049476285, 1.0760862897769572, 1.0747737305113443, 1.0747757245754372, 1.0754183930129253, 1.0764454194086135, 1.0761366671529309, 1.0770396399380537, 1.075420929880565, 1.0764233799795015, 1.0764275703132642, 1.0770707911458508, 1.0765290667466538, 1.0764258722170625, 1.0763554189397, 1.0765224922271002, 1.076932328087943, 1.0759979858382778, 1.075444309190772, 1.0745618018414977, 1.074888639262157, 1.07548409514435, 1.0755820734356032, 1.0762018572129248, 1.0778490616182976, 1.0801632876075156, 1.0776856068907112, 1.0789037868502889, 1.0761641819880319, 1.0768241958665143, 1.0754952843749073, 1.076996196666962, 1.075648849037872, 1.075853002482447, 1.0755178566245218, 1.0760811729775666, 1.0774069380486149, 1.0770121139454332, 1.076859390402858, 1.0774987990828766, 1.0781592702239213, 1.078550290395865, 1.0798232708071254, 1.0796942965346212, 1.0790769910968974, 1.079447602599321, 1.077065895539395, 1.0774555016425247, 1.0784227619030204, 1.080550750683877, 1.0795129449496716, 1.0801867902376774, 1.0807487588797884, 1.082852577145268, 1.0834661839630804, 1.0854201996071977, 1.0832184239952827, 1.0842606672903978, 1.0797942408982952, 1.080870031331756, 1.0852651529515709, 1.080314244738549, 1.080340989509044, 1.0853216904529015, 1.0818024997053475, 1.0834906655187873, 1.0850309607234887, 1.090221136270094, 1.090838386703203, 1.0882489931798724, 1.0832337720445029, 1.0838924677696917, 1.0838634349246723, 1.082424486994939, 1.0833989843750627, 1.0810770360119824, 1.0801140093451063, 1.0766894024581157, 1.0760553006468148, 1.0752162557517366, 1.074430623665232, 1.086893799465474, 1.0808669097709342, 1.0761170328544278, 1.0740000880606264, 1.0737296540553152, 1.0737047308967227, 1.073353598466256, 1.0748271540859453, 1.0736130630637233, 1.072536636651639, 1.0719853338153882, 1.0713644548394214, 1.0730125735741727, 1.0737513358565582, 1.07442893574782, 1.073689721488013, 1.0748258515923286, 1.075555501704537, 1.0746890926987471, 1.0783968228228966, 1.0786850195995887, 1.079353762573405, 1.081279554977793, 1.082033624398493, 1.078909305124643, 1.0782397298390054, 1.0796379238513891, 1.07906756886512, 1.0794026125436542, 1.084530341801385, 1.0817241018824586, 1.0808391445767507, 1.079293619235748, 1.081525230838356, 1.0764549430367982, 1.0767571010025851, 1.0755462090565848, 1.0765380833928024, 1.0797310948176142, 1.0819080549312148, 1.0818825339644609, 1.0819632222108262, 1.0871699498400509, 1.0808621517738881, 1.0740362084753603, 1.0773490497044154, 1.075222641376439, 1.0741030655079482, 1.0757965233134128, 1.0767968219685045, 1.0796113131668768, 1.0815450140995344, 1.0831092648905487, 1.0850967940047065, 1.0874279941048333, 1.0862567244687886, 1.0875989621495965, 1.0907086045871228, 1.0881345912153497, 1.0856720455761613, 1.0823800238873962, 1.096323657309872, 1.090515073688551, 1.0886325752010877, 1.0860466630196532, 1.0875510930623522, 1.0915688652123137, 1.0935013548689718, 1.0950033699937642, 1.0926842241255912, 1.0939743972959972, 1.093719886441536, 1.0917443297375207, 1.0926783169040148, 1.0958090045769227, 1.0949535972770603, 1.0910537352507141, 1.091066452474234, 1.0853209677588176, 1.0861703558704146, 1.0867674524952429, 1.0881391971177852, 1.0915571682167367], 'val_acc': [0.3973727407811702, 0.3908045964777372, 0.3908045964777372, 0.3940886687273267, 0.3940886687273267, 0.3908045964777372, 0.36945812665965955, 0.37931034301693606, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.38916256035294244, 0.3957307049499944, 0.40558292169876287, 0.3875205242281477, 0.39408866882519966, 0.3940886686294537, 0.39573070475424843, 0.39244663260253193, 0.39244663260253193, 0.39080459667348316, 0.3694581263660406, 0.3760262706694736, 0.3957307049499944, 0.35796387300311244, 0.3825944159516364, 0.40229884964491935, 0.3990147771995839, 0.3711001626865813, 0.3596059098130181, 0.3382594401906864, 0.3596059097151451, 0.3825944157558905, 0.37766830728363326, 0.3776683070878873, 0.37602627096309255, 0.3628899821604805, 0.36945812656178656, 0.37602627106096553, 0.3711001626865813, 0.3661740542143241, 0.3678160904369918, 0.36453201808952934, 0.37110016249083533, 0.3596059097151451, 0.36453201818740233, 0.36945812665965955, 0.3678160904369918, 0.3661740541164511, 0.36453201818740233, 0.37931034321268203, 0.36945812665965955, 0.37602627096309255, 0.3628899821604805, 0.35796387407971525, 0.3908045962819912, 0.40065681332437864, 0.3628899824540995, 0.37931034321268203, 0.3825944156580175, 0.3743842350340438, 0.38587848810335296, 0.38587848810335296, 0.3711001627844543, 0.3842364519785582, 0.3743842352297897, 0.379310343408428, 0.3694581267575325, 0.38587848810335296, 0.3678160904369918, 0.3825944156580175, 0.3842364518806852, 0.35303776550958504, 0.3711001628823273, 0.38587848820122594, 0.37274219890924903, 0.38259441585376347, 0.36781609033911883, 0.3711001627844543, 0.38587848800547997, 0.3678160904369918, 0.3760262711588385, 0.3825944156580175, 0.37766830728363326, 0.3875205240324017, 0.3579638732967314, 0.36453201828527526, 0.3399014759239892, 0.3957307049499944, 0.36781609014337285, 0.4039408855739681, 0.37274219890924903, 0.37766830738150625, 0.3957307051457404, 0.39244663250465894, 0.3940886686294537, 0.41215106610006885, 0.3579638736882233, 0.3678160904369918, 0.39244663250465894, 0.40886699385047937, 0.37274219890924903, 0.3908045963798642, 0.379310343310555, 0.3908045961841182, 0.37602627106096553, 0.37274219881137605, 0.37438423513191676, 0.3711001628823273, 0.3760262711588385, 0.3760262711588385, 0.3711001627844543, 0.3711001627844543, 0.37274219890924903, 0.37766830728363326, 0.3743842350340438, 0.36945812656178656, 0.3760262712567115, 0.3825944157558905, 0.36453201818740233, 0.37766830738150625, 0.372742199007122, 0.36781609063273774, 0.366174054507943, 0.3678160905348648, 0.37766830738150625, 0.3743842350340438, 0.37602627106096553, 0.3678160905348648, 0.3661740543121971, 0.3743842348382978, 0.3825944155601445, 0.3990147771995839, 0.40065681312863266, 0.3661740542143241, 0.36453201799165635, 0.4121510664915608, 0.39244663250465894, 0.385878487907607, 0.36453201838314825, 0.379310343506301, 0.3628899821604805, 0.3711001630780732, 0.3793103437020469, 0.3842364520764312, 0.35796387407971525, 0.3743842353276627, 0.38259441585376347, 0.3711001629802002, 0.37602627145245743, 0.372742199007122, 0.3711001628823273, 0.37602627145245743, 0.3875205243260207, 0.3743842348382978, 0.37602627106096553, 0.38587848820122594, 0.3875205244238936, 0.36124794574206687, 0.3825944161473824, 0.3990147772974569, 0.38423645168493925, 0.3875205241302747, 0.3957307049499944, 0.4039408856718411, 0.4039408855739681, 0.36945812665965955, 0.3842364518806852, 0.38587848800547997, 0.38095237963109574, 0.3628899821604805, 0.35632183766130154, 0.3579638736882233, 0.3711001627844543, 0.36945812665965955, 0.36124794613355876, 0.3825944157558905, 0.359605910008764, 0.37438423513191676, 0.40886699414409833, 0.3628899822583535, 0.41050903036676606, 0.38916256035294244, 0.3825944157558905, 0.3743842349361708, 0.379310343408428, 0.3743842349361708, 0.38095237943534976, 0.37602627106096553, 0.3825944157558905, 0.38095237953322275, 0.3973727408790432, 0.36124794574206687, 0.379310343310555, 0.3957307049499944, 0.3579638738839693, 0.3940886687273267, 0.37274219871350306, 0.3513957289932984, 0.3711001626865813, 0.36945812656178656, 0.36453201828527526, 0.3711001627844543, 0.3694581267575325, 0.379310343604174, 0.38095237963109574, 0.38095237963109574, 0.36453201838314825, 0.3842364520764312, 0.3809523797289687, 0.3842364518806852, 0.38095237943534976, 0.3973727407811702, 0.41379310261635555, 0.4121510668830527, 0.40558292199238183, 0.40065681332437864, 0.4022988493513004, 0.40558292169876287, 0.38423645217430413, 0.39408866882519966, 0.4088669942419713, 0.4088669943398443, 0.4121510666873067, 0.3973727412705351, 0.40065681361799754, 0.4154351388390233, 0.3940886687273267, 0.3842364520764312, 0.3908045964777372, 0.38587848800547997, 0.3842364518806852, 0.3842364518806852, 0.3875205241302747, 0.38423645178281224, 0.3825944156580175, 0.38423645178281224, 0.4006568134222516, 0.3776683071857603, 0.3776683071857603, 0.3940886687273267, 0.3825944155601445, 0.37931034321268203, 0.3990147771017109, 0.38095237943534976, 0.37602627106096553, 0.4006568135201246, 0.3875205241302747, 0.4039408855739681, 0.39244663260253193, 0.3825944155601445, 0.3891625600593235, 0.38752052383665575, 0.385878487809734, 0.3825944155601445, 0.3809523792396038, 0.3825944154622715, 0.4022988494491734, 0.38095237963109574, 0.40722495811717657, 0.3908045964777372, 0.39244663250465894, 0.4105090301710201, 0.3940886685315807, 0.385878487809734, 0.385878487809734, 0.385878487907607, 0.38423645178281224, 0.38423645178281224, 0.39737274097691616, 0.3711001626865813, 0.39408866882519966, 0.3957307049499944, 0.36781609014337285, 0.3940886687273267, 0.36781609014337285, 0.40065681332437864, 0.38095237953322275, 0.40558292179663585, 0.3743842353276627, 0.3957307049499944, 0.379310343506301, 0.3760262711588385, 0.3760262711588385, 0.35960590932365316, 0.39408866882519966, 0.3908045965756102, 0.38095237953322275, 0.3842364518806852, 0.40558292179663585, 0.4039408857697141, 0.385878487907607, 0.4006568134222516, 0.3711001629802002, 0.40065681332437864, 0.3711001628823273, 0.3825944155601445, 0.38095237943534976, 0.36945812685540547], 'loss': [1.0822274154706168, 1.0754348325043979, 1.0745040212568562, 1.0745346977970194, 1.0741443565004416, 1.073912011769273, 1.073867262853979, 1.073947102724894, 1.073583901148802, 1.0736537126545054, 1.073578516217962, 1.0734148685447489, 1.073319519422872, 1.0733829299037707, 1.0733133334643543, 1.0734286047839532, 1.0730572985427824, 1.073130723632092, 1.0730382847834907, 1.072600173509586, 1.0722680990211284, 1.0732142085167418, 1.072432932520794, 1.072041757395625, 1.0726710586822008, 1.0723272833736035, 1.0728119299397088, 1.072775346493574, 1.0740650819801942, 1.072988943348675, 1.0733536742306342, 1.0732750026597135, 1.0727413903026854, 1.0730785033051728, 1.0731550051691106, 1.0731985738390035, 1.0729898083136558, 1.0731166641814998, 1.0730781420545166, 1.072463609795306, 1.0726428010380489, 1.072479345274657, 1.072165196632213, 1.0723146917883621, 1.0724899225411229, 1.0718752543294698, 1.071918342196721, 1.0717906445693186, 1.071989720949647, 1.0721808000756485, 1.0720109792215868, 1.0720589343037694, 1.073233032765085, 1.0724170440031517, 1.0729970804719708, 1.0724951758277, 1.0718565970230887, 1.0733212624731985, 1.0731615727442245, 1.0732190956080474, 1.0723833041024649, 1.0718222286177368, 1.0716120107462763, 1.071011585915113, 1.0709450255674013, 1.071650289999631, 1.0705801691607528, 1.0713976889909904, 1.0699807251258553, 1.0701968879915116, 1.0703206638535925, 1.070913833416463, 1.0707529353410066, 1.0706326376975683, 1.0711576093638457, 1.0707821941963211, 1.0707095325849874, 1.070914135037996, 1.0701872806039925, 1.0690762937436113, 1.0701226110086304, 1.0706979025071162, 1.0690273575714238, 1.069519390462605, 1.0687218608307887, 1.0684125974193002, 1.0688344578478615, 1.0704252538250212, 1.069350641121365, 1.0672004828462855, 1.0674699814657411, 1.0827339869015515, 1.0901055707089464, 1.0891840951888223, 1.0747853551312394, 1.0750442651263008, 1.0756614412370404, 1.0736762130040163, 1.0729468559582376, 1.0734942720656033, 1.0730355997105154, 1.0726357893287768, 1.073140968777071, 1.072662248699572, 1.072529484946625, 1.0730199512270198, 1.0727919615269685, 1.0727608337539423, 1.0724525226459856, 1.072345077771181, 1.0724226956984346, 1.0724555318115674, 1.0721482205929453, 1.071881452086525, 1.0718738686132725, 1.0717208200901196, 1.071341019636307, 1.0712607055718895, 1.0710328704277838, 1.0711766878437456, 1.0719471939290572, 1.0718303335765549, 1.0712026728252122, 1.071659078000752, 1.0715005066850103, 1.0714650179081628, 1.0713346586090338, 1.0709654710620826, 1.0708059189990315, 1.07019715353204, 1.0706041967599544, 1.0705686804695052, 1.0701998225962113, 1.07145637237071, 1.0810963496045656, 1.0731172922455554, 1.0734409672034104, 1.0729044057505333, 1.0718641627740566, 1.070605125858064, 1.0711552814781298, 1.071597171074556, 1.0707037233963639, 1.072267870687606, 1.070195605084147, 1.0714852686535405, 1.0701617029413306, 1.0708830307641313, 1.071044541041709, 1.0699940389676261, 1.0690756815904465, 1.0697420074220065, 1.0690510341029393, 1.0684056052437063, 1.0697206598042952, 1.0685413376261812, 1.0694787387240838, 1.0696109738438035, 1.07029497589174, 1.0695678225288157, 1.0697353918694372, 1.0769657026815709, 1.0748244121089363, 1.0771677118062484, 1.0729683017828626, 1.0733986755660916, 1.0723455449149348, 1.0715509294729213, 1.0709289926278274, 1.0702189014677639, 1.0703009146194928, 1.069818085811466, 1.069023687981482, 1.0691641989674656, 1.0690472174474102, 1.0685263467275632, 1.0680041509487301, 1.0679912479506382, 1.0679293683911741, 1.069319186171467, 1.0678636913182065, 1.0691392285133534, 1.067244418152059, 1.068105216143802, 1.067341178202776, 1.06657614159633, 1.0662573429593316, 1.0656122454382801, 1.0651714797382237, 1.0659377869149742, 1.0677837301328688, 1.0690418157734176, 1.064277783997005, 1.0648076431462408, 1.0668747596916965, 1.0690678949473575, 1.0647322287060152, 1.0668562697678865, 1.0646719795477708, 1.062848863954172, 1.0631275280789918, 1.0642601376931036, 1.063318616017179, 1.0635118429176127, 1.0641748805800013, 1.0637710321120903, 1.0647244414264907, 1.0659791107295231, 1.0645913110376628, 1.0636828613477076, 1.0639497857319011, 1.0627778543339128, 1.0623084386516155, 1.063948101331566, 1.0653932180248002, 1.0674077776668007, 1.0698595694203152, 1.0779335007775246, 1.0807575623357566, 1.0716164988413974, 1.0702281609208186, 1.0702940672574837, 1.0705803436420291, 1.0710731875969888, 1.0699457970487998, 1.0705727987465672, 1.0708535116066433, 1.0695187902548475, 1.067766133177207, 1.0678834426574393, 1.066923663993146, 1.066054403806369, 1.0645282126550066, 1.064600038234703, 1.063968084088586, 1.0627213508931028, 1.063161594715941, 1.0623394601398914, 1.0637006766497477, 1.0646276658565357, 1.061708165390046, 1.0625705922653543, 1.0648285403143944, 1.0641613645475259, 1.0604569107600061, 1.0617235479413607, 1.0615826063821938, 1.0634888413016066, 1.0620674444664675, 1.0631159832345387, 1.0621593231048427, 1.060696395417748, 1.0608517665393054, 1.0601931007980565, 1.0597415081529402, 1.0579049105027374, 1.0575031194353985, 1.058918661111679, 1.0589387477545769, 1.0594030143299142, 1.060475762081342, 1.0605746112564995, 1.060466537632247, 1.0596041516356889, 1.05818870748093, 1.0582880285486305, 1.0590495423859394, 1.0590687923118074, 1.0573430739878629, 1.0569738656833187, 1.0553453920313465, 1.060218863467661, 1.0575869249367371, 1.0580514047669678, 1.055189196431906, 1.0564664839719111, 1.0534314942310967, 1.0532309603152579, 1.0512648724922165, 1.0585869843465348, 1.0557409552333292, 1.058275332587945, 1.0572616506650951, 1.0562193203266153, 1.0577596277671673, 1.0571135118022348, 1.0570524541749111, 1.056713595429485, 1.0569219504538503, 1.056335139176684, 1.0547704071969222, 1.0558224437662709, 1.0588816059688277, 1.0587277573481721, 1.056432630149246, 1.0576458913834432, 1.0558831661878425, 1.0560312471840172, 1.0558076893279684, 1.0544038161115235], 'acc': [0.37084188991258765, 0.37700205571352824, 0.39014373612599695, 0.3946611901573087, 0.3942505148891551, 0.39383983609612716, 0.39753593344708, 0.398767968100444, 0.40000000095465343, 0.39425051351837065, 0.39425051547663414, 0.39425051156010715, 0.3946611925072249, 0.3963039000053915, 0.402053387833327, 0.3967145809892267, 0.3934291602404945, 0.39876796907957573, 0.39876796751296495, 0.39630389941791244, 0.39507186636787667, 0.39507186715118203, 0.4123203284808987, 0.41149897226562737, 0.4061601644423953, 0.4078028770687644, 0.4049281317840122, 0.39999999938804265, 0.3958932261447397, 0.39835728793663167, 0.4028747428736403, 0.40328542029588377, 0.4061601638181988, 0.4012320343963419, 0.40246406447226507, 0.4036960967757129, 0.40205338963248155, 0.39835729048237417, 0.4041067767436989, 0.4110882948433839, 0.40492812939737854, 0.4078028748779571, 0.4073921956932765, 0.4045174529909843, 0.4061601624474144, 0.4073921986673892, 0.4032854232699964, 0.41149897386895556, 0.4086242279600069, 0.40492812939737854, 0.40616016600900606, 0.39794660992690917, 0.4123203304758796, 0.40985626218500076, 0.40657084303959684, 0.40739219886321554, 0.4049281303765103, 0.399178643564424, 0.40328542049171007, 0.40410677537291445, 0.4078028766771117, 0.41108829406007846, 0.40287474134374696, 0.40903490757305766, 0.41067761902446864, 0.41683778169218766, 0.40780287589380626, 0.4008213538408769, 0.4041067775270043, 0.4016427118185854, 0.4065708442145549, 0.40451745177930876, 0.4065708418279213, 0.4000000007588271, 0.4012320334172102, 0.4086242301140967, 0.4131416837537558, 0.4114989728531064, 0.41149897191069207, 0.4012320318505994, 0.4045174515834824, 0.40657084401872856, 0.4049281323714912, 0.4090349063613821, 0.41560575142043815, 0.41108829347259945, 0.4135523623142399, 0.4036961004964135, 0.40369609693482184, 0.410677619220295, 0.4151950716115611, 0.4102669413696814, 0.3876796696709901, 0.3716632425662673, 0.38850102663284947, 0.4016427114269327, 0.3975359348545819, 0.3987679651630488, 0.39712525622066286, 0.3885010266695669, 0.398767968100444, 0.40944558597443287, 0.3979466102818444, 0.4053388105770401, 0.40739219647658187, 0.4036960965798866, 0.4102669388239389, 0.40574948799928356, 0.41026693999889696, 0.40739219905904184, 0.4053388090104293, 0.403285419316752, 0.40410677811448337, 0.4061601634265461, 0.40205339041578697, 0.40410677831030967, 0.40205338708673904, 0.40410677533619704, 0.4053388109686928, 0.4020533864625426, 0.39137576839272736, 0.39548254539344835, 0.40616016616811496, 0.4061601658131797, 0.40328542287834374, 0.4000000001346306, 0.40205338904500254, 0.4045174515834824, 0.40739219549745015, 0.39999999977969536, 0.39958932153742904, 0.4110882942559048, 0.4090349073772313, 0.39630390039704416, 0.39671457860259307, 0.3864476385792178, 0.39014373910010963, 0.3975359358337136, 0.38932238386397, 0.3995893211457764, 0.40287474365694564, 0.4024640642764387, 0.4098562617933481, 0.40780287628545897, 0.4024640649006352, 0.40985626277247983, 0.41108829664253843, 0.3963039025878515, 0.39096509394459655, 0.4102669404272671, 0.4082135546868342, 0.40451745612420587, 0.4049281292015522, 0.402874743265293, 0.4069815196785349, 0.4082135524960269, 0.4193018475964329, 0.4156057500129363, 0.4102669382364598, 0.4135523593768447, 0.41273100629479487, 0.40123203322138384, 0.39548254817173467, 0.37618069832329876, 0.387679673000038, 0.4008213551749439, 0.40410677733117795, 0.4094455830003202, 0.40739219588910286, 0.41478439595175476, 0.41314168610367197, 0.41190965050789363, 0.41149897523974, 0.41190965324946255, 0.411498974652261, 0.4094455849585837, 0.40862422972244405, 0.40985626277247983, 0.41724845774364666, 0.40574948482934453, 0.4073921967091257, 0.40739219902232443, 0.40533880724799215, 0.40780287389882536, 0.41478439262270683, 0.4090349069855786, 0.41519507301906294, 0.4168377807130559, 0.41601642923433435, 0.411498974652261, 0.40698151846685937, 0.3967145772318086, 0.41806981572135515, 0.40698152183262476, 0.3905544129607614, 0.4098562610100427, 0.4151950700449503, 0.39794661126097614, 0.41396303797404627, 0.41683778329551585, 0.41314168293373293, 0.42258726936101426, 0.41232032691428794, 0.4143737148088107, 0.4160164280593762, 0.4180698173246834, 0.4127310039448787, 0.4078028727238673, 0.4156057480546728, 0.4127310045323577, 0.41848049494275325, 0.41478439262270683, 0.42258727112345135, 0.4106776158545296, 0.4114989755946753, 0.40246406803385676, 0.39178644780995175, 0.4008213555665966, 0.40533880681962203, 0.4045174519751351, 0.40985626061839, 0.4090349079279929, 0.40533880959790836, 0.40616016600900606, 0.4123203298884006, 0.42176591310902545, 0.405338809402082, 0.4098562633966763, 0.41683778051722953, 0.4168377811047086, 0.4193018468131275, 0.4180698157580726, 0.4209445574812331, 0.420123203456769, 0.41971252482285, 0.4193018475964329, 0.4184804919319231, 0.4217659144798099, 0.412320330671706, 0.41560574981710996, 0.42094455728540675, 0.4119096487087391, 0.4106776162461823, 0.3958932261447397, 0.4188911697825367, 0.41560575181209086, 0.40616016502987434, 0.41067761702948774, 0.41108829585923307, 0.4102669402314407, 0.411498972069801, 0.42012320244091983, 0.42094456022280197, 0.42135523725339274, 0.42135523568678196, 0.42258727053597234, 0.42135523768176286, 0.4193018475964329, 0.3938398366836062, 0.41724845872277844, 0.41765913575336916, 0.4201232036158779, 0.39425051547663414, 0.4213552374492191, 0.41355236313426275, 0.4205338800589896, 0.4176591385316555, 0.4123203276975933, 0.4201232010701354, 0.42422998179155696, 0.4135523599643237, 0.41478439360183855, 0.4151950712199084, 0.3999999997429779, 0.418891169390884, 0.395071869733642, 0.42546201069252204, 0.4156057482504992, 0.42340862456043643, 0.41765913872748184, 0.4279260795341625, 0.40985626163423916, 0.41232032730594065, 0.4176591389233082, 0.41149897249817113, 0.41724845970191016, 0.4213552374492191, 0.4131416819546012, 0.4008213530208541, 0.41601642704352704, 0.40410677831030967, 0.41683778270803684, 0.4180698129797863, 0.4221765928811851, 0.41560575083295914, 0.4213552358826083, 0.4209445574812331, 0.41190964988369716, 0.4209445578728858, 0.4197125265852871]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
