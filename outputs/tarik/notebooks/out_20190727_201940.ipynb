{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf48.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 20:19:40 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '0Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023501565E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023556966EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0874, Accuracy:0.3943, Validation Loss:1.0782, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0755, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0753, Accuracy:0.3832, Validation Loss:1.0763, Validation Accuracy:0.3727\n",
    "Epoch #4: Loss:1.0758, Accuracy:0.3786, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #9: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0726, Validation Accuracy:0.3990\n",
    "Epoch #10: Loss:1.0726, Accuracy:0.3975, Validation Loss:1.0721, Validation Accuracy:0.3957\n",
    "Epoch #11: Loss:1.0723, Accuracy:0.4053, Validation Loss:1.0715, Validation Accuracy:0.4023\n",
    "Epoch #12: Loss:1.0718, Accuracy:0.4049, Validation Loss:1.0709, Validation Accuracy:0.3974\n",
    "Epoch #13: Loss:1.0711, Accuracy:0.4094, Validation Loss:1.0701, Validation Accuracy:0.3924\n",
    "Epoch #14: Loss:1.0705, Accuracy:0.4103, Validation Loss:1.0693, Validation Accuracy:0.4039\n",
    "Epoch #15: Loss:1.0702, Accuracy:0.4033, Validation Loss:1.0686, Validation Accuracy:0.3892\n",
    "Epoch #16: Loss:1.0689, Accuracy:0.4070, Validation Loss:1.0674, Validation Accuracy:0.4039\n",
    "Epoch #17: Loss:1.0681, Accuracy:0.4136, Validation Loss:1.0665, Validation Accuracy:0.4056\n",
    "Epoch #18: Loss:1.0672, Accuracy:0.4148, Validation Loss:1.0654, Validation Accuracy:0.3924\n",
    "Epoch #19: Loss:1.0665, Accuracy:0.4049, Validation Loss:1.0645, Validation Accuracy:0.3908\n",
    "Epoch #20: Loss:1.0661, Accuracy:0.4053, Validation Loss:1.0641, Validation Accuracy:0.3908\n",
    "Epoch #21: Loss:1.0656, Accuracy:0.4082, Validation Loss:1.0636, Validation Accuracy:0.3924\n",
    "Epoch #22: Loss:1.0651, Accuracy:0.4078, Validation Loss:1.0629, Validation Accuracy:0.3859\n",
    "Epoch #23: Loss:1.0647, Accuracy:0.4037, Validation Loss:1.0623, Validation Accuracy:0.3875\n",
    "Epoch #24: Loss:1.0649, Accuracy:0.4053, Validation Loss:1.0622, Validation Accuracy:0.3892\n",
    "Epoch #25: Loss:1.0646, Accuracy:0.4103, Validation Loss:1.0617, Validation Accuracy:0.3875\n",
    "Epoch #26: Loss:1.0644, Accuracy:0.4140, Validation Loss:1.0615, Validation Accuracy:0.3924\n",
    "Epoch #27: Loss:1.0642, Accuracy:0.4123, Validation Loss:1.0611, Validation Accuracy:0.3859\n",
    "Epoch #28: Loss:1.0638, Accuracy:0.4049, Validation Loss:1.0608, Validation Accuracy:0.3842\n",
    "Epoch #29: Loss:1.0635, Accuracy:0.4103, Validation Loss:1.0606, Validation Accuracy:0.3842\n",
    "Epoch #30: Loss:1.0637, Accuracy:0.4099, Validation Loss:1.0603, Validation Accuracy:0.3924\n",
    "Epoch #31: Loss:1.0635, Accuracy:0.4131, Validation Loss:1.0602, Validation Accuracy:0.3859\n",
    "Epoch #32: Loss:1.0632, Accuracy:0.4131, Validation Loss:1.0599, Validation Accuracy:0.3908\n",
    "Epoch #33: Loss:1.0636, Accuracy:0.4049, Validation Loss:1.0599, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0626, Accuracy:0.4103, Validation Loss:1.0595, Validation Accuracy:0.3875\n",
    "Epoch #35: Loss:1.0631, Accuracy:0.4172, Validation Loss:1.0599, Validation Accuracy:0.3974\n",
    "Epoch #36: Loss:1.0629, Accuracy:0.4172, Validation Loss:1.0591, Validation Accuracy:0.3875\n",
    "Epoch #37: Loss:1.0625, Accuracy:0.4115, Validation Loss:1.0594, Validation Accuracy:0.3957\n",
    "Epoch #38: Loss:1.0623, Accuracy:0.4078, Validation Loss:1.0589, Validation Accuracy:0.3826\n",
    "Epoch #39: Loss:1.0620, Accuracy:0.4115, Validation Loss:1.0587, Validation Accuracy:0.3842\n",
    "Epoch #40: Loss:1.0620, Accuracy:0.4164, Validation Loss:1.0585, Validation Accuracy:0.3842\n",
    "Epoch #41: Loss:1.0614, Accuracy:0.4127, Validation Loss:1.0583, Validation Accuracy:0.4007\n",
    "Epoch #42: Loss:1.0616, Accuracy:0.4082, Validation Loss:1.0581, Validation Accuracy:0.3990\n",
    "Epoch #43: Loss:1.0614, Accuracy:0.4115, Validation Loss:1.0579, Validation Accuracy:0.3842\n",
    "Epoch #44: Loss:1.0609, Accuracy:0.4148, Validation Loss:1.0572, Validation Accuracy:0.3875\n",
    "Epoch #45: Loss:1.0604, Accuracy:0.4119, Validation Loss:1.0569, Validation Accuracy:0.3974\n",
    "Epoch #46: Loss:1.0601, Accuracy:0.4099, Validation Loss:1.0565, Validation Accuracy:0.3892\n",
    "Epoch #47: Loss:1.0596, Accuracy:0.4148, Validation Loss:1.0563, Validation Accuracy:0.3892\n",
    "Epoch #48: Loss:1.0595, Accuracy:0.4148, Validation Loss:1.0560, Validation Accuracy:0.3908\n",
    "Epoch #49: Loss:1.0588, Accuracy:0.4127, Validation Loss:1.0554, Validation Accuracy:0.3875\n",
    "Epoch #50: Loss:1.0584, Accuracy:0.4123, Validation Loss:1.0548, Validation Accuracy:0.3908\n",
    "Epoch #51: Loss:1.0577, Accuracy:0.4099, Validation Loss:1.0544, Validation Accuracy:0.3875\n",
    "Epoch #52: Loss:1.0570, Accuracy:0.4127, Validation Loss:1.0537, Validation Accuracy:0.3892\n",
    "Epoch #53: Loss:1.0563, Accuracy:0.4185, Validation Loss:1.0528, Validation Accuracy:0.3859\n",
    "Epoch #54: Loss:1.0561, Accuracy:0.4156, Validation Loss:1.0524, Validation Accuracy:0.3924\n",
    "Epoch #55: Loss:1.0549, Accuracy:0.4103, Validation Loss:1.0516, Validation Accuracy:0.3810\n",
    "Epoch #56: Loss:1.0538, Accuracy:0.4136, Validation Loss:1.0503, Validation Accuracy:0.3760\n",
    "Epoch #57: Loss:1.0522, Accuracy:0.4209, Validation Loss:1.0490, Validation Accuracy:0.3793\n",
    "Epoch #58: Loss:1.0507, Accuracy:0.4181, Validation Loss:1.0475, Validation Accuracy:0.3941\n",
    "Epoch #59: Loss:1.0495, Accuracy:0.4201, Validation Loss:1.0463, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0483, Accuracy:0.4296, Validation Loss:1.0451, Validation Accuracy:0.3810\n",
    "Epoch #61: Loss:1.0466, Accuracy:0.4341, Validation Loss:1.0439, Validation Accuracy:0.3924\n",
    "Epoch #62: Loss:1.0446, Accuracy:0.4349, Validation Loss:1.0433, Validation Accuracy:0.4401\n",
    "Epoch #63: Loss:1.0426, Accuracy:0.4402, Validation Loss:1.0411, Validation Accuracy:0.4039\n",
    "Epoch #64: Loss:1.0405, Accuracy:0.4357, Validation Loss:1.0404, Validation Accuracy:0.4269\n",
    "Epoch #65: Loss:1.0389, Accuracy:0.4398, Validation Loss:1.0404, Validation Accuracy:0.4384\n",
    "Epoch #66: Loss:1.0348, Accuracy:0.4468, Validation Loss:1.0399, Validation Accuracy:0.4335\n",
    "Epoch #67: Loss:1.0345, Accuracy:0.4472, Validation Loss:1.0404, Validation Accuracy:0.4516\n",
    "Epoch #68: Loss:1.0338, Accuracy:0.4526, Validation Loss:1.0404, Validation Accuracy:0.4368\n",
    "Epoch #69: Loss:1.0297, Accuracy:0.4476, Validation Loss:1.0415, Validation Accuracy:0.4417\n",
    "Epoch #70: Loss:1.0310, Accuracy:0.4563, Validation Loss:1.0383, Validation Accuracy:0.4417\n",
    "Epoch #71: Loss:1.0260, Accuracy:0.4616, Validation Loss:1.0381, Validation Accuracy:0.4532\n",
    "Epoch #72: Loss:1.0265, Accuracy:0.4604, Validation Loss:1.0386, Validation Accuracy:0.4433\n",
    "Epoch #73: Loss:1.0248, Accuracy:0.4723, Validation Loss:1.0357, Validation Accuracy:0.4516\n",
    "Epoch #74: Loss:1.0242, Accuracy:0.4665, Validation Loss:1.0362, Validation Accuracy:0.4516\n",
    "Epoch #75: Loss:1.0292, Accuracy:0.4567, Validation Loss:1.0429, Validation Accuracy:0.4433\n",
    "Epoch #76: Loss:1.0302, Accuracy:0.4559, Validation Loss:1.0361, Validation Accuracy:0.4450\n",
    "Epoch #77: Loss:1.0251, Accuracy:0.4710, Validation Loss:1.0369, Validation Accuracy:0.4532\n",
    "Epoch #78: Loss:1.0221, Accuracy:0.4739, Validation Loss:1.0359, Validation Accuracy:0.4499\n",
    "Epoch #79: Loss:1.0187, Accuracy:0.4780, Validation Loss:1.0373, Validation Accuracy:0.4417\n",
    "Epoch #80: Loss:1.0184, Accuracy:0.4727, Validation Loss:1.0363, Validation Accuracy:0.4483\n",
    "Epoch #81: Loss:1.0162, Accuracy:0.4739, Validation Loss:1.0343, Validation Accuracy:0.4384\n",
    "Epoch #82: Loss:1.0168, Accuracy:0.4850, Validation Loss:1.0409, Validation Accuracy:0.4581\n",
    "Epoch #83: Loss:1.0173, Accuracy:0.4747, Validation Loss:1.0352, Validation Accuracy:0.4450\n",
    "Epoch #84: Loss:1.0166, Accuracy:0.4842, Validation Loss:1.0364, Validation Accuracy:0.4417\n",
    "Epoch #85: Loss:1.0144, Accuracy:0.4793, Validation Loss:1.0340, Validation Accuracy:0.4466\n",
    "Epoch #86: Loss:1.0159, Accuracy:0.4854, Validation Loss:1.0371, Validation Accuracy:0.4499\n",
    "Epoch #87: Loss:1.0146, Accuracy:0.4723, Validation Loss:1.0379, Validation Accuracy:0.4466\n",
    "Epoch #88: Loss:1.0131, Accuracy:0.4875, Validation Loss:1.0339, Validation Accuracy:0.4401\n",
    "Epoch #89: Loss:1.0120, Accuracy:0.4830, Validation Loss:1.0359, Validation Accuracy:0.4417\n",
    "Epoch #90: Loss:1.0102, Accuracy:0.4875, Validation Loss:1.0346, Validation Accuracy:0.4483\n",
    "Epoch #91: Loss:1.0101, Accuracy:0.4850, Validation Loss:1.0361, Validation Accuracy:0.4433\n",
    "Epoch #92: Loss:1.0106, Accuracy:0.4895, Validation Loss:1.0354, Validation Accuracy:0.4450\n",
    "Epoch #93: Loss:1.0085, Accuracy:0.4842, Validation Loss:1.0346, Validation Accuracy:0.4450\n",
    "Epoch #94: Loss:1.0087, Accuracy:0.4924, Validation Loss:1.0337, Validation Accuracy:0.4433\n",
    "Epoch #95: Loss:1.0119, Accuracy:0.4842, Validation Loss:1.0395, Validation Accuracy:0.4433\n",
    "Epoch #96: Loss:1.0064, Accuracy:0.4928, Validation Loss:1.0342, Validation Accuracy:0.4433\n",
    "Epoch #97: Loss:1.0072, Accuracy:0.4953, Validation Loss:1.0333, Validation Accuracy:0.4433\n",
    "Epoch #98: Loss:1.0078, Accuracy:0.4883, Validation Loss:1.0355, Validation Accuracy:0.4384\n",
    "Epoch #99: Loss:1.0079, Accuracy:0.4887, Validation Loss:1.0376, Validation Accuracy:0.4417\n",
    "Epoch #100: Loss:1.0069, Accuracy:0.4916, Validation Loss:1.0340, Validation Accuracy:0.4401\n",
    "Epoch #101: Loss:1.0037, Accuracy:0.4961, Validation Loss:1.0373, Validation Accuracy:0.4499\n",
    "Epoch #102: Loss:1.0039, Accuracy:0.4928, Validation Loss:1.0337, Validation Accuracy:0.4483\n",
    "Epoch #103: Loss:1.0034, Accuracy:0.4973, Validation Loss:1.0365, Validation Accuracy:0.4384\n",
    "Epoch #104: Loss:1.0023, Accuracy:0.4969, Validation Loss:1.0359, Validation Accuracy:0.4433\n",
    "Epoch #105: Loss:1.0020, Accuracy:0.4957, Validation Loss:1.0371, Validation Accuracy:0.4417\n",
    "Epoch #106: Loss:1.0003, Accuracy:0.5014, Validation Loss:1.0339, Validation Accuracy:0.4450\n",
    "Epoch #107: Loss:1.0003, Accuracy:0.5035, Validation Loss:1.0346, Validation Accuracy:0.4417\n",
    "Epoch #108: Loss:0.9994, Accuracy:0.4986, Validation Loss:1.0354, Validation Accuracy:0.4368\n",
    "Epoch #109: Loss:0.9978, Accuracy:0.5014, Validation Loss:1.0400, Validation Accuracy:0.4450\n",
    "Epoch #110: Loss:0.9990, Accuracy:0.4994, Validation Loss:1.0338, Validation Accuracy:0.4417\n",
    "Epoch #111: Loss:0.9979, Accuracy:0.4899, Validation Loss:1.0366, Validation Accuracy:0.4466\n",
    "Epoch #112: Loss:0.9965, Accuracy:0.5010, Validation Loss:1.0375, Validation Accuracy:0.4417\n",
    "Epoch #113: Loss:0.9999, Accuracy:0.4953, Validation Loss:1.0393, Validation Accuracy:0.4384\n",
    "Epoch #114: Loss:0.9971, Accuracy:0.5018, Validation Loss:1.0382, Validation Accuracy:0.4466\n",
    "Epoch #115: Loss:0.9959, Accuracy:0.5043, Validation Loss:1.0358, Validation Accuracy:0.4401\n",
    "Epoch #116: Loss:0.9951, Accuracy:0.5055, Validation Loss:1.0363, Validation Accuracy:0.4417\n",
    "Epoch #117: Loss:0.9955, Accuracy:0.4986, Validation Loss:1.0386, Validation Accuracy:0.4466\n",
    "Epoch #118: Loss:0.9966, Accuracy:0.4982, Validation Loss:1.0357, Validation Accuracy:0.4384\n",
    "Epoch #119: Loss:0.9943, Accuracy:0.5031, Validation Loss:1.0380, Validation Accuracy:0.4499\n",
    "Epoch #120: Loss:0.9944, Accuracy:0.4977, Validation Loss:1.0369, Validation Accuracy:0.4466\n",
    "Epoch #121: Loss:0.9980, Accuracy:0.5018, Validation Loss:1.0351, Validation Accuracy:0.4401\n",
    "Epoch #122: Loss:0.9961, Accuracy:0.4957, Validation Loss:1.0406, Validation Accuracy:0.4433\n",
    "Epoch #123: Loss:0.9977, Accuracy:0.5010, Validation Loss:1.0355, Validation Accuracy:0.4433\n",
    "Epoch #124: Loss:1.0008, Accuracy:0.4953, Validation Loss:1.0383, Validation Accuracy:0.4483\n",
    "Epoch #125: Loss:1.0009, Accuracy:0.4973, Validation Loss:1.0324, Validation Accuracy:0.4466\n",
    "Epoch #126: Loss:0.9960, Accuracy:0.4990, Validation Loss:1.0316, Validation Accuracy:0.4499\n",
    "Epoch #127: Loss:0.9906, Accuracy:0.5031, Validation Loss:1.0369, Validation Accuracy:0.4483\n",
    "Epoch #128: Loss:0.9915, Accuracy:0.5113, Validation Loss:1.0343, Validation Accuracy:0.4384\n",
    "Epoch #129: Loss:0.9920, Accuracy:0.5002, Validation Loss:1.0368, Validation Accuracy:0.4565\n",
    "Epoch #130: Loss:0.9909, Accuracy:0.5072, Validation Loss:1.0349, Validation Accuracy:0.4450\n",
    "Epoch #131: Loss:0.9967, Accuracy:0.5043, Validation Loss:1.0424, Validation Accuracy:0.4647\n",
    "Epoch #132: Loss:0.9985, Accuracy:0.4895, Validation Loss:1.0384, Validation Accuracy:0.4483\n",
    "Epoch #133: Loss:0.9941, Accuracy:0.5055, Validation Loss:1.0353, Validation Accuracy:0.4565\n",
    "Epoch #134: Loss:0.9890, Accuracy:0.5084, Validation Loss:1.0391, Validation Accuracy:0.4401\n",
    "Epoch #135: Loss:0.9899, Accuracy:0.5101, Validation Loss:1.0346, Validation Accuracy:0.4499\n",
    "Epoch #136: Loss:0.9889, Accuracy:0.5014, Validation Loss:1.0316, Validation Accuracy:0.4581\n",
    "Epoch #137: Loss:0.9865, Accuracy:0.5121, Validation Loss:1.0323, Validation Accuracy:0.4614\n",
    "Epoch #138: Loss:0.9863, Accuracy:0.5084, Validation Loss:1.0354, Validation Accuracy:0.4335\n",
    "Epoch #139: Loss:0.9903, Accuracy:0.5068, Validation Loss:1.0373, Validation Accuracy:0.4532\n",
    "Epoch #140: Loss:0.9911, Accuracy:0.4977, Validation Loss:1.0368, Validation Accuracy:0.4532\n",
    "Epoch #141: Loss:0.9914, Accuracy:0.5092, Validation Loss:1.0363, Validation Accuracy:0.4483\n",
    "Epoch #142: Loss:0.9891, Accuracy:0.5076, Validation Loss:1.0352, Validation Accuracy:0.4532\n",
    "Epoch #143: Loss:0.9872, Accuracy:0.5142, Validation Loss:1.0369, Validation Accuracy:0.4532\n",
    "Epoch #144: Loss:0.9844, Accuracy:0.5175, Validation Loss:1.0331, Validation Accuracy:0.4565\n",
    "Epoch #145: Loss:0.9831, Accuracy:0.5113, Validation Loss:1.0338, Validation Accuracy:0.4598\n",
    "Epoch #146: Loss:0.9823, Accuracy:0.5150, Validation Loss:1.0365, Validation Accuracy:0.4466\n",
    "Epoch #147: Loss:0.9815, Accuracy:0.5142, Validation Loss:1.0345, Validation Accuracy:0.4598\n",
    "Epoch #148: Loss:0.9813, Accuracy:0.5129, Validation Loss:1.0344, Validation Accuracy:0.4598\n",
    "Epoch #149: Loss:0.9829, Accuracy:0.5138, Validation Loss:1.0377, Validation Accuracy:0.4433\n",
    "Epoch #150: Loss:0.9818, Accuracy:0.5175, Validation Loss:1.0396, Validation Accuracy:0.4532\n",
    "Epoch #151: Loss:0.9803, Accuracy:0.5121, Validation Loss:1.0339, Validation Accuracy:0.4614\n",
    "Epoch #152: Loss:0.9789, Accuracy:0.5162, Validation Loss:1.0378, Validation Accuracy:0.4565\n",
    "Epoch #153: Loss:0.9791, Accuracy:0.5175, Validation Loss:1.0439, Validation Accuracy:0.4565\n",
    "Epoch #154: Loss:0.9866, Accuracy:0.5101, Validation Loss:1.0407, Validation Accuracy:0.4499\n",
    "Epoch #155: Loss:0.9776, Accuracy:0.5175, Validation Loss:1.0363, Validation Accuracy:0.4565\n",
    "Epoch #156: Loss:0.9791, Accuracy:0.5211, Validation Loss:1.0354, Validation Accuracy:0.4631\n",
    "Epoch #157: Loss:0.9776, Accuracy:0.5183, Validation Loss:1.0354, Validation Accuracy:0.4663\n",
    "Epoch #158: Loss:0.9774, Accuracy:0.5199, Validation Loss:1.0419, Validation Accuracy:0.4433\n",
    "Epoch #159: Loss:0.9772, Accuracy:0.5183, Validation Loss:1.0334, Validation Accuracy:0.4696\n",
    "Epoch #160: Loss:0.9752, Accuracy:0.5216, Validation Loss:1.0403, Validation Accuracy:0.4532\n",
    "Epoch #161: Loss:0.9748, Accuracy:0.5220, Validation Loss:1.0367, Validation Accuracy:0.4581\n",
    "Epoch #162: Loss:0.9739, Accuracy:0.5170, Validation Loss:1.0419, Validation Accuracy:0.4598\n",
    "Epoch #163: Loss:0.9738, Accuracy:0.5285, Validation Loss:1.0381, Validation Accuracy:0.4598\n",
    "Epoch #164: Loss:0.9745, Accuracy:0.5244, Validation Loss:1.0408, Validation Accuracy:0.4680\n",
    "Epoch #165: Loss:0.9769, Accuracy:0.5064, Validation Loss:1.0368, Validation Accuracy:0.4663\n",
    "Epoch #166: Loss:0.9763, Accuracy:0.5170, Validation Loss:1.0431, Validation Accuracy:0.4483\n",
    "Epoch #167: Loss:0.9748, Accuracy:0.5170, Validation Loss:1.0417, Validation Accuracy:0.4466\n",
    "Epoch #168: Loss:0.9728, Accuracy:0.5150, Validation Loss:1.0343, Validation Accuracy:0.4631\n",
    "Epoch #169: Loss:0.9736, Accuracy:0.5232, Validation Loss:1.0386, Validation Accuracy:0.4713\n",
    "Epoch #170: Loss:0.9772, Accuracy:0.5179, Validation Loss:1.0445, Validation Accuracy:0.4450\n",
    "Epoch #171: Loss:0.9787, Accuracy:0.5175, Validation Loss:1.0437, Validation Accuracy:0.4384\n",
    "Epoch #172: Loss:0.9755, Accuracy:0.5244, Validation Loss:1.0342, Validation Accuracy:0.4663\n",
    "Epoch #173: Loss:0.9683, Accuracy:0.5269, Validation Loss:1.0358, Validation Accuracy:0.4565\n",
    "Epoch #174: Loss:0.9701, Accuracy:0.5273, Validation Loss:1.0389, Validation Accuracy:0.4532\n",
    "Epoch #175: Loss:0.9708, Accuracy:0.5269, Validation Loss:1.0350, Validation Accuracy:0.4647\n",
    "Epoch #176: Loss:0.9672, Accuracy:0.5322, Validation Loss:1.0500, Validation Accuracy:0.4581\n",
    "Epoch #177: Loss:0.9731, Accuracy:0.5113, Validation Loss:1.0435, Validation Accuracy:0.4384\n",
    "Epoch #178: Loss:0.9699, Accuracy:0.5294, Validation Loss:1.0358, Validation Accuracy:0.4696\n",
    "Epoch #179: Loss:0.9646, Accuracy:0.5298, Validation Loss:1.0380, Validation Accuracy:0.4614\n",
    "Epoch #180: Loss:0.9642, Accuracy:0.5306, Validation Loss:1.0335, Validation Accuracy:0.4729\n",
    "Epoch #181: Loss:0.9648, Accuracy:0.5281, Validation Loss:1.0364, Validation Accuracy:0.4696\n",
    "Epoch #182: Loss:0.9655, Accuracy:0.5318, Validation Loss:1.0402, Validation Accuracy:0.4696\n",
    "Epoch #183: Loss:0.9672, Accuracy:0.5285, Validation Loss:1.0353, Validation Accuracy:0.4713\n",
    "Epoch #184: Loss:0.9675, Accuracy:0.5269, Validation Loss:1.0420, Validation Accuracy:0.4647\n",
    "Epoch #185: Loss:0.9664, Accuracy:0.5302, Validation Loss:1.0340, Validation Accuracy:0.4778\n",
    "Epoch #186: Loss:0.9636, Accuracy:0.5331, Validation Loss:1.0375, Validation Accuracy:0.4713\n",
    "Epoch #187: Loss:0.9688, Accuracy:0.5261, Validation Loss:1.0424, Validation Accuracy:0.4614\n",
    "Epoch #188: Loss:0.9614, Accuracy:0.5306, Validation Loss:1.0314, Validation Accuracy:0.4663\n",
    "Epoch #189: Loss:0.9582, Accuracy:0.5372, Validation Loss:1.0608, Validation Accuracy:0.4548\n",
    "Epoch #190: Loss:0.9667, Accuracy:0.5232, Validation Loss:1.0379, Validation Accuracy:0.4680\n",
    "Epoch #191: Loss:0.9652, Accuracy:0.5285, Validation Loss:1.0433, Validation Accuracy:0.4745\n",
    "Epoch #192: Loss:0.9640, Accuracy:0.5326, Validation Loss:1.0338, Validation Accuracy:0.4778\n",
    "Epoch #193: Loss:0.9582, Accuracy:0.5265, Validation Loss:1.0312, Validation Accuracy:0.4745\n",
    "Epoch #194: Loss:0.9532, Accuracy:0.5446, Validation Loss:1.0367, Validation Accuracy:0.4811\n",
    "Epoch #195: Loss:0.9531, Accuracy:0.5384, Validation Loss:1.0369, Validation Accuracy:0.4696\n",
    "Epoch #196: Loss:0.9532, Accuracy:0.5396, Validation Loss:1.0389, Validation Accuracy:0.4631\n",
    "Epoch #197: Loss:0.9530, Accuracy:0.5413, Validation Loss:1.0428, Validation Accuracy:0.4614\n",
    "Epoch #198: Loss:0.9519, Accuracy:0.5339, Validation Loss:1.0413, Validation Accuracy:0.4614\n",
    "Epoch #199: Loss:0.9486, Accuracy:0.5380, Validation Loss:1.0378, Validation Accuracy:0.4696\n",
    "Epoch #200: Loss:0.9510, Accuracy:0.5392, Validation Loss:1.0504, Validation Accuracy:0.4729\n",
    "Epoch #201: Loss:0.9594, Accuracy:0.5199, Validation Loss:1.0378, Validation Accuracy:0.4778\n",
    "Epoch #202: Loss:0.9488, Accuracy:0.5429, Validation Loss:1.0358, Validation Accuracy:0.4729\n",
    "Epoch #203: Loss:0.9469, Accuracy:0.5405, Validation Loss:1.0376, Validation Accuracy:0.4795\n",
    "Epoch #204: Loss:0.9495, Accuracy:0.5392, Validation Loss:1.0370, Validation Accuracy:0.4696\n",
    "Epoch #205: Loss:0.9444, Accuracy:0.5515, Validation Loss:1.0424, Validation Accuracy:0.4696\n",
    "Epoch #206: Loss:0.9436, Accuracy:0.5388, Validation Loss:1.0397, Validation Accuracy:0.4631\n",
    "Epoch #207: Loss:0.9424, Accuracy:0.5466, Validation Loss:1.0391, Validation Accuracy:0.4729\n",
    "Epoch #208: Loss:0.9437, Accuracy:0.5396, Validation Loss:1.0426, Validation Accuracy:0.4729\n",
    "Epoch #209: Loss:0.9398, Accuracy:0.5524, Validation Loss:1.0388, Validation Accuracy:0.4631\n",
    "Epoch #210: Loss:0.9467, Accuracy:0.5409, Validation Loss:1.0434, Validation Accuracy:0.4745\n",
    "Epoch #211: Loss:0.9445, Accuracy:0.5446, Validation Loss:1.0381, Validation Accuracy:0.4663\n",
    "Epoch #212: Loss:0.9380, Accuracy:0.5474, Validation Loss:1.0417, Validation Accuracy:0.4729\n",
    "Epoch #213: Loss:0.9401, Accuracy:0.5441, Validation Loss:1.0405, Validation Accuracy:0.4647\n",
    "Epoch #214: Loss:0.9476, Accuracy:0.5363, Validation Loss:1.0386, Validation Accuracy:0.4745\n",
    "Epoch #215: Loss:0.9383, Accuracy:0.5474, Validation Loss:1.0364, Validation Accuracy:0.4778\n",
    "Epoch #216: Loss:0.9388, Accuracy:0.5507, Validation Loss:1.0394, Validation Accuracy:0.4696\n",
    "Epoch #217: Loss:0.9379, Accuracy:0.5429, Validation Loss:1.0449, Validation Accuracy:0.4729\n",
    "Epoch #218: Loss:0.9349, Accuracy:0.5483, Validation Loss:1.0378, Validation Accuracy:0.4713\n",
    "Epoch #219: Loss:0.9311, Accuracy:0.5499, Validation Loss:1.0422, Validation Accuracy:0.4631\n",
    "Epoch #220: Loss:0.9362, Accuracy:0.5499, Validation Loss:1.0412, Validation Accuracy:0.4762\n",
    "Epoch #221: Loss:0.9408, Accuracy:0.5355, Validation Loss:1.0550, Validation Accuracy:0.4696\n",
    "Epoch #222: Loss:0.9377, Accuracy:0.5413, Validation Loss:1.0435, Validation Accuracy:0.4647\n",
    "Epoch #223: Loss:0.9354, Accuracy:0.5540, Validation Loss:1.0413, Validation Accuracy:0.4762\n",
    "Epoch #224: Loss:0.9313, Accuracy:0.5458, Validation Loss:1.0381, Validation Accuracy:0.4745\n",
    "Epoch #225: Loss:0.9333, Accuracy:0.5446, Validation Loss:1.0420, Validation Accuracy:0.4647\n",
    "Epoch #226: Loss:0.9302, Accuracy:0.5478, Validation Loss:1.0431, Validation Accuracy:0.4713\n",
    "Epoch #227: Loss:0.9341, Accuracy:0.5429, Validation Loss:1.0492, Validation Accuracy:0.4598\n",
    "Epoch #228: Loss:0.9407, Accuracy:0.5400, Validation Loss:1.0487, Validation Accuracy:0.4598\n",
    "Epoch #229: Loss:0.9303, Accuracy:0.5446, Validation Loss:1.0466, Validation Accuracy:0.4778\n",
    "Epoch #230: Loss:0.9277, Accuracy:0.5565, Validation Loss:1.0375, Validation Accuracy:0.4647\n",
    "Epoch #231: Loss:0.9266, Accuracy:0.5622, Validation Loss:1.0415, Validation Accuracy:0.4696\n",
    "Epoch #232: Loss:0.9246, Accuracy:0.5585, Validation Loss:1.0393, Validation Accuracy:0.4696\n",
    "Epoch #233: Loss:0.9291, Accuracy:0.5458, Validation Loss:1.0433, Validation Accuracy:0.4778\n",
    "Epoch #234: Loss:0.9253, Accuracy:0.5552, Validation Loss:1.0413, Validation Accuracy:0.4713\n",
    "Epoch #235: Loss:0.9256, Accuracy:0.5585, Validation Loss:1.0451, Validation Accuracy:0.4828\n",
    "Epoch #236: Loss:0.9226, Accuracy:0.5569, Validation Loss:1.0465, Validation Accuracy:0.4729\n",
    "Epoch #237: Loss:0.9170, Accuracy:0.5643, Validation Loss:1.0463, Validation Accuracy:0.4647\n",
    "Epoch #238: Loss:0.9205, Accuracy:0.5667, Validation Loss:1.0448, Validation Accuracy:0.4680\n",
    "Epoch #239: Loss:0.9202, Accuracy:0.5626, Validation Loss:1.0476, Validation Accuracy:0.4745\n",
    "Epoch #240: Loss:0.9212, Accuracy:0.5577, Validation Loss:1.0588, Validation Accuracy:0.4828\n",
    "Epoch #241: Loss:0.9270, Accuracy:0.5491, Validation Loss:1.0536, Validation Accuracy:0.4647\n",
    "Epoch #242: Loss:0.9182, Accuracy:0.5593, Validation Loss:1.0513, Validation Accuracy:0.4696\n",
    "Epoch #243: Loss:0.9124, Accuracy:0.5671, Validation Loss:1.0472, Validation Accuracy:0.4713\n",
    "Epoch #244: Loss:0.9137, Accuracy:0.5667, Validation Loss:1.0504, Validation Accuracy:0.4745\n",
    "Epoch #245: Loss:0.9132, Accuracy:0.5692, Validation Loss:1.0451, Validation Accuracy:0.4762\n",
    "Epoch #246: Loss:0.9146, Accuracy:0.5556, Validation Loss:1.0492, Validation Accuracy:0.4680\n",
    "Epoch #247: Loss:0.9126, Accuracy:0.5655, Validation Loss:1.0489, Validation Accuracy:0.4631\n",
    "Epoch #248: Loss:0.9141, Accuracy:0.5606, Validation Loss:1.0531, Validation Accuracy:0.4598\n",
    "Epoch #249: Loss:0.9207, Accuracy:0.5589, Validation Loss:1.0555, Validation Accuracy:0.4532\n",
    "Epoch #250: Loss:0.9161, Accuracy:0.5655, Validation Loss:1.0621, Validation Accuracy:0.4745\n",
    "Epoch #251: Loss:0.9228, Accuracy:0.5585, Validation Loss:1.0487, Validation Accuracy:0.4663\n",
    "Epoch #252: Loss:0.9156, Accuracy:0.5733, Validation Loss:1.0520, Validation Accuracy:0.4696\n",
    "Epoch #253: Loss:0.9104, Accuracy:0.5655, Validation Loss:1.0473, Validation Accuracy:0.4729\n",
    "Epoch #254: Loss:0.9058, Accuracy:0.5676, Validation Loss:1.0568, Validation Accuracy:0.4778\n",
    "Epoch #255: Loss:0.9087, Accuracy:0.5696, Validation Loss:1.0480, Validation Accuracy:0.4729\n",
    "Epoch #256: Loss:0.9104, Accuracy:0.5639, Validation Loss:1.0502, Validation Accuracy:0.4663\n",
    "Epoch #257: Loss:0.9070, Accuracy:0.5667, Validation Loss:1.0539, Validation Accuracy:0.4581\n",
    "Epoch #258: Loss:0.9074, Accuracy:0.5671, Validation Loss:1.0522, Validation Accuracy:0.4647\n",
    "Epoch #259: Loss:0.9135, Accuracy:0.5655, Validation Loss:1.0584, Validation Accuracy:0.4516\n",
    "Epoch #260: Loss:0.9193, Accuracy:0.5536, Validation Loss:1.0608, Validation Accuracy:0.4860\n",
    "Epoch #261: Loss:0.9191, Accuracy:0.5593, Validation Loss:1.0596, Validation Accuracy:0.4565\n",
    "Epoch #262: Loss:0.9160, Accuracy:0.5713, Validation Loss:1.0644, Validation Accuracy:0.4745\n",
    "Epoch #263: Loss:0.9158, Accuracy:0.5569, Validation Loss:1.0466, Validation Accuracy:0.4844\n",
    "Epoch #264: Loss:0.9098, Accuracy:0.5655, Validation Loss:1.0523, Validation Accuracy:0.4647\n",
    "Epoch #265: Loss:0.9098, Accuracy:0.5585, Validation Loss:1.0506, Validation Accuracy:0.4680\n",
    "Epoch #266: Loss:0.8991, Accuracy:0.5717, Validation Loss:1.0495, Validation Accuracy:0.4729\n",
    "Epoch #267: Loss:0.8995, Accuracy:0.5741, Validation Loss:1.0561, Validation Accuracy:0.4745\n",
    "Epoch #268: Loss:0.8983, Accuracy:0.5676, Validation Loss:1.0522, Validation Accuracy:0.4663\n",
    "Epoch #269: Loss:0.8970, Accuracy:0.5733, Validation Loss:1.0598, Validation Accuracy:0.4631\n",
    "Epoch #270: Loss:0.8988, Accuracy:0.5700, Validation Loss:1.0575, Validation Accuracy:0.4631\n",
    "Epoch #271: Loss:0.8966, Accuracy:0.5708, Validation Loss:1.0601, Validation Accuracy:0.4614\n",
    "Epoch #272: Loss:0.8930, Accuracy:0.5819, Validation Loss:1.0591, Validation Accuracy:0.4647\n",
    "Epoch #273: Loss:0.8932, Accuracy:0.5766, Validation Loss:1.0609, Validation Accuracy:0.4663\n",
    "Epoch #274: Loss:0.8965, Accuracy:0.5749, Validation Loss:1.0621, Validation Accuracy:0.4598\n",
    "Epoch #275: Loss:0.9110, Accuracy:0.5655, Validation Loss:1.0697, Validation Accuracy:0.4598\n",
    "Epoch #276: Loss:0.9095, Accuracy:0.5696, Validation Loss:1.0628, Validation Accuracy:0.4516\n",
    "Epoch #277: Loss:0.9085, Accuracy:0.5643, Validation Loss:1.0768, Validation Accuracy:0.4631\n",
    "Epoch #278: Loss:0.9227, Accuracy:0.5507, Validation Loss:1.0616, Validation Accuracy:0.4778\n",
    "Epoch #279: Loss:0.8955, Accuracy:0.5762, Validation Loss:1.0642, Validation Accuracy:0.4778\n",
    "Epoch #280: Loss:0.9005, Accuracy:0.5713, Validation Loss:1.0642, Validation Accuracy:0.4745\n",
    "Epoch #281: Loss:0.8945, Accuracy:0.5745, Validation Loss:1.0594, Validation Accuracy:0.4696\n",
    "Epoch #282: Loss:0.8924, Accuracy:0.5799, Validation Loss:1.0595, Validation Accuracy:0.4631\n",
    "Epoch #283: Loss:0.8909, Accuracy:0.5770, Validation Loss:1.0616, Validation Accuracy:0.4598\n",
    "Epoch #284: Loss:0.8874, Accuracy:0.5823, Validation Loss:1.0652, Validation Accuracy:0.4598\n",
    "Epoch #285: Loss:0.8858, Accuracy:0.5873, Validation Loss:1.0684, Validation Accuracy:0.4631\n",
    "Epoch #286: Loss:0.8859, Accuracy:0.5811, Validation Loss:1.0695, Validation Accuracy:0.4647\n",
    "Epoch #287: Loss:0.8879, Accuracy:0.5754, Validation Loss:1.0760, Validation Accuracy:0.4745\n",
    "Epoch #288: Loss:0.9014, Accuracy:0.5713, Validation Loss:1.0793, Validation Accuracy:0.4663\n",
    "Epoch #289: Loss:0.8957, Accuracy:0.5696, Validation Loss:1.1112, Validation Accuracy:0.4548\n",
    "Epoch #290: Loss:0.9145, Accuracy:0.5544, Validation Loss:1.0643, Validation Accuracy:0.4647\n",
    "Epoch #291: Loss:0.9128, Accuracy:0.5573, Validation Loss:1.0760, Validation Accuracy:0.4565\n",
    "Epoch #292: Loss:0.8973, Accuracy:0.5618, Validation Loss:1.0584, Validation Accuracy:0.4795\n",
    "Epoch #293: Loss:0.8965, Accuracy:0.5655, Validation Loss:1.0931, Validation Accuracy:0.4581\n",
    "Epoch #294: Loss:0.9087, Accuracy:0.5622, Validation Loss:1.0619, Validation Accuracy:0.4745\n",
    "Epoch #295: Loss:0.8857, Accuracy:0.5901, Validation Loss:1.0626, Validation Accuracy:0.4647\n",
    "Epoch #296: Loss:0.8949, Accuracy:0.5655, Validation Loss:1.0612, Validation Accuracy:0.4713\n",
    "Epoch #297: Loss:0.8819, Accuracy:0.5860, Validation Loss:1.0649, Validation Accuracy:0.4647\n",
    "Epoch #298: Loss:0.8804, Accuracy:0.5893, Validation Loss:1.0706, Validation Accuracy:0.4581\n",
    "Epoch #299: Loss:0.8816, Accuracy:0.5860, Validation Loss:1.0728, Validation Accuracy:0.4647\n",
    "Epoch #300: Loss:0.8792, Accuracy:0.5897, Validation Loss:1.0730, Validation Accuracy:0.4663\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07299793, Accuracy:0.4663\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03   01   02\n",
    "t:03  36   60   46\n",
    "t:01  33  118   89\n",
    "t:02  11   86  130\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.45      0.25      0.32       142\n",
    "          01       0.45      0.49      0.47       240\n",
    "          02       0.49      0.57      0.53       227\n",
    "\n",
    "    accuracy                           0.47       609\n",
    "   macro avg       0.46      0.44      0.44       609\n",
    "weighted avg       0.46      0.47      0.46       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 21:00:18 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 37 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0781816592553175, 1.0744174601409235, 1.0762954045986306, 1.0747854836860118, 1.0739309006723865, 1.073809548551813, 1.07361471731283, 1.0731592624645514, 1.0725617342198814, 1.0721273203005737, 1.0715183005935844, 1.0709211822409543, 1.0701311752322469, 1.0693478819184703, 1.0685679200051845, 1.067383832140705, 1.0664640561308962, 1.065353011067082, 1.0644787017543524, 1.0640842930240975, 1.0636258604882778, 1.0628991193567787, 1.0623013178507488, 1.062189821735, 1.0616742131745287, 1.061547211434062, 1.061133740766491, 1.0608331053127795, 1.0606426458640639, 1.0603287946218731, 1.0601750674897619, 1.0598964072604877, 1.059925728830798, 1.059457335174573, 1.0598773124378498, 1.0591370476094764, 1.0593689883675286, 1.0588579273771965, 1.0587105647292239, 1.0584562316120942, 1.0583159017249673, 1.0580646784239018, 1.0579097807309505, 1.057197527932416, 1.0569478120709874, 1.0565311904807004, 1.0562853464743578, 1.0559787525136286, 1.055396376180727, 1.0547782934357968, 1.0544064412954797, 1.0536762773501267, 1.0527545577786825, 1.052353308118623, 1.0515504926687782, 1.0503242152860794, 1.049026121451154, 1.0475183719484678, 1.0462557533495924, 1.0450982511141422, 1.0438825305068042, 1.043278128447008, 1.0410699768019427, 1.040401884487697, 1.0404360920729112, 1.0398997332662197, 1.040427399777818, 1.0404487490066754, 1.0415049440950792, 1.0382638165516218, 1.0381258030243108, 1.038631733513035, 1.0356660086924612, 1.036211883199626, 1.042934190561423, 1.0361388301222978, 1.0369301686928973, 1.0359392836567607, 1.0372751502959403, 1.0363342151266013, 1.0343394255990466, 1.0408802813497082, 1.0352457908573995, 1.0363621561006569, 1.0339715680465322, 1.0371170693821898, 1.0379485193340259, 1.0338661564981997, 1.0359458032695728, 1.0346112088812591, 1.0361332842477633, 1.0354398718021187, 1.0345982657669017, 1.0337076729350099, 1.039468858629612, 1.0341985816830288, 1.0333411071100846, 1.0354735404987054, 1.0375974798828902, 1.0340408822781542, 1.0373437633655342, 1.0336680380973127, 1.036509067554192, 1.035943115090306, 1.0370778095937518, 1.0339336712372127, 1.0345956376816448, 1.0354224402329018, 1.0399814429150034, 1.033808143268078, 1.036605711053745, 1.0374688389657558, 1.0393306711820154, 1.038162404288995, 1.0357880155832702, 1.036285230287386, 1.0386116471392377, 1.0357484549332918, 1.0379857241813772, 1.036909356101589, 1.0351487666319548, 1.0405535807554749, 1.0354960698799547, 1.0382500653979423, 1.0323713256416258, 1.031640234056169, 1.0368942496028832, 1.0342725799197243, 1.0367780007752292, 1.0348710778898793, 1.0424200858192882, 1.0384440844869378, 1.0352800205619073, 1.0391080710296756, 1.034598249715733, 1.0316215757470217, 1.0323037302552773, 1.0354351091071694, 1.0373437388972893, 1.0368250590827077, 1.0363345496564467, 1.0352215776694036, 1.0368637512078622, 1.0330724659420194, 1.0337896920581562, 1.0364505354015308, 1.0345006779888384, 1.0343601032235157, 1.037658676921049, 1.0396465339096896, 1.0338763217816407, 1.037848773261009, 1.0439191622099853, 1.040668717354585, 1.0362943359979464, 1.035422930772277, 1.035394549761304, 1.0418508557850503, 1.033352260910623, 1.0402534965223866, 1.036723869578983, 1.04191304310202, 1.0381453779139151, 1.0408420190826817, 1.0368445062480733, 1.0430909970711018, 1.041703457511313, 1.0342687010177838, 1.038610109358977, 1.0445202839589862, 1.043675349655214, 1.0341766395396592, 1.0358005315799432, 1.0388978923287102, 1.0350499863695042, 1.0500235483172689, 1.043485544976734, 1.0357747322624344, 1.0379668059215952, 1.0335028404476998, 1.036439114603503, 1.0401714781822242, 1.0352999518070314, 1.0420200959802262, 1.0340071172745553, 1.0375031407047766, 1.0423948242159313, 1.0314005636816541, 1.0607955316800397, 1.0378846828573443, 1.0432811529178339, 1.0338169541852227, 1.0312062167181757, 1.0367133494081169, 1.036903485875999, 1.038866175610835, 1.0428323428619084, 1.0412602518579643, 1.037765085599301, 1.0503766072794722, 1.0377695399943636, 1.0358347722462244, 1.0376365642829481, 1.0369948072386492, 1.042350246792748, 1.0397031970799262, 1.0391245062519567, 1.0425823868201871, 1.0388107313506905, 1.0434196136267901, 1.0381012270211782, 1.041715749574608, 1.0405269939519697, 1.038565257304212, 1.0363869226624813, 1.039423371183461, 1.0449147034552688, 1.0378264846472904, 1.0421704149794304, 1.0412460426587384, 1.054998307979753, 1.043521323814768, 1.041299664523997, 1.038128577038181, 1.0420481702572801, 1.043064105686883, 1.0491661951068196, 1.0486589584053052, 1.046561408121206, 1.037544198419856, 1.0415172682607114, 1.0392890906294774, 1.0432654910878398, 1.0413093208679425, 1.045057816849945, 1.046520281112057, 1.046287496297426, 1.0448358595273373, 1.0475754489256635, 1.0587979883983218, 1.0536145602149525, 1.0512550429170355, 1.0472376814420978, 1.0503664784047795, 1.0450901155205587, 1.0492255738607572, 1.0489048922590434, 1.0531188468823487, 1.0554598407400848, 1.062100682744056, 1.0486861972386026, 1.0519543179541777, 1.0473293434027184, 1.0568442015812314, 1.0480053998370868, 1.0502378314195204, 1.053898406733433, 1.052186657642496, 1.0584205825536317, 1.060786008443347, 1.0595884591292082, 1.0644457808073322, 1.0466087704221603, 1.0523490529929476, 1.0506272709428384, 1.0494649322162122, 1.056143152889947, 1.0522281495221142, 1.059769250097729, 1.0574592774724725, 1.060070125144495, 1.0590898051050497, 1.060887770504004, 1.0621094090989462, 1.0697110249295414, 1.0628222336714295, 1.0768174721885393, 1.0615841050453374, 1.0641684064332684, 1.064185156610799, 1.0593616181406482, 1.059462742656714, 1.0615626951352324, 1.0652296032224382, 1.0683988270109706, 1.0694585173392335, 1.0760190175671882, 1.0793059571035977, 1.1112384815717173, 1.0642520852864081, 1.0760003062108858, 1.0583856925980015, 1.0931260117952069, 1.061859350486342, 1.0626305750829637, 1.0612031148963765, 1.064889566847452, 1.0706252347072358, 1.0728315942980386, 1.0729979895214337], 'val_acc': [0.39408866882519966, 0.39408866882519966, 0.372742199007122, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3957307049499944, 0.3990147771995839, 0.3957307051457404, 0.40229884964491935, 0.3973727408790432, 0.39244663260253193, 0.4039408856718411, 0.38916256025506946, 0.4039408856718411, 0.40558292179663585, 0.3924466327982779, 0.3908045964777372, 0.3908045965756102, 0.3924466327982779, 0.38587848800547997, 0.3875205242281477, 0.38916256015719647, 0.3875205241302747, 0.39244663260253193, 0.385878487907607, 0.38423645178281224, 0.38423645168493925, 0.39244663250465894, 0.385878487907607, 0.3908045962819912, 0.3940886685315807, 0.3875205240324017, 0.39737274097691616, 0.38752052393452874, 0.39573070465637544, 0.3825944154622715, 0.38423645168493925, 0.38423645168493925, 0.40065681312863266, 0.3990147770038379, 0.38423645178281224, 0.38752052383665575, 0.3973727407811702, 0.3891625600593235, 0.3891625600593235, 0.3908045962819912, 0.38752052373878276, 0.3908045962819912, 0.38752052373878276, 0.38916256015719647, 0.385878487711861, 0.39244663240678596, 0.38095237943534976, 0.37602627096309255, 0.379310343310555, 0.3940886685315807, 0.3940886683358347, 0.3809523797289687, 0.3924466327004049, 0.4400656810045634, 0.4039408858675871, 0.42692939200620544, 0.43842364517338756, 0.43349753244365574, 0.45155993426961855, 0.43678160469324523, 0.4417077173251041, 0.44170771296975647, 0.45320196594119266, 0.4433497491924242, 0.45155993417174556, 0.4515599298163979, 0.4433497491924242, 0.44499178928107463, 0.45320196574544674, 0.4499178935937302, 0.44170771703148515, 0.4482758617264101, 0.43842364487976865, 0.4581280742177039, 0.4449917893789476, 0.44170771752085003, 0.44663382560161535, 0.44991789814482375, 0.4466338214420137, 0.4400656810045634, 0.4417077173251041, 0.4482758618242831, 0.44334975344989885, 0.4449917895746936, 0.4449917895746936, 0.4433497532541529, 0.4433497535477718, 0.44334975344989885, 0.44334975335202587, 0.43842364478189566, 0.44170771712935814, 0.4400656809066904, 0.4499178979490778, 0.4482758617264101, 0.43842364487976865, 0.44334975344989885, 0.44170771703148515, 0.4449917893789476, 0.44170771693361216, 0.4367816087549739, 0.4449917893789476, 0.4417077172272311, 0.44663382560161535, 0.44170771703148515, 0.43842364507551457, 0.4466338253079964, 0.4400656810045634, 0.44170771693361216, 0.44663382569948834, 0.43842364478189566, 0.44991789765545886, 0.4466338254058694, 0.4400656809066904, 0.44334975286266093, 0.44334975335202587, 0.44827586133491815, 0.44663382550374237, 0.4499178979490778, 0.4482758615306641, 0.43842364497764164, 0.4564860422525108, 0.44499178928107463, 0.4646962230722305, 0.44827586104129924, 0.4564860422525108, 0.44006568051519845, 0.44991789785120484, 0.45812807847517856, 0.46141215072476804, 0.43349753640751143, 0.4532019701986673, 0.45320196990504835, 0.4482758617264101, 0.45320196990504835, 0.45320197000292134, 0.4564860422525108, 0.4597701146978463, 0.44663382560161535, 0.4597701144042273, 0.4597701146978463, 0.44334975335202587, 0.45320196980717536, 0.46141215052902207, 0.4564860424482568, 0.4564860423503838, 0.4499178974597129, 0.4564860423503838, 0.46305418704530876, 0.46633825919702526, 0.44334975286266093, 0.46962233115299584, 0.4532019701007943, 0.4581280781815596, 0.4597701145999733, 0.4597701145021003, 0.467980295223947, 0.4663382590991523, 0.44827586133491815, 0.4466338253079964, 0.4630541869474358, 0.47126436776715547, 0.44499178928107463, 0.4384236445861497, 0.46633825919702526, 0.45648604215463784, 0.45320196990504835, 0.4646962230722305, 0.45812807847517856, 0.4384236443904037, 0.46962233154448774, 0.46141215082264103, 0.4729064037940772, 0.46962233144661475, 0.46962233144661475, 0.4712643674735365, 0.46469622287648454, 0.47783251216846145, 0.47126436776715547, 0.46141215043114914, 0.46633825929489825, 0.4548440060298431, 0.4679802950282011, 0.47454843991887197, 0.4778325077152409, 0.474548439820999, 0.4811165798669574, 0.46962233154448774, 0.4630541868495628, 0.46141215043114914, 0.46141215062689506, 0.4696223269933942, 0.4729064038919502, 0.47783251216846145, 0.4729064037940772, 0.4794745482932562, 0.46962233144661475, 0.46962233144661475, 0.46305418714318175, 0.4729064038919502, 0.47290640359833125, 0.4630541868495628, 0.47454844001674495, 0.46633825929489825, 0.4729064037940772, 0.4646962232679765, 0.47454844001674495, 0.47783251216846145, 0.46962233144661475, 0.47290639924298367, 0.4712643674735365, 0.4630541824942152, 0.4761904760436667, 0.4696223269933942, 0.4646962231701035, 0.47619047149257315, 0.474548439820999, 0.46469621871688294, 0.4712643631181889, 0.4597701147957193, 0.4597701142084814, 0.4778325119727155, 0.46469622287648454, 0.4696223269933942, 0.46962233144661475, 0.47783251216846145, 0.4712643674735365, 0.4827586204449727, 0.4729064035004583, 0.46469622287648454, 0.467980295223947, 0.47454843962525306, 0.4827586204449727, 0.46469622258286564, 0.4696223269933942, 0.4712643672777906, 0.4745484353677784, 0.4761904759457937, 0.46798029532182, 0.4630541869474358, 0.4597701147957193, 0.4532019701007943, 0.47454843991887197, 0.4663382590991523, 0.46962233154448774, 0.47290640340258533, 0.4778325119727155, 0.47290640369620424, 0.46633825919702526, 0.45812807837730557, 0.46469622287648454, 0.4515599339759996, 0.48604269269456224, 0.45648604156739997, 0.47454843962525306, 0.48440065656976744, 0.46469622287648454, 0.467980295419693, 0.47290640359833125, 0.474548439723126, 0.46633825919702526, 0.4630541866538168, 0.4630541868495628, 0.46141215082264103, 0.4646962230722305, 0.4663382590991523, 0.4597701145021003, 0.4597701144042273, 0.4515599337802536, 0.4630541869474358, 0.47783251167909657, 0.47783251177696956, 0.4745484394295071, 0.46962233144661475, 0.4630541869474358, 0.4597701145021003, 0.4597701146978463, 0.4630541869474358, 0.4646962230722305, 0.4745484395273801, 0.46633825880553337, 0.4548440060298431, 0.4646962230722305, 0.4564860422525108, 0.4794745481953832, 0.45812807808368666, 0.474548439820999, 0.46469622277861156, 0.4712643673756635, 0.46469622297435753, 0.4581280782794326, 0.4646962226807386, 0.4663382590012793], 'loss': [1.0874074774356348, 1.0754540410619498, 1.0753241204138408, 1.075815473151158, 1.0741604602312405, 1.073813463236517, 1.0737343219271431, 1.0735296729653767, 1.0732336174536046, 1.0726123284020708, 1.0722550234510668, 1.0717787139959158, 1.0711364483686443, 1.0704564066638202, 1.0701985170219468, 1.0689277661164929, 1.0680849713221712, 1.067226030792299, 1.0664565361500766, 1.0660667171713263, 1.0656035135414077, 1.065136065473302, 1.0646880672697658, 1.0649494919688796, 1.0646120852758263, 1.0643563547173565, 1.0642089258718785, 1.0637540743825862, 1.0634937772515862, 1.0636946290915017, 1.063451840647437, 1.0632090143109738, 1.0636480041108336, 1.06262980716674, 1.0630823242101337, 1.0628581203719185, 1.0624545214357317, 1.0623277974569332, 1.0619642905875644, 1.062037408914899, 1.061375558449747, 1.0616302766839092, 1.0614118737606542, 1.0609443757078731, 1.0604027462201442, 1.0601420540584432, 1.059608630820711, 1.0595174130962615, 1.0587555831462696, 1.0583516346110946, 1.0577442646516177, 1.0569975868632417, 1.0563267666211609, 1.0560920476423887, 1.0548913030408982, 1.053755353167806, 1.0521720940572281, 1.0507254442884693, 1.0494997592922108, 1.0483220922873495, 1.0465817476934476, 1.0445702696727777, 1.042589496880831, 1.0404996377486713, 1.038882120531932, 1.0348420372733835, 1.034463766859787, 1.0338329197200171, 1.0296510239646175, 1.031021663491486, 1.02599816317431, 1.0264904384005975, 1.0248279790858714, 1.0241884152257712, 1.0291917332388782, 1.0302297098191122, 1.0251187958022163, 1.0220998934407008, 1.0186669707543061, 1.0183965086692168, 1.0162389420875533, 1.0168190117244602, 1.0173036968438778, 1.0165962411637668, 1.0144191649904977, 1.0159309555372908, 1.0145910613590687, 1.0131058824625347, 1.0120151504109283, 1.0101912355520888, 1.0101434417818607, 1.0105996384512963, 1.0084502583901251, 1.0087243646566872, 1.0119471283174393, 1.0063974141095453, 1.0071519543747638, 1.0077696565729881, 1.0078762112701698, 1.0068837315639676, 1.0037364857887097, 1.0038750017448128, 1.0034093364308256, 1.0023231431444077, 1.0019670480575404, 1.0003429885517645, 1.0003106375739315, 0.9993572804472529, 0.9977996065876078, 0.9990018920487203, 0.9979363443425548, 0.996483461127389, 0.9998794423726061, 0.9970853613632171, 0.9959172997876115, 0.9950710473853704, 0.9954718439975558, 0.9965837451222006, 0.9942712770839981, 0.9943725425604677, 0.9980431164559398, 0.9960665586792713, 0.9977410058466071, 1.0007861636747324, 1.0009404324652968, 0.9960320426208528, 0.9905916140064811, 0.9915188824861202, 0.9920083746528234, 0.9908512515698615, 0.9967034867901576, 0.9985133573994255, 0.994101442522092, 0.9890048412326915, 0.9898966115358184, 0.9889019619023286, 0.9864811219718667, 0.9863170536147006, 0.9903434893433808, 0.9911488535467848, 0.9914359719846283, 0.9890770888671249, 0.9872157271637809, 0.9843954294613989, 0.9831354367170001, 0.9822549661816513, 0.9815093528563482, 0.9812605433395512, 0.98293020470676, 0.981804865047917, 0.9802950190567628, 0.9788550615555451, 0.9791051231125787, 0.9866436268759459, 0.9775968472081288, 0.9791361222766508, 0.9775845608672077, 0.977350270552312, 0.9772213553745889, 0.9751673165777626, 0.9748358378420131, 0.9739214154729119, 0.9738262631809932, 0.9744676270768873, 0.9769397775984887, 0.9762746122093906, 0.9748014803050236, 0.9728399804974973, 0.9735630445167025, 0.9772048524273005, 0.9787206522248364, 0.9754560299722566, 0.9683216252610913, 0.9700618341962904, 0.9708431706291449, 0.9672474662870841, 0.9731166594816674, 0.9698717103601726, 0.9645582653413808, 0.9642099655629184, 0.9647803555768618, 0.9654829144722628, 0.9672273761193121, 0.9674587867343205, 0.966430547986432, 0.9635614187810455, 0.9688111128258754, 0.9613846733340002, 0.958170539373245, 0.9666995589493237, 0.9651758188339719, 0.9639918415942965, 0.9582215104504532, 0.9531856839172159, 0.9530893413682738, 0.9531753307740056, 0.9530285208621798, 0.9518693097318223, 0.9485692870690348, 0.951014423419318, 0.9594420280544664, 0.9487889178724505, 0.9469275715904314, 0.9494863991375087, 0.9444094715911505, 0.9435739987929499, 0.9423613161766553, 0.9437023519245751, 0.9398390334244872, 0.9466541161282597, 0.9445280383744524, 0.938003510224501, 0.9400858521951053, 0.9476432630168828, 0.9382791274137321, 0.938820468204467, 0.9378924325995867, 0.9348911894175551, 0.9310513197029395, 0.9362477790158877, 0.9407817027162477, 0.93772739671339, 0.9354077605496197, 0.931284719815734, 0.9332827971456477, 0.9302412674657129, 0.9341023838985137, 0.940722633901318, 0.9302609295570875, 0.9276815137334428, 0.9265768370344408, 0.9246164716489506, 0.9291468163780119, 0.9253489657593948, 0.9256126443952996, 0.9226456370931386, 0.91703930748072, 0.920548121150759, 0.9201922719728286, 0.9211687755046194, 0.9269910876021493, 0.9182006839854027, 0.9124233726358512, 0.9137146137088721, 0.9131806242637321, 0.914599499051331, 0.9126223943071933, 0.914089558672856, 0.920666388417661, 0.9161292070725615, 0.9228415950367828, 0.9156008843768548, 0.9104183855977147, 0.9057947897323593, 0.9086743453200103, 0.9103766815618323, 0.9069817696263903, 0.9074009112019313, 0.9135334864044581, 0.9193414043596883, 0.9190675331581789, 0.9160116306565381, 0.9158060538450551, 0.9098046773757777, 0.909755677486592, 0.8990853874100797, 0.8995206882087112, 0.8983206061611919, 0.897001556914445, 0.8988080093013677, 0.8966020671738736, 0.89299852532283, 0.8932015493909926, 0.8965260492213208, 0.9109874323163434, 0.9095194084198812, 0.9084565255431424, 0.9226738875651507, 0.8954898321897832, 0.9004821565606511, 0.8944697177140865, 0.8924228905652338, 0.890925342786973, 0.887362276504172, 0.8858471991345134, 0.8859188497923238, 0.8878609973057584, 0.9014215658087995, 0.8956524663637306, 0.9145335699988096, 0.9128122084439413, 0.8973313685560128, 0.8965147707251797, 0.9086615146063192, 0.8857201690791324, 0.894886082716791, 0.881925406837855, 0.8804476908100214, 0.8815563165186856, 0.879204624462911], 'acc': [0.394250513714197, 0.3942505146933287, 0.3831622166555275, 0.37864476364006494, 0.39425051273506523, 0.3942505134816532, 0.39425051113173704, 0.3942505126983478, 0.3942505148891551, 0.39753593325125364, 0.4053388079945801, 0.40492812939737854, 0.40944558675773823, 0.41026694195716046, 0.40328542307417004, 0.4069815186994032, 0.41355236133510814, 0.41478439598847217, 0.40492812939737854, 0.4053388105770401, 0.40821355230020057, 0.4078028727238673, 0.40369610088806623, 0.40533881116451914, 0.4102669413696814, 0.41396304114398524, 0.4123203276975933, 0.4049281313923595, 0.4102669421897042, 0.409856263788329, 0.4131416849287139, 0.4131416823462539, 0.4049281325673176, 0.4102669427771833, 0.41724845872277844, 0.4172484595060838, 0.41149897543556635, 0.40780287291969364, 0.411498974652261, 0.41642710646075143, 0.4127310076655793, 0.4082135544910078, 0.41149897246145367, 0.41478439657595123, 0.4119096491003918, 0.4098562607774989, 0.4147843952051668, 0.4147843957559284, 0.4127310045323577, 0.41232032691428794, 0.4098562649632871, 0.41273100688227393, 0.4184804943552742, 0.41560574864215183, 0.4102669408189198, 0.413552362155131, 0.42094455806871217, 0.4180698173246834, 0.4201232016576144, 0.42956878726487285, 0.4340862441968624, 0.43490759786639127, 0.44024640490631795, 0.43572895173174647, 0.4398357283040973, 0.4468172483987632, 0.4472279253926365, 0.4525667364592425, 0.4476386018357483, 0.45626283142356167, 0.4616016404584693, 0.4603696082284563, 0.47227926228570255, 0.46652977504524606, 0.4566735129948759, 0.4558521565837782, 0.47104722665320675, 0.4739219695880428, 0.47802874776372184, 0.4726899375171387, 0.4739219729538081, 0.48501026684253856, 0.47474332423670335, 0.4841889118022253, 0.4792607788187767, 0.48542094465643476, 0.4722792630690079, 0.48747433372591553, 0.4829568803188003, 0.4874743313759994, 0.48501026547175413, 0.48952771946634843, 0.484188912193878, 0.4924024651427533, 0.4841889098439618, 0.49281314338501964, 0.49527720807758935, 0.4882956873954444, 0.48870636779180054, 0.4915811067733921, 0.4960985605721601, 0.4928131402150806, 0.49733059577628574, 0.4969199199573705, 0.4956878829540903, 0.5014373727402893, 0.50349076180977, 0.49856262863049516, 0.5014373709778521, 0.49938398406246115, 0.4899383988468554, 0.5010266933597823, 0.4952772086650684, 0.5018480489875746, 0.5043121170826271, 0.505544146962724, 0.4985626296096269, 0.4981519504249463, 0.5030800839958739, 0.49774127261105017, 0.501848049183401, 0.49568788549983284, 0.5010266953547633, 0.495277207881763, 0.4973305955804594, 0.4989733084026548, 0.5030800830167422, 0.5112936365530966, 0.5002053385152954, 0.5071868572391769, 0.5043121172417361, 0.48952772475365985, 0.5055441483335084, 0.5084188926391289, 0.5100616042863662, 0.5014373699987204, 0.5121149892434936, 0.5084188900933863, 0.5067761788378017, 0.4977412712402657, 0.5092402467003103, 0.5075975370113365, 0.5141683785088008, 0.517453799257533, 0.5112936381197074, 0.5149897333532878, 0.5141683765505374, 0.5129363468295495, 0.5137577024573419, 0.5174538033698863, 0.5121149929641943, 0.5162217667949762, 0.5174537988658803, 0.5100616011531446, 0.517453799845012, 0.5211498986034667, 0.5182751560602834, 0.5199178665325627, 0.5182751560602834, 0.5215605772006683, 0.5219712532888447, 0.5170431208561578, 0.5285420935746336, 0.5244353207229834, 0.5063655021988636, 0.5170431194853734, 0.5170431228144213, 0.5149897370739884, 0.523203288456253, 0.5178644790296927, 0.5174537994533593, 0.5244353193521989, 0.5268993879980131, 0.5273100636578194, 0.5268993862355759, 0.5322381948788308, 0.5112936342031804, 0.5293634456041167, 0.5297741293662382, 0.5305954844065516, 0.5281314179148272, 0.5318275162816293, 0.5285420951412444, 0.5268993842773124, 0.5301848062010027, 0.5330595510941022, 0.5260780309994363, 0.5305954846023779, 0.5371663270789739, 0.5232032864979895, 0.528542094357939, 0.5326488723010743, 0.5264887086175061, 0.5445585247917097, 0.5383983593457043, 0.5396303937665247, 0.5412731038471512, 0.5338809077010263, 0.5379876823151136, 0.5392197161484548, 0.519917859801032, 0.5429158139277778, 0.5404517430544389, 0.539219713994365, 0.5515400454738546, 0.5388090377470796, 0.5466119140570168, 0.5396303941581773, 0.5523613995350362, 0.540862426033255, 0.5445585236167516, 0.5474332671390667, 0.5441478438445919, 0.536344965894609, 0.547433267922372, 0.550718686321188, 0.5429158111862089, 0.5482546245292961, 0.5498973330433119, 0.5498973316725274, 0.5355236148442576, 0.5412731042388039, 0.554004102884132, 0.5457905511102148, 0.5445585230292725, 0.5478439449529628, 0.5429158145152568, 0.5400410686430256, 0.5445585168852208, 0.55646816918003, 0.5622176629561908, 0.5585215649810416, 0.5457905574500928, 0.5552361442323094, 0.5585215564870737, 0.5568788475814052, 0.5642710435317038, 0.5667351096317753, 0.5626283390076499, 0.5577002099407282, 0.5490759795696094, 0.5593429113315606, 0.5671457858790607, 0.5667351159716534, 0.5691991794525475, 0.5556468214587265, 0.5655030795191348, 0.5605749517006061, 0.5589322346926225, 0.5655030821383121, 0.5585215636102571, 0.5733059590357285, 0.5655030840965757, 0.5675564723827511, 0.5696098515874796, 0.5638603714702066, 0.5667351171466115, 0.5671457858790607, 0.5655030756026078, 0.5535934244827568, 0.5593429194338757, 0.5712525636263697, 0.5568788519630197, 0.5655030835090966, 0.5585215568787263, 0.5716632433985293, 0.5741273055820739, 0.567556471795272, 0.5733059526958505, 0.5700205354719926, 0.5708418862041262, 0.581930183458622, 0.5765913742278881, 0.5749486671580916, 0.5655030844882284, 0.5696098537415695, 0.5642710443150092, 0.550718689650236, 0.5761807019705645, 0.5712525651929804, 0.5745379878999761, 0.5798767928225304, 0.5770020577941832, 0.582340859510081, 0.5872689934726614, 0.5811088258725662, 0.5753593467344249, 0.5712525697704214, 0.569609856287312, 0.5544147872337326, 0.5572895321268321, 0.5618069782149375, 0.5655030759942605, 0.5622176560288337, 0.5901437369949764, 0.5655030775608713, 0.586036957876883, 0.5893223784297889, 0.5860369653917191, 0.5897330627793894]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
