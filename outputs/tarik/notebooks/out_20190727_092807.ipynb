{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf36.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 09:28:07 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'Front', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000213856B4E10>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000021382E36EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0892, Accuracy:0.3729, Validation Loss:1.0795, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0762, Accuracy:0.3860, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0758, Accuracy:0.3943, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0752, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0727, Accuracy:0.3959, Validation Loss:1.0721, Validation Accuracy:0.4023\n",
    "Epoch #17: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0711, Validation Accuracy:0.4089\n",
    "Epoch #18: Loss:1.0709, Accuracy:0.4127, Validation Loss:1.0695, Validation Accuracy:0.4187\n",
    "Epoch #19: Loss:1.0695, Accuracy:0.4324, Validation Loss:1.0671, Validation Accuracy:0.4483\n",
    "Epoch #20: Loss:1.0668, Accuracy:0.4419, Validation Loss:1.0633, Validation Accuracy:0.4417\n",
    "Epoch #21: Loss:1.0629, Accuracy:0.4374, Validation Loss:1.0572, Validation Accuracy:0.4647\n",
    "Epoch #22: Loss:1.0572, Accuracy:0.4657, Validation Loss:1.0481, Validation Accuracy:0.4614\n",
    "Epoch #23: Loss:1.0482, Accuracy:0.4727, Validation Loss:1.0354, Validation Accuracy:0.4729\n",
    "Epoch #24: Loss:1.0374, Accuracy:0.4739, Validation Loss:1.0222, Validation Accuracy:0.4762\n",
    "Epoch #25: Loss:1.0270, Accuracy:0.4739, Validation Loss:1.0137, Validation Accuracy:0.4762\n",
    "Epoch #26: Loss:1.0238, Accuracy:0.4735, Validation Loss:1.0123, Validation Accuracy:0.4762\n",
    "Epoch #27: Loss:1.0239, Accuracy:0.4702, Validation Loss:1.0122, Validation Accuracy:0.4745\n",
    "Epoch #28: Loss:1.0222, Accuracy:0.4760, Validation Loss:1.0117, Validation Accuracy:0.4713\n",
    "Epoch #29: Loss:1.0199, Accuracy:0.4727, Validation Loss:1.0131, Validation Accuracy:0.4614\n",
    "Epoch #30: Loss:1.0186, Accuracy:0.4756, Validation Loss:1.0107, Validation Accuracy:0.4778\n",
    "Epoch #31: Loss:1.0203, Accuracy:0.4768, Validation Loss:1.0102, Validation Accuracy:0.4680\n",
    "Epoch #32: Loss:1.0194, Accuracy:0.4735, Validation Loss:1.0099, Validation Accuracy:0.4647\n",
    "Epoch #33: Loss:1.0193, Accuracy:0.4772, Validation Loss:1.0085, Validation Accuracy:0.4795\n",
    "Epoch #34: Loss:1.0176, Accuracy:0.4739, Validation Loss:1.0089, Validation Accuracy:0.4647\n",
    "Epoch #35: Loss:1.0153, Accuracy:0.4813, Validation Loss:1.0071, Validation Accuracy:0.4680\n",
    "Epoch #36: Loss:1.0138, Accuracy:0.4838, Validation Loss:1.0071, Validation Accuracy:0.4647\n",
    "Epoch #37: Loss:1.0128, Accuracy:0.4854, Validation Loss:1.0059, Validation Accuracy:0.4795\n",
    "Epoch #38: Loss:1.0120, Accuracy:0.4936, Validation Loss:1.0045, Validation Accuracy:0.4877\n",
    "Epoch #39: Loss:1.0108, Accuracy:0.4932, Validation Loss:1.0033, Validation Accuracy:0.4910\n",
    "Epoch #40: Loss:1.0102, Accuracy:0.4957, Validation Loss:1.0041, Validation Accuracy:0.4910\n",
    "Epoch #41: Loss:1.0098, Accuracy:0.4924, Validation Loss:1.0015, Validation Accuracy:0.4860\n",
    "Epoch #42: Loss:1.0119, Accuracy:0.4879, Validation Loss:1.0056, Validation Accuracy:0.4844\n",
    "Epoch #43: Loss:1.0099, Accuracy:0.4899, Validation Loss:1.0005, Validation Accuracy:0.5041\n",
    "Epoch #44: Loss:1.0064, Accuracy:0.5031, Validation Loss:1.0025, Validation Accuracy:0.4959\n",
    "Epoch #45: Loss:1.0054, Accuracy:0.5023, Validation Loss:0.9988, Validation Accuracy:0.4877\n",
    "Epoch #46: Loss:1.0034, Accuracy:0.5051, Validation Loss:0.9986, Validation Accuracy:0.4943\n",
    "Epoch #47: Loss:1.0024, Accuracy:0.5072, Validation Loss:0.9963, Validation Accuracy:0.4877\n",
    "Epoch #48: Loss:1.0014, Accuracy:0.5043, Validation Loss:0.9970, Validation Accuracy:0.4959\n",
    "Epoch #49: Loss:1.0048, Accuracy:0.4994, Validation Loss:0.9963, Validation Accuracy:0.4975\n",
    "Epoch #50: Loss:1.0033, Accuracy:0.4940, Validation Loss:0.9962, Validation Accuracy:0.4959\n",
    "Epoch #51: Loss:0.9988, Accuracy:0.5014, Validation Loss:0.9930, Validation Accuracy:0.4910\n",
    "Epoch #52: Loss:0.9984, Accuracy:0.5051, Validation Loss:0.9945, Validation Accuracy:0.4926\n",
    "Epoch #53: Loss:0.9967, Accuracy:0.5060, Validation Loss:0.9925, Validation Accuracy:0.5041\n",
    "Epoch #54: Loss:0.9972, Accuracy:0.5039, Validation Loss:0.9914, Validation Accuracy:0.5025\n",
    "Epoch #55: Loss:0.9940, Accuracy:0.5088, Validation Loss:0.9890, Validation Accuracy:0.5025\n",
    "Epoch #56: Loss:0.9928, Accuracy:0.5084, Validation Loss:0.9884, Validation Accuracy:0.5041\n",
    "Epoch #57: Loss:0.9909, Accuracy:0.5129, Validation Loss:0.9868, Validation Accuracy:0.5057\n",
    "Epoch #58: Loss:0.9900, Accuracy:0.5097, Validation Loss:0.9855, Validation Accuracy:0.5074\n",
    "Epoch #59: Loss:0.9883, Accuracy:0.5105, Validation Loss:0.9839, Validation Accuracy:0.5123\n",
    "Epoch #60: Loss:0.9875, Accuracy:0.5129, Validation Loss:0.9848, Validation Accuracy:0.5140\n",
    "Epoch #61: Loss:0.9876, Accuracy:0.5113, Validation Loss:0.9860, Validation Accuracy:0.5057\n",
    "Epoch #62: Loss:0.9858, Accuracy:0.5088, Validation Loss:0.9796, Validation Accuracy:0.5057\n",
    "Epoch #63: Loss:0.9816, Accuracy:0.5179, Validation Loss:0.9791, Validation Accuracy:0.5172\n",
    "Epoch #64: Loss:0.9829, Accuracy:0.5088, Validation Loss:0.9803, Validation Accuracy:0.5074\n",
    "Epoch #65: Loss:0.9814, Accuracy:0.5154, Validation Loss:0.9834, Validation Accuracy:0.5025\n",
    "Epoch #66: Loss:0.9852, Accuracy:0.5072, Validation Loss:0.9939, Validation Accuracy:0.5222\n",
    "Epoch #67: Loss:0.9865, Accuracy:0.5092, Validation Loss:0.9787, Validation Accuracy:0.5057\n",
    "Epoch #68: Loss:0.9810, Accuracy:0.5138, Validation Loss:0.9725, Validation Accuracy:0.5041\n",
    "Epoch #69: Loss:0.9733, Accuracy:0.5187, Validation Loss:0.9668, Validation Accuracy:0.5238\n",
    "Epoch #70: Loss:0.9690, Accuracy:0.5207, Validation Loss:0.9663, Validation Accuracy:0.5156\n",
    "Epoch #71: Loss:0.9678, Accuracy:0.5236, Validation Loss:0.9628, Validation Accuracy:0.5255\n",
    "Epoch #72: Loss:0.9639, Accuracy:0.5269, Validation Loss:0.9639, Validation Accuracy:0.5287\n",
    "Epoch #73: Loss:0.9647, Accuracy:0.5203, Validation Loss:0.9575, Validation Accuracy:0.5271\n",
    "Epoch #74: Loss:0.9605, Accuracy:0.5265, Validation Loss:0.9538, Validation Accuracy:0.5287\n",
    "Epoch #75: Loss:0.9564, Accuracy:0.5310, Validation Loss:0.9589, Validation Accuracy:0.5320\n",
    "Epoch #76: Loss:0.9505, Accuracy:0.5318, Validation Loss:0.9490, Validation Accuracy:0.5287\n",
    "Epoch #77: Loss:0.9453, Accuracy:0.5372, Validation Loss:0.9415, Validation Accuracy:0.5435\n",
    "Epoch #78: Loss:0.9382, Accuracy:0.5376, Validation Loss:0.9432, Validation Accuracy:0.5369\n",
    "Epoch #79: Loss:0.9370, Accuracy:0.5372, Validation Loss:0.9307, Validation Accuracy:0.5337\n",
    "Epoch #80: Loss:0.9273, Accuracy:0.5392, Validation Loss:0.9243, Validation Accuracy:0.5353\n",
    "Epoch #81: Loss:0.9192, Accuracy:0.5425, Validation Loss:0.9601, Validation Accuracy:0.5304\n",
    "Epoch #82: Loss:0.9303, Accuracy:0.5384, Validation Loss:0.9458, Validation Accuracy:0.5205\n",
    "Epoch #83: Loss:0.9493, Accuracy:0.5244, Validation Loss:0.9670, Validation Accuracy:0.5090\n",
    "Epoch #84: Loss:0.9556, Accuracy:0.5060, Validation Loss:0.9106, Validation Accuracy:0.5386\n",
    "Epoch #85: Loss:0.9433, Accuracy:0.5154, Validation Loss:0.9459, Validation Accuracy:0.5189\n",
    "Epoch #86: Loss:0.9322, Accuracy:0.5310, Validation Loss:0.9178, Validation Accuracy:0.5353\n",
    "Epoch #87: Loss:0.9143, Accuracy:0.5483, Validation Loss:0.9163, Validation Accuracy:0.5550\n",
    "Epoch #88: Loss:0.9154, Accuracy:0.5421, Validation Loss:0.9116, Validation Accuracy:0.5402\n",
    "Epoch #89: Loss:0.9167, Accuracy:0.5446, Validation Loss:0.9248, Validation Accuracy:0.5304\n",
    "Epoch #90: Loss:0.9124, Accuracy:0.5462, Validation Loss:0.9061, Validation Accuracy:0.5304\n",
    "Epoch #91: Loss:0.9070, Accuracy:0.5483, Validation Loss:0.9092, Validation Accuracy:0.5435\n",
    "Epoch #92: Loss:0.9089, Accuracy:0.5487, Validation Loss:0.9397, Validation Accuracy:0.5353\n",
    "Epoch #93: Loss:0.9203, Accuracy:0.5335, Validation Loss:0.9221, Validation Accuracy:0.5238\n",
    "Epoch #94: Loss:0.9153, Accuracy:0.5433, Validation Loss:0.9119, Validation Accuracy:0.5271\n",
    "Epoch #95: Loss:0.9033, Accuracy:0.5454, Validation Loss:0.9026, Validation Accuracy:0.5369\n",
    "Epoch #96: Loss:0.8999, Accuracy:0.5552, Validation Loss:0.9176, Validation Accuracy:0.5402\n",
    "Epoch #97: Loss:0.9082, Accuracy:0.5433, Validation Loss:0.8963, Validation Accuracy:0.5550\n",
    "Epoch #98: Loss:0.9158, Accuracy:0.5355, Validation Loss:0.9591, Validation Accuracy:0.5090\n",
    "Epoch #99: Loss:0.9185, Accuracy:0.5437, Validation Loss:0.9089, Validation Accuracy:0.5271\n",
    "Epoch #100: Loss:0.9028, Accuracy:0.5483, Validation Loss:0.9028, Validation Accuracy:0.5304\n",
    "Epoch #101: Loss:0.8949, Accuracy:0.5511, Validation Loss:0.9065, Validation Accuracy:0.5337\n",
    "Epoch #102: Loss:0.8914, Accuracy:0.5520, Validation Loss:0.8967, Validation Accuracy:0.5468\n",
    "Epoch #103: Loss:0.8865, Accuracy:0.5569, Validation Loss:0.8985, Validation Accuracy:0.5501\n",
    "Epoch #104: Loss:0.8880, Accuracy:0.5606, Validation Loss:0.8984, Validation Accuracy:0.5419\n",
    "Epoch #105: Loss:0.8876, Accuracy:0.5569, Validation Loss:0.8943, Validation Accuracy:0.5501\n",
    "Epoch #106: Loss:0.8827, Accuracy:0.5602, Validation Loss:0.8975, Validation Accuracy:0.5501\n",
    "Epoch #107: Loss:0.8894, Accuracy:0.5552, Validation Loss:0.9120, Validation Accuracy:0.5517\n",
    "Epoch #108: Loss:0.8951, Accuracy:0.5536, Validation Loss:0.9063, Validation Accuracy:0.5517\n",
    "Epoch #109: Loss:0.8901, Accuracy:0.5552, Validation Loss:0.8941, Validation Accuracy:0.5320\n",
    "Epoch #110: Loss:0.8817, Accuracy:0.5618, Validation Loss:0.9097, Validation Accuracy:0.5369\n",
    "Epoch #111: Loss:0.8830, Accuracy:0.5548, Validation Loss:0.8900, Validation Accuracy:0.5501\n",
    "Epoch #112: Loss:0.8779, Accuracy:0.5663, Validation Loss:0.8911, Validation Accuracy:0.5517\n",
    "Epoch #113: Loss:0.8804, Accuracy:0.5643, Validation Loss:0.8920, Validation Accuracy:0.5550\n",
    "Epoch #114: Loss:0.8798, Accuracy:0.5618, Validation Loss:0.8920, Validation Accuracy:0.5501\n",
    "Epoch #115: Loss:0.8756, Accuracy:0.5696, Validation Loss:0.8977, Validation Accuracy:0.5402\n",
    "Epoch #116: Loss:0.8780, Accuracy:0.5626, Validation Loss:0.8895, Validation Accuracy:0.5484\n",
    "Epoch #117: Loss:0.8762, Accuracy:0.5634, Validation Loss:0.8930, Validation Accuracy:0.5452\n",
    "Epoch #118: Loss:0.8759, Accuracy:0.5696, Validation Loss:0.8875, Validation Accuracy:0.5534\n",
    "Epoch #119: Loss:0.8750, Accuracy:0.5688, Validation Loss:0.8873, Validation Accuracy:0.5501\n",
    "Epoch #120: Loss:0.8704, Accuracy:0.5733, Validation Loss:0.8868, Validation Accuracy:0.5501\n",
    "Epoch #121: Loss:0.8697, Accuracy:0.5680, Validation Loss:0.8868, Validation Accuracy:0.5484\n",
    "Epoch #122: Loss:0.8726, Accuracy:0.5717, Validation Loss:0.8914, Validation Accuracy:0.5402\n",
    "Epoch #123: Loss:0.8692, Accuracy:0.5626, Validation Loss:0.8855, Validation Accuracy:0.5501\n",
    "Epoch #124: Loss:0.8692, Accuracy:0.5758, Validation Loss:0.8898, Validation Accuracy:0.5402\n",
    "Epoch #125: Loss:0.8717, Accuracy:0.5659, Validation Loss:0.8899, Validation Accuracy:0.5435\n",
    "Epoch #126: Loss:0.8708, Accuracy:0.5680, Validation Loss:0.8846, Validation Accuracy:0.5517\n",
    "Epoch #127: Loss:0.8657, Accuracy:0.5708, Validation Loss:0.8840, Validation Accuracy:0.5484\n",
    "Epoch #128: Loss:0.8664, Accuracy:0.5717, Validation Loss:0.8845, Validation Accuracy:0.5452\n",
    "Epoch #129: Loss:0.8687, Accuracy:0.5692, Validation Loss:0.8896, Validation Accuracy:0.5402\n",
    "Epoch #130: Loss:0.8700, Accuracy:0.5737, Validation Loss:0.8930, Validation Accuracy:0.5369\n",
    "Epoch #131: Loss:0.8702, Accuracy:0.5696, Validation Loss:0.8902, Validation Accuracy:0.5435\n",
    "Epoch #132: Loss:0.8684, Accuracy:0.5659, Validation Loss:0.8873, Validation Accuracy:0.5452\n",
    "Epoch #133: Loss:0.8663, Accuracy:0.5696, Validation Loss:0.8822, Validation Accuracy:0.5435\n",
    "Epoch #134: Loss:0.8620, Accuracy:0.5749, Validation Loss:0.8859, Validation Accuracy:0.5501\n",
    "Epoch #135: Loss:0.8658, Accuracy:0.5737, Validation Loss:0.8819, Validation Accuracy:0.5452\n",
    "Epoch #136: Loss:0.8660, Accuracy:0.5729, Validation Loss:0.8889, Validation Accuracy:0.5517\n",
    "Epoch #137: Loss:0.8741, Accuracy:0.5602, Validation Loss:0.9215, Validation Accuracy:0.5304\n",
    "Epoch #138: Loss:0.8932, Accuracy:0.5507, Validation Loss:0.9065, Validation Accuracy:0.5468\n",
    "Epoch #139: Loss:0.8795, Accuracy:0.5614, Validation Loss:0.8795, Validation Accuracy:0.5517\n",
    "Epoch #140: Loss:0.8665, Accuracy:0.5708, Validation Loss:0.9028, Validation Accuracy:0.5353\n",
    "Epoch #141: Loss:0.8747, Accuracy:0.5647, Validation Loss:0.8818, Validation Accuracy:0.5484\n",
    "Epoch #142: Loss:0.8631, Accuracy:0.5688, Validation Loss:0.8905, Validation Accuracy:0.5402\n",
    "Epoch #143: Loss:0.8669, Accuracy:0.5671, Validation Loss:0.8784, Validation Accuracy:0.5419\n",
    "Epoch #144: Loss:0.8667, Accuracy:0.5696, Validation Loss:0.9030, Validation Accuracy:0.5419\n",
    "Epoch #145: Loss:0.8709, Accuracy:0.5626, Validation Loss:0.8882, Validation Accuracy:0.5435\n",
    "Epoch #146: Loss:0.8713, Accuracy:0.5639, Validation Loss:0.8904, Validation Accuracy:0.5287\n",
    "Epoch #147: Loss:0.8637, Accuracy:0.5696, Validation Loss:0.8853, Validation Accuracy:0.5402\n",
    "Epoch #148: Loss:0.8664, Accuracy:0.5692, Validation Loss:0.8789, Validation Accuracy:0.5452\n",
    "Epoch #149: Loss:0.8581, Accuracy:0.5725, Validation Loss:0.8894, Validation Accuracy:0.5468\n",
    "Epoch #150: Loss:0.8625, Accuracy:0.5708, Validation Loss:0.8771, Validation Accuracy:0.5468\n",
    "Epoch #151: Loss:0.8556, Accuracy:0.5704, Validation Loss:0.8872, Validation Accuracy:0.5402\n",
    "Epoch #152: Loss:0.8599, Accuracy:0.5708, Validation Loss:0.8782, Validation Accuracy:0.5468\n",
    "Epoch #153: Loss:0.8581, Accuracy:0.5717, Validation Loss:0.8885, Validation Accuracy:0.5402\n",
    "Epoch #154: Loss:0.8575, Accuracy:0.5717, Validation Loss:0.8792, Validation Accuracy:0.5419\n",
    "Epoch #155: Loss:0.8606, Accuracy:0.5733, Validation Loss:0.8796, Validation Accuracy:0.5419\n",
    "Epoch #156: Loss:0.8640, Accuracy:0.5626, Validation Loss:0.8885, Validation Accuracy:0.5287\n",
    "Epoch #157: Loss:0.8587, Accuracy:0.5630, Validation Loss:0.8775, Validation Accuracy:0.5419\n",
    "Epoch #158: Loss:0.8556, Accuracy:0.5717, Validation Loss:0.8769, Validation Accuracy:0.5468\n",
    "Epoch #159: Loss:0.8548, Accuracy:0.5737, Validation Loss:0.8832, Validation Accuracy:0.5402\n",
    "Epoch #160: Loss:0.8558, Accuracy:0.5717, Validation Loss:0.8821, Validation Accuracy:0.5337\n",
    "Epoch #161: Loss:0.8569, Accuracy:0.5614, Validation Loss:0.8781, Validation Accuracy:0.5452\n",
    "Epoch #162: Loss:0.8562, Accuracy:0.5676, Validation Loss:0.8806, Validation Accuracy:0.5386\n",
    "Epoch #163: Loss:0.8539, Accuracy:0.5737, Validation Loss:0.8786, Validation Accuracy:0.5452\n",
    "Epoch #164: Loss:0.8603, Accuracy:0.5688, Validation Loss:0.8868, Validation Accuracy:0.5337\n",
    "Epoch #165: Loss:0.8595, Accuracy:0.5634, Validation Loss:0.8773, Validation Accuracy:0.5419\n",
    "Epoch #166: Loss:0.8511, Accuracy:0.5692, Validation Loss:0.8814, Validation Accuracy:0.5320\n",
    "Epoch #167: Loss:0.8542, Accuracy:0.5676, Validation Loss:0.8811, Validation Accuracy:0.5419\n",
    "Epoch #168: Loss:0.8533, Accuracy:0.5758, Validation Loss:0.8824, Validation Accuracy:0.5419\n",
    "Epoch #169: Loss:0.8573, Accuracy:0.5663, Validation Loss:0.8858, Validation Accuracy:0.5337\n",
    "Epoch #170: Loss:0.8588, Accuracy:0.5680, Validation Loss:0.8786, Validation Accuracy:0.5484\n",
    "Epoch #171: Loss:0.8630, Accuracy:0.5676, Validation Loss:0.8819, Validation Accuracy:0.5402\n",
    "Epoch #172: Loss:0.8610, Accuracy:0.5655, Validation Loss:0.8859, Validation Accuracy:0.5320\n",
    "Epoch #173: Loss:0.8599, Accuracy:0.5626, Validation Loss:0.8841, Validation Accuracy:0.5353\n",
    "Epoch #174: Loss:0.8694, Accuracy:0.5647, Validation Loss:0.8729, Validation Accuracy:0.5402\n",
    "Epoch #175: Loss:0.8540, Accuracy:0.5766, Validation Loss:0.9081, Validation Accuracy:0.5435\n",
    "Epoch #176: Loss:0.8669, Accuracy:0.5639, Validation Loss:0.8724, Validation Accuracy:0.5435\n",
    "Epoch #177: Loss:0.8573, Accuracy:0.5684, Validation Loss:0.8899, Validation Accuracy:0.5304\n",
    "Epoch #178: Loss:0.8597, Accuracy:0.5639, Validation Loss:0.8715, Validation Accuracy:0.5386\n",
    "Epoch #179: Loss:0.8588, Accuracy:0.5704, Validation Loss:0.8759, Validation Accuracy:0.5320\n",
    "Epoch #180: Loss:0.8563, Accuracy:0.5713, Validation Loss:0.8958, Validation Accuracy:0.5435\n",
    "Epoch #181: Loss:0.8753, Accuracy:0.5614, Validation Loss:0.8759, Validation Accuracy:0.5402\n",
    "Epoch #182: Loss:0.8635, Accuracy:0.5663, Validation Loss:0.8993, Validation Accuracy:0.5287\n",
    "Epoch #183: Loss:0.8662, Accuracy:0.5598, Validation Loss:0.8704, Validation Accuracy:0.5402\n",
    "Epoch #184: Loss:0.8507, Accuracy:0.5721, Validation Loss:0.8754, Validation Accuracy:0.5287\n",
    "Epoch #185: Loss:0.8486, Accuracy:0.5680, Validation Loss:0.8707, Validation Accuracy:0.5452\n",
    "Epoch #186: Loss:0.8458, Accuracy:0.5733, Validation Loss:0.8716, Validation Accuracy:0.5452\n",
    "Epoch #187: Loss:0.8461, Accuracy:0.5708, Validation Loss:0.8720, Validation Accuracy:0.5369\n",
    "Epoch #188: Loss:0.8459, Accuracy:0.5684, Validation Loss:0.8726, Validation Accuracy:0.5353\n",
    "Epoch #189: Loss:0.8465, Accuracy:0.5737, Validation Loss:0.8821, Validation Accuracy:0.5369\n",
    "Epoch #190: Loss:0.8550, Accuracy:0.5745, Validation Loss:0.8759, Validation Accuracy:0.5419\n",
    "Epoch #191: Loss:0.8665, Accuracy:0.5606, Validation Loss:0.9284, Validation Accuracy:0.5435\n",
    "Epoch #192: Loss:0.8855, Accuracy:0.5446, Validation Loss:0.8865, Validation Accuracy:0.5320\n",
    "Epoch #193: Loss:0.8610, Accuracy:0.5630, Validation Loss:0.8792, Validation Accuracy:0.5304\n",
    "Epoch #194: Loss:0.8531, Accuracy:0.5680, Validation Loss:0.8748, Validation Accuracy:0.5386\n",
    "Epoch #195: Loss:0.8522, Accuracy:0.5729, Validation Loss:0.8798, Validation Accuracy:0.5369\n",
    "Epoch #196: Loss:0.8501, Accuracy:0.5659, Validation Loss:0.8866, Validation Accuracy:0.5337\n",
    "Epoch #197: Loss:0.8607, Accuracy:0.5618, Validation Loss:0.8868, Validation Accuracy:0.5320\n",
    "Epoch #198: Loss:0.8555, Accuracy:0.5717, Validation Loss:0.8917, Validation Accuracy:0.5435\n",
    "Epoch #199: Loss:0.8537, Accuracy:0.5667, Validation Loss:0.8693, Validation Accuracy:0.5337\n",
    "Epoch #200: Loss:0.8452, Accuracy:0.5762, Validation Loss:0.8749, Validation Accuracy:0.5337\n",
    "Epoch #201: Loss:0.8440, Accuracy:0.5745, Validation Loss:0.8712, Validation Accuracy:0.5419\n",
    "Epoch #202: Loss:0.8454, Accuracy:0.5762, Validation Loss:0.8865, Validation Accuracy:0.5435\n",
    "Epoch #203: Loss:0.8531, Accuracy:0.5692, Validation Loss:0.8703, Validation Accuracy:0.5337\n",
    "Epoch #204: Loss:0.8440, Accuracy:0.5717, Validation Loss:0.8821, Validation Accuracy:0.5353\n",
    "Epoch #205: Loss:0.8516, Accuracy:0.5700, Validation Loss:0.8736, Validation Accuracy:0.5320\n",
    "Epoch #206: Loss:0.8446, Accuracy:0.5684, Validation Loss:0.8787, Validation Accuracy:0.5402\n",
    "Epoch #207: Loss:0.8478, Accuracy:0.5708, Validation Loss:0.8766, Validation Accuracy:0.5402\n",
    "Epoch #208: Loss:0.8457, Accuracy:0.5741, Validation Loss:0.8720, Validation Accuracy:0.5353\n",
    "Epoch #209: Loss:0.8443, Accuracy:0.5721, Validation Loss:0.8745, Validation Accuracy:0.5353\n",
    "Epoch #210: Loss:0.8417, Accuracy:0.5737, Validation Loss:0.8712, Validation Accuracy:0.5386\n",
    "Epoch #211: Loss:0.8458, Accuracy:0.5733, Validation Loss:0.8698, Validation Accuracy:0.5369\n",
    "Epoch #212: Loss:0.8431, Accuracy:0.5737, Validation Loss:0.8898, Validation Accuracy:0.5435\n",
    "Epoch #213: Loss:0.8490, Accuracy:0.5717, Validation Loss:0.8704, Validation Accuracy:0.5419\n",
    "Epoch #214: Loss:0.8473, Accuracy:0.5696, Validation Loss:0.8717, Validation Accuracy:0.5320\n",
    "Epoch #215: Loss:0.8383, Accuracy:0.5737, Validation Loss:0.8703, Validation Accuracy:0.5304\n",
    "Epoch #216: Loss:0.8387, Accuracy:0.5733, Validation Loss:0.8713, Validation Accuracy:0.5353\n",
    "Epoch #217: Loss:0.8400, Accuracy:0.5774, Validation Loss:0.8702, Validation Accuracy:0.5337\n",
    "Epoch #218: Loss:0.8389, Accuracy:0.5745, Validation Loss:0.8717, Validation Accuracy:0.5386\n",
    "Epoch #219: Loss:0.8448, Accuracy:0.5708, Validation Loss:0.8814, Validation Accuracy:0.5419\n",
    "Epoch #220: Loss:0.8420, Accuracy:0.5754, Validation Loss:0.8716, Validation Accuracy:0.5320\n",
    "Epoch #221: Loss:0.8406, Accuracy:0.5721, Validation Loss:0.8710, Validation Accuracy:0.5304\n",
    "Epoch #222: Loss:0.8398, Accuracy:0.5774, Validation Loss:0.8687, Validation Accuracy:0.5304\n",
    "Epoch #223: Loss:0.8370, Accuracy:0.5770, Validation Loss:0.8692, Validation Accuracy:0.5304\n",
    "Epoch #224: Loss:0.8398, Accuracy:0.5729, Validation Loss:0.8721, Validation Accuracy:0.5353\n",
    "Epoch #225: Loss:0.8396, Accuracy:0.5737, Validation Loss:0.8750, Validation Accuracy:0.5287\n",
    "Epoch #226: Loss:0.8417, Accuracy:0.5696, Validation Loss:0.8879, Validation Accuracy:0.5484\n",
    "Epoch #227: Loss:0.8417, Accuracy:0.5770, Validation Loss:0.8733, Validation Accuracy:0.5320\n",
    "Epoch #228: Loss:0.8401, Accuracy:0.5762, Validation Loss:0.8702, Validation Accuracy:0.5304\n",
    "Epoch #229: Loss:0.8382, Accuracy:0.5749, Validation Loss:0.8706, Validation Accuracy:0.5353\n",
    "Epoch #230: Loss:0.8381, Accuracy:0.5758, Validation Loss:0.8727, Validation Accuracy:0.5320\n",
    "Epoch #231: Loss:0.8415, Accuracy:0.5713, Validation Loss:0.8710, Validation Accuracy:0.5320\n",
    "Epoch #232: Loss:0.8470, Accuracy:0.5704, Validation Loss:0.8903, Validation Accuracy:0.5320\n",
    "Epoch #233: Loss:0.8511, Accuracy:0.5676, Validation Loss:0.8765, Validation Accuracy:0.5222\n",
    "Epoch #234: Loss:0.8460, Accuracy:0.5733, Validation Loss:0.8668, Validation Accuracy:0.5287\n",
    "Epoch #235: Loss:0.8486, Accuracy:0.5667, Validation Loss:0.8663, Validation Accuracy:0.5255\n",
    "Epoch #236: Loss:0.8401, Accuracy:0.5749, Validation Loss:0.8803, Validation Accuracy:0.5419\n",
    "Epoch #237: Loss:0.8440, Accuracy:0.5680, Validation Loss:0.8787, Validation Accuracy:0.5419\n",
    "Epoch #238: Loss:0.8464, Accuracy:0.5708, Validation Loss:0.8907, Validation Accuracy:0.5255\n",
    "Epoch #239: Loss:0.8528, Accuracy:0.5655, Validation Loss:0.8734, Validation Accuracy:0.5238\n",
    "Epoch #240: Loss:0.8400, Accuracy:0.5770, Validation Loss:0.8688, Validation Accuracy:0.5484\n",
    "Epoch #241: Loss:0.8350, Accuracy:0.5786, Validation Loss:0.8669, Validation Accuracy:0.5337\n",
    "Epoch #242: Loss:0.8363, Accuracy:0.5762, Validation Loss:0.8684, Validation Accuracy:0.5287\n",
    "Epoch #243: Loss:0.8328, Accuracy:0.5770, Validation Loss:0.8676, Validation Accuracy:0.5238\n",
    "Epoch #244: Loss:0.8325, Accuracy:0.5836, Validation Loss:0.8702, Validation Accuracy:0.5402\n",
    "Epoch #245: Loss:0.8338, Accuracy:0.5799, Validation Loss:0.8714, Validation Accuracy:0.5402\n",
    "Epoch #246: Loss:0.8351, Accuracy:0.5770, Validation Loss:0.8735, Validation Accuracy:0.5320\n",
    "Epoch #247: Loss:0.8338, Accuracy:0.5782, Validation Loss:0.8680, Validation Accuracy:0.5320\n",
    "Epoch #248: Loss:0.8352, Accuracy:0.5819, Validation Loss:0.8753, Validation Accuracy:0.5287\n",
    "Epoch #249: Loss:0.8404, Accuracy:0.5770, Validation Loss:0.8760, Validation Accuracy:0.5337\n",
    "Epoch #250: Loss:0.8418, Accuracy:0.5754, Validation Loss:0.8669, Validation Accuracy:0.5320\n",
    "Epoch #251: Loss:0.8299, Accuracy:0.5795, Validation Loss:0.8771, Validation Accuracy:0.5419\n",
    "Epoch #252: Loss:0.8333, Accuracy:0.5795, Validation Loss:0.8668, Validation Accuracy:0.5320\n",
    "Epoch #253: Loss:0.8342, Accuracy:0.5782, Validation Loss:0.8675, Validation Accuracy:0.5320\n",
    "Epoch #254: Loss:0.8340, Accuracy:0.5803, Validation Loss:0.8667, Validation Accuracy:0.5320\n",
    "Epoch #255: Loss:0.8405, Accuracy:0.5774, Validation Loss:0.8995, Validation Accuracy:0.5238\n",
    "Epoch #256: Loss:0.8482, Accuracy:0.5696, Validation Loss:0.8635, Validation Accuracy:0.5435\n",
    "Epoch #257: Loss:0.8347, Accuracy:0.5774, Validation Loss:0.8695, Validation Accuracy:0.5501\n",
    "Epoch #258: Loss:0.8311, Accuracy:0.5807, Validation Loss:0.8647, Validation Accuracy:0.5337\n",
    "Epoch #259: Loss:0.8296, Accuracy:0.5799, Validation Loss:0.8644, Validation Accuracy:0.5287\n",
    "Epoch #260: Loss:0.8292, Accuracy:0.5762, Validation Loss:0.8653, Validation Accuracy:0.5386\n",
    "Epoch #261: Loss:0.8292, Accuracy:0.5803, Validation Loss:0.8699, Validation Accuracy:0.5484\n",
    "Epoch #262: Loss:0.8307, Accuracy:0.5840, Validation Loss:0.8713, Validation Accuracy:0.5452\n",
    "Epoch #263: Loss:0.8314, Accuracy:0.5791, Validation Loss:0.8695, Validation Accuracy:0.5304\n",
    "Epoch #264: Loss:0.8345, Accuracy:0.5786, Validation Loss:0.8885, Validation Accuracy:0.5205\n",
    "Epoch #265: Loss:0.8424, Accuracy:0.5803, Validation Loss:0.8644, Validation Accuracy:0.5386\n",
    "Epoch #266: Loss:0.8302, Accuracy:0.5803, Validation Loss:0.8637, Validation Accuracy:0.5320\n",
    "Epoch #267: Loss:0.8285, Accuracy:0.5832, Validation Loss:0.8671, Validation Accuracy:0.5419\n",
    "Epoch #268: Loss:0.8297, Accuracy:0.5815, Validation Loss:0.8659, Validation Accuracy:0.5304\n",
    "Epoch #269: Loss:0.8282, Accuracy:0.5786, Validation Loss:0.8680, Validation Accuracy:0.5468\n",
    "Epoch #270: Loss:0.8291, Accuracy:0.5815, Validation Loss:0.9019, Validation Accuracy:0.5402\n",
    "Epoch #271: Loss:0.8457, Accuracy:0.5725, Validation Loss:0.8772, Validation Accuracy:0.5484\n",
    "Epoch #272: Loss:0.8320, Accuracy:0.5803, Validation Loss:0.8635, Validation Accuracy:0.5304\n",
    "Epoch #273: Loss:0.8300, Accuracy:0.5803, Validation Loss:0.8640, Validation Accuracy:0.5337\n",
    "Epoch #274: Loss:0.8286, Accuracy:0.5807, Validation Loss:0.8659, Validation Accuracy:0.5255\n",
    "Epoch #275: Loss:0.8272, Accuracy:0.5811, Validation Loss:0.8628, Validation Accuracy:0.5287\n",
    "Epoch #276: Loss:0.8252, Accuracy:0.5840, Validation Loss:0.8633, Validation Accuracy:0.5435\n",
    "Epoch #277: Loss:0.8249, Accuracy:0.5864, Validation Loss:0.8631, Validation Accuracy:0.5337\n",
    "Epoch #278: Loss:0.8269, Accuracy:0.5811, Validation Loss:0.8633, Validation Accuracy:0.5402\n",
    "Epoch #279: Loss:0.8259, Accuracy:0.5869, Validation Loss:0.8639, Validation Accuracy:0.5402\n",
    "Epoch #280: Loss:0.8249, Accuracy:0.5823, Validation Loss:0.8699, Validation Accuracy:0.5452\n",
    "Epoch #281: Loss:0.8250, Accuracy:0.5852, Validation Loss:0.8640, Validation Accuracy:0.5304\n",
    "Epoch #282: Loss:0.8235, Accuracy:0.5897, Validation Loss:0.8637, Validation Accuracy:0.5435\n",
    "Epoch #283: Loss:0.8249, Accuracy:0.5856, Validation Loss:0.8640, Validation Accuracy:0.5320\n",
    "Epoch #284: Loss:0.8254, Accuracy:0.5848, Validation Loss:0.8648, Validation Accuracy:0.5304\n",
    "Epoch #285: Loss:0.8240, Accuracy:0.5799, Validation Loss:0.8653, Validation Accuracy:0.5304\n",
    "Epoch #286: Loss:0.8209, Accuracy:0.5906, Validation Loss:0.8927, Validation Accuracy:0.5452\n",
    "Epoch #287: Loss:0.8366, Accuracy:0.5799, Validation Loss:0.8652, Validation Accuracy:0.5419\n",
    "Epoch #288: Loss:0.8242, Accuracy:0.5819, Validation Loss:0.8654, Validation Accuracy:0.5386\n",
    "Epoch #289: Loss:0.8214, Accuracy:0.5869, Validation Loss:0.8634, Validation Accuracy:0.5353\n",
    "Epoch #290: Loss:0.8228, Accuracy:0.5860, Validation Loss:0.8626, Validation Accuracy:0.5452\n",
    "Epoch #291: Loss:0.8239, Accuracy:0.5852, Validation Loss:0.8716, Validation Accuracy:0.5517\n",
    "Epoch #292: Loss:0.8276, Accuracy:0.5782, Validation Loss:0.8656, Validation Accuracy:0.5419\n",
    "Epoch #293: Loss:0.8237, Accuracy:0.5864, Validation Loss:0.8673, Validation Accuracy:0.5435\n",
    "Epoch #294: Loss:0.8263, Accuracy:0.5877, Validation Loss:0.8667, Validation Accuracy:0.5369\n",
    "Epoch #295: Loss:0.8280, Accuracy:0.5754, Validation Loss:0.8630, Validation Accuracy:0.5320\n",
    "Epoch #296: Loss:0.8220, Accuracy:0.5828, Validation Loss:0.8654, Validation Accuracy:0.5452\n",
    "Epoch #297: Loss:0.8232, Accuracy:0.5823, Validation Loss:0.8653, Validation Accuracy:0.5435\n",
    "Epoch #298: Loss:0.8236, Accuracy:0.5828, Validation Loss:0.8625, Validation Accuracy:0.5452\n",
    "Epoch #299: Loss:0.8202, Accuracy:0.5877, Validation Loss:0.8602, Validation Accuracy:0.5386\n",
    "Epoch #300: Loss:0.8200, Accuracy:0.5864, Validation Loss:0.8657, Validation Accuracy:0.5435\n",
    "\n",
    "Test:\n",
    "Test Loss:0.86568350, Accuracy:0.5435\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02   03\n",
    "t:01  165  38   37\n",
    "t:02  147  63   17\n",
    "t:03   37   2  103\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.47      0.69      0.56       240\n",
    "          02       0.61      0.28      0.38       227\n",
    "          03       0.66      0.73      0.69       142\n",
    "\n",
    "    accuracy                           0.54       609\n",
    "   macro avg       0.58      0.56      0.54       609\n",
    "weighted avg       0.57      0.54      0.52       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 09:44:11 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 16 minutes, 4 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0795041751391783, 1.0743637789646392, 1.0755365429253414, 1.075851354888703, 1.0745422254837989, 1.0742717239461312, 1.0744538812214517, 1.0743940939456957, 1.0741758356344915, 1.0740353340781577, 1.0739707202942697, 1.0737655532771144, 1.0735057394688547, 1.0731786174335698, 1.072706001341245, 1.072055604657516, 1.0710913496847418, 1.0695208289548876, 1.0671292093195548, 1.0632590515468703, 1.057171720589323, 1.0481061892360692, 1.0354437732148445, 1.0222327255067372, 1.013681611991281, 1.012325581462904, 1.0121528336958736, 1.0116537241708665, 1.0130611806863243, 1.0106579954009534, 1.0101571576348667, 1.0098643244193692, 1.0085130352496319, 1.0088905309417173, 1.0070892562615656, 1.0070649561623635, 1.0058511111928128, 1.0044939512102475, 1.003306108742512, 1.0040713315722587, 1.0014586243136177, 1.0055801400605877, 1.0005090485261188, 1.0024814582223376, 0.9987818308064503, 0.9986354192880965, 0.9963134024139304, 0.9969641193380496, 0.9962760153270903, 0.9962282783683689, 0.9930041063399542, 0.9945303595124795, 0.9924586960443331, 0.9913798018629327, 0.9890343422568686, 0.988424378760734, 0.986804850583006, 0.9855322970936843, 0.9839453350734241, 0.9848048816173535, 0.986001715088517, 0.9795609644089622, 0.9791394458419975, 0.9802545110384623, 0.9834288660137133, 0.9938973965511729, 0.9787206915016049, 0.9724893066879172, 0.966835545005861, 0.9662771021399787, 0.9627701077359455, 0.9638633125129787, 0.9574520839454701, 0.9538463205343788, 0.9588553754762671, 0.9489672053819416, 0.9415148246072979, 0.9432166967485925, 0.9307063225846377, 0.9242502499879484, 0.9600758869659725, 0.9457548182390397, 0.9669598938013336, 0.910599069250824, 0.9459316705052293, 0.9177643441410096, 0.9163437231225138, 0.9116396131773887, 0.9248028545348319, 0.9061077519982124, 0.909227470459022, 0.9397214473957695, 0.9221318939636494, 0.9118506386753765, 0.9025795002680498, 0.9175815411976406, 0.8963300543661383, 0.9590837254704317, 0.9088517471487298, 0.9028155586402404, 0.9065191097838929, 0.8966831775330166, 0.8984919232492181, 0.8983737182343143, 0.894300664489101, 0.8974846131695903, 0.9119980642556753, 0.9062712319770274, 0.8941044168323523, 0.9097120185399487, 0.8900191371859784, 0.8911330375961091, 0.8920345635249697, 0.8920371148778105, 0.8977244449366489, 0.8894682342587238, 0.8930125322639453, 0.8874651054639143, 0.887315469030872, 0.8868315652477722, 0.8868328391624789, 0.8913550212465483, 0.8854732642620068, 0.8897645649651589, 0.8898808243630947, 0.8845839704003045, 0.8840430258530114, 0.8845274063753964, 0.8895556233786597, 0.8930311043469973, 0.8901789011234916, 0.8873284878010429, 0.8821992999422922, 0.8859165082815637, 0.8818526584135096, 0.8889190743513686, 0.9214924044405494, 0.9064511532462485, 0.879492159547477, 0.9028200393631345, 0.8817763791491441, 0.8905076383565643, 0.8784455809687158, 0.9029895838454047, 0.8882159607359537, 0.8904106382078725, 0.8853378131471831, 0.8788889105292572, 0.8893926043815801, 0.8770636553051828, 0.8871802329429852, 0.8782486529968838, 0.8884811881923519, 0.8792235749499943, 0.8795902657587148, 0.8885328549469633, 0.8774546390683781, 0.8768948562235276, 0.8832370624166405, 0.8820652736623104, 0.8781356564883528, 0.8805756716696891, 0.8786243589836584, 0.8867640963133137, 0.8772745344830656, 0.8814463881631985, 0.8810553757232202, 0.8823512329648086, 0.8857976618071495, 0.8785780777876404, 0.8819375267169746, 0.8858865572901194, 0.8840736831937518, 0.8728699041117588, 0.9080939236141387, 0.8723536773855463, 0.8899389404577184, 0.8714669415320473, 0.8758996457889162, 0.895848303690724, 0.8759268611326985, 0.8993196491341677, 0.8703716632180614, 0.875446554181611, 0.8706634840354543, 0.8716390621290222, 0.8719808052130325, 0.8726237759801555, 0.882131800862956, 0.8759335782531839, 0.9283583542004791, 0.8864633299055553, 0.8792444229713214, 0.8748257166059147, 0.8798008320366808, 0.8866346609612012, 0.8868497423154772, 0.8917011051929643, 0.8692787336598476, 0.8748873044508823, 0.8712175454216442, 0.8864543489830443, 0.8703063792000068, 0.882089966232162, 0.8735876512057675, 0.878681641396239, 0.8765730081716391, 0.8719851098624356, 0.8745479336121595, 0.8712396647151076, 0.8697834728386602, 0.8897857177629456, 0.8703987132543805, 0.8717024697263057, 0.8702754644533292, 0.8712528310777323, 0.8701778623858109, 0.8716540257331773, 0.8813871912572576, 0.8716372206489049, 0.8709861492288524, 0.8687065116486135, 0.8692442885369112, 0.8720904208952178, 0.8750406761083306, 0.8879294916131031, 0.8732624933833167, 0.8702319890798997, 0.8705888216327172, 0.8727120235439983, 0.8709961447026734, 0.8903433516107756, 0.8764793993217017, 0.866808032265242, 0.8663302860628013, 0.8803172667429756, 0.8787006591928417, 0.8906632204752641, 0.8734325165623319, 0.8687568294199425, 0.8668690806343442, 0.8683687116712185, 0.8675793420113562, 0.8702107302856759, 0.8713741376873699, 0.8734780593067163, 0.868042445633016, 0.8753440578778585, 0.8759683926508736, 0.8669022963747798, 0.8771252075243857, 0.8667751661270906, 0.8675232968111147, 0.8667337619416624, 0.8995263846636993, 0.8635376294453939, 0.8695286573056126, 0.8646756015191917, 0.8644285149174958, 0.8652986153984696, 0.8698518396598365, 0.8712737139418403, 0.86951455429857, 0.8884858334397252, 0.8644375520190973, 0.8637032218167348, 0.8670527808771932, 0.8659173287390096, 0.8680273966836225, 0.9018507212253627, 0.8772490941636473, 0.863459518879701, 0.8639682093864591, 0.8658872606914815, 0.8627781580234396, 0.8633043013378513, 0.8631111202373097, 0.8632907065851935, 0.8639123115829255, 0.869853865434775, 0.8640451765021275, 0.8636804303316451, 0.8639923980083372, 0.8648022151932928, 0.865320564099329, 0.8927183440949138, 0.8652003135391448, 0.8654407708124182, 0.8634145551518658, 0.8625674553105397, 0.8716348833442713, 0.8655708808029814, 0.8672599206221319, 0.8667279157928254, 0.862972788035576, 0.8653541125100235, 0.8653388188781801, 0.8625322459953759, 0.8602278625045112, 0.8656835111686945], 'val_acc': [0.3727421997901058, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.40229884954704637, 0.40886699404622534, 0.4187192108928668, 0.44827586133491815, 0.44170771663999325, 0.4646962226807386, 0.46141215052902207, 0.4729064035004583, 0.4761904756521748, 0.4761904757500478, 0.4761904756521748, 0.474548439723126, 0.4712643674735365, 0.46141215062689506, 0.47783251177696956, 0.46798029512607403, 0.46469622277861156, 0.4794745480975103, 0.46469622277861156, 0.46798029512607403, 0.46469622277861156, 0.4794745479996373, 0.487684728623611, 0.4909688009710735, 0.4909688009710735, 0.48604269279243517, 0.48440065656976744, 0.5041050902630504, 0.4958949094433307, 0.487684728721484, 0.49425287331853596, 0.487684728819357, 0.4958949093454577, 0.4975369452745065, 0.4958949093454577, 0.4909688008732005, 0.4926108371937412, 0.5041050899694315, 0.5024630539425097, 0.5024630540403826, 0.5041050901651773, 0.5057471263878451, 0.5073891624147668, 0.5123152662380576, 0.5139573068160729, 0.5057471263878451, 0.5057471262899721, 0.5172413792614083, 0.5073891624147668, 0.5024630539425097, 0.5221674875379196, 0.5057471261920992, 0.5041050900673044, 0.5238095193073667, 0.515599338487647, 0.5254515554321615, 0.528735627681751, 0.5270935915569562, 0.528735627681751, 0.5320196998334675, 0.528735627583878, 0.5435139530006496, 0.5369458082078518, 0.5336617358603892, 0.53530377218093, 0.5303776679661474, 0.5205254468620313, 0.5090311984416886, 0.5385878443326465, 0.5188834151904571, 0.5353037764384045, 0.5550082054827209, 0.5402298803595682, 0.5303776680640203, 0.5303776682597663, 0.5435139569645053, 0.535303772083057, 0.5238095235648413, 0.5270935957165579, 0.5369458125631993, 0.5402298803595682, 0.5550082054827209, 0.5090311982459427, 0.5270935959123039, 0.5303776680640203, 0.5336617402157369, 0.5467980294098408, 0.5500821017573032, 0.5418719208397106, 0.5500821017573032, 0.5500821016594303, 0.551724137882098, 0.551724137882098, 0.5320197040909421, 0.5369458125631993, 0.5500820971083367, 0.551724137784225, 0.5550082101316874, 0.5500821016594303, 0.5402298848127888, 0.5484400655346355, 0.545155993187173, 0.5533661739090198, 0.5500821017573032, 0.5500821017573032, 0.5484400656325085, 0.5402298850085347, 0.5500821015615572, 0.5402298850085347, 0.5435139571602513, 0.551724137784225, 0.5484400655346355, 0.545155993187173, 0.5402298850085347, 0.5369458126610723, 0.5435139572581242, 0.545155993382919, 0.5435139570623783, 0.5500821015615572, 0.545155993187173, 0.551724137686352, 0.5303776681618934, 0.5467980294098408, 0.551724137588479, 0.5353037765362776, 0.5484400656325085, 0.5402298850085347, 0.5418719209375835, 0.5418719209375835, 0.5435139571602513, 0.5287356321349715, 0.5402298850085347, 0.5451559930893001, 0.5467980292140948, 0.5467980292140948, 0.5402298849106618, 0.5467980292140948, 0.5402298848127888, 0.5418719208397106, 0.5418719209375835, 0.5287356321349715, 0.5418719210354566, 0.5467980293119677, 0.5402298846170428, 0.5336617402157369, 0.5451559930893001, 0.5385878485901211, 0.5451559930893001, 0.5336617404114828, 0.5418719208397106, 0.5320197040909421, 0.5418719207418377, 0.5418719208397106, 0.5336617402157369, 0.5484400654367625, 0.5402298848127888, 0.5320197042866881, 0.5353037766341505, 0.5402298846170428, 0.5435139571602513, 0.5435139569645053, 0.5303776681618934, 0.538587848687994, 0.5320197039930691, 0.5435139570623783, 0.5402298847149158, 0.5287356319392256, 0.5402298847149158, 0.5287356317434796, 0.5451559929914271, 0.5451559929914271, 0.5369458125631993, 0.5353037764384045, 0.5369458125631993, 0.5418719207418377, 0.5435139571602513, 0.5320197042866881, 0.5303776680640203, 0.5385878484922481, 0.5369458123674533, 0.5336617405093558, 0.5320197041888151, 0.5435139570623783, 0.5336617400199909, 0.5336617403136098, 0.5418719207418377, 0.5435139569645053, 0.5336617403136098, 0.5353037764384045, 0.5320197038951961, 0.5402298846170428, 0.5402298847149158, 0.5353037764384045, 0.5353037763405316, 0.5385878483943751, 0.5369458123674533, 0.5435139569645053, 0.5418719208397106, 0.5320197038951961, 0.5303776677704014, 0.5353037762426586, 0.5336617400199909, 0.5385878484922481, 0.5418719207418377, 0.5320197038951961, 0.5303776677704014, 0.5303776679661474, 0.5303776678682743, 0.5353037762426586, 0.5287356315477337, 0.5484400653388896, 0.5320197040909421, 0.5303776677704014, 0.5353037760469127, 0.5320197039930691, 0.5320197038951961, 0.5320197041888151, 0.5221674873421737, 0.5287356317434796, 0.5254515594938901, 0.5418719207418377, 0.5418719207418377, 0.5254515595917632, 0.5238095234669684, 0.5484400653388896, 0.533661739922118, 0.5287356315477337, 0.5238095231733494, 0.5402298846170428, 0.5402298846170428, 0.5320197039930691, 0.5320197039930691, 0.5287356317434796, 0.5336617402157369, 0.5320197038951961, 0.5418719207418377, 0.5320197039930691, 0.5320197040909421, 0.5320197037973232, 0.5238095234669684, 0.5435139569645053, 0.5500821012679383, 0.5336617401178638, 0.5287356315477337, 0.5385878484922481, 0.5484400653388896, 0.5451559929914271, 0.5303776677704014, 0.5205254512173789, 0.5385878484922481, 0.5320197039930691, 0.5418719207418377, 0.5303776676725285, 0.5467980291162219, 0.5402298848127888, 0.5484400652410166, 0.5303776676725285, 0.5336617400199909, 0.5254515593960172, 0.5287356315477337, 0.5435139568666324, 0.5336617401178638, 0.5402298846170428, 0.5402298845191699, 0.5451559928935541, 0.5303776677704014, 0.5435139568666324, 0.5320197038951961, 0.5303776677704014, 0.5303776677704014, 0.545155993187173, 0.5418719206439646, 0.5385878484922481, 0.5353037763405316, 0.5451559930893001, 0.5517241374906061, 0.5418719207418377, 0.5435139570623783, 0.5369458124653264, 0.5320197037973232, 0.5451559930893001, 0.5435139569645053, 0.5451559929914271, 0.5385878483943751, 0.5435139567687594], 'loss': [1.089214042422708, 1.0762432877287973, 1.0743506825680116, 1.0757922886333426, 1.0751731318370028, 1.0741525126188933, 1.0742104420671719, 1.0743710919327314, 1.0741860068064695, 1.074000336893775, 1.0739186364277677, 1.0738867288742222, 1.0736778005192658, 1.0734033157693288, 1.0731289759308895, 1.0727461568139172, 1.0720108809657165, 1.0709302268233878, 1.0695038900727853, 1.066778367990341, 1.0629306925885242, 1.05719274245738, 1.0482318013110934, 1.0373765510700077, 1.0269979429930387, 1.0238484705253303, 1.0239087843552261, 1.0222204561351016, 1.019858608010858, 1.0186270772064492, 1.0202946980141516, 1.0194490584015112, 1.0192982008814566, 1.0176362836140627, 1.015346197009821, 1.0138319276931105, 1.0128061075474937, 1.0119618620226272, 1.0108293250356122, 1.010228823832173, 1.0098193707162595, 1.0118922473468819, 1.0099071429250666, 1.0064133236295634, 1.0054039538029038, 1.0034060708551191, 1.0024021881072183, 1.001447925479505, 1.0047563854184238, 1.0033062673447313, 0.9988367814547717, 0.9983980622869253, 0.9966515227264937, 0.9971635420464392, 0.9939847536400358, 0.9927871295803626, 0.990874891237067, 0.9900183241225367, 0.9883314422513425, 0.9874782343419916, 0.9875572590612532, 0.9857765917905302, 0.9815992155114238, 0.9829256088092342, 0.9813851196907873, 0.9851872283085661, 0.986483480749189, 0.9810106741574266, 0.973314893417045, 0.9689501653951297, 0.9677985496344752, 0.9638759128856463, 0.9647081968475906, 0.9604672038579624, 0.9564212724658253, 0.9504815285700302, 0.9453411204369406, 0.9381585073911679, 0.9370369824051122, 0.9272726343397731, 0.9192160611524719, 0.9303137432133637, 0.9492728250227425, 0.955621448385642, 0.9433160466335148, 0.9322422171520257, 0.9143045830041231, 0.9154000093559954, 0.9167349213692197, 0.9124412593410735, 0.9070000431131289, 0.9088644311658166, 0.9202967231768112, 0.9152804372002212, 0.9032979886389856, 0.8998934533072204, 0.9082274904486091, 0.9157892273681609, 0.9185144846199474, 0.9027577981077425, 0.8948513236623525, 0.8913841902842512, 0.886454837180261, 0.8879936572952192, 0.8876227342127775, 0.8826569161620718, 0.8893520560352709, 0.8950808810012786, 0.8901164745403266, 0.881689241069543, 0.8830444421121961, 0.8778991581233375, 0.8804316392179877, 0.8797678741586282, 0.875599250607422, 0.8780315536248365, 0.876215866042848, 0.8759327095392059, 0.8749626241915035, 0.8704172035017542, 0.8697265203239002, 0.8725789366316746, 0.8691818873250754, 0.8692201184296265, 0.8717137035158381, 0.8707505628803183, 0.8657339391767122, 0.8664284870854638, 0.8687477143882971, 0.8700401491208243, 0.8702374768942533, 0.8683787662880131, 0.8663164505234, 0.8620152905002022, 0.8658047677310341, 0.8660288000008899, 0.8740555739011119, 0.8932333199640075, 0.8794716696474831, 0.8664655684935239, 0.8747441051921805, 0.8631340506140456, 0.8668625062985587, 0.8667209509217029, 0.8709368392182572, 0.8712896648863258, 0.8636751492899791, 0.866395569582005, 0.858085275357264, 0.8624631189957291, 0.8556209912290319, 0.8599340675303089, 0.8581002037627986, 0.8574654379664506, 0.8606008685834599, 0.8639999629290931, 0.8586651458877312, 0.8555640591243454, 0.8547782016485869, 0.8558473062466302, 0.8568699420599967, 0.8561659329236165, 0.8538582361454347, 0.8603374965381818, 0.8595033853205812, 0.8511116536246188, 0.8542175419276745, 0.8532949810400147, 0.8573496389682778, 0.8588279181682109, 0.8630195873229166, 0.8609833416507964, 0.8598620446066102, 0.8693953926068801, 0.8539919602552724, 0.8669099503474069, 0.8573255601360078, 0.8596784370880597, 0.8587521300423562, 0.8563397315493354, 0.8753429014579961, 0.8634849464868862, 0.8662455817267636, 0.8506738832109518, 0.8486022219765602, 0.8458167890258883, 0.8461293964170578, 0.8458699659400407, 0.846461488383017, 0.85501957101254, 0.8665265900153644, 0.8855135346584986, 0.8610441568206223, 0.8530946212627559, 0.8521643310356923, 0.8500906989314962, 0.8606549613774435, 0.8554675055480346, 0.8537073626410545, 0.8452297427815824, 0.8440400367889561, 0.8453938770343146, 0.8530721658798703, 0.8439845060665749, 0.8515696533406785, 0.844587947578156, 0.8478160802833353, 0.8457317456817235, 0.8442976280649095, 0.8417452995047677, 0.8458092257717063, 0.8431339123165827, 0.8490301530219202, 0.8473070880715607, 0.8382588180428413, 0.8387231828250924, 0.8399524350430686, 0.8389304056059899, 0.8447877488341909, 0.842005875198748, 0.8406245458542199, 0.839792943123185, 0.8370306453420886, 0.8397751362171996, 0.8396459231141656, 0.8416625223854973, 0.8416812222841095, 0.840139525382181, 0.8382107513151619, 0.8381210878154826, 0.8415360743260236, 0.8470341048691062, 0.8511081681114447, 0.8459694598244936, 0.8486242051242069, 0.8401322314626627, 0.8440446755724521, 0.8463875672900456, 0.8527662791510627, 0.840033559632742, 0.8349694953562053, 0.8363051766487607, 0.8328256495679428, 0.8324859797587385, 0.8338207964534877, 0.8351266301632907, 0.8337651337196694, 0.8351660336312328, 0.8403651391211476, 0.8418210974464182, 0.8299071296039793, 0.8333093023153301, 0.8341974968293364, 0.833964459289026, 0.8404969513783465, 0.8482343152563185, 0.8346885480430337, 0.8310984523634156, 0.8296206643693991, 0.8292257091347931, 0.8291757910892948, 0.8307376827547438, 0.831441003610466, 0.8345435567215482, 0.842365646142, 0.8301508649418731, 0.8284721527745836, 0.8296574232269851, 0.8281647327254685, 0.8291154912609828, 0.8456823830976623, 0.8320199168438295, 0.8300391259134673, 0.8285801192573453, 0.8271693066405075, 0.8252495269266242, 0.8249366437629997, 0.8268930576420418, 0.8258570474276062, 0.8248946648114026, 0.8249725377045618, 0.8234912831680485, 0.8249217030204052, 0.8253676273005209, 0.8240046550850604, 0.8209092389386782, 0.8366217725575582, 0.8242425527415971, 0.8213942880503206, 0.8227703775468548, 0.8239062438510527, 0.8275730773653582, 0.8236569125794287, 0.8263171396705894, 0.8279651098427586, 0.8219955410555893, 0.823174418071457, 0.8236023077240225, 0.8202008174430173, 0.8200020649839476], 'acc': [0.37289527581212945, 0.38603695974947244, 0.39425051351837065, 0.3942505153175252, 0.39425051508498143, 0.394250513714197, 0.3942505113275634, 0.3942505113275634, 0.3942505156724605, 0.3942505141058497, 0.3942505148891551, 0.3942505128941742, 0.39425051547663414, 0.39425051273506523, 0.394250513714197, 0.39589322179984265, 0.4020533862667162, 0.4127310051198368, 0.43244353094629684, 0.44188911717775176, 0.4373716629873311, 0.46570842117989086, 0.472689936538007, 0.47392196978386913, 0.4739219735412872, 0.4735112945524329, 0.4702258710254144, 0.4759753572867392, 0.472689939120467, 0.4755646828018909, 0.4767967131103578, 0.4735112935733012, 0.4772073917075594, 0.4739219735412872, 0.4813141665174731, 0.4837782326175447, 0.4854209440689557, 0.49363449878026816, 0.49322381881228217, 0.495687887849749, 0.4924024631844898, 0.4878850087982428, 0.48993840005853095, 0.5030800822334368, 0.5022587276214936, 0.5051334736528338, 0.5071868582183086, 0.5043121188083468, 0.4993839852374192, 0.4940451760066853, 0.5014373715653312, 0.5051334695404804, 0.5059548245807938, 0.5039014392320136, 0.5088295681031088, 0.508418891072518, 0.5129363478086812, 0.5096509233392484, 0.5104722783795618, 0.5129363462420704, 0.5112936357697911, 0.5088295673198034, 0.5178644760922975, 0.5088295706488514, 0.5154004119504894, 0.5071868586099613, 0.5092402488544002, 0.5137576997157729, 0.5186858297618263, 0.5207392200062652, 0.5236139623536221, 0.5268993864314022, 0.52032854160489, 0.5264887097924642, 0.5310061554889169, 0.5318275190231981, 0.5371663257081896, 0.5375770027387803, 0.537166326491495, 0.5392197141901913, 0.5425051301656563, 0.5383983614997943, 0.5244353143831053, 0.5059548247766201, 0.5154004148878846, 0.5310061600663579, 0.5482546219835536, 0.5420944584958117, 0.5445585261624941, 0.5462012342848572, 0.54825461701446, 0.5486652934575718, 0.5334702225681203, 0.5433264911541948, 0.5453798808111547, 0.5552361430573513, 0.5433264915458476, 0.5355236185649582, 0.5437371662265221, 0.5482546202211165, 0.551129366876653, 0.55195072172114, 0.5568788477772315, 0.5605749461440335, 0.5568788531379778, 0.560164268134311, 0.5552361405116087, 0.5535934335642038, 0.5552361355425152, 0.561806979585722, 0.5548254577286189, 0.5663244395285416, 0.564271042552572, 0.5618069812257677, 0.5696098521749586, 0.5626283392034762, 0.5634496879039115, 0.5696098527624377, 0.5687885050411342, 0.5733059584482495, 0.5679671503924736, 0.5716632487592755, 0.5626283346260352, 0.5757700180126166, 0.5659137536123303, 0.5679671424859848, 0.5708418860082999, 0.5716632466051857, 0.5691991812884195, 0.5737166303139203, 0.5696098537415695, 0.5659137559622465, 0.569609852370785, 0.5749486610140399, 0.5737166366537984, 0.5728952735111699, 0.5601642748658417, 0.5507186837020106, 0.5613963079158775, 0.5708418923481778, 0.5646817270979989, 0.568788503278697, 0.5671457860748871, 0.5696098551123538, 0.5626283332552509, 0.5638603718618592, 0.5696098527624377, 0.569199175731847, 0.5724845962847528, 0.5708418869874315, 0.5704312155134135, 0.570841887770737, 0.5716632414402658, 0.5716632487592755, 0.5733059526958505, 0.5626283322761191, 0.5630390114607997, 0.5716632402653077, 0.5737166297264412, 0.5716632457484455, 0.5613963077200511, 0.5675564646720886, 0.5737166293347885, 0.5687884992887352, 0.5634496880997377, 0.5691991739694098, 0.5675564644762623, 0.5757700251358001, 0.5663244351469271, 0.5679671476509047, 0.5675564710119666, 0.5655030761900869, 0.5626283324719454, 0.5646817207581208, 0.5765913795886344, 0.5638603684593765, 0.5683778274230644, 0.5638603657178076, 0.5704312070194456, 0.5712525655846331, 0.5613963083075302, 0.5663244308387474, 0.5597535966602929, 0.5720739252023873, 0.5679671434651166, 0.573305950737587, 0.5708418856166472, 0.5683778256606272, 0.5737166283556568, 0.5745379832001437, 0.5605749443815964, 0.5445585228334462, 0.5630390122441051, 0.5679671438567693, 0.5728952748819544, 0.565913753416504, 0.561806984750642, 0.5716632424193976, 0.566735115188348, 0.576180694259902, 0.5745379830043174, 0.57618069758895, 0.5691991763193259, 0.5716632489551019, 0.570020531751292, 0.5683778256606272, 0.5708418925440042, 0.5741273136843891, 0.5720739180792039, 0.5737166287473096, 0.5733059544582876, 0.5737166299222676, 0.5716632426152239, 0.5696098515874796, 0.5737166289431358, 0.5733059580565968, 0.5774127265266324, 0.5745379920857643, 0.5708418869874315, 0.5753593447761614, 0.572073925006561, 0.5774127280932433, 0.5770020518459579, 0.5728952814176587, 0.5737166293347885, 0.5696098598856211, 0.5770020514543052, 0.5761807001346925, 0.5749486614056926, 0.5757700184042693, 0.5712525640180224, 0.5704312159050662, 0.5675564681969629, 0.5733059503459343, 0.5667351084568173, 0.5749486675497443, 0.5679671418985057, 0.5708418921523515, 0.5655030795191348, 0.5770020512584788, 0.5786447619265844, 0.5761806940640757, 0.5770020496918681, 0.5835728933434222, 0.5798767987707557, 0.5770020570108778, 0.5782340904525662, 0.5819301894068473, 0.5770020516501315, 0.5753593435277684, 0.5794661152044606, 0.5794661235026021, 0.5782340827419038, 0.5802874722030373, 0.577412735412253, 0.5696098535457431, 0.5774127280932433, 0.5806981482544964, 0.5798767945849675, 0.5761806950432075, 0.5802874718113846, 0.5839835688074022, 0.5790554460803586, 0.5786447674831571, 0.5802874723988637, 0.5802874706364265, 0.5831622170961368, 0.5815195112012984, 0.5786447607516263, 0.5815195027073305, 0.5724846041912416, 0.580287478347089, 0.5802874702447739, 0.5806981557693325, 0.5811088331915759, 0.5839835747556276, 0.5864476426181362, 0.5811088274391769, 0.5868583176170287, 0.5823408579434702, 0.5852156030323961, 0.5897330560478586, 0.5856262800629869, 0.5848049248268472, 0.5798767934100094, 0.5905544126547827, 0.5798767941933148, 0.5819301822836639, 0.5868583115464119, 0.586036957289404, 0.5852156030323961, 0.5782340862667781, 0.586447637844869, 0.5876796671742042, 0.5753593407861995, 0.5827515361490191, 0.5823408650666536, 0.5827515371281509, 0.587679675668172, 0.5864476358866055]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
