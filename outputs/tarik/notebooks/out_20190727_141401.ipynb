{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf42.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 14:14:01 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '03', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001A600D89550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001A661707EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0861, Accuracy:0.3943, Validation Loss:1.0791, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0762, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0753, Accuracy:0.3943, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0723, Accuracy:0.3943, Validation Loss:1.0717, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0706, Validation Accuracy:0.4072\n",
    "Epoch #12: Loss:1.0705, Accuracy:0.4082, Validation Loss:1.0690, Validation Accuracy:0.4401\n",
    "Epoch #13: Loss:1.0687, Accuracy:0.4185, Validation Loss:1.0665, Validation Accuracy:0.4269\n",
    "Epoch #14: Loss:1.0660, Accuracy:0.4209, Validation Loss:1.0628, Validation Accuracy:0.4335\n",
    "Epoch #15: Loss:1.0616, Accuracy:0.4218, Validation Loss:1.0567, Validation Accuracy:0.4516\n",
    "Epoch #16: Loss:1.0547, Accuracy:0.4287, Validation Loss:1.0468, Validation Accuracy:0.4631\n",
    "Epoch #17: Loss:1.0433, Accuracy:0.4333, Validation Loss:1.0309, Validation Accuracy:0.4614\n",
    "Epoch #18: Loss:1.0243, Accuracy:0.4452, Validation Loss:1.0055, Validation Accuracy:0.4680\n",
    "Epoch #19: Loss:0.9948, Accuracy:0.4517, Validation Loss:0.9681, Validation Accuracy:0.4778\n",
    "Epoch #20: Loss:0.9481, Accuracy:0.4752, Validation Loss:0.9078, Validation Accuracy:0.6043\n",
    "Epoch #21: Loss:0.8785, Accuracy:0.6189, Validation Loss:0.8192, Validation Accuracy:0.6568\n",
    "Epoch #22: Loss:0.7888, Accuracy:0.6727, Validation Loss:0.7271, Validation Accuracy:0.7011\n",
    "Epoch #23: Loss:0.7133, Accuracy:0.6891, Validation Loss:0.6641, Validation Accuracy:0.7225\n",
    "Epoch #24: Loss:0.6577, Accuracy:0.7133, Validation Loss:0.6310, Validation Accuracy:0.7209\n",
    "Epoch #25: Loss:0.6208, Accuracy:0.7097, Validation Loss:0.5869, Validation Accuracy:0.7389\n",
    "Epoch #26: Loss:0.5907, Accuracy:0.7298, Validation Loss:0.5723, Validation Accuracy:0.7488\n",
    "Epoch #27: Loss:0.5535, Accuracy:0.7466, Validation Loss:0.5393, Validation Accuracy:0.7619\n",
    "Epoch #28: Loss:0.5308, Accuracy:0.7585, Validation Loss:0.5265, Validation Accuracy:0.7668\n",
    "Epoch #29: Loss:0.5065, Accuracy:0.7708, Validation Loss:0.5074, Validation Accuracy:0.7734\n",
    "Epoch #30: Loss:0.4933, Accuracy:0.7782, Validation Loss:0.4969, Validation Accuracy:0.7865\n",
    "Epoch #31: Loss:0.4811, Accuracy:0.7901, Validation Loss:0.4747, Validation Accuracy:0.8030\n",
    "Epoch #32: Loss:0.4526, Accuracy:0.8037, Validation Loss:0.4530, Validation Accuracy:0.8161\n",
    "Epoch #33: Loss:0.4278, Accuracy:0.8131, Validation Loss:0.4314, Validation Accuracy:0.8342\n",
    "Epoch #34: Loss:0.4136, Accuracy:0.8234, Validation Loss:0.4326, Validation Accuracy:0.8276\n",
    "Epoch #35: Loss:0.4043, Accuracy:0.8263, Validation Loss:0.4091, Validation Accuracy:0.8391\n",
    "Epoch #36: Loss:0.3892, Accuracy:0.8378, Validation Loss:0.3938, Validation Accuracy:0.8407\n",
    "Epoch #37: Loss:0.3767, Accuracy:0.8476, Validation Loss:0.3744, Validation Accuracy:0.8539\n",
    "Epoch #38: Loss:0.3687, Accuracy:0.8485, Validation Loss:0.3630, Validation Accuracy:0.8456\n",
    "Epoch #39: Loss:0.3552, Accuracy:0.8530, Validation Loss:0.3563, Validation Accuracy:0.8440\n",
    "Epoch #40: Loss:0.3612, Accuracy:0.8559, Validation Loss:0.3496, Validation Accuracy:0.8621\n",
    "Epoch #41: Loss:0.3474, Accuracy:0.8600, Validation Loss:0.3374, Validation Accuracy:0.8588\n",
    "Epoch #42: Loss:0.3331, Accuracy:0.8665, Validation Loss:0.3321, Validation Accuracy:0.8686\n",
    "Epoch #43: Loss:0.3277, Accuracy:0.8694, Validation Loss:0.3518, Validation Accuracy:0.8555\n",
    "Epoch #44: Loss:0.3272, Accuracy:0.8686, Validation Loss:0.3349, Validation Accuracy:0.8604\n",
    "Epoch #45: Loss:0.3145, Accuracy:0.8772, Validation Loss:0.3103, Validation Accuracy:0.8686\n",
    "Epoch #46: Loss:0.3021, Accuracy:0.8813, Validation Loss:0.2992, Validation Accuracy:0.8736\n",
    "Epoch #47: Loss:0.2943, Accuracy:0.8858, Validation Loss:0.2963, Validation Accuracy:0.8818\n",
    "Epoch #48: Loss:0.2918, Accuracy:0.8846, Validation Loss:0.2982, Validation Accuracy:0.8768\n",
    "Epoch #49: Loss:0.2938, Accuracy:0.8825, Validation Loss:0.2867, Validation Accuracy:0.8801\n",
    "Epoch #50: Loss:0.2878, Accuracy:0.8891, Validation Loss:0.3181, Validation Accuracy:0.8654\n",
    "Epoch #51: Loss:0.2938, Accuracy:0.8825, Validation Loss:0.3373, Validation Accuracy:0.8637\n",
    "Epoch #52: Loss:0.3064, Accuracy:0.8809, Validation Loss:0.3341, Validation Accuracy:0.8473\n",
    "Epoch #53: Loss:0.2878, Accuracy:0.8858, Validation Loss:0.2650, Validation Accuracy:0.8933\n",
    "Epoch #54: Loss:0.2621, Accuracy:0.8990, Validation Loss:0.2583, Validation Accuracy:0.9048\n",
    "Epoch #55: Loss:0.2578, Accuracy:0.9010, Validation Loss:0.2589, Validation Accuracy:0.9031\n",
    "Epoch #56: Loss:0.2524, Accuracy:0.9051, Validation Loss:0.2594, Validation Accuracy:0.9064\n",
    "Epoch #57: Loss:0.2451, Accuracy:0.9072, Validation Loss:0.2408, Validation Accuracy:0.9048\n",
    "Epoch #58: Loss:0.2408, Accuracy:0.9105, Validation Loss:0.2352, Validation Accuracy:0.9080\n",
    "Epoch #59: Loss:0.2326, Accuracy:0.9125, Validation Loss:0.2408, Validation Accuracy:0.9080\n",
    "Epoch #60: Loss:0.2340, Accuracy:0.9109, Validation Loss:0.2434, Validation Accuracy:0.9048\n",
    "Epoch #61: Loss:0.2343, Accuracy:0.9084, Validation Loss:0.2503, Validation Accuracy:0.8966\n",
    "Epoch #62: Loss:0.2311, Accuracy:0.9129, Validation Loss:0.2271, Validation Accuracy:0.9130\n",
    "Epoch #63: Loss:0.2272, Accuracy:0.9138, Validation Loss:0.2280, Validation Accuracy:0.9097\n",
    "Epoch #64: Loss:0.2237, Accuracy:0.9138, Validation Loss:0.2443, Validation Accuracy:0.8966\n",
    "Epoch #65: Loss:0.2224, Accuracy:0.9150, Validation Loss:0.2083, Validation Accuracy:0.9195\n",
    "Epoch #66: Loss:0.2085, Accuracy:0.9228, Validation Loss:0.1953, Validation Accuracy:0.9278\n",
    "Epoch #67: Loss:0.1988, Accuracy:0.9253, Validation Loss:0.2068, Validation Accuracy:0.9195\n",
    "Epoch #68: Loss:0.2019, Accuracy:0.9261, Validation Loss:0.2102, Validation Accuracy:0.9212\n",
    "Epoch #69: Loss:0.1979, Accuracy:0.9248, Validation Loss:0.2170, Validation Accuracy:0.9146\n",
    "Epoch #70: Loss:0.2049, Accuracy:0.9203, Validation Loss:0.2295, Validation Accuracy:0.9031\n",
    "Epoch #71: Loss:0.2050, Accuracy:0.9228, Validation Loss:0.2310, Validation Accuracy:0.8966\n",
    "Epoch #72: Loss:0.2022, Accuracy:0.9232, Validation Loss:0.1787, Validation Accuracy:0.9343\n",
    "Epoch #73: Loss:0.1793, Accuracy:0.9363, Validation Loss:0.1781, Validation Accuracy:0.9343\n",
    "Epoch #74: Loss:0.1793, Accuracy:0.9318, Validation Loss:0.1751, Validation Accuracy:0.9392\n",
    "Epoch #75: Loss:0.1763, Accuracy:0.9347, Validation Loss:0.1715, Validation Accuracy:0.9294\n",
    "Epoch #76: Loss:0.1705, Accuracy:0.9409, Validation Loss:0.1684, Validation Accuracy:0.9360\n",
    "Epoch #77: Loss:0.1692, Accuracy:0.9429, Validation Loss:0.1677, Validation Accuracy:0.9343\n",
    "Epoch #78: Loss:0.1741, Accuracy:0.9372, Validation Loss:0.1663, Validation Accuracy:0.9360\n",
    "Epoch #79: Loss:0.1650, Accuracy:0.9417, Validation Loss:0.1652, Validation Accuracy:0.9409\n",
    "Epoch #80: Loss:0.1690, Accuracy:0.9376, Validation Loss:0.1800, Validation Accuracy:0.9442\n",
    "Epoch #81: Loss:0.1723, Accuracy:0.9359, Validation Loss:0.1660, Validation Accuracy:0.9442\n",
    "Epoch #82: Loss:0.1606, Accuracy:0.9421, Validation Loss:0.1672, Validation Accuracy:0.9343\n",
    "Epoch #83: Loss:0.1668, Accuracy:0.9388, Validation Loss:0.1843, Validation Accuracy:0.9278\n",
    "Epoch #84: Loss:0.1690, Accuracy:0.9355, Validation Loss:0.1714, Validation Accuracy:0.9343\n",
    "Epoch #85: Loss:0.1646, Accuracy:0.9405, Validation Loss:0.1940, Validation Accuracy:0.9195\n",
    "Epoch #86: Loss:0.1891, Accuracy:0.9273, Validation Loss:0.1858, Validation Accuracy:0.9130\n",
    "Epoch #87: Loss:0.1747, Accuracy:0.9359, Validation Loss:0.1551, Validation Accuracy:0.9392\n",
    "Epoch #88: Loss:0.1602, Accuracy:0.9458, Validation Loss:0.1609, Validation Accuracy:0.9392\n",
    "Epoch #89: Loss:0.1595, Accuracy:0.9409, Validation Loss:0.1525, Validation Accuracy:0.9409\n",
    "Epoch #90: Loss:0.1602, Accuracy:0.9376, Validation Loss:0.2061, Validation Accuracy:0.9097\n",
    "Epoch #91: Loss:0.1875, Accuracy:0.9269, Validation Loss:0.2469, Validation Accuracy:0.8933\n",
    "Epoch #92: Loss:0.1984, Accuracy:0.9220, Validation Loss:0.1497, Validation Accuracy:0.9425\n",
    "Epoch #93: Loss:0.1611, Accuracy:0.9400, Validation Loss:0.1515, Validation Accuracy:0.9442\n",
    "Epoch #94: Loss:0.1530, Accuracy:0.9441, Validation Loss:0.1438, Validation Accuracy:0.9491\n",
    "Epoch #95: Loss:0.1480, Accuracy:0.9470, Validation Loss:0.1630, Validation Accuracy:0.9475\n",
    "Epoch #96: Loss:0.1461, Accuracy:0.9433, Validation Loss:0.1410, Validation Accuracy:0.9507\n",
    "Epoch #97: Loss:0.1425, Accuracy:0.9491, Validation Loss:0.1483, Validation Accuracy:0.9524\n",
    "Epoch #98: Loss:0.1447, Accuracy:0.9470, Validation Loss:0.1330, Validation Accuracy:0.9442\n",
    "Epoch #99: Loss:0.1383, Accuracy:0.9491, Validation Loss:0.1348, Validation Accuracy:0.9540\n",
    "Epoch #100: Loss:0.1419, Accuracy:0.9487, Validation Loss:0.1304, Validation Accuracy:0.9557\n",
    "Epoch #101: Loss:0.1453, Accuracy:0.9462, Validation Loss:0.1319, Validation Accuracy:0.9573\n",
    "Epoch #102: Loss:0.1477, Accuracy:0.9441, Validation Loss:0.1348, Validation Accuracy:0.9524\n",
    "Epoch #103: Loss:0.1391, Accuracy:0.9470, Validation Loss:0.1351, Validation Accuracy:0.9491\n",
    "Epoch #104: Loss:0.1358, Accuracy:0.9507, Validation Loss:0.1300, Validation Accuracy:0.9475\n",
    "Epoch #105: Loss:0.1331, Accuracy:0.9524, Validation Loss:0.1325, Validation Accuracy:0.9540\n",
    "Epoch #106: Loss:0.1390, Accuracy:0.9495, Validation Loss:0.1426, Validation Accuracy:0.9475\n",
    "Epoch #107: Loss:0.1393, Accuracy:0.9466, Validation Loss:0.1282, Validation Accuracy:0.9507\n",
    "Epoch #108: Loss:0.1280, Accuracy:0.9511, Validation Loss:0.1787, Validation Accuracy:0.9360\n",
    "Epoch #109: Loss:0.1493, Accuracy:0.9454, Validation Loss:0.1774, Validation Accuracy:0.9392\n",
    "Epoch #110: Loss:0.1467, Accuracy:0.9458, Validation Loss:0.1677, Validation Accuracy:0.9442\n",
    "Epoch #111: Loss:0.1384, Accuracy:0.9462, Validation Loss:0.1567, Validation Accuracy:0.9442\n",
    "Epoch #112: Loss:0.1364, Accuracy:0.9495, Validation Loss:0.1371, Validation Accuracy:0.9540\n",
    "Epoch #113: Loss:0.1422, Accuracy:0.9458, Validation Loss:0.1318, Validation Accuracy:0.9507\n",
    "Epoch #114: Loss:0.1400, Accuracy:0.9507, Validation Loss:0.1254, Validation Accuracy:0.9557\n",
    "Epoch #115: Loss:0.1390, Accuracy:0.9478, Validation Loss:0.1386, Validation Accuracy:0.9458\n",
    "Epoch #116: Loss:0.1385, Accuracy:0.9466, Validation Loss:0.1668, Validation Accuracy:0.9278\n",
    "Epoch #117: Loss:0.1547, Accuracy:0.9384, Validation Loss:0.1973, Validation Accuracy:0.9195\n",
    "Epoch #118: Loss:0.1795, Accuracy:0.9257, Validation Loss:0.1466, Validation Accuracy:0.9409\n",
    "Epoch #119: Loss:0.1975, Accuracy:0.9183, Validation Loss:0.1435, Validation Accuracy:0.9524\n",
    "Epoch #120: Loss:0.1849, Accuracy:0.9331, Validation Loss:0.2353, Validation Accuracy:0.9031\n",
    "Epoch #121: Loss:0.1869, Accuracy:0.9211, Validation Loss:0.1339, Validation Accuracy:0.9557\n",
    "Epoch #122: Loss:0.1536, Accuracy:0.9421, Validation Loss:0.1785, Validation Accuracy:0.9163\n",
    "Epoch #123: Loss:0.1585, Accuracy:0.9392, Validation Loss:0.1586, Validation Accuracy:0.9458\n",
    "Epoch #124: Loss:0.1453, Accuracy:0.9425, Validation Loss:0.1539, Validation Accuracy:0.9458\n",
    "Epoch #125: Loss:0.1413, Accuracy:0.9458, Validation Loss:0.1369, Validation Accuracy:0.9491\n",
    "Epoch #126: Loss:0.1457, Accuracy:0.9413, Validation Loss:0.1290, Validation Accuracy:0.9475\n",
    "Epoch #127: Loss:0.1425, Accuracy:0.9470, Validation Loss:0.1190, Validation Accuracy:0.9606\n",
    "Epoch #128: Loss:0.1297, Accuracy:0.9499, Validation Loss:0.1611, Validation Accuracy:0.9409\n",
    "Epoch #129: Loss:0.1317, Accuracy:0.9511, Validation Loss:0.1195, Validation Accuracy:0.9573\n",
    "Epoch #130: Loss:0.1260, Accuracy:0.9561, Validation Loss:0.1278, Validation Accuracy:0.9507\n",
    "Epoch #131: Loss:0.1310, Accuracy:0.9532, Validation Loss:0.1179, Validation Accuracy:0.9606\n",
    "Epoch #132: Loss:0.1230, Accuracy:0.9573, Validation Loss:0.1293, Validation Accuracy:0.9540\n",
    "Epoch #133: Loss:0.1223, Accuracy:0.9532, Validation Loss:0.1285, Validation Accuracy:0.9540\n",
    "Epoch #134: Loss:0.1232, Accuracy:0.9532, Validation Loss:0.1169, Validation Accuracy:0.9573\n",
    "Epoch #135: Loss:0.1167, Accuracy:0.9585, Validation Loss:0.1165, Validation Accuracy:0.9655\n",
    "Epoch #136: Loss:0.1163, Accuracy:0.9561, Validation Loss:0.1216, Validation Accuracy:0.9573\n",
    "Epoch #137: Loss:0.1260, Accuracy:0.9569, Validation Loss:0.1210, Validation Accuracy:0.9540\n",
    "Epoch #138: Loss:0.1385, Accuracy:0.9478, Validation Loss:0.1485, Validation Accuracy:0.9491\n",
    "Epoch #139: Loss:0.1311, Accuracy:0.9520, Validation Loss:0.1420, Validation Accuracy:0.9507\n",
    "Epoch #140: Loss:0.1203, Accuracy:0.9544, Validation Loss:0.1167, Validation Accuracy:0.9622\n",
    "Epoch #141: Loss:0.1183, Accuracy:0.9561, Validation Loss:0.1241, Validation Accuracy:0.9540\n",
    "Epoch #142: Loss:0.1194, Accuracy:0.9552, Validation Loss:0.1296, Validation Accuracy:0.9540\n",
    "Epoch #143: Loss:0.1172, Accuracy:0.9581, Validation Loss:0.1239, Validation Accuracy:0.9524\n",
    "Epoch #144: Loss:0.1170, Accuracy:0.9593, Validation Loss:0.1395, Validation Accuracy:0.9507\n",
    "Epoch #145: Loss:0.1180, Accuracy:0.9544, Validation Loss:0.1174, Validation Accuracy:0.9540\n",
    "Epoch #146: Loss:0.1117, Accuracy:0.9614, Validation Loss:0.1131, Validation Accuracy:0.9639\n",
    "Epoch #147: Loss:0.1122, Accuracy:0.9606, Validation Loss:0.1210, Validation Accuracy:0.9524\n",
    "Epoch #148: Loss:0.1125, Accuracy:0.9585, Validation Loss:0.1232, Validation Accuracy:0.9524\n",
    "Epoch #149: Loss:0.1142, Accuracy:0.9573, Validation Loss:0.1310, Validation Accuracy:0.9557\n",
    "Epoch #150: Loss:0.1238, Accuracy:0.9528, Validation Loss:0.1440, Validation Accuracy:0.9507\n",
    "Epoch #151: Loss:0.1226, Accuracy:0.9536, Validation Loss:0.1287, Validation Accuracy:0.9524\n",
    "Epoch #152: Loss:0.1259, Accuracy:0.9507, Validation Loss:0.1252, Validation Accuracy:0.9524\n",
    "Epoch #153: Loss:0.1235, Accuracy:0.9520, Validation Loss:0.1235, Validation Accuracy:0.9524\n",
    "Epoch #154: Loss:0.1205, Accuracy:0.9573, Validation Loss:0.1146, Validation Accuracy:0.9606\n",
    "Epoch #155: Loss:0.1110, Accuracy:0.9602, Validation Loss:0.1123, Validation Accuracy:0.9639\n",
    "Epoch #156: Loss:0.1129, Accuracy:0.9589, Validation Loss:0.1198, Validation Accuracy:0.9573\n",
    "Epoch #157: Loss:0.1186, Accuracy:0.9569, Validation Loss:0.1202, Validation Accuracy:0.9524\n",
    "Epoch #158: Loss:0.1099, Accuracy:0.9589, Validation Loss:0.1332, Validation Accuracy:0.9557\n",
    "Epoch #159: Loss:0.1153, Accuracy:0.9540, Validation Loss:0.1306, Validation Accuracy:0.9540\n",
    "Epoch #160: Loss:0.1162, Accuracy:0.9561, Validation Loss:0.1250, Validation Accuracy:0.9540\n",
    "Epoch #161: Loss:0.1178, Accuracy:0.9565, Validation Loss:0.1400, Validation Accuracy:0.9524\n",
    "Epoch #162: Loss:0.1179, Accuracy:0.9552, Validation Loss:0.1216, Validation Accuracy:0.9639\n",
    "Epoch #163: Loss:0.1114, Accuracy:0.9593, Validation Loss:0.1165, Validation Accuracy:0.9507\n",
    "Epoch #164: Loss:0.1105, Accuracy:0.9602, Validation Loss:0.1153, Validation Accuracy:0.9573\n",
    "Epoch #165: Loss:0.1085, Accuracy:0.9618, Validation Loss:0.1106, Validation Accuracy:0.9639\n",
    "Epoch #166: Loss:0.1083, Accuracy:0.9581, Validation Loss:0.1119, Validation Accuracy:0.9639\n",
    "Epoch #167: Loss:0.1108, Accuracy:0.9606, Validation Loss:0.1122, Validation Accuracy:0.9606\n",
    "Epoch #168: Loss:0.1070, Accuracy:0.9634, Validation Loss:0.1110, Validation Accuracy:0.9606\n",
    "Epoch #169: Loss:0.1092, Accuracy:0.9639, Validation Loss:0.1230, Validation Accuracy:0.9557\n",
    "Epoch #170: Loss:0.1369, Accuracy:0.9470, Validation Loss:0.1102, Validation Accuracy:0.9622\n",
    "Epoch #171: Loss:0.1414, Accuracy:0.9487, Validation Loss:0.1351, Validation Accuracy:0.9557\n",
    "Epoch #172: Loss:0.1332, Accuracy:0.9503, Validation Loss:0.1663, Validation Accuracy:0.9425\n",
    "Epoch #173: Loss:0.1248, Accuracy:0.9503, Validation Loss:0.1313, Validation Accuracy:0.9524\n",
    "Epoch #174: Loss:0.1115, Accuracy:0.9589, Validation Loss:0.1151, Validation Accuracy:0.9589\n",
    "Epoch #175: Loss:0.1083, Accuracy:0.9643, Validation Loss:0.1106, Validation Accuracy:0.9655\n",
    "Epoch #176: Loss:0.1060, Accuracy:0.9610, Validation Loss:0.1093, Validation Accuracy:0.9622\n",
    "Epoch #177: Loss:0.1038, Accuracy:0.9634, Validation Loss:0.1265, Validation Accuracy:0.9540\n",
    "Epoch #178: Loss:0.1027, Accuracy:0.9647, Validation Loss:0.1127, Validation Accuracy:0.9589\n",
    "Epoch #179: Loss:0.1164, Accuracy:0.9569, Validation Loss:0.1108, Validation Accuracy:0.9622\n",
    "Epoch #180: Loss:0.1111, Accuracy:0.9593, Validation Loss:0.1191, Validation Accuracy:0.9557\n",
    "Epoch #181: Loss:0.1185, Accuracy:0.9532, Validation Loss:0.1284, Validation Accuracy:0.9475\n",
    "Epoch #182: Loss:0.1228, Accuracy:0.9528, Validation Loss:0.1133, Validation Accuracy:0.9589\n",
    "Epoch #183: Loss:0.1080, Accuracy:0.9602, Validation Loss:0.1176, Validation Accuracy:0.9573\n",
    "Epoch #184: Loss:0.1121, Accuracy:0.9569, Validation Loss:0.1403, Validation Accuracy:0.9540\n",
    "Epoch #185: Loss:0.1101, Accuracy:0.9589, Validation Loss:0.1306, Validation Accuracy:0.9540\n",
    "Epoch #186: Loss:0.1081, Accuracy:0.9585, Validation Loss:0.1163, Validation Accuracy:0.9589\n",
    "Epoch #187: Loss:0.1055, Accuracy:0.9598, Validation Loss:0.1084, Validation Accuracy:0.9655\n",
    "Epoch #188: Loss:0.1066, Accuracy:0.9626, Validation Loss:0.1174, Validation Accuracy:0.9606\n",
    "Epoch #189: Loss:0.1038, Accuracy:0.9618, Validation Loss:0.1154, Validation Accuracy:0.9557\n",
    "Epoch #190: Loss:0.1102, Accuracy:0.9606, Validation Loss:0.1531, Validation Accuracy:0.9475\n",
    "Epoch #191: Loss:0.1222, Accuracy:0.9556, Validation Loss:0.2039, Validation Accuracy:0.9278\n",
    "Epoch #192: Loss:0.1428, Accuracy:0.9450, Validation Loss:0.1364, Validation Accuracy:0.9491\n",
    "Epoch #193: Loss:0.1410, Accuracy:0.9441, Validation Loss:0.1101, Validation Accuracy:0.9622\n",
    "Epoch #194: Loss:0.1200, Accuracy:0.9548, Validation Loss:0.1176, Validation Accuracy:0.9557\n",
    "Epoch #195: Loss:0.1213, Accuracy:0.9524, Validation Loss:0.1278, Validation Accuracy:0.9475\n",
    "Epoch #196: Loss:0.1154, Accuracy:0.9561, Validation Loss:0.1187, Validation Accuracy:0.9573\n",
    "Epoch #197: Loss:0.1092, Accuracy:0.9577, Validation Loss:0.1307, Validation Accuracy:0.9557\n",
    "Epoch #198: Loss:0.1056, Accuracy:0.9626, Validation Loss:0.1112, Validation Accuracy:0.9622\n",
    "Epoch #199: Loss:0.1051, Accuracy:0.9639, Validation Loss:0.1071, Validation Accuracy:0.9622\n",
    "Epoch #200: Loss:0.1045, Accuracy:0.9606, Validation Loss:0.1067, Validation Accuracy:0.9622\n",
    "Epoch #201: Loss:0.0999, Accuracy:0.9651, Validation Loss:0.1057, Validation Accuracy:0.9639\n",
    "Epoch #202: Loss:0.0999, Accuracy:0.9655, Validation Loss:0.1180, Validation Accuracy:0.9573\n",
    "Epoch #203: Loss:0.1000, Accuracy:0.9622, Validation Loss:0.1076, Validation Accuracy:0.9639\n",
    "Epoch #204: Loss:0.0977, Accuracy:0.9651, Validation Loss:0.1065, Validation Accuracy:0.9639\n",
    "Epoch #205: Loss:0.0986, Accuracy:0.9618, Validation Loss:0.1080, Validation Accuracy:0.9639\n",
    "Epoch #206: Loss:0.1070, Accuracy:0.9610, Validation Loss:0.1268, Validation Accuracy:0.9524\n",
    "Epoch #207: Loss:0.1048, Accuracy:0.9643, Validation Loss:0.1251, Validation Accuracy:0.9589\n",
    "Epoch #208: Loss:0.1009, Accuracy:0.9598, Validation Loss:0.1161, Validation Accuracy:0.9606\n",
    "Epoch #209: Loss:0.1009, Accuracy:0.9626, Validation Loss:0.1144, Validation Accuracy:0.9589\n",
    "Epoch #210: Loss:0.1115, Accuracy:0.9593, Validation Loss:0.1116, Validation Accuracy:0.9589\n",
    "Epoch #211: Loss:0.1085, Accuracy:0.9577, Validation Loss:0.1295, Validation Accuracy:0.9475\n",
    "Epoch #212: Loss:0.1230, Accuracy:0.9552, Validation Loss:0.1061, Validation Accuracy:0.9639\n",
    "Epoch #213: Loss:0.1080, Accuracy:0.9610, Validation Loss:0.1182, Validation Accuracy:0.9589\n",
    "Epoch #214: Loss:0.1033, Accuracy:0.9618, Validation Loss:0.1548, Validation Accuracy:0.9425\n",
    "Epoch #215: Loss:0.1064, Accuracy:0.9622, Validation Loss:0.1237, Validation Accuracy:0.9589\n",
    "Epoch #216: Loss:0.0990, Accuracy:0.9655, Validation Loss:0.1096, Validation Accuracy:0.9589\n",
    "Epoch #217: Loss:0.1011, Accuracy:0.9630, Validation Loss:0.1164, Validation Accuracy:0.9606\n",
    "Epoch #218: Loss:0.0968, Accuracy:0.9651, Validation Loss:0.1089, Validation Accuracy:0.9639\n",
    "Epoch #219: Loss:0.0955, Accuracy:0.9639, Validation Loss:0.1073, Validation Accuracy:0.9606\n",
    "Epoch #220: Loss:0.0969, Accuracy:0.9651, Validation Loss:0.1100, Validation Accuracy:0.9606\n",
    "Epoch #221: Loss:0.0971, Accuracy:0.9643, Validation Loss:0.1454, Validation Accuracy:0.9475\n",
    "Epoch #222: Loss:0.1047, Accuracy:0.9630, Validation Loss:0.1397, Validation Accuracy:0.9540\n",
    "Epoch #223: Loss:0.1124, Accuracy:0.9585, Validation Loss:0.1062, Validation Accuracy:0.9639\n",
    "Epoch #224: Loss:0.0986, Accuracy:0.9639, Validation Loss:0.1127, Validation Accuracy:0.9639\n",
    "Epoch #225: Loss:0.0992, Accuracy:0.9655, Validation Loss:0.1123, Validation Accuracy:0.9557\n",
    "Epoch #226: Loss:0.0981, Accuracy:0.9663, Validation Loss:0.1262, Validation Accuracy:0.9557\n",
    "Epoch #227: Loss:0.1020, Accuracy:0.9639, Validation Loss:0.1194, Validation Accuracy:0.9557\n",
    "Epoch #228: Loss:0.0959, Accuracy:0.9659, Validation Loss:0.1420, Validation Accuracy:0.9491\n",
    "Epoch #229: Loss:0.0998, Accuracy:0.9626, Validation Loss:0.1142, Validation Accuracy:0.9573\n",
    "Epoch #230: Loss:0.1037, Accuracy:0.9602, Validation Loss:0.1162, Validation Accuracy:0.9589\n",
    "Epoch #231: Loss:0.1018, Accuracy:0.9610, Validation Loss:0.1117, Validation Accuracy:0.9606\n",
    "Epoch #232: Loss:0.1059, Accuracy:0.9585, Validation Loss:0.1183, Validation Accuracy:0.9573\n",
    "Epoch #233: Loss:0.1127, Accuracy:0.9556, Validation Loss:0.1314, Validation Accuracy:0.9409\n",
    "Epoch #234: Loss:0.1141, Accuracy:0.9585, Validation Loss:0.1182, Validation Accuracy:0.9524\n",
    "Epoch #235: Loss:0.1224, Accuracy:0.9561, Validation Loss:0.1262, Validation Accuracy:0.9573\n",
    "Epoch #236: Loss:0.1157, Accuracy:0.9561, Validation Loss:0.1614, Validation Accuracy:0.9425\n",
    "Epoch #237: Loss:0.1095, Accuracy:0.9552, Validation Loss:0.1242, Validation Accuracy:0.9606\n",
    "Epoch #238: Loss:0.1078, Accuracy:0.9565, Validation Loss:0.1081, Validation Accuracy:0.9655\n",
    "Epoch #239: Loss:0.0969, Accuracy:0.9676, Validation Loss:0.1052, Validation Accuracy:0.9622\n",
    "Epoch #240: Loss:0.0917, Accuracy:0.9671, Validation Loss:0.1057, Validation Accuracy:0.9639\n",
    "Epoch #241: Loss:0.0936, Accuracy:0.9634, Validation Loss:0.1160, Validation Accuracy:0.9589\n",
    "Epoch #242: Loss:0.0920, Accuracy:0.9671, Validation Loss:0.1045, Validation Accuracy:0.9622\n",
    "Epoch #243: Loss:0.0916, Accuracy:0.9671, Validation Loss:0.1082, Validation Accuracy:0.9622\n",
    "Epoch #244: Loss:0.1023, Accuracy:0.9598, Validation Loss:0.1060, Validation Accuracy:0.9655\n",
    "Epoch #245: Loss:0.1265, Accuracy:0.9511, Validation Loss:0.1540, Validation Accuracy:0.9458\n",
    "Epoch #246: Loss:0.1206, Accuracy:0.9552, Validation Loss:0.2168, Validation Accuracy:0.9195\n",
    "Epoch #247: Loss:0.1382, Accuracy:0.9417, Validation Loss:0.1169, Validation Accuracy:0.9655\n",
    "Epoch #248: Loss:0.1213, Accuracy:0.9536, Validation Loss:0.1266, Validation Accuracy:0.9409\n",
    "Epoch #249: Loss:0.1139, Accuracy:0.9561, Validation Loss:0.1108, Validation Accuracy:0.9606\n",
    "Epoch #250: Loss:0.1051, Accuracy:0.9630, Validation Loss:0.1196, Validation Accuracy:0.9606\n",
    "Epoch #251: Loss:0.0956, Accuracy:0.9651, Validation Loss:0.1171, Validation Accuracy:0.9573\n",
    "Epoch #252: Loss:0.0944, Accuracy:0.9639, Validation Loss:0.1155, Validation Accuracy:0.9589\n",
    "Epoch #253: Loss:0.0977, Accuracy:0.9634, Validation Loss:0.1072, Validation Accuracy:0.9655\n",
    "Epoch #254: Loss:0.0986, Accuracy:0.9610, Validation Loss:0.1080, Validation Accuracy:0.9622\n",
    "Epoch #255: Loss:0.0983, Accuracy:0.9626, Validation Loss:0.1174, Validation Accuracy:0.9589\n",
    "Epoch #256: Loss:0.0997, Accuracy:0.9663, Validation Loss:0.1088, Validation Accuracy:0.9622\n",
    "Epoch #257: Loss:0.0911, Accuracy:0.9663, Validation Loss:0.1218, Validation Accuracy:0.9606\n",
    "Epoch #258: Loss:0.0948, Accuracy:0.9655, Validation Loss:0.1115, Validation Accuracy:0.9573\n",
    "Epoch #259: Loss:0.0961, Accuracy:0.9643, Validation Loss:0.1288, Validation Accuracy:0.9573\n",
    "Epoch #260: Loss:0.1020, Accuracy:0.9618, Validation Loss:0.1322, Validation Accuracy:0.9507\n",
    "Epoch #261: Loss:0.0941, Accuracy:0.9655, Validation Loss:0.1141, Validation Accuracy:0.9672\n",
    "Epoch #262: Loss:0.0883, Accuracy:0.9680, Validation Loss:0.1093, Validation Accuracy:0.9573\n",
    "Epoch #263: Loss:0.0954, Accuracy:0.9676, Validation Loss:0.1129, Validation Accuracy:0.9672\n",
    "Epoch #264: Loss:0.0924, Accuracy:0.9655, Validation Loss:0.1301, Validation Accuracy:0.9524\n",
    "Epoch #265: Loss:0.0942, Accuracy:0.9655, Validation Loss:0.1531, Validation Accuracy:0.9425\n",
    "Epoch #266: Loss:0.1048, Accuracy:0.9622, Validation Loss:0.1397, Validation Accuracy:0.9507\n",
    "Epoch #267: Loss:0.1005, Accuracy:0.9602, Validation Loss:0.1121, Validation Accuracy:0.9688\n",
    "Epoch #268: Loss:0.0877, Accuracy:0.9684, Validation Loss:0.1056, Validation Accuracy:0.9672\n",
    "Epoch #269: Loss:0.0867, Accuracy:0.9700, Validation Loss:0.1088, Validation Accuracy:0.9688\n",
    "Epoch #270: Loss:0.0902, Accuracy:0.9671, Validation Loss:0.1066, Validation Accuracy:0.9622\n",
    "Epoch #271: Loss:0.0919, Accuracy:0.9651, Validation Loss:0.1122, Validation Accuracy:0.9606\n",
    "Epoch #272: Loss:0.0991, Accuracy:0.9618, Validation Loss:0.1050, Validation Accuracy:0.9655\n",
    "Epoch #273: Loss:0.0901, Accuracy:0.9671, Validation Loss:0.1085, Validation Accuracy:0.9672\n",
    "Epoch #274: Loss:0.0890, Accuracy:0.9659, Validation Loss:0.1081, Validation Accuracy:0.9672\n",
    "Epoch #275: Loss:0.0933, Accuracy:0.9647, Validation Loss:0.1156, Validation Accuracy:0.9672\n",
    "Epoch #276: Loss:0.0941, Accuracy:0.9639, Validation Loss:0.1247, Validation Accuracy:0.9557\n",
    "Epoch #277: Loss:0.1002, Accuracy:0.9589, Validation Loss:0.1467, Validation Accuracy:0.9491\n",
    "Epoch #278: Loss:0.0934, Accuracy:0.9676, Validation Loss:0.1220, Validation Accuracy:0.9540\n",
    "Epoch #279: Loss:0.0889, Accuracy:0.9667, Validation Loss:0.1112, Validation Accuracy:0.9655\n",
    "Epoch #280: Loss:0.0879, Accuracy:0.9696, Validation Loss:0.1045, Validation Accuracy:0.9639\n",
    "Epoch #281: Loss:0.0860, Accuracy:0.9692, Validation Loss:0.1080, Validation Accuracy:0.9688\n",
    "Epoch #282: Loss:0.0853, Accuracy:0.9708, Validation Loss:0.1311, Validation Accuracy:0.9557\n",
    "Epoch #283: Loss:0.0929, Accuracy:0.9659, Validation Loss:0.1378, Validation Accuracy:0.9475\n",
    "Epoch #284: Loss:0.1044, Accuracy:0.9606, Validation Loss:0.1611, Validation Accuracy:0.9442\n",
    "Epoch #285: Loss:0.1004, Accuracy:0.9626, Validation Loss:0.1156, Validation Accuracy:0.9589\n",
    "Epoch #286: Loss:0.0929, Accuracy:0.9659, Validation Loss:0.1127, Validation Accuracy:0.9622\n",
    "Epoch #287: Loss:0.0984, Accuracy:0.9618, Validation Loss:0.1254, Validation Accuracy:0.9557\n",
    "Epoch #288: Loss:0.1003, Accuracy:0.9614, Validation Loss:0.1089, Validation Accuracy:0.9622\n",
    "Epoch #289: Loss:0.0855, Accuracy:0.9692, Validation Loss:0.1047, Validation Accuracy:0.9622\n",
    "Epoch #290: Loss:0.0871, Accuracy:0.9700, Validation Loss:0.1063, Validation Accuracy:0.9704\n",
    "Epoch #291: Loss:0.0928, Accuracy:0.9639, Validation Loss:0.1074, Validation Accuracy:0.9688\n",
    "Epoch #292: Loss:0.0935, Accuracy:0.9655, Validation Loss:0.1190, Validation Accuracy:0.9557\n",
    "Epoch #293: Loss:0.0876, Accuracy:0.9684, Validation Loss:0.1071, Validation Accuracy:0.9639\n",
    "Epoch #294: Loss:0.0873, Accuracy:0.9671, Validation Loss:0.1157, Validation Accuracy:0.9606\n",
    "Epoch #295: Loss:0.0843, Accuracy:0.9696, Validation Loss:0.1228, Validation Accuracy:0.9540\n",
    "Epoch #296: Loss:0.0872, Accuracy:0.9700, Validation Loss:0.1278, Validation Accuracy:0.9524\n",
    "Epoch #297: Loss:0.0856, Accuracy:0.9680, Validation Loss:0.1055, Validation Accuracy:0.9672\n",
    "Epoch #298: Loss:0.0842, Accuracy:0.9704, Validation Loss:0.1131, Validation Accuracy:0.9622\n",
    "Epoch #299: Loss:0.0981, Accuracy:0.9606, Validation Loss:0.1131, Validation Accuracy:0.9606\n",
    "Epoch #300: Loss:0.1120, Accuracy:0.9548, Validation Loss:0.1181, Validation Accuracy:0.9540\n",
    "\n",
    "Test:\n",
    "Test Loss:0.11810120, Accuracy:0.9540\n",
    "Labels: ['01', '03', '02']\n",
    "Confusion Matrix:\n",
    "       01   03   02\n",
    "t:01  235    3    2\n",
    "t:03    3  139    0\n",
    "t:02   20    0  207\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.91      0.98      0.94       240\n",
    "          03       0.98      0.98      0.98       142\n",
    "          02       0.99      0.91      0.95       227\n",
    "\n",
    "    accuracy                           0.95       609\n",
    "   macro avg       0.96      0.96      0.96       609\n",
    "weighted avg       0.96      0.95      0.95       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 14:54:58 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 56 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0790961632391893, 1.0746449547252437, 1.0751257526071984, 1.0751050146929737, 1.0743131717829086, 1.0738838529351897, 1.0735985993947497, 1.0731623276505369, 1.072557285305706, 1.0716921500188767, 1.0706035989062932, 1.068992711053106, 1.066461722447563, 1.0628006354537112, 1.0567306266433891, 1.0467829271686098, 1.030866280565121, 1.0054831306139629, 0.9681364565060057, 0.9078011810290206, 0.8192006744970437, 0.7270569581116362, 0.6640536258569101, 0.6310151618689739, 0.5869180666793548, 0.5723070345684421, 0.5392891748981131, 0.5265185313271772, 0.5074358055940011, 0.4969479961348285, 0.47471875882109593, 0.45302067053533346, 0.4313987033512009, 0.4326398603630379, 0.40907111587782796, 0.39378704730121566, 0.3744000075094414, 0.36298019976059986, 0.3563416755845394, 0.34957940325948406, 0.33741844879778343, 0.332081556075508, 0.3517992181339483, 0.33494282184758994, 0.31032282779565196, 0.29924151732025084, 0.2962877764588311, 0.2981617255946881, 0.2866725215966674, 0.3180513749471047, 0.3373224993644677, 0.33408847549083004, 0.2650248837001218, 0.2583087039209156, 0.25893389346760093, 0.25938809940771906, 0.2408310274772456, 0.23523987058935494, 0.2407918660511524, 0.24344678949839962, 0.2503419373129389, 0.22711743497594042, 0.22802335767714652, 0.24434048553992962, 0.2082864586355651, 0.19527366774520655, 0.20682967976592054, 0.21022288630944364, 0.2170102973338614, 0.22947396264581257, 0.2309825944978811, 0.17874350727876812, 0.17810757671083724, 0.1750506206930956, 0.17147613888107888, 0.16835339181431017, 0.1677134723352094, 0.1662571919326516, 0.1652058798984941, 0.18000152315607995, 0.16600797020743047, 0.1672190651664593, 0.18430882555314865, 0.1714006096418268, 0.19400018642003508, 0.18583593929831813, 0.15506742703797194, 0.1608952220731181, 0.15248588304997274, 0.20610922154143135, 0.246940038719005, 0.14970869961239042, 0.15153415168349574, 0.14382487413135459, 0.16299113127202627, 0.14099635330620658, 0.14834627939758238, 0.13296577046902114, 0.13484491547340244, 0.13043015320557483, 0.13191541146077154, 0.1348070601866946, 0.13508475915649645, 0.12998311730539075, 0.13245368701190197, 0.1425858503222857, 0.12818678070856823, 0.17867822245326143, 0.17735521277574876, 0.16772636404178412, 0.15671907370215762, 0.13708670018928978, 0.13182748846819836, 0.1253697150715662, 0.13857670260474012, 0.16680484824188432, 0.19728515048821768, 0.14664940019742218, 0.1435179869921141, 0.23528617605787192, 0.13391066654562364, 0.17847923777862917, 0.1586290188561911, 0.15394616768082178, 0.13692538498951295, 0.12897010093741423, 0.11904315569717895, 0.16105962244943642, 0.11946790119746244, 0.1277725157528284, 0.11792576029187157, 0.12934692431553244, 0.12851598518724708, 0.11689312196962157, 0.11652387589314105, 0.12159384847029872, 0.12098022152735487, 0.148462025207056, 0.14196060233319727, 0.11674818808858227, 0.12408739028648398, 0.12955423736905033, 0.12388570946817132, 0.13950259068427218, 0.11736607223700224, 0.11313730172433681, 0.12097511586101575, 0.12320109313638339, 0.1309532898253408, 0.1440120695895945, 0.1287075778093244, 0.12524452291685959, 0.12352612713728045, 0.11456884041257288, 0.11227800513576404, 0.11979535883919555, 0.12023898819690855, 0.13315707430463705, 0.13057112838149265, 0.12503191650109532, 0.1400067859829353, 0.12162899892709916, 0.11654501208922351, 0.11532193724500331, 0.11057231092002787, 0.1119067896347132, 0.11221474482508129, 0.11100245194971464, 0.1230303723921721, 0.11017623061714893, 0.13510262513493473, 0.16629423484915778, 0.13128439957285162, 0.11507433031384386, 0.11059367312928922, 0.109286662233972, 0.126450454146404, 0.11271396053003756, 0.11084637212244357, 0.11906826218409687, 0.12844567659061726, 0.11331788713513141, 0.11764065228855278, 0.14028220243250403, 0.1305960849147712, 0.116282804210002, 0.10841612241342541, 0.11736409466450633, 0.11540608345385647, 0.1531112285817198, 0.20392959617530967, 0.13637066181070112, 0.11011943776521385, 0.11761968232972672, 0.1278326909726085, 0.11869063055867632, 0.13067115344143854, 0.11117183818361051, 0.10712589334531371, 0.1066867136847601, 0.1056503469035739, 0.11795136750918891, 0.107641547049697, 0.10648475112684058, 0.10798388458971907, 0.12676788355133609, 0.1251424935161578, 0.11611792666620417, 0.11439135451671134, 0.11159442879002669, 0.1295415229349105, 0.10614702347757782, 0.11820034655835632, 0.1547867231230039, 0.12371956947303954, 0.10959628482512848, 0.11637761398587125, 0.10889376144495308, 0.10726603723707653, 0.10998073483286624, 0.145430707995136, 0.1396935980993343, 0.10620068134785873, 0.11273119742989736, 0.11225075817900926, 0.12619251252590924, 0.11942419244591239, 0.14195199594611213, 0.1142018239315116, 0.11619455708658753, 0.11166681513214738, 0.11825599679218725, 0.13139879561605908, 0.11815050110979425, 0.12619676317389572, 0.16142957560926038, 0.12422236227636854, 0.10813958181927748, 0.10524458936086821, 0.10572583364148445, 0.11599681285410288, 0.10453847455616264, 0.1081913989801908, 0.10598844257970945, 0.15395918097010583, 0.21676237311073515, 0.11687301800315603, 0.1266172071567114, 0.11075978223326172, 0.11957891324568656, 0.11705952419093481, 0.1154560648112853, 0.10718602875525925, 0.10804054818815002, 0.11738312408352525, 0.10882776289297443, 0.12175472349858245, 0.1114674348793985, 0.12880044620123207, 0.13223203456362675, 0.11407295701343242, 0.10934587783365218, 0.11291233908953925, 0.1300851585976596, 0.15312922003331833, 0.1396731716413999, 0.11214638678800493, 0.10555864324906385, 0.10878121499308616, 0.10656895810258017, 0.11216307419225305, 0.10503576171075182, 0.10850359714775054, 0.10810787481529568, 0.11556513967185185, 0.12472842736490841, 0.14674376291398736, 0.12196181041658022, 0.11123669468025464, 0.10451794562521827, 0.10801098591147973, 0.13110529918878144, 0.13778140896255356, 0.16109453934460438, 0.11563374544990865, 0.11269573105135183, 0.12536120252020058, 0.10888270968923036, 0.10466725835561361, 0.10631724102009693, 0.10736214929858257, 0.11900045219900574, 0.10705247364803684, 0.11570673713641018, 0.12284358819619384, 0.1277992769170473, 0.1054902634597177, 0.11306093441099173, 0.113124711442072, 0.11810120810526736], 'val_acc': [0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.40722495762781163, 0.44006568031945253, 0.4269293912232216, 0.43349753582027356, 0.4515599336823806, 0.4630541864580709, 0.46141215052902207, 0.4679802948324551, 0.47783251187484255, 0.604269290694658, 0.6568144465902169, 0.701149422155421, 0.7224958919734986, 0.7208538563380688, 0.738916253417192, 0.7487684706553254, 0.7619047595558104, 0.7668308674408297, 0.7733990123315007, 0.7865353011341126, 0.802955662675679, 0.81609195177191, 0.8341543492425252, 0.8275862049390921, 0.8390804578126554, 0.8407224940353231, 0.853858783033681, 0.8456486023118344, 0.8440065662849126, 0.8620689638534007, 0.8587848916038112, 0.8686371087440716, 0.8555008195499677, 0.8604269281200979, 0.8686371087440716, 0.8735632169227099, 0.8817733976445566, 0.8768472895637913, 0.8801313618133808, 0.8653530365923551, 0.8637110002718144, 0.8472906390238669, 0.8932676509096118, 0.904761903587429, 0.9031198673647612, 0.9064039395164778, 0.904761903783175, 0.9080459759348914, 0.9080459760327645, 0.904761903783175, 0.8965517232570742, 0.9129720844071487, 0.9096880121575592, 0.8965517232570742, 0.9195402291020737, 0.9277504098239203, 0.9195402291999466, 0.9211822653247413, 0.9146141207276894, 0.9031198678541261, 0.8965517231592013, 0.9343185544209723, 0.9343185544209723, 0.9392446629911025, 0.9293924459487151, 0.93596059064364, 0.9343185543230993, 0.93596059064364, 0.9408866990180242, 0.9441707713654868, 0.9441707714633597, 0.9343185544209723, 0.9277504099217934, 0.9343185542252264, 0.9195402291020737, 0.9129720845050217, 0.9392446628932295, 0.9392446626974835, 0.9408866990180242, 0.9096880123533052, 0.8932676511053577, 0.942528735142819, 0.9441707712676137, 0.949096879739871, 0.9474548437129492, 0.9507389159625387, 0.9523809520873334, 0.9441707712676137, 0.9540229881142552, 0.9556650241411769, 0.9573070603638447, 0.9523809518915874, 0.949096879641998, 0.9474548436150763, 0.9540229881142552, 0.9474548436150763, 0.9507389159625387, 0.935960590839386, 0.9392446628932295, 0.9441707714633597, 0.9441707714633597, 0.9540229882121282, 0.9507389159625387, 0.9556650243369229, 0.9458128074902815, 0.9277504099217934, 0.9195402291999466, 0.9408866991158973, 0.9523809518915874, 0.9031198676583803, 0.95566502423905, 0.9162561568524842, 0.9458128073924085, 0.9458128075881544, 0.949096879641998, 0.9474548436150763, 0.9605911326134342, 0.9408866990180242, 0.9573070604617177, 0.9507389158646657, 0.9605911326134342, 0.9540229882121282, 0.9540229882121282, 0.9573070603638447, 0.9655172410856914, 0.9573070604617177, 0.9540229882121282, 0.9490968798377439, 0.9507389158646657, 0.9622331688361019, 0.9540229881142552, 0.9540229882121282, 0.9523809520873334, 0.9507389159625387, 0.9540229882121282, 0.9638752049608966, 0.9523809520873334, 0.9523809520873334, 0.9556650243369229, 0.9507389159625387, 0.9523809520873334, 0.9523809520873334, 0.9523809519894605, 0.9605911327113071, 0.9638752049608966, 0.9573070603638447, 0.9523809520873334, 0.9556650243369229, 0.9540229882121282, 0.9540229882121282, 0.9523809520873334, 0.9638752049608966, 0.9507389159625387, 0.9573070605595907, 0.9638752049608966, 0.9638752049608966, 0.9605911328091802, 0.9605911327113071, 0.9556650243369229, 0.9622331688361019, 0.9556650243369229, 0.942528735338565, 0.9523809520873334, 0.9589490966843854, 0.9655172410856914, 0.9622331688361019, 0.9540229882121282, 0.9589490965865124, 0.9622331688361019, 0.9556650243369229, 0.9474548436150763, 0.9589490965865124, 0.9573070604617177, 0.9540229882121282, 0.9540229882121282, 0.9589490965865124, 0.9655172410856914, 0.9605911328091802, 0.955665024434796, 0.9474548436150763, 0.9277504101175392, 0.949096879739871, 0.9622331688361019, 0.95566502423905, 0.9474548436150763, 0.9573070605595907, 0.9556650243369229, 0.9622331689339749, 0.9622331688361019, 0.9622331688361019, 0.9638752049608966, 0.9573070604617177, 0.9638752049608966, 0.9638752049608966, 0.9638752050587697, 0.9523809520873334, 0.9589490966843854, 0.9605911328091802, 0.9589490966843854, 0.9589490965865124, 0.9474548436150763, 0.9638752049608966, 0.9589490966843854, 0.942528735338565, 0.9589490966843854, 0.9589490966843854, 0.9605911328091802, 0.9638752050587697, 0.9605911327113071, 0.9605911328091802, 0.9474548437129492, 0.9540229882121282, 0.9638752050587697, 0.9638752049608966, 0.955665024434796, 0.955665024434796, 0.9556650243369229, 0.949096879739871, 0.9573070605595907, 0.9589490966843854, 0.9605911328091802, 0.9573070604617177, 0.9408866990180242, 0.9523809520873334, 0.9573070605595907, 0.942528735338565, 0.9605911328091802, 0.9655172411835644, 0.9622331688361019, 0.9638752049608966, 0.9589490966843854, 0.9622331688361019, 0.9622331689339749, 0.9655172411835644, 0.9458128075881544, 0.9195402292978196, 0.9655172411835644, 0.9408866990180242, 0.9605911327113071, 0.9605911328091802, 0.9573070605595907, 0.9589490966843854, 0.9655172410856914, 0.9622331688361019, 0.9589490965865124, 0.9622331689339749, 0.9605911326134342, 0.9573070604617177, 0.9573070603638447, 0.9507389159625387, 0.9671592773083592, 0.9573070604617177, 0.9671592772104861, 0.9523809520873334, 0.942528735240692, 0.9507389159625387, 0.9688013134331539, 0.9671592772104861, 0.9688013134331539, 0.9622331688361019, 0.9605911327113071, 0.9655172410856914, 0.9671592773083592, 0.9671592773083592, 0.9671592773083592, 0.955665024434796, 0.9490968798377439, 0.9540229883100012, 0.9655172411835644, 0.9638752049608966, 0.9688013134331539, 0.955665024434796, 0.9474548437129492, 0.9441707713654868, 0.9589490966843854, 0.9622331688361019, 0.9556650243369229, 0.9622331688361019, 0.9622331688361019, 0.9704433495579486, 0.9688013134331539, 0.955665024434796, 0.9638752049608966, 0.9605911328091802, 0.9540229883100012, 0.9523809521852065, 0.9671592773083592, 0.9622331688361019, 0.9605911327113071, 0.9540229881142552], 'loss': [1.086052649417697, 1.0762213947347057, 1.0748906206546134, 1.0752711223136229, 1.0747180722332588, 1.074004770549171, 1.0738288085318688, 1.0734294084552867, 1.0731160938862168, 1.0722790911946698, 1.0715980564054768, 1.070528559860997, 1.06866253709891, 1.0660234375900801, 1.0616205704530406, 1.0546561881502061, 1.0433254720249214, 1.0242776341996398, 0.994824340333684, 0.9480525686021214, 0.878463543024396, 0.7888420385746495, 0.7133371247647969, 0.6576592605461575, 0.6208484773273586, 0.5906991436006597, 0.5534916309605389, 0.5308315927732652, 0.506514175667655, 0.49329812201141576, 0.4810930761223701, 0.4526438042001313, 0.42780695207554703, 0.4135938319582224, 0.40428115328968917, 0.3892338451051614, 0.37674624552716957, 0.3687348870036538, 0.35522557989038234, 0.3612172973474193, 0.3474423585731146, 0.3331169752369671, 0.32765336702491715, 0.3272481691176397, 0.31447578573129015, 0.3021203122711769, 0.29431284232795607, 0.29176624554383435, 0.2937597135988349, 0.28781904843798406, 0.29376626977930326, 0.30637970250979585, 0.28784968452776727, 0.26205935431701693, 0.2577682710158996, 0.25237652577413916, 0.24509107110681475, 0.2407779616741674, 0.23262778002868198, 0.23402694979609895, 0.2342961660156015, 0.23109140273726697, 0.22717370635063006, 0.22370434448704338, 0.22242124693227255, 0.2084755047146055, 0.19884070695303305, 0.20191731889879433, 0.19791157261546877, 0.20490482633118756, 0.20499120386474187, 0.20223108647540364, 0.17934719354097847, 0.17929557069248733, 0.1763115516678264, 0.17049014083046687, 0.16922153527609377, 0.17408948010120548, 0.1649819563485269, 0.1689969985208472, 0.17232417592767327, 0.16057521787145054, 0.16681807451791586, 0.16895467761360888, 0.1646050923414054, 0.18905866326003104, 0.17467404728797428, 0.16015425038655925, 0.159477521017102, 0.16021427493198206, 0.1874936071081083, 0.19838333317141757, 0.161066617711124, 0.1529998317819846, 0.1480206579520717, 0.14607171785904885, 0.1424668736281581, 0.14468150925832118, 0.13829680695915614, 0.14189289912306063, 0.14529145756541337, 0.14768739504858208, 0.13913740642384093, 0.13583041783230995, 0.13308745057798263, 0.1389726341811049, 0.13931188840090128, 0.12801448698834472, 0.14934587812766403, 0.14665795059174727, 0.138438012027153, 0.1363905175755401, 0.14221412020542293, 0.13998071898302747, 0.1389981581567494, 0.138484758879125, 0.15467650027490495, 0.1794934124794829, 0.19745072854617782, 0.1848803764373615, 0.186903060651413, 0.1536280580920605, 0.15848928560833667, 0.14527079220058003, 0.14132677212632902, 0.1457474109756873, 0.14245635695158823, 0.12970850784063828, 0.13173889314614282, 0.12604990924959064, 0.13097282126392917, 0.12303988637069901, 0.12227581353892536, 0.12322194633481438, 0.11665827738736934, 0.11631720730656227, 0.1260190308950765, 0.13847591821907482, 0.13112817132191493, 0.12029602280387644, 0.11827208195378892, 0.11940836791385126, 0.11719710797499827, 0.11695849482406091, 0.11801650069455102, 0.11165542239648361, 0.1122110703673206, 0.11249613059374831, 0.1142210471709651, 0.12383319091747919, 0.12256892286776518, 0.12592798880850264, 0.12345257757013584, 0.12050326142834931, 0.11099512905066018, 0.11293447133023636, 0.11863813105611096, 0.10985967170531255, 0.11530936378411934, 0.11616765317608443, 0.11776430235995894, 0.11787281824578005, 0.11144764346263737, 0.11045052305751268, 0.1084631444553575, 0.10833795243526141, 0.11080527130889208, 0.10704796615016533, 0.10921998046017281, 0.1369211230006306, 0.1414365803191794, 0.1331888964105191, 0.12477218077902431, 0.11152482022372605, 0.10832341073168866, 0.10595644509584262, 0.10377184631153788, 0.10272595713209567, 0.11639651857178804, 0.11110309913785062, 0.11854356415952255, 0.12278764247894287, 0.10795519930870871, 0.11207053323545985, 0.11006402335984507, 0.1080811815676748, 0.10553804585821085, 0.10659538083375113, 0.10382607686017818, 0.11015362261624306, 0.12221982390361645, 0.14278874277334194, 0.14098617183599138, 0.12000712129981611, 0.1212611074687519, 0.11536236567541314, 0.10917120753066496, 0.10559076267223828, 0.10510560686277413, 0.10449672140624734, 0.09989283801777407, 0.09989831380897969, 0.10002646284059333, 0.09773251555049199, 0.09864145219631998, 0.1070260049687274, 0.10475360827646706, 0.10094251164543065, 0.10090137974682285, 0.11148524656432855, 0.10849445982390606, 0.12299609653024458, 0.1079971386483073, 0.10327638893156815, 0.10644033818335503, 0.09904482731216986, 0.10114567370691338, 0.0967842120960263, 0.09546276450279557, 0.09689354033379584, 0.09706919590428136, 0.10473213638245937, 0.112427579968005, 0.09862732797004359, 0.09923576843567208, 0.09807957022525447, 0.10202769104398986, 0.09590446917795303, 0.09980737096658722, 0.10365398408695903, 0.10184884674434055, 0.10585995350163085, 0.11271052120035434, 0.11412570983538638, 0.12244561517752661, 0.11566151461929267, 0.10954304058143001, 0.10779555741644004, 0.09693392746808348, 0.09171256780379607, 0.0935871746964768, 0.09199397713863874, 0.09160617653288146, 0.10233892799647681, 0.12654853710403677, 0.12059588469029452, 0.13822172971966332, 0.12128516436847084, 0.11387711515539235, 0.10507166933046475, 0.09556852078535719, 0.09443765262436328, 0.0976946450846396, 0.09860602400630897, 0.09826250704774132, 0.09965628767711181, 0.09107853211233014, 0.0947663546419364, 0.09606249023083543, 0.10197210120958959, 0.09412825017616734, 0.08826089053428149, 0.09544686692512501, 0.09238449603747538, 0.09420515094143654, 0.1047940909464746, 0.10045887919666831, 0.08765219277424979, 0.08673048395150985, 0.09020699720669087, 0.09194714635365797, 0.09908590912818908, 0.09006439734716924, 0.08899412247250947, 0.09334311880614968, 0.0940625845192639, 0.10019156478146508, 0.09337920044114702, 0.08885468584494914, 0.08792481685259994, 0.08597641049040906, 0.0852981120500966, 0.09285778635519976, 0.10438282184715877, 0.10041191234297331, 0.09294938526481573, 0.09842570636429092, 0.10032594546645084, 0.08548303143383297, 0.08712057459342161, 0.0927799406421258, 0.09347408748627688, 0.08762488652793289, 0.08732138658267517, 0.084294998064423, 0.08715104653788788, 0.08563513611376408, 0.08416090234961109, 0.09810084857245491, 0.1120122212465294], 'acc': [0.3942505128941742, 0.39425051446078496, 0.39425051211086876, 0.39425051449750237, 0.3942505152808078, 0.39425051195175986, 0.39425051312671794, 0.394250513714197, 0.39425051171921605, 0.39425051391002336, 0.3958932235989972, 0.40821355370770246, 0.41848049118533515, 0.42094456025951943, 0.42176591408815717, 0.42874743281203864, 0.43326488723500306, 0.44517453773065757, 0.4517453807947327, 0.47515400616295284, 0.6188911702598634, 0.6726899348245264, 0.6891170393514927, 0.7133470194050908, 0.7096509208424625, 0.7297741284360632, 0.7466119096019674, 0.7585215605259921, 0.7708418923971344, 0.7782340873683014, 0.7901437418172003, 0.8036961002761089, 0.8131416803267947, 0.8234086211701928, 0.8262833654758133, 0.8377823385859416, 0.8476386010279645, 0.8484599580265413, 0.8529774104545249, 0.8558521539768399, 0.8599589307450171, 0.8665297718508288, 0.8694045173314073, 0.8685831628785731, 0.8772073928580392, 0.881314170017869, 0.8858316206834155, 0.8845995915499066, 0.8825462009138151, 0.8891170457403273, 0.8825462016971204, 0.8809034908331885, 0.8858316242082898, 0.898973305832434, 0.9010266950977411, 0.9051334716700921, 0.9071868587813093, 0.9104722812924787, 0.912525668991175, 0.9108829602813329, 0.9084188949645667, 0.9129363487633346, 0.9137577026286898, 0.913757703803648, 0.9149897350912466, 0.9227926031030423, 0.9252566776970819, 0.9260780333248742, 0.9248459915850441, 0.9203285460844177, 0.9227926100303995, 0.9232032811127648, 0.9363449727974877, 0.9318275189987199, 0.9347022548103724, 0.9408624275753875, 0.9429158075634214, 0.9371663284252801, 0.9416837824198745, 0.9375769975493821, 0.9359342876645819, 0.9420944511523237, 0.9388090392892121, 0.9355236183446536, 0.9404517491740123, 0.927310057293463, 0.935934295766897, 0.9457905503024311, 0.9408624267920821, 0.9375769977452084, 0.926899380067046, 0.9219712480627291, 0.94004106384528, 0.9441478404176309, 0.9470225831566406, 0.9433264843981859, 0.9490759708553369, 0.9470225827649879, 0.9490759718344687, 0.9486652942163988, 0.9462012358269897, 0.9441478400259782, 0.9470225831566406, 0.9507186826984007, 0.9523613929748536, 0.9494866488650594, 0.9466119061260497, 0.9511293609039495, 0.945379872488535, 0.9457905514773892, 0.9462012273330218, 0.9494866575548536, 0.9457905510857365, 0.9507186902132367, 0.9478439393719119, 0.9466119061260497, 0.9383983539604798, 0.9256673533568882, 0.9182751515317992, 0.9330595449255722, 0.9211499003414256, 0.9420944517398028, 0.939219707825835, 0.9425051303370043, 0.9457905503024311, 0.9412730965036631, 0.9470225829608142, 0.9498973260914765, 0.951129358945686, 0.9560574915374819, 0.9531827491901249, 0.9572895245875176, 0.9531827557258292, 0.9531827487984722, 0.9585215576375534, 0.9560574913416555, 0.9568788463819687, 0.9478439385886065, 0.9519507163359154, 0.954414781848508, 0.9560574909500028, 0.9552361363013422, 0.9581108809986154, 0.9593429126778668, 0.9544147808693763, 0.9613963011598685, 0.9605749459237288, 0.9585215564625954, 0.957289525370823, 0.9527720707887497, 0.9535934271998474, 0.950718689821584, 0.9519507145734784, 0.957289524783344, 0.960164268305659, 0.9589322356472759, 0.9568788471652742, 0.9589322354514496, 0.9540041026638274, 0.9560574919291346, 0.956468169351378, 0.9552361363013422, 0.9593429136569984, 0.9601642673265273, 0.961806978582112, 0.9581108800194836, 0.9605749459237288, 0.9634496894460439, 0.9638603658891557, 0.9470225910631294, 0.9486653005562768, 0.9503080044928517, 0.9503080050803308, 0.9589322340806652, 0.9642710450738362, 0.9609856239334513, 0.9634496886627385, 0.9646817232793851, 0.9568788465777951, 0.9593429134611721, 0.9531827480151668, 0.9527720700054443, 0.9601642679140063, 0.956878854484284, 0.9589322354514496, 0.9585215580292061, 0.959753598006599, 0.9626283326432935, 0.9618069777988065, 0.9605749459237288, 0.9556468147027174, 0.9449691956537705, 0.9441478392426728, 0.954825457899967, 0.9523613941498116, 0.9560574909500028, 0.9577002025972402, 0.9626283338182516, 0.9638603672599401, 0.9605749447487708, 0.965092400897455, 0.9655030765572613, 0.9622176563960081, 0.9650924007016286, 0.9618069789737647, 0.9609856235417987, 0.9642710444863573, 0.9597535906875893, 0.9626283326432935, 0.9593429124820404, 0.9577002022055875, 0.9552361370846476, 0.9609856235417987, 0.9618069793654174, 0.9622176565918344, 0.9655030779280457, 0.9630390114363214, 0.9650923995266705, 0.9638603670641137, 0.9650924001141495, 0.9642710458571416, 0.9630390102613633, 0.9585215586166852, 0.9638603672599401, 0.9655030771447403, 0.9663244321850536, 0.9638603674557664, 0.9659137547628102, 0.962628334014078, 0.96016426771818, 0.9609856219751879, 0.9585215564625954, 0.9556468131361067, 0.9585215582250325, 0.9560574913416555, 0.9560574927124399, 0.955236137280474, 0.9564681683722462, 0.9675564662142211, 0.9671457880086722, 0.9634496890543912, 0.9671457887919777, 0.9671457874211932, 0.9597535889251521, 0.9511293664605222, 0.9552361370846476, 0.9416837743175592, 0.9535934246541049, 0.9560574919291346, 0.9630390116321478, 0.9650923979600597, 0.9638603662808083, 0.9634496898376966, 0.960985623150146, 0.9626283347973833, 0.9663244325767063, 0.96632443238088, 0.9655030777322193, 0.9642710446821835, 0.961806979169591, 0.9655030777322193, 0.9679671422656801, 0.9675564664100474, 0.9655030777322193, 0.9655030773405666, 0.9622176563960081, 0.9601642679140063, 0.9683778208628817, 0.9700205311393346, 0.9671457882044986, 0.9650924010932813, 0.9618069807362017, 0.9671457876170195, 0.9659137557419418, 0.9646817209294689, 0.963860366672461, 0.9589322346681443, 0.9675564666058738, 0.9667351111739079, 0.9696098535212648, 0.9691991759031949, 0.9708418877462587, 0.9659137557419418, 0.9605749461195552, 0.962628334014078, 0.9659137537836784, 0.9618069779946329, 0.9613963005723895, 0.9691991759031949, 0.9700205309435083, 0.9638603664766346, 0.9655030783196984, 0.9683778208628817, 0.9671457887919777, 0.9696098539129175, 0.9700205315309873, 0.9679671424615065, 0.970431208561578, 0.9605749455320761, 0.9548254650231505]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
