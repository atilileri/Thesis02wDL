{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf26.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 03:50:02 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '3Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '03', '02', '01', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001488273CE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000148F0C16EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6109, Accuracy:0.2025, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6069, Accuracy:0.2329, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6067, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6062, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6028, Accuracy:0.2308, Validation Loss:1.6064, Validation Accuracy:0.2397\n",
    "Epoch #22: Loss:1.6022, Accuracy:0.2366, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6025, Accuracy:0.2333, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6018, Accuracy:0.2337, Validation Loss:1.6062, Validation Accuracy:0.2430\n",
    "Epoch #25: Loss:1.6015, Accuracy:0.2374, Validation Loss:1.6075, Validation Accuracy:0.2430\n",
    "Epoch #26: Loss:1.6009, Accuracy:0.2419, Validation Loss:1.6075, Validation Accuracy:0.2463\n",
    "Epoch #27: Loss:1.6006, Accuracy:0.2353, Validation Loss:1.6070, Validation Accuracy:0.2299\n",
    "Epoch #28: Loss:1.6010, Accuracy:0.2341, Validation Loss:1.6066, Validation Accuracy:0.2447\n",
    "Epoch #29: Loss:1.6005, Accuracy:0.2398, Validation Loss:1.6071, Validation Accuracy:0.2381\n",
    "Epoch #30: Loss:1.6007, Accuracy:0.2423, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #31: Loss:1.6004, Accuracy:0.2427, Validation Loss:1.6077, Validation Accuracy:0.2414\n",
    "Epoch #32: Loss:1.5999, Accuracy:0.2411, Validation Loss:1.6067, Validation Accuracy:0.2397\n",
    "Epoch #33: Loss:1.5996, Accuracy:0.2353, Validation Loss:1.6083, Validation Accuracy:0.2348\n",
    "Epoch #34: Loss:1.6004, Accuracy:0.2345, Validation Loss:1.6065, Validation Accuracy:0.2447\n",
    "Epoch #35: Loss:1.5993, Accuracy:0.2435, Validation Loss:1.6078, Validation Accuracy:0.2381\n",
    "Epoch #36: Loss:1.5995, Accuracy:0.2456, Validation Loss:1.6077, Validation Accuracy:0.2414\n",
    "Epoch #37: Loss:1.5991, Accuracy:0.2423, Validation Loss:1.6077, Validation Accuracy:0.2447\n",
    "Epoch #38: Loss:1.5995, Accuracy:0.2394, Validation Loss:1.6077, Validation Accuracy:0.2381\n",
    "Epoch #39: Loss:1.6007, Accuracy:0.2292, Validation Loss:1.6070, Validation Accuracy:0.2332\n",
    "Epoch #40: Loss:1.5996, Accuracy:0.2419, Validation Loss:1.6094, Validation Accuracy:0.2397\n",
    "Epoch #41: Loss:1.6001, Accuracy:0.2415, Validation Loss:1.6073, Validation Accuracy:0.2332\n",
    "Epoch #42: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6067, Validation Accuracy:0.2447\n",
    "Epoch #43: Loss:1.6005, Accuracy:0.2378, Validation Loss:1.6067, Validation Accuracy:0.2348\n",
    "Epoch #44: Loss:1.6021, Accuracy:0.2230, Validation Loss:1.6102, Validation Accuracy:0.2184\n",
    "Epoch #45: Loss:1.6039, Accuracy:0.2279, Validation Loss:1.6083, Validation Accuracy:0.2332\n",
    "Epoch #46: Loss:1.6036, Accuracy:0.2337, Validation Loss:1.6110, Validation Accuracy:0.2332\n",
    "Epoch #47: Loss:1.6053, Accuracy:0.2320, Validation Loss:1.6073, Validation Accuracy:0.2332\n",
    "Epoch #48: Loss:1.6021, Accuracy:0.2279, Validation Loss:1.6072, Validation Accuracy:0.2085\n",
    "Epoch #49: Loss:1.6025, Accuracy:0.2242, Validation Loss:1.6068, Validation Accuracy:0.2315\n",
    "Epoch #50: Loss:1.6010, Accuracy:0.2308, Validation Loss:1.6069, Validation Accuracy:0.2414\n",
    "Epoch #51: Loss:1.6009, Accuracy:0.2390, Validation Loss:1.6077, Validation Accuracy:0.2365\n",
    "Epoch #52: Loss:1.6003, Accuracy:0.2411, Validation Loss:1.6075, Validation Accuracy:0.2430\n",
    "Epoch #53: Loss:1.6003, Accuracy:0.2374, Validation Loss:1.6079, Validation Accuracy:0.2430\n",
    "Epoch #54: Loss:1.5993, Accuracy:0.2394, Validation Loss:1.6085, Validation Accuracy:0.2348\n",
    "Epoch #55: Loss:1.5999, Accuracy:0.2448, Validation Loss:1.6096, Validation Accuracy:0.2365\n",
    "Epoch #56: Loss:1.6001, Accuracy:0.2402, Validation Loss:1.6089, Validation Accuracy:0.2414\n",
    "Epoch #57: Loss:1.5996, Accuracy:0.2345, Validation Loss:1.6093, Validation Accuracy:0.2348\n",
    "Epoch #58: Loss:1.5991, Accuracy:0.2448, Validation Loss:1.6114, Validation Accuracy:0.2299\n",
    "Epoch #59: Loss:1.5988, Accuracy:0.2468, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #60: Loss:1.5987, Accuracy:0.2378, Validation Loss:1.6071, Validation Accuracy:0.2315\n",
    "Epoch #61: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.6072, Validation Accuracy:0.2381\n",
    "Epoch #62: Loss:1.5986, Accuracy:0.2435, Validation Loss:1.6062, Validation Accuracy:0.2282\n",
    "Epoch #63: Loss:1.6025, Accuracy:0.2366, Validation Loss:1.6087, Validation Accuracy:0.2332\n",
    "Epoch #64: Loss:1.6005, Accuracy:0.2411, Validation Loss:1.6058, Validation Accuracy:0.2381\n",
    "Epoch #65: Loss:1.5987, Accuracy:0.2456, Validation Loss:1.6039, Validation Accuracy:0.2414\n",
    "Epoch #66: Loss:1.5989, Accuracy:0.2324, Validation Loss:1.6048, Validation Accuracy:0.2348\n",
    "Epoch #67: Loss:1.5978, Accuracy:0.2423, Validation Loss:1.6070, Validation Accuracy:0.2282\n",
    "Epoch #68: Loss:1.5973, Accuracy:0.2419, Validation Loss:1.6055, Validation Accuracy:0.2414\n",
    "Epoch #69: Loss:1.5968, Accuracy:0.2501, Validation Loss:1.6055, Validation Accuracy:0.2348\n",
    "Epoch #70: Loss:1.5972, Accuracy:0.2493, Validation Loss:1.6064, Validation Accuracy:0.2430\n",
    "Epoch #71: Loss:1.5971, Accuracy:0.2464, Validation Loss:1.6076, Validation Accuracy:0.2381\n",
    "Epoch #72: Loss:1.5972, Accuracy:0.2464, Validation Loss:1.6074, Validation Accuracy:0.2430\n",
    "Epoch #73: Loss:1.5971, Accuracy:0.2480, Validation Loss:1.6076, Validation Accuracy:0.2447\n",
    "Epoch #74: Loss:1.5963, Accuracy:0.2464, Validation Loss:1.6067, Validation Accuracy:0.2381\n",
    "Epoch #75: Loss:1.5962, Accuracy:0.2423, Validation Loss:1.6073, Validation Accuracy:0.2365\n",
    "Epoch #76: Loss:1.5964, Accuracy:0.2485, Validation Loss:1.6083, Validation Accuracy:0.2430\n",
    "Epoch #77: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.6097, Validation Accuracy:0.2365\n",
    "Epoch #78: Loss:1.5968, Accuracy:0.2460, Validation Loss:1.6098, Validation Accuracy:0.2365\n",
    "Epoch #79: Loss:1.5975, Accuracy:0.2448, Validation Loss:1.6098, Validation Accuracy:0.2365\n",
    "Epoch #80: Loss:1.5994, Accuracy:0.2378, Validation Loss:1.6113, Validation Accuracy:0.2266\n",
    "Epoch #81: Loss:1.5966, Accuracy:0.2497, Validation Loss:1.6101, Validation Accuracy:0.2365\n",
    "Epoch #82: Loss:1.5967, Accuracy:0.2493, Validation Loss:1.6095, Validation Accuracy:0.2397\n",
    "Epoch #83: Loss:1.5969, Accuracy:0.2493, Validation Loss:1.6102, Validation Accuracy:0.2397\n",
    "Epoch #84: Loss:1.5973, Accuracy:0.2468, Validation Loss:1.6090, Validation Accuracy:0.2348\n",
    "Epoch #85: Loss:1.5963, Accuracy:0.2517, Validation Loss:1.6097, Validation Accuracy:0.2397\n",
    "Epoch #86: Loss:1.5965, Accuracy:0.2480, Validation Loss:1.6088, Validation Accuracy:0.2397\n",
    "Epoch #87: Loss:1.5958, Accuracy:0.2493, Validation Loss:1.6088, Validation Accuracy:0.2332\n",
    "Epoch #88: Loss:1.5956, Accuracy:0.2501, Validation Loss:1.6086, Validation Accuracy:0.2365\n",
    "Epoch #89: Loss:1.5955, Accuracy:0.2493, Validation Loss:1.6095, Validation Accuracy:0.2414\n",
    "Epoch #90: Loss:1.5946, Accuracy:0.2485, Validation Loss:1.6085, Validation Accuracy:0.2397\n",
    "Epoch #91: Loss:1.5955, Accuracy:0.2526, Validation Loss:1.6090, Validation Accuracy:0.2250\n",
    "Epoch #92: Loss:1.5945, Accuracy:0.2493, Validation Loss:1.6098, Validation Accuracy:0.2266\n",
    "Epoch #93: Loss:1.5952, Accuracy:0.2476, Validation Loss:1.6111, Validation Accuracy:0.2299\n",
    "Epoch #94: Loss:1.5934, Accuracy:0.2530, Validation Loss:1.6095, Validation Accuracy:0.2266\n",
    "Epoch #95: Loss:1.5940, Accuracy:0.2517, Validation Loss:1.6095, Validation Accuracy:0.2282\n",
    "Epoch #96: Loss:1.5934, Accuracy:0.2448, Validation Loss:1.6108, Validation Accuracy:0.2332\n",
    "Epoch #97: Loss:1.5935, Accuracy:0.2493, Validation Loss:1.6101, Validation Accuracy:0.2250\n",
    "Epoch #98: Loss:1.5939, Accuracy:0.2476, Validation Loss:1.6098, Validation Accuracy:0.2266\n",
    "Epoch #99: Loss:1.5924, Accuracy:0.2526, Validation Loss:1.6081, Validation Accuracy:0.2299\n",
    "Epoch #100: Loss:1.5924, Accuracy:0.2513, Validation Loss:1.6096, Validation Accuracy:0.2315\n",
    "Epoch #101: Loss:1.5928, Accuracy:0.2464, Validation Loss:1.6084, Validation Accuracy:0.2250\n",
    "Epoch #102: Loss:1.5924, Accuracy:0.2530, Validation Loss:1.6105, Validation Accuracy:0.2167\n",
    "Epoch #103: Loss:1.5925, Accuracy:0.2472, Validation Loss:1.6135, Validation Accuracy:0.2151\n",
    "Epoch #104: Loss:1.5914, Accuracy:0.2501, Validation Loss:1.6114, Validation Accuracy:0.2184\n",
    "Epoch #105: Loss:1.5917, Accuracy:0.2505, Validation Loss:1.6136, Validation Accuracy:0.2233\n",
    "Epoch #106: Loss:1.5918, Accuracy:0.2517, Validation Loss:1.6120, Validation Accuracy:0.2233\n",
    "Epoch #107: Loss:1.5918, Accuracy:0.2501, Validation Loss:1.6144, Validation Accuracy:0.2217\n",
    "Epoch #108: Loss:1.5912, Accuracy:0.2538, Validation Loss:1.6139, Validation Accuracy:0.2085\n",
    "Epoch #109: Loss:1.5941, Accuracy:0.2460, Validation Loss:1.6112, Validation Accuracy:0.2184\n",
    "Epoch #110: Loss:1.5987, Accuracy:0.2439, Validation Loss:1.6138, Validation Accuracy:0.2217\n",
    "Epoch #111: Loss:1.6019, Accuracy:0.2390, Validation Loss:1.6117, Validation Accuracy:0.2102\n",
    "Epoch #112: Loss:1.5987, Accuracy:0.2308, Validation Loss:1.6130, Validation Accuracy:0.2200\n",
    "Epoch #113: Loss:1.5969, Accuracy:0.2349, Validation Loss:1.6102, Validation Accuracy:0.2135\n",
    "Epoch #114: Loss:1.5977, Accuracy:0.2357, Validation Loss:1.6106, Validation Accuracy:0.2135\n",
    "Epoch #115: Loss:1.5970, Accuracy:0.2419, Validation Loss:1.6104, Validation Accuracy:0.2315\n",
    "Epoch #116: Loss:1.5955, Accuracy:0.2435, Validation Loss:1.6116, Validation Accuracy:0.2167\n",
    "Epoch #117: Loss:1.5943, Accuracy:0.2480, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #118: Loss:1.5952, Accuracy:0.2485, Validation Loss:1.6095, Validation Accuracy:0.2397\n",
    "Epoch #119: Loss:1.5916, Accuracy:0.2480, Validation Loss:1.6116, Validation Accuracy:0.2167\n",
    "Epoch #120: Loss:1.5918, Accuracy:0.2497, Validation Loss:1.6072, Validation Accuracy:0.2282\n",
    "Epoch #121: Loss:1.5920, Accuracy:0.2493, Validation Loss:1.6100, Validation Accuracy:0.2217\n",
    "Epoch #122: Loss:1.5924, Accuracy:0.2480, Validation Loss:1.6131, Validation Accuracy:0.2365\n",
    "Epoch #123: Loss:1.5916, Accuracy:0.2460, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #124: Loss:1.5914, Accuracy:0.2472, Validation Loss:1.6113, Validation Accuracy:0.2233\n",
    "Epoch #125: Loss:1.5916, Accuracy:0.2480, Validation Loss:1.6100, Validation Accuracy:0.2299\n",
    "Epoch #126: Loss:1.5916, Accuracy:0.2480, Validation Loss:1.6127, Validation Accuracy:0.2348\n",
    "Epoch #127: Loss:1.5917, Accuracy:0.2522, Validation Loss:1.6109, Validation Accuracy:0.2315\n",
    "Epoch #128: Loss:1.5913, Accuracy:0.2448, Validation Loss:1.6143, Validation Accuracy:0.2315\n",
    "Epoch #129: Loss:1.5906, Accuracy:0.2489, Validation Loss:1.6121, Validation Accuracy:0.2315\n",
    "Epoch #130: Loss:1.5904, Accuracy:0.2485, Validation Loss:1.6153, Validation Accuracy:0.2282\n",
    "Epoch #131: Loss:1.5894, Accuracy:0.2542, Validation Loss:1.6118, Validation Accuracy:0.2233\n",
    "Epoch #132: Loss:1.5890, Accuracy:0.2534, Validation Loss:1.6128, Validation Accuracy:0.2250\n",
    "Epoch #133: Loss:1.5874, Accuracy:0.2538, Validation Loss:1.6106, Validation Accuracy:0.2233\n",
    "Epoch #134: Loss:1.5880, Accuracy:0.2480, Validation Loss:1.6128, Validation Accuracy:0.2332\n",
    "Epoch #135: Loss:1.5890, Accuracy:0.2485, Validation Loss:1.6105, Validation Accuracy:0.2299\n",
    "Epoch #136: Loss:1.5863, Accuracy:0.2591, Validation Loss:1.6099, Validation Accuracy:0.2332\n",
    "Epoch #137: Loss:1.5880, Accuracy:0.2505, Validation Loss:1.6099, Validation Accuracy:0.2348\n",
    "Epoch #138: Loss:1.5866, Accuracy:0.2530, Validation Loss:1.6098, Validation Accuracy:0.2332\n",
    "Epoch #139: Loss:1.5864, Accuracy:0.2538, Validation Loss:1.6115, Validation Accuracy:0.2348\n",
    "Epoch #140: Loss:1.5876, Accuracy:0.2509, Validation Loss:1.6152, Validation Accuracy:0.2282\n",
    "Epoch #141: Loss:1.5864, Accuracy:0.2530, Validation Loss:1.6167, Validation Accuracy:0.2233\n",
    "Epoch #142: Loss:1.5863, Accuracy:0.2513, Validation Loss:1.6168, Validation Accuracy:0.2217\n",
    "Epoch #143: Loss:1.5861, Accuracy:0.2501, Validation Loss:1.6141, Validation Accuracy:0.2282\n",
    "Epoch #144: Loss:1.5870, Accuracy:0.2480, Validation Loss:1.6191, Validation Accuracy:0.2200\n",
    "Epoch #145: Loss:1.5883, Accuracy:0.2485, Validation Loss:1.6163, Validation Accuracy:0.2282\n",
    "Epoch #146: Loss:1.5880, Accuracy:0.2485, Validation Loss:1.6150, Validation Accuracy:0.2233\n",
    "Epoch #147: Loss:1.5920, Accuracy:0.2448, Validation Loss:1.6156, Validation Accuracy:0.2282\n",
    "Epoch #148: Loss:1.5919, Accuracy:0.2341, Validation Loss:1.6146, Validation Accuracy:0.2299\n",
    "Epoch #149: Loss:1.5916, Accuracy:0.2370, Validation Loss:1.6136, Validation Accuracy:0.2348\n",
    "Epoch #150: Loss:1.5881, Accuracy:0.2464, Validation Loss:1.6182, Validation Accuracy:0.2184\n",
    "Epoch #151: Loss:1.5891, Accuracy:0.2489, Validation Loss:1.6115, Validation Accuracy:0.2282\n",
    "Epoch #152: Loss:1.5857, Accuracy:0.2530, Validation Loss:1.6139, Validation Accuracy:0.2217\n",
    "Epoch #153: Loss:1.5864, Accuracy:0.2468, Validation Loss:1.6148, Validation Accuracy:0.2217\n",
    "Epoch #154: Loss:1.5849, Accuracy:0.2513, Validation Loss:1.6117, Validation Accuracy:0.2233\n",
    "Epoch #155: Loss:1.5865, Accuracy:0.2522, Validation Loss:1.6143, Validation Accuracy:0.2233\n",
    "Epoch #156: Loss:1.5861, Accuracy:0.2493, Validation Loss:1.6155, Validation Accuracy:0.2266\n",
    "Epoch #157: Loss:1.5848, Accuracy:0.2542, Validation Loss:1.6186, Validation Accuracy:0.2266\n",
    "Epoch #158: Loss:1.5844, Accuracy:0.2530, Validation Loss:1.6192, Validation Accuracy:0.2315\n",
    "Epoch #159: Loss:1.5842, Accuracy:0.2546, Validation Loss:1.6182, Validation Accuracy:0.2299\n",
    "Epoch #160: Loss:1.5860, Accuracy:0.2444, Validation Loss:1.6183, Validation Accuracy:0.2217\n",
    "Epoch #161: Loss:1.5838, Accuracy:0.2530, Validation Loss:1.6179, Validation Accuracy:0.2266\n",
    "Epoch #162: Loss:1.5871, Accuracy:0.2489, Validation Loss:1.6213, Validation Accuracy:0.2233\n",
    "Epoch #163: Loss:1.5889, Accuracy:0.2439, Validation Loss:1.6187, Validation Accuracy:0.2250\n",
    "Epoch #164: Loss:1.5909, Accuracy:0.2476, Validation Loss:1.6187, Validation Accuracy:0.2282\n",
    "Epoch #165: Loss:1.5844, Accuracy:0.2530, Validation Loss:1.6226, Validation Accuracy:0.2200\n",
    "Epoch #166: Loss:1.5858, Accuracy:0.2460, Validation Loss:1.6162, Validation Accuracy:0.2167\n",
    "Epoch #167: Loss:1.5862, Accuracy:0.2497, Validation Loss:1.6179, Validation Accuracy:0.2184\n",
    "Epoch #168: Loss:1.5838, Accuracy:0.2439, Validation Loss:1.6205, Validation Accuracy:0.2217\n",
    "Epoch #169: Loss:1.5834, Accuracy:0.2530, Validation Loss:1.6182, Validation Accuracy:0.2167\n",
    "Epoch #170: Loss:1.5831, Accuracy:0.2546, Validation Loss:1.6183, Validation Accuracy:0.2250\n",
    "Epoch #171: Loss:1.5829, Accuracy:0.2546, Validation Loss:1.6192, Validation Accuracy:0.2250\n",
    "Epoch #172: Loss:1.5827, Accuracy:0.2530, Validation Loss:1.6210, Validation Accuracy:0.2250\n",
    "Epoch #173: Loss:1.5846, Accuracy:0.2534, Validation Loss:1.6215, Validation Accuracy:0.2217\n",
    "Epoch #174: Loss:1.5827, Accuracy:0.2550, Validation Loss:1.6170, Validation Accuracy:0.2217\n",
    "Epoch #175: Loss:1.5814, Accuracy:0.2595, Validation Loss:1.6143, Validation Accuracy:0.2282\n",
    "Epoch #176: Loss:1.5821, Accuracy:0.2587, Validation Loss:1.6132, Validation Accuracy:0.2233\n",
    "Epoch #177: Loss:1.5801, Accuracy:0.2571, Validation Loss:1.6125, Validation Accuracy:0.2200\n",
    "Epoch #178: Loss:1.5797, Accuracy:0.2579, Validation Loss:1.6133, Validation Accuracy:0.2266\n",
    "Epoch #179: Loss:1.5808, Accuracy:0.2563, Validation Loss:1.6151, Validation Accuracy:0.2348\n",
    "Epoch #180: Loss:1.5819, Accuracy:0.2612, Validation Loss:1.6168, Validation Accuracy:0.2250\n",
    "Epoch #181: Loss:1.5816, Accuracy:0.2542, Validation Loss:1.6171, Validation Accuracy:0.2217\n",
    "Epoch #182: Loss:1.5795, Accuracy:0.2706, Validation Loss:1.6152, Validation Accuracy:0.2266\n",
    "Epoch #183: Loss:1.5791, Accuracy:0.2579, Validation Loss:1.6156, Validation Accuracy:0.2348\n",
    "Epoch #184: Loss:1.5812, Accuracy:0.2489, Validation Loss:1.6134, Validation Accuracy:0.2299\n",
    "Epoch #185: Loss:1.5829, Accuracy:0.2501, Validation Loss:1.6172, Validation Accuracy:0.2282\n",
    "Epoch #186: Loss:1.5800, Accuracy:0.2439, Validation Loss:1.6164, Validation Accuracy:0.2332\n",
    "Epoch #187: Loss:1.5805, Accuracy:0.2579, Validation Loss:1.6224, Validation Accuracy:0.2250\n",
    "Epoch #188: Loss:1.5796, Accuracy:0.2575, Validation Loss:1.6236, Validation Accuracy:0.2282\n",
    "Epoch #189: Loss:1.5797, Accuracy:0.2575, Validation Loss:1.6222, Validation Accuracy:0.2299\n",
    "Epoch #190: Loss:1.5803, Accuracy:0.2571, Validation Loss:1.6198, Validation Accuracy:0.2266\n",
    "Epoch #191: Loss:1.5811, Accuracy:0.2567, Validation Loss:1.6176, Validation Accuracy:0.2151\n",
    "Epoch #192: Loss:1.5836, Accuracy:0.2624, Validation Loss:1.6147, Validation Accuracy:0.2282\n",
    "Epoch #193: Loss:1.5847, Accuracy:0.2493, Validation Loss:1.6140, Validation Accuracy:0.2299\n",
    "Epoch #194: Loss:1.5810, Accuracy:0.2579, Validation Loss:1.6155, Validation Accuracy:0.2167\n",
    "Epoch #195: Loss:1.5801, Accuracy:0.2694, Validation Loss:1.6132, Validation Accuracy:0.2167\n",
    "Epoch #196: Loss:1.5800, Accuracy:0.2632, Validation Loss:1.6162, Validation Accuracy:0.2315\n",
    "Epoch #197: Loss:1.5824, Accuracy:0.2534, Validation Loss:1.6175, Validation Accuracy:0.2250\n",
    "Epoch #198: Loss:1.5815, Accuracy:0.2550, Validation Loss:1.6189, Validation Accuracy:0.2282\n",
    "Epoch #199: Loss:1.5793, Accuracy:0.2591, Validation Loss:1.6218, Validation Accuracy:0.2135\n",
    "Epoch #200: Loss:1.5803, Accuracy:0.2628, Validation Loss:1.6208, Validation Accuracy:0.2315\n",
    "Epoch #201: Loss:1.5793, Accuracy:0.2595, Validation Loss:1.6217, Validation Accuracy:0.2282\n",
    "Epoch #202: Loss:1.5784, Accuracy:0.2587, Validation Loss:1.6228, Validation Accuracy:0.2315\n",
    "Epoch #203: Loss:1.5780, Accuracy:0.2567, Validation Loss:1.6208, Validation Accuracy:0.2282\n",
    "Epoch #204: Loss:1.5772, Accuracy:0.2587, Validation Loss:1.6185, Validation Accuracy:0.2315\n",
    "Epoch #205: Loss:1.5776, Accuracy:0.2571, Validation Loss:1.6170, Validation Accuracy:0.2233\n",
    "Epoch #206: Loss:1.5755, Accuracy:0.2571, Validation Loss:1.6161, Validation Accuracy:0.2365\n",
    "Epoch #207: Loss:1.5774, Accuracy:0.2612, Validation Loss:1.6195, Validation Accuracy:0.2217\n",
    "Epoch #208: Loss:1.5761, Accuracy:0.2645, Validation Loss:1.6221, Validation Accuracy:0.2282\n",
    "Epoch #209: Loss:1.5771, Accuracy:0.2542, Validation Loss:1.6246, Validation Accuracy:0.2217\n",
    "Epoch #210: Loss:1.5784, Accuracy:0.2612, Validation Loss:1.6264, Validation Accuracy:0.2217\n",
    "Epoch #211: Loss:1.5782, Accuracy:0.2608, Validation Loss:1.6354, Validation Accuracy:0.2184\n",
    "Epoch #212: Loss:1.5793, Accuracy:0.2600, Validation Loss:1.6253, Validation Accuracy:0.2233\n",
    "Epoch #213: Loss:1.5797, Accuracy:0.2571, Validation Loss:1.6219, Validation Accuracy:0.2003\n",
    "Epoch #214: Loss:1.5773, Accuracy:0.2657, Validation Loss:1.6290, Validation Accuracy:0.2184\n",
    "Epoch #215: Loss:1.5786, Accuracy:0.2567, Validation Loss:1.6191, Validation Accuracy:0.2250\n",
    "Epoch #216: Loss:1.5788, Accuracy:0.2583, Validation Loss:1.6229, Validation Accuracy:0.2282\n",
    "Epoch #217: Loss:1.5763, Accuracy:0.2628, Validation Loss:1.6207, Validation Accuracy:0.2266\n",
    "Epoch #218: Loss:1.5755, Accuracy:0.2579, Validation Loss:1.6195, Validation Accuracy:0.2250\n",
    "Epoch #219: Loss:1.5750, Accuracy:0.2595, Validation Loss:1.6235, Validation Accuracy:0.2250\n",
    "Epoch #220: Loss:1.5767, Accuracy:0.2612, Validation Loss:1.6236, Validation Accuracy:0.2135\n",
    "Epoch #221: Loss:1.5776, Accuracy:0.2554, Validation Loss:1.6312, Validation Accuracy:0.2151\n",
    "Epoch #222: Loss:1.5789, Accuracy:0.2546, Validation Loss:1.6292, Validation Accuracy:0.2102\n",
    "Epoch #223: Loss:1.5843, Accuracy:0.2505, Validation Loss:1.6329, Validation Accuracy:0.2053\n",
    "Epoch #224: Loss:1.5843, Accuracy:0.2460, Validation Loss:1.6231, Validation Accuracy:0.1888\n",
    "Epoch #225: Loss:1.5829, Accuracy:0.2489, Validation Loss:1.6203, Validation Accuracy:0.1938\n",
    "Epoch #226: Loss:1.5803, Accuracy:0.2522, Validation Loss:1.6218, Validation Accuracy:0.2003\n",
    "Epoch #227: Loss:1.5809, Accuracy:0.2567, Validation Loss:1.6201, Validation Accuracy:0.1839\n",
    "Epoch #228: Loss:1.5808, Accuracy:0.2542, Validation Loss:1.6197, Validation Accuracy:0.2151\n",
    "Epoch #229: Loss:1.5807, Accuracy:0.2517, Validation Loss:1.6190, Validation Accuracy:0.2135\n",
    "Epoch #230: Loss:1.5802, Accuracy:0.2542, Validation Loss:1.6201, Validation Accuracy:0.2282\n",
    "Epoch #231: Loss:1.5790, Accuracy:0.2563, Validation Loss:1.6245, Validation Accuracy:0.2200\n",
    "Epoch #232: Loss:1.5780, Accuracy:0.2591, Validation Loss:1.6212, Validation Accuracy:0.2118\n",
    "Epoch #233: Loss:1.5781, Accuracy:0.2669, Validation Loss:1.6238, Validation Accuracy:0.2053\n",
    "Epoch #234: Loss:1.5780, Accuracy:0.2604, Validation Loss:1.6247, Validation Accuracy:0.2102\n",
    "Epoch #235: Loss:1.5758, Accuracy:0.2682, Validation Loss:1.6242, Validation Accuracy:0.2151\n",
    "Epoch #236: Loss:1.5769, Accuracy:0.2678, Validation Loss:1.6243, Validation Accuracy:0.2118\n",
    "Epoch #237: Loss:1.5753, Accuracy:0.2645, Validation Loss:1.6251, Validation Accuracy:0.2102\n",
    "Epoch #238: Loss:1.5740, Accuracy:0.2674, Validation Loss:1.6244, Validation Accuracy:0.2151\n",
    "Epoch #239: Loss:1.5741, Accuracy:0.2653, Validation Loss:1.6270, Validation Accuracy:0.2102\n",
    "Epoch #240: Loss:1.5736, Accuracy:0.2710, Validation Loss:1.6261, Validation Accuracy:0.2102\n",
    "Epoch #241: Loss:1.5751, Accuracy:0.2702, Validation Loss:1.6276, Validation Accuracy:0.2118\n",
    "Epoch #242: Loss:1.5773, Accuracy:0.2674, Validation Loss:1.6267, Validation Accuracy:0.2167\n",
    "Epoch #243: Loss:1.5754, Accuracy:0.2674, Validation Loss:1.6298, Validation Accuracy:0.2102\n",
    "Epoch #244: Loss:1.5772, Accuracy:0.2637, Validation Loss:1.6254, Validation Accuracy:0.2167\n",
    "Epoch #245: Loss:1.5768, Accuracy:0.2624, Validation Loss:1.6303, Validation Accuracy:0.2233\n",
    "Epoch #246: Loss:1.5769, Accuracy:0.2653, Validation Loss:1.6248, Validation Accuracy:0.2233\n",
    "Epoch #247: Loss:1.5754, Accuracy:0.2649, Validation Loss:1.6271, Validation Accuracy:0.2167\n",
    "Epoch #248: Loss:1.5762, Accuracy:0.2657, Validation Loss:1.6273, Validation Accuracy:0.2118\n",
    "Epoch #249: Loss:1.5752, Accuracy:0.2665, Validation Loss:1.6276, Validation Accuracy:0.2233\n",
    "Epoch #250: Loss:1.5750, Accuracy:0.2678, Validation Loss:1.6343, Validation Accuracy:0.2053\n",
    "Epoch #251: Loss:1.5782, Accuracy:0.2595, Validation Loss:1.6316, Validation Accuracy:0.2085\n",
    "Epoch #252: Loss:1.5784, Accuracy:0.2665, Validation Loss:1.6285, Validation Accuracy:0.2299\n",
    "Epoch #253: Loss:1.5788, Accuracy:0.2452, Validation Loss:1.6285, Validation Accuracy:0.2250\n",
    "Epoch #254: Loss:1.5775, Accuracy:0.2637, Validation Loss:1.6296, Validation Accuracy:0.2085\n",
    "Epoch #255: Loss:1.5751, Accuracy:0.2661, Validation Loss:1.6255, Validation Accuracy:0.2135\n",
    "Epoch #256: Loss:1.5751, Accuracy:0.2686, Validation Loss:1.6235, Validation Accuracy:0.2200\n",
    "Epoch #257: Loss:1.5819, Accuracy:0.2608, Validation Loss:1.6220, Validation Accuracy:0.2135\n",
    "Epoch #258: Loss:1.5750, Accuracy:0.2715, Validation Loss:1.6270, Validation Accuracy:0.2085\n",
    "Epoch #259: Loss:1.5782, Accuracy:0.2616, Validation Loss:1.6221, Validation Accuracy:0.2135\n",
    "Epoch #260: Loss:1.5787, Accuracy:0.2632, Validation Loss:1.6251, Validation Accuracy:0.2135\n",
    "Epoch #261: Loss:1.5754, Accuracy:0.2702, Validation Loss:1.6257, Validation Accuracy:0.2217\n",
    "Epoch #262: Loss:1.5750, Accuracy:0.2715, Validation Loss:1.6251, Validation Accuracy:0.2151\n",
    "Epoch #263: Loss:1.5742, Accuracy:0.2756, Validation Loss:1.6251, Validation Accuracy:0.2184\n",
    "Epoch #264: Loss:1.5753, Accuracy:0.2665, Validation Loss:1.6210, Validation Accuracy:0.2250\n",
    "Epoch #265: Loss:1.5772, Accuracy:0.2674, Validation Loss:1.6195, Validation Accuracy:0.2365\n",
    "Epoch #266: Loss:1.5781, Accuracy:0.2600, Validation Loss:1.6209, Validation Accuracy:0.2315\n",
    "Epoch #267: Loss:1.5781, Accuracy:0.2620, Validation Loss:1.6187, Validation Accuracy:0.2332\n",
    "Epoch #268: Loss:1.5767, Accuracy:0.2624, Validation Loss:1.6237, Validation Accuracy:0.2167\n",
    "Epoch #269: Loss:1.5719, Accuracy:0.2653, Validation Loss:1.6234, Validation Accuracy:0.2151\n",
    "Epoch #270: Loss:1.5727, Accuracy:0.2715, Validation Loss:1.6285, Validation Accuracy:0.1954\n",
    "Epoch #271: Loss:1.5731, Accuracy:0.2702, Validation Loss:1.6238, Validation Accuracy:0.1938\n",
    "Epoch #272: Loss:1.5775, Accuracy:0.2682, Validation Loss:1.6224, Validation Accuracy:0.2151\n",
    "Epoch #273: Loss:1.5821, Accuracy:0.2649, Validation Loss:1.6195, Validation Accuracy:0.2184\n",
    "Epoch #274: Loss:1.5767, Accuracy:0.2698, Validation Loss:1.6218, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5777, Accuracy:0.2645, Validation Loss:1.6177, Validation Accuracy:0.2118\n",
    "Epoch #276: Loss:1.5759, Accuracy:0.2661, Validation Loss:1.6204, Validation Accuracy:0.2299\n",
    "Epoch #277: Loss:1.5782, Accuracy:0.2645, Validation Loss:1.6176, Validation Accuracy:0.2266\n",
    "Epoch #278: Loss:1.5770, Accuracy:0.2678, Validation Loss:1.6162, Validation Accuracy:0.2167\n",
    "Epoch #279: Loss:1.5731, Accuracy:0.2793, Validation Loss:1.6203, Validation Accuracy:0.2299\n",
    "Epoch #280: Loss:1.5745, Accuracy:0.2719, Validation Loss:1.6186, Validation Accuracy:0.2200\n",
    "Epoch #281: Loss:1.5747, Accuracy:0.2682, Validation Loss:1.6217, Validation Accuracy:0.2200\n",
    "Epoch #282: Loss:1.5734, Accuracy:0.2653, Validation Loss:1.6194, Validation Accuracy:0.2151\n",
    "Epoch #283: Loss:1.5725, Accuracy:0.2665, Validation Loss:1.6185, Validation Accuracy:0.2217\n",
    "Epoch #284: Loss:1.5735, Accuracy:0.2649, Validation Loss:1.6212, Validation Accuracy:0.2266\n",
    "Epoch #285: Loss:1.5711, Accuracy:0.2719, Validation Loss:1.6198, Validation Accuracy:0.2233\n",
    "Epoch #286: Loss:1.5719, Accuracy:0.2739, Validation Loss:1.6222, Validation Accuracy:0.2184\n",
    "Epoch #287: Loss:1.5701, Accuracy:0.2715, Validation Loss:1.6209, Validation Accuracy:0.2184\n",
    "Epoch #288: Loss:1.5701, Accuracy:0.2694, Validation Loss:1.6261, Validation Accuracy:0.2151\n",
    "Epoch #289: Loss:1.5689, Accuracy:0.2694, Validation Loss:1.6247, Validation Accuracy:0.2135\n",
    "Epoch #290: Loss:1.5695, Accuracy:0.2702, Validation Loss:1.6235, Validation Accuracy:0.2217\n",
    "Epoch #291: Loss:1.5685, Accuracy:0.2739, Validation Loss:1.6291, Validation Accuracy:0.2069\n",
    "Epoch #292: Loss:1.5682, Accuracy:0.2768, Validation Loss:1.6274, Validation Accuracy:0.2282\n",
    "Epoch #293: Loss:1.5677, Accuracy:0.2743, Validation Loss:1.6269, Validation Accuracy:0.2233\n",
    "Epoch #294: Loss:1.5685, Accuracy:0.2756, Validation Loss:1.6259, Validation Accuracy:0.2282\n",
    "Epoch #295: Loss:1.5681, Accuracy:0.2727, Validation Loss:1.6323, Validation Accuracy:0.2217\n",
    "Epoch #296: Loss:1.5724, Accuracy:0.2690, Validation Loss:1.6361, Validation Accuracy:0.1987\n",
    "Epoch #297: Loss:1.5721, Accuracy:0.2727, Validation Loss:1.6334, Validation Accuracy:0.2020\n",
    "Epoch #298: Loss:1.5717, Accuracy:0.2702, Validation Loss:1.6311, Validation Accuracy:0.2020\n",
    "Epoch #299: Loss:1.5712, Accuracy:0.2739, Validation Loss:1.6258, Validation Accuracy:0.2053\n",
    "Epoch #300: Loss:1.5688, Accuracy:0.2719, Validation Loss:1.6211, Validation Accuracy:0.2085\n",
    "\n",
    "Test:\n",
    "Test Loss:1.62109911, Accuracy:0.2085\n",
    "Labels: ['04', '03', '02', '01', '05']\n",
    "Confusion Matrix:\n",
    "      04  03  02  01  05\n",
    "t:04  19   4   0  40  49\n",
    "t:03  14   7   0  50  44\n",
    "t:02  16   8   1  39  50\n",
    "t:01  19   1   2  37  67\n",
    "t:05  17   4   1  57  63\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.22      0.17      0.19       112\n",
    "          03       0.29      0.06      0.10       115\n",
    "          02       0.25      0.01      0.02       114\n",
    "          01       0.17      0.29      0.21       126\n",
    "          05       0.23      0.44      0.30       142\n",
    "\n",
    "    accuracy                           0.21       609\n",
    "   macro avg       0.23      0.20      0.17       609\n",
    "weighted avg       0.23      0.21      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 04:30:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6054765136762597, 1.6062692902945532, 1.6053855679501061, 1.6058231876009987, 1.6055057636035488, 1.6053115951603856, 1.6054252983118318, 1.6053031314965736, 1.6055310561347673, 1.6055997974179648, 1.6052256100283468, 1.60514943450934, 1.6051182016754777, 1.6051355796103015, 1.6050077316600506, 1.6052258960131942, 1.606630651234406, 1.6062529206471685, 1.6058624728364115, 1.6057774398127214, 1.6063972877947177, 1.6055719116442702, 1.6054748666697536, 1.606239431401583, 1.6074638955894558, 1.6075009773125986, 1.6069926450209469, 1.6066087907171014, 1.6071086838131858, 1.607570483375261, 1.6076542346544063, 1.6067047712250884, 1.6082536080004939, 1.60653768912912, 1.6077785227686314, 1.6076946599142892, 1.6077166341599964, 1.607689921100347, 1.6070147163566502, 1.6093670638715496, 1.6073402019557108, 1.6067483051265599, 1.6066700312108633, 1.6101515173716303, 1.608264670191923, 1.6110112236442629, 1.6072698222788293, 1.6072251356294003, 1.6067615406853812, 1.6068588205550496, 1.6076763701947843, 1.6074695966905366, 1.6079349451268639, 1.6084890363643127, 1.6095664050974479, 1.6088519180545275, 1.6092783118703682, 1.6113506810026998, 1.6094071686952964, 1.6070871893408263, 1.6071866124330092, 1.6062406340647606, 1.6087483555225317, 1.6058289946006437, 1.6039268095505061, 1.6048177919168582, 1.6070305907667564, 1.6055158008691321, 1.6054612928619134, 1.6063674494550733, 1.6075624276460294, 1.607432870050565, 1.607557827028735, 1.6067170681820322, 1.6073082460362726, 1.6083091597251704, 1.6097028026439872, 1.609847552279142, 1.6098493443334043, 1.611312322232915, 1.61013984621452, 1.6095491199462089, 1.6101595137898361, 1.6089763721613266, 1.6097424216262617, 1.608788154786835, 1.608840116139116, 1.608629955446779, 1.609519762554388, 1.608487815692507, 1.6089810618430327, 1.6098270283152514, 1.611119708991403, 1.6095178403290622, 1.6094518287232749, 1.6108465319979564, 1.6101264215651012, 1.60984139920064, 1.608083168861314, 1.6095979609121438, 1.6083970803932603, 1.6104732252694116, 1.613532036200337, 1.611385645341795, 1.6135863809554252, 1.6120292748919458, 1.6143767851326853, 1.6138910241119184, 1.6111988608277295, 1.613844729800921, 1.6116656053242424, 1.6130045376387723, 1.6102231232012043, 1.6105713762086014, 1.6104343209556367, 1.611624763125465, 1.6094474076050256, 1.6094708796988175, 1.6116078142657853, 1.6072294404745493, 1.610032948758606, 1.613082805290598, 1.6096937490018521, 1.6113265543344182, 1.6099849772962247, 1.6127193852989936, 1.610870615406381, 1.6143314979346515, 1.6120905539476618, 1.6152577224036155, 1.6118254733986064, 1.6128420217088095, 1.6105813800016255, 1.6128289754363312, 1.6104608621503331, 1.6098571706483713, 1.609864836647397, 1.6098220596955524, 1.6114900765943605, 1.615196364266532, 1.6166500332711757, 1.6167545416476496, 1.614099696351977, 1.6191167473205792, 1.6163180261997168, 1.6150050730932326, 1.6155778138312604, 1.61456652854268, 1.6136417678620036, 1.6181849861771407, 1.6115188553611242, 1.6138904922701456, 1.6148236702228416, 1.6117080234737426, 1.6143227001324858, 1.6154578638389976, 1.6185696862992787, 1.6192145144019416, 1.6181689907960313, 1.6183265294934728, 1.6179032924727266, 1.6212764473384238, 1.6186573738339303, 1.61865747366437, 1.6226453189975132, 1.6161952981807914, 1.6179264913051588, 1.6204873072885724, 1.618175266411504, 1.6182590732825017, 1.619186125952622, 1.620997532052164, 1.6214849173729056, 1.6169723522878436, 1.61433440163022, 1.6132014398700107, 1.6124681424233323, 1.6133144692638628, 1.6151232345546604, 1.6168153765557827, 1.6170858426634314, 1.6152389350978809, 1.61559823561576, 1.613363587406077, 1.6171880966336856, 1.616393540880363, 1.6224236658641271, 1.623627644650063, 1.6221816255932762, 1.6197630288369942, 1.6176176476361128, 1.6146647937975103, 1.6140138507868074, 1.6154554886575208, 1.613216231217721, 1.6161660682195904, 1.6174528620317457, 1.6188527347614807, 1.621843115058047, 1.62081839435402, 1.6217411877877999, 1.6228117275316336, 1.6208241469362883, 1.6185250611140811, 1.6169654150510264, 1.616106537175296, 1.6195204751244907, 1.6221190966996066, 1.6245920444748476, 1.6263554849843869, 1.6354244557899011, 1.6253430029050078, 1.6218864301155353, 1.6289997989516736, 1.6191346461354021, 1.6228828359707235, 1.6206742435057566, 1.6194608683265097, 1.6235123635904347, 1.6236177579131228, 1.6311875491698191, 1.6292017978204687, 1.6328550759207439, 1.6231234649132038, 1.6203356699403284, 1.6217677587358823, 1.620103227876873, 1.6196770711093897, 1.6190102893143452, 1.6200691095518165, 1.6244691506590945, 1.6212342666287727, 1.6238469145763879, 1.6246918136458874, 1.6242179179622231, 1.6243141590080825, 1.6251462425895904, 1.6243650472810116, 1.6270321912953418, 1.6261036609389707, 1.627631674650659, 1.626711592885661, 1.6298292855715322, 1.6253827773095744, 1.6302746076301988, 1.624842244220289, 1.6270815803499645, 1.6272900554738412, 1.6276388387570435, 1.6343429397871145, 1.6316064363238456, 1.6285388154545049, 1.6284784973157058, 1.629559669588587, 1.625507145679643, 1.6234864695318814, 1.6220386132035154, 1.6269603742558771, 1.6221285884212, 1.625097189239289, 1.6256525530212227, 1.6250859352168192, 1.6250763242859363, 1.6210365266048263, 1.6195030488404147, 1.6209200706779467, 1.6186525013254978, 1.623742607035269, 1.6234271857147342, 1.6284545180441319, 1.6238230887696465, 1.6224027190889632, 1.6195065358589436, 1.6217953982611595, 1.617690178952585, 1.6203906236610977, 1.6176184386455368, 1.6162469680673384, 1.6202734605040652, 1.6186412864522197, 1.6216999589907517, 1.6194026779462942, 1.6184740961087356, 1.6211564597629367, 1.6198226303498342, 1.6222333653611307, 1.6208838563051522, 1.6260771831659653, 1.6246974098271336, 1.6235491133284294, 1.6291177901141163, 1.627396788698895, 1.6269120328336317, 1.6259447169812833, 1.6322609587451704, 1.636145249571902, 1.6334387774537937, 1.6310630011049594, 1.6258337963586567, 1.6210992422401416], 'val_acc': [0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.23973727360832672, 0.2331691292070207, 0.2331691292070207, 0.24302134605366216, 0.2430213459557892, 0.24630541820537868, 0.22988505695743122, 0.24466338208058394, 0.23809523748353198, 0.23645320135873724, 0.24137930983099445, 0.23973727370619968, 0.23481116542968844, 0.24466338208058394, 0.238095237385659, 0.24137930963524848, 0.24466338208058394, 0.23809523767927793, 0.23316912950063964, 0.23973727360832672, 0.2331691292070207, 0.24466338208058394, 0.23481116552756143, 0.2183908044753599, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.20853858762871846, 0.23152709318009895, 0.24137930983099445, 0.23645320135873724, 0.2430213459557892, 0.2430213459557892, 0.2348111651360695, 0.23645320126086425, 0.24137930973312147, 0.2348111651360695, 0.2298850566638123, 0.23316912901127476, 0.23152709308222597, 0.23809523748353198, 0.2282430211262554, 0.23316912910914772, 0.23809523758140494, 0.24137930983099445, 0.23481116542968844, 0.2282430207347635, 0.24137930963524848, 0.2348111651360695, 0.2430213458579162, 0.23809523748353198, 0.2430213458579162, 0.24466338198271095, 0.23809523767927793, 0.2364532014566102, 0.2430213458579162, 0.23645320116299126, 0.23645320116299126, 0.23645320116299126, 0.22660098490358768, 0.23645320126086425, 0.23973727351045374, 0.23973727360832672, 0.2348111656254344, 0.23973727351045374, 0.23973727351045374, 0.23316912950063964, 0.23645320175022916, 0.24137930963524848, 0.23973727360832672, 0.22495894877879294, 0.22660098490358768, 0.2298850571531772, 0.22660098490358768, 0.22824302102838243, 0.2331691293048937, 0.22495894887666593, 0.22660098490358768, 0.2298850571531772, 0.23152709327797194, 0.22495894887666593, 0.21674876805694623, 0.2151067318342785, 0.21839080418174098, 0.2233169125561252, 0.2233169125561252, 0.22167487643133046, 0.20853858733509953, 0.21839080427961396, 0.22167487662707644, 0.21018062365564025, 0.22003284030653572, 0.21346469590522973, 0.2134646956116108, 0.23152709298435298, 0.21674876815481922, 0.23316912901127476, 0.23973727370619968, 0.21674876795907325, 0.22824302102838243, 0.22167487652920345, 0.23645320135873724, 0.22988505734892314, 0.2233169125561252, 0.22988505725105016, 0.23481116572330737, 0.2315270933758449, 0.2315270934737179, 0.23152709357159088, 0.2282430211262554, 0.22331691275187118, 0.22495894887666593, 0.2233169126539982, 0.2331691293048937, 0.22988505734892314, 0.23316912950063964, 0.23481116552756143, 0.23316912950063964, 0.2348111656254344, 0.2282430212241284, 0.2233169126539982, 0.22167487662707644, 0.22824302102838243, 0.2200328405022817, 0.2282430212241284, 0.22331691275187118, 0.2282430212241284, 0.22988505734892314, 0.23481116354563358, 0.21839080437748695, 0.2282430213220014, 0.22167487662707644, 0.22167487643133046, 0.2233169125561252, 0.22331691275187118, 0.22660098480571472, 0.22660098500146067, 0.23152709318009895, 0.22988505725105016, 0.22167487652920345, 0.22660098500146067, 0.2233169126539982, 0.22495894887666593, 0.2282430212241284, 0.2200328404044087, 0.21674876805694623, 0.21839080437748695, 0.22167487662707644, 0.21674876815481922, 0.22495894887666593, 0.22495894877879294, 0.22495894877879294, 0.22167487662707644, 0.22167487652920345, 0.22824302102838243, 0.2233169126539982, 0.22003284020866276, 0.22660098480571472, 0.2348111656254344, 0.22495894877879294, 0.2216748763334575, 0.22660098490358768, 0.2348111656254344, 0.22988505725105016, 0.2282430207347635, 0.23316912940276668, 0.224958948485174, 0.2282430212241284, 0.22988505734892314, 0.22660098509933366, 0.21510673004809663, 0.2282430212241284, 0.22988505734892314, 0.21674876627076436, 0.21674876627076436, 0.2315270934737179, 0.22495894868091998, 0.2282430212241284, 0.21346469402117488, 0.2315270934737179, 0.2282430212241284, 0.2315270934737179, 0.2282430211262554, 0.2315270933758449, 0.22331691294761713, 0.23645320184810212, 0.2216748745472756, 0.2282430212241284, 0.22167487652920345, 0.22167487643133046, 0.21839080427961396, 0.22331691275187118, 0.20032840492494391, 0.21839080437748695, 0.22495894877879294, 0.22824302102838243, 0.22660098509933366, 0.2249589489745389, 0.22495894877879294, 0.21346469570948376, 0.21510673163853256, 0.21018062316627534, 0.2052545148897641, 0.18883415403330855, 0.19376026201620086, 0.20032840661325282, 0.18390804536530536, 0.21510673173640554, 0.2134646956116108, 0.22824302083263648, 0.22003284020866276, 0.21182266007405393, 0.20525451329932815, 0.2101806216737124, 0.2151067323236434, 0.21182265779850715, 0.2101806216737124, 0.2151067323236434, 0.2101806238513862, 0.21018062394925918, 0.21182265987830795, 0.21674876617289138, 0.2101806237535132, 0.21674876844843816, 0.22331691275187118, 0.22331691294761713, 0.21674876835056517, 0.21182265987830795, 0.22331691294761713, 0.205254515281256, 0.20853858762871846, 0.22988505744679613, 0.22495894887666593, 0.20853858762871846, 0.21346469590522973, 0.22003284060015468, 0.21346469590522973, 0.2085385875308455, 0.21346469610097568, 0.21346469610097568, 0.22167487662707644, 0.21510673212789747, 0.21839080418174098, 0.22495894907241187, 0.23645319967042833, 0.23152709327797194, 0.2331691293048937, 0.2167487682526922, 0.21510673212789747, 0.19540229863036052, 0.19376026250556577, 0.21510673222577043, 0.2183908044753599, 0.23481116542968844, 0.21182265987830795, 0.2298850570553042, 0.22660098480571472, 0.21674876835056517, 0.2298850571531772, 0.22003284060015468, 0.2200328404044087, 0.2151067319321515, 0.22167487662707644, 0.22660098480571472, 0.2233169126539982, 0.21839080408386802, 0.21839080437748695, 0.21510673203002448, 0.21346469590522973, 0.2216748745472756, 0.20689655140605073, 0.2282430211262554, 0.2233169126539982, 0.2282430211262554, 0.22167487662707644, 0.19868637058633107, 0.2019704430316665, 0.20197044293379354, 0.20525451518338303, 0.20853858743297252], 'loss': [1.6108938656303673, 1.606922195922178, 1.6066603552389438, 1.6062257556210309, 1.6056351577966366, 1.6053351748405786, 1.605178083041855, 1.6047818798304094, 1.6046025198832674, 1.6044670896118916, 1.6045652011581515, 1.6041959023818344, 1.6040031345473178, 1.6037052535423264, 1.603473605708175, 1.6035153454089313, 1.6030409128024592, 1.60374434944051, 1.6031127961998848, 1.602822335153145, 1.6028326417875975, 1.602161405463483, 1.6024601325362124, 1.6018311060919164, 1.6015271544701264, 1.6009030114943488, 1.6005754601539284, 1.6009943315380653, 1.6004788792353637, 1.6007390682212626, 1.6004324911556205, 1.599863476136382, 1.599568191005464, 1.6003817528425055, 1.5993103593281897, 1.5995384052304027, 1.5990541853699107, 1.5995319251407099, 1.6006862999232643, 1.5996231374309782, 1.6001254243282812, 1.6020810176704454, 1.600537955344825, 1.6020908127575195, 1.6039461639137973, 1.603572881882685, 1.6053065197668526, 1.6020862409955912, 1.6024560191548092, 1.6009797532455632, 1.6009142487935217, 1.6002528699271732, 1.6003037544246572, 1.5993358153337325, 1.5998742795333236, 1.6000850817016508, 1.599599164810024, 1.5991004399450408, 1.5988206289142555, 1.5986564408092774, 1.5975092423280406, 1.5986128843295746, 1.602470533558965, 1.600538707954438, 1.5987149322791756, 1.5989434976107775, 1.597773161854832, 1.5973204253390585, 1.5967762968623418, 1.5972089566734047, 1.5970604592035438, 1.5972206395264767, 1.5971000955334924, 1.596332521115485, 1.5961528507346245, 1.5964472055924745, 1.597468980137083, 1.5968486755046023, 1.5975402746357223, 1.5994178708818658, 1.5966498073855955, 1.5966952335663156, 1.5969370823866043, 1.5973307034807773, 1.5963083859097051, 1.596483802697497, 1.5958186066860536, 1.595636069211627, 1.5954718354791098, 1.5945796220943913, 1.5955103954984913, 1.5944686366792087, 1.5952079931568561, 1.5934313668852225, 1.5940203566326008, 1.5933518332867163, 1.5934690127382534, 1.5938934277704855, 1.5923846371120007, 1.592428222916699, 1.5928100550199191, 1.5924222578992588, 1.592501420896401, 1.5914452093582623, 1.5916579018383301, 1.5917695797933935, 1.5918079627367996, 1.5911936143095733, 1.5941277454521132, 1.5987393373826202, 1.60186477632738, 1.5987373347644689, 1.5969101247356658, 1.5977036153511346, 1.596950542216918, 1.5954983927141224, 1.5943193363213197, 1.595247110742808, 1.5916140983236888, 1.5917977215083472, 1.5920121983091444, 1.5924312696319831, 1.5915715127510213, 1.5914344064508865, 1.5915533284142276, 1.591620944755523, 1.5916772555521626, 1.5912713044967495, 1.5906185065451588, 1.5904370068524651, 1.5893844200110778, 1.5889917884763995, 1.5873881092306525, 1.5880282662977183, 1.5890378117806123, 1.586282822826315, 1.5880126122821283, 1.5865755420445906, 1.5863635144928887, 1.5876139799427447, 1.586435513770556, 1.5863485810693039, 1.5860778927558257, 1.5869622560497183, 1.5882898998945891, 1.5879604659286124, 1.5919550147634267, 1.5918695716642501, 1.59155067349851, 1.5881114991538579, 1.5890877404007333, 1.5856960351462237, 1.586427615898101, 1.58493043935764, 1.5865021602818608, 1.5861338411268513, 1.5848329187663428, 1.5844002166078321, 1.5842309289399603, 1.586039176124322, 1.5838482018124151, 1.5870963081931677, 1.588906098636024, 1.5908530907464469, 1.5844024716951028, 1.58575539520389, 1.586151846870015, 1.5838489882020734, 1.5833998680114747, 1.5831270696201363, 1.5828919995247217, 1.5826929409156345, 1.5846158631283644, 1.5827301026369758, 1.5814426125931789, 1.5821345961803772, 1.5800777710438754, 1.5796706556539515, 1.5807930795074243, 1.5819400325692898, 1.5816367863629632, 1.57951326947927, 1.5791229131040632, 1.5812300653183484, 1.5828988797366008, 1.5799784840989162, 1.5804555674597958, 1.5795776047990553, 1.5797053307723217, 1.5803112172982532, 1.5811135245544465, 1.5836172289916868, 1.5846537757458383, 1.5809895508098406, 1.5801382037893212, 1.5799781068884127, 1.5823855101450268, 1.5814996473108718, 1.5792842915905085, 1.5803406058885234, 1.5792868150088333, 1.57842279983497, 1.5780020114577527, 1.577190448811901, 1.5775666413121154, 1.5755054256999272, 1.577379714243221, 1.5760579489584576, 1.5770696734990428, 1.578417589826016, 1.578209841226895, 1.5793315932491232, 1.5796924391811145, 1.5772573275242987, 1.5785653232304222, 1.5788386458978516, 1.5762724960609138, 1.575464109522606, 1.5749951433596914, 1.5766512090420577, 1.5776331333653884, 1.5789266052676911, 1.5843054509505599, 1.5843351937417376, 1.5828725491216296, 1.5803054157958139, 1.5808862900587077, 1.5807810153314954, 1.5807330528079118, 1.5802095241859953, 1.578966109855464, 1.578032072662573, 1.5780542139155174, 1.5779925821253407, 1.5758127836231333, 1.5768859676267086, 1.5752563458448563, 1.5740411326136188, 1.5740657799542561, 1.5736141373244643, 1.575121123834802, 1.5772934162641208, 1.5753645768645363, 1.5771667036432506, 1.5768101436156756, 1.576856218324305, 1.5753758681138683, 1.5762414656625392, 1.5752169164544014, 1.5750023146184808, 1.578177068561499, 1.5784202576662725, 1.5787607220408852, 1.577469605586857, 1.5750864792653423, 1.5750718565692157, 1.581863540545626, 1.5749871103180997, 1.57815402567509, 1.5786525373341367, 1.5754481289176236, 1.5749556098385757, 1.5741961998126834, 1.5752672231662446, 1.5772465272605787, 1.5780649920019036, 1.5780637094861918, 1.5766980212816712, 1.5718947755237869, 1.5727079293566317, 1.5731094425463823, 1.577468171060942, 1.5820831553891943, 1.57671618402861, 1.5776547326689139, 1.5758815838326174, 1.5781724971912234, 1.5770040673152133, 1.5730982505320523, 1.5745429568711737, 1.5746625634923852, 1.573381504238998, 1.57250554963059, 1.5734872854221038, 1.5710790425110646, 1.5718520378919598, 1.5700533194708384, 1.570068152141767, 1.568925116096434, 1.5694567219677402, 1.5684613835395484, 1.568199734961962, 1.5677053425591094, 1.568468990854659, 1.568091704664289, 1.5723752460930136, 1.5720728356735418, 1.571680493031684, 1.5711674426370577, 1.5687540694673447], 'acc': [0.20246406556154914, 0.23285421049203225, 0.23285420929871545, 0.23285420929871545, 0.2328542106694999, 0.23285421029620593, 0.23285420970872686, 0.23285420990455322, 0.23285421029620593, 0.2328542071079082, 0.23285420970872686, 0.23285420969036816, 0.23285420851541005, 0.23285421049203225, 0.2328542094945418, 0.23285420970872686, 0.2328542083195837, 0.2328542087112364, 0.2328542091028891, 0.23285420951290053, 0.23080082199167176, 0.23655030787970252, 0.23326488634766496, 0.23367556435738746, 0.23737166211835167, 0.24188911630877236, 0.23531827582715717, 0.2340862425629363, 0.239835729197555, 0.24229979510180025, 0.24271047234657608, 0.2410677626392435, 0.23531827580879847, 0.2344969207684852, 0.24353182658522526, 0.24558521585053242, 0.2422997941226685, 0.23942505239950804, 0.2291581099527817, 0.24188911826703582, 0.24147844007984567, 0.23285421049203225, 0.2377823409113796, 0.2229979466975837, 0.22792607770441004, 0.2336755641615611, 0.23203285327927042, 0.22792607749022498, 0.22422997933760805, 0.23080082240168318, 0.23901437552802618, 0.24106776107263272, 0.23737166251000438, 0.23942505236279057, 0.24476386063275152, 0.24024640564066674, 0.23449691918351567, 0.24476386002691375, 0.24681724890056822, 0.23778234032390053, 0.2468172485089155, 0.24353182537354973, 0.23655030807552885, 0.24106776087680637, 0.2455852174171432, 0.232443531876472, 0.24229979471014756, 0.24188911728790408, 0.25010267043260576, 0.24928131304237633, 0.24640657190669488, 0.24640657069501937, 0.24804928116729863, 0.24640657245745648, 0.242299796080932, 0.24845995898119477, 0.24681724948804726, 0.24599589327277588, 0.24476386159352453, 0.23778234151721736, 0.2496919898404233, 0.24928131402150805, 0.24928131402150805, 0.2468172485089155, 0.25174537914244793, 0.24804928098983098, 0.249281313434029, 0.25010266984512675, 0.24928131501899853, 0.24845995659456116, 0.25256673594519835, 0.24928131320148522, 0.2476386031575761, 0.2529774133858005, 0.2517453818840168, 0.2447638612385893, 0.249281313434029, 0.24763860395924023, 0.25256673653267736, 0.2513347028951625, 0.24640657265328283, 0.2529774131899742, 0.24722792749776978, 0.2501026700409531, 0.2505133490298073, 0.25174538012157965, 0.25010266886599497, 0.25379876901359283, 0.24599589386025494, 0.24394250617991728, 0.23901437259063094, 0.23080082024759335, 0.23490759701577055, 0.23572895244773653, 0.2418891169146101, 0.24353182854348873, 0.24804928214643035, 0.24845995800206302, 0.24804928175477767, 0.24969199163957786, 0.24928131460898711, 0.24804928018816688, 0.24599589327277588, 0.24722792651863804, 0.24804928116729863, 0.24804928057981956, 0.25215605813130215, 0.2447638594394347, 0.24887063583431793, 0.2484599585895421, 0.2542094460441836, 0.2533880905938589, 0.2537987686219401, 0.24804928018816688, 0.24845995659456116, 0.25913757783431535, 0.2505133470715439, 0.2529774133674418, 0.25379876879940777, 0.2509240260603981, 0.25297741379581196, 0.2513347023076835, 0.25010267082425847, 0.24804928236061544, 0.2484599589995535, 0.2484599578062367, 0.24476385924360836, 0.2340862421529249, 0.23696098450028186, 0.2464065708908457, 0.24887063855752808, 0.2529774147382262, 0.24681724831308918, 0.2513347001168762, 0.25215605773964944, 0.24928131362985537, 0.2542094452058021, 0.2529774121924837, 0.25462012540633183, 0.24435318321050806, 0.2529774147382262, 0.24887063519176272, 0.24394250576990587, 0.2476386031575761, 0.25297741395492085, 0.24599589268529684, 0.2496919926187096, 0.2439425053966119, 0.2529774121924837, 0.2546201250146791, 0.2546201228605893, 0.25297741395492085, 0.25338809078968527, 0.25503079946280993, 0.25954825468743853, 0.2587269008037246, 0.25708418837318187, 0.2579055440193329, 0.2562628358786111, 0.2611909669037962, 0.25420944680913027, 0.2706365485578102, 0.25790554595923765, 0.24887063836170173, 0.25010266867016867, 0.2439425051824268, 0.2579055445884532, 0.2574948677536888, 0.25749486697038343, 0.25708418956649864, 0.25667351193007015, 0.2624229987788739, 0.24928131521482488, 0.25790554200599325, 0.26940451906936613, 0.26324435323170814, 0.2533880911996967, 0.2550308004419417, 0.2591375774426627, 0.26283367522198564, 0.25954825564821155, 0.25872689982459285, 0.2566735109509384, 0.25872690023460426, 0.2570841877857028, 0.25708418798152916, 0.26119096435805367, 0.26447638706504933, 0.2542094450466931, 0.26119096435805367, 0.2607802877191156, 0.2599589332846401, 0.257084190723098, 0.2657084191359534, 0.2566735099350892, 0.2583162212273913, 0.26283367580946465, 0.25790554200599325, 0.25954825228244616, 0.2611909667079698, 0.2554414784883816, 0.25462012462302647, 0.2505133470899026, 0.24599589346860223, 0.2488706365992646, 0.2521560555488422, 0.2566735109509384, 0.2542094446550405, 0.251745380317406, 0.2542094456341722, 0.2562628347220117, 0.25913757609023697, 0.2669404531651209, 0.26036960970939305, 0.2681724854318513, 0.2677618070488348, 0.2644763838951103, 0.267351127221599, 0.2652977413220572, 0.27104722817086097, 0.2702258707439141, 0.26735112980405895, 0.2673511294124063, 0.2636550290506234, 0.2624229995621793, 0.2652977426928416, 0.26488706389981376, 0.2657084191359534, 0.26652977535122474, 0.2677618078137815, 0.2595482564315169, 0.26652977495957203, 0.24517453705750453, 0.263655031437257, 0.26611909832063396, 0.26858316007580846, 0.2607802883065946, 0.2714579069638889, 0.2616016427594289, 0.2632443549941453, 0.2702258731305477, 0.27145790676806253, 0.27556468134543244, 0.26652977535122474, 0.26735112780907805, 0.25995893365793404, 0.262012320573325, 0.2624229991705266, 0.2652977409304045, 0.27145790340229714, 0.27022587332637404, 0.2681724830452177, 0.26488706409564006, 0.2698151937133233, 0.264476384874242, 0.26611909579325016, 0.264476387456702, 0.2677618089887396, 0.27926077932058174, 0.27186858458195867, 0.268172484862731, 0.26529774112623083, 0.26652977276876477, 0.2648870613173538, 0.27186858281952153, 0.27392196950236875, 0.2714579063947685, 0.2694045176985817, 0.2694045162910798, 0.2702258707439141, 0.27392196911071603, 0.2767967158029701, 0.27433264970289856, 0.2755646835362397, 0.2726899388389666, 0.26899383808553096, 0.2726899362565066, 0.27022587391385305, 0.2739219726723078, 0.27186858458195867]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
