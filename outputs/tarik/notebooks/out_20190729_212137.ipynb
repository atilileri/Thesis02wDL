{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf21.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 21:21:37 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'Split', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002658038CE10>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000265E8176EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0930, Accuracy:0.3461, Validation Loss:1.0737, Validation Accuracy:0.3953\n",
    "Epoch #2: Loss:1.0745, Accuracy:0.3907, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0750, Accuracy:0.3934, Validation Loss:1.0739, Validation Accuracy:0.3998\n",
    "Epoch #4: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3945\n",
    "Epoch #5: Loss:1.0739, Accuracy:0.3915, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0742, Accuracy:0.3944, Validation Loss:1.0740, Validation Accuracy:0.3937\n",
    "Epoch #7: Loss:1.0750, Accuracy:0.3979, Validation Loss:1.0749, Validation Accuracy:0.3929\n",
    "Epoch #8: Loss:1.0747, Accuracy:0.3947, Validation Loss:1.0746, Validation Accuracy:0.3949\n",
    "Epoch #9: Loss:1.0747, Accuracy:0.3954, Validation Loss:1.0746, Validation Accuracy:0.3949\n",
    "Epoch #10: Loss:1.0749, Accuracy:0.3945, Validation Loss:1.0747, Validation Accuracy:0.3966\n",
    "Epoch #11: Loss:1.0745, Accuracy:0.3975, Validation Loss:1.0745, Validation Accuracy:0.3937\n",
    "Epoch #12: Loss:1.0738, Accuracy:0.3996, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #13: Loss:1.0747, Accuracy:0.3937, Validation Loss:1.0742, Validation Accuracy:0.3998\n",
    "Epoch #14: Loss:1.0741, Accuracy:0.3944, Validation Loss:1.0740, Validation Accuracy:0.3986\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3898, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3982\n",
    "Epoch #17: Loss:1.0750, Accuracy:0.3806, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #18: Loss:1.0737, Accuracy:0.3945, Validation Loss:1.0738, Validation Accuracy:0.3945\n",
    "Epoch #19: Loss:1.0742, Accuracy:0.3901, Validation Loss:1.0739, Validation Accuracy:0.3933\n",
    "Epoch #20: Loss:1.0736, Accuracy:0.3953, Validation Loss:1.0737, Validation Accuracy:0.3994\n",
    "Epoch #21: Loss:1.0736, Accuracy:0.3977, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #22: Loss:1.0739, Accuracy:0.4018, Validation Loss:1.0740, Validation Accuracy:0.3998\n",
    "Epoch #23: Loss:1.0734, Accuracy:0.4030, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #24: Loss:1.0733, Accuracy:0.3986, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #25: Loss:1.0736, Accuracy:0.4007, Validation Loss:1.0741, Validation Accuracy:0.3949\n",
    "Epoch #26: Loss:1.0733, Accuracy:0.4014, Validation Loss:1.0737, Validation Accuracy:0.3986\n",
    "Epoch #27: Loss:1.0733, Accuracy:0.3977, Validation Loss:1.0739, Validation Accuracy:0.3966\n",
    "Epoch #28: Loss:1.0731, Accuracy:0.3990, Validation Loss:1.0737, Validation Accuracy:0.3957\n",
    "Epoch #29: Loss:1.0728, Accuracy:0.4021, Validation Loss:1.0735, Validation Accuracy:0.3982\n",
    "Epoch #30: Loss:1.0730, Accuracy:0.4011, Validation Loss:1.0734, Validation Accuracy:0.3978\n",
    "Epoch #31: Loss:1.0727, Accuracy:0.3997, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #32: Loss:1.0732, Accuracy:0.4024, Validation Loss:1.0738, Validation Accuracy:0.3937\n",
    "Epoch #33: Loss:1.0729, Accuracy:0.4012, Validation Loss:1.0733, Validation Accuracy:0.3970\n",
    "Epoch #34: Loss:1.0730, Accuracy:0.3977, Validation Loss:1.0732, Validation Accuracy:0.3982\n",
    "Epoch #35: Loss:1.0729, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.3949\n",
    "Epoch #36: Loss:1.0728, Accuracy:0.4015, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #37: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #38: Loss:1.0736, Accuracy:0.3957, Validation Loss:1.0742, Validation Accuracy:0.3904\n",
    "Epoch #39: Loss:1.0752, Accuracy:0.3934, Validation Loss:1.0754, Validation Accuracy:0.3846\n",
    "Epoch #40: Loss:1.0756, Accuracy:0.3831, Validation Loss:1.0743, Validation Accuracy:0.3867\n",
    "Epoch #41: Loss:1.0748, Accuracy:0.3847, Validation Loss:1.0748, Validation Accuracy:0.3896\n",
    "Epoch #42: Loss:1.0756, Accuracy:0.3941, Validation Loss:1.0742, Validation Accuracy:0.3970\n",
    "Epoch #43: Loss:1.0740, Accuracy:0.3945, Validation Loss:1.0741, Validation Accuracy:0.3937\n",
    "Epoch #44: Loss:1.0743, Accuracy:0.3979, Validation Loss:1.0740, Validation Accuracy:0.3994\n",
    "Epoch #45: Loss:1.0736, Accuracy:0.3983, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #46: Loss:1.0734, Accuracy:0.3995, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #47: Loss:1.0740, Accuracy:0.3941, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #48: Loss:1.0744, Accuracy:0.3946, Validation Loss:1.0746, Validation Accuracy:0.3949\n",
    "Epoch #49: Loss:1.0742, Accuracy:0.3894, Validation Loss:1.0745, Validation Accuracy:0.3920\n",
    "Epoch #50: Loss:1.0734, Accuracy:0.3973, Validation Loss:1.0737, Validation Accuracy:0.3916\n",
    "Epoch #51: Loss:1.0732, Accuracy:0.4010, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #52: Loss:1.0737, Accuracy:0.3948, Validation Loss:1.0735, Validation Accuracy:0.3994\n",
    "Epoch #53: Loss:1.0734, Accuracy:0.3989, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #54: Loss:1.0741, Accuracy:0.3915, Validation Loss:1.0735, Validation Accuracy:0.4011\n",
    "Epoch #55: Loss:1.0726, Accuracy:0.4041, Validation Loss:1.0732, Validation Accuracy:0.3945\n",
    "Epoch #56: Loss:1.0726, Accuracy:0.4002, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #57: Loss:1.0731, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.3867\n",
    "Epoch #58: Loss:1.0732, Accuracy:0.4007, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #59: Loss:1.0750, Accuracy:0.3925, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #60: Loss:1.0733, Accuracy:0.3972, Validation Loss:1.0734, Validation Accuracy:0.3978\n",
    "Epoch #61: Loss:1.0731, Accuracy:0.4005, Validation Loss:1.0734, Validation Accuracy:0.3986\n",
    "Epoch #62: Loss:1.0732, Accuracy:0.3962, Validation Loss:1.0735, Validation Accuracy:0.3986\n",
    "Epoch #63: Loss:1.0729, Accuracy:0.3985, Validation Loss:1.0735, Validation Accuracy:0.3904\n",
    "Epoch #64: Loss:1.0734, Accuracy:0.3954, Validation Loss:1.0735, Validation Accuracy:0.3945\n",
    "Epoch #65: Loss:1.0738, Accuracy:0.3965, Validation Loss:1.0733, Validation Accuracy:0.3966\n",
    "Epoch #66: Loss:1.0726, Accuracy:0.4010, Validation Loss:1.0737, Validation Accuracy:0.3970\n",
    "Epoch #67: Loss:1.0725, Accuracy:0.3990, Validation Loss:1.0734, Validation Accuracy:0.3900\n",
    "Epoch #68: Loss:1.0726, Accuracy:0.4017, Validation Loss:1.0738, Validation Accuracy:0.3949\n",
    "Epoch #69: Loss:1.0732, Accuracy:0.4005, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #70: Loss:1.0729, Accuracy:0.4006, Validation Loss:1.0741, Validation Accuracy:0.3978\n",
    "Epoch #71: Loss:1.0730, Accuracy:0.4001, Validation Loss:1.0741, Validation Accuracy:0.3883\n",
    "Epoch #72: Loss:1.0724, Accuracy:0.4066, Validation Loss:1.0729, Validation Accuracy:0.3998\n",
    "Epoch #73: Loss:1.0726, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.3982\n",
    "Epoch #74: Loss:1.0719, Accuracy:0.4047, Validation Loss:1.0729, Validation Accuracy:0.4007\n",
    "Epoch #75: Loss:1.0730, Accuracy:0.4002, Validation Loss:1.0731, Validation Accuracy:0.3801\n",
    "Epoch #76: Loss:1.0729, Accuracy:0.3960, Validation Loss:1.0725, Validation Accuracy:0.3982\n",
    "Epoch #77: Loss:1.0728, Accuracy:0.4004, Validation Loss:1.0725, Validation Accuracy:0.3970\n",
    "Epoch #78: Loss:1.0729, Accuracy:0.3984, Validation Loss:1.0724, Validation Accuracy:0.4019\n",
    "Epoch #79: Loss:1.0729, Accuracy:0.3982, Validation Loss:1.0730, Validation Accuracy:0.3760\n",
    "Epoch #80: Loss:1.0723, Accuracy:0.3994, Validation Loss:1.0728, Validation Accuracy:0.4007\n",
    "Epoch #81: Loss:1.0727, Accuracy:0.3987, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #82: Loss:1.0729, Accuracy:0.4006, Validation Loss:1.0725, Validation Accuracy:0.4007\n",
    "Epoch #83: Loss:1.0730, Accuracy:0.3988, Validation Loss:1.0730, Validation Accuracy:0.3896\n",
    "Epoch #84: Loss:1.0728, Accuracy:0.3965, Validation Loss:1.0726, Validation Accuracy:0.3974\n",
    "Epoch #85: Loss:1.0733, Accuracy:0.3964, Validation Loss:1.0727, Validation Accuracy:0.3970\n",
    "Epoch #86: Loss:1.0726, Accuracy:0.4009, Validation Loss:1.0727, Validation Accuracy:0.4002\n",
    "Epoch #87: Loss:1.0727, Accuracy:0.4000, Validation Loss:1.0729, Validation Accuracy:0.3953\n",
    "Epoch #88: Loss:1.0736, Accuracy:0.3968, Validation Loss:1.0731, Validation Accuracy:0.3970\n",
    "Epoch #89: Loss:1.0735, Accuracy:0.3980, Validation Loss:1.0731, Validation Accuracy:0.3978\n",
    "Epoch #90: Loss:1.0721, Accuracy:0.4028, Validation Loss:1.0726, Validation Accuracy:0.3990\n",
    "Epoch #91: Loss:1.0724, Accuracy:0.4035, Validation Loss:1.0731, Validation Accuracy:0.3961\n",
    "Epoch #92: Loss:1.0727, Accuracy:0.3999, Validation Loss:1.0736, Validation Accuracy:0.3982\n",
    "Epoch #93: Loss:1.0729, Accuracy:0.4022, Validation Loss:1.0736, Validation Accuracy:0.3863\n",
    "Epoch #94: Loss:1.0727, Accuracy:0.3995, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #95: Loss:1.0725, Accuracy:0.4011, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #96: Loss:1.0728, Accuracy:0.3911, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #97: Loss:1.0727, Accuracy:0.3995, Validation Loss:1.0732, Validation Accuracy:0.3949\n",
    "Epoch #98: Loss:1.0744, Accuracy:0.3966, Validation Loss:1.0739, Validation Accuracy:0.3764\n",
    "Epoch #99: Loss:1.0748, Accuracy:0.3803, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #100: Loss:1.0732, Accuracy:0.3977, Validation Loss:1.0728, Validation Accuracy:0.3957\n",
    "Epoch #101: Loss:1.0733, Accuracy:0.3891, Validation Loss:1.0727, Validation Accuracy:0.3961\n",
    "Epoch #102: Loss:1.0725, Accuracy:0.3977, Validation Loss:1.0726, Validation Accuracy:0.3949\n",
    "Epoch #103: Loss:1.0721, Accuracy:0.4028, Validation Loss:1.0731, Validation Accuracy:0.3863\n",
    "Epoch #104: Loss:1.0727, Accuracy:0.3897, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #105: Loss:1.0723, Accuracy:0.3985, Validation Loss:1.0726, Validation Accuracy:0.3982\n",
    "Epoch #106: Loss:1.0721, Accuracy:0.3930, Validation Loss:1.0727, Validation Accuracy:0.3961\n",
    "Epoch #107: Loss:1.0715, Accuracy:0.4035, Validation Loss:1.0727, Validation Accuracy:0.3883\n",
    "Epoch #108: Loss:1.0717, Accuracy:0.4010, Validation Loss:1.0747, Validation Accuracy:0.3949\n",
    "Epoch #109: Loss:1.0747, Accuracy:0.3946, Validation Loss:1.0737, Validation Accuracy:0.3970\n",
    "Epoch #110: Loss:1.0755, Accuracy:0.3986, Validation Loss:1.0733, Validation Accuracy:0.4002\n",
    "Epoch #111: Loss:1.0730, Accuracy:0.3970, Validation Loss:1.0730, Validation Accuracy:0.3953\n",
    "Epoch #112: Loss:1.0737, Accuracy:0.3939, Validation Loss:1.0733, Validation Accuracy:0.3961\n",
    "Epoch #113: Loss:1.0733, Accuracy:0.4026, Validation Loss:1.0731, Validation Accuracy:0.3986\n",
    "Epoch #114: Loss:1.0730, Accuracy:0.4002, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #115: Loss:1.0728, Accuracy:0.4027, Validation Loss:1.0730, Validation Accuracy:0.3986\n",
    "Epoch #116: Loss:1.0724, Accuracy:0.3966, Validation Loss:1.0726, Validation Accuracy:0.3961\n",
    "Epoch #117: Loss:1.0723, Accuracy:0.4002, Validation Loss:1.0731, Validation Accuracy:0.3953\n",
    "Epoch #118: Loss:1.0722, Accuracy:0.4003, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #119: Loss:1.0722, Accuracy:0.4001, Validation Loss:1.0730, Validation Accuracy:0.3998\n",
    "Epoch #120: Loss:1.0726, Accuracy:0.3961, Validation Loss:1.0731, Validation Accuracy:0.3978\n",
    "Epoch #121: Loss:1.0721, Accuracy:0.4010, Validation Loss:1.0728, Validation Accuracy:0.3978\n",
    "Epoch #122: Loss:1.0723, Accuracy:0.3974, Validation Loss:1.0729, Validation Accuracy:0.3929\n",
    "Epoch #123: Loss:1.0722, Accuracy:0.4000, Validation Loss:1.0727, Validation Accuracy:0.3970\n",
    "Epoch #124: Loss:1.0719, Accuracy:0.4007, Validation Loss:1.0725, Validation Accuracy:0.3994\n",
    "Epoch #125: Loss:1.0721, Accuracy:0.4014, Validation Loss:1.0731, Validation Accuracy:0.3978\n",
    "Epoch #126: Loss:1.0724, Accuracy:0.4015, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #127: Loss:1.0722, Accuracy:0.4003, Validation Loss:1.0731, Validation Accuracy:0.3945\n",
    "Epoch #128: Loss:1.0724, Accuracy:0.3997, Validation Loss:1.0730, Validation Accuracy:0.3986\n",
    "Epoch #129: Loss:1.0719, Accuracy:0.4010, Validation Loss:1.0727, Validation Accuracy:0.3945\n",
    "Epoch #130: Loss:1.0720, Accuracy:0.4003, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #131: Loss:1.0719, Accuracy:0.4010, Validation Loss:1.0726, Validation Accuracy:0.3945\n",
    "Epoch #132: Loss:1.0720, Accuracy:0.4015, Validation Loss:1.0725, Validation Accuracy:0.3957\n",
    "Epoch #133: Loss:1.0725, Accuracy:0.3993, Validation Loss:1.0725, Validation Accuracy:0.3937\n",
    "Epoch #134: Loss:1.0726, Accuracy:0.3960, Validation Loss:1.0725, Validation Accuracy:0.3949\n",
    "Epoch #135: Loss:1.0719, Accuracy:0.4033, Validation Loss:1.0728, Validation Accuracy:0.3957\n",
    "Epoch #136: Loss:1.0718, Accuracy:0.3984, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #137: Loss:1.0716, Accuracy:0.4031, Validation Loss:1.0726, Validation Accuracy:0.3957\n",
    "Epoch #138: Loss:1.0725, Accuracy:0.3989, Validation Loss:1.0724, Validation Accuracy:0.3949\n",
    "Epoch #139: Loss:1.0715, Accuracy:0.3999, Validation Loss:1.0725, Validation Accuracy:0.3941\n",
    "Epoch #140: Loss:1.0712, Accuracy:0.4003, Validation Loss:1.0726, Validation Accuracy:0.3966\n",
    "Epoch #141: Loss:1.0710, Accuracy:0.3990, Validation Loss:1.0722, Validation Accuracy:0.3924\n",
    "Epoch #142: Loss:1.0710, Accuracy:0.4008, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #143: Loss:1.0718, Accuracy:0.3963, Validation Loss:1.0725, Validation Accuracy:0.3937\n",
    "Epoch #144: Loss:1.0720, Accuracy:0.3931, Validation Loss:1.0731, Validation Accuracy:0.3961\n",
    "Epoch #145: Loss:1.0712, Accuracy:0.4005, Validation Loss:1.0727, Validation Accuracy:0.3982\n",
    "Epoch #146: Loss:1.0712, Accuracy:0.4010, Validation Loss:1.0727, Validation Accuracy:0.3924\n",
    "Epoch #147: Loss:1.0712, Accuracy:0.3991, Validation Loss:1.0724, Validation Accuracy:0.3974\n",
    "Epoch #148: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0725, Validation Accuracy:0.3970\n",
    "Epoch #149: Loss:1.0709, Accuracy:0.4015, Validation Loss:1.0720, Validation Accuracy:0.3953\n",
    "Epoch #150: Loss:1.0705, Accuracy:0.4014, Validation Loss:1.0723, Validation Accuracy:0.3986\n",
    "Epoch #151: Loss:1.0720, Accuracy:0.4027, Validation Loss:1.0721, Validation Accuracy:0.3982\n",
    "Epoch #152: Loss:1.0705, Accuracy:0.4024, Validation Loss:1.0717, Validation Accuracy:0.4048\n",
    "Epoch #153: Loss:1.0710, Accuracy:0.4013, Validation Loss:1.0720, Validation Accuracy:0.4023\n",
    "Epoch #154: Loss:1.0706, Accuracy:0.3995, Validation Loss:1.0718, Validation Accuracy:0.4039\n",
    "Epoch #155: Loss:1.0705, Accuracy:0.3989, Validation Loss:1.0717, Validation Accuracy:0.3863\n",
    "Epoch #156: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0714, Validation Accuracy:0.3966\n",
    "Epoch #157: Loss:1.0701, Accuracy:0.3975, Validation Loss:1.0717, Validation Accuracy:0.3851\n",
    "Epoch #158: Loss:1.0705, Accuracy:0.4017, Validation Loss:1.0723, Validation Accuracy:0.3961\n",
    "Epoch #159: Loss:1.0698, Accuracy:0.4025, Validation Loss:1.0715, Validation Accuracy:0.3875\n",
    "Epoch #160: Loss:1.0704, Accuracy:0.4027, Validation Loss:1.0720, Validation Accuracy:0.3978\n",
    "Epoch #161: Loss:1.0705, Accuracy:0.4003, Validation Loss:1.0720, Validation Accuracy:0.3883\n",
    "Epoch #162: Loss:1.0714, Accuracy:0.3929, Validation Loss:1.0723, Validation Accuracy:0.3978\n",
    "Epoch #163: Loss:1.0718, Accuracy:0.4006, Validation Loss:1.0718, Validation Accuracy:0.3871\n",
    "Epoch #164: Loss:1.0714, Accuracy:0.3976, Validation Loss:1.0721, Validation Accuracy:0.4007\n",
    "Epoch #165: Loss:1.0709, Accuracy:0.3987, Validation Loss:1.0722, Validation Accuracy:0.4011\n",
    "Epoch #166: Loss:1.0706, Accuracy:0.3982, Validation Loss:1.0720, Validation Accuracy:0.4019\n",
    "Epoch #167: Loss:1.0711, Accuracy:0.3843, Validation Loss:1.0723, Validation Accuracy:0.3974\n",
    "Epoch #168: Loss:1.0715, Accuracy:0.3996, Validation Loss:1.0717, Validation Accuracy:0.4011\n",
    "Epoch #169: Loss:1.0712, Accuracy:0.3978, Validation Loss:1.0715, Validation Accuracy:0.3961\n",
    "Epoch #170: Loss:1.0701, Accuracy:0.4036, Validation Loss:1.0718, Validation Accuracy:0.3982\n",
    "Epoch #171: Loss:1.0706, Accuracy:0.4001, Validation Loss:1.0718, Validation Accuracy:0.3982\n",
    "Epoch #172: Loss:1.0698, Accuracy:0.4022, Validation Loss:1.0711, Validation Accuracy:0.3970\n",
    "Epoch #173: Loss:1.0705, Accuracy:0.4009, Validation Loss:1.0713, Validation Accuracy:0.3970\n",
    "Epoch #174: Loss:1.0700, Accuracy:0.3966, Validation Loss:1.0713, Validation Accuracy:0.3970\n",
    "Epoch #175: Loss:1.0694, Accuracy:0.3983, Validation Loss:1.0716, Validation Accuracy:0.3990\n",
    "Epoch #176: Loss:1.0698, Accuracy:0.4024, Validation Loss:1.0779, Validation Accuracy:0.3846\n",
    "Epoch #177: Loss:1.0715, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3867\n",
    "Epoch #178: Loss:1.0721, Accuracy:0.4005, Validation Loss:1.0727, Validation Accuracy:0.4048\n",
    "Epoch #179: Loss:1.0702, Accuracy:0.4032, Validation Loss:1.0715, Validation Accuracy:0.4068\n",
    "Epoch #180: Loss:1.0700, Accuracy:0.3970, Validation Loss:1.0713, Validation Accuracy:0.3863\n",
    "Epoch #181: Loss:1.0697, Accuracy:0.3984, Validation Loss:1.0715, Validation Accuracy:0.3937\n",
    "Epoch #182: Loss:1.0696, Accuracy:0.4016, Validation Loss:1.0735, Validation Accuracy:0.3945\n",
    "Epoch #183: Loss:1.0707, Accuracy:0.4026, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0710, Accuracy:0.3933, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #185: Loss:1.0698, Accuracy:0.3918, Validation Loss:1.0727, Validation Accuracy:0.3916\n",
    "Epoch #186: Loss:1.0694, Accuracy:0.3996, Validation Loss:1.0726, Validation Accuracy:0.3920\n",
    "Epoch #187: Loss:1.0699, Accuracy:0.3986, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #188: Loss:1.0702, Accuracy:0.4042, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #189: Loss:1.0703, Accuracy:0.3963, Validation Loss:1.0730, Validation Accuracy:0.3961\n",
    "Epoch #190: Loss:1.0700, Accuracy:0.3914, Validation Loss:1.0734, Validation Accuracy:0.3879\n",
    "Epoch #191: Loss:1.0696, Accuracy:0.3967, Validation Loss:1.0732, Validation Accuracy:0.3994\n",
    "Epoch #192: Loss:1.0698, Accuracy:0.3993, Validation Loss:1.0722, Validation Accuracy:0.3986\n",
    "Epoch #193: Loss:1.0691, Accuracy:0.3992, Validation Loss:1.0727, Validation Accuracy:0.4015\n",
    "Epoch #194: Loss:1.0699, Accuracy:0.3945, Validation Loss:1.0721, Validation Accuracy:0.3953\n",
    "Epoch #195: Loss:1.0689, Accuracy:0.3962, Validation Loss:1.0722, Validation Accuracy:0.3924\n",
    "Epoch #196: Loss:1.0692, Accuracy:0.4030, Validation Loss:1.0722, Validation Accuracy:0.4007\n",
    "Epoch #197: Loss:1.0688, Accuracy:0.4014, Validation Loss:1.0729, Validation Accuracy:0.3879\n",
    "Epoch #198: Loss:1.0690, Accuracy:0.4010, Validation Loss:1.0735, Validation Accuracy:0.3986\n",
    "Epoch #199: Loss:1.0692, Accuracy:0.3997, Validation Loss:1.0735, Validation Accuracy:0.3879\n",
    "Epoch #200: Loss:1.0681, Accuracy:0.4026, Validation Loss:1.0731, Validation Accuracy:0.3982\n",
    "Epoch #201: Loss:1.0682, Accuracy:0.4026, Validation Loss:1.0738, Validation Accuracy:0.3867\n",
    "Epoch #202: Loss:1.0709, Accuracy:0.3913, Validation Loss:1.0730, Validation Accuracy:0.3875\n",
    "Epoch #203: Loss:1.0681, Accuracy:0.3972, Validation Loss:1.0744, Validation Accuracy:0.3871\n",
    "Epoch #204: Loss:1.0711, Accuracy:0.3941, Validation Loss:1.0734, Validation Accuracy:0.3998\n",
    "Epoch #205: Loss:1.0688, Accuracy:0.4033, Validation Loss:1.0725, Validation Accuracy:0.3867\n",
    "Epoch #206: Loss:1.0687, Accuracy:0.3986, Validation Loss:1.0722, Validation Accuracy:0.3978\n",
    "Epoch #207: Loss:1.0689, Accuracy:0.3971, Validation Loss:1.0724, Validation Accuracy:0.3801\n",
    "Epoch #208: Loss:1.0692, Accuracy:0.3967, Validation Loss:1.0720, Validation Accuracy:0.3949\n",
    "Epoch #209: Loss:1.0708, Accuracy:0.4005, Validation Loss:1.0721, Validation Accuracy:0.3974\n",
    "Epoch #210: Loss:1.0698, Accuracy:0.3975, Validation Loss:1.0714, Validation Accuracy:0.4027\n",
    "Epoch #211: Loss:1.0689, Accuracy:0.4043, Validation Loss:1.0714, Validation Accuracy:0.4002\n",
    "Epoch #212: Loss:1.0688, Accuracy:0.3997, Validation Loss:1.0720, Validation Accuracy:0.3990\n",
    "Epoch #213: Loss:1.0688, Accuracy:0.3997, Validation Loss:1.0716, Validation Accuracy:0.3982\n",
    "Epoch #214: Loss:1.0710, Accuracy:0.4028, Validation Loss:1.0719, Validation Accuracy:0.4011\n",
    "Epoch #215: Loss:1.0696, Accuracy:0.3994, Validation Loss:1.0722, Validation Accuracy:0.3937\n",
    "Epoch #216: Loss:1.0693, Accuracy:0.3978, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #217: Loss:1.0687, Accuracy:0.3994, Validation Loss:1.0726, Validation Accuracy:0.3978\n",
    "Epoch #218: Loss:1.0685, Accuracy:0.3953, Validation Loss:1.0731, Validation Accuracy:0.3888\n",
    "Epoch #219: Loss:1.0704, Accuracy:0.3856, Validation Loss:1.0732, Validation Accuracy:0.3945\n",
    "Epoch #220: Loss:1.0699, Accuracy:0.4002, Validation Loss:1.0722, Validation Accuracy:0.4002\n",
    "Epoch #221: Loss:1.0704, Accuracy:0.3968, Validation Loss:1.0714, Validation Accuracy:0.4002\n",
    "Epoch #222: Loss:1.0689, Accuracy:0.4007, Validation Loss:1.0711, Validation Accuracy:0.3966\n",
    "Epoch #223: Loss:1.0685, Accuracy:0.3997, Validation Loss:1.0717, Validation Accuracy:0.3961\n",
    "Epoch #224: Loss:1.0687, Accuracy:0.4004, Validation Loss:1.0713, Validation Accuracy:0.3986\n",
    "Epoch #225: Loss:1.0691, Accuracy:0.3970, Validation Loss:1.0724, Validation Accuracy:0.3933\n",
    "Epoch #226: Loss:1.0692, Accuracy:0.4003, Validation Loss:1.0733, Validation Accuracy:0.3978\n",
    "Epoch #227: Loss:1.0697, Accuracy:0.4007, Validation Loss:1.0729, Validation Accuracy:0.3986\n",
    "Epoch #228: Loss:1.0695, Accuracy:0.3948, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #229: Loss:1.0698, Accuracy:0.3930, Validation Loss:1.0732, Validation Accuracy:0.3773\n",
    "Epoch #230: Loss:1.0695, Accuracy:0.3954, Validation Loss:1.0730, Validation Accuracy:0.3961\n",
    "Epoch #231: Loss:1.0689, Accuracy:0.4014, Validation Loss:1.0725, Validation Accuracy:0.4019\n",
    "Epoch #232: Loss:1.0689, Accuracy:0.4014, Validation Loss:1.0718, Validation Accuracy:0.3982\n",
    "Epoch #233: Loss:1.0683, Accuracy:0.4037, Validation Loss:1.0720, Validation Accuracy:0.3949\n",
    "Epoch #234: Loss:1.0693, Accuracy:0.3983, Validation Loss:1.0735, Validation Accuracy:0.3933\n",
    "Epoch #235: Loss:1.0684, Accuracy:0.4002, Validation Loss:1.0722, Validation Accuracy:0.3974\n",
    "Epoch #236: Loss:1.0683, Accuracy:0.4024, Validation Loss:1.0723, Validation Accuracy:0.3961\n",
    "Epoch #237: Loss:1.0704, Accuracy:0.4028, Validation Loss:1.0726, Validation Accuracy:0.3937\n",
    "Epoch #238: Loss:1.0694, Accuracy:0.4025, Validation Loss:1.0708, Validation Accuracy:0.3961\n",
    "Epoch #239: Loss:1.0691, Accuracy:0.4005, Validation Loss:1.0716, Validation Accuracy:0.3990\n",
    "Epoch #240: Loss:1.0690, Accuracy:0.4011, Validation Loss:1.0711, Validation Accuracy:0.3986\n",
    "Epoch #241: Loss:1.0688, Accuracy:0.3994, Validation Loss:1.0714, Validation Accuracy:0.4019\n",
    "Epoch #242: Loss:1.0698, Accuracy:0.4005, Validation Loss:1.0722, Validation Accuracy:0.3982\n",
    "Epoch #243: Loss:1.0700, Accuracy:0.3954, Validation Loss:1.0711, Validation Accuracy:0.4019\n",
    "Epoch #244: Loss:1.0690, Accuracy:0.4005, Validation Loss:1.0713, Validation Accuracy:0.3970\n",
    "Epoch #245: Loss:1.0689, Accuracy:0.4028, Validation Loss:1.0715, Validation Accuracy:0.3953\n",
    "Epoch #246: Loss:1.0685, Accuracy:0.4035, Validation Loss:1.0714, Validation Accuracy:0.3966\n",
    "Epoch #247: Loss:1.0685, Accuracy:0.3935, Validation Loss:1.0710, Validation Accuracy:0.3978\n",
    "Epoch #248: Loss:1.0688, Accuracy:0.3889, Validation Loss:1.0716, Validation Accuracy:0.4035\n",
    "Epoch #249: Loss:1.0704, Accuracy:0.4010, Validation Loss:1.0717, Validation Accuracy:0.3888\n",
    "Epoch #250: Loss:1.0687, Accuracy:0.4030, Validation Loss:1.0715, Validation Accuracy:0.3978\n",
    "Epoch #251: Loss:1.0679, Accuracy:0.4042, Validation Loss:1.0711, Validation Accuracy:0.3994\n",
    "Epoch #252: Loss:1.0678, Accuracy:0.4025, Validation Loss:1.0718, Validation Accuracy:0.3986\n",
    "Epoch #253: Loss:1.0680, Accuracy:0.4037, Validation Loss:1.0724, Validation Accuracy:0.3978\n",
    "Epoch #254: Loss:1.0677, Accuracy:0.4038, Validation Loss:1.0721, Validation Accuracy:0.3957\n",
    "Epoch #255: Loss:1.0679, Accuracy:0.4021, Validation Loss:1.0718, Validation Accuracy:0.3966\n",
    "Epoch #256: Loss:1.0668, Accuracy:0.4034, Validation Loss:1.0719, Validation Accuracy:0.3879\n",
    "Epoch #257: Loss:1.0675, Accuracy:0.4035, Validation Loss:1.0715, Validation Accuracy:0.3966\n",
    "Epoch #258: Loss:1.0674, Accuracy:0.4017, Validation Loss:1.0724, Validation Accuracy:0.4023\n",
    "Epoch #259: Loss:1.0682, Accuracy:0.4024, Validation Loss:1.0722, Validation Accuracy:0.4007\n",
    "Epoch #260: Loss:1.0672, Accuracy:0.3965, Validation Loss:1.0711, Validation Accuracy:0.3826\n",
    "Epoch #261: Loss:1.0678, Accuracy:0.3995, Validation Loss:1.0712, Validation Accuracy:0.3929\n",
    "Epoch #262: Loss:1.0714, Accuracy:0.3923, Validation Loss:1.0724, Validation Accuracy:0.3912\n",
    "Epoch #263: Loss:1.0684, Accuracy:0.3953, Validation Loss:1.0714, Validation Accuracy:0.3924\n",
    "Epoch #264: Loss:1.0690, Accuracy:0.4018, Validation Loss:1.0728, Validation Accuracy:0.3994\n",
    "Epoch #265: Loss:1.0689, Accuracy:0.4036, Validation Loss:1.0711, Validation Accuracy:0.4011\n",
    "Epoch #266: Loss:1.0688, Accuracy:0.4033, Validation Loss:1.0740, Validation Accuracy:0.3994\n",
    "Epoch #267: Loss:1.0699, Accuracy:0.3965, Validation Loss:1.0721, Validation Accuracy:0.3994\n",
    "Epoch #268: Loss:1.0688, Accuracy:0.3982, Validation Loss:1.0709, Validation Accuracy:0.4015\n",
    "Epoch #269: Loss:1.0685, Accuracy:0.4028, Validation Loss:1.0721, Validation Accuracy:0.3974\n",
    "Epoch #270: Loss:1.0685, Accuracy:0.4027, Validation Loss:1.0729, Validation Accuracy:0.3994\n",
    "Epoch #271: Loss:1.0690, Accuracy:0.4028, Validation Loss:1.0722, Validation Accuracy:0.3986\n",
    "Epoch #272: Loss:1.0684, Accuracy:0.4032, Validation Loss:1.0720, Validation Accuracy:0.3978\n",
    "Epoch #273: Loss:1.0677, Accuracy:0.4020, Validation Loss:1.0718, Validation Accuracy:0.3978\n",
    "Epoch #274: Loss:1.0667, Accuracy:0.4020, Validation Loss:1.0735, Validation Accuracy:0.3966\n",
    "Epoch #275: Loss:1.0679, Accuracy:0.4024, Validation Loss:1.0733, Validation Accuracy:0.3953\n",
    "Epoch #276: Loss:1.0670, Accuracy:0.4022, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #277: Loss:1.0675, Accuracy:0.4006, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #278: Loss:1.0669, Accuracy:0.4009, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #279: Loss:1.0667, Accuracy:0.4004, Validation Loss:1.0725, Validation Accuracy:0.3970\n",
    "Epoch #280: Loss:1.0672, Accuracy:0.4027, Validation Loss:1.0732, Validation Accuracy:0.4002\n",
    "Epoch #281: Loss:1.0683, Accuracy:0.4026, Validation Loss:1.0744, Validation Accuracy:0.3937\n",
    "Epoch #282: Loss:1.0671, Accuracy:0.4044, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #283: Loss:1.0667, Accuracy:0.4027, Validation Loss:1.0729, Validation Accuracy:0.3953\n",
    "Epoch #284: Loss:1.0666, Accuracy:0.4042, Validation Loss:1.0735, Validation Accuracy:0.3961\n",
    "Epoch #285: Loss:1.0675, Accuracy:0.4026, Validation Loss:1.0739, Validation Accuracy:0.3978\n",
    "Epoch #286: Loss:1.0669, Accuracy:0.4036, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #287: Loss:1.0665, Accuracy:0.4032, Validation Loss:1.0729, Validation Accuracy:0.3978\n",
    "Epoch #288: Loss:1.0661, Accuracy:0.4037, Validation Loss:1.0737, Validation Accuracy:0.3953\n",
    "Epoch #289: Loss:1.0664, Accuracy:0.4027, Validation Loss:1.0736, Validation Accuracy:0.4002\n",
    "Epoch #290: Loss:1.0664, Accuracy:0.4032, Validation Loss:1.0740, Validation Accuracy:0.3982\n",
    "Epoch #291: Loss:1.0677, Accuracy:0.4057, Validation Loss:1.0725, Validation Accuracy:0.3961\n",
    "Epoch #292: Loss:1.0670, Accuracy:0.4041, Validation Loss:1.0724, Validation Accuracy:0.3957\n",
    "Epoch #293: Loss:1.0676, Accuracy:0.4028, Validation Loss:1.0731, Validation Accuracy:0.4035\n",
    "Epoch #294: Loss:1.0661, Accuracy:0.4009, Validation Loss:1.0725, Validation Accuracy:0.3966\n",
    "Epoch #295: Loss:1.0661, Accuracy:0.4041, Validation Loss:1.0713, Validation Accuracy:0.3966\n",
    "Epoch #296: Loss:1.0663, Accuracy:0.4042, Validation Loss:1.0713, Validation Accuracy:0.4007\n",
    "Epoch #297: Loss:1.0669, Accuracy:0.4026, Validation Loss:1.0708, Validation Accuracy:0.3941\n",
    "Epoch #298: Loss:1.0663, Accuracy:0.4028, Validation Loss:1.0706, Validation Accuracy:0.4011\n",
    "Epoch #299: Loss:1.0677, Accuracy:0.4041, Validation Loss:1.0712, Validation Accuracy:0.4048\n",
    "Epoch #300: Loss:1.0676, Accuracy:0.4038, Validation Loss:1.0708, Validation Accuracy:0.4007\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07083189, Accuracy:0.4007\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  793  165   2\n",
    "t:02  725  178   5\n",
    "t:03  473   90   5\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.40      0.83      0.54       960\n",
    "          02       0.41      0.20      0.27       908\n",
    "          03       0.42      0.01      0.02       568\n",
    "\n",
    "    accuracy                           0.40      2436\n",
    "   macro avg       0.41      0.34      0.27      2436\n",
    "weighted avg       0.41      0.40      0.31      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 22:24:12 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0737248148236955, 1.0747449370636337, 1.073893193736648, 1.0739256748424961, 1.0739211744471333, 1.0740358240302952, 1.0748703180275527, 1.0746393722462144, 1.0745987571127504, 1.0747266770975148, 1.0744895492672724, 1.0744494191922969, 1.0741933752554782, 1.0740018669999096, 1.074546008078727, 1.074090048010126, 1.07425964779063, 1.073829271523236, 1.0739123390617433, 1.0736944428805648, 1.0738789340349646, 1.0739900713483688, 1.0738903910459947, 1.073833825748738, 1.074113944872651, 1.0737290067234257, 1.0738743398772868, 1.0736816144733399, 1.0735301942073654, 1.0734310128614428, 1.073805355477607, 1.0738471452825762, 1.073333583833353, 1.0731994340376705, 1.0732745202304108, 1.073720629187836, 1.0734866859486145, 1.0742481991966761, 1.0754160779254582, 1.074327874261953, 1.074782659659049, 1.074247043512529, 1.074098829173885, 1.0739795748627636, 1.0739869578131314, 1.0742011021314974, 1.0741535233355117, 1.0746316361701351, 1.0745018939862305, 1.0737173128597841, 1.0735098127465335, 1.0735095733492246, 1.073910049225505, 1.0735266680396445, 1.0732196690805245, 1.0729463920608922, 1.0732092829956406, 1.0743987258823438, 1.0734691292977294, 1.0734021741964155, 1.073416618095047, 1.073524841534093, 1.073478863352821, 1.0735460977836195, 1.073344735872178, 1.0737261568579963, 1.0733892430225616, 1.0738377643531962, 1.0747417890771074, 1.0741307156035074, 1.0740515800140957, 1.0728746568432386, 1.0731816967132644, 1.0728967504939815, 1.0730508458242431, 1.072497561451641, 1.0725263840654997, 1.0724393851651346, 1.0730433994521844, 1.0727885303826168, 1.0730571785975365, 1.0725095154616633, 1.0729865516935075, 1.072554910907213, 1.0726994109662686, 1.072653011539691, 1.0728869704385893, 1.0730893198883984, 1.073105567390304, 1.0726308427224998, 1.0730905953690728, 1.0736352377532934, 1.073586262309884, 1.0737828857988756, 1.073392351468404, 1.073166471788253, 1.0731931278858278, 1.073873432007525, 1.072957129509774, 1.0727890937394893, 1.0726691907262567, 1.0725939735794694, 1.0730505839161488, 1.0726946435734164, 1.0726209031341503, 1.0727345377745103, 1.0727356118325921, 1.0746967745531957, 1.0736805924836834, 1.0732585185854306, 1.072986555021189, 1.0733485715142612, 1.073066971376416, 1.0730864336142203, 1.072969484603268, 1.072640348733548, 1.0730585772024195, 1.0731611776430228, 1.0730032713346684, 1.0731100364663135, 1.072821319005368, 1.0729112018309594, 1.0726680888722486, 1.0725142116029862, 1.073053938023171, 1.0730061439066294, 1.0730640007357293, 1.073042919678837, 1.0726989967678175, 1.0728581938250312, 1.0725626554003687, 1.0725429688376942, 1.0724621572713742, 1.0725343708921535, 1.072823138072573, 1.0727329761132427, 1.0726199774515062, 1.0724215904871623, 1.0724887963390506, 1.0726127888768764, 1.0722376450724986, 1.072260847428358, 1.0724534493166042, 1.0730980775626422, 1.0727035502103357, 1.0727293244723617, 1.0724186094719397, 1.0725015053412401, 1.0719807341768237, 1.0723011190276623, 1.0720798777437759, 1.0717278994950168, 1.0720286985923504, 1.071818026807312, 1.0717402209202056, 1.0713958797000704, 1.0716511052230309, 1.0723237359073559, 1.0714722251265703, 1.0719982250570663, 1.0720285036293744, 1.072289296167433, 1.071808461093746, 1.0720703325835355, 1.072234784832533, 1.0720104049579264, 1.0722653114149723, 1.0716583071083858, 1.0714756236679253, 1.0717752302808714, 1.0717795936540626, 1.0711099412445169, 1.0712641391456617, 1.0713306232821962, 1.0715623365834428, 1.07791753472953, 1.0731224778837758, 1.0727498022402058, 1.0714706211841751, 1.0712716520713468, 1.071516317295519, 1.0734602275544591, 1.0733580708699468, 1.0732854424634786, 1.0727191591889205, 1.0725802521791756, 1.0727036533684566, 1.0727932132132143, 1.072977758589245, 1.073445196222202, 1.073155431911863, 1.0722019218263172, 1.072695445936106, 1.0721281046546347, 1.0721884355169211, 1.072196149865199, 1.072905324558515, 1.0734619207570117, 1.073487230122383, 1.0730619802459316, 1.0738143216212983, 1.0729902624496686, 1.0744038683244552, 1.0734129922926328, 1.0724992640499997, 1.072219490613452, 1.072449223552823, 1.0719969529040734, 1.0721324693980476, 1.0714106735924782, 1.0714037980156383, 1.072015262589666, 1.0716374750403543, 1.0719189105558473, 1.072157367892649, 1.0727397681065576, 1.0725607511836712, 1.0730681969418705, 1.0732204884731125, 1.0722133214837812, 1.0714400018181511, 1.0711134447057062, 1.071702495192855, 1.071266141235339, 1.0724391627977243, 1.0732813560708208, 1.0729443768562354, 1.0726753028938532, 1.0732165725751854, 1.0729844211944806, 1.0724807290608072, 1.071837804000366, 1.0719998278249856, 1.0735183188872188, 1.0721978358251512, 1.0723160564018588, 1.072628096210937, 1.070783081313072, 1.0715509839050092, 1.0710575954471706, 1.0713855542964341, 1.0721768735860564, 1.0710712553832331, 1.071349926770027, 1.071470836700477, 1.0713701680767516, 1.0710175170491285, 1.071587217265162, 1.0716711670307104, 1.0714589619675685, 1.0711373740620604, 1.071834366114073, 1.0724229043340447, 1.07205845236974, 1.0717918326702025, 1.0718727197944629, 1.0715330432005508, 1.0723920751283518, 1.0722283539905142, 1.071102572975096, 1.0711797931902907, 1.0724124673552113, 1.0714064047645857, 1.0728359758756039, 1.0711438360276873, 1.07404542517388, 1.0720925740224778, 1.0708767379250237, 1.0721075824524577, 1.0728935698178796, 1.0722429016345045, 1.0720013139283129, 1.071807568492169, 1.0735305878524906, 1.0732617752109646, 1.074134370180578, 1.0734315268903334, 1.0742140223435777, 1.0725396871566772, 1.0732379707405328, 1.0743843872950387, 1.0738989991703252, 1.0729299490087725, 1.0735303556977822, 1.0739059685094798, 1.0739060237098406, 1.0728502557391213, 1.0736577256363993, 1.0736452714954494, 1.0739890687375624, 1.0725045305950496, 1.0724288395472936, 1.0731250711262519, 1.0725045756166205, 1.0712878559219035, 1.0713228363121672, 1.0707760076413209, 1.0705778559636208, 1.0711644056003864, 1.0708319314790673], 'val_acc': [0.3953201959187957, 0.39408866867839015, 0.3998357980024247, 0.39449918010747687, 0.3940886710762782, 0.3936781596471915, 0.39285714422736456, 0.3949096867407875, 0.3949096870344065, 0.39655172555708923, 0.39367815994081046, 0.39244663495148346, 0.3998357954087907, 0.39860427076201915, 0.39408866867839015, 0.3981937593329325, 0.39244663495148346, 0.3944991801564134, 0.3932676506159928, 0.3994252864265285, 0.40229884959598283, 0.3998357955066637, 0.3973727412215986, 0.39737274136840806, 0.3949096894322945, 0.39860427076201915, 0.39655172320813775, 0.39573070749469186, 0.3981937616329475, 0.3977832525528123, 0.38916256045081543, 0.3936781623876349, 0.3969622343436055, 0.39819375908825, 0.3949096870344065, 0.3990147771017109, 0.39614121398119306, 0.3903940874465385, 0.3846469612544393, 0.3866995087593843, 0.3895730692862681, 0.3969622321414634, 0.39367815989187394, 0.39942528877548006, 0.4006568135201246, 0.39901477734639335, 0.3891625628976399, 0.3949096869365335, 0.3920361237181427, 0.391625617182705, 0.39614121407906605, 0.39942528872654354, 0.38423645481687463, 0.40106732255132327, 0.39449917790533484, 0.39737274117266214, 0.3866995087104478, 0.39408866867839015, 0.3957307075436284, 0.39778325015492433, 0.39860426826625817, 0.3986042707130827, 0.39039408754441146, 0.39449917800320783, 0.39655172320813775, 0.3969622322393364, 0.3899835810089738, 0.394909689383358, 0.39244663284721437, 0.3977832526017488, 0.388341542584164, 0.39983579785561524, 0.3981937592350595, 0.4006568135690611, 0.38013136435807826, 0.3981937593329325, 0.39696223444147843, 0.40188834291373565, 0.37602627399715494, 0.4006568135201246, 0.3891625628976399, 0.4006568159180126, 0.38957307227139404, 0.39737274117266214, 0.39696223449041496, 0.4002463067889409, 0.3953201984145567, 0.39696223458828794, 0.39778325035067025, 0.39901477734639335, 0.3961412164280176, 0.3981937617308205, 0.38628899718348814, 0.3973727412215986, 0.3842364544253827, 0.3957307050478674, 0.39490968688759703, 0.3764367807283386, 0.3973727410258526, 0.39573070739681887, 0.39614121407906605, 0.39490968928548503, 0.38628899997286803, 0.3940886712720241, 0.398193761584011, 0.3961412164280176, 0.38834154238841806, 0.39490968708334295, 0.3969622322393364, 0.4002463068868139, 0.3953201983656202, 0.3961412141280026, 0.3986042684130676, 0.39573070759256485, 0.39860427076201915, 0.39614121652589057, 0.39532019616347813, 0.39573070509680386, 0.3998357979045517, 0.3977832526506853, 0.3977832526506853, 0.3928571441294916, 0.39696223219039994, 0.39942528623078255, 0.3977832503017338, 0.39408867132096065, 0.39449917800320783, 0.3986042682173217, 0.3944991779542713, 0.39408866892307265, 0.39449918030322284, 0.3957307074457554, 0.39367816228976193, 0.39490968708334295, 0.39573070749469186, 0.3940886689720091, 0.39573070749469186, 0.3949096870344065, 0.39408867141883364, 0.39655172560602575, 0.3924466351961659, 0.3940886713698971, 0.39367816233869846, 0.3961412142258755, 0.398193761681884, 0.3924466327982779, 0.3973727436194866, 0.39696223458828794, 0.3953201984634932, 0.3986042705662732, 0.3981937617308205, 0.4047619061321265, 0.40229885199387083, 0.40394088802079264, 0.38628899948350315, 0.39655172555708923, 0.38505747238990706, 0.39614121628120813, 0.3875205242281477, 0.3977832502038608, 0.38834154233948154, 0.3977832499102419, 0.3871100175459005, 0.4006568135201246, 0.40106732255132327, 0.4018883428647992, 0.3973727410258526, 0.4010673224045138, 0.39614121652589057, 0.3981937593329325, 0.3981937593329325, 0.3969622321414634, 0.3969622322393364, 0.3969622346372244, 0.3990147796953449, 0.3846469634565814, 0.3866995063125598, 0.40476190378317495, 0.4068144514349294, 0.38628899997286803, 0.3936781599897469, 0.3944991779542713, 0.39573070749469186, 0.3973727412215986, 0.39162561703589555, 0.3920361237670792, 0.3940886712720241, 0.39408867132096065, 0.396141214274812, 0.38793103565723436, 0.39942528627971907, 0.39860426846200414, 0.401477833784664, 0.39532019861030265, 0.3924466330918968, 0.4006568158690761, 0.38793103335721935, 0.39860426836413115, 0.3879310333082829, 0.398193761584011, 0.3866995062146868, 0.3875205242281477, 0.3871100152458855, 0.39983579535985425, 0.3866995062146868, 0.39778325039960677, 0.3801313619112538, 0.3949096869365335, 0.3973727412705351, 0.4027093608782601, 0.4002463045378624, 0.3990147775421393, 0.39819376177975696, 0.40106732264919626, 0.3936781623876349, 0.3940886690209456, 0.39778325245493934, 0.38875205181110867, 0.39449918020534985, 0.4002463069846869, 0.4002463069357504, 0.39655172570389874, 0.3961412142258755, 0.39860426826625817, 0.3932676509585482, 0.39778325245493934, 0.3986042681683852, 0.39408867122308766, 0.3772577986439265, 0.3961412164280176, 0.4018883428158627, 0.3981937592350595, 0.39490968708334295, 0.39326765100748473, 0.3973727436194866, 0.3961412164280176, 0.3936781600386834, 0.39614121637908106, 0.39901477979321787, 0.39860427076201915, 0.4018883428158627, 0.3981937594308054, 0.4018883428158627, 0.39696223449041496, 0.3953201983166837, 0.39655172306132824, 0.3977832503017338, 0.40353037894065746, 0.38875205425793313, 0.3977832526996218, 0.39942528647546505, 0.3986042708109557, 0.3977832526996218, 0.39573070739681887, 0.39655172311026476, 0.3879310360487263, 0.39655172545921624, 0.4022988518470614, 0.40065681567333017, 0.38259441820271495, 0.3928571442763011, 0.39121510790682384, 0.3924466327493414, 0.39942528887335305, 0.40106732494921127, 0.39942528637759206, 0.3994252864265285, 0.401477831484649, 0.39737274117266214, 0.39942528872654354, 0.39860426826625817, 0.3977832502038608, 0.3977832526996218, 0.39655172306132824, 0.3953201983166837, 0.39573070499893087, 0.39573070499893087, 0.3858784908437964, 0.39696223219039994, 0.40024630443998943, 0.39367815984293747, 0.39573070499893087, 0.3953201983166837, 0.39614121652589057, 0.3977832502038608, 0.40065681572226663, 0.3977832502038608, 0.3953201983166837, 0.4002463043910529, 0.398193759186123, 0.3961412163301446, 0.3957307049499944, 0.4035303790874669, 0.3965517254102798, 0.3965517254102798, 0.40065681572226663, 0.39408867122308766, 0.40106732475346535, 0.40476190618106295, 0.4006568159180126], 'loss': [1.0930010965962185, 1.0744561320212833, 1.0749713717544838, 1.073805765155894, 1.073932202053266, 1.0741952036440494, 1.0749678937316676, 1.0747416466413338, 1.074744767427934, 1.0749031598563068, 1.0745006589184551, 1.073836054816628, 1.0746759035259301, 1.0740576442507013, 1.0741165294784296, 1.07474272454299, 1.074984914859952, 1.0737177303929104, 1.0742471201463892, 1.0736108534634725, 1.0736077127025847, 1.0739072222484456, 1.0733795350581958, 1.0733403692010492, 1.0736259416877856, 1.073316933780725, 1.0733495417561618, 1.0731434654161425, 1.072825494488162, 1.0729988039396627, 1.07271623929668, 1.073173694835796, 1.0729234187999546, 1.072993983135576, 1.0728896573583693, 1.0728272006741784, 1.0723572430914188, 1.0735844357547328, 1.0751729175540212, 1.0755990242321634, 1.0748495902858475, 1.0756127225789691, 1.0739738256779539, 1.0743022938283806, 1.0736399204089657, 1.0733657413439583, 1.0739678901323793, 1.0743968825075905, 1.0741941555814332, 1.0733911533375295, 1.0731720959626183, 1.0737425241626999, 1.0733957269353298, 1.074121043961151, 1.0725527259113852, 1.0726431446643336, 1.0730546120500664, 1.0732270957018561, 1.0750429825126757, 1.0732671459597485, 1.0730968021514236, 1.0731989157028512, 1.0729140602342893, 1.0734368798669114, 1.07376966540084, 1.0726311569096372, 1.0725369653662618, 1.072582449266798, 1.0732223692371126, 1.0728564692718536, 1.0730275126453297, 1.0724252648422117, 1.0726297015771729, 1.0719061500972302, 1.0729591567903083, 1.0729422622148017, 1.072800585768306, 1.0728851790790441, 1.0729225946402892, 1.0723177193126638, 1.0726619543725704, 1.0728997731355672, 1.0730019431828963, 1.0728194958865032, 1.0733246091944482, 1.0726372484308984, 1.0726515888923003, 1.0735986278776761, 1.0734601135371402, 1.072126098726809, 1.0723820849365766, 1.0727404101428555, 1.0728718624467477, 1.0727363291217562, 1.0724552084533097, 1.072780757371405, 1.0726961309660141, 1.0744089680285913, 1.0747885198808549, 1.0732329100798776, 1.073263965788808, 1.0724547459114748, 1.072082133606474, 1.0726799128726279, 1.0723224414202712, 1.0720815443160352, 1.0714892954796982, 1.0716660238634146, 1.0746953381160447, 1.0755393006228813, 1.0729758888789027, 1.0737231105260046, 1.0733051901725283, 1.0729692127180785, 1.0727894861840124, 1.0723783242873832, 1.072267173545806, 1.0721805177430108, 1.072151199821574, 1.0725748500784809, 1.0721164668609964, 1.072307537076899, 1.072174268728409, 1.0718992960281686, 1.0721214439834656, 1.0723759268833137, 1.0721950294055977, 1.0724294732483506, 1.0719276479137507, 1.0720137858537677, 1.0719000744379032, 1.0719672338918493, 1.0724997902063373, 1.0725919336508922, 1.0718969434683328, 1.0718457046720282, 1.0715718739820947, 1.072470072897063, 1.0714545571583742, 1.0712143901437214, 1.0710034520719085, 1.0710368240638435, 1.0717829947598905, 1.0719526486230337, 1.0712311403952095, 1.0711865342128448, 1.0711743579508097, 1.0713077952974386, 1.070885216382005, 1.0704622551156264, 1.0719692674260854, 1.0705328098802351, 1.070953120881772, 1.070602032780892, 1.0705000271297822, 1.0699846237836677, 1.070133566415775, 1.0705467536953686, 1.0698244340121135, 1.070363967815219, 1.070523204842632, 1.0713817048121772, 1.0717572487355258, 1.071367288125369, 1.0708603443795894, 1.070560934166644, 1.071050001218823, 1.071472304068062, 1.0711650855976942, 1.0700693846238467, 1.0706317190761683, 1.0697926963379252, 1.0705285988304405, 1.0699791510247108, 1.0693913579231904, 1.0698482012601849, 1.0715057990389438, 1.0721070138825528, 1.0702299364783192, 1.0700249374279986, 1.0696606724658786, 1.0695625423161157, 1.0706780785163081, 1.0710281442567797, 1.0698059533900548, 1.0694081505955613, 1.0698524894166042, 1.0701857547740428, 1.0703253546779405, 1.0700383597086098, 1.0696299668455025, 1.0697802932355438, 1.0691196771128222, 1.0699192985616914, 1.0688815461536698, 1.0691769013904204, 1.0687911181234482, 1.0690002705282255, 1.0691658277041614, 1.068099370139825, 1.0682029773567248, 1.0708565430474721, 1.0681427146864624, 1.0710517027539639, 1.0687927112931832, 1.0686898928648147, 1.0688501973661308, 1.0691873523488915, 1.070763876550741, 1.0697752199133808, 1.0689351607152324, 1.0687897816820555, 1.0687590594653966, 1.0709606488382548, 1.069634804294829, 1.069256110553624, 1.0687408256824502, 1.0685278571361878, 1.0703730105619411, 1.0699314191845652, 1.0703791146405668, 1.0688756938342932, 1.068452301407252, 1.0686790043813248, 1.0690764317032737, 1.069213378943457, 1.0696765342042676, 1.0694698599084937, 1.0698290162507513, 1.0695296465249031, 1.0688998430171786, 1.0688565964571504, 1.0683435568329736, 1.0693396255220966, 1.068424056442856, 1.0683096439197077, 1.0703631455403824, 1.0694150166345082, 1.0691300441597031, 1.0690131399176204, 1.0688185007909974, 1.0698029414829042, 1.0699892587975066, 1.069011605936399, 1.0688542226991125, 1.0685016841124706, 1.0685422257721058, 1.0688158673672217, 1.0704308969529013, 1.0686945172550741, 1.0678759659095467, 1.0677903119053929, 1.0680198536761243, 1.0677230411976024, 1.0678700623326234, 1.0667966528839643, 1.0674972003490284, 1.0674010616063583, 1.0681750073814784, 1.0672009483255156, 1.0677625148448122, 1.071399008762665, 1.0684000716806683, 1.0689677005920566, 1.0688863670556696, 1.0687621698242438, 1.0698891149164471, 1.0688108767816908, 1.0684586236119515, 1.068494379422503, 1.0690052244452726, 1.0683607749136077, 1.0677318849602764, 1.0666814116971448, 1.0679017909253647, 1.0669795161155216, 1.0674809257107838, 1.0669194763446002, 1.0666828244129, 1.0672195490380822, 1.0682735546658415, 1.0670793298823142, 1.0667314968069965, 1.0665511624524235, 1.0674900747667349, 1.066924200508384, 1.0664777963558016, 1.066146888062205, 1.0663503121546407, 1.0663776558282683, 1.0677437647167418, 1.06702649152744, 1.0676239942867898, 1.0660914848227765, 1.066074666644024, 1.0662664939980242, 1.0668927017912972, 1.066276294886454, 1.067670075506645, 1.0675535262732536], 'acc': [0.34609856262833677, 0.3906570841644334, 0.3934291581231221, 0.39466119097733154, 0.39147843942505134, 0.3943531827637792, 0.3979466119096509, 0.39466119095285324, 0.39537987678447545, 0.39445585214381834, 0.3975359342671028, 0.39958932235745187, 0.39373716633667444, 0.3943531827637792, 0.3898357289527721, 0.39425051335926176, 0.3805954825339621, 0.3944558521560575, 0.3901437371785636, 0.3952772073921971, 0.39774127310673557, 0.4018480492813142, 0.402977412718767, 0.39856262833675565, 0.40071868584386133, 0.4014373716510052, 0.397741273100616, 0.39897330595482544, 0.4020533881025882, 0.4011293634619311, 0.39969199176196935, 0.4023613963161406, 0.40123203282973113, 0.3977412730761377, 0.397125256649033, 0.40154004105552266, 0.4020533881025882, 0.3956878850102669, 0.3934291580986438, 0.3830595482668593, 0.3847022587268994, 0.3941478439547443, 0.39445585216829665, 0.3979466119096509, 0.39825462011096413, 0.3994866529651736, 0.3941478439180268, 0.39455852157281407, 0.3894250513469414, 0.397330595485606, 0.40102669402069624, 0.39476386036960986, 0.398870636550308, 0.3914784394372905, 0.404106776168459, 0.4002053388090349, 0.3963039014251325, 0.4007186858316222, 0.392505133482465, 0.3972279260657896, 0.40051334701034813, 0.3962012320206151, 0.398459958919999, 0.3953798767967146, 0.39650924023416745, 0.40102669404517455, 0.39897330597930375, 0.401745379901275, 0.40051334701034813, 0.40061601640262645, 0.4001026693922783, 0.4065708418768779, 0.39712525666127213, 0.40472279260780286, 0.40020533882127407, 0.3959958932238193, 0.4004106776425481, 0.39835728952772076, 0.3981519507064467, 0.3993839835728953, 0.3986652977535122, 0.40061601641486555, 0.39876796712131224, 0.3965092402586457, 0.39640657082965, 0.4009240246284179, 0.4, 0.39681724847219807, 0.3980492813141684, 0.402772073897493, 0.40349075975359344, 0.3998973306077217, 0.40215605749486655, 0.3994866529774127, 0.4011293634619311, 0.39106776181922065, 0.3994866529835323, 0.3966119096631632, 0.3802874743204097, 0.39774127308837687, 0.389117043133389, 0.397741273100616, 0.40277207392197123, 0.3897330595727329, 0.3984599589567165, 0.39301848049893273, 0.40349075977807175, 0.40102669404517455, 0.39455852156057497, 0.39856262831227734, 0.39702258726899387, 0.3939425051457094, 0.40256673511905594, 0.4002053388090349, 0.4026694045174538, 0.396611909650924, 0.4002053388335132, 0.40030800821355234, 0.40010266940451744, 0.3960985626160976, 0.4010266940329354, 0.39743326488706365, 0.4, 0.4007186858316222, 0.4014373716754835, 0.4015400410432835, 0.40030800823803064, 0.39969199178644765, 0.4010266940512941, 0.40030800821355234, 0.40102669404517455, 0.4015400410677618, 0.39928131418061696, 0.3959958932238193, 0.4032854209323194, 0.39835728952772076, 0.4030800821355236, 0.39887063653806887, 0.39989733057100424, 0.4003080082257915, 0.39897330595482544, 0.40082135523613965, 0.39630390144349126, 0.3931211498728523, 0.400513346998109, 0.40102669402069624, 0.3990759753348646, 0.4012320328419703, 0.40154004108000096, 0.4014373716510052, 0.40266940450521466, 0.4023613962916623, 0.40133470226484647, 0.39948665298965186, 0.39887063653806887, 0.4032854209323194, 0.3975359342671028, 0.40174537988903586, 0.40246406573289717, 0.40266940452969296, 0.40030800820131324, 0.39291581107605655, 0.4006160164271047, 0.3976386036838594, 0.39866529774127313, 0.39815195073092496, 0.3842915811210687, 0.39958932239416933, 0.3978439424806552, 0.40359342914587176, 0.4001026694167566, 0.4021560575071057, 0.4009240246284179, 0.3966119096631632, 0.39825462013544244, 0.40236139632837975, 0.40041067760583066, 0.40051334703482644, 0.4031827515400411, 0.3970225872751134, 0.39835728950324245, 0.4016427104722793, 0.402566735088458, 0.3933264887063655, 0.39178644762636455, 0.3995893224064085, 0.39856262833675565, 0.4042094455882754, 0.39630390143737165, 0.3913757699960556, 0.3967145790554415, 0.39928131418061696, 0.3991786447760995, 0.3944558521560575, 0.39620123203285423, 0.4029774127310062, 0.4014373716510052, 0.4010266940574137, 0.39969199178644765, 0.4025667351006972, 0.4025667351006972, 0.3912731006037773, 0.39722792605355045, 0.39414784394862473, 0.40328542095679765, 0.39856262833981543, 0.397125256649033, 0.39671457906768065, 0.40051334703482644, 0.39753593429158113, 0.4043121149774939, 0.3996919917986868, 0.3996919917742085, 0.40277207392197123, 0.399383983548417, 0.3978439425173726, 0.39938398358513444, 0.395277207379958, 0.3856262833430782, 0.4002053388090349, 0.39681724844771976, 0.40071868584386133, 0.39969199178644765, 0.4004106776180698, 0.3970225872567547, 0.4003080081890741, 0.400718685819383, 0.39476386037266964, 0.3930184804958729, 0.3953798767967146, 0.40143737168772264, 0.40143737166936394, 0.40369609856262834, 0.3982546201232033, 0.40020533879679576, 0.4023613963069612, 0.40277207392197123, 0.40246406571453847, 0.4005133470225873, 0.40112936343745287, 0.3993839835728953, 0.4005133470225873, 0.3953798767722363, 0.4005133470225873, 0.40277207392197123, 0.40349075975359344, 0.3935318275154004, 0.3889117042998758, 0.4010266940574137, 0.402977412718767, 0.4042094455852156, 0.4024640656839406, 0.4036960985748675, 0.40379876797938496, 0.4020533880964686, 0.40338809034907597, 0.40349075975359344, 0.4017453798767967, 0.40236139630390144, 0.3965092402586457, 0.3994866529774127, 0.3922997946489518, 0.3952772073677188, 0.40184804929355333, 0.40359342917035, 0.40328542095679765, 0.39650924023416745, 0.3981519507186858, 0.40277207390973213, 0.4026694044929755, 0.40277207392197123, 0.4031827515278019, 0.40195071869807075, 0.4019507187103099, 0.40236139630390144, 0.4021560574826274, 0.4006160164332243, 0.4009240246284179, 0.40041067763030896, 0.40266940450521466, 0.40256673513741464, 0.4044147843942505, 0.4026694045174538, 0.4042094455852156, 0.40256673511905594, 0.4035934291336326, 0.4031827515400411, 0.40369609858710664, 0.40266940450521466, 0.4031827515522802, 0.40574948666521654, 0.4041067761806982, 0.402772073897493, 0.4009240246406571, 0.4041067761806982, 0.40420944557297644, 0.4025667351006972, 0.40277207392197123, 0.404106776168459, 0.40379876795490666]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
