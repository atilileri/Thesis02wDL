{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf75.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 00:59:32 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000018C065F9550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000018C6A0E7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0885, Accuracy:0.3639, Validation Loss:1.0815, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0780, Accuracy:0.3729, Validation Loss:1.0760, Validation Accuracy:0.3727\n",
    "Epoch #3: Loss:1.0749, Accuracy:0.3881, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0753, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3934, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0739, Accuracy:0.3934, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0739, Accuracy:0.3967, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #29: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #30: Loss:1.0737, Accuracy:0.3910, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #32: Loss:1.0738, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #33: Loss:1.0740, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #34: Loss:1.0746, Accuracy:0.3910, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #35: Loss:1.0740, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #36: Loss:1.0741, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #37: Loss:1.0745, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #38: Loss:1.0749, Accuracy:0.4004, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0743, Accuracy:0.3967, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #40: Loss:1.0739, Accuracy:0.3992, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #41: Loss:1.0739, Accuracy:0.3992, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #42: Loss:1.0741, Accuracy:0.3979, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #43: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0734, Validation Accuracy:0.3924\n",
    "Epoch #44: Loss:1.0739, Accuracy:0.3979, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #45: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #46: Loss:1.0737, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3924\n",
    "Epoch #47: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0736, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #49: Loss:1.0742, Accuracy:0.3963, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #50: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #51: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #54: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #55: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #57: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #58: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #59: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #62: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #64: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #65: Loss:1.0738, Accuracy:0.3975, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #66: Loss:1.0740, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #67: Loss:1.0739, Accuracy:0.4012, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #68: Loss:1.0739, Accuracy:0.4000, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #69: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #70: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #72: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #73: Loss:1.0736, Accuracy:0.3938, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #74: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #75: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #77: Loss:1.0739, Accuracy:0.3979, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #78: Loss:1.0735, Accuracy:0.3971, Validation Loss:1.0735, Validation Accuracy:0.3974\n",
    "Epoch #79: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #80: Loss:1.0738, Accuracy:0.3996, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #81: Loss:1.0735, Accuracy:0.3979, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #82: Loss:1.0734, Accuracy:0.3988, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #83: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #84: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #85: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #86: Loss:1.0735, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #87: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #88: Loss:1.0737, Accuracy:0.3971, Validation Loss:1.0736, Validation Accuracy:0.4023\n",
    "Epoch #89: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #90: Loss:1.0733, Accuracy:0.3959, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #91: Loss:1.0732, Accuracy:0.4000, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #92: Loss:1.0731, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #93: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #94: Loss:1.0731, Accuracy:0.3951, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #95: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0733, Validation Accuracy:0.3990\n",
    "Epoch #96: Loss:1.0732, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #97: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #98: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #99: Loss:1.0728, Accuracy:0.3988, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #100: Loss:1.0727, Accuracy:0.3955, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #101: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #102: Loss:1.0730, Accuracy:0.3996, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #103: Loss:1.0731, Accuracy:0.3992, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #104: Loss:1.0730, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #105: Loss:1.0731, Accuracy:0.3975, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #106: Loss:1.0731, Accuracy:0.3979, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #107: Loss:1.0731, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #108: Loss:1.0730, Accuracy:0.3930, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #109: Loss:1.0732, Accuracy:0.3930, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #110: Loss:1.0730, Accuracy:0.3955, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #111: Loss:1.0730, Accuracy:0.3955, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #112: Loss:1.0729, Accuracy:0.3926, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #113: Loss:1.0729, Accuracy:0.3955, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #114: Loss:1.0729, Accuracy:0.3930, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #115: Loss:1.0729, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #116: Loss:1.0731, Accuracy:0.3906, Validation Loss:1.0729, Validation Accuracy:0.3990\n",
    "Epoch #117: Loss:1.0729, Accuracy:0.3938, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #118: Loss:1.0733, Accuracy:0.3951, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #119: Loss:1.0729, Accuracy:0.3984, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #120: Loss:1.0730, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #121: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #122: Loss:1.0729, Accuracy:0.3959, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #123: Loss:1.0731, Accuracy:0.3975, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #124: Loss:1.0726, Accuracy:0.3926, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #125: Loss:1.0731, Accuracy:0.3967, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #126: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #127: Loss:1.0732, Accuracy:0.3877, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #128: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #129: Loss:1.0734, Accuracy:0.3926, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #130: Loss:1.0729, Accuracy:0.3959, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #131: Loss:1.0723, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #132: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #133: Loss:1.0726, Accuracy:0.3975, Validation Loss:1.0733, Validation Accuracy:0.3990\n",
    "Epoch #134: Loss:1.0725, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #135: Loss:1.0724, Accuracy:0.3971, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #136: Loss:1.0725, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #137: Loss:1.0728, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #138: Loss:1.0726, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #139: Loss:1.0723, Accuracy:0.3934, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #140: Loss:1.0723, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #141: Loss:1.0725, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #142: Loss:1.0722, Accuracy:0.3963, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #143: Loss:1.0722, Accuracy:0.3955, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #144: Loss:1.0725, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #145: Loss:1.0721, Accuracy:0.3951, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #146: Loss:1.0722, Accuracy:0.3967, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #147: Loss:1.0722, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #148: Loss:1.0724, Accuracy:0.3996, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #149: Loss:1.0724, Accuracy:0.3955, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #150: Loss:1.0723, Accuracy:0.3951, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #151: Loss:1.0723, Accuracy:0.3922, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #152: Loss:1.0721, Accuracy:0.3959, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #153: Loss:1.0722, Accuracy:0.3967, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #154: Loss:1.0718, Accuracy:0.3922, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #155: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #156: Loss:1.0726, Accuracy:0.3947, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #157: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #158: Loss:1.0723, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #159: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #160: Loss:1.0722, Accuracy:0.3951, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #161: Loss:1.0721, Accuracy:0.4000, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #162: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #163: Loss:1.0724, Accuracy:0.3930, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #164: Loss:1.0721, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #165: Loss:1.0722, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #166: Loss:1.0719, Accuracy:0.3938, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #167: Loss:1.0720, Accuracy:0.3967, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #168: Loss:1.0721, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #169: Loss:1.0720, Accuracy:0.3930, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #170: Loss:1.0721, Accuracy:0.3934, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #171: Loss:1.0717, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #172: Loss:1.0720, Accuracy:0.3922, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #173: Loss:1.0720, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #174: Loss:1.0721, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #175: Loss:1.0718, Accuracy:0.3955, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #176: Loss:1.0719, Accuracy:0.3967, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #177: Loss:1.0718, Accuracy:0.3947, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #178: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #179: Loss:1.0718, Accuracy:0.3959, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #180: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #181: Loss:1.0719, Accuracy:0.3947, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #182: Loss:1.0718, Accuracy:0.3955, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #183: Loss:1.0719, Accuracy:0.3930, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #184: Loss:1.0719, Accuracy:0.3934, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #185: Loss:1.0720, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #186: Loss:1.0719, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #187: Loss:1.0718, Accuracy:0.3996, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #188: Loss:1.0719, Accuracy:0.3934, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #189: Loss:1.0713, Accuracy:0.3901, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #190: Loss:1.0727, Accuracy:0.3959, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #191: Loss:1.0722, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #192: Loss:1.0718, Accuracy:0.3979, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #193: Loss:1.0722, Accuracy:0.3984, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #194: Loss:1.0717, Accuracy:0.3992, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #195: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.4007\n",
    "Epoch #196: Loss:1.0718, Accuracy:0.3893, Validation Loss:1.0735, Validation Accuracy:0.3974\n",
    "Epoch #197: Loss:1.0715, Accuracy:0.3934, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #198: Loss:1.0714, Accuracy:0.3926, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #199: Loss:1.0714, Accuracy:0.3996, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #200: Loss:1.0714, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #201: Loss:1.0715, Accuracy:0.3934, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #202: Loss:1.0714, Accuracy:0.3926, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #203: Loss:1.0714, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #204: Loss:1.0711, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #205: Loss:1.0711, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #206: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #207: Loss:1.0710, Accuracy:0.4012, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #208: Loss:1.0712, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #209: Loss:1.0710, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #210: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #211: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #212: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0743, Validation Accuracy:0.4007\n",
    "Epoch #213: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #214: Loss:1.0714, Accuracy:0.3979, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #215: Loss:1.0707, Accuracy:0.3971, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #216: Loss:1.0712, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #217: Loss:1.0710, Accuracy:0.3992, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #218: Loss:1.0708, Accuracy:0.3971, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #219: Loss:1.0707, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.4007\n",
    "Epoch #220: Loss:1.0708, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #221: Loss:1.0714, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.4007\n",
    "Epoch #222: Loss:1.0707, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #223: Loss:1.0712, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #224: Loss:1.0708, Accuracy:0.4016, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #225: Loss:1.0709, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #226: Loss:1.0706, Accuracy:0.3988, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #227: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #228: Loss:1.0707, Accuracy:0.3996, Validation Loss:1.0748, Validation Accuracy:0.4007\n",
    "Epoch #229: Loss:1.0708, Accuracy:0.3996, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #230: Loss:1.0707, Accuracy:0.3975, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #231: Loss:1.0707, Accuracy:0.4041, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #232: Loss:1.0705, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.4007\n",
    "Epoch #233: Loss:1.0710, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.4007\n",
    "Epoch #234: Loss:1.0703, Accuracy:0.4008, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #235: Loss:1.0710, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.4007\n",
    "Epoch #236: Loss:1.0704, Accuracy:0.4004, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #237: Loss:1.0711, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.4007\n",
    "Epoch #238: Loss:1.0706, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #239: Loss:1.0706, Accuracy:0.3988, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #240: Loss:1.0706, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #241: Loss:1.0704, Accuracy:0.3996, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #242: Loss:1.0704, Accuracy:0.3971, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #243: Loss:1.0703, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3990\n",
    "Epoch #244: Loss:1.0706, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.4007\n",
    "Epoch #245: Loss:1.0707, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #246: Loss:1.0703, Accuracy:0.3992, Validation Loss:1.0747, Validation Accuracy:0.4007\n",
    "Epoch #247: Loss:1.0705, Accuracy:0.3988, Validation Loss:1.0752, Validation Accuracy:0.3974\n",
    "Epoch #248: Loss:1.0714, Accuracy:0.4012, Validation Loss:1.0744, Validation Accuracy:0.4007\n",
    "Epoch #249: Loss:1.0702, Accuracy:0.4000, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #250: Loss:1.0707, Accuracy:0.4008, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #251: Loss:1.0701, Accuracy:0.3996, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #252: Loss:1.0716, Accuracy:0.3996, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #253: Loss:1.0703, Accuracy:0.4012, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #254: Loss:1.0707, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.4023\n",
    "Epoch #255: Loss:1.0705, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #256: Loss:1.0702, Accuracy:0.3992, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #257: Loss:1.0704, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #258: Loss:1.0705, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.4007\n",
    "Epoch #259: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.4007\n",
    "Epoch #260: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #261: Loss:1.0701, Accuracy:0.4004, Validation Loss:1.0756, Validation Accuracy:0.4007\n",
    "Epoch #262: Loss:1.0702, Accuracy:0.4004, Validation Loss:1.0746, Validation Accuracy:0.4023\n",
    "Epoch #263: Loss:1.0698, Accuracy:0.3992, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #264: Loss:1.0708, Accuracy:0.3992, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #265: Loss:1.0699, Accuracy:0.3992, Validation Loss:1.0754, Validation Accuracy:0.4007\n",
    "Epoch #266: Loss:1.0713, Accuracy:0.4008, Validation Loss:1.0755, Validation Accuracy:0.4007\n",
    "Epoch #267: Loss:1.0709, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #268: Loss:1.0705, Accuracy:0.3996, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #269: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #270: Loss:1.0703, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #271: Loss:1.0700, Accuracy:0.3979, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #272: Loss:1.0700, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #273: Loss:1.0702, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #274: Loss:1.0697, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.4023\n",
    "Epoch #275: Loss:1.0700, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.4023\n",
    "Epoch #276: Loss:1.0699, Accuracy:0.3992, Validation Loss:1.0744, Validation Accuracy:0.4023\n",
    "Epoch #277: Loss:1.0698, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #278: Loss:1.0708, Accuracy:0.3988, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #279: Loss:1.0699, Accuracy:0.3992, Validation Loss:1.0762, Validation Accuracy:0.4007\n",
    "Epoch #280: Loss:1.0704, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #281: Loss:1.0702, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #282: Loss:1.0697, Accuracy:0.4000, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #283: Loss:1.0700, Accuracy:0.4016, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #284: Loss:1.0697, Accuracy:0.4000, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #285: Loss:1.0698, Accuracy:0.4025, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #286: Loss:1.0698, Accuracy:0.3988, Validation Loss:1.0744, Validation Accuracy:0.4023\n",
    "Epoch #287: Loss:1.0701, Accuracy:0.4000, Validation Loss:1.0746, Validation Accuracy:0.4023\n",
    "Epoch #288: Loss:1.0700, Accuracy:0.3992, Validation Loss:1.0747, Validation Accuracy:0.4023\n",
    "Epoch #289: Loss:1.0699, Accuracy:0.3984, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #290: Loss:1.0695, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.4023\n",
    "Epoch #291: Loss:1.0697, Accuracy:0.3992, Validation Loss:1.0750, Validation Accuracy:0.4023\n",
    "Epoch #292: Loss:1.0695, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #293: Loss:1.0701, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #294: Loss:1.0702, Accuracy:0.4004, Validation Loss:1.0755, Validation Accuracy:0.3990\n",
    "Epoch #295: Loss:1.0698, Accuracy:0.4004, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #296: Loss:1.0698, Accuracy:0.3996, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #297: Loss:1.0698, Accuracy:0.4012, Validation Loss:1.0754, Validation Accuracy:0.4023\n",
    "Epoch #298: Loss:1.0696, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.4007\n",
    "Epoch #299: Loss:1.0695, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #300: Loss:1.0698, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07426310, Accuracy:0.4023\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  27  200   0\n",
    "t:01  22  218   0\n",
    "t:03  12  130   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.44      0.12      0.19       227\n",
    "          01       0.40      0.91      0.55       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.40       609\n",
    "   macro avg       0.28      0.34      0.25       609\n",
    "weighted avg       0.32      0.40      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 01:40:09 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 37 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0815165647732212, 1.0760472410026638, 1.0748124868411737, 1.0752746058606553, 1.075637435090953, 1.0748144523263565, 1.074801396658072, 1.0746616007659235, 1.074682354535571, 1.0744730085379188, 1.0743058379647767, 1.074436293251213, 1.0741307069906851, 1.0739107028212649, 1.0739737764759407, 1.0739817245448948, 1.073946436246236, 1.073903758929085, 1.0739082080390066, 1.0739041116633048, 1.073788495095101, 1.073671169273176, 1.0736228368552447, 1.0736267208465802, 1.073633631266201, 1.0736296646700705, 1.0736771921806147, 1.0736172892189966, 1.0735260228609609, 1.073666565915438, 1.0737798683749045, 1.0737075594258425, 1.07415804624166, 1.0740248767417444, 1.0732932437229625, 1.0738914091207319, 1.0738190342052816, 1.073588159871219, 1.073657400698106, 1.0738559456294394, 1.0736705266391897, 1.0733850708931734, 1.0733519739705353, 1.0732056350739327, 1.0730335101705466, 1.0733667416329846, 1.0732987784399775, 1.0732721720618763, 1.0744057784135315, 1.074589358174742, 1.074653202677008, 1.074279891837798, 1.0741742970712471, 1.0741222065266325, 1.0739115791759273, 1.073698488753809, 1.0736255136812458, 1.073541922130804, 1.0738796034861473, 1.0738885522084478, 1.0739678430244057, 1.0739062544943272, 1.0739734525163773, 1.0738124132939355, 1.0737502676708552, 1.0735912777128673, 1.073629807760367, 1.0734832280962339, 1.0737118748412735, 1.0736235894984605, 1.0736831287640851, 1.0736785880450546, 1.0736887689881724, 1.0737307525816417, 1.0736464177837903, 1.0735249075004816, 1.0735849774334034, 1.073514881196672, 1.0736923423306695, 1.0735026096867026, 1.073440841070341, 1.0736025143139467, 1.0734799591387043, 1.0734592885610896, 1.0735414190636872, 1.0733217919009856, 1.0736674082103035, 1.07357189749262, 1.073383677769177, 1.0734814712762442, 1.073382453965436, 1.0734119421155581, 1.073312955145374, 1.0735994628301786, 1.0732598314535833, 1.0734091766166374, 1.0730927622768482, 1.0731206171226815, 1.0731552629830998, 1.0736121712451303, 1.0733992679561497, 1.0733774436518477, 1.0738666425589074, 1.073205414663982, 1.0737307674583347, 1.073417881830964, 1.0732201711689113, 1.0733020354570035, 1.0732613862637423, 1.073484377712256, 1.073138933267891, 1.0730882654049125, 1.072955686274812, 1.073003098882478, 1.0732237903159632, 1.0728621868468662, 1.073125890714586, 1.073192186152015, 1.073675475097055, 1.0733255850661956, 1.0735684682191495, 1.0735512404214769, 1.0735782151934745, 1.0739176195047564, 1.073485620699101, 1.0736049480234657, 1.0737424744369557, 1.0743028912050971, 1.0737359619688713, 1.0734660333796284, 1.0737251497450329, 1.073616736040914, 1.0733045897460336, 1.0732229638765207, 1.0733851413617188, 1.0734254227483213, 1.0733192125564726, 1.0735523884715314, 1.0733811011651075, 1.0733624452049118, 1.0736891319011819, 1.073476951502031, 1.0735719350758444, 1.0738323161558956, 1.073830983713147, 1.0741465770943803, 1.0741433541371512, 1.0737577128684384, 1.0737233140394213, 1.0737913508329093, 1.0736268563027844, 1.0735113534629834, 1.0737470041942128, 1.0736361328995678, 1.0741447852358639, 1.0741483857870493, 1.0740258055563239, 1.073869984138188, 1.0737581754161416, 1.0736551102746297, 1.073606326270769, 1.0739243562977105, 1.0736188947273593, 1.0733389832899096, 1.073448828092741, 1.0737706154633821, 1.07418016279468, 1.073634122784306, 1.07356281938224, 1.0734970362119878, 1.073897952712424, 1.0742612902949793, 1.0733794353669892, 1.0734119190175349, 1.073782488434577, 1.073665708156642, 1.073273075820973, 1.0735768802060282, 1.0736765387806007, 1.0736916644624106, 1.0734648506825388, 1.0733195003030336, 1.0733353846961837, 1.0731415985448802, 1.0730833670580133, 1.0733732889438499, 1.0732163089053777, 1.073194536865247, 1.0742635184712401, 1.075549465682119, 1.0744915202333423, 1.0741886659991762, 1.0744966374242246, 1.0739521426324579, 1.073688755677447, 1.07348133582004, 1.0735761434182354, 1.0737286719978345, 1.0738671534558626, 1.0738881002310265, 1.0738897681823505, 1.0735887476963362, 1.073602864503469, 1.0742055369519639, 1.0742911444900463, 1.0743555184851334, 1.0745416572332773, 1.0741411206757494, 1.074192635140004, 1.0741438307785636, 1.0741303967333389, 1.0743425082299118, 1.0742340459807949, 1.074018915690029, 1.0745061201415038, 1.0744898184179672, 1.0738754618931285, 1.0741420970565971, 1.0743895702565636, 1.0745108413383095, 1.0745975701092498, 1.073913827514022, 1.07424010588422, 1.0750471876172596, 1.074212850999754, 1.0739874839782715, 1.0744528811553429, 1.07482935544501, 1.0740082776800948, 1.074780582794415, 1.0742243266066502, 1.074351776605365, 1.0748970128828277, 1.0741222445013487, 1.0743451582387162, 1.0754802518681743, 1.0747036107850976, 1.0739417684880774, 1.074478357296272, 1.0744807082052497, 1.0738352511708176, 1.074066366272411, 1.0751129635449113, 1.0746086841733584, 1.0740923184675144, 1.0747198441932941, 1.0752469557650963, 1.0744384672058431, 1.0750373631275345, 1.0749588846573102, 1.0740960446876062, 1.0747094357933709, 1.0739194017717208, 1.0746162084523092, 1.0745049901000776, 1.0739023556262988, 1.0741609497414826, 1.075330050512292, 1.07437252274092, 1.0740361495558264, 1.075620061266794, 1.0746396365032602, 1.0740177584399144, 1.0739993352216648, 1.075434773034846, 1.0755248643298847, 1.074112686421875, 1.0745163263358506, 1.0749189268387793, 1.0743277654271994, 1.0738464247416981, 1.0741421628272396, 1.0741557479883455, 1.0748551839286666, 1.0749185921131879, 1.074358670582325, 1.0740349263393234, 1.0740149874601066, 1.0761876906863184, 1.0743125880684563, 1.0737556148632406, 1.074711842881439, 1.0747278823053896, 1.0742165474664598, 1.0739988985124285, 1.0743878155897795, 1.0745673181583923, 1.0747171861589053, 1.0742072180182671, 1.0751410344942842, 1.074954852486283, 1.074536846189076, 1.0740869156832766, 1.0755204293136722, 1.0742179983355142, 1.0740483828953333, 1.075381572023401, 1.0752106102425085, 1.0741958960719493, 1.0742632037117368], 'val_acc': [0.37274219930074093, 0.3727421994964869, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3973727417599, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3940886696081835, 0.39408866941243753, 0.3940886695103105, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3924466333855158, 0.3924466333855158, 0.39408866941243753, 0.38752052481538557, 0.3891625611359263, 0.39080459726072103, 0.3924466333855158, 0.39080459726072103, 0.3924466333855158, 0.39080459716284804, 0.3940886695103105, 0.39080459726072103, 0.3924466332876428, 0.39080459716284804, 0.3940886695103105, 0.3924466332876428, 0.3924466333855158, 0.39901477798256774, 0.3924466332876428, 0.39408866931456454, 0.3924466331897698, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3957307055372323, 0.3940886695103105, 0.39408866941243753, 0.39408866941243753, 0.397372741857773, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39573070563510526, 0.39408866941243753, 0.39573070563510526, 0.3924466333855158, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3940886695103105, 0.397372741662027, 0.3940886695103105, 0.39408866941243753, 0.3940886695103105, 0.39901477788469475, 0.3973727417599, 0.3973727417599, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3940886695103105, 0.3990147776889488, 0.40229885013428424, 0.3940886695103105, 0.39408866941243753, 0.40229885013428424, 0.397372741857773, 0.3924466333855158, 0.3973727417599, 0.39573070563510526, 0.3973727417599, 0.39573070563510526, 0.39901477788469475, 0.39901477778682176, 0.39901477788469475, 0.39901477788469475, 0.397372741662027, 0.3957307055372323, 0.39573070563510526, 0.39901477788469475, 0.3957307055372323, 0.39737274156415403, 0.397372741857773, 0.3940886695103105, 0.3990147776889488, 0.3957307055372323, 0.397372741857773, 0.3940886695103105, 0.3940886695103105, 0.3990147776889488, 0.3940886695103105, 0.397372741857773, 0.39737274156415403, 0.3990147776889488, 0.39573070563510526, 0.39573070563510526, 0.3940886695103105, 0.3940886695103105, 0.39573070563510526, 0.39901477778682176, 0.39408866941243753, 0.3940886695103105, 0.40229884993853826, 0.397372741857773, 0.39244663377700767, 0.3990147776889488, 0.3924466335812617, 0.4006568138137435, 0.3940886695103105, 0.3940886695103105, 0.39901477788469475, 0.39901477788469475, 0.39901477788469475, 0.39901477798256774, 0.3924466335812617, 0.39901477788469475, 0.397372741662027, 0.397372741662027, 0.397372741662027, 0.3940886695103105, 0.3924466333855158, 0.3924466335812617, 0.3940886695103105, 0.3924466333855158, 0.3924466333855158, 0.3924466335812617, 0.39080459726072103, 0.39408866941243753, 0.3924466333855158, 0.39901477798256774, 0.39901477798256774, 0.39573070563510526, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3957307055372323, 0.3924466332876428, 0.3891625609401803, 0.3957307055372323, 0.3957307055372323, 0.3924466332876428, 0.3924466331897698, 0.39408866941243753, 0.39080459706497506, 0.3924466332876428, 0.3957307055372323, 0.3957307055372323, 0.3924466331897698, 0.3957307055372323, 0.3924466332876428, 0.39901477778682176, 0.39901477778682176, 0.39408866941243753, 0.39080459706497506, 0.3924466331897698, 0.397372741662027, 0.3973727417599, 0.397372741662027, 0.3957307055372323, 0.3940886695103105, 0.3940886695103105, 0.39901477778682176, 0.39901477798256774, 0.3957307055372323, 0.39573070563510526, 0.3940886695103105, 0.39573070563510526, 0.39408866941243753, 0.39901477778682176, 0.39408866941243753, 0.3990147776889488, 0.4006568140094895, 0.4006568140094895, 0.397372741857773, 0.4006568140094895, 0.39901477798256774, 0.397372741857773, 0.39901477788469475, 0.4006568140094895, 0.4006568140094895, 0.4006568140094895, 0.39901477788469475, 0.39573070563510526, 0.39573070563510526, 0.39901477788469475, 0.397372741857773, 0.39573070573297825, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.3973727417599, 0.4006568141073625, 0.4006568141073625, 0.39901477798256774, 0.4006568141073625, 0.4006568141073625, 0.3924466335812617, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.3973727417599, 0.4006568141073625, 0.4022988502321572, 0.4006568141073625, 0.4006568141073625, 0.4022988502321572, 0.4006568141073625, 0.39901477798256774, 0.4006568141073625, 0.4022988502321572, 0.4006568141073625, 0.3973727417599, 0.4006568141073625, 0.4006568141073625, 0.4006568141073625, 0.4022988502321572, 0.39901477798256774, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4006568141073625, 0.4006568141073625, 0.4022988502321572, 0.4006568141073625, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4006568141073625, 0.4006568141073625, 0.4022988502321572, 0.4006568141073625, 0.39901477798256774, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.397372741955646, 0.4022988502321572, 0.4006568141073625, 0.4022988502321572, 0.4022988502321572, 0.39901477798256774, 0.39901477798256774, 0.4022988502321572, 0.39573070583085124, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.4022988502321572, 0.39901477798256774, 0.4022988502321572, 0.397372741955646, 0.4022988502321572, 0.4006568141073625, 0.4022988502321572, 0.4022988502321572], 'loss': [1.0884698666586279, 1.0779564114321918, 1.074920890953017, 1.0747284327689137, 1.0752722102758576, 1.07454854913316, 1.0744298101205845, 1.074365285485677, 1.0742231227534018, 1.0742098105271984, 1.0738973728685164, 1.0742384209525169, 1.0740540214143004, 1.0740855173408619, 1.074157923888377, 1.074048576462685, 1.07423555860774, 1.0740491841118438, 1.073900634699044, 1.0739003908952403, 1.0739123697887945, 1.0739925453060706, 1.0738947803234906, 1.0738824243663028, 1.073820962455483, 1.0738981032028825, 1.0737706065912267, 1.0737684759516002, 1.0737905764237077, 1.0736857151348733, 1.073867049648042, 1.0738466782246772, 1.0740158983324588, 1.0745962062655534, 1.0740277324124283, 1.0740913358801933, 1.0745373355779315, 1.0749413975944755, 1.0743221677549075, 1.0739063709423526, 1.073861494984715, 1.0740559281264976, 1.0739265280337793, 1.0738837173097677, 1.0737156070478153, 1.0736783641564527, 1.0736382978897565, 1.0736414715005143, 1.0741708103391423, 1.0743711834325929, 1.0743186690724116, 1.0744455025670954, 1.074018873277386, 1.073750065239548, 1.07384901619545, 1.073861401771373, 1.0737533930145984, 1.0735847684147422, 1.0737561233724167, 1.0739768236324772, 1.074114397419062, 1.0740713312885355, 1.0738947714623484, 1.073915250247509, 1.0737722594635197, 1.0739524670450105, 1.0738879496556777, 1.0739135782331901, 1.0736830188997961, 1.073681461835544, 1.0737331097130902, 1.0739227318421036, 1.0736199351551596, 1.0737185011654173, 1.0737483114677289, 1.073519335047665, 1.0739437978125697, 1.0735008351856679, 1.073740194710373, 1.0737905563025505, 1.0734827195839225, 1.0733843717242169, 1.073571231135108, 1.0733305390610588, 1.0736339153450372, 1.0734503651056937, 1.0735464037810998, 1.0737266536610817, 1.0732567682403313, 1.0733269643979395, 1.0732445035871785, 1.0730919084020218, 1.0731692553056094, 1.073081124245019, 1.0731813997213846, 1.0731875760354546, 1.0728886553883796, 1.072871484550852, 1.072845152271357, 1.0727069368107853, 1.0730929712495274, 1.0730257938529921, 1.0730676385656275, 1.0730172065249215, 1.0730708914860563, 1.0730807478667774, 1.0730705905499154, 1.0729582061513003, 1.073194573106707, 1.073007287577682, 1.072970217943681, 1.072943755831317, 1.072855503064651, 1.0728630681791835, 1.072948088734057, 1.073103334918404, 1.0728781242879755, 1.0733108987553652, 1.0729217710925814, 1.0730314555843752, 1.0729390374444103, 1.0729449398463757, 1.0730873857435506, 1.0725956020413971, 1.073092706786044, 1.0729344366022693, 1.0731515440852735, 1.0738671555900965, 1.0733606064833654, 1.072871382231585, 1.0723020525194047, 1.07274234099065, 1.0726313658073943, 1.0725330279348322, 1.0724332422691205, 1.07251922080649, 1.0727643828617228, 1.0725505558127495, 1.0722951706919581, 1.072298106471616, 1.0725423779086165, 1.0722370852190366, 1.072180545354526, 1.0725223726315665, 1.072074787670582, 1.072207158987527, 1.0722015772022506, 1.0723663223352766, 1.0724291679550735, 1.0722807442138327, 1.0722576258853231, 1.0721218984964203, 1.0721665199777184, 1.0718348604452927, 1.07212782694329, 1.0726042850306392, 1.0721234652051201, 1.0722503390889881, 1.0720641442148102, 1.0722491698098624, 1.0720741396322389, 1.072083477856442, 1.0723789353145465, 1.0721410906535154, 1.0721704571643649, 1.0719121553569848, 1.072025064276474, 1.0720503080062553, 1.071990291342843, 1.072065440732106, 1.0717197612080975, 1.0719788231644052, 1.0719632503677932, 1.072062299285826, 1.0718049800861054, 1.071922907839076, 1.0717916354506412, 1.0715761488223223, 1.071774335661463, 1.0720689527307938, 1.0719192507820208, 1.071766467358787, 1.0719248367775638, 1.0719279864485505, 1.0719942316137545, 1.0719015425480367, 1.0718223672627913, 1.0718945541421, 1.071300315318411, 1.0726898908125546, 1.0721522207867193, 1.071816670331622, 1.0721992771238762, 1.0716883299531879, 1.0715725935949683, 1.0718064776681042, 1.0715294076676731, 1.0714203144245813, 1.071419747849999, 1.0714144189744514, 1.071502177622284, 1.0713935077068009, 1.0713575482123687, 1.0711347251457355, 1.0710909365138968, 1.0712925392009884, 1.0710293704724165, 1.0712066906923141, 1.0710139352438142, 1.0709630050698344, 1.070940780786518, 1.0708638653373326, 1.0708893750482515, 1.0714147598591672, 1.070691701078317, 1.0712125157428718, 1.0709620821892114, 1.0707775374457578, 1.0707114159448925, 1.0707801726319706, 1.071436236085833, 1.07074222109401, 1.0711546268306473, 1.0707850074865979, 1.0709343401558344, 1.0705791426879914, 1.070894970247633, 1.070657687657178, 1.0707891705099808, 1.070707816558697, 1.0707081445188738, 1.0705193110804783, 1.0710249458250325, 1.0703130987880167, 1.0709736234598337, 1.0704372719328017, 1.071140755569176, 1.070582298870204, 1.0706122877171886, 1.0705613613128662, 1.0703758845828641, 1.0704062304702382, 1.070283643127222, 1.07059313462745, 1.0706628785241066, 1.0703279875142373, 1.0705037604612002, 1.071443323922598, 1.0701539306425216, 1.0706702860963417, 1.0701283556235155, 1.0715781020922337, 1.0703352522311513, 1.070744807568419, 1.070506270954986, 1.0701525336173525, 1.0703551064771304, 1.0704866378459108, 1.0700742168348183, 1.0708535492053022, 1.0700890730538652, 1.0702310692847876, 1.0698300180983495, 1.0707692668668054, 1.069880406567693, 1.0712991828056821, 1.070870971141165, 1.0705396980720379, 1.070143060713578, 1.0702745932573166, 1.0699801244774882, 1.0700496596232576, 1.0701915149571224, 1.0696654479361658, 1.0700260950065001, 1.0698840798783351, 1.0698187723296868, 1.0708250406586415, 1.0699456067055892, 1.0704158407461961, 1.070172012562135, 1.069706701498012, 1.0699807747188779, 1.0696502312497682, 1.0697901654781992, 1.0698325889066505, 1.0701342745238507, 1.070012710079765, 1.069937259901231, 1.069524030078363, 1.0697274971791606, 1.0694726144998226, 1.0700757757594208, 1.0701895432305777, 1.0698238000732672, 1.0698336354026559, 1.0698458486024358, 1.0696420045359178, 1.069508191006874, 1.06977243981567], 'acc': [0.3638603691937253, 0.3728952763996085, 0.38809034983480245, 0.394250514301676, 0.39425051508498143, 0.39425051113173704, 0.394250513714197, 0.3942505141058497, 0.3934291594571891, 0.39425051547663414, 0.3942505115233897, 0.3942505138733059, 0.3942505146933287, 0.3942505156724605, 0.39425051449750237, 0.3942505115233897, 0.39425051504826397, 0.3942505113275634, 0.39425051351837065, 0.3942505129308916, 0.3934291594571891, 0.39466119191974586, 0.3967145784067667, 0.3950718693419893, 0.3942505148891551, 0.394250514301676, 0.394250514301676, 0.39630389941791244, 0.395482545197622, 0.39096509237798577, 0.3958932245781289, 0.3987679653221577, 0.39753593563788725, 0.39096509237798577, 0.3958932243823026, 0.39630390219619877, 0.3946611903164176, 0.40041067818107057, 0.39671457781928765, 0.39917864689347193, 0.39917864352770654, 0.39794661306013074, 0.39753593309214474, 0.39794660969436535, 0.3979466103185618, 0.39548254539344835, 0.39671458118505304, 0.39589322199566895, 0.3963039020003724, 0.39425051171921605, 0.39425051508498143, 0.3942505152808078, 0.3942505126983478, 0.394250513714197, 0.3942505123434126, 0.39425051312671794, 0.3954825453567309, 0.39753593602953996, 0.3954825445734255, 0.3942505156724605, 0.39425051171921605, 0.39507186660042043, 0.3942505121475862, 0.39425051391002336, 0.39753593207629556, 0.396303900629588, 0.4012320345921683, 0.40000000033045696, 0.39794661403926246, 0.39425051211086876, 0.3942505146933287, 0.3942505142649586, 0.39383983707525894, 0.3983572902865478, 0.3979466116893463, 0.39671457961844225, 0.3979466102818444, 0.3971252550457048, 0.394250513714197, 0.3995893221616255, 0.3979466128643044, 0.3987679653221577, 0.3983572906782005, 0.39548254516090453, 0.39671457824765777, 0.3971252550457048, 0.39466119058567883, 0.3971252554373575, 0.39671457781928765, 0.395893225165608, 0.39999999837219347, 0.39835729048237417, 0.3983572916573323, 0.39507187090860013, 0.3979466138434361, 0.39589322496978163, 0.3991786461101665, 0.3995893215741465, 0.39876796590963676, 0.3954825467642328, 0.3987679678679002, 0.3995893205582973, 0.3991786431727713, 0.39753593344708, 0.39753593367962375, 0.3979466134517834, 0.3963038992220861, 0.39301848007668216, 0.3930184824265983, 0.39548254398594646, 0.39548254398594646, 0.3926078024218949, 0.39548254735171184, 0.39301847827752756, 0.39630390039704416, 0.390554414759916, 0.39383983527610433, 0.3950718693052719, 0.3983572878999142, 0.3958932216407337, 0.3942505128941742, 0.3958932243823026, 0.39753593563788725, 0.3926078014794806, 0.3967145804384651, 0.3942505129308916, 0.38767966943844634, 0.39507187032112107, 0.3926078048085285, 0.395893225165608, 0.39876796551798405, 0.39753593344708, 0.39753593465875553, 0.3971252584114701, 0.3971252544582257, 0.39383983550864815, 0.3958932227789744, 0.39630390023793527, 0.3934291578905783, 0.3954825467642328, 0.39425051547663414, 0.3963039020003724, 0.3954825448059693, 0.39917864313605383, 0.3950718677753785, 0.3967145778560051, 0.39753593563788725, 0.3995893201666446, 0.39548254735171184, 0.3950718687545103, 0.39219712425306347, 0.39589322575308705, 0.3967145774643524, 0.3921971251954778, 0.3946611913322668, 0.39466119191974586, 0.39466118992476495, 0.3946611925072249, 0.3946611889456332, 0.39507186715118203, 0.4000000011504798, 0.39466119035313507, 0.3930184794892031, 0.3963039010212407, 0.39507186953781565, 0.39383983707525894, 0.39671458138087934, 0.3942505125392389, 0.39301848223077196, 0.3934291564830764, 0.3963039000421089, 0.39219712699463233, 0.3954825440226639, 0.3950718707127738, 0.39548254437759917, 0.3967145791900721, 0.3946611907447878, 0.3958932255572607, 0.39589322199566895, 0.3979466122768253, 0.3946611887865243, 0.39548254793919085, 0.3930184818391193, 0.39342916004466816, 0.39425051195175986, 0.3983572912656796, 0.3995893207541237, 0.3934291558955974, 0.39014373514686523, 0.3958932259489134, 0.39548254496507823, 0.3979466142350888, 0.39835728848739327, 0.39917864591434016, 0.39425051171921605, 0.3893223804982046, 0.3934291558955974, 0.39260780285026503, 0.3995893215741465, 0.3979466102818444, 0.3934291573030993, 0.39260780183441585, 0.39466119231139857, 0.39712525524153114, 0.4000000011504798, 0.40041067739776515, 0.4012320306389239, 0.39753593407127646, 0.39548254735171184, 0.40164271201441176, 0.40082135619079307, 0.4008213555665966, 0.4008213539999858, 0.3979466112976936, 0.3971252570406857, 0.4008213569740985, 0.39917864708929823, 0.39712525860729647, 0.4008213565824458, 0.40082135380415945, 0.3995893247073681, 0.3983572886832196, 0.395482545197622, 0.4016427114269327, 0.40164271021525716, 0.3987679653221577, 0.39999999880056364, 0.3995893201666446, 0.3995893211824938, 0.3975359354053435, 0.404106777918657, 0.4008213559949667, 0.4008213545874649, 0.4008213547832912, 0.39917864532686115, 0.4004106775935915, 0.4004106785727233, 0.4008213532166804, 0.398767968687923, 0.40082135716992484, 0.3995893215741465, 0.3971252574323384, 0.4012320316180556, 0.400821355015835, 0.40041067638191596, 0.39917864591434016, 0.39876796947122845, 0.40123203420051556, 0.4000000001346306, 0.40082135736575114, 0.3995893245115417, 0.3995893221616255, 0.4012320330255575, 0.3979466102818444, 0.3991786433318802, 0.399178646734363, 0.39712525801981746, 0.40041067876854963, 0.40082135482000864, 0.39958932353241, 0.40041067818107057, 0.40041067618608966, 0.39917864493520844, 0.3991786427811186, 0.3991786425485748, 0.4008213532166804, 0.3987679677087913, 0.39958932353241, 0.40082135560331406, 0.39917864552268745, 0.3979466106734971, 0.39917864352770654, 0.3991786451310348, 0.3991786445435557, 0.399178644898491, 0.39917864313605383, 0.4000000003671744, 0.39876796892046684, 0.39917864431101197, 0.40000000134630614, 0.39917864591434016, 0.4000000015421325, 0.40164271005614827, 0.3999999985680198, 0.4024640646680914, 0.39876796610546306, 0.3999999985680198, 0.3991786447026646, 0.39835729106985324, 0.3991786466976456, 0.39917864708929823, 0.3991786429402275, 0.39917864630599287, 0.40041067818107057, 0.4004106770061125, 0.39958932431571537, 0.4012320316180556, 0.3999999987638462, 0.39917864391935926, 0.3991786425485748]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
