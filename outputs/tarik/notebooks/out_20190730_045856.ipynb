{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf1.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 04:58:56 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '0', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['my', 'ck', 'mb', 'eb', 'sg', 'ib', 'yd', 'ds', 'eo', 'sk', 'eg', 'ek', 'aa', 'ce', 'by'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001A50FF4D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001A50A736EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7025, Accuracy:0.0448, Validation Loss:2.6951, Validation Accuracy:0.0443\n",
    "Epoch #2: Loss:2.6920, Accuracy:0.0460, Validation Loss:2.6860, Validation Accuracy:0.0411\n",
    "Epoch #3: Loss:2.6843, Accuracy:0.0661, Validation Loss:2.6784, Validation Accuracy:0.1067\n",
    "Epoch #4: Loss:2.6780, Accuracy:0.1072, Validation Loss:2.6712, Validation Accuracy:0.1609\n",
    "Epoch #5: Loss:2.6708, Accuracy:0.1470, Validation Loss:2.6635, Validation Accuracy:0.1642\n",
    "Epoch #6: Loss:2.6640, Accuracy:0.1470, Validation Loss:2.6555, Validation Accuracy:0.1576\n",
    "Epoch #7: Loss:2.6561, Accuracy:0.1450, Validation Loss:2.6466, Validation Accuracy:0.1527\n",
    "Epoch #8: Loss:2.6471, Accuracy:0.1499, Validation Loss:2.6345, Validation Accuracy:0.1576\n",
    "Epoch #9: Loss:2.6352, Accuracy:0.1400, Validation Loss:2.6193, Validation Accuracy:0.1461\n",
    "Epoch #10: Loss:2.6208, Accuracy:0.1355, Validation Loss:2.6012, Validation Accuracy:0.1494\n",
    "Epoch #11: Loss:2.6039, Accuracy:0.1372, Validation Loss:2.5812, Validation Accuracy:0.1511\n",
    "Epoch #12: Loss:2.5851, Accuracy:0.1388, Validation Loss:2.5610, Validation Accuracy:0.1478\n",
    "Epoch #13: Loss:2.5658, Accuracy:0.1372, Validation Loss:2.5411, Validation Accuracy:0.1396\n",
    "Epoch #14: Loss:2.5507, Accuracy:0.1458, Validation Loss:2.5304, Validation Accuracy:0.1626\n",
    "Epoch #15: Loss:2.5342, Accuracy:0.1573, Validation Loss:2.5123, Validation Accuracy:0.1675\n",
    "Epoch #16: Loss:2.5242, Accuracy:0.1552, Validation Loss:2.5026, Validation Accuracy:0.1691\n",
    "Epoch #17: Loss:2.5166, Accuracy:0.1602, Validation Loss:2.4966, Validation Accuracy:0.1806\n",
    "Epoch #18: Loss:2.5098, Accuracy:0.1561, Validation Loss:2.4874, Validation Accuracy:0.1741\n",
    "Epoch #19: Loss:2.5052, Accuracy:0.1639, Validation Loss:2.4867, Validation Accuracy:0.1905\n",
    "Epoch #20: Loss:2.4983, Accuracy:0.1569, Validation Loss:2.4771, Validation Accuracy:0.1773\n",
    "Epoch #21: Loss:2.4931, Accuracy:0.1643, Validation Loss:2.4733, Validation Accuracy:0.1970\n",
    "Epoch #22: Loss:2.4878, Accuracy:0.1643, Validation Loss:2.4689, Validation Accuracy:0.1872\n",
    "Epoch #23: Loss:2.4826, Accuracy:0.1676, Validation Loss:2.4723, Validation Accuracy:0.2003\n",
    "Epoch #24: Loss:2.4863, Accuracy:0.1626, Validation Loss:2.4629, Validation Accuracy:0.1856\n",
    "Epoch #25: Loss:2.4754, Accuracy:0.1704, Validation Loss:2.4640, Validation Accuracy:0.1954\n",
    "Epoch #26: Loss:2.4749, Accuracy:0.1639, Validation Loss:2.4577, Validation Accuracy:0.1954\n",
    "Epoch #27: Loss:2.4709, Accuracy:0.1700, Validation Loss:2.4557, Validation Accuracy:0.2020\n",
    "Epoch #28: Loss:2.4677, Accuracy:0.1659, Validation Loss:2.4521, Validation Accuracy:0.1970\n",
    "Epoch #29: Loss:2.4654, Accuracy:0.1754, Validation Loss:2.4529, Validation Accuracy:0.2118\n",
    "Epoch #30: Loss:2.4632, Accuracy:0.1692, Validation Loss:2.4467, Validation Accuracy:0.1987\n",
    "Epoch #31: Loss:2.4605, Accuracy:0.1749, Validation Loss:2.4464, Validation Accuracy:0.2036\n",
    "Epoch #32: Loss:2.4584, Accuracy:0.1725, Validation Loss:2.4446, Validation Accuracy:0.2053\n",
    "Epoch #33: Loss:2.4560, Accuracy:0.1749, Validation Loss:2.4428, Validation Accuracy:0.2036\n",
    "Epoch #34: Loss:2.4535, Accuracy:0.1741, Validation Loss:2.4417, Validation Accuracy:0.2020\n",
    "Epoch #35: Loss:2.4522, Accuracy:0.1721, Validation Loss:2.4406, Validation Accuracy:0.2036\n",
    "Epoch #36: Loss:2.4505, Accuracy:0.1758, Validation Loss:2.4397, Validation Accuracy:0.2085\n",
    "Epoch #37: Loss:2.4496, Accuracy:0.1770, Validation Loss:2.4385, Validation Accuracy:0.2003\n",
    "Epoch #38: Loss:2.4488, Accuracy:0.1754, Validation Loss:2.4433, Validation Accuracy:0.2003\n",
    "Epoch #39: Loss:2.4490, Accuracy:0.1749, Validation Loss:2.4397, Validation Accuracy:0.1954\n",
    "Epoch #40: Loss:2.4497, Accuracy:0.1774, Validation Loss:2.4389, Validation Accuracy:0.2118\n",
    "Epoch #41: Loss:2.4465, Accuracy:0.1762, Validation Loss:2.4365, Validation Accuracy:0.2053\n",
    "Epoch #42: Loss:2.4443, Accuracy:0.1745, Validation Loss:2.4362, Validation Accuracy:0.2135\n",
    "Epoch #43: Loss:2.4429, Accuracy:0.1758, Validation Loss:2.4357, Validation Accuracy:0.2069\n",
    "Epoch #44: Loss:2.4435, Accuracy:0.1680, Validation Loss:2.4426, Validation Accuracy:0.2085\n",
    "Epoch #45: Loss:2.4453, Accuracy:0.1741, Validation Loss:2.4395, Validation Accuracy:0.1970\n",
    "Epoch #46: Loss:2.4447, Accuracy:0.1766, Validation Loss:2.4412, Validation Accuracy:0.2069\n",
    "Epoch #47: Loss:2.4454, Accuracy:0.1758, Validation Loss:2.4362, Validation Accuracy:0.2036\n",
    "Epoch #48: Loss:2.4420, Accuracy:0.1774, Validation Loss:2.4342, Validation Accuracy:0.2102\n",
    "Epoch #49: Loss:2.4439, Accuracy:0.1828, Validation Loss:2.4335, Validation Accuracy:0.2085\n",
    "Epoch #50: Loss:2.4412, Accuracy:0.1762, Validation Loss:2.4326, Validation Accuracy:0.2102\n",
    "Epoch #51: Loss:2.4392, Accuracy:0.1758, Validation Loss:2.4338, Validation Accuracy:0.2118\n",
    "Epoch #52: Loss:2.4373, Accuracy:0.1770, Validation Loss:2.4327, Validation Accuracy:0.2118\n",
    "Epoch #53: Loss:2.4380, Accuracy:0.1758, Validation Loss:2.4372, Validation Accuracy:0.2053\n",
    "Epoch #54: Loss:2.4424, Accuracy:0.1778, Validation Loss:2.4329, Validation Accuracy:0.2069\n",
    "Epoch #55: Loss:2.4397, Accuracy:0.1704, Validation Loss:2.4322, Validation Accuracy:0.2102\n",
    "Epoch #56: Loss:2.4389, Accuracy:0.1774, Validation Loss:2.4307, Validation Accuracy:0.2135\n",
    "Epoch #57: Loss:2.4368, Accuracy:0.1770, Validation Loss:2.4307, Validation Accuracy:0.2135\n",
    "Epoch #58: Loss:2.4363, Accuracy:0.1766, Validation Loss:2.4315, Validation Accuracy:0.2167\n",
    "Epoch #59: Loss:2.4357, Accuracy:0.1758, Validation Loss:2.4303, Validation Accuracy:0.2085\n",
    "Epoch #60: Loss:2.4352, Accuracy:0.1774, Validation Loss:2.4310, Validation Accuracy:0.2085\n",
    "Epoch #61: Loss:2.4354, Accuracy:0.1786, Validation Loss:2.4305, Validation Accuracy:0.2085\n",
    "Epoch #62: Loss:2.4353, Accuracy:0.1770, Validation Loss:2.4314, Validation Accuracy:0.2135\n",
    "Epoch #63: Loss:2.4309, Accuracy:0.1770, Validation Loss:2.4304, Validation Accuracy:0.2036\n",
    "Epoch #64: Loss:2.4326, Accuracy:0.1815, Validation Loss:2.4332, Validation Accuracy:0.2135\n",
    "Epoch #65: Loss:2.4297, Accuracy:0.1819, Validation Loss:2.4303, Validation Accuracy:0.2053\n",
    "Epoch #66: Loss:2.4291, Accuracy:0.1815, Validation Loss:2.4310, Validation Accuracy:0.2151\n",
    "Epoch #67: Loss:2.4296, Accuracy:0.1811, Validation Loss:2.4302, Validation Accuracy:0.2102\n",
    "Epoch #68: Loss:2.4284, Accuracy:0.1795, Validation Loss:2.4307, Validation Accuracy:0.2118\n",
    "Epoch #69: Loss:2.4271, Accuracy:0.1811, Validation Loss:2.4292, Validation Accuracy:0.2102\n",
    "Epoch #70: Loss:2.4288, Accuracy:0.1778, Validation Loss:2.4302, Validation Accuracy:0.2151\n",
    "Epoch #71: Loss:2.4286, Accuracy:0.1786, Validation Loss:2.4304, Validation Accuracy:0.2069\n",
    "Epoch #72: Loss:2.4279, Accuracy:0.1758, Validation Loss:2.4303, Validation Accuracy:0.2069\n",
    "Epoch #73: Loss:2.4232, Accuracy:0.1778, Validation Loss:2.4307, Validation Accuracy:0.2118\n",
    "Epoch #74: Loss:2.4241, Accuracy:0.1774, Validation Loss:2.4295, Validation Accuracy:0.2053\n",
    "Epoch #75: Loss:2.4226, Accuracy:0.1815, Validation Loss:2.4305, Validation Accuracy:0.2102\n",
    "Epoch #76: Loss:2.4229, Accuracy:0.1815, Validation Loss:2.4325, Validation Accuracy:0.2085\n",
    "Epoch #77: Loss:2.4220, Accuracy:0.1811, Validation Loss:2.4303, Validation Accuracy:0.2069\n",
    "Epoch #78: Loss:2.4211, Accuracy:0.1774, Validation Loss:2.4305, Validation Accuracy:0.2053\n",
    "Epoch #79: Loss:2.4205, Accuracy:0.1815, Validation Loss:2.4325, Validation Accuracy:0.2069\n",
    "Epoch #80: Loss:2.4217, Accuracy:0.1807, Validation Loss:2.4314, Validation Accuracy:0.2069\n",
    "Epoch #81: Loss:2.4198, Accuracy:0.1815, Validation Loss:2.4323, Validation Accuracy:0.2118\n",
    "Epoch #82: Loss:2.4193, Accuracy:0.1786, Validation Loss:2.4318, Validation Accuracy:0.2069\n",
    "Epoch #83: Loss:2.4186, Accuracy:0.1836, Validation Loss:2.4316, Validation Accuracy:0.2102\n",
    "Epoch #84: Loss:2.4177, Accuracy:0.1828, Validation Loss:2.4316, Validation Accuracy:0.2102\n",
    "Epoch #85: Loss:2.4166, Accuracy:0.1815, Validation Loss:2.4314, Validation Accuracy:0.2053\n",
    "Epoch #86: Loss:2.4156, Accuracy:0.1832, Validation Loss:2.4333, Validation Accuracy:0.2135\n",
    "Epoch #87: Loss:2.4152, Accuracy:0.1803, Validation Loss:2.4313, Validation Accuracy:0.2085\n",
    "Epoch #88: Loss:2.4146, Accuracy:0.1803, Validation Loss:2.4352, Validation Accuracy:0.2167\n",
    "Epoch #89: Loss:2.4148, Accuracy:0.1807, Validation Loss:2.4341, Validation Accuracy:0.2053\n",
    "Epoch #90: Loss:2.4133, Accuracy:0.1795, Validation Loss:2.4340, Validation Accuracy:0.2118\n",
    "Epoch #91: Loss:2.4132, Accuracy:0.1815, Validation Loss:2.4329, Validation Accuracy:0.2020\n",
    "Epoch #92: Loss:2.4126, Accuracy:0.1803, Validation Loss:2.4342, Validation Accuracy:0.2102\n",
    "Epoch #93: Loss:2.4120, Accuracy:0.1811, Validation Loss:2.4357, Validation Accuracy:0.2085\n",
    "Epoch #94: Loss:2.4101, Accuracy:0.1848, Validation Loss:2.4322, Validation Accuracy:0.2085\n",
    "Epoch #95: Loss:2.4094, Accuracy:0.1844, Validation Loss:2.4340, Validation Accuracy:0.2151\n",
    "Epoch #96: Loss:2.4113, Accuracy:0.1819, Validation Loss:2.4325, Validation Accuracy:0.2135\n",
    "Epoch #97: Loss:2.4084, Accuracy:0.1836, Validation Loss:2.4320, Validation Accuracy:0.2085\n",
    "Epoch #98: Loss:2.4076, Accuracy:0.1828, Validation Loss:2.4335, Validation Accuracy:0.2069\n",
    "Epoch #99: Loss:2.4060, Accuracy:0.1852, Validation Loss:2.4372, Validation Accuracy:0.2118\n",
    "Epoch #100: Loss:2.4053, Accuracy:0.1823, Validation Loss:2.4362, Validation Accuracy:0.2069\n",
    "Epoch #101: Loss:2.4038, Accuracy:0.1852, Validation Loss:2.4439, Validation Accuracy:0.1987\n",
    "Epoch #102: Loss:2.4109, Accuracy:0.1864, Validation Loss:2.4396, Validation Accuracy:0.2102\n",
    "Epoch #103: Loss:2.4090, Accuracy:0.1840, Validation Loss:2.4384, Validation Accuracy:0.2003\n",
    "Epoch #104: Loss:2.4089, Accuracy:0.1823, Validation Loss:2.4368, Validation Accuracy:0.2102\n",
    "Epoch #105: Loss:2.4068, Accuracy:0.1860, Validation Loss:2.4367, Validation Accuracy:0.2036\n",
    "Epoch #106: Loss:2.4037, Accuracy:0.1864, Validation Loss:2.4359, Validation Accuracy:0.2085\n",
    "Epoch #107: Loss:2.4012, Accuracy:0.1860, Validation Loss:2.4385, Validation Accuracy:0.2036\n",
    "Epoch #108: Loss:2.4029, Accuracy:0.1848, Validation Loss:2.4377, Validation Accuracy:0.2069\n",
    "Epoch #109: Loss:2.3990, Accuracy:0.1934, Validation Loss:2.4430, Validation Accuracy:0.2069\n",
    "Epoch #110: Loss:2.3993, Accuracy:0.1893, Validation Loss:2.4419, Validation Accuracy:0.2085\n",
    "Epoch #111: Loss:2.3970, Accuracy:0.1877, Validation Loss:2.4389, Validation Accuracy:0.2020\n",
    "Epoch #112: Loss:2.3958, Accuracy:0.1914, Validation Loss:2.4421, Validation Accuracy:0.2020\n",
    "Epoch #113: Loss:2.3965, Accuracy:0.1906, Validation Loss:2.4426, Validation Accuracy:0.2085\n",
    "Epoch #114: Loss:2.3949, Accuracy:0.1943, Validation Loss:2.4409, Validation Accuracy:0.2003\n",
    "Epoch #115: Loss:2.3933, Accuracy:0.1930, Validation Loss:2.4442, Validation Accuracy:0.1970\n",
    "Epoch #116: Loss:2.3916, Accuracy:0.1951, Validation Loss:2.4445, Validation Accuracy:0.2053\n",
    "Epoch #117: Loss:2.3970, Accuracy:0.1823, Validation Loss:2.4576, Validation Accuracy:0.1921\n",
    "Epoch #118: Loss:2.3980, Accuracy:0.1906, Validation Loss:2.4416, Validation Accuracy:0.2069\n",
    "Epoch #119: Loss:2.3901, Accuracy:0.1938, Validation Loss:2.4421, Validation Accuracy:0.2020\n",
    "Epoch #120: Loss:2.3884, Accuracy:0.1963, Validation Loss:2.4523, Validation Accuracy:0.2053\n",
    "Epoch #121: Loss:2.3895, Accuracy:0.1897, Validation Loss:2.4449, Validation Accuracy:0.2020\n",
    "Epoch #122: Loss:2.3867, Accuracy:0.1947, Validation Loss:2.4463, Validation Accuracy:0.1987\n",
    "Epoch #123: Loss:2.3838, Accuracy:0.1947, Validation Loss:2.4488, Validation Accuracy:0.2069\n",
    "Epoch #124: Loss:2.3816, Accuracy:0.1930, Validation Loss:2.4450, Validation Accuracy:0.2036\n",
    "Epoch #125: Loss:2.3822, Accuracy:0.1901, Validation Loss:2.4480, Validation Accuracy:0.2020\n",
    "Epoch #126: Loss:2.3847, Accuracy:0.1897, Validation Loss:2.4604, Validation Accuracy:0.2020\n",
    "Epoch #127: Loss:2.3879, Accuracy:0.1938, Validation Loss:2.4491, Validation Accuracy:0.1970\n",
    "Epoch #128: Loss:2.3808, Accuracy:0.1975, Validation Loss:2.4564, Validation Accuracy:0.2036\n",
    "Epoch #129: Loss:2.3834, Accuracy:0.1963, Validation Loss:2.4594, Validation Accuracy:0.2020\n",
    "Epoch #130: Loss:2.3836, Accuracy:0.1938, Validation Loss:2.4482, Validation Accuracy:0.1970\n",
    "Epoch #131: Loss:2.3755, Accuracy:0.1963, Validation Loss:2.4528, Validation Accuracy:0.2085\n",
    "Epoch #132: Loss:2.3742, Accuracy:0.1975, Validation Loss:2.4623, Validation Accuracy:0.1970\n",
    "Epoch #133: Loss:2.3741, Accuracy:0.1943, Validation Loss:2.4543, Validation Accuracy:0.2003\n",
    "Epoch #134: Loss:2.3734, Accuracy:0.1992, Validation Loss:2.4534, Validation Accuracy:0.1970\n",
    "Epoch #135: Loss:2.3708, Accuracy:0.1918, Validation Loss:2.4610, Validation Accuracy:0.2020\n",
    "Epoch #136: Loss:2.3666, Accuracy:0.1971, Validation Loss:2.4562, Validation Accuracy:0.1954\n",
    "Epoch #137: Loss:2.3677, Accuracy:0.2000, Validation Loss:2.4564, Validation Accuracy:0.1938\n",
    "Epoch #138: Loss:2.3651, Accuracy:0.2012, Validation Loss:2.4610, Validation Accuracy:0.1954\n",
    "Epoch #139: Loss:2.3643, Accuracy:0.1996, Validation Loss:2.4627, Validation Accuracy:0.2003\n",
    "Epoch #140: Loss:2.3623, Accuracy:0.2016, Validation Loss:2.4606, Validation Accuracy:0.2003\n",
    "Epoch #141: Loss:2.3624, Accuracy:0.1979, Validation Loss:2.4728, Validation Accuracy:0.1938\n",
    "Epoch #142: Loss:2.3644, Accuracy:0.2029, Validation Loss:2.4629, Validation Accuracy:0.1954\n",
    "Epoch #143: Loss:2.3581, Accuracy:0.2008, Validation Loss:2.4664, Validation Accuracy:0.1905\n",
    "Epoch #144: Loss:2.3576, Accuracy:0.2016, Validation Loss:2.4689, Validation Accuracy:0.1970\n",
    "Epoch #145: Loss:2.3547, Accuracy:0.2053, Validation Loss:2.4699, Validation Accuracy:0.1954\n",
    "Epoch #146: Loss:2.3532, Accuracy:0.2021, Validation Loss:2.4695, Validation Accuracy:0.1987\n",
    "Epoch #147: Loss:2.3510, Accuracy:0.2057, Validation Loss:2.4697, Validation Accuracy:0.1970\n",
    "Epoch #148: Loss:2.3505, Accuracy:0.2033, Validation Loss:2.4722, Validation Accuracy:0.1872\n",
    "Epoch #149: Loss:2.3477, Accuracy:0.2086, Validation Loss:2.4745, Validation Accuracy:0.1938\n",
    "Epoch #150: Loss:2.3459, Accuracy:0.1992, Validation Loss:2.4735, Validation Accuracy:0.1921\n",
    "Epoch #151: Loss:2.3440, Accuracy:0.2037, Validation Loss:2.4754, Validation Accuracy:0.1856\n",
    "Epoch #152: Loss:2.3429, Accuracy:0.2041, Validation Loss:2.4791, Validation Accuracy:0.1954\n",
    "Epoch #153: Loss:2.3502, Accuracy:0.2062, Validation Loss:2.4772, Validation Accuracy:0.1954\n",
    "Epoch #154: Loss:2.3402, Accuracy:0.2111, Validation Loss:2.4787, Validation Accuracy:0.1872\n",
    "Epoch #155: Loss:2.3382, Accuracy:0.2070, Validation Loss:2.4938, Validation Accuracy:0.1888\n",
    "Epoch #156: Loss:2.3356, Accuracy:0.2078, Validation Loss:2.4863, Validation Accuracy:0.1905\n",
    "Epoch #157: Loss:2.3482, Accuracy:0.2066, Validation Loss:2.4768, Validation Accuracy:0.1938\n",
    "Epoch #158: Loss:2.3372, Accuracy:0.2127, Validation Loss:2.4874, Validation Accuracy:0.1888\n",
    "Epoch #159: Loss:2.3332, Accuracy:0.2103, Validation Loss:2.4906, Validation Accuracy:0.1970\n",
    "Epoch #160: Loss:2.3354, Accuracy:0.2090, Validation Loss:2.4876, Validation Accuracy:0.1872\n",
    "Epoch #161: Loss:2.3261, Accuracy:0.2144, Validation Loss:2.4900, Validation Accuracy:0.1856\n",
    "Epoch #162: Loss:2.3263, Accuracy:0.2164, Validation Loss:2.4833, Validation Accuracy:0.1905\n",
    "Epoch #163: Loss:2.3244, Accuracy:0.2131, Validation Loss:2.4909, Validation Accuracy:0.1938\n",
    "Epoch #164: Loss:2.3323, Accuracy:0.2082, Validation Loss:2.4906, Validation Accuracy:0.1888\n",
    "Epoch #165: Loss:2.3345, Accuracy:0.2053, Validation Loss:2.4864, Validation Accuracy:0.1888\n",
    "Epoch #166: Loss:2.3255, Accuracy:0.2111, Validation Loss:2.4902, Validation Accuracy:0.1888\n",
    "Epoch #167: Loss:2.3148, Accuracy:0.2160, Validation Loss:2.4979, Validation Accuracy:0.1856\n",
    "Epoch #168: Loss:2.3160, Accuracy:0.2136, Validation Loss:2.4975, Validation Accuracy:0.1856\n",
    "Epoch #169: Loss:2.3136, Accuracy:0.2168, Validation Loss:2.4947, Validation Accuracy:0.1905\n",
    "Epoch #170: Loss:2.3169, Accuracy:0.2131, Validation Loss:2.5029, Validation Accuracy:0.1938\n",
    "Epoch #171: Loss:2.3279, Accuracy:0.2094, Validation Loss:2.4969, Validation Accuracy:0.1773\n",
    "Epoch #172: Loss:2.3187, Accuracy:0.2246, Validation Loss:2.5040, Validation Accuracy:0.1888\n",
    "Epoch #173: Loss:2.3180, Accuracy:0.2193, Validation Loss:2.4993, Validation Accuracy:0.1905\n",
    "Epoch #174: Loss:2.3081, Accuracy:0.2177, Validation Loss:2.5022, Validation Accuracy:0.1806\n",
    "Epoch #175: Loss:2.3061, Accuracy:0.2177, Validation Loss:2.5011, Validation Accuracy:0.1921\n",
    "Epoch #176: Loss:2.3079, Accuracy:0.2148, Validation Loss:2.5067, Validation Accuracy:0.1856\n",
    "Epoch #177: Loss:2.3189, Accuracy:0.2136, Validation Loss:2.5087, Validation Accuracy:0.1823\n",
    "Epoch #178: Loss:2.3318, Accuracy:0.2053, Validation Loss:2.5025, Validation Accuracy:0.1938\n",
    "Epoch #179: Loss:2.3188, Accuracy:0.2086, Validation Loss:2.5194, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.3171, Accuracy:0.2189, Validation Loss:2.5085, Validation Accuracy:0.1856\n",
    "Epoch #181: Loss:2.3033, Accuracy:0.2193, Validation Loss:2.5068, Validation Accuracy:0.1757\n",
    "Epoch #182: Loss:2.3036, Accuracy:0.2181, Validation Loss:2.5085, Validation Accuracy:0.1839\n",
    "Epoch #183: Loss:2.2924, Accuracy:0.2242, Validation Loss:2.4962, Validation Accuracy:0.1970\n",
    "Epoch #184: Loss:2.2974, Accuracy:0.2214, Validation Loss:2.5049, Validation Accuracy:0.1872\n",
    "Epoch #185: Loss:2.3081, Accuracy:0.2185, Validation Loss:2.5200, Validation Accuracy:0.1823\n",
    "Epoch #186: Loss:2.3090, Accuracy:0.2185, Validation Loss:2.5360, Validation Accuracy:0.1724\n",
    "Epoch #187: Loss:2.3138, Accuracy:0.2123, Validation Loss:2.5092, Validation Accuracy:0.1888\n",
    "Epoch #188: Loss:2.2953, Accuracy:0.2185, Validation Loss:2.5101, Validation Accuracy:0.1856\n",
    "Epoch #189: Loss:2.2853, Accuracy:0.2292, Validation Loss:2.5173, Validation Accuracy:0.1839\n",
    "Epoch #190: Loss:2.2893, Accuracy:0.2329, Validation Loss:2.5078, Validation Accuracy:0.1872\n",
    "Epoch #191: Loss:2.2779, Accuracy:0.2275, Validation Loss:2.5209, Validation Accuracy:0.1872\n",
    "Epoch #192: Loss:2.2760, Accuracy:0.2320, Validation Loss:2.5187, Validation Accuracy:0.1839\n",
    "Epoch #193: Loss:2.2728, Accuracy:0.2349, Validation Loss:2.5203, Validation Accuracy:0.1872\n",
    "Epoch #194: Loss:2.2749, Accuracy:0.2308, Validation Loss:2.5229, Validation Accuracy:0.1938\n",
    "Epoch #195: Loss:2.2740, Accuracy:0.2300, Validation Loss:2.5297, Validation Accuracy:0.1823\n",
    "Epoch #196: Loss:2.2689, Accuracy:0.2349, Validation Loss:2.5235, Validation Accuracy:0.1856\n",
    "Epoch #197: Loss:2.2680, Accuracy:0.2390, Validation Loss:2.5323, Validation Accuracy:0.1823\n",
    "Epoch #198: Loss:2.2677, Accuracy:0.2357, Validation Loss:2.5393, Validation Accuracy:0.1872\n",
    "Epoch #199: Loss:2.2687, Accuracy:0.2296, Validation Loss:2.5365, Validation Accuracy:0.1708\n",
    "Epoch #200: Loss:2.2611, Accuracy:0.2460, Validation Loss:2.5252, Validation Accuracy:0.1921\n",
    "Epoch #201: Loss:2.2583, Accuracy:0.2345, Validation Loss:2.5327, Validation Accuracy:0.1773\n",
    "Epoch #202: Loss:2.2558, Accuracy:0.2444, Validation Loss:2.5364, Validation Accuracy:0.1823\n",
    "Epoch #203: Loss:2.2612, Accuracy:0.2337, Validation Loss:2.5435, Validation Accuracy:0.1691\n",
    "Epoch #204: Loss:2.2568, Accuracy:0.2456, Validation Loss:2.5291, Validation Accuracy:0.1905\n",
    "Epoch #205: Loss:2.2530, Accuracy:0.2435, Validation Loss:2.5384, Validation Accuracy:0.1741\n",
    "Epoch #206: Loss:2.2493, Accuracy:0.2505, Validation Loss:2.5449, Validation Accuracy:0.1839\n",
    "Epoch #207: Loss:2.2500, Accuracy:0.2456, Validation Loss:2.5409, Validation Accuracy:0.1872\n",
    "Epoch #208: Loss:2.2525, Accuracy:0.2386, Validation Loss:2.5433, Validation Accuracy:0.1839\n",
    "Epoch #209: Loss:2.2448, Accuracy:0.2456, Validation Loss:2.5473, Validation Accuracy:0.1888\n",
    "Epoch #210: Loss:2.2448, Accuracy:0.2468, Validation Loss:2.5553, Validation Accuracy:0.1757\n",
    "Epoch #211: Loss:2.2395, Accuracy:0.2456, Validation Loss:2.5422, Validation Accuracy:0.1757\n",
    "Epoch #212: Loss:2.2402, Accuracy:0.2501, Validation Loss:2.5462, Validation Accuracy:0.1708\n",
    "Epoch #213: Loss:2.2425, Accuracy:0.2390, Validation Loss:2.5551, Validation Accuracy:0.1790\n",
    "Epoch #214: Loss:2.2384, Accuracy:0.2468, Validation Loss:2.5591, Validation Accuracy:0.1856\n",
    "Epoch #215: Loss:2.2331, Accuracy:0.2505, Validation Loss:2.5555, Validation Accuracy:0.1872\n",
    "Epoch #216: Loss:2.2359, Accuracy:0.2505, Validation Loss:2.5550, Validation Accuracy:0.1823\n",
    "Epoch #217: Loss:2.2371, Accuracy:0.2452, Validation Loss:2.5613, Validation Accuracy:0.1724\n",
    "Epoch #218: Loss:2.2267, Accuracy:0.2526, Validation Loss:2.5576, Validation Accuracy:0.1856\n",
    "Epoch #219: Loss:2.2275, Accuracy:0.2509, Validation Loss:2.5673, Validation Accuracy:0.1658\n",
    "Epoch #220: Loss:2.2250, Accuracy:0.2546, Validation Loss:2.5558, Validation Accuracy:0.1724\n",
    "Epoch #221: Loss:2.2211, Accuracy:0.2571, Validation Loss:2.5628, Validation Accuracy:0.1856\n",
    "Epoch #222: Loss:2.2267, Accuracy:0.2501, Validation Loss:2.5594, Validation Accuracy:0.1773\n",
    "Epoch #223: Loss:2.2131, Accuracy:0.2579, Validation Loss:2.5627, Validation Accuracy:0.1921\n",
    "Epoch #224: Loss:2.2142, Accuracy:0.2550, Validation Loss:2.5626, Validation Accuracy:0.1741\n",
    "Epoch #225: Loss:2.2177, Accuracy:0.2546, Validation Loss:2.5685, Validation Accuracy:0.1691\n",
    "Epoch #226: Loss:2.2190, Accuracy:0.2530, Validation Loss:2.5820, Validation Accuracy:0.1773\n",
    "Epoch #227: Loss:2.2202, Accuracy:0.2485, Validation Loss:2.5724, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.2093, Accuracy:0.2657, Validation Loss:2.5802, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.2065, Accuracy:0.2575, Validation Loss:2.5749, Validation Accuracy:0.1675\n",
    "Epoch #230: Loss:2.2070, Accuracy:0.2554, Validation Loss:2.5693, Validation Accuracy:0.1675\n",
    "Epoch #231: Loss:2.2004, Accuracy:0.2604, Validation Loss:2.5805, Validation Accuracy:0.1741\n",
    "Epoch #232: Loss:2.1973, Accuracy:0.2587, Validation Loss:2.5797, Validation Accuracy:0.1741\n",
    "Epoch #233: Loss:2.2055, Accuracy:0.2649, Validation Loss:2.5826, Validation Accuracy:0.1609\n",
    "Epoch #234: Loss:2.1898, Accuracy:0.2669, Validation Loss:2.5766, Validation Accuracy:0.1658\n",
    "Epoch #235: Loss:2.1926, Accuracy:0.2628, Validation Loss:2.5925, Validation Accuracy:0.1675\n",
    "Epoch #236: Loss:2.1909, Accuracy:0.2628, Validation Loss:2.5826, Validation Accuracy:0.1593\n",
    "Epoch #237: Loss:2.1921, Accuracy:0.2616, Validation Loss:2.5871, Validation Accuracy:0.1626\n",
    "Epoch #238: Loss:2.1900, Accuracy:0.2628, Validation Loss:2.5999, Validation Accuracy:0.1609\n",
    "Epoch #239: Loss:2.1998, Accuracy:0.2591, Validation Loss:2.5922, Validation Accuracy:0.1675\n",
    "Epoch #240: Loss:2.2077, Accuracy:0.2534, Validation Loss:2.5955, Validation Accuracy:0.1544\n",
    "Epoch #241: Loss:2.2001, Accuracy:0.2567, Validation Loss:2.5858, Validation Accuracy:0.1741\n",
    "Epoch #242: Loss:2.1875, Accuracy:0.2612, Validation Loss:2.5967, Validation Accuracy:0.1708\n",
    "Epoch #243: Loss:2.1793, Accuracy:0.2698, Validation Loss:2.5970, Validation Accuracy:0.1494\n",
    "Epoch #244: Loss:2.1753, Accuracy:0.2706, Validation Loss:2.6091, Validation Accuracy:0.1691\n",
    "Epoch #245: Loss:2.1763, Accuracy:0.2661, Validation Loss:2.6023, Validation Accuracy:0.1576\n",
    "Epoch #246: Loss:2.1757, Accuracy:0.2678, Validation Loss:2.5977, Validation Accuracy:0.1609\n",
    "Epoch #247: Loss:2.1701, Accuracy:0.2719, Validation Loss:2.6055, Validation Accuracy:0.1658\n",
    "Epoch #248: Loss:2.1740, Accuracy:0.2612, Validation Loss:2.6014, Validation Accuracy:0.1708\n",
    "Epoch #249: Loss:2.1657, Accuracy:0.2702, Validation Loss:2.6047, Validation Accuracy:0.1626\n",
    "Epoch #250: Loss:2.1561, Accuracy:0.2780, Validation Loss:2.6073, Validation Accuracy:0.1560\n",
    "Epoch #251: Loss:2.1634, Accuracy:0.2698, Validation Loss:2.6173, Validation Accuracy:0.1576\n",
    "Epoch #252: Loss:2.1594, Accuracy:0.2743, Validation Loss:2.6161, Validation Accuracy:0.1708\n",
    "Epoch #253: Loss:2.1565, Accuracy:0.2834, Validation Loss:2.6150, Validation Accuracy:0.1544\n",
    "Epoch #254: Loss:2.1640, Accuracy:0.2706, Validation Loss:2.6193, Validation Accuracy:0.1527\n",
    "Epoch #255: Loss:2.1634, Accuracy:0.2686, Validation Loss:2.6128, Validation Accuracy:0.1494\n",
    "Epoch #256: Loss:2.1605, Accuracy:0.2731, Validation Loss:2.6215, Validation Accuracy:0.1511\n",
    "Epoch #257: Loss:2.1539, Accuracy:0.2723, Validation Loss:2.6203, Validation Accuracy:0.1560\n",
    "Epoch #258: Loss:2.1494, Accuracy:0.2776, Validation Loss:2.6272, Validation Accuracy:0.1642\n",
    "Epoch #259: Loss:2.1777, Accuracy:0.2645, Validation Loss:2.6295, Validation Accuracy:0.1609\n",
    "Epoch #260: Loss:2.1695, Accuracy:0.2702, Validation Loss:2.6300, Validation Accuracy:0.1478\n",
    "Epoch #261: Loss:2.1746, Accuracy:0.2657, Validation Loss:2.6174, Validation Accuracy:0.1658\n",
    "Epoch #262: Loss:2.1705, Accuracy:0.2682, Validation Loss:2.6248, Validation Accuracy:0.1593\n",
    "Epoch #263: Loss:2.1706, Accuracy:0.2735, Validation Loss:2.6182, Validation Accuracy:0.1708\n",
    "Epoch #264: Loss:2.1578, Accuracy:0.2715, Validation Loss:2.6263, Validation Accuracy:0.1527\n",
    "Epoch #265: Loss:2.1489, Accuracy:0.2686, Validation Loss:2.6247, Validation Accuracy:0.1593\n",
    "Epoch #266: Loss:2.1392, Accuracy:0.2805, Validation Loss:2.6257, Validation Accuracy:0.1494\n",
    "Epoch #267: Loss:2.1321, Accuracy:0.2838, Validation Loss:2.6306, Validation Accuracy:0.1429\n",
    "Epoch #268: Loss:2.1379, Accuracy:0.2760, Validation Loss:2.6295, Validation Accuracy:0.1494\n",
    "Epoch #269: Loss:2.1260, Accuracy:0.2809, Validation Loss:2.6358, Validation Accuracy:0.1560\n",
    "Epoch #270: Loss:2.1257, Accuracy:0.2858, Validation Loss:2.6406, Validation Accuracy:0.1560\n",
    "Epoch #271: Loss:2.1280, Accuracy:0.2895, Validation Loss:2.6429, Validation Accuracy:0.1675\n",
    "Epoch #272: Loss:2.1380, Accuracy:0.2821, Validation Loss:2.6432, Validation Accuracy:0.1708\n",
    "Epoch #273: Loss:2.1372, Accuracy:0.2760, Validation Loss:2.6354, Validation Accuracy:0.1576\n",
    "Epoch #274: Loss:2.1204, Accuracy:0.2936, Validation Loss:2.6441, Validation Accuracy:0.1576\n",
    "Epoch #275: Loss:2.1247, Accuracy:0.2830, Validation Loss:2.6434, Validation Accuracy:0.1494\n",
    "Epoch #276: Loss:2.1172, Accuracy:0.2825, Validation Loss:2.6439, Validation Accuracy:0.1560\n",
    "Epoch #277: Loss:2.1114, Accuracy:0.2982, Validation Loss:2.6476, Validation Accuracy:0.1511\n",
    "Epoch #278: Loss:2.1143, Accuracy:0.2920, Validation Loss:2.6452, Validation Accuracy:0.1511\n",
    "Epoch #279: Loss:2.1124, Accuracy:0.2908, Validation Loss:2.6536, Validation Accuracy:0.1544\n",
    "Epoch #280: Loss:2.1069, Accuracy:0.2949, Validation Loss:2.6525, Validation Accuracy:0.1494\n",
    "Epoch #281: Loss:2.1050, Accuracy:0.2965, Validation Loss:2.6594, Validation Accuracy:0.1461\n",
    "Epoch #282: Loss:2.1106, Accuracy:0.2867, Validation Loss:2.6700, Validation Accuracy:0.1560\n",
    "Epoch #283: Loss:2.1242, Accuracy:0.2842, Validation Loss:2.6633, Validation Accuracy:0.1642\n",
    "Epoch #284: Loss:2.0988, Accuracy:0.2994, Validation Loss:2.6604, Validation Accuracy:0.1544\n",
    "Epoch #285: Loss:2.1079, Accuracy:0.2875, Validation Loss:2.6612, Validation Accuracy:0.1478\n",
    "Epoch #286: Loss:2.0984, Accuracy:0.3027, Validation Loss:2.6621, Validation Accuracy:0.1544\n",
    "Epoch #287: Loss:2.0900, Accuracy:0.3047, Validation Loss:2.6634, Validation Accuracy:0.1560\n",
    "Epoch #288: Loss:2.0899, Accuracy:0.3047, Validation Loss:2.6683, Validation Accuracy:0.1626\n",
    "Epoch #289: Loss:2.0901, Accuracy:0.2940, Validation Loss:2.6724, Validation Accuracy:0.1412\n",
    "Epoch #290: Loss:2.1062, Accuracy:0.2879, Validation Loss:2.6756, Validation Accuracy:0.1511\n",
    "Epoch #291: Loss:2.1169, Accuracy:0.2858, Validation Loss:2.6812, Validation Accuracy:0.1461\n",
    "Epoch #292: Loss:2.1079, Accuracy:0.2891, Validation Loss:2.6586, Validation Accuracy:0.1609\n",
    "Epoch #293: Loss:2.0912, Accuracy:0.3006, Validation Loss:2.6776, Validation Accuracy:0.1527\n",
    "Epoch #294: Loss:2.0974, Accuracy:0.2936, Validation Loss:2.6684, Validation Accuracy:0.1576\n",
    "Epoch #295: Loss:2.0849, Accuracy:0.2986, Validation Loss:2.6871, Validation Accuracy:0.1429\n",
    "Epoch #296: Loss:2.0781, Accuracy:0.3047, Validation Loss:2.6774, Validation Accuracy:0.1576\n",
    "Epoch #297: Loss:2.0739, Accuracy:0.3088, Validation Loss:2.6799, Validation Accuracy:0.1593\n",
    "Epoch #298: Loss:2.0764, Accuracy:0.3068, Validation Loss:2.6819, Validation Accuracy:0.1576\n",
    "Epoch #299: Loss:2.0758, Accuracy:0.3002, Validation Loss:2.6885, Validation Accuracy:0.1494\n",
    "Epoch #300: Loss:2.0758, Accuracy:0.3084, Validation Loss:2.6860, Validation Accuracy:0.1379\n",
    "\n",
    "Test:\n",
    "Test Loss:2.68595958, Accuracy:0.1379\n",
    "Labels: ['my', 'ck', 'mb', 'eb', 'sg', 'ib', 'yd', 'ds', 'eo', 'sk', 'eg', 'ek', 'aa', 'ce', 'by']\n",
    "Confusion Matrix:\n",
    "      my  ck  mb  eb  sg  ib  yd  ds  eo  sk  eg  ek  aa  ce  by\n",
    "t:my   0   1   1   3   4   0   5   1   0   0   2   0   2   0   1\n",
    "t:ck   0   1   0   2   1   1   3   2   1   0   6   0   4   0   2\n",
    "t:mb   0   2   2   8  15   2   4   3   4   0   4   0   1   0   7\n",
    "t:eb   0   2   1   6   7   2   8   4   6   0   5   3   5   0   1\n",
    "t:sg   0   1   5   2  23   0   9   4   3   0   0   0   0   0   4\n",
    "t:ib   0   0   2   1  17   2  19   1   1   0   4   1   0   0   6\n",
    "t:yd   0   0   2   1  20   5  23   2   6   0   0   0   0   0   3\n",
    "t:ds   0   1   0   4   3   0   0   4   1   0  11   1   5   0   1\n",
    "t:eo   0   0   2   2   7   2   5   4   2   0   2   0   1   0   7\n",
    "t:sk   0   1   1   6   5   0   3   1   1   0   7   1   4   0   3\n",
    "t:eg   0   8   0   5   3   0   0   3   3   0  13   0  10   0   5\n",
    "t:ek   0   0   4   4  13   0   4   3   5   0   3   0   5   0   7\n",
    "t:aa   0   2   2   5   1   1   2   5   1   0   7   0   5   0   3\n",
    "t:ce   0   1   2   2   5   0   5   2   1   0   3   0   3   0   3\n",
    "t:by   0   3   1   3  11   1   2   4   4   1   6   0   1   0   3\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ck       0.04      0.04      0.04        23\n",
    "          mb       0.08      0.04      0.05        52\n",
    "          eb       0.11      0.12      0.12        50\n",
    "          sg       0.17      0.45      0.25        51\n",
    "          ib       0.12      0.04      0.06        54\n",
    "          yd       0.25      0.37      0.30        62\n",
    "          ds       0.09      0.13      0.11        31\n",
    "          eo       0.05      0.06      0.05        34\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eg       0.18      0.26      0.21        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          aa       0.11      0.15      0.12        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          by       0.05      0.07      0.06        40\n",
    "\n",
    "    accuracy                           0.14       609\n",
    "   macro avg       0.08      0.12      0.09       609\n",
    "weighted avg       0.10      0.14      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 05:14:41 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 44 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6951005227851557, 2.685987047373955, 2.6783865872275068, 2.671226079436554, 2.66354448533019, 2.6555457330493897, 2.6466050245883235, 2.634469606997736, 2.6192923672680783, 2.6012109469114657, 2.5811712886703817, 2.5609880108355694, 2.541090373726705, 2.53043372290475, 2.512258636931872, 2.5026386872496706, 2.496627653369371, 2.4874448834968903, 2.4867213112967357, 2.4771200682729337, 2.4733366097135496, 2.468853245032049, 2.4723137082724738, 2.4629257316464077, 2.4639839652332376, 2.4576551092082055, 2.4556620759134025, 2.4520538604905453, 2.452913190735189, 2.446687628678696, 2.4463830205607295, 2.444594091969758, 2.4428411438351585, 2.4416987406600676, 2.440613457330538, 2.4397070388292836, 2.4385387325913253, 2.443335640802368, 2.4397059465668276, 2.438906197868936, 2.43648242245754, 2.436178742566915, 2.4357324590040936, 2.44256392958129, 2.4394519376050074, 2.4411718132852136, 2.436231977246665, 2.4342347048773556, 2.4335237273637493, 2.4326252240461277, 2.433841112603499, 2.432700690769014, 2.4371888230391128, 2.4328986701902693, 2.432219630587473, 2.4306654644325643, 2.430673672452153, 2.4314577559923696, 2.430252771267946, 2.43103307143025, 2.4304596579133584, 2.4314403686617396, 2.430401168629062, 2.433210165434087, 2.430271391406631, 2.431039883389653, 2.430224963596889, 2.430681541831231, 2.429249677360547, 2.430186185539258, 2.4303881926293833, 2.430335498599975, 2.4307440270735516, 2.4294907533867995, 2.4305183268924457, 2.432468958088917, 2.430318017702776, 2.430522048806126, 2.432514001191739, 2.431404719407531, 2.4322807597017837, 2.431824372515498, 2.431584591544516, 2.431609742159914, 2.4314278632353483, 2.4332788299848684, 2.4312663046988754, 2.435242937899184, 2.4340519145596007, 2.4340372394849905, 2.4328753247441135, 2.4341868676967024, 2.4357099239462117, 2.432181672705414, 2.4339641334583804, 2.432509641537721, 2.4320464333877188, 2.4334653171608207, 2.437191124228617, 2.436160069185329, 2.443870138847965, 2.4395527048846968, 2.4384030681134052, 2.4367696401129413, 2.4366903093648076, 2.4358590577036288, 2.4385162887510603, 2.4377033326817656, 2.443003612981837, 2.441908599512135, 2.4389113330684467, 2.4420768443391045, 2.4426203627500236, 2.4409256079318293, 2.444191139907085, 2.4444863541764383, 2.4576217822840647, 2.4416380464932796, 2.4420864899170223, 2.452267533648386, 2.4449143836455196, 2.4462583123756745, 2.4487882051953345, 2.44497678315111, 2.4480429795771004, 2.460415546921478, 2.4490764814448864, 2.456369248125549, 2.4593601614383642, 2.4481538790591637, 2.452820137020794, 2.462284179743875, 2.454310986013052, 2.453431194443225, 2.4609997018021708, 2.4561550296194645, 2.4563894087849385, 2.4610405782564913, 2.4626598702667186, 2.4605959291724346, 2.4727567483247404, 2.4629126417225806, 2.4664231579879234, 2.4689017132976763, 2.4698858077107197, 2.4694961679392846, 2.4697283058135184, 2.472150284668495, 2.4745193545649986, 2.47351530424284, 2.475430594680736, 2.4790945252761465, 2.4772471449841027, 2.478650039835712, 2.4938339732942127, 2.4862994125911166, 2.476826012428171, 2.487428494470656, 2.490579074630988, 2.4875720929238203, 2.490043035281703, 2.483250262114802, 2.490857324772476, 2.490639318582068, 2.4864432157945555, 2.4901981032736393, 2.4979457530286315, 2.497524877291399, 2.494669439757399, 2.5028711993901798, 2.4969396963103847, 2.5039673462289893, 2.4993460483738943, 2.502236233556212, 2.5010562013522746, 2.506684242210952, 2.508664837415974, 2.502467583357211, 2.519361990425974, 2.5084885583918277, 2.5067523935158262, 2.508517630190293, 2.496210462745579, 2.5048924154052985, 2.519975307931258, 2.5360322163022797, 2.5091699093629183, 2.5100975216707377, 2.5173366426051347, 2.507805527137418, 2.5208879656392367, 2.5187382490568364, 2.5202782510340898, 2.5228573477326943, 2.5296654712977666, 2.5235350507820766, 2.5322575733579438, 2.5393356795381443, 2.5364938784506914, 2.5252370865669937, 2.5327013165296983, 2.5364056502656984, 2.543508127600884, 2.52913876354988, 2.5384243532941846, 2.5449039967385025, 2.5408752579211407, 2.5432598720043162, 2.5473051701468985, 2.555255347284777, 2.5422136924536947, 2.5461729864768796, 2.555091747117943, 2.5591361389567306, 2.5554559175995575, 2.5550431234300235, 2.561311597698819, 2.5575986264765946, 2.5673149606864443, 2.555762632335544, 2.562826898101907, 2.5593857381535674, 2.5627278248077543, 2.562630573516996, 2.5685469787109074, 2.582036636145831, 2.5723978341702365, 2.5802407707095343, 2.574911279631366, 2.5693100156455206, 2.58051351412568, 2.5796529647751982, 2.582620489186254, 2.5766268727814623, 2.5924740522757346, 2.5825682281469087, 2.5870659597988785, 2.5999008560024066, 2.592246306549348, 2.595478204670798, 2.585836940994012, 2.5967461526491764, 2.5970480117108825, 2.609111442550258, 2.602342759447145, 2.5977291951234314, 2.605472578007991, 2.6013501575231945, 2.604710840435059, 2.6073225828618645, 2.6173140912611887, 2.616080486911467, 2.614954138820003, 2.6193304378998103, 2.61280102408774, 2.6214605528732826, 2.6203473655656837, 2.627226436862413, 2.6295348063282584, 2.6299713436997387, 2.6174145931093564, 2.6247561929261156, 2.6181903386546668, 2.6263225383946462, 2.62472362980271, 2.625720587857251, 2.6305979621429945, 2.629513192842355, 2.635814286217901, 2.64059161200312, 2.6429068435393335, 2.6431528280912753, 2.635447821985129, 2.644134191065195, 2.6433627366628163, 2.643935212948052, 2.6475509317050427, 2.6452373366050534, 2.6535681703407774, 2.6525429497015693, 2.659384784244356, 2.670000029706407, 2.6633256049383256, 2.6604350889453356, 2.661172110067408, 2.6620952871632695, 2.663361590875585, 2.6682505811180777, 2.6723600800205727, 2.6756457699147744, 2.6811710014719092, 2.658606791926918, 2.6775984842397507, 2.6683727841463387, 2.687063398032353, 2.6774331168783907, 2.679943375986785, 2.6819389238341884, 2.688506266557916, 2.685959528623935], 'val_acc': [0.0443349753663996, 0.04105090301893713, 0.10673234800155136, 0.16091953961817893, 0.1642036119656414, 0.15763546727071645, 0.15270935860271329, 0.15763546697709752, 0.14614121420140727, 0.14942528645099679, 0.1510673226736645, 0.14778325112142, 0.13957307049744627, 0.1625615756451007, 0.16748768401948494, 0.16912972024215267, 0.18062397152527995, 0.17405582900802882, 0.1904761883719214, 0.17733990145336426, 0.19704433296897336, 0.18719211592658597, 0.20032840531643584, 0.18555007980179122, 0.19540229684417862, 0.19540229664843267, 0.20197044144123058, 0.1970443328711004, 0.21182265828787203, 0.19868636899589515, 0.2036124776638983, 0.20525451378869305, 0.2036124776638983, 0.20197044144123058, 0.20361247756602532, 0.20853858594040955, 0.2003284051206899, 0.20032840521856285, 0.19540229655055968, 0.21182265809212608, 0.2052545135929471, 0.2134646943147938, 0.2068965499134878, 0.2085385858425366, 0.19704433277322742, 0.2068965498156148, 0.20361247756602532, 0.21018062216307729, 0.20853858594040955, 0.21018062206520433, 0.21182265828787203, 0.21182265828787203, 0.20525451378869305, 0.2068965499134878, 0.21018062216307729, 0.21346469441266677, 0.21346469441266677, 0.2167487665643833, 0.20853858603828254, 0.20853858603828254, 0.20853858613615553, 0.21346469441266677, 0.20361247756602532, 0.2134646943147938, 0.20525451378869305, 0.21510673053746154, 0.21018062216307729, 0.21182265828787203, 0.21018062216307729, 0.2151067306353345, 0.2068965499134878, 0.2068965499134878, 0.21182265818999907, 0.20525451369082007, 0.21018062216307729, 0.20853858594040955, 0.2068965499134878, 0.20525451388656604, 0.20689654971774185, 0.2068965499134878, 0.21182265818999907, 0.2068965499134878, 0.21018062216307729, 0.21018062206520433, 0.20525451378869305, 0.2134646943147938, 0.20853858613615553, 0.2167487665643833, 0.20525451388656604, 0.21182265818999907, 0.20197044153910357, 0.21018062206520433, 0.2085385858425366, 0.20853858603828254, 0.21510673043958856, 0.2134646943147938, 0.2085385858425366, 0.20689654971774185, 0.21182265818999907, 0.2068965498156148, 0.19868636899589515, 0.21018062196733134, 0.2003284051206899, 0.21018062206520433, 0.2036124776638983, 0.20853858594040955, 0.2036124772724064, 0.20689654961986886, 0.20689654971774185, 0.2085385858425366, 0.20197044124548463, 0.20197044124548463, 0.2085385858425366, 0.2003284051206899, 0.19704433296897336, 0.2052545135929471, 0.192118226478644, 0.20689654961986886, 0.20197044114761165, 0.2052545135929471, 0.20197044124548463, 0.19868636889802216, 0.20689654961986886, 0.20361247737027938, 0.20197044114761165, 0.20197044104973866, 0.1970443349509012, 0.20361247737027938, 0.20197044124548463, 0.1970443349509012, 0.2085385857446636, 0.19704433277322742, 0.2003284050228169, 0.19704433267535443, 0.20197044124548463, 0.19540229664843267, 0.19376026052363793, 0.19540229655055968, 0.2003284051206899, 0.2003284051206899, 0.19376026042576494, 0.19540229664843267, 0.19047618827404844, 0.19704433306684635, 0.19540229674630563, 0.1986863690937681, 0.1970443328711004, 0.18719211592658597, 0.19376026071938388, 0.19211822449671614, 0.18555007989966418, 0.19540229655055968, 0.19540229684417862, 0.18719211592658597, 0.18883415403330855, 0.1904761883719214, 0.19376026042576494, 0.1888341520513807, 0.19704433296897336, 0.18719211602445893, 0.18555007980179122, 0.19047618817617545, 0.19376026250556577, 0.18883415214925367, 0.18883415224712666, 0.1888341542290545, 0.18555008197946501, 0.185550082077338, 0.1904761883719214, 0.19376026250556577, 0.17733989917781748, 0.18883415393543557, 0.19047618827404844, 0.18062397331146185, 0.1921182243009702, 0.18555008197946501, 0.18226600982774852, 0.19376026052363793, 0.1756978649370776, 0.18555008188159203, 0.1756978653285695, 0.18390804585467027, 0.1970443328711004, 0.18719211810425976, 0.1822660094362566, 0.1724137927853611, 0.1888341543269275, 0.18555007980179122, 0.18390804575679728, 0.18719211602445893, 0.18719211810425976, 0.18390804546317835, 0.18719211820213275, 0.19376026260343873, 0.1822660093383836, 0.18555007980179122, 0.18226600972987553, 0.18719211800638677, 0.17077175656269336, 0.1921182243009702, 0.1773399013554913, 0.1822660093383836, 0.1691297206336446, 0.19047619025597626, 0.17405582881228285, 0.18390804556105134, 0.18719211810425976, 0.18390804595254326, 0.18883415383756258, 0.17569786523069655, 0.1756978649370776, 0.17077175656269336, 0.17898193708879412, 0.18555008178371907, 0.1871921179085138, 0.18226600953412955, 0.17241379288323408, 0.18555008178371907, 0.1658456478946902, 0.1724137926874881, 0.18555007980179122, 0.17733990106187233, 0.19211822608715207, 0.17405582881228285, 0.16912972024215267, 0.17733990106187233, 0.1724137927853611, 0.1691297205357716, 0.16748768411735793, 0.1674876842152309, 0.1740558286165369, 0.1740558287144099, 0.16091953952030597, 0.16584564809043614, 0.16748768441097686, 0.15927750329763823, 0.16256157593871964, 0.16091953961817893, 0.16748768411735793, 0.154351394923254, 0.1740558286165369, 0.17077175626907443, 0.14942528645099679, 0.16912972034002563, 0.1576354670749705, 0.1609195397160519, 0.1658456477968172, 0.1707717563669474, 0.1625615756451007, 0.1559934312437947, 0.15763546727071645, 0.17077175656269336, 0.15435139472750803, 0.1527093589942052, 0.14942528645099679, 0.15106732257579153, 0.1559934312437947, 0.1642036120635144, 0.16091953961817893, 0.14778325061982098, 0.16584564799256318, 0.15927750378700312, 0.17077175656269336, 0.15270935860271329, 0.15927750310189226, 0.14942528654886975, 0.14285714195181778, 0.14942528664674273, 0.1559934311459217, 0.15599343104804875, 0.16748768441097686, 0.17077175675843934, 0.1576354675643354, 0.15763546727071645, 0.14942528654886975, 0.15599343134166768, 0.15106732277153748, 0.15106732286941046, 0.15435139511899995, 0.14942528664674273, 0.14614121429928026, 0.15599343104804875, 0.1642036120635144, 0.15435139502112696, 0.14778325061982098, 0.15435139511899995, 0.1559934311459217, 0.16256157584084666, 0.14121510582702304, 0.1510673226736645, 0.14614121439715325, 0.16091953942243298, 0.1527093590920782, 0.1576354671728435, 0.14285714214756376, 0.15763546746646243, 0.1592775033955112, 0.15763546736858944, 0.1494252868424887, 0.13793103357743355], 'loss': [2.7024521303862272, 2.6919666277554493, 2.6843182562802608, 2.6779638780950275, 2.6707735576668803, 2.663959219715189, 2.6560975642664477, 2.6471195590079932, 2.635225668775962, 2.620837097637952, 2.6038698396154007, 2.5850986664789657, 2.5657923461475414, 2.550695888609367, 2.5342111632564475, 2.524195829015493, 2.516648569635787, 2.5098370010603137, 2.5052013923010543, 2.498328455664539, 2.4930917155326515, 2.48782492085404, 2.482621927672588, 2.48629940256201, 2.4753586741688314, 2.4748536868751416, 2.4709102374082716, 2.4676938460348077, 2.4653980406892373, 2.463212281428813, 2.460513849767571, 2.458368650354154, 2.4560170908972956, 2.45350245401355, 2.4521589243926063, 2.450463369201096, 2.449606150229609, 2.448848486925787, 2.44903706348897, 2.449655453625156, 2.446540310740226, 2.4443221492689005, 2.442929814630465, 2.4434799992328307, 2.445321090020683, 2.4446957456992147, 2.445444487546259, 2.4419550343460616, 2.443884058605719, 2.441209267786641, 2.43920373084364, 2.4372905585310543, 2.4380370685452064, 2.442383148244274, 2.439663317462992, 2.4389380626365145, 2.4368138737257503, 2.4363175565457196, 2.435689516968306, 2.4352293917285834, 2.435392443110566, 2.4352771806031526, 2.4309477067825975, 2.4326254150461124, 2.4296894417161568, 2.4290542012122622, 2.429573411373632, 2.4284185990171023, 2.4270657033156566, 2.428817540860029, 2.4286069068086222, 2.4279015009408123, 2.423210448944593, 2.4241406695309116, 2.4226359466262912, 2.422906414243475, 2.4219948488584047, 2.421119367513324, 2.4205274619116186, 2.421651848728407, 2.419758668915202, 2.419316264980872, 2.418570436736152, 2.4177069741352875, 2.4165552868735376, 2.415647810146794, 2.4152453630122315, 2.414566261910315, 2.414757641336022, 2.4133326560809625, 2.4131642900697994, 2.412616898244901, 2.41203733861324, 2.4101354364007403, 2.409381186056431, 2.41128301238622, 2.408432139410375, 2.4076049295538993, 2.405955341268614, 2.405257422527493, 2.403786692629115, 2.4108545352301314, 2.4089973490830565, 2.4089003640278652, 2.4067921235086494, 2.403707817007139, 2.401195781970171, 2.402893131518511, 2.3990284934425747, 2.3993477184914465, 2.397019338020309, 2.3957966444183914, 2.3964916543549335, 2.3948541880143495, 2.393318162072121, 2.3915893433275164, 2.3969631238150155, 2.39798490271676, 2.39005613562018, 2.3884094336194424, 2.3894638729291287, 2.38666960414675, 2.3837657140265747, 2.381597943766161, 2.3822291469182324, 2.384663768079002, 2.3878522767178576, 2.3807928166595085, 2.383444513089848, 2.3836252308479327, 2.375457491571164, 2.374163103103638, 2.3741048732577408, 2.3734227454637846, 2.370829713613835, 2.366631153623671, 2.3677265113384083, 2.3651441665155932, 2.3643111827192365, 2.3622983032673046, 2.362436927466422, 2.3644303965127933, 2.3580967984405143, 2.3575718858158807, 2.354679517236823, 2.3532332112901755, 2.351036325568291, 2.3505114431988288, 2.347654514978554, 2.345854816201776, 2.3439919502093804, 2.342855704736416, 2.350163154582468, 2.3402431458663155, 2.338156441349758, 2.335601569397004, 2.3482121428425065, 2.337164515391512, 2.333212887530944, 2.3354345440130215, 2.3260621740587926, 2.326343450409186, 2.3244348777637835, 2.332328096992916, 2.334490961901216, 2.3255470956375466, 2.314819377550599, 2.3159850667879076, 2.3135552997706608, 2.316876675509819, 2.3279384515123933, 2.3187309111413037, 2.318019013825873, 2.308060862738983, 2.3060888144514644, 2.3079470212454667, 2.318855816236022, 2.3318298319281983, 2.318841167641861, 2.3171409898714854, 2.303273068414332, 2.30359544744237, 2.2923731293766405, 2.2973609371106973, 2.308066051941388, 2.3089786186844905, 2.313807063719575, 2.2952938237474196, 2.2852673356293165, 2.2893120598254506, 2.2778632056296972, 2.2760481894138658, 2.2727857319481317, 2.274893855559018, 2.2739918071386507, 2.2688920838632134, 2.2680287573616607, 2.2676746858953205, 2.2686733671771915, 2.26113472591925, 2.2583006630687987, 2.2557914769135463, 2.2611849565525564, 2.2567747896456867, 2.2530355505385193, 2.249295530280049, 2.25002619177409, 2.25252310696079, 2.2448134600504224, 2.2448105526166287, 2.239482216179004, 2.240187034469855, 2.2425363158788034, 2.2383508809538104, 2.233076649671707, 2.2359464408435863, 2.237087030430349, 2.2266538301777303, 2.227543233209567, 2.2249754012732534, 2.2210802069434883, 2.226659083023698, 2.213065854822586, 2.214151797891887, 2.21767657062601, 2.219009315894125, 2.220239748651242, 2.2092955728331143, 2.206523188722207, 2.2069624549309577, 2.200369426601966, 2.197290639563997, 2.2055034953710724, 2.1897637495514792, 2.192564313661391, 2.1908636609142076, 2.192138469586382, 2.1899599153647924, 2.1997972558900805, 2.2077384530641213, 2.2000814872600705, 2.187496486385745, 2.1792593666170657, 2.175336146599458, 2.1763476381556455, 2.17565794118376, 2.1701438410326195, 2.1740172480166082, 2.1657286803580407, 2.1560783754384003, 2.1633818759565724, 2.1593784359691077, 2.1565471223247616, 2.1639611930573013, 2.1633801543492313, 2.160462161696667, 2.153925634262743, 2.149384625883318, 2.1776772100577855, 2.1695145187926244, 2.174608609221065, 2.1704568876622883, 2.170590391883615, 2.15777196042102, 2.14889570986221, 2.1391589387975922, 2.1320733763598807, 2.137901043255471, 2.1259703645960752, 2.1257300166378768, 2.1280454382025, 2.1379517507259362, 2.137212248749312, 2.120448294410471, 2.1247497695672193, 2.1172200462411808, 2.111411903526259, 2.1142530994493614, 2.112418693487649, 2.1069190134013214, 2.1049542992022, 2.110584943250464, 2.124210821480722, 2.0988150310712186, 2.1079457596342177, 2.0984494903493958, 2.090033979925042, 2.0898539126041733, 2.0901442865571447, 2.106232769337523, 2.116895394844196, 2.1079278068620813, 2.091182054680231, 2.09740482796389, 2.0848666226349817, 2.0780919978750805, 2.0739233251959392, 2.076412575053973, 2.0757691164036305, 2.0758234730001837], 'acc': [0.04476386014624543, 0.04599589319169154, 0.06611909606251139, 0.10718685802860181, 0.14702258639389484, 0.1470225881746907, 0.14496919849937212, 0.14989733050368895, 0.14004106804330735, 0.13552361486873588, 0.13716632514518878, 0.13880903403249853, 0.13716632375604562, 0.14579055453717585, 0.15728952743311927, 0.15523613973442288, 0.16016427056378163, 0.15605749436472477, 0.163860369713889, 0.15687885099000753, 0.1642710465302947, 0.16427104635282708, 0.16755646788486464, 0.16262833668221194, 0.1704312125821378, 0.16386037047883567, 0.17002053377075116, 0.1659137579817057, 0.17535934300148512, 0.1691991783387852, 0.17494866618507943, 0.17248459969335514, 0.17494866479593627, 0.17412731034310203, 0.17207392146944755, 0.1757700212253927, 0.17700205407960215, 0.17535934182652702, 0.17494866577506799, 0.17741273169767197, 0.17618069862927743, 0.17453798719622515, 0.17577002158032795, 0.16796714609041352, 0.17412731034310203, 0.17659137509074788, 0.1757700204420873, 0.1774127311101929, 0.18275153953926274, 0.17618069825598345, 0.17577001983624954, 0.17700205447125483, 0.1757700200320759, 0.17782340812242498, 0.17043121199465874, 0.17741273169767197, 0.17700205407960215, 0.17659137605152092, 0.17577002022790222, 0.1774127299168761, 0.17864476453352268, 0.17700205347376438, 0.17700205288628534, 0.1815195070950647, 0.18193018369728534, 0.18151950807419645, 0.18110883024194158, 0.17946611800722517, 0.181108830651953, 0.17782340890573037, 0.1786447629669119, 0.17577002061955493, 0.1778234095115681, 0.17741273011270245, 0.1815195066850533, 0.1815195070950647, 0.18110883004611522, 0.1774127307001815, 0.18151950727253235, 0.18069815301552444, 0.18151950805583775, 0.17864476394604364, 0.18357289495287, 0.1827515393434364, 0.18151950727253235, 0.1831622173531589, 0.18028747381248514, 0.1802874752016283, 0.18069815184056637, 0.17946611920054198, 0.1815195062934006, 0.18028747539745463, 0.18110883026030028, 0.18480492762961181, 0.18439425059902104, 0.18193018528225485, 0.18357289575453412, 0.18275154073257954, 0.18521560661846606, 0.18234086250867199, 0.18521560524768163, 0.1864476396868606, 0.18398357378261535, 0.18234086311450973, 0.18603696067964762, 0.1864476394726755, 0.1860369602879949, 0.18480492723795913, 0.19342915821491571, 0.18932238121419473, 0.18767967035026276, 0.1913757693045438, 0.19055441485170957, 0.1942505122393798, 0.19301847959935542, 0.1950718688646626, 0.18234086193955165, 0.19055441467424192, 0.19383983519043033, 0.19630390150468696, 0.18973306041723403, 0.19466119083658137, 0.19466119124659279, 0.19301848136179256, 0.1901437362728667, 0.18973306022140768, 0.1938398358146268, 0.19753593496473418, 0.19630390211052473, 0.1938398365979322, 0.19630390170051332, 0.19753593357559102, 0.19425051382434932, 0.19917864582866615, 0.1917864465309609, 0.19712525654500027, 0.20000000008567403, 0.20123203213821936, 0.1995893220759515, 0.20164271036212694, 0.1979466113894872, 0.20287474241467227, 0.20082135551764, 0.2016427101663006, 0.20533880970806068, 0.2020533877660117, 0.20574948613281369, 0.20328542003274208, 0.2086242292451173, 0.19917864385204392, 0.2036960984341173, 0.2041067752688817, 0.2061601637692422, 0.21108829458024223, 0.20698151859537042, 0.20780287520229448, 0.20657084197479106, 0.21273100564000053, 0.21026694151655115, 0.20903490846651535, 0.2143737165039325, 0.2164271038293349, 0.21314168404137573, 0.20821355262453797, 0.2053388079272648, 0.21108829458024223, 0.2160164267803854, 0.2135523606803138, 0.21683778121486091, 0.21314168364972305, 0.2094455841263217, 0.22464065813063597, 0.21930184852660803, 0.2176591374301323, 0.21765913725266467, 0.2147843937303496, 0.21355236146361922, 0.20533880833727622, 0.20862423024260776, 0.2188911712818322, 0.2193018477065852, 0.21806981446072307, 0.22422997951507567, 0.2213552366169571, 0.21848049190132524, 0.21848049366376238, 0.21232032780774565, 0.21848049289881571, 0.22915811014860807, 0.2328542083195837, 0.22751540088800434, 0.2320328542951196, 0.23490759640993278, 0.2308008204434197, 0.2299794657764004, 0.23490759644665024, 0.23901437358812141, 0.23572895223355148, 0.2295687873750252, 0.24599589268529684, 0.23449692059101754, 0.24435318100134204, 0.2336755649448665, 0.24558521365972516, 0.24353182893514144, 0.25051334628823846, 0.2455852156730648, 0.23860369499091985, 0.24558521467557434, 0.2468172485089155, 0.24558521490811813, 0.25010267043260576, 0.23901437298228365, 0.24681724870474187, 0.2505133472673702, 0.25051334728572894, 0.24517453903412673, 0.25256673457441386, 0.2509240250812664, 0.25462012423137376, 0.2570841881773555, 0.25010266827851596, 0.25790554341349514, 0.25503080028283276, 0.2546201234847858, 0.252977411800831, 0.2484599607436319, 0.26570841991925875, 0.25749486638290436, 0.2554414779009026, 0.2603696099052194, 0.2587268976337856, 0.26488706409564006, 0.26694044979935555, 0.26283367737607544, 0.26283367659277007, 0.26160164197612346, 0.26283367620111736, 0.2591375760718782, 0.2533880896147272, 0.25667351193007015, 0.2611909653371854, 0.2698151937133233, 0.2706365525110546, 0.2661190965581968, 0.2677618084012605, 0.27186858121619334, 0.26119096592466445, 0.27022587371802675, 0.2780287486571796, 0.2698151951208252, 0.2743326485279404, 0.2833675571046081, 0.2706365507486175, 0.26858316306827984, 0.2731006168486891, 0.2722792627875075, 0.27761806786917076, 0.2644763872608757, 0.2702258725614274, 0.2657084177284515, 0.26817248545021, 0.27351129427093257, 0.2714579032064708, 0.26858316125076653, 0.28049281256644387, 0.283778235310157, 0.27597535958769875, 0.2809034912003629, 0.28583162297213593, 0.28952772255061343, 0.28213552503370404, 0.27597535919604604, 0.293634496344678, 0.28295687729573105, 0.2825462020642948, 0.2981519511225777, 0.2919917843057879, 0.29075975481734384, 0.29486652884395215, 0.29650924065029843, 0.2866529760541857, 0.28418891273240043, 0.29938398162687097, 0.28747433485191703, 0.30266940237560314, 0.3047227937950001, 0.30472279422337023, 0.2940451759577287, 0.287885010315897, 0.2858316204263934, 0.2891170431701065, 0.30061601863015114, 0.2936344961488516, 0.2985626281531684, 0.3047227948108493, 0.3088295701715246, 0.30677618168952286, 0.3002053405837112, 0.3084188890285805]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
