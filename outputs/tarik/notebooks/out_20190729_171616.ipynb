{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf15.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 17:16:16 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'All', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001D6856D6E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001D6D9F47EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1132, Accuracy:0.3006, Validation Loss:1.1030, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0981, Accuracy:0.3943, Validation Loss:1.0931, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0900, Accuracy:0.3943, Validation Loss:1.0871, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0837, Accuracy:0.3943, Validation Loss:1.0806, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0788, Accuracy:0.3943, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0756, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #10: Loss:1.0749, Accuracy:0.3885, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #11: Loss:1.0749, Accuracy:0.3951, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0742, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0742, Accuracy:0.3938, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0742, Accuracy:0.3938, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #30: Loss:1.0740, Accuracy:0.3930, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0737, Accuracy:0.3938, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #35: Loss:1.0738, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #36: Loss:1.0739, Accuracy:0.4049, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #37: Loss:1.0738, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0738, Accuracy:0.3914, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #42: Loss:1.0740, Accuracy:0.3996, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #43: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #44: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #45: Loss:1.0736, Accuracy:0.3938, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #47: Loss:1.0739, Accuracy:0.3955, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #49: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #50: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #51: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #54: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #55: Loss:1.0744, Accuracy:0.3934, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #57: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #58: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #59: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #60: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #61: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3777\n",
    "Epoch #64: Loss:1.0737, Accuracy:0.4078, Validation Loss:1.0741, Validation Accuracy:0.3810\n",
    "Epoch #65: Loss:1.0737, Accuracy:0.4049, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #66: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #67: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #68: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #69: Loss:1.0736, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #70: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #71: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #72: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0740, Validation Accuracy:0.3760\n",
    "Epoch #73: Loss:1.0734, Accuracy:0.4099, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #74: Loss:1.0733, Accuracy:0.3951, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #75: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #76: Loss:1.0733, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #77: Loss:1.0735, Accuracy:0.3971, Validation Loss:1.0738, Validation Accuracy:0.3810\n",
    "Epoch #78: Loss:1.0733, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #80: Loss:1.0732, Accuracy:0.3951, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #81: Loss:1.0733, Accuracy:0.3873, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #82: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #83: Loss:1.0737, Accuracy:0.3918, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #84: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #85: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #86: Loss:1.0735, Accuracy:0.3938, Validation Loss:1.0745, Validation Accuracy:0.3678\n",
    "Epoch #87: Loss:1.0740, Accuracy:0.3910, Validation Loss:1.0743, Validation Accuracy:0.3629\n",
    "Epoch #88: Loss:1.0735, Accuracy:0.3860, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #89: Loss:1.0742, Accuracy:0.3951, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #90: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.3892\n",
    "Epoch #91: Loss:1.0731, Accuracy:0.4033, Validation Loss:1.0743, Validation Accuracy:0.3629\n",
    "Epoch #92: Loss:1.0730, Accuracy:0.4066, Validation Loss:1.0741, Validation Accuracy:0.3695\n",
    "Epoch #93: Loss:1.0736, Accuracy:0.3901, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #94: Loss:1.0732, Accuracy:0.4021, Validation Loss:1.0737, Validation Accuracy:0.3793\n",
    "Epoch #95: Loss:1.0731, Accuracy:0.4140, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #96: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.3892\n",
    "Epoch #97: Loss:1.0728, Accuracy:0.3975, Validation Loss:1.0729, Validation Accuracy:0.3908\n",
    "Epoch #98: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #99: Loss:1.0728, Accuracy:0.4094, Validation Loss:1.0734, Validation Accuracy:0.3777\n",
    "Epoch #100: Loss:1.0725, Accuracy:0.4156, Validation Loss:1.0738, Validation Accuracy:0.3777\n",
    "Epoch #101: Loss:1.0727, Accuracy:0.4078, Validation Loss:1.0740, Validation Accuracy:0.3892\n",
    "Epoch #102: Loss:1.0725, Accuracy:0.4111, Validation Loss:1.0738, Validation Accuracy:0.3711\n",
    "Epoch #103: Loss:1.0727, Accuracy:0.4131, Validation Loss:1.0738, Validation Accuracy:0.3727\n",
    "Epoch #104: Loss:1.0724, Accuracy:0.4070, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #105: Loss:1.0724, Accuracy:0.4045, Validation Loss:1.0734, Validation Accuracy:0.3777\n",
    "Epoch #106: Loss:1.0723, Accuracy:0.4082, Validation Loss:1.0736, Validation Accuracy:0.3777\n",
    "Epoch #107: Loss:1.0721, Accuracy:0.4053, Validation Loss:1.0739, Validation Accuracy:0.3695\n",
    "Epoch #108: Loss:1.0721, Accuracy:0.4078, Validation Loss:1.0737, Validation Accuracy:0.3711\n",
    "Epoch #109: Loss:1.0718, Accuracy:0.4099, Validation Loss:1.0736, Validation Accuracy:0.3744\n",
    "Epoch #110: Loss:1.0718, Accuracy:0.4074, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #111: Loss:1.0717, Accuracy:0.4033, Validation Loss:1.0739, Validation Accuracy:0.3727\n",
    "Epoch #112: Loss:1.0719, Accuracy:0.4082, Validation Loss:1.0741, Validation Accuracy:0.3711\n",
    "Epoch #113: Loss:1.0717, Accuracy:0.4082, Validation Loss:1.0739, Validation Accuracy:0.3810\n",
    "Epoch #114: Loss:1.0713, Accuracy:0.4078, Validation Loss:1.0733, Validation Accuracy:0.3793\n",
    "Epoch #115: Loss:1.0726, Accuracy:0.4053, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #116: Loss:1.0715, Accuracy:0.4070, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #117: Loss:1.0715, Accuracy:0.4078, Validation Loss:1.0736, Validation Accuracy:0.3826\n",
    "Epoch #118: Loss:1.0713, Accuracy:0.4074, Validation Loss:1.0739, Validation Accuracy:0.3678\n",
    "Epoch #119: Loss:1.0714, Accuracy:0.4025, Validation Loss:1.0738, Validation Accuracy:0.3826\n",
    "Epoch #120: Loss:1.0712, Accuracy:0.4074, Validation Loss:1.0735, Validation Accuracy:0.3629\n",
    "Epoch #121: Loss:1.0716, Accuracy:0.4074, Validation Loss:1.0744, Validation Accuracy:0.3612\n",
    "Epoch #122: Loss:1.0705, Accuracy:0.4099, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #123: Loss:1.0709, Accuracy:0.4086, Validation Loss:1.0732, Validation Accuracy:0.3744\n",
    "Epoch #124: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #125: Loss:1.0728, Accuracy:0.3996, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #126: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0732, Validation Accuracy:0.3629\n",
    "Epoch #127: Loss:1.0726, Accuracy:0.3963, Validation Loss:1.0727, Validation Accuracy:0.3990\n",
    "Epoch #128: Loss:1.0729, Accuracy:0.3926, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #129: Loss:1.0722, Accuracy:0.3955, Validation Loss:1.0721, Validation Accuracy:0.3990\n",
    "Epoch #130: Loss:1.0727, Accuracy:0.3906, Validation Loss:1.0722, Validation Accuracy:0.3892\n",
    "Epoch #131: Loss:1.0720, Accuracy:0.3979, Validation Loss:1.0726, Validation Accuracy:0.3974\n",
    "Epoch #132: Loss:1.0718, Accuracy:0.4008, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #133: Loss:1.0718, Accuracy:0.3930, Validation Loss:1.0729, Validation Accuracy:0.3580\n",
    "Epoch #134: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0725, Validation Accuracy:0.3727\n",
    "Epoch #135: Loss:1.0715, Accuracy:0.3984, Validation Loss:1.0725, Validation Accuracy:0.3678\n",
    "Epoch #136: Loss:1.0703, Accuracy:0.4078, Validation Loss:1.0721, Validation Accuracy:0.3760\n",
    "Epoch #137: Loss:1.0701, Accuracy:0.4090, Validation Loss:1.0719, Validation Accuracy:0.3793\n",
    "Epoch #138: Loss:1.0701, Accuracy:0.4131, Validation Loss:1.0724, Validation Accuracy:0.3875\n",
    "Epoch #139: Loss:1.0708, Accuracy:0.4090, Validation Loss:1.0735, Validation Accuracy:0.3826\n",
    "Epoch #140: Loss:1.0717, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3793\n",
    "Epoch #141: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #142: Loss:1.0713, Accuracy:0.4074, Validation Loss:1.0731, Validation Accuracy:0.3859\n",
    "Epoch #143: Loss:1.0717, Accuracy:0.3996, Validation Loss:1.0729, Validation Accuracy:0.3875\n",
    "Epoch #144: Loss:1.0703, Accuracy:0.4115, Validation Loss:1.0724, Validation Accuracy:0.3990\n",
    "Epoch #145: Loss:1.0707, Accuracy:0.4107, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #146: Loss:1.0697, Accuracy:0.4131, Validation Loss:1.0731, Validation Accuracy:0.3744\n",
    "Epoch #147: Loss:1.0703, Accuracy:0.4082, Validation Loss:1.0729, Validation Accuracy:0.3744\n",
    "Epoch #148: Loss:1.0698, Accuracy:0.4136, Validation Loss:1.0739, Validation Accuracy:0.3727\n",
    "Epoch #149: Loss:1.0693, Accuracy:0.4086, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #150: Loss:1.0702, Accuracy:0.4066, Validation Loss:1.0736, Validation Accuracy:0.3892\n",
    "Epoch #151: Loss:1.0693, Accuracy:0.4099, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #152: Loss:1.0696, Accuracy:0.4119, Validation Loss:1.0741, Validation Accuracy:0.3645\n",
    "Epoch #153: Loss:1.0688, Accuracy:0.4078, Validation Loss:1.0746, Validation Accuracy:0.3793\n",
    "Epoch #154: Loss:1.0687, Accuracy:0.4131, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #155: Loss:1.0692, Accuracy:0.4111, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #156: Loss:1.0682, Accuracy:0.4078, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #157: Loss:1.0688, Accuracy:0.4094, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #158: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0734, Validation Accuracy:0.3810\n",
    "Epoch #159: Loss:1.0695, Accuracy:0.4099, Validation Loss:1.0746, Validation Accuracy:0.3727\n",
    "Epoch #160: Loss:1.0693, Accuracy:0.4140, Validation Loss:1.0758, Validation Accuracy:0.3842\n",
    "Epoch #161: Loss:1.0695, Accuracy:0.4119, Validation Loss:1.0760, Validation Accuracy:0.3678\n",
    "Epoch #162: Loss:1.0703, Accuracy:0.3967, Validation Loss:1.0762, Validation Accuracy:0.3711\n",
    "Epoch #163: Loss:1.0713, Accuracy:0.4066, Validation Loss:1.0762, Validation Accuracy:0.3744\n",
    "Epoch #164: Loss:1.0700, Accuracy:0.4094, Validation Loss:1.0768, Validation Accuracy:0.3645\n",
    "Epoch #165: Loss:1.0697, Accuracy:0.4115, Validation Loss:1.0766, Validation Accuracy:0.3744\n",
    "Epoch #166: Loss:1.0694, Accuracy:0.4136, Validation Loss:1.0767, Validation Accuracy:0.3662\n",
    "Epoch #167: Loss:1.0695, Accuracy:0.4012, Validation Loss:1.0767, Validation Accuracy:0.3629\n",
    "Epoch #168: Loss:1.0700, Accuracy:0.4115, Validation Loss:1.0758, Validation Accuracy:0.3842\n",
    "Epoch #169: Loss:1.0691, Accuracy:0.4107, Validation Loss:1.0767, Validation Accuracy:0.3744\n",
    "Epoch #170: Loss:1.0693, Accuracy:0.4127, Validation Loss:1.0771, Validation Accuracy:0.3662\n",
    "Epoch #171: Loss:1.0695, Accuracy:0.4094, Validation Loss:1.0768, Validation Accuracy:0.3810\n",
    "Epoch #172: Loss:1.0695, Accuracy:0.4168, Validation Loss:1.0769, Validation Accuracy:0.3678\n",
    "Epoch #173: Loss:1.0696, Accuracy:0.4144, Validation Loss:1.0771, Validation Accuracy:0.3744\n",
    "Epoch #174: Loss:1.0706, Accuracy:0.4025, Validation Loss:1.0766, Validation Accuracy:0.3629\n",
    "Epoch #175: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0749, Validation Accuracy:0.3711\n",
    "Epoch #176: Loss:1.0726, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3810\n",
    "Epoch #177: Loss:1.0725, Accuracy:0.3873, Validation Loss:1.0760, Validation Accuracy:0.3892\n",
    "Epoch #178: Loss:1.0710, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3711\n",
    "Epoch #179: Loss:1.0708, Accuracy:0.4119, Validation Loss:1.0733, Validation Accuracy:0.3662\n",
    "Epoch #180: Loss:1.0705, Accuracy:0.4016, Validation Loss:1.0736, Validation Accuracy:0.3678\n",
    "Epoch #181: Loss:1.0697, Accuracy:0.4094, Validation Loss:1.0735, Validation Accuracy:0.3727\n",
    "Epoch #182: Loss:1.0702, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.3711\n",
    "Epoch #183: Loss:1.0700, Accuracy:0.4123, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0701, Accuracy:0.4111, Validation Loss:1.0728, Validation Accuracy:0.3859\n",
    "Epoch #185: Loss:1.0694, Accuracy:0.4099, Validation Loss:1.0731, Validation Accuracy:0.3760\n",
    "Epoch #186: Loss:1.0696, Accuracy:0.4099, Validation Loss:1.0744, Validation Accuracy:0.3793\n",
    "Epoch #187: Loss:1.0695, Accuracy:0.4119, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #188: Loss:1.0691, Accuracy:0.4111, Validation Loss:1.0767, Validation Accuracy:0.3777\n",
    "Epoch #189: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0754, Validation Accuracy:0.3793\n",
    "Epoch #190: Loss:1.0735, Accuracy:0.4033, Validation Loss:1.0778, Validation Accuracy:0.3957\n",
    "Epoch #191: Loss:1.0698, Accuracy:0.4057, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #192: Loss:1.0716, Accuracy:0.3971, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #193: Loss:1.0711, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #194: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3629\n",
    "Epoch #195: Loss:1.0724, Accuracy:0.3934, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #196: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #197: Loss:1.0722, Accuracy:0.3971, Validation Loss:1.0749, Validation Accuracy:0.3678\n",
    "Epoch #198: Loss:1.0723, Accuracy:0.3914, Validation Loss:1.0748, Validation Accuracy:0.3662\n",
    "Epoch #199: Loss:1.0717, Accuracy:0.3938, Validation Loss:1.0754, Validation Accuracy:0.3842\n",
    "Epoch #200: Loss:1.0718, Accuracy:0.3984, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #201: Loss:1.0718, Accuracy:0.3967, Validation Loss:1.0749, Validation Accuracy:0.3777\n",
    "Epoch #202: Loss:1.0725, Accuracy:0.3877, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #203: Loss:1.0719, Accuracy:0.3938, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #204: Loss:1.0727, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #205: Loss:1.0734, Accuracy:0.3840, Validation Loss:1.0752, Validation Accuracy:0.3695\n",
    "Epoch #206: Loss:1.0722, Accuracy:0.3819, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #207: Loss:1.0734, Accuracy:0.3979, Validation Loss:1.0767, Validation Accuracy:0.3941\n",
    "Epoch #208: Loss:1.0724, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #209: Loss:1.0743, Accuracy:0.3713, Validation Loss:1.0759, Validation Accuracy:0.3645\n",
    "Epoch #210: Loss:1.0722, Accuracy:0.3889, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #211: Loss:1.0723, Accuracy:0.3996, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #212: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #213: Loss:1.0717, Accuracy:0.4000, Validation Loss:1.0753, Validation Accuracy:0.3892\n",
    "Epoch #214: Loss:1.0720, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #215: Loss:1.0716, Accuracy:0.3967, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #216: Loss:1.0715, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #217: Loss:1.0716, Accuracy:0.4004, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #218: Loss:1.0719, Accuracy:0.3918, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #219: Loss:1.0717, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #220: Loss:1.0716, Accuracy:0.3984, Validation Loss:1.0752, Validation Accuracy:0.3908\n",
    "Epoch #221: Loss:1.0719, Accuracy:0.3988, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #222: Loss:1.0716, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #223: Loss:1.0719, Accuracy:0.3959, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #224: Loss:1.0716, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.3924\n",
    "Epoch #225: Loss:1.0719, Accuracy:0.3988, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #226: Loss:1.0713, Accuracy:0.3938, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #227: Loss:1.0719, Accuracy:0.3938, Validation Loss:1.0766, Validation Accuracy:0.3957\n",
    "Epoch #228: Loss:1.0729, Accuracy:0.3979, Validation Loss:1.0757, Validation Accuracy:0.3924\n",
    "Epoch #229: Loss:1.0720, Accuracy:0.3930, Validation Loss:1.0755, Validation Accuracy:0.3924\n",
    "Epoch #230: Loss:1.0721, Accuracy:0.3926, Validation Loss:1.0761, Validation Accuracy:0.3892\n",
    "Epoch #231: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0765, Validation Accuracy:0.3859\n",
    "Epoch #232: Loss:1.0720, Accuracy:0.3938, Validation Loss:1.0760, Validation Accuracy:0.3859\n",
    "Epoch #233: Loss:1.0722, Accuracy:0.3959, Validation Loss:1.0764, Validation Accuracy:0.3859\n",
    "Epoch #234: Loss:1.0720, Accuracy:0.3955, Validation Loss:1.0761, Validation Accuracy:0.3842\n",
    "Epoch #235: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0762, Validation Accuracy:0.3826\n",
    "Epoch #236: Loss:1.0719, Accuracy:0.3959, Validation Loss:1.0755, Validation Accuracy:0.3793\n",
    "Epoch #237: Loss:1.0716, Accuracy:0.4025, Validation Loss:1.0755, Validation Accuracy:0.3826\n",
    "Epoch #238: Loss:1.0720, Accuracy:0.3988, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #239: Loss:1.0715, Accuracy:0.3984, Validation Loss:1.0755, Validation Accuracy:0.3826\n",
    "Epoch #240: Loss:1.0715, Accuracy:0.3971, Validation Loss:1.0759, Validation Accuracy:0.3826\n",
    "Epoch #241: Loss:1.0717, Accuracy:0.3979, Validation Loss:1.0759, Validation Accuracy:0.3842\n",
    "Epoch #242: Loss:1.0711, Accuracy:0.3996, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #243: Loss:1.0719, Accuracy:0.3860, Validation Loss:1.0760, Validation Accuracy:0.3842\n",
    "Epoch #244: Loss:1.0711, Accuracy:0.3984, Validation Loss:1.0761, Validation Accuracy:0.3826\n",
    "Epoch #245: Loss:1.0715, Accuracy:0.3967, Validation Loss:1.0761, Validation Accuracy:0.3875\n",
    "Epoch #246: Loss:1.0713, Accuracy:0.3996, Validation Loss:1.0758, Validation Accuracy:0.3875\n",
    "Epoch #247: Loss:1.0714, Accuracy:0.3988, Validation Loss:1.0760, Validation Accuracy:0.3859\n",
    "Epoch #248: Loss:1.0714, Accuracy:0.4000, Validation Loss:1.0761, Validation Accuracy:0.3842\n",
    "Epoch #249: Loss:1.0714, Accuracy:0.3996, Validation Loss:1.0763, Validation Accuracy:0.3892\n",
    "Epoch #250: Loss:1.0715, Accuracy:0.3938, Validation Loss:1.0767, Validation Accuracy:0.3826\n",
    "Epoch #251: Loss:1.0713, Accuracy:0.3947, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #252: Loss:1.0711, Accuracy:0.3975, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #253: Loss:1.0713, Accuracy:0.3963, Validation Loss:1.0758, Validation Accuracy:0.3875\n",
    "Epoch #254: Loss:1.0711, Accuracy:0.3963, Validation Loss:1.0760, Validation Accuracy:0.3810\n",
    "Epoch #255: Loss:1.0711, Accuracy:0.3963, Validation Loss:1.0758, Validation Accuracy:0.3908\n",
    "Epoch #256: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #257: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0757, Validation Accuracy:0.3826\n",
    "Epoch #258: Loss:1.0713, Accuracy:0.3984, Validation Loss:1.0758, Validation Accuracy:0.3875\n",
    "Epoch #259: Loss:1.0708, Accuracy:0.4045, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #260: Loss:1.0708, Accuracy:0.4025, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #261: Loss:1.0707, Accuracy:0.3963, Validation Loss:1.0757, Validation Accuracy:0.3826\n",
    "Epoch #262: Loss:1.0708, Accuracy:0.4029, Validation Loss:1.0759, Validation Accuracy:0.3842\n",
    "Epoch #263: Loss:1.0707, Accuracy:0.3959, Validation Loss:1.0762, Validation Accuracy:0.3842\n",
    "Epoch #264: Loss:1.0706, Accuracy:0.3922, Validation Loss:1.0756, Validation Accuracy:0.3826\n",
    "Epoch #265: Loss:1.0709, Accuracy:0.3897, Validation Loss:1.0758, Validation Accuracy:0.3924\n",
    "Epoch #266: Loss:1.0710, Accuracy:0.3918, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #267: Loss:1.0706, Accuracy:0.3988, Validation Loss:1.0757, Validation Accuracy:0.3826\n",
    "Epoch #268: Loss:1.0707, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3875\n",
    "Epoch #269: Loss:1.0707, Accuracy:0.3959, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #270: Loss:1.0707, Accuracy:0.3963, Validation Loss:1.0760, Validation Accuracy:0.3842\n",
    "Epoch #271: Loss:1.0702, Accuracy:0.3967, Validation Loss:1.0755, Validation Accuracy:0.3826\n",
    "Epoch #272: Loss:1.0704, Accuracy:0.3910, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #273: Loss:1.0706, Accuracy:0.3943, Validation Loss:1.0764, Validation Accuracy:0.3859\n",
    "Epoch #274: Loss:1.0706, Accuracy:0.4016, Validation Loss:1.0770, Validation Accuracy:0.3875\n",
    "Epoch #275: Loss:1.0706, Accuracy:0.3918, Validation Loss:1.0770, Validation Accuracy:0.3810\n",
    "Epoch #276: Loss:1.0704, Accuracy:0.3955, Validation Loss:1.0765, Validation Accuracy:0.3892\n",
    "Epoch #277: Loss:1.0703, Accuracy:0.3979, Validation Loss:1.0757, Validation Accuracy:0.3859\n",
    "Epoch #278: Loss:1.0702, Accuracy:0.3959, Validation Loss:1.0759, Validation Accuracy:0.3859\n",
    "Epoch #279: Loss:1.0707, Accuracy:0.4025, Validation Loss:1.0762, Validation Accuracy:0.3924\n",
    "Epoch #280: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0756, Validation Accuracy:0.3826\n",
    "Epoch #281: Loss:1.0708, Accuracy:0.3947, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #282: Loss:1.0702, Accuracy:0.3996, Validation Loss:1.0758, Validation Accuracy:0.3842\n",
    "Epoch #283: Loss:1.0700, Accuracy:0.4016, Validation Loss:1.0760, Validation Accuracy:0.3892\n",
    "Epoch #284: Loss:1.0703, Accuracy:0.3971, Validation Loss:1.0768, Validation Accuracy:0.3826\n",
    "Epoch #285: Loss:1.0699, Accuracy:0.3996, Validation Loss:1.0766, Validation Accuracy:0.3892\n",
    "Epoch #286: Loss:1.0699, Accuracy:0.3996, Validation Loss:1.0763, Validation Accuracy:0.3842\n",
    "Epoch #287: Loss:1.0697, Accuracy:0.3975, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #288: Loss:1.0697, Accuracy:0.4025, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #289: Loss:1.0697, Accuracy:0.4021, Validation Loss:1.0761, Validation Accuracy:0.3892\n",
    "Epoch #290: Loss:1.0697, Accuracy:0.3996, Validation Loss:1.0758, Validation Accuracy:0.3842\n",
    "Epoch #291: Loss:1.0698, Accuracy:0.3988, Validation Loss:1.0762, Validation Accuracy:0.3842\n",
    "Epoch #292: Loss:1.0698, Accuracy:0.3992, Validation Loss:1.0766, Validation Accuracy:0.3859\n",
    "Epoch #293: Loss:1.0697, Accuracy:0.3975, Validation Loss:1.0769, Validation Accuracy:0.3908\n",
    "Epoch #294: Loss:1.0705, Accuracy:0.3975, Validation Loss:1.0772, Validation Accuracy:0.3974\n",
    "Epoch #295: Loss:1.0692, Accuracy:0.3959, Validation Loss:1.0768, Validation Accuracy:0.3875\n",
    "Epoch #296: Loss:1.0704, Accuracy:0.3979, Validation Loss:1.0770, Validation Accuracy:0.3875\n",
    "Epoch #297: Loss:1.0690, Accuracy:0.4016, Validation Loss:1.0777, Validation Accuracy:0.3842\n",
    "Epoch #298: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0772, Validation Accuracy:0.3859\n",
    "Epoch #299: Loss:1.0689, Accuracy:0.3963, Validation Loss:1.0770, Validation Accuracy:0.3941\n",
    "Epoch #300: Loss:1.0700, Accuracy:0.3963, Validation Loss:1.0779, Validation Accuracy:0.3892\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07790399, Accuracy:0.3892\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  34  193   0\n",
    "t:01  37  203   0\n",
    "t:03  19  123   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.38      0.15      0.21       227\n",
    "          01       0.39      0.85      0.53       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.26      0.33      0.25       609\n",
    "weighted avg       0.29      0.39      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 17:56:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 31 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.1029643078742943, 1.0930898804186993, 1.087086626853066, 1.0805650618667477, 1.0769309500363855, 1.075575473077583, 1.0748123864234962, 1.074850916079504, 1.0749866461323203, 1.0750711203012953, 1.0748394786430697, 1.0747454170327273, 1.0747503590309757, 1.0747395370198392, 1.0748592262784835, 1.0745145509199947, 1.0743776528510358, 1.074867981799522, 1.074834752552615, 1.074676382913574, 1.0748014058581323, 1.0745733026996231, 1.0745969717138506, 1.0745113904057269, 1.0744554041250194, 1.0745637089943847, 1.074390021255255, 1.0744225154760827, 1.0743339006928192, 1.074360606705614, 1.074334827354193, 1.0745784770483258, 1.0744909940682021, 1.0742400796542615, 1.0746250283737684, 1.0743479953806585, 1.0741767869598564, 1.0746345091335878, 1.07475105020996, 1.074350283650929, 1.0744873876446377, 1.0743485330948102, 1.0742303046882642, 1.0743914758434827, 1.0743966650688785, 1.0744186870766, 1.0743552593174825, 1.0749408417734607, 1.0748772717070305, 1.0748359019728913, 1.074948549074884, 1.0750805918610546, 1.0749358723707778, 1.07463513258447, 1.0742806055275678, 1.0744070354940856, 1.0743757999197798, 1.074442499572616, 1.074288308326834, 1.074149606263109, 1.074277634495389, 1.074263017165837, 1.0742138428445325, 1.074132371222836, 1.073999466371458, 1.0740294593504105, 1.0738926928227366, 1.073928265148783, 1.07383766824193, 1.0739090272358485, 1.0739611387252808, 1.0739942327117293, 1.0740188507023702, 1.0740526409571982, 1.0739701943248754, 1.0738827236767472, 1.0738346778309011, 1.0739783361823296, 1.074251608308313, 1.0737465371443524, 1.0742798448587678, 1.074272871017456, 1.0743947508691372, 1.0744462404736548, 1.074366359287882, 1.0745261271403146, 1.0743368057586093, 1.0744373864923988, 1.0748637940104568, 1.074010719023706, 1.0743018414195145, 1.0741259773768033, 1.0741594865404327, 1.073707402241837, 1.0731575324617584, 1.0735457558154278, 1.0729097717109768, 1.073295611661839, 1.073437379498787, 1.0737805059194956, 1.0739832599762038, 1.0737909170598623, 1.0738164658225424, 1.0736592146759159, 1.073425951849651, 1.0735584346727394, 1.0738709129527677, 1.0737065740406806, 1.0736005760374523, 1.0739821598839094, 1.0739073023224504, 1.0741157404503408, 1.0738898415870854, 1.0733157795638286, 1.0740339389967017, 1.0739167525459001, 1.0735686676842826, 1.0738511987899129, 1.0737938886792788, 1.073463860599474, 1.074428541123965, 1.0736151645923484, 1.0732302918222738, 1.0749900891080082, 1.0747930044415352, 1.0732343169464462, 1.072715172822448, 1.072329756074351, 1.0721293024241632, 1.0722074849264962, 1.072551111478132, 1.0723287135313688, 1.0729354625852237, 1.0725447509089128, 1.0724989297159004, 1.072099921738573, 1.071944906010808, 1.072405969959566, 1.0734538476063895, 1.0737520303632238, 1.0740463510522702, 1.0731387046366099, 1.072910882177807, 1.072410606202625, 1.0729972041886429, 1.0731256002275815, 1.0729424454308496, 1.0738616185431018, 1.0734831263083346, 1.073573701487386, 1.0733271706084704, 1.0740755097619419, 1.0745837921383736, 1.0741872221769762, 1.0739732520725145, 1.0754637872839992, 1.0736155341607205, 1.0733682623833467, 1.0746184716670972, 1.0758315271931915, 1.0760440387945065, 1.0762476014777749, 1.076179139719808, 1.0767658809918683, 1.0765958711235786, 1.0766703841721483, 1.076736793533726, 1.0757839538780927, 1.0766531274040736, 1.0771124290519551, 1.076838873877314, 1.0769295643507357, 1.077108357927482, 1.0765542525963243, 1.0748930503973624, 1.0740853657667664, 1.0760091339621833, 1.0745179079632061, 1.073294266104111, 1.073611461078788, 1.073486679293252, 1.073185393767208, 1.073034419802022, 1.072846234529868, 1.073136972480611, 1.0744479630381016, 1.0746013505509726, 1.0766667166758446, 1.0754367898054702, 1.077765761925082, 1.0742562392662311, 1.0749487414931624, 1.0741831555546602, 1.0743383384494751, 1.0748015509058886, 1.074957407558297, 1.0749197896869702, 1.0748073085775516, 1.0753942315018628, 1.075238293419135, 1.0749440279304492, 1.0746242917817215, 1.0750635924989171, 1.0752080017318475, 1.0752005373511604, 1.074993083825448, 1.076666037241618, 1.0751265156249499, 1.075893301094694, 1.0755401325147531, 1.075564062849837, 1.0751238080668333, 1.0753334807644923, 1.0745482560253299, 1.0743470160636213, 1.0744295871903744, 1.0742859887372096, 1.075054543946177, 1.075532167024409, 1.0751544213647326, 1.0745256749671472, 1.0746823858549246, 1.074541347367423, 1.075235054606483, 1.0753473444720991, 1.0750275149525483, 1.076557915198979, 1.0756618988337776, 1.075474300212265, 1.0761483461398798, 1.07652912410022, 1.0760203100777612, 1.0764342151056174, 1.0761072702204262, 1.0762346571889416, 1.0755321280709629, 1.0754810029454223, 1.075341654332792, 1.0755358591847035, 1.0759057890996948, 1.075911458881422, 1.0755687748465828, 1.0759833027380832, 1.0761110618196685, 1.076107002244207, 1.075779808174409, 1.0760080272145263, 1.0760818989993317, 1.0762661881439008, 1.0767388110873344, 1.0764557516633584, 1.0760745921941421, 1.0758372255538289, 1.0760228620178398, 1.0758472769131213, 1.0757455105460532, 1.0757114178637175, 1.0757801266530855, 1.0756418939881724, 1.0754793389090176, 1.0757301178667542, 1.075941806160562, 1.076162996159007, 1.075570349035592, 1.0758363235564459, 1.0760204320274942, 1.0756772220232609, 1.0753276536030134, 1.075947520768114, 1.0759721078308933, 1.075534853441962, 1.0755127423698287, 1.0763979590389332, 1.0770150726456165, 1.077006971307576, 1.0765016182694334, 1.0757355860301427, 1.0759206667713734, 1.076238287885005, 1.0756492544277547, 1.0756290054869377, 1.0757799612477494, 1.0760478807004605, 1.0768293138403806, 1.0766160785662522, 1.0762567858782113, 1.0766546689035075, 1.0768182641766928, 1.076137744342948, 1.0758329068107166, 1.076200125056926, 1.0766314444283547, 1.0768693256847963, 1.0771915754073946, 1.0768341237101062, 1.0769863688495556, 1.0776829502265441, 1.077249946265385, 1.0769620344948103, 1.0779041535357146], 'val_acc': [0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3924466327004049, 0.40558292189450884, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3908045965756102, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3990147771995839, 0.4022988494491734, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39244663250465894, 0.39408866882519966, 0.3924466327004049, 0.39408866882519966, 0.39408866882519966, 0.3957307049499944, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3957307049499944, 0.3957307049499944, 0.39408866882519966, 0.39408866882519966, 0.37766830747937924, 0.38095237953322275, 0.3957307049499944, 0.39408866882519966, 0.3957307049499944, 0.3957307049499944, 0.3957307049499944, 0.3957307049499944, 0.3957307049499944, 0.37602627135458444, 0.39408866882519966, 0.3957307049499944, 0.3957307049499944, 0.39408866882519966, 0.38095237963109574, 0.3908045965756102, 0.39408866882519966, 0.3924466327004049, 0.38916256015719647, 0.39408866882519966, 0.3957307049499944, 0.3957307049499944, 0.3957307049499944, 0.3678160905348648, 0.36288998167111564, 0.3924466327004049, 0.39408866882519966, 0.38916256045081543, 0.3628899820626076, 0.36945812656178656, 0.3924466327004049, 0.37931034321268203, 0.3940886684337077, 0.38916256045081543, 0.3908045963798642, 0.3940886684337077, 0.3776683071857603, 0.3776683071857603, 0.38916256025506946, 0.3711001629802002, 0.37274219920286794, 0.39080459667348316, 0.37766830777299815, 0.37766830777299815, 0.36945812685540547, 0.3711001629802002, 0.3743842352297897, 0.38587848810335296, 0.372742199007122, 0.3711001628823273, 0.3809523793374768, 0.379310343310555, 0.39080459667348316, 0.3891625600593235, 0.3825944155601445, 0.36781609033911883, 0.3825944155601445, 0.3628899818668616, 0.3612479460356858, 0.39244663221104, 0.3743842349361708, 0.39737274117266214, 0.3825944151686526, 0.3628899817689886, 0.3990147772974569, 0.3924466319174211, 0.3990147771017109, 0.3891625597657046, 0.39737274117266214, 0.3924466327004049, 0.3579638732967314, 0.372742199007122, 0.36781609014337285, 0.3760262712567115, 0.37931034321268203, 0.38752052383665575, 0.3825944155601445, 0.379310343310555, 0.3908045964777372, 0.38587848800547997, 0.3875205243260207, 0.3990147771017109, 0.3990147772974569, 0.3743842352297897, 0.3743842352297897, 0.37274219920286794, 0.3842364520764312, 0.3891625608423073, 0.3842364518806852, 0.36453201808952934, 0.379310343310555, 0.38423645168493925, 0.39244663221104, 0.38916256064656135, 0.3940886687273267, 0.38095237943534976, 0.37274219871350306, 0.38423645158706626, 0.36781609033911883, 0.3711001626865813, 0.3743842349361708, 0.36453201808952934, 0.3743842349361708, 0.3661740542143241, 0.3628899819647346, 0.38423645158706626, 0.3743842349361708, 0.3661740542143241, 0.38095237963109574, 0.36781609033911883, 0.3743842349361708, 0.3628899818668616, 0.37110016249083533, 0.3809523791417308, 0.38916256045081543, 0.37110016249083533, 0.3661740539207051, 0.36781609014337285, 0.37274219871350306, 0.3711001625887083, 0.39573070455850246, 0.385878487809734, 0.37602627096309255, 0.379310343310555, 0.3743842349361708, 0.37766830738150625, 0.379310343408428, 0.39573070475424843, 0.3891625600593235, 0.38752052393452874, 0.38752052373878276, 0.3628899819647346, 0.38916256045081543, 0.38916256045081543, 0.36781609033911883, 0.36617405441007006, 0.3842364519785582, 0.3825944157558905, 0.3776683070878873, 0.38916256015719647, 0.38916256045081543, 0.38916256045081543, 0.3694581263660406, 0.3875205243260207, 0.39408866882519966, 0.39573070475424843, 0.36453201818740233, 0.3908045965756102, 0.39408866882519966, 0.38916256045081543, 0.38916256064656135, 0.3924466327982779, 0.3908045965756102, 0.3908045965756102, 0.3908045965756102, 0.38916256025506946, 0.3940886686294537, 0.3908045965756102, 0.3924466327004049, 0.39244663250465894, 0.3957307048521214, 0.3924466327004049, 0.3924466327004049, 0.39244663289615084, 0.3957307049499944, 0.3924466327004049, 0.3924466327982779, 0.38916256054868836, 0.38587848820122594, 0.3858784882990989, 0.38587848810335296, 0.38423645217430413, 0.3825944159516364, 0.379310343408428, 0.38259441585376347, 0.38095237963109574, 0.3825944159516364, 0.3825944159516364, 0.3842364520764312, 0.3875205242281477, 0.38423645178281224, 0.3825944156580175, 0.3875205243260207, 0.3875205243260207, 0.38587848820122594, 0.3842364518806852, 0.38916256035294244, 0.3825944157558905, 0.39408866882519966, 0.3924466327004049, 0.3875205243260207, 0.38095237943534976, 0.3908045965756102, 0.3908045965756102, 0.3825944155601445, 0.3875205243260207, 0.38587848800547997, 0.38587848800547997, 0.3825944155601445, 0.3842364518806852, 0.3842364518806852, 0.3825944155601445, 0.3924466327004049, 0.3825944155601445, 0.3825944155601445, 0.3875205242281477, 0.3875205241302747, 0.3842364518806852, 0.3825944155601445, 0.3908045965756102, 0.385878487809734, 0.38752052393452874, 0.38095237943534976, 0.38916256035294244, 0.385878487809734, 0.385878487809734, 0.3924466327004049, 0.3825944155601445, 0.39244663260253193, 0.38423645168493925, 0.38916256035294244, 0.3825944155601445, 0.38916256035294244, 0.38423645168493925, 0.379310343310555, 0.379310343604174, 0.38916256035294244, 0.38423645168493925, 0.38423645158706626, 0.38587848800547997, 0.3908045964777372, 0.39737274107478915, 0.3875205241302747, 0.3875205241302747, 0.38423645178281224, 0.385878487809734, 0.3940886687273267, 0.38916256035294244], 'loss': [1.1131702321755568, 1.0981193272729675, 1.0899871460466168, 1.083742035340969, 1.07883203582842, 1.0756169561977504, 1.0745806809078742, 1.0743833190361822, 1.0745326153306745, 1.0749238368177316, 1.074858360809467, 1.0744943393084547, 1.0742400207069132, 1.0742719109298267, 1.0745043231231721, 1.074214407991335, 1.0742138918420372, 1.0741534048037362, 1.0740256866635238, 1.0741257343938464, 1.074054085155777, 1.0741129909942282, 1.0742217755170818, 1.0740556962681012, 1.0739338725009737, 1.0739106816188022, 1.0738640240820037, 1.0741049333274733, 1.0741901459390377, 1.0740172368545062, 1.0736945065629555, 1.0736565803355504, 1.074086348672667, 1.073685533750718, 1.0738398381571994, 1.073867405954083, 1.0738080283210019, 1.0737911345288005, 1.0741257174548673, 1.0737368031938463, 1.0737813775299512, 1.0740227694873692, 1.0738293252196889, 1.0738360501412738, 1.0736315813887045, 1.0736888770939632, 1.0739404720936958, 1.0740339492135957, 1.0741009965324793, 1.0741743075529409, 1.0739846057715603, 1.0741049002328202, 1.073861314873431, 1.0740923156973272, 1.0744359553961784, 1.0741420766411376, 1.0739321047275707, 1.0738407900201221, 1.0737516420822613, 1.0737178414754065, 1.0738337135412854, 1.0736824905603084, 1.073607191461802, 1.073722931932375, 1.0736992686191378, 1.0736160477084056, 1.0737446090279175, 1.0736504891080287, 1.073597265464814, 1.0735042590624988, 1.0734742629699394, 1.0736942950215427, 1.073363375712714, 1.0732613501852297, 1.073347435350046, 1.0733255991455954, 1.0734580230908717, 1.073301020833746, 1.0736034648863932, 1.0731769226904522, 1.0732645624227348, 1.073840958969304, 1.073719352520467, 1.0734880074338502, 1.073257258638464, 1.0734547129401926, 1.073968136873578, 1.0735109708637183, 1.0741552351926142, 1.073479260311479, 1.0731252221845748, 1.073007089597244, 1.0736436621609164, 1.0732311353546393, 1.0730689745419324, 1.072874849728735, 1.0728164591094063, 1.0728986266212541, 1.072833464424713, 1.0725482524542838, 1.0726953833499728, 1.0724903905660954, 1.0726693348228564, 1.0723877740836487, 1.0724150097100886, 1.07228830022244, 1.0721357822418214, 1.072076709559321, 1.0718398112291183, 1.0717597810149928, 1.071660523199203, 1.0719165826969812, 1.071731328523624, 1.0713177721603206, 1.0725660544890887, 1.071501617167275, 1.071491126700838, 1.0712867863613966, 1.0714133322361314, 1.0711884431525667, 1.0716495236821733, 1.070528190434591, 1.0708904674655357, 1.0724559915628766, 1.0727950055006839, 1.0726078967049382, 1.072624527355484, 1.0728840251722864, 1.0721812203679486, 1.0727431866667354, 1.0720151641775206, 1.071784696441901, 1.0717661852219755, 1.071237695094741, 1.0714681043761956, 1.0703173799926007, 1.0700980942352107, 1.0701204133963929, 1.0707739338492956, 1.071707328731764, 1.0716206140341944, 1.0712511536032268, 1.071661867351258, 1.0702549387052565, 1.0707396233595863, 1.0697130373615993, 1.07026171356256, 1.0697740286037907, 1.0692842977982038, 1.0702175374393346, 1.0692784357854228, 1.0695528776004328, 1.0688089072092357, 1.0686586287476934, 1.0691570971780733, 1.068173476900653, 1.0687837798982185, 1.0700122146880602, 1.0695025115042496, 1.0692711255878393, 1.0695083578509228, 1.0702560467396918, 1.0712591849802946, 1.0700439192675957, 1.069715534930846, 1.0693662472574128, 1.0694679201016435, 1.0700079420019224, 1.069106668270589, 1.0692712643797637, 1.0695120462401937, 1.0694705689467443, 1.0696179970578736, 1.0706075309483178, 1.0711366959910618, 1.0726049368386397, 1.072530128138266, 1.0709633640684877, 1.0707661973867084, 1.0704970590387772, 1.0697317592906757, 1.0701722059896104, 1.0700049044904767, 1.0701377496582283, 1.0693935080964951, 1.069560580038192, 1.0695432727586562, 1.069125443464432, 1.0734289912472514, 1.0734568208150062, 1.0697887041730312, 1.0716219295956027, 1.0711483435464346, 1.0729303108348494, 1.0723963253796713, 1.073493339246793, 1.0721584312724872, 1.07228324349656, 1.0717140698579792, 1.0718429852804854, 1.0718130600281075, 1.0724922244798476, 1.0719151762231909, 1.0727117927657015, 1.0734168271019717, 1.0722093604183784, 1.0734418682004392, 1.0723559806479077, 1.0742893525462376, 1.0721528371011941, 1.0723475003389362, 1.0719870416045922, 1.0716885810515229, 1.0720254647413563, 1.0716076006879551, 1.0715082711018087, 1.0716010544089567, 1.0718663110380544, 1.0716834972036937, 1.0715738093338953, 1.071904487482576, 1.07163237272102, 1.0718648464527953, 1.0716415027818151, 1.0719174457036984, 1.0713004689441814, 1.071919277071708, 1.0729046810334224, 1.0719643017594573, 1.0721083911292606, 1.0722546132438238, 1.0719789479547457, 1.0721683022912278, 1.0720315466181698, 1.0720623708603563, 1.0719470937149236, 1.071593505792794, 1.0719523901322539, 1.0715408426045883, 1.0714656203679236, 1.0716639032598883, 1.0711226884834086, 1.071879729842748, 1.0711345779822348, 1.0714581897371358, 1.071273630894185, 1.0713757199183627, 1.0713680665840604, 1.0714175287947763, 1.0714672855772767, 1.0712604047336618, 1.071081110094607, 1.0713187884501119, 1.0711020154385107, 1.0711126785748304, 1.0708686363036137, 1.0711245283698645, 1.071257799753663, 1.0708243272632545, 1.0708209473004822, 1.0707386616563894, 1.0707837662412891, 1.0706782334149496, 1.0705959713189754, 1.0708962507561246, 1.0710338339423742, 1.0705550718601236, 1.0707378482916516, 1.0706984627662988, 1.070712899231568, 1.0701975561510122, 1.070392844123762, 1.0705662368014608, 1.0705753841928878, 1.0706355675045225, 1.0704005529748342, 1.0703437696002591, 1.070191966287899, 1.0706592399726413, 1.070631558351693, 1.0707773515086398, 1.0701776856024896, 1.0700420428595259, 1.0702987708105445, 1.0699230502518295, 1.0698991030393439, 1.069693341098527, 1.069708349180907, 1.0696766509657278, 1.0697301557666223, 1.0697848145721873, 1.0697606055888307, 1.0697242073944218, 1.070514767174848, 1.069192359334879, 1.0704211038730473, 1.0689834718097162, 1.0701072411860284, 1.0688516064590987, 1.0700222288558616], 'acc': [0.3006160160109737, 0.3942505125392389, 0.39425051250252147, 0.39425051547663414, 0.3942505148891551, 0.39425051547663414, 0.39425051113173704, 0.3942505113642808, 0.3942505146933287, 0.3885010274528723, 0.395071869146163, 0.39425051351837065, 0.39425051449750237, 0.3942505156724605, 0.39425051508498143, 0.39507186636787667, 0.3938398348844516, 0.3942505123434126, 0.3942505113275634, 0.3942505142649586, 0.39425051508498143, 0.39425051391002336, 0.3942505148524376, 0.3942505133225443, 0.39425051250252147, 0.3942505115233897, 0.39425051391002336, 0.39425051195175986, 0.39383983605940975, 0.393018479648312, 0.39383983370949355, 0.39425051328582683, 0.394250514301676, 0.39425051547663414, 0.39794661321923963, 0.40492812978903125, 0.3975359322721219, 0.3934291598488418, 0.3942505133225443, 0.394250513714197, 0.39137577234597176, 0.3995893239607801, 0.3963039031753305, 0.3946611931314214, 0.39383983410114626, 0.39589322496978163, 0.3954825459809274, 0.3938398374669116, 0.39425051547663414, 0.3942505123066951, 0.39425051113173704, 0.39425051351837065, 0.3942505148891551, 0.3942505141058497, 0.3934291586738837, 0.3942505141058497, 0.394250514301676, 0.3942505123434126, 0.39425051171921605, 0.3942505152808078, 0.39425051156010715, 0.39425051273506523, 0.3942505141058497, 0.40780287703204693, 0.4049281335464493, 0.39425051508498143, 0.39425051508498143, 0.3946611928988776, 0.3946611921155722, 0.39466118914145953, 0.3946611925072249, 0.3926078040252231, 0.409856263788329, 0.39507186992946836, 0.3942505141058497, 0.3946611928988776, 0.3971252568081419, 0.3938398374301942, 0.3942505148891551, 0.39507186699207314, 0.3872689957736209, 0.3963039023920251, 0.3917864485932571, 0.39425051328582683, 0.39425051391002336, 0.3938398368794326, 0.39096509296546483, 0.3860369633110642, 0.39507186738372585, 0.39630389941791244, 0.4032854189250993, 0.40657084108133335, 0.3901437393326534, 0.4020533868541952, 0.4139630403606799, 0.40082135521166135, 0.3975359354420609, 0.4045174559283795, 0.4094455863660855, 0.4156057488379782, 0.4078028727605847, 0.4110882972300175, 0.4131416845003437, 0.406981520853493, 0.40451745416594237, 0.40821355465011677, 0.40533881077286643, 0.4078028725647584, 0.40985626081421633, 0.40739219749243105, 0.40328542287834374, 0.4082135539035288, 0.40821355092941614, 0.40780287291969364, 0.40533880779875375, 0.4069815200701876, 0.40780287628545897, 0.4073921960849292, 0.4024640654146794, 0.4073921966724082, 0.4073921992548682, 0.4098562614016954, 0.4086242285474859, 0.4008213534492242, 0.3995893231407573, 0.40082135380415945, 0.3963039025878515, 0.39260780382939675, 0.3954825475475382, 0.3905544143315458, 0.3979466102818444, 0.4008213575615775, 0.39301848164329295, 0.4032854206875364, 0.39835728809574056, 0.4078028727238673, 0.40903490694886113, 0.4131416825420803, 0.4090349069855786, 0.40082135560331406, 0.4028747416986822, 0.40739219510579744, 0.3995893229082135, 0.41149897265728, 0.41067761761696675, 0.41314168610367197, 0.4082135525327444, 0.41355236074762913, 0.40862423171742496, 0.40657084202374766, 0.40985626120586904, 0.4119096506670026, 0.4078028756612625, 0.41314168571201926, 0.41108829488010135, 0.40780287311552, 0.4094455839794519, 0.40328541974512216, 0.4098562628091973, 0.4139630369949145, 0.4119096522703308, 0.39671458138087934, 0.4065708404571369, 0.4094455869535646, 0.4114989736364118, 0.4135523625467837, 0.40123203361303655, 0.41149897445643463, 0.4106776182411632, 0.41273100731064405, 0.40944558358779926, 0.4168377830629721, 0.414373714417158, 0.402464066467246, 0.3979466123135428, 0.3991786427444012, 0.3872689957736209, 0.3991786425485748, 0.4119096496878708, 0.4016427118185854, 0.4094455873452173, 0.39753593563788725, 0.4123203276975933, 0.4110882944517312, 0.40985626081421633, 0.4098562641799817, 0.4119096520745044, 0.4110882936684258, 0.39917864689347193, 0.4032854226825174, 0.40574948760763085, 0.39712525860729647, 0.40082135403670327, 0.39425051508498143, 0.3934291574622082, 0.3983572906782005, 0.3971252570406857, 0.3913757699960556, 0.39383983805439066, 0.39835729048237417, 0.39671457805183147, 0.3876796735875171, 0.3938398362919535, 0.40000000232543786, 0.38398357384993065, 0.3819301827854689, 0.3979466101227355, 0.4004106755986106, 0.3712525689014419, 0.3889117024884821, 0.3995893207541237, 0.400410679551855, 0.40000000212961156, 0.39466119152809315, 0.39671458138087934, 0.4000000011504798, 0.4004106765777423, 0.3917864464391673, 0.394250514301676, 0.39835729146150595, 0.3987679692754021, 0.398767968100444, 0.39589322297480073, 0.39999999837219347, 0.3987679692754021, 0.39383983648777987, 0.3938398368794326, 0.39794661364760975, 0.39301848085998753, 0.3926078050043549, 0.39712525801981746, 0.3938398353128218, 0.3958932216040163, 0.3954825442184903, 0.3946611913322668, 0.3958932247739553, 0.40246406368895965, 0.39876796829627037, 0.3983572908740269, 0.3971252546540521, 0.39794661071021453, 0.3995893213416027, 0.3860369597861899, 0.39835728852411073, 0.3967145791900721, 0.39958932235745187, 0.3987679661421805, 0.40000000095465343, 0.3995893211457764, 0.3938398342969726, 0.39466119051224396, 0.3975359348545819, 0.39630390020121786, 0.3963038996137388, 0.39630390020121786, 0.39958932431571537, 0.3979466132559571, 0.39835728989489516, 0.40451745318681065, 0.4024640642764387, 0.39630390376280955, 0.4028747446727948, 0.39589322418647627, 0.3921971275821114, 0.38973305952377635, 0.39178644702664633, 0.39876796551798405, 0.4004106771652214, 0.3958932261447397, 0.3963039023920251, 0.39671457883513683, 0.3909650931612912, 0.3942505133225443, 0.40164270962777815, 0.39178644820160446, 0.3954825467642328, 0.39794661047767077, 0.3958932259489134, 0.4024640672872688, 0.3979466098901917, 0.39466119035313507, 0.39958932333658365, 0.4016427126018908, 0.39712525582901015, 0.39958932353241, 0.3995893211457764, 0.3975359324679482, 0.4024640672505514, 0.4020533892408289, 0.3995893201666446, 0.3987679684920967, 0.399178644151903, 0.39753593207629556, 0.3975359358337136, 0.3958932235989972, 0.3979466102818444, 0.40164271021525716, 0.4008213563866194, 0.39630390297950413, 0.39630390180454605]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
