{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf79.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 03:41:56 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '1Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['by', 'eg', 'ck', 'ek', 'eb', 'sg', 'ib', 'aa', 'yd', 'ds', 'mb', 'sk', 'ce', 'eo', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000207BE32D278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000207BBD86EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7055, Accuracy:0.0690, Validation Loss:2.7014, Validation Accuracy:0.0854\n",
    "Epoch #2: Loss:2.6981, Accuracy:0.0949, Validation Loss:2.6933, Validation Accuracy:0.0920\n",
    "Epoch #3: Loss:2.6904, Accuracy:0.0928, Validation Loss:2.6906, Validation Accuracy:0.0887\n",
    "Epoch #4: Loss:2.6879, Accuracy:0.0801, Validation Loss:2.6871, Validation Accuracy:0.0837\n",
    "Epoch #5: Loss:2.6845, Accuracy:0.0830, Validation Loss:2.6814, Validation Accuracy:0.0903\n",
    "Epoch #6: Loss:2.6783, Accuracy:0.0916, Validation Loss:2.6750, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6727, Accuracy:0.0998, Validation Loss:2.6692, Validation Accuracy:0.1067\n",
    "Epoch #8: Loss:2.6642, Accuracy:0.1023, Validation Loss:2.6585, Validation Accuracy:0.1182\n",
    "Epoch #9: Loss:2.6515, Accuracy:0.1248, Validation Loss:2.6472, Validation Accuracy:0.1429\n",
    "Epoch #10: Loss:2.6441, Accuracy:0.1351, Validation Loss:2.6412, Validation Accuracy:0.1445\n",
    "Epoch #11: Loss:2.6354, Accuracy:0.1363, Validation Loss:2.6278, Validation Accuracy:0.1593\n",
    "Epoch #12: Loss:2.6187, Accuracy:0.1569, Validation Loss:2.6107, Validation Accuracy:0.1626\n",
    "Epoch #13: Loss:2.6010, Accuracy:0.1639, Validation Loss:2.5946, Validation Accuracy:0.1642\n",
    "Epoch #14: Loss:2.5829, Accuracy:0.1602, Validation Loss:2.5799, Validation Accuracy:0.1626\n",
    "Epoch #15: Loss:2.5706, Accuracy:0.1622, Validation Loss:2.5632, Validation Accuracy:0.1609\n",
    "Epoch #16: Loss:2.5534, Accuracy:0.1622, Validation Loss:2.5527, Validation Accuracy:0.1609\n",
    "Epoch #17: Loss:2.5434, Accuracy:0.1647, Validation Loss:2.5474, Validation Accuracy:0.1609\n",
    "Epoch #18: Loss:2.5375, Accuracy:0.1606, Validation Loss:2.5356, Validation Accuracy:0.1691\n",
    "Epoch #19: Loss:2.5251, Accuracy:0.1667, Validation Loss:2.5521, Validation Accuracy:0.1576\n",
    "Epoch #20: Loss:2.5244, Accuracy:0.1614, Validation Loss:2.5280, Validation Accuracy:0.1593\n",
    "Epoch #21: Loss:2.5098, Accuracy:0.1692, Validation Loss:2.5152, Validation Accuracy:0.1642\n",
    "Epoch #22: Loss:2.5045, Accuracy:0.1688, Validation Loss:2.5092, Validation Accuracy:0.1642\n",
    "Epoch #23: Loss:2.4968, Accuracy:0.1651, Validation Loss:2.5009, Validation Accuracy:0.1511\n",
    "Epoch #24: Loss:2.4881, Accuracy:0.1663, Validation Loss:2.5018, Validation Accuracy:0.1494\n",
    "Epoch #25: Loss:2.4846, Accuracy:0.1647, Validation Loss:2.4986, Validation Accuracy:0.1494\n",
    "Epoch #26: Loss:2.4826, Accuracy:0.1676, Validation Loss:2.4916, Validation Accuracy:0.1576\n",
    "Epoch #27: Loss:2.4764, Accuracy:0.1663, Validation Loss:2.4935, Validation Accuracy:0.1642\n",
    "Epoch #28: Loss:2.4888, Accuracy:0.1626, Validation Loss:2.4904, Validation Accuracy:0.1642\n",
    "Epoch #29: Loss:2.4804, Accuracy:0.1704, Validation Loss:2.4847, Validation Accuracy:0.1675\n",
    "Epoch #30: Loss:2.4777, Accuracy:0.1667, Validation Loss:2.4878, Validation Accuracy:0.1658\n",
    "Epoch #31: Loss:2.4716, Accuracy:0.1758, Validation Loss:2.4847, Validation Accuracy:0.1658\n",
    "Epoch #32: Loss:2.4684, Accuracy:0.1737, Validation Loss:2.4831, Validation Accuracy:0.1724\n",
    "Epoch #33: Loss:2.4666, Accuracy:0.1684, Validation Loss:2.4818, Validation Accuracy:0.1593\n",
    "Epoch #34: Loss:2.4653, Accuracy:0.1717, Validation Loss:2.4801, Validation Accuracy:0.1642\n",
    "Epoch #35: Loss:2.4619, Accuracy:0.1713, Validation Loss:2.4796, Validation Accuracy:0.1642\n",
    "Epoch #36: Loss:2.4612, Accuracy:0.1708, Validation Loss:2.4804, Validation Accuracy:0.1593\n",
    "Epoch #37: Loss:2.4608, Accuracy:0.1692, Validation Loss:2.4797, Validation Accuracy:0.1626\n",
    "Epoch #38: Loss:2.4589, Accuracy:0.1725, Validation Loss:2.4767, Validation Accuracy:0.1691\n",
    "Epoch #39: Loss:2.4567, Accuracy:0.1725, Validation Loss:2.4767, Validation Accuracy:0.1691\n",
    "Epoch #40: Loss:2.4566, Accuracy:0.1700, Validation Loss:2.4748, Validation Accuracy:0.1675\n",
    "Epoch #41: Loss:2.4550, Accuracy:0.1713, Validation Loss:2.4761, Validation Accuracy:0.1741\n",
    "Epoch #42: Loss:2.4561, Accuracy:0.1745, Validation Loss:2.4743, Validation Accuracy:0.1790\n",
    "Epoch #43: Loss:2.4550, Accuracy:0.1717, Validation Loss:2.4752, Validation Accuracy:0.1691\n",
    "Epoch #44: Loss:2.4525, Accuracy:0.1758, Validation Loss:2.4736, Validation Accuracy:0.1773\n",
    "Epoch #45: Loss:2.4524, Accuracy:0.1762, Validation Loss:2.4752, Validation Accuracy:0.1773\n",
    "Epoch #46: Loss:2.4500, Accuracy:0.1774, Validation Loss:2.4735, Validation Accuracy:0.1741\n",
    "Epoch #47: Loss:2.4488, Accuracy:0.1778, Validation Loss:2.4727, Validation Accuracy:0.1708\n",
    "Epoch #48: Loss:2.4480, Accuracy:0.1786, Validation Loss:2.4719, Validation Accuracy:0.1724\n",
    "Epoch #49: Loss:2.4464, Accuracy:0.1803, Validation Loss:2.4721, Validation Accuracy:0.1708\n",
    "Epoch #50: Loss:2.4461, Accuracy:0.1786, Validation Loss:2.4711, Validation Accuracy:0.1691\n",
    "Epoch #51: Loss:2.4475, Accuracy:0.1766, Validation Loss:2.4703, Validation Accuracy:0.1675\n",
    "Epoch #52: Loss:2.4467, Accuracy:0.1741, Validation Loss:2.4724, Validation Accuracy:0.1757\n",
    "Epoch #53: Loss:2.4465, Accuracy:0.1754, Validation Loss:2.4700, Validation Accuracy:0.1741\n",
    "Epoch #54: Loss:2.4467, Accuracy:0.1696, Validation Loss:2.4715, Validation Accuracy:0.1790\n",
    "Epoch #55: Loss:2.4459, Accuracy:0.1680, Validation Loss:2.4697, Validation Accuracy:0.1708\n",
    "Epoch #56: Loss:2.4455, Accuracy:0.1749, Validation Loss:2.4682, Validation Accuracy:0.1724\n",
    "Epoch #57: Loss:2.4463, Accuracy:0.1741, Validation Loss:2.4695, Validation Accuracy:0.1757\n",
    "Epoch #58: Loss:2.4460, Accuracy:0.1708, Validation Loss:2.4680, Validation Accuracy:0.1724\n",
    "Epoch #59: Loss:2.4464, Accuracy:0.1713, Validation Loss:2.4677, Validation Accuracy:0.1790\n",
    "Epoch #60: Loss:2.4443, Accuracy:0.1741, Validation Loss:2.4690, Validation Accuracy:0.1741\n",
    "Epoch #61: Loss:2.4436, Accuracy:0.1786, Validation Loss:2.4699, Validation Accuracy:0.1773\n",
    "Epoch #62: Loss:2.4430, Accuracy:0.1721, Validation Loss:2.4692, Validation Accuracy:0.1773\n",
    "Epoch #63: Loss:2.4430, Accuracy:0.1745, Validation Loss:2.4683, Validation Accuracy:0.1757\n",
    "Epoch #64: Loss:2.4428, Accuracy:0.1704, Validation Loss:2.4685, Validation Accuracy:0.1823\n",
    "Epoch #65: Loss:2.4411, Accuracy:0.1729, Validation Loss:2.4683, Validation Accuracy:0.1806\n",
    "Epoch #66: Loss:2.4408, Accuracy:0.1745, Validation Loss:2.4665, Validation Accuracy:0.1823\n",
    "Epoch #67: Loss:2.4406, Accuracy:0.1799, Validation Loss:2.4664, Validation Accuracy:0.1790\n",
    "Epoch #68: Loss:2.4394, Accuracy:0.1741, Validation Loss:2.4678, Validation Accuracy:0.1757\n",
    "Epoch #69: Loss:2.4387, Accuracy:0.1791, Validation Loss:2.4658, Validation Accuracy:0.1790\n",
    "Epoch #70: Loss:2.4380, Accuracy:0.1799, Validation Loss:2.4681, Validation Accuracy:0.1823\n",
    "Epoch #71: Loss:2.4383, Accuracy:0.1782, Validation Loss:2.4651, Validation Accuracy:0.1823\n",
    "Epoch #72: Loss:2.4382, Accuracy:0.1766, Validation Loss:2.4658, Validation Accuracy:0.1741\n",
    "Epoch #73: Loss:2.4372, Accuracy:0.1828, Validation Loss:2.4674, Validation Accuracy:0.1741\n",
    "Epoch #74: Loss:2.4379, Accuracy:0.1844, Validation Loss:2.4674, Validation Accuracy:0.1757\n",
    "Epoch #75: Loss:2.4377, Accuracy:0.1836, Validation Loss:2.4662, Validation Accuracy:0.1806\n",
    "Epoch #76: Loss:2.4379, Accuracy:0.1799, Validation Loss:2.4686, Validation Accuracy:0.1790\n",
    "Epoch #77: Loss:2.4385, Accuracy:0.1774, Validation Loss:2.4689, Validation Accuracy:0.1856\n",
    "Epoch #78: Loss:2.4386, Accuracy:0.1762, Validation Loss:2.4653, Validation Accuracy:0.1823\n",
    "Epoch #79: Loss:2.4412, Accuracy:0.1791, Validation Loss:2.4644, Validation Accuracy:0.1724\n",
    "Epoch #80: Loss:2.4409, Accuracy:0.1713, Validation Loss:2.4635, Validation Accuracy:0.1790\n",
    "Epoch #81: Loss:2.4393, Accuracy:0.1803, Validation Loss:2.4613, Validation Accuracy:0.1823\n",
    "Epoch #82: Loss:2.4391, Accuracy:0.1749, Validation Loss:2.4624, Validation Accuracy:0.1823\n",
    "Epoch #83: Loss:2.4353, Accuracy:0.1815, Validation Loss:2.4611, Validation Accuracy:0.1708\n",
    "Epoch #84: Loss:2.4343, Accuracy:0.1799, Validation Loss:2.4622, Validation Accuracy:0.1823\n",
    "Epoch #85: Loss:2.4325, Accuracy:0.1807, Validation Loss:2.4599, Validation Accuracy:0.1691\n",
    "Epoch #86: Loss:2.4339, Accuracy:0.1819, Validation Loss:2.4599, Validation Accuracy:0.1888\n",
    "Epoch #87: Loss:2.4356, Accuracy:0.1807, Validation Loss:2.4581, Validation Accuracy:0.1839\n",
    "Epoch #88: Loss:2.4378, Accuracy:0.1766, Validation Loss:2.4587, Validation Accuracy:0.1856\n",
    "Epoch #89: Loss:2.4356, Accuracy:0.1766, Validation Loss:2.4628, Validation Accuracy:0.1823\n",
    "Epoch #90: Loss:2.4343, Accuracy:0.1778, Validation Loss:2.4611, Validation Accuracy:0.1773\n",
    "Epoch #91: Loss:2.4363, Accuracy:0.1762, Validation Loss:2.4617, Validation Accuracy:0.1872\n",
    "Epoch #92: Loss:2.4367, Accuracy:0.1729, Validation Loss:2.4602, Validation Accuracy:0.1839\n",
    "Epoch #93: Loss:2.4354, Accuracy:0.1782, Validation Loss:2.4593, Validation Accuracy:0.1839\n",
    "Epoch #94: Loss:2.4386, Accuracy:0.1766, Validation Loss:2.4603, Validation Accuracy:0.1773\n",
    "Epoch #95: Loss:2.4385, Accuracy:0.1741, Validation Loss:2.4609, Validation Accuracy:0.1823\n",
    "Epoch #96: Loss:2.4371, Accuracy:0.1745, Validation Loss:2.4627, Validation Accuracy:0.1823\n",
    "Epoch #97: Loss:2.4358, Accuracy:0.1754, Validation Loss:2.4616, Validation Accuracy:0.1806\n",
    "Epoch #98: Loss:2.4365, Accuracy:0.1729, Validation Loss:2.4607, Validation Accuracy:0.1823\n",
    "Epoch #99: Loss:2.4371, Accuracy:0.1721, Validation Loss:2.4601, Validation Accuracy:0.1806\n",
    "Epoch #100: Loss:2.4369, Accuracy:0.1733, Validation Loss:2.4608, Validation Accuracy:0.1823\n",
    "Epoch #101: Loss:2.4366, Accuracy:0.1733, Validation Loss:2.4607, Validation Accuracy:0.1790\n",
    "Epoch #102: Loss:2.4364, Accuracy:0.1749, Validation Loss:2.4610, Validation Accuracy:0.1790\n",
    "Epoch #103: Loss:2.4364, Accuracy:0.1708, Validation Loss:2.4612, Validation Accuracy:0.1773\n",
    "Epoch #104: Loss:2.4372, Accuracy:0.1721, Validation Loss:2.4593, Validation Accuracy:0.1790\n",
    "Epoch #105: Loss:2.4362, Accuracy:0.1745, Validation Loss:2.4574, Validation Accuracy:0.1757\n",
    "Epoch #106: Loss:2.4354, Accuracy:0.1741, Validation Loss:2.4620, Validation Accuracy:0.1757\n",
    "Epoch #107: Loss:2.4359, Accuracy:0.1741, Validation Loss:2.4560, Validation Accuracy:0.1773\n",
    "Epoch #108: Loss:2.4346, Accuracy:0.1754, Validation Loss:2.4606, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.4449, Accuracy:0.1696, Validation Loss:2.4960, Validation Accuracy:0.1741\n",
    "Epoch #110: Loss:2.4693, Accuracy:0.1696, Validation Loss:2.4689, Validation Accuracy:0.1675\n",
    "Epoch #111: Loss:2.4475, Accuracy:0.1791, Validation Loss:2.4809, Validation Accuracy:0.1675\n",
    "Epoch #112: Loss:2.4498, Accuracy:0.1770, Validation Loss:2.4666, Validation Accuracy:0.1675\n",
    "Epoch #113: Loss:2.4441, Accuracy:0.1819, Validation Loss:2.4658, Validation Accuracy:0.1757\n",
    "Epoch #114: Loss:2.4386, Accuracy:0.1795, Validation Loss:2.4749, Validation Accuracy:0.1691\n",
    "Epoch #115: Loss:2.4386, Accuracy:0.1791, Validation Loss:2.4659, Validation Accuracy:0.1724\n",
    "Epoch #116: Loss:2.4385, Accuracy:0.1832, Validation Loss:2.4646, Validation Accuracy:0.1773\n",
    "Epoch #117: Loss:2.4365, Accuracy:0.1811, Validation Loss:2.4689, Validation Accuracy:0.1773\n",
    "Epoch #118: Loss:2.4373, Accuracy:0.1778, Validation Loss:2.4640, Validation Accuracy:0.1741\n",
    "Epoch #119: Loss:2.4385, Accuracy:0.1795, Validation Loss:2.4626, Validation Accuracy:0.1724\n",
    "Epoch #120: Loss:2.4385, Accuracy:0.1823, Validation Loss:2.4671, Validation Accuracy:0.1757\n",
    "Epoch #121: Loss:2.4393, Accuracy:0.1795, Validation Loss:2.4663, Validation Accuracy:0.1741\n",
    "Epoch #122: Loss:2.4369, Accuracy:0.1799, Validation Loss:2.4663, Validation Accuracy:0.1741\n",
    "Epoch #123: Loss:2.4364, Accuracy:0.1803, Validation Loss:2.4682, Validation Accuracy:0.1741\n",
    "Epoch #124: Loss:2.4372, Accuracy:0.1803, Validation Loss:2.4680, Validation Accuracy:0.1741\n",
    "Epoch #125: Loss:2.4357, Accuracy:0.1815, Validation Loss:2.4656, Validation Accuracy:0.1741\n",
    "Epoch #126: Loss:2.4364, Accuracy:0.1848, Validation Loss:2.4692, Validation Accuracy:0.1741\n",
    "Epoch #127: Loss:2.4352, Accuracy:0.1795, Validation Loss:2.4671, Validation Accuracy:0.1724\n",
    "Epoch #128: Loss:2.4352, Accuracy:0.1791, Validation Loss:2.4663, Validation Accuracy:0.1724\n",
    "Epoch #129: Loss:2.4338, Accuracy:0.1791, Validation Loss:2.4673, Validation Accuracy:0.1724\n",
    "Epoch #130: Loss:2.4331, Accuracy:0.1803, Validation Loss:2.4638, Validation Accuracy:0.1691\n",
    "Epoch #131: Loss:2.4333, Accuracy:0.1799, Validation Loss:2.4649, Validation Accuracy:0.1757\n",
    "Epoch #132: Loss:2.4328, Accuracy:0.1758, Validation Loss:2.4661, Validation Accuracy:0.1724\n",
    "Epoch #133: Loss:2.4325, Accuracy:0.1762, Validation Loss:2.4658, Validation Accuracy:0.1741\n",
    "Epoch #134: Loss:2.4315, Accuracy:0.1811, Validation Loss:2.4682, Validation Accuracy:0.1675\n",
    "Epoch #135: Loss:2.4315, Accuracy:0.1799, Validation Loss:2.4680, Validation Accuracy:0.1675\n",
    "Epoch #136: Loss:2.4320, Accuracy:0.1807, Validation Loss:2.4664, Validation Accuracy:0.1708\n",
    "Epoch #137: Loss:2.4319, Accuracy:0.1803, Validation Loss:2.4675, Validation Accuracy:0.1708\n",
    "Epoch #138: Loss:2.4317, Accuracy:0.1795, Validation Loss:2.4677, Validation Accuracy:0.1757\n",
    "Epoch #139: Loss:2.4314, Accuracy:0.1799, Validation Loss:2.4654, Validation Accuracy:0.1790\n",
    "Epoch #140: Loss:2.4304, Accuracy:0.1815, Validation Loss:2.4651, Validation Accuracy:0.1773\n",
    "Epoch #141: Loss:2.4305, Accuracy:0.1782, Validation Loss:2.4639, Validation Accuracy:0.1741\n",
    "Epoch #142: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4632, Validation Accuracy:0.1724\n",
    "Epoch #143: Loss:2.4304, Accuracy:0.1799, Validation Loss:2.4648, Validation Accuracy:0.1757\n",
    "Epoch #144: Loss:2.4305, Accuracy:0.1799, Validation Loss:2.4655, Validation Accuracy:0.1773\n",
    "Epoch #145: Loss:2.4312, Accuracy:0.1786, Validation Loss:2.4678, Validation Accuracy:0.1741\n",
    "Epoch #146: Loss:2.4317, Accuracy:0.1791, Validation Loss:2.4658, Validation Accuracy:0.1741\n",
    "Epoch #147: Loss:2.4310, Accuracy:0.1795, Validation Loss:2.4648, Validation Accuracy:0.1773\n",
    "Epoch #148: Loss:2.4316, Accuracy:0.1815, Validation Loss:2.4626, Validation Accuracy:0.1773\n",
    "Epoch #149: Loss:2.4316, Accuracy:0.1819, Validation Loss:2.4654, Validation Accuracy:0.1806\n",
    "Epoch #150: Loss:2.4318, Accuracy:0.1815, Validation Loss:2.4626, Validation Accuracy:0.1724\n",
    "Epoch #151: Loss:2.4298, Accuracy:0.1786, Validation Loss:2.4663, Validation Accuracy:0.1790\n",
    "Epoch #152: Loss:2.4291, Accuracy:0.1795, Validation Loss:2.4650, Validation Accuracy:0.1773\n",
    "Epoch #153: Loss:2.4289, Accuracy:0.1786, Validation Loss:2.4648, Validation Accuracy:0.1773\n",
    "Epoch #154: Loss:2.4294, Accuracy:0.1795, Validation Loss:2.4649, Validation Accuracy:0.1773\n",
    "Epoch #155: Loss:2.4293, Accuracy:0.1782, Validation Loss:2.4700, Validation Accuracy:0.1773\n",
    "Epoch #156: Loss:2.4301, Accuracy:0.1803, Validation Loss:2.4692, Validation Accuracy:0.1675\n",
    "Epoch #157: Loss:2.4296, Accuracy:0.1815, Validation Loss:2.4688, Validation Accuracy:0.1708\n",
    "Epoch #158: Loss:2.4282, Accuracy:0.1799, Validation Loss:2.4622, Validation Accuracy:0.1757\n",
    "Epoch #159: Loss:2.4315, Accuracy:0.1811, Validation Loss:2.4636, Validation Accuracy:0.1724\n",
    "Epoch #160: Loss:2.4344, Accuracy:0.1799, Validation Loss:2.4671, Validation Accuracy:0.1790\n",
    "Epoch #161: Loss:2.4437, Accuracy:0.1811, Validation Loss:2.4675, Validation Accuracy:0.1560\n",
    "Epoch #162: Loss:2.4463, Accuracy:0.1795, Validation Loss:2.4625, Validation Accuracy:0.1757\n",
    "Epoch #163: Loss:2.4394, Accuracy:0.1782, Validation Loss:2.4724, Validation Accuracy:0.1691\n",
    "Epoch #164: Loss:2.4319, Accuracy:0.1832, Validation Loss:2.4623, Validation Accuracy:0.1691\n",
    "Epoch #165: Loss:2.4343, Accuracy:0.1815, Validation Loss:2.4664, Validation Accuracy:0.1691\n",
    "Epoch #166: Loss:2.4349, Accuracy:0.1766, Validation Loss:2.4611, Validation Accuracy:0.1593\n",
    "Epoch #167: Loss:2.4318, Accuracy:0.1848, Validation Loss:2.4637, Validation Accuracy:0.1691\n",
    "Epoch #168: Loss:2.4296, Accuracy:0.1803, Validation Loss:2.4667, Validation Accuracy:0.1675\n",
    "Epoch #169: Loss:2.4301, Accuracy:0.1758, Validation Loss:2.4628, Validation Accuracy:0.1724\n",
    "Epoch #170: Loss:2.4306, Accuracy:0.1840, Validation Loss:2.4639, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4321, Accuracy:0.1811, Validation Loss:2.4658, Validation Accuracy:0.1658\n",
    "Epoch #172: Loss:2.4326, Accuracy:0.1811, Validation Loss:2.4670, Validation Accuracy:0.1642\n",
    "Epoch #173: Loss:2.4340, Accuracy:0.1811, Validation Loss:2.4638, Validation Accuracy:0.1691\n",
    "Epoch #174: Loss:2.4338, Accuracy:0.1811, Validation Loss:2.4650, Validation Accuracy:0.1741\n",
    "Epoch #175: Loss:2.4341, Accuracy:0.1844, Validation Loss:2.4620, Validation Accuracy:0.1724\n",
    "Epoch #176: Loss:2.4326, Accuracy:0.1823, Validation Loss:2.4648, Validation Accuracy:0.1741\n",
    "Epoch #177: Loss:2.4320, Accuracy:0.1786, Validation Loss:2.4656, Validation Accuracy:0.1773\n",
    "Epoch #178: Loss:2.4325, Accuracy:0.1823, Validation Loss:2.4624, Validation Accuracy:0.1757\n",
    "Epoch #179: Loss:2.4324, Accuracy:0.1807, Validation Loss:2.4633, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.4325, Accuracy:0.1803, Validation Loss:2.4625, Validation Accuracy:0.1773\n",
    "Epoch #181: Loss:2.4310, Accuracy:0.1803, Validation Loss:2.4668, Validation Accuracy:0.1790\n",
    "Epoch #182: Loss:2.4320, Accuracy:0.1766, Validation Loss:2.4638, Validation Accuracy:0.1790\n",
    "Epoch #183: Loss:2.4328, Accuracy:0.1811, Validation Loss:2.4644, Validation Accuracy:0.1773\n",
    "Epoch #184: Loss:2.4325, Accuracy:0.1832, Validation Loss:2.4651, Validation Accuracy:0.1724\n",
    "Epoch #185: Loss:2.4360, Accuracy:0.1786, Validation Loss:2.4640, Validation Accuracy:0.1773\n",
    "Epoch #186: Loss:2.4312, Accuracy:0.1832, Validation Loss:2.4652, Validation Accuracy:0.1724\n",
    "Epoch #187: Loss:2.4324, Accuracy:0.1811, Validation Loss:2.4629, Validation Accuracy:0.1741\n",
    "Epoch #188: Loss:2.4324, Accuracy:0.1778, Validation Loss:2.4632, Validation Accuracy:0.1741\n",
    "Epoch #189: Loss:2.4312, Accuracy:0.1803, Validation Loss:2.4627, Validation Accuracy:0.1757\n",
    "Epoch #190: Loss:2.4317, Accuracy:0.1823, Validation Loss:2.4627, Validation Accuracy:0.1741\n",
    "Epoch #191: Loss:2.4301, Accuracy:0.1803, Validation Loss:2.4632, Validation Accuracy:0.1626\n",
    "Epoch #192: Loss:2.4303, Accuracy:0.1815, Validation Loss:2.4625, Validation Accuracy:0.1724\n",
    "Epoch #193: Loss:2.4307, Accuracy:0.1811, Validation Loss:2.4637, Validation Accuracy:0.1626\n",
    "Epoch #194: Loss:2.4291, Accuracy:0.1860, Validation Loss:2.4623, Validation Accuracy:0.1724\n",
    "Epoch #195: Loss:2.4294, Accuracy:0.1864, Validation Loss:2.4667, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4289, Accuracy:0.1795, Validation Loss:2.4645, Validation Accuracy:0.1658\n",
    "Epoch #197: Loss:2.4291, Accuracy:0.1803, Validation Loss:2.4658, Validation Accuracy:0.1642\n",
    "Epoch #198: Loss:2.4304, Accuracy:0.1823, Validation Loss:2.4643, Validation Accuracy:0.1675\n",
    "Epoch #199: Loss:2.4372, Accuracy:0.1774, Validation Loss:2.4698, Validation Accuracy:0.1773\n",
    "Epoch #200: Loss:2.4346, Accuracy:0.1799, Validation Loss:2.4643, Validation Accuracy:0.1675\n",
    "Epoch #201: Loss:2.4346, Accuracy:0.1807, Validation Loss:2.4677, Validation Accuracy:0.1708\n",
    "Epoch #202: Loss:2.4337, Accuracy:0.1778, Validation Loss:2.4668, Validation Accuracy:0.1708\n",
    "Epoch #203: Loss:2.4338, Accuracy:0.1782, Validation Loss:2.4662, Validation Accuracy:0.1708\n",
    "Epoch #204: Loss:2.4331, Accuracy:0.1791, Validation Loss:2.4675, Validation Accuracy:0.1708\n",
    "Epoch #205: Loss:2.4332, Accuracy:0.1795, Validation Loss:2.4665, Validation Accuracy:0.1708\n",
    "Epoch #206: Loss:2.4334, Accuracy:0.1786, Validation Loss:2.4658, Validation Accuracy:0.1724\n",
    "Epoch #207: Loss:2.4329, Accuracy:0.1795, Validation Loss:2.4673, Validation Accuracy:0.1708\n",
    "Epoch #208: Loss:2.4329, Accuracy:0.1795, Validation Loss:2.4667, Validation Accuracy:0.1675\n",
    "Epoch #209: Loss:2.4329, Accuracy:0.1807, Validation Loss:2.4664, Validation Accuracy:0.1675\n",
    "Epoch #210: Loss:2.4327, Accuracy:0.1807, Validation Loss:2.4674, Validation Accuracy:0.1675\n",
    "Epoch #211: Loss:2.4326, Accuracy:0.1799, Validation Loss:2.4668, Validation Accuracy:0.1708\n",
    "Epoch #212: Loss:2.4323, Accuracy:0.1795, Validation Loss:2.4659, Validation Accuracy:0.1675\n",
    "Epoch #213: Loss:2.4328, Accuracy:0.1815, Validation Loss:2.4670, Validation Accuracy:0.1708\n",
    "Epoch #214: Loss:2.4321, Accuracy:0.1795, Validation Loss:2.4677, Validation Accuracy:0.1708\n",
    "Epoch #215: Loss:2.4321, Accuracy:0.1791, Validation Loss:2.4663, Validation Accuracy:0.1708\n",
    "Epoch #216: Loss:2.4326, Accuracy:0.1823, Validation Loss:2.4661, Validation Accuracy:0.1675\n",
    "Epoch #217: Loss:2.4321, Accuracy:0.1807, Validation Loss:2.4694, Validation Accuracy:0.1724\n",
    "Epoch #218: Loss:2.4324, Accuracy:0.1811, Validation Loss:2.4656, Validation Accuracy:0.1691\n",
    "Epoch #219: Loss:2.4324, Accuracy:0.1791, Validation Loss:2.4676, Validation Accuracy:0.1708\n",
    "Epoch #220: Loss:2.4328, Accuracy:0.1778, Validation Loss:2.4669, Validation Accuracy:0.1675\n",
    "Epoch #221: Loss:2.4313, Accuracy:0.1819, Validation Loss:2.4670, Validation Accuracy:0.1675\n",
    "Epoch #222: Loss:2.4319, Accuracy:0.1807, Validation Loss:2.4675, Validation Accuracy:0.1708\n",
    "Epoch #223: Loss:2.4326, Accuracy:0.1823, Validation Loss:2.4674, Validation Accuracy:0.1708\n",
    "Epoch #224: Loss:2.4315, Accuracy:0.1807, Validation Loss:2.4677, Validation Accuracy:0.1708\n",
    "Epoch #225: Loss:2.4321, Accuracy:0.1815, Validation Loss:2.4668, Validation Accuracy:0.1708\n",
    "Epoch #226: Loss:2.4311, Accuracy:0.1811, Validation Loss:2.4665, Validation Accuracy:0.1675\n",
    "Epoch #227: Loss:2.4315, Accuracy:0.1811, Validation Loss:2.4678, Validation Accuracy:0.1708\n",
    "Epoch #228: Loss:2.4314, Accuracy:0.1807, Validation Loss:2.4673, Validation Accuracy:0.1708\n",
    "Epoch #229: Loss:2.4312, Accuracy:0.1811, Validation Loss:2.4666, Validation Accuracy:0.1675\n",
    "Epoch #230: Loss:2.4314, Accuracy:0.1819, Validation Loss:2.4675, Validation Accuracy:0.1708\n",
    "Epoch #231: Loss:2.4309, Accuracy:0.1819, Validation Loss:2.4665, Validation Accuracy:0.1708\n",
    "Epoch #232: Loss:2.4312, Accuracy:0.1811, Validation Loss:2.4681, Validation Accuracy:0.1708\n",
    "Epoch #233: Loss:2.4306, Accuracy:0.1815, Validation Loss:2.4673, Validation Accuracy:0.1708\n",
    "Epoch #234: Loss:2.4307, Accuracy:0.1815, Validation Loss:2.4675, Validation Accuracy:0.1724\n",
    "Epoch #235: Loss:2.4305, Accuracy:0.1819, Validation Loss:2.4684, Validation Accuracy:0.1724\n",
    "Epoch #236: Loss:2.4311, Accuracy:0.1828, Validation Loss:2.4669, Validation Accuracy:0.1708\n",
    "Epoch #237: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4680, Validation Accuracy:0.1708\n",
    "Epoch #238: Loss:2.4301, Accuracy:0.1815, Validation Loss:2.4668, Validation Accuracy:0.1708\n",
    "Epoch #239: Loss:2.4312, Accuracy:0.1807, Validation Loss:2.4692, Validation Accuracy:0.1724\n",
    "Epoch #240: Loss:2.4305, Accuracy:0.1815, Validation Loss:2.4666, Validation Accuracy:0.1708\n",
    "Epoch #241: Loss:2.4308, Accuracy:0.1819, Validation Loss:2.4679, Validation Accuracy:0.1724\n",
    "Epoch #242: Loss:2.4308, Accuracy:0.1832, Validation Loss:2.4674, Validation Accuracy:0.1708\n",
    "Epoch #243: Loss:2.4298, Accuracy:0.1823, Validation Loss:2.4670, Validation Accuracy:0.1675\n",
    "Epoch #244: Loss:2.4303, Accuracy:0.1815, Validation Loss:2.4691, Validation Accuracy:0.1691\n",
    "Epoch #245: Loss:2.4303, Accuracy:0.1840, Validation Loss:2.4680, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4297, Accuracy:0.1836, Validation Loss:2.4678, Validation Accuracy:0.1724\n",
    "Epoch #247: Loss:2.4305, Accuracy:0.1819, Validation Loss:2.4684, Validation Accuracy:0.1724\n",
    "Epoch #248: Loss:2.4317, Accuracy:0.1828, Validation Loss:2.4685, Validation Accuracy:0.1691\n",
    "Epoch #249: Loss:2.4312, Accuracy:0.1778, Validation Loss:2.4675, Validation Accuracy:0.1675\n",
    "Epoch #250: Loss:2.4296, Accuracy:0.1832, Validation Loss:2.4714, Validation Accuracy:0.1724\n",
    "Epoch #251: Loss:2.4303, Accuracy:0.1836, Validation Loss:2.4676, Validation Accuracy:0.1691\n",
    "Epoch #252: Loss:2.4305, Accuracy:0.1840, Validation Loss:2.4696, Validation Accuracy:0.1724\n",
    "Epoch #253: Loss:2.4299, Accuracy:0.1844, Validation Loss:2.4693, Validation Accuracy:0.1691\n",
    "Epoch #254: Loss:2.4314, Accuracy:0.1799, Validation Loss:2.4685, Validation Accuracy:0.1691\n",
    "Epoch #255: Loss:2.4306, Accuracy:0.1840, Validation Loss:2.4696, Validation Accuracy:0.1691\n",
    "Epoch #256: Loss:2.4294, Accuracy:0.1840, Validation Loss:2.4678, Validation Accuracy:0.1691\n",
    "Epoch #257: Loss:2.4299, Accuracy:0.1815, Validation Loss:2.4686, Validation Accuracy:0.1691\n",
    "Epoch #258: Loss:2.4295, Accuracy:0.1840, Validation Loss:2.4697, Validation Accuracy:0.1724\n",
    "Epoch #259: Loss:2.4318, Accuracy:0.1828, Validation Loss:2.4679, Validation Accuracy:0.1691\n",
    "Epoch #260: Loss:2.4299, Accuracy:0.1836, Validation Loss:2.4717, Validation Accuracy:0.1724\n",
    "Epoch #261: Loss:2.4301, Accuracy:0.1840, Validation Loss:2.4680, Validation Accuracy:0.1691\n",
    "Epoch #262: Loss:2.4294, Accuracy:0.1840, Validation Loss:2.4692, Validation Accuracy:0.1691\n",
    "Epoch #263: Loss:2.4293, Accuracy:0.1840, Validation Loss:2.4698, Validation Accuracy:0.1691\n",
    "Epoch #264: Loss:2.4291, Accuracy:0.1844, Validation Loss:2.4690, Validation Accuracy:0.1675\n",
    "Epoch #265: Loss:2.4289, Accuracy:0.1836, Validation Loss:2.4701, Validation Accuracy:0.1724\n",
    "Epoch #266: Loss:2.4288, Accuracy:0.1840, Validation Loss:2.4693, Validation Accuracy:0.1691\n",
    "Epoch #267: Loss:2.4290, Accuracy:0.1828, Validation Loss:2.4691, Validation Accuracy:0.1691\n",
    "Epoch #268: Loss:2.4305, Accuracy:0.1864, Validation Loss:2.4696, Validation Accuracy:0.1691\n",
    "Epoch #269: Loss:2.4311, Accuracy:0.1840, Validation Loss:2.4720, Validation Accuracy:0.1691\n",
    "Epoch #270: Loss:2.4286, Accuracy:0.1823, Validation Loss:2.4691, Validation Accuracy:0.1691\n",
    "Epoch #271: Loss:2.4290, Accuracy:0.1836, Validation Loss:2.4716, Validation Accuracy:0.1724\n",
    "Epoch #272: Loss:2.4306, Accuracy:0.1828, Validation Loss:2.4695, Validation Accuracy:0.1691\n",
    "Epoch #273: Loss:2.4296, Accuracy:0.1819, Validation Loss:2.4687, Validation Accuracy:0.1691\n",
    "Epoch #274: Loss:2.4281, Accuracy:0.1844, Validation Loss:2.4732, Validation Accuracy:0.1724\n",
    "Epoch #275: Loss:2.4299, Accuracy:0.1823, Validation Loss:2.4705, Validation Accuracy:0.1691\n",
    "Epoch #276: Loss:2.4302, Accuracy:0.1819, Validation Loss:2.4701, Validation Accuracy:0.1691\n",
    "Epoch #277: Loss:2.4289, Accuracy:0.1840, Validation Loss:2.4723, Validation Accuracy:0.1724\n",
    "Epoch #278: Loss:2.4286, Accuracy:0.1819, Validation Loss:2.4702, Validation Accuracy:0.1691\n",
    "Epoch #279: Loss:2.4280, Accuracy:0.1832, Validation Loss:2.4709, Validation Accuracy:0.1691\n",
    "Epoch #280: Loss:2.4290, Accuracy:0.1836, Validation Loss:2.4705, Validation Accuracy:0.1691\n",
    "Epoch #281: Loss:2.4289, Accuracy:0.1848, Validation Loss:2.4700, Validation Accuracy:0.1691\n",
    "Epoch #282: Loss:2.4292, Accuracy:0.1848, Validation Loss:2.4729, Validation Accuracy:0.1724\n",
    "Epoch #283: Loss:2.4277, Accuracy:0.1828, Validation Loss:2.4698, Validation Accuracy:0.1691\n",
    "Epoch #284: Loss:2.4285, Accuracy:0.1848, Validation Loss:2.4713, Validation Accuracy:0.1691\n",
    "Epoch #285: Loss:2.4277, Accuracy:0.1840, Validation Loss:2.4706, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.4286, Accuracy:0.1836, Validation Loss:2.4710, Validation Accuracy:0.1691\n",
    "Epoch #287: Loss:2.4272, Accuracy:0.1840, Validation Loss:2.4729, Validation Accuracy:0.1691\n",
    "Epoch #288: Loss:2.4290, Accuracy:0.1819, Validation Loss:2.4708, Validation Accuracy:0.1642\n",
    "Epoch #289: Loss:2.4283, Accuracy:0.1848, Validation Loss:2.4729, Validation Accuracy:0.1691\n",
    "Epoch #290: Loss:2.4282, Accuracy:0.1840, Validation Loss:2.4716, Validation Accuracy:0.1691\n",
    "Epoch #291: Loss:2.4281, Accuracy:0.1844, Validation Loss:2.4733, Validation Accuracy:0.1724\n",
    "Epoch #292: Loss:2.4273, Accuracy:0.1836, Validation Loss:2.4706, Validation Accuracy:0.1691\n",
    "Epoch #293: Loss:2.4274, Accuracy:0.1836, Validation Loss:2.4716, Validation Accuracy:0.1724\n",
    "Epoch #294: Loss:2.4274, Accuracy:0.1836, Validation Loss:2.4715, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4269, Accuracy:0.1840, Validation Loss:2.4716, Validation Accuracy:0.1691\n",
    "Epoch #296: Loss:2.4274, Accuracy:0.1852, Validation Loss:2.4731, Validation Accuracy:0.1691\n",
    "Epoch #297: Loss:2.4289, Accuracy:0.1840, Validation Loss:2.4724, Validation Accuracy:0.1691\n",
    "Epoch #298: Loss:2.4267, Accuracy:0.1819, Validation Loss:2.4718, Validation Accuracy:0.1691\n",
    "Epoch #299: Loss:2.4268, Accuracy:0.1836, Validation Loss:2.4737, Validation Accuracy:0.1691\n",
    "Epoch #300: Loss:2.4276, Accuracy:0.1840, Validation Loss:2.4732, Validation Accuracy:0.1691\n",
    "\n",
    "Test:\n",
    "Test Loss:2.47317576, Accuracy:0.1691\n",
    "Labels: ['by', 'eg', 'ck', 'ek', 'eb', 'sg', 'ib', 'aa', 'yd', 'ds', 'mb', 'sk', 'ce', 'eo', 'my']\n",
    "Confusion Matrix:\n",
    "      by  eg  ck  ek  eb  sg  ib  aa  yd  ds  mb  sk  ce  eo  my\n",
    "t:by   3  14   0   0   0  20   0   0   2   1   0   0   0   0   0\n",
    "t:eg   2  25   0   0   0  12   1   0   0  10   0   0   0   0   0\n",
    "t:ck   1   9   0   0   0  11   1   0   0   1   0   0   0   0   0\n",
    "t:ek   0  18   0   0   0  18   0   0   9   3   0   0   0   0   0\n",
    "t:eb   3  20   0   0   0  14   0   0   8   3   2   0   0   0   0\n",
    "t:sg   0   7   0   0   1  33   1   0   9   0   0   0   0   0   0\n",
    "t:ib   0   7   0   0   0  17   1   0  29   0   0   0   0   0   0\n",
    "t:aa   4  15   0   0   0   5   0   0   3   7   0   0   0   0   0\n",
    "t:yd   0   3   0   0   0  24   1   0  34   0   0   0   0   0   0\n",
    "t:ds   0  11   0   0   1  11   0   0   1   7   0   0   0   0   0\n",
    "t:mb   1  11   0   0   0  27   0   0  13   0   0   0   0   0   0\n",
    "t:sk   1  13   0   0   1  13   0   0   2   3   0   0   0   0   0\n",
    "t:ce   1   6   0   0   0  13   1   0   3   3   0   0   0   0   0\n",
    "t:eo   1   4   0   0   1  27   0   0   1   0   0   0   0   0   0\n",
    "t:my   0   7   0   0   0   7   2   0   2   2   0   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          by       0.18      0.07      0.11        40\n",
    "          eg       0.15      0.50      0.23        50\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          sg       0.13      0.65      0.22        51\n",
    "          ib       0.12      0.02      0.03        54\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          yd       0.29      0.55      0.38        62\n",
    "          ds       0.17      0.23      0.20        31\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.07      0.13      0.08       609\n",
    "weighted avg       0.08      0.17      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 04:22:31 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.701431358976317, 2.693263908520904, 2.6906110942853103, 2.6871398360466916, 2.68144289339313, 2.6749775562380336, 2.669231760090795, 2.6584999800119884, 2.6472068913464475, 2.641169216441012, 2.6278003201696087, 2.6107477746377827, 2.594582984795907, 2.5798663674121225, 2.5632196506255953, 2.552690204141175, 2.5473958040497378, 2.5355678434638165, 2.5521305535227206, 2.527986588736473, 2.5151668667597527, 2.509173080447468, 2.5008945484662486, 2.501813311490715, 2.4986490271557336, 2.4915558869028325, 2.493467302353707, 2.4904011673919477, 2.484697562328896, 2.4878346027411853, 2.48466590864122, 2.4830671636929065, 2.4817689424273612, 2.480078985342643, 2.4796328810831203, 2.480435830618948, 2.4796579870684394, 2.476665447889682, 2.4766750844632854, 2.4748402855470655, 2.4761330028277118, 2.4743096096370802, 2.4752224407759793, 2.473551022008135, 2.47524982915919, 2.473460819137899, 2.4727451723001663, 2.471906436487959, 2.4721284139724005, 2.471064179205934, 2.470281272490428, 2.4723795482090543, 2.470009036839302, 2.471498691781205, 2.4697467157210427, 2.46815459873093, 2.469540980844858, 2.468002127113405, 2.4677287044587786, 2.4689939088617834, 2.4698547631844705, 2.4692249039711034, 2.4682933074500175, 2.4685430597201945, 2.4683000896560343, 2.4665236907639527, 2.4663589259086574, 2.467843526298385, 2.4658468302051815, 2.4680531831406216, 2.4650606259532357, 2.465814911868968, 2.4674036639860306, 2.467353973091138, 2.466186079877155, 2.4686091721351513, 2.468866169746286, 2.4653070160908066, 2.46442900502623, 2.4635169862330644, 2.4612898998855566, 2.4623590490500917, 2.4610881785845327, 2.4622426984345385, 2.459886605712189, 2.459873934684716, 2.4580888963489502, 2.4586582587074566, 2.4627857114293894, 2.461136038080225, 2.461677653840414, 2.4602111763946333, 2.4592540342428024, 2.4603497359552993, 2.4609061923911812, 2.4626645149268542, 2.461563701504361, 2.460698438199674, 2.460145529072077, 2.4608480166919127, 2.460677685213011, 2.460960484099114, 2.461173308110981, 2.4592633815038774, 2.4574363282552887, 2.462033260436285, 2.4560301961569952, 2.460555291919677, 2.4960386984062506, 2.4688906755744924, 2.4809328143428306, 2.4665538199820936, 2.4658471864628284, 2.474851629808423, 2.465855442635923, 2.464605806300597, 2.4689446149396974, 2.463961774296753, 2.462613792059261, 2.4670797349588427, 2.4662618456998677, 2.4662651291425983, 2.468150614517663, 2.468021092939455, 2.4656355377097046, 2.469172726711029, 2.4670676116285652, 2.466293251181667, 2.4672752131382234, 2.463756653279898, 2.4649231594380097, 2.4660821794876324, 2.4657688117379624, 2.468158111979417, 2.46799830027989, 2.4664298419294686, 2.4675288658423966, 2.467677792109096, 2.465436124645039, 2.4650905543360215, 2.4639005543563166, 2.46321039011913, 2.464778906410355, 2.4655496999743733, 2.467816270630935, 2.465848732464419, 2.464797238802479, 2.4625740027780014, 2.4653990405729447, 2.462609168148197, 2.466341860775877, 2.465006187435833, 2.4647679728240215, 2.464880009981603, 2.4699916365894388, 2.4692319676598107, 2.4688326678252572, 2.46222224219875, 2.463636265991161, 2.4670630758031837, 2.4674726859689344, 2.462496403598629, 2.4724150091556494, 2.462261262589879, 2.4664367564597547, 2.4611173662646064, 2.4637140765761703, 2.466692010757371, 2.4627587438999923, 2.4638853488101553, 2.465833429828262, 2.466976201397249, 2.463825261064351, 2.464999034095476, 2.462048161401733, 2.4648335625972653, 2.46561728479044, 2.4623937258383717, 2.4633462961475643, 2.462526714077528, 2.4667690966908373, 2.4638265318471224, 2.4644261948972304, 2.4650565454329567, 2.464009165176617, 2.465236417961434, 2.4629395990731875, 2.4632327458737127, 2.4627226338597943, 2.462739348215814, 2.4631828588413684, 2.462456658947448, 2.463716035993228, 2.4623381167601286, 2.466703308822682, 2.4644641574771926, 2.4658024577280178, 2.4642722035080733, 2.4697931599734453, 2.4642695609376153, 2.467659171970411, 2.466812156495594, 2.4661837771216835, 2.4674985913807537, 2.4665048071512055, 2.4658149917333194, 2.467265600445627, 2.466684313243246, 2.4664421629631654, 2.4674376384378065, 2.4667561011165624, 2.465899821768449, 2.4669631600184196, 2.467683556827614, 2.4662755279509696, 2.466105302174886, 2.4694055179852765, 2.4655533553344275, 2.467560830374657, 2.4668684193653427, 2.4669530560034643, 2.467511890752758, 2.467412209080162, 2.4677340103487664, 2.4667744675684835, 2.4665179820287797, 2.467810137518521, 2.4673465535362755, 2.4665807327026217, 2.4674540912772245, 2.466483760937094, 2.4680506180855635, 2.4672809120860983, 2.467498848199453, 2.4683816139333943, 2.4669415116897357, 2.4680156414144734, 2.466753398256349, 2.4691683439589878, 2.466605334054856, 2.4678836996332176, 2.467405436269951, 2.4670038340714178, 2.4690856189759103, 2.4679930386284887, 2.467780400183792, 2.4683929390116472, 2.468471792139639, 2.4674765664368428, 2.471375695198823, 2.467594528824629, 2.4695676688490242, 2.469255433685478, 2.468543953104755, 2.4696395698635056, 2.4678442274604135, 2.4685722994686934, 2.4697285042999217, 2.4679073841113763, 2.471740182397401, 2.467986344899646, 2.4691909001574337, 2.4697708378871672, 2.4690032901826555, 2.4700779480299926, 2.469330706619864, 2.469083547983655, 2.4695706958645474, 2.472019784360488, 2.469123700177924, 2.4716141541015926, 2.469518161563842, 2.4687363169659142, 2.473209919013413, 2.47053059960038, 2.4700871833243787, 2.472301530524819, 2.470189257404096, 2.470913558170713, 2.4705296247855, 2.4700183195042102, 2.4729027834236135, 2.4698461495792534, 2.471255453153588, 2.4705875409256257, 2.471006532020757, 2.4728957267817604, 2.4708375966020406, 2.4728763107400025, 2.471567942786882, 2.47332387213245, 2.4705503049546667, 2.4716421669144153, 2.47153553939218, 2.471623771492092, 2.473082241753639, 2.4723714866074435, 2.4717602522306645, 2.4736591485529305, 2.473175686959954], 'val_acc': [0.08538587838533672, 0.09195402248690672, 0.08866995023731723, 0.08374384225442491, 0.09031198685147689, 0.10180623962716712, 0.10673234800155136, 0.11822660097298755, 0.1428571424411827, 0.1444991783702315, 0.15927750359125717, 0.16256157593871964, 0.16420361216138737, 0.16256157584084666, 0.1609195398139249, 0.1609195397160519, 0.16091953961817893, 0.16912972034002563, 0.15763546746646243, 0.1592775033955112, 0.1642036118677684, 0.1642036119656414, 0.1510673226736645, 0.14942528664674273, 0.14942528664674273, 0.15763546727071645, 0.1642036118677684, 0.16420361167202246, 0.16748768431310387, 0.16584564818830913, 0.16584564799256318, 0.1724137927853611, 0.1592775033955112, 0.1642036119656414, 0.1642036118677684, 0.15927750329763823, 0.1625615756451007, 0.16912972043789862, 0.16912972014427968, 0.1674876842152309, 0.17405582881228285, 0.1789819371866671, 0.16912972043789862, 0.17733990106187233, 0.17733990096399938, 0.1740558287144099, 0.17077175656269336, 0.1724137926874881, 0.17077175646482037, 0.16912972034002563, 0.16748768411735793, 0.1756978649370776, 0.1740558287144099, 0.17898193728454007, 0.17077175646482037, 0.17241379258961514, 0.17569786483920463, 0.17241379258961514, 0.17898193708879412, 0.1740558287144099, 0.17733990096399938, 0.17733990106187233, 0.17569786483920463, 0.1822660093383836, 0.1806239734093348, 0.18226600963200254, 0.17898193728454007, 0.1756978649370776, 0.1789819371866671, 0.1822660094362566, 0.18226600953412955, 0.17405582891015584, 0.17405582881228285, 0.1756978649370776, 0.18062397331146185, 0.1789819371866671, 0.1855500815879731, 0.18226600924051062, 0.17241379249174216, 0.1789819371866671, 0.18226600924051062, 0.1822660094362566, 0.17077175646482037, 0.18226600953412955, 0.16912972034002563, 0.18883415403330855, 0.1839080456589243, 0.18555008178371907, 0.18226600953412955, 0.1773399008661264, 0.1871921179085138, 0.1839080456589243, 0.18390804546317835, 0.17733990106187233, 0.1822660094362566, 0.1822660094362566, 0.18062397331146185, 0.1822660094362566, 0.18062397331146185, 0.1822660093383836, 0.17898193708879412, 0.17898193708879412, 0.1773399008661264, 0.17898193708879412, 0.1756978649370776, 0.17569786474133164, 0.17733990096399938, 0.18226600924051062, 0.1740558286165369, 0.16748768411735793, 0.16748768382373896, 0.16748768392161195, 0.17569786464345866, 0.16912972014427968, 0.1724137926874881, 0.17733990106187233, 0.17733990106187233, 0.17405582891015584, 0.1724137926874881, 0.1756978649370776, 0.17405582881228285, 0.17405582891015584, 0.17405582881228285, 0.17405582881228285, 0.1740558287144099, 0.17405582891015584, 0.17241379249174216, 0.17241379258961514, 0.17241379249174216, 0.16912972024215267, 0.17569786483920463, 0.17241379249174216, 0.1740558285186639, 0.16748768401948494, 0.16748768401948494, 0.17077175626907443, 0.1707717563669474, 0.17569786474133164, 0.17898193699092113, 0.1773399008661264, 0.17405582842079093, 0.17241379229599618, 0.17569786474133164, 0.1773399008661264, 0.1740558286165369, 0.1740558287144099, 0.1773399008661264, 0.1773399008661264, 0.18062397311571587, 0.17241379249174216, 0.1789819371866671, 0.1773399008661264, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.16748768401948494, 0.17077175626907443, 0.17569786464345866, 0.17241379249174216, 0.1789819371866671, 0.1559934311459217, 0.17569786483920463, 0.16912972024215267, 0.16912972024215267, 0.16912972014427968, 0.15927750329763823, 0.1691297200464067, 0.16748768411735793, 0.17241379249174216, 0.17077175626907443, 0.1658456477968172, 0.1642036119656414, 0.16912972024215267, 0.1740558286165369, 0.17241379239386917, 0.17405582881228285, 0.17733990096399938, 0.1756978649370776, 0.17569786483920463, 0.17733990096399938, 0.17898193689304814, 0.17898193708879412, 0.17733990096399938, 0.17241379239386917, 0.17733990106187233, 0.17241379239386917, 0.1740558286165369, 0.1740558285186639, 0.17569786464345866, 0.1740558285186639, 0.1625615756451007, 0.17241379249174216, 0.1625615756451007, 0.17241379249174216, 0.17241379239386917, 0.1658456477968172, 0.16420361176989545, 0.1674876842152309, 0.1773399007682534, 0.16748768411735793, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.17241379249174216, 0.17077175626907443, 0.16748768401948494, 0.16748768401948494, 0.16748768401948494, 0.17077175626907443, 0.16748768401948494, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.16748768401948494, 0.17241379239386917, 0.16912972024215267, 0.17077175626907443, 0.16748768401948494, 0.16748768401948494, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.16748768401948494, 0.17077175626907443, 0.17077175626907443, 0.16748768401948494, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.17241379239386917, 0.17241379239386917, 0.17077175626907443, 0.17077175626907443, 0.17077175626907443, 0.17241379239386917, 0.17077175626907443, 0.17241379239386917, 0.17077175626907443, 0.16748768401948494, 0.16912972014427968, 0.16912972014427968, 0.17241379239386917, 0.17241379239386917, 0.16912972014427968, 0.16748768401948494, 0.17241379239386917, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16748768401948494, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972034002563, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16420361176989545, 0.16912972014427968, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968], 'loss': [2.7055018736351686, 2.6980693261970976, 2.6904456642863686, 2.687909091718388, 2.684490866142132, 2.678327820286369, 2.67266095600089, 2.664190879054138, 2.651549379340922, 2.6440668907008864, 2.6354279058914654, 2.6186574244646077, 2.601000235066032, 2.5829374418121587, 2.570597995918634, 2.55343783259147, 2.543360684392878, 2.537477638883023, 2.5251252855853132, 2.524382547578283, 2.509821458812612, 2.5045116373645695, 2.496776289225114, 2.4881397395163347, 2.484608016905109, 2.4826036236858955, 2.4763795143769753, 2.488824734991336, 2.4804391249004576, 2.4777044283535936, 2.471612268306881, 2.468389930960089, 2.4666040503758424, 2.4652612310170636, 2.461927614270784, 2.461163856459349, 2.460808395213415, 2.4588869134993034, 2.4567035183524695, 2.456636703773201, 2.4549736935500004, 2.4561043632593487, 2.4550254066866772, 2.452496541550027, 2.452437708754804, 2.450000309111891, 2.4488298625671887, 2.448020766647934, 2.4463558527968012, 2.44614359514914, 2.447464055740858, 2.4466929573787555, 2.4464504836276326, 2.4466556671463735, 2.445873223486867, 2.4455134441720388, 2.4462971400431295, 2.445973073187795, 2.4463950356908404, 2.444310857432089, 2.44356842824321, 2.4429518035794677, 2.4429962521461, 2.442753012713955, 2.4410707491379253, 2.4408349180123645, 2.4406488487118323, 2.439399585293059, 2.4386518822068797, 2.438033903991417, 2.438324458692108, 2.4381729212140155, 2.437174086208461, 2.4379147931046066, 2.437735850316543, 2.437862769485254, 2.438491994203728, 2.438638697316759, 2.4412234107571704, 2.44089370028439, 2.43928653493799, 2.439069460255899, 2.4353137704626002, 2.434336039760519, 2.4325383804172462, 2.4339200113833073, 2.4356113986068193, 2.4378185501333625, 2.435624545506628, 2.434317169248201, 2.436254249705916, 2.436659586356161, 2.435446422839312, 2.438574728192245, 2.4384890853991497, 2.437135259130897, 2.43580708161027, 2.436510042194468, 2.437141001151083, 2.4369151309285564, 2.436649187193759, 2.4364396667088815, 2.43642356860809, 2.437201624778262, 2.4362246044362594, 2.435433766680332, 2.4359021453152447, 2.4345951067593554, 2.444945960367974, 2.4693229151457485, 2.447483645523353, 2.449806937446829, 2.4440964914200487, 2.4386458726879017, 2.4385735523529366, 2.438475207185843, 2.4365455537361287, 2.4372686253925613, 2.438453428848079, 2.438511041155586, 2.439302631276344, 2.43687973580566, 2.4364054197158658, 2.4371901033350576, 2.4357450937588356, 2.436406943342769, 2.4352390091522027, 2.4351656256759924, 2.4338024077719, 2.4330953955405548, 2.4333352528558376, 2.432803272956206, 2.432515545596332, 2.4314643942110346, 2.431545116671302, 2.4320014798910465, 2.4318836500022933, 2.4317371108938293, 2.431427055315805, 2.4303828831815624, 2.430517838280304, 2.430804420839345, 2.4303737695212235, 2.430456759748518, 2.4312057235647275, 2.43169303968457, 2.430975984988516, 2.4315714543360216, 2.431579111537894, 2.4318092300172216, 2.429758987485506, 2.4291035819592173, 2.4288665033219043, 2.4293522189530012, 2.429305356532886, 2.4301477666263462, 2.429598829535733, 2.4282269687378433, 2.4315371145213165, 2.434405860176321, 2.4437304089446332, 2.446278215874392, 2.4393967309282054, 2.431917911388546, 2.434304976316448, 2.4348577080321263, 2.4317724825175637, 2.4296485072533454, 2.4301251411437987, 2.4306001650479296, 2.4320891103215776, 2.4325716782399516, 2.433995569119463, 2.433786791795578, 2.434112286420818, 2.4325545992449813, 2.4320192538737273, 2.432508717327392, 2.4323975551299735, 2.432508858322363, 2.4309958614607856, 2.432026670600844, 2.432829529352991, 2.43249617092908, 2.4359539747727723, 2.4312351382488586, 2.4324217785555233, 2.4324302739920802, 2.4312133757730283, 2.431703523150215, 2.4301099495231737, 2.4303012551223473, 2.4306799375545807, 2.429123032509179, 2.4293773628603015, 2.428933894903508, 2.4291179203644426, 2.4304324149106318, 2.4371967474293172, 2.4345748122957453, 2.4346376844010558, 2.4337219898216045, 2.4337997796843918, 2.43306451274629, 2.4331951525177065, 2.433385234789682, 2.4328581134396656, 2.432945550589591, 2.4329150170516183, 2.4327208757890078, 2.432611520187566, 2.4322564050647024, 2.432752602105268, 2.4320855315950616, 2.4321433859439354, 2.4325853996942666, 2.432145633736675, 2.43238351320584, 2.4324435773571413, 2.4327792302783755, 2.431342924987511, 2.431854543548835, 2.432555014690579, 2.4315088180056343, 2.4321359960450284, 2.4310846802635115, 2.4315026529026227, 2.4314078794122964, 2.431180923332669, 2.431444272466264, 2.4309402833973848, 2.4312409325546795, 2.4305876090541267, 2.4307485237748225, 2.430527212095946, 2.431103565169066, 2.4307923939683356, 2.4301348433602272, 2.4311570989033036, 2.4304779836039767, 2.4307636473457914, 2.4307922620792897, 2.429829165578133, 2.430271574506035, 2.430309266671997, 2.4297331807060165, 2.4305159906587073, 2.431657789864824, 2.4312345657505294, 2.4296386799038805, 2.4302955368950627, 2.430501927683241, 2.42987322807312, 2.431382391242276, 2.430583029361231, 2.4293887690597002, 2.429858589466103, 2.4294746084624492, 2.431800979461513, 2.4299355663558053, 2.430080321388323, 2.4294135427572887, 2.4293383939065483, 2.429064234179393, 2.4288946616820977, 2.428772313394096, 2.429031942904118, 2.4305396987672214, 2.4311123327063338, 2.428565568943533, 2.429021507122189, 2.4306414143995094, 2.4295883940475433, 2.4280772431430386, 2.4298542914694097, 2.4302149728583116, 2.428948598035307, 2.428567811644787, 2.427982439495455, 2.4290173065491034, 2.428909092811099, 2.42921792613897, 2.4276991157805896, 2.4285055984951387, 2.4276635273770877, 2.4286269530623357, 2.4272196700196003, 2.429007658184921, 2.428345623643002, 2.428165610859771, 2.4280617907796307, 2.4273450112195967, 2.4274264647975348, 2.42741488225651, 2.4268613538213333, 2.427381680584541, 2.4288585476806768, 2.426745569632528, 2.426782289031105, 2.4276475505907187], 'acc': [0.06899383977147343, 0.09486653032488891, 0.09281314183370779, 0.08008213527271145, 0.08295687880420587, 0.09158110877449263, 0.09979466133171529, 0.10225872682594911, 0.12484599521395118, 0.13511293647654002, 0.13634496932157011, 0.15687885020670214, 0.16386036932223633, 0.16016426995794386, 0.162217658262478, 0.16221765884995706, 0.16468172475420229, 0.16057494914262446, 0.16673511204288727, 0.16139630420129647, 0.169199179532102, 0.1687885011307268, 0.1650924035472302, 0.16632443442481745, 0.16468172414836452, 0.16755646786650594, 0.16632443483482887, 0.16262833646802688, 0.17043121060551558, 0.16673511321784534, 0.17577002101120762, 0.1737166327250322, 0.16837782390430966, 0.1716632440472041, 0.17125256701661332, 0.1708418894169022, 0.16919917816131758, 0.17248459928334373, 0.17248459930170243, 0.17002053396657751, 0.17125256586001394, 0.1745379869820401, 0.17166324385137774, 0.17577001965878192, 0.17618069766850442, 0.17741273089600784, 0.17782340792659862, 0.1786447649435341, 0.18028747361665878, 0.17864476357274967, 0.17659137546404186, 0.17412731036146073, 0.1753593436073229, 0.16960985517354962, 0.16796714568040208, 0.1749486661667207, 0.17412730916814392, 0.17084188959436986, 0.17125256723079837, 0.17412730895395886, 0.17864476357274967, 0.1720739224485793, 0.17453798719622515, 0.1704312109971683, 0.172895277898904, 0.17453798737369278, 0.17987679599858897, 0.17412730995144932, 0.17905544195576614, 0.17987679640860038, 0.17823408533048335, 0.17659137507238917, 0.18275154112423225, 0.18439425138232643, 0.183572894579576, 0.179876795625295, 0.1774127303085288, 0.17618069825598345, 0.17905544156411346, 0.171252567408266, 0.18028747502416068, 0.17494866520594768, 0.18151950688087964, 0.17987679779774354, 0.18069815244640414, 0.1819301838931117, 0.18069815125308733, 0.17659137685318502, 0.17659137546404186, 0.17782340814078368, 0.17618069745431936, 0.172895277898904, 0.17823408554466844, 0.17659137585569457, 0.17412730996980805, 0.1745379881753569, 0.17535934221817973, 0.1728952767055872, 0.17207392307277578, 0.1733059551069624, 0.1733059559086265, 0.17494866540177403, 0.17084189000438127, 0.17207392264440563, 0.17453798797953055, 0.17412730995144932, 0.17412730995144932, 0.17535934321567018, 0.16960985576102866, 0.16960985517354962, 0.17905544177829852, 0.17700205448961356, 0.18193018588809262, 0.17946611957383596, 0.17905544156411346, 0.1831622181364643, 0.18110882887115715, 0.1778234094932094, 0.1794661185947042, 0.18234086311450973, 0.17946611877217186, 0.17987679738773213, 0.18028747343919116, 0.18028747461414923, 0.18151950688087964, 0.18480492743378546, 0.1794661193780096, 0.17905544117246075, 0.17905544079916677, 0.18028747404502893, 0.17987679582112134, 0.1757700212253927, 0.17618069747267806, 0.18110882926280983, 0.17987679719190577, 0.18069815086143462, 0.1802874740266702, 0.17946611998384737, 0.17987679740609086, 0.1815195074867174, 0.17823408593632112, 0.18234086272285704, 0.1798767960353064, 0.1798767972102645, 0.17864476357274967, 0.1790554405849817, 0.17946611800722517, 0.18151950823330537, 0.18193018410729678, 0.1815195062934006, 0.17864476316273825, 0.17946611898635692, 0.17864476277108554, 0.1794661185947042, 0.1782340857588535, 0.1802874744366816, 0.181519507076706, 0.1798767976019172, 0.18110882906698347, 0.17987679621277403, 0.18110883004611522, 0.1794661185763455, 0.17823408593632112, 0.1831622185464757, 0.1815195066850533, 0.17659137507238917, 0.18480492782543817, 0.18028747343919116, 0.17577002120703397, 0.18398357337260393, 0.18110883004611522, 0.18110883004611522, 0.18110882885279841, 0.18110883022358285, 0.18439425138232643, 0.18234086211701928, 0.1786447649435341, 0.18234086152954024, 0.1806981514672724, 0.18028747463250797, 0.18028747502416068, 0.17659137466237776, 0.18110882985028887, 0.18316221794063794, 0.17864476316273825, 0.1831622173531589, 0.18110882906698347, 0.177823408709904, 0.1802874732433648, 0.18234086250867199, 0.18028747463250797, 0.1815195078600114, 0.18110883026030028, 0.18603696087547397, 0.1864476394726755, 0.17946611898635692, 0.18028747365337622, 0.18234086133371388, 0.17741273148348688, 0.17987679640860038, 0.18069815301552444, 0.17782340833661003, 0.17823408515301573, 0.17905544197412487, 0.17946611957383596, 0.17864476277108554, 0.17946611839887788, 0.17946611879053057, 0.18069815283805682, 0.18069815084307592, 0.1798767964269591, 0.17946611916382454, 0.1815195082700228, 0.17946611839887788, 0.17905544099499313, 0.18234086290032467, 0.18069815184056637, 0.18110882965446254, 0.1790554417599398, 0.17782340890573037, 0.18193018588809262, 0.18069815225057778, 0.18234086131535515, 0.1806981526422305, 0.18151950648922696, 0.181108830651953, 0.18110883045612663, 0.18069815164474, 0.1811088298686476, 0.18193018391147042, 0.18193018489060217, 0.181108830651953, 0.18151950766418504, 0.1815195062934006, 0.18193018489060217, 0.18275154108751482, 0.1823408627044983, 0.181519507076706, 0.18069815262387176, 0.1815195062934006, 0.1819301858697339, 0.18316221715733255, 0.1823408629186834, 0.1815195062934006, 0.18398357219764583, 0.18357289577289285, 0.18193018410729678, 0.18275154032256813, 0.17782340892408907, 0.18316221696150622, 0.18357289438374969, 0.18398357178763441, 0.18439425059902104, 0.17987679719190577, 0.18398357337260393, 0.18398357239347218, 0.18151950688087964, 0.18398357376425664, 0.1827515397350891, 0.18357289418792333, 0.18398357317677758, 0.18398357219764583, 0.1839835735684303, 0.1843942512048588, 0.18357289636037188, 0.18398357355007156, 0.18275153912925132, 0.18644763888519647, 0.1839835739784417, 0.18234086170700786, 0.18357289516705508, 0.1827515397350891, 0.1819301850864285, 0.18439425159651152, 0.18234086231284563, 0.18193018410729678, 0.18398357337260393, 0.18193018371564407, 0.1831622185464757, 0.183572894579576, 0.18480492704213278, 0.18480492843127594, 0.18275153914761005, 0.18480492743378546, 0.1839835731951363, 0.18357289497122872, 0.1839835735684303, 0.18193018510478723, 0.18480492802126453, 0.18398357258929854, 0.18439424940570423, 0.1835728955770665, 0.1835728947937611, 0.18357289614618683, 0.18398357317677758, 0.18521560505185528, 0.183983573586789, 0.18193018528225485, 0.18357289477540237, 0.183983573586789]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
