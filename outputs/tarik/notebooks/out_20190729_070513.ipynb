{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf84.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 07:05:13 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '2Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023D1D439550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023D35A17EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0942, Accuracy:0.3725, Validation Loss:1.0822, Validation Accuracy:0.3859\n",
    "Epoch #2: Loss:1.0805, Accuracy:0.3914, Validation Loss:1.0769, Validation Accuracy:0.3645\n",
    "Epoch #3: Loss:1.0761, Accuracy:0.3856, Validation Loss:1.0746, Validation Accuracy:0.3826\n",
    "Epoch #4: Loss:1.0750, Accuracy:0.3934, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0743, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #9: Loss:1.0744, Accuracy:0.3922, Validation Loss:1.0740, Validation Accuracy:0.4056\n",
    "Epoch #10: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #11: Loss:1.0745, Accuracy:0.3930, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0742, Accuracy:0.3930, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #15: Loss:1.0742, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.4007\n",
    "Epoch #16: Loss:1.0743, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0741, Accuracy:0.3955, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #19: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #20: Loss:1.0741, Accuracy:0.3926, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #21: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #22: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0732, Validation Accuracy:0.4023\n",
    "Epoch #23: Loss:1.0741, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #24: Loss:1.0740, Accuracy:0.3951, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #25: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #28: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0731, Validation Accuracy:0.4023\n",
    "Epoch #29: Loss:1.0738, Accuracy:0.3938, Validation Loss:1.0730, Validation Accuracy:0.4007\n",
    "Epoch #30: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0730, Validation Accuracy:0.4023\n",
    "Epoch #31: Loss:1.0739, Accuracy:0.3963, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #32: Loss:1.0740, Accuracy:0.3951, Validation Loss:1.0731, Validation Accuracy:0.4023\n",
    "Epoch #33: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #34: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.4039\n",
    "Epoch #35: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #36: Loss:1.0741, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #37: Loss:1.0739, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #38: Loss:1.0739, Accuracy:0.3955, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #39: Loss:1.0740, Accuracy:0.3959, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0740, Accuracy:0.3979, Validation Loss:1.0733, Validation Accuracy:0.3990\n",
    "Epoch #41: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #43: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #44: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #45: Loss:1.0739, Accuracy:0.3955, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #46: Loss:1.0741, Accuracy:0.3988, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #47: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #48: Loss:1.0739, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #49: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #50: Loss:1.0740, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #51: Loss:1.0739, Accuracy:0.3971, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #52: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.4007\n",
    "Epoch #53: Loss:1.0740, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.4007\n",
    "Epoch #54: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #55: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #57: Loss:1.0739, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #58: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #59: Loss:1.0741, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #60: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0746, Accuracy:0.3873, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #64: Loss:1.0741, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #65: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #66: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #67: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #68: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #69: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #70: Loss:1.0740, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #72: Loss:1.0737, Accuracy:0.3951, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #73: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #74: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.4039\n",
    "Epoch #75: Loss:1.0738, Accuracy:0.3984, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #76: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #77: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #78: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #79: Loss:1.0736, Accuracy:0.3992, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #80: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #81: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #82: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.4039\n",
    "Epoch #83: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.4072\n",
    "Epoch #84: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #85: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #86: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #87: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #88: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #89: Loss:1.0736, Accuracy:0.3934, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #90: Loss:1.0734, Accuracy:0.3955, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #91: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #92: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.4023\n",
    "Epoch #93: Loss:1.0732, Accuracy:0.3967, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #94: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #95: Loss:1.0734, Accuracy:0.3922, Validation Loss:1.0730, Validation Accuracy:0.4007\n",
    "Epoch #96: Loss:1.0732, Accuracy:0.3967, Validation Loss:1.0731, Validation Accuracy:0.4039\n",
    "Epoch #97: Loss:1.0733, Accuracy:0.4016, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #98: Loss:1.0732, Accuracy:0.4033, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #99: Loss:1.0731, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #100: Loss:1.0730, Accuracy:0.3967, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #101: Loss:1.0732, Accuracy:0.3967, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #102: Loss:1.0731, Accuracy:0.4000, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #103: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #104: Loss:1.0727, Accuracy:0.4033, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #105: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #106: Loss:1.0728, Accuracy:0.3979, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #107: Loss:1.0727, Accuracy:0.3963, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #108: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #109: Loss:1.0728, Accuracy:0.3988, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #110: Loss:1.0732, Accuracy:0.3988, Validation Loss:1.0733, Validation Accuracy:0.4056\n",
    "Epoch #111: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #112: Loss:1.0725, Accuracy:0.3975, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #113: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #114: Loss:1.0727, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #115: Loss:1.0729, Accuracy:0.4016, Validation Loss:1.0733, Validation Accuracy:0.3990\n",
    "Epoch #116: Loss:1.0725, Accuracy:0.3926, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #117: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #118: Loss:1.0724, Accuracy:0.3979, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #119: Loss:1.0725, Accuracy:0.4016, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #120: Loss:1.0721, Accuracy:0.4008, Validation Loss:1.0734, Validation Accuracy:0.4007\n",
    "Epoch #121: Loss:1.0727, Accuracy:0.3938, Validation Loss:1.0733, Validation Accuracy:0.3826\n",
    "Epoch #122: Loss:1.0720, Accuracy:0.4016, Validation Loss:1.0733, Validation Accuracy:0.3990\n",
    "Epoch #123: Loss:1.0721, Accuracy:0.3975, Validation Loss:1.0734, Validation Accuracy:0.4039\n",
    "Epoch #124: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #125: Loss:1.0720, Accuracy:0.3988, Validation Loss:1.0731, Validation Accuracy:0.3859\n",
    "Epoch #126: Loss:1.0721, Accuracy:0.3984, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #127: Loss:1.0720, Accuracy:0.3951, Validation Loss:1.0731, Validation Accuracy:0.3826\n",
    "Epoch #128: Loss:1.0725, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3826\n",
    "Epoch #129: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #130: Loss:1.0724, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #131: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0734, Validation Accuracy:0.4007\n",
    "Epoch #132: Loss:1.0716, Accuracy:0.3881, Validation Loss:1.0735, Validation Accuracy:0.3859\n",
    "Epoch #133: Loss:1.0722, Accuracy:0.3992, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #134: Loss:1.0724, Accuracy:0.4025, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #135: Loss:1.0715, Accuracy:0.3955, Validation Loss:1.0733, Validation Accuracy:0.3875\n",
    "Epoch #136: Loss:1.0723, Accuracy:0.3893, Validation Loss:1.0731, Validation Accuracy:0.3875\n",
    "Epoch #137: Loss:1.0717, Accuracy:0.3967, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #138: Loss:1.0718, Accuracy:0.3988, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #139: Loss:1.0716, Accuracy:0.3984, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #140: Loss:1.0717, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.4023\n",
    "Epoch #141: Loss:1.0717, Accuracy:0.3959, Validation Loss:1.0737, Validation Accuracy:0.4023\n",
    "Epoch #142: Loss:1.0721, Accuracy:0.3984, Validation Loss:1.0738, Validation Accuracy:0.4039\n",
    "Epoch #143: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0730, Validation Accuracy:0.4023\n",
    "Epoch #144: Loss:1.0716, Accuracy:0.3922, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #145: Loss:1.0717, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #146: Loss:1.0718, Accuracy:0.3979, Validation Loss:1.0736, Validation Accuracy:0.4039\n",
    "Epoch #147: Loss:1.0717, Accuracy:0.3984, Validation Loss:1.0736, Validation Accuracy:0.4039\n",
    "Epoch #148: Loss:1.0715, Accuracy:0.3971, Validation Loss:1.0732, Validation Accuracy:0.3875\n",
    "Epoch #149: Loss:1.0720, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3859\n",
    "Epoch #150: Loss:1.0718, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3810\n",
    "Epoch #151: Loss:1.0720, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #152: Loss:1.0718, Accuracy:0.3959, Validation Loss:1.0731, Validation Accuracy:0.3974\n",
    "Epoch #153: Loss:1.0718, Accuracy:0.3947, Validation Loss:1.0730, Validation Accuracy:0.3859\n",
    "Epoch #154: Loss:1.0718, Accuracy:0.3930, Validation Loss:1.0730, Validation Accuracy:0.3793\n",
    "Epoch #155: Loss:1.0716, Accuracy:0.3930, Validation Loss:1.0725, Validation Accuracy:0.3859\n",
    "Epoch #156: Loss:1.0716, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #157: Loss:1.0715, Accuracy:0.3955, Validation Loss:1.0728, Validation Accuracy:0.3875\n",
    "Epoch #158: Loss:1.0714, Accuracy:0.3930, Validation Loss:1.0727, Validation Accuracy:0.3842\n",
    "Epoch #159: Loss:1.0715, Accuracy:0.3947, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #160: Loss:1.0715, Accuracy:0.3926, Validation Loss:1.0721, Validation Accuracy:0.3859\n",
    "Epoch #161: Loss:1.0721, Accuracy:0.3959, Validation Loss:1.0721, Validation Accuracy:0.3924\n",
    "Epoch #162: Loss:1.0709, Accuracy:0.3984, Validation Loss:1.0725, Validation Accuracy:0.4023\n",
    "Epoch #163: Loss:1.0720, Accuracy:0.3967, Validation Loss:1.0723, Validation Accuracy:0.3892\n",
    "Epoch #164: Loss:1.0710, Accuracy:0.3926, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #165: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0728, Validation Accuracy:0.3859\n",
    "Epoch #166: Loss:1.0716, Accuracy:0.3979, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #167: Loss:1.0708, Accuracy:0.3930, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #168: Loss:1.0719, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.3859\n",
    "Epoch #169: Loss:1.0720, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #170: Loss:1.0716, Accuracy:0.3963, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #171: Loss:1.0707, Accuracy:0.3951, Validation Loss:1.0726, Validation Accuracy:0.3892\n",
    "Epoch #172: Loss:1.0711, Accuracy:0.3967, Validation Loss:1.0729, Validation Accuracy:0.3990\n",
    "Epoch #173: Loss:1.0708, Accuracy:0.3926, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #174: Loss:1.0722, Accuracy:0.3926, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #175: Loss:1.0711, Accuracy:0.4016, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #176: Loss:1.0714, Accuracy:0.3951, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #177: Loss:1.0710, Accuracy:0.3943, Validation Loss:1.0725, Validation Accuracy:0.3924\n",
    "Epoch #178: Loss:1.0705, Accuracy:0.3947, Validation Loss:1.0724, Validation Accuracy:0.3990\n",
    "Epoch #179: Loss:1.0707, Accuracy:0.3975, Validation Loss:1.0724, Validation Accuracy:0.3974\n",
    "Epoch #180: Loss:1.0709, Accuracy:0.3951, Validation Loss:1.0722, Validation Accuracy:0.4023\n",
    "Epoch #181: Loss:1.0714, Accuracy:0.3988, Validation Loss:1.0723, Validation Accuracy:0.3875\n",
    "Epoch #182: Loss:1.0706, Accuracy:0.3959, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #183: Loss:1.0705, Accuracy:0.3967, Validation Loss:1.0723, Validation Accuracy:0.4007\n",
    "Epoch #184: Loss:1.0709, Accuracy:0.4029, Validation Loss:1.0723, Validation Accuracy:0.3892\n",
    "Epoch #185: Loss:1.0705, Accuracy:0.3963, Validation Loss:1.0731, Validation Accuracy:0.4023\n",
    "Epoch #186: Loss:1.0704, Accuracy:0.3988, Validation Loss:1.0723, Validation Accuracy:0.4039\n",
    "Epoch #187: Loss:1.0703, Accuracy:0.3922, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #188: Loss:1.0699, Accuracy:0.3934, Validation Loss:1.0728, Validation Accuracy:0.4023\n",
    "Epoch #189: Loss:1.0697, Accuracy:0.3992, Validation Loss:1.0724, Validation Accuracy:0.3990\n",
    "Epoch #190: Loss:1.0696, Accuracy:0.4000, Validation Loss:1.0727, Validation Accuracy:0.4007\n",
    "Epoch #191: Loss:1.0693, Accuracy:0.3984, Validation Loss:1.0728, Validation Accuracy:0.3990\n",
    "Epoch #192: Loss:1.0692, Accuracy:0.3975, Validation Loss:1.0721, Validation Accuracy:0.3974\n",
    "Epoch #193: Loss:1.0694, Accuracy:0.4033, Validation Loss:1.0719, Validation Accuracy:0.4056\n",
    "Epoch #194: Loss:1.0696, Accuracy:0.3938, Validation Loss:1.0728, Validation Accuracy:0.3974\n",
    "Epoch #195: Loss:1.0692, Accuracy:0.3975, Validation Loss:1.0721, Validation Accuracy:0.4072\n",
    "Epoch #196: Loss:1.0690, Accuracy:0.4029, Validation Loss:1.0721, Validation Accuracy:0.4089\n",
    "Epoch #197: Loss:1.0687, Accuracy:0.4000, Validation Loss:1.0724, Validation Accuracy:0.3957\n",
    "Epoch #198: Loss:1.0696, Accuracy:0.4025, Validation Loss:1.0720, Validation Accuracy:0.4105\n",
    "Epoch #199: Loss:1.0693, Accuracy:0.3996, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #200: Loss:1.0692, Accuracy:0.3996, Validation Loss:1.0722, Validation Accuracy:0.4122\n",
    "Epoch #201: Loss:1.0690, Accuracy:0.4066, Validation Loss:1.0725, Validation Accuracy:0.4105\n",
    "Epoch #202: Loss:1.0688, Accuracy:0.4053, Validation Loss:1.0728, Validation Accuracy:0.4089\n",
    "Epoch #203: Loss:1.0691, Accuracy:0.3984, Validation Loss:1.0726, Validation Accuracy:0.3957\n",
    "Epoch #204: Loss:1.0690, Accuracy:0.4000, Validation Loss:1.0720, Validation Accuracy:0.3957\n",
    "Epoch #205: Loss:1.0699, Accuracy:0.3992, Validation Loss:1.0722, Validation Accuracy:0.4089\n",
    "Epoch #206: Loss:1.0690, Accuracy:0.3996, Validation Loss:1.0715, Validation Accuracy:0.4122\n",
    "Epoch #207: Loss:1.0690, Accuracy:0.3988, Validation Loss:1.0717, Validation Accuracy:0.4122\n",
    "Epoch #208: Loss:1.0691, Accuracy:0.3943, Validation Loss:1.0723, Validation Accuracy:0.4122\n",
    "Epoch #209: Loss:1.0690, Accuracy:0.3975, Validation Loss:1.0717, Validation Accuracy:0.4056\n",
    "Epoch #210: Loss:1.0689, Accuracy:0.3984, Validation Loss:1.0716, Validation Accuracy:0.4089\n",
    "Epoch #211: Loss:1.0692, Accuracy:0.3951, Validation Loss:1.0718, Validation Accuracy:0.3941\n",
    "Epoch #212: Loss:1.0695, Accuracy:0.3975, Validation Loss:1.0719, Validation Accuracy:0.4089\n",
    "Epoch #213: Loss:1.0684, Accuracy:0.4000, Validation Loss:1.0722, Validation Accuracy:0.4171\n",
    "Epoch #214: Loss:1.0683, Accuracy:0.4000, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #215: Loss:1.0688, Accuracy:0.4000, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #216: Loss:1.0693, Accuracy:0.4103, Validation Loss:1.0732, Validation Accuracy:0.3826\n",
    "Epoch #217: Loss:1.0686, Accuracy:0.4131, Validation Loss:1.0737, Validation Accuracy:0.4023\n",
    "Epoch #218: Loss:1.0682, Accuracy:0.4016, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #219: Loss:1.0693, Accuracy:0.4041, Validation Loss:1.0729, Validation Accuracy:0.3990\n",
    "Epoch #220: Loss:1.0692, Accuracy:0.4033, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #221: Loss:1.0682, Accuracy:0.4016, Validation Loss:1.0730, Validation Accuracy:0.3875\n",
    "Epoch #222: Loss:1.0684, Accuracy:0.4057, Validation Loss:1.0725, Validation Accuracy:0.3924\n",
    "Epoch #223: Loss:1.0682, Accuracy:0.4021, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #224: Loss:1.0684, Accuracy:0.4062, Validation Loss:1.0726, Validation Accuracy:0.3974\n",
    "Epoch #225: Loss:1.0679, Accuracy:0.4070, Validation Loss:1.0725, Validation Accuracy:0.3924\n",
    "Epoch #226: Loss:1.0684, Accuracy:0.4082, Validation Loss:1.0725, Validation Accuracy:0.3892\n",
    "Epoch #227: Loss:1.0677, Accuracy:0.4053, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #228: Loss:1.0681, Accuracy:0.4041, Validation Loss:1.0734, Validation Accuracy:0.4072\n",
    "Epoch #229: Loss:1.0679, Accuracy:0.4049, Validation Loss:1.0728, Validation Accuracy:0.3974\n",
    "Epoch #230: Loss:1.0678, Accuracy:0.4045, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #231: Loss:1.0683, Accuracy:0.4033, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #232: Loss:1.0687, Accuracy:0.4082, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #233: Loss:1.0673, Accuracy:0.4045, Validation Loss:1.0726, Validation Accuracy:0.4039\n",
    "Epoch #234: Loss:1.0683, Accuracy:0.4049, Validation Loss:1.0721, Validation Accuracy:0.4039\n",
    "Epoch #235: Loss:1.0678, Accuracy:0.4066, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #236: Loss:1.0677, Accuracy:0.4062, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #237: Loss:1.0673, Accuracy:0.4062, Validation Loss:1.0735, Validation Accuracy:0.4056\n",
    "Epoch #238: Loss:1.0675, Accuracy:0.4066, Validation Loss:1.0733, Validation Accuracy:0.4039\n",
    "Epoch #239: Loss:1.0677, Accuracy:0.4053, Validation Loss:1.0728, Validation Accuracy:0.3957\n",
    "Epoch #240: Loss:1.0673, Accuracy:0.4045, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #241: Loss:1.0675, Accuracy:0.4062, Validation Loss:1.0726, Validation Accuracy:0.4056\n",
    "Epoch #242: Loss:1.0674, Accuracy:0.4049, Validation Loss:1.0725, Validation Accuracy:0.4039\n",
    "Epoch #243: Loss:1.0677, Accuracy:0.4074, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #244: Loss:1.0680, Accuracy:0.4016, Validation Loss:1.0725, Validation Accuracy:0.3974\n",
    "Epoch #245: Loss:1.0679, Accuracy:0.4086, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #246: Loss:1.0676, Accuracy:0.4066, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #247: Loss:1.0681, Accuracy:0.4062, Validation Loss:1.0726, Validation Accuracy:0.3892\n",
    "Epoch #248: Loss:1.0691, Accuracy:0.4033, Validation Loss:1.0744, Validation Accuracy:0.4122\n",
    "Epoch #249: Loss:1.0678, Accuracy:0.4107, Validation Loss:1.0721, Validation Accuracy:0.3924\n",
    "Epoch #250: Loss:1.0685, Accuracy:0.4037, Validation Loss:1.0717, Validation Accuracy:0.3875\n",
    "Epoch #251: Loss:1.0677, Accuracy:0.4008, Validation Loss:1.0723, Validation Accuracy:0.4023\n",
    "Epoch #252: Loss:1.0678, Accuracy:0.4111, Validation Loss:1.0714, Validation Accuracy:0.4089\n",
    "Epoch #253: Loss:1.0675, Accuracy:0.4090, Validation Loss:1.0717, Validation Accuracy:0.4072\n",
    "Epoch #254: Loss:1.0679, Accuracy:0.4070, Validation Loss:1.0724, Validation Accuracy:0.4056\n",
    "Epoch #255: Loss:1.0671, Accuracy:0.4057, Validation Loss:1.0724, Validation Accuracy:0.4056\n",
    "Epoch #256: Loss:1.0668, Accuracy:0.4107, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #257: Loss:1.0671, Accuracy:0.4090, Validation Loss:1.0721, Validation Accuracy:0.4171\n",
    "Epoch #258: Loss:1.0676, Accuracy:0.4033, Validation Loss:1.0726, Validation Accuracy:0.4138\n",
    "Epoch #259: Loss:1.0680, Accuracy:0.4041, Validation Loss:1.0725, Validation Accuracy:0.4039\n",
    "Epoch #260: Loss:1.0681, Accuracy:0.4074, Validation Loss:1.0730, Validation Accuracy:0.4105\n",
    "Epoch #261: Loss:1.0682, Accuracy:0.4053, Validation Loss:1.0723, Validation Accuracy:0.4138\n",
    "Epoch #262: Loss:1.0680, Accuracy:0.4057, Validation Loss:1.0728, Validation Accuracy:0.4220\n",
    "Epoch #263: Loss:1.0678, Accuracy:0.4099, Validation Loss:1.0724, Validation Accuracy:0.4204\n",
    "Epoch #264: Loss:1.0679, Accuracy:0.4045, Validation Loss:1.0725, Validation Accuracy:0.4056\n",
    "Epoch #265: Loss:1.0676, Accuracy:0.4066, Validation Loss:1.0732, Validation Accuracy:0.4171\n",
    "Epoch #266: Loss:1.0679, Accuracy:0.4078, Validation Loss:1.0724, Validation Accuracy:0.4154\n",
    "Epoch #267: Loss:1.0684, Accuracy:0.4111, Validation Loss:1.0715, Validation Accuracy:0.4105\n",
    "Epoch #268: Loss:1.0677, Accuracy:0.4062, Validation Loss:1.0726, Validation Accuracy:0.4236\n",
    "Epoch #269: Loss:1.0678, Accuracy:0.4103, Validation Loss:1.0717, Validation Accuracy:0.4154\n",
    "Epoch #270: Loss:1.0673, Accuracy:0.4103, Validation Loss:1.0719, Validation Accuracy:0.4204\n",
    "Epoch #271: Loss:1.0673, Accuracy:0.4045, Validation Loss:1.0725, Validation Accuracy:0.4089\n",
    "Epoch #272: Loss:1.0677, Accuracy:0.4086, Validation Loss:1.0735, Validation Accuracy:0.4220\n",
    "Epoch #273: Loss:1.0691, Accuracy:0.4099, Validation Loss:1.0729, Validation Accuracy:0.4187\n",
    "Epoch #274: Loss:1.0688, Accuracy:0.4070, Validation Loss:1.0727, Validation Accuracy:0.4138\n",
    "Epoch #275: Loss:1.0685, Accuracy:0.4115, Validation Loss:1.0734, Validation Accuracy:0.4220\n",
    "Epoch #276: Loss:1.0689, Accuracy:0.4115, Validation Loss:1.0736, Validation Accuracy:0.4204\n",
    "Epoch #277: Loss:1.0683, Accuracy:0.4107, Validation Loss:1.0730, Validation Accuracy:0.4204\n",
    "Epoch #278: Loss:1.0689, Accuracy:0.4099, Validation Loss:1.0732, Validation Accuracy:0.4204\n",
    "Epoch #279: Loss:1.0682, Accuracy:0.4103, Validation Loss:1.0730, Validation Accuracy:0.4187\n",
    "Epoch #280: Loss:1.0682, Accuracy:0.4099, Validation Loss:1.0730, Validation Accuracy:0.4220\n",
    "Epoch #281: Loss:1.0684, Accuracy:0.4119, Validation Loss:1.0724, Validation Accuracy:0.4154\n",
    "Epoch #282: Loss:1.0683, Accuracy:0.4049, Validation Loss:1.0721, Validation Accuracy:0.4171\n",
    "Epoch #283: Loss:1.0683, Accuracy:0.4094, Validation Loss:1.0732, Validation Accuracy:0.4122\n",
    "Epoch #284: Loss:1.0680, Accuracy:0.4041, Validation Loss:1.0727, Validation Accuracy:0.4089\n",
    "Epoch #285: Loss:1.0679, Accuracy:0.4078, Validation Loss:1.0735, Validation Accuracy:0.4204\n",
    "Epoch #286: Loss:1.0681, Accuracy:0.4103, Validation Loss:1.0728, Validation Accuracy:0.4204\n",
    "Epoch #287: Loss:1.0686, Accuracy:0.4074, Validation Loss:1.0727, Validation Accuracy:0.4154\n",
    "Epoch #288: Loss:1.0684, Accuracy:0.4094, Validation Loss:1.0730, Validation Accuracy:0.4138\n",
    "Epoch #289: Loss:1.0681, Accuracy:0.4074, Validation Loss:1.0727, Validation Accuracy:0.4171\n",
    "Epoch #290: Loss:1.0681, Accuracy:0.4111, Validation Loss:1.0731, Validation Accuracy:0.4171\n",
    "Epoch #291: Loss:1.0682, Accuracy:0.4144, Validation Loss:1.0733, Validation Accuracy:0.4204\n",
    "Epoch #292: Loss:1.0680, Accuracy:0.4057, Validation Loss:1.0731, Validation Accuracy:0.4187\n",
    "Epoch #293: Loss:1.0682, Accuracy:0.4115, Validation Loss:1.0728, Validation Accuracy:0.4171\n",
    "Epoch #294: Loss:1.0679, Accuracy:0.4119, Validation Loss:1.0728, Validation Accuracy:0.4187\n",
    "Epoch #295: Loss:1.0678, Accuracy:0.4131, Validation Loss:1.0724, Validation Accuracy:0.4171\n",
    "Epoch #296: Loss:1.0681, Accuracy:0.4119, Validation Loss:1.0720, Validation Accuracy:0.4105\n",
    "Epoch #297: Loss:1.0680, Accuracy:0.4136, Validation Loss:1.0718, Validation Accuracy:0.4056\n",
    "Epoch #298: Loss:1.0676, Accuracy:0.4140, Validation Loss:1.0714, Validation Accuracy:0.4072\n",
    "Epoch #299: Loss:1.0677, Accuracy:0.4144, Validation Loss:1.0714, Validation Accuracy:0.4039\n",
    "Epoch #300: Loss:1.0679, Accuracy:0.4144, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07282495, Accuracy:0.4056\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  145   95   0\n",
    "t:02  125  102   0\n",
    "t:03   97   45   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.40      0.60      0.48       240\n",
    "          02       0.42      0.45      0.43       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.41       609\n",
    "   macro avg       0.27      0.35      0.30       609\n",
    "weighted avg       0.31      0.41      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 07:45:39 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 25 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.082214760075649, 1.0768670194058974, 1.0745500038410056, 1.074761002130305, 1.0750427161922986, 1.0744357749159112, 1.0739810719278646, 1.0739627256377773, 1.0740126578874385, 1.0741554708120662, 1.0740310814971799, 1.0738114881985292, 1.0735721200557764, 1.07345350407223, 1.0734178172347972, 1.0734103769308632, 1.0732438231532406, 1.073311736039536, 1.0733255936790178, 1.0732868144469112, 1.0731892597499153, 1.0732450911955684, 1.073322647702322, 1.0733145318790804, 1.073329406027332, 1.0733947335009897, 1.0732180267719214, 1.0730537078259221, 1.0729973819259744, 1.0730003188983561, 1.0729545767867115, 1.073085665115582, 1.073215848315134, 1.0733836846202855, 1.0732906113312946, 1.073286747697539, 1.0732483319656798, 1.0732957367435074, 1.0732197556002387, 1.0733107351904432, 1.0731068728201103, 1.0733499260763033, 1.0734447262361524, 1.073434576808134, 1.073349892995236, 1.0733845740899273, 1.0733230848030504, 1.0733307500190923, 1.073338645823875, 1.0734220981989393, 1.0734234096968702, 1.073422694245387, 1.073417072812912, 1.073417481921968, 1.0734149178456398, 1.0734373274303617, 1.073424619798394, 1.0734240075050316, 1.0734398269105232, 1.0734449307906804, 1.0734237301330065, 1.0733905165457764, 1.0735591009919867, 1.0733930553708757, 1.0734138594472349, 1.073361590773797, 1.0733674864463618, 1.0733578475238068, 1.0733715834093016, 1.0733498225266906, 1.0733361698332287, 1.0733188993629368, 1.0733734124595504, 1.0733212712167324, 1.073309273751107, 1.0732953701113246, 1.0732917182746975, 1.0733864342637838, 1.0732991348933705, 1.0736457847413563, 1.0734024736877341, 1.073275561990409, 1.073327548006681, 1.0732627868260851, 1.0732266121897205, 1.0732083101382202, 1.0732284633592628, 1.0732484946305725, 1.0732271074269988, 1.0731153995141216, 1.0732766113844998, 1.0731867577250565, 1.0731803572236611, 1.0732153812652738, 1.0730437664758592, 1.0730859941645405, 1.073027048596412, 1.0730173566267016, 1.0730010092943565, 1.0730006444238873, 1.0729508433240191, 1.072990046150383, 1.0728726886176123, 1.0728577622051896, 1.0729120151554226, 1.072863791572245, 1.0728110102401383, 1.0728592220785582, 1.0733715434771258, 1.0732625861864764, 1.073638409229335, 1.0735770489390457, 1.0732095077120025, 1.0732851901469365, 1.0733345028606347, 1.0735350502731373, 1.0735751423733966, 1.073347118883493, 1.073271896255819, 1.073394492537713, 1.073264036664039, 1.0733232435530238, 1.0733766266082112, 1.07322489804235, 1.073141117401311, 1.0730890855804844, 1.073129006207283, 1.0734092832981854, 1.073984126739314, 1.0740136385746972, 1.0734467547515343, 1.0734851790962157, 1.0732106395151424, 1.0736419185629031, 1.073285865470498, 1.0731306821841913, 1.0738997929201926, 1.073478828901532, 1.0735035242118272, 1.073578412896894, 1.0736871531052739, 1.073846347226298, 1.073005423952989, 1.0730012481044275, 1.0733956321706912, 1.0736323221172215, 1.0735809934158826, 1.0731791987990706, 1.0732774121812216, 1.0733786441618194, 1.0731972113422963, 1.0731488898861388, 1.073004019475727, 1.0730377899602128, 1.0725054969928536, 1.0733198430542092, 1.072834138408279, 1.0727369472115302, 1.0728045435766085, 1.0721444494422825, 1.0720557446158774, 1.072474762332459, 1.072256346445757, 1.0725149740334998, 1.0727884260500202, 1.0735055907019253, 1.0729529683421595, 1.0732066044079258, 1.0744908543055869, 1.0732378793271695, 1.0726121100298878, 1.0728776069305996, 1.0735669071451197, 1.0730379038843616, 1.073026668262012, 1.0723564057123094, 1.0725160886109952, 1.0723575743156897, 1.0724355493273054, 1.0722246352087688, 1.0723019994929899, 1.0729587107456375, 1.0723220166705905, 1.072330063199762, 1.0731093677981147, 1.0723195737610114, 1.0723725939031892, 1.0728166909836392, 1.0724284768300298, 1.0726870415833196, 1.0728079548414509, 1.072111399302929, 1.071937240207528, 1.072752980175864, 1.0720666271125154, 1.0720820352557454, 1.0724417947979004, 1.072032855062062, 1.0727498114402658, 1.0721814603053879, 1.0724517790163288, 1.0727732884277068, 1.072611849296269, 1.0719533661511926, 1.072229548628107, 1.0715034051090235, 1.0717145993400286, 1.0723193124401549, 1.0717015965231533, 1.0716081466189356, 1.0718070157055783, 1.0719137301390198, 1.0722420280202856, 1.0731842075466913, 1.0732046700463507, 1.0731676928515506, 1.0737395161282643, 1.072962063090946, 1.0729155769489083, 1.0734172452651025, 1.0729646097458838, 1.072455123140307, 1.0730510599703234, 1.0725826199223059, 1.0724572328902622, 1.072546602469947, 1.0730540527303034, 1.0734235995704513, 1.0728037202691014, 1.0729343476162363, 1.073043397103233, 1.073778583498424, 1.0725747166791768, 1.0721207479342256, 1.0734854627321115, 1.0730214365597428, 1.0734975003256586, 1.0733489780786198, 1.072837706465635, 1.0735444400110856, 1.0726003308210075, 1.0725371947233704, 1.0727814453576, 1.0725290270274497, 1.073196905782853, 1.0725460582961786, 1.0725666251284345, 1.0744188928056038, 1.072110164928906, 1.0716701336877883, 1.0722998836749098, 1.0714212984874332, 1.071744667093938, 1.07240353272662, 1.0724395212085767, 1.0733144136485209, 1.0721246499341892, 1.0726134187873753, 1.0724930958990588, 1.0729566601109621, 1.0722871147744566, 1.07281369235128, 1.0723750857492582, 1.072499873015681, 1.073212484029322, 1.0723819235471277, 1.071459378906463, 1.0725767397136718, 1.071694800810665, 1.0718654384362483, 1.072463284768103, 1.0735172563781488, 1.0729167257819465, 1.0727048182526637, 1.0734214093689065, 1.0736108595514533, 1.0729724656184907, 1.0732120234390785, 1.0730493049120473, 1.0730442675855163, 1.0723640840433306, 1.072095503556513, 1.0732378200161437, 1.072687970985137, 1.0734683504245552, 1.0728057577887975, 1.072663085409769, 1.0729544951606462, 1.0727312308422645, 1.0730706795878795, 1.0732864184528346, 1.0731436687541518, 1.072819602509046, 1.072758284891376, 1.072445010904021, 1.071961961552036, 1.0718105892438214, 1.0714381758998377, 1.0714191071114125, 1.0728250027485864], 'val_acc': [0.38587848810335296, 0.36453201799165635, 0.3825944157558905, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.40065681332437864, 0.40558292169876287, 0.4006568134222516, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.4022988494491734, 0.40065681332437864, 0.3940886686294537, 0.3940886686294537, 0.39737274107478915, 0.3957307049499944, 0.4022988494491734, 0.4039408856718411, 0.40229884954704637, 0.40229884954704637, 0.4022988494491734, 0.39408866882519966, 0.3940886686294537, 0.39244663250465894, 0.4022988494491734, 0.40065681332437864, 0.4022988494491734, 0.39737274117266214, 0.4022988494491734, 0.39737274107478915, 0.4039408855739681, 0.4022988494491734, 0.40065681332437864, 0.3924466327004049, 0.3908045964777372, 0.39408866882519966, 0.3990147771995839, 0.39408866882519966, 0.3908045964777372, 0.3990147771995839, 0.3990147771995839, 0.39737274117266214, 0.39573070475424843, 0.40065681332437864, 0.39408866892307265, 0.40065681332437864, 0.4022988494491734, 0.39737274107478915, 0.40065681332437864, 0.40065681332437864, 0.39737274107478915, 0.39408866882519966, 0.39737274107478915, 0.39737274107478915, 0.3990147771995839, 0.3957307049499944, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.39737274117266214, 0.39408866882519966, 0.3940886686294537, 0.3990147771995839, 0.39737274107478915, 0.3940886686294537, 0.3940886686294537, 0.39408866882519966, 0.40065681332437864, 0.3924466327004049, 0.40229884954704637, 0.4039408857697141, 0.3924466327004049, 0.3924466327004049, 0.3924466327004049, 0.40229884954704637, 0.4006568134222516, 0.3940886686294537, 0.3940886686294537, 0.4039408857697141, 0.40722495811717657, 0.39737274107478915, 0.4039408857697141, 0.3990147772974569, 0.3924466327004049, 0.40065681332437864, 0.40065681332437864, 0.3990147772974569, 0.3957307051457404, 0.40229884954704637, 0.3990147771995839, 0.3940886686294537, 0.4006568134222516, 0.4039408858675871, 0.3957307049499944, 0.4039408857697141, 0.39737274107478915, 0.39737274107478915, 0.4039408857697141, 0.3957307049499944, 0.39737274107478915, 0.40229884954704637, 0.40229884954704637, 0.39737274107478915, 0.4039408855739681, 0.40229884954704637, 0.40229884954704637, 0.40558292189450884, 0.39737274107478915, 0.4006568134222516, 0.3990147771995839, 0.4006568134222516, 0.3990147772974569, 0.3924466327982779, 0.3990147771995839, 0.40065681332437864, 0.40065681332437864, 0.4006568134222516, 0.3825944159516364, 0.3990147772974569, 0.4039408856718411, 0.3990147771995839, 0.38587848810335296, 0.38916256054868836, 0.3825944159516364, 0.3825944159516364, 0.39737274107478915, 0.40065681332437864, 0.40065681332437864, 0.38587848800547997, 0.3973727412705351, 0.39737274107478915, 0.3875205244238936, 0.3875205244238936, 0.4006568134222516, 0.39901477739532987, 0.3842364520764312, 0.4022988494491734, 0.40229884954704637, 0.4039408856718411, 0.40229884954704637, 0.4039408856718411, 0.40229884954704637, 0.4039408856718411, 0.4039408856718411, 0.3875205244238936, 0.3858784882990989, 0.3809523797289687, 0.3990147771995839, 0.39737274097691616, 0.385878487907607, 0.379310343310555, 0.38587848820122594, 0.39737274107478915, 0.3875205242281477, 0.3842364520764312, 0.4039408856718411, 0.38587848810335296, 0.3924466329940238, 0.4022988494491734, 0.38916256045081543, 0.3908045968692291, 0.38587848820122594, 0.39408866892307265, 0.39244663289615084, 0.38587848810335296, 0.39737274107478915, 0.39244663289615084, 0.38916256064656135, 0.3990147774932028, 0.3842364522721771, 0.3990147771995839, 0.40722495811717657, 0.39408866911881857, 0.39244663289615084, 0.3990147776889488, 0.39737274156415403, 0.40229885003641125, 0.3875205243260207, 0.39737274156415403, 0.4006568138137435, 0.38916256054868836, 0.4022988498406653, 0.403940886161206, 0.39408866892307265, 0.40229885003641125, 0.39901477739532987, 0.40065681371587053, 0.39901477778682176, 0.39737274136840806, 0.4055829216008899, 0.39737274156415403, 0.4072249577256846, 0.40886699394835235, 0.3957307052436133, 0.41050903026889307, 0.39408866911881857, 0.4121510663936878, 0.41050903026889307, 0.40886699414409833, 0.3957307052436133, 0.3957307049499944, 0.40886699414409833, 0.4121510664915608, 0.4121510664915608, 0.4121510664915608, 0.40558292169876287, 0.40886699414409833, 0.3940886690209456, 0.40886699414409833, 0.41707717486594503, 0.3940886690209456, 0.3990147774932028, 0.38259441585376347, 0.40229884974279234, 0.39737274136840806, 0.3990147774932028, 0.39737274136840806, 0.3875205243260207, 0.39244663260253193, 0.39080459667348316, 0.3973727412705351, 0.39244663260253193, 0.38916256035294244, 0.3990147774932028, 0.4072249582150495, 0.3973727412705351, 0.3973727412705351, 0.3973727412705351, 0.3990147774932028, 0.40394088596546, 0.40394088596546, 0.4022988498406653, 0.39737274136840806, 0.40558292218812775, 0.40394088596546, 0.3957307052436133, 0.40065681361799754, 0.40558292209025476, 0.40394088596546, 0.40558292209025476, 0.39737274136840806, 0.4006568135201246, 0.3908045965756102, 0.38916256054868836, 0.4121510662958148, 0.39244663289615084, 0.3875205243260207, 0.40229884974279234, 0.40886699414409833, 0.4072249583129225, 0.40558292209025476, 0.40558292209025476, 0.40229884993853826, 0.41707717476807205, 0.41379310242060957, 0.4039408857697141, 0.41050903026889307, 0.41379310261635555, 0.4220032830445833, 0.42036124701766153, 0.40558292189450884, 0.41707717476807205, 0.4154351387411503, 0.41050903036676606, 0.42364531916937803, 0.4154351387411503, 0.42036124701766153, 0.40886699404622534, 0.4220032830445833, 0.4187192107949938, 0.41379310251848256, 0.4220032830445833, 0.42036124691978854, 0.42036124691978854, 0.42036124691978854, 0.4187192107949938, 0.4220032830445833, 0.4154351385454043, 0.41707717437658015, 0.4121510662958148, 0.4088669937526064, 0.42036124682191556, 0.42036124652829665, 0.4154351384475313, 0.4137931023227366, 0.41707717427870716, 0.41707717427870716, 0.4203612462346777, 0.4187192104035019, 0.41707717427870716, 0.41871921010988294, 0.41707717427870716, 0.4105090296816552, 0.4055829216008899, 0.4072249574320657, 0.4039408847909843, 0.405582921209398], 'loss': [1.0941729200449324, 1.0804759101946007, 1.0761439759628484, 1.0749641123248812, 1.0749683368866938, 1.0748976075428958, 1.0744571896793906, 1.0742671085823732, 1.0744066487592348, 1.074339521445288, 1.0744753766598398, 1.0742640349409665, 1.0742536236373306, 1.0742448797460944, 1.0741905070428241, 1.0743323607121649, 1.0741895143990643, 1.0741470910195696, 1.074115077964579, 1.0740710047970563, 1.0739576565901112, 1.074045881892132, 1.0740686807299542, 1.0739682246037823, 1.0739175736292188, 1.0739741746894633, 1.073945231408309, 1.073959507149103, 1.0738239716700215, 1.0738253982160126, 1.0738828257124038, 1.0739733226000652, 1.0739535395859203, 1.0739605285303793, 1.0738592737264456, 1.0740953645177445, 1.0738963991709558, 1.0739296991967078, 1.0740394262317758, 1.0740316146208275, 1.0738664066522272, 1.0740017793017003, 1.0741201408100325, 1.0739328313901928, 1.0739411681094944, 1.0740988773486941, 1.0738782788204217, 1.073902157885338, 1.0738884906749215, 1.0739616697084242, 1.0738953701524518, 1.0739026154825575, 1.0739503253411953, 1.0738766667289656, 1.0739253431375022, 1.073921168560365, 1.0738511746913746, 1.0738308247109947, 1.074067882345932, 1.073799807384029, 1.0738734419585743, 1.0738941078558106, 1.074624306612191, 1.0740631802615688, 1.073780246926529, 1.0737629346044646, 1.0738725107063747, 1.0737282133689896, 1.0737238092344155, 1.073955851022223, 1.0737933305744274, 1.0736579847531642, 1.0737041071944657, 1.0736857098475618, 1.0737509017117948, 1.073597413313707, 1.0736050098094119, 1.0738652512278155, 1.07361111543017, 1.0738265638233944, 1.0739196907078705, 1.0735550512278593, 1.0736284762682122, 1.0735714699942962, 1.073449088024163, 1.0735271164523992, 1.0732870009400761, 1.0734415870917162, 1.0735998044513335, 1.073404156109146, 1.0733114102048307, 1.0734731382902643, 1.0731892955866194, 1.0736340609908839, 1.0733675451494096, 1.0731763891126096, 1.0733006160117273, 1.0731794566344433, 1.0731119769309825, 1.072971651735247, 1.0732382815966126, 1.0731308323157152, 1.0727852694552538, 1.072668509904364, 1.0729321802910838, 1.0727932265162223, 1.0726933512599561, 1.0726295714016079, 1.072844844334424, 1.0731957707806534, 1.0729227667226928, 1.0725433604673194, 1.072828707313146, 1.0727408921204553, 1.0728529849825943, 1.0725404876458327, 1.0724878498171388, 1.0724356407012783, 1.07249326671663, 1.0720714997461933, 1.0726567219904561, 1.0719799673777586, 1.0721288832306128, 1.0722659547715705, 1.0720057787591672, 1.0720585696261522, 1.0720497288008735, 1.0724842041180118, 1.072360156742699, 1.0724356450094579, 1.0733897620892379, 1.071631980627714, 1.072207154483521, 1.0724292297872429, 1.0715462715963564, 1.0722680491833227, 1.0716601630255917, 1.071819128050207, 1.071595158811957, 1.071660044648564, 1.0716606158250657, 1.0721217419822113, 1.0716402059218233, 1.0715920234362937, 1.0717184379605051, 1.071753213880488, 1.0716884466656913, 1.0715273993216012, 1.071950359266152, 1.071754864402865, 1.0720130032337667, 1.071803189939542, 1.0717605781261437, 1.0718235160291072, 1.0716288274808097, 1.071637446630662, 1.07149776717231, 1.0714227823261362, 1.071549844056429, 1.0715466642281848, 1.0721071537025655, 1.0709064570295737, 1.0720420117250948, 1.0709597956718115, 1.0716200827573115, 1.0716089557573292, 1.0708250559330965, 1.0718787805255678, 1.072022792005441, 1.0716004551803306, 1.0707394112306943, 1.0710874343554833, 1.0707987944448263, 1.0722272121441192, 1.071105675090265, 1.0713514085667823, 1.0710461499999435, 1.0705318167469096, 1.0707344396403193, 1.0709478823311276, 1.0713812118193453, 1.0705561577661815, 1.0705344336723155, 1.0709045126697612, 1.0705474966115776, 1.0703817521277394, 1.070251968264335, 1.0698531972309402, 1.0696576185050197, 1.0695519046372213, 1.0692906875140369, 1.0692435344876206, 1.069359832133111, 1.069636674632282, 1.0692369905095815, 1.0689785315026004, 1.0686544035984016, 1.0696144976899853, 1.0692786538380616, 1.0692495872352648, 1.0689515682216542, 1.0687976081758064, 1.0691427894196717, 1.0689639760238678, 1.069890909713886, 1.0690186515236293, 1.0690455931657639, 1.0691072981460383, 1.068950255401815, 1.068859664811246, 1.0692230097811815, 1.0694543690652083, 1.0683978974207227, 1.0682544947649664, 1.0688018567263469, 1.0692656749083032, 1.0685548660936297, 1.0682179361398214, 1.0692972451999203, 1.069211004500027, 1.0682483810663712, 1.0683891031042017, 1.0682408290232477, 1.0684400876689495, 1.0679400748051167, 1.0683716691250185, 1.0677168566098694, 1.0680880723303103, 1.0678619596747647, 1.067787584582883, 1.0683078440307836, 1.0686850039621154, 1.0673436395931049, 1.06828379107207, 1.0678228874715692, 1.0676790942891177, 1.0673039490192577, 1.0675008159398542, 1.0677344902340147, 1.0672518516223288, 1.0674777442180157, 1.0673981861412158, 1.0677012672169741, 1.0680121806123173, 1.0678667025889215, 1.0675984066369841, 1.0681116956949723, 1.0691015293465993, 1.0678053679652282, 1.0685234029190251, 1.0676922294883022, 1.067817962830561, 1.0675367969262282, 1.0679170468504668, 1.0671013018923374, 1.0667985214589801, 1.0671436256451774, 1.0675679562272966, 1.0680103014626787, 1.0681324986706524, 1.068213661397507, 1.068039076724826, 1.0678352814190686, 1.0678640964829211, 1.0675836551360771, 1.0679115636148002, 1.0683632988704548, 1.0676752937403058, 1.0677521499030644, 1.0672572055636491, 1.0673165942609188, 1.0677110173619013, 1.0690964034940182, 1.0687928409302259, 1.0684669346780014, 1.0688648191565606, 1.0682843137815503, 1.0689249615894452, 1.0681841982463547, 1.0681790935430193, 1.0683559252251345, 1.0682835977914642, 1.0683129199476458, 1.0680295944213867, 1.0678868260961294, 1.0681072665925389, 1.0685596657484708, 1.0684492439215187, 1.068101611715078, 1.0680881562908573, 1.068156047866085, 1.0679730004598473, 1.0681577150337016, 1.0678910454685437, 1.067773690164946, 1.0680547161023963, 1.068033049434607, 1.067616193642117, 1.0677277950290782, 1.0679238334573515], 'acc': [0.3724846019147603, 0.3913757696044029, 0.3856262833430782, 0.39342915926136274, 0.394250513714197, 0.39425051273506523, 0.39425051113173704, 0.39383983805439066, 0.39219712542802154, 0.3942505128941742, 0.39301847968502945, 0.39425051508498143, 0.39425051113173704, 0.39301848262242467, 0.39383983707525894, 0.3958932253981518, 0.39425051351837065, 0.39548254735171184, 0.3946611887498068, 0.39260780167530696, 0.3938398372710853, 0.3938398349211691, 0.39630390023793527, 0.3950718675795522, 0.3942505152808078, 0.3938398366836062, 0.3950718681670312, 0.39383983370949355, 0.3938398362919535, 0.3934291594571891, 0.39630390235530766, 0.39507186738372585, 0.3946611921155722, 0.39383983550864815, 0.3946611913322668, 0.3958932259489134, 0.39712525801981746, 0.3954825471558855, 0.3958932245781289, 0.39794661361089234, 0.39589322356227974, 0.39466119152809315, 0.39425051309000053, 0.3958932235989972, 0.3954825477433645, 0.3987679686512056, 0.39589322575308705, 0.397125256649033, 0.3946611889456332, 0.3950718701252947, 0.39712525524153114, 0.39589322340317085, 0.39589322536143434, 0.3950718681670312, 0.39383983413786366, 0.3938398362919535, 0.39548254735171184, 0.3950718693419893, 0.39383983527610433, 0.3946611927030512, 0.3942505148891551, 0.3942505129308916, 0.3872689928362257, 0.4008213536083331, 0.3963039031753305, 0.39548254437759917, 0.3946611887498068, 0.3958932261447397, 0.39425051449750237, 0.3971252564164892, 0.3934291584780574, 0.3950718701252947, 0.3942505141058497, 0.39671457883513683, 0.39835728750826155, 0.39425051391002336, 0.39507186836285757, 0.3938398374669116, 0.39917864391935926, 0.395071869146163, 0.3942505115233897, 0.3963039025878515, 0.396714580597574, 0.39671457883513683, 0.3975359324679482, 0.39835728809574056, 0.3967145809892267, 0.39671457961844225, 0.3934291598488418, 0.3954825485266699, 0.3983572893074161, 0.3963039000053915, 0.39671457781928765, 0.39425051449750237, 0.3921971260155006, 0.39671457781928765, 0.40164271123110634, 0.4032854226825174, 0.4004106799435077, 0.3967145770359823, 0.3967145772318086, 0.40000000173795885, 0.3967145770359823, 0.4032854232699964, 0.3995893240831716, 0.3979466108693234, 0.3963039010212407, 0.40082135716992484, 0.39876796512633134, 0.398767968100444, 0.3942505115233897, 0.3975359358337136, 0.4016427088811902, 0.3971252584114701, 0.40164271162275905, 0.39260780085528413, 0.40205339002413426, 0.3979466134517834, 0.4016427082569937, 0.4008213536083331, 0.39383983766273795, 0.4016427100194308, 0.3975359325046657, 0.39917864352770654, 0.39876796692548594, 0.3983572900907215, 0.3950718675795522, 0.3946611932905303, 0.3983572902498304, 0.39383983394203737, 0.3963039021594813, 0.3880903512055869, 0.39917864591434016, 0.40246406368895965, 0.3954825467642328, 0.38932238249318557, 0.39671457742763494, 0.39876796590963676, 0.39835729044565676, 0.39425051449750237, 0.3958932247739553, 0.39835728907487233, 0.3958932245781289, 0.3921971266029797, 0.3958932231706271, 0.3979466134517834, 0.3983572873124352, 0.397125256649033, 0.3942505146933287, 0.39466119191974586, 0.3942505121475862, 0.3958932245781289, 0.39466119191974586, 0.39301848164329295, 0.39301848203494566, 0.3971252578239911, 0.3954825481350172, 0.39301848262242467, 0.3946611889456332, 0.3926078034377441, 0.3958932247739553, 0.39835728750826155, 0.3967145770359823, 0.3926078040252231, 0.395893222583148, 0.3979466102818444, 0.39301848085998753, 0.39630390180454605, 0.39425051171921605, 0.3963039025878515, 0.3950718685586839, 0.39671457684015593, 0.39260780520018124, 0.39260780085528413, 0.4016427124060645, 0.3950718677753785, 0.39425051351837065, 0.39466119309470393, 0.39753593563788725, 0.3950718667595293, 0.39876796751296495, 0.39589322496978163, 0.39671457883513683, 0.4028747416986822, 0.3963039035669832, 0.3987679692754021, 0.392197123628867, 0.3934291588697101, 0.3991786427444012, 0.4000000015421325, 0.39835728809574056, 0.3975359358337136, 0.40328542287834374, 0.3938398362919535, 0.39753593465875553, 0.40287474388948946, 0.39999999798054076, 0.40246406705472504, 0.3995893221249081, 0.3995893207541237, 0.4065708442145549, 0.4053388103812137, 0.3983572902865478, 0.40000000212961156, 0.3991786441151856, 0.39958932294493094, 0.3987679657138104, 0.3942505123066951, 0.39753593207629556, 0.39835728848739327, 0.39507186836285757, 0.3975359330554273, 0.40000000173795885, 0.4000000007588271, 0.4000000011504798, 0.41026693862811253, 0.41314168489199643, 0.40164271021525716, 0.40410677494454433, 0.40328542229086467, 0.4016427124060645, 0.40574948662849913, 0.40205338963248155, 0.4061601644423953, 0.40698151905433844, 0.4082135511252425, 0.4053388101853874, 0.4041067763520462, 0.4049281317840122, 0.40451745217096147, 0.4032854232699964, 0.4082135535118761, 0.40451745373757225, 0.40492812978903125, 0.4065708432354232, 0.40616016600900606, 0.40616016600900606, 0.40657084104461594, 0.4053388097937347, 0.40451745279515794, 0.40616016463822163, 0.40492813217566487, 0.4073921960849292, 0.40164271162275905, 0.4086242295266177, 0.40657084065296323, 0.4061601662048324, 0.40328541935346945, 0.4106776182044458, 0.4036961001047608, 0.40082135619079307, 0.4110882972300175, 0.4090349095313211, 0.40698151846685937, 0.40574948620012896, 0.41067761980777406, 0.4090349095313211, 0.40328542189921196, 0.4041067775270043, 0.40739219945069455, 0.4053388101853874, 0.40574948561264995, 0.40985626120586904, 0.40451745534090044, 0.4065708412771597, 0.40780287291969364, 0.4110882942559048, 0.40616016362237245, 0.41026694140639886, 0.4102669423855306, 0.4045174523667878, 0.4086242285474859, 0.40985626515911344, 0.40698151889522954, 0.4114989756313927, 0.41149897543556635, 0.4106776158545296, 0.40985626159752175, 0.4102669390197652, 0.40985626061839, 0.41190965129119905, 0.40492813315479664, 0.4094455833919729, 0.404106777918657, 0.4078028733113463, 0.4102669394114179, 0.40739219905904184, 0.40944558754104365, 0.40739219729660475, 0.41108829585923307, 0.4143737153962897, 0.40574948858676263, 0.41149897445643463, 0.4119096516461343, 0.41314168571201926, 0.4119096522703308, 0.41355236352591546, 0.4139630413398116, 0.414373715004637, 0.4143737177462059]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
