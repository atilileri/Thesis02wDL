{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf34.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 15:57:30 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': 'Split', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 15 Label(s): ['mb', 'yd', 'ek', 'eb', 'my', 'eg', 'ib', 'aa', 'by', 'eo', 'sg', 'ce', 'ck', 'sk', 'ds'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000020381345240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000203D2766EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6898, Accuracy:0.0863, Validation Loss:2.6714, Validation Accuracy:0.0891\n",
    "Epoch #2: Loss:2.6564, Accuracy:0.1161, Validation Loss:2.6351, Validation Accuracy:0.1330\n",
    "Epoch #3: Loss:2.6144, Accuracy:0.1443, Validation Loss:2.5874, Validation Accuracy:0.1511\n",
    "Epoch #4: Loss:2.5664, Accuracy:0.1553, Validation Loss:2.5458, Validation Accuracy:0.1720\n",
    "Epoch #5: Loss:2.5307, Accuracy:0.1623, Validation Loss:2.5229, Validation Accuracy:0.1572\n",
    "Epoch #6: Loss:2.5155, Accuracy:0.1634, Validation Loss:2.5112, Validation Accuracy:0.1576\n",
    "Epoch #7: Loss:2.5003, Accuracy:0.1681, Validation Loss:2.4936, Validation Accuracy:0.1732\n",
    "Epoch #8: Loss:2.4904, Accuracy:0.1698, Validation Loss:2.4864, Validation Accuracy:0.1695\n",
    "Epoch #9: Loss:2.4804, Accuracy:0.1686, Validation Loss:2.4701, Validation Accuracy:0.1745\n",
    "Epoch #10: Loss:2.4746, Accuracy:0.1721, Validation Loss:2.4742, Validation Accuracy:0.1683\n",
    "Epoch #11: Loss:2.4687, Accuracy:0.1684, Validation Loss:2.4658, Validation Accuracy:0.1687\n",
    "Epoch #12: Loss:2.4668, Accuracy:0.1746, Validation Loss:2.4684, Validation Accuracy:0.1720\n",
    "Epoch #13: Loss:2.4678, Accuracy:0.1658, Validation Loss:2.4883, Validation Accuracy:0.1593\n",
    "Epoch #14: Loss:2.4611, Accuracy:0.1761, Validation Loss:2.4628, Validation Accuracy:0.1773\n",
    "Epoch #15: Loss:2.4603, Accuracy:0.1749, Validation Loss:2.4637, Validation Accuracy:0.1765\n",
    "Epoch #16: Loss:2.4609, Accuracy:0.1713, Validation Loss:2.4622, Validation Accuracy:0.1671\n",
    "Epoch #17: Loss:2.4573, Accuracy:0.1679, Validation Loss:2.4559, Validation Accuracy:0.1605\n",
    "Epoch #18: Loss:2.4551, Accuracy:0.1774, Validation Loss:2.4581, Validation Accuracy:0.1675\n",
    "Epoch #19: Loss:2.4551, Accuracy:0.1762, Validation Loss:2.4554, Validation Accuracy:0.1724\n",
    "Epoch #20: Loss:2.4519, Accuracy:0.1758, Validation Loss:2.4541, Validation Accuracy:0.1679\n",
    "Epoch #21: Loss:2.4496, Accuracy:0.1763, Validation Loss:2.4519, Validation Accuracy:0.1786\n",
    "Epoch #22: Loss:2.4502, Accuracy:0.1774, Validation Loss:2.4509, Validation Accuracy:0.1798\n",
    "Epoch #23: Loss:2.4484, Accuracy:0.1778, Validation Loss:2.4502, Validation Accuracy:0.1745\n",
    "Epoch #24: Loss:2.4462, Accuracy:0.1779, Validation Loss:2.4537, Validation Accuracy:0.1765\n",
    "Epoch #25: Loss:2.4496, Accuracy:0.1775, Validation Loss:2.4479, Validation Accuracy:0.1765\n",
    "Epoch #26: Loss:2.4449, Accuracy:0.1808, Validation Loss:2.4486, Validation Accuracy:0.1749\n",
    "Epoch #27: Loss:2.4459, Accuracy:0.1777, Validation Loss:2.4480, Validation Accuracy:0.1753\n",
    "Epoch #28: Loss:2.4443, Accuracy:0.1777, Validation Loss:2.4481, Validation Accuracy:0.1765\n",
    "Epoch #29: Loss:2.4419, Accuracy:0.1773, Validation Loss:2.4464, Validation Accuracy:0.1773\n",
    "Epoch #30: Loss:2.4417, Accuracy:0.1798, Validation Loss:2.4440, Validation Accuracy:0.1712\n",
    "Epoch #31: Loss:2.4415, Accuracy:0.1780, Validation Loss:2.4471, Validation Accuracy:0.1716\n",
    "Epoch #32: Loss:2.4493, Accuracy:0.1709, Validation Loss:2.4449, Validation Accuracy:0.1716\n",
    "Epoch #33: Loss:2.4425, Accuracy:0.1795, Validation Loss:2.4501, Validation Accuracy:0.1736\n",
    "Epoch #34: Loss:2.4445, Accuracy:0.1789, Validation Loss:2.4468, Validation Accuracy:0.1794\n",
    "Epoch #35: Loss:2.4412, Accuracy:0.1798, Validation Loss:2.4441, Validation Accuracy:0.1728\n",
    "Epoch #36: Loss:2.4387, Accuracy:0.1773, Validation Loss:2.4428, Validation Accuracy:0.1761\n",
    "Epoch #37: Loss:2.4381, Accuracy:0.1784, Validation Loss:2.4434, Validation Accuracy:0.1704\n",
    "Epoch #38: Loss:2.4385, Accuracy:0.1805, Validation Loss:2.4427, Validation Accuracy:0.1736\n",
    "Epoch #39: Loss:2.4406, Accuracy:0.1809, Validation Loss:2.4446, Validation Accuracy:0.1753\n",
    "Epoch #40: Loss:2.4413, Accuracy:0.1807, Validation Loss:2.4413, Validation Accuracy:0.1802\n",
    "Epoch #41: Loss:2.4372, Accuracy:0.1801, Validation Loss:2.4423, Validation Accuracy:0.1749\n",
    "Epoch #42: Loss:2.4368, Accuracy:0.1785, Validation Loss:2.4422, Validation Accuracy:0.1769\n",
    "Epoch #43: Loss:2.4382, Accuracy:0.1789, Validation Loss:2.4425, Validation Accuracy:0.1798\n",
    "Epoch #44: Loss:2.4367, Accuracy:0.1779, Validation Loss:2.4412, Validation Accuracy:0.1757\n",
    "Epoch #45: Loss:2.4356, Accuracy:0.1787, Validation Loss:2.4426, Validation Accuracy:0.1765\n",
    "Epoch #46: Loss:2.4364, Accuracy:0.1773, Validation Loss:2.4410, Validation Accuracy:0.1778\n",
    "Epoch #47: Loss:2.4364, Accuracy:0.1778, Validation Loss:2.4403, Validation Accuracy:0.1741\n",
    "Epoch #48: Loss:2.4400, Accuracy:0.1754, Validation Loss:2.4396, Validation Accuracy:0.1683\n",
    "Epoch #49: Loss:2.4392, Accuracy:0.1778, Validation Loss:2.4416, Validation Accuracy:0.1753\n",
    "Epoch #50: Loss:2.4372, Accuracy:0.1803, Validation Loss:2.4413, Validation Accuracy:0.1757\n",
    "Epoch #51: Loss:2.4382, Accuracy:0.1776, Validation Loss:2.4409, Validation Accuracy:0.1782\n",
    "Epoch #52: Loss:2.4376, Accuracy:0.1818, Validation Loss:2.4396, Validation Accuracy:0.1786\n",
    "Epoch #53: Loss:2.4350, Accuracy:0.1790, Validation Loss:2.4396, Validation Accuracy:0.1700\n",
    "Epoch #54: Loss:2.4379, Accuracy:0.1808, Validation Loss:2.4405, Validation Accuracy:0.1691\n",
    "Epoch #55: Loss:2.4367, Accuracy:0.1786, Validation Loss:2.4390, Validation Accuracy:0.1716\n",
    "Epoch #56: Loss:2.4429, Accuracy:0.1784, Validation Loss:2.4502, Validation Accuracy:0.1753\n",
    "Epoch #57: Loss:2.4377, Accuracy:0.1835, Validation Loss:2.4400, Validation Accuracy:0.1753\n",
    "Epoch #58: Loss:2.4330, Accuracy:0.1840, Validation Loss:2.4383, Validation Accuracy:0.1798\n",
    "Epoch #59: Loss:2.4326, Accuracy:0.1834, Validation Loss:2.4392, Validation Accuracy:0.1786\n",
    "Epoch #60: Loss:2.4340, Accuracy:0.1834, Validation Loss:2.4386, Validation Accuracy:0.1819\n",
    "Epoch #61: Loss:2.4330, Accuracy:0.1820, Validation Loss:2.4449, Validation Accuracy:0.1773\n",
    "Epoch #62: Loss:2.4356, Accuracy:0.1821, Validation Loss:2.4403, Validation Accuracy:0.1741\n",
    "Epoch #63: Loss:2.4333, Accuracy:0.1809, Validation Loss:2.4429, Validation Accuracy:0.1790\n",
    "Epoch #64: Loss:2.4332, Accuracy:0.1840, Validation Loss:2.4405, Validation Accuracy:0.1757\n",
    "Epoch #65: Loss:2.4332, Accuracy:0.1822, Validation Loss:2.4384, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4324, Accuracy:0.1803, Validation Loss:2.4398, Validation Accuracy:0.1712\n",
    "Epoch #67: Loss:2.4324, Accuracy:0.1784, Validation Loss:2.4401, Validation Accuracy:0.1691\n",
    "Epoch #68: Loss:2.4339, Accuracy:0.1742, Validation Loss:2.4410, Validation Accuracy:0.1683\n",
    "Epoch #69: Loss:2.4344, Accuracy:0.1783, Validation Loss:2.4387, Validation Accuracy:0.1695\n",
    "Epoch #70: Loss:2.4333, Accuracy:0.1782, Validation Loss:2.4389, Validation Accuracy:0.1753\n",
    "Epoch #71: Loss:2.4324, Accuracy:0.1816, Validation Loss:2.4374, Validation Accuracy:0.1749\n",
    "Epoch #72: Loss:2.4325, Accuracy:0.1808, Validation Loss:2.4375, Validation Accuracy:0.1782\n",
    "Epoch #73: Loss:2.4324, Accuracy:0.1819, Validation Loss:2.4388, Validation Accuracy:0.1773\n",
    "Epoch #74: Loss:2.4321, Accuracy:0.1809, Validation Loss:2.4387, Validation Accuracy:0.1736\n",
    "Epoch #75: Loss:2.4308, Accuracy:0.1792, Validation Loss:2.4371, Validation Accuracy:0.1769\n",
    "Epoch #76: Loss:2.4303, Accuracy:0.1808, Validation Loss:2.4378, Validation Accuracy:0.1761\n",
    "Epoch #77: Loss:2.4307, Accuracy:0.1805, Validation Loss:2.4392, Validation Accuracy:0.1810\n",
    "Epoch #78: Loss:2.4319, Accuracy:0.1839, Validation Loss:2.4390, Validation Accuracy:0.1851\n",
    "Epoch #79: Loss:2.4308, Accuracy:0.1848, Validation Loss:2.4367, Validation Accuracy:0.1810\n",
    "Epoch #80: Loss:2.4340, Accuracy:0.1836, Validation Loss:2.4378, Validation Accuracy:0.1761\n",
    "Epoch #81: Loss:2.4322, Accuracy:0.1832, Validation Loss:2.4379, Validation Accuracy:0.1819\n",
    "Epoch #82: Loss:2.4309, Accuracy:0.1857, Validation Loss:2.4377, Validation Accuracy:0.1835\n",
    "Epoch #83: Loss:2.4312, Accuracy:0.1854, Validation Loss:2.4362, Validation Accuracy:0.1814\n",
    "Epoch #84: Loss:2.4352, Accuracy:0.1819, Validation Loss:2.4408, Validation Accuracy:0.1757\n",
    "Epoch #85: Loss:2.4310, Accuracy:0.1846, Validation Loss:2.4370, Validation Accuracy:0.1773\n",
    "Epoch #86: Loss:2.4301, Accuracy:0.1814, Validation Loss:2.4391, Validation Accuracy:0.1769\n",
    "Epoch #87: Loss:2.4306, Accuracy:0.1801, Validation Loss:2.4394, Validation Accuracy:0.1773\n",
    "Epoch #88: Loss:2.4308, Accuracy:0.1783, Validation Loss:2.4383, Validation Accuracy:0.1769\n",
    "Epoch #89: Loss:2.4340, Accuracy:0.1808, Validation Loss:2.4383, Validation Accuracy:0.1765\n",
    "Epoch #90: Loss:2.4343, Accuracy:0.1814, Validation Loss:2.4373, Validation Accuracy:0.1860\n",
    "Epoch #91: Loss:2.4317, Accuracy:0.1820, Validation Loss:2.4356, Validation Accuracy:0.1786\n",
    "Epoch #92: Loss:2.4298, Accuracy:0.1847, Validation Loss:2.4372, Validation Accuracy:0.1802\n",
    "Epoch #93: Loss:2.4290, Accuracy:0.1862, Validation Loss:2.4356, Validation Accuracy:0.1814\n",
    "Epoch #94: Loss:2.4293, Accuracy:0.1839, Validation Loss:2.4355, Validation Accuracy:0.1741\n",
    "Epoch #95: Loss:2.4299, Accuracy:0.1821, Validation Loss:2.4357, Validation Accuracy:0.1765\n",
    "Epoch #96: Loss:2.4294, Accuracy:0.1797, Validation Loss:2.4351, Validation Accuracy:0.1839\n",
    "Epoch #97: Loss:2.4307, Accuracy:0.1844, Validation Loss:2.4442, Validation Accuracy:0.1741\n",
    "Epoch #98: Loss:2.4309, Accuracy:0.1817, Validation Loss:2.4393, Validation Accuracy:0.1806\n",
    "Epoch #99: Loss:2.4313, Accuracy:0.1852, Validation Loss:2.4386, Validation Accuracy:0.1794\n",
    "Epoch #100: Loss:2.4298, Accuracy:0.1836, Validation Loss:2.4429, Validation Accuracy:0.1794\n",
    "Epoch #101: Loss:2.4320, Accuracy:0.1818, Validation Loss:2.4375, Validation Accuracy:0.1847\n",
    "Epoch #102: Loss:2.4287, Accuracy:0.1863, Validation Loss:2.4373, Validation Accuracy:0.1831\n",
    "Epoch #103: Loss:2.4292, Accuracy:0.1847, Validation Loss:2.4374, Validation Accuracy:0.1851\n",
    "Epoch #104: Loss:2.4302, Accuracy:0.1829, Validation Loss:2.4389, Validation Accuracy:0.1773\n",
    "Epoch #105: Loss:2.4321, Accuracy:0.1809, Validation Loss:2.4372, Validation Accuracy:0.1736\n",
    "Epoch #106: Loss:2.4291, Accuracy:0.1843, Validation Loss:2.4368, Validation Accuracy:0.1806\n",
    "Epoch #107: Loss:2.4295, Accuracy:0.1828, Validation Loss:2.4368, Validation Accuracy:0.1806\n",
    "Epoch #108: Loss:2.4289, Accuracy:0.1828, Validation Loss:2.4362, Validation Accuracy:0.1745\n",
    "Epoch #109: Loss:2.4300, Accuracy:0.1825, Validation Loss:2.4358, Validation Accuracy:0.1761\n",
    "Epoch #110: Loss:2.4290, Accuracy:0.1830, Validation Loss:2.4353, Validation Accuracy:0.1765\n",
    "Epoch #111: Loss:2.4287, Accuracy:0.1824, Validation Loss:2.4386, Validation Accuracy:0.1806\n",
    "Epoch #112: Loss:2.4292, Accuracy:0.1822, Validation Loss:2.4358, Validation Accuracy:0.1757\n",
    "Epoch #113: Loss:2.4339, Accuracy:0.1834, Validation Loss:2.4380, Validation Accuracy:0.1835\n",
    "Epoch #114: Loss:2.4296, Accuracy:0.1821, Validation Loss:2.4372, Validation Accuracy:0.1835\n",
    "Epoch #115: Loss:2.4292, Accuracy:0.1809, Validation Loss:2.4360, Validation Accuracy:0.1823\n",
    "Epoch #116: Loss:2.4297, Accuracy:0.1833, Validation Loss:2.4371, Validation Accuracy:0.1851\n",
    "Epoch #117: Loss:2.4293, Accuracy:0.1824, Validation Loss:2.4362, Validation Accuracy:0.1761\n",
    "Epoch #118: Loss:2.4280, Accuracy:0.1842, Validation Loss:2.4349, Validation Accuracy:0.1827\n",
    "Epoch #119: Loss:2.4281, Accuracy:0.1842, Validation Loss:2.4374, Validation Accuracy:0.1823\n",
    "Epoch #120: Loss:2.4289, Accuracy:0.1838, Validation Loss:2.4358, Validation Accuracy:0.1839\n",
    "Epoch #121: Loss:2.4275, Accuracy:0.1836, Validation Loss:2.4355, Validation Accuracy:0.1806\n",
    "Epoch #122: Loss:2.4292, Accuracy:0.1848, Validation Loss:2.4362, Validation Accuracy:0.1786\n",
    "Epoch #123: Loss:2.4278, Accuracy:0.1805, Validation Loss:2.4351, Validation Accuracy:0.1806\n",
    "Epoch #124: Loss:2.4273, Accuracy:0.1832, Validation Loss:2.4349, Validation Accuracy:0.1794\n",
    "Epoch #125: Loss:2.4278, Accuracy:0.1821, Validation Loss:2.4349, Validation Accuracy:0.1749\n",
    "Epoch #126: Loss:2.4269, Accuracy:0.1839, Validation Loss:2.4348, Validation Accuracy:0.1827\n",
    "Epoch #127: Loss:2.4273, Accuracy:0.1852, Validation Loss:2.4358, Validation Accuracy:0.1868\n",
    "Epoch #128: Loss:2.4271, Accuracy:0.1851, Validation Loss:2.4355, Validation Accuracy:0.1827\n",
    "Epoch #129: Loss:2.4307, Accuracy:0.1842, Validation Loss:2.4338, Validation Accuracy:0.1778\n",
    "Epoch #130: Loss:2.4278, Accuracy:0.1798, Validation Loss:2.4348, Validation Accuracy:0.1786\n",
    "Epoch #131: Loss:2.4268, Accuracy:0.1848, Validation Loss:2.4326, Validation Accuracy:0.1827\n",
    "Epoch #132: Loss:2.4277, Accuracy:0.1853, Validation Loss:2.4332, Validation Accuracy:0.1892\n",
    "Epoch #133: Loss:2.4286, Accuracy:0.1798, Validation Loss:2.4357, Validation Accuracy:0.1892\n",
    "Epoch #134: Loss:2.4264, Accuracy:0.1841, Validation Loss:2.4356, Validation Accuracy:0.1897\n",
    "Epoch #135: Loss:2.4283, Accuracy:0.1832, Validation Loss:2.4332, Validation Accuracy:0.1868\n",
    "Epoch #136: Loss:2.4260, Accuracy:0.1863, Validation Loss:2.4330, Validation Accuracy:0.1819\n",
    "Epoch #137: Loss:2.4269, Accuracy:0.1843, Validation Loss:2.4336, Validation Accuracy:0.1806\n",
    "Epoch #138: Loss:2.4266, Accuracy:0.1838, Validation Loss:2.4332, Validation Accuracy:0.1856\n",
    "Epoch #139: Loss:2.4253, Accuracy:0.1845, Validation Loss:2.4348, Validation Accuracy:0.1856\n",
    "Epoch #140: Loss:2.4269, Accuracy:0.1854, Validation Loss:2.4331, Validation Accuracy:0.1806\n",
    "Epoch #141: Loss:2.4272, Accuracy:0.1855, Validation Loss:2.4325, Validation Accuracy:0.1819\n",
    "Epoch #142: Loss:2.4264, Accuracy:0.1861, Validation Loss:2.4318, Validation Accuracy:0.1897\n",
    "Epoch #143: Loss:2.4262, Accuracy:0.1850, Validation Loss:2.4329, Validation Accuracy:0.1897\n",
    "Epoch #144: Loss:2.4253, Accuracy:0.1863, Validation Loss:2.4342, Validation Accuracy:0.1901\n",
    "Epoch #145: Loss:2.4268, Accuracy:0.1789, Validation Loss:2.4326, Validation Accuracy:0.1835\n",
    "Epoch #146: Loss:2.4252, Accuracy:0.1862, Validation Loss:2.4325, Validation Accuracy:0.1851\n",
    "Epoch #147: Loss:2.4256, Accuracy:0.1850, Validation Loss:2.4333, Validation Accuracy:0.1856\n",
    "Epoch #148: Loss:2.4265, Accuracy:0.1819, Validation Loss:2.4333, Validation Accuracy:0.1802\n",
    "Epoch #149: Loss:2.4260, Accuracy:0.1830, Validation Loss:2.4329, Validation Accuracy:0.1806\n",
    "Epoch #150: Loss:2.4270, Accuracy:0.1837, Validation Loss:2.4323, Validation Accuracy:0.1814\n",
    "Epoch #151: Loss:2.4237, Accuracy:0.1853, Validation Loss:2.4325, Validation Accuracy:0.1823\n",
    "Epoch #152: Loss:2.4243, Accuracy:0.1856, Validation Loss:2.4326, Validation Accuracy:0.1769\n",
    "Epoch #153: Loss:2.4245, Accuracy:0.1853, Validation Loss:2.4357, Validation Accuracy:0.1905\n",
    "Epoch #154: Loss:2.4270, Accuracy:0.1867, Validation Loss:2.4350, Validation Accuracy:0.1823\n",
    "Epoch #155: Loss:2.4258, Accuracy:0.1853, Validation Loss:2.4315, Validation Accuracy:0.1888\n",
    "Epoch #156: Loss:2.4245, Accuracy:0.1856, Validation Loss:2.4317, Validation Accuracy:0.1814\n",
    "Epoch #157: Loss:2.4240, Accuracy:0.1840, Validation Loss:2.4311, Validation Accuracy:0.1868\n",
    "Epoch #158: Loss:2.4247, Accuracy:0.1866, Validation Loss:2.4318, Validation Accuracy:0.1819\n",
    "Epoch #159: Loss:2.4239, Accuracy:0.1851, Validation Loss:2.4318, Validation Accuracy:0.1860\n",
    "Epoch #160: Loss:2.4242, Accuracy:0.1850, Validation Loss:2.4355, Validation Accuracy:0.1909\n",
    "Epoch #161: Loss:2.4259, Accuracy:0.1835, Validation Loss:2.4311, Validation Accuracy:0.1864\n",
    "Epoch #162: Loss:2.4265, Accuracy:0.1847, Validation Loss:2.4319, Validation Accuracy:0.1810\n",
    "Epoch #163: Loss:2.4844, Accuracy:0.1698, Validation Loss:2.5061, Validation Accuracy:0.1449\n",
    "Epoch #164: Loss:2.4771, Accuracy:0.1659, Validation Loss:2.4419, Validation Accuracy:0.1786\n",
    "Epoch #165: Loss:2.4351, Accuracy:0.1857, Validation Loss:2.4405, Validation Accuracy:0.1778\n",
    "Epoch #166: Loss:2.4318, Accuracy:0.1851, Validation Loss:2.4427, Validation Accuracy:0.1736\n",
    "Epoch #167: Loss:2.4306, Accuracy:0.1815, Validation Loss:2.4421, Validation Accuracy:0.1769\n",
    "Epoch #168: Loss:2.4293, Accuracy:0.1824, Validation Loss:2.4387, Validation Accuracy:0.1765\n",
    "Epoch #169: Loss:2.4289, Accuracy:0.1803, Validation Loss:2.4393, Validation Accuracy:0.1773\n",
    "Epoch #170: Loss:2.4322, Accuracy:0.1789, Validation Loss:2.4402, Validation Accuracy:0.1724\n",
    "Epoch #171: Loss:2.4340, Accuracy:0.1768, Validation Loss:2.4426, Validation Accuracy:0.1745\n",
    "Epoch #172: Loss:2.4326, Accuracy:0.1801, Validation Loss:2.4389, Validation Accuracy:0.1724\n",
    "Epoch #173: Loss:2.4339, Accuracy:0.1812, Validation Loss:2.4390, Validation Accuracy:0.1757\n",
    "Epoch #174: Loss:2.4300, Accuracy:0.1823, Validation Loss:2.4372, Validation Accuracy:0.1732\n",
    "Epoch #175: Loss:2.4282, Accuracy:0.1851, Validation Loss:2.4377, Validation Accuracy:0.1720\n",
    "Epoch #176: Loss:2.4279, Accuracy:0.1822, Validation Loss:2.4356, Validation Accuracy:0.1761\n",
    "Epoch #177: Loss:2.4285, Accuracy:0.1811, Validation Loss:2.4367, Validation Accuracy:0.1765\n",
    "Epoch #178: Loss:2.4279, Accuracy:0.1843, Validation Loss:2.4377, Validation Accuracy:0.1773\n",
    "Epoch #179: Loss:2.4286, Accuracy:0.1836, Validation Loss:2.4392, Validation Accuracy:0.1745\n",
    "Epoch #180: Loss:2.4289, Accuracy:0.1838, Validation Loss:2.4398, Validation Accuracy:0.1761\n",
    "Epoch #181: Loss:2.4277, Accuracy:0.1859, Validation Loss:2.4384, Validation Accuracy:0.1782\n",
    "Epoch #182: Loss:2.4268, Accuracy:0.1860, Validation Loss:2.4374, Validation Accuracy:0.1872\n",
    "Epoch #183: Loss:2.4286, Accuracy:0.1861, Validation Loss:2.4367, Validation Accuracy:0.1839\n",
    "Epoch #184: Loss:2.4274, Accuracy:0.1845, Validation Loss:2.4389, Validation Accuracy:0.1823\n",
    "Epoch #185: Loss:2.4282, Accuracy:0.1832, Validation Loss:2.4377, Validation Accuracy:0.1802\n",
    "Epoch #186: Loss:2.4269, Accuracy:0.1859, Validation Loss:2.4377, Validation Accuracy:0.1831\n",
    "Epoch #187: Loss:2.4249, Accuracy:0.1867, Validation Loss:2.4355, Validation Accuracy:0.1802\n",
    "Epoch #188: Loss:2.4265, Accuracy:0.1843, Validation Loss:2.4372, Validation Accuracy:0.1802\n",
    "Epoch #189: Loss:2.4256, Accuracy:0.1816, Validation Loss:2.4350, Validation Accuracy:0.1802\n",
    "Epoch #190: Loss:2.4246, Accuracy:0.1824, Validation Loss:2.4354, Validation Accuracy:0.1806\n",
    "Epoch #191: Loss:2.4252, Accuracy:0.1841, Validation Loss:2.4367, Validation Accuracy:0.1827\n",
    "Epoch #192: Loss:2.4261, Accuracy:0.1840, Validation Loss:2.4369, Validation Accuracy:0.1810\n",
    "Epoch #193: Loss:2.4263, Accuracy:0.1828, Validation Loss:2.4349, Validation Accuracy:0.1827\n",
    "Epoch #194: Loss:2.4262, Accuracy:0.1854, Validation Loss:2.4347, Validation Accuracy:0.1806\n",
    "Epoch #195: Loss:2.4249, Accuracy:0.1851, Validation Loss:2.4352, Validation Accuracy:0.1798\n",
    "Epoch #196: Loss:2.4242, Accuracy:0.1845, Validation Loss:2.4343, Validation Accuracy:0.1794\n",
    "Epoch #197: Loss:2.4235, Accuracy:0.1833, Validation Loss:2.4336, Validation Accuracy:0.1802\n",
    "Epoch #198: Loss:2.4250, Accuracy:0.1831, Validation Loss:2.4343, Validation Accuracy:0.1810\n",
    "Epoch #199: Loss:2.4266, Accuracy:0.1860, Validation Loss:2.4342, Validation Accuracy:0.1798\n",
    "Epoch #200: Loss:2.4241, Accuracy:0.1861, Validation Loss:2.4345, Validation Accuracy:0.1823\n",
    "Epoch #201: Loss:2.4229, Accuracy:0.1829, Validation Loss:2.4310, Validation Accuracy:0.1798\n",
    "Epoch #202: Loss:2.4233, Accuracy:0.1854, Validation Loss:2.4313, Validation Accuracy:0.1814\n",
    "Epoch #203: Loss:2.4233, Accuracy:0.1829, Validation Loss:2.4307, Validation Accuracy:0.1798\n",
    "Epoch #204: Loss:2.4243, Accuracy:0.1831, Validation Loss:2.4299, Validation Accuracy:0.1761\n",
    "Epoch #205: Loss:2.4241, Accuracy:0.1809, Validation Loss:2.4320, Validation Accuracy:0.1757\n",
    "Epoch #206: Loss:2.4241, Accuracy:0.1825, Validation Loss:2.4329, Validation Accuracy:0.1814\n",
    "Epoch #207: Loss:2.4238, Accuracy:0.1860, Validation Loss:2.4318, Validation Accuracy:0.1778\n",
    "Epoch #208: Loss:2.4227, Accuracy:0.1863, Validation Loss:2.4329, Validation Accuracy:0.1827\n",
    "Epoch #209: Loss:2.4230, Accuracy:0.1836, Validation Loss:2.4320, Validation Accuracy:0.1819\n",
    "Epoch #210: Loss:2.4225, Accuracy:0.1873, Validation Loss:2.4306, Validation Accuracy:0.1802\n",
    "Epoch #211: Loss:2.4212, Accuracy:0.1852, Validation Loss:2.4325, Validation Accuracy:0.1773\n",
    "Epoch #212: Loss:2.4241, Accuracy:0.1851, Validation Loss:2.4348, Validation Accuracy:0.1741\n",
    "Epoch #213: Loss:2.4261, Accuracy:0.1779, Validation Loss:2.4320, Validation Accuracy:0.1724\n",
    "Epoch #214: Loss:2.4230, Accuracy:0.1823, Validation Loss:2.4319, Validation Accuracy:0.1806\n",
    "Epoch #215: Loss:2.4218, Accuracy:0.1846, Validation Loss:2.4318, Validation Accuracy:0.1786\n",
    "Epoch #216: Loss:2.4225, Accuracy:0.1810, Validation Loss:2.4318, Validation Accuracy:0.1802\n",
    "Epoch #217: Loss:2.4210, Accuracy:0.1839, Validation Loss:2.4312, Validation Accuracy:0.1819\n",
    "Epoch #218: Loss:2.4217, Accuracy:0.1863, Validation Loss:2.4306, Validation Accuracy:0.1843\n",
    "Epoch #219: Loss:2.4209, Accuracy:0.1833, Validation Loss:2.4313, Validation Accuracy:0.1819\n",
    "Epoch #220: Loss:2.4228, Accuracy:0.1816, Validation Loss:2.4320, Validation Accuracy:0.1802\n",
    "Epoch #221: Loss:2.4215, Accuracy:0.1854, Validation Loss:2.4311, Validation Accuracy:0.1806\n",
    "Epoch #222: Loss:2.4211, Accuracy:0.1866, Validation Loss:2.4315, Validation Accuracy:0.1823\n",
    "Epoch #223: Loss:2.4220, Accuracy:0.1831, Validation Loss:2.4318, Validation Accuracy:0.1843\n",
    "Epoch #224: Loss:2.4203, Accuracy:0.1842, Validation Loss:2.4308, Validation Accuracy:0.1847\n",
    "Epoch #225: Loss:2.4209, Accuracy:0.1857, Validation Loss:2.4326, Validation Accuracy:0.1847\n",
    "Epoch #226: Loss:2.4226, Accuracy:0.1771, Validation Loss:2.4314, Validation Accuracy:0.1773\n",
    "Epoch #227: Loss:2.4208, Accuracy:0.1834, Validation Loss:2.4302, Validation Accuracy:0.1831\n",
    "Epoch #228: Loss:2.4218, Accuracy:0.1814, Validation Loss:2.4301, Validation Accuracy:0.1794\n",
    "Epoch #229: Loss:2.4209, Accuracy:0.1838, Validation Loss:2.4315, Validation Accuracy:0.1888\n",
    "Epoch #230: Loss:2.4197, Accuracy:0.1869, Validation Loss:2.4309, Validation Accuracy:0.1790\n",
    "Epoch #231: Loss:2.4196, Accuracy:0.1875, Validation Loss:2.4294, Validation Accuracy:0.1856\n",
    "Epoch #232: Loss:2.4230, Accuracy:0.1829, Validation Loss:2.4287, Validation Accuracy:0.1856\n",
    "Epoch #233: Loss:2.4178, Accuracy:0.1845, Validation Loss:2.4294, Validation Accuracy:0.1876\n",
    "Epoch #234: Loss:2.4198, Accuracy:0.1839, Validation Loss:2.4335, Validation Accuracy:0.1806\n",
    "Epoch #235: Loss:2.4223, Accuracy:0.1845, Validation Loss:2.4347, Validation Accuracy:0.1798\n",
    "Epoch #236: Loss:2.4211, Accuracy:0.1820, Validation Loss:2.4296, Validation Accuracy:0.1819\n",
    "Epoch #237: Loss:2.4194, Accuracy:0.1851, Validation Loss:2.4296, Validation Accuracy:0.1851\n",
    "Epoch #238: Loss:2.4179, Accuracy:0.1831, Validation Loss:2.4287, Validation Accuracy:0.1827\n",
    "Epoch #239: Loss:2.4193, Accuracy:0.1842, Validation Loss:2.4293, Validation Accuracy:0.1884\n",
    "Epoch #240: Loss:2.4160, Accuracy:0.1853, Validation Loss:2.4286, Validation Accuracy:0.1856\n",
    "Epoch #241: Loss:2.4174, Accuracy:0.1870, Validation Loss:2.4309, Validation Accuracy:0.1843\n",
    "Epoch #242: Loss:2.4174, Accuracy:0.1850, Validation Loss:2.4318, Validation Accuracy:0.1868\n",
    "Epoch #243: Loss:2.4194, Accuracy:0.1832, Validation Loss:2.4343, Validation Accuracy:0.1831\n",
    "Epoch #244: Loss:2.4186, Accuracy:0.1839, Validation Loss:2.4337, Validation Accuracy:0.1819\n",
    "Epoch #245: Loss:2.4182, Accuracy:0.1856, Validation Loss:2.4314, Validation Accuracy:0.1835\n",
    "Epoch #246: Loss:2.4168, Accuracy:0.1869, Validation Loss:2.4326, Validation Accuracy:0.1892\n",
    "Epoch #247: Loss:2.4186, Accuracy:0.1847, Validation Loss:2.4340, Validation Accuracy:0.1880\n",
    "Epoch #248: Loss:2.4186, Accuracy:0.1864, Validation Loss:2.4367, Validation Accuracy:0.1868\n",
    "Epoch #249: Loss:2.4194, Accuracy:0.1819, Validation Loss:2.4341, Validation Accuracy:0.1761\n",
    "Epoch #250: Loss:2.4181, Accuracy:0.1837, Validation Loss:2.4356, Validation Accuracy:0.1839\n",
    "Epoch #251: Loss:2.4210, Accuracy:0.1803, Validation Loss:2.4341, Validation Accuracy:0.1868\n",
    "Epoch #252: Loss:2.4180, Accuracy:0.1835, Validation Loss:2.4336, Validation Accuracy:0.1819\n",
    "Epoch #253: Loss:2.4192, Accuracy:0.1850, Validation Loss:2.4342, Validation Accuracy:0.1880\n",
    "Epoch #254: Loss:2.4213, Accuracy:0.1815, Validation Loss:2.4354, Validation Accuracy:0.1749\n",
    "Epoch #255: Loss:2.4315, Accuracy:0.1787, Validation Loss:2.4399, Validation Accuracy:0.1884\n",
    "Epoch #256: Loss:2.4244, Accuracy:0.1800, Validation Loss:2.4339, Validation Accuracy:0.1843\n",
    "Epoch #257: Loss:2.4208, Accuracy:0.1816, Validation Loss:2.4312, Validation Accuracy:0.1769\n",
    "Epoch #258: Loss:2.4236, Accuracy:0.1781, Validation Loss:2.4299, Validation Accuracy:0.1872\n",
    "Epoch #259: Loss:2.4228, Accuracy:0.1734, Validation Loss:2.4324, Validation Accuracy:0.1827\n",
    "Epoch #260: Loss:2.4256, Accuracy:0.1824, Validation Loss:2.4328, Validation Accuracy:0.1831\n",
    "Epoch #261: Loss:2.4252, Accuracy:0.1806, Validation Loss:2.4349, Validation Accuracy:0.1769\n",
    "Epoch #262: Loss:2.4245, Accuracy:0.1792, Validation Loss:2.4309, Validation Accuracy:0.1851\n",
    "Epoch #263: Loss:2.4255, Accuracy:0.1830, Validation Loss:2.4292, Validation Accuracy:0.1827\n",
    "Epoch #264: Loss:2.4251, Accuracy:0.1787, Validation Loss:2.4300, Validation Accuracy:0.1765\n",
    "Epoch #265: Loss:2.4240, Accuracy:0.1786, Validation Loss:2.4303, Validation Accuracy:0.1839\n",
    "Epoch #266: Loss:2.4240, Accuracy:0.1779, Validation Loss:2.4298, Validation Accuracy:0.1843\n",
    "Epoch #267: Loss:2.4224, Accuracy:0.1795, Validation Loss:2.4314, Validation Accuracy:0.1724\n",
    "Epoch #268: Loss:2.4236, Accuracy:0.1820, Validation Loss:2.4294, Validation Accuracy:0.1856\n",
    "Epoch #269: Loss:2.4228, Accuracy:0.1828, Validation Loss:2.4295, Validation Accuracy:0.1851\n",
    "Epoch #270: Loss:2.4286, Accuracy:0.1799, Validation Loss:2.4335, Validation Accuracy:0.1802\n",
    "Epoch #271: Loss:2.4259, Accuracy:0.1810, Validation Loss:2.4318, Validation Accuracy:0.1802\n",
    "Epoch #272: Loss:2.4234, Accuracy:0.1809, Validation Loss:2.4299, Validation Accuracy:0.1839\n",
    "Epoch #273: Loss:2.4233, Accuracy:0.1814, Validation Loss:2.4311, Validation Accuracy:0.1753\n",
    "Epoch #274: Loss:2.4229, Accuracy:0.1816, Validation Loss:2.4302, Validation Accuracy:0.1814\n",
    "Epoch #275: Loss:2.4230, Accuracy:0.1790, Validation Loss:2.4293, Validation Accuracy:0.1819\n",
    "Epoch #276: Loss:2.4234, Accuracy:0.1792, Validation Loss:2.4293, Validation Accuracy:0.1831\n",
    "Epoch #277: Loss:2.4231, Accuracy:0.1809, Validation Loss:2.4311, Validation Accuracy:0.1819\n",
    "Epoch #278: Loss:2.4226, Accuracy:0.1838, Validation Loss:2.4293, Validation Accuracy:0.1847\n",
    "Epoch #279: Loss:2.4209, Accuracy:0.1824, Validation Loss:2.4289, Validation Accuracy:0.1827\n",
    "Epoch #280: Loss:2.4208, Accuracy:0.1799, Validation Loss:2.4309, Validation Accuracy:0.1798\n",
    "Epoch #281: Loss:2.4219, Accuracy:0.1833, Validation Loss:2.4302, Validation Accuracy:0.1814\n",
    "Epoch #282: Loss:2.4211, Accuracy:0.1811, Validation Loss:2.4288, Validation Accuracy:0.1810\n",
    "Epoch #283: Loss:2.4223, Accuracy:0.1807, Validation Loss:2.4328, Validation Accuracy:0.1843\n",
    "Epoch #284: Loss:2.4274, Accuracy:0.1782, Validation Loss:2.4314, Validation Accuracy:0.1798\n",
    "Epoch #285: Loss:2.4218, Accuracy:0.1805, Validation Loss:2.4306, Validation Accuracy:0.1786\n",
    "Epoch #286: Loss:2.4197, Accuracy:0.1811, Validation Loss:2.4308, Validation Accuracy:0.1769\n",
    "Epoch #287: Loss:2.4204, Accuracy:0.1797, Validation Loss:2.4302, Validation Accuracy:0.1798\n",
    "Epoch #288: Loss:2.4194, Accuracy:0.1844, Validation Loss:2.4316, Validation Accuracy:0.1851\n",
    "Epoch #289: Loss:2.4190, Accuracy:0.1820, Validation Loss:2.4301, Validation Accuracy:0.1814\n",
    "Epoch #290: Loss:2.4189, Accuracy:0.1818, Validation Loss:2.4332, Validation Accuracy:0.1823\n",
    "Epoch #291: Loss:2.4189, Accuracy:0.1836, Validation Loss:2.4313, Validation Accuracy:0.1778\n",
    "Epoch #292: Loss:2.4186, Accuracy:0.1842, Validation Loss:2.4301, Validation Accuracy:0.1814\n",
    "Epoch #293: Loss:2.4199, Accuracy:0.1833, Validation Loss:2.4327, Validation Accuracy:0.1765\n",
    "Epoch #294: Loss:2.4212, Accuracy:0.1805, Validation Loss:2.4307, Validation Accuracy:0.1819\n",
    "Epoch #295: Loss:2.4192, Accuracy:0.1836, Validation Loss:2.4319, Validation Accuracy:0.1835\n",
    "Epoch #296: Loss:2.4206, Accuracy:0.1836, Validation Loss:2.4321, Validation Accuracy:0.1819\n",
    "Epoch #297: Loss:2.4183, Accuracy:0.1854, Validation Loss:2.4304, Validation Accuracy:0.1827\n",
    "Epoch #298: Loss:2.4172, Accuracy:0.1835, Validation Loss:2.4297, Validation Accuracy:0.1814\n",
    "Epoch #299: Loss:2.4172, Accuracy:0.1818, Validation Loss:2.4284, Validation Accuracy:0.1810\n",
    "Epoch #300: Loss:2.4173, Accuracy:0.1834, Validation Loss:2.4276, Validation Accuracy:0.1839\n",
    "\n",
    "Test:\n",
    "Test Loss:2.42763853, Accuracy:0.1839\n",
    "Labels: ['mb', 'yd', 'ek', 'eb', 'my', 'eg', 'ib', 'aa', 'by', 'eo', 'sg', 'ce', 'ck', 'sk', 'ds']\n",
    "Confusion Matrix:\n",
    "      mb  yd  ek  eb  my   eg  ib  aa  by  eo   sg  ce  ck  sk  ds\n",
    "t:mb   0  16   0  23   0   45  17   0  18   0   82   0   0   0   6\n",
    "t:yd   0  77   0  12   0    8  45   0   1   0  106   0   0   0   0\n",
    "t:ek   0  10   0  13   0   51  13   5  15   0   78   0   0   0   6\n",
    "t:eb   0  15   0  23   0   74  10   2  18   0   56   0   0   0   3\n",
    "t:my   0  14   0   1   0   21  10   1   4   0   21   0   0   0   8\n",
    "t:eg   0   0   0  14   0  109   1   5  20   0   34   0   0   0  15\n",
    "t:ib   0  51   0   8   0   21  52   0   6   0   78   0   0   0   1\n",
    "t:aa   0   6   0   7   0   60   4  14  10   0   15   0   0   0  21\n",
    "t:by   0   4   0  21   0   40   2   3  21   0   65   0   0   0   6\n",
    "t:eo   0   8   0  16   0   13   4   0  13   0   81   0   0   0   0\n",
    "t:sg   0  13   0  20   0   13  17   0  10   0  130   0   0   0   0\n",
    "t:ce   0   0   0   7   0   28   6   4   4   0   55   0   0   0   5\n",
    "t:ck   0   0   0   7   0   44   0   1   3   0   30   0   0   0   6\n",
    "t:sk   0   5   0  11   0   42   1   3  13   0   41   0   0   0  14\n",
    "t:ds   0   0   0   5   0   52   3   9   5   0   30   0   0   0  22\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          mb       0.00      0.00      0.00       207\n",
    "          yd       0.35      0.31      0.33       249\n",
    "          ek       0.00      0.00      0.00       191\n",
    "          eb       0.12      0.11      0.12       201\n",
    "          my       0.00      0.00      0.00        80\n",
    "          eg       0.18      0.55      0.27       198\n",
    "          ib       0.28      0.24      0.26       217\n",
    "          aa       0.30      0.10      0.15       137\n",
    "          by       0.13      0.13      0.13       162\n",
    "          eo       0.00      0.00      0.00       135\n",
    "          sg       0.14      0.64      0.24       203\n",
    "          ce       0.00      0.00      0.00       109\n",
    "          ck       0.00      0.00      0.00        91\n",
    "          sk       0.00      0.00      0.00       130\n",
    "          ds       0.19      0.17      0.18       126\n",
    "\n",
    "    accuracy                           0.18      2436\n",
    "   macro avg       0.11      0.15      0.11      2436\n",
    "weighted avg       0.13      0.18      0.13      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 16:59:47 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 16 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6714498229410455, 2.6350605988933142, 2.5874480606104155, 2.5458144495639896, 2.5228694287818447, 2.511174032449331, 2.4936450956685987, 2.4864497685863074, 2.4700602197099006, 2.4741657939059003, 2.465798078107912, 2.468400514380294, 2.488274452134306, 2.4628419637288563, 2.4637157619488845, 2.462243300940603, 2.4558694495747635, 2.45813326608567, 2.455414786910384, 2.454085492931172, 2.451886333268264, 2.45088442714735, 2.450214674124381, 2.453726391878426, 2.4479347868701704, 2.4485539978948134, 2.4479931634047936, 2.4481173489481356, 2.446406512033372, 2.4439687881563685, 2.447094793585917, 2.4448572927703607, 2.450116017769123, 2.4468402193097645, 2.4440505348011388, 2.4427691097134243, 2.44335051005697, 2.442722000707742, 2.444592154476247, 2.441281907076906, 2.4422808773999143, 2.442231535715814, 2.4425469791556425, 2.4412166688634063, 2.44257049216034, 2.4409887129058587, 2.4402508269781356, 2.439645660726112, 2.441594879419737, 2.4413318250371123, 2.440935249203336, 2.439589213072177, 2.4395833309060833, 2.440494612911456, 2.4389942330484122, 2.4502286214155125, 2.440020867364943, 2.438302404579075, 2.4392176598359407, 2.4385818088387423, 2.4448676790509904, 2.4402979853117994, 2.4429106735830826, 2.4404851031812345, 2.438399353246579, 2.4397759230070317, 2.4400539621343755, 2.441005338784705, 2.43866496211398, 2.4389089543635425, 2.437448047847779, 2.4374807029717855, 2.438819759976492, 2.438738187937118, 2.437053196927401, 2.4378222561822147, 2.4391781148456393, 2.4390072799081284, 2.436708528224275, 2.4378172689666497, 2.437939356504794, 2.4376967262556204, 2.4362019975784377, 2.4407774458573566, 2.437029061842043, 2.439078985959634, 2.4394269936973436, 2.4382628748569584, 2.4382736667232168, 2.4372653448327224, 2.435614204563335, 2.437184964886244, 2.4355736753623476, 2.435498227822565, 2.4356545983081186, 2.4351286555354426, 2.444190291935587, 2.439319292704264, 2.4385723871942027, 2.442930873783155, 2.437513189362775, 2.4372613856749386, 2.4373894007922394, 2.4389066778380295, 2.437226327965021, 2.4367946768041904, 2.4367932459012236, 2.436186131193916, 2.435839028977017, 2.435283622522464, 2.438584438489967, 2.4358241499351165, 2.4380078656332835, 2.4371694516274336, 2.4359752337137857, 2.4370845710898466, 2.436225327756409, 2.434888233300696, 2.4373639181916937, 2.4358116421597735, 2.435512534894771, 2.436227695499539, 2.435095406909686, 2.434853103556265, 2.43487326539013, 2.4348084632986287, 2.4357956078252183, 2.4355309036956436, 2.4338471822942225, 2.4348380784878785, 2.432618367848138, 2.433188615369875, 2.4357378913459717, 2.4355629128579825, 2.4332037328303544, 2.433044399925445, 2.433606201400506, 2.4332289515653462, 2.4347611730321876, 2.433065692779466, 2.4325329564475076, 2.4318135649895627, 2.4329279443900576, 2.4342453984791423, 2.432594094566132, 2.432469494041355, 2.433260640487295, 2.433301865760916, 2.4328586859460337, 2.4323358672788773, 2.432468574426836, 2.4325745031359944, 2.4357007802609347, 2.4350497894882177, 2.4314521185087257, 2.4316711543228826, 2.4311129678842076, 2.431751407817471, 2.4317665428950868, 2.4355051208207956, 2.4310576590802673, 2.4318515350078713, 2.506098568341611, 2.441923760036725, 2.440485574537506, 2.4426918021955317, 2.442123659334355, 2.4386501958217526, 2.439321166384592, 2.4402087380733395, 2.4426333089963164, 2.4389174720532396, 2.439034363319134, 2.437209193146679, 2.437710882603437, 2.435623605850295, 2.4367488436706743, 2.4377162530895915, 2.439204732772752, 2.439848044822956, 2.4384449115527675, 2.437435953096412, 2.436659085926751, 2.438867898605923, 2.4377115833739733, 2.437717338696685, 2.435508556553883, 2.4371762800294974, 2.434981536395444, 2.4353866624127467, 2.436666181326304, 2.436879135313488, 2.434947088238445, 2.434724739619664, 2.435233267657275, 2.4342944704253098, 2.4336448811936653, 2.4342834914259135, 2.4342363248709193, 2.4345459894985204, 2.4310052191291147, 2.431259572212332, 2.430744556762119, 2.4298986143666537, 2.43202997624189, 2.4328910051699735, 2.431760096980629, 2.4329351063432365, 2.432034642042589, 2.430589716226988, 2.432466686261307, 2.4348384966012486, 2.431965658817385, 2.4318815600891615, 2.431788545523958, 2.4317627622576183, 2.4311541645789188, 2.430588931285689, 2.4313370844804987, 2.4320045645013817, 2.4311343785772968, 2.4314945045558884, 2.431815194378933, 2.4307672597700347, 2.432600365875194, 2.4313993367851268, 2.4302082613771185, 2.430072775419514, 2.431524955188895, 2.4308629619272666, 2.4294371792835556, 2.428730879706898, 2.429448770576314, 2.4334846666489525, 2.434723534216043, 2.4296129411468756, 2.429626765901036, 2.4286914556876, 2.4292869712723104, 2.4285873630755446, 2.4309002437027805, 2.431754318951386, 2.4342952279621746, 2.4336829878426536, 2.431353918241554, 2.432646063161014, 2.433993234227248, 2.436747948328654, 2.4340659879111306, 2.435649529270742, 2.434136869089161, 2.4335575655763373, 2.4341701443363686, 2.4354337397075834, 2.439927154378155, 2.433901752744402, 2.431150856863689, 2.4298563402861797, 2.4323768369082748, 2.432764212290446, 2.434941223689488, 2.4308995856048634, 2.4292337499032857, 2.4300070020365596, 2.430266421808202, 2.42977452434734, 2.431407220649406, 2.4293747194882096, 2.429463092525213, 2.4335003079256206, 2.4318209837614413, 2.4299450374784923, 2.431062376166408, 2.4301652403300618, 2.4292940427908563, 2.429270418993945, 2.4311052762424614, 2.4293241348172643, 2.428886928386093, 2.430944977526986, 2.430241506087956, 2.428838409226516, 2.4328169615202153, 2.431406293205048, 2.4305783907572427, 2.430784620087722, 2.4302250927892226, 2.431581227845942, 2.4300782888002193, 2.4331721770156585, 2.4312988004856706, 2.4301259529414434, 2.4327066492760316, 2.4307034078294225, 2.431943036065313, 2.4321359112149192, 2.4304169148255648, 2.4297032892606136, 2.428365628707585, 2.4276386023742225], 'val_acc': [0.08908045939085714, 0.13300492604720376, 0.15106732242898205, 0.17200328336267048, 0.15722495801930358, 0.15763546709943874, 0.17323481062754426, 0.16954022910207364, 0.1744663376477356, 0.16830870225316003, 0.16871921108861274, 0.17200328350947997, 0.15927750319976525, 0.1773399010374041, 0.17651888292607024, 0.16707717511062747, 0.1605090302933613, 0.16748768397054845, 0.17241379249174216, 0.16789819307515186, 0.17857142810653193, 0.17980295512672323, 0.17446633757433086, 0.17651888275479252, 0.17651888282819725, 0.17487684662999778, 0.17528735575906945, 0.17651888299947496, 0.17733990091506288, 0.1711822653981461, 0.17159277445381302, 0.17159277442934479, 0.17364531948746523, 0.1793924461689293, 0.17282430152294084, 0.17610837384593506, 0.17036124731128047, 0.1736453196342747, 0.1752873556856647, 0.18021346428026316, 0.17487684672787077, 0.1769293918349277, 0.17980295512672323, 0.17569786483920463, 0.1765188828526655, 0.1777504099217933, 0.17405582854313217, 0.16830870198400932, 0.1752873556856647, 0.17569786481473637, 0.17816091897746025, 0.17857142805759543, 0.16995073828008178, 0.16912972019321618, 0.17159277442934479, 0.17528735563672823, 0.17528735575906945, 0.17980295522459622, 0.17857142795972245, 0.18185550042952614, 0.17733990089059462, 0.17405582854313217, 0.1789819371377306, 0.17569786476579988, 0.17077175626907443, 0.17118226532474135, 0.1691297200464067, 0.16830870193507283, 0.16954022910207364, 0.17528735563672823, 0.17487684677680726, 0.17816091902639675, 0.1773399010374041, 0.17364531970767944, 0.17692939195726892, 0.17610837372359384, 0.18103448226925578, 0.18513957263017913, 0.18103448226925578, 0.17610837387040332, 0.18185550030718492, 0.18349753657878914, 0.18144499130045447, 0.17569786474133164, 0.17733990084165815, 0.17692939181045947, 0.1773399008661264, 0.17692939190833243, 0.176518882901602, 0.18596059057023528, 0.17857142803312717, 0.18021346428026316, 0.18144499108024026, 0.17405582864100513, 0.1765188827303243, 0.18390804541424186, 0.17405582849419568, 0.1806239731891206, 0.17939244599765158, 0.17939244597318332, 0.1847290636234487, 0.18308702744971747, 0.18513957263017913, 0.17733990101293587, 0.1736453195119335, 0.18062397321358886, 0.1806239731891206, 0.17446633752539437, 0.17610837367465734, 0.1765188826813878, 0.1806239731891206, 0.17569786466792692, 0.18349753652985265, 0.18349753648091616, 0.1822660094362566, 0.18513957253230617, 0.17610837377253033, 0.18267651839405052, 0.1822660093873201, 0.18390804556105134, 0.18062397311571587, 0.17857142803312717, 0.18062397321358886, 0.17939244609552454, 0.17487684662999778, 0.1826765182227728, 0.18678160873050564, 0.18267651834511406, 0.17775040989732507, 0.17857142791078595, 0.1826765182717093, 0.18924466299110249, 0.18924466308897547, 0.18965517207123767, 0.18678160877944214, 0.18185550042952614, 0.18062397316465237, 0.18555008151456837, 0.1855500815879731, 0.18062397306677938, 0.18185550025824843, 0.18965517207123767, 0.18965517207123767, 0.19006568105349986, 0.18349753655432088, 0.18513957267911563, 0.1855500817103143, 0.18021346410898545, 0.18062397294443816, 0.18144499112917675, 0.18226600924051062, 0.1769293918349277, 0.19047619008469854, 0.18226600924051062, 0.1888341539599038, 0.18144499122704974, 0.18678160868156915, 0.18185550016037544, 0.18596059054576705, 0.19088669911589723, 0.18637109969930696, 0.18103448209797807, 0.14490968732802542, 0.17857142791078595, 0.17775040984838858, 0.1736453195119335, 0.1769293918838642, 0.17651888287713374, 0.17733990093953111, 0.17241379258961514, 0.1744663377211403, 0.17241379244280566, 0.17569786469239515, 0.17323481050520304, 0.17200328350947997, 0.17610837382146682, 0.1765188828526655, 0.17733990096399938, 0.17446633769667208, 0.17610837382146682, 0.17816091897746025, 0.18719211776170433, 0.18390804551211484, 0.1822660093383836, 0.18021346406004896, 0.1830870274252492, 0.18021346418239018, 0.18021346418239018, 0.1802134640355807, 0.18062397321358886, 0.1826765183695823, 0.18103448226925578, 0.18267651856532824, 0.1806239732380571, 0.17980295493097728, 0.17939244587531036, 0.1802134640355807, 0.18103448224478755, 0.17980295520012798, 0.1822660093383836, 0.17980295517565972, 0.18144499142279572, 0.17980295507778676, 0.1761083739193398, 0.17569786469239515, 0.18144499112917675, 0.1777504100196663, 0.18267651846745528, 0.18185550033165315, 0.18021346413345368, 0.17733990096399938, 0.17405582864100513, 0.17241379236940094, 0.1806239732869936, 0.17857142798419068, 0.18021346418239018, 0.18185550030718492, 0.18431855464118652, 0.18185550047846263, 0.18021346415792192, 0.1806239730178429, 0.1822660094851931, 0.18431855471459124, 0.18472906355004395, 0.18472906357451221, 0.17733990096399938, 0.1830870274252492, 0.17939244587531036, 0.18883415388649907, 0.1789819370153894, 0.1855500815879731, 0.18555008168584608, 0.18760262691524424, 0.1806239732380571, 0.179802955004382, 0.1818555002827167, 0.1851395725567744, 0.18267651819830458, 0.18842364490423688, 0.18555008161244135, 0.18431855449437703, 0.18678160880391037, 0.18308702740078098, 0.18185550038058965, 0.18349753640751143, 0.18924466299110249, 0.1880131358241017, 0.18678160863263266, 0.17610837384593506, 0.18390804541424186, 0.18678160865710092, 0.1818555002337802, 0.18801313579963347, 0.1748768464831883, 0.18842364487976865, 0.1843185542986311, 0.17692939195726892, 0.1871921176882996, 0.1826765183695823, 0.18308702740078098, 0.1769293918838642, 0.18513957272805212, 0.1826765182717093, 0.17651888282819725, 0.18390804541424186, 0.18431855434756755, 0.17241379254067865, 0.18555008156350486, 0.18513957272805212, 0.18021346415792192, 0.18021346415792192, 0.18390804541424186, 0.1752873557346012, 0.18144499117811325, 0.18185550025824843, 0.18308702732737625, 0.18185550033165315, 0.18472906364791694, 0.1826765183695823, 0.17980295520012798, 0.18144499117811325, 0.18103448226925578, 0.18431855449437703, 0.1798029551511915, 0.17857142798419068, 0.17692939185939593, 0.179802955004382, 0.18513957263017913, 0.18144499130045447, 0.1822660093383836, 0.17775040989732507, 0.18144499127598623, 0.17651888287713374, 0.18185550030718492, 0.18349753638304317, 0.1818555003561214, 0.18267651841851879, 0.18144499132492273, 0.18103448229372404, 0.18390804551211484], 'loss': [2.6898260990941796, 2.6563991402698495, 2.6143506934265828, 2.566381651566993, 2.5306774154091274, 2.515535578345861, 2.500340617510817, 2.490397558760594, 2.480443569473173, 2.4745904075536393, 2.4686753105578725, 2.466785798572172, 2.4677753739288457, 2.461077264104291, 2.460330975373912, 2.4608798387359054, 2.4572657461773444, 2.455140872951406, 2.455135201820358, 2.451943640052905, 2.4495549786506983, 2.450210398178571, 2.448401967896573, 2.4461750789344676, 2.4495627021397897, 2.444867598300597, 2.445948903027012, 2.444326861340407, 2.441926168563185, 2.4417297585544153, 2.4415426003614735, 2.449330964372388, 2.442524262571237, 2.4445200331646806, 2.441208433174744, 2.438721211881853, 2.4381244889519786, 2.438549036362822, 2.4406235862316783, 2.4413362255331426, 2.4372454374967414, 2.436798142995188, 2.438196510358023, 2.4367300581883113, 2.4355599907634193, 2.436427676751139, 2.4364026180283, 2.439980642017153, 2.439182345823096, 2.4371745874259996, 2.4382061972999964, 2.437593716125958, 2.4349660776479043, 2.437936950953834, 2.436705973505729, 2.4429313005607964, 2.4376509438305174, 2.4330074277991387, 2.432623967301919, 2.4340156253603205, 2.433012601237522, 2.435612184947521, 2.4333036806549133, 2.4332487534203815, 2.4331968974283833, 2.432390882445067, 2.4324444999929815, 2.4339201088069156, 2.434396284267888, 2.433347779475688, 2.432389676154761, 2.4325352622743015, 2.4323611409267607, 2.4320725086533312, 2.43077138924256, 2.430349778345723, 2.4307308002174266, 2.4319304753133157, 2.430801568236929, 2.434024414830139, 2.432240556740418, 2.4308543129868085, 2.4312084162749303, 2.435244762432404, 2.4310316525445583, 2.4301279263819513, 2.430605396842565, 2.4308449512144867, 2.4340279263882176, 2.43432534950225, 2.4316799741506085, 2.4297898074195126, 2.4289538629245953, 2.429314284843586, 2.429853755394781, 2.429421064153589, 2.4307302385874596, 2.430906522739105, 2.431306545053909, 2.429832863464982, 2.4320211060972428, 2.428719017001393, 2.429238412904054, 2.4301770543170904, 2.432083413537278, 2.4291346101545455, 2.4295147270147806, 2.4289489006849285, 2.4299536307489604, 2.429040180045721, 2.4287392497797033, 2.4291920098925517, 2.4338678879904307, 2.4295951862844354, 2.4292156963622547, 2.429665308664467, 2.4292673932453446, 2.4280040959313176, 2.428117217763004, 2.4289458671389665, 2.427450809145855, 2.4291948989186687, 2.427751874923706, 2.4272883425013485, 2.4278035936414337, 2.426873024875868, 2.4272755804982276, 2.427067582367382, 2.430695393442863, 2.4278035761149757, 2.426807638360245, 2.4276881604713583, 2.428589910060718, 2.4263596180283313, 2.428326276097699, 2.425976751570339, 2.426919215317869, 2.4265673370087173, 2.425315800241866, 2.4269301172644204, 2.4272228236071136, 2.4263509123721896, 2.426242799876407, 2.42528331676303, 2.426797203361621, 2.425171809715412, 2.4255575590309912, 2.4265490453590846, 2.4259529324282854, 2.4270206283984486, 2.4236673464765293, 2.424336374173174, 2.4245413522700754, 2.4270017981284453, 2.425826242376402, 2.424505099036121, 2.4239565499754168, 2.4247105292960605, 2.4239185587826206, 2.424240231856673, 2.4259024211758216, 2.426510470799597, 2.484359023115718, 2.4771080519384427, 2.435125822404082, 2.431777510652797, 2.4305882699680526, 2.4292966679136367, 2.428884235985225, 2.432242880024215, 2.4339611574364883, 2.4326131664017634, 2.43389289305685, 2.4300114392744687, 2.428238305418888, 2.4278914245981453, 2.428460355656837, 2.4279304911713337, 2.428611690846312, 2.4289021008313316, 2.427675941984267, 2.4267865184885764, 2.428579545853319, 2.4273555407044336, 2.4282381337770937, 2.426925251175491, 2.424949891268595, 2.4264919492498316, 2.4255745839778893, 2.424634480427423, 2.425233261394305, 2.4261273849181815, 2.4262646989411154, 2.4261915155994327, 2.424907152070157, 2.4241601064709424, 2.4235207559636485, 2.424993939761998, 2.426572073264778, 2.4240716568987963, 2.42290317331741, 2.4233239678142007, 2.4233249101305887, 2.424303170445029, 2.4240799254705285, 2.424060853452898, 2.4237660809464034, 2.4226968863171963, 2.423029704945778, 2.4225290916294044, 2.421170209908143, 2.424068448577818, 2.426127631463554, 2.422958361980117, 2.421824839374613, 2.4225105022258093, 2.4209957920795104, 2.421720655351204, 2.420941933567274, 2.4228006692882436, 2.4214542013908558, 2.421098292485889, 2.421982677075897, 2.4202643242704793, 2.4209376048258444, 2.4225705830223507, 2.4207649485531286, 2.4217615087419073, 2.420934300745782, 2.4197017766611775, 2.4196084332906733, 2.422957107810269, 2.417789732504185, 2.419848776842779, 2.4223268615636493, 2.421082631273681, 2.419392221125734, 2.4179247590795434, 2.4192854539569644, 2.4160458719461118, 2.4173621971749184, 2.4174142374395102, 2.4193777567062535, 2.4186480868768396, 2.4181801612861835, 2.416778853394902, 2.4185765538617083, 2.418632432028988, 2.4194222388571047, 2.4180726928632605, 2.4209732693078827, 2.417982024970241, 2.4192478923582197, 2.4212737608249673, 2.431454577534106, 2.424357312220078, 2.4207676332344508, 2.4235581494944296, 2.4227994190349227, 2.4255759845279323, 2.4251872950755593, 2.424468014127665, 2.4254750310517923, 2.42512291735937, 2.4239850094186206, 2.42398855524631, 2.4224297821154583, 2.4236497757615982, 2.4227887089981923, 2.4285640317066983, 2.4258707484180677, 2.423386063565953, 2.4232607047415855, 2.422936442353642, 2.4230323966768488, 2.423373094672295, 2.4230751721031614, 2.4226289285037064, 2.4209113766280534, 2.4207529190384633, 2.4218653938363954, 2.4210525063273844, 2.422298812866211, 2.4274387035526535, 2.4217560706442143, 2.419742703388849, 2.4204005293288025, 2.4193727093800383, 2.41902149717421, 2.4188805287378767, 2.4188822580803593, 2.418631780710553, 2.419901272011978, 2.4212323950546235, 2.41916288193736, 2.420567234830445, 2.418311581327685, 2.4172497923614062, 2.4171846433831434, 2.417281766397997], 'acc': [0.08634496919917864, 0.11611909651535982, 0.14425051335314215, 0.1553388090349076, 0.16232032854821402, 0.16344969199484624, 0.16806981519507186, 0.1698151950718686, 0.1685831622237787, 0.17207392197431234, 0.16837782341168403, 0.1746406570903085, 0.16581108830180746, 0.17607802874743325, 0.17494866529774128, 0.17125256673511294, 0.16786447639215654, 0.17741273101227975, 0.1761806981550105, 0.1757700205400005, 0.17628336755646817, 0.17741273100921995, 0.17782340863034957, 0.177926078034867, 0.1775154004137374, 0.18080082135523615, 0.17772073921971251, 0.1777207392258321, 0.1773100616047025, 0.1797741273100616, 0.17802874743326488, 0.17094455853379972, 0.17946611909956903, 0.17885010267246432, 0.1797741273100616, 0.17731006160776228, 0.17843942505745428, 0.18049281314780335, 0.1809034907597536, 0.18069815196295783, 0.18008213552361396, 0.17854209445891195, 0.1788501026755241, 0.177926078034867, 0.17874743326794684, 0.17731006161388185, 0.17782340862728976, 0.17535934292193067, 0.17782340862728976, 0.180287474344888, 0.1776180698029559, 0.18182751540041067, 0.17895277207392196, 0.18080082136135572, 0.1786447638634294, 0.1784394250513347, 0.18347022586045078, 0.18398357288303807, 0.1833675564681725, 0.1833675564681725, 0.1820328542094456, 0.18213552361702282, 0.1809034907597536, 0.18398357290139677, 0.1822381930184805, 0.18028747433264888, 0.17843942505745428, 0.17422997947223867, 0.1783367556590564, 0.17823408624841935, 0.1816221766036149, 0.18080082136135572, 0.18193018480798792, 0.18090349074751444, 0.17915811089519604, 0.18080082136747527, 0.18049281315392293, 0.18388090349687933, 0.18480492813141683, 0.18357289526496826, 0.18316221765913757, 0.18572895278431306, 0.18542094456464114, 0.18193018480492812, 0.1845995893254417, 0.18141683778540066, 0.18008213552667374, 0.17833675564681725, 0.18080082135523615, 0.18141683778234086, 0.18203285421250537, 0.18470225872995918, 0.18624229980078078, 0.18388090349687933, 0.18213552361396304, 0.179671457893305, 0.18439425052558617, 0.1817248460020128, 0.18521560574948664, 0.18357289528332696, 0.18182751540653025, 0.18634496920529822, 0.18470225872689938, 0.18285420943334607, 0.1809034907597536, 0.18429158111188937, 0.18275154004412755, 0.18275154004106775, 0.18254620123815243, 0.18295687886234183, 0.1824435318275154, 0.18223819302460006, 0.18336755647429206, 0.1821355236262022, 0.18090349076281337, 0.18326488706977462, 0.18244353183057518, 0.18418891169207297, 0.1841889117073719, 0.18377823409236188, 0.18357289527720738, 0.18480492813447663, 0.18049281314474355, 0.18316221767137672, 0.18213552362008262, 0.18388090349381955, 0.1852156057617258, 0.1851129363449692, 0.1841889117073719, 0.1797741273100616, 0.18480492814365598, 0.18531827516012367, 0.1797741273100616, 0.18408624229979467, 0.18316221767137672, 0.18634496920529822, 0.18429158112106872, 0.18377823408930208, 0.18449691992398404, 0.18542094455852157, 0.1855236139691586, 0.18613963039320353, 0.1850102669526909, 0.18634496920529822, 0.17885010266940451, 0.18624229980078078, 0.18501026694657133, 0.1819301848110477, 0.18295687885622225, 0.18367556468478463, 0.18531827514176497, 0.18562628337367604, 0.18531827516624325, 0.1866529774188506, 0.18531827516012367, 0.18562628337061626, 0.18398357290139677, 0.18655030801127334, 0.1851129363449692, 0.1850102669526909, 0.1834702258788095, 0.18470225873301896, 0.1698151950718686, 0.1659137577063249, 0.1857289527751337, 0.18511293634802897, 0.18151950717461918, 0.18244353183057518, 0.18028747433876843, 0.17885010266940451, 0.176796714585175, 0.18008213552361396, 0.1812114989855451, 0.18234086242605774, 0.18511293634802897, 0.1822381930184805, 0.18110882958102764, 0.18429158112106872, 0.18357289528026718, 0.18377823409236188, 0.1859342915872284, 0.18603696099786543, 0.1861396303962633, 0.1844969199178645, 0.18316221766219737, 0.18593429158110883, 0.1866529774188506, 0.18429158110882957, 0.18162217659749536, 0.182443531833635, 0.18408624229979467, 0.1839835728952772, 0.1827515400533069, 0.18542094456464114, 0.1851129363449692, 0.18449691993010361, 0.18326488706977462, 0.18305954826685927, 0.18603696098562628, 0.18613963040238288, 0.18285420945782435, 0.18542094455852157, 0.18285420943334607, 0.18305954825767992, 0.18090349076281337, 0.18254620123203286, 0.18603696099174585, 0.18634496919917864, 0.18357289527720738, 0.18726899384289553, 0.18521560575254645, 0.18511293635108877, 0.17792607802874744, 0.18234086242299793, 0.18459958932238194, 0.1810061601765102, 0.18388090349687933, 0.18634496920529822, 0.18326488706365504, 0.18162217659137578, 0.18542094456464114, 0.18655030801433312, 0.18305954825767992, 0.1841889117104317, 0.1857289527751337, 0.17710472279566758, 0.18336755647123226, 0.18141683778540066, 0.18377823408930208, 0.18685831622788548, 0.18747433264887065, 0.18285420943334607, 0.18449691992092426, 0.18388090349687933, 0.18449691992398404, 0.18203285421556517, 0.18511293635108877, 0.18305954825767992, 0.18418891170431212, 0.18531827515400412, 0.18696098562934316, 0.18501026694657133, 0.18316221766219737, 0.18388090349075975, 0.18562628336755646, 0.1868583162217659, 0.18470225872995918, 0.1864476386036961, 0.1819301848110477, 0.18367556468478463, 0.18028747433876843, 0.18347022587268993, 0.1850102669526909, 0.1815195071868583, 0.17874743327100664, 0.1799794661252161, 0.18162217659137578, 0.17813141684084213, 0.17340862424221862, 0.1824435318275154, 0.1805954825523208, 0.17915811088601666, 0.18295687885316245, 0.17874743326488707, 0.1786447638634294, 0.177926078034867, 0.17946611909650925, 0.1820328542094456, 0.18275154004718733, 0.17987679671763884, 0.1810061601765102, 0.18090349076587317, 0.18141683778540066, 0.18162217659137578, 0.17895277208004154, 0.17915811088907646, 0.18090349076587317, 0.1837782340862423, 0.182443531833635, 0.17987679670233991, 0.18326488706977462, 0.1811088295687885, 0.18069815195377847, 0.17823408625453893, 0.18049281314474355, 0.1811088295687885, 0.17967145791166372, 0.184394250513347, 0.1820328542094456, 0.18182751541264983, 0.18357289528944654, 0.18418891170431212, 0.18326488706977462, 0.18049281314780335, 0.18357289528332696, 0.18357289528026718, 0.18542094455852157, 0.1834702258849291, 0.18182751540041067, 0.18336755647123226]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
