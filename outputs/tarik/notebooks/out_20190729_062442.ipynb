{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf83.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 06:24:42 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '2Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '01', '02', '03', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000142018FAEB8>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001425C5E7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6083, Accuracy:0.2057, Validation Loss:1.6043, Validation Accuracy:0.2135\n",
    "Epoch #2: Loss:1.6054, Accuracy:0.2316, Validation Loss:1.6022, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6003, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6027, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6048, Accuracy:0.2333, Validation Loss:1.6046, Validation Accuracy:0.2529\n",
    "Epoch #10: Loss:1.6047, Accuracy:0.2444, Validation Loss:1.6019, Validation Accuracy:0.2348\n",
    "Epoch #11: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6073, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6043, Accuracy:0.2312, Validation Loss:1.6021, Validation Accuracy:0.2529\n",
    "Epoch #21: Loss:1.6040, Accuracy:0.2366, Validation Loss:1.6018, Validation Accuracy:0.2496\n",
    "Epoch #22: Loss:1.6036, Accuracy:0.2353, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6038, Accuracy:0.2337, Validation Loss:1.6009, Validation Accuracy:0.2529\n",
    "Epoch #25: Loss:1.6033, Accuracy:0.2357, Validation Loss:1.6007, Validation Accuracy:0.2545\n",
    "Epoch #26: Loss:1.6031, Accuracy:0.2353, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #27: Loss:1.6030, Accuracy:0.2341, Validation Loss:1.6002, Validation Accuracy:0.2496\n",
    "Epoch #28: Loss:1.6030, Accuracy:0.2341, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #29: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #30: Loss:1.6040, Accuracy:0.2337, Validation Loss:1.6034, Validation Accuracy:0.2381\n",
    "Epoch #31: Loss:1.6038, Accuracy:0.2374, Validation Loss:1.6027, Validation Accuracy:0.2430\n",
    "Epoch #32: Loss:1.6035, Accuracy:0.2353, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #33: Loss:1.6031, Accuracy:0.2324, Validation Loss:1.6024, Validation Accuracy:0.2447\n",
    "Epoch #34: Loss:1.6027, Accuracy:0.2353, Validation Loss:1.6022, Validation Accuracy:0.2562\n",
    "Epoch #35: Loss:1.6023, Accuracy:0.2382, Validation Loss:1.6020, Validation Accuracy:0.2578\n",
    "Epoch #36: Loss:1.6022, Accuracy:0.2349, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #37: Loss:1.6024, Accuracy:0.2398, Validation Loss:1.6015, Validation Accuracy:0.2545\n",
    "Epoch #38: Loss:1.6019, Accuracy:0.2398, Validation Loss:1.6006, Validation Accuracy:0.2529\n",
    "Epoch #39: Loss:1.6017, Accuracy:0.2398, Validation Loss:1.6010, Validation Accuracy:0.2512\n",
    "Epoch #40: Loss:1.6018, Accuracy:0.2394, Validation Loss:1.6012, Validation Accuracy:0.2512\n",
    "Epoch #41: Loss:1.6013, Accuracy:0.2386, Validation Loss:1.5995, Validation Accuracy:0.2529\n",
    "Epoch #42: Loss:1.6018, Accuracy:0.2402, Validation Loss:1.5993, Validation Accuracy:0.2578\n",
    "Epoch #43: Loss:1.6012, Accuracy:0.2398, Validation Loss:1.5992, Validation Accuracy:0.2479\n",
    "Epoch #44: Loss:1.6018, Accuracy:0.2357, Validation Loss:1.5989, Validation Accuracy:0.2463\n",
    "Epoch #45: Loss:1.6007, Accuracy:0.2361, Validation Loss:1.5992, Validation Accuracy:0.2479\n",
    "Epoch #46: Loss:1.6016, Accuracy:0.2419, Validation Loss:1.5989, Validation Accuracy:0.2562\n",
    "Epoch #47: Loss:1.6012, Accuracy:0.2415, Validation Loss:1.5996, Validation Accuracy:0.2447\n",
    "Epoch #48: Loss:1.6016, Accuracy:0.2390, Validation Loss:1.5991, Validation Accuracy:0.2578\n",
    "Epoch #49: Loss:1.6008, Accuracy:0.2427, Validation Loss:1.5992, Validation Accuracy:0.2529\n",
    "Epoch #50: Loss:1.6008, Accuracy:0.2411, Validation Loss:1.5984, Validation Accuracy:0.2545\n",
    "Epoch #51: Loss:1.6006, Accuracy:0.2386, Validation Loss:1.5990, Validation Accuracy:0.2578\n",
    "Epoch #52: Loss:1.6009, Accuracy:0.2407, Validation Loss:1.5992, Validation Accuracy:0.2545\n",
    "Epoch #53: Loss:1.6011, Accuracy:0.2419, Validation Loss:1.5997, Validation Accuracy:0.2463\n",
    "Epoch #54: Loss:1.6005, Accuracy:0.2394, Validation Loss:1.5995, Validation Accuracy:0.2578\n",
    "Epoch #55: Loss:1.6005, Accuracy:0.2411, Validation Loss:1.5994, Validation Accuracy:0.2594\n",
    "Epoch #56: Loss:1.6003, Accuracy:0.2394, Validation Loss:1.6000, Validation Accuracy:0.2545\n",
    "Epoch #57: Loss:1.6000, Accuracy:0.2423, Validation Loss:1.6001, Validation Accuracy:0.2545\n",
    "Epoch #58: Loss:1.5999, Accuracy:0.2423, Validation Loss:1.5997, Validation Accuracy:0.2529\n",
    "Epoch #59: Loss:1.5998, Accuracy:0.2419, Validation Loss:1.5996, Validation Accuracy:0.2545\n",
    "Epoch #60: Loss:1.6002, Accuracy:0.2431, Validation Loss:1.6000, Validation Accuracy:0.2562\n",
    "Epoch #61: Loss:1.6003, Accuracy:0.2439, Validation Loss:1.6005, Validation Accuracy:0.2529\n",
    "Epoch #62: Loss:1.5998, Accuracy:0.2398, Validation Loss:1.6017, Validation Accuracy:0.2397\n",
    "Epoch #63: Loss:1.6002, Accuracy:0.2427, Validation Loss:1.6013, Validation Accuracy:0.2463\n",
    "Epoch #64: Loss:1.6001, Accuracy:0.2439, Validation Loss:1.6011, Validation Accuracy:0.2447\n",
    "Epoch #65: Loss:1.5999, Accuracy:0.2419, Validation Loss:1.6012, Validation Accuracy:0.2479\n",
    "Epoch #66: Loss:1.5997, Accuracy:0.2427, Validation Loss:1.6010, Validation Accuracy:0.2512\n",
    "Epoch #67: Loss:1.5997, Accuracy:0.2431, Validation Loss:1.6009, Validation Accuracy:0.2512\n",
    "Epoch #68: Loss:1.5995, Accuracy:0.2431, Validation Loss:1.6015, Validation Accuracy:0.2479\n",
    "Epoch #69: Loss:1.6003, Accuracy:0.2407, Validation Loss:1.6007, Validation Accuracy:0.2512\n",
    "Epoch #70: Loss:1.6009, Accuracy:0.2415, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #71: Loss:1.5995, Accuracy:0.2423, Validation Loss:1.6004, Validation Accuracy:0.2479\n",
    "Epoch #72: Loss:1.5998, Accuracy:0.2431, Validation Loss:1.6005, Validation Accuracy:0.2463\n",
    "Epoch #73: Loss:1.5997, Accuracy:0.2398, Validation Loss:1.6002, Validation Accuracy:0.2397\n",
    "Epoch #74: Loss:1.6007, Accuracy:0.2390, Validation Loss:1.5992, Validation Accuracy:0.2447\n",
    "Epoch #75: Loss:1.6013, Accuracy:0.2394, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #76: Loss:1.6029, Accuracy:0.2411, Validation Loss:1.5987, Validation Accuracy:0.2545\n",
    "Epoch #77: Loss:1.6001, Accuracy:0.2382, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #78: Loss:1.5995, Accuracy:0.2423, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #79: Loss:1.5999, Accuracy:0.2435, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #80: Loss:1.5994, Accuracy:0.2398, Validation Loss:1.5991, Validation Accuracy:0.2512\n",
    "Epoch #81: Loss:1.5997, Accuracy:0.2427, Validation Loss:1.5985, Validation Accuracy:0.2512\n",
    "Epoch #82: Loss:1.5989, Accuracy:0.2431, Validation Loss:1.5994, Validation Accuracy:0.2496\n",
    "Epoch #83: Loss:1.5993, Accuracy:0.2394, Validation Loss:1.5998, Validation Accuracy:0.2447\n",
    "Epoch #84: Loss:1.5991, Accuracy:0.2423, Validation Loss:1.5988, Validation Accuracy:0.2512\n",
    "Epoch #85: Loss:1.5990, Accuracy:0.2419, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #86: Loss:1.5992, Accuracy:0.2419, Validation Loss:1.6004, Validation Accuracy:0.2447\n",
    "Epoch #87: Loss:1.5988, Accuracy:0.2439, Validation Loss:1.5997, Validation Accuracy:0.2479\n",
    "Epoch #88: Loss:1.5989, Accuracy:0.2407, Validation Loss:1.6022, Validation Accuracy:0.2447\n",
    "Epoch #89: Loss:1.5991, Accuracy:0.2415, Validation Loss:1.6014, Validation Accuracy:0.2479\n",
    "Epoch #90: Loss:1.5986, Accuracy:0.2419, Validation Loss:1.6014, Validation Accuracy:0.2479\n",
    "Epoch #91: Loss:1.5992, Accuracy:0.2407, Validation Loss:1.6009, Validation Accuracy:0.2479\n",
    "Epoch #92: Loss:1.5992, Accuracy:0.2419, Validation Loss:1.6009, Validation Accuracy:0.2414\n",
    "Epoch #93: Loss:1.5998, Accuracy:0.2411, Validation Loss:1.6000, Validation Accuracy:0.2447\n",
    "Epoch #94: Loss:1.6008, Accuracy:0.2357, Validation Loss:1.6001, Validation Accuracy:0.2430\n",
    "Epoch #95: Loss:1.5996, Accuracy:0.2361, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #96: Loss:1.5992, Accuracy:0.2402, Validation Loss:1.6012, Validation Accuracy:0.2397\n",
    "Epoch #97: Loss:1.5994, Accuracy:0.2361, Validation Loss:1.6000, Validation Accuracy:0.2479\n",
    "Epoch #98: Loss:1.5985, Accuracy:0.2394, Validation Loss:1.5997, Validation Accuracy:0.2496\n",
    "Epoch #99: Loss:1.5989, Accuracy:0.2390, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #100: Loss:1.5985, Accuracy:0.2394, Validation Loss:1.5998, Validation Accuracy:0.2463\n",
    "Epoch #101: Loss:1.5988, Accuracy:0.2390, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #102: Loss:1.5986, Accuracy:0.2394, Validation Loss:1.5990, Validation Accuracy:0.2414\n",
    "Epoch #103: Loss:1.6005, Accuracy:0.2382, Validation Loss:1.6001, Validation Accuracy:0.2479\n",
    "Epoch #104: Loss:1.5984, Accuracy:0.2382, Validation Loss:1.5999, Validation Accuracy:0.2463\n",
    "Epoch #105: Loss:1.5986, Accuracy:0.2431, Validation Loss:1.5986, Validation Accuracy:0.2496\n",
    "Epoch #106: Loss:1.5983, Accuracy:0.2390, Validation Loss:1.5984, Validation Accuracy:0.2512\n",
    "Epoch #107: Loss:1.5982, Accuracy:0.2386, Validation Loss:1.5984, Validation Accuracy:0.2496\n",
    "Epoch #108: Loss:1.5984, Accuracy:0.2419, Validation Loss:1.5985, Validation Accuracy:0.2496\n",
    "Epoch #109: Loss:1.5981, Accuracy:0.2390, Validation Loss:1.5987, Validation Accuracy:0.2496\n",
    "Epoch #110: Loss:1.5982, Accuracy:0.2394, Validation Loss:1.5982, Validation Accuracy:0.2496\n",
    "Epoch #111: Loss:1.5981, Accuracy:0.2415, Validation Loss:1.5980, Validation Accuracy:0.2496\n",
    "Epoch #112: Loss:1.5984, Accuracy:0.2390, Validation Loss:1.5983, Validation Accuracy:0.2512\n",
    "Epoch #113: Loss:1.5985, Accuracy:0.2394, Validation Loss:1.5982, Validation Accuracy:0.2496\n",
    "Epoch #114: Loss:1.5991, Accuracy:0.2394, Validation Loss:1.5985, Validation Accuracy:0.2512\n",
    "Epoch #115: Loss:1.5991, Accuracy:0.2427, Validation Loss:1.6042, Validation Accuracy:0.2282\n",
    "Epoch #116: Loss:1.6043, Accuracy:0.2345, Validation Loss:1.6075, Validation Accuracy:0.2348\n",
    "Epoch #117: Loss:1.6022, Accuracy:0.2341, Validation Loss:1.6056, Validation Accuracy:0.2365\n",
    "Epoch #118: Loss:1.6006, Accuracy:0.2480, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #119: Loss:1.6022, Accuracy:0.2349, Validation Loss:1.6042, Validation Accuracy:0.2315\n",
    "Epoch #120: Loss:1.5997, Accuracy:0.2370, Validation Loss:1.6004, Validation Accuracy:0.2447\n",
    "Epoch #121: Loss:1.5997, Accuracy:0.2456, Validation Loss:1.5973, Validation Accuracy:0.2512\n",
    "Epoch #122: Loss:1.5993, Accuracy:0.2407, Validation Loss:1.5992, Validation Accuracy:0.2397\n",
    "Epoch #123: Loss:1.6011, Accuracy:0.2370, Validation Loss:1.5978, Validation Accuracy:0.2463\n",
    "Epoch #124: Loss:1.5998, Accuracy:0.2435, Validation Loss:1.5972, Validation Accuracy:0.2463\n",
    "Epoch #125: Loss:1.6000, Accuracy:0.2357, Validation Loss:1.5963, Validation Accuracy:0.2463\n",
    "Epoch #126: Loss:1.6005, Accuracy:0.2366, Validation Loss:1.5949, Validation Accuracy:0.2430\n",
    "Epoch #127: Loss:1.6010, Accuracy:0.2382, Validation Loss:1.5956, Validation Accuracy:0.2315\n",
    "Epoch #128: Loss:1.6007, Accuracy:0.2382, Validation Loss:1.5955, Validation Accuracy:0.2414\n",
    "Epoch #129: Loss:1.6011, Accuracy:0.2374, Validation Loss:1.5950, Validation Accuracy:0.2414\n",
    "Epoch #130: Loss:1.6017, Accuracy:0.2337, Validation Loss:1.5975, Validation Accuracy:0.2512\n",
    "Epoch #131: Loss:1.6005, Accuracy:0.2415, Validation Loss:1.5955, Validation Accuracy:0.2430\n",
    "Epoch #132: Loss:1.6006, Accuracy:0.2374, Validation Loss:1.5951, Validation Accuracy:0.2430\n",
    "Epoch #133: Loss:1.6007, Accuracy:0.2349, Validation Loss:1.5952, Validation Accuracy:0.2463\n",
    "Epoch #134: Loss:1.6001, Accuracy:0.2386, Validation Loss:1.5950, Validation Accuracy:0.2496\n",
    "Epoch #135: Loss:1.6004, Accuracy:0.2374, Validation Loss:1.5948, Validation Accuracy:0.2463\n",
    "Epoch #136: Loss:1.5996, Accuracy:0.2378, Validation Loss:1.5946, Validation Accuracy:0.2447\n",
    "Epoch #137: Loss:1.6000, Accuracy:0.2398, Validation Loss:1.5945, Validation Accuracy:0.2479\n",
    "Epoch #138: Loss:1.5992, Accuracy:0.2378, Validation Loss:1.5944, Validation Accuracy:0.2463\n",
    "Epoch #139: Loss:1.5995, Accuracy:0.2361, Validation Loss:1.5955, Validation Accuracy:0.2529\n",
    "Epoch #140: Loss:1.5991, Accuracy:0.2402, Validation Loss:1.5954, Validation Accuracy:0.2496\n",
    "Epoch #141: Loss:1.5989, Accuracy:0.2386, Validation Loss:1.5967, Validation Accuracy:0.2512\n",
    "Epoch #142: Loss:1.5993, Accuracy:0.2382, Validation Loss:1.5966, Validation Accuracy:0.2512\n",
    "Epoch #143: Loss:1.6002, Accuracy:0.2423, Validation Loss:1.5966, Validation Accuracy:0.2447\n",
    "Epoch #144: Loss:1.5987, Accuracy:0.2402, Validation Loss:1.5970, Validation Accuracy:0.2529\n",
    "Epoch #145: Loss:1.5992, Accuracy:0.2398, Validation Loss:1.5966, Validation Accuracy:0.2529\n",
    "Epoch #146: Loss:1.5994, Accuracy:0.2361, Validation Loss:1.5968, Validation Accuracy:0.2512\n",
    "Epoch #147: Loss:1.5995, Accuracy:0.2398, Validation Loss:1.5965, Validation Accuracy:0.2447\n",
    "Epoch #148: Loss:1.5982, Accuracy:0.2407, Validation Loss:1.5967, Validation Accuracy:0.2512\n",
    "Epoch #149: Loss:1.5988, Accuracy:0.2394, Validation Loss:1.5966, Validation Accuracy:0.2529\n",
    "Epoch #150: Loss:1.5984, Accuracy:0.2398, Validation Loss:1.5967, Validation Accuracy:0.2512\n",
    "Epoch #151: Loss:1.5982, Accuracy:0.2411, Validation Loss:1.5968, Validation Accuracy:0.2512\n",
    "Epoch #152: Loss:1.5985, Accuracy:0.2411, Validation Loss:1.5966, Validation Accuracy:0.2512\n",
    "Epoch #153: Loss:1.5984, Accuracy:0.2411, Validation Loss:1.5970, Validation Accuracy:0.2529\n",
    "Epoch #154: Loss:1.5981, Accuracy:0.2411, Validation Loss:1.5967, Validation Accuracy:0.2545\n",
    "Epoch #155: Loss:1.5982, Accuracy:0.2415, Validation Loss:1.5970, Validation Accuracy:0.2545\n",
    "Epoch #156: Loss:1.5983, Accuracy:0.2415, Validation Loss:1.5970, Validation Accuracy:0.2545\n",
    "Epoch #157: Loss:1.5979, Accuracy:0.2411, Validation Loss:1.5971, Validation Accuracy:0.2512\n",
    "Epoch #158: Loss:1.5981, Accuracy:0.2411, Validation Loss:1.5971, Validation Accuracy:0.2545\n",
    "Epoch #159: Loss:1.5983, Accuracy:0.2415, Validation Loss:1.5970, Validation Accuracy:0.2562\n",
    "Epoch #160: Loss:1.6011, Accuracy:0.2374, Validation Loss:1.5976, Validation Accuracy:0.2562\n",
    "Epoch #161: Loss:1.5979, Accuracy:0.2398, Validation Loss:1.5991, Validation Accuracy:0.2430\n",
    "Epoch #162: Loss:1.5996, Accuracy:0.2435, Validation Loss:1.5977, Validation Accuracy:0.2496\n",
    "Epoch #163: Loss:1.5985, Accuracy:0.2402, Validation Loss:1.5983, Validation Accuracy:0.2512\n",
    "Epoch #164: Loss:1.5991, Accuracy:0.2357, Validation Loss:1.5976, Validation Accuracy:0.2529\n",
    "Epoch #165: Loss:1.5981, Accuracy:0.2407, Validation Loss:1.5979, Validation Accuracy:0.2414\n",
    "Epoch #166: Loss:1.5984, Accuracy:0.2419, Validation Loss:1.5978, Validation Accuracy:0.2529\n",
    "Epoch #167: Loss:1.5989, Accuracy:0.2366, Validation Loss:1.5973, Validation Accuracy:0.2529\n",
    "Epoch #168: Loss:1.5980, Accuracy:0.2427, Validation Loss:1.5979, Validation Accuracy:0.2414\n",
    "Epoch #169: Loss:1.5983, Accuracy:0.2419, Validation Loss:1.5969, Validation Accuracy:0.2512\n",
    "Epoch #170: Loss:1.5984, Accuracy:0.2402, Validation Loss:1.5971, Validation Accuracy:0.2545\n",
    "Epoch #171: Loss:1.5983, Accuracy:0.2411, Validation Loss:1.5969, Validation Accuracy:0.2562\n",
    "Epoch #172: Loss:1.5980, Accuracy:0.2423, Validation Loss:1.5969, Validation Accuracy:0.2562\n",
    "Epoch #173: Loss:1.5980, Accuracy:0.2419, Validation Loss:1.5973, Validation Accuracy:0.2512\n",
    "Epoch #174: Loss:1.5986, Accuracy:0.2427, Validation Loss:1.5977, Validation Accuracy:0.2562\n",
    "Epoch #175: Loss:1.5976, Accuracy:0.2427, Validation Loss:1.5975, Validation Accuracy:0.2529\n",
    "Epoch #176: Loss:1.5982, Accuracy:0.2411, Validation Loss:1.5970, Validation Accuracy:0.2562\n",
    "Epoch #177: Loss:1.5987, Accuracy:0.2398, Validation Loss:1.5974, Validation Accuracy:0.2512\n",
    "Epoch #178: Loss:1.5977, Accuracy:0.2448, Validation Loss:1.5978, Validation Accuracy:0.2496\n",
    "Epoch #179: Loss:1.5976, Accuracy:0.2419, Validation Loss:1.5975, Validation Accuracy:0.2562\n",
    "Epoch #180: Loss:1.5973, Accuracy:0.2402, Validation Loss:1.5975, Validation Accuracy:0.2545\n",
    "Epoch #181: Loss:1.5982, Accuracy:0.2427, Validation Loss:1.5976, Validation Accuracy:0.2562\n",
    "Epoch #182: Loss:1.5969, Accuracy:0.2419, Validation Loss:1.5975, Validation Accuracy:0.2562\n",
    "Epoch #183: Loss:1.5975, Accuracy:0.2444, Validation Loss:1.5974, Validation Accuracy:0.2578\n",
    "Epoch #184: Loss:1.5970, Accuracy:0.2423, Validation Loss:1.5976, Validation Accuracy:0.2562\n",
    "Epoch #185: Loss:1.5976, Accuracy:0.2407, Validation Loss:1.5981, Validation Accuracy:0.2496\n",
    "Epoch #186: Loss:1.5971, Accuracy:0.2390, Validation Loss:1.5977, Validation Accuracy:0.2578\n",
    "Epoch #187: Loss:1.5974, Accuracy:0.2435, Validation Loss:1.5975, Validation Accuracy:0.2578\n",
    "Epoch #188: Loss:1.5991, Accuracy:0.2394, Validation Loss:1.5983, Validation Accuracy:0.2430\n",
    "Epoch #189: Loss:1.5978, Accuracy:0.2419, Validation Loss:1.5982, Validation Accuracy:0.2414\n",
    "Epoch #190: Loss:1.5977, Accuracy:0.2411, Validation Loss:1.5980, Validation Accuracy:0.2496\n",
    "Epoch #191: Loss:1.5972, Accuracy:0.2411, Validation Loss:1.5984, Validation Accuracy:0.2578\n",
    "Epoch #192: Loss:1.5972, Accuracy:0.2431, Validation Loss:1.5979, Validation Accuracy:0.2578\n",
    "Epoch #193: Loss:1.5971, Accuracy:0.2415, Validation Loss:1.5975, Validation Accuracy:0.2578\n",
    "Epoch #194: Loss:1.5969, Accuracy:0.2423, Validation Loss:1.5979, Validation Accuracy:0.2562\n",
    "Epoch #195: Loss:1.5968, Accuracy:0.2415, Validation Loss:1.5978, Validation Accuracy:0.2578\n",
    "Epoch #196: Loss:1.5967, Accuracy:0.2419, Validation Loss:1.5977, Validation Accuracy:0.2578\n",
    "Epoch #197: Loss:1.5970, Accuracy:0.2444, Validation Loss:1.5974, Validation Accuracy:0.2578\n",
    "Epoch #198: Loss:1.5972, Accuracy:0.2415, Validation Loss:1.5977, Validation Accuracy:0.2578\n",
    "Epoch #199: Loss:1.5969, Accuracy:0.2444, Validation Loss:1.5974, Validation Accuracy:0.2578\n",
    "Epoch #200: Loss:1.5971, Accuracy:0.2452, Validation Loss:1.5979, Validation Accuracy:0.2578\n",
    "Epoch #201: Loss:1.5971, Accuracy:0.2452, Validation Loss:1.5982, Validation Accuracy:0.2578\n",
    "Epoch #202: Loss:1.5969, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2578\n",
    "Epoch #203: Loss:1.5972, Accuracy:0.2456, Validation Loss:1.5983, Validation Accuracy:0.2578\n",
    "Epoch #204: Loss:1.5964, Accuracy:0.2460, Validation Loss:1.5981, Validation Accuracy:0.2512\n",
    "Epoch #205: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.5981, Validation Accuracy:0.2512\n",
    "Epoch #206: Loss:1.5979, Accuracy:0.2419, Validation Loss:1.5986, Validation Accuracy:0.2512\n",
    "Epoch #207: Loss:1.5966, Accuracy:0.2444, Validation Loss:1.5987, Validation Accuracy:0.2693\n",
    "Epoch #208: Loss:1.5984, Accuracy:0.2435, Validation Loss:1.5981, Validation Accuracy:0.2578\n",
    "Epoch #209: Loss:1.5971, Accuracy:0.2493, Validation Loss:1.5987, Validation Accuracy:0.2365\n",
    "Epoch #210: Loss:1.5973, Accuracy:0.2423, Validation Loss:1.5980, Validation Accuracy:0.2512\n",
    "Epoch #211: Loss:1.5969, Accuracy:0.2415, Validation Loss:1.5979, Validation Accuracy:0.2496\n",
    "Epoch #212: Loss:1.5965, Accuracy:0.2439, Validation Loss:1.5979, Validation Accuracy:0.2512\n",
    "Epoch #213: Loss:1.5967, Accuracy:0.2435, Validation Loss:1.5978, Validation Accuracy:0.2578\n",
    "Epoch #214: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.5977, Validation Accuracy:0.2578\n",
    "Epoch #215: Loss:1.5966, Accuracy:0.2472, Validation Loss:1.5973, Validation Accuracy:0.2578\n",
    "Epoch #216: Loss:1.5963, Accuracy:0.2464, Validation Loss:1.5978, Validation Accuracy:0.2529\n",
    "Epoch #217: Loss:1.5963, Accuracy:0.2439, Validation Loss:1.5975, Validation Accuracy:0.2512\n",
    "Epoch #218: Loss:1.5967, Accuracy:0.2419, Validation Loss:1.5977, Validation Accuracy:0.2512\n",
    "Epoch #219: Loss:1.5975, Accuracy:0.2423, Validation Loss:1.5992, Validation Accuracy:0.2512\n",
    "Epoch #220: Loss:1.5958, Accuracy:0.2468, Validation Loss:1.5992, Validation Accuracy:0.2545\n",
    "Epoch #221: Loss:1.5976, Accuracy:0.2402, Validation Loss:1.5981, Validation Accuracy:0.2578\n",
    "Epoch #222: Loss:1.5970, Accuracy:0.2468, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #223: Loss:1.5968, Accuracy:0.2431, Validation Loss:1.5988, Validation Accuracy:0.2627\n",
    "Epoch #224: Loss:1.5973, Accuracy:0.2386, Validation Loss:1.5983, Validation Accuracy:0.2529\n",
    "Epoch #225: Loss:1.5970, Accuracy:0.2468, Validation Loss:1.5993, Validation Accuracy:0.2496\n",
    "Epoch #226: Loss:1.5963, Accuracy:0.2460, Validation Loss:1.5982, Validation Accuracy:0.2496\n",
    "Epoch #227: Loss:1.5964, Accuracy:0.2476, Validation Loss:1.5980, Validation Accuracy:0.2594\n",
    "Epoch #228: Loss:1.5958, Accuracy:0.2444, Validation Loss:1.5981, Validation Accuracy:0.2578\n",
    "Epoch #229: Loss:1.5963, Accuracy:0.2423, Validation Loss:1.5979, Validation Accuracy:0.2512\n",
    "Epoch #230: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.5981, Validation Accuracy:0.2693\n",
    "Epoch #231: Loss:1.5968, Accuracy:0.2419, Validation Loss:1.5981, Validation Accuracy:0.2512\n",
    "Epoch #232: Loss:1.5962, Accuracy:0.2464, Validation Loss:1.5980, Validation Accuracy:0.2578\n",
    "Epoch #233: Loss:1.5956, Accuracy:0.2431, Validation Loss:1.5983, Validation Accuracy:0.2529\n",
    "Epoch #234: Loss:1.5966, Accuracy:0.2431, Validation Loss:1.5985, Validation Accuracy:0.2496\n",
    "Epoch #235: Loss:1.5983, Accuracy:0.2407, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #236: Loss:1.5961, Accuracy:0.2468, Validation Loss:1.5987, Validation Accuracy:0.2512\n",
    "Epoch #237: Loss:1.5965, Accuracy:0.2448, Validation Loss:1.5982, Validation Accuracy:0.2529\n",
    "Epoch #238: Loss:1.5958, Accuracy:0.2456, Validation Loss:1.5982, Validation Accuracy:0.2578\n",
    "Epoch #239: Loss:1.5965, Accuracy:0.2460, Validation Loss:1.5982, Validation Accuracy:0.2512\n",
    "Epoch #240: Loss:1.5954, Accuracy:0.2501, Validation Loss:1.5981, Validation Accuracy:0.2594\n",
    "Epoch #241: Loss:1.5956, Accuracy:0.2489, Validation Loss:1.5982, Validation Accuracy:0.2512\n",
    "Epoch #242: Loss:1.5963, Accuracy:0.2407, Validation Loss:1.5983, Validation Accuracy:0.2512\n",
    "Epoch #243: Loss:1.5953, Accuracy:0.2448, Validation Loss:1.5984, Validation Accuracy:0.2529\n",
    "Epoch #244: Loss:1.5967, Accuracy:0.2448, Validation Loss:1.5982, Validation Accuracy:0.2529\n",
    "Epoch #245: Loss:1.5977, Accuracy:0.2402, Validation Loss:1.5991, Validation Accuracy:0.2512\n",
    "Epoch #246: Loss:1.5965, Accuracy:0.2382, Validation Loss:1.5990, Validation Accuracy:0.2709\n",
    "Epoch #247: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.5988, Validation Accuracy:0.2512\n",
    "Epoch #248: Loss:1.5973, Accuracy:0.2415, Validation Loss:1.5991, Validation Accuracy:0.2529\n",
    "Epoch #249: Loss:1.5975, Accuracy:0.2423, Validation Loss:1.5995, Validation Accuracy:0.2578\n",
    "Epoch #250: Loss:1.5955, Accuracy:0.2419, Validation Loss:1.5989, Validation Accuracy:0.2512\n",
    "Epoch #251: Loss:1.5971, Accuracy:0.2419, Validation Loss:1.5990, Validation Accuracy:0.2430\n",
    "Epoch #252: Loss:1.5952, Accuracy:0.2415, Validation Loss:1.5983, Validation Accuracy:0.2578\n",
    "Epoch #253: Loss:1.5969, Accuracy:0.2407, Validation Loss:1.5983, Validation Accuracy:0.2578\n",
    "Epoch #254: Loss:1.5972, Accuracy:0.2444, Validation Loss:1.6005, Validation Accuracy:0.2414\n",
    "Epoch #255: Loss:1.5969, Accuracy:0.2448, Validation Loss:1.5981, Validation Accuracy:0.2447\n",
    "Epoch #256: Loss:1.5956, Accuracy:0.2522, Validation Loss:1.5985, Validation Accuracy:0.2644\n",
    "Epoch #257: Loss:1.5964, Accuracy:0.2480, Validation Loss:1.5979, Validation Accuracy:0.2529\n",
    "Epoch #258: Loss:1.5952, Accuracy:0.2452, Validation Loss:1.5978, Validation Accuracy:0.2512\n",
    "Epoch #259: Loss:1.5956, Accuracy:0.2415, Validation Loss:1.5978, Validation Accuracy:0.2512\n",
    "Epoch #260: Loss:1.5958, Accuracy:0.2427, Validation Loss:1.5975, Validation Accuracy:0.2496\n",
    "Epoch #261: Loss:1.5956, Accuracy:0.2505, Validation Loss:1.5977, Validation Accuracy:0.2529\n",
    "Epoch #262: Loss:1.5955, Accuracy:0.2493, Validation Loss:1.5975, Validation Accuracy:0.2496\n",
    "Epoch #263: Loss:1.5953, Accuracy:0.2464, Validation Loss:1.5976, Validation Accuracy:0.2529\n",
    "Epoch #264: Loss:1.5956, Accuracy:0.2464, Validation Loss:1.5986, Validation Accuracy:0.2512\n",
    "Epoch #265: Loss:1.5969, Accuracy:0.2407, Validation Loss:1.5984, Validation Accuracy:0.2430\n",
    "Epoch #266: Loss:1.5951, Accuracy:0.2485, Validation Loss:1.5985, Validation Accuracy:0.2512\n",
    "Epoch #267: Loss:1.5955, Accuracy:0.2452, Validation Loss:1.5982, Validation Accuracy:0.2529\n",
    "Epoch #268: Loss:1.5954, Accuracy:0.2423, Validation Loss:1.5984, Validation Accuracy:0.2512\n",
    "Epoch #269: Loss:1.5950, Accuracy:0.2489, Validation Loss:1.5980, Validation Accuracy:0.2594\n",
    "Epoch #270: Loss:1.5954, Accuracy:0.2468, Validation Loss:1.5982, Validation Accuracy:0.2578\n",
    "Epoch #271: Loss:1.5956, Accuracy:0.2485, Validation Loss:1.5979, Validation Accuracy:0.2644\n",
    "Epoch #272: Loss:1.5953, Accuracy:0.2431, Validation Loss:1.5988, Validation Accuracy:0.2512\n",
    "Epoch #273: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.5993, Validation Accuracy:0.2496\n",
    "Epoch #274: Loss:1.5976, Accuracy:0.2370, Validation Loss:1.5998, Validation Accuracy:0.2512\n",
    "Epoch #275: Loss:1.5960, Accuracy:0.2489, Validation Loss:1.5993, Validation Accuracy:0.2496\n",
    "Epoch #276: Loss:1.5976, Accuracy:0.2444, Validation Loss:1.5992, Validation Accuracy:0.2430\n",
    "Epoch #277: Loss:1.5951, Accuracy:0.2444, Validation Loss:1.5989, Validation Accuracy:0.2397\n",
    "Epoch #278: Loss:1.5956, Accuracy:0.2505, Validation Loss:1.5982, Validation Accuracy:0.2496\n",
    "Epoch #279: Loss:1.5952, Accuracy:0.2435, Validation Loss:1.5989, Validation Accuracy:0.2430\n",
    "Epoch #280: Loss:1.5953, Accuracy:0.2468, Validation Loss:1.5978, Validation Accuracy:0.2578\n",
    "Epoch #281: Loss:1.5952, Accuracy:0.2513, Validation Loss:1.5980, Validation Accuracy:0.2529\n",
    "Epoch #282: Loss:1.5945, Accuracy:0.2460, Validation Loss:1.5983, Validation Accuracy:0.2496\n",
    "Epoch #283: Loss:1.5950, Accuracy:0.2480, Validation Loss:1.5981, Validation Accuracy:0.2578\n",
    "Epoch #284: Loss:1.5946, Accuracy:0.2476, Validation Loss:1.5984, Validation Accuracy:0.2512\n",
    "Epoch #285: Loss:1.5948, Accuracy:0.2402, Validation Loss:1.5986, Validation Accuracy:0.2529\n",
    "Epoch #286: Loss:1.5945, Accuracy:0.2517, Validation Loss:1.5984, Validation Accuracy:0.2644\n",
    "Epoch #287: Loss:1.5958, Accuracy:0.2505, Validation Loss:1.5987, Validation Accuracy:0.2430\n",
    "Epoch #288: Loss:1.5946, Accuracy:0.2427, Validation Loss:1.5988, Validation Accuracy:0.2479\n",
    "Epoch #289: Loss:1.5946, Accuracy:0.2444, Validation Loss:1.5987, Validation Accuracy:0.2512\n",
    "Epoch #290: Loss:1.5943, Accuracy:0.2468, Validation Loss:1.5984, Validation Accuracy:0.2512\n",
    "Epoch #291: Loss:1.5947, Accuracy:0.2489, Validation Loss:1.5985, Validation Accuracy:0.2578\n",
    "Epoch #292: Loss:1.5946, Accuracy:0.2489, Validation Loss:1.5992, Validation Accuracy:0.2463\n",
    "Epoch #293: Loss:1.5942, Accuracy:0.2497, Validation Loss:1.5987, Validation Accuracy:0.2512\n",
    "Epoch #294: Loss:1.5945, Accuracy:0.2489, Validation Loss:1.5991, Validation Accuracy:0.2430\n",
    "Epoch #295: Loss:1.5942, Accuracy:0.2493, Validation Loss:1.5996, Validation Accuracy:0.2463\n",
    "Epoch #296: Loss:1.5945, Accuracy:0.2505, Validation Loss:1.5987, Validation Accuracy:0.2578\n",
    "Epoch #297: Loss:1.5953, Accuracy:0.2509, Validation Loss:1.5991, Validation Accuracy:0.2496\n",
    "Epoch #298: Loss:1.5943, Accuracy:0.2517, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #299: Loss:1.5943, Accuracy:0.2456, Validation Loss:1.5988, Validation Accuracy:0.2512\n",
    "Epoch #300: Loss:1.5938, Accuracy:0.2468, Validation Loss:1.5990, Validation Accuracy:0.2430\n",
    "\n",
    "Test:\n",
    "Test Loss:1.59900260, Accuracy:0.2430\n",
    "Labels: ['04', '01', '02', '03', '05']\n",
    "Confusion Matrix:\n",
    "      04  01  02  03   05\n",
    "t:04  22   0  16   4   70\n",
    "t:01  20   0   7   1   98\n",
    "t:02   9   0  14   4   87\n",
    "t:03  20   1   8   7   79\n",
    "t:05  26   0   5   6  105\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.23      0.20      0.21       112\n",
    "          01       0.00      0.00      0.00       126\n",
    "          02       0.28      0.12      0.17       114\n",
    "          03       0.32      0.06      0.10       115\n",
    "          05       0.24      0.74      0.36       142\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.21      0.22      0.17       609\n",
    "weighted avg       0.21      0.24      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 07:05:08 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 26 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6042550101460298, 1.6022142866757898, 1.6030208132732873, 1.6024790342609674, 1.601676452336053, 1.6002821370298639, 1.6026865964256876, 1.602966177052465, 1.6046287165878246, 1.6019000942483912, 1.6042343932028083, 1.6073253109733068, 1.6049725690302983, 1.604821006457011, 1.6041632837849884, 1.6038717103904887, 1.6035312656894303, 1.6031843179160934, 1.6054289775528932, 1.6021069607319698, 1.6017977780309216, 1.6017159900837539, 1.6017283627943844, 1.600883575300082, 1.6006560918732817, 1.6004481112036995, 1.6001653383518089, 1.6045918662364065, 1.603464965554098, 1.6034068918384745, 1.6027131297905457, 1.6026088866498474, 1.60242162451564, 1.6021805978173693, 1.6019707556037088, 1.6021810454883794, 1.6015343315691393, 1.6005989726149585, 1.600995275383121, 1.6011837325464133, 1.5994581667269, 1.5992546128521998, 1.5992208235761018, 1.598872925064638, 1.5991729756294213, 1.5988801359543072, 1.5995540462299715, 1.5991159659888357, 1.5992319119974898, 1.5984328724872108, 1.5989705672600782, 1.5991782241658428, 1.5997094775264096, 1.5994865845381137, 1.5994165973318817, 1.5999956506813688, 1.6000767345303188, 1.5996919349692333, 1.599593684003858, 1.6000177036169518, 1.6004629371984447, 1.6017314297420833, 1.6013088950578411, 1.601127311709675, 1.6012361086843832, 1.6009555550044394, 1.6008541472439695, 1.6015072301495055, 1.600658347845469, 1.6006841957079758, 1.6004367616572013, 1.6004887461075055, 1.6002195758381108, 1.5992387131908647, 1.6004470533925323, 1.5987247138579295, 1.6002794670549716, 1.5992408155024738, 1.5991800177860729, 1.5990827617974117, 1.59849046916993, 1.5994256640889961, 1.5997625096090908, 1.598825557478543, 1.6006537822667013, 1.6004048037803036, 1.5997482418818232, 1.602237704352205, 1.6013932932773833, 1.6014301401053743, 1.6008529854916977, 1.6009242119656015, 1.5999652196229581, 1.600079649774899, 1.600414461102979, 1.6011882221757485, 1.6000190998728836, 1.5996578481592765, 1.5994047515693752, 1.5998246771557185, 1.5991815561535714, 1.5990265384683469, 1.6001389952520235, 1.5998773594403697, 1.5986475729198486, 1.5984236184012126, 1.598402426747853, 1.598450687131271, 1.5987011983085344, 1.5982063869733136, 1.5979800392645724, 1.5983455778147004, 1.598191480526979, 1.5985163994414857, 1.604200073259413, 1.6074595521823527, 1.6056393568934675, 1.602960769374578, 1.604201297846138, 1.600432276921515, 1.5973318609698066, 1.5991510904481259, 1.597777701168029, 1.597189817522547, 1.5962965370986262, 1.5948652265890086, 1.595617457367908, 1.5955440692713696, 1.5949605507607922, 1.597517646200747, 1.595472065881751, 1.5950553926145306, 1.595216925312537, 1.5949555527791992, 1.594760815694023, 1.5946179651861707, 1.5945172431237984, 1.5944129496763884, 1.5955236349591284, 1.5953802179624685, 1.5967333657400948, 1.5966450802015357, 1.5966438119634618, 1.5969765622823304, 1.5965814627645833, 1.5968149330815657, 1.5964686870574951, 1.5966966667003037, 1.5966327372442912, 1.5966801580732874, 1.5967725341151697, 1.596583474054321, 1.596978218293151, 1.596667313614894, 1.5969754995775145, 1.5969765476013835, 1.5971238528957898, 1.5971114005165539, 1.5969846452202507, 1.5975654573471871, 1.5990857206933409, 1.5976782238542153, 1.5983382819712848, 1.5975773814080776, 1.5979130686993277, 1.5977618065960888, 1.5973081714022532, 1.5978833977224791, 1.5969456925572236, 1.5971447661983946, 1.596872666003473, 1.596928880132478, 1.5973103525994838, 1.5976965441100899, 1.5975372438947555, 1.5969749354376581, 1.5974077180101367, 1.5977998245721576, 1.5975094800707939, 1.5975192170620747, 1.5975768319491683, 1.597502130592985, 1.5974072151387657, 1.5975906876311905, 1.5981150366402612, 1.5977499163992495, 1.5975427431817517, 1.5982737363069908, 1.5981697591850519, 1.5980236861114627, 1.598414855246082, 1.597871905672922, 1.5975292907364067, 1.5979222654317595, 1.597785116807972, 1.5977457438783693, 1.5974415223586735, 1.5977111651588152, 1.597383951905913, 1.5978975108104387, 1.5982229564773234, 1.598344648412883, 1.5982688622325902, 1.5980645964298341, 1.5980813573733927, 1.5986056014626289, 1.598720737865993, 1.598054187834165, 1.5986708946807435, 1.598037493052741, 1.597900196053516, 1.5978900896896087, 1.597801914943263, 1.5976610602612173, 1.597348817267833, 1.5977779133566494, 1.5974569606467812, 1.5977277904504235, 1.5992049585618018, 1.5991895220354078, 1.5981279529374222, 1.6008027267377756, 1.5987562414852075, 1.5982962862415657, 1.5993021245073216, 1.598190900727446, 1.5979653385472414, 1.598098327569382, 1.5979437344571443, 1.5981196294277173, 1.5981285278433062, 1.5980370759181006, 1.5982633901542826, 1.5984759135003552, 1.599432291264213, 1.598733843449497, 1.598194717969409, 1.5981775226655657, 1.598196838485392, 1.5980682805645445, 1.5981929049703287, 1.5983426412338106, 1.598410593269298, 1.598179856153153, 1.5990559025155304, 1.5989709562073005, 1.5988487893920422, 1.5991438995245446, 1.5994916821544003, 1.5988901730241447, 1.5989623606107113, 1.5983299903681714, 1.5983055912215134, 1.6005424852245937, 1.5981112886923678, 1.5984647393422369, 1.5979119002916935, 1.5977904211319922, 1.5977902246030484, 1.5974698867312402, 1.5976537466049194, 1.5975376373441348, 1.597557057105066, 1.598649468327978, 1.5983547030998568, 1.5984801253661733, 1.5982384897022217, 1.5984087121506239, 1.5979971779978335, 1.5981915010803047, 1.597916193196339, 1.5987592338536956, 1.5993004978583951, 1.5998273590711145, 1.5992585559588153, 1.5992381936810873, 1.598869604821667, 1.5982458049244872, 1.5988739233690334, 1.5978171764727689, 1.5980013466037943, 1.5983019548488173, 1.5981215645722764, 1.598405186178649, 1.5986190662399693, 1.5984140771558915, 1.5987344853004994, 1.598834040716951, 1.598693589663075, 1.5984186672029042, 1.598488269768325, 1.5992256563481047, 1.5987038260022994, 1.5990577345019688, 1.5995539945530381, 1.5986800747747687, 1.5991265341174623, 1.5996287166582932, 1.5988206256590844, 1.5990025476477612], 'val_acc': [0.21346469541586482, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.25287356091837576, 0.23481116552756143, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.25287356319392257, 0.2495894908464601, 0.2331691293048937, 0.2331691293048937, 0.25287356319392257, 0.2545155970431705, 0.2413793101246134, 0.2495894908464601, 0.2331691293048937, 0.2331691293048937, 0.2380952378750239, 0.24302134624940813, 0.2331691293048937, 0.24466338237420288, 0.25615763544351206, 0.25779966929276, 0.24302134624940813, 0.2545155992208443, 0.2528735630960496, 0.25123152697125484, 0.25123152697125484, 0.25287356319392257, 0.2577996715683068, 0.24794745472166535, 0.24630541849899762, 0.24794745254399153, 0.25615763544351206, 0.2446633822763299, 0.2577996715683068, 0.2528735630960496, 0.2545155993187173, 0.2577996715683068, 0.2545155993187173, 0.24630541849899762, 0.2577996715683068, 0.25944170769310154, 0.2545155992208443, 0.2545155970431705, 0.2528735630960496, 0.2545155970431705, 0.25615763316796525, 0.25287356091837576, 0.23973727380407267, 0.24630541651706978, 0.24466338237420288, 0.2479474548195383, 0.251231524793581, 0.251231524793581, 0.2479474548195383, 0.251231524793581, 0.24466338237420288, 0.2479474527397375, 0.24630541849899762, 0.2397372719200178, 0.24466338256994882, 0.2413793101246134, 0.2545155970431705, 0.23645320165235617, 0.24958948866878627, 0.24958948866878627, 0.251231524793581, 0.251231524793581, 0.2495894908464601, 0.24466338247207586, 0.25123152697125484, 0.24302134615153514, 0.24466338247207586, 0.24794745254399153, 0.2446633822763299, 0.24794745472166535, 0.24794745472166535, 0.24794745472166535, 0.2413793099288674, 0.24466338029440204, 0.24302134615153514, 0.24137930814268554, 0.23973727390194566, 0.2479474548195383, 0.24958949094433308, 0.2479474548195383, 0.24630541849899762, 0.24958949094433308, 0.24137930814268554, 0.24794745472166535, 0.24630541661494276, 0.24958949094433308, 0.2512315270691278, 0.24958948876665926, 0.24958949094433308, 0.24958949094433308, 0.24958948876665926, 0.24958948876665926, 0.2512315270691278, 0.24958949094433308, 0.2512315270691278, 0.22824302093050947, 0.23481116552756143, 0.23645319976830131, 0.2331691293048937, 0.23152709318009895, 0.24466338039227503, 0.2512315270691278, 0.23973727399981865, 0.24630541869474357, 0.24630541849899762, 0.24630541849899762, 0.24302134624940813, 0.23152709327797194, 0.2413793101246134, 0.2413793101246134, 0.2512315270691278, 0.24302134634728112, 0.24302134624940813, 0.24630541849899762, 0.24958949094433308, 0.24630541849899762, 0.24466338237420288, 0.2479474548195383, 0.24630541849899762, 0.25287356319392257, 0.2495894908464601, 0.2512315270691278, 0.2512315270691278, 0.24466338247207586, 0.25287356091837576, 0.25287356091837576, 0.2512315270691278, 0.24466338247207586, 0.2512315270691278, 0.25287356091837576, 0.2512315270691278, 0.2512315270691278, 0.2512315270691278, 0.25287356091837576, 0.2545155970431705, 0.2545155970431705, 0.2545155970431705, 0.2512315270691278, 0.2545155970431705, 0.25615763326583824, 0.25615763326583824, 0.2430213441696073, 0.24958948866878627, 0.2512315270691278, 0.25287356091837576, 0.24137930814268554, 0.25287356091837576, 0.25287356091837576, 0.24137930814268554, 0.2512315270691278, 0.2545155971410435, 0.25615763326583824, 0.25615763326583824, 0.2512315270691278, 0.25615763326583824, 0.25287356091837576, 0.25615763326583824, 0.2512315270691278, 0.24958948866878627, 0.25615763326583824, 0.2545155970431705, 0.25615763326583824, 0.25615763326583824, 0.25779966948850597, 0.25615763316796525, 0.24958949094433308, 0.25779966948850597, 0.25779966948850597, 0.24302134644515408, 0.24137930814268554, 0.24958949094433308, 0.25779966948850597, 0.25779966948850597, 0.25779966948850597, 0.25615763326583824, 0.25779966948850597, 0.25779966948850597, 0.25779966948850597, 0.257799669390633, 0.25779966948850597, 0.25779966948850597, 0.25779966948850597, 0.25779966948850597, 0.25779966948850597, 0.251231524793581, 0.251231524793581, 0.251231524793581, 0.26929392255781515, 0.25779966948850597, 0.23645320184810212, 0.251231524793581, 0.24958949094433308, 0.251231524793581, 0.25779966948850597, 0.257799669390633, 0.25779966948850597, 0.25287356101624875, 0.251231524793581, 0.251231524793581, 0.251231524793581, 0.2545155992208443, 0.25779966948850597, 0.24137930804481256, 0.2627257778628902, 0.25287356101624875, 0.24958948896240524, 0.24958949094433308, 0.2594417057111737, 0.25779966948850597, 0.251231524793581, 0.26929392255781515, 0.251231524793581, 0.25779966948850597, 0.25287356101624875, 0.24958949094433308, 0.24302134436535328, 0.251231524793581, 0.25287356101624875, 0.25779966929276, 0.251231524793581, 0.2594417056133007, 0.251231524793581, 0.251231524793581, 0.25287356091837576, 0.25287356091837576, 0.251231524793581, 0.2709359586826099, 0.251231524793581, 0.25287356101624875, 0.2577996715683068, 0.251231524793581, 0.2430213442674803, 0.2577996715683068, 0.2577996715683068, 0.24137930804481256, 0.24466338256994882, 0.26436781388981195, 0.25287356101624875, 0.251231524793581, 0.251231524793581, 0.24958948896240524, 0.25287356101624875, 0.24958948896240524, 0.25287356101624875, 0.251231524793581, 0.24302134436535328, 0.2512315270691278, 0.25287356101624875, 0.251231524793581, 0.2594417056133007, 0.25779966948850597, 0.26436781388981195, 0.251231524793581, 0.24958948896240524, 0.25123152697125484, 0.24958949094433308, 0.2430213442674803, 0.2397372740976916, 0.24958948866878627, 0.2430213442674803, 0.25779966929276, 0.25287356101624875, 0.24958948866878627, 0.25779966929276, 0.251231524793581, 0.25287356101624875, 0.26436781388981195, 0.2430213442674803, 0.2479474548195383, 0.251231524793581, 0.251231524793581, 0.2577996715683068, 0.2463054185968706, 0.251231524793581, 0.2430213442674803, 0.2463054185968706, 0.2577996715683068, 0.24958948876665926, 0.2479474548195383, 0.251231524793581, 0.2430213442674803], 'loss': [1.6083040392129573, 1.6054131612640632, 1.604514675267668, 1.604176801283991, 1.6038689963381882, 1.6035154535540321, 1.6042118682998407, 1.6055586197048242, 1.604799384894557, 1.6046994015421465, 1.6036455714971867, 1.6056346185887864, 1.6059358589947836, 1.60513317864044, 1.604886600466969, 1.604091761097526, 1.60396577181023, 1.6043994528067431, 1.6042784724636978, 1.6042852508458758, 1.603980243769025, 1.6036118033485491, 1.603914953257269, 1.6038275983544101, 1.603264987346328, 1.6031394934507366, 1.6030083483494282, 1.6030027286717534, 1.6044305590878276, 1.603984463435179, 1.6037858076898468, 1.603533313455523, 1.6031014878647039, 1.6027334648480895, 1.6022608124499937, 1.6022310482158308, 1.602373857713578, 1.6018949471459987, 1.6017125888526806, 1.6018420340344157, 1.6013447710620794, 1.6017907577373653, 1.601163360029765, 1.6018426169115416, 1.6007263888568604, 1.6016381424800081, 1.6012131285618463, 1.6015689641298454, 1.60083201391741, 1.6008377999006111, 1.6005761712973123, 1.6008545008038593, 1.6010557389112468, 1.6005262023369635, 1.6005421154308124, 1.6002855518270567, 1.6000245674434874, 1.5999096069492598, 1.5998245570204341, 1.6001531081033193, 1.6003121986036672, 1.5997953196081047, 1.6002420848399952, 1.600089894036248, 1.5999147707921524, 1.5996734433105595, 1.5997469673411313, 1.5994849715634294, 1.600345122691787, 1.6008850158362418, 1.5994504621631067, 1.599844162077385, 1.5996634699235952, 1.600672991711501, 1.6012982094312351, 1.6029172773478702, 1.6000849282227503, 1.5995263405159514, 1.599943131147224, 1.5994267443122316, 1.5996587108537645, 1.598876222643764, 1.5992916392105072, 1.5991470536167371, 1.598980197622546, 1.5991754117687624, 1.5988262428640094, 1.5989102681804241, 1.5991445601598437, 1.5986185780785656, 1.5992232935629342, 1.599175029319904, 1.5997965939970231, 1.600751814900972, 1.599563431397111, 1.5991518922899783, 1.5994306744980862, 1.5984633614150405, 1.598895928110675, 1.5985107863463415, 1.5987735316983482, 1.5986027495327426, 1.6004887100607463, 1.598441760349078, 1.5985999992495934, 1.598315856980592, 1.5981924550978799, 1.5983672875888049, 1.5980786591829461, 1.5981908789895154, 1.5981046288410006, 1.598397616879896, 1.5985433064936612, 1.5990653159437238, 1.5991213965464912, 1.6042838707596858, 1.6022080019024607, 1.6005826365042026, 1.6021854524494932, 1.5996650875471456, 1.599707934596945, 1.5993361737938632, 1.6010877034013031, 1.599801938832418, 1.5999593092920354, 1.600544103655727, 1.6009957044765935, 1.6006803910590295, 1.60112141461343, 1.6017136741223033, 1.6004764090328492, 1.6006385718038194, 1.6006746913862915, 1.600088506949266, 1.6004252482243877, 1.5995939259166836, 1.5999738262908905, 1.5991809461640627, 1.5995460361425882, 1.599131668274897, 1.5988897847933445, 1.599253182636394, 1.6002196309502854, 1.598743347563538, 1.5991839358938793, 1.5994486840109072, 1.5994519787892179, 1.5982012810893127, 1.5988175798490551, 1.5983719347438774, 1.5982267696999426, 1.5985426812690875, 1.5984260914017288, 1.598056165148835, 1.598236946893179, 1.5983437975329295, 1.5978966355568573, 1.598108935845706, 1.5983073529766325, 1.6010869033038004, 1.5979257051460063, 1.5996010996722587, 1.5985056125163053, 1.599146461780556, 1.598101795625393, 1.5984464650770966, 1.5988540424703328, 1.597979624168584, 1.5982794878663957, 1.59837994467796, 1.5982506409807617, 1.597953779986262, 1.597957816016258, 1.598637674525533, 1.5976420606186257, 1.5981967011026779, 1.5987241093872508, 1.5977096738266994, 1.5976160172319511, 1.5973238497544118, 1.5982060950394774, 1.5969271782732108, 1.5975252890733722, 1.5970351897715542, 1.5975882636937762, 1.5971237810240635, 1.5974318308507147, 1.599082576469719, 1.5977531274975691, 1.5976679003948548, 1.5972122089084413, 1.597196537855959, 1.5970936236195497, 1.5969032669948602, 1.596802628652271, 1.5966689867650214, 1.5970117552324488, 1.597155691759787, 1.596932874518988, 1.5970925168579853, 1.59711084189601, 1.5968618375809531, 1.5971846891869266, 1.5963625885867485, 1.5971502038732446, 1.5978552150530492, 1.5965926070477683, 1.598403603242408, 1.5971350410390928, 1.5973002578688353, 1.5968647253831554, 1.59647078171403, 1.5966746994602117, 1.5960317132409347, 1.596627323926107, 1.5962790290433033, 1.596337405807918, 1.5966538777831154, 1.597477740526689, 1.5957646600519608, 1.5976070273338647, 1.5969672117389937, 1.5968069408953312, 1.59728617780752, 1.5969799181763886, 1.5963367103795985, 1.5963756453574804, 1.595846401298805, 1.5963299368930792, 1.596013896959763, 1.596811585259878, 1.5961919880011244, 1.5956231579398716, 1.596592982446878, 1.5982777254782174, 1.5960925016070293, 1.5965353775318154, 1.59576263662726, 1.596452217572034, 1.595428201941739, 1.5956360923680926, 1.5962992204532975, 1.5952938962031684, 1.5966554537935669, 1.59770499406654, 1.5964865469100293, 1.595989710545393, 1.5973082655998716, 1.597502703784183, 1.595473313331604, 1.597064907105307, 1.5952198144101999, 1.5968785053405918, 1.5972309596729475, 1.5969498884016973, 1.5955732847875639, 1.5964294059075859, 1.5952206009956846, 1.5955500920939985, 1.595833138812494, 1.5955741084821415, 1.5954660294237077, 1.5953438409300067, 1.5955971473541104, 1.596887829122602, 1.5950770840262976, 1.5955451891407584, 1.5953648800722628, 1.5949516298344981, 1.5953823885633716, 1.5955534810647827, 1.595251432483446, 1.5959889222464279, 1.597618475635928, 1.595965847685107, 1.597581955099008, 1.595062686824211, 1.5956279331164194, 1.5951830265213576, 1.5952769035676178, 1.595184115364811, 1.5944964373136203, 1.5950291129842675, 1.5945729015788992, 1.5947505179371921, 1.594535352513041, 1.5958026945713364, 1.5945584658969354, 1.5946317577753713, 1.5942620734169743, 1.5946519659285183, 1.5945575496744082, 1.5941702061854839, 1.594495899182815, 1.5941625654330245, 1.5945072936815892, 1.5953155915595179, 1.5942541658021585, 1.5943107020928384, 1.5938242880470699], 'acc': [0.20574948574116098, 0.23162217626703838, 0.23285421010037957, 0.23285420990455322, 0.23285420912124782, 0.2328542087112364, 0.23285420929871545, 0.23285421049203225, 0.23326488730843797, 0.2443531837979871, 0.23285420931707418, 0.23285421049203225, 0.23285420833794243, 0.23285421011873828, 0.23285421011873828, 0.2328542083195837, 0.23285420931707418, 0.23285420990455322, 0.2328542094945418, 0.2312114996281003, 0.23655030884047554, 0.2353182752029607, 0.2328542083195837, 0.23367556531816047, 0.2357289536226946, 0.23531827422382895, 0.23408624234875125, 0.23408624236710998, 0.23285421029620593, 0.23367556355572333, 0.23737166211835167, 0.235318274811308, 0.23244353070151388, 0.23531827461548166, 0.2381930200960602, 0.2349075968015855, 0.23983572861007596, 0.2398357286284347, 0.2398357295892077, 0.23942505220368168, 0.23860369536421383, 0.2402464058364931, 0.23983572960756644, 0.2357289538368797, 0.23613962926414223, 0.24188911693296883, 0.24147844025731333, 0.23901437494054711, 0.2427104725240437, 0.2410677614642854, 0.2386036959700516, 0.24065708404204195, 0.24188911670042504, 0.23942505099200614, 0.24106776244341716, 0.2394250523811493, 0.24229979551181166, 0.2422997945326799, 0.24188911789374185, 0.24312114935880813, 0.24394250617991728, 0.2398357286284347, 0.2427104709207155, 0.24394250576990587, 0.24188911711043645, 0.24271047134908563, 0.2431211487713291, 0.24312114935880813, 0.24065708462952098, 0.24147843888652887, 0.24229979433685359, 0.24312114896715545, 0.23983572861007596, 0.23901437435306808, 0.23942505255861693, 0.2410677630308962, 0.23819301952693986, 0.2422997941226685, 0.24353182932679412, 0.2398357286284347, 0.24271047271987006, 0.2431211503562986, 0.23942505218532295, 0.24229979374937452, 0.24188911750208916, 0.24188911810792693, 0.24394250459494776, 0.24065708482534734, 0.24147843849487619, 0.2418891169146101, 0.2406570850211737, 0.241889116112946, 0.24106776087680637, 0.2357289518602575, 0.23613962945996858, 0.24024640564066674, 0.2361396310449381, 0.2394250504045271, 0.23901437356976268, 0.2394250504045271, 0.23901437157478175, 0.23942505020870075, 0.23819301737285004, 0.2381930181377967, 0.24312115033793988, 0.23901437298228365, 0.23860369790995636, 0.24188911711043645, 0.23901437415724172, 0.23942505181202897, 0.24147843927818158, 0.23901437396141537, 0.2394250515978439, 0.23942505198949662, 0.2427104727382288, 0.2344969205543001, 0.2340862421529249, 0.2480492823422567, 0.23490759738906453, 0.2369609866543717, 0.24558521526305338, 0.24065708464787972, 0.23696098606689264, 0.24353182952262048, 0.2357289536226946, 0.2365503077022349, 0.2381930173544913, 0.23819301774614401, 0.2373716621367104, 0.23367556453485508, 0.24147843947400793, 0.2373716637033212, 0.23490759779907594, 0.2386036957558665, 0.23737166251000438, 0.23778234073391197, 0.23983572999921912, 0.23778234071555324, 0.23613963006580635, 0.24024640564066674, 0.23860369614751922, 0.23819301952693986, 0.2422997945143212, 0.24024640818640927, 0.23983573056833946, 0.23613963125912316, 0.2398357284142496, 0.240657084256227, 0.2394250504045271, 0.23983572900172867, 0.2410677614642854, 0.24106776107263272, 0.2410677628717873, 0.24106776207012318, 0.24147843806650604, 0.24147843927818158, 0.2410677634225489, 0.24106776246177586, 0.24147843988401935, 0.23737166309748342, 0.23983572821842328, 0.24353182658522526, 0.24024640761728894, 0.23572895203772512, 0.2406570838462156, 0.24188911748373043, 0.23655030727386475, 0.2427104736990018, 0.24188911671878377, 0.2402464070298099, 0.24106776109099143, 0.24229979433685359, 0.24188911728790408, 0.24271047213239103, 0.24271047174073831, 0.2410677614642854, 0.23983572821842328, 0.24476386120187182, 0.24188911630877236, 0.24024640722563625, 0.2427104727382288, 0.24188911671878377, 0.24435318321050806, 0.24229979647258468, 0.2406570858044791, 0.23901437260898967, 0.24353182658522526, 0.23942505220368168, 0.2418891176795568, 0.24106776068098001, 0.2410677614642854, 0.24312115033793988, 0.24147843988401935, 0.24229979512015898, 0.24147844045313965, 0.24188911826703582, 0.24435318201719122, 0.24147843869070254, 0.244353180413863, 0.245174539015768, 0.24517453784080992, 0.241889116112946, 0.24558521545887974, 0.24599589464356034, 0.2427104719549234, 0.24188911748373043, 0.24435318319214933, 0.2435318267810516, 0.24928131462734582, 0.24229979374937452, 0.24147844007984567, 0.24394250479077412, 0.24353182854348873, 0.24394250598409092, 0.247227925931159, 0.24640657108667205, 0.24394250480913285, 0.24188911630877236, 0.24229979588510564, 0.24681724792143647, 0.24024640642397213, 0.24681724811726283, 0.24312115131707163, 0.23860369516838748, 0.24681724811726283, 0.24599589427026636, 0.24763860413670785, 0.24435318201719122, 0.24229979353518946, 0.2439425055740795, 0.2418891176979155, 0.24640656993007268, 0.2431211487713291, 0.24312115033793988, 0.24065708365038924, 0.24681724733395743, 0.24476386041856643, 0.24558521586889115, 0.2459958936827873, 0.2501026692576477, 0.24887063601178555, 0.2406570850395324, 0.2447638594394347, 0.2447638602227401, 0.2402464058548518, 0.23819301874363447, 0.24394250576990587, 0.24147843966983426, 0.24229979353518946, 0.24188911787538314, 0.24188911650459868, 0.24147844027567203, 0.2406570850211737, 0.24435318281885535, 0.24476386081021914, 0.25215605715217043, 0.2480492835172148, 0.245174539015768, 0.24147843888652887, 0.2427104715632707, 0.25051334589658575, 0.24928131323820266, 0.24640657169250982, 0.24640657149668346, 0.24065708560865273, 0.2484599582162481, 0.2451745394074207, 0.24229979492433262, 0.24887063718674365, 0.24681724772561012, 0.24845995996032652, 0.24312115053376623, 0.24394250694486394, 0.2369609866543717, 0.2488706369909173, 0.24435318280049662, 0.2443531818213649, 0.2505133450765629, 0.24353182756435698, 0.2468172487231006, 0.2513347011327254, 0.2459958936827873, 0.24804928136312496, 0.2476386031575761, 0.24024640701145117, 0.25174538053159107, 0.2505133472673702, 0.24271047311152277, 0.24435318201719122, 0.24681724909639458, 0.24887063836170173, 0.2488706369909173, 0.24969199222705693, 0.2488706369909173, 0.24928131362985537, 0.25051334589658575, 0.25092402664787716, 0.251745380317406, 0.2455852148897594, 0.24681724792143647]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
