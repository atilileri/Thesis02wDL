{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf10.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 15:08:04 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '3', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['sg', 'eo', 'eb', 'my', 'ck', 'ek', 'mb', 'ce', 'sk', 'eg', 'aa', 'yd', 'ds', 'ib', 'by'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F8A2A3D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F8A0196EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7189, Accuracy:0.0480, Validation Loss:2.7089, Validation Accuracy:0.0493\n",
    "Epoch #2: Loss:2.7055, Accuracy:0.0522, Validation Loss:2.6998, Validation Accuracy:0.0788\n",
    "Epoch #3: Loss:2.6978, Accuracy:0.0891, Validation Loss:2.6933, Validation Accuracy:0.0887\n",
    "Epoch #4: Loss:2.6907, Accuracy:0.0899, Validation Loss:2.6869, Validation Accuracy:0.0903\n",
    "Epoch #5: Loss:2.6853, Accuracy:0.0899, Validation Loss:2.6810, Validation Accuracy:0.0903\n",
    "Epoch #6: Loss:2.6787, Accuracy:0.0895, Validation Loss:2.6745, Validation Accuracy:0.0887\n",
    "Epoch #7: Loss:2.6715, Accuracy:0.0912, Validation Loss:2.6678, Validation Accuracy:0.0854\n",
    "Epoch #8: Loss:2.6638, Accuracy:0.0982, Validation Loss:2.6597, Validation Accuracy:0.0985\n",
    "Epoch #9: Loss:2.6545, Accuracy:0.1150, Validation Loss:2.6502, Validation Accuracy:0.1281\n",
    "Epoch #10: Loss:2.6435, Accuracy:0.1277, Validation Loss:2.6378, Validation Accuracy:0.1248\n",
    "Epoch #11: Loss:2.6289, Accuracy:0.1396, Validation Loss:2.6226, Validation Accuracy:0.1330\n",
    "Epoch #12: Loss:2.6118, Accuracy:0.1458, Validation Loss:2.6042, Validation Accuracy:0.1560\n",
    "Epoch #13: Loss:2.5916, Accuracy:0.1548, Validation Loss:2.5853, Validation Accuracy:0.1560\n",
    "Epoch #14: Loss:2.5703, Accuracy:0.1589, Validation Loss:2.5665, Validation Accuracy:0.1560\n",
    "Epoch #15: Loss:2.5525, Accuracy:0.1585, Validation Loss:2.5469, Validation Accuracy:0.1544\n",
    "Epoch #16: Loss:2.5351, Accuracy:0.1536, Validation Loss:2.5366, Validation Accuracy:0.1494\n",
    "Epoch #17: Loss:2.5244, Accuracy:0.1507, Validation Loss:2.5258, Validation Accuracy:0.1576\n",
    "Epoch #18: Loss:2.5184, Accuracy:0.1680, Validation Loss:2.5227, Validation Accuracy:0.1790\n",
    "Epoch #19: Loss:2.5086, Accuracy:0.1667, Validation Loss:2.5100, Validation Accuracy:0.1724\n",
    "Epoch #20: Loss:2.5017, Accuracy:0.1696, Validation Loss:2.5073, Validation Accuracy:0.1724\n",
    "Epoch #21: Loss:2.4953, Accuracy:0.1721, Validation Loss:2.4978, Validation Accuracy:0.1708\n",
    "Epoch #22: Loss:2.4903, Accuracy:0.1659, Validation Loss:2.5014, Validation Accuracy:0.1708\n",
    "Epoch #23: Loss:2.4912, Accuracy:0.1647, Validation Loss:2.5027, Validation Accuracy:0.1724\n",
    "Epoch #24: Loss:2.4952, Accuracy:0.1704, Validation Loss:2.5811, Validation Accuracy:0.1560\n",
    "Epoch #25: Loss:2.5171, Accuracy:0.1688, Validation Loss:2.5531, Validation Accuracy:0.1527\n",
    "Epoch #26: Loss:2.5157, Accuracy:0.1610, Validation Loss:2.5534, Validation Accuracy:0.1675\n",
    "Epoch #27: Loss:2.5010, Accuracy:0.1676, Validation Loss:2.5012, Validation Accuracy:0.1708\n",
    "Epoch #28: Loss:2.4866, Accuracy:0.1721, Validation Loss:2.4926, Validation Accuracy:0.1773\n",
    "Epoch #29: Loss:2.4763, Accuracy:0.1741, Validation Loss:2.5052, Validation Accuracy:0.1642\n",
    "Epoch #30: Loss:2.4755, Accuracy:0.1741, Validation Loss:2.4936, Validation Accuracy:0.1642\n",
    "Epoch #31: Loss:2.4729, Accuracy:0.1713, Validation Loss:2.4910, Validation Accuracy:0.1724\n",
    "Epoch #32: Loss:2.4711, Accuracy:0.1766, Validation Loss:2.4926, Validation Accuracy:0.1773\n",
    "Epoch #33: Loss:2.4713, Accuracy:0.1786, Validation Loss:2.4908, Validation Accuracy:0.1790\n",
    "Epoch #34: Loss:2.4708, Accuracy:0.1782, Validation Loss:2.4903, Validation Accuracy:0.1806\n",
    "Epoch #35: Loss:2.4695, Accuracy:0.1778, Validation Loss:2.4892, Validation Accuracy:0.1806\n",
    "Epoch #36: Loss:2.4682, Accuracy:0.1807, Validation Loss:2.4893, Validation Accuracy:0.1773\n",
    "Epoch #37: Loss:2.4663, Accuracy:0.1795, Validation Loss:2.4892, Validation Accuracy:0.1790\n",
    "Epoch #38: Loss:2.4651, Accuracy:0.1803, Validation Loss:2.4872, Validation Accuracy:0.1806\n",
    "Epoch #39: Loss:2.4634, Accuracy:0.1803, Validation Loss:2.4878, Validation Accuracy:0.1773\n",
    "Epoch #40: Loss:2.4629, Accuracy:0.1799, Validation Loss:2.4839, Validation Accuracy:0.1806\n",
    "Epoch #41: Loss:2.4619, Accuracy:0.1778, Validation Loss:2.4847, Validation Accuracy:0.1790\n",
    "Epoch #42: Loss:2.4608, Accuracy:0.1758, Validation Loss:2.4854, Validation Accuracy:0.1724\n",
    "Epoch #43: Loss:2.4591, Accuracy:0.1754, Validation Loss:2.4857, Validation Accuracy:0.1773\n",
    "Epoch #44: Loss:2.4576, Accuracy:0.1762, Validation Loss:2.4851, Validation Accuracy:0.1773\n",
    "Epoch #45: Loss:2.4568, Accuracy:0.1778, Validation Loss:2.4834, Validation Accuracy:0.1773\n",
    "Epoch #46: Loss:2.4562, Accuracy:0.1786, Validation Loss:2.4835, Validation Accuracy:0.1724\n",
    "Epoch #47: Loss:2.4557, Accuracy:0.1782, Validation Loss:2.4850, Validation Accuracy:0.1691\n",
    "Epoch #48: Loss:2.4562, Accuracy:0.1770, Validation Loss:2.4852, Validation Accuracy:0.1757\n",
    "Epoch #49: Loss:2.4561, Accuracy:0.1791, Validation Loss:2.4865, Validation Accuracy:0.1741\n",
    "Epoch #50: Loss:2.4577, Accuracy:0.1770, Validation Loss:2.4848, Validation Accuracy:0.1757\n",
    "Epoch #51: Loss:2.4552, Accuracy:0.1778, Validation Loss:2.4879, Validation Accuracy:0.1658\n",
    "Epoch #52: Loss:2.4551, Accuracy:0.1791, Validation Loss:2.4822, Validation Accuracy:0.1757\n",
    "Epoch #53: Loss:2.4521, Accuracy:0.1791, Validation Loss:2.4821, Validation Accuracy:0.1691\n",
    "Epoch #54: Loss:2.4522, Accuracy:0.1799, Validation Loss:2.4818, Validation Accuracy:0.1691\n",
    "Epoch #55: Loss:2.4519, Accuracy:0.1770, Validation Loss:2.4811, Validation Accuracy:0.1757\n",
    "Epoch #56: Loss:2.4523, Accuracy:0.1754, Validation Loss:2.4834, Validation Accuracy:0.1593\n",
    "Epoch #57: Loss:2.4510, Accuracy:0.1791, Validation Loss:2.4809, Validation Accuracy:0.1773\n",
    "Epoch #58: Loss:2.4508, Accuracy:0.1774, Validation Loss:2.4807, Validation Accuracy:0.1708\n",
    "Epoch #59: Loss:2.4491, Accuracy:0.1774, Validation Loss:2.4787, Validation Accuracy:0.1741\n",
    "Epoch #60: Loss:2.4509, Accuracy:0.1754, Validation Loss:2.4791, Validation Accuracy:0.1708\n",
    "Epoch #61: Loss:2.4506, Accuracy:0.1762, Validation Loss:2.4770, Validation Accuracy:0.1757\n",
    "Epoch #62: Loss:2.4512, Accuracy:0.1795, Validation Loss:2.4759, Validation Accuracy:0.1757\n",
    "Epoch #63: Loss:2.4494, Accuracy:0.1762, Validation Loss:2.4729, Validation Accuracy:0.1790\n",
    "Epoch #64: Loss:2.4495, Accuracy:0.1778, Validation Loss:2.4749, Validation Accuracy:0.1790\n",
    "Epoch #65: Loss:2.4491, Accuracy:0.1766, Validation Loss:2.4765, Validation Accuracy:0.1773\n",
    "Epoch #66: Loss:2.4483, Accuracy:0.1803, Validation Loss:2.4779, Validation Accuracy:0.1724\n",
    "Epoch #67: Loss:2.4497, Accuracy:0.1758, Validation Loss:2.4769, Validation Accuracy:0.1773\n",
    "Epoch #68: Loss:2.4494, Accuracy:0.1778, Validation Loss:2.4771, Validation Accuracy:0.1773\n",
    "Epoch #69: Loss:2.4490, Accuracy:0.1770, Validation Loss:2.4776, Validation Accuracy:0.1708\n",
    "Epoch #70: Loss:2.4502, Accuracy:0.1754, Validation Loss:2.4771, Validation Accuracy:0.1724\n",
    "Epoch #71: Loss:2.4496, Accuracy:0.1770, Validation Loss:2.4787, Validation Accuracy:0.1708\n",
    "Epoch #72: Loss:2.4489, Accuracy:0.1770, Validation Loss:2.4767, Validation Accuracy:0.1724\n",
    "Epoch #73: Loss:2.4478, Accuracy:0.1782, Validation Loss:2.4773, Validation Accuracy:0.1708\n",
    "Epoch #74: Loss:2.4483, Accuracy:0.1778, Validation Loss:2.4763, Validation Accuracy:0.1708\n",
    "Epoch #75: Loss:2.4494, Accuracy:0.1782, Validation Loss:2.4785, Validation Accuracy:0.1757\n",
    "Epoch #76: Loss:2.4511, Accuracy:0.1749, Validation Loss:2.4793, Validation Accuracy:0.1757\n",
    "Epoch #77: Loss:2.4488, Accuracy:0.1758, Validation Loss:2.4783, Validation Accuracy:0.1724\n",
    "Epoch #78: Loss:2.4458, Accuracy:0.1774, Validation Loss:2.4830, Validation Accuracy:0.1691\n",
    "Epoch #79: Loss:2.4461, Accuracy:0.1749, Validation Loss:2.4751, Validation Accuracy:0.1757\n",
    "Epoch #80: Loss:2.4451, Accuracy:0.1766, Validation Loss:2.4786, Validation Accuracy:0.1757\n",
    "Epoch #81: Loss:2.4467, Accuracy:0.1745, Validation Loss:2.4796, Validation Accuracy:0.1757\n",
    "Epoch #82: Loss:2.4456, Accuracy:0.1774, Validation Loss:2.4807, Validation Accuracy:0.1757\n",
    "Epoch #83: Loss:2.4444, Accuracy:0.1762, Validation Loss:2.4830, Validation Accuracy:0.1741\n",
    "Epoch #84: Loss:2.4445, Accuracy:0.1754, Validation Loss:2.4832, Validation Accuracy:0.1708\n",
    "Epoch #85: Loss:2.4452, Accuracy:0.1770, Validation Loss:2.4782, Validation Accuracy:0.1658\n",
    "Epoch #86: Loss:2.4465, Accuracy:0.1704, Validation Loss:2.4767, Validation Accuracy:0.1675\n",
    "Epoch #87: Loss:2.4450, Accuracy:0.1749, Validation Loss:2.4745, Validation Accuracy:0.1626\n",
    "Epoch #88: Loss:2.4451, Accuracy:0.1749, Validation Loss:2.4753, Validation Accuracy:0.1658\n",
    "Epoch #89: Loss:2.4456, Accuracy:0.1745, Validation Loss:2.4759, Validation Accuracy:0.1675\n",
    "Epoch #90: Loss:2.4445, Accuracy:0.1725, Validation Loss:2.4710, Validation Accuracy:0.1691\n",
    "Epoch #91: Loss:2.4443, Accuracy:0.1741, Validation Loss:2.4735, Validation Accuracy:0.1642\n",
    "Epoch #92: Loss:2.4445, Accuracy:0.1733, Validation Loss:2.4711, Validation Accuracy:0.1658\n",
    "Epoch #93: Loss:2.4422, Accuracy:0.1725, Validation Loss:2.4709, Validation Accuracy:0.1642\n",
    "Epoch #94: Loss:2.4407, Accuracy:0.1725, Validation Loss:2.4689, Validation Accuracy:0.1593\n",
    "Epoch #95: Loss:2.4418, Accuracy:0.1725, Validation Loss:2.4683, Validation Accuracy:0.1675\n",
    "Epoch #96: Loss:2.4411, Accuracy:0.1770, Validation Loss:2.4727, Validation Accuracy:0.1658\n",
    "Epoch #97: Loss:2.4417, Accuracy:0.1754, Validation Loss:2.4714, Validation Accuracy:0.1658\n",
    "Epoch #98: Loss:2.4434, Accuracy:0.1717, Validation Loss:2.4692, Validation Accuracy:0.1658\n",
    "Epoch #99: Loss:2.4452, Accuracy:0.1782, Validation Loss:2.4681, Validation Accuracy:0.1593\n",
    "Epoch #100: Loss:2.4424, Accuracy:0.1754, Validation Loss:2.4718, Validation Accuracy:0.1642\n",
    "Epoch #101: Loss:2.4410, Accuracy:0.1725, Validation Loss:2.4691, Validation Accuracy:0.1724\n",
    "Epoch #102: Loss:2.4403, Accuracy:0.1733, Validation Loss:2.4698, Validation Accuracy:0.1626\n",
    "Epoch #103: Loss:2.4388, Accuracy:0.1741, Validation Loss:2.4670, Validation Accuracy:0.1724\n",
    "Epoch #104: Loss:2.4404, Accuracy:0.1774, Validation Loss:2.4693, Validation Accuracy:0.1626\n",
    "Epoch #105: Loss:2.4400, Accuracy:0.1737, Validation Loss:2.4670, Validation Accuracy:0.1724\n",
    "Epoch #106: Loss:2.4406, Accuracy:0.1737, Validation Loss:2.4671, Validation Accuracy:0.1691\n",
    "Epoch #107: Loss:2.4396, Accuracy:0.1762, Validation Loss:2.4691, Validation Accuracy:0.1658\n",
    "Epoch #108: Loss:2.4379, Accuracy:0.1758, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #109: Loss:2.4387, Accuracy:0.1766, Validation Loss:2.4668, Validation Accuracy:0.1708\n",
    "Epoch #110: Loss:2.4406, Accuracy:0.1741, Validation Loss:2.4712, Validation Accuracy:0.1708\n",
    "Epoch #111: Loss:2.4411, Accuracy:0.1770, Validation Loss:2.4756, Validation Accuracy:0.1560\n",
    "Epoch #112: Loss:2.4494, Accuracy:0.1733, Validation Loss:2.6254, Validation Accuracy:0.1346\n",
    "Epoch #113: Loss:2.5000, Accuracy:0.1598, Validation Loss:2.5079, Validation Accuracy:0.1560\n",
    "Epoch #114: Loss:2.4596, Accuracy:0.1729, Validation Loss:2.4922, Validation Accuracy:0.1691\n",
    "Epoch #115: Loss:2.4629, Accuracy:0.1655, Validation Loss:2.4775, Validation Accuracy:0.1724\n",
    "Epoch #116: Loss:2.4407, Accuracy:0.1762, Validation Loss:2.4711, Validation Accuracy:0.1642\n",
    "Epoch #117: Loss:2.4448, Accuracy:0.1786, Validation Loss:2.4661, Validation Accuracy:0.1708\n",
    "Epoch #118: Loss:2.4392, Accuracy:0.1733, Validation Loss:2.4742, Validation Accuracy:0.1658\n",
    "Epoch #119: Loss:2.4393, Accuracy:0.1725, Validation Loss:2.4667, Validation Accuracy:0.1724\n",
    "Epoch #120: Loss:2.4381, Accuracy:0.1786, Validation Loss:2.4670, Validation Accuracy:0.1708\n",
    "Epoch #121: Loss:2.4371, Accuracy:0.1803, Validation Loss:2.4684, Validation Accuracy:0.1675\n",
    "Epoch #122: Loss:2.4370, Accuracy:0.1733, Validation Loss:2.4675, Validation Accuracy:0.1741\n",
    "Epoch #123: Loss:2.4359, Accuracy:0.1791, Validation Loss:2.4692, Validation Accuracy:0.1790\n",
    "Epoch #124: Loss:2.4350, Accuracy:0.1811, Validation Loss:2.4690, Validation Accuracy:0.1741\n",
    "Epoch #125: Loss:2.4361, Accuracy:0.1791, Validation Loss:2.4695, Validation Accuracy:0.1757\n",
    "Epoch #126: Loss:2.4367, Accuracy:0.1811, Validation Loss:2.4705, Validation Accuracy:0.1757\n",
    "Epoch #127: Loss:2.4375, Accuracy:0.1815, Validation Loss:2.4716, Validation Accuracy:0.1741\n",
    "Epoch #128: Loss:2.4373, Accuracy:0.1815, Validation Loss:2.4704, Validation Accuracy:0.1757\n",
    "Epoch #129: Loss:2.4367, Accuracy:0.1799, Validation Loss:2.4709, Validation Accuracy:0.1757\n",
    "Epoch #130: Loss:2.4366, Accuracy:0.1791, Validation Loss:2.4715, Validation Accuracy:0.1757\n",
    "Epoch #131: Loss:2.4371, Accuracy:0.1795, Validation Loss:2.4719, Validation Accuracy:0.1741\n",
    "Epoch #132: Loss:2.4366, Accuracy:0.1791, Validation Loss:2.4698, Validation Accuracy:0.1757\n",
    "Epoch #133: Loss:2.4367, Accuracy:0.1795, Validation Loss:2.4691, Validation Accuracy:0.1757\n",
    "Epoch #134: Loss:2.4359, Accuracy:0.1774, Validation Loss:2.4705, Validation Accuracy:0.1741\n",
    "Epoch #135: Loss:2.4362, Accuracy:0.1799, Validation Loss:2.4697, Validation Accuracy:0.1757\n",
    "Epoch #136: Loss:2.4359, Accuracy:0.1799, Validation Loss:2.4679, Validation Accuracy:0.1741\n",
    "Epoch #137: Loss:2.4353, Accuracy:0.1791, Validation Loss:2.4668, Validation Accuracy:0.1757\n",
    "Epoch #138: Loss:2.4361, Accuracy:0.1799, Validation Loss:2.4675, Validation Accuracy:0.1773\n",
    "Epoch #139: Loss:2.4353, Accuracy:0.1803, Validation Loss:2.4671, Validation Accuracy:0.1773\n",
    "Epoch #140: Loss:2.4355, Accuracy:0.1811, Validation Loss:2.4687, Validation Accuracy:0.1773\n",
    "Epoch #141: Loss:2.4347, Accuracy:0.1807, Validation Loss:2.4675, Validation Accuracy:0.1773\n",
    "Epoch #142: Loss:2.4353, Accuracy:0.1811, Validation Loss:2.4684, Validation Accuracy:0.1773\n",
    "Epoch #143: Loss:2.4352, Accuracy:0.1811, Validation Loss:2.4678, Validation Accuracy:0.1773\n",
    "Epoch #144: Loss:2.4355, Accuracy:0.1807, Validation Loss:2.4684, Validation Accuracy:0.1773\n",
    "Epoch #145: Loss:2.4347, Accuracy:0.1807, Validation Loss:2.4667, Validation Accuracy:0.1773\n",
    "Epoch #146: Loss:2.4352, Accuracy:0.1807, Validation Loss:2.4674, Validation Accuracy:0.1773\n",
    "Epoch #147: Loss:2.4350, Accuracy:0.1807, Validation Loss:2.4667, Validation Accuracy:0.1773\n",
    "Epoch #148: Loss:2.4349, Accuracy:0.1803, Validation Loss:2.4674, Validation Accuracy:0.1773\n",
    "Epoch #149: Loss:2.4346, Accuracy:0.1811, Validation Loss:2.4675, Validation Accuracy:0.1757\n",
    "Epoch #150: Loss:2.4352, Accuracy:0.1828, Validation Loss:2.4660, Validation Accuracy:0.1773\n",
    "Epoch #151: Loss:2.4351, Accuracy:0.1836, Validation Loss:2.4681, Validation Accuracy:0.1757\n",
    "Epoch #152: Loss:2.4351, Accuracy:0.1832, Validation Loss:2.4660, Validation Accuracy:0.1757\n",
    "Epoch #153: Loss:2.4350, Accuracy:0.1836, Validation Loss:2.4656, Validation Accuracy:0.1773\n",
    "Epoch #154: Loss:2.4347, Accuracy:0.1828, Validation Loss:2.4684, Validation Accuracy:0.1757\n",
    "Epoch #155: Loss:2.4349, Accuracy:0.1815, Validation Loss:2.4669, Validation Accuracy:0.1757\n",
    "Epoch #156: Loss:2.4340, Accuracy:0.1815, Validation Loss:2.4663, Validation Accuracy:0.1757\n",
    "Epoch #157: Loss:2.4339, Accuracy:0.1815, Validation Loss:2.4650, Validation Accuracy:0.1757\n",
    "Epoch #158: Loss:2.4343, Accuracy:0.1807, Validation Loss:2.4676, Validation Accuracy:0.1741\n",
    "Epoch #159: Loss:2.4337, Accuracy:0.1799, Validation Loss:2.4660, Validation Accuracy:0.1741\n",
    "Epoch #160: Loss:2.4338, Accuracy:0.1799, Validation Loss:2.4662, Validation Accuracy:0.1741\n",
    "Epoch #161: Loss:2.4339, Accuracy:0.1795, Validation Loss:2.4660, Validation Accuracy:0.1741\n",
    "Epoch #162: Loss:2.4332, Accuracy:0.1795, Validation Loss:2.4664, Validation Accuracy:0.1741\n",
    "Epoch #163: Loss:2.4332, Accuracy:0.1795, Validation Loss:2.4645, Validation Accuracy:0.1741\n",
    "Epoch #164: Loss:2.4333, Accuracy:0.1795, Validation Loss:2.4645, Validation Accuracy:0.1741\n",
    "Epoch #165: Loss:2.4340, Accuracy:0.1791, Validation Loss:2.4669, Validation Accuracy:0.1724\n",
    "Epoch #166: Loss:2.4344, Accuracy:0.1782, Validation Loss:2.4668, Validation Accuracy:0.1724\n",
    "Epoch #167: Loss:2.4338, Accuracy:0.1774, Validation Loss:2.4664, Validation Accuracy:0.1724\n",
    "Epoch #168: Loss:2.4339, Accuracy:0.1786, Validation Loss:2.4662, Validation Accuracy:0.1724\n",
    "Epoch #169: Loss:2.4335, Accuracy:0.1807, Validation Loss:2.4651, Validation Accuracy:0.1724\n",
    "Epoch #170: Loss:2.4338, Accuracy:0.1807, Validation Loss:2.4647, Validation Accuracy:0.1724\n",
    "Epoch #171: Loss:2.4336, Accuracy:0.1799, Validation Loss:2.4639, Validation Accuracy:0.1724\n",
    "Epoch #172: Loss:2.4333, Accuracy:0.1799, Validation Loss:2.4641, Validation Accuracy:0.1724\n",
    "Epoch #173: Loss:2.4323, Accuracy:0.1795, Validation Loss:2.4628, Validation Accuracy:0.1724\n",
    "Epoch #174: Loss:2.4326, Accuracy:0.1799, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #175: Loss:2.4319, Accuracy:0.1799, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #176: Loss:2.4321, Accuracy:0.1791, Validation Loss:2.4628, Validation Accuracy:0.1708\n",
    "Epoch #177: Loss:2.4319, Accuracy:0.1799, Validation Loss:2.4611, Validation Accuracy:0.1708\n",
    "Epoch #178: Loss:2.4311, Accuracy:0.1828, Validation Loss:2.4621, Validation Accuracy:0.1708\n",
    "Epoch #179: Loss:2.4310, Accuracy:0.1791, Validation Loss:2.4610, Validation Accuracy:0.1724\n",
    "Epoch #180: Loss:2.4321, Accuracy:0.1766, Validation Loss:2.4613, Validation Accuracy:0.1724\n",
    "Epoch #181: Loss:2.4309, Accuracy:0.1791, Validation Loss:2.4612, Validation Accuracy:0.1757\n",
    "Epoch #182: Loss:2.4317, Accuracy:0.1799, Validation Loss:2.4628, Validation Accuracy:0.1741\n",
    "Epoch #183: Loss:2.4330, Accuracy:0.1786, Validation Loss:2.4615, Validation Accuracy:0.1642\n",
    "Epoch #184: Loss:2.4306, Accuracy:0.1807, Validation Loss:2.4636, Validation Accuracy:0.1741\n",
    "Epoch #185: Loss:2.4323, Accuracy:0.1848, Validation Loss:2.4643, Validation Accuracy:0.1741\n",
    "Epoch #186: Loss:2.4328, Accuracy:0.1778, Validation Loss:2.4624, Validation Accuracy:0.1724\n",
    "Epoch #187: Loss:2.4315, Accuracy:0.1811, Validation Loss:2.4648, Validation Accuracy:0.1741\n",
    "Epoch #188: Loss:2.4333, Accuracy:0.1823, Validation Loss:2.4637, Validation Accuracy:0.1741\n",
    "Epoch #189: Loss:2.4309, Accuracy:0.1832, Validation Loss:2.4620, Validation Accuracy:0.1658\n",
    "Epoch #190: Loss:2.4319, Accuracy:0.1803, Validation Loss:2.4632, Validation Accuracy:0.1741\n",
    "Epoch #191: Loss:2.4310, Accuracy:0.1799, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #192: Loss:2.4307, Accuracy:0.1819, Validation Loss:2.4610, Validation Accuracy:0.1724\n",
    "Epoch #193: Loss:2.4306, Accuracy:0.1807, Validation Loss:2.4637, Validation Accuracy:0.1741\n",
    "Epoch #194: Loss:2.4312, Accuracy:0.1803, Validation Loss:2.4623, Validation Accuracy:0.1724\n",
    "Epoch #195: Loss:2.4312, Accuracy:0.1807, Validation Loss:2.4631, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4321, Accuracy:0.1811, Validation Loss:2.4638, Validation Accuracy:0.1741\n",
    "Epoch #197: Loss:2.4307, Accuracy:0.1774, Validation Loss:2.4627, Validation Accuracy:0.1773\n",
    "Epoch #198: Loss:2.4307, Accuracy:0.1795, Validation Loss:2.4646, Validation Accuracy:0.1757\n",
    "Epoch #199: Loss:2.4309, Accuracy:0.1811, Validation Loss:2.4644, Validation Accuracy:0.1757\n",
    "Epoch #200: Loss:2.4311, Accuracy:0.1819, Validation Loss:2.4625, Validation Accuracy:0.1790\n",
    "Epoch #201: Loss:2.4302, Accuracy:0.1791, Validation Loss:2.4633, Validation Accuracy:0.1741\n",
    "Epoch #202: Loss:2.4312, Accuracy:0.1799, Validation Loss:2.4628, Validation Accuracy:0.1757\n",
    "Epoch #203: Loss:2.4319, Accuracy:0.1799, Validation Loss:2.4619, Validation Accuracy:0.1675\n",
    "Epoch #204: Loss:2.4304, Accuracy:0.1782, Validation Loss:2.4669, Validation Accuracy:0.1741\n",
    "Epoch #205: Loss:2.4314, Accuracy:0.1803, Validation Loss:2.4629, Validation Accuracy:0.1691\n",
    "Epoch #206: Loss:2.4331, Accuracy:0.1811, Validation Loss:2.4645, Validation Accuracy:0.1708\n",
    "Epoch #207: Loss:2.4319, Accuracy:0.1778, Validation Loss:2.4624, Validation Accuracy:0.1626\n",
    "Epoch #208: Loss:2.4315, Accuracy:0.1733, Validation Loss:2.4642, Validation Accuracy:0.1724\n",
    "Epoch #209: Loss:2.4320, Accuracy:0.1828, Validation Loss:2.4624, Validation Accuracy:0.1708\n",
    "Epoch #210: Loss:2.4314, Accuracy:0.1807, Validation Loss:2.4623, Validation Accuracy:0.1642\n",
    "Epoch #211: Loss:2.4318, Accuracy:0.1811, Validation Loss:2.4643, Validation Accuracy:0.1708\n",
    "Epoch #212: Loss:2.4301, Accuracy:0.1819, Validation Loss:2.4622, Validation Accuracy:0.1773\n",
    "Epoch #213: Loss:2.4301, Accuracy:0.1815, Validation Loss:2.4627, Validation Accuracy:0.1724\n",
    "Epoch #214: Loss:2.4314, Accuracy:0.1811, Validation Loss:2.4658, Validation Accuracy:0.1757\n",
    "Epoch #215: Loss:2.4298, Accuracy:0.1823, Validation Loss:2.4633, Validation Accuracy:0.1724\n",
    "Epoch #216: Loss:2.4304, Accuracy:0.1791, Validation Loss:2.4636, Validation Accuracy:0.1675\n",
    "Epoch #217: Loss:2.4305, Accuracy:0.1803, Validation Loss:2.4623, Validation Accuracy:0.1708\n",
    "Epoch #218: Loss:2.4300, Accuracy:0.1848, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #219: Loss:2.4300, Accuracy:0.1828, Validation Loss:2.4637, Validation Accuracy:0.1708\n",
    "Epoch #220: Loss:2.4309, Accuracy:0.1811, Validation Loss:2.4627, Validation Accuracy:0.1691\n",
    "Epoch #221: Loss:2.4300, Accuracy:0.1828, Validation Loss:2.4587, Validation Accuracy:0.1609\n",
    "Epoch #222: Loss:2.4298, Accuracy:0.1860, Validation Loss:2.4596, Validation Accuracy:0.1691\n",
    "Epoch #223: Loss:2.4297, Accuracy:0.1803, Validation Loss:2.4638, Validation Accuracy:0.1708\n",
    "Epoch #224: Loss:2.4295, Accuracy:0.1823, Validation Loss:2.4607, Validation Accuracy:0.1708\n",
    "Epoch #225: Loss:2.4294, Accuracy:0.1823, Validation Loss:2.4605, Validation Accuracy:0.1741\n",
    "Epoch #226: Loss:2.4303, Accuracy:0.1823, Validation Loss:2.4630, Validation Accuracy:0.1708\n",
    "Epoch #227: Loss:2.4303, Accuracy:0.1815, Validation Loss:2.4624, Validation Accuracy:0.1757\n",
    "Epoch #228: Loss:2.4296, Accuracy:0.1819, Validation Loss:2.4634, Validation Accuracy:0.1724\n",
    "Epoch #229: Loss:2.4289, Accuracy:0.1815, Validation Loss:2.4615, Validation Accuracy:0.1691\n",
    "Epoch #230: Loss:2.4267, Accuracy:0.1807, Validation Loss:2.4601, Validation Accuracy:0.1741\n",
    "Epoch #231: Loss:2.4279, Accuracy:0.1852, Validation Loss:2.4608, Validation Accuracy:0.1741\n",
    "Epoch #232: Loss:2.4275, Accuracy:0.1807, Validation Loss:2.4609, Validation Accuracy:0.1724\n",
    "Epoch #233: Loss:2.4273, Accuracy:0.1815, Validation Loss:2.4604, Validation Accuracy:0.1724\n",
    "Epoch #234: Loss:2.4275, Accuracy:0.1815, Validation Loss:2.4609, Validation Accuracy:0.1724\n",
    "Epoch #235: Loss:2.4279, Accuracy:0.1811, Validation Loss:2.4623, Validation Accuracy:0.1691\n",
    "Epoch #236: Loss:2.4275, Accuracy:0.1819, Validation Loss:2.4615, Validation Accuracy:0.1675\n",
    "Epoch #237: Loss:2.4273, Accuracy:0.1803, Validation Loss:2.4612, Validation Accuracy:0.1691\n",
    "Epoch #238: Loss:2.4264, Accuracy:0.1832, Validation Loss:2.4611, Validation Accuracy:0.1658\n",
    "Epoch #239: Loss:2.4270, Accuracy:0.1836, Validation Loss:2.4617, Validation Accuracy:0.1691\n",
    "Epoch #240: Loss:2.4275, Accuracy:0.1815, Validation Loss:2.4596, Validation Accuracy:0.1724\n",
    "Epoch #241: Loss:2.4270, Accuracy:0.1786, Validation Loss:2.4602, Validation Accuracy:0.1724\n",
    "Epoch #242: Loss:2.4278, Accuracy:0.1852, Validation Loss:2.4609, Validation Accuracy:0.1691\n",
    "Epoch #243: Loss:2.4267, Accuracy:0.1828, Validation Loss:2.4640, Validation Accuracy:0.1708\n",
    "Epoch #244: Loss:2.4275, Accuracy:0.1819, Validation Loss:2.4609, Validation Accuracy:0.1691\n",
    "Epoch #245: Loss:2.4298, Accuracy:0.1844, Validation Loss:2.4599, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4269, Accuracy:0.1807, Validation Loss:2.4668, Validation Accuracy:0.1708\n",
    "Epoch #247: Loss:2.4269, Accuracy:0.1815, Validation Loss:2.4604, Validation Accuracy:0.1642\n",
    "Epoch #248: Loss:2.4270, Accuracy:0.1823, Validation Loss:2.4628, Validation Accuracy:0.1691\n",
    "Epoch #249: Loss:2.4280, Accuracy:0.1823, Validation Loss:2.4651, Validation Accuracy:0.1724\n",
    "Epoch #250: Loss:2.4267, Accuracy:0.1819, Validation Loss:2.4620, Validation Accuracy:0.1642\n",
    "Epoch #251: Loss:2.4283, Accuracy:0.1832, Validation Loss:2.4625, Validation Accuracy:0.1675\n",
    "Epoch #252: Loss:2.4296, Accuracy:0.1819, Validation Loss:2.4655, Validation Accuracy:0.1708\n",
    "Epoch #253: Loss:2.4287, Accuracy:0.1807, Validation Loss:2.4645, Validation Accuracy:0.1675\n",
    "Epoch #254: Loss:2.4289, Accuracy:0.1762, Validation Loss:2.4665, Validation Accuracy:0.1741\n",
    "Epoch #255: Loss:2.4285, Accuracy:0.1782, Validation Loss:2.4643, Validation Accuracy:0.1757\n",
    "Epoch #256: Loss:2.4279, Accuracy:0.1786, Validation Loss:2.4641, Validation Accuracy:0.1691\n",
    "Epoch #257: Loss:2.4284, Accuracy:0.1795, Validation Loss:2.4650, Validation Accuracy:0.1691\n",
    "Epoch #258: Loss:2.4272, Accuracy:0.1791, Validation Loss:2.4639, Validation Accuracy:0.1757\n",
    "Epoch #259: Loss:2.4278, Accuracy:0.1836, Validation Loss:2.4641, Validation Accuracy:0.1741\n",
    "Epoch #260: Loss:2.4269, Accuracy:0.1786, Validation Loss:2.4618, Validation Accuracy:0.1642\n",
    "Epoch #261: Loss:2.4267, Accuracy:0.1823, Validation Loss:2.4643, Validation Accuracy:0.1675\n",
    "Epoch #262: Loss:2.4267, Accuracy:0.1815, Validation Loss:2.4616, Validation Accuracy:0.1642\n",
    "Epoch #263: Loss:2.4262, Accuracy:0.1786, Validation Loss:2.4623, Validation Accuracy:0.1724\n",
    "Epoch #264: Loss:2.4268, Accuracy:0.1807, Validation Loss:2.4622, Validation Accuracy:0.1642\n",
    "Epoch #265: Loss:2.4286, Accuracy:0.1766, Validation Loss:2.4630, Validation Accuracy:0.1593\n",
    "Epoch #266: Loss:2.4283, Accuracy:0.1791, Validation Loss:2.4627, Validation Accuracy:0.1675\n",
    "Epoch #267: Loss:2.4269, Accuracy:0.1786, Validation Loss:2.4612, Validation Accuracy:0.1658\n",
    "Epoch #268: Loss:2.4287, Accuracy:0.1811, Validation Loss:2.4641, Validation Accuracy:0.1675\n",
    "Epoch #269: Loss:2.4280, Accuracy:0.1811, Validation Loss:2.4601, Validation Accuracy:0.1675\n",
    "Epoch #270: Loss:2.4273, Accuracy:0.1799, Validation Loss:2.4602, Validation Accuracy:0.1708\n",
    "Epoch #271: Loss:2.4280, Accuracy:0.1828, Validation Loss:2.4612, Validation Accuracy:0.1642\n",
    "Epoch #272: Loss:2.4266, Accuracy:0.1819, Validation Loss:2.4655, Validation Accuracy:0.1658\n",
    "Epoch #273: Loss:2.4282, Accuracy:0.1807, Validation Loss:2.4639, Validation Accuracy:0.1642\n",
    "Epoch #274: Loss:2.4281, Accuracy:0.1799, Validation Loss:2.4639, Validation Accuracy:0.1675\n",
    "Epoch #275: Loss:2.4285, Accuracy:0.1807, Validation Loss:2.4631, Validation Accuracy:0.1675\n",
    "Epoch #276: Loss:2.4281, Accuracy:0.1803, Validation Loss:2.4624, Validation Accuracy:0.1658\n",
    "Epoch #277: Loss:2.4296, Accuracy:0.1791, Validation Loss:2.4614, Validation Accuracy:0.1642\n",
    "Epoch #278: Loss:2.4289, Accuracy:0.1766, Validation Loss:2.4623, Validation Accuracy:0.1626\n",
    "Epoch #279: Loss:2.4302, Accuracy:0.1778, Validation Loss:2.4630, Validation Accuracy:0.1626\n",
    "Epoch #280: Loss:2.4274, Accuracy:0.1819, Validation Loss:2.4675, Validation Accuracy:0.1626\n",
    "Epoch #281: Loss:2.4284, Accuracy:0.1819, Validation Loss:2.4646, Validation Accuracy:0.1626\n",
    "Epoch #282: Loss:2.4273, Accuracy:0.1815, Validation Loss:2.4628, Validation Accuracy:0.1642\n",
    "Epoch #283: Loss:2.4268, Accuracy:0.1807, Validation Loss:2.4638, Validation Accuracy:0.1593\n",
    "Epoch #284: Loss:2.4264, Accuracy:0.1766, Validation Loss:2.4623, Validation Accuracy:0.1658\n",
    "Epoch #285: Loss:2.4286, Accuracy:0.1799, Validation Loss:2.4612, Validation Accuracy:0.1658\n",
    "Epoch #286: Loss:2.4291, Accuracy:0.1786, Validation Loss:2.4640, Validation Accuracy:0.1527\n",
    "Epoch #287: Loss:2.4304, Accuracy:0.1819, Validation Loss:2.4623, Validation Accuracy:0.1576\n",
    "Epoch #288: Loss:2.4319, Accuracy:0.1811, Validation Loss:2.4606, Validation Accuracy:0.1626\n",
    "Epoch #289: Loss:2.4320, Accuracy:0.1778, Validation Loss:2.4612, Validation Accuracy:0.1658\n",
    "Epoch #290: Loss:2.4305, Accuracy:0.1795, Validation Loss:2.4625, Validation Accuracy:0.1642\n",
    "Epoch #291: Loss:2.4310, Accuracy:0.1828, Validation Loss:2.4627, Validation Accuracy:0.1658\n",
    "Epoch #292: Loss:2.4301, Accuracy:0.1807, Validation Loss:2.4590, Validation Accuracy:0.1691\n",
    "Epoch #293: Loss:2.4305, Accuracy:0.1774, Validation Loss:2.4597, Validation Accuracy:0.1675\n",
    "Epoch #294: Loss:2.4301, Accuracy:0.1815, Validation Loss:2.4624, Validation Accuracy:0.1609\n",
    "Epoch #295: Loss:2.4304, Accuracy:0.1799, Validation Loss:2.4612, Validation Accuracy:0.1691\n",
    "Epoch #296: Loss:2.4295, Accuracy:0.1848, Validation Loss:2.4592, Validation Accuracy:0.1658\n",
    "Epoch #297: Loss:2.4307, Accuracy:0.1832, Validation Loss:2.4608, Validation Accuracy:0.1642\n",
    "Epoch #298: Loss:2.4315, Accuracy:0.1823, Validation Loss:2.4593, Validation Accuracy:0.1609\n",
    "Epoch #299: Loss:2.4298, Accuracy:0.1819, Validation Loss:2.4589, Validation Accuracy:0.1691\n",
    "Epoch #300: Loss:2.4301, Accuracy:0.1815, Validation Loss:2.4576, Validation Accuracy:0.1790\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45760322, Accuracy:0.1790\n",
    "Labels: ['sg', 'eo', 'eb', 'my', 'ck', 'ek', 'mb', 'ce', 'sk', 'eg', 'aa', 'yd', 'ds', 'ib', 'by']\n",
    "Confusion Matrix:\n",
    "      sg  eo  eb  my  ck  ek  mb  ce  sk  eg  aa  yd  ds  ib  by\n",
    "t:sg  25   0   2   0   0   0   0   0   0   2   0  17   0   0   5\n",
    "t:eo  23   0   2   0   0   0   0   0   0   1   0   5   0   0   3\n",
    "t:eb  13   0   2   0   0   0   0   0   0  12   0  16   2   0   5\n",
    "t:my   2   0   1   0   0   0   0   0   0   5   0   8   2   0   2\n",
    "t:ck   8   0   0   0   0   0   0   0   0   9   1   1   1   0   3\n",
    "t:ek  17   0   6   0   0   0   0   0   0  12   0   8   0   0   5\n",
    "t:mb  19   0   7   0   0   0   0   0   0   6   0  16   2   0   2\n",
    "t:ce  11   0   2   0   0   0   0   0   0   4   2   7   0   0   1\n",
    "t:sk  10   0   4   0   0   0   0   0   0  10   1   3   5   0   0\n",
    "t:eg   5   0   5   0   0   0   0   0   0  26   4   3   3   0   4\n",
    "t:aa   5   0   4   0   0   0   0   0   0  15   2   3   3   0   2\n",
    "t:yd  20   0   0   0   0   0   0   0   0   1   0  41   0   0   0\n",
    "t:ds   6   0   1   0   0   0   0   0   0   9   2   4   7   0   2\n",
    "t:ib  13   0   1   0   0   0   0   0   0   4   1  32   1   0   2\n",
    "t:by  16   0   6   0   0   0   0   0   0   6   1   5   0   0   6\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          sg       0.13      0.49      0.20        51\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          eb       0.05      0.04      0.04        50\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eg       0.21      0.52      0.30        50\n",
    "          aa       0.14      0.06      0.08        34\n",
    "          yd       0.24      0.66      0.35        62\n",
    "          ds       0.27      0.23      0.25        31\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          by       0.14      0.15      0.15        40\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.08      0.14      0.09       609\n",
    "weighted avg       0.09      0.18      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 15:23:38 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7089376097242233, 2.6997570443427428, 2.6932678250060684, 2.686923709800482, 2.6809930836625875, 2.674540806677933, 2.6678027754346725, 2.659728890765085, 2.6501605581180216, 2.6377818067672805, 2.622632179354212, 2.6041934826886908, 2.5853024836635745, 2.5664546994740154, 2.546919036968588, 2.5366182918423306, 2.52584386850617, 2.5226584670970396, 2.5099960962931314, 2.507342844369572, 2.497751638024116, 2.5013851708379287, 2.5026576973143078, 2.5811216717674617, 2.553144671059594, 2.553427248361271, 2.5012322912858234, 2.492627641054601, 2.5052332396577732, 2.493569246066615, 2.4910103836278807, 2.492573467577228, 2.4907603001555394, 2.4903134180015725, 2.4891738594067703, 2.48931604770604, 2.489155478469648, 2.4872420464438956, 2.487815952849114, 2.483879956510071, 2.4846685441648235, 2.485440151640543, 2.485698162432766, 2.4850753751294365, 2.4833874808156433, 2.4834596308189854, 2.484985136633436, 2.4851567251928923, 2.4865135739393813, 2.4848467978742126, 2.4878599459706074, 2.4821662977215495, 2.4821487651474174, 2.4818017850759975, 2.481070374424626, 2.4834242473877906, 2.480874254981481, 2.480675971762496, 2.478654680580928, 2.4790535936214653, 2.4769668563441885, 2.4758838655913404, 2.472942618900919, 2.474905740255597, 2.4764980894004185, 2.477883620019421, 2.476869914723539, 2.477097653403071, 2.4776011910931817, 2.477125133003899, 2.4787020338775685, 2.4767028083550713, 2.477287144888015, 2.4762542987691947, 2.478537474164039, 2.479311519851434, 2.478322139514491, 2.483036362674632, 2.4750631160924, 2.478647843566043, 2.4796254157237034, 2.480748936460523, 2.482953674100303, 2.4832412484048425, 2.478182452848588, 2.4766539180611544, 2.4745424678564465, 2.4753036557747223, 2.475890802045174, 2.470966251417138, 2.473469151260426, 2.471060923559129, 2.4708534672929736, 2.468904359783054, 2.468347029537207, 2.4727239679233195, 2.4713510613527596, 2.4691761322992383, 2.4681045801573003, 2.471750333782879, 2.469089018691741, 2.469759903909342, 2.466992213808257, 2.469274595257488, 2.4669514735931246, 2.4670656400752575, 2.469136934170778, 2.466077349651819, 2.4668242496809936, 2.471244717662166, 2.475566325712282, 2.625391366250801, 2.507914652769593, 2.4921823988602863, 2.4774983277657543, 2.4711031193412194, 2.4661036258064857, 2.47417001732073, 2.466714023565032, 2.4669788279165386, 2.4684037136522616, 2.4675226920148225, 2.469212755193851, 2.4690366938392128, 2.4695050595037653, 2.4704917518571876, 2.471569317315012, 2.4704388448561745, 2.4709103428475765, 2.4715033215646476, 2.4719268778470544, 2.469775609390685, 2.4691277728683647, 2.4705330311566933, 2.4696507704473287, 2.4678892361119464, 2.4667813143706674, 2.4674909263604574, 2.4670511744488244, 2.468673664556544, 2.467505761163771, 2.468406671765207, 2.467829250937025, 2.468388965760154, 2.466682853761369, 2.4673829665912197, 2.466664419189854, 2.4673992522635877, 2.4674731659380282, 2.4659839565139294, 2.46806096286805, 2.4660106413861604, 2.465644262498627, 2.468384847656651, 2.466862351632079, 2.466307872230392, 2.4649702355583702, 2.4676297102459936, 2.4660223877097196, 2.4661873937240375, 2.46598711154731, 2.466435457881057, 2.4644693204726296, 2.4645425932747975, 2.4669198782377446, 2.4668021487876506, 2.4663682747356996, 2.4662116948019697, 2.4651180942266055, 2.464727350056465, 2.4638777350753007, 2.4641147446749834, 2.4627512785405754, 2.461504044008177, 2.4614976560345228, 2.4627752018288045, 2.4610740645178435, 2.4621469919708954, 2.460992220391585, 2.461289697875726, 2.461199471907467, 2.462847921844382, 2.461456390828726, 2.463622208299308, 2.464287031264532, 2.4623796935934936, 2.464785049310067, 2.4637110264626236, 2.4620220089585128, 2.4631685439393243, 2.461496515227069, 2.4610441909439262, 2.4636886523079204, 2.4622728437038477, 2.4630509229324917, 2.4638437026827207, 2.4627179856762313, 2.464558242381304, 2.4644292751556547, 2.462470179903879, 2.463320781053189, 2.462762162015943, 2.46187953252119, 2.4669146569100113, 2.462928773147132, 2.464451869328817, 2.462359007551948, 2.4641840579278753, 2.4624472349539572, 2.4622831465966986, 2.4642817899707112, 2.4622451346887546, 2.462707886359179, 2.4657832703175413, 2.46333836021486, 2.463620783660212, 2.4623095938333344, 2.461463842877418, 2.46370362491639, 2.462703432355608, 2.458703896486505, 2.4596141238126457, 2.46384074143784, 2.460687259539399, 2.4604729318070686, 2.4630263395888856, 2.462380315674154, 2.463408160875192, 2.461521395713042, 2.460101349991922, 2.4608437259404727, 2.460855507889796, 2.4604294347058375, 2.4609350343838896, 2.462330294360081, 2.461490015678218, 2.4611812330819114, 2.461053695193261, 2.4616511917270856, 2.459554803391004, 2.4601736194003, 2.4608697123911187, 2.4640059713855362, 2.4608922552788393, 2.4598859159034268, 2.4667930947540233, 2.460381513354422, 2.46280106574248, 2.46512562026727, 2.4619970795360495, 2.4625109334297366, 2.465466591329214, 2.4645161824469106, 2.466489782082819, 2.464305301800933, 2.4641090829188403, 2.4650205007719093, 2.4638853668187837, 2.4641036071213596, 2.4617536318517477, 2.4643115304373757, 2.4616075192374747, 2.462322240196817, 2.4622001088115772, 2.4629986740293957, 2.462742533394073, 2.461180308377997, 2.464060869514453, 2.4601481066548767, 2.460225588778165, 2.4612215356090776, 2.4654944765156714, 2.463854402156886, 2.46386904943557, 2.463062237440463, 2.4623924444853182, 2.4613970120747886, 2.4622871797464554, 2.4630408846881786, 2.467504254311372, 2.4645860782397793, 2.462817025302079, 2.4637500167088753, 2.4623250193979547, 2.4612145694018586, 2.464010227294195, 2.462333969686223, 2.460618661933736, 2.4611989864574864, 2.462467178726823, 2.462671998686391, 2.458966899192196, 2.4596636436255697, 2.4624096893128895, 2.4612134489519844, 2.4591925312537084, 2.4607765083438267, 2.459252325380573, 2.4589061834933528, 2.4576035274073407], 'val_acc': [0.04926108324224334, 0.07881773398403072, 0.08866995033519022, 0.09031198645998496, 0.09031198645998496, 0.08866995033519022, 0.08538587808560072, 0.09852216698608571, 0.12807881762388304, 0.12479474508067462, 0.13300492580252132, 0.1559934317453937, 0.15599343164752075, 0.15599343154964776, 0.154351395522726, 0.14942528724621473, 0.15763546787018845, 0.1789819366973022, 0.17241379219812322, 0.17241379229599618, 0.17077175607332848, 0.17077175617120144, 0.17241379229599618, 0.1559934317453937, 0.15270935939793126, 0.167487683725866, 0.17077175607332848, 0.17733990057250745, 0.16420361137840353, 0.16420361157414948, 0.17241379219812322, 0.17733990057250745, 0.1789819366973022, 0.18062397282209694, 0.18062397282209694, 0.17733990067038044, 0.17898193679517518, 0.18062397282209694, 0.17733990067038044, 0.18062397282209694, 0.1789819366973022, 0.17241379219812322, 0.17733990057250745, 0.17733990057250745, 0.17733990057250745, 0.17241379210025023, 0.16912972084162467, 0.1756978644477127, 0.17405582832291797, 0.1756978644477127, 0.16584564859203518, 0.1756978644477127, 0.16912971975278776, 0.16912971975278776, 0.1756978644477127, 0.1592775030040193, 0.17733990057250745, 0.1707717559754555, 0.17405582832291797, 0.1707717559754555, 0.1756978644477127, 0.1756978644477127, 0.1789819365994292, 0.1789819366973022, 0.17733990047463447, 0.17241379229599618, 0.17733990057250745, 0.17733990057250745, 0.17077175617120144, 0.17241379219812322, 0.17077175607332848, 0.17241379229599618, 0.17077175617120144, 0.17077175607332848, 0.1756978644477127, 0.1756978644477127, 0.17241379219812322, 0.1691297200464067, 0.1756978644477127, 0.1756978644477127, 0.1756978644477127, 0.1756978644477127, 0.1740558292160089, 0.1707717558775825, 0.16584564859203518, 0.16748768471682993, 0.16256157634244567, 0.16584564750319827, 0.16748768362799302, 0.16912971975278776, 0.16420361137840353, 0.16584564750319827, 0.16420361246724044, 0.15927750409285618, 0.16748768362799302, 0.16584564760107126, 0.16584564750319827, 0.16584564760107126, 0.15927750310189226, 0.16420361137840353, 0.17241379200237725, 0.16256157634244567, 0.17241379200237725, 0.1625615752536088, 0.17241379200237725, 0.16912971985066075, 0.1658456477968172, 0.1691297200464067, 0.17077175617120144, 0.17077175626907443, 0.15599343075442978, 0.13464696202518905, 0.15599343085230277, 0.16912972014427968, 0.17241379249174216, 0.16420361176989545, 0.17077175646482037, 0.16584564760107126, 0.17241379258961514, 0.1707717563669474, 0.16748768392161195, 0.1740558287144099, 0.17898193708879412, 0.17405582881228285, 0.1756978649370776, 0.1756978649370776, 0.17405582881228285, 0.1756978649370776, 0.17569786483920463, 0.17569786483920463, 0.1740558286165369, 0.17569786483920463, 0.17569786483920463, 0.1740558286165369, 0.17569786483920463, 0.1740558287144099, 0.17569786483920463, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.17733990096399938, 0.1756978649370776, 0.17733990106187233, 0.1756978649370776, 0.1756978649370776, 0.17733990106187233, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.17405582881228285, 0.17405582881228285, 0.17405582881228285, 0.17405582881228285, 0.17405582881228285, 0.17405582881228285, 0.17405582881228285, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.1724137926874881, 0.17077175666056635, 0.17077175656269336, 0.17077175666056635, 0.1724137927853611, 0.1724137926874881, 0.1756978649370776, 0.17405582891015584, 0.1642036118677684, 0.17405582891015584, 0.17405582891015584, 0.17241379258961514, 0.17405582891015584, 0.17405582891015584, 0.1658456478946902, 0.17405582881228285, 0.1724137926874881, 0.1724137926874881, 0.17405582891015584, 0.1724137927853611, 0.1724137927853611, 0.17405582891015584, 0.17733990096399938, 0.17569786503495058, 0.1756978649370776, 0.17898193728454007, 0.17405582881228285, 0.1756978649370776, 0.16748768411735793, 0.17405582891015584, 0.16912972034002563, 0.17077175656269336, 0.16256157584084666, 0.1724137927853611, 0.17077175666056635, 0.1642036120635144, 0.17077175666056635, 0.17733990115974532, 0.1724137927853611, 0.17569786513282357, 0.1724137926874881, 0.16748768431310387, 0.17077175666056635, 0.1724137926874881, 0.17077175656269336, 0.16912972034002563, 0.16091953961817893, 0.1691297205357716, 0.17077175656269336, 0.17077175656269336, 0.17405582881228285, 0.17077175666056635, 0.17569786503495058, 0.1724137927853611, 0.16912972043789862, 0.17405582891015584, 0.17405582891015584, 0.1724137927853611, 0.1724137927853611, 0.1724137927853611, 0.1691297205357716, 0.1674876842152309, 0.1691297205357716, 0.16584564809043614, 0.1691297205357716, 0.1724137927853611, 0.1724137927853611, 0.16912972034002563, 0.17077175666056635, 0.16912972043789862, 0.16912972043789862, 0.17077175666056635, 0.1642036119656414, 0.1691297205357716, 0.17241379288323408, 0.1642036120635144, 0.16748768431310387, 0.1707717563669474, 0.16748768411735793, 0.1740558287144099, 0.17569786483920463, 0.16912972043789862, 0.16912972043789862, 0.17569786483920463, 0.17405582891015584, 0.1642036120635144, 0.1674876842152309, 0.1642036120635144, 0.17241379258961514, 0.16420361176989545, 0.15927750349338418, 0.1674876842152309, 0.16584564799256318, 0.1674876842152309, 0.16748768411735793, 0.1707717563669474, 0.1642036119656414, 0.16584564799256318, 0.1642036120635144, 0.16748768431310387, 0.16748768431310387, 0.16584564809043614, 0.1642036120635144, 0.16256157593871964, 0.16256157574297367, 0.1625615756451007, 0.16256157574297367, 0.1642036119656414, 0.15927750359125717, 0.16584564828618212, 0.16584564818830913, 0.1527093589942052, 0.15763546746646243, 0.16256157584084666, 0.16584564809043614, 0.1642036119656414, 0.16584564809043614, 0.16912972043789862, 0.16748768431310387, 0.1609195397160519, 0.16912972043789862, 0.16584564799256318, 0.1642036118677684, 0.16091953961817893, 0.1691297205357716, 0.17898193699092113], 'loss': [2.718869895758815, 2.705460733848431, 2.6977929739981463, 2.690674899686778, 2.685275417970191, 2.678653978175451, 2.6714811319198453, 2.6638017982428077, 2.6545068611599336, 2.6434897569660287, 2.628910990955893, 2.611839529425212, 2.5915674190012092, 2.5702730924931396, 2.5525176540782075, 2.5351034588392753, 2.5243977840431415, 2.5183992106811712, 2.5085930458084515, 2.501699845648889, 2.4952664268579814, 2.490271174785293, 2.491190427137841, 2.495234735202985, 2.5170989561374673, 2.5156583777198556, 2.5010016783062192, 2.4866033604992, 2.476301178844068, 2.475517867184273, 2.4729479253169693, 2.4711377230023457, 2.471306563598664, 2.4707860745933266, 2.4694555698723764, 2.4682097184339833, 2.4663367242049388, 2.465127030926808, 2.4634211745839836, 2.4629264445765062, 2.4619188765970343, 2.4608266021681517, 2.4590579563587354, 2.4576280337339553, 2.4568134407732765, 2.4561656687538727, 2.4556852357343484, 2.456200513506817, 2.456111642320543, 2.457726311928438, 2.4552129433140375, 2.455078689861102, 2.452134420347899, 2.4521807615761886, 2.451858648382418, 2.452341735534355, 2.451038557401183, 2.450765726649541, 2.4490956596280515, 2.4509498026336733, 2.450551328326153, 2.4512070854586496, 2.4494228528510376, 2.44945192209749, 2.449075624046874, 2.448324762967088, 2.449721571943843, 2.449432187990976, 2.4490430984653733, 2.450239231160534, 2.4496252757078323, 2.4488570318084966, 2.447813345421511, 2.4482888890487704, 2.44943417866372, 2.451145579829598, 2.448787527554334, 2.4458102003994417, 2.446088912305891, 2.4451393542593265, 2.4466510585690915, 2.4456272499272464, 2.4443891306922176, 2.4444557195816197, 2.4451578823692746, 2.44648915903769, 2.445037605238646, 2.4450936365421305, 2.4455831029331905, 2.4444905906732077, 2.444335971377958, 2.4445002176923185, 2.44221247579038, 2.440744317287782, 2.441846645343475, 2.441097081613247, 2.4417024801399183, 2.4433951267226766, 2.4452137268544223, 2.44243765885825, 2.441003934558657, 2.4403245096578736, 2.438760475945913, 2.4404113779322567, 2.439963826800274, 2.44057767102361, 2.43956152212938, 2.437860952804221, 2.43870640868279, 2.440598411481728, 2.4410505824510076, 2.449356142451386, 2.499964773091937, 2.4596324379194443, 2.4628836299849244, 2.4406588869662746, 2.444796341106877, 2.439180050542467, 2.4393231029627995, 2.4381141269965827, 2.4371034410699925, 2.436993535294425, 2.4359016352855205, 2.4350467699509135, 2.4360633061896606, 2.436710245359605, 2.4375087976945253, 2.4372672069244072, 2.436686741497972, 2.4365635771036636, 2.4370585026437497, 2.4366056078023735, 2.4367369629274402, 2.435885691887544, 2.43621809360183, 2.435938072889982, 2.4352955039766533, 2.43607620664201, 2.4352915820644623, 2.4355284084774387, 2.4347413791523334, 2.4352882255029384, 2.4352036110919113, 2.435511285617366, 2.4346974203473977, 2.4351920527354403, 2.4350240387221382, 2.4348983067506635, 2.4346054418865415, 2.435154094049818, 2.435107102971792, 2.435083764093857, 2.4349992817677024, 2.4347487251861386, 2.4349385103895433, 2.4340229872560597, 2.4338679603482665, 2.4342818590160267, 2.433674309826485, 2.433832393681489, 2.433879992800327, 2.433168258804071, 2.4331786040163137, 2.4333297726554792, 2.4340214134976117, 2.4344424132204154, 2.4337830790259267, 2.4339481516295636, 2.433535210846386, 2.433778900091653, 2.4336239897984497, 2.4332970993229988, 2.432340919016813, 2.4325718552669704, 2.4319362412243164, 2.432121833491864, 2.43194630944019, 2.4311441357865227, 2.4310314406604494, 2.4321130994409015, 2.4309014920581293, 2.431729340014761, 2.4330278318275904, 2.430583090948618, 2.432340099777284, 2.432774635017285, 2.431506040698449, 2.433280238380667, 2.4308854940246016, 2.431912801202073, 2.430982016929611, 2.4306939822202835, 2.4305690764401726, 2.4311783389144366, 2.431155944115327, 2.4321167503294268, 2.4307032396171615, 2.430729176816999, 2.4309014704193177, 2.431084831833105, 2.430218312970422, 2.4311596854756257, 2.4318622141648123, 2.430432029916031, 2.431408441336003, 2.4330538239567185, 2.4318913410821246, 2.431474153217104, 2.4320470928411466, 2.431435035924892, 2.431812594999278, 2.430051973370311, 2.4300974955548984, 2.4314457518364123, 2.4297801616010726, 2.430396519843068, 2.430498700954586, 2.4300233700926546, 2.4300243305229796, 2.430873395481149, 2.430020956532911, 2.4297695075706778, 2.4296539333077183, 2.429494238976825, 2.429446752556051, 2.4302529371249846, 2.430336858114912, 2.429635231499799, 2.4288979951361123, 2.426738495209868, 2.4278572900094537, 2.4274646529916377, 2.4273178150521657, 2.4274769561736247, 2.4279283318921037, 2.427454825837999, 2.427335735219215, 2.426377860917203, 2.42701991186984, 2.427486773633859, 2.426988278669009, 2.427845020313772, 2.426723314849258, 2.4274556592749374, 2.429816948266, 2.4269163894457493, 2.4268553527228884, 2.4269950034437238, 2.4280227796252993, 2.4267255405136203, 2.4282627281956604, 2.4296070536548844, 2.4286719206667042, 2.428938300017214, 2.4285306482099656, 2.4279016248988907, 2.4283817561500127, 2.4272335137674697, 2.427774131273587, 2.4268929399259282, 2.4267063334737227, 2.426684287735079, 2.4262340805124207, 2.4268426300808636, 2.428623951827721, 2.4282591180390156, 2.4268920155276508, 2.4287125574734665, 2.427971739641695, 2.4273467405620788, 2.427999376810062, 2.426572984053124, 2.4282204574138477, 2.4281418329391635, 2.4285349352403833, 2.4280904667088627, 2.4296479705912377, 2.428921531700745, 2.430164774928005, 2.427429657354492, 2.428440030056838, 2.427303176151409, 2.4268125010222135, 2.4264114190910386, 2.4286259476898633, 2.42911781941596, 2.4304235537683696, 2.4319216836894073, 2.4320437051432333, 2.4305426104112815, 2.4310085726469692, 2.430058192031829, 2.4305092390557825, 2.4301221461756275, 2.4304421559985903, 2.429523876511341, 2.430719297279812, 2.431459744020654, 2.429773901718108, 2.430148391018658], 'acc': [0.04804928129121998, 0.05215605746732845, 0.08911704327107944, 0.08993839889887177, 0.0899383988896924, 0.08952772108497561, 0.09117043175308122, 0.09815195045860396, 0.11498973338694543, 0.12772073992958305, 0.13963039044359626, 0.1457905555163076, 0.15482546250800577, 0.15893223712209315, 0.15852156026897, 0.15359342808718554, 0.15071868593565493, 0.16796714568040208, 0.16673511380532438, 0.1696098557793874, 0.17207392187945897, 0.1659137578042381, 0.16468172455837593, 0.17043121160300606, 0.16878850171820584, 0.16098562579992126, 0.16755646866817003, 0.17207392264440563, 0.17412730936397028, 0.17412730936397028, 0.1712525656641876, 0.17659137507238917, 0.17864476453352268, 0.17823408732546428, 0.177823408709904, 0.18069815225057778, 0.17946611900471565, 0.18028747343919116, 0.1802874752016283, 0.17987679779774354, 0.17782340872826272, 0.17577002001371717, 0.1753593437847905, 0.176180697258493, 0.1778234083182513, 0.1786447645518814, 0.17823408515301573, 0.17700205288628534, 0.17905544038915536, 0.17700205386541706, 0.1778234075349459, 0.17905544038915536, 0.17905544038915536, 0.17987679719190577, 0.17700205386541706, 0.1753593426098324, 0.17905544156411346, 0.1774127310918342, 0.17741273071854022, 0.17535934221817973, 0.1761806976501457, 0.17946611900471565, 0.17618069825598345, 0.17782340970739446, 0.17659137665735866, 0.180287475219987, 0.17577001983624954, 0.17782340890573037, 0.17700205427542848, 0.17535934319731147, 0.17700205288628534, 0.17700205445289613, 0.1782340869338116, 0.17782340970739446, 0.17823408652380018, 0.17494866638090575, 0.1757700206011962, 0.1774127318934983, 0.1749486646001099, 0.17659137567822694, 0.17453798756951913, 0.17741273089600784, 0.17618069825598345, 0.17535934382150795, 0.17700205447125483, 0.17043121138882097, 0.17494866502848005, 0.17494866559760036, 0.17453798758787786, 0.17248459851839704, 0.17412730975562296, 0.17330595373617794, 0.17248459947917005, 0.17248459887333228, 0.17248459869586466, 0.17700205327793803, 0.17535934341149653, 0.17166324543634723, 0.17823408632797383, 0.17535934182652702, 0.17248459908751737, 0.17330595471530968, 0.17412731116312485, 0.17741273011270245, 0.17371663135424775, 0.1737166315500741, 0.17618069784597204, 0.17577002104792508, 0.1765913756598682, 0.17412730916814392, 0.17700205347376438, 0.1733059543420157, 0.15975359453068133, 0.17289527690141354, 0.16550307995362448, 0.17618069806015713, 0.1786447629669119, 0.17330595494785347, 0.17248459850003833, 0.1786447645518814, 0.18028747384920257, 0.1733059551069624, 0.17905544234741885, 0.18110883026030028, 0.17905544117246075, 0.18110882926280983, 0.1815195062934006, 0.18151950828838154, 0.17987679719190577, 0.17905544156411346, 0.17946611939636833, 0.17905544138664584, 0.17946612017967373, 0.1774127303085288, 0.17987679621277403, 0.17987679640860038, 0.1790554409766344, 0.1798767977793848, 0.18028747541581336, 0.1811088286753308, 0.18069815244640414, 0.18110882926280983, 0.1811088294769949, 0.1806981524280454, 0.18069815086143462, 0.18069815203639272, 0.1806981512347286, 0.18028747361665878, 0.18110882945863618, 0.1827515393434364, 0.18357289438374969, 0.18316221754898526, 0.18357289477540237, 0.18275153993091545, 0.18151950688087964, 0.1815195066850533, 0.18151950727253235, 0.18069815205475143, 0.17987679660442674, 0.1798767964269591, 0.1794661193780096, 0.1794661185763455, 0.17946611900471565, 0.17946611879053057, 0.17905544117246075, 0.1782340861505062, 0.17741273208932465, 0.17864476357274967, 0.1806981512347286, 0.18069815164474, 0.17987679740609086, 0.1798767960169477, 0.17946611996548867, 0.17987679621277403, 0.17987679740609086, 0.1790554413682871, 0.17987679758355848, 0.18275153891506626, 0.1790554409766344, 0.17659137507238917, 0.17905544158247216, 0.17987679619441532, 0.17864476335856458, 0.18069815166309874, 0.18480492704213278, 0.17782340931574178, 0.18110883026030028, 0.18234086311450973, 0.18316221656985351, 0.180287475219987, 0.17987679582112134, 0.18193018530061358, 0.18069815144891366, 0.1802874744183229, 0.18069815105726098, 0.18110882928116856, 0.1774127307001815, 0.17946611798886647, 0.18110882887115715, 0.1819301858697339, 0.17905544115410205, 0.1798767968002531, 0.17987679640860038, 0.17823408730710558, 0.1802874740083115, 0.18110883026030028, 0.1778234095115681, 0.17330595530278875, 0.1827515409100472, 0.18069815184056637, 0.18110882928116856, 0.18193018391147042, 0.1815195066850533, 0.1811088284611457, 0.182340862135378, 0.17905544134992837, 0.18028747343919116, 0.18480492702377405, 0.18275153993091545, 0.18110882847950444, 0.18275153953926274, 0.1860369598779835, 0.180287475219987, 0.1823408627044983, 0.1823408634878037, 0.18234086231284563, 0.1815195070950647, 0.18193018448059073, 0.18151950609757425, 0.18069815203639272, 0.18521560604934575, 0.18069815164474, 0.18151950729089106, 0.1815195078783701, 0.18110883045612663, 0.18193018590645135, 0.18028747502416068, 0.18316221774481162, 0.18357289418792333, 0.18151950768254377, 0.17864476474770774, 0.18521560644099844, 0.18275154071422084, 0.18193018369728534, 0.1843942512048588, 0.1806981526422305, 0.18151950689923838, 0.1823408617253666, 0.18234086290032467, 0.18193018528225485, 0.1831622185464757, 0.18193018510478723, 0.18069815164474, 0.17618069747267806, 0.17823408515301573, 0.17864476414187, 0.1794661184172366, 0.17905544257996264, 0.18357289595036047, 0.17864476433769633, 0.18234086329197738, 0.18151950825166407, 0.17864476355439093, 0.18069815303388317, 0.17659137685318502, 0.1790554404075141, 0.1786447649251754, 0.18110882967282124, 0.1811088298686476, 0.17987679583948007, 0.18275154051839448, 0.18193018510478723, 0.18069815184056637, 0.17987679779774354, 0.18069815185892507, 0.18028747404502893, 0.17905544234741885, 0.17659137489492155, 0.1778234095115681, 0.1819301850864285, 0.18193018588809262, 0.18151950805583775, 0.18069815285641555, 0.17659137528657423, 0.17987679660442674, 0.17864476414187, 0.18193018432148184, 0.18110882928116856, 0.17782340911991543, 0.1794661185947042, 0.1827515409284059, 0.18069815303388317, 0.1774127303085288, 0.1815195062934006, 0.179876795625295, 0.1848049284129172, 0.18316221794063794, 0.18234086290032467, 0.18193018369728534, 0.1815195078783701]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
