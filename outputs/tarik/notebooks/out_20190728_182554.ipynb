{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf67.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 18:25:54 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'Split', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 15 Label(s): ['yd', 'my', 'ib', 'aa', 'sk', 'eg', 'ek', 'ck', 'ce', 'mb', 'eb', 'eo', 'sg', 'by', 'ds'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002370126FF98>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023727AB6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6746, Accuracy:0.0886, Validation Loss:2.6564, Validation Accuracy:0.0891\n",
    "Epoch #2: Loss:2.6450, Accuracy:0.0906, Validation Loss:2.6247, Validation Accuracy:0.0977\n",
    "Epoch #3: Loss:2.6023, Accuracy:0.1143, Validation Loss:2.5634, Validation Accuracy:0.1289\n",
    "Epoch #4: Loss:2.5418, Accuracy:0.1506, Validation Loss:2.5034, Validation Accuracy:0.1523\n",
    "Epoch #5: Loss:2.5070, Accuracy:0.1536, Validation Loss:2.4848, Validation Accuracy:0.1613\n",
    "Epoch #6: Loss:2.4861, Accuracy:0.1687, Validation Loss:2.4675, Validation Accuracy:0.1724\n",
    "Epoch #7: Loss:2.4753, Accuracy:0.1717, Validation Loss:2.4619, Validation Accuracy:0.1769\n",
    "Epoch #8: Loss:2.4686, Accuracy:0.1748, Validation Loss:2.4549, Validation Accuracy:0.1753\n",
    "Epoch #9: Loss:2.4621, Accuracy:0.1734, Validation Loss:2.4514, Validation Accuracy:0.1745\n",
    "Epoch #10: Loss:2.4612, Accuracy:0.1729, Validation Loss:2.4527, Validation Accuracy:0.1658\n",
    "Epoch #11: Loss:2.4602, Accuracy:0.1745, Validation Loss:2.4493, Validation Accuracy:0.1773\n",
    "Epoch #12: Loss:2.4642, Accuracy:0.1766, Validation Loss:2.5033, Validation Accuracy:0.1613\n",
    "Epoch #13: Loss:2.4870, Accuracy:0.1699, Validation Loss:2.4522, Validation Accuracy:0.1691\n",
    "Epoch #14: Loss:2.4637, Accuracy:0.1738, Validation Loss:2.4485, Validation Accuracy:0.1794\n",
    "Epoch #15: Loss:2.4566, Accuracy:0.1714, Validation Loss:2.4486, Validation Accuracy:0.1741\n",
    "Epoch #16: Loss:2.4594, Accuracy:0.1746, Validation Loss:2.4432, Validation Accuracy:0.1798\n",
    "Epoch #17: Loss:2.4557, Accuracy:0.1721, Validation Loss:2.4445, Validation Accuracy:0.1741\n",
    "Epoch #18: Loss:2.4531, Accuracy:0.1745, Validation Loss:2.4416, Validation Accuracy:0.1712\n",
    "Epoch #19: Loss:2.4480, Accuracy:0.1756, Validation Loss:2.4442, Validation Accuracy:0.1700\n",
    "Epoch #20: Loss:2.4509, Accuracy:0.1735, Validation Loss:2.4417, Validation Accuracy:0.1753\n",
    "Epoch #21: Loss:2.4506, Accuracy:0.1734, Validation Loss:2.4414, Validation Accuracy:0.1749\n",
    "Epoch #22: Loss:2.4495, Accuracy:0.1722, Validation Loss:2.4418, Validation Accuracy:0.1724\n",
    "Epoch #23: Loss:2.4532, Accuracy:0.1756, Validation Loss:2.4379, Validation Accuracy:0.1749\n",
    "Epoch #24: Loss:2.4494, Accuracy:0.1739, Validation Loss:2.4393, Validation Accuracy:0.1691\n",
    "Epoch #25: Loss:2.4474, Accuracy:0.1777, Validation Loss:2.4400, Validation Accuracy:0.1753\n",
    "Epoch #26: Loss:2.4474, Accuracy:0.1757, Validation Loss:2.4419, Validation Accuracy:0.1691\n",
    "Epoch #27: Loss:2.4481, Accuracy:0.1789, Validation Loss:2.4392, Validation Accuracy:0.1757\n",
    "Epoch #28: Loss:2.4470, Accuracy:0.1752, Validation Loss:2.4372, Validation Accuracy:0.1683\n",
    "Epoch #29: Loss:2.4468, Accuracy:0.1786, Validation Loss:2.4404, Validation Accuracy:0.1753\n",
    "Epoch #30: Loss:2.4460, Accuracy:0.1817, Validation Loss:2.4390, Validation Accuracy:0.1741\n",
    "Epoch #31: Loss:2.4467, Accuracy:0.1771, Validation Loss:2.4400, Validation Accuracy:0.1683\n",
    "Epoch #32: Loss:2.4487, Accuracy:0.1729, Validation Loss:2.4360, Validation Accuracy:0.1728\n",
    "Epoch #33: Loss:2.4481, Accuracy:0.1748, Validation Loss:2.4358, Validation Accuracy:0.1728\n",
    "Epoch #34: Loss:2.4476, Accuracy:0.1795, Validation Loss:2.4340, Validation Accuracy:0.1683\n",
    "Epoch #35: Loss:2.4452, Accuracy:0.1763, Validation Loss:2.4342, Validation Accuracy:0.1736\n",
    "Epoch #36: Loss:2.4440, Accuracy:0.1769, Validation Loss:2.4339, Validation Accuracy:0.1732\n",
    "Epoch #37: Loss:2.4436, Accuracy:0.1784, Validation Loss:2.4329, Validation Accuracy:0.1704\n",
    "Epoch #38: Loss:2.4482, Accuracy:0.1784, Validation Loss:2.4316, Validation Accuracy:0.1732\n",
    "Epoch #39: Loss:2.4431, Accuracy:0.1765, Validation Loss:2.4326, Validation Accuracy:0.1761\n",
    "Epoch #40: Loss:2.4433, Accuracy:0.1836, Validation Loss:2.4337, Validation Accuracy:0.1782\n",
    "Epoch #41: Loss:2.4415, Accuracy:0.1797, Validation Loss:2.4326, Validation Accuracy:0.1675\n",
    "Epoch #42: Loss:2.4419, Accuracy:0.1814, Validation Loss:2.4311, Validation Accuracy:0.1782\n",
    "Epoch #43: Loss:2.4409, Accuracy:0.1773, Validation Loss:2.4327, Validation Accuracy:0.1695\n",
    "Epoch #44: Loss:2.4425, Accuracy:0.1761, Validation Loss:2.4328, Validation Accuracy:0.1720\n",
    "Epoch #45: Loss:2.4447, Accuracy:0.1760, Validation Loss:2.4322, Validation Accuracy:0.1712\n",
    "Epoch #46: Loss:2.4439, Accuracy:0.1787, Validation Loss:2.4312, Validation Accuracy:0.1728\n",
    "Epoch #47: Loss:2.4429, Accuracy:0.1795, Validation Loss:2.4308, Validation Accuracy:0.1741\n",
    "Epoch #48: Loss:2.4414, Accuracy:0.1792, Validation Loss:2.4338, Validation Accuracy:0.1749\n",
    "Epoch #49: Loss:2.4419, Accuracy:0.1813, Validation Loss:2.4327, Validation Accuracy:0.1765\n",
    "Epoch #50: Loss:2.4418, Accuracy:0.1841, Validation Loss:2.4309, Validation Accuracy:0.1757\n",
    "Epoch #51: Loss:2.4396, Accuracy:0.1832, Validation Loss:2.4312, Validation Accuracy:0.1716\n",
    "Epoch #52: Loss:2.4412, Accuracy:0.1818, Validation Loss:2.4311, Validation Accuracy:0.1720\n",
    "Epoch #53: Loss:2.4409, Accuracy:0.1806, Validation Loss:2.4307, Validation Accuracy:0.1827\n",
    "Epoch #54: Loss:2.4388, Accuracy:0.1799, Validation Loss:2.4308, Validation Accuracy:0.1745\n",
    "Epoch #55: Loss:2.4404, Accuracy:0.1821, Validation Loss:2.4302, Validation Accuracy:0.1712\n",
    "Epoch #56: Loss:2.4413, Accuracy:0.1795, Validation Loss:2.4303, Validation Accuracy:0.1704\n",
    "Epoch #57: Loss:2.4390, Accuracy:0.1831, Validation Loss:2.4308, Validation Accuracy:0.1700\n",
    "Epoch #58: Loss:2.4385, Accuracy:0.1820, Validation Loss:2.4291, Validation Accuracy:0.1753\n",
    "Epoch #59: Loss:2.4400, Accuracy:0.1794, Validation Loss:2.4293, Validation Accuracy:0.1786\n",
    "Epoch #60: Loss:2.4415, Accuracy:0.1780, Validation Loss:2.4327, Validation Accuracy:0.1761\n",
    "Epoch #61: Loss:2.4426, Accuracy:0.1789, Validation Loss:2.4305, Validation Accuracy:0.1700\n",
    "Epoch #62: Loss:2.4383, Accuracy:0.1772, Validation Loss:2.4292, Validation Accuracy:0.1757\n",
    "Epoch #63: Loss:2.4387, Accuracy:0.1784, Validation Loss:2.4293, Validation Accuracy:0.1724\n",
    "Epoch #64: Loss:2.4377, Accuracy:0.1764, Validation Loss:2.4292, Validation Accuracy:0.1769\n",
    "Epoch #65: Loss:2.4383, Accuracy:0.1834, Validation Loss:2.4298, Validation Accuracy:0.1786\n",
    "Epoch #66: Loss:2.4383, Accuracy:0.1803, Validation Loss:2.4299, Validation Accuracy:0.1765\n",
    "Epoch #67: Loss:2.4376, Accuracy:0.1807, Validation Loss:2.4300, Validation Accuracy:0.1700\n",
    "Epoch #68: Loss:2.4374, Accuracy:0.1807, Validation Loss:2.4298, Validation Accuracy:0.1679\n",
    "Epoch #69: Loss:2.4396, Accuracy:0.1833, Validation Loss:2.4305, Validation Accuracy:0.1806\n",
    "Epoch #70: Loss:2.4372, Accuracy:0.1810, Validation Loss:2.4290, Validation Accuracy:0.1786\n",
    "Epoch #71: Loss:2.4393, Accuracy:0.1817, Validation Loss:2.4289, Validation Accuracy:0.1753\n",
    "Epoch #72: Loss:2.4378, Accuracy:0.1811, Validation Loss:2.4278, Validation Accuracy:0.1757\n",
    "Epoch #73: Loss:2.4374, Accuracy:0.1787, Validation Loss:2.4278, Validation Accuracy:0.1712\n",
    "Epoch #74: Loss:2.4375, Accuracy:0.1780, Validation Loss:2.4286, Validation Accuracy:0.1786\n",
    "Epoch #75: Loss:2.4369, Accuracy:0.1815, Validation Loss:2.4288, Validation Accuracy:0.1749\n",
    "Epoch #76: Loss:2.4381, Accuracy:0.1790, Validation Loss:2.4286, Validation Accuracy:0.1798\n",
    "Epoch #77: Loss:2.4372, Accuracy:0.1836, Validation Loss:2.4290, Validation Accuracy:0.1831\n",
    "Epoch #78: Loss:2.4378, Accuracy:0.1829, Validation Loss:2.4306, Validation Accuracy:0.1695\n",
    "Epoch #79: Loss:2.4370, Accuracy:0.1809, Validation Loss:2.4284, Validation Accuracy:0.1728\n",
    "Epoch #80: Loss:2.4370, Accuracy:0.1806, Validation Loss:2.4287, Validation Accuracy:0.1757\n",
    "Epoch #81: Loss:2.4404, Accuracy:0.1802, Validation Loss:2.4308, Validation Accuracy:0.1749\n",
    "Epoch #82: Loss:2.4406, Accuracy:0.1809, Validation Loss:2.4310, Validation Accuracy:0.1769\n",
    "Epoch #83: Loss:2.4524, Accuracy:0.1774, Validation Loss:2.4982, Validation Accuracy:0.1527\n",
    "Epoch #84: Loss:2.4710, Accuracy:0.1693, Validation Loss:2.4431, Validation Accuracy:0.1716\n",
    "Epoch #85: Loss:2.4481, Accuracy:0.1825, Validation Loss:2.4403, Validation Accuracy:0.1708\n",
    "Epoch #86: Loss:2.4427, Accuracy:0.1812, Validation Loss:2.4367, Validation Accuracy:0.1753\n",
    "Epoch #87: Loss:2.4408, Accuracy:0.1807, Validation Loss:2.4346, Validation Accuracy:0.1724\n",
    "Epoch #88: Loss:2.4407, Accuracy:0.1800, Validation Loss:2.4374, Validation Accuracy:0.1704\n",
    "Epoch #89: Loss:2.4414, Accuracy:0.1794, Validation Loss:2.4368, Validation Accuracy:0.1736\n",
    "Epoch #90: Loss:2.4421, Accuracy:0.1779, Validation Loss:2.4365, Validation Accuracy:0.1724\n",
    "Epoch #91: Loss:2.4433, Accuracy:0.1776, Validation Loss:2.4384, Validation Accuracy:0.1712\n",
    "Epoch #92: Loss:2.4446, Accuracy:0.1818, Validation Loss:2.4370, Validation Accuracy:0.1741\n",
    "Epoch #93: Loss:2.4420, Accuracy:0.1786, Validation Loss:2.4351, Validation Accuracy:0.1695\n",
    "Epoch #94: Loss:2.4407, Accuracy:0.1783, Validation Loss:2.4333, Validation Accuracy:0.1704\n",
    "Epoch #95: Loss:2.4409, Accuracy:0.1781, Validation Loss:2.4342, Validation Accuracy:0.1704\n",
    "Epoch #96: Loss:2.4407, Accuracy:0.1793, Validation Loss:2.4355, Validation Accuracy:0.1708\n",
    "Epoch #97: Loss:2.4435, Accuracy:0.1814, Validation Loss:2.4344, Validation Accuracy:0.1712\n",
    "Epoch #98: Loss:2.4403, Accuracy:0.1823, Validation Loss:2.4339, Validation Accuracy:0.1745\n",
    "Epoch #99: Loss:2.4390, Accuracy:0.1777, Validation Loss:2.4316, Validation Accuracy:0.1728\n",
    "Epoch #100: Loss:2.4390, Accuracy:0.1805, Validation Loss:2.4328, Validation Accuracy:0.1745\n",
    "Epoch #101: Loss:2.4391, Accuracy:0.1806, Validation Loss:2.4328, Validation Accuracy:0.1724\n",
    "Epoch #102: Loss:2.4394, Accuracy:0.1781, Validation Loss:2.4317, Validation Accuracy:0.1708\n",
    "Epoch #103: Loss:2.4388, Accuracy:0.1778, Validation Loss:2.4340, Validation Accuracy:0.1761\n",
    "Epoch #104: Loss:2.4402, Accuracy:0.1804, Validation Loss:2.4332, Validation Accuracy:0.1728\n",
    "Epoch #105: Loss:2.4391, Accuracy:0.1815, Validation Loss:2.4329, Validation Accuracy:0.1728\n",
    "Epoch #106: Loss:2.4391, Accuracy:0.1819, Validation Loss:2.4331, Validation Accuracy:0.1753\n",
    "Epoch #107: Loss:2.4396, Accuracy:0.1802, Validation Loss:2.4325, Validation Accuracy:0.1741\n",
    "Epoch #108: Loss:2.4409, Accuracy:0.1787, Validation Loss:2.4330, Validation Accuracy:0.1745\n",
    "Epoch #109: Loss:2.4391, Accuracy:0.1820, Validation Loss:2.4362, Validation Accuracy:0.1741\n",
    "Epoch #110: Loss:2.4431, Accuracy:0.1763, Validation Loss:2.4388, Validation Accuracy:0.1802\n",
    "Epoch #111: Loss:2.4425, Accuracy:0.1845, Validation Loss:2.4332, Validation Accuracy:0.1745\n",
    "Epoch #112: Loss:2.4396, Accuracy:0.1801, Validation Loss:2.4322, Validation Accuracy:0.1753\n",
    "Epoch #113: Loss:2.4380, Accuracy:0.1803, Validation Loss:2.4313, Validation Accuracy:0.1724\n",
    "Epoch #114: Loss:2.4384, Accuracy:0.1808, Validation Loss:2.4360, Validation Accuracy:0.1761\n",
    "Epoch #115: Loss:2.4438, Accuracy:0.1829, Validation Loss:2.4325, Validation Accuracy:0.1782\n",
    "Epoch #116: Loss:2.4385, Accuracy:0.1818, Validation Loss:2.4320, Validation Accuracy:0.1724\n",
    "Epoch #117: Loss:2.4377, Accuracy:0.1830, Validation Loss:2.4323, Validation Accuracy:0.1724\n",
    "Epoch #118: Loss:2.4379, Accuracy:0.1832, Validation Loss:2.4335, Validation Accuracy:0.1716\n",
    "Epoch #119: Loss:2.4401, Accuracy:0.1822, Validation Loss:2.4323, Validation Accuracy:0.1757\n",
    "Epoch #120: Loss:2.4392, Accuracy:0.1817, Validation Loss:2.4327, Validation Accuracy:0.1736\n",
    "Epoch #121: Loss:2.4370, Accuracy:0.1821, Validation Loss:2.4330, Validation Accuracy:0.1724\n",
    "Epoch #122: Loss:2.4375, Accuracy:0.1821, Validation Loss:2.4330, Validation Accuracy:0.1724\n",
    "Epoch #123: Loss:2.4368, Accuracy:0.1817, Validation Loss:2.4329, Validation Accuracy:0.1732\n",
    "Epoch #124: Loss:2.4371, Accuracy:0.1831, Validation Loss:2.4343, Validation Accuracy:0.1769\n",
    "Epoch #125: Loss:2.4401, Accuracy:0.1830, Validation Loss:2.4381, Validation Accuracy:0.1769\n",
    "Epoch #126: Loss:2.4418, Accuracy:0.1823, Validation Loss:2.4325, Validation Accuracy:0.1745\n",
    "Epoch #127: Loss:2.4370, Accuracy:0.1825, Validation Loss:2.4331, Validation Accuracy:0.1769\n",
    "Epoch #128: Loss:2.4364, Accuracy:0.1828, Validation Loss:2.4326, Validation Accuracy:0.1732\n",
    "Epoch #129: Loss:2.4370, Accuracy:0.1836, Validation Loss:2.4330, Validation Accuracy:0.1773\n",
    "Epoch #130: Loss:2.4376, Accuracy:0.1809, Validation Loss:2.4332, Validation Accuracy:0.1745\n",
    "Epoch #131: Loss:2.4365, Accuracy:0.1829, Validation Loss:2.4319, Validation Accuracy:0.1741\n",
    "Epoch #132: Loss:2.4367, Accuracy:0.1836, Validation Loss:2.4330, Validation Accuracy:0.1745\n",
    "Epoch #133: Loss:2.4365, Accuracy:0.1812, Validation Loss:2.4315, Validation Accuracy:0.1724\n",
    "Epoch #134: Loss:2.4370, Accuracy:0.1814, Validation Loss:2.4331, Validation Accuracy:0.1778\n",
    "Epoch #135: Loss:2.4406, Accuracy:0.1826, Validation Loss:2.4326, Validation Accuracy:0.1741\n",
    "Epoch #136: Loss:2.4359, Accuracy:0.1853, Validation Loss:2.4317, Validation Accuracy:0.1786\n",
    "Epoch #137: Loss:2.4363, Accuracy:0.1829, Validation Loss:2.4317, Validation Accuracy:0.1778\n",
    "Epoch #138: Loss:2.4374, Accuracy:0.1856, Validation Loss:2.4329, Validation Accuracy:0.1773\n",
    "Epoch #139: Loss:2.4369, Accuracy:0.1828, Validation Loss:2.4319, Validation Accuracy:0.1773\n",
    "Epoch #140: Loss:2.4383, Accuracy:0.1813, Validation Loss:2.4362, Validation Accuracy:0.1765\n",
    "Epoch #141: Loss:2.4389, Accuracy:0.1833, Validation Loss:2.4328, Validation Accuracy:0.1769\n",
    "Epoch #142: Loss:2.4359, Accuracy:0.1840, Validation Loss:2.4329, Validation Accuracy:0.1745\n",
    "Epoch #143: Loss:2.4362, Accuracy:0.1838, Validation Loss:2.4317, Validation Accuracy:0.1761\n",
    "Epoch #144: Loss:2.4355, Accuracy:0.1829, Validation Loss:2.4327, Validation Accuracy:0.1782\n",
    "Epoch #145: Loss:2.4364, Accuracy:0.1844, Validation Loss:2.4334, Validation Accuracy:0.1757\n",
    "Epoch #146: Loss:2.4361, Accuracy:0.1831, Validation Loss:2.4318, Validation Accuracy:0.1753\n",
    "Epoch #147: Loss:2.4356, Accuracy:0.1829, Validation Loss:2.4312, Validation Accuracy:0.1773\n",
    "Epoch #148: Loss:2.4399, Accuracy:0.1849, Validation Loss:2.4326, Validation Accuracy:0.1786\n",
    "Epoch #149: Loss:2.4367, Accuracy:0.1819, Validation Loss:2.4323, Validation Accuracy:0.1778\n",
    "Epoch #150: Loss:2.4353, Accuracy:0.1834, Validation Loss:2.4321, Validation Accuracy:0.1769\n",
    "Epoch #151: Loss:2.4361, Accuracy:0.1854, Validation Loss:2.4313, Validation Accuracy:0.1724\n",
    "Epoch #152: Loss:2.4349, Accuracy:0.1823, Validation Loss:2.4317, Validation Accuracy:0.1753\n",
    "Epoch #153: Loss:2.4361, Accuracy:0.1838, Validation Loss:2.4310, Validation Accuracy:0.1769\n",
    "Epoch #154: Loss:2.4348, Accuracy:0.1839, Validation Loss:2.4327, Validation Accuracy:0.1741\n",
    "Epoch #155: Loss:2.4380, Accuracy:0.1803, Validation Loss:2.4332, Validation Accuracy:0.1769\n",
    "Epoch #156: Loss:2.4395, Accuracy:0.1838, Validation Loss:2.4328, Validation Accuracy:0.1778\n",
    "Epoch #157: Loss:2.4379, Accuracy:0.1826, Validation Loss:2.4359, Validation Accuracy:0.1753\n",
    "Epoch #158: Loss:2.4347, Accuracy:0.1851, Validation Loss:2.4337, Validation Accuracy:0.1765\n",
    "Epoch #159: Loss:2.4350, Accuracy:0.1868, Validation Loss:2.4330, Validation Accuracy:0.1753\n",
    "Epoch #160: Loss:2.4353, Accuracy:0.1836, Validation Loss:2.4323, Validation Accuracy:0.1749\n",
    "Epoch #161: Loss:2.4350, Accuracy:0.1833, Validation Loss:2.4312, Validation Accuracy:0.1786\n",
    "Epoch #162: Loss:2.4344, Accuracy:0.1841, Validation Loss:2.4314, Validation Accuracy:0.1806\n",
    "Epoch #163: Loss:2.4349, Accuracy:0.1852, Validation Loss:2.4315, Validation Accuracy:0.1778\n",
    "Epoch #164: Loss:2.4356, Accuracy:0.1857, Validation Loss:2.4319, Validation Accuracy:0.1786\n",
    "Epoch #165: Loss:2.4344, Accuracy:0.1855, Validation Loss:2.4321, Validation Accuracy:0.1847\n",
    "Epoch #166: Loss:2.4338, Accuracy:0.1839, Validation Loss:2.4298, Validation Accuracy:0.1806\n",
    "Epoch #167: Loss:2.4333, Accuracy:0.1855, Validation Loss:2.4312, Validation Accuracy:0.1806\n",
    "Epoch #168: Loss:2.4340, Accuracy:0.1843, Validation Loss:2.4303, Validation Accuracy:0.1790\n",
    "Epoch #169: Loss:2.4342, Accuracy:0.1872, Validation Loss:2.4311, Validation Accuracy:0.1790\n",
    "Epoch #170: Loss:2.4337, Accuracy:0.1870, Validation Loss:2.4316, Validation Accuracy:0.1782\n",
    "Epoch #171: Loss:2.4338, Accuracy:0.1854, Validation Loss:2.4315, Validation Accuracy:0.1794\n",
    "Epoch #172: Loss:2.4357, Accuracy:0.1854, Validation Loss:2.4309, Validation Accuracy:0.1790\n",
    "Epoch #173: Loss:2.4332, Accuracy:0.1867, Validation Loss:2.4301, Validation Accuracy:0.1823\n",
    "Epoch #174: Loss:2.4368, Accuracy:0.1837, Validation Loss:2.4313, Validation Accuracy:0.1798\n",
    "Epoch #175: Loss:2.4350, Accuracy:0.1851, Validation Loss:2.4305, Validation Accuracy:0.1773\n",
    "Epoch #176: Loss:2.4362, Accuracy:0.1869, Validation Loss:2.4300, Validation Accuracy:0.1819\n",
    "Epoch #177: Loss:2.4332, Accuracy:0.1846, Validation Loss:2.4301, Validation Accuracy:0.1835\n",
    "Epoch #178: Loss:2.4332, Accuracy:0.1867, Validation Loss:2.4307, Validation Accuracy:0.1802\n",
    "Epoch #179: Loss:2.4355, Accuracy:0.1802, Validation Loss:2.4338, Validation Accuracy:0.1782\n",
    "Epoch #180: Loss:2.4345, Accuracy:0.1843, Validation Loss:2.4301, Validation Accuracy:0.1761\n",
    "Epoch #181: Loss:2.4351, Accuracy:0.1861, Validation Loss:2.4315, Validation Accuracy:0.1761\n",
    "Epoch #182: Loss:2.4348, Accuracy:0.1860, Validation Loss:2.4321, Validation Accuracy:0.1835\n",
    "Epoch #183: Loss:2.4337, Accuracy:0.1862, Validation Loss:2.4322, Validation Accuracy:0.1823\n",
    "Epoch #184: Loss:2.4386, Accuracy:0.1821, Validation Loss:2.4321, Validation Accuracy:0.1827\n",
    "Epoch #185: Loss:2.4339, Accuracy:0.1816, Validation Loss:2.4308, Validation Accuracy:0.1782\n",
    "Epoch #186: Loss:2.4324, Accuracy:0.1859, Validation Loss:2.4320, Validation Accuracy:0.1782\n",
    "Epoch #187: Loss:2.4337, Accuracy:0.1862, Validation Loss:2.4296, Validation Accuracy:0.1790\n",
    "Epoch #188: Loss:2.4351, Accuracy:0.1860, Validation Loss:2.4313, Validation Accuracy:0.1810\n",
    "Epoch #189: Loss:2.4327, Accuracy:0.1873, Validation Loss:2.4317, Validation Accuracy:0.1810\n",
    "Epoch #190: Loss:2.4327, Accuracy:0.1872, Validation Loss:2.4308, Validation Accuracy:0.1806\n",
    "Epoch #191: Loss:2.4315, Accuracy:0.1888, Validation Loss:2.4307, Validation Accuracy:0.1839\n",
    "Epoch #192: Loss:2.4326, Accuracy:0.1847, Validation Loss:2.4313, Validation Accuracy:0.1798\n",
    "Epoch #193: Loss:2.4322, Accuracy:0.1876, Validation Loss:2.4306, Validation Accuracy:0.1802\n",
    "Epoch #194: Loss:2.4318, Accuracy:0.1851, Validation Loss:2.4302, Validation Accuracy:0.1790\n",
    "Epoch #195: Loss:2.4313, Accuracy:0.1872, Validation Loss:2.4309, Validation Accuracy:0.1819\n",
    "Epoch #196: Loss:2.4329, Accuracy:0.1871, Validation Loss:2.4329, Validation Accuracy:0.1782\n",
    "Epoch #197: Loss:2.4326, Accuracy:0.1845, Validation Loss:2.4305, Validation Accuracy:0.1806\n",
    "Epoch #198: Loss:2.4326, Accuracy:0.1861, Validation Loss:2.4318, Validation Accuracy:0.1769\n",
    "Epoch #199: Loss:2.4331, Accuracy:0.1842, Validation Loss:2.4319, Validation Accuracy:0.1790\n",
    "Epoch #200: Loss:2.4321, Accuracy:0.1835, Validation Loss:2.4368, Validation Accuracy:0.1798\n",
    "Epoch #201: Loss:2.4334, Accuracy:0.1847, Validation Loss:2.4317, Validation Accuracy:0.1790\n",
    "Epoch #202: Loss:2.4337, Accuracy:0.1847, Validation Loss:2.4311, Validation Accuracy:0.1839\n",
    "Epoch #203: Loss:2.4304, Accuracy:0.1878, Validation Loss:2.4301, Validation Accuracy:0.1778\n",
    "Epoch #204: Loss:2.4319, Accuracy:0.1894, Validation Loss:2.4299, Validation Accuracy:0.1806\n",
    "Epoch #205: Loss:2.4304, Accuracy:0.1885, Validation Loss:2.4292, Validation Accuracy:0.1810\n",
    "Epoch #206: Loss:2.4331, Accuracy:0.1859, Validation Loss:2.4300, Validation Accuracy:0.1810\n",
    "Epoch #207: Loss:2.4351, Accuracy:0.1854, Validation Loss:2.4333, Validation Accuracy:0.1814\n",
    "Epoch #208: Loss:2.4312, Accuracy:0.1873, Validation Loss:2.4314, Validation Accuracy:0.1819\n",
    "Epoch #209: Loss:2.4327, Accuracy:0.1859, Validation Loss:2.4298, Validation Accuracy:0.1835\n",
    "Epoch #210: Loss:2.4295, Accuracy:0.1877, Validation Loss:2.4301, Validation Accuracy:0.1843\n",
    "Epoch #211: Loss:2.4298, Accuracy:0.1869, Validation Loss:2.4295, Validation Accuracy:0.1823\n",
    "Epoch #212: Loss:2.4298, Accuracy:0.1881, Validation Loss:2.4310, Validation Accuracy:0.1851\n",
    "Epoch #213: Loss:2.4290, Accuracy:0.1887, Validation Loss:2.4298, Validation Accuracy:0.1823\n",
    "Epoch #214: Loss:2.4287, Accuracy:0.1880, Validation Loss:2.4295, Validation Accuracy:0.1827\n",
    "Epoch #215: Loss:2.4283, Accuracy:0.1896, Validation Loss:2.4291, Validation Accuracy:0.1839\n",
    "Epoch #216: Loss:2.4300, Accuracy:0.1862, Validation Loss:2.4289, Validation Accuracy:0.1835\n",
    "Epoch #217: Loss:2.4303, Accuracy:0.1887, Validation Loss:2.4291, Validation Accuracy:0.1794\n",
    "Epoch #218: Loss:2.4293, Accuracy:0.1896, Validation Loss:2.4311, Validation Accuracy:0.1798\n",
    "Epoch #219: Loss:2.4292, Accuracy:0.1894, Validation Loss:2.4296, Validation Accuracy:0.1814\n",
    "Epoch #220: Loss:2.4318, Accuracy:0.1858, Validation Loss:2.4288, Validation Accuracy:0.1819\n",
    "Epoch #221: Loss:2.4283, Accuracy:0.1892, Validation Loss:2.4287, Validation Accuracy:0.1790\n",
    "Epoch #222: Loss:2.4284, Accuracy:0.1882, Validation Loss:2.4295, Validation Accuracy:0.1790\n",
    "Epoch #223: Loss:2.4275, Accuracy:0.1909, Validation Loss:2.4299, Validation Accuracy:0.1782\n",
    "Epoch #224: Loss:2.4280, Accuracy:0.1910, Validation Loss:2.4288, Validation Accuracy:0.1798\n",
    "Epoch #225: Loss:2.4284, Accuracy:0.1897, Validation Loss:2.4287, Validation Accuracy:0.1778\n",
    "Epoch #226: Loss:2.4292, Accuracy:0.1868, Validation Loss:2.4285, Validation Accuracy:0.1786\n",
    "Epoch #227: Loss:2.4299, Accuracy:0.1866, Validation Loss:2.4288, Validation Accuracy:0.1814\n",
    "Epoch #228: Loss:2.4277, Accuracy:0.1873, Validation Loss:2.4290, Validation Accuracy:0.1827\n",
    "Epoch #229: Loss:2.4300, Accuracy:0.1848, Validation Loss:2.4286, Validation Accuracy:0.1831\n",
    "Epoch #230: Loss:2.4293, Accuracy:0.1871, Validation Loss:2.4305, Validation Accuracy:0.1810\n",
    "Epoch #231: Loss:2.4273, Accuracy:0.1881, Validation Loss:2.4294, Validation Accuracy:0.1782\n",
    "Epoch #232: Loss:2.4297, Accuracy:0.1862, Validation Loss:2.4281, Validation Accuracy:0.1839\n",
    "Epoch #233: Loss:2.4270, Accuracy:0.1889, Validation Loss:2.4306, Validation Accuracy:0.1823\n",
    "Epoch #234: Loss:2.4299, Accuracy:0.1871, Validation Loss:2.4281, Validation Accuracy:0.1847\n",
    "Epoch #235: Loss:2.4264, Accuracy:0.1905, Validation Loss:2.4285, Validation Accuracy:0.1827\n",
    "Epoch #236: Loss:2.4265, Accuracy:0.1920, Validation Loss:2.4292, Validation Accuracy:0.1790\n",
    "Epoch #237: Loss:2.4283, Accuracy:0.1863, Validation Loss:2.4279, Validation Accuracy:0.1843\n",
    "Epoch #238: Loss:2.4275, Accuracy:0.1889, Validation Loss:2.4289, Validation Accuracy:0.1810\n",
    "Epoch #239: Loss:2.4273, Accuracy:0.1863, Validation Loss:2.4281, Validation Accuracy:0.1831\n",
    "Epoch #240: Loss:2.4274, Accuracy:0.1896, Validation Loss:2.4281, Validation Accuracy:0.1843\n",
    "Epoch #241: Loss:2.4271, Accuracy:0.1875, Validation Loss:2.4281, Validation Accuracy:0.1831\n",
    "Epoch #242: Loss:2.4251, Accuracy:0.1911, Validation Loss:2.4288, Validation Accuracy:0.1794\n",
    "Epoch #243: Loss:2.4270, Accuracy:0.1890, Validation Loss:2.4283, Validation Accuracy:0.1831\n",
    "Epoch #244: Loss:2.4272, Accuracy:0.1906, Validation Loss:2.4279, Validation Accuracy:0.1794\n",
    "Epoch #245: Loss:2.4250, Accuracy:0.1903, Validation Loss:2.4279, Validation Accuracy:0.1839\n",
    "Epoch #246: Loss:2.4248, Accuracy:0.1906, Validation Loss:2.4275, Validation Accuracy:0.1827\n",
    "Epoch #247: Loss:2.4254, Accuracy:0.1896, Validation Loss:2.4274, Validation Accuracy:0.1806\n",
    "Epoch #248: Loss:2.4258, Accuracy:0.1892, Validation Loss:2.4275, Validation Accuracy:0.1806\n",
    "Epoch #249: Loss:2.4256, Accuracy:0.1909, Validation Loss:2.4276, Validation Accuracy:0.1827\n",
    "Epoch #250: Loss:2.4262, Accuracy:0.1876, Validation Loss:2.4286, Validation Accuracy:0.1802\n",
    "Epoch #251: Loss:2.4326, Accuracy:0.1841, Validation Loss:2.4276, Validation Accuracy:0.1786\n",
    "Epoch #252: Loss:2.4268, Accuracy:0.1870, Validation Loss:2.4272, Validation Accuracy:0.1810\n",
    "Epoch #253: Loss:2.4254, Accuracy:0.1919, Validation Loss:2.4270, Validation Accuracy:0.1835\n",
    "Epoch #254: Loss:2.4250, Accuracy:0.1934, Validation Loss:2.4283, Validation Accuracy:0.1831\n",
    "Epoch #255: Loss:2.4252, Accuracy:0.1917, Validation Loss:2.4284, Validation Accuracy:0.1798\n",
    "Epoch #256: Loss:2.4245, Accuracy:0.1909, Validation Loss:2.4284, Validation Accuracy:0.1814\n",
    "Epoch #257: Loss:2.4252, Accuracy:0.1915, Validation Loss:2.4287, Validation Accuracy:0.1798\n",
    "Epoch #258: Loss:2.4251, Accuracy:0.1926, Validation Loss:2.4276, Validation Accuracy:0.1827\n",
    "Epoch #259: Loss:2.4240, Accuracy:0.1939, Validation Loss:2.4304, Validation Accuracy:0.1798\n",
    "Epoch #260: Loss:2.4328, Accuracy:0.1836, Validation Loss:2.4297, Validation Accuracy:0.1798\n",
    "Epoch #261: Loss:2.4237, Accuracy:0.1949, Validation Loss:2.4303, Validation Accuracy:0.1806\n",
    "Epoch #262: Loss:2.4254, Accuracy:0.1884, Validation Loss:2.4277, Validation Accuracy:0.1802\n",
    "Epoch #263: Loss:2.4232, Accuracy:0.1937, Validation Loss:2.4285, Validation Accuracy:0.1827\n",
    "Epoch #264: Loss:2.4243, Accuracy:0.1914, Validation Loss:2.4284, Validation Accuracy:0.1831\n",
    "Epoch #265: Loss:2.4251, Accuracy:0.1901, Validation Loss:2.4279, Validation Accuracy:0.1806\n",
    "Epoch #266: Loss:2.4266, Accuracy:0.1902, Validation Loss:2.4309, Validation Accuracy:0.1778\n",
    "Epoch #267: Loss:2.4254, Accuracy:0.1899, Validation Loss:2.4311, Validation Accuracy:0.1814\n",
    "Epoch #268: Loss:2.4247, Accuracy:0.1880, Validation Loss:2.4285, Validation Accuracy:0.1798\n",
    "Epoch #269: Loss:2.4224, Accuracy:0.1906, Validation Loss:2.4293, Validation Accuracy:0.1794\n",
    "Epoch #270: Loss:2.4257, Accuracy:0.1902, Validation Loss:2.4284, Validation Accuracy:0.1839\n",
    "Epoch #271: Loss:2.4224, Accuracy:0.1913, Validation Loss:2.4287, Validation Accuracy:0.1827\n",
    "Epoch #272: Loss:2.4243, Accuracy:0.1896, Validation Loss:2.4290, Validation Accuracy:0.1851\n",
    "Epoch #273: Loss:2.4230, Accuracy:0.1938, Validation Loss:2.4314, Validation Accuracy:0.1847\n",
    "Epoch #274: Loss:2.4235, Accuracy:0.1936, Validation Loss:2.4290, Validation Accuracy:0.1802\n",
    "Epoch #275: Loss:2.4232, Accuracy:0.1905, Validation Loss:2.4303, Validation Accuracy:0.1810\n",
    "Epoch #276: Loss:2.4227, Accuracy:0.1874, Validation Loss:2.4294, Validation Accuracy:0.1843\n",
    "Epoch #277: Loss:2.4240, Accuracy:0.1902, Validation Loss:2.4294, Validation Accuracy:0.1802\n",
    "Epoch #278: Loss:2.4225, Accuracy:0.1944, Validation Loss:2.4278, Validation Accuracy:0.1802\n",
    "Epoch #279: Loss:2.4224, Accuracy:0.1937, Validation Loss:2.4279, Validation Accuracy:0.1806\n",
    "Epoch #280: Loss:2.4222, Accuracy:0.1921, Validation Loss:2.4281, Validation Accuracy:0.1810\n",
    "Epoch #281: Loss:2.4227, Accuracy:0.1937, Validation Loss:2.4280, Validation Accuracy:0.1802\n",
    "Epoch #282: Loss:2.4226, Accuracy:0.1907, Validation Loss:2.4282, Validation Accuracy:0.1835\n",
    "Epoch #283: Loss:2.4233, Accuracy:0.1885, Validation Loss:2.4290, Validation Accuracy:0.1814\n",
    "Epoch #284: Loss:2.4240, Accuracy:0.1862, Validation Loss:2.4267, Validation Accuracy:0.1810\n",
    "Epoch #285: Loss:2.4232, Accuracy:0.1927, Validation Loss:2.4280, Validation Accuracy:0.1794\n",
    "Epoch #286: Loss:2.4218, Accuracy:0.1934, Validation Loss:2.4278, Validation Accuracy:0.1810\n",
    "Epoch #287: Loss:2.4223, Accuracy:0.1905, Validation Loss:2.4269, Validation Accuracy:0.1798\n",
    "Epoch #288: Loss:2.4216, Accuracy:0.1948, Validation Loss:2.4273, Validation Accuracy:0.1794\n",
    "Epoch #289: Loss:2.4215, Accuracy:0.1910, Validation Loss:2.4286, Validation Accuracy:0.1798\n",
    "Epoch #290: Loss:2.4216, Accuracy:0.1920, Validation Loss:2.4289, Validation Accuracy:0.1782\n",
    "Epoch #291: Loss:2.4216, Accuracy:0.1936, Validation Loss:2.4283, Validation Accuracy:0.1786\n",
    "Epoch #292: Loss:2.4219, Accuracy:0.1872, Validation Loss:2.4284, Validation Accuracy:0.1831\n",
    "Epoch #293: Loss:2.4218, Accuracy:0.1890, Validation Loss:2.4280, Validation Accuracy:0.1778\n",
    "Epoch #294: Loss:2.4212, Accuracy:0.1879, Validation Loss:2.4283, Validation Accuracy:0.1794\n",
    "Epoch #295: Loss:2.4213, Accuracy:0.1929, Validation Loss:2.4275, Validation Accuracy:0.1778\n",
    "Epoch #296: Loss:2.4204, Accuracy:0.1934, Validation Loss:2.4265, Validation Accuracy:0.1802\n",
    "Epoch #297: Loss:2.4207, Accuracy:0.1927, Validation Loss:2.4261, Validation Accuracy:0.1782\n",
    "Epoch #298: Loss:2.4198, Accuracy:0.1920, Validation Loss:2.4271, Validation Accuracy:0.1794\n",
    "Epoch #299: Loss:2.4220, Accuracy:0.1935, Validation Loss:2.4276, Validation Accuracy:0.1819\n",
    "Epoch #300: Loss:2.4194, Accuracy:0.1935, Validation Loss:2.4279, Validation Accuracy:0.1790\n",
    "\n",
    "Test:\n",
    "Test Loss:2.42789388, Accuracy:0.1790\n",
    "Labels: ['yd', 'my', 'ib', 'aa', 'sk', 'eg', 'ek', 'ck', 'ce', 'mb', 'eb', 'eo', 'sg', 'by', 'ds']\n",
    "Confusion Matrix:\n",
    "       yd  my  ib  aa  sk   eg  ek  ck  ce  mb  eb  eo   sg  by  ds\n",
    "t:yd  124   0  16   0   0    4   0   0   0  16   2   0   87   0   0\n",
    "t:my   16   0   1   1   0   25   0   0   0   8   2   0   18   1   8\n",
    "t:ib  130   0  12   0   0   15   0   0   0   8   1   0   46   2   3\n",
    "t:aa    6   0   0   7   0   62   0   0   0   8   2   0   26   6  20\n",
    "t:sk    9   0   0   4   0   46   0   0   0   8   3   0   44   5  11\n",
    "t:eg    4   0   1   9   0  105   0   0   0  11   2   0   34   7  25\n",
    "t:ek   24   0   1   4   0   57   0   0   0  12   2   0   79   8   4\n",
    "t:ck    1   0   2   0   0   32   0   0   0   7   3   0   34   6   6\n",
    "t:ce   11   0   2   5   0   27   0   0   0   5   5   0   44   1   9\n",
    "t:mb   43   0   7   1   0   32   0   0   0  27   4   0   85   6   2\n",
    "t:eb   27   0   7   4   0   64   0   0   0  10  10   0   66   7   6\n",
    "t:eo    6   0   2   0   0   14   0   0   0   8   5   0   90  10   0\n",
    "t:sg   37   0   9   0   0   17   0   0   0  18   5   0  108   8   1\n",
    "t:by    6   0   3   0   0   38   0   0   0   9   2   0   83  15   6\n",
    "t:ds    2   0   1   1   0   50   0   0   0   6   4   0   28   6  28\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.28      0.50      0.36       249\n",
    "          my       0.00      0.00      0.00        80\n",
    "          ib       0.19      0.06      0.09       217\n",
    "          aa       0.19      0.05      0.08       137\n",
    "          sk       0.00      0.00      0.00       130\n",
    "          eg       0.18      0.53      0.27       198\n",
    "          ek       0.00      0.00      0.00       191\n",
    "          ck       0.00      0.00      0.00        91\n",
    "          ce       0.00      0.00      0.00       109\n",
    "          mb       0.17      0.13      0.15       207\n",
    "          eb       0.19      0.05      0.08       201\n",
    "          eo       0.00      0.00      0.00       135\n",
    "          sg       0.12      0.53      0.20       203\n",
    "          by       0.17      0.09      0.12       162\n",
    "          ds       0.22      0.22      0.22       126\n",
    "\n",
    "    accuracy                           0.18      2436\n",
    "   macro avg       0.11      0.14      0.10      2436\n",
    "weighted avg       0.13      0.18      0.13      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 19:28:06 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 11 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6563632515655167, 2.6246810425287004, 2.5633789698282876, 2.5033817032875096, 2.484777946190294, 2.4675462825349204, 2.4619455478461507, 2.454944840009968, 2.451386971622461, 2.4526590960366383, 2.4492850428927317, 2.5032895962005766, 2.452176406465728, 2.448456952528805, 2.4485566678697057, 2.443159223190082, 2.444517613631751, 2.441627071408803, 2.4441846145197674, 2.4417233333994797, 2.44138081946788, 2.441821658552574, 2.4378641282004874, 2.4392749914786305, 2.43997864848483, 2.4418952394588826, 2.4391539648835883, 2.4372062029313963, 2.4404264925344434, 2.439018305886555, 2.4399760896936424, 2.4359967082200575, 2.435846737452916, 2.4340075025417534, 2.4342412408349547, 2.433850758572909, 2.432894639389464, 2.4316223023951737, 2.4325535422671214, 2.4336615787155327, 2.4326115112586564, 2.4311013456635875, 2.432678419185194, 2.4327822808170163, 2.4322392137962803, 2.4311838001257486, 2.4308232223654813, 2.4338124001946158, 2.432723295512458, 2.430949884878199, 2.4312198561400615, 2.4311359390640885, 2.4307110337005264, 2.430843731061187, 2.4301808397170945, 2.4303199078257642, 2.4307576611711474, 2.4291282451798764, 2.4293339346429983, 2.432667921329367, 2.4305277685030733, 2.429203841095096, 2.429308685371637, 2.429154221060241, 2.4298060292680863, 2.42992954418577, 2.4299915565058514, 2.4297846142685864, 2.430461468563487, 2.429006013964197, 2.4288630000084686, 2.427810111069327, 2.4277908340072005, 2.4285600878334983, 2.428763035287215, 2.4285948429201625, 2.429031461721962, 2.430631341605351, 2.42840229075139, 2.4287414186693765, 2.4308489465165413, 2.4310380218455747, 2.498240021062015, 2.4431462651990317, 2.440284603726492, 2.4367079746547002, 2.4345881700124257, 2.437413001099635, 2.4367576530218518, 2.4365167891842194, 2.4383553965338347, 2.4369773919554962, 2.435051346451582, 2.4332562448160204, 2.4342466939259046, 2.435523913607417, 2.434359730953849, 2.433871227336439, 2.43162872756056, 2.4327802853827016, 2.4328001293251273, 2.4317425734108107, 2.433973931326655, 2.433179694443501, 2.4329161295554127, 2.433051347341052, 2.4324784936576056, 2.4330183124698834, 2.4362121583597216, 2.43881716555954, 2.4332366412496333, 2.432167957568991, 2.431255159315413, 2.4359903073271703, 2.4324559292378294, 2.43204282813863, 2.4322960560740703, 2.4335296157937134, 2.4323425175521174, 2.432734535245473, 2.4330141000168273, 2.4329888170771605, 2.432941616462369, 2.4343101633789113, 2.4380610451126725, 2.432544163295201, 2.4331342414486388, 2.4326031869659674, 2.432950262561416, 2.433195675926647, 2.4319163782060245, 2.433012531700197, 2.431520534070646, 2.433129403782987, 2.4325877390863075, 2.4316627384210845, 2.431662600224437, 2.432887623854263, 2.431915488736383, 2.436220710109216, 2.432835638425229, 2.432883796237764, 2.4317229166015224, 2.432669746856188, 2.4334496386924207, 2.4318172732010264, 2.4311566254971257, 2.4326294564652717, 2.432284708289286, 2.4321157450746433, 2.431346079790338, 2.431742641921897, 2.4310382226809297, 2.43267643627862, 2.43323768888201, 2.4328289353005794, 2.4358624593769194, 2.4337021040016014, 2.4329935971935, 2.4322633574944605, 2.4311555152260413, 2.4313508998388533, 2.4314682088266255, 2.4319315712244443, 2.432090295750911, 2.429840545153187, 2.431190660630149, 2.4303309118806435, 2.431064064279566, 2.4315587201924944, 2.431472249023237, 2.4308940616538766, 2.430141480685455, 2.4313051786720266, 2.4305289805620567, 2.429971166823689, 2.430148910027615, 2.4306575273253843, 2.4338309682648758, 2.430117516290574, 2.431536945020428, 2.432138515810661, 2.4321815916665863, 2.4320890449342274, 2.4308123525923304, 2.4320039666932205, 2.4296267028708374, 2.431296558411446, 2.431731986686318, 2.430781207061166, 2.4306502960781353, 2.431296631620435, 2.430627522210182, 2.4302239613775747, 2.4308745524370416, 2.4328581163252907, 2.430543378068896, 2.431791214715867, 2.4319407517099614, 2.4367715196656476, 2.4316579034958763, 2.4310640219984383, 2.4301117460911694, 2.4298631201432452, 2.4292411283514967, 2.429990686610806, 2.433322442184724, 2.4313880763030404, 2.429760013699336, 2.430147974361927, 2.4295419605298973, 2.431009926427957, 2.4298324310916595, 2.4295315456703572, 2.429122575593895, 2.4288775032181262, 2.42909178671187, 2.4311315159883797, 2.429588701533175, 2.4287775134413896, 2.4287413912649423, 2.429548203651541, 2.429872532391979, 2.428776638848441, 2.4287037363976287, 2.4285170393820077, 2.4288130781333437, 2.4290375646894984, 2.428599406932962, 2.4305012276998688, 2.4293951651537165, 2.4280922620363032, 2.430589137210439, 2.4280725484606864, 2.428537593490776, 2.429173250699474, 2.4278655169632635, 2.42894755090986, 2.4281272097369917, 2.42808440792541, 2.4281051522992514, 2.4288115282168334, 2.4283250341274467, 2.427945696465879, 2.4279429877332865, 2.4274973607024144, 2.4274450444627083, 2.427454930025173, 2.427614977794328, 2.4285999424939084, 2.427550097795934, 2.4272164722968794, 2.4269863854488127, 2.4283176729048805, 2.42841111497926, 2.4283536373100842, 2.42873093921367, 2.4275513114208853, 2.4303937694317796, 2.429738203292997, 2.430319899995926, 2.427732883024294, 2.428474085671561, 2.428417172142242, 2.427936106871306, 2.4308701853446775, 2.4310689199538458, 2.4285330283035003, 2.429266402678341, 2.4284491730832505, 2.4286891513661604, 2.4289815891748185, 2.431430931749015, 2.4289997109442902, 2.430308838391735, 2.4293801580939585, 2.4294162661766965, 2.4277641252539626, 2.427949513903588, 2.4280614402689564, 2.4280486510109234, 2.4282329564024074, 2.4289996158117537, 2.4266675023609783, 2.4280471265413883, 2.427811743982124, 2.4269428910880255, 2.4272726016678834, 2.428570666728153, 2.428910366224342, 2.428331194252803, 2.428365315905541, 2.4279876550038657, 2.428333284036671, 2.427532555825996, 2.4264951366900616, 2.4261386876035793, 2.4270663805587342, 2.427554775732883, 2.427893953370343], 'val_acc': [0.08908045941532539, 0.09770114919283902, 0.12889983484212597, 0.15229884969385582, 0.1613300486493776, 0.17241379249174216, 0.17692939195726892, 0.17528735578353768, 0.17446633769667208, 0.16584564796809492, 0.1773399009884676, 0.1613300485270364, 0.1691297202176844, 0.17939244619339753, 0.1740558287144099, 0.17980295517565972, 0.1740558287144099, 0.17118226547155083, 0.169950738402423, 0.17528735580800595, 0.17487684677680726, 0.17241379258961514, 0.17487684682574373, 0.16912972029108916, 0.17528735585694244, 0.1691297203155574, 0.17569786488814113, 0.1683087021797553, 0.17528735578353768, 0.1740558287144099, 0.1683087021308188, 0.17282430157187734, 0.17282430159634557, 0.1683087021308188, 0.17364531958533821, 0.17323481062754426, 0.17036124736021696, 0.17323481065201252, 0.17610837387040332, 0.17816091905086498, 0.16748768409288967, 0.17816091905086498, 0.16954022932228785, 0.17200328350947997, 0.1711822653981461, 0.17282430152294084, 0.17405582864100513, 0.174876846752339, 0.17651888292607024, 0.17569786483920463, 0.17159277442934479, 0.1720032835339482, 0.18267651846745528, 0.1744663377211403, 0.17118226542261433, 0.17036124736021696, 0.16995073830455004, 0.17528735585694244, 0.17857142815546842, 0.17610837382146682, 0.169950738402423, 0.17569786481473637, 0.17241379256514688, 0.17692939198173718, 0.17857142808206367, 0.17651888292607024, 0.16995073832901827, 0.16789819312408835, 0.18062397326252536, 0.17857142813100016, 0.17528735583247418, 0.17569786488814113, 0.17118226542261433, 0.17857142817993665, 0.17487684682574373, 0.17980295522459622, 0.18308702749865396, 0.16954022927335136, 0.17282430162081383, 0.17569786483920463, 0.174876846850212, 0.17692939200620542, 0.1527093587250545, 0.17159277452721775, 0.17077175644035214, 0.17528735580800595, 0.17241379258961514, 0.1703612474336217, 0.17364531973214767, 0.17241379261408338, 0.17118226552048732, 0.17405582876334635, 0.16954022934675608, 0.1703612474336217, 0.17036124740915345, 0.17077175644035214, 0.17118226547155083, 0.17446633779454504, 0.17282430169421856, 0.1744663378190133, 0.1724137926385516, 0.17077175651375687, 0.17610837384593506, 0.1728243016697503, 0.1728243016697503, 0.17528735588141067, 0.1740558288367511, 0.17446633784348153, 0.1740558286654734, 0.18021346428026316, 0.17446633784348153, 0.17528735593034717, 0.17241379266301987, 0.1761083739193398, 0.1781609191242697, 0.17241379266301987, 0.17241379266301987, 0.1715927746006225, 0.1756978649370776, 0.17364531983002066, 0.17241379266301987, 0.17241379266301987, 0.17323481074988548, 0.17692939203067365, 0.17692939203067365, 0.1744663378190133, 0.17692939200620542, 0.17323481070094898, 0.17733990106187233, 0.1744663378190133, 0.17405582881228285, 0.1744663378190133, 0.17241379266301987, 0.17775041011753928, 0.17405582878781461, 0.17857142815546842, 0.17775041011753928, 0.17733990106187233, 0.1773399010374041, 0.17651888292607024, 0.17692939195726892, 0.1744663377700768, 0.17610837394380804, 0.1781609191242697, 0.17569786488814113, 0.1752873559058789, 0.17733990106187233, 0.17857142817993665, 0.1777504100686028, 0.17692939203067365, 0.17241379261408338, 0.1752873559058789, 0.17692939200620542, 0.17405582873887812, 0.17692939195726892, 0.17775041014200751, 0.17528735588141067, 0.17651888297500673, 0.1752873559058789, 0.17487684682574373, 0.17857142815546842, 0.18062397331146185, 0.17775041009307102, 0.17857142815546842, 0.1847290637213217, 0.18062397336039832, 0.18062397333593008, 0.17898193721113534, 0.17898193721113534, 0.17816091914873797, 0.1793924462178658, 0.17898193721113534, 0.1822660094362566, 0.17980295524906448, 0.1773399010863406, 0.18185550047846263, 0.18349753657878914, 0.18021346432919963, 0.17816091914873797, 0.1761083739193398, 0.17610837394380804, 0.18349753665219387, 0.18226600953412955, 0.1826765185897965, 0.17816091905086498, 0.17816091914873797, 0.17898193721113534, 0.18103448234266054, 0.18103448236712877, 0.18062397331146185, 0.18390804556105134, 0.17980295529800094, 0.18021346415792192, 0.17898193721113534, 0.18185550042952614, 0.1781609191732062, 0.18062397331146185, 0.17692939203067365, 0.17898193721113534, 0.1798029553224692, 0.17898193716219885, 0.18390804558551957, 0.17775041009307102, 0.18062397331146185, 0.18103448234266054, 0.18103448236712877, 0.18144499139832745, 0.18185550050293087, 0.18349753652985265, 0.18431855459225002, 0.18226600946072483, 0.18513957275252038, 0.18226600946072483, 0.18267651854086, 0.18390804556105134, 0.18349753652985265, 0.17939244619339753, 0.1798029552735327, 0.18144499134939096, 0.18185550042952614, 0.17898193723560357, 0.17898193723560357, 0.17816091900192849, 0.17980295529800094, 0.17775041011753928, 0.17857142817993665, 0.18144499144726395, 0.18267651851639177, 0.1830870275720587, 0.18103448241606526, 0.1781609191732062, 0.18390804556105134, 0.1822660094851931, 0.18472906364791694, 0.18267651854086, 0.17898193726007183, 0.18431855461671826, 0.18103448236712877, 0.18308702759652692, 0.18431855461671826, 0.1830870275720587, 0.17939244629127052, 0.1830870275720587, 0.17939244631573875, 0.18390804558551957, 0.18267651856532824, 0.18062397331146185, 0.18062397331146185, 0.18267651846745528, 0.18021346428026316, 0.17857142815546842, 0.181034482391597, 0.18349753657878914, 0.1830870275720587, 0.1798029553224692, 0.18144499152066867, 0.1798029553224692, 0.18267651856532824, 0.17980295534693744, 0.1798029553224692, 0.18062397331146185, 0.18021346437813612, 0.18267651856532824, 0.1830870275720587, 0.1806239734582713, 0.17775041004413455, 0.18144499147173218, 0.1798029553224692, 0.17939244629127052, 0.18390804560998783, 0.18267651856532824, 0.1851395727035839, 0.18472906369685343, 0.18021346437813612, 0.18103448248947, 0.18431855466565475, 0.18021346437813612, 0.18021346437813612, 0.18062397343380307, 0.181034482391597, 0.18021346437813612, 0.1834975366277256, 0.18144499149620044, 0.18103448236712877, 0.17939244629127052, 0.18103448236712877, 0.17980295522459622, 0.17939244629127052, 0.1798029552735327, 0.17816091919767446, 0.1785714282044049, 0.18308702754759046, 0.17775041014200751, 0.17939244631573875, 0.17775041016647578, 0.1802134643536679, 0.1781609192221427, 0.179392446340207, 0.1818555004539944, 0.17898193730900833], 'loss': [2.6745522670432527, 2.6449600485071265, 2.6022858225099847, 2.5418427210813674, 2.5069836939629586, 2.4860765839993832, 2.4752653364772916, 2.468583983023798, 2.4621317293609684, 2.461190393181552, 2.460182656399768, 2.4642174739367664, 2.486977197893836, 2.463709917107647, 2.4566230073846587, 2.459367861052558, 2.4556810697246134, 2.4530747069960013, 2.4479558138876727, 2.4508920062493984, 2.4506110676994557, 2.449521908867775, 2.453188586871482, 2.4494052456144924, 2.4474432264754906, 2.447445292149726, 2.4480975169665515, 2.446956229552596, 2.4467970369288072, 2.446047680089116, 2.446687461316463, 2.448671820031544, 2.448082332199849, 2.4476347178159554, 2.445180217049695, 2.4440020967557934, 2.443550147853593, 2.4481609717531616, 2.443093334820726, 2.4433396486286267, 2.441499736029999, 2.441914636839097, 2.4409350199865854, 2.442544187216788, 2.444727935242702, 2.443930838289202, 2.442947267751674, 2.4413748064569867, 2.4418593162873443, 2.441824931432579, 2.4396238617828496, 2.4412001191712993, 2.44090040146203, 2.4388332257280605, 2.4404307390874904, 2.4413183691075697, 2.4389691676936844, 2.4385409227876447, 2.4400347378709233, 2.4415465237423626, 2.44262506653396, 2.4383478794254563, 2.438707422475795, 2.437737549110115, 2.4382845398826523, 2.438315022505774, 2.4375632847114264, 2.437414628328484, 2.439595656228506, 2.4371648087393822, 2.439276260027406, 2.437753015768846, 2.4373798492752794, 2.4374875159723803, 2.43690294220707, 2.4380586971737275, 2.437157036097877, 2.4377894211598736, 2.4369781463787543, 2.4369977148161777, 2.4403543457603063, 2.4405808257860815, 2.452411595409166, 2.471025775737097, 2.448134695480002, 2.442733968993232, 2.4408447970599854, 2.4406697204715173, 2.4413873696963644, 2.442124836841403, 2.44330421492794, 2.4445610860045197, 2.442007464990479, 2.4407348479088817, 2.4408732941997613, 2.4406696327413133, 2.4434824896544156, 2.44026515802074, 2.4390043214606063, 2.4389851114833134, 2.4391409107302247, 2.4394370722330083, 2.438817894385336, 2.4401770651462877, 2.439130212540989, 2.4391116915787023, 2.4396107590418823, 2.440876403334694, 2.4391146406255952, 2.443085604379799, 2.4424539959650997, 2.4396135330200197, 2.437965199981627, 2.4384274417125225, 2.443828633482696, 2.438467710659489, 2.4377088593751255, 2.437867812013724, 2.4400663905564763, 2.4392496579971157, 2.437022696873001, 2.4374596541911915, 2.4367630181126527, 2.4370509578462007, 2.4400936850287342, 2.441763904598949, 2.4370141869452944, 2.436430169424727, 2.4369673691735865, 2.4376212126910075, 2.4364611820518602, 2.4367191575146308, 2.4364765795838905, 2.4369527587655635, 2.440573441957791, 2.4359005553032094, 2.4363349696204404, 2.437387958150625, 2.436924944570177, 2.438291404085727, 2.438885881376952, 2.435946220930597, 2.4361984244607067, 2.4354653062761686, 2.436382615835515, 2.436076412063849, 2.435584492849863, 2.439949858506847, 2.4366527635703585, 2.435269244152907, 2.4360648857249863, 2.4349248130218695, 2.4361468810565174, 2.434811439357499, 2.438014966408575, 2.439492693965685, 2.437864377611227, 2.434726765904828, 2.4350160482238206, 2.4353317978445754, 2.4349899522088148, 2.4343610420853694, 2.4348649045525144, 2.4356359900879907, 2.43441907679031, 2.4337517329065217, 2.4333350609460163, 2.4339801916596335, 2.4341930725246486, 2.433681733309611, 2.4337719593204756, 2.435732847562316, 2.4332496963242485, 2.436828621016391, 2.435016683092842, 2.4362366787951584, 2.433172110512516, 2.4331921281755826, 2.4354973690220953, 2.434498859186192, 2.4350644344666654, 2.4348405963341557, 2.4336506392187163, 2.4386310882881683, 2.433904103527813, 2.4323840079611085, 2.433744221791105, 2.435101625317176, 2.4326573808579965, 2.4326553422077972, 2.431496950243533, 2.4325932086615594, 2.4321577247408137, 2.4317789817982387, 2.4313434061328487, 2.4328568158942816, 2.432628812241603, 2.4325778413847, 2.4330752490237506, 2.432096991548793, 2.4334316481799805, 2.43368815190983, 2.430366446203275, 2.4318885622083286, 2.4304281443296274, 2.4330920698216807, 2.4351230034837976, 2.4312427545230246, 2.4326644538119586, 2.4294873487288458, 2.4297558944083337, 2.429759248130375, 2.4290173492392477, 2.4287459982494064, 2.428336733714266, 2.429959955548359, 2.430253385371496, 2.4293393614845353, 2.429205394427634, 2.431830016004966, 2.428254992565335, 2.4283672876181788, 2.427520127854553, 2.4279962077522668, 2.4284060190345715, 2.4292356355968687, 2.4298543201579696, 2.4276928279923706, 2.430027748182324, 2.429340864451759, 2.427308052031656, 2.4297092635528754, 2.42695963554069, 2.4298726480354764, 2.4264347183141375, 2.4265460534262218, 2.4282582206647745, 2.4274792552728672, 2.427318062772496, 2.4274024445418214, 2.427145717178282, 2.4250871901639433, 2.4270355522265423, 2.42716997878997, 2.4250252631655465, 2.424781735672843, 2.425364584110111, 2.4258257028748123, 2.4256157929892415, 2.4262208064233985, 2.432567552668358, 2.426807680658736, 2.4254487483163634, 2.425032800619607, 2.425193417782167, 2.4245262442673012, 2.425229257724613, 2.425122419568792, 2.4239751268461256, 2.432814818975617, 2.4236958066051257, 2.4254208894725697, 2.4231768941977188, 2.4242770880399545, 2.4250669890605447, 2.426604376093808, 2.4253900028596913, 2.4246825589291614, 2.4223928718841052, 2.4256881710929794, 2.4224160060255926, 2.424277801141602, 2.4230085907530734, 2.4235052460272946, 2.4231646117733243, 2.422749654566238, 2.423980552704672, 2.4224790122719515, 2.42241673547874, 2.4221505069145186, 2.422711111241053, 2.422567980392268, 2.4232776092552797, 2.423999858734789, 2.4231510764519535, 2.4217907113461035, 2.42233871542208, 2.421607994690568, 2.4214650863984284, 2.421592547124906, 2.421621927392556, 2.4219115285658006, 2.4217836185157666, 2.421195770925565, 2.421303321301815, 2.4203897310723024, 2.4207135643068036, 2.419781144884333, 2.421984691443629, 2.419435038458885], 'acc': [0.08860369610162241, 0.09055441478439424, 0.11427104723098587, 0.1506160164332243, 0.15359342916117066, 0.16868583162829617, 0.17166324435930233, 0.17484599589934338, 0.17340862423609904, 0.1728952772073922, 0.17453798768273124, 0.17659137577308032, 0.16991786447944582, 0.17381930184804928, 0.17135523612739123, 0.1746406570903085, 0.17207392197431234, 0.17453798769191062, 0.17556468173096557, 0.1735112936406165, 0.17340862423303927, 0.17217659137882982, 0.175564681724846, 0.17392197125256673, 0.1777207392074734, 0.1756673511171243, 0.17885010266940451, 0.17515400410983598, 0.17864476386648917, 0.18172484600813238, 0.17710472279872738, 0.17289527721045198, 0.1748459958962836, 0.1794661191026288, 0.17628336755952795, 0.17689938398663269, 0.1784394250543945, 0.17843942505745428, 0.17648870635326394, 0.18357289527720738, 0.17967145790554415, 0.18141683779458, 0.17731006158940357, 0.17607802875355283, 0.17597535935515496, 0.17874743327100664, 0.17946611909650925, 0.17915811088295688, 0.18131416839006256, 0.18408624230285445, 0.18316221766219737, 0.18182751540347047, 0.18059548254926103, 0.17987679671763884, 0.18213552361396304, 0.17946611909956903, 0.18305954825462012, 0.18203285419720644, 0.17936344969199178, 0.17802874743326488, 0.17885010266940451, 0.17720739220018505, 0.1784394250513347, 0.17638603696098562, 0.1833675564681725, 0.180287474344888, 0.1806981519507187, 0.18069815195683825, 0.18326488706977462, 0.18100616017039062, 0.18172484599589322, 0.1811088295687885, 0.17874743326488707, 0.17802874743632469, 0.18151950719909746, 0.17895277208004154, 0.18357289528944654, 0.182854209448645, 0.18090349076587317, 0.18059548254620122, 0.1801848049281314, 0.18090349076281337, 0.17741273100616017, 0.1693018480492813, 0.18254620123815243, 0.18121149897942554, 0.1806981519507187, 0.17997946611909652, 0.17936344969811135, 0.1779260780318072, 0.17761806981825487, 0.18182751540653025, 0.1786447638634294, 0.17833675565293683, 0.17813141683778233, 0.17926078029053413, 0.18141683779458, 0.18234086242299793, 0.17772073923195167, 0.18049281314780335, 0.18059548254926103, 0.17813141684084213, 0.17782340863646912, 0.1803901437402261, 0.18151950719297788, 0.18193018480492812, 0.18018480491589228, 0.17874743326794684, 0.18203285421556517, 0.17628336756258775, 0.1844969199178645, 0.18008213552667374, 0.18028747433264888, 0.18080082135523615, 0.18285420944558523, 0.18182751540653025, 0.18295687885622225, 0.18316221765913757, 0.18223819302460006, 0.1817248460020128, 0.18213552361396304, 0.18213552361396304, 0.1817248460020128, 0.18305954825767992, 0.18295687886234183, 0.18234086242299793, 0.18254620123815243, 0.18275154004106775, 0.18357289528332696, 0.1809034907597536, 0.18285420945782435, 0.18357289528332696, 0.18121149897942554, 0.18141683779458, 0.1826488706396101, 0.1853182751570639, 0.18285420944558523, 0.18562628336755646, 0.18275154004718733, 0.181314168383943, 0.18326488706365504, 0.1839835728952772, 0.18377823409848146, 0.18285420945782435, 0.18439425051640682, 0.1830595482607397, 0.1828542094517048, 0.184907597511456, 0.18193018480492812, 0.18336755647429206, 0.18542094456158134, 0.18234086242605774, 0.18377823408930208, 0.18388090349075975, 0.18028747433876843, 0.1837782340862423, 0.1826488706365503, 0.18511293635108877, 0.18675564682336804, 0.18357289528332696, 0.18326488707589417, 0.18408624229979467, 0.18521560574948664, 0.1857289527751337, 0.1855236139691586, 0.18388090349687933, 0.185523613963039, 0.18429158110882957, 0.18716632443531828, 0.18696098562934316, 0.18542094457076072, 0.18542094456158134, 0.1866529774188506, 0.18367556468172486, 0.18511293635108877, 0.18685831622788548, 0.1845995893254417, 0.1866529774188506, 0.18018480493425099, 0.18429158110882957, 0.1861396303962633, 0.18603696099174585, 0.18624229979772097, 0.18213552361396304, 0.18162217659749536, 0.18593429158110883, 0.18624229980078078, 0.18603696098562628, 0.18726899384289553, 0.18716632443837805, 0.18880903491371712, 0.18470225873301896, 0.18757700205950767, 0.18511293634802897, 0.18716632443531828, 0.1870636550369204, 0.18449691993010361, 0.18613963039014375, 0.18418891170431212, 0.1834702258788095, 0.18470225873301896, 0.18470225873301896, 0.18778234087466214, 0.18942505133470225, 0.18850102669404517, 0.18593429158110883, 0.18542094456158134, 0.18726899385207488, 0.18593429158110883, 0.18767967146096534, 0.1868583162248257, 0.18809034907903516, 0.1887063655030801, 0.18798767967757748, 0.18963039014985675, 0.18624229980690032, 0.18870636550919964, 0.18963039014985675, 0.18942505134082183, 0.18583162217965116, 0.1892197125379065, 0.18819301849273196, 0.19086242299794662, 0.19096509241470322, 0.18973305954825462, 0.18675564681724846, 0.1865503080204527, 0.18726899383983572, 0.18480492814365598, 0.1870636550338606, 0.18809034907597535, 0.18624229980078078, 0.188911704312115, 0.1870636550369204, 0.1904517453798768, 0.19199178645987774, 0.18634496919917864, 0.1889117043151748, 0.18634496920223842, 0.18963039014985675, 0.18747433265499022, 0.19106776181922067, 0.189014373722752, 0.19055441478439425, 0.1903490759814789, 0.19055441479051383, 0.18963039014373717, 0.1892197125379065, 0.19086242301018577, 0.18757700205950767, 0.1840862423120338, 0.1869609856385225, 0.19188911704312114, 0.19342915811700254, 0.19168377824632538, 0.1908624230010064, 0.1914784394311709, 0.19260780288698248, 0.19394250513347022, 0.18357289526496826, 0.19486652976188817, 0.18839835728952772, 0.19373716633667445, 0.19137577002359366, 0.19014373716632443, 0.19024640657084188, 0.18993839835728954, 0.18798767967757748, 0.1905544147721551, 0.19024640658308103, 0.19127310061601643, 0.18963039014985675, 0.19383983573201255, 0.19363449692297766, 0.1904517453798768, 0.18737166325659232, 0.19024640657084188, 0.19435318275154004, 0.19373716633667445, 0.19209445585827564, 0.1937371663274951, 0.1906570841919715, 0.18850102669710497, 0.18624229980078078, 0.19271047227926077, 0.1934291581231221, 0.1904517453798768, 0.19476386036960985, 0.19096509241470322, 0.1919917864506984, 0.19363449690767873, 0.18716632443837805, 0.18901437371663243, 0.1878850102791796, 0.19291581109135547, 0.19342915811394273, 0.19271047228538035, 0.1919917864476386, 0.19353182752763956, 0.19353182752151998]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
