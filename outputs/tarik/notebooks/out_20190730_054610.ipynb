{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf4.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 05:46:10 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '1', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eg', 'ce', 'ib', 'mb', 'aa', 'sg', 'sk', 'my', 'eb', 'ek', 'eo', 'ds', 'ck', 'yd', 'by'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000021B1AD19F98>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000021B783C6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7028, Accuracy:0.0690, Validation Loss:2.6970, Validation Accuracy:0.0887\n",
    "Epoch #2: Loss:2.6938, Accuracy:0.0891, Validation Loss:2.6877, Validation Accuracy:0.0903\n",
    "Epoch #3: Loss:2.6847, Accuracy:0.0903, Validation Loss:2.6799, Validation Accuracy:0.1018\n",
    "Epoch #4: Loss:2.6772, Accuracy:0.0998, Validation Loss:2.6730, Validation Accuracy:0.1166\n",
    "Epoch #5: Loss:2.6710, Accuracy:0.1047, Validation Loss:2.6675, Validation Accuracy:0.1117\n",
    "Epoch #6: Loss:2.6662, Accuracy:0.1064, Validation Loss:2.6632, Validation Accuracy:0.1034\n",
    "Epoch #7: Loss:2.6616, Accuracy:0.1027, Validation Loss:2.6588, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6568, Accuracy:0.1051, Validation Loss:2.6539, Validation Accuracy:0.1199\n",
    "Epoch #9: Loss:2.6512, Accuracy:0.1211, Validation Loss:2.6478, Validation Accuracy:0.1248\n",
    "Epoch #10: Loss:2.6436, Accuracy:0.1183, Validation Loss:2.6400, Validation Accuracy:0.1215\n",
    "Epoch #11: Loss:2.6350, Accuracy:0.1166, Validation Loss:2.6311, Validation Accuracy:0.1215\n",
    "Epoch #12: Loss:2.6251, Accuracy:0.1203, Validation Loss:2.6208, Validation Accuracy:0.1248\n",
    "Epoch #13: Loss:2.6149, Accuracy:0.1261, Validation Loss:2.6123, Validation Accuracy:0.1412\n",
    "Epoch #14: Loss:2.6061, Accuracy:0.1425, Validation Loss:2.5980, Validation Accuracy:0.1396\n",
    "Epoch #15: Loss:2.5911, Accuracy:0.1429, Validation Loss:2.5863, Validation Accuracy:0.1560\n",
    "Epoch #16: Loss:2.5789, Accuracy:0.1524, Validation Loss:2.5712, Validation Accuracy:0.1445\n",
    "Epoch #17: Loss:2.5630, Accuracy:0.1499, Validation Loss:2.5571, Validation Accuracy:0.1675\n",
    "Epoch #18: Loss:2.5493, Accuracy:0.1626, Validation Loss:2.5443, Validation Accuracy:0.1691\n",
    "Epoch #19: Loss:2.5390, Accuracy:0.1610, Validation Loss:2.5432, Validation Accuracy:0.1609\n",
    "Epoch #20: Loss:2.5310, Accuracy:0.1573, Validation Loss:2.5243, Validation Accuracy:0.1658\n",
    "Epoch #21: Loss:2.5186, Accuracy:0.1639, Validation Loss:2.5162, Validation Accuracy:0.1741\n",
    "Epoch #22: Loss:2.5105, Accuracy:0.1659, Validation Loss:2.5051, Validation Accuracy:0.1658\n",
    "Epoch #23: Loss:2.5007, Accuracy:0.1630, Validation Loss:2.5003, Validation Accuracy:0.1708\n",
    "Epoch #24: Loss:2.4949, Accuracy:0.1667, Validation Loss:2.4934, Validation Accuracy:0.1691\n",
    "Epoch #25: Loss:2.4884, Accuracy:0.1700, Validation Loss:2.4894, Validation Accuracy:0.1691\n",
    "Epoch #26: Loss:2.4824, Accuracy:0.1634, Validation Loss:2.4849, Validation Accuracy:0.1724\n",
    "Epoch #27: Loss:2.4791, Accuracy:0.1614, Validation Loss:2.4811, Validation Accuracy:0.1741\n",
    "Epoch #28: Loss:2.4763, Accuracy:0.1643, Validation Loss:2.4782, Validation Accuracy:0.1790\n",
    "Epoch #29: Loss:2.4726, Accuracy:0.1717, Validation Loss:2.4764, Validation Accuracy:0.1724\n",
    "Epoch #30: Loss:2.4702, Accuracy:0.1655, Validation Loss:2.4742, Validation Accuracy:0.1806\n",
    "Epoch #31: Loss:2.4662, Accuracy:0.1704, Validation Loss:2.4726, Validation Accuracy:0.1773\n",
    "Epoch #32: Loss:2.4644, Accuracy:0.1733, Validation Loss:2.4713, Validation Accuracy:0.1888\n",
    "Epoch #33: Loss:2.4649, Accuracy:0.1741, Validation Loss:2.4715, Validation Accuracy:0.1823\n",
    "Epoch #34: Loss:2.4643, Accuracy:0.1700, Validation Loss:2.4683, Validation Accuracy:0.1872\n",
    "Epoch #35: Loss:2.4581, Accuracy:0.1704, Validation Loss:2.4682, Validation Accuracy:0.1823\n",
    "Epoch #36: Loss:2.4567, Accuracy:0.1676, Validation Loss:2.4661, Validation Accuracy:0.1856\n",
    "Epoch #37: Loss:2.4548, Accuracy:0.1762, Validation Loss:2.4650, Validation Accuracy:0.1839\n",
    "Epoch #38: Loss:2.4533, Accuracy:0.1733, Validation Loss:2.4635, Validation Accuracy:0.1856\n",
    "Epoch #39: Loss:2.4522, Accuracy:0.1713, Validation Loss:2.4626, Validation Accuracy:0.1888\n",
    "Epoch #40: Loss:2.4497, Accuracy:0.1725, Validation Loss:2.4628, Validation Accuracy:0.1839\n",
    "Epoch #41: Loss:2.4501, Accuracy:0.1729, Validation Loss:2.4614, Validation Accuracy:0.1888\n",
    "Epoch #42: Loss:2.4488, Accuracy:0.1749, Validation Loss:2.4625, Validation Accuracy:0.1856\n",
    "Epoch #43: Loss:2.4478, Accuracy:0.1737, Validation Loss:2.4616, Validation Accuracy:0.1839\n",
    "Epoch #44: Loss:2.4458, Accuracy:0.1725, Validation Loss:2.4598, Validation Accuracy:0.1872\n",
    "Epoch #45: Loss:2.4450, Accuracy:0.1766, Validation Loss:2.4590, Validation Accuracy:0.1872\n",
    "Epoch #46: Loss:2.4446, Accuracy:0.1692, Validation Loss:2.4578, Validation Accuracy:0.1856\n",
    "Epoch #47: Loss:2.4428, Accuracy:0.1725, Validation Loss:2.4588, Validation Accuracy:0.1856\n",
    "Epoch #48: Loss:2.4413, Accuracy:0.1721, Validation Loss:2.4575, Validation Accuracy:0.1921\n",
    "Epoch #49: Loss:2.4401, Accuracy:0.1725, Validation Loss:2.4605, Validation Accuracy:0.1856\n",
    "Epoch #50: Loss:2.4422, Accuracy:0.1737, Validation Loss:2.4591, Validation Accuracy:0.1921\n",
    "Epoch #51: Loss:2.4385, Accuracy:0.1733, Validation Loss:2.4582, Validation Accuracy:0.1905\n",
    "Epoch #52: Loss:2.4375, Accuracy:0.1758, Validation Loss:2.4569, Validation Accuracy:0.1921\n",
    "Epoch #53: Loss:2.4366, Accuracy:0.1762, Validation Loss:2.4575, Validation Accuracy:0.1856\n",
    "Epoch #54: Loss:2.4384, Accuracy:0.1729, Validation Loss:2.4573, Validation Accuracy:0.1921\n",
    "Epoch #55: Loss:2.4361, Accuracy:0.1708, Validation Loss:2.4590, Validation Accuracy:0.1790\n",
    "Epoch #56: Loss:2.4361, Accuracy:0.1754, Validation Loss:2.4561, Validation Accuracy:0.1888\n",
    "Epoch #57: Loss:2.4332, Accuracy:0.1733, Validation Loss:2.4570, Validation Accuracy:0.1823\n",
    "Epoch #58: Loss:2.4327, Accuracy:0.1745, Validation Loss:2.4569, Validation Accuracy:0.1905\n",
    "Epoch #59: Loss:2.4320, Accuracy:0.1762, Validation Loss:2.4572, Validation Accuracy:0.1888\n",
    "Epoch #60: Loss:2.4313, Accuracy:0.1766, Validation Loss:2.4574, Validation Accuracy:0.1806\n",
    "Epoch #61: Loss:2.4322, Accuracy:0.1758, Validation Loss:2.4568, Validation Accuracy:0.1823\n",
    "Epoch #62: Loss:2.4302, Accuracy:0.1770, Validation Loss:2.4573, Validation Accuracy:0.1823\n",
    "Epoch #63: Loss:2.4326, Accuracy:0.1786, Validation Loss:2.4596, Validation Accuracy:0.1921\n",
    "Epoch #64: Loss:2.4332, Accuracy:0.1774, Validation Loss:2.4572, Validation Accuracy:0.1921\n",
    "Epoch #65: Loss:2.4302, Accuracy:0.1811, Validation Loss:2.4568, Validation Accuracy:0.1790\n",
    "Epoch #66: Loss:2.4278, Accuracy:0.1803, Validation Loss:2.4583, Validation Accuracy:0.1806\n",
    "Epoch #67: Loss:2.4283, Accuracy:0.1791, Validation Loss:2.4601, Validation Accuracy:0.1872\n",
    "Epoch #68: Loss:2.4277, Accuracy:0.1832, Validation Loss:2.4577, Validation Accuracy:0.1888\n",
    "Epoch #69: Loss:2.4260, Accuracy:0.1770, Validation Loss:2.4582, Validation Accuracy:0.1823\n",
    "Epoch #70: Loss:2.4253, Accuracy:0.1807, Validation Loss:2.4572, Validation Accuracy:0.1790\n",
    "Epoch #71: Loss:2.4238, Accuracy:0.1782, Validation Loss:2.4593, Validation Accuracy:0.1806\n",
    "Epoch #72: Loss:2.4256, Accuracy:0.1836, Validation Loss:2.4586, Validation Accuracy:0.1905\n",
    "Epoch #73: Loss:2.4218, Accuracy:0.1823, Validation Loss:2.4609, Validation Accuracy:0.1773\n",
    "Epoch #74: Loss:2.4251, Accuracy:0.1786, Validation Loss:2.4589, Validation Accuracy:0.1823\n",
    "Epoch #75: Loss:2.4229, Accuracy:0.1840, Validation Loss:2.4570, Validation Accuracy:0.1757\n",
    "Epoch #76: Loss:2.4189, Accuracy:0.1840, Validation Loss:2.4556, Validation Accuracy:0.1823\n",
    "Epoch #77: Loss:2.4175, Accuracy:0.1848, Validation Loss:2.4557, Validation Accuracy:0.1823\n",
    "Epoch #78: Loss:2.4169, Accuracy:0.1840, Validation Loss:2.4561, Validation Accuracy:0.1823\n",
    "Epoch #79: Loss:2.4149, Accuracy:0.1852, Validation Loss:2.4579, Validation Accuracy:0.1888\n",
    "Epoch #80: Loss:2.4142, Accuracy:0.1856, Validation Loss:2.4574, Validation Accuracy:0.1806\n",
    "Epoch #81: Loss:2.4123, Accuracy:0.1881, Validation Loss:2.4573, Validation Accuracy:0.1888\n",
    "Epoch #82: Loss:2.4117, Accuracy:0.1897, Validation Loss:2.4583, Validation Accuracy:0.1823\n",
    "Epoch #83: Loss:2.4101, Accuracy:0.1844, Validation Loss:2.4617, Validation Accuracy:0.1806\n",
    "Epoch #84: Loss:2.4107, Accuracy:0.1864, Validation Loss:2.4590, Validation Accuracy:0.1790\n",
    "Epoch #85: Loss:2.4090, Accuracy:0.1881, Validation Loss:2.4596, Validation Accuracy:0.1790\n",
    "Epoch #86: Loss:2.4062, Accuracy:0.1910, Validation Loss:2.4604, Validation Accuracy:0.1839\n",
    "Epoch #87: Loss:2.4049, Accuracy:0.1885, Validation Loss:2.4613, Validation Accuracy:0.1757\n",
    "Epoch #88: Loss:2.4040, Accuracy:0.1873, Validation Loss:2.4625, Validation Accuracy:0.1806\n",
    "Epoch #89: Loss:2.4031, Accuracy:0.1910, Validation Loss:2.4631, Validation Accuracy:0.1773\n",
    "Epoch #90: Loss:2.4034, Accuracy:0.1906, Validation Loss:2.4634, Validation Accuracy:0.1872\n",
    "Epoch #91: Loss:2.4048, Accuracy:0.1943, Validation Loss:2.4639, Validation Accuracy:0.1724\n",
    "Epoch #92: Loss:2.4024, Accuracy:0.1906, Validation Loss:2.4659, Validation Accuracy:0.1872\n",
    "Epoch #93: Loss:2.4004, Accuracy:0.1926, Validation Loss:2.4655, Validation Accuracy:0.1741\n",
    "Epoch #94: Loss:2.3977, Accuracy:0.1934, Validation Loss:2.4657, Validation Accuracy:0.1773\n",
    "Epoch #95: Loss:2.3962, Accuracy:0.1938, Validation Loss:2.4659, Validation Accuracy:0.1741\n",
    "Epoch #96: Loss:2.3995, Accuracy:0.1926, Validation Loss:2.4726, Validation Accuracy:0.1806\n",
    "Epoch #97: Loss:2.3989, Accuracy:0.1959, Validation Loss:2.4678, Validation Accuracy:0.1773\n",
    "Epoch #98: Loss:2.3937, Accuracy:0.1967, Validation Loss:2.4670, Validation Accuracy:0.1872\n",
    "Epoch #99: Loss:2.3943, Accuracy:0.1959, Validation Loss:2.4681, Validation Accuracy:0.1823\n",
    "Epoch #100: Loss:2.3946, Accuracy:0.1922, Validation Loss:2.4699, Validation Accuracy:0.1856\n",
    "Epoch #101: Loss:2.3927, Accuracy:0.1959, Validation Loss:2.4701, Validation Accuracy:0.1823\n",
    "Epoch #102: Loss:2.3938, Accuracy:0.1984, Validation Loss:2.4683, Validation Accuracy:0.1773\n",
    "Epoch #103: Loss:2.3873, Accuracy:0.2033, Validation Loss:2.4701, Validation Accuracy:0.1741\n",
    "Epoch #104: Loss:2.3850, Accuracy:0.2070, Validation Loss:2.4698, Validation Accuracy:0.1938\n",
    "Epoch #105: Loss:2.3835, Accuracy:0.2041, Validation Loss:2.4707, Validation Accuracy:0.1839\n",
    "Epoch #106: Loss:2.3822, Accuracy:0.2053, Validation Loss:2.4736, Validation Accuracy:0.1856\n",
    "Epoch #107: Loss:2.3866, Accuracy:0.2053, Validation Loss:2.4741, Validation Accuracy:0.1806\n",
    "Epoch #108: Loss:2.3829, Accuracy:0.2070, Validation Loss:2.4732, Validation Accuracy:0.1675\n",
    "Epoch #109: Loss:2.3782, Accuracy:0.2094, Validation Loss:2.4742, Validation Accuracy:0.1741\n",
    "Epoch #110: Loss:2.3794, Accuracy:0.2094, Validation Loss:2.4769, Validation Accuracy:0.1626\n",
    "Epoch #111: Loss:2.3771, Accuracy:0.2086, Validation Loss:2.4800, Validation Accuracy:0.1708\n",
    "Epoch #112: Loss:2.3770, Accuracy:0.2053, Validation Loss:2.4801, Validation Accuracy:0.1724\n",
    "Epoch #113: Loss:2.3740, Accuracy:0.2094, Validation Loss:2.4833, Validation Accuracy:0.1757\n",
    "Epoch #114: Loss:2.3730, Accuracy:0.2090, Validation Loss:2.4812, Validation Accuracy:0.1675\n",
    "Epoch #115: Loss:2.3681, Accuracy:0.2090, Validation Loss:2.4816, Validation Accuracy:0.1675\n",
    "Epoch #116: Loss:2.3700, Accuracy:0.2115, Validation Loss:2.4852, Validation Accuracy:0.1642\n",
    "Epoch #117: Loss:2.3697, Accuracy:0.2144, Validation Loss:2.4897, Validation Accuracy:0.1724\n",
    "Epoch #118: Loss:2.3717, Accuracy:0.2029, Validation Loss:2.4902, Validation Accuracy:0.1609\n",
    "Epoch #119: Loss:2.3658, Accuracy:0.2189, Validation Loss:2.4849, Validation Accuracy:0.1626\n",
    "Epoch #120: Loss:2.3625, Accuracy:0.2209, Validation Loss:2.4877, Validation Accuracy:0.1642\n",
    "Epoch #121: Loss:2.3595, Accuracy:0.2181, Validation Loss:2.4860, Validation Accuracy:0.1757\n",
    "Epoch #122: Loss:2.3550, Accuracy:0.2234, Validation Loss:2.4877, Validation Accuracy:0.1691\n",
    "Epoch #123: Loss:2.3563, Accuracy:0.2156, Validation Loss:2.4945, Validation Accuracy:0.1757\n",
    "Epoch #124: Loss:2.3646, Accuracy:0.2111, Validation Loss:2.4996, Validation Accuracy:0.1773\n",
    "Epoch #125: Loss:2.3665, Accuracy:0.2193, Validation Loss:2.4967, Validation Accuracy:0.1708\n",
    "Epoch #126: Loss:2.3690, Accuracy:0.2152, Validation Loss:2.4994, Validation Accuracy:0.1642\n",
    "Epoch #127: Loss:2.3587, Accuracy:0.2103, Validation Loss:2.4925, Validation Accuracy:0.1691\n",
    "Epoch #128: Loss:2.3492, Accuracy:0.2148, Validation Loss:2.4976, Validation Accuracy:0.1708\n",
    "Epoch #129: Loss:2.3497, Accuracy:0.2230, Validation Loss:2.4933, Validation Accuracy:0.1691\n",
    "Epoch #130: Loss:2.3446, Accuracy:0.2214, Validation Loss:2.4964, Validation Accuracy:0.1724\n",
    "Epoch #131: Loss:2.3445, Accuracy:0.2222, Validation Loss:2.4989, Validation Accuracy:0.1773\n",
    "Epoch #132: Loss:2.3457, Accuracy:0.2189, Validation Loss:2.5136, Validation Accuracy:0.1626\n",
    "Epoch #133: Loss:2.3455, Accuracy:0.2259, Validation Loss:2.5029, Validation Accuracy:0.1708\n",
    "Epoch #134: Loss:2.3392, Accuracy:0.2263, Validation Loss:2.5051, Validation Accuracy:0.1790\n",
    "Epoch #135: Loss:2.3415, Accuracy:0.2267, Validation Loss:2.5194, Validation Accuracy:0.1527\n",
    "Epoch #136: Loss:2.3446, Accuracy:0.2189, Validation Loss:2.5059, Validation Accuracy:0.1675\n",
    "Epoch #137: Loss:2.3337, Accuracy:0.2370, Validation Loss:2.5104, Validation Accuracy:0.1576\n",
    "Epoch #138: Loss:2.3396, Accuracy:0.2136, Validation Loss:2.5072, Validation Accuracy:0.1724\n",
    "Epoch #139: Loss:2.3322, Accuracy:0.2251, Validation Loss:2.5070, Validation Accuracy:0.1642\n",
    "Epoch #140: Loss:2.3336, Accuracy:0.2205, Validation Loss:2.5195, Validation Accuracy:0.1691\n",
    "Epoch #141: Loss:2.3382, Accuracy:0.2300, Validation Loss:2.5100, Validation Accuracy:0.1626\n",
    "Epoch #142: Loss:2.3338, Accuracy:0.2177, Validation Loss:2.5143, Validation Accuracy:0.1642\n",
    "Epoch #143: Loss:2.3283, Accuracy:0.2324, Validation Loss:2.5151, Validation Accuracy:0.1658\n",
    "Epoch #144: Loss:2.3259, Accuracy:0.2271, Validation Loss:2.5117, Validation Accuracy:0.1658\n",
    "Epoch #145: Loss:2.3187, Accuracy:0.2316, Validation Loss:2.5242, Validation Accuracy:0.1642\n",
    "Epoch #146: Loss:2.3269, Accuracy:0.2283, Validation Loss:2.5219, Validation Accuracy:0.1675\n",
    "Epoch #147: Loss:2.3212, Accuracy:0.2279, Validation Loss:2.5218, Validation Accuracy:0.1642\n",
    "Epoch #148: Loss:2.3244, Accuracy:0.2345, Validation Loss:2.5267, Validation Accuracy:0.1544\n",
    "Epoch #149: Loss:2.3227, Accuracy:0.2333, Validation Loss:2.5240, Validation Accuracy:0.1609\n",
    "Epoch #150: Loss:2.3270, Accuracy:0.2234, Validation Loss:2.5232, Validation Accuracy:0.1642\n",
    "Epoch #151: Loss:2.3323, Accuracy:0.2197, Validation Loss:2.5185, Validation Accuracy:0.1626\n",
    "Epoch #152: Loss:2.3311, Accuracy:0.2296, Validation Loss:2.5318, Validation Accuracy:0.1675\n",
    "Epoch #153: Loss:2.3294, Accuracy:0.2246, Validation Loss:2.5221, Validation Accuracy:0.1609\n",
    "Epoch #154: Loss:2.3198, Accuracy:0.2308, Validation Loss:2.5255, Validation Accuracy:0.1576\n",
    "Epoch #155: Loss:2.3217, Accuracy:0.2300, Validation Loss:2.5356, Validation Accuracy:0.1544\n",
    "Epoch #156: Loss:2.3249, Accuracy:0.2222, Validation Loss:2.5194, Validation Accuracy:0.1626\n",
    "Epoch #157: Loss:2.3165, Accuracy:0.2300, Validation Loss:2.5258, Validation Accuracy:0.1691\n",
    "Epoch #158: Loss:2.3086, Accuracy:0.2361, Validation Loss:2.5235, Validation Accuracy:0.1576\n",
    "Epoch #159: Loss:2.3034, Accuracy:0.2353, Validation Loss:2.5259, Validation Accuracy:0.1642\n",
    "Epoch #160: Loss:2.2999, Accuracy:0.2431, Validation Loss:2.5302, Validation Accuracy:0.1658\n",
    "Epoch #161: Loss:2.3000, Accuracy:0.2329, Validation Loss:2.5326, Validation Accuracy:0.1658\n",
    "Epoch #162: Loss:2.2986, Accuracy:0.2386, Validation Loss:2.5390, Validation Accuracy:0.1593\n",
    "Epoch #163: Loss:2.2949, Accuracy:0.2394, Validation Loss:2.5417, Validation Accuracy:0.1544\n",
    "Epoch #164: Loss:2.2943, Accuracy:0.2390, Validation Loss:2.5436, Validation Accuracy:0.1658\n",
    "Epoch #165: Loss:2.2943, Accuracy:0.2456, Validation Loss:2.5412, Validation Accuracy:0.1478\n",
    "Epoch #166: Loss:2.2897, Accuracy:0.2489, Validation Loss:2.5484, Validation Accuracy:0.1642\n",
    "Epoch #167: Loss:2.2917, Accuracy:0.2431, Validation Loss:2.5473, Validation Accuracy:0.1494\n",
    "Epoch #168: Loss:2.2919, Accuracy:0.2382, Validation Loss:2.5484, Validation Accuracy:0.1593\n",
    "Epoch #169: Loss:2.2928, Accuracy:0.2366, Validation Loss:2.5454, Validation Accuracy:0.1445\n",
    "Epoch #170: Loss:2.2974, Accuracy:0.2427, Validation Loss:2.5516, Validation Accuracy:0.1527\n",
    "Epoch #171: Loss:2.2986, Accuracy:0.2394, Validation Loss:2.5535, Validation Accuracy:0.1626\n",
    "Epoch #172: Loss:2.2931, Accuracy:0.2431, Validation Loss:2.5532, Validation Accuracy:0.1544\n",
    "Epoch #173: Loss:2.2887, Accuracy:0.2464, Validation Loss:2.5492, Validation Accuracy:0.1593\n",
    "Epoch #174: Loss:2.2847, Accuracy:0.2394, Validation Loss:2.5509, Validation Accuracy:0.1609\n",
    "Epoch #175: Loss:2.2771, Accuracy:0.2509, Validation Loss:2.5579, Validation Accuracy:0.1494\n",
    "Epoch #176: Loss:2.2769, Accuracy:0.2480, Validation Loss:2.5636, Validation Accuracy:0.1560\n",
    "Epoch #177: Loss:2.2784, Accuracy:0.2407, Validation Loss:2.5647, Validation Accuracy:0.1461\n",
    "Epoch #178: Loss:2.2760, Accuracy:0.2505, Validation Loss:2.5673, Validation Accuracy:0.1527\n",
    "Epoch #179: Loss:2.2814, Accuracy:0.2452, Validation Loss:2.5701, Validation Accuracy:0.1511\n",
    "Epoch #180: Loss:2.2738, Accuracy:0.2497, Validation Loss:2.5630, Validation Accuracy:0.1511\n",
    "Epoch #181: Loss:2.2660, Accuracy:0.2493, Validation Loss:2.5634, Validation Accuracy:0.1609\n",
    "Epoch #182: Loss:2.2670, Accuracy:0.2530, Validation Loss:2.5666, Validation Accuracy:0.1609\n",
    "Epoch #183: Loss:2.2621, Accuracy:0.2526, Validation Loss:2.5736, Validation Accuracy:0.1478\n",
    "Epoch #184: Loss:2.2664, Accuracy:0.2505, Validation Loss:2.5719, Validation Accuracy:0.1478\n",
    "Epoch #185: Loss:2.2744, Accuracy:0.2509, Validation Loss:2.5739, Validation Accuracy:0.1576\n",
    "Epoch #186: Loss:2.2613, Accuracy:0.2538, Validation Loss:2.5766, Validation Accuracy:0.1494\n",
    "Epoch #187: Loss:2.2599, Accuracy:0.2517, Validation Loss:2.5766, Validation Accuracy:0.1527\n",
    "Epoch #188: Loss:2.2592, Accuracy:0.2571, Validation Loss:2.5798, Validation Accuracy:0.1560\n",
    "Epoch #189: Loss:2.2742, Accuracy:0.2472, Validation Loss:2.6013, Validation Accuracy:0.1576\n",
    "Epoch #190: Loss:2.2762, Accuracy:0.2546, Validation Loss:2.5838, Validation Accuracy:0.1544\n",
    "Epoch #191: Loss:2.2690, Accuracy:0.2439, Validation Loss:2.5726, Validation Accuracy:0.1461\n",
    "Epoch #192: Loss:2.2695, Accuracy:0.2526, Validation Loss:2.5727, Validation Accuracy:0.1494\n",
    "Epoch #193: Loss:2.2615, Accuracy:0.2546, Validation Loss:2.5824, Validation Accuracy:0.1511\n",
    "Epoch #194: Loss:2.2571, Accuracy:0.2546, Validation Loss:2.5938, Validation Accuracy:0.1527\n",
    "Epoch #195: Loss:2.2668, Accuracy:0.2485, Validation Loss:2.5824, Validation Accuracy:0.1494\n",
    "Epoch #196: Loss:2.2598, Accuracy:0.2538, Validation Loss:2.5853, Validation Accuracy:0.1527\n",
    "Epoch #197: Loss:2.2635, Accuracy:0.2501, Validation Loss:2.5874, Validation Accuracy:0.1544\n",
    "Epoch #198: Loss:2.2543, Accuracy:0.2517, Validation Loss:2.5865, Validation Accuracy:0.1527\n",
    "Epoch #199: Loss:2.2468, Accuracy:0.2575, Validation Loss:2.5883, Validation Accuracy:0.1412\n",
    "Epoch #200: Loss:2.2415, Accuracy:0.2608, Validation Loss:2.5930, Validation Accuracy:0.1494\n",
    "Epoch #201: Loss:2.2428, Accuracy:0.2550, Validation Loss:2.5840, Validation Accuracy:0.1363\n",
    "Epoch #202: Loss:2.2357, Accuracy:0.2620, Validation Loss:2.5932, Validation Accuracy:0.1560\n",
    "Epoch #203: Loss:2.2357, Accuracy:0.2612, Validation Loss:2.5929, Validation Accuracy:0.1379\n",
    "Epoch #204: Loss:2.2270, Accuracy:0.2694, Validation Loss:2.6049, Validation Accuracy:0.1527\n",
    "Epoch #205: Loss:2.2264, Accuracy:0.2678, Validation Loss:2.6039, Validation Accuracy:0.1412\n",
    "Epoch #206: Loss:2.2271, Accuracy:0.2628, Validation Loss:2.6080, Validation Accuracy:0.1527\n",
    "Epoch #207: Loss:2.2228, Accuracy:0.2608, Validation Loss:2.5994, Validation Accuracy:0.1445\n",
    "Epoch #208: Loss:2.2258, Accuracy:0.2674, Validation Loss:2.6202, Validation Accuracy:0.1544\n",
    "Epoch #209: Loss:2.2281, Accuracy:0.2674, Validation Loss:2.6070, Validation Accuracy:0.1412\n",
    "Epoch #210: Loss:2.2143, Accuracy:0.2735, Validation Loss:2.6104, Validation Accuracy:0.1445\n",
    "Epoch #211: Loss:2.2130, Accuracy:0.2669, Validation Loss:2.6164, Validation Accuracy:0.1445\n",
    "Epoch #212: Loss:2.2120, Accuracy:0.2727, Validation Loss:2.6140, Validation Accuracy:0.1346\n",
    "Epoch #213: Loss:2.2122, Accuracy:0.2698, Validation Loss:2.6238, Validation Accuracy:0.1576\n",
    "Epoch #214: Loss:2.2066, Accuracy:0.2669, Validation Loss:2.6160, Validation Accuracy:0.1346\n",
    "Epoch #215: Loss:2.2018, Accuracy:0.2776, Validation Loss:2.6217, Validation Accuracy:0.1478\n",
    "Epoch #216: Loss:2.2041, Accuracy:0.2706, Validation Loss:2.6223, Validation Accuracy:0.1445\n",
    "Epoch #217: Loss:2.2078, Accuracy:0.2743, Validation Loss:2.6212, Validation Accuracy:0.1478\n",
    "Epoch #218: Loss:2.2123, Accuracy:0.2768, Validation Loss:2.6269, Validation Accuracy:0.1609\n",
    "Epoch #219: Loss:2.2028, Accuracy:0.2747, Validation Loss:2.6236, Validation Accuracy:0.1494\n",
    "Epoch #220: Loss:2.2161, Accuracy:0.2669, Validation Loss:2.6294, Validation Accuracy:0.1576\n",
    "Epoch #221: Loss:2.2136, Accuracy:0.2628, Validation Loss:2.6331, Validation Accuracy:0.1576\n",
    "Epoch #222: Loss:2.2050, Accuracy:0.2661, Validation Loss:2.6229, Validation Accuracy:0.1494\n",
    "Epoch #223: Loss:2.1982, Accuracy:0.2764, Validation Loss:2.6295, Validation Accuracy:0.1511\n",
    "Epoch #224: Loss:2.1947, Accuracy:0.2727, Validation Loss:2.6268, Validation Accuracy:0.1461\n",
    "Epoch #225: Loss:2.1836, Accuracy:0.2752, Validation Loss:2.6326, Validation Accuracy:0.1560\n",
    "Epoch #226: Loss:2.1848, Accuracy:0.2817, Validation Loss:2.6379, Validation Accuracy:0.1609\n",
    "Epoch #227: Loss:2.1855, Accuracy:0.2825, Validation Loss:2.6361, Validation Accuracy:0.1494\n",
    "Epoch #228: Loss:2.1797, Accuracy:0.2834, Validation Loss:2.6403, Validation Accuracy:0.1511\n",
    "Epoch #229: Loss:2.1765, Accuracy:0.2805, Validation Loss:2.6421, Validation Accuracy:0.1544\n",
    "Epoch #230: Loss:2.1753, Accuracy:0.2817, Validation Loss:2.6472, Validation Accuracy:0.1511\n",
    "Epoch #231: Loss:2.1733, Accuracy:0.2850, Validation Loss:2.6496, Validation Accuracy:0.1527\n",
    "Epoch #232: Loss:2.1724, Accuracy:0.2862, Validation Loss:2.6509, Validation Accuracy:0.1576\n",
    "Epoch #233: Loss:2.1680, Accuracy:0.2867, Validation Loss:2.6504, Validation Accuracy:0.1461\n",
    "Epoch #234: Loss:2.1735, Accuracy:0.2850, Validation Loss:2.6474, Validation Accuracy:0.1527\n",
    "Epoch #235: Loss:2.1803, Accuracy:0.2784, Validation Loss:2.6468, Validation Accuracy:0.1511\n",
    "Epoch #236: Loss:2.1758, Accuracy:0.2825, Validation Loss:2.6676, Validation Accuracy:0.1658\n",
    "Epoch #237: Loss:2.1726, Accuracy:0.2784, Validation Loss:2.6695, Validation Accuracy:0.1511\n",
    "Epoch #238: Loss:2.1712, Accuracy:0.2830, Validation Loss:2.6588, Validation Accuracy:0.1527\n",
    "Epoch #239: Loss:2.1674, Accuracy:0.2867, Validation Loss:2.6633, Validation Accuracy:0.1478\n",
    "Epoch #240: Loss:2.1558, Accuracy:0.2903, Validation Loss:2.6620, Validation Accuracy:0.1494\n",
    "Epoch #241: Loss:2.1542, Accuracy:0.2912, Validation Loss:2.6723, Validation Accuracy:0.1494\n",
    "Epoch #242: Loss:2.1530, Accuracy:0.2891, Validation Loss:2.6682, Validation Accuracy:0.1494\n",
    "Epoch #243: Loss:2.1639, Accuracy:0.2854, Validation Loss:2.6634, Validation Accuracy:0.1527\n",
    "Epoch #244: Loss:2.1722, Accuracy:0.2768, Validation Loss:2.6769, Validation Accuracy:0.1478\n",
    "Epoch #245: Loss:2.1647, Accuracy:0.2846, Validation Loss:2.6881, Validation Accuracy:0.1461\n",
    "Epoch #246: Loss:2.1763, Accuracy:0.2801, Validation Loss:2.6708, Validation Accuracy:0.1527\n",
    "Epoch #247: Loss:2.1748, Accuracy:0.2846, Validation Loss:2.6769, Validation Accuracy:0.1642\n",
    "Epoch #248: Loss:2.1722, Accuracy:0.2801, Validation Loss:2.6826, Validation Accuracy:0.1609\n",
    "Epoch #249: Loss:2.1669, Accuracy:0.2830, Validation Loss:2.6805, Validation Accuracy:0.1560\n",
    "Epoch #250: Loss:2.1505, Accuracy:0.2924, Validation Loss:2.6744, Validation Accuracy:0.1511\n",
    "Epoch #251: Loss:2.1491, Accuracy:0.2936, Validation Loss:2.6743, Validation Accuracy:0.1626\n",
    "Epoch #252: Loss:2.1458, Accuracy:0.2965, Validation Loss:2.6775, Validation Accuracy:0.1478\n",
    "Epoch #253: Loss:2.1376, Accuracy:0.2953, Validation Loss:2.6873, Validation Accuracy:0.1626\n",
    "Epoch #254: Loss:2.1576, Accuracy:0.2895, Validation Loss:2.6829, Validation Accuracy:0.1478\n",
    "Epoch #255: Loss:2.1434, Accuracy:0.2932, Validation Loss:2.6847, Validation Accuracy:0.1560\n",
    "Epoch #256: Loss:2.1447, Accuracy:0.2916, Validation Loss:2.6873, Validation Accuracy:0.1609\n",
    "Epoch #257: Loss:2.1423, Accuracy:0.2940, Validation Loss:2.6821, Validation Accuracy:0.1511\n",
    "Epoch #258: Loss:2.1389, Accuracy:0.2973, Validation Loss:2.6924, Validation Accuracy:0.1478\n",
    "Epoch #259: Loss:2.1498, Accuracy:0.2977, Validation Loss:2.6922, Validation Accuracy:0.1527\n",
    "Epoch #260: Loss:2.1420, Accuracy:0.2899, Validation Loss:2.6905, Validation Accuracy:0.1461\n",
    "Epoch #261: Loss:2.1304, Accuracy:0.2973, Validation Loss:2.6939, Validation Accuracy:0.1494\n",
    "Epoch #262: Loss:2.1252, Accuracy:0.3047, Validation Loss:2.6887, Validation Accuracy:0.1429\n",
    "Epoch #263: Loss:2.1251, Accuracy:0.3006, Validation Loss:2.7000, Validation Accuracy:0.1511\n",
    "Epoch #264: Loss:2.1177, Accuracy:0.3035, Validation Loss:2.7001, Validation Accuracy:0.1511\n",
    "Epoch #265: Loss:2.1141, Accuracy:0.3010, Validation Loss:2.7008, Validation Accuracy:0.1560\n",
    "Epoch #266: Loss:2.1067, Accuracy:0.3101, Validation Loss:2.7011, Validation Accuracy:0.1494\n",
    "Epoch #267: Loss:2.1068, Accuracy:0.3092, Validation Loss:2.7112, Validation Accuracy:0.1527\n",
    "Epoch #268: Loss:2.1157, Accuracy:0.3051, Validation Loss:2.7146, Validation Accuracy:0.1412\n",
    "Epoch #269: Loss:2.1229, Accuracy:0.3027, Validation Loss:2.7234, Validation Accuracy:0.1478\n",
    "Epoch #270: Loss:2.1194, Accuracy:0.3047, Validation Loss:2.7156, Validation Accuracy:0.1626\n",
    "Epoch #271: Loss:2.1116, Accuracy:0.3109, Validation Loss:2.7051, Validation Accuracy:0.1363\n",
    "Epoch #272: Loss:2.1077, Accuracy:0.3125, Validation Loss:2.7148, Validation Accuracy:0.1478\n",
    "Epoch #273: Loss:2.1233, Accuracy:0.2945, Validation Loss:2.7171, Validation Accuracy:0.1527\n",
    "Epoch #274: Loss:2.1164, Accuracy:0.3043, Validation Loss:2.7263, Validation Accuracy:0.1544\n",
    "Epoch #275: Loss:2.1009, Accuracy:0.3072, Validation Loss:2.7206, Validation Accuracy:0.1609\n",
    "Epoch #276: Loss:2.0984, Accuracy:0.3101, Validation Loss:2.7168, Validation Accuracy:0.1412\n",
    "Epoch #277: Loss:2.1086, Accuracy:0.3105, Validation Loss:2.7208, Validation Accuracy:0.1560\n",
    "Epoch #278: Loss:2.1126, Accuracy:0.3039, Validation Loss:2.7229, Validation Accuracy:0.1412\n",
    "Epoch #279: Loss:2.1055, Accuracy:0.3060, Validation Loss:2.7281, Validation Accuracy:0.1609\n",
    "Epoch #280: Loss:2.1147, Accuracy:0.3014, Validation Loss:2.7301, Validation Accuracy:0.1642\n",
    "Epoch #281: Loss:2.1189, Accuracy:0.3060, Validation Loss:2.7223, Validation Accuracy:0.1560\n",
    "Epoch #282: Loss:2.1091, Accuracy:0.2990, Validation Loss:2.7248, Validation Accuracy:0.1544\n",
    "Epoch #283: Loss:2.1014, Accuracy:0.3158, Validation Loss:2.7272, Validation Accuracy:0.1511\n",
    "Epoch #284: Loss:2.0865, Accuracy:0.3158, Validation Loss:2.7304, Validation Accuracy:0.1642\n",
    "Epoch #285: Loss:2.0818, Accuracy:0.3232, Validation Loss:2.7248, Validation Accuracy:0.1363\n",
    "Epoch #286: Loss:2.0853, Accuracy:0.3183, Validation Loss:2.7388, Validation Accuracy:0.1609\n",
    "Epoch #287: Loss:2.0815, Accuracy:0.3228, Validation Loss:2.7394, Validation Accuracy:0.1494\n",
    "Epoch #288: Loss:2.0806, Accuracy:0.3244, Validation Loss:2.7437, Validation Accuracy:0.1527\n",
    "Epoch #289: Loss:2.0737, Accuracy:0.3183, Validation Loss:2.7519, Validation Accuracy:0.1494\n",
    "Epoch #290: Loss:2.0679, Accuracy:0.3207, Validation Loss:2.7542, Validation Accuracy:0.1576\n",
    "Epoch #291: Loss:2.0730, Accuracy:0.3216, Validation Loss:2.7558, Validation Accuracy:0.1593\n",
    "Epoch #292: Loss:2.0658, Accuracy:0.3248, Validation Loss:2.7550, Validation Accuracy:0.1527\n",
    "Epoch #293: Loss:2.0631, Accuracy:0.3285, Validation Loss:2.7546, Validation Accuracy:0.1527\n",
    "Epoch #294: Loss:2.0675, Accuracy:0.3232, Validation Loss:2.7499, Validation Accuracy:0.1511\n",
    "Epoch #295: Loss:2.0799, Accuracy:0.3187, Validation Loss:2.7643, Validation Accuracy:0.1544\n",
    "Epoch #296: Loss:2.1030, Accuracy:0.3023, Validation Loss:2.7586, Validation Accuracy:0.1593\n",
    "Epoch #297: Loss:2.1261, Accuracy:0.2990, Validation Loss:2.7523, Validation Accuracy:0.1544\n",
    "Epoch #298: Loss:2.0762, Accuracy:0.3199, Validation Loss:2.7519, Validation Accuracy:0.1576\n",
    "Epoch #299: Loss:2.0750, Accuracy:0.3158, Validation Loss:2.7635, Validation Accuracy:0.1544\n",
    "Epoch #300: Loss:2.0875, Accuracy:0.3154, Validation Loss:2.7807, Validation Accuracy:0.1445\n",
    "\n",
    "Test:\n",
    "Test Loss:2.78067112, Accuracy:0.1445\n",
    "Labels: ['eg', 'ce', 'ib', 'mb', 'aa', 'sg', 'sk', 'my', 'eb', 'ek', 'eo', 'ds', 'ck', 'yd', 'by']\n",
    "Confusion Matrix:\n",
    "      eg  ce  ib  mb  aa  sg  sk  my  eb  ek  eo  ds  ck  yd  by\n",
    "t:eg  16   0   0   1  19   1   0   0   8   2   1   0   0   1   1\n",
    "t:ce   4   0   2   2   2   2   0   0   6   1   2   1   1   4   0\n",
    "t:ib   5   0   5   4   3   5   0   0   5   1   5   0   0  19   2\n",
    "t:mb   4   0   5   3   3   3   1   0  10   2   7   0   0  13   1\n",
    "t:aa  11   0   0   0   9   0   0   0   8   2   2   0   0   1   1\n",
    "t:sg   6   0   4   8   2   7   0   0   4   0   6   1   0  13   0\n",
    "t:sk   5   0   0   2   5   2   0   0   9   1   4   0   0   5   0\n",
    "t:my   2   0   2   2   5   0   0   0   0   2   2   0   0   4   1\n",
    "t:eb   6   0   4   5   3   1   0   0  12   3  11   0   0   5   0\n",
    "t:ek   6   0   2   0   9   3   0   0  10   2   8   0   0   8   0\n",
    "t:eo   3   0   3   5   3   3   0   0   5   0   6   0   1   5   0\n",
    "t:ds   7   0   0   0  11   1   0   0   5   1   3   0   1   2   0\n",
    "t:ck   3   0   0   1   7   0   0   0   6   1   1   0   1   2   1\n",
    "t:yd   4   0  16   6   0   5   0   0   1   1   2   0   0  27   0\n",
    "t:by  13   0   1   1   3   1   0   0   8   2   7   0   0   4   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eg       0.17      0.32      0.22        50\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ib       0.11      0.09      0.10        54\n",
    "          mb       0.07      0.06      0.07        52\n",
    "          aa       0.11      0.26      0.15        34\n",
    "          sg       0.21      0.14      0.16        51\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eb       0.12      0.24      0.16        50\n",
    "          ek       0.10      0.04      0.06        48\n",
    "          eo       0.09      0.18      0.12        34\n",
    "          ds       0.00      0.00      0.00        31\n",
    "          ck       0.25      0.04      0.07        23\n",
    "          yd       0.24      0.44      0.31        62\n",
    "          by       0.00      0.00      0.00        40\n",
    "\n",
    "    accuracy                           0.14       609\n",
    "   macro avg       0.10      0.12      0.10       609\n",
    "weighted avg       0.11      0.14      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 06:01:47 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 37 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6970063900125436, 2.687730117775928, 2.6798677358329783, 2.6730307448282224, 2.66754221563856, 2.6631528536478677, 2.6588151983439627, 2.65386241371017, 2.6477770038034723, 2.6400351457799403, 2.631089201114448, 2.6208015204650428, 2.612338304911145, 2.5980151306427954, 2.586291794314956, 2.5712237867032757, 2.5570754171005023, 2.5443348512665196, 2.5431788163427846, 2.52432119944217, 2.5162094766870506, 2.5051124503068345, 2.500301283568584, 2.493425111661012, 2.4894123433650224, 2.4849022029851655, 2.4811156222777218, 2.4782212356041216, 2.476374707198495, 2.4742351388696378, 2.4725806529103047, 2.471303047217759, 2.471506588173226, 2.4682555977738354, 2.468229464513719, 2.4661248043448665, 2.464984741899963, 2.4634908705900846, 2.4626499764829237, 2.462818156713727, 2.461365750270524, 2.4624645709991455, 2.461561337284658, 2.4598407424337956, 2.4589801946492815, 2.4577739285718043, 2.4588147885302214, 2.457504514402944, 2.4604767061806663, 2.4590610847097314, 2.4582276872813407, 2.4569270614724243, 2.45747291826458, 2.457276282443593, 2.4589803508545574, 2.4561256610701236, 2.4569790821357316, 2.4568693814019262, 2.4572104276303195, 2.457363759747084, 2.4567705817606256, 2.457322712210795, 2.4596307665256445, 2.4571890881887604, 2.45680267352776, 2.458262050876085, 2.460115563497559, 2.4577014751622244, 2.4581552499229296, 2.4572114419858835, 2.4592782609372694, 2.458637452869384, 2.460925738799748, 2.458933710073211, 2.456996032561379, 2.4555641003625928, 2.4556886930575317, 2.4560690947941373, 2.4579392697032056, 2.457431693382451, 2.45730286043853, 2.4583252016546693, 2.4616733192418794, 2.4589809271306633, 2.459637331845138, 2.460384237746691, 2.461348987760998, 2.4624655156691477, 2.4630653905163844, 2.4633590121966082, 2.4638815090574067, 2.465860153066701, 2.4654821304264916, 2.4657455351943844, 2.4659224045883454, 2.472639115964642, 2.4678145919135837, 2.4669565723838867, 2.4681346733581844, 2.469921384930415, 2.4700925459806946, 2.468271079713292, 2.470125205802604, 2.4698364225710163, 2.470747742159613, 2.4735819596570896, 2.4740593147591023, 2.473244996297927, 2.474237784180539, 2.476896306368322, 2.479984895740628, 2.4800867439295073, 2.483324624438983, 2.481179197042055, 2.4816095069515685, 2.485188998220785, 2.4896845336030857, 2.49019990217901, 2.484898211725044, 2.487669566190497, 2.486046279005229, 2.487659080079428, 2.4944520239368058, 2.499618185760548, 2.4966772091995515, 2.4993780986428846, 2.492494085935145, 2.497585935937164, 2.4933271846552003, 2.4963695709341267, 2.4989440856113028, 2.5136033564757048, 2.502870301307716, 2.5050804168719965, 2.519383090274479, 2.505933046732434, 2.5103824796347785, 2.507192133682702, 2.506984465619418, 2.519531800437639, 2.5100223618775166, 2.5143136931170385, 2.5151056577810906, 2.5117217429557264, 2.5242053800811517, 2.521870751294792, 2.5218002310723113, 2.526722959305461, 2.524028654756217, 2.523207188435572, 2.518452172991873, 2.531766140206498, 2.5220538227037452, 2.525483611769277, 2.5356072345977934, 2.519350362332975, 2.525842775852222, 2.523473819879867, 2.5259348719773818, 2.5301722665921416, 2.5326488386038295, 2.5390061372998116, 2.541650540723002, 2.543587180389755, 2.541208330633605, 2.548448900871089, 2.547270265510321, 2.5483882908750637, 2.545434119861897, 2.551633437474569, 2.553502639526217, 2.5531536143010083, 2.5491908825872764, 2.550893377592215, 2.557926331443348, 2.563574442918273, 2.5646908713874756, 2.5673326330231916, 2.570057471201729, 2.563025572029828, 2.563394613845399, 2.566628048964126, 2.573570696982648, 2.571884743685793, 2.573919975894621, 2.576643227356408, 2.576587664865704, 2.5797862685568425, 2.6013023684960475, 2.583830396921568, 2.572565851148909, 2.572725446744897, 2.5824082246163402, 2.593839689624329, 2.5824094225816148, 2.5852931618494743, 2.5873974313093915, 2.5864526540383523, 2.588344844104034, 2.5929933721796043, 2.5839823881785073, 2.5931847804089876, 2.5929070599560666, 2.6048788973459076, 2.6039203129378445, 2.6079570256626274, 2.599366978862994, 2.6201829416998503, 2.6069699257661165, 2.610415060140425, 2.6163784960416345, 2.6140008372039043, 2.623817874488768, 2.616020040167572, 2.621741592003207, 2.622345590043342, 2.6211575467402515, 2.626897444670228, 2.623570540072687, 2.629411000532078, 2.633116253099614, 2.6228861871415563, 2.6295388476993455, 2.6267806273963066, 2.632580393053628, 2.637871898845303, 2.6361191409757767, 2.640328246384419, 2.642054026545758, 2.647204472317876, 2.6496048455167873, 2.6508672491865988, 2.650440548441093, 2.6474239716584655, 2.6468031508190486, 2.6676449908803055, 2.669527389341583, 2.658791818055026, 2.663316461644541, 2.662038470332454, 2.6722959634314227, 2.6682263074445802, 2.663438453267165, 2.6768687487823035, 2.688103244026698, 2.670836460805683, 2.6768624187494536, 2.682612059738836, 2.6805437006582378, 2.6744072699585963, 2.6743003296343173, 2.6774671637561718, 2.687342638648398, 2.6828967186030495, 2.684748800713049, 2.687327972578102, 2.6820636273213405, 2.692402499845658, 2.6921913753002147, 2.690475050843212, 2.69387818909631, 2.6886772659220326, 2.6999587503755818, 2.700088429333541, 2.7008253506251743, 2.7011142747938535, 2.7111616032855657, 2.714576609616209, 2.723427415481342, 2.715611161856816, 2.7051349410478314, 2.714847187299055, 2.7170552154284198, 2.7262982793629464, 2.7205827803838822, 2.716781432992719, 2.720778130545405, 2.722913759682566, 2.728073103674527, 2.730093366798313, 2.722289312062005, 2.724783049819896, 2.7271663508391732, 2.730421045535108, 2.724828693471323, 2.7387824547897615, 2.739375174730674, 2.7436934022480632, 2.751933228206165, 2.7541766843968034, 2.755783961128523, 2.755010611905253, 2.7545848981108767, 2.7499096581501328, 2.7642960622784343, 2.7585895554772737, 2.7523182471984713, 2.7518993171760795, 2.7635196938694797, 2.780671136132602], 'val_acc': [0.08866995023731723, 0.09031198645998496, 0.10180623943142116, 0.11658456484819281, 0.11165845647380856, 0.10344827584983485, 0.1018062397250401, 0.11986863610681837, 0.12479474448120262, 0.12151067223161312, 0.12151067223161312, 0.12479474448120262, 0.14121510582702304, 0.1395730697022283, 0.1559934311459217, 0.14449917846810445, 0.16748768401948494, 0.16912972024215267, 0.16091953952030597, 0.16584564818830913, 0.1740558287144099, 0.16584564809043614, 0.17077175646482037, 0.16912972024215267, 0.16912972034002563, 0.17241379258961514, 0.1740558287144099, 0.17898193708879412, 0.17241379249174216, 0.18062397331146185, 0.17733990115974532, 0.18883415403330855, 0.1822660093383836, 0.1871921179085138, 0.18226600963200254, 0.18555008188159203, 0.18390804556105134, 0.18555008178371907, 0.18883415413118151, 0.18390804556105134, 0.1888341542290545, 0.18555008178371907, 0.1839080456589243, 0.1871921179085138, 0.18719211781064082, 0.18555008188159203, 0.18555008178371907, 0.19211822638077103, 0.18555008178371907, 0.19211822628289804, 0.1904761901581033, 0.19211822638077103, 0.18555008178371907, 0.19211822628289804, 0.17898193728454007, 0.18883415413118151, 0.1822660093383836, 0.19047619025597626, 0.18883415393543557, 0.18062397331146185, 0.1822660094362566, 0.18226600953412955, 0.19211822657651698, 0.19211822638077103, 0.17898193738241305, 0.1806239734093348, 0.18719211820213275, 0.18883415413118151, 0.18226600953412955, 0.1789819371866671, 0.18062397360508078, 0.19047619025597626, 0.1773399013554913, 0.18226600953412955, 0.17569786464345866, 0.1822660094362566, 0.1822660093383836, 0.18226600914263763, 0.1888341542290545, 0.18062397331146185, 0.18883415393543557, 0.1822660093383836, 0.1806239734093348, 0.17898193699092113, 0.17898193708879412, 0.18390804556105134, 0.1756978649370776, 0.18062397311571587, 0.1773399008661264, 0.1871921175170219, 0.17241379258961514, 0.18719211800638677, 0.1740558286165369, 0.17733990115974532, 0.17405582842079093, 0.18062397331146185, 0.17733990106187233, 0.1871921175170219, 0.1822660093383836, 0.18555008178371907, 0.1822660093383836, 0.17733990115974532, 0.1740558285186639, 0.1937602623098198, 0.18390804556105134, 0.1855500815879731, 0.18062397311571587, 0.16748768382373896, 0.17405582842079093, 0.16256157554722772, 0.17077175626907443, 0.17241379219812322, 0.17569786503495058, 0.16748768392161195, 0.16748768362799302, 0.16420361137840353, 0.17241379239386917, 0.160919539226687, 0.16256157544935473, 0.16420361137840353, 0.17569786474133164, 0.16912972014427968, 0.17569786503495058, 0.1773399008661264, 0.17077175626907443, 0.16420361246724044, 0.1691297200464067, 0.17077175626907443, 0.1691297200464067, 0.17241379229599618, 0.1773399007682534, 0.1625615752536088, 0.1707717559754555, 0.17898193679517518, 0.15270935860271329, 0.16748768392161195, 0.15763546787018845, 0.17241379210025023, 0.16420361157414948, 0.1691297206336446, 0.16256157554722772, 0.16420361157414948, 0.1658456478946902, 0.16584564769894422, 0.16420361246724044, 0.16748768401948494, 0.16420361167202246, 0.154351395522726, 0.16091953942243298, 0.16420361167202246, 0.16256157574297367, 0.167487683725866, 0.16091953942243298, 0.15763546787018845, 0.15435139462963504, 0.16256157554722772, 0.16912972014427968, 0.15763546687922456, 0.16420361137840353, 0.16584564769894422, 0.16584564760107126, 0.15927750409285618, 0.15435139482538102, 0.16584564760107126, 0.14778325032620204, 0.16420361147627652, 0.14942528734408772, 0.15927750399498322, 0.14449917797873957, 0.1527093595936772, 0.16256157544935473, 0.15435139571847195, 0.15927750310189226, 0.160919539226687, 0.14942528645099679, 0.1559934318432667, 0.14614121429928026, 0.15270935860271329, 0.1510673226736645, 0.15106732238004555, 0.16091953942243298, 0.16091953961817893, 0.14778325061982098, 0.14778325071769394, 0.1576354675643354, 0.14942528664674273, 0.1527093589942052, 0.1559934311459217, 0.15763546736858944, 0.15435139472750803, 0.14614121439715325, 0.14942528654886975, 0.15106732247791854, 0.1527093585048403, 0.1494252863531238, 0.15270935928782414, 0.15435139521687294, 0.1527093585048403, 0.14121510572915008, 0.14942528645099679, 0.1362889974526388, 0.15599343134166768, 0.13793103377317953, 0.15270935870058627, 0.1412151056312771, 0.15270935928782414, 0.1444991783702315, 0.15435139502112696, 0.14121510582702304, 0.14449917846810445, 0.14449917797873957, 0.13464696132784407, 0.15763546746646243, 0.13464696132784407, 0.14778325071769394, 0.14449917846810445, 0.14778325061982098, 0.1609195398139249, 0.14942528664674273, 0.1576354675643354, 0.15763546727071645, 0.14942528694036167, 0.1510673230651564, 0.1461412145928992, 0.15599343153741363, 0.16091953961817893, 0.14942528674461572, 0.15106732296728345, 0.15435139521687294, 0.15106732277153748, 0.1527093590920782, 0.15763546727071645, 0.14614121449502623, 0.15270935938569713, 0.15106732277153748, 0.16584564809043614, 0.15106732247791854, 0.1527093590920782, 0.147783250521948, 0.14942528694036167, 0.14942528674461572, 0.14942528694036167, 0.15270935928782414, 0.147783250521948, 0.14614121429928026, 0.1527093589942052, 0.16420361235713332, 0.16091953961817893, 0.1559934312437947, 0.15106732286941046, 0.16256157603659263, 0.14778325061982098, 0.1625615761344656, 0.14778325071769394, 0.1559934312437947, 0.16091953991179786, 0.1510673232609024, 0.14778325061982098, 0.1527093590920782, 0.14614121429928026, 0.14942528694036167, 0.14285714224543672, 0.1510673231630294, 0.15106732286941046, 0.15599343163528662, 0.14942528703823466, 0.15270935938569713, 0.14121510612064198, 0.147783250521948, 0.16256157603659263, 0.13628899764838476, 0.14778325081556692, 0.15270935918995115, 0.15435139521687294, 0.16091953991179786, 0.14121510621851496, 0.15599343153741363, 0.14121510631638795, 0.1609195397160519, 0.1642036119656414, 0.1559934312437947, 0.15435139521687294, 0.15106732286941046, 0.1642036124550063, 0.13628899784413073, 0.16091954000967085, 0.14942528694036167, 0.1527093590920782, 0.14942528694036167, 0.15763546736858944, 0.15927750368913016, 0.15270935928782414, 0.15270935918995115, 0.15106732277153748, 0.15435139511899995, 0.1592775033955112, 0.15435139521687294, 0.15763546746646243, 0.154351394923254, 0.1444991783702315], 'loss': [2.702839764530409, 2.693829618880881, 2.684652132371123, 2.6771817223002534, 2.670969690777193, 2.666193506311342, 2.6615950304379945, 2.656821112270473, 2.65120991103703, 2.6436269613262073, 2.6350344364158427, 2.6251197718007364, 2.6148550212750443, 2.606120148135896, 2.591071257111473, 2.5789149060141625, 2.5629747544470756, 2.5493136131787937, 2.5390210742088803, 2.530974970901771, 2.5185819520108264, 2.5104570258569425, 2.5006598691920723, 2.4948982317100072, 2.488417313965439, 2.482364973052571, 2.4791418655207513, 2.4763433127432632, 2.4726166277695487, 2.47020272278443, 2.4661862063946423, 2.4644485992572633, 2.4649416690489594, 2.4642734174121332, 2.4580751317237683, 2.456723234736699, 2.4548196458718614, 2.453336726272865, 2.4522146799725917, 2.4496517777687714, 2.4501250534331773, 2.4487504636972104, 2.447797955918361, 2.4458090005714057, 2.4450037527378092, 2.444622770031375, 2.4427603628601138, 2.441275653897859, 2.4400642190380997, 2.442188541110781, 2.4385366603334337, 2.437458053604533, 2.436588148316808, 2.4383917404151307, 2.4360624618843594, 2.4360819743154476, 2.4332141900699, 2.43274275801265, 2.4320353046824557, 2.4312730930179542, 2.432231285929435, 2.4301513434925117, 2.4325881982486104, 2.4331671506227655, 2.4302098440683353, 2.4278446337525605, 2.428263122589926, 2.4276593968119218, 2.4259864512410254, 2.4252692468357284, 2.423832559585571, 2.4255939505183477, 2.4217847061353055, 2.4251450995889776, 2.4228604039617143, 2.4188662481993375, 2.417506173601875, 2.416855846244452, 2.414853427933961, 2.4141865927091124, 2.412279154779485, 2.4116942424304186, 2.4101206241936652, 2.4107224622056713, 2.40900176183399, 2.4061807823376977, 2.4049451150443764, 2.4040159560326924, 2.4030567766459816, 2.4034295222108124, 2.404835423111181, 2.4024467302788453, 2.400388835049263, 2.3976932479615574, 2.3962149538788218, 2.39953858113142, 2.398926350125542, 2.39372898372047, 2.3942835667784452, 2.3945793903828645, 2.392695412253942, 2.3938014722702685, 2.38729736692362, 2.3850492047578156, 2.383485210700691, 2.382233875732892, 2.3866083913760017, 2.3829015902180446, 2.3781519545177168, 2.3794033733971065, 2.377070058004078, 2.377038624937774, 2.374015045361842, 2.3729838071662543, 2.3681033599548025, 2.3699832568668, 2.369657046985822, 2.3716674200563217, 2.365837249814607, 2.362499889162287, 2.3594742706424157, 2.355049775219551, 2.3563396422525207, 2.3645819016795384, 2.3664854142700134, 2.3690333874563416, 2.3587473179035854, 2.349237014919336, 2.3497174165087316, 2.344633538032704, 2.3444831272415065, 2.345716355811399, 2.345542177086738, 2.3392357351354014, 2.341524091736247, 2.344586912464557, 2.3337431029372637, 2.339623277534939, 2.3322209163368117, 2.3335817113794097, 2.3381949976973955, 2.33380266596894, 2.3283290344097285, 2.3259024017890133, 2.3186862584257026, 2.326892113342912, 2.3212180663428024, 2.3243940202607267, 2.322655979955466, 2.3270247202879104, 2.3323332356721225, 2.331081937813416, 2.329386199426357, 2.3198337816359818, 2.3216634764074056, 2.3248786420058423, 2.316507172535577, 2.3085885982004277, 2.303388093384384, 2.2998634790737773, 2.2999551387293384, 2.2985566506395596, 2.2949065256412515, 2.294285445830171, 2.2943441275453664, 2.289745811466319, 2.2916938435125647, 2.291927453967335, 2.292826843947111, 2.2974010741686186, 2.2985587044663007, 2.2930854148198936, 2.2887145596608, 2.2846688145239984, 2.277054021588586, 2.276911106442524, 2.2783946680582035, 2.2759862292718593, 2.2813817738017996, 2.273828784836881, 2.265970681384359, 2.26699780810785, 2.262082114014048, 2.2663723688106026, 2.2744168735872305, 2.2613358900042773, 2.259867378818426, 2.2591931423367417, 2.2742485421394174, 2.2761885809457767, 2.268978481273142, 2.2695237384929303, 2.26153058163684, 2.257116357403859, 2.2667510879602766, 2.2597915050185438, 2.263492347572374, 2.2542531703776647, 2.246815381059901, 2.241453259485703, 2.2428441550942173, 2.235672637618298, 2.235674302925564, 2.2270038024111205, 2.226448588008998, 2.2271243671127414, 2.222777164545392, 2.225830471638047, 2.228102639960068, 2.214284877757517, 2.2129793165155993, 2.2119756635943966, 2.2122125576653766, 2.206558674708529, 2.2017974988635807, 2.2041316037305325, 2.2078047657404594, 2.2123308699723387, 2.2027666737166762, 2.2161049163317044, 2.213554886674979, 2.204962754102703, 2.1981704926343912, 2.194655268393013, 2.1835996784468694, 2.1847854368495745, 2.185512372257773, 2.1797334151101553, 2.1764950653365993, 2.1752669072004314, 2.1733052641459314, 2.1723550187488847, 2.1679947090344753, 2.173483083869887, 2.1802747385702586, 2.175775697441806, 2.1725549668502024, 2.1712137679544563, 2.1673932058855248, 2.155838645312331, 2.1542242339993893, 2.153016272315744, 2.163859347983797, 2.1721836412222233, 2.164726138065973, 2.176254456048139, 2.174793145397116, 2.1722311430153662, 2.1669401262819887, 2.150453446582113, 2.1490989429505207, 2.145807846077169, 2.137640944153866, 2.157623107526337, 2.1434415795719843, 2.144728251110602, 2.14231014868562, 2.1388702680442857, 2.149771106512395, 2.1420002100159254, 2.13035542754422, 2.125221719291421, 2.1251458517579818, 2.1176866650826143, 2.114051095954691, 2.1067224096223804, 2.106760027863896, 2.115732774205766, 2.122875312221613, 2.1194041494960905, 2.111574653335665, 2.10771617047351, 2.123289966974905, 2.1163511257641616, 2.10093955797826, 2.098374125600106, 2.1085883068108218, 2.1126304555967357, 2.105486059972148, 2.114660513033857, 2.1188974367764453, 2.1090989064876546, 2.1013862472784837, 2.086494988239766, 2.081808109988422, 2.0852995679608606, 2.081450463712093, 2.08059685489725, 2.0737425896176567, 2.0679356046281065, 2.0729812527094533, 2.0657875101179557, 2.063141950297894, 2.067486740970024, 2.0799447629486023, 2.102963019788143, 2.1260838258927364, 2.076161778878872, 2.074995050586959, 2.0874953354163828], 'acc': [0.0689938403773112, 0.0891170430752531, 0.09034907572445683, 0.09979466151836226, 0.104722792347721, 0.10636550282000026, 0.10266940426655129, 0.10513346995661146, 0.12114989746216631, 0.11827515451815095, 0.11663244386840405, 0.12032854241267367, 0.12607802984895647, 0.14250513378844368, 0.14291581199399256, 0.15236139662211926, 0.1498973306995153, 0.16262833588054781, 0.1609856271707057, 0.1572895288039037, 0.16386037012390042, 0.16591375841007586, 0.16303901328443257, 0.16673511362785676, 0.17002053437658893, 0.1634496909025024, 0.16139630400547011, 0.16427104772361154, 0.17166324365555138, 0.16550307959868923, 0.17043121080134194, 0.17330595450112463, 0.17412731114476612, 0.17002053435823022, 0.17043121160300606, 0.16755646708320054, 0.17618069864763616, 0.17330595375453667, 0.1712525658416552, 0.17248459930170243, 0.17289527811308905, 0.17494866461846864, 0.1737166325475646, 0.17248459851839704, 0.17659137526821553, 0.16919917816131758, 0.17248460067248686, 0.17207392227111165, 0.17248459891004975, 0.17371663252920586, 0.1733059539320043, 0.1757700196404232, 0.17618069766850442, 0.17289527750725128, 0.17084189000438127, 0.17535934398061687, 0.1733059555169738, 0.17453798837118326, 0.176180697258493, 0.17659137685318502, 0.17577002022790222, 0.17700205366959074, 0.1786447629669119, 0.17741273169767197, 0.18110883026030028, 0.1802874732250061, 0.17905544117246075, 0.18316221795899668, 0.177002052690459, 0.1806981524280454, 0.17823408673798524, 0.18357289497122872, 0.18234086350616244, 0.17864476394604364, 0.18398357239347218, 0.1839835739784417, 0.18480492762961181, 0.18398357178763441, 0.18521560526604036, 0.1856262822966311, 0.18809034896582305, 0.1897330586364382, 0.1843942504031947, 0.18644763949103424, 0.18809034916164938, 0.1909650928797908, 0.1885010263880665, 0.18726899394386848, 0.19096509190065905, 0.19055441428258924, 0.19425051441182836, 0.19055441428258924, 0.192607802764591, 0.19342915917568873, 0.19383983462131, 0.19260780374372274, 0.19589322310331175, 0.19671457816198376, 0.1958932240824435, 0.19219712573400025, 0.19589322449245491, 0.19835729000504745, 0.20328542001438338, 0.20698151861372915, 0.2041067772455039, 0.20533880951223432, 0.20533880851474387, 0.20698151918284946, 0.20944558410796296, 0.20944558490962709, 0.20862422889018206, 0.2053388091205816, 0.20944558410796296, 0.2090349066857195, 0.20903490650825188, 0.2114989743707606, 0.21437371648557377, 0.2028747425921399, 0.21889116930521, 0.22094455839304952, 0.2180698140874291, 0.2234086252580678, 0.21560574894813053, 0.21108829497189494, 0.21930184772494393, 0.21519507154424578, 0.21026694112489847, 0.21478439469112262, 0.22299794763999797, 0.2213552365985984, 0.2221765910330739, 0.21889117071271189, 0.22587269020154002, 0.22628336783796854, 0.2266940444585479, 0.21889117030270044, 0.2369609864952628, 0.21355236246110967, 0.22505133612199976, 0.22053387999167431, 0.22997946597222674, 0.21765913860509037, 0.2324435322864834, 0.22710472284156438, 0.2316221768361587, 0.22833675610578524, 0.22792607809606275, 0.23449692096431152, 0.23326488907087511, 0.22340862429729477, 0.21971252632214547, 0.2295687881766893, 0.22464065813063597, 0.23080082062088733, 0.22997946659642324, 0.2221765912656177, 0.22997946696971722, 0.23613963063492668, 0.23531827620045115, 0.24312114935880813, 0.23285420872959514, 0.2386036957558665, 0.23942505081453852, 0.23901437339229506, 0.24558521526305338, 0.2488706369909173, 0.2431211487713291, 0.2381930189211021, 0.23655030923212822, 0.2427104725240437, 0.2394250523811493, 0.24312115131707163, 0.24640656850421208, 0.2394250519711379, 0.2509240241021346, 0.24804928057981956, 0.2406570842378683, 0.2505133468757175, 0.24517453686167817, 0.24969199222705693, 0.24928131521482488, 0.2529774121924837, 0.25256673614102465, 0.2505133468757175, 0.25092402230298005, 0.2537987686035814, 0.2517453775391197, 0.2570841881773555, 0.247227925931159, 0.25462012366225345, 0.24394250576990587, 0.25256673516189293, 0.25462012366225345, 0.25462012327060074, 0.24845995800206302, 0.2537987688177665, 0.2501026704509645, 0.251745380904885, 0.25749486438792346, 0.26078028611578735, 0.2550308003011915, 0.26201232096497773, 0.26119096435805367, 0.2694045168785589, 0.26776180761795515, 0.26283367420613646, 0.2607802873274629, 0.2673511299998853, 0.26735113058736437, 0.27351129403838875, 0.26694045375259995, 0.2726899394264456, 0.26981519531665155, 0.26694045140268374, 0.27761806786917076, 0.2706365495736594, 0.2743326500945512, 0.2767967130246838, 0.2747433259501839, 0.2669404531651209, 0.2628336746345066, 0.26611909593400035, 0.27638603798907396, 0.27268993962227195, 0.2751540039599064, 0.28172484561647965, 0.282546201244272, 0.2833675545221481, 0.2804928113914858, 0.2817248460448498, 0.2850102663652119, 0.2862423019977076, 0.286652975075054, 0.28501026558190645, 0.2784394270585548, 0.2825462030434266, 0.2784394243169859, 0.28295688023312626, 0.286652975662533, 0.290349077199274, 0.29117043024460637, 0.28911704156677825, 0.28542094656574163, 0.27679671560714375, 0.28459958956716486, 0.2800821345567214, 0.2845995893346211, 0.280082136943355, 0.2829568790581682, 0.29240246270716314, 0.29363449712798334, 0.2965092384962086, 0.29527720779608896, 0.28952772176730807, 0.2932238202932189, 0.29158110825432887, 0.2940451727877897, 0.29733059690228725, 0.2977412733086815, 0.2899383968029179, 0.2973305968655698, 0.30472279105343125, 0.3006160170268229, 0.3034907581992218, 0.301026693665761, 0.3100616012632969, 0.30924024700628905, 0.30513347203726643, 0.30266940590047736, 0.3047227904659522, 0.3108829563036102, 0.31252566912580565, 0.2944558500142068, 0.3043121157852776, 0.3071868602867244, 0.31006160008833883, 0.31047227790223497, 0.30390143757972876, 0.30595482586590417, 0.30143737386629077, 0.30595482704086224, 0.29897330498793284, 0.31581108713296896, 0.31581108772044797, 0.32320328523735736, 0.318275154603825, 0.322792609577551, 0.32443531687989124, 0.318275154603825, 0.3207392197247648, 0.3215605747650781, 0.3248459972762474, 0.3285420932931332, 0.32320328582483643, 0.3186858326135475, 0.30225872831912504, 0.29897330377625736, 0.319917864060255, 0.3158110875246216, 0.31540041088568355]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
