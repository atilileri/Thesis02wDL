{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf13.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 15:54:58 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'All', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['yd', 'mb', 'eo', 'ek', 'by', 'eb', 'ck', 'aa', 'ds', 'sk', 'my', 'sg', 'ib', 'eg', 'ce'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002510058D278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000025170156EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7070, Accuracy:0.0637, Validation Loss:2.6981, Validation Accuracy:0.0805\n",
    "Epoch #2: Loss:2.6944, Accuracy:0.0821, Validation Loss:2.6859, Validation Accuracy:0.1084\n",
    "Epoch #3: Loss:2.6840, Accuracy:0.1097, Validation Loss:2.6788, Validation Accuracy:0.0936\n",
    "Epoch #4: Loss:2.6793, Accuracy:0.0883, Validation Loss:2.6769, Validation Accuracy:0.1117\n",
    "Epoch #5: Loss:2.6751, Accuracy:0.0945, Validation Loss:2.6701, Validation Accuracy:0.0854\n",
    "Epoch #6: Loss:2.6685, Accuracy:0.0895, Validation Loss:2.6654, Validation Accuracy:0.0887\n",
    "Epoch #7: Loss:2.6635, Accuracy:0.0891, Validation Loss:2.6605, Validation Accuracy:0.0936\n",
    "Epoch #8: Loss:2.6588, Accuracy:0.0986, Validation Loss:2.6551, Validation Accuracy:0.1067\n",
    "Epoch #9: Loss:2.6534, Accuracy:0.1109, Validation Loss:2.6489, Validation Accuracy:0.1264\n",
    "Epoch #10: Loss:2.6463, Accuracy:0.1277, Validation Loss:2.6409, Validation Accuracy:0.1494\n",
    "Epoch #11: Loss:2.6380, Accuracy:0.1388, Validation Loss:2.6311, Validation Accuracy:0.1511\n",
    "Epoch #12: Loss:2.6279, Accuracy:0.1409, Validation Loss:2.6188, Validation Accuracy:0.1511\n",
    "Epoch #13: Loss:2.6159, Accuracy:0.1400, Validation Loss:2.6060, Validation Accuracy:0.1461\n",
    "Epoch #14: Loss:2.6010, Accuracy:0.1417, Validation Loss:2.5886, Validation Accuracy:0.1494\n",
    "Epoch #15: Loss:2.5858, Accuracy:0.1413, Validation Loss:2.5735, Validation Accuracy:0.1658\n",
    "Epoch #16: Loss:2.5696, Accuracy:0.1507, Validation Loss:2.5587, Validation Accuracy:0.1642\n",
    "Epoch #17: Loss:2.5558, Accuracy:0.1511, Validation Loss:2.5418, Validation Accuracy:0.1708\n",
    "Epoch #18: Loss:2.5413, Accuracy:0.1520, Validation Loss:2.5293, Validation Accuracy:0.1609\n",
    "Epoch #19: Loss:2.5271, Accuracy:0.1540, Validation Loss:2.5092, Validation Accuracy:0.1609\n",
    "Epoch #20: Loss:2.5184, Accuracy:0.1561, Validation Loss:2.5040, Validation Accuracy:0.1478\n",
    "Epoch #21: Loss:2.5207, Accuracy:0.1478, Validation Loss:2.5066, Validation Accuracy:0.1511\n",
    "Epoch #22: Loss:2.5044, Accuracy:0.1561, Validation Loss:2.4797, Validation Accuracy:0.1576\n",
    "Epoch #23: Loss:2.4964, Accuracy:0.1577, Validation Loss:2.4799, Validation Accuracy:0.1691\n",
    "Epoch #24: Loss:2.4906, Accuracy:0.1602, Validation Loss:2.4610, Validation Accuracy:0.1806\n",
    "Epoch #25: Loss:2.4841, Accuracy:0.1655, Validation Loss:2.4623, Validation Accuracy:0.1872\n",
    "Epoch #26: Loss:2.4819, Accuracy:0.1688, Validation Loss:2.4570, Validation Accuracy:0.1806\n",
    "Epoch #27: Loss:2.4796, Accuracy:0.1696, Validation Loss:2.4600, Validation Accuracy:0.1773\n",
    "Epoch #28: Loss:2.4772, Accuracy:0.1667, Validation Loss:2.4504, Validation Accuracy:0.1790\n",
    "Epoch #29: Loss:2.4755, Accuracy:0.1647, Validation Loss:2.4579, Validation Accuracy:0.1823\n",
    "Epoch #30: Loss:2.4779, Accuracy:0.1655, Validation Loss:2.4487, Validation Accuracy:0.1790\n",
    "Epoch #31: Loss:2.4733, Accuracy:0.1692, Validation Loss:2.4562, Validation Accuracy:0.1790\n",
    "Epoch #32: Loss:2.4766, Accuracy:0.1704, Validation Loss:2.4535, Validation Accuracy:0.1839\n",
    "Epoch #33: Loss:2.5261, Accuracy:0.1598, Validation Loss:2.5520, Validation Accuracy:0.1560\n",
    "Epoch #34: Loss:2.5318, Accuracy:0.1565, Validation Loss:2.5703, Validation Accuracy:0.1461\n",
    "Epoch #35: Loss:2.5210, Accuracy:0.1544, Validation Loss:2.4585, Validation Accuracy:0.1905\n",
    "Epoch #36: Loss:2.5038, Accuracy:0.1544, Validation Loss:2.4611, Validation Accuracy:0.1757\n",
    "Epoch #37: Loss:2.4916, Accuracy:0.1593, Validation Loss:2.4790, Validation Accuracy:0.1872\n",
    "Epoch #38: Loss:2.4870, Accuracy:0.1749, Validation Loss:2.4696, Validation Accuracy:0.1856\n",
    "Epoch #39: Loss:2.4757, Accuracy:0.1737, Validation Loss:2.4606, Validation Accuracy:0.1675\n",
    "Epoch #40: Loss:2.4751, Accuracy:0.1704, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #41: Loss:2.4688, Accuracy:0.1733, Validation Loss:2.4612, Validation Accuracy:0.1708\n",
    "Epoch #42: Loss:2.4666, Accuracy:0.1770, Validation Loss:2.4546, Validation Accuracy:0.1757\n",
    "Epoch #43: Loss:2.4659, Accuracy:0.1758, Validation Loss:2.4470, Validation Accuracy:0.1708\n",
    "Epoch #44: Loss:2.4662, Accuracy:0.1754, Validation Loss:2.4439, Validation Accuracy:0.1806\n",
    "Epoch #45: Loss:2.4654, Accuracy:0.1762, Validation Loss:2.4457, Validation Accuracy:0.1757\n",
    "Epoch #46: Loss:2.4676, Accuracy:0.1733, Validation Loss:2.4478, Validation Accuracy:0.1856\n",
    "Epoch #47: Loss:2.4702, Accuracy:0.1684, Validation Loss:2.4509, Validation Accuracy:0.1839\n",
    "Epoch #48: Loss:2.4704, Accuracy:0.1684, Validation Loss:2.4511, Validation Accuracy:0.1839\n",
    "Epoch #49: Loss:2.4692, Accuracy:0.1733, Validation Loss:2.4528, Validation Accuracy:0.1856\n",
    "Epoch #50: Loss:2.4698, Accuracy:0.1741, Validation Loss:2.4523, Validation Accuracy:0.1806\n",
    "Epoch #51: Loss:2.4709, Accuracy:0.1758, Validation Loss:2.4548, Validation Accuracy:0.1856\n",
    "Epoch #52: Loss:2.4722, Accuracy:0.1770, Validation Loss:2.4553, Validation Accuracy:0.1839\n",
    "Epoch #53: Loss:2.4716, Accuracy:0.1745, Validation Loss:2.4573, Validation Accuracy:0.1856\n",
    "Epoch #54: Loss:2.4709, Accuracy:0.1749, Validation Loss:2.4521, Validation Accuracy:0.1806\n",
    "Epoch #55: Loss:2.4680, Accuracy:0.1770, Validation Loss:2.4528, Validation Accuracy:0.1823\n",
    "Epoch #56: Loss:2.4662, Accuracy:0.1733, Validation Loss:2.4510, Validation Accuracy:0.1888\n",
    "Epoch #57: Loss:2.4666, Accuracy:0.1717, Validation Loss:2.4493, Validation Accuracy:0.1888\n",
    "Epoch #58: Loss:2.4640, Accuracy:0.1688, Validation Loss:2.4477, Validation Accuracy:0.1888\n",
    "Epoch #59: Loss:2.4620, Accuracy:0.1762, Validation Loss:2.4471, Validation Accuracy:0.1921\n",
    "Epoch #60: Loss:2.4607, Accuracy:0.1725, Validation Loss:2.4412, Validation Accuracy:0.1938\n",
    "Epoch #61: Loss:2.4617, Accuracy:0.1713, Validation Loss:2.4410, Validation Accuracy:0.1905\n",
    "Epoch #62: Loss:2.4607, Accuracy:0.1700, Validation Loss:2.4419, Validation Accuracy:0.1921\n",
    "Epoch #63: Loss:2.4599, Accuracy:0.1676, Validation Loss:2.4407, Validation Accuracy:0.1938\n",
    "Epoch #64: Loss:2.4599, Accuracy:0.1692, Validation Loss:2.4423, Validation Accuracy:0.1905\n",
    "Epoch #65: Loss:2.4600, Accuracy:0.1704, Validation Loss:2.4368, Validation Accuracy:0.1888\n",
    "Epoch #66: Loss:2.4593, Accuracy:0.1704, Validation Loss:2.4367, Validation Accuracy:0.1888\n",
    "Epoch #67: Loss:2.4600, Accuracy:0.1713, Validation Loss:2.4388, Validation Accuracy:0.1921\n",
    "Epoch #68: Loss:2.4590, Accuracy:0.1741, Validation Loss:2.4341, Validation Accuracy:0.1856\n",
    "Epoch #69: Loss:2.4566, Accuracy:0.1704, Validation Loss:2.4333, Validation Accuracy:0.1856\n",
    "Epoch #70: Loss:2.4560, Accuracy:0.1704, Validation Loss:2.4345, Validation Accuracy:0.1888\n",
    "Epoch #71: Loss:2.4570, Accuracy:0.1721, Validation Loss:2.4325, Validation Accuracy:0.1856\n",
    "Epoch #72: Loss:2.4568, Accuracy:0.1708, Validation Loss:2.4314, Validation Accuracy:0.1872\n",
    "Epoch #73: Loss:2.4539, Accuracy:0.1717, Validation Loss:2.4376, Validation Accuracy:0.1872\n",
    "Epoch #74: Loss:2.4544, Accuracy:0.1721, Validation Loss:2.4333, Validation Accuracy:0.1938\n",
    "Epoch #75: Loss:2.4537, Accuracy:0.1729, Validation Loss:2.4319, Validation Accuracy:0.1872\n",
    "Epoch #76: Loss:2.4540, Accuracy:0.1717, Validation Loss:2.4356, Validation Accuracy:0.1856\n",
    "Epoch #77: Loss:2.4570, Accuracy:0.1737, Validation Loss:2.4439, Validation Accuracy:0.1905\n",
    "Epoch #78: Loss:2.4585, Accuracy:0.1749, Validation Loss:2.4407, Validation Accuracy:0.1888\n",
    "Epoch #79: Loss:2.4590, Accuracy:0.1745, Validation Loss:2.4416, Validation Accuracy:0.1888\n",
    "Epoch #80: Loss:2.4581, Accuracy:0.1725, Validation Loss:2.4436, Validation Accuracy:0.1856\n",
    "Epoch #81: Loss:2.4566, Accuracy:0.1721, Validation Loss:2.4422, Validation Accuracy:0.1856\n",
    "Epoch #82: Loss:2.4562, Accuracy:0.1717, Validation Loss:2.4429, Validation Accuracy:0.1839\n",
    "Epoch #83: Loss:2.4562, Accuracy:0.1733, Validation Loss:2.4444, Validation Accuracy:0.1872\n",
    "Epoch #84: Loss:2.4556, Accuracy:0.1733, Validation Loss:2.4440, Validation Accuracy:0.1839\n",
    "Epoch #85: Loss:2.4550, Accuracy:0.1721, Validation Loss:2.4417, Validation Accuracy:0.1872\n",
    "Epoch #86: Loss:2.4547, Accuracy:0.1741, Validation Loss:2.4435, Validation Accuracy:0.1872\n",
    "Epoch #87: Loss:2.4549, Accuracy:0.1733, Validation Loss:2.4407, Validation Accuracy:0.1872\n",
    "Epoch #88: Loss:2.4558, Accuracy:0.1745, Validation Loss:2.4395, Validation Accuracy:0.1888\n",
    "Epoch #89: Loss:2.4556, Accuracy:0.1754, Validation Loss:2.4419, Validation Accuracy:0.1905\n",
    "Epoch #90: Loss:2.4564, Accuracy:0.1754, Validation Loss:2.4424, Validation Accuracy:0.1921\n",
    "Epoch #91: Loss:2.4563, Accuracy:0.1770, Validation Loss:2.4401, Validation Accuracy:0.1921\n",
    "Epoch #92: Loss:2.4555, Accuracy:0.1766, Validation Loss:2.4393, Validation Accuracy:0.1888\n",
    "Epoch #93: Loss:2.4545, Accuracy:0.1762, Validation Loss:2.4422, Validation Accuracy:0.1905\n",
    "Epoch #94: Loss:2.4553, Accuracy:0.1758, Validation Loss:2.4362, Validation Accuracy:0.1938\n",
    "Epoch #95: Loss:2.4585, Accuracy:0.1758, Validation Loss:2.4376, Validation Accuracy:0.1905\n",
    "Epoch #96: Loss:2.4547, Accuracy:0.1758, Validation Loss:2.4413, Validation Accuracy:0.1905\n",
    "Epoch #97: Loss:2.4535, Accuracy:0.1758, Validation Loss:2.4369, Validation Accuracy:0.1839\n",
    "Epoch #98: Loss:2.4539, Accuracy:0.1762, Validation Loss:2.4377, Validation Accuracy:0.1856\n",
    "Epoch #99: Loss:2.4529, Accuracy:0.1770, Validation Loss:2.4411, Validation Accuracy:0.1872\n",
    "Epoch #100: Loss:2.4518, Accuracy:0.1778, Validation Loss:2.4372, Validation Accuracy:0.1872\n",
    "Epoch #101: Loss:2.4528, Accuracy:0.1782, Validation Loss:2.4391, Validation Accuracy:0.1872\n",
    "Epoch #102: Loss:2.4529, Accuracy:0.1758, Validation Loss:2.4374, Validation Accuracy:0.1905\n",
    "Epoch #103: Loss:2.4546, Accuracy:0.1766, Validation Loss:2.4373, Validation Accuracy:0.1921\n",
    "Epoch #104: Loss:2.4531, Accuracy:0.1762, Validation Loss:2.4356, Validation Accuracy:0.1888\n",
    "Epoch #105: Loss:2.4527, Accuracy:0.1749, Validation Loss:2.4377, Validation Accuracy:0.1888\n",
    "Epoch #106: Loss:2.4530, Accuracy:0.1745, Validation Loss:2.4363, Validation Accuracy:0.1905\n",
    "Epoch #107: Loss:2.4520, Accuracy:0.1741, Validation Loss:2.4373, Validation Accuracy:0.1905\n",
    "Epoch #108: Loss:2.4541, Accuracy:0.1745, Validation Loss:2.4358, Validation Accuracy:0.1872\n",
    "Epoch #109: Loss:2.4506, Accuracy:0.1749, Validation Loss:2.4407, Validation Accuracy:0.1888\n",
    "Epoch #110: Loss:2.4515, Accuracy:0.1749, Validation Loss:2.4339, Validation Accuracy:0.1839\n",
    "Epoch #111: Loss:2.4499, Accuracy:0.1754, Validation Loss:2.4361, Validation Accuracy:0.1856\n",
    "Epoch #112: Loss:2.4489, Accuracy:0.1762, Validation Loss:2.4371, Validation Accuracy:0.1872\n",
    "Epoch #113: Loss:2.4491, Accuracy:0.1766, Validation Loss:2.4341, Validation Accuracy:0.1856\n",
    "Epoch #114: Loss:2.4501, Accuracy:0.1758, Validation Loss:2.4345, Validation Accuracy:0.1872\n",
    "Epoch #115: Loss:2.4487, Accuracy:0.1762, Validation Loss:2.4365, Validation Accuracy:0.1872\n",
    "Epoch #116: Loss:2.4474, Accuracy:0.1754, Validation Loss:2.4328, Validation Accuracy:0.1856\n",
    "Epoch #117: Loss:2.4478, Accuracy:0.1774, Validation Loss:2.4372, Validation Accuracy:0.1856\n",
    "Epoch #118: Loss:2.4537, Accuracy:0.1762, Validation Loss:2.4360, Validation Accuracy:0.1839\n",
    "Epoch #119: Loss:2.4520, Accuracy:0.1745, Validation Loss:2.4343, Validation Accuracy:0.1839\n",
    "Epoch #120: Loss:2.4489, Accuracy:0.1758, Validation Loss:2.4411, Validation Accuracy:0.1839\n",
    "Epoch #121: Loss:2.4508, Accuracy:0.1758, Validation Loss:2.4330, Validation Accuracy:0.1872\n",
    "Epoch #122: Loss:2.4512, Accuracy:0.1733, Validation Loss:2.4347, Validation Accuracy:0.1872\n",
    "Epoch #123: Loss:2.4498, Accuracy:0.1766, Validation Loss:2.4433, Validation Accuracy:0.1806\n",
    "Epoch #124: Loss:2.4523, Accuracy:0.1786, Validation Loss:2.4347, Validation Accuracy:0.1839\n",
    "Epoch #125: Loss:2.4505, Accuracy:0.1762, Validation Loss:2.4365, Validation Accuracy:0.1888\n",
    "Epoch #126: Loss:2.4527, Accuracy:0.1713, Validation Loss:2.4383, Validation Accuracy:0.1888\n",
    "Epoch #127: Loss:2.4508, Accuracy:0.1737, Validation Loss:2.4345, Validation Accuracy:0.1872\n",
    "Epoch #128: Loss:2.4516, Accuracy:0.1741, Validation Loss:2.4385, Validation Accuracy:0.1839\n",
    "Epoch #129: Loss:2.4498, Accuracy:0.1741, Validation Loss:2.4412, Validation Accuracy:0.1839\n",
    "Epoch #130: Loss:2.4490, Accuracy:0.1754, Validation Loss:2.4322, Validation Accuracy:0.1856\n",
    "Epoch #131: Loss:2.4503, Accuracy:0.1721, Validation Loss:2.4402, Validation Accuracy:0.1839\n",
    "Epoch #132: Loss:2.4510, Accuracy:0.1721, Validation Loss:2.4340, Validation Accuracy:0.1839\n",
    "Epoch #133: Loss:2.4495, Accuracy:0.1741, Validation Loss:2.4366, Validation Accuracy:0.1856\n",
    "Epoch #134: Loss:2.4488, Accuracy:0.1721, Validation Loss:2.4349, Validation Accuracy:0.1856\n",
    "Epoch #135: Loss:2.4486, Accuracy:0.1725, Validation Loss:2.4326, Validation Accuracy:0.1888\n",
    "Epoch #136: Loss:2.4484, Accuracy:0.1733, Validation Loss:2.4381, Validation Accuracy:0.1872\n",
    "Epoch #137: Loss:2.4482, Accuracy:0.1741, Validation Loss:2.4330, Validation Accuracy:0.1856\n",
    "Epoch #138: Loss:2.4477, Accuracy:0.1737, Validation Loss:2.4333, Validation Accuracy:0.1872\n",
    "Epoch #139: Loss:2.4499, Accuracy:0.1741, Validation Loss:2.4379, Validation Accuracy:0.1856\n",
    "Epoch #140: Loss:2.4504, Accuracy:0.1749, Validation Loss:2.4320, Validation Accuracy:0.1872\n",
    "Epoch #141: Loss:2.4490, Accuracy:0.1762, Validation Loss:2.4413, Validation Accuracy:0.1856\n",
    "Epoch #142: Loss:2.4483, Accuracy:0.1749, Validation Loss:2.4327, Validation Accuracy:0.1856\n",
    "Epoch #143: Loss:2.4498, Accuracy:0.1729, Validation Loss:2.4380, Validation Accuracy:0.1888\n",
    "Epoch #144: Loss:2.4486, Accuracy:0.1745, Validation Loss:2.4336, Validation Accuracy:0.1905\n",
    "Epoch #145: Loss:2.4473, Accuracy:0.1754, Validation Loss:2.4383, Validation Accuracy:0.1839\n",
    "Epoch #146: Loss:2.4472, Accuracy:0.1762, Validation Loss:2.4331, Validation Accuracy:0.1888\n",
    "Epoch #147: Loss:2.4475, Accuracy:0.1741, Validation Loss:2.4338, Validation Accuracy:0.1905\n",
    "Epoch #148: Loss:2.4480, Accuracy:0.1729, Validation Loss:2.4370, Validation Accuracy:0.1856\n",
    "Epoch #149: Loss:2.4481, Accuracy:0.1725, Validation Loss:2.4322, Validation Accuracy:0.1888\n",
    "Epoch #150: Loss:2.4477, Accuracy:0.1721, Validation Loss:2.4383, Validation Accuracy:0.1839\n",
    "Epoch #151: Loss:2.4478, Accuracy:0.1749, Validation Loss:2.4338, Validation Accuracy:0.1888\n",
    "Epoch #152: Loss:2.4517, Accuracy:0.1717, Validation Loss:2.4319, Validation Accuracy:0.1921\n",
    "Epoch #153: Loss:2.4524, Accuracy:0.1737, Validation Loss:2.4440, Validation Accuracy:0.1856\n",
    "Epoch #154: Loss:2.4508, Accuracy:0.1749, Validation Loss:2.4325, Validation Accuracy:0.1905\n",
    "Epoch #155: Loss:2.4526, Accuracy:0.1713, Validation Loss:2.4353, Validation Accuracy:0.1921\n",
    "Epoch #156: Loss:2.4500, Accuracy:0.1708, Validation Loss:2.4366, Validation Accuracy:0.1839\n",
    "Epoch #157: Loss:2.4507, Accuracy:0.1717, Validation Loss:2.4381, Validation Accuracy:0.1888\n",
    "Epoch #158: Loss:2.4504, Accuracy:0.1745, Validation Loss:2.4382, Validation Accuracy:0.1856\n",
    "Epoch #159: Loss:2.4494, Accuracy:0.1770, Validation Loss:2.4363, Validation Accuracy:0.1839\n",
    "Epoch #160: Loss:2.4494, Accuracy:0.1758, Validation Loss:2.4378, Validation Accuracy:0.1839\n",
    "Epoch #161: Loss:2.4493, Accuracy:0.1749, Validation Loss:2.4366, Validation Accuracy:0.1856\n",
    "Epoch #162: Loss:2.4481, Accuracy:0.1741, Validation Loss:2.4346, Validation Accuracy:0.1888\n",
    "Epoch #163: Loss:2.4473, Accuracy:0.1729, Validation Loss:2.4379, Validation Accuracy:0.1888\n",
    "Epoch #164: Loss:2.4475, Accuracy:0.1733, Validation Loss:2.4355, Validation Accuracy:0.1905\n",
    "Epoch #165: Loss:2.4483, Accuracy:0.1729, Validation Loss:2.4341, Validation Accuracy:0.1888\n",
    "Epoch #166: Loss:2.4497, Accuracy:0.1729, Validation Loss:2.4376, Validation Accuracy:0.1872\n",
    "Epoch #167: Loss:2.4491, Accuracy:0.1721, Validation Loss:2.4388, Validation Accuracy:0.1856\n",
    "Epoch #168: Loss:2.4496, Accuracy:0.1725, Validation Loss:2.4357, Validation Accuracy:0.1888\n",
    "Epoch #169: Loss:2.4502, Accuracy:0.1692, Validation Loss:2.4393, Validation Accuracy:0.1905\n",
    "Epoch #170: Loss:2.4502, Accuracy:0.1704, Validation Loss:2.4372, Validation Accuracy:0.1839\n",
    "Epoch #171: Loss:2.4481, Accuracy:0.1729, Validation Loss:2.4360, Validation Accuracy:0.1905\n",
    "Epoch #172: Loss:2.4498, Accuracy:0.1708, Validation Loss:2.4358, Validation Accuracy:0.2003\n",
    "Epoch #173: Loss:2.4491, Accuracy:0.1684, Validation Loss:2.4396, Validation Accuracy:0.1970\n",
    "Epoch #174: Loss:2.4494, Accuracy:0.1713, Validation Loss:2.4385, Validation Accuracy:0.1856\n",
    "Epoch #175: Loss:2.4496, Accuracy:0.1749, Validation Loss:2.4367, Validation Accuracy:0.1823\n",
    "Epoch #176: Loss:2.4494, Accuracy:0.1704, Validation Loss:2.4379, Validation Accuracy:0.1872\n",
    "Epoch #177: Loss:2.4488, Accuracy:0.1762, Validation Loss:2.4375, Validation Accuracy:0.1905\n",
    "Epoch #178: Loss:2.4491, Accuracy:0.1725, Validation Loss:2.4376, Validation Accuracy:0.1872\n",
    "Epoch #179: Loss:2.4483, Accuracy:0.1749, Validation Loss:2.4377, Validation Accuracy:0.1856\n",
    "Epoch #180: Loss:2.4487, Accuracy:0.1774, Validation Loss:2.4372, Validation Accuracy:0.1921\n",
    "Epoch #181: Loss:2.4488, Accuracy:0.1733, Validation Loss:2.4368, Validation Accuracy:0.1888\n",
    "Epoch #182: Loss:2.4479, Accuracy:0.1749, Validation Loss:2.4359, Validation Accuracy:0.1888\n",
    "Epoch #183: Loss:2.4490, Accuracy:0.1737, Validation Loss:2.4386, Validation Accuracy:0.1856\n",
    "Epoch #184: Loss:2.4485, Accuracy:0.1766, Validation Loss:2.4383, Validation Accuracy:0.1806\n",
    "Epoch #185: Loss:2.4486, Accuracy:0.1758, Validation Loss:2.4375, Validation Accuracy:0.1856\n",
    "Epoch #186: Loss:2.4482, Accuracy:0.1745, Validation Loss:2.4384, Validation Accuracy:0.1806\n",
    "Epoch #187: Loss:2.4476, Accuracy:0.1758, Validation Loss:2.4374, Validation Accuracy:0.1839\n",
    "Epoch #188: Loss:2.4474, Accuracy:0.1745, Validation Loss:2.4378, Validation Accuracy:0.1839\n",
    "Epoch #189: Loss:2.4480, Accuracy:0.1741, Validation Loss:2.4377, Validation Accuracy:0.1856\n",
    "Epoch #190: Loss:2.4489, Accuracy:0.1758, Validation Loss:2.4349, Validation Accuracy:0.1839\n",
    "Epoch #191: Loss:2.4494, Accuracy:0.1717, Validation Loss:2.4403, Validation Accuracy:0.1921\n",
    "Epoch #192: Loss:2.4480, Accuracy:0.1717, Validation Loss:2.4346, Validation Accuracy:0.1856\n",
    "Epoch #193: Loss:2.4482, Accuracy:0.1737, Validation Loss:2.4412, Validation Accuracy:0.1921\n",
    "Epoch #194: Loss:2.4477, Accuracy:0.1733, Validation Loss:2.4339, Validation Accuracy:0.1839\n",
    "Epoch #195: Loss:2.4469, Accuracy:0.1725, Validation Loss:2.4410, Validation Accuracy:0.1938\n",
    "Epoch #196: Loss:2.4470, Accuracy:0.1713, Validation Loss:2.4346, Validation Accuracy:0.1856\n",
    "Epoch #197: Loss:2.4464, Accuracy:0.1770, Validation Loss:2.4411, Validation Accuracy:0.1938\n",
    "Epoch #198: Loss:2.4460, Accuracy:0.1754, Validation Loss:2.4353, Validation Accuracy:0.1872\n",
    "Epoch #199: Loss:2.4469, Accuracy:0.1770, Validation Loss:2.4385, Validation Accuracy:0.1905\n",
    "Epoch #200: Loss:2.4485, Accuracy:0.1745, Validation Loss:2.4390, Validation Accuracy:0.1905\n",
    "Epoch #201: Loss:2.4474, Accuracy:0.1745, Validation Loss:2.4359, Validation Accuracy:0.1839\n",
    "Epoch #202: Loss:2.4459, Accuracy:0.1766, Validation Loss:2.4389, Validation Accuracy:0.1888\n",
    "Epoch #203: Loss:2.4458, Accuracy:0.1778, Validation Loss:2.4372, Validation Accuracy:0.1839\n",
    "Epoch #204: Loss:2.4447, Accuracy:0.1741, Validation Loss:2.4361, Validation Accuracy:0.1872\n",
    "Epoch #205: Loss:2.4445, Accuracy:0.1762, Validation Loss:2.4336, Validation Accuracy:0.1921\n",
    "Epoch #206: Loss:2.4426, Accuracy:0.1799, Validation Loss:2.4374, Validation Accuracy:0.1888\n",
    "Epoch #207: Loss:2.4444, Accuracy:0.1774, Validation Loss:2.4332, Validation Accuracy:0.1839\n",
    "Epoch #208: Loss:2.4436, Accuracy:0.1770, Validation Loss:2.4375, Validation Accuracy:0.1888\n",
    "Epoch #209: Loss:2.4450, Accuracy:0.1754, Validation Loss:2.4333, Validation Accuracy:0.1905\n",
    "Epoch #210: Loss:2.4438, Accuracy:0.1774, Validation Loss:2.4382, Validation Accuracy:0.1839\n",
    "Epoch #211: Loss:2.4467, Accuracy:0.1754, Validation Loss:2.4379, Validation Accuracy:0.1856\n",
    "Epoch #212: Loss:2.4471, Accuracy:0.1774, Validation Loss:2.4398, Validation Accuracy:0.1856\n",
    "Epoch #213: Loss:2.4472, Accuracy:0.1766, Validation Loss:2.4388, Validation Accuracy:0.1839\n",
    "Epoch #214: Loss:2.4469, Accuracy:0.1770, Validation Loss:2.4417, Validation Accuracy:0.1872\n",
    "Epoch #215: Loss:2.4481, Accuracy:0.1758, Validation Loss:2.4403, Validation Accuracy:0.1921\n",
    "Epoch #216: Loss:2.4485, Accuracy:0.1745, Validation Loss:2.4384, Validation Accuracy:0.1856\n",
    "Epoch #217: Loss:2.4514, Accuracy:0.1725, Validation Loss:2.4460, Validation Accuracy:0.1839\n",
    "Epoch #218: Loss:2.4488, Accuracy:0.1737, Validation Loss:2.4373, Validation Accuracy:0.1839\n",
    "Epoch #219: Loss:2.4463, Accuracy:0.1749, Validation Loss:2.4452, Validation Accuracy:0.1888\n",
    "Epoch #220: Loss:2.4482, Accuracy:0.1745, Validation Loss:2.4377, Validation Accuracy:0.1839\n",
    "Epoch #221: Loss:2.4482, Accuracy:0.1737, Validation Loss:2.4397, Validation Accuracy:0.1856\n",
    "Epoch #222: Loss:2.4504, Accuracy:0.1737, Validation Loss:2.4413, Validation Accuracy:0.1888\n",
    "Epoch #223: Loss:2.4512, Accuracy:0.1729, Validation Loss:2.4368, Validation Accuracy:0.1839\n",
    "Epoch #224: Loss:2.4505, Accuracy:0.1741, Validation Loss:2.4492, Validation Accuracy:0.1856\n",
    "Epoch #225: Loss:2.4474, Accuracy:0.1758, Validation Loss:2.4380, Validation Accuracy:0.1856\n",
    "Epoch #226: Loss:2.4485, Accuracy:0.1766, Validation Loss:2.4398, Validation Accuracy:0.1856\n",
    "Epoch #227: Loss:2.4473, Accuracy:0.1741, Validation Loss:2.4423, Validation Accuracy:0.1888\n",
    "Epoch #228: Loss:2.4480, Accuracy:0.1754, Validation Loss:2.4380, Validation Accuracy:0.1888\n",
    "Epoch #229: Loss:2.4475, Accuracy:0.1766, Validation Loss:2.4436, Validation Accuracy:0.1888\n",
    "Epoch #230: Loss:2.4477, Accuracy:0.1758, Validation Loss:2.4384, Validation Accuracy:0.1888\n",
    "Epoch #231: Loss:2.4481, Accuracy:0.1745, Validation Loss:2.4393, Validation Accuracy:0.1839\n",
    "Epoch #232: Loss:2.4506, Accuracy:0.1713, Validation Loss:2.4442, Validation Accuracy:0.1872\n",
    "Epoch #233: Loss:2.4474, Accuracy:0.1770, Validation Loss:2.4368, Validation Accuracy:0.1856\n",
    "Epoch #234: Loss:2.4482, Accuracy:0.1770, Validation Loss:2.4407, Validation Accuracy:0.1905\n",
    "Epoch #235: Loss:2.4490, Accuracy:0.1754, Validation Loss:2.4428, Validation Accuracy:0.1872\n",
    "Epoch #236: Loss:2.4460, Accuracy:0.1725, Validation Loss:2.4366, Validation Accuracy:0.1839\n",
    "Epoch #237: Loss:2.4473, Accuracy:0.1754, Validation Loss:2.4414, Validation Accuracy:0.1872\n",
    "Epoch #238: Loss:2.4461, Accuracy:0.1754, Validation Loss:2.4404, Validation Accuracy:0.1872\n",
    "Epoch #239: Loss:2.4455, Accuracy:0.1749, Validation Loss:2.4391, Validation Accuracy:0.1872\n",
    "Epoch #240: Loss:2.4456, Accuracy:0.1733, Validation Loss:2.4369, Validation Accuracy:0.1872\n",
    "Epoch #241: Loss:2.4449, Accuracy:0.1737, Validation Loss:2.4362, Validation Accuracy:0.1872\n",
    "Epoch #242: Loss:2.4450, Accuracy:0.1745, Validation Loss:2.4338, Validation Accuracy:0.1839\n",
    "Epoch #243: Loss:2.4437, Accuracy:0.1717, Validation Loss:2.4296, Validation Accuracy:0.1905\n",
    "Epoch #244: Loss:2.4416, Accuracy:0.1725, Validation Loss:2.4304, Validation Accuracy:0.1905\n",
    "Epoch #245: Loss:2.4409, Accuracy:0.1737, Validation Loss:2.4308, Validation Accuracy:0.1905\n",
    "Epoch #246: Loss:2.4418, Accuracy:0.1713, Validation Loss:2.4309, Validation Accuracy:0.1921\n",
    "Epoch #247: Loss:2.4423, Accuracy:0.1713, Validation Loss:2.4297, Validation Accuracy:0.1905\n",
    "Epoch #248: Loss:2.4415, Accuracy:0.1729, Validation Loss:2.4295, Validation Accuracy:0.1905\n",
    "Epoch #249: Loss:2.4405, Accuracy:0.1713, Validation Loss:2.4314, Validation Accuracy:0.1905\n",
    "Epoch #250: Loss:2.4408, Accuracy:0.1725, Validation Loss:2.4286, Validation Accuracy:0.1954\n",
    "Epoch #251: Loss:2.4449, Accuracy:0.1713, Validation Loss:2.4296, Validation Accuracy:0.1872\n",
    "Epoch #252: Loss:2.4444, Accuracy:0.1688, Validation Loss:2.4376, Validation Accuracy:0.1823\n",
    "Epoch #253: Loss:2.4457, Accuracy:0.1696, Validation Loss:2.4304, Validation Accuracy:0.1921\n",
    "Epoch #254: Loss:2.4478, Accuracy:0.1700, Validation Loss:2.4358, Validation Accuracy:0.1773\n",
    "Epoch #255: Loss:2.4499, Accuracy:0.1634, Validation Loss:2.4440, Validation Accuracy:0.1691\n",
    "Epoch #256: Loss:2.4498, Accuracy:0.1589, Validation Loss:2.4322, Validation Accuracy:0.1872\n",
    "Epoch #257: Loss:2.4457, Accuracy:0.1655, Validation Loss:2.4338, Validation Accuracy:0.1790\n",
    "Epoch #258: Loss:2.4456, Accuracy:0.1667, Validation Loss:2.4344, Validation Accuracy:0.1856\n",
    "Epoch #259: Loss:2.4453, Accuracy:0.1692, Validation Loss:2.4349, Validation Accuracy:0.1823\n",
    "Epoch #260: Loss:2.4447, Accuracy:0.1696, Validation Loss:2.4331, Validation Accuracy:0.1691\n",
    "Epoch #261: Loss:2.4449, Accuracy:0.1651, Validation Loss:2.4367, Validation Accuracy:0.1839\n",
    "Epoch #262: Loss:2.4431, Accuracy:0.1713, Validation Loss:2.4328, Validation Accuracy:0.1856\n",
    "Epoch #263: Loss:2.4453, Accuracy:0.1704, Validation Loss:2.4287, Validation Accuracy:0.1658\n",
    "Epoch #264: Loss:2.4434, Accuracy:0.1680, Validation Loss:2.4333, Validation Accuracy:0.1806\n",
    "Epoch #265: Loss:2.4433, Accuracy:0.1696, Validation Loss:2.4293, Validation Accuracy:0.1938\n",
    "Epoch #266: Loss:2.4441, Accuracy:0.1737, Validation Loss:2.4277, Validation Accuracy:0.1806\n",
    "Epoch #267: Loss:2.4420, Accuracy:0.1684, Validation Loss:2.4309, Validation Accuracy:0.1773\n",
    "Epoch #268: Loss:2.4428, Accuracy:0.1700, Validation Loss:2.4295, Validation Accuracy:0.1757\n",
    "Epoch #269: Loss:2.4427, Accuracy:0.1725, Validation Loss:2.4323, Validation Accuracy:0.1757\n",
    "Epoch #270: Loss:2.4421, Accuracy:0.1700, Validation Loss:2.4304, Validation Accuracy:0.1905\n",
    "Epoch #271: Loss:2.4436, Accuracy:0.1684, Validation Loss:2.4313, Validation Accuracy:0.1691\n",
    "Epoch #272: Loss:2.4435, Accuracy:0.1671, Validation Loss:2.4363, Validation Accuracy:0.1856\n",
    "Epoch #273: Loss:2.4422, Accuracy:0.1713, Validation Loss:2.4335, Validation Accuracy:0.1675\n",
    "Epoch #274: Loss:2.4408, Accuracy:0.1663, Validation Loss:2.4400, Validation Accuracy:0.1675\n",
    "Epoch #275: Loss:2.4410, Accuracy:0.1749, Validation Loss:2.4379, Validation Accuracy:0.1888\n",
    "Epoch #276: Loss:2.4410, Accuracy:0.1713, Validation Loss:2.4361, Validation Accuracy:0.1872\n",
    "Epoch #277: Loss:2.4407, Accuracy:0.1733, Validation Loss:2.4377, Validation Accuracy:0.1823\n",
    "Epoch #278: Loss:2.4406, Accuracy:0.1737, Validation Loss:2.4391, Validation Accuracy:0.1626\n",
    "Epoch #279: Loss:2.4400, Accuracy:0.1741, Validation Loss:2.4372, Validation Accuracy:0.1839\n",
    "Epoch #280: Loss:2.4407, Accuracy:0.1721, Validation Loss:2.4367, Validation Accuracy:0.1839\n",
    "Epoch #281: Loss:2.4392, Accuracy:0.1704, Validation Loss:2.4369, Validation Accuracy:0.1724\n",
    "Epoch #282: Loss:2.4383, Accuracy:0.1700, Validation Loss:2.4351, Validation Accuracy:0.1757\n",
    "Epoch #283: Loss:2.4388, Accuracy:0.1754, Validation Loss:2.4321, Validation Accuracy:0.1856\n",
    "Epoch #284: Loss:2.4401, Accuracy:0.1799, Validation Loss:2.4371, Validation Accuracy:0.1872\n",
    "Epoch #285: Loss:2.4402, Accuracy:0.1774, Validation Loss:2.4317, Validation Accuracy:0.1872\n",
    "Epoch #286: Loss:2.4392, Accuracy:0.1749, Validation Loss:2.4302, Validation Accuracy:0.1856\n",
    "Epoch #287: Loss:2.4396, Accuracy:0.1749, Validation Loss:2.4355, Validation Accuracy:0.1757\n",
    "Epoch #288: Loss:2.4389, Accuracy:0.1807, Validation Loss:2.4301, Validation Accuracy:0.1856\n",
    "Epoch #289: Loss:2.4407, Accuracy:0.1766, Validation Loss:2.4342, Validation Accuracy:0.1856\n",
    "Epoch #290: Loss:2.4391, Accuracy:0.1828, Validation Loss:2.4364, Validation Accuracy:0.1806\n",
    "Epoch #291: Loss:2.4396, Accuracy:0.1815, Validation Loss:2.4324, Validation Accuracy:0.1888\n",
    "Epoch #292: Loss:2.4404, Accuracy:0.1795, Validation Loss:2.4339, Validation Accuracy:0.1872\n",
    "Epoch #293: Loss:2.4401, Accuracy:0.1782, Validation Loss:2.4311, Validation Accuracy:0.1888\n",
    "Epoch #294: Loss:2.4403, Accuracy:0.1762, Validation Loss:2.4333, Validation Accuracy:0.1856\n",
    "Epoch #295: Loss:2.4404, Accuracy:0.1774, Validation Loss:2.4305, Validation Accuracy:0.1856\n",
    "Epoch #296: Loss:2.4381, Accuracy:0.1823, Validation Loss:2.4336, Validation Accuracy:0.1839\n",
    "Epoch #297: Loss:2.4392, Accuracy:0.1823, Validation Loss:2.4317, Validation Accuracy:0.1839\n",
    "Epoch #298: Loss:2.4390, Accuracy:0.1823, Validation Loss:2.4335, Validation Accuracy:0.1691\n",
    "Epoch #299: Loss:2.4389, Accuracy:0.1774, Validation Loss:2.4310, Validation Accuracy:0.1741\n",
    "Epoch #300: Loss:2.4391, Accuracy:0.1778, Validation Loss:2.4325, Validation Accuracy:0.1724\n",
    "\n",
    "Test:\n",
    "Test Loss:2.43251157, Accuracy:0.1724\n",
    "Labels: ['yd', 'mb', 'eo', 'ek', 'by', 'eb', 'ck', 'aa', 'ds', 'sk', 'my', 'sg', 'ib', 'eg', 'ce']\n",
    "Confusion Matrix:\n",
    "      yd  mb  eo  ek  by  eb  ck  aa  ds  sk  my  sg  ib  eg  ce\n",
    "t:yd  32   0   0   0  16   0   0   0   0   0   0  13   0   1   0\n",
    "t:mb  11   0   0   0  10   6   0   0   0   0   0  13   0  12   0\n",
    "t:eo   5   0   0   0  16   5   0   0   0   0   0   3   0   5   0\n",
    "t:ek   6   0   0   0  18   3   0   0   1   0   0   3   0  17   0\n",
    "t:by   2   0   0   0  16   3   0   0   2   0   0   2   0  15   0\n",
    "t:eb   4   0   0   0  15   4   0   0   2   0   0   8   0  17   0\n",
    "t:ck   0   0   0   0   5   3   0   0   2   0   0   3   0  10   0\n",
    "t:aa   1   0   0   0   4   3   0   0   7   0   0   5   0  14   0\n",
    "t:ds   1   0   0   0   7   0   0   0   6   0   0   1   0  16   0\n",
    "t:sk   1   0   0   0  13   1   0   0   2   0   0   4   0  12   0\n",
    "t:my   3   0   0   0   1   0   0   0   2   0   0   6   0   8   0\n",
    "t:sg  14   0   0   0  20   2   0   0   1   0   0  12   0   2   0\n",
    "t:ib  24   0   0   0   6   1   0   0   1   0   0  16   0   6   0\n",
    "t:eg   0   0   0   0   4   3   0   0   7   0   0   1   0  35   0\n",
    "t:ce   2   0   0   0   7   1   0   0   0   0   0   8   0   9   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.30      0.52      0.38        62\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          by       0.10      0.40      0.16        40\n",
    "          eb       0.11      0.08      0.09        50\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ds       0.18      0.19      0.19        31\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          my       0.00      0.00      0.00        20\n",
    "          sg       0.12      0.24      0.16        51\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          eg       0.20      0.70      0.31        50\n",
    "          ce       0.00      0.00      0.00        27\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.07      0.14      0.09       609\n",
    "weighted avg       0.08      0.17      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 16:35:30 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 31 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6981085094520805, 2.6859275738789727, 2.6788144201676443, 2.676855460763565, 2.670084941563348, 2.665381870442032, 2.660519364236415, 2.655144037284287, 2.6488648365283836, 2.64089518697391, 2.6310599578425213, 2.6188258694114745, 2.6060036696823947, 2.588563015504032, 2.5734729563269907, 2.558656880421004, 2.541848706885903, 2.529283862591573, 2.5091666055626076, 2.5040253931274163, 2.506649078015232, 2.4797075730434974, 2.47985302481941, 2.4609724523985914, 2.462264703803853, 2.456957074025973, 2.4600304343626025, 2.450366277412828, 2.457921252070585, 2.4487186734899513, 2.4562177215695185, 2.4534871942304037, 2.551967070803462, 2.570325513974395, 2.4585375406080474, 2.4611188907341415, 2.4789501575413597, 2.469597992247157, 2.460566587244544, 2.45534801013364, 2.4612321137207482, 2.454635555521021, 2.447032552243062, 2.443863653001331, 2.4457050344626894, 2.447786641238358, 2.45088660305944, 2.4511067017741586, 2.4528478284187503, 2.452321284705978, 2.4548032643955526, 2.4552597893869934, 2.4572573174005266, 2.452093409004274, 2.4528050336540232, 2.4510430530178526, 2.449310770567219, 2.447698886367096, 2.4470502649034773, 2.4412301823814904, 2.4410208825798847, 2.4419476155968525, 2.4407000784412003, 2.4422662724023576, 2.4367640652680045, 2.436687117922678, 2.4387540175213993, 2.4340886392421126, 2.4332909419618804, 2.434471982257511, 2.4325398020752154, 2.431373250895533, 2.4375635536237694, 2.4333189825706296, 2.43190142086574, 2.4356488876154856, 2.4439123663409004, 2.440700084705071, 2.4415551453388384, 2.443607556996087, 2.4421699176280955, 2.442870160433263, 2.444355771655128, 2.443958044052124, 2.4417356129350334, 2.443515103047313, 2.4407429311467315, 2.4394778371444477, 2.44194583470011, 2.4424134267766293, 2.4401084887374602, 2.43931162807546, 2.4421565971155275, 2.436175955144447, 2.4375977249959813, 2.441256861381343, 2.4369077705984634, 2.4377223912914006, 2.441148512077645, 2.437153112320673, 2.439103429149133, 2.4374420631108027, 2.437273334008328, 2.4355884819782427, 2.437699187173828, 2.4362858414454216, 2.4373093669246177, 2.4357533122126886, 2.4407274343305816, 2.433867881646493, 2.4361259999925085, 2.4370504302540046, 2.4340816671625145, 2.4344958280303404, 2.4365066331008385, 2.432839890027477, 2.4371899705019295, 2.436013501265953, 2.4342521540637088, 2.4410582434367663, 2.433040075114208, 2.4346814249536672, 2.4432671285419434, 2.434654938176348, 2.436458618965838, 2.438284680956886, 2.4345232128901237, 2.438466577498588, 2.441170525276798, 2.432214082363987, 2.4401680170413114, 2.43399800259883, 2.4366002184612605, 2.434944318824605, 2.432597147811614, 2.438119883607761, 2.4330312853376266, 2.4333498712830943, 2.4378595528344214, 2.4319938693336276, 2.441336998994323, 2.4326779317777536, 2.4380319447352967, 2.4336431519738557, 2.4382500272666294, 2.433072347359117, 2.433842015383866, 2.436989627252463, 2.4321816836671877, 2.438335628149349, 2.4337766965230307, 2.431905693608552, 2.444021584365168, 2.4325086851229614, 2.435335163608169, 2.4366155594636263, 2.438091764309136, 2.4381711784450486, 2.4363293158401214, 2.4378017271289294, 2.4366008197928495, 2.4346402294334326, 2.4378896597375226, 2.435498843247863, 2.4340768130542023, 2.437601798077914, 2.438845612536902, 2.4356996406279565, 2.439289522484214, 2.4371647376732284, 2.4359751655941917, 2.435772009866774, 2.4396439178040854, 2.4384522246218276, 2.436656468020284, 2.4379070427617417, 2.4375077163057375, 2.4375868031544052, 2.437653428032285, 2.437178271157401, 2.43683539000638, 2.4358711669401973, 2.438562801123057, 2.438263641789629, 2.4374588945229063, 2.4383995865757635, 2.437369723625371, 2.437763849502714, 2.4377138634229136, 2.4348524462413317, 2.440337172478486, 2.434641835333287, 2.4411551235931848, 2.4338552603384938, 2.440953679859932, 2.4346395999144255, 2.441076340933739, 2.435310638205367, 2.438451614285925, 2.4390250137091076, 2.435870848265775, 2.438929888219473, 2.437203205669259, 2.436071597492362, 2.433642205346394, 2.437400846450004, 2.433191976328006, 2.4374613570070816, 2.4332576925531395, 2.4381518955105435, 2.437862140596011, 2.439766087555533, 2.438793278680059, 2.441678566689953, 2.440267113824979, 2.4383615265143135, 2.4459978336183896, 2.437318205637689, 2.4451582423963374, 2.437692165374756, 2.439721405016769, 2.441316875918158, 2.436800608298266, 2.449169582921296, 2.4380021013062576, 2.439789523827814, 2.442266442309851, 2.437987023777954, 2.4435896587685018, 2.438430315168033, 2.439344823458316, 2.4442436742077907, 2.436818521402544, 2.4407182397513556, 2.4428235619330447, 2.4365530264592916, 2.4414413034035065, 2.4403691068658686, 2.4390692648237757, 2.436895240899573, 2.4361828172148154, 2.4338080679450327, 2.429617864157766, 2.4303980821067674, 2.430817504234502, 2.430858226441006, 2.429726579114917, 2.4294690032702166, 2.43141558683173, 2.4286098159201237, 2.4295772225985974, 2.4376114636219195, 2.4304492035131346, 2.435754271367892, 2.4440232763932452, 2.4322263286227273, 2.433757361911592, 2.4344461433993185, 2.434919971550627, 2.4330996503970894, 2.4367405060672604, 2.4328068398881233, 2.4286777750024653, 2.4332637050860426, 2.4292579350996095, 2.4277023771909265, 2.4308926530659494, 2.4295106462657157, 2.4323332818662395, 2.430435547883483, 2.4313062787643207, 2.4362938736851385, 2.433486875446363, 2.4399907530234954, 2.4379351276090775, 2.436096098622665, 2.4376675499288125, 2.439110048886003, 2.4371609930530167, 2.4366856921091062, 2.4368997136947557, 2.4350630888602223, 2.432143850279559, 2.437126893323826, 2.4316616344138713, 2.4301663537330813, 2.435541346742602, 2.430109499710534, 2.434171730270135, 2.4363932527344803, 2.4323894452970407, 2.433912192267933, 2.4311481986335544, 2.433326567726574, 2.4304604910081635, 2.433626651763916, 2.4316932145010663, 2.4335417524347163, 2.4309858625941283, 2.432511222773585], 'val_acc': [0.08045976980908946, 0.10837438373485418, 0.09359605891143748, 0.1116584562780626, 0.0853858784832097, 0.0886699507327992, 0.09359605861170148, 0.10673234761005944, 0.12643678160307834, 0.14942528705046879, 0.1510673232731365, 0.1510673232731365, 0.14614121489875226, 0.14942528714834175, 0.16584564750319827, 0.16420361147627652, 0.17077175607332848, 0.16091954021765092, 0.16091954021765092, 0.147783251023547, 0.15106732317526353, 0.15763546687922456, 0.16912972074375168, 0.18062397272422395, 0.18719211712552997, 0.18062397282209694, 0.17733990037676148, 0.17898193759039313, 0.18226600875114574, 0.17898193650155622, 0.1789819365994292, 0.18390804516955941, 0.15599343075442978, 0.14614121509449823, 0.19047618947299244, 0.17569786434983972, 0.18719211722340295, 0.18555008109860818, 0.167487683725866, 0.17405582822504498, 0.1707717558775825, 0.17569786425196673, 0.1707717569664194, 0.18062397272422395, 0.17569786534080364, 0.18555008100073522, 0.18390804497381344, 0.18390804497381344, 0.18555008119648117, 0.18062397291996993, 0.18555008119648117, 0.18390804497381344, 0.18555008109860818, 0.180623972626351, 0.1822660088490187, 0.1888341533481977, 0.1888341533481977, 0.18883415433916162, 0.1921182254999142, 0.19376026162470894, 0.19047619046395636, 0.1921182254999142, 0.19376026172258193, 0.19047618937511945, 0.18883415433916162, 0.18883415433916162, 0.1921182254999142, 0.1855500820895721, 0.1855500820895721, 0.1888341532503247, 0.1855500820895721, 0.18719211821436688, 0.18719211712552997, 0.19376026172258193, 0.18719211821436688, 0.18555008100073522, 0.1904761895708654, 0.1888341532503247, 0.1888341533481977, 0.18555008109860818, 0.18555008109860818, 0.18390804497381344, 0.18719211722340295, 0.18390804497381344, 0.18719211722340295, 0.18719211722340295, 0.18719211712552997, 0.1888341532503247, 0.19047618937511945, 0.1921182254999142, 0.1921182254999142, 0.1888341532503247, 0.19047618937511945, 0.19376026162470894, 0.19047618937511945, 0.19047618937511945, 0.18390804487594048, 0.18555008100073522, 0.18719211712552997, 0.18719211712552997, 0.18719211712552997, 0.19047618937511945, 0.1921182254999142, 0.1888341532503247, 0.1888341532503247, 0.19047618937511945, 0.19047618937511945, 0.18719211712552997, 0.1888341533481977, 0.18390804487594048, 0.18555008100073522, 0.18719211712552997, 0.18555008100073522, 0.18719211712552997, 0.18719211712552997, 0.18555008100073522, 0.18555008100073522, 0.18390804487594048, 0.18390804487594048, 0.18390804487594048, 0.18719211712552997, 0.18719211712552997, 0.180623972626351, 0.18390804487594048, 0.1888341532503247, 0.1888341532503247, 0.18719211712552997, 0.18390804487594048, 0.18390804487594048, 0.18555008100073522, 0.18390804487594048, 0.18390804487594048, 0.18555008100073522, 0.18555008100073522, 0.1888341532503247, 0.18719211712552997, 0.18555008100073522, 0.18719211712552997, 0.18555008100073522, 0.18719211712552997, 0.18555008100073522, 0.18555008100073522, 0.1888341533481977, 0.19047618937511945, 0.18390804487594048, 0.1888341532503247, 0.19047618947299244, 0.18555008100073522, 0.1888341532503247, 0.18390804487594048, 0.1888341533481977, 0.1921182265887511, 0.18555008129435416, 0.19047619046395636, 0.19211822559778718, 0.18390804487594048, 0.1888341533481977, 0.18555008100073522, 0.18390804596477736, 0.18390804487594048, 0.18555008100073522, 0.18883415433916162, 0.1888341532503247, 0.19047618937511945, 0.1888341532503247, 0.18719211712552997, 0.18555008100073522, 0.18883415433916162, 0.19047618947299244, 0.18390804487594048, 0.19047618937511945, 0.2003284063196339, 0.1970443340700444, 0.18555008109860818, 0.18226600983998262, 0.18719211722340295, 0.1904761895708654, 0.18719211712552997, 0.18555008109860818, 0.19211822559778718, 0.1888341532503247, 0.1888341532503247, 0.18555008100073522, 0.180623972626351, 0.18555008100073522, 0.180623972626351, 0.18390804487594048, 0.18390804487594048, 0.18555008100073522, 0.18390804487594048, 0.19211822579353313, 0.18555008100073522, 0.19211822579353313, 0.18390804487594048, 0.19376026191832788, 0.18555008100073522, 0.19376026191832788, 0.18719211712552997, 0.1904761895708654, 0.1904761895708654, 0.18390804487594048, 0.18883415344607066, 0.18390804487594048, 0.18719211712552997, 0.1921182254999142, 0.1888341532503247, 0.18390804487594048, 0.1888341532503247, 0.19047618937511945, 0.18390804487594048, 0.18555008100073522, 0.18555008100073522, 0.18390804487594048, 0.18719211722340295, 0.19211822559778718, 0.18555008100073522, 0.18390804497381344, 0.18390804487594048, 0.1888341533481977, 0.18390804487594048, 0.18555008100073522, 0.1888341533481977, 0.18390804487594048, 0.18555008109860818, 0.18555008100073522, 0.18555008100073522, 0.1888341533481977, 0.1888341533481977, 0.1888341533481977, 0.1888341533481977, 0.18390804487594048, 0.18719211722340295, 0.18555008100073522, 0.19047618947299244, 0.18719211722340295, 0.18390804487594048, 0.18719211712552997, 0.18719211712552997, 0.18719211712552997, 0.18719211722340295, 0.18719211722340295, 0.18390804497381344, 0.19047618947299244, 0.19047618947299244, 0.19047618947299244, 0.19211822559778718, 0.19047618937511945, 0.19047618937511945, 0.19047618947299244, 0.19540229774950368, 0.18719211821436688, 0.18226600983998262, 0.1921182265887511, 0.1773399013677254, 0.16912972084162467, 0.1871921181164939, 0.17898193759039313, 0.18555008100073522, 0.18226600974210966, 0.1691297199485337, 0.18390804497381344, 0.18555008100073522, 0.16584564750319827, 0.18062397291996993, 0.19376026182045492, 0.18062397291996993, 0.1773399007682534, 0.17569786454558567, 0.17569786454558567, 0.19047618947299244, 0.16912971975278776, 0.18555008109860818, 0.16748768362799302, 0.16748768382373896, 0.1888341533481977, 0.18719211722340295, 0.1822660088490187, 0.16256157554722772, 0.18390804497381344, 0.18390804497381344, 0.17241379229599618, 0.1756978644477127, 0.18555008139222715, 0.1871921175170219, 0.18719211761489485, 0.18555008109860818, 0.1756978644477127, 0.18555008119648117, 0.1855500814901001, 0.18062397311571587, 0.18883415373968962, 0.18719211761489485, 0.18883415344607066, 0.18555008129435416, 0.18555008109860818, 0.18390804497381344, 0.18390804497381344, 0.1691297199485337, 0.1740558291181359, 0.17241379200237725], 'loss': [2.7069867379856305, 2.6943589299611244, 2.6840131393448283, 2.6793232033629684, 2.675098219642404, 2.668539737627002, 2.6635206163786274, 2.658824604198918, 2.653375321589946, 2.646314777436932, 2.6379535824855984, 2.6279074886251523, 2.615869028220676, 2.600996268309607, 2.5857981673011543, 2.5696197072093736, 2.5557639308044307, 2.541348348359063, 2.527069662728594, 2.5183732776916004, 2.520680913494353, 2.5044265157633006, 2.4963501037268667, 2.4906272311474997, 2.4840847517675444, 2.4818924979262773, 2.479649307203978, 2.4772054049513423, 2.4755430093291357, 2.4779367728889357, 2.4732845068467473, 2.4766496384168306, 2.5260640963881413, 2.5318368900972716, 2.52103966618955, 2.5038475395962445, 2.49157753401958, 2.4870137277325077, 2.475668511106738, 2.475107943595557, 2.468803725839885, 2.4665811942098568, 2.4659161174077027, 2.4662128479818546, 2.465378091467479, 2.467647184140873, 2.470195096229381, 2.4704456695540973, 2.469167660883565, 2.4697766096440184, 2.470861342506487, 2.472207957371549, 2.471573156889459, 2.4708708732769473, 2.4680374169986106, 2.4661815286906594, 2.466623095565263, 2.4640045104330324, 2.4619941158216347, 2.4607122660172793, 2.4617425651276137, 2.460706865420332, 2.4598712655797876, 2.4599169567625134, 2.460022855930994, 2.459311727038154, 2.460005270333261, 2.458986744009249, 2.456620830970623, 2.4560024573817634, 2.4569846691781736, 2.456771383246357, 2.4538959597170473, 2.4544064744052454, 2.4536837297788145, 2.4540422553154477, 2.456965016633333, 2.458516135108055, 2.4589658369029084, 2.4581357128566297, 2.4566449010641422, 2.4562345438179785, 2.4562089826047298, 2.4555654542402077, 2.4549926118439473, 2.4546623363142386, 2.4548696330930175, 2.455766885236548, 2.4555698510312935, 2.4564137485237825, 2.4563325342456417, 2.45548999823584, 2.4545280702305035, 2.45527876074554, 2.4584932898104315, 2.4547346080842694, 2.453500343592994, 2.4538766377270833, 2.452873558087515, 2.451846644325178, 2.452819107396402, 2.4528645873804114, 2.4545980495593875, 2.4530887777066086, 2.452721851905024, 2.4530481935771338, 2.451964926181143, 2.4541490933733554, 2.4506210445623378, 2.45145769129054, 2.449906789595586, 2.4488754852106926, 2.4491152964088707, 2.450145902477006, 2.448658118747343, 2.4474450372817333, 2.4477698894986384, 2.453669039669468, 2.4519505061163307, 2.448934661289505, 2.450765183427251, 2.451171020022163, 2.4498367045204743, 2.4523309181847854, 2.450501165742502, 2.4527355906899704, 2.4508074698751714, 2.451588837273067, 2.4497891093181634, 2.4490109924418237, 2.450289362020316, 2.4509619418110935, 2.4494698241506025, 2.4488420166274114, 2.4486219661681314, 2.4484181582315747, 2.4481900389434377, 2.447710006486709, 2.4498556134147567, 2.450386861119672, 2.449009460198561, 2.448331671035265, 2.4497886788918497, 2.4486301555281056, 2.4472741640079194, 2.4471978198331485, 2.4474691166770044, 2.448022171408244, 2.4480870461317057, 2.4477300704627067, 2.4477562710489824, 2.451743415248957, 2.452427134817386, 2.45078905426746, 2.4525836156379026, 2.449971335918262, 2.450701124222616, 2.4503898309241574, 2.449436461705202, 2.4494147818680907, 2.4492832277834538, 2.4481164423102473, 2.4472577951772014, 2.4474671964038324, 2.4482978232342605, 2.4496685294400007, 2.4490658160842176, 2.4496180091795248, 2.4501709493523505, 2.4501849463343377, 2.4481098374791705, 2.449754955538489, 2.4490515800961723, 2.44939720870533, 2.4495995986632986, 2.4494367247979008, 2.4488359845884036, 2.4490563357390416, 2.4482612059591244, 2.4486523079921088, 2.448825935465599, 2.447943986465799, 2.449020908304798, 2.448497365779211, 2.4485982177193892, 2.4481662146119856, 2.4476424556982836, 2.4474442945123944, 2.4480230278547785, 2.4489141495565616, 2.4494383976444816, 2.448013719738876, 2.4481838542577914, 2.4476880112712633, 2.4469487563295775, 2.4469625716826267, 2.44644367934742, 2.4459684587357224, 2.446890957301647, 2.4484907112082417, 2.4474208452863127, 2.445893710643604, 2.4458106290633186, 2.444668875582654, 2.4445372909491065, 2.4426277629648636, 2.444398784539538, 2.443601184410236, 2.445015430940005, 2.443800830155672, 2.446664553158582, 2.4470975696673385, 2.4472127324991404, 2.446866538735141, 2.4481209147882166, 2.4485081171353005, 2.4513906973343365, 2.4488218521924967, 2.4463116720227003, 2.448242905399393, 2.448169087188689, 2.4504207527857784, 2.4511995679788767, 2.4504695716090272, 2.447419990993868, 2.448514163077979, 2.447329476139139, 2.448024720381907, 2.44753352758576, 2.4476789037794546, 2.4481206847901706, 2.4506204872405504, 2.4473551490713192, 2.448217819554605, 2.449019495515608, 2.445982301308634, 2.4472664642138158, 2.446118783069587, 2.445514187039291, 2.4456424916794166, 2.4448591143198817, 2.4450048030524285, 2.4436566198141425, 2.4415940359143016, 2.4409344082740296, 2.4418269863363653, 2.442284453037583, 2.4414838008567292, 2.4405019221609376, 2.4408257546121335, 2.4448692534248933, 2.4443804978834773, 2.445688870506365, 2.4478418632209666, 2.4499145843654686, 2.4497899644918264, 2.4456885964473907, 2.445558908392027, 2.445250797565468, 2.444739375515885, 2.4448691426850933, 2.443066296685158, 2.4452600545706935, 2.443377782187178, 2.443302155324321, 2.4441397358504653, 2.4419781890493155, 2.4427579498878496, 2.442673822594864, 2.4421189529450276, 2.4435658047576214, 2.4435424799791843, 2.442210296732689, 2.4408304009839004, 2.4409927525804274, 2.4409535776173557, 2.440718988618322, 2.440598117448466, 2.43998108673879, 2.440696732953833, 2.439249803006527, 2.4383324979511865, 2.4388383386561023, 2.4401362288903896, 2.440216650365559, 2.4391945260261365, 2.4396317857981216, 2.4388634044286897, 2.440687548893923, 2.4391347925276237, 2.4395938051799484, 2.440386506321494, 2.4400927851087504, 2.4403021772294564, 2.4403515840213155, 2.438126099721607, 2.439170885281886, 2.438992016320356, 2.4388909801075838, 2.439134664653018], 'acc': [0.06365503035409249, 0.08213552317641354, 0.10965092358709116, 0.08829568843577187, 0.09445585229680768, 0.08952772107579625, 0.0891170426836004, 0.09856262788084742, 0.11088295741125298, 0.12772073814878718, 0.13880903422832488, 0.1408624221228476, 0.14004106788419846, 0.14168377894395676, 0.14127310132588694, 0.15071868515234954, 0.15112936433703014, 0.15195071959152848, 0.1540041062743757, 0.1560574945605511, 0.14784394241333987, 0.15605749457890983, 0.1577002050511891, 0.16016427075960799, 0.1655030809878324, 0.16878850171820584, 0.16960985595685502, 0.1667351122387136, 0.16468172455837593, 0.16550308038199463, 0.16919917933627565, 0.1704312117988324, 0.15975359394320227, 0.15646817178696823, 0.15441478449828325, 0.154414785477415, 0.15934291472180423, 0.1749486642084572, 0.17371663137260648, 0.1704312121904851, 0.1733059558902678, 0.17700205447125483, 0.17577002063791364, 0.17535934221817973, 0.17618069884346252, 0.17330595414618938, 0.16837782351265698, 0.16837782292517794, 0.17330595471530968, 0.17412730995144932, 0.1757700212253927, 0.17700205288628534, 0.17453798758787786, 0.17494866597089434, 0.17700205288628534, 0.17330595453784206, 0.1716632432638987, 0.16878850152237948, 0.17618069862927743, 0.17248459850003833, 0.17125256723079837, 0.17002053437658893, 0.16755646827651735, 0.16919917796549122, 0.17043121078298323, 0.17043121160300606, 0.1712525668391457, 0.17412731114476612, 0.1704312106238743, 0.17043121236795272, 0.17207392088196852, 0.17084188843777048, 0.1716632432822574, 0.17207392266276436, 0.1728952767239459, 0.17166324386973644, 0.1737166323333795, 0.17494866440428355, 0.17453798758787786, 0.17248460026247545, 0.17207392305441704, 0.1716632452588796, 0.17330595412783065, 0.17330595492949477, 0.17207392186110024, 0.17412731075311344, 0.1733059555169738, 0.1745379881753569, 0.17535934184488575, 0.1753593426098324, 0.17700205228044757, 0.1765913768348263, 0.1761806976501457, 0.17577002063791364, 0.1757700196404232, 0.17577002159868668, 0.17577002022790222, 0.17618069724013427, 0.17700205366959074, 0.17782340872826272, 0.17823408534884208, 0.17577001965878192, 0.17659137585569457, 0.17618069706266665, 0.17494866557924166, 0.17453798737369278, 0.17412730995144932, 0.1745379881753569, 0.17494866520594768, 0.17494866598925307, 0.17535934223653843, 0.17618069903928885, 0.17659137646153233, 0.1757700200320759, 0.1761806976501457, 0.17535934358896416, 0.17741273208932465, 0.1761806992351152, 0.17453798717786645, 0.17577002161704539, 0.17577002024626096, 0.17330595410947192, 0.17659137665735866, 0.17864476472934904, 0.1761806992351152, 0.1712525666249606, 0.17371663174590046, 0.17412730995144932, 0.1741273109489398, 0.17535934341149653, 0.17207392207528532, 0.17207392285859072, 0.17412730897231757, 0.17207392303605834, 0.17248459889169102, 0.17330595569444143, 0.1741273107347547, 0.1737166323333795, 0.17412731034310203, 0.17494866477757753, 0.17618069727685173, 0.17494866540177403, 0.1728952767055872, 0.17453798717786645, 0.17535934321567018, 0.17618069745431936, 0.17412730955979663, 0.17289527652811956, 0.17248460065412816, 0.17207392225275295, 0.1749486642084572, 0.17166324347808376, 0.1737166323333795, 0.1749486642084572, 0.17125256762245109, 0.17084188922107588, 0.17166324345972503, 0.1745379869820401, 0.17700205347376438, 0.17577001942623813, 0.17494866481429497, 0.17412730996980805, 0.17289527650976083, 0.173305954323657, 0.172895277898904, 0.17289527748889258, 0.17207392305441704, 0.17248459987082276, 0.16919917855297026, 0.17043121162136476, 0.17289527809473035, 0.17084188959436986, 0.1683778229068192, 0.17125256564582889, 0.1749486646001099, 0.17043121138882097, 0.17618069847016854, 0.17248460008500782, 0.17494866536505657, 0.1774127318934983, 0.17330595471530968, 0.17494866540177403, 0.17371663174590046, 0.17659137587405327, 0.17577001942623813, 0.1745379869820401, 0.17577002142121906, 0.1745379881753569, 0.1741273101656344, 0.17577002061955493, 0.1716632440472041, 0.17166324504469455, 0.1737166315500741, 0.17330595430529827, 0.17248460065412816, 0.17125256603748157, 0.17700205368794944, 0.1753593426098324, 0.17700205327793803, 0.17453798796117184, 0.17453798678621374, 0.17659137583733583, 0.1778234095115681, 0.1741273109489398, 0.17618069825598345, 0.1798767960169477, 0.1774127303085288, 0.17700205425706977, 0.1753593418081683, 0.17741273011270245, 0.17535934317895274, 0.17741273207096594, 0.17659137468073646, 0.17700205292300278, 0.17577002022790222, 0.17453798838954196, 0.17248459891004975, 0.17371663251084712, 0.17494866559760036, 0.17453798878119467, 0.1737166335083376, 0.1737166335266963, 0.1728952779172627, 0.17412730897231757, 0.17577002142121906, 0.17659137606987962, 0.17412730955979663, 0.17535934262819114, 0.17659137663899996, 0.1757700212253927, 0.17453798717786645, 0.171252567408266, 0.17700205366959074, 0.17700205425706977, 0.17535934202235337, 0.1724845996749964, 0.17535934300148512, 0.17535934380314924, 0.17494866481429497, 0.1733059559086265, 0.17371663213755315, 0.17453798657202868, 0.17166324541798852, 0.17248459910587607, 0.17371663313504362, 0.17125256723079837, 0.17125256703497202, 0.17289527748889258, 0.1712525662149492, 0.17248459947917005, 0.1712525658232965, 0.16878849993741, 0.16960985716853053, 0.17002053280997814, 0.1634496928791246, 0.15893223770957218, 0.1655030799719832, 0.166735113022019, 0.16919917894462297, 0.16960985615268134, 0.1650924035472302, 0.17125256762245109, 0.17043121197630004, 0.16796714667789256, 0.1696098563485077, 0.17371663135424775, 0.16837782390430966, 0.17002053457241528, 0.17248460067248686, 0.1700205330058045, 0.16837782310264557, 0.1671457908542739, 0.17125256703497202, 0.16632443641979835, 0.1749486653834153, 0.17125256742662473, 0.17330595532114745, 0.17371663252920586, 0.17412730975562296, 0.17207392186110024, 0.1704312114071797, 0.17002053318327212, 0.17535934319731147, 0.17987679621277403, 0.1774127315018456, 0.17494866501012132, 0.17494866461846864, 0.18069815182220764, 0.17659137624734725, 0.18275154073257954, 0.18151950807419645, 0.17946611996548867, 0.17823408591796241, 0.1761806976501457, 0.1774127303085288, 0.1823408615111815, 0.1823408617253666, 0.1823408629186834, 0.17741273167931323, 0.17782340771241353]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
