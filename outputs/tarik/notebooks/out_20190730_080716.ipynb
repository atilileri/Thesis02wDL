{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf13.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 08:07:16 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': 'Front', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['sg', 'ek', 'mb', 'ce', 'my', 'by', 'ib', 'yd', 'sk', 'eo', 'aa', 'eg', 'eb', 'ds', 'ck'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001B851FDD240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001B84E766EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7051, Accuracy:0.0949, Validation Loss:2.6954, Validation Accuracy:0.0821\n",
    "Epoch #2: Loss:2.6914, Accuracy:0.0813, Validation Loss:2.6860, Validation Accuracy:0.0821\n",
    "Epoch #3: Loss:2.6826, Accuracy:0.0875, Validation Loss:2.6778, Validation Accuracy:0.1002\n",
    "Epoch #4: Loss:2.6739, Accuracy:0.1097, Validation Loss:2.6695, Validation Accuracy:0.1281\n",
    "Epoch #5: Loss:2.6652, Accuracy:0.1310, Validation Loss:2.6605, Validation Accuracy:0.1363\n",
    "Epoch #6: Loss:2.6549, Accuracy:0.1339, Validation Loss:2.6504, Validation Accuracy:0.1363\n",
    "Epoch #7: Loss:2.6431, Accuracy:0.1355, Validation Loss:2.6386, Validation Accuracy:0.1544\n",
    "Epoch #8: Loss:2.6296, Accuracy:0.1515, Validation Loss:2.6244, Validation Accuracy:0.1445\n",
    "Epoch #9: Loss:2.6136, Accuracy:0.1466, Validation Loss:2.6081, Validation Accuracy:0.1511\n",
    "Epoch #10: Loss:2.5954, Accuracy:0.1470, Validation Loss:2.5899, Validation Accuracy:0.1593\n",
    "Epoch #11: Loss:2.5756, Accuracy:0.1556, Validation Loss:2.5707, Validation Accuracy:0.1576\n",
    "Epoch #12: Loss:2.5554, Accuracy:0.1634, Validation Loss:2.5513, Validation Accuracy:0.1642\n",
    "Epoch #13: Loss:2.5363, Accuracy:0.1626, Validation Loss:2.5348, Validation Accuracy:0.1609\n",
    "Epoch #14: Loss:2.5231, Accuracy:0.1634, Validation Loss:2.5223, Validation Accuracy:0.1757\n",
    "Epoch #15: Loss:2.5119, Accuracy:0.1688, Validation Loss:2.5113, Validation Accuracy:0.1724\n",
    "Epoch #16: Loss:2.5009, Accuracy:0.1643, Validation Loss:2.5044, Validation Accuracy:0.1724\n",
    "Epoch #17: Loss:2.4947, Accuracy:0.1688, Validation Loss:2.4988, Validation Accuracy:0.1658\n",
    "Epoch #18: Loss:2.4888, Accuracy:0.1733, Validation Loss:2.4950, Validation Accuracy:0.1790\n",
    "Epoch #19: Loss:2.4850, Accuracy:0.1713, Validation Loss:2.4910, Validation Accuracy:0.1675\n",
    "Epoch #20: Loss:2.4820, Accuracy:0.1684, Validation Loss:2.4865, Validation Accuracy:0.1708\n",
    "Epoch #21: Loss:2.4759, Accuracy:0.1717, Validation Loss:2.4827, Validation Accuracy:0.1708\n",
    "Epoch #22: Loss:2.4737, Accuracy:0.1754, Validation Loss:2.4812, Validation Accuracy:0.1790\n",
    "Epoch #23: Loss:2.4711, Accuracy:0.1713, Validation Loss:2.4775, Validation Accuracy:0.1806\n",
    "Epoch #24: Loss:2.4686, Accuracy:0.1754, Validation Loss:2.4760, Validation Accuracy:0.1773\n",
    "Epoch #25: Loss:2.4673, Accuracy:0.1688, Validation Loss:2.4739, Validation Accuracy:0.1741\n",
    "Epoch #26: Loss:2.4672, Accuracy:0.1766, Validation Loss:2.4710, Validation Accuracy:0.1773\n",
    "Epoch #27: Loss:2.4633, Accuracy:0.1741, Validation Loss:2.4691, Validation Accuracy:0.1757\n",
    "Epoch #28: Loss:2.4616, Accuracy:0.1749, Validation Loss:2.4696, Validation Accuracy:0.1757\n",
    "Epoch #29: Loss:2.4601, Accuracy:0.1778, Validation Loss:2.4677, Validation Accuracy:0.1839\n",
    "Epoch #30: Loss:2.4588, Accuracy:0.1762, Validation Loss:2.4663, Validation Accuracy:0.1790\n",
    "Epoch #31: Loss:2.4580, Accuracy:0.1741, Validation Loss:2.4663, Validation Accuracy:0.1773\n",
    "Epoch #32: Loss:2.4568, Accuracy:0.1770, Validation Loss:2.4663, Validation Accuracy:0.1790\n",
    "Epoch #33: Loss:2.4557, Accuracy:0.1774, Validation Loss:2.4647, Validation Accuracy:0.1790\n",
    "Epoch #34: Loss:2.4544, Accuracy:0.1758, Validation Loss:2.4633, Validation Accuracy:0.1724\n",
    "Epoch #35: Loss:2.4532, Accuracy:0.1717, Validation Loss:2.4632, Validation Accuracy:0.1724\n",
    "Epoch #36: Loss:2.4526, Accuracy:0.1729, Validation Loss:2.4627, Validation Accuracy:0.1790\n",
    "Epoch #37: Loss:2.4542, Accuracy:0.1749, Validation Loss:2.4645, Validation Accuracy:0.1724\n",
    "Epoch #38: Loss:2.4529, Accuracy:0.1684, Validation Loss:2.4611, Validation Accuracy:0.1790\n",
    "Epoch #39: Loss:2.4526, Accuracy:0.1729, Validation Loss:2.4608, Validation Accuracy:0.1790\n",
    "Epoch #40: Loss:2.4514, Accuracy:0.1717, Validation Loss:2.4616, Validation Accuracy:0.1806\n",
    "Epoch #41: Loss:2.4516, Accuracy:0.1782, Validation Loss:2.4615, Validation Accuracy:0.1823\n",
    "Epoch #42: Loss:2.4476, Accuracy:0.1766, Validation Loss:2.4624, Validation Accuracy:0.1691\n",
    "Epoch #43: Loss:2.4476, Accuracy:0.1745, Validation Loss:2.4603, Validation Accuracy:0.1806\n",
    "Epoch #44: Loss:2.4465, Accuracy:0.1766, Validation Loss:2.4596, Validation Accuracy:0.1773\n",
    "Epoch #45: Loss:2.4470, Accuracy:0.1795, Validation Loss:2.4591, Validation Accuracy:0.1773\n",
    "Epoch #46: Loss:2.4454, Accuracy:0.1791, Validation Loss:2.4604, Validation Accuracy:0.1806\n",
    "Epoch #47: Loss:2.4439, Accuracy:0.1786, Validation Loss:2.4587, Validation Accuracy:0.1724\n",
    "Epoch #48: Loss:2.4431, Accuracy:0.1774, Validation Loss:2.4583, Validation Accuracy:0.1806\n",
    "Epoch #49: Loss:2.4428, Accuracy:0.1840, Validation Loss:2.4596, Validation Accuracy:0.1839\n",
    "Epoch #50: Loss:2.4434, Accuracy:0.1803, Validation Loss:2.4595, Validation Accuracy:0.1806\n",
    "Epoch #51: Loss:2.4418, Accuracy:0.1786, Validation Loss:2.4577, Validation Accuracy:0.1757\n",
    "Epoch #52: Loss:2.4415, Accuracy:0.1786, Validation Loss:2.4579, Validation Accuracy:0.1757\n",
    "Epoch #53: Loss:2.4401, Accuracy:0.1811, Validation Loss:2.4579, Validation Accuracy:0.1806\n",
    "Epoch #54: Loss:2.4389, Accuracy:0.1803, Validation Loss:2.4584, Validation Accuracy:0.1790\n",
    "Epoch #55: Loss:2.4390, Accuracy:0.1803, Validation Loss:2.4587, Validation Accuracy:0.1790\n",
    "Epoch #56: Loss:2.4392, Accuracy:0.1770, Validation Loss:2.4601, Validation Accuracy:0.1806\n",
    "Epoch #57: Loss:2.4393, Accuracy:0.1860, Validation Loss:2.4583, Validation Accuracy:0.1773\n",
    "Epoch #58: Loss:2.4374, Accuracy:0.1803, Validation Loss:2.4587, Validation Accuracy:0.1773\n",
    "Epoch #59: Loss:2.4365, Accuracy:0.1840, Validation Loss:2.4593, Validation Accuracy:0.1773\n",
    "Epoch #60: Loss:2.4355, Accuracy:0.1819, Validation Loss:2.4594, Validation Accuracy:0.1773\n",
    "Epoch #61: Loss:2.4354, Accuracy:0.1852, Validation Loss:2.4598, Validation Accuracy:0.1757\n",
    "Epoch #62: Loss:2.4356, Accuracy:0.1774, Validation Loss:2.4594, Validation Accuracy:0.1806\n",
    "Epoch #63: Loss:2.4342, Accuracy:0.1832, Validation Loss:2.4592, Validation Accuracy:0.1790\n",
    "Epoch #64: Loss:2.4334, Accuracy:0.1848, Validation Loss:2.4598, Validation Accuracy:0.1757\n",
    "Epoch #65: Loss:2.4322, Accuracy:0.1844, Validation Loss:2.4607, Validation Accuracy:0.1724\n",
    "Epoch #66: Loss:2.4315, Accuracy:0.1852, Validation Loss:2.4606, Validation Accuracy:0.1757\n",
    "Epoch #67: Loss:2.4313, Accuracy:0.1869, Validation Loss:2.4608, Validation Accuracy:0.1773\n",
    "Epoch #68: Loss:2.4298, Accuracy:0.1869, Validation Loss:2.4596, Validation Accuracy:0.1773\n",
    "Epoch #69: Loss:2.4305, Accuracy:0.1840, Validation Loss:2.4605, Validation Accuracy:0.1823\n",
    "Epoch #70: Loss:2.4313, Accuracy:0.1836, Validation Loss:2.4602, Validation Accuracy:0.1773\n",
    "Epoch #71: Loss:2.4288, Accuracy:0.1864, Validation Loss:2.4609, Validation Accuracy:0.1757\n",
    "Epoch #72: Loss:2.4284, Accuracy:0.1885, Validation Loss:2.4617, Validation Accuracy:0.1741\n",
    "Epoch #73: Loss:2.4272, Accuracy:0.1869, Validation Loss:2.4609, Validation Accuracy:0.1806\n",
    "Epoch #74: Loss:2.4267, Accuracy:0.1889, Validation Loss:2.4605, Validation Accuracy:0.1823\n",
    "Epoch #75: Loss:2.4253, Accuracy:0.1873, Validation Loss:2.4611, Validation Accuracy:0.1757\n",
    "Epoch #76: Loss:2.4254, Accuracy:0.1889, Validation Loss:2.4641, Validation Accuracy:0.1773\n",
    "Epoch #77: Loss:2.4269, Accuracy:0.1856, Validation Loss:2.4613, Validation Accuracy:0.1691\n",
    "Epoch #78: Loss:2.4241, Accuracy:0.1897, Validation Loss:2.4626, Validation Accuracy:0.1823\n",
    "Epoch #79: Loss:2.4260, Accuracy:0.1811, Validation Loss:2.4621, Validation Accuracy:0.1773\n",
    "Epoch #80: Loss:2.4235, Accuracy:0.1848, Validation Loss:2.4615, Validation Accuracy:0.1790\n",
    "Epoch #81: Loss:2.4222, Accuracy:0.1848, Validation Loss:2.4602, Validation Accuracy:0.1757\n",
    "Epoch #82: Loss:2.4222, Accuracy:0.1864, Validation Loss:2.4619, Validation Accuracy:0.1773\n",
    "Epoch #83: Loss:2.4219, Accuracy:0.1910, Validation Loss:2.4611, Validation Accuracy:0.1823\n",
    "Epoch #84: Loss:2.4224, Accuracy:0.1893, Validation Loss:2.4641, Validation Accuracy:0.1724\n",
    "Epoch #85: Loss:2.4236, Accuracy:0.1823, Validation Loss:2.4637, Validation Accuracy:0.1790\n",
    "Epoch #86: Loss:2.4209, Accuracy:0.1910, Validation Loss:2.4625, Validation Accuracy:0.1773\n",
    "Epoch #87: Loss:2.4201, Accuracy:0.1897, Validation Loss:2.4615, Validation Accuracy:0.1872\n",
    "Epoch #88: Loss:2.4203, Accuracy:0.1926, Validation Loss:2.4606, Validation Accuracy:0.1691\n",
    "Epoch #89: Loss:2.4181, Accuracy:0.1893, Validation Loss:2.4632, Validation Accuracy:0.1823\n",
    "Epoch #90: Loss:2.4163, Accuracy:0.1918, Validation Loss:2.4627, Validation Accuracy:0.1691\n",
    "Epoch #91: Loss:2.4156, Accuracy:0.1926, Validation Loss:2.4637, Validation Accuracy:0.1872\n",
    "Epoch #92: Loss:2.4150, Accuracy:0.1910, Validation Loss:2.4623, Validation Accuracy:0.1741\n",
    "Epoch #93: Loss:2.4139, Accuracy:0.1926, Validation Loss:2.4633, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4118, Accuracy:0.1897, Validation Loss:2.4661, Validation Accuracy:0.1823\n",
    "Epoch #95: Loss:2.4124, Accuracy:0.1943, Validation Loss:2.4645, Validation Accuracy:0.1708\n",
    "Epoch #96: Loss:2.4108, Accuracy:0.1901, Validation Loss:2.4675, Validation Accuracy:0.1839\n",
    "Epoch #97: Loss:2.4095, Accuracy:0.1906, Validation Loss:2.4669, Validation Accuracy:0.1823\n",
    "Epoch #98: Loss:2.4109, Accuracy:0.1910, Validation Loss:2.4665, Validation Accuracy:0.1741\n",
    "Epoch #99: Loss:2.4084, Accuracy:0.1906, Validation Loss:2.4681, Validation Accuracy:0.1708\n",
    "Epoch #100: Loss:2.4084, Accuracy:0.1918, Validation Loss:2.4671, Validation Accuracy:0.1741\n",
    "Epoch #101: Loss:2.4086, Accuracy:0.1951, Validation Loss:2.4723, Validation Accuracy:0.1823\n",
    "Epoch #102: Loss:2.4071, Accuracy:0.1926, Validation Loss:2.4685, Validation Accuracy:0.1675\n",
    "Epoch #103: Loss:2.4076, Accuracy:0.1910, Validation Loss:2.4684, Validation Accuracy:0.1658\n",
    "Epoch #104: Loss:2.4074, Accuracy:0.1914, Validation Loss:2.4743, Validation Accuracy:0.1741\n",
    "Epoch #105: Loss:2.4053, Accuracy:0.1914, Validation Loss:2.4695, Validation Accuracy:0.1708\n",
    "Epoch #106: Loss:2.4089, Accuracy:0.1906, Validation Loss:2.4749, Validation Accuracy:0.1626\n",
    "Epoch #107: Loss:2.4078, Accuracy:0.1881, Validation Loss:2.4699, Validation Accuracy:0.1724\n",
    "Epoch #108: Loss:2.3995, Accuracy:0.1959, Validation Loss:2.4714, Validation Accuracy:0.1691\n",
    "Epoch #109: Loss:2.4009, Accuracy:0.1988, Validation Loss:2.4724, Validation Accuracy:0.1626\n",
    "Epoch #110: Loss:2.3983, Accuracy:0.1992, Validation Loss:2.4786, Validation Accuracy:0.1724\n",
    "Epoch #111: Loss:2.3971, Accuracy:0.1951, Validation Loss:2.4746, Validation Accuracy:0.1658\n",
    "Epoch #112: Loss:2.3993, Accuracy:0.1951, Validation Loss:2.4795, Validation Accuracy:0.1691\n",
    "Epoch #113: Loss:2.3977, Accuracy:0.1971, Validation Loss:2.4803, Validation Accuracy:0.1741\n",
    "Epoch #114: Loss:2.3997, Accuracy:0.1996, Validation Loss:2.4753, Validation Accuracy:0.1757\n",
    "Epoch #115: Loss:2.3965, Accuracy:0.1975, Validation Loss:2.4746, Validation Accuracy:0.1626\n",
    "Epoch #116: Loss:2.3933, Accuracy:0.1959, Validation Loss:2.4772, Validation Accuracy:0.1675\n",
    "Epoch #117: Loss:2.3917, Accuracy:0.2004, Validation Loss:2.4848, Validation Accuracy:0.1642\n",
    "Epoch #118: Loss:2.3875, Accuracy:0.2004, Validation Loss:2.4783, Validation Accuracy:0.1658\n",
    "Epoch #119: Loss:2.3911, Accuracy:0.1955, Validation Loss:2.4786, Validation Accuracy:0.1658\n",
    "Epoch #120: Loss:2.3879, Accuracy:0.2008, Validation Loss:2.4819, Validation Accuracy:0.1675\n",
    "Epoch #121: Loss:2.3873, Accuracy:0.1959, Validation Loss:2.4846, Validation Accuracy:0.1642\n",
    "Epoch #122: Loss:2.3858, Accuracy:0.2037, Validation Loss:2.4839, Validation Accuracy:0.1691\n",
    "Epoch #123: Loss:2.3814, Accuracy:0.2062, Validation Loss:2.4843, Validation Accuracy:0.1773\n",
    "Epoch #124: Loss:2.3812, Accuracy:0.2045, Validation Loss:2.4837, Validation Accuracy:0.1724\n",
    "Epoch #125: Loss:2.3795, Accuracy:0.2053, Validation Loss:2.4929, Validation Accuracy:0.1675\n",
    "Epoch #126: Loss:2.3805, Accuracy:0.2066, Validation Loss:2.4877, Validation Accuracy:0.1642\n",
    "Epoch #127: Loss:2.3818, Accuracy:0.2053, Validation Loss:2.4944, Validation Accuracy:0.1642\n",
    "Epoch #128: Loss:2.3882, Accuracy:0.1996, Validation Loss:2.4852, Validation Accuracy:0.1757\n",
    "Epoch #129: Loss:2.3838, Accuracy:0.1992, Validation Loss:2.4936, Validation Accuracy:0.1724\n",
    "Epoch #130: Loss:2.3820, Accuracy:0.2025, Validation Loss:2.4923, Validation Accuracy:0.1724\n",
    "Epoch #131: Loss:2.3733, Accuracy:0.2053, Validation Loss:2.4882, Validation Accuracy:0.1741\n",
    "Epoch #132: Loss:2.3752, Accuracy:0.2008, Validation Loss:2.4893, Validation Accuracy:0.1675\n",
    "Epoch #133: Loss:2.3712, Accuracy:0.2086, Validation Loss:2.4977, Validation Accuracy:0.1642\n",
    "Epoch #134: Loss:2.3701, Accuracy:0.2103, Validation Loss:2.4894, Validation Accuracy:0.1708\n",
    "Epoch #135: Loss:2.3682, Accuracy:0.2029, Validation Loss:2.4965, Validation Accuracy:0.1724\n",
    "Epoch #136: Loss:2.3650, Accuracy:0.2127, Validation Loss:2.4966, Validation Accuracy:0.1675\n",
    "Epoch #137: Loss:2.3697, Accuracy:0.2074, Validation Loss:2.4961, Validation Accuracy:0.1626\n",
    "Epoch #138: Loss:2.3783, Accuracy:0.1975, Validation Loss:2.4972, Validation Accuracy:0.1642\n",
    "Epoch #139: Loss:2.3795, Accuracy:0.2033, Validation Loss:2.4956, Validation Accuracy:0.1708\n",
    "Epoch #140: Loss:2.3644, Accuracy:0.2082, Validation Loss:2.4931, Validation Accuracy:0.1675\n",
    "Epoch #141: Loss:2.3627, Accuracy:0.2156, Validation Loss:2.4998, Validation Accuracy:0.1691\n",
    "Epoch #142: Loss:2.3596, Accuracy:0.2136, Validation Loss:2.5022, Validation Accuracy:0.1675\n",
    "Epoch #143: Loss:2.3621, Accuracy:0.2057, Validation Loss:2.4978, Validation Accuracy:0.1675\n",
    "Epoch #144: Loss:2.3570, Accuracy:0.2094, Validation Loss:2.4980, Validation Accuracy:0.1773\n",
    "Epoch #145: Loss:2.3562, Accuracy:0.2144, Validation Loss:2.4995, Validation Accuracy:0.1757\n",
    "Epoch #146: Loss:2.3589, Accuracy:0.2078, Validation Loss:2.5094, Validation Accuracy:0.1658\n",
    "Epoch #147: Loss:2.3513, Accuracy:0.2172, Validation Loss:2.5107, Validation Accuracy:0.1642\n",
    "Epoch #148: Loss:2.3546, Accuracy:0.2094, Validation Loss:2.5033, Validation Accuracy:0.1708\n",
    "Epoch #149: Loss:2.3518, Accuracy:0.2111, Validation Loss:2.5093, Validation Accuracy:0.1741\n",
    "Epoch #150: Loss:2.3533, Accuracy:0.2115, Validation Loss:2.5100, Validation Accuracy:0.1642\n",
    "Epoch #151: Loss:2.3559, Accuracy:0.2136, Validation Loss:2.5061, Validation Accuracy:0.1675\n",
    "Epoch #152: Loss:2.3488, Accuracy:0.2107, Validation Loss:2.5078, Validation Accuracy:0.1708\n",
    "Epoch #153: Loss:2.3412, Accuracy:0.2209, Validation Loss:2.5053, Validation Accuracy:0.1724\n",
    "Epoch #154: Loss:2.3428, Accuracy:0.2185, Validation Loss:2.5072, Validation Accuracy:0.1790\n",
    "Epoch #155: Loss:2.3394, Accuracy:0.2205, Validation Loss:2.5093, Validation Accuracy:0.1741\n",
    "Epoch #156: Loss:2.3411, Accuracy:0.2177, Validation Loss:2.5126, Validation Accuracy:0.1724\n",
    "Epoch #157: Loss:2.3384, Accuracy:0.2185, Validation Loss:2.5079, Validation Accuracy:0.1806\n",
    "Epoch #158: Loss:2.3375, Accuracy:0.2214, Validation Loss:2.5161, Validation Accuracy:0.1741\n",
    "Epoch #159: Loss:2.3380, Accuracy:0.2197, Validation Loss:2.5143, Validation Accuracy:0.1724\n",
    "Epoch #160: Loss:2.3327, Accuracy:0.2226, Validation Loss:2.5138, Validation Accuracy:0.1773\n",
    "Epoch #161: Loss:2.3293, Accuracy:0.2214, Validation Loss:2.5197, Validation Accuracy:0.1691\n",
    "Epoch #162: Loss:2.3261, Accuracy:0.2226, Validation Loss:2.5214, Validation Accuracy:0.1708\n",
    "Epoch #163: Loss:2.3247, Accuracy:0.2246, Validation Loss:2.5161, Validation Accuracy:0.1823\n",
    "Epoch #164: Loss:2.3233, Accuracy:0.2255, Validation Loss:2.5239, Validation Accuracy:0.1823\n",
    "Epoch #165: Loss:2.3266, Accuracy:0.2230, Validation Loss:2.5200, Validation Accuracy:0.1773\n",
    "Epoch #166: Loss:2.3317, Accuracy:0.2218, Validation Loss:2.5212, Validation Accuracy:0.1823\n",
    "Epoch #167: Loss:2.3276, Accuracy:0.2238, Validation Loss:2.5276, Validation Accuracy:0.1658\n",
    "Epoch #168: Loss:2.3275, Accuracy:0.2238, Validation Loss:2.5226, Validation Accuracy:0.1823\n",
    "Epoch #169: Loss:2.3244, Accuracy:0.2283, Validation Loss:2.5243, Validation Accuracy:0.1773\n",
    "Epoch #170: Loss:2.3249, Accuracy:0.2304, Validation Loss:2.5338, Validation Accuracy:0.1741\n",
    "Epoch #171: Loss:2.3267, Accuracy:0.2193, Validation Loss:2.5190, Validation Accuracy:0.1741\n",
    "Epoch #172: Loss:2.3210, Accuracy:0.2329, Validation Loss:2.5383, Validation Accuracy:0.1708\n",
    "Epoch #173: Loss:2.3127, Accuracy:0.2320, Validation Loss:2.5248, Validation Accuracy:0.1708\n",
    "Epoch #174: Loss:2.3109, Accuracy:0.2316, Validation Loss:2.5383, Validation Accuracy:0.1757\n",
    "Epoch #175: Loss:2.3085, Accuracy:0.2316, Validation Loss:2.5361, Validation Accuracy:0.1741\n",
    "Epoch #176: Loss:2.3079, Accuracy:0.2320, Validation Loss:2.5355, Validation Accuracy:0.1773\n",
    "Epoch #177: Loss:2.3055, Accuracy:0.2308, Validation Loss:2.5355, Validation Accuracy:0.1773\n",
    "Epoch #178: Loss:2.3028, Accuracy:0.2337, Validation Loss:2.5356, Validation Accuracy:0.1691\n",
    "Epoch #179: Loss:2.3016, Accuracy:0.2316, Validation Loss:2.5454, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.2972, Accuracy:0.2353, Validation Loss:2.5367, Validation Accuracy:0.1741\n",
    "Epoch #181: Loss:2.2964, Accuracy:0.2402, Validation Loss:2.5421, Validation Accuracy:0.1708\n",
    "Epoch #182: Loss:2.2953, Accuracy:0.2366, Validation Loss:2.5410, Validation Accuracy:0.1757\n",
    "Epoch #183: Loss:2.2914, Accuracy:0.2415, Validation Loss:2.5471, Validation Accuracy:0.1691\n",
    "Epoch #184: Loss:2.2936, Accuracy:0.2320, Validation Loss:2.5462, Validation Accuracy:0.1658\n",
    "Epoch #185: Loss:2.2906, Accuracy:0.2431, Validation Loss:2.5487, Validation Accuracy:0.1888\n",
    "Epoch #186: Loss:2.2944, Accuracy:0.2386, Validation Loss:2.5525, Validation Accuracy:0.1741\n",
    "Epoch #187: Loss:2.2969, Accuracy:0.2394, Validation Loss:2.5511, Validation Accuracy:0.1773\n",
    "Epoch #188: Loss:2.3094, Accuracy:0.2287, Validation Loss:2.5489, Validation Accuracy:0.1757\n",
    "Epoch #189: Loss:2.3098, Accuracy:0.2312, Validation Loss:2.5486, Validation Accuracy:0.1741\n",
    "Epoch #190: Loss:2.2966, Accuracy:0.2415, Validation Loss:2.5448, Validation Accuracy:0.1773\n",
    "Epoch #191: Loss:2.2867, Accuracy:0.2382, Validation Loss:2.5410, Validation Accuracy:0.1724\n",
    "Epoch #192: Loss:2.2779, Accuracy:0.2439, Validation Loss:2.5581, Validation Accuracy:0.1773\n",
    "Epoch #193: Loss:2.2789, Accuracy:0.2448, Validation Loss:2.5465, Validation Accuracy:0.1691\n",
    "Epoch #194: Loss:2.2727, Accuracy:0.2526, Validation Loss:2.5576, Validation Accuracy:0.1757\n",
    "Epoch #195: Loss:2.2735, Accuracy:0.2480, Validation Loss:2.5517, Validation Accuracy:0.1773\n",
    "Epoch #196: Loss:2.2678, Accuracy:0.2493, Validation Loss:2.5577, Validation Accuracy:0.1708\n",
    "Epoch #197: Loss:2.2699, Accuracy:0.2476, Validation Loss:2.5623, Validation Accuracy:0.1773\n",
    "Epoch #198: Loss:2.2715, Accuracy:0.2464, Validation Loss:2.5620, Validation Accuracy:0.1691\n",
    "Epoch #199: Loss:2.2776, Accuracy:0.2427, Validation Loss:2.5609, Validation Accuracy:0.1773\n",
    "Epoch #200: Loss:2.2862, Accuracy:0.2415, Validation Loss:2.5685, Validation Accuracy:0.1790\n",
    "Epoch #201: Loss:2.2661, Accuracy:0.2501, Validation Loss:2.5644, Validation Accuracy:0.1823\n",
    "Epoch #202: Loss:2.2671, Accuracy:0.2513, Validation Loss:2.5723, Validation Accuracy:0.1675\n",
    "Epoch #203: Loss:2.2591, Accuracy:0.2579, Validation Loss:2.5631, Validation Accuracy:0.1823\n",
    "Epoch #204: Loss:2.2571, Accuracy:0.2559, Validation Loss:2.5746, Validation Accuracy:0.1790\n",
    "Epoch #205: Loss:2.2552, Accuracy:0.2567, Validation Loss:2.5779, Validation Accuracy:0.1741\n",
    "Epoch #206: Loss:2.2585, Accuracy:0.2497, Validation Loss:2.5758, Validation Accuracy:0.1741\n",
    "Epoch #207: Loss:2.2593, Accuracy:0.2517, Validation Loss:2.5883, Validation Accuracy:0.1790\n",
    "Epoch #208: Loss:2.2545, Accuracy:0.2526, Validation Loss:2.5742, Validation Accuracy:0.1773\n",
    "Epoch #209: Loss:2.2543, Accuracy:0.2513, Validation Loss:2.5812, Validation Accuracy:0.1773\n",
    "Epoch #210: Loss:2.2514, Accuracy:0.2546, Validation Loss:2.5794, Validation Accuracy:0.1790\n",
    "Epoch #211: Loss:2.2423, Accuracy:0.2579, Validation Loss:2.5789, Validation Accuracy:0.1757\n",
    "Epoch #212: Loss:2.2417, Accuracy:0.2583, Validation Loss:2.5864, Validation Accuracy:0.1823\n",
    "Epoch #213: Loss:2.2485, Accuracy:0.2563, Validation Loss:2.5867, Validation Accuracy:0.1724\n",
    "Epoch #214: Loss:2.2437, Accuracy:0.2604, Validation Loss:2.5896, Validation Accuracy:0.1741\n",
    "Epoch #215: Loss:2.2370, Accuracy:0.2657, Validation Loss:2.5782, Validation Accuracy:0.1790\n",
    "Epoch #216: Loss:2.2388, Accuracy:0.2595, Validation Loss:2.5975, Validation Accuracy:0.1708\n",
    "Epoch #217: Loss:2.2319, Accuracy:0.2645, Validation Loss:2.5857, Validation Accuracy:0.1823\n",
    "Epoch #218: Loss:2.2314, Accuracy:0.2649, Validation Loss:2.5939, Validation Accuracy:0.1773\n",
    "Epoch #219: Loss:2.2322, Accuracy:0.2653, Validation Loss:2.5913, Validation Accuracy:0.1839\n",
    "Epoch #220: Loss:2.2323, Accuracy:0.2719, Validation Loss:2.6108, Validation Accuracy:0.1658\n",
    "Epoch #221: Loss:2.2315, Accuracy:0.2559, Validation Loss:2.5973, Validation Accuracy:0.1773\n",
    "Epoch #222: Loss:2.2252, Accuracy:0.2706, Validation Loss:2.5959, Validation Accuracy:0.1658\n",
    "Epoch #223: Loss:2.2317, Accuracy:0.2612, Validation Loss:2.6093, Validation Accuracy:0.1691\n",
    "Epoch #224: Loss:2.2293, Accuracy:0.2678, Validation Loss:2.6279, Validation Accuracy:0.1724\n",
    "Epoch #225: Loss:2.2430, Accuracy:0.2554, Validation Loss:2.6168, Validation Accuracy:0.1724\n",
    "Epoch #226: Loss:2.2410, Accuracy:0.2546, Validation Loss:2.5989, Validation Accuracy:0.1757\n",
    "Epoch #227: Loss:2.2454, Accuracy:0.2538, Validation Loss:2.6140, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.2353, Accuracy:0.2624, Validation Loss:2.5995, Validation Accuracy:0.1724\n",
    "Epoch #229: Loss:2.2311, Accuracy:0.2620, Validation Loss:2.6051, Validation Accuracy:0.1773\n",
    "Epoch #230: Loss:2.2301, Accuracy:0.2645, Validation Loss:2.6138, Validation Accuracy:0.1675\n",
    "Epoch #231: Loss:2.2296, Accuracy:0.2637, Validation Loss:2.6016, Validation Accuracy:0.1708\n",
    "Epoch #232: Loss:2.2445, Accuracy:0.2563, Validation Loss:2.6137, Validation Accuracy:0.1757\n",
    "Epoch #233: Loss:2.2201, Accuracy:0.2674, Validation Loss:2.6096, Validation Accuracy:0.1741\n",
    "Epoch #234: Loss:2.2207, Accuracy:0.2678, Validation Loss:2.6208, Validation Accuracy:0.1691\n",
    "Epoch #235: Loss:2.2180, Accuracy:0.2657, Validation Loss:2.6145, Validation Accuracy:0.1773\n",
    "Epoch #236: Loss:2.2087, Accuracy:0.2743, Validation Loss:2.6230, Validation Accuracy:0.1708\n",
    "Epoch #237: Loss:2.2113, Accuracy:0.2686, Validation Loss:2.6185, Validation Accuracy:0.1675\n",
    "Epoch #238: Loss:2.2116, Accuracy:0.2702, Validation Loss:2.6198, Validation Accuracy:0.1790\n",
    "Epoch #239: Loss:2.2026, Accuracy:0.2756, Validation Loss:2.6201, Validation Accuracy:0.1773\n",
    "Epoch #240: Loss:2.1956, Accuracy:0.2801, Validation Loss:2.6255, Validation Accuracy:0.1757\n",
    "Epoch #241: Loss:2.1994, Accuracy:0.2760, Validation Loss:2.6226, Validation Accuracy:0.1773\n",
    "Epoch #242: Loss:2.2037, Accuracy:0.2756, Validation Loss:2.6294, Validation Accuracy:0.1708\n",
    "Epoch #243: Loss:2.1915, Accuracy:0.2825, Validation Loss:2.6421, Validation Accuracy:0.1691\n",
    "Epoch #244: Loss:2.1896, Accuracy:0.2797, Validation Loss:2.6287, Validation Accuracy:0.1691\n",
    "Epoch #245: Loss:2.1977, Accuracy:0.2756, Validation Loss:2.6446, Validation Accuracy:0.1773\n",
    "Epoch #246: Loss:2.1935, Accuracy:0.2776, Validation Loss:2.6364, Validation Accuracy:0.1757\n",
    "Epoch #247: Loss:2.1847, Accuracy:0.2850, Validation Loss:2.6388, Validation Accuracy:0.1708\n",
    "Epoch #248: Loss:2.1819, Accuracy:0.2797, Validation Loss:2.6468, Validation Accuracy:0.1626\n",
    "Epoch #249: Loss:2.1796, Accuracy:0.2838, Validation Loss:2.6436, Validation Accuracy:0.1642\n",
    "Epoch #250: Loss:2.1743, Accuracy:0.2862, Validation Loss:2.6481, Validation Accuracy:0.1609\n",
    "Epoch #251: Loss:2.1732, Accuracy:0.2895, Validation Loss:2.6413, Validation Accuracy:0.1708\n",
    "Epoch #252: Loss:2.1731, Accuracy:0.2916, Validation Loss:2.6436, Validation Accuracy:0.1724\n",
    "Epoch #253: Loss:2.1712, Accuracy:0.2867, Validation Loss:2.6614, Validation Accuracy:0.1642\n",
    "Epoch #254: Loss:2.1707, Accuracy:0.2903, Validation Loss:2.6456, Validation Accuracy:0.1757\n",
    "Epoch #255: Loss:2.1688, Accuracy:0.2875, Validation Loss:2.6576, Validation Accuracy:0.1691\n",
    "Epoch #256: Loss:2.1722, Accuracy:0.2801, Validation Loss:2.6752, Validation Accuracy:0.1642\n",
    "Epoch #257: Loss:2.1723, Accuracy:0.2850, Validation Loss:2.6539, Validation Accuracy:0.1658\n",
    "Epoch #258: Loss:2.1650, Accuracy:0.2916, Validation Loss:2.6567, Validation Accuracy:0.1658\n",
    "Epoch #259: Loss:2.1712, Accuracy:0.2821, Validation Loss:2.6811, Validation Accuracy:0.1856\n",
    "Epoch #260: Loss:2.1694, Accuracy:0.2895, Validation Loss:2.6512, Validation Accuracy:0.1757\n",
    "Epoch #261: Loss:2.1729, Accuracy:0.2817, Validation Loss:2.6646, Validation Accuracy:0.1593\n",
    "Epoch #262: Loss:2.1639, Accuracy:0.2879, Validation Loss:2.6820, Validation Accuracy:0.1609\n",
    "Epoch #263: Loss:2.1580, Accuracy:0.2867, Validation Loss:2.6695, Validation Accuracy:0.1609\n",
    "Epoch #264: Loss:2.1555, Accuracy:0.2916, Validation Loss:2.6699, Validation Accuracy:0.1658\n",
    "Epoch #265: Loss:2.1599, Accuracy:0.2920, Validation Loss:2.6803, Validation Accuracy:0.1626\n",
    "Epoch #266: Loss:2.1498, Accuracy:0.2965, Validation Loss:2.6820, Validation Accuracy:0.1642\n",
    "Epoch #267: Loss:2.1564, Accuracy:0.2887, Validation Loss:2.6720, Validation Accuracy:0.1691\n",
    "Epoch #268: Loss:2.1731, Accuracy:0.2768, Validation Loss:2.6735, Validation Accuracy:0.1576\n",
    "Epoch #269: Loss:2.1648, Accuracy:0.2854, Validation Loss:2.7012, Validation Accuracy:0.1626\n",
    "Epoch #270: Loss:2.1647, Accuracy:0.2871, Validation Loss:2.6934, Validation Accuracy:0.1642\n",
    "Epoch #271: Loss:2.1572, Accuracy:0.2862, Validation Loss:2.6620, Validation Accuracy:0.1724\n",
    "Epoch #272: Loss:2.1755, Accuracy:0.2830, Validation Loss:2.6862, Validation Accuracy:0.1626\n",
    "Epoch #273: Loss:2.1558, Accuracy:0.2903, Validation Loss:2.6746, Validation Accuracy:0.1675\n",
    "Epoch #274: Loss:2.1422, Accuracy:0.2924, Validation Loss:2.6760, Validation Accuracy:0.1626\n",
    "Epoch #275: Loss:2.1477, Accuracy:0.2875, Validation Loss:2.6914, Validation Accuracy:0.1560\n",
    "Epoch #276: Loss:2.1393, Accuracy:0.2982, Validation Loss:2.6810, Validation Accuracy:0.1675\n",
    "Epoch #277: Loss:2.1451, Accuracy:0.2953, Validation Loss:2.6910, Validation Accuracy:0.1642\n",
    "Epoch #278: Loss:2.1392, Accuracy:0.2924, Validation Loss:2.6960, Validation Accuracy:0.1658\n",
    "Epoch #279: Loss:2.1394, Accuracy:0.2969, Validation Loss:2.7100, Validation Accuracy:0.1708\n",
    "Epoch #280: Loss:2.1472, Accuracy:0.2977, Validation Loss:2.6964, Validation Accuracy:0.1609\n",
    "Epoch #281: Loss:2.1352, Accuracy:0.2986, Validation Loss:2.6842, Validation Accuracy:0.1790\n",
    "Epoch #282: Loss:2.1515, Accuracy:0.2895, Validation Loss:2.6868, Validation Accuracy:0.1576\n",
    "Epoch #283: Loss:2.1395, Accuracy:0.2940, Validation Loss:2.7123, Validation Accuracy:0.1675\n",
    "Epoch #284: Loss:2.1350, Accuracy:0.2895, Validation Loss:2.7119, Validation Accuracy:0.1560\n",
    "Epoch #285: Loss:2.1337, Accuracy:0.2957, Validation Loss:2.6940, Validation Accuracy:0.1658\n",
    "Epoch #286: Loss:2.1256, Accuracy:0.2957, Validation Loss:2.7127, Validation Accuracy:0.1626\n",
    "Epoch #287: Loss:2.1284, Accuracy:0.2977, Validation Loss:2.7057, Validation Accuracy:0.1691\n",
    "Epoch #288: Loss:2.1352, Accuracy:0.2949, Validation Loss:2.7069, Validation Accuracy:0.1708\n",
    "Epoch #289: Loss:2.1538, Accuracy:0.2871, Validation Loss:2.6992, Validation Accuracy:0.1741\n",
    "Epoch #290: Loss:2.1232, Accuracy:0.3051, Validation Loss:2.7117, Validation Accuracy:0.1609\n",
    "Epoch #291: Loss:2.1102, Accuracy:0.3117, Validation Loss:2.7275, Validation Accuracy:0.1511\n",
    "Epoch #292: Loss:2.1110, Accuracy:0.3010, Validation Loss:2.7159, Validation Accuracy:0.1691\n",
    "Epoch #293: Loss:2.1101, Accuracy:0.3138, Validation Loss:2.7074, Validation Accuracy:0.1675\n",
    "Epoch #294: Loss:2.1087, Accuracy:0.3051, Validation Loss:2.7174, Validation Accuracy:0.1576\n",
    "Epoch #295: Loss:2.1035, Accuracy:0.3084, Validation Loss:2.7132, Validation Accuracy:0.1527\n",
    "Epoch #296: Loss:2.1017, Accuracy:0.3076, Validation Loss:2.7243, Validation Accuracy:0.1560\n",
    "Epoch #297: Loss:2.0993, Accuracy:0.3183, Validation Loss:2.7230, Validation Accuracy:0.1576\n",
    "Epoch #298: Loss:2.0975, Accuracy:0.3183, Validation Loss:2.7166, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.1031, Accuracy:0.3121, Validation Loss:2.7373, Validation Accuracy:0.1609\n",
    "Epoch #300: Loss:2.1032, Accuracy:0.3076, Validation Loss:2.7544, Validation Accuracy:0.1609\n",
    "\n",
    "Test:\n",
    "Test Loss:2.75437164, Accuracy:0.1609\n",
    "Labels: ['sg', 'ek', 'mb', 'ce', 'my', 'by', 'ib', 'yd', 'sk', 'eo', 'aa', 'eg', 'eb', 'ds', 'ck']\n",
    "Confusion Matrix:\n",
    "      sg  ek  mb  ce  my  by  ib  yd  sk  eo  aa  eg  eb  ds  ck\n",
    "t:sg  13   1   5   0   0  13   2   9   0   3   0   3   1   1   0\n",
    "t:ek   3   1   0   0   0  12   1   5   0   5   3  14   2   2   0\n",
    "t:mb  10   2   4   0   0   8   1  10   0   2   1  10   2   2   0\n",
    "t:ce   3   1   1   0   0   4   0   3   0   3   1   8   0   3   0\n",
    "t:my   1   0   2   0   0   2   2   5   0   2   1   4   1   0   0\n",
    "t:by   3   1   1   0   0   9   3   4   0   4   1   7   4   3   0\n",
    "t:ib   6   1   4   0   0   5   5  25   0   1   0   6   0   1   0\n",
    "t:yd   7   2   4   0   0   5   9  31   0   4   0   0   0   0   0\n",
    "t:sk   4   3   1   0   0   4   0   0   0   6   2   8   4   1   0\n",
    "t:eo   5   2   0   0   0   3   1   8   1   6   1   5   1   1   0\n",
    "t:aa   1   0   2   0   0   6   0   4   0   4   1   9   2   5   0\n",
    "t:eg   3   0   3   0   0   7   0   1   0   4   3  18   5   6   0\n",
    "t:eb   5   1   4   0   0  13   0   3   0   8   1  14   0   1   0\n",
    "t:ds   6   0   0   0   0   3   0   0   0   3   0   9   0  10   0\n",
    "t:ck   2   1   1   0   0   3   0   0   0   3   0  10   1   2   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          sg       0.18      0.25      0.21        51\n",
    "          ek       0.06      0.02      0.03        48\n",
    "          mb       0.12      0.08      0.10        52\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          my       0.00      0.00      0.00        20\n",
    "          by       0.09      0.23      0.13        40\n",
    "          ib       0.21      0.09      0.13        54\n",
    "          yd       0.29      0.50      0.36        62\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eo       0.10      0.18      0.13        34\n",
    "          aa       0.07      0.03      0.04        34\n",
    "          eg       0.14      0.36      0.21        50\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          ds       0.26      0.32      0.29        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.10      0.14      0.11       609\n",
    "weighted avg       0.12      0.16      0.13       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 08:22:51 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.695425633726449, 2.6859556689050983, 2.677822952787277, 2.6694649037077705, 2.6604959878623973, 2.6504263384588835, 2.6386211826687767, 2.624392038104178, 2.6081079504955773, 2.589893042356118, 2.5706581020198627, 2.5513249461482506, 2.534821902981337, 2.522288310703973, 2.5113239108243794, 2.504448681237858, 2.4988346283854717, 2.4949749201193625, 2.4910211837154694, 2.486543289742055, 2.4826940715019337, 2.4812075085632124, 2.4774573968940574, 2.4759567750890072, 2.473869881606454, 2.4710327118684114, 2.469105079647747, 2.4695774248276634, 2.467653010670579, 2.466346352362672, 2.4662835202585103, 2.466268421981135, 2.46465526073437, 2.4632703044340136, 2.4632481608680514, 2.4626685970131006, 2.46447672123588, 2.461076113195059, 2.4608436836593452, 2.461574996437737, 2.461519020923057, 2.4623639325202977, 2.4603342594967295, 2.4596490057426914, 2.4590751635421477, 2.460424326519269, 2.458746539352367, 2.4582948081794824, 2.4596383418942906, 2.459529297692435, 2.457710856483096, 2.457863010209182, 2.457914096381277, 2.458369581961671, 2.458700922322391, 2.460111579675784, 2.458307443189699, 2.458663737636873, 2.4592555199546378, 2.4593589266728495, 2.459794652090088, 2.459410206241952, 2.4592380061721175, 2.4597595513160595, 2.4606706403159158, 2.460593443198744, 2.460792767003252, 2.4595559614241025, 2.4605323340505216, 2.4602470417523814, 2.4609497501736595, 2.4616750147933835, 2.4609398818368393, 2.4604708091378797, 2.4610871654034443, 2.4640775384574103, 2.4612820058424876, 2.462635996697963, 2.46209537336979, 2.461511932570359, 2.460196249199227, 2.461934330036683, 2.4611032925215848, 2.4640516111220436, 2.463681381519988, 2.462498697741278, 2.4614664185027575, 2.4605997383888134, 2.4632082408284908, 2.4626757135532173, 2.4637368993806135, 2.4622780282313403, 2.463315843165606, 2.466135182012674, 2.4645332076475146, 2.467481703985305, 2.46689130011059, 2.466507605143956, 2.4680580583894978, 2.467138337384304, 2.4723493301222477, 2.4685292063871236, 2.4683873896136856, 2.4743078444000144, 2.4695314585868946, 2.474858166549006, 2.4699045554757704, 2.471380250990293, 2.4723982646547515, 2.478632921851523, 2.474596274505891, 2.479468882769004, 2.4802956487157664, 2.4753280746917223, 2.4746051010827124, 2.47719835138869, 2.484786863201749, 2.4782787367628125, 2.4786215619304888, 2.4819025821090723, 2.484603650072721, 2.483855860182413, 2.484291937550888, 2.4837430549176847, 2.492851544287796, 2.4877070722908807, 2.4943777759282657, 2.4852260372713086, 2.4935925257421285, 2.492288970007685, 2.488219701206351, 2.4892678213824193, 2.4977244215058576, 2.489416271594945, 2.496549041791894, 2.4966028734968213, 2.496097840307577, 2.4972137813693394, 2.495632766698577, 2.493051319482487, 2.4998497692822235, 2.5022098665754196, 2.497808926994186, 2.4980090647103945, 2.49949589151467, 2.5093622497345622, 2.5106987765269912, 2.5032641684089, 2.509311539003219, 2.5100294525791664, 2.506087402600569, 2.5078293282801685, 2.505335700140015, 2.50718905185831, 2.5092829827996113, 2.512569975578922, 2.5078579502544183, 2.5160934795887013, 2.5142517015460286, 2.513839918991615, 2.5197148276080052, 2.5214366035899896, 2.5160610930281515, 2.5238510560128096, 2.520003537630604, 2.521151852333683, 2.5275602246740183, 2.5225781595765664, 2.524312222923943, 2.533795990575906, 2.5189659885193523, 2.5382578271167424, 2.524848140127749, 2.538302052001452, 2.5361092791377224, 2.535461859162805, 2.5354984988915703, 2.5356216180109232, 2.5454464737809155, 2.536680965392265, 2.5420974628091444, 2.540967500464278, 2.5470822268518907, 2.54621124854816, 2.548730067235887, 2.552501516389142, 2.5511310112300176, 2.548924632456111, 2.5485947652794847, 2.544780994675234, 2.5409797882211618, 2.5581391276592886, 2.54647531141397, 2.557558459405633, 2.5517236857578673, 2.5576591855786703, 2.5623467281729915, 2.5619642789336456, 2.5608975922532857, 2.5685326626343876, 2.5644020008531894, 2.572276241086387, 2.563125792786797, 2.5745651608421687, 2.577944281848976, 2.575814317991385, 2.588260297117562, 2.5742145997941592, 2.581209235199175, 2.5794465130773085, 2.5789485870323743, 2.5864312828859477, 2.586744037559271, 2.5896185889032672, 2.5782482189497924, 2.5974919768585556, 2.585722468365198, 2.5938636078231636, 2.5912695952824185, 2.6107623146476806, 2.597293378488575, 2.595878263999676, 2.6092624503794952, 2.6279285987609713, 2.616757612118776, 2.598921664242674, 2.614015011951841, 2.599543430143585, 2.605126796684829, 2.6138214904490753, 2.6015657016209195, 2.613715598931649, 2.6095855298692174, 2.620789180248242, 2.6144975374876376, 2.6229931885385747, 2.6185210369686382, 2.6197508164423047, 2.620089058022585, 2.6254742599668957, 2.6225661589398563, 2.6293957025938237, 2.6421160435637425, 2.6286752928653963, 2.6445678830734027, 2.636426284003923, 2.6387634394791326, 2.6468474935428263, 2.643615464271583, 2.648072810791592, 2.6413398172663545, 2.643590899328097, 2.661353002823828, 2.6455790303610818, 2.657614038495595, 2.675218320636718, 2.6538825665397208, 2.656729278893306, 2.6810878145283668, 2.6511765266286917, 2.6645548555064082, 2.681962501435053, 2.669450590763186, 2.669927896145725, 2.680288534054811, 2.6820485169077153, 2.672009489219177, 2.673467818543633, 2.7012233820259084, 2.693391714973011, 2.662004091078033, 2.6862062264741544, 2.6745715066912923, 2.676014497753826, 2.6914326502576054, 2.681010546159666, 2.690961057133667, 2.6959549856107614, 2.7099666685502126, 2.696442410276441, 2.6842096270794547, 2.6868160311224427, 2.712313610931923, 2.7119468941868625, 2.6939861214611134, 2.712672042924978, 2.7057418604006713, 2.706869433470352, 2.699218637641819, 2.7117278157000864, 2.7275357156355784, 2.7159201327607354, 2.7074353569638356, 2.717408411217049, 2.713193040763216, 2.7242646526624807, 2.723027238313396, 2.716579693683067, 2.737327369367352, 2.7543717491607165], 'val_acc': [0.08210180583601123, 0.08210180583601123, 0.10016420350237237, 0.12807881772175603, 0.13628899735476582, 0.13628899735476582, 0.15435139482538102, 0.14449917817448552, 0.1510673226736645, 0.1592775030040193, 0.15763546796806144, 0.16420361236936745, 0.16091954002190498, 0.17569786524293066, 0.17241379299334117, 0.17241379309121416, 0.16584564760107126, 0.1789819366973022, 0.167487683725866, 0.17077175607332848, 0.17077175607332848, 0.17898193689304814, 0.1806239730178429, 0.1773399007682534, 0.17405582842079093, 0.1773399007682534, 0.17569786464345866, 0.17569786464345866, 0.1839080452674324, 0.17898193689304814, 0.17733990067038044, 0.17898193689304814, 0.17898193689304814, 0.17241379229599618, 0.17241379229599618, 0.17898193679517518, 0.17241379210025023, 0.17898193689304814, 0.17898193689304814, 0.1806239730178429, 0.18226600904476467, 0.16912971985066075, 0.1806239730178429, 0.17733990067038044, 0.17733990067038044, 0.18062397282209694, 0.17241379219812322, 0.1806239730178429, 0.18390804507168643, 0.18062397291996993, 0.1756978644477127, 0.1756978644477127, 0.18062397291996993, 0.1789819366973022, 0.17898193679517518, 0.18062397291996993, 0.17733990057250745, 0.17733990057250745, 0.17733990067038044, 0.17733990057250745, 0.1756978644477127, 0.18062397291996993, 0.17898193679517518, 0.1756978644477127, 0.17241379219812322, 0.1756978644477127, 0.17733990057250745, 0.17733990067038044, 0.18226600904476467, 0.17733990057250745, 0.1756978644477127, 0.17405582832291797, 0.18062397291996993, 0.18226600904476467, 0.17569786434983972, 0.17733990057250745, 0.16912971985066075, 0.18226600904476467, 0.17733990057250745, 0.17898193689304814, 0.17569786454558567, 0.17733990067038044, 0.18226600904476467, 0.17241379309121416, 0.17898193679517518, 0.17733990047463447, 0.1871921175170219, 0.16912971985066075, 0.18226600924051062, 0.16912971985066075, 0.1871921175170219, 0.17405582842079093, 0.1773399008661264, 0.18226600914263763, 0.1707717559754555, 0.18390804536530536, 0.18226600914263763, 0.1740558285186639, 0.17077175617120144, 0.17405582842079093, 0.18226600924051062, 0.16748768401948494, 0.16584564750319827, 0.1740558285186639, 0.17077175617120144, 0.16256157614669972, 0.17241379229599618, 0.1691297200464067, 0.16256157544935473, 0.17241379239386917, 0.16584564760107126, 0.1691297199485337, 0.17405582832291797, 0.17569786474133164, 0.1625615752536088, 0.16748768392161195, 0.16420361157414948, 0.1658456477968172, 0.16584564760107126, 0.167487683725866, 0.16420361147627652, 0.16912971985066075, 0.17733990047463447, 0.17241379229599618, 0.16748768382373896, 0.16420361246724044, 0.16420361236936745, 0.17569786434983972, 0.17241379219812322, 0.17241379210025023, 0.17405582822504498, 0.16748768382373896, 0.16420361246724044, 0.1707717558775825, 0.17241379210025023, 0.16748768362799302, 0.16256157614669972, 0.16420361217362148, 0.1707717558775825, 0.167487683725866, 0.16912971985066075, 0.16748768362799302, 0.16748768461895694, 0.1773399014655984, 0.17569786524293066, 0.16584564859203518, 0.16420361147627652, 0.17077175686854643, 0.1740558292160089, 0.16420361227149446, 0.16748768461895694, 0.1707717558775825, 0.17241379299334117, 0.17898193749252014, 0.1740558292160089, 0.17241379299334117, 0.18062397272422395, 0.1740558291181359, 0.17241379309121416, 0.1773399013677254, 0.16912972074375168, 0.17077175686854643, 0.18226600983998262, 0.18226600974210966, 0.1773399014655984, 0.18226600983998262, 0.1658456483962892, 0.18226600875114574, 0.1773399014655984, 0.1740558291181359, 0.174055828127172, 0.1707717559754555, 0.17077175686854643, 0.17569786434983972, 0.1740558292160089, 0.17733990047463447, 0.1773399014655984, 0.16912972074375168, 0.17569786425196673, 0.1740558292160089, 0.17077175686854643, 0.17569786534080364, 0.16912972084162467, 0.1658456484941622, 0.1888341532503247, 0.1740558291181359, 0.17733990126985244, 0.17569786425196673, 0.1740558292160089, 0.1773399014655984, 0.17241379200237725, 0.1773399013677254, 0.1691297206458787, 0.17569786524293066, 0.1773399013677254, 0.1707717569664194, 0.1773399014655984, 0.16912972074375168, 0.17733990047463447, 0.1789819365994292, 0.1822660088490187, 0.16748768461895694, 0.1822660088490187, 0.17898193759039313, 0.17405582842079093, 0.17405582842079093, 0.1789819366973022, 0.17733990037676148, 0.17733990037676148, 0.1789819366973022, 0.17569786434983972, 0.18226600894689168, 0.17241379219812322, 0.1740558292160089, 0.17898193650155622, 0.1707717559754555, 0.18226600875114574, 0.17733990057250745, 0.18390804497381344, 0.16584564760107126, 0.17733990047463447, 0.1658456484941622, 0.16912971975278776, 0.17241379229599618, 0.17241379229599618, 0.1756978644477127, 0.17241379309121416, 0.17241379200237725, 0.1773399014655984, 0.16748768362799302, 0.17077175686854643, 0.17569786434983972, 0.17405582822504498, 0.16912971975278776, 0.17733990057250745, 0.1707717558775825, 0.167487683725866, 0.17898193650155622, 0.1773399014655984, 0.17569786434983972, 0.17733990037676148, 0.17077175607332848, 0.16912971985066075, 0.16912971985066075, 0.17733990047463447, 0.17569786434983972, 0.1707717558775825, 0.16256157535148175, 0.16420361137840353, 0.16091953912881404, 0.1707717559754555, 0.17241379200237725, 0.16420361167202246, 0.17569786464345866, 0.16912971985066075, 0.16420361167202246, 0.16584564760107126, 0.16584564760107126, 0.1855500815879731, 0.17569786434983972, 0.15927750310189226, 0.16091953912881404, 0.16091953942243298, 0.16584564769894422, 0.1625615752536088, 0.16420361176989545, 0.16912971975278776, 0.15763546796806144, 0.16256157535148175, 0.16420361167202246, 0.17241379239386917, 0.1625615752536088, 0.16748768392161195, 0.16256157554722772, 0.1559934318432667, 0.167487683725866, 0.16420361157414948, 0.1658456478946902, 0.17077175617120144, 0.16091953912881404, 0.1789819365994292, 0.1576354671728435, 0.16748768382373896, 0.15599343095017576, 0.16584564769894422, 0.16256157574297367, 0.16912972014427968, 0.1707717559754555, 0.17405582842079093, 0.16091953942243298, 0.15106732257579153, 0.1691297200464067, 0.16748768401948494, 0.1576354670749705, 0.15270935860271329, 0.15599343085230277, 0.15763546697709752, 0.17405582842079093, 0.16091953932456, 0.16091953942243298], 'loss': [2.7051175140991837, 2.6914108554440603, 2.6826321650824263, 2.6738993470428905, 2.6652366987733624, 2.654906161854644, 2.6431025858532475, 2.629582868121733, 2.613632223787249, 2.5953746848527413, 2.575642704718901, 2.555425133107869, 2.5363251594057807, 2.523105815499715, 2.511931922058795, 2.5009340411583745, 2.4946934338712596, 2.4887942338626243, 2.4849882976719977, 2.4820263015660906, 2.475904272028553, 2.4736595977258387, 2.471072830948252, 2.468620545467557, 2.4672953143501672, 2.4672205508856804, 2.4633112440363827, 2.4615886760688173, 2.4601485205382048, 2.458841190690622, 2.4580458745819342, 2.4567930519213665, 2.4556635944750274, 2.45438819714885, 2.453229211732837, 2.452579634586154, 2.4541628628051257, 2.4528619743715323, 2.452561608574963, 2.4514250816016228, 2.4515503932318405, 2.4476118167078225, 2.447559584827149, 2.4464904698013523, 2.4469519989201665, 2.4453890754456884, 2.4439297856246665, 2.4431122341195173, 2.4428493749434455, 2.4433607907265853, 2.441781048608267, 2.4415287799169394, 2.4400934356438797, 2.438856707461316, 2.4390471002159666, 2.43920557611532, 2.4392982114756623, 2.437436429088365, 2.4365405750470486, 2.4354515490835453, 2.4354156144590595, 2.435624764146746, 2.4342225522231273, 2.433376874767045, 2.432212484641731, 2.431453945210827, 2.431342520997754, 2.429836806036853, 2.430480323337187, 2.4313062900880036, 2.4288105811916094, 2.428444323549525, 2.4272169313881187, 2.4266860698527624, 2.4252654106954776, 2.4254278297541814, 2.426907195986174, 2.4241282127231543, 2.425992143227579, 2.423496060400773, 2.4222099043750176, 2.4222170398954983, 2.4218793137607144, 2.42236608861653, 2.423627843308498, 2.4209096745054337, 2.420136495880033, 2.4202659375858504, 2.4180878236798047, 2.416254464312011, 2.4156080418298864, 2.414998791790596, 2.4139190680682048, 2.411841255438646, 2.4123556985992183, 2.410813989972187, 2.409547615345009, 2.4108967853522643, 2.408409452242528, 2.408409606259951, 2.408644076490304, 2.407062262333394, 2.407586067070462, 2.407430130940933, 2.405316292529723, 2.4088878518012513, 2.407811768294849, 2.3995048178294844, 2.4009142504580456, 2.3982946736611868, 2.397095412346372, 2.3992522985783444, 2.3977005838123926, 2.3996629774692857, 2.3965320163194157, 2.3933410320438644, 2.391733896463069, 2.387548135976772, 2.391107894067157, 2.3878724104080358, 2.3873098980474765, 2.3858437613540118, 2.3814091933091808, 2.3811558530560752, 2.3795106232778247, 2.380452594130436, 2.381783946583648, 2.3882039745730297, 2.3838243783132254, 2.3819561628345594, 2.373276314157725, 2.3751757710866124, 2.3712375413710576, 2.370077606490995, 2.368201346857592, 2.365026594138488, 2.369666792088221, 2.378299116353969, 2.3795042685169947, 2.364398238791088, 2.362748627692033, 2.359607314182258, 2.362133654823538, 2.357008912969664, 2.356167516865035, 2.3588828855471444, 2.351283112296823, 2.3546469053938157, 2.3517735790667835, 2.3533071875327423, 2.355893474731602, 2.3487693734727113, 2.3412231914316606, 2.34281134859982, 2.3393937867769714, 2.341127203524235, 2.338384691351983, 2.337454151470803, 2.3380277274325643, 2.3326957879859562, 2.329335432718422, 2.3260707952158652, 2.3247094410890425, 2.323251205988733, 2.326573864880039, 2.3316601439912708, 2.327582485817786, 2.3274646571039908, 2.324412169152951, 2.324899402927814, 2.326706259451363, 2.32097232160627, 2.3126534533451717, 2.310919671185942, 2.3084992450855104, 2.307944778348386, 2.305504202402103, 2.302812381642555, 2.301589349946447, 2.297215718655126, 2.296400304300829, 2.295292614323892, 2.2913525514778903, 2.29358846852422, 2.290596955317002, 2.2943653244747026, 2.2969038829176824, 2.3093895083824956, 2.309761020241332, 2.2966461458734906, 2.2866593976774743, 2.2779042342849825, 2.2789042352406153, 2.272691084569974, 2.2735378872442538, 2.2678476253329363, 2.2699237400501415, 2.271451639492654, 2.277607006801472, 2.2862330085198246, 2.2661457935153093, 2.2671157169146214, 2.2591198268122743, 2.257137857766122, 2.255159027326768, 2.2585441643207713, 2.259313950842166, 2.254488785800503, 2.2543219994225785, 2.2514011413409722, 2.2423418139040594, 2.2416789071515844, 2.2484515991054277, 2.243711488986162, 2.236979347088009, 2.238831307413152, 2.2319058856924947, 2.2313800263453802, 2.232177287890926, 2.232318921451451, 2.2314859196390704, 2.2252408420280756, 2.2316682060641186, 2.2293253367931203, 2.243011074927798, 2.240992597877612, 2.2453823148347514, 2.2352812640720816, 2.231121888542567, 2.230067281801353, 2.22963215301169, 2.244526775074201, 2.220097111578595, 2.2207238016187287, 2.218013483396056, 2.2086808693237616, 2.2113251565662986, 2.2115918328874655, 2.2025660757656214, 2.195579996735653, 2.1993799333944457, 2.203718630733921, 2.191547905982642, 2.1895597635108586, 2.1977343136280223, 2.1935410010986014, 2.184743436993515, 2.181938663596245, 2.1795695648545848, 2.1742624577555567, 2.173163120066116, 2.173077823200265, 2.1711833184259874, 2.1707132954372272, 2.168812779087795, 2.1721532621912396, 2.1722675063037284, 2.164982959328246, 2.17123815097848, 2.1694451573937825, 2.1728935762597303, 2.163915262281038, 2.15800067854613, 2.1554602963723686, 2.159881544504812, 2.1497693336964634, 2.156409265569103, 2.1730737343461115, 2.1648161385827973, 2.1646977109341163, 2.1571648646673873, 2.175532998686209, 2.1557633206095295, 2.1421800857207125, 2.1477324152874018, 2.139349166176892, 2.1450869428058916, 2.139237782793613, 2.1394117566839137, 2.147217718825448, 2.135221262976864, 2.1515145151032558, 2.139518038399166, 2.1349728003664428, 2.1336878737385025, 2.12561948686165, 2.128364009680934, 2.135170898006682, 2.1538422244774975, 2.1231812268556753, 2.1101903309322725, 2.111038984067631, 2.110092722268075, 2.1087174081704454, 2.103467384941524, 2.1017313097536685, 2.0992582485661124, 2.0974947829510886, 2.103120172684687, 2.103233674909055], 'acc': [0.09486652932739845, 0.0813141683319266, 0.08747433259379447, 0.10965092414703212, 0.13100616029584186, 0.13388090381815693, 0.13552361448626254, 0.15154004078014185, 0.14661191055662087, 0.14702258797886436, 0.15564681696084, 0.1634496924874719, 0.1626283358621891, 0.16344969289748332, 0.16878850032906267, 0.16427104733195882, 0.16878850072071538, 0.17330595473366842, 0.1712525678182774, 0.16837782331683063, 0.17166324385137774, 0.1753593428240175, 0.17125256686668378, 0.1753593428240175, 0.1687885011307268, 0.17659137585569457, 0.1741273095414379, 0.1749486653834153, 0.17782340792659862, 0.17618069784597204, 0.17412730955979663, 0.177002052690459, 0.17741273191185702, 0.17577002061955493, 0.17166324347808376, 0.17289527631393448, 0.17494866557924166, 0.16837782310264557, 0.1728952767239459, 0.17166324443885678, 0.1782340867196265, 0.1765913748582041, 0.17453798835282452, 0.17659137526821553, 0.17946611998384737, 0.1790554417599398, 0.1786447629669119, 0.1774127303085288, 0.1839835739784417, 0.18028747461414923, 0.17864476357274967, 0.17864476373185856, 0.18110883006447395, 0.18028747523834573, 0.1802874748099756, 0.17700205327793803, 0.18603696187296442, 0.18028747500580194, 0.1839835720018195, 0.18193018469477582, 0.18521560603098702, 0.1774127298985174, 0.1831622177264529, 0.18480492843127594, 0.18439425116814137, 0.18521560485602895, 0.18685831511412312, 0.18685831709074532, 0.18398357337260393, 0.18357289438374969, 0.18644763751441204, 0.18850102636970778, 0.1868583153283082, 0.18891170341865726, 0.18726899470881514, 0.18891170361448362, 0.18562628445072096, 0.1897330586364382, 0.18110883063359426, 0.18480492921458133, 0.18480492901875498, 0.18644763929520788, 0.1909650928797908, 0.18932238279916422, 0.18234086211701928, 0.19096509190065905, 0.18973305922391723, 0.1926078019812856, 0.18932238221168518, 0.19178644751009266, 0.1926078027462323, 0.19096509150900634, 0.19260780296041735, 0.1897330606130604, 0.19425051382434932, 0.1901437374478248, 0.19055441508425336, 0.1909650917048327, 0.19055441406840418, 0.19178644852594184, 0.1950718676897045, 0.1926078035478964, 0.19096509188230032, 0.1913757693045438, 0.19137576991038155, 0.19055441506589463, 0.1880903479866913, 0.19589322468828127, 0.19876796781894362, 0.1991786448495344, 0.19507186865047751, 0.19507186962960926, 0.19712525615334756, 0.19958932344673597, 0.19753593476890782, 0.19589322351332317, 0.20041067651042704, 0.20041067768538512, 0.19548254587689465, 0.20082135548092256, 0.19589322349496446, 0.20369609825664967, 0.20616016337758952, 0.20451745329696294, 0.2053388079272648, 0.2065708421706174, 0.20533880968970195, 0.19958932147011374, 0.19917864465370805, 0.20246406559826657, 0.20533880970806068, 0.2008213551259873, 0.20862423004678143, 0.21026694073324576, 0.20287474421382684, 0.21273100620912086, 0.20739219781676846, 0.19753593318393833, 0.20328542183189666, 0.2082135530345494, 0.2156057491623156, 0.21355236146361922, 0.20574948752195682, 0.2094455854603887, 0.21437371750142295, 0.2078028756123059, 0.21724845885128946, 0.20944558430378932, 0.21108829458024223, 0.21149897339162885, 0.2135523606619551, 0.21067761754965145, 0.22094455937218127, 0.2184804917054989, 0.22053388136245877, 0.21765913764431735, 0.21848049309464207, 0.2213552362069457, 0.21971252495136104, 0.2225872680636647, 0.22135523642113075, 0.22258726984446053, 0.22464065693731916, 0.22546201160433846, 0.2229979468750513, 0.22176591284588376, 0.22381930115041793, 0.22381930132788555, 0.22833675590995892, 0.23039014380448164, 0.21930184852660803, 0.23285420951290053, 0.23203285347509678, 0.23162217744199648, 0.23162217548373298, 0.2320328531018028, 0.23080082238332447, 0.23367556453485508, 0.23162217648122346, 0.23531827579043973, 0.24024640742146258, 0.23655030727386475, 0.24147843849487619, 0.23203285466841359, 0.24312114935880813, 0.23860369694918332, 0.2394250504045271, 0.22874743352802873, 0.23121149863060986, 0.24147844048985712, 0.2381930193127548, 0.24394250537825315, 0.2447638602410988, 0.2525667349844253, 0.24804928116729863, 0.24928131402150805, 0.24763860452836053, 0.2464065728491092, 0.2427104727382288, 0.241478439688193, 0.2501026670668404, 0.2513347034826416, 0.25790554537175864, 0.25585215767306224, 0.2566735129092019, 0.24969199398949407, 0.2517453803357647, 0.252566735749372, 0.25133470328681523, 0.2546201236438947, 0.2579055436093215, 0.25831622124575004, 0.2562628347220117, 0.26036960810606485, 0.26570841894012703, 0.25954825603986426, 0.2644763857126236, 0.2648870627248556, 0.26529774231954767, 0.271868581607846, 0.2558521563022778, 0.2706365481661575, 0.26119096455388, 0.2677618078137815, 0.2554414788800343, 0.25462012327060074, 0.25379876602112145, 0.26242299779974215, 0.262012319985846, 0.2644763858900912, 0.26365503261221507, 0.25626283509530573, 0.267351127221599, 0.2677618084012605, 0.26570841753262514, 0.27433264751209124, 0.26858316402905286, 0.27022587035226137, 0.275564679974648, 0.28008213357758965, 0.2759753613501359, 0.275564679974648, 0.2825462028108828, 0.27967145756284806, 0.27556468157797626, 0.2776180676733444, 0.2850102677727137, 0.279671458309436, 0.2837782337435462, 0.28624229843611587, 0.2895277209472852, 0.29158110649189173, 0.2866529766416648, 0.2903490781784058, 0.2874743346560907, 0.28008213674752863, 0.28501026519025374, 0.29158110825432887, 0.28213552303872313, 0.28952772176730807, 0.28172484819893967, 0.28788500812508977, 0.286652975075054, 0.2915811074710235, 0.2919917858723987, 0.2965092388878613, 0.2887063659069719, 0.27679671263303113, 0.28542094359162895, 0.28706365288895014, 0.286242299611074, 0.2829568810531491, 0.29034907383350866, 0.29240246211968407, 0.287474331094499, 0.29815195253007953, 0.2952772074411537, 0.2924024648979704, 0.2969199168975838, 0.2977412727212025, 0.2985626267456666, 0.2895277187931954, 0.2940451727877897, 0.28952772153476425, 0.29568788482667974, 0.29568788564670256, 0.29774127467946593, 0.29486653041056293, 0.2870636570380209, 0.3051334696506328, 0.3117043115397498, 0.3010266928824556, 0.3137576998259253, 0.3051334714130699, 0.30841889118267035, 0.30759753770896786, 0.31827515499547765, 0.31827515182553867, 0.312114989941125, 0.3075975369256625]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
