{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf7.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 13:46:15 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'Split', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 15 Label(s): ['sg', 'ce', 'ib', 'eo', 'by', 'ek', 'yd', 'ck', 'eg', 'eb', 'ds', 'mb', 'aa', 'my', 'sk'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000017E97E74240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000017ECEB67EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6992, Accuracy:0.1064, Validation Loss:2.6846, Validation Accuracy:0.1154\n",
    "Epoch #2: Loss:2.6760, Accuracy:0.1281, Validation Loss:2.6641, Validation Accuracy:0.1383\n",
    "Epoch #3: Loss:2.6517, Accuracy:0.1354, Validation Loss:2.6375, Validation Accuracy:0.1449\n",
    "Epoch #4: Loss:2.6109, Accuracy:0.1569, Validation Loss:2.5882, Validation Accuracy:0.1556\n",
    "Epoch #5: Loss:2.5598, Accuracy:0.1641, Validation Loss:2.5458, Validation Accuracy:0.1617\n",
    "Epoch #6: Loss:2.5235, Accuracy:0.1628, Validation Loss:2.5238, Validation Accuracy:0.1663\n",
    "Epoch #7: Loss:2.5054, Accuracy:0.1664, Validation Loss:2.5013, Validation Accuracy:0.1679\n",
    "Epoch #8: Loss:2.4835, Accuracy:0.1705, Validation Loss:2.4947, Validation Accuracy:0.1630\n",
    "Epoch #9: Loss:2.4736, Accuracy:0.1731, Validation Loss:2.4867, Validation Accuracy:0.1617\n",
    "Epoch #10: Loss:2.4650, Accuracy:0.1753, Validation Loss:2.4837, Validation Accuracy:0.1675\n",
    "Epoch #11: Loss:2.4649, Accuracy:0.1764, Validation Loss:2.4785, Validation Accuracy:0.1683\n",
    "Epoch #12: Loss:2.4612, Accuracy:0.1763, Validation Loss:2.4731, Validation Accuracy:0.1667\n",
    "Epoch #13: Loss:2.4549, Accuracy:0.1765, Validation Loss:2.4781, Validation Accuracy:0.1617\n",
    "Epoch #14: Loss:2.4583, Accuracy:0.1736, Validation Loss:2.4709, Validation Accuracy:0.1683\n",
    "Epoch #15: Loss:2.4509, Accuracy:0.1780, Validation Loss:2.4699, Validation Accuracy:0.1687\n",
    "Epoch #16: Loss:2.4500, Accuracy:0.1791, Validation Loss:2.4699, Validation Accuracy:0.1658\n",
    "Epoch #17: Loss:2.4500, Accuracy:0.1760, Validation Loss:2.4719, Validation Accuracy:0.1663\n",
    "Epoch #18: Loss:2.4463, Accuracy:0.1747, Validation Loss:2.4732, Validation Accuracy:0.1679\n",
    "Epoch #19: Loss:2.4485, Accuracy:0.1744, Validation Loss:2.4830, Validation Accuracy:0.1642\n",
    "Epoch #20: Loss:2.4531, Accuracy:0.1738, Validation Loss:2.4754, Validation Accuracy:0.1609\n",
    "Epoch #21: Loss:2.4471, Accuracy:0.1751, Validation Loss:2.4707, Validation Accuracy:0.1675\n",
    "Epoch #22: Loss:2.4448, Accuracy:0.1752, Validation Loss:2.4669, Validation Accuracy:0.1667\n",
    "Epoch #23: Loss:2.4430, Accuracy:0.1746, Validation Loss:2.4669, Validation Accuracy:0.1683\n",
    "Epoch #24: Loss:2.4450, Accuracy:0.1749, Validation Loss:2.4679, Validation Accuracy:0.1626\n",
    "Epoch #25: Loss:2.4426, Accuracy:0.1746, Validation Loss:2.4729, Validation Accuracy:0.1622\n",
    "Epoch #26: Loss:2.4428, Accuracy:0.1743, Validation Loss:2.4647, Validation Accuracy:0.1650\n",
    "Epoch #27: Loss:2.4402, Accuracy:0.1746, Validation Loss:2.4653, Validation Accuracy:0.1646\n",
    "Epoch #28: Loss:2.4449, Accuracy:0.1770, Validation Loss:2.4660, Validation Accuracy:0.1654\n",
    "Epoch #29: Loss:2.4398, Accuracy:0.1775, Validation Loss:2.4636, Validation Accuracy:0.1634\n",
    "Epoch #30: Loss:2.4383, Accuracy:0.1778, Validation Loss:2.4632, Validation Accuracy:0.1634\n",
    "Epoch #31: Loss:2.4365, Accuracy:0.1783, Validation Loss:2.4609, Validation Accuracy:0.1667\n",
    "Epoch #32: Loss:2.4380, Accuracy:0.1785, Validation Loss:2.4641, Validation Accuracy:0.1622\n",
    "Epoch #33: Loss:2.4363, Accuracy:0.1792, Validation Loss:2.4632, Validation Accuracy:0.1671\n",
    "Epoch #34: Loss:2.4363, Accuracy:0.1796, Validation Loss:2.4640, Validation Accuracy:0.1658\n",
    "Epoch #35: Loss:2.4363, Accuracy:0.1781, Validation Loss:2.4645, Validation Accuracy:0.1638\n",
    "Epoch #36: Loss:2.4371, Accuracy:0.1822, Validation Loss:2.4661, Validation Accuracy:0.1605\n",
    "Epoch #37: Loss:2.4359, Accuracy:0.1815, Validation Loss:2.4643, Validation Accuracy:0.1704\n",
    "Epoch #38: Loss:2.4345, Accuracy:0.1816, Validation Loss:2.4619, Validation Accuracy:0.1646\n",
    "Epoch #39: Loss:2.4382, Accuracy:0.1828, Validation Loss:2.4601, Validation Accuracy:0.1695\n",
    "Epoch #40: Loss:2.4337, Accuracy:0.1810, Validation Loss:2.4627, Validation Accuracy:0.1667\n",
    "Epoch #41: Loss:2.4351, Accuracy:0.1803, Validation Loss:2.4622, Validation Accuracy:0.1691\n",
    "Epoch #42: Loss:2.4347, Accuracy:0.1809, Validation Loss:2.4618, Validation Accuracy:0.1654\n",
    "Epoch #43: Loss:2.4359, Accuracy:0.1774, Validation Loss:2.4615, Validation Accuracy:0.1667\n",
    "Epoch #44: Loss:2.4366, Accuracy:0.1759, Validation Loss:2.4622, Validation Accuracy:0.1638\n",
    "Epoch #45: Loss:2.4363, Accuracy:0.1773, Validation Loss:2.4616, Validation Accuracy:0.1642\n",
    "Epoch #46: Loss:2.4331, Accuracy:0.1769, Validation Loss:2.4600, Validation Accuracy:0.1675\n",
    "Epoch #47: Loss:2.4332, Accuracy:0.1816, Validation Loss:2.4607, Validation Accuracy:0.1691\n",
    "Epoch #48: Loss:2.4364, Accuracy:0.1763, Validation Loss:2.4628, Validation Accuracy:0.1708\n",
    "Epoch #49: Loss:2.4344, Accuracy:0.1767, Validation Loss:2.4633, Validation Accuracy:0.1638\n",
    "Epoch #50: Loss:2.4371, Accuracy:0.1806, Validation Loss:2.4618, Validation Accuracy:0.1675\n",
    "Epoch #51: Loss:2.4352, Accuracy:0.1804, Validation Loss:2.4615, Validation Accuracy:0.1671\n",
    "Epoch #52: Loss:2.4310, Accuracy:0.1793, Validation Loss:2.4630, Validation Accuracy:0.1687\n",
    "Epoch #53: Loss:2.4315, Accuracy:0.1842, Validation Loss:2.4593, Validation Accuracy:0.1716\n",
    "Epoch #54: Loss:2.4304, Accuracy:0.1833, Validation Loss:2.4593, Validation Accuracy:0.1683\n",
    "Epoch #55: Loss:2.4307, Accuracy:0.1859, Validation Loss:2.4601, Validation Accuracy:0.1663\n",
    "Epoch #56: Loss:2.4307, Accuracy:0.1820, Validation Loss:2.4594, Validation Accuracy:0.1712\n",
    "Epoch #57: Loss:2.4319, Accuracy:0.1832, Validation Loss:2.4577, Validation Accuracy:0.1691\n",
    "Epoch #58: Loss:2.4302, Accuracy:0.1810, Validation Loss:2.4578, Validation Accuracy:0.1691\n",
    "Epoch #59: Loss:2.4307, Accuracy:0.1835, Validation Loss:2.4571, Validation Accuracy:0.1687\n",
    "Epoch #60: Loss:2.4315, Accuracy:0.1857, Validation Loss:2.4584, Validation Accuracy:0.1667\n",
    "Epoch #61: Loss:2.4305, Accuracy:0.1845, Validation Loss:2.4587, Validation Accuracy:0.1683\n",
    "Epoch #62: Loss:2.4288, Accuracy:0.1831, Validation Loss:2.4601, Validation Accuracy:0.1667\n",
    "Epoch #63: Loss:2.4305, Accuracy:0.1852, Validation Loss:2.4609, Validation Accuracy:0.1687\n",
    "Epoch #64: Loss:2.4364, Accuracy:0.1763, Validation Loss:2.4743, Validation Accuracy:0.1667\n",
    "Epoch #65: Loss:2.4449, Accuracy:0.1763, Validation Loss:2.4598, Validation Accuracy:0.1700\n",
    "Epoch #66: Loss:2.4349, Accuracy:0.1780, Validation Loss:2.4592, Validation Accuracy:0.1683\n",
    "Epoch #67: Loss:2.4274, Accuracy:0.1853, Validation Loss:2.4584, Validation Accuracy:0.1650\n",
    "Epoch #68: Loss:2.4291, Accuracy:0.1819, Validation Loss:2.4589, Validation Accuracy:0.1654\n",
    "Epoch #69: Loss:2.4334, Accuracy:0.1830, Validation Loss:2.4597, Validation Accuracy:0.1667\n",
    "Epoch #70: Loss:2.4318, Accuracy:0.1838, Validation Loss:2.4629, Validation Accuracy:0.1658\n",
    "Epoch #71: Loss:2.4326, Accuracy:0.1831, Validation Loss:2.4639, Validation Accuracy:0.1700\n",
    "Epoch #72: Loss:2.4334, Accuracy:0.1844, Validation Loss:2.4660, Validation Accuracy:0.1720\n",
    "Epoch #73: Loss:2.4372, Accuracy:0.1790, Validation Loss:2.4647, Validation Accuracy:0.1626\n",
    "Epoch #74: Loss:2.4327, Accuracy:0.1850, Validation Loss:2.4642, Validation Accuracy:0.1626\n",
    "Epoch #75: Loss:2.4296, Accuracy:0.1847, Validation Loss:2.4609, Validation Accuracy:0.1679\n",
    "Epoch #76: Loss:2.4300, Accuracy:0.1840, Validation Loss:2.4629, Validation Accuracy:0.1683\n",
    "Epoch #77: Loss:2.4288, Accuracy:0.1840, Validation Loss:2.4613, Validation Accuracy:0.1691\n",
    "Epoch #78: Loss:2.4305, Accuracy:0.1856, Validation Loss:2.4649, Validation Accuracy:0.1679\n",
    "Epoch #79: Loss:2.4305, Accuracy:0.1842, Validation Loss:2.4623, Validation Accuracy:0.1663\n",
    "Epoch #80: Loss:2.4311, Accuracy:0.1842, Validation Loss:2.4631, Validation Accuracy:0.1593\n",
    "Epoch #81: Loss:2.4277, Accuracy:0.1822, Validation Loss:2.4643, Validation Accuracy:0.1642\n",
    "Epoch #82: Loss:2.4304, Accuracy:0.1797, Validation Loss:2.4623, Validation Accuracy:0.1654\n",
    "Epoch #83: Loss:2.4301, Accuracy:0.1819, Validation Loss:2.4623, Validation Accuracy:0.1654\n",
    "Epoch #84: Loss:2.4302, Accuracy:0.1855, Validation Loss:2.4623, Validation Accuracy:0.1630\n",
    "Epoch #85: Loss:2.4283, Accuracy:0.1811, Validation Loss:2.4605, Validation Accuracy:0.1667\n",
    "Epoch #86: Loss:2.4274, Accuracy:0.1834, Validation Loss:2.4595, Validation Accuracy:0.1675\n",
    "Epoch #87: Loss:2.4295, Accuracy:0.1839, Validation Loss:2.4575, Validation Accuracy:0.1704\n",
    "Epoch #88: Loss:2.4280, Accuracy:0.1845, Validation Loss:2.4605, Validation Accuracy:0.1654\n",
    "Epoch #89: Loss:2.4293, Accuracy:0.1828, Validation Loss:2.4598, Validation Accuracy:0.1626\n",
    "Epoch #90: Loss:2.4319, Accuracy:0.1838, Validation Loss:2.4580, Validation Accuracy:0.1691\n",
    "Epoch #91: Loss:2.4272, Accuracy:0.1869, Validation Loss:2.4571, Validation Accuracy:0.1671\n",
    "Epoch #92: Loss:2.4264, Accuracy:0.1847, Validation Loss:2.4581, Validation Accuracy:0.1683\n",
    "Epoch #93: Loss:2.4247, Accuracy:0.1841, Validation Loss:2.4591, Validation Accuracy:0.1650\n",
    "Epoch #94: Loss:2.4288, Accuracy:0.1851, Validation Loss:2.4592, Validation Accuracy:0.1667\n",
    "Epoch #95: Loss:2.4281, Accuracy:0.1860, Validation Loss:2.4591, Validation Accuracy:0.1683\n",
    "Epoch #96: Loss:2.4290, Accuracy:0.1825, Validation Loss:2.4586, Validation Accuracy:0.1663\n",
    "Epoch #97: Loss:2.4269, Accuracy:0.1846, Validation Loss:2.4630, Validation Accuracy:0.1667\n",
    "Epoch #98: Loss:2.4304, Accuracy:0.1851, Validation Loss:2.4621, Validation Accuracy:0.1667\n",
    "Epoch #99: Loss:2.4304, Accuracy:0.1839, Validation Loss:2.4580, Validation Accuracy:0.1671\n",
    "Epoch #100: Loss:2.4270, Accuracy:0.1842, Validation Loss:2.4576, Validation Accuracy:0.1704\n",
    "Epoch #101: Loss:2.4253, Accuracy:0.1836, Validation Loss:2.4596, Validation Accuracy:0.1720\n",
    "Epoch #102: Loss:2.4270, Accuracy:0.1820, Validation Loss:2.4578, Validation Accuracy:0.1695\n",
    "Epoch #103: Loss:2.4263, Accuracy:0.1832, Validation Loss:2.4596, Validation Accuracy:0.1720\n",
    "Epoch #104: Loss:2.4262, Accuracy:0.1851, Validation Loss:2.4581, Validation Accuracy:0.1683\n",
    "Epoch #105: Loss:2.4276, Accuracy:0.1860, Validation Loss:2.4587, Validation Accuracy:0.1658\n",
    "Epoch #106: Loss:2.4278, Accuracy:0.1847, Validation Loss:2.4589, Validation Accuracy:0.1683\n",
    "Epoch #107: Loss:2.4262, Accuracy:0.1870, Validation Loss:2.4602, Validation Accuracy:0.1728\n",
    "Epoch #108: Loss:2.4292, Accuracy:0.1843, Validation Loss:2.4653, Validation Accuracy:0.1679\n",
    "Epoch #109: Loss:2.4257, Accuracy:0.1843, Validation Loss:2.4558, Validation Accuracy:0.1691\n",
    "Epoch #110: Loss:2.4236, Accuracy:0.1859, Validation Loss:2.4564, Validation Accuracy:0.1650\n",
    "Epoch #111: Loss:2.4235, Accuracy:0.1842, Validation Loss:2.4578, Validation Accuracy:0.1650\n",
    "Epoch #112: Loss:2.4244, Accuracy:0.1849, Validation Loss:2.4585, Validation Accuracy:0.1691\n",
    "Epoch #113: Loss:2.4253, Accuracy:0.1860, Validation Loss:2.4575, Validation Accuracy:0.1663\n",
    "Epoch #114: Loss:2.4256, Accuracy:0.1848, Validation Loss:2.4620, Validation Accuracy:0.1597\n",
    "Epoch #115: Loss:2.4263, Accuracy:0.1821, Validation Loss:2.4617, Validation Accuracy:0.1650\n",
    "Epoch #116: Loss:2.4258, Accuracy:0.1861, Validation Loss:2.4600, Validation Accuracy:0.1671\n",
    "Epoch #117: Loss:2.4258, Accuracy:0.1834, Validation Loss:2.4583, Validation Accuracy:0.1675\n",
    "Epoch #118: Loss:2.4255, Accuracy:0.1834, Validation Loss:2.4586, Validation Accuracy:0.1568\n",
    "Epoch #119: Loss:2.4242, Accuracy:0.1860, Validation Loss:2.4575, Validation Accuracy:0.1679\n",
    "Epoch #120: Loss:2.4244, Accuracy:0.1859, Validation Loss:2.4568, Validation Accuracy:0.1622\n",
    "Epoch #121: Loss:2.4236, Accuracy:0.1839, Validation Loss:2.4559, Validation Accuracy:0.1687\n",
    "Epoch #122: Loss:2.4238, Accuracy:0.1852, Validation Loss:2.4578, Validation Accuracy:0.1658\n",
    "Epoch #123: Loss:2.4249, Accuracy:0.1855, Validation Loss:2.4586, Validation Accuracy:0.1654\n",
    "Epoch #124: Loss:2.4234, Accuracy:0.1843, Validation Loss:2.4567, Validation Accuracy:0.1609\n",
    "Epoch #125: Loss:2.4233, Accuracy:0.1833, Validation Loss:2.4581, Validation Accuracy:0.1626\n",
    "Epoch #126: Loss:2.4246, Accuracy:0.1829, Validation Loss:2.4554, Validation Accuracy:0.1671\n",
    "Epoch #127: Loss:2.4215, Accuracy:0.1854, Validation Loss:2.4664, Validation Accuracy:0.1646\n",
    "Epoch #128: Loss:2.4242, Accuracy:0.1844, Validation Loss:2.4538, Validation Accuracy:0.1675\n",
    "Epoch #129: Loss:2.4260, Accuracy:0.1851, Validation Loss:2.4533, Validation Accuracy:0.1675\n",
    "Epoch #130: Loss:2.4253, Accuracy:0.1796, Validation Loss:2.4548, Validation Accuracy:0.1675\n",
    "Epoch #131: Loss:2.4272, Accuracy:0.1866, Validation Loss:2.4592, Validation Accuracy:0.1650\n",
    "Epoch #132: Loss:2.4261, Accuracy:0.1852, Validation Loss:2.4579, Validation Accuracy:0.1650\n",
    "Epoch #133: Loss:2.4315, Accuracy:0.1826, Validation Loss:2.4581, Validation Accuracy:0.1675\n",
    "Epoch #134: Loss:2.4300, Accuracy:0.1857, Validation Loss:2.4602, Validation Accuracy:0.1700\n",
    "Epoch #135: Loss:2.4288, Accuracy:0.1859, Validation Loss:2.4600, Validation Accuracy:0.1626\n",
    "Epoch #136: Loss:2.4250, Accuracy:0.1852, Validation Loss:2.4592, Validation Accuracy:0.1654\n",
    "Epoch #137: Loss:2.4262, Accuracy:0.1851, Validation Loss:2.4569, Validation Accuracy:0.1654\n",
    "Epoch #138: Loss:2.4242, Accuracy:0.1857, Validation Loss:2.4570, Validation Accuracy:0.1646\n",
    "Epoch #139: Loss:2.4259, Accuracy:0.1848, Validation Loss:2.4582, Validation Accuracy:0.1671\n",
    "Epoch #140: Loss:2.4262, Accuracy:0.1873, Validation Loss:2.4573, Validation Accuracy:0.1704\n",
    "Epoch #141: Loss:2.4247, Accuracy:0.1825, Validation Loss:2.4567, Validation Accuracy:0.1671\n",
    "Epoch #142: Loss:2.4244, Accuracy:0.1845, Validation Loss:2.4564, Validation Accuracy:0.1700\n",
    "Epoch #143: Loss:2.4228, Accuracy:0.1864, Validation Loss:2.4557, Validation Accuracy:0.1658\n",
    "Epoch #144: Loss:2.4215, Accuracy:0.1913, Validation Loss:2.4534, Validation Accuracy:0.1736\n",
    "Epoch #145: Loss:2.4216, Accuracy:0.1886, Validation Loss:2.4525, Validation Accuracy:0.1720\n",
    "Epoch #146: Loss:2.4249, Accuracy:0.1848, Validation Loss:2.4540, Validation Accuracy:0.1708\n",
    "Epoch #147: Loss:2.4230, Accuracy:0.1870, Validation Loss:2.4558, Validation Accuracy:0.1667\n",
    "Epoch #148: Loss:2.4206, Accuracy:0.1876, Validation Loss:2.4530, Validation Accuracy:0.1720\n",
    "Epoch #149: Loss:2.4214, Accuracy:0.1864, Validation Loss:2.4556, Validation Accuracy:0.1687\n",
    "Epoch #150: Loss:2.4212, Accuracy:0.1866, Validation Loss:2.4548, Validation Accuracy:0.1691\n",
    "Epoch #151: Loss:2.4207, Accuracy:0.1854, Validation Loss:2.4564, Validation Accuracy:0.1613\n",
    "Epoch #152: Loss:2.4219, Accuracy:0.1870, Validation Loss:2.4526, Validation Accuracy:0.1679\n",
    "Epoch #153: Loss:2.4217, Accuracy:0.1859, Validation Loss:2.4523, Validation Accuracy:0.1663\n",
    "Epoch #154: Loss:2.4233, Accuracy:0.1851, Validation Loss:2.4506, Validation Accuracy:0.1708\n",
    "Epoch #155: Loss:2.4204, Accuracy:0.1847, Validation Loss:2.4514, Validation Accuracy:0.1687\n",
    "Epoch #156: Loss:2.4194, Accuracy:0.1855, Validation Loss:2.4501, Validation Accuracy:0.1712\n",
    "Epoch #157: Loss:2.4190, Accuracy:0.1868, Validation Loss:2.4498, Validation Accuracy:0.1700\n",
    "Epoch #158: Loss:2.4221, Accuracy:0.1849, Validation Loss:2.4525, Validation Accuracy:0.1667\n",
    "Epoch #159: Loss:2.4236, Accuracy:0.1845, Validation Loss:2.4514, Validation Accuracy:0.1683\n",
    "Epoch #160: Loss:2.4203, Accuracy:0.1824, Validation Loss:2.4559, Validation Accuracy:0.1675\n",
    "Epoch #161: Loss:2.4222, Accuracy:0.1830, Validation Loss:2.4547, Validation Accuracy:0.1658\n",
    "Epoch #162: Loss:2.4218, Accuracy:0.1857, Validation Loss:2.4527, Validation Accuracy:0.1691\n",
    "Epoch #163: Loss:2.4188, Accuracy:0.1836, Validation Loss:2.4512, Validation Accuracy:0.1658\n",
    "Epoch #164: Loss:2.4199, Accuracy:0.1886, Validation Loss:2.4529, Validation Accuracy:0.1671\n",
    "Epoch #165: Loss:2.4174, Accuracy:0.1876, Validation Loss:2.4527, Validation Accuracy:0.1675\n",
    "Epoch #166: Loss:2.4210, Accuracy:0.1871, Validation Loss:2.4528, Validation Accuracy:0.1712\n",
    "Epoch #167: Loss:2.4190, Accuracy:0.1875, Validation Loss:2.4527, Validation Accuracy:0.1691\n",
    "Epoch #168: Loss:2.4190, Accuracy:0.1863, Validation Loss:2.4530, Validation Accuracy:0.1712\n",
    "Epoch #169: Loss:2.4191, Accuracy:0.1863, Validation Loss:2.4523, Validation Accuracy:0.1736\n",
    "Epoch #170: Loss:2.4165, Accuracy:0.1896, Validation Loss:2.4515, Validation Accuracy:0.1700\n",
    "Epoch #171: Loss:2.4179, Accuracy:0.1883, Validation Loss:2.4553, Validation Accuracy:0.1695\n",
    "Epoch #172: Loss:2.4191, Accuracy:0.1829, Validation Loss:2.4535, Validation Accuracy:0.1679\n",
    "Epoch #173: Loss:2.4185, Accuracy:0.1839, Validation Loss:2.4521, Validation Accuracy:0.1708\n",
    "Epoch #174: Loss:2.4164, Accuracy:0.1869, Validation Loss:2.4527, Validation Accuracy:0.1683\n",
    "Epoch #175: Loss:2.4171, Accuracy:0.1896, Validation Loss:2.4517, Validation Accuracy:0.1671\n",
    "Epoch #176: Loss:2.4152, Accuracy:0.1878, Validation Loss:2.4520, Validation Accuracy:0.1691\n",
    "Epoch #177: Loss:2.4170, Accuracy:0.1858, Validation Loss:2.4515, Validation Accuracy:0.1663\n",
    "Epoch #178: Loss:2.4159, Accuracy:0.1872, Validation Loss:2.4499, Validation Accuracy:0.1724\n",
    "Epoch #179: Loss:2.4171, Accuracy:0.1886, Validation Loss:2.4510, Validation Accuracy:0.1691\n",
    "Epoch #180: Loss:2.4166, Accuracy:0.1848, Validation Loss:2.4500, Validation Accuracy:0.1700\n",
    "Epoch #181: Loss:2.4169, Accuracy:0.1845, Validation Loss:2.4530, Validation Accuracy:0.1667\n",
    "Epoch #182: Loss:2.4176, Accuracy:0.1812, Validation Loss:2.4510, Validation Accuracy:0.1724\n",
    "Epoch #183: Loss:2.4178, Accuracy:0.1835, Validation Loss:2.4519, Validation Accuracy:0.1691\n",
    "Epoch #184: Loss:2.4156, Accuracy:0.1875, Validation Loss:2.4505, Validation Accuracy:0.1667\n",
    "Epoch #185: Loss:2.4162, Accuracy:0.1870, Validation Loss:2.4505, Validation Accuracy:0.1691\n",
    "Epoch #186: Loss:2.4159, Accuracy:0.1859, Validation Loss:2.4519, Validation Accuracy:0.1695\n",
    "Epoch #187: Loss:2.4160, Accuracy:0.1874, Validation Loss:2.4526, Validation Accuracy:0.1712\n",
    "Epoch #188: Loss:2.4147, Accuracy:0.1877, Validation Loss:2.4550, Validation Accuracy:0.1663\n",
    "Epoch #189: Loss:2.4179, Accuracy:0.1836, Validation Loss:2.4528, Validation Accuracy:0.1704\n",
    "Epoch #190: Loss:2.4145, Accuracy:0.1871, Validation Loss:2.4534, Validation Accuracy:0.1687\n",
    "Epoch #191: Loss:2.4170, Accuracy:0.1855, Validation Loss:2.4529, Validation Accuracy:0.1671\n",
    "Epoch #192: Loss:2.4157, Accuracy:0.1884, Validation Loss:2.4507, Validation Accuracy:0.1671\n",
    "Epoch #193: Loss:2.4145, Accuracy:0.1893, Validation Loss:2.4501, Validation Accuracy:0.1765\n",
    "Epoch #194: Loss:2.4148, Accuracy:0.1887, Validation Loss:2.4506, Validation Accuracy:0.1765\n",
    "Epoch #195: Loss:2.4144, Accuracy:0.1924, Validation Loss:2.4547, Validation Accuracy:0.1695\n",
    "Epoch #196: Loss:2.4171, Accuracy:0.1887, Validation Loss:2.4503, Validation Accuracy:0.1736\n",
    "Epoch #197: Loss:2.4145, Accuracy:0.1921, Validation Loss:2.4501, Validation Accuracy:0.1786\n",
    "Epoch #198: Loss:2.4165, Accuracy:0.1927, Validation Loss:2.4484, Validation Accuracy:0.1782\n",
    "Epoch #199: Loss:2.4192, Accuracy:0.1876, Validation Loss:2.4529, Validation Accuracy:0.1741\n",
    "Epoch #200: Loss:2.4171, Accuracy:0.1919, Validation Loss:2.4549, Validation Accuracy:0.1835\n",
    "Epoch #201: Loss:2.4189, Accuracy:0.1860, Validation Loss:2.4521, Validation Accuracy:0.1749\n",
    "Epoch #202: Loss:2.4152, Accuracy:0.1925, Validation Loss:2.4527, Validation Accuracy:0.1778\n",
    "Epoch #203: Loss:2.4189, Accuracy:0.1911, Validation Loss:2.4568, Validation Accuracy:0.1691\n",
    "Epoch #204: Loss:2.4169, Accuracy:0.1910, Validation Loss:2.4520, Validation Accuracy:0.1712\n",
    "Epoch #205: Loss:2.4152, Accuracy:0.1927, Validation Loss:2.4518, Validation Accuracy:0.1683\n",
    "Epoch #206: Loss:2.4183, Accuracy:0.1885, Validation Loss:2.4506, Validation Accuracy:0.1695\n",
    "Epoch #207: Loss:2.4138, Accuracy:0.1915, Validation Loss:2.4527, Validation Accuracy:0.1708\n",
    "Epoch #208: Loss:2.4168, Accuracy:0.1911, Validation Loss:2.4536, Validation Accuracy:0.1716\n",
    "Epoch #209: Loss:2.4167, Accuracy:0.1906, Validation Loss:2.4504, Validation Accuracy:0.1675\n",
    "Epoch #210: Loss:2.4132, Accuracy:0.1891, Validation Loss:2.4535, Validation Accuracy:0.1736\n",
    "Epoch #211: Loss:2.4145, Accuracy:0.1887, Validation Loss:2.4569, Validation Accuracy:0.1716\n",
    "Epoch #212: Loss:2.4161, Accuracy:0.1888, Validation Loss:2.4527, Validation Accuracy:0.1753\n",
    "Epoch #213: Loss:2.4173, Accuracy:0.1938, Validation Loss:2.4529, Validation Accuracy:0.1736\n",
    "Epoch #214: Loss:2.4134, Accuracy:0.1938, Validation Loss:2.4553, Validation Accuracy:0.1638\n",
    "Epoch #215: Loss:2.4150, Accuracy:0.1923, Validation Loss:2.4525, Validation Accuracy:0.1753\n",
    "Epoch #216: Loss:2.4131, Accuracy:0.1938, Validation Loss:2.4513, Validation Accuracy:0.1769\n",
    "Epoch #217: Loss:2.4124, Accuracy:0.1955, Validation Loss:2.4524, Validation Accuracy:0.1753\n",
    "Epoch #218: Loss:2.4113, Accuracy:0.1946, Validation Loss:2.4550, Validation Accuracy:0.1741\n",
    "Epoch #219: Loss:2.4148, Accuracy:0.1914, Validation Loss:2.4578, Validation Accuracy:0.1724\n",
    "Epoch #220: Loss:2.4133, Accuracy:0.1891, Validation Loss:2.4550, Validation Accuracy:0.1667\n",
    "Epoch #221: Loss:2.4126, Accuracy:0.1930, Validation Loss:2.4522, Validation Accuracy:0.1745\n",
    "Epoch #222: Loss:2.4111, Accuracy:0.1939, Validation Loss:2.4524, Validation Accuracy:0.1778\n",
    "Epoch #223: Loss:2.4098, Accuracy:0.1952, Validation Loss:2.4509, Validation Accuracy:0.1745\n",
    "Epoch #224: Loss:2.4119, Accuracy:0.1950, Validation Loss:2.4500, Validation Accuracy:0.1736\n",
    "Epoch #225: Loss:2.4111, Accuracy:0.1946, Validation Loss:2.4515, Validation Accuracy:0.1724\n",
    "Epoch #226: Loss:2.4123, Accuracy:0.1905, Validation Loss:2.4496, Validation Accuracy:0.1782\n",
    "Epoch #227: Loss:2.4109, Accuracy:0.1924, Validation Loss:2.4498, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.4122, Accuracy:0.1918, Validation Loss:2.4501, Validation Accuracy:0.1732\n",
    "Epoch #229: Loss:2.4153, Accuracy:0.1905, Validation Loss:2.4531, Validation Accuracy:0.1749\n",
    "Epoch #230: Loss:2.4112, Accuracy:0.1886, Validation Loss:2.4514, Validation Accuracy:0.1700\n",
    "Epoch #231: Loss:2.4099, Accuracy:0.1914, Validation Loss:2.4516, Validation Accuracy:0.1728\n",
    "Epoch #232: Loss:2.4138, Accuracy:0.1925, Validation Loss:2.4604, Validation Accuracy:0.1667\n",
    "Epoch #233: Loss:2.4155, Accuracy:0.1935, Validation Loss:2.4549, Validation Accuracy:0.1691\n",
    "Epoch #234: Loss:2.4225, Accuracy:0.1924, Validation Loss:2.4621, Validation Accuracy:0.1687\n",
    "Epoch #235: Loss:2.4251, Accuracy:0.1879, Validation Loss:2.4652, Validation Accuracy:0.1605\n",
    "Epoch #236: Loss:2.4245, Accuracy:0.1852, Validation Loss:2.4634, Validation Accuracy:0.1658\n",
    "Epoch #237: Loss:2.4264, Accuracy:0.1847, Validation Loss:2.4592, Validation Accuracy:0.1626\n",
    "Epoch #238: Loss:2.4197, Accuracy:0.1883, Validation Loss:2.4583, Validation Accuracy:0.1667\n",
    "Epoch #239: Loss:2.4200, Accuracy:0.1873, Validation Loss:2.4543, Validation Accuracy:0.1683\n",
    "Epoch #240: Loss:2.4205, Accuracy:0.1884, Validation Loss:2.4521, Validation Accuracy:0.1712\n",
    "Epoch #241: Loss:2.4162, Accuracy:0.1908, Validation Loss:2.4507, Validation Accuracy:0.1683\n",
    "Epoch #242: Loss:2.4172, Accuracy:0.1902, Validation Loss:2.4541, Validation Accuracy:0.1691\n",
    "Epoch #243: Loss:2.4163, Accuracy:0.1905, Validation Loss:2.4519, Validation Accuracy:0.1728\n",
    "Epoch #244: Loss:2.4150, Accuracy:0.1882, Validation Loss:2.4509, Validation Accuracy:0.1724\n",
    "Epoch #245: Loss:2.4148, Accuracy:0.1875, Validation Loss:2.4509, Validation Accuracy:0.1646\n",
    "Epoch #246: Loss:2.4149, Accuracy:0.1862, Validation Loss:2.4496, Validation Accuracy:0.1708\n",
    "Epoch #247: Loss:2.4134, Accuracy:0.1903, Validation Loss:2.4527, Validation Accuracy:0.1675\n",
    "Epoch #248: Loss:2.4133, Accuracy:0.1894, Validation Loss:2.4515, Validation Accuracy:0.1691\n",
    "Epoch #249: Loss:2.4126, Accuracy:0.1922, Validation Loss:2.4508, Validation Accuracy:0.1753\n",
    "Epoch #250: Loss:2.4156, Accuracy:0.1890, Validation Loss:2.4640, Validation Accuracy:0.1704\n",
    "Epoch #251: Loss:2.4310, Accuracy:0.1849, Validation Loss:2.4581, Validation Accuracy:0.1704\n",
    "Epoch #252: Loss:2.4228, Accuracy:0.1889, Validation Loss:2.4557, Validation Accuracy:0.1712\n",
    "Epoch #253: Loss:2.4169, Accuracy:0.1882, Validation Loss:2.4548, Validation Accuracy:0.1679\n",
    "Epoch #254: Loss:2.4138, Accuracy:0.1874, Validation Loss:2.4547, Validation Accuracy:0.1691\n",
    "Epoch #255: Loss:2.4180, Accuracy:0.1897, Validation Loss:2.4572, Validation Accuracy:0.1683\n",
    "Epoch #256: Loss:2.4182, Accuracy:0.1868, Validation Loss:2.4548, Validation Accuracy:0.1716\n",
    "Epoch #257: Loss:2.4136, Accuracy:0.1874, Validation Loss:2.4561, Validation Accuracy:0.1691\n",
    "Epoch #258: Loss:2.4163, Accuracy:0.1883, Validation Loss:2.4563, Validation Accuracy:0.1695\n",
    "Epoch #259: Loss:2.4127, Accuracy:0.1907, Validation Loss:2.4577, Validation Accuracy:0.1671\n",
    "Epoch #260: Loss:2.4181, Accuracy:0.1901, Validation Loss:2.4555, Validation Accuracy:0.1695\n",
    "Epoch #261: Loss:2.4133, Accuracy:0.1903, Validation Loss:2.4541, Validation Accuracy:0.1720\n",
    "Epoch #262: Loss:2.4128, Accuracy:0.1934, Validation Loss:2.4547, Validation Accuracy:0.1712\n",
    "Epoch #263: Loss:2.4124, Accuracy:0.1928, Validation Loss:2.4508, Validation Accuracy:0.1695\n",
    "Epoch #264: Loss:2.4114, Accuracy:0.1943, Validation Loss:2.4527, Validation Accuracy:0.1753\n",
    "Epoch #265: Loss:2.4107, Accuracy:0.1936, Validation Loss:2.4509, Validation Accuracy:0.1679\n",
    "Epoch #266: Loss:2.4102, Accuracy:0.1912, Validation Loss:2.4522, Validation Accuracy:0.1671\n",
    "Epoch #267: Loss:2.4090, Accuracy:0.1961, Validation Loss:2.4516, Validation Accuracy:0.1695\n",
    "Epoch #268: Loss:2.4095, Accuracy:0.1962, Validation Loss:2.4504, Validation Accuracy:0.1757\n",
    "Epoch #269: Loss:2.4103, Accuracy:0.1943, Validation Loss:2.4521, Validation Accuracy:0.1745\n",
    "Epoch #270: Loss:2.4071, Accuracy:0.1961, Validation Loss:2.4538, Validation Accuracy:0.1741\n",
    "Epoch #271: Loss:2.4096, Accuracy:0.1952, Validation Loss:2.4512, Validation Accuracy:0.1741\n",
    "Epoch #272: Loss:2.4077, Accuracy:0.1961, Validation Loss:2.4512, Validation Accuracy:0.1708\n",
    "Epoch #273: Loss:2.4089, Accuracy:0.1938, Validation Loss:2.4494, Validation Accuracy:0.1712\n",
    "Epoch #274: Loss:2.4105, Accuracy:0.1961, Validation Loss:2.4490, Validation Accuracy:0.1741\n",
    "Epoch #275: Loss:2.4083, Accuracy:0.1966, Validation Loss:2.4509, Validation Accuracy:0.1765\n",
    "Epoch #276: Loss:2.4077, Accuracy:0.1989, Validation Loss:2.4513, Validation Accuracy:0.1724\n",
    "Epoch #277: Loss:2.4073, Accuracy:0.1968, Validation Loss:2.4500, Validation Accuracy:0.1753\n",
    "Epoch #278: Loss:2.4080, Accuracy:0.1972, Validation Loss:2.4518, Validation Accuracy:0.1753\n",
    "Epoch #279: Loss:2.4040, Accuracy:0.1972, Validation Loss:2.4448, Validation Accuracy:0.1745\n",
    "Epoch #280: Loss:2.4039, Accuracy:0.1964, Validation Loss:2.4414, Validation Accuracy:0.1757\n",
    "Epoch #281: Loss:2.4048, Accuracy:0.2009, Validation Loss:2.4449, Validation Accuracy:0.1769\n",
    "Epoch #282: Loss:2.4113, Accuracy:0.1977, Validation Loss:2.4445, Validation Accuracy:0.1753\n",
    "Epoch #283: Loss:2.4083, Accuracy:0.1934, Validation Loss:2.4462, Validation Accuracy:0.1757\n",
    "Epoch #284: Loss:2.4067, Accuracy:0.1995, Validation Loss:2.4464, Validation Accuracy:0.1704\n",
    "Epoch #285: Loss:2.4033, Accuracy:0.2000, Validation Loss:2.4455, Validation Accuracy:0.1749\n",
    "Epoch #286: Loss:2.4040, Accuracy:0.1990, Validation Loss:2.4463, Validation Accuracy:0.1749\n",
    "Epoch #287: Loss:2.4034, Accuracy:0.2020, Validation Loss:2.4477, Validation Accuracy:0.1761\n",
    "Epoch #288: Loss:2.4031, Accuracy:0.2014, Validation Loss:2.4471, Validation Accuracy:0.1786\n",
    "Epoch #289: Loss:2.4022, Accuracy:0.2011, Validation Loss:2.4455, Validation Accuracy:0.1790\n",
    "Epoch #290: Loss:2.4048, Accuracy:0.2014, Validation Loss:2.4471, Validation Accuracy:0.1802\n",
    "Epoch #291: Loss:2.4027, Accuracy:0.2026, Validation Loss:2.4465, Validation Accuracy:0.1790\n",
    "Epoch #292: Loss:2.4007, Accuracy:0.1986, Validation Loss:2.4486, Validation Accuracy:0.1765\n",
    "Epoch #293: Loss:2.4038, Accuracy:0.1997, Validation Loss:2.4469, Validation Accuracy:0.1769\n",
    "Epoch #294: Loss:2.4032, Accuracy:0.1996, Validation Loss:2.4465, Validation Accuracy:0.1757\n",
    "Epoch #295: Loss:2.4042, Accuracy:0.1961, Validation Loss:2.4483, Validation Accuracy:0.1778\n",
    "Epoch #296: Loss:2.4028, Accuracy:0.1990, Validation Loss:2.4490, Validation Accuracy:0.1786\n",
    "Epoch #297: Loss:2.4014, Accuracy:0.1992, Validation Loss:2.4478, Validation Accuracy:0.1732\n",
    "Epoch #298: Loss:2.4049, Accuracy:0.2003, Validation Loss:2.4484, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.4032, Accuracy:0.2003, Validation Loss:2.4484, Validation Accuracy:0.1769\n",
    "Epoch #300: Loss:2.4054, Accuracy:0.1960, Validation Loss:2.4495, Validation Accuracy:0.1724\n",
    "\n",
    "Test:\n",
    "Test Loss:2.44954538, Accuracy:0.1724\n",
    "Labels: ['sg', 'ce', 'ib', 'eo', 'by', 'ek', 'yd', 'ck', 'eg', 'eb', 'ds', 'mb', 'aa', 'my', 'sk']\n",
    "Confusion Matrix:\n",
    "       sg  ce  ib  eo  by  ek  yd  ck   eg  eb  ds  mb  aa  my  sk\n",
    "t:sg  100   0  54   1   6   0  13   0   17  12   0   0   0   0   0\n",
    "t:ce   41   0   5   1   5   0   7   0   37   7   4   0   2   0   0\n",
    "t:ib   56   0  51   0   3   0  72   0   27   6   2   0   0   0   0\n",
    "t:eo   87   0  10   3  11   0   5   0   12   6   0   0   1   0   0\n",
    "t:by   69   0  15   0  21   0   4   0   36  12   2   0   3   0   0\n",
    "t:ek   62   0  18   0  13   0   7   0   63  23   5   0   0   0   0\n",
    "t:yd  103   0  38   1   2   0  92   0   10   1   2   0   0   0   0\n",
    "t:ck   17   0   6   0   5   0   0   0   49   7   5   0   2   0   0\n",
    "t:eg   39   0   3   0  14   0   0   0  109   9  21   0   3   0   0\n",
    "t:eb   47   0  22   2  12   0  17   0   80  15   4   1   1   0   0\n",
    "t:ds   26   0   2   2   3   0   1   0   66   3  21   0   2   0   0\n",
    "t:mb   80   0  39   0  13   0  15   0   40  17   3   0   0   0   0\n",
    "t:aa   22   0   3   0  11   0   8   0   59  11  15   0   8   0   0\n",
    "t:my   15   0  13   0   0   0   9   0   31   6   6   0   0   0   0\n",
    "t:sk   35   0   8   0   6   0   6   0   62   7   6   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          sg       0.13      0.49      0.20       203\n",
    "          ce       0.00      0.00      0.00       109\n",
    "          ib       0.18      0.24      0.20       217\n",
    "          eo       0.30      0.02      0.04       135\n",
    "          by       0.17      0.13      0.15       162\n",
    "          ek       0.00      0.00      0.00       191\n",
    "          yd       0.36      0.37      0.36       249\n",
    "          ck       0.00      0.00      0.00        91\n",
    "          eg       0.16      0.55      0.24       198\n",
    "          eb       0.11      0.07      0.09       201\n",
    "          ds       0.22      0.17      0.19       126\n",
    "          mb       0.00      0.00      0.00       207\n",
    "          aa       0.36      0.06      0.10       137\n",
    "          my       0.00      0.00      0.00        80\n",
    "          sk       0.00      0.00      0.00       130\n",
    "\n",
    "    accuracy                           0.17      2436\n",
    "   macro avg       0.13      0.14      0.10      2436\n",
    "weighted avg       0.14      0.17      0.13      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 14:48:15 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 1 minutes, 59 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6845690209681567, 2.664148700256849, 2.6375351441513337, 2.5882494363487254, 2.5458474926564887, 2.52379956699553, 2.5012805207413797, 2.494745895388874, 2.4867348424319564, 2.4836589323084537, 2.4784862192589268, 2.473065441660889, 2.478074058914811, 2.4708887497192533, 2.46993413386478, 2.469944311088725, 2.4719001600895023, 2.47323797958825, 2.4829660158830715, 2.475440735495932, 2.470662772361868, 2.4668565168365078, 2.4669034136535695, 2.4678824300249222, 2.4729045157753577, 2.464715935718054, 2.4652753612286546, 2.4660341680930755, 2.4636279984648004, 2.4631719808468873, 2.460892218087107, 2.4641010992241217, 2.4632055070404153, 2.463956882213724, 2.4644544809714133, 2.466100096115338, 2.464270136822229, 2.461921410020349, 2.460123059001854, 2.4627128465618013, 2.462242105715772, 2.4618424006870816, 2.461506537420213, 2.462245865604169, 2.4615733537376414, 2.460023050042013, 2.4607421330043246, 2.4627705904454826, 2.4633499405458448, 2.4617784743630042, 2.4615081507584144, 2.4629541778407855, 2.4592543591810956, 2.4593467422698323, 2.4600539802526216, 2.45937514970651, 2.457717952274141, 2.457834385885981, 2.4570837110917165, 2.45842336707906, 2.4587017863450575, 2.460124299639747, 2.4608707596320043, 2.474342264761087, 2.4598400209141875, 2.4592053009371453, 2.458408982099962, 2.458878221182988, 2.4597225032612218, 2.4629310269661135, 2.463884135968188, 2.4660144564749182, 2.464676461196298, 2.46424025463549, 2.460879624183542, 2.46289318026776, 2.4613303989416666, 2.4648713431334848, 2.462326973334126, 2.4630745733508532, 2.464292575573099, 2.4623339865203757, 2.4623036408072037, 2.46230954802878, 2.4605433236201995, 2.4594503901470666, 2.4574779332760714, 2.460531337312094, 2.4598132616584913, 2.457988881125239, 2.457116485620759, 2.458073898684998, 2.4591216351989846, 2.4592170750566305, 2.4591010587751767, 2.458574759744854, 2.462987608118793, 2.462147091018351, 2.4580302199315165, 2.457577006961716, 2.459594273214857, 2.4578027893561254, 2.4596177126190737, 2.4581070232078166, 2.4586813716074123, 2.4589127315872017, 2.4602449351343614, 2.4652782093323706, 2.4558035278163715, 2.4563606885462166, 2.4578315358248055, 2.458501764901949, 2.4574739071731693, 2.462025795076868, 2.461684071958946, 2.4600061505103152, 2.4583176125838055, 2.458645055250973, 2.457534547314072, 2.456788102981492, 2.455856963331476, 2.457829177868973, 2.458612986581862, 2.456695195293583, 2.458074519591183, 2.455389233449801, 2.4663658776306754, 2.4537636485984566, 2.4532763030141447, 2.454793033145723, 2.4592351404512653, 2.457922774582661, 2.4580990375556384, 2.4602219588650858, 2.4600187534181943, 2.4592096601996714, 2.4569490327819423, 2.4569978815777156, 2.4582413910644982, 2.457277709823132, 2.456738145872094, 2.4564465031835248, 2.455727848121881, 2.4533924019003934, 2.4524614450771036, 2.454002905361758, 2.4557899179912748, 2.452966186604868, 2.455573233086096, 2.4548313923069998, 2.4563558336549205, 2.4526074382863414, 2.4523015257172984, 2.4506012536034794, 2.4513825418913893, 2.4501416029405516, 2.4498455794574006, 2.4525222715681605, 2.4513665247824785, 2.4558990792492144, 2.4546838253002448, 2.4527410337294655, 2.451194222533253, 2.4529184343779615, 2.452744667948956, 2.4528066880988764, 2.4527338267547156, 2.4530468241530294, 2.4523238662036966, 2.451482008634921, 2.4553027168674815, 2.4534836007260727, 2.4520570628944482, 2.4527145156327923, 2.451661339730073, 2.4519655203388635, 2.451527705920741, 2.449925964101782, 2.4509510285357146, 2.4499908887302544, 2.4530260245788273, 2.4509869338256385, 2.451879182276859, 2.4504686411965655, 2.4504587869534546, 2.451887649464098, 2.452598532628152, 2.454955870294806, 2.452832992049469, 2.4534038506900933, 2.4529197329566594, 2.450743726908867, 2.450113237002017, 2.450646674887496, 2.4546722308755506, 2.4503249667939686, 2.450078522239021, 2.448438832716793, 2.4529294055475193, 2.454883459557845, 2.4520941622347276, 2.4526951422636536, 2.4568070899481063, 2.4519697789879658, 2.45178850765886, 2.4505825222810893, 2.4526686386521814, 2.4536241189208132, 2.4503645497589863, 2.453541348133181, 2.456887194675765, 2.452661614504158, 2.452914652566017, 2.4553078093943728, 2.4525339967511557, 2.4513342012521275, 2.4524396158791526, 2.455015274495718, 2.457785984956964, 2.4550479993052865, 2.4522113768729477, 2.452429321990616, 2.450852028450551, 2.450033688584376, 2.4514543203689, 2.4495922223296267, 2.4498388688944046, 2.450136831437034, 2.453123199920153, 2.4514038821159327, 2.451583894798517, 2.4603863435817273, 2.454858981525565, 2.4621392682268115, 2.465156081470558, 2.4634206639526317, 2.4591951197982813, 2.4583188759282306, 2.454327984200714, 2.452142976579212, 2.4506649618665572, 2.4540886679306406, 2.4519121012664193, 2.4509450050410377, 2.450919903753622, 2.4496337255625105, 2.452683935807452, 2.451468671288201, 2.4507605708487126, 2.4639854951836595, 2.4581248572307266, 2.455697510238547, 2.4547699852334257, 2.454701948635684, 2.457230269223794, 2.454778103601365, 2.4561273499662652, 2.4562905508113415, 2.457715050536032, 2.4555253677180247, 2.4541483637930335, 2.4546691361319257, 2.45084992574745, 2.4527426764295606, 2.450925900626848, 2.4521614009719372, 2.4515537060736046, 2.4504157514211973, 2.452127763986196, 2.4537898754251413, 2.4511527925093577, 2.451201256860066, 2.449429069246565, 2.4490062196070728, 2.4509334998765016, 2.4512740598719303, 2.450015143220648, 2.451814607250671, 2.4447507235804213, 2.441353508991561, 2.4448632559752816, 2.444528871764886, 2.446212797133598, 2.446353541219176, 2.44550166028278, 2.4462511887887035, 2.4476828884412893, 2.447138726809146, 2.4454650174220793, 2.447052210618318, 2.4464972211026597, 2.448569821215224, 2.446931968964575, 2.446520103413875, 2.4482705589194214, 2.449012654168265, 2.4478268329733113, 2.4484265390875306, 2.44843449028842, 2.4495455181265897], 'val_acc': [0.11535303775459675, 0.13834154251075925, 0.14490968696100176, 0.15558292196791357, 0.16174055753376684, 0.16625615692588888, 0.16789819302621536, 0.1629720846762994, 0.16174055748483035, 0.16748768397054845, 0.1683087020818823, 0.1666666659326193, 0.16174055743589386, 0.16830870203294582, 0.1687192110641445, 0.16584564777234898, 0.16625615685248413, 0.1678981929772789, 0.16420361162308597, 0.16091953930009176, 0.16748768379927073, 0.1666666658102781, 0.16830870200847758, 0.16256157540041824, 0.16215106636921955, 0.16502462963654685, 0.16461412067875292, 0.16543513876561852, 0.16338259353622037, 0.1633825935117521, 0.1666666658102781, 0.16215106641815605, 0.16707717484147677, 0.16584564772341248, 0.16379310259188729, 0.16050903026889307, 0.17036124714000275, 0.16461412082556237, 0.16954022917547837, 0.16666666590815107, 0.16912972007087496, 0.16543513879008676, 0.16666666585921458, 0.1637931027142285, 0.16420361172095896, 0.16748768387267546, 0.16912972011981142, 0.17077175612226494, 0.1637931025429508, 0.16748768384820722, 0.16707717481700854, 0.16871921094180328, 0.17159277430700356, 0.16830870210635054, 0.1662561567790794, 0.17118226525133662, 0.1691297202176844, 0.1691297200464067, 0.1687192111620175, 0.16666666595708757, 0.16830870198400932, 0.16666666590815107, 0.1687192110641445, 0.1666666659326193, 0.16995073810880407, 0.16830870200847758, 0.16502462963654685, 0.16543513881455502, 0.16666666588368284, 0.16584564774788071, 0.16995073820667705, 0.17200328346054347, 0.1625615754248865, 0.1625615754248865, 0.1678981929772789, 0.16830870203294582, 0.1691297200953432, 0.16789819300174713, 0.16625615685248413, 0.159277503175297, 0.16420361159861774, 0.16543513881455502, 0.16543513876561852, 0.16297208450502168, 0.16666666588368284, 0.16748768389714372, 0.170361247164471, 0.1654351387411503, 0.16256157552275946, 0.16912972016874792, 0.16707717491488153, 0.16830870203294582, 0.16502462968548334, 0.1666666658102781, 0.16830870200847758, 0.16625615685248413, 0.16666666585921458, 0.16666666578580985, 0.16707717486594503, 0.170361247164471, 0.17200328328926576, 0.16954022907760538, 0.1720032834360752, 0.16830870210635054, 0.16584564787022193, 0.16830870215528704, 0.17282430140059962, 0.16789819290387414, 0.16912972007087496, 0.16502462973441984, 0.16502462963654685, 0.1691297200464067, 0.1662561567790794, 0.15968801218202744, 0.16502462968548334, 0.16707717484147677, 0.16748768401948494, 0.15681444898810487, 0.16789819295281064, 0.16215106651602904, 0.1687192110641445, 0.1658456478457537, 0.16543513879008676, 0.16091953932456, 0.16256157552275946, 0.16707717489041327, 0.16461412062981642, 0.16748768399501668, 0.16748768397054845, 0.16748768392161195, 0.16502462973441984, 0.1650246297099516, 0.1674876839460802, 0.16995073810880407, 0.16256157549829123, 0.16543513876561852, 0.16543513876561852, 0.16461412065428466, 0.16707717493934976, 0.1703612472134075, 0.16707717486594503, 0.16995073815774056, 0.16584564782128544, 0.17364531943852873, 0.172003283313734, 0.1707717561956697, 0.16666666583474635, 0.1720032832647975, 0.16871921103967627, 0.1691297200464067, 0.1613300484291634, 0.16789819290387414, 0.1662561567790794, 0.17077175622013793, 0.16871921108861274, 0.17118226525133662, 0.16995073810880407, 0.16666666588368284, 0.16830870200847758, 0.16748768399501668, 0.16584564777234898, 0.16912972007087496, 0.1658456477968172, 0.16707717491488153, 0.16748768392161195, 0.17118226532474135, 0.16912972011981142, 0.17118226527580488, 0.17364531953640172, 0.16995073818220882, 0.16954022905313715, 0.16789819295281064, 0.17077175626907443, 0.16830870203294582, 0.16707717489041327, 0.1691297200953432, 0.1662561568280159, 0.1724137924183374, 0.16912972007087496, 0.16995073818220882, 0.1666666658102781, 0.17241379234493268, 0.16912972002193846, 0.16666666578580985, 0.16912972011981142, 0.16954022907760538, 0.17118226532474135, 0.1662561567301429, 0.170361247164471, 0.1687192110641445, 0.16707717493934976, 0.16707717481700854, 0.17651888260798307, 0.1765188825835148, 0.16954022905313715, 0.17364531938959224, 0.17857142788631772, 0.17816091880618254, 0.1740558283963227, 0.18349753638304317, 0.17487684667893427, 0.17775040979945209, 0.16912972002193846, 0.1711822651779319, 0.16830870198400932, 0.16954022905313715, 0.17077175617120144, 0.1715927741112576, 0.16748768384820722, 0.17364531953640172, 0.17159277420913058, 0.17528735556332348, 0.17364531938959224, 0.1637931025429508, 0.17528735558779174, 0.1769293917859912, 0.17528735546545052, 0.17405582846972742, 0.17241379234493268, 0.16666666578580985, 0.1744663374519896, 0.1777504097505156, 0.1744663375009261, 0.173645319365124, 0.17241379234493268, 0.17816091885511903, 0.17241379222259146, 0.17323481038286181, 0.1748768464831883, 0.16995073813327233, 0.17282430130272664, 0.1666666658102781, 0.16912972011981142, 0.16871921094180328, 0.1605090302933613, 0.16584564777234898, 0.16256157537595, 0.1666666657613416, 0.1683087019595411, 0.17118226515346363, 0.16830870183719987, 0.16912971997300197, 0.17282430125379014, 0.1724137922470597, 0.16461412055641167, 0.17077175612226494, 0.16748768387267546, 0.1691297199485337, 0.17528735548991875, 0.17036124709106626, 0.1703612472134075, 0.17118226520240012, 0.16789819295281064, 0.1691297199974702, 0.16830870188613634, 0.17159277418466232, 0.1691297199485337, 0.1695402289797324, 0.16707717489041327, 0.16954022915101014, 0.17200328328926576, 0.17118226505559067, 0.16954022900420065, 0.17528735548991875, 0.1678981928794059, 0.16707717479254028, 0.1695402289797324, 0.17569786437430795, 0.17446633730518016, 0.17405582832291797, 0.17405582832291797, 0.17077175607332848, 0.17118226505559067, 0.17405582842079093, 0.1765188826324513, 0.17241379232046444, 0.17528735553885524, 0.17528735553885524, 0.1744663374519896, 0.17569786459452216, 0.17692939163918175, 0.17528735551438698, 0.17569786461899042, 0.17036124711553452, 0.17487684645872006, 0.17487684650765656, 0.1761083736991256, 0.17857142791078595, 0.1789819369175164, 0.1802134642068584, 0.1789819369664529, 0.17651888275479252, 0.1769293918838642, 0.17569786464345866, 0.17775040979945209, 0.17857142778844473, 0.17323481035839355, 0.1740558285186639, 0.17692939168811822, 0.17241379244280566], 'loss': [2.699172099170254, 2.67601626550882, 2.651675354430808, 2.610891116864872, 2.5598202848336533, 2.523523325792818, 2.5054019743901748, 2.483453818757921, 2.4735833819152395, 2.4649833012410505, 2.464877487257031, 2.4611518789365796, 2.454930507867488, 2.4582991317067546, 2.4509070601061875, 2.4500433037658, 2.449993207861021, 2.4462954821772644, 2.448499500286408, 2.4530793962537385, 2.4470791518076243, 2.4448355962608384, 2.4430258193789567, 2.4450001355314157, 2.4426335164408908, 2.442847488156579, 2.4402099182473562, 2.4449468903473024, 2.439805319960357, 2.4382856211378345, 2.436489333264392, 2.438046726700706, 2.436292006005007, 2.4363360157247933, 2.4362593079984065, 2.4370912367803115, 2.435920499237656, 2.434482484872336, 2.4382054913460105, 2.4336923654074543, 2.435103467455635, 2.4347166042797865, 2.435869415193123, 2.436599878021334, 2.436315515447691, 2.4330901825452487, 2.433196020713822, 2.4364121870828117, 2.4343852257581706, 2.437131418878293, 2.435244987828531, 2.4310481164489683, 2.43152124485196, 2.430362286166244, 2.430727001577922, 2.430694432033406, 2.431853530637048, 2.4302261711880413, 2.4307263490845292, 2.4315467788453464, 2.43047924903384, 2.4288393020629884, 2.4305262789833963, 2.436387610778182, 2.4448664372461777, 2.4349001883481316, 2.427367511862847, 2.429108406924614, 2.4334002731761895, 2.4317788090793995, 2.43255797108096, 2.4333773607101286, 2.437249946300499, 2.432678333983529, 2.429640361073081, 2.4300291767355353, 2.4287556914578228, 2.4304932605069767, 2.4305170473376827, 2.431134222665117, 2.4277211423282505, 2.430409138354433, 2.4300565615816527, 2.4301964745139686, 2.4283257586265736, 2.427435090164874, 2.4295199810357064, 2.4280110444376355, 2.429311795499046, 2.4318651504829925, 2.427227418280725, 2.426441113415195, 2.4246728941155653, 2.4287889633335373, 2.4280827411635943, 2.4290335706127255, 2.426930689077358, 2.4303727963621857, 2.4304277054827805, 2.426992694161511, 2.4252982944433694, 2.4270181440474805, 2.4263005877422357, 2.426220240876905, 2.4276325130854297, 2.4277909511902984, 2.4261974036081617, 2.42923126093416, 2.425734927375214, 2.423603250554455, 2.423508919484806, 2.4243912666485294, 2.4252756928516366, 2.425562255338477, 2.4263397155111575, 2.4258036377493606, 2.4257721138196313, 2.4254650655468386, 2.4242297481952018, 2.4243915057525007, 2.4236304579819006, 2.4237831649349455, 2.424869659059591, 2.423413623774566, 2.423335328777713, 2.4245607733481718, 2.421547446848186, 2.4241705235514552, 2.4259925268513958, 2.4253123847366114, 2.4271517331595294, 2.4261385473137764, 2.4315378964559256, 2.4299709467917254, 2.4288068806611047, 2.4250246457740268, 2.426249105778563, 2.4242495320416086, 2.4259172975160257, 2.426228643981338, 2.4246894200969282, 2.4243516733024646, 2.4228424374817332, 2.4214950895407363, 2.421575813665527, 2.424935040287903, 2.422996437182417, 2.420552732420653, 2.4214225068963775, 2.4212078384305418, 2.4207401174784198, 2.42193116505288, 2.4217347761933565, 2.423285757540677, 2.420405440653619, 2.4193856299045886, 2.419044548481152, 2.4221263199126697, 2.4236037703754967, 2.4203287109946814, 2.4222096473529353, 2.4217729179766145, 2.418752565325163, 2.4199233319480316, 2.4174191014722632, 2.420970682833474, 2.4189519501319903, 2.4190499903974594, 2.419086593377272, 2.416537628232576, 2.4178797497641624, 2.419143345321718, 2.4184633733800305, 2.4164484171896743, 2.4170571231254563, 2.4152324656931037, 2.4170258022676503, 2.4159476438831744, 2.4170908071177206, 2.416618239757217, 2.416872930820473, 2.417608876884351, 2.417774611772698, 2.415569747889556, 2.416227632969067, 2.415884608654516, 2.4159907483956653, 2.4146553330352907, 2.417858609526554, 2.41449681434788, 2.4170104095333658, 2.4156776219667595, 2.414497472716063, 2.4147909725471197, 2.4144358996248343, 2.4170513560394977, 2.414536648168701, 2.416537942827605, 2.419191790018728, 2.4171277911756075, 2.4188990481335524, 2.4152413400536443, 2.418852797377036, 2.416922333010413, 2.4152056931959773, 2.418327235588058, 2.413759246352272, 2.4167765960066716, 2.4167096356346867, 2.4132025657003666, 2.4145158717764477, 2.416136630604644, 2.417283349947763, 2.413360189461365, 2.4149609419844236, 2.413117908795022, 2.412417884920657, 2.411254976123755, 2.4147688715854465, 2.413323838607976, 2.4125805675616254, 2.411125940072218, 2.40981504383518, 2.4119039194784615, 2.4111077769825835, 2.412316295204711, 2.410868835449219, 2.412190196450486, 2.4152686460796566, 2.4111629310819405, 2.409910148663687, 2.4138309189915903, 2.4155133148483183, 2.422511283572939, 2.425066900449122, 2.4244961575071424, 2.4263993046856513, 2.4196795848360786, 2.4199503595089764, 2.4205069233504655, 2.4161925596867744, 2.417177745106284, 2.416315825323305, 2.414984683970896, 2.414752893428293, 2.4149442297722037, 2.4134145122778734, 2.413298868986126, 2.4125953034943377, 2.41559850655542, 2.430958657391997, 2.422832914740153, 2.4168593303868415, 2.413840626299504, 2.417957493509845, 2.4182433955723255, 2.41364878952136, 2.4162996915821178, 2.412718365177726, 2.4180584012605326, 2.413337320077101, 2.4128054801933083, 2.4123949401432485, 2.4114297070787183, 2.410658972366145, 2.4101615748121508, 2.4090060785320997, 2.4094734672648217, 2.4103144486092445, 2.4070562659347816, 2.4095978906267232, 2.4076512876232545, 2.4088606725728, 2.410536636708943, 2.4082795448616543, 2.407702272383829, 2.407298399829277, 2.4080458612657427, 2.403952906410797, 2.4039360558472618, 2.4048231482261015, 2.411278920594672, 2.408332391832888, 2.406662505754945, 2.4032842482384713, 2.403955517167673, 2.403361106163667, 2.4031173013808544, 2.402167707693895, 2.4048125238633986, 2.4027196603144465, 2.4006990773477104, 2.4038498844209393, 2.4031981852999458, 2.4041785175550645, 2.4027510056505457, 2.4013813159793798, 2.404871035013845, 2.4031501793518695, 2.4054403803431765], 'acc': [0.10636550308620171, 0.1281314168500215, 0.13542094456464113, 0.15687885010878896, 0.16406570842195095, 0.1628336755677415, 0.1664271047227926, 0.17053388090349075, 0.1731006160194869, 0.17525667351129365, 0.17638603696098562, 0.17628336756258775, 0.17648870636550307, 0.1736139630512535, 0.17802874744550404, 0.17905544148455899, 0.1759753593429158, 0.1747433265009455, 0.17443531828739314, 0.17381930184804928, 0.1750513347053185, 0.17515400410983598, 0.1746406570841889, 0.17494866530386086, 0.1746406570841889, 0.17433264887369634, 0.1746406570841889, 0.17700205339420993, 0.17751540041067762, 0.17782340862423, 0.17833675564681725, 0.17854209445585215, 0.17915811088907646, 0.17956878850714628, 0.17813141685002148, 0.1822381930184805, 0.1815195071868583, 0.18162217659749536, 0.18275154004718733, 0.18100616016733084, 0.18028747433264888, 0.18090349076587317, 0.17741273101227975, 0.1758726899506375, 0.17731006160776228, 0.17689938398663269, 0.18162217659137578, 0.17628336755952795, 0.17669404517759776, 0.1805954825523208, 0.1803901437432859, 0.17926078028747433, 0.1841889117104317, 0.18326488706365504, 0.18593429158110883, 0.18203285421556517, 0.18316221766525714, 0.18100616017039062, 0.1834702258849291, 0.1857289527751337, 0.18449691992398404, 0.1830595482607397, 0.18521560575560622, 0.17628336755646817, 0.17628336756258775, 0.17802874743938446, 0.1853182751570639, 0.18193018480798792, 0.18295687885010267, 0.1837782340862423, 0.1830595482607397, 0.184394250513347, 0.17895277207698176, 0.1850102669526909, 0.18470225873913854, 0.18398357290139677, 0.1839835728952772, 0.18562628335531733, 0.18418891170431212, 0.18418891169207297, 0.18223819300624133, 0.179671457893305, 0.18193018481716727, 0.1855236139691586, 0.18110882957490806, 0.18336755647123226, 0.18388090349075975, 0.18449691992092426, 0.18275154004106775, 0.18377823409236188, 0.1868583162217659, 0.18470225872995918, 0.18408624229979467, 0.1851129363449692, 0.18603696099786543, 0.1825462012197937, 0.1845995893285015, 0.18511293635720835, 0.18388090349687933, 0.1841889117104317, 0.18357289527720738, 0.18203285421250537, 0.18316221767137672, 0.1851129363449692, 0.18603696099174585, 0.18470225872689938, 0.18696098562628335, 0.18429158112106872, 0.18429158111188937, 0.18593429158110883, 0.18418891169207297, 0.1849075975359343, 0.18603696098562628, 0.1848049281375364, 0.1821355236262022, 0.1861396303962633, 0.1833675564681725, 0.18336755648041164, 0.18603696098562628, 0.1859342915872284, 0.18388090349687933, 0.18521560575560622, 0.1855236139660988, 0.18429158111188937, 0.18326488706671482, 0.18285420944558523, 0.18542094457076072, 0.184394250513347, 0.18511293634802897, 0.17956878851326585, 0.18655030800821354, 0.18521560575560622, 0.1826488706365503, 0.18572895277207394, 0.18593429159334798, 0.18521560573724752, 0.18511293635720835, 0.18572895278431306, 0.1848049281375364, 0.18726899383983572, 0.1825462012197937, 0.18449691992398404, 0.1864476386036961, 0.19127310060377728, 0.18860369610162242, 0.18480492813447663, 0.18696098562934316, 0.18757700205950767, 0.1864476386067559, 0.18655030800821354, 0.18542094457076072, 0.18696098562934316, 0.18593429158110883, 0.18511293633273004, 0.18470225872689938, 0.185523613963039, 0.1867556468050093, 0.18490759754205385, 0.1844969199178645, 0.182443531833635, 0.18295687885622225, 0.18572895277819348, 0.18357289528026718, 0.18860369609856262, 0.18757700205950767, 0.18706365503080083, 0.18747433265499022, 0.18634496920223842, 0.1863449692114178, 0.18963039014985675, 0.18829568788501028, 0.18285420945782435, 0.18388090349687933, 0.18685831622788548, 0.18963039014373717, 0.187782340862423, 0.18583162218271096, 0.18716632444755743, 0.18860369609856262, 0.18480492813141683, 0.18449691992398404, 0.18121149897636574, 0.1834702258788095, 0.18747433266110977, 0.18696098563240293, 0.18593429158110883, 0.18737166325659232, 0.18767967146096534, 0.18357289528332696, 0.18706365503080083, 0.1855236139660988, 0.1883983572925875, 0.1893223819332446, 0.1887063655030801, 0.1924024640657084, 0.18870636551531922, 0.19209445585215607, 0.19271047227926077, 0.1875770020533881, 0.19188911704312114, 0.18603696099174585, 0.19250513347022588, 0.19106776181004131, 0.19096509241470322, 0.19271047228232058, 0.18850102670016475, 0.19147843942811113, 0.19106776181004131, 0.19055441479051383, 0.18911704313338903, 0.18870636551531922, 0.18880903491371712, 0.19383983573201255, 0.19383983574119193, 0.19229979466119096, 0.19383983572895278, 0.19548254620123204, 0.19455852156363473, 0.19137577002665346, 0.18911704312726946, 0.19301848049893272, 0.1939425051395898, 0.19517453798767967, 0.19496919918476432, 0.1945585215666945, 0.19045174538293658, 0.19240246407182798, 0.1917864476386037, 0.19045174538599638, 0.18860369611080177, 0.19137577002359366, 0.19250513347328566, 0.19353182752151998, 0.1924024640687682, 0.18788501027306004, 0.18521560574948664, 0.18470225871466023, 0.18829568787277112, 0.18726899385207488, 0.1883983572925875, 0.19075975359342917, 0.19024640658308103, 0.1904517453798768, 0.1881930184804928, 0.18747433265499022, 0.18624229980078078, 0.19034907597535936, 0.18942505133776205, 0.1921971252627931, 0.189014373722752, 0.18490759753899408, 0.18891170432435414, 0.18819301849273196, 0.18737166325659232, 0.18973305954825462, 0.18675564681724846, 0.18737166324435317, 0.18829568788807005, 0.19065708417667257, 0.19014373716632443, 0.19034907597841913, 0.19342915811088296, 0.19281314168683802, 0.19425051335926175, 0.19363449692603743, 0.19117043121761854, 0.19609856263445632, 0.196201232035914, 0.19425051335314217, 0.19609856263139655, 0.19517453799073947, 0.19609856263445632, 0.19383983574119193, 0.19609856263445632, 0.19661190965092404, 0.198870636550308, 0.1968172484660785, 0.19722792608108852, 0.19722792607802875, 0.19640657084188912, 0.20092402464677667, 0.19774127308837686, 0.19342915811394273, 0.19948665297741272, 0.2, 0.19897330596094503, 0.2019507186919512, 0.20143737166630413, 0.20112936345275179, 0.2014373716754835, 0.20256673512517548, 0.19856262833675564, 0.19969199178644764, 0.19958932238804974, 0.19609856262833675, 0.19897330595482546, 0.19917864476997996, 0.20030800821967193, 0.20030800821967193, 0.19599589323605845]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
