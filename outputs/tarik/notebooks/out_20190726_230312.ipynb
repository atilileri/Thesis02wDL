{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf19.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 23:03:12 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '1Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eo', 'ek', 'aa', 'sg', 'ib', 'ck', 'eb', 'eg', 'mb', 'my', 'yd', 'by', 'ds', 'ce', 'sk'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000249015BE278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000024941ED6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7044, Accuracy:0.0924, Validation Loss:2.6993, Validation Accuracy:0.0805\n",
    "Epoch #2: Loss:2.6971, Accuracy:0.0908, Validation Loss:2.6938, Validation Accuracy:0.0920\n",
    "Epoch #3: Loss:2.6906, Accuracy:0.0940, Validation Loss:2.6873, Validation Accuracy:0.0887\n",
    "Epoch #4: Loss:2.6847, Accuracy:0.0920, Validation Loss:2.6811, Validation Accuracy:0.0920\n",
    "Epoch #5: Loss:2.6785, Accuracy:0.0920, Validation Loss:2.6748, Validation Accuracy:0.0936\n",
    "Epoch #6: Loss:2.6723, Accuracy:0.0879, Validation Loss:2.6677, Validation Accuracy:0.0952\n",
    "Epoch #7: Loss:2.6654, Accuracy:0.0846, Validation Loss:2.6610, Validation Accuracy:0.0903\n",
    "Epoch #8: Loss:2.6587, Accuracy:0.0747, Validation Loss:2.6546, Validation Accuracy:0.0887\n",
    "Epoch #9: Loss:2.6513, Accuracy:0.1097, Validation Loss:2.6455, Validation Accuracy:0.1412\n",
    "Epoch #10: Loss:2.6411, Accuracy:0.1400, Validation Loss:2.6385, Validation Accuracy:0.1429\n",
    "Epoch #11: Loss:2.6357, Accuracy:0.1396, Validation Loss:2.6270, Validation Accuracy:0.1560\n",
    "Epoch #12: Loss:2.6228, Accuracy:0.1577, Validation Loss:2.6155, Validation Accuracy:0.1658\n",
    "Epoch #13: Loss:2.6122, Accuracy:0.1565, Validation Loss:2.6171, Validation Accuracy:0.1346\n",
    "Epoch #14: Loss:2.5958, Accuracy:0.1515, Validation Loss:2.5885, Validation Accuracy:0.1626\n",
    "Epoch #15: Loss:2.5788, Accuracy:0.1598, Validation Loss:2.5692, Validation Accuracy:0.1527\n",
    "Epoch #16: Loss:2.5639, Accuracy:0.1606, Validation Loss:2.5598, Validation Accuracy:0.1708\n",
    "Epoch #17: Loss:2.5517, Accuracy:0.1651, Validation Loss:2.5488, Validation Accuracy:0.1642\n",
    "Epoch #18: Loss:2.5441, Accuracy:0.1602, Validation Loss:2.5395, Validation Accuracy:0.1642\n",
    "Epoch #19: Loss:2.5305, Accuracy:0.1630, Validation Loss:2.5336, Validation Accuracy:0.1593\n",
    "Epoch #20: Loss:2.5254, Accuracy:0.1610, Validation Loss:2.5301, Validation Accuracy:0.1609\n",
    "Epoch #21: Loss:2.5177, Accuracy:0.1680, Validation Loss:2.5231, Validation Accuracy:0.1724\n",
    "Epoch #22: Loss:2.5102, Accuracy:0.1692, Validation Loss:2.5111, Validation Accuracy:0.1757\n",
    "Epoch #23: Loss:2.5053, Accuracy:0.1725, Validation Loss:2.5116, Validation Accuracy:0.1675\n",
    "Epoch #24: Loss:2.4985, Accuracy:0.1762, Validation Loss:2.4982, Validation Accuracy:0.1708\n",
    "Epoch #25: Loss:2.4893, Accuracy:0.1713, Validation Loss:2.4970, Validation Accuracy:0.1675\n",
    "Epoch #26: Loss:2.4855, Accuracy:0.1713, Validation Loss:2.4913, Validation Accuracy:0.1773\n",
    "Epoch #27: Loss:2.4816, Accuracy:0.1733, Validation Loss:2.4807, Validation Accuracy:0.1773\n",
    "Epoch #28: Loss:2.4762, Accuracy:0.1762, Validation Loss:2.4768, Validation Accuracy:0.1839\n",
    "Epoch #29: Loss:2.4734, Accuracy:0.1737, Validation Loss:2.4712, Validation Accuracy:0.1823\n",
    "Epoch #30: Loss:2.4701, Accuracy:0.1737, Validation Loss:2.4683, Validation Accuracy:0.1806\n",
    "Epoch #31: Loss:2.4692, Accuracy:0.1733, Validation Loss:2.4640, Validation Accuracy:0.1806\n",
    "Epoch #32: Loss:2.4711, Accuracy:0.1758, Validation Loss:2.4698, Validation Accuracy:0.1773\n",
    "Epoch #33: Loss:2.4699, Accuracy:0.1725, Validation Loss:2.4634, Validation Accuracy:0.1741\n",
    "Epoch #34: Loss:2.4662, Accuracy:0.1721, Validation Loss:2.4654, Validation Accuracy:0.1790\n",
    "Epoch #35: Loss:2.4644, Accuracy:0.1708, Validation Loss:2.4627, Validation Accuracy:0.1823\n",
    "Epoch #36: Loss:2.4617, Accuracy:0.1708, Validation Loss:2.4569, Validation Accuracy:0.1806\n",
    "Epoch #37: Loss:2.4690, Accuracy:0.1717, Validation Loss:2.4584, Validation Accuracy:0.1872\n",
    "Epoch #38: Loss:2.4628, Accuracy:0.1725, Validation Loss:2.4670, Validation Accuracy:0.1806\n",
    "Epoch #39: Loss:2.4582, Accuracy:0.1741, Validation Loss:2.4598, Validation Accuracy:0.1691\n",
    "Epoch #40: Loss:2.4590, Accuracy:0.1729, Validation Loss:2.4603, Validation Accuracy:0.1872\n",
    "Epoch #41: Loss:2.4581, Accuracy:0.1733, Validation Loss:2.4548, Validation Accuracy:0.1888\n",
    "Epoch #42: Loss:2.4570, Accuracy:0.1717, Validation Loss:2.4563, Validation Accuracy:0.1938\n",
    "Epoch #43: Loss:2.4573, Accuracy:0.1741, Validation Loss:2.4541, Validation Accuracy:0.1872\n",
    "Epoch #44: Loss:2.4550, Accuracy:0.1749, Validation Loss:2.4512, Validation Accuracy:0.1757\n",
    "Epoch #45: Loss:2.4519, Accuracy:0.1733, Validation Loss:2.4573, Validation Accuracy:0.1856\n",
    "Epoch #46: Loss:2.4528, Accuracy:0.1725, Validation Loss:2.4514, Validation Accuracy:0.1790\n",
    "Epoch #47: Loss:2.4504, Accuracy:0.1729, Validation Loss:2.4506, Validation Accuracy:0.1839\n",
    "Epoch #48: Loss:2.4524, Accuracy:0.1721, Validation Loss:2.4490, Validation Accuracy:0.1823\n",
    "Epoch #49: Loss:2.4530, Accuracy:0.1717, Validation Loss:2.4485, Validation Accuracy:0.1888\n",
    "Epoch #50: Loss:2.4523, Accuracy:0.1721, Validation Loss:2.4482, Validation Accuracy:0.1872\n",
    "Epoch #51: Loss:2.4504, Accuracy:0.1725, Validation Loss:2.4480, Validation Accuracy:0.1790\n",
    "Epoch #52: Loss:2.4498, Accuracy:0.1725, Validation Loss:2.4509, Validation Accuracy:0.1724\n",
    "Epoch #53: Loss:2.4498, Accuracy:0.1717, Validation Loss:2.4519, Validation Accuracy:0.1724\n",
    "Epoch #54: Loss:2.4463, Accuracy:0.1704, Validation Loss:2.4494, Validation Accuracy:0.1708\n",
    "Epoch #55: Loss:2.4489, Accuracy:0.1655, Validation Loss:2.4474, Validation Accuracy:0.1790\n",
    "Epoch #56: Loss:2.4482, Accuracy:0.1737, Validation Loss:2.4505, Validation Accuracy:0.1823\n",
    "Epoch #57: Loss:2.4477, Accuracy:0.1737, Validation Loss:2.4450, Validation Accuracy:0.1806\n",
    "Epoch #58: Loss:2.4506, Accuracy:0.1696, Validation Loss:2.4585, Validation Accuracy:0.1741\n",
    "Epoch #59: Loss:2.4512, Accuracy:0.1725, Validation Loss:2.4511, Validation Accuracy:0.1708\n",
    "Epoch #60: Loss:2.4476, Accuracy:0.1680, Validation Loss:2.4513, Validation Accuracy:0.1823\n",
    "Epoch #61: Loss:2.4472, Accuracy:0.1725, Validation Loss:2.4474, Validation Accuracy:0.1741\n",
    "Epoch #62: Loss:2.4465, Accuracy:0.1721, Validation Loss:2.4455, Validation Accuracy:0.1724\n",
    "Epoch #63: Loss:2.4477, Accuracy:0.1749, Validation Loss:2.4482, Validation Accuracy:0.1856\n",
    "Epoch #64: Loss:2.4464, Accuracy:0.1766, Validation Loss:2.4443, Validation Accuracy:0.1806\n",
    "Epoch #65: Loss:2.4459, Accuracy:0.1766, Validation Loss:2.4471, Validation Accuracy:0.1856\n",
    "Epoch #66: Loss:2.4446, Accuracy:0.1762, Validation Loss:2.4449, Validation Accuracy:0.1856\n",
    "Epoch #67: Loss:2.4440, Accuracy:0.1758, Validation Loss:2.4447, Validation Accuracy:0.1856\n",
    "Epoch #68: Loss:2.4444, Accuracy:0.1774, Validation Loss:2.4457, Validation Accuracy:0.1905\n",
    "Epoch #69: Loss:2.4448, Accuracy:0.1733, Validation Loss:2.4447, Validation Accuracy:0.1905\n",
    "Epoch #70: Loss:2.4470, Accuracy:0.1791, Validation Loss:2.4434, Validation Accuracy:0.1872\n",
    "Epoch #71: Loss:2.4501, Accuracy:0.1704, Validation Loss:2.4422, Validation Accuracy:0.1839\n",
    "Epoch #72: Loss:2.4455, Accuracy:0.1725, Validation Loss:2.4501, Validation Accuracy:0.1856\n",
    "Epoch #73: Loss:2.4443, Accuracy:0.1745, Validation Loss:2.4409, Validation Accuracy:0.1856\n",
    "Epoch #74: Loss:2.4442, Accuracy:0.1737, Validation Loss:2.4417, Validation Accuracy:0.1872\n",
    "Epoch #75: Loss:2.4455, Accuracy:0.1758, Validation Loss:2.4432, Validation Accuracy:0.1888\n",
    "Epoch #76: Loss:2.4436, Accuracy:0.1745, Validation Loss:2.4397, Validation Accuracy:0.1823\n",
    "Epoch #77: Loss:2.4433, Accuracy:0.1774, Validation Loss:2.4456, Validation Accuracy:0.1872\n",
    "Epoch #78: Loss:2.4435, Accuracy:0.1749, Validation Loss:2.4399, Validation Accuracy:0.1888\n",
    "Epoch #79: Loss:2.4441, Accuracy:0.1708, Validation Loss:2.4418, Validation Accuracy:0.1856\n",
    "Epoch #80: Loss:2.4447, Accuracy:0.1770, Validation Loss:2.4445, Validation Accuracy:0.1856\n",
    "Epoch #81: Loss:2.4423, Accuracy:0.1758, Validation Loss:2.4384, Validation Accuracy:0.1823\n",
    "Epoch #82: Loss:2.4428, Accuracy:0.1778, Validation Loss:2.4434, Validation Accuracy:0.1872\n",
    "Epoch #83: Loss:2.4419, Accuracy:0.1758, Validation Loss:2.4385, Validation Accuracy:0.1938\n",
    "Epoch #84: Loss:2.4429, Accuracy:0.1749, Validation Loss:2.4408, Validation Accuracy:0.1806\n",
    "Epoch #85: Loss:2.4420, Accuracy:0.1770, Validation Loss:2.4443, Validation Accuracy:0.1872\n",
    "Epoch #86: Loss:2.4418, Accuracy:0.1770, Validation Loss:2.4395, Validation Accuracy:0.1888\n",
    "Epoch #87: Loss:2.4418, Accuracy:0.1758, Validation Loss:2.4403, Validation Accuracy:0.1905\n",
    "Epoch #88: Loss:2.4402, Accuracy:0.1778, Validation Loss:2.4398, Validation Accuracy:0.1823\n",
    "Epoch #89: Loss:2.4404, Accuracy:0.1832, Validation Loss:2.4445, Validation Accuracy:0.1773\n",
    "Epoch #90: Loss:2.4409, Accuracy:0.1766, Validation Loss:2.4416, Validation Accuracy:0.1691\n",
    "Epoch #91: Loss:2.4393, Accuracy:0.1762, Validation Loss:2.4423, Validation Accuracy:0.1708\n",
    "Epoch #92: Loss:2.4402, Accuracy:0.1795, Validation Loss:2.4484, Validation Accuracy:0.1675\n",
    "Epoch #93: Loss:2.4414, Accuracy:0.1733, Validation Loss:2.4423, Validation Accuracy:0.1724\n",
    "Epoch #94: Loss:2.4415, Accuracy:0.1717, Validation Loss:2.4395, Validation Accuracy:0.1806\n",
    "Epoch #95: Loss:2.4403, Accuracy:0.1741, Validation Loss:2.4438, Validation Accuracy:0.1806\n",
    "Epoch #96: Loss:2.4385, Accuracy:0.1717, Validation Loss:2.4416, Validation Accuracy:0.1839\n",
    "Epoch #97: Loss:2.4393, Accuracy:0.1762, Validation Loss:2.4411, Validation Accuracy:0.1806\n",
    "Epoch #98: Loss:2.4499, Accuracy:0.1815, Validation Loss:2.4735, Validation Accuracy:0.1773\n",
    "Epoch #99: Loss:2.4591, Accuracy:0.1786, Validation Loss:2.4657, Validation Accuracy:0.1675\n",
    "Epoch #100: Loss:2.4577, Accuracy:0.1778, Validation Loss:2.4627, Validation Accuracy:0.1658\n",
    "Epoch #101: Loss:2.4527, Accuracy:0.1782, Validation Loss:2.4578, Validation Accuracy:0.1691\n",
    "Epoch #102: Loss:2.4483, Accuracy:0.1786, Validation Loss:2.4684, Validation Accuracy:0.1806\n",
    "Epoch #103: Loss:2.4710, Accuracy:0.1749, Validation Loss:2.4677, Validation Accuracy:0.1691\n",
    "Epoch #104: Loss:2.4496, Accuracy:0.1774, Validation Loss:2.4519, Validation Accuracy:0.1708\n",
    "Epoch #105: Loss:2.4534, Accuracy:0.1766, Validation Loss:2.4503, Validation Accuracy:0.1626\n",
    "Epoch #106: Loss:2.4503, Accuracy:0.1713, Validation Loss:2.4552, Validation Accuracy:0.1658\n",
    "Epoch #107: Loss:2.4435, Accuracy:0.1717, Validation Loss:2.4464, Validation Accuracy:0.1675\n",
    "Epoch #108: Loss:2.4443, Accuracy:0.1782, Validation Loss:2.4448, Validation Accuracy:0.1708\n",
    "Epoch #109: Loss:2.4408, Accuracy:0.1758, Validation Loss:2.4502, Validation Accuracy:0.1741\n",
    "Epoch #110: Loss:2.4424, Accuracy:0.1758, Validation Loss:2.4472, Validation Accuracy:0.1773\n",
    "Epoch #111: Loss:2.4388, Accuracy:0.1786, Validation Loss:2.4463, Validation Accuracy:0.1806\n",
    "Epoch #112: Loss:2.4378, Accuracy:0.1778, Validation Loss:2.4458, Validation Accuracy:0.1724\n",
    "Epoch #113: Loss:2.4391, Accuracy:0.1791, Validation Loss:2.4470, Validation Accuracy:0.1708\n",
    "Epoch #114: Loss:2.4389, Accuracy:0.1807, Validation Loss:2.4456, Validation Accuracy:0.1675\n",
    "Epoch #115: Loss:2.4391, Accuracy:0.1815, Validation Loss:2.4457, Validation Accuracy:0.1691\n",
    "Epoch #116: Loss:2.4393, Accuracy:0.1819, Validation Loss:2.4462, Validation Accuracy:0.1675\n",
    "Epoch #117: Loss:2.4388, Accuracy:0.1811, Validation Loss:2.4449, Validation Accuracy:0.1773\n",
    "Epoch #118: Loss:2.4377, Accuracy:0.1811, Validation Loss:2.4472, Validation Accuracy:0.1790\n",
    "Epoch #119: Loss:2.4380, Accuracy:0.1791, Validation Loss:2.4464, Validation Accuracy:0.1839\n",
    "Epoch #120: Loss:2.4367, Accuracy:0.1823, Validation Loss:2.4453, Validation Accuracy:0.1724\n",
    "Epoch #121: Loss:2.4359, Accuracy:0.1848, Validation Loss:2.4454, Validation Accuracy:0.1724\n",
    "Epoch #122: Loss:2.4359, Accuracy:0.1836, Validation Loss:2.4453, Validation Accuracy:0.1757\n",
    "Epoch #123: Loss:2.4359, Accuracy:0.1774, Validation Loss:2.4412, Validation Accuracy:0.1741\n",
    "Epoch #124: Loss:2.4358, Accuracy:0.1848, Validation Loss:2.4396, Validation Accuracy:0.1708\n",
    "Epoch #125: Loss:2.4371, Accuracy:0.1840, Validation Loss:2.4447, Validation Accuracy:0.1708\n",
    "Epoch #126: Loss:2.4340, Accuracy:0.1848, Validation Loss:2.4412, Validation Accuracy:0.1675\n",
    "Epoch #127: Loss:2.4347, Accuracy:0.1852, Validation Loss:2.4402, Validation Accuracy:0.1708\n",
    "Epoch #128: Loss:2.4357, Accuracy:0.1848, Validation Loss:2.4412, Validation Accuracy:0.1741\n",
    "Epoch #129: Loss:2.4353, Accuracy:0.1836, Validation Loss:2.4432, Validation Accuracy:0.1741\n",
    "Epoch #130: Loss:2.4352, Accuracy:0.1819, Validation Loss:2.4435, Validation Accuracy:0.1708\n",
    "Epoch #131: Loss:2.4355, Accuracy:0.1832, Validation Loss:2.4412, Validation Accuracy:0.1724\n",
    "Epoch #132: Loss:2.4344, Accuracy:0.1828, Validation Loss:2.4446, Validation Accuracy:0.1691\n",
    "Epoch #133: Loss:2.4345, Accuracy:0.1791, Validation Loss:2.4445, Validation Accuracy:0.1658\n",
    "Epoch #134: Loss:2.4339, Accuracy:0.1807, Validation Loss:2.4443, Validation Accuracy:0.1773\n",
    "Epoch #135: Loss:2.4344, Accuracy:0.1791, Validation Loss:2.4469, Validation Accuracy:0.1724\n",
    "Epoch #136: Loss:2.4342, Accuracy:0.1823, Validation Loss:2.4449, Validation Accuracy:0.1708\n",
    "Epoch #137: Loss:2.4349, Accuracy:0.1807, Validation Loss:2.4437, Validation Accuracy:0.1724\n",
    "Epoch #138: Loss:2.4375, Accuracy:0.1803, Validation Loss:2.4427, Validation Accuracy:0.1724\n",
    "Epoch #139: Loss:2.4375, Accuracy:0.1795, Validation Loss:2.4450, Validation Accuracy:0.1691\n",
    "Epoch #140: Loss:2.4374, Accuracy:0.1803, Validation Loss:2.4440, Validation Accuracy:0.1708\n",
    "Epoch #141: Loss:2.4363, Accuracy:0.1803, Validation Loss:2.4447, Validation Accuracy:0.1708\n",
    "Epoch #142: Loss:2.4364, Accuracy:0.1795, Validation Loss:2.4432, Validation Accuracy:0.1724\n",
    "Epoch #143: Loss:2.4368, Accuracy:0.1803, Validation Loss:2.4437, Validation Accuracy:0.1708\n",
    "Epoch #144: Loss:2.4360, Accuracy:0.1807, Validation Loss:2.4451, Validation Accuracy:0.1708\n",
    "Epoch #145: Loss:2.4356, Accuracy:0.1815, Validation Loss:2.4404, Validation Accuracy:0.1741\n",
    "Epoch #146: Loss:2.4372, Accuracy:0.1811, Validation Loss:2.4395, Validation Accuracy:0.1741\n",
    "Epoch #147: Loss:2.4360, Accuracy:0.1832, Validation Loss:2.4415, Validation Accuracy:0.1806\n",
    "Epoch #148: Loss:2.4358, Accuracy:0.1848, Validation Loss:2.4396, Validation Accuracy:0.1757\n",
    "Epoch #149: Loss:2.4371, Accuracy:0.1782, Validation Loss:2.4421, Validation Accuracy:0.1773\n",
    "Epoch #150: Loss:2.4369, Accuracy:0.1803, Validation Loss:2.4414, Validation Accuracy:0.1773\n",
    "Epoch #151: Loss:2.4374, Accuracy:0.1811, Validation Loss:2.4418, Validation Accuracy:0.1724\n",
    "Epoch #152: Loss:2.4366, Accuracy:0.1844, Validation Loss:2.4425, Validation Accuracy:0.1691\n",
    "Epoch #153: Loss:2.4366, Accuracy:0.1852, Validation Loss:2.4419, Validation Accuracy:0.1708\n",
    "Epoch #154: Loss:2.4371, Accuracy:0.1844, Validation Loss:2.4417, Validation Accuracy:0.1675\n",
    "Epoch #155: Loss:2.4372, Accuracy:0.1828, Validation Loss:2.4431, Validation Accuracy:0.1708\n",
    "Epoch #156: Loss:2.4369, Accuracy:0.1836, Validation Loss:2.4424, Validation Accuracy:0.1675\n",
    "Epoch #157: Loss:2.4370, Accuracy:0.1840, Validation Loss:2.4432, Validation Accuracy:0.1691\n",
    "Epoch #158: Loss:2.4380, Accuracy:0.1807, Validation Loss:2.4437, Validation Accuracy:0.1675\n",
    "Epoch #159: Loss:2.4377, Accuracy:0.1836, Validation Loss:2.4432, Validation Accuracy:0.1691\n",
    "Epoch #160: Loss:2.4404, Accuracy:0.1803, Validation Loss:2.4444, Validation Accuracy:0.1642\n",
    "Epoch #161: Loss:2.4401, Accuracy:0.1807, Validation Loss:2.4457, Validation Accuracy:0.1675\n",
    "Epoch #162: Loss:2.4397, Accuracy:0.1791, Validation Loss:2.4443, Validation Accuracy:0.1642\n",
    "Epoch #163: Loss:2.4393, Accuracy:0.1828, Validation Loss:2.4445, Validation Accuracy:0.1658\n",
    "Epoch #164: Loss:2.4379, Accuracy:0.1799, Validation Loss:2.4436, Validation Accuracy:0.1675\n",
    "Epoch #165: Loss:2.4380, Accuracy:0.1782, Validation Loss:2.4430, Validation Accuracy:0.1691\n",
    "Epoch #166: Loss:2.4384, Accuracy:0.1832, Validation Loss:2.4439, Validation Accuracy:0.1675\n",
    "Epoch #167: Loss:2.4389, Accuracy:0.1807, Validation Loss:2.4440, Validation Accuracy:0.1691\n",
    "Epoch #168: Loss:2.4393, Accuracy:0.1815, Validation Loss:2.4455, Validation Accuracy:0.1741\n",
    "Epoch #169: Loss:2.4388, Accuracy:0.1795, Validation Loss:2.4436, Validation Accuracy:0.1708\n",
    "Epoch #170: Loss:2.4404, Accuracy:0.1828, Validation Loss:2.4442, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4389, Accuracy:0.1803, Validation Loss:2.4437, Validation Accuracy:0.1708\n",
    "Epoch #172: Loss:2.4385, Accuracy:0.1811, Validation Loss:2.4442, Validation Accuracy:0.1708\n",
    "Epoch #173: Loss:2.4388, Accuracy:0.1819, Validation Loss:2.4446, Validation Accuracy:0.1675\n",
    "Epoch #174: Loss:2.4374, Accuracy:0.1828, Validation Loss:2.4450, Validation Accuracy:0.1691\n",
    "Epoch #175: Loss:2.4379, Accuracy:0.1828, Validation Loss:2.4453, Validation Accuracy:0.1724\n",
    "Epoch #176: Loss:2.4375, Accuracy:0.1836, Validation Loss:2.4445, Validation Accuracy:0.1724\n",
    "Epoch #177: Loss:2.4385, Accuracy:0.1848, Validation Loss:2.4446, Validation Accuracy:0.1724\n",
    "Epoch #178: Loss:2.4382, Accuracy:0.1782, Validation Loss:2.4458, Validation Accuracy:0.1724\n",
    "Epoch #179: Loss:2.4380, Accuracy:0.1803, Validation Loss:2.4441, Validation Accuracy:0.1691\n",
    "Epoch #180: Loss:2.4381, Accuracy:0.1828, Validation Loss:2.4440, Validation Accuracy:0.1675\n",
    "Epoch #181: Loss:2.4392, Accuracy:0.1844, Validation Loss:2.4455, Validation Accuracy:0.1691\n",
    "Epoch #182: Loss:2.4382, Accuracy:0.1815, Validation Loss:2.4424, Validation Accuracy:0.1708\n",
    "Epoch #183: Loss:2.4395, Accuracy:0.1815, Validation Loss:2.4428, Validation Accuracy:0.1708\n",
    "Epoch #184: Loss:2.4392, Accuracy:0.1807, Validation Loss:2.4458, Validation Accuracy:0.1724\n",
    "Epoch #185: Loss:2.4401, Accuracy:0.1823, Validation Loss:2.4439, Validation Accuracy:0.1708\n",
    "Epoch #186: Loss:2.4387, Accuracy:0.1848, Validation Loss:2.4434, Validation Accuracy:0.1741\n",
    "Epoch #187: Loss:2.4413, Accuracy:0.1786, Validation Loss:2.4457, Validation Accuracy:0.1790\n",
    "Epoch #188: Loss:2.4395, Accuracy:0.1823, Validation Loss:2.4441, Validation Accuracy:0.1790\n",
    "Epoch #189: Loss:2.4397, Accuracy:0.1844, Validation Loss:2.4451, Validation Accuracy:0.1790\n",
    "Epoch #190: Loss:2.4413, Accuracy:0.1819, Validation Loss:2.4451, Validation Accuracy:0.1708\n",
    "Epoch #191: Loss:2.4408, Accuracy:0.1811, Validation Loss:2.4452, Validation Accuracy:0.1724\n",
    "Epoch #192: Loss:2.4407, Accuracy:0.1803, Validation Loss:2.4454, Validation Accuracy:0.1724\n",
    "Epoch #193: Loss:2.4410, Accuracy:0.1823, Validation Loss:2.4449, Validation Accuracy:0.1724\n",
    "Epoch #194: Loss:2.4409, Accuracy:0.1819, Validation Loss:2.4463, Validation Accuracy:0.1790\n",
    "Epoch #195: Loss:2.4396, Accuracy:0.1819, Validation Loss:2.4428, Validation Accuracy:0.1757\n",
    "Epoch #196: Loss:2.4376, Accuracy:0.1828, Validation Loss:2.4423, Validation Accuracy:0.1773\n",
    "Epoch #197: Loss:2.4383, Accuracy:0.1819, Validation Loss:2.4447, Validation Accuracy:0.1773\n",
    "Epoch #198: Loss:2.4417, Accuracy:0.1815, Validation Loss:2.4471, Validation Accuracy:0.1806\n",
    "Epoch #199: Loss:2.4459, Accuracy:0.1791, Validation Loss:2.4449, Validation Accuracy:0.1741\n",
    "Epoch #200: Loss:2.4425, Accuracy:0.1819, Validation Loss:2.4454, Validation Accuracy:0.1724\n",
    "Epoch #201: Loss:2.4413, Accuracy:0.1778, Validation Loss:2.4445, Validation Accuracy:0.1773\n",
    "Epoch #202: Loss:2.4385, Accuracy:0.1815, Validation Loss:2.4433, Validation Accuracy:0.1806\n",
    "Epoch #203: Loss:2.4391, Accuracy:0.1828, Validation Loss:2.4409, Validation Accuracy:0.1790\n",
    "Epoch #204: Loss:2.4378, Accuracy:0.1832, Validation Loss:2.4411, Validation Accuracy:0.1773\n",
    "Epoch #205: Loss:2.4397, Accuracy:0.1819, Validation Loss:2.4404, Validation Accuracy:0.1757\n",
    "Epoch #206: Loss:2.4381, Accuracy:0.1828, Validation Loss:2.4437, Validation Accuracy:0.1773\n",
    "Epoch #207: Loss:2.4379, Accuracy:0.1836, Validation Loss:2.4435, Validation Accuracy:0.1757\n",
    "Epoch #208: Loss:2.4380, Accuracy:0.1840, Validation Loss:2.4421, Validation Accuracy:0.1790\n",
    "Epoch #209: Loss:2.4382, Accuracy:0.1840, Validation Loss:2.4428, Validation Accuracy:0.1773\n",
    "Epoch #210: Loss:2.4393, Accuracy:0.1828, Validation Loss:2.4429, Validation Accuracy:0.1773\n",
    "Epoch #211: Loss:2.4395, Accuracy:0.1811, Validation Loss:2.4439, Validation Accuracy:0.1773\n",
    "Epoch #212: Loss:2.4388, Accuracy:0.1828, Validation Loss:2.4443, Validation Accuracy:0.1790\n",
    "Epoch #213: Loss:2.4412, Accuracy:0.1828, Validation Loss:2.4420, Validation Accuracy:0.1790\n",
    "Epoch #214: Loss:2.4426, Accuracy:0.1803, Validation Loss:2.4450, Validation Accuracy:0.1790\n",
    "Epoch #215: Loss:2.4420, Accuracy:0.1828, Validation Loss:2.4427, Validation Accuracy:0.1806\n",
    "Epoch #216: Loss:2.4408, Accuracy:0.1791, Validation Loss:2.4432, Validation Accuracy:0.1806\n",
    "Epoch #217: Loss:2.4409, Accuracy:0.1811, Validation Loss:2.4428, Validation Accuracy:0.1872\n",
    "Epoch #218: Loss:2.4390, Accuracy:0.1828, Validation Loss:2.4419, Validation Accuracy:0.1839\n",
    "Epoch #219: Loss:2.4377, Accuracy:0.1815, Validation Loss:2.4431, Validation Accuracy:0.1741\n",
    "Epoch #220: Loss:2.4378, Accuracy:0.1819, Validation Loss:2.4439, Validation Accuracy:0.1757\n",
    "Epoch #221: Loss:2.4356, Accuracy:0.1836, Validation Loss:2.4440, Validation Accuracy:0.1724\n",
    "Epoch #222: Loss:2.4361, Accuracy:0.1807, Validation Loss:2.4439, Validation Accuracy:0.1691\n",
    "Epoch #223: Loss:2.4354, Accuracy:0.1815, Validation Loss:2.4416, Validation Accuracy:0.1741\n",
    "Epoch #224: Loss:2.4356, Accuracy:0.1852, Validation Loss:2.4427, Validation Accuracy:0.1724\n",
    "Epoch #225: Loss:2.4340, Accuracy:0.1811, Validation Loss:2.4439, Validation Accuracy:0.1724\n",
    "Epoch #226: Loss:2.4353, Accuracy:0.1815, Validation Loss:2.4426, Validation Accuracy:0.1724\n",
    "Epoch #227: Loss:2.4352, Accuracy:0.1819, Validation Loss:2.4428, Validation Accuracy:0.1741\n",
    "Epoch #228: Loss:2.4369, Accuracy:0.1852, Validation Loss:2.4447, Validation Accuracy:0.1675\n",
    "Epoch #229: Loss:2.4333, Accuracy:0.1832, Validation Loss:2.4428, Validation Accuracy:0.1724\n",
    "Epoch #230: Loss:2.4348, Accuracy:0.1823, Validation Loss:2.4431, Validation Accuracy:0.1823\n",
    "Epoch #231: Loss:2.4335, Accuracy:0.1803, Validation Loss:2.4444, Validation Accuracy:0.1757\n",
    "Epoch #232: Loss:2.4327, Accuracy:0.1786, Validation Loss:2.4411, Validation Accuracy:0.1757\n",
    "Epoch #233: Loss:2.4354, Accuracy:0.1799, Validation Loss:2.4376, Validation Accuracy:0.1724\n",
    "Epoch #234: Loss:2.4347, Accuracy:0.1786, Validation Loss:2.4421, Validation Accuracy:0.1741\n",
    "Epoch #235: Loss:2.4328, Accuracy:0.1811, Validation Loss:2.4446, Validation Accuracy:0.1773\n",
    "Epoch #236: Loss:2.4337, Accuracy:0.1786, Validation Loss:2.4432, Validation Accuracy:0.1790\n",
    "Epoch #237: Loss:2.4313, Accuracy:0.1803, Validation Loss:2.4447, Validation Accuracy:0.1773\n",
    "Epoch #238: Loss:2.4306, Accuracy:0.1791, Validation Loss:2.4455, Validation Accuracy:0.1691\n",
    "Epoch #239: Loss:2.4296, Accuracy:0.1807, Validation Loss:2.4459, Validation Accuracy:0.1658\n",
    "Epoch #240: Loss:2.4305, Accuracy:0.1819, Validation Loss:2.4471, Validation Accuracy:0.1724\n",
    "Epoch #241: Loss:2.4305, Accuracy:0.1795, Validation Loss:2.4450, Validation Accuracy:0.1658\n",
    "Epoch #242: Loss:2.4299, Accuracy:0.1795, Validation Loss:2.4453, Validation Accuracy:0.1658\n",
    "Epoch #243: Loss:2.4295, Accuracy:0.1795, Validation Loss:2.4434, Validation Accuracy:0.1724\n",
    "Epoch #244: Loss:2.4300, Accuracy:0.1791, Validation Loss:2.4429, Validation Accuracy:0.1642\n",
    "Epoch #245: Loss:2.4284, Accuracy:0.1811, Validation Loss:2.4409, Validation Accuracy:0.1609\n",
    "Epoch #246: Loss:2.4296, Accuracy:0.1803, Validation Loss:2.4418, Validation Accuracy:0.1708\n",
    "Epoch #247: Loss:2.4287, Accuracy:0.1786, Validation Loss:2.4437, Validation Accuracy:0.1708\n",
    "Epoch #248: Loss:2.4287, Accuracy:0.1815, Validation Loss:2.4418, Validation Accuracy:0.1724\n",
    "Epoch #249: Loss:2.4304, Accuracy:0.1782, Validation Loss:2.4416, Validation Accuracy:0.1708\n",
    "Epoch #250: Loss:2.4294, Accuracy:0.1791, Validation Loss:2.4423, Validation Accuracy:0.1741\n",
    "Epoch #251: Loss:2.4299, Accuracy:0.1823, Validation Loss:2.4428, Validation Accuracy:0.1741\n",
    "Epoch #252: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4437, Validation Accuracy:0.1724\n",
    "Epoch #253: Loss:2.4292, Accuracy:0.1852, Validation Loss:2.4431, Validation Accuracy:0.1708\n",
    "Epoch #254: Loss:2.4287, Accuracy:0.1811, Validation Loss:2.4436, Validation Accuracy:0.1658\n",
    "Epoch #255: Loss:2.4301, Accuracy:0.1819, Validation Loss:2.4429, Validation Accuracy:0.1691\n",
    "Epoch #256: Loss:2.4308, Accuracy:0.1778, Validation Loss:2.4436, Validation Accuracy:0.1741\n",
    "Epoch #257: Loss:2.4301, Accuracy:0.1782, Validation Loss:2.4438, Validation Accuracy:0.1724\n",
    "Epoch #258: Loss:2.4318, Accuracy:0.1807, Validation Loss:2.4431, Validation Accuracy:0.1741\n",
    "Epoch #259: Loss:2.4299, Accuracy:0.1795, Validation Loss:2.4443, Validation Accuracy:0.1691\n",
    "Epoch #260: Loss:2.4296, Accuracy:0.1807, Validation Loss:2.4425, Validation Accuracy:0.1724\n",
    "Epoch #261: Loss:2.4284, Accuracy:0.1815, Validation Loss:2.4427, Validation Accuracy:0.1741\n",
    "Epoch #262: Loss:2.4278, Accuracy:0.1762, Validation Loss:2.4442, Validation Accuracy:0.1658\n",
    "Epoch #263: Loss:2.4294, Accuracy:0.1819, Validation Loss:2.4414, Validation Accuracy:0.1724\n",
    "Epoch #264: Loss:2.4309, Accuracy:0.1803, Validation Loss:2.4415, Validation Accuracy:0.1757\n",
    "Epoch #265: Loss:2.4289, Accuracy:0.1782, Validation Loss:2.4429, Validation Accuracy:0.1593\n",
    "Epoch #266: Loss:2.4292, Accuracy:0.1840, Validation Loss:2.4406, Validation Accuracy:0.1724\n",
    "Epoch #267: Loss:2.4306, Accuracy:0.1823, Validation Loss:2.4439, Validation Accuracy:0.1741\n",
    "Epoch #268: Loss:2.4278, Accuracy:0.1836, Validation Loss:2.4445, Validation Accuracy:0.1691\n",
    "Epoch #269: Loss:2.4281, Accuracy:0.1803, Validation Loss:2.4421, Validation Accuracy:0.1741\n",
    "Epoch #270: Loss:2.4278, Accuracy:0.1786, Validation Loss:2.4409, Validation Accuracy:0.1790\n",
    "Epoch #271: Loss:2.4288, Accuracy:0.1832, Validation Loss:2.4420, Validation Accuracy:0.1724\n",
    "Epoch #272: Loss:2.4272, Accuracy:0.1811, Validation Loss:2.4441, Validation Accuracy:0.1773\n",
    "Epoch #273: Loss:2.4276, Accuracy:0.1807, Validation Loss:2.4466, Validation Accuracy:0.1724\n",
    "Epoch #274: Loss:2.4283, Accuracy:0.1774, Validation Loss:2.4450, Validation Accuracy:0.1773\n",
    "Epoch #275: Loss:2.4272, Accuracy:0.1786, Validation Loss:2.4452, Validation Accuracy:0.1741\n",
    "Epoch #276: Loss:2.4271, Accuracy:0.1786, Validation Loss:2.4463, Validation Accuracy:0.1757\n",
    "Epoch #277: Loss:2.4280, Accuracy:0.1758, Validation Loss:2.4445, Validation Accuracy:0.1741\n",
    "Epoch #278: Loss:2.4263, Accuracy:0.1791, Validation Loss:2.4436, Validation Accuracy:0.1741\n",
    "Epoch #279: Loss:2.4281, Accuracy:0.1758, Validation Loss:2.4431, Validation Accuracy:0.1675\n",
    "Epoch #280: Loss:2.4269, Accuracy:0.1848, Validation Loss:2.4433, Validation Accuracy:0.1708\n",
    "Epoch #281: Loss:2.4276, Accuracy:0.1828, Validation Loss:2.4442, Validation Accuracy:0.1724\n",
    "Epoch #282: Loss:2.4271, Accuracy:0.1828, Validation Loss:2.4415, Validation Accuracy:0.1773\n",
    "Epoch #283: Loss:2.4274, Accuracy:0.1840, Validation Loss:2.4438, Validation Accuracy:0.1691\n",
    "Epoch #284: Loss:2.4251, Accuracy:0.1869, Validation Loss:2.4430, Validation Accuracy:0.1708\n",
    "Epoch #285: Loss:2.4251, Accuracy:0.1848, Validation Loss:2.4414, Validation Accuracy:0.1741\n",
    "Epoch #286: Loss:2.4260, Accuracy:0.1856, Validation Loss:2.4425, Validation Accuracy:0.1724\n",
    "Epoch #287: Loss:2.4239, Accuracy:0.1864, Validation Loss:2.4441, Validation Accuracy:0.1724\n",
    "Epoch #288: Loss:2.4256, Accuracy:0.1836, Validation Loss:2.4435, Validation Accuracy:0.1724\n",
    "Epoch #289: Loss:2.4243, Accuracy:0.1848, Validation Loss:2.4448, Validation Accuracy:0.1708\n",
    "Epoch #290: Loss:2.4260, Accuracy:0.1836, Validation Loss:2.4431, Validation Accuracy:0.1691\n",
    "Epoch #291: Loss:2.4251, Accuracy:0.1860, Validation Loss:2.4432, Validation Accuracy:0.1708\n",
    "Epoch #292: Loss:2.4245, Accuracy:0.1864, Validation Loss:2.4422, Validation Accuracy:0.1626\n",
    "Epoch #293: Loss:2.4247, Accuracy:0.1848, Validation Loss:2.4424, Validation Accuracy:0.1642\n",
    "Epoch #294: Loss:2.4244, Accuracy:0.1856, Validation Loss:2.4413, Validation Accuracy:0.1790\n",
    "Epoch #295: Loss:2.4227, Accuracy:0.1864, Validation Loss:2.4423, Validation Accuracy:0.1790\n",
    "Epoch #296: Loss:2.4236, Accuracy:0.1856, Validation Loss:2.4411, Validation Accuracy:0.1724\n",
    "Epoch #297: Loss:2.4230, Accuracy:0.1856, Validation Loss:2.4412, Validation Accuracy:0.1675\n",
    "Epoch #298: Loss:2.4232, Accuracy:0.1852, Validation Loss:2.4415, Validation Accuracy:0.1724\n",
    "Epoch #299: Loss:2.4231, Accuracy:0.1848, Validation Loss:2.4425, Validation Accuracy:0.1741\n",
    "Epoch #300: Loss:2.4241, Accuracy:0.1889, Validation Loss:2.4439, Validation Accuracy:0.1724\n",
    "\n",
    "Test:\n",
    "Test Loss:2.44391441, Accuracy:0.1724\n",
    "Labels: ['eo', 'ek', 'aa', 'sg', 'ib', 'ck', 'eb', 'eg', 'mb', 'my', 'yd', 'by', 'ds', 'ce', 'sk']\n",
    "Confusion Matrix:\n",
    "      eo  ek  aa  sg  ib  ck  eb  eg  mb  my  yd  by  ds  ce  sk\n",
    "t:eo   0   0   0  14   3   0   3   4   0   0   1   9   0   0   0\n",
    "t:ek   0   0   0  13   4   0   0  21   0   0   2   7   1   0   0\n",
    "t:aa   0   0   0   3   3   0   1  15   0   0   0   3   9   0   0\n",
    "t:sg   0   0   0  27   7   0   1   4   0   0   2  10   0   0   0\n",
    "t:ib   0   0   0  11  19   0   0   8   0   0  11   4   1   0   0\n",
    "t:ck   0   0   0   5   0   0   2  10   0   0   0   4   2   0   0\n",
    "t:eb   0   0   0  12   4   0   2  18   0   0   4   9   1   0   0\n",
    "t:eg   0   0   0   5   0   0   3  25   0   0   1   6  10   0   0\n",
    "t:mb   0   0   0  17   9   0   2  14   0   0   4   6   0   0   0\n",
    "t:my   0   0   0   4   7   0   0   8   0   0   1   0   0   0   0\n",
    "t:yd   0   0   0  17  20   0   0   2   0   0  18   5   0   0   0\n",
    "t:by   0   0   0  16   1   0   3  10   0   0   3   7   0   0   0\n",
    "t:ds   0   0   0   5   1   0   2  13   0   0   0   3   7   0   0\n",
    "t:ce   0   0   0  14   1   0   0   7   0   0   1   3   1   0   0\n",
    "t:sk   0   0   0   7   0   0   1  14   0   0   1   6   4   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          sg       0.16      0.53      0.24        51\n",
    "          ib       0.24      0.35      0.29        54\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eb       0.10      0.04      0.06        50\n",
    "          eg       0.14      0.50      0.22        50\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          my       0.00      0.00      0.00        20\n",
    "          yd       0.37      0.29      0.32        62\n",
    "          by       0.09      0.17      0.11        40\n",
    "          ds       0.19      0.23      0.21        31\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sk       0.00      0.00      0.00        33\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.09      0.14      0.10       609\n",
    "weighted avg       0.11      0.17      0.12       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 23:44:03 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 50 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.699323263465869, 2.693778562624075, 2.687343066549066, 2.68105815314307, 2.6748088478846306, 2.667735175741913, 2.6609648444578173, 2.654553799402146, 2.645521746088914, 2.6384857581754035, 2.6270175023227686, 2.6155369782878455, 2.6170944776049585, 2.588503208849426, 2.5691565022679974, 2.559752095509045, 2.548752398717971, 2.5394786296806897, 2.5336117388188155, 2.530125393264595, 2.5231152230687135, 2.5110703446399207, 2.5115650584936535, 2.498238893173794, 2.4969918767023946, 2.491333295950553, 2.480701204199705, 2.4768345422541174, 2.4712240574590876, 2.4683307027581876, 2.4640086652414355, 2.4698217052152787, 2.4633863144515966, 2.4654160739948794, 2.462681474356816, 2.4568740335004082, 2.4584498977034746, 2.4669900065768138, 2.459820236478533, 2.4602734725463566, 2.4548203851201853, 2.4562708008269762, 2.4540720130813924, 2.4512070509404777, 2.457333174049365, 2.4513673179451074, 2.4505815764366115, 2.448969412711258, 2.4485201592907333, 2.448241398643782, 2.447955939178592, 2.4509279962830943, 2.4519077611869973, 2.4493855925029133, 2.4473944505055747, 2.450505862682324, 2.4449989396363057, 2.4584885700582872, 2.4511184179528396, 2.451340201648781, 2.447384429095414, 2.4455431263239316, 2.4481886971760267, 2.444322572357353, 2.447142540331936, 2.4448810860832726, 2.444736517121639, 2.445701027151399, 2.444678685543768, 2.443416366436211, 2.4422025567009333, 2.4500513327337057, 2.4408761272680977, 2.4416616745966016, 2.443161568226681, 2.4397208588855412, 2.4455534687574665, 2.439895158918033, 2.4417734666802415, 2.4444915520146564, 2.4384169480679265, 2.443387478638948, 2.4384970770680847, 2.440820262937123, 2.4442739181330637, 2.439463812729408, 2.440262570169759, 2.4398040481780354, 2.444485476843046, 2.441642491884028, 2.442281622017546, 2.4483807165242966, 2.442270829368303, 2.4394885205674446, 2.4438119936850664, 2.44162485008365, 2.4410847971592045, 2.4734912423664714, 2.4656724988533356, 2.4626667123709995, 2.4577817329632237, 2.4683677985750396, 2.4676522406059727, 2.451901151237425, 2.45034836430855, 2.4552468141702986, 2.446374462155873, 2.4448314758357155, 2.450169386730601, 2.44721588164519, 2.446327936864643, 2.4457523286440495, 2.446953585582414, 2.445596885994346, 2.445660327651426, 2.446176622888725, 2.4448924279956787, 2.447173434525288, 2.446377094547541, 2.4452568357213966, 2.4454267858871686, 2.4453466052100774, 2.44116692824904, 2.439623355473987, 2.4446619813665382, 2.4412280170396827, 2.440163498441574, 2.441167951608918, 2.443207250244316, 2.44348297487143, 2.4411528278845678, 2.444566986635205, 2.444458066144796, 2.4442551668445858, 2.4468972526356114, 2.4449357465765944, 2.4436509675776037, 2.4426818328537965, 2.4450093051678636, 2.4440391016711156, 2.4447211909959665, 2.443238191416698, 2.443688642019513, 2.4451300585015456, 2.440400409776785, 2.4395345451405093, 2.441506826623124, 2.4396327313139716, 2.442120372759689, 2.4413538002615494, 2.44178056521173, 2.442498149542973, 2.4418953244126294, 2.441655705519302, 2.4431469683185196, 2.4424021326262375, 2.4431623026655225, 2.443748736812172, 2.44323037293157, 2.444408557293646, 2.4457471703465155, 2.4442918954420167, 2.4445333187216023, 2.4435684516512115, 2.4430433920843067, 2.4439057861447138, 2.443992734542621, 2.4454589845315966, 2.443555885152081, 2.4442015821710594, 2.443712139364534, 2.444162334714617, 2.444635763152675, 2.445002902317517, 2.445339768978175, 2.44453844922321, 2.444629264386808, 2.4457889683728147, 2.4440979186341485, 2.444046500085414, 2.4455092109874355, 2.442437792059236, 2.4427657789001715, 2.4458468265721365, 2.4438789485906343, 2.4434431773688408, 2.4456811475832083, 2.4440912299947004, 2.445119456508868, 2.4451272569853684, 2.4452349669827615, 2.4453825899728607, 2.4449126387660334, 2.446287072546572, 2.4428476762693307, 2.4422701861470792, 2.4447340812589147, 2.4471226396231818, 2.44492929322379, 2.445435623034272, 2.4445276268205816, 2.4432753389104835, 2.440884817214239, 2.441109369541037, 2.4404228242551556, 2.4436986606892304, 2.4435389143688533, 2.442115452880734, 2.442777977005406, 2.4429094936264364, 2.443855886193136, 2.4443252560344626, 2.442028820025314, 2.4450463990272557, 2.4427032920918834, 2.443186894230459, 2.4428348791814596, 2.441863951816152, 2.4431441605384716, 2.443859176682721, 2.4440371606541778, 2.4438779800396246, 2.4416244738599153, 2.4427320781012476, 2.443872507765571, 2.442602577663603, 2.442783305210433, 2.4446534037785774, 2.4428185982069945, 2.4431256856432886, 2.444385245907287, 2.441111280021605, 2.437613927672062, 2.442116309856546, 2.444629419417608, 2.443151466169185, 2.4446913737968856, 2.4454739156419225, 2.4459354470320327, 2.4471119178339764, 2.4450376253018433, 2.4452808443548646, 2.443436056130821, 2.442856924874442, 2.440895375751314, 2.4418324347591556, 2.4436877364987026, 2.4417839942894544, 2.4416245165325345, 2.442268602562264, 2.4427620679482644, 2.443660855097528, 2.4431196453144595, 2.443576762633175, 2.4429406884855824, 2.443579896917484, 2.4438482004237683, 2.4431335491499877, 2.4442711056551127, 2.442510637352228, 2.4426702811017216, 2.444194470328846, 2.4414230931568617, 2.4414917749332874, 2.4428631084893135, 2.440629715598471, 2.443933136553208, 2.4445355012890544, 2.442107534173674, 2.4408516210483997, 2.4419704791164554, 2.444120574663034, 2.4465686507608697, 2.4450283567306443, 2.4452095196164887, 2.4462960244008083, 2.4444817694145664, 2.4436056488644704, 2.443114766542156, 2.443327764376435, 2.444190073091604, 2.441530381125965, 2.4438091303131655, 2.4430034442488195, 2.4414322019993575, 2.4424812237813165, 2.4440622306222397, 2.4435234715785885, 2.444835476883135, 2.443111738743649, 2.443174382931689, 2.4422463419402174, 2.4423585670139207, 2.4413254163143865, 2.4423076116001274, 2.441053632836428, 2.4411825859683685, 2.4415081623935544, 2.4424778150611717, 2.443914166029255], 'val_acc': [0.08045977000483542, 0.09195402258477971, 0.08866995062880915, 0.09195402278052567, 0.09359605880744742, 0.09523809493224218, 0.0903119867536039, 0.08866995062880915, 0.14121510592489603, 0.14285714185394482, 0.1559934311459217, 0.16584564769894422, 0.1346469611320981, 0.16256157544935473, 0.15270935870058627, 0.17077175617120144, 0.16420361157414948, 0.16420361157414948, 0.15927750319976525, 0.16091953942243298, 0.17241379258961514, 0.1756978649370776, 0.16748768382373896, 0.17077175646482037, 0.16748768392161195, 0.1773399008661264, 0.17733990057250745, 0.18390804516955941, 0.18226600894689168, 0.18062397272422395, 0.180623972626351, 0.17733990047463447, 0.17405582902026293, 0.1789819365994292, 0.1822660088490187, 0.180623972626351, 0.1871921173212759, 0.18062397282209694, 0.1691297206458787, 0.1871921174191489, 0.18883415354394364, 0.19376026191832788, 0.1871921174191489, 0.17569786534080364, 0.18555008129435416, 0.17898193759039313, 0.18390804487594048, 0.18226600875114574, 0.1888341533481977, 0.1871921173212759, 0.17898193759039313, 0.17241379299334117, 0.17241379299334117, 0.1707717569664194, 0.17898193749252014, 0.18226600983998262, 0.18062397371518787, 0.17405582832291797, 0.1707717559754555, 0.18226600894689168, 0.1740558292160089, 0.17241379309121416, 0.18555008129435416, 0.180623972626351, 0.18555008129435416, 0.18555008129435416, 0.18555008129435416, 0.1904761896687384, 0.1904761896687384, 0.1871921174191489, 0.18390804487594048, 0.18555008129435416, 0.18555008100073522, 0.1871921174191489, 0.18883415354394364, 0.18226600875114574, 0.1871921174191489, 0.18883415344607066, 0.18555008119648117, 0.18555008129435416, 0.18226600974210966, 0.1871921174191489, 0.19376026191832788, 0.18062397371518787, 0.1871921173212759, 0.18883415433916162, 0.1904761896687384, 0.18226600875114574, 0.17733990067038044, 0.16912972084162467, 0.1707717569664194, 0.16748768452108395, 0.17241379289546818, 0.18062397272422395, 0.18062397282209694, 0.18390804516955941, 0.18062397282209694, 0.17733990057250745, 0.16748768461895694, 0.16584564859203518, 0.16912972074375168, 0.18062397282209694, 0.16912972014427968, 0.17077175686854643, 0.16256157535148175, 0.16584564809043614, 0.16748768471682993, 0.1707717569664194, 0.1740558285186639, 0.1773399014655984, 0.180623972626351, 0.17241379239386917, 0.1707717559754555, 0.16748768362799302, 0.16912971975278776, 0.16748768362799302, 0.17733990047463447, 0.17898193679517518, 0.18390804516955941, 0.17241379200237725, 0.17241379200237725, 0.1756978644477127, 0.174055828127172, 0.1707717558775825, 0.17077175607332848, 0.16748768362799302, 0.1707717558775825, 0.17405582832291797, 0.17405582832291797, 0.1707717558775825, 0.17241379200237725, 0.1691297199485337, 0.16584564760107126, 0.17733990037676148, 0.17241379210025023, 0.1707717559754555, 0.17241379210025023, 0.17241379210025023, 0.16912971985066075, 0.1707717559754555, 0.1707717559754555, 0.17241379210025023, 0.1707717559754555, 0.1707717559754555, 0.17405582822504498, 0.17405582822504498, 0.18062397291996993, 0.1756978644477127, 0.17733990067038044, 0.17733990057250745, 0.17241379210025023, 0.16912971985066075, 0.1707717558775825, 0.16748768362799302, 0.1707717559754555, 0.16748768362799302, 0.16912971975278776, 0.16748768471682993, 0.16912972084162467, 0.16420361246724044, 0.16748768362799302, 0.16420361246724044, 0.16584564859203518, 0.167487683725866, 0.16912972084162467, 0.16748768471682993, 0.16912972084162467, 0.17405582832291797, 0.1707717569664194, 0.1707717569664194, 0.1707717559754555, 0.1707717559754555, 0.16748768471682993, 0.16912971975278776, 0.17241379200237725, 0.17241379200237725, 0.17241379200237725, 0.17241379210025023, 0.16912971975278776, 0.16748768471682993, 0.16912971975278776, 0.1707717569664194, 0.1707717569664194, 0.17241379200237725, 0.1707717569664194, 0.1740558292160089, 0.17898193650155622, 0.17898193759039313, 0.17898193650155622, 0.1707717569664194, 0.17241379309121416, 0.17241379309121416, 0.17241379309121416, 0.17898193650155622, 0.17569786425196673, 0.17733990037676148, 0.17733990037676148, 0.18062397272422395, 0.17405582822504498, 0.17241379210025023, 0.17733990047463447, 0.180623972626351, 0.17898193650155622, 0.17733990037676148, 0.17569786434983972, 0.17733990047463447, 0.17569786434983972, 0.1789819365994292, 0.17733990047463447, 0.17733990047463447, 0.17733990047463447, 0.1789819365994292, 0.1789819365994292, 0.1789819365994292, 0.18062397272422395, 0.18062397272422395, 0.1871921174191489, 0.18390804516955941, 0.17405582822504498, 0.17569786434983972, 0.17241379210025023, 0.16912971985066075, 0.17405582822504498, 0.17241379210025023, 0.17241379210025023, 0.17241379210025023, 0.17405582822504498, 0.167487683725866, 0.17241379219812322, 0.18226600904476467, 0.17569786454558567, 0.17569786454558567, 0.17241379219812322, 0.17405582822504498, 0.1773399007682534, 0.1789819365994292, 0.1773399014655984, 0.16912971985066075, 0.16584564760107126, 0.17241379309121416, 0.16584564750319827, 0.16584564760107126, 0.17241379309121416, 0.16420361246724044, 0.16091954021765092, 0.17077175686854643, 0.1707717558775825, 0.17241379210025023, 0.1707717559754555, 0.17405582822504498, 0.17405582822504498, 0.17241379210025023, 0.17077175686854643, 0.16584564760107126, 0.16912971985066075, 0.17405582822504498, 0.17241379210025023, 0.1740558292160089, 0.16912971985066075, 0.17241379210025023, 0.1740558291181359, 0.16584564760107126, 0.17241379200237725, 0.17569786425196673, 0.15927750310189226, 0.17241379200237725, 0.174055828127172, 0.16912971985066075, 0.17405582822504498, 0.1789819366973022, 0.17241379219812322, 0.17733990057250745, 0.17241379210025023, 0.17733990047463447, 0.174055828127172, 0.1756978644477127, 0.174055828127172, 0.174055828127172, 0.16748768471682993, 0.1707717559754555, 0.17241379200237725, 0.17733990047463447, 0.1691297199485337, 0.17077175677067344, 0.17405582822504498, 0.17241379200237725, 0.17241379210025023, 0.17241379210025023, 0.1707717558775825, 0.1691297199485337, 0.17077175607332848, 0.1625615752536088, 0.16420361137840353, 0.17898193679517518, 0.1789819366973022, 0.17241379219812322, 0.16748768362799302, 0.17241379210025023, 0.17405582832291797, 0.17241379219812322], 'loss': [2.704391577013709, 2.6970595222723803, 2.6906286191646567, 2.6846930391245065, 2.678522574779189, 2.6723168500395036, 2.665408317655998, 2.6587369818951805, 2.651299017214922, 2.6410869616013044, 2.6357073348160887, 2.6228122029705947, 2.612194837730768, 2.5957804714140216, 2.5788108138333112, 2.5639437426776612, 2.551718595825916, 2.54413996672973, 2.5304559984735886, 2.5253973948147754, 2.5177088080490395, 2.5101635746887334, 2.505311569198201, 2.4985390029648737, 2.489309963112739, 2.485493970945386, 2.481640383839852, 2.4761958459074735, 2.4733826963318934, 2.4701143658381466, 2.469200810171985, 2.4710571117714446, 2.469856116610141, 2.4662067404517893, 2.464411033790949, 2.4617052507106774, 2.4690236583137906, 2.462800900500413, 2.458204414614417, 2.4590216540702805, 2.4580572946850037, 2.4570001551258, 2.4573365012723074, 2.4550246282769423, 2.4518597670404327, 2.4528147706260914, 2.450354324278156, 2.452362533761246, 2.4530280894567347, 2.452285159246143, 2.450351091380971, 2.449790212016331, 2.4498174115128095, 2.4462965774340306, 2.4489002222887546, 2.448197277370664, 2.447728089385454, 2.4506261836331973, 2.4512457133808176, 2.447568417672504, 2.4471660070595553, 2.446471375949084, 2.4477149579559265, 2.446432018867508, 2.445924981663604, 2.444596534685922, 2.444017046041312, 2.444376318557551, 2.4448224229244726, 2.4469816901111017, 2.4501228439244893, 2.445482608474011, 2.4442916808431887, 2.4442248833008127, 2.445490807429476, 2.443577552673998, 2.4432889276461434, 2.4434622584427164, 2.4440694666006726, 2.4447014432668195, 2.4423443288039377, 2.4427892146414067, 2.4419397099551725, 2.4429426461519403, 2.441981408826135, 2.4418135267018783, 2.441802763204555, 2.4401734306092626, 2.4403957340506803, 2.440882081907143, 2.439285760346869, 2.4401869642661094, 2.441446024191698, 2.4414784884795515, 2.4402895529901714, 2.438485058230296, 2.4392818766208153, 2.449875184593749, 2.459111911902927, 2.45766255429638, 2.452671564579989, 2.448251444994791, 2.4710456340954288, 2.4495992337408987, 2.453408736614721, 2.450257811164464, 2.443491240546444, 2.4442972529840175, 2.440809811018331, 2.4424407628037845, 2.43884724777582, 2.4377894267409244, 2.439078475317671, 2.438876711416538, 2.439083862500514, 2.439323147807033, 2.4387810781506296, 2.4376688522479864, 2.43800606384904, 2.4366947229882774, 2.4359348753394534, 2.4359268460675185, 2.4359086782780515, 2.435779913348094, 2.4371426799703673, 2.434013889066003, 2.434726682091151, 2.435666025933299, 2.4353487635540034, 2.43524378692345, 2.4355120722518073, 2.434379898400277, 2.434541619729702, 2.4339022978130553, 2.434446637684315, 2.4342347565128084, 2.4348613846718163, 2.4374548623204477, 2.437525584614497, 2.437359435259684, 2.4363148483652353, 2.436362389029908, 2.4367702232494, 2.4359605262411694, 2.435610835899807, 2.437186424835017, 2.4359821171731184, 2.4358360902484684, 2.4370561665333272, 2.436896474454437, 2.4373770500355434, 2.436621023937907, 2.436619801296101, 2.4371204737520316, 2.4372336262305416, 2.436850784740409, 2.4370253867437217, 2.4380050365440167, 2.437746354050215, 2.4404068654077986, 2.440119986759319, 2.439673819825879, 2.4393411456192298, 2.437905992668512, 2.4379657670947315, 2.4383553779100735, 2.4388549880080643, 2.439299724821682, 2.438801208413847, 2.440415192531609, 2.438854638555946, 2.4385184819693437, 2.4388154692718382, 2.437441003983515, 2.4379239805914783, 2.4374660021960124, 2.4385268811572502, 2.438166905036942, 2.437979614171649, 2.4381323497153407, 2.439172280642531, 2.438175803190384, 2.43954449620335, 2.4392004284281015, 2.4400627816727027, 2.4387291080897837, 2.44131802641146, 2.4394677863228735, 2.439680509939331, 2.4412609677050394, 2.4407683421454145, 2.440707017753648, 2.4409874691855493, 2.4409486362331947, 2.439614128234205, 2.437552573303912, 2.4382621388171, 2.441675483127884, 2.4459049994451063, 2.442515276687591, 2.441342289198106, 2.4385310807512037, 2.4391294668342542, 2.437799997055555, 2.439662033625452, 2.438058825439986, 2.437869712018869, 2.437951217686616, 2.4382000758662605, 2.4392862439400362, 2.4395116138262427, 2.4388181432806246, 2.4412145819262556, 2.442594031582623, 2.4419992975630556, 2.4408029848055675, 2.440914701876944, 2.438959120822883, 2.4377281028387237, 2.4377502133958884, 2.4356489663251373, 2.436104722287376, 2.4354040502277976, 2.435641233534294, 2.4340484012078947, 2.4353038516622303, 2.435230656864707, 2.436926203537771, 2.433318668322397, 2.434783553293843, 2.433461322725676, 2.4327139731060554, 2.435358093798283, 2.4347153554951633, 2.4327843474166837, 2.433666309145197, 2.4312838160771366, 2.430578269312269, 2.4295539712024663, 2.4305386954509256, 2.430508815972957, 2.4299342594107562, 2.429506139148187, 2.429992499831276, 2.428428422352127, 2.4295978353253624, 2.4287222601794607, 2.4286778988534663, 2.4304217368915095, 2.429426250222772, 2.4298627520488765, 2.430772378655185, 2.4292235458656015, 2.428740930165598, 2.430071059487439, 2.4307626027101366, 2.4300619106762706, 2.4317833133791504, 2.429852136792099, 2.4296158141424034, 2.428360401678379, 2.4277700052124276, 2.4294005579037834, 2.4308980819380994, 2.428927837994554, 2.4292445233715143, 2.430552662863134, 2.427793806925936, 2.428126077926134, 2.4278113586457115, 2.428849355200233, 2.4271510934927627, 2.4276235792915926, 2.428293075894428, 2.427161637762489, 2.4271469957285103, 2.4279894608491746, 2.4263072254721387, 2.428094154608568, 2.4269065824622245, 2.4275536781953346, 2.427144977835904, 2.4274151786396883, 2.425130506805326, 2.4250882195741, 2.426020304035602, 2.423896498846567, 2.4255808378881496, 2.424256795730434, 2.42603255728187, 2.4250893541919623, 2.424493504941341, 2.4246991526664403, 2.4244336505200583, 2.4227364361898123, 2.4235918382844397, 2.422997750589735, 2.423229851712926, 2.4231093933450123, 2.424133924683996], 'acc': [0.09240246441146431, 0.09075975334252664, 0.09404517409125883, 0.09199178600090975, 0.09199178638338308, 0.0878850098202116, 0.08459958966813783, 0.07474332605115687, 0.10965092454786418, 0.14004106669088165, 0.13963039104943403, 0.15770020640361482, 0.15646817299864377, 0.15154004119015327, 0.1597535945123226, 0.1605749495709946, 0.1650924035472302, 0.1601642705454229, 0.16303901389027034, 0.16098562540826855, 0.1679671460720548, 0.16919917894462297, 0.17248459889169102, 0.1761806988251038, 0.171252567408266, 0.17125256783663614, 0.17330595453784206, 0.17618069763178698, 0.17371663174590046, 0.17371663313504362, 0.1733059551253211, 0.1757700200320759, 0.17248459906915864, 0.17207392266276436, 0.17084188979019618, 0.17084188982691365, 0.17166324465304184, 0.17248459969335514, 0.17412731116312485, 0.17289527709723987, 0.17330595412783065, 0.17166324502633581, 0.17412731036146073, 0.17494866516923024, 0.17330595571280016, 0.17248460008500782, 0.17289527729306622, 0.1720739216836326, 0.1716632440472041, 0.17207392285859072, 0.17248459987082276, 0.17248459949752878, 0.17166324426138915, 0.1704312106238743, 0.16550308059617969, 0.17371663213755315, 0.17371663274339091, 0.16960985617104007, 0.17248460047666053, 0.16796714530710813, 0.17248460028083418, 0.172073922466938, 0.17494866481429497, 0.17659137587405327, 0.17659137685318502, 0.17618069745431936, 0.1757700204420873, 0.1774127307001815, 0.17330595373617794, 0.17905544158247216, 0.17043121238631145, 0.17248459988918147, 0.17453798778370422, 0.17371663235173823, 0.1757700208153813, 0.17453798835282452, 0.17741273208932465, 0.17494866442264229, 0.17084188841941175, 0.17700205447125483, 0.17577002001371717, 0.1778234094932094, 0.1757700196404232, 0.17494866479593627, 0.17700205228044757, 0.17700205386541706, 0.17577002063791364, 0.1778234085324364, 0.18316221674732114, 0.17659137487656282, 0.1761806980417984, 0.17946611998384737, 0.17330595373617794, 0.17166324365555138, 0.17412730918650265, 0.17166324424303042, 0.1761806976501457, 0.1815195066850533, 0.17864476355439093, 0.17782340872826272, 0.17823408730710558, 0.1786447641602287, 0.1749486649917626, 0.17741273089600784, 0.17659137685318502, 0.1712525673899073, 0.17166324522216217, 0.17823408691545287, 0.17577002061955493, 0.17577002142121906, 0.1786447629669119, 0.17782340890573037, 0.17905544195576614, 0.18069815262387176, 0.18151950729089106, 0.18193018449894946, 0.18110882926280983, 0.18110882847950444, 0.17905544197412487, 0.18234086331033608, 0.1848049278437969, 0.18357289438374969, 0.17741273169767197, 0.18480492860874356, 0.1839835735684303, 0.18480492704213278, 0.18521560583516067, 0.1848049284129172, 0.1835728945612173, 0.18193018449894946, 0.18316221696150622, 0.18275153953926274, 0.17905544078080807, 0.18069815225057778, 0.17905544158247216, 0.1823408626861396, 0.18069815301552444, 0.18028747541581336, 0.17946611838051915, 0.18028747461414923, 0.1802874752016283, 0.17946611898635692, 0.18028747342083243, 0.18069815125308733, 0.1815195062934006, 0.18110882965446254, 0.1831622181364643, 0.1848049280029058, 0.1782340857588535, 0.18028747523834573, 0.181108830651953, 0.1843942507948474, 0.18521560642263973, 0.18439425099067375, 0.1827515393617951, 0.18357289614618683, 0.18398357258929854, 0.18069815225057778, 0.18357289536288143, 0.1802874740266702, 0.18069815203639272, 0.17905544236577756, 0.1827515393434364, 0.17987679640860038, 0.17823408632797383, 0.18316221754898526, 0.18069815105726098, 0.18151950766418504, 0.17946611998384737, 0.18275154051839448, 0.18028747384920257, 0.18110883045612663, 0.1819301837340028, 0.1827515401451005, 0.18275154010838307, 0.18357289634201315, 0.18480492821709085, 0.1782340857404948, 0.18028747365337622, 0.18275154010838307, 0.18439424979735694, 0.18151950746835868, 0.1815195062934006, 0.18069815125308733, 0.18234086154789894, 0.18480492901875498, 0.1786447649251754, 0.18234086350616244, 0.18439424961988932, 0.1819301858697339, 0.18110883026030028, 0.1802874748099756, 0.18234086192119292, 0.18193018489060217, 0.18193018569226627, 0.1827515393434364, 0.18193018369728534, 0.181519507076706, 0.17905544078080807, 0.18193018528225485, 0.177823408709904, 0.18151950607921552, 0.18275153955762147, 0.18316221674732114, 0.1819301858697339, 0.1827515389517837, 0.18357289538124014, 0.1839835720018195, 0.18398357337260393, 0.18275154051839448, 0.18110882887115715, 0.1827515389517837, 0.1827515393434364, 0.18028747500580194, 0.18275154110587352, 0.1790554409766344, 0.18110882906698347, 0.18275154051839448, 0.18151950609757425, 0.18193018471313452, 0.18357289475704366, 0.18069815262387176, 0.18151950729089106, 0.18521560544350799, 0.18110882967282124, 0.18151950670341202, 0.18193018528225485, 0.18521560603098702, 0.18316221776317032, 0.1823408617253666, 0.1802874740266702, 0.1786447641602287, 0.17987679662278547, 0.17864476453352268, 0.1811088284611457, 0.17864476355439093, 0.18028747539745463, 0.17905544138664584, 0.18069815301552444, 0.1819301843031231, 0.17946611898635692, 0.17946611920054198, 0.17946611898635692, 0.17905544037079665, 0.1811088286753308, 0.18028747463250797, 0.17864476275272684, 0.18151950768254377, 0.17823408613214747, 0.17905544158247216, 0.18234086152954024, 0.18234086131535515, 0.1852156056209756, 0.18110883022358285, 0.18193018449894946, 0.17782340849571893, 0.17823408652380018, 0.1806981512347286, 0.1794661188088893, 0.18069815283805682, 0.181519507076706, 0.17618069786433077, 0.1819301850864285, 0.18028747424085528, 0.17823408730710558, 0.1839835728034836, 0.18234086133371388, 0.18357289475704366, 0.1802874744183229, 0.17864476433769633, 0.18316221754898526, 0.1811088286936895, 0.18069815125308733, 0.17741273167931323, 0.17864476355439093, 0.1786447645518814, 0.1757700208153813, 0.17905544232906012, 0.17577002042372858, 0.1848049287862112, 0.18275153994927415, 0.1827515389517837, 0.1839835733542452, 0.18685831709074532, 0.18480492882292862, 0.18562628284739274, 0.18644763949103424, 0.18357289438374969, 0.18480492764797055, 0.18357289516705508, 0.1860369618546057, 0.18644763886683777, 0.18480492723795913, 0.18562628423653588, 0.18644763788770602, 0.18562628225991368, 0.18562628445072096, 0.18521560464184386, 0.18480492743378546, 0.18891170441614774]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
