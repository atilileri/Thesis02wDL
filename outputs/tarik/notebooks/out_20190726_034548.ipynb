{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf18.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 03:45:48 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '1', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001BE3B745E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001BE5B5C7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0956, Accuracy:0.3729, Validation Loss:1.0860, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0812, Accuracy:0.3729, Validation Loss:1.0767, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0756, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0752, Accuracy:0.3943, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0723, Accuracy:0.3943, Validation Loss:1.0719, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0714, Accuracy:0.3955, Validation Loss:1.0708, Validation Accuracy:0.3990\n",
    "Epoch #21: Loss:1.0701, Accuracy:0.4086, Validation Loss:1.0689, Validation Accuracy:0.4417\n",
    "Epoch #22: Loss:1.0679, Accuracy:0.4329, Validation Loss:1.0657, Validation Accuracy:0.4499\n",
    "Epoch #23: Loss:1.0640, Accuracy:0.4460, Validation Loss:1.0600, Validation Accuracy:0.4565\n",
    "Epoch #24: Loss:1.0568, Accuracy:0.4517, Validation Loss:1.0498, Validation Accuracy:0.4631\n",
    "Epoch #25: Loss:1.0447, Accuracy:0.4665, Validation Loss:1.0320, Validation Accuracy:0.4680\n",
    "Epoch #26: Loss:1.0248, Accuracy:0.4645, Validation Loss:1.0067, Validation Accuracy:0.4565\n",
    "Epoch #27: Loss:0.9974, Accuracy:0.4678, Validation Loss:0.9797, Validation Accuracy:0.4614\n",
    "Epoch #28: Loss:0.9740, Accuracy:0.4682, Validation Loss:0.9656, Validation Accuracy:0.5140\n",
    "Epoch #29: Loss:0.9597, Accuracy:0.5125, Validation Loss:0.9513, Validation Accuracy:0.5123\n",
    "Epoch #30: Loss:0.9506, Accuracy:0.5253, Validation Loss:0.9523, Validation Accuracy:0.5156\n",
    "Epoch #31: Loss:0.9488, Accuracy:0.5133, Validation Loss:0.9388, Validation Accuracy:0.5222\n",
    "Epoch #32: Loss:0.9490, Accuracy:0.5158, Validation Loss:0.9474, Validation Accuracy:0.5107\n",
    "Epoch #33: Loss:0.9433, Accuracy:0.5170, Validation Loss:0.9348, Validation Accuracy:0.5271\n",
    "Epoch #34: Loss:0.9402, Accuracy:0.5240, Validation Loss:0.9304, Validation Accuracy:0.5189\n",
    "Epoch #35: Loss:0.9400, Accuracy:0.5285, Validation Loss:0.9348, Validation Accuracy:0.5172\n",
    "Epoch #36: Loss:0.9422, Accuracy:0.5199, Validation Loss:0.9350, Validation Accuracy:0.5205\n",
    "Epoch #37: Loss:0.9360, Accuracy:0.5351, Validation Loss:0.9318, Validation Accuracy:0.5156\n",
    "Epoch #38: Loss:0.9353, Accuracy:0.5285, Validation Loss:0.9280, Validation Accuracy:0.5255\n",
    "Epoch #39: Loss:0.9322, Accuracy:0.5294, Validation Loss:0.9234, Validation Accuracy:0.5402\n",
    "Epoch #40: Loss:0.9315, Accuracy:0.5285, Validation Loss:0.9212, Validation Accuracy:0.5419\n",
    "Epoch #41: Loss:0.9320, Accuracy:0.5211, Validation Loss:0.9230, Validation Accuracy:0.5353\n",
    "Epoch #42: Loss:0.9310, Accuracy:0.5273, Validation Loss:0.9196, Validation Accuracy:0.5369\n",
    "Epoch #43: Loss:0.9279, Accuracy:0.5306, Validation Loss:0.9203, Validation Accuracy:0.5402\n",
    "Epoch #44: Loss:0.9268, Accuracy:0.5343, Validation Loss:0.9193, Validation Accuracy:0.5369\n",
    "Epoch #45: Loss:0.9273, Accuracy:0.5306, Validation Loss:0.9198, Validation Accuracy:0.5337\n",
    "Epoch #46: Loss:0.9283, Accuracy:0.5298, Validation Loss:0.9172, Validation Accuracy:0.5402\n",
    "Epoch #47: Loss:0.9277, Accuracy:0.5290, Validation Loss:0.9164, Validation Accuracy:0.5419\n",
    "Epoch #48: Loss:0.9268, Accuracy:0.5335, Validation Loss:0.9213, Validation Accuracy:0.5402\n",
    "Epoch #49: Loss:0.9276, Accuracy:0.5232, Validation Loss:0.9158, Validation Accuracy:0.5402\n",
    "Epoch #50: Loss:0.9257, Accuracy:0.5343, Validation Loss:0.9165, Validation Accuracy:0.5337\n",
    "Epoch #51: Loss:0.9302, Accuracy:0.5240, Validation Loss:0.9364, Validation Accuracy:0.5337\n",
    "Epoch #52: Loss:0.9323, Accuracy:0.5351, Validation Loss:0.9284, Validation Accuracy:0.5337\n",
    "Epoch #53: Loss:0.9313, Accuracy:0.5273, Validation Loss:0.9310, Validation Accuracy:0.5320\n",
    "Epoch #54: Loss:0.9299, Accuracy:0.5368, Validation Loss:0.9246, Validation Accuracy:0.5320\n",
    "Epoch #55: Loss:0.9301, Accuracy:0.5162, Validation Loss:0.9256, Validation Accuracy:0.5287\n",
    "Epoch #56: Loss:0.9276, Accuracy:0.5376, Validation Loss:0.9152, Validation Accuracy:0.5369\n",
    "Epoch #57: Loss:0.9232, Accuracy:0.5363, Validation Loss:0.9158, Validation Accuracy:0.5337\n",
    "Epoch #58: Loss:0.9229, Accuracy:0.5368, Validation Loss:0.9143, Validation Accuracy:0.5337\n",
    "Epoch #59: Loss:0.9222, Accuracy:0.5326, Validation Loss:0.9145, Validation Accuracy:0.5337\n",
    "Epoch #60: Loss:0.9223, Accuracy:0.5306, Validation Loss:0.9152, Validation Accuracy:0.5402\n",
    "Epoch #61: Loss:0.9260, Accuracy:0.5347, Validation Loss:0.9153, Validation Accuracy:0.5304\n",
    "Epoch #62: Loss:0.9259, Accuracy:0.5306, Validation Loss:0.9187, Validation Accuracy:0.5468\n",
    "Epoch #63: Loss:0.9220, Accuracy:0.5396, Validation Loss:0.9135, Validation Accuracy:0.5337\n",
    "Epoch #64: Loss:0.9206, Accuracy:0.5355, Validation Loss:0.9217, Validation Accuracy:0.5402\n",
    "Epoch #65: Loss:0.9248, Accuracy:0.5363, Validation Loss:0.9152, Validation Accuracy:0.5320\n",
    "Epoch #66: Loss:0.9223, Accuracy:0.5343, Validation Loss:0.9249, Validation Accuracy:0.5435\n",
    "Epoch #67: Loss:0.9233, Accuracy:0.5433, Validation Loss:0.9169, Validation Accuracy:0.5271\n",
    "Epoch #68: Loss:0.9262, Accuracy:0.5351, Validation Loss:0.9190, Validation Accuracy:0.5452\n",
    "Epoch #69: Loss:0.9211, Accuracy:0.5326, Validation Loss:0.9122, Validation Accuracy:0.5271\n",
    "Epoch #70: Loss:0.9192, Accuracy:0.5405, Validation Loss:0.9133, Validation Accuracy:0.5369\n",
    "Epoch #71: Loss:0.9193, Accuracy:0.5396, Validation Loss:0.9122, Validation Accuracy:0.5304\n",
    "Epoch #72: Loss:0.9185, Accuracy:0.5417, Validation Loss:0.9126, Validation Accuracy:0.5304\n",
    "Epoch #73: Loss:0.9188, Accuracy:0.5368, Validation Loss:0.9152, Validation Accuracy:0.5419\n",
    "Epoch #74: Loss:0.9190, Accuracy:0.5421, Validation Loss:0.9106, Validation Accuracy:0.5386\n",
    "Epoch #75: Loss:0.9170, Accuracy:0.5396, Validation Loss:0.9116, Validation Accuracy:0.5369\n",
    "Epoch #76: Loss:0.9163, Accuracy:0.5405, Validation Loss:0.9103, Validation Accuracy:0.5369\n",
    "Epoch #77: Loss:0.9160, Accuracy:0.5425, Validation Loss:0.9155, Validation Accuracy:0.5452\n",
    "Epoch #78: Loss:0.9203, Accuracy:0.5425, Validation Loss:0.9150, Validation Accuracy:0.5468\n",
    "Epoch #79: Loss:0.9175, Accuracy:0.5470, Validation Loss:0.9135, Validation Accuracy:0.5337\n",
    "Epoch #80: Loss:0.9195, Accuracy:0.5384, Validation Loss:0.9255, Validation Accuracy:0.5287\n",
    "Epoch #81: Loss:0.9226, Accuracy:0.5302, Validation Loss:0.9329, Validation Accuracy:0.5369\n",
    "Epoch #82: Loss:0.9324, Accuracy:0.5368, Validation Loss:0.9317, Validation Accuracy:0.5369\n",
    "Epoch #83: Loss:0.9185, Accuracy:0.5437, Validation Loss:0.9230, Validation Accuracy:0.5353\n",
    "Epoch #84: Loss:0.9232, Accuracy:0.5359, Validation Loss:0.9218, Validation Accuracy:0.5337\n",
    "Epoch #85: Loss:0.9239, Accuracy:0.5372, Validation Loss:0.9106, Validation Accuracy:0.5320\n",
    "Epoch #86: Loss:0.9233, Accuracy:0.5437, Validation Loss:0.9114, Validation Accuracy:0.5419\n",
    "Epoch #87: Loss:0.9226, Accuracy:0.5335, Validation Loss:0.9123, Validation Accuracy:0.5402\n",
    "Epoch #88: Loss:0.9214, Accuracy:0.5400, Validation Loss:0.9108, Validation Accuracy:0.5353\n",
    "Epoch #89: Loss:0.9172, Accuracy:0.5437, Validation Loss:0.9114, Validation Accuracy:0.5369\n",
    "Epoch #90: Loss:0.9159, Accuracy:0.5417, Validation Loss:0.9110, Validation Accuracy:0.5353\n",
    "Epoch #91: Loss:0.9145, Accuracy:0.5433, Validation Loss:0.9106, Validation Accuracy:0.5386\n",
    "Epoch #92: Loss:0.9156, Accuracy:0.5437, Validation Loss:0.9095, Validation Accuracy:0.5255\n",
    "Epoch #93: Loss:0.9162, Accuracy:0.5446, Validation Loss:0.9148, Validation Accuracy:0.5468\n",
    "Epoch #94: Loss:0.9153, Accuracy:0.5396, Validation Loss:0.9092, Validation Accuracy:0.5271\n",
    "Epoch #95: Loss:0.9155, Accuracy:0.5425, Validation Loss:0.9128, Validation Accuracy:0.5435\n",
    "Epoch #96: Loss:0.9129, Accuracy:0.5388, Validation Loss:0.9086, Validation Accuracy:0.5337\n",
    "Epoch #97: Loss:0.9118, Accuracy:0.5433, Validation Loss:0.9092, Validation Accuracy:0.5435\n",
    "Epoch #98: Loss:0.9119, Accuracy:0.5417, Validation Loss:0.9093, Validation Accuracy:0.5452\n",
    "Epoch #99: Loss:0.9107, Accuracy:0.5446, Validation Loss:0.9086, Validation Accuracy:0.5402\n",
    "Epoch #100: Loss:0.9106, Accuracy:0.5454, Validation Loss:0.9076, Validation Accuracy:0.5287\n",
    "Epoch #101: Loss:0.9169, Accuracy:0.5413, Validation Loss:0.9143, Validation Accuracy:0.5484\n",
    "Epoch #102: Loss:0.9120, Accuracy:0.5446, Validation Loss:0.9070, Validation Accuracy:0.5369\n",
    "Epoch #103: Loss:0.9142, Accuracy:0.5429, Validation Loss:0.9077, Validation Accuracy:0.5271\n",
    "Epoch #104: Loss:0.9114, Accuracy:0.5454, Validation Loss:0.9155, Validation Accuracy:0.5435\n",
    "Epoch #105: Loss:0.9139, Accuracy:0.5425, Validation Loss:0.9066, Validation Accuracy:0.5320\n",
    "Epoch #106: Loss:0.9113, Accuracy:0.5400, Validation Loss:0.9068, Validation Accuracy:0.5353\n",
    "Epoch #107: Loss:0.9089, Accuracy:0.5454, Validation Loss:0.9108, Validation Accuracy:0.5468\n",
    "Epoch #108: Loss:0.9116, Accuracy:0.5421, Validation Loss:0.9074, Validation Accuracy:0.5287\n",
    "Epoch #109: Loss:0.9124, Accuracy:0.5437, Validation Loss:0.9149, Validation Accuracy:0.5435\n",
    "Epoch #110: Loss:0.9084, Accuracy:0.5478, Validation Loss:0.9060, Validation Accuracy:0.5320\n",
    "Epoch #111: Loss:0.9120, Accuracy:0.5421, Validation Loss:0.9055, Validation Accuracy:0.5337\n",
    "Epoch #112: Loss:0.9106, Accuracy:0.5396, Validation Loss:0.9153, Validation Accuracy:0.5353\n",
    "Epoch #113: Loss:0.9099, Accuracy:0.5413, Validation Loss:0.9071, Validation Accuracy:0.5320\n",
    "Epoch #114: Loss:0.9069, Accuracy:0.5409, Validation Loss:0.9098, Validation Accuracy:0.5452\n",
    "Epoch #115: Loss:0.9059, Accuracy:0.5425, Validation Loss:0.9064, Validation Accuracy:0.5320\n",
    "Epoch #116: Loss:0.9067, Accuracy:0.5487, Validation Loss:0.9088, Validation Accuracy:0.5435\n",
    "Epoch #117: Loss:0.9061, Accuracy:0.5474, Validation Loss:0.9045, Validation Accuracy:0.5402\n",
    "Epoch #118: Loss:0.9057, Accuracy:0.5437, Validation Loss:0.9049, Validation Accuracy:0.5452\n",
    "Epoch #119: Loss:0.9058, Accuracy:0.5483, Validation Loss:0.9045, Validation Accuracy:0.5287\n",
    "Epoch #120: Loss:0.9063, Accuracy:0.5433, Validation Loss:0.9041, Validation Accuracy:0.5304\n",
    "Epoch #121: Loss:0.9052, Accuracy:0.5462, Validation Loss:0.9038, Validation Accuracy:0.5402\n",
    "Epoch #122: Loss:0.9053, Accuracy:0.5441, Validation Loss:0.9144, Validation Accuracy:0.5353\n",
    "Epoch #123: Loss:0.9084, Accuracy:0.5450, Validation Loss:0.9077, Validation Accuracy:0.5419\n",
    "Epoch #124: Loss:0.9130, Accuracy:0.5499, Validation Loss:0.9177, Validation Accuracy:0.5353\n",
    "Epoch #125: Loss:0.9180, Accuracy:0.5351, Validation Loss:0.9119, Validation Accuracy:0.5369\n",
    "Epoch #126: Loss:0.9068, Accuracy:0.5446, Validation Loss:0.9037, Validation Accuracy:0.5320\n",
    "Epoch #127: Loss:0.9062, Accuracy:0.5499, Validation Loss:0.9043, Validation Accuracy:0.5386\n",
    "Epoch #128: Loss:0.9030, Accuracy:0.5437, Validation Loss:0.9086, Validation Accuracy:0.5386\n",
    "Epoch #129: Loss:0.9033, Accuracy:0.5474, Validation Loss:0.9046, Validation Accuracy:0.5304\n",
    "Epoch #130: Loss:0.9055, Accuracy:0.5425, Validation Loss:0.9132, Validation Accuracy:0.5353\n",
    "Epoch #131: Loss:0.9055, Accuracy:0.5417, Validation Loss:0.9028, Validation Accuracy:0.5452\n",
    "Epoch #132: Loss:0.9033, Accuracy:0.5507, Validation Loss:0.9026, Validation Accuracy:0.5353\n",
    "Epoch #133: Loss:0.9021, Accuracy:0.5437, Validation Loss:0.9085, Validation Accuracy:0.5320\n",
    "Epoch #134: Loss:0.9019, Accuracy:0.5495, Validation Loss:0.9073, Validation Accuracy:0.5337\n",
    "Epoch #135: Loss:0.9045, Accuracy:0.5405, Validation Loss:0.9111, Validation Accuracy:0.5369\n",
    "Epoch #136: Loss:0.9059, Accuracy:0.5454, Validation Loss:0.9057, Validation Accuracy:0.5369\n",
    "Epoch #137: Loss:0.9074, Accuracy:0.5474, Validation Loss:0.9027, Validation Accuracy:0.5271\n",
    "Epoch #138: Loss:0.9054, Accuracy:0.5400, Validation Loss:0.9049, Validation Accuracy:0.5353\n",
    "Epoch #139: Loss:0.9054, Accuracy:0.5409, Validation Loss:0.9076, Validation Accuracy:0.5369\n",
    "Epoch #140: Loss:0.9022, Accuracy:0.5470, Validation Loss:0.9034, Validation Accuracy:0.5369\n",
    "Epoch #141: Loss:0.9003, Accuracy:0.5446, Validation Loss:0.9023, Validation Accuracy:0.5337\n",
    "Epoch #142: Loss:0.9007, Accuracy:0.5478, Validation Loss:0.9025, Validation Accuracy:0.5337\n",
    "Epoch #143: Loss:0.9012, Accuracy:0.5458, Validation Loss:0.9071, Validation Accuracy:0.5353\n",
    "Epoch #144: Loss:0.9020, Accuracy:0.5450, Validation Loss:0.9032, Validation Accuracy:0.5287\n",
    "Epoch #145: Loss:0.9030, Accuracy:0.5515, Validation Loss:0.9104, Validation Accuracy:0.5353\n",
    "Epoch #146: Loss:0.9051, Accuracy:0.5478, Validation Loss:0.9189, Validation Accuracy:0.5337\n",
    "Epoch #147: Loss:0.9084, Accuracy:0.5441, Validation Loss:0.9036, Validation Accuracy:0.5337\n",
    "Epoch #148: Loss:0.9119, Accuracy:0.5441, Validation Loss:0.9017, Validation Accuracy:0.5369\n",
    "Epoch #149: Loss:0.9030, Accuracy:0.5437, Validation Loss:0.9095, Validation Accuracy:0.5402\n",
    "Epoch #150: Loss:0.9077, Accuracy:0.5483, Validation Loss:0.9058, Validation Accuracy:0.5304\n",
    "Epoch #151: Loss:0.9062, Accuracy:0.5454, Validation Loss:0.9108, Validation Accuracy:0.5337\n",
    "Epoch #152: Loss:0.9013, Accuracy:0.5446, Validation Loss:0.9046, Validation Accuracy:0.5353\n",
    "Epoch #153: Loss:0.9017, Accuracy:0.5483, Validation Loss:0.9126, Validation Accuracy:0.5320\n",
    "Epoch #154: Loss:0.9019, Accuracy:0.5450, Validation Loss:0.9020, Validation Accuracy:0.5386\n",
    "Epoch #155: Loss:0.9009, Accuracy:0.5520, Validation Loss:0.9017, Validation Accuracy:0.5386\n",
    "Epoch #156: Loss:0.8976, Accuracy:0.5425, Validation Loss:0.9012, Validation Accuracy:0.5337\n",
    "Epoch #157: Loss:0.8977, Accuracy:0.5462, Validation Loss:0.9025, Validation Accuracy:0.5304\n",
    "Epoch #158: Loss:0.8970, Accuracy:0.5450, Validation Loss:0.9023, Validation Accuracy:0.5304\n",
    "Epoch #159: Loss:0.8972, Accuracy:0.5429, Validation Loss:0.9027, Validation Accuracy:0.5353\n",
    "Epoch #160: Loss:0.8973, Accuracy:0.5474, Validation Loss:0.9060, Validation Accuracy:0.5353\n",
    "Epoch #161: Loss:0.8997, Accuracy:0.5409, Validation Loss:0.9007, Validation Accuracy:0.5419\n",
    "Epoch #162: Loss:0.8979, Accuracy:0.5483, Validation Loss:0.9020, Validation Accuracy:0.5419\n",
    "Epoch #163: Loss:0.8993, Accuracy:0.5433, Validation Loss:0.9258, Validation Accuracy:0.5271\n",
    "Epoch #164: Loss:0.9037, Accuracy:0.5450, Validation Loss:0.9154, Validation Accuracy:0.5255\n",
    "Epoch #165: Loss:0.9067, Accuracy:0.5446, Validation Loss:0.9082, Validation Accuracy:0.5353\n",
    "Epoch #166: Loss:0.9020, Accuracy:0.5454, Validation Loss:0.9003, Validation Accuracy:0.5386\n",
    "Epoch #167: Loss:0.9002, Accuracy:0.5454, Validation Loss:0.9004, Validation Accuracy:0.5419\n",
    "Epoch #168: Loss:0.8989, Accuracy:0.5458, Validation Loss:0.9078, Validation Accuracy:0.5337\n",
    "Epoch #169: Loss:0.8987, Accuracy:0.5478, Validation Loss:0.9010, Validation Accuracy:0.5419\n",
    "Epoch #170: Loss:0.8949, Accuracy:0.5474, Validation Loss:0.9043, Validation Accuracy:0.5304\n",
    "Epoch #171: Loss:0.8946, Accuracy:0.5470, Validation Loss:0.9032, Validation Accuracy:0.5337\n",
    "Epoch #172: Loss:0.8983, Accuracy:0.5483, Validation Loss:0.9097, Validation Accuracy:0.5337\n",
    "Epoch #173: Loss:0.8980, Accuracy:0.5450, Validation Loss:0.9001, Validation Accuracy:0.5419\n",
    "Epoch #174: Loss:0.8971, Accuracy:0.5474, Validation Loss:0.9020, Validation Accuracy:0.5353\n",
    "Epoch #175: Loss:0.8966, Accuracy:0.5499, Validation Loss:0.9101, Validation Accuracy:0.5304\n",
    "Epoch #176: Loss:0.8963, Accuracy:0.5520, Validation Loss:0.9114, Validation Accuracy:0.5271\n",
    "Epoch #177: Loss:0.9002, Accuracy:0.5417, Validation Loss:0.9126, Validation Accuracy:0.5386\n",
    "Epoch #178: Loss:0.8961, Accuracy:0.5577, Validation Loss:0.9033, Validation Accuracy:0.5337\n",
    "Epoch #179: Loss:0.8964, Accuracy:0.5515, Validation Loss:0.9018, Validation Accuracy:0.5287\n",
    "Epoch #180: Loss:0.8946, Accuracy:0.5441, Validation Loss:0.9013, Validation Accuracy:0.5287\n",
    "Epoch #181: Loss:0.8962, Accuracy:0.5487, Validation Loss:0.9030, Validation Accuracy:0.5386\n",
    "Epoch #182: Loss:0.8998, Accuracy:0.5483, Validation Loss:0.9022, Validation Accuracy:0.5320\n",
    "Epoch #183: Loss:0.8977, Accuracy:0.5483, Validation Loss:0.9045, Validation Accuracy:0.5320\n",
    "Epoch #184: Loss:0.8932, Accuracy:0.5487, Validation Loss:0.9009, Validation Accuracy:0.5369\n",
    "Epoch #185: Loss:0.8942, Accuracy:0.5532, Validation Loss:0.9147, Validation Accuracy:0.5304\n",
    "Epoch #186: Loss:0.8971, Accuracy:0.5503, Validation Loss:0.9069, Validation Accuracy:0.5369\n",
    "Epoch #187: Loss:0.8964, Accuracy:0.5528, Validation Loss:0.9100, Validation Accuracy:0.5337\n",
    "Epoch #188: Loss:0.8961, Accuracy:0.5524, Validation Loss:0.9001, Validation Accuracy:0.5337\n",
    "Epoch #189: Loss:0.8946, Accuracy:0.5478, Validation Loss:0.8996, Validation Accuracy:0.5386\n",
    "Epoch #190: Loss:0.8927, Accuracy:0.5520, Validation Loss:0.9032, Validation Accuracy:0.5353\n",
    "Epoch #191: Loss:0.8937, Accuracy:0.5474, Validation Loss:0.9010, Validation Accuracy:0.5402\n",
    "Epoch #192: Loss:0.8969, Accuracy:0.5491, Validation Loss:0.8995, Validation Accuracy:0.5369\n",
    "Epoch #193: Loss:0.9002, Accuracy:0.5470, Validation Loss:0.9217, Validation Accuracy:0.5353\n",
    "Epoch #194: Loss:0.9046, Accuracy:0.5417, Validation Loss:0.9175, Validation Accuracy:0.5287\n",
    "Epoch #195: Loss:0.9031, Accuracy:0.5429, Validation Loss:0.9273, Validation Accuracy:0.5238\n",
    "Epoch #196: Loss:0.9073, Accuracy:0.5405, Validation Loss:0.9045, Validation Accuracy:0.5271\n",
    "Epoch #197: Loss:0.8978, Accuracy:0.5487, Validation Loss:0.9013, Validation Accuracy:0.5320\n",
    "Epoch #198: Loss:0.8928, Accuracy:0.5499, Validation Loss:0.8996, Validation Accuracy:0.5419\n",
    "Epoch #199: Loss:0.8915, Accuracy:0.5511, Validation Loss:0.9041, Validation Accuracy:0.5304\n",
    "Epoch #200: Loss:0.8929, Accuracy:0.5573, Validation Loss:0.8999, Validation Accuracy:0.5320\n",
    "Epoch #201: Loss:0.8936, Accuracy:0.5540, Validation Loss:0.9070, Validation Accuracy:0.5369\n",
    "Epoch #202: Loss:0.8953, Accuracy:0.5491, Validation Loss:0.8999, Validation Accuracy:0.5369\n",
    "Epoch #203: Loss:0.8904, Accuracy:0.5524, Validation Loss:0.9033, Validation Accuracy:0.5287\n",
    "Epoch #204: Loss:0.8923, Accuracy:0.5495, Validation Loss:0.8989, Validation Accuracy:0.5353\n",
    "Epoch #205: Loss:0.8912, Accuracy:0.5520, Validation Loss:0.8989, Validation Accuracy:0.5386\n",
    "Epoch #206: Loss:0.8907, Accuracy:0.5524, Validation Loss:0.9030, Validation Accuracy:0.5271\n",
    "Epoch #207: Loss:0.8887, Accuracy:0.5540, Validation Loss:0.9084, Validation Accuracy:0.5386\n",
    "Epoch #208: Loss:0.8989, Accuracy:0.5413, Validation Loss:0.9097, Validation Accuracy:0.5337\n",
    "Epoch #209: Loss:0.8932, Accuracy:0.5478, Validation Loss:0.8993, Validation Accuracy:0.5304\n",
    "Epoch #210: Loss:0.8897, Accuracy:0.5462, Validation Loss:0.8987, Validation Accuracy:0.5386\n",
    "Epoch #211: Loss:0.8913, Accuracy:0.5495, Validation Loss:0.8984, Validation Accuracy:0.5386\n",
    "Epoch #212: Loss:0.8887, Accuracy:0.5536, Validation Loss:0.8996, Validation Accuracy:0.5386\n",
    "Epoch #213: Loss:0.8889, Accuracy:0.5515, Validation Loss:0.8989, Validation Accuracy:0.5386\n",
    "Epoch #214: Loss:0.8898, Accuracy:0.5544, Validation Loss:0.9001, Validation Accuracy:0.5402\n",
    "Epoch #215: Loss:0.8904, Accuracy:0.5524, Validation Loss:0.8996, Validation Accuracy:0.5386\n",
    "Epoch #216: Loss:0.8917, Accuracy:0.5515, Validation Loss:0.8992, Validation Accuracy:0.5369\n",
    "Epoch #217: Loss:0.8884, Accuracy:0.5536, Validation Loss:0.9031, Validation Accuracy:0.5255\n",
    "Epoch #218: Loss:0.8887, Accuracy:0.5503, Validation Loss:0.8988, Validation Accuracy:0.5386\n",
    "Epoch #219: Loss:0.8884, Accuracy:0.5515, Validation Loss:0.9055, Validation Accuracy:0.5452\n",
    "Epoch #220: Loss:0.8990, Accuracy:0.5450, Validation Loss:0.9015, Validation Accuracy:0.5271\n",
    "Epoch #221: Loss:0.8938, Accuracy:0.5466, Validation Loss:0.9090, Validation Accuracy:0.5255\n",
    "Epoch #222: Loss:0.8954, Accuracy:0.5536, Validation Loss:0.9101, Validation Accuracy:0.5402\n",
    "Epoch #223: Loss:0.8931, Accuracy:0.5540, Validation Loss:0.9120, Validation Accuracy:0.5320\n",
    "Epoch #224: Loss:0.8953, Accuracy:0.5499, Validation Loss:0.8985, Validation Accuracy:0.5337\n",
    "Epoch #225: Loss:0.8911, Accuracy:0.5503, Validation Loss:0.8974, Validation Accuracy:0.5337\n",
    "Epoch #226: Loss:0.8916, Accuracy:0.5503, Validation Loss:0.9085, Validation Accuracy:0.5320\n",
    "Epoch #227: Loss:0.8976, Accuracy:0.5429, Validation Loss:0.9015, Validation Accuracy:0.5419\n",
    "Epoch #228: Loss:0.8915, Accuracy:0.5515, Validation Loss:0.9049, Validation Accuracy:0.5304\n",
    "Epoch #229: Loss:0.8894, Accuracy:0.5499, Validation Loss:0.9033, Validation Accuracy:0.5386\n",
    "Epoch #230: Loss:0.8961, Accuracy:0.5581, Validation Loss:0.8989, Validation Accuracy:0.5255\n",
    "Epoch #231: Loss:0.8879, Accuracy:0.5544, Validation Loss:0.8976, Validation Accuracy:0.5320\n",
    "Epoch #232: Loss:0.8887, Accuracy:0.5503, Validation Loss:0.8976, Validation Accuracy:0.5353\n",
    "Epoch #233: Loss:0.8864, Accuracy:0.5507, Validation Loss:0.9015, Validation Accuracy:0.5304\n",
    "Epoch #234: Loss:0.8858, Accuracy:0.5556, Validation Loss:0.8971, Validation Accuracy:0.5353\n",
    "Epoch #235: Loss:0.8854, Accuracy:0.5503, Validation Loss:0.8984, Validation Accuracy:0.5287\n",
    "Epoch #236: Loss:0.8850, Accuracy:0.5548, Validation Loss:0.8969, Validation Accuracy:0.5353\n",
    "Epoch #237: Loss:0.8842, Accuracy:0.5532, Validation Loss:0.8968, Validation Accuracy:0.5337\n",
    "Epoch #238: Loss:0.8839, Accuracy:0.5511, Validation Loss:0.8973, Validation Accuracy:0.5304\n",
    "Epoch #239: Loss:0.8842, Accuracy:0.5495, Validation Loss:0.8978, Validation Accuracy:0.5304\n",
    "Epoch #240: Loss:0.8837, Accuracy:0.5524, Validation Loss:0.8982, Validation Accuracy:0.5353\n",
    "Epoch #241: Loss:0.8870, Accuracy:0.5561, Validation Loss:0.9005, Validation Accuracy:0.5287\n",
    "Epoch #242: Loss:0.8849, Accuracy:0.5515, Validation Loss:0.8983, Validation Accuracy:0.5304\n",
    "Epoch #243: Loss:0.8820, Accuracy:0.5495, Validation Loss:0.8961, Validation Accuracy:0.5369\n",
    "Epoch #244: Loss:0.8848, Accuracy:0.5503, Validation Loss:0.9006, Validation Accuracy:0.5337\n",
    "Epoch #245: Loss:0.8847, Accuracy:0.5524, Validation Loss:0.9012, Validation Accuracy:0.5287\n",
    "Epoch #246: Loss:0.8847, Accuracy:0.5524, Validation Loss:0.8984, Validation Accuracy:0.5402\n",
    "Epoch #247: Loss:0.8842, Accuracy:0.5552, Validation Loss:0.8950, Validation Accuracy:0.5369\n",
    "Epoch #248: Loss:0.8836, Accuracy:0.5540, Validation Loss:0.8981, Validation Accuracy:0.5419\n",
    "Epoch #249: Loss:0.8877, Accuracy:0.5441, Validation Loss:0.8950, Validation Accuracy:0.5320\n",
    "Epoch #250: Loss:0.8865, Accuracy:0.5478, Validation Loss:0.9109, Validation Accuracy:0.5304\n",
    "Epoch #251: Loss:0.8908, Accuracy:0.5487, Validation Loss:0.8955, Validation Accuracy:0.5386\n",
    "Epoch #252: Loss:0.8805, Accuracy:0.5520, Validation Loss:0.8948, Validation Accuracy:0.5271\n",
    "Epoch #253: Loss:0.8833, Accuracy:0.5528, Validation Loss:0.8944, Validation Accuracy:0.5304\n",
    "Epoch #254: Loss:0.8806, Accuracy:0.5544, Validation Loss:0.8932, Validation Accuracy:0.5304\n",
    "Epoch #255: Loss:0.8804, Accuracy:0.5532, Validation Loss:0.8937, Validation Accuracy:0.5238\n",
    "Epoch #256: Loss:0.8776, Accuracy:0.5532, Validation Loss:0.8927, Validation Accuracy:0.5337\n",
    "Epoch #257: Loss:0.8831, Accuracy:0.5581, Validation Loss:0.8919, Validation Accuracy:0.5320\n",
    "Epoch #258: Loss:0.8779, Accuracy:0.5569, Validation Loss:0.8934, Validation Accuracy:0.5238\n",
    "Epoch #259: Loss:0.8765, Accuracy:0.5565, Validation Loss:0.8941, Validation Accuracy:0.5468\n",
    "Epoch #260: Loss:0.8781, Accuracy:0.5524, Validation Loss:0.8966, Validation Accuracy:0.5337\n",
    "Epoch #261: Loss:0.8796, Accuracy:0.5544, Validation Loss:0.8931, Validation Accuracy:0.5271\n",
    "Epoch #262: Loss:0.8795, Accuracy:0.5585, Validation Loss:0.8919, Validation Accuracy:0.5369\n",
    "Epoch #263: Loss:0.8803, Accuracy:0.5507, Validation Loss:0.8902, Validation Accuracy:0.5337\n",
    "Epoch #264: Loss:0.8761, Accuracy:0.5515, Validation Loss:0.8901, Validation Accuracy:0.5353\n",
    "Epoch #265: Loss:0.8749, Accuracy:0.5532, Validation Loss:0.8883, Validation Accuracy:0.5287\n",
    "Epoch #266: Loss:0.8744, Accuracy:0.5536, Validation Loss:0.8882, Validation Accuracy:0.5337\n",
    "Epoch #267: Loss:0.8738, Accuracy:0.5524, Validation Loss:0.8878, Validation Accuracy:0.5304\n",
    "Epoch #268: Loss:0.8731, Accuracy:0.5544, Validation Loss:0.8920, Validation Accuracy:0.5337\n",
    "Epoch #269: Loss:0.8752, Accuracy:0.5544, Validation Loss:0.8870, Validation Accuracy:0.5271\n",
    "Epoch #270: Loss:0.8728, Accuracy:0.5528, Validation Loss:0.8865, Validation Accuracy:0.5287\n",
    "Epoch #271: Loss:0.8715, Accuracy:0.5540, Validation Loss:0.8857, Validation Accuracy:0.5369\n",
    "Epoch #272: Loss:0.8700, Accuracy:0.5561, Validation Loss:0.8881, Validation Accuracy:0.5304\n",
    "Epoch #273: Loss:0.8724, Accuracy:0.5544, Validation Loss:0.8882, Validation Accuracy:0.5271\n",
    "Epoch #274: Loss:0.8709, Accuracy:0.5515, Validation Loss:0.8856, Validation Accuracy:0.5337\n",
    "Epoch #275: Loss:0.8725, Accuracy:0.5536, Validation Loss:0.8866, Validation Accuracy:0.5287\n",
    "Epoch #276: Loss:0.8732, Accuracy:0.5458, Validation Loss:0.9057, Validation Accuracy:0.5287\n",
    "Epoch #277: Loss:0.8817, Accuracy:0.5499, Validation Loss:0.8862, Validation Accuracy:0.5468\n",
    "Epoch #278: Loss:0.8752, Accuracy:0.5593, Validation Loss:0.8860, Validation Accuracy:0.5419\n",
    "Epoch #279: Loss:0.8770, Accuracy:0.5507, Validation Loss:0.9093, Validation Accuracy:0.5222\n",
    "Epoch #280: Loss:0.8819, Accuracy:0.5515, Validation Loss:0.9044, Validation Accuracy:0.5320\n",
    "Epoch #281: Loss:0.8798, Accuracy:0.5466, Validation Loss:0.8930, Validation Accuracy:0.5402\n",
    "Epoch #282: Loss:0.8734, Accuracy:0.5593, Validation Loss:0.8908, Validation Accuracy:0.5419\n",
    "Epoch #283: Loss:0.8734, Accuracy:0.5569, Validation Loss:0.8907, Validation Accuracy:0.5402\n",
    "Epoch #284: Loss:0.8716, Accuracy:0.5528, Validation Loss:0.8843, Validation Accuracy:0.5386\n",
    "Epoch #285: Loss:0.8722, Accuracy:0.5536, Validation Loss:0.8864, Validation Accuracy:0.5353\n",
    "Epoch #286: Loss:0.8713, Accuracy:0.5540, Validation Loss:0.8837, Validation Accuracy:0.5386\n",
    "Epoch #287: Loss:0.8762, Accuracy:0.5598, Validation Loss:0.8850, Validation Accuracy:0.5435\n",
    "Epoch #288: Loss:0.8704, Accuracy:0.5528, Validation Loss:0.8834, Validation Accuracy:0.5287\n",
    "Epoch #289: Loss:0.8662, Accuracy:0.5593, Validation Loss:0.8849, Validation Accuracy:0.5320\n",
    "Epoch #290: Loss:0.8667, Accuracy:0.5598, Validation Loss:0.8826, Validation Accuracy:0.5369\n",
    "Epoch #291: Loss:0.8660, Accuracy:0.5581, Validation Loss:0.8861, Validation Accuracy:0.5271\n",
    "Epoch #292: Loss:0.8669, Accuracy:0.5540, Validation Loss:0.8929, Validation Accuracy:0.5238\n",
    "Epoch #293: Loss:0.8721, Accuracy:0.5565, Validation Loss:0.8855, Validation Accuracy:0.5337\n",
    "Epoch #294: Loss:0.8664, Accuracy:0.5618, Validation Loss:0.8820, Validation Accuracy:0.5304\n",
    "Epoch #295: Loss:0.8649, Accuracy:0.5565, Validation Loss:0.8820, Validation Accuracy:0.5287\n",
    "Epoch #296: Loss:0.8636, Accuracy:0.5598, Validation Loss:0.8869, Validation Accuracy:0.5369\n",
    "Epoch #297: Loss:0.8680, Accuracy:0.5634, Validation Loss:0.8820, Validation Accuracy:0.5353\n",
    "Epoch #298: Loss:0.8634, Accuracy:0.5598, Validation Loss:0.8824, Validation Accuracy:0.5205\n",
    "Epoch #299: Loss:0.8625, Accuracy:0.5610, Validation Loss:0.8817, Validation Accuracy:0.5402\n",
    "Epoch #300: Loss:0.8621, Accuracy:0.5647, Validation Loss:0.8838, Validation Accuracy:0.5468\n",
    "\n",
    "Test:\n",
    "Test Loss:0.88378084, Accuracy:0.5468\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "       02   01  03\n",
    "t:02  138   80   9\n",
    "t:01   83  117  40\n",
    "t:03   14   50  78\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.59      0.61      0.60       227\n",
    "          01       0.47      0.49      0.48       240\n",
    "          03       0.61      0.55      0.58       142\n",
    "\n",
    "    accuracy                           0.55       609\n",
    "   macro avg       0.56      0.55      0.55       609\n",
    "weighted avg       0.55      0.55      0.55       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 04:01:34 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 46 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0859562191861407, 1.0766613305300132, 1.074347856205281, 1.0752642403290973, 1.0751979482193494, 1.0746520007968146, 1.074358338988669, 1.074279868348283, 1.0742409025702766, 1.0742278065783246, 1.0741767489851402, 1.0740778716326935, 1.0739419912469799, 1.073830573625361, 1.0736442095736174, 1.073405927037958, 1.073106006252746, 1.0726286337293427, 1.0719246527635797, 1.070781389284995, 1.0689337750765295, 1.0656800307272298, 1.0599776475104596, 1.0498391808743155, 1.0320293396368794, 1.0066739910929074, 0.9796651164299162, 0.9656075509310943, 0.9512668272544598, 0.9523422870729944, 0.9388010924672846, 0.9473578774283085, 0.9347652324119029, 0.930424768642839, 0.9347627301912981, 0.934966392802879, 0.931760160010828, 0.9279664683420278, 0.923361794408319, 0.9211558455904129, 0.9229510203370908, 0.9196074533540822, 0.9202511997645711, 0.9193414667948517, 0.919808483280376, 0.9172109348824851, 0.9164153734842936, 0.9212904909169929, 0.915840081980663, 0.9164718192786419, 0.9364375492622112, 0.9284443052727209, 0.930984743612349, 0.9245558944828992, 0.9255860917673909, 0.9152348224556897, 0.9157941483315968, 0.9142608545683875, 0.9145260343606445, 0.9151992691170014, 0.9152854326714828, 0.9186827325077088, 0.913497464214444, 0.9217470807982195, 0.9151655520711627, 0.9248974186250534, 0.9168623229748706, 0.919016703792002, 0.9122135891702962, 0.9132889706904469, 0.9122459233688016, 0.9126428971541143, 0.9151791443769959, 0.9106193043915509, 0.9115865837372779, 0.9103202404842784, 0.9155105401338224, 0.9150267108320602, 0.9134932674210647, 0.9254743109391437, 0.9329209380549163, 0.9317270644584117, 0.922965588142915, 0.921761296946427, 0.9106387598761196, 0.9113889320926322, 0.9123011539722311, 0.9108247576871725, 0.9114196480592874, 0.9109952414564311, 0.9106265878050981, 0.9095045199926655, 0.9147768091098428, 0.9091728072252571, 0.9127607165494772, 0.9086274494874262, 0.9092033617993686, 0.9093143468224161, 0.9085593143315934, 0.90755665840578, 0.9143314691404208, 0.9069593951032666, 0.9076548086598589, 0.9155171702452285, 0.9065857208616823, 0.9067961693005805, 0.9107927862059306, 0.9073720827869985, 0.9149045684850471, 0.905972318696271, 0.9055085489511099, 0.9153384108261522, 0.9070776712718268, 0.9098263438699281, 0.9064381922211358, 0.9087570799982606, 0.9045251061763669, 0.9048789514500911, 0.9045268802220011, 0.9041191724133609, 0.9038469492899764, 0.9143863424096006, 0.9076978209962203, 0.9176914351327079, 0.9119408681670629, 0.9036718882950656, 0.9042619146736972, 0.9086419200075084, 0.9046251872685939, 0.9131745706833838, 0.9027948385388981, 0.9025532362496325, 0.9085321550690286, 0.9072656618746239, 0.9111202469795991, 0.9057014456327717, 0.9027377886725176, 0.9049160507903702, 0.9076341678356302, 0.903440237143161, 0.9022529708536583, 0.902527903884111, 0.907119943590587, 0.9031720561738477, 0.9103642197078085, 0.9189219864326941, 0.9035936824010902, 0.9017026411684471, 0.9095094572930109, 0.9058142720380636, 0.9108009520422649, 0.9046437402664147, 0.9125885971269779, 0.9019702513425417, 0.9017162144869223, 0.9011615079220489, 0.9024769618789159, 0.9022803609985828, 0.9027383873615359, 0.9060202142092199, 0.9006931421596233, 0.9020261349544932, 0.9257577620507853, 0.9154110746626392, 0.9081955220311734, 0.9002992310155984, 0.9004250781485208, 0.9077606661174881, 0.9009758494366175, 0.9043396568454937, 0.9031677493712389, 0.9097374811744063, 0.9000771600624611, 0.9020452643262928, 0.9101318357809033, 0.9114197561110573, 0.9125899777232328, 0.9033480263108691, 0.9017849377614916, 0.9013476803384978, 0.9030322828903574, 0.9022296227649319, 0.9044692804073465, 0.9009122454082633, 0.9147262446007314, 0.9069185788408288, 0.9099516481014308, 0.9001088011245226, 0.8995631029844675, 0.9031963535326063, 0.9009765576455002, 0.8994755098972415, 0.9217109369918435, 0.9175010150289301, 0.927337168668487, 0.9044707328423687, 0.9012940968394475, 0.8995951492406661, 0.90412856283642, 0.8998810725259077, 0.907039434158156, 0.8999251708608543, 0.9033025528605544, 0.8989443367925184, 0.8988618440620222, 0.9030058815561491, 0.9083583904996099, 0.9096553891358901, 0.8993464540965451, 0.898712080589852, 0.8984203857349841, 0.8995971298961608, 0.898904610541458, 0.9000647941050662, 0.8995522157899265, 0.8992193190530798, 0.9030883217288551, 0.8987803738105473, 0.9054829021197038, 0.9015008326625981, 0.9090439268129408, 0.910069743694343, 0.9120135506972891, 0.8984532872052812, 0.8973864947242298, 0.908467456429267, 0.901526382208262, 0.904948227119759, 0.9033237957993556, 0.89894511361036, 0.8975838190816307, 0.8976026485706198, 0.901524207372775, 0.8970976838924615, 0.8983660406862769, 0.8968893686930338, 0.8967918798645533, 0.8972500116367058, 0.8978195603453663, 0.8981868803794748, 0.9005109928120142, 0.898293429896945, 0.8961164990473655, 0.9005915304318634, 0.9011981638977289, 0.8984492878217024, 0.8949568037720541, 0.8980654730585408, 0.8950273729310247, 0.9109415647823039, 0.8954742755404442, 0.8947979918450166, 0.8943570766151441, 0.8932206476067479, 0.8936729905836296, 0.8926737839952478, 0.8919396886880371, 0.8933804139910856, 0.8941340838942817, 0.8966283318640171, 0.8931303436337238, 0.8919244527033788, 0.8901829684309184, 0.8900687325568426, 0.8882961636302115, 0.888168912998757, 0.887796202120914, 0.8919703016923175, 0.8869857647148847, 0.886450703214542, 0.8856707575481709, 0.8881386307072757, 0.8881918854509864, 0.8855852181296826, 0.8865637355445837, 0.9057349368856458, 0.8861733314830486, 0.885988658676398, 0.9093138618618005, 0.904384995231096, 0.8929790851517851, 0.8907630009995697, 0.8907122526850019, 0.8843009928960127, 0.8863776175455115, 0.8837270581076297, 0.8849604804136092, 0.8833762962242653, 0.8849423020735554, 0.8826007418248846, 0.886058884496955, 0.8929392042614165, 0.8855283555921858, 0.8819861240770624, 0.8820100241693957, 0.8869495703081779, 0.8820013166061176, 0.8824485204685694, 0.8816575717260489, 0.8837808957828089], 'val_acc': [0.372742199007122, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.3990147774932028, 0.4417077173251041, 0.4499178980469508, 0.45648603809290916, 0.4630541827878341, 0.46798029116221834, 0.4564860383865281, 0.46141214685878534, 0.5139573036352011, 0.5123152667274224, 0.5155993391727579, 0.5221674836719369, 0.5106732304068817, 0.5270935919484482, 0.5188834112266014, 0.5172413752975527, 0.5205254473513962, 0.5155993391727579, 0.5254515562151453, 0.5402298813382981, 0.5418719176588387, 0.5353037729639137, 0.5369458090887085, 0.540229881436171, 0.5369458091865815, 0.533661736839119, 0.540229881436171, 0.5418719177567117, 0.540229881436171, 0.540229881436171, 0.533661736839119, 0.533661736839119, 0.5336617366433731, 0.5320197006164513, 0.5320197005185784, 0.5287356283668618, 0.5369458091865815, 0.533661736839119, 0.533661736936992, 0.533661736936992, 0.540229881436171, 0.5303776645895295, 0.54679802593535, 0.533661736936992, 0.5402298813382981, 0.5320197007143243, 0.5435139535878876, 0.5270935922420671, 0.5451559898105552, 0.52709359233994, 0.5369458091865815, 0.5303776645895295, 0.5303776644916566, 0.5418719176588387, 0.5385878453113763, 0.5369458091865815, 0.5369458090887085, 0.5451559897126823, 0.5467980260332229, 0.5336617367412461, 0.5287356282689889, 0.5369458091865815, 0.5369458091865815, 0.5353037728660408, 0.5336617365455001, 0.5320197007143243, 0.5418719175609658, 0.540229881436171, 0.5353037729639137, 0.5369458089908355, 0.5353037730617868, 0.5385878452135033, 0.5254515561172723, 0.546798025837477, 0.5270935922420671, 0.5435139536857605, 0.533661736839119, 0.5435139535878876, 0.5451559897126823, 0.5402298813382981, 0.5287356283668618, 0.5484400620601447, 0.5369458091865815, 0.5270935922420671, 0.5435139535878876, 0.5320197007143243, 0.5353037728660408, 0.5467980260332229, 0.5287356283668618, 0.5435139535878876, 0.5320197006164513, 0.533661736839119, 0.5353037727681679, 0.5320197006164513, 0.5451559897126823, 0.5320197006164513, 0.5435139538815065, 0.5402298813382981, 0.5451559898105552, 0.5287356283668618, 0.5303776643937836, 0.5402298813382981, 0.5353037726702948, 0.5418719174630928, 0.5353037727681679, 0.5369458087950896, 0.5320197004207053, 0.5385878452135033, 0.5385878451156303, 0.5303776644916566, 0.5353037727681679, 0.5451559897126823, 0.5353037728660408, 0.5320197004207053, 0.5336617367412461, 0.5369458087950896, 0.5369458089908355, 0.5270935921441942, 0.5353037728660408, 0.5369458088929626, 0.5369458089908355, 0.5336617366433731, 0.533661736839119, 0.5353037726702948, 0.5287356281711159, 0.5353037727681679, 0.5336617367412461, 0.5336617366433731, 0.5369458089908355, 0.5402298811425521, 0.5303776642959106, 0.5336617366433731, 0.5353037726702948, 0.5320197005185784, 0.5385878451156303, 0.5385878451156303, 0.5336617365455001, 0.5303776643937836, 0.5303776642959106, 0.5353037729639137, 0.5353037727681679, 0.5418719172673468, 0.5418719171694738, 0.5270935922420671, 0.5254515559215264, 0.5353037727681679, 0.5385878450177574, 0.5418719172673468, 0.5336617366433731, 0.5418719173652198, 0.5303776642959106, 0.5336617364476272, 0.5336617367412461, 0.5418719172673468, 0.5353037725724219, 0.5303776643937836, 0.5270935919484482, 0.5385878450177574, 0.5336617365455001, 0.5287356283668618, 0.5287356281711159, 0.5385878449198843, 0.5320197006164513, 0.5320197005185784, 0.5369458086972166, 0.5303776644916566, 0.5369458087950896, 0.5336617367412461, 0.5336617364476272, 0.5385878450177574, 0.5353037727681679, 0.5402298809468061, 0.5369458088929626, 0.5353037727681679, 0.5287356280732429, 0.5238095197967316, 0.5270935919484482, 0.5320197005185784, 0.5418719171694738, 0.5303776643937836, 0.5320197002249594, 0.5369458088929626, 0.5369458085014707, 0.5287356281711159, 0.5353037727681679, 0.5385878449198843, 0.5270935920463211, 0.5385878447241383, 0.5336617367412461, 0.5303776641001646, 0.5385878450177574, 0.5385878449198843, 0.5385878450177574, 0.5385878450177574, 0.5402298808489331, 0.5385878450177574, 0.5369458088929626, 0.5254515558236534, 0.5385878448220114, 0.5451559892233173, 0.5270935919484482, 0.5254515560193994, 0.5402298810446791, 0.5320197002249594, 0.5336617363497541, 0.5336617365455001, 0.5320197004207053, 0.5418719170716009, 0.5303776642959106, 0.5385878447241383, 0.5254515558236534, 0.5320197003228324, 0.5353037725724219, 0.5303776641001646, 0.5353037725724219, 0.5287356281711159, 0.5353037726702948, 0.5336617365455001, 0.5303776641980377, 0.5303776642959106, 0.5353037723766759, 0.5287356281711159, 0.5303776641001646, 0.5369458087950896, 0.5336617365455001, 0.5287356279753699, 0.5402298808489331, 0.5369458086972166, 0.5418719169737278, 0.5320197002249594, 0.5303776640022917, 0.5385878447241383, 0.5270935919484482, 0.5303776641980377, 0.5303776641980377, 0.5238095194052397, 0.5336617364476272, 0.5320197003228324, 0.5238095196009857, 0.5467980253481121, 0.5336617363497541, 0.5270935918505751, 0.5369458085993436, 0.5336617361540081, 0.5353037725724219, 0.5287356281711159, 0.5336617365455001, 0.5303776640022917, 0.5336617367412461, 0.5270935919484482, 0.5287356279753699, 0.5369458087950896, 0.5303776642959106, 0.5270935916548292, 0.5336617365455001, 0.5287356280732429, 0.5287356282689889, 0.546798025543858, 0.5418719172673468, 0.5221674836719369, 0.5320197004207053, 0.5402298811425521, 0.5418719173652198, 0.540229881240425, 0.5385878449198843, 0.5353037725724219, 0.5385878449198843, 0.5435139532942685, 0.5287356281711159, 0.5320197004207053, 0.5369458087950896, 0.5270935918505751, 0.5238095196009857, 0.5336617365455001, 0.5303776642959106, 0.5287356280732429, 0.5369458088929626, 0.5353037727681679, 0.5205254473513962, 0.5402298811425521, 0.546798025739604], 'loss': [1.095639753684371, 1.08116535264119, 1.0756383916925356, 1.0747025385529598, 1.0751566545674442, 1.0749500886615542, 1.0744899942644812, 1.0741346847350104, 1.0741023229132933, 1.0741147964642033, 1.0741066900366876, 1.0740511733648468, 1.0739001704437288, 1.0738388462967452, 1.073705611630387, 1.0734662857388568, 1.0732929629222079, 1.072858225736285, 1.0723338300932115, 1.071441591838547, 1.0701421727390015, 1.067893061843496, 1.0640379522860173, 1.056764340841305, 1.0447112703715018, 1.0248047334702353, 0.9974025384601382, 0.9739720393744827, 0.9597364198745398, 0.9505514977649008, 0.9488144104975205, 0.9489866329414399, 0.943294766698285, 0.9401913382434257, 0.939982614012959, 0.9422436440749824, 0.9359640460484326, 0.9352549194555263, 0.9321894737729302, 0.9314917316671759, 0.9320153297584894, 0.930977675361555, 0.9278741708526377, 0.926812208359736, 0.9273270158307508, 0.9283388994312873, 0.9277432520041965, 0.9267669823869787, 0.9275595553112226, 0.9257305298008224, 0.9301995522432503, 0.9323147976178163, 0.9313072913726008, 0.9298788430999192, 0.930136274557094, 0.92764488005785, 0.9232247774850662, 0.9228987017451371, 0.9222387126828611, 0.9223070117727198, 0.92600450860891, 0.9259098914369666, 0.9219811049330161, 0.9205604854305667, 0.9247703582599178, 0.9223050444277895, 0.9233243614741174, 0.9262054334920534, 0.9210848234027808, 0.9192215000334706, 0.9192536881082601, 0.9185473340982284, 0.9188109445620856, 0.9189961790304164, 0.917036629261667, 0.9162771974011368, 0.9159720509693607, 0.9202551763894866, 0.9174744888497574, 0.9194522578858252, 0.922583789330978, 0.9324066601249961, 0.918498817900123, 0.9231614788454906, 0.9239124934286552, 0.9232585147665756, 0.9225926584286857, 0.9214125403878135, 0.9172221425622396, 0.915911250486511, 0.9145496997255564, 0.9155939918279158, 0.9162438660186909, 0.915291904717745, 0.9155483259802236, 0.9128715032914336, 0.9118463977651184, 0.9118522094015713, 0.910690614868728, 0.9105533712943232, 0.916898980155373, 0.9119534308415909, 0.9142343340223574, 0.9113990749421795, 0.9138754627787846, 0.9113264169536333, 0.9088963434436728, 0.9116118464871353, 0.9124325129040948, 0.9084329633742142, 0.9120317081406376, 0.91055832736546, 0.909899448956797, 0.9069270543249236, 0.9058707025751196, 0.9067174893629869, 0.9061196233702391, 0.9057207756218724, 0.9057535954079834, 0.9062696549926695, 0.9052072916677111, 0.9052782585243915, 0.9084320629401863, 0.9129752489086049, 0.9180161343462903, 0.9068140439429078, 0.9062129735701873, 0.9029519802735816, 0.9033285225686106, 0.9054795521975053, 0.9054594271971215, 0.903287791153244, 0.9020957227115514, 0.9019176449374252, 0.9045307645807521, 0.9058834232833596, 0.9074190931888086, 0.9053507411259645, 0.9054467963731754, 0.902209886510759, 0.9003248806362034, 0.900730926373656, 0.9012187768546462, 0.9019715107441928, 0.9029532454097051, 0.9051099232579648, 0.9084112590098528, 0.9119365560445453, 0.9029562469135809, 0.9076685859926917, 0.906239644800613, 0.901272869575195, 0.9017351942385491, 0.9018766356444702, 0.9009055093083783, 0.8975990470674738, 0.8977345014010122, 0.8970249173577561, 0.8971860662867646, 0.8972582020064399, 0.8996872398153223, 0.8978666674185093, 0.8993237407055723, 0.9037187092113299, 0.9067178750674583, 0.9019662378260243, 0.9001596855431856, 0.8989385431307297, 0.8987006242759908, 0.8948705047797374, 0.8946160874327594, 0.8982829950428597, 0.8980448781832043, 0.897069832431707, 0.8965954528207407, 0.8962531772482322, 0.9001767418957344, 0.8961067373992482, 0.8963851864577809, 0.8945845977971196, 0.8961722467959049, 0.8998088268773512, 0.8976797632368193, 0.8931552975574313, 0.8941637757867269, 0.8971272483498653, 0.896368241922077, 0.8961295984608927, 0.8945967235359568, 0.892736438902007, 0.8937437915704088, 0.8968512619790111, 0.9002426395181268, 0.9046061015227003, 0.9031437972243072, 0.907304132596668, 0.897840855400665, 0.8927758860392248, 0.8914937427156515, 0.8929014503099101, 0.893589840189877, 0.8953328528443402, 0.8904317280595063, 0.8923099331297669, 0.8911650132839195, 0.8907295665212236, 0.8887280091612736, 0.8988963787805373, 0.893237003237315, 0.889729738186517, 0.8913015884785191, 0.8887114401470709, 0.8889156283049613, 0.8898455058279958, 0.8904380201314265, 0.8917309560570139, 0.8884074678656012, 0.8887328815166465, 0.8884279828541577, 0.8989519771119653, 0.8938124521801849, 0.895430900012688, 0.8930777191136652, 0.8953025195632872, 0.8911438515788476, 0.8916345406117135, 0.897597522314569, 0.8915032280788774, 0.8893818091562886, 0.8961251101209887, 0.8878767053449423, 0.888692068564084, 0.8864203307173335, 0.8858430136155788, 0.8853870643727344, 0.8850316863040415, 0.8842333172136264, 0.8838873697012601, 0.8841551618654381, 0.8836750243723515, 0.8870321467427013, 0.8848554514272012, 0.8820049173778087, 0.8847824318453027, 0.8847396481208488, 0.8846869278737407, 0.8841572664112036, 0.8836466254639674, 0.8876889816299846, 0.8865290176207524, 0.8908225925061737, 0.8805302630459748, 0.8833484673646931, 0.8806143610139647, 0.8804146919896715, 0.8776431972975604, 0.8830815537264705, 0.8778543378538175, 0.8765260960042355, 0.8780971132264734, 0.8795786893098506, 0.8795265619514904, 0.8802563055095242, 0.8761196340623577, 0.8748559095531518, 0.8743770561423879, 0.8738138912884361, 0.8730863171926024, 0.875210825912272, 0.872762549633363, 0.8714983410903805, 0.8700196311948726, 0.8724190254965357, 0.8708546616703088, 0.8724974493960825, 0.8731774261110372, 0.8817064985602299, 0.8752236235313102, 0.8770111478084901, 0.8819498530648328, 0.8798400994443796, 0.8733606329933574, 0.8733559370040893, 0.8715872479904848, 0.872168950574354, 0.8713193449641156, 0.8762205195377984, 0.8703773406741555, 0.8661855071477087, 0.8667492334358011, 0.8659811477151984, 0.8668788986529168, 0.8720839571169514, 0.8663981442089198, 0.8649191711962345, 0.8636368563532585, 0.8679520334306439, 0.8634262270995968, 0.8625056667004768, 0.8620537369158233], 'acc': [0.37289527581212945, 0.3728952769870876, 0.3942505148891551, 0.3942505113275634, 0.39425051449750237, 0.39425051508498143, 0.3942505125392389, 0.39425051171921605, 0.39425051449750237, 0.3942505113275634, 0.3942505129308916, 0.39425051547663414, 0.3942505119150424, 0.3942505113275634, 0.3942505119150424, 0.3942505138733059, 0.39425051250252147, 0.39425051547663414, 0.3942505146933287, 0.3954825459809274, 0.4086242299182704, 0.4328542086010841, 0.44599589316262356, 0.45174537899557815, 0.466529774457767, 0.46447638832568144, 0.46776180789945554, 0.46817248332671807, 0.5125256699947851, 0.5252566767424284, 0.5133470246434456, 0.5158110878061221, 0.517043122618595, 0.5240246365692092, 0.5285420897804981, 0.5199178649659519, 0.5351129328445732, 0.5285420974911605, 0.5293634527273002, 0.5285420965120288, 0.5211498995825985, 0.5273100620912086, 0.5305954847982043, 0.5342915804234374, 0.5305954816649827, 0.5297741285829328, 0.5289527739342723, 0.5334702273413876, 0.5232032874771212, 0.5342915855149224, 0.524024644475698, 0.5351129397719303, 0.5273100642452984, 0.5367556490692514, 0.516221767774108, 0.5375770050886964, 0.5363449728219661, 0.5367556514191677, 0.5326488728885533, 0.5305954865606414, 0.5347022625455132, 0.5305954849940306, 0.5396303856642094, 0.5355236160192157, 0.536344971647008, 0.5342915833608326, 0.5433264893917578, 0.5351129380094931, 0.5326488728885533, 0.5404517482193588, 0.5396303929832192, 0.541683781465221, 0.5367556496567305, 0.5420944557542429, 0.5396303920040875, 0.5404517486110115, 0.5425051287948718, 0.5425051365055342, 0.5470225887376915, 0.538398358562399, 0.5301848077676135, 0.5367556502442096, 0.5437371675973066, 0.535934292266501, 0.5371663266873212, 0.543737162628213, 0.5334702275372138, 0.5400410672722411, 0.5437371707305282, 0.5416837828360054, 0.5433264921333266, 0.5437371624323867, 0.5445585247917097, 0.5396303923957402, 0.5425051295781772, 0.5388090371596005, 0.5433264873600594, 0.5416837798986102, 0.5445585257708414, 0.5453798736879713, 0.5412731048262829, 0.5445585240084043, 0.5429158077837261, 0.5453798790487175, 0.5425051355264026, 0.540041070601289, 0.5453798796361966, 0.5420944588874644, 0.5437371699472228, 0.5478439459320945, 0.5420944517642811, 0.53963039454983, 0.5412731050221092, 0.5408624252499497, 0.5425051368971869, 0.5486653011682342, 0.5474332677265457, 0.5437371640724323, 0.5482546237459908, 0.5433264903708894, 0.546201236438947, 0.544147844432071, 0.5449692008431687, 0.5498973267034338, 0.5351129368345351, 0.5445585242042307, 0.5498973351974017, 0.543737163803171, 0.5474332687056774, 0.5425051297740036, 0.5416837826401791, 0.5507186811562681, 0.5437371675973066, 0.5494866566002001, 0.540451749394317, 0.5453798776779332, 0.547433267922372, 0.5400410715804208, 0.540862426033255, 0.5470225841602505, 0.5445585224417935, 0.5478439459320945, 0.5457905560793084, 0.5449692018223005, 0.5515400444947229, 0.5478439378297794, 0.544147844432071, 0.5441478402462828, 0.5437371703388755, 0.5482546187768972, 0.5453798734921449, 0.5445585220501408, 0.5482546217877272, 0.5449692024097795, 0.5519507199587029, 0.5425051372888396, 0.5462012362431207, 0.5449691945032907, 0.5429158143194305, 0.5474332683140247, 0.5408624252499497, 0.5482546225710326, 0.5433264899792367, 0.5449691958740751, 0.544558524400057, 0.5453798727088396, 0.5453798784612386, 0.5457905590167036, 0.5478439463237472, 0.5474332689015038, 0.5470225889335179, 0.54825461701446, 0.544969197440686, 0.5474332673348931, 0.549897334805749, 0.5519507155770883, 0.5416837798986102, 0.5577002041883292, 0.551540045082202, 0.5441478465861609, 0.5486653013640606, 0.548254623354338, 0.54825461701446, 0.5486653023431923, 0.5531827547711758, 0.550308003929851, 0.552772070813228, 0.5523613931951582, 0.547843943386352, 0.5519507230919245, 0.5474332683140247, 0.5490759714672943, 0.5470225889335179, 0.5416837808777419, 0.5429158115778615, 0.5404517488068379, 0.5486653011682342, 0.5498973338266173, 0.5511293645267369, 0.5572895323226584, 0.5540041038632638, 0.5490759799612621, 0.5523613997308625, 0.5494866540544576, 0.5519507228960981, 0.5523613967934673, 0.5540041021008266, 0.5412731034554985, 0.5478439378297794, 0.5462012285324582, 0.5494866546419367, 0.5535934333683774, 0.5515400433197648, 0.5544147888003433, 0.5523613935868108, 0.5515400373715394, 0.5535934318017666, 0.5503080120321662, 0.5515400433197648, 0.5449692020181268, 0.5466119138611906, 0.5535934321934193, 0.5540041024924793, 0.5498973267034338, 0.5503080041256774, 0.5503080110530344, 0.5429158145152568, 0.5515400429281121, 0.5498973259201285, 0.55811088638384, 0.5544147876253852, 0.5503080094864237, 0.550718690237715, 0.5556468133564113, 0.5503080098780764, 0.5548254594910561, 0.5531827555544812, 0.5511293591659906, 0.5494866546419367, 0.5523613926076791, 0.5560574977060119, 0.5515400369798867, 0.5494866528794996, 0.5503080112488608, 0.5523614005141679, 0.5523613918243737, 0.5552361418823931, 0.5540041064824411, 0.5441478458028555, 0.5478439443654838, 0.5486653015598869, 0.5519507173395255, 0.5527720710090543, 0.5544147797188964, 0.5531827549670022, 0.5531827541836968, 0.5581108856005346, 0.5568788525504987, 0.5564681685925509, 0.5523613993392098, 0.5544147882128643, 0.5585215566829, 0.5507186811562681, 0.5515400373715394, 0.5531827484312978, 0.5535934260493676, 0.5523613991433834, 0.5544147799147228, 0.5544147880170379, 0.552772077153106, 0.5540041105947945, 0.5560574911703074, 0.554414782264639, 0.5515400442988966, 0.5535934254618886, 0.5457905539252185, 0.5498973344140964, 0.5593429142689558, 0.550718690237715, 0.5515400452780282, 0.546611911902927, 0.5593429155173488, 0.5568788505922353, 0.5527720700299226, 0.5535934272243257, 0.5540041105947945, 0.5597535968561193, 0.5527720744115371, 0.5593429200213548, 0.5597535899287621, 0.5581108810230937, 0.5540041102031418, 0.5564681685925509, 0.5618069839673365, 0.556468175519908, 0.5597535887538041, 0.563449696202053, 0.5597535954853348, 0.560985628731197, 0.5646817284687833]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
