{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf73.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 23:38:13 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'AllShfRnd', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['mb', 'yd', 'eg', 'eo', 'eb', 'ck', 'sk', 'ib', 'by', 'ds', 'sg', 'my', 'aa', 'ek', 'ce'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000014604C7D278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000014660E46EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7098, Accuracy:0.0842, Validation Loss:2.6987, Validation Accuracy:0.1034\n",
    "Epoch #2: Loss:2.6940, Accuracy:0.1018, Validation Loss:2.6883, Validation Accuracy:0.0788\n",
    "Epoch #3: Loss:2.6842, Accuracy:0.0797, Validation Loss:2.6792, Validation Accuracy:0.0936\n",
    "Epoch #4: Loss:2.6749, Accuracy:0.1162, Validation Loss:2.6706, Validation Accuracy:0.1232\n",
    "Epoch #5: Loss:2.6654, Accuracy:0.1240, Validation Loss:2.6591, Validation Accuracy:0.1232\n",
    "Epoch #6: Loss:2.6532, Accuracy:0.1199, Validation Loss:2.6484, Validation Accuracy:0.1199\n",
    "Epoch #7: Loss:2.6397, Accuracy:0.1265, Validation Loss:2.6375, Validation Accuracy:0.0985\n",
    "Epoch #8: Loss:2.6273, Accuracy:0.1294, Validation Loss:2.6256, Validation Accuracy:0.1199\n",
    "Epoch #9: Loss:2.6145, Accuracy:0.1429, Validation Loss:2.6057, Validation Accuracy:0.1314\n",
    "Epoch #10: Loss:2.5975, Accuracy:0.1388, Validation Loss:2.5820, Validation Accuracy:0.1691\n",
    "Epoch #11: Loss:2.5690, Accuracy:0.1622, Validation Loss:2.5625, Validation Accuracy:0.1609\n",
    "Epoch #12: Loss:2.5460, Accuracy:0.1671, Validation Loss:2.5402, Validation Accuracy:0.1478\n",
    "Epoch #13: Loss:2.5323, Accuracy:0.1593, Validation Loss:2.5273, Validation Accuracy:0.1527\n",
    "Epoch #14: Loss:2.5400, Accuracy:0.1655, Validation Loss:2.5401, Validation Accuracy:0.1790\n",
    "Epoch #15: Loss:2.5287, Accuracy:0.1651, Validation Loss:2.5329, Validation Accuracy:0.1593\n",
    "Epoch #16: Loss:2.5215, Accuracy:0.1626, Validation Loss:2.5168, Validation Accuracy:0.1626\n",
    "Epoch #17: Loss:2.5101, Accuracy:0.1639, Validation Loss:2.5111, Validation Accuracy:0.1527\n",
    "Epoch #18: Loss:2.5042, Accuracy:0.1630, Validation Loss:2.5034, Validation Accuracy:0.1626\n",
    "Epoch #19: Loss:2.4964, Accuracy:0.1634, Validation Loss:2.4941, Validation Accuracy:0.1790\n",
    "Epoch #20: Loss:2.4849, Accuracy:0.1737, Validation Loss:2.4903, Validation Accuracy:0.1691\n",
    "Epoch #21: Loss:2.4827, Accuracy:0.1639, Validation Loss:2.4815, Validation Accuracy:0.1675\n",
    "Epoch #22: Loss:2.4789, Accuracy:0.1622, Validation Loss:2.4730, Validation Accuracy:0.1724\n",
    "Epoch #23: Loss:2.4737, Accuracy:0.1754, Validation Loss:2.4781, Validation Accuracy:0.1741\n",
    "Epoch #24: Loss:2.4792, Accuracy:0.1688, Validation Loss:2.4763, Validation Accuracy:0.1790\n",
    "Epoch #25: Loss:2.4770, Accuracy:0.1688, Validation Loss:2.4736, Validation Accuracy:0.1806\n",
    "Epoch #26: Loss:2.4752, Accuracy:0.1647, Validation Loss:2.4711, Validation Accuracy:0.1757\n",
    "Epoch #27: Loss:2.4737, Accuracy:0.1741, Validation Loss:2.4692, Validation Accuracy:0.1757\n",
    "Epoch #28: Loss:2.4713, Accuracy:0.1745, Validation Loss:2.4682, Validation Accuracy:0.1741\n",
    "Epoch #29: Loss:2.4698, Accuracy:0.1749, Validation Loss:2.4666, Validation Accuracy:0.1823\n",
    "Epoch #30: Loss:2.4689, Accuracy:0.1745, Validation Loss:2.4649, Validation Accuracy:0.1806\n",
    "Epoch #31: Loss:2.4678, Accuracy:0.1749, Validation Loss:2.4648, Validation Accuracy:0.1741\n",
    "Epoch #32: Loss:2.4672, Accuracy:0.1745, Validation Loss:2.4635, Validation Accuracy:0.1757\n",
    "Epoch #33: Loss:2.4666, Accuracy:0.1778, Validation Loss:2.4626, Validation Accuracy:0.1757\n",
    "Epoch #34: Loss:2.4660, Accuracy:0.1725, Validation Loss:2.4632, Validation Accuracy:0.1741\n",
    "Epoch #35: Loss:2.4640, Accuracy:0.1729, Validation Loss:2.4619, Validation Accuracy:0.1757\n",
    "Epoch #36: Loss:2.4633, Accuracy:0.1745, Validation Loss:2.4616, Validation Accuracy:0.1757\n",
    "Epoch #37: Loss:2.4627, Accuracy:0.1737, Validation Loss:2.4606, Validation Accuracy:0.1757\n",
    "Epoch #38: Loss:2.4619, Accuracy:0.1758, Validation Loss:2.4597, Validation Accuracy:0.1757\n",
    "Epoch #39: Loss:2.4607, Accuracy:0.1754, Validation Loss:2.4597, Validation Accuracy:0.1741\n",
    "Epoch #40: Loss:2.4601, Accuracy:0.1754, Validation Loss:2.4585, Validation Accuracy:0.1741\n",
    "Epoch #41: Loss:2.4597, Accuracy:0.1762, Validation Loss:2.4578, Validation Accuracy:0.1741\n",
    "Epoch #42: Loss:2.4587, Accuracy:0.1774, Validation Loss:2.4586, Validation Accuracy:0.1724\n",
    "Epoch #43: Loss:2.4582, Accuracy:0.1754, Validation Loss:2.4568, Validation Accuracy:0.1708\n",
    "Epoch #44: Loss:2.4577, Accuracy:0.1762, Validation Loss:2.4563, Validation Accuracy:0.1741\n",
    "Epoch #45: Loss:2.4569, Accuracy:0.1791, Validation Loss:2.4565, Validation Accuracy:0.1741\n",
    "Epoch #46: Loss:2.4567, Accuracy:0.1782, Validation Loss:2.4554, Validation Accuracy:0.1757\n",
    "Epoch #47: Loss:2.4571, Accuracy:0.1778, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #48: Loss:2.4564, Accuracy:0.1774, Validation Loss:2.4545, Validation Accuracy:0.1724\n",
    "Epoch #49: Loss:2.4563, Accuracy:0.1774, Validation Loss:2.4551, Validation Accuracy:0.1724\n",
    "Epoch #50: Loss:2.4567, Accuracy:0.1754, Validation Loss:2.4539, Validation Accuracy:0.1675\n",
    "Epoch #51: Loss:2.4553, Accuracy:0.1762, Validation Loss:2.4561, Validation Accuracy:0.1724\n",
    "Epoch #52: Loss:2.4528, Accuracy:0.1766, Validation Loss:2.4547, Validation Accuracy:0.1675\n",
    "Epoch #53: Loss:2.4537, Accuracy:0.1754, Validation Loss:2.4549, Validation Accuracy:0.1724\n",
    "Epoch #54: Loss:2.4535, Accuracy:0.1786, Validation Loss:2.4529, Validation Accuracy:0.1708\n",
    "Epoch #55: Loss:2.4523, Accuracy:0.1782, Validation Loss:2.4528, Validation Accuracy:0.1724\n",
    "Epoch #56: Loss:2.4517, Accuracy:0.1729, Validation Loss:2.4525, Validation Accuracy:0.1675\n",
    "Epoch #57: Loss:2.4513, Accuracy:0.1741, Validation Loss:2.4530, Validation Accuracy:0.1691\n",
    "Epoch #58: Loss:2.4518, Accuracy:0.1807, Validation Loss:2.4529, Validation Accuracy:0.1691\n",
    "Epoch #59: Loss:2.4510, Accuracy:0.1745, Validation Loss:2.4523, Validation Accuracy:0.1675\n",
    "Epoch #60: Loss:2.4507, Accuracy:0.1778, Validation Loss:2.4526, Validation Accuracy:0.1691\n",
    "Epoch #61: Loss:2.4507, Accuracy:0.1762, Validation Loss:2.4522, Validation Accuracy:0.1691\n",
    "Epoch #62: Loss:2.4505, Accuracy:0.1795, Validation Loss:2.4516, Validation Accuracy:0.1724\n",
    "Epoch #63: Loss:2.4499, Accuracy:0.1754, Validation Loss:2.4516, Validation Accuracy:0.1708\n",
    "Epoch #64: Loss:2.4490, Accuracy:0.1774, Validation Loss:2.4521, Validation Accuracy:0.1691\n",
    "Epoch #65: Loss:2.4494, Accuracy:0.1795, Validation Loss:2.4509, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4489, Accuracy:0.1700, Validation Loss:2.4512, Validation Accuracy:0.1691\n",
    "Epoch #67: Loss:2.4498, Accuracy:0.1795, Validation Loss:2.4517, Validation Accuracy:0.1691\n",
    "Epoch #68: Loss:2.4485, Accuracy:0.1708, Validation Loss:2.4505, Validation Accuracy:0.1675\n",
    "Epoch #69: Loss:2.4475, Accuracy:0.1749, Validation Loss:2.4528, Validation Accuracy:0.1691\n",
    "Epoch #70: Loss:2.4482, Accuracy:0.1791, Validation Loss:2.4500, Validation Accuracy:0.1675\n",
    "Epoch #71: Loss:2.4475, Accuracy:0.1729, Validation Loss:2.4505, Validation Accuracy:0.1691\n",
    "Epoch #72: Loss:2.4483, Accuracy:0.1811, Validation Loss:2.4512, Validation Accuracy:0.1691\n",
    "Epoch #73: Loss:2.4483, Accuracy:0.1721, Validation Loss:2.4499, Validation Accuracy:0.1675\n",
    "Epoch #74: Loss:2.4475, Accuracy:0.1766, Validation Loss:2.4515, Validation Accuracy:0.1675\n",
    "Epoch #75: Loss:2.4460, Accuracy:0.1791, Validation Loss:2.4492, Validation Accuracy:0.1675\n",
    "Epoch #76: Loss:2.4474, Accuracy:0.1758, Validation Loss:2.4521, Validation Accuracy:0.1691\n",
    "Epoch #77: Loss:2.4479, Accuracy:0.1774, Validation Loss:2.4499, Validation Accuracy:0.1658\n",
    "Epoch #78: Loss:2.4470, Accuracy:0.1803, Validation Loss:2.4482, Validation Accuracy:0.1691\n",
    "Epoch #79: Loss:2.4457, Accuracy:0.1795, Validation Loss:2.4482, Validation Accuracy:0.1675\n",
    "Epoch #80: Loss:2.4462, Accuracy:0.1774, Validation Loss:2.4504, Validation Accuracy:0.1691\n",
    "Epoch #81: Loss:2.4459, Accuracy:0.1778, Validation Loss:2.4477, Validation Accuracy:0.1658\n",
    "Epoch #82: Loss:2.4454, Accuracy:0.1725, Validation Loss:2.4473, Validation Accuracy:0.1691\n",
    "Epoch #83: Loss:2.4460, Accuracy:0.1795, Validation Loss:2.4489, Validation Accuracy:0.1691\n",
    "Epoch #84: Loss:2.4463, Accuracy:0.1737, Validation Loss:2.4476, Validation Accuracy:0.1658\n",
    "Epoch #85: Loss:2.4465, Accuracy:0.1795, Validation Loss:2.4497, Validation Accuracy:0.1691\n",
    "Epoch #86: Loss:2.4464, Accuracy:0.1717, Validation Loss:2.4476, Validation Accuracy:0.1691\n",
    "Epoch #87: Loss:2.4445, Accuracy:0.1770, Validation Loss:2.4474, Validation Accuracy:0.1691\n",
    "Epoch #88: Loss:2.4449, Accuracy:0.1758, Validation Loss:2.4471, Validation Accuracy:0.1691\n",
    "Epoch #89: Loss:2.4445, Accuracy:0.1754, Validation Loss:2.4468, Validation Accuracy:0.1691\n",
    "Epoch #90: Loss:2.4474, Accuracy:0.1774, Validation Loss:2.4490, Validation Accuracy:0.1626\n",
    "Epoch #91: Loss:2.4449, Accuracy:0.1725, Validation Loss:2.4476, Validation Accuracy:0.1675\n",
    "Epoch #92: Loss:2.4434, Accuracy:0.1758, Validation Loss:2.4499, Validation Accuracy:0.1675\n",
    "Epoch #93: Loss:2.4450, Accuracy:0.1799, Validation Loss:2.4461, Validation Accuracy:0.1691\n",
    "Epoch #94: Loss:2.4438, Accuracy:0.1782, Validation Loss:2.4465, Validation Accuracy:0.1691\n",
    "Epoch #95: Loss:2.4432, Accuracy:0.1782, Validation Loss:2.4464, Validation Accuracy:0.1691\n",
    "Epoch #96: Loss:2.4433, Accuracy:0.1774, Validation Loss:2.4480, Validation Accuracy:0.1691\n",
    "Epoch #97: Loss:2.4430, Accuracy:0.1782, Validation Loss:2.4497, Validation Accuracy:0.1691\n",
    "Epoch #98: Loss:2.4436, Accuracy:0.1782, Validation Loss:2.4489, Validation Accuracy:0.1691\n",
    "Epoch #99: Loss:2.4440, Accuracy:0.1782, Validation Loss:2.4516, Validation Accuracy:0.1691\n",
    "Epoch #100: Loss:2.4482, Accuracy:0.1778, Validation Loss:2.4488, Validation Accuracy:0.1691\n",
    "Epoch #101: Loss:2.4459, Accuracy:0.1778, Validation Loss:2.4501, Validation Accuracy:0.1642\n",
    "Epoch #102: Loss:2.4449, Accuracy:0.1782, Validation Loss:2.4490, Validation Accuracy:0.1691\n",
    "Epoch #103: Loss:2.4458, Accuracy:0.1782, Validation Loss:2.4490, Validation Accuracy:0.1691\n",
    "Epoch #104: Loss:2.4450, Accuracy:0.1770, Validation Loss:2.4530, Validation Accuracy:0.1708\n",
    "Epoch #105: Loss:2.4504, Accuracy:0.1766, Validation Loss:2.4533, Validation Accuracy:0.1741\n",
    "Epoch #106: Loss:2.4504, Accuracy:0.1745, Validation Loss:2.4557, Validation Accuracy:0.1560\n",
    "Epoch #107: Loss:2.4493, Accuracy:0.1778, Validation Loss:2.4585, Validation Accuracy:0.1576\n",
    "Epoch #108: Loss:2.4478, Accuracy:0.1758, Validation Loss:2.4558, Validation Accuracy:0.1576\n",
    "Epoch #109: Loss:2.4475, Accuracy:0.1762, Validation Loss:2.4552, Validation Accuracy:0.1724\n",
    "Epoch #110: Loss:2.4464, Accuracy:0.1782, Validation Loss:2.4545, Validation Accuracy:0.1724\n",
    "Epoch #111: Loss:2.4467, Accuracy:0.1807, Validation Loss:2.4554, Validation Accuracy:0.1724\n",
    "Epoch #112: Loss:2.4457, Accuracy:0.1782, Validation Loss:2.4557, Validation Accuracy:0.1724\n",
    "Epoch #113: Loss:2.4460, Accuracy:0.1782, Validation Loss:2.4532, Validation Accuracy:0.1691\n",
    "Epoch #114: Loss:2.4465, Accuracy:0.1791, Validation Loss:2.4506, Validation Accuracy:0.1675\n",
    "Epoch #115: Loss:2.4455, Accuracy:0.1803, Validation Loss:2.4480, Validation Accuracy:0.1708\n",
    "Epoch #116: Loss:2.4446, Accuracy:0.1782, Validation Loss:2.4467, Validation Accuracy:0.1675\n",
    "Epoch #117: Loss:2.4447, Accuracy:0.1811, Validation Loss:2.4469, Validation Accuracy:0.1675\n",
    "Epoch #118: Loss:2.4413, Accuracy:0.1795, Validation Loss:2.4481, Validation Accuracy:0.1576\n",
    "Epoch #119: Loss:2.4419, Accuracy:0.1770, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #120: Loss:2.4426, Accuracy:0.1791, Validation Loss:2.4455, Validation Accuracy:0.1675\n",
    "Epoch #121: Loss:2.4429, Accuracy:0.1778, Validation Loss:2.4452, Validation Accuracy:0.1626\n",
    "Epoch #122: Loss:2.4424, Accuracy:0.1795, Validation Loss:2.4443, Validation Accuracy:0.1691\n",
    "Epoch #123: Loss:2.4424, Accuracy:0.1782, Validation Loss:2.4441, Validation Accuracy:0.1576\n",
    "Epoch #124: Loss:2.4422, Accuracy:0.1758, Validation Loss:2.4427, Validation Accuracy:0.1642\n",
    "Epoch #125: Loss:2.4423, Accuracy:0.1786, Validation Loss:2.4422, Validation Accuracy:0.1642\n",
    "Epoch #126: Loss:2.4422, Accuracy:0.1778, Validation Loss:2.4425, Validation Accuracy:0.1642\n",
    "Epoch #127: Loss:2.4437, Accuracy:0.1799, Validation Loss:2.4469, Validation Accuracy:0.1691\n",
    "Epoch #128: Loss:2.4466, Accuracy:0.1758, Validation Loss:2.4486, Validation Accuracy:0.1626\n",
    "Epoch #129: Loss:2.4435, Accuracy:0.1811, Validation Loss:2.4503, Validation Accuracy:0.1675\n",
    "Epoch #130: Loss:2.4452, Accuracy:0.1815, Validation Loss:2.4489, Validation Accuracy:0.1626\n",
    "Epoch #131: Loss:2.4461, Accuracy:0.1799, Validation Loss:2.4483, Validation Accuracy:0.1691\n",
    "Epoch #132: Loss:2.4454, Accuracy:0.1799, Validation Loss:2.4495, Validation Accuracy:0.1675\n",
    "Epoch #133: Loss:2.4453, Accuracy:0.1795, Validation Loss:2.4492, Validation Accuracy:0.1609\n",
    "Epoch #134: Loss:2.4475, Accuracy:0.1749, Validation Loss:2.4491, Validation Accuracy:0.1642\n",
    "Epoch #135: Loss:2.4479, Accuracy:0.1795, Validation Loss:2.4537, Validation Accuracy:0.1658\n",
    "Epoch #136: Loss:2.4544, Accuracy:0.1811, Validation Loss:2.4503, Validation Accuracy:0.1691\n",
    "Epoch #137: Loss:2.4499, Accuracy:0.1778, Validation Loss:2.4527, Validation Accuracy:0.1741\n",
    "Epoch #138: Loss:2.4480, Accuracy:0.1815, Validation Loss:2.4527, Validation Accuracy:0.1642\n",
    "Epoch #139: Loss:2.4487, Accuracy:0.1803, Validation Loss:2.4513, Validation Accuracy:0.1691\n",
    "Epoch #140: Loss:2.4459, Accuracy:0.1786, Validation Loss:2.4500, Validation Accuracy:0.1527\n",
    "Epoch #141: Loss:2.4455, Accuracy:0.1770, Validation Loss:2.4495, Validation Accuracy:0.1642\n",
    "Epoch #142: Loss:2.4460, Accuracy:0.1786, Validation Loss:2.4493, Validation Accuracy:0.1642\n",
    "Epoch #143: Loss:2.4461, Accuracy:0.1786, Validation Loss:2.4486, Validation Accuracy:0.1642\n",
    "Epoch #144: Loss:2.4456, Accuracy:0.1786, Validation Loss:2.4484, Validation Accuracy:0.1642\n",
    "Epoch #145: Loss:2.4466, Accuracy:0.1786, Validation Loss:2.4474, Validation Accuracy:0.1527\n",
    "Epoch #146: Loss:2.4471, Accuracy:0.1774, Validation Loss:2.4473, Validation Accuracy:0.1642\n",
    "Epoch #147: Loss:2.4472, Accuracy:0.1795, Validation Loss:2.4471, Validation Accuracy:0.1642\n",
    "Epoch #148: Loss:2.4477, Accuracy:0.1766, Validation Loss:2.4471, Validation Accuracy:0.1642\n",
    "Epoch #149: Loss:2.4461, Accuracy:0.1770, Validation Loss:2.4479, Validation Accuracy:0.1741\n",
    "Epoch #150: Loss:2.4479, Accuracy:0.1791, Validation Loss:2.4474, Validation Accuracy:0.1642\n",
    "Epoch #151: Loss:2.4474, Accuracy:0.1766, Validation Loss:2.4473, Validation Accuracy:0.1642\n",
    "Epoch #152: Loss:2.4465, Accuracy:0.1774, Validation Loss:2.4475, Validation Accuracy:0.1691\n",
    "Epoch #153: Loss:2.4470, Accuracy:0.1782, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #154: Loss:2.4470, Accuracy:0.1786, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #155: Loss:2.4464, Accuracy:0.1770, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #156: Loss:2.4466, Accuracy:0.1786, Validation Loss:2.4468, Validation Accuracy:0.1642\n",
    "Epoch #157: Loss:2.4465, Accuracy:0.1774, Validation Loss:2.4472, Validation Accuracy:0.1642\n",
    "Epoch #158: Loss:2.4465, Accuracy:0.1786, Validation Loss:2.4474, Validation Accuracy:0.1642\n",
    "Epoch #159: Loss:2.4466, Accuracy:0.1766, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #160: Loss:2.4477, Accuracy:0.1782, Validation Loss:2.4472, Validation Accuracy:0.1642\n",
    "Epoch #161: Loss:2.4461, Accuracy:0.1786, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #162: Loss:2.4466, Accuracy:0.1778, Validation Loss:2.4466, Validation Accuracy:0.1642\n",
    "Epoch #163: Loss:2.4459, Accuracy:0.1762, Validation Loss:2.4471, Validation Accuracy:0.1691\n",
    "Epoch #164: Loss:2.4465, Accuracy:0.1774, Validation Loss:2.4465, Validation Accuracy:0.1642\n",
    "Epoch #165: Loss:2.4469, Accuracy:0.1766, Validation Loss:2.4467, Validation Accuracy:0.1642\n",
    "Epoch #166: Loss:2.4496, Accuracy:0.1791, Validation Loss:2.4474, Validation Accuracy:0.1691\n",
    "Epoch #167: Loss:2.4463, Accuracy:0.1778, Validation Loss:2.4494, Validation Accuracy:0.1576\n",
    "Epoch #168: Loss:2.4470, Accuracy:0.1745, Validation Loss:2.4491, Validation Accuracy:0.1691\n",
    "Epoch #169: Loss:2.4487, Accuracy:0.1791, Validation Loss:2.4472, Validation Accuracy:0.1642\n",
    "Epoch #170: Loss:2.4463, Accuracy:0.1754, Validation Loss:2.4476, Validation Accuracy:0.1642\n",
    "Epoch #171: Loss:2.4458, Accuracy:0.1815, Validation Loss:2.4474, Validation Accuracy:0.1691\n",
    "Epoch #172: Loss:2.4460, Accuracy:0.1803, Validation Loss:2.4469, Validation Accuracy:0.1642\n",
    "Epoch #173: Loss:2.4458, Accuracy:0.1786, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #174: Loss:2.4459, Accuracy:0.1786, Validation Loss:2.4466, Validation Accuracy:0.1642\n",
    "Epoch #175: Loss:2.4459, Accuracy:0.1786, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #176: Loss:2.4461, Accuracy:0.1803, Validation Loss:2.4473, Validation Accuracy:0.1642\n",
    "Epoch #177: Loss:2.4460, Accuracy:0.1782, Validation Loss:2.4473, Validation Accuracy:0.1642\n",
    "Epoch #178: Loss:2.4454, Accuracy:0.1791, Validation Loss:2.4473, Validation Accuracy:0.1642\n",
    "Epoch #179: Loss:2.4461, Accuracy:0.1778, Validation Loss:2.4469, Validation Accuracy:0.1642\n",
    "Epoch #180: Loss:2.4456, Accuracy:0.1786, Validation Loss:2.4467, Validation Accuracy:0.1642\n",
    "Epoch #181: Loss:2.4459, Accuracy:0.1786, Validation Loss:2.4463, Validation Accuracy:0.1642\n",
    "Epoch #182: Loss:2.4454, Accuracy:0.1786, Validation Loss:2.4468, Validation Accuracy:0.1642\n",
    "Epoch #183: Loss:2.4456, Accuracy:0.1786, Validation Loss:2.4466, Validation Accuracy:0.1642\n",
    "Epoch #184: Loss:2.4454, Accuracy:0.1770, Validation Loss:2.4465, Validation Accuracy:0.1642\n",
    "Epoch #185: Loss:2.4451, Accuracy:0.1786, Validation Loss:2.4469, Validation Accuracy:0.1642\n",
    "Epoch #186: Loss:2.4469, Accuracy:0.1786, Validation Loss:2.4469, Validation Accuracy:0.1642\n",
    "Epoch #187: Loss:2.4455, Accuracy:0.1725, Validation Loss:2.4478, Validation Accuracy:0.1642\n",
    "Epoch #188: Loss:2.4458, Accuracy:0.1762, Validation Loss:2.4475, Validation Accuracy:0.1642\n",
    "Epoch #189: Loss:2.4459, Accuracy:0.1762, Validation Loss:2.4466, Validation Accuracy:0.1642\n",
    "Epoch #190: Loss:2.4456, Accuracy:0.1778, Validation Loss:2.4468, Validation Accuracy:0.1642\n",
    "Epoch #191: Loss:2.4455, Accuracy:0.1786, Validation Loss:2.4467, Validation Accuracy:0.1642\n",
    "Epoch #192: Loss:2.4450, Accuracy:0.1786, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #193: Loss:2.4450, Accuracy:0.1786, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #194: Loss:2.4462, Accuracy:0.1795, Validation Loss:2.4475, Validation Accuracy:0.1642\n",
    "Epoch #195: Loss:2.4458, Accuracy:0.1741, Validation Loss:2.4482, Validation Accuracy:0.1527\n",
    "Epoch #196: Loss:2.4452, Accuracy:0.1762, Validation Loss:2.4479, Validation Accuracy:0.1823\n",
    "Epoch #197: Loss:2.4450, Accuracy:0.1832, Validation Loss:2.4469, Validation Accuracy:0.1642\n",
    "Epoch #198: Loss:2.4454, Accuracy:0.1741, Validation Loss:2.4466, Validation Accuracy:0.1773\n",
    "Epoch #199: Loss:2.4448, Accuracy:0.1795, Validation Loss:2.4471, Validation Accuracy:0.1773\n",
    "Epoch #200: Loss:2.4452, Accuracy:0.1799, Validation Loss:2.4471, Validation Accuracy:0.1642\n",
    "Epoch #201: Loss:2.4449, Accuracy:0.1778, Validation Loss:2.4465, Validation Accuracy:0.1642\n",
    "Epoch #202: Loss:2.4457, Accuracy:0.1795, Validation Loss:2.4465, Validation Accuracy:0.1642\n",
    "Epoch #203: Loss:2.4466, Accuracy:0.1741, Validation Loss:2.4478, Validation Accuracy:0.1527\n",
    "Epoch #204: Loss:2.4464, Accuracy:0.1758, Validation Loss:2.4488, Validation Accuracy:0.1823\n",
    "Epoch #205: Loss:2.4450, Accuracy:0.1770, Validation Loss:2.4489, Validation Accuracy:0.1527\n",
    "Epoch #206: Loss:2.4454, Accuracy:0.1717, Validation Loss:2.4473, Validation Accuracy:0.1773\n",
    "Epoch #207: Loss:2.4450, Accuracy:0.1786, Validation Loss:2.4468, Validation Accuracy:0.1773\n",
    "Epoch #208: Loss:2.4452, Accuracy:0.1795, Validation Loss:2.4467, Validation Accuracy:0.1773\n",
    "Epoch #209: Loss:2.4448, Accuracy:0.1795, Validation Loss:2.4468, Validation Accuracy:0.1773\n",
    "Epoch #210: Loss:2.4446, Accuracy:0.1782, Validation Loss:2.4478, Validation Accuracy:0.1823\n",
    "Epoch #211: Loss:2.4444, Accuracy:0.1770, Validation Loss:2.4476, Validation Accuracy:0.1773\n",
    "Epoch #212: Loss:2.4447, Accuracy:0.1754, Validation Loss:2.4470, Validation Accuracy:0.1773\n",
    "Epoch #213: Loss:2.4468, Accuracy:0.1819, Validation Loss:2.4470, Validation Accuracy:0.1773\n",
    "Epoch #214: Loss:2.4446, Accuracy:0.1786, Validation Loss:2.4483, Validation Accuracy:0.1658\n",
    "Epoch #215: Loss:2.4454, Accuracy:0.1766, Validation Loss:2.4478, Validation Accuracy:0.1823\n",
    "Epoch #216: Loss:2.4458, Accuracy:0.1791, Validation Loss:2.4475, Validation Accuracy:0.1658\n",
    "Epoch #217: Loss:2.4444, Accuracy:0.1791, Validation Loss:2.4473, Validation Accuracy:0.1773\n",
    "Epoch #218: Loss:2.4445, Accuracy:0.1786, Validation Loss:2.4472, Validation Accuracy:0.1642\n",
    "Epoch #219: Loss:2.4446, Accuracy:0.1762, Validation Loss:2.4468, Validation Accuracy:0.1773\n",
    "Epoch #220: Loss:2.4457, Accuracy:0.1774, Validation Loss:2.4473, Validation Accuracy:0.1773\n",
    "Epoch #221: Loss:2.4451, Accuracy:0.1766, Validation Loss:2.4478, Validation Accuracy:0.1527\n",
    "Epoch #222: Loss:2.4471, Accuracy:0.1811, Validation Loss:2.4484, Validation Accuracy:0.1823\n",
    "Epoch #223: Loss:2.4451, Accuracy:0.1807, Validation Loss:2.4489, Validation Accuracy:0.1658\n",
    "Epoch #224: Loss:2.4444, Accuracy:0.1774, Validation Loss:2.4470, Validation Accuracy:0.1773\n",
    "Epoch #225: Loss:2.4454, Accuracy:0.1791, Validation Loss:2.4470, Validation Accuracy:0.1773\n",
    "Epoch #226: Loss:2.4443, Accuracy:0.1786, Validation Loss:2.4480, Validation Accuracy:0.1658\n",
    "Epoch #227: Loss:2.4440, Accuracy:0.1803, Validation Loss:2.4471, Validation Accuracy:0.1773\n",
    "Epoch #228: Loss:2.4449, Accuracy:0.1795, Validation Loss:2.4469, Validation Accuracy:0.1773\n",
    "Epoch #229: Loss:2.4443, Accuracy:0.1774, Validation Loss:2.4477, Validation Accuracy:0.1527\n",
    "Epoch #230: Loss:2.4457, Accuracy:0.1786, Validation Loss:2.4477, Validation Accuracy:0.1773\n",
    "Epoch #231: Loss:2.4445, Accuracy:0.1791, Validation Loss:2.4483, Validation Accuracy:0.1658\n",
    "Epoch #232: Loss:2.4443, Accuracy:0.1754, Validation Loss:2.4472, Validation Accuracy:0.1741\n",
    "Epoch #233: Loss:2.4442, Accuracy:0.1791, Validation Loss:2.4472, Validation Accuracy:0.1741\n",
    "Epoch #234: Loss:2.4441, Accuracy:0.1791, Validation Loss:2.4474, Validation Accuracy:0.1741\n",
    "Epoch #235: Loss:2.4438, Accuracy:0.1766, Validation Loss:2.4473, Validation Accuracy:0.1658\n",
    "Epoch #236: Loss:2.4442, Accuracy:0.1770, Validation Loss:2.4469, Validation Accuracy:0.1773\n",
    "Epoch #237: Loss:2.4442, Accuracy:0.1791, Validation Loss:2.4471, Validation Accuracy:0.1741\n",
    "Epoch #238: Loss:2.4446, Accuracy:0.1795, Validation Loss:2.4473, Validation Accuracy:0.1773\n",
    "Epoch #239: Loss:2.4442, Accuracy:0.1791, Validation Loss:2.4475, Validation Accuracy:0.1741\n",
    "Epoch #240: Loss:2.4451, Accuracy:0.1782, Validation Loss:2.4478, Validation Accuracy:0.1626\n",
    "Epoch #241: Loss:2.4471, Accuracy:0.1774, Validation Loss:2.4487, Validation Accuracy:0.1790\n",
    "Epoch #242: Loss:2.4436, Accuracy:0.1795, Validation Loss:2.4510, Validation Accuracy:0.1658\n",
    "Epoch #243: Loss:2.4459, Accuracy:0.1774, Validation Loss:2.4478, Validation Accuracy:0.1741\n",
    "Epoch #244: Loss:2.4441, Accuracy:0.1791, Validation Loss:2.4472, Validation Accuracy:0.1741\n",
    "Epoch #245: Loss:2.4450, Accuracy:0.1778, Validation Loss:2.4480, Validation Accuracy:0.1658\n",
    "Epoch #246: Loss:2.4432, Accuracy:0.1766, Validation Loss:2.4481, Validation Accuracy:0.1790\n",
    "Epoch #247: Loss:2.4444, Accuracy:0.1778, Validation Loss:2.4471, Validation Accuracy:0.1741\n",
    "Epoch #248: Loss:2.4435, Accuracy:0.1795, Validation Loss:2.4474, Validation Accuracy:0.1773\n",
    "Epoch #249: Loss:2.4436, Accuracy:0.1791, Validation Loss:2.4473, Validation Accuracy:0.1741\n",
    "Epoch #250: Loss:2.4439, Accuracy:0.1791, Validation Loss:2.4473, Validation Accuracy:0.1773\n",
    "Epoch #251: Loss:2.4440, Accuracy:0.1729, Validation Loss:2.4476, Validation Accuracy:0.1741\n",
    "Epoch #252: Loss:2.4438, Accuracy:0.1791, Validation Loss:2.4480, Validation Accuracy:0.1741\n",
    "Epoch #253: Loss:2.4438, Accuracy:0.1791, Validation Loss:2.4480, Validation Accuracy:0.1741\n",
    "Epoch #254: Loss:2.4434, Accuracy:0.1791, Validation Loss:2.4470, Validation Accuracy:0.1741\n",
    "Epoch #255: Loss:2.4453, Accuracy:0.1807, Validation Loss:2.4466, Validation Accuracy:0.1741\n",
    "Epoch #256: Loss:2.4439, Accuracy:0.1795, Validation Loss:2.4475, Validation Accuracy:0.1658\n",
    "Epoch #257: Loss:2.4438, Accuracy:0.1774, Validation Loss:2.4472, Validation Accuracy:0.1741\n",
    "Epoch #258: Loss:2.4434, Accuracy:0.1791, Validation Loss:2.4468, Validation Accuracy:0.1741\n",
    "Epoch #259: Loss:2.4437, Accuracy:0.1791, Validation Loss:2.4473, Validation Accuracy:0.1741\n",
    "Epoch #260: Loss:2.4430, Accuracy:0.1791, Validation Loss:2.4469, Validation Accuracy:0.1741\n",
    "Epoch #261: Loss:2.4435, Accuracy:0.1791, Validation Loss:2.4473, Validation Accuracy:0.1741\n",
    "Epoch #262: Loss:2.4432, Accuracy:0.1791, Validation Loss:2.4471, Validation Accuracy:0.1741\n",
    "Epoch #263: Loss:2.4436, Accuracy:0.1782, Validation Loss:2.4477, Validation Accuracy:0.1757\n",
    "Epoch #264: Loss:2.4434, Accuracy:0.1795, Validation Loss:2.4475, Validation Accuracy:0.1790\n",
    "Epoch #265: Loss:2.4431, Accuracy:0.1774, Validation Loss:2.4468, Validation Accuracy:0.1773\n",
    "Epoch #266: Loss:2.4437, Accuracy:0.1795, Validation Loss:2.4466, Validation Accuracy:0.1741\n",
    "Epoch #267: Loss:2.4435, Accuracy:0.1791, Validation Loss:2.4471, Validation Accuracy:0.1741\n",
    "Epoch #268: Loss:2.4436, Accuracy:0.1754, Validation Loss:2.4481, Validation Accuracy:0.1626\n",
    "Epoch #269: Loss:2.4441, Accuracy:0.1782, Validation Loss:2.4480, Validation Accuracy:0.1790\n",
    "Epoch #270: Loss:2.4434, Accuracy:0.1758, Validation Loss:2.4484, Validation Accuracy:0.1626\n",
    "Epoch #271: Loss:2.4433, Accuracy:0.1786, Validation Loss:2.4475, Validation Accuracy:0.1741\n",
    "Epoch #272: Loss:2.4433, Accuracy:0.1791, Validation Loss:2.4475, Validation Accuracy:0.1741\n",
    "Epoch #273: Loss:2.4428, Accuracy:0.1791, Validation Loss:2.4477, Validation Accuracy:0.1741\n",
    "Epoch #274: Loss:2.4429, Accuracy:0.1791, Validation Loss:2.4479, Validation Accuracy:0.1741\n",
    "Epoch #275: Loss:2.4429, Accuracy:0.1791, Validation Loss:2.4480, Validation Accuracy:0.1741\n",
    "Epoch #276: Loss:2.4426, Accuracy:0.1791, Validation Loss:2.4476, Validation Accuracy:0.1741\n",
    "Epoch #277: Loss:2.4429, Accuracy:0.1791, Validation Loss:2.4468, Validation Accuracy:0.1741\n",
    "Epoch #278: Loss:2.4432, Accuracy:0.1791, Validation Loss:2.4471, Validation Accuracy:0.1741\n",
    "Epoch #279: Loss:2.4425, Accuracy:0.1791, Validation Loss:2.4487, Validation Accuracy:0.1626\n",
    "Epoch #280: Loss:2.4430, Accuracy:0.1782, Validation Loss:2.4483, Validation Accuracy:0.1741\n",
    "Epoch #281: Loss:2.4433, Accuracy:0.1803, Validation Loss:2.4484, Validation Accuracy:0.1757\n",
    "Epoch #282: Loss:2.4428, Accuracy:0.1770, Validation Loss:2.4484, Validation Accuracy:0.1626\n",
    "Epoch #283: Loss:2.4427, Accuracy:0.1782, Validation Loss:2.4474, Validation Accuracy:0.1741\n",
    "Epoch #284: Loss:2.4444, Accuracy:0.1782, Validation Loss:2.4476, Validation Accuracy:0.1741\n",
    "Epoch #285: Loss:2.4438, Accuracy:0.1803, Validation Loss:2.4486, Validation Accuracy:0.1741\n",
    "Epoch #286: Loss:2.4421, Accuracy:0.1815, Validation Loss:2.4490, Validation Accuracy:0.1806\n",
    "Epoch #287: Loss:2.4437, Accuracy:0.1799, Validation Loss:2.4480, Validation Accuracy:0.1741\n",
    "Epoch #288: Loss:2.4428, Accuracy:0.1791, Validation Loss:2.4475, Validation Accuracy:0.1741\n",
    "Epoch #289: Loss:2.4430, Accuracy:0.1791, Validation Loss:2.4481, Validation Accuracy:0.1741\n",
    "Epoch #290: Loss:2.4421, Accuracy:0.1774, Validation Loss:2.4487, Validation Accuracy:0.1626\n",
    "Epoch #291: Loss:2.4425, Accuracy:0.1791, Validation Loss:2.4486, Validation Accuracy:0.1757\n",
    "Epoch #292: Loss:2.4424, Accuracy:0.1799, Validation Loss:2.4478, Validation Accuracy:0.1757\n",
    "Epoch #293: Loss:2.4420, Accuracy:0.1786, Validation Loss:2.4476, Validation Accuracy:0.1741\n",
    "Epoch #294: Loss:2.4425, Accuracy:0.1791, Validation Loss:2.4476, Validation Accuracy:0.1741\n",
    "Epoch #295: Loss:2.4423, Accuracy:0.1795, Validation Loss:2.4486, Validation Accuracy:0.1757\n",
    "Epoch #296: Loss:2.4422, Accuracy:0.1823, Validation Loss:2.4482, Validation Accuracy:0.1757\n",
    "Epoch #297: Loss:2.4422, Accuracy:0.1799, Validation Loss:2.4480, Validation Accuracy:0.1741\n",
    "Epoch #298: Loss:2.4423, Accuracy:0.1791, Validation Loss:2.4478, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.4427, Accuracy:0.1786, Validation Loss:2.4473, Validation Accuracy:0.1741\n",
    "Epoch #300: Loss:2.4423, Accuracy:0.1791, Validation Loss:2.4478, Validation Accuracy:0.1741\n",
    "\n",
    "Test:\n",
    "Test Loss:2.44776368, Accuracy:0.1741\n",
    "Labels: ['mb', 'yd', 'eg', 'eo', 'eb', 'ck', 'sk', 'ib', 'by', 'ds', 'sg', 'my', 'aa', 'ek', 'ce']\n",
    "Confusion Matrix:\n",
    "      mb  yd  eg  eo  eb  ck  sk  ib  by  ds  sg  my  aa  ek  ce\n",
    "t:mb   0  15  10   0   1   0   0   1   0   1  24   0   0   0   0\n",
    "t:yd   0  37   0   0   3   0   0   2   0   0  20   0   0   0   0\n",
    "t:eg   0   1  30   0   9   0   0   0   0   2   8   0   0   0   0\n",
    "t:eo   0   4   1   0   6   0   0   0   0   1  22   0   0   0   0\n",
    "t:eb   0  10  13   0   5   0   0   0   0   2  20   0   0   0   0\n",
    "t:ck   0   1  10   0   3   0   0   0   0   2   7   0   0   0   0\n",
    "t:sk   0   4   8   0   6   0   0   0   0   5  10   0   0   0   0\n",
    "t:ib   0  30   6   0   2   0   0   0   0   0  16   0   0   0   0\n",
    "t:by   0   3  11   0   8   0   0   0   0   0  18   0   0   0   0\n",
    "t:ds   0   1  11   0   1   0   0   0   0  10   8   0   0   0   0\n",
    "t:sg   0  18   4   0   3   0   0   1   0   1  24   0   0   0   0\n",
    "t:my   0   7   5   0   1   0   0   0   0   2   5   0   0   0   0\n",
    "t:aa   0   2  14   0   2   0   0   0   0  10   6   0   0   0   0\n",
    "t:ek   0   8  12   0   4   0   0   0   0   1  23   0   0   0   0\n",
    "t:ce   0   6   9   0   3   0   0   1   0   1   7   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          yd       0.25      0.60      0.35        62\n",
    "          eg       0.21      0.60      0.31        50\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          eb       0.09      0.10      0.09        50\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          by       0.00      0.00      0.00        40\n",
    "          ds       0.26      0.32      0.29        31\n",
    "          sg       0.11      0.47      0.18        51\n",
    "          my       0.00      0.00      0.00        20\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          ce       0.00      0.00      0.00        27\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.06      0.14      0.08       609\n",
    "weighted avg       0.07      0.17      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 00:18:46 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 32 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.698672529903343, 2.6883271993283175, 2.6791688025682823, 2.670590907286345, 2.6590625382409305, 2.648430219816261, 2.6375450230584354, 2.6256390968567045, 2.6056970140616884, 2.5820468012335267, 2.5624930717674967, 2.5402487087719545, 2.5272938919380574, 2.5400646621566296, 2.532924420727885, 2.5167772789502574, 2.5111447014832144, 2.503389888209075, 2.4941207873214446, 2.4902771137813824, 2.4815446089445468, 2.4729820268690488, 2.4781456819700294, 2.476327881632963, 2.473631140438011, 2.4711293434274606, 2.469211045157146, 2.468167254490218, 2.4666466642483114, 2.4649374508505386, 2.464780531101822, 2.463540095218101, 2.4626065142244737, 2.463155248090747, 2.461888470673209, 2.4616254135501405, 2.4606263935076584, 2.459656576021943, 2.4596748962778174, 2.4584841935701167, 2.4578066248024624, 2.45860715374375, 2.4568406674270755, 2.4562684734075138, 2.4565305075621957, 2.4554432464154874, 2.455298861063564, 2.4545469691209214, 2.455131562864056, 2.453873765879664, 2.4561434919610985, 2.454716287027243, 2.4549098930922635, 2.452920461914614, 2.4527881114157943, 2.4525283416503756, 2.4529668345239948, 2.4529407345406917, 2.4522844762441953, 2.452595761648344, 2.452233617137414, 2.4516290514339953, 2.4515713455250308, 2.4520509074669947, 2.4509336079282713, 2.4512341468792243, 2.4516741928012893, 2.450471331137546, 2.452796463895901, 2.449967389036282, 2.450518795226399, 2.4512298064083105, 2.449861964177224, 2.4514529799005667, 2.449157060660752, 2.4521258296246207, 2.449949127112703, 2.4481693641305555, 2.448185544491597, 2.450387969197115, 2.4477227035610154, 2.447307377221745, 2.4488963951618214, 2.447604927914875, 2.4497167136281583, 2.4476482511936934, 2.447438115948331, 2.447077386289199, 2.44678895970675, 2.4489758691959977, 2.4475759810023314, 2.4499196372008676, 2.4461001633423303, 2.4464615522738553, 2.446378372377167, 2.4479789405033507, 2.449744521299215, 2.4489051743681207, 2.4515687871253355, 2.448811110213081, 2.4500501527770595, 2.449041967125753, 2.4489521244280836, 2.453006687618437, 2.4532877807742466, 2.455711986826754, 2.4584578445979526, 2.455839628069272, 2.455243239849072, 2.454501484023722, 2.455413480501848, 2.455691271423315, 2.453195835373476, 2.4506294155747237, 2.4479667960325093, 2.4466783499287073, 2.4468845150545118, 2.448086835676422, 2.447040408311415, 2.4455375749685104, 2.4452407223054733, 2.4442533753775613, 2.44406137403792, 2.442728984140606, 2.4422260329053906, 2.442464667979524, 2.4468726180065636, 2.4486436217485, 2.450267711492203, 2.4488933184268245, 2.4482610159123865, 2.4494741698986986, 2.4491765714435547, 2.449088922275111, 2.453688687291639, 2.450285329803066, 2.4526837886064903, 2.4527325164312606, 2.451312078043745, 2.44996340169108, 2.44948317577882, 2.4492961753569604, 2.4485712517266984, 2.448448706143008, 2.4473574239827927, 2.447341129697602, 2.447119049250786, 2.447108292227308, 2.4479335818580417, 2.447361300927273, 2.447295898678659, 2.4474985521219437, 2.447036715368136, 2.4470163186391196, 2.447026614485116, 2.4468401688073067, 2.4471845619001216, 2.44738540508477, 2.4470109548083276, 2.4471831544866705, 2.4469737188373686, 2.4466303890366077, 2.4470876660840264, 2.446477238572094, 2.4467355782175297, 2.4473573249353366, 2.449356037994911, 2.449061583611374, 2.4471892922969873, 2.4476078269125403, 2.4474473755152157, 2.4469295828213244, 2.4469982920021844, 2.446624754293407, 2.447034840904825, 2.447348858531081, 2.4472856392414113, 2.447322175224818, 2.446892340977987, 2.4466656808586933, 2.44633769440925, 2.446796919520461, 2.446560137377584, 2.4464997959450154, 2.446863976213928, 2.4468719238913903, 2.447754261724663, 2.447473865815963, 2.4466044754034586, 2.4467732330848433, 2.446729911371992, 2.4469575228166502, 2.4469970979518294, 2.4475305718545646, 2.448157140578347, 2.447935398772041, 2.446890212044927, 2.4466314519371695, 2.4470591036165485, 2.447114230376746, 2.446528682959295, 2.446521369890235, 2.4478015355484435, 2.448778455480566, 2.4489043518435976, 2.44726368554903, 2.446751778936151, 2.4467286026145048, 2.446804795946394, 2.4477937738296434, 2.447601542292753, 2.447009963159295, 2.44703327376267, 2.4483463458826975, 2.4477979612272165, 2.4474704202955775, 2.447310442994968, 2.4472278593404737, 2.446770069047148, 2.4473434332360577, 2.4477967472107736, 2.4484205222482163, 2.4488808247451908, 2.447021405685124, 2.4470070547658236, 2.447987828935896, 2.4471256157447554, 2.4468576798493835, 2.4476758075269376, 2.447704880891371, 2.4483138117296943, 2.4472433444118655, 2.4472378482568047, 2.4474097327841524, 2.447341677003306, 2.446948118789247, 2.447126821148376, 2.447277583512179, 2.4475146737591973, 2.4477680062229803, 2.448747100892717, 2.450967288761108, 2.4478166671026322, 2.4471930901601007, 2.4479606621371115, 2.4481341228109277, 2.447128358145653, 2.447414389189045, 2.4472530377517976, 2.44734440844243, 2.4475625172037208, 2.447963633952274, 2.4480419550427466, 2.4470254654563317, 2.446565884087473, 2.447458159551636, 2.447232152832357, 2.4468006617917215, 2.4473097148199976, 2.446943595491607, 2.44727963727879, 2.4470769834440134, 2.4476967445148037, 2.4475490220857568, 2.4467535042410415, 2.44655556436047, 2.4470750036693754, 2.4481454976086545, 2.4479960886324177, 2.448414946228804, 2.4475336987005276, 2.4475161947053055, 2.4476656698436767, 2.447915109703302, 2.4479536849681183, 2.4475940416990634, 2.4468417500431707, 2.447103549302701, 2.4486943512714556, 2.4483368052246144, 2.448358303611893, 2.448384920756022, 2.4474118221765275, 2.4476179422807616, 2.448581540917332, 2.448990299196666, 2.4480242459057586, 2.4475027211193967, 2.4481368374158987, 2.4487351568657383, 2.448649931815262, 2.4478469685772173, 2.447575355789736, 2.4476309167144725, 2.4486124096637094, 2.448212631034538, 2.4480301801402775, 2.447773721222024, 2.4472871390469555, 2.4477637628420625], 'val_acc': [0.10344827536046994, 0.07881773398403072, 0.09359605870957445, 0.12315270924949881, 0.12315270924949881, 0.11986863670629037, 0.09852216638661371, 0.1198686360089454, 0.13136288937187351, 0.1691297206336446, 0.16091953961817893, 0.14778325022832905, 0.15270935889633222, 0.178981937578159, 0.15927750359125717, 0.16256157584084666, 0.15270935928782414, 0.16256157584084666, 0.17898193748028604, 0.16912972073151755, 0.16748768441097686, 0.17241379288323408, 0.1740558286165369, 0.17898193708879412, 0.18062397321358886, 0.17569786464345866, 0.17569786464345866, 0.1740558285186639, 0.18226600924051062, 0.18062397311571587, 0.1740558285186639, 0.17569786474133164, 0.17569786474133164, 0.1740558286165369, 0.17569786474133164, 0.17569786474133164, 0.17569786474133164, 0.17569786474133164, 0.1740558286165369, 0.1740558286165369, 0.1740558286165369, 0.17241379249174216, 0.17077175666056635, 0.1740558286165369, 0.1740558286165369, 0.17569786474133164, 0.1740558286165369, 0.17241379239386917, 0.17241379249174216, 0.16748768431310387, 0.17241379249174216, 0.16748768431310387, 0.17241379249174216, 0.17077175626907443, 0.17241379239386917, 0.16748768431310387, 0.16912972014427968, 0.16912972014427968, 0.16748768431310387, 0.16912972014427968, 0.16912972014427968, 0.17241379239386917, 0.17077175626907443, 0.16912972014427968, 0.17077175626907443, 0.16912972014427968, 0.16912972014427968, 0.16748768431310387, 0.16912972014427968, 0.16748768431310387, 0.16912972014427968, 0.16912972014427968, 0.16748768431310387, 0.16748768401948494, 0.16748768431310387, 0.16912972014427968, 0.16584564818830913, 0.16912972014427968, 0.1674876842152309, 0.16912972014427968, 0.16584564818830913, 0.16912972014427968, 0.16912972014427968, 0.16584564818830913, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16256157554722772, 0.16748768431310387, 0.16748768401948494, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972014427968, 0.16912972024215267, 0.1642036118677684, 0.16912972014427968, 0.16912972014427968, 0.17077175626907443, 0.17405582891015584, 0.1559934318432667, 0.15763546796806144, 0.15763546787018845, 0.17241379239386917, 0.17241379239386917, 0.17241379239386917, 0.17241379239386917, 0.1691297200464067, 0.16748768411735793, 0.1707717563669474, 0.16748768411735793, 0.16748768382373896, 0.1576354671728435, 0.16420361137840353, 0.167487683725866, 0.16256157554722772, 0.16912971985066075, 0.1576354670749705, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16912971985066075, 0.16256157544935473, 0.167487683725866, 0.16256157544935473, 0.16912971985066075, 0.167487683725866, 0.16091953932456, 0.16420361157414948, 0.16584564760107126, 0.16912972014427968, 0.17405582842079093, 0.16420361157414948, 0.16912971985066075, 0.1527093585048403, 0.16420361157414948, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.1527093585048403, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.17405582842079093, 0.16420361167202246, 0.16420361167202246, 0.16912971985066075, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16912971985066075, 0.16420361167202246, 0.16420361167202246, 0.16912971985066075, 0.15763546697709752, 0.16912971985066075, 0.16420361167202246, 0.16420361167202246, 0.16912971985066075, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.1527093585048403, 0.1822660088490187, 0.16420361167202246, 0.17733990067038044, 0.17733990067038044, 0.16420361167202246, 0.16420361167202246, 0.16420361167202246, 0.1527093585048403, 0.1822660088490187, 0.1527093585048403, 0.17733990067038044, 0.17733990067038044, 0.17733990067038044, 0.17733990067038044, 0.1822660088490187, 0.17733990067038044, 0.17733990067038044, 0.17733990067038044, 0.16584564750319827, 0.1822660088490187, 0.16584564750319827, 0.17733990067038044, 0.16420361167202246, 0.17733990067038044, 0.17733990067038044, 0.1527093585048403, 0.1822660088490187, 0.16584564750319827, 0.17733990067038044, 0.17733990067038044, 0.16584564750319827, 0.17733990067038044, 0.17733990067038044, 0.1527093585048403, 0.17733990067038044, 0.16584564750319827, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.16584564750319827, 0.17733990067038044, 0.17405582842079093, 0.17733990067038044, 0.17405582842079093, 0.1625615752536088, 0.1789819365994292, 0.16584564750319827, 0.17405582842079093, 0.17405582842079093, 0.16584564750319827, 0.1789819365994292, 0.17405582842079093, 0.17733990067038044, 0.17405582842079093, 0.17733990067038044, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.16584564750319827, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17569786454558567, 0.1789819365994292, 0.17733990067038044, 0.17405582842079093, 0.17405582842079093, 0.1625615752536088, 0.1789819365994292, 0.1625615752536088, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.1625615752536088, 0.17405582842079093, 0.17569786454558567, 0.1625615752536088, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.18062397272422395, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.1625615752536088, 0.17569786454558567, 0.17569786454558567, 0.17405582842079093, 0.17405582842079093, 0.17569786454558567, 0.17569786454558567, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093, 0.17405582842079093], 'loss': [2.7097804027416377, 2.6940409137483003, 2.684191976092924, 2.6749071237732496, 2.6653781355284076, 2.6531756729071145, 2.639744551813333, 2.627271696772174, 2.6145189124700714, 2.597525344492229, 2.5689936473384285, 2.5460469359489926, 2.532309086758498, 2.5400042743408706, 2.5287113537289034, 2.5214948906790795, 2.510146360769409, 2.5041881290059806, 2.49636047145914, 2.4849189630034525, 2.48266935818494, 2.4789042618729984, 2.4736909259271327, 2.479172901647047, 2.4769676782267296, 2.475171180623268, 2.4736699178723094, 2.4713169774480424, 2.4697927670801936, 2.468913983711227, 2.4677994215023347, 2.467212383066604, 2.466628523675813, 2.4660244833027805, 2.4640226058646637, 2.46332117047398, 2.462736486556349, 2.4619053559626396, 2.460667736770191, 2.4600643503592488, 2.4597094265587276, 2.4586801107904015, 2.4581609459628315, 2.45766394456554, 2.456915058884043, 2.456667709252673, 2.4570595361368857, 2.45641348993509, 2.4562871560913337, 2.456706671939983, 2.455270178068345, 2.4527515783447016, 2.453712933949621, 2.4535465127878364, 2.452341985996254, 2.451676828611558, 2.451348156899642, 2.4518101910546086, 2.4509572050654667, 2.450651358187321, 2.450725937770867, 2.450463579224855, 2.4499306347825445, 2.449016063658853, 2.449403937200746, 2.4488950695100504, 2.449767748873826, 2.448549507481851, 2.4474815065121502, 2.4481822465234715, 2.4475176807301735, 2.4482977667383587, 2.448323759552879, 2.4474848717879465, 2.445990900435242, 2.4474492187617494, 2.4478535798051273, 2.446952209041838, 2.445654105846397, 2.446175384325658, 2.445864737449975, 2.445403651975753, 2.4459550690112417, 2.4463040592734084, 2.4464553627390146, 2.446440592928344, 2.4444764900011693, 2.444875570978717, 2.444534148523695, 2.4474052163854516, 2.444878788895186, 2.443407378989813, 2.4449784120250286, 2.443763922763801, 2.4432202473313414, 2.4433356603313032, 2.442982522760818, 2.443553634541725, 2.444028997519178, 2.4481869474328763, 2.4459074440433257, 2.4449407478622343, 2.445797543163417, 2.4450190868221022, 2.4503663535969946, 2.450377690669692, 2.449306918463423, 2.4477909537556237, 2.4475467050344792, 2.446424582851496, 2.4466533489540616, 2.4457382139484007, 2.4460058107513176, 2.446514900313266, 2.445519829481779, 2.4446365129286747, 2.4446718707466517, 2.4413203165026904, 2.4419484016097304, 2.4426280434861076, 2.442892591272781, 2.4423945373088674, 2.442422372751412, 2.4421615487006654, 2.442309782784088, 2.442157255207978, 2.4436613580284665, 2.4466485169389163, 2.4434828656410046, 2.445198512517941, 2.4460590428150657, 2.4453753032723493, 2.445336906914838, 2.447450017243685, 2.447920148818155, 2.4544395901094473, 2.4499182631592484, 2.4480134355948446, 2.4486567371924557, 2.4459062530274753, 2.44554406837761, 2.446047494935304, 2.4460552820679586, 2.445571318984766, 2.4466290068577448, 2.4470835491861895, 2.447211089124425, 2.447680227663483, 2.446063452726517, 2.447864205734441, 2.447350701071643, 2.446500431879345, 2.4470362388133022, 2.4470218258961514, 2.4464150489478143, 2.4466335822424603, 2.4465271953684593, 2.446529916669309, 2.4465677833165476, 2.447700364291056, 2.4461047863813397, 2.4465915485084424, 2.4459075940463086, 2.4465164624200466, 2.44687839562888, 2.4495790629416274, 2.446317553569159, 2.4470188698974233, 2.4486907093431913, 2.4463385644634648, 2.4458069610399877, 2.4459511908662392, 2.445812278116998, 2.445864244555056, 2.445876079907897, 2.4460583328466394, 2.4460253129994354, 2.445408467443572, 2.4461183010430307, 2.4455611917272484, 2.445923707813208, 2.445410595292673, 2.445615881326507, 2.4453861850487866, 2.4451378043916927, 2.446850408910481, 2.445499379825788, 2.4457982558734117, 2.4459089002080523, 2.4455561994282373, 2.4455436403012127, 2.4450251408915746, 2.445037287999962, 2.4462304675358766, 2.4457697844848005, 2.4452373602551845, 2.445010075481031, 2.4454337773137023, 2.4448360600755445, 2.4451584344037505, 2.4449249436967917, 2.4456981743140878, 2.446631427075584, 2.446356520662562, 2.4449664303899055, 2.4454197409706193, 2.4450094094756203, 2.4451516718345503, 2.444802517078251, 2.444613164748989, 2.444364844504323, 2.4447337387523613, 2.446799991458838, 2.444605885100316, 2.445437646646519, 2.445827568923668, 2.4443705470654997, 2.444517698229216, 2.4445567589276136, 2.445682402851645, 2.445131691672229, 2.4471159287791475, 2.445059172769347, 2.4444154212607008, 2.4453811892249013, 2.444259211953416, 2.4440436704937194, 2.444936580285889, 2.4442852454998167, 2.4457159985262265, 2.4444573087124364, 2.444348352400919, 2.4441808049438913, 2.4441188274712533, 2.443825235699726, 2.444159842810347, 2.4442094409245487, 2.4446027524662215, 2.444206458978829, 2.4451447203908367, 2.447095963989195, 2.4436191629335378, 2.4459121149912995, 2.4441236635987518, 2.4450402594689717, 2.4432207248538917, 2.444357282082403, 2.443526071297804, 2.443610742302646, 2.443944367539956, 2.44403390316503, 2.4437606911394876, 2.443796626547279, 2.443390165755881, 2.4453424247138553, 2.4439396539997515, 2.443786105091322, 2.443390544679865, 2.4436999971127364, 2.4429875227949704, 2.4434507527635327, 2.4432321829472725, 2.4435544871696457, 2.443443867802865, 2.443137983913539, 2.4437224661300316, 2.4434718340574104, 2.443556576930522, 2.444065287470573, 2.443446964894477, 2.4433008085286105, 2.443279570573654, 2.4428028169353886, 2.4428869799666826, 2.4428696955498728, 2.4425624722083246, 2.442921087139686, 2.4431704186316145, 2.4425367300515304, 2.4430069775551986, 2.4433095812552765, 2.4427631039394244, 2.44269010036633, 2.4444007637564407, 2.4438241305537294, 2.4420873289480345, 2.443677617392256, 2.4428089641202893, 2.442994155825041, 2.442089224840826, 2.442527538843958, 2.4423659289397253, 2.442007896787577, 2.4425266784808963, 2.442264635802784, 2.4421768275619287, 2.4422049118018494, 2.4423157320864637, 2.4426761716298255, 2.4422909216714346], 'acc': [0.08418891144423024, 0.10184804903041166, 0.07967145844712639, 0.11622176564449647, 0.12402464079783437, 0.11991786481296257, 0.12648870689790598, 0.12936344979602454, 0.1429158113881548, 0.1388090338366722, 0.1622176590274247, 0.1671457896793158, 0.1593429161109474, 0.16550308059617969, 0.16509240137478165, 0.16262833764298495, 0.16386036871639856, 0.16303901426356432, 0.1634496917041665, 0.17371663196008552, 0.16386036871639856, 0.16221765806665167, 0.17535934398061687, 0.16878850013323632, 0.1687885007390741, 0.16468172495002864, 0.17412731055728708, 0.17453798717786645, 0.17494866579342672, 0.17453798837118326, 0.1749486653834153, 0.17453798874447723, 0.17782340931574178, 0.17248459850003833, 0.17289527631393448, 0.1745379885670096, 0.17371663133588905, 0.17577002142121906, 0.1753593428240175, 0.17535934301984385, 0.17618069745431936, 0.17741273091436657, 0.1753593425914737, 0.1761806992351152, 0.1790554425616039, 0.1782340863096151, 0.17782340892408907, 0.1774127307001815, 0.17741273208932465, 0.1753593433931378, 0.1761806992167565, 0.1765913756598682, 0.17535934300148512, 0.1786447629669119, 0.17823408732546428, 0.1728952767055872, 0.17412730914978522, 0.18069815203639272, 0.17453798719622515, 0.17782340929738305, 0.1761806984518098, 0.17946611978802104, 0.17535934280565876, 0.17741273167931323, 0.17946612019803246, 0.1700205341807626, 0.1794661188088893, 0.17084188802775907, 0.1749486653834153, 0.1790554409766344, 0.17289527770307764, 0.18110882985028887, 0.17207392225275295, 0.17659137685318502, 0.17905544117246075, 0.17577002101120762, 0.17741273208932465, 0.18028747541581336, 0.1794661192189007, 0.1774127310918342, 0.17782340970739446, 0.17248459889169102, 0.1794661189679982, 0.1737166317642592, 0.17946611920054198, 0.17166324385137774, 0.17700205425706977, 0.17577002022790222, 0.17535934398061687, 0.17741273089600784, 0.17248459891004975, 0.17577002061955493, 0.17987679599858897, 0.17823408515301573, 0.17823408595467985, 0.17741273208932465, 0.17823408513465702, 0.17823408673798524, 0.17823408556302714, 0.17782340929738305, 0.17782340911991543, 0.1782340861505062, 0.1782340865421589, 0.17700205229880628, 0.17659137468073646, 0.17453798756951913, 0.1778234095115681, 0.1757700200504346, 0.17618069745431936, 0.17823408634633253, 0.18069815244640414, 0.17823408534884208, 0.17823408673798524, 0.1790554409766344, 0.1802874749874432, 0.1782340857588535, 0.18110882983193016, 0.17946611800722517, 0.17700205308211167, 0.17905544158247216, 0.17782340773077226, 0.17946611879053057, 0.17823408533048335, 0.17577001944459683, 0.17864476394604364, 0.1778234095115681, 0.1798767968186118, 0.17577001983624954, 0.18110883026030028, 0.18151950729089106, 0.17987679599858897, 0.179876795625295, 0.17946611879053057, 0.1749486642084572, 0.1794661188088893, 0.18110882926280983, 0.17782340890573037, 0.18151950611593298, 0.180287475219987, 0.17864476277108554, 0.17700205228044757, 0.17864476414187, 0.17864476275272684, 0.17864476457024012, 0.17864476316273825, 0.17741273091436657, 0.17946612017967373, 0.17659137685318502, 0.17700205366959074, 0.17905544197412487, 0.17659137468073646, 0.1774127303085288, 0.1782340869338116, 0.17864476335856458, 0.17700205447125483, 0.17864476355439093, 0.1774127307001815, 0.17864476435605506, 0.1765913746990952, 0.17823408593632112, 0.1786447629669119, 0.17782340792659862, 0.17618069706266665, 0.1774127311101929, 0.17659137605152092, 0.1790554409766344, 0.17782340812242498, 0.1745379885670096, 0.17905544197412487, 0.17535934341149653, 0.18151950666669459, 0.18028747422249655, 0.17864476472934904, 0.17864476376857602, 0.17864476414187, 0.18028747539745463, 0.17823408595467985, 0.1790554405849817, 0.17782340851407766, 0.1786447629669119, 0.17864476414187, 0.1786447629669119, 0.17864476277108554, 0.17700205347376438, 0.17864476357274967, 0.17864476316273825, 0.17248459908751737, 0.17618069862927743, 0.17618069864763616, 0.17782340890573037, 0.1786447629669119, 0.17864476435605506, 0.17864476394604364, 0.17946611978802104, 0.17412730995144932, 0.17618069786433077, 0.18316221874230207, 0.17412730995144932, 0.1794661181846928, 0.1798767976019172, 0.17782340792659862, 0.1794661185947042, 0.174127309382329, 0.17577002024626096, 0.17700205407960215, 0.17166324347808376, 0.17864476294855317, 0.1794661197696623, 0.17946611939636833, 0.1782340857588535, 0.1770020524762739, 0.17535934202235337, 0.18193018569226627, 0.17864476294855317, 0.17659137646153233, 0.17905544117246075, 0.17905544056662298, 0.17864476357274967, 0.1761806984518098, 0.1774127298985174, 0.17659137624734725, 0.181108830651953, 0.18069815127144603, 0.17741273050435516, 0.17905544216995123, 0.17864476394604364, 0.18028747500580194, 0.17946611839887788, 0.17741273011270245, 0.17864476316273825, 0.1790554405849817, 0.17535934202235337, 0.17905544195576614, 0.17905544056662298, 0.17659137487656282, 0.17700205229880628, 0.17905544037079665, 0.17946611998384737, 0.1790554417599398, 0.1782340857588535, 0.1774127307001815, 0.17946611800722517, 0.1774127318751396, 0.1790554421515925, 0.17782340970739446, 0.1765913768348263, 0.17782340929738305, 0.1794661193780096, 0.17905544079916677, 0.17905544038915536, 0.1728952767055872, 0.17905544216995123, 0.17905544195576614, 0.17905544038915536, 0.18069815107561968, 0.1794661193780096, 0.1774127303085288, 0.17905544117246075, 0.1790554421515925, 0.17905544119081948, 0.17905544216995123, 0.1790554425432452, 0.17823408534884208, 0.17946611998384737, 0.1774127307001815, 0.17946611996548867, 0.17905544177829852, 0.17535934321567018, 0.17823408593632112, 0.17577002142121906, 0.1786447629669119, 0.1790554425432452, 0.17905544158247216, 0.1790554425432452, 0.17905544216995123, 0.17905544234741885, 0.17905544177829852, 0.1790554421515925, 0.17905544197412487, 0.17823408730710558, 0.18028747459579053, 0.17700205288628534, 0.17823408513465702, 0.1782340857404948, 0.18028747461414923, 0.18151950729089106, 0.17987679699607942, 0.17905544078080807, 0.17905544195576614, 0.17741273171603067, 0.17905544234741885, 0.17987679719190577, 0.17864476373185856, 0.1790554404075141, 0.17946611820305153, 0.1823408627044983, 0.17987679740609086, 0.17905544156411346, 0.17864476453352268, 0.1790554417599398]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
