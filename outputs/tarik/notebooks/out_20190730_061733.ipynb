{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf6.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 06:17:33 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '1', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002A04CAF6E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002A0472E6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0953, Accuracy:0.3729, Validation Loss:1.0806, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0778, Accuracy:0.3873, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0754, Accuracy:0.3967, Validation Loss:1.0754, Validation Accuracy:0.3810\n",
    "Epoch #5: Loss:1.0751, Accuracy:0.3938, Validation Loss:1.0748, Validation Accuracy:0.4007\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #7: Loss:1.0742, Accuracy:0.3930, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #9: Loss:1.0741, Accuracy:0.3967, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #10: Loss:1.0742, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #11: Loss:1.0741, Accuracy:0.4041, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #12: Loss:1.0740, Accuracy:0.3951, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #14: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #15: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #16: Loss:1.0737, Accuracy:0.4033, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #17: Loss:1.0737, Accuracy:0.3922, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #18: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0734, Validation Accuracy:0.3924\n",
    "Epoch #19: Loss:1.0737, Accuracy:0.4008, Validation Loss:1.0733, Validation Accuracy:0.4039\n",
    "Epoch #20: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #21: Loss:1.0732, Accuracy:0.4066, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #22: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #23: Loss:1.0733, Accuracy:0.3963, Validation Loss:1.0731, Validation Accuracy:0.3875\n",
    "Epoch #24: Loss:1.0733, Accuracy:0.4029, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #26: Loss:1.0732, Accuracy:0.4021, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #27: Loss:1.0730, Accuracy:0.4029, Validation Loss:1.0728, Validation Accuracy:0.3875\n",
    "Epoch #28: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0729, Validation Accuracy:0.4056\n",
    "Epoch #29: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0727, Validation Accuracy:0.3842\n",
    "Epoch #30: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0726, Validation Accuracy:0.3859\n",
    "Epoch #31: Loss:1.0728, Accuracy:0.4057, Validation Loss:1.0726, Validation Accuracy:0.3974\n",
    "Epoch #32: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0727, Validation Accuracy:0.4007\n",
    "Epoch #33: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0724, Validation Accuracy:0.4089\n",
    "Epoch #34: Loss:1.0729, Accuracy:0.4057, Validation Loss:1.0724, Validation Accuracy:0.4122\n",
    "Epoch #35: Loss:1.0732, Accuracy:0.3992, Validation Loss:1.0726, Validation Accuracy:0.4154\n",
    "Epoch #36: Loss:1.0726, Accuracy:0.4070, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0722, Validation Accuracy:0.3957\n",
    "Epoch #38: Loss:1.0723, Accuracy:0.4016, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #39: Loss:1.0725, Accuracy:0.4078, Validation Loss:1.0722, Validation Accuracy:0.3859\n",
    "Epoch #40: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0721, Validation Accuracy:0.4122\n",
    "Epoch #41: Loss:1.0720, Accuracy:0.4041, Validation Loss:1.0720, Validation Accuracy:0.4072\n",
    "Epoch #42: Loss:1.0719, Accuracy:0.4078, Validation Loss:1.0719, Validation Accuracy:0.4138\n",
    "Epoch #43: Loss:1.0718, Accuracy:0.4074, Validation Loss:1.0719, Validation Accuracy:0.4154\n",
    "Epoch #44: Loss:1.0719, Accuracy:0.4053, Validation Loss:1.0718, Validation Accuracy:0.4105\n",
    "Epoch #45: Loss:1.0719, Accuracy:0.4066, Validation Loss:1.0722, Validation Accuracy:0.3957\n",
    "Epoch #46: Loss:1.0714, Accuracy:0.4123, Validation Loss:1.0718, Validation Accuracy:0.4072\n",
    "Epoch #47: Loss:1.0713, Accuracy:0.4099, Validation Loss:1.0721, Validation Accuracy:0.4056\n",
    "Epoch #48: Loss:1.0712, Accuracy:0.4127, Validation Loss:1.0719, Validation Accuracy:0.4056\n",
    "Epoch #49: Loss:1.0713, Accuracy:0.4140, Validation Loss:1.0717, Validation Accuracy:0.4056\n",
    "Epoch #50: Loss:1.0709, Accuracy:0.4119, Validation Loss:1.0717, Validation Accuracy:0.4056\n",
    "Epoch #51: Loss:1.0708, Accuracy:0.4099, Validation Loss:1.0716, Validation Accuracy:0.4138\n",
    "Epoch #52: Loss:1.0701, Accuracy:0.4144, Validation Loss:1.0715, Validation Accuracy:0.4056\n",
    "Epoch #53: Loss:1.0698, Accuracy:0.4136, Validation Loss:1.0719, Validation Accuracy:0.3990\n",
    "Epoch #54: Loss:1.0695, Accuracy:0.4189, Validation Loss:1.0716, Validation Accuracy:0.4072\n",
    "Epoch #55: Loss:1.0692, Accuracy:0.4127, Validation Loss:1.0715, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0692, Accuracy:0.4123, Validation Loss:1.0717, Validation Accuracy:0.4023\n",
    "Epoch #57: Loss:1.0684, Accuracy:0.4226, Validation Loss:1.0718, Validation Accuracy:0.3974\n",
    "Epoch #58: Loss:1.0688, Accuracy:0.4156, Validation Loss:1.0717, Validation Accuracy:0.3990\n",
    "Epoch #59: Loss:1.0674, Accuracy:0.4205, Validation Loss:1.0716, Validation Accuracy:0.4007\n",
    "Epoch #60: Loss:1.0669, Accuracy:0.4144, Validation Loss:1.0718, Validation Accuracy:0.3957\n",
    "Epoch #61: Loss:1.0664, Accuracy:0.4279, Validation Loss:1.0721, Validation Accuracy:0.4039\n",
    "Epoch #62: Loss:1.0657, Accuracy:0.4218, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #63: Loss:1.0653, Accuracy:0.4263, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #64: Loss:1.0632, Accuracy:0.4246, Validation Loss:1.0733, Validation Accuracy:0.4154\n",
    "Epoch #65: Loss:1.0622, Accuracy:0.4275, Validation Loss:1.0762, Validation Accuracy:0.4072\n",
    "Epoch #66: Loss:1.0612, Accuracy:0.4275, Validation Loss:1.0772, Validation Accuracy:0.3875\n",
    "Epoch #67: Loss:1.0594, Accuracy:0.4333, Validation Loss:1.0770, Validation Accuracy:0.4072\n",
    "Epoch #68: Loss:1.0596, Accuracy:0.4341, Validation Loss:1.0760, Validation Accuracy:0.4039\n",
    "Epoch #69: Loss:1.0562, Accuracy:0.4407, Validation Loss:1.0817, Validation Accuracy:0.3990\n",
    "Epoch #70: Loss:1.0575, Accuracy:0.4394, Validation Loss:1.0779, Validation Accuracy:0.3957\n",
    "Epoch #71: Loss:1.0547, Accuracy:0.4517, Validation Loss:1.0878, Validation Accuracy:0.3924\n",
    "Epoch #72: Loss:1.0553, Accuracy:0.4431, Validation Loss:1.0789, Validation Accuracy:0.4122\n",
    "Epoch #73: Loss:1.0542, Accuracy:0.4517, Validation Loss:1.0907, Validation Accuracy:0.3826\n",
    "Epoch #74: Loss:1.0596, Accuracy:0.4394, Validation Loss:1.0832, Validation Accuracy:0.3908\n",
    "Epoch #75: Loss:1.0556, Accuracy:0.4366, Validation Loss:1.0786, Validation Accuracy:0.4007\n",
    "Epoch #76: Loss:1.0513, Accuracy:0.4546, Validation Loss:1.0794, Validation Accuracy:0.3908\n",
    "Epoch #77: Loss:1.0484, Accuracy:0.4616, Validation Loss:1.0807, Validation Accuracy:0.3974\n",
    "Epoch #78: Loss:1.0469, Accuracy:0.4653, Validation Loss:1.0830, Validation Accuracy:0.4007\n",
    "Epoch #79: Loss:1.0444, Accuracy:0.4702, Validation Loss:1.0883, Validation Accuracy:0.4039\n",
    "Epoch #80: Loss:1.0461, Accuracy:0.4575, Validation Loss:1.0935, Validation Accuracy:0.3957\n",
    "Epoch #81: Loss:1.0458, Accuracy:0.4612, Validation Loss:1.0874, Validation Accuracy:0.4007\n",
    "Epoch #82: Loss:1.0424, Accuracy:0.4624, Validation Loss:1.0922, Validation Accuracy:0.3842\n",
    "Epoch #83: Loss:1.0397, Accuracy:0.4731, Validation Loss:1.0892, Validation Accuracy:0.3941\n",
    "Epoch #84: Loss:1.0373, Accuracy:0.4776, Validation Loss:1.0910, Validation Accuracy:0.3875\n",
    "Epoch #85: Loss:1.0382, Accuracy:0.4760, Validation Loss:1.0971, Validation Accuracy:0.3924\n",
    "Epoch #86: Loss:1.0422, Accuracy:0.4727, Validation Loss:1.0930, Validation Accuracy:0.3908\n",
    "Epoch #87: Loss:1.0372, Accuracy:0.4797, Validation Loss:1.0940, Validation Accuracy:0.3892\n",
    "Epoch #88: Loss:1.0342, Accuracy:0.4793, Validation Loss:1.1136, Validation Accuracy:0.3859\n",
    "Epoch #89: Loss:1.0412, Accuracy:0.4649, Validation Loss:1.0948, Validation Accuracy:0.3875\n",
    "Epoch #90: Loss:1.0409, Accuracy:0.4583, Validation Loss:1.0955, Validation Accuracy:0.3859\n",
    "Epoch #91: Loss:1.0359, Accuracy:0.4760, Validation Loss:1.1007, Validation Accuracy:0.3826\n",
    "Epoch #92: Loss:1.0279, Accuracy:0.4949, Validation Loss:1.0974, Validation Accuracy:0.3957\n",
    "Epoch #93: Loss:1.0275, Accuracy:0.4871, Validation Loss:1.0952, Validation Accuracy:0.3924\n",
    "Epoch #94: Loss:1.0261, Accuracy:0.4842, Validation Loss:1.1053, Validation Accuracy:0.3744\n",
    "Epoch #95: Loss:1.0280, Accuracy:0.4817, Validation Loss:1.1013, Validation Accuracy:0.3810\n",
    "Epoch #96: Loss:1.0250, Accuracy:0.4838, Validation Loss:1.1083, Validation Accuracy:0.4039\n",
    "Epoch #97: Loss:1.0289, Accuracy:0.4772, Validation Loss:1.1063, Validation Accuracy:0.3892\n",
    "Epoch #98: Loss:1.0196, Accuracy:0.5006, Validation Loss:1.1065, Validation Accuracy:0.3810\n",
    "Epoch #99: Loss:1.0170, Accuracy:0.4994, Validation Loss:1.1036, Validation Accuracy:0.3875\n",
    "Epoch #100: Loss:1.0165, Accuracy:0.4965, Validation Loss:1.1082, Validation Accuracy:0.3990\n",
    "Epoch #101: Loss:1.0121, Accuracy:0.4994, Validation Loss:1.1168, Validation Accuracy:0.4039\n",
    "Epoch #102: Loss:1.0143, Accuracy:0.4973, Validation Loss:1.1208, Validation Accuracy:0.4122\n",
    "Epoch #103: Loss:1.0101, Accuracy:0.5023, Validation Loss:1.1196, Validation Accuracy:0.3974\n",
    "Epoch #104: Loss:1.0060, Accuracy:0.5109, Validation Loss:1.1235, Validation Accuracy:0.3957\n",
    "Epoch #105: Loss:1.0061, Accuracy:0.5027, Validation Loss:1.1232, Validation Accuracy:0.3908\n",
    "Epoch #106: Loss:1.0034, Accuracy:0.5125, Validation Loss:1.1189, Validation Accuracy:0.3662\n",
    "Epoch #107: Loss:1.0059, Accuracy:0.5084, Validation Loss:1.1196, Validation Accuracy:0.3695\n",
    "Epoch #108: Loss:1.0029, Accuracy:0.5113, Validation Loss:1.1263, Validation Accuracy:0.3678\n",
    "Epoch #109: Loss:0.9979, Accuracy:0.5133, Validation Loss:1.1207, Validation Accuracy:0.3842\n",
    "Epoch #110: Loss:1.0008, Accuracy:0.5125, Validation Loss:1.1734, Validation Accuracy:0.4089\n",
    "Epoch #111: Loss:1.0022, Accuracy:0.5179, Validation Loss:1.1233, Validation Accuracy:0.3793\n",
    "Epoch #112: Loss:1.0010, Accuracy:0.5179, Validation Loss:1.1331, Validation Accuracy:0.3678\n",
    "Epoch #113: Loss:0.9929, Accuracy:0.5240, Validation Loss:1.1329, Validation Accuracy:0.3793\n",
    "Epoch #114: Loss:0.9883, Accuracy:0.5314, Validation Loss:1.1393, Validation Accuracy:0.3957\n",
    "Epoch #115: Loss:0.9907, Accuracy:0.5207, Validation Loss:1.1472, Validation Accuracy:0.3744\n",
    "Epoch #116: Loss:0.9928, Accuracy:0.5183, Validation Loss:1.1534, Validation Accuracy:0.3908\n",
    "Epoch #117: Loss:0.9928, Accuracy:0.4990, Validation Loss:1.1362, Validation Accuracy:0.3727\n",
    "Epoch #118: Loss:0.9894, Accuracy:0.5248, Validation Loss:1.1350, Validation Accuracy:0.3810\n",
    "Epoch #119: Loss:0.9821, Accuracy:0.5347, Validation Loss:1.1354, Validation Accuracy:0.3842\n",
    "Epoch #120: Loss:0.9769, Accuracy:0.5363, Validation Loss:1.1430, Validation Accuracy:0.3645\n",
    "Epoch #121: Loss:0.9784, Accuracy:0.5372, Validation Loss:1.1533, Validation Accuracy:0.3777\n",
    "Epoch #122: Loss:0.9781, Accuracy:0.5253, Validation Loss:1.1409, Validation Accuracy:0.3793\n",
    "Epoch #123: Loss:0.9709, Accuracy:0.5474, Validation Loss:1.1694, Validation Accuracy:0.3695\n",
    "Epoch #124: Loss:0.9734, Accuracy:0.5294, Validation Loss:1.1707, Validation Accuracy:0.3662\n",
    "Epoch #125: Loss:0.9827, Accuracy:0.5216, Validation Loss:1.1670, Validation Accuracy:0.3810\n",
    "Epoch #126: Loss:0.9694, Accuracy:0.5388, Validation Loss:1.1605, Validation Accuracy:0.3678\n",
    "Epoch #127: Loss:0.9611, Accuracy:0.5474, Validation Loss:1.1611, Validation Accuracy:0.3662\n",
    "Epoch #128: Loss:0.9685, Accuracy:0.5318, Validation Loss:1.1886, Validation Accuracy:0.3892\n",
    "Epoch #129: Loss:0.9669, Accuracy:0.5433, Validation Loss:1.1653, Validation Accuracy:0.3711\n",
    "Epoch #130: Loss:0.9582, Accuracy:0.5491, Validation Loss:1.1679, Validation Accuracy:0.3810\n",
    "Epoch #131: Loss:0.9488, Accuracy:0.5581, Validation Loss:1.1956, Validation Accuracy:0.3826\n",
    "Epoch #132: Loss:0.9468, Accuracy:0.5565, Validation Loss:1.1824, Validation Accuracy:0.3793\n",
    "Epoch #133: Loss:0.9461, Accuracy:0.5639, Validation Loss:1.1948, Validation Accuracy:0.3810\n",
    "Epoch #134: Loss:0.9392, Accuracy:0.5684, Validation Loss:1.2003, Validation Accuracy:0.3678\n",
    "Epoch #135: Loss:0.9384, Accuracy:0.5639, Validation Loss:1.2308, Validation Accuracy:0.3744\n",
    "Epoch #136: Loss:0.9453, Accuracy:0.5647, Validation Loss:1.2178, Validation Accuracy:0.3760\n",
    "Epoch #137: Loss:0.9384, Accuracy:0.5655, Validation Loss:1.2304, Validation Accuracy:0.3695\n",
    "Epoch #138: Loss:0.9432, Accuracy:0.5441, Validation Loss:1.2261, Validation Accuracy:0.3760\n",
    "Epoch #139: Loss:0.9350, Accuracy:0.5614, Validation Loss:1.2145, Validation Accuracy:0.3695\n",
    "Epoch #140: Loss:0.9306, Accuracy:0.5717, Validation Loss:1.1933, Validation Accuracy:0.3727\n",
    "Epoch #141: Loss:0.9275, Accuracy:0.5680, Validation Loss:1.2244, Validation Accuracy:0.3777\n",
    "Epoch #142: Loss:0.9243, Accuracy:0.5807, Validation Loss:1.2198, Validation Accuracy:0.3695\n",
    "Epoch #143: Loss:0.9180, Accuracy:0.5778, Validation Loss:1.2341, Validation Accuracy:0.3727\n",
    "Epoch #144: Loss:0.9379, Accuracy:0.5466, Validation Loss:1.2394, Validation Accuracy:0.3810\n",
    "Epoch #145: Loss:0.9374, Accuracy:0.5626, Validation Loss:1.2079, Validation Accuracy:0.3645\n",
    "Epoch #146: Loss:0.9264, Accuracy:0.5622, Validation Loss:1.2399, Validation Accuracy:0.3810\n",
    "Epoch #147: Loss:0.9343, Accuracy:0.5626, Validation Loss:1.2023, Validation Accuracy:0.3580\n",
    "Epoch #148: Loss:0.9146, Accuracy:0.5786, Validation Loss:1.2323, Validation Accuracy:0.3826\n",
    "Epoch #149: Loss:0.9140, Accuracy:0.5795, Validation Loss:1.2391, Validation Accuracy:0.3810\n",
    "Epoch #150: Loss:0.9031, Accuracy:0.5922, Validation Loss:1.2417, Validation Accuracy:0.3727\n",
    "Epoch #151: Loss:0.9015, Accuracy:0.5934, Validation Loss:1.2544, Validation Accuracy:0.3596\n",
    "Epoch #152: Loss:0.9050, Accuracy:0.5832, Validation Loss:1.2680, Validation Accuracy:0.3760\n",
    "Epoch #153: Loss:0.9288, Accuracy:0.5733, Validation Loss:1.2266, Validation Accuracy:0.3596\n",
    "Epoch #154: Loss:0.9352, Accuracy:0.5565, Validation Loss:1.2571, Validation Accuracy:0.3711\n",
    "Epoch #155: Loss:0.9139, Accuracy:0.5754, Validation Loss:1.2127, Validation Accuracy:0.3826\n",
    "Epoch #156: Loss:0.9176, Accuracy:0.5754, Validation Loss:1.2150, Validation Accuracy:0.3580\n",
    "Epoch #157: Loss:0.9077, Accuracy:0.5778, Validation Loss:1.2516, Validation Accuracy:0.3875\n",
    "Epoch #158: Loss:0.8953, Accuracy:0.5922, Validation Loss:1.2315, Validation Accuracy:0.3744\n",
    "Epoch #159: Loss:0.8899, Accuracy:0.5963, Validation Loss:1.2626, Validation Accuracy:0.3514\n",
    "Epoch #160: Loss:0.8872, Accuracy:0.5934, Validation Loss:1.2938, Validation Accuracy:0.3514\n",
    "Epoch #161: Loss:0.8851, Accuracy:0.5975, Validation Loss:1.2545, Validation Accuracy:0.3612\n",
    "Epoch #162: Loss:0.8885, Accuracy:0.5885, Validation Loss:1.3075, Validation Accuracy:0.3612\n",
    "Epoch #163: Loss:0.8832, Accuracy:0.5984, Validation Loss:1.2979, Validation Accuracy:0.3645\n",
    "Epoch #164: Loss:0.8749, Accuracy:0.6021, Validation Loss:1.3298, Validation Accuracy:0.3826\n",
    "Epoch #165: Loss:0.8941, Accuracy:0.5918, Validation Loss:1.2701, Validation Accuracy:0.3629\n",
    "Epoch #166: Loss:0.8808, Accuracy:0.5926, Validation Loss:1.2652, Validation Accuracy:0.3596\n",
    "Epoch #167: Loss:0.8919, Accuracy:0.5889, Validation Loss:1.2638, Validation Accuracy:0.3612\n",
    "Epoch #168: Loss:0.8781, Accuracy:0.6012, Validation Loss:1.3228, Validation Accuracy:0.3777\n",
    "Epoch #169: Loss:0.8780, Accuracy:0.6053, Validation Loss:1.2798, Validation Accuracy:0.3678\n",
    "Epoch #170: Loss:0.8787, Accuracy:0.5967, Validation Loss:1.2982, Validation Accuracy:0.3842\n",
    "Epoch #171: Loss:0.8761, Accuracy:0.5992, Validation Loss:1.3049, Validation Accuracy:0.3711\n",
    "Epoch #172: Loss:0.8698, Accuracy:0.6045, Validation Loss:1.2666, Validation Accuracy:0.3645\n",
    "Epoch #173: Loss:0.8752, Accuracy:0.6012, Validation Loss:1.3026, Validation Accuracy:0.3580\n",
    "Epoch #174: Loss:0.8649, Accuracy:0.6082, Validation Loss:1.2975, Validation Accuracy:0.3695\n",
    "Epoch #175: Loss:0.8762, Accuracy:0.5996, Validation Loss:1.2960, Validation Accuracy:0.3612\n",
    "Epoch #176: Loss:0.8738, Accuracy:0.6008, Validation Loss:1.4231, Validation Accuracy:0.3842\n",
    "Epoch #177: Loss:0.8800, Accuracy:0.5979, Validation Loss:1.3122, Validation Accuracy:0.3842\n",
    "Epoch #178: Loss:0.8642, Accuracy:0.5975, Validation Loss:1.3003, Validation Accuracy:0.3629\n",
    "Epoch #179: Loss:0.8540, Accuracy:0.6185, Validation Loss:1.3069, Validation Accuracy:0.3711\n",
    "Epoch #180: Loss:0.8666, Accuracy:0.6021, Validation Loss:1.3143, Validation Accuracy:0.3695\n",
    "Epoch #181: Loss:0.8688, Accuracy:0.5988, Validation Loss:1.3613, Validation Accuracy:0.3678\n",
    "Epoch #182: Loss:0.8792, Accuracy:0.5906, Validation Loss:1.3700, Validation Accuracy:0.3744\n",
    "Epoch #183: Loss:0.8768, Accuracy:0.5979, Validation Loss:1.3439, Validation Accuracy:0.3727\n",
    "Epoch #184: Loss:0.8693, Accuracy:0.5951, Validation Loss:1.2676, Validation Accuracy:0.3612\n",
    "Epoch #185: Loss:0.8644, Accuracy:0.6115, Validation Loss:1.3694, Validation Accuracy:0.3924\n",
    "Epoch #186: Loss:0.8686, Accuracy:0.6062, Validation Loss:1.2715, Validation Accuracy:0.3580\n",
    "Epoch #187: Loss:0.8526, Accuracy:0.6131, Validation Loss:1.3204, Validation Accuracy:0.3744\n",
    "Epoch #188: Loss:0.8435, Accuracy:0.6172, Validation Loss:1.3115, Validation Accuracy:0.3645\n",
    "Epoch #189: Loss:0.8574, Accuracy:0.6049, Validation Loss:1.3472, Validation Accuracy:0.3695\n",
    "Epoch #190: Loss:0.8513, Accuracy:0.6119, Validation Loss:1.3795, Validation Accuracy:0.3727\n",
    "Epoch #191: Loss:0.8424, Accuracy:0.6193, Validation Loss:1.3997, Validation Accuracy:0.3760\n",
    "Epoch #192: Loss:0.8389, Accuracy:0.6193, Validation Loss:1.3415, Validation Accuracy:0.3645\n",
    "Epoch #193: Loss:0.8325, Accuracy:0.6259, Validation Loss:1.3106, Validation Accuracy:0.3530\n",
    "Epoch #194: Loss:0.8412, Accuracy:0.6197, Validation Loss:1.3648, Validation Accuracy:0.3662\n",
    "Epoch #195: Loss:0.8537, Accuracy:0.6066, Validation Loss:1.3290, Validation Accuracy:0.3596\n",
    "Epoch #196: Loss:0.8304, Accuracy:0.6279, Validation Loss:1.3837, Validation Accuracy:0.3777\n",
    "Epoch #197: Loss:0.8471, Accuracy:0.6140, Validation Loss:1.3984, Validation Accuracy:0.3810\n",
    "Epoch #198: Loss:0.8330, Accuracy:0.6193, Validation Loss:1.3391, Validation Accuracy:0.3580\n",
    "Epoch #199: Loss:0.8284, Accuracy:0.6226, Validation Loss:1.3666, Validation Accuracy:0.3810\n",
    "Epoch #200: Loss:0.8256, Accuracy:0.6238, Validation Loss:1.3413, Validation Accuracy:0.3530\n",
    "Epoch #201: Loss:0.8242, Accuracy:0.6275, Validation Loss:1.3980, Validation Accuracy:0.3760\n",
    "Epoch #202: Loss:0.8197, Accuracy:0.6296, Validation Loss:1.4183, Validation Accuracy:0.3711\n",
    "Epoch #203: Loss:0.8119, Accuracy:0.6398, Validation Loss:1.3744, Validation Accuracy:0.3580\n",
    "Epoch #204: Loss:0.8253, Accuracy:0.6279, Validation Loss:1.3289, Validation Accuracy:0.3629\n",
    "Epoch #205: Loss:0.8197, Accuracy:0.6275, Validation Loss:1.3962, Validation Accuracy:0.3662\n",
    "Epoch #206: Loss:0.8165, Accuracy:0.6341, Validation Loss:1.4242, Validation Accuracy:0.3727\n",
    "Epoch #207: Loss:0.8095, Accuracy:0.6324, Validation Loss:1.4126, Validation Accuracy:0.3662\n",
    "Epoch #208: Loss:0.8218, Accuracy:0.6246, Validation Loss:1.3763, Validation Accuracy:0.3711\n",
    "Epoch #209: Loss:0.8209, Accuracy:0.6316, Validation Loss:1.3775, Validation Accuracy:0.3777\n",
    "Epoch #210: Loss:0.8297, Accuracy:0.6226, Validation Loss:1.3564, Validation Accuracy:0.3612\n",
    "Epoch #211: Loss:0.8396, Accuracy:0.6201, Validation Loss:1.4363, Validation Accuracy:0.3580\n",
    "Epoch #212: Loss:0.8250, Accuracy:0.6172, Validation Loss:1.3504, Validation Accuracy:0.3678\n",
    "Epoch #213: Loss:0.8089, Accuracy:0.6382, Validation Loss:1.5523, Validation Accuracy:0.3777\n",
    "Epoch #214: Loss:0.8394, Accuracy:0.6255, Validation Loss:1.3877, Validation Accuracy:0.3645\n",
    "Epoch #215: Loss:0.8098, Accuracy:0.6316, Validation Loss:1.3224, Validation Accuracy:0.3481\n",
    "Epoch #216: Loss:0.8178, Accuracy:0.6374, Validation Loss:1.4115, Validation Accuracy:0.3695\n",
    "Epoch #217: Loss:0.8055, Accuracy:0.6411, Validation Loss:1.4829, Validation Accuracy:0.3727\n",
    "Epoch #218: Loss:0.8261, Accuracy:0.6172, Validation Loss:1.4168, Validation Accuracy:0.3530\n",
    "Epoch #219: Loss:0.8184, Accuracy:0.6337, Validation Loss:1.4037, Validation Accuracy:0.3629\n",
    "Epoch #220: Loss:0.8059, Accuracy:0.6419, Validation Loss:1.4093, Validation Accuracy:0.3678\n",
    "Epoch #221: Loss:0.8103, Accuracy:0.6341, Validation Loss:1.3915, Validation Accuracy:0.3580\n",
    "Epoch #222: Loss:0.8081, Accuracy:0.6255, Validation Loss:1.4139, Validation Accuracy:0.3695\n",
    "Epoch #223: Loss:0.8031, Accuracy:0.6370, Validation Loss:1.4411, Validation Accuracy:0.3678\n",
    "Epoch #224: Loss:0.7857, Accuracy:0.6468, Validation Loss:1.4190, Validation Accuracy:0.3514\n",
    "Epoch #225: Loss:0.7830, Accuracy:0.6501, Validation Loss:1.4424, Validation Accuracy:0.3563\n",
    "Epoch #226: Loss:0.7869, Accuracy:0.6509, Validation Loss:1.4590, Validation Accuracy:0.3777\n",
    "Epoch #227: Loss:0.8044, Accuracy:0.6419, Validation Loss:1.4916, Validation Accuracy:0.3695\n",
    "Epoch #228: Loss:0.7997, Accuracy:0.6431, Validation Loss:1.4052, Validation Accuracy:0.3333\n",
    "Epoch #229: Loss:0.7761, Accuracy:0.6616, Validation Loss:1.4714, Validation Accuracy:0.3727\n",
    "Epoch #230: Loss:0.7766, Accuracy:0.6571, Validation Loss:1.4722, Validation Accuracy:0.3678\n",
    "Epoch #231: Loss:0.7710, Accuracy:0.6542, Validation Loss:1.4676, Validation Accuracy:0.3498\n",
    "Epoch #232: Loss:0.7674, Accuracy:0.6616, Validation Loss:1.4277, Validation Accuracy:0.3596\n",
    "Epoch #233: Loss:0.7781, Accuracy:0.6542, Validation Loss:1.5343, Validation Accuracy:0.3448\n",
    "Epoch #234: Loss:0.7641, Accuracy:0.6665, Validation Loss:1.5174, Validation Accuracy:0.3612\n",
    "Epoch #235: Loss:0.7650, Accuracy:0.6530, Validation Loss:1.5325, Validation Accuracy:0.3530\n",
    "Epoch #236: Loss:0.7625, Accuracy:0.6669, Validation Loss:1.4305, Validation Accuracy:0.3514\n",
    "Epoch #237: Loss:0.7702, Accuracy:0.6628, Validation Loss:1.4534, Validation Accuracy:0.3465\n",
    "Epoch #238: Loss:0.7632, Accuracy:0.6628, Validation Loss:1.6498, Validation Accuracy:0.3793\n",
    "Epoch #239: Loss:0.7738, Accuracy:0.6554, Validation Loss:1.4442, Validation Accuracy:0.3530\n",
    "Epoch #240: Loss:0.7645, Accuracy:0.6583, Validation Loss:1.5508, Validation Accuracy:0.3629\n",
    "Epoch #241: Loss:0.7923, Accuracy:0.6439, Validation Loss:1.4117, Validation Accuracy:0.3744\n",
    "Epoch #242: Loss:0.8278, Accuracy:0.6218, Validation Loss:1.3401, Validation Accuracy:0.3530\n",
    "Epoch #243: Loss:0.8518, Accuracy:0.6012, Validation Loss:1.3806, Validation Accuracy:0.3957\n",
    "Epoch #244: Loss:0.7973, Accuracy:0.6353, Validation Loss:1.4530, Validation Accuracy:0.3760\n",
    "Epoch #245: Loss:0.7722, Accuracy:0.6542, Validation Loss:1.3701, Validation Accuracy:0.3580\n",
    "Epoch #246: Loss:0.7843, Accuracy:0.6476, Validation Loss:1.4484, Validation Accuracy:0.3596\n",
    "Epoch #247: Loss:0.7860, Accuracy:0.6394, Validation Loss:1.5444, Validation Accuracy:0.3810\n",
    "Epoch #248: Loss:0.7700, Accuracy:0.6538, Validation Loss:1.5052, Validation Accuracy:0.3744\n",
    "Epoch #249: Loss:0.7791, Accuracy:0.6501, Validation Loss:1.4244, Validation Accuracy:0.3760\n",
    "Epoch #250: Loss:0.8163, Accuracy:0.6197, Validation Loss:1.4061, Validation Accuracy:0.3612\n",
    "Epoch #251: Loss:0.7871, Accuracy:0.6448, Validation Loss:1.4371, Validation Accuracy:0.3793\n",
    "Epoch #252: Loss:0.7594, Accuracy:0.6632, Validation Loss:1.4661, Validation Accuracy:0.3629\n",
    "Epoch #253: Loss:0.7590, Accuracy:0.6579, Validation Loss:1.4926, Validation Accuracy:0.3563\n",
    "Epoch #254: Loss:0.7541, Accuracy:0.6698, Validation Loss:1.5112, Validation Accuracy:0.3432\n",
    "Epoch #255: Loss:0.7443, Accuracy:0.6715, Validation Loss:1.5078, Validation Accuracy:0.3760\n",
    "Epoch #256: Loss:0.7415, Accuracy:0.6752, Validation Loss:1.5547, Validation Accuracy:0.3498\n",
    "Epoch #257: Loss:0.7423, Accuracy:0.6719, Validation Loss:1.4809, Validation Accuracy:0.3793\n",
    "Epoch #258: Loss:0.7539, Accuracy:0.6649, Validation Loss:1.4780, Validation Accuracy:0.3481\n",
    "Epoch #259: Loss:0.7435, Accuracy:0.6793, Validation Loss:1.5871, Validation Accuracy:0.3481\n",
    "Epoch #260: Loss:0.7368, Accuracy:0.6723, Validation Loss:1.5515, Validation Accuracy:0.3563\n",
    "Epoch #261: Loss:0.7333, Accuracy:0.6801, Validation Loss:1.5275, Validation Accuracy:0.3481\n",
    "Epoch #262: Loss:0.7453, Accuracy:0.6756, Validation Loss:1.4961, Validation Accuracy:0.3514\n",
    "Epoch #263: Loss:0.7336, Accuracy:0.6850, Validation Loss:1.6212, Validation Accuracy:0.3727\n",
    "Epoch #264: Loss:0.7339, Accuracy:0.6838, Validation Loss:1.5529, Validation Accuracy:0.3547\n",
    "Epoch #265: Loss:0.7278, Accuracy:0.6747, Validation Loss:1.5081, Validation Accuracy:0.3432\n",
    "Epoch #266: Loss:0.7272, Accuracy:0.6825, Validation Loss:1.5753, Validation Accuracy:0.3580\n",
    "Epoch #267: Loss:0.7191, Accuracy:0.6862, Validation Loss:1.5527, Validation Accuracy:0.3777\n",
    "Epoch #268: Loss:0.7295, Accuracy:0.6780, Validation Loss:1.5809, Validation Accuracy:0.3580\n",
    "Epoch #269: Loss:0.7354, Accuracy:0.6727, Validation Loss:1.6045, Validation Accuracy:0.3580\n",
    "Epoch #270: Loss:0.7617, Accuracy:0.6641, Validation Loss:1.4517, Validation Accuracy:0.3580\n",
    "Epoch #271: Loss:0.7557, Accuracy:0.6575, Validation Loss:1.4812, Validation Accuracy:0.3678\n",
    "Epoch #272: Loss:0.7557, Accuracy:0.6587, Validation Loss:1.5040, Validation Accuracy:0.3842\n",
    "Epoch #273: Loss:0.7359, Accuracy:0.6834, Validation Loss:1.5567, Validation Accuracy:0.3448\n",
    "Epoch #274: Loss:0.7271, Accuracy:0.6789, Validation Loss:1.5726, Validation Accuracy:0.3481\n",
    "Epoch #275: Loss:0.7196, Accuracy:0.6752, Validation Loss:1.5240, Validation Accuracy:0.3547\n",
    "Epoch #276: Loss:0.7193, Accuracy:0.6924, Validation Loss:1.5281, Validation Accuracy:0.3498\n",
    "Epoch #277: Loss:0.7168, Accuracy:0.6899, Validation Loss:1.7134, Validation Accuracy:0.3695\n",
    "Epoch #278: Loss:0.7198, Accuracy:0.6789, Validation Loss:1.6381, Validation Accuracy:0.3580\n",
    "Epoch #279: Loss:0.7063, Accuracy:0.6887, Validation Loss:1.5223, Validation Accuracy:0.3580\n",
    "Epoch #280: Loss:0.7152, Accuracy:0.6842, Validation Loss:1.5243, Validation Accuracy:0.3350\n",
    "Epoch #281: Loss:0.7129, Accuracy:0.6953, Validation Loss:1.6076, Validation Accuracy:0.3711\n",
    "Epoch #282: Loss:0.7164, Accuracy:0.6850, Validation Loss:1.6932, Validation Accuracy:0.3695\n",
    "Epoch #283: Loss:0.7110, Accuracy:0.6932, Validation Loss:1.7074, Validation Accuracy:0.3596\n",
    "Epoch #284: Loss:0.7203, Accuracy:0.6772, Validation Loss:1.5631, Validation Accuracy:0.3563\n",
    "Epoch #285: Loss:0.7063, Accuracy:0.6895, Validation Loss:1.4757, Validation Accuracy:0.3514\n",
    "Epoch #286: Loss:0.7133, Accuracy:0.6883, Validation Loss:1.6768, Validation Accuracy:0.3727\n",
    "Epoch #287: Loss:0.7007, Accuracy:0.6920, Validation Loss:1.6535, Validation Accuracy:0.3842\n",
    "Epoch #288: Loss:0.7399, Accuracy:0.6702, Validation Loss:1.6823, Validation Accuracy:0.3530\n",
    "Epoch #289: Loss:0.7396, Accuracy:0.6645, Validation Loss:1.4696, Validation Accuracy:0.3629\n",
    "Epoch #290: Loss:0.7180, Accuracy:0.6932, Validation Loss:1.5860, Validation Accuracy:0.3777\n",
    "Epoch #291: Loss:0.7007, Accuracy:0.6936, Validation Loss:1.6348, Validation Accuracy:0.3629\n",
    "Epoch #292: Loss:0.6926, Accuracy:0.6982, Validation Loss:1.6454, Validation Accuracy:0.3448\n",
    "Epoch #293: Loss:0.6963, Accuracy:0.7043, Validation Loss:1.7004, Validation Accuracy:0.3580\n",
    "Epoch #294: Loss:0.6880, Accuracy:0.7055, Validation Loss:1.7018, Validation Accuracy:0.3514\n",
    "Epoch #295: Loss:0.6896, Accuracy:0.6973, Validation Loss:1.6199, Validation Accuracy:0.3596\n",
    "Epoch #296: Loss:0.6961, Accuracy:0.6932, Validation Loss:1.6732, Validation Accuracy:0.3580\n",
    "Epoch #297: Loss:0.6803, Accuracy:0.7084, Validation Loss:1.6435, Validation Accuracy:0.3530\n",
    "Epoch #298: Loss:0.6780, Accuracy:0.7076, Validation Loss:1.6498, Validation Accuracy:0.3530\n",
    "Epoch #299: Loss:0.6741, Accuracy:0.7105, Validation Loss:1.6730, Validation Accuracy:0.3563\n",
    "Epoch #300: Loss:0.6766, Accuracy:0.7084, Validation Loss:1.7475, Validation Accuracy:0.3547\n",
    "\n",
    "Test:\n",
    "Test Loss:1.74750233, Accuracy:0.3547\n",
    "Labels: ['03', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03  02   01\n",
    "t:03  20  48   74\n",
    "t:02  39  80  108\n",
    "t:01  44  80  116\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.19      0.14      0.16       142\n",
    "          02       0.38      0.35      0.37       227\n",
    "          01       0.39      0.48      0.43       240\n",
    "\n",
    "    accuracy                           0.35       609\n",
    "   macro avg       0.32      0.33      0.32       609\n",
    "weighted avg       0.34      0.35      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 06:33:09 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.080645768904725, 1.074982065089622, 1.0749736139535513, 1.0754283962187117, 1.074778320949849, 1.0742360177298484, 1.074180548218475, 1.0741477106592339, 1.0742127969738688, 1.0741530556984136, 1.0739926589142121, 1.0742255386656336, 1.074012553554842, 1.0738671953454981, 1.0738473091219447, 1.073670545822294, 1.0736344046584883, 1.0734054785839637, 1.0733210343641209, 1.073297001458154, 1.0732812405807044, 1.0734109952923503, 1.0731365956696384, 1.0730480887423988, 1.072980723749045, 1.0729794341746615, 1.0728213994569575, 1.0728543750171005, 1.0727063953778622, 1.072629275971837, 1.0726466503832337, 1.0727300647835818, 1.072417617039923, 1.0723540741821815, 1.0725662209130273, 1.0723579613054524, 1.0722082405059012, 1.072255181953042, 1.0721853713096656, 1.0720946683085024, 1.0719937647896252, 1.071923412517178, 1.0719057761976871, 1.071836969926831, 1.0721930517938925, 1.071847555672594, 1.072134988257059, 1.071893367665546, 1.0717229506456598, 1.0716687223594177, 1.071604575429644, 1.0715484331394065, 1.071866347088994, 1.071611449440516, 1.0714585622543185, 1.0717043029067943, 1.0717822867269782, 1.0716905725021864, 1.0716477227328447, 1.0718350020926966, 1.0720972599850107, 1.073649292508957, 1.073629661733881, 1.0733105795724052, 1.0761583628521372, 1.0772061398855375, 1.077045280162141, 1.0760130569069648, 1.0816686253242305, 1.0779291622352913, 1.0878022734950525, 1.0789282359121664, 1.0907071959991956, 1.0832120614686036, 1.078621156109965, 1.07940450521134, 1.0806870812852982, 1.0829687273169581, 1.0882923826208255, 1.0934623289969558, 1.0873551834588764, 1.0922368193299117, 1.0892301322204139, 1.0909597090703904, 1.097094091484308, 1.0930377317375346, 1.0940297746110237, 1.1136399523182259, 1.0948284089271658, 1.0955214157871815, 1.1007376210442905, 1.0974068164042456, 1.09520850451709, 1.1052952822793294, 1.1013199014616717, 1.1083223383219176, 1.1062975636452486, 1.1065192048381312, 1.1036120923281891, 1.10820017874926, 1.1167858406436464, 1.120849891444928, 1.1196396309753944, 1.1234840563756883, 1.1232478082277897, 1.1188658331023844, 1.1196302682503887, 1.1262643879466065, 1.1207272152986825, 1.1734453034518388, 1.1233289666559505, 1.133117100288128, 1.1329396358264492, 1.1393291834735715, 1.147177814850079, 1.1534317749474436, 1.1361524757297559, 1.1349757101343967, 1.1353955552691506, 1.1430080851114834, 1.1532632823060887, 1.1408638885651512, 1.1693820624515927, 1.1707498416524802, 1.1670119496206148, 1.1605236818050515, 1.1610924442021913, 1.188618306064449, 1.1653414130798114, 1.167881917092209, 1.195610405776301, 1.1824204483251461, 1.1948381022279486, 1.2003124517760253, 1.230820797542829, 1.217843059640017, 1.2303804566316026, 1.2260947587650592, 1.2144739297027463, 1.1933389962796115, 1.2244126160548043, 1.2197766108270154, 1.2340594426360232, 1.239389259043977, 1.207931627780933, 1.239945455920716, 1.202301746518741, 1.232284728725164, 1.239135224635182, 1.2416650172329105, 1.2544222659078137, 1.2680021722132742, 1.2265671495537844, 1.257140880147812, 1.2126890546191111, 1.2150057012028685, 1.251565065289953, 1.231532550406182, 1.2625687165409083, 1.2937524672995255, 1.2544820248004056, 1.3074577678796302, 1.297896209804491, 1.3298469719236903, 1.2700630444023997, 1.2652392534199606, 1.263840027434877, 1.322839267148173, 1.2798036055024622, 1.2981788640343301, 1.3048504859160124, 1.266581863996822, 1.3025936555784128, 1.297496502622595, 1.2959588135796032, 1.4231172571041313, 1.312164087013658, 1.3002961836816447, 1.3069061882585924, 1.3142691932875534, 1.3613125387279468, 1.3699695378884502, 1.3438987107503981, 1.2676364821557733, 1.3693639431485205, 1.2714527132867397, 1.3203654935207274, 1.3115496750927127, 1.3471630987862648, 1.3794710546095774, 1.3996970667236153, 1.341496340942696, 1.310573965262114, 1.364829152871431, 1.3289711900141048, 1.3837005935474764, 1.398359707032127, 1.339139909971328, 1.3665725184583115, 1.3413364771747434, 1.398011800299333, 1.4183178889536114, 1.3744418955788824, 1.328949798308374, 1.39620909686942, 1.4242377876256682, 1.4125942808066683, 1.3763409876471082, 1.377495429981714, 1.3563941674083715, 1.4363061369737773, 1.3504105518604148, 1.55234879656574, 1.3876884241996728, 1.322386827766406, 1.4115118714193209, 1.48292831344949, 1.4167555887710872, 1.4037354814595189, 1.4092715324830931, 1.391523339869745, 1.4139378922326225, 1.441067469922584, 1.4190003039997394, 1.4424295075029772, 1.4589717765942778, 1.4916475122589588, 1.4051707844037336, 1.4713527079677737, 1.472197501334455, 1.467635876635221, 1.42767935687881, 1.5342981325973235, 1.5174213211328917, 1.5325355907574858, 1.4304871915400714, 1.4534420939697617, 1.6498357464723008, 1.4441946750791201, 1.5508376563515374, 1.4116562947459605, 1.3401489443771162, 1.3805632366139704, 1.4529655635454777, 1.3701028581127548, 1.4484026921402253, 1.5443832911489828, 1.5052176974285607, 1.424371503061066, 1.4061158053784926, 1.4371174049299142, 1.466141782762186, 1.4926112580964914, 1.5112235873007813, 1.5077506591533791, 1.5547131082694519, 1.480882148437312, 1.4779609228394106, 1.5870537182380413, 1.5515360982938744, 1.5274834108274362, 1.4960840077235782, 1.621156307081088, 1.5528634187623198, 1.5081439914766008, 1.575304583962915, 1.5527083353064526, 1.5809190326136322, 1.6045277324216118, 1.45169540813991, 1.4812443869063028, 1.5040473573900797, 1.5566546265127623, 1.5725780124539028, 1.5240220448066448, 1.5281273586605177, 1.713432091601768, 1.638100860741338, 1.5223080976843246, 1.524278940238389, 1.6076078452108724, 1.693217669214521, 1.7074287366397276, 1.5630788078840534, 1.4757156501262647, 1.6768498195607477, 1.653477181746259, 1.6822679624181662, 1.4695929396524414, 1.5860017170068275, 1.63481714259619, 1.6453771965061306, 1.7004472532100083, 1.7017581466775027, 1.6198869101911146, 1.6732415017627535, 1.6435234897046644, 1.6498140501858565, 1.6729699504395033, 1.7475023657230322], 'val_acc': [0.37274219920286794, 0.39408866892307265, 0.39408866892307265, 0.38095237963109574, 0.40065681332437864, 0.3990147770038379, 0.39737274107478915, 0.39573070475424843, 0.3957307048521214, 0.3875205242281477, 0.3957307048521214, 0.39408866892307265, 0.39737274107478915, 0.3990147771017109, 0.38423645178281224, 0.3990147770038379, 0.3957307049499944, 0.3924466327004049, 0.4039408855739681, 0.3973727412705351, 0.4022988493513004, 0.39737274117266214, 0.3875205242281477, 0.39408866882519966, 0.38916256035294244, 0.4072249578235576, 0.3875205242281477, 0.40558292179663585, 0.3842364519785582, 0.38587848800547997, 0.39737274107478915, 0.40065681332437864, 0.40886699404622534, 0.4121510663936878, 0.4154351385454043, 0.39408866882519966, 0.3957307050478674, 0.39244663260253193, 0.38587848800547997, 0.4121510662958148, 0.4072249579214306, 0.41379310251848256, 0.4154351387411503, 0.41050903046463905, 0.39573070475424843, 0.4072249580193036, 0.40558292189450884, 0.40558292179663585, 0.40558292189450884, 0.40558292199238183, 0.41379310251848256, 0.4055829216008899, 0.39901477739532987, 0.4072249578235576, 0.39408866882519966, 0.40229884954704637, 0.39737274107478915, 0.3990147772974569, 0.4006568134222516, 0.3957307048521214, 0.4039408855739681, 0.39737274136840806, 0.39573070475424843, 0.41543513825178535, 0.4072249577256846, 0.3875205243260207, 0.4072249579214306, 0.40394088508460324, 0.3990147771017109, 0.3957307048521214, 0.39244663260253193, 0.41215106600219586, 0.3825944156580175, 0.3908045961841182, 0.4006568129328867, 0.39080459598837225, 0.3973727404875513, 0.40065681273714077, 0.40394088547609514, 0.3957307048521214, 0.4006568129328867, 0.38423645168493925, 0.3940886685315807, 0.38752052354303684, 0.39244663240678596, 0.39080459598837225, 0.3891625598635775, 0.385878487809734, 0.38752052364090983, 0.385878487809734, 0.3825944155601445, 0.39573070455850246, 0.39244663240678596, 0.3743842349361708, 0.3809523792396038, 0.4039408855739681, 0.3891625599614505, 0.38095237953322275, 0.38752052393452874, 0.3990147771017109, 0.40394088547609514, 0.41215106580644995, 0.3973727408790432, 0.3957307048521214, 0.39080459608624524, 0.3661740536270862, 0.3694581261702946, 0.3678160899476269, 0.38423645178281224, 0.40886699404622534, 0.37931034311480905, 0.36781609014337285, 0.3793103429190631, 0.3957307048521214, 0.3743842344468059, 0.3908045964777372, 0.3727421985177571, 0.38095237953322275, 0.3842364514891933, 0.3645320176980374, 0.3776683065006494, 0.379310343506301, 0.3694581259745487, 0.3661740542143241, 0.38095237982684166, 0.3678160899476269, 0.36617405382283214, 0.3891625598635775, 0.37110016239296234, 0.38095237943534976, 0.3825944155601445, 0.379310343506301, 0.38095237963109574, 0.3678160904369918, 0.3743842349361708, 0.3760262711588385, 0.3694581261702946, 0.3760262712567115, 0.36945812685540547, 0.372742199007122, 0.37766830738150625, 0.36945812695327845, 0.3727421993986139, 0.3809523793374768, 0.36453201808952934, 0.3809523793374768, 0.3579638737860963, 0.38259441585376347, 0.38095237982684166, 0.37274219930074093, 0.35960591040025597, 0.3760262716482034, 0.35960590991089103, 0.3711001628823273, 0.3825944155601445, 0.35796387398184226, 0.3875205240324017, 0.37438423552340866, 0.35139572977628225, 0.3513957294826633, 0.3612479465250507, 0.3612479465250507, 0.3645320185788942, 0.3825944157558905, 0.36288998255197247, 0.35960591049812896, 0.36124794672079663, 0.37766830787087113, 0.3678160910242297, 0.3842364519785582, 0.37110016337169216, 0.3645320186767672, 0.3579638742754612, 0.36945812705115144, 0.36124794623143175, 0.38423645178281224, 0.3842364519785582, 0.36288998274771844, 0.37110016356743813, 0.36945812695327845, 0.36781609033911883, 0.37438423513191676, 0.37274219881137605, 0.36124794613355876, 0.3924466327982779, 0.3579638742754612, 0.37438423562128165, 0.3645320186767672, 0.36945812685540547, 0.372742199007122, 0.37602627106096553, 0.36453201848102124, 0.353037765705331, 0.36617405489943494, 0.359605910106637, 0.37766830747937924, 0.38095237982684166, 0.35796387398184226, 0.3809523802183336, 0.35303776550958504, 0.37602627135458444, 0.3711001630780732, 0.3579638738839693, 0.36288998264984546, 0.36617405470368897, 0.372742199007122, 0.366174054507943, 0.3711001632738192, 0.37766830767512516, 0.3612479466229237, 0.3579638737860963, 0.3678160908284837, 0.3776683071857603, 0.3645320187746402, 0.3481116570373278, 0.36945812714902443, 0.37274219881137605, 0.35303776541171206, 0.3628899824540995, 0.3678160908284837, 0.35796387398184226, 0.36945812714902443, 0.3678160908284837, 0.3513957293847903, 0.3563218378570475, 0.37766830787087113, 0.36945812695327845, 0.33333333181630215, 0.37274219910499495, 0.3678160908284837, 0.34975369325999556, 0.359605910106637, 0.3448275844941194, 0.36124794632930474, 0.35303776550958504, 0.3513957295805363, 0.3464696209125331, 0.379310343506301, 0.35303776560745803, 0.3628899821604805, 0.37438423552340866, 0.353037765705331, 0.3957307054393593, 0.3760262712567115, 0.3579638742754612, 0.359605910008764, 0.38095237982684166, 0.3743842352297897, 0.3760262716482034, 0.3612479464271777, 0.3793103438977929, 0.3628899824540995, 0.35632183775917453, 0.3431855485650706, 0.37602627135458444, 0.3497536930642496, 0.3793103438977929, 0.3481116572330738, 0.34811165684158185, 0.35632183775917453, 0.3481116571352008, 0.3513957294826633, 0.37274219920286794, 0.3546798015365068, 0.34318554895656256, 0.3579638738839693, 0.37766830777299815, 0.35796387398184226, 0.3579638737860963, 0.35796387398184226, 0.3678160908284837, 0.38423645217430413, 0.3448275849834843, 0.3481116570373278, 0.35467980183012576, 0.34975369325999556, 0.36945812646391357, 0.3579638738839693, 0.35796387398184226, 0.33497536813684287, 0.3711001630780732, 0.36945812695327845, 0.3596059098130181, 0.3563218379549205, 0.3513957295805363, 0.37274219910499495, 0.3842364523700501, 0.35303776521596614, 0.36288998264984546, 0.3776683079687441, 0.3628899823562265, 0.34482758478773834, 0.3579638736882233, 0.3513957292869173, 0.359605910106637, 0.3579638737860963, 0.35303776550958504, 0.353037765705331, 0.3563218379549205, 0.3546798014386338], 'loss': [1.0953087122778138, 1.0777911892172247, 1.0745707521693173, 1.0753756554954106, 1.0751393240335296, 1.074464855448666, 1.0741882030968792, 1.074095827108536, 1.0741077619411616, 1.074211129809307, 1.0741307240981586, 1.0740425462840275, 1.074087765280471, 1.0738528475869118, 1.0739092787188427, 1.0737138575352192, 1.0736598284582337, 1.0735925322440616, 1.07368892835151, 1.0735880684803643, 1.0731824895439697, 1.073550639553971, 1.073316320354689, 1.073308629372771, 1.0732261513782477, 1.0732292068567608, 1.0730070634544262, 1.0730402868631195, 1.0729367274768054, 1.0729740795413572, 1.0727543128834123, 1.0728496770349616, 1.0726378046756408, 1.0728639194852763, 1.0732073014766528, 1.072648735212839, 1.0726441132214524, 1.0722605304306783, 1.0725114363174908, 1.0721863961072917, 1.0720291945968565, 1.071924831539209, 1.0718472040654208, 1.0718852230166018, 1.0718668093671544, 1.07143043894053, 1.0712515106436165, 1.0712097302599364, 1.0713267926562737, 1.0708822003135445, 1.0707501870650775, 1.0701217873630093, 1.0698263073359182, 1.0694603232632427, 1.0692471802846606, 1.0692180319733198, 1.0683658577823052, 1.0687937819247864, 1.0673613585485815, 1.066903500684233, 1.066416120039609, 1.0656706175030624, 1.065289554067216, 1.0631670296804125, 1.0621726485982812, 1.0612254842840425, 1.0593877754661827, 1.0596353889735572, 1.056227973109643, 1.057487943040272, 1.0547274054443077, 1.0552766151741544, 1.0541743101770138, 1.059640450937792, 1.055583812519755, 1.051279188915934, 1.0484349616988728, 1.0468802454045665, 1.044433330657301, 1.0461346403529268, 1.0458263904896605, 1.0424144093260872, 1.0396968137067446, 1.0373305461734716, 1.0381801781468323, 1.042206670616197, 1.0371653181816274, 1.0342188735272606, 1.0411596962069094, 1.0409431440874293, 1.035866542665376, 1.0279085983730685, 1.027499266033055, 1.026059250958891, 1.0280149589084258, 1.0249665965289796, 1.0289401576749109, 1.019570601108872, 1.017009512550777, 1.0165251813630058, 1.0120954292266031, 1.01426611940474, 1.0101354737546164, 1.0059763864570086, 1.0061244086808003, 1.0033505977301627, 1.005899851767679, 1.0029220408727502, 0.9979395398858636, 1.0008463608410814, 1.002156567035025, 1.0010182967910533, 0.9928532182803144, 0.9882919126467539, 0.990727623581152, 0.9927598041185853, 0.9927536972004775, 0.9894208572728433, 0.9820683196829575, 0.9768971220913364, 0.9784110244539485, 0.9780501207042279, 0.9709132640513551, 0.9733696302349317, 0.9826589357926371, 0.9694326636482803, 0.961124141847818, 0.9684875781531207, 0.9669388087133607, 0.9582447234854806, 0.9488111436489426, 0.946809777439987, 0.9460807779241637, 0.9392410539503705, 0.9384149259610343, 0.9452616628435359, 0.9383853074461528, 0.9431564788553993, 0.9350044996097102, 0.9306253519880698, 0.9274517108772324, 0.9242795625506486, 0.9179838425814494, 0.9378644826231062, 0.9373668619984229, 0.926391577549294, 0.9343454310291847, 0.9146411603970694, 0.9139789790588237, 0.9030684652269744, 0.9014646919111452, 0.904979240086534, 0.9287724701041313, 0.9351779789160899, 0.913919383887148, 0.9176231900279772, 0.907650461848022, 0.8952989552789645, 0.8899148011354451, 0.887159809121361, 0.8850707495237033, 0.88846153390481, 0.883248292004548, 0.8749269306537307, 0.8941001290658172, 0.8808370257794734, 0.8919350655171906, 0.8780508398520138, 0.8779655947087971, 0.8787406116295644, 0.8760904711619539, 0.8698307966304756, 0.8752039109656943, 0.8648763018956664, 0.8762262799411829, 0.8737888299219417, 0.8800385472955645, 0.86422874464881, 0.8539885044587466, 0.8666309795340473, 0.8688277558624378, 0.8791936944397568, 0.8768269269618165, 0.869262009846846, 0.8643981103290033, 0.8686321954218025, 0.8526161166187185, 0.8435066105648722, 0.857362240785446, 0.851338696283971, 0.8424257600821509, 0.8389333843940092, 0.8325086528270886, 0.8411500233889115, 0.8537366281789431, 0.8304245880741847, 0.8470789985979852, 0.8329930105738081, 0.8284454661962678, 0.8256193960716592, 0.8241909761203633, 0.8196590757712692, 0.8118683917811275, 0.8253165385316774, 0.8197094262992577, 0.8165000439424535, 0.8095174613184998, 0.8217900667347213, 0.8208988237674721, 0.8296984522249664, 0.8395824051001234, 0.8249962005037547, 0.8089319275879517, 0.839359202179331, 0.8097550050189118, 0.8178303647824626, 0.8054531461404334, 0.8261342279720111, 0.8183584616659113, 0.805894822384053, 0.8103163112115567, 0.8081071507269841, 0.8030567881018229, 0.7856947027927061, 0.783044517309514, 0.7869276339023754, 0.8044378863223035, 0.7996521708656875, 0.7761036836146329, 0.7766231507980847, 0.7709984413896988, 0.7674267790890328, 0.7781475707980396, 0.7641062516940937, 0.7650413212345366, 0.7625297169910564, 0.7701700670028859, 0.7631823646214464, 0.7737848393726153, 0.7644841107989239, 0.7923408403044119, 0.8277869731493799, 0.8517600227919937, 0.7973382397843582, 0.7722425898487318, 0.7842501832719212, 0.7859908668167537, 0.7699598247265669, 0.7790896177781436, 0.8162775292044058, 0.7871054799894532, 0.7593787968770679, 0.7589904772427537, 0.7540782058508244, 0.744317928158527, 0.7414909586279789, 0.7423495295845752, 0.7538771648426565, 0.7435201458617647, 0.7368490434525194, 0.7332626609097271, 0.7453115257883953, 0.7335831511192008, 0.7339483653740226, 0.7278065902251727, 0.7272253235997116, 0.719089281681382, 0.729450917366349, 0.7354025103473076, 0.7617248920199807, 0.755650038836673, 0.7557240680747453, 0.7359413724905167, 0.7271368193185794, 0.7196446821430136, 0.7193302115620529, 0.7168442535694131, 0.719807262611585, 0.7062869874848478, 0.7152126561444888, 0.7128977773370684, 0.716376157291616, 0.7110386153755737, 0.7203079849787561, 0.7062931361874026, 0.7132743690048156, 0.7006806486196342, 0.7399405162192468, 0.7395684267216395, 0.7180346592740602, 0.7006969957381058, 0.6925866687811866, 0.6963011434190817, 0.6879652879321355, 0.6896386582748602, 0.6960888794070641, 0.680313801520659, 0.6780155353967169, 0.6741470161894264, 0.6765778675950773], 'acc': [0.372895278357872, 0.387268992444573, 0.3942505146933287, 0.3967145790309632, 0.3938398357044745, 0.3991786452901437, 0.3930184806641612, 0.394661189765656, 0.39671457981426855, 0.4008213536083331, 0.40410677811448337, 0.3950718677753785, 0.3942505146933287, 0.39507186895033664, 0.3975359330554273, 0.4032854211159066, 0.39219712679880603, 0.39630390337115684, 0.4008213575615775, 0.3987679672804212, 0.4065708404571369, 0.39425051171921605, 0.396303900629588, 0.40287474111120314, 0.4008213559949667, 0.40205338708673904, 0.40287474545610025, 0.4016427082569937, 0.40451745416594237, 0.4004106765777423, 0.4057494883909363, 0.3999999980172582, 0.40082135521166135, 0.40574948901513275, 0.39917864708929823, 0.406981518271033, 0.402464066467246, 0.4016427086486464, 0.4078028756612625, 0.4024640672505514, 0.404106774748718, 0.4078028764812853, 0.40739219510579744, 0.4053388097937347, 0.40657084006548416, 0.4123203263268089, 0.4098562617933481, 0.41273100672316504, 0.41396303914900434, 0.41190965184196066, 0.40985626198917446, 0.4143737153962897, 0.41355236274261004, 0.41889117256082303, 0.4127310065273387, 0.41232033008422697, 0.42258726994849327, 0.4156057502087626, 0.42053387888403154, 0.4143737187620551, 0.42792607777172537, 0.4217659119340673, 0.42628336671196704, 0.42464065643551413, 0.4275153983912184, 0.42751540093696094, 0.43326488758993836, 0.43408624106364085, 0.4406570842868249, 0.43942505303594365, 0.4517453811863854, 0.4431211488570031, 0.45174537860392544, 0.4394250522526383, 0.4365503087303232, 0.4546201215754789, 0.46160164280838545, 0.4652977390210976, 0.47022587220037254, 0.4574948658443819, 0.46119096323205216, 0.46242299569460893, 0.47310061435190315, 0.4776180709289574, 0.47597535767839183, 0.4726899381046178, 0.47967145843182746, 0.4792607799937348, 0.4648870653562722, 0.45831622225547963, 0.47597536045667815, 0.4948665288929087, 0.4870636529746242, 0.4841889110189199, 0.4817248480887873, 0.48377823300919737, 0.47720739033677495, 0.5006160179325198, 0.49938398308332943, 0.4965092405401461, 0.49938398308332943, 0.49733059675541746, 0.5022587277806025, 0.5108829581517214, 0.5026694050437371, 0.5125256670573899, 0.5084188934591517, 0.5112936328323959, 0.5133470211185713, 0.5125256713655695, 0.5178644745256866, 0.51786447347312, 0.5240246442798716, 0.5314168417967811, 0.5207392207895706, 0.5182751550811517, 0.49897330370282245, 0.5248459952078316, 0.534702260783076, 0.5363449696887446, 0.5371663257081896, 0.5252566749799913, 0.5474332669432403, 0.5293634493982523, 0.5215605708607903, 0.5388090369637742, 0.5474332645933241, 0.53182751784824, 0.543326492916632, 0.5490759716631206, 0.5581108869713189, 0.5564681768906924, 0.563860367088592, 0.5683778276188908, 0.5638603708827276, 0.5646817203664681, 0.5655030759942605, 0.544147839267151, 0.5613963081117038, 0.5716632396778287, 0.5679671426818111, 0.5806981506044125, 0.5778234114637121, 0.5466119134695379, 0.5626283328635981, 0.5622176548538756, 0.5626283384201708, 0.578644759968321, 0.5794661227192967, 0.5921971239103674, 0.5934291565687505, 0.5831622145503943, 0.5733059543358962, 0.5564681683967245, 0.5753593394154151, 0.5753593411778523, 0.577823405123834, 0.5921971227354093, 0.5963038983286284, 0.593429158527014, 0.597535929616227, 0.5885010249560864, 0.5983572903355044, 0.6020533896814382, 0.5917864472714293, 0.5926077981993892, 0.5889117006158927, 0.6012320311162506, 0.6053388084719068, 0.59671457692583, 0.5991786426342488, 0.6045174516691564, 0.6012320311162506, 0.60821355316918, 0.5995893265921967, 0.6008213562397496, 0.5979466099758657, 0.5975359339244067, 0.6184804942084045, 0.6020533865482166, 0.5987679659953108, 0.5905544116756509, 0.5979466121299556, 0.5950718720101233, 0.6114989715679959, 0.6061601603789986, 0.6131416820402752, 0.6172484601792368, 0.6049281277206154, 0.6119096487944131, 0.6193018498361967, 0.6193018486612386, 0.6258726874171341, 0.6197125251043504, 0.606570842305248, 0.6279260792281838, 0.6139630364931095, 0.619301844353059, 0.6225872694099708, 0.6238193012850485, 0.6275154023934194, 0.629568789700463, 0.6398357293689031, 0.6279260802073156, 0.6275154010226349, 0.634086242911752, 0.6324435342019099, 0.6246406555420564, 0.6316221770075068, 0.6225872666684019, 0.6201232058556417, 0.6172484595917578, 0.6381930210507136, 0.6254620117573277, 0.6316221746575906, 0.6373716615063943, 0.6410677633980706, 0.6172484576334943, 0.6336755639228977, 0.6418891196133419, 0.6340862444783628, 0.6254620115615015, 0.6369609868257198, 0.6468172510301797, 0.6501026648515549, 0.6509240246651353, 0.6418891168717731, 0.6431211540341623, 0.6616016460639985, 0.6570841881528772, 0.6542094487429154, 0.6616016437140824, 0.6542094463929993, 0.6665297780683154, 0.6529774083738699, 0.666940454511427, 0.6628336785265553, 0.662833678918208, 0.6554414806179931, 0.6583162257069191, 0.6439425047662958, 0.6217659180903582, 0.601232035816083, 0.6353182785075303, 0.6542094460013466, 0.6476386031330978, 0.6394250523383123, 0.6537987675999714, 0.6501026688415167, 0.6197125278459192, 0.6447638627440043, 0.6632443577112358, 0.6579055397907076, 0.6698151992087001, 0.6714579063519315, 0.675154006676997, 0.6718685861240912, 0.6648870610603317, 0.6792607774969489, 0.6722792629588557, 0.6800821319497831, 0.6755646835117615, 0.6850102635624472, 0.6837782316873695, 0.6747433290589272, 0.6825462041939063, 0.6862423025607084, 0.678028743859434, 0.6726899427310152, 0.6640657094225012, 0.6574948673375578, 0.6587269033249888, 0.6833675532859943, 0.6788500994872264, 0.6751540068728233, 0.69240246820254, 0.689938393804327, 0.678850098703921, 0.6887063611459439, 0.6841889150578383, 0.6952772093749389, 0.685010270881457, 0.6932238165113226, 0.6772073890149471, 0.689527725267704, 0.6882956920218418, 0.6919917911719493, 0.6702258700952393, 0.6644763889988345, 0.6932238216762425, 0.6936345000776177, 0.6981519465573759, 0.7043121108284232, 0.7055441432909799, 0.6973305982485933, 0.693223823634506, 0.7084188903381692, 0.7075975347103769, 0.7104722747078177, 0.7084188949156102]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
