{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf26.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 13:51:42 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '2', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '04', '03', '02', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000015B021CBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000015B60B96EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6088, Accuracy:0.2070, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6067, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6032, Accuracy:0.2353, Validation Loss:1.6029, Validation Accuracy:0.2430\n",
    "Epoch #13: Loss:1.6029, Accuracy:0.2382, Validation Loss:1.6028, Validation Accuracy:0.2447\n",
    "Epoch #14: Loss:1.6029, Accuracy:0.2386, Validation Loss:1.6024, Validation Accuracy:0.2414\n",
    "Epoch #15: Loss:1.6025, Accuracy:0.2386, Validation Loss:1.6022, Validation Accuracy:0.2414\n",
    "Epoch #16: Loss:1.6029, Accuracy:0.2386, Validation Loss:1.6025, Validation Accuracy:0.2447\n",
    "Epoch #17: Loss:1.6028, Accuracy:0.2370, Validation Loss:1.6016, Validation Accuracy:0.2430\n",
    "Epoch #18: Loss:1.6020, Accuracy:0.2407, Validation Loss:1.6015, Validation Accuracy:0.2447\n",
    "Epoch #19: Loss:1.6022, Accuracy:0.2411, Validation Loss:1.6016, Validation Accuracy:0.2463\n",
    "Epoch #20: Loss:1.6018, Accuracy:0.2435, Validation Loss:1.6011, Validation Accuracy:0.2463\n",
    "Epoch #21: Loss:1.6015, Accuracy:0.2398, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #22: Loss:1.6016, Accuracy:0.2398, Validation Loss:1.6003, Validation Accuracy:0.2447\n",
    "Epoch #23: Loss:1.6013, Accuracy:0.2402, Validation Loss:1.6002, Validation Accuracy:0.2463\n",
    "Epoch #24: Loss:1.6016, Accuracy:0.2427, Validation Loss:1.6005, Validation Accuracy:0.2463\n",
    "Epoch #25: Loss:1.6012, Accuracy:0.2419, Validation Loss:1.6006, Validation Accuracy:0.2463\n",
    "Epoch #26: Loss:1.6011, Accuracy:0.2456, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #27: Loss:1.6011, Accuracy:0.2402, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #28: Loss:1.6014, Accuracy:0.2398, Validation Loss:1.6005, Validation Accuracy:0.2512\n",
    "Epoch #29: Loss:1.6011, Accuracy:0.2382, Validation Loss:1.6010, Validation Accuracy:0.2512\n",
    "Epoch #30: Loss:1.6013, Accuracy:0.2390, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #31: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #32: Loss:1.6026, Accuracy:0.2357, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #33: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #34: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #35: Loss:1.6045, Accuracy:0.2337, Validation Loss:1.6020, Validation Accuracy:0.2447\n",
    "Epoch #36: Loss:1.6037, Accuracy:0.2337, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #37: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #38: Loss:1.6033, Accuracy:0.2333, Validation Loss:1.6017, Validation Accuracy:0.2479\n",
    "Epoch #39: Loss:1.6027, Accuracy:0.2361, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #40: Loss:1.6019, Accuracy:0.2357, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #41: Loss:1.6018, Accuracy:0.2361, Validation Loss:1.6010, Validation Accuracy:0.2430\n",
    "Epoch #42: Loss:1.6015, Accuracy:0.2423, Validation Loss:1.6012, Validation Accuracy:0.2496\n",
    "Epoch #43: Loss:1.6016, Accuracy:0.2382, Validation Loss:1.6001, Validation Accuracy:0.2447\n",
    "Epoch #44: Loss:1.6015, Accuracy:0.2394, Validation Loss:1.5999, Validation Accuracy:0.2463\n",
    "Epoch #45: Loss:1.6021, Accuracy:0.2366, Validation Loss:1.5996, Validation Accuracy:0.2463\n",
    "Epoch #46: Loss:1.6018, Accuracy:0.2370, Validation Loss:1.5993, Validation Accuracy:0.2463\n",
    "Epoch #47: Loss:1.6019, Accuracy:0.2386, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #48: Loss:1.6028, Accuracy:0.2353, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #49: Loss:1.6032, Accuracy:0.2353, Validation Loss:1.5999, Validation Accuracy:0.2479\n",
    "Epoch #50: Loss:1.6024, Accuracy:0.2324, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #51: Loss:1.6023, Accuracy:0.2411, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #52: Loss:1.6016, Accuracy:0.2353, Validation Loss:1.5994, Validation Accuracy:0.2447\n",
    "Epoch #53: Loss:1.6016, Accuracy:0.2361, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #54: Loss:1.6014, Accuracy:0.2370, Validation Loss:1.5993, Validation Accuracy:0.2463\n",
    "Epoch #55: Loss:1.6016, Accuracy:0.2394, Validation Loss:1.5992, Validation Accuracy:0.2479\n",
    "Epoch #56: Loss:1.6014, Accuracy:0.2378, Validation Loss:1.5991, Validation Accuracy:0.2479\n",
    "Epoch #57: Loss:1.6015, Accuracy:0.2390, Validation Loss:1.5991, Validation Accuracy:0.2430\n",
    "Epoch #58: Loss:1.6016, Accuracy:0.2378, Validation Loss:1.5991, Validation Accuracy:0.2479\n",
    "Epoch #59: Loss:1.6015, Accuracy:0.2386, Validation Loss:1.5988, Validation Accuracy:0.2447\n",
    "Epoch #60: Loss:1.6015, Accuracy:0.2390, Validation Loss:1.5989, Validation Accuracy:0.2463\n",
    "Epoch #61: Loss:1.6013, Accuracy:0.2382, Validation Loss:1.5987, Validation Accuracy:0.2463\n",
    "Epoch #62: Loss:1.6011, Accuracy:0.2382, Validation Loss:1.5987, Validation Accuracy:0.2430\n",
    "Epoch #63: Loss:1.6013, Accuracy:0.2341, Validation Loss:1.5988, Validation Accuracy:0.2414\n",
    "Epoch #64: Loss:1.6012, Accuracy:0.2374, Validation Loss:1.5987, Validation Accuracy:0.2447\n",
    "Epoch #65: Loss:1.6012, Accuracy:0.2394, Validation Loss:1.5987, Validation Accuracy:0.2430\n",
    "Epoch #66: Loss:1.6007, Accuracy:0.2370, Validation Loss:1.5986, Validation Accuracy:0.2430\n",
    "Epoch #67: Loss:1.6008, Accuracy:0.2353, Validation Loss:1.5989, Validation Accuracy:0.2414\n",
    "Epoch #68: Loss:1.6012, Accuracy:0.2366, Validation Loss:1.5985, Validation Accuracy:0.2414\n",
    "Epoch #69: Loss:1.6011, Accuracy:0.2411, Validation Loss:1.5986, Validation Accuracy:0.2414\n",
    "Epoch #70: Loss:1.6009, Accuracy:0.2394, Validation Loss:1.5983, Validation Accuracy:0.2430\n",
    "Epoch #71: Loss:1.6008, Accuracy:0.2398, Validation Loss:1.5984, Validation Accuracy:0.2447\n",
    "Epoch #72: Loss:1.6004, Accuracy:0.2378, Validation Loss:1.5983, Validation Accuracy:0.2447\n",
    "Epoch #73: Loss:1.6001, Accuracy:0.2419, Validation Loss:1.5981, Validation Accuracy:0.2463\n",
    "Epoch #74: Loss:1.6007, Accuracy:0.2402, Validation Loss:1.5980, Validation Accuracy:0.2479\n",
    "Epoch #75: Loss:1.6002, Accuracy:0.2394, Validation Loss:1.5979, Validation Accuracy:0.2430\n",
    "Epoch #76: Loss:1.6000, Accuracy:0.2407, Validation Loss:1.5978, Validation Accuracy:0.2479\n",
    "Epoch #77: Loss:1.6003, Accuracy:0.2411, Validation Loss:1.5984, Validation Accuracy:0.2348\n",
    "Epoch #78: Loss:1.6002, Accuracy:0.2374, Validation Loss:1.5985, Validation Accuracy:0.2414\n",
    "Epoch #79: Loss:1.6010, Accuracy:0.2378, Validation Loss:1.5977, Validation Accuracy:0.2463\n",
    "Epoch #80: Loss:1.6000, Accuracy:0.2398, Validation Loss:1.5974, Validation Accuracy:0.2430\n",
    "Epoch #81: Loss:1.6000, Accuracy:0.2366, Validation Loss:1.5980, Validation Accuracy:0.2381\n",
    "Epoch #82: Loss:1.5996, Accuracy:0.2357, Validation Loss:1.5975, Validation Accuracy:0.2447\n",
    "Epoch #83: Loss:1.5996, Accuracy:0.2435, Validation Loss:1.5976, Validation Accuracy:0.2463\n",
    "Epoch #84: Loss:1.5998, Accuracy:0.2390, Validation Loss:1.5977, Validation Accuracy:0.2463\n",
    "Epoch #85: Loss:1.5994, Accuracy:0.2390, Validation Loss:1.5982, Validation Accuracy:0.2365\n",
    "Epoch #86: Loss:1.5996, Accuracy:0.2374, Validation Loss:1.5979, Validation Accuracy:0.2414\n",
    "Epoch #87: Loss:1.5999, Accuracy:0.2402, Validation Loss:1.5980, Validation Accuracy:0.2463\n",
    "Epoch #88: Loss:1.5994, Accuracy:0.2423, Validation Loss:1.5978, Validation Accuracy:0.2479\n",
    "Epoch #89: Loss:1.5996, Accuracy:0.2431, Validation Loss:1.5980, Validation Accuracy:0.2463\n",
    "Epoch #90: Loss:1.5994, Accuracy:0.2382, Validation Loss:1.5979, Validation Accuracy:0.2479\n",
    "Epoch #91: Loss:1.5995, Accuracy:0.2419, Validation Loss:1.5978, Validation Accuracy:0.2463\n",
    "Epoch #92: Loss:1.5992, Accuracy:0.2402, Validation Loss:1.5983, Validation Accuracy:0.2315\n",
    "Epoch #93: Loss:1.5994, Accuracy:0.2357, Validation Loss:1.5978, Validation Accuracy:0.2315\n",
    "Epoch #94: Loss:1.5987, Accuracy:0.2361, Validation Loss:1.5976, Validation Accuracy:0.2381\n",
    "Epoch #95: Loss:1.5987, Accuracy:0.2366, Validation Loss:1.5986, Validation Accuracy:0.2315\n",
    "Epoch #96: Loss:1.5991, Accuracy:0.2398, Validation Loss:1.5989, Validation Accuracy:0.2315\n",
    "Epoch #97: Loss:1.5990, Accuracy:0.2357, Validation Loss:1.5987, Validation Accuracy:0.2381\n",
    "Epoch #98: Loss:1.5988, Accuracy:0.2378, Validation Loss:1.5986, Validation Accuracy:0.2348\n",
    "Epoch #99: Loss:1.5995, Accuracy:0.2341, Validation Loss:1.6007, Validation Accuracy:0.2250\n",
    "Epoch #100: Loss:1.6004, Accuracy:0.2329, Validation Loss:1.6022, Validation Accuracy:0.2381\n",
    "Epoch #101: Loss:1.6008, Accuracy:0.2398, Validation Loss:1.5992, Validation Accuracy:0.2184\n",
    "Epoch #102: Loss:1.5988, Accuracy:0.2472, Validation Loss:1.6002, Validation Accuracy:0.2282\n",
    "Epoch #103: Loss:1.5992, Accuracy:0.2407, Validation Loss:1.5991, Validation Accuracy:0.2250\n",
    "Epoch #104: Loss:1.5989, Accuracy:0.2411, Validation Loss:1.5990, Validation Accuracy:0.2463\n",
    "Epoch #105: Loss:1.5988, Accuracy:0.2411, Validation Loss:1.5993, Validation Accuracy:0.2332\n",
    "Epoch #106: Loss:1.5987, Accuracy:0.2402, Validation Loss:1.5980, Validation Accuracy:0.2447\n",
    "Epoch #107: Loss:1.5983, Accuracy:0.2374, Validation Loss:1.5979, Validation Accuracy:0.2414\n",
    "Epoch #108: Loss:1.5982, Accuracy:0.2374, Validation Loss:1.5978, Validation Accuracy:0.2381\n",
    "Epoch #109: Loss:1.5985, Accuracy:0.2366, Validation Loss:1.5982, Validation Accuracy:0.2447\n",
    "Epoch #110: Loss:1.5983, Accuracy:0.2411, Validation Loss:1.5990, Validation Accuracy:0.2397\n",
    "Epoch #111: Loss:1.5979, Accuracy:0.2398, Validation Loss:1.5987, Validation Accuracy:0.2332\n",
    "Epoch #112: Loss:1.5978, Accuracy:0.2427, Validation Loss:1.5989, Validation Accuracy:0.2332\n",
    "Epoch #113: Loss:1.5985, Accuracy:0.2390, Validation Loss:1.5991, Validation Accuracy:0.2348\n",
    "Epoch #114: Loss:1.5986, Accuracy:0.2382, Validation Loss:1.5985, Validation Accuracy:0.2381\n",
    "Epoch #115: Loss:1.5992, Accuracy:0.2439, Validation Loss:1.6005, Validation Accuracy:0.2200\n",
    "Epoch #116: Loss:1.5972, Accuracy:0.2472, Validation Loss:1.6021, Validation Accuracy:0.2430\n",
    "Epoch #117: Loss:1.5985, Accuracy:0.2415, Validation Loss:1.6018, Validation Accuracy:0.2315\n",
    "Epoch #118: Loss:1.5994, Accuracy:0.2407, Validation Loss:1.6017, Validation Accuracy:0.2118\n",
    "Epoch #119: Loss:1.5992, Accuracy:0.2370, Validation Loss:1.6008, Validation Accuracy:0.2217\n",
    "Epoch #120: Loss:1.5984, Accuracy:0.2366, Validation Loss:1.6030, Validation Accuracy:0.2233\n",
    "Epoch #121: Loss:1.5988, Accuracy:0.2374, Validation Loss:1.6034, Validation Accuracy:0.2397\n",
    "Epoch #122: Loss:1.5982, Accuracy:0.2431, Validation Loss:1.6029, Validation Accuracy:0.2397\n",
    "Epoch #123: Loss:1.5985, Accuracy:0.2427, Validation Loss:1.6026, Validation Accuracy:0.2414\n",
    "Epoch #124: Loss:1.5979, Accuracy:0.2382, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #125: Loss:1.5982, Accuracy:0.2382, Validation Loss:1.5990, Validation Accuracy:0.2299\n",
    "Epoch #126: Loss:1.5977, Accuracy:0.2370, Validation Loss:1.5983, Validation Accuracy:0.2397\n",
    "Epoch #127: Loss:1.5979, Accuracy:0.2419, Validation Loss:1.5999, Validation Accuracy:0.2365\n",
    "Epoch #128: Loss:1.5983, Accuracy:0.2402, Validation Loss:1.5991, Validation Accuracy:0.2381\n",
    "Epoch #129: Loss:1.5981, Accuracy:0.2386, Validation Loss:1.6004, Validation Accuracy:0.2315\n",
    "Epoch #130: Loss:1.5983, Accuracy:0.2398, Validation Loss:1.6006, Validation Accuracy:0.2381\n",
    "Epoch #131: Loss:1.5990, Accuracy:0.2402, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #132: Loss:1.5994, Accuracy:0.2390, Validation Loss:1.5998, Validation Accuracy:0.2184\n",
    "Epoch #133: Loss:1.5982, Accuracy:0.2415, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #134: Loss:1.5995, Accuracy:0.2366, Validation Loss:1.5999, Validation Accuracy:0.2447\n",
    "Epoch #135: Loss:1.5992, Accuracy:0.2390, Validation Loss:1.6008, Validation Accuracy:0.2381\n",
    "Epoch #136: Loss:1.5994, Accuracy:0.2382, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #137: Loss:1.5999, Accuracy:0.2386, Validation Loss:1.6019, Validation Accuracy:0.2463\n",
    "Epoch #138: Loss:1.5995, Accuracy:0.2386, Validation Loss:1.6037, Validation Accuracy:0.2447\n",
    "Epoch #139: Loss:1.5988, Accuracy:0.2415, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #140: Loss:1.5980, Accuracy:0.2419, Validation Loss:1.6014, Validation Accuracy:0.2414\n",
    "Epoch #141: Loss:1.5984, Accuracy:0.2427, Validation Loss:1.5997, Validation Accuracy:0.2397\n",
    "Epoch #142: Loss:1.5973, Accuracy:0.2448, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #143: Loss:1.5978, Accuracy:0.2345, Validation Loss:1.6001, Validation Accuracy:0.2381\n",
    "Epoch #144: Loss:1.5976, Accuracy:0.2415, Validation Loss:1.6000, Validation Accuracy:0.2463\n",
    "Epoch #145: Loss:1.5979, Accuracy:0.2439, Validation Loss:1.6011, Validation Accuracy:0.2430\n",
    "Epoch #146: Loss:1.5977, Accuracy:0.2427, Validation Loss:1.6012, Validation Accuracy:0.2397\n",
    "Epoch #147: Loss:1.5970, Accuracy:0.2427, Validation Loss:1.6003, Validation Accuracy:0.2414\n",
    "Epoch #148: Loss:1.5978, Accuracy:0.2411, Validation Loss:1.6000, Validation Accuracy:0.2414\n",
    "Epoch #149: Loss:1.5978, Accuracy:0.2448, Validation Loss:1.6011, Validation Accuracy:0.2397\n",
    "Epoch #150: Loss:1.5973, Accuracy:0.2435, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #151: Loss:1.5971, Accuracy:0.2448, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #152: Loss:1.5979, Accuracy:0.2444, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #153: Loss:1.5975, Accuracy:0.2407, Validation Loss:1.6008, Validation Accuracy:0.2381\n",
    "Epoch #154: Loss:1.5967, Accuracy:0.2444, Validation Loss:1.6017, Validation Accuracy:0.2381\n",
    "Epoch #155: Loss:1.5968, Accuracy:0.2480, Validation Loss:1.6001, Validation Accuracy:0.2381\n",
    "Epoch #156: Loss:1.5963, Accuracy:0.2489, Validation Loss:1.5989, Validation Accuracy:0.2414\n",
    "Epoch #157: Loss:1.5959, Accuracy:0.2534, Validation Loss:1.5982, Validation Accuracy:0.2430\n",
    "Epoch #158: Loss:1.5963, Accuracy:0.2415, Validation Loss:1.5989, Validation Accuracy:0.2397\n",
    "Epoch #159: Loss:1.5963, Accuracy:0.2439, Validation Loss:1.5994, Validation Accuracy:0.2447\n",
    "Epoch #160: Loss:1.5962, Accuracy:0.2427, Validation Loss:1.5990, Validation Accuracy:0.2447\n",
    "Epoch #161: Loss:1.5960, Accuracy:0.2505, Validation Loss:1.5987, Validation Accuracy:0.2447\n",
    "Epoch #162: Loss:1.5963, Accuracy:0.2476, Validation Loss:1.5993, Validation Accuracy:0.2447\n",
    "Epoch #163: Loss:1.5960, Accuracy:0.2431, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #164: Loss:1.5964, Accuracy:0.2427, Validation Loss:1.5992, Validation Accuracy:0.2348\n",
    "Epoch #165: Loss:1.5962, Accuracy:0.2452, Validation Loss:1.5990, Validation Accuracy:0.2381\n",
    "Epoch #166: Loss:1.5961, Accuracy:0.2460, Validation Loss:1.5989, Validation Accuracy:0.2430\n",
    "Epoch #167: Loss:1.5960, Accuracy:0.2546, Validation Loss:1.5997, Validation Accuracy:0.2282\n",
    "Epoch #168: Loss:1.5958, Accuracy:0.2559, Validation Loss:1.6000, Validation Accuracy:0.2381\n",
    "Epoch #169: Loss:1.5959, Accuracy:0.2579, Validation Loss:1.6001, Validation Accuracy:0.2414\n",
    "Epoch #170: Loss:1.5954, Accuracy:0.2542, Validation Loss:1.5984, Validation Accuracy:0.2397\n",
    "Epoch #171: Loss:1.5952, Accuracy:0.2522, Validation Loss:1.5984, Validation Accuracy:0.2332\n",
    "Epoch #172: Loss:1.5956, Accuracy:0.2464, Validation Loss:1.5990, Validation Accuracy:0.2447\n",
    "Epoch #173: Loss:1.5953, Accuracy:0.2546, Validation Loss:1.5979, Validation Accuracy:0.2463\n",
    "Epoch #174: Loss:1.5958, Accuracy:0.2522, Validation Loss:1.5986, Validation Accuracy:0.2447\n",
    "Epoch #175: Loss:1.5961, Accuracy:0.2513, Validation Loss:1.5998, Validation Accuracy:0.2447\n",
    "Epoch #176: Loss:1.5957, Accuracy:0.2538, Validation Loss:1.5977, Validation Accuracy:0.2397\n",
    "Epoch #177: Loss:1.5950, Accuracy:0.2460, Validation Loss:1.5986, Validation Accuracy:0.2348\n",
    "Epoch #178: Loss:1.5968, Accuracy:0.2419, Validation Loss:1.6004, Validation Accuracy:0.2479\n",
    "Epoch #179: Loss:1.5948, Accuracy:0.2448, Validation Loss:1.5995, Validation Accuracy:0.2414\n",
    "Epoch #180: Loss:1.5950, Accuracy:0.2534, Validation Loss:1.5992, Validation Accuracy:0.2332\n",
    "Epoch #181: Loss:1.5953, Accuracy:0.2480, Validation Loss:1.6007, Validation Accuracy:0.2332\n",
    "Epoch #182: Loss:1.5965, Accuracy:0.2476, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #183: Loss:1.5965, Accuracy:0.2472, Validation Loss:1.5982, Validation Accuracy:0.2315\n",
    "Epoch #184: Loss:1.5960, Accuracy:0.2419, Validation Loss:1.5994, Validation Accuracy:0.2447\n",
    "Epoch #185: Loss:1.5955, Accuracy:0.2546, Validation Loss:1.5994, Validation Accuracy:0.2447\n",
    "Epoch #186: Loss:1.5954, Accuracy:0.2575, Validation Loss:1.6000, Validation Accuracy:0.2447\n",
    "Epoch #187: Loss:1.5952, Accuracy:0.2513, Validation Loss:1.5997, Validation Accuracy:0.2348\n",
    "Epoch #188: Loss:1.5950, Accuracy:0.2476, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #189: Loss:1.5952, Accuracy:0.2480, Validation Loss:1.6009, Validation Accuracy:0.2414\n",
    "Epoch #190: Loss:1.5953, Accuracy:0.2554, Validation Loss:1.6007, Validation Accuracy:0.2463\n",
    "Epoch #191: Loss:1.5956, Accuracy:0.2526, Validation Loss:1.5989, Validation Accuracy:0.2479\n",
    "Epoch #192: Loss:1.5951, Accuracy:0.2538, Validation Loss:1.5992, Validation Accuracy:0.2397\n",
    "Epoch #193: Loss:1.5949, Accuracy:0.2534, Validation Loss:1.6004, Validation Accuracy:0.2512\n",
    "Epoch #194: Loss:1.5954, Accuracy:0.2505, Validation Loss:1.6026, Validation Accuracy:0.2315\n",
    "Epoch #195: Loss:1.5953, Accuracy:0.2509, Validation Loss:1.5999, Validation Accuracy:0.2315\n",
    "Epoch #196: Loss:1.5955, Accuracy:0.2480, Validation Loss:1.5998, Validation Accuracy:0.2447\n",
    "Epoch #197: Loss:1.5955, Accuracy:0.2513, Validation Loss:1.6015, Validation Accuracy:0.2397\n",
    "Epoch #198: Loss:1.5951, Accuracy:0.2501, Validation Loss:1.6019, Validation Accuracy:0.2282\n",
    "Epoch #199: Loss:1.5957, Accuracy:0.2530, Validation Loss:1.6043, Validation Accuracy:0.2266\n",
    "Epoch #200: Loss:1.5940, Accuracy:0.2501, Validation Loss:1.6031, Validation Accuracy:0.2365\n",
    "Epoch #201: Loss:1.5950, Accuracy:0.2489, Validation Loss:1.6013, Validation Accuracy:0.2397\n",
    "Epoch #202: Loss:1.5937, Accuracy:0.2501, Validation Loss:1.6007, Validation Accuracy:0.2266\n",
    "Epoch #203: Loss:1.5935, Accuracy:0.2485, Validation Loss:1.6006, Validation Accuracy:0.2332\n",
    "Epoch #204: Loss:1.5943, Accuracy:0.2480, Validation Loss:1.6010, Validation Accuracy:0.2299\n",
    "Epoch #205: Loss:1.5947, Accuracy:0.2468, Validation Loss:1.6019, Validation Accuracy:0.2315\n",
    "Epoch #206: Loss:1.5936, Accuracy:0.2448, Validation Loss:1.6020, Validation Accuracy:0.2299\n",
    "Epoch #207: Loss:1.5942, Accuracy:0.2452, Validation Loss:1.6013, Validation Accuracy:0.2315\n",
    "Epoch #208: Loss:1.5941, Accuracy:0.2476, Validation Loss:1.6012, Validation Accuracy:0.2282\n",
    "Epoch #209: Loss:1.5938, Accuracy:0.2460, Validation Loss:1.6006, Validation Accuracy:0.2282\n",
    "Epoch #210: Loss:1.5936, Accuracy:0.2456, Validation Loss:1.6016, Validation Accuracy:0.2315\n",
    "Epoch #211: Loss:1.5937, Accuracy:0.2501, Validation Loss:1.6037, Validation Accuracy:0.2233\n",
    "Epoch #212: Loss:1.5939, Accuracy:0.2468, Validation Loss:1.6020, Validation Accuracy:0.2233\n",
    "Epoch #213: Loss:1.5933, Accuracy:0.2530, Validation Loss:1.6019, Validation Accuracy:0.2315\n",
    "Epoch #214: Loss:1.5934, Accuracy:0.2489, Validation Loss:1.6024, Validation Accuracy:0.2282\n",
    "Epoch #215: Loss:1.5930, Accuracy:0.2460, Validation Loss:1.6031, Validation Accuracy:0.2250\n",
    "Epoch #216: Loss:1.5930, Accuracy:0.2460, Validation Loss:1.6025, Validation Accuracy:0.2250\n",
    "Epoch #217: Loss:1.5932, Accuracy:0.2530, Validation Loss:1.6023, Validation Accuracy:0.2315\n",
    "Epoch #218: Loss:1.5927, Accuracy:0.2522, Validation Loss:1.6026, Validation Accuracy:0.2315\n",
    "Epoch #219: Loss:1.5929, Accuracy:0.2501, Validation Loss:1.6017, Validation Accuracy:0.2315\n",
    "Epoch #220: Loss:1.5933, Accuracy:0.2501, Validation Loss:1.6024, Validation Accuracy:0.2315\n",
    "Epoch #221: Loss:1.5933, Accuracy:0.2452, Validation Loss:1.6021, Validation Accuracy:0.2282\n",
    "Epoch #222: Loss:1.5928, Accuracy:0.2468, Validation Loss:1.6015, Validation Accuracy:0.2496\n",
    "Epoch #223: Loss:1.5929, Accuracy:0.2542, Validation Loss:1.6030, Validation Accuracy:0.2233\n",
    "Epoch #224: Loss:1.5929, Accuracy:0.2567, Validation Loss:1.6036, Validation Accuracy:0.2299\n",
    "Epoch #225: Loss:1.5937, Accuracy:0.2464, Validation Loss:1.6038, Validation Accuracy:0.2233\n",
    "Epoch #226: Loss:1.5944, Accuracy:0.2489, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #227: Loss:1.5951, Accuracy:0.2493, Validation Loss:1.6033, Validation Accuracy:0.2233\n",
    "Epoch #228: Loss:1.5933, Accuracy:0.2476, Validation Loss:1.6027, Validation Accuracy:0.2430\n",
    "Epoch #229: Loss:1.5946, Accuracy:0.2505, Validation Loss:1.6031, Validation Accuracy:0.2397\n",
    "Epoch #230: Loss:1.5940, Accuracy:0.2517, Validation Loss:1.6039, Validation Accuracy:0.2299\n",
    "Epoch #231: Loss:1.5937, Accuracy:0.2480, Validation Loss:1.6034, Validation Accuracy:0.2315\n",
    "Epoch #232: Loss:1.5934, Accuracy:0.2542, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #233: Loss:1.5934, Accuracy:0.2534, Validation Loss:1.6021, Validation Accuracy:0.2397\n",
    "Epoch #234: Loss:1.5929, Accuracy:0.2497, Validation Loss:1.6017, Validation Accuracy:0.2348\n",
    "Epoch #235: Loss:1.5928, Accuracy:0.2468, Validation Loss:1.6019, Validation Accuracy:0.2299\n",
    "Epoch #236: Loss:1.5929, Accuracy:0.2464, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #237: Loss:1.5933, Accuracy:0.2480, Validation Loss:1.6035, Validation Accuracy:0.2315\n",
    "Epoch #238: Loss:1.5938, Accuracy:0.2423, Validation Loss:1.6040, Validation Accuracy:0.2348\n",
    "Epoch #239: Loss:1.5942, Accuracy:0.2485, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #240: Loss:1.5940, Accuracy:0.2480, Validation Loss:1.6032, Validation Accuracy:0.2496\n",
    "Epoch #241: Loss:1.5937, Accuracy:0.2452, Validation Loss:1.6048, Validation Accuracy:0.2250\n",
    "Epoch #242: Loss:1.5934, Accuracy:0.2497, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #243: Loss:1.5936, Accuracy:0.2522, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #244: Loss:1.5940, Accuracy:0.2530, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #245: Loss:1.5935, Accuracy:0.2501, Validation Loss:1.6034, Validation Accuracy:0.2381\n",
    "Epoch #246: Loss:1.5935, Accuracy:0.2559, Validation Loss:1.6059, Validation Accuracy:0.2299\n",
    "Epoch #247: Loss:1.5932, Accuracy:0.2554, Validation Loss:1.6062, Validation Accuracy:0.2266\n",
    "Epoch #248: Loss:1.5931, Accuracy:0.2538, Validation Loss:1.6056, Validation Accuracy:0.2233\n",
    "Epoch #249: Loss:1.5933, Accuracy:0.2501, Validation Loss:1.6049, Validation Accuracy:0.2315\n",
    "Epoch #250: Loss:1.5932, Accuracy:0.2567, Validation Loss:1.6060, Validation Accuracy:0.2447\n",
    "Epoch #251: Loss:1.5932, Accuracy:0.2501, Validation Loss:1.6060, Validation Accuracy:0.2397\n",
    "Epoch #252: Loss:1.5935, Accuracy:0.2530, Validation Loss:1.6066, Validation Accuracy:0.2233\n",
    "Epoch #253: Loss:1.5943, Accuracy:0.2554, Validation Loss:1.6073, Validation Accuracy:0.2250\n",
    "Epoch #254: Loss:1.5943, Accuracy:0.2460, Validation Loss:1.6067, Validation Accuracy:0.2184\n",
    "Epoch #255: Loss:1.5938, Accuracy:0.2550, Validation Loss:1.6066, Validation Accuracy:0.2447\n",
    "Epoch #256: Loss:1.5928, Accuracy:0.2530, Validation Loss:1.6082, Validation Accuracy:0.2217\n",
    "Epoch #257: Loss:1.5934, Accuracy:0.2497, Validation Loss:1.6072, Validation Accuracy:0.2184\n",
    "Epoch #258: Loss:1.5941, Accuracy:0.2517, Validation Loss:1.6062, Validation Accuracy:0.2233\n",
    "Epoch #259: Loss:1.5935, Accuracy:0.2550, Validation Loss:1.6065, Validation Accuracy:0.2217\n",
    "Epoch #260: Loss:1.5927, Accuracy:0.2550, Validation Loss:1.6047, Validation Accuracy:0.2381\n",
    "Epoch #261: Loss:1.5927, Accuracy:0.2583, Validation Loss:1.6043, Validation Accuracy:0.2315\n",
    "Epoch #262: Loss:1.5926, Accuracy:0.2546, Validation Loss:1.6049, Validation Accuracy:0.2250\n",
    "Epoch #263: Loss:1.5929, Accuracy:0.2522, Validation Loss:1.6037, Validation Accuracy:0.2365\n",
    "Epoch #264: Loss:1.5931, Accuracy:0.2542, Validation Loss:1.6036, Validation Accuracy:0.2365\n",
    "Epoch #265: Loss:1.5925, Accuracy:0.2579, Validation Loss:1.6049, Validation Accuracy:0.2250\n",
    "Epoch #266: Loss:1.5927, Accuracy:0.2509, Validation Loss:1.6041, Validation Accuracy:0.2348\n",
    "Epoch #267: Loss:1.5920, Accuracy:0.2579, Validation Loss:1.6041, Validation Accuracy:0.2365\n",
    "Epoch #268: Loss:1.5923, Accuracy:0.2571, Validation Loss:1.6041, Validation Accuracy:0.2282\n",
    "Epoch #269: Loss:1.5920, Accuracy:0.2579, Validation Loss:1.6046, Validation Accuracy:0.2414\n",
    "Epoch #270: Loss:1.5919, Accuracy:0.2579, Validation Loss:1.6045, Validation Accuracy:0.2315\n",
    "Epoch #271: Loss:1.5925, Accuracy:0.2550, Validation Loss:1.6052, Validation Accuracy:0.2299\n",
    "Epoch #272: Loss:1.5923, Accuracy:0.2550, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #273: Loss:1.5918, Accuracy:0.2550, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #274: Loss:1.5919, Accuracy:0.2563, Validation Loss:1.6048, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5923, Accuracy:0.2554, Validation Loss:1.6041, Validation Accuracy:0.2282\n",
    "Epoch #276: Loss:1.5919, Accuracy:0.2554, Validation Loss:1.6060, Validation Accuracy:0.2184\n",
    "Epoch #277: Loss:1.5926, Accuracy:0.2587, Validation Loss:1.6068, Validation Accuracy:0.2381\n",
    "Epoch #278: Loss:1.5922, Accuracy:0.2600, Validation Loss:1.6070, Validation Accuracy:0.2315\n",
    "Epoch #279: Loss:1.5920, Accuracy:0.2616, Validation Loss:1.6083, Validation Accuracy:0.2233\n",
    "Epoch #280: Loss:1.5919, Accuracy:0.2591, Validation Loss:1.6061, Validation Accuracy:0.2266\n",
    "Epoch #281: Loss:1.5918, Accuracy:0.2559, Validation Loss:1.6055, Validation Accuracy:0.2496\n",
    "Epoch #282: Loss:1.5916, Accuracy:0.2628, Validation Loss:1.6063, Validation Accuracy:0.2381\n",
    "Epoch #283: Loss:1.5918, Accuracy:0.2620, Validation Loss:1.6069, Validation Accuracy:0.2397\n",
    "Epoch #284: Loss:1.5917, Accuracy:0.2591, Validation Loss:1.6067, Validation Accuracy:0.2266\n",
    "Epoch #285: Loss:1.5919, Accuracy:0.2587, Validation Loss:1.6063, Validation Accuracy:0.2217\n",
    "Epoch #286: Loss:1.5918, Accuracy:0.2587, Validation Loss:1.6071, Validation Accuracy:0.2365\n",
    "Epoch #287: Loss:1.5914, Accuracy:0.2612, Validation Loss:1.6080, Validation Accuracy:0.2479\n",
    "Epoch #288: Loss:1.5922, Accuracy:0.2616, Validation Loss:1.6071, Validation Accuracy:0.2430\n",
    "Epoch #289: Loss:1.5915, Accuracy:0.2591, Validation Loss:1.6083, Validation Accuracy:0.2348\n",
    "Epoch #290: Loss:1.5913, Accuracy:0.2579, Validation Loss:1.6061, Validation Accuracy:0.2447\n",
    "Epoch #291: Loss:1.5906, Accuracy:0.2616, Validation Loss:1.6083, Validation Accuracy:0.2365\n",
    "Epoch #292: Loss:1.5909, Accuracy:0.2591, Validation Loss:1.6080, Validation Accuracy:0.2365\n",
    "Epoch #293: Loss:1.5905, Accuracy:0.2600, Validation Loss:1.6076, Validation Accuracy:0.2463\n",
    "Epoch #294: Loss:1.5904, Accuracy:0.2612, Validation Loss:1.6090, Validation Accuracy:0.2200\n",
    "Epoch #295: Loss:1.5913, Accuracy:0.2575, Validation Loss:1.6083, Validation Accuracy:0.2332\n",
    "Epoch #296: Loss:1.5910, Accuracy:0.2616, Validation Loss:1.6077, Validation Accuracy:0.2332\n",
    "Epoch #297: Loss:1.5899, Accuracy:0.2645, Validation Loss:1.6082, Validation Accuracy:0.2282\n",
    "Epoch #298: Loss:1.5913, Accuracy:0.2583, Validation Loss:1.6064, Validation Accuracy:0.2250\n",
    "Epoch #299: Loss:1.5916, Accuracy:0.2591, Validation Loss:1.6047, Validation Accuracy:0.2299\n",
    "Epoch #300: Loss:1.5909, Accuracy:0.2604, Validation Loss:1.6069, Validation Accuracy:0.2282\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60687447, Accuracy:0.2282\n",
    "Labels: ['01', '04', '03', '02', '05']\n",
    "Confusion Matrix:\n",
    "      01  04  03  02  05\n",
    "t:01  51  19   2   3  51\n",
    "t:04  40  19   3   1  49\n",
    "t:03  56  10   3   1  45\n",
    "t:02  45  18   3   3  45\n",
    "t:05  61  14   3   1  63\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.20      0.40      0.27       126\n",
    "          04       0.24      0.17      0.20       112\n",
    "          03       0.21      0.03      0.05       115\n",
    "          02       0.33      0.03      0.05       114\n",
    "          05       0.25      0.44      0.32       142\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.25      0.21      0.18       609\n",
    "weighted avg       0.25      0.23      0.18       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 14:07:17 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6069260538113723, 1.6053954118186813, 1.60455633266806, 1.6043861456496766, 1.6041331451710417, 1.6040319012499404, 1.6037781385365377, 1.6031617141513794, 1.603034149641278, 1.6028011745615742, 1.6028521532691367, 1.6028642550673586, 1.6028232347397577, 1.6024158429629698, 1.602247494977879, 1.602523786093801, 1.601620419076315, 1.6015191861169875, 1.6015844824670376, 1.6010840107458957, 1.6007002825024483, 1.6003261325003086, 1.6002312509101404, 1.6005386877529726, 1.6005609990732228, 1.6006556148403774, 1.6006943694084932, 1.6005015257739865, 1.6010060228150467, 1.604207816382347, 1.6007626005777194, 1.6009149680583936, 1.604630845520884, 1.602324937951976, 1.6019922492930847, 1.601037030541055, 1.6009395652999627, 1.6016546724660838, 1.6007177334505154, 1.6004346832265994, 1.6010288608876746, 1.6012348087354638, 1.60010283510086, 1.5999172017687844, 1.599611349489497, 1.5993257625936874, 1.6008638873671859, 1.600651375961617, 1.5998586332073743, 1.600637153451666, 1.599591187851378, 1.5994206410519203, 1.5994085979774864, 1.5992726847064516, 1.59916852554077, 1.5990779448808317, 1.5990547810869264, 1.5990910990093337, 1.5987903848657468, 1.5988749418352626, 1.5986941467560767, 1.5986526024165413, 1.5988466393184193, 1.5986785980672475, 1.598720503558079, 1.5986057478806068, 1.5988566051367272, 1.5984777734784656, 1.5985801368707115, 1.5983441292945975, 1.5984485617216388, 1.5982592327058414, 1.598106524235705, 1.5980115026871755, 1.5978834112289504, 1.5977886363203302, 1.5984436688556265, 1.5984614795847676, 1.5976789757144472, 1.5974304503799464, 1.5979850517313665, 1.597505110433732, 1.5975884717869249, 1.597690058654948, 1.5982321469458844, 1.597908324404499, 1.5980232102530343, 1.5977842502406079, 1.5979701372594473, 1.597928343931051, 1.5977692821342957, 1.5983419702166604, 1.5977772407735313, 1.597576369792957, 1.5986060976786372, 1.598909654640799, 1.5986852841619983, 1.5986282465297406, 1.6006959154100842, 1.6022253083478053, 1.5991840507400839, 1.600249410458582, 1.5991020840768548, 1.5990167584129547, 1.599296114714862, 1.5980013133269813, 1.597861671682649, 1.5978298312533274, 1.5981692678626926, 1.5990282721903133, 1.5987313285054048, 1.5989373290088573, 1.5991426148438102, 1.5984805593349662, 1.6004705299884814, 1.602076987914851, 1.6017855127848233, 1.6017002573937227, 1.6007584754273614, 1.6029559548069494, 1.6034180636476414, 1.6028655031435988, 1.6026235178773627, 1.600307929887756, 1.5990064709840346, 1.5982940122607503, 1.5999151090487276, 1.5990955798301008, 1.6004121969094613, 1.6006019949325787, 1.6005485026511457, 1.599791203422108, 1.5998528391269629, 1.5999350015361518, 1.6008409009191202, 1.6007235197010885, 1.6018533597047302, 1.6036691420966964, 1.601233388598525, 1.6013754923355403, 1.5997398670866767, 1.600774138040339, 1.6001470592025857, 1.5999593413717836, 1.6011446906232285, 1.6012126559694413, 1.6003333494581025, 1.599989395031984, 1.6010951817720787, 1.601238239574902, 1.6002663826120311, 1.600405270438672, 1.6007748863771436, 1.6016960694089115, 1.600129860766807, 1.5989329822740728, 1.5981641653527572, 1.5988545956087035, 1.5994477714419562, 1.599032490124256, 1.5987482425223039, 1.5992982925844115, 1.59983339861696, 1.599179373194627, 1.5990361317820934, 1.5989010067800387, 1.5996592371726075, 1.5999722780265244, 1.6000752879676756, 1.5984032829406813, 1.5983820384359124, 1.5989662179805961, 1.5978695240318286, 1.598637757042946, 1.5997726561009198, 1.5976679493445285, 1.5986471111551295, 1.6004183587965315, 1.599527612499807, 1.5991577413086038, 1.6006841855291858, 1.5992055113483923, 1.598191333130271, 1.5993841500900845, 1.5994084353125937, 1.6000117192714673, 1.599729841174359, 1.6006611033613458, 1.6009369673595835, 1.6006832490805138, 1.5989392962557538, 1.5991794976890576, 1.6004484776401364, 1.6026284044795045, 1.599851043157781, 1.5997748212469818, 1.601487486429011, 1.6019047383212888, 1.6043017619153352, 1.6031064208113697, 1.6013394400404004, 1.6006637570893236, 1.600642053168787, 1.600992514777849, 1.6019081599606668, 1.6019590617400672, 1.6013451010135595, 1.601226406144391, 1.6005684949690093, 1.6015815905162267, 1.6037479578372098, 1.602032482330435, 1.6018677092538092, 1.6024226993567054, 1.603081133369546, 1.6025002723061197, 1.6023018788821592, 1.6026207731275135, 1.6017495871373193, 1.602425866526336, 1.6021087424116964, 1.6014975131243125, 1.6029856545584542, 1.6035762058298773, 1.6037963673790492, 1.6022271154745067, 1.6032928870425045, 1.6026542423589671, 1.6031063519087918, 1.6039488315582275, 1.603414057706573, 1.6013631033779951, 1.6021092122020002, 1.6017019169279703, 1.6018764121191842, 1.6012068376165305, 1.6035375935690743, 1.6039721936427902, 1.6044645507151662, 1.6032269613691934, 1.604837652301945, 1.603922846282057, 1.6040676773475309, 1.6038324177167294, 1.6033540848636472, 1.605891466923731, 1.6062278332577158, 1.6056083618909462, 1.6048661836458153, 1.6059665709293534, 1.606049715396023, 1.6065868070755882, 1.6073083163090722, 1.6066905383406014, 1.6066025949659801, 1.6082298307387504, 1.6072281802620598, 1.6062250015966606, 1.606485825258327, 1.6047388526606443, 1.6043057445626345, 1.6048957334559149, 1.6037036887139131, 1.6036403739002147, 1.6049157935018805, 1.604055551081064, 1.604071196664143, 1.604128717005938, 1.6045915520641407, 1.6044865500163563, 1.6051856527970538, 1.6047497641276844, 1.6054271808007277, 1.6048461668597067, 1.6040874658938504, 1.6059772774503736, 1.606782702585355, 1.6069920039529284, 1.6083411093807376, 1.6061050535618573, 1.6054882453188717, 1.606312335614109, 1.6068669502762543, 1.6066581249628553, 1.6062687627591914, 1.6071395684149856, 1.6080286446071805, 1.6071035231667004, 1.6083271983026088, 1.606125880149002, 1.6082717798809307, 1.608049988746643, 1.6075797304144046, 1.6089549976812403, 1.6083277512849454, 1.607727317974485, 1.6081605695542835, 1.6064232172832895, 1.6047272523635714, 1.6068742835071483], 'val_acc': [0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.24302134615153514, 0.2446633822763299, 0.2413793100267404, 0.2413793100267404, 0.24466338237420288, 0.24302134615153514, 0.24466338237420288, 0.24630541849899762, 0.24630541840112463, 0.2446633822763299, 0.2446633822763299, 0.24630541840112463, 0.24630541840112463, 0.24630541830325164, 0.24302134615153514, 0.2446633821784569, 0.25123152687338185, 0.25123152687338185, 0.23316912950063964, 0.24302134624940813, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.24466338237420288, 0.23316912950063964, 0.23316912950063964, 0.24794745462379236, 0.24466338237420288, 0.24302134624940813, 0.24302134605366216, 0.24958949065071412, 0.2446633822763299, 0.24630541840112463, 0.24630541840112463, 0.24630541849899762, 0.23316912950063964, 0.23481116552756143, 0.24794745472166535, 0.23481116582118036, 0.24794745472166535, 0.2446633822763299, 0.24794745472166535, 0.24630541849899762, 0.24794745472166535, 0.24794745472166535, 0.24302134615153514, 0.24794745472166535, 0.2446633822763299, 0.2463054185968706, 0.2463054185968706, 0.24302134634728112, 0.24137931022248635, 0.2446633822763299, 0.24302134615153514, 0.24302134634728112, 0.24137931022248635, 0.24137931022248635, 0.2413793100267404, 0.24302134615153514, 0.24466338247207586, 0.24466338247207586, 0.24630541840112463, 0.24794745472166535, 0.24302134615153514, 0.24794745472166535, 0.23481116552756143, 0.2413793101246134, 0.24630541840112463, 0.24302134624940813, 0.23809523767927793, 0.24466338237420288, 0.24630541840112463, 0.2463054185968706, 0.23645320155448318, 0.2413793100267404, 0.2463054185968706, 0.24794745452591938, 0.24630541840112463, 0.24794745462379236, 0.24630541840112463, 0.23152709318009895, 0.23152709318009895, 0.23809523767927793, 0.23152709308222597, 0.23152709308222597, 0.23809523777715091, 0.23481116533181545, 0.22495894868091998, 0.23809523758140494, 0.2183908036923761, 0.2282430207347635, 0.224958948583047, 0.24630541820537868, 0.2331691292070207, 0.24466338198271095, 0.24137930963524848, 0.238095237287786, 0.24466338208058394, 0.23973727370619968, 0.23316912901127476, 0.23316912910914772, 0.2348111652339425, 0.238095237385659, 0.22003284011078977, 0.2430213459557892, 0.23152709278860703, 0.21182265909532413, 0.2216748758440926, 0.22331691226250627, 0.23973727370619968, 0.23973727370619968, 0.2413793099288674, 0.24302134605366216, 0.2298850566638123, 0.23973727370619968, 0.23645320135873724, 0.238095237287786, 0.23152709278860703, 0.23809523718991302, 0.23481116494032353, 0.21839080458546703, 0.24137930943950253, 0.244663381786965, 0.23809523718991302, 0.2430213454664243, 0.24630541781388676, 0.244663381786965, 0.23809523718991302, 0.2413793095373755, 0.23973727341258075, 0.24137930963524848, 0.23809523718991302, 0.24630541791175975, 0.24302134556429728, 0.2397372733147078, 0.24137930934162954, 0.24137930934162954, 0.2397372732168348, 0.23809523709204006, 0.2430213454664243, 0.2430213453685513, 0.23809523718991302, 0.238095237287786, 0.23809523709204006, 0.24137930934162954, 0.24302134556429728, 0.2397372733147078, 0.24466338188483797, 0.244663381786965, 0.244663381786965, 0.24466338168909202, 0.24794745413442745, 0.23481116484245057, 0.238095237287786, 0.24302134576004322, 0.22824302044114456, 0.238095237385659, 0.24137930983099445, 0.23973727360832672, 0.23316912901127476, 0.2446633821784569, 0.24630541830325164, 0.24466338198271095, 0.2446633821784569, 0.23973727341258075, 0.23481116474457758, 0.24794745433017343, 0.24137930973312147, 0.23316912861978284, 0.23316912970861975, 0.24958949016134924, 0.23152709338807903, 0.24466338208058394, 0.24466338188483797, 0.24466338188483797, 0.23481116474457758, 0.23645320195820924, 0.2413793095373755, 0.24630541791175975, 0.24794745384080852, 0.2397372733147078, 0.251231525992525, 0.23152709338807903, 0.23152709338807903, 0.244663381786965, 0.2397372732168348, 0.22824302123636253, 0.22660098412060387, 0.23645320116299126, 0.23973727351045374, 0.22660098412060387, 0.23316912861978284, 0.22988505736115727, 0.2315270924949881, 0.2298850572632843, 0.23152709269073404, 0.2282430202453986, 0.2282430202453986, 0.23152709358382498, 0.2233169121646333, 0.2233169121646333, 0.23152709358382498, 0.2282430202453986, 0.22495894809368208, 0.22495894809368208, 0.23152709288648002, 0.23152709358382498, 0.23152709358382498, 0.23152709358382498, 0.2282430213342355, 0.24958949035709518, 0.22331691206676033, 0.22988505736115727, 0.22331691295985126, 0.2430213453685513, 0.22331691206676033, 0.2430213464573882, 0.23973727420779872, 0.22988505736115727, 0.23152709348595202, 0.23316912901127476, 0.23973727341258075, 0.23481116563766852, 0.22988505637019335, 0.23316912961074676, 0.23152709269073404, 0.2348111657355415, 0.23316912951287377, 0.2495894898677303, 0.2249589479958091, 0.23316912961074676, 0.23152709348595202, 0.23316912961074676, 0.23809523718991302, 0.22988505745903023, 0.22660098520944075, 0.22331691295985126, 0.23152709269073404, 0.2446633822763299, 0.23973727370619968, 0.22331691196888734, 0.2249589479958091, 0.2183908035945031, 0.24466338198271095, 0.2216748758440926, 0.21839080349663012, 0.22331691286197827, 0.22167487673718353, 0.23809523699416707, 0.2315270924949881, 0.22495894888890006, 0.23645320195820924, 0.23645320086937233, 0.22495894898677302, 0.23481116474457758, 0.23645320195820924, 0.2282430213342355, 0.24137930934162954, 0.2315270924949881, 0.22988505637019335, 0.23316912970861975, 0.23316912871765583, 0.2348111658334145, 0.2282430213342355, 0.21839080438972105, 0.23809523718991302, 0.23152709269073404, 0.22331691196888734, 0.22660098421847683, 0.2495894902592222, 0.238095237287786, 0.23973727341258075, 0.22660098421847683, 0.2216748757462196, 0.23645320116299126, 0.24794745413442745, 0.24302134566217024, 0.23481116494032353, 0.24466338168909202, 0.23645320086937233, 0.23645320106511827, 0.24630541781388676, 0.22003284071026177, 0.23316912861978284, 0.2331691288155288, 0.22824302044114456, 0.2249589479958091, 0.22988505637019335, 0.2282430213342355], 'loss': [1.608797152771842, 1.606676176881888, 1.6053833765660468, 1.6051249741039237, 1.6047404625087793, 1.6041811691906906, 1.6043370350675172, 1.6037589541695692, 1.6039437785040915, 1.6031680147750667, 1.6030257158455663, 1.6031683928667888, 1.6029267746809817, 1.6029430781546559, 1.6024747016248762, 1.6028700350735956, 1.6028282519973034, 1.60203666109324, 1.602151493468079, 1.6018051807395732, 1.6014522305259469, 1.6015605770342158, 1.6012719418233914, 1.6016234652462435, 1.6012388491777425, 1.6010749652889964, 1.6011284371421077, 1.601420000299536, 1.601102166694782, 1.6013213390687162, 1.6042872741237069, 1.6026416563645036, 1.6052146344214249, 1.6036476743784285, 1.6044947893467771, 1.6036575664974582, 1.6031883895274794, 1.6033283071596276, 1.6027207385343203, 1.601942576422094, 1.6017871620718704, 1.6014659884039626, 1.6015825434142315, 1.6014869760438892, 1.6020756233888975, 1.6017572780898954, 1.6019269062508303, 1.6028310781631627, 1.6032390521537108, 1.6023577443383312, 1.602284139137738, 1.6016019974890676, 1.6016278468607876, 1.6013819435538696, 1.6016203474459951, 1.6014384226632559, 1.6015032220425303, 1.6016313497535501, 1.6015257870147361, 1.6014791582154542, 1.601310434723292, 1.6010552132643714, 1.601304867233339, 1.6011766937479102, 1.6012039799465045, 1.6006716305715103, 1.6007874169633618, 1.6011780440195384, 1.6011382832909022, 1.6008867530607345, 1.6007619217925493, 1.600430054635238, 1.600122087153566, 1.6007074068214369, 1.600156536043547, 1.5999762829813868, 1.6002639249609727, 1.600200257066339, 1.600951318966045, 1.6000187334338742, 1.6000395573629735, 1.5995884679915724, 1.5996167521701945, 1.5997549804084354, 1.5993607168080135, 1.5995749026108572, 1.5999181560911926, 1.5994276187258334, 1.5995688750268988, 1.5994311308224343, 1.5994507772966577, 1.5991509244672082, 1.5993979991094287, 1.5987072357651635, 1.5986720489525452, 1.599071981774708, 1.5989765736601436, 1.5988486819198733, 1.599524079211194, 1.6004154232248389, 1.600797323965194, 1.5987875425350495, 1.599198543511377, 1.5988968839390811, 1.5988068966405347, 1.5987336223375137, 1.598306675319554, 1.598201505506308, 1.598514627627034, 1.5983405286526533, 1.597900341668413, 1.5977880955231998, 1.5984816934538573, 1.598626297112608, 1.5992076518354474, 1.59721419679066, 1.598522355913871, 1.5993702688745894, 1.5991558354003719, 1.598423458663345, 1.5988144344862483, 1.5981749145891633, 1.5984727621078492, 1.5979388090619315, 1.5981660137431088, 1.5976871823872874, 1.5979081077007786, 1.5983163040521453, 1.5981148099507638, 1.5982960950177798, 1.5989782896864342, 1.5993958148623395, 1.5981536973917998, 1.599455767931145, 1.5991843877142216, 1.599375358254513, 1.5998585603075595, 1.5995484623820875, 1.5987973628837224, 1.598023613620343, 1.5983940613098457, 1.5973136779953567, 1.5978140065802195, 1.597568115218709, 1.5979148316921885, 1.5977293356243345, 1.5969536123334505, 1.59778834554449, 1.5978076524558253, 1.5973155973873099, 1.5970594308214756, 1.5978700916380364, 1.5975281460329247, 1.5967067607374408, 1.5968027374338076, 1.5962768342705478, 1.5959012038898663, 1.596328421686709, 1.5962718973414365, 1.5962299748367843, 1.5960165482037365, 1.5962784879261462, 1.596019836030212, 1.5963938183853024, 1.5961792080309356, 1.5961183412853452, 1.5960181827173097, 1.5958193103880363, 1.5959469384481284, 1.5953996969689088, 1.5952449682067307, 1.5956203376977596, 1.5952962812701779, 1.5958065124507803, 1.5960731692382688, 1.5957438612375905, 1.594989300704345, 1.5967575693522147, 1.5947965303730427, 1.5950258186955226, 1.595302256960154, 1.5965493729961482, 1.5965090703180929, 1.5959688434365837, 1.5954683632821274, 1.5954411801371486, 1.5952438462196679, 1.5950265973989968, 1.595193170522028, 1.5952796605578194, 1.5955919094888582, 1.595068534150016, 1.5948503320956378, 1.59535683528109, 1.5952614579112623, 1.5955035550883174, 1.5954999589332566, 1.5950589538354893, 1.5956602911195226, 1.5940186958782971, 1.5950499529221709, 1.5936831659360098, 1.5935425866555875, 1.5942962995055274, 1.5946756599375356, 1.5936296280404625, 1.5941939479271734, 1.5941159301714731, 1.593835641275441, 1.59357953619908, 1.5936679616356288, 1.5938721473212114, 1.5932777143356982, 1.5933787181881665, 1.5930088061816394, 1.5929981754056237, 1.5931869890655581, 1.5927093765329285, 1.5929443170402573, 1.5933333857593106, 1.5932611435590582, 1.5928433666973387, 1.5929214322836247, 1.5928796986534854, 1.5937130358184877, 1.594411200713328, 1.595066035895377, 1.5933407351221638, 1.5945532336616908, 1.593997678081113, 1.5937463606652293, 1.5933714248806055, 1.593370423179877, 1.5928515429859045, 1.5927589695066886, 1.5928673338351553, 1.5933049153987877, 1.5937579706219431, 1.5942378345211428, 1.593961572500225, 1.5937319610642702, 1.5933870797774141, 1.593643241643416, 1.5940145513605044, 1.593490110383631, 1.593486834012997, 1.5931637905951153, 1.5930595139948005, 1.5933329970929657, 1.5931612869552518, 1.5932451089060038, 1.593473470284464, 1.5942573551279808, 1.5942609832027366, 1.593840128440387, 1.5928226676075365, 1.5933503046662412, 1.594090286368462, 1.593545386335933, 1.5927196030254482, 1.5927288440218696, 1.592578173858674, 1.5929234074371306, 1.593115507895452, 1.5924888428231774, 1.5927174969620284, 1.5919755525412747, 1.5923119766266685, 1.5920141381159945, 1.5919461653217888, 1.5925135794116732, 1.5923387192113199, 1.591822061156835, 1.5918734554882168, 1.5922543739636086, 1.5918632207709906, 1.5925893106989302, 1.5921923149782529, 1.5920397314937207, 1.591924151209101, 1.591815556393022, 1.5916195316236366, 1.5917765128783867, 1.5916950098053386, 1.5919243916349, 1.591817307717012, 1.5914093886557545, 1.592203287030637, 1.5915441697627857, 1.5913449111660403, 1.590603461206816, 1.5909099062854994, 1.590517893904778, 1.5904323499060755, 1.5912747209321791, 1.590991808354732, 1.5898908310602333, 1.5913289741324204, 1.591599046718903, 1.5909062200503183], 'acc': [0.20698151939703455, 0.2328542083195837, 0.2328542114528053, 0.2328542085337688, 0.23285420833794243, 0.23285420929871545, 0.23285421086532623, 0.23285421049203225, 0.23285420892542147, 0.23285420990455322, 0.23285420990455322, 0.23531827461548166, 0.23819301911692844, 0.2386036965391719, 0.2386036973224773, 0.23860369694918332, 0.23696098530194598, 0.24065708325873655, 0.24106776246177586, 0.24353182756435698, 0.23983572980339277, 0.23983572939338135, 0.24024640642397213, 0.2427104725240437, 0.24188911568457586, 0.2455852160647175, 0.24024640644233083, 0.23983572941174008, 0.23819301835198178, 0.23901437259063094, 0.23285421010037957, 0.23572895225191018, 0.23285420969036816, 0.2328542089437802, 0.23367556551398683, 0.23367556396573477, 0.23285421049203225, 0.2332648869535027, 0.2361396294783273, 0.2357289516460724, 0.23613963143659078, 0.2422997949059739, 0.23819301755031766, 0.23942505099200614, 0.23655030844882283, 0.23696098685019804, 0.2386036965391719, 0.2353182744196553, 0.23531827461548166, 0.23244353248230976, 0.24106776107263272, 0.23531827502549307, 0.2361396298516213, 0.2369609866727304, 0.239425050563636, 0.23778234071555324, 0.2390143739797741, 0.2377823402871831, 0.23860369577422524, 0.23901437280481602, 0.23819301934947223, 0.23819301796032907, 0.23408624315041537, 0.23737166409497387, 0.23942505181202897, 0.23696098489193457, 0.2353182740280026, 0.23655030586636286, 0.2410677626576022, 0.23942505020870075, 0.2398357284142496, 0.23778234151721736, 0.24188911630877236, 0.2402464066381572, 0.23942505314609597, 0.24065708464787972, 0.24106775966513083, 0.23737166211835167, 0.23778234208633767, 0.23983572980339277, 0.2365503078613438, 0.23572895303521557, 0.2435318279560097, 0.23901437456725314, 0.23901437591967886, 0.23737166270583074, 0.24024640605067815, 0.2422997953159853, 0.24312114976881954, 0.23819301934947223, 0.24188911591711965, 0.24024640740310388, 0.23572895364105334, 0.2361396314549495, 0.2365503070780384, 0.23983572999921912, 0.23572895340850955, 0.23778234189051134, 0.23408624275876266, 0.23285421029620593, 0.23983573017668675, 0.24722792573533264, 0.2406570842378683, 0.24106776087680637, 0.24106776068098001, 0.24024640740310388, 0.23737166350749483, 0.23737166090667616, 0.23655030846718156, 0.24106776285342857, 0.23983572782677057, 0.24271047312988148, 0.23901437358812141, 0.23819301774614401, 0.24394250498660047, 0.24722792614534406, 0.24147844025731333, 0.2406570842378683, 0.23696098647690406, 0.23655030923212822, 0.23737166407661517, 0.2431211499462872, 0.24271047271987006, 0.2381930181377967, 0.23819301970440748, 0.23696098608525137, 0.24188911732462154, 0.24024640857806195, 0.23860369556004016, 0.23983573056833946, 0.2402464058548518, 0.23901437378394774, 0.24147844029403076, 0.2365503090363019, 0.23901437533219982, 0.23819301990023384, 0.2386036957558665, 0.23860369556004016, 0.24147843888652887, 0.2418891173062628, 0.24271047232821738, 0.24476386006363118, 0.23449691937934203, 0.24147843869070254, 0.24394250498660047, 0.24271047334406656, 0.24271047134908563, 0.2410677622475908, 0.24476386082857787, 0.2435318263893989, 0.2447638594394347, 0.2443531815888211, 0.24065708464787972, 0.24435318299632297, 0.24804928273390942, 0.2488706358159592, 0.2533880913771643, 0.2414784389048876, 0.24394250694486394, 0.24271047193656467, 0.25051334863815466, 0.2476386047241869, 0.24312115014211352, 0.2427104733073491, 0.24517453883830037, 0.24599589270365557, 0.25462012423137376, 0.25585215728140953, 0.25790554635089036, 0.2542094432475386, 0.25215605852295486, 0.246406570499193, 0.2546201240355474, 0.25215605754382314, 0.2513347029135212, 0.2537987686035814, 0.24599589387861365, 0.241889116112946, 0.24476386120187182, 0.25338808801139895, 0.24804928018816688, 0.24763860454671927, 0.2472279251478536, 0.24188911652295741, 0.2546201224872953, 0.2574948665787307, 0.2513347015243781, 0.24763860531166593, 0.2480492823422567, 0.2554414798591661, 0.2525667341827612, 0.25379876938688684, 0.2533880911996967, 0.2505133468757175, 0.2509240250812664, 0.2480492813814837, 0.2513347015243781, 0.2501026670668404, 0.2529774127799628, 0.25010267023677946, 0.2488706365992646, 0.2501026690618213, 0.24845996035197923, 0.24804928195060402, 0.24681724909639458, 0.24476386120187182, 0.2451745384466477, 0.24763860395924023, 0.24599589425190763, 0.24558521761296956, 0.2501026690618213, 0.24681724831308918, 0.2529774127799628, 0.2488706362076119, 0.24599589407444, 0.24599589523103937, 0.25297741254741896, 0.25215605814966086, 0.25010266867016867, 0.2501026690801801, 0.24517453764498356, 0.24681724948804726, 0.2542094475924357, 0.25667351055928567, 0.24640657245745648, 0.2488706354243065, 0.24928131637142426, 0.24763860374505514, 0.25051334883398096, 0.2517453787507952, 0.24804928079400465, 0.2542094438350176, 0.2533880898105535, 0.24969199224541563, 0.2468172475297838, 0.2464065713008571, 0.24804928116729863, 0.24229979373101582, 0.24845995841207447, 0.2480492815589513, 0.24517453825082133, 0.24969199105209883, 0.25215605854131357, 0.2529774141507472, 0.2501026692576477, 0.25585215728140953, 0.2554414804466451, 0.25379876782027605, 0.2501026686885274, 0.2566735123217228, 0.2501026672626668, 0.2529774127983215, 0.25544147770507625, 0.24599589346860223, 0.2550308006928442, 0.2529774127799628, 0.24969199242288326, 0.25174537933827423, 0.2550308020452699, 0.2550308014577909, 0.258316221619044, 0.25462012344806834, 0.25215605715217043, 0.25420944543834584, 0.2579055447842796, 0.25092402269463276, 0.25790554537175864, 0.257084189762325, 0.2579055422018196, 0.2579055451759323, 0.2550308008703118, 0.2550307988753309, 0.25503079969535375, 0.2562628339203476, 0.2554414804466451, 0.25544147966333974, 0.25872690139120363, 0.25995893130801784, 0.26160164154775334, 0.25913757804850046, 0.25585215728140953, 0.26283367420613646, 0.26201232037749866, 0.25913757861762077, 0.25872689943294014, 0.25872690158703, 0.26119096592466445, 0.26160164238613487, 0.2591375760718782, 0.2579055447842796, 0.261601641351927, 0.25913757783431535, 0.25995893130801784, 0.2611909653371854, 0.25749486581378406, 0.26160164295525523, 0.2644763862817439, 0.2583162209948475, 0.25913757705101, 0.2603696109027099]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
