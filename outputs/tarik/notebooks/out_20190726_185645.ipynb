{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf13.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 18:56:45 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'AllShfRnd', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['by', 'eg', 'sg', 'ds', 'ek', 'yd', 'eb', 'sk', 'my', 'eo', 'aa', 'ib', 'mb', 'ck', 'ce'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F6183CD278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F614F76EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7128, Accuracy:0.0431, Validation Loss:2.7058, Validation Accuracy:0.0821\n",
    "Epoch #2: Loss:2.7023, Accuracy:0.0817, Validation Loss:2.6988, Validation Accuracy:0.0870\n",
    "Epoch #3: Loss:2.6964, Accuracy:0.0830, Validation Loss:2.6931, Validation Accuracy:0.0837\n",
    "Epoch #4: Loss:2.6904, Accuracy:0.0825, Validation Loss:2.6867, Validation Accuracy:0.0805\n",
    "Epoch #5: Loss:2.6842, Accuracy:0.0883, Validation Loss:2.6821, Validation Accuracy:0.0985\n",
    "Epoch #6: Loss:2.6796, Accuracy:0.1113, Validation Loss:2.6774, Validation Accuracy:0.0985\n",
    "Epoch #7: Loss:2.6746, Accuracy:0.1072, Validation Loss:2.6705, Validation Accuracy:0.1149\n",
    "Epoch #8: Loss:2.6685, Accuracy:0.1179, Validation Loss:2.6637, Validation Accuracy:0.1051\n",
    "Epoch #9: Loss:2.6615, Accuracy:0.1248, Validation Loss:2.6560, Validation Accuracy:0.1281\n",
    "Epoch #10: Loss:2.6525, Accuracy:0.1318, Validation Loss:2.6464, Validation Accuracy:0.1494\n",
    "Epoch #11: Loss:2.6418, Accuracy:0.1487, Validation Loss:2.6358, Validation Accuracy:0.1511\n",
    "Epoch #12: Loss:2.6297, Accuracy:0.1511, Validation Loss:2.6234, Validation Accuracy:0.1576\n",
    "Epoch #13: Loss:2.6155, Accuracy:0.1474, Validation Loss:2.6068, Validation Accuracy:0.1576\n",
    "Epoch #14: Loss:2.6027, Accuracy:0.1515, Validation Loss:2.5937, Validation Accuracy:0.1593\n",
    "Epoch #15: Loss:2.5834, Accuracy:0.1630, Validation Loss:2.5790, Validation Accuracy:0.1593\n",
    "Epoch #16: Loss:2.5598, Accuracy:0.1602, Validation Loss:2.5549, Validation Accuracy:0.1658\n",
    "Epoch #17: Loss:2.5404, Accuracy:0.1593, Validation Loss:2.5465, Validation Accuracy:0.1626\n",
    "Epoch #18: Loss:2.5315, Accuracy:0.1659, Validation Loss:2.5331, Validation Accuracy:0.1675\n",
    "Epoch #19: Loss:2.5186, Accuracy:0.1610, Validation Loss:2.5207, Validation Accuracy:0.1626\n",
    "Epoch #20: Loss:2.5103, Accuracy:0.1696, Validation Loss:2.5228, Validation Accuracy:0.1691\n",
    "Epoch #21: Loss:2.5020, Accuracy:0.1717, Validation Loss:2.5135, Validation Accuracy:0.1658\n",
    "Epoch #22: Loss:2.4967, Accuracy:0.1630, Validation Loss:2.5152, Validation Accuracy:0.1741\n",
    "Epoch #23: Loss:2.4941, Accuracy:0.1766, Validation Loss:2.5030, Validation Accuracy:0.1642\n",
    "Epoch #24: Loss:2.4855, Accuracy:0.1749, Validation Loss:2.5036, Validation Accuracy:0.1724\n",
    "Epoch #25: Loss:2.4803, Accuracy:0.1762, Validation Loss:2.4967, Validation Accuracy:0.1658\n",
    "Epoch #26: Loss:2.4773, Accuracy:0.1741, Validation Loss:2.4951, Validation Accuracy:0.1708\n",
    "Epoch #27: Loss:2.4740, Accuracy:0.1754, Validation Loss:2.4919, Validation Accuracy:0.1724\n",
    "Epoch #28: Loss:2.4717, Accuracy:0.1754, Validation Loss:2.4897, Validation Accuracy:0.1675\n",
    "Epoch #29: Loss:2.4678, Accuracy:0.1770, Validation Loss:2.4890, Validation Accuracy:0.1708\n",
    "Epoch #30: Loss:2.4662, Accuracy:0.1762, Validation Loss:2.4899, Validation Accuracy:0.1724\n",
    "Epoch #31: Loss:2.4637, Accuracy:0.1774, Validation Loss:2.4884, Validation Accuracy:0.1741\n",
    "Epoch #32: Loss:2.4627, Accuracy:0.1766, Validation Loss:2.4852, Validation Accuracy:0.1708\n",
    "Epoch #33: Loss:2.4616, Accuracy:0.1762, Validation Loss:2.4865, Validation Accuracy:0.1724\n",
    "Epoch #34: Loss:2.4621, Accuracy:0.1749, Validation Loss:2.4820, Validation Accuracy:0.1708\n",
    "Epoch #35: Loss:2.4598, Accuracy:0.1762, Validation Loss:2.4812, Validation Accuracy:0.1724\n",
    "Epoch #36: Loss:2.4587, Accuracy:0.1745, Validation Loss:2.4818, Validation Accuracy:0.1675\n",
    "Epoch #37: Loss:2.4570, Accuracy:0.1754, Validation Loss:2.4820, Validation Accuracy:0.1675\n",
    "Epoch #38: Loss:2.4587, Accuracy:0.1762, Validation Loss:2.4815, Validation Accuracy:0.1675\n",
    "Epoch #39: Loss:2.4617, Accuracy:0.1725, Validation Loss:2.4936, Validation Accuracy:0.1823\n",
    "Epoch #40: Loss:2.4620, Accuracy:0.1762, Validation Loss:2.4762, Validation Accuracy:0.1675\n",
    "Epoch #41: Loss:2.4565, Accuracy:0.1770, Validation Loss:2.4794, Validation Accuracy:0.1790\n",
    "Epoch #42: Loss:2.4561, Accuracy:0.1782, Validation Loss:2.4752, Validation Accuracy:0.1708\n",
    "Epoch #43: Loss:2.4535, Accuracy:0.1799, Validation Loss:2.4745, Validation Accuracy:0.1741\n",
    "Epoch #44: Loss:2.4529, Accuracy:0.1791, Validation Loss:2.4817, Validation Accuracy:0.1741\n",
    "Epoch #45: Loss:2.4523, Accuracy:0.1774, Validation Loss:2.4785, Validation Accuracy:0.1691\n",
    "Epoch #46: Loss:2.4516, Accuracy:0.1778, Validation Loss:2.4735, Validation Accuracy:0.1708\n",
    "Epoch #47: Loss:2.4516, Accuracy:0.1766, Validation Loss:2.4708, Validation Accuracy:0.1691\n",
    "Epoch #48: Loss:2.4522, Accuracy:0.1758, Validation Loss:2.4713, Validation Accuracy:0.1741\n",
    "Epoch #49: Loss:2.4465, Accuracy:0.1774, Validation Loss:2.4672, Validation Accuracy:0.1708\n",
    "Epoch #50: Loss:2.4484, Accuracy:0.1778, Validation Loss:2.4715, Validation Accuracy:0.1757\n",
    "Epoch #51: Loss:2.4462, Accuracy:0.1770, Validation Loss:2.4687, Validation Accuracy:0.1691\n",
    "Epoch #52: Loss:2.4459, Accuracy:0.1774, Validation Loss:2.4747, Validation Accuracy:0.1757\n",
    "Epoch #53: Loss:2.4487, Accuracy:0.1774, Validation Loss:2.4726, Validation Accuracy:0.1708\n",
    "Epoch #54: Loss:2.4504, Accuracy:0.1774, Validation Loss:2.4710, Validation Accuracy:0.1724\n",
    "Epoch #55: Loss:2.4483, Accuracy:0.1778, Validation Loss:2.4708, Validation Accuracy:0.1675\n",
    "Epoch #56: Loss:2.4474, Accuracy:0.1782, Validation Loss:2.4710, Validation Accuracy:0.1675\n",
    "Epoch #57: Loss:2.4477, Accuracy:0.1766, Validation Loss:2.4709, Validation Accuracy:0.1675\n",
    "Epoch #58: Loss:2.4470, Accuracy:0.1774, Validation Loss:2.4706, Validation Accuracy:0.1675\n",
    "Epoch #59: Loss:2.4480, Accuracy:0.1799, Validation Loss:2.4720, Validation Accuracy:0.1691\n",
    "Epoch #60: Loss:2.4467, Accuracy:0.1778, Validation Loss:2.4710, Validation Accuracy:0.1658\n",
    "Epoch #61: Loss:2.4469, Accuracy:0.1774, Validation Loss:2.4714, Validation Accuracy:0.1724\n",
    "Epoch #62: Loss:2.4441, Accuracy:0.1774, Validation Loss:2.4676, Validation Accuracy:0.1691\n",
    "Epoch #63: Loss:2.4440, Accuracy:0.1786, Validation Loss:2.4691, Validation Accuracy:0.1675\n",
    "Epoch #64: Loss:2.4445, Accuracy:0.1778, Validation Loss:2.4696, Validation Accuracy:0.1724\n",
    "Epoch #65: Loss:2.4430, Accuracy:0.1795, Validation Loss:2.4689, Validation Accuracy:0.1691\n",
    "Epoch #66: Loss:2.4441, Accuracy:0.1799, Validation Loss:2.4688, Validation Accuracy:0.1691\n",
    "Epoch #67: Loss:2.4434, Accuracy:0.1782, Validation Loss:2.4687, Validation Accuracy:0.1658\n",
    "Epoch #68: Loss:2.4423, Accuracy:0.1791, Validation Loss:2.4695, Validation Accuracy:0.1708\n",
    "Epoch #69: Loss:2.4419, Accuracy:0.1774, Validation Loss:2.4670, Validation Accuracy:0.1658\n",
    "Epoch #70: Loss:2.4402, Accuracy:0.1778, Validation Loss:2.4687, Validation Accuracy:0.1708\n",
    "Epoch #71: Loss:2.4401, Accuracy:0.1786, Validation Loss:2.4692, Validation Accuracy:0.1691\n",
    "Epoch #72: Loss:2.4403, Accuracy:0.1807, Validation Loss:2.4712, Validation Accuracy:0.1708\n",
    "Epoch #73: Loss:2.4393, Accuracy:0.1786, Validation Loss:2.4685, Validation Accuracy:0.1675\n",
    "Epoch #74: Loss:2.4391, Accuracy:0.1786, Validation Loss:2.4700, Validation Accuracy:0.1708\n",
    "Epoch #75: Loss:2.4404, Accuracy:0.1799, Validation Loss:2.4671, Validation Accuracy:0.1675\n",
    "Epoch #76: Loss:2.4413, Accuracy:0.1733, Validation Loss:2.4638, Validation Accuracy:0.1741\n",
    "Epoch #77: Loss:2.4376, Accuracy:0.1803, Validation Loss:2.4680, Validation Accuracy:0.1691\n",
    "Epoch #78: Loss:2.4361, Accuracy:0.1791, Validation Loss:2.4636, Validation Accuracy:0.1757\n",
    "Epoch #79: Loss:2.4350, Accuracy:0.1786, Validation Loss:2.4682, Validation Accuracy:0.1691\n",
    "Epoch #80: Loss:2.4351, Accuracy:0.1799, Validation Loss:2.4647, Validation Accuracy:0.1708\n",
    "Epoch #81: Loss:2.4360, Accuracy:0.1791, Validation Loss:2.4665, Validation Accuracy:0.1724\n",
    "Epoch #82: Loss:2.4351, Accuracy:0.1828, Validation Loss:2.4662, Validation Accuracy:0.1724\n",
    "Epoch #83: Loss:2.4352, Accuracy:0.1807, Validation Loss:2.4672, Validation Accuracy:0.1741\n",
    "Epoch #84: Loss:2.4338, Accuracy:0.1815, Validation Loss:2.4663, Validation Accuracy:0.1708\n",
    "Epoch #85: Loss:2.4330, Accuracy:0.1848, Validation Loss:2.4655, Validation Accuracy:0.1724\n",
    "Epoch #86: Loss:2.4324, Accuracy:0.1881, Validation Loss:2.4656, Validation Accuracy:0.1708\n",
    "Epoch #87: Loss:2.4335, Accuracy:0.1840, Validation Loss:2.4665, Validation Accuracy:0.1691\n",
    "Epoch #88: Loss:2.4378, Accuracy:0.1811, Validation Loss:2.4818, Validation Accuracy:0.1790\n",
    "Epoch #89: Loss:2.4405, Accuracy:0.1836, Validation Loss:2.4676, Validation Accuracy:0.1773\n",
    "Epoch #90: Loss:2.4375, Accuracy:0.1811, Validation Loss:2.4753, Validation Accuracy:0.1823\n",
    "Epoch #91: Loss:2.4385, Accuracy:0.1791, Validation Loss:2.4721, Validation Accuracy:0.1708\n",
    "Epoch #92: Loss:2.4391, Accuracy:0.1749, Validation Loss:2.4724, Validation Accuracy:0.1773\n",
    "Epoch #93: Loss:2.4350, Accuracy:0.1848, Validation Loss:2.4628, Validation Accuracy:0.1741\n",
    "Epoch #94: Loss:2.4333, Accuracy:0.1848, Validation Loss:2.4674, Validation Accuracy:0.1773\n",
    "Epoch #95: Loss:2.4346, Accuracy:0.1873, Validation Loss:2.4651, Validation Accuracy:0.1708\n",
    "Epoch #96: Loss:2.4325, Accuracy:0.1877, Validation Loss:2.4637, Validation Accuracy:0.1741\n",
    "Epoch #97: Loss:2.4315, Accuracy:0.1869, Validation Loss:2.4653, Validation Accuracy:0.1757\n",
    "Epoch #98: Loss:2.4313, Accuracy:0.1852, Validation Loss:2.4634, Validation Accuracy:0.1708\n",
    "Epoch #99: Loss:2.4322, Accuracy:0.1877, Validation Loss:2.4657, Validation Accuracy:0.1724\n",
    "Epoch #100: Loss:2.4330, Accuracy:0.1836, Validation Loss:2.4682, Validation Accuracy:0.1823\n",
    "Epoch #101: Loss:2.4341, Accuracy:0.1844, Validation Loss:2.4761, Validation Accuracy:0.1642\n",
    "Epoch #102: Loss:2.4564, Accuracy:0.1671, Validation Loss:2.5113, Validation Accuracy:0.1839\n",
    "Epoch #103: Loss:2.4478, Accuracy:0.1885, Validation Loss:2.4684, Validation Accuracy:0.1724\n",
    "Epoch #104: Loss:2.4437, Accuracy:0.1819, Validation Loss:2.4675, Validation Accuracy:0.1823\n",
    "Epoch #105: Loss:2.4390, Accuracy:0.1860, Validation Loss:2.4707, Validation Accuracy:0.1724\n",
    "Epoch #106: Loss:2.4335, Accuracy:0.1840, Validation Loss:2.4654, Validation Accuracy:0.1626\n",
    "Epoch #107: Loss:2.4310, Accuracy:0.1844, Validation Loss:2.4751, Validation Accuracy:0.1790\n",
    "Epoch #108: Loss:2.4308, Accuracy:0.1856, Validation Loss:2.4610, Validation Accuracy:0.1658\n",
    "Epoch #109: Loss:2.4300, Accuracy:0.1864, Validation Loss:2.4638, Validation Accuracy:0.1741\n",
    "Epoch #110: Loss:2.4286, Accuracy:0.1864, Validation Loss:2.4706, Validation Accuracy:0.1823\n",
    "Epoch #111: Loss:2.4289, Accuracy:0.1869, Validation Loss:2.4628, Validation Accuracy:0.1741\n",
    "Epoch #112: Loss:2.4289, Accuracy:0.1828, Validation Loss:2.4624, Validation Accuracy:0.1724\n",
    "Epoch #113: Loss:2.4267, Accuracy:0.1864, Validation Loss:2.4637, Validation Accuracy:0.1823\n",
    "Epoch #114: Loss:2.4271, Accuracy:0.1873, Validation Loss:2.4625, Validation Accuracy:0.1724\n",
    "Epoch #115: Loss:2.4274, Accuracy:0.1864, Validation Loss:2.4648, Validation Accuracy:0.1724\n",
    "Epoch #116: Loss:2.4286, Accuracy:0.1869, Validation Loss:2.4671, Validation Accuracy:0.1741\n",
    "Epoch #117: Loss:2.4266, Accuracy:0.1926, Validation Loss:2.4635, Validation Accuracy:0.1790\n",
    "Epoch #118: Loss:2.4292, Accuracy:0.1897, Validation Loss:2.4665, Validation Accuracy:0.1839\n",
    "Epoch #119: Loss:2.4298, Accuracy:0.1844, Validation Loss:2.4641, Validation Accuracy:0.1856\n",
    "Epoch #120: Loss:2.4301, Accuracy:0.1860, Validation Loss:2.4680, Validation Accuracy:0.1856\n",
    "Epoch #121: Loss:2.4287, Accuracy:0.1848, Validation Loss:2.4639, Validation Accuracy:0.1806\n",
    "Epoch #122: Loss:2.4287, Accuracy:0.1901, Validation Loss:2.4678, Validation Accuracy:0.1773\n",
    "Epoch #123: Loss:2.4273, Accuracy:0.1848, Validation Loss:2.4651, Validation Accuracy:0.1757\n",
    "Epoch #124: Loss:2.4258, Accuracy:0.1864, Validation Loss:2.4603, Validation Accuracy:0.1741\n",
    "Epoch #125: Loss:2.4248, Accuracy:0.1877, Validation Loss:2.4594, Validation Accuracy:0.1741\n",
    "Epoch #126: Loss:2.4264, Accuracy:0.1869, Validation Loss:2.4587, Validation Accuracy:0.1773\n",
    "Epoch #127: Loss:2.4254, Accuracy:0.1893, Validation Loss:2.4562, Validation Accuracy:0.1757\n",
    "Epoch #128: Loss:2.4241, Accuracy:0.1897, Validation Loss:2.4621, Validation Accuracy:0.1691\n",
    "Epoch #129: Loss:2.4250, Accuracy:0.1860, Validation Loss:2.4619, Validation Accuracy:0.1691\n",
    "Epoch #130: Loss:2.4256, Accuracy:0.1889, Validation Loss:2.4607, Validation Accuracy:0.1675\n",
    "Epoch #131: Loss:2.4256, Accuracy:0.1856, Validation Loss:2.4633, Validation Accuracy:0.1773\n",
    "Epoch #132: Loss:2.4224, Accuracy:0.1881, Validation Loss:2.4635, Validation Accuracy:0.1691\n",
    "Epoch #133: Loss:2.4239, Accuracy:0.1877, Validation Loss:2.4697, Validation Accuracy:0.1741\n",
    "Epoch #134: Loss:2.4231, Accuracy:0.1910, Validation Loss:2.4662, Validation Accuracy:0.1708\n",
    "Epoch #135: Loss:2.4245, Accuracy:0.1856, Validation Loss:2.4665, Validation Accuracy:0.1724\n",
    "Epoch #136: Loss:2.4240, Accuracy:0.1840, Validation Loss:2.4658, Validation Accuracy:0.1691\n",
    "Epoch #137: Loss:2.4227, Accuracy:0.1844, Validation Loss:2.4657, Validation Accuracy:0.1691\n",
    "Epoch #138: Loss:2.4258, Accuracy:0.1840, Validation Loss:2.4634, Validation Accuracy:0.1691\n",
    "Epoch #139: Loss:2.4251, Accuracy:0.1873, Validation Loss:2.4655, Validation Accuracy:0.1658\n",
    "Epoch #140: Loss:2.4233, Accuracy:0.1897, Validation Loss:2.4585, Validation Accuracy:0.1708\n",
    "Epoch #141: Loss:2.4249, Accuracy:0.1873, Validation Loss:2.4633, Validation Accuracy:0.1675\n",
    "Epoch #142: Loss:2.4270, Accuracy:0.1848, Validation Loss:2.4620, Validation Accuracy:0.1691\n",
    "Epoch #143: Loss:2.4280, Accuracy:0.1803, Validation Loss:2.4613, Validation Accuracy:0.1675\n",
    "Epoch #144: Loss:2.4266, Accuracy:0.1852, Validation Loss:2.4660, Validation Accuracy:0.1708\n",
    "Epoch #145: Loss:2.4273, Accuracy:0.1860, Validation Loss:2.4601, Validation Accuracy:0.1757\n",
    "Epoch #146: Loss:2.4373, Accuracy:0.1807, Validation Loss:2.4601, Validation Accuracy:0.1741\n",
    "Epoch #147: Loss:2.4432, Accuracy:0.1795, Validation Loss:2.4650, Validation Accuracy:0.1691\n",
    "Epoch #148: Loss:2.4418, Accuracy:0.1795, Validation Loss:2.4649, Validation Accuracy:0.1675\n",
    "Epoch #149: Loss:2.4371, Accuracy:0.1782, Validation Loss:2.4626, Validation Accuracy:0.1724\n",
    "Epoch #150: Loss:2.4385, Accuracy:0.1721, Validation Loss:2.4675, Validation Accuracy:0.1724\n",
    "Epoch #151: Loss:2.4388, Accuracy:0.1819, Validation Loss:2.4633, Validation Accuracy:0.1675\n",
    "Epoch #152: Loss:2.4317, Accuracy:0.1811, Validation Loss:2.4607, Validation Accuracy:0.1839\n",
    "Epoch #153: Loss:2.4311, Accuracy:0.1745, Validation Loss:2.4687, Validation Accuracy:0.1708\n",
    "Epoch #154: Loss:2.4330, Accuracy:0.1770, Validation Loss:2.4714, Validation Accuracy:0.1757\n",
    "Epoch #155: Loss:2.4292, Accuracy:0.1877, Validation Loss:2.4653, Validation Accuracy:0.1691\n",
    "Epoch #156: Loss:2.4318, Accuracy:0.1852, Validation Loss:2.4699, Validation Accuracy:0.1691\n",
    "Epoch #157: Loss:2.4328, Accuracy:0.1832, Validation Loss:2.4695, Validation Accuracy:0.1708\n",
    "Epoch #158: Loss:2.4313, Accuracy:0.1807, Validation Loss:2.4737, Validation Accuracy:0.1724\n",
    "Epoch #159: Loss:2.4317, Accuracy:0.1815, Validation Loss:2.4656, Validation Accuracy:0.1757\n",
    "Epoch #160: Loss:2.4327, Accuracy:0.1828, Validation Loss:2.4736, Validation Accuracy:0.1790\n",
    "Epoch #161: Loss:2.4362, Accuracy:0.1819, Validation Loss:2.4679, Validation Accuracy:0.1773\n",
    "Epoch #162: Loss:2.4325, Accuracy:0.1840, Validation Loss:2.4686, Validation Accuracy:0.1708\n",
    "Epoch #163: Loss:2.4299, Accuracy:0.1860, Validation Loss:2.4767, Validation Accuracy:0.1658\n",
    "Epoch #164: Loss:2.4314, Accuracy:0.1832, Validation Loss:2.4708, Validation Accuracy:0.1691\n",
    "Epoch #165: Loss:2.4290, Accuracy:0.1860, Validation Loss:2.4659, Validation Accuracy:0.1757\n",
    "Epoch #166: Loss:2.4291, Accuracy:0.1864, Validation Loss:2.4653, Validation Accuracy:0.1708\n",
    "Epoch #167: Loss:2.4276, Accuracy:0.1856, Validation Loss:2.4633, Validation Accuracy:0.1675\n",
    "Epoch #168: Loss:2.4287, Accuracy:0.1832, Validation Loss:2.4631, Validation Accuracy:0.1675\n",
    "Epoch #169: Loss:2.4276, Accuracy:0.1840, Validation Loss:2.4647, Validation Accuracy:0.1642\n",
    "Epoch #170: Loss:2.4275, Accuracy:0.1869, Validation Loss:2.4618, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4251, Accuracy:0.1881, Validation Loss:2.4625, Validation Accuracy:0.1708\n",
    "Epoch #172: Loss:2.4246, Accuracy:0.1873, Validation Loss:2.4616, Validation Accuracy:0.1741\n",
    "Epoch #173: Loss:2.4244, Accuracy:0.1881, Validation Loss:2.4610, Validation Accuracy:0.1773\n",
    "Epoch #174: Loss:2.4243, Accuracy:0.1873, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #175: Loss:2.4235, Accuracy:0.1856, Validation Loss:2.4644, Validation Accuracy:0.1823\n",
    "Epoch #176: Loss:2.4239, Accuracy:0.1864, Validation Loss:2.4642, Validation Accuracy:0.1806\n",
    "Epoch #177: Loss:2.4247, Accuracy:0.1893, Validation Loss:2.4631, Validation Accuracy:0.1806\n",
    "Epoch #178: Loss:2.4239, Accuracy:0.1873, Validation Loss:2.4625, Validation Accuracy:0.1872\n",
    "Epoch #179: Loss:2.4228, Accuracy:0.1918, Validation Loss:2.4660, Validation Accuracy:0.1856\n",
    "Epoch #180: Loss:2.4229, Accuracy:0.1873, Validation Loss:2.4644, Validation Accuracy:0.1839\n",
    "Epoch #181: Loss:2.4240, Accuracy:0.1889, Validation Loss:2.4595, Validation Accuracy:0.1823\n",
    "Epoch #182: Loss:2.4243, Accuracy:0.1828, Validation Loss:2.4588, Validation Accuracy:0.1757\n",
    "Epoch #183: Loss:2.4236, Accuracy:0.1811, Validation Loss:2.4628, Validation Accuracy:0.1790\n",
    "Epoch #184: Loss:2.4227, Accuracy:0.1856, Validation Loss:2.4631, Validation Accuracy:0.1888\n",
    "Epoch #185: Loss:2.4233, Accuracy:0.1881, Validation Loss:2.4624, Validation Accuracy:0.1888\n",
    "Epoch #186: Loss:2.4229, Accuracy:0.1877, Validation Loss:2.4632, Validation Accuracy:0.1872\n",
    "Epoch #187: Loss:2.4230, Accuracy:0.1893, Validation Loss:2.4612, Validation Accuracy:0.1856\n",
    "Epoch #188: Loss:2.4241, Accuracy:0.1864, Validation Loss:2.4602, Validation Accuracy:0.1724\n",
    "Epoch #189: Loss:2.4262, Accuracy:0.1844, Validation Loss:2.4630, Validation Accuracy:0.1773\n",
    "Epoch #190: Loss:2.4252, Accuracy:0.1836, Validation Loss:2.4598, Validation Accuracy:0.1691\n",
    "Epoch #191: Loss:2.4240, Accuracy:0.1864, Validation Loss:2.4639, Validation Accuracy:0.1757\n",
    "Epoch #192: Loss:2.4241, Accuracy:0.1844, Validation Loss:2.4610, Validation Accuracy:0.1757\n",
    "Epoch #193: Loss:2.4236, Accuracy:0.1864, Validation Loss:2.4627, Validation Accuracy:0.1823\n",
    "Epoch #194: Loss:2.4256, Accuracy:0.1873, Validation Loss:2.4636, Validation Accuracy:0.1642\n",
    "Epoch #195: Loss:2.4231, Accuracy:0.1860, Validation Loss:2.4693, Validation Accuracy:0.1741\n",
    "Epoch #196: Loss:2.4235, Accuracy:0.1889, Validation Loss:2.4633, Validation Accuracy:0.1741\n",
    "Epoch #197: Loss:2.4242, Accuracy:0.1889, Validation Loss:2.4624, Validation Accuracy:0.1708\n",
    "Epoch #198: Loss:2.4229, Accuracy:0.1889, Validation Loss:2.4653, Validation Accuracy:0.1724\n",
    "Epoch #199: Loss:2.4229, Accuracy:0.1897, Validation Loss:2.4647, Validation Accuracy:0.1658\n",
    "Epoch #200: Loss:2.4228, Accuracy:0.1947, Validation Loss:2.4698, Validation Accuracy:0.1757\n",
    "Epoch #201: Loss:2.4221, Accuracy:0.1901, Validation Loss:2.4640, Validation Accuracy:0.1658\n",
    "Epoch #202: Loss:2.4245, Accuracy:0.1889, Validation Loss:2.4684, Validation Accuracy:0.1626\n",
    "Epoch #203: Loss:2.4224, Accuracy:0.1897, Validation Loss:2.4680, Validation Accuracy:0.1626\n",
    "Epoch #204: Loss:2.4240, Accuracy:0.1877, Validation Loss:2.4666, Validation Accuracy:0.1642\n",
    "Epoch #205: Loss:2.4226, Accuracy:0.1869, Validation Loss:2.4685, Validation Accuracy:0.1658\n",
    "Epoch #206: Loss:2.4213, Accuracy:0.1840, Validation Loss:2.4728, Validation Accuracy:0.1642\n",
    "Epoch #207: Loss:2.4275, Accuracy:0.1782, Validation Loss:2.4774, Validation Accuracy:0.1757\n",
    "Epoch #208: Loss:2.4243, Accuracy:0.1848, Validation Loss:2.4724, Validation Accuracy:0.1675\n",
    "Epoch #209: Loss:2.4233, Accuracy:0.1840, Validation Loss:2.4788, Validation Accuracy:0.1790\n",
    "Epoch #210: Loss:2.4242, Accuracy:0.1864, Validation Loss:2.4748, Validation Accuracy:0.1708\n",
    "Epoch #211: Loss:2.4242, Accuracy:0.1873, Validation Loss:2.4720, Validation Accuracy:0.1741\n",
    "Epoch #212: Loss:2.4233, Accuracy:0.1852, Validation Loss:2.4725, Validation Accuracy:0.1757\n",
    "Epoch #213: Loss:2.4238, Accuracy:0.1864, Validation Loss:2.4752, Validation Accuracy:0.1741\n",
    "Epoch #214: Loss:2.4225, Accuracy:0.1860, Validation Loss:2.4753, Validation Accuracy:0.1773\n",
    "Epoch #215: Loss:2.4229, Accuracy:0.1823, Validation Loss:2.4741, Validation Accuracy:0.1724\n",
    "Epoch #216: Loss:2.4221, Accuracy:0.1860, Validation Loss:2.4737, Validation Accuracy:0.1790\n",
    "Epoch #217: Loss:2.4221, Accuracy:0.1877, Validation Loss:2.4757, Validation Accuracy:0.1806\n",
    "Epoch #218: Loss:2.4217, Accuracy:0.1832, Validation Loss:2.4730, Validation Accuracy:0.1757\n",
    "Epoch #219: Loss:2.4211, Accuracy:0.1844, Validation Loss:2.4748, Validation Accuracy:0.1856\n",
    "Epoch #220: Loss:2.4214, Accuracy:0.1869, Validation Loss:2.4707, Validation Accuracy:0.1773\n",
    "Epoch #221: Loss:2.4220, Accuracy:0.1832, Validation Loss:2.4694, Validation Accuracy:0.1790\n",
    "Epoch #222: Loss:2.4224, Accuracy:0.1893, Validation Loss:2.4670, Validation Accuracy:0.1806\n",
    "Epoch #223: Loss:2.4225, Accuracy:0.1844, Validation Loss:2.4653, Validation Accuracy:0.1724\n",
    "Epoch #224: Loss:2.4202, Accuracy:0.1897, Validation Loss:2.4714, Validation Accuracy:0.1806\n",
    "Epoch #225: Loss:2.4200, Accuracy:0.1881, Validation Loss:2.4672, Validation Accuracy:0.1724\n",
    "Epoch #226: Loss:2.4197, Accuracy:0.1852, Validation Loss:2.4678, Validation Accuracy:0.1839\n",
    "Epoch #227: Loss:2.4186, Accuracy:0.1873, Validation Loss:2.4628, Validation Accuracy:0.1757\n",
    "Epoch #228: Loss:2.4187, Accuracy:0.1893, Validation Loss:2.4690, Validation Accuracy:0.1872\n",
    "Epoch #229: Loss:2.4196, Accuracy:0.1885, Validation Loss:2.4605, Validation Accuracy:0.1708\n",
    "Epoch #230: Loss:2.4233, Accuracy:0.1856, Validation Loss:2.4566, Validation Accuracy:0.1888\n",
    "Epoch #231: Loss:2.4229, Accuracy:0.1819, Validation Loss:2.4573, Validation Accuracy:0.1872\n",
    "Epoch #232: Loss:2.4203, Accuracy:0.1873, Validation Loss:2.4554, Validation Accuracy:0.1839\n",
    "Epoch #233: Loss:2.4205, Accuracy:0.1864, Validation Loss:2.4571, Validation Accuracy:0.1938\n",
    "Epoch #234: Loss:2.4178, Accuracy:0.1889, Validation Loss:2.4573, Validation Accuracy:0.1675\n",
    "Epoch #235: Loss:2.4184, Accuracy:0.1852, Validation Loss:2.4654, Validation Accuracy:0.1773\n",
    "Epoch #236: Loss:2.4169, Accuracy:0.1877, Validation Loss:2.4642, Validation Accuracy:0.1675\n",
    "Epoch #237: Loss:2.4183, Accuracy:0.1873, Validation Loss:2.4664, Validation Accuracy:0.1691\n",
    "Epoch #238: Loss:2.4184, Accuracy:0.1918, Validation Loss:2.4667, Validation Accuracy:0.1806\n",
    "Epoch #239: Loss:2.4178, Accuracy:0.1906, Validation Loss:2.4656, Validation Accuracy:0.1658\n",
    "Epoch #240: Loss:2.4164, Accuracy:0.1893, Validation Loss:2.4721, Validation Accuracy:0.1741\n",
    "Epoch #241: Loss:2.4165, Accuracy:0.1889, Validation Loss:2.4705, Validation Accuracy:0.1691\n",
    "Epoch #242: Loss:2.4167, Accuracy:0.1864, Validation Loss:2.4753, Validation Accuracy:0.1658\n",
    "Epoch #243: Loss:2.4171, Accuracy:0.1889, Validation Loss:2.4693, Validation Accuracy:0.1658\n",
    "Epoch #244: Loss:2.4186, Accuracy:0.1910, Validation Loss:2.4712, Validation Accuracy:0.1658\n",
    "Epoch #245: Loss:2.4187, Accuracy:0.1873, Validation Loss:2.4684, Validation Accuracy:0.1658\n",
    "Epoch #246: Loss:2.4191, Accuracy:0.1881, Validation Loss:2.4701, Validation Accuracy:0.1823\n",
    "Epoch #247: Loss:2.4183, Accuracy:0.1906, Validation Loss:2.4663, Validation Accuracy:0.1691\n",
    "Epoch #248: Loss:2.4179, Accuracy:0.1901, Validation Loss:2.4723, Validation Accuracy:0.1806\n",
    "Epoch #249: Loss:2.4174, Accuracy:0.1910, Validation Loss:2.4667, Validation Accuracy:0.1675\n",
    "Epoch #250: Loss:2.4156, Accuracy:0.1922, Validation Loss:2.4698, Validation Accuracy:0.1691\n",
    "Epoch #251: Loss:2.4145, Accuracy:0.1877, Validation Loss:2.4727, Validation Accuracy:0.1741\n",
    "Epoch #252: Loss:2.4146, Accuracy:0.1975, Validation Loss:2.4721, Validation Accuracy:0.1691\n",
    "Epoch #253: Loss:2.4145, Accuracy:0.1881, Validation Loss:2.4747, Validation Accuracy:0.1626\n",
    "Epoch #254: Loss:2.4155, Accuracy:0.1860, Validation Loss:2.4777, Validation Accuracy:0.1708\n",
    "Epoch #255: Loss:2.4166, Accuracy:0.1856, Validation Loss:2.4754, Validation Accuracy:0.1658\n",
    "Epoch #256: Loss:2.4162, Accuracy:0.1844, Validation Loss:2.4753, Validation Accuracy:0.1724\n",
    "Epoch #257: Loss:2.4171, Accuracy:0.1901, Validation Loss:2.4728, Validation Accuracy:0.1675\n",
    "Epoch #258: Loss:2.4160, Accuracy:0.1836, Validation Loss:2.4777, Validation Accuracy:0.1741\n",
    "Epoch #259: Loss:2.4170, Accuracy:0.1856, Validation Loss:2.4726, Validation Accuracy:0.1724\n",
    "Epoch #260: Loss:2.4154, Accuracy:0.1864, Validation Loss:2.4696, Validation Accuracy:0.1724\n",
    "Epoch #261: Loss:2.4156, Accuracy:0.1823, Validation Loss:2.4721, Validation Accuracy:0.1757\n",
    "Epoch #262: Loss:2.4164, Accuracy:0.1852, Validation Loss:2.4794, Validation Accuracy:0.1741\n",
    "Epoch #263: Loss:2.4152, Accuracy:0.1815, Validation Loss:2.4735, Validation Accuracy:0.1724\n",
    "Epoch #264: Loss:2.4166, Accuracy:0.1856, Validation Loss:2.4735, Validation Accuracy:0.1741\n",
    "Epoch #265: Loss:2.4174, Accuracy:0.1869, Validation Loss:2.4747, Validation Accuracy:0.1724\n",
    "Epoch #266: Loss:2.4174, Accuracy:0.1869, Validation Loss:2.4775, Validation Accuracy:0.1741\n",
    "Epoch #267: Loss:2.4187, Accuracy:0.1848, Validation Loss:2.4746, Validation Accuracy:0.1757\n",
    "Epoch #268: Loss:2.4189, Accuracy:0.1869, Validation Loss:2.4693, Validation Accuracy:0.1708\n",
    "Epoch #269: Loss:2.4203, Accuracy:0.1782, Validation Loss:2.4733, Validation Accuracy:0.1741\n",
    "Epoch #270: Loss:2.4186, Accuracy:0.1807, Validation Loss:2.4738, Validation Accuracy:0.1675\n",
    "Epoch #271: Loss:2.4186, Accuracy:0.1844, Validation Loss:2.4688, Validation Accuracy:0.1708\n",
    "Epoch #272: Loss:2.4189, Accuracy:0.1869, Validation Loss:2.4698, Validation Accuracy:0.1675\n",
    "Epoch #273: Loss:2.4196, Accuracy:0.1823, Validation Loss:2.4761, Validation Accuracy:0.1642\n",
    "Epoch #274: Loss:2.4192, Accuracy:0.1832, Validation Loss:2.4741, Validation Accuracy:0.1724\n",
    "Epoch #275: Loss:2.4195, Accuracy:0.1836, Validation Loss:2.4693, Validation Accuracy:0.1708\n",
    "Epoch #276: Loss:2.4176, Accuracy:0.1844, Validation Loss:2.4730, Validation Accuracy:0.1691\n",
    "Epoch #277: Loss:2.4165, Accuracy:0.1889, Validation Loss:2.4722, Validation Accuracy:0.1675\n",
    "Epoch #278: Loss:2.4169, Accuracy:0.1869, Validation Loss:2.4718, Validation Accuracy:0.1741\n",
    "Epoch #279: Loss:2.4157, Accuracy:0.1864, Validation Loss:2.4707, Validation Accuracy:0.1708\n",
    "Epoch #280: Loss:2.4147, Accuracy:0.1885, Validation Loss:2.4706, Validation Accuracy:0.1691\n",
    "Epoch #281: Loss:2.4171, Accuracy:0.1791, Validation Loss:2.4718, Validation Accuracy:0.1741\n",
    "Epoch #282: Loss:2.4186, Accuracy:0.1848, Validation Loss:2.4689, Validation Accuracy:0.1708\n",
    "Epoch #283: Loss:2.4168, Accuracy:0.1869, Validation Loss:2.4725, Validation Accuracy:0.1691\n",
    "Epoch #284: Loss:2.4169, Accuracy:0.1873, Validation Loss:2.4741, Validation Accuracy:0.1724\n",
    "Epoch #285: Loss:2.4161, Accuracy:0.1889, Validation Loss:2.4712, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.4160, Accuracy:0.1836, Validation Loss:2.4741, Validation Accuracy:0.1691\n",
    "Epoch #287: Loss:2.4159, Accuracy:0.1848, Validation Loss:2.4754, Validation Accuracy:0.1691\n",
    "Epoch #288: Loss:2.4156, Accuracy:0.1836, Validation Loss:2.4717, Validation Accuracy:0.1708\n",
    "Epoch #289: Loss:2.4142, Accuracy:0.1832, Validation Loss:2.4749, Validation Accuracy:0.1724\n",
    "Epoch #290: Loss:2.4139, Accuracy:0.1836, Validation Loss:2.4747, Validation Accuracy:0.1757\n",
    "Epoch #291: Loss:2.4134, Accuracy:0.1860, Validation Loss:2.4738, Validation Accuracy:0.1757\n",
    "Epoch #292: Loss:2.4133, Accuracy:0.1840, Validation Loss:2.4730, Validation Accuracy:0.1708\n",
    "Epoch #293: Loss:2.4161, Accuracy:0.1823, Validation Loss:2.4775, Validation Accuracy:0.1790\n",
    "Epoch #294: Loss:2.4139, Accuracy:0.1852, Validation Loss:2.4733, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4156, Accuracy:0.1860, Validation Loss:2.4739, Validation Accuracy:0.1757\n",
    "Epoch #296: Loss:2.4178, Accuracy:0.1869, Validation Loss:2.4678, Validation Accuracy:0.1691\n",
    "Epoch #297: Loss:2.4183, Accuracy:0.1840, Validation Loss:2.4704, Validation Accuracy:0.1708\n",
    "Epoch #298: Loss:2.4169, Accuracy:0.1844, Validation Loss:2.4772, Validation Accuracy:0.1806\n",
    "Epoch #299: Loss:2.4178, Accuracy:0.1840, Validation Loss:2.4716, Validation Accuracy:0.1741\n",
    "Epoch #300: Loss:2.4152, Accuracy:0.1848, Validation Loss:2.4737, Validation Accuracy:0.1724\n",
    "\n",
    "Test:\n",
    "Test Loss:2.47365642, Accuracy:0.1724\n",
    "Labels: ['by', 'eg', 'sg', 'ds', 'ek', 'yd', 'eb', 'sk', 'my', 'eo', 'aa', 'ib', 'mb', 'ck', 'ce']\n",
    "Confusion Matrix:\n",
    "      by  eg  sg  ds  ek  yd  eb  sk  my  eo  aa  ib  mb  ck  ce\n",
    "t:by   3  14  11   2   3   5   0   0   0   0   0   1   1   0   0\n",
    "t:eg   0  36   6   6   0   2   0   0   0   0   0   0   0   0   0\n",
    "t:sg   1   6  19   0   3  18   0   0   0   0   0   3   1   0   0\n",
    "t:ds   0  15   8   6   0   2   0   0   0   0   0   0   0   0   0\n",
    "t:ek   2  16  12   2   5   9   0   0   0   0   0   2   0   0   0\n",
    "t:yd   0   3  14   0   4  33   1   0   0   0   0   6   1   0   0\n",
    "t:eb   1  24  10   2   3   7   0   0   0   0   0   1   2   0   0\n",
    "t:sk   1  20   4   0   3   3   1   0   0   0   0   1   0   0   0\n",
    "t:my   0   9   4   0   1   5   0   0   0   0   0   0   1   0   0\n",
    "t:eo   2   4  13   0   5   4   0   0   0   0   0   3   3   0   0\n",
    "t:aa   1  17   3   7   0   4   2   0   0   0   0   0   0   0   0\n",
    "t:ib   0   8  12   0   2  29   0   0   0   0   0   3   0   0   0\n",
    "t:mb   0  14  16   3   2  17   0   0   0   0   0   0   0   0   0\n",
    "t:ck   0   9   7   4   0   1   1   0   0   0   0   0   1   0   0\n",
    "t:ce   2  10   5   1   1   8   0   0   0   0   0   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          by       0.23      0.07      0.11        40\n",
    "          eg       0.18      0.72      0.28        50\n",
    "          sg       0.13      0.37      0.19        51\n",
    "          ds       0.18      0.19      0.19        31\n",
    "          ek       0.16      0.10      0.12        48\n",
    "          yd       0.22      0.53      0.32        62\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ib       0.15      0.06      0.08        54\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ce       0.00      0.00      0.00        27\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.08      0.14      0.09       609\n",
    "weighted avg       0.10      0.17      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 19:37:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 41 minutes, 3 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.70581685655027, 2.698823162291829, 2.693111488971804, 2.686692607813868, 2.682131104085637, 2.677359426354344, 2.6705049787248885, 2.6636779672406576, 2.6560319115962887, 2.6463530701760978, 2.6357974822102315, 2.623436729309007, 2.606801265174728, 2.593696488926955, 2.579013589567739, 2.554938096718248, 2.546458391524692, 2.5331479324691597, 2.52066764925501, 2.522767985787102, 2.5134859785853543, 2.5151695993733525, 2.502989838276003, 2.5036370738582265, 2.49672260033869, 2.4951171170314543, 2.491876044296866, 2.4896822668648704, 2.4889981652715525, 2.489923021084765, 2.4883556080177693, 2.4851792387187186, 2.4864643955074115, 2.4820237504241893, 2.4812060240258527, 2.481764095757395, 2.48203981646959, 2.481533886372358, 2.4935672643345175, 2.476203928635821, 2.479377243906406, 2.4751667256034264, 2.4745477725719582, 2.481726678525677, 2.4784587326112444, 2.4734927664445148, 2.4708048134601763, 2.4712939849628017, 2.4672222325367295, 2.4715394398261763, 2.468695443252037, 2.474733206634647, 2.4725698434269097, 2.470958729291393, 2.47081541466987, 2.471005442890236, 2.4708575787411142, 2.4705693111043847, 2.4720468521118164, 2.471003653380671, 2.4713974989497993, 2.4675977096964767, 2.4691143583977357, 2.4695931929477135, 2.468857604686067, 2.468813648364814, 2.4686814043517966, 2.4695204631448378, 2.466987849456336, 2.468724143133179, 2.4692369125942486, 2.4712122868630293, 2.468456707564481, 2.4699807785610455, 2.4671060259902027, 2.4637912936594293, 2.468011000669257, 2.4635760474870554, 2.468154555275327, 2.464726812929551, 2.4665426002151665, 2.466182875907284, 2.46722473495308, 2.466297179803081, 2.4654581993084235, 2.4656037031527616, 2.46647366749242, 2.4818045912900777, 2.467569179331336, 2.4752792920580835, 2.472071544290176, 2.472397559969296, 2.4628173314487602, 2.467419323271327, 2.465059860195041, 2.463714462587203, 2.4653463195305934, 2.4633953352084106, 2.465688655724862, 2.468156396070333, 2.476147862295016, 2.5112746118128983, 2.46839582313262, 2.4674983917198743, 2.47073351103684, 2.465431566504618, 2.4750956553347985, 2.4610222004513043, 2.4637674811634134, 2.470646610009455, 2.4628344188965796, 2.462429073643802, 2.4637104079053906, 2.4625250944754566, 2.4648162774460265, 2.4670624717311513, 2.4634729292983883, 2.4665163929630776, 2.464142759053774, 2.468013531273026, 2.4638840451420627, 2.467807943597803, 2.4651067895059318, 2.460333803800135, 2.459357242474611, 2.4586526826880446, 2.456211025100232, 2.462143229733547, 2.461930729094006, 2.460706125143518, 2.4632776703544828, 2.463470888451011, 2.469672728054629, 2.466222643264996, 2.466476992041802, 2.4658326534997848, 2.465725722962804, 2.463388878723671, 2.465485531316798, 2.4584510114979863, 2.4633217202423046, 2.46201860885119, 2.461337354187112, 2.465968018877878, 2.4601295456314713, 2.4600549116118984, 2.465023862121532, 2.464923294502722, 2.4626253704327863, 2.467497907835862, 2.463258634842871, 2.4607303310889135, 2.4687481838689846, 2.4713551379581196, 2.4653299500789547, 2.469939547806538, 2.469512091090135, 2.473746268032807, 2.4655917141042125, 2.473550213185829, 2.4678786638726544, 2.4686451052210012, 2.4766533687979915, 2.470803727070099, 2.4659211713887985, 2.4653021463228173, 2.4633013822370757, 2.4631239577075728, 2.4646919039865627, 2.461815854794482, 2.462526472135522, 2.4615554915273132, 2.4609886836535826, 2.4614875351854146, 2.464376557245239, 2.4642145711995895, 2.463105089754502, 2.46245391028268, 2.4659716159056364, 2.4643937942429717, 2.4594684829461357, 2.458803140862626, 2.4628287097699144, 2.4631406376123035, 2.4624225130222115, 2.463228321232036, 2.4612440095942203, 2.4602222771480164, 2.46300346119259, 2.4598121263319244, 2.463923996109485, 2.4610099271796217, 2.462655703226725, 2.4635738038468635, 2.4692504081037048, 2.4632905942857364, 2.462447923196752, 2.46531531689398, 2.4646501834756633, 2.469765528473752, 2.463970334659069, 2.4683908167339506, 2.467982516891655, 2.466568551823032, 2.4684944693090882, 2.4727545724126507, 2.4773674144337723, 2.472404786518642, 2.4787615700112577, 2.474793465070928, 2.4720449846953594, 2.4725312227490304, 2.4752173302404596, 2.475250596483353, 2.474074871865008, 2.4736905015748123, 2.47565540734966, 2.4729632430867414, 2.474784980658044, 2.4706531541883847, 2.4693731091096875, 2.466954010460764, 2.4652672377713207, 2.471428370045128, 2.46723634073104, 2.4678076758173297, 2.4627848446662792, 2.4690260648335927, 2.46050836143431, 2.4565671950529753, 2.457326971251389, 2.4553525757124075, 2.4571073532887477, 2.457286351224276, 2.465428908274483, 2.4641522472519397, 2.4664433237367076, 2.4666690963438187, 2.465613643132603, 2.472141250209464, 2.470466312712245, 2.4752519537858384, 2.4692505509982556, 2.471189240908192, 2.4684074054210643, 2.4701376575946026, 2.466335473976699, 2.4722958640707735, 2.466731718999803, 2.469775184621952, 2.4726521467731897, 2.472084133495838, 2.4747491755900515, 2.4776977729327574, 2.4754101337470447, 2.4753244702256176, 2.4727787767920786, 2.4776849288658553, 2.472562127904156, 2.4695680117959458, 2.472095899785485, 2.4794284396962385, 2.4735294926929945, 2.473525663510528, 2.474677622220395, 2.47751828171741, 2.474635289807625, 2.4693369830183207, 2.473296952365067, 2.473797863144397, 2.468847433334501, 2.4698414923913763, 2.4760745346839794, 2.474107221233825, 2.4692737989629236, 2.473022443320364, 2.4722360127860887, 2.471791197317966, 2.470666128230604, 2.4706454547168, 2.471848599429201, 2.4689158575092436, 2.472497685202237, 2.4740847221931994, 2.471172079076908, 2.4740930347411307, 2.475387444049854, 2.471701452884768, 2.4749207841155956, 2.4747015294574557, 2.4737590009160035, 2.4729638385459514, 2.477525964354842, 2.4733032048825168, 2.4738703674479265, 2.4677840410586454, 2.4703665699669095, 2.4772344410713085, 2.471576139844697, 2.4736564269011048], 'val_acc': [0.08210180623362022, 0.08702791430826845, 0.08374384205867896, 0.08045976971121649, 0.09852216708395868, 0.09852216708395868, 0.11494252773243414, 0.10509031197462959, 0.12807881722015702, 0.14942528703823466, 0.1510673232609024, 0.15763546746646243, 0.15763546746646243, 0.15927750368913016, 0.15927750368913016, 0.16584564818830913, 0.1625615761344656, 0.16748768450884982, 0.16256157574297367, 0.16912972073151755, 0.16584564818830913, 0.17405582910590178, 0.16420361225926033, 0.17241379298110704, 0.16584564838405508, 0.1707717568563123, 0.17241379298110704, 0.1674876846067228, 0.1707717568563123, 0.17241379298110704, 0.17405582910590178, 0.17077175675843934, 0.17241379298110704, 0.17077175675843934, 0.17241379288323408, 0.16748768450884982, 0.16748768441097686, 0.16748768450884982, 0.18226600972987553, 0.16748768441097686, 0.17898193738241305, 0.17077175675843934, 0.17405582900802882, 0.17405582900802882, 0.1691297205357716, 0.17077175666056635, 0.1691297206336446, 0.17405582910590178, 0.1707717568563123, 0.17569786523069655, 0.16912972073151755, 0.17569786523069655, 0.17077175666056635, 0.1724137927853611, 0.16748768441097686, 0.16748768441097686, 0.16748768441097686, 0.16748768441097686, 0.1691297205357716, 0.16584564828618212, 0.17241379298110704, 0.1691297205357716, 0.16748768441097686, 0.17241379288323408, 0.1691297205357716, 0.1691297205357716, 0.16584564828618212, 0.17077175666056635, 0.16584564828618212, 0.17077175666056635, 0.1691297205357716, 0.17077175666056635, 0.16748768441097686, 0.17077175666056635, 0.16748768441097686, 0.17405582910590178, 0.16912972073151755, 0.17569786523069655, 0.16912972073151755, 0.17077175666056635, 0.1724137927853611, 0.17241379298110704, 0.17405582910590178, 0.1707717568563123, 0.17241379307898003, 0.17077175695418528, 0.16912972082939054, 0.17898193530261222, 0.17733990145336426, 0.1822660076500747, 0.17077175675843934, 0.17733989917781748, 0.17405582920377477, 0.1773399013554913, 0.1707717568563123, 0.17405582910590178, 0.17569786523069655, 0.17077175666056635, 0.17241379298110704, 0.18226600982774852, 0.16420361225926033, 0.18390804595254326, 0.17241379298110704, 0.1822660075522017, 0.17241379298110704, 0.1625615761344656, 0.1789819354004852, 0.16584564838405508, 0.17405582910590178, 0.1822660076500747, 0.17405582920377477, 0.17241379307898003, 0.1822660076500747, 0.17241379298110704, 0.17241379307898003, 0.17405582920377477, 0.17898193530261222, 0.18390804367699645, 0.18555007989966418, 0.18555007980179122, 0.18062397142740697, 0.17733990145336426, 0.1756978653285695, 0.17405582920377477, 0.17405582910590178, 0.1773399013554913, 0.17569786523069655, 0.16912972082939054, 0.16912972073151755, 0.1674876846067228, 0.17733990145336426, 0.16912972073151755, 0.17405582910590178, 0.1707717568563123, 0.17241379298110704, 0.16912972073151755, 0.1691297205357716, 0.1691297205357716, 0.16584564848192807, 0.17077175695418528, 0.16748768450884982, 0.1691297206336446, 0.16748768441097686, 0.1707717568563123, 0.17569786523069655, 0.17405582910590178, 0.1691297206336446, 0.1674876846067228, 0.17241379288323408, 0.17241379298110704, 0.16748768450884982, 0.18390804377486944, 0.17077175675843934, 0.1756978653285695, 0.16912972073151755, 0.16912972082939054, 0.17077175695418528, 0.17241379307898003, 0.1756978653285695, 0.17898193748028604, 0.1773399013554913, 0.17077175695418528, 0.16584564848192807, 0.16912972073151755, 0.17569786523069655, 0.1707717568563123, 0.1674876846067228, 0.1674876846067228, 0.1642036124550063, 0.1707717568563123, 0.17077175675843934, 0.17405582900802882, 0.17733990115974532, 0.1724137927853611, 0.18226600963200254, 0.1806239735072078, 0.1806239735072078, 0.18719211592658597, 0.18555008197946501, 0.18390804585467027, 0.18226600982774852, 0.17569786523069655, 0.178981937578159, 0.1888341520513807, 0.1888341520513807, 0.18719211592658597, 0.18555007980179122, 0.17241379080343325, 0.17733990145336426, 0.16912972082939054, 0.1756978653285695, 0.17569786513282357, 0.18226600982774852, 0.16420361225926033, 0.17405582920377477, 0.174055826928228, 0.1707717546786385, 0.17241379080343325, 0.16584564848192807, 0.17569786305302273, 0.16584564848192807, 0.16256157623233858, 0.16256157633021157, 0.1642036124550063, 0.16584564857980105, 0.16420361235713332, 0.17569786315089572, 0.1674876847045958, 0.1789819354004852, 0.17077175695418528, 0.17405582920377477, 0.1756978653285695, 0.17405582920377477, 0.17733990145336426, 0.17241379307898003, 0.178981937578159, 0.18062397360508078, 0.1756978653285695, 0.18555007980179122, 0.1773399013554913, 0.17898193748028604, 0.18062397360508078, 0.17241379298110704, 0.18062397360508078, 0.17241379288323408, 0.18390804575679728, 0.17569786513282357, 0.18719211820213275, 0.17077175675843934, 0.1888341543269275, 0.18719211602445893, 0.18390804575679728, 0.19376026042576494, 0.1674876846067228, 0.1773399013554913, 0.1674876846067228, 0.16912972073151755, 0.18062397360508078, 0.16584564838405508, 0.17405582900802882, 0.16912972073151755, 0.16584564838405508, 0.16584564828618212, 0.16584564838405508, 0.16584564838405508, 0.18226600953412955, 0.16912972073151755, 0.1806239734093348, 0.1674876846067228, 0.1691297205357716, 0.17405582891015584, 0.1691297206336446, 0.16256157623233858, 0.17077175695418528, 0.16584564857980105, 0.17241379307898003, 0.1674876847045958, 0.17405582910590178, 0.17241379298110704, 0.17241379307898003, 0.17569786513282357, 0.17405582900802882, 0.17241379307898003, 0.17405582910590178, 0.17241379307898003, 0.17405582910590178, 0.17569786523069655, 0.17077175695418528, 0.17405582910590178, 0.1674876847045958, 0.1707717568563123, 0.1674876847045958, 0.1642036124550063, 0.17241379298110704, 0.1707717568563123, 0.16912972082939054, 0.1674876847045958, 0.17405582910590178, 0.17077175695418528, 0.16912972082939054, 0.17405582910590178, 0.17077175695418528, 0.16912972082939054, 0.17241379298110704, 0.16912972082939054, 0.16912972073151755, 0.16912972082939054, 0.17077175695418528, 0.17241379288323408, 0.17569786513282357, 0.17569786513282357, 0.17077175675843934, 0.178981937578159, 0.1691297206336446, 0.17569786523069655, 0.1691297206336446, 0.17077175675843934, 0.18062397142740697, 0.17405582900802882, 0.17241379298110704], 'loss': [2.7127837391604634, 2.7022858435613175, 2.6963546019560014, 2.6904223864083416, 2.684171907515007, 2.6796057946872907, 2.674563056487567, 2.6684868200603695, 2.6614584044509355, 2.6524756709653006, 2.641774941078202, 2.6296594712768493, 2.6155397904727002, 2.6027154659098914, 2.5833564878733983, 2.559806214612612, 2.5403774084251762, 2.5314695407232954, 2.5186302442570243, 2.5102529377907943, 2.502030598211582, 2.4967076928219023, 2.494144323670154, 2.48549563556726, 2.4803472366176345, 2.477267909980163, 2.4739533111055283, 2.471651816025407, 2.467758530219233, 2.4661934661669407, 2.463698228624567, 2.462673194893087, 2.46157327648061, 2.4621139587073357, 2.459769296548205, 2.4586648227253, 2.45696335142398, 2.4586631989332197, 2.46168186454068, 2.461990908969354, 2.456538000635543, 2.456114226053383, 2.453470672082607, 2.4528874365945614, 2.4523121274717044, 2.4516105801662627, 2.4515949515591413, 2.452151794355263, 2.4464771665342044, 2.4484094253555706, 2.446194290674198, 2.4458813065131344, 2.448695570634376, 2.450392168797017, 2.44833501682634, 2.4474111095835784, 2.4477435214808345, 2.447032631691966, 2.447981471741224, 2.446697700097086, 2.4468977212416316, 2.4441234189137298, 2.4439593127131216, 2.4445114858341412, 2.442992328863124, 2.4440918803949376, 2.443374522559697, 2.4423220127270207, 2.4419425989812895, 2.440223305719834, 2.440142181913466, 2.4403474571768506, 2.439278551685247, 2.439089361206462, 2.4403853571145686, 2.441290881354706, 2.437645397049201, 2.4361009655547092, 2.4349815555666505, 2.4350768012922157, 2.436030009541913, 2.435065720947861, 2.435205904610103, 2.4337730414568766, 2.4329965929231117, 2.432356441486053, 2.433547586386209, 2.4378266986635433, 2.4404788335490766, 2.4375346283648294, 2.438519724389611, 2.439068299788959, 2.4350484192983326, 2.4333090436532023, 2.434614747750441, 2.4325280681038293, 2.431536517838433, 2.4312639597749808, 2.432168121161647, 2.4330287500573378, 2.4340526719847255, 2.456434816795208, 2.4477713525172864, 2.4436962764121177, 2.4389987295413165, 2.43350447453023, 2.4309790709180263, 2.43084076415342, 2.4300164048431836, 2.4285504069905994, 2.4288811384040474, 2.428920851889577, 2.426735422106984, 2.4271162101620276, 2.427372485656268, 2.428619356762457, 2.426630443126514, 2.4291990222382593, 2.4297992984372243, 2.4300533941883815, 2.428719143015648, 2.4286696410521835, 2.427260520766648, 2.4258243026185085, 2.424786353747703, 2.4263619869396673, 2.425371486793064, 2.4241466750354492, 2.4249774786970697, 2.425608382332741, 2.4255910328036707, 2.422445364017996, 2.423900173136341, 2.423121680175499, 2.424531009309835, 2.4239542172919553, 2.4226602271352218, 2.4258203707191734, 2.42506435705651, 2.4232981467393877, 2.424854737189761, 2.427016428608669, 2.4279517547795417, 2.4266006347335094, 2.427336245151026, 2.4372693348714214, 2.4432295772818815, 2.441768634784393, 2.4370568503589354, 2.4384575580424594, 2.4388261121401307, 2.431725196133404, 2.4311292740353814, 2.432994355900821, 2.4292407852423508, 2.4317849391294946, 2.432764265718401, 2.4312907533234394, 2.43165176379852, 2.4326801397962003, 2.4362069869188314, 2.432504304184806, 2.4298807608273485, 2.4313759416035805, 2.42904682257337, 2.4290662463930355, 2.4276429733456526, 2.4287147116612116, 2.4276499695356866, 2.427542512568605, 2.4250958874485087, 2.424629997766483, 2.4243888897083132, 2.4243173398521156, 2.423526547625814, 2.423903273459088, 2.424669023216138, 2.423897911537844, 2.4228397738517433, 2.4228882474331397, 2.423977480385093, 2.4242883012524867, 2.4235851967359228, 2.4227136145871766, 2.423339485681522, 2.422871558426342, 2.422960597631623, 2.4241000243036166, 2.4261856205409558, 2.425216733454679, 2.423955115547415, 2.4241485521289112, 2.4236493176258564, 2.425570163393902, 2.4231133563807368, 2.4234877764566725, 2.4241553157751565, 2.422937916044827, 2.422916501454504, 2.4228452136139604, 2.4220722805548007, 2.424522915023553, 2.422354646972562, 2.4240308028227004, 2.4226255343435237, 2.4213006943892648, 2.427514138016123, 2.4243347890567977, 2.4232593420839406, 2.4242296357908777, 2.4241635050372174, 2.4232519725509736, 2.4238117668418178, 2.4224984463725003, 2.422861178944488, 2.422083813649673, 2.4221258117432956, 2.421716472206664, 2.421149331098709, 2.42139815491083, 2.4220232801026143, 2.4223805667438545, 2.4225355151252823, 2.420223369050075, 2.420024571232727, 2.419668962333726, 2.41862397820553, 2.4186759879212114, 2.4196012188521743, 2.4232978694492786, 2.422875080950696, 2.4203116322934504, 2.420456510696568, 2.417777865231649, 2.4184137155387924, 2.4169398065954755, 2.4182826888145117, 2.418388455800207, 2.417774867717735, 2.41640090394069, 2.416511408455318, 2.4166741847012814, 2.417089733989332, 2.418566475756604, 2.4186815875756422, 2.4191108358958906, 2.4182562980808515, 2.417876664324218, 2.4173559496290142, 2.415560019628223, 2.4145160267730024, 2.414626582887873, 2.414500803036856, 2.4155265737607983, 2.4165649051783755, 2.4161912247385575, 2.4171436693634094, 2.41597565302369, 2.4169737873625707, 2.415387174285168, 2.415554420757098, 2.4163926286129493, 2.4152188097427025, 2.4165550908513627, 2.417392981664356, 2.4174099516819636, 2.4186946712235406, 2.418891082015615, 2.4203371549289083, 2.418594316094808, 2.4186155425939226, 2.4189200729315288, 2.4196158428701287, 2.41920037044392, 2.4195343712761663, 2.417634431047851, 2.4164955957714294, 2.416895438023906, 2.4156984298870547, 2.414713949246573, 2.4170803866102464, 2.418644934758024, 2.416757831925974, 2.4169485198888445, 2.416145526995649, 2.416028667622278, 2.4159387052915915, 2.415559924064965, 2.4142089213189157, 2.4138868245745586, 2.4134436380202278, 2.413340802163314, 2.4160801773932925, 2.4139491996725972, 2.415585484396995, 2.4178247008235547, 2.4182975513489584, 2.416855313009305, 2.4177702406348636, 2.41519165332802], 'acc': [0.0431211494597811, 0.08172484653747547, 0.08295687939168492, 0.08254620079448335, 0.08829568742910204, 0.11129363442348504, 0.1071868582611456, 0.11786447629424336, 0.12484599621144163, 0.13182751432948533, 0.14866529823695854, 0.15112936358126283, 0.1474332638161383, 0.15154004197345866, 0.16303901369444398, 0.1601642709554343, 0.15934291689425278, 0.16591375700257396, 0.16098562697487934, 0.1696098555652023, 0.17166324543634723, 0.16303901545688113, 0.17659137587405327, 0.17494866518758895, 0.1761806992351152, 0.17412730895395886, 0.17535934358896416, 0.17535934380314924, 0.177002052690459, 0.17618069825598345, 0.17741273009434372, 0.17659137605152092, 0.17618069884346252, 0.17494866477757753, 0.17618069763178698, 0.1745379885670096, 0.17535934380314924, 0.17618069886182122, 0.17248460067248686, 0.17618069902093014, 0.17700205249463263, 0.17823408714799666, 0.1798767968002531, 0.17905544038915536, 0.17741273050435516, 0.17782340970739446, 0.17659137624734725, 0.17577002140286033, 0.17741273148348688, 0.17782340890573037, 0.17700205386541706, 0.17741273148348688, 0.1774127307001815, 0.17741273130601926, 0.17782340872826272, 0.17823408691545287, 0.17659137626570598, 0.17741273208932465, 0.1798767977793848, 0.1778234075349459, 0.17741273208932465, 0.17741273208932465, 0.1786447645518814, 0.17782340952992684, 0.17946611798886647, 0.17987679599858897, 0.17823408732546428, 0.17905544078080807, 0.1774127299168761, 0.177823408709904, 0.17864476435605506, 0.18069815203639272, 0.17864476396440235, 0.17864476396440235, 0.17987679699607942, 0.17330595395036302, 0.18028747381248514, 0.1790554417599398, 0.17864476275272684, 0.17987679762027592, 0.17905544195576614, 0.18275154051839448, 0.18069815107561968, 0.1815195062934006, 0.18480492741542676, 0.18809034837834399, 0.18398357299930995, 0.18110882945863618, 0.18357289516705508, 0.18110882928116856, 0.17905544117246075, 0.17494866636254705, 0.18480492723795913, 0.1848049286271023, 0.18726899314220435, 0.1876796713293945, 0.186858316894919, 0.18521560644099844, 0.18767967195359098, 0.18357289538124014, 0.18439424960153059, 0.1671457908542739, 0.18850102578222874, 0.18193018487224344, 0.18603696009216858, 0.18398357221600456, 0.18439425100903248, 0.18562628245574003, 0.18644763966850186, 0.18644763909938153, 0.18685831568324346, 0.18275154112423225, 0.18644763792442345, 0.18726899294637803, 0.18644763751441204, 0.18685831589742852, 0.1926078035478964, 0.189733059829755, 0.18439424940570423, 0.18603696046546256, 0.1848049280029058, 0.1901437370561721, 0.1848049284129172, 0.18644763888519647, 0.18767967095610052, 0.1868583161116136, 0.1893223814283798, 0.18973306022140768, 0.18603696205043205, 0.18891170381030997, 0.18562628247409876, 0.18809034976748715, 0.18767967154357956, 0.19096509268396444, 0.1856262826699251, 0.18398357180599315, 0.18439425058066233, 0.18398357178763441, 0.18726899294637803, 0.189733059829755, 0.18726899472717387, 0.1848049284129172, 0.18028747539745463, 0.18521560563933434, 0.18603696007380985, 0.18069815185892507, 0.17946611978802104, 0.17946611861306294, 0.17823408554466844, 0.17207392107779484, 0.18193018487224344, 0.18110882906698347, 0.17453798838954196, 0.17700205349212308, 0.1876796715619383, 0.18521560503349657, 0.1831622185464757, 0.18069815086143462, 0.18151950666669459, 0.18275154112423225, 0.18193018469477582, 0.1839835731951363, 0.18603696069800632, 0.1831622173531589, 0.1860369610529416, 0.1864476385119025, 0.18562628247409876, 0.1831622173531589, 0.1839835739784417, 0.18685831648490758, 0.18809034796833257, 0.18726899433552116, 0.18809034994495477, 0.18726899394386848, 0.18562628286575145, 0.18644763790606472, 0.18932238121419473, 0.18726899431716246, 0.19178644774263645, 0.18726899470881514, 0.18891170498526808, 0.1827515397350891, 0.181108830651953, 0.18562628404070955, 0.18809034818251766, 0.18767967115192688, 0.18932238082254202, 0.1864476381202498, 0.18439425059902104, 0.18357289538124014, 0.1864476374960533, 0.184394250011542, 0.1864476394726755, 0.18726899353385706, 0.1860369618546057, 0.18891170459361536, 0.18891170420196268, 0.18891170422032139, 0.18973305922391723, 0.19466119163824547, 0.19014373725199846, 0.18891170400613633, 0.18973305844061183, 0.18767967173940592, 0.1868583169132777, 0.18398357276676616, 0.17823408613214747, 0.18480492743378546, 0.18398357317677758, 0.18644763829771743, 0.18726899449463008, 0.18521560683265115, 0.18644763868937012, 0.1860369610529416, 0.18234086272285704, 0.18603696108965903, 0.1876796705460891, 0.18316221774481162, 0.1843942499931833, 0.18685831552413454, 0.18316221676567987, 0.18932238085925945, 0.18439425081320612, 0.189733059829755, 0.18809035015913986, 0.18521560583516067, 0.18726899433552116, 0.1893223818383912, 0.18850102658389287, 0.18562628386324193, 0.1819301838931117, 0.18726899431716246, 0.1864476377102384, 0.18891170457525663, 0.18521560563933434, 0.18767967056444784, 0.18726899410297737, 0.19178644792010408, 0.19055441526172098, 0.18932238281752295, 0.1889117043794303, 0.18644763868937012, 0.18891170478944172, 0.1909650928797908, 0.1872689945129888, 0.1880903499633135, 0.19055441487006827, 0.19014373725199846, 0.19096509307561713, 0.19219712473650977, 0.1876796713293945, 0.1975359337897761, 0.1880903499633135, 0.18603696126712665, 0.1856262830615778, 0.1843942512048588, 0.19014373723363975, 0.18357289555870776, 0.1856262838448832, 0.1864476394543168, 0.1823408627044983, 0.18521560563933434, 0.1815195062934006, 0.18562628286575145, 0.18685831632579866, 0.18685831630743993, 0.18480492803962323, 0.1868583173049304, 0.1782340865421589, 0.1806981526422305, 0.18439425022572709, 0.18685831552413454, 0.1823408617253666, 0.18316221656985351, 0.1835728947937611, 0.18439425020736835, 0.18891170400613633, 0.1868583153283082, 0.18644763769187966, 0.1885010263880665, 0.17905544177829852, 0.18480492782543817, 0.18685831709074532, 0.18726899394386848, 0.18891170537692076, 0.18357289616454553, 0.18480492764797055, 0.18357289636037188, 0.18316221656985351, 0.18357289516705508, 0.18603696087547397, 0.18398357219764583, 0.18234086290032467, 0.18521560483767022, 0.1860369598779835, 0.18685831589742852, 0.18398357298095122, 0.1843942512048588, 0.18398357339096266, 0.18480492900039625]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
