{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf50.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 21:41:19 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '1Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '01', '03', '02', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002520254CE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000025233DD6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6109, Accuracy:0.1930, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6061, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6007, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6001, Accuracy:0.2329, Validation Loss:1.5977, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.5966, Accuracy:0.2353, Validation Loss:1.5930, Validation Accuracy:0.2414\n",
    "Epoch #14: Loss:1.5910, Accuracy:0.2468, Validation Loss:1.5853, Validation Accuracy:0.2562\n",
    "Epoch #15: Loss:1.5820, Accuracy:0.2608, Validation Loss:1.5730, Validation Accuracy:0.2742\n",
    "Epoch #16: Loss:1.5683, Accuracy:0.2752, Validation Loss:1.5549, Validation Accuracy:0.2939\n",
    "Epoch #17: Loss:1.5495, Accuracy:0.3014, Validation Loss:1.5328, Validation Accuracy:0.3005\n",
    "Epoch #18: Loss:1.5247, Accuracy:0.3109, Validation Loss:1.5088, Validation Accuracy:0.3169\n",
    "Epoch #19: Loss:1.5010, Accuracy:0.3158, Validation Loss:1.4903, Validation Accuracy:0.3218\n",
    "Epoch #20: Loss:1.4855, Accuracy:0.3216, Validation Loss:1.4844, Validation Accuracy:0.3120\n",
    "Epoch #21: Loss:1.4874, Accuracy:0.3216, Validation Loss:1.4880, Validation Accuracy:0.3284\n",
    "Epoch #22: Loss:1.4801, Accuracy:0.3265, Validation Loss:1.4775, Validation Accuracy:0.3202\n",
    "Epoch #23: Loss:1.4733, Accuracy:0.3331, Validation Loss:1.4763, Validation Accuracy:0.3284\n",
    "Epoch #24: Loss:1.4707, Accuracy:0.3363, Validation Loss:1.4702, Validation Accuracy:0.3547\n",
    "Epoch #25: Loss:1.4686, Accuracy:0.3363, Validation Loss:1.4707, Validation Accuracy:0.3530\n",
    "Epoch #26: Loss:1.4651, Accuracy:0.3433, Validation Loss:1.4658, Validation Accuracy:0.3399\n",
    "Epoch #27: Loss:1.4626, Accuracy:0.3409, Validation Loss:1.4654, Validation Accuracy:0.3580\n",
    "Epoch #28: Loss:1.4603, Accuracy:0.3466, Validation Loss:1.4617, Validation Accuracy:0.3629\n",
    "Epoch #29: Loss:1.4594, Accuracy:0.3450, Validation Loss:1.4646, Validation Accuracy:0.3580\n",
    "Epoch #30: Loss:1.4569, Accuracy:0.3425, Validation Loss:1.4586, Validation Accuracy:0.3612\n",
    "Epoch #31: Loss:1.4588, Accuracy:0.3520, Validation Loss:1.4717, Validation Accuracy:0.3432\n",
    "Epoch #32: Loss:1.4562, Accuracy:0.3528, Validation Loss:1.4561, Validation Accuracy:0.3465\n",
    "Epoch #33: Loss:1.4536, Accuracy:0.3556, Validation Loss:1.4666, Validation Accuracy:0.3514\n",
    "Epoch #34: Loss:1.4569, Accuracy:0.3499, Validation Loss:1.4539, Validation Accuracy:0.3662\n",
    "Epoch #35: Loss:1.4517, Accuracy:0.3499, Validation Loss:1.4592, Validation Accuracy:0.3596\n",
    "Epoch #36: Loss:1.4535, Accuracy:0.3441, Validation Loss:1.4519, Validation Accuracy:0.3612\n",
    "Epoch #37: Loss:1.4509, Accuracy:0.3450, Validation Loss:1.4501, Validation Accuracy:0.3678\n",
    "Epoch #38: Loss:1.4528, Accuracy:0.3552, Validation Loss:1.4568, Validation Accuracy:0.3481\n",
    "Epoch #39: Loss:1.4504, Accuracy:0.3556, Validation Loss:1.4500, Validation Accuracy:0.3760\n",
    "Epoch #40: Loss:1.4500, Accuracy:0.3552, Validation Loss:1.4540, Validation Accuracy:0.3612\n",
    "Epoch #41: Loss:1.4519, Accuracy:0.3544, Validation Loss:1.4500, Validation Accuracy:0.3498\n",
    "Epoch #42: Loss:1.4501, Accuracy:0.3618, Validation Loss:1.4466, Validation Accuracy:0.3662\n",
    "Epoch #43: Loss:1.4483, Accuracy:0.3569, Validation Loss:1.4488, Validation Accuracy:0.3612\n",
    "Epoch #44: Loss:1.4464, Accuracy:0.3478, Validation Loss:1.4458, Validation Accuracy:0.3645\n",
    "Epoch #45: Loss:1.4450, Accuracy:0.3470, Validation Loss:1.4445, Validation Accuracy:0.3629\n",
    "Epoch #46: Loss:1.4442, Accuracy:0.3524, Validation Loss:1.4458, Validation Accuracy:0.3645\n",
    "Epoch #47: Loss:1.4447, Accuracy:0.3573, Validation Loss:1.4420, Validation Accuracy:0.3629\n",
    "Epoch #48: Loss:1.4429, Accuracy:0.3626, Validation Loss:1.4475, Validation Accuracy:0.3514\n",
    "Epoch #49: Loss:1.4447, Accuracy:0.3581, Validation Loss:1.4429, Validation Accuracy:0.3563\n",
    "Epoch #50: Loss:1.4454, Accuracy:0.3515, Validation Loss:1.4405, Validation Accuracy:0.3727\n",
    "Epoch #51: Loss:1.4462, Accuracy:0.3499, Validation Loss:1.4390, Validation Accuracy:0.3662\n",
    "Epoch #52: Loss:1.4450, Accuracy:0.3651, Validation Loss:1.4509, Validation Accuracy:0.3530\n",
    "Epoch #53: Loss:1.4435, Accuracy:0.3585, Validation Loss:1.4380, Validation Accuracy:0.3744\n",
    "Epoch #54: Loss:1.4416, Accuracy:0.3577, Validation Loss:1.4418, Validation Accuracy:0.3580\n",
    "Epoch #55: Loss:1.4457, Accuracy:0.3450, Validation Loss:1.4417, Validation Accuracy:0.3629\n",
    "Epoch #56: Loss:1.4484, Accuracy:0.3532, Validation Loss:1.4381, Validation Accuracy:0.3744\n",
    "Epoch #57: Loss:1.4432, Accuracy:0.3540, Validation Loss:1.4454, Validation Accuracy:0.3530\n",
    "Epoch #58: Loss:1.4400, Accuracy:0.3618, Validation Loss:1.4356, Validation Accuracy:0.3711\n",
    "Epoch #59: Loss:1.4392, Accuracy:0.3655, Validation Loss:1.4368, Validation Accuracy:0.3612\n",
    "Epoch #60: Loss:1.4376, Accuracy:0.3548, Validation Loss:1.4352, Validation Accuracy:0.3727\n",
    "Epoch #61: Loss:1.4383, Accuracy:0.3577, Validation Loss:1.4339, Validation Accuracy:0.3744\n",
    "Epoch #62: Loss:1.4405, Accuracy:0.3593, Validation Loss:1.4471, Validation Accuracy:0.3645\n",
    "Epoch #63: Loss:1.4397, Accuracy:0.3630, Validation Loss:1.4321, Validation Accuracy:0.3695\n",
    "Epoch #64: Loss:1.4351, Accuracy:0.3630, Validation Loss:1.4399, Validation Accuracy:0.3711\n",
    "Epoch #65: Loss:1.4349, Accuracy:0.3663, Validation Loss:1.4353, Validation Accuracy:0.3760\n",
    "Epoch #66: Loss:1.4379, Accuracy:0.3552, Validation Loss:1.4426, Validation Accuracy:0.3662\n",
    "Epoch #67: Loss:1.4365, Accuracy:0.3643, Validation Loss:1.4309, Validation Accuracy:0.3711\n",
    "Epoch #68: Loss:1.4344, Accuracy:0.3548, Validation Loss:1.4330, Validation Accuracy:0.3727\n",
    "Epoch #69: Loss:1.4324, Accuracy:0.3622, Validation Loss:1.4295, Validation Accuracy:0.3727\n",
    "Epoch #70: Loss:1.4353, Accuracy:0.3671, Validation Loss:1.4300, Validation Accuracy:0.3727\n",
    "Epoch #71: Loss:1.4324, Accuracy:0.3647, Validation Loss:1.4289, Validation Accuracy:0.3744\n",
    "Epoch #72: Loss:1.4316, Accuracy:0.3680, Validation Loss:1.4292, Validation Accuracy:0.3727\n",
    "Epoch #73: Loss:1.4309, Accuracy:0.3663, Validation Loss:1.4308, Validation Accuracy:0.3793\n",
    "Epoch #74: Loss:1.4296, Accuracy:0.3647, Validation Loss:1.4259, Validation Accuracy:0.3744\n",
    "Epoch #75: Loss:1.4286, Accuracy:0.3655, Validation Loss:1.4277, Validation Accuracy:0.3744\n",
    "Epoch #76: Loss:1.4290, Accuracy:0.3680, Validation Loss:1.4257, Validation Accuracy:0.3744\n",
    "Epoch #77: Loss:1.4297, Accuracy:0.3622, Validation Loss:1.4267, Validation Accuracy:0.3875\n",
    "Epoch #78: Loss:1.4301, Accuracy:0.3630, Validation Loss:1.4404, Validation Accuracy:0.3678\n",
    "Epoch #79: Loss:1.4307, Accuracy:0.3593, Validation Loss:1.4231, Validation Accuracy:0.3711\n",
    "Epoch #80: Loss:1.4279, Accuracy:0.3708, Validation Loss:1.4222, Validation Accuracy:0.3810\n",
    "Epoch #81: Loss:1.4291, Accuracy:0.3643, Validation Loss:1.4241, Validation Accuracy:0.3744\n",
    "Epoch #82: Loss:1.4301, Accuracy:0.3655, Validation Loss:1.4616, Validation Accuracy:0.3629\n",
    "Epoch #83: Loss:1.4429, Accuracy:0.3585, Validation Loss:1.4217, Validation Accuracy:0.3793\n",
    "Epoch #84: Loss:1.4283, Accuracy:0.3602, Validation Loss:1.4215, Validation Accuracy:0.3826\n",
    "Epoch #85: Loss:1.4265, Accuracy:0.3688, Validation Loss:1.4240, Validation Accuracy:0.3826\n",
    "Epoch #86: Loss:1.4264, Accuracy:0.3655, Validation Loss:1.4279, Validation Accuracy:0.3810\n",
    "Epoch #87: Loss:1.4262, Accuracy:0.3651, Validation Loss:1.4221, Validation Accuracy:0.3810\n",
    "Epoch #88: Loss:1.4257, Accuracy:0.3713, Validation Loss:1.4201, Validation Accuracy:0.3793\n",
    "Epoch #89: Loss:1.4244, Accuracy:0.3655, Validation Loss:1.4199, Validation Accuracy:0.3777\n",
    "Epoch #90: Loss:1.4236, Accuracy:0.3692, Validation Loss:1.4211, Validation Accuracy:0.3793\n",
    "Epoch #91: Loss:1.4232, Accuracy:0.3729, Validation Loss:1.4189, Validation Accuracy:0.3777\n",
    "Epoch #92: Loss:1.4246, Accuracy:0.3639, Validation Loss:1.4444, Validation Accuracy:0.3678\n",
    "Epoch #93: Loss:1.4254, Accuracy:0.3729, Validation Loss:1.4284, Validation Accuracy:0.3793\n",
    "Epoch #94: Loss:1.4350, Accuracy:0.3655, Validation Loss:1.4191, Validation Accuracy:0.3810\n",
    "Epoch #95: Loss:1.4268, Accuracy:0.3704, Validation Loss:1.4370, Validation Accuracy:0.3727\n",
    "Epoch #96: Loss:1.4260, Accuracy:0.3717, Validation Loss:1.4182, Validation Accuracy:0.3744\n",
    "Epoch #97: Loss:1.4213, Accuracy:0.3745, Validation Loss:1.4201, Validation Accuracy:0.3826\n",
    "Epoch #98: Loss:1.4208, Accuracy:0.3696, Validation Loss:1.4178, Validation Accuracy:0.3842\n",
    "Epoch #99: Loss:1.4192, Accuracy:0.3713, Validation Loss:1.4284, Validation Accuracy:0.3760\n",
    "Epoch #100: Loss:1.4203, Accuracy:0.3749, Validation Loss:1.4175, Validation Accuracy:0.3727\n",
    "Epoch #101: Loss:1.4217, Accuracy:0.3737, Validation Loss:1.4196, Validation Accuracy:0.3793\n",
    "Epoch #102: Loss:1.4304, Accuracy:0.3671, Validation Loss:1.4679, Validation Accuracy:0.3678\n",
    "Epoch #103: Loss:1.4399, Accuracy:0.3721, Validation Loss:1.4256, Validation Accuracy:0.3760\n",
    "Epoch #104: Loss:1.4280, Accuracy:0.3762, Validation Loss:1.4190, Validation Accuracy:0.3760\n",
    "Epoch #105: Loss:1.4247, Accuracy:0.3688, Validation Loss:1.4236, Validation Accuracy:0.3744\n",
    "Epoch #106: Loss:1.4209, Accuracy:0.3745, Validation Loss:1.4185, Validation Accuracy:0.3744\n",
    "Epoch #107: Loss:1.4193, Accuracy:0.3688, Validation Loss:1.4222, Validation Accuracy:0.3859\n",
    "Epoch #108: Loss:1.4176, Accuracy:0.3778, Validation Loss:1.4174, Validation Accuracy:0.3777\n",
    "Epoch #109: Loss:1.4174, Accuracy:0.3754, Validation Loss:1.4208, Validation Accuracy:0.3744\n",
    "Epoch #110: Loss:1.4171, Accuracy:0.3725, Validation Loss:1.4168, Validation Accuracy:0.3810\n",
    "Epoch #111: Loss:1.4176, Accuracy:0.3708, Validation Loss:1.4172, Validation Accuracy:0.3744\n",
    "Epoch #112: Loss:1.4164, Accuracy:0.3799, Validation Loss:1.4161, Validation Accuracy:0.3777\n",
    "Epoch #113: Loss:1.4190, Accuracy:0.3774, Validation Loss:1.4166, Validation Accuracy:0.3826\n",
    "Epoch #114: Loss:1.4183, Accuracy:0.3700, Validation Loss:1.4234, Validation Accuracy:0.3842\n",
    "Epoch #115: Loss:1.4176, Accuracy:0.3758, Validation Loss:1.4167, Validation Accuracy:0.3777\n",
    "Epoch #116: Loss:1.4176, Accuracy:0.3754, Validation Loss:1.4163, Validation Accuracy:0.3826\n",
    "Epoch #117: Loss:1.4162, Accuracy:0.3786, Validation Loss:1.4187, Validation Accuracy:0.3777\n",
    "Epoch #118: Loss:1.4179, Accuracy:0.3700, Validation Loss:1.4274, Validation Accuracy:0.3793\n",
    "Epoch #119: Loss:1.4195, Accuracy:0.3766, Validation Loss:1.4159, Validation Accuracy:0.3826\n",
    "Epoch #120: Loss:1.4197, Accuracy:0.3749, Validation Loss:1.4159, Validation Accuracy:0.3892\n",
    "Epoch #121: Loss:1.4195, Accuracy:0.3749, Validation Loss:1.4368, Validation Accuracy:0.3826\n",
    "Epoch #122: Loss:1.4232, Accuracy:0.3786, Validation Loss:1.4141, Validation Accuracy:0.3727\n",
    "Epoch #123: Loss:1.4213, Accuracy:0.3782, Validation Loss:1.4163, Validation Accuracy:0.3777\n",
    "Epoch #124: Loss:1.4178, Accuracy:0.3721, Validation Loss:1.4287, Validation Accuracy:0.3859\n",
    "Epoch #125: Loss:1.4178, Accuracy:0.3717, Validation Loss:1.4139, Validation Accuracy:0.3810\n",
    "Epoch #126: Loss:1.4130, Accuracy:0.3815, Validation Loss:1.4168, Validation Accuracy:0.3777\n",
    "Epoch #127: Loss:1.4143, Accuracy:0.3754, Validation Loss:1.4181, Validation Accuracy:0.3760\n",
    "Epoch #128: Loss:1.4139, Accuracy:0.3721, Validation Loss:1.4165, Validation Accuracy:0.3826\n",
    "Epoch #129: Loss:1.4123, Accuracy:0.3745, Validation Loss:1.4140, Validation Accuracy:0.3777\n",
    "Epoch #130: Loss:1.4116, Accuracy:0.3741, Validation Loss:1.4281, Validation Accuracy:0.3793\n",
    "Epoch #131: Loss:1.4122, Accuracy:0.3828, Validation Loss:1.4196, Validation Accuracy:0.3612\n",
    "Epoch #132: Loss:1.4192, Accuracy:0.3770, Validation Loss:1.4168, Validation Accuracy:0.3711\n",
    "Epoch #133: Loss:1.4174, Accuracy:0.3762, Validation Loss:1.4318, Validation Accuracy:0.3810\n",
    "Epoch #134: Loss:1.4193, Accuracy:0.3749, Validation Loss:1.4129, Validation Accuracy:0.3760\n",
    "Epoch #135: Loss:1.4141, Accuracy:0.3770, Validation Loss:1.4128, Validation Accuracy:0.3727\n",
    "Epoch #136: Loss:1.4160, Accuracy:0.3598, Validation Loss:1.4211, Validation Accuracy:0.3810\n",
    "Epoch #137: Loss:1.4114, Accuracy:0.3754, Validation Loss:1.4120, Validation Accuracy:0.3727\n",
    "Epoch #138: Loss:1.4103, Accuracy:0.3774, Validation Loss:1.4123, Validation Accuracy:0.3760\n",
    "Epoch #139: Loss:1.4116, Accuracy:0.3733, Validation Loss:1.4136, Validation Accuracy:0.3727\n",
    "Epoch #140: Loss:1.4109, Accuracy:0.3725, Validation Loss:1.4153, Validation Accuracy:0.3777\n",
    "Epoch #141: Loss:1.4116, Accuracy:0.3791, Validation Loss:1.4136, Validation Accuracy:0.3678\n",
    "Epoch #142: Loss:1.4090, Accuracy:0.3696, Validation Loss:1.4263, Validation Accuracy:0.3777\n",
    "Epoch #143: Loss:1.4129, Accuracy:0.3823, Validation Loss:1.4215, Validation Accuracy:0.3629\n",
    "Epoch #144: Loss:1.4139, Accuracy:0.3700, Validation Loss:1.4170, Validation Accuracy:0.3760\n",
    "Epoch #145: Loss:1.4119, Accuracy:0.3684, Validation Loss:1.4213, Validation Accuracy:0.3777\n",
    "Epoch #146: Loss:1.4141, Accuracy:0.3758, Validation Loss:1.4114, Validation Accuracy:0.3744\n",
    "Epoch #147: Loss:1.4103, Accuracy:0.3729, Validation Loss:1.4114, Validation Accuracy:0.3760\n",
    "Epoch #148: Loss:1.4118, Accuracy:0.3733, Validation Loss:1.4191, Validation Accuracy:0.3810\n",
    "Epoch #149: Loss:1.4092, Accuracy:0.3754, Validation Loss:1.4149, Validation Accuracy:0.3678\n",
    "Epoch #150: Loss:1.4083, Accuracy:0.3762, Validation Loss:1.4167, Validation Accuracy:0.3810\n",
    "Epoch #151: Loss:1.4086, Accuracy:0.3717, Validation Loss:1.4117, Validation Accuracy:0.3744\n",
    "Epoch #152: Loss:1.4073, Accuracy:0.3848, Validation Loss:1.4103, Validation Accuracy:0.3727\n",
    "Epoch #153: Loss:1.4106, Accuracy:0.3741, Validation Loss:1.4163, Validation Accuracy:0.3695\n",
    "Epoch #154: Loss:1.4101, Accuracy:0.3721, Validation Loss:1.4177, Validation Accuracy:0.3760\n",
    "Epoch #155: Loss:1.4141, Accuracy:0.3717, Validation Loss:1.4178, Validation Accuracy:0.3645\n",
    "Epoch #156: Loss:1.4184, Accuracy:0.3741, Validation Loss:1.4107, Validation Accuracy:0.3711\n",
    "Epoch #157: Loss:1.4080, Accuracy:0.3700, Validation Loss:1.4234, Validation Accuracy:0.3810\n",
    "Epoch #158: Loss:1.4169, Accuracy:0.3700, Validation Loss:1.4219, Validation Accuracy:0.3629\n",
    "Epoch #159: Loss:1.4123, Accuracy:0.3733, Validation Loss:1.4277, Validation Accuracy:0.3777\n",
    "Epoch #160: Loss:1.4118, Accuracy:0.3766, Validation Loss:1.4110, Validation Accuracy:0.3744\n",
    "Epoch #161: Loss:1.4072, Accuracy:0.3791, Validation Loss:1.4139, Validation Accuracy:0.3727\n",
    "Epoch #162: Loss:1.4058, Accuracy:0.3725, Validation Loss:1.4127, Validation Accuracy:0.3744\n",
    "Epoch #163: Loss:1.4074, Accuracy:0.3741, Validation Loss:1.4121, Validation Accuracy:0.3711\n",
    "Epoch #164: Loss:1.4073, Accuracy:0.3754, Validation Loss:1.4162, Validation Accuracy:0.3744\n",
    "Epoch #165: Loss:1.4046, Accuracy:0.3754, Validation Loss:1.4110, Validation Accuracy:0.3727\n",
    "Epoch #166: Loss:1.4048, Accuracy:0.3717, Validation Loss:1.4121, Validation Accuracy:0.3744\n",
    "Epoch #167: Loss:1.4048, Accuracy:0.3741, Validation Loss:1.4107, Validation Accuracy:0.3662\n",
    "Epoch #168: Loss:1.4046, Accuracy:0.3704, Validation Loss:1.4210, Validation Accuracy:0.3777\n",
    "Epoch #169: Loss:1.4091, Accuracy:0.3774, Validation Loss:1.4099, Validation Accuracy:0.3744\n",
    "Epoch #170: Loss:1.4064, Accuracy:0.3807, Validation Loss:1.4122, Validation Accuracy:0.3596\n",
    "Epoch #171: Loss:1.4049, Accuracy:0.3713, Validation Loss:1.4207, Validation Accuracy:0.3810\n",
    "Epoch #172: Loss:1.4037, Accuracy:0.3795, Validation Loss:1.4120, Validation Accuracy:0.3629\n",
    "Epoch #173: Loss:1.4034, Accuracy:0.3832, Validation Loss:1.4161, Validation Accuracy:0.3810\n",
    "Epoch #174: Loss:1.4038, Accuracy:0.3749, Validation Loss:1.4108, Validation Accuracy:0.3711\n",
    "Epoch #175: Loss:1.4068, Accuracy:0.3774, Validation Loss:1.4114, Validation Accuracy:0.3744\n",
    "Epoch #176: Loss:1.4083, Accuracy:0.3766, Validation Loss:1.4107, Validation Accuracy:0.3777\n",
    "Epoch #177: Loss:1.4120, Accuracy:0.3737, Validation Loss:1.4398, Validation Accuracy:0.3793\n",
    "Epoch #178: Loss:1.4113, Accuracy:0.3828, Validation Loss:1.4209, Validation Accuracy:0.3629\n",
    "Epoch #179: Loss:1.4176, Accuracy:0.3729, Validation Loss:1.4130, Validation Accuracy:0.3760\n",
    "Epoch #180: Loss:1.4066, Accuracy:0.3791, Validation Loss:1.4162, Validation Accuracy:0.3826\n",
    "Epoch #181: Loss:1.4034, Accuracy:0.3815, Validation Loss:1.4118, Validation Accuracy:0.3629\n",
    "Epoch #182: Loss:1.4084, Accuracy:0.3733, Validation Loss:1.4232, Validation Accuracy:0.3793\n",
    "Epoch #183: Loss:1.4043, Accuracy:0.3807, Validation Loss:1.4098, Validation Accuracy:0.3711\n",
    "Epoch #184: Loss:1.4036, Accuracy:0.3745, Validation Loss:1.4102, Validation Accuracy:0.3760\n",
    "Epoch #185: Loss:1.4023, Accuracy:0.3692, Validation Loss:1.4122, Validation Accuracy:0.3760\n",
    "Epoch #186: Loss:1.4025, Accuracy:0.3758, Validation Loss:1.4120, Validation Accuracy:0.3711\n",
    "Epoch #187: Loss:1.4005, Accuracy:0.3754, Validation Loss:1.4100, Validation Accuracy:0.3727\n",
    "Epoch #188: Loss:1.4022, Accuracy:0.3762, Validation Loss:1.4160, Validation Accuracy:0.3760\n",
    "Epoch #189: Loss:1.4056, Accuracy:0.3864, Validation Loss:1.4092, Validation Accuracy:0.3678\n",
    "Epoch #190: Loss:1.4041, Accuracy:0.3799, Validation Loss:1.4107, Validation Accuracy:0.3760\n",
    "Epoch #191: Loss:1.4038, Accuracy:0.3782, Validation Loss:1.4119, Validation Accuracy:0.3744\n",
    "Epoch #192: Loss:1.4020, Accuracy:0.3737, Validation Loss:1.4192, Validation Accuracy:0.3727\n",
    "Epoch #193: Loss:1.4032, Accuracy:0.3774, Validation Loss:1.4087, Validation Accuracy:0.3678\n",
    "Epoch #194: Loss:1.3991, Accuracy:0.3807, Validation Loss:1.4107, Validation Accuracy:0.3760\n",
    "Epoch #195: Loss:1.3998, Accuracy:0.3770, Validation Loss:1.4167, Validation Accuracy:0.3727\n",
    "Epoch #196: Loss:1.4014, Accuracy:0.3791, Validation Loss:1.4101, Validation Accuracy:0.3629\n",
    "Epoch #197: Loss:1.4034, Accuracy:0.3799, Validation Loss:1.4075, Validation Accuracy:0.3727\n",
    "Epoch #198: Loss:1.4003, Accuracy:0.3717, Validation Loss:1.4167, Validation Accuracy:0.3826\n",
    "Epoch #199: Loss:1.4012, Accuracy:0.3786, Validation Loss:1.4089, Validation Accuracy:0.3695\n",
    "Epoch #200: Loss:1.3998, Accuracy:0.3848, Validation Loss:1.4100, Validation Accuracy:0.3727\n",
    "Epoch #201: Loss:1.4007, Accuracy:0.3782, Validation Loss:1.4233, Validation Accuracy:0.3793\n",
    "Epoch #202: Loss:1.4075, Accuracy:0.3758, Validation Loss:1.4099, Validation Accuracy:0.3793\n",
    "Epoch #203: Loss:1.4000, Accuracy:0.3864, Validation Loss:1.4147, Validation Accuracy:0.3727\n",
    "Epoch #204: Loss:1.4067, Accuracy:0.3749, Validation Loss:1.4222, Validation Accuracy:0.3727\n",
    "Epoch #205: Loss:1.3984, Accuracy:0.3803, Validation Loss:1.4134, Validation Accuracy:0.3727\n",
    "Epoch #206: Loss:1.4051, Accuracy:0.3774, Validation Loss:1.4103, Validation Accuracy:0.3826\n",
    "Epoch #207: Loss:1.4024, Accuracy:0.3725, Validation Loss:1.4241, Validation Accuracy:0.3859\n",
    "Epoch #208: Loss:1.4038, Accuracy:0.3786, Validation Loss:1.4108, Validation Accuracy:0.3678\n",
    "Epoch #209: Loss:1.3996, Accuracy:0.3754, Validation Loss:1.4127, Validation Accuracy:0.3793\n",
    "Epoch #210: Loss:1.3967, Accuracy:0.3770, Validation Loss:1.4075, Validation Accuracy:0.3744\n",
    "Epoch #211: Loss:1.3985, Accuracy:0.3811, Validation Loss:1.4072, Validation Accuracy:0.3711\n",
    "Epoch #212: Loss:1.3978, Accuracy:0.3766, Validation Loss:1.4166, Validation Accuracy:0.3826\n",
    "Epoch #213: Loss:1.3984, Accuracy:0.3782, Validation Loss:1.4112, Validation Accuracy:0.3793\n",
    "Epoch #214: Loss:1.3974, Accuracy:0.3836, Validation Loss:1.4083, Validation Accuracy:0.3662\n",
    "Epoch #215: Loss:1.4003, Accuracy:0.3782, Validation Loss:1.4084, Validation Accuracy:0.3645\n",
    "Epoch #216: Loss:1.3965, Accuracy:0.3749, Validation Loss:1.4130, Validation Accuracy:0.3859\n",
    "Epoch #217: Loss:1.3969, Accuracy:0.3786, Validation Loss:1.4112, Validation Accuracy:0.3810\n",
    "Epoch #218: Loss:1.3979, Accuracy:0.3799, Validation Loss:1.4077, Validation Accuracy:0.3744\n",
    "Epoch #219: Loss:1.3928, Accuracy:0.3791, Validation Loss:1.4163, Validation Accuracy:0.3793\n",
    "Epoch #220: Loss:1.3975, Accuracy:0.3778, Validation Loss:1.4074, Validation Accuracy:0.3859\n",
    "Epoch #221: Loss:1.3985, Accuracy:0.3828, Validation Loss:1.4067, Validation Accuracy:0.3678\n",
    "Epoch #222: Loss:1.3963, Accuracy:0.3832, Validation Loss:1.4072, Validation Accuracy:0.3810\n",
    "Epoch #223: Loss:1.3954, Accuracy:0.3832, Validation Loss:1.4184, Validation Accuracy:0.3892\n",
    "Epoch #224: Loss:1.4001, Accuracy:0.3745, Validation Loss:1.4095, Validation Accuracy:0.3530\n",
    "Epoch #225: Loss:1.3989, Accuracy:0.3840, Validation Loss:1.4076, Validation Accuracy:0.3727\n",
    "Epoch #226: Loss:1.4003, Accuracy:0.3791, Validation Loss:1.4086, Validation Accuracy:0.3760\n",
    "Epoch #227: Loss:1.3948, Accuracy:0.3782, Validation Loss:1.4109, Validation Accuracy:0.3711\n",
    "Epoch #228: Loss:1.3960, Accuracy:0.3807, Validation Loss:1.4076, Validation Accuracy:0.3727\n",
    "Epoch #229: Loss:1.3926, Accuracy:0.3823, Validation Loss:1.4066, Validation Accuracy:0.3711\n",
    "Epoch #230: Loss:1.3924, Accuracy:0.3815, Validation Loss:1.4087, Validation Accuracy:0.3695\n",
    "Epoch #231: Loss:1.3941, Accuracy:0.3786, Validation Loss:1.4087, Validation Accuracy:0.3580\n",
    "Epoch #232: Loss:1.3986, Accuracy:0.3889, Validation Loss:1.4126, Validation Accuracy:0.3793\n",
    "Epoch #233: Loss:1.3995, Accuracy:0.3885, Validation Loss:1.4271, Validation Accuracy:0.3727\n",
    "Epoch #234: Loss:1.3991, Accuracy:0.3864, Validation Loss:1.4107, Validation Accuracy:0.3629\n",
    "Epoch #235: Loss:1.4011, Accuracy:0.3762, Validation Loss:1.4184, Validation Accuracy:0.3547\n",
    "Epoch #236: Loss:1.4021, Accuracy:0.3897, Validation Loss:1.4078, Validation Accuracy:0.3547\n",
    "Epoch #237: Loss:1.3979, Accuracy:0.3864, Validation Loss:1.4091, Validation Accuracy:0.3760\n",
    "Epoch #238: Loss:1.3923, Accuracy:0.3815, Validation Loss:1.4169, Validation Accuracy:0.3514\n",
    "Epoch #239: Loss:1.3958, Accuracy:0.3860, Validation Loss:1.4109, Validation Accuracy:0.3645\n",
    "Epoch #240: Loss:1.3935, Accuracy:0.3840, Validation Loss:1.4090, Validation Accuracy:0.3645\n",
    "Epoch #241: Loss:1.3912, Accuracy:0.3856, Validation Loss:1.4075, Validation Accuracy:0.3695\n",
    "Epoch #242: Loss:1.3980, Accuracy:0.3811, Validation Loss:1.4208, Validation Accuracy:0.3793\n",
    "Epoch #243: Loss:1.4007, Accuracy:0.3906, Validation Loss:1.4250, Validation Accuracy:0.3563\n",
    "Epoch #244: Loss:1.4010, Accuracy:0.3864, Validation Loss:1.4154, Validation Accuracy:0.3645\n",
    "Epoch #245: Loss:1.3954, Accuracy:0.3860, Validation Loss:1.4049, Validation Accuracy:0.3793\n",
    "Epoch #246: Loss:1.3872, Accuracy:0.3864, Validation Loss:1.4037, Validation Accuracy:0.3678\n",
    "Epoch #247: Loss:1.3874, Accuracy:0.3840, Validation Loss:1.4068, Validation Accuracy:0.3810\n",
    "Epoch #248: Loss:1.3883, Accuracy:0.3934, Validation Loss:1.4026, Validation Accuracy:0.3810\n",
    "Epoch #249: Loss:1.3947, Accuracy:0.3889, Validation Loss:1.4052, Validation Accuracy:0.3695\n",
    "Epoch #250: Loss:1.3924, Accuracy:0.3938, Validation Loss:1.4095, Validation Accuracy:0.3777\n",
    "Epoch #251: Loss:1.3928, Accuracy:0.3836, Validation Loss:1.4148, Validation Accuracy:0.3678\n",
    "Epoch #252: Loss:1.3908, Accuracy:0.3877, Validation Loss:1.4038, Validation Accuracy:0.3875\n",
    "Epoch #253: Loss:1.3826, Accuracy:0.3943, Validation Loss:1.4030, Validation Accuracy:0.3711\n",
    "Epoch #254: Loss:1.3835, Accuracy:0.4004, Validation Loss:1.4025, Validation Accuracy:0.3810\n",
    "Epoch #255: Loss:1.3843, Accuracy:0.3906, Validation Loss:1.4078, Validation Accuracy:0.3662\n",
    "Epoch #256: Loss:1.3850, Accuracy:0.3955, Validation Loss:1.4034, Validation Accuracy:0.3826\n",
    "Epoch #257: Loss:1.3791, Accuracy:0.4016, Validation Loss:1.4037, Validation Accuracy:0.3711\n",
    "Epoch #258: Loss:1.3788, Accuracy:0.4037, Validation Loss:1.4078, Validation Accuracy:0.3645\n",
    "Epoch #259: Loss:1.3797, Accuracy:0.4053, Validation Loss:1.4056, Validation Accuracy:0.3727\n",
    "Epoch #260: Loss:1.3776, Accuracy:0.4086, Validation Loss:1.4031, Validation Accuracy:0.3612\n",
    "Epoch #261: Loss:1.3822, Accuracy:0.4033, Validation Loss:1.4087, Validation Accuracy:0.3612\n",
    "Epoch #262: Loss:1.3756, Accuracy:0.4037, Validation Loss:1.4079, Validation Accuracy:0.3760\n",
    "Epoch #263: Loss:1.3782, Accuracy:0.3988, Validation Loss:1.4020, Validation Accuracy:0.3563\n",
    "Epoch #264: Loss:1.3783, Accuracy:0.3984, Validation Loss:1.4165, Validation Accuracy:0.3481\n",
    "Epoch #265: Loss:1.3781, Accuracy:0.3984, Validation Loss:1.4015, Validation Accuracy:0.3695\n",
    "Epoch #266: Loss:1.3734, Accuracy:0.4070, Validation Loss:1.4042, Validation Accuracy:0.3662\n",
    "Epoch #267: Loss:1.3718, Accuracy:0.4041, Validation Loss:1.4043, Validation Accuracy:0.3662\n",
    "Epoch #268: Loss:1.3742, Accuracy:0.3963, Validation Loss:1.4007, Validation Accuracy:0.3711\n",
    "Epoch #269: Loss:1.3781, Accuracy:0.3967, Validation Loss:1.4145, Validation Accuracy:0.3711\n",
    "Epoch #270: Loss:1.3803, Accuracy:0.3996, Validation Loss:1.4074, Validation Accuracy:0.3695\n",
    "Epoch #271: Loss:1.3753, Accuracy:0.4086, Validation Loss:1.4132, Validation Accuracy:0.3530\n",
    "Epoch #272: Loss:1.3853, Accuracy:0.3897, Validation Loss:1.4398, Validation Accuracy:0.3678\n",
    "Epoch #273: Loss:1.3805, Accuracy:0.3959, Validation Loss:1.4284, Validation Accuracy:0.3629\n",
    "Epoch #274: Loss:1.3938, Accuracy:0.3934, Validation Loss:1.4509, Validation Accuracy:0.3514\n",
    "Epoch #275: Loss:1.3918, Accuracy:0.3844, Validation Loss:1.4044, Validation Accuracy:0.3645\n",
    "Epoch #276: Loss:1.3861, Accuracy:0.3901, Validation Loss:1.4208, Validation Accuracy:0.3629\n",
    "Epoch #277: Loss:1.3740, Accuracy:0.3959, Validation Loss:1.3995, Validation Accuracy:0.3744\n",
    "Epoch #278: Loss:1.3738, Accuracy:0.3963, Validation Loss:1.4161, Validation Accuracy:0.3695\n",
    "Epoch #279: Loss:1.3772, Accuracy:0.3959, Validation Loss:1.4016, Validation Accuracy:0.3580\n",
    "Epoch #280: Loss:1.3701, Accuracy:0.3988, Validation Loss:1.4071, Validation Accuracy:0.3514\n",
    "Epoch #281: Loss:1.3674, Accuracy:0.4029, Validation Loss:1.3978, Validation Accuracy:0.3629\n",
    "Epoch #282: Loss:1.3687, Accuracy:0.4078, Validation Loss:1.4017, Validation Accuracy:0.3678\n",
    "Epoch #283: Loss:1.3704, Accuracy:0.4053, Validation Loss:1.4183, Validation Accuracy:0.3760\n",
    "Epoch #284: Loss:1.3689, Accuracy:0.4045, Validation Loss:1.4008, Validation Accuracy:0.3563\n",
    "Epoch #285: Loss:1.3673, Accuracy:0.4053, Validation Loss:1.4111, Validation Accuracy:0.3760\n",
    "Epoch #286: Loss:1.3630, Accuracy:0.4086, Validation Loss:1.3999, Validation Accuracy:0.3645\n",
    "Epoch #287: Loss:1.3613, Accuracy:0.4029, Validation Loss:1.4106, Validation Accuracy:0.3777\n",
    "Epoch #288: Loss:1.3648, Accuracy:0.3938, Validation Loss:1.4048, Validation Accuracy:0.3695\n",
    "Epoch #289: Loss:1.3683, Accuracy:0.4074, Validation Loss:1.4019, Validation Accuracy:0.3629\n",
    "Epoch #290: Loss:1.3761, Accuracy:0.3984, Validation Loss:1.4278, Validation Accuracy:0.3432\n",
    "Epoch #291: Loss:1.3687, Accuracy:0.3922, Validation Loss:1.4099, Validation Accuracy:0.3695\n",
    "Epoch #292: Loss:1.3626, Accuracy:0.4123, Validation Loss:1.4091, Validation Accuracy:0.3727\n",
    "Epoch #293: Loss:1.3686, Accuracy:0.4025, Validation Loss:1.3978, Validation Accuracy:0.3678\n",
    "Epoch #294: Loss:1.3659, Accuracy:0.4062, Validation Loss:1.4181, Validation Accuracy:0.3448\n",
    "Epoch #295: Loss:1.3661, Accuracy:0.3975, Validation Loss:1.3999, Validation Accuracy:0.3777\n",
    "Epoch #296: Loss:1.3642, Accuracy:0.4057, Validation Loss:1.4159, Validation Accuracy:0.3662\n",
    "Epoch #297: Loss:1.3689, Accuracy:0.4012, Validation Loss:1.4017, Validation Accuracy:0.3662\n",
    "Epoch #298: Loss:1.3641, Accuracy:0.3979, Validation Loss:1.4420, Validation Accuracy:0.3448\n",
    "Epoch #299: Loss:1.3661, Accuracy:0.4107, Validation Loss:1.4061, Validation Accuracy:0.3645\n",
    "Epoch #300: Loss:1.3586, Accuracy:0.4123, Validation Loss:1.4106, Validation Accuracy:0.3547\n",
    "\n",
    "Test:\n",
    "Test Loss:1.41057920, Accuracy:0.3547\n",
    "Labels: ['05', '01', '03', '02', '04']\n",
    "Confusion Matrix:\n",
    "      05  01  03  02  04\n",
    "t:05  89  23  12   0  18\n",
    "t:01  42  31  24   5  24\n",
    "t:03  12  13  27   3  60\n",
    "t:02  24  26  21   7  36\n",
    "t:04  10   8  26   6  62\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.50      0.63      0.56       142\n",
    "          01       0.31      0.25      0.27       126\n",
    "          03       0.25      0.23      0.24       115\n",
    "          02       0.33      0.06      0.10       114\n",
    "          04       0.31      0.55      0.40       112\n",
    "\n",
    "    accuracy                           0.35       609\n",
    "   macro avg       0.34      0.34      0.31       609\n",
    "weighted avg       0.35      0.35      0.32       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 22:21:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 28 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.606696216147913, 1.6053513143645914, 1.605474571876338, 1.605465795410482, 1.6051317504278348, 1.6048701337992852, 1.6046563304703811, 1.6042860597616737, 1.6035202387322738, 1.6024256940741453, 1.6007172793198885, 1.5977470778870857, 1.5930425868245768, 1.585322658220927, 1.5730128724782533, 1.5548833821990415, 1.532831160305756, 1.5088285309536311, 1.4902701060760197, 1.4844385111468963, 1.487989204661991, 1.4775253777042008, 1.476293809895445, 1.4701841999157308, 1.4706704845569405, 1.4657733602868317, 1.465445275768662, 1.461735292412769, 1.464598507716738, 1.4585788852867039, 1.4717310212907337, 1.4561407284196375, 1.466590463821524, 1.4538813508398623, 1.4591506476864242, 1.4519057416759298, 1.4501045384430533, 1.4568073808266024, 1.4500372574247162, 1.454024988442219, 1.4500129667213202, 1.4465562116923592, 1.44881317180953, 1.4457769084642282, 1.4444844364532696, 1.4457869169551556, 1.4419619847205276, 1.447459529969101, 1.4429006834922753, 1.440513502396582, 1.4389574286972948, 1.4509467857420346, 1.4379730780527902, 1.441815948251433, 1.4416884518609259, 1.4380580699698287, 1.445403125485763, 1.4355604370630826, 1.436833601084053, 1.435163057887887, 1.4339312900267602, 1.4470798855736142, 1.432077573633742, 1.439873822412663, 1.4353186418661734, 1.4425723991174808, 1.4308660633262547, 1.4329592610031905, 1.4294739497706221, 1.4299672914451762, 1.4289118916726073, 1.4292027748668528, 1.4308071725669949, 1.4259439777270915, 1.4277057027190385, 1.4256517415367715, 1.4266690610860564, 1.4403866134057883, 1.423117025536661, 1.4222349711435378, 1.424063215702038, 1.4616091819036574, 1.4217202769124448, 1.4215274966996292, 1.4240458806355794, 1.4278877368701504, 1.4221270620725033, 1.4201286261892083, 1.4198746618574671, 1.4211001646734027, 1.418855807659857, 1.4444180777898954, 1.428439979874246, 1.4191008371672607, 1.4369503584597108, 1.4181682368608923, 1.4201210690249364, 1.4177690552568984, 1.42840154946144, 1.4175455292261685, 1.4196048262475551, 1.4678532437150702, 1.425608719902477, 1.419035821908409, 1.4235571256803565, 1.4185493088316643, 1.4222005971742577, 1.4174041726514819, 1.4208121644256542, 1.4168039697340165, 1.4172453459456245, 1.41614347882263, 1.4166034317173197, 1.4234218025833907, 1.4167163945575458, 1.4163260459899902, 1.4187078777400928, 1.4273894950869832, 1.4158844593514754, 1.415866187249107, 1.4368492836631186, 1.4141468551554313, 1.416278883545661, 1.4286531916588594, 1.4139251528898091, 1.4167923244153728, 1.4180865833911989, 1.416458376326976, 1.4139761654614227, 1.428134920561842, 1.419592827020216, 1.416755862619685, 1.4318016812523402, 1.4128914055565895, 1.4127556496653064, 1.421140515354075, 1.4119634072377372, 1.4122864189993571, 1.4136331586414956, 1.4152586215431076, 1.4136181228070814, 1.4263271784351768, 1.4214626668122015, 1.417008697301492, 1.4212557218344928, 1.4114392077785798, 1.4114478006347255, 1.4191283680535303, 1.414889814818434, 1.416651251868074, 1.4117328100799535, 1.4103250405667058, 1.4162553764133423, 1.4177154717578482, 1.4178242818475357, 1.4106621288117909, 1.4233728891914506, 1.4219304399537336, 1.4276763764508251, 1.411046340547759, 1.4139403008866585, 1.4127173535342288, 1.4121245267160225, 1.416181840137112, 1.4109799991100294, 1.4120829166058444, 1.4107242276515868, 1.4210332853257754, 1.4098990488130667, 1.412151643795333, 1.4206716802907107, 1.411966401563685, 1.4161188508489448, 1.4107870628876835, 1.4114322922695641, 1.4106573328400285, 1.439782363244857, 1.4209377728463786, 1.4130178844595973, 1.4161593407049946, 1.4118048254100757, 1.4231857703432857, 1.4097995047498806, 1.4102290855056938, 1.4121906634034782, 1.4119755742193638, 1.4100451729763512, 1.4159761301206641, 1.4091904341489419, 1.4107090806334672, 1.411911642022908, 1.4191983540852864, 1.4086837670681707, 1.410690582836007, 1.4167051084327384, 1.4100793836935008, 1.4075211613440552, 1.4167018615945024, 1.4089242047668482, 1.4100209777969837, 1.4233468816002406, 1.4098691591879808, 1.4147296973637171, 1.4222217874573957, 1.4134199950103885, 1.410300695054441, 1.4240585432459765, 1.4108087902977353, 1.412738766967761, 1.4074602087926003, 1.4072315174174819, 1.4165671423738226, 1.411237198143757, 1.4083179319629138, 1.4084316552761935, 1.413023016135681, 1.4111808372052823, 1.4077050002729168, 1.4163110312961398, 1.4073733931104537, 1.4067396790718993, 1.4071694910036912, 1.418372119784551, 1.4094830318820497, 1.4076482517574416, 1.4086334345180218, 1.4108785173575866, 1.4076132921162496, 1.4066318531928979, 1.408720850944519, 1.40866384776355, 1.4125807915610828, 1.427110542608991, 1.4107304878031288, 1.4183580200073167, 1.407849810589319, 1.409062678003546, 1.4169321598482054, 1.410883030867929, 1.4089720534965127, 1.407505048319624, 1.4207798590996779, 1.4249854604598924, 1.4153855285425296, 1.404909762842902, 1.40370791807942, 1.4068282233866174, 1.4026206164132982, 1.405168096420213, 1.4094924574415084, 1.4148344034436104, 1.4038066603671546, 1.402973941785753, 1.402518283361676, 1.4078101506961391, 1.4034469773616698, 1.403704129807859, 1.407782685580512, 1.4055648360933577, 1.4030631962668132, 1.408669650065292, 1.407871263955027, 1.4019735286192745, 1.4165232980192588, 1.4015307741603633, 1.404169158003796, 1.404289639837832, 1.4007143453619946, 1.4145045765906523, 1.4073884119149696, 1.413178133847091, 1.4397973291979635, 1.4284366823378063, 1.4509199757881353, 1.4044023313741574, 1.4208076665749887, 1.399471421351378, 1.4161357188655435, 1.4015840728490419, 1.4071379878446582, 1.3978018729361799, 1.4016868001330272, 1.418334883226354, 1.4008030088860022, 1.4110717569861702, 1.3998708764124779, 1.4105947279969264, 1.4047674345852705, 1.4018699036443174, 1.4277727049951288, 1.4098615736405447, 1.409086179654978, 1.3978065318857704, 1.4180548498391714, 1.3998560230132981, 1.4159370281034698, 1.4016536131672475, 1.441994104870826, 1.4061211803668043, 1.4105792916662783], 'val_acc': [0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.24137931022248635, 0.25615763544351206, 0.2742200308343264, 0.2939244642339904, 0.30049260883104234, 0.3169129700789898, 0.321839078551247, 0.3119868619003515, 0.32840722314829895, 0.32019704232857926, 0.32840722324617194, 0.3546798011450149, 0.35303776502022016, 0.33990147602186216, 0.3579638732967314, 0.3628899817689886, 0.3579638734924774, 0.361247945252702, 0.3431855482714517, 0.3464696203252952, 0.35139572909117134, 0.3661740541164511, 0.3596059098130181, 0.36124794583993985, 0.36781609033911883, 0.34811165684158185, 0.37602627076734657, 0.36124794583993985, 0.3497536926727577, 0.3661740539207051, 0.36124794593781284, 0.36453201808952934, 0.3628899818668616, 0.36453201789378337, 0.3628899819647346, 0.35139572879755243, 0.35632183736768264, 0.37274219861563007, 0.3661740542143241, 0.3530377649223472, 0.3743842347404248, 0.35796387359035037, 0.3628899818668616, 0.37438423454467884, 0.35303776502022016, 0.37110016249083533, 0.36124794574206687, 0.37274219861563007, 0.3743842347404248, 0.36453201799165635, 0.36945812656178656, 0.37110016249083533, 0.37602627076734657, 0.3661740539207051, 0.3711001625887083, 0.37274219871350306, 0.37274219881137605, 0.37274219871350306, 0.3743842348382978, 0.37274219861563007, 0.37931034321268203, 0.3743842347404248, 0.3743842348382978, 0.3743842347404248, 0.3875205241302747, 0.36781609004549987, 0.37110016249083533, 0.38095237953322275, 0.3743842347404248, 0.36288998167111564, 0.37931034321268203, 0.3825944156580175, 0.3825944155601445, 0.3809523791417308, 0.3809523793374768, 0.379310343408428, 0.3776683071857603, 0.37931034311480905, 0.3776683071857603, 0.36781609004549987, 0.37931034311480905, 0.38095237943534976, 0.3727421985177571, 0.3743842347404248, 0.3825944154622715, 0.3842364514891933, 0.37602627076734657, 0.3727421985177571, 0.37931034321268203, 0.36781609004549987, 0.37602627086521956, 0.37602627096309255, 0.3743842347404248, 0.3743842347404248, 0.385878487711861, 0.3776683069900143, 0.3743842346425518, 0.3809523793374768, 0.3743842346425518, 0.3776683071857603, 0.3825944154622715, 0.38423645158706626, 0.3776683070878873, 0.3825944155601445, 0.3776683069900143, 0.37931034321268203, 0.3825944155601445, 0.38916256035294244, 0.3825944155601445, 0.37274219871350306, 0.37766830728363326, 0.385878487711861, 0.38095237953322275, 0.3776683069900143, 0.37602627086521956, 0.3825944155601445, 0.37766830728363326, 0.37931034301693606, 0.36124794593781284, 0.3711001626865813, 0.3809523792396038, 0.37602627106096553, 0.37274219881137605, 0.3809523793374768, 0.37274219890924903, 0.37602627106096553, 0.37274219871350306, 0.3776683071857603, 0.3678160904369918, 0.3776683069900143, 0.3628899824540995, 0.37602627096309255, 0.3776683070878873, 0.3743842350340438, 0.3760262711588385, 0.3809523792396038, 0.36781609063273774, 0.38095237943534976, 0.3743842348382978, 0.37274219890924903, 0.3694581263660406, 0.37602627106096553, 0.36453201838314825, 0.3711001628823273, 0.38095237943534976, 0.3628899824540995, 0.3776683068921413, 0.37438423513191676, 0.372742199007122, 0.37438423513191676, 0.3711001626865813, 0.3743842347404248, 0.372742199007122, 0.3743842350340438, 0.36617405441007006, 0.3776683070878873, 0.37438423513191676, 0.35960591020451, 0.38095237943534976, 0.3628899822583535, 0.38095237943534976, 0.3711001628823273, 0.37438423542553567, 0.37766830777299815, 0.3793103429190631, 0.36288998255197247, 0.3760262712567115, 0.3825944157558905, 0.3628899820626076, 0.37931034321268203, 0.3711001628823273, 0.37602627145245743, 0.37602627086521956, 0.3711001629802002, 0.37274219890924903, 0.37602627096309255, 0.36781609063273774, 0.3760262715503304, 0.3743842352297897, 0.37274219881137605, 0.3678160907306107, 0.3760262712567115, 0.37274219910499495, 0.3628899823562265, 0.37274219890924903, 0.38259441585376347, 0.3694581267575325, 0.37274219920286794, 0.37931034321268203, 0.379310343604174, 0.37274219930074093, 0.37274219890924903, 0.37274219930074093, 0.3825944157558905, 0.385878487907607, 0.3678160907306107, 0.3793103437020469, 0.3743842352297897, 0.3711001630780732, 0.3825944157558905, 0.379310343408428, 0.36617405470368897, 0.36453201838314825, 0.38587848810335296, 0.3809523797289687, 0.37438423562128165, 0.379310343506301, 0.38587848820122594, 0.3678160909263567, 0.38095237963109574, 0.38916256035294244, 0.35303776560745803, 0.37274219930074093, 0.3760262715503304, 0.3711001628823273, 0.3727421993986139, 0.3711001629802002, 0.36945812714902443, 0.35796387407971525, 0.3793103437020469, 0.37274219920286794, 0.36288998255197247, 0.3546798017322528, 0.35467980183012576, 0.3760262717460764, 0.3513957294826633, 0.36453201828527526, 0.3645320185788942, 0.36945812685540547, 0.379310343408428, 0.35632183756342856, 0.3645320186767672, 0.3793103438977929, 0.3678160909263567, 0.3809523801204606, 0.38095238002258763, 0.36945812695327845, 0.37766830777299815, 0.3678160908284837, 0.3875205245217666, 0.3711001630780732, 0.38095237982684166, 0.36617405480156195, 0.3825944159516364, 0.3711001630780732, 0.3645320186767672, 0.37274219910499495, 0.3612479460356858, 0.3612479460356858, 0.37602627194182237, 0.35632183775917453, 0.34811165693945484, 0.36945812695327845, 0.36617405489943494, 0.36617405441007006, 0.37110016337169216, 0.3711001632738192, 0.3694581273447704, 0.35303776502022016, 0.36781609033911883, 0.3628899821604805, 0.3513957289932984, 0.3645320186767672, 0.3628899821604805, 0.37438423552340866, 0.36945812656178656, 0.3579638743733342, 0.35139572909117134, 0.36288998255197247, 0.3678160907306107, 0.37602627096309255, 0.3563218378570475, 0.37602627106096553, 0.36453201848102124, 0.37766830767512516, 0.3694581267575325, 0.36288998274771844, 0.3431855482714517, 0.36945812695327845, 0.37274219920286794, 0.3678160909263567, 0.34482758459199236, 0.37766830777299815, 0.366174054605816, 0.36617405470368897, 0.34482758439624644, 0.3645320187746402, 0.3546798012428879], 'loss': [1.6108599623125925, 1.6061095039457756, 1.605656777612972, 1.6058919154643034, 1.6055213121418102, 1.6051988438169569, 1.604884234786768, 1.6046833167085903, 1.6042218467293334, 1.603328887246228, 1.60208499490358, 1.6001058830128068, 1.5966431326445123, 1.591029437366697, 1.5819864804250259, 1.5683037507705375, 1.5494785565370406, 1.5247087025789265, 1.5009605932529457, 1.4854766227870997, 1.4873745720489313, 1.4800897812696452, 1.4732554779894789, 1.470663953855542, 1.4685714799031095, 1.465052866103468, 1.4625514301676035, 1.460274586883169, 1.4593543877102266, 1.4568741384228152, 1.458824027735105, 1.4561857656776538, 1.4536251680072574, 1.4568890250928592, 1.4517217922504433, 1.4535000605260078, 1.4508741517820887, 1.4527821585383014, 1.4504027265298047, 1.449992136641939, 1.4519117949679647, 1.45013111323057, 1.4483084184188373, 1.4464226063272057, 1.4450467698627918, 1.44419938813, 1.4446975781442692, 1.4429100655432354, 1.4447412472241223, 1.4454383852055919, 1.4462253874577047, 1.4450037391768344, 1.4434833544725265, 1.4416076500068211, 1.4456569200668492, 1.448413089021765, 1.4432174188155658, 1.4400274007961735, 1.4392284681664844, 1.437582509571522, 1.4382555901392284, 1.440549755145392, 1.4397397531376237, 1.4350587779246806, 1.4349094508854516, 1.4378747661990061, 1.4364504375986495, 1.434388433687496, 1.4323675331393797, 1.4353458035408349, 1.4324408980610435, 1.4316199405482173, 1.4309148782087793, 1.4296221479008575, 1.4286429822322524, 1.428985682503154, 1.4297315430102653, 1.4301193406204913, 1.4306573728271579, 1.4278599723408598, 1.4291493594034497, 1.4300681690905372, 1.4429366889185975, 1.4283093421610964, 1.4264905025826833, 1.4264155658608344, 1.4262204951084614, 1.4257492983855262, 1.4243715262266154, 1.4236399520349208, 1.423223292215649, 1.4245883738480554, 1.4254469450494347, 1.4349730014801025, 1.4268262713841589, 1.4259746560815423, 1.421270988364484, 1.4208385492986721, 1.4191959680228263, 1.4203071140900285, 1.4217483898452665, 1.430432170862045, 1.4399395981853258, 1.4279831975392492, 1.424719565751861, 1.42086874121758, 1.41934826961533, 1.4176207098872755, 1.4174317839209305, 1.4170962119249348, 1.417586729609746, 1.416392656958813, 1.4189683889706277, 1.4182888610651851, 1.417610602310306, 1.4176052695182315, 1.4162021485687037, 1.4179298713711497, 1.4194703400257431, 1.4196724448115918, 1.4194674542797174, 1.4232002589736876, 1.4213411317468914, 1.4178026704083233, 1.417780150672004, 1.412972156418912, 1.414299947965806, 1.4138574937530612, 1.4122547010621496, 1.4115544030308969, 1.4122011906802043, 1.4191929933716385, 1.4173671645550268, 1.4192878952261359, 1.4140535003105963, 1.4160326434846287, 1.4114149775103622, 1.410302730948039, 1.4115883414015877, 1.4108725463585197, 1.411636335159474, 1.4089610630481884, 1.4129170248885419, 1.4138661120216949, 1.4118755420375408, 1.4140578971995954, 1.4102606989275015, 1.4117935397541743, 1.409199293242343, 1.408297145146364, 1.408626884160835, 1.4072919282580303, 1.410641674584187, 1.4100532021121077, 1.4141452205744123, 1.4184488184887771, 1.4079990376192442, 1.4169052563164024, 1.4123067195410601, 1.411793394451024, 1.4071896135439863, 1.405805760340524, 1.4073512417090257, 1.40733203550139, 1.404567905764805, 1.4047565581617414, 1.4048085832987478, 1.4046312836406167, 1.4091083742999444, 1.406355435304818, 1.4049108665826628, 1.4037053716745709, 1.403383938487795, 1.4038090693632435, 1.4067983673828093, 1.4082823262811932, 1.4119930472462083, 1.4113368989505808, 1.417618731943244, 1.4066219469360257, 1.4034111284377393, 1.4084086424516211, 1.4043388557140342, 1.4035759147922116, 1.4023170030582122, 1.4024609358158935, 1.4004899375492543, 1.4021535758854673, 1.4055590211488382, 1.4041051563051448, 1.4037864653236811, 1.401975115026047, 1.4031925498092934, 1.3991364389963954, 1.3997671175786357, 1.40138920445217, 1.4034366012353916, 1.4002928615350743, 1.4011952456507595, 1.3997758403695828, 1.4007452210851274, 1.4074803849754882, 1.399977120139026, 1.406714147463961, 1.3983981474713867, 1.4050996573309145, 1.4024317338481331, 1.4038364367318594, 1.3995750902124988, 1.396655081968288, 1.3985432688460457, 1.3977833262214425, 1.3983557061248246, 1.3973599553352998, 1.4003296690065514, 1.3965492712154035, 1.3969041927149652, 1.3978532746097636, 1.3928150122660141, 1.3975492971388956, 1.398527031121068, 1.396261919842119, 1.3953803690062412, 1.4001186175023261, 1.3988595606610026, 1.4002532619715227, 1.3948406151922332, 1.3960333985224886, 1.3925947719530893, 1.3924146234621992, 1.3941238795462576, 1.3985701793028344, 1.399506687971111, 1.3991236506056737, 1.4011019255346342, 1.4021370019266493, 1.3979043292803441, 1.392262824164279, 1.3958131433267613, 1.3935402036447544, 1.3911576977011115, 1.3979527211531966, 1.4007031638519476, 1.401038805121514, 1.3954330570154367, 1.3871875864279588, 1.3873925344655156, 1.3883289973593835, 1.3947377723345278, 1.392411118415347, 1.3928130396582508, 1.390835446009156, 1.3826151770977513, 1.3835495425935154, 1.3842596002183165, 1.3849594782999655, 1.3790834570322683, 1.3788458839334257, 1.3796631404261814, 1.3775868565639675, 1.382244692837678, 1.3756353829186065, 1.3781999432820313, 1.3782722157864111, 1.3780641993947587, 1.3733642519866662, 1.3717988045553406, 1.37423629261385, 1.3780982750886763, 1.3803202521384863, 1.3752504354629673, 1.3852907973393278, 1.3804949021682114, 1.3938073255687768, 1.3917788966235685, 1.3860806952266966, 1.374040078431429, 1.3738230523632293, 1.377209993603293, 1.370078244689064, 1.3673910112106824, 1.3686548506699547, 1.3704332169566067, 1.3689375350117927, 1.36732920449862, 1.3630383549774452, 1.361298665128939, 1.3648245040396156, 1.3683340427077526, 1.376094069324235, 1.3687257095039258, 1.3626088646647867, 1.3685531109020694, 1.36586988696817, 1.3660996463998878, 1.3641978863083606, 1.368889922380937, 1.3640535681154693, 1.3660862152581341, 1.3585846187642467], 'acc': [0.19301848095178115, 0.2328542098861945, 0.23285421029620593, 0.23285420851541005, 0.23285420951290053, 0.23285421011873828, 0.2328542087112364, 0.23285421029620593, 0.23285420833794243, 0.23285421051039099, 0.2328542091028891, 0.2328542087112364, 0.23531827340380612, 0.24681724987969997, 0.2607802892857264, 0.2751540033907861, 0.30143737069635174, 0.3108829590451791, 0.31581108869957975, 0.32156057711499425, 0.3215605759400362, 0.3264887057902632, 0.3330595488543383, 0.33634497077802855, 0.3363449709738549, 0.34332649067686816, 0.3408624214068575, 0.3466119106422949, 0.344969200365842, 0.3425051326624422, 0.35195071670308986, 0.3527720762841266, 0.35564681839893975, 0.3498973327618115, 0.34989732900439346, 0.34414784512970237, 0.34496919899505757, 0.35523614140506643, 0.3556468192189626, 0.3552361396059118, 0.35441478534890397, 0.3618069803200708, 0.3568788479241013, 0.3478439416973498, 0.34702258786871204, 0.3523613984702304, 0.3572895273046082, 0.36262833594786315, 0.35811088489066406, 0.3515400434299171, 0.3498973311952007, 0.3650924024395874, 0.3585215587880332, 0.35770020691765897, 0.3449691989583401, 0.3531827509280837, 0.354004105968397, 0.3618069812992025, 0.36550308005765725, 0.354825461400363, 0.3577002071134853, 0.3593429165699154, 0.3630390121951485, 0.363039014153412, 0.36632443552634064, 0.3552361382351274, 0.36427104877005856, 0.3548254610087103, 0.3622176583297933, 0.36714578994245745, 0.3646817234507332, 0.36796714815270976, 0.3663244343146651, 0.3646817248215176, 0.36550307786684993, 0.3679671436119863, 0.3622176593089251, 0.36303901255008375, 0.3593429177448735, 0.3708418896800439, 0.36427104622431605, 0.3655030808409626, 0.35852155898385957, 0.36016427121857597, 0.3687884996314313, 0.36550307966600454, 0.36509240381037183, 0.3712525676897664, 0.3655030814284417, 0.36919918002778745, 0.37289527914117737, 0.36386036978120434, 0.3728952787862421, 0.36550308205263815, 0.3704312112786687, 0.3716632441328781, 0.3745379884017811, 0.3696098562383554, 0.37125256886472446, 0.3749486633149995, 0.3737166348056872, 0.3671457920965473, 0.37207392413758156, 0.3761806963650353, 0.368788499435605, 0.3745379878510195, 0.36878850139386843, 0.37782340660477076, 0.3753593432829855, 0.37248459936901773, 0.37084188807671564, 0.37987679590679535, 0.3774127303942028, 0.37002053444390426, 0.3757700224676661, 0.37535934426211726, 0.3786447646191967, 0.3700205318981617, 0.37659137378727875, 0.37494866448995756, 0.3749486656649157, 0.3786447626609332, 0.3782340858261688, 0.37207392390503774, 0.37166324632368536, 0.3815195051673991, 0.3753593432829855, 0.3720739237092114, 0.3745379868718878, 0.37412730905799163, 0.38275154158320024, 0.3770020514053486, 0.3761806961324915, 0.37494866762317913, 0.37700205516276664, 0.3597535952038344, 0.37535934230385376, 0.3774127303942028, 0.373305954409331, 0.37248460015232315, 0.37905544027900306, 0.36960985463502716, 0.3823408614193879, 0.37002053561886233, 0.36837782537912683, 0.3757700219169045, 0.37289527737874023, 0.3733059549968101, 0.3753593417163747, 0.37618069734416704, 0.37166324276209367, 0.38480492873113503, 0.3741273116404516, 0.37207392273007966, 0.3716632445245308, 0.3741273086663389, 0.3700205346397306, 0.3700205322898144, 0.373305954409331, 0.37659137770380574, 0.37905544282474557, 0.37248460132728123, 0.3741273086663389, 0.3753593436746382, 0.3753593432829855, 0.3716632453078362, 0.3741273112487989, 0.3704312112786687, 0.37741273156916094, 0.3806981525504369, 0.3712525657315029, 0.37946611691794113, 0.38316221642298376, 0.37494866448995756, 0.3774127313733346, 0.3765913743747578, 0.37371663022824625, 0.38275154240322307, 0.3728952763996085, 0.37905544106230843, 0.3815195053999429, 0.37330595264689387, 0.38069815274626323, 0.3745379880835633, 0.36919917901193827, 0.3757700225043835, 0.37535934387046455, 0.3761806959733826, 0.3864476387750442, 0.37987679629844806, 0.37823408523868973, 0.3737166314032043, 0.3774127287908746, 0.38069815368867754, 0.3770020533268946, 0.37905543988735035, 0.3798767975101236, 0.3716632441328781, 0.3786447657941548, 0.38480492928189663, 0.3782340844186669, 0.37577002227183975, 0.3864476367800632, 0.3749486650774366, 0.38028747571567245, 0.3774127303942028, 0.37248459737403683, 0.3786447657941548, 0.37535934191220105, 0.3770020543427438, 0.3811088315025737, 0.3765913755497159, 0.37823408641364786, 0.38357289724771004, 0.3782340858261688, 0.37494866390247855, 0.3786447638358913, 0.3798767972775798, 0.37905544145396114, 0.3778234068005971, 0.38275154236650566, 0.3831622166555275, 0.38316221841796466, 0.374537989809283, 0.38398357287079893, 0.3790554416497875, 0.3782340870011269, 0.3806981529053721, 0.38234086200686696, 0.3815195087289908, 0.37864476403171765, 0.38891170565842115, 0.38850102569043515, 0.3864476372084334, 0.37618069734416704, 0.3897330585446446, 0.3864476363884105, 0.3815195049715728, 0.3860369599452988, 0.3839835716958408, 0.38562628173974994, 0.38110883013178926, 0.3905544149190249, 0.3864476385792178, 0.3860369623319324, 0.38644764053748126, 0.3839835726382551, 0.39342915765803455, 0.38891170605007386, 0.39383983707525894, 0.3835728970151662, 0.3876796726083853, 0.3942505146933287, 0.40041067974768135, 0.39055441515156863, 0.39548254793919085, 0.4016427098236045, 0.4036961001047608, 0.4053388101853874, 0.40862422815583327, 0.40328541951257835, 0.40369609932145545, 0.3987679688837494, 0.3983572916573323, 0.3983572902865478, 0.406981520853493, 0.4041067771353516, 0.39630390141289334, 0.39671458138087934, 0.3995893205582973, 0.40862422878002974, 0.3897330589362973, 0.3958932245781289, 0.3934291558955974, 0.3843942510763478, 0.39014373910010963, 0.39589322379482356, 0.39630390141289334, 0.3958932232073445, 0.3987679677087913, 0.40287474111120314, 0.4078028751105009, 0.4053388097937347, 0.40451745573255316, 0.40533880681962203, 0.40862423030992306, 0.4028747446727948, 0.3938398342969726, 0.4073921986673892, 0.39835728750826155, 0.3921971271904587, 0.41232032773431077, 0.40246406505974414, 0.40616016264324073, 0.39753593524623454, 0.4057494858084763, 0.40123203083475023, 0.3979466098901917, 0.41067762000360036, 0.4123203267184616]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
