{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf30.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 06:56:31 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '1', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000022302234E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000022369566EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1002, Accuracy:0.3261, Validation Loss:1.0854, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0822, Accuracy:0.3943, Validation Loss:1.0783, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0768, Accuracy:0.3943, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0743, Accuracy:0.3967, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #5: Loss:1.0743, Accuracy:0.3959, Validation Loss:1.0757, Validation Accuracy:0.3678\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3938, Validation Loss:1.0756, Validation Accuracy:0.3760\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.3908\n",
    "Epoch #8: Loss:1.0742, Accuracy:0.3979, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #9: Loss:1.0740, Accuracy:0.3955, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #10: Loss:1.0740, Accuracy:0.3992, Validation Loss:1.0750, Validation Accuracy:0.3859\n",
    "Epoch #11: Loss:1.0738, Accuracy:0.4000, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #12: Loss:1.0737, Accuracy:0.4025, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #13: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #14: Loss:1.0737, Accuracy:0.4016, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #15: Loss:1.0736, Accuracy:0.4045, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #16: Loss:1.0735, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #17: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #18: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #19: Loss:1.0733, Accuracy:0.4033, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #20: Loss:1.0732, Accuracy:0.4045, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #21: Loss:1.0731, Accuracy:0.4045, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #22: Loss:1.0729, Accuracy:0.4025, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #23: Loss:1.0729, Accuracy:0.4049, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #24: Loss:1.0729, Accuracy:0.4041, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #25: Loss:1.0732, Accuracy:0.4045, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #26: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #27: Loss:1.0732, Accuracy:0.4033, Validation Loss:1.0753, Validation Accuracy:0.3777\n",
    "Epoch #28: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0757, Validation Accuracy:0.3777\n",
    "Epoch #29: Loss:1.0738, Accuracy:0.4029, Validation Loss:1.0750, Validation Accuracy:0.3777\n",
    "Epoch #30: Loss:1.0734, Accuracy:0.4066, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #31: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #32: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0754, Validation Accuracy:0.3793\n",
    "Epoch #33: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0755, Validation Accuracy:0.3924\n",
    "Epoch #34: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #35: Loss:1.0731, Accuracy:0.3934, Validation Loss:1.0751, Validation Accuracy:0.3777\n",
    "Epoch #36: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #37: Loss:1.0730, Accuracy:0.3996, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #38: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0753, Validation Accuracy:0.3892\n",
    "Epoch #39: Loss:1.0731, Accuracy:0.3947, Validation Loss:1.0756, Validation Accuracy:0.3744\n",
    "Epoch #40: Loss:1.0727, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #41: Loss:1.0727, Accuracy:0.4012, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #42: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #43: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #44: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #45: Loss:1.0729, Accuracy:0.4029, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #46: Loss:1.0720, Accuracy:0.4037, Validation Loss:1.0751, Validation Accuracy:0.3744\n",
    "Epoch #47: Loss:1.0727, Accuracy:0.4037, Validation Loss:1.0754, Validation Accuracy:0.3760\n",
    "Epoch #48: Loss:1.0727, Accuracy:0.4025, Validation Loss:1.0755, Validation Accuracy:0.3760\n",
    "Epoch #49: Loss:1.0725, Accuracy:0.4000, Validation Loss:1.0754, Validation Accuracy:0.3793\n",
    "Epoch #50: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #51: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #52: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #53: Loss:1.0724, Accuracy:0.4053, Validation Loss:1.0755, Validation Accuracy:0.3777\n",
    "Epoch #54: Loss:1.0721, Accuracy:0.4057, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #55: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0777, Validation Accuracy:0.3695\n",
    "Epoch #56: Loss:1.0726, Accuracy:0.3979, Validation Loss:1.0763, Validation Accuracy:0.3810\n",
    "Epoch #57: Loss:1.0729, Accuracy:0.3947, Validation Loss:1.0757, Validation Accuracy:0.3810\n",
    "Epoch #58: Loss:1.0723, Accuracy:0.4016, Validation Loss:1.0756, Validation Accuracy:0.3793\n",
    "Epoch #59: Loss:1.0723, Accuracy:0.4025, Validation Loss:1.0762, Validation Accuracy:0.3826\n",
    "Epoch #60: Loss:1.0725, Accuracy:0.4000, Validation Loss:1.0756, Validation Accuracy:0.3744\n",
    "Epoch #61: Loss:1.0725, Accuracy:0.3979, Validation Loss:1.0753, Validation Accuracy:0.3711\n",
    "Epoch #62: Loss:1.0725, Accuracy:0.3988, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #63: Loss:1.0726, Accuracy:0.4004, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #64: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #65: Loss:1.0725, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3760\n",
    "Epoch #66: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #67: Loss:1.0726, Accuracy:0.4000, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #68: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #69: Loss:1.0720, Accuracy:0.3996, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #70: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3875\n",
    "Epoch #71: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3859\n",
    "Epoch #72: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #73: Loss:1.0719, Accuracy:0.3996, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #74: Loss:1.0719, Accuracy:0.3975, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #75: Loss:1.0723, Accuracy:0.4012, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #76: Loss:1.0721, Accuracy:0.4016, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #77: Loss:1.0720, Accuracy:0.4000, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #78: Loss:1.0720, Accuracy:0.3992, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0718, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #80: Loss:1.0714, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #81: Loss:1.0721, Accuracy:0.4033, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #82: Loss:1.0725, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3826\n",
    "Epoch #83: Loss:1.0723, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #84: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #85: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #86: Loss:1.0719, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #87: Loss:1.0721, Accuracy:0.4029, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #88: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0751, Validation Accuracy:0.3793\n",
    "Epoch #89: Loss:1.0720, Accuracy:0.4041, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #90: Loss:1.0718, Accuracy:0.4029, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #91: Loss:1.0714, Accuracy:0.4057, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #92: Loss:1.0711, Accuracy:0.4057, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #93: Loss:1.0713, Accuracy:0.4062, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #94: Loss:1.0710, Accuracy:0.4070, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #95: Loss:1.0708, Accuracy:0.4057, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #96: Loss:1.0720, Accuracy:0.4045, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #97: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0739, Validation Accuracy:0.3793\n",
    "Epoch #98: Loss:1.0716, Accuracy:0.4041, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #99: Loss:1.0715, Accuracy:0.4025, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #100: Loss:1.0712, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #101: Loss:1.0715, Accuracy:0.3984, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #102: Loss:1.0710, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #103: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #104: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #105: Loss:1.0724, Accuracy:0.3926, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #106: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #107: Loss:1.0717, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #108: Loss:1.0709, Accuracy:0.4045, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #109: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #110: Loss:1.0710, Accuracy:0.4004, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #111: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #112: Loss:1.0716, Accuracy:0.4070, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #113: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #114: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #115: Loss:1.0706, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #116: Loss:1.0712, Accuracy:0.4008, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #117: Loss:1.0710, Accuracy:0.4021, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #118: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #119: Loss:1.0704, Accuracy:0.4029, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #120: Loss:1.0712, Accuracy:0.4000, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #121: Loss:1.0711, Accuracy:0.4090, Validation Loss:1.0756, Validation Accuracy:0.3826\n",
    "Epoch #122: Loss:1.0713, Accuracy:0.4037, Validation Loss:1.0752, Validation Accuracy:0.3990\n",
    "Epoch #123: Loss:1.0708, Accuracy:0.4029, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #124: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #125: Loss:1.0707, Accuracy:0.4033, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #126: Loss:1.0707, Accuracy:0.4004, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #127: Loss:1.0711, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #128: Loss:1.0713, Accuracy:0.4037, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #129: Loss:1.0714, Accuracy:0.4008, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #130: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #131: Loss:1.0710, Accuracy:0.4057, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #132: Loss:1.0713, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #133: Loss:1.0715, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #134: Loss:1.0711, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #135: Loss:1.0706, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #136: Loss:1.0709, Accuracy:0.4057, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #137: Loss:1.0708, Accuracy:0.4021, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #138: Loss:1.0711, Accuracy:0.4025, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #139: Loss:1.0712, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #140: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #141: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0760, Validation Accuracy:0.3908\n",
    "Epoch #142: Loss:1.0719, Accuracy:0.4037, Validation Loss:1.0757, Validation Accuracy:0.3875\n",
    "Epoch #143: Loss:1.0717, Accuracy:0.4021, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #144: Loss:1.0711, Accuracy:0.4016, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #145: Loss:1.0708, Accuracy:0.4000, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #146: Loss:1.0705, Accuracy:0.4004, Validation Loss:1.0725, Validation Accuracy:0.3924\n",
    "Epoch #147: Loss:1.0706, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #148: Loss:1.0708, Accuracy:0.4070, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #149: Loss:1.0705, Accuracy:0.4025, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #150: Loss:1.0707, Accuracy:0.3996, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #151: Loss:1.0706, Accuracy:0.4057, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #152: Loss:1.0701, Accuracy:0.4053, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #153: Loss:1.0703, Accuracy:0.4037, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #154: Loss:1.0703, Accuracy:0.4037, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #155: Loss:1.0701, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #156: Loss:1.0702, Accuracy:0.3967, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #157: Loss:1.0700, Accuracy:0.4025, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #158: Loss:1.0700, Accuracy:0.4025, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #159: Loss:1.0703, Accuracy:0.4045, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #160: Loss:1.0698, Accuracy:0.3996, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #161: Loss:1.0701, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #162: Loss:1.0700, Accuracy:0.4045, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #163: Loss:1.0699, Accuracy:0.4057, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #164: Loss:1.0703, Accuracy:0.4037, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #165: Loss:1.0702, Accuracy:0.4066, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #166: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #167: Loss:1.0702, Accuracy:0.4037, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #168: Loss:1.0701, Accuracy:0.4037, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #169: Loss:1.0700, Accuracy:0.4049, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #170: Loss:1.0698, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #171: Loss:1.0697, Accuracy:0.4057, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #172: Loss:1.0701, Accuracy:0.4025, Validation Loss:1.0737, Validation Accuracy:0.4023\n",
    "Epoch #173: Loss:1.0696, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #174: Loss:1.0706, Accuracy:0.3996, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #175: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0744, Validation Accuracy:0.4023\n",
    "Epoch #176: Loss:1.0707, Accuracy:0.4041, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #177: Loss:1.0702, Accuracy:0.4029, Validation Loss:1.0732, Validation Accuracy:0.3842\n",
    "Epoch #178: Loss:1.0701, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.3892\n",
    "Epoch #179: Loss:1.0702, Accuracy:0.4025, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #180: Loss:1.0705, Accuracy:0.3975, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #181: Loss:1.0701, Accuracy:0.4025, Validation Loss:1.0737, Validation Accuracy:0.3826\n",
    "Epoch #182: Loss:1.0706, Accuracy:0.3963, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #183: Loss:1.0696, Accuracy:0.4045, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #184: Loss:1.0699, Accuracy:0.4041, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #185: Loss:1.0697, Accuracy:0.4012, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #186: Loss:1.0701, Accuracy:0.4041, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #187: Loss:1.0694, Accuracy:0.4045, Validation Loss:1.0739, Validation Accuracy:0.3826\n",
    "Epoch #188: Loss:1.0695, Accuracy:0.4049, Validation Loss:1.0739, Validation Accuracy:0.3826\n",
    "Epoch #189: Loss:1.0694, Accuracy:0.4000, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #190: Loss:1.0694, Accuracy:0.4037, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #191: Loss:1.0696, Accuracy:0.3996, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #192: Loss:1.0693, Accuracy:0.4037, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #193: Loss:1.0699, Accuracy:0.4021, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #194: Loss:1.0691, Accuracy:0.3996, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #195: Loss:1.0698, Accuracy:0.4045, Validation Loss:1.0742, Validation Accuracy:0.3826\n",
    "Epoch #196: Loss:1.0693, Accuracy:0.3984, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #197: Loss:1.0696, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3826\n",
    "Epoch #198: Loss:1.0707, Accuracy:0.3996, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #199: Loss:1.0690, Accuracy:0.4062, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #200: Loss:1.0700, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #201: Loss:1.0696, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3826\n",
    "Epoch #202: Loss:1.0690, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #203: Loss:1.0696, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #204: Loss:1.0698, Accuracy:0.4045, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #205: Loss:1.0696, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #206: Loss:1.0692, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #207: Loss:1.0691, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #208: Loss:1.0690, Accuracy:0.4033, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #209: Loss:1.0691, Accuracy:0.3996, Validation Loss:1.0739, Validation Accuracy:0.3826\n",
    "Epoch #210: Loss:1.0688, Accuracy:0.4049, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #211: Loss:1.0695, Accuracy:0.4000, Validation Loss:1.0733, Validation Accuracy:0.3826\n",
    "Epoch #212: Loss:1.0695, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3777\n",
    "Epoch #213: Loss:1.0697, Accuracy:0.3996, Validation Loss:1.0730, Validation Accuracy:0.4007\n",
    "Epoch #214: Loss:1.0699, Accuracy:0.4016, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #215: Loss:1.0697, Accuracy:0.4033, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #216: Loss:1.0698, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.3793\n",
    "Epoch #217: Loss:1.0698, Accuracy:0.4000, Validation Loss:1.0732, Validation Accuracy:0.3793\n",
    "Epoch #218: Loss:1.0693, Accuracy:0.3996, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #219: Loss:1.0686, Accuracy:0.4041, Validation Loss:1.0718, Validation Accuracy:0.3859\n",
    "Epoch #220: Loss:1.0700, Accuracy:0.3992, Validation Loss:1.0708, Validation Accuracy:0.3908\n",
    "Epoch #221: Loss:1.0704, Accuracy:0.4025, Validation Loss:1.0721, Validation Accuracy:0.3859\n",
    "Epoch #222: Loss:1.0699, Accuracy:0.4025, Validation Loss:1.0724, Validation Accuracy:0.3842\n",
    "Epoch #223: Loss:1.0701, Accuracy:0.4016, Validation Loss:1.0722, Validation Accuracy:0.3826\n",
    "Epoch #224: Loss:1.0704, Accuracy:0.4012, Validation Loss:1.0694, Validation Accuracy:0.3875\n",
    "Epoch #225: Loss:1.0693, Accuracy:0.4021, Validation Loss:1.0721, Validation Accuracy:0.3744\n",
    "Epoch #226: Loss:1.0697, Accuracy:0.4053, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #227: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #228: Loss:1.0708, Accuracy:0.3996, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #229: Loss:1.0710, Accuracy:0.4057, Validation Loss:1.0736, Validation Accuracy:0.3777\n",
    "Epoch #230: Loss:1.0696, Accuracy:0.4000, Validation Loss:1.0759, Validation Accuracy:0.3810\n",
    "Epoch #231: Loss:1.0716, Accuracy:0.4000, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #232: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #233: Loss:1.0707, Accuracy:0.3959, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #234: Loss:1.0708, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #235: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3793\n",
    "Epoch #236: Loss:1.0705, Accuracy:0.3979, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #237: Loss:1.0704, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #238: Loss:1.0713, Accuracy:0.4029, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #239: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0765, Validation Accuracy:0.3777\n",
    "Epoch #240: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #241: Loss:1.0707, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #242: Loss:1.0703, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #243: Loss:1.0703, Accuracy:0.3984, Validation Loss:1.0743, Validation Accuracy:0.3810\n",
    "Epoch #244: Loss:1.0704, Accuracy:0.3975, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #245: Loss:1.0701, Accuracy:0.4025, Validation Loss:1.0747, Validation Accuracy:0.3777\n",
    "Epoch #246: Loss:1.0704, Accuracy:0.3971, Validation Loss:1.0747, Validation Accuracy:0.3760\n",
    "Epoch #247: Loss:1.0702, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #248: Loss:1.0703, Accuracy:0.4021, Validation Loss:1.0740, Validation Accuracy:0.3810\n",
    "Epoch #249: Loss:1.0702, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.3760\n",
    "Epoch #250: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #251: Loss:1.0701, Accuracy:0.4004, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #252: Loss:1.0702, Accuracy:0.4000, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #253: Loss:1.0707, Accuracy:0.3996, Validation Loss:1.0745, Validation Accuracy:0.3760\n",
    "Epoch #254: Loss:1.0702, Accuracy:0.4021, Validation Loss:1.0749, Validation Accuracy:0.3744\n",
    "Epoch #255: Loss:1.0701, Accuracy:0.4021, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #256: Loss:1.0703, Accuracy:0.4041, Validation Loss:1.0742, Validation Accuracy:0.3793\n",
    "Epoch #257: Loss:1.0700, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.3744\n",
    "Epoch #258: Loss:1.0698, Accuracy:0.4037, Validation Loss:1.0749, Validation Accuracy:0.3777\n",
    "Epoch #259: Loss:1.0697, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.3777\n",
    "Epoch #260: Loss:1.0700, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3711\n",
    "Epoch #261: Loss:1.0703, Accuracy:0.4025, Validation Loss:1.0745, Validation Accuracy:0.3727\n",
    "Epoch #262: Loss:1.0697, Accuracy:0.4053, Validation Loss:1.0745, Validation Accuracy:0.3727\n",
    "Epoch #263: Loss:1.0702, Accuracy:0.3971, Validation Loss:1.0748, Validation Accuracy:0.3777\n",
    "Epoch #264: Loss:1.0700, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3711\n",
    "Epoch #265: Loss:1.0691, Accuracy:0.4078, Validation Loss:1.0727, Validation Accuracy:0.3727\n",
    "Epoch #266: Loss:1.0695, Accuracy:0.4057, Validation Loss:1.0735, Validation Accuracy:0.3793\n",
    "Epoch #267: Loss:1.0692, Accuracy:0.4082, Validation Loss:1.0735, Validation Accuracy:0.3727\n",
    "Epoch #268: Loss:1.0692, Accuracy:0.4086, Validation Loss:1.0734, Validation Accuracy:0.3711\n",
    "Epoch #269: Loss:1.0693, Accuracy:0.4082, Validation Loss:1.0738, Validation Accuracy:0.3711\n",
    "Epoch #270: Loss:1.0693, Accuracy:0.4103, Validation Loss:1.0734, Validation Accuracy:0.3793\n",
    "Epoch #271: Loss:1.0698, Accuracy:0.4049, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #272: Loss:1.0693, Accuracy:0.4053, Validation Loss:1.0740, Validation Accuracy:0.3744\n",
    "Epoch #273: Loss:1.0693, Accuracy:0.4074, Validation Loss:1.0733, Validation Accuracy:0.3727\n",
    "Epoch #274: Loss:1.0697, Accuracy:0.4074, Validation Loss:1.0732, Validation Accuracy:0.3727\n",
    "Epoch #275: Loss:1.0688, Accuracy:0.4086, Validation Loss:1.0730, Validation Accuracy:0.3793\n",
    "Epoch #276: Loss:1.0696, Accuracy:0.4057, Validation Loss:1.0734, Validation Accuracy:0.3744\n",
    "Epoch #277: Loss:1.0690, Accuracy:0.4094, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #278: Loss:1.0697, Accuracy:0.4053, Validation Loss:1.0731, Validation Accuracy:0.3793\n",
    "Epoch #279: Loss:1.0692, Accuracy:0.4074, Validation Loss:1.0730, Validation Accuracy:0.3744\n",
    "Epoch #280: Loss:1.0691, Accuracy:0.4082, Validation Loss:1.0729, Validation Accuracy:0.3744\n",
    "Epoch #281: Loss:1.0690, Accuracy:0.4078, Validation Loss:1.0730, Validation Accuracy:0.3744\n",
    "Epoch #282: Loss:1.0694, Accuracy:0.4078, Validation Loss:1.0727, Validation Accuracy:0.3727\n",
    "Epoch #283: Loss:1.0688, Accuracy:0.4049, Validation Loss:1.0727, Validation Accuracy:0.3793\n",
    "Epoch #284: Loss:1.0689, Accuracy:0.4099, Validation Loss:1.0727, Validation Accuracy:0.3727\n",
    "Epoch #285: Loss:1.0685, Accuracy:0.4082, Validation Loss:1.0729, Validation Accuracy:0.3793\n",
    "Epoch #286: Loss:1.0687, Accuracy:0.4045, Validation Loss:1.0727, Validation Accuracy:0.3744\n",
    "Epoch #287: Loss:1.0684, Accuracy:0.4082, Validation Loss:1.0728, Validation Accuracy:0.3711\n",
    "Epoch #288: Loss:1.0686, Accuracy:0.4049, Validation Loss:1.0725, Validation Accuracy:0.3793\n",
    "Epoch #289: Loss:1.0688, Accuracy:0.4082, Validation Loss:1.0729, Validation Accuracy:0.3744\n",
    "Epoch #290: Loss:1.0684, Accuracy:0.4082, Validation Loss:1.0726, Validation Accuracy:0.3727\n",
    "Epoch #291: Loss:1.0684, Accuracy:0.4086, Validation Loss:1.0727, Validation Accuracy:0.3727\n",
    "Epoch #292: Loss:1.0682, Accuracy:0.4078, Validation Loss:1.0726, Validation Accuracy:0.3744\n",
    "Epoch #293: Loss:1.0685, Accuracy:0.4078, Validation Loss:1.0723, Validation Accuracy:0.3793\n",
    "Epoch #294: Loss:1.0693, Accuracy:0.4070, Validation Loss:1.0725, Validation Accuracy:0.3793\n",
    "Epoch #295: Loss:1.0681, Accuracy:0.4053, Validation Loss:1.0730, Validation Accuracy:0.3744\n",
    "Epoch #296: Loss:1.0695, Accuracy:0.4025, Validation Loss:1.0726, Validation Accuracy:0.3793\n",
    "Epoch #297: Loss:1.0695, Accuracy:0.4057, Validation Loss:1.0728, Validation Accuracy:0.3744\n",
    "Epoch #298: Loss:1.0688, Accuracy:0.4066, Validation Loss:1.0729, Validation Accuracy:0.3744\n",
    "Epoch #299: Loss:1.0688, Accuracy:0.4066, Validation Loss:1.0723, Validation Accuracy:0.3727\n",
    "Epoch #300: Loss:1.0687, Accuracy:0.4070, Validation Loss:1.0725, Validation Accuracy:0.3727\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07246542, Accuracy:0.3727\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03   01  02\n",
    "t:03   0  120  22\n",
    "t:01   0  182  58\n",
    "t:02   0  182  45\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.38      0.76      0.50       240\n",
    "          02       0.36      0.20      0.26       227\n",
    "\n",
    "    accuracy                           0.37       609\n",
    "   macro avg       0.25      0.32      0.25       609\n",
    "weighted avg       0.28      0.37      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 07:12:17 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 46 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0854437392333458, 1.0782932279928172, 1.0753614393556843, 1.0752561797062163, 1.0757205314041163, 1.0755872859547682, 1.075201124001802, 1.074920117757199, 1.0750639358373306, 1.0749986222616361, 1.0748048515742636, 1.0748523313227938, 1.0747988304285385, 1.074877240387677, 1.0748748620742647, 1.0750613827227764, 1.0748264028129515, 1.074903447639766, 1.0748470684969171, 1.0748996020146386, 1.0747664309487555, 1.0750324164313831, 1.0751412284785304, 1.0750408117798553, 1.0755181684478359, 1.0751885774687593, 1.0753285959240642, 1.0756530932017736, 1.0750171006802463, 1.0754373239961947, 1.0756107490442461, 1.0754459867336479, 1.0754744094385107, 1.0752670046535424, 1.075126344542981, 1.0752333249951818, 1.075172538044809, 1.075283018630518, 1.075588911037727, 1.0752666172722878, 1.0754105328339074, 1.0751762513456673, 1.075106656218593, 1.0752979879113058, 1.0745594979115503, 1.0750945350415209, 1.0753597939151458, 1.07546148198383, 1.0754283423885727, 1.0752347893707075, 1.0752522301399845, 1.0753149361837477, 1.075471867872968, 1.0755204285306883, 1.0776977999065505, 1.0762801491372496, 1.0757110618018164, 1.075634586595745, 1.0762491746880543, 1.075566598738747, 1.0753130292266069, 1.0754203436214154, 1.0754316336611418, 1.0753143197797201, 1.0751418405761468, 1.075027083528453, 1.0750130919987344, 1.0751812534379255, 1.0753837630079297, 1.0752941522692225, 1.075266854124899, 1.0752837661843386, 1.0751779181225154, 1.075165692417101, 1.0752120082601537, 1.074879170834333, 1.0745948308402877, 1.0745970136034861, 1.0739884313887171, 1.0746438920204275, 1.0750584214778955, 1.0742785030202129, 1.0742507507452628, 1.074662681283622, 1.074955490813858, 1.074847585657743, 1.0749123859875307, 1.0751341714451856, 1.0750388764395502, 1.074841179283969, 1.0748743130068474, 1.0749853001831005, 1.0748381422854019, 1.0748123194783779, 1.074235381751225, 1.0735964105634266, 1.073912335734062, 1.0730531207837886, 1.0732294000036806, 1.0740302339171737, 1.0746651989681575, 1.0742369412592871, 1.074485006982274, 1.0746786263580197, 1.0741459234986204, 1.0755652813684373, 1.0741161295933088, 1.0741445765706705, 1.074419524477816, 1.074450720315692, 1.0741468241257817, 1.07421603872271, 1.0742352046011312, 1.0742695693703512, 1.0740755898220393, 1.0743060337107366, 1.074169948770495, 1.0743150196247695, 1.075052320663565, 1.074967657208247, 1.0755824126633518, 1.0751511758967183, 1.0758254567194847, 1.075205644558999, 1.0750449707942644, 1.0745890937219504, 1.074291229052301, 1.0743534997570494, 1.0741234559730943, 1.0740576845475998, 1.0741901188256902, 1.074336090502872, 1.0740695886424023, 1.0737868114841005, 1.0738450966053603, 1.0742284897317245, 1.0743750772256961, 1.0740916999102814, 1.0747535496901213, 1.0748602331957011, 1.0760071866813747, 1.075668494298149, 1.073828886295187, 1.0735900388366875, 1.0723105054379292, 1.0725006550207905, 1.0738201620935024, 1.0738678747797248, 1.0736123906763513, 1.073787134856426, 1.0739992228634838, 1.073858662779108, 1.073525461070056, 1.0737693996852256, 1.073550463701508, 1.0737143118784738, 1.073639345090769, 1.0737307653051291, 1.0736428679308085, 1.0739249351185138, 1.0736494455822974, 1.073762292345169, 1.0738491276019118, 1.0737437197727522, 1.0738458429847053, 1.0740834212264012, 1.074109034585248, 1.0740956381232476, 1.0741526720363324, 1.0739815319308703, 1.0740398417161212, 1.0737009894084462, 1.0739333566969447, 1.0735499383193519, 1.0743867229358317, 1.0735857343830302, 1.0732465054601283, 1.073162483855813, 1.0732374455541225, 1.073392566593214, 1.0736843711636923, 1.0738165288527415, 1.0738092480817647, 1.0741246948492742, 1.0737797458379335, 1.073827745291987, 1.073883163322173, 1.073927162903283, 1.0740717663162056, 1.0743083906878392, 1.0744566964398463, 1.0745609243123597, 1.0744362909022616, 1.0747356892415063, 1.0742015541089187, 1.0741809841447276, 1.073913125373264, 1.0750407029451017, 1.0744097181924654, 1.073981483385872, 1.0739486325159058, 1.0737791519446913, 1.073780854934542, 1.0740097506684427, 1.0740753063818895, 1.0742648127435268, 1.0741820319728508, 1.074020490074784, 1.073918454557021, 1.0734734977603155, 1.0733215137459766, 1.0743302562945387, 1.0730194724447817, 1.0730845452529456, 1.0731151160739718, 1.0734469728125335, 1.0731904653492819, 1.073503857175705, 1.0718420005979992, 1.0707597566159879, 1.0721394571373224, 1.0724393894715458, 1.0721959368935947, 1.069442566783949, 1.072110823418315, 1.0723344972372446, 1.072878463906412, 1.0732201547262508, 1.0736075246275352, 1.0758605756978878, 1.0753316194161602, 1.075113734979739, 1.0752212294608305, 1.0745795448425368, 1.0752224863456388, 1.0749204749935757, 1.075181577201743, 1.0748485698684291, 1.0765202031738457, 1.0750922054688528, 1.074915002719522, 1.0746726359444103, 1.0743169085732822, 1.074853850507188, 1.0746912658703933, 1.0747113002736384, 1.0744028614072376, 1.0740178016997715, 1.074127977704767, 1.074207452326181, 1.074234328442215, 1.0745763373492387, 1.0744543692161297, 1.0748705634929863, 1.074417398676692, 1.074210348779149, 1.0746006114142281, 1.074917587936414, 1.0745340646389865, 1.074718211280497, 1.07451102396929, 1.0745021464985187, 1.0748320011474033, 1.073752941169175, 1.0727490214095718, 1.0734853609442123, 1.0734877230107098, 1.0733565035320463, 1.0737546709762222, 1.0734479394060834, 1.0733360241982346, 1.0739951975435655, 1.0732663930539035, 1.073181406422006, 1.072971147269451, 1.0734090826585767, 1.0729881462400965, 1.0731103529875305, 1.0730039327602667, 1.0729009756705248, 1.0729855134569366, 1.0726794489890288, 1.072742443366591, 1.0726916883966606, 1.072947343777749, 1.072718234876498, 1.072832128097271, 1.0724957836868336, 1.0728641932429548, 1.072573759481433, 1.0726534269126178, 1.0725792131596206, 1.07228762430119, 1.0724513933967879, 1.0729907099249327, 1.0726237449739955, 1.0727844647390306, 1.0728942578649285, 1.072302252201024, 1.0724653649604183], 'val_acc': [0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.379310343408428, 0.3678160905348648, 0.37602627145245743, 0.3908045962819912, 0.38752052373878276, 0.38752052373878276, 0.385878487809734, 0.38752052393452874, 0.3908045962819912, 0.38916256015719647, 0.38587848800547997, 0.379310343506301, 0.38587848800547997, 0.3875205242281477, 0.38587848800547997, 0.38587848800547997, 0.3825944157558905, 0.385878487907607, 0.3825944157558905, 0.38587848810335296, 0.3842364519785582, 0.3842364514891933, 0.385878487809734, 0.3776683071857603, 0.37766830747937924, 0.3776683071857603, 0.38916256025506946, 0.385878487613988, 0.379310343604174, 0.39244663240678596, 0.3809523797289687, 0.37766830728363326, 0.38095237963109574, 0.38095237963109574, 0.3891625600593235, 0.37438423513191676, 0.3809523797289687, 0.38752052393452874, 0.38752052393452874, 0.38423645168493925, 0.3793103437020469, 0.38916256045081543, 0.3743842352297897, 0.3760262712567115, 0.3760262711588385, 0.37931034321268203, 0.38095237953322275, 0.379310343408428, 0.38095237963109574, 0.37766830728363326, 0.38916256045081543, 0.3694581267575325, 0.38095237963109574, 0.38095237943534976, 0.379310343604174, 0.3825944160495094, 0.3743842353276627, 0.3711001628823273, 0.3891625599614505, 0.3891625599614505, 0.3842364522721771, 0.3760262711588385, 0.38423645217430413, 0.3825944160495094, 0.38423645168493925, 0.38752052383665575, 0.38752052383665575, 0.38587848800547997, 0.3842364522721771, 0.38916256045081543, 0.38423645178281224, 0.38423645178281224, 0.3908045962819912, 0.3908045962819912, 0.3908045962819912, 0.3908045965756102, 0.3875205244238936, 0.38587848800547997, 0.38259441585376347, 0.3858784882990989, 0.38587848839697186, 0.38752052393452874, 0.385878487711861, 0.3875205241302747, 0.3793103438977929, 0.39244663289615084, 0.3809523793374768, 0.3940886690209456, 0.3908045968692291, 0.38916256045081543, 0.38916256035294244, 0.3940886690209456, 0.3940886687273267, 0.3793103437999199, 0.3940886687273267, 0.3940886687273267, 0.39408866882519966, 0.38916256064656135, 0.39408866882519966, 0.3940886687273267, 0.3891625613316722, 0.38916256064656135, 0.385878487711861, 0.38916256064656135, 0.3875205245217666, 0.38587848810335296, 0.3891625600593235, 0.3875205243260207, 0.3924466330918968, 0.38587848820122594, 0.3875205240324017, 0.3940886685315807, 0.39901477788469475, 0.3875205240324017, 0.3940886686294537, 0.39080459726072103, 0.3842364519785582, 0.3825944155601445, 0.3990147775910758, 0.39408866931456454, 0.3875205241302747, 0.3875205240324017, 0.39573070573297825, 0.3842364518806852, 0.3908045961841182, 0.38916256035294244, 0.3908045965756102, 0.385878487809734, 0.385878487809734, 0.38916256045081543, 0.385878487907607, 0.3875205240324017, 0.38916256035294244, 0.3908045964777372, 0.3875205242281477, 0.38752052393452874, 0.3908045965756102, 0.3908045965756102, 0.38752052393452874, 0.38916256025506946, 0.39244663250465894, 0.39244663250465894, 0.39244663250465894, 0.3875205242281477, 0.3908045965756102, 0.3940886686294537, 0.3957307049499944, 0.39408866892307265, 0.3957307049499944, 0.3957307049499944, 0.3875205247175126, 0.39408866882519966, 0.3875205247175126, 0.3957307049499944, 0.39408866892307265, 0.3875205247175126, 0.39244663240678596, 0.3875205247175126, 0.3891625609401803, 0.3940886686294537, 0.3875205245217666, 0.3842364525657961, 0.3842364525657961, 0.3908045967713561, 0.3940886686294537, 0.3842364525657961, 0.3875205247175126, 0.3842364523700501, 0.4022988492534274, 0.3842364525657961, 0.3842364525657961, 0.4022988492534274, 0.3842364525657961, 0.3842364524679231, 0.38916256074443434, 0.40394088547609514, 0.3842364525657961, 0.38259441624525536, 0.4022988492534274, 0.3842364525657961, 0.3842364525657961, 0.3940886686294537, 0.3842364525657961, 0.38259441624525536, 0.38259441624525536, 0.3940886686294537, 0.3842364525657961, 0.3842364525657961, 0.3842364525657961, 0.3940886686294537, 0.3842364525657961, 0.38259441624525536, 0.39901477690596493, 0.38259441624525536, 0.38259441247714565, 0.39901477690596493, 0.3940886686294537, 0.38259441624525536, 0.3875205245217666, 0.3875205245217666, 0.3842364525657961, 0.3940886687273267, 0.3842364525657961, 0.3842364525657961, 0.3940886687273267, 0.38259441624525536, 0.3842364524679231, 0.38259441644100134, 0.3776683079687441, 0.40065681322650565, 0.3957307049499944, 0.3957307050478674, 0.37931034419141185, 0.37931034419141185, 0.3957307050478674, 0.38587848859271784, 0.3908045965756102, 0.38587848810335296, 0.3842364519785582, 0.3825944159516364, 0.3875205243260207, 0.37438423542553567, 0.3924466327004049, 0.39408866892307265, 0.3957307049499944, 0.37766830767512516, 0.38095237992471465, 0.37931034409353886, 0.38259441634312835, 0.38916256054868836, 0.3842364524679231, 0.37931034409353886, 0.37931034409353886, 0.38587848820122594, 0.38423645217430413, 0.3776683079687441, 0.38423645217430413, 0.38423645217430413, 0.38423645217430413, 0.38095237992471465, 0.3875205243260207, 0.37766830767512516, 0.3760262715503304, 0.38916256045081543, 0.3809523802183336, 0.3760262718439494, 0.3760262715503304, 0.37602627145245743, 0.37438423571915463, 0.3760262715503304, 0.37438423571915463, 0.38587848810335296, 0.379310343604174, 0.3743842353276627, 0.37766830747937924, 0.37766830747937924, 0.3711001630780732, 0.37274219910499495, 0.37274219930074093, 0.3776683080666171, 0.3711001630780732, 0.37274219930074093, 0.3793103437999199, 0.37274219930074093, 0.3711001630780732, 0.3711001630780732, 0.379310343604174, 0.3940886687273267, 0.37438423542553567, 0.37274219920286794, 0.37274219930074093, 0.3793103437999199, 0.37438423542553567, 0.3940886687273267, 0.3793103437020469, 0.37438423542553567, 0.37438423542553567, 0.37438423542553567, 0.37274219930074093, 0.3793103437020469, 0.37274219930074093, 0.3793103437999199, 0.37438423542553567, 0.3711001630780732, 0.3793103437020469, 0.37438423542553567, 0.37274219930074093, 0.37274219930074093, 0.37438423542553567, 0.3793103437020469, 0.3793103437020469, 0.37438423542553567, 0.3793103437999199, 0.37438423542553567, 0.37438423542553567, 0.37274219920286794, 0.37274219920286794], 'loss': [1.100165511401527, 1.0822108930141285, 1.0768355461606254, 1.0742841689248839, 1.074338202212136, 1.0745402904996146, 1.0743336058250443, 1.0741994264923815, 1.0739793317273902, 1.0740234431789641, 1.0738247599200301, 1.0737076189973271, 1.0735886378944288, 1.073656395671304, 1.0735793315409634, 1.073487951476471, 1.073379029483521, 1.0733109824711293, 1.0732768135638697, 1.0731876630313097, 1.073141539072354, 1.0729339545267564, 1.0729477844688682, 1.0728730904737782, 1.0731998881275404, 1.0733684981383338, 1.0732155554593221, 1.0736925505514752, 1.073774251898701, 1.0734064974089668, 1.0736438141711193, 1.0734648521431174, 1.0735404514434157, 1.0735604052181362, 1.0731318432692385, 1.0730507148609514, 1.0729633989275358, 1.073078114736741, 1.0730723339918948, 1.0727166317816388, 1.072655939370455, 1.0732193957119263, 1.0728455391752647, 1.0728835382500712, 1.0729058596142997, 1.0719986072066383, 1.0726883122563606, 1.072742921958469, 1.072517890998715, 1.072519495403987, 1.0723409530318493, 1.0723683551106855, 1.072375807429241, 1.0721233314557241, 1.0730782636137224, 1.0725532130783832, 1.072932366864637, 1.0723320308896795, 1.0722543541166083, 1.0724753054260474, 1.0724668761298397, 1.072499529341163, 1.0726011262537274, 1.072790718470266, 1.0724525490335861, 1.0723715948617922, 1.0725521886617986, 1.0718822761727553, 1.0719732619408955, 1.0719325106736326, 1.0720752403721427, 1.0719441020268434, 1.07187537324502, 1.0718655196548244, 1.0723365405256988, 1.0721438024567873, 1.0720240544979087, 1.0719630089628622, 1.0717875906084597, 1.0713820293943495, 1.0720729881733104, 1.0724838432590085, 1.072315310795449, 1.0722414228705655, 1.0718568060677154, 1.0718694035277474, 1.072147225746139, 1.071620904444669, 1.0719870570259173, 1.0717777066651801, 1.0714351171830352, 1.0711430365544814, 1.0712685433256552, 1.0710110698637287, 1.0707924724359532, 1.0720074769163033, 1.0715538722044144, 1.071642186901163, 1.0715389247792457, 1.071188392384586, 1.071451085793654, 1.070978756949642, 1.0716250880298184, 1.0710051243310101, 1.0723860454755152, 1.071222408498337, 1.071712330087744, 1.0709302761716275, 1.0715803205599774, 1.0710338104432124, 1.0711752565489658, 1.0715969666318481, 1.0715428290670657, 1.071146794951672, 1.0706348549413975, 1.0712473194212395, 1.071044660202042, 1.071655479250992, 1.070428861974446, 1.0712243296527275, 1.0710654416368237, 1.071289187094514, 1.0708411920241996, 1.0709434486267748, 1.0706922526721836, 1.0707403305374865, 1.0710762065049315, 1.071326657927746, 1.0713628308239413, 1.0713142332844665, 1.0710341502019267, 1.0713345388122653, 1.0714546133115797, 1.0711017959171742, 1.0706496535385415, 1.0708835940096657, 1.0708357768871457, 1.0711342114932239, 1.0711937942544048, 1.071985699166018, 1.0717727911790538, 1.0718803944284176, 1.071667429015377, 1.071078589222025, 1.0708289015709251, 1.0704689628534494, 1.0706131728522832, 1.0707909474382655, 1.0704594863758439, 1.0706520366472876, 1.0705698308513885, 1.0700506967195984, 1.070281586069346, 1.070276736968352, 1.0701426416941493, 1.0701588156776507, 1.0699879574335087, 1.0699870994203635, 1.0702732346141117, 1.069759557085605, 1.070064638380642, 1.0699595420512331, 1.0698620210682832, 1.07034425515169, 1.0701760083498162, 1.06999518073315, 1.07017213040064, 1.0701210439572344, 1.070018179411761, 1.0698381486614628, 1.0697448734385278, 1.0701144846068271, 1.069633046606483, 1.0706461206843476, 1.0708756150651026, 1.070665274410522, 1.0702272406348947, 1.0701413832161215, 1.0702163442204375, 1.070526201524284, 1.0701087716668538, 1.0705572509178145, 1.06960020848613, 1.0698805822239275, 1.0696994678685308, 1.0701108249550728, 1.069412395596749, 1.069541337357899, 1.0694428414045174, 1.0693678620414813, 1.0695765991720085, 1.069344246167177, 1.0698656775378594, 1.0690689860428138, 1.0698246289572433, 1.069278461977197, 1.0696193500221143, 1.0707446719586726, 1.0690112614288956, 1.0699942800298607, 1.06958583615399, 1.0689829669204336, 1.069582011077928, 1.0697876905758523, 1.0696467310985744, 1.0692339963247155, 1.0691337783723396, 1.0690444427838806, 1.0691092727120652, 1.06881717245192, 1.0695317384398693, 1.0694899629028916, 1.0697273378254697, 1.0699367725384064, 1.0696703870683235, 1.0697567629863105, 1.069793412719664, 1.069311260246888, 1.0685697012613442, 1.0700167174701574, 1.0704481888600688, 1.0698801872911394, 1.070141693111318, 1.0703967190865864, 1.0693303100871845, 1.069652693717142, 1.0700143734287677, 1.0707635274413185, 1.0710256895735033, 1.0695827302501921, 1.071587843327062, 1.0712456330626408, 1.0707084679750447, 1.0708066495781807, 1.070576950852631, 1.0705080330494248, 1.0704095959908173, 1.0713039211668762, 1.0713995271639658, 1.070638213069532, 1.0706943798848492, 1.0702747310211527, 1.0702538009052158, 1.0704163633577632, 1.0701211987579629, 1.0704470677542246, 1.0702070961742676, 1.0703026438640617, 1.0702033122217387, 1.0701295269098614, 1.0701230441275564, 1.0701630008294107, 1.0706776217513505, 1.0702302435340334, 1.0701407649433834, 1.0702822239247192, 1.06999077610901, 1.0698246524074484, 1.0697498846347817, 1.0699507168920623, 1.0702857392034981, 1.0696959710463851, 1.0701603207500074, 1.0700470744706767, 1.0691073103851851, 1.0694797358718495, 1.0692209550242648, 1.0692138982749329, 1.0693204736317943, 1.0692803808795843, 1.0698083850147788, 1.0693005355721381, 1.0692938448712077, 1.0696733529073257, 1.0687804264699163, 1.0696076822966276, 1.0689919969629214, 1.0696655083975508, 1.0691550734106765, 1.069127900546581, 1.0690017149923274, 1.0694183112659494, 1.068772474010867, 1.0689307029242388, 1.0685346056548477, 1.0686723638119393, 1.0683980571171097, 1.068635731751914, 1.0687770632992535, 1.0684063891855353, 1.06841110257887, 1.068231808576251, 1.0685303533346502, 1.0692810891345297, 1.0681262285557616, 1.0694677982976548, 1.0694855398710748, 1.068845667927172, 1.0687672682121794, 1.0686945959283096], 'acc': [0.3260780300937394, 0.39425051351837065, 0.3942505156724605, 0.39671458138087934, 0.3958932261447397, 0.39383983648777987, 0.40000000212961156, 0.39794661403926246, 0.39548254793919085, 0.3991786437235329, 0.40000000212961156, 0.40246406505974414, 0.4008213565824458, 0.40164271021525716, 0.40451745612420587, 0.40123203478799463, 0.4020533876742181, 0.400821355015835, 0.40328541912092564, 0.4045174525626141, 0.40451745573255316, 0.4024640676422041, 0.4049281310007068, 0.40410677592367605, 0.40451745432505126, 0.3991786461101665, 0.403285419316752, 0.3979466118851726, 0.40287474209033486, 0.40657084401872856, 0.3983572908740269, 0.3999999981763671, 0.40000000095465343, 0.3971252544582257, 0.3934291584780574, 0.40123203122640294, 0.3995893201666446, 0.4008213559949667, 0.39466119051224396, 0.40041067974768135, 0.40123203165477306, 0.39794661266847803, 0.3991786457185138, 0.4008213532166804, 0.40287474388948946, 0.40369610030058717, 0.4036960967389955, 0.40246406666307233, 0.4000000011504798, 0.4020533888124587, 0.39917864313605383, 0.40205339041578697, 0.40533880936536454, 0.40574948799928356, 0.40123203282973113, 0.3979466124726517, 0.3946611917239195, 0.4016427127977171, 0.4024640676422041, 0.3999999997429779, 0.3979466142350888, 0.398767964930505, 0.40041067622280707, 0.3954825455892747, 0.4004106799435077, 0.3983572906782005, 0.40000000095465343, 0.4008213575615775, 0.39958932036247097, 0.40041067974768135, 0.40041067778941786, 0.40041067618608966, 0.39958932353241, 0.3975359358337136, 0.40123203400468926, 0.4016427102519746, 0.4000000019337852, 0.3991786437602503, 0.4000000011504798, 0.4008213575615775, 0.40328542229086467, 0.4024640672505514, 0.4000000011137624, 0.40246406392150347, 0.4004106759902633, 0.40246406705472504, 0.4028747448686212, 0.40369609752230085, 0.404106777918657, 0.40287474111120314, 0.40574948522099724, 0.40574948561264995, 0.4061601662048324, 0.4069815186626857, 0.4057494854168236, 0.4045174515834824, 0.40123203204642577, 0.4041067741612389, 0.40246406705472504, 0.4028747413070295, 0.39835729106985324, 0.40287474248198757, 0.4012320334172102, 0.40328541912092564, 0.3926078018711333, 0.40492812939737854, 0.40287474209033486, 0.4045174515834824, 0.40369609932145545, 0.4004106775935915, 0.40328541951257835, 0.4069815188585121, 0.4041067771353516, 0.4049281335464493, 0.4020533902199606, 0.4008213546241823, 0.4020533870500216, 0.40451745295426683, 0.40287474310618404, 0.40000000017134807, 0.4090349073772313, 0.40369609932145545, 0.4028747446727948, 0.40369609971310816, 0.40328542150755925, 0.40041067778941786, 0.4012320306389239, 0.4036960967389955, 0.40082135380415945, 0.4032854189250993, 0.4057494874118045, 0.4024640658430495, 0.40041067818107057, 0.40082135364505056, 0.4016427100194308, 0.40574948776673975, 0.4020533882616971, 0.40246406368895965, 0.40246406744637775, 0.40205338861663237, 0.4004106759902633, 0.4036960987339764, 0.4020533864625426, 0.4016427124060645, 0.4000000005630007, 0.40041067540278424, 0.3946611928988776, 0.406981520266014, 0.4024640642764387, 0.3995893207541237, 0.40574948878258893, 0.40533880779875375, 0.40369610030058717, 0.40369609752230085, 0.403696099284738, 0.39671457781928765, 0.4024640642764387, 0.402464066467246, 0.40451745334591954, 0.3995893245115417, 0.4036960965431691, 0.40451745432505126, 0.4057494864326728, 0.40369609693482184, 0.4065708404571369, 0.40328542150755925, 0.40369609634734277, 0.4036960995172818, 0.40492813021740137, 0.39630390180454605, 0.4057494854168236, 0.4024640678380304, 0.39794661047767077, 0.39958932431571537, 0.40616016362237245, 0.4041067763520462, 0.40287474545610025, 0.40123203322138384, 0.40246406548811425, 0.39753593642119267, 0.40246406787474787, 0.3963039031753305, 0.404517454716704, 0.4041067775270043, 0.40123203322138384, 0.40410677392869515, 0.4045174523667878, 0.4049281299848576, 0.4000000003671744, 0.40369609634734277, 0.3995893201666446, 0.4036960973264745, 0.4020533874416743, 0.3995893221616255, 0.40451745377428966, 0.39835728989489516, 0.40287474404859835, 0.3995893231407573, 0.406160165421527, 0.3999999999755217, 0.39753593602953996, 0.40287474153957326, 0.39876796712131224, 0.4045174559283795, 0.40041067618608966, 0.40123203083475023, 0.40369610088806623, 0.403285422486691, 0.3995893231407573, 0.40492813080488044, 0.40000000017134807, 0.4012320338088629, 0.39958932431571537, 0.4016427114269327, 0.40328542307417004, 0.3983572916940497, 0.4000000003671744, 0.39958932310403983, 0.40410677733117795, 0.39917864493520844, 0.4024640662714196, 0.4024640668588987, 0.40164271201441176, 0.40123203204642577, 0.40205338724584794, 0.4053388089737119, 0.40328542052842753, 0.3995893211457764, 0.4057494879625661, 0.4000000019337852, 0.3999999991922163, 0.403285418729273, 0.39589322183656006, 0.3979466106734971, 0.3979466124726517, 0.3979466128643044, 0.4004106775935915, 0.40287474545610025, 0.4004106755986106, 0.39794661364760975, 0.4004106799802251, 0.40041067935602864, 0.39835728848739327, 0.39753593524623454, 0.402464063884786, 0.39712525821564376, 0.40041067739776515, 0.40205338802915336, 0.4016427122102381, 0.40082135716992484, 0.4004106755986106, 0.399999999583869, 0.3995893201666446, 0.4020533866583689, 0.4020533858750635, 0.40410677533619704, 0.40451745534090044, 0.4036960995172818, 0.40492813135564204, 0.40123203420051556, 0.4024640660755933, 0.40533881077286643, 0.39712525606155397, 0.4028747444769685, 0.40780287589380626, 0.4057494858084763, 0.4082135532793323, 0.40862422776418056, 0.4082135517127215, 0.41026694199387786, 0.4049281313923595, 0.4053388070154484, 0.40739219729660475, 0.40739219510579744, 0.40862423132577225, 0.4057494870201518, 0.4094455863660855, 0.4053388109686928, 0.40739219941397714, 0.408213553083506, 0.40780287350717265, 0.40780287589380626, 0.4049281335464493, 0.4098562610100427, 0.4082135546868342, 0.40451745612420587, 0.4082135525327444, 0.4049281295932049, 0.4082135535118761, 0.4082135521043742, 0.40862422815583327, 0.4078028727238673, 0.4078028744863044, 0.40698151748772765, 0.40533880920625565, 0.4024640654513968, 0.4057494854168236, 0.4065708404571369, 0.4065708408487896, 0.40698151787938036]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
