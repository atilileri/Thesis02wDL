{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf14.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 08:22:56 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': 'Front', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '05', '01', '02', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002738032BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000273F6C76EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6088, Accuracy:0.2066, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6065, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6060, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6020, Accuracy:0.2353, Validation Loss:1.6042, Validation Accuracy:0.2414\n",
    "Epoch #18: Loss:1.6018, Accuracy:0.2378, Validation Loss:1.6041, Validation Accuracy:0.2414\n",
    "Epoch #19: Loss:1.6014, Accuracy:0.2366, Validation Loss:1.6041, Validation Accuracy:0.2414\n",
    "Epoch #20: Loss:1.6016, Accuracy:0.2407, Validation Loss:1.6042, Validation Accuracy:0.2414\n",
    "Epoch #21: Loss:1.6005, Accuracy:0.2398, Validation Loss:1.6040, Validation Accuracy:0.2414\n",
    "Epoch #22: Loss:1.6004, Accuracy:0.2370, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #23: Loss:1.6002, Accuracy:0.2435, Validation Loss:1.6042, Validation Accuracy:0.2365\n",
    "Epoch #24: Loss:1.5999, Accuracy:0.2435, Validation Loss:1.6037, Validation Accuracy:0.2430\n",
    "Epoch #25: Loss:1.5990, Accuracy:0.2439, Validation Loss:1.6037, Validation Accuracy:0.2430\n",
    "Epoch #26: Loss:1.5988, Accuracy:0.2435, Validation Loss:1.6038, Validation Accuracy:0.2447\n",
    "Epoch #27: Loss:1.5987, Accuracy:0.2448, Validation Loss:1.6040, Validation Accuracy:0.2381\n",
    "Epoch #28: Loss:1.5976, Accuracy:0.2444, Validation Loss:1.6034, Validation Accuracy:0.2397\n",
    "Epoch #29: Loss:1.5978, Accuracy:0.2452, Validation Loss:1.6034, Validation Accuracy:0.2381\n",
    "Epoch #30: Loss:1.5972, Accuracy:0.2460, Validation Loss:1.6036, Validation Accuracy:0.2430\n",
    "Epoch #31: Loss:1.5965, Accuracy:0.2468, Validation Loss:1.6034, Validation Accuracy:0.2381\n",
    "Epoch #32: Loss:1.5964, Accuracy:0.2439, Validation Loss:1.6038, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.5954, Accuracy:0.2485, Validation Loss:1.6036, Validation Accuracy:0.2447\n",
    "Epoch #34: Loss:1.5949, Accuracy:0.2452, Validation Loss:1.6035, Validation Accuracy:0.2594\n",
    "Epoch #35: Loss:1.5952, Accuracy:0.2522, Validation Loss:1.6039, Validation Accuracy:0.2512\n",
    "Epoch #36: Loss:1.5943, Accuracy:0.2468, Validation Loss:1.6037, Validation Accuracy:0.2447\n",
    "Epoch #37: Loss:1.5936, Accuracy:0.2468, Validation Loss:1.6044, Validation Accuracy:0.2545\n",
    "Epoch #38: Loss:1.5927, Accuracy:0.2513, Validation Loss:1.6042, Validation Accuracy:0.2430\n",
    "Epoch #39: Loss:1.5919, Accuracy:0.2550, Validation Loss:1.6044, Validation Accuracy:0.2594\n",
    "Epoch #40: Loss:1.5914, Accuracy:0.2509, Validation Loss:1.6049, Validation Accuracy:0.2529\n",
    "Epoch #41: Loss:1.5929, Accuracy:0.2489, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #42: Loss:1.5904, Accuracy:0.2526, Validation Loss:1.6070, Validation Accuracy:0.2529\n",
    "Epoch #43: Loss:1.5908, Accuracy:0.2534, Validation Loss:1.6053, Validation Accuracy:0.2430\n",
    "Epoch #44: Loss:1.5871, Accuracy:0.2604, Validation Loss:1.6056, Validation Accuracy:0.2545\n",
    "Epoch #45: Loss:1.5860, Accuracy:0.2674, Validation Loss:1.6062, Validation Accuracy:0.2282\n",
    "Epoch #46: Loss:1.5846, Accuracy:0.2624, Validation Loss:1.6074, Validation Accuracy:0.2397\n",
    "Epoch #47: Loss:1.5821, Accuracy:0.2690, Validation Loss:1.6084, Validation Accuracy:0.2348\n",
    "Epoch #48: Loss:1.5807, Accuracy:0.2694, Validation Loss:1.6109, Validation Accuracy:0.2299\n",
    "Epoch #49: Loss:1.5776, Accuracy:0.2739, Validation Loss:1.6137, Validation Accuracy:0.2233\n",
    "Epoch #50: Loss:1.5790, Accuracy:0.2706, Validation Loss:1.6165, Validation Accuracy:0.2348\n",
    "Epoch #51: Loss:1.5758, Accuracy:0.2710, Validation Loss:1.6240, Validation Accuracy:0.2315\n",
    "Epoch #52: Loss:1.5746, Accuracy:0.2809, Validation Loss:1.6207, Validation Accuracy:0.2184\n",
    "Epoch #53: Loss:1.5754, Accuracy:0.2674, Validation Loss:1.6227, Validation Accuracy:0.2217\n",
    "Epoch #54: Loss:1.5699, Accuracy:0.2821, Validation Loss:1.6314, Validation Accuracy:0.2003\n",
    "Epoch #55: Loss:1.5713, Accuracy:0.2735, Validation Loss:1.6270, Validation Accuracy:0.2332\n",
    "Epoch #56: Loss:1.5689, Accuracy:0.2842, Validation Loss:1.6310, Validation Accuracy:0.2135\n",
    "Epoch #57: Loss:1.5654, Accuracy:0.2825, Validation Loss:1.6306, Validation Accuracy:0.2299\n",
    "Epoch #58: Loss:1.5644, Accuracy:0.2846, Validation Loss:1.6352, Validation Accuracy:0.2003\n",
    "Epoch #59: Loss:1.5597, Accuracy:0.2936, Validation Loss:1.6448, Validation Accuracy:0.2135\n",
    "Epoch #60: Loss:1.5632, Accuracy:0.2821, Validation Loss:1.6400, Validation Accuracy:0.2118\n",
    "Epoch #61: Loss:1.5610, Accuracy:0.2706, Validation Loss:1.6409, Validation Accuracy:0.2200\n",
    "Epoch #62: Loss:1.5553, Accuracy:0.2862, Validation Loss:1.6422, Validation Accuracy:0.2102\n",
    "Epoch #63: Loss:1.5541, Accuracy:0.2887, Validation Loss:1.6452, Validation Accuracy:0.2102\n",
    "Epoch #64: Loss:1.5509, Accuracy:0.2957, Validation Loss:1.6557, Validation Accuracy:0.2085\n",
    "Epoch #65: Loss:1.5500, Accuracy:0.2990, Validation Loss:1.6511, Validation Accuracy:0.2053\n",
    "Epoch #66: Loss:1.5468, Accuracy:0.2994, Validation Loss:1.6555, Validation Accuracy:0.2135\n",
    "Epoch #67: Loss:1.5465, Accuracy:0.3014, Validation Loss:1.6625, Validation Accuracy:0.2020\n",
    "Epoch #68: Loss:1.5420, Accuracy:0.2945, Validation Loss:1.6649, Validation Accuracy:0.1987\n",
    "Epoch #69: Loss:1.5425, Accuracy:0.3055, Validation Loss:1.6641, Validation Accuracy:0.2135\n",
    "Epoch #70: Loss:1.5421, Accuracy:0.3014, Validation Loss:1.6753, Validation Accuracy:0.2053\n",
    "Epoch #71: Loss:1.5365, Accuracy:0.3133, Validation Loss:1.6687, Validation Accuracy:0.2085\n",
    "Epoch #72: Loss:1.5417, Accuracy:0.3002, Validation Loss:1.6746, Validation Accuracy:0.2069\n",
    "Epoch #73: Loss:1.5342, Accuracy:0.3133, Validation Loss:1.6707, Validation Accuracy:0.2102\n",
    "Epoch #74: Loss:1.5287, Accuracy:0.3211, Validation Loss:1.6745, Validation Accuracy:0.2135\n",
    "Epoch #75: Loss:1.5247, Accuracy:0.3138, Validation Loss:1.6832, Validation Accuracy:0.2184\n",
    "Epoch #76: Loss:1.5208, Accuracy:0.3224, Validation Loss:1.6896, Validation Accuracy:0.2118\n",
    "Epoch #77: Loss:1.5236, Accuracy:0.3228, Validation Loss:1.7007, Validation Accuracy:0.2233\n",
    "Epoch #78: Loss:1.5247, Accuracy:0.3162, Validation Loss:1.6998, Validation Accuracy:0.2217\n",
    "Epoch #79: Loss:1.5165, Accuracy:0.3265, Validation Loss:1.6990, Validation Accuracy:0.1987\n",
    "Epoch #80: Loss:1.5119, Accuracy:0.3409, Validation Loss:1.7202, Validation Accuracy:0.2135\n",
    "Epoch #81: Loss:1.5162, Accuracy:0.3281, Validation Loss:1.7028, Validation Accuracy:0.2085\n",
    "Epoch #82: Loss:1.5080, Accuracy:0.3351, Validation Loss:1.7088, Validation Accuracy:0.2266\n",
    "Epoch #83: Loss:1.5056, Accuracy:0.3421, Validation Loss:1.7195, Validation Accuracy:0.1987\n",
    "Epoch #84: Loss:1.5025, Accuracy:0.3441, Validation Loss:1.7258, Validation Accuracy:0.2135\n",
    "Epoch #85: Loss:1.5026, Accuracy:0.3298, Validation Loss:1.7288, Validation Accuracy:0.2003\n",
    "Epoch #86: Loss:1.5020, Accuracy:0.3351, Validation Loss:1.7353, Validation Accuracy:0.2102\n",
    "Epoch #87: Loss:1.5014, Accuracy:0.3405, Validation Loss:1.7301, Validation Accuracy:0.2102\n",
    "Epoch #88: Loss:1.4999, Accuracy:0.3380, Validation Loss:1.7281, Validation Accuracy:0.2167\n",
    "Epoch #89: Loss:1.4896, Accuracy:0.3454, Validation Loss:1.7396, Validation Accuracy:0.2250\n",
    "Epoch #90: Loss:1.4873, Accuracy:0.3532, Validation Loss:1.7398, Validation Accuracy:0.1905\n",
    "Epoch #91: Loss:1.4882, Accuracy:0.3503, Validation Loss:1.7460, Validation Accuracy:0.2200\n",
    "Epoch #92: Loss:1.4863, Accuracy:0.3458, Validation Loss:1.7534, Validation Accuracy:0.2118\n",
    "Epoch #93: Loss:1.4825, Accuracy:0.3593, Validation Loss:1.7527, Validation Accuracy:0.2102\n",
    "Epoch #94: Loss:1.4873, Accuracy:0.3548, Validation Loss:1.7543, Validation Accuracy:0.1987\n",
    "Epoch #95: Loss:1.4773, Accuracy:0.3622, Validation Loss:1.7638, Validation Accuracy:0.2151\n",
    "Epoch #96: Loss:1.4734, Accuracy:0.3663, Validation Loss:1.7654, Validation Accuracy:0.1872\n",
    "Epoch #97: Loss:1.4793, Accuracy:0.3602, Validation Loss:1.7654, Validation Accuracy:0.1954\n",
    "Epoch #98: Loss:1.4751, Accuracy:0.3528, Validation Loss:1.7782, Validation Accuracy:0.2299\n",
    "Epoch #99: Loss:1.4719, Accuracy:0.3647, Validation Loss:1.7778, Validation Accuracy:0.1905\n",
    "Epoch #100: Loss:1.4705, Accuracy:0.3528, Validation Loss:1.7790, Validation Accuracy:0.2135\n",
    "Epoch #101: Loss:1.4727, Accuracy:0.3478, Validation Loss:1.7925, Validation Accuracy:0.1987\n",
    "Epoch #102: Loss:1.4745, Accuracy:0.3532, Validation Loss:1.7712, Validation Accuracy:0.1987\n",
    "Epoch #103: Loss:1.4672, Accuracy:0.3622, Validation Loss:1.7791, Validation Accuracy:0.1823\n",
    "Epoch #104: Loss:1.4606, Accuracy:0.3721, Validation Loss:1.7775, Validation Accuracy:0.2250\n",
    "Epoch #105: Loss:1.4640, Accuracy:0.3626, Validation Loss:1.7881, Validation Accuracy:0.1987\n",
    "Epoch #106: Loss:1.4532, Accuracy:0.3745, Validation Loss:1.7999, Validation Accuracy:0.1905\n",
    "Epoch #107: Loss:1.4482, Accuracy:0.3754, Validation Loss:1.7935, Validation Accuracy:0.1921\n",
    "Epoch #108: Loss:1.4473, Accuracy:0.3778, Validation Loss:1.8142, Validation Accuracy:0.1921\n",
    "Epoch #109: Loss:1.4485, Accuracy:0.3754, Validation Loss:1.8009, Validation Accuracy:0.2151\n",
    "Epoch #110: Loss:1.4496, Accuracy:0.3749, Validation Loss:1.8157, Validation Accuracy:0.1905\n",
    "Epoch #111: Loss:1.4482, Accuracy:0.3737, Validation Loss:1.8184, Validation Accuracy:0.1888\n",
    "Epoch #112: Loss:1.4371, Accuracy:0.3869, Validation Loss:1.8166, Validation Accuracy:0.1806\n",
    "Epoch #113: Loss:1.4334, Accuracy:0.3885, Validation Loss:1.8462, Validation Accuracy:0.1708\n",
    "Epoch #114: Loss:1.4378, Accuracy:0.3786, Validation Loss:1.8272, Validation Accuracy:0.1954\n",
    "Epoch #115: Loss:1.4523, Accuracy:0.3581, Validation Loss:1.8127, Validation Accuracy:0.1708\n",
    "Epoch #116: Loss:1.4595, Accuracy:0.3733, Validation Loss:1.8489, Validation Accuracy:0.1905\n",
    "Epoch #117: Loss:1.4518, Accuracy:0.3507, Validation Loss:1.7911, Validation Accuracy:0.1823\n",
    "Epoch #118: Loss:1.4468, Accuracy:0.3758, Validation Loss:1.8109, Validation Accuracy:0.1724\n",
    "Epoch #119: Loss:1.4315, Accuracy:0.3885, Validation Loss:1.8360, Validation Accuracy:0.2036\n",
    "Epoch #120: Loss:1.4430, Accuracy:0.3774, Validation Loss:1.8237, Validation Accuracy:0.1806\n",
    "Epoch #121: Loss:1.4286, Accuracy:0.3889, Validation Loss:1.8500, Validation Accuracy:0.1856\n",
    "Epoch #122: Loss:1.4234, Accuracy:0.3828, Validation Loss:1.8464, Validation Accuracy:0.1757\n",
    "Epoch #123: Loss:1.4140, Accuracy:0.3955, Validation Loss:1.8373, Validation Accuracy:0.1790\n",
    "Epoch #124: Loss:1.4198, Accuracy:0.3811, Validation Loss:1.8519, Validation Accuracy:0.1856\n",
    "Epoch #125: Loss:1.4176, Accuracy:0.3959, Validation Loss:1.8490, Validation Accuracy:0.1675\n",
    "Epoch #126: Loss:1.4108, Accuracy:0.4000, Validation Loss:1.8499, Validation Accuracy:0.1741\n",
    "Epoch #127: Loss:1.4069, Accuracy:0.4037, Validation Loss:1.8705, Validation Accuracy:0.1691\n",
    "Epoch #128: Loss:1.4064, Accuracy:0.3996, Validation Loss:1.8716, Validation Accuracy:0.1773\n",
    "Epoch #129: Loss:1.4021, Accuracy:0.3988, Validation Loss:1.8736, Validation Accuracy:0.1675\n",
    "Epoch #130: Loss:1.3984, Accuracy:0.4045, Validation Loss:1.8808, Validation Accuracy:0.1708\n",
    "Epoch #131: Loss:1.4028, Accuracy:0.4021, Validation Loss:1.8682, Validation Accuracy:0.1741\n",
    "Epoch #132: Loss:1.4010, Accuracy:0.4029, Validation Loss:1.8797, Validation Accuracy:0.1806\n",
    "Epoch #133: Loss:1.4024, Accuracy:0.3996, Validation Loss:1.8807, Validation Accuracy:0.1741\n",
    "Epoch #134: Loss:1.4022, Accuracy:0.3979, Validation Loss:1.8918, Validation Accuracy:0.1839\n",
    "Epoch #135: Loss:1.4018, Accuracy:0.3934, Validation Loss:1.8717, Validation Accuracy:0.1757\n",
    "Epoch #136: Loss:1.3991, Accuracy:0.4016, Validation Loss:1.8705, Validation Accuracy:0.1790\n",
    "Epoch #137: Loss:1.4073, Accuracy:0.3922, Validation Loss:1.8882, Validation Accuracy:0.1626\n",
    "Epoch #138: Loss:1.3920, Accuracy:0.4127, Validation Loss:1.8846, Validation Accuracy:0.1757\n",
    "Epoch #139: Loss:1.3828, Accuracy:0.4090, Validation Loss:1.8971, Validation Accuracy:0.1675\n",
    "Epoch #140: Loss:1.3821, Accuracy:0.4160, Validation Loss:1.8999, Validation Accuracy:0.1773\n",
    "Epoch #141: Loss:1.3815, Accuracy:0.4123, Validation Loss:1.9073, Validation Accuracy:0.1691\n",
    "Epoch #142: Loss:1.3801, Accuracy:0.4156, Validation Loss:1.9124, Validation Accuracy:0.1708\n",
    "Epoch #143: Loss:1.3765, Accuracy:0.4160, Validation Loss:1.9093, Validation Accuracy:0.1790\n",
    "Epoch #144: Loss:1.3757, Accuracy:0.4193, Validation Loss:1.9152, Validation Accuracy:0.1609\n",
    "Epoch #145: Loss:1.3766, Accuracy:0.4205, Validation Loss:1.8933, Validation Accuracy:0.1790\n",
    "Epoch #146: Loss:1.3749, Accuracy:0.4144, Validation Loss:1.9233, Validation Accuracy:0.1724\n",
    "Epoch #147: Loss:1.3760, Accuracy:0.4123, Validation Loss:1.9140, Validation Accuracy:0.1954\n",
    "Epoch #148: Loss:1.3816, Accuracy:0.4053, Validation Loss:1.9133, Validation Accuracy:0.1773\n",
    "Epoch #149: Loss:1.3659, Accuracy:0.4152, Validation Loss:1.9261, Validation Accuracy:0.1593\n",
    "Epoch #150: Loss:1.3619, Accuracy:0.4267, Validation Loss:1.9034, Validation Accuracy:0.1724\n",
    "Epoch #151: Loss:1.3580, Accuracy:0.4201, Validation Loss:1.9417, Validation Accuracy:0.1839\n",
    "Epoch #152: Loss:1.3602, Accuracy:0.4214, Validation Loss:1.9249, Validation Accuracy:0.1757\n",
    "Epoch #153: Loss:1.3634, Accuracy:0.4238, Validation Loss:1.9314, Validation Accuracy:0.1724\n",
    "Epoch #154: Loss:1.3519, Accuracy:0.4304, Validation Loss:1.9376, Validation Accuracy:0.1823\n",
    "Epoch #155: Loss:1.3512, Accuracy:0.4259, Validation Loss:1.9362, Validation Accuracy:0.1741\n",
    "Epoch #156: Loss:1.3477, Accuracy:0.4287, Validation Loss:1.9461, Validation Accuracy:0.1724\n",
    "Epoch #157: Loss:1.3515, Accuracy:0.4201, Validation Loss:1.9587, Validation Accuracy:0.1856\n",
    "Epoch #158: Loss:1.3644, Accuracy:0.4148, Validation Loss:1.9434, Validation Accuracy:0.1741\n",
    "Epoch #159: Loss:1.3561, Accuracy:0.4251, Validation Loss:1.9374, Validation Accuracy:0.1741\n",
    "Epoch #160: Loss:1.3464, Accuracy:0.4222, Validation Loss:1.9370, Validation Accuracy:0.1626\n",
    "Epoch #161: Loss:1.3384, Accuracy:0.4361, Validation Loss:1.9505, Validation Accuracy:0.1691\n",
    "Epoch #162: Loss:1.3369, Accuracy:0.4349, Validation Loss:1.9627, Validation Accuracy:0.1823\n",
    "Epoch #163: Loss:1.3429, Accuracy:0.4242, Validation Loss:1.9819, Validation Accuracy:0.1790\n",
    "Epoch #164: Loss:1.3385, Accuracy:0.4279, Validation Loss:1.9657, Validation Accuracy:0.1773\n",
    "Epoch #165: Loss:1.3291, Accuracy:0.4394, Validation Loss:1.9621, Validation Accuracy:0.1806\n",
    "Epoch #166: Loss:1.3292, Accuracy:0.4452, Validation Loss:1.9527, Validation Accuracy:0.1691\n",
    "Epoch #167: Loss:1.3227, Accuracy:0.4423, Validation Loss:1.9840, Validation Accuracy:0.1658\n",
    "Epoch #168: Loss:1.3325, Accuracy:0.4283, Validation Loss:1.9771, Validation Accuracy:0.1741\n",
    "Epoch #169: Loss:1.3454, Accuracy:0.4279, Validation Loss:1.9857, Validation Accuracy:0.1856\n",
    "Epoch #170: Loss:1.3407, Accuracy:0.4370, Validation Loss:1.9841, Validation Accuracy:0.1790\n",
    "Epoch #171: Loss:1.3414, Accuracy:0.4378, Validation Loss:1.9732, Validation Accuracy:0.1823\n",
    "Epoch #172: Loss:1.3497, Accuracy:0.4316, Validation Loss:1.9821, Validation Accuracy:0.1708\n",
    "Epoch #173: Loss:1.3362, Accuracy:0.4448, Validation Loss:1.9754, Validation Accuracy:0.1691\n",
    "Epoch #174: Loss:1.3336, Accuracy:0.4378, Validation Loss:1.9907, Validation Accuracy:0.1905\n",
    "Epoch #175: Loss:1.3299, Accuracy:0.4394, Validation Loss:1.9658, Validation Accuracy:0.1593\n",
    "Epoch #176: Loss:1.3170, Accuracy:0.4448, Validation Loss:1.9859, Validation Accuracy:0.1642\n",
    "Epoch #177: Loss:1.3150, Accuracy:0.4460, Validation Loss:1.9826, Validation Accuracy:0.1708\n",
    "Epoch #178: Loss:1.3129, Accuracy:0.4501, Validation Loss:2.0173, Validation Accuracy:0.1494\n",
    "Epoch #179: Loss:1.3087, Accuracy:0.4464, Validation Loss:1.9884, Validation Accuracy:0.1839\n",
    "Epoch #180: Loss:1.3041, Accuracy:0.4480, Validation Loss:2.0297, Validation Accuracy:0.1724\n",
    "Epoch #181: Loss:1.3169, Accuracy:0.4353, Validation Loss:2.0101, Validation Accuracy:0.1773\n",
    "Epoch #182: Loss:1.3080, Accuracy:0.4604, Validation Loss:2.0141, Validation Accuracy:0.1626\n",
    "Epoch #183: Loss:1.2952, Accuracy:0.4587, Validation Loss:1.9982, Validation Accuracy:0.1757\n",
    "Epoch #184: Loss:1.2909, Accuracy:0.4682, Validation Loss:2.0207, Validation Accuracy:0.1724\n",
    "Epoch #185: Loss:1.2941, Accuracy:0.4608, Validation Loss:2.0071, Validation Accuracy:0.1741\n",
    "Epoch #186: Loss:1.2899, Accuracy:0.4571, Validation Loss:2.0544, Validation Accuracy:0.1609\n",
    "Epoch #187: Loss:1.2891, Accuracy:0.4628, Validation Loss:2.0145, Validation Accuracy:0.1757\n",
    "Epoch #188: Loss:1.2798, Accuracy:0.4690, Validation Loss:2.0396, Validation Accuracy:0.1675\n",
    "Epoch #189: Loss:1.2744, Accuracy:0.4715, Validation Loss:2.0511, Validation Accuracy:0.1691\n",
    "Epoch #190: Loss:1.2744, Accuracy:0.4735, Validation Loss:2.0318, Validation Accuracy:0.1642\n",
    "Epoch #191: Loss:1.2810, Accuracy:0.4567, Validation Loss:2.0448, Validation Accuracy:0.1626\n",
    "Epoch #192: Loss:1.2724, Accuracy:0.4743, Validation Loss:2.0883, Validation Accuracy:0.1708\n",
    "Epoch #193: Loss:1.2796, Accuracy:0.4620, Validation Loss:2.0501, Validation Accuracy:0.1757\n",
    "Epoch #194: Loss:1.2784, Accuracy:0.4674, Validation Loss:2.0647, Validation Accuracy:0.1642\n",
    "Epoch #195: Loss:1.2688, Accuracy:0.4632, Validation Loss:2.0394, Validation Accuracy:0.1790\n",
    "Epoch #196: Loss:1.2752, Accuracy:0.4665, Validation Loss:2.1153, Validation Accuracy:0.1658\n",
    "Epoch #197: Loss:1.2775, Accuracy:0.4710, Validation Loss:2.0077, Validation Accuracy:0.1724\n",
    "Epoch #198: Loss:1.2608, Accuracy:0.4776, Validation Loss:2.1039, Validation Accuracy:0.1708\n",
    "Epoch #199: Loss:1.2606, Accuracy:0.4805, Validation Loss:2.0463, Validation Accuracy:0.1741\n",
    "Epoch #200: Loss:1.2587, Accuracy:0.4743, Validation Loss:2.1119, Validation Accuracy:0.1741\n",
    "Epoch #201: Loss:1.2617, Accuracy:0.4665, Validation Loss:2.0507, Validation Accuracy:0.1806\n",
    "Epoch #202: Loss:1.2661, Accuracy:0.4756, Validation Loss:2.0951, Validation Accuracy:0.1675\n",
    "Epoch #203: Loss:1.2490, Accuracy:0.4780, Validation Loss:2.0730, Validation Accuracy:0.1675\n",
    "Epoch #204: Loss:1.2487, Accuracy:0.4838, Validation Loss:2.1104, Validation Accuracy:0.1790\n",
    "Epoch #205: Loss:1.2458, Accuracy:0.4850, Validation Loss:2.0850, Validation Accuracy:0.1675\n",
    "Epoch #206: Loss:1.2442, Accuracy:0.4830, Validation Loss:2.1172, Validation Accuracy:0.1741\n",
    "Epoch #207: Loss:1.2537, Accuracy:0.4776, Validation Loss:2.1087, Validation Accuracy:0.1724\n",
    "Epoch #208: Loss:1.2508, Accuracy:0.4834, Validation Loss:2.1194, Validation Accuracy:0.1741\n",
    "Epoch #209: Loss:1.2348, Accuracy:0.4891, Validation Loss:2.0952, Validation Accuracy:0.1806\n",
    "Epoch #210: Loss:1.2392, Accuracy:0.4949, Validation Loss:2.1059, Validation Accuracy:0.1626\n",
    "Epoch #211: Loss:1.2384, Accuracy:0.4977, Validation Loss:2.1029, Validation Accuracy:0.1642\n",
    "Epoch #212: Loss:1.2361, Accuracy:0.4965, Validation Loss:2.1280, Validation Accuracy:0.1658\n",
    "Epoch #213: Loss:1.2299, Accuracy:0.4940, Validation Loss:2.1086, Validation Accuracy:0.1691\n",
    "Epoch #214: Loss:1.2275, Accuracy:0.4994, Validation Loss:2.1145, Validation Accuracy:0.1741\n",
    "Epoch #215: Loss:1.2230, Accuracy:0.4998, Validation Loss:2.1137, Validation Accuracy:0.1724\n",
    "Epoch #216: Loss:1.2263, Accuracy:0.4961, Validation Loss:2.1419, Validation Accuracy:0.1724\n",
    "Epoch #217: Loss:1.2255, Accuracy:0.4932, Validation Loss:2.1338, Validation Accuracy:0.1856\n",
    "Epoch #218: Loss:1.2375, Accuracy:0.4830, Validation Loss:2.1454, Validation Accuracy:0.1741\n",
    "Epoch #219: Loss:1.2408, Accuracy:0.4895, Validation Loss:2.1175, Validation Accuracy:0.1642\n",
    "Epoch #220: Loss:1.2237, Accuracy:0.4940, Validation Loss:2.1089, Validation Accuracy:0.1757\n",
    "Epoch #221: Loss:1.2274, Accuracy:0.4994, Validation Loss:2.1166, Validation Accuracy:0.1741\n",
    "Epoch #222: Loss:1.2154, Accuracy:0.5039, Validation Loss:2.1278, Validation Accuracy:0.1757\n",
    "Epoch #223: Loss:1.2131, Accuracy:0.5018, Validation Loss:2.1406, Validation Accuracy:0.1806\n",
    "Epoch #224: Loss:1.2048, Accuracy:0.5068, Validation Loss:2.1594, Validation Accuracy:0.1856\n",
    "Epoch #225: Loss:1.2110, Accuracy:0.5092, Validation Loss:2.1265, Validation Accuracy:0.1773\n",
    "Epoch #226: Loss:1.2133, Accuracy:0.5043, Validation Loss:2.1828, Validation Accuracy:0.1856\n",
    "Epoch #227: Loss:1.2027, Accuracy:0.5031, Validation Loss:2.1166, Validation Accuracy:0.1757\n",
    "Epoch #228: Loss:1.2008, Accuracy:0.5080, Validation Loss:2.1795, Validation Accuracy:0.1790\n",
    "Epoch #229: Loss:1.1956, Accuracy:0.5084, Validation Loss:2.1406, Validation Accuracy:0.1708\n",
    "Epoch #230: Loss:1.1931, Accuracy:0.5097, Validation Loss:2.1534, Validation Accuracy:0.1724\n",
    "Epoch #231: Loss:1.2235, Accuracy:0.4895, Validation Loss:2.1413, Validation Accuracy:0.1658\n",
    "Epoch #232: Loss:1.1871, Accuracy:0.5224, Validation Loss:2.1949, Validation Accuracy:0.1741\n",
    "Epoch #233: Loss:1.1892, Accuracy:0.5158, Validation Loss:2.1384, Validation Accuracy:0.1856\n",
    "Epoch #234: Loss:1.1858, Accuracy:0.5203, Validation Loss:2.1802, Validation Accuracy:0.1823\n",
    "Epoch #235: Loss:1.1811, Accuracy:0.5187, Validation Loss:2.1881, Validation Accuracy:0.1757\n",
    "Epoch #236: Loss:1.1905, Accuracy:0.5187, Validation Loss:2.2210, Validation Accuracy:0.1839\n",
    "Epoch #237: Loss:1.1962, Accuracy:0.5158, Validation Loss:2.1739, Validation Accuracy:0.1790\n",
    "Epoch #238: Loss:1.2001, Accuracy:0.5060, Validation Loss:2.2451, Validation Accuracy:0.1724\n",
    "Epoch #239: Loss:1.1942, Accuracy:0.5088, Validation Loss:2.1703, Validation Accuracy:0.1593\n",
    "Epoch #240: Loss:1.1761, Accuracy:0.5269, Validation Loss:2.1753, Validation Accuracy:0.1888\n",
    "Epoch #241: Loss:1.1817, Accuracy:0.5211, Validation Loss:2.2372, Validation Accuracy:0.1675\n",
    "Epoch #242: Loss:1.1787, Accuracy:0.5150, Validation Loss:2.1613, Validation Accuracy:0.1675\n",
    "Epoch #243: Loss:1.1748, Accuracy:0.5261, Validation Loss:2.1886, Validation Accuracy:0.1987\n",
    "Epoch #244: Loss:1.1741, Accuracy:0.5220, Validation Loss:2.2158, Validation Accuracy:0.1642\n",
    "Epoch #245: Loss:1.1747, Accuracy:0.5322, Validation Loss:2.1887, Validation Accuracy:0.1839\n",
    "Epoch #246: Loss:1.1711, Accuracy:0.5281, Validation Loss:2.2034, Validation Accuracy:0.1839\n",
    "Epoch #247: Loss:1.1615, Accuracy:0.5331, Validation Loss:2.1991, Validation Accuracy:0.1790\n",
    "Epoch #248: Loss:1.1609, Accuracy:0.5355, Validation Loss:2.2449, Validation Accuracy:0.1823\n",
    "Epoch #249: Loss:1.1651, Accuracy:0.5306, Validation Loss:2.2228, Validation Accuracy:0.1741\n",
    "Epoch #250: Loss:1.1594, Accuracy:0.5314, Validation Loss:2.2072, Validation Accuracy:0.1839\n",
    "Epoch #251: Loss:1.1590, Accuracy:0.5207, Validation Loss:2.2044, Validation Accuracy:0.1691\n",
    "Epoch #252: Loss:1.1589, Accuracy:0.5343, Validation Loss:2.2151, Validation Accuracy:0.1741\n",
    "Epoch #253: Loss:1.1432, Accuracy:0.5450, Validation Loss:2.2335, Validation Accuracy:0.1806\n",
    "Epoch #254: Loss:1.1449, Accuracy:0.5474, Validation Loss:2.2288, Validation Accuracy:0.1823\n",
    "Epoch #255: Loss:1.1481, Accuracy:0.5326, Validation Loss:2.2558, Validation Accuracy:0.1757\n",
    "Epoch #256: Loss:1.1800, Accuracy:0.5138, Validation Loss:2.2031, Validation Accuracy:0.1839\n",
    "Epoch #257: Loss:1.1519, Accuracy:0.5326, Validation Loss:2.1999, Validation Accuracy:0.1856\n",
    "Epoch #258: Loss:1.1455, Accuracy:0.5396, Validation Loss:2.2524, Validation Accuracy:0.1773\n",
    "Epoch #259: Loss:1.1651, Accuracy:0.5372, Validation Loss:2.2249, Validation Accuracy:0.1691\n",
    "Epoch #260: Loss:1.1430, Accuracy:0.5372, Validation Loss:2.2544, Validation Accuracy:0.1806\n",
    "Epoch #261: Loss:1.1397, Accuracy:0.5417, Validation Loss:2.2288, Validation Accuracy:0.1757\n",
    "Epoch #262: Loss:1.1395, Accuracy:0.5450, Validation Loss:2.2414, Validation Accuracy:0.1888\n",
    "Epoch #263: Loss:1.1488, Accuracy:0.5331, Validation Loss:2.2734, Validation Accuracy:0.1757\n",
    "Epoch #264: Loss:1.1306, Accuracy:0.5458, Validation Loss:2.2416, Validation Accuracy:0.1741\n",
    "Epoch #265: Loss:1.1237, Accuracy:0.5585, Validation Loss:2.2789, Validation Accuracy:0.1872\n",
    "Epoch #266: Loss:1.1135, Accuracy:0.5655, Validation Loss:2.2622, Validation Accuracy:0.1773\n",
    "Epoch #267: Loss:1.1225, Accuracy:0.5491, Validation Loss:2.3150, Validation Accuracy:0.1741\n",
    "Epoch #268: Loss:1.1207, Accuracy:0.5520, Validation Loss:2.2507, Validation Accuracy:0.1888\n",
    "Epoch #269: Loss:1.1318, Accuracy:0.5470, Validation Loss:2.2726, Validation Accuracy:0.1872\n",
    "Epoch #270: Loss:1.1295, Accuracy:0.5458, Validation Loss:2.2684, Validation Accuracy:0.1708\n",
    "Epoch #271: Loss:1.1107, Accuracy:0.5548, Validation Loss:2.3211, Validation Accuracy:0.1790\n",
    "Epoch #272: Loss:1.1213, Accuracy:0.5503, Validation Loss:2.2595, Validation Accuracy:0.1757\n",
    "Epoch #273: Loss:1.1113, Accuracy:0.5598, Validation Loss:2.2601, Validation Accuracy:0.1872\n",
    "Epoch #274: Loss:1.1099, Accuracy:0.5598, Validation Loss:2.3501, Validation Accuracy:0.1757\n",
    "Epoch #275: Loss:1.1169, Accuracy:0.5540, Validation Loss:2.2657, Validation Accuracy:0.1921\n",
    "Epoch #276: Loss:1.1138, Accuracy:0.5577, Validation Loss:2.2814, Validation Accuracy:0.1921\n",
    "Epoch #277: Loss:1.1149, Accuracy:0.5614, Validation Loss:2.2937, Validation Accuracy:0.1724\n",
    "Epoch #278: Loss:1.1304, Accuracy:0.5524, Validation Loss:2.2934, Validation Accuracy:0.1839\n",
    "Epoch #279: Loss:1.1368, Accuracy:0.5470, Validation Loss:2.2738, Validation Accuracy:0.1741\n",
    "Epoch #280: Loss:1.1376, Accuracy:0.5458, Validation Loss:2.2802, Validation Accuracy:0.1741\n",
    "Epoch #281: Loss:1.1484, Accuracy:0.5294, Validation Loss:2.2643, Validation Accuracy:0.1823\n",
    "Epoch #282: Loss:1.1267, Accuracy:0.5544, Validation Loss:2.2824, Validation Accuracy:0.1741\n",
    "Epoch #283: Loss:1.1031, Accuracy:0.5647, Validation Loss:2.2947, Validation Accuracy:0.1757\n",
    "Epoch #284: Loss:1.0914, Accuracy:0.5745, Validation Loss:2.2808, Validation Accuracy:0.1691\n",
    "Epoch #285: Loss:1.0894, Accuracy:0.5721, Validation Loss:2.3004, Validation Accuracy:0.1790\n",
    "Epoch #286: Loss:1.0828, Accuracy:0.5754, Validation Loss:2.3263, Validation Accuracy:0.1708\n",
    "Epoch #287: Loss:1.0833, Accuracy:0.5741, Validation Loss:2.2999, Validation Accuracy:0.1839\n",
    "Epoch #288: Loss:1.0815, Accuracy:0.5733, Validation Loss:2.3011, Validation Accuracy:0.1872\n",
    "Epoch #289: Loss:1.0879, Accuracy:0.5643, Validation Loss:2.3195, Validation Accuracy:0.1872\n",
    "Epoch #290: Loss:1.0851, Accuracy:0.5799, Validation Loss:2.3284, Validation Accuracy:0.1823\n",
    "Epoch #291: Loss:1.0730, Accuracy:0.5864, Validation Loss:2.3425, Validation Accuracy:0.1839\n",
    "Epoch #292: Loss:1.0749, Accuracy:0.5770, Validation Loss:2.3296, Validation Accuracy:0.1872\n",
    "Epoch #293: Loss:1.0711, Accuracy:0.5778, Validation Loss:2.3271, Validation Accuracy:0.1773\n",
    "Epoch #294: Loss:1.0674, Accuracy:0.5893, Validation Loss:2.4031, Validation Accuracy:0.1839\n",
    "Epoch #295: Loss:1.0672, Accuracy:0.5869, Validation Loss:2.3086, Validation Accuracy:0.1806\n",
    "Epoch #296: Loss:1.0676, Accuracy:0.5836, Validation Loss:2.3286, Validation Accuracy:0.1773\n",
    "Epoch #297: Loss:1.0708, Accuracy:0.5786, Validation Loss:2.3882, Validation Accuracy:0.1642\n",
    "Epoch #298: Loss:1.0666, Accuracy:0.5836, Validation Loss:2.3150, Validation Accuracy:0.1856\n",
    "Epoch #299: Loss:1.0718, Accuracy:0.5840, Validation Loss:2.3615, Validation Accuracy:0.1856\n",
    "Epoch #300: Loss:1.0879, Accuracy:0.5630, Validation Loss:2.3571, Validation Accuracy:0.1806\n",
    "\n",
    "Test:\n",
    "Test Loss:2.35707188, Accuracy:0.1806\n",
    "Labels: ['03', '05', '01', '02', '04']\n",
    "Confusion Matrix:\n",
    "      03  05  01  02  04\n",
    "t:03  13  24  14  45  19\n",
    "t:05  25  27  21  50  19\n",
    "t:01  15  33  13  44  21\n",
    "t:02  15  28  12  41  18\n",
    "t:04  18  22  17  39  16\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.15      0.11      0.13       115\n",
    "          05       0.20      0.19      0.20       142\n",
    "          01       0.17      0.10      0.13       126\n",
    "          02       0.19      0.36      0.25       114\n",
    "          04       0.17      0.14      0.16       112\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.18      0.18      0.17       609\n",
    "weighted avg       0.18      0.18      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 08:38:32 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6061982439069324, 1.606139868742531, 1.605611556856503, 1.6055788265660478, 1.6052199936852667, 1.605174646393223, 1.6049518538226049, 1.6049765628351171, 1.6050415702641303, 1.6046629333730988, 1.604559580680772, 1.6044725410652474, 1.6044267333787063, 1.604374178916167, 1.60432310448883, 1.603946260435045, 1.6041831451488051, 1.6040970950291074, 1.6040701513807174, 1.6042438463624475, 1.6039543439602029, 1.6041149176987521, 1.6042262934307354, 1.6037017565055434, 1.6037268505503588, 1.6037732940197773, 1.6039545434253362, 1.6033900700179227, 1.603365114365501, 1.603630328021809, 1.6033884345604281, 1.6037789375715459, 1.6036287172283055, 1.6034592987085603, 1.603864966159188, 1.603688374332998, 1.6043642130978588, 1.6042466451381814, 1.604448681785947, 1.6049294023482474, 1.6053569363843043, 1.606993721623726, 1.605317994878797, 1.6056065539812612, 1.6062257340780424, 1.607398379220947, 1.6083532650090986, 1.6109180939804353, 1.6136547289849894, 1.6165088197867858, 1.6240303289322626, 1.6206891295945116, 1.622704783879673, 1.6313878113804583, 1.627017046626174, 1.6310218177209739, 1.630590996718759, 1.6351691932709542, 1.6448460179205207, 1.6399663091684602, 1.6409490709430088, 1.6422408004895415, 1.6451549393007125, 1.6556700354530698, 1.6511190857597564, 1.6555293123123094, 1.662501368812348, 1.6648697880492813, 1.6640965834822756, 1.6753070738123752, 1.6686964401079125, 1.6745582106469692, 1.6706804281776566, 1.6744745057596166, 1.6831539170495395, 1.689550158425505, 1.7007290126850647, 1.699790947934481, 1.6989645278708296, 1.7202143990151793, 1.7028413016612112, 1.7087547683167732, 1.7194514345065715, 1.7258041670365483, 1.7288165333235792, 1.7352940370688101, 1.7301374094435342, 1.728079411588083, 1.7396178593972242, 1.7398259248248071, 1.7460015184186362, 1.7533994719312695, 1.752650500518348, 1.7543477552081956, 1.76382565694098, 1.765400753232646, 1.765427009029733, 1.7782145746431524, 1.7778256436678381, 1.7790409087743273, 1.792467732539122, 1.7711641641673197, 1.7791167043504261, 1.777501623031541, 1.7880714039497188, 1.7999446018184544, 1.7934869179388964, 1.8141513598963546, 1.8008572268368574, 1.8156599013871944, 1.8183679195068936, 1.8165865819442448, 1.8461861134749915, 1.8271735358512264, 1.8127225504328661, 1.8489246158960027, 1.7910801734047375, 1.8108921806604796, 1.836036122882699, 1.8236597952584328, 1.8499921664032835, 1.846448058369516, 1.837337120021701, 1.8518781902754835, 1.8490284916215343, 1.8499100094749814, 1.8705330084893113, 1.8716204099858726, 1.8736045327288373, 1.8808101075036185, 1.8681972582743478, 1.8796910891196215, 1.8806558829810232, 1.8918287211842528, 1.8716640446965134, 1.8705142493709945, 1.888190467760872, 1.8845543262406523, 1.8971492408335895, 1.8999013196071382, 1.907286719344128, 1.9123722040790252, 1.9093057473109079, 1.915166398183074, 1.8933175545803627, 1.9233034811975138, 1.913996452964194, 1.9132689235637146, 1.9260929345301612, 1.903414293071515, 1.9416702361333937, 1.9249232537640726, 1.931356117838905, 1.9375598003907353, 1.9362206220235338, 1.9461125452530208, 1.9586536232473815, 1.9434237619143206, 1.9373711639241435, 1.936967222757136, 1.9505258585236147, 1.9627499979705059, 1.9818981249735665, 1.9656525820934128, 1.9620729530190404, 1.9526862918058248, 1.9839750267993446, 1.977050215152684, 1.985703399811668, 1.9840689392512656, 1.9732062736364029, 1.9820729969757531, 1.9753743690027197, 1.990742734696086, 1.965810156416619, 1.9859319042493948, 1.982635210300314, 2.0172909308341143, 1.9883563992229394, 2.0297403456933782, 2.010144739119682, 2.0140991559365307, 1.998162504095945, 2.020741070041124, 2.0071083990419636, 2.0544224478341087, 2.0144684749283814, 2.0395961672997434, 2.051136011756308, 2.0318494187591503, 2.044791609587145, 2.0883064555808635, 2.050061611902146, 2.064727369396166, 2.0393874574764608, 2.1153016141287018, 2.0076858304404275, 2.1038698288803226, 2.046344044956276, 2.111911276486903, 2.05073567094474, 2.0951261594769206, 2.0730290769160478, 2.110427969977969, 2.0849973360697427, 2.117184225170092, 2.1086565152373415, 2.119386778284959, 2.0951968652665713, 2.1059461869238243, 2.1029015456514406, 2.12800399462382, 2.108554098210703, 2.1144944478334073, 2.113718575444715, 2.141910214338005, 2.133782696449894, 2.1454176530853672, 2.1174660265347836, 2.1088527427322563, 2.116615944503759, 2.1277648106780154, 2.1405718177801676, 2.159366752126534, 2.1265182428563563, 2.182803751016877, 2.1165889115952115, 2.1795037442631715, 2.1406041205614463, 2.1533604576474143, 2.141320579744912, 2.194864623848049, 2.138444678145285, 2.1802007885793553, 2.1880778240648593, 2.221016462604792, 2.1739273059544306, 2.2450880283988366, 2.17029829682975, 2.1752968437370215, 2.2372297054441104, 2.1613000688098727, 2.1886188862554743, 2.215772720393289, 2.1886805239177884, 2.2033741196192347, 2.1990645996651232, 2.244929918123192, 2.2228400734649307, 2.207228225635973, 2.2043524837650494, 2.2150692951503057, 2.233518265738276, 2.228800712156374, 2.255807073245495, 2.2030693408107913, 2.199906006626699, 2.252366944878364, 2.224896475990809, 2.254415268185495, 2.2288350812320052, 2.241411716871465, 2.273426539009232, 2.241602824043562, 2.2789186812778217, 2.262229651653121, 2.31501459174947, 2.2506573321588323, 2.2726469079066183, 2.2684292315653782, 2.321117735065654, 2.259522576245964, 2.2601231792681715, 2.35010287287983, 2.2657413592283753, 2.281430599920464, 2.293688900169285, 2.2933958849100446, 2.2737627174270956, 2.2802280479268293, 2.2642770557372245, 2.282386541366577, 2.294650722607016, 2.2807573489171924, 2.300448121695683, 2.326282481646107, 2.2998611954436905, 2.3010974764236676, 2.3195320996157642, 2.3284443607079766, 2.3425012523513318, 2.3295530912715616, 2.3271237680281716, 2.403060955367065, 2.3086493085757853, 2.3286178425223567, 2.388208011492524, 2.314959353805567, 2.3615200038026707, 2.357071944253981], 'val_acc': [0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.24137931032035934, 0.24137931032035934, 0.24137931032035934, 0.24137930804481256, 0.24137931032035934, 0.24302134634728112, 0.2364532019459751, 0.2430213441696073, 0.2430213441696073, 0.24466338256994882, 0.23809523797289686, 0.2397372740976916, 0.23809523797289686, 0.2430213441696073, 0.23809523797289686, 0.23645320184810212, 0.24466338256994882, 0.25944170541755474, 0.251231524793581, 0.24466338029440204, 0.2545155970431705, 0.24302134644515408, 0.25944170541755474, 0.25287356091837576, 0.23316912940276668, 0.25287356091837576, 0.2430213442674803, 0.2545155971410435, 0.2282430211262554, 0.2397372719200178, 0.23481116354563358, 0.22988505734892314, 0.22331691076994334, 0.23481116364350654, 0.2315270933758449, 0.2183908023955591, 0.22167487474302158, 0.20032840690687176, 0.2331691275187118, 0.21346469411904784, 0.2298850552691223, 0.2003284050228169, 0.21346469600310272, 0.21182265809212608, 0.22003284069802764, 0.21018062196733134, 0.2101806216737124, 0.2085385875308455, 0.20525451329932815, 0.21346469619884867, 0.20197044104973866, 0.19868637087995, 0.21346469610097568, 0.20525451378869305, 0.20853858554891766, 0.20689654971774185, 0.21018062196733134, 0.2134646939233019, 0.21839080278705103, 0.2118226578963801, 0.22331691125930825, 0.22167487474302158, 0.19868636899589515, 0.21346469411904784, 0.20853858554891766, 0.22660098301953283, 0.19868637107569595, 0.21346469451053976, 0.20032840521856285, 0.21018062206520433, 0.2101806216737124, 0.21674876636863732, 0.22495894709048403, 0.19047619045172223, 0.22003283852035385, 0.2118226578963801, 0.21018062177158536, 0.19868636880014917, 0.21510673014596962, 0.1871921179085138, 0.19540229882610646, 0.22988505517124935, 0.19047619035384925, 0.2134646939233019, 0.19868637078207702, 0.19868636899589515, 0.1822660094362566, 0.22495894907241187, 0.19868636889802216, 0.1904761901581033, 0.19211822657651698, 0.19211822638077103, 0.21510673212789747, 0.19047619035384925, 0.18883415403330855, 0.18062397360508078, 0.17077175656269336, 0.19540229872823348, 0.17077175656269336, 0.1904761901581033, 0.18226600972987553, 0.1724137927853611, 0.2036124794500802, 0.18062397142740697, 0.18555008188159203, 0.17569786503495058, 0.17898193738241305, 0.185550082077338, 0.16748768411735793, 0.17405582900802882, 0.1691297205357716, 0.1773399013554913, 0.16748768411735793, 0.17077175695418528, 0.17405582891015584, 0.1806239734093348, 0.17405582910590178, 0.18390804377486944, 0.17569786483920463, 0.17898193738241305, 0.16256157574297367, 0.17569786483920463, 0.1674876842152309, 0.1773399012576183, 0.16912972043789862, 0.17077175666056635, 0.17898193728454007, 0.1609195397160519, 0.17898193748028604, 0.1724137926874881, 0.19540229863036052, 0.1773399013554913, 0.15927750368913016, 0.17241379298110704, 0.18390804546317835, 0.17569786503495058, 0.17241379258961514, 0.18226600963200254, 0.17405582881228285, 0.1724137927853611, 0.18555008178371907, 0.17405582891015584, 0.17405582900802882, 0.16256157544935473, 0.16912972043789862, 0.18226600953412955, 0.17898193699092113, 0.17733990096399938, 0.18062397331146185, 0.16912972024215267, 0.16584564809043614, 0.17405582891015584, 0.18555008178371907, 0.17898193699092113, 0.1822660076500747, 0.17077175666056635, 0.16912972043789862, 0.19047618986448436, 0.15927750349338418, 0.16420361176989545, 0.1707717563669474, 0.14942528654886975, 0.18390804575679728, 0.17241379239386917, 0.1773399008661264, 0.1625615756451007, 0.1756978649370776, 0.17241379229599618, 0.17405582891015584, 0.16091953942243298, 0.17569786483920463, 0.16748768411735793, 0.16912972014427968, 0.16420361176989545, 0.1625615756451007, 0.17077175617120144, 0.17569786464345866, 0.16420361176989545, 0.17898193699092113, 0.1658456478946902, 0.1724137927853611, 0.17077175646482037, 0.1740558287144099, 0.1740558285186639, 0.18062397331146185, 0.16748768401948494, 0.16748768441097686, 0.17898193699092113, 0.16748768450884982, 0.17405582891015584, 0.17241379249174216, 0.17405582842079093, 0.18062397321358886, 0.1625615756451007, 0.1642036118677684, 0.16584564769894422, 0.16912972014427968, 0.1740558286165369, 0.17241379258961514, 0.17241379239386917, 0.1855500814901001, 0.1740558285186639, 0.1642036120635144, 0.1756978649370776, 0.1740558286165369, 0.1756978649370776, 0.1806239730178429, 0.18555008168584608, 0.17733990096399938, 0.1855500815879731, 0.17569786513282357, 0.17898193708879412, 0.17077175656269336, 0.17241379249174216, 0.1658456478946902, 0.1740558285186639, 0.18555008197946501, 0.1822660093383836, 0.17569786503495058, 0.1839080456589243, 0.17898193689304814, 0.17241379239386917, 0.15927750359125717, 0.18883415413118151, 0.16748768401948494, 0.16748768411735793, 0.19868637068420403, 0.16420361167202246, 0.18390804556105134, 0.18390804556105134, 0.1789819371866671, 0.18226600953412955, 0.1740558286165369, 0.18390804556105134, 0.16912972024215267, 0.1740558287144099, 0.18062397311571587, 0.18226600924051062, 0.1756978649370776, 0.1839080456589243, 0.1855500814901001, 0.1773399007682534, 0.16912972024215267, 0.18062397311571587, 0.17569786483920463, 0.18883415383756258, 0.1756978644477127, 0.1740558287144099, 0.18719211781064082, 0.17733990096399938, 0.1740558286165369, 0.18883415393543557, 0.18719211761489485, 0.1707717563669474, 0.17898193699092113, 0.17569786474133164, 0.18719211771276784, 0.17569786474133164, 0.19211822618502505, 0.19211822608715207, 0.17241379249174216, 0.18390804516955941, 0.1740558286165369, 0.17405582881228285, 0.18226600924051062, 0.17405582842079093, 0.17569786474133164, 0.16912972014427968, 0.17898193699092113, 0.17077175607332848, 0.18390804556105134, 0.18719211771276784, 0.1871921179085138, 0.1822660093383836, 0.18390804556105134, 0.1871921179085138, 0.17733990115974532, 0.18390804516955941, 0.1806239734093348, 0.17733990106187233, 0.16420361167202246, 0.18555008168584608, 0.1855500815879731, 0.18062397321358886], 'loss': [1.6087759527582408, 1.6065360036963554, 1.6059731778178128, 1.6055002581167515, 1.605036516307071, 1.6049922962208303, 1.6046615856139321, 1.6044603809438938, 1.6042449436148578, 1.6040506287521894, 1.6036819174549173, 1.603412128914553, 1.6031175096421761, 1.6028238360641918, 1.6027606857875534, 1.602738868529302, 1.6020084420268785, 1.6017898160574129, 1.601396489094415, 1.6016154642222598, 1.6004967814843023, 1.6004276879269483, 1.6001743793487548, 1.5999086152356752, 1.5990422428511006, 1.5988071792179555, 1.5987308607943493, 1.5976044605889603, 1.597755260487112, 1.597166594000078, 1.5965093571547366, 1.5963509908691813, 1.5954177014392015, 1.5949035937781206, 1.5952279124661393, 1.594308586140188, 1.5935872265445623, 1.5927026735928513, 1.5918965858110903, 1.5913524026498658, 1.5929285698113256, 1.5903959283593743, 1.5908059057024226, 1.5870830555960873, 1.5859725949700607, 1.5845511917705652, 1.5820687902536725, 1.580702336955609, 1.5776171966744643, 1.579019497650115, 1.5757666420398062, 1.5745567102451834, 1.5754286947681184, 1.5699461092449556, 1.5712792446970696, 1.568939037440494, 1.5654325656577548, 1.5644494142375687, 1.5597231291157998, 1.5631593486856385, 1.5610173079022636, 1.5552985264780095, 1.554085591196769, 1.5509025186483865, 1.5500488795783731, 1.5468446636591604, 1.5464531649799074, 1.5420434773090683, 1.5424564309188717, 1.5421450042626696, 1.5365389276578931, 1.5416924441374793, 1.5342165865202948, 1.5286825875237247, 1.5247137264549364, 1.5208336193703529, 1.5235623188332121, 1.5246772246194327, 1.516521804190759, 1.5119289523032657, 1.5162004020424593, 1.5079655705046604, 1.5056004140411314, 1.5025152507014343, 1.5025606273870449, 1.5019939877414117, 1.5014012480173757, 1.4998575575297863, 1.4896409160547432, 1.4872511909238122, 1.4881809827972976, 1.4862870873856593, 1.4824720091888302, 1.4873151588244116, 1.4773460240824268, 1.4733939766149502, 1.479325398723203, 1.4750869513047549, 1.471947468575511, 1.4704806211303147, 1.472667830093196, 1.4745470207574676, 1.4671955172775708, 1.4606390917325656, 1.4640296100346215, 1.4531866642973505, 1.4481623563433574, 1.4472979153940566, 1.4485063736933212, 1.4495591769228235, 1.4481941297558543, 1.4371007436110008, 1.4334319317854896, 1.4377788847232011, 1.4523317683159203, 1.4595242734807228, 1.4517646098773338, 1.4468319288269451, 1.4314953222901425, 1.4430313704194964, 1.428619165538028, 1.423421911439367, 1.4139639251286, 1.419834027838658, 1.4176343676490706, 1.4108223449033388, 1.4068866209817372, 1.4063803599355647, 1.4020872778961055, 1.3983866942736647, 1.4027621290766972, 1.4010478516623714, 1.4023729831531062, 1.402212055801611, 1.4018123287929403, 1.3991495005648729, 1.407274913053493, 1.3919753762486045, 1.3827824232269852, 1.3821251528463814, 1.3814782553384926, 1.3801321860945934, 1.376508002310563, 1.375697807364885, 1.3765801020961033, 1.3748817398807596, 1.3760129335724598, 1.3816248570624319, 1.365856148330093, 1.3618679179792776, 1.3580294861196247, 1.3601703798501643, 1.363351793945203, 1.3518962266753587, 1.3511594181432862, 1.3477298950022985, 1.3514989802969555, 1.3643503577802216, 1.3561014818704593, 1.3464338833301708, 1.3383570434131662, 1.3368805419248233, 1.342914582033177, 1.3385437185514635, 1.3290508492526578, 1.3291571547608112, 1.3227050846851827, 1.3324788985066345, 1.3453702954051432, 1.340713522860157, 1.3413601709342347, 1.349747320220211, 1.3362495877169975, 1.3335679918344017, 1.3298885013043757, 1.3169529585378126, 1.3149914468338357, 1.3129332820003283, 1.3087221439858971, 1.3040843175421994, 1.3169097503352705, 1.3080214707024043, 1.2952452060867874, 1.2908971192166057, 1.2941165321416679, 1.289893710392946, 1.2890848437863454, 1.2797538005351041, 1.2744440321560024, 1.2744050643282505, 1.2809973928228295, 1.2724060745454666, 1.2796383393129038, 1.278391998553423, 1.268800601332584, 1.2752464926952698, 1.277490790570786, 1.2607974402958362, 1.2606114126084031, 1.2587321698543228, 1.2616746549488826, 1.2660729892444806, 1.2490067401705827, 1.248712110911062, 1.2458285070297899, 1.2442359797029279, 1.2536645232774393, 1.250847405182997, 1.2348323506251497, 1.2391961344458484, 1.2383624678030152, 1.2360681437369, 1.2299271485154388, 1.2275360735534888, 1.2230169222340201, 1.2263400731390262, 1.2254631342094782, 1.237472418152576, 1.2407781231329915, 1.2237214612275422, 1.2274464435890715, 1.2154144152968327, 1.2131327997242891, 1.204836962453149, 1.2110248798707182, 1.2132781163378172, 1.202660702777839, 1.2008143937073694, 1.1956489549280438, 1.1930527230307797, 1.2234688796546669, 1.1871222427493493, 1.1892022191621439, 1.1857834005257921, 1.1810917049952356, 1.1905180233949508, 1.1961787651206923, 1.2000700291666897, 1.1941730603545109, 1.1760960634239401, 1.1816729922558982, 1.1787001928509628, 1.1748158730520606, 1.1740533417010455, 1.1746560500632566, 1.171128022842094, 1.1615128518130011, 1.160884098301678, 1.1650988655658228, 1.1594221558169417, 1.1590261006991722, 1.1589303722127018, 1.1431953315617367, 1.144912419475814, 1.148138089248532, 1.1800228612868449, 1.1519431236098678, 1.1455479458372206, 1.1651336798188132, 1.1429775641928952, 1.1397477527418665, 1.1394968766206588, 1.148833969584236, 1.1306048639011579, 1.123652315188727, 1.1135313000277571, 1.1224880612116819, 1.1206522592039323, 1.1318317936186428, 1.1295484277501977, 1.1107371975019482, 1.1212546501805896, 1.1113165943040006, 1.1099267043127417, 1.1169450557696992, 1.1137681682007023, 1.1148988739910557, 1.1303525996159234, 1.1367758242746153, 1.137646382545299, 1.1484234185679003, 1.1267319920126662, 1.1030952707208401, 1.091443454068789, 1.0894248307852774, 1.082787052904556, 1.0832990169525147, 1.081470126929469, 1.0879299057582565, 1.0850505056322477, 1.07304575139737, 1.074904989608749, 1.0710880187013065, 1.0674252566860443, 1.067214565600213, 1.0676193503628522, 1.070759653555539, 1.0665942884323778, 1.071766733290968, 1.0878817390857047], 'acc': [0.20657084195643236, 0.23285420929871545, 0.23285420912124782, 0.23285421049203225, 0.23285420929871545, 0.23285421029620593, 0.2328542098861945, 0.23285420931707418, 0.23285420851541005, 0.2328542091028891, 0.2328542083195837, 0.23285420951290053, 0.2328542102778472, 0.23285420951290053, 0.23285420851541005, 0.23285420990455322, 0.2353182759862661, 0.2377823401464329, 0.23655030962378093, 0.2406570830629102, 0.23983572980339277, 0.23696098724185075, 0.2435318267810516, 0.2435318287393151, 0.24394250537825315, 0.24353182815183605, 0.24476386122023056, 0.24435318280049662, 0.24517453803663625, 0.24599589327277588, 0.24681724792143647, 0.24394250520078553, 0.24845995937284748, 0.24517453825082133, 0.25215605773964944, 0.24681724909639458, 0.2468172492922209, 0.2513347019160308, 0.2550308000870064, 0.2509240254912778, 0.24887063738256998, 0.25256673516189293, 0.2533880907529678, 0.26036961051105717, 0.267351130391538, 0.26242299936635294, 0.2689938389055538, 0.2694045177169404, 0.27392197204811125, 0.27063654894946293, 0.2710472275833819, 0.28090348861790293, 0.26735112780907805, 0.2821355216679387, 0.27351129208012526, 0.28418891351570585, 0.2825462026517739, 0.28459958897968585, 0.29363449751963605, 0.2821355256211831, 0.27063655231522826, 0.2862422992194213, 0.28870636473201383, 0.29568788564670256, 0.29897330498793284, 0.29938398162687097, 0.3014373720671362, 0.29445585177664396, 0.30554414785618167, 0.30143737288715905, 0.3133470204454183, 0.30020533803796867, 0.3133470224036818, 0.32114989711029085, 0.3137576998259253, 0.3223819298053914, 0.3227926101650301, 0.31622176514269146, 0.3264887083360057, 0.34086242316929466, 0.3281314178291532, 0.3351129347538801, 0.34209445465271965, 0.34414784176393703, 0.32977412653899535, 0.33511293714051377, 0.34045174656707405, 0.33798767968369703, 0.34537987700478007, 0.35318275347382627, 0.3503080105757077, 0.34579055638528705, 0.35934291539495733, 0.354825461400363, 0.36221765813396695, 0.3663244372887778, 0.36016427063109696, 0.35277207569664754, 0.36468172325490683, 0.3527720753049948, 0.34784394130569707, 0.35318275253141196, 0.36221765852561966, 0.3720739224975359, 0.36262833755119134, 0.3745379890259776, 0.37535934445794356, 0.37782340801227265, 0.3753593436746382, 0.37494866488161027, 0.3737166318315745, 0.3868583173355283, 0.3885010258862615, 0.3786447626609332, 0.35811088214909514, 0.373305954409331, 0.3507186865904493, 0.37577002270020987, 0.38850102823617766, 0.377412728827592, 0.3889117050709421, 0.3827515388416314, 0.39548254833084356, 0.3811088319309438, 0.3958932221914953, 0.39999999899638994, 0.4036960973264745, 0.3995893219657992, 0.39876796907957573, 0.404517453970116, 0.4020533884575235, 0.4028747413070295, 0.3995893201666446, 0.3979466118851726, 0.3934291570705555, 0.40164270923612544, 0.3921971251954778, 0.4127310039448787, 0.4090349099229738, 0.4160164286101378, 0.412320327501767, 0.41560574946217466, 0.41601642488943724, 0.41930184798808556, 0.42053388322892865, 0.4143737187620551, 0.41232033028005327, 0.4053388105770401, 0.415195073606542, 0.42669404667995303, 0.42012320283257254, 0.4213552366659137, 0.42381930178685356, 0.43039014269683884, 0.42587269226383623, 0.4287474350028459, 0.42012320302839884, 0.4147843928552506, 0.42505133483688934, 0.4221765937012079, 0.4361396297781864, 0.4349075990780668, 0.4242299796007497, 0.4279260791792272, 0.4394250506860275, 0.4451745379264839, 0.44229979362086347, 0.4283367571522323, 0.4279260789834009, 0.43696098337428035, 0.4377823400179219, 0.43162217594270097, 0.444763859133456, 0.43778234315114345, 0.4394250522526383, 0.4447638610917195, 0.4459958915960128, 0.4501026679358198, 0.4464065698015616, 0.4480492836028888, 0.43531827705107184, 0.4603696081917389, 0.45872689893113516, 0.46817248493004626, 0.46078028698476675, 0.45708418685552765, 0.4628336772659231, 0.46899384114531767, 0.4714579062662575, 0.47351129513991197, 0.45667351025330705, 0.47433264978857254, 0.4620123184681918, 0.46735112867805745, 0.463244352301533, 0.466529773870288, 0.47104722766905593, 0.4776180697172819, 0.48049281402290234, 0.47433265057187796, 0.46652977226695974, 0.47556468162693283, 0.47802874576874094, 0.4837782353591136, 0.48501026785838774, 0.48295688055134406, 0.4776180709289574, 0.48336755813269644, 0.4891170410649732, 0.49486652850125606, 0.49774127672340346, 0.4965092411643426, 0.49404517447679197, 0.49938398210419765, 0.4997946612888783, 0.4960985605721601, 0.493223819044826, 0.48295687918055963, 0.4895277212287856, 0.4940451744400745, 0.49938398351169955, 0.5039014359029657, 0.5018480470293112, 0.5067761829501549, 0.5092402496377056, 0.5043121139126882, 0.5030800818417841, 0.5080082142377537, 0.5084188912683444, 0.5096509239267275, 0.48952772083713286, 0.5223819310660235, 0.5158110889810801, 0.5203285429756744, 0.5186858328950479, 0.5186858327359389, 0.5158110897643855, 0.5059548277140153, 0.508829569865546, 0.5268993856480969, 0.5211498989951194, 0.5149897327658087, 0.5260780331535261, 0.5219712546229117, 0.5322381956621361, 0.5281314190897853, 0.5330595508982758, 0.5355236140609522, 0.5305954838190725, 0.5314168331069868, 0.5207392192229598, 0.5342915770209545, 0.5449692028014322, 0.5474332687056774, 0.5326488713219425, 0.5137577028489945, 0.5326488744551641, 0.5396303886750395, 0.5371663282539321, 0.5371663286455848, 0.5416837810735683, 0.5449691996682107, 0.533059548156707, 0.5457905588208772, 0.5585215566829, 0.5655030835090966, 0.5490759780029986, 0.5519507207420082, 0.547022591283434, 0.5457905560793084, 0.5548254575327927, 0.5503080047131564, 0.5597535899287621, 0.5597535958769875, 0.5540041026883057, 0.5577002008592813, 0.5613963065450931, 0.5523613977725991, 0.5470225899126495, 0.5457905503269094, 0.5293634499857313, 0.5544147803063755, 0.5646817207581208, 0.5745379914982852, 0.5720739259856927, 0.5753593394154151, 0.5741273146635208, 0.5733059519125451, 0.5642710427483983, 0.579876792039225, 0.5864476428139626, 0.5770020532901772, 0.5778234043405286, 0.5893223782339625, 0.5868583150712862, 0.5835728998791265, 0.5786447674831571, 0.5835728925601168, 0.5839835697865339, 0.5630390100900152]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
