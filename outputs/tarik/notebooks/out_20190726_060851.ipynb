{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf27.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 06:08:51 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '0', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001DF54414E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001DF4BCA6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0896, Accuracy:0.3729, Validation Loss:1.0855, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0826, Accuracy:0.3729, Validation Loss:1.0798, Validation Accuracy:0.3727\n",
    "Epoch #3: Loss:1.0782, Accuracy:0.3729, Validation Loss:1.0764, Validation Accuracy:0.3727\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3749, Validation Loss:1.0752, Validation Accuracy:0.3974\n",
    "Epoch #5: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #12: Loss:1.0733, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #13: Loss:1.0729, Accuracy:0.3926, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #14: Loss:1.0728, Accuracy:0.3930, Validation Loss:1.0734, Validation Accuracy:0.4007\n",
    "Epoch #15: Loss:1.0732, Accuracy:0.3910, Validation Loss:1.0729, Validation Accuracy:0.4105\n",
    "Epoch #16: Loss:1.0736, Accuracy:0.3914, Validation Loss:1.0732, Validation Accuracy:0.4122\n",
    "Epoch #17: Loss:1.0735, Accuracy:0.3938, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #18: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0742, Accuracy:0.3938, Validation Loss:1.0731, Validation Accuracy:0.3974\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0732, Validation Accuracy:0.4023\n",
    "Epoch #21: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0727, Validation Accuracy:0.4154\n",
    "Epoch #23: Loss:1.0741, Accuracy:0.3848, Validation Loss:1.0732, Validation Accuracy:0.4056\n",
    "Epoch #24: Loss:1.0739, Accuracy:0.3889, Validation Loss:1.0732, Validation Accuracy:0.4220\n",
    "Epoch #25: Loss:1.0736, Accuracy:0.3938, Validation Loss:1.0732, Validation Accuracy:0.4154\n",
    "Epoch #26: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0736, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0732, Accuracy:0.3938, Validation Loss:1.0743, Validation Accuracy:0.4007\n",
    "Epoch #35: Loss:1.0731, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0727, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0726, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0725, Accuracy:0.3959, Validation Loss:1.0731, Validation Accuracy:0.4039\n",
    "Epoch #39: Loss:1.0724, Accuracy:0.3959, Validation Loss:1.0720, Validation Accuracy:0.4039\n",
    "Epoch #40: Loss:1.0725, Accuracy:0.3971, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #41: Loss:1.0725, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0726, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #43: Loss:1.0724, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0722, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #45: Loss:1.0722, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0723, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #47: Loss:1.0729, Accuracy:0.3893, Validation Loss:1.0739, Validation Accuracy:0.3859\n",
    "Epoch #48: Loss:1.0729, Accuracy:0.3864, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #49: Loss:1.0735, Accuracy:0.3910, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #50: Loss:1.0727, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #51: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0730, Accuracy:0.3963, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0729, Accuracy:0.3951, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #54: Loss:1.0726, Accuracy:0.3967, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #55: Loss:1.0724, Accuracy:0.3951, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0726, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #57: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #58: Loss:1.0723, Accuracy:0.3897, Validation Loss:1.0730, Validation Accuracy:0.3744\n",
    "Epoch #59: Loss:1.0723, Accuracy:0.3951, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0722, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0722, Accuracy:0.3947, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #62: Loss:1.0719, Accuracy:0.3938, Validation Loss:1.0728, Validation Accuracy:0.3924\n",
    "Epoch #63: Loss:1.0718, Accuracy:0.3918, Validation Loss:1.0729, Validation Accuracy:0.3744\n",
    "Epoch #64: Loss:1.0720, Accuracy:0.3938, Validation Loss:1.0729, Validation Accuracy:0.3662\n",
    "Epoch #65: Loss:1.0723, Accuracy:0.3938, Validation Loss:1.0736, Validation Accuracy:0.3760\n",
    "Epoch #66: Loss:1.0720, Accuracy:0.3914, Validation Loss:1.0741, Validation Accuracy:0.3711\n",
    "Epoch #67: Loss:1.0720, Accuracy:0.3938, Validation Loss:1.0736, Validation Accuracy:0.3711\n",
    "Epoch #68: Loss:1.0720, Accuracy:0.3914, Validation Loss:1.0735, Validation Accuracy:0.3810\n",
    "Epoch #69: Loss:1.0717, Accuracy:0.3955, Validation Loss:1.0736, Validation Accuracy:0.3777\n",
    "Epoch #70: Loss:1.0718, Accuracy:0.3938, Validation Loss:1.0733, Validation Accuracy:0.3711\n",
    "Epoch #71: Loss:1.0716, Accuracy:0.3930, Validation Loss:1.0729, Validation Accuracy:0.3711\n",
    "Epoch #72: Loss:1.0717, Accuracy:0.3938, Validation Loss:1.0727, Validation Accuracy:0.3810\n",
    "Epoch #73: Loss:1.0719, Accuracy:0.3906, Validation Loss:1.0729, Validation Accuracy:0.3793\n",
    "Epoch #74: Loss:1.0717, Accuracy:0.3930, Validation Loss:1.0729, Validation Accuracy:0.3793\n",
    "Epoch #75: Loss:1.0715, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.4056\n",
    "Epoch #76: Loss:1.0720, Accuracy:0.3963, Validation Loss:1.0731, Validation Accuracy:0.3826\n",
    "Epoch #77: Loss:1.0713, Accuracy:0.3967, Validation Loss:1.0728, Validation Accuracy:0.3826\n",
    "Epoch #78: Loss:1.0715, Accuracy:0.3992, Validation Loss:1.0730, Validation Accuracy:0.3826\n",
    "Epoch #79: Loss:1.0715, Accuracy:0.3975, Validation Loss:1.0722, Validation Accuracy:0.4138\n",
    "Epoch #80: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0718, Validation Accuracy:0.3842\n",
    "Epoch #81: Loss:1.0713, Accuracy:0.3988, Validation Loss:1.0726, Validation Accuracy:0.3990\n",
    "Epoch #82: Loss:1.0715, Accuracy:0.3963, Validation Loss:1.0724, Validation Accuracy:0.3826\n",
    "Epoch #83: Loss:1.0715, Accuracy:0.4000, Validation Loss:1.0715, Validation Accuracy:0.3908\n",
    "Epoch #84: Loss:1.0714, Accuracy:0.3979, Validation Loss:1.0730, Validation Accuracy:0.3793\n",
    "Epoch #85: Loss:1.0718, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.3711\n",
    "Epoch #86: Loss:1.0717, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #87: Loss:1.0714, Accuracy:0.4016, Validation Loss:1.0734, Validation Accuracy:0.3826\n",
    "Epoch #88: Loss:1.0712, Accuracy:0.3926, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #89: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.4023\n",
    "Epoch #90: Loss:1.0709, Accuracy:0.4021, Validation Loss:1.0721, Validation Accuracy:0.3990\n",
    "Epoch #91: Loss:1.0706, Accuracy:0.3988, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #92: Loss:1.0708, Accuracy:0.3922, Validation Loss:1.0727, Validation Accuracy:0.4039\n",
    "Epoch #93: Loss:1.0714, Accuracy:0.4021, Validation Loss:1.0727, Validation Accuracy:0.4187\n",
    "Epoch #94: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0726, Validation Accuracy:0.4187\n",
    "Epoch #95: Loss:1.0712, Accuracy:0.3979, Validation Loss:1.0722, Validation Accuracy:0.3842\n",
    "Epoch #96: Loss:1.0711, Accuracy:0.3930, Validation Loss:1.0719, Validation Accuracy:0.4122\n",
    "Epoch #97: Loss:1.0714, Accuracy:0.4033, Validation Loss:1.0721, Validation Accuracy:0.4089\n",
    "Epoch #98: Loss:1.0717, Accuracy:0.3996, Validation Loss:1.0728, Validation Accuracy:0.4138\n",
    "Epoch #99: Loss:1.0716, Accuracy:0.3938, Validation Loss:1.0725, Validation Accuracy:0.3875\n",
    "Epoch #100: Loss:1.0716, Accuracy:0.3975, Validation Loss:1.0736, Validation Accuracy:0.4122\n",
    "Epoch #101: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #102: Loss:1.0719, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #103: Loss:1.0715, Accuracy:0.3984, Validation Loss:1.0737, Validation Accuracy:0.3810\n",
    "Epoch #104: Loss:1.0716, Accuracy:0.3955, Validation Loss:1.0746, Validation Accuracy:0.3793\n",
    "Epoch #105: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #106: Loss:1.0720, Accuracy:0.4025, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #107: Loss:1.0720, Accuracy:0.3910, Validation Loss:1.0741, Validation Accuracy:0.3793\n",
    "Epoch #108: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0731, Validation Accuracy:0.3826\n",
    "Epoch #109: Loss:1.0716, Accuracy:0.3955, Validation Loss:1.0728, Validation Accuracy:0.3810\n",
    "Epoch #110: Loss:1.0715, Accuracy:0.3910, Validation Loss:1.0730, Validation Accuracy:0.3760\n",
    "Epoch #111: Loss:1.0716, Accuracy:0.4008, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #112: Loss:1.0714, Accuracy:0.3979, Validation Loss:1.0728, Validation Accuracy:0.4105\n",
    "Epoch #113: Loss:1.0711, Accuracy:0.4025, Validation Loss:1.0727, Validation Accuracy:0.3810\n",
    "Epoch #114: Loss:1.0711, Accuracy:0.3955, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #115: Loss:1.0713, Accuracy:0.4066, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #116: Loss:1.0711, Accuracy:0.4045, Validation Loss:1.0729, Validation Accuracy:0.3924\n",
    "Epoch #117: Loss:1.0709, Accuracy:0.4004, Validation Loss:1.0728, Validation Accuracy:0.3924\n",
    "Epoch #118: Loss:1.0711, Accuracy:0.3984, Validation Loss:1.0728, Validation Accuracy:0.4007\n",
    "Epoch #119: Loss:1.0716, Accuracy:0.3988, Validation Loss:1.0731, Validation Accuracy:0.4154\n",
    "Epoch #120: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3777\n",
    "Epoch #121: Loss:1.0714, Accuracy:0.3947, Validation Loss:1.0731, Validation Accuracy:0.3859\n",
    "Epoch #122: Loss:1.0709, Accuracy:0.4025, Validation Loss:1.0730, Validation Accuracy:0.4171\n",
    "Epoch #123: Loss:1.0710, Accuracy:0.4057, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #124: Loss:1.0705, Accuracy:0.4049, Validation Loss:1.0728, Validation Accuracy:0.3990\n",
    "Epoch #125: Loss:1.0712, Accuracy:0.3975, Validation Loss:1.0733, Validation Accuracy:0.3859\n",
    "Epoch #126: Loss:1.0704, Accuracy:0.4090, Validation Loss:1.0728, Validation Accuracy:0.4171\n",
    "Epoch #127: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0726, Validation Accuracy:0.4089\n",
    "Epoch #128: Loss:1.0705, Accuracy:0.4107, Validation Loss:1.0743, Validation Accuracy:0.3810\n",
    "Epoch #129: Loss:1.0712, Accuracy:0.4062, Validation Loss:1.0734, Validation Accuracy:0.3842\n",
    "Epoch #130: Loss:1.0712, Accuracy:0.4086, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #131: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #132: Loss:1.0703, Accuracy:0.4053, Validation Loss:1.0726, Validation Accuracy:0.3793\n",
    "Epoch #133: Loss:1.0703, Accuracy:0.4074, Validation Loss:1.0724, Validation Accuracy:0.4236\n",
    "Epoch #134: Loss:1.0711, Accuracy:0.4057, Validation Loss:1.0732, Validation Accuracy:0.3875\n",
    "Epoch #135: Loss:1.0709, Accuracy:0.3988, Validation Loss:1.0718, Validation Accuracy:0.4072\n",
    "Epoch #136: Loss:1.0708, Accuracy:0.4000, Validation Loss:1.0724, Validation Accuracy:0.4056\n",
    "Epoch #137: Loss:1.0713, Accuracy:0.3979, Validation Loss:1.0713, Validation Accuracy:0.3941\n",
    "Epoch #138: Loss:1.0711, Accuracy:0.3897, Validation Loss:1.0713, Validation Accuracy:0.3859\n",
    "Epoch #139: Loss:1.0714, Accuracy:0.3938, Validation Loss:1.0718, Validation Accuracy:0.3974\n",
    "Epoch #140: Loss:1.0718, Accuracy:0.3881, Validation Loss:1.0726, Validation Accuracy:0.3744\n",
    "Epoch #141: Loss:1.0723, Accuracy:0.3807, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #142: Loss:1.0718, Accuracy:0.3918, Validation Loss:1.0741, Validation Accuracy:0.3727\n",
    "Epoch #143: Loss:1.0718, Accuracy:0.3807, Validation Loss:1.0738, Validation Accuracy:0.3645\n",
    "Epoch #144: Loss:1.0718, Accuracy:0.3786, Validation Loss:1.0737, Validation Accuracy:0.3695\n",
    "Epoch #145: Loss:1.0722, Accuracy:0.3910, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #146: Loss:1.0717, Accuracy:0.3934, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #147: Loss:1.0720, Accuracy:0.3934, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #148: Loss:1.0717, Accuracy:0.3819, Validation Loss:1.0735, Validation Accuracy:0.3760\n",
    "Epoch #149: Loss:1.0719, Accuracy:0.3901, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #150: Loss:1.0717, Accuracy:0.3860, Validation Loss:1.0732, Validation Accuracy:0.4105\n",
    "Epoch #151: Loss:1.0716, Accuracy:0.3934, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #152: Loss:1.0719, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #153: Loss:1.0713, Accuracy:0.3938, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #154: Loss:1.0715, Accuracy:0.3922, Validation Loss:1.0730, Validation Accuracy:0.3793\n",
    "Epoch #155: Loss:1.0717, Accuracy:0.3984, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #156: Loss:1.0714, Accuracy:0.3934, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #157: Loss:1.0711, Accuracy:0.3938, Validation Loss:1.0731, Validation Accuracy:0.4072\n",
    "Epoch #158: Loss:1.0714, Accuracy:0.3926, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #159: Loss:1.0712, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #160: Loss:1.0713, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #161: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #162: Loss:1.0718, Accuracy:0.3869, Validation Loss:1.0734, Validation Accuracy:0.3859\n",
    "Epoch #163: Loss:1.0719, Accuracy:0.3901, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #164: Loss:1.0713, Accuracy:0.3930, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #165: Loss:1.0711, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #166: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #167: Loss:1.0723, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #168: Loss:1.0713, Accuracy:0.3943, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #169: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #170: Loss:1.0714, Accuracy:0.3938, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #171: Loss:1.0722, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #172: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #173: Loss:1.0715, Accuracy:0.3881, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #174: Loss:1.0712, Accuracy:0.3934, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #175: Loss:1.0712, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #176: Loss:1.0711, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #177: Loss:1.0709, Accuracy:0.3938, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #178: Loss:1.0710, Accuracy:0.3947, Validation Loss:1.0728, Validation Accuracy:0.4072\n",
    "Epoch #179: Loss:1.0711, Accuracy:0.3910, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #180: Loss:1.0710, Accuracy:0.3930, Validation Loss:1.0728, Validation Accuracy:0.4105\n",
    "Epoch #181: Loss:1.0710, Accuracy:0.3951, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #182: Loss:1.0709, Accuracy:0.3943, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #183: Loss:1.0708, Accuracy:0.3943, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #184: Loss:1.0710, Accuracy:0.3943, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #185: Loss:1.0712, Accuracy:0.3947, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #186: Loss:1.0711, Accuracy:0.3918, Validation Loss:1.0727, Validation Accuracy:0.4072\n",
    "Epoch #187: Loss:1.0707, Accuracy:0.3938, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #188: Loss:1.0713, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #189: Loss:1.0706, Accuracy:0.3943, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #190: Loss:1.0717, Accuracy:0.3943, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #191: Loss:1.0707, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #192: Loss:1.0713, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #193: Loss:1.0709, Accuracy:0.3943, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #194: Loss:1.0709, Accuracy:0.3864, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #195: Loss:1.0706, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #196: Loss:1.0709, Accuracy:0.3943, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #197: Loss:1.0706, Accuracy:0.3943, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #198: Loss:1.0706, Accuracy:0.3943, Validation Loss:1.0722, Validation Accuracy:0.3941\n",
    "Epoch #199: Loss:1.0704, Accuracy:0.3943, Validation Loss:1.0721, Validation Accuracy:0.4105\n",
    "Epoch #200: Loss:1.0706, Accuracy:0.3951, Validation Loss:1.0725, Validation Accuracy:0.3941\n",
    "Epoch #201: Loss:1.0705, Accuracy:0.3938, Validation Loss:1.0722, Validation Accuracy:0.3957\n",
    "Epoch #202: Loss:1.0706, Accuracy:0.3963, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #203: Loss:1.0710, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #204: Loss:1.0708, Accuracy:0.3943, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #205: Loss:1.0705, Accuracy:0.3938, Validation Loss:1.0720, Validation Accuracy:0.4056\n",
    "Epoch #206: Loss:1.0704, Accuracy:0.3951, Validation Loss:1.0718, Validation Accuracy:0.4089\n",
    "Epoch #207: Loss:1.0706, Accuracy:0.3906, Validation Loss:1.0717, Validation Accuracy:0.3941\n",
    "Epoch #208: Loss:1.0705, Accuracy:0.3943, Validation Loss:1.0720, Validation Accuracy:0.3941\n",
    "Epoch #209: Loss:1.0705, Accuracy:0.3943, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #210: Loss:1.0702, Accuracy:0.3943, Validation Loss:1.0718, Validation Accuracy:0.3974\n",
    "Epoch #211: Loss:1.0704, Accuracy:0.3979, Validation Loss:1.0717, Validation Accuracy:0.4105\n",
    "Epoch #212: Loss:1.0707, Accuracy:0.3951, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #213: Loss:1.0702, Accuracy:0.3943, Validation Loss:1.0716, Validation Accuracy:0.3941\n",
    "Epoch #214: Loss:1.0701, Accuracy:0.3943, Validation Loss:1.0716, Validation Accuracy:0.3941\n",
    "Epoch #215: Loss:1.0701, Accuracy:0.3943, Validation Loss:1.0714, Validation Accuracy:0.3974\n",
    "Epoch #216: Loss:1.0699, Accuracy:0.3963, Validation Loss:1.0712, Validation Accuracy:0.3974\n",
    "Epoch #217: Loss:1.0698, Accuracy:0.3951, Validation Loss:1.0712, Validation Accuracy:0.3941\n",
    "Epoch #218: Loss:1.0699, Accuracy:0.3943, Validation Loss:1.0714, Validation Accuracy:0.3941\n",
    "Epoch #219: Loss:1.0698, Accuracy:0.3943, Validation Loss:1.0710, Validation Accuracy:0.3941\n",
    "Epoch #220: Loss:1.0709, Accuracy:0.3947, Validation Loss:1.0710, Validation Accuracy:0.4039\n",
    "Epoch #221: Loss:1.0706, Accuracy:0.3975, Validation Loss:1.0723, Validation Accuracy:0.3957\n",
    "Epoch #222: Loss:1.0699, Accuracy:0.3947, Validation Loss:1.0706, Validation Accuracy:0.4039\n",
    "Epoch #223: Loss:1.0696, Accuracy:0.3996, Validation Loss:1.0710, Validation Accuracy:0.4039\n",
    "Epoch #224: Loss:1.0704, Accuracy:0.3959, Validation Loss:1.0706, Validation Accuracy:0.4039\n",
    "Epoch #225: Loss:1.0697, Accuracy:0.3996, Validation Loss:1.0708, Validation Accuracy:0.4056\n",
    "Epoch #226: Loss:1.0700, Accuracy:0.3967, Validation Loss:1.0707, Validation Accuracy:0.4007\n",
    "Epoch #227: Loss:1.0696, Accuracy:0.3893, Validation Loss:1.0707, Validation Accuracy:0.3941\n",
    "Epoch #228: Loss:1.0696, Accuracy:0.3963, Validation Loss:1.0712, Validation Accuracy:0.3941\n",
    "Epoch #229: Loss:1.0697, Accuracy:0.3967, Validation Loss:1.0706, Validation Accuracy:0.4039\n",
    "Epoch #230: Loss:1.0694, Accuracy:0.3967, Validation Loss:1.0702, Validation Accuracy:0.4007\n",
    "Epoch #231: Loss:1.0700, Accuracy:0.3963, Validation Loss:1.0701, Validation Accuracy:0.4007\n",
    "Epoch #232: Loss:1.0694, Accuracy:0.3971, Validation Loss:1.0707, Validation Accuracy:0.3941\n",
    "Epoch #233: Loss:1.0695, Accuracy:0.3963, Validation Loss:1.0697, Validation Accuracy:0.4023\n",
    "Epoch #234: Loss:1.0696, Accuracy:0.3951, Validation Loss:1.0695, Validation Accuracy:0.4023\n",
    "Epoch #235: Loss:1.0694, Accuracy:0.3922, Validation Loss:1.0702, Validation Accuracy:0.4007\n",
    "Epoch #236: Loss:1.0692, Accuracy:0.3971, Validation Loss:1.0696, Validation Accuracy:0.4007\n",
    "Epoch #237: Loss:1.0691, Accuracy:0.3984, Validation Loss:1.0694, Validation Accuracy:0.3990\n",
    "Epoch #238: Loss:1.0693, Accuracy:0.3975, Validation Loss:1.0696, Validation Accuracy:0.4039\n",
    "Epoch #239: Loss:1.0696, Accuracy:0.3934, Validation Loss:1.0693, Validation Accuracy:0.3990\n",
    "Epoch #240: Loss:1.0696, Accuracy:0.3963, Validation Loss:1.0704, Validation Accuracy:0.4007\n",
    "Epoch #241: Loss:1.0692, Accuracy:0.3971, Validation Loss:1.0702, Validation Accuracy:0.4007\n",
    "Epoch #242: Loss:1.0691, Accuracy:0.4000, Validation Loss:1.0701, Validation Accuracy:0.4072\n",
    "Epoch #243: Loss:1.0689, Accuracy:0.3971, Validation Loss:1.0701, Validation Accuracy:0.4072\n",
    "Epoch #244: Loss:1.0696, Accuracy:0.3967, Validation Loss:1.0704, Validation Accuracy:0.3990\n",
    "Epoch #245: Loss:1.0696, Accuracy:0.3979, Validation Loss:1.0708, Validation Accuracy:0.4023\n",
    "Epoch #246: Loss:1.0695, Accuracy:0.3988, Validation Loss:1.0710, Validation Accuracy:0.3990\n",
    "Epoch #247: Loss:1.0698, Accuracy:0.3959, Validation Loss:1.0710, Validation Accuracy:0.3957\n",
    "Epoch #248: Loss:1.0698, Accuracy:0.3959, Validation Loss:1.0710, Validation Accuracy:0.3908\n",
    "Epoch #249: Loss:1.0698, Accuracy:0.3975, Validation Loss:1.0708, Validation Accuracy:0.3744\n",
    "Epoch #250: Loss:1.0704, Accuracy:0.3943, Validation Loss:1.0714, Validation Accuracy:0.3924\n",
    "Epoch #251: Loss:1.0696, Accuracy:0.3943, Validation Loss:1.0712, Validation Accuracy:0.3990\n",
    "Epoch #252: Loss:1.0699, Accuracy:0.3959, Validation Loss:1.0719, Validation Accuracy:0.3957\n",
    "Epoch #253: Loss:1.0694, Accuracy:0.3959, Validation Loss:1.0711, Validation Accuracy:0.3990\n",
    "Epoch #254: Loss:1.0700, Accuracy:0.3930, Validation Loss:1.0709, Validation Accuracy:0.3892\n",
    "Epoch #255: Loss:1.0693, Accuracy:0.3984, Validation Loss:1.0711, Validation Accuracy:0.3957\n",
    "Epoch #256: Loss:1.0696, Accuracy:0.3992, Validation Loss:1.0707, Validation Accuracy:0.3941\n",
    "Epoch #257: Loss:1.0695, Accuracy:0.3951, Validation Loss:1.0705, Validation Accuracy:0.3892\n",
    "Epoch #258: Loss:1.0694, Accuracy:0.3914, Validation Loss:1.0708, Validation Accuracy:0.4023\n",
    "Epoch #259: Loss:1.0696, Accuracy:0.3951, Validation Loss:1.0709, Validation Accuracy:0.3957\n",
    "Epoch #260: Loss:1.0694, Accuracy:0.3943, Validation Loss:1.0708, Validation Accuracy:0.4007\n",
    "Epoch #261: Loss:1.0693, Accuracy:0.3967, Validation Loss:1.0711, Validation Accuracy:0.3957\n",
    "Epoch #262: Loss:1.0694, Accuracy:0.3979, Validation Loss:1.0708, Validation Accuracy:0.3990\n",
    "Epoch #263: Loss:1.0695, Accuracy:0.3996, Validation Loss:1.0699, Validation Accuracy:0.3957\n",
    "Epoch #264: Loss:1.0696, Accuracy:0.3963, Validation Loss:1.0705, Validation Accuracy:0.3957\n",
    "Epoch #265: Loss:1.0694, Accuracy:0.3963, Validation Loss:1.0702, Validation Accuracy:0.3777\n",
    "Epoch #266: Loss:1.0692, Accuracy:0.3979, Validation Loss:1.0705, Validation Accuracy:0.3842\n",
    "Epoch #267: Loss:1.0690, Accuracy:0.3988, Validation Loss:1.0706, Validation Accuracy:0.3941\n",
    "Epoch #268: Loss:1.0692, Accuracy:0.3963, Validation Loss:1.0707, Validation Accuracy:0.4007\n",
    "Epoch #269: Loss:1.0694, Accuracy:0.3992, Validation Loss:1.0702, Validation Accuracy:0.3810\n",
    "Epoch #270: Loss:1.0691, Accuracy:0.3963, Validation Loss:1.0701, Validation Accuracy:0.3875\n",
    "Epoch #271: Loss:1.0692, Accuracy:0.4004, Validation Loss:1.0706, Validation Accuracy:0.3957\n",
    "Epoch #272: Loss:1.0692, Accuracy:0.4000, Validation Loss:1.0709, Validation Accuracy:0.3957\n",
    "Epoch #273: Loss:1.0692, Accuracy:0.3967, Validation Loss:1.0706, Validation Accuracy:0.4007\n",
    "Epoch #274: Loss:1.0693, Accuracy:0.3947, Validation Loss:1.0699, Validation Accuracy:0.3908\n",
    "Epoch #275: Loss:1.0689, Accuracy:0.4021, Validation Loss:1.0696, Validation Accuracy:0.3924\n",
    "Epoch #276: Loss:1.0695, Accuracy:0.4033, Validation Loss:1.0695, Validation Accuracy:0.3924\n",
    "Epoch #277: Loss:1.0695, Accuracy:0.3963, Validation Loss:1.0703, Validation Accuracy:0.3990\n",
    "Epoch #278: Loss:1.0697, Accuracy:0.3955, Validation Loss:1.0710, Validation Accuracy:0.4039\n",
    "Epoch #279: Loss:1.0694, Accuracy:0.3967, Validation Loss:1.0714, Validation Accuracy:0.4039\n",
    "Epoch #280: Loss:1.0697, Accuracy:0.3914, Validation Loss:1.0701, Validation Accuracy:0.4072\n",
    "Epoch #281: Loss:1.0690, Accuracy:0.3963, Validation Loss:1.0696, Validation Accuracy:0.3957\n",
    "Epoch #282: Loss:1.0698, Accuracy:0.4016, Validation Loss:1.0708, Validation Accuracy:0.4007\n",
    "Epoch #283: Loss:1.0689, Accuracy:0.3988, Validation Loss:1.0702, Validation Accuracy:0.4039\n",
    "Epoch #284: Loss:1.0689, Accuracy:0.3979, Validation Loss:1.0703, Validation Accuracy:0.3957\n",
    "Epoch #285: Loss:1.0689, Accuracy:0.3979, Validation Loss:1.0705, Validation Accuracy:0.3842\n",
    "Epoch #286: Loss:1.0688, Accuracy:0.4016, Validation Loss:1.0704, Validation Accuracy:0.3990\n",
    "Epoch #287: Loss:1.0690, Accuracy:0.3992, Validation Loss:1.0700, Validation Accuracy:0.3924\n",
    "Epoch #288: Loss:1.0692, Accuracy:0.4000, Validation Loss:1.0699, Validation Accuracy:0.3826\n",
    "Epoch #289: Loss:1.0691, Accuracy:0.3984, Validation Loss:1.0705, Validation Accuracy:0.3826\n",
    "Epoch #290: Loss:1.0689, Accuracy:0.4049, Validation Loss:1.0693, Validation Accuracy:0.3842\n",
    "Epoch #291: Loss:1.0689, Accuracy:0.3996, Validation Loss:1.0696, Validation Accuracy:0.3957\n",
    "Epoch #292: Loss:1.0693, Accuracy:0.3967, Validation Loss:1.0694, Validation Accuracy:0.3957\n",
    "Epoch #293: Loss:1.0685, Accuracy:0.4041, Validation Loss:1.0703, Validation Accuracy:0.3990\n",
    "Epoch #294: Loss:1.0690, Accuracy:0.3967, Validation Loss:1.0706, Validation Accuracy:0.4007\n",
    "Epoch #295: Loss:1.0685, Accuracy:0.3996, Validation Loss:1.0693, Validation Accuracy:0.3826\n",
    "Epoch #296: Loss:1.0686, Accuracy:0.4000, Validation Loss:1.0693, Validation Accuracy:0.3941\n",
    "Epoch #297: Loss:1.0686, Accuracy:0.3992, Validation Loss:1.0694, Validation Accuracy:0.3842\n",
    "Epoch #298: Loss:1.0686, Accuracy:0.3971, Validation Loss:1.0699, Validation Accuracy:0.4056\n",
    "Epoch #299: Loss:1.0686, Accuracy:0.3947, Validation Loss:1.0689, Validation Accuracy:0.3957\n",
    "Epoch #300: Loss:1.0694, Accuracy:0.3938, Validation Loss:1.0699, Validation Accuracy:0.3941\n",
    "\n",
    "Test:\n",
    "Test Loss:1.06992745, Accuracy:0.3941\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  03   01\n",
    "t:02  18   1  208\n",
    "t:03   6   1  135\n",
    "t:01  19   0  221\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.42      0.08      0.13       227\n",
    "          03       0.50      0.01      0.01       142\n",
    "          01       0.39      0.92      0.55       240\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.44      0.34      0.23       609\n",
    "weighted avg       0.43      0.39      0.27       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 06:24:35 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 44 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0855223222319128, 1.079794046914049, 1.076367052905078, 1.0751601394957118, 1.0748164953269395, 1.074413844517299, 1.0744288183002442, 1.074171653913551, 1.0738996337787272, 1.0736288564350023, 1.073597266952001, 1.0738921629384233, 1.073687006100058, 1.0734156307524256, 1.072939689131989, 1.073157124331432, 1.0732499942403708, 1.073859442435266, 1.0731071319877612, 1.073154958989624, 1.0731696681240313, 1.0727456412683372, 1.0731521692182042, 1.0732065832673623, 1.0731780924429055, 1.0744574596533438, 1.074193625027323, 1.074860238285096, 1.0747201280249359, 1.0745082341978702, 1.0743059438633409, 1.0740179263899479, 1.0744346004401522, 1.0742831521825054, 1.0733702327621786, 1.0731359933593199, 1.0731063443060187, 1.0731418400953947, 1.0720168968726849, 1.0729722692852928, 1.0735880582790656, 1.0733982631921377, 1.0734540631226914, 1.0735468061882483, 1.0740032904645296, 1.073607022539148, 1.0738875190612718, 1.0738682570715843, 1.0735348351483274, 1.0734277288314744, 1.073517270667604, 1.0729134452754054, 1.0731469948695016, 1.0729948591520437, 1.073188714597417, 1.073871115941328, 1.0734170853406533, 1.0729767857318246, 1.0728989837596374, 1.0730642557927148, 1.072994292075998, 1.0728038950702437, 1.0729404507795186, 1.0728748316443808, 1.073568475266004, 1.0740843794028747, 1.073611060974046, 1.0735361675910762, 1.0735687164250265, 1.073335733906976, 1.072850246147569, 1.0727411919626697, 1.0728986016635238, 1.072874338168816, 1.0733066198273833, 1.0730543984176686, 1.0728387897237768, 1.0729676964639248, 1.0722067923772902, 1.071826793877362, 1.07256620818954, 1.072367651709195, 1.071467613547502, 1.0729711644950954, 1.0732850920782104, 1.0733839385027957, 1.0733547658951608, 1.0733362747530633, 1.0731440297097017, 1.072055014483447, 1.0722800068471623, 1.0727025814635804, 1.0726712771824427, 1.0726348114718358, 1.072246829277189, 1.0718580474602961, 1.0720600749080014, 1.0728179357321979, 1.072493294776954, 1.0735687166207726, 1.0730694387542399, 1.0735703845720965, 1.0737299412146382, 1.074559894688611, 1.0744305412561828, 1.073262309988927, 1.0741312206280837, 1.0731050903574, 1.0727991689797889, 1.0730352505478757, 1.0729601612232003, 1.0728092966800058, 1.0727112269753893, 1.0727320523880581, 1.073137138668931, 1.0729023916967984, 1.072783395183106, 1.072829540139936, 1.0730637229722122, 1.07381951006371, 1.0731159260707537, 1.0729605411661083, 1.072986466935507, 1.0728037247712585, 1.0733217169302829, 1.0728035339189477, 1.0725728381052002, 1.074283987822008, 1.073368996039204, 1.0749670137912768, 1.0729926664058016, 1.0726462884489538, 1.0723561320594575, 1.0731567690525148, 1.0718131746564592, 1.0723693312095304, 1.0712975850833462, 1.0712582587412818, 1.0717836861148453, 1.0725951913150855, 1.0743836362177908, 1.0740799469313598, 1.0738454918164533, 1.0736873431746008, 1.074079634129316, 1.0736612576764988, 1.0734866121523878, 1.073535100188357, 1.0732114095797485, 1.073175744470117, 1.07343125088853, 1.0736791868319457, 1.0729745344575403, 1.0729952471205362, 1.073514047123137, 1.073149064295789, 1.0730953308553335, 1.0731265249314959, 1.0733140411439592, 1.0733968365955822, 1.0731488591540232, 1.0733856501054686, 1.0733092406700397, 1.07338497027975, 1.0730521128878414, 1.0730061593705602, 1.0738258765053084, 1.0728950300827402, 1.0731962202805017, 1.0730424935398821, 1.074600172747532, 1.0729401540286436, 1.0729737244607584, 1.0730264813246202, 1.0730450699482057, 1.073030691037233, 1.072836213902691, 1.0728341209868884, 1.0729358593622844, 1.07278406796197, 1.0728566384276341, 1.072630913974029, 1.0727750838096506, 1.0727431126220277, 1.0727243120055676, 1.07270615226138, 1.072942818131157, 1.0730151623144917, 1.0727584371817327, 1.072743199141742, 1.0730737195226359, 1.0729882190575937, 1.072412805799976, 1.0723024033169049, 1.073050018406071, 1.0725776745963762, 1.072337586891475, 1.0721649947424827, 1.0721101416351368, 1.0725445968568423, 1.072205209183967, 1.072284796555054, 1.0732588411747723, 1.072366705277479, 1.0719523882043773, 1.0718132774230882, 1.071653863479351, 1.0719656797465433, 1.0722980726332891, 1.0717699555144913, 1.0716669244327763, 1.0723547434376182, 1.0716211220313763, 1.0716495729236573, 1.0713792904257187, 1.0712341555625151, 1.0712234887779248, 1.0713512312211035, 1.0710330620187845, 1.070957298740769, 1.0723418101105588, 1.0706053533773312, 1.0710441770616228, 1.0706292085459668, 1.0707899261577962, 1.0707482487110083, 1.070672713672782, 1.0712340314595765, 1.0706469928494031, 1.070243366050407, 1.070128014913725, 1.0706589415742847, 1.0697370401548438, 1.0695399821098215, 1.070167824552564, 1.0696205241339547, 1.069356474187378, 1.0695799048898256, 1.069317379804276, 1.0704488840400683, 1.0702190773044704, 1.0701158027147817, 1.0701195765011415, 1.0703957605440237, 1.0707523928487241, 1.0710037236143215, 1.070956743017989, 1.0710339133179638, 1.070821595896641, 1.0714269970438164, 1.071243917805025, 1.0719124382156848, 1.071137546709997, 1.0708628133404234, 1.071135938656937, 1.0706682197370356, 1.0705215586425831, 1.0707640440397466, 1.0708805169965245, 1.070787065134847, 1.071081905137925, 1.070823677654924, 1.069883327961751, 1.0705150397148822, 1.0702288452236133, 1.070491242095559, 1.0705545948839736, 1.0707471151461547, 1.070187304016013, 1.0700776796231324, 1.0705585240926256, 1.070872997415477, 1.070577743409694, 1.0698995018631758, 1.0696399172734352, 1.0694759393168984, 1.0702716429245296, 1.0710272070613793, 1.071398724084613, 1.0701484274981645, 1.0695957640317468, 1.0707844489900937, 1.0702116236898112, 1.0703420239716328, 1.0705199758407518, 1.070384384376075, 1.0700497621385923, 1.0699175537513395, 1.070524553164277, 1.0692595601669086, 1.0695776044833054, 1.0694196423873525, 1.0702728174002887, 1.0706145473693196, 1.0693293437973423, 1.0692667927843793, 1.0694421934964033, 1.069919301762761, 1.0688856278342762, 1.0699274189562242], 'val_acc': [0.3727421995943599, 0.3727421995943599, 0.3727421995943599, 0.39737274097691616, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3973727406832972, 0.4022988489598085, 0.39901477680809194, 0.40065681283501375, 0.4105090295837822, 0.41215106590432293, 0.3973727405854242, 0.3940886683358347, 0.3973727405854242, 0.40229884905768143, 0.3940886683358347, 0.41543513893689626, 0.40558292169876287, 0.4220032829467103, 0.41543513825178535, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.4006568129328867, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.4039408851824762, 0.4039408851824762, 0.3973727405854242, 0.3940886683358347, 0.3973727405854242, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.39573070446062947, 0.385878487613988, 0.39244663230891297, 0.3908045961841182, 0.39573070455850246, 0.3940886683358347, 0.3940886684337077, 0.39573070455850246, 0.39573070455850246, 0.3940886683358347, 0.3940886683358347, 0.39573070455850246, 0.3743842346425518, 0.3940886684337077, 0.3940886684337077, 0.39244663230891297, 0.39244663230891297, 0.3743842347404248, 0.3661740541164511, 0.37602627096309255, 0.37110016249083533, 0.37110016249083533, 0.3809523792396038, 0.3776683069900143, 0.37110016249083533, 0.37110016249083533, 0.3809523791417308, 0.37931034311480905, 0.37931034311480905, 0.4055829216008899, 0.38259441526652554, 0.3825944154622715, 0.3825944151686526, 0.41379310242060957, 0.3842364514891933, 0.39901477680809194, 0.3825944154622715, 0.3908045962819912, 0.37931034311480905, 0.37110016249083533, 0.3990147770038379, 0.38259441536439853, 0.38423645158706626, 0.4022988491555544, 0.3990147771017109, 0.3940886683358347, 0.40394088547609514, 0.4187192107949938, 0.4187192107949938, 0.3842364514891933, 0.4121510662958148, 0.40886699404622534, 0.4137931023227366, 0.38752052383665575, 0.4121510662958148, 0.3990147771017109, 0.385878487809734, 0.38095237904385787, 0.37931034311480905, 0.38916256015719647, 0.38423645178281224, 0.3793103429190631, 0.38259441585376347, 0.3809523791417308, 0.3760262706694736, 0.40394088547609514, 0.4105090300731471, 0.3809523791417308, 0.3908045963798642, 0.3957307048521214, 0.39244663250465894, 0.39244663250465894, 0.40065681312863266, 0.4154351386432773, 0.3776683068921413, 0.38587848800547997, 0.41707717476807205, 0.3990147770038379, 0.3990147770038379, 0.38587848800547997, 0.41707717476807205, 0.40886699414409833, 0.38095237953322275, 0.38423645158706626, 0.39901477680809194, 0.38916256015719647, 0.379310343408428, 0.423645319267251, 0.3875205241302747, 0.4072249574320657, 0.4055829215030169, 0.3940886683358347, 0.3858784875161151, 0.39737274117266214, 0.3743842347404248, 0.3940886682379618, 0.3727421985177571, 0.36453201808952934, 0.3694581262681676, 0.3940886682379618, 0.3940886682379618, 0.3940886682379618, 0.3760262706694736, 0.3940886682379618, 0.4105090295837822, 0.3940886683358347, 0.3940886683358347, 0.4072249573341927, 0.3793103429190631, 0.3940886682379618, 0.3940886682379618, 0.4072249573341927, 0.3940886682379618, 0.3940886683358347, 0.3940886683358347, 0.3940886682379618, 0.38587848800547997, 0.3940886682379618, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3973727401939323, 0.3940886682379618, 0.3940886683358347, 0.3940886683358347, 0.39244663211316705, 0.3940886683358347, 0.4072249574320657, 0.3940886683358347, 0.4105090296816552, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.4072249574320657, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886682379618, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.4105090296816552, 0.3940886683358347, 0.39573070436275654, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.40558292130727097, 0.40886699404622534, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3973727405854242, 0.4105090296816552, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.3973727405854242, 0.3973727405854242, 0.3940886683358347, 0.3940886683358347, 0.3940886683358347, 0.4039408851824762, 0.39573070455850246, 0.4039408851824762, 0.4039408851824762, 0.4039408851824762, 0.40558292130727097, 0.40065681283501375, 0.3940886681400888, 0.3940886683358347, 0.4039408851824762, 0.4006568129328867, 0.4006568129328867, 0.3940886683358347, 0.40229884954704637, 0.40229884954704637, 0.4006568129328867, 0.4006568129328867, 0.39901477680809194, 0.4039408858675871, 0.39901477739532987, 0.4006568129328867, 0.4006568129328867, 0.40722495752993865, 0.40722495811717657, 0.39901477690596493, 0.4022988491555544, 0.39901477680809194, 0.39573070455850246, 0.3908045961841182, 0.3743842349361708, 0.39244663240678596, 0.39901477690596493, 0.39573070455850246, 0.39901477690596493, 0.38916256015719647, 0.39573070455850246, 0.3940886685315807, 0.38916256054868836, 0.4022988491555544, 0.39573070465637544, 0.4006568129328867, 0.39573070455850246, 0.39901477680809194, 0.3957307052436133, 0.39573070455850246, 0.3776683071857603, 0.38423645178281224, 0.3940886686294537, 0.40065681312863266, 0.38095237953322275, 0.3875205241302747, 0.39573070475424843, 0.39573070455850246, 0.4006568130307597, 0.3908045962819912, 0.39244663289615084, 0.39244663289615084, 0.3990147770038379, 0.40394088537822215, 0.40394088528034916, 0.4072249582150495, 0.3957307052436133, 0.4006568130307597, 0.40394088537822215, 0.3957307052436133, 0.38423645178281224, 0.39901477690596493, 0.39244663250465894, 0.3825944157558905, 0.3825944156580175, 0.3842364518806852, 0.3957307052436133, 0.3957307052436133, 0.39901477690596493, 0.4006568130307597, 0.3825944156580175, 0.39408866911881857, 0.38423645178281224, 0.4055829215030169, 0.3957307052436133, 0.3940886687273267], 'loss': [1.0896473393058386, 1.0825877020736006, 1.0781929957058884, 1.0746183555473783, 1.0734734022152252, 1.0736477183610262, 1.0736740635161037, 1.073514495542162, 1.0736834879039006, 1.073578203238501, 1.0734344699299556, 1.0733037805655163, 1.0728571576504247, 1.072832626043159, 1.0731687521298074, 1.0735723862657802, 1.0734904734260982, 1.0746223388021732, 1.0742343999521933, 1.073447606206185, 1.0736658045888192, 1.073389558723575, 1.0741357903216164, 1.0739440370634106, 1.0736422028629686, 1.0742120396185215, 1.0738314680495056, 1.0744116250004856, 1.073863699304005, 1.073613427649778, 1.0734883287359311, 1.0735804200417207, 1.0731094709901594, 1.0731608219949615, 1.0731490484742903, 1.072672915605549, 1.0725884807183268, 1.0724773815280357, 1.0724334965496338, 1.0724869131797148, 1.07253981679372, 1.0725853237528087, 1.0723674384965054, 1.0721665644302996, 1.072189347112448, 1.0722921101709166, 1.072857831782629, 1.0728826491494932, 1.0735280788899446, 1.0726857105074967, 1.0733913490659648, 1.072973639225813, 1.0729146054148428, 1.0726238742746121, 1.072366574804396, 1.0726366675609926, 1.0721373219754418, 1.0723154724500996, 1.0722507953154232, 1.0722460102006885, 1.072206237869341, 1.0718508047734443, 1.071759267264568, 1.0719723908073848, 1.0723324779612329, 1.0720132696555136, 1.0719717028694231, 1.0719618015955117, 1.0717452922640884, 1.071765606701986, 1.0716316095857406, 1.0716889142010981, 1.0719336117072762, 1.071716708128457, 1.0715204076355733, 1.0719542580218775, 1.0713156093072598, 1.071514757066292, 1.0715429981631175, 1.0711335617903566, 1.0712905079921902, 1.0714751029651024, 1.0715180430324172, 1.071398476066041, 1.071840628902036, 1.0717150114889753, 1.0713646561702908, 1.0711712031393814, 1.0713131182981956, 1.0709406281398797, 1.0706252499037945, 1.0708295084367787, 1.0714058206801051, 1.0710012880438897, 1.0712100279649426, 1.0710713655796873, 1.071374424331242, 1.0717030754813912, 1.071595510956688, 1.0715775605344675, 1.071582058420906, 1.071945720825352, 1.0715332199171095, 1.0715734225279008, 1.0725901448016784, 1.0719684342828864, 1.07201940513979, 1.0724036121270495, 1.0716051518305127, 1.0714978374250126, 1.0715839234710474, 1.071398941545271, 1.0711170618049417, 1.0711384493222715, 1.0712558906915497, 1.0711337323061494, 1.0708599960534724, 1.071066373623372, 1.0716373514101, 1.0705245550653038, 1.0713553196106114, 1.070864693584873, 1.0709937186211775, 1.0704690526887866, 1.0711918231642956, 1.0703615541575626, 1.071108121157182, 1.0705176400452914, 1.07116270045725, 1.0711926821565727, 1.0711316920893392, 1.0703086386960634, 1.0703130027351928, 1.0711211794945248, 1.0709145075976236, 1.0707540933601176, 1.0712671414537842, 1.0710928159083184, 1.0713845835084543, 1.0717554406708516, 1.072317945638966, 1.0718227849603923, 1.0717886210956613, 1.0718067781146792, 1.072219961575659, 1.0716604638638192, 1.0719882061349293, 1.0716807404093185, 1.0718743891197064, 1.0717374625392029, 1.0716306896425125, 1.0719248740824832, 1.0713326407164274, 1.07148647890933, 1.0716643655079836, 1.0714021189746425, 1.0711167655196767, 1.07143461175523, 1.0712073304569942, 1.0713459886809393, 1.071599644067596, 1.0718406213137648, 1.0719226237929578, 1.071336330917092, 1.0710791043432342, 1.0715888285783772, 1.0723278402547816, 1.071341707868008, 1.0716098121059503, 1.0714453730495068, 1.0722267149899773, 1.0716492069330548, 1.0714886151300074, 1.0711809751679031, 1.0711854947421096, 1.0711493427014205, 1.0708950029016766, 1.0710431229652075, 1.071141556694767, 1.070985882140283, 1.0709997482613127, 1.0709359949863912, 1.0708447856824745, 1.071003279157243, 1.0711616722220514, 1.0710774460857164, 1.0707389716984554, 1.0713057987009476, 1.0705853496978415, 1.071672564708232, 1.0707441507668465, 1.0712649608294822, 1.0708552736031691, 1.0709283492403598, 1.0706102657611856, 1.0709116832431582, 1.0706091998783716, 1.0706040724102233, 1.0704459167359057, 1.07064413823631, 1.070543552179356, 1.0706003876926962, 1.0709868227921473, 1.0707511818139706, 1.0705094386419967, 1.0703720652836795, 1.070575868373534, 1.070511015974276, 1.0705234958895422, 1.0701954891059922, 1.0703950591645446, 1.0706602811323789, 1.0702240727520576, 1.0700713446987238, 1.0700662966871164, 1.0698505308104247, 1.0698071773536886, 1.0699261655063355, 1.069784936376176, 1.0708981268214983, 1.0706086179314207, 1.069933325945719, 1.0695519126171449, 1.0703997972319992, 1.0696763271668608, 1.0699711705624935, 1.0696381406372821, 1.0696201216758399, 1.0697313824228682, 1.0693623063500657, 1.0700147139218308, 1.0694382671947598, 1.0694547791745384, 1.0695764489731994, 1.0693701963894666, 1.0692229534321498, 1.069081410979833, 1.0693141723315573, 1.0696310116769843, 1.0696158150627872, 1.0691692771852874, 1.0691452606502745, 1.0689458320762588, 1.0696425625430974, 1.0696041255516193, 1.0694565236935625, 1.0698219064324788, 1.069778600365719, 1.0697921101317514, 1.070416717656584, 1.069619373521276, 1.069890186723008, 1.0694210411342018, 1.0699996777383698, 1.0693108386817165, 1.0696103966456418, 1.0694826495720866, 1.0693513085465167, 1.0695894363724476, 1.0693908142113342, 1.0693006952195687, 1.069370093091068, 1.069489726785272, 1.0696328587600583, 1.0694217666708223, 1.0692366906994422, 1.0689656826994502, 1.0692191175856385, 1.0693861555514639, 1.0691278350916242, 1.0691551817026472, 1.0692336346824067, 1.069172172771587, 1.0692750022151876, 1.068933852888965, 1.0694828276271937, 1.0694564805138527, 1.0697232264512864, 1.0694287603640704, 1.069725497449448, 1.0689583725018668, 1.069848433167538, 1.068896501216066, 1.0689377624640963, 1.068901765273092, 1.0687547211284756, 1.06901302014533, 1.0691966236983972, 1.069054713631068, 1.068855412295222, 1.068890159379775, 1.069318576661958, 1.0685360752826354, 1.0690141930962, 1.0684991967261939, 1.068624723031046, 1.0685517526015609, 1.0686251461628282, 1.0685653096106997, 1.0693616296721191], 'acc': [0.37289527956954754, 0.3728952787495247, 0.3728952756163031, 0.37494866683987377, 0.3967145809892267, 0.39425051250252147, 0.3942505115233897, 0.39425051171921605, 0.3942505156724605, 0.39425051547663414, 0.39425051113173704, 0.3963039004337616, 0.3926078040252231, 0.39301848223077196, 0.3909650913621366, 0.391375770779361, 0.39383983707525894, 0.3942505129308916, 0.39383983472534273, 0.3946611927397687, 0.39589322575308705, 0.40123203165477306, 0.3848049276785684, 0.3889117042876367, 0.3938398348844516, 0.39466119309470393, 0.39425051171921605, 0.3942505123434126, 0.3942505133225443, 0.3942505115233897, 0.3942505133225443, 0.3938398349211691, 0.39425051312671794, 0.3938398359003008, 0.3938398380911081, 0.39425051312671794, 0.39425051312671794, 0.39589322238732166, 0.395893225165608, 0.39712525622066286, 0.39425051351837065, 0.3938398372710853, 0.39425051211086876, 0.3942505148891551, 0.3942505157091779, 0.394250514301676, 0.3893223804982046, 0.3864476387750442, 0.3909650925738121, 0.3954825441817728, 0.3967145772318086, 0.39630390020121786, 0.39507186754283474, 0.3967145772318086, 0.39507187032112107, 0.3942505152808078, 0.39794661149351995, 0.38973306011125536, 0.39507186953781565, 0.3942505141058497, 0.3946611889456332, 0.393839833746211, 0.39178644976821525, 0.3938398335136672, 0.3938398374669116, 0.39137577038770827, 0.3938398353128218, 0.391375768196901, 0.39548254696005913, 0.3938398353128218, 0.39301848262242467, 0.3938398374669116, 0.39055441554322134, 0.3930184818391193, 0.39671457742763494, 0.3963039035669832, 0.3967145770359823, 0.39917864708929823, 0.39753593344708, 0.40492813217566487, 0.3987679657138104, 0.3963039007886969, 0.3999999981763671, 0.3979466102818444, 0.39630390023793527, 0.39425051171921605, 0.4016427126018908, 0.39260780461270217, 0.40041067579443695, 0.4020533868541952, 0.3987679657138104, 0.3921971271904587, 0.40205338747839175, 0.4032854226825174, 0.39794661165262885, 0.3930184798808558, 0.40328541912092564, 0.39958932235745187, 0.39383983648777987, 0.39753593642119267, 0.40287474506444754, 0.3963039035669832, 0.39835728907487233, 0.3954825453567309, 0.402464066467246, 0.4024640652922879, 0.3909650917537893, 0.3983572918531586, 0.3954825485633874, 0.3909650921821594, 0.4008213536083331, 0.3979466106734971, 0.40246406587976696, 0.39548254637258007, 0.40657084343124955, 0.4045174555367268, 0.40041067876854963, 0.39835728911158974, 0.3987679677087913, 0.4028747448686212, 0.39466119152809315, 0.40246406744637775, 0.4057494854168236, 0.40492813018068397, 0.3975359348545819, 0.4090349089438421, 0.40533880920625565, 0.41067761902446864, 0.4061601649931569, 0.4086242315215986, 0.4049281292015522, 0.4053388109686928, 0.40739219628075557, 0.40574948760763085, 0.39876796852881413, 0.4000000007588271, 0.3979466138434361, 0.389733059915429, 0.39383983805439066, 0.388090348464018, 0.38069815270954577, 0.39178644820160446, 0.3806981499679769, 0.3786447646191967, 0.39096509394459655, 0.3934291590655364, 0.3934291598488418, 0.3819301867387133, 0.3901437377293252, 0.38603695974947244, 0.3934291573030993, 0.39425051449750237, 0.3938398346886253, 0.39219712542802154, 0.39835728950324245, 0.39342916004466816, 0.3938398368794326, 0.39260780461270217, 0.3942505146933287, 0.39425051195175986, 0.39425051351837065, 0.3868583148265032, 0.3901437389042833, 0.3930184784733539, 0.3942505123066951, 0.39425051312671794, 0.39425051171921605, 0.39425051113173704, 0.39425051351837065, 0.3938398367203237, 0.39425051156010715, 0.39589322199566895, 0.38809034728905994, 0.393429158282231, 0.39425051351837065, 0.3942505152808078, 0.3938398372710853, 0.3946611907080703, 0.3909650925738121, 0.3930184785100714, 0.39507186895033664, 0.3942505152808078, 0.3942505148891551, 0.3942505156724605, 0.3946611887865243, 0.3917864495723889, 0.39383983805439066, 0.39425051406913225, 0.3942505123066951, 0.39425051273506523, 0.39425051211086876, 0.3942505113275634, 0.3942505126983478, 0.3864476389708705, 0.39425051508498143, 0.39425051171921605, 0.39425051273506523, 0.3942505152808078, 0.3942505146933287, 0.395071869733642, 0.39383983449279897, 0.39630390376280955, 0.3942505126983478, 0.39425051547663414, 0.39383983370949355, 0.3950718705169474, 0.39055441593487406, 0.39425051449750237, 0.39425051211086876, 0.39425051195175986, 0.3979466138434361, 0.3950718707127738, 0.394250514301676, 0.3942505115233897, 0.3942505119150424, 0.39630390219619877, 0.395071869146163, 0.39425051547663414, 0.3942505148891551, 0.3946611917239195, 0.3975359350504082, 0.39466118992476495, 0.39958932039918843, 0.3958932245781289, 0.3995893231407573, 0.39671457726852605, 0.3893223839006874, 0.39630390297950413, 0.39671457684015593, 0.39671458118505304, 0.39630390337115684, 0.3971252550457048, 0.39630390141289334, 0.39507186992946836, 0.3921971271904587, 0.39712525821564376, 0.3983572883282844, 0.3975359338387327, 0.3934291590655364, 0.3963039023920251, 0.3971252570406857, 0.39999999880056364, 0.39712525860729647, 0.39671457781928765, 0.3979466118851726, 0.39876796907957573, 0.3958932245781289, 0.395893225165608, 0.39753593325125364, 0.39425051171921605, 0.39425051113173704, 0.39589322536143434, 0.39589322496978163, 0.3930184802725085, 0.3983572902865478, 0.39917864352770654, 0.39507186953781565, 0.39137576917603273, 0.3950718675795522, 0.39425051351837065, 0.39671457883513683, 0.3979466142350888, 0.3995893211457764, 0.3963038994546299, 0.3963039035669832, 0.3979466128275869, 0.398767964930505, 0.396303900629588, 0.39917864493520844, 0.39630390020121786, 0.4004106799435077, 0.40000000212961156, 0.3967145800100949, 0.3946611917239195, 0.4020533864625426, 0.40328541994094846, 0.3963038996137388, 0.3954825455892747, 0.3967145804384651, 0.39137577097518733, 0.39630390219619877, 0.401642710447801, 0.3987679677087913, 0.39794661208099896, 0.39794660992690917, 0.40164271201441176, 0.3991786447026646, 0.4000000007588271, 0.39835728927069863, 0.4049281295932049, 0.3995893205582973, 0.396714578015114, 0.4041067745528916, 0.3967145791900721, 0.3995893207541237, 0.3999999986047373, 0.39917864434772937, 0.3971252588031228, 0.39466119231139857, 0.39383983805439066]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
