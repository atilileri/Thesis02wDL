{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf8.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 14:36:45 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '2', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '03', '02', '05', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001B1097CBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001B104F96EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6156, Accuracy:0.1795, Validation Loss:1.6132, Validation Accuracy:0.1773\n",
    "Epoch #2: Loss:1.6091, Accuracy:0.2000, Validation Loss:1.6085, Validation Accuracy:0.2135\n",
    "Epoch #3: Loss:1.6064, Accuracy:0.2390, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6067, Accuracy:0.2329, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2348\n",
    "Epoch #15: Loss:1.6043, Accuracy:0.2324, Validation Loss:1.6049, Validation Accuracy:0.2348\n",
    "Epoch #16: Loss:1.6039, Accuracy:0.2337, Validation Loss:1.6062, Validation Accuracy:0.2315\n",
    "Epoch #17: Loss:1.6034, Accuracy:0.2337, Validation Loss:1.6056, Validation Accuracy:0.2282\n",
    "Epoch #18: Loss:1.6033, Accuracy:0.2337, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6031, Accuracy:0.2337, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6035, Accuracy:0.2324, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #22: Loss:1.6031, Accuracy:0.2333, Validation Loss:1.6039, Validation Accuracy:0.2315\n",
    "Epoch #23: Loss:1.6025, Accuracy:0.2300, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6022, Accuracy:0.2341, Validation Loss:1.6039, Validation Accuracy:0.2348\n",
    "Epoch #25: Loss:1.6020, Accuracy:0.2345, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #26: Loss:1.6017, Accuracy:0.2415, Validation Loss:1.6035, Validation Accuracy:0.2479\n",
    "Epoch #27: Loss:1.6012, Accuracy:0.2398, Validation Loss:1.6034, Validation Accuracy:0.2479\n",
    "Epoch #28: Loss:1.6004, Accuracy:0.2415, Validation Loss:1.6030, Validation Accuracy:0.2479\n",
    "Epoch #29: Loss:1.6000, Accuracy:0.2402, Validation Loss:1.6037, Validation Accuracy:0.2463\n",
    "Epoch #30: Loss:1.5998, Accuracy:0.2423, Validation Loss:1.6057, Validation Accuracy:0.2479\n",
    "Epoch #31: Loss:1.5997, Accuracy:0.2423, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #32: Loss:1.6000, Accuracy:0.2431, Validation Loss:1.6020, Validation Accuracy:0.2463\n",
    "Epoch #33: Loss:1.6000, Accuracy:0.2402, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #34: Loss:1.5999, Accuracy:0.2431, Validation Loss:1.6027, Validation Accuracy:0.2430\n",
    "Epoch #35: Loss:1.6008, Accuracy:0.2427, Validation Loss:1.6027, Validation Accuracy:0.2463\n",
    "Epoch #36: Loss:1.6000, Accuracy:0.2435, Validation Loss:1.6023, Validation Accuracy:0.2496\n",
    "Epoch #37: Loss:1.5998, Accuracy:0.2415, Validation Loss:1.6018, Validation Accuracy:0.2463\n",
    "Epoch #38: Loss:1.5989, Accuracy:0.2485, Validation Loss:1.6017, Validation Accuracy:0.2529\n",
    "Epoch #39: Loss:1.6005, Accuracy:0.2308, Validation Loss:1.6011, Validation Accuracy:0.2677\n",
    "Epoch #40: Loss:1.5982, Accuracy:0.2386, Validation Loss:1.6016, Validation Accuracy:0.2479\n",
    "Epoch #41: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6025, Validation Accuracy:0.2496\n",
    "Epoch #42: Loss:1.5982, Accuracy:0.2435, Validation Loss:1.6033, Validation Accuracy:0.2381\n",
    "Epoch #43: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.6017, Validation Accuracy:0.2562\n",
    "Epoch #44: Loss:1.5974, Accuracy:0.2448, Validation Loss:1.6030, Validation Accuracy:0.2644\n",
    "Epoch #45: Loss:1.6006, Accuracy:0.2329, Validation Loss:1.6031, Validation Accuracy:0.2496\n",
    "Epoch #46: Loss:1.6019, Accuracy:0.2386, Validation Loss:1.6002, Validation Accuracy:0.2463\n",
    "Epoch #47: Loss:1.5983, Accuracy:0.2435, Validation Loss:1.5986, Validation Accuracy:0.2562\n",
    "Epoch #48: Loss:1.5979, Accuracy:0.2402, Validation Loss:1.5996, Validation Accuracy:0.2529\n",
    "Epoch #49: Loss:1.5982, Accuracy:0.2374, Validation Loss:1.6001, Validation Accuracy:0.2529\n",
    "Epoch #50: Loss:1.5976, Accuracy:0.2439, Validation Loss:1.5993, Validation Accuracy:0.2463\n",
    "Epoch #51: Loss:1.5983, Accuracy:0.2444, Validation Loss:1.5999, Validation Accuracy:0.2447\n",
    "Epoch #52: Loss:1.5985, Accuracy:0.2431, Validation Loss:1.6001, Validation Accuracy:0.2496\n",
    "Epoch #53: Loss:1.5975, Accuracy:0.2460, Validation Loss:1.6017, Validation Accuracy:0.2414\n",
    "Epoch #54: Loss:1.5982, Accuracy:0.2431, Validation Loss:1.6070, Validation Accuracy:0.2365\n",
    "Epoch #55: Loss:1.6012, Accuracy:0.2353, Validation Loss:1.6012, Validation Accuracy:0.2594\n",
    "Epoch #56: Loss:1.5991, Accuracy:0.2337, Validation Loss:1.6040, Validation Accuracy:0.2677\n",
    "Epoch #57: Loss:1.5989, Accuracy:0.2411, Validation Loss:1.6017, Validation Accuracy:0.2529\n",
    "Epoch #58: Loss:1.5972, Accuracy:0.2390, Validation Loss:1.6014, Validation Accuracy:0.2512\n",
    "Epoch #59: Loss:1.5974, Accuracy:0.2431, Validation Loss:1.6009, Validation Accuracy:0.2512\n",
    "Epoch #60: Loss:1.5978, Accuracy:0.2419, Validation Loss:1.6010, Validation Accuracy:0.2545\n",
    "Epoch #61: Loss:1.5974, Accuracy:0.2415, Validation Loss:1.6004, Validation Accuracy:0.2644\n",
    "Epoch #62: Loss:1.5972, Accuracy:0.2435, Validation Loss:1.6001, Validation Accuracy:0.2529\n",
    "Epoch #63: Loss:1.5969, Accuracy:0.2423, Validation Loss:1.5997, Validation Accuracy:0.2660\n",
    "Epoch #64: Loss:1.5965, Accuracy:0.2423, Validation Loss:1.5996, Validation Accuracy:0.2660\n",
    "Epoch #65: Loss:1.5963, Accuracy:0.2444, Validation Loss:1.5994, Validation Accuracy:0.2627\n",
    "Epoch #66: Loss:1.5961, Accuracy:0.2448, Validation Loss:1.6002, Validation Accuracy:0.2627\n",
    "Epoch #67: Loss:1.5960, Accuracy:0.2435, Validation Loss:1.5996, Validation Accuracy:0.2627\n",
    "Epoch #68: Loss:1.5958, Accuracy:0.2411, Validation Loss:1.5998, Validation Accuracy:0.2677\n",
    "Epoch #69: Loss:1.5954, Accuracy:0.2427, Validation Loss:1.5993, Validation Accuracy:0.2611\n",
    "Epoch #70: Loss:1.5952, Accuracy:0.2444, Validation Loss:1.5995, Validation Accuracy:0.2627\n",
    "Epoch #71: Loss:1.5951, Accuracy:0.2485, Validation Loss:1.5986, Validation Accuracy:0.2512\n",
    "Epoch #72: Loss:1.5950, Accuracy:0.2513, Validation Loss:1.5985, Validation Accuracy:0.2627\n",
    "Epoch #73: Loss:1.5950, Accuracy:0.2435, Validation Loss:1.5987, Validation Accuracy:0.2709\n",
    "Epoch #74: Loss:1.5944, Accuracy:0.2435, Validation Loss:1.5981, Validation Accuracy:0.2709\n",
    "Epoch #75: Loss:1.5941, Accuracy:0.2435, Validation Loss:1.5981, Validation Accuracy:0.2627\n",
    "Epoch #76: Loss:1.5946, Accuracy:0.2431, Validation Loss:1.5981, Validation Accuracy:0.2578\n",
    "Epoch #77: Loss:1.5947, Accuracy:0.2419, Validation Loss:1.5994, Validation Accuracy:0.2627\n",
    "Epoch #78: Loss:1.5941, Accuracy:0.2431, Validation Loss:1.5984, Validation Accuracy:0.2627\n",
    "Epoch #79: Loss:1.5943, Accuracy:0.2448, Validation Loss:1.5976, Validation Accuracy:0.2611\n",
    "Epoch #80: Loss:1.5933, Accuracy:0.2407, Validation Loss:1.5983, Validation Accuracy:0.2611\n",
    "Epoch #81: Loss:1.5927, Accuracy:0.2411, Validation Loss:1.5995, Validation Accuracy:0.2529\n",
    "Epoch #82: Loss:1.5934, Accuracy:0.2419, Validation Loss:1.5993, Validation Accuracy:0.2479\n",
    "Epoch #83: Loss:1.5930, Accuracy:0.2476, Validation Loss:1.6003, Validation Accuracy:0.2545\n",
    "Epoch #84: Loss:1.5935, Accuracy:0.2407, Validation Loss:1.5998, Validation Accuracy:0.2627\n",
    "Epoch #85: Loss:1.5920, Accuracy:0.2386, Validation Loss:1.5996, Validation Accuracy:0.2512\n",
    "Epoch #86: Loss:1.5928, Accuracy:0.2509, Validation Loss:1.6005, Validation Accuracy:0.2611\n",
    "Epoch #87: Loss:1.5931, Accuracy:0.2559, Validation Loss:1.5983, Validation Accuracy:0.2677\n",
    "Epoch #88: Loss:1.5945, Accuracy:0.2464, Validation Loss:1.5985, Validation Accuracy:0.2578\n",
    "Epoch #89: Loss:1.5959, Accuracy:0.2415, Validation Loss:1.5976, Validation Accuracy:0.2611\n",
    "Epoch #90: Loss:1.5955, Accuracy:0.2444, Validation Loss:1.5996, Validation Accuracy:0.2512\n",
    "Epoch #91: Loss:1.5999, Accuracy:0.2333, Validation Loss:1.6041, Validation Accuracy:0.2479\n",
    "Epoch #92: Loss:1.5997, Accuracy:0.2419, Validation Loss:1.6027, Validation Accuracy:0.2282\n",
    "Epoch #93: Loss:1.5990, Accuracy:0.2390, Validation Loss:1.6018, Validation Accuracy:0.2529\n",
    "Epoch #94: Loss:1.5984, Accuracy:0.2398, Validation Loss:1.5987, Validation Accuracy:0.2414\n",
    "Epoch #95: Loss:1.5975, Accuracy:0.2398, Validation Loss:1.6004, Validation Accuracy:0.2594\n",
    "Epoch #96: Loss:1.5962, Accuracy:0.2468, Validation Loss:1.5982, Validation Accuracy:0.2562\n",
    "Epoch #97: Loss:1.5941, Accuracy:0.2468, Validation Loss:1.5975, Validation Accuracy:0.2627\n",
    "Epoch #98: Loss:1.5930, Accuracy:0.2501, Validation Loss:1.5992, Validation Accuracy:0.2627\n",
    "Epoch #99: Loss:1.5939, Accuracy:0.2390, Validation Loss:1.5984, Validation Accuracy:0.2578\n",
    "Epoch #100: Loss:1.5941, Accuracy:0.2439, Validation Loss:1.5975, Validation Accuracy:0.2463\n",
    "Epoch #101: Loss:1.5957, Accuracy:0.2415, Validation Loss:1.5981, Validation Accuracy:0.2660\n",
    "Epoch #102: Loss:1.5975, Accuracy:0.2329, Validation Loss:1.5991, Validation Accuracy:0.2660\n",
    "Epoch #103: Loss:1.5943, Accuracy:0.2386, Validation Loss:1.5971, Validation Accuracy:0.2414\n",
    "Epoch #104: Loss:1.5950, Accuracy:0.2353, Validation Loss:1.5972, Validation Accuracy:0.2677\n",
    "Epoch #105: Loss:1.5952, Accuracy:0.2402, Validation Loss:1.5975, Validation Accuracy:0.2726\n",
    "Epoch #106: Loss:1.5939, Accuracy:0.2415, Validation Loss:1.5971, Validation Accuracy:0.2512\n",
    "Epoch #107: Loss:1.5943, Accuracy:0.2526, Validation Loss:1.5958, Validation Accuracy:0.2545\n",
    "Epoch #108: Loss:1.5931, Accuracy:0.2505, Validation Loss:1.5980, Validation Accuracy:0.2627\n",
    "Epoch #109: Loss:1.5947, Accuracy:0.2476, Validation Loss:1.5988, Validation Accuracy:0.2578\n",
    "Epoch #110: Loss:1.5931, Accuracy:0.2464, Validation Loss:1.5980, Validation Accuracy:0.2479\n",
    "Epoch #111: Loss:1.5945, Accuracy:0.2534, Validation Loss:1.5966, Validation Accuracy:0.2709\n",
    "Epoch #112: Loss:1.5935, Accuracy:0.2411, Validation Loss:1.5968, Validation Accuracy:0.2594\n",
    "Epoch #113: Loss:1.5933, Accuracy:0.2452, Validation Loss:1.5978, Validation Accuracy:0.2644\n",
    "Epoch #114: Loss:1.5938, Accuracy:0.2444, Validation Loss:1.5969, Validation Accuracy:0.2512\n",
    "Epoch #115: Loss:1.5933, Accuracy:0.2374, Validation Loss:1.5968, Validation Accuracy:0.2709\n",
    "Epoch #116: Loss:1.5924, Accuracy:0.2460, Validation Loss:1.5969, Validation Accuracy:0.2693\n",
    "Epoch #117: Loss:1.5920, Accuracy:0.2452, Validation Loss:1.5964, Validation Accuracy:0.2627\n",
    "Epoch #118: Loss:1.5918, Accuracy:0.2435, Validation Loss:1.5979, Validation Accuracy:0.2644\n",
    "Epoch #119: Loss:1.5912, Accuracy:0.2444, Validation Loss:1.5980, Validation Accuracy:0.2660\n",
    "Epoch #120: Loss:1.5908, Accuracy:0.2456, Validation Loss:1.5979, Validation Accuracy:0.2660\n",
    "Epoch #121: Loss:1.5905, Accuracy:0.2485, Validation Loss:1.5973, Validation Accuracy:0.2644\n",
    "Epoch #122: Loss:1.5904, Accuracy:0.2431, Validation Loss:1.5967, Validation Accuracy:0.2660\n",
    "Epoch #123: Loss:1.5910, Accuracy:0.2411, Validation Loss:1.5986, Validation Accuracy:0.2611\n",
    "Epoch #124: Loss:1.5920, Accuracy:0.2419, Validation Loss:1.5985, Validation Accuracy:0.2644\n",
    "Epoch #125: Loss:1.5914, Accuracy:0.2423, Validation Loss:1.6001, Validation Accuracy:0.2644\n",
    "Epoch #126: Loss:1.5900, Accuracy:0.2505, Validation Loss:1.5987, Validation Accuracy:0.2496\n",
    "Epoch #127: Loss:1.5921, Accuracy:0.2501, Validation Loss:1.5996, Validation Accuracy:0.2594\n",
    "Epoch #128: Loss:1.5931, Accuracy:0.2419, Validation Loss:1.5981, Validation Accuracy:0.2644\n",
    "Epoch #129: Loss:1.5890, Accuracy:0.2480, Validation Loss:1.5982, Validation Accuracy:0.2512\n",
    "Epoch #130: Loss:1.5904, Accuracy:0.2526, Validation Loss:1.5992, Validation Accuracy:0.2611\n",
    "Epoch #131: Loss:1.5928, Accuracy:0.2423, Validation Loss:1.6004, Validation Accuracy:0.2611\n",
    "Epoch #132: Loss:1.5942, Accuracy:0.2456, Validation Loss:1.6024, Validation Accuracy:0.2627\n",
    "Epoch #133: Loss:1.5921, Accuracy:0.2431, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #134: Loss:1.5910, Accuracy:0.2485, Validation Loss:1.5990, Validation Accuracy:0.2545\n",
    "Epoch #135: Loss:1.5888, Accuracy:0.2554, Validation Loss:1.5972, Validation Accuracy:0.2726\n",
    "Epoch #136: Loss:1.5882, Accuracy:0.2489, Validation Loss:1.5983, Validation Accuracy:0.2709\n",
    "Epoch #137: Loss:1.5894, Accuracy:0.2513, Validation Loss:1.5998, Validation Accuracy:0.2545\n",
    "Epoch #138: Loss:1.5879, Accuracy:0.2534, Validation Loss:1.6016, Validation Accuracy:0.2660\n",
    "Epoch #139: Loss:1.5906, Accuracy:0.2431, Validation Loss:1.6049, Validation Accuracy:0.2529\n",
    "Epoch #140: Loss:1.5887, Accuracy:0.2472, Validation Loss:1.6031, Validation Accuracy:0.2562\n",
    "Epoch #141: Loss:1.5886, Accuracy:0.2550, Validation Loss:1.6023, Validation Accuracy:0.2529\n",
    "Epoch #142: Loss:1.5889, Accuracy:0.2559, Validation Loss:1.6047, Validation Accuracy:0.2545\n",
    "Epoch #143: Loss:1.5909, Accuracy:0.2497, Validation Loss:1.6052, Validation Accuracy:0.2414\n",
    "Epoch #144: Loss:1.5926, Accuracy:0.2501, Validation Loss:1.6051, Validation Accuracy:0.2447\n",
    "Epoch #145: Loss:1.5892, Accuracy:0.2534, Validation Loss:1.6063, Validation Accuracy:0.2578\n",
    "Epoch #146: Loss:1.5888, Accuracy:0.2485, Validation Loss:1.6035, Validation Accuracy:0.2496\n",
    "Epoch #147: Loss:1.5888, Accuracy:0.2587, Validation Loss:1.6045, Validation Accuracy:0.2479\n",
    "Epoch #148: Loss:1.5884, Accuracy:0.2505, Validation Loss:1.6040, Validation Accuracy:0.2627\n",
    "Epoch #149: Loss:1.5880, Accuracy:0.2587, Validation Loss:1.6050, Validation Accuracy:0.2496\n",
    "Epoch #150: Loss:1.5881, Accuracy:0.2513, Validation Loss:1.6034, Validation Accuracy:0.2594\n",
    "Epoch #151: Loss:1.5889, Accuracy:0.2448, Validation Loss:1.6031, Validation Accuracy:0.2447\n",
    "Epoch #152: Loss:1.5871, Accuracy:0.2472, Validation Loss:1.6063, Validation Accuracy:0.2496\n",
    "Epoch #153: Loss:1.5868, Accuracy:0.2513, Validation Loss:1.6057, Validation Accuracy:0.2381\n",
    "Epoch #154: Loss:1.5879, Accuracy:0.2595, Validation Loss:1.6126, Validation Accuracy:0.2315\n",
    "Epoch #155: Loss:1.5878, Accuracy:0.2583, Validation Loss:1.6116, Validation Accuracy:0.2332\n",
    "Epoch #156: Loss:1.5889, Accuracy:0.2534, Validation Loss:1.6115, Validation Accuracy:0.2348\n",
    "Epoch #157: Loss:1.5884, Accuracy:0.2526, Validation Loss:1.6095, Validation Accuracy:0.2397\n",
    "Epoch #158: Loss:1.5906, Accuracy:0.2505, Validation Loss:1.6095, Validation Accuracy:0.2315\n",
    "Epoch #159: Loss:1.5907, Accuracy:0.2530, Validation Loss:1.6104, Validation Accuracy:0.2332\n",
    "Epoch #160: Loss:1.5919, Accuracy:0.2505, Validation Loss:1.6101, Validation Accuracy:0.2397\n",
    "Epoch #161: Loss:1.5902, Accuracy:0.2538, Validation Loss:1.6093, Validation Accuracy:0.2348\n",
    "Epoch #162: Loss:1.5888, Accuracy:0.2497, Validation Loss:1.6076, Validation Accuracy:0.2414\n",
    "Epoch #163: Loss:1.5892, Accuracy:0.2534, Validation Loss:1.6090, Validation Accuracy:0.2365\n",
    "Epoch #164: Loss:1.5912, Accuracy:0.2517, Validation Loss:1.6073, Validation Accuracy:0.2381\n",
    "Epoch #165: Loss:1.5898, Accuracy:0.2522, Validation Loss:1.6066, Validation Accuracy:0.2479\n",
    "Epoch #166: Loss:1.5898, Accuracy:0.2460, Validation Loss:1.6090, Validation Accuracy:0.2562\n",
    "Epoch #167: Loss:1.5889, Accuracy:0.2505, Validation Loss:1.6071, Validation Accuracy:0.2414\n",
    "Epoch #168: Loss:1.5879, Accuracy:0.2534, Validation Loss:1.6101, Validation Accuracy:0.2184\n",
    "Epoch #169: Loss:1.5868, Accuracy:0.2419, Validation Loss:1.6096, Validation Accuracy:0.2414\n",
    "Epoch #170: Loss:1.5891, Accuracy:0.2542, Validation Loss:1.6126, Validation Accuracy:0.2348\n",
    "Epoch #171: Loss:1.5937, Accuracy:0.2444, Validation Loss:1.6132, Validation Accuracy:0.2447\n",
    "Epoch #172: Loss:1.5949, Accuracy:0.2505, Validation Loss:1.6107, Validation Accuracy:0.2332\n",
    "Epoch #173: Loss:1.5935, Accuracy:0.2530, Validation Loss:1.6130, Validation Accuracy:0.2430\n",
    "Epoch #174: Loss:1.5944, Accuracy:0.2456, Validation Loss:1.6126, Validation Accuracy:0.2250\n",
    "Epoch #175: Loss:1.6076, Accuracy:0.2423, Validation Loss:1.6273, Validation Accuracy:0.2315\n",
    "Epoch #176: Loss:1.6054, Accuracy:0.2448, Validation Loss:1.6160, Validation Accuracy:0.2200\n",
    "Epoch #177: Loss:1.6097, Accuracy:0.2197, Validation Loss:1.6194, Validation Accuracy:0.1921\n",
    "Epoch #178: Loss:1.6053, Accuracy:0.2172, Validation Loss:1.6047, Validation Accuracy:0.2447\n",
    "Epoch #179: Loss:1.5974, Accuracy:0.2476, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #180: Loss:1.6004, Accuracy:0.2374, Validation Loss:1.6051, Validation Accuracy:0.2381\n",
    "Epoch #181: Loss:1.5985, Accuracy:0.2431, Validation Loss:1.6030, Validation Accuracy:0.2381\n",
    "Epoch #182: Loss:1.5953, Accuracy:0.2505, Validation Loss:1.6042, Validation Accuracy:0.2397\n",
    "Epoch #183: Loss:1.5958, Accuracy:0.2526, Validation Loss:1.6044, Validation Accuracy:0.2397\n",
    "Epoch #184: Loss:1.5953, Accuracy:0.2550, Validation Loss:1.6040, Validation Accuracy:0.2348\n",
    "Epoch #185: Loss:1.5943, Accuracy:0.2546, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #186: Loss:1.5938, Accuracy:0.2554, Validation Loss:1.6047, Validation Accuracy:0.2397\n",
    "Epoch #187: Loss:1.5940, Accuracy:0.2517, Validation Loss:1.6065, Validation Accuracy:0.2381\n",
    "Epoch #188: Loss:1.5946, Accuracy:0.2517, Validation Loss:1.6055, Validation Accuracy:0.2381\n",
    "Epoch #189: Loss:1.5935, Accuracy:0.2522, Validation Loss:1.6062, Validation Accuracy:0.2365\n",
    "Epoch #190: Loss:1.5929, Accuracy:0.2509, Validation Loss:1.6059, Validation Accuracy:0.2414\n",
    "Epoch #191: Loss:1.5927, Accuracy:0.2517, Validation Loss:1.6071, Validation Accuracy:0.2414\n",
    "Epoch #192: Loss:1.5926, Accuracy:0.2522, Validation Loss:1.6070, Validation Accuracy:0.2414\n",
    "Epoch #193: Loss:1.5922, Accuracy:0.2530, Validation Loss:1.6067, Validation Accuracy:0.2397\n",
    "Epoch #194: Loss:1.5925, Accuracy:0.2554, Validation Loss:1.6067, Validation Accuracy:0.2381\n",
    "Epoch #195: Loss:1.5923, Accuracy:0.2534, Validation Loss:1.6069, Validation Accuracy:0.2381\n",
    "Epoch #196: Loss:1.5930, Accuracy:0.2517, Validation Loss:1.6075, Validation Accuracy:0.2381\n",
    "Epoch #197: Loss:1.5935, Accuracy:0.2554, Validation Loss:1.6074, Validation Accuracy:0.2430\n",
    "Epoch #198: Loss:1.5922, Accuracy:0.2534, Validation Loss:1.6067, Validation Accuracy:0.2365\n",
    "Epoch #199: Loss:1.5920, Accuracy:0.2513, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #200: Loss:1.5913, Accuracy:0.2517, Validation Loss:1.6054, Validation Accuracy:0.2414\n",
    "Epoch #201: Loss:1.5915, Accuracy:0.2489, Validation Loss:1.6063, Validation Accuracy:0.2381\n",
    "Epoch #202: Loss:1.5912, Accuracy:0.2542, Validation Loss:1.6068, Validation Accuracy:0.2381\n",
    "Epoch #203: Loss:1.5911, Accuracy:0.2509, Validation Loss:1.6069, Validation Accuracy:0.2414\n",
    "Epoch #204: Loss:1.5912, Accuracy:0.2530, Validation Loss:1.6084, Validation Accuracy:0.2397\n",
    "Epoch #205: Loss:1.5908, Accuracy:0.2538, Validation Loss:1.6073, Validation Accuracy:0.2430\n",
    "Epoch #206: Loss:1.5904, Accuracy:0.2530, Validation Loss:1.6086, Validation Accuracy:0.2414\n",
    "Epoch #207: Loss:1.5909, Accuracy:0.2517, Validation Loss:1.6095, Validation Accuracy:0.2397\n",
    "Epoch #208: Loss:1.5907, Accuracy:0.2538, Validation Loss:1.6092, Validation Accuracy:0.2414\n",
    "Epoch #209: Loss:1.5904, Accuracy:0.2522, Validation Loss:1.6112, Validation Accuracy:0.2381\n",
    "Epoch #210: Loss:1.5909, Accuracy:0.2534, Validation Loss:1.6103, Validation Accuracy:0.2381\n",
    "Epoch #211: Loss:1.5900, Accuracy:0.2534, Validation Loss:1.6088, Validation Accuracy:0.2430\n",
    "Epoch #212: Loss:1.5902, Accuracy:0.2538, Validation Loss:1.6088, Validation Accuracy:0.2414\n",
    "Epoch #213: Loss:1.5912, Accuracy:0.2542, Validation Loss:1.6099, Validation Accuracy:0.2381\n",
    "Epoch #214: Loss:1.5933, Accuracy:0.2513, Validation Loss:1.6070, Validation Accuracy:0.2397\n",
    "Epoch #215: Loss:1.5933, Accuracy:0.2444, Validation Loss:1.6061, Validation Accuracy:0.2463\n",
    "Epoch #216: Loss:1.5936, Accuracy:0.2530, Validation Loss:1.6085, Validation Accuracy:0.2512\n",
    "Epoch #217: Loss:1.5936, Accuracy:0.2530, Validation Loss:1.6081, Validation Accuracy:0.2479\n",
    "Epoch #218: Loss:1.5941, Accuracy:0.2407, Validation Loss:1.6104, Validation Accuracy:0.2479\n",
    "Epoch #219: Loss:1.5938, Accuracy:0.2444, Validation Loss:1.6096, Validation Accuracy:0.2479\n",
    "Epoch #220: Loss:1.5927, Accuracy:0.2415, Validation Loss:1.6082, Validation Accuracy:0.2496\n",
    "Epoch #221: Loss:1.5914, Accuracy:0.2513, Validation Loss:1.6096, Validation Accuracy:0.2479\n",
    "Epoch #222: Loss:1.5931, Accuracy:0.2431, Validation Loss:1.6104, Validation Accuracy:0.2479\n",
    "Epoch #223: Loss:1.5919, Accuracy:0.2546, Validation Loss:1.6081, Validation Accuracy:0.2430\n",
    "Epoch #224: Loss:1.5986, Accuracy:0.2329, Validation Loss:1.6092, Validation Accuracy:0.2250\n",
    "Epoch #225: Loss:1.5976, Accuracy:0.2464, Validation Loss:1.6064, Validation Accuracy:0.2365\n",
    "Epoch #226: Loss:1.5955, Accuracy:0.2480, Validation Loss:1.6069, Validation Accuracy:0.2430\n",
    "Epoch #227: Loss:1.5967, Accuracy:0.2423, Validation Loss:1.6073, Validation Accuracy:0.2430\n",
    "Epoch #228: Loss:1.5960, Accuracy:0.2452, Validation Loss:1.6066, Validation Accuracy:0.2496\n",
    "Epoch #229: Loss:1.5959, Accuracy:0.2460, Validation Loss:1.6076, Validation Accuracy:0.2512\n",
    "Epoch #230: Loss:1.5955, Accuracy:0.2448, Validation Loss:1.6044, Validation Accuracy:0.2545\n",
    "Epoch #231: Loss:1.5984, Accuracy:0.2398, Validation Loss:1.6060, Validation Accuracy:0.2578\n",
    "Epoch #232: Loss:1.5970, Accuracy:0.2444, Validation Loss:1.6037, Validation Accuracy:0.2381\n",
    "Epoch #233: Loss:1.5944, Accuracy:0.2480, Validation Loss:1.6044, Validation Accuracy:0.2463\n",
    "Epoch #234: Loss:1.5935, Accuracy:0.2493, Validation Loss:1.6045, Validation Accuracy:0.2496\n",
    "Epoch #235: Loss:1.5934, Accuracy:0.2476, Validation Loss:1.6056, Validation Accuracy:0.2529\n",
    "Epoch #236: Loss:1.5935, Accuracy:0.2366, Validation Loss:1.6047, Validation Accuracy:0.2594\n",
    "Epoch #237: Loss:1.5923, Accuracy:0.2526, Validation Loss:1.6045, Validation Accuracy:0.2463\n",
    "Epoch #238: Loss:1.5919, Accuracy:0.2505, Validation Loss:1.6055, Validation Accuracy:0.2447\n",
    "Epoch #239: Loss:1.5907, Accuracy:0.2476, Validation Loss:1.6066, Validation Accuracy:0.2512\n",
    "Epoch #240: Loss:1.5906, Accuracy:0.2444, Validation Loss:1.6062, Validation Accuracy:0.2660\n",
    "Epoch #241: Loss:1.5911, Accuracy:0.2456, Validation Loss:1.6068, Validation Accuracy:0.2496\n",
    "Epoch #242: Loss:1.5902, Accuracy:0.2423, Validation Loss:1.6069, Validation Accuracy:0.2545\n",
    "Epoch #243: Loss:1.5904, Accuracy:0.2460, Validation Loss:1.6077, Validation Accuracy:0.2512\n",
    "Epoch #244: Loss:1.5899, Accuracy:0.2390, Validation Loss:1.6095, Validation Accuracy:0.2627\n",
    "Epoch #245: Loss:1.5903, Accuracy:0.2349, Validation Loss:1.6087, Validation Accuracy:0.2512\n",
    "Epoch #246: Loss:1.5902, Accuracy:0.2435, Validation Loss:1.6064, Validation Accuracy:0.2447\n",
    "Epoch #247: Loss:1.5896, Accuracy:0.2497, Validation Loss:1.6082, Validation Accuracy:0.2545\n",
    "Epoch #248: Loss:1.5886, Accuracy:0.2468, Validation Loss:1.6090, Validation Accuracy:0.2627\n",
    "Epoch #249: Loss:1.5897, Accuracy:0.2419, Validation Loss:1.6104, Validation Accuracy:0.2627\n",
    "Epoch #250: Loss:1.5893, Accuracy:0.2472, Validation Loss:1.6073, Validation Accuracy:0.2512\n",
    "Epoch #251: Loss:1.5880, Accuracy:0.2575, Validation Loss:1.6076, Validation Accuracy:0.2479\n",
    "Epoch #252: Loss:1.5884, Accuracy:0.2534, Validation Loss:1.6075, Validation Accuracy:0.2512\n",
    "Epoch #253: Loss:1.5878, Accuracy:0.2517, Validation Loss:1.6088, Validation Accuracy:0.2611\n",
    "Epoch #254: Loss:1.5865, Accuracy:0.2546, Validation Loss:1.6094, Validation Accuracy:0.2545\n",
    "Epoch #255: Loss:1.5874, Accuracy:0.2550, Validation Loss:1.6085, Validation Accuracy:0.2463\n",
    "Epoch #256: Loss:1.5868, Accuracy:0.2579, Validation Loss:1.6094, Validation Accuracy:0.2512\n",
    "Epoch #257: Loss:1.5872, Accuracy:0.2460, Validation Loss:1.6109, Validation Accuracy:0.2463\n",
    "Epoch #258: Loss:1.5854, Accuracy:0.2522, Validation Loss:1.6121, Validation Accuracy:0.2430\n",
    "Epoch #259: Loss:1.5869, Accuracy:0.2530, Validation Loss:1.6102, Validation Accuracy:0.2447\n",
    "Epoch #260: Loss:1.5858, Accuracy:0.2509, Validation Loss:1.6086, Validation Accuracy:0.2381\n",
    "Epoch #261: Loss:1.5878, Accuracy:0.2501, Validation Loss:1.6073, Validation Accuracy:0.2365\n",
    "Epoch #262: Loss:1.5898, Accuracy:0.2513, Validation Loss:1.6085, Validation Accuracy:0.2414\n",
    "Epoch #263: Loss:1.5901, Accuracy:0.2497, Validation Loss:1.6070, Validation Accuracy:0.2430\n",
    "Epoch #264: Loss:1.5921, Accuracy:0.2472, Validation Loss:1.6047, Validation Accuracy:0.2447\n",
    "Epoch #265: Loss:1.5977, Accuracy:0.2448, Validation Loss:1.6046, Validation Accuracy:0.2545\n",
    "Epoch #266: Loss:1.5964, Accuracy:0.2419, Validation Loss:1.6041, Validation Accuracy:0.2496\n",
    "Epoch #267: Loss:1.5950, Accuracy:0.2435, Validation Loss:1.6043, Validation Accuracy:0.2463\n",
    "Epoch #268: Loss:1.5945, Accuracy:0.2456, Validation Loss:1.6029, Validation Accuracy:0.2463\n",
    "Epoch #269: Loss:1.5943, Accuracy:0.2427, Validation Loss:1.6029, Validation Accuracy:0.2512\n",
    "Epoch #270: Loss:1.5940, Accuracy:0.2480, Validation Loss:1.6028, Validation Accuracy:0.2512\n",
    "Epoch #271: Loss:1.5922, Accuracy:0.2468, Validation Loss:1.6036, Validation Accuracy:0.2512\n",
    "Epoch #272: Loss:1.5924, Accuracy:0.2468, Validation Loss:1.6045, Validation Accuracy:0.2529\n",
    "Epoch #273: Loss:1.5930, Accuracy:0.2468, Validation Loss:1.6063, Validation Accuracy:0.2479\n",
    "Epoch #274: Loss:1.5920, Accuracy:0.2468, Validation Loss:1.6042, Validation Accuracy:0.2594\n",
    "Epoch #275: Loss:1.5924, Accuracy:0.2456, Validation Loss:1.6043, Validation Accuracy:0.2496\n",
    "Epoch #276: Loss:1.5912, Accuracy:0.2472, Validation Loss:1.6059, Validation Accuracy:0.2447\n",
    "Epoch #277: Loss:1.5915, Accuracy:0.2468, Validation Loss:1.6053, Validation Accuracy:0.2463\n",
    "Epoch #278: Loss:1.5904, Accuracy:0.2489, Validation Loss:1.6073, Validation Accuracy:0.2447\n",
    "Epoch #279: Loss:1.5902, Accuracy:0.2476, Validation Loss:1.6064, Validation Accuracy:0.2479\n",
    "Epoch #280: Loss:1.5894, Accuracy:0.2513, Validation Loss:1.6060, Validation Accuracy:0.2463\n",
    "Epoch #281: Loss:1.5886, Accuracy:0.2542, Validation Loss:1.6062, Validation Accuracy:0.2447\n",
    "Epoch #282: Loss:1.5907, Accuracy:0.2480, Validation Loss:1.6058, Validation Accuracy:0.2479\n",
    "Epoch #283: Loss:1.5910, Accuracy:0.2480, Validation Loss:1.6060, Validation Accuracy:0.2447\n",
    "Epoch #284: Loss:1.5918, Accuracy:0.2394, Validation Loss:1.6067, Validation Accuracy:0.2463\n",
    "Epoch #285: Loss:1.5924, Accuracy:0.2456, Validation Loss:1.6071, Validation Accuracy:0.2397\n",
    "Epoch #286: Loss:1.5944, Accuracy:0.2444, Validation Loss:1.6087, Validation Accuracy:0.2332\n",
    "Epoch #287: Loss:1.5948, Accuracy:0.2435, Validation Loss:1.6065, Validation Accuracy:0.2365\n",
    "Epoch #288: Loss:1.5943, Accuracy:0.2468, Validation Loss:1.6062, Validation Accuracy:0.2397\n",
    "Epoch #289: Loss:1.5942, Accuracy:0.2493, Validation Loss:1.6064, Validation Accuracy:0.2397\n",
    "Epoch #290: Loss:1.5946, Accuracy:0.2501, Validation Loss:1.6062, Validation Accuracy:0.2447\n",
    "Epoch #291: Loss:1.5933, Accuracy:0.2448, Validation Loss:1.6055, Validation Accuracy:0.2430\n",
    "Epoch #292: Loss:1.5925, Accuracy:0.2468, Validation Loss:1.6062, Validation Accuracy:0.2463\n",
    "Epoch #293: Loss:1.5929, Accuracy:0.2485, Validation Loss:1.6075, Validation Accuracy:0.2414\n",
    "Epoch #294: Loss:1.5915, Accuracy:0.2489, Validation Loss:1.6075, Validation Accuracy:0.2479\n",
    "Epoch #295: Loss:1.5915, Accuracy:0.2468, Validation Loss:1.6080, Validation Accuracy:0.2447\n",
    "Epoch #296: Loss:1.5905, Accuracy:0.2505, Validation Loss:1.6080, Validation Accuracy:0.2447\n",
    "Epoch #297: Loss:1.5904, Accuracy:0.2501, Validation Loss:1.6080, Validation Accuracy:0.2463\n",
    "Epoch #298: Loss:1.5899, Accuracy:0.2468, Validation Loss:1.6070, Validation Accuracy:0.2479\n",
    "Epoch #299: Loss:1.5891, Accuracy:0.2517, Validation Loss:1.6060, Validation Accuracy:0.2529\n",
    "Epoch #300: Loss:1.5893, Accuracy:0.2468, Validation Loss:1.6058, Validation Accuracy:0.2562\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60579550, Accuracy:0.2562\n",
    "Labels: ['01', '03', '02', '05', '04']\n",
    "Confusion Matrix:\n",
    "      01  03  02   05  04\n",
    "t:01   0   2   3  109  12\n",
    "t:03   0   5   1   91  18\n",
    "t:02   0   4   3   90  17\n",
    "t:05   0   2   2  127  11\n",
    "t:04   0   0   4   87  21\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.00      0.00      0.00       126\n",
    "          03       0.38      0.04      0.08       115\n",
    "          02       0.23      0.03      0.05       114\n",
    "          05       0.25      0.89      0.39       142\n",
    "          04       0.27      0.19      0.22       112\n",
    "\n",
    "    accuracy                           0.26       609\n",
    "   macro avg       0.23      0.23      0.15       609\n",
    "weighted avg       0.22      0.26      0.16       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 14:52:20 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.613169374724327, 1.6085110063036088, 1.6064634559972728, 1.6058380975707607, 1.6068634053169213, 1.6067387801281532, 1.6061361303861896, 1.604995407690164, 1.6046040641458947, 1.6048164301122154, 1.604541076619441, 1.604907278356881, 1.6044231682575394, 1.6048744116314917, 1.6049438926386717, 1.6062343529684007, 1.6056331648615194, 1.605522198630084, 1.6050673159472462, 1.605139902464079, 1.6051788011012211, 1.6038927898814135, 1.6044998213966883, 1.6038637190616776, 1.6036419610084571, 1.6035078348980356, 1.603427553020283, 1.6029766752997838, 1.6036628676556992, 1.6056721846654105, 1.605640129307025, 1.6020149765734994, 1.602164942838484, 1.6026785453943588, 1.6026659849633529, 1.6023308214882912, 1.6017669599827482, 1.6016990461177232, 1.6010811056801055, 1.6016075842094735, 1.602517072594616, 1.6033487380627536, 1.601677538530384, 1.60298394882816, 1.603140487263747, 1.6001873572275949, 1.5985657587427224, 1.5996055460132792, 1.6000602194436861, 1.5992851055705881, 1.5998936519638463, 1.6001240220563164, 1.6017005467062513, 1.6069752138432218, 1.6011870952662577, 1.603979927369918, 1.6016696924450753, 1.6013638156975432, 1.6009094836480904, 1.6009536946348368, 1.6004190431244072, 1.6001465569184528, 1.5997320875549943, 1.5995519811100951, 1.5993616101385533, 1.600160763377235, 1.5996344067976, 1.5997578488977868, 1.5993293768470902, 1.599473501269649, 1.5986281496354904, 1.5984514517149901, 1.5987310100267282, 1.5980628529205698, 1.598062227512228, 1.5981449517123218, 1.5994374429063845, 1.5983560052019818, 1.5976103661682806, 1.5982838804498682, 1.5994692081692574, 1.5993233388672126, 1.6003172096164746, 1.5998348986182502, 1.599627639859768, 1.6004942810202663, 1.5983319399979314, 1.5985252457886494, 1.5975743504776352, 1.5996412992085924, 1.60405782212569, 1.602738683055383, 1.601796776790337, 1.5987478253876635, 1.6003934062760452, 1.5982293614808758, 1.597499659300242, 1.5992265019706513, 1.598352166819455, 1.5974869461873873, 1.598133775596744, 1.5990850483059686, 1.5971035282012864, 1.5971796473454567, 1.5975303512880172, 1.59711873316021, 1.5957630034933732, 1.5980082540872258, 1.598755628800353, 1.5980159995591112, 1.5965747500483822, 1.59675936824191, 1.5977590087990847, 1.5969007083739357, 1.59678839815074, 1.596869677158412, 1.5963861335478784, 1.597854036611485, 1.5980407344100902, 1.5979429346391525, 1.597311833809162, 1.5966847250222769, 1.598594060672328, 1.598454016770048, 1.6001405723772222, 1.5986660415511609, 1.5996334793532423, 1.5981347834926911, 1.598248542040244, 1.5991670913101221, 1.6004232690839344, 1.6023970388230824, 1.6022914720482035, 1.5989969667346997, 1.597242660906123, 1.5982827617617077, 1.599796668257815, 1.601614461352281, 1.6049208862245181, 1.603140841563934, 1.60234041323607, 1.6047387573323617, 1.6051757809368066, 1.605138263678903, 1.6062632509444539, 1.6035444431117016, 1.6044868025286445, 1.603951184033173, 1.6050223589726464, 1.6033656387689275, 1.6030681119568047, 1.6063397876147567, 1.6057345630304372, 1.6125569427737658, 1.6115863291892316, 1.6114798684425542, 1.6095115502283883, 1.6095484373800468, 1.610441204558061, 1.6100671410756353, 1.60925227609174, 1.6075758922276238, 1.6090161794512143, 1.6073142673777439, 1.6066130415363655, 1.608976862504956, 1.60711379317423, 1.6101038798518565, 1.6096327396840688, 1.6126480732841053, 1.613210522482548, 1.610691907174873, 1.612986889183032, 1.612593679592527, 1.6273099232972745, 1.615959200561536, 1.619370805218889, 1.604734270834962, 1.6062145477836747, 1.605054254210837, 1.6030204284367302, 1.6041555457514496, 1.6043884382263585, 1.6040324272193345, 1.6044112336263672, 1.6046928543175383, 1.6064814507276162, 1.6055449401999538, 1.60617482270709, 1.6058856992690238, 1.6071334582048489, 1.6070173344588632, 1.6066533199867787, 1.6067170630926373, 1.6069421061545561, 1.6074653370627041, 1.6073679618647534, 1.6067170916715474, 1.6055238194066315, 1.6053593301616476, 1.606253971215735, 1.6068045621239297, 1.6069278746402909, 1.6084160182276384, 1.6072515564403316, 1.6085979335609524, 1.6095393349971678, 1.6092182853930495, 1.6112163771549468, 1.6103175414606856, 1.6087814931603293, 1.6088318701448112, 1.6099064252255193, 1.6069631570665708, 1.606100326492673, 1.6085466432258217, 1.608099190668128, 1.6104295177412737, 1.6096266232100613, 1.6082039846379572, 1.6096023025575334, 1.6103735676735689, 1.6080640196212994, 1.6091983420116756, 1.6064250011162218, 1.6068886209199778, 1.60734914735033, 1.6065514396955618, 1.6075810182270744, 1.6044019695573253, 1.6059562506151122, 1.6036575096972863, 1.604366968222244, 1.6044514901532327, 1.6055599190722938, 1.6046964217876565, 1.604531277576691, 1.6055112597586094, 1.6066156645322276, 1.6062195911783304, 1.6067798080898466, 1.6068649511227662, 1.6077308014695868, 1.6095208922043223, 1.608670286944347, 1.6063554427893878, 1.6082077971820175, 1.6089878949429992, 1.610377671683363, 1.6072640430751106, 1.607565076284612, 1.6074985353817492, 1.6088060923593581, 1.609372664554953, 1.6085164292496805, 1.6093924386160714, 1.610872131263094, 1.6121173666420046, 1.6102092730000688, 1.608614791398761, 1.6072649348937036, 1.6085013744279082, 1.606951375704485, 1.6047028774894125, 1.6045728943422315, 1.6041058270606305, 1.6043065602360491, 1.6029110138835188, 1.6028985528914603, 1.6028055858925254, 1.603565528083513, 1.604472172867097, 1.60630786458064, 1.604189497888186, 1.6043043406726105, 1.6059312736263807, 1.605329843186001, 1.607325919547496, 1.6064042027164953, 1.605964413417384, 1.6062472477335061, 1.6058299950582444, 1.605981794679889, 1.6067326325305382, 1.6070649594508957, 1.608688659268647, 1.6065064022693727, 1.6061603400507585, 1.606359771907036, 1.6062246245899419, 1.6054532433965523, 1.6061784438116014, 1.607459305542443, 1.6074971917814809, 1.607991733183023, 1.6080182195688508, 1.6080200703469012, 1.6070246377406254, 1.6059921590369715, 1.6057955559055597], 'val_acc': [0.17733990096399938, 0.2134646956116108, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.2348111652339425, 0.2348111652339425, 0.23152709298435298, 0.2282430207347635, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23481116503819652, 0.23152709288648002, 0.23316912871765583, 0.2348111651360695, 0.23481116533181545, 0.24794745442804642, 0.24794745423230044, 0.24794745433017343, 0.2463054181075057, 0.24794745433017343, 0.23481116542968844, 0.2463054181075057, 0.2430213458579162, 0.2430213458579162, 0.24630541869474357, 0.24958949065071412, 0.2463054181075057, 0.2528735628024306, 0.2676518881213293, 0.24794745433017343, 0.24958949045496817, 0.238095237385659, 0.2561576351498931, 0.2643678159696128, 0.24958949045496817, 0.24630541830325164, 0.2561576351498931, 0.25287356319392257, 0.2528735630960496, 0.24630541849899762, 0.2446633822763299, 0.2495894907485871, 0.24137930983099445, 0.23645320155448318, 0.25944170759522855, 0.26765188802345635, 0.2528735628024306, 0.2512315265797629, 0.2512315265797629, 0.25451559882935243, 0.2643678155781209, 0.2528735628024306, 0.26600985199653454, 0.26600985170291563, 0.26272577935545316, 0.26272577945332615, 0.26272577964907207, 0.26765188831707526, 0.2610837435242773, 0.26272577964907207, 0.2512315266776359, 0.26272577945332615, 0.27093596046879176, 0.2709359603709188, 0.26272577935545316, 0.2577996711768149, 0.26272577964907207, 0.26272577955119913, 0.2610837435242773, 0.2610837436221503, 0.2528735629981766, 0.24794745433017343, 0.25451559902509835, 0.26272577964907207, 0.25123152648188996, 0.26108374313278543, 0.2676518878277104, 0.2577996711768149, 0.2610837435242773, 0.251231524891454, 0.24794745442804642, 0.22824302063689053, 0.2528735628024306, 0.24137930983099445, 0.25944170749735557, 0.2561576352477661, 0.26272577964907207, 0.26272577984481804, 0.2577996714704338, 0.24630541830325164, 0.26600985209440753, 0.2660098521922805, 0.24137930983099445, 0.26765188802345635, 0.2725779963978406, 0.25123152648188996, 0.25451559873147944, 0.26272577945332615, 0.25779967127468784, 0.24794745413442745, 0.27093596046879176, 0.25944170749735557, 0.26436781577386687, 0.25123152648188996, 0.27093596046879176, 0.2692939241482511, 0.26272577955119913, 0.2643678156759939, 0.2660098518007886, 0.2660098518986616, 0.26436781577386687, 0.26600985199653454, 0.2610837434264044, 0.2643678156759939, 0.2643678159696128, 0.24958949035709518, 0.25944170730160965, 0.26436781577386687, 0.25123152648188996, 0.2610837433285314, 0.2610837433285314, 0.26272577974694505, 0.2331691292070207, 0.25451559892722536, 0.27257799649571357, 0.2709359603709188, 0.25451559882935243, 0.26600985199653454, 0.2528735626066847, 0.2561576350520201, 0.2528735629003036, 0.25451559912297134, 0.24137930973312147, 0.244663381786965, 0.25779967127468784, 0.24958949016134924, 0.24794745413442745, 0.26272577955119913, 0.2495894902592222, 0.2594417073994826, 0.24466338198271095, 0.24958949055284116, 0.238095237385659, 0.23152709278860703, 0.23316912891340177, 0.2348111651360695, 0.23973727351045374, 0.23152709288648002, 0.23316912901127476, 0.23973727370619968, 0.2348111652339425, 0.2413793099288674, 0.23645320135873724, 0.23809523777715091, 0.24794745442804642, 0.2561576352477661, 0.24137930973312147, 0.21839080408386802, 0.2413793100267404, 0.2348111652339425, 0.24466338208058394, 0.23316912910914772, 0.24302134576004322, 0.22495894838730102, 0.23152709278860703, 0.2200328399150438, 0.19211822608715207, 0.24466338208058394, 0.2331691292070207, 0.23809523767927793, 0.23809523758140494, 0.23973727360832672, 0.23973727360832672, 0.2348111652339425, 0.23645320135873724, 0.23973727370619968, 0.23809523758140494, 0.23809523758140494, 0.2364532014566102, 0.2413793099288674, 0.2413793099288674, 0.2413793099288674, 0.23973727370619968, 0.23809523758140494, 0.23809523758140494, 0.23809523758140494, 0.24302134605366216, 0.23645320135873724, 0.23645320135873724, 0.2413793099288674, 0.23809523758140494, 0.23809523758140494, 0.2413793099288674, 0.23973727370619968, 0.2430213459557892, 0.2413793099288674, 0.23973727380407267, 0.2413793099288674, 0.23809523767927793, 0.23809523767927793, 0.24302134605366216, 0.2413793099288674, 0.23809523767927793, 0.23973727399981865, 0.24630541820537868, 0.2512315265797629, 0.24794745452591938, 0.24794745462379236, 0.24794745452591938, 0.2495894908464601, 0.2479474548195383, 0.24794745442804642, 0.24302134644515408, 0.22495894828942806, 0.2364532014566102, 0.24302134624940813, 0.24302134644515408, 0.24958949045496817, 0.25123152697125484, 0.2545155992208443, 0.2577996714704338, 0.23809523758140494, 0.24630541830325164, 0.24958949055284116, 0.25287356319392257, 0.25944170769310154, 0.24630541849899762, 0.2446633821784569, 0.25123152697125484, 0.2660098521922805, 0.24958949065071412, 0.2545155993187173, 0.2512315265797629, 0.26272577974694505, 0.25123152677550886, 0.2446633822763299, 0.25451559912297134, 0.2627257777650172, 0.26272577994269103, 0.2512315266776359, 0.24794745442804642, 0.25123152677550886, 0.2610837434264044, 0.25451559892722536, 0.24630541820537868, 0.2512315266776359, 0.24630541840112463, 0.24302134615153514, 0.2446633821784569, 0.23809523767927793, 0.2364532014566102, 0.2413793100267404, 0.24302134605366216, 0.2446633822763299, 0.25451559912297134, 0.24958949065071412, 0.24630541830325164, 0.24630541820537868, 0.25123152677550886, 0.2512315265797629, 0.25123152677550886, 0.2528735629003036, 0.24794745442804642, 0.2594417073994826, 0.24958949055284116, 0.2446633822763299, 0.24630541820537868, 0.24466338198271095, 0.24794745442804642, 0.24630541830325164, 0.24466338198271095, 0.24794745433017343, 0.24466338208058394, 0.24630541820537868, 0.23973727351045374, 0.23316912910914772, 0.23645320135873724, 0.23973727341258075, 0.23973727341258075, 0.24466338188483797, 0.2430213459557892, 0.24630541820537868, 0.24137930963524848, 0.24794745433017343, 0.24466338198271095, 0.24466338208058394, 0.2463054181075057, 0.24794745433017343, 0.2528735627045577, 0.2561576350520201], 'loss': [1.6155921387231815, 1.6091174986328187, 1.6063531638170905, 1.6056738331087805, 1.6055563475806611, 1.6067402355969564, 1.605787789992973, 1.6049939317624917, 1.6050158948134592, 1.604770600379615, 1.604565784671713, 1.6045792657002287, 1.6047740941174955, 1.6044366947189739, 1.6042626020110364, 1.603945579323191, 1.603383475603264, 1.6032797074660627, 1.6030552434235872, 1.6036767904273783, 1.60349399764435, 1.6030605486041467, 1.6024604763583237, 1.602158568037609, 1.601993563581541, 1.6016527547483816, 1.6012196715607536, 1.6004101768411405, 1.5999824755000873, 1.5997666600793294, 1.5996792799638282, 1.5999881874119721, 1.6000002267179547, 1.5998713694558742, 1.6007828821147003, 1.6000491017433651, 1.599775240896174, 1.598904792484072, 1.6005140445070836, 1.5982031404604902, 1.598364055083273, 1.5981913076533918, 1.5972477257863698, 1.5974075657631093, 1.6005941346930284, 1.6019254225725021, 1.5983101115824017, 1.597868955845216, 1.5982242485825775, 1.5976018500279108, 1.5982690407265383, 1.5985283906944479, 1.5974961027717198, 1.5981591366644512, 1.6012421166382775, 1.5991160044190331, 1.5988611179700378, 1.5971846580015805, 1.5974352458664034, 1.5978064169384372, 1.5974176266844513, 1.5971726067501906, 1.5969265340534813, 1.5964692784530672, 1.596331556034284, 1.596134306322133, 1.5959985457896206, 1.5957963964532778, 1.5954350926303276, 1.5952028843411674, 1.595104596551194, 1.5950129407632032, 1.5950087046965926, 1.5944392995912682, 1.594066083602592, 1.5945819532601986, 1.594650710730582, 1.5940856063635198, 1.594339241629019, 1.5933498269478643, 1.59271495724116, 1.5934406395565557, 1.5930319395398211, 1.5934649768061706, 1.5920017601283425, 1.5928023341744832, 1.5930955166199858, 1.5944715993849894, 1.5959461206772978, 1.5954653248405064, 1.5998985124564513, 1.5996798911378614, 1.5989685023834572, 1.5984463290267412, 1.5974726636796517, 1.596164230252683, 1.5941022220823065, 1.5930177593622854, 1.593939154789433, 1.594126309457501, 1.595711540980016, 1.597532449365886, 1.594268476351086, 1.5950361087826488, 1.5951924860110274, 1.5938964391391135, 1.5942989188787628, 1.59312754181621, 1.5947402953122432, 1.5931367382131807, 1.5944787896389345, 1.5934555018951762, 1.593342380504099, 1.593824686651602, 1.593255531518611, 1.592416370282183, 1.5919603049632705, 1.59175813149133, 1.5911843714528016, 1.590840419410925, 1.5905411596415713, 1.5904285288444535, 1.5910221985967743, 1.5920272202951953, 1.5913838436471364, 1.5900113145918326, 1.5921030730437449, 1.5931132382191182, 1.5889734960434618, 1.5904426511553034, 1.5928073239767087, 1.5942272595556366, 1.5920762614792623, 1.5910213150282906, 1.5888185060978914, 1.5881817213563703, 1.5893984787762776, 1.5879399015673377, 1.5906278319427365, 1.588725736743371, 1.588565604055197, 1.588909567112306, 1.5909026718237562, 1.5925656744586858, 1.5891824933782495, 1.5888457722242852, 1.5887844638413227, 1.5884331248868906, 1.588020144155138, 1.5880591389090128, 1.5888844768124684, 1.5871068256836407, 1.5868141229147783, 1.5878583204574899, 1.5877869715680821, 1.5889010727038373, 1.5883559449742217, 1.5906095683452286, 1.590735689570527, 1.5919311563092335, 1.590240099102075, 1.5888225941687395, 1.5891867442297496, 1.5911696528017643, 1.5897579459439068, 1.5897668418943025, 1.5889381251540762, 1.5879028005521645, 1.5868406639451609, 1.5890696458992772, 1.5936629045180961, 1.5949180380764438, 1.5935282462920985, 1.5943775791407122, 1.6076153387034453, 1.6053936731154423, 1.6096967227649885, 1.6053109407424926, 1.5973790946682376, 1.6003916800144518, 1.5985114759977839, 1.5953286166063814, 1.595759902206045, 1.5952733473121752, 1.5943090756081457, 1.5937703041570142, 1.5940048198190804, 1.5945959209171898, 1.5935291276575358, 1.592918221318991, 1.592742382231679, 1.5925944416429962, 1.5922294041948886, 1.5925122006473111, 1.592298062087574, 1.5929793751949648, 1.593542872415186, 1.5922476250532962, 1.5920148173396838, 1.5912531458621642, 1.5915061382297617, 1.5911779053646926, 1.5910549722902585, 1.591197708451038, 1.590767995190082, 1.5904067707257594, 1.590866587000461, 1.5906573137463484, 1.5904067307282277, 1.5909124123731924, 1.5900427983282037, 1.5901616295749892, 1.591167349639125, 1.5932815015683184, 1.5933412437321468, 1.5935592300348458, 1.5936247941160104, 1.5940858264233786, 1.5937657422843166, 1.59268580031346, 1.5913625576168116, 1.593062769069319, 1.5919251172204771, 1.5986178981694843, 1.5976138665690804, 1.5955372866174278, 1.5966985601174513, 1.5960421710533284, 1.595860966421985, 1.5954947320342798, 1.5984183003035903, 1.5969720509997138, 1.5944303021049109, 1.5934687861671684, 1.5934142519561172, 1.593528903191584, 1.5922944639742498, 1.5918933019011416, 1.5907258229089223, 1.5905596809465539, 1.5910851080559607, 1.5901949429169329, 1.5903653631954466, 1.5898541766270475, 1.5902671833057913, 1.5901684442829547, 1.5896001027594846, 1.588600541678787, 1.5897380638416299, 1.5893163945395843, 1.5879644365036512, 1.5883584352978934, 1.5878438114385585, 1.5864830654504607, 1.58736738360638, 1.586765551273338, 1.5871573676808415, 1.5854372302609547, 1.5868807926315056, 1.585762041403283, 1.5878486189264536, 1.5898230689751784, 1.5900737079506782, 1.592064870897015, 1.5977429824198541, 1.5963773210435432, 1.594951921175148, 1.5944556528537914, 1.5943257331358578, 1.5939876076132364, 1.5922073309426434, 1.592372036518747, 1.5930030127080803, 1.5919557653658198, 1.5923919815791951, 1.5911899278296093, 1.591509354481707, 1.590406877500076, 1.590223940246159, 1.589426255617788, 1.588644307986422, 1.5906639239136933, 1.5909917079937286, 1.5917995348113763, 1.592425153191819, 1.5944442952193274, 1.5947778160322374, 1.594321906737968, 1.5941759922176415, 1.5945829422811708, 1.5933087182975159, 1.5924738343981013, 1.5928575893202357, 1.5914629519108139, 1.5914623015715113, 1.5905299807476068, 1.5904261949370775, 1.589928396673418, 1.589142336199171, 1.5893483274526419], 'acc': [0.17946611800722517, 0.20000000122391468, 0.23901437533219982, 0.23285420851541005, 0.2328542102778472, 0.23285420851541005, 0.23285421049203225, 0.2328542087112364, 0.23285420929871545, 0.23285420872959514, 0.23285420851541005, 0.23285420969036816, 0.2328542094945418, 0.23285420951290053, 0.23244353070151388, 0.23367556394737604, 0.23367556396573477, 0.2336755641432024, 0.23367556355572333, 0.23285420912124782, 0.23244353265977738, 0.23326488691678526, 0.22997946716554357, 0.23408624156544586, 0.23449692098267025, 0.24147843910071395, 0.23983572802259692, 0.241478439688193, 0.24024640565902544, 0.24229979551181166, 0.242299796080932, 0.24312114975046084, 0.2402464058364931, 0.24312114998300463, 0.24271047271987006, 0.24353182658522526, 0.2414784385132349, 0.24845995919537986, 0.23080082140419272, 0.23860369556004016, 0.24435318338797568, 0.24353182658522526, 0.24271047271987006, 0.24476386063275152, 0.23285421010037957, 0.2386036951867462, 0.24353182854348873, 0.24024640603231942, 0.23737166348913613, 0.24394250655321126, 0.2443531847587601, 0.2431211487713291, 0.24599589346860223, 0.24312114955463449, 0.23531827539878705, 0.23367556394737604, 0.24106776285342857, 0.23901437396141537, 0.24312114955463449, 0.24188911787538314, 0.24147843910071395, 0.24353182719106303, 0.24229979433685359, 0.24229979570763802, 0.24435318280049662, 0.24476386002691375, 0.24353182697687795, 0.24106776109099143, 0.24271047072488913, 0.24435318240884393, 0.2484599586079008, 0.2513347029135212, 0.24353182797436843, 0.2435318287393151, 0.24353182756435698, 0.24312114955463449, 0.24188911787538314, 0.2431211503562986, 0.24476386002691375, 0.240657084256227, 0.24106776128681778, 0.2418891168962514, 0.24763860474254562, 0.24065708308126893, 0.23860369595169287, 0.2509240260603981, 0.2558521568897568, 0.2464065712824984, 0.24147843986566062, 0.24435318281885535, 0.2332648861518386, 0.2418891168962514, 0.23901437494054711, 0.239835729197555, 0.239835729197555, 0.2468172475297838, 0.24681724733395743, 0.2501026696676591, 0.23901437417560045, 0.2439425046133065, 0.24147843869070254, 0.2328542083195837, 0.23860369655753064, 0.235318274811308, 0.2402464058364931, 0.24147844047149838, 0.2525667349660666, 0.25051334628823846, 0.24763860356758752, 0.2464065703217254, 0.2533880882072253, 0.24106776162339433, 0.2451745374491572, 0.24435318397545472, 0.23737166407661517, 0.2459958932911346, 0.24517453864247402, 0.2435318271727043, 0.24435318280049662, 0.24558521526305338, 0.24845996015615288, 0.2431211485387853, 0.24106776244341716, 0.24188911865868853, 0.2422997941226685, 0.25051334883398096, 0.25010266927600644, 0.2418891173062628, 0.2480492815589513, 0.2525667347335228, 0.2422997949059739, 0.24558521506722703, 0.24312115014211352, 0.24845995919537986, 0.25544147770507625, 0.24887063421263098, 0.25133470172020445, 0.25338809100387033, 0.24312114975046084, 0.24722792495202725, 0.2550308014577909, 0.2558521559106251, 0.24969199085627247, 0.2501026700409531, 0.2533880896147272, 0.24845995976450017, 0.2587268976337856, 0.2505133460924121, 0.25872690060789827, 0.2513347028951625, 0.24476386041856643, 0.24722792536203866, 0.2513347038742943, 0.25954825326157793, 0.2583162235773075, 0.2533880905938589, 0.2525667349660666, 0.25051334746319653, 0.2529774147382262, 0.2505133480506756, 0.25379876901359283, 0.24969199242288326, 0.2533880893821834, 0.251745380904885, 0.2521560587187812, 0.24599589250782922, 0.2505133460924121, 0.253388090808044, 0.24188911748373043, 0.25420944778826204, 0.2443531837979871, 0.2505133472673702, 0.25297741199665735, 0.245585214479748, 0.24229979512015898, 0.24476386043692516, 0.21971252653633055, 0.2172484594387685, 0.2476386031575761, 0.23737166272418944, 0.24312115112124527, 0.2505133468940762, 0.25256673555354564, 0.2550307992669836, 0.25462012287894803, 0.2554414798591661, 0.251745380317406, 0.25174537972992694, 0.252156056368865, 0.25092402568710415, 0.251745380317406, 0.25215605656469137, 0.2529774125841364, 0.2554414790758607, 0.2533880905938589, 0.25174538053159107, 0.2554414788800343, 0.25338808982891226, 0.2513347023076835, 0.251745380317406, 0.2488706369909173, 0.25420944567088966, 0.2509240250812664, 0.25297741199665735, 0.25379876879940777, 0.2529774137590945, 0.2517453787507952, 0.25379876680442687, 0.25215605715217043, 0.2533880906122176, 0.25338809039803256, 0.2537987668411443, 0.2542094454750633, 0.2513347034826416, 0.24435318299632297, 0.2529774121924837, 0.25297741238831006, 0.240657084256227, 0.24435318360216074, 0.24147843990237805, 0.2513347034826416, 0.24312114955463449, 0.2546201222363928, 0.23285421008202084, 0.24640657226163015, 0.2480492815589513, 0.24229979373101582, 0.24517453764498356, 0.24599589428862506, 0.2447638598310874, 0.23983572681092138, 0.2443531826046703, 0.24804928097147227, 0.24928131384404043, 0.24763860335340246, 0.23655030727386475, 0.25256673555354564, 0.2505133480690343, 0.24763860335340246, 0.24435318164389727, 0.245585214479748, 0.2422997945143212, 0.24599589407444, 0.23901437378394774, 0.23490759858238133, 0.2435318283660211, 0.2496919928328947, 0.24681724893728565, 0.24188911728790408, 0.2472279276935961, 0.2574948641920971, 0.2533880896147272, 0.25174538012157965, 0.2546201236438947, 0.25503079969535375, 0.2579055451759323, 0.24599589444773398, 0.25215605773964944, 0.25297741156828724, 0.25092402371048195, 0.2501026710200848, 0.2513347023076835, 0.249691993402015, 0.24722792432783075, 0.2447638614160569, 0.24188911591711965, 0.24353182797436843, 0.2455852160647175, 0.24271047332570783, 0.24804928038399324, 0.24681724792143647, 0.2468172496838736, 0.2468172475297838, 0.24681724733395743, 0.2455852166338378, 0.24722792534367993, 0.24681724833144789, 0.24887063720510236, 0.2476386043325342, 0.2513347030909889, 0.2542094438350176, 0.24804928018816688, 0.24804928038399324, 0.23942505081453852, 0.24558521565470606, 0.24435318319214933, 0.24353182854348873, 0.24681724931057963, 0.2492813142173344, 0.25010267000423564, 0.2447638606143928, 0.24681724891892692, 0.24845995898119477, 0.24887063583431793, 0.24681724792143647, 0.2505133468757175, 0.25010267082425847, 0.24681724733395743, 0.25174537933827423, 0.24681724792143647]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
