{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf76.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 01:40:15 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '0Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['sk', 'ib', 'aa', 'mb', 'eb', 'yd', 'my', 'eo', 'sg', 'eg', 'by', 'ck', 'ce', 'ds', 'ek'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000024C10382DD8>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000024C2A8D7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7008, Accuracy:0.0891, Validation Loss:2.6962, Validation Accuracy:0.0887\n",
    "Epoch #2: Loss:2.6925, Accuracy:0.0891, Validation Loss:2.6887, Validation Accuracy:0.0887\n",
    "Epoch #3: Loss:2.6855, Accuracy:0.0891, Validation Loss:2.6821, Validation Accuracy:0.0887\n",
    "Epoch #4: Loss:2.6789, Accuracy:0.0891, Validation Loss:2.6764, Validation Accuracy:0.0887\n",
    "Epoch #5: Loss:2.6730, Accuracy:0.0891, Validation Loss:2.6711, Validation Accuracy:0.0887\n",
    "Epoch #6: Loss:2.6670, Accuracy:0.0891, Validation Loss:2.6654, Validation Accuracy:0.0887\n",
    "Epoch #7: Loss:2.6600, Accuracy:0.0945, Validation Loss:2.6594, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6532, Accuracy:0.1018, Validation Loss:2.6520, Validation Accuracy:0.1117\n",
    "Epoch #9: Loss:2.6438, Accuracy:0.1203, Validation Loss:2.6440, Validation Accuracy:0.1166\n",
    "Epoch #10: Loss:2.6330, Accuracy:0.1216, Validation Loss:2.6342, Validation Accuracy:0.1199\n",
    "Epoch #11: Loss:2.6202, Accuracy:0.1269, Validation Loss:2.6241, Validation Accuracy:0.1215\n",
    "Epoch #12: Loss:2.6110, Accuracy:0.1199, Validation Loss:2.6086, Validation Accuracy:0.1281\n",
    "Epoch #13: Loss:2.5909, Accuracy:0.1376, Validation Loss:2.6003, Validation Accuracy:0.1297\n",
    "Epoch #14: Loss:2.5845, Accuracy:0.1326, Validation Loss:2.5935, Validation Accuracy:0.1330\n",
    "Epoch #15: Loss:2.5693, Accuracy:0.1548, Validation Loss:2.5792, Validation Accuracy:0.1445\n",
    "Epoch #16: Loss:2.5512, Accuracy:0.1598, Validation Loss:2.5664, Validation Accuracy:0.1363\n",
    "Epoch #17: Loss:2.5339, Accuracy:0.1470, Validation Loss:2.5551, Validation Accuracy:0.1412\n",
    "Epoch #18: Loss:2.5244, Accuracy:0.1511, Validation Loss:2.5579, Validation Accuracy:0.1297\n",
    "Epoch #19: Loss:2.5079, Accuracy:0.1540, Validation Loss:2.5462, Validation Accuracy:0.1363\n",
    "Epoch #20: Loss:2.4996, Accuracy:0.1606, Validation Loss:2.5368, Validation Accuracy:0.1412\n",
    "Epoch #21: Loss:2.4895, Accuracy:0.1606, Validation Loss:2.5335, Validation Accuracy:0.1412\n",
    "Epoch #22: Loss:2.4835, Accuracy:0.1602, Validation Loss:2.5328, Validation Accuracy:0.1396\n",
    "Epoch #23: Loss:2.4779, Accuracy:0.1622, Validation Loss:2.5272, Validation Accuracy:0.1346\n",
    "Epoch #24: Loss:2.4768, Accuracy:0.1671, Validation Loss:2.5275, Validation Accuracy:0.1429\n",
    "Epoch #25: Loss:2.4710, Accuracy:0.1692, Validation Loss:2.5201, Validation Accuracy:0.1478\n",
    "Epoch #26: Loss:2.4712, Accuracy:0.1684, Validation Loss:2.5220, Validation Accuracy:0.1429\n",
    "Epoch #27: Loss:2.4730, Accuracy:0.1663, Validation Loss:2.5403, Validation Accuracy:0.1445\n",
    "Epoch #28: Loss:2.4715, Accuracy:0.1729, Validation Loss:2.5230, Validation Accuracy:0.1346\n",
    "Epoch #29: Loss:2.4621, Accuracy:0.1647, Validation Loss:2.5202, Validation Accuracy:0.1478\n",
    "Epoch #30: Loss:2.4648, Accuracy:0.1692, Validation Loss:2.5197, Validation Accuracy:0.1461\n",
    "Epoch #31: Loss:2.4603, Accuracy:0.1766, Validation Loss:2.5263, Validation Accuracy:0.1363\n",
    "Epoch #32: Loss:2.4673, Accuracy:0.1692, Validation Loss:2.5182, Validation Accuracy:0.1379\n",
    "Epoch #33: Loss:2.5090, Accuracy:0.1598, Validation Loss:2.5567, Validation Accuracy:0.1429\n",
    "Epoch #34: Loss:2.4939, Accuracy:0.1663, Validation Loss:2.5256, Validation Accuracy:0.1478\n",
    "Epoch #35: Loss:2.4825, Accuracy:0.1655, Validation Loss:2.5590, Validation Accuracy:0.1511\n",
    "Epoch #36: Loss:2.4738, Accuracy:0.1630, Validation Loss:2.5152, Validation Accuracy:0.1363\n",
    "Epoch #37: Loss:2.4548, Accuracy:0.1717, Validation Loss:2.5237, Validation Accuracy:0.1396\n",
    "Epoch #38: Loss:2.4631, Accuracy:0.1696, Validation Loss:2.5144, Validation Accuracy:0.1511\n",
    "Epoch #39: Loss:2.4519, Accuracy:0.1754, Validation Loss:2.5176, Validation Accuracy:0.1445\n",
    "Epoch #40: Loss:2.4550, Accuracy:0.1708, Validation Loss:2.5178, Validation Accuracy:0.1445\n",
    "Epoch #41: Loss:2.4518, Accuracy:0.1655, Validation Loss:2.5101, Validation Accuracy:0.1576\n",
    "Epoch #42: Loss:2.4497, Accuracy:0.1676, Validation Loss:2.5124, Validation Accuracy:0.1593\n",
    "Epoch #43: Loss:2.4480, Accuracy:0.1684, Validation Loss:2.5112, Validation Accuracy:0.1576\n",
    "Epoch #44: Loss:2.4466, Accuracy:0.1655, Validation Loss:2.5111, Validation Accuracy:0.1560\n",
    "Epoch #45: Loss:2.4490, Accuracy:0.1708, Validation Loss:2.5095, Validation Accuracy:0.1675\n",
    "Epoch #46: Loss:2.4509, Accuracy:0.1704, Validation Loss:2.5107, Validation Accuracy:0.1691\n",
    "Epoch #47: Loss:2.4503, Accuracy:0.1680, Validation Loss:2.5085, Validation Accuracy:0.1691\n",
    "Epoch #48: Loss:2.4492, Accuracy:0.1643, Validation Loss:2.5067, Validation Accuracy:0.1560\n",
    "Epoch #49: Loss:2.4468, Accuracy:0.1704, Validation Loss:2.5089, Validation Accuracy:0.1396\n",
    "Epoch #50: Loss:2.4621, Accuracy:0.1634, Validation Loss:2.5521, Validation Accuracy:0.1429\n",
    "Epoch #51: Loss:2.4698, Accuracy:0.1634, Validation Loss:2.5147, Validation Accuracy:0.1478\n",
    "Epoch #52: Loss:2.4707, Accuracy:0.1713, Validation Loss:2.5045, Validation Accuracy:0.1544\n",
    "Epoch #53: Loss:2.4493, Accuracy:0.1671, Validation Loss:2.5068, Validation Accuracy:0.1560\n",
    "Epoch #54: Loss:2.4517, Accuracy:0.1671, Validation Loss:2.5009, Validation Accuracy:0.1626\n",
    "Epoch #55: Loss:2.4459, Accuracy:0.1713, Validation Loss:2.4992, Validation Accuracy:0.1527\n",
    "Epoch #56: Loss:2.4467, Accuracy:0.1692, Validation Loss:2.4979, Validation Accuracy:0.1708\n",
    "Epoch #57: Loss:2.4438, Accuracy:0.1696, Validation Loss:2.4991, Validation Accuracy:0.1724\n",
    "Epoch #58: Loss:2.4455, Accuracy:0.1676, Validation Loss:2.4977, Validation Accuracy:0.1609\n",
    "Epoch #59: Loss:2.4440, Accuracy:0.1692, Validation Loss:2.4972, Validation Accuracy:0.1757\n",
    "Epoch #60: Loss:2.4436, Accuracy:0.1676, Validation Loss:2.4969, Validation Accuracy:0.1609\n",
    "Epoch #61: Loss:2.4425, Accuracy:0.1692, Validation Loss:2.4973, Validation Accuracy:0.1609\n",
    "Epoch #62: Loss:2.4423, Accuracy:0.1704, Validation Loss:2.4978, Validation Accuracy:0.1708\n",
    "Epoch #63: Loss:2.4423, Accuracy:0.1704, Validation Loss:2.4959, Validation Accuracy:0.1691\n",
    "Epoch #64: Loss:2.4418, Accuracy:0.1688, Validation Loss:2.4956, Validation Accuracy:0.1642\n",
    "Epoch #65: Loss:2.4420, Accuracy:0.1696, Validation Loss:2.4967, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4418, Accuracy:0.1667, Validation Loss:2.4951, Validation Accuracy:0.1724\n",
    "Epoch #67: Loss:2.4419, Accuracy:0.1663, Validation Loss:2.4951, Validation Accuracy:0.1773\n",
    "Epoch #68: Loss:2.4404, Accuracy:0.1688, Validation Loss:2.4949, Validation Accuracy:0.1691\n",
    "Epoch #69: Loss:2.4397, Accuracy:0.1680, Validation Loss:2.4942, Validation Accuracy:0.1560\n",
    "Epoch #70: Loss:2.4387, Accuracy:0.1700, Validation Loss:2.4932, Validation Accuracy:0.1757\n",
    "Epoch #71: Loss:2.4388, Accuracy:0.1741, Validation Loss:2.4927, Validation Accuracy:0.1675\n",
    "Epoch #72: Loss:2.4397, Accuracy:0.1713, Validation Loss:2.4937, Validation Accuracy:0.1675\n",
    "Epoch #73: Loss:2.4384, Accuracy:0.1684, Validation Loss:2.4924, Validation Accuracy:0.1741\n",
    "Epoch #74: Loss:2.4395, Accuracy:0.1729, Validation Loss:2.4924, Validation Accuracy:0.1823\n",
    "Epoch #75: Loss:2.4384, Accuracy:0.1733, Validation Loss:2.4928, Validation Accuracy:0.1691\n",
    "Epoch #76: Loss:2.4393, Accuracy:0.1696, Validation Loss:2.4915, Validation Accuracy:0.1773\n",
    "Epoch #77: Loss:2.4376, Accuracy:0.1766, Validation Loss:2.4910, Validation Accuracy:0.1856\n",
    "Epoch #78: Loss:2.4363, Accuracy:0.1725, Validation Loss:2.4906, Validation Accuracy:0.1642\n",
    "Epoch #79: Loss:2.4370, Accuracy:0.1708, Validation Loss:2.4909, Validation Accuracy:0.1724\n",
    "Epoch #80: Loss:2.4372, Accuracy:0.1704, Validation Loss:2.4907, Validation Accuracy:0.1823\n",
    "Epoch #81: Loss:2.4361, Accuracy:0.1708, Validation Loss:2.4906, Validation Accuracy:0.1741\n",
    "Epoch #82: Loss:2.4354, Accuracy:0.1708, Validation Loss:2.4895, Validation Accuracy:0.1839\n",
    "Epoch #83: Loss:2.4347, Accuracy:0.1696, Validation Loss:2.4895, Validation Accuracy:0.1872\n",
    "Epoch #84: Loss:2.4349, Accuracy:0.1721, Validation Loss:2.4888, Validation Accuracy:0.1806\n",
    "Epoch #85: Loss:2.4360, Accuracy:0.1713, Validation Loss:2.4903, Validation Accuracy:0.1708\n",
    "Epoch #86: Loss:2.4350, Accuracy:0.1737, Validation Loss:2.4905, Validation Accuracy:0.1790\n",
    "Epoch #87: Loss:2.4361, Accuracy:0.1729, Validation Loss:2.4898, Validation Accuracy:0.1872\n",
    "Epoch #88: Loss:2.4356, Accuracy:0.1700, Validation Loss:2.4900, Validation Accuracy:0.1691\n",
    "Epoch #89: Loss:2.4351, Accuracy:0.1696, Validation Loss:2.4904, Validation Accuracy:0.1609\n",
    "Epoch #90: Loss:2.4355, Accuracy:0.1733, Validation Loss:2.4903, Validation Accuracy:0.1708\n",
    "Epoch #91: Loss:2.4354, Accuracy:0.1663, Validation Loss:2.4899, Validation Accuracy:0.1658\n",
    "Epoch #92: Loss:2.4353, Accuracy:0.1704, Validation Loss:2.4893, Validation Accuracy:0.1806\n",
    "Epoch #93: Loss:2.4329, Accuracy:0.1721, Validation Loss:2.4907, Validation Accuracy:0.1642\n",
    "Epoch #94: Loss:2.4340, Accuracy:0.1692, Validation Loss:2.4896, Validation Accuracy:0.1658\n",
    "Epoch #95: Loss:2.4343, Accuracy:0.1741, Validation Loss:2.4885, Validation Accuracy:0.1888\n",
    "Epoch #96: Loss:2.4329, Accuracy:0.1745, Validation Loss:2.4893, Validation Accuracy:0.1642\n",
    "Epoch #97: Loss:2.4325, Accuracy:0.1700, Validation Loss:2.4880, Validation Accuracy:0.1856\n",
    "Epoch #98: Loss:2.4321, Accuracy:0.1717, Validation Loss:2.4881, Validation Accuracy:0.1724\n",
    "Epoch #99: Loss:2.4328, Accuracy:0.1704, Validation Loss:2.4870, Validation Accuracy:0.1823\n",
    "Epoch #100: Loss:2.4329, Accuracy:0.1717, Validation Loss:2.4869, Validation Accuracy:0.1724\n",
    "Epoch #101: Loss:2.4314, Accuracy:0.1729, Validation Loss:2.4867, Validation Accuracy:0.1773\n",
    "Epoch #102: Loss:2.4319, Accuracy:0.1708, Validation Loss:2.4864, Validation Accuracy:0.1790\n",
    "Epoch #103: Loss:2.4339, Accuracy:0.1713, Validation Loss:2.4871, Validation Accuracy:0.1741\n",
    "Epoch #104: Loss:2.4329, Accuracy:0.1733, Validation Loss:2.4873, Validation Accuracy:0.1773\n",
    "Epoch #105: Loss:2.4322, Accuracy:0.1758, Validation Loss:2.4862, Validation Accuracy:0.1642\n",
    "Epoch #106: Loss:2.4318, Accuracy:0.1778, Validation Loss:2.4825, Validation Accuracy:0.1691\n",
    "Epoch #107: Loss:2.4511, Accuracy:0.1758, Validation Loss:2.4996, Validation Accuracy:0.1544\n",
    "Epoch #108: Loss:2.4656, Accuracy:0.1717, Validation Loss:2.4890, Validation Accuracy:0.1675\n",
    "Epoch #109: Loss:2.4468, Accuracy:0.1692, Validation Loss:2.4963, Validation Accuracy:0.1773\n",
    "Epoch #110: Loss:2.4394, Accuracy:0.1704, Validation Loss:2.4939, Validation Accuracy:0.1626\n",
    "Epoch #111: Loss:2.4358, Accuracy:0.1700, Validation Loss:2.4930, Validation Accuracy:0.1757\n",
    "Epoch #112: Loss:2.4335, Accuracy:0.1684, Validation Loss:2.4913, Validation Accuracy:0.1708\n",
    "Epoch #113: Loss:2.4323, Accuracy:0.1729, Validation Loss:2.4884, Validation Accuracy:0.1560\n",
    "Epoch #114: Loss:2.4324, Accuracy:0.1766, Validation Loss:2.4918, Validation Accuracy:0.1724\n",
    "Epoch #115: Loss:2.4322, Accuracy:0.1725, Validation Loss:2.4925, Validation Accuracy:0.1576\n",
    "Epoch #116: Loss:2.4319, Accuracy:0.1708, Validation Loss:2.4929, Validation Accuracy:0.1626\n",
    "Epoch #117: Loss:2.4314, Accuracy:0.1741, Validation Loss:2.4924, Validation Accuracy:0.1790\n",
    "Epoch #118: Loss:2.4307, Accuracy:0.1729, Validation Loss:2.4900, Validation Accuracy:0.1609\n",
    "Epoch #119: Loss:2.4285, Accuracy:0.1729, Validation Loss:2.4887, Validation Accuracy:0.1675\n",
    "Epoch #120: Loss:2.4294, Accuracy:0.1729, Validation Loss:2.4887, Validation Accuracy:0.1806\n",
    "Epoch #121: Loss:2.4296, Accuracy:0.1721, Validation Loss:2.4895, Validation Accuracy:0.1626\n",
    "Epoch #122: Loss:2.4291, Accuracy:0.1770, Validation Loss:2.4889, Validation Accuracy:0.1675\n",
    "Epoch #123: Loss:2.4279, Accuracy:0.1778, Validation Loss:2.4883, Validation Accuracy:0.1609\n",
    "Epoch #124: Loss:2.4285, Accuracy:0.1729, Validation Loss:2.4886, Validation Accuracy:0.1609\n",
    "Epoch #125: Loss:2.4274, Accuracy:0.1725, Validation Loss:2.4894, Validation Accuracy:0.1675\n",
    "Epoch #126: Loss:2.4293, Accuracy:0.1770, Validation Loss:2.4894, Validation Accuracy:0.1757\n",
    "Epoch #127: Loss:2.4279, Accuracy:0.1754, Validation Loss:2.4897, Validation Accuracy:0.1626\n",
    "Epoch #128: Loss:2.4276, Accuracy:0.1758, Validation Loss:2.4882, Validation Accuracy:0.1724\n",
    "Epoch #129: Loss:2.4271, Accuracy:0.1762, Validation Loss:2.4874, Validation Accuracy:0.1741\n",
    "Epoch #130: Loss:2.4262, Accuracy:0.1733, Validation Loss:2.4874, Validation Accuracy:0.1593\n",
    "Epoch #131: Loss:2.4266, Accuracy:0.1737, Validation Loss:2.4879, Validation Accuracy:0.1741\n",
    "Epoch #132: Loss:2.4260, Accuracy:0.1741, Validation Loss:2.4880, Validation Accuracy:0.1757\n",
    "Epoch #133: Loss:2.4259, Accuracy:0.1749, Validation Loss:2.4881, Validation Accuracy:0.1708\n",
    "Epoch #134: Loss:2.4258, Accuracy:0.1782, Validation Loss:2.4883, Validation Accuracy:0.1724\n",
    "Epoch #135: Loss:2.4258, Accuracy:0.1811, Validation Loss:2.4861, Validation Accuracy:0.1593\n",
    "Epoch #136: Loss:2.4249, Accuracy:0.1799, Validation Loss:2.4852, Validation Accuracy:0.1741\n",
    "Epoch #137: Loss:2.4244, Accuracy:0.1791, Validation Loss:2.4899, Validation Accuracy:0.1560\n",
    "Epoch #138: Loss:2.4266, Accuracy:0.1758, Validation Loss:2.4895, Validation Accuracy:0.1708\n",
    "Epoch #139: Loss:2.4270, Accuracy:0.1737, Validation Loss:2.4873, Validation Accuracy:0.1691\n",
    "Epoch #140: Loss:2.4278, Accuracy:0.1737, Validation Loss:2.4843, Validation Accuracy:0.1708\n",
    "Epoch #141: Loss:2.4275, Accuracy:0.1803, Validation Loss:2.4852, Validation Accuracy:0.1691\n",
    "Epoch #142: Loss:2.4264, Accuracy:0.1807, Validation Loss:2.4858, Validation Accuracy:0.1675\n",
    "Epoch #143: Loss:2.4257, Accuracy:0.1828, Validation Loss:2.4885, Validation Accuracy:0.1691\n",
    "Epoch #144: Loss:2.4241, Accuracy:0.1840, Validation Loss:2.4868, Validation Accuracy:0.1773\n",
    "Epoch #145: Loss:2.4235, Accuracy:0.1836, Validation Loss:2.4869, Validation Accuracy:0.1642\n",
    "Epoch #146: Loss:2.4240, Accuracy:0.1807, Validation Loss:2.4873, Validation Accuracy:0.1741\n",
    "Epoch #147: Loss:2.4228, Accuracy:0.1840, Validation Loss:2.4872, Validation Accuracy:0.1757\n",
    "Epoch #148: Loss:2.4228, Accuracy:0.1832, Validation Loss:2.4876, Validation Accuracy:0.1757\n",
    "Epoch #149: Loss:2.4243, Accuracy:0.1848, Validation Loss:2.4881, Validation Accuracy:0.1708\n",
    "Epoch #150: Loss:2.4228, Accuracy:0.1815, Validation Loss:2.4876, Validation Accuracy:0.1675\n",
    "Epoch #151: Loss:2.4232, Accuracy:0.1762, Validation Loss:2.4876, Validation Accuracy:0.1773\n",
    "Epoch #152: Loss:2.4223, Accuracy:0.1782, Validation Loss:2.4873, Validation Accuracy:0.1708\n",
    "Epoch #153: Loss:2.4219, Accuracy:0.1828, Validation Loss:2.4876, Validation Accuracy:0.1741\n",
    "Epoch #154: Loss:2.4218, Accuracy:0.1836, Validation Loss:2.4877, Validation Accuracy:0.1593\n",
    "Epoch #155: Loss:2.4223, Accuracy:0.1819, Validation Loss:2.4870, Validation Accuracy:0.1823\n",
    "Epoch #156: Loss:2.4210, Accuracy:0.1836, Validation Loss:2.4878, Validation Accuracy:0.1773\n",
    "Epoch #157: Loss:2.4213, Accuracy:0.1844, Validation Loss:2.4868, Validation Accuracy:0.1724\n",
    "Epoch #158: Loss:2.4212, Accuracy:0.1815, Validation Loss:2.4873, Validation Accuracy:0.1757\n",
    "Epoch #159: Loss:2.4221, Accuracy:0.1844, Validation Loss:2.4879, Validation Accuracy:0.1741\n",
    "Epoch #160: Loss:2.4213, Accuracy:0.1856, Validation Loss:2.4875, Validation Accuracy:0.1642\n",
    "Epoch #161: Loss:2.4220, Accuracy:0.1799, Validation Loss:2.4875, Validation Accuracy:0.1658\n",
    "Epoch #162: Loss:2.4226, Accuracy:0.1832, Validation Loss:2.4896, Validation Accuracy:0.1757\n",
    "Epoch #163: Loss:2.4226, Accuracy:0.1803, Validation Loss:2.4887, Validation Accuracy:0.1741\n",
    "Epoch #164: Loss:2.4219, Accuracy:0.1795, Validation Loss:2.4902, Validation Accuracy:0.1593\n",
    "Epoch #165: Loss:2.4209, Accuracy:0.1803, Validation Loss:2.4873, Validation Accuracy:0.1741\n",
    "Epoch #166: Loss:2.4204, Accuracy:0.1815, Validation Loss:2.4878, Validation Accuracy:0.1741\n",
    "Epoch #167: Loss:2.4216, Accuracy:0.1811, Validation Loss:2.4872, Validation Accuracy:0.1757\n",
    "Epoch #168: Loss:2.4209, Accuracy:0.1832, Validation Loss:2.4866, Validation Accuracy:0.1741\n",
    "Epoch #169: Loss:2.4193, Accuracy:0.1823, Validation Loss:2.4883, Validation Accuracy:0.1790\n",
    "Epoch #170: Loss:2.4199, Accuracy:0.1832, Validation Loss:2.4869, Validation Accuracy:0.1741\n",
    "Epoch #171: Loss:2.4198, Accuracy:0.1856, Validation Loss:2.4869, Validation Accuracy:0.1757\n",
    "Epoch #172: Loss:2.4192, Accuracy:0.1836, Validation Loss:2.4874, Validation Accuracy:0.1741\n",
    "Epoch #173: Loss:2.4198, Accuracy:0.1786, Validation Loss:2.4873, Validation Accuracy:0.1626\n",
    "Epoch #174: Loss:2.4196, Accuracy:0.1836, Validation Loss:2.4868, Validation Accuracy:0.1741\n",
    "Epoch #175: Loss:2.4192, Accuracy:0.1844, Validation Loss:2.4878, Validation Accuracy:0.1757\n",
    "Epoch #176: Loss:2.4186, Accuracy:0.1828, Validation Loss:2.4871, Validation Accuracy:0.1675\n",
    "Epoch #177: Loss:2.4202, Accuracy:0.1815, Validation Loss:2.4869, Validation Accuracy:0.1642\n",
    "Epoch #178: Loss:2.4188, Accuracy:0.1823, Validation Loss:2.4875, Validation Accuracy:0.1773\n",
    "Epoch #179: Loss:2.4189, Accuracy:0.1864, Validation Loss:2.4864, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.4183, Accuracy:0.1836, Validation Loss:2.4872, Validation Accuracy:0.1773\n",
    "Epoch #181: Loss:2.4180, Accuracy:0.1828, Validation Loss:2.4873, Validation Accuracy:0.1708\n",
    "Epoch #182: Loss:2.4177, Accuracy:0.1828, Validation Loss:2.4877, Validation Accuracy:0.1757\n",
    "Epoch #183: Loss:2.4185, Accuracy:0.1828, Validation Loss:2.4887, Validation Accuracy:0.1757\n",
    "Epoch #184: Loss:2.4179, Accuracy:0.1860, Validation Loss:2.4875, Validation Accuracy:0.1724\n",
    "Epoch #185: Loss:2.4171, Accuracy:0.1819, Validation Loss:2.4871, Validation Accuracy:0.1658\n",
    "Epoch #186: Loss:2.4176, Accuracy:0.1823, Validation Loss:2.4874, Validation Accuracy:0.1708\n",
    "Epoch #187: Loss:2.4167, Accuracy:0.1844, Validation Loss:2.4873, Validation Accuracy:0.1691\n",
    "Epoch #188: Loss:2.4187, Accuracy:0.1840, Validation Loss:2.4874, Validation Accuracy:0.1708\n",
    "Epoch #189: Loss:2.4167, Accuracy:0.1807, Validation Loss:2.4876, Validation Accuracy:0.1708\n",
    "Epoch #190: Loss:2.4177, Accuracy:0.1848, Validation Loss:2.4873, Validation Accuracy:0.1790\n",
    "Epoch #191: Loss:2.4168, Accuracy:0.1852, Validation Loss:2.4871, Validation Accuracy:0.1741\n",
    "Epoch #192: Loss:2.4169, Accuracy:0.1832, Validation Loss:2.4874, Validation Accuracy:0.1724\n",
    "Epoch #193: Loss:2.4168, Accuracy:0.1836, Validation Loss:2.4877, Validation Accuracy:0.1741\n",
    "Epoch #194: Loss:2.4177, Accuracy:0.1786, Validation Loss:2.4893, Validation Accuracy:0.1806\n",
    "Epoch #195: Loss:2.4162, Accuracy:0.1836, Validation Loss:2.4877, Validation Accuracy:0.1658\n",
    "Epoch #196: Loss:2.4177, Accuracy:0.1811, Validation Loss:2.4873, Validation Accuracy:0.1741\n",
    "Epoch #197: Loss:2.4177, Accuracy:0.1803, Validation Loss:2.4888, Validation Accuracy:0.1773\n",
    "Epoch #198: Loss:2.4178, Accuracy:0.1832, Validation Loss:2.4874, Validation Accuracy:0.1741\n",
    "Epoch #199: Loss:2.4158, Accuracy:0.1819, Validation Loss:2.4882, Validation Accuracy:0.1790\n",
    "Epoch #200: Loss:2.4179, Accuracy:0.1848, Validation Loss:2.4872, Validation Accuracy:0.1741\n",
    "Epoch #201: Loss:2.4154, Accuracy:0.1815, Validation Loss:2.4880, Validation Accuracy:0.1708\n",
    "Epoch #202: Loss:2.4168, Accuracy:0.1856, Validation Loss:2.4887, Validation Accuracy:0.1757\n",
    "Epoch #203: Loss:2.4154, Accuracy:0.1844, Validation Loss:2.4878, Validation Accuracy:0.1724\n",
    "Epoch #204: Loss:2.4183, Accuracy:0.1828, Validation Loss:2.4868, Validation Accuracy:0.1708\n",
    "Epoch #205: Loss:2.4167, Accuracy:0.1856, Validation Loss:2.4889, Validation Accuracy:0.1741\n",
    "Epoch #206: Loss:2.4144, Accuracy:0.1840, Validation Loss:2.4876, Validation Accuracy:0.1724\n",
    "Epoch #207: Loss:2.4189, Accuracy:0.1811, Validation Loss:2.4873, Validation Accuracy:0.1724\n",
    "Epoch #208: Loss:2.4174, Accuracy:0.1807, Validation Loss:2.4881, Validation Accuracy:0.1790\n",
    "Epoch #209: Loss:2.4172, Accuracy:0.1828, Validation Loss:2.4885, Validation Accuracy:0.1708\n",
    "Epoch #210: Loss:2.4157, Accuracy:0.1823, Validation Loss:2.4907, Validation Accuracy:0.1773\n",
    "Epoch #211: Loss:2.4164, Accuracy:0.1795, Validation Loss:2.4881, Validation Accuracy:0.1741\n",
    "Epoch #212: Loss:2.4168, Accuracy:0.1844, Validation Loss:2.4880, Validation Accuracy:0.1741\n",
    "Epoch #213: Loss:2.4148, Accuracy:0.1852, Validation Loss:2.4877, Validation Accuracy:0.1691\n",
    "Epoch #214: Loss:2.4154, Accuracy:0.1840, Validation Loss:2.4878, Validation Accuracy:0.1708\n",
    "Epoch #215: Loss:2.4148, Accuracy:0.1828, Validation Loss:2.4877, Validation Accuracy:0.1691\n",
    "Epoch #216: Loss:2.4146, Accuracy:0.1819, Validation Loss:2.4875, Validation Accuracy:0.1790\n",
    "Epoch #217: Loss:2.4145, Accuracy:0.1864, Validation Loss:2.4878, Validation Accuracy:0.1773\n",
    "Epoch #218: Loss:2.4143, Accuracy:0.1836, Validation Loss:2.4879, Validation Accuracy:0.1741\n",
    "Epoch #219: Loss:2.4146, Accuracy:0.1828, Validation Loss:2.4885, Validation Accuracy:0.1708\n",
    "Epoch #220: Loss:2.4140, Accuracy:0.1828, Validation Loss:2.4887, Validation Accuracy:0.1757\n",
    "Epoch #221: Loss:2.4163, Accuracy:0.1828, Validation Loss:2.4896, Validation Accuracy:0.1741\n",
    "Epoch #222: Loss:2.4178, Accuracy:0.1819, Validation Loss:2.4892, Validation Accuracy:0.1626\n",
    "Epoch #223: Loss:2.4146, Accuracy:0.1799, Validation Loss:2.4885, Validation Accuracy:0.1708\n",
    "Epoch #224: Loss:2.4152, Accuracy:0.1852, Validation Loss:2.4894, Validation Accuracy:0.1724\n",
    "Epoch #225: Loss:2.4153, Accuracy:0.1873, Validation Loss:2.4895, Validation Accuracy:0.1609\n",
    "Epoch #226: Loss:2.4145, Accuracy:0.1832, Validation Loss:2.4877, Validation Accuracy:0.1708\n",
    "Epoch #227: Loss:2.4158, Accuracy:0.1860, Validation Loss:2.4894, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.4147, Accuracy:0.1873, Validation Loss:2.4885, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4157, Accuracy:0.1819, Validation Loss:2.4891, Validation Accuracy:0.1691\n",
    "Epoch #230: Loss:2.4149, Accuracy:0.1823, Validation Loss:2.4894, Validation Accuracy:0.1708\n",
    "Epoch #231: Loss:2.4154, Accuracy:0.1873, Validation Loss:2.4897, Validation Accuracy:0.1724\n",
    "Epoch #232: Loss:2.4150, Accuracy:0.1828, Validation Loss:2.4911, Validation Accuracy:0.1708\n",
    "Epoch #233: Loss:2.4156, Accuracy:0.1844, Validation Loss:2.4909, Validation Accuracy:0.1773\n",
    "Epoch #234: Loss:2.4235, Accuracy:0.1815, Validation Loss:2.4945, Validation Accuracy:0.1691\n",
    "Epoch #235: Loss:2.4245, Accuracy:0.1856, Validation Loss:2.4940, Validation Accuracy:0.1593\n",
    "Epoch #236: Loss:2.4202, Accuracy:0.1807, Validation Loss:2.4956, Validation Accuracy:0.1642\n",
    "Epoch #237: Loss:2.4189, Accuracy:0.1819, Validation Loss:2.4930, Validation Accuracy:0.1658\n",
    "Epoch #238: Loss:2.4182, Accuracy:0.1823, Validation Loss:2.4932, Validation Accuracy:0.1658\n",
    "Epoch #239: Loss:2.4178, Accuracy:0.1819, Validation Loss:2.4911, Validation Accuracy:0.1642\n",
    "Epoch #240: Loss:2.4186, Accuracy:0.1840, Validation Loss:2.4908, Validation Accuracy:0.1658\n",
    "Epoch #241: Loss:2.4179, Accuracy:0.1807, Validation Loss:2.4930, Validation Accuracy:0.1658\n",
    "Epoch #242: Loss:2.4175, Accuracy:0.1828, Validation Loss:2.4926, Validation Accuracy:0.1609\n",
    "Epoch #243: Loss:2.4167, Accuracy:0.1828, Validation Loss:2.4922, Validation Accuracy:0.1658\n",
    "Epoch #244: Loss:2.4164, Accuracy:0.1836, Validation Loss:2.4919, Validation Accuracy:0.1658\n",
    "Epoch #245: Loss:2.4173, Accuracy:0.1840, Validation Loss:2.4914, Validation Accuracy:0.1642\n",
    "Epoch #246: Loss:2.4166, Accuracy:0.1864, Validation Loss:2.4930, Validation Accuracy:0.1626\n",
    "Epoch #247: Loss:2.4162, Accuracy:0.1856, Validation Loss:2.4916, Validation Accuracy:0.1675\n",
    "Epoch #248: Loss:2.4160, Accuracy:0.1815, Validation Loss:2.4924, Validation Accuracy:0.1626\n",
    "Epoch #249: Loss:2.4164, Accuracy:0.1819, Validation Loss:2.4912, Validation Accuracy:0.1626\n",
    "Epoch #250: Loss:2.4179, Accuracy:0.1852, Validation Loss:2.4917, Validation Accuracy:0.1658\n",
    "Epoch #251: Loss:2.4178, Accuracy:0.1844, Validation Loss:2.4907, Validation Accuracy:0.1658\n",
    "Epoch #252: Loss:2.4152, Accuracy:0.1848, Validation Loss:2.4941, Validation Accuracy:0.1626\n",
    "Epoch #253: Loss:2.4158, Accuracy:0.1840, Validation Loss:2.4911, Validation Accuracy:0.1658\n",
    "Epoch #254: Loss:2.4159, Accuracy:0.1832, Validation Loss:2.4915, Validation Accuracy:0.1642\n",
    "Epoch #255: Loss:2.4153, Accuracy:0.1860, Validation Loss:2.4909, Validation Accuracy:0.1658\n",
    "Epoch #256: Loss:2.4154, Accuracy:0.1848, Validation Loss:2.4907, Validation Accuracy:0.1626\n",
    "Epoch #257: Loss:2.4157, Accuracy:0.1828, Validation Loss:2.4905, Validation Accuracy:0.1675\n",
    "Epoch #258: Loss:2.4150, Accuracy:0.1828, Validation Loss:2.4912, Validation Accuracy:0.1626\n",
    "Epoch #259: Loss:2.4151, Accuracy:0.1828, Validation Loss:2.4907, Validation Accuracy:0.1626\n",
    "Epoch #260: Loss:2.4150, Accuracy:0.1840, Validation Loss:2.4915, Validation Accuracy:0.1675\n",
    "Epoch #261: Loss:2.4157, Accuracy:0.1856, Validation Loss:2.4909, Validation Accuracy:0.1642\n",
    "Epoch #262: Loss:2.4153, Accuracy:0.1869, Validation Loss:2.4908, Validation Accuracy:0.1626\n",
    "Epoch #263: Loss:2.4156, Accuracy:0.1819, Validation Loss:2.4918, Validation Accuracy:0.1642\n",
    "Epoch #264: Loss:2.4154, Accuracy:0.1848, Validation Loss:2.4916, Validation Accuracy:0.1626\n",
    "Epoch #265: Loss:2.4143, Accuracy:0.1840, Validation Loss:2.4913, Validation Accuracy:0.1626\n",
    "Epoch #266: Loss:2.4144, Accuracy:0.1836, Validation Loss:2.4911, Validation Accuracy:0.1658\n",
    "Epoch #267: Loss:2.4143, Accuracy:0.1836, Validation Loss:2.4908, Validation Accuracy:0.1626\n",
    "Epoch #268: Loss:2.4142, Accuracy:0.1852, Validation Loss:2.4911, Validation Accuracy:0.1642\n",
    "Epoch #269: Loss:2.4141, Accuracy:0.1819, Validation Loss:2.4915, Validation Accuracy:0.1642\n",
    "Epoch #270: Loss:2.4146, Accuracy:0.1848, Validation Loss:2.4916, Validation Accuracy:0.1642\n",
    "Epoch #271: Loss:2.4139, Accuracy:0.1848, Validation Loss:2.4917, Validation Accuracy:0.1642\n",
    "Epoch #272: Loss:2.4145, Accuracy:0.1852, Validation Loss:2.4908, Validation Accuracy:0.1642\n",
    "Epoch #273: Loss:2.4147, Accuracy:0.1828, Validation Loss:2.4925, Validation Accuracy:0.1642\n",
    "Epoch #274: Loss:2.4136, Accuracy:0.1856, Validation Loss:2.4913, Validation Accuracy:0.1658\n",
    "Epoch #275: Loss:2.4155, Accuracy:0.1828, Validation Loss:2.4912, Validation Accuracy:0.1642\n",
    "Epoch #276: Loss:2.4166, Accuracy:0.1803, Validation Loss:2.4917, Validation Accuracy:0.1626\n",
    "Epoch #277: Loss:2.4141, Accuracy:0.1815, Validation Loss:2.4903, Validation Accuracy:0.1642\n",
    "Epoch #278: Loss:2.4141, Accuracy:0.1840, Validation Loss:2.4917, Validation Accuracy:0.1626\n",
    "Epoch #279: Loss:2.4136, Accuracy:0.1856, Validation Loss:2.4900, Validation Accuracy:0.1658\n",
    "Epoch #280: Loss:2.4143, Accuracy:0.1836, Validation Loss:2.4901, Validation Accuracy:0.1658\n",
    "Epoch #281: Loss:2.4138, Accuracy:0.1856, Validation Loss:2.4907, Validation Accuracy:0.1626\n",
    "Epoch #282: Loss:2.4135, Accuracy:0.1815, Validation Loss:2.4909, Validation Accuracy:0.1642\n",
    "Epoch #283: Loss:2.4132, Accuracy:0.1856, Validation Loss:2.4910, Validation Accuracy:0.1658\n",
    "Epoch #284: Loss:2.4128, Accuracy:0.1864, Validation Loss:2.4906, Validation Accuracy:0.1626\n",
    "Epoch #285: Loss:2.4141, Accuracy:0.1836, Validation Loss:2.4904, Validation Accuracy:0.1626\n",
    "Epoch #286: Loss:2.4140, Accuracy:0.1840, Validation Loss:2.4918, Validation Accuracy:0.1675\n",
    "Epoch #287: Loss:2.4135, Accuracy:0.1844, Validation Loss:2.4900, Validation Accuracy:0.1642\n",
    "Epoch #288: Loss:2.4131, Accuracy:0.1844, Validation Loss:2.4915, Validation Accuracy:0.1658\n",
    "Epoch #289: Loss:2.4125, Accuracy:0.1873, Validation Loss:2.4896, Validation Accuracy:0.1675\n",
    "Epoch #290: Loss:2.4134, Accuracy:0.1823, Validation Loss:2.4905, Validation Accuracy:0.1642\n",
    "Epoch #291: Loss:2.4129, Accuracy:0.1881, Validation Loss:2.4912, Validation Accuracy:0.1626\n",
    "Epoch #292: Loss:2.4124, Accuracy:0.1840, Validation Loss:2.4906, Validation Accuracy:0.1626\n",
    "Epoch #293: Loss:2.4129, Accuracy:0.1828, Validation Loss:2.4912, Validation Accuracy:0.1642\n",
    "Epoch #294: Loss:2.4124, Accuracy:0.1848, Validation Loss:2.4899, Validation Accuracy:0.1658\n",
    "Epoch #295: Loss:2.4133, Accuracy:0.1819, Validation Loss:2.4913, Validation Accuracy:0.1642\n",
    "Epoch #296: Loss:2.4123, Accuracy:0.1869, Validation Loss:2.4901, Validation Accuracy:0.1642\n",
    "Epoch #297: Loss:2.4131, Accuracy:0.1848, Validation Loss:2.4909, Validation Accuracy:0.1626\n",
    "Epoch #298: Loss:2.4139, Accuracy:0.1869, Validation Loss:2.4914, Validation Accuracy:0.1642\n",
    "Epoch #299: Loss:2.4133, Accuracy:0.1840, Validation Loss:2.4916, Validation Accuracy:0.1642\n",
    "Epoch #300: Loss:2.4124, Accuracy:0.1832, Validation Loss:2.4944, Validation Accuracy:0.1626\n",
    "\n",
    "Test:\n",
    "Test Loss:2.49441957, Accuracy:0.1626\n",
    "Labels: ['sk', 'ib', 'aa', 'mb', 'eb', 'yd', 'my', 'eo', 'sg', 'eg', 'by', 'ck', 'ce', 'ds', 'ek']\n",
    "Confusion Matrix:\n",
    "      sk  ib  aa  mb  eb  yd  my  eo  sg  eg  by  ck  ce  ds  ek\n",
    "t:sk   0   0   0   0   3   3   0   0   6  12   8   0   0   1   0\n",
    "t:ib   0   0   0   0   3  32   0   0  11   4   3   0   0   1   0\n",
    "t:aa   0   0   0   0   4   3   0   0   2  13   6   0   0   6   0\n",
    "t:mb   0   0   0   0   5  11   0   0  17  10   6   0   0   3   0\n",
    "t:eb   0   0   0   0   6   8   0   0  11  18   7   0   0   0   0\n",
    "t:yd   0   0   0   0   3  29   0   0  23   3   4   0   0   0   0\n",
    "t:my   0   0   0   0   2   4   0   0   6   5   2   0   0   1   0\n",
    "t:eo   0   0   0   0   5   2   0   0  16   5   6   0   0   0   0\n",
    "t:sg   0   0   0   0   4   8   0   0  27   3   9   0   0   0   0\n",
    "t:eg   0   0   0   1   6   0   0   0   6  20  11   0   0   6   0\n",
    "t:by   0   0   0   1   7   2   0   0  12   7  11   0   0   0   0\n",
    "t:ck   0   0   0   0   1   0   0   0   8   6   7   0   0   1   0\n",
    "t:ce   0   0   0   0   2   1   0   0  10  11   2   0   0   1   0\n",
    "t:ds   0   0   0   0   2   2   0   0   9   9   3   0   0   6   0\n",
    "t:ek   0   0   0   0   7   5   0   0  12  16   8   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eb       0.10      0.12      0.11        50\n",
    "          yd       0.26      0.47      0.34        62\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          sg       0.15      0.53      0.24        51\n",
    "          eg       0.14      0.40      0.21        50\n",
    "          by       0.12      0.28      0.17        40\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ds       0.23      0.19      0.21        31\n",
    "          ek       0.00      0.00      0.00        48\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.07      0.13      0.08       609\n",
    "weighted avg       0.08      0.16      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 02:20:46 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 30 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6961851679828563, 2.688671935759546, 2.6821373338965557, 2.6764374949857714, 2.6711369555180493, 2.665427585344988, 2.6594213386278827, 2.651983016034457, 2.644049693406705, 2.6341976806252263, 2.6241059843542542, 2.6085779643411118, 2.600332753020163, 2.5935499351012883, 2.57917846364928, 2.566394143895367, 2.555143416221506, 2.5578908728457046, 2.5461963115654553, 2.5368134462578933, 2.53351668849563, 2.532758071113298, 2.5271544021925902, 2.5274966448202902, 2.520081340777267, 2.5220387757118115, 2.540339108562626, 2.523007132932666, 2.5201616584765305, 2.5196889398133226, 2.526310268489794, 2.518239130919007, 2.5567472278582444, 2.525649135727405, 2.5590286885184805, 2.5152139976889827, 2.523735288328725, 2.514419274181372, 2.51764989721364, 2.5178171981535913, 2.5101045285931165, 2.5123847266919115, 2.5112372873647657, 2.5111294047194357, 2.509502799640148, 2.5106696514856246, 2.5084807203321033, 2.5067332131522044, 2.508862798437109, 2.5521464159923233, 2.514684708443377, 2.5045204471876272, 2.506782323856072, 2.500868139987313, 2.499209239956585, 2.4978939488603564, 2.499138038538164, 2.4976619593615603, 2.4972222211521444, 2.496945127869279, 2.4973489369077635, 2.497807550900088, 2.4958665699794373, 2.495629578388383, 2.4966924660311545, 2.495067644980545, 2.495053106145123, 2.494914725496264, 2.49419062125859, 2.4931968585610975, 2.4927413534061076, 2.493705854823045, 2.492435208290864, 2.492355154065663, 2.4927560347445885, 2.4914712584860417, 2.4909918656685863, 2.4906171550500176, 2.4909398868949153, 2.490677374728599, 2.490606743322413, 2.489534014747256, 2.4894633633749828, 2.488790613481368, 2.4902749042009877, 2.4905065209994763, 2.489844920795735, 2.49000049224628, 2.4904239052426442, 2.4903156451991038, 2.4899255203691806, 2.4893328344880654, 2.49065572131052, 2.4895510501266505, 2.4885408933135285, 2.4893399126619737, 2.488015621949495, 2.4881295813324025, 2.486979780134505, 2.486910507596772, 2.4867376584333347, 2.486427146617219, 2.487055084388244, 2.4872713061584824, 2.486224021426171, 2.4824930533203977, 2.49963001429741, 2.4890483463143283, 2.496271345611472, 2.4939297701924894, 2.493015134667332, 2.491274463719335, 2.488420055417592, 2.491806290615564, 2.4925029810230526, 2.492924167996361, 2.4924482888188857, 2.4899612361770154, 2.4887036486408003, 2.4887424392261726, 2.4895194876565916, 2.488856944348816, 2.4882768742948134, 2.4885705973714445, 2.4893936198724704, 2.4893880375891877, 2.489654796268357, 2.4882310020121055, 2.48736314898837, 2.487395226661795, 2.4879282350806378, 2.4880045603453036, 2.4881481709347177, 2.4883244808867255, 2.486124916421173, 2.485156680562813, 2.4899374001914842, 2.489541080393423, 2.4872847918806404, 2.4843288102173453, 2.4851662208294045, 2.4857983142871576, 2.4885062752490366, 2.4868020886075124, 2.4869427790586975, 2.4873283477056596, 2.487167219027314, 2.4875958251639934, 2.4881212331587066, 2.487625343654739, 2.4875643546945354, 2.4872653672260605, 2.4875703413889716, 2.487719797344239, 2.487016728750395, 2.48778419345862, 2.4867973268912933, 2.48725915542377, 2.487854496011593, 2.4874868013197173, 2.4875451598457126, 2.489560146049913, 2.4886744351222596, 2.4902452894032296, 2.4872807755650364, 2.4878267783836776, 2.4872114756228694, 2.486602899867717, 2.488336234648631, 2.4869137150900706, 2.4869464023164145, 2.4874097239990736, 2.487251791069269, 2.486802733394704, 2.4877629401452825, 2.487124375326097, 2.4869205693306005, 2.487454126621115, 2.4863899223910177, 2.487157404716379, 2.4873124664444446, 2.4877140964389044, 2.488658704585434, 2.4874737188342366, 2.4871074444750456, 2.4874380715374875, 2.4873128465830985, 2.487431530881985, 2.4876341177716434, 2.4873465880971826, 2.4871241298606637, 2.4873587662363286, 2.4876768933532665, 2.4892523519707037, 2.487655388702117, 2.4873253140347735, 2.488797942993089, 2.487415124238614, 2.4881599391818243, 2.4871928018497913, 2.4880103970983347, 2.488728745621805, 2.4878026108044904, 2.486838489526207, 2.488936517038956, 2.4876169688595926, 2.4872770301618403, 2.48807203906706, 2.4884606981512363, 2.490726355848641, 2.4881156857182045, 2.4880298996598067, 2.4877133533872406, 2.4878137522730333, 2.487738690352792, 2.487529573377913, 2.4877661942261193, 2.4879153048855134, 2.4885474551095945, 2.48873747706609, 2.4895867592791228, 2.489151929986888, 2.488470678846237, 2.4894344344710677, 2.489482170255313, 2.4876827449829904, 2.489356996977858, 2.488496488342536, 2.4890821152328466, 2.4894310437595513, 2.4896876502702585, 2.4911199469480216, 2.490942364255783, 2.494527398659091, 2.493966743863862, 2.495641715029386, 2.4930499668779045, 2.4932376418403415, 2.4910898506152024, 2.490796742963869, 2.4930461349549944, 2.492607350419895, 2.4921837150561204, 2.4918890206880366, 2.491446696674491, 2.492967479921914, 2.4916160744790763, 2.4923767722494694, 2.491198217536037, 2.4917364183122106, 2.4907330151261955, 2.494095932282446, 2.491138303221153, 2.4915347220666693, 2.4909178321976184, 2.4907294318006543, 2.4904878550562364, 2.491191038357213, 2.4906982545586445, 2.491460938759038, 2.490938912080035, 2.490765520700289, 2.4917675994691395, 2.4915785938256674, 2.491281088937092, 2.491068436790178, 2.4908472501194145, 2.491121257662969, 2.491473762076868, 2.491583043522827, 2.491727874784047, 2.4907615247422643, 2.492465658532379, 2.4912741896749915, 2.4912297980147238, 2.4917346139259524, 2.490307520567294, 2.4917396716100635, 2.489964042391096, 2.4901096214018823, 2.4906988069537435, 2.4908538792520907, 2.491044307968691, 2.490553906398454, 2.4903620710513863, 2.491811019837954, 2.4899844727884175, 2.491536676002841, 2.489642902352344, 2.4905064207775447, 2.491213897178913, 2.490577537242219, 2.4912420137370943, 2.4899066273606274, 2.4913276322369504, 2.490125781405344, 2.490888125790751, 2.4913608272283145, 2.4915938839340837, 2.4944197575642755], 'val_acc': [0.08866995063492622, 0.08866995063492622, 0.08866995063492622, 0.08866995063492622, 0.08866995063492622, 0.08866995063492622, 0.1018062392356752, 0.11165845618018963, 0.11658456475031982, 0.1198686370977823, 0.12151067312470407, 0.12807881772175603, 0.12972085384655077, 0.13300492609614026, 0.14449917797873957, 0.13628899824785679, 0.14121510672011398, 0.12972085374867778, 0.13628899725689286, 0.14121510572915008, 0.1412151056312771, 0.13957307059531923, 0.13464696132784407, 0.1428571423433097, 0.14778325081556692, 0.1428571424411827, 0.1444991783702315, 0.13464696152359001, 0.147783250521948, 0.1461412145928992, 0.13628899774625775, 0.13793103396892548, 0.1428571423433097, 0.14778325061982098, 0.15106732286941046, 0.13628899784413073, 0.1395730701915932, 0.1510673230651564, 0.1444991783702315, 0.1444991783702315, 0.15763546746646243, 0.15927750368913016, 0.15763546776008136, 0.15599343163528662, 0.16748768450884982, 0.1691297205357716, 0.1691297205357716, 0.15599343134166768, 0.13957306989797427, 0.14285714224543672, 0.14778325081556692, 0.1543513953147459, 0.15599343143954067, 0.1625615761344656, 0.15270935918995115, 0.17077175675843934, 0.17241379298110704, 0.16091953991179786, 0.17569786513282357, 0.16091953991179786, 0.16091953991179786, 0.17077175675843934, 0.1691297206336446, 0.16420361216138737, 0.1707717568563123, 0.17241379288323408, 0.1773399012576183, 0.1691297206336446, 0.15599343143954067, 0.17569786523069655, 0.16748768441097686, 0.16748768441097686, 0.17405582900802882, 0.18226600972987553, 0.1691297206336446, 0.1773399013554913, 0.18555008197946501, 0.1642036120635144, 0.1724137927853611, 0.18226600963200254, 0.17405582900802882, 0.18390804595254326, 0.18719211820213275, 0.18062397370295377, 0.17077175675843934, 0.178981937578159, 0.18719211820213275, 0.1691297205357716, 0.16091954010754383, 0.17077175666056635, 0.16584564838405508, 0.18062397370295377, 0.16420361235713332, 0.16584564848192807, 0.1888341520513807, 0.16420361235713332, 0.18555007980179122, 0.17241379080343325, 0.1822660075522017, 0.17241379080343325, 0.17733989917781748, 0.1789819354004852, 0.17405582712397397, 0.17733989927569047, 0.1642036102773325, 0.16912972073151755, 0.1543513953147459, 0.1674876847045958, 0.1773399012576183, 0.16256157603659263, 0.17569786513282357, 0.1707717568563123, 0.15599343163528662, 0.17241379090130624, 0.15763546795582734, 0.16256157425041073, 0.1789819354983582, 0.16091954000967085, 0.167487682526922, 0.18062397152527995, 0.1625615761344656, 0.16748768262479496, 0.16091954000967085, 0.16091954000967085, 0.167487682526922, 0.17569786305302273, 0.16256157603659263, 0.17241379090130624, 0.174055826928228, 0.1592775038848761, 0.174055826928228, 0.17569786305302273, 0.1707717568563123, 0.17241379307898003, 0.1592775038848761, 0.174055826928228, 0.15599343143954067, 0.1707717547765115, 0.16912972082939054, 0.17077175695418528, 0.16912972034002563, 0.16748768450884982, 0.16912972073151755, 0.1773399013554913, 0.16420361216138737, 0.17405582910590178, 0.1756978653285695, 0.1756978653285695, 0.17077175666056635, 0.16748768431310387, 0.1773399013554913, 0.17077175695418528, 0.17405582891015584, 0.15927750368913016, 0.18226600972987553, 0.17733990145336426, 0.17241379298110704, 0.17569786523069655, 0.17405582910590178, 0.1642036120635144, 0.16584564809043614, 0.17569786513282357, 0.17405582900802882, 0.15927750368913016, 0.17405582900802882, 0.17405582910590178, 0.17569786513282357, 0.17405582900802882, 0.17898193728454007, 0.17405582900802882, 0.17569786523069655, 0.17405582891015584, 0.16256157593871964, 0.17405582900802882, 0.17569786513282357, 0.1674876842152309, 0.1642036118677684, 0.1773399013554913, 0.17569786513282357, 0.17733990106187233, 0.17077175656269336, 0.17569786513282357, 0.17569786513282357, 0.1724137927853611, 0.16584564809043614, 0.17077175646482037, 0.1691297206336446, 0.17077175675843934, 0.17077175646482037, 0.17898193728454007, 0.17405582900802882, 0.1724137927853611, 0.17405582891015584, 0.1806239735072078, 0.16584564818830913, 0.17405582900802882, 0.17733990115974532, 0.17405582891015584, 0.17898193728454007, 0.17405582900802882, 0.17077175656269336, 0.17569786503495058, 0.1724137927853611, 0.17077175656269336, 0.17405582891015584, 0.1724137927853611, 0.1724137927853611, 0.17898193728454007, 0.17077175656269336, 0.17733990115974532, 0.17405582891015584, 0.17405582900802882, 0.16912972043789862, 0.17077175656269336, 0.1691297205357716, 0.17898193728454007, 0.17733990115974532, 0.17405582891015584, 0.17077175656269336, 0.17569786513282357, 0.17405582891015584, 0.16256157574297367, 0.17077175656269336, 0.1724137927853611, 0.16091953961817893, 0.17077175656269336, 0.1724137927853611, 0.16912972043789862, 0.16912972043789862, 0.17077175675843934, 0.1724137927853611, 0.17077175675843934, 0.17733990115974532, 0.16912972043789862, 0.15927750359125717, 0.1642036118677684, 0.16584564818830913, 0.16584564809043614, 0.1642036118677684, 0.16584564818830913, 0.16584564818830913, 0.16091953961817893, 0.16584564818830913, 0.16584564818830913, 0.1642036118677684, 0.16256157574297367, 0.16748768441097686, 0.16256157574297367, 0.16256157574297367, 0.16584564818830913, 0.16584564818830913, 0.16256157574297367, 0.16584564818830913, 0.1642036120635144, 0.16584564818830913, 0.16256157574297367, 0.16748768441097686, 0.16256157574297367, 0.16256157574297367, 0.16748768441097686, 0.1642036120635144, 0.16256157574297367, 0.1642036120635144, 0.16256157574297367, 0.16256157574297367, 0.16584564818830913, 0.16256157574297367, 0.1642036120635144, 0.1642036120635144, 0.1642036120635144, 0.1642036120635144, 0.1642036120635144, 0.1642036118677684, 0.16584564818830913, 0.1642036120635144, 0.16256157574297367, 0.1642036120635144, 0.16256157574297367, 0.16584564818830913, 0.16584564818830913, 0.16256157574297367, 0.1642036120635144, 0.16584564818830913, 0.16256157574297367, 0.16256157574297367, 0.16748768441097686, 0.1642036120635144, 0.16584564809043614, 0.16748768441097686, 0.1642036120635144, 0.16256157574297367, 0.16256157574297367, 0.1642036120635144, 0.16584564828618212, 0.1642036120635144, 0.1642036120635144, 0.16256157574297367, 0.1642036120635144, 0.1642036119656414, 0.16256157574297367], 'loss': [2.7008426028845003, 2.6925319185981516, 2.6855465825333487, 2.6788573253326105, 2.6729666199772266, 2.6669622413431595, 2.6600322826197504, 2.653184959580032, 2.643752675281658, 2.633047275327804, 2.62022495720176, 2.6109628627432446, 2.5908902309268895, 2.584490836376527, 2.5693210474519512, 2.551204581720873, 2.53394352572165, 2.5243537465160144, 2.5079055831172874, 2.499603829589468, 2.489469461421457, 2.4835250641041466, 2.477929101098, 2.4768298500617183, 2.4709681658774185, 2.471212040863977, 2.472994102834431, 2.471452505534679, 2.4621004548160936, 2.464784544351409, 2.460342357878323, 2.4672678043710135, 2.508989007174357, 2.493916221614736, 2.4825326082398025, 2.473841065107185, 2.4547905146463695, 2.4631398675867664, 2.451904504842582, 2.4550137513961636, 2.4518352652477287, 2.4496516237513486, 2.4479932049706243, 2.4466220364188755, 2.448981661767196, 2.4508892936628213, 2.4502699138202706, 2.4492150495674085, 2.4467989614122456, 2.462050705915604, 2.46977391429016, 2.4707417635947038, 2.4492905751880434, 2.451741223266727, 2.445945521889281, 2.44673482133133, 2.4437909769571293, 2.44550555667838, 2.4440335673228426, 2.443608418920936, 2.4425369151074294, 2.442335005903146, 2.4422589789670597, 2.4418101344020458, 2.4420377507102073, 2.4418036298340597, 2.441895395823328, 2.4404063626236496, 2.4396550697467654, 2.438748537392587, 2.4387603873344905, 2.43974017391949, 2.4383711438893783, 2.439492486781408, 2.438414182212563, 2.4392501986736637, 2.4375941302007718, 2.436323256561154, 2.436984032820872, 2.4371917883228718, 2.4361177920316033, 2.435368097783114, 2.4347102396297258, 2.434881997549069, 2.4359623071838943, 2.4349684186539857, 2.43613973084906, 2.4356218985218776, 2.435121201293914, 2.4354902678691386, 2.4354346220498213, 2.435312904028922, 2.432902303711345, 2.4339999988093757, 2.434268315274123, 2.432935383285585, 2.4325480546305065, 2.4320663394379665, 2.4327867462894512, 2.43286360000438, 2.431407098848472, 2.43187257093081, 2.433920038211517, 2.432943784333842, 2.432158548092695, 2.431841065702497, 2.4511185564789195, 2.465578676396082, 2.4467656316698454, 2.4394032290339225, 2.435780012142487, 2.4335068339439876, 2.432303460080031, 2.432385218951247, 2.432237218097005, 2.4319148171364158, 2.431415100802631, 2.4306601180677787, 2.428513577439702, 2.4293642213457174, 2.429574620111767, 2.429051066032425, 2.4279023010872716, 2.4284840384058395, 2.42739952761045, 2.4292941741630036, 2.427856253108939, 2.4276068333972405, 2.427125323234887, 2.4262273639624126, 2.4265534329463323, 2.426004533454378, 2.425927799487261, 2.4258211504017795, 2.4257586064035155, 2.424883053385991, 2.4244030649901904, 2.4265871843028606, 2.4269668210948026, 2.4278085027142473, 2.4274841979298993, 2.426359423866507, 2.4257335340707455, 2.4241243080436816, 2.4235157479985294, 2.4240341025945833, 2.422779715771058, 2.422781033780296, 2.4243380590630754, 2.4227610254189806, 2.423228519016712, 2.422319503094871, 2.4218625537668657, 2.421760683138023, 2.422346670084176, 2.421038952794163, 2.421324473876483, 2.4211751017482372, 2.422080675630354, 2.42127478254894, 2.4220002243895795, 2.4225837625272466, 2.422631068102388, 2.421934754549845, 2.4208539635738555, 2.420359619148458, 2.421633698269572, 2.4208873189695073, 2.419286789590573, 2.4198762673372114, 2.419803958066435, 2.4192306505825973, 2.4197808227499897, 2.419636346574192, 2.419176447807641, 2.4185720225379206, 2.4202193032055175, 2.4188066133973045, 2.4188949937448365, 2.4183274310227536, 2.41799641840267, 2.417699694388701, 2.4185444141560266, 2.4179046702335993, 2.4171324981556292, 2.417611890358112, 2.416671987040087, 2.4186753046830822, 2.4166984286885977, 2.4177331260587156, 2.416836610613907, 2.4169433932529585, 2.4168014722193534, 2.4177108792064126, 2.4162346918235325, 2.417690164597372, 2.4176830391619486, 2.4178092745050512, 2.4157737066124008, 2.4178530364065933, 2.4153929193406625, 2.4167640782969197, 2.4154085975897632, 2.418261475142023, 2.4166957048420055, 2.414445055192011, 2.4188565896521848, 2.4174352572439144, 2.4172183208152256, 2.415737903583221, 2.416419154075137, 2.4167585424818787, 2.414843254265599, 2.415445481286646, 2.414776492069879, 2.4146190016666234, 2.4145043497457643, 2.4143320217759214, 2.4145690094518955, 2.4139714491685558, 2.4162789375140683, 2.4178398535726497, 2.4145588021014017, 2.4152349919509106, 2.4153023707058887, 2.4144791190873915, 2.4157790289767225, 2.4146632353138386, 2.4156677333236476, 2.414927661688176, 2.415389529097007, 2.414987729756005, 2.41561112384287, 2.4234653801888655, 2.424472709651845, 2.42021468983049, 2.4188784788276627, 2.4181646058202033, 2.4177532679736, 2.4185535747167757, 2.4179157648732774, 2.417471116475256, 2.416650332170835, 2.4163901450452863, 2.4172860749693132, 2.41656680547726, 2.416226018282912, 2.4159959374022435, 2.416399724478594, 2.417904592882192, 2.41779032987246, 2.4152205319375226, 2.4157968865772537, 2.4158954624277853, 2.4152813405226876, 2.41544431014717, 2.415670714094409, 2.414974024163624, 2.415139278396199, 2.4149776149334605, 2.4157149807383638, 2.4152693350946635, 2.415574620244929, 2.415367477287747, 2.4142824236617195, 2.4143916229937354, 2.414294370537666, 2.4141676430829495, 2.4140924474297116, 2.414556522291054, 2.413934537958071, 2.4144671145405856, 2.414718434942821, 2.4136143737260323, 2.4154627205654826, 2.4165888171421184, 2.4141072162612507, 2.414070937667784, 2.4135510350644465, 2.414315553759158, 2.4138456352437547, 2.413532085340371, 2.413184523729328, 2.412760284059591, 2.414113566713901, 2.4139899972527914, 2.413499331131608, 2.4130642394510384, 2.4125118912612633, 2.4133974127211366, 2.412883259971039, 2.4124004370867596, 2.4129015645452103, 2.4124312201074996, 2.413311027354528, 2.4122792662046773, 2.413127713820283, 2.4138942157463372, 2.4132692027630505, 2.4124421437907757], 'acc': [0.08911704328025881, 0.08911704305689437, 0.0891170430752531, 0.0891170428702474, 0.08911704306607374, 0.0891170436719115, 0.09445585251099275, 0.10184804981371705, 0.12032854280432637, 0.12156057426939265, 0.12689938410596435, 0.11991786443048924, 0.13757700199831194, 0.13264886976145132, 0.1548254626854734, 0.15975359392484356, 0.14702258755049422, 0.15112936374955108, 0.15400410570525536, 0.1605749475943724, 0.1605749475943724, 0.16016427214875115, 0.16221765884995706, 0.16714579046262118, 0.16919917816131758, 0.16837782349429825, 0.16632443600978694, 0.17289527748889258, 0.1646817239708969, 0.169199178571329, 0.17659137487656282, 0.16919917796549122, 0.1597535925540591, 0.16632443483482887, 0.16550308057782095, 0.16303901348025893, 0.1716632440472041, 0.16960985715017182, 0.1753593426098324, 0.1708418890068908, 0.16550307901121017, 0.16755646864981133, 0.16837782253352523, 0.16550308016780954, 0.17084188902524952, 0.170431211015527, 0.16796714667789256, 0.16427104613864202, 0.1704312117988324, 0.16344969092086112, 0.16344969150834016, 0.17125256603748157, 0.16714579005260977, 0.16714579026679482, 0.17125256564582889, 0.1691991779471325, 0.16960985535101725, 0.16755646747485323, 0.1691991791404493, 0.16755646866817003, 0.1691991791220906, 0.17043121138882097, 0.17043121121135335, 0.16878850150402078, 0.16960985597521372, 0.1667351138236831, 0.1663244350306552, 0.16878850032906267, 0.16796714665953383, 0.17002053476824164, 0.17412730895395886, 0.1712525672491571, 0.1683778236901246, 0.17289527690141354, 0.17330595373617794, 0.1696098555652023, 0.17659137487656282, 0.17248459850003833, 0.17084188920271715, 0.17043121199465874, 0.1708418886152381, 0.1708418902002076, 0.16960985732763945, 0.17207392207528532, 0.17125256762245109, 0.1737166335083376, 0.1728952767055872, 0.17002053437658893, 0.16960985517354962, 0.1733059551253211, 0.1663244354223079, 0.17043121060551558, 0.17207392146944755, 0.16919917814295884, 0.17412730975562296, 0.17453798737369278, 0.17002053437658893, 0.1716632448488682, 0.17043121138882097, 0.17166324502633581, 0.17289527809473035, 0.1708418886152381, 0.17125256682078696, 0.17330595453784206, 0.1757700204420873, 0.1778234091015567, 0.17577002083374, 0.17166324385137774, 0.16919917929955822, 0.17043121119299465, 0.17002053318327212, 0.16837782231934018, 0.172895277898904, 0.17659137687154375, 0.17248460028083418, 0.1708418886335968, 0.17412731114476612, 0.17289527809473035, 0.17289527770307764, 0.17289527709723987, 0.17207392207528532, 0.17700205349212308, 0.17782340851407766, 0.17289527652811956, 0.17248460049501924, 0.17700205447125483, 0.17535934321567018, 0.17577002158032795, 0.17618069706266665, 0.17330595473366842, 0.1737166325475646, 0.17412731114476612, 0.1749486649917626, 0.1782340865421589, 0.18110883004611522, 0.1798767977793848, 0.17905544099499313, 0.17577002140286033, 0.17371663333086998, 0.17371663274339091, 0.18028747482833432, 0.18069815144891366, 0.18275154030420943, 0.18398357378261535, 0.18357289418792333, 0.18069815084307592, 0.18398357376425664, 0.18316221676567987, 0.1848049286271023, 0.1815195062934006, 0.17618069825598345, 0.1782340867196265, 0.18275154071422084, 0.18357289538124014, 0.18193018567390756, 0.18357289498958745, 0.18439425099067375, 0.18151950766418504, 0.18439425157815278, 0.18562628307993653, 0.17987679640860038, 0.1831622181364643, 0.18028747383084384, 0.17946611820305153, 0.18028747363501751, 0.18151950609757425, 0.18110882906698347, 0.18316221774481162, 0.18234086211701928, 0.18316221696150622, 0.18562628366741557, 0.18357289418792333, 0.1786447633769233, 0.18357289636037188, 0.1843942504031947, 0.18275154053675322, 0.18151950727253235, 0.18234086170700786, 0.1864476385119025, 0.18357289577289285, 0.18275153914761005, 0.1827515397350891, 0.18275153912925132, 0.18603696205043205, 0.18193018530061358, 0.1823408622944869, 0.18439425099067375, 0.18398357317677758, 0.18069815125308733, 0.18480492860874356, 0.18521560464184386, 0.1831622181364643, 0.18357289555870776, 0.1786447629669119, 0.18357289497122872, 0.1811088294769949, 0.18028747363501751, 0.18316221733480018, 0.18193018449894946, 0.18480492723795913, 0.18151950766418504, 0.1856262830615778, 0.18439424961988932, 0.1827515401267418, 0.1856262830615778, 0.1839835720201782, 0.18110882887115715, 0.18069815301552444, 0.18275154071422084, 0.18234086329197738, 0.17946611939636833, 0.184394250011542, 0.18521560544350799, 0.1839835731951363, 0.18275153994927415, 0.1819301850864285, 0.18644763847518506, 0.18357289596871917, 0.18275153953926274, 0.1827515401451005, 0.1827515393434364, 0.1819301858697339, 0.17987679740609086, 0.18521560563933434, 0.1872689941213361, 0.18316221715733255, 0.1860369610713003, 0.1872689929280193, 0.18193018530061358, 0.1823408627044983, 0.18726899372968342, 0.18275154071422084, 0.18439425020736835, 0.1815195068625209, 0.1856262822782724, 0.18069815144891366, 0.18193018449894946, 0.18234086135207261, 0.1819301838931117, 0.18398357298095122, 0.18069815082471719, 0.18275153914761005, 0.1827515401451005, 0.18357289459793474, 0.18398357258929854, 0.18644763790606472, 0.18562628345323048, 0.1815195066850533, 0.18193018449894946, 0.18521560583516067, 0.1843942512048588, 0.1848049291962226, 0.18398357298095122, 0.18316221794063794, 0.18603695989634222, 0.18480492762961181, 0.18275154071422084, 0.18275153914761005, 0.18275154073257954, 0.18398357298095122, 0.18562628363069814, 0.18685831669909264, 0.18193018489060217, 0.1848049291778639, 0.18398357221600456, 0.18357289595036047, 0.18357289596871917, 0.18521560681429242, 0.1819301850864285, 0.18480492860874356, 0.18480492780707944, 0.18521560601262832, 0.1827515397350891, 0.1856262826699251, 0.18275153953926274, 0.18028747482833432, 0.18151950825166407, 0.1839835720018195, 0.1856262838448832, 0.18357289575453412, 0.18562628366741557, 0.18151950648922696, 0.18562628404070955, 0.18644763908102283, 0.18357289418792333, 0.18398357258929854, 0.18439425022572709, 0.18439424981571564, 0.18726899372968342, 0.18234086211701928, 0.18809034857417034, 0.18398357376425664, 0.18275154073257954, 0.18480492843127594, 0.1819301854964399, 0.1868583173049304, 0.18480492741542676, 0.18685831511412312, 0.1839835739784417, 0.18316221815482303]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
