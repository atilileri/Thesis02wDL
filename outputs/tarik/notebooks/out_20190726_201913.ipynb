{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf15.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 20:19:13 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000019500B58550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001956D0D7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0969, Accuracy:0.3450, Validation Loss:1.0852, Validation Accuracy:0.3695\n",
    "Epoch #2: Loss:1.0815, Accuracy:0.3733, Validation Loss:1.0767, Validation Accuracy:0.3924\n",
    "Epoch #3: Loss:1.0753, Accuracy:0.3897, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #4: Loss:1.0739, Accuracy:0.3963, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3975, Validation Loss:1.0750, Validation Accuracy:0.3777\n",
    "Epoch #6: Loss:1.0741, Accuracy:0.3975, Validation Loss:1.0749, Validation Accuracy:0.3760\n",
    "Epoch #7: Loss:1.0739, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #8: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0748, Validation Accuracy:0.4039\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.4000, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #10: Loss:1.0739, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #11: Loss:1.0738, Accuracy:0.4000, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #12: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #13: Loss:1.0732, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #14: Loss:1.0732, Accuracy:0.4021, Validation Loss:1.0744, Validation Accuracy:0.3826\n",
    "Epoch #15: Loss:1.0733, Accuracy:0.3996, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #16: Loss:1.0733, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #17: Loss:1.0732, Accuracy:0.4070, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #18: Loss:1.0729, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #19: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #20: Loss:1.0733, Accuracy:0.4086, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #21: Loss:1.0726, Accuracy:0.4049, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #22: Loss:1.0725, Accuracy:0.4041, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #23: Loss:1.0723, Accuracy:0.4053, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #24: Loss:1.0723, Accuracy:0.4086, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #25: Loss:1.0724, Accuracy:0.4090, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #26: Loss:1.0724, Accuracy:0.4053, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #27: Loss:1.0721, Accuracy:0.4070, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #28: Loss:1.0720, Accuracy:0.4086, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #29: Loss:1.0717, Accuracy:0.4164, Validation Loss:1.0754, Validation Accuracy:0.3826\n",
    "Epoch #30: Loss:1.0711, Accuracy:0.4193, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #31: Loss:1.0717, Accuracy:0.4099, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #32: Loss:1.0725, Accuracy:0.3992, Validation Loss:1.0753, Validation Accuracy:0.3645\n",
    "Epoch #33: Loss:1.0718, Accuracy:0.4115, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #34: Loss:1.0724, Accuracy:0.4037, Validation Loss:1.0743, Validation Accuracy:0.3826\n",
    "Epoch #35: Loss:1.0725, Accuracy:0.3984, Validation Loss:1.0747, Validation Accuracy:0.3826\n",
    "Epoch #36: Loss:1.0719, Accuracy:0.4107, Validation Loss:1.0751, Validation Accuracy:0.3744\n",
    "Epoch #37: Loss:1.0721, Accuracy:0.4140, Validation Loss:1.0749, Validation Accuracy:0.3711\n",
    "Epoch #38: Loss:1.0723, Accuracy:0.4086, Validation Loss:1.0756, Validation Accuracy:0.3760\n",
    "Epoch #39: Loss:1.0709, Accuracy:0.4115, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #40: Loss:1.0728, Accuracy:0.3984, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #41: Loss:1.0722, Accuracy:0.4099, Validation Loss:1.0747, Validation Accuracy:0.3826\n",
    "Epoch #42: Loss:1.0716, Accuracy:0.4049, Validation Loss:1.0759, Validation Accuracy:0.3760\n",
    "Epoch #43: Loss:1.0722, Accuracy:0.4136, Validation Loss:1.0759, Validation Accuracy:0.3793\n",
    "Epoch #44: Loss:1.0711, Accuracy:0.4152, Validation Loss:1.0756, Validation Accuracy:0.3727\n",
    "Epoch #45: Loss:1.0710, Accuracy:0.4070, Validation Loss:1.0751, Validation Accuracy:0.3793\n",
    "Epoch #46: Loss:1.0722, Accuracy:0.3951, Validation Loss:1.0752, Validation Accuracy:0.3744\n",
    "Epoch #47: Loss:1.0721, Accuracy:0.4107, Validation Loss:1.0760, Validation Accuracy:0.3744\n",
    "Epoch #48: Loss:1.0710, Accuracy:0.4123, Validation Loss:1.0764, Validation Accuracy:0.3629\n",
    "Epoch #49: Loss:1.0718, Accuracy:0.4041, Validation Loss:1.0766, Validation Accuracy:0.3596\n",
    "Epoch #50: Loss:1.0715, Accuracy:0.4111, Validation Loss:1.0756, Validation Accuracy:0.3793\n",
    "Epoch #51: Loss:1.0718, Accuracy:0.4086, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #52: Loss:1.0710, Accuracy:0.4041, Validation Loss:1.0753, Validation Accuracy:0.3711\n",
    "Epoch #53: Loss:1.0708, Accuracy:0.4094, Validation Loss:1.0759, Validation Accuracy:0.3727\n",
    "Epoch #54: Loss:1.0705, Accuracy:0.4107, Validation Loss:1.0755, Validation Accuracy:0.3695\n",
    "Epoch #55: Loss:1.0712, Accuracy:0.4107, Validation Loss:1.0761, Validation Accuracy:0.3678\n",
    "Epoch #56: Loss:1.0726, Accuracy:0.4053, Validation Loss:1.0764, Validation Accuracy:0.3580\n",
    "Epoch #57: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0754, Validation Accuracy:0.3760\n",
    "Epoch #58: Loss:1.0733, Accuracy:0.4119, Validation Loss:1.0762, Validation Accuracy:0.3810\n",
    "Epoch #59: Loss:1.0742, Accuracy:0.3856, Validation Loss:1.0761, Validation Accuracy:0.3842\n",
    "Epoch #60: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0759, Validation Accuracy:0.3957\n",
    "Epoch #61: Loss:1.0730, Accuracy:0.3959, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #62: Loss:1.0723, Accuracy:0.4029, Validation Loss:1.0758, Validation Accuracy:0.3760\n",
    "Epoch #63: Loss:1.0721, Accuracy:0.4086, Validation Loss:1.0758, Validation Accuracy:0.3793\n",
    "Epoch #64: Loss:1.0719, Accuracy:0.4099, Validation Loss:1.0759, Validation Accuracy:0.3727\n",
    "Epoch #65: Loss:1.0717, Accuracy:0.4074, Validation Loss:1.0761, Validation Accuracy:0.3777\n",
    "Epoch #66: Loss:1.0714, Accuracy:0.4099, Validation Loss:1.0757, Validation Accuracy:0.3842\n",
    "Epoch #67: Loss:1.0711, Accuracy:0.4090, Validation Loss:1.0755, Validation Accuracy:0.3826\n",
    "Epoch #68: Loss:1.0716, Accuracy:0.4057, Validation Loss:1.0771, Validation Accuracy:0.3793\n",
    "Epoch #69: Loss:1.0708, Accuracy:0.4156, Validation Loss:1.0775, Validation Accuracy:0.3760\n",
    "Epoch #70: Loss:1.0710, Accuracy:0.4082, Validation Loss:1.0773, Validation Accuracy:0.3826\n",
    "Epoch #71: Loss:1.0704, Accuracy:0.4205, Validation Loss:1.0791, Validation Accuracy:0.3793\n",
    "Epoch #72: Loss:1.0710, Accuracy:0.4057, Validation Loss:1.0763, Validation Accuracy:0.3859\n",
    "Epoch #73: Loss:1.0699, Accuracy:0.4144, Validation Loss:1.0769, Validation Accuracy:0.3760\n",
    "Epoch #74: Loss:1.0693, Accuracy:0.4168, Validation Loss:1.0768, Validation Accuracy:0.3744\n",
    "Epoch #75: Loss:1.0700, Accuracy:0.4140, Validation Loss:1.0775, Validation Accuracy:0.3678\n",
    "Epoch #76: Loss:1.0707, Accuracy:0.4127, Validation Loss:1.0780, Validation Accuracy:0.3875\n",
    "Epoch #77: Loss:1.0704, Accuracy:0.4193, Validation Loss:1.0786, Validation Accuracy:0.3875\n",
    "Epoch #78: Loss:1.0698, Accuracy:0.4189, Validation Loss:1.0769, Validation Accuracy:0.3662\n",
    "Epoch #79: Loss:1.0681, Accuracy:0.4283, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #80: Loss:1.0701, Accuracy:0.4115, Validation Loss:1.0748, Validation Accuracy:0.3695\n",
    "Epoch #81: Loss:1.0704, Accuracy:0.3971, Validation Loss:1.0744, Validation Accuracy:0.3662\n",
    "Epoch #82: Loss:1.0706, Accuracy:0.4111, Validation Loss:1.0757, Validation Accuracy:0.3744\n",
    "Epoch #83: Loss:1.0720, Accuracy:0.3955, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #84: Loss:1.0684, Accuracy:0.4177, Validation Loss:1.0787, Validation Accuracy:0.3941\n",
    "Epoch #85: Loss:1.0690, Accuracy:0.4099, Validation Loss:1.0772, Validation Accuracy:0.3826\n",
    "Epoch #86: Loss:1.0676, Accuracy:0.4066, Validation Loss:1.0762, Validation Accuracy:0.3760\n",
    "Epoch #87: Loss:1.0656, Accuracy:0.4209, Validation Loss:1.0762, Validation Accuracy:0.3777\n",
    "Epoch #88: Loss:1.0646, Accuracy:0.4337, Validation Loss:1.0765, Validation Accuracy:0.3760\n",
    "Epoch #89: Loss:1.0639, Accuracy:0.4201, Validation Loss:1.0772, Validation Accuracy:0.3777\n",
    "Epoch #90: Loss:1.0633, Accuracy:0.4292, Validation Loss:1.0790, Validation Accuracy:0.3810\n",
    "Epoch #91: Loss:1.0611, Accuracy:0.4312, Validation Loss:1.0829, Validation Accuracy:0.3727\n",
    "Epoch #92: Loss:1.0604, Accuracy:0.4341, Validation Loss:1.0793, Validation Accuracy:0.3727\n",
    "Epoch #93: Loss:1.0610, Accuracy:0.4148, Validation Loss:1.0837, Validation Accuracy:0.3941\n",
    "Epoch #94: Loss:1.0664, Accuracy:0.4119, Validation Loss:1.0795, Validation Accuracy:0.3957\n",
    "Epoch #95: Loss:1.0639, Accuracy:0.4053, Validation Loss:1.0862, Validation Accuracy:0.3908\n",
    "Epoch #96: Loss:1.0618, Accuracy:0.4329, Validation Loss:1.0838, Validation Accuracy:0.3892\n",
    "Epoch #97: Loss:1.0632, Accuracy:0.4090, Validation Loss:1.0822, Validation Accuracy:0.3744\n",
    "Epoch #98: Loss:1.0665, Accuracy:0.4016, Validation Loss:1.0775, Validation Accuracy:0.3793\n",
    "Epoch #99: Loss:1.0644, Accuracy:0.4193, Validation Loss:1.0777, Validation Accuracy:0.3645\n",
    "Epoch #100: Loss:1.0640, Accuracy:0.4152, Validation Loss:1.0796, Validation Accuracy:0.3810\n",
    "Epoch #101: Loss:1.0647, Accuracy:0.4119, Validation Loss:1.0817, Validation Accuracy:0.3810\n",
    "Epoch #102: Loss:1.0619, Accuracy:0.4164, Validation Loss:1.0799, Validation Accuracy:0.3793\n",
    "Epoch #103: Loss:1.0620, Accuracy:0.4177, Validation Loss:1.0802, Validation Accuracy:0.3810\n",
    "Epoch #104: Loss:1.0611, Accuracy:0.4086, Validation Loss:1.0790, Validation Accuracy:0.3892\n",
    "Epoch #105: Loss:1.0603, Accuracy:0.4193, Validation Loss:1.0773, Validation Accuracy:0.3727\n",
    "Epoch #106: Loss:1.0610, Accuracy:0.4148, Validation Loss:1.0768, Validation Accuracy:0.3875\n",
    "Epoch #107: Loss:1.0614, Accuracy:0.4136, Validation Loss:1.0761, Validation Accuracy:0.3727\n",
    "Epoch #108: Loss:1.0609, Accuracy:0.4201, Validation Loss:1.0807, Validation Accuracy:0.3810\n",
    "Epoch #109: Loss:1.0618, Accuracy:0.4103, Validation Loss:1.0791, Validation Accuracy:0.3859\n",
    "Epoch #110: Loss:1.0625, Accuracy:0.4045, Validation Loss:1.1110, Validation Accuracy:0.3892\n",
    "Epoch #111: Loss:1.0858, Accuracy:0.4062, Validation Loss:1.1018, Validation Accuracy:0.3826\n",
    "Epoch #112: Loss:1.0745, Accuracy:0.3963, Validation Loss:1.0873, Validation Accuracy:0.3924\n",
    "Epoch #113: Loss:1.0711, Accuracy:0.4136, Validation Loss:1.0776, Validation Accuracy:0.3810\n",
    "Epoch #114: Loss:1.0648, Accuracy:0.3910, Validation Loss:1.0767, Validation Accuracy:0.3842\n",
    "Epoch #115: Loss:1.0616, Accuracy:0.4156, Validation Loss:1.0772, Validation Accuracy:0.3892\n",
    "Epoch #116: Loss:1.0612, Accuracy:0.4312, Validation Loss:1.0769, Validation Accuracy:0.3760\n",
    "Epoch #117: Loss:1.0599, Accuracy:0.4238, Validation Loss:1.0762, Validation Accuracy:0.3695\n",
    "Epoch #118: Loss:1.0594, Accuracy:0.4333, Validation Loss:1.0765, Validation Accuracy:0.3826\n",
    "Epoch #119: Loss:1.0556, Accuracy:0.4345, Validation Loss:1.0791, Validation Accuracy:0.3908\n",
    "Epoch #120: Loss:1.0558, Accuracy:0.4181, Validation Loss:1.0800, Validation Accuracy:0.3810\n",
    "Epoch #121: Loss:1.0554, Accuracy:0.4279, Validation Loss:1.0832, Validation Accuracy:0.3842\n",
    "Epoch #122: Loss:1.0537, Accuracy:0.4304, Validation Loss:1.0874, Validation Accuracy:0.3842\n",
    "Epoch #123: Loss:1.0579, Accuracy:0.4292, Validation Loss:1.0907, Validation Accuracy:0.3875\n",
    "Epoch #124: Loss:1.0586, Accuracy:0.4267, Validation Loss:1.0866, Validation Accuracy:0.3744\n",
    "Epoch #125: Loss:1.0545, Accuracy:0.4308, Validation Loss:1.0857, Validation Accuracy:0.3711\n",
    "Epoch #126: Loss:1.0512, Accuracy:0.4300, Validation Loss:1.0814, Validation Accuracy:0.3760\n",
    "Epoch #127: Loss:1.0519, Accuracy:0.4361, Validation Loss:1.0874, Validation Accuracy:0.3842\n",
    "Epoch #128: Loss:1.0507, Accuracy:0.4386, Validation Loss:1.0830, Validation Accuracy:0.3793\n",
    "Epoch #129: Loss:1.0593, Accuracy:0.4283, Validation Loss:1.0912, Validation Accuracy:0.3875\n",
    "Epoch #130: Loss:1.0611, Accuracy:0.4066, Validation Loss:1.0884, Validation Accuracy:0.3842\n",
    "Epoch #131: Loss:1.0530, Accuracy:0.4316, Validation Loss:1.0811, Validation Accuracy:0.3941\n",
    "Epoch #132: Loss:1.0473, Accuracy:0.4427, Validation Loss:1.0829, Validation Accuracy:0.3859\n",
    "Epoch #133: Loss:1.0478, Accuracy:0.4316, Validation Loss:1.0961, Validation Accuracy:0.3744\n",
    "Epoch #134: Loss:1.0589, Accuracy:0.4172, Validation Loss:1.1056, Validation Accuracy:0.3727\n",
    "Epoch #135: Loss:1.0733, Accuracy:0.4193, Validation Loss:1.0963, Validation Accuracy:0.3727\n",
    "Epoch #136: Loss:1.0684, Accuracy:0.4053, Validation Loss:1.0952, Validation Accuracy:0.3842\n",
    "Epoch #137: Loss:1.0655, Accuracy:0.4193, Validation Loss:1.0845, Validation Accuracy:0.3826\n",
    "Epoch #138: Loss:1.0628, Accuracy:0.4230, Validation Loss:1.0808, Validation Accuracy:0.3629\n",
    "Epoch #139: Loss:1.0607, Accuracy:0.4222, Validation Loss:1.0794, Validation Accuracy:0.3908\n",
    "Epoch #140: Loss:1.0597, Accuracy:0.4218, Validation Loss:1.0810, Validation Accuracy:0.3842\n",
    "Epoch #141: Loss:1.0563, Accuracy:0.4308, Validation Loss:1.0849, Validation Accuracy:0.3498\n",
    "Epoch #142: Loss:1.0555, Accuracy:0.4333, Validation Loss:1.0823, Validation Accuracy:0.3711\n",
    "Epoch #143: Loss:1.0551, Accuracy:0.4324, Validation Loss:1.0867, Validation Accuracy:0.3727\n",
    "Epoch #144: Loss:1.0530, Accuracy:0.4238, Validation Loss:1.0868, Validation Accuracy:0.3695\n",
    "Epoch #145: Loss:1.0520, Accuracy:0.4283, Validation Loss:1.0865, Validation Accuracy:0.3793\n",
    "Epoch #146: Loss:1.0528, Accuracy:0.4251, Validation Loss:1.0895, Validation Accuracy:0.3826\n",
    "Epoch #147: Loss:1.0543, Accuracy:0.4279, Validation Loss:1.0952, Validation Accuracy:0.3547\n",
    "Epoch #148: Loss:1.0515, Accuracy:0.4353, Validation Loss:1.0965, Validation Accuracy:0.3777\n",
    "Epoch #149: Loss:1.0497, Accuracy:0.4357, Validation Loss:1.0918, Validation Accuracy:0.3711\n",
    "Epoch #150: Loss:1.0565, Accuracy:0.4214, Validation Loss:1.0925, Validation Accuracy:0.3678\n",
    "Epoch #151: Loss:1.0596, Accuracy:0.4205, Validation Loss:1.0951, Validation Accuracy:0.3530\n",
    "Epoch #152: Loss:1.0613, Accuracy:0.4275, Validation Loss:1.0927, Validation Accuracy:0.3612\n",
    "Epoch #153: Loss:1.0594, Accuracy:0.4275, Validation Loss:1.0902, Validation Accuracy:0.3793\n",
    "Epoch #154: Loss:1.0584, Accuracy:0.4304, Validation Loss:1.0914, Validation Accuracy:0.3744\n",
    "Epoch #155: Loss:1.0483, Accuracy:0.4386, Validation Loss:1.0926, Validation Accuracy:0.3695\n",
    "Epoch #156: Loss:1.0518, Accuracy:0.4329, Validation Loss:1.0859, Validation Accuracy:0.3711\n",
    "Epoch #157: Loss:1.0483, Accuracy:0.4402, Validation Loss:1.0814, Validation Accuracy:0.3875\n",
    "Epoch #158: Loss:1.0510, Accuracy:0.4341, Validation Loss:1.0836, Validation Accuracy:0.3777\n",
    "Epoch #159: Loss:1.0490, Accuracy:0.4394, Validation Loss:1.0844, Validation Accuracy:0.3744\n",
    "Epoch #160: Loss:1.0441, Accuracy:0.4444, Validation Loss:1.0886, Validation Accuracy:0.3810\n",
    "Epoch #161: Loss:1.0450, Accuracy:0.4333, Validation Loss:1.0898, Validation Accuracy:0.3875\n",
    "Epoch #162: Loss:1.0429, Accuracy:0.4345, Validation Loss:1.0919, Validation Accuracy:0.3760\n",
    "Epoch #163: Loss:1.0397, Accuracy:0.4398, Validation Loss:1.0942, Validation Accuracy:0.3842\n",
    "Epoch #164: Loss:1.0369, Accuracy:0.4386, Validation Loss:1.0955, Validation Accuracy:0.3908\n",
    "Epoch #165: Loss:1.0355, Accuracy:0.4485, Validation Loss:1.1033, Validation Accuracy:0.3842\n",
    "Epoch #166: Loss:1.0425, Accuracy:0.4312, Validation Loss:1.0893, Validation Accuracy:0.3859\n",
    "Epoch #167: Loss:1.0487, Accuracy:0.4390, Validation Loss:1.0876, Validation Accuracy:0.3810\n",
    "Epoch #168: Loss:1.0479, Accuracy:0.4411, Validation Loss:1.0847, Validation Accuracy:0.3645\n",
    "Epoch #169: Loss:1.0464, Accuracy:0.4472, Validation Loss:1.0903, Validation Accuracy:0.3760\n",
    "Epoch #170: Loss:1.0473, Accuracy:0.4394, Validation Loss:1.0929, Validation Accuracy:0.3727\n",
    "Epoch #171: Loss:1.0461, Accuracy:0.4415, Validation Loss:1.0996, Validation Accuracy:0.3629\n",
    "Epoch #172: Loss:1.0593, Accuracy:0.4234, Validation Loss:1.0969, Validation Accuracy:0.3678\n",
    "Epoch #173: Loss:1.0524, Accuracy:0.4283, Validation Loss:1.0927, Validation Accuracy:0.3924\n",
    "Epoch #174: Loss:1.0498, Accuracy:0.4214, Validation Loss:1.0857, Validation Accuracy:0.3892\n",
    "Epoch #175: Loss:1.0476, Accuracy:0.4329, Validation Loss:1.0860, Validation Accuracy:0.3678\n",
    "Epoch #176: Loss:1.0473, Accuracy:0.4353, Validation Loss:1.0851, Validation Accuracy:0.3695\n",
    "Epoch #177: Loss:1.0470, Accuracy:0.4329, Validation Loss:1.0885, Validation Accuracy:0.3695\n",
    "Epoch #178: Loss:1.0441, Accuracy:0.4341, Validation Loss:1.0902, Validation Accuracy:0.3842\n",
    "Epoch #179: Loss:1.0422, Accuracy:0.4398, Validation Loss:1.0926, Validation Accuracy:0.3777\n",
    "Epoch #180: Loss:1.0430, Accuracy:0.4287, Validation Loss:1.0905, Validation Accuracy:0.3842\n",
    "Epoch #181: Loss:1.0417, Accuracy:0.4390, Validation Loss:1.0913, Validation Accuracy:0.3793\n",
    "Epoch #182: Loss:1.0400, Accuracy:0.4411, Validation Loss:1.0968, Validation Accuracy:0.3793\n",
    "Epoch #183: Loss:1.0386, Accuracy:0.4386, Validation Loss:1.1092, Validation Accuracy:0.3695\n",
    "Epoch #184: Loss:1.0403, Accuracy:0.4287, Validation Loss:1.1165, Validation Accuracy:0.3678\n",
    "Epoch #185: Loss:1.0387, Accuracy:0.4427, Validation Loss:1.1135, Validation Accuracy:0.3596\n",
    "Epoch #186: Loss:1.0369, Accuracy:0.4407, Validation Loss:1.1082, Validation Accuracy:0.3842\n",
    "Epoch #187: Loss:1.0344, Accuracy:0.4448, Validation Loss:1.1053, Validation Accuracy:0.3727\n",
    "Epoch #188: Loss:1.0317, Accuracy:0.4435, Validation Loss:1.1108, Validation Accuracy:0.3810\n",
    "Epoch #189: Loss:1.0284, Accuracy:0.4509, Validation Loss:1.1094, Validation Accuracy:0.3793\n",
    "Epoch #190: Loss:1.0285, Accuracy:0.4382, Validation Loss:1.1110, Validation Accuracy:0.3727\n",
    "Epoch #191: Loss:1.0284, Accuracy:0.4444, Validation Loss:1.1166, Validation Accuracy:0.3777\n",
    "Epoch #192: Loss:1.0270, Accuracy:0.4407, Validation Loss:1.1207, Validation Accuracy:0.3530\n",
    "Epoch #193: Loss:1.0264, Accuracy:0.4526, Validation Loss:1.1193, Validation Accuracy:0.3892\n",
    "Epoch #194: Loss:1.0340, Accuracy:0.4435, Validation Loss:1.1034, Validation Accuracy:0.3859\n",
    "Epoch #195: Loss:1.0330, Accuracy:0.4394, Validation Loss:1.1058, Validation Accuracy:0.3810\n",
    "Epoch #196: Loss:1.0322, Accuracy:0.4407, Validation Loss:1.1059, Validation Accuracy:0.3678\n",
    "Epoch #197: Loss:1.0334, Accuracy:0.4448, Validation Loss:1.1325, Validation Accuracy:0.3580\n",
    "Epoch #198: Loss:1.0351, Accuracy:0.4407, Validation Loss:1.1333, Validation Accuracy:0.3859\n",
    "Epoch #199: Loss:1.0328, Accuracy:0.4513, Validation Loss:1.1039, Validation Accuracy:0.3727\n",
    "Epoch #200: Loss:1.0240, Accuracy:0.4517, Validation Loss:1.1011, Validation Accuracy:0.3760\n",
    "Epoch #201: Loss:1.0221, Accuracy:0.4517, Validation Loss:1.1048, Validation Accuracy:0.3793\n",
    "Epoch #202: Loss:1.0197, Accuracy:0.4653, Validation Loss:1.1180, Validation Accuracy:0.3695\n",
    "Epoch #203: Loss:1.0172, Accuracy:0.4546, Validation Loss:1.1136, Validation Accuracy:0.3727\n",
    "Epoch #204: Loss:1.0138, Accuracy:0.4628, Validation Loss:1.1182, Validation Accuracy:0.3727\n",
    "Epoch #205: Loss:1.0127, Accuracy:0.4542, Validation Loss:1.1169, Validation Accuracy:0.3711\n",
    "Epoch #206: Loss:1.0119, Accuracy:0.4608, Validation Loss:1.1243, Validation Accuracy:0.3596\n",
    "Epoch #207: Loss:1.0122, Accuracy:0.4550, Validation Loss:1.1307, Validation Accuracy:0.3744\n",
    "Epoch #208: Loss:1.0108, Accuracy:0.4686, Validation Loss:1.1388, Validation Accuracy:0.3842\n",
    "Epoch #209: Loss:1.0084, Accuracy:0.4706, Validation Loss:1.1370, Validation Accuracy:0.3727\n",
    "Epoch #210: Loss:1.0032, Accuracy:0.4645, Validation Loss:1.1373, Validation Accuracy:0.3678\n",
    "Epoch #211: Loss:1.0031, Accuracy:0.4686, Validation Loss:1.1274, Validation Accuracy:0.3826\n",
    "Epoch #212: Loss:1.0040, Accuracy:0.4653, Validation Loss:1.1249, Validation Accuracy:0.3695\n",
    "Epoch #213: Loss:1.0044, Accuracy:0.4612, Validation Loss:1.1325, Validation Accuracy:0.3793\n",
    "Epoch #214: Loss:1.0047, Accuracy:0.4690, Validation Loss:1.1497, Validation Accuracy:0.3744\n",
    "Epoch #215: Loss:1.0021, Accuracy:0.4715, Validation Loss:1.1462, Validation Accuracy:0.3711\n",
    "Epoch #216: Loss:1.0010, Accuracy:0.4641, Validation Loss:1.1416, Validation Accuracy:0.3645\n",
    "Epoch #217: Loss:1.0078, Accuracy:0.4624, Validation Loss:1.1361, Validation Accuracy:0.3711\n",
    "Epoch #218: Loss:1.0072, Accuracy:0.4575, Validation Loss:1.1570, Validation Accuracy:0.3875\n",
    "Epoch #219: Loss:1.0112, Accuracy:0.4612, Validation Loss:1.2027, Validation Accuracy:0.3662\n",
    "Epoch #220: Loss:1.0620, Accuracy:0.4472, Validation Loss:1.1524, Validation Accuracy:0.3826\n",
    "Epoch #221: Loss:1.0311, Accuracy:0.4690, Validation Loss:1.1016, Validation Accuracy:0.3596\n",
    "Epoch #222: Loss:1.0429, Accuracy:0.4205, Validation Loss:1.0882, Validation Accuracy:0.3810\n",
    "Epoch #223: Loss:1.0433, Accuracy:0.4485, Validation Loss:1.0933, Validation Accuracy:0.3957\n",
    "Epoch #224: Loss:1.0437, Accuracy:0.4386, Validation Loss:1.0969, Validation Accuracy:0.3777\n",
    "Epoch #225: Loss:1.0452, Accuracy:0.4205, Validation Loss:1.0880, Validation Accuracy:0.3777\n",
    "Epoch #226: Loss:1.0418, Accuracy:0.4415, Validation Loss:1.0906, Validation Accuracy:0.3711\n",
    "Epoch #227: Loss:1.0339, Accuracy:0.4439, Validation Loss:1.0945, Validation Accuracy:0.3793\n",
    "Epoch #228: Loss:1.0309, Accuracy:0.4423, Validation Loss:1.0993, Validation Accuracy:0.3777\n",
    "Epoch #229: Loss:1.0298, Accuracy:0.4439, Validation Loss:1.1071, Validation Accuracy:0.3596\n",
    "Epoch #230: Loss:1.0282, Accuracy:0.4460, Validation Loss:1.1102, Validation Accuracy:0.3711\n",
    "Epoch #231: Loss:1.0278, Accuracy:0.4501, Validation Loss:1.1100, Validation Accuracy:0.3777\n",
    "Epoch #232: Loss:1.0282, Accuracy:0.4546, Validation Loss:1.1084, Validation Accuracy:0.3678\n",
    "Epoch #233: Loss:1.0222, Accuracy:0.4637, Validation Loss:1.1134, Validation Accuracy:0.3629\n",
    "Epoch #234: Loss:1.0204, Accuracy:0.4550, Validation Loss:1.1154, Validation Accuracy:0.3662\n",
    "Epoch #235: Loss:1.0150, Accuracy:0.4538, Validation Loss:1.1239, Validation Accuracy:0.3760\n",
    "Epoch #236: Loss:1.0051, Accuracy:0.4559, Validation Loss:1.1307, Validation Accuracy:0.3810\n",
    "Epoch #237: Loss:0.9990, Accuracy:0.4731, Validation Loss:1.1484, Validation Accuracy:0.3777\n",
    "Epoch #238: Loss:0.9969, Accuracy:0.4715, Validation Loss:1.1422, Validation Accuracy:0.3760\n",
    "Epoch #239: Loss:0.9970, Accuracy:0.4674, Validation Loss:1.1467, Validation Accuracy:0.3875\n",
    "Epoch #240: Loss:0.9966, Accuracy:0.4719, Validation Loss:1.1408, Validation Accuracy:0.3859\n",
    "Epoch #241: Loss:0.9930, Accuracy:0.4698, Validation Loss:1.1410, Validation Accuracy:0.3875\n",
    "Epoch #242: Loss:0.9948, Accuracy:0.4587, Validation Loss:1.1353, Validation Accuracy:0.3777\n",
    "Epoch #243: Loss:0.9968, Accuracy:0.4710, Validation Loss:1.1364, Validation Accuracy:0.3990\n",
    "Epoch #244: Loss:0.9970, Accuracy:0.4653, Validation Loss:1.1496, Validation Accuracy:0.3908\n",
    "Epoch #245: Loss:1.0115, Accuracy:0.4612, Validation Loss:1.1555, Validation Accuracy:0.3826\n",
    "Epoch #246: Loss:1.0080, Accuracy:0.4686, Validation Loss:1.1582, Validation Accuracy:0.3793\n",
    "Epoch #247: Loss:1.0224, Accuracy:0.4612, Validation Loss:1.1544, Validation Accuracy:0.3810\n",
    "Epoch #248: Loss:1.0031, Accuracy:0.4706, Validation Loss:1.1244, Validation Accuracy:0.3941\n",
    "Epoch #249: Loss:0.9951, Accuracy:0.4743, Validation Loss:1.1327, Validation Accuracy:0.3974\n",
    "Epoch #250: Loss:0.9906, Accuracy:0.4702, Validation Loss:1.1309, Validation Accuracy:0.3777\n",
    "Epoch #251: Loss:0.9888, Accuracy:0.4789, Validation Loss:1.1286, Validation Accuracy:0.4023\n",
    "Epoch #252: Loss:0.9960, Accuracy:0.4665, Validation Loss:1.1295, Validation Accuracy:0.3826\n",
    "Epoch #253: Loss:1.0016, Accuracy:0.4690, Validation Loss:1.1411, Validation Accuracy:0.3793\n",
    "Epoch #254: Loss:1.0028, Accuracy:0.4616, Validation Loss:1.1378, Validation Accuracy:0.3744\n",
    "Epoch #255: Loss:1.0007, Accuracy:0.4665, Validation Loss:1.1437, Validation Accuracy:0.3875\n",
    "Epoch #256: Loss:0.9962, Accuracy:0.4665, Validation Loss:1.1428, Validation Accuracy:0.3826\n",
    "Epoch #257: Loss:0.9886, Accuracy:0.4710, Validation Loss:1.1600, Validation Accuracy:0.3941\n",
    "Epoch #258: Loss:0.9876, Accuracy:0.4805, Validation Loss:1.1680, Validation Accuracy:0.3645\n",
    "Epoch #259: Loss:0.9943, Accuracy:0.4612, Validation Loss:1.1669, Validation Accuracy:0.3727\n",
    "Epoch #260: Loss:0.9877, Accuracy:0.4710, Validation Loss:1.1501, Validation Accuracy:0.3727\n",
    "Epoch #261: Loss:0.9844, Accuracy:0.4793, Validation Loss:1.1494, Validation Accuracy:0.3645\n",
    "Epoch #262: Loss:0.9784, Accuracy:0.4830, Validation Loss:1.1518, Validation Accuracy:0.3777\n",
    "Epoch #263: Loss:0.9810, Accuracy:0.4731, Validation Loss:1.1520, Validation Accuracy:0.3842\n",
    "Epoch #264: Loss:0.9836, Accuracy:0.4768, Validation Loss:1.1518, Validation Accuracy:0.3744\n",
    "Epoch #265: Loss:0.9848, Accuracy:0.4764, Validation Loss:1.1633, Validation Accuracy:0.3678\n",
    "Epoch #266: Loss:0.9765, Accuracy:0.4743, Validation Loss:1.1739, Validation Accuracy:0.3744\n",
    "Epoch #267: Loss:0.9849, Accuracy:0.4858, Validation Loss:1.1703, Validation Accuracy:0.3563\n",
    "Epoch #268: Loss:0.9781, Accuracy:0.4743, Validation Loss:1.1777, Validation Accuracy:0.3810\n",
    "Epoch #269: Loss:0.9791, Accuracy:0.4862, Validation Loss:1.1882, Validation Accuracy:0.3727\n",
    "Epoch #270: Loss:0.9864, Accuracy:0.4743, Validation Loss:1.1559, Validation Accuracy:0.3596\n",
    "Epoch #271: Loss:0.9775, Accuracy:0.4797, Validation Loss:1.1569, Validation Accuracy:0.3793\n",
    "Epoch #272: Loss:0.9771, Accuracy:0.4801, Validation Loss:1.1582, Validation Accuracy:0.3793\n",
    "Epoch #273: Loss:0.9749, Accuracy:0.4805, Validation Loss:1.1674, Validation Accuracy:0.3842\n",
    "Epoch #274: Loss:0.9767, Accuracy:0.4895, Validation Loss:1.1815, Validation Accuracy:0.3760\n",
    "Epoch #275: Loss:0.9814, Accuracy:0.4838, Validation Loss:1.1910, Validation Accuracy:0.3875\n",
    "Epoch #276: Loss:0.9825, Accuracy:0.4780, Validation Loss:1.1779, Validation Accuracy:0.3727\n",
    "Epoch #277: Loss:0.9764, Accuracy:0.4801, Validation Loss:1.1659, Validation Accuracy:0.3810\n",
    "Epoch #278: Loss:0.9780, Accuracy:0.4883, Validation Loss:1.1631, Validation Accuracy:0.3695\n",
    "Epoch #279: Loss:0.9722, Accuracy:0.5002, Validation Loss:1.1563, Validation Accuracy:0.3810\n",
    "Epoch #280: Loss:0.9753, Accuracy:0.4842, Validation Loss:1.1549, Validation Accuracy:0.3711\n",
    "Epoch #281: Loss:0.9711, Accuracy:0.4920, Validation Loss:1.1627, Validation Accuracy:0.3842\n",
    "Epoch #282: Loss:0.9739, Accuracy:0.4961, Validation Loss:1.1644, Validation Accuracy:0.3777\n",
    "Epoch #283: Loss:0.9796, Accuracy:0.4867, Validation Loss:1.1668, Validation Accuracy:0.3760\n",
    "Epoch #284: Loss:0.9755, Accuracy:0.4809, Validation Loss:1.1567, Validation Accuracy:0.3547\n",
    "Epoch #285: Loss:0.9804, Accuracy:0.4809, Validation Loss:1.1582, Validation Accuracy:0.3563\n",
    "Epoch #286: Loss:0.9727, Accuracy:0.4887, Validation Loss:1.1564, Validation Accuracy:0.3826\n",
    "Epoch #287: Loss:0.9685, Accuracy:0.4821, Validation Loss:1.1758, Validation Accuracy:0.3793\n",
    "Epoch #288: Loss:0.9671, Accuracy:0.4969, Validation Loss:1.1801, Validation Accuracy:0.3662\n",
    "Epoch #289: Loss:0.9641, Accuracy:0.4862, Validation Loss:1.1739, Validation Accuracy:0.3629\n",
    "Epoch #290: Loss:0.9650, Accuracy:0.4936, Validation Loss:1.1747, Validation Accuracy:0.3777\n",
    "Epoch #291: Loss:0.9622, Accuracy:0.4842, Validation Loss:1.1775, Validation Accuracy:0.3645\n",
    "Epoch #292: Loss:0.9604, Accuracy:0.4858, Validation Loss:1.1762, Validation Accuracy:0.3875\n",
    "Epoch #293: Loss:0.9579, Accuracy:0.4994, Validation Loss:1.1785, Validation Accuracy:0.3777\n",
    "Epoch #294: Loss:0.9626, Accuracy:0.4920, Validation Loss:1.1738, Validation Accuracy:0.3990\n",
    "Epoch #295: Loss:0.9623, Accuracy:0.4850, Validation Loss:1.1594, Validation Accuracy:0.3974\n",
    "Epoch #296: Loss:0.9617, Accuracy:0.5031, Validation Loss:1.1538, Validation Accuracy:0.3760\n",
    "Epoch #297: Loss:0.9596, Accuracy:0.4949, Validation Loss:1.1538, Validation Accuracy:0.3793\n",
    "Epoch #298: Loss:0.9564, Accuracy:0.4998, Validation Loss:1.1701, Validation Accuracy:0.3777\n",
    "Epoch #299: Loss:0.9542, Accuracy:0.4973, Validation Loss:1.1800, Validation Accuracy:0.3711\n",
    "Epoch #300: Loss:0.9553, Accuracy:0.4953, Validation Loss:1.2142, Validation Accuracy:0.3547\n",
    "\n",
    "Test:\n",
    "Test Loss:1.21424747, Accuracy:0.3547\n",
    "Labels: ['03', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03   02   01\n",
    "t:03   0   58   84\n",
    "t:02   0  107  120\n",
    "t:01   0  131  109\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       142\n",
    "          02       0.36      0.47      0.41       227\n",
    "          01       0.35      0.45      0.39       240\n",
    "\n",
    "    accuracy                           0.35       609\n",
    "   macro avg       0.24      0.31      0.27       609\n",
    "weighted avg       0.27      0.35      0.31       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 21:00:12 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 59 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0852278514057154, 1.0766767224263283, 1.0740834040007567, 1.0749990936179075, 1.074989475444424, 1.0749251619348386, 1.0747391760642893, 1.074797710761648, 1.074781962999178, 1.074571474432358, 1.0744673142879468, 1.0743223133149797, 1.074397598385615, 1.074412001764833, 1.0744391446826103, 1.074569007446026, 1.074609051980017, 1.0743962066318407, 1.0746995729374376, 1.074711146025822, 1.0746880209896168, 1.0749097864811838, 1.0748841044155053, 1.0751518435861873, 1.0750682753295147, 1.0751140119602722, 1.0750256226763546, 1.074894042829379, 1.0753580051885645, 1.0749540221319214, 1.0741456598288124, 1.0752783598766733, 1.0748621552252808, 1.0742884502426548, 1.0747373448608348, 1.0751497569342552, 1.074880896922207, 1.0755502488617044, 1.0751744426529983, 1.0736202579022236, 1.0747106425672133, 1.0759319881304537, 1.0758779311219264, 1.0756385279406468, 1.0751226310463766, 1.0751783796914889, 1.0759569680553742, 1.07644693800577, 1.0765866415058254, 1.0756007550385198, 1.0754708911006283, 1.0752522215271623, 1.0759421451925644, 1.0754598549434118, 1.076059590615271, 1.076382119471608, 1.0753544563143125, 1.0761922143754505, 1.0760527150384311, 1.0759258894693284, 1.075727256256567, 1.0757721958097761, 1.075780851108883, 1.0758869391552528, 1.0761111992333323, 1.0756845411604457, 1.0754737752216008, 1.0771224126831456, 1.0774896086143155, 1.0772942402484187, 1.079119910160309, 1.0762938932440747, 1.0768959845228148, 1.0767645054850086, 1.077456421648536, 1.0780122792975264, 1.0786106856585724, 1.07690937785288, 1.075357417950685, 1.0747568969460348, 1.0743825592235197, 1.0756983802040614, 1.0767881827205663, 1.0787372640005277, 1.0771598974472196, 1.07618667750523, 1.0762434557740912, 1.076461662408362, 1.0772333284121234, 1.0789587294135383, 1.0828711689007888, 1.0793057098764505, 1.083651782452375, 1.079531281452461, 1.0861884418183751, 1.0837505779830106, 1.082182692385268, 1.0774567503060026, 1.0777054598374516, 1.0795686158836377, 1.0816948472572665, 1.079901187682191, 1.0802461873917353, 1.0789750634351583, 1.077269367200792, 1.0767757548095753, 1.0761372718122009, 1.0807348478016594, 1.0790563872686552, 1.1110482728735762, 1.101845685288628, 1.0872893932417695, 1.077644193505223, 1.076699512736942, 1.0772151287357599, 1.0769210777846463, 1.0761695814445884, 1.0764582435094272, 1.0791262304058606, 1.0800486924417305, 1.0832468768450232, 1.0874464429658035, 1.090737550912428, 1.0865770689959597, 1.0856751362091215, 1.0814323114056892, 1.0874102362271012, 1.0830083966059443, 1.091152763914788, 1.088366540390478, 1.0810805510221835, 1.082887862703483, 1.0960657831483287, 1.105631977662273, 1.096335802955189, 1.0952341930423855, 1.084481071368814, 1.0808035487611893, 1.0794128189337469, 1.0810159967450672, 1.084892432090684, 1.0822708107567773, 1.0867455134838087, 1.0867865874458025, 1.0865246830706918, 1.0894864543122416, 1.095173365768345, 1.0965130990753424, 1.0917673081600021, 1.0924924228383206, 1.0951086793627058, 1.0926999867647544, 1.0901839161545577, 1.091400180935664, 1.0925571207929714, 1.0859375898473955, 1.0814050436019897, 1.0835592147751982, 1.0843528203776318, 1.0886012161110814, 1.0897965795300864, 1.0918725126091091, 1.094190293932196, 1.0955115331609064, 1.1033190835285656, 1.0893302361170452, 1.0876269041023818, 1.0847442267563543, 1.0902727583946266, 1.0928590605020132, 1.0995701302840009, 1.0969288174937708, 1.0926790217852163, 1.0857486417532358, 1.0859599414913135, 1.0850548127601887, 1.088519372767807, 1.0902180199944131, 1.0925522739272595, 1.0904636159906247, 1.0912645325089128, 1.0968273187114297, 1.109156106101664, 1.1164956995223347, 1.1134828466108475, 1.10822477207591, 1.1052803227858394, 1.11077149552469, 1.1093735211392732, 1.1109962553422048, 1.1166167660495527, 1.1206545318875993, 1.1193031428874225, 1.1033765690275796, 1.1058183237053882, 1.1058959332592968, 1.1324621832429482, 1.1332926965503662, 1.1039310173056591, 1.1010794931248882, 1.104750467442918, 1.117996306450692, 1.1135526748713602, 1.1181537386623315, 1.116869768681393, 1.124255968236375, 1.1307350712260982, 1.1388393131578693, 1.1370412434263182, 1.1373472511278977, 1.1274452152706327, 1.1249268904499623, 1.1325355658585998, 1.1497217757361275, 1.146154420129184, 1.1415883700052898, 1.136130314938149, 1.1569738172740966, 1.2027495643383959, 1.152359806062357, 1.1016447117371708, 1.0882206262626084, 1.0932556598253047, 1.0968933189639514, 1.0880378474938655, 1.090642633305003, 1.09451199830655, 1.099333097390549, 1.1071009698564, 1.1101920452023961, 1.1100255464293882, 1.1083899374274393, 1.1134101534124665, 1.115431971542158, 1.123869864615705, 1.130711224474539, 1.148364820895328, 1.142248019404795, 1.1466736380494091, 1.1407668056159184, 1.1410142210708267, 1.1352678960180047, 1.1363929752841568, 1.1496219022324912, 1.155468499131978, 1.1581863922438598, 1.1544382697451487, 1.1243737814657402, 1.1327333426827868, 1.1309357529203292, 1.1285808145119052, 1.1295402318190275, 1.1411406216754507, 1.1378413998630443, 1.143711556159021, 1.1427599453965236, 1.1600186339348604, 1.1679761188566586, 1.1669143056634612, 1.1500643393872014, 1.1494209447322026, 1.151827019227941, 1.1519612946925297, 1.1517629709541308, 1.1633235474525414, 1.1739137893044107, 1.1703454872657513, 1.1777265377232593, 1.1882343750281874, 1.1559052054322216, 1.156874749656577, 1.1581901092638915, 1.167408165869063, 1.1814814637642972, 1.1910323317610767, 1.1778664563481247, 1.1659264910984508, 1.1631040766908618, 1.1563459668057696, 1.1548738242761647, 1.1627110213481735, 1.1643634911241203, 1.166814011306011, 1.1566879896107565, 1.1582130559755273, 1.15635614618292, 1.1757665285335972, 1.1801106984587921, 1.1738886398634887, 1.1747105250804883, 1.177458736109616, 1.1761755663381617, 1.1784768284639506, 1.1737536207599983, 1.1594259774156392, 1.153755998572301, 1.153771806428781, 1.1700725134566108, 1.1800422408114906, 1.214247446342055], 'val_acc': [0.3694581276383893, 0.3924466327004049, 0.4022988498406653, 0.38916256054868836, 0.37766830757725217, 0.3760262715503304, 0.38916256064656135, 0.4039408857697141, 0.38587848810335296, 0.3842364519785582, 0.38916256054868836, 0.3924466327982779, 0.38587848849484485, 0.38259441585376347, 0.39080459667348316, 0.38916256045081543, 0.3875205243260207, 0.3875205244238936, 0.39080459667348316, 0.3875205244238936, 0.3875205245217666, 0.3842364520764312, 0.38916256054868836, 0.3875205247175126, 0.38587848839697186, 0.3891625608423073, 0.3924466327982779, 0.38423645266366907, 0.3825944165388743, 0.3875205245217666, 0.40229884964491935, 0.3645320192640051, 0.39080459667348316, 0.38259441673462025, 0.3825944169303662, 0.37438423591490055, 0.37110016395893003, 0.37602627135458444, 0.38423645315303395, 0.39737274156415403, 0.3825944159516364, 0.3760262721375683, 0.3793103437999199, 0.3727421957283772, 0.37931034468077673, 0.3743842362085195, 0.3743842353276627, 0.3628899832370833, 0.35960591098749384, 0.3793103438977929, 0.3809523793374768, 0.37110016386105704, 0.37274219910499495, 0.36945812714902443, 0.3678160905348648, 0.35796387407971525, 0.37602627106096553, 0.38095238002258763, 0.3842364524679231, 0.39573070465637544, 0.38916256035294244, 0.37602627106096553, 0.379310343408428, 0.37274219881137605, 0.37766830767512516, 0.38423645178281224, 0.3825944157558905, 0.379310343604174, 0.3760262712567115, 0.3825944157558905, 0.3793103438977929, 0.38587848820122594, 0.3760262712567115, 0.3743842353276627, 0.3678160907306107, 0.3875205245217666, 0.3875205241302747, 0.366174054605816, 0.38916256015719647, 0.36945812685540547, 0.366174054605816, 0.3743842347404248, 0.37931034468077673, 0.3940886687273267, 0.38259441247714565, 0.3760262711588385, 0.37766830767512516, 0.37602627135458444, 0.37766830747937924, 0.3809523801204606, 0.37274219930074093, 0.3727421998879788, 0.3940886686294537, 0.39573070573297825, 0.3908045964777372, 0.3891625615274182, 0.3743842350340438, 0.3793103437020469, 0.36453201848102124, 0.38095237953322275, 0.3809523804140796, 0.379310343408428, 0.3809523803162066, 0.38916256035294244, 0.37274219930074093, 0.38752052540262344, 0.3727421993986139, 0.3809523806098255, 0.3858784882990989, 0.38916256025506946, 0.38259441683249323, 0.39244663250465894, 0.3809523801204606, 0.3842364525657961, 0.38916256035294244, 0.3760262716482034, 0.36945812714902443, 0.38259441585376347, 0.3908045930032464, 0.38095237982684166, 0.38423645217430413, 0.3842364522721771, 0.3875205245217666, 0.3743842358170276, 0.37110016376318405, 0.37602627135458444, 0.3842364523700501, 0.3793103438977929, 0.38752052094940287, 0.38423645217430413, 0.3940886695103105, 0.3858784888863368, 0.37438423513191676, 0.3727421999858518, 0.37274219910499495, 0.38423645276154206, 0.3825944155601445, 0.3628899824540995, 0.3908045964777372, 0.3842364518806852, 0.34975369345574153, 0.3711001628823273, 0.37274219920286794, 0.3694581277362623, 0.37931034458290375, 0.3825944160495094, 0.3546798016343798, 0.37766830767512516, 0.37110016386105704, 0.3678160904369918, 0.3530377661946959, 0.36124794613355876, 0.3793103437020469, 0.37438423571915463, 0.3694581267575325, 0.37110016346956515, 0.3875205246196396, 0.3776683080666171, 0.37438423611064653, 0.38095238002258763, 0.38752052510900453, 0.37602627194182237, 0.38423645217430413, 0.390804597358594, 0.38423645217430413, 0.38587848859271784, 0.3809523803162066, 0.36453201838314825, 0.3760262721375683, 0.37274219910499495, 0.3628899820626076, 0.36781609014337285, 0.39244663260253193, 0.38916256035294244, 0.3678160908284837, 0.3694581267575325, 0.36945812665965955, 0.38423645217430413, 0.3776683081644901, 0.38423645266366907, 0.379310343506301, 0.379310343506301, 0.3694581279320083, 0.36781609180721353, 0.35960591040025597, 0.38423645217430413, 0.3727421993986139, 0.3809523762544779, 0.3793103438977929, 0.37274219930074093, 0.377668308360236, 0.35303776531383907, 0.38916256035294244, 0.38587848472673514, 0.3809523762544779, 0.3678160905348648, 0.35796387359035037, 0.38587848800547997, 0.37274219920286794, 0.37602627252906023, 0.379310343408428, 0.36945812685540547, 0.37274219890924903, 0.3727421997901058, 0.37110016366531107, 0.359605910008764, 0.3743842350340438, 0.38423645158706626, 0.3727421996922329, 0.36781609024124584, 0.3825944157558905, 0.3694581275405163, 0.379310343506301, 0.3743842348382978, 0.37110016346956515, 0.36453201936187807, 0.37110016376318405, 0.38752052393452874, 0.36617405480156195, 0.38259441585376347, 0.35960591128111274, 0.3809523802183336, 0.3957307048521214, 0.37766830400488843, 0.37766830728363326, 0.3711001627844543, 0.37931034419141185, 0.37766830747937924, 0.3596059105960019, 0.3711001629802002, 0.3776683079687441, 0.3678160910242297, 0.36288998264984546, 0.36617405529092684, 0.3760262722354413, 0.3809523792396038, 0.377668308360236, 0.3760262711588385, 0.3875205240324017, 0.3858784882990989, 0.38752052530475045, 0.37766830738150625, 0.3990147775910758, 0.39080459310111937, 0.3825944159516364, 0.379310343408428, 0.3809523762544779, 0.3940886684337077, 0.39737273750242535, 0.37766830757725217, 0.4022988459746826, 0.3825944159516364, 0.3793103447786497, 0.3743842353276627, 0.3875205207536569, 0.3825944170282392, 0.39408866882519966, 0.36453201945975106, 0.37274219920286794, 0.372742199007122, 0.3645320192640051, 0.37766830728363326, 0.3842364485040674, 0.3743842352297897, 0.3678160907306107, 0.37438423611064653, 0.3563218385421584, 0.38095237963109574, 0.3727421995943599, 0.359605910106637, 0.37931034468077673, 0.3793103437999199, 0.3842364520764312, 0.37602627243118725, 0.3875205241302747, 0.3727422000837248, 0.38095237963109574, 0.3694581275405163, 0.38095237953322275, 0.37110016395893003, 0.3842364518806852, 0.37766830767512516, 0.3760262712567115, 0.35467980241736363, 0.35632183756342856, 0.38259441673462025, 0.379310343604174, 0.3661740543121971, 0.36288998294346436, 0.377668308360236, 0.3645320191661321, 0.3875205243260207, 0.377668308360236, 0.3990147771017109, 0.3973727406832972, 0.37602627252906023, 0.3793103437020469, 0.377668308458109, 0.3711001627844543, 0.35467980241736363], 'loss': [1.0969485609438385, 1.0815180606665797, 1.0753076758962392, 1.07389002140053, 1.074388523757825, 1.0741385822178646, 1.0739484758592484, 1.0739455231406116, 1.0737916978232915, 1.0738555775530774, 1.0737562225584623, 1.0733195376836788, 1.0732189184341587, 1.0732142880711957, 1.0732690522313362, 1.073316540022895, 1.0732339219146196, 1.0729469056491734, 1.0731179802324737, 1.0732592792726394, 1.0725905618628437, 1.0724543382010177, 1.072256706088965, 1.0723010956629102, 1.0723502974735393, 1.0723985429661964, 1.0721232499919633, 1.0719715903182294, 1.0717227667998483, 1.0711316350059588, 1.0716835913961673, 1.0724876503190466, 1.0718274341716414, 1.07244304631525, 1.072455247667536, 1.0719175805790957, 1.0721077119545281, 1.0723401569977433, 1.0708556777397955, 1.072796258446617, 1.0721527356141891, 1.0715619834786323, 1.0722266620189502, 1.071053275141628, 1.0710405233214768, 1.0721573658302823, 1.0721121908947673, 1.0710057922946843, 1.0718273511902263, 1.0714765601089602, 1.071848788300579, 1.0710204637515717, 1.0708452136609588, 1.0705346538300877, 1.0711951700324152, 1.0725822116315242, 1.0733991102026719, 1.0732576832879006, 1.074205931305151, 1.0728998004533428, 1.072965347595528, 1.0723439893683369, 1.0720600637322333, 1.0718564672391762, 1.071749930606975, 1.071437222414193, 1.0710600917099438, 1.071635443963554, 1.0708440819315352, 1.071043383756947, 1.0703893935166344, 1.0710076650799667, 1.0699355917544826, 1.0693462429594944, 1.0699805327754246, 1.0706977314038444, 1.0703887995263635, 1.0698381001944415, 1.0681367130984516, 1.0700717229862722, 1.070372475441966, 1.0705676854268726, 1.0719505486302308, 1.0683576767939071, 1.0690356584055467, 1.067616508188189, 1.0655850132388012, 1.0646450974858028, 1.063867658805064, 1.0633128974471984, 1.0610581247223965, 1.0604320350368899, 1.0610212609018879, 1.0663977488844791, 1.0638663116666571, 1.0618342468625956, 1.0632180507667746, 1.0664979333505493, 1.0643759359814058, 1.0639763442887418, 1.0646891162136007, 1.0619277866958838, 1.061957197218705, 1.0611397434309033, 1.060319573433737, 1.0609793711002358, 1.0614415037558553, 1.0608747320253502, 1.0618450778710524, 1.0624638224039724, 1.0858228967909451, 1.07446912230897, 1.0710725880256668, 1.0648336147136022, 1.0615865075857487, 1.0612035347940496, 1.0599043473570744, 1.0594333486635337, 1.0555787728307673, 1.0557686407218478, 1.0554214596993134, 1.0537047227060525, 1.0578686012134904, 1.0586114732636562, 1.054458454012626, 1.0511613300448814, 1.051851265435346, 1.0506729876970609, 1.059336013568745, 1.0610918653574324, 1.0529910086116752, 1.0472766828732814, 1.0478165184937462, 1.058852249004513, 1.073259701670073, 1.068421744468031, 1.0654648911536841, 1.062761023304056, 1.0607248954459627, 1.059722713523332, 1.0562619204393893, 1.0555191516386655, 1.0551018430467014, 1.0529672435666502, 1.0520396753992634, 1.0527801768246128, 1.0543327801526206, 1.051469384963018, 1.0497026689243512, 1.0565343714347855, 1.0595626329739236, 1.0613134461017115, 1.0593891343052138, 1.0584168510515342, 1.0483417282359067, 1.0517943879172542, 1.048260833693236, 1.050958209507764, 1.0490289102100003, 1.0441479914976586, 1.0449893290991656, 1.0428509685293115, 1.0397464032045869, 1.0369324943612979, 1.0355421924003585, 1.0424648358836557, 1.048728170972585, 1.0479444775003672, 1.0464485189018797, 1.0472734643693333, 1.0460709196341356, 1.0592603564017606, 1.0524108411839854, 1.0498338846210582, 1.0475668857719374, 1.0473361865206177, 1.04699914137196, 1.0440656814731857, 1.0421829811111858, 1.043028072507964, 1.0416674229153862, 1.0399953203279624, 1.0386065967763474, 1.040325171404061, 1.0387270201892578, 1.0369001444360313, 1.034401702929816, 1.0317398416433001, 1.028359009989478, 1.028450548379573, 1.0283580665470884, 1.0270238378943848, 1.0263526573318231, 1.0340055375618122, 1.0330204624414934, 1.0321728028311132, 1.0333668857629295, 1.0350589886827881, 1.0327791450449573, 1.0240153123221114, 1.022117072160239, 1.019732058758119, 1.0172298943482385, 1.0138328237455239, 1.0126545777800637, 1.011859882122682, 1.0121987120571567, 1.010796727634798, 1.0084402850521175, 1.003208382908079, 1.0030744622130658, 1.0040196388898688, 1.0043647592806964, 1.004663642059851, 1.0021363863465234, 1.0009538538891676, 1.007835614852592, 1.0071904903075044, 1.0111888866160195, 1.0619865478676203, 1.0310894897586267, 1.042923369691602, 1.0432810672254778, 1.0437039185843184, 1.0451576453704363, 1.0418478255888766, 1.033858041743723, 1.030877148933724, 1.0298212096921227, 1.0282003219612326, 1.027759339775148, 1.0281632341153812, 1.0221915855055228, 1.0203831573286586, 1.0149586107451813, 1.0050628362984628, 0.9989554122243329, 0.9968667019074458, 0.9970285699108054, 0.9965991209664629, 0.9929936755364436, 0.9948493086337064, 0.996805672185377, 0.9970229031858503, 1.0115290841037978, 1.0080324676736914, 1.0224277333067673, 1.003053282418535, 0.9950687059876365, 0.99063308026022, 0.9887833516455774, 0.9960162095220672, 1.0015767886163762, 1.0027593943372644, 1.0007431301982497, 0.9961899511623187, 0.9886488929421505, 0.9876430842176355, 0.9942722826522968, 0.9877163813833828, 0.9843963315109943, 0.9783901978567151, 0.981038095525158, 0.9835935377976733, 0.9848318697245948, 0.9764699580977829, 0.9849482428856209, 0.9781161157991852, 0.9791070694796113, 0.9863798881213524, 0.9775081153523016, 0.9770883429466577, 0.974943296669445, 0.9766806227715353, 0.9813590930472654, 0.9825175263798457, 0.9764069683008371, 0.978010409544136, 0.9722011302041322, 0.9753314061086525, 0.9710902315145645, 0.9738656153913886, 0.9795681528976565, 0.9754985857793194, 0.9803609412308836, 0.9726609642745533, 0.9684685478465024, 0.967091952750815, 0.9641377956715452, 0.9650283112418235, 0.9622115548386466, 0.9603523363812504, 0.9579342199057279, 0.962635646267838, 0.9622783841538478, 0.9616820391198693, 0.9595643932324905, 0.9563929656203033, 0.954242944032015, 0.955280377071741], 'acc': [0.34496919856668745, 0.3733059559759418, 0.38973305772462175, 0.39630390278367783, 0.3975359348545819, 0.3975359354420609, 0.3967145784434841, 0.3938398354719307, 0.40000000173795885, 0.3979466124726517, 0.40000000232543786, 0.4008213563866194, 0.4016427122102381, 0.40205338904500254, 0.39958932333658365, 0.4045174539333986, 0.406981520853493, 0.40246406803385676, 0.40123203322138384, 0.4086242287433123, 0.4049281299848576, 0.4041067757278497, 0.4053388074071011, 0.4086242299549878, 0.40903490538225035, 0.40533880819040646, 0.40698151748772765, 0.4086242316807075, 0.4164271034866388, 0.41930184798808556, 0.40985626061839, 0.3991786447393821, 0.4114989746155435, 0.4036960991256291, 0.3983572873124352, 0.41067761980777406, 0.41396303758239356, 0.4086242309341196, 0.41149897386895556, 0.39835729048237417, 0.4098562622217182, 0.404928133350623, 0.4135523609434555, 0.4151950741940211, 0.406981518271033, 0.39507186797120486, 0.41067761745785786, 0.41232033008422697, 0.4041067739654126, 0.41108829386425216, 0.4086242311299459, 0.40410677772283066, 0.4094455859377154, 0.41067761702948774, 0.41067761863281593, 0.4053388070154484, 0.3983572902865478, 0.4119096491003918, 0.3856262843222099, 0.4045174515834824, 0.39589322199566895, 0.40287474388948946, 0.4086242279600069, 0.4098562649632871, 0.4073921986673892, 0.40985626061839, 0.4090349071446875, 0.40574948799928356, 0.4156057502087626, 0.4082135517127215, 0.42053388244562323, 0.4057494869834344, 0.41437371660796524, 0.4168377838829949, 0.41396303957737446, 0.4127310055114895, 0.4193018460298221, 0.41889117193662656, 0.4283367546064898, 0.41149897148232195, 0.3971252558657276, 0.41108829386425216, 0.3954825446101429, 0.4176591393149609, 0.4098562610100427, 0.4065708408487896, 0.42094455626955757, 0.4336755630539183, 0.4201232026367462, 0.4291581123883719, 0.43121149891211025, 0.43408624028033543, 0.41478439516844934, 0.4119096495287619, 0.40533880998956107, 0.4328542099718685, 0.40903490694886113, 0.4016427122102381, 0.41930184798808556, 0.41519507184410487, 0.4119096526619835, 0.4164271026666159, 0.41765913951078726, 0.4086242315215986, 0.4193018468131275, 0.41478439579264587, 0.4135523617634783, 0.4201232032242252, 0.4102669401947233, 0.40451745573255316, 0.40616016463822163, 0.39630390337115684, 0.41355236133510814, 0.39096509355294384, 0.41560575181209086, 0.4312114967580204, 0.42381929982859007, 0.4332648862191539, 0.4344969198566687, 0.4180698163088342, 0.42792607777172537, 0.4303901414851633, 0.42915811160506656, 0.42669404354673146, 0.43080082188151947, 0.4299794658620744, 0.43613963052477434, 0.43860369482569134, 0.4283367569931234, 0.4065708408487896, 0.43162217594270097, 0.4427104721813476, 0.43162217731348546, 0.41724845774364666, 0.41930184935887, 0.40533880959790836, 0.41930184583399577, 0.4229979461590612, 0.42217659350538156, 0.42176591330485175, 0.43080081969071216, 0.433264887394112, 0.4324435295755124, 0.4238193019826799, 0.42833675441066343, 0.42505133522854205, 0.42792607640094094, 0.43531827548446106, 0.435728954277489, 0.42135523568678196, 0.42053388084229504, 0.4275153983912184, 0.4275154023444628, 0.4303901414851633, 0.4386036956457142, 0.43285421036352123, 0.44024640494303535, 0.43408624004779167, 0.43942504951106937, 0.44435318366947607, 0.433264887981591, 0.4344969177025789, 0.43983572947905536, 0.4386036938465596, 0.4484599602418269, 0.4312114981655223, 0.4390143734228929, 0.44106775998334863, 0.4472279251968102, 0.4394250499027221, 0.44147844034298733, 0.4234086241687837, 0.4283367551939688, 0.4213552366659137, 0.4328542109877177, 0.4353182747011557, 0.432854207426126, 0.4340862436461008, 0.43983573049490454, 0.42874743163708057, 0.4390143744387421, 0.4410677613541331, 0.438603697212325, 0.4287474326162123, 0.44271047198552127, 0.44065708569432677, 0.44476385835015064, 0.4435318266708993, 0.4509240227803068, 0.4381930197900815, 0.44435318190703893, 0.44065708530267406, 0.4525667330934771, 0.44353182686672565, 0.4394250520568119, 0.44065708530267406, 0.4447638622666776, 0.4406570851068477, 0.4513347003983766, 0.4517453811863854, 0.4517453778206201, 0.4652977435618211, 0.4546201217345878, 0.4628336762867914, 0.45420944611149894, 0.46078028678894045, 0.4550308023267703, 0.4685831637230742, 0.4706365488393106, 0.46447638793402873, 0.46858316431055325, 0.4652977433659947, 0.46119096307294327, 0.4689938389545104, 0.47145790466292925, 0.4640657091410008, 0.46242300007622344, 0.4574948674477101, 0.46119096519031566, 0.44722792542935397, 0.46899383836703135, 0.4205338820172531, 0.44845995691277896, 0.43860369623319323, 0.42053388182142676, 0.4414784399513346, 0.4439425066388853, 0.4422997928375581, 0.4439425060514062, 0.4459958915960128, 0.4501026679358198, 0.45462012114710876, 0.46365503289371546, 0.4550307993526576, 0.45379876669427455, 0.4558521561921255, 0.4731006173260158, 0.47145790646208385, 0.467351129889733, 0.47186858306430446, 0.4698151969689363, 0.45872689811111234, 0.4710472294314931, 0.4652977419952103, 0.46119096463955406, 0.4685831611406142, 0.4611909649944893, 0.4706365480560052, 0.47433264877272335, 0.4702258734120481, 0.4788501033547967, 0.4665297720711334, 0.4689938397378158, 0.4616016449991927, 0.46652977622020414, 0.4665297760243778, 0.4710472272406858, 0.4804928136312496, 0.4611909636237049, 0.47104722903984036, 0.479260778427124, 0.482956876793926, 0.47310061713018947, 0.47679671291453146, 0.47638603490480896, 0.4743326493969199, 0.4858316238778328, 0.47433264720611257, 0.4862422975426582, 0.47433264877272335, 0.47967145800345734, 0.4800821354257008, 0.4804928134354233, 0.4895277210329592, 0.48377823637496276, 0.47802874815537455, 0.480082133271611, 0.488295687199618, 0.5002053398860798, 0.4841889125855307, 0.49199178556642004, 0.4960985649170572, 0.48665297715570893, 0.4809034910534931, 0.48090349242427755, 0.4887063657968196, 0.48213552625761874, 0.4969199161999524, 0.4862422995009217, 0.49363449780113644, 0.48418891340555353, 0.4858316214911992, 0.4993839846499402, 0.4919917857622464, 0.48501026547175413, 0.5030800820376105, 0.49486653300526207, 0.49979466403044714, 0.4973305965595911, 0.49527720846924206]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
