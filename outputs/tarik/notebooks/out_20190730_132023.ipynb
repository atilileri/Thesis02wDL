{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf24.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 13:20:23 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '1', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000012C05B34E10>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000012C022E6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0839, Accuracy:0.3729, Validation Loss:1.0765, Validation Accuracy:0.3777\n",
    "Epoch #2: Loss:1.0754, Accuracy:0.3926, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #3: Loss:1.0754, Accuracy:0.3947, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0755, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0743, Accuracy:0.3963, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #7: Loss:1.0740, Accuracy:0.4012, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #8: Loss:1.0739, Accuracy:0.3996, Validation Loss:1.0742, Validation Accuracy:0.3810\n",
    "Epoch #9: Loss:1.0740, Accuracy:0.3930, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0737, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3777\n",
    "Epoch #11: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #12: Loss:1.0738, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #13: Loss:1.0737, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.3810\n",
    "Epoch #14: Loss:1.0740, Accuracy:0.3926, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #18: Loss:1.0738, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.4008, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #21: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0736, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #23: Loss:1.0735, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #24: Loss:1.0733, Accuracy:0.3992, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #25: Loss:1.0733, Accuracy:0.3996, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #26: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #27: Loss:1.0733, Accuracy:0.3971, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #28: Loss:1.0735, Accuracy:0.3996, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #29: Loss:1.0736, Accuracy:0.3996, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #30: Loss:1.0739, Accuracy:0.3988, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #31: Loss:1.0738, Accuracy:0.3992, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #32: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0749, Validation Accuracy:0.3810\n",
    "Epoch #33: Loss:1.0738, Accuracy:0.3992, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #34: Loss:1.0737, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #35: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #36: Loss:1.0738, Accuracy:0.3984, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #37: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0750, Validation Accuracy:0.3859\n",
    "Epoch #38: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0751, Validation Accuracy:0.3793\n",
    "Epoch #39: Loss:1.0738, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #40: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #41: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #42: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #43: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0751, Validation Accuracy:0.3777\n",
    "Epoch #44: Loss:1.0737, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.3777\n",
    "Epoch #45: Loss:1.0734, Accuracy:0.3971, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #46: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #47: Loss:1.0736, Accuracy:0.4016, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #48: Loss:1.0733, Accuracy:0.4000, Validation Loss:1.0753, Validation Accuracy:0.3892\n",
    "Epoch #49: Loss:1.0738, Accuracy:0.4008, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #50: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3810\n",
    "Epoch #51: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0754, Validation Accuracy:0.3810\n",
    "Epoch #52: Loss:1.0735, Accuracy:0.3971, Validation Loss:1.0754, Validation Accuracy:0.3826\n",
    "Epoch #53: Loss:1.0734, Accuracy:0.3988, Validation Loss:1.0755, Validation Accuracy:0.3875\n",
    "Epoch #54: Loss:1.0734, Accuracy:0.4008, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #55: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0755, Validation Accuracy:0.3711\n",
    "Epoch #56: Loss:1.0734, Accuracy:0.3979, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #57: Loss:1.0733, Accuracy:0.4016, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #58: Loss:1.0733, Accuracy:0.3959, Validation Loss:1.0756, Validation Accuracy:0.4023\n",
    "Epoch #59: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #60: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #61: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #62: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0758, Validation Accuracy:0.3777\n",
    "Epoch #63: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0758, Validation Accuracy:0.3892\n",
    "Epoch #64: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0758, Validation Accuracy:0.3892\n",
    "Epoch #65: Loss:1.0732, Accuracy:0.4004, Validation Loss:1.0759, Validation Accuracy:0.3892\n",
    "Epoch #66: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0759, Validation Accuracy:0.3859\n",
    "Epoch #67: Loss:1.0728, Accuracy:0.3947, Validation Loss:1.0759, Validation Accuracy:0.3859\n",
    "Epoch #68: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0761, Validation Accuracy:0.3777\n",
    "Epoch #69: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0763, Validation Accuracy:0.3892\n",
    "Epoch #70: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0763, Validation Accuracy:0.3892\n",
    "Epoch #71: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0765, Validation Accuracy:0.3892\n",
    "Epoch #72: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0764, Validation Accuracy:0.3875\n",
    "Epoch #73: Loss:1.0726, Accuracy:0.4004, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #74: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0767, Validation Accuracy:0.3760\n",
    "Epoch #75: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0768, Validation Accuracy:0.3875\n",
    "Epoch #76: Loss:1.0731, Accuracy:0.3885, Validation Loss:1.0777, Validation Accuracy:0.3908\n",
    "Epoch #77: Loss:1.0728, Accuracy:0.3901, Validation Loss:1.0771, Validation Accuracy:0.3777\n",
    "Epoch #78: Loss:1.0722, Accuracy:0.3996, Validation Loss:1.0770, Validation Accuracy:0.3777\n",
    "Epoch #79: Loss:1.0718, Accuracy:0.4021, Validation Loss:1.0774, Validation Accuracy:0.3859\n",
    "Epoch #80: Loss:1.0726, Accuracy:0.3996, Validation Loss:1.0772, Validation Accuracy:0.3941\n",
    "Epoch #81: Loss:1.0718, Accuracy:0.4016, Validation Loss:1.0774, Validation Accuracy:0.3875\n",
    "Epoch #82: Loss:1.0720, Accuracy:0.4041, Validation Loss:1.0779, Validation Accuracy:0.3859\n",
    "Epoch #83: Loss:1.0717, Accuracy:0.4012, Validation Loss:1.0777, Validation Accuracy:0.3908\n",
    "Epoch #84: Loss:1.0718, Accuracy:0.3979, Validation Loss:1.0779, Validation Accuracy:0.3777\n",
    "Epoch #85: Loss:1.0715, Accuracy:0.4004, Validation Loss:1.0781, Validation Accuracy:0.3908\n",
    "Epoch #86: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0783, Validation Accuracy:0.3924\n",
    "Epoch #87: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0784, Validation Accuracy:0.3924\n",
    "Epoch #88: Loss:1.0713, Accuracy:0.4029, Validation Loss:1.0786, Validation Accuracy:0.3859\n",
    "Epoch #89: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0787, Validation Accuracy:0.3875\n",
    "Epoch #90: Loss:1.0709, Accuracy:0.3992, Validation Loss:1.0790, Validation Accuracy:0.3842\n",
    "Epoch #91: Loss:1.0710, Accuracy:0.4025, Validation Loss:1.0792, Validation Accuracy:0.3875\n",
    "Epoch #92: Loss:1.0713, Accuracy:0.3984, Validation Loss:1.0796, Validation Accuracy:0.3826\n",
    "Epoch #93: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0796, Validation Accuracy:0.3875\n",
    "Epoch #94: Loss:1.0709, Accuracy:0.4021, Validation Loss:1.0798, Validation Accuracy:0.3875\n",
    "Epoch #95: Loss:1.0708, Accuracy:0.4025, Validation Loss:1.0803, Validation Accuracy:0.3859\n",
    "Epoch #96: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0800, Validation Accuracy:0.3826\n",
    "Epoch #97: Loss:1.0706, Accuracy:0.4012, Validation Loss:1.0800, Validation Accuracy:0.3826\n",
    "Epoch #98: Loss:1.0708, Accuracy:0.4025, Validation Loss:1.0798, Validation Accuracy:0.3777\n",
    "Epoch #99: Loss:1.0705, Accuracy:0.3996, Validation Loss:1.0800, Validation Accuracy:0.3842\n",
    "Epoch #100: Loss:1.0706, Accuracy:0.4057, Validation Loss:1.0805, Validation Accuracy:0.3711\n",
    "Epoch #101: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0802, Validation Accuracy:0.3793\n",
    "Epoch #102: Loss:1.0703, Accuracy:0.4041, Validation Loss:1.0804, Validation Accuracy:0.3842\n",
    "Epoch #103: Loss:1.0707, Accuracy:0.4029, Validation Loss:1.0807, Validation Accuracy:0.3727\n",
    "Epoch #104: Loss:1.0701, Accuracy:0.4066, Validation Loss:1.0804, Validation Accuracy:0.3826\n",
    "Epoch #105: Loss:1.0710, Accuracy:0.4021, Validation Loss:1.0805, Validation Accuracy:0.3793\n",
    "Epoch #106: Loss:1.0708, Accuracy:0.3967, Validation Loss:1.0823, Validation Accuracy:0.3678\n",
    "Epoch #107: Loss:1.0711, Accuracy:0.3951, Validation Loss:1.0805, Validation Accuracy:0.3760\n",
    "Epoch #108: Loss:1.0705, Accuracy:0.4012, Validation Loss:1.0805, Validation Accuracy:0.3760\n",
    "Epoch #109: Loss:1.0701, Accuracy:0.4045, Validation Loss:1.0808, Validation Accuracy:0.3810\n",
    "Epoch #110: Loss:1.0699, Accuracy:0.4037, Validation Loss:1.0812, Validation Accuracy:0.3826\n",
    "Epoch #111: Loss:1.0699, Accuracy:0.4041, Validation Loss:1.0816, Validation Accuracy:0.3859\n",
    "Epoch #112: Loss:1.0701, Accuracy:0.4021, Validation Loss:1.0814, Validation Accuracy:0.3810\n",
    "Epoch #113: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0813, Validation Accuracy:0.3777\n",
    "Epoch #114: Loss:1.0696, Accuracy:0.4037, Validation Loss:1.0813, Validation Accuracy:0.3777\n",
    "Epoch #115: Loss:1.0701, Accuracy:0.4025, Validation Loss:1.0816, Validation Accuracy:0.3842\n",
    "Epoch #116: Loss:1.0700, Accuracy:0.4008, Validation Loss:1.0815, Validation Accuracy:0.3810\n",
    "Epoch #117: Loss:1.0696, Accuracy:0.4021, Validation Loss:1.0815, Validation Accuracy:0.3760\n",
    "Epoch #118: Loss:1.0697, Accuracy:0.4029, Validation Loss:1.0817, Validation Accuracy:0.3810\n",
    "Epoch #119: Loss:1.0709, Accuracy:0.3967, Validation Loss:1.0818, Validation Accuracy:0.3859\n",
    "Epoch #120: Loss:1.0711, Accuracy:0.4078, Validation Loss:1.0820, Validation Accuracy:0.3777\n",
    "Epoch #121: Loss:1.0695, Accuracy:0.4037, Validation Loss:1.0824, Validation Accuracy:0.3859\n",
    "Epoch #122: Loss:1.0702, Accuracy:0.4029, Validation Loss:1.0817, Validation Accuracy:0.3810\n",
    "Epoch #123: Loss:1.0698, Accuracy:0.3996, Validation Loss:1.0819, Validation Accuracy:0.3777\n",
    "Epoch #124: Loss:1.0697, Accuracy:0.4029, Validation Loss:1.0818, Validation Accuracy:0.3777\n",
    "Epoch #125: Loss:1.0700, Accuracy:0.4037, Validation Loss:1.0816, Validation Accuracy:0.3777\n",
    "Epoch #126: Loss:1.0694, Accuracy:0.4041, Validation Loss:1.0816, Validation Accuracy:0.3760\n",
    "Epoch #127: Loss:1.0695, Accuracy:0.4037, Validation Loss:1.0821, Validation Accuracy:0.3859\n",
    "Epoch #128: Loss:1.0696, Accuracy:0.3984, Validation Loss:1.0825, Validation Accuracy:0.3859\n",
    "Epoch #129: Loss:1.0694, Accuracy:0.4029, Validation Loss:1.0823, Validation Accuracy:0.3760\n",
    "Epoch #130: Loss:1.0697, Accuracy:0.4021, Validation Loss:1.0829, Validation Accuracy:0.3760\n",
    "Epoch #131: Loss:1.0693, Accuracy:0.4041, Validation Loss:1.0826, Validation Accuracy:0.3810\n",
    "Epoch #132: Loss:1.0690, Accuracy:0.4033, Validation Loss:1.0831, Validation Accuracy:0.3842\n",
    "Epoch #133: Loss:1.0688, Accuracy:0.4041, Validation Loss:1.0828, Validation Accuracy:0.3760\n",
    "Epoch #134: Loss:1.0693, Accuracy:0.4033, Validation Loss:1.0827, Validation Accuracy:0.3777\n",
    "Epoch #135: Loss:1.0686, Accuracy:0.4041, Validation Loss:1.0835, Validation Accuracy:0.3859\n",
    "Epoch #136: Loss:1.0685, Accuracy:0.4045, Validation Loss:1.0831, Validation Accuracy:0.3760\n",
    "Epoch #137: Loss:1.0686, Accuracy:0.4029, Validation Loss:1.0833, Validation Accuracy:0.3760\n",
    "Epoch #138: Loss:1.0686, Accuracy:0.4012, Validation Loss:1.0841, Validation Accuracy:0.3810\n",
    "Epoch #139: Loss:1.0691, Accuracy:0.4000, Validation Loss:1.0837, Validation Accuracy:0.3810\n",
    "Epoch #140: Loss:1.0689, Accuracy:0.4066, Validation Loss:1.0847, Validation Accuracy:0.3875\n",
    "Epoch #141: Loss:1.0696, Accuracy:0.4045, Validation Loss:1.0832, Validation Accuracy:0.3777\n",
    "Epoch #142: Loss:1.0690, Accuracy:0.4016, Validation Loss:1.0836, Validation Accuracy:0.3760\n",
    "Epoch #143: Loss:1.0687, Accuracy:0.4025, Validation Loss:1.0828, Validation Accuracy:0.3777\n",
    "Epoch #144: Loss:1.0685, Accuracy:0.4053, Validation Loss:1.0833, Validation Accuracy:0.3760\n",
    "Epoch #145: Loss:1.0707, Accuracy:0.3926, Validation Loss:1.0837, Validation Accuracy:0.3842\n",
    "Epoch #146: Loss:1.0685, Accuracy:0.4041, Validation Loss:1.0843, Validation Accuracy:0.3760\n",
    "Epoch #147: Loss:1.0694, Accuracy:0.4004, Validation Loss:1.0839, Validation Accuracy:0.3842\n",
    "Epoch #148: Loss:1.0682, Accuracy:0.4070, Validation Loss:1.0838, Validation Accuracy:0.3842\n",
    "Epoch #149: Loss:1.0682, Accuracy:0.4066, Validation Loss:1.0838, Validation Accuracy:0.3842\n",
    "Epoch #150: Loss:1.0682, Accuracy:0.4012, Validation Loss:1.0838, Validation Accuracy:0.3760\n",
    "Epoch #151: Loss:1.0677, Accuracy:0.4008, Validation Loss:1.0847, Validation Accuracy:0.3842\n",
    "Epoch #152: Loss:1.0680, Accuracy:0.4057, Validation Loss:1.0849, Validation Accuracy:0.3760\n",
    "Epoch #153: Loss:1.0679, Accuracy:0.4037, Validation Loss:1.0853, Validation Accuracy:0.3842\n",
    "Epoch #154: Loss:1.0674, Accuracy:0.4057, Validation Loss:1.0853, Validation Accuracy:0.3760\n",
    "Epoch #155: Loss:1.0676, Accuracy:0.4053, Validation Loss:1.0851, Validation Accuracy:0.3744\n",
    "Epoch #156: Loss:1.0674, Accuracy:0.4029, Validation Loss:1.0849, Validation Accuracy:0.3760\n",
    "Epoch #157: Loss:1.0675, Accuracy:0.4033, Validation Loss:1.0848, Validation Accuracy:0.3760\n",
    "Epoch #158: Loss:1.0672, Accuracy:0.4008, Validation Loss:1.0859, Validation Accuracy:0.3842\n",
    "Epoch #159: Loss:1.0680, Accuracy:0.4070, Validation Loss:1.0851, Validation Accuracy:0.3842\n",
    "Epoch #160: Loss:1.0670, Accuracy:0.4062, Validation Loss:1.0849, Validation Accuracy:0.3760\n",
    "Epoch #161: Loss:1.0675, Accuracy:0.4037, Validation Loss:1.0856, Validation Accuracy:0.3744\n",
    "Epoch #162: Loss:1.0671, Accuracy:0.4086, Validation Loss:1.0854, Validation Accuracy:0.3645\n",
    "Epoch #163: Loss:1.0677, Accuracy:0.3951, Validation Loss:1.0857, Validation Accuracy:0.3760\n",
    "Epoch #164: Loss:1.0672, Accuracy:0.4008, Validation Loss:1.0853, Validation Accuracy:0.3842\n",
    "Epoch #165: Loss:1.0680, Accuracy:0.4025, Validation Loss:1.0851, Validation Accuracy:0.3793\n",
    "Epoch #166: Loss:1.0675, Accuracy:0.4029, Validation Loss:1.0850, Validation Accuracy:0.3645\n",
    "Epoch #167: Loss:1.0674, Accuracy:0.4012, Validation Loss:1.0865, Validation Accuracy:0.3612\n",
    "Epoch #168: Loss:1.0678, Accuracy:0.3992, Validation Loss:1.0852, Validation Accuracy:0.3760\n",
    "Epoch #169: Loss:1.0665, Accuracy:0.4033, Validation Loss:1.0862, Validation Accuracy:0.3859\n",
    "Epoch #170: Loss:1.0685, Accuracy:0.4049, Validation Loss:1.0849, Validation Accuracy:0.3793\n",
    "Epoch #171: Loss:1.0676, Accuracy:0.4049, Validation Loss:1.0856, Validation Accuracy:0.3498\n",
    "Epoch #172: Loss:1.0673, Accuracy:0.4086, Validation Loss:1.0850, Validation Accuracy:0.3842\n",
    "Epoch #173: Loss:1.0694, Accuracy:0.4062, Validation Loss:1.0858, Validation Accuracy:0.3859\n",
    "Epoch #174: Loss:1.0667, Accuracy:0.4049, Validation Loss:1.0846, Validation Accuracy:0.3793\n",
    "Epoch #175: Loss:1.0680, Accuracy:0.4041, Validation Loss:1.0840, Validation Accuracy:0.3432\n",
    "Epoch #176: Loss:1.0677, Accuracy:0.4004, Validation Loss:1.0860, Validation Accuracy:0.3777\n",
    "Epoch #177: Loss:1.0672, Accuracy:0.4086, Validation Loss:1.0849, Validation Accuracy:0.3777\n",
    "Epoch #178: Loss:1.0673, Accuracy:0.4029, Validation Loss:1.0841, Validation Accuracy:0.3744\n",
    "Epoch #179: Loss:1.0665, Accuracy:0.4033, Validation Loss:1.0846, Validation Accuracy:0.3645\n",
    "Epoch #180: Loss:1.0669, Accuracy:0.3910, Validation Loss:1.0846, Validation Accuracy:0.3629\n",
    "Epoch #181: Loss:1.0669, Accuracy:0.4070, Validation Loss:1.0853, Validation Accuracy:0.3727\n",
    "Epoch #182: Loss:1.0674, Accuracy:0.4041, Validation Loss:1.0860, Validation Accuracy:0.3793\n",
    "Epoch #183: Loss:1.0671, Accuracy:0.4086, Validation Loss:1.0868, Validation Accuracy:0.3892\n",
    "Epoch #184: Loss:1.0664, Accuracy:0.4070, Validation Loss:1.0849, Validation Accuracy:0.3744\n",
    "Epoch #185: Loss:1.0667, Accuracy:0.3988, Validation Loss:1.0839, Validation Accuracy:0.3662\n",
    "Epoch #186: Loss:1.0667, Accuracy:0.4066, Validation Loss:1.0840, Validation Accuracy:0.3645\n",
    "Epoch #187: Loss:1.0664, Accuracy:0.4066, Validation Loss:1.0862, Validation Accuracy:0.3859\n",
    "Epoch #188: Loss:1.0667, Accuracy:0.4029, Validation Loss:1.0852, Validation Accuracy:0.3793\n",
    "Epoch #189: Loss:1.0667, Accuracy:0.4049, Validation Loss:1.0855, Validation Accuracy:0.3793\n",
    "Epoch #190: Loss:1.0659, Accuracy:0.4053, Validation Loss:1.0858, Validation Accuracy:0.3760\n",
    "Epoch #191: Loss:1.0666, Accuracy:0.4066, Validation Loss:1.0854, Validation Accuracy:0.3711\n",
    "Epoch #192: Loss:1.0659, Accuracy:0.4070, Validation Loss:1.0852, Validation Accuracy:0.3711\n",
    "Epoch #193: Loss:1.0667, Accuracy:0.4029, Validation Loss:1.0851, Validation Accuracy:0.3793\n",
    "Epoch #194: Loss:1.0664, Accuracy:0.4029, Validation Loss:1.0849, Validation Accuracy:0.3711\n",
    "Epoch #195: Loss:1.0668, Accuracy:0.4078, Validation Loss:1.0865, Validation Accuracy:0.3875\n",
    "Epoch #196: Loss:1.0659, Accuracy:0.4057, Validation Loss:1.0851, Validation Accuracy:0.3744\n",
    "Epoch #197: Loss:1.0673, Accuracy:0.4053, Validation Loss:1.0853, Validation Accuracy:0.3711\n",
    "Epoch #198: Loss:1.0656, Accuracy:0.4037, Validation Loss:1.0868, Validation Accuracy:0.3612\n",
    "Epoch #199: Loss:1.0671, Accuracy:0.4037, Validation Loss:1.0861, Validation Accuracy:0.3859\n",
    "Epoch #200: Loss:1.0658, Accuracy:0.4082, Validation Loss:1.0854, Validation Accuracy:0.3744\n",
    "Epoch #201: Loss:1.0662, Accuracy:0.4004, Validation Loss:1.0854, Validation Accuracy:0.3612\n",
    "Epoch #202: Loss:1.0666, Accuracy:0.4004, Validation Loss:1.0861, Validation Accuracy:0.3826\n",
    "Epoch #203: Loss:1.0658, Accuracy:0.4066, Validation Loss:1.0853, Validation Accuracy:0.3744\n",
    "Epoch #204: Loss:1.0666, Accuracy:0.4016, Validation Loss:1.0853, Validation Accuracy:0.3662\n",
    "Epoch #205: Loss:1.0665, Accuracy:0.4062, Validation Loss:1.0867, Validation Accuracy:0.3892\n",
    "Epoch #206: Loss:1.0660, Accuracy:0.4062, Validation Loss:1.0850, Validation Accuracy:0.3760\n",
    "Epoch #207: Loss:1.0663, Accuracy:0.4049, Validation Loss:1.0850, Validation Accuracy:0.3711\n",
    "Epoch #208: Loss:1.0665, Accuracy:0.4012, Validation Loss:1.0859, Validation Accuracy:0.3514\n",
    "Epoch #209: Loss:1.0666, Accuracy:0.3893, Validation Loss:1.0858, Validation Accuracy:0.3744\n",
    "Epoch #210: Loss:1.0656, Accuracy:0.4062, Validation Loss:1.0858, Validation Accuracy:0.3810\n",
    "Epoch #211: Loss:1.0658, Accuracy:0.4082, Validation Loss:1.0854, Validation Accuracy:0.3662\n",
    "Epoch #212: Loss:1.0659, Accuracy:0.4045, Validation Loss:1.0851, Validation Accuracy:0.3695\n",
    "Epoch #213: Loss:1.0661, Accuracy:0.4021, Validation Loss:1.0858, Validation Accuracy:0.3629\n",
    "Epoch #214: Loss:1.0653, Accuracy:0.4094, Validation Loss:1.0865, Validation Accuracy:0.3859\n",
    "Epoch #215: Loss:1.0663, Accuracy:0.4041, Validation Loss:1.0862, Validation Accuracy:0.3793\n",
    "Epoch #216: Loss:1.0654, Accuracy:0.4074, Validation Loss:1.0856, Validation Accuracy:0.3415\n",
    "Epoch #217: Loss:1.0661, Accuracy:0.3992, Validation Loss:1.0858, Validation Accuracy:0.3563\n",
    "Epoch #218: Loss:1.0656, Accuracy:0.4094, Validation Loss:1.0855, Validation Accuracy:0.3711\n",
    "Epoch #219: Loss:1.0661, Accuracy:0.4053, Validation Loss:1.0862, Validation Accuracy:0.3760\n",
    "Epoch #220: Loss:1.0659, Accuracy:0.4045, Validation Loss:1.0862, Validation Accuracy:0.3547\n",
    "Epoch #221: Loss:1.0658, Accuracy:0.4021, Validation Loss:1.0860, Validation Accuracy:0.3875\n",
    "Epoch #222: Loss:1.0654, Accuracy:0.4070, Validation Loss:1.0862, Validation Accuracy:0.3875\n",
    "Epoch #223: Loss:1.0654, Accuracy:0.4094, Validation Loss:1.0858, Validation Accuracy:0.3826\n",
    "Epoch #224: Loss:1.0653, Accuracy:0.4049, Validation Loss:1.0855, Validation Accuracy:0.3727\n",
    "Epoch #225: Loss:1.0657, Accuracy:0.4049, Validation Loss:1.0861, Validation Accuracy:0.3826\n",
    "Epoch #226: Loss:1.0657, Accuracy:0.3971, Validation Loss:1.0864, Validation Accuracy:0.3514\n",
    "Epoch #227: Loss:1.0656, Accuracy:0.4053, Validation Loss:1.0861, Validation Accuracy:0.3727\n",
    "Epoch #228: Loss:1.0663, Accuracy:0.4037, Validation Loss:1.0877, Validation Accuracy:0.3892\n",
    "Epoch #229: Loss:1.0658, Accuracy:0.4066, Validation Loss:1.0853, Validation Accuracy:0.3498\n",
    "Epoch #230: Loss:1.0656, Accuracy:0.3984, Validation Loss:1.0856, Validation Accuracy:0.3514\n",
    "Epoch #231: Loss:1.0658, Accuracy:0.4045, Validation Loss:1.0858, Validation Accuracy:0.3875\n",
    "Epoch #232: Loss:1.0652, Accuracy:0.4094, Validation Loss:1.0853, Validation Accuracy:0.3711\n",
    "Epoch #233: Loss:1.0653, Accuracy:0.4074, Validation Loss:1.0848, Validation Accuracy:0.3530\n",
    "Epoch #234: Loss:1.0660, Accuracy:0.4012, Validation Loss:1.0862, Validation Accuracy:0.3645\n",
    "Epoch #235: Loss:1.0660, Accuracy:0.4045, Validation Loss:1.0857, Validation Accuracy:0.3629\n",
    "Epoch #236: Loss:1.0661, Accuracy:0.4041, Validation Loss:1.0864, Validation Accuracy:0.3875\n",
    "Epoch #237: Loss:1.0655, Accuracy:0.4021, Validation Loss:1.0850, Validation Accuracy:0.3727\n",
    "Epoch #238: Loss:1.0656, Accuracy:0.4074, Validation Loss:1.0855, Validation Accuracy:0.3744\n",
    "Epoch #239: Loss:1.0651, Accuracy:0.4148, Validation Loss:1.0862, Validation Accuracy:0.3612\n",
    "Epoch #240: Loss:1.0659, Accuracy:0.4012, Validation Loss:1.0858, Validation Accuracy:0.3399\n",
    "Epoch #241: Loss:1.0651, Accuracy:0.4082, Validation Loss:1.0857, Validation Accuracy:0.3727\n",
    "Epoch #242: Loss:1.0648, Accuracy:0.4037, Validation Loss:1.0866, Validation Accuracy:0.3826\n",
    "Epoch #243: Loss:1.0664, Accuracy:0.3922, Validation Loss:1.0852, Validation Accuracy:0.3563\n",
    "Epoch #244: Loss:1.0651, Accuracy:0.4037, Validation Loss:1.0864, Validation Accuracy:0.3695\n",
    "Epoch #245: Loss:1.0658, Accuracy:0.4029, Validation Loss:1.0879, Validation Accuracy:0.3777\n",
    "Epoch #246: Loss:1.0653, Accuracy:0.4053, Validation Loss:1.0860, Validation Accuracy:0.3530\n",
    "Epoch #247: Loss:1.0650, Accuracy:0.4066, Validation Loss:1.0854, Validation Accuracy:0.3760\n",
    "Epoch #248: Loss:1.0652, Accuracy:0.4066, Validation Loss:1.0854, Validation Accuracy:0.3612\n",
    "Epoch #249: Loss:1.0652, Accuracy:0.3955, Validation Loss:1.0856, Validation Accuracy:0.3498\n",
    "Epoch #250: Loss:1.0651, Accuracy:0.4082, Validation Loss:1.0864, Validation Accuracy:0.3744\n",
    "Epoch #251: Loss:1.0659, Accuracy:0.4045, Validation Loss:1.0871, Validation Accuracy:0.3744\n",
    "Epoch #252: Loss:1.0657, Accuracy:0.4029, Validation Loss:1.0860, Validation Accuracy:0.3711\n",
    "Epoch #253: Loss:1.0648, Accuracy:0.4070, Validation Loss:1.0864, Validation Accuracy:0.3645\n",
    "Epoch #254: Loss:1.0661, Accuracy:0.4033, Validation Loss:1.0856, Validation Accuracy:0.3350\n",
    "Epoch #255: Loss:1.0661, Accuracy:0.3951, Validation Loss:1.0857, Validation Accuracy:0.3875\n",
    "Epoch #256: Loss:1.0656, Accuracy:0.4057, Validation Loss:1.0855, Validation Accuracy:0.3695\n",
    "Epoch #257: Loss:1.0656, Accuracy:0.3979, Validation Loss:1.0856, Validation Accuracy:0.3662\n",
    "Epoch #258: Loss:1.0651, Accuracy:0.4062, Validation Loss:1.0855, Validation Accuracy:0.3629\n",
    "Epoch #259: Loss:1.0658, Accuracy:0.4066, Validation Loss:1.0850, Validation Accuracy:0.3777\n",
    "Epoch #260: Loss:1.0667, Accuracy:0.4025, Validation Loss:1.0875, Validation Accuracy:0.3777\n",
    "Epoch #261: Loss:1.0647, Accuracy:0.4033, Validation Loss:1.0850, Validation Accuracy:0.3399\n",
    "Epoch #262: Loss:1.0658, Accuracy:0.4078, Validation Loss:1.0850, Validation Accuracy:0.3629\n",
    "Epoch #263: Loss:1.0650, Accuracy:0.4078, Validation Loss:1.0862, Validation Accuracy:0.3645\n",
    "Epoch #264: Loss:1.0651, Accuracy:0.4057, Validation Loss:1.0850, Validation Accuracy:0.3481\n",
    "Epoch #265: Loss:1.0651, Accuracy:0.4004, Validation Loss:1.0852, Validation Accuracy:0.3777\n",
    "Epoch #266: Loss:1.0649, Accuracy:0.4090, Validation Loss:1.0865, Validation Accuracy:0.3777\n",
    "Epoch #267: Loss:1.0645, Accuracy:0.4078, Validation Loss:1.0859, Validation Accuracy:0.3793\n",
    "Epoch #268: Loss:1.0651, Accuracy:0.4074, Validation Loss:1.0852, Validation Accuracy:0.3645\n",
    "Epoch #269: Loss:1.0654, Accuracy:0.4066, Validation Loss:1.0871, Validation Accuracy:0.3793\n",
    "Epoch #270: Loss:1.0652, Accuracy:0.4029, Validation Loss:1.0877, Validation Accuracy:0.3695\n",
    "Epoch #271: Loss:1.0658, Accuracy:0.4045, Validation Loss:1.0856, Validation Accuracy:0.3514\n",
    "Epoch #272: Loss:1.0650, Accuracy:0.4000, Validation Loss:1.0852, Validation Accuracy:0.3547\n",
    "Epoch #273: Loss:1.0659, Accuracy:0.3988, Validation Loss:1.0868, Validation Accuracy:0.3793\n",
    "Epoch #274: Loss:1.0664, Accuracy:0.3893, Validation Loss:1.0860, Validation Accuracy:0.3481\n",
    "Epoch #275: Loss:1.0651, Accuracy:0.3988, Validation Loss:1.0859, Validation Accuracy:0.3760\n",
    "Epoch #276: Loss:1.0646, Accuracy:0.4057, Validation Loss:1.0855, Validation Accuracy:0.3481\n",
    "Epoch #277: Loss:1.0650, Accuracy:0.4025, Validation Loss:1.0865, Validation Accuracy:0.3711\n",
    "Epoch #278: Loss:1.0647, Accuracy:0.4053, Validation Loss:1.0851, Validation Accuracy:0.3727\n",
    "Epoch #279: Loss:1.0646, Accuracy:0.4029, Validation Loss:1.0850, Validation Accuracy:0.3498\n",
    "Epoch #280: Loss:1.0651, Accuracy:0.4074, Validation Loss:1.0850, Validation Accuracy:0.3662\n",
    "Epoch #281: Loss:1.0644, Accuracy:0.4041, Validation Loss:1.0852, Validation Accuracy:0.3645\n",
    "Epoch #282: Loss:1.0646, Accuracy:0.4094, Validation Loss:1.0859, Validation Accuracy:0.3596\n",
    "Epoch #283: Loss:1.0645, Accuracy:0.4045, Validation Loss:1.0874, Validation Accuracy:0.3908\n",
    "Epoch #284: Loss:1.0642, Accuracy:0.4049, Validation Loss:1.0861, Validation Accuracy:0.3727\n",
    "Epoch #285: Loss:1.0644, Accuracy:0.4029, Validation Loss:1.0849, Validation Accuracy:0.3481\n",
    "Epoch #286: Loss:1.0640, Accuracy:0.4070, Validation Loss:1.0860, Validation Accuracy:0.3563\n",
    "Epoch #287: Loss:1.0642, Accuracy:0.3984, Validation Loss:1.0868, Validation Accuracy:0.3826\n",
    "Epoch #288: Loss:1.0642, Accuracy:0.4066, Validation Loss:1.0868, Validation Accuracy:0.3695\n",
    "Epoch #289: Loss:1.0642, Accuracy:0.4029, Validation Loss:1.0857, Validation Accuracy:0.3695\n",
    "Epoch #290: Loss:1.0643, Accuracy:0.4099, Validation Loss:1.0854, Validation Accuracy:0.3645\n",
    "Epoch #291: Loss:1.0642, Accuracy:0.4111, Validation Loss:1.0857, Validation Accuracy:0.3612\n",
    "Epoch #292: Loss:1.0643, Accuracy:0.4045, Validation Loss:1.0866, Validation Accuracy:0.3481\n",
    "Epoch #293: Loss:1.0642, Accuracy:0.4053, Validation Loss:1.0861, Validation Accuracy:0.3465\n",
    "Epoch #294: Loss:1.0639, Accuracy:0.4000, Validation Loss:1.0861, Validation Accuracy:0.3596\n",
    "Epoch #295: Loss:1.0643, Accuracy:0.4045, Validation Loss:1.0865, Validation Accuracy:0.3727\n",
    "Epoch #296: Loss:1.0656, Accuracy:0.4045, Validation Loss:1.0860, Validation Accuracy:0.3777\n",
    "Epoch #297: Loss:1.0641, Accuracy:0.4082, Validation Loss:1.0865, Validation Accuracy:0.3481\n",
    "Epoch #298: Loss:1.0647, Accuracy:0.4053, Validation Loss:1.0862, Validation Accuracy:0.3432\n",
    "Epoch #299: Loss:1.0643, Accuracy:0.4107, Validation Loss:1.0859, Validation Accuracy:0.3727\n",
    "Epoch #300: Loss:1.0643, Accuracy:0.3992, Validation Loss:1.0848, Validation Accuracy:0.3596\n",
    "\n",
    "Test:\n",
    "Test Loss:1.08483076, Accuracy:0.3596\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  03   01\n",
    "t:02  72   0  155\n",
    "t:03  63   0   79\n",
    "t:01  93   0  147\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.32      0.32      0.32       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.39      0.61      0.47       240\n",
    "\n",
    "    accuracy                           0.36       609\n",
    "   macro avg       0.23      0.31      0.26       609\n",
    "weighted avg       0.27      0.36      0.30       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 13:35:58 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.076470229817533, 1.075056298612961, 1.0753788611376032, 1.0752606133522071, 1.0737482039212005, 1.074240978323963, 1.0743958206208077, 1.0741680819412758, 1.0742593762909838, 1.0741814688117242, 1.074004136282822, 1.0744059185676387, 1.0742890877872462, 1.074064654669738, 1.0741048571707188, 1.0740416316171781, 1.0744295715306975, 1.0742319957376114, 1.0740700864243782, 1.074137094768593, 1.0742910867449882, 1.0742805785146252, 1.074155939819386, 1.0743897820536923, 1.074450042447433, 1.07448393449016, 1.0747694105937564, 1.074752877889987, 1.0749315426658919, 1.075109176056334, 1.074690492478106, 1.0749402126459457, 1.074844386777267, 1.075049746016955, 1.0750425071356136, 1.0750695388696856, 1.0749580836648425, 1.075116680369197, 1.0751667670624205, 1.0751520508811587, 1.0751960426324303, 1.0751458007126606, 1.0751419053680595, 1.0751188966049545, 1.075183157263131, 1.075102618175187, 1.075208998861767, 1.0752967114519016, 1.075497475359436, 1.0751115109141434, 1.0753612246223663, 1.0754395543256612, 1.075512960626574, 1.075515228343519, 1.0754616401465655, 1.0754787127176921, 1.075444367131576, 1.0756129024455505, 1.0754541518848713, 1.0755263373182324, 1.0755862625948902, 1.0757592571975758, 1.0757752159741907, 1.075774707426187, 1.0758734309223095, 1.0758614561632154, 1.07592510159184, 1.0760937138339766, 1.076253850863289, 1.0762871345275729, 1.0765404282336557, 1.0764415438343542, 1.0766674011994661, 1.0767117781787867, 1.0768071528530276, 1.0777194971717245, 1.077117325637141, 1.077005742610186, 1.0774328736053116, 1.0771597711910754, 1.0773504796286522, 1.0779259645292911, 1.077696296577579, 1.077938753591578, 1.0780973827897622, 1.0782854551164975, 1.0783514782712964, 1.0786283116035273, 1.0786986922591386, 1.0790115250351002, 1.0791776268353015, 1.0796497095198858, 1.079595738052343, 1.0798257499297068, 1.0803356292016792, 1.0799835843992938, 1.0799968418816628, 1.0798143880512132, 1.0799896227706633, 1.080485502095841, 1.0801858516357998, 1.080428718737585, 1.080665248172428, 1.0804484111726382, 1.080465184448192, 1.0823194115424195, 1.0805357048664186, 1.080539208523354, 1.0807875696269946, 1.0812028143402, 1.0816394967594365, 1.0813685266059412, 1.0812543573833646, 1.0812889848436629, 1.081623000072924, 1.0815283915483698, 1.0815302450668636, 1.0816874952347604, 1.0817835749859488, 1.0820369565819676, 1.0824030671017901, 1.0816811669636242, 1.0819394727450091, 1.0818338429399312, 1.0816024810026823, 1.0816240688458647, 1.0821131653777876, 1.0824989881030054, 1.0822673484022394, 1.0828929783283978, 1.0825509058039373, 1.0831143756218145, 1.082829451130333, 1.0826718728922076, 1.0834684366075864, 1.0830955795075115, 1.083305737067913, 1.0841053704714345, 1.0837438605689063, 1.0847363368239504, 1.0832481957813007, 1.083556626622117, 1.0827791005715557, 1.0832581124673728, 1.083729541947689, 1.0843229515016177, 1.083894012401061, 1.0838118676090085, 1.0837743769725556, 1.0837634694204346, 1.0847262835071982, 1.084863153192993, 1.0853326539883668, 1.085296348006463, 1.085118591296066, 1.0849193900285292, 1.0847646636132928, 1.085925095578524, 1.0851369334754881, 1.084897215143213, 1.085578061089727, 1.0854014915785766, 1.0856711723534345, 1.0852686429062892, 1.0851251346920119, 1.0849511975725297, 1.0864593450267523, 1.0851812944036399, 1.0861514023763597, 1.084887205869302, 1.0856280939528116, 1.0849762898556312, 1.0857957031926497, 1.0845564525507159, 1.0840148044924431, 1.0860146659935637, 1.0849060244943904, 1.0840544724112073, 1.0845763922129164, 1.084637374909249, 1.0853456939969743, 1.0859850353403828, 1.0868359117085122, 1.0848645668703152, 1.0839466228469448, 1.0839797113525065, 1.0861609888389976, 1.0852259624572027, 1.0854857398567137, 1.0857986894930134, 1.0853810204660952, 1.0852048532128922, 1.0851252492033985, 1.0848853997213304, 1.086518678954865, 1.0850700738982026, 1.0852537292173539, 1.086836229404205, 1.0860612014635835, 1.0854010957802458, 1.085370615981091, 1.0861415894356463, 1.08529063829256, 1.0852647150678587, 1.0866996803502926, 1.0850189462279647, 1.085028871722605, 1.0859004470515135, 1.085756788308593, 1.0858173080657307, 1.0853936654593557, 1.0850999864255657, 1.085763058051687, 1.0864780187998304, 1.0862097213616708, 1.0855754052085438, 1.0858113409458905, 1.0854968294525773, 1.086222797387535, 1.086157720664452, 1.0860335938448977, 1.0861775023596627, 1.085828750200068, 1.0855405369807152, 1.0861224368679503, 1.0863798365412871, 1.0860761860125563, 1.0876933132682136, 1.085309414440775, 1.085642118955089, 1.0857537468078688, 1.0852912411901163, 1.0848443752830643, 1.0861741083204648, 1.085710750816295, 1.0864215958098864, 1.0850083242691992, 1.0854905781096034, 1.086168871137309, 1.0857610089829794, 1.0856915763250516, 1.0865606392545653, 1.085224103653568, 1.0863564333500728, 1.0878645995959078, 1.0859885180525004, 1.0854293331136844, 1.0853506659443546, 1.0856214489647125, 1.086446361784473, 1.0870712688208017, 1.0859858003155938, 1.0863878562532623, 1.0856390803905542, 1.0856642151505294, 1.0855296511563957, 1.085609280612864, 1.085477532424363, 1.0850434882691733, 1.087460326248006, 1.085018261900089, 1.084985721287469, 1.086189953172931, 1.0849823571974029, 1.0852312797004562, 1.0865245108142472, 1.0859247262058978, 1.0851673559210766, 1.0871164775247057, 1.0876609635079044, 1.0856244184309234, 1.085217374886198, 1.0867649647598392, 1.0859631188397336, 1.0858661404188434, 1.0854550486519223, 1.0865494948498329, 1.0850697986793831, 1.0849571368964435, 1.0850447394773486, 1.0851799396458517, 1.085910517006672, 1.0874347990173816, 1.086142882729203, 1.0849364745401593, 1.0860223132009772, 1.0868165870801176, 1.0868201026775566, 1.0856966128685988, 1.0854378110669516, 1.0856717819063535, 1.0865564720188259, 1.0860803342609375, 1.0860866347361473, 1.0864888803516506, 1.0859629918006057, 1.0864631068726087, 1.0861875075229088, 1.0858890769516893, 1.0848306991000873], 'val_acc': [0.37766830777299815, 0.39573070455850246, 0.3940886684337077, 0.3940886684337077, 0.3940886684337077, 0.39244663211316705, 0.38752052364090983, 0.3809523791417308, 0.3940886684337077, 0.3776683067942683, 0.385878487711861, 0.38916256025506946, 0.3809523792396038, 0.3940886684337077, 0.3940886684337077, 0.3940886684337077, 0.39244663260253193, 0.3940886684337077, 0.385878487711861, 0.3842364513913203, 0.3940886684337077, 0.3908045961841182, 0.39244663230891297, 0.38916256035294244, 0.3891625600593235, 0.38752052393452874, 0.38752052393452874, 0.39244663230891297, 0.3891625600593235, 0.3891625600593235, 0.38752052393452874, 0.3809523791417308, 0.385878487711861, 0.3825944154622715, 0.3891625600593235, 0.3891625600593235, 0.385878487711861, 0.37931034301693606, 0.3891625600593235, 0.38423645168493925, 0.3891625600593235, 0.385878487809734, 0.3776683068921413, 0.3776683068921413, 0.3891625600593235, 0.385878487809734, 0.38752052393452874, 0.3891625600593235, 0.38423645168493925, 0.3809523793374768, 0.3809523793374768, 0.3825944154622715, 0.38752052393452874, 0.3891625600593235, 0.37110016239296234, 0.38423645158706626, 0.3891625600593235, 0.4022988492534274, 0.3891625600593235, 0.3891625600593235, 0.385878487809734, 0.3776683068921413, 0.3891625600593235, 0.3891625600593235, 0.3891625600593235, 0.385878487809734, 0.385878487711861, 0.3776683068921413, 0.3891625600593235, 0.3891625600593235, 0.3891625600593235, 0.38752052393452874, 0.38752052393452874, 0.37602627076734657, 0.38752052393452874, 0.3908045964777372, 0.3776683068921413, 0.3776683068921413, 0.3858784875161151, 0.3940886684337077, 0.38752052393452874, 0.3858784875161151, 0.3908045961841182, 0.3776683068921413, 0.3908045961841182, 0.39244663230891297, 0.39244663230891297, 0.3858784875161151, 0.38752052383665575, 0.3842364513913203, 0.38752052393452874, 0.38259441526652554, 0.38752052393452874, 0.38752052383665575, 0.3858784875161151, 0.3825944154622715, 0.38259441526652554, 0.3776683067942683, 0.3842364513913203, 0.37110016249083533, 0.3793103429190631, 0.3842364513913203, 0.37274219861563007, 0.38259441526652554, 0.3793103429190631, 0.36781609033911883, 0.3760262706694736, 0.3760262706694736, 0.3809523792396038, 0.38259441526652554, 0.3858784875161151, 0.3809523792396038, 0.3776683067942683, 0.3776683067942683, 0.3842364513913203, 0.3809523792396038, 0.3760262706694736, 0.3809523792396038, 0.385878487613988, 0.3776683067942683, 0.385878487613988, 0.3809523792396038, 0.3776683067942683, 0.3776683067942683, 0.3776683067942683, 0.3760262706694736, 0.385878487613988, 0.385878487613988, 0.3760262706694736, 0.3760262706694736, 0.3809523792396038, 0.3842364514891933, 0.3760262706694736, 0.3776683067942683, 0.385878487613988, 0.3760262706694736, 0.3760262706694736, 0.3809523792396038, 0.3809523792396038, 0.38752052373878276, 0.3776683067942683, 0.3760262706694736, 0.3776683067942683, 0.3760262706694736, 0.3842364514891933, 0.37602627076734657, 0.3842364514891933, 0.3842364514891933, 0.3842364514891933, 0.3760262706694736, 0.3842364514891933, 0.3760262706694736, 0.3842364514891933, 0.3760262706694736, 0.37438423454467884, 0.3760262706694736, 0.3760262706694736, 0.3842364514891933, 0.3842364514891933, 0.3760262706694736, 0.37438423454467884, 0.36453201789378337, 0.3760262706694736, 0.3842364514891933, 0.37931034301693606, 0.36453201789378337, 0.3612479456441939, 0.3760262706694736, 0.385878487613988, 0.3793103429190631, 0.3497536926727577, 0.3842364514891933, 0.385878487613988, 0.37931034282119014, 0.34318554797783274, 0.3776683070878873, 0.3776683069900143, 0.37438423454467884, 0.36453201789378337, 0.3628899817689886, 0.3727421983220111, 0.3793103429190631, 0.3891625598635775, 0.37438423454467884, 0.3661740539207051, 0.36453201789378337, 0.385878487613988, 0.3793103429190631, 0.3793103429190631, 0.37602627096309255, 0.37110016239296234, 0.37110016219721637, 0.3793103429190631, 0.37110016219721637, 0.38752052373878276, 0.37438423454467884, 0.37110016219721637, 0.3612479456441939, 0.385878487613988, 0.37438423454467884, 0.3612479455463209, 0.3825944151686526, 0.37438423454467884, 0.3661740540185781, 0.3891625598635775, 0.3760262706694736, 0.37110016219721637, 0.35139572869967944, 0.37438423454467884, 0.38095237904385787, 0.3661740540185781, 0.3694581261702946, 0.3628899818668616, 0.385878487613988, 0.3793103429190631, 0.341543511853038, 0.3563218370740637, 0.37110016219721637, 0.3760262706694736, 0.3546798011450149, 0.38752052373878276, 0.38752052373878276, 0.3825944151686526, 0.3727421983220111, 0.3825944151686526, 0.35139572869967944, 0.3727421983220111, 0.3891625598635775, 0.3497536925748847, 0.3513957288954254, 0.38752052373878276, 0.37110016219721637, 0.3530377648244742, 0.36453201789378337, 0.36288998157324265, 0.38752052373878276, 0.3727421983220111, 0.3743842344468059, 0.3612479456441939, 0.3399014759239892, 0.3727421983220111, 0.3825944151686526, 0.35632183717193666, 0.3694581260724216, 0.3776683068921413, 0.3530377649223472, 0.37602627057160065, 0.3612479455463209, 0.3497536926727577, 0.3743842344468059, 0.3743842346425518, 0.37110016219721637, 0.36453201789378337, 0.334975367353859, 0.38752052364090983, 0.3694581260724216, 0.3661740540185781, 0.36288998157324265, 0.3776683066963954, 0.3776683068921413, 0.33990147582611624, 0.36288998157324265, 0.36453201789378337, 0.34811165645008996, 0.3776683066963954, 0.3776683068921413, 0.37931034282119014, 0.3645320176980374, 0.37931034282119014, 0.36945812646391357, 0.35139572860180646, 0.3546798010471419, 0.37931034282119014, 0.34811165635221697, 0.37602627057160065, 0.34811165645008996, 0.3711001625887083, 0.3727421983220111, 0.3497536926727577, 0.36617405382283214, 0.3645320176980374, 0.3596059092257802, 0.39080459598837225, 0.3727421983220111, 0.34811165654796294, 0.35632183717193666, 0.3825944151686526, 0.3694581260724216, 0.3694581260724216, 0.3645320176001644, 0.3612479455463209, 0.34811165635221697, 0.3464696202274222, 0.3596059092257802, 0.3727421983220111, 0.3776683066963954, 0.34811165635221697, 0.3431855481735787, 0.3727421983220111, 0.35960590942152615], 'loss': [1.083929212382197, 1.0754053127594307, 1.0754327174819225, 1.0755098486338308, 1.0745639384404835, 1.0743339848469415, 1.0740396763020228, 1.0739154389261956, 1.0739892501361072, 1.073705124659215, 1.0736681210186936, 1.0737643812716129, 1.0736775088848765, 1.0739957225396157, 1.0741168247845627, 1.0736621587428223, 1.0737014658886794, 1.0738364233373372, 1.0735870219354022, 1.0734143172446218, 1.0735764721335816, 1.073562310902245, 1.0734987317659037, 1.073331367896078, 1.0732733679992708, 1.0734037684708895, 1.073305971764441, 1.0734502648915598, 1.0736036632095274, 1.073852283802855, 1.0738148078291814, 1.073731788717501, 1.073826587812122, 1.0736970377653776, 1.0735712937505828, 1.073751619219535, 1.073549570929588, 1.0737072823228777, 1.0737888107064812, 1.0734798050024672, 1.0737307504462021, 1.0735831234244595, 1.0734577603898254, 1.0736703183371918, 1.0734236728973703, 1.073452796779374, 1.0735893930497844, 1.0732510476631305, 1.0738328938611479, 1.0734097843542236, 1.073504346058354, 1.073504093638191, 1.0734252489078215, 1.073390067823124, 1.0734906974514407, 1.0734038791617329, 1.073338440311518, 1.0733241426871298, 1.0734672918946346, 1.0731881999382002, 1.0732255345252506, 1.0736375323556042, 1.0733811913084934, 1.0730749595336602, 1.0731737392394205, 1.072971147237617, 1.0728065108371712, 1.073049339474594, 1.0728283027359102, 1.0729016849882058, 1.0729407882788342, 1.0725759071001526, 1.072603561697065, 1.072391134708569, 1.072595258121373, 1.0731201497436305, 1.0727779699791629, 1.0722159827269568, 1.07184136657989, 1.0725984118067997, 1.0717742974263686, 1.0720453471863294, 1.0716543793433502, 1.0717983490632546, 1.0715399129679561, 1.0714742111229554, 1.0713151384428052, 1.071295834958431, 1.0711063905907852, 1.07090360912699, 1.0710009698750302, 1.0712946978927393, 1.0708609901659298, 1.070869854979936, 1.0708228461306688, 1.070975205638815, 1.0705623067135195, 1.0708283851278881, 1.0704806354745948, 1.070606889959723, 1.071201016281175, 1.07031723885076, 1.0706751853778378, 1.0700599191125169, 1.0709586464648864, 1.0707876179007778, 1.0711404529685113, 1.0704870540258575, 1.0700921168807107, 1.0699414797632112, 1.0699358359009823, 1.0701024582253833, 1.069952867652846, 1.0695708572497358, 1.0700906989021224, 1.070008747092997, 1.0695520113136245, 1.0696930167611374, 1.0709201214494646, 1.071102489289317, 1.069527674504619, 1.0701974909897947, 1.0698117802520062, 1.0697189994416443, 1.0699807779010562, 1.0693631512428456, 1.0694854330967585, 1.0696436180960716, 1.0693833824545451, 1.0696801394162971, 1.0692765370531494, 1.0690129108742277, 1.0687951744459494, 1.0692694583223097, 1.0686389788954656, 1.068534010097966, 1.068594411755979, 1.0685796086058725, 1.06913119850707, 1.0688621781445138, 1.0695857719719042, 1.069034728132479, 1.0686937946558488, 1.0685428494545468, 1.0706760845634726, 1.0685151247518019, 1.069410801276534, 1.0682397141838464, 1.0681990821258733, 1.0681727096530202, 1.067674789242676, 1.0679869902941725, 1.0678980890975107, 1.0673776530632004, 1.0676398718381563, 1.067439763648799, 1.0675263280006895, 1.0671885510000114, 1.06795961685494, 1.0669700083057003, 1.067502410651722, 1.067117269425911, 1.0677039965956607, 1.0671855123136078, 1.067980711611879, 1.067538530332107, 1.0674252085127625, 1.067785184730984, 1.0665493384034237, 1.0684891054028114, 1.0676298066086347, 1.067272015420808, 1.0693943721313006, 1.0666616804056344, 1.0680143198193466, 1.067718685725876, 1.0672104017935249, 1.0673273344549066, 1.066459895110473, 1.0669003002942221, 1.0668810360240741, 1.067382654810833, 1.0671378621820062, 1.0663689201127822, 1.0666547452644646, 1.0666664288029288, 1.0664161039818483, 1.066668274711045, 1.066671254306848, 1.0659024686539198, 1.0665700334787858, 1.0659055322102697, 1.0666674782363297, 1.0663988370425403, 1.0667882464015264, 1.065909442030184, 1.067341951227286, 1.065555072711968, 1.0671417602523396, 1.065766273631697, 1.066202468402087, 1.0666291895833104, 1.0657686557613115, 1.0666456908415964, 1.066533338264763, 1.0660273966113645, 1.0663129888765621, 1.0664992364769843, 1.0665585511518945, 1.0656427371183705, 1.0658374747211683, 1.0658823216965065, 1.0661005263945404, 1.065322322473389, 1.0662568840402842, 1.0653741953554094, 1.0660547353893335, 1.0656026478420781, 1.0661262991002454, 1.0659010403454916, 1.065848566472408, 1.0654001105247826, 1.065393533550004, 1.0652587621363772, 1.0656675975670316, 1.0657297712087141, 1.0656221925845137, 1.0663316505400797, 1.0657577451004874, 1.065560221916841, 1.0657981088274069, 1.065200315706539, 1.0652703995577364, 1.0660294947438171, 1.0660290147734375, 1.0661455226385128, 1.0655305433567055, 1.0656312950337936, 1.0651180211523474, 1.0658845487806097, 1.06511971132956, 1.0648335451952486, 1.0663504759144244, 1.065102941642307, 1.0657507181657169, 1.0652993176752048, 1.0650296538272677, 1.0652105191894625, 1.065185110818679, 1.0651278372417974, 1.065863087632573, 1.0656951384867486, 1.0647623544356173, 1.066113836466654, 1.0661170234915167, 1.0655766655042676, 1.0655974196702303, 1.0650622164199484, 1.0657553434861513, 1.0666641184436711, 1.0647114463410583, 1.0657721349590858, 1.0649561109484098, 1.0651380138965114, 1.065054559071206, 1.0648528627301632, 1.064519916520716, 1.0650538809245618, 1.0654312144069946, 1.0651810778239914, 1.0657517947210668, 1.0650301582759412, 1.065897937623872, 1.066428947154991, 1.065132831645942, 1.0646230548803812, 1.0650485958651597, 1.0646522333489796, 1.0645633713175873, 1.0651157701774299, 1.0643772048871865, 1.0646464103545985, 1.0644930863037736, 1.0642055568753817, 1.0643845611529184, 1.0640495626343838, 1.0641791082750356, 1.0641817151643411, 1.0641558885084774, 1.0642647628177118, 1.0642202574124815, 1.0643162692107215, 1.0641615569958696, 1.0638723182971963, 1.0642770872468577, 1.0656324489405513, 1.064082210313613, 1.0647256878611977, 1.0643381787521393, 1.0643212958283004], 'acc': [0.37289527522465044, 0.39260780382939675, 0.39466119309470393, 0.39425051171921605, 0.3942505121475862, 0.3963038992220861, 0.40123203083475023, 0.39958932333658365, 0.3930184790975504, 0.3971252574323384, 0.39794660969436535, 0.40164270845282, 0.4028747444769685, 0.3926078024586123, 0.3942505156724605, 0.3942505146933287, 0.39671457922678954, 0.4012320310305766, 0.4004106779485268, 0.4008213532166804, 0.3971252574323384, 0.3991786427444012, 0.4016427114269327, 0.3991786466976456, 0.39958932372823636, 0.40205338904500254, 0.39712525860729647, 0.3995893225532782, 0.39958932036247097, 0.3987679657138104, 0.3991786456817964, 0.3987679663012894, 0.39917864591434016, 0.3983572889157634, 0.39835729146150595, 0.39835729044565676, 0.398767964930505, 0.395893225165608, 0.3979466142350888, 0.40041067974768135, 0.39794661306013074, 0.3983572902865478, 0.3999999999755217, 0.3991786465018192, 0.39712525762816475, 0.40041067638191596, 0.40164271064362733, 0.39999999798054076, 0.40082135560331406, 0.40123203122640294, 0.39753593423038536, 0.3971252578239911, 0.3987679673171386, 0.4008213575615775, 0.39589322418647627, 0.3979466122768253, 0.40164271201441176, 0.39589322238732166, 0.39630390278367783, 0.4012320306389239, 0.40082135325339785, 0.39794661008601806, 0.40123203517964734, 0.40082135521166135, 0.4004106767735687, 0.4004106765777423, 0.39466119231139857, 0.3983572902865478, 0.39999999895967253, 0.4008213565457283, 0.39958932036247097, 0.40082135716992484, 0.40041067622280707, 0.4020533888491762, 0.398767964930505, 0.3885010282728951, 0.3901437387084569, 0.39958932235745187, 0.4020533892408289, 0.3995893219657992, 0.4016427109985626, 0.40410677533619704, 0.4012320324380785, 0.39794661008601806, 0.40041067834017946, 0.40410677576456716, 0.40123203181388195, 0.40287474209033486, 0.4032854208833628, 0.39917864630599287, 0.40246406447226507, 0.39835729146150595, 0.39958932094995003, 0.402053387833327, 0.4024640676422041, 0.40164271064362733, 0.40123203361303655, 0.40246406705472504, 0.3995893245115417, 0.4057494864326728, 0.4032854220950383, 0.4041067741612389, 0.4028747413070295, 0.4065708398696578, 0.4020533882616971, 0.39671458118505304, 0.39507187032112107, 0.401232034983821, 0.40451745177930876, 0.4036960973264745, 0.40410677693952524, 0.40205338747839175, 0.4020533882616971, 0.40369609892980274, 0.4024640646680914, 0.40082135364505056, 0.40205338904500254, 0.40287474111120314, 0.3967145809892267, 0.40780287569797996, 0.40369609634734277, 0.40287474248198757, 0.3995893245115417, 0.40287474506444754, 0.40369609990893446, 0.40410677772283066, 0.40369609752230085, 0.3983572918531586, 0.40287474189450856, 0.40205338607088986, 0.40410677831030967, 0.403285422486691, 0.404106777918657, 0.4032854232699964, 0.4041067739654126, 0.4045174551450741, 0.4028747422861612, 0.40123203420051556, 0.40000000232543786, 0.40657084147298606, 0.4045174543617687, 0.40164271263860823, 0.4024640642764387, 0.40533880920625565, 0.39260780382939675, 0.40410677811448337, 0.40041067876854963, 0.4069815200701876, 0.4065708412771597, 0.4012320306389239, 0.4008213532166804, 0.4057494881951099, 0.4036961004964135, 0.4057494852577147, 0.40533881116451914, 0.40287474428114217, 0.40328542029588377, 0.40082135716992484, 0.40698152120842823, 0.4061601634265461, 0.4036960983423237, 0.4086242279600069, 0.3950718667595293, 0.40082135380415945, 0.40246406587976696, 0.4028747436936631, 0.40123203361303655, 0.3991786425485748, 0.4032854206875364, 0.40492812939737854, 0.40492812978903125, 0.4086242309341196, 0.4061601658131797, 0.40492813315479664, 0.40410677811448337, 0.4004106779852442, 0.4086242319132513, 0.40287474346111934, 0.40328542189921196, 0.3909650907746575, 0.40698152046184033, 0.4041067771353516, 0.408624230970837, 0.40698152144097205, 0.39876796947122845, 0.40657084124044224, 0.40657084264794413, 0.4028747416986822, 0.40492813080488044, 0.40533880779875375, 0.4065708402613105, 0.406981520266014, 0.40287474428114217, 0.4028747422861612, 0.4078028764812853, 0.4057494844376918, 0.4053388105770401, 0.4036960985014326, 0.4036960971306482, 0.4082135535118761, 0.40041067818107057, 0.40041067638191596, 0.4065708428437705, 0.4016427127977171, 0.40616016600900606, 0.4061601624474144, 0.4049281306090541, 0.40123203220553466, 0.38932238170988015, 0.40616016659648513, 0.4082135546868342, 0.4045174555367268, 0.40205339041578697, 0.4094455830003202, 0.4041067785428535, 0.40739219588910286, 0.39917864689347193, 0.40944558319614655, 0.4053388070154484, 0.4045174525626141, 0.40205338963248155, 0.40698151964181745, 0.40944558378362556, 0.4049281305723367, 0.40492813119653315, 0.3971252558657276, 0.40533880936536454, 0.4036960971306482, 0.4065708420604651, 0.3983572900907215, 0.40451745373757225, 0.40944558358779926, 0.4073921984715628, 0.4012320324380785, 0.40451745573255316, 0.40410677811448337, 0.40205339041578697, 0.4073921955341676, 0.41478439556010205, 0.4012320339679718, 0.40821355190854786, 0.403696099284738, 0.39219712597878315, 0.4036960995172818, 0.40287474150285585, 0.4053388113603455, 0.4065708432354232, 0.4065708436270759, 0.39548254793919085, 0.4082135519452653, 0.4045174545575951, 0.40287474291035774, 0.40698151987436126, 0.4032854226825174, 0.3950718693419893, 0.4057494889784153, 0.3979466098901917, 0.40616016205576166, 0.4065708398696578, 0.40246406548811425, 0.4032854208833628, 0.40780287350717265, 0.40780287373971646, 0.40574948858676263, 0.40041067778941786, 0.40903490639809953, 0.40780287291969364, 0.40739219902232443, 0.40657084147298606, 0.40287474545610025, 0.4045174555367268, 0.4000000011504798, 0.3987679688837494, 0.38932237971489925, 0.39876796555470145, 0.40574948482934453, 0.4024640668588987, 0.40533881077286643, 0.40287474291035774, 0.40739219510579744, 0.40410677533619704, 0.40944558515441004, 0.4045174515834824, 0.4049281335464493, 0.40287474388948946, 0.40698152046184033, 0.39835729146150595, 0.4065708442145549, 0.4028747448686212, 0.40985626202589187, 0.41108829781749656, 0.40451745240350523, 0.4053388076029274, 0.4000000011504798, 0.4045174535417459, 0.4045174535417459, 0.4082135515536126, 0.40533881077286643, 0.4106776168703788, 0.39917864708929823]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
