{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf45.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 16:17:12 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '03', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000029091794E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002908E8F6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0974, Accuracy:0.3450, Validation Loss:1.0888, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0847, Accuracy:0.3943, Validation Loss:1.0794, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0773, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0732, Accuracy:0.3947, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0724, Validation Accuracy:0.3957\n",
    "Epoch #16: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0717, Validation Accuracy:0.4138\n",
    "Epoch #17: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0706, Validation Accuracy:0.4384\n",
    "Epoch #18: Loss:1.0707, Accuracy:0.4144, Validation Loss:1.0689, Validation Accuracy:0.4466\n",
    "Epoch #19: Loss:1.0693, Accuracy:0.4160, Validation Loss:1.0666, Validation Accuracy:0.4401\n",
    "Epoch #20: Loss:1.0668, Accuracy:0.4230, Validation Loss:1.0630, Validation Accuracy:0.4401\n",
    "Epoch #21: Loss:1.0643, Accuracy:0.4226, Validation Loss:1.0589, Validation Accuracy:0.4286\n",
    "Epoch #22: Loss:1.0601, Accuracy:0.4226, Validation Loss:1.0549, Validation Accuracy:0.4433\n",
    "Epoch #23: Loss:1.0578, Accuracy:0.4234, Validation Loss:1.0521, Validation Accuracy:0.4269\n",
    "Epoch #24: Loss:1.0563, Accuracy:0.4193, Validation Loss:1.0526, Validation Accuracy:0.4335\n",
    "Epoch #25: Loss:1.0545, Accuracy:0.4214, Validation Loss:1.0530, Validation Accuracy:0.4450\n",
    "Epoch #26: Loss:1.0535, Accuracy:0.4218, Validation Loss:1.0528, Validation Accuracy:0.4236\n",
    "Epoch #27: Loss:1.0532, Accuracy:0.4189, Validation Loss:1.0533, Validation Accuracy:0.4269\n",
    "Epoch #28: Loss:1.0508, Accuracy:0.4222, Validation Loss:1.0549, Validation Accuracy:0.4417\n",
    "Epoch #29: Loss:1.0514, Accuracy:0.4214, Validation Loss:1.0538, Validation Accuracy:0.4269\n",
    "Epoch #30: Loss:1.0497, Accuracy:0.4222, Validation Loss:1.0540, Validation Accuracy:0.4286\n",
    "Epoch #31: Loss:1.0503, Accuracy:0.4246, Validation Loss:1.0547, Validation Accuracy:0.4269\n",
    "Epoch #32: Loss:1.0492, Accuracy:0.4275, Validation Loss:1.0535, Validation Accuracy:0.4368\n",
    "Epoch #33: Loss:1.0480, Accuracy:0.4238, Validation Loss:1.0531, Validation Accuracy:0.4269\n",
    "Epoch #34: Loss:1.0478, Accuracy:0.4242, Validation Loss:1.0521, Validation Accuracy:0.4319\n",
    "Epoch #35: Loss:1.0459, Accuracy:0.4230, Validation Loss:1.0509, Validation Accuracy:0.4368\n",
    "Epoch #36: Loss:1.0464, Accuracy:0.4287, Validation Loss:1.0504, Validation Accuracy:0.4351\n",
    "Epoch #37: Loss:1.0447, Accuracy:0.4267, Validation Loss:1.0512, Validation Accuracy:0.4253\n",
    "Epoch #38: Loss:1.0440, Accuracy:0.4263, Validation Loss:1.0486, Validation Accuracy:0.4351\n",
    "Epoch #39: Loss:1.0431, Accuracy:0.4296, Validation Loss:1.0485, Validation Accuracy:0.4384\n",
    "Epoch #40: Loss:1.0417, Accuracy:0.4300, Validation Loss:1.0479, Validation Accuracy:0.4417\n",
    "Epoch #41: Loss:1.0406, Accuracy:0.4287, Validation Loss:1.0464, Validation Accuracy:0.4401\n",
    "Epoch #42: Loss:1.0396, Accuracy:0.4366, Validation Loss:1.0458, Validation Accuracy:0.4286\n",
    "Epoch #43: Loss:1.0383, Accuracy:0.4415, Validation Loss:1.0457, Validation Accuracy:0.4401\n",
    "Epoch #44: Loss:1.0370, Accuracy:0.4427, Validation Loss:1.0447, Validation Accuracy:0.4433\n",
    "Epoch #45: Loss:1.0358, Accuracy:0.4402, Validation Loss:1.0430, Validation Accuracy:0.4368\n",
    "Epoch #46: Loss:1.0381, Accuracy:0.4345, Validation Loss:1.0424, Validation Accuracy:0.4433\n",
    "Epoch #47: Loss:1.0370, Accuracy:0.4345, Validation Loss:1.0431, Validation Accuracy:0.4450\n",
    "Epoch #48: Loss:1.0319, Accuracy:0.4415, Validation Loss:1.0415, Validation Accuracy:0.4302\n",
    "Epoch #49: Loss:1.0290, Accuracy:0.4493, Validation Loss:1.0452, Validation Accuracy:0.4401\n",
    "Epoch #50: Loss:1.0277, Accuracy:0.4567, Validation Loss:1.0398, Validation Accuracy:0.4548\n",
    "Epoch #51: Loss:1.0250, Accuracy:0.4526, Validation Loss:1.0417, Validation Accuracy:0.4368\n",
    "Epoch #52: Loss:1.0230, Accuracy:0.4538, Validation Loss:1.0389, Validation Accuracy:0.4565\n",
    "Epoch #53: Loss:1.0201, Accuracy:0.4526, Validation Loss:1.0376, Validation Accuracy:0.4433\n",
    "Epoch #54: Loss:1.0188, Accuracy:0.4542, Validation Loss:1.0431, Validation Accuracy:0.4368\n",
    "Epoch #55: Loss:1.0163, Accuracy:0.4534, Validation Loss:1.0365, Validation Accuracy:0.4466\n",
    "Epoch #56: Loss:1.0127, Accuracy:0.4600, Validation Loss:1.0375, Validation Accuracy:0.4384\n",
    "Epoch #57: Loss:1.0084, Accuracy:0.4620, Validation Loss:1.0360, Validation Accuracy:0.4450\n",
    "Epoch #58: Loss:1.0073, Accuracy:0.4772, Validation Loss:1.0363, Validation Accuracy:0.4450\n",
    "Epoch #59: Loss:1.0045, Accuracy:0.4669, Validation Loss:1.0545, Validation Accuracy:0.4417\n",
    "Epoch #60: Loss:1.0130, Accuracy:0.4780, Validation Loss:1.0336, Validation Accuracy:0.4417\n",
    "Epoch #61: Loss:1.0020, Accuracy:0.4776, Validation Loss:1.0330, Validation Accuracy:0.4483\n",
    "Epoch #62: Loss:0.9994, Accuracy:0.4731, Validation Loss:1.0430, Validation Accuracy:0.4483\n",
    "Epoch #63: Loss:1.0017, Accuracy:0.4752, Validation Loss:1.0380, Validation Accuracy:0.4401\n",
    "Epoch #64: Loss:0.9996, Accuracy:0.4739, Validation Loss:1.0570, Validation Accuracy:0.4483\n",
    "Epoch #65: Loss:0.9974, Accuracy:0.4871, Validation Loss:1.0322, Validation Accuracy:0.4499\n",
    "Epoch #66: Loss:0.9979, Accuracy:0.4776, Validation Loss:1.0286, Validation Accuracy:0.4384\n",
    "Epoch #67: Loss:0.9931, Accuracy:0.4797, Validation Loss:1.0355, Validation Accuracy:0.4483\n",
    "Epoch #68: Loss:0.9881, Accuracy:0.4887, Validation Loss:1.0286, Validation Accuracy:0.4483\n",
    "Epoch #69: Loss:0.9943, Accuracy:0.4789, Validation Loss:1.0365, Validation Accuracy:0.4532\n",
    "Epoch #70: Loss:0.9903, Accuracy:0.4817, Validation Loss:1.0327, Validation Accuracy:0.4499\n",
    "Epoch #71: Loss:0.9908, Accuracy:0.4940, Validation Loss:1.0294, Validation Accuracy:0.4433\n",
    "Epoch #72: Loss:0.9851, Accuracy:0.4834, Validation Loss:1.0399, Validation Accuracy:0.4581\n",
    "Epoch #73: Loss:0.9829, Accuracy:0.4903, Validation Loss:1.0200, Validation Accuracy:0.4565\n",
    "Epoch #74: Loss:0.9796, Accuracy:0.4945, Validation Loss:1.0323, Validation Accuracy:0.4614\n",
    "Epoch #75: Loss:0.9821, Accuracy:0.4875, Validation Loss:1.0253, Validation Accuracy:0.4548\n",
    "Epoch #76: Loss:0.9804, Accuracy:0.4949, Validation Loss:1.0164, Validation Accuracy:0.4647\n",
    "Epoch #77: Loss:0.9728, Accuracy:0.4986, Validation Loss:1.0172, Validation Accuracy:0.4581\n",
    "Epoch #78: Loss:0.9763, Accuracy:0.4916, Validation Loss:1.0199, Validation Accuracy:0.4548\n",
    "Epoch #79: Loss:0.9740, Accuracy:0.5006, Validation Loss:1.0239, Validation Accuracy:0.4647\n",
    "Epoch #80: Loss:0.9765, Accuracy:0.4994, Validation Loss:1.0104, Validation Accuracy:0.4598\n",
    "Epoch #81: Loss:0.9662, Accuracy:0.4994, Validation Loss:1.0077, Validation Accuracy:0.4499\n",
    "Epoch #82: Loss:0.9643, Accuracy:0.5084, Validation Loss:1.0098, Validation Accuracy:0.4598\n",
    "Epoch #83: Loss:0.9572, Accuracy:0.5101, Validation Loss:1.0046, Validation Accuracy:0.4565\n",
    "Epoch #84: Loss:0.9536, Accuracy:0.5101, Validation Loss:1.0060, Validation Accuracy:0.4647\n",
    "Epoch #85: Loss:0.9503, Accuracy:0.5146, Validation Loss:1.0049, Validation Accuracy:0.4745\n",
    "Epoch #86: Loss:0.9455, Accuracy:0.5170, Validation Loss:0.9957, Validation Accuracy:0.4663\n",
    "Epoch #87: Loss:0.9428, Accuracy:0.5142, Validation Loss:0.9914, Validation Accuracy:0.4778\n",
    "Epoch #88: Loss:0.9424, Accuracy:0.5216, Validation Loss:1.0188, Validation Accuracy:0.4532\n",
    "Epoch #89: Loss:0.9569, Accuracy:0.5064, Validation Loss:0.9785, Validation Accuracy:0.4877\n",
    "Epoch #90: Loss:0.9446, Accuracy:0.5236, Validation Loss:1.0074, Validation Accuracy:0.4893\n",
    "Epoch #91: Loss:0.9448, Accuracy:0.5203, Validation Loss:0.9731, Validation Accuracy:0.4860\n",
    "Epoch #92: Loss:0.9372, Accuracy:0.5248, Validation Loss:0.9954, Validation Accuracy:0.4762\n",
    "Epoch #93: Loss:0.9407, Accuracy:0.5248, Validation Loss:0.9781, Validation Accuracy:0.4975\n",
    "Epoch #94: Loss:0.9341, Accuracy:0.5318, Validation Loss:0.9635, Validation Accuracy:0.4910\n",
    "Epoch #95: Loss:0.9262, Accuracy:0.5244, Validation Loss:1.0047, Validation Accuracy:0.4696\n",
    "Epoch #96: Loss:0.9417, Accuracy:0.5322, Validation Loss:0.9697, Validation Accuracy:0.5025\n",
    "Epoch #97: Loss:0.9256, Accuracy:0.5310, Validation Loss:0.9539, Validation Accuracy:0.4959\n",
    "Epoch #98: Loss:0.9175, Accuracy:0.5298, Validation Loss:0.9644, Validation Accuracy:0.5074\n",
    "Epoch #99: Loss:0.9085, Accuracy:0.5388, Validation Loss:0.9610, Validation Accuracy:0.5074\n",
    "Epoch #100: Loss:0.9116, Accuracy:0.5306, Validation Loss:0.9461, Validation Accuracy:0.4943\n",
    "Epoch #101: Loss:0.9156, Accuracy:0.5343, Validation Loss:0.9729, Validation Accuracy:0.4959\n",
    "Epoch #102: Loss:0.9069, Accuracy:0.5355, Validation Loss:0.9509, Validation Accuracy:0.5107\n",
    "Epoch #103: Loss:0.9032, Accuracy:0.5405, Validation Loss:0.9450, Validation Accuracy:0.5123\n",
    "Epoch #104: Loss:0.8996, Accuracy:0.5326, Validation Loss:0.9703, Validation Accuracy:0.4943\n",
    "Epoch #105: Loss:0.9043, Accuracy:0.5409, Validation Loss:0.9504, Validation Accuracy:0.5156\n",
    "Epoch #106: Loss:0.8941, Accuracy:0.5474, Validation Loss:0.9363, Validation Accuracy:0.5090\n",
    "Epoch #107: Loss:0.8903, Accuracy:0.5520, Validation Loss:0.9345, Validation Accuracy:0.4975\n",
    "Epoch #108: Loss:0.8868, Accuracy:0.5540, Validation Loss:0.9405, Validation Accuracy:0.5123\n",
    "Epoch #109: Loss:0.8887, Accuracy:0.5466, Validation Loss:0.9322, Validation Accuracy:0.5090\n",
    "Epoch #110: Loss:0.8826, Accuracy:0.5458, Validation Loss:0.9315, Validation Accuracy:0.5090\n",
    "Epoch #111: Loss:0.8812, Accuracy:0.5491, Validation Loss:0.9424, Validation Accuracy:0.5304\n",
    "Epoch #112: Loss:0.8900, Accuracy:0.5515, Validation Loss:0.9287, Validation Accuracy:0.5353\n",
    "Epoch #113: Loss:0.8886, Accuracy:0.5483, Validation Loss:0.9246, Validation Accuracy:0.5189\n",
    "Epoch #114: Loss:0.8773, Accuracy:0.5634, Validation Loss:0.9440, Validation Accuracy:0.5255\n",
    "Epoch #115: Loss:0.8868, Accuracy:0.5515, Validation Loss:0.9243, Validation Accuracy:0.5337\n",
    "Epoch #116: Loss:0.8863, Accuracy:0.5441, Validation Loss:0.9401, Validation Accuracy:0.5353\n",
    "Epoch #117: Loss:0.8815, Accuracy:0.5593, Validation Loss:0.9235, Validation Accuracy:0.5189\n",
    "Epoch #118: Loss:0.8730, Accuracy:0.5614, Validation Loss:0.9193, Validation Accuracy:0.5320\n",
    "Epoch #119: Loss:0.8695, Accuracy:0.5630, Validation Loss:0.9193, Validation Accuracy:0.5304\n",
    "Epoch #120: Loss:0.8694, Accuracy:0.5651, Validation Loss:0.9186, Validation Accuracy:0.5140\n",
    "Epoch #121: Loss:0.8681, Accuracy:0.5577, Validation Loss:0.9176, Validation Accuracy:0.5320\n",
    "Epoch #122: Loss:0.8697, Accuracy:0.5581, Validation Loss:0.9171, Validation Accuracy:0.5271\n",
    "Epoch #123: Loss:0.8680, Accuracy:0.5626, Validation Loss:0.9228, Validation Accuracy:0.5337\n",
    "Epoch #124: Loss:0.8658, Accuracy:0.5708, Validation Loss:0.9125, Validation Accuracy:0.5337\n",
    "Epoch #125: Loss:0.8680, Accuracy:0.5680, Validation Loss:0.9280, Validation Accuracy:0.5205\n",
    "Epoch #126: Loss:0.8719, Accuracy:0.5634, Validation Loss:0.9337, Validation Accuracy:0.5287\n",
    "Epoch #127: Loss:0.8724, Accuracy:0.5634, Validation Loss:0.9245, Validation Accuracy:0.5402\n",
    "Epoch #128: Loss:0.8730, Accuracy:0.5651, Validation Loss:0.9287, Validation Accuracy:0.5337\n",
    "Epoch #129: Loss:0.8757, Accuracy:0.5667, Validation Loss:0.9152, Validation Accuracy:0.5501\n",
    "Epoch #130: Loss:0.8611, Accuracy:0.5708, Validation Loss:0.9184, Validation Accuracy:0.5484\n",
    "Epoch #131: Loss:0.8685, Accuracy:0.5639, Validation Loss:0.9119, Validation Accuracy:0.5386\n",
    "Epoch #132: Loss:0.8606, Accuracy:0.5733, Validation Loss:0.9135, Validation Accuracy:0.5353\n",
    "Epoch #133: Loss:0.8559, Accuracy:0.5791, Validation Loss:0.9109, Validation Accuracy:0.5386\n",
    "Epoch #134: Loss:0.8564, Accuracy:0.5721, Validation Loss:0.9122, Validation Accuracy:0.5287\n",
    "Epoch #135: Loss:0.8565, Accuracy:0.5708, Validation Loss:0.9173, Validation Accuracy:0.5304\n",
    "Epoch #136: Loss:0.8579, Accuracy:0.5725, Validation Loss:0.9129, Validation Accuracy:0.5337\n",
    "Epoch #137: Loss:0.8600, Accuracy:0.5823, Validation Loss:0.9080, Validation Accuracy:0.5189\n",
    "Epoch #138: Loss:0.8524, Accuracy:0.5721, Validation Loss:0.9243, Validation Accuracy:0.5386\n",
    "Epoch #139: Loss:0.8586, Accuracy:0.5721, Validation Loss:0.9075, Validation Accuracy:0.5337\n",
    "Epoch #140: Loss:0.8529, Accuracy:0.5799, Validation Loss:0.9074, Validation Accuracy:0.5386\n",
    "Epoch #141: Loss:0.8521, Accuracy:0.5770, Validation Loss:0.9073, Validation Accuracy:0.5337\n",
    "Epoch #142: Loss:0.8557, Accuracy:0.5708, Validation Loss:0.9080, Validation Accuracy:0.5255\n",
    "Epoch #143: Loss:0.8535, Accuracy:0.5807, Validation Loss:0.9152, Validation Accuracy:0.5386\n",
    "Epoch #144: Loss:0.8507, Accuracy:0.5791, Validation Loss:0.9082, Validation Accuracy:0.5402\n",
    "Epoch #145: Loss:0.8540, Accuracy:0.5754, Validation Loss:0.9062, Validation Accuracy:0.5402\n",
    "Epoch #146: Loss:0.8514, Accuracy:0.5725, Validation Loss:0.9291, Validation Accuracy:0.5484\n",
    "Epoch #147: Loss:0.8556, Accuracy:0.5782, Validation Loss:0.9114, Validation Accuracy:0.5320\n",
    "Epoch #148: Loss:0.8501, Accuracy:0.5782, Validation Loss:0.9066, Validation Accuracy:0.5369\n",
    "Epoch #149: Loss:0.8510, Accuracy:0.5713, Validation Loss:0.9216, Validation Accuracy:0.5271\n",
    "Epoch #150: Loss:0.8526, Accuracy:0.5725, Validation Loss:0.9057, Validation Accuracy:0.5320\n",
    "Epoch #151: Loss:0.8541, Accuracy:0.5647, Validation Loss:0.9132, Validation Accuracy:0.5419\n",
    "Epoch #152: Loss:0.8485, Accuracy:0.5778, Validation Loss:0.9045, Validation Accuracy:0.5353\n",
    "Epoch #153: Loss:0.8450, Accuracy:0.5811, Validation Loss:0.9111, Validation Accuracy:0.5386\n",
    "Epoch #154: Loss:0.8517, Accuracy:0.5733, Validation Loss:0.9156, Validation Accuracy:0.5320\n",
    "Epoch #155: Loss:0.8498, Accuracy:0.5770, Validation Loss:0.9132, Validation Accuracy:0.5369\n",
    "Epoch #156: Loss:0.8480, Accuracy:0.5795, Validation Loss:0.9133, Validation Accuracy:0.5337\n",
    "Epoch #157: Loss:0.8485, Accuracy:0.5758, Validation Loss:0.9144, Validation Accuracy:0.5435\n",
    "Epoch #158: Loss:0.8468, Accuracy:0.5795, Validation Loss:0.9066, Validation Accuracy:0.5386\n",
    "Epoch #159: Loss:0.8467, Accuracy:0.5828, Validation Loss:0.9357, Validation Accuracy:0.5156\n",
    "Epoch #160: Loss:0.8676, Accuracy:0.5643, Validation Loss:0.9019, Validation Accuracy:0.5419\n",
    "Epoch #161: Loss:0.8476, Accuracy:0.5795, Validation Loss:0.9163, Validation Accuracy:0.5402\n",
    "Epoch #162: Loss:0.8568, Accuracy:0.5659, Validation Loss:0.9164, Validation Accuracy:0.5452\n",
    "Epoch #163: Loss:0.8478, Accuracy:0.5799, Validation Loss:0.9085, Validation Accuracy:0.5386\n",
    "Epoch #164: Loss:0.8430, Accuracy:0.5828, Validation Loss:0.9060, Validation Accuracy:0.5369\n",
    "Epoch #165: Loss:0.8399, Accuracy:0.5791, Validation Loss:0.9100, Validation Accuracy:0.5435\n",
    "Epoch #166: Loss:0.8388, Accuracy:0.5737, Validation Loss:0.9048, Validation Accuracy:0.5419\n",
    "Epoch #167: Loss:0.8384, Accuracy:0.5717, Validation Loss:0.9020, Validation Accuracy:0.5402\n",
    "Epoch #168: Loss:0.8439, Accuracy:0.5667, Validation Loss:0.9065, Validation Accuracy:0.5320\n",
    "Epoch #169: Loss:0.8449, Accuracy:0.5729, Validation Loss:0.9136, Validation Accuracy:0.5468\n",
    "Epoch #170: Loss:0.8499, Accuracy:0.5733, Validation Loss:0.9096, Validation Accuracy:0.5419\n",
    "Epoch #171: Loss:0.8466, Accuracy:0.5766, Validation Loss:0.9057, Validation Accuracy:0.5369\n",
    "Epoch #172: Loss:0.8414, Accuracy:0.5766, Validation Loss:0.9114, Validation Accuracy:0.5369\n",
    "Epoch #173: Loss:0.8464, Accuracy:0.5680, Validation Loss:0.9018, Validation Accuracy:0.5386\n",
    "Epoch #174: Loss:0.8506, Accuracy:0.5729, Validation Loss:0.9114, Validation Accuracy:0.5452\n",
    "Epoch #175: Loss:0.8469, Accuracy:0.5741, Validation Loss:0.9307, Validation Accuracy:0.5222\n",
    "Epoch #176: Loss:0.8582, Accuracy:0.5713, Validation Loss:0.9323, Validation Accuracy:0.5452\n",
    "Epoch #177: Loss:0.8460, Accuracy:0.5741, Validation Loss:0.9010, Validation Accuracy:0.5402\n",
    "Epoch #178: Loss:0.8355, Accuracy:0.5811, Validation Loss:0.9022, Validation Accuracy:0.5419\n",
    "Epoch #179: Loss:0.8347, Accuracy:0.5828, Validation Loss:0.8996, Validation Accuracy:0.5484\n",
    "Epoch #180: Loss:0.8350, Accuracy:0.5856, Validation Loss:0.9023, Validation Accuracy:0.5320\n",
    "Epoch #181: Loss:0.8335, Accuracy:0.5832, Validation Loss:0.8989, Validation Accuracy:0.5304\n",
    "Epoch #182: Loss:0.8360, Accuracy:0.5823, Validation Loss:0.8985, Validation Accuracy:0.5402\n",
    "Epoch #183: Loss:0.8387, Accuracy:0.5786, Validation Loss:0.9006, Validation Accuracy:0.5419\n",
    "Epoch #184: Loss:0.8361, Accuracy:0.5782, Validation Loss:0.9099, Validation Accuracy:0.5353\n",
    "Epoch #185: Loss:0.8422, Accuracy:0.5721, Validation Loss:0.8998, Validation Accuracy:0.5517\n",
    "Epoch #186: Loss:0.8396, Accuracy:0.5860, Validation Loss:0.8951, Validation Accuracy:0.5534\n",
    "Epoch #187: Loss:0.8303, Accuracy:0.5844, Validation Loss:0.8951, Validation Accuracy:0.5452\n",
    "Epoch #188: Loss:0.8327, Accuracy:0.5807, Validation Loss:0.9043, Validation Accuracy:0.5452\n",
    "Epoch #189: Loss:0.8314, Accuracy:0.5840, Validation Loss:0.8980, Validation Accuracy:0.5369\n",
    "Epoch #190: Loss:0.8304, Accuracy:0.5799, Validation Loss:0.8983, Validation Accuracy:0.5271\n",
    "Epoch #191: Loss:0.8322, Accuracy:0.5840, Validation Loss:0.8974, Validation Accuracy:0.5386\n",
    "Epoch #192: Loss:0.8305, Accuracy:0.5786, Validation Loss:0.9006, Validation Accuracy:0.5501\n",
    "Epoch #193: Loss:0.8302, Accuracy:0.5815, Validation Loss:0.8986, Validation Accuracy:0.5452\n",
    "Epoch #194: Loss:0.8313, Accuracy:0.5811, Validation Loss:0.9036, Validation Accuracy:0.5583\n",
    "Epoch #195: Loss:0.8318, Accuracy:0.5774, Validation Loss:0.9015, Validation Accuracy:0.5419\n",
    "Epoch #196: Loss:0.8319, Accuracy:0.5828, Validation Loss:0.8986, Validation Accuracy:0.5402\n",
    "Epoch #197: Loss:0.8338, Accuracy:0.5811, Validation Loss:0.8960, Validation Accuracy:0.5386\n",
    "Epoch #198: Loss:0.8293, Accuracy:0.5836, Validation Loss:0.8955, Validation Accuracy:0.5484\n",
    "Epoch #199: Loss:0.8273, Accuracy:0.5836, Validation Loss:0.8967, Validation Accuracy:0.5337\n",
    "Epoch #200: Loss:0.8301, Accuracy:0.5815, Validation Loss:0.9017, Validation Accuracy:0.5386\n",
    "Epoch #201: Loss:0.8285, Accuracy:0.5840, Validation Loss:0.9003, Validation Accuracy:0.5353\n",
    "Epoch #202: Loss:0.8305, Accuracy:0.5803, Validation Loss:0.8965, Validation Accuracy:0.5255\n",
    "Epoch #203: Loss:0.8292, Accuracy:0.5791, Validation Loss:0.8944, Validation Accuracy:0.5287\n",
    "Epoch #204: Loss:0.8261, Accuracy:0.5807, Validation Loss:0.8965, Validation Accuracy:0.5484\n",
    "Epoch #205: Loss:0.8268, Accuracy:0.5893, Validation Loss:0.9018, Validation Accuracy:0.5402\n",
    "Epoch #206: Loss:0.8264, Accuracy:0.5881, Validation Loss:0.8989, Validation Accuracy:0.5517\n",
    "Epoch #207: Loss:0.8241, Accuracy:0.5832, Validation Loss:0.8957, Validation Accuracy:0.5468\n",
    "Epoch #208: Loss:0.8224, Accuracy:0.5959, Validation Loss:0.8930, Validation Accuracy:0.5468\n",
    "Epoch #209: Loss:0.8248, Accuracy:0.5926, Validation Loss:0.8942, Validation Accuracy:0.5369\n",
    "Epoch #210: Loss:0.8306, Accuracy:0.5782, Validation Loss:0.8943, Validation Accuracy:0.5435\n",
    "Epoch #211: Loss:0.8249, Accuracy:0.5852, Validation Loss:0.9023, Validation Accuracy:0.5337\n",
    "Epoch #212: Loss:0.8312, Accuracy:0.5840, Validation Loss:0.9008, Validation Accuracy:0.5353\n",
    "Epoch #213: Loss:0.8403, Accuracy:0.5811, Validation Loss:0.9327, Validation Accuracy:0.5287\n",
    "Epoch #214: Loss:0.8496, Accuracy:0.5577, Validation Loss:0.9021, Validation Accuracy:0.5435\n",
    "Epoch #215: Loss:0.8547, Accuracy:0.5573, Validation Loss:0.9386, Validation Accuracy:0.5287\n",
    "Epoch #216: Loss:0.8562, Accuracy:0.5634, Validation Loss:0.9067, Validation Accuracy:0.5550\n",
    "Epoch #217: Loss:0.8488, Accuracy:0.5676, Validation Loss:0.9370, Validation Accuracy:0.5205\n",
    "Epoch #218: Loss:0.8498, Accuracy:0.5774, Validation Loss:0.9143, Validation Accuracy:0.5484\n",
    "Epoch #219: Loss:0.8420, Accuracy:0.5786, Validation Loss:0.9224, Validation Accuracy:0.5419\n",
    "Epoch #220: Loss:0.8349, Accuracy:0.5778, Validation Loss:0.8971, Validation Accuracy:0.5583\n",
    "Epoch #221: Loss:0.8258, Accuracy:0.6033, Validation Loss:0.9031, Validation Accuracy:0.5402\n",
    "Epoch #222: Loss:0.8272, Accuracy:0.5930, Validation Loss:0.9013, Validation Accuracy:0.5517\n",
    "Epoch #223: Loss:0.8227, Accuracy:0.5897, Validation Loss:0.8975, Validation Accuracy:0.5468\n",
    "Epoch #224: Loss:0.8254, Accuracy:0.5856, Validation Loss:0.8949, Validation Accuracy:0.5468\n",
    "Epoch #225: Loss:0.8173, Accuracy:0.5938, Validation Loss:0.8941, Validation Accuracy:0.5402\n",
    "Epoch #226: Loss:0.8207, Accuracy:0.5959, Validation Loss:0.8958, Validation Accuracy:0.5419\n",
    "Epoch #227: Loss:0.8180, Accuracy:0.5910, Validation Loss:0.8995, Validation Accuracy:0.5501\n",
    "Epoch #228: Loss:0.8164, Accuracy:0.5930, Validation Loss:0.8913, Validation Accuracy:0.5567\n",
    "Epoch #229: Loss:0.8197, Accuracy:0.5922, Validation Loss:0.8922, Validation Accuracy:0.5501\n",
    "Epoch #230: Loss:0.8170, Accuracy:0.5873, Validation Loss:0.9045, Validation Accuracy:0.5599\n",
    "Epoch #231: Loss:0.8167, Accuracy:0.5947, Validation Loss:0.8930, Validation Accuracy:0.5468\n",
    "Epoch #232: Loss:0.8151, Accuracy:0.5979, Validation Loss:0.8948, Validation Accuracy:0.5369\n",
    "Epoch #233: Loss:0.8179, Accuracy:0.5926, Validation Loss:0.8979, Validation Accuracy:0.5369\n",
    "Epoch #234: Loss:0.8165, Accuracy:0.5938, Validation Loss:0.8903, Validation Accuracy:0.5484\n",
    "Epoch #235: Loss:0.8132, Accuracy:0.5930, Validation Loss:0.8930, Validation Accuracy:0.5419\n",
    "Epoch #236: Loss:0.8148, Accuracy:0.5881, Validation Loss:0.8959, Validation Accuracy:0.5452\n",
    "Epoch #237: Loss:0.8138, Accuracy:0.5844, Validation Loss:0.9129, Validation Accuracy:0.5517\n",
    "Epoch #238: Loss:0.8168, Accuracy:0.5943, Validation Loss:0.8920, Validation Accuracy:0.5419\n",
    "Epoch #239: Loss:0.8181, Accuracy:0.5869, Validation Loss:0.9098, Validation Accuracy:0.5353\n",
    "Epoch #240: Loss:0.8340, Accuracy:0.5856, Validation Loss:0.9083, Validation Accuracy:0.5402\n",
    "Epoch #241: Loss:0.8381, Accuracy:0.5844, Validation Loss:0.8994, Validation Accuracy:0.5567\n",
    "Epoch #242: Loss:0.8252, Accuracy:0.5819, Validation Loss:0.9097, Validation Accuracy:0.5468\n",
    "Epoch #243: Loss:0.8191, Accuracy:0.5938, Validation Loss:0.8930, Validation Accuracy:0.5468\n",
    "Epoch #244: Loss:0.8161, Accuracy:0.6025, Validation Loss:0.9040, Validation Accuracy:0.5369\n",
    "Epoch #245: Loss:0.8180, Accuracy:0.5791, Validation Loss:0.8982, Validation Accuracy:0.5287\n",
    "Epoch #246: Loss:0.8174, Accuracy:0.5967, Validation Loss:0.8916, Validation Accuracy:0.5452\n",
    "Epoch #247: Loss:0.8194, Accuracy:0.5934, Validation Loss:0.8899, Validation Accuracy:0.5452\n",
    "Epoch #248: Loss:0.8166, Accuracy:0.5910, Validation Loss:0.8992, Validation Accuracy:0.5534\n",
    "Epoch #249: Loss:0.8112, Accuracy:0.5967, Validation Loss:0.9013, Validation Accuracy:0.5452\n",
    "Epoch #250: Loss:0.8109, Accuracy:0.6008, Validation Loss:0.8925, Validation Accuracy:0.5452\n",
    "Epoch #251: Loss:0.8065, Accuracy:0.5984, Validation Loss:0.9002, Validation Accuracy:0.5484\n",
    "Epoch #252: Loss:0.8084, Accuracy:0.6033, Validation Loss:0.8937, Validation Accuracy:0.5468\n",
    "Epoch #253: Loss:0.8106, Accuracy:0.5938, Validation Loss:0.9021, Validation Accuracy:0.5468\n",
    "Epoch #254: Loss:0.8142, Accuracy:0.5975, Validation Loss:0.8904, Validation Accuracy:0.5484\n",
    "Epoch #255: Loss:0.8106, Accuracy:0.6041, Validation Loss:0.8995, Validation Accuracy:0.5419\n",
    "Epoch #256: Loss:0.8153, Accuracy:0.5918, Validation Loss:0.8906, Validation Accuracy:0.5452\n",
    "Epoch #257: Loss:0.8290, Accuracy:0.5795, Validation Loss:0.9136, Validation Accuracy:0.5337\n",
    "Epoch #258: Loss:0.8218, Accuracy:0.5893, Validation Loss:0.9118, Validation Accuracy:0.5599\n",
    "Epoch #259: Loss:0.8188, Accuracy:0.5955, Validation Loss:0.8921, Validation Accuracy:0.5501\n",
    "Epoch #260: Loss:0.8196, Accuracy:0.5996, Validation Loss:0.9049, Validation Accuracy:0.5369\n",
    "Epoch #261: Loss:0.8185, Accuracy:0.6004, Validation Loss:0.8951, Validation Accuracy:0.5337\n",
    "Epoch #262: Loss:0.8091, Accuracy:0.5918, Validation Loss:0.9072, Validation Accuracy:0.5452\n",
    "Epoch #263: Loss:0.8063, Accuracy:0.6012, Validation Loss:0.8939, Validation Accuracy:0.5369\n",
    "Epoch #264: Loss:0.8025, Accuracy:0.6016, Validation Loss:0.8935, Validation Accuracy:0.5402\n",
    "Epoch #265: Loss:0.8022, Accuracy:0.6037, Validation Loss:0.8874, Validation Accuracy:0.5419\n",
    "Epoch #266: Loss:0.8027, Accuracy:0.6082, Validation Loss:0.8886, Validation Accuracy:0.5517\n",
    "Epoch #267: Loss:0.7998, Accuracy:0.6062, Validation Loss:0.8889, Validation Accuracy:0.5484\n",
    "Epoch #268: Loss:0.7996, Accuracy:0.6033, Validation Loss:0.8913, Validation Accuracy:0.5501\n",
    "Epoch #269: Loss:0.8007, Accuracy:0.6033, Validation Loss:0.8900, Validation Accuracy:0.5468\n",
    "Epoch #270: Loss:0.8013, Accuracy:0.6037, Validation Loss:0.8976, Validation Accuracy:0.5517\n",
    "Epoch #271: Loss:0.8016, Accuracy:0.6066, Validation Loss:0.8883, Validation Accuracy:0.5452\n",
    "Epoch #272: Loss:0.7986, Accuracy:0.6053, Validation Loss:0.8951, Validation Accuracy:0.5550\n",
    "Epoch #273: Loss:0.7972, Accuracy:0.5959, Validation Loss:0.8845, Validation Accuracy:0.5484\n",
    "Epoch #274: Loss:0.8017, Accuracy:0.6049, Validation Loss:0.8949, Validation Accuracy:0.5402\n",
    "Epoch #275: Loss:0.8170, Accuracy:0.5955, Validation Loss:0.8936, Validation Accuracy:0.5468\n",
    "Epoch #276: Loss:0.8173, Accuracy:0.5938, Validation Loss:0.9052, Validation Accuracy:0.5435\n",
    "Epoch #277: Loss:0.8084, Accuracy:0.6000, Validation Loss:0.9003, Validation Accuracy:0.5435\n",
    "Epoch #278: Loss:0.8018, Accuracy:0.6012, Validation Loss:0.9114, Validation Accuracy:0.5468\n",
    "Epoch #279: Loss:0.8046, Accuracy:0.6012, Validation Loss:0.8986, Validation Accuracy:0.5402\n",
    "Epoch #280: Loss:0.8091, Accuracy:0.5951, Validation Loss:0.9083, Validation Accuracy:0.5484\n",
    "Epoch #281: Loss:0.8083, Accuracy:0.5971, Validation Loss:0.8932, Validation Accuracy:0.5517\n",
    "Epoch #282: Loss:0.8024, Accuracy:0.6004, Validation Loss:0.9011, Validation Accuracy:0.5419\n",
    "Epoch #283: Loss:0.7993, Accuracy:0.6099, Validation Loss:0.8884, Validation Accuracy:0.5501\n",
    "Epoch #284: Loss:0.7952, Accuracy:0.6029, Validation Loss:0.8932, Validation Accuracy:0.5452\n",
    "Epoch #285: Loss:0.8029, Accuracy:0.6062, Validation Loss:0.8900, Validation Accuracy:0.5452\n",
    "Epoch #286: Loss:0.8072, Accuracy:0.5955, Validation Loss:0.8890, Validation Accuracy:0.5468\n",
    "Epoch #287: Loss:0.8046, Accuracy:0.6004, Validation Loss:0.8887, Validation Accuracy:0.5468\n",
    "Epoch #288: Loss:0.7968, Accuracy:0.6041, Validation Loss:0.8874, Validation Accuracy:0.5452\n",
    "Epoch #289: Loss:0.7978, Accuracy:0.6082, Validation Loss:0.8933, Validation Accuracy:0.5419\n",
    "Epoch #290: Loss:0.8062, Accuracy:0.6074, Validation Loss:0.9333, Validation Accuracy:0.5402\n",
    "Epoch #291: Loss:0.8117, Accuracy:0.5918, Validation Loss:0.9104, Validation Accuracy:0.5452\n",
    "Epoch #292: Loss:0.8030, Accuracy:0.5971, Validation Loss:0.8853, Validation Accuracy:0.5419\n",
    "Epoch #293: Loss:0.8030, Accuracy:0.6008, Validation Loss:0.9129, Validation Accuracy:0.5369\n",
    "Epoch #294: Loss:0.8095, Accuracy:0.5992, Validation Loss:0.8865, Validation Accuracy:0.5501\n",
    "Epoch #295: Loss:0.8094, Accuracy:0.5934, Validation Loss:0.9145, Validation Accuracy:0.5402\n",
    "Epoch #296: Loss:0.8051, Accuracy:0.6094, Validation Loss:0.9144, Validation Accuracy:0.5452\n",
    "Epoch #297: Loss:0.8065, Accuracy:0.6000, Validation Loss:0.9006, Validation Accuracy:0.5419\n",
    "Epoch #298: Loss:0.8019, Accuracy:0.5996, Validation Loss:0.8875, Validation Accuracy:0.5550\n",
    "Epoch #299: Loss:0.7933, Accuracy:0.6029, Validation Loss:0.8959, Validation Accuracy:0.5517\n",
    "Epoch #300: Loss:0.7986, Accuracy:0.6057, Validation Loss:0.8873, Validation Accuracy:0.5567\n",
    "\n",
    "Test:\n",
    "Test Loss:0.88725972, Accuracy:0.5567\n",
    "Labels: ['01', '03', '02']\n",
    "Confusion Matrix:\n",
    "       01  03   02\n",
    "t:01  129  29   82\n",
    "t:03   78  46   18\n",
    "t:02   59   4  164\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.48      0.54      0.51       240\n",
    "          03       0.58      0.32      0.42       142\n",
    "          02       0.62      0.72      0.67       227\n",
    "\n",
    "    accuracy                           0.56       609\n",
    "   macro avg       0.56      0.53      0.53       609\n",
    "weighted avg       0.56      0.56      0.55       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 16:58:03 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 50 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0888211723227414, 1.0793706198239756, 1.0750249733870056, 1.0743871514237378, 1.0749575453634528, 1.075033864364248, 1.0745079897111665, 1.0742508771971528, 1.0740695109312561, 1.073998178558788, 1.0738044061097018, 1.0736011830456738, 1.0733071569542971, 1.0729471161251976, 1.0724337756731632, 1.0716974332023332, 1.0705660669674426, 1.0689248580650743, 1.066613617788982, 1.0629519153698324, 1.0589375456761452, 1.0549268783215426, 1.0520793502945422, 1.0525693701601577, 1.0530450346043152, 1.0527572946986934, 1.0533063822779163, 1.0549490557515562, 1.0537850598396339, 1.054049131709758, 1.0546526554574325, 1.0534615884665002, 1.0531135836649803, 1.0521424711239944, 1.0509377353884317, 1.050439735351525, 1.051211660327191, 1.0485780885066893, 1.0485316033433811, 1.047942544634902, 1.0464019057002953, 1.0458003419569168, 1.0457384421907623, 1.044655904785557, 1.0429518201276782, 1.0423850265434027, 1.0430921592148654, 1.0415104852717108, 1.0451960383573384, 1.039824684265212, 1.0416592414351715, 1.0388775345531396, 1.0376340524707912, 1.043148319709477, 1.036487009920706, 1.0374610825320967, 1.0360424183859613, 1.0362572149298657, 1.0544981120646686, 1.0336125511645489, 1.0329647995959754, 1.042987814482014, 1.0379837841431692, 1.0570177775494178, 1.0321952725083174, 1.0286403432463973, 1.035529769504403, 1.0286347619418441, 1.036530184236849, 1.032733839329436, 1.0294266704268056, 1.0398999450633484, 1.020006337385068, 1.032324112499093, 1.0252948476763195, 1.0163572130140608, 1.0172265034003798, 1.0199224849052617, 1.023864657812322, 1.0104323226242817, 1.0076811290139636, 1.0098279648030724, 1.0046256606410486, 1.005967831963976, 1.0048984770704372, 0.9956861280259632, 0.99143414133288, 1.0188327969001432, 0.9785153024106582, 1.0074264335710623, 0.9731420762041715, 0.9953959456022541, 0.9781350621644695, 0.9634534392646577, 1.004720648521273, 0.9696770720489702, 0.9539468092675671, 0.9643679172143169, 0.9609766677878369, 0.9460841832294057, 0.9729010258206398, 0.9508978815501546, 0.9449619810373716, 0.970345475795038, 0.9503675275444006, 0.9363083773841607, 0.9344828196347054, 0.9405119117649122, 0.9321735802738146, 0.931478338875794, 0.94244304822975, 0.9287432997880507, 0.9246335038410619, 0.9439564023307587, 0.9242679631377284, 0.9400825731468514, 0.923469934655332, 0.9193355893266613, 0.919319437055165, 0.9185578549045256, 0.9176412972519159, 0.9170833499169311, 0.92278595873092, 0.912525333402975, 0.9279993874294613, 0.933664278248065, 0.9245189469436119, 0.9286553561198104, 0.9151833844302323, 0.9183691997441948, 0.9118920515714999, 0.9134839471729322, 0.9108994044302328, 0.9121667866049141, 0.9173139977729183, 0.912927445323988, 0.907951070659462, 0.9242568834270358, 0.907492936636231, 0.9074370661392588, 0.90731652049204, 0.9079887791611683, 0.9151748299402948, 0.9082444268102912, 0.9062278030932635, 0.9290993213653564, 0.9114218509843197, 0.9065765594613964, 0.921589742055276, 0.9056580685238141, 0.9132151111397642, 0.9045299929742547, 0.9110878899766894, 0.9156445955799523, 0.9131892205067652, 0.9132628058173582, 0.9143580282458726, 0.9065557757426169, 0.9357010615478791, 0.901893451198177, 0.9163407606052844, 0.916434567648006, 0.9085310908961178, 0.9060235112562947, 0.9100187733059838, 0.9048224519234769, 0.9019869811624924, 0.906545087915336, 0.9136170937705705, 0.9095672682196831, 0.905700736934524, 0.9113572139066625, 0.9017549178674695, 0.911408401573037, 0.9306788284007356, 0.9323017346643657, 0.9009615533261855, 0.9021806542705041, 0.8995931581127624, 0.9022626921852626, 0.8989436666562248, 0.898512211241354, 0.9006477651141939, 0.9099211904215695, 0.8998468845152894, 0.8950671518377482, 0.895085388607971, 0.904266381792247, 0.8979776191398232, 0.8983235693917486, 0.8973943022475845, 0.9005935274321457, 0.8985936727821338, 0.9036471717463338, 0.9014783723796725, 0.8985570725940523, 0.8959762595948719, 0.8955443590536885, 0.8966754001545397, 0.9016804650107824, 0.9003111508679508, 0.8965088680068456, 0.8944168960911104, 0.8965347280056019, 0.9018107453003306, 0.8988975957892407, 0.8956739972964883, 0.8930171468770759, 0.8941672105703057, 0.8943275122219706, 0.9023066643619381, 0.9007810912108773, 0.9326910183738997, 0.9020723882567119, 0.9385654128635262, 0.9067083518884843, 0.9369664744203314, 0.9143363552531977, 0.922354636129683, 0.8971054346494878, 0.9031335779011543, 0.9012783887155342, 0.8975339581813718, 0.8949423723032909, 0.8941463100890612, 0.8958457106439938, 0.8995177064623151, 0.8912578999311075, 0.8922458626758093, 0.9045028213796944, 0.8930166449844348, 0.894808380278852, 0.8978717560055612, 0.8902530847316109, 0.8929656566853202, 0.8959307472889843, 0.9129463562432965, 0.8920160788424888, 0.9098409610037341, 0.9082947206027402, 0.8994076816710737, 0.9097003119528195, 0.8930070333684411, 0.9040177682741913, 0.89818892739285, 0.891649920560652, 0.8898562691090338, 0.8992357696414189, 0.9013123185372314, 0.8924926451861565, 0.9002162779884777, 0.8936901964577548, 0.9021455378367983, 0.8903939016346861, 0.8994991117706048, 0.8905986423171408, 0.9135791054500148, 0.911828607467595, 0.8920710368696692, 0.9049218094407632, 0.895086729467796, 0.9071624062722932, 0.8939042200008637, 0.8935178979277023, 0.8873986136932874, 0.888588488884943, 0.8889147224293162, 0.8912962784712342, 0.8900346301850819, 0.8975930512440812, 0.8883464530183764, 0.8950867720425423, 0.8845144023840454, 0.8948730783313757, 0.8936053994058193, 0.9051518271905057, 0.9002765683313504, 0.9114204846775199, 0.8985510979379926, 0.9082509772530918, 0.8932293968639154, 0.9010919401015358, 0.8884205899410843, 0.8932084852251513, 0.8900165029347237, 0.8889571777705488, 0.8886601661617924, 0.8873910754185005, 0.8932867226342263, 0.9332512504557279, 0.9103641129283874, 0.8853272206481846, 0.9129441765141605, 0.8864591981977078, 0.9145357009812529, 0.914436287578495, 0.900613146364591, 0.8875204491106355, 0.8959124602120498, 0.8872597702031065], 'val_acc': [0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.3940886680422158, 0.39573070416701056, 0.4137931017354987, 0.43842364478189566, 0.44663382550374237, 0.4400656809066904, 0.44006568061307144, 0.42857142754376226, 0.4433497529605339, 0.4269293916147135, 0.4334975359181465, 0.44499178898745567, 0.42364531916937803, 0.4269293914189675, 0.4417077167378662, 0.4269293914189675, 0.42857142764163525, 0.4269293914189675, 0.43678160846135494, 0.4269293914189675, 0.4318555000869707, 0.43678160855922793, 0.4351395723365602, 0.42528735548991875, 0.4351395723365602, 0.43842364478189566, 0.4417077172272311, 0.4400656808088174, 0.4285714280331272, 0.4400656810045634, 0.44334975344989885, 0.4367816086571009, 0.44334975344989885, 0.4449917894768206, 0.43021346415792194, 0.4400656811024364, 0.4548440061277161, 0.4367816090485928, 0.4564860425461298, 0.44334975344989885, 0.4367816089507198, 0.44663382560161535, 0.43842364487976865, 0.44499178918320165, 0.44499178908532866, 0.4417077168357392, 0.44170771654212027, 0.4482758612370452, 0.4482758612370452, 0.4400656804173255, 0.44827586133491815, 0.44991789736183996, 0.4384236444882767, 0.4482758612370452, 0.4482758611391722, 0.45320196961142944, 0.44991789736183996, 0.443349752569042, 0.4581280781815596, 0.45648604205676485, 0.46141215043114914, 0.4548440059319701, 0.46469622258286564, 0.45812807808368666, 0.4548440060298431, 0.46469622277861156, 0.4597701144042273, 0.44991789775333185, 0.4597701145021003, 0.45648604205676485, 0.46469622277861156, 0.474548439820999, 0.4663382586097874, 0.4778325115812236, 0.4532019693178105, 0.487684728819357, 0.4893267645526598, 0.48604269249881626, 0.47619047506493695, 0.4975369452745065, 0.4909688009710735, 0.4696223306636309, 0.5024630537467637, 0.4958949092475847, 0.507389162023275, 0.5073891621211479, 0.494252872927044, 0.49589490905183875, 0.5106732342728645, 0.5123152703976592, 0.494252872829171, 0.5155993426472487, 0.5090311981480697, 0.4975369451766335, 0.5123152702997862, 0.5090311980501967, 0.5090311979523238, 0.5303776680640203, 0.5353037760469127, 0.5188834147010922, 0.5254515593960172, 0.5336617401178638, 0.5353037765362776, 0.5188834147989652, 0.5320197037973232, 0.5303776672810365, 0.5139573060330891, 0.5320197036015772, 0.5270935954229389, 0.5336617405093558, 0.5336617400199909, 0.520525450728014, 0.5287356314498607, 0.5402298848127888, 0.5336617363497541, 0.5500821014636843, 0.5484400654367625, 0.5385878485901211, 0.5353037762426586, 0.5385878483943751, 0.5287356316456067, 0.5303776682597663, 0.5336617405093558, 0.5188834146032193, 0.5385878483943751, 0.5336617402157369, 0.5385878487858671, 0.5336617398242449, 0.5254515591023983, 0.5385878484922481, 0.5402298844212969, 0.5402298848127888, 0.5484400617665258, 0.5320196997355945, 0.5369458120738344, 0.5270935956186849, 0.5320197036994502, 0.5418719168758549, 0.5353037759490397, 0.5385878446262654, 0.5320196998334675, 0.5369458085993436, 0.5336617362518812, 0.5435139530985226, 0.5385878481986291, 0.5155993426472487, 0.5418719207418377, 0.5402298806531872, 0.5451559894190633, 0.5385878482965022, 0.5369458123674533, 0.5435139530006496, 0.5418719204482186, 0.5402298848127888, 0.5320197036994502, 0.5467980292140948, 0.5418719207418377, 0.5369458085014707, 0.5369458085014707, 0.5385878485901211, 0.5451559928935541, 0.5221674870485546, 0.5451559893211904, 0.5402298846170428, 0.5418719166801089, 0.5484400655346355, 0.5320197037973232, 0.5303776675746554, 0.5402298845191699, 0.5418719168758549, 0.5353037725724219, 0.5517241332331314, 0.5533661737132738, 0.5451559929914271, 0.5451559892233173, 0.5369458121717074, 0.5270935953250659, 0.5385878485901211, 0.5500820972062097, 0.5451559891254444, 0.5582922781238023, 0.5418719168758549, 0.5402298850085347, 0.5385878442347735, 0.5484400653388896, 0.5336617400199909, 0.5385878483943751, 0.5353037760469127, 0.5254515592002712, 0.5287356316456067, 0.5484400612771609, 0.5402298809468061, 0.5517241337224963, 0.5467980251523662, 0.5467980292140948, 0.5369458123674533, 0.5435139570623783, 0.533661739922118, 0.5353037759490397, 0.5287356316456067, 0.5435139566708864, 0.528735627877497, 0.5550082061678318, 0.520525450728014, 0.5484400615707798, 0.5418719172673468, 0.558292281891912, 0.5402298843234239, 0.5517241337224963, 0.5467980250544932, 0.5467980253481121, 0.5402298846170428, 0.5418719206439646, 0.5500820973040826, 0.5566502459628633, 0.5500821014636843, 0.5599343143464701, 0.5467980293119677, 0.5369458122695804, 0.5369458123674533, 0.5484400652410166, 0.5418719210354566, 0.5451559891254444, 0.5517241339182423, 0.5418719210354566, 0.5353037761447856, 0.5402298846170428, 0.5566502422926265, 0.5467980253481121, 0.5467980292140948, 0.5369458123674533, 0.5287356315477337, 0.5451559890275715, 0.545155993285046, 0.5533661698472911, 0.5451559894190633, 0.5451559887339524, 0.5484400615707798, 0.5467980254459851, 0.5467980254459851, 0.5484400609835419, 0.5418719207418377, 0.545155993187173, 0.5336617362518812, 0.559934314542216, 0.5500820974019557, 0.5369458122695804, 0.5336617400199909, 0.5451559893211904, 0.5369458084035976, 0.5402298807510602, 0.5418719208397106, 0.5517241335267504, 0.5484400612771609, 0.5500820974019557, 0.5467980251523662, 0.5517241337224963, 0.5451559889296984, 0.5550082059720859, 0.5484400656325085, 0.5402298848127888, 0.5467980292140948, 0.5435139530985226, 0.5435139530006496, 0.5467980251523662, 0.5402298848127888, 0.5484400653388896, 0.5517241335267504, 0.5418719168758549, 0.5500820974019557, 0.5451559889296984, 0.5451559930893001, 0.5467980292140948, 0.5467980292140948, 0.545155993187173, 0.5418719166801089, 0.5402298805553142, 0.5451559891254444, 0.5418719211333295, 0.5369458122695804, 0.5500820974019557, 0.5402298807510602, 0.5451559890275715, 0.541871916582236, 0.5550082055805939, 0.551724137784225, 0.5566502418032616], 'loss': [1.0974218129622129, 1.0846725888320798, 1.0773499739488293, 1.0745729735744562, 1.0744844685834536, 1.0749778083217707, 1.0746673647138372, 1.074380498549287, 1.0740322273614715, 1.0740604042762114, 1.0738077798663224, 1.0736827661858936, 1.0734509985549738, 1.0731943936318586, 1.0728149462529522, 1.072320477428867, 1.0716604966157761, 1.0706718342504953, 1.0692781856172628, 1.0668107181114337, 1.0642836684808594, 1.0600659902090899, 1.0578233967571533, 1.056339417639699, 1.0544947906196485, 1.053450914036322, 1.0532402737674282, 1.0508254223046116, 1.0513949849032769, 1.049703809661787, 1.0503032928619542, 1.0492397119377184, 1.0479790142674221, 1.047841469218354, 1.0459292012808015, 1.046378647326444, 1.0447095369656227, 1.0439631303967392, 1.0430926068852324, 1.041703962692245, 1.0406094298960002, 1.039575930885221, 1.0382533101820113, 1.0369937893791121, 1.0357859138101033, 1.038125813276616, 1.0369617925777084, 1.0319481397311545, 1.028989890762423, 1.0276748536793359, 1.0250443127121034, 1.022955022063833, 1.0201294628746456, 1.0188237092333408, 1.016314100876481, 1.0127335516334315, 1.0084173593677779, 1.0073128769285136, 1.004461089198839, 1.0130135394219746, 1.0019988977688783, 0.9994068014058733, 1.0016751841842761, 0.9996395326492967, 0.9974471666240105, 0.9979045916876509, 0.9931155744274539, 0.9881137907137861, 0.9943113194843582, 0.9902713175182225, 0.9908492741888308, 0.9850619822067401, 0.9828555275037792, 0.9795598372786442, 0.9821085617038015, 0.9803856197813453, 0.9727918432723326, 0.9763330186661754, 0.9739919401292194, 0.9765407157874451, 0.9661662796194793, 0.9643321052713806, 0.9572339913928288, 0.9536231956932334, 0.9502997502653995, 0.9454695243855031, 0.9428363790991859, 0.9424165131864607, 0.9569302931458553, 0.9446360239747613, 0.9447640602103984, 0.9371804318144091, 0.9406565719316627, 0.9340875170314091, 0.926218246728243, 0.9416789318501827, 0.9255890795092808, 0.9175227150045626, 0.9084538832827026, 0.911631545100124, 0.9155732360219074, 0.9068670713681215, 0.9032093104885344, 0.899560885434278, 0.9042793800209092, 0.8940594224469618, 0.8902818384846133, 0.8868083364909679, 0.8887346370753811, 0.8825617574078836, 0.8812282077340864, 0.8899882517556146, 0.8885703242779758, 0.877327654273603, 0.8868487958056236, 0.8862790627401222, 0.8815007194846073, 0.8730366862285308, 0.8694697519592192, 0.8694068101642068, 0.8680886013307121, 0.8697262130478813, 0.8680013098511118, 0.8658371837721713, 0.868024124771173, 0.8718769458285103, 0.8724261819459574, 0.8730126758130914, 0.875668854855414, 0.8611488957424673, 0.8685032675398449, 0.8605708725153788, 0.8559254496494113, 0.8564321280260105, 0.8564511878289726, 0.8579382070525716, 0.8599767561810707, 0.8524142704949976, 0.8586309237401833, 0.8529087578736292, 0.8520964477096495, 0.8556959595278792, 0.85346499032798, 0.8506788775661398, 0.8539842565446419, 0.8514479957811641, 0.8555764963984245, 0.8500743441268404, 0.8510373825410064, 0.8525655407190812, 0.8540532435969406, 0.8485291656527431, 0.8450252863905513, 0.8516779757623065, 0.8498430235185173, 0.8480037350674184, 0.848450442946667, 0.8467979994153095, 0.8467365100643228, 0.867559939537205, 0.8475885626961318, 0.8567898817130917, 0.8478415633618709, 0.8429804048499042, 0.8398754079728645, 0.8388248238230632, 0.8384347243965039, 0.8439400088370947, 0.8448788825736153, 0.8499055026493033, 0.8465734747156225, 0.8413616103802863, 0.8464420023395296, 0.8506430117990936, 0.8468971125643846, 0.8582308263014964, 0.8459838622649347, 0.8354949849586957, 0.8346775939821952, 0.8349852640770788, 0.8335379534678292, 0.836049194845086, 0.8387486493073449, 0.8361365128101999, 0.8422407797229853, 0.8395922887007069, 0.8303170393624589, 0.8326835464403125, 0.8313930661526548, 0.8303737415670125, 0.832160911325067, 0.8305463312587699, 0.830163589811423, 0.8313376284477891, 0.831820550248853, 0.8318552767716394, 0.8337679309032291, 0.829322407891863, 0.8273321153936445, 0.8300859081671713, 0.8285157924804845, 0.8305111255244308, 0.8291583510884514, 0.8261247879425848, 0.8268256178626779, 0.8263739905318196, 0.8240996077564953, 0.8224330880069145, 0.8248153621900742, 0.8306054341719625, 0.8249132262607864, 0.8311563507487397, 0.8403219671709582, 0.8495779272711987, 0.8546799553248428, 0.856166365577455, 0.8487894146349396, 0.8497669520564148, 0.8420489639716962, 0.8349479923992431, 0.8258200221972299, 0.8272211268697186, 0.8226742866592486, 0.8253574889543365, 0.8172808937957889, 0.8206935540851381, 0.8180228647999694, 0.8163883030047406, 0.8197295990323139, 0.8170345528414606, 0.8167220671074101, 0.8151413578761921, 0.8178930609622775, 0.8164980309210274, 0.8132069939705381, 0.8148234298097035, 0.8137904393844291, 0.816824280236536, 0.818108220168942, 0.8340451362931018, 0.8381345057144791, 0.825212052984649, 0.8190970339569468, 0.8161025123919304, 0.8179685824216024, 0.817437620721069, 0.8193883166420876, 0.8166213673977392, 0.8112104368650448, 0.8108542948288104, 0.8064819116856773, 0.8083817580397369, 0.8105970279392031, 0.8141678381749492, 0.8105970284042907, 0.815287288454279, 0.8289661104429429, 0.8218009670167489, 0.8188355999316034, 0.8195771795523484, 0.8184987798853331, 0.8091158491874867, 0.8063337948777592, 0.8024619897043436, 0.8022373803587175, 0.80273225473917, 0.7997511364106525, 0.799556923279772, 0.8007083620868425, 0.8013215087767255, 0.8015687190286922, 0.7985978289551314, 0.7971967659446982, 0.8016871864056441, 0.8169763291151372, 0.8173305110275378, 0.8084189295279172, 0.8018484462213222, 0.8045504316901769, 0.8090924430432016, 0.8082697661995153, 0.8024040995437262, 0.7993430267369233, 0.7952290654672, 0.8028533635932562, 0.8072115485183512, 0.8046302587099878, 0.7968206485683669, 0.7977871278717777, 0.8061525361983438, 0.8116834050331272, 0.8030493245477305, 0.802999924340532, 0.8095030574338392, 0.8093864395388343, 0.8051242381640283, 0.8064775684286192, 0.8019415187150302, 0.79333266323352, 0.7985610715417647], 'acc': [0.3449691983708611, 0.3942505148891551, 0.3942505126983478, 0.39425051312671794, 0.3942505125392389, 0.3942505125392389, 0.39425051351837065, 0.394250513714197, 0.3942505113275634, 0.3942505133225443, 0.3942505125392389, 0.3942505146933287, 0.3942505123434126, 0.3946611909406141, 0.3950718705169474, 0.39712525821564376, 0.4045174539333986, 0.4143737157879424, 0.4160164262969391, 0.42299794557158216, 0.4225872693242968, 0.42258727010760216, 0.42340862338547836, 0.4193018464214748, 0.42135523670263114, 0.42176591588731177, 0.41889116997836306, 0.4221765905312689, 0.42135523435271494, 0.4221765903354425, 0.4246406548321859, 0.4275153987828711, 0.4238193008077218, 0.4242299780341389, 0.4229979443966241, 0.4287474339869967, 0.4266940471083232, 0.42628336808275147, 0.4295687880481783, 0.4299794670370325, 0.42874743140453675, 0.4365503063436896, 0.4414784405388137, 0.4427104739805022, 0.4402464063138198, 0.4344969184858843, 0.4344969200524951, 0.44147844034298733, 0.44928131567379287, 0.45667351201574413, 0.4525667370100041, 0.4537987684934291, 0.4525667366183514, 0.4542094478739361, 0.453388091854491, 0.45995893135697447, 0.4620123212464781, 0.4772073919401032, 0.46694045145164037, 0.4780287459645673, 0.47761807128389266, 0.4731006157226876, 0.47515400302973126, 0.4739219695880428, 0.4870636571236949, 0.4776180685423238, 0.4796714581992837, 0.48870636403438245, 0.47885010495812497, 0.4817248480887873, 0.49404517228598466, 0.4833675563702593, 0.4903490772482306, 0.49445585009988086, 0.4874743343133946, 0.4948665294803878, 0.4985626294138006, 0.4915811079483502, 0.5006160165250179, 0.4993839819083713, 0.49938398699985637, 0.5084188924433025, 0.5100616033072344, 0.5100616046780189, 0.5145790567143497, 0.5170431212478105, 0.5141683761588847, 0.5215605772006683, 0.5063655031779953, 0.5236139648993647, 0.5203285447381115, 0.524845999907664, 0.5248459977535741, 0.531827516673282, 0.5244353209188097, 0.5322381889306055, 0.5310061574471805, 0.5297741313245018, 0.5388090367679479, 0.530595485777336, 0.5342915839483117, 0.5355236152359103, 0.5404517478277061, 0.5326488673319807, 0.540862426620734, 0.5474332617783204, 0.5519507151854356, 0.5540041100073154, 0.5466119138611906, 0.5457905551001766, 0.549075978786304, 0.5515400448863756, 0.5482546217877272, 0.5634496882955641, 0.5515400439072439, 0.544147839267151, 0.559342912114866, 0.5613963013801732, 0.5630390156465879, 0.5650923987678433, 0.5577002068075066, 0.5581108784773511, 0.5626283324719454, 0.5708418929356569, 0.5679671480425574, 0.563449690449654, 0.5634496886872168, 0.565092398572017, 0.5667351082609909, 0.5708418937189623, 0.5638603657178076, 0.573305950150108, 0.5790554403279596, 0.5720739182750303, 0.5708418863999526, 0.5724846034079362, 0.5823408579434702, 0.5720739240274292, 0.5720739178833776, 0.5798768013164982, 0.5770020514543052, 0.5708418931314833, 0.5806981517793707, 0.5790554379780435, 0.5753593394154151, 0.5724846028204571, 0.5782340854834727, 0.5782340831335565, 0.5712525622555852, 0.5724845990263216, 0.5646817201706418, 0.577823405123834, 0.5811088252850871, 0.5733059526958505, 0.5770020506709997, 0.5794661175543767, 0.5757700229817102, 0.5794661150086342, 0.5827515363448454, 0.5642710514381926, 0.5794661154002869, 0.5659137605396873, 0.5798767928225304, 0.5827515373239772, 0.5790554391530015, 0.5737166281598305, 0.5716632477801439, 0.5667351147966952, 0.5728952737069962, 0.5733059580565968, 0.576591374815367, 0.5765913720737982, 0.5679671494133418, 0.5728952812218323, 0.5741273079319901, 0.571252568987116, 0.5741273089111218, 0.5811088248934344, 0.5827515383031089, 0.5856262810421186, 0.5831622151378734, 0.5823408642833483, 0.5786447605557999, 0.5782340852876463, 0.5720739259856927, 0.586036964216761, 0.5843942468171247, 0.5806981500169335, 0.5839835686115759, 0.5798767941933148, 0.5839835699823602, 0.5786447674831571, 0.5815195113971248, 0.5811088264600452, 0.5774127269182852, 0.5827515363448454, 0.5811088326040968, 0.5835728925601168, 0.5835728917768114, 0.5815195029031569, 0.5839835717447974, 0.5802874729863428, 0.5790554393488279, 0.5806981557693325, 0.5893223803880523, 0.5880903451839267, 0.5831622143545679, 0.5958932242354328, 0.592607799178521, 0.5782340881026501, 0.5852156014657853, 0.5839835719406238, 0.581108828614135, 0.5577002026217185, 0.5572895291894369, 0.5634496882955641, 0.567556464867915, 0.5774127269182852, 0.578644759968321, 0.5778234098971012, 0.603285418227468, 0.5930184787548543, 0.5897330566353377, 0.585626287577823, 0.5938398328160358, 0.5958932199272532, 0.5909650950461197, 0.5930184793423333, 0.5921971243020201, 0.5872689966793178, 0.5946611878563491, 0.5979466119341292, 0.5926078013326108, 0.5938398324243831, 0.5930184779715489, 0.5880903473380166, 0.5843942462296456, 0.59425051696981, 0.586858317812855, 0.585626282412903, 0.5843942481879091, 0.5819301816961848, 0.5938398326202095, 0.6024640631871546, 0.5790554379780435, 0.596714578688267, 0.5934291583311876, 0.5909650889020681, 0.5967145767300036, 0.6008213531065281, 0.5983572879855883, 0.6032854197940787, 0.5938398359492574, 0.5975359311828379, 0.6041067758135238, 0.5917864435507286, 0.5794661161835923, 0.5893223801922259, 0.5954825434841414, 0.5995893218189294, 0.6004106733343685, 0.591786444334034, 0.6012320287663344, 0.6016427071677096, 0.6036960970204959, 0.6082135498401321, 0.6061601637080465, 0.6032854184232943, 0.6032854168566835, 0.6036960970204959, 0.6065708407386372, 0.6053388065136434, 0.5958932203189059, 0.6049281286997472, 0.5954825448549259, 0.5938398326202095, 0.5999999982620412, 0.6012320317037296, 0.6012320299412925, 0.5950718656702453, 0.5971252539564207, 0.6004106782300271, 0.6098562628581539, 0.6028747419801825, 0.606160161749783, 0.5954825434841414, 0.6004106729427158, 0.604106774834392, 0.6082135504276112, 0.6073921967580823, 0.5917864510655648, 0.597125253564768, 0.6008213556522706, 0.5991786471382549, 0.5934291567645769, 0.6094455871983475, 0.5999999988495202, 0.5995893216231031, 0.6028747421760089, 0.6057494876565874]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
