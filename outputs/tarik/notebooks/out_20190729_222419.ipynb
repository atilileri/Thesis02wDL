{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf22.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 22:24:19 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'AllShfUni', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ek', 'yd', 'eg', 'ce', 'sg', 'my', 'eb', 'ib', 'by', 'ds', 'eo', 'sk', 'ck', 'mb', 'aa'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001D18156F278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001D1E37E6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7181, Accuracy:0.0813, Validation Loss:2.7109, Validation Accuracy:0.0854\n",
    "Epoch #2: Loss:2.7067, Accuracy:0.0850, Validation Loss:2.7019, Validation Accuracy:0.0854\n",
    "Epoch #3: Loss:2.6994, Accuracy:0.0850, Validation Loss:2.6941, Validation Accuracy:0.0854\n",
    "Epoch #4: Loss:2.6906, Accuracy:0.0850, Validation Loss:2.6844, Validation Accuracy:0.0854\n",
    "Epoch #5: Loss:2.6822, Accuracy:0.0850, Validation Loss:2.6767, Validation Accuracy:0.0854\n",
    "Epoch #6: Loss:2.6741, Accuracy:0.0883, Validation Loss:2.6693, Validation Accuracy:0.0969\n",
    "Epoch #7: Loss:2.6667, Accuracy:0.1014, Validation Loss:2.6626, Validation Accuracy:0.1330\n",
    "Epoch #8: Loss:2.6596, Accuracy:0.1203, Validation Loss:2.6556, Validation Accuracy:0.1330\n",
    "Epoch #9: Loss:2.6517, Accuracy:0.1248, Validation Loss:2.6473, Validation Accuracy:0.1264\n",
    "Epoch #10: Loss:2.6432, Accuracy:0.1363, Validation Loss:2.6384, Validation Accuracy:0.1544\n",
    "Epoch #11: Loss:2.6317, Accuracy:0.1552, Validation Loss:2.6267, Validation Accuracy:0.1675\n",
    "Epoch #12: Loss:2.6190, Accuracy:0.1589, Validation Loss:2.6148, Validation Accuracy:0.1691\n",
    "Epoch #13: Loss:2.6049, Accuracy:0.1532, Validation Loss:2.6016, Validation Accuracy:0.1576\n",
    "Epoch #14: Loss:2.5901, Accuracy:0.1495, Validation Loss:2.5962, Validation Accuracy:0.1182\n",
    "Epoch #15: Loss:2.5772, Accuracy:0.1380, Validation Loss:2.5691, Validation Accuracy:0.1330\n",
    "Epoch #16: Loss:2.5657, Accuracy:0.1462, Validation Loss:2.5803, Validation Accuracy:0.1494\n",
    "Epoch #17: Loss:2.5652, Accuracy:0.1515, Validation Loss:2.6259, Validation Accuracy:0.1330\n",
    "Epoch #18: Loss:2.6025, Accuracy:0.1343, Validation Loss:2.6323, Validation Accuracy:0.1215\n",
    "Epoch #19: Loss:2.6002, Accuracy:0.1285, Validation Loss:2.5413, Validation Accuracy:0.1560\n",
    "Epoch #20: Loss:2.5270, Accuracy:0.1593, Validation Loss:2.5530, Validation Accuracy:0.1494\n",
    "Epoch #21: Loss:2.5451, Accuracy:0.1585, Validation Loss:2.5489, Validation Accuracy:0.1511\n",
    "Epoch #22: Loss:2.5266, Accuracy:0.1634, Validation Loss:2.5238, Validation Accuracy:0.1560\n",
    "Epoch #23: Loss:2.5092, Accuracy:0.1626, Validation Loss:2.5219, Validation Accuracy:0.1576\n",
    "Epoch #24: Loss:2.5085, Accuracy:0.1630, Validation Loss:2.5137, Validation Accuracy:0.1576\n",
    "Epoch #25: Loss:2.4989, Accuracy:0.1630, Validation Loss:2.5094, Validation Accuracy:0.1560\n",
    "Epoch #26: Loss:2.4926, Accuracy:0.1663, Validation Loss:2.5015, Validation Accuracy:0.1626\n",
    "Epoch #27: Loss:2.4894, Accuracy:0.1651, Validation Loss:2.5022, Validation Accuracy:0.1527\n",
    "Epoch #28: Loss:2.4866, Accuracy:0.1655, Validation Loss:2.4979, Validation Accuracy:0.1626\n",
    "Epoch #29: Loss:2.4837, Accuracy:0.1676, Validation Loss:2.4957, Validation Accuracy:0.1626\n",
    "Epoch #30: Loss:2.4808, Accuracy:0.1676, Validation Loss:2.4913, Validation Accuracy:0.1642\n",
    "Epoch #31: Loss:2.4798, Accuracy:0.1647, Validation Loss:2.4859, Validation Accuracy:0.1609\n",
    "Epoch #32: Loss:2.4758, Accuracy:0.1696, Validation Loss:2.4840, Validation Accuracy:0.1691\n",
    "Epoch #33: Loss:2.4734, Accuracy:0.1692, Validation Loss:2.4801, Validation Accuracy:0.1675\n",
    "Epoch #34: Loss:2.4705, Accuracy:0.1647, Validation Loss:2.4769, Validation Accuracy:0.1675\n",
    "Epoch #35: Loss:2.4700, Accuracy:0.1651, Validation Loss:2.4792, Validation Accuracy:0.1741\n",
    "Epoch #36: Loss:2.4683, Accuracy:0.1676, Validation Loss:2.4772, Validation Accuracy:0.1773\n",
    "Epoch #37: Loss:2.4676, Accuracy:0.1696, Validation Loss:2.4761, Validation Accuracy:0.1741\n",
    "Epoch #38: Loss:2.4666, Accuracy:0.1729, Validation Loss:2.4755, Validation Accuracy:0.1790\n",
    "Epoch #39: Loss:2.4659, Accuracy:0.1745, Validation Loss:2.4748, Validation Accuracy:0.1773\n",
    "Epoch #40: Loss:2.4670, Accuracy:0.1700, Validation Loss:2.4768, Validation Accuracy:0.1691\n",
    "Epoch #41: Loss:2.4653, Accuracy:0.1696, Validation Loss:2.4731, Validation Accuracy:0.1757\n",
    "Epoch #42: Loss:2.4654, Accuracy:0.1688, Validation Loss:2.4723, Validation Accuracy:0.1741\n",
    "Epoch #43: Loss:2.4640, Accuracy:0.1667, Validation Loss:2.4722, Validation Accuracy:0.1806\n",
    "Epoch #44: Loss:2.4642, Accuracy:0.1725, Validation Loss:2.4703, Validation Accuracy:0.1675\n",
    "Epoch #45: Loss:2.4618, Accuracy:0.1713, Validation Loss:2.4677, Validation Accuracy:0.1708\n",
    "Epoch #46: Loss:2.4609, Accuracy:0.1708, Validation Loss:2.4672, Validation Accuracy:0.1888\n",
    "Epoch #47: Loss:2.4611, Accuracy:0.1729, Validation Loss:2.4679, Validation Accuracy:0.1675\n",
    "Epoch #48: Loss:2.4602, Accuracy:0.1708, Validation Loss:2.4667, Validation Accuracy:0.1658\n",
    "Epoch #49: Loss:2.4580, Accuracy:0.1733, Validation Loss:2.4651, Validation Accuracy:0.1823\n",
    "Epoch #50: Loss:2.4583, Accuracy:0.1704, Validation Loss:2.4621, Validation Accuracy:0.1806\n",
    "Epoch #51: Loss:2.4579, Accuracy:0.1659, Validation Loss:2.4616, Validation Accuracy:0.1708\n",
    "Epoch #52: Loss:2.4575, Accuracy:0.1729, Validation Loss:2.4600, Validation Accuracy:0.1675\n",
    "Epoch #53: Loss:2.4570, Accuracy:0.1729, Validation Loss:2.4585, Validation Accuracy:0.1691\n",
    "Epoch #54: Loss:2.4560, Accuracy:0.1667, Validation Loss:2.4580, Validation Accuracy:0.1642\n",
    "Epoch #55: Loss:2.4543, Accuracy:0.1717, Validation Loss:2.4579, Validation Accuracy:0.1741\n",
    "Epoch #56: Loss:2.4525, Accuracy:0.1778, Validation Loss:2.4590, Validation Accuracy:0.1741\n",
    "Epoch #57: Loss:2.4515, Accuracy:0.1671, Validation Loss:2.4587, Validation Accuracy:0.1675\n",
    "Epoch #58: Loss:2.4503, Accuracy:0.1696, Validation Loss:2.4582, Validation Accuracy:0.1773\n",
    "Epoch #59: Loss:2.4492, Accuracy:0.1733, Validation Loss:2.4583, Validation Accuracy:0.1773\n",
    "Epoch #60: Loss:2.4484, Accuracy:0.1782, Validation Loss:2.4559, Validation Accuracy:0.1675\n",
    "Epoch #61: Loss:2.4478, Accuracy:0.1704, Validation Loss:2.4545, Validation Accuracy:0.1773\n",
    "Epoch #62: Loss:2.4464, Accuracy:0.1745, Validation Loss:2.4574, Validation Accuracy:0.1757\n",
    "Epoch #63: Loss:2.4467, Accuracy:0.1729, Validation Loss:2.4589, Validation Accuracy:0.1757\n",
    "Epoch #64: Loss:2.4472, Accuracy:0.1737, Validation Loss:2.4557, Validation Accuracy:0.1675\n",
    "Epoch #65: Loss:2.4477, Accuracy:0.1671, Validation Loss:2.4545, Validation Accuracy:0.1741\n",
    "Epoch #66: Loss:2.4506, Accuracy:0.1721, Validation Loss:2.4588, Validation Accuracy:0.1806\n",
    "Epoch #67: Loss:2.4544, Accuracy:0.1733, Validation Loss:2.4613, Validation Accuracy:0.1691\n",
    "Epoch #68: Loss:2.4542, Accuracy:0.1713, Validation Loss:2.4631, Validation Accuracy:0.1790\n",
    "Epoch #69: Loss:2.4518, Accuracy:0.1758, Validation Loss:2.4606, Validation Accuracy:0.1626\n",
    "Epoch #70: Loss:2.4516, Accuracy:0.1708, Validation Loss:2.4586, Validation Accuracy:0.1724\n",
    "Epoch #71: Loss:2.4494, Accuracy:0.1741, Validation Loss:2.4585, Validation Accuracy:0.1773\n",
    "Epoch #72: Loss:2.4496, Accuracy:0.1828, Validation Loss:2.4549, Validation Accuracy:0.1773\n",
    "Epoch #73: Loss:2.4493, Accuracy:0.1770, Validation Loss:2.4547, Validation Accuracy:0.1839\n",
    "Epoch #74: Loss:2.4482, Accuracy:0.1786, Validation Loss:2.4556, Validation Accuracy:0.1773\n",
    "Epoch #75: Loss:2.4475, Accuracy:0.1778, Validation Loss:2.4551, Validation Accuracy:0.1773\n",
    "Epoch #76: Loss:2.4471, Accuracy:0.1811, Validation Loss:2.4543, Validation Accuracy:0.1790\n",
    "Epoch #77: Loss:2.4462, Accuracy:0.1819, Validation Loss:2.4521, Validation Accuracy:0.1790\n",
    "Epoch #78: Loss:2.4452, Accuracy:0.1844, Validation Loss:2.4506, Validation Accuracy:0.1856\n",
    "Epoch #79: Loss:2.4434, Accuracy:0.1807, Validation Loss:2.4502, Validation Accuracy:0.1790\n",
    "Epoch #80: Loss:2.4405, Accuracy:0.1840, Validation Loss:2.4499, Validation Accuracy:0.1790\n",
    "Epoch #81: Loss:2.4408, Accuracy:0.1803, Validation Loss:2.4489, Validation Accuracy:0.1806\n",
    "Epoch #82: Loss:2.4413, Accuracy:0.1795, Validation Loss:2.4486, Validation Accuracy:0.1839\n",
    "Epoch #83: Loss:2.4416, Accuracy:0.1795, Validation Loss:2.4512, Validation Accuracy:0.1839\n",
    "Epoch #84: Loss:2.4492, Accuracy:0.1786, Validation Loss:2.4488, Validation Accuracy:0.1806\n",
    "Epoch #85: Loss:2.4443, Accuracy:0.1799, Validation Loss:2.4479, Validation Accuracy:0.1741\n",
    "Epoch #86: Loss:2.4432, Accuracy:0.1754, Validation Loss:2.4470, Validation Accuracy:0.1888\n",
    "Epoch #87: Loss:2.4430, Accuracy:0.1762, Validation Loss:2.4491, Validation Accuracy:0.1839\n",
    "Epoch #88: Loss:2.4419, Accuracy:0.1766, Validation Loss:2.4477, Validation Accuracy:0.1823\n",
    "Epoch #89: Loss:2.4416, Accuracy:0.1749, Validation Loss:2.4472, Validation Accuracy:0.1839\n",
    "Epoch #90: Loss:2.4410, Accuracy:0.1774, Validation Loss:2.4490, Validation Accuracy:0.1790\n",
    "Epoch #91: Loss:2.4408, Accuracy:0.1708, Validation Loss:2.4500, Validation Accuracy:0.1823\n",
    "Epoch #92: Loss:2.4411, Accuracy:0.1770, Validation Loss:2.4494, Validation Accuracy:0.1872\n",
    "Epoch #93: Loss:2.4417, Accuracy:0.1745, Validation Loss:2.4502, Validation Accuracy:0.1872\n",
    "Epoch #94: Loss:2.4412, Accuracy:0.1799, Validation Loss:2.4541, Validation Accuracy:0.1708\n",
    "Epoch #95: Loss:2.4403, Accuracy:0.1762, Validation Loss:2.4517, Validation Accuracy:0.1872\n",
    "Epoch #96: Loss:2.4417, Accuracy:0.1745, Validation Loss:2.4504, Validation Accuracy:0.1872\n",
    "Epoch #97: Loss:2.4406, Accuracy:0.1733, Validation Loss:2.4529, Validation Accuracy:0.1741\n",
    "Epoch #98: Loss:2.4380, Accuracy:0.1766, Validation Loss:2.4540, Validation Accuracy:0.1856\n",
    "Epoch #99: Loss:2.4396, Accuracy:0.1754, Validation Loss:2.4537, Validation Accuracy:0.1790\n",
    "Epoch #100: Loss:2.4391, Accuracy:0.1741, Validation Loss:2.4552, Validation Accuracy:0.1741\n",
    "Epoch #101: Loss:2.4377, Accuracy:0.1786, Validation Loss:2.4568, Validation Accuracy:0.1856\n",
    "Epoch #102: Loss:2.4382, Accuracy:0.1737, Validation Loss:2.4570, Validation Accuracy:0.1790\n",
    "Epoch #103: Loss:2.4375, Accuracy:0.1754, Validation Loss:2.4557, Validation Accuracy:0.1790\n",
    "Epoch #104: Loss:2.4384, Accuracy:0.1774, Validation Loss:2.4549, Validation Accuracy:0.1823\n",
    "Epoch #105: Loss:2.4374, Accuracy:0.1717, Validation Loss:2.4581, Validation Accuracy:0.1806\n",
    "Epoch #106: Loss:2.4379, Accuracy:0.1762, Validation Loss:2.4571, Validation Accuracy:0.1658\n",
    "Epoch #107: Loss:2.4378, Accuracy:0.1713, Validation Loss:2.4561, Validation Accuracy:0.1724\n",
    "Epoch #108: Loss:2.4369, Accuracy:0.1807, Validation Loss:2.4613, Validation Accuracy:0.1642\n",
    "Epoch #109: Loss:2.4360, Accuracy:0.1848, Validation Loss:2.4546, Validation Accuracy:0.1823\n",
    "Epoch #110: Loss:2.4380, Accuracy:0.1803, Validation Loss:2.4573, Validation Accuracy:0.1773\n",
    "Epoch #111: Loss:2.4394, Accuracy:0.1836, Validation Loss:2.4631, Validation Accuracy:0.1757\n",
    "Epoch #112: Loss:2.4434, Accuracy:0.1774, Validation Loss:2.4623, Validation Accuracy:0.1741\n",
    "Epoch #113: Loss:2.4441, Accuracy:0.1741, Validation Loss:2.4596, Validation Accuracy:0.1790\n",
    "Epoch #114: Loss:2.4431, Accuracy:0.1799, Validation Loss:2.4580, Validation Accuracy:0.1823\n",
    "Epoch #115: Loss:2.4418, Accuracy:0.1803, Validation Loss:2.4606, Validation Accuracy:0.1856\n",
    "Epoch #116: Loss:2.4443, Accuracy:0.1762, Validation Loss:2.4657, Validation Accuracy:0.1839\n",
    "Epoch #117: Loss:2.4550, Accuracy:0.1786, Validation Loss:2.4922, Validation Accuracy:0.1478\n",
    "Epoch #118: Loss:2.4716, Accuracy:0.1692, Validation Loss:2.4650, Validation Accuracy:0.1856\n",
    "Epoch #119: Loss:2.4631, Accuracy:0.1770, Validation Loss:2.4747, Validation Accuracy:0.1708\n",
    "Epoch #120: Loss:2.4505, Accuracy:0.1795, Validation Loss:2.4799, Validation Accuracy:0.1494\n",
    "Epoch #121: Loss:2.4511, Accuracy:0.1795, Validation Loss:2.4648, Validation Accuracy:0.1806\n",
    "Epoch #122: Loss:2.4492, Accuracy:0.1807, Validation Loss:2.4617, Validation Accuracy:0.1872\n",
    "Epoch #123: Loss:2.4436, Accuracy:0.1828, Validation Loss:2.4654, Validation Accuracy:0.1839\n",
    "Epoch #124: Loss:2.4448, Accuracy:0.1811, Validation Loss:2.4611, Validation Accuracy:0.1839\n",
    "Epoch #125: Loss:2.4457, Accuracy:0.1803, Validation Loss:2.4619, Validation Accuracy:0.1888\n",
    "Epoch #126: Loss:2.4421, Accuracy:0.1819, Validation Loss:2.4617, Validation Accuracy:0.1773\n",
    "Epoch #127: Loss:2.4439, Accuracy:0.1828, Validation Loss:2.4569, Validation Accuracy:0.1888\n",
    "Epoch #128: Loss:2.4414, Accuracy:0.1815, Validation Loss:2.4590, Validation Accuracy:0.1856\n",
    "Epoch #129: Loss:2.4412, Accuracy:0.1754, Validation Loss:2.4567, Validation Accuracy:0.1856\n",
    "Epoch #130: Loss:2.4398, Accuracy:0.1836, Validation Loss:2.4576, Validation Accuracy:0.1806\n",
    "Epoch #131: Loss:2.4377, Accuracy:0.1840, Validation Loss:2.4582, Validation Accuracy:0.1839\n",
    "Epoch #132: Loss:2.4365, Accuracy:0.1811, Validation Loss:2.4573, Validation Accuracy:0.1823\n",
    "Epoch #133: Loss:2.4363, Accuracy:0.1832, Validation Loss:2.4578, Validation Accuracy:0.1806\n",
    "Epoch #134: Loss:2.4354, Accuracy:0.1860, Validation Loss:2.4542, Validation Accuracy:0.1839\n",
    "Epoch #135: Loss:2.4339, Accuracy:0.1856, Validation Loss:2.4556, Validation Accuracy:0.1790\n",
    "Epoch #136: Loss:2.4329, Accuracy:0.1856, Validation Loss:2.4571, Validation Accuracy:0.1741\n",
    "Epoch #137: Loss:2.4322, Accuracy:0.1852, Validation Loss:2.4609, Validation Accuracy:0.1790\n",
    "Epoch #138: Loss:2.4306, Accuracy:0.1762, Validation Loss:2.4580, Validation Accuracy:0.1790\n",
    "Epoch #139: Loss:2.4321, Accuracy:0.1823, Validation Loss:2.4569, Validation Accuracy:0.1790\n",
    "Epoch #140: Loss:2.4305, Accuracy:0.1844, Validation Loss:2.4562, Validation Accuracy:0.1823\n",
    "Epoch #141: Loss:2.4297, Accuracy:0.1807, Validation Loss:2.4537, Validation Accuracy:0.1790\n",
    "Epoch #142: Loss:2.4289, Accuracy:0.1832, Validation Loss:2.4530, Validation Accuracy:0.1839\n",
    "Epoch #143: Loss:2.4298, Accuracy:0.1819, Validation Loss:2.4544, Validation Accuracy:0.1773\n",
    "Epoch #144: Loss:2.4294, Accuracy:0.1873, Validation Loss:2.4557, Validation Accuracy:0.1708\n",
    "Epoch #145: Loss:2.4288, Accuracy:0.1848, Validation Loss:2.4566, Validation Accuracy:0.1806\n",
    "Epoch #146: Loss:2.4309, Accuracy:0.1811, Validation Loss:2.4558, Validation Accuracy:0.1806\n",
    "Epoch #147: Loss:2.4333, Accuracy:0.1782, Validation Loss:2.4664, Validation Accuracy:0.1724\n",
    "Epoch #148: Loss:2.4761, Accuracy:0.1713, Validation Loss:2.4885, Validation Accuracy:0.1609\n",
    "Epoch #149: Loss:2.4824, Accuracy:0.1667, Validation Loss:2.4650, Validation Accuracy:0.1790\n",
    "Epoch #150: Loss:2.4679, Accuracy:0.1639, Validation Loss:2.4856, Validation Accuracy:0.1593\n",
    "Epoch #151: Loss:2.4855, Accuracy:0.1651, Validation Loss:2.5164, Validation Accuracy:0.1609\n",
    "Epoch #152: Loss:2.5827, Accuracy:0.1503, Validation Loss:2.5048, Validation Accuracy:0.1576\n",
    "Epoch #153: Loss:2.5140, Accuracy:0.1589, Validation Loss:2.4903, Validation Accuracy:0.1626\n",
    "Epoch #154: Loss:2.4692, Accuracy:0.1651, Validation Loss:2.4708, Validation Accuracy:0.1626\n",
    "Epoch #155: Loss:2.4521, Accuracy:0.1803, Validation Loss:2.4733, Validation Accuracy:0.1675\n",
    "Epoch #156: Loss:2.4512, Accuracy:0.1828, Validation Loss:2.4679, Validation Accuracy:0.1773\n",
    "Epoch #157: Loss:2.4441, Accuracy:0.1819, Validation Loss:2.4639, Validation Accuracy:0.1790\n",
    "Epoch #158: Loss:2.4418, Accuracy:0.1828, Validation Loss:2.4633, Validation Accuracy:0.1823\n",
    "Epoch #159: Loss:2.4410, Accuracy:0.1832, Validation Loss:2.4605, Validation Accuracy:0.1790\n",
    "Epoch #160: Loss:2.4379, Accuracy:0.1786, Validation Loss:2.4569, Validation Accuracy:0.1773\n",
    "Epoch #161: Loss:2.4362, Accuracy:0.1811, Validation Loss:2.4564, Validation Accuracy:0.1823\n",
    "Epoch #162: Loss:2.4363, Accuracy:0.1807, Validation Loss:2.4560, Validation Accuracy:0.1888\n",
    "Epoch #163: Loss:2.4366, Accuracy:0.1836, Validation Loss:2.4557, Validation Accuracy:0.1905\n",
    "Epoch #164: Loss:2.4363, Accuracy:0.1836, Validation Loss:2.4557, Validation Accuracy:0.1938\n",
    "Epoch #165: Loss:2.4362, Accuracy:0.1832, Validation Loss:2.4584, Validation Accuracy:0.1888\n",
    "Epoch #166: Loss:2.4362, Accuracy:0.1823, Validation Loss:2.4570, Validation Accuracy:0.1888\n",
    "Epoch #167: Loss:2.4364, Accuracy:0.1811, Validation Loss:2.4590, Validation Accuracy:0.1856\n",
    "Epoch #168: Loss:2.4357, Accuracy:0.1795, Validation Loss:2.4589, Validation Accuracy:0.1839\n",
    "Epoch #169: Loss:2.4350, Accuracy:0.1799, Validation Loss:2.4601, Validation Accuracy:0.1839\n",
    "Epoch #170: Loss:2.4347, Accuracy:0.1823, Validation Loss:2.4590, Validation Accuracy:0.1839\n",
    "Epoch #171: Loss:2.4356, Accuracy:0.1848, Validation Loss:2.4569, Validation Accuracy:0.1888\n",
    "Epoch #172: Loss:2.4346, Accuracy:0.1823, Validation Loss:2.4557, Validation Accuracy:0.1905\n",
    "Epoch #173: Loss:2.4342, Accuracy:0.1828, Validation Loss:2.4547, Validation Accuracy:0.1905\n",
    "Epoch #174: Loss:2.4340, Accuracy:0.1832, Validation Loss:2.4543, Validation Accuracy:0.1757\n",
    "Epoch #175: Loss:2.4358, Accuracy:0.1791, Validation Loss:2.4552, Validation Accuracy:0.1741\n",
    "Epoch #176: Loss:2.4352, Accuracy:0.1766, Validation Loss:2.4557, Validation Accuracy:0.1757\n",
    "Epoch #177: Loss:2.4345, Accuracy:0.1758, Validation Loss:2.4561, Validation Accuracy:0.1823\n",
    "Epoch #178: Loss:2.4353, Accuracy:0.1828, Validation Loss:2.4570, Validation Accuracy:0.1806\n",
    "Epoch #179: Loss:2.4359, Accuracy:0.1791, Validation Loss:2.4558, Validation Accuracy:0.1921\n",
    "Epoch #180: Loss:2.4374, Accuracy:0.1766, Validation Loss:2.4564, Validation Accuracy:0.1856\n",
    "Epoch #181: Loss:2.4383, Accuracy:0.1799, Validation Loss:2.4578, Validation Accuracy:0.1790\n",
    "Epoch #182: Loss:2.4372, Accuracy:0.1819, Validation Loss:2.4562, Validation Accuracy:0.1856\n",
    "Epoch #183: Loss:2.4366, Accuracy:0.1832, Validation Loss:2.4565, Validation Accuracy:0.1872\n",
    "Epoch #184: Loss:2.4361, Accuracy:0.1832, Validation Loss:2.4574, Validation Accuracy:0.1856\n",
    "Epoch #185: Loss:2.4351, Accuracy:0.1828, Validation Loss:2.4574, Validation Accuracy:0.1856\n",
    "Epoch #186: Loss:2.4351, Accuracy:0.1836, Validation Loss:2.4563, Validation Accuracy:0.1806\n",
    "Epoch #187: Loss:2.4343, Accuracy:0.1819, Validation Loss:2.4567, Validation Accuracy:0.1806\n",
    "Epoch #188: Loss:2.4350, Accuracy:0.1811, Validation Loss:2.4571, Validation Accuracy:0.1806\n",
    "Epoch #189: Loss:2.4343, Accuracy:0.1828, Validation Loss:2.4566, Validation Accuracy:0.1905\n",
    "Epoch #190: Loss:2.4344, Accuracy:0.1828, Validation Loss:2.4565, Validation Accuracy:0.1823\n",
    "Epoch #191: Loss:2.4353, Accuracy:0.1811, Validation Loss:2.4583, Validation Accuracy:0.1708\n",
    "Epoch #192: Loss:2.4339, Accuracy:0.1815, Validation Loss:2.4571, Validation Accuracy:0.1708\n",
    "Epoch #193: Loss:2.4330, Accuracy:0.1819, Validation Loss:2.4582, Validation Accuracy:0.1691\n",
    "Epoch #194: Loss:2.4327, Accuracy:0.1807, Validation Loss:2.4568, Validation Accuracy:0.1691\n",
    "Epoch #195: Loss:2.4325, Accuracy:0.1811, Validation Loss:2.4563, Validation Accuracy:0.1806\n",
    "Epoch #196: Loss:2.4326, Accuracy:0.1852, Validation Loss:2.4560, Validation Accuracy:0.1806\n",
    "Epoch #197: Loss:2.4320, Accuracy:0.1836, Validation Loss:2.4560, Validation Accuracy:0.1790\n",
    "Epoch #198: Loss:2.4322, Accuracy:0.1828, Validation Loss:2.4567, Validation Accuracy:0.1790\n",
    "Epoch #199: Loss:2.4322, Accuracy:0.1836, Validation Loss:2.4574, Validation Accuracy:0.1741\n",
    "Epoch #200: Loss:2.4325, Accuracy:0.1832, Validation Loss:2.4563, Validation Accuracy:0.1790\n",
    "Epoch #201: Loss:2.4316, Accuracy:0.1836, Validation Loss:2.4569, Validation Accuracy:0.1806\n",
    "Epoch #202: Loss:2.4319, Accuracy:0.1848, Validation Loss:2.4573, Validation Accuracy:0.1806\n",
    "Epoch #203: Loss:2.4313, Accuracy:0.1844, Validation Loss:2.4573, Validation Accuracy:0.1790\n",
    "Epoch #204: Loss:2.4315, Accuracy:0.1828, Validation Loss:2.4572, Validation Accuracy:0.1806\n",
    "Epoch #205: Loss:2.4312, Accuracy:0.1856, Validation Loss:2.4574, Validation Accuracy:0.1806\n",
    "Epoch #206: Loss:2.4314, Accuracy:0.1860, Validation Loss:2.4571, Validation Accuracy:0.1806\n",
    "Epoch #207: Loss:2.4311, Accuracy:0.1856, Validation Loss:2.4571, Validation Accuracy:0.1806\n",
    "Epoch #208: Loss:2.4307, Accuracy:0.1856, Validation Loss:2.4569, Validation Accuracy:0.1823\n",
    "Epoch #209: Loss:2.4310, Accuracy:0.1856, Validation Loss:2.4572, Validation Accuracy:0.1823\n",
    "Epoch #210: Loss:2.4309, Accuracy:0.1844, Validation Loss:2.4577, Validation Accuracy:0.1790\n",
    "Epoch #211: Loss:2.4305, Accuracy:0.1860, Validation Loss:2.4573, Validation Accuracy:0.1806\n",
    "Epoch #212: Loss:2.4309, Accuracy:0.1860, Validation Loss:2.4578, Validation Accuracy:0.1806\n",
    "Epoch #213: Loss:2.4306, Accuracy:0.1860, Validation Loss:2.4579, Validation Accuracy:0.1790\n",
    "Epoch #214: Loss:2.4307, Accuracy:0.1836, Validation Loss:2.4576, Validation Accuracy:0.1790\n",
    "Epoch #215: Loss:2.4302, Accuracy:0.1860, Validation Loss:2.4596, Validation Accuracy:0.1806\n",
    "Epoch #216: Loss:2.4305, Accuracy:0.1869, Validation Loss:2.4582, Validation Accuracy:0.1806\n",
    "Epoch #217: Loss:2.4304, Accuracy:0.1836, Validation Loss:2.4586, Validation Accuracy:0.1790\n",
    "Epoch #218: Loss:2.4310, Accuracy:0.1836, Validation Loss:2.4589, Validation Accuracy:0.1806\n",
    "Epoch #219: Loss:2.4304, Accuracy:0.1860, Validation Loss:2.4582, Validation Accuracy:0.1806\n",
    "Epoch #220: Loss:2.4304, Accuracy:0.1856, Validation Loss:2.4579, Validation Accuracy:0.1806\n",
    "Epoch #221: Loss:2.4300, Accuracy:0.1864, Validation Loss:2.4587, Validation Accuracy:0.1806\n",
    "Epoch #222: Loss:2.4311, Accuracy:0.1864, Validation Loss:2.4591, Validation Accuracy:0.1806\n",
    "Epoch #223: Loss:2.4293, Accuracy:0.1860, Validation Loss:2.4586, Validation Accuracy:0.1741\n",
    "Epoch #224: Loss:2.4302, Accuracy:0.1819, Validation Loss:2.4580, Validation Accuracy:0.1790\n",
    "Epoch #225: Loss:2.4295, Accuracy:0.1844, Validation Loss:2.4587, Validation Accuracy:0.1806\n",
    "Epoch #226: Loss:2.4298, Accuracy:0.1869, Validation Loss:2.4593, Validation Accuracy:0.1806\n",
    "Epoch #227: Loss:2.4297, Accuracy:0.1869, Validation Loss:2.4578, Validation Accuracy:0.1806\n",
    "Epoch #228: Loss:2.4297, Accuracy:0.1864, Validation Loss:2.4576, Validation Accuracy:0.1790\n",
    "Epoch #229: Loss:2.4298, Accuracy:0.1852, Validation Loss:2.4584, Validation Accuracy:0.1806\n",
    "Epoch #230: Loss:2.4302, Accuracy:0.1852, Validation Loss:2.4593, Validation Accuracy:0.1806\n",
    "Epoch #231: Loss:2.4298, Accuracy:0.1869, Validation Loss:2.4588, Validation Accuracy:0.1790\n",
    "Epoch #232: Loss:2.4295, Accuracy:0.1856, Validation Loss:2.4581, Validation Accuracy:0.1806\n",
    "Epoch #233: Loss:2.4291, Accuracy:0.1869, Validation Loss:2.4583, Validation Accuracy:0.1806\n",
    "Epoch #234: Loss:2.4293, Accuracy:0.1864, Validation Loss:2.4582, Validation Accuracy:0.1806\n",
    "Epoch #235: Loss:2.4289, Accuracy:0.1864, Validation Loss:2.4589, Validation Accuracy:0.1806\n",
    "Epoch #236: Loss:2.4289, Accuracy:0.1864, Validation Loss:2.4591, Validation Accuracy:0.1806\n",
    "Epoch #237: Loss:2.4287, Accuracy:0.1860, Validation Loss:2.4591, Validation Accuracy:0.1806\n",
    "Epoch #238: Loss:2.4291, Accuracy:0.1860, Validation Loss:2.4597, Validation Accuracy:0.1806\n",
    "Epoch #239: Loss:2.4286, Accuracy:0.1864, Validation Loss:2.4582, Validation Accuracy:0.1806\n",
    "Epoch #240: Loss:2.4292, Accuracy:0.1856, Validation Loss:2.4591, Validation Accuracy:0.1806\n",
    "Epoch #241: Loss:2.4294, Accuracy:0.1856, Validation Loss:2.4589, Validation Accuracy:0.1806\n",
    "Epoch #242: Loss:2.4281, Accuracy:0.1852, Validation Loss:2.4579, Validation Accuracy:0.1806\n",
    "Epoch #243: Loss:2.4290, Accuracy:0.1840, Validation Loss:2.4588, Validation Accuracy:0.1806\n",
    "Epoch #244: Loss:2.4280, Accuracy:0.1844, Validation Loss:2.4597, Validation Accuracy:0.1806\n",
    "Epoch #245: Loss:2.4280, Accuracy:0.1848, Validation Loss:2.4574, Validation Accuracy:0.1806\n",
    "Epoch #246: Loss:2.4277, Accuracy:0.1856, Validation Loss:2.4573, Validation Accuracy:0.1806\n",
    "Epoch #247: Loss:2.4276, Accuracy:0.1856, Validation Loss:2.4578, Validation Accuracy:0.1806\n",
    "Epoch #248: Loss:2.4276, Accuracy:0.1856, Validation Loss:2.4585, Validation Accuracy:0.1806\n",
    "Epoch #249: Loss:2.4278, Accuracy:0.1856, Validation Loss:2.4580, Validation Accuracy:0.1806\n",
    "Epoch #250: Loss:2.4276, Accuracy:0.1848, Validation Loss:2.4581, Validation Accuracy:0.1806\n",
    "Epoch #251: Loss:2.4274, Accuracy:0.1856, Validation Loss:2.4599, Validation Accuracy:0.1806\n",
    "Epoch #252: Loss:2.4273, Accuracy:0.1856, Validation Loss:2.4594, Validation Accuracy:0.1806\n",
    "Epoch #253: Loss:2.4273, Accuracy:0.1852, Validation Loss:2.4593, Validation Accuracy:0.1806\n",
    "Epoch #254: Loss:2.4270, Accuracy:0.1856, Validation Loss:2.4589, Validation Accuracy:0.1806\n",
    "Epoch #255: Loss:2.4267, Accuracy:0.1856, Validation Loss:2.4583, Validation Accuracy:0.1806\n",
    "Epoch #256: Loss:2.4264, Accuracy:0.1856, Validation Loss:2.4596, Validation Accuracy:0.1806\n",
    "Epoch #257: Loss:2.4267, Accuracy:0.1852, Validation Loss:2.4592, Validation Accuracy:0.1806\n",
    "Epoch #258: Loss:2.4265, Accuracy:0.1852, Validation Loss:2.4591, Validation Accuracy:0.1806\n",
    "Epoch #259: Loss:2.4264, Accuracy:0.1852, Validation Loss:2.4602, Validation Accuracy:0.1806\n",
    "Epoch #260: Loss:2.4264, Accuracy:0.1852, Validation Loss:2.4602, Validation Accuracy:0.1806\n",
    "Epoch #261: Loss:2.4264, Accuracy:0.1856, Validation Loss:2.4598, Validation Accuracy:0.1806\n",
    "Epoch #262: Loss:2.4268, Accuracy:0.1856, Validation Loss:2.4590, Validation Accuracy:0.1806\n",
    "Epoch #263: Loss:2.4277, Accuracy:0.1856, Validation Loss:2.4597, Validation Accuracy:0.1823\n",
    "Epoch #264: Loss:2.4277, Accuracy:0.1856, Validation Loss:2.4610, Validation Accuracy:0.1806\n",
    "Epoch #265: Loss:2.4281, Accuracy:0.1860, Validation Loss:2.4618, Validation Accuracy:0.1757\n",
    "Epoch #266: Loss:2.4286, Accuracy:0.1869, Validation Loss:2.4629, Validation Accuracy:0.1806\n",
    "Epoch #267: Loss:2.4289, Accuracy:0.1860, Validation Loss:2.4628, Validation Accuracy:0.1790\n",
    "Epoch #268: Loss:2.4286, Accuracy:0.1864, Validation Loss:2.4600, Validation Accuracy:0.1823\n",
    "Epoch #269: Loss:2.4282, Accuracy:0.1860, Validation Loss:2.4610, Validation Accuracy:0.1741\n",
    "Epoch #270: Loss:2.4275, Accuracy:0.1848, Validation Loss:2.4590, Validation Accuracy:0.1724\n",
    "Epoch #271: Loss:2.4275, Accuracy:0.1844, Validation Loss:2.4598, Validation Accuracy:0.1741\n",
    "Epoch #272: Loss:2.4272, Accuracy:0.1852, Validation Loss:2.4571, Validation Accuracy:0.1823\n",
    "Epoch #273: Loss:2.4272, Accuracy:0.1844, Validation Loss:2.4575, Validation Accuracy:0.1773\n",
    "Epoch #274: Loss:2.4280, Accuracy:0.1844, Validation Loss:2.4576, Validation Accuracy:0.1823\n",
    "Epoch #275: Loss:2.4279, Accuracy:0.1848, Validation Loss:2.4565, Validation Accuracy:0.1823\n",
    "Epoch #276: Loss:2.4276, Accuracy:0.1848, Validation Loss:2.4576, Validation Accuracy:0.1823\n",
    "Epoch #277: Loss:2.4277, Accuracy:0.1844, Validation Loss:2.4566, Validation Accuracy:0.1823\n",
    "Epoch #278: Loss:2.4275, Accuracy:0.1844, Validation Loss:2.4558, Validation Accuracy:0.1823\n",
    "Epoch #279: Loss:2.4270, Accuracy:0.1844, Validation Loss:2.4601, Validation Accuracy:0.1757\n",
    "Epoch #280: Loss:2.4275, Accuracy:0.1860, Validation Loss:2.4638, Validation Accuracy:0.1773\n",
    "Epoch #281: Loss:2.4274, Accuracy:0.1819, Validation Loss:2.4586, Validation Accuracy:0.1708\n",
    "Epoch #282: Loss:2.4292, Accuracy:0.1803, Validation Loss:2.4577, Validation Accuracy:0.1741\n",
    "Epoch #283: Loss:2.4309, Accuracy:0.1807, Validation Loss:2.4602, Validation Accuracy:0.1741\n",
    "Epoch #284: Loss:2.4308, Accuracy:0.1803, Validation Loss:2.4596, Validation Accuracy:0.1741\n",
    "Epoch #285: Loss:2.4301, Accuracy:0.1799, Validation Loss:2.4583, Validation Accuracy:0.1741\n",
    "Epoch #286: Loss:2.4287, Accuracy:0.1815, Validation Loss:2.4595, Validation Accuracy:0.1708\n",
    "Epoch #287: Loss:2.4285, Accuracy:0.1807, Validation Loss:2.4596, Validation Accuracy:0.1658\n",
    "Epoch #288: Loss:2.4291, Accuracy:0.1811, Validation Loss:2.4601, Validation Accuracy:0.1724\n",
    "Epoch #289: Loss:2.4281, Accuracy:0.1807, Validation Loss:2.4623, Validation Accuracy:0.1675\n",
    "Epoch #290: Loss:2.4276, Accuracy:0.1799, Validation Loss:2.4604, Validation Accuracy:0.1675\n",
    "Epoch #291: Loss:2.4290, Accuracy:0.1811, Validation Loss:2.4607, Validation Accuracy:0.1642\n",
    "Epoch #292: Loss:2.4300, Accuracy:0.1815, Validation Loss:2.4647, Validation Accuracy:0.1626\n",
    "Epoch #293: Loss:2.4286, Accuracy:0.1803, Validation Loss:2.4620, Validation Accuracy:0.1658\n",
    "Epoch #294: Loss:2.4294, Accuracy:0.1807, Validation Loss:2.4624, Validation Accuracy:0.1658\n",
    "Epoch #295: Loss:2.4285, Accuracy:0.1815, Validation Loss:2.4630, Validation Accuracy:0.1675\n",
    "Epoch #296: Loss:2.4283, Accuracy:0.1823, Validation Loss:2.4635, Validation Accuracy:0.1724\n",
    "Epoch #297: Loss:2.4292, Accuracy:0.1815, Validation Loss:2.4618, Validation Accuracy:0.1675\n",
    "Epoch #298: Loss:2.4276, Accuracy:0.1819, Validation Loss:2.4611, Validation Accuracy:0.1708\n",
    "Epoch #299: Loss:2.4304, Accuracy:0.1823, Validation Loss:2.4588, Validation Accuracy:0.1741\n",
    "Epoch #300: Loss:2.4280, Accuracy:0.1807, Validation Loss:2.4600, Validation Accuracy:0.1658\n",
    "\n",
    "Test:\n",
    "Test Loss:2.46002483, Accuracy:0.1658\n",
    "Labels: ['ek', 'yd', 'eg', 'ce', 'sg', 'my', 'eb', 'ib', 'by', 'ds', 'eo', 'sk', 'ck', 'mb', 'aa']\n",
    "Confusion Matrix:\n",
    "      ek  yd  eg  ce  sg  my  eb  ib  by  ds  eo  sk  ck  mb  aa\n",
    "t:ek   0   4  26   0   6   0   0   0   1   0  10   0   0   0   1\n",
    "t:yd   0  34   4   0  16   0   0   0   0   1   7   0   0   0   0\n",
    "t:eg   0   0  28   0   7   0   0   0   1   5   5   0   0   0   4\n",
    "t:ce   0   2   4   0  11   0   0   0   0   2   7   0   0   0   1\n",
    "t:sg   0   9   6   0  17   0   0   0   0   1  18   0   0   0   0\n",
    "t:my   0   2   6   0   7   0   0   0   0   2   3   0   0   0   0\n",
    "t:eb   0   7  21   0  11   0   0   0   0   1  10   0   0   0   0\n",
    "t:ib   0  26   3   0  21   0   0   0   0   0   4   0   0   0   0\n",
    "t:by   0   1  14   0   6   0   0   0   2   1  16   0   0   0   0\n",
    "t:ds   0   1  12   0   2   0   0   0   0   6   9   0   0   0   1\n",
    "t:eo   0   1   8   0  11   0   0   0   1   0  13   0   0   0   0\n",
    "t:sk   0   1  16   0   8   0   0   0   0   6   2   0   0   0   0\n",
    "t:ck   0   0  10   0   8   0   0   0   0   0   4   0   0   0   1\n",
    "t:mb   0   7  10   0  19   0   0   0   1   1  14   0   0   0   0\n",
    "t:aa   0   1  17   0   4   0   0   0   0   6   5   0   0   0   1\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          yd       0.35      0.55      0.43        62\n",
    "          eg       0.15      0.56      0.24        50\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sg       0.11      0.33      0.17        51\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          by       0.33      0.05      0.09        40\n",
    "          ds       0.19      0.19      0.19        31\n",
    "          eo       0.10      0.38      0.16        34\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          aa       0.11      0.03      0.05        34\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.09      0.14      0.09       609\n",
    "weighted avg       0.10      0.17      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 23:04:59 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 40 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7109475233676203, 2.7019037934163914, 2.6941456814313365, 2.684381551147486, 2.6766625542946048, 2.669251809174987, 2.662605532675933, 2.6555610818815936, 2.6472771809801876, 2.6384316469452456, 2.6267408600385944, 2.614769232488422, 2.601591953503087, 2.59616724219424, 2.5691188552305224, 2.580320007891099, 2.6258632948833145, 2.6322876467493366, 2.5413085654842833, 2.5530052576550513, 2.5488641191585897, 2.523776118195507, 2.521923643810604, 2.513737087766525, 2.5094361880729936, 2.5014890359931785, 2.5021906666372016, 2.4979075148383583, 2.495693580270401, 2.4913243597559935, 2.485937248897083, 2.4840185614837997, 2.480101361063314, 2.4768810236982524, 2.4792428768327084, 2.477201974646407, 2.4760736362100233, 2.4755340628631792, 2.4747789818273587, 2.4768266869687485, 2.4730777243283777, 2.4723226992759018, 2.472200386629903, 2.4703194359057448, 2.4677118581699817, 2.467169408532003, 2.467909151697394, 2.4666921603072844, 2.465092808937987, 2.462058465077568, 2.4615658253480257, 2.459958997266046, 2.458460270281887, 2.4580064207462256, 2.457915821686167, 2.45896601285449, 2.4587080275092417, 2.4582181705042645, 2.4582616766098098, 2.4559067043373344, 2.4545091517844617, 2.457366366300285, 2.4588521474296434, 2.4557411897750128, 2.4545441950091784, 2.458834926483079, 2.4613112361951806, 2.463141962812452, 2.460599393484432, 2.458649041030207, 2.4584750459699207, 2.4549225508090116, 2.454738815038271, 2.4555804514141113, 2.45509846731164, 2.4542860886929265, 2.4520584483843524, 2.4506157611195483, 2.45019257362253, 2.449895885777591, 2.4488770664227615, 2.4486204630439894, 2.45124976114295, 2.448816038313366, 2.447927024759878, 2.4469948848480074, 2.44909975094161, 2.44771273421928, 2.447205363431783, 2.448953829375394, 2.449959572117121, 2.4494323558212305, 2.4501927897260694, 2.454086740224428, 2.451679187063709, 2.450421836771597, 2.4529459202426604, 2.4540379125692184, 2.453673887331106, 2.4551988082566285, 2.4568205267337744, 2.4569918565170714, 2.4556742896783135, 2.454944264516846, 2.458093466234129, 2.457059422541526, 2.4560897009713307, 2.461300950919466, 2.454582467259249, 2.4572683085361726, 2.463148024281845, 2.462326265908227, 2.4595910852961547, 2.4579601984697415, 2.460617585330957, 2.4656843500967294, 2.492184111245943, 2.4649641310248667, 2.4746937967090576, 2.4798664933159236, 2.4647838775747517, 2.461673992607981, 2.4653622536432174, 2.461108747569994, 2.46185477300622, 2.4616705044149767, 2.456884737672477, 2.4590057864760726, 2.45667536622785, 2.4575762744803344, 2.458222400182965, 2.457269787984137, 2.457791497163193, 2.454248192666591, 2.455603370134075, 2.4570919469072314, 2.4609255587134653, 2.457984793558105, 2.4569427610813888, 2.4562051116147847, 2.4537166801383736, 2.4530328608107292, 2.454438114401155, 2.45566878686789, 2.456557376435629, 2.455817679466285, 2.466438826669026, 2.488475267131536, 2.46499284147629, 2.4855676398097195, 2.516433001347559, 2.5048020179635784, 2.4903467014701106, 2.4708258072143705, 2.4732672898052948, 2.4678648966678063, 2.4638608617735613, 2.4632790296144282, 2.4605042339350005, 2.456874918663639, 2.4563687399690375, 2.4560445877914554, 2.4557273728506908, 2.45565161368334, 2.4583828061672266, 2.457014822216065, 2.459030870537844, 2.458946008400377, 2.460147197610639, 2.4590259303013093, 2.456872009487183, 2.4557431953881173, 2.454721827812383, 2.4542780470574037, 2.4552297553013895, 2.4556807700440606, 2.4561077738043124, 2.457005985460454, 2.455763062428567, 2.4563664039367525, 2.4578302932294522, 2.4562040698547865, 2.456465605640255, 2.457393263361137, 2.457400107422877, 2.456281362104494, 2.4567302169862444, 2.4571383249974996, 2.4565816469771913, 2.45650140718482, 2.4583082281309983, 2.4571416518958333, 2.4582338438832703, 2.4568076642667522, 2.456273572589768, 2.4559600611625636, 2.4559507287781814, 2.4567035176288123, 2.4574071401837227, 2.456273268792038, 2.4568768408889645, 2.4573381941502515, 2.457347361325043, 2.457227822790788, 2.457371045020218, 2.4570861097627086, 2.4571134496009215, 2.456907707677882, 2.45718403872598, 2.457667973632687, 2.4573087719665176, 2.457783258607235, 2.457915836954352, 2.4575578626153503, 2.459606506945856, 2.4582081725836193, 2.4585520641752847, 2.4589282730334303, 2.458171520327112, 2.4579374230358204, 2.4587188467799344, 2.4591229639225602, 2.4586066448042545, 2.458030066858176, 2.4587463584831, 2.459280941090952, 2.457757748210763, 2.457559503357986, 2.458374391048413, 2.4592659011458724, 2.4588236656095006, 2.458075985728422, 2.4583242291887406, 2.458221615633158, 2.458932258029681, 2.4591296866218055, 2.459122194640938, 2.459650306278849, 2.458198817101214, 2.4591338560107503, 2.4588784372865273, 2.457913314571913, 2.4587832119664537, 2.4596840176480548, 2.4573978923615956, 2.457250377423266, 2.457844823843544, 2.4584801725566092, 2.458033576191744, 2.458133948064594, 2.459854376139899, 2.4593616416693127, 2.459311490771414, 2.4589142533162938, 2.4582749207814536, 2.459647745921694, 2.4591686533785415, 2.4590981734797284, 2.4602053098882166, 2.460193624245905, 2.4597562506476844, 2.4590189762303396, 2.4596709332051145, 2.461009905256074, 2.4618437650364218, 2.462931679191652, 2.462770255719891, 2.459978153357169, 2.46104793595563, 2.4589611642270643, 2.4598377964570997, 2.4571112838676217, 2.457530940107524, 2.4576337529324936, 2.4564951942080544, 2.4575723027948086, 2.4565899301632284, 2.4557577001637427, 2.460134233355718, 2.4638062032376995, 2.4586331092665348, 2.4576623846940415, 2.460150065680443, 2.4595838661851555, 2.458302491991391, 2.4595482819186056, 2.4596026523164145, 2.4601463269326094, 2.4623479075815484, 2.4603777953556607, 2.4607384968273744, 2.4646873270545298, 2.4619998051027947, 2.46240292707296, 2.4630198204654388, 2.4635087034385195, 2.461818195133178, 2.461068326420776, 2.4587637043155866, 2.460024819976982], 'val_acc': [0.08538587808560072, 0.08538587808560072, 0.08538587808560072, 0.08538587808560072, 0.08538587808560072, 0.09688013105703693, 0.13300492510517634, 0.13300492520304932, 0.1264367814012153, 0.15435139462963504, 0.167487683725866, 0.1691297199485337, 0.15763546697709752, 0.11822660097298755, 0.13300492500730335, 0.1494252863531238, 0.13300492500730335, 0.12151067213374014, 0.15599343085230277, 0.1494252863531238, 0.15106732247791854, 0.15599343095017576, 0.1576354670749705, 0.1576354670749705, 0.15599343085230277, 0.16256157544935473, 0.15270935870058627, 0.16256157544935473, 0.16256157544935473, 0.16420361157414948, 0.16091953932456, 0.1691297199485337, 0.16748768382373896, 0.16748768382373896, 0.17405582842079093, 0.17733990096399938, 0.1740558287144099, 0.17898193699092113, 0.1773399008661264, 0.16912972014427968, 0.17569786464345866, 0.1740558285186639, 0.18062397321358886, 0.16748768401948494, 0.17077175626907443, 0.18883415373968962, 0.16748768392161195, 0.16584564769894422, 0.18226600924051062, 0.1806239730178429, 0.17077175617120144, 0.167487683725866, 0.1691297199485337, 0.16420361176989545, 0.1740558286165369, 0.1740558287144099, 0.16748768401948494, 0.17733990096399938, 0.1773399007682534, 0.16748768411735793, 0.17733990096399938, 0.17569786483920463, 0.17569786483920463, 0.16748768401948494, 0.1740558286165369, 0.18062397321358886, 0.16912972014427968, 0.17898193699092113, 0.1625615756451007, 0.17241379258961514, 0.1773399008661264, 0.1773399008661264, 0.18390804556105134, 0.17733990096399938, 0.17733990096399938, 0.17898193699092113, 0.17898193699092113, 0.18555008129435416, 0.17898193699092113, 0.17898193689304814, 0.18062397311571587, 0.18390804536530536, 0.1839080452674324, 0.1806239730178429, 0.17405582842079093, 0.18883415393543557, 0.1839080452674324, 0.18226600924051062, 0.18390804536530536, 0.17898193699092113, 0.18226600904476467, 0.18719211781064082, 0.18719211781064082, 0.17077175607332848, 0.18719211781064082, 0.18719211781064082, 0.1740558285186639, 0.1855500815879731, 0.17898193708879412, 0.1740558285186639, 0.1855500815879731, 0.17898193708879412, 0.17898193699092113, 0.1822660094362566, 0.18062397331146185, 0.1658456478946902, 0.17241379258961514, 0.16420361176989545, 0.18226600953412955, 0.17733990067038044, 0.17569786464345866, 0.1740558285186639, 0.17898193699092113, 0.18226600924051062, 0.1855500814901001, 0.18390804516955941, 0.14778325121929298, 0.18555008139222715, 0.17077175607332848, 0.1494252863531238, 0.1806239730178429, 0.18719211761489485, 0.1839080452674324, 0.18390804516955941, 0.18883415373968962, 0.17733990067038044, 0.18883415373968962, 0.18555008129435416, 0.18555008139222715, 0.18062397291996993, 0.1839080452674324, 0.18226600914263763, 0.1806239730178429, 0.1839080452674324, 0.17898193689304814, 0.17405582842079093, 0.17898193679517518, 0.17898193708879412, 0.17898193699092113, 0.18226600924051062, 0.17898193699092113, 0.18390804546317835, 0.17733990067038044, 0.17077175617120144, 0.18062397311571587, 0.18062397291996993, 0.17241379239386917, 0.16091953942243298, 0.17898193699092113, 0.15927750409285618, 0.16091953932456, 0.15763546787018845, 0.16256157535148175, 0.16256157584084666, 0.16748768411735793, 0.1773399007682534, 0.17898193689304814, 0.18226600924051062, 0.17898193689304814, 0.1773399008661264, 0.1822660093383836, 0.18883415383756258, 0.19047618986448436, 0.1937602622119468, 0.18883415383756258, 0.18883415383756258, 0.1855500815879731, 0.18390804546317835, 0.18390804546317835, 0.18390804536530536, 0.18883415373968962, 0.19047618986448436, 0.19047618976661138, 0.17569786464345866, 0.1740558285186639, 0.17569786474133164, 0.18226600924051062, 0.18062397311571587, 0.1921182259892791, 0.18555008168584608, 0.17898193699092113, 0.1855500814901001, 0.18719211761489485, 0.1855500814901001, 0.1855500814901001, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.19047618986448436, 0.18226600924051062, 0.17077175626907443, 0.1707717563669474, 0.16912972014427968, 0.16912972014427968, 0.18062397311571587, 0.18062397311571587, 0.17898193699092113, 0.17898193699092113, 0.1740558285186639, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18226600924051062, 0.18226600924051062, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.17898193699092113, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.1740558285186639, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.17898193699092113, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18062397311571587, 0.18226600924051062, 0.18062397311571587, 0.17569786474133164, 0.18062397311571587, 0.17898193699092113, 0.18226600924051062, 0.1740558286165369, 0.17241379249174216, 0.1740558286165369, 0.18226600924051062, 0.1773399008661264, 0.18226600924051062, 0.18226600924051062, 0.18226600924051062, 0.18226600924051062, 0.18226600924051062, 0.17569786474133164, 0.1773399008661264, 0.17077175626907443, 0.1740558285186639, 0.1740558285186639, 0.1740558285186639, 0.1740558285186639, 0.17077175626907443, 0.1658456478946902, 0.17241379239386917, 0.16748768401948494, 0.16748768401948494, 0.16420361176989545, 0.1625615756451007, 0.1658456478946902, 0.1658456478946902, 0.16748768401948494, 0.17241379239386917, 0.16748768401948494, 0.17077175626907443, 0.1740558285186639, 0.1658456478946902], 'loss': [2.718051924108235, 2.7067468569753594, 2.699421324720128, 2.690640321355581, 2.6822114605678427, 2.674079952690391, 2.666726244548508, 2.659600881972107, 2.6517217460843816, 2.6432324102037494, 2.6316966209568284, 2.618980548513988, 2.6049182814494296, 2.5900758872531524, 2.5771861213433422, 2.5656783466221618, 2.565167959906482, 2.6024788056555717, 2.600197645575114, 2.527047727338098, 2.5451249530917566, 2.526596218555615, 2.5091528350078103, 2.5085257145413626, 2.4988977139490585, 2.4926041741145952, 2.489393517417829, 2.486576577525364, 2.4836662464807655, 2.480797456177353, 2.479825819982885, 2.4758469935070564, 2.473390202649565, 2.47051486440263, 2.469984421837746, 2.4683116685683233, 2.4675665574396906, 2.46663596879775, 2.465880625154938, 2.4669890924645643, 2.465253764941707, 2.465445716865743, 2.463996358573804, 2.464181127391557, 2.461848050319194, 2.460857435810003, 2.461108061224528, 2.460192939241319, 2.457972255526627, 2.4583157373894413, 2.4579448455168236, 2.4574840263664357, 2.45695760372483, 2.4560015805692887, 2.454268821211077, 2.4525281516433495, 2.4515298246113426, 2.450321296010419, 2.449219415613758, 2.4484006624202217, 2.4477551888146682, 2.4464050234220847, 2.446711661634504, 2.447235804465762, 2.4477342014195247, 2.4506357510231846, 2.454364896848706, 2.4542202452125, 2.45177469684358, 2.451569050587178, 2.449363285413268, 2.449639297951418, 2.449255472923451, 2.4481965668147594, 2.4474707999023813, 2.4471022273970338, 2.4461706606024833, 2.4452237641297327, 2.4433563811088734, 2.4405329461459995, 2.4408067261658655, 2.4412716237916103, 2.441628594075385, 2.4491792486923187, 2.4443325164136946, 2.4431611732780567, 2.442964395409492, 2.4419205893726073, 2.4416025689495173, 2.441048964337891, 2.4407615247448367, 2.4410569264413886, 2.4417275088034125, 2.4412089813906066, 2.440275125483957, 2.441707998330588, 2.440580578751143, 2.437974927459654, 2.4396423585605818, 2.4391194416022643, 2.437672011024898, 2.4381839775696426, 2.4375225198342325, 2.4384307669418304, 2.437389871374048, 2.4379106200451233, 2.4377563917171785, 2.436931735925851, 2.436049699097933, 2.4380335992856192, 2.439371662609876, 2.443361624751003, 2.4440913896541088, 2.4431253345105683, 2.441780104627355, 2.4442926254115798, 2.455026076314875, 2.4716004967934295, 2.463057419700544, 2.4505185524785786, 2.4510809514556824, 2.449205650783907, 2.4436016977690085, 2.4447718750524814, 2.4457142967952596, 2.4420575539434224, 2.4439326908064576, 2.4413514534795553, 2.4412420704624247, 2.4398370462766175, 2.4377453413342547, 2.4364959648257654, 2.4362504763280097, 2.435400534557366, 2.4339496881810057, 2.432863565538943, 2.432201070903018, 2.430551894930109, 2.432104436083251, 2.430529776637804, 2.429702472980507, 2.42894538511241, 2.429797739757405, 2.4293524003861133, 2.4287585542432093, 2.430908463867783, 2.43326108842415, 2.476104991000291, 2.482412058714724, 2.4678766915440806, 2.4855445348751375, 2.5827444503439527, 2.5140179417706126, 2.469175717473275, 2.4521392992634548, 2.451167445212174, 2.444062429091279, 2.441812955868073, 2.4409555442034585, 2.437939892265586, 2.4361878861147277, 2.4363097387662416, 2.4366200585140096, 2.43632533212462, 2.436190079859395, 2.43616848843788, 2.4363750487137623, 2.4357259865903758, 2.4350360161469946, 2.4346890382942967, 2.4355551213454416, 2.4346046672954205, 2.434160469493827, 2.4340139393933744, 2.4357990853350753, 2.4352440669551276, 2.43451775035819, 2.4352859032961867, 2.4359360380583963, 2.4374033395269814, 2.438261109651726, 2.4371950831990956, 2.436606473746486, 2.4360642821881804, 2.4350979544543634, 2.4351313184663743, 2.4342721175364157, 2.4349577766669115, 2.4342578085051425, 2.4343674233806696, 2.4353315233939483, 2.433876797403888, 2.432985558206296, 2.4326886882037844, 2.4325122703027433, 2.432622923449569, 2.431960378486273, 2.432180714949935, 2.4322334387463957, 2.432506902115056, 2.431574142933871, 2.4319197514708284, 2.431330179531716, 2.4314979170382145, 2.431230014746194, 2.4313602671730936, 2.431085080336741, 2.430687330880449, 2.4310179294257193, 2.4308763798746975, 2.430503971229099, 2.4309122286293294, 2.4305863636964644, 2.430651161166432, 2.430248975753784, 2.43049220872366, 2.430422290394683, 2.4309611011089975, 2.4303829336068468, 2.430405053563676, 2.430002166407309, 2.4311013538979407, 2.429346995089333, 2.4302404156945325, 2.429515138837591, 2.429806477235328, 2.429718540336562, 2.4296592398101056, 2.429782534722675, 2.4301959784858282, 2.429819787160572, 2.4294944465527544, 2.4290988041879706, 2.429260993052802, 2.428894921543662, 2.4288835822189614, 2.428747231563748, 2.429083275060634, 2.4285764715754765, 2.4291899301188193, 2.4293826868891473, 2.428145771104942, 2.4289596599719854, 2.428023023624929, 2.4280388347422073, 2.4277045987225168, 2.4275566035472393, 2.427611333780465, 2.4278004970393874, 2.4276395198500866, 2.4273595111815593, 2.4272841717917815, 2.4272552744808626, 2.4269588210009942, 2.426747878425175, 2.426415459086518, 2.4267008241931514, 2.4264918283270616, 2.4264154154172424, 2.4263672916796173, 2.426374320866391, 2.426753339924117, 2.427692710594475, 2.427745999545777, 2.428076113225009, 2.4285852115012294, 2.4289441930195146, 2.428556928987131, 2.4281632133577884, 2.4274505621109164, 2.4274829606011172, 2.4272172709510067, 2.427198134408594, 2.4279640838106067, 2.427941985688415, 2.4276037480552093, 2.427680284923107, 2.427461776694233, 2.4270123919422377, 2.427475539761647, 2.4274180050013734, 2.4292358484601095, 2.430934428581222, 2.430847699931025, 2.430101151730735, 2.4286629597509175, 2.428527306824984, 2.4291149812067805, 2.428053930603748, 2.4276398206393575, 2.4290219055308944, 2.429997548528276, 2.428646181349392, 2.4293559029361798, 2.4285103550192266, 2.428254751894753, 2.4291507888378794, 2.427604847424329, 2.430365348400766, 2.4280005333604753], 'acc': [0.08131416852775296, 0.0850102670903813, 0.08501026728620764, 0.08501026649372288, 0.08501026728620764, 0.0882956876249284, 0.10143737237311487, 0.12032854163854763, 0.12484599641644735, 0.13634497030070186, 0.15523613873693243, 0.15893223712209315, 0.15318275144824747, 0.1494866530998042, 0.13798767936547923, 0.14620123235107202, 0.15154004117179457, 0.134291580643742, 0.12854209553901663, 0.15934291609258866, 0.15852156048315508, 0.1634496924874719, 0.16262833605801544, 0.16303901349861763, 0.1630390140860967, 0.1663244342289911, 0.16509240137478165, 0.16550308057782095, 0.16755646825815862, 0.1675564688639964, 0.16468172534168135, 0.1696098567401604, 0.1691991791404493, 0.16468172573333403, 0.16509240297810987, 0.16755646749321196, 0.1696098569727042, 0.1728952767239459, 0.17453798876283594, 0.17002053380746862, 0.16960985597521372, 0.1687885009165417, 0.1667351122387136, 0.17248459910587607, 0.17125256603748157, 0.17084188998602254, 0.17289527731142496, 0.1708418882235854, 0.17330595412783065, 0.17043121040968925, 0.16591375700257396, 0.1728952767055872, 0.17289527611810812, 0.16673511225707233, 0.1716632432822574, 0.177823407749131, 0.1671457914233942, 0.16960985617104007, 0.1733059543420157, 0.1782340869338116, 0.17043121178047368, 0.17453798756951913, 0.17289527788054526, 0.1737166331166849, 0.1671457908542739, 0.17207392227111165, 0.17330595530278875, 0.1712525660558403, 0.17577001944459683, 0.1708418889885321, 0.17412731036146073, 0.1827515409284059, 0.17700205229880628, 0.17864476335856458, 0.17782340773077226, 0.18110882887115715, 0.18193018588809262, 0.18439425018900962, 0.18069815262387176, 0.18398357219764583, 0.18028747424085528, 0.179466120161315, 0.17946611800722517, 0.17864476316273825, 0.17987679740609086, 0.17535934182652702, 0.17618069745431936, 0.17659137507238917, 0.17494866557924166, 0.17741273169767197, 0.17084189000438127, 0.17700205249463263, 0.17453798680457247, 0.179876795625295, 0.1761806980417984, 0.17453798837118326, 0.17330595375453667, 0.17659137605152092, 0.17535934380314924, 0.1741273089906763, 0.17864476414187, 0.17371663174590046, 0.17535934221817973, 0.17741273011270245, 0.17166324543634723, 0.1761806992351152, 0.1712525666249606, 0.18069815301552444, 0.18480492782543817, 0.1802874748099756, 0.18357289575453412, 0.17741273050435516, 0.17412731075311344, 0.17987679781610227, 0.18028747482833432, 0.17618069825598345, 0.17864476457024012, 0.1691991791404493, 0.17700205447125483, 0.17946611800722517, 0.17946611918218328, 0.18069815125308733, 0.18275153953926274, 0.18110882887115715, 0.1802874744366816, 0.18193018449894946, 0.18275153893342497, 0.18151950648922696, 0.17535934241400608, 0.18357289438374969, 0.1839835735684303, 0.18110883024194158, 0.18316221874230207, 0.1860369610713003, 0.18562628366741557, 0.1856262830615778, 0.1852156046602026, 0.17618069704430794, 0.18234086350616244, 0.18439424960153059, 0.18069815180384893, 0.1831622185464757, 0.1819301854780812, 0.18726899331967198, 0.18480492803962323, 0.18110883004611522, 0.1782340869338116, 0.17125256783663614, 0.16673511321784534, 0.16386036951806265, 0.1650924019622607, 0.150308007515921, 0.15893223868870393, 0.1650924023539134, 0.18028747422249655, 0.18275154110587352, 0.18193018432148184, 0.18275154110587352, 0.1831622185464757, 0.1786447629669119, 0.18110882887115715, 0.18069815223221905, 0.18357289577289285, 0.18357289634201315, 0.18316221776317032, 0.1823408629186834, 0.1811088304194092, 0.17946611996548867, 0.179876795625295, 0.18234086170700786, 0.18480492743378546, 0.18234086311450973, 0.1827515397350891, 0.18316221713897385, 0.1790554405849817, 0.17659137626570598, 0.17577002042372858, 0.18275154112423225, 0.1790554409582757, 0.17659137528657423, 0.1798767977793848, 0.18193018567390756, 0.18316221696150622, 0.18316221754898526, 0.1827515397350891, 0.18357289497122872, 0.1819301854964399, 0.18110883006447395, 0.18275153952090403, 0.18275153993091545, 0.18110883026030028, 0.18151950688087964, 0.18193018392982913, 0.18069815084307592, 0.18110882945863618, 0.18521560505185528, 0.18357289495287, 0.1827515389517837, 0.18357289418792333, 0.18316221874230207, 0.18357289554034906, 0.1848049291962226, 0.1843942499931833, 0.18275154112423225, 0.1856262830615778, 0.18603696144459428, 0.18562628445072096, 0.18562628286575145, 0.1856262842548946, 0.18439424942406296, 0.186036961462953, 0.18603696048382126, 0.18603696087547397, 0.18357289477540237, 0.18603695989634222, 0.18685831513248186, 0.18357289575453412, 0.18357289514869635, 0.18603695989634222, 0.18562628364905684, 0.18644763810189108, 0.18644763751441204, 0.18603696087547397, 0.18193018410729678, 0.18439425059902104, 0.18685831554249327, 0.1868583173049304, 0.18644763888519647, 0.18521560522932293, 0.18521560563933434, 0.18685831552413454, 0.18562628404070955, 0.1868583165032663, 0.1864476374960533, 0.18644763927684918, 0.18644763790606472, 0.1860369598779835, 0.18603696128548539, 0.18644763868937012, 0.1856262822782724, 0.18562628325740416, 0.18521560544350799, 0.18398357337260393, 0.1843942512048588, 0.1848049272196004, 0.1856262826699251, 0.18562628307993653, 0.18562628423653588, 0.18562628323904543, 0.18480492821709085, 0.18562628445072096, 0.18562628286575145, 0.18521560622681338, 0.18562628423653588, 0.18562628405906825, 0.18562628247409876, 0.18521560642263973, 0.18521560681429242, 0.18521560681429242, 0.18521560544350799, 0.18562628404070955, 0.18562628404070955, 0.1856262826515664, 0.18562628345323048, 0.18603696148131174, 0.18685831534666691, 0.18603696126712665, 0.18644763966850186, 0.18603696187296442, 0.18480492782543817, 0.18439425081320612, 0.18521560603098702, 0.18439425020736835, 0.1843942505623036, 0.1848049276112531, 0.18480492743378546, 0.184394250011542, 0.18439424961988932, 0.184394250011542, 0.18603696009216858, 0.18193018528225485, 0.18028747463250797, 0.1806981526422305, 0.1802874752016283, 0.17987679779774354, 0.1815195078783701, 0.18069815103890224, 0.18110882847950444, 0.1806981514672724, 0.17987679719190577, 0.18110883004611522, 0.1815195062934006, 0.1802874752016283, 0.18069815084307592, 0.181519507076706, 0.18234086329197738, 0.1815195074867174, 0.1819301843031231, 0.18234086329197738, 0.18069815164474]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
