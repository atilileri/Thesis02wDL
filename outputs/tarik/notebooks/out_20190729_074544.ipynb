{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf85.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 07:45:44 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '3Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ib', 'ds', 'aa', 'ce', 'yd', 'ck', 'eg', 'mb', 'by', 'sk', 'eb', 'my', 'ek', 'sg', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002058155E278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000205BB9A6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7109, Accuracy:0.1055, Validation Loss:2.7023, Validation Accuracy:0.1248\n",
    "Epoch #2: Loss:2.6982, Accuracy:0.1183, Validation Loss:2.6922, Validation Accuracy:0.1199\n",
    "Epoch #3: Loss:2.6880, Accuracy:0.1207, Validation Loss:2.6833, Validation Accuracy:0.1182\n",
    "Epoch #4: Loss:2.6799, Accuracy:0.1207, Validation Loss:2.6752, Validation Accuracy:0.1199\n",
    "Epoch #5: Loss:2.6722, Accuracy:0.1199, Validation Loss:2.6677, Validation Accuracy:0.1166\n",
    "Epoch #6: Loss:2.6643, Accuracy:0.1211, Validation Loss:2.6590, Validation Accuracy:0.1166\n",
    "Epoch #7: Loss:2.6548, Accuracy:0.1298, Validation Loss:2.6491, Validation Accuracy:0.1215\n",
    "Epoch #8: Loss:2.6436, Accuracy:0.1331, Validation Loss:2.6376, Validation Accuracy:0.1248\n",
    "Epoch #9: Loss:2.6303, Accuracy:0.1450, Validation Loss:2.6230, Validation Accuracy:0.1429\n",
    "Epoch #10: Loss:2.6140, Accuracy:0.1556, Validation Loss:2.6068, Validation Accuracy:0.1478\n",
    "Epoch #11: Loss:2.5965, Accuracy:0.1610, Validation Loss:2.5900, Validation Accuracy:0.1494\n",
    "Epoch #12: Loss:2.5872, Accuracy:0.1540, Validation Loss:2.5839, Validation Accuracy:0.1511\n",
    "Epoch #13: Loss:2.5694, Accuracy:0.1561, Validation Loss:2.5783, Validation Accuracy:0.1511\n",
    "Epoch #14: Loss:2.5692, Accuracy:0.1528, Validation Loss:2.5653, Validation Accuracy:0.1494\n",
    "Epoch #15: Loss:2.5500, Accuracy:0.1544, Validation Loss:2.5494, Validation Accuracy:0.1412\n",
    "Epoch #16: Loss:2.5310, Accuracy:0.1684, Validation Loss:2.5356, Validation Accuracy:0.1445\n",
    "Epoch #17: Loss:2.5204, Accuracy:0.1704, Validation Loss:2.5372, Validation Accuracy:0.1544\n",
    "Epoch #18: Loss:2.5255, Accuracy:0.1606, Validation Loss:2.5372, Validation Accuracy:0.1396\n",
    "Epoch #19: Loss:2.5282, Accuracy:0.1581, Validation Loss:2.5331, Validation Accuracy:0.1511\n",
    "Epoch #20: Loss:2.5046, Accuracy:0.1630, Validation Loss:2.5295, Validation Accuracy:0.1544\n",
    "Epoch #21: Loss:2.4966, Accuracy:0.1684, Validation Loss:2.5226, Validation Accuracy:0.1527\n",
    "Epoch #22: Loss:2.4913, Accuracy:0.1659, Validation Loss:2.5188, Validation Accuracy:0.1478\n",
    "Epoch #23: Loss:2.4837, Accuracy:0.1688, Validation Loss:2.5111, Validation Accuracy:0.1593\n",
    "Epoch #24: Loss:2.4839, Accuracy:0.1688, Validation Loss:2.5067, Validation Accuracy:0.1396\n",
    "Epoch #25: Loss:2.4774, Accuracy:0.1688, Validation Loss:2.5068, Validation Accuracy:0.1346\n",
    "Epoch #26: Loss:2.4737, Accuracy:0.1729, Validation Loss:2.5025, Validation Accuracy:0.1691\n",
    "Epoch #27: Loss:2.4718, Accuracy:0.1688, Validation Loss:2.4994, Validation Accuracy:0.1544\n",
    "Epoch #28: Loss:2.4682, Accuracy:0.1795, Validation Loss:2.5039, Validation Accuracy:0.1494\n",
    "Epoch #29: Loss:2.4657, Accuracy:0.1786, Validation Loss:2.4984, Validation Accuracy:0.1527\n",
    "Epoch #30: Loss:2.4633, Accuracy:0.1762, Validation Loss:2.4964, Validation Accuracy:0.1675\n",
    "Epoch #31: Loss:2.4600, Accuracy:0.1778, Validation Loss:2.4917, Validation Accuracy:0.1675\n",
    "Epoch #32: Loss:2.4586, Accuracy:0.1782, Validation Loss:2.4943, Validation Accuracy:0.1658\n",
    "Epoch #33: Loss:2.4589, Accuracy:0.1815, Validation Loss:2.4930, Validation Accuracy:0.1642\n",
    "Epoch #34: Loss:2.4576, Accuracy:0.1778, Validation Loss:2.4923, Validation Accuracy:0.1658\n",
    "Epoch #35: Loss:2.4587, Accuracy:0.1795, Validation Loss:2.4942, Validation Accuracy:0.1576\n",
    "Epoch #36: Loss:2.4561, Accuracy:0.1782, Validation Loss:2.4891, Validation Accuracy:0.1642\n",
    "Epoch #37: Loss:2.4551, Accuracy:0.1778, Validation Loss:2.4893, Validation Accuracy:0.1626\n",
    "Epoch #38: Loss:2.4538, Accuracy:0.1778, Validation Loss:2.4872, Validation Accuracy:0.1642\n",
    "Epoch #39: Loss:2.4516, Accuracy:0.1786, Validation Loss:2.4890, Validation Accuracy:0.1626\n",
    "Epoch #40: Loss:2.4534, Accuracy:0.1774, Validation Loss:2.4936, Validation Accuracy:0.1626\n",
    "Epoch #41: Loss:2.4528, Accuracy:0.1729, Validation Loss:2.4921, Validation Accuracy:0.1626\n",
    "Epoch #42: Loss:2.4523, Accuracy:0.1749, Validation Loss:2.4907, Validation Accuracy:0.1609\n",
    "Epoch #43: Loss:2.4514, Accuracy:0.1729, Validation Loss:2.4887, Validation Accuracy:0.1609\n",
    "Epoch #44: Loss:2.4509, Accuracy:0.1729, Validation Loss:2.4874, Validation Accuracy:0.1626\n",
    "Epoch #45: Loss:2.4495, Accuracy:0.1717, Validation Loss:2.4874, Validation Accuracy:0.1609\n",
    "Epoch #46: Loss:2.4483, Accuracy:0.1713, Validation Loss:2.4870, Validation Accuracy:0.1609\n",
    "Epoch #47: Loss:2.4479, Accuracy:0.1729, Validation Loss:2.4862, Validation Accuracy:0.1642\n",
    "Epoch #48: Loss:2.4477, Accuracy:0.1708, Validation Loss:2.4863, Validation Accuracy:0.1609\n",
    "Epoch #49: Loss:2.4469, Accuracy:0.1713, Validation Loss:2.4869, Validation Accuracy:0.1560\n",
    "Epoch #50: Loss:2.4476, Accuracy:0.1713, Validation Loss:2.4852, Validation Accuracy:0.1609\n",
    "Epoch #51: Loss:2.4468, Accuracy:0.1700, Validation Loss:2.4855, Validation Accuracy:0.1642\n",
    "Epoch #52: Loss:2.4470, Accuracy:0.1725, Validation Loss:2.4852, Validation Accuracy:0.1642\n",
    "Epoch #53: Loss:2.4457, Accuracy:0.1708, Validation Loss:2.4847, Validation Accuracy:0.1642\n",
    "Epoch #54: Loss:2.4437, Accuracy:0.1737, Validation Loss:2.4848, Validation Accuracy:0.1642\n",
    "Epoch #55: Loss:2.4441, Accuracy:0.1717, Validation Loss:2.4844, Validation Accuracy:0.1642\n",
    "Epoch #56: Loss:2.4438, Accuracy:0.1725, Validation Loss:2.4836, Validation Accuracy:0.1658\n",
    "Epoch #57: Loss:2.4432, Accuracy:0.1745, Validation Loss:2.4836, Validation Accuracy:0.1642\n",
    "Epoch #58: Loss:2.4423, Accuracy:0.1729, Validation Loss:2.4838, Validation Accuracy:0.1642\n",
    "Epoch #59: Loss:2.4420, Accuracy:0.1696, Validation Loss:2.4829, Validation Accuracy:0.1642\n",
    "Epoch #60: Loss:2.4418, Accuracy:0.1733, Validation Loss:2.4822, Validation Accuracy:0.1642\n",
    "Epoch #61: Loss:2.4411, Accuracy:0.1713, Validation Loss:2.4814, Validation Accuracy:0.1642\n",
    "Epoch #62: Loss:2.4400, Accuracy:0.1713, Validation Loss:2.4816, Validation Accuracy:0.1642\n",
    "Epoch #63: Loss:2.4414, Accuracy:0.1749, Validation Loss:2.4815, Validation Accuracy:0.1642\n",
    "Epoch #64: Loss:2.4394, Accuracy:0.1700, Validation Loss:2.4806, Validation Accuracy:0.1642\n",
    "Epoch #65: Loss:2.4400, Accuracy:0.1700, Validation Loss:2.4807, Validation Accuracy:0.1642\n",
    "Epoch #66: Loss:2.4395, Accuracy:0.1762, Validation Loss:2.4815, Validation Accuracy:0.1626\n",
    "Epoch #67: Loss:2.4391, Accuracy:0.1725, Validation Loss:2.4813, Validation Accuracy:0.1642\n",
    "Epoch #68: Loss:2.4386, Accuracy:0.1729, Validation Loss:2.4816, Validation Accuracy:0.1626\n",
    "Epoch #69: Loss:2.4374, Accuracy:0.1737, Validation Loss:2.4801, Validation Accuracy:0.1609\n",
    "Epoch #70: Loss:2.4397, Accuracy:0.1692, Validation Loss:2.4801, Validation Accuracy:0.1609\n",
    "Epoch #71: Loss:2.4380, Accuracy:0.1758, Validation Loss:2.4816, Validation Accuracy:0.1593\n",
    "Epoch #72: Loss:2.4379, Accuracy:0.1721, Validation Loss:2.4802, Validation Accuracy:0.1609\n",
    "Epoch #73: Loss:2.4372, Accuracy:0.1741, Validation Loss:2.4812, Validation Accuracy:0.1642\n",
    "Epoch #74: Loss:2.4367, Accuracy:0.1721, Validation Loss:2.4795, Validation Accuracy:0.1626\n",
    "Epoch #75: Loss:2.4361, Accuracy:0.1729, Validation Loss:2.4804, Validation Accuracy:0.1609\n",
    "Epoch #76: Loss:2.4359, Accuracy:0.1733, Validation Loss:2.4800, Validation Accuracy:0.1626\n",
    "Epoch #77: Loss:2.4358, Accuracy:0.1725, Validation Loss:2.4800, Validation Accuracy:0.1609\n",
    "Epoch #78: Loss:2.4364, Accuracy:0.1758, Validation Loss:2.4805, Validation Accuracy:0.1609\n",
    "Epoch #79: Loss:2.4379, Accuracy:0.1692, Validation Loss:2.4792, Validation Accuracy:0.1626\n",
    "Epoch #80: Loss:2.4353, Accuracy:0.1749, Validation Loss:2.4817, Validation Accuracy:0.1609\n",
    "Epoch #81: Loss:2.4367, Accuracy:0.1786, Validation Loss:2.4787, Validation Accuracy:0.1626\n",
    "Epoch #82: Loss:2.4350, Accuracy:0.1733, Validation Loss:2.4806, Validation Accuracy:0.1626\n",
    "Epoch #83: Loss:2.4338, Accuracy:0.1745, Validation Loss:2.4792, Validation Accuracy:0.1626\n",
    "Epoch #84: Loss:2.4336, Accuracy:0.1725, Validation Loss:2.4790, Validation Accuracy:0.1626\n",
    "Epoch #85: Loss:2.4329, Accuracy:0.1741, Validation Loss:2.4786, Validation Accuracy:0.1626\n",
    "Epoch #86: Loss:2.4331, Accuracy:0.1770, Validation Loss:2.4792, Validation Accuracy:0.1609\n",
    "Epoch #87: Loss:2.4327, Accuracy:0.1749, Validation Loss:2.4782, Validation Accuracy:0.1626\n",
    "Epoch #88: Loss:2.4320, Accuracy:0.1795, Validation Loss:2.4789, Validation Accuracy:0.1593\n",
    "Epoch #89: Loss:2.4320, Accuracy:0.1791, Validation Loss:2.4779, Validation Accuracy:0.1626\n",
    "Epoch #90: Loss:2.4320, Accuracy:0.1741, Validation Loss:2.4775, Validation Accuracy:0.1609\n",
    "Epoch #91: Loss:2.4311, Accuracy:0.1754, Validation Loss:2.4780, Validation Accuracy:0.1609\n",
    "Epoch #92: Loss:2.4311, Accuracy:0.1762, Validation Loss:2.4779, Validation Accuracy:0.1626\n",
    "Epoch #93: Loss:2.4306, Accuracy:0.1754, Validation Loss:2.4776, Validation Accuracy:0.1626\n",
    "Epoch #94: Loss:2.4314, Accuracy:0.1758, Validation Loss:2.4783, Validation Accuracy:0.1609\n",
    "Epoch #95: Loss:2.4308, Accuracy:0.1770, Validation Loss:2.4775, Validation Accuracy:0.1626\n",
    "Epoch #96: Loss:2.4299, Accuracy:0.1758, Validation Loss:2.4785, Validation Accuracy:0.1626\n",
    "Epoch #97: Loss:2.4303, Accuracy:0.1815, Validation Loss:2.4775, Validation Accuracy:0.1626\n",
    "Epoch #98: Loss:2.4301, Accuracy:0.1758, Validation Loss:2.4771, Validation Accuracy:0.1626\n",
    "Epoch #99: Loss:2.4302, Accuracy:0.1766, Validation Loss:2.4780, Validation Accuracy:0.1593\n",
    "Epoch #100: Loss:2.4313, Accuracy:0.1774, Validation Loss:2.4773, Validation Accuracy:0.1626\n",
    "Epoch #101: Loss:2.4288, Accuracy:0.1795, Validation Loss:2.4790, Validation Accuracy:0.1642\n",
    "Epoch #102: Loss:2.4288, Accuracy:0.1762, Validation Loss:2.4772, Validation Accuracy:0.1609\n",
    "Epoch #103: Loss:2.4296, Accuracy:0.1749, Validation Loss:2.4778, Validation Accuracy:0.1626\n",
    "Epoch #104: Loss:2.4276, Accuracy:0.1766, Validation Loss:2.4777, Validation Accuracy:0.1593\n",
    "Epoch #105: Loss:2.4279, Accuracy:0.1770, Validation Loss:2.4782, Validation Accuracy:0.1642\n",
    "Epoch #106: Loss:2.4278, Accuracy:0.1828, Validation Loss:2.4774, Validation Accuracy:0.1593\n",
    "Epoch #107: Loss:2.4275, Accuracy:0.1749, Validation Loss:2.4765, Validation Accuracy:0.1626\n",
    "Epoch #108: Loss:2.4270, Accuracy:0.1819, Validation Loss:2.4780, Validation Accuracy:0.1626\n",
    "Epoch #109: Loss:2.4267, Accuracy:0.1795, Validation Loss:2.4773, Validation Accuracy:0.1593\n",
    "Epoch #110: Loss:2.4282, Accuracy:0.1754, Validation Loss:2.4773, Validation Accuracy:0.1626\n",
    "Epoch #111: Loss:2.4276, Accuracy:0.1807, Validation Loss:2.4788, Validation Accuracy:0.1658\n",
    "Epoch #112: Loss:2.4268, Accuracy:0.1754, Validation Loss:2.4774, Validation Accuracy:0.1593\n",
    "Epoch #113: Loss:2.4257, Accuracy:0.1803, Validation Loss:2.4788, Validation Accuracy:0.1642\n",
    "Epoch #114: Loss:2.4256, Accuracy:0.1807, Validation Loss:2.4772, Validation Accuracy:0.1593\n",
    "Epoch #115: Loss:2.4269, Accuracy:0.1749, Validation Loss:2.4781, Validation Accuracy:0.1675\n",
    "Epoch #116: Loss:2.4279, Accuracy:0.1811, Validation Loss:2.4783, Validation Accuracy:0.1609\n",
    "Epoch #117: Loss:2.4262, Accuracy:0.1758, Validation Loss:2.4773, Validation Accuracy:0.1593\n",
    "Epoch #118: Loss:2.4257, Accuracy:0.1786, Validation Loss:2.4800, Validation Accuracy:0.1593\n",
    "Epoch #119: Loss:2.4250, Accuracy:0.1786, Validation Loss:2.4784, Validation Accuracy:0.1658\n",
    "Epoch #120: Loss:2.4240, Accuracy:0.1795, Validation Loss:2.4791, Validation Accuracy:0.1626\n",
    "Epoch #121: Loss:2.4239, Accuracy:0.1811, Validation Loss:2.4775, Validation Accuracy:0.1658\n",
    "Epoch #122: Loss:2.4235, Accuracy:0.1770, Validation Loss:2.4773, Validation Accuracy:0.1626\n",
    "Epoch #123: Loss:2.4234, Accuracy:0.1774, Validation Loss:2.4789, Validation Accuracy:0.1609\n",
    "Epoch #124: Loss:2.4231, Accuracy:0.1807, Validation Loss:2.4789, Validation Accuracy:0.1609\n",
    "Epoch #125: Loss:2.4235, Accuracy:0.1795, Validation Loss:2.4788, Validation Accuracy:0.1626\n",
    "Epoch #126: Loss:2.4219, Accuracy:0.1803, Validation Loss:2.4800, Validation Accuracy:0.1609\n",
    "Epoch #127: Loss:2.4224, Accuracy:0.1832, Validation Loss:2.4798, Validation Accuracy:0.1642\n",
    "Epoch #128: Loss:2.4222, Accuracy:0.1819, Validation Loss:2.4806, Validation Accuracy:0.1593\n",
    "Epoch #129: Loss:2.4215, Accuracy:0.1823, Validation Loss:2.4790, Validation Accuracy:0.1609\n",
    "Epoch #130: Loss:2.4217, Accuracy:0.1811, Validation Loss:2.4789, Validation Accuracy:0.1609\n",
    "Epoch #131: Loss:2.4210, Accuracy:0.1803, Validation Loss:2.4806, Validation Accuracy:0.1642\n",
    "Epoch #132: Loss:2.4206, Accuracy:0.1836, Validation Loss:2.4799, Validation Accuracy:0.1593\n",
    "Epoch #133: Loss:2.4210, Accuracy:0.1807, Validation Loss:2.4794, Validation Accuracy:0.1593\n",
    "Epoch #134: Loss:2.4201, Accuracy:0.1828, Validation Loss:2.4822, Validation Accuracy:0.1626\n",
    "Epoch #135: Loss:2.4200, Accuracy:0.1803, Validation Loss:2.4807, Validation Accuracy:0.1560\n",
    "Epoch #136: Loss:2.4197, Accuracy:0.1844, Validation Loss:2.4811, Validation Accuracy:0.1609\n",
    "Epoch #137: Loss:2.4200, Accuracy:0.1803, Validation Loss:2.4804, Validation Accuracy:0.1609\n",
    "Epoch #138: Loss:2.4192, Accuracy:0.1815, Validation Loss:2.4816, Validation Accuracy:0.1609\n",
    "Epoch #139: Loss:2.4206, Accuracy:0.1807, Validation Loss:2.4797, Validation Accuracy:0.1511\n",
    "Epoch #140: Loss:2.4201, Accuracy:0.1823, Validation Loss:2.4824, Validation Accuracy:0.1675\n",
    "Epoch #141: Loss:2.4178, Accuracy:0.1819, Validation Loss:2.4809, Validation Accuracy:0.1494\n",
    "Epoch #142: Loss:2.4203, Accuracy:0.1832, Validation Loss:2.4818, Validation Accuracy:0.1609\n",
    "Epoch #143: Loss:2.4182, Accuracy:0.1803, Validation Loss:2.4819, Validation Accuracy:0.1494\n",
    "Epoch #144: Loss:2.4176, Accuracy:0.1844, Validation Loss:2.4830, Validation Accuracy:0.1626\n",
    "Epoch #145: Loss:2.4173, Accuracy:0.1836, Validation Loss:2.4811, Validation Accuracy:0.1494\n",
    "Epoch #146: Loss:2.4187, Accuracy:0.1811, Validation Loss:2.4822, Validation Accuracy:0.1544\n",
    "Epoch #147: Loss:2.4173, Accuracy:0.1803, Validation Loss:2.4828, Validation Accuracy:0.1642\n",
    "Epoch #148: Loss:2.4177, Accuracy:0.1815, Validation Loss:2.4832, Validation Accuracy:0.1511\n",
    "Epoch #149: Loss:2.4178, Accuracy:0.1828, Validation Loss:2.4830, Validation Accuracy:0.1544\n",
    "Epoch #150: Loss:2.4176, Accuracy:0.1811, Validation Loss:2.4825, Validation Accuracy:0.1544\n",
    "Epoch #151: Loss:2.4184, Accuracy:0.1819, Validation Loss:2.4828, Validation Accuracy:0.1675\n",
    "Epoch #152: Loss:2.4169, Accuracy:0.1807, Validation Loss:2.4835, Validation Accuracy:0.1658\n",
    "Epoch #153: Loss:2.4176, Accuracy:0.1848, Validation Loss:2.4852, Validation Accuracy:0.1511\n",
    "Epoch #154: Loss:2.4168, Accuracy:0.1828, Validation Loss:2.4836, Validation Accuracy:0.1494\n",
    "Epoch #155: Loss:2.4162, Accuracy:0.1795, Validation Loss:2.4838, Validation Accuracy:0.1527\n",
    "Epoch #156: Loss:2.4180, Accuracy:0.1819, Validation Loss:2.4846, Validation Accuracy:0.1708\n",
    "Epoch #157: Loss:2.4166, Accuracy:0.1828, Validation Loss:2.4836, Validation Accuracy:0.1642\n",
    "Epoch #158: Loss:2.4155, Accuracy:0.1848, Validation Loss:2.4848, Validation Accuracy:0.1609\n",
    "Epoch #159: Loss:2.4161, Accuracy:0.1860, Validation Loss:2.4848, Validation Accuracy:0.1560\n",
    "Epoch #160: Loss:2.4146, Accuracy:0.1852, Validation Loss:2.4861, Validation Accuracy:0.1626\n",
    "Epoch #161: Loss:2.4166, Accuracy:0.1864, Validation Loss:2.4850, Validation Accuracy:0.1527\n",
    "Epoch #162: Loss:2.4154, Accuracy:0.1832, Validation Loss:2.4847, Validation Accuracy:0.1544\n",
    "Epoch #163: Loss:2.4139, Accuracy:0.1864, Validation Loss:2.4853, Validation Accuracy:0.1626\n",
    "Epoch #164: Loss:2.4143, Accuracy:0.1819, Validation Loss:2.4852, Validation Accuracy:0.1675\n",
    "Epoch #165: Loss:2.4141, Accuracy:0.1848, Validation Loss:2.4868, Validation Accuracy:0.1609\n",
    "Epoch #166: Loss:2.4141, Accuracy:0.1873, Validation Loss:2.4864, Validation Accuracy:0.1560\n",
    "Epoch #167: Loss:2.4134, Accuracy:0.1869, Validation Loss:2.4862, Validation Accuracy:0.1642\n",
    "Epoch #168: Loss:2.4145, Accuracy:0.1885, Validation Loss:2.4863, Validation Accuracy:0.1626\n",
    "Epoch #169: Loss:2.4133, Accuracy:0.1848, Validation Loss:2.4855, Validation Accuracy:0.1626\n",
    "Epoch #170: Loss:2.4135, Accuracy:0.1918, Validation Loss:2.4864, Validation Accuracy:0.1626\n",
    "Epoch #171: Loss:2.4165, Accuracy:0.1881, Validation Loss:2.4886, Validation Accuracy:0.1626\n",
    "Epoch #172: Loss:2.4145, Accuracy:0.1901, Validation Loss:2.4866, Validation Accuracy:0.1626\n",
    "Epoch #173: Loss:2.4133, Accuracy:0.1869, Validation Loss:2.4874, Validation Accuracy:0.1708\n",
    "Epoch #174: Loss:2.4130, Accuracy:0.1901, Validation Loss:2.4874, Validation Accuracy:0.1560\n",
    "Epoch #175: Loss:2.4131, Accuracy:0.1852, Validation Loss:2.4879, Validation Accuracy:0.1560\n",
    "Epoch #176: Loss:2.4130, Accuracy:0.1844, Validation Loss:2.4880, Validation Accuracy:0.1642\n",
    "Epoch #177: Loss:2.4129, Accuracy:0.1910, Validation Loss:2.4886, Validation Accuracy:0.1626\n",
    "Epoch #178: Loss:2.4121, Accuracy:0.1910, Validation Loss:2.4877, Validation Accuracy:0.1626\n",
    "Epoch #179: Loss:2.4119, Accuracy:0.1885, Validation Loss:2.4878, Validation Accuracy:0.1708\n",
    "Epoch #180: Loss:2.4120, Accuracy:0.1832, Validation Loss:2.4875, Validation Accuracy:0.1626\n",
    "Epoch #181: Loss:2.4123, Accuracy:0.1877, Validation Loss:2.4890, Validation Accuracy:0.1626\n",
    "Epoch #182: Loss:2.4115, Accuracy:0.1918, Validation Loss:2.4876, Validation Accuracy:0.1658\n",
    "Epoch #183: Loss:2.4119, Accuracy:0.1864, Validation Loss:2.4893, Validation Accuracy:0.1658\n",
    "Epoch #184: Loss:2.4124, Accuracy:0.1869, Validation Loss:2.4896, Validation Accuracy:0.1642\n",
    "Epoch #185: Loss:2.4124, Accuracy:0.1918, Validation Loss:2.4888, Validation Accuracy:0.1658\n",
    "Epoch #186: Loss:2.4136, Accuracy:0.1881, Validation Loss:2.4897, Validation Accuracy:0.1609\n",
    "Epoch #187: Loss:2.4139, Accuracy:0.1881, Validation Loss:2.4916, Validation Accuracy:0.1658\n",
    "Epoch #188: Loss:2.4140, Accuracy:0.1877, Validation Loss:2.4894, Validation Accuracy:0.1642\n",
    "Epoch #189: Loss:2.4127, Accuracy:0.1926, Validation Loss:2.4896, Validation Accuracy:0.1658\n",
    "Epoch #190: Loss:2.4106, Accuracy:0.1947, Validation Loss:2.4903, Validation Accuracy:0.1675\n",
    "Epoch #191: Loss:2.4116, Accuracy:0.1930, Validation Loss:2.4899, Validation Accuracy:0.1626\n",
    "Epoch #192: Loss:2.4130, Accuracy:0.1877, Validation Loss:2.4899, Validation Accuracy:0.1626\n",
    "Epoch #193: Loss:2.4139, Accuracy:0.1869, Validation Loss:2.4904, Validation Accuracy:0.1708\n",
    "Epoch #194: Loss:2.4119, Accuracy:0.1906, Validation Loss:2.4912, Validation Accuracy:0.1642\n",
    "Epoch #195: Loss:2.4135, Accuracy:0.1889, Validation Loss:2.4907, Validation Accuracy:0.1609\n",
    "Epoch #196: Loss:2.4116, Accuracy:0.1893, Validation Loss:2.4901, Validation Accuracy:0.1642\n",
    "Epoch #197: Loss:2.4101, Accuracy:0.1881, Validation Loss:2.4932, Validation Accuracy:0.1691\n",
    "Epoch #198: Loss:2.4096, Accuracy:0.1873, Validation Loss:2.4915, Validation Accuracy:0.1658\n",
    "Epoch #199: Loss:2.4099, Accuracy:0.1873, Validation Loss:2.4924, Validation Accuracy:0.1642\n",
    "Epoch #200: Loss:2.4100, Accuracy:0.1918, Validation Loss:2.4920, Validation Accuracy:0.1626\n",
    "Epoch #201: Loss:2.4123, Accuracy:0.1860, Validation Loss:2.4930, Validation Accuracy:0.1609\n",
    "Epoch #202: Loss:2.4128, Accuracy:0.1885, Validation Loss:2.4918, Validation Accuracy:0.1741\n",
    "Epoch #203: Loss:2.4132, Accuracy:0.1881, Validation Loss:2.4925, Validation Accuracy:0.1658\n",
    "Epoch #204: Loss:2.4097, Accuracy:0.1881, Validation Loss:2.4928, Validation Accuracy:0.1658\n",
    "Epoch #205: Loss:2.4104, Accuracy:0.1893, Validation Loss:2.4925, Validation Accuracy:0.1626\n",
    "Epoch #206: Loss:2.4097, Accuracy:0.1877, Validation Loss:2.4948, Validation Accuracy:0.1658\n",
    "Epoch #207: Loss:2.4091, Accuracy:0.1906, Validation Loss:2.4924, Validation Accuracy:0.1642\n",
    "Epoch #208: Loss:2.4103, Accuracy:0.1877, Validation Loss:2.4943, Validation Accuracy:0.1642\n",
    "Epoch #209: Loss:2.4083, Accuracy:0.1869, Validation Loss:2.4925, Validation Accuracy:0.1626\n",
    "Epoch #210: Loss:2.4095, Accuracy:0.1906, Validation Loss:2.4945, Validation Accuracy:0.1691\n",
    "Epoch #211: Loss:2.4085, Accuracy:0.1918, Validation Loss:2.4939, Validation Accuracy:0.1675\n",
    "Epoch #212: Loss:2.4084, Accuracy:0.1893, Validation Loss:2.4930, Validation Accuracy:0.1691\n",
    "Epoch #213: Loss:2.4074, Accuracy:0.1901, Validation Loss:2.4941, Validation Accuracy:0.1675\n",
    "Epoch #214: Loss:2.4097, Accuracy:0.1864, Validation Loss:2.4935, Validation Accuracy:0.1642\n",
    "Epoch #215: Loss:2.4110, Accuracy:0.1910, Validation Loss:2.4942, Validation Accuracy:0.1658\n",
    "Epoch #216: Loss:2.4110, Accuracy:0.1877, Validation Loss:2.4941, Validation Accuracy:0.1626\n",
    "Epoch #217: Loss:2.4131, Accuracy:0.1877, Validation Loss:2.4964, Validation Accuracy:0.1675\n",
    "Epoch #218: Loss:2.4103, Accuracy:0.1893, Validation Loss:2.4946, Validation Accuracy:0.1609\n",
    "Epoch #219: Loss:2.4092, Accuracy:0.1906, Validation Loss:2.4977, Validation Accuracy:0.1658\n",
    "Epoch #220: Loss:2.4108, Accuracy:0.1877, Validation Loss:2.4951, Validation Accuracy:0.1626\n",
    "Epoch #221: Loss:2.4072, Accuracy:0.1881, Validation Loss:2.4970, Validation Accuracy:0.1675\n",
    "Epoch #222: Loss:2.4086, Accuracy:0.1877, Validation Loss:2.4946, Validation Accuracy:0.1609\n",
    "Epoch #223: Loss:2.4084, Accuracy:0.1897, Validation Loss:2.4958, Validation Accuracy:0.1675\n",
    "Epoch #224: Loss:2.4083, Accuracy:0.1897, Validation Loss:2.4957, Validation Accuracy:0.1658\n",
    "Epoch #225: Loss:2.4107, Accuracy:0.1885, Validation Loss:2.4949, Validation Accuracy:0.1642\n",
    "Epoch #226: Loss:2.4101, Accuracy:0.1873, Validation Loss:2.4964, Validation Accuracy:0.1691\n",
    "Epoch #227: Loss:2.4084, Accuracy:0.1885, Validation Loss:2.4964, Validation Accuracy:0.1658\n",
    "Epoch #228: Loss:2.4071, Accuracy:0.1877, Validation Loss:2.4975, Validation Accuracy:0.1642\n",
    "Epoch #229: Loss:2.4078, Accuracy:0.1918, Validation Loss:2.4959, Validation Accuracy:0.1658\n",
    "Epoch #230: Loss:2.4082, Accuracy:0.1869, Validation Loss:2.4965, Validation Accuracy:0.1675\n",
    "Epoch #231: Loss:2.4071, Accuracy:0.1885, Validation Loss:2.4951, Validation Accuracy:0.1675\n",
    "Epoch #232: Loss:2.4075, Accuracy:0.1893, Validation Loss:2.4967, Validation Accuracy:0.1626\n",
    "Epoch #233: Loss:2.4063, Accuracy:0.1910, Validation Loss:2.4964, Validation Accuracy:0.1658\n",
    "Epoch #234: Loss:2.4061, Accuracy:0.1873, Validation Loss:2.4968, Validation Accuracy:0.1675\n",
    "Epoch #235: Loss:2.4065, Accuracy:0.1860, Validation Loss:2.4972, Validation Accuracy:0.1658\n",
    "Epoch #236: Loss:2.4065, Accuracy:0.1877, Validation Loss:2.4976, Validation Accuracy:0.1658\n",
    "Epoch #237: Loss:2.4075, Accuracy:0.1893, Validation Loss:2.4970, Validation Accuracy:0.1675\n",
    "Epoch #238: Loss:2.4061, Accuracy:0.1893, Validation Loss:2.4977, Validation Accuracy:0.1675\n",
    "Epoch #239: Loss:2.4074, Accuracy:0.1836, Validation Loss:2.4977, Validation Accuracy:0.1757\n",
    "Epoch #240: Loss:2.4066, Accuracy:0.1856, Validation Loss:2.4977, Validation Accuracy:0.1658\n",
    "Epoch #241: Loss:2.4073, Accuracy:0.1893, Validation Loss:2.4981, Validation Accuracy:0.1675\n",
    "Epoch #242: Loss:2.4065, Accuracy:0.1893, Validation Loss:2.4972, Validation Accuracy:0.1675\n",
    "Epoch #243: Loss:2.4059, Accuracy:0.1885, Validation Loss:2.4984, Validation Accuracy:0.1642\n",
    "Epoch #244: Loss:2.4052, Accuracy:0.1901, Validation Loss:2.4989, Validation Accuracy:0.1626\n",
    "Epoch #245: Loss:2.4054, Accuracy:0.1885, Validation Loss:2.4985, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4072, Accuracy:0.1901, Validation Loss:2.4982, Validation Accuracy:0.1691\n",
    "Epoch #247: Loss:2.4082, Accuracy:0.1906, Validation Loss:2.4986, Validation Accuracy:0.1511\n",
    "Epoch #248: Loss:2.4071, Accuracy:0.1893, Validation Loss:2.5014, Validation Accuracy:0.1658\n",
    "Epoch #249: Loss:2.4073, Accuracy:0.1906, Validation Loss:2.4991, Validation Accuracy:0.1626\n",
    "Epoch #250: Loss:2.4071, Accuracy:0.1864, Validation Loss:2.5003, Validation Accuracy:0.1658\n",
    "Epoch #251: Loss:2.4089, Accuracy:0.1836, Validation Loss:2.5003, Validation Accuracy:0.1626\n",
    "Epoch #252: Loss:2.4064, Accuracy:0.1844, Validation Loss:2.5030, Validation Accuracy:0.1691\n",
    "Epoch #253: Loss:2.4060, Accuracy:0.1889, Validation Loss:2.4978, Validation Accuracy:0.1658\n",
    "Epoch #254: Loss:2.4058, Accuracy:0.1922, Validation Loss:2.4995, Validation Accuracy:0.1675\n",
    "Epoch #255: Loss:2.4060, Accuracy:0.1893, Validation Loss:2.5006, Validation Accuracy:0.1658\n",
    "Epoch #256: Loss:2.4034, Accuracy:0.1864, Validation Loss:2.4998, Validation Accuracy:0.1544\n",
    "Epoch #257: Loss:2.4058, Accuracy:0.1893, Validation Loss:2.4996, Validation Accuracy:0.1642\n",
    "Epoch #258: Loss:2.4037, Accuracy:0.1881, Validation Loss:2.5004, Validation Accuracy:0.1642\n",
    "Epoch #259: Loss:2.4040, Accuracy:0.1869, Validation Loss:2.4993, Validation Accuracy:0.1691\n",
    "Epoch #260: Loss:2.4032, Accuracy:0.1906, Validation Loss:2.5005, Validation Accuracy:0.1642\n",
    "Epoch #261: Loss:2.4047, Accuracy:0.1897, Validation Loss:2.4997, Validation Accuracy:0.1691\n",
    "Epoch #262: Loss:2.4034, Accuracy:0.1856, Validation Loss:2.5004, Validation Accuracy:0.1609\n",
    "Epoch #263: Loss:2.4049, Accuracy:0.1906, Validation Loss:2.5006, Validation Accuracy:0.1626\n",
    "Epoch #264: Loss:2.4039, Accuracy:0.1901, Validation Loss:2.5013, Validation Accuracy:0.1691\n",
    "Epoch #265: Loss:2.4031, Accuracy:0.1926, Validation Loss:2.5002, Validation Accuracy:0.1675\n",
    "Epoch #266: Loss:2.4042, Accuracy:0.1877, Validation Loss:2.5019, Validation Accuracy:0.1675\n",
    "Epoch #267: Loss:2.4030, Accuracy:0.1869, Validation Loss:2.5004, Validation Accuracy:0.1658\n",
    "Epoch #268: Loss:2.4031, Accuracy:0.1906, Validation Loss:2.5015, Validation Accuracy:0.1691\n",
    "Epoch #269: Loss:2.4035, Accuracy:0.1881, Validation Loss:2.5019, Validation Accuracy:0.1675\n",
    "Epoch #270: Loss:2.4032, Accuracy:0.1910, Validation Loss:2.5012, Validation Accuracy:0.1642\n",
    "Epoch #271: Loss:2.4033, Accuracy:0.1901, Validation Loss:2.5021, Validation Accuracy:0.1609\n",
    "Epoch #272: Loss:2.4026, Accuracy:0.1897, Validation Loss:2.5015, Validation Accuracy:0.1560\n",
    "Epoch #273: Loss:2.4028, Accuracy:0.1918, Validation Loss:2.5026, Validation Accuracy:0.1642\n",
    "Epoch #274: Loss:2.4032, Accuracy:0.1897, Validation Loss:2.5015, Validation Accuracy:0.1642\n",
    "Epoch #275: Loss:2.4027, Accuracy:0.1877, Validation Loss:2.5028, Validation Accuracy:0.1658\n",
    "Epoch #276: Loss:2.4025, Accuracy:0.1873, Validation Loss:2.5027, Validation Accuracy:0.1642\n",
    "Epoch #277: Loss:2.4021, Accuracy:0.1852, Validation Loss:2.5021, Validation Accuracy:0.1494\n",
    "Epoch #278: Loss:2.4029, Accuracy:0.1947, Validation Loss:2.5024, Validation Accuracy:0.1642\n",
    "Epoch #279: Loss:2.4020, Accuracy:0.1930, Validation Loss:2.5040, Validation Accuracy:0.1642\n",
    "Epoch #280: Loss:2.4023, Accuracy:0.1906, Validation Loss:2.5037, Validation Accuracy:0.1675\n",
    "Epoch #281: Loss:2.4027, Accuracy:0.1901, Validation Loss:2.5025, Validation Accuracy:0.1658\n",
    "Epoch #282: Loss:2.4027, Accuracy:0.1881, Validation Loss:2.5032, Validation Accuracy:0.1527\n",
    "Epoch #283: Loss:2.4029, Accuracy:0.1881, Validation Loss:2.5046, Validation Accuracy:0.1642\n",
    "Epoch #284: Loss:2.4034, Accuracy:0.1893, Validation Loss:2.5037, Validation Accuracy:0.1576\n",
    "Epoch #285: Loss:2.4025, Accuracy:0.1926, Validation Loss:2.5034, Validation Accuracy:0.1544\n",
    "Epoch #286: Loss:2.4039, Accuracy:0.1951, Validation Loss:2.5038, Validation Accuracy:0.1478\n",
    "Epoch #287: Loss:2.4046, Accuracy:0.1889, Validation Loss:2.5053, Validation Accuracy:0.1527\n",
    "Epoch #288: Loss:2.4030, Accuracy:0.1877, Validation Loss:2.5024, Validation Accuracy:0.1544\n",
    "Epoch #289: Loss:2.4033, Accuracy:0.1897, Validation Loss:2.5038, Validation Accuracy:0.1626\n",
    "Epoch #290: Loss:2.4018, Accuracy:0.1881, Validation Loss:2.5041, Validation Accuracy:0.1658\n",
    "Epoch #291: Loss:2.4020, Accuracy:0.1873, Validation Loss:2.5034, Validation Accuracy:0.1658\n",
    "Epoch #292: Loss:2.4026, Accuracy:0.1893, Validation Loss:2.5037, Validation Accuracy:0.1626\n",
    "Epoch #293: Loss:2.4022, Accuracy:0.1926, Validation Loss:2.5038, Validation Accuracy:0.1544\n",
    "Epoch #294: Loss:2.4015, Accuracy:0.1897, Validation Loss:2.5049, Validation Accuracy:0.1626\n",
    "Epoch #295: Loss:2.4010, Accuracy:0.1922, Validation Loss:2.5047, Validation Accuracy:0.1576\n",
    "Epoch #296: Loss:2.4021, Accuracy:0.1914, Validation Loss:2.5053, Validation Accuracy:0.1609\n",
    "Epoch #297: Loss:2.4010, Accuracy:0.1934, Validation Loss:2.5042, Validation Accuracy:0.1544\n",
    "Epoch #298: Loss:2.4007, Accuracy:0.1926, Validation Loss:2.5049, Validation Accuracy:0.1658\n",
    "Epoch #299: Loss:2.3999, Accuracy:0.1922, Validation Loss:2.5048, Validation Accuracy:0.1527\n",
    "Epoch #300: Loss:2.4004, Accuracy:0.1901, Validation Loss:2.5042, Validation Accuracy:0.1642\n",
    "\n",
    "Test:\n",
    "Test Loss:2.50419378, Accuracy:0.1642\n",
    "Labels: ['ib', 'ds', 'aa', 'ce', 'yd', 'ck', 'eg', 'mb', 'by', 'sk', 'eb', 'my', 'ek', 'sg', 'eo']\n",
    "Confusion Matrix:\n",
    "      ib  ds  aa  ce  yd  ck  eg  mb  by  sk  eb  my  ek  sg  eo\n",
    "t:ib   1   0   0   0  25   0   5   3   0   0   1   0   6  13   0\n",
    "t:ds   0   8   0   0   0   0  17   0   1   0   2   0   2   1   0\n",
    "t:aa   0   7   0   0   1   0  13   0   2   0   3   0   8   0   0\n",
    "t:ce   0   0   0   0   0   0   8   0   2   0   2   0   6   9   0\n",
    "t:yd   0   0   0   0  32   0   3   1   0   0   0   0   5  21   0\n",
    "t:ck   0   2   0   0   0   0   9   0   1   0   2   0   6   3   0\n",
    "t:eg   0   4   0   0   0   0  29   1   1   0   2   0   6   7   0\n",
    "t:mb   1   0   0   0   8   0  12   0   5   0   3   0   3  20   0\n",
    "t:by   1   3   0   0   3   0   6   0   3   0   6   0   9   9   0\n",
    "t:sk   0   3   0   0   1   0  12   1   1   0   1   0   6   8   0\n",
    "t:eb   0   0   0   0  11   0  18   0   4   0   3   0   5   9   0\n",
    "t:my   0   1   0   0   8   0   5   0   0   0   1   0   3   2   0\n",
    "t:ek   0   1   0   0   9   0  18   0   1   0   2   0   6  11   0\n",
    "t:sg   0   1   0   0  13   0   4   2   3   0   0   0  10  18   0\n",
    "t:eo   1   0   0   0   2   0   4   1   0   0   4   0   8  14   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ib       0.25      0.02      0.03        54\n",
    "          ds       0.27      0.26      0.26        31\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          yd       0.28      0.52      0.37        62\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eg       0.18      0.58      0.27        50\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          by       0.12      0.07      0.09        40\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eb       0.09      0.06      0.07        50\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ek       0.07      0.12      0.09        48\n",
    "          sg       0.12      0.35      0.18        51\n",
    "          eo       0.00      0.00      0.00        34\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.09      0.13      0.09       609\n",
    "weighted avg       0.11      0.16      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 08:26:15 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 30 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7022957077558796, 2.6921845637323036, 2.6833402738586827, 2.6751587124685154, 2.6677413930250897, 2.65898231489122, 2.649139702222226, 2.637585066026459, 2.622973803424679, 2.606807095272396, 2.59001523129067, 2.5839413155867352, 2.578284526301918, 2.565273804813379, 2.5493737320203107, 2.5355928343505107, 2.5371870982823115, 2.537176812615105, 2.5330634070147435, 2.5294694755660685, 2.522638768006624, 2.5188041058275696, 2.511120568355316, 2.5067312748757096, 2.5067953382219588, 2.5025254055392763, 2.4994196014842767, 2.503896113099723, 2.498415578566553, 2.4963864896489287, 2.4917207977846143, 2.4942912162818347, 2.4930028492594, 2.4922756459716897, 2.4942273150132404, 2.489079367742554, 2.489281075341361, 2.487207736483544, 2.488993412951139, 2.493568991596867, 2.4920621125764644, 2.4906990888279257, 2.488721607549633, 2.4874495238506147, 2.487419798652135, 2.4870184387871, 2.4862128713448057, 2.4862720343866958, 2.4869467135524905, 2.4851756459973715, 2.4854521187655445, 2.485211092067274, 2.484683745404574, 2.4848354442170493, 2.4844316203018715, 2.4835947624764025, 2.4836304755437943, 2.483809612850446, 2.482914674849737, 2.4821923053127595, 2.481354091750773, 2.48164638862234, 2.4815167789584507, 2.480605728716294, 2.480738931101531, 2.481458704264097, 2.4812802736003605, 2.481557870733327, 2.4800557941443033, 2.4801102459724316, 2.4816093609250824, 2.480152662164472, 2.481227320403301, 2.4795414829880538, 2.4803578634371704, 2.480009519407902, 2.479988313856579, 2.480527509413721, 2.479164670057876, 2.4817187915294627, 2.4787096644465754, 2.4806343632182855, 2.4791975005702627, 2.4790315890351344, 2.478569326338118, 2.479207748654245, 2.4782285796010437, 2.47893951639949, 2.477887501661805, 2.4774667649042037, 2.477984484780598, 2.477870410299066, 2.477576399475874, 2.478340480910929, 2.4775475112871193, 2.4785120487213135, 2.4775071179338277, 2.477071342405623, 2.478030137044847, 2.477323498436187, 2.4789814933375967, 2.4772474147220356, 2.477804363654752, 2.4777153505284604, 2.47820836295831, 2.4773514591805843, 2.4765353653035533, 2.4779899034202586, 2.4773339700620554, 2.4772514326036075, 2.478758073988415, 2.477368803055611, 2.4787792713184076, 2.47724392026516, 2.478101435161772, 2.4783226347517693, 2.477268476204332, 2.4800471617474735, 2.4784088722003506, 2.479130125593865, 2.477508380495269, 2.477292026400762, 2.4789472730288953, 2.478907346333972, 2.4788445933111785, 2.4800325172092332, 2.479758820118771, 2.4805628756192712, 2.4789657592773438, 2.478862864630563, 2.480604476725135, 2.47988832564581, 2.4793642682982195, 2.4822337635240728, 2.480692898307136, 2.48108581917235, 2.4804258507069306, 2.4816186627730947, 2.479707405093464, 2.4823552340709516, 2.4808832354146273, 2.4817913095352098, 2.4818909426628077, 2.482959021488434, 2.4811244836973243, 2.4821766977043964, 2.482780763081142, 2.483244966795096, 2.482990948045978, 2.482456790206859, 2.4828194984661534, 2.4834884273986315, 2.485195154039731, 2.483643684872657, 2.483809363470093, 2.4845910816161307, 2.483613352861702, 2.4848230049527924, 2.4848378158750988, 2.4860677241496068, 2.4850315902816447, 2.4846858676822707, 2.485290577063224, 2.485223980764254, 2.486783996982919, 2.486399228545441, 2.4861715619004223, 2.486343054152866, 2.4855390652059923, 2.486369163140483, 2.4886307328792627, 2.4866113091141524, 2.4873882361820767, 2.487361393538602, 2.4879100185701217, 2.488000078937299, 2.48857223811408, 2.487658811124479, 2.4878247328383973, 2.4875019058609635, 2.489002534321376, 2.4876105726646087, 2.4893284568254193, 2.489648007798469, 2.4887503520608534, 2.489697803613196, 2.4915809686156525, 2.48935516401269, 2.489608922419681, 2.490295030017596, 2.4898883963649103, 2.489858209205966, 2.4904288568324446, 2.491164945029273, 2.490696931315956, 2.4900871619019407, 2.4931757262187637, 2.491464702170862, 2.492421074258087, 2.4919500515378754, 2.492962209657691, 2.4917648919110227, 2.492455406141986, 2.4927508318169753, 2.492467258559855, 2.4948415145498193, 2.492406969195712, 2.494325739996774, 2.492498718459031, 2.494487235503048, 2.4938901210653373, 2.4929587645287974, 2.494068923254906, 2.4934536533794183, 2.4941715830065347, 2.4940882719600532, 2.496356877983106, 2.494615447932276, 2.4977182468953, 2.4951111338604455, 2.4969884910802733, 2.4945977196121842, 2.4957791181229214, 2.4957160515151, 2.494902330862086, 2.4964224035516747, 2.4964163491291367, 2.4975004027825465, 2.495893598190082, 2.496489733897994, 2.4951062292496755, 2.496696291690194, 2.4964194849794135, 2.4967733443468467, 2.4971778882156648, 2.497619067898329, 2.4970456395047442, 2.497717335110619, 2.497664495446216, 2.497691414822107, 2.4981279854704006, 2.497176454180763, 2.4983631374409243, 2.498907797442281, 2.4984892420776568, 2.498179949758871, 2.4985724920513985, 2.50135953477255, 2.499091526557659, 2.500319797612959, 2.500323449841078, 2.502967108646637, 2.4977606740491143, 2.499477310916669, 2.5006485071479787, 2.4998403785655454, 2.49963960937287, 2.500372214466089, 2.4992692779829153, 2.5005029355755384, 2.4997039205335043, 2.500412390541365, 2.5005598800327196, 2.501263240288044, 2.5002467460037257, 2.501867532338611, 2.5003984185862422, 2.501461094823377, 2.5018784263842604, 2.5012158259186643, 2.5020981093345607, 2.5015249322787882, 2.5025689554919164, 2.5015023169650625, 2.502815190990179, 2.5026732364115847, 2.502068012218757, 2.502402858389618, 2.50395803890009, 2.5036602353031805, 2.5024833972818157, 2.5031983813237284, 2.5045537185199156, 2.503725937043113, 2.5033831439777745, 2.503795272219553, 2.505340012032019, 2.502428402845887, 2.5037524715824473, 2.504096527992211, 2.503425024217377, 2.5036634764647836, 2.5037619233718647, 2.504853893774875, 2.5046705068234347, 2.5052769532540355, 2.5041944753556025, 2.504912374837841, 2.504838564517267, 2.5041935056301172], 'val_acc': [0.12479474467694857, 0.11986863630256434, 0.1182266000798966, 0.11986863699990932, 0.11658456475031982, 0.11658456475031982, 0.12151067312470407, 0.12479474527642058, 0.14285714185394482, 0.147783250424075, 0.14942528664674273, 0.15106732257579153, 0.1510673226736645, 0.14942528645099679, 0.1412151056312771, 0.14449917797873957, 0.15435139502112696, 0.1395730697022283, 0.15106732277153748, 0.154351394923254, 0.15270935879845923, 0.14778325032620204, 0.15927750319976525, 0.1395730697022283, 0.13464696122997108, 0.16912972014427968, 0.15435139502112696, 0.14942528654886975, 0.15270935870058627, 0.167487683725866, 0.16748768382373896, 0.16584564769894422, 0.16420361157414948, 0.16584564769894422, 0.15763546697709752, 0.16420361157414948, 0.16256157535148175, 0.16420361137840353, 0.16256157634244567, 0.16256157535148175, 0.16256157535148175, 0.160919539226687, 0.160919539226687, 0.16256157535148175, 0.160919539226687, 0.16091953932456, 0.16420361147627652, 0.16091953932456, 0.1559934318432667, 0.160919539226687, 0.16420361157414948, 0.16420361157414948, 0.16420361147627652, 0.16420361157414948, 0.16420361157414948, 0.16584564769894422, 0.16420361157414948, 0.16420361157414948, 0.16420361157414948, 0.16420361157414948, 0.16420361157414948, 0.16420361157414948, 0.16420361157414948, 0.16420361157414948, 0.16420361167202246, 0.16256157574297367, 0.16420361157414948, 0.16256157574297367, 0.16091953942243298, 0.16091953942243298, 0.15927750359125717, 0.16091953942243298, 0.16420361157414948, 0.16256157554722772, 0.16091953932456, 0.16256157554722772, 0.1609195397160519, 0.1609195397160519, 0.16256157554722772, 0.1609195397160519, 0.16256157554722772, 0.16256157584084666, 0.16256157554722772, 0.16256157554722772, 0.16256157554722772, 0.1609195397160519, 0.16256157554722772, 0.15927750359125717, 0.16256157554722772, 0.1609195397160519, 0.1609195397160519, 0.16256157554722772, 0.16256157554722772, 0.1609195397160519, 0.16256157554722772, 0.16256157584084666, 0.16256157554722772, 0.16256157554722772, 0.15927750359125717, 0.16256157554722772, 0.1642036119656414, 0.16091953942243298, 0.16256157584084666, 0.15927750329763823, 0.1642036119656414, 0.15927750359125717, 0.16256157554722772, 0.16256157584084666, 0.15927750329763823, 0.16256157584084666, 0.16584564809043614, 0.15927750329763823, 0.1642036119656414, 0.15927750359125717, 0.16748768392161195, 0.1609195397160519, 0.15927750329763823, 0.15927750359125717, 0.16584564769894422, 0.16256157584084666, 0.1658456477968172, 0.16256157584084666, 0.1609195397160519, 0.1609195397160519, 0.16256157574297367, 0.1609195397160519, 0.16420361157414948, 0.15927750349338418, 0.1609195397160519, 0.16091953961817893, 0.16420361157414948, 0.15927750359125717, 0.15927750359125717, 0.16256157574297367, 0.15599343075442978, 0.1609195397160519, 0.16091953932456, 0.1609195397160519, 0.15106732238004555, 0.16748768382373896, 0.1494252862552508, 0.16091953961817893, 0.1494252862552508, 0.16256157535148175, 0.1494252862552508, 0.15435139462963504, 0.1642036118677684, 0.15106732238004555, 0.15435139462963504, 0.15435139462963504, 0.16748768382373896, 0.16584564799256318, 0.15106732238004555, 0.1494252862552508, 0.1527093585048403, 0.17077175607332848, 0.1642036118677684, 0.160919539226687, 0.15599343075442978, 0.16256157574297367, 0.1527093585048403, 0.15435139462963504, 0.16256157535148175, 0.16748768382373896, 0.160919539226687, 0.15599343075442978, 0.1642036118677684, 0.16256157535148175, 0.16256157574297367, 0.16256157535148175, 0.16256157535148175, 0.16256157535148175, 0.17077175646482037, 0.15599343075442978, 0.15599343075442978, 0.16420361147627652, 0.16256157535148175, 0.16256157535148175, 0.17077175607332848, 0.16256157535148175, 0.16256157535148175, 0.16584564760107126, 0.16584564760107126, 0.16420361147627652, 0.16584564760107126, 0.16091953912881404, 0.16584564760107126, 0.16420361147627652, 0.16584564760107126, 0.167487683725866, 0.16256157535148175, 0.16256157535148175, 0.17077175646482037, 0.16420361147627652, 0.160919539226687, 0.16420361147627652, 0.16912971985066075, 0.16584564760107126, 0.16420361147627652, 0.16256157535148175, 0.160919539226687, 0.1740558287144099, 0.16584564760107126, 0.16584564760107126, 0.16256157535148175, 0.16584564760107126, 0.16420361147627652, 0.16420361147627652, 0.16256157535148175, 0.16912971985066075, 0.167487683725866, 0.16912971985066075, 0.167487683725866, 0.16420361147627652, 0.16584564760107126, 0.16256157535148175, 0.167487683725866, 0.160919539226687, 0.16584564760107126, 0.16256157535148175, 0.167487683725866, 0.160919539226687, 0.167487683725866, 0.16584564760107126, 0.16420361147627652, 0.16912971985066075, 0.16584564760107126, 0.16420361147627652, 0.16584564760107126, 0.167487683725866, 0.167487683725866, 0.16256157535148175, 0.16584564760107126, 0.167487683725866, 0.16584564760107126, 0.16584564760107126, 0.167487683725866, 0.167487683725866, 0.17569786483920463, 0.16584564760107126, 0.167487683725866, 0.167487683725866, 0.16420361147627652, 0.16256157535148175, 0.16912971985066075, 0.16912971985066075, 0.15106732238004555, 0.16584564760107126, 0.16256157535148175, 0.16584564769894422, 0.16256157535148175, 0.1691297199485337, 0.16584564760107126, 0.167487683725866, 0.16584564760107126, 0.15435139462963504, 0.16420361147627652, 0.16420361147627652, 0.16912971985066075, 0.16420361147627652, 0.16912971985066075, 0.160919539226687, 0.16256157544935473, 0.16912971985066075, 0.167487683725866, 0.167487683725866, 0.16584564760107126, 0.16912971985066075, 0.167487683725866, 0.16420361147627652, 0.16091953932456, 0.15599343075442978, 0.16420361147627652, 0.16420361147627652, 0.16584564760107126, 0.16420361147627652, 0.1494252862552508, 0.16420361157414948, 0.16420361147627652, 0.167487683725866, 0.16584564760107126, 0.1527093585048403, 0.16420361157414948, 0.1576354670749705, 0.15435139462963504, 0.14778325112142, 0.1527093585048403, 0.15435139462963504, 0.16256157544935473, 0.16584564769894422, 0.16584564760107126, 0.16256157535148175, 0.15435139462963504, 0.16256157535148175, 0.1576354670749705, 0.160919539226687, 0.15435139462963504, 0.16584564760107126, 0.1527093585048403, 0.16420361147627652], 'loss': [2.7109328562718886, 2.698237227610249, 2.688044920790122, 2.6799459969483364, 2.67218603235985, 2.664260476619556, 2.6548408624817457, 2.643640930990419, 2.630286221925238, 2.6140433356502464, 2.5965191663902645, 2.58720577022623, 2.569395679125306, 2.5692445052967425, 2.5500386130393653, 2.5309890623699713, 2.520350528203976, 2.5254784143436124, 2.528156974232417, 2.504552105466933, 2.496556202635873, 2.4913312627059967, 2.483703791679053, 2.4839457173122272, 2.477406431664187, 2.473711550456053, 2.4718004121917474, 2.4681996647092594, 2.465719806389153, 2.4633489184800603, 2.460032589283812, 2.4585930334713915, 2.458869035043266, 2.4576450144240987, 2.4586584222390178, 2.456073602269073, 2.4550722928996938, 2.4538061205611337, 2.451621270130792, 2.4533739888937323, 2.4528482585961813, 2.452303500537755, 2.4513962400033, 2.4508759169607925, 2.4494758028269303, 2.448317702643925, 2.4479004698367577, 2.4477220366867662, 2.4469240694809744, 2.447623800986601, 2.446815278446895, 2.446969433829525, 2.445719608584958, 2.443655551483499, 2.444057371582094, 2.4438133290660944, 2.443231454179517, 2.4423307826142047, 2.4419622904955727, 2.441755368528425, 2.441126732463954, 2.44000199844705, 2.441408489274293, 2.4394087754235865, 2.439970274382793, 2.4395097328162536, 2.439101252957291, 2.438582423580256, 2.437448913115985, 2.4397287069160103, 2.438036001977, 2.437890474899104, 2.4371926607292536, 2.436652995428755, 2.4361405646776517, 2.4358615673543, 2.4357709743648583, 2.436374784348192, 2.4378904525749, 2.4352816682576646, 2.436666347554577, 2.4350485931921297, 2.433767740927193, 2.4335601919240775, 2.4329275626666247, 2.43305004186454, 2.432734026703257, 2.4319999652721553, 2.431993816618557, 2.4319621631496986, 2.4310845813712056, 2.431064223166119, 2.4306484899971275, 2.4314328012525177, 2.43081336843894, 2.4299076889085085, 2.4303162742199595, 2.430121049058511, 2.4301522833610707, 2.431264738478455, 2.4287636343703376, 2.428794964431982, 2.4295847921156053, 2.427592558929318, 2.4278549423452764, 2.4277811282469264, 2.4275085733166955, 2.426990760180495, 2.426664383357555, 2.4281869193122128, 2.4275814651708583, 2.4268183238207683, 2.4256727787503474, 2.4255549214459053, 2.4268641328909557, 2.427917262318198, 2.4262135262851596, 2.4257023323732723, 2.4249538099496517, 2.4239603793596585, 2.4238520748561414, 2.423475443313254, 2.423404306356912, 2.4230572238350305, 2.423484450346146, 2.4218963722918314, 2.422448414502937, 2.422205341621101, 2.4215469469035185, 2.4217307314980445, 2.42097330161923, 2.4206139544931524, 2.4210129740791397, 2.4201034094518703, 2.420043595566642, 2.419671813369532, 2.419958712873518, 2.4191638313034964, 2.4206496840874516, 2.420133799351216, 2.4177524822203775, 2.4203398877835127, 2.4182002387741997, 2.41764132961845, 2.4172735590219987, 2.4187155849879773, 2.4172844609685504, 2.4177037580791687, 2.4177555495463845, 2.4176272863235315, 2.4183923030046466, 2.4168867194432253, 2.417617512532573, 2.416792912845494, 2.4161765677238636, 2.4179830456171683, 2.4166401257015595, 2.41545905557746, 2.416117924782285, 2.4145912058789136, 2.416638095863546, 2.4154464993878313, 2.4139419001475497, 2.414345071545861, 2.4140638437604025, 2.4141232970314106, 2.413397167742375, 2.4145092078058137, 2.4132562775386677, 2.4135150732200983, 2.4165215702761857, 2.414460862686502, 2.413314043374032, 2.4129894886173506, 2.41308721918345, 2.413018415498048, 2.4129058714520024, 2.4120690882328355, 2.4119057847244294, 2.4120032342797186, 2.4123364291886285, 2.411519484353506, 2.411899231395682, 2.412370279143723, 2.4124477238625714, 2.4135875391519535, 2.413859429447558, 2.414010027840397, 2.4127154979862473, 2.410597422382425, 2.4116147696359937, 2.412965120965695, 2.413923741659834, 2.411886762250865, 2.4134991549858076, 2.4115613828694307, 2.4101292473090012, 2.409624887247105, 2.4099192195359684, 2.409951056200376, 2.4123202132003754, 2.4127763157752504, 2.4131862886142925, 2.409736836666444, 2.410436348259082, 2.409742859795353, 2.4091334426182742, 2.4102736446646937, 2.4083340569443283, 2.40948825088125, 2.408501649147676, 2.408384221779982, 2.4073917726716467, 2.4096776093790417, 2.411048272502985, 2.4109694470125547, 2.413104462770466, 2.410293778942351, 2.4092218075934375, 2.4108157191188426, 2.4071530548698847, 2.4085728357459977, 2.408375103419811, 2.4082928964979105, 2.4106710886318825, 2.4101286283019143, 2.4083695015133775, 2.4070605170310646, 2.407755787514563, 2.408218347952841, 2.4070999720258146, 2.4075120638038587, 2.406332799688257, 2.406132337053209, 2.4064956269469837, 2.4065234057957143, 2.4074537928344286, 2.4060503505338633, 2.407419305856223, 2.4065758887257664, 2.4072761036287345, 2.406465145988386, 2.4058868627528636, 2.405233229063375, 2.4054199035652366, 2.4071771466022156, 2.408202630930123, 2.4070768252535277, 2.4072962361439543, 2.407149789955092, 2.4089383160553917, 2.406435892077687, 2.4059710577038524, 2.405768507462018, 2.406030760947194, 2.4033949817720135, 2.4057547045439422, 2.4036707164325755, 2.404006631761116, 2.403207023824265, 2.404658054473219, 2.403417661792199, 2.4048975374664368, 2.4038688065335, 2.4030818859899314, 2.4042393625639304, 2.403002276998281, 2.403091654787318, 2.4035291725605177, 2.4032257952484506, 2.403273636017002, 2.4026194378580645, 2.4028430438384385, 2.403217459899934, 2.402727525581814, 2.4025047343369628, 2.402083507553508, 2.4029013274386677, 2.401984870996808, 2.4023434403006303, 2.402692905050039, 2.402724713123799, 2.402917724957946, 2.403404317303605, 2.402546372403844, 2.4038687594372634, 2.404579481011299, 2.402976147692796, 2.4033391928036356, 2.4017652441099195, 2.4019793936359317, 2.4025529344468635, 2.4021837502779166, 2.401511169018442, 2.400966139838436, 2.402059951797893, 2.4010016857476204, 2.4006534243511224, 2.399858593206386, 2.4004170840770556], 'acc': [0.10554414737885492, 0.11827515451815095, 0.12073922023574919, 0.1207392198440965, 0.1199178642071248, 0.12114989745298695, 0.12977412684497402, 0.13305954896449063, 0.14496919830354577, 0.15564681834998317, 0.16098562658322665, 0.15400410768187756, 0.15605749594969426, 0.15277207302851353, 0.1544147839108042, 0.1683778240817773, 0.1704312121904851, 0.1605749483960365, 0.1581108834525643, 0.16303901445939065, 0.16837782351265698, 0.16591375759005303, 0.16878850032906267, 0.16878850152237948, 0.1687885007390741, 0.17289527611810812, 0.1687884999557687, 0.17946611877217186, 0.1786447649251754, 0.17618069864763616, 0.17782340792659862, 0.178234087343823, 0.1815195082700228, 0.17782340888737164, 0.17946611820305153, 0.17823408730710558, 0.1778234095115681, 0.17782340872826272, 0.17864476335856458, 0.17741273089600784, 0.17289527691977224, 0.17494866559760036, 0.17289527631393448, 0.17289527650976083, 0.17166324424303042, 0.17125256760409235, 0.17289527809473035, 0.17084188959436986, 0.1712525678182774, 0.1712525668391457, 0.17002053416240387, 0.17248460067248686, 0.1708418882235854, 0.17371663274339091, 0.17166324463468313, 0.17248459928334373, 0.17453798876283594, 0.1728952767239459, 0.16960985617104007, 0.17330595491113604, 0.17125256664331934, 0.17125256701661332, 0.1749486646001099, 0.17002053435823022, 0.17002053359328353, 0.17618069823762475, 0.17248460067248686, 0.17289527650976083, 0.17371663235173823, 0.169199179532102, 0.17577002140286033, 0.1720739220569266, 0.17412730955979663, 0.17207392187945897, 0.17289527729306622, 0.17330595571280016, 0.17248460067248686, 0.17577001962206448, 0.1691991775554798, 0.17494866557924166, 0.1786447645518814, 0.1733059539320043, 0.17453798678621374, 0.17248460028083418, 0.17412731055728708, 0.17700205327793803, 0.17494866598925307, 0.17946611998384737, 0.17905544119081948, 0.1741273101656344, 0.17535934241400608, 0.1761806980417984, 0.17535934319731147, 0.17577002042372858, 0.17700205366959074, 0.1757700196404232, 0.181519507076706, 0.17577002142121906, 0.17659137605152092, 0.17741273128766055, 0.17946611978802104, 0.17618069864763616, 0.17494866479593627, 0.17659137528657423, 0.17700205229880628, 0.18275153993091545, 0.1749486642084572, 0.1819301838931117, 0.17946611978802104, 0.17535934202235337, 0.1806981516263813, 0.17535934398061687, 0.1802874740266702, 0.18069815105726098, 0.17494866557924166, 0.18110882945863618, 0.1757700200504346, 0.1786447649435341, 0.1786447637502173, 0.17946611998384737, 0.18110882926280983, 0.1770020538837758, 0.17741273169767197, 0.18069815185892507, 0.1794661185763455, 0.18028747342083243, 0.18316221696150622, 0.1819301850864285, 0.1823408629186834, 0.18110883026030028, 0.18028747383084384, 0.18357289555870776, 0.1806981524280454, 0.18275154071422084, 0.18028747500580194, 0.18439424961988932, 0.18028747463250797, 0.1815195082700228, 0.18069815084307592, 0.1823408627044983, 0.1819301854964399, 0.18316221833229065, 0.18028747363501751, 0.18439425058066233, 0.18357289436539095, 0.18110882924445112, 0.18028747342083243, 0.18151950746835868, 0.1827515393434364, 0.18110883045612663, 0.18193018449894946, 0.18069815125308733, 0.18480492823544958, 0.18275154050003575, 0.17946611800722517, 0.1819301842847644, 0.18275153955762147, 0.18480492782543817, 0.1860369602879949, 0.18521560483767022, 0.18644763966850186, 0.18316221815482303, 0.18644763788770602, 0.18193018567390756, 0.18480492782543817, 0.18726899372968342, 0.18685831593414595, 0.18850102777720967, 0.18480492860874356, 0.19178644713679868, 0.1880903485558116, 0.19014373764365117, 0.1868583161116136, 0.19014373607704038, 0.18521560522932293, 0.18439425020736835, 0.1909650928797908, 0.1909650921148441, 0.1885010259964138, 0.18316221655149478, 0.18767967056444784, 0.19178644870340947, 0.18644763966850186, 0.186858316894919, 0.19178644711843995, 0.18809035015913986, 0.18809034837834399, 0.18767967056444784, 0.1926078019629269, 0.19466119144241914, 0.19301848077431352, 0.1876796717210472, 0.18685831728657168, 0.19055441584920002, 0.18891170340029856, 0.18932238221168518, 0.18809034875163796, 0.18726899331967198, 0.1872689933380307, 0.19178644674514597, 0.18603696009216858, 0.1885010259780551, 0.18809035015913986, 0.18809034896582305, 0.1893223826216966, 0.18767967195359098, 0.19055441467424192, 0.1876796709377418, 0.1868583173049304, 0.190554414888427, 0.1917864480975717, 0.18932238082254202, 0.19014373725199846, 0.18644763949103424, 0.19096509286143207, 0.18767967115192688, 0.1876796717210472, 0.18932238258497916, 0.19055441565337367, 0.18767967214941733, 0.18809034837834399, 0.1876796715619383, 0.18973306043559277, 0.189733059829755, 0.1885010267797192, 0.18726899394386848, 0.18850102578222874, 0.18767967074191547, 0.19178644792010408, 0.18685831710910406, 0.18850102758138332, 0.18932238240751154, 0.190965091686474, 0.18726899290966056, 0.18603696048382126, 0.18767967154357956, 0.18932238084090075, 0.18932238162420614, 0.18357289595036047, 0.18562628364905684, 0.18932238240751154, 0.1893223826216966, 0.18850102658389287, 0.19014373686034577, 0.18850102679807793, 0.19014373666451942, 0.19055441506589463, 0.18932238201585883, 0.19055441469260065, 0.18644763810189108, 0.18357289616454553, 0.18439425081320612, 0.18891170459361536, 0.192197126339838, 0.18932238201585883, 0.18644763868937012, 0.18932238084090075, 0.18809034974912844, 0.186858316894919, 0.19055441469260065, 0.18973306002558135, 0.18562628343487178, 0.19055441586755875, 0.19014373785783623, 0.19260780335207006, 0.1876796715619383, 0.1868583165032663, 0.1905544136951102, 0.18809034816415893, 0.1909650928797908, 0.19014373746618354, 0.1897330594381023, 0.19178644752845136, 0.18973306041723403, 0.18767967136611194, 0.1872689949046415, 0.18521560524768163, 0.1946611910324077, 0.19301848057848717, 0.19055441467424192, 0.1901437382311302, 0.1880903479866913, 0.18809034818251766, 0.18932238084090075, 0.1926078023729383, 0.19507186825882483, 0.188911704397789, 0.1876796703686215, 0.189733059829755, 0.18809034896582305, 0.1872689941213361, 0.18932238123255343, 0.19260780313788498, 0.1897330606130604, 0.19219712534234754, 0.19137576989202285, 0.19342915721742523, 0.1926078031562437, 0.19219712455904214, 0.19014373666451942]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
