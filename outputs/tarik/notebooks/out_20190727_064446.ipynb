{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf31.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 06:44:46 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'All', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eg', 'ib', 'aa', 'yd', 'by', 'ce', 'my', 'eb', 'ek', 'eo', 'ds', 'sk', 'ck', 'sg', 'mb'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001EF03EA3DD8>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001EF37126EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7199, Accuracy:0.0563, Validation Loss:2.7110, Validation Accuracy:0.0558\n",
    "Epoch #2: Loss:2.7063, Accuracy:0.0768, Validation Loss:2.6997, Validation Accuracy:0.0821\n",
    "Epoch #3: Loss:2.6963, Accuracy:0.0825, Validation Loss:2.6914, Validation Accuracy:0.1018\n",
    "Epoch #4: Loss:2.6885, Accuracy:0.1023, Validation Loss:2.6852, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6831, Accuracy:0.1023, Validation Loss:2.6804, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6785, Accuracy:0.1023, Validation Loss:2.6766, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6751, Accuracy:0.1023, Validation Loss:2.6736, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6724, Accuracy:0.1023, Validation Loss:2.6711, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6701, Accuracy:0.1023, Validation Loss:2.6690, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6682, Accuracy:0.1023, Validation Loss:2.6673, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6666, Accuracy:0.1023, Validation Loss:2.6657, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6651, Accuracy:0.1023, Validation Loss:2.6643, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6637, Accuracy:0.1023, Validation Loss:2.6629, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6623, Accuracy:0.1023, Validation Loss:2.6614, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6607, Accuracy:0.1023, Validation Loss:2.6597, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6590, Accuracy:0.1023, Validation Loss:2.6576, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6567, Accuracy:0.1023, Validation Loss:2.6547, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6533, Accuracy:0.1027, Validation Loss:2.6503, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6480, Accuracy:0.1039, Validation Loss:2.6433, Validation Accuracy:0.1067\n",
    "Epoch #20: Loss:2.6392, Accuracy:0.1084, Validation Loss:2.6312, Validation Accuracy:0.1117\n",
    "Epoch #21: Loss:2.6244, Accuracy:0.1191, Validation Loss:2.6110, Validation Accuracy:0.1346\n",
    "Epoch #22: Loss:2.6003, Accuracy:0.1433, Validation Loss:2.5799, Validation Accuracy:0.1609\n",
    "Epoch #23: Loss:2.5661, Accuracy:0.1786, Validation Loss:2.5404, Validation Accuracy:0.1954\n",
    "Epoch #24: Loss:2.5287, Accuracy:0.1943, Validation Loss:2.5011, Validation Accuracy:0.1790\n",
    "Epoch #25: Loss:2.4947, Accuracy:0.1819, Validation Loss:2.4669, Validation Accuracy:0.1773\n",
    "Epoch #26: Loss:2.4634, Accuracy:0.1737, Validation Loss:2.4372, Validation Accuracy:0.1741\n",
    "Epoch #27: Loss:2.4354, Accuracy:0.1807, Validation Loss:2.4110, Validation Accuracy:0.1806\n",
    "Epoch #28: Loss:2.4083, Accuracy:0.1832, Validation Loss:2.3875, Validation Accuracy:0.1938\n",
    "Epoch #29: Loss:2.3840, Accuracy:0.1897, Validation Loss:2.3655, Validation Accuracy:0.2020\n",
    "Epoch #30: Loss:2.3588, Accuracy:0.2016, Validation Loss:2.3418, Validation Accuracy:0.1970\n",
    "Epoch #31: Loss:2.3369, Accuracy:0.2004, Validation Loss:2.3293, Validation Accuracy:0.2003\n",
    "Epoch #32: Loss:2.3147, Accuracy:0.2107, Validation Loss:2.2963, Validation Accuracy:0.1987\n",
    "Epoch #33: Loss:2.2907, Accuracy:0.2177, Validation Loss:2.2752, Validation Accuracy:0.2102\n",
    "Epoch #34: Loss:2.2771, Accuracy:0.2131, Validation Loss:2.2762, Validation Accuracy:0.2118\n",
    "Epoch #35: Loss:2.2530, Accuracy:0.2234, Validation Loss:2.2539, Validation Accuracy:0.1987\n",
    "Epoch #36: Loss:2.2356, Accuracy:0.2242, Validation Loss:2.2402, Validation Accuracy:0.2397\n",
    "Epoch #37: Loss:2.2223, Accuracy:0.2554, Validation Loss:2.2140, Validation Accuracy:0.2282\n",
    "Epoch #38: Loss:2.2044, Accuracy:0.2632, Validation Loss:2.2025, Validation Accuracy:0.2381\n",
    "Epoch #39: Loss:2.1892, Accuracy:0.2579, Validation Loss:2.1895, Validation Accuracy:0.2365\n",
    "Epoch #40: Loss:2.1724, Accuracy:0.2813, Validation Loss:2.1723, Validation Accuracy:0.2726\n",
    "Epoch #41: Loss:2.1553, Accuracy:0.2834, Validation Loss:2.1569, Validation Accuracy:0.2660\n",
    "Epoch #42: Loss:2.1382, Accuracy:0.2945, Validation Loss:2.1435, Validation Accuracy:0.2677\n",
    "Epoch #43: Loss:2.1229, Accuracy:0.2998, Validation Loss:2.1275, Validation Accuracy:0.2791\n",
    "Epoch #44: Loss:2.1019, Accuracy:0.3097, Validation Loss:2.1044, Validation Accuracy:0.2939\n",
    "Epoch #45: Loss:2.0860, Accuracy:0.3236, Validation Loss:2.0915, Validation Accuracy:0.3038\n",
    "Epoch #46: Loss:2.0690, Accuracy:0.3187, Validation Loss:2.0786, Validation Accuracy:0.2841\n",
    "Epoch #47: Loss:2.0594, Accuracy:0.3146, Validation Loss:2.0664, Validation Accuracy:0.3268\n",
    "Epoch #48: Loss:2.0460, Accuracy:0.3199, Validation Loss:2.0564, Validation Accuracy:0.3005\n",
    "Epoch #49: Loss:2.0313, Accuracy:0.3220, Validation Loss:2.0601, Validation Accuracy:0.3038\n",
    "Epoch #50: Loss:2.0257, Accuracy:0.3285, Validation Loss:2.0449, Validation Accuracy:0.3153\n",
    "Epoch #51: Loss:2.0163, Accuracy:0.3162, Validation Loss:2.0364, Validation Accuracy:0.3169\n",
    "Epoch #52: Loss:2.0083, Accuracy:0.3232, Validation Loss:2.0279, Validation Accuracy:0.3136\n",
    "Epoch #53: Loss:1.9968, Accuracy:0.3294, Validation Loss:2.0290, Validation Accuracy:0.3071\n",
    "Epoch #54: Loss:1.9933, Accuracy:0.3269, Validation Loss:2.0251, Validation Accuracy:0.3038\n",
    "Epoch #55: Loss:1.9909, Accuracy:0.3203, Validation Loss:2.0212, Validation Accuracy:0.2972\n",
    "Epoch #56: Loss:1.9775, Accuracy:0.3244, Validation Loss:2.0111, Validation Accuracy:0.3005\n",
    "Epoch #57: Loss:1.9691, Accuracy:0.3314, Validation Loss:2.0125, Validation Accuracy:0.3071\n",
    "Epoch #58: Loss:1.9701, Accuracy:0.3277, Validation Loss:2.0075, Validation Accuracy:0.3120\n",
    "Epoch #59: Loss:1.9590, Accuracy:0.3331, Validation Loss:1.9938, Validation Accuracy:0.3054\n",
    "Epoch #60: Loss:1.9504, Accuracy:0.3326, Validation Loss:1.9853, Validation Accuracy:0.3202\n",
    "Epoch #61: Loss:1.9516, Accuracy:0.3265, Validation Loss:1.9938, Validation Accuracy:0.3202\n",
    "Epoch #62: Loss:1.9495, Accuracy:0.3294, Validation Loss:1.9769, Validation Accuracy:0.3136\n",
    "Epoch #63: Loss:1.9400, Accuracy:0.3326, Validation Loss:1.9758, Validation Accuracy:0.3120\n",
    "Epoch #64: Loss:1.9299, Accuracy:0.3322, Validation Loss:1.9719, Validation Accuracy:0.3136\n",
    "Epoch #65: Loss:1.9238, Accuracy:0.3376, Validation Loss:1.9739, Validation Accuracy:0.3120\n",
    "Epoch #66: Loss:1.9155, Accuracy:0.3388, Validation Loss:1.9619, Validation Accuracy:0.3169\n",
    "Epoch #67: Loss:1.9094, Accuracy:0.3347, Validation Loss:1.9527, Validation Accuracy:0.3284\n",
    "Epoch #68: Loss:1.9048, Accuracy:0.3355, Validation Loss:1.9568, Validation Accuracy:0.3383\n",
    "Epoch #69: Loss:1.8982, Accuracy:0.3417, Validation Loss:1.9445, Validation Accuracy:0.3300\n",
    "Epoch #70: Loss:1.8939, Accuracy:0.3413, Validation Loss:1.9421, Validation Accuracy:0.3202\n",
    "Epoch #71: Loss:1.8864, Accuracy:0.3421, Validation Loss:1.9357, Validation Accuracy:0.3317\n",
    "Epoch #72: Loss:1.8800, Accuracy:0.3437, Validation Loss:1.9431, Validation Accuracy:0.3284\n",
    "Epoch #73: Loss:1.8772, Accuracy:0.3495, Validation Loss:1.9224, Validation Accuracy:0.3432\n",
    "Epoch #74: Loss:1.8686, Accuracy:0.3474, Validation Loss:1.9179, Validation Accuracy:0.3383\n",
    "Epoch #75: Loss:1.8657, Accuracy:0.3524, Validation Loss:1.9352, Validation Accuracy:0.3300\n",
    "Epoch #76: Loss:1.8711, Accuracy:0.3499, Validation Loss:1.9526, Validation Accuracy:0.3136\n",
    "Epoch #77: Loss:1.8789, Accuracy:0.3429, Validation Loss:1.9504, Validation Accuracy:0.3054\n",
    "Epoch #78: Loss:1.8866, Accuracy:0.3515, Validation Loss:1.9108, Validation Accuracy:0.3415\n",
    "Epoch #79: Loss:1.8559, Accuracy:0.3598, Validation Loss:1.9298, Validation Accuracy:0.3218\n",
    "Epoch #80: Loss:1.8703, Accuracy:0.3602, Validation Loss:1.9104, Validation Accuracy:0.3284\n",
    "Epoch #81: Loss:1.8382, Accuracy:0.3585, Validation Loss:1.8889, Validation Accuracy:0.3498\n",
    "Epoch #82: Loss:1.8287, Accuracy:0.3639, Validation Loss:1.8757, Validation Accuracy:0.3432\n",
    "Epoch #83: Loss:1.8160, Accuracy:0.3639, Validation Loss:1.8718, Validation Accuracy:0.3383\n",
    "Epoch #84: Loss:1.8075, Accuracy:0.3659, Validation Loss:1.8656, Validation Accuracy:0.3415\n",
    "Epoch #85: Loss:1.7985, Accuracy:0.3704, Validation Loss:1.8596, Validation Accuracy:0.3383\n",
    "Epoch #86: Loss:1.8002, Accuracy:0.3639, Validation Loss:1.8527, Validation Accuracy:0.3399\n",
    "Epoch #87: Loss:1.7917, Accuracy:0.3671, Validation Loss:1.8524, Validation Accuracy:0.3547\n",
    "Epoch #88: Loss:1.7864, Accuracy:0.3713, Validation Loss:1.8443, Validation Accuracy:0.3465\n",
    "Epoch #89: Loss:1.7782, Accuracy:0.3671, Validation Loss:1.8430, Validation Accuracy:0.3350\n",
    "Epoch #90: Loss:1.7712, Accuracy:0.3713, Validation Loss:1.8397, Validation Accuracy:0.3448\n",
    "Epoch #91: Loss:1.7727, Accuracy:0.3700, Validation Loss:1.8309, Validation Accuracy:0.3448\n",
    "Epoch #92: Loss:1.7660, Accuracy:0.3733, Validation Loss:1.8406, Validation Accuracy:0.3399\n",
    "Epoch #93: Loss:1.7608, Accuracy:0.3741, Validation Loss:1.8263, Validation Accuracy:0.3465\n",
    "Epoch #94: Loss:1.7586, Accuracy:0.3733, Validation Loss:1.8308, Validation Accuracy:0.3481\n",
    "Epoch #95: Loss:1.7533, Accuracy:0.3749, Validation Loss:1.8258, Validation Accuracy:0.3530\n",
    "Epoch #96: Loss:1.7474, Accuracy:0.3786, Validation Loss:1.8159, Validation Accuracy:0.3432\n",
    "Epoch #97: Loss:1.7367, Accuracy:0.3852, Validation Loss:1.8074, Validation Accuracy:0.3530\n",
    "Epoch #98: Loss:1.7347, Accuracy:0.3807, Validation Loss:1.8044, Validation Accuracy:0.3612\n",
    "Epoch #99: Loss:1.7334, Accuracy:0.3819, Validation Loss:1.8105, Validation Accuracy:0.3448\n",
    "Epoch #100: Loss:1.7319, Accuracy:0.3856, Validation Loss:1.8432, Validation Accuracy:0.3333\n",
    "Epoch #101: Loss:1.7457, Accuracy:0.3815, Validation Loss:1.8135, Validation Accuracy:0.3563\n",
    "Epoch #102: Loss:1.7400, Accuracy:0.3832, Validation Loss:1.7946, Validation Accuracy:0.3547\n",
    "Epoch #103: Loss:1.7302, Accuracy:0.3864, Validation Loss:1.7937, Validation Accuracy:0.3530\n",
    "Epoch #104: Loss:1.7187, Accuracy:0.3947, Validation Loss:1.7968, Validation Accuracy:0.3547\n",
    "Epoch #105: Loss:1.7216, Accuracy:0.3864, Validation Loss:1.7857, Validation Accuracy:0.3563\n",
    "Epoch #106: Loss:1.7046, Accuracy:0.3885, Validation Loss:1.7785, Validation Accuracy:0.3580\n",
    "Epoch #107: Loss:1.6994, Accuracy:0.3934, Validation Loss:1.7845, Validation Accuracy:0.3547\n",
    "Epoch #108: Loss:1.6957, Accuracy:0.3922, Validation Loss:1.7784, Validation Accuracy:0.3744\n",
    "Epoch #109: Loss:1.6934, Accuracy:0.3971, Validation Loss:1.7757, Validation Accuracy:0.3760\n",
    "Epoch #110: Loss:1.6897, Accuracy:0.4008, Validation Loss:1.7636, Validation Accuracy:0.3563\n",
    "Epoch #111: Loss:1.6796, Accuracy:0.4021, Validation Loss:1.7625, Validation Accuracy:0.3596\n",
    "Epoch #112: Loss:1.6789, Accuracy:0.3967, Validation Loss:1.7567, Validation Accuracy:0.3612\n",
    "Epoch #113: Loss:1.6775, Accuracy:0.3988, Validation Loss:1.7563, Validation Accuracy:0.3530\n",
    "Epoch #114: Loss:1.6743, Accuracy:0.4016, Validation Loss:1.7526, Validation Accuracy:0.3629\n",
    "Epoch #115: Loss:1.6658, Accuracy:0.4078, Validation Loss:1.7514, Validation Accuracy:0.3711\n",
    "Epoch #116: Loss:1.6615, Accuracy:0.4049, Validation Loss:1.7483, Validation Accuracy:0.3629\n",
    "Epoch #117: Loss:1.6574, Accuracy:0.3992, Validation Loss:1.7431, Validation Accuracy:0.3695\n",
    "Epoch #118: Loss:1.6548, Accuracy:0.4070, Validation Loss:1.7419, Validation Accuracy:0.3727\n",
    "Epoch #119: Loss:1.6542, Accuracy:0.4049, Validation Loss:1.7356, Validation Accuracy:0.3645\n",
    "Epoch #120: Loss:1.6494, Accuracy:0.4041, Validation Loss:1.7360, Validation Accuracy:0.3563\n",
    "Epoch #121: Loss:1.6513, Accuracy:0.4094, Validation Loss:1.7370, Validation Accuracy:0.3662\n",
    "Epoch #122: Loss:1.6481, Accuracy:0.4172, Validation Loss:1.7338, Validation Accuracy:0.3744\n",
    "Epoch #123: Loss:1.6392, Accuracy:0.4218, Validation Loss:1.7273, Validation Accuracy:0.3645\n",
    "Epoch #124: Loss:1.6396, Accuracy:0.4107, Validation Loss:1.7274, Validation Accuracy:0.3727\n",
    "Epoch #125: Loss:1.6321, Accuracy:0.4205, Validation Loss:1.7291, Validation Accuracy:0.3744\n",
    "Epoch #126: Loss:1.6300, Accuracy:0.4177, Validation Loss:1.7176, Validation Accuracy:0.3662\n",
    "Epoch #127: Loss:1.6230, Accuracy:0.4152, Validation Loss:1.7227, Validation Accuracy:0.3826\n",
    "Epoch #128: Loss:1.6197, Accuracy:0.4156, Validation Loss:1.7155, Validation Accuracy:0.3678\n",
    "Epoch #129: Loss:1.6144, Accuracy:0.4230, Validation Loss:1.7236, Validation Accuracy:0.3678\n",
    "Epoch #130: Loss:1.6177, Accuracy:0.4193, Validation Loss:1.7148, Validation Accuracy:0.3727\n",
    "Epoch #131: Loss:1.6241, Accuracy:0.4156, Validation Loss:1.7096, Validation Accuracy:0.3711\n",
    "Epoch #132: Loss:1.6106, Accuracy:0.4226, Validation Loss:1.7108, Validation Accuracy:0.3793\n",
    "Epoch #133: Loss:1.6101, Accuracy:0.4193, Validation Loss:1.6973, Validation Accuracy:0.3760\n",
    "Epoch #134: Loss:1.6084, Accuracy:0.4214, Validation Loss:1.6988, Validation Accuracy:0.3810\n",
    "Epoch #135: Loss:1.6064, Accuracy:0.4246, Validation Loss:1.6929, Validation Accuracy:0.3826\n",
    "Epoch #136: Loss:1.5978, Accuracy:0.4300, Validation Loss:1.6949, Validation Accuracy:0.3793\n",
    "Epoch #137: Loss:1.5928, Accuracy:0.4357, Validation Loss:1.6887, Validation Accuracy:0.3924\n",
    "Epoch #138: Loss:1.6018, Accuracy:0.4312, Validation Loss:1.6896, Validation Accuracy:0.3924\n",
    "Epoch #139: Loss:1.5953, Accuracy:0.4345, Validation Loss:1.6901, Validation Accuracy:0.3826\n",
    "Epoch #140: Loss:1.5882, Accuracy:0.4353, Validation Loss:1.6772, Validation Accuracy:0.3892\n",
    "Epoch #141: Loss:1.5839, Accuracy:0.4382, Validation Loss:1.6814, Validation Accuracy:0.3941\n",
    "Epoch #142: Loss:1.5821, Accuracy:0.4411, Validation Loss:1.6839, Validation Accuracy:0.3908\n",
    "Epoch #143: Loss:1.5742, Accuracy:0.4427, Validation Loss:1.6817, Validation Accuracy:0.3826\n",
    "Epoch #144: Loss:1.5706, Accuracy:0.4419, Validation Loss:1.6673, Validation Accuracy:0.3974\n",
    "Epoch #145: Loss:1.5691, Accuracy:0.4456, Validation Loss:1.6726, Validation Accuracy:0.3941\n",
    "Epoch #146: Loss:1.5709, Accuracy:0.4485, Validation Loss:1.6687, Validation Accuracy:0.3941\n",
    "Epoch #147: Loss:1.5622, Accuracy:0.4472, Validation Loss:1.6843, Validation Accuracy:0.4007\n",
    "Epoch #148: Loss:1.5675, Accuracy:0.4472, Validation Loss:1.6940, Validation Accuracy:0.3908\n",
    "Epoch #149: Loss:1.5641, Accuracy:0.4526, Validation Loss:1.6706, Validation Accuracy:0.3990\n",
    "Epoch #150: Loss:1.5642, Accuracy:0.4579, Validation Loss:1.6608, Validation Accuracy:0.4105\n",
    "Epoch #151: Loss:1.5514, Accuracy:0.4608, Validation Loss:1.6699, Validation Accuracy:0.4039\n",
    "Epoch #152: Loss:1.5557, Accuracy:0.4550, Validation Loss:1.6553, Validation Accuracy:0.4072\n",
    "Epoch #153: Loss:1.5456, Accuracy:0.4674, Validation Loss:1.6526, Validation Accuracy:0.4089\n",
    "Epoch #154: Loss:1.5484, Accuracy:0.4591, Validation Loss:1.6743, Validation Accuracy:0.4039\n",
    "Epoch #155: Loss:1.5477, Accuracy:0.4706, Validation Loss:1.6951, Validation Accuracy:0.4007\n",
    "Epoch #156: Loss:1.5539, Accuracy:0.4669, Validation Loss:1.6656, Validation Accuracy:0.4138\n",
    "Epoch #157: Loss:1.5392, Accuracy:0.4710, Validation Loss:1.6489, Validation Accuracy:0.4187\n",
    "Epoch #158: Loss:1.5351, Accuracy:0.4752, Validation Loss:1.6651, Validation Accuracy:0.4122\n",
    "Epoch #159: Loss:1.5370, Accuracy:0.4752, Validation Loss:1.6512, Validation Accuracy:0.4154\n",
    "Epoch #160: Loss:1.5271, Accuracy:0.4731, Validation Loss:1.6875, Validation Accuracy:0.4072\n",
    "Epoch #161: Loss:1.5424, Accuracy:0.4719, Validation Loss:1.7047, Validation Accuracy:0.4072\n",
    "Epoch #162: Loss:1.5500, Accuracy:0.4756, Validation Loss:1.6729, Validation Accuracy:0.4236\n",
    "Epoch #163: Loss:1.5356, Accuracy:0.4706, Validation Loss:1.6547, Validation Accuracy:0.4072\n",
    "Epoch #164: Loss:1.5330, Accuracy:0.4743, Validation Loss:1.6669, Validation Accuracy:0.4138\n",
    "Epoch #165: Loss:1.5391, Accuracy:0.4739, Validation Loss:1.6540, Validation Accuracy:0.4154\n",
    "Epoch #166: Loss:1.5373, Accuracy:0.4801, Validation Loss:1.6274, Validation Accuracy:0.4351\n",
    "Epoch #167: Loss:1.5263, Accuracy:0.4838, Validation Loss:1.6499, Validation Accuracy:0.4204\n",
    "Epoch #168: Loss:1.5211, Accuracy:0.4813, Validation Loss:1.6268, Validation Accuracy:0.4368\n",
    "Epoch #169: Loss:1.5118, Accuracy:0.4854, Validation Loss:1.6296, Validation Accuracy:0.4335\n",
    "Epoch #170: Loss:1.5173, Accuracy:0.4809, Validation Loss:1.6325, Validation Accuracy:0.4302\n",
    "Epoch #171: Loss:1.5153, Accuracy:0.4760, Validation Loss:1.6232, Validation Accuracy:0.4253\n",
    "Epoch #172: Loss:1.5016, Accuracy:0.4813, Validation Loss:1.6409, Validation Accuracy:0.4171\n",
    "Epoch #173: Loss:1.5090, Accuracy:0.4867, Validation Loss:1.6429, Validation Accuracy:0.4236\n",
    "Epoch #174: Loss:1.4995, Accuracy:0.4912, Validation Loss:1.6423, Validation Accuracy:0.4220\n",
    "Epoch #175: Loss:1.4952, Accuracy:0.4973, Validation Loss:1.6126, Validation Accuracy:0.4417\n",
    "Epoch #176: Loss:1.4970, Accuracy:0.4879, Validation Loss:1.6390, Validation Accuracy:0.4253\n",
    "Epoch #177: Loss:1.5002, Accuracy:0.4830, Validation Loss:1.6135, Validation Accuracy:0.4319\n",
    "Epoch #178: Loss:1.4970, Accuracy:0.4940, Validation Loss:1.6123, Validation Accuracy:0.4220\n",
    "Epoch #179: Loss:1.4870, Accuracy:0.4928, Validation Loss:1.6054, Validation Accuracy:0.4401\n",
    "Epoch #180: Loss:1.4825, Accuracy:0.4936, Validation Loss:1.6100, Validation Accuracy:0.4384\n",
    "Epoch #181: Loss:1.4796, Accuracy:0.4879, Validation Loss:1.5998, Validation Accuracy:0.4433\n",
    "Epoch #182: Loss:1.4706, Accuracy:0.4961, Validation Loss:1.6046, Validation Accuracy:0.4351\n",
    "Epoch #183: Loss:1.4663, Accuracy:0.5023, Validation Loss:1.6012, Validation Accuracy:0.4417\n",
    "Epoch #184: Loss:1.4704, Accuracy:0.5018, Validation Loss:1.6093, Validation Accuracy:0.4319\n",
    "Epoch #185: Loss:1.4673, Accuracy:0.5068, Validation Loss:1.5953, Validation Accuracy:0.4253\n",
    "Epoch #186: Loss:1.4685, Accuracy:0.5047, Validation Loss:1.6007, Validation Accuracy:0.4450\n",
    "Epoch #187: Loss:1.4668, Accuracy:0.4965, Validation Loss:1.5987, Validation Accuracy:0.4335\n",
    "Epoch #188: Loss:1.4608, Accuracy:0.5068, Validation Loss:1.5999, Validation Accuracy:0.4319\n",
    "Epoch #189: Loss:1.4610, Accuracy:0.5018, Validation Loss:1.5912, Validation Accuracy:0.4384\n",
    "Epoch #190: Loss:1.4503, Accuracy:0.5084, Validation Loss:1.5848, Validation Accuracy:0.4466\n",
    "Epoch #191: Loss:1.4463, Accuracy:0.5101, Validation Loss:1.5963, Validation Accuracy:0.4401\n",
    "Epoch #192: Loss:1.4428, Accuracy:0.5113, Validation Loss:1.5829, Validation Accuracy:0.4565\n",
    "Epoch #193: Loss:1.4406, Accuracy:0.5154, Validation Loss:1.5870, Validation Accuracy:0.4269\n",
    "Epoch #194: Loss:1.4556, Accuracy:0.5047, Validation Loss:1.6081, Validation Accuracy:0.4417\n",
    "Epoch #195: Loss:1.4521, Accuracy:0.5092, Validation Loss:1.5776, Validation Accuracy:0.4483\n",
    "Epoch #196: Loss:1.4603, Accuracy:0.5023, Validation Loss:1.5994, Validation Accuracy:0.4319\n",
    "Epoch #197: Loss:1.4580, Accuracy:0.5064, Validation Loss:1.6024, Validation Accuracy:0.4351\n",
    "Epoch #198: Loss:1.4473, Accuracy:0.5113, Validation Loss:1.6229, Validation Accuracy:0.4319\n",
    "Epoch #199: Loss:1.4331, Accuracy:0.5179, Validation Loss:1.5906, Validation Accuracy:0.4483\n",
    "Epoch #200: Loss:1.4376, Accuracy:0.5060, Validation Loss:1.5811, Validation Accuracy:0.4516\n",
    "Epoch #201: Loss:1.4329, Accuracy:0.5175, Validation Loss:1.5893, Validation Accuracy:0.4384\n",
    "Epoch #202: Loss:1.4429, Accuracy:0.5113, Validation Loss:1.5743, Validation Accuracy:0.4516\n",
    "Epoch #203: Loss:1.4301, Accuracy:0.5232, Validation Loss:1.5682, Validation Accuracy:0.4516\n",
    "Epoch #204: Loss:1.4305, Accuracy:0.5203, Validation Loss:1.5727, Validation Accuracy:0.4450\n",
    "Epoch #205: Loss:1.4280, Accuracy:0.5240, Validation Loss:1.5799, Validation Accuracy:0.4466\n",
    "Epoch #206: Loss:1.4206, Accuracy:0.5162, Validation Loss:1.5607, Validation Accuracy:0.4450\n",
    "Epoch #207: Loss:1.4150, Accuracy:0.5253, Validation Loss:1.5700, Validation Accuracy:0.4614\n",
    "Epoch #208: Loss:1.4122, Accuracy:0.5306, Validation Loss:1.5580, Validation Accuracy:0.4598\n",
    "Epoch #209: Loss:1.4113, Accuracy:0.5290, Validation Loss:1.5563, Validation Accuracy:0.4483\n",
    "Epoch #210: Loss:1.4076, Accuracy:0.5269, Validation Loss:1.5646, Validation Accuracy:0.4581\n",
    "Epoch #211: Loss:1.4070, Accuracy:0.5281, Validation Loss:1.5711, Validation Accuracy:0.4581\n",
    "Epoch #212: Loss:1.4085, Accuracy:0.5232, Validation Loss:1.5687, Validation Accuracy:0.4516\n",
    "Epoch #213: Loss:1.4255, Accuracy:0.5162, Validation Loss:1.5604, Validation Accuracy:0.4483\n",
    "Epoch #214: Loss:1.4108, Accuracy:0.5203, Validation Loss:1.5616, Validation Accuracy:0.4532\n",
    "Epoch #215: Loss:1.4302, Accuracy:0.5105, Validation Loss:1.5803, Validation Accuracy:0.4499\n",
    "Epoch #216: Loss:1.4116, Accuracy:0.5187, Validation Loss:1.5555, Validation Accuracy:0.4598\n",
    "Epoch #217: Loss:1.3991, Accuracy:0.5298, Validation Loss:1.5523, Validation Accuracy:0.4581\n",
    "Epoch #218: Loss:1.3965, Accuracy:0.5335, Validation Loss:1.5604, Validation Accuracy:0.4581\n",
    "Epoch #219: Loss:1.3995, Accuracy:0.5306, Validation Loss:1.5472, Validation Accuracy:0.4614\n",
    "Epoch #220: Loss:1.3885, Accuracy:0.5376, Validation Loss:1.5498, Validation Accuracy:0.4614\n",
    "Epoch #221: Loss:1.3875, Accuracy:0.5421, Validation Loss:1.5481, Validation Accuracy:0.4647\n",
    "Epoch #222: Loss:1.3953, Accuracy:0.5302, Validation Loss:1.5513, Validation Accuracy:0.4598\n",
    "Epoch #223: Loss:1.3881, Accuracy:0.5314, Validation Loss:1.5440, Validation Accuracy:0.4532\n",
    "Epoch #224: Loss:1.3848, Accuracy:0.5454, Validation Loss:1.5565, Validation Accuracy:0.4532\n",
    "Epoch #225: Loss:1.3849, Accuracy:0.5380, Validation Loss:1.5378, Validation Accuracy:0.4614\n",
    "Epoch #226: Loss:1.3913, Accuracy:0.5294, Validation Loss:1.5300, Validation Accuracy:0.4729\n",
    "Epoch #227: Loss:1.3857, Accuracy:0.5343, Validation Loss:1.5497, Validation Accuracy:0.4598\n",
    "Epoch #228: Loss:1.3749, Accuracy:0.5326, Validation Loss:1.5511, Validation Accuracy:0.4663\n",
    "Epoch #229: Loss:1.3654, Accuracy:0.5421, Validation Loss:1.5357, Validation Accuracy:0.4614\n",
    "Epoch #230: Loss:1.3730, Accuracy:0.5331, Validation Loss:1.5252, Validation Accuracy:0.4614\n",
    "Epoch #231: Loss:1.3641, Accuracy:0.5421, Validation Loss:1.5759, Validation Accuracy:0.4483\n",
    "Epoch #232: Loss:1.3834, Accuracy:0.5261, Validation Loss:1.5216, Validation Accuracy:0.4713\n",
    "Epoch #233: Loss:1.3764, Accuracy:0.5413, Validation Loss:1.5404, Validation Accuracy:0.4647\n",
    "Epoch #234: Loss:1.3594, Accuracy:0.5376, Validation Loss:1.5389, Validation Accuracy:0.4680\n",
    "Epoch #235: Loss:1.3495, Accuracy:0.5495, Validation Loss:1.5158, Validation Accuracy:0.4729\n",
    "Epoch #236: Loss:1.3546, Accuracy:0.5483, Validation Loss:1.5130, Validation Accuracy:0.4680\n",
    "Epoch #237: Loss:1.3456, Accuracy:0.5536, Validation Loss:1.5165, Validation Accuracy:0.4696\n",
    "Epoch #238: Loss:1.3419, Accuracy:0.5536, Validation Loss:1.5139, Validation Accuracy:0.4631\n",
    "Epoch #239: Loss:1.3448, Accuracy:0.5495, Validation Loss:1.5159, Validation Accuracy:0.4696\n",
    "Epoch #240: Loss:1.3443, Accuracy:0.5503, Validation Loss:1.5097, Validation Accuracy:0.4663\n",
    "Epoch #241: Loss:1.3449, Accuracy:0.5503, Validation Loss:1.5158, Validation Accuracy:0.4680\n",
    "Epoch #242: Loss:1.3470, Accuracy:0.5561, Validation Loss:1.5218, Validation Accuracy:0.4614\n",
    "Epoch #243: Loss:1.3440, Accuracy:0.5515, Validation Loss:1.5012, Validation Accuracy:0.4811\n",
    "Epoch #244: Loss:1.3372, Accuracy:0.5515, Validation Loss:1.5162, Validation Accuracy:0.4778\n",
    "Epoch #245: Loss:1.3379, Accuracy:0.5499, Validation Loss:1.4942, Validation Accuracy:0.4828\n",
    "Epoch #246: Loss:1.3296, Accuracy:0.5561, Validation Loss:1.5070, Validation Accuracy:0.4663\n",
    "Epoch #247: Loss:1.3293, Accuracy:0.5565, Validation Loss:1.5008, Validation Accuracy:0.4713\n",
    "Epoch #248: Loss:1.3286, Accuracy:0.5532, Validation Loss:1.5405, Validation Accuracy:0.4680\n",
    "Epoch #249: Loss:1.3357, Accuracy:0.5626, Validation Loss:1.4999, Validation Accuracy:0.4828\n",
    "Epoch #250: Loss:1.3320, Accuracy:0.5573, Validation Loss:1.5028, Validation Accuracy:0.4844\n",
    "Epoch #251: Loss:1.3315, Accuracy:0.5593, Validation Loss:1.5005, Validation Accuracy:0.4778\n",
    "Epoch #252: Loss:1.3226, Accuracy:0.5528, Validation Loss:1.5017, Validation Accuracy:0.4647\n",
    "Epoch #253: Loss:1.3412, Accuracy:0.5487, Validation Loss:1.5147, Validation Accuracy:0.4663\n",
    "Epoch #254: Loss:1.3257, Accuracy:0.5548, Validation Loss:1.5172, Validation Accuracy:0.4565\n",
    "Epoch #255: Loss:1.3313, Accuracy:0.5520, Validation Loss:1.5362, Validation Accuracy:0.4663\n",
    "Epoch #256: Loss:1.3475, Accuracy:0.5491, Validation Loss:1.4973, Validation Accuracy:0.4713\n",
    "Epoch #257: Loss:1.3424, Accuracy:0.5507, Validation Loss:1.5133, Validation Accuracy:0.4745\n",
    "Epoch #258: Loss:1.3300, Accuracy:0.5548, Validation Loss:1.5305, Validation Accuracy:0.4696\n",
    "Epoch #259: Loss:1.3145, Accuracy:0.5561, Validation Loss:1.4815, Validation Accuracy:0.4778\n",
    "Epoch #260: Loss:1.3076, Accuracy:0.5659, Validation Loss:1.4830, Validation Accuracy:0.4680\n",
    "Epoch #261: Loss:1.2979, Accuracy:0.5639, Validation Loss:1.4838, Validation Accuracy:0.4729\n",
    "Epoch #262: Loss:1.2938, Accuracy:0.5634, Validation Loss:1.4742, Validation Accuracy:0.4828\n",
    "Epoch #263: Loss:1.2964, Accuracy:0.5725, Validation Loss:1.4702, Validation Accuracy:0.4860\n",
    "Epoch #264: Loss:1.2926, Accuracy:0.5745, Validation Loss:1.4945, Validation Accuracy:0.4860\n",
    "Epoch #265: Loss:1.2997, Accuracy:0.5626, Validation Loss:1.4808, Validation Accuracy:0.4926\n",
    "Epoch #266: Loss:1.3043, Accuracy:0.5676, Validation Loss:1.4727, Validation Accuracy:0.4795\n",
    "Epoch #267: Loss:1.2953, Accuracy:0.5713, Validation Loss:1.4840, Validation Accuracy:0.4828\n",
    "Epoch #268: Loss:1.2905, Accuracy:0.5766, Validation Loss:1.4818, Validation Accuracy:0.4811\n",
    "Epoch #269: Loss:1.2888, Accuracy:0.5713, Validation Loss:1.4663, Validation Accuracy:0.4877\n",
    "Epoch #270: Loss:1.2804, Accuracy:0.5786, Validation Loss:1.4706, Validation Accuracy:0.4828\n",
    "Epoch #271: Loss:1.2803, Accuracy:0.5791, Validation Loss:1.4762, Validation Accuracy:0.4844\n",
    "Epoch #272: Loss:1.2782, Accuracy:0.5770, Validation Loss:1.4610, Validation Accuracy:0.4943\n",
    "Epoch #273: Loss:1.2760, Accuracy:0.5770, Validation Loss:1.4602, Validation Accuracy:0.4959\n",
    "Epoch #274: Loss:1.2736, Accuracy:0.5799, Validation Loss:1.4737, Validation Accuracy:0.4860\n",
    "Epoch #275: Loss:1.2881, Accuracy:0.5704, Validation Loss:1.4641, Validation Accuracy:0.4844\n",
    "Epoch #276: Loss:1.2737, Accuracy:0.5856, Validation Loss:1.4773, Validation Accuracy:0.4893\n",
    "Epoch #277: Loss:1.2732, Accuracy:0.5713, Validation Loss:1.4830, Validation Accuracy:0.4860\n",
    "Epoch #278: Loss:1.2760, Accuracy:0.5762, Validation Loss:1.4576, Validation Accuracy:0.4893\n",
    "Epoch #279: Loss:1.2686, Accuracy:0.5823, Validation Loss:1.4661, Validation Accuracy:0.5057\n",
    "Epoch #280: Loss:1.2725, Accuracy:0.5811, Validation Loss:1.4566, Validation Accuracy:0.5057\n",
    "Epoch #281: Loss:1.2682, Accuracy:0.5869, Validation Loss:1.4680, Validation Accuracy:0.5107\n",
    "Epoch #282: Loss:1.2811, Accuracy:0.5807, Validation Loss:1.4767, Validation Accuracy:0.4943\n",
    "Epoch #283: Loss:1.2840, Accuracy:0.5733, Validation Loss:1.4594, Validation Accuracy:0.4893\n",
    "Epoch #284: Loss:1.2864, Accuracy:0.5700, Validation Loss:1.4622, Validation Accuracy:0.4762\n",
    "Epoch #285: Loss:1.2598, Accuracy:0.5815, Validation Loss:1.4609, Validation Accuracy:0.4926\n",
    "Epoch #286: Loss:1.2545, Accuracy:0.5799, Validation Loss:1.4540, Validation Accuracy:0.4943\n",
    "Epoch #287: Loss:1.2561, Accuracy:0.5840, Validation Loss:1.4657, Validation Accuracy:0.4943\n",
    "Epoch #288: Loss:1.2548, Accuracy:0.5860, Validation Loss:1.4425, Validation Accuracy:0.4959\n",
    "Epoch #289: Loss:1.2462, Accuracy:0.5897, Validation Loss:1.4484, Validation Accuracy:0.4992\n",
    "Epoch #290: Loss:1.2458, Accuracy:0.5869, Validation Loss:1.4507, Validation Accuracy:0.5025\n",
    "Epoch #291: Loss:1.2416, Accuracy:0.5910, Validation Loss:1.4357, Validation Accuracy:0.5074\n",
    "Epoch #292: Loss:1.2403, Accuracy:0.5914, Validation Loss:1.4408, Validation Accuracy:0.5222\n",
    "Epoch #293: Loss:1.2450, Accuracy:0.5889, Validation Loss:1.4370, Validation Accuracy:0.5140\n",
    "Epoch #294: Loss:1.2425, Accuracy:0.5869, Validation Loss:1.4372, Validation Accuracy:0.5090\n",
    "Epoch #295: Loss:1.2347, Accuracy:0.5951, Validation Loss:1.4386, Validation Accuracy:0.5041\n",
    "Epoch #296: Loss:1.2353, Accuracy:0.5988, Validation Loss:1.4338, Validation Accuracy:0.5041\n",
    "Epoch #297: Loss:1.2378, Accuracy:0.5934, Validation Loss:1.4362, Validation Accuracy:0.5008\n",
    "Epoch #298: Loss:1.2350, Accuracy:0.5947, Validation Loss:1.4403, Validation Accuracy:0.5041\n",
    "Epoch #299: Loss:1.2315, Accuracy:0.5922, Validation Loss:1.4479, Validation Accuracy:0.4975\n",
    "Epoch #300: Loss:1.2483, Accuracy:0.5848, Validation Loss:1.4572, Validation Accuracy:0.4893\n",
    "\n",
    "Test:\n",
    "Test Loss:1.45717812, Accuracy:0.4893\n",
    "Labels: ['eg', 'ib', 'aa', 'yd', 'by', 'ce', 'my', 'eb', 'ek', 'eo', 'ds', 'sk', 'ck', 'sg', 'mb']\n",
    "Confusion Matrix:\n",
    "      eg  ib  aa  yd  by  ce  my  eb  ek  eo  ds  sk  ck  sg  mb\n",
    "t:eg  25   1   3   2   0   5   0   2   2   0   0   2   1   4   3\n",
    "t:ib   1  25   1   9   4   4   0   0   1   2   0   0   0   5   2\n",
    "t:aa   4   0  14   4   0   0   2   1   0   0   0   5   0   1   3\n",
    "t:yd   0   1   1  53   0   0   0   0   4   0   0   0   0   3   0\n",
    "t:by   3   3   0   8  21   0   0   0   0   3   0   0   0   2   0\n",
    "t:ce   1   5   0   0   6   8   0   1   0   2   0   1   2   1   0\n",
    "t:my   0   1   1   0   0   0   1   5   1   0   1   2   2   0   6\n",
    "t:eb   0   0   1   1   0   0   0  20  22   0   0   1   0   0   5\n",
    "t:ek   0   0   0   3   0   0   0   4  38   0   0   1   0   0   2\n",
    "t:eo   3   2   0   0   1   2   0   0   0  26   0   0   0   0   0\n",
    "t:ds   0   0   0   0   0   0   2   9   3   0   1   5   0   0  11\n",
    "t:sk   0   0   2   0   0   1   4   5   1   0   2  13   4   0   1\n",
    "t:ck   2   2   3   0   0   4   1   1   0   0   1   6   3   0   0\n",
    "t:sg   2   6   2  15   0   0   0   0   2   0   0   0   0  24   0\n",
    "t:mb   1   2   1   3   0   0   0   7   8   0   1   1   2   0  26\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eg       0.60      0.50      0.54        50\n",
    "          ib       0.52      0.46      0.49        54\n",
    "          aa       0.48      0.41      0.44        34\n",
    "          yd       0.54      0.85      0.66        62\n",
    "          by       0.66      0.53      0.58        40\n",
    "          ce       0.33      0.30      0.31        27\n",
    "          my       0.10      0.05      0.07        20\n",
    "          eb       0.36      0.40      0.38        50\n",
    "          ek       0.46      0.79      0.58        48\n",
    "          eo       0.79      0.76      0.78        34\n",
    "          ds       0.17      0.03      0.05        31\n",
    "          sk       0.35      0.39      0.37        33\n",
    "          ck       0.21      0.13      0.16        23\n",
    "          sg       0.60      0.47      0.53        51\n",
    "          mb       0.44      0.50      0.47        52\n",
    "\n",
    "    accuracy                           0.49       609\n",
    "   macro avg       0.44      0.44      0.43       609\n",
    "weighted avg       0.47      0.49      0.47       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 07:28:10 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 43 minutes, 23 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.711023982131031, 2.699695997833227, 2.691437732997199, 2.6852292333330428, 2.6803883903328027, 2.6766236724916155, 2.6736133494009136, 2.6710594624329866, 2.6690353305860497, 2.667301663037004, 2.6657318861417973, 2.6643012799261436, 2.662857344976591, 2.6613842693259957, 2.659691509550624, 2.657621170303896, 2.6546939124027498, 2.6503414661426263, 2.643279278415373, 2.6312084656043595, 2.611027255238375, 2.579940683148765, 2.540396291829878, 2.5010913800331953, 2.466851981402618, 2.4371841141743027, 2.411024531315896, 2.387455779734895, 2.3654737550832565, 2.341830040237978, 2.3293217994113666, 2.29626120604905, 2.2752444548364146, 2.276210105673629, 2.2538507164796977, 2.2401900921744864, 2.213990442858541, 2.2024531200014312, 2.1894989690952897, 2.1723314905401523, 2.15687116613529, 2.1434652041919127, 2.127474492798102, 2.10439718728778, 2.0914890073203103, 2.0785880484212993, 2.06642789950316, 2.0564173994393182, 2.060134758502979, 2.044865240604419, 2.0363807826989584, 2.0279289395938367, 2.0290349124883393, 2.0251448150534546, 2.0212108940130777, 2.011080874206593, 2.012511090496295, 2.007548978567515, 1.993779396971654, 1.9852608326816403, 1.993797135470536, 1.9768613451611623, 1.9757972276465254, 1.971882983381525, 1.9739453714273638, 1.961941021416575, 1.952733434284066, 1.9567619082571446, 1.9444977628382165, 1.9420720172437345, 1.9357260806219918, 1.9431366525064353, 1.922357083932911, 1.9179482266233472, 1.9351733885766642, 1.9526052106973182, 1.9503555094275764, 1.9107502458130785, 1.9298223415619047, 1.9104475284053384, 1.88894833191275, 1.8756693738630448, 1.8718047290795738, 1.8656476119468952, 1.8595902577213856, 1.8526538943226505, 1.8524493401860955, 1.8443339511091486, 1.84299744075938, 1.8396842477748352, 1.830898794634589, 1.8406409892346862, 1.8263229808979629, 1.8308285400393758, 1.8258228834430963, 1.815867163277612, 1.8074379499714166, 1.80442171085057, 1.8105141216115215, 1.8432031681972185, 1.8135411056195965, 1.7946206838235088, 1.7936907832454188, 1.796804461964637, 1.7857378852386976, 1.7784846672675096, 1.7844644121348565, 1.7784236005961602, 1.7756685866119435, 1.7635884840891671, 1.762474853612715, 1.7566687336500446, 1.7562773043690447, 1.752615823730068, 1.7514479373671934, 1.7483077618876115, 1.7430705031737905, 1.7419238219707471, 1.7355977271382248, 1.736016635628561, 1.7370491386047138, 1.7338365819458108, 1.727333938546956, 1.7274378177959149, 1.7291014811088299, 1.71761358958747, 1.722666922656969, 1.7155001799657035, 1.7235938772583634, 1.7148118351872135, 1.7095761765008686, 1.7108484089668161, 1.697349556561174, 1.6987970873640088, 1.6929180798272194, 1.694866213696735, 1.6886542481546136, 1.6895995040245244, 1.6900816742814038, 1.6772200832226005, 1.681386206341886, 1.6839305201579002, 1.6817325768603872, 1.6673146997179304, 1.6726164212955044, 1.668736092367, 1.684263113684255, 1.6940377065896597, 1.670572281079535, 1.6608010745792359, 1.6699478747613716, 1.6552558644064541, 1.6525894976993305, 1.6742607306181307, 1.6951152440558122, 1.6655541445038393, 1.64886574126621, 1.6650892241639261, 1.6511769140099462, 1.6875346601498733, 1.704726272420147, 1.672932212771649, 1.654740816266666, 1.6669274928730304, 1.6540185326621646, 1.6274369757359446, 1.6498609464156804, 1.6267647553351516, 1.6296235587209316, 1.6325430368946494, 1.6231563103022835, 1.6408894946813974, 1.6428752282178656, 1.6423433539511143, 1.6125598241542947, 1.6389820068732075, 1.6134618968994943, 1.612294833061143, 1.6054026708618565, 1.6099703842391717, 1.599807205262834, 1.6045584617968656, 1.6011579563269278, 1.6093369231043975, 1.595341141196503, 1.6007387800561188, 1.5987246617895041, 1.5998820724158451, 1.5911840108423594, 1.5847766597086965, 1.5963448777378877, 1.5828642434087292, 1.5870123128781373, 1.6080682796406236, 1.5776320022510972, 1.5994272280992154, 1.602399961897501, 1.6228980287933976, 1.5905998017400356, 1.5810795208111967, 1.5893159335469964, 1.5742627000573821, 1.5682193215061682, 1.5727083596885694, 1.5799290157108277, 1.5607191132402969, 1.5700158916279208, 1.5579769045652818, 1.5562628944127626, 1.564643928961605, 1.5711367024576723, 1.568687803052329, 1.5603638699489275, 1.5616083959444795, 1.5802727094033278, 1.5554705417802182, 1.552348744105823, 1.5603614943759587, 1.5471743691730968, 1.5497678990043051, 1.5480797355398168, 1.5512810767382041, 1.5439804704318494, 1.5565004456415161, 1.537832074564666, 1.5300208819519319, 1.5496986019983277, 1.5511271982944657, 1.5356574831729257, 1.5251748301517005, 1.5759210408419029, 1.5215990128384043, 1.5403991268186146, 1.5388557867855077, 1.5157785114200635, 1.512980444677945, 1.51653243324831, 1.5139327078617264, 1.5159015087854295, 1.509743244972918, 1.5158421505848174, 1.5218158803745638, 1.5011647127336274, 1.5162357739822814, 1.4942387839647742, 1.507037658605278, 1.5007575202262264, 1.540453359019776, 1.4998733500150232, 1.5027864965899238, 1.500511197816758, 1.501713215815414, 1.5147490822427183, 1.517234343026072, 1.5362258484015128, 1.4973273492603272, 1.5133274844518827, 1.5305236998841485, 1.4814702485778257, 1.4830050846234526, 1.4837620763355874, 1.4741683973271662, 1.4702083009412918, 1.4944549293940879, 1.4807985474910643, 1.4727242938403426, 1.483986760986654, 1.4818257809859778, 1.4663058715109363, 1.4705676684042894, 1.476236607445089, 1.4610447840541845, 1.46017263538536, 1.4737483171015147, 1.4640882060249842, 1.4773454861883655, 1.483025931959669, 1.4576490146577457, 1.4661122852162578, 1.4565890418680627, 1.468034512107987, 1.4766701551885244, 1.459441470199422, 1.462187073305127, 1.4608860282083647, 1.4539904418250023, 1.4657489613359198, 1.442480624994425, 1.44838946499848, 1.4506965809071983, 1.4357186054752769, 1.4407834249177003, 1.4369797154600397, 1.4372456359550088, 1.4385546011290526, 1.433800380609697, 1.4362091935914139, 1.4403469738701882, 1.4478557229237798, 1.4571781499045235], 'val_acc': [0.0558292278392953, 0.08210180623362022, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10673234790367837, 0.11165845637593559, 0.13464696122997108, 0.16091953952030597, 0.1954022981409956, 0.17898193650155622, 0.17733990057250745, 0.17405582832291797, 0.18062397291996993, 0.19376026201620086, 0.20197044273804757, 0.19704433416791736, 0.20032840661325282, 0.1986863702927121, 0.21018062316627534, 0.21182265958468902, 0.19868637009696616, 0.23973727390194566, 0.2282430207347635, 0.23809523767927793, 0.23645320135873724, 0.2725779965935865, 0.26600985199653454, 0.26765188821920227, 0.2791461390129647, 0.2939244662159182, 0.30377668098275884, 0.28407224768096784, 0.32676518741499616, 0.3004926090267883, 0.30377668078701286, 0.3152709337584491, 0.3169129699811168, 0.3136288976336543, 0.3070607532323483, 0.30377668078701286, 0.2972085385633807, 0.30049260863529637, 0.30706075531214916, 0.31198686150885957, 0.3054187171075536, 0.32019704242645225, 0.32019704242645225, 0.3136288977315273, 0.31198686160673256, 0.3136288975357814, 0.31198686150885957, 0.31691296978537087, 0.32840722314829895, 0.3382594397991945, 0.3300492593709667, 0.32019704232857926, 0.3316912952021425, 0.32840722275680706, 0.34318554797783274, 0.33825944168324934, 0.3300492587837288, 0.31362889981132813, 0.30541871889373545, 0.341543511755165, 0.3218390783555011, 0.3284072226589341, 0.3497536923791387, 0.34318554778208676, 0.3382594395055755, 0.341543511755165, 0.33825944168324934, 0.3399014778080441, 0.35467980075352296, 0.3464696202274222, 0.334975367353859, 0.34482758429837346, 0.3448275841026275, 0.33990147582611624, 0.3464696202274222, 0.348111656156471, 0.3530377648244742, 0.34318554797783274, 0.35303776502022016, 0.3612479455463209, 0.34482758429837346, 0.3333333311311913, 0.3563218370740637, 0.35467980085139594, 0.3530377646287282, 0.35467980094926893, 0.35632183726980965, 0.35796387310098543, 0.35467980085139594, 0.3743842348382978, 0.37602627106096553, 0.35632183726980965, 0.35960590951939914, 0.3612479455463209, 0.3530377649223472, 0.3628899817689886, 0.37110016249083533, 0.3628899818668616, 0.36945812646391357, 0.37274219861563007, 0.36453201789378337, 0.35632183726980965, 0.3661740540185781, 0.37438423454467884, 0.36453201808952934, 0.37274219861563007, 0.3743842346425518, 0.3661740541164511, 0.38259441526652554, 0.36781609004549987, 0.36781609024124584, 0.37274219861563007, 0.37110016219721637, 0.379310343310555, 0.37602627076734657, 0.38095237904385787, 0.38259441526652554, 0.37931034311480905, 0.39244663211316705, 0.39244663240678596, 0.38259441536439853, 0.3891625596678316, 0.3940886684337077, 0.3908045958904993, 0.38259441526652554, 0.3973727404875513, 0.3940886680422158, 0.3940886680422158, 0.40065681283501375, 0.3908045958904993, 0.39901477631872706, 0.41050902938803624, 0.40394088528034916, 0.40722495723631974, 0.4088669930674955, 0.4039408847909843, 0.40065681273714077, 0.4137931018333717, 0.41871920991413697, 0.41215106600219586, 0.41543513795816644, 0.40722495723631974, 0.4072249574320657, 0.42364531868001315, 0.40722495723631974, 0.4137931018333717, 0.41543513776242047, 0.4351395720429413, 0.42036124682191556, 0.43678160797199006, 0.43349753582027356, 0.430213463864303, 0.4252873551962998, 0.41707717427870716, 0.4236453188757591, 0.4220032827509644, 0.44170771605275533, 0.42528735500055387, 0.43185549959760583, 0.4220032829467103, 0.44006568071094443, 0.4384236443904037, 0.443349752666915, 0.43513957174932233, 0.4417077163463743, 0.43185549959760583, 0.42528735509842686, 0.44499178898745567, 0.43349753621176546, 0.4318554999890977, 0.4384236439989118, 0.4466338248186315, 0.44006568031945253, 0.45648604205676485, 0.4269293911253486, 0.4417077168357392, 0.44827586143279113, 0.43185549959760583, 0.4351395719450683, 0.43185549959760583, 0.4482758612370452, 0.4515599332908887, 0.4384236441946578, 0.4515599333887617, 0.4515599336823806, 0.44499178918320165, 0.4466338254058694, 0.44499178879170975, 0.46141215043114914, 0.4597701142084814, 0.44827586104129924, 0.4581280782794326, 0.45812807808368666, 0.4515599336823806, 0.44827586104129924, 0.45320196990504835, 0.44991789765545886, 0.4597701142084814, 0.4581280781815596, 0.4581280778879407, 0.46141215052902207, 0.46141215062689506, 0.46469622287648454, 0.4597701144042273, 0.45320197000292134, 0.4532019701007943, 0.46141215052902207, 0.47290640359833125, 0.45977011401273543, 0.46633825880553337, 0.46141215043114914, 0.46141215052902207, 0.4482758612370452, 0.4712643673756635, 0.46469622297435753, 0.4679802950282011, 0.47290640369620424, 0.46798029512607403, 0.46962233134874176, 0.4630541868495628, 0.4696223312508688, 0.46633825919702526, 0.46798029532182, 0.46141215072476804, 0.48111658441805094, 0.47783251207058847, 0.4827586205428457, 0.4663382590991523, 0.4712643674735365, 0.4679802950282011, 0.4827586204449727, 0.48440065656976744, 0.47783251216846145, 0.4646962230722305, 0.4663382590991523, 0.4564860424482568, 0.46633825929489825, 0.4712643675714095, 0.4745484395273801, 0.46962233105512285, 0.47783251177696956, 0.46798029512607403, 0.47290640359833125, 0.4827586204449727, 0.48604269259668925, 0.48604269269456224, 0.49261083738948713, 0.4794745479996373, 0.4827586202492268, 0.481116584222305, 0.487684728721484, 0.4827586202492268, 0.4844006563740215, 0.49425287322066297, 0.4958949094433307, 0.4860426924009433, 0.4844006563740215, 0.48932676484627874, 0.48604269259668925, 0.48932676474840575, 0.5057471218367515, 0.5057471217388786, 0.5106732302111358, 0.49425286886531533, 0.48932676474840575, 0.4761904756521748, 0.4926108370958682, 0.49425287341640894, 0.49425287322066297, 0.4958949094433307, 0.4991789814971742, 0.5024630541382556, 0.5073891578636733, 0.522167483378318, 0.5139573024607252, 0.5090311985395616, 0.5041050901651773, 0.5041050900673044, 0.5008210180134609, 0.5041050902630504, 0.4975369452745065, 0.4893267649441517], 'loss': [2.719864331525454, 2.7062880346662457, 2.6963289984442613, 2.688539643845764, 2.683056796209034, 2.678461899160115, 2.675145834623176, 2.672367487404136, 2.670091125656692, 2.6681932895824896, 2.666623966209208, 2.6651208655300573, 2.663746606348966, 2.6623079569187986, 2.6607286364146083, 2.6590495806699908, 2.6566587245439846, 2.653313645102405, 2.6479801633274778, 2.639248552949032, 2.624389182860357, 2.6003010086944705, 2.566130771676128, 2.528736166787588, 2.4947495765020227, 2.4634485700047235, 2.4353979942000623, 2.408318891074868, 2.3840464273762163, 2.3587941650492454, 2.3368674908820117, 2.3147139994760315, 2.2907310913720416, 2.277105413667965, 2.253018131295269, 2.2355560159781143, 2.222338927991581, 2.204364893617571, 2.189212392243027, 2.172370597224461, 2.1553490422344797, 2.1382197805009096, 2.1228732776837673, 2.1018933275641847, 2.0859693474348564, 2.068969442516382, 2.0593857724074223, 2.045988713446584, 2.0313404965939217, 2.0257219694478312, 2.0163392603519763, 2.0083384394400907, 1.9967978060857472, 1.9933417749111169, 1.9908941486288145, 1.977457991468833, 1.9690504623879153, 1.9700962127356558, 1.9590496696241093, 1.9503502692040477, 1.9515516032428468, 1.9494855575248202, 1.940018039562374, 1.9298853021872362, 1.9237892509241123, 1.915513257128502, 1.909409436699791, 1.9048472207184934, 1.8981664710955453, 1.8938530504825912, 1.886423950518426, 1.8799690438981418, 1.8772059428373646, 1.8686310313320746, 1.8657158179449596, 1.871138391553499, 1.8788856917582988, 1.8866183617276577, 1.8558500997339675, 1.8703116151096884, 1.8382185625099794, 1.8287412084348393, 1.816034867680293, 1.8075140883056044, 1.79850778104833, 1.8002452403857723, 1.7917395062515133, 1.7864167343664463, 1.7782132252041074, 1.7711971490534915, 1.7727484493040206, 1.765978224957993, 1.7607702687535687, 1.7586273704466144, 1.7532737565970764, 1.7473789665488493, 1.73674293641437, 1.7347345596466222, 1.733390410186329, 1.7319296882382653, 1.7457334802870388, 1.7400333792766751, 1.7302432031357313, 1.7187370139225797, 1.7216168153946894, 1.7046225831738733, 1.6993637246028108, 1.6956812126680567, 1.6933936438276538, 1.6897198787215308, 1.6796106502015977, 1.6788696206325868, 1.6775030727993536, 1.6742894786094493, 1.6658429509070865, 1.6615210720645819, 1.6573656079215926, 1.6547572119280054, 1.654179696233855, 1.6493592767010479, 1.6512976248895852, 1.6481022026994145, 1.6391674049091536, 1.639565843380452, 1.6321038465969862, 1.6300104215159799, 1.6229932802168985, 1.6196729737385587, 1.6144480413969537, 1.6176981573966493, 1.6241426412574564, 1.610604017960707, 1.6101038051581726, 1.6084078112177291, 1.6064238515477895, 1.5978225556731958, 1.5928286147558224, 1.601757717524221, 1.5952626943098691, 1.5882037399730644, 1.5839467970497554, 1.5820795349516663, 1.5741980202144177, 1.5706398017597394, 1.5690554999717696, 1.570877656310001, 1.562155905347585, 1.56748872229206, 1.5641142922994782, 1.564168609828675, 1.5513630576202266, 1.5556518379912485, 1.5456192382796834, 1.5484119051046195, 1.547692589740244, 1.5538832145060357, 1.5391777524713128, 1.5350596732427453, 1.5369920465735685, 1.5270614812506298, 1.542418129145487, 1.5499684346040417, 1.5356178109895522, 1.5329887333836643, 1.539145627158868, 1.5373012288150356, 1.5263262729625193, 1.5211440253306707, 1.5118492411881745, 1.5172925740541618, 1.5152980958167044, 1.5016402613700537, 1.5090050038371, 1.4995200653095755, 1.4951857866937375, 1.4969718592367622, 1.5002261135367643, 1.4970258267753178, 1.4869690051069004, 1.4824971955414914, 1.4796470013487266, 1.4706437497168352, 1.4663063592734522, 1.470424322181169, 1.4672547011404802, 1.4684668643273857, 1.4667573408424488, 1.460811422248151, 1.4609867914501402, 1.4503419032576637, 1.4462582313059782, 1.4428274331396365, 1.4406205859272387, 1.4556049033112104, 1.4520829603657341, 1.460348047906613, 1.4580306956410654, 1.4472644532240881, 1.4331354738016149, 1.437561692594258, 1.4328875418805977, 1.4429205287898101, 1.4301192311535627, 1.4304564846614547, 1.42797342203481, 1.42061399698747, 1.4149591548731684, 1.4121580242865874, 1.411273498799522, 1.4075954243387774, 1.406956081028102, 1.4084697021351213, 1.4254892160760304, 1.410817759678349, 1.4302020660416057, 1.4116301468020838, 1.3991448999185583, 1.3964866613215734, 1.3995388342859318, 1.3884741379250247, 1.3875279342369378, 1.3953412913688643, 1.388062606748859, 1.3847997582179075, 1.3848861876944007, 1.3912526394552274, 1.3857383740756057, 1.3748814423715798, 1.365362542170029, 1.3729891807391659, 1.364052929476791, 1.3833748265213546, 1.3763822765076186, 1.3594348551556314, 1.3494866765744877, 1.3545563566611287, 1.3455731101594177, 1.3418513969229477, 1.3447708861294223, 1.3442515384489995, 1.3449316025759406, 1.3469875785604395, 1.3439646133407186, 1.3371850422030847, 1.3379439883653144, 1.3296089235027713, 1.3292984725513497, 1.328579724544862, 1.3357130703740052, 1.3319970062870754, 1.3315428909090266, 1.3226490336521939, 1.3411877987565934, 1.3257315006589008, 1.3312536644494999, 1.3475310795116229, 1.342386098123429, 1.3299975922954645, 1.3144890014640604, 1.30757250364801, 1.297865770044268, 1.293796598299328, 1.29637955811479, 1.2926137164388105, 1.2996689613350119, 1.304301586581941, 1.2953455726713616, 1.2904898434938592, 1.288794114506465, 1.2803500996478039, 1.280264932370039, 1.2781782975187046, 1.2759742799970404, 1.2736084524366156, 1.2881130503433196, 1.2736978297850434, 1.2732435709152379, 1.2760344054909458, 1.2685783678011728, 1.2724731008130177, 1.268246957751515, 1.2811350908612325, 1.2839719137861498, 1.286384145533035, 1.2598491963419827, 1.2545438305798007, 1.2561069064071781, 1.2547585699347745, 1.246233218504418, 1.245760620350221, 1.241550437234021, 1.2402571681588581, 1.24500907539587, 1.2424671989691576, 1.2347352147836705, 1.2353115775012382, 1.237813439751063, 1.2350402303789676, 1.2315119324278783, 1.2483262777818056], 'acc': [0.056262833443020896, 0.07679671433733229, 0.08254620118613605, 0.10225872684430783, 0.10225872722678116, 0.10225872663012275, 0.10225872664848147, 0.10225872702177545, 0.10225872704013417, 0.10225872703095482, 0.10225872664848147, 0.10225872683512846, 0.10225872684430783, 0.10225872683512846, 0.10225872663012275, 0.10225872704013417, 0.10225872644347576, 0.10266940425737194, 0.10390143808153375, 0.10841889091034934, 0.11909650836514742, 0.14332648820456048, 0.17864476335856458, 0.19425051323687026, 0.1819301854780812, 0.17371663313504362, 0.18069815244640414, 0.18316221872394334, 0.18973305961556994, 0.2016427105579533, 0.20041067709790608, 0.2106776169621724, 0.21765913764431735, 0.21314168325807034, 0.22340862470730619, 0.22422998012091344, 0.25544147811508766, 0.26324435364171955, 0.25790554396425674, 0.2813141662359727, 0.28336755808373987, 0.29445584981838047, 0.29979466081155154, 0.3096509252118379, 0.32361396183957797, 0.31868583085111035, 0.31457905741198106, 0.31991786527193056, 0.3219712523464305, 0.32854209368478593, 0.31622176553434417, 0.32320328680396815, 0.3293634484925554, 0.3268993851707701, 0.320328542106695, 0.3244353198540039, 0.3314168375620362, 0.3277207376653409, 0.3330595498334701, 0.33264887162792117, 0.32648870696522125, 0.32936345146666807, 0.3326488720195739, 0.3322381941689603, 0.33757700049901646, 0.33880903687810016, 0.33470225834748585, 0.3355236145260398, 0.34168377605551814, 0.3412730998449502, 0.3420944558276778, 0.34373716473334626, 0.34948665416460994, 0.3474332635285184, 0.352361394516986, 0.34989732861274075, 0.34291581325462467, 0.3515400428424381, 0.3597535955954871, 0.3601642702027268, 0.3585215587880332, 0.36386037060122717, 0.3638603676271145, 0.36591375790827085, 0.3704312112786687, 0.3638603680187672, 0.3671457905299365, 0.3712525661231556, 0.36714579150906823, 0.3712525661231556, 0.3700205318981617, 0.3733059548009837, 0.3741273078830335, 0.3733059549968101, 0.3749486650774366, 0.37864476364006494, 0.38521560650831377, 0.38069815032291215, 0.3819301857595816, 0.385626282327229, 0.3815195079456854, 0.38316221880961737, 0.3864476363884105, 0.3946611925072249, 0.3864476389708705, 0.3885010266695669, 0.3934291578905783, 0.39219712304138793, 0.39712525860729647, 0.40082135716992484, 0.40205338607088986, 0.39671457684015593, 0.39876796907957573, 0.4016427124060645, 0.4078028727605847, 0.40492813315479664, 0.3991786433318802, 0.406981518271033, 0.40492813018068397, 0.4041067767436989, 0.4094455863660855, 0.4172484595060838, 0.4217659125215464, 0.4106776172253141, 0.42053388322892865, 0.41765913731997995, 0.41519507102408204, 0.41560574946217466, 0.42299794655071393, 0.41930184661730113, 0.4156057488379782, 0.4225872669376632, 0.41930184798808556, 0.4213552374492191, 0.42464065643551413, 0.4299794658620744, 0.4357289522825081, 0.4312114993037629, 0.4344969184858843, 0.4353182762677665, 0.438193017831818, 0.4410677605708277, 0.44271047300137045, 0.4418891160027937, 0.445585214565422, 0.4484599606334796, 0.4472279262126594, 0.4472279282076403, 0.45256673603087233, 0.4579055460449117, 0.46078028522232967, 0.455030802718423, 0.4673511278947521, 0.4591375790949475, 0.4706365524009023, 0.46694045145164037, 0.4710472268490331, 0.47515400302973126, 0.4751540047921684, 0.4731006181093212, 0.4718685838476099, 0.4755646834260874, 0.4706365494635071, 0.4743326468144599, 0.4739219713871973, 0.4800821366373763, 0.48377823300919737, 0.48131416870828037, 0.48542094567228394, 0.48090348889940326, 0.47597536045667815, 0.481314168512454, 0.4866529769231651, 0.4911704311135858, 0.49733059440550126, 0.4878850125556609, 0.48295687816471045, 0.49404517541920623, 0.4928131411942124, 0.4936344984253329, 0.4878850134980752, 0.49609856037633376, 0.5022587283680816, 0.50184804977088, 0.5067761806002388, 0.5047227930973688, 0.49650923956101434, 0.5067761825952197, 0.5018480461725708, 0.5084188947932187, 0.5100616025239291, 0.511293637923881, 0.5154004109713576, 0.5047227938806741, 0.5092402488544002, 0.5022587262139918, 0.5063655021988636, 0.5112936359656175, 0.5178644758964711, 0.5059548255599255, 0.5174538014116229, 0.5112936367856403, 0.523203288456253, 0.5203285439548062, 0.5240246438882189, 0.5162217650325391, 0.5252566761549493, 0.5305954847982043, 0.5289527683776997, 0.5268993864314022, 0.5281314179148272, 0.5232032894353847, 0.5162217652283655, 0.5203285437589799, 0.5104722789670407, 0.518685831328437, 0.5297741226347076, 0.5334702269497349, 0.5305954838190725, 0.5375770007805168, 0.5420944529392391, 0.5301848083550925, 0.5314168410134756, 0.545379880419502, 0.5379876834900716, 0.5293634505732103, 0.5342915833608326, 0.5326488713219425, 0.5420944592791171, 0.5330595499191441, 0.5420944579083327, 0.5260780311952626, 0.5412731030638458, 0.5375769975738603, 0.5494866573835056, 0.5482546172102863, 0.5535934329767247, 0.5535934319975929, 0.5494866554252421, 0.5503080041256774, 0.550308010857208, 0.5560574909744811, 0.5515400448863756, 0.5515400448863756, 0.5498973282700447, 0.5560574990767962, 0.5564681680050718, 0.5531827522254333, 0.5626283393993026, 0.557289524611996, 0.5593429204130075, 0.552772077153106, 0.5486653013640606, 0.5548254660267604, 0.5519507228960981, 0.5490759706839888, 0.5507186811562681, 0.5548254594910561, 0.5560574911703074, 0.5659137538081567, 0.5638603653261549, 0.5634496879039115, 0.5724846024288044, 0.5745379843751018, 0.5626283336469036, 0.5675564644762623, 0.571252563234717, 0.5765913780220235, 0.5712525622555852, 0.5786447613391054, 0.5790554381738698, 0.5770020575983569, 0.5770020518459579, 0.5798767959557519, 0.5704312091735354, 0.5856262879694757, 0.5712525640180224, 0.576180693672423, 0.5823408662416117, 0.5811088264600452, 0.5868583146796335, 0.5806981490378017, 0.5733059515208924, 0.5700205373078646, 0.5815195027073305, 0.5798767957599256, 0.5839835701781866, 0.5860369582685357, 0.5897330574186431, 0.5868583137005017, 0.5909650938711617, 0.591375774622453, 0.5889117000284136, 0.5868583144838071, 0.5950718676285087, 0.5987679673660952, 0.5934291561770978, 0.5946611931436605, 0.5921971213646249, 0.5848049313625516]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
