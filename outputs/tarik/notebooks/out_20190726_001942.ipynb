{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf5.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 00:19:42 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '1', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '05', '01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002F00228BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002F0716D7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6065, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6060, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6027, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6021, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6022, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6016, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6027, Accuracy:0.2337, Validation Loss:1.6015, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6027, Accuracy:0.2382, Validation Loss:1.6016, Validation Accuracy:0.2430\n",
    "Epoch #13: Loss:1.6025, Accuracy:0.2394, Validation Loss:1.6017, Validation Accuracy:0.2447\n",
    "Epoch #14: Loss:1.6020, Accuracy:0.2382, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #15: Loss:1.6019, Accuracy:0.2378, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #16: Loss:1.6024, Accuracy:0.2304, Validation Loss:1.6016, Validation Accuracy:0.2365\n",
    "Epoch #17: Loss:1.6030, Accuracy:0.2333, Validation Loss:1.6027, Validation Accuracy:0.2315\n",
    "Epoch #18: Loss:1.6025, Accuracy:0.2423, Validation Loss:1.6025, Validation Accuracy:0.2348\n",
    "Epoch #19: Loss:1.6024, Accuracy:0.2361, Validation Loss:1.6021, Validation Accuracy:0.2365\n",
    "Epoch #20: Loss:1.6027, Accuracy:0.2324, Validation Loss:1.6019, Validation Accuracy:0.2315\n",
    "Epoch #21: Loss:1.6028, Accuracy:0.2316, Validation Loss:1.6022, Validation Accuracy:0.2414\n",
    "Epoch #22: Loss:1.6023, Accuracy:0.2419, Validation Loss:1.6022, Validation Accuracy:0.2315\n",
    "Epoch #23: Loss:1.6026, Accuracy:0.2427, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #24: Loss:1.6024, Accuracy:0.2361, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #25: Loss:1.6022, Accuracy:0.2366, Validation Loss:1.6020, Validation Accuracy:0.2381\n",
    "Epoch #26: Loss:1.6020, Accuracy:0.2374, Validation Loss:1.6020, Validation Accuracy:0.2381\n",
    "Epoch #27: Loss:1.6023, Accuracy:0.2415, Validation Loss:1.6018, Validation Accuracy:0.2299\n",
    "Epoch #28: Loss:1.6012, Accuracy:0.2423, Validation Loss:1.6019, Validation Accuracy:0.2430\n",
    "Epoch #29: Loss:1.6009, Accuracy:0.2407, Validation Loss:1.6020, Validation Accuracy:0.2430\n",
    "Epoch #30: Loss:1.6010, Accuracy:0.2398, Validation Loss:1.6021, Validation Accuracy:0.2250\n",
    "Epoch #31: Loss:1.6007, Accuracy:0.2390, Validation Loss:1.6023, Validation Accuracy:0.2250\n",
    "Epoch #32: Loss:1.6007, Accuracy:0.2402, Validation Loss:1.6020, Validation Accuracy:0.2266\n",
    "Epoch #33: Loss:1.6005, Accuracy:0.2398, Validation Loss:1.6021, Validation Accuracy:0.2397\n",
    "Epoch #34: Loss:1.6003, Accuracy:0.2415, Validation Loss:1.6025, Validation Accuracy:0.2250\n",
    "Epoch #35: Loss:1.6002, Accuracy:0.2415, Validation Loss:1.6024, Validation Accuracy:0.2282\n",
    "Epoch #36: Loss:1.5998, Accuracy:0.2435, Validation Loss:1.6023, Validation Accuracy:0.2282\n",
    "Epoch #37: Loss:1.6002, Accuracy:0.2444, Validation Loss:1.6021, Validation Accuracy:0.2365\n",
    "Epoch #38: Loss:1.6000, Accuracy:0.2468, Validation Loss:1.6019, Validation Accuracy:0.2282\n",
    "Epoch #39: Loss:1.6001, Accuracy:0.2439, Validation Loss:1.6016, Validation Accuracy:0.2299\n",
    "Epoch #40: Loss:1.6001, Accuracy:0.2431, Validation Loss:1.6020, Validation Accuracy:0.2299\n",
    "Epoch #41: Loss:1.6008, Accuracy:0.2439, Validation Loss:1.6023, Validation Accuracy:0.2315\n",
    "Epoch #42: Loss:1.6019, Accuracy:0.2345, Validation Loss:1.6025, Validation Accuracy:0.2430\n",
    "Epoch #43: Loss:1.6009, Accuracy:0.2361, Validation Loss:1.6021, Validation Accuracy:0.2315\n",
    "Epoch #44: Loss:1.5998, Accuracy:0.2444, Validation Loss:1.6011, Validation Accuracy:0.2315\n",
    "Epoch #45: Loss:1.5993, Accuracy:0.2444, Validation Loss:1.6008, Validation Accuracy:0.2315\n",
    "Epoch #46: Loss:1.5996, Accuracy:0.2444, Validation Loss:1.6005, Validation Accuracy:0.2315\n",
    "Epoch #47: Loss:1.5990, Accuracy:0.2435, Validation Loss:1.6004, Validation Accuracy:0.2365\n",
    "Epoch #48: Loss:1.5991, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2365\n",
    "Epoch #49: Loss:1.5986, Accuracy:0.2472, Validation Loss:1.6008, Validation Accuracy:0.2282\n",
    "Epoch #50: Loss:1.5981, Accuracy:0.2460, Validation Loss:1.6007, Validation Accuracy:0.2299\n",
    "Epoch #51: Loss:1.5983, Accuracy:0.2444, Validation Loss:1.6012, Validation Accuracy:0.2315\n",
    "Epoch #52: Loss:1.5981, Accuracy:0.2427, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #53: Loss:1.5980, Accuracy:0.2415, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #54: Loss:1.5984, Accuracy:0.2448, Validation Loss:1.6005, Validation Accuracy:0.2414\n",
    "Epoch #55: Loss:1.5975, Accuracy:0.2444, Validation Loss:1.6016, Validation Accuracy:0.2315\n",
    "Epoch #56: Loss:1.5979, Accuracy:0.2460, Validation Loss:1.6013, Validation Accuracy:0.2332\n",
    "Epoch #57: Loss:1.5984, Accuracy:0.2431, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #58: Loss:1.6002, Accuracy:0.2357, Validation Loss:1.6030, Validation Accuracy:0.2365\n",
    "Epoch #59: Loss:1.6007, Accuracy:0.2411, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #60: Loss:1.5989, Accuracy:0.2431, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #61: Loss:1.5978, Accuracy:0.2370, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #62: Loss:1.5970, Accuracy:0.2456, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #63: Loss:1.5971, Accuracy:0.2419, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #64: Loss:1.5962, Accuracy:0.2464, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #65: Loss:1.5971, Accuracy:0.2402, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #66: Loss:1.5964, Accuracy:0.2431, Validation Loss:1.6001, Validation Accuracy:0.2463\n",
    "Epoch #67: Loss:1.5959, Accuracy:0.2439, Validation Loss:1.6004, Validation Accuracy:0.2365\n",
    "Epoch #68: Loss:1.5957, Accuracy:0.2402, Validation Loss:1.6009, Validation Accuracy:0.2430\n",
    "Epoch #69: Loss:1.5945, Accuracy:0.2472, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #70: Loss:1.5954, Accuracy:0.2407, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #71: Loss:1.5965, Accuracy:0.2497, Validation Loss:1.6005, Validation Accuracy:0.2397\n",
    "Epoch #72: Loss:1.5963, Accuracy:0.2439, Validation Loss:1.6010, Validation Accuracy:0.2479\n",
    "Epoch #73: Loss:1.5964, Accuracy:0.2435, Validation Loss:1.6022, Validation Accuracy:0.2299\n",
    "Epoch #74: Loss:1.5943, Accuracy:0.2485, Validation Loss:1.6014, Validation Accuracy:0.2463\n",
    "Epoch #75: Loss:1.5940, Accuracy:0.2415, Validation Loss:1.6004, Validation Accuracy:0.2447\n",
    "Epoch #76: Loss:1.5936, Accuracy:0.2444, Validation Loss:1.6023, Validation Accuracy:0.2299\n",
    "Epoch #77: Loss:1.5938, Accuracy:0.2595, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #78: Loss:1.5957, Accuracy:0.2513, Validation Loss:1.6022, Validation Accuracy:0.2348\n",
    "Epoch #79: Loss:1.5965, Accuracy:0.2476, Validation Loss:1.6032, Validation Accuracy:0.2430\n",
    "Epoch #80: Loss:1.5994, Accuracy:0.2329, Validation Loss:1.6024, Validation Accuracy:0.2397\n",
    "Epoch #81: Loss:1.5950, Accuracy:0.2497, Validation Loss:1.6058, Validation Accuracy:0.2282\n",
    "Epoch #82: Loss:1.5984, Accuracy:0.2485, Validation Loss:1.6028, Validation Accuracy:0.2381\n",
    "Epoch #83: Loss:1.5962, Accuracy:0.2407, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #84: Loss:1.5948, Accuracy:0.2419, Validation Loss:1.6009, Validation Accuracy:0.2479\n",
    "Epoch #85: Loss:1.5950, Accuracy:0.2509, Validation Loss:1.6010, Validation Accuracy:0.2479\n",
    "Epoch #86: Loss:1.5955, Accuracy:0.2464, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #87: Loss:1.5950, Accuracy:0.2468, Validation Loss:1.6025, Validation Accuracy:0.2496\n",
    "Epoch #88: Loss:1.5949, Accuracy:0.2419, Validation Loss:1.6029, Validation Accuracy:0.2430\n",
    "Epoch #89: Loss:1.5949, Accuracy:0.2509, Validation Loss:1.6029, Validation Accuracy:0.2447\n",
    "Epoch #90: Loss:1.5939, Accuracy:0.2456, Validation Loss:1.6029, Validation Accuracy:0.2381\n",
    "Epoch #91: Loss:1.5955, Accuracy:0.2472, Validation Loss:1.6033, Validation Accuracy:0.2529\n",
    "Epoch #92: Loss:1.5939, Accuracy:0.2497, Validation Loss:1.6047, Validation Accuracy:0.2447\n",
    "Epoch #93: Loss:1.5928, Accuracy:0.2509, Validation Loss:1.6057, Validation Accuracy:0.2381\n",
    "Epoch #94: Loss:1.5937, Accuracy:0.2448, Validation Loss:1.6034, Validation Accuracy:0.2365\n",
    "Epoch #95: Loss:1.5916, Accuracy:0.2374, Validation Loss:1.6027, Validation Accuracy:0.2512\n",
    "Epoch #96: Loss:1.5922, Accuracy:0.2485, Validation Loss:1.6036, Validation Accuracy:0.2397\n",
    "Epoch #97: Loss:1.5910, Accuracy:0.2501, Validation Loss:1.6042, Validation Accuracy:0.2430\n",
    "Epoch #98: Loss:1.5908, Accuracy:0.2480, Validation Loss:1.6049, Validation Accuracy:0.2381\n",
    "Epoch #99: Loss:1.5898, Accuracy:0.2497, Validation Loss:1.6066, Validation Accuracy:0.2397\n",
    "Epoch #100: Loss:1.5888, Accuracy:0.2550, Validation Loss:1.6065, Validation Accuracy:0.2414\n",
    "Epoch #101: Loss:1.5885, Accuracy:0.2509, Validation Loss:1.6061, Validation Accuracy:0.2479\n",
    "Epoch #102: Loss:1.5892, Accuracy:0.2526, Validation Loss:1.6082, Validation Accuracy:0.2397\n",
    "Epoch #103: Loss:1.5897, Accuracy:0.2501, Validation Loss:1.6083, Validation Accuracy:0.2348\n",
    "Epoch #104: Loss:1.5890, Accuracy:0.2460, Validation Loss:1.6080, Validation Accuracy:0.2447\n",
    "Epoch #105: Loss:1.5882, Accuracy:0.2600, Validation Loss:1.6089, Validation Accuracy:0.2282\n",
    "Epoch #106: Loss:1.5889, Accuracy:0.2534, Validation Loss:1.6082, Validation Accuracy:0.2447\n",
    "Epoch #107: Loss:1.5910, Accuracy:0.2538, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #108: Loss:1.5874, Accuracy:0.2554, Validation Loss:1.6127, Validation Accuracy:0.2348\n",
    "Epoch #109: Loss:1.5889, Accuracy:0.2559, Validation Loss:1.6078, Validation Accuracy:0.2315\n",
    "Epoch #110: Loss:1.5885, Accuracy:0.2554, Validation Loss:1.6119, Validation Accuracy:0.2348\n",
    "Epoch #111: Loss:1.5875, Accuracy:0.2563, Validation Loss:1.6083, Validation Accuracy:0.2250\n",
    "Epoch #112: Loss:1.5882, Accuracy:0.2538, Validation Loss:1.6070, Validation Accuracy:0.2365\n",
    "Epoch #113: Loss:1.5845, Accuracy:0.2538, Validation Loss:1.6056, Validation Accuracy:0.2430\n",
    "Epoch #114: Loss:1.5842, Accuracy:0.2612, Validation Loss:1.6138, Validation Accuracy:0.2397\n",
    "Epoch #115: Loss:1.5825, Accuracy:0.2583, Validation Loss:1.6061, Validation Accuracy:0.2348\n",
    "Epoch #116: Loss:1.5825, Accuracy:0.2608, Validation Loss:1.6047, Validation Accuracy:0.2463\n",
    "Epoch #117: Loss:1.5814, Accuracy:0.2616, Validation Loss:1.6061, Validation Accuracy:0.2397\n",
    "Epoch #118: Loss:1.5817, Accuracy:0.2604, Validation Loss:1.6104, Validation Accuracy:0.2496\n",
    "Epoch #119: Loss:1.5814, Accuracy:0.2793, Validation Loss:1.6162, Validation Accuracy:0.2430\n",
    "Epoch #120: Loss:1.5825, Accuracy:0.2690, Validation Loss:1.6193, Validation Accuracy:0.2447\n",
    "Epoch #121: Loss:1.5850, Accuracy:0.2657, Validation Loss:1.6180, Validation Accuracy:0.2397\n",
    "Epoch #122: Loss:1.5819, Accuracy:0.2632, Validation Loss:1.6182, Validation Accuracy:0.2348\n",
    "Epoch #123: Loss:1.5864, Accuracy:0.2608, Validation Loss:1.6240, Validation Accuracy:0.2348\n",
    "Epoch #124: Loss:1.5861, Accuracy:0.2534, Validation Loss:1.6130, Validation Accuracy:0.2381\n",
    "Epoch #125: Loss:1.5828, Accuracy:0.2604, Validation Loss:1.6241, Validation Accuracy:0.2233\n",
    "Epoch #126: Loss:1.5845, Accuracy:0.2624, Validation Loss:1.6166, Validation Accuracy:0.2282\n",
    "Epoch #127: Loss:1.5899, Accuracy:0.2583, Validation Loss:1.6152, Validation Accuracy:0.2118\n",
    "Epoch #128: Loss:1.5919, Accuracy:0.2571, Validation Loss:1.6159, Validation Accuracy:0.2167\n",
    "Epoch #129: Loss:1.5904, Accuracy:0.2513, Validation Loss:1.6103, Validation Accuracy:0.2315\n",
    "Epoch #130: Loss:1.5887, Accuracy:0.2559, Validation Loss:1.6110, Validation Accuracy:0.2381\n",
    "Epoch #131: Loss:1.5883, Accuracy:0.2505, Validation Loss:1.6136, Validation Accuracy:0.2233\n",
    "Epoch #132: Loss:1.5848, Accuracy:0.2579, Validation Loss:1.6116, Validation Accuracy:0.2430\n",
    "Epoch #133: Loss:1.5830, Accuracy:0.2575, Validation Loss:1.6178, Validation Accuracy:0.2020\n",
    "Epoch #134: Loss:1.5810, Accuracy:0.2657, Validation Loss:1.6140, Validation Accuracy:0.2200\n",
    "Epoch #135: Loss:1.5818, Accuracy:0.2669, Validation Loss:1.6148, Validation Accuracy:0.2167\n",
    "Epoch #136: Loss:1.5805, Accuracy:0.2653, Validation Loss:1.6219, Validation Accuracy:0.2118\n",
    "Epoch #137: Loss:1.5855, Accuracy:0.2604, Validation Loss:1.6236, Validation Accuracy:0.2167\n",
    "Epoch #138: Loss:1.5905, Accuracy:0.2620, Validation Loss:1.6212, Validation Accuracy:0.2118\n",
    "Epoch #139: Loss:1.5851, Accuracy:0.2616, Validation Loss:1.6256, Validation Accuracy:0.2102\n",
    "Epoch #140: Loss:1.5835, Accuracy:0.2632, Validation Loss:1.6237, Validation Accuracy:0.2233\n",
    "Epoch #141: Loss:1.5812, Accuracy:0.2678, Validation Loss:1.6179, Validation Accuracy:0.2200\n",
    "Epoch #142: Loss:1.5829, Accuracy:0.2550, Validation Loss:1.6152, Validation Accuracy:0.2299\n",
    "Epoch #143: Loss:1.5813, Accuracy:0.2612, Validation Loss:1.6144, Validation Accuracy:0.2332\n",
    "Epoch #144: Loss:1.5815, Accuracy:0.2632, Validation Loss:1.6150, Validation Accuracy:0.2299\n",
    "Epoch #145: Loss:1.5802, Accuracy:0.2690, Validation Loss:1.6187, Validation Accuracy:0.2266\n",
    "Epoch #146: Loss:1.5788, Accuracy:0.2641, Validation Loss:1.6205, Validation Accuracy:0.2282\n",
    "Epoch #147: Loss:1.5768, Accuracy:0.2649, Validation Loss:1.6252, Validation Accuracy:0.2135\n",
    "Epoch #148: Loss:1.5772, Accuracy:0.2723, Validation Loss:1.6276, Validation Accuracy:0.2217\n",
    "Epoch #149: Loss:1.5754, Accuracy:0.2702, Validation Loss:1.6271, Validation Accuracy:0.2266\n",
    "Epoch #150: Loss:1.5758, Accuracy:0.2706, Validation Loss:1.6255, Validation Accuracy:0.2299\n",
    "Epoch #151: Loss:1.5764, Accuracy:0.2731, Validation Loss:1.6220, Validation Accuracy:0.2529\n",
    "Epoch #152: Loss:1.5729, Accuracy:0.2690, Validation Loss:1.6251, Validation Accuracy:0.2479\n",
    "Epoch #153: Loss:1.5742, Accuracy:0.2669, Validation Loss:1.6273, Validation Accuracy:0.2397\n",
    "Epoch #154: Loss:1.5713, Accuracy:0.2587, Validation Loss:1.6207, Validation Accuracy:0.2397\n",
    "Epoch #155: Loss:1.5695, Accuracy:0.2649, Validation Loss:1.6286, Validation Accuracy:0.2315\n",
    "Epoch #156: Loss:1.5691, Accuracy:0.2669, Validation Loss:1.6314, Validation Accuracy:0.2266\n",
    "Epoch #157: Loss:1.5678, Accuracy:0.2809, Validation Loss:1.6365, Validation Accuracy:0.2167\n",
    "Epoch #158: Loss:1.5730, Accuracy:0.2637, Validation Loss:1.6302, Validation Accuracy:0.2365\n",
    "Epoch #159: Loss:1.5776, Accuracy:0.2665, Validation Loss:1.6297, Validation Accuracy:0.2332\n",
    "Epoch #160: Loss:1.5882, Accuracy:0.2649, Validation Loss:1.6323, Validation Accuracy:0.2381\n",
    "Epoch #161: Loss:1.5830, Accuracy:0.2645, Validation Loss:1.6123, Validation Accuracy:0.2430\n",
    "Epoch #162: Loss:1.5770, Accuracy:0.2669, Validation Loss:1.6194, Validation Accuracy:0.2184\n",
    "Epoch #163: Loss:1.5748, Accuracy:0.2649, Validation Loss:1.6277, Validation Accuracy:0.2135\n",
    "Epoch #164: Loss:1.5752, Accuracy:0.2604, Validation Loss:1.6284, Validation Accuracy:0.2200\n",
    "Epoch #165: Loss:1.5818, Accuracy:0.2620, Validation Loss:1.6377, Validation Accuracy:0.2135\n",
    "Epoch #166: Loss:1.5771, Accuracy:0.2612, Validation Loss:1.6257, Validation Accuracy:0.2200\n",
    "Epoch #167: Loss:1.5809, Accuracy:0.2665, Validation Loss:1.6275, Validation Accuracy:0.2233\n",
    "Epoch #168: Loss:1.5801, Accuracy:0.2665, Validation Loss:1.6240, Validation Accuracy:0.2135\n",
    "Epoch #169: Loss:1.5772, Accuracy:0.2678, Validation Loss:1.6226, Validation Accuracy:0.2233\n",
    "Epoch #170: Loss:1.5753, Accuracy:0.2764, Validation Loss:1.6275, Validation Accuracy:0.2151\n",
    "Epoch #171: Loss:1.5724, Accuracy:0.2702, Validation Loss:1.6253, Validation Accuracy:0.2250\n",
    "Epoch #172: Loss:1.5695, Accuracy:0.2702, Validation Loss:1.6241, Validation Accuracy:0.2217\n",
    "Epoch #173: Loss:1.5693, Accuracy:0.2686, Validation Loss:1.6288, Validation Accuracy:0.2217\n",
    "Epoch #174: Loss:1.5694, Accuracy:0.2661, Validation Loss:1.6339, Validation Accuracy:0.2167\n",
    "Epoch #175: Loss:1.5666, Accuracy:0.2682, Validation Loss:1.6413, Validation Accuracy:0.2151\n",
    "Epoch #176: Loss:1.5634, Accuracy:0.2690, Validation Loss:1.6402, Validation Accuracy:0.2266\n",
    "Epoch #177: Loss:1.5633, Accuracy:0.2801, Validation Loss:1.6354, Validation Accuracy:0.2381\n",
    "Epoch #178: Loss:1.5697, Accuracy:0.2694, Validation Loss:1.6439, Validation Accuracy:0.2200\n",
    "Epoch #179: Loss:1.5630, Accuracy:0.2875, Validation Loss:1.6446, Validation Accuracy:0.2118\n",
    "Epoch #180: Loss:1.5636, Accuracy:0.2727, Validation Loss:1.6467, Validation Accuracy:0.2167\n",
    "Epoch #181: Loss:1.5653, Accuracy:0.2739, Validation Loss:1.6403, Validation Accuracy:0.2135\n",
    "Epoch #182: Loss:1.5687, Accuracy:0.2723, Validation Loss:1.6375, Validation Accuracy:0.2167\n",
    "Epoch #183: Loss:1.5698, Accuracy:0.2653, Validation Loss:1.6374, Validation Accuracy:0.2069\n",
    "Epoch #184: Loss:1.5713, Accuracy:0.2600, Validation Loss:1.6452, Validation Accuracy:0.2118\n",
    "Epoch #185: Loss:1.5704, Accuracy:0.2706, Validation Loss:1.6351, Validation Accuracy:0.2266\n",
    "Epoch #186: Loss:1.5657, Accuracy:0.2657, Validation Loss:1.6429, Validation Accuracy:0.2053\n",
    "Epoch #187: Loss:1.5640, Accuracy:0.2727, Validation Loss:1.6433, Validation Accuracy:0.2053\n",
    "Epoch #188: Loss:1.5604, Accuracy:0.2690, Validation Loss:1.6455, Validation Accuracy:0.2200\n",
    "Epoch #189: Loss:1.5599, Accuracy:0.2760, Validation Loss:1.6422, Validation Accuracy:0.2118\n",
    "Epoch #190: Loss:1.5617, Accuracy:0.2784, Validation Loss:1.6412, Validation Accuracy:0.2282\n",
    "Epoch #191: Loss:1.5610, Accuracy:0.2793, Validation Loss:1.6384, Validation Accuracy:0.2135\n",
    "Epoch #192: Loss:1.5609, Accuracy:0.2760, Validation Loss:1.6414, Validation Accuracy:0.2299\n",
    "Epoch #193: Loss:1.5565, Accuracy:0.2772, Validation Loss:1.6548, Validation Accuracy:0.2069\n",
    "Epoch #194: Loss:1.5586, Accuracy:0.2821, Validation Loss:1.6552, Validation Accuracy:0.2167\n",
    "Epoch #195: Loss:1.5568, Accuracy:0.2838, Validation Loss:1.6490, Validation Accuracy:0.2003\n",
    "Epoch #196: Loss:1.5593, Accuracy:0.2809, Validation Loss:1.6687, Validation Accuracy:0.1839\n",
    "Epoch #197: Loss:1.5741, Accuracy:0.2772, Validation Loss:1.6666, Validation Accuracy:0.2135\n",
    "Epoch #198: Loss:1.5710, Accuracy:0.2694, Validation Loss:1.6576, Validation Accuracy:0.2151\n",
    "Epoch #199: Loss:1.5671, Accuracy:0.2715, Validation Loss:1.6414, Validation Accuracy:0.2184\n",
    "Epoch #200: Loss:1.5670, Accuracy:0.2747, Validation Loss:1.6374, Validation Accuracy:0.2184\n",
    "Epoch #201: Loss:1.5645, Accuracy:0.2756, Validation Loss:1.6347, Validation Accuracy:0.2184\n",
    "Epoch #202: Loss:1.5671, Accuracy:0.2698, Validation Loss:1.6289, Validation Accuracy:0.2184\n",
    "Epoch #203: Loss:1.5671, Accuracy:0.2731, Validation Loss:1.6345, Validation Accuracy:0.2200\n",
    "Epoch #204: Loss:1.5645, Accuracy:0.2838, Validation Loss:1.6357, Validation Accuracy:0.2069\n",
    "Epoch #205: Loss:1.5641, Accuracy:0.2793, Validation Loss:1.6380, Validation Accuracy:0.2184\n",
    "Epoch #206: Loss:1.5611, Accuracy:0.2772, Validation Loss:1.6363, Validation Accuracy:0.2200\n",
    "Epoch #207: Loss:1.5613, Accuracy:0.2768, Validation Loss:1.6375, Validation Accuracy:0.2167\n",
    "Epoch #208: Loss:1.5604, Accuracy:0.2764, Validation Loss:1.6413, Validation Accuracy:0.2118\n",
    "Epoch #209: Loss:1.5645, Accuracy:0.2706, Validation Loss:1.6530, Validation Accuracy:0.2200\n",
    "Epoch #210: Loss:1.5598, Accuracy:0.2780, Validation Loss:1.6405, Validation Accuracy:0.2299\n",
    "Epoch #211: Loss:1.5597, Accuracy:0.2727, Validation Loss:1.6445, Validation Accuracy:0.2167\n",
    "Epoch #212: Loss:1.5576, Accuracy:0.2719, Validation Loss:1.6494, Validation Accuracy:0.2200\n",
    "Epoch #213: Loss:1.5589, Accuracy:0.2719, Validation Loss:1.6562, Validation Accuracy:0.2085\n",
    "Epoch #214: Loss:1.5617, Accuracy:0.2780, Validation Loss:1.6557, Validation Accuracy:0.2200\n",
    "Epoch #215: Loss:1.5597, Accuracy:0.2719, Validation Loss:1.6547, Validation Accuracy:0.2282\n",
    "Epoch #216: Loss:1.5586, Accuracy:0.2789, Validation Loss:1.6472, Validation Accuracy:0.2200\n",
    "Epoch #217: Loss:1.5616, Accuracy:0.2727, Validation Loss:1.6441, Validation Accuracy:0.2135\n",
    "Epoch #218: Loss:1.5648, Accuracy:0.2632, Validation Loss:1.6314, Validation Accuracy:0.2250\n",
    "Epoch #219: Loss:1.5617, Accuracy:0.2752, Validation Loss:1.6365, Validation Accuracy:0.2266\n",
    "Epoch #220: Loss:1.5594, Accuracy:0.2665, Validation Loss:1.6451, Validation Accuracy:0.2118\n",
    "Epoch #221: Loss:1.5619, Accuracy:0.2706, Validation Loss:1.6377, Validation Accuracy:0.2282\n",
    "Epoch #222: Loss:1.5514, Accuracy:0.2834, Validation Loss:1.6592, Validation Accuracy:0.2414\n",
    "Epoch #223: Loss:1.5499, Accuracy:0.2805, Validation Loss:1.6589, Validation Accuracy:0.2200\n",
    "Epoch #224: Loss:1.5496, Accuracy:0.2924, Validation Loss:1.6650, Validation Accuracy:0.2102\n",
    "Epoch #225: Loss:1.5490, Accuracy:0.2776, Validation Loss:1.6555, Validation Accuracy:0.2118\n",
    "Epoch #226: Loss:1.5440, Accuracy:0.2871, Validation Loss:1.6557, Validation Accuracy:0.2036\n",
    "Epoch #227: Loss:1.5441, Accuracy:0.2891, Validation Loss:1.6578, Validation Accuracy:0.1954\n",
    "Epoch #228: Loss:1.5444, Accuracy:0.2813, Validation Loss:1.6596, Validation Accuracy:0.2102\n",
    "Epoch #229: Loss:1.5432, Accuracy:0.2945, Validation Loss:1.6601, Validation Accuracy:0.2053\n",
    "Epoch #230: Loss:1.5494, Accuracy:0.2735, Validation Loss:1.6587, Validation Accuracy:0.2118\n",
    "Epoch #231: Loss:1.5542, Accuracy:0.2862, Validation Loss:1.6551, Validation Accuracy:0.2397\n",
    "Epoch #232: Loss:1.5510, Accuracy:0.2801, Validation Loss:1.6574, Validation Accuracy:0.2184\n",
    "Epoch #233: Loss:1.5500, Accuracy:0.2793, Validation Loss:1.6630, Validation Accuracy:0.2020\n",
    "Epoch #234: Loss:1.5580, Accuracy:0.2838, Validation Loss:1.6542, Validation Accuracy:0.2020\n",
    "Epoch #235: Loss:1.5592, Accuracy:0.2842, Validation Loss:1.6481, Validation Accuracy:0.2200\n",
    "Epoch #236: Loss:1.5485, Accuracy:0.2871, Validation Loss:1.6503, Validation Accuracy:0.2053\n",
    "Epoch #237: Loss:1.5450, Accuracy:0.2912, Validation Loss:1.6581, Validation Accuracy:0.2135\n",
    "Epoch #238: Loss:1.5453, Accuracy:0.2961, Validation Loss:1.6620, Validation Accuracy:0.2036\n",
    "Epoch #239: Loss:1.5445, Accuracy:0.2957, Validation Loss:1.6631, Validation Accuracy:0.2167\n",
    "Epoch #240: Loss:1.5475, Accuracy:0.2830, Validation Loss:1.6698, Validation Accuracy:0.2167\n",
    "Epoch #241: Loss:1.5472, Accuracy:0.2875, Validation Loss:1.6727, Validation Accuracy:0.2085\n",
    "Epoch #242: Loss:1.5457, Accuracy:0.2928, Validation Loss:1.6666, Validation Accuracy:0.2151\n",
    "Epoch #243: Loss:1.5401, Accuracy:0.2932, Validation Loss:1.6704, Validation Accuracy:0.2118\n",
    "Epoch #244: Loss:1.5446, Accuracy:0.2908, Validation Loss:1.6715, Validation Accuracy:0.2118\n",
    "Epoch #245: Loss:1.5474, Accuracy:0.2891, Validation Loss:1.6733, Validation Accuracy:0.2151\n",
    "Epoch #246: Loss:1.5510, Accuracy:0.2916, Validation Loss:1.6680, Validation Accuracy:0.2200\n",
    "Epoch #247: Loss:1.5699, Accuracy:0.2682, Validation Loss:1.6493, Validation Accuracy:0.2151\n",
    "Epoch #248: Loss:1.5900, Accuracy:0.2480, Validation Loss:1.6453, Validation Accuracy:0.1872\n",
    "Epoch #249: Loss:1.5876, Accuracy:0.2283, Validation Loss:1.6236, Validation Accuracy:0.1888\n",
    "Epoch #250: Loss:1.5851, Accuracy:0.2509, Validation Loss:1.6230, Validation Accuracy:0.2184\n",
    "Epoch #251: Loss:1.5811, Accuracy:0.2669, Validation Loss:1.6269, Validation Accuracy:0.2102\n",
    "Epoch #252: Loss:1.5736, Accuracy:0.2661, Validation Loss:1.6349, Validation Accuracy:0.2151\n",
    "Epoch #253: Loss:1.5666, Accuracy:0.2674, Validation Loss:1.6399, Validation Accuracy:0.2266\n",
    "Epoch #254: Loss:1.5607, Accuracy:0.2637, Validation Loss:1.6548, Validation Accuracy:0.2348\n",
    "Epoch #255: Loss:1.5600, Accuracy:0.2768, Validation Loss:1.6563, Validation Accuracy:0.2266\n",
    "Epoch #256: Loss:1.5590, Accuracy:0.2727, Validation Loss:1.6625, Validation Accuracy:0.2315\n",
    "Epoch #257: Loss:1.5595, Accuracy:0.2747, Validation Loss:1.6711, Validation Accuracy:0.2266\n",
    "Epoch #258: Loss:1.5575, Accuracy:0.2821, Validation Loss:1.6714, Validation Accuracy:0.2266\n",
    "Epoch #259: Loss:1.5603, Accuracy:0.2801, Validation Loss:1.6848, Validation Accuracy:0.1970\n",
    "Epoch #260: Loss:1.5592, Accuracy:0.2706, Validation Loss:1.6820, Validation Accuracy:0.2250\n",
    "Epoch #261: Loss:1.5612, Accuracy:0.2772, Validation Loss:1.6840, Validation Accuracy:0.1823\n",
    "Epoch #262: Loss:1.5655, Accuracy:0.2637, Validation Loss:1.6634, Validation Accuracy:0.2217\n",
    "Epoch #263: Loss:1.5626, Accuracy:0.2825, Validation Loss:1.6485, Validation Accuracy:0.2250\n",
    "Epoch #264: Loss:1.5570, Accuracy:0.2842, Validation Loss:1.6550, Validation Accuracy:0.1921\n",
    "Epoch #265: Loss:1.5509, Accuracy:0.2834, Validation Loss:1.6563, Validation Accuracy:0.2233\n",
    "Epoch #266: Loss:1.5618, Accuracy:0.2871, Validation Loss:1.6657, Validation Accuracy:0.2135\n",
    "Epoch #267: Loss:1.5653, Accuracy:0.2669, Validation Loss:1.6554, Validation Accuracy:0.2102\n",
    "Epoch #268: Loss:1.5696, Accuracy:0.2628, Validation Loss:1.6694, Validation Accuracy:0.2282\n",
    "Epoch #269: Loss:1.5674, Accuracy:0.2727, Validation Loss:1.6572, Validation Accuracy:0.2217\n",
    "Epoch #270: Loss:1.5618, Accuracy:0.2797, Validation Loss:1.6412, Validation Accuracy:0.2397\n",
    "Epoch #271: Loss:1.5598, Accuracy:0.2764, Validation Loss:1.6517, Validation Accuracy:0.2447\n",
    "Epoch #272: Loss:1.5565, Accuracy:0.2760, Validation Loss:1.6470, Validation Accuracy:0.2381\n",
    "Epoch #273: Loss:1.5503, Accuracy:0.2887, Validation Loss:1.6648, Validation Accuracy:0.2299\n",
    "Epoch #274: Loss:1.5509, Accuracy:0.2875, Validation Loss:1.6688, Validation Accuracy:0.2102\n",
    "Epoch #275: Loss:1.5565, Accuracy:0.2747, Validation Loss:1.6607, Validation Accuracy:0.2184\n",
    "Epoch #276: Loss:1.5500, Accuracy:0.2825, Validation Loss:1.6621, Validation Accuracy:0.2299\n",
    "Epoch #277: Loss:1.5467, Accuracy:0.2834, Validation Loss:1.6631, Validation Accuracy:0.2102\n",
    "Epoch #278: Loss:1.5452, Accuracy:0.2842, Validation Loss:1.6705, Validation Accuracy:0.2217\n",
    "Epoch #279: Loss:1.5400, Accuracy:0.2928, Validation Loss:1.6800, Validation Accuracy:0.1938\n",
    "Epoch #280: Loss:1.5469, Accuracy:0.2813, Validation Loss:1.6862, Validation Accuracy:0.2069\n",
    "Epoch #281: Loss:1.5695, Accuracy:0.2579, Validation Loss:1.7008, Validation Accuracy:0.2151\n",
    "Epoch #282: Loss:1.5854, Accuracy:0.2460, Validation Loss:1.6631, Validation Accuracy:0.1823\n",
    "Epoch #283: Loss:1.5857, Accuracy:0.2411, Validation Loss:1.6490, Validation Accuracy:0.1839\n",
    "Epoch #284: Loss:1.5821, Accuracy:0.2563, Validation Loss:1.6507, Validation Accuracy:0.2167\n",
    "Epoch #285: Loss:1.5816, Accuracy:0.2616, Validation Loss:1.6279, Validation Accuracy:0.2299\n",
    "Epoch #286: Loss:1.5790, Accuracy:0.2620, Validation Loss:1.6189, Validation Accuracy:0.2479\n",
    "Epoch #287: Loss:1.5766, Accuracy:0.2624, Validation Loss:1.6261, Validation Accuracy:0.2496\n",
    "Epoch #288: Loss:1.5746, Accuracy:0.2628, Validation Loss:1.6317, Validation Accuracy:0.2365\n",
    "Epoch #289: Loss:1.5721, Accuracy:0.2686, Validation Loss:1.6280, Validation Accuracy:0.2512\n",
    "Epoch #290: Loss:1.5707, Accuracy:0.2719, Validation Loss:1.6303, Validation Accuracy:0.2594\n",
    "Epoch #291: Loss:1.5703, Accuracy:0.2710, Validation Loss:1.6369, Validation Accuracy:0.2397\n",
    "Epoch #292: Loss:1.5695, Accuracy:0.2747, Validation Loss:1.6372, Validation Accuracy:0.2430\n",
    "Epoch #293: Loss:1.5675, Accuracy:0.2694, Validation Loss:1.6459, Validation Accuracy:0.2332\n",
    "Epoch #294: Loss:1.5682, Accuracy:0.2739, Validation Loss:1.6352, Validation Accuracy:0.2348\n",
    "Epoch #295: Loss:1.5666, Accuracy:0.2842, Validation Loss:1.6355, Validation Accuracy:0.2282\n",
    "Epoch #296: Loss:1.5652, Accuracy:0.2727, Validation Loss:1.6538, Validation Accuracy:0.2315\n",
    "Epoch #297: Loss:1.5645, Accuracy:0.2739, Validation Loss:1.6432, Validation Accuracy:0.2512\n",
    "Epoch #298: Loss:1.5612, Accuracy:0.2690, Validation Loss:1.6534, Validation Accuracy:0.2348\n",
    "Epoch #299: Loss:1.5595, Accuracy:0.2805, Validation Loss:1.6540, Validation Accuracy:0.2135\n",
    "Epoch #300: Loss:1.5544, Accuracy:0.2809, Validation Loss:1.6483, Validation Accuracy:0.2348\n",
    "\n",
    "Test:\n",
    "Test Loss:1.64834094, Accuracy:0.2348\n",
    "Labels: ['04', '05', '01', '02', '03']\n",
    "Confusion Matrix:\n",
    "      04   05  01  02  03\n",
    "t:04  21   81   4   2   4\n",
    "t:05  19  104  10   4   5\n",
    "t:01  19   91   6   4   6\n",
    "t:02  15   80   8   5   6\n",
    "t:03  11   84  11   2   7\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.25      0.19      0.21       112\n",
    "          05       0.24      0.73      0.36       142\n",
    "          01       0.15      0.05      0.07       126\n",
    "          02       0.29      0.04      0.08       114\n",
    "          03       0.25      0.06      0.10       115\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.24      0.21      0.16       609\n",
    "weighted avg       0.23      0.23      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 00:35:29 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 46 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6047027298969587, 1.6054823057992118, 1.6042275340686292, 1.6041913437725874, 1.6031277367634138, 1.6034811895664884, 1.6026585305657097, 1.6021387054415173, 1.602203636725352, 1.6015916099689278, 1.601496233532973, 1.6015901567509216, 1.60167294711315, 1.6007535113098195, 1.6008538806379722, 1.601560403952262, 1.6026519818846228, 1.60252684384144, 1.6020925949359763, 1.6019329783951708, 1.6021725256454769, 1.6021852974821194, 1.60078788332164, 1.600747994405687, 1.601976541658536, 1.6020489985915436, 1.601781061521696, 1.6019004937658952, 1.6020202186503043, 1.6021123269117132, 1.6022825890965453, 1.6019506879236507, 1.602101050574204, 1.6024968678923859, 1.6024061870105162, 1.6023030604047728, 1.6021082873023398, 1.601904581528775, 1.6015782612689415, 1.6019846925203045, 1.6022754819522351, 1.6025385564966939, 1.6020763914769114, 1.601132556722669, 1.6008214148003088, 1.6005333458457283, 1.600399146134826, 1.6006128400417383, 1.6008016057006635, 1.600690688209972, 1.6012212022380485, 1.6006743569287956, 1.6008825437188736, 1.6005369117498789, 1.6016410184024004, 1.6013118150003243, 1.6009838731809594, 1.6029887538042367, 1.601900172351029, 1.601199228971071, 1.6013539105604826, 1.600695827324402, 1.600339774231997, 1.6006235471499966, 1.6011303162144126, 1.6001093602924317, 1.6003838875415095, 1.6009102869895095, 1.6011367779842933, 1.6008722756688034, 1.6004765020019707, 1.6010085776913145, 1.6021619363762867, 1.6014028787612915, 1.6004031873101672, 1.6022791283079751, 1.603616177937863, 1.60223098067423, 1.603201532011549, 1.602444486860767, 1.6058249072292559, 1.602828006634767, 1.602768600084903, 1.6009014803787758, 1.6009799799895639, 1.6019177937938271, 1.6024977969027114, 1.6029319749481377, 1.6028510073722877, 1.6028734020803166, 1.6033068961893593, 1.6047063345588095, 1.605655658617004, 1.6033896456406818, 1.6026790239932307, 1.6035563742194465, 1.6042410810592727, 1.60486443387268, 1.6066304182967137, 1.6064918438593547, 1.606088848341079, 1.6081949668173328, 1.6082921531204324, 1.6080175972924444, 1.6089035489876282, 1.6081844166973345, 1.610018974846024, 1.6127020173472137, 1.6077603530414, 1.6118833376660526, 1.6083031771413994, 1.6069944028196663, 1.6055789958863032, 1.6138008511908144, 1.6061239078127105, 1.6047316991245413, 1.6060620597234891, 1.6103869846888952, 1.6161638314305071, 1.6192577129905839, 1.6180066367479773, 1.618193633842155, 1.6239861974183758, 1.613015996411516, 1.624091508744777, 1.6165947865187045, 1.61520701225951, 1.6159155045824098, 1.610341181700257, 1.6110010260627383, 1.6135914877717719, 1.6115705364051907, 1.6177771120823075, 1.6139858041099335, 1.6147702429290671, 1.6218918460147527, 1.6236482805806427, 1.6211765624815215, 1.6256274869680796, 1.6236525866002676, 1.6179029015680448, 1.6152020224992474, 1.6143791746036174, 1.6150365713586166, 1.6187362731579684, 1.620458600556322, 1.6251853649643646, 1.6276226370596925, 1.6270818172025758, 1.6255281631190985, 1.62201273911105, 1.6251200737037095, 1.6272870311987615, 1.6207344044605498, 1.6286415409767765, 1.6314352960226375, 1.6364628824302911, 1.6301773821779073, 1.6297369937004127, 1.6322837476855625, 1.6123213556599734, 1.6194394791654765, 1.6277306259950786, 1.6283713359942382, 1.637747737182968, 1.6257165637118085, 1.6275345218201185, 1.624026450421814, 1.6225630095831083, 1.6274919913124373, 1.6252653371720087, 1.624078461689315, 1.6287922305230829, 1.633932386517329, 1.6413023824175004, 1.640201834622275, 1.6353770564929606, 1.643913028079692, 1.6445647284315137, 1.6467287327072695, 1.6402744043049553, 1.6375097087255643, 1.6374170245795414, 1.6451784100242828, 1.635112089280816, 1.6428867496293167, 1.6432636154109035, 1.6455337016649043, 1.6422322839743202, 1.64118791977173, 1.6383680481041594, 1.6413812911373444, 1.6548080859317373, 1.6551570459735414, 1.64902026449714, 1.6687394785763594, 1.6665633096679286, 1.6575517313820975, 1.6413663433886123, 1.6373798355876128, 1.6346844031501482, 1.6288826978461104, 1.6344987056133977, 1.6356630135443802, 1.6379737552555127, 1.6363323784031107, 1.63750042782237, 1.641323344460849, 1.652985115943871, 1.6405071691534985, 1.6444785264129513, 1.6493634437692577, 1.6562215817972945, 1.6557210953952057, 1.6547321167289721, 1.6471998078873984, 1.6441449439780074, 1.6313887143565713, 1.6365235996951024, 1.645076197747918, 1.6376685583336992, 1.659198849659248, 1.6589321896360425, 1.6650046090578603, 1.6555470202748215, 1.6557493078689074, 1.6577790787654558, 1.6595895523312447, 1.6601248306202379, 1.6586918280825436, 1.6551217676579266, 1.657371875492027, 1.6629859375444735, 1.65421487349399, 1.6481066341274868, 1.650276892290914, 1.658052414900368, 1.661959663987747, 1.6631004986504616, 1.669832599769868, 1.6727321241876763, 1.6665576453670883, 1.6703897839892283, 1.6715352482396393, 1.6732881460675268, 1.6680401095811566, 1.6493332816657957, 1.6452519952370028, 1.6236136586012315, 1.623034368203387, 1.6268678022722893, 1.6349189898063397, 1.639913831438337, 1.6547822478565284, 1.6563431799705393, 1.662498038194841, 1.6710704917390946, 1.671403428408117, 1.6847879639987289, 1.6819969401962456, 1.6840315747926584, 1.6634328271367866, 1.6485030711773776, 1.6549748673619111, 1.6563483695874268, 1.6656849887374978, 1.6554405626600794, 1.6693531392243108, 1.6572351117047965, 1.6411580358232771, 1.6516532281349445, 1.6469538106119692, 1.6648311025794895, 1.668827860030439, 1.6606810905271758, 1.6620739637728787, 1.6631015139847554, 1.6705360093531743, 1.6799790786796407, 1.6862215204974897, 1.7008323221175345, 1.6631336188668688, 1.6490251832016192, 1.6506612504448601, 1.6279247022418946, 1.6189215983859033, 1.6260558838523276, 1.6317320361317476, 1.6279648873214847, 1.6302576472215073, 1.6369255058871115, 1.6371972690074903, 1.6458737672060386, 1.6352225806325527, 1.6355219319927672, 1.6538210197035315, 1.6432342494062602, 1.6534340123237647, 1.65397661327337, 1.648340890364498], 'val_acc': [0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.24302134436535328, 0.24466338058802098, 0.24302134436535328, 0.23973727221363675, 0.23645319996404726, 0.23152709158966303, 0.23481116383925252, 0.23645319996404726, 0.23152709149179004, 0.24137930853417747, 0.23152709158966303, 0.2348111639371255, 0.24302134446322624, 0.238095236088842, 0.23809523599096902, 0.2298850554648683, 0.24302134446322624, 0.24302134446322624, 0.22495894699261107, 0.22495894699261107, 0.2266009831174058, 0.23973727221363675, 0.22495894709048403, 0.22824301934007354, 0.2282430194379465, 0.23645319996404726, 0.22824301934007354, 0.2298850554648683, 0.2298850554648683, 0.23152709158966303, 0.24302134446322624, 0.23152709158966303, 0.23152709158966303, 0.23152709158966303, 0.23152709158966303, 0.23645319996404726, 0.23645319996404726, 0.22824301934007354, 0.2298850554648683, 0.23152709158966303, 0.23645319996404726, 0.23316912781233076, 0.2413793083384315, 0.23152709158966303, 0.23316912781233076, 0.2413793082405585, 0.2364532019459751, 0.23973727221363675, 0.23809523807076985, 0.2397372741955646, 0.23973727221363675, 0.24302134446322624, 0.24302134456109922, 0.238095236186715, 0.24630541671281572, 0.23645320006192025, 0.24302134644515408, 0.2413793083384315, 0.23316912781233076, 0.2397372719200178, 0.24794745283761047, 0.2298850554648683, 0.2463054185968706, 0.24466338256994882, 0.22988505556274125, 0.23481116374137953, 0.23481116572330737, 0.24302134615153514, 0.23973727221363675, 0.22824301934007354, 0.238095236186715, 0.24302134634728112, 0.2479474548195383, 0.24794745303335644, 0.2397372720178908, 0.2495894890602782, 0.24302134436535328, 0.24466338088163994, 0.23809523599096902, 0.25287356101624875, 0.24466338078376695, 0.23809523599096902, 0.23645319986617427, 0.2512315253808189, 0.23973727211576376, 0.24302134475684517, 0.23809523589309606, 0.2397372720178908, 0.24137930853417747, 0.24794745264186452, 0.23973727250725568, 0.23481116364350654, 0.24466338039227503, 0.2282430195358195, 0.24466338029440204, 0.22824301934007354, 0.23481116364350654, 0.23152709158966303, 0.23481116582118036, 0.22495894709048403, 0.23645319967042833, 0.24302134436535328, 0.23973727250725568, 0.23481116582118036, 0.2463054164191968, 0.2397372719200178, 0.24958948886453225, 0.2430213441696073, 0.24466338068589397, 0.23973727380407267, 0.23481116383925252, 0.23481116572330737, 0.23809523628458798, 0.2233169126539982, 0.2282430213220014, 0.21182265779850715, 0.21674876636863732, 0.23152709158966303, 0.238095236186715, 0.22331691096568929, 0.2430213446589722, 0.20197044322741248, 0.2200328387160998, 0.2167487665643833, 0.21182265809212608, 0.2167487664665103, 0.2118226579942531, 0.2101806216737124, 0.22331691076994334, 0.2200328387160998, 0.2298850553669953, 0.2331691276165848, 0.2298850553669953, 0.2266009832152788, 0.22824301924220056, 0.21346469610097568, 0.22167487672494943, 0.22660098292165984, 0.22988505556274125, 0.2528735614077407, 0.24794745283761047, 0.2397372719200178, 0.23973727240938272, 0.23152709318009895, 0.22660098331315176, 0.2167487665643833, 0.2364532002576662, 0.23316912781233076, 0.23809523648033393, 0.24302134446322624, 0.21839080259130506, 0.21346469411904784, 0.22003283852035385, 0.21346469619884867, 0.22003283891184577, 0.22331691116143526, 0.21346469402117488, 0.22331691096568929, 0.21510673034171557, 0.22495894718835702, 0.22167487484089454, 0.22167487493876753, 0.21674876627076436, 0.2151067323236434, 0.2266009831174058, 0.23809523807076985, 0.22003283891184577, 0.2118226579942531, 0.21674876676012925, 0.21346469619884867, 0.21674876844843816, 0.2068965516017967, 0.21182266007405393, 0.22660098331315176, 0.20525451537912898, 0.20525451547700196, 0.2200328387160998, 0.2118226578963801, 0.22824301934007354, 0.21346469541586482, 0.2298850554648683, 0.20689654971774185, 0.2167487665643833, 0.20032840492494391, 0.1839080456589243, 0.21346469441266677, 0.21510673053746154, 0.218390802884924, 0.21839080278705103, 0.21839080437748695, 0.21839080268917804, 0.2200328387160998, 0.20689655130817777, 0.21839080437748695, 0.22003283842248086, 0.21674876627076436, 0.2118226578963801, 0.2200328404044087, 0.22988505566061423, 0.21674876627076436, 0.2200328387160998, 0.20853858564679062, 0.22003283861822684, 0.2282430194379465, 0.22003283852035385, 0.2134646939233019, 0.22495894718835702, 0.2266009832152788, 0.211822659780435, 0.2282430195358195, 0.24137930804481256, 0.22003283861822684, 0.2101806216737124, 0.21182266007405393, 0.2036124794500802, 0.19540229853248753, 0.21018062365564025, 0.20525451508551004, 0.211822659780435, 0.23973727231150974, 0.2183908023955591, 0.20197044332528544, 0.20197044322741248, 0.22003283842248086, 0.20525451557487495, 0.21346469411904784, 0.2036124788628423, 0.21674876844843816, 0.21674876627076436, 0.20853858733509953, 0.21510673014596962, 0.211822659780435, 0.2118226578963801, 0.21510673014596962, 0.22003283852035385, 0.21510673034171557, 0.1871921174191489, 0.18883415403330855, 0.2183908023955591, 0.21018062216307729, 0.21510673004809663, 0.22660098341102475, 0.23481116413287145, 0.22660098341102475, 0.23152709198115495, 0.22660098331315176, 0.22660098341102475, 0.19704433465728227, 0.22495894718835702, 0.1822660093383836, 0.2216748746451486, 0.22495894699261107, 0.1921182259892791, 0.2233169113571812, 0.21346469590522973, 0.21018062326414833, 0.22824301924220056, 0.22167487613771153, 0.23973727231150974, 0.24466338029440204, 0.238095236088842, 0.22988505556274125, 0.21018062394925918, 0.2183908044753599, 0.22988505556274125, 0.21018062355776726, 0.22167487523238646, 0.1937602623098198, 0.20689654961986886, 0.21510673024384258, 0.1822660093383836, 0.18390804585467027, 0.2167487664665103, 0.22988505517124935, 0.24794745254399153, 0.24958948896240524, 0.23645319996404726, 0.25123152697125484, 0.2594417058090467, 0.2397372741955646, 0.24302134456109922, 0.23316912969638562, 0.23481116572330737, 0.2282430212241284, 0.23152709139391706, 0.2512315250872, 0.23481116413287145, 0.21346469610097568, 0.23481116413287145], 'loss': [1.606498875216537, 1.6059978212419233, 1.6057748264845393, 1.6051609394241897, 1.6050115517767058, 1.60412977828627, 1.6042862410907628, 1.6035988848801757, 1.603087324185538, 1.6030929960509346, 1.6026814785826133, 1.6026718542071583, 1.6024908111325524, 1.6020077129164272, 1.6019224525232334, 1.6023783654402903, 1.603046626036172, 1.602509779166392, 1.6023657318013405, 1.6027081640838843, 1.6027952300449662, 1.6023253024236377, 1.6026255392195998, 1.6023656285518983, 1.602173597562974, 1.6019743538979876, 1.602334302161998, 1.6011817550757093, 1.6009176851053257, 1.601032714530428, 1.600707314342444, 1.6006962223464214, 1.6004586284410292, 1.60031741365515, 1.6001952164961328, 1.5998482099548748, 1.6001627755116143, 1.6000419108040278, 1.6001079403644225, 1.600051136330168, 1.6007607525134233, 1.6018543099475837, 1.6009245839696644, 1.5998385816628928, 1.5993282685289638, 1.5995940157030641, 1.599037876608925, 1.5990734991841247, 1.598621506906388, 1.5981080626560187, 1.59831086110775, 1.5981278848843898, 1.5980310742615185, 1.5983930296476863, 1.5974880376635636, 1.5979002452239364, 1.5984002479537556, 1.6001953659116366, 1.600685570078464, 1.5989199483663885, 1.5977623342733362, 1.5970423060031398, 1.597106206784258, 1.596243387081295, 1.597067750699711, 1.5963607820397285, 1.5958795617982837, 1.5956549984718496, 1.5944786330268124, 1.595379735067395, 1.5965462535803323, 1.5962752339776292, 1.5964372557046722, 1.5942883415633404, 1.5939896590411051, 1.5935544423743684, 1.593830662586361, 1.5956904704565875, 1.596460132726164, 1.5993759179751732, 1.5949660871552735, 1.5983715700173036, 1.596207432090869, 1.5948431441426523, 1.5950179188158478, 1.5954789030968042, 1.5950418676439007, 1.5949078935372512, 1.5949091716468702, 1.5938626260483288, 1.5955028322932656, 1.5938860767920648, 1.5928168021677946, 1.593693129384787, 1.5916396352055135, 1.5922453180230862, 1.5909964771975726, 1.5908047375982548, 1.5898100552372865, 1.5888077768212225, 1.588486559288213, 1.5891729690700587, 1.5896722784766917, 1.5890334575328005, 1.5882426189935672, 1.5888523770064054, 1.5910059523533502, 1.5873592505464809, 1.588851790163796, 1.5885069911240062, 1.5874590128599007, 1.5881516290151607, 1.5845409594032553, 1.5841685369029428, 1.5824967484699382, 1.5824979559841588, 1.5814317072686228, 1.5816809641996694, 1.5814447137119834, 1.5824835054193924, 1.585018638663713, 1.5819423673578845, 1.5863739043535394, 1.5861412648547601, 1.5828192230612346, 1.584541142500891, 1.5898649031131908, 1.591916671376943, 1.590358625349323, 1.58871231416902, 1.5883462648861706, 1.5847764972299032, 1.5829633227608777, 1.5809535693828576, 1.581775720310407, 1.5804990221097974, 1.5854706795063842, 1.5904991789764937, 1.5851153454496631, 1.5834818634898755, 1.581192783408586, 1.5829205389140324, 1.5812709627699804, 1.5815468220740128, 1.580236918138038, 1.5788092497682669, 1.5767883410933572, 1.577158311798832, 1.5754040032196828, 1.575819197327694, 1.5763884640817034, 1.5729202888339942, 1.574249148662575, 1.5713108377534997, 1.5695320709530087, 1.5690648094094999, 1.5678118638678988, 1.5729797569878048, 1.57761566761338, 1.5882181102979844, 1.5830086921519568, 1.5770095653847258, 1.5748343617519558, 1.5752491981341854, 1.5818142818963992, 1.577091555135206, 1.5809311912779447, 1.5800609872081686, 1.577241207882609, 1.5752868547576653, 1.5724110494648895, 1.569469766793065, 1.5693137525777796, 1.5694074336508217, 1.5665879284821496, 1.5634076143437097, 1.5632714545702298, 1.5697356608858832, 1.5630332496866308, 1.563614300095325, 1.5653151949817885, 1.568738690489861, 1.56983019008284, 1.5712874324414765, 1.5704257058411897, 1.5656772788300406, 1.5639779873207609, 1.5603561289256602, 1.5599452545021104, 1.5616650685637394, 1.5609900403071724, 1.5608690697554446, 1.5565213641102065, 1.5585556809662304, 1.5568153107680334, 1.5592556666055009, 1.5740751160733264, 1.5709846078982344, 1.5670884442770017, 1.5669607637844047, 1.5645236875976625, 1.5671052273783597, 1.5671253588654912, 1.564458259371027, 1.5640748633985893, 1.5610652575992217, 1.561299569895625, 1.5604278892951824, 1.5645020974490187, 1.5597662759757385, 1.559680808300355, 1.5575776653857691, 1.5589472460795721, 1.5617156854645182, 1.559652456414773, 1.5585708976036714, 1.5616116921759728, 1.5647919361596234, 1.5617079012203021, 1.559415052314069, 1.5619394988249948, 1.5513772549815246, 1.5499252116655666, 1.549607344623464, 1.5490325089597603, 1.5440288814431098, 1.5440623724974647, 1.5443513505512685, 1.5432477233346238, 1.5494215358698882, 1.5542353645732025, 1.550960887139338, 1.5500004198516908, 1.5579702137432059, 1.5591713433392973, 1.5485035173702044, 1.5450375983357674, 1.5452911639360432, 1.5444686190058807, 1.5475008845084501, 1.5472151981486921, 1.5456920633570614, 1.5400648022579217, 1.5445540311644943, 1.5474323368170424, 1.5509585393282912, 1.5698754587702193, 1.5899834231919088, 1.587622622934455, 1.5851219415175106, 1.5811359255710422, 1.573596022946634, 1.5666038946938956, 1.5607322828970407, 1.5600317534969572, 1.5590209759725928, 1.5594586909918815, 1.5574682929432613, 1.5602520042865917, 1.5591621226598595, 1.5611531326658181, 1.5654675618334228, 1.5626183291969848, 1.5569530381803884, 1.5509401360576403, 1.5617604371213816, 1.5652923611889629, 1.5696105069448325, 1.5674141244477071, 1.561827039375932, 1.5597961619159768, 1.5564965226077445, 1.5502978573100032, 1.5509148700036552, 1.5564927597555047, 1.549983734076028, 1.5466924290392678, 1.5452303576028812, 1.5400287380943063, 1.5468753174345107, 1.569480213639183, 1.585374550114422, 1.5856996676270723, 1.5820609303225728, 1.5815664534206508, 1.578980399450972, 1.5765858814701652, 1.5745688505975617, 1.5721181118023224, 1.5707412500401052, 1.5703097251896005, 1.5694711658744107, 1.5675278690561376, 1.5681565207867163, 1.5666486945240405, 1.5651530707396522, 1.5645125333778178, 1.561165321485218, 1.5595099579871803, 1.5543811502397917], 'acc': [0.23285421049203225, 0.2328542087112364, 0.23285420969036816, 0.2328542091028891, 0.2328542102778472, 0.23285420992291195, 0.23285420851541005, 0.2328542087112364, 0.23285420872959514, 0.23285420931707418, 0.23367556555070426, 0.2381930193311135, 0.23942505099200614, 0.23819301874363447, 0.23778234032390053, 0.23039014302117625, 0.23326488593765352, 0.2422997935535482, 0.23613962945996858, 0.23244353269649481, 0.23162217546537428, 0.24188911728790408, 0.24271047134908563, 0.2361396298516213, 0.23655030884047554, 0.2373716637033212, 0.24147844007984567, 0.2422997949059739, 0.24065708523535875, 0.23983572939338135, 0.23901437317810997, 0.24024640683398354, 0.2398357295892077, 0.24147843908235522, 0.24147843908235522, 0.24353182817019475, 0.2443531822313763, 0.24681724772561012, 0.24394250694486394, 0.24312115014211352, 0.24394250479077412, 0.23449692035847375, 0.23613963004744762, 0.2443531837979871, 0.244353183583802, 0.24435318319214933, 0.2435318283660211, 0.24271047193656467, 0.2472279271244758, 0.2459958928811232, 0.244353183583802, 0.24271047332570783, 0.2414784382623324, 0.24476386041856643, 0.2443531830146817, 0.24599589387861365, 0.24312115112124527, 0.2357289536226946, 0.2410677626576022, 0.24312115151289798, 0.23696098471446694, 0.24558521626054383, 0.24188911787538314, 0.24640657131921584, 0.2402464068156248, 0.24312114896715545, 0.24394250617991728, 0.24024640799058292, 0.24722792691029072, 0.24065708404204195, 0.24969199105209883, 0.24394250459494776, 0.2435318271727043, 0.24845995999704396, 0.241478439688193, 0.24435318319214933, 0.2595482554523852, 0.2513347030909889, 0.247638605899145, 0.2328542091028891, 0.24969199242288326, 0.2484599586079008, 0.24065708484370604, 0.24188911591711965, 0.25092402527709273, 0.24640656971588762, 0.246817249506406, 0.24188911591711965, 0.2509240239063083, 0.245585214479748, 0.24722792730194343, 0.24969199183540422, 0.2509240244937873, 0.2447638610060455, 0.23737166329330978, 0.24845995997868525, 0.25010266829687466, 0.24804928057981956, 0.24969199062372868, 0.2550308014577909, 0.250924024512146, 0.2525667361593834, 0.2501026712159112, 0.24599589386025494, 0.259958929900516, 0.2533880909855116, 0.2537987680161024, 0.2554414790758607, 0.25585215512731974, 0.25544147770507625, 0.2562628327453895, 0.2537987676244497, 0.2537987695827132, 0.26119096651214346, 0.2583162225981757, 0.2607802869358102, 0.2616016421719498, 0.26036961049269847, 0.27926077814562367, 0.2689938378897046, 0.26570842031091146, 0.2632443508450745, 0.26078028713163653, 0.25338809235629606, 0.2603696106885248, 0.26242299975800565, 0.2583162224023494, 0.25708418657402726, 0.2513347011327254, 0.2558521563022778, 0.2505133474815553, 0.25790554302184243, 0.2574948671662098, 0.2657084205067378, 0.26694045279182693, 0.26529774112623083, 0.2603696088893702, 0.2620123221399358, 0.26160164295525523, 0.26324435182420625, 0.26776180605134436, 0.2550308004786591, 0.2611909627547254, 0.2632443534275345, 0.26899384125547, 0.2640657102302849, 0.26488706350816105, 0.27227926122089674, 0.2702258725430686, 0.2706365519235756, 0.2731006182194735, 0.2689938388688364, 0.2669404535567736, 0.2587269009995509, 0.26488706252902927, 0.26694045218598916, 0.2809034908087102, 0.2636550284631443, 0.2665297747637457, 0.2648870652705982, 0.2644763858900912, 0.26694045296929453, 0.2648870652705982, 0.2603696090851966, 0.2620123193616495, 0.2611909649455327, 0.26652977417626667, 0.2665297759387038, 0.26776180800960786, 0.27638603916403204, 0.27022587113556673, 0.2702258746971585, 0.268583161874963, 0.26611909734150224, 0.26817248363269675, 0.268993840294697, 0.2800821349483741, 0.2694045196568452, 0.2874743330527625, 0.272689940209751, 0.27392197130152335, 0.2722792610250704, 0.26529774347614704, 0.259958933070455, 0.27063655172774925, 0.26570841952760604, 0.2726899388389666, 0.26899383749805195, 0.27597535857184957, 0.27843942588359666, 0.27926077814562367, 0.2759753573968915, 0.2772073916586028, 0.282135521276286, 0.2837782358609186, 0.28090348920538194, 0.2772073912669501, 0.26940451691527634, 0.27145790676806253, 0.27474332692931563, 0.27556468373206605, 0.269815194104976, 0.2731006152820783, 0.28377823472267794, 0.27926077971223445, 0.2772073906427536, 0.2767967124372048, 0.27638603521078764, 0.2706365485578102, 0.2780287476780478, 0.272689940209751, 0.271868581607846, 0.2718685822320425, 0.27802874826552687, 0.2718685830153479, 0.2788501035200252, 0.2726899384473139, 0.26324435323170814, 0.2751540035682537, 0.2665297759387038, 0.2706365481661575, 0.28336755491380083, 0.28049281475725113, 0.29240246509379675, 0.27761807100239233, 0.28706365288895014, 0.28911704352504175, 0.28131416604014636, 0.29445585099333854, 0.27351129309597444, 0.2862422992194213, 0.2800821349483741, 0.27926077814562367, 0.2837782323360443, 0.2841889109332459, 0.2870636526931238, 0.29117043004878, 0.2960985648313832, 0.2956878854141588, 0.2829568794865383, 0.2874743320736307, 0.29281313954192756, 0.29322382013411, 0.2907597556006492, 0.28911704156677825, 0.2915811066877181, 0.26817248466690463, 0.2480492791723177, 0.22833675493082717, 0.2509240241021346, 0.2669404515985101, 0.2661190975373286, 0.26735113019571166, 0.2636550304581253, 0.2767967161946228, 0.2726899392306193, 0.2747433263418366, 0.28213552542535675, 0.28008213612333216, 0.27063655231522826, 0.27720739440017167, 0.2636550330038678, 0.28254620323925295, 0.2841889125365741, 0.2833675578879135, 0.2870636530847765, 0.26694045175761905, 0.26283367659277007, 0.2726899406014037, 0.27967145873780613, 0.27638603759742125, 0.27597535997935146, 0.2887063643403611, 0.287474334264438, 0.2747433279084474, 0.2825462026517739, 0.2833675569087818, 0.28418890975828776, 0.2928131403252329, 0.2813141676434746, 0.25790554421515927, 0.24599589428862506, 0.2410677622475908, 0.25626283313704223, 0.2616016431510816, 0.262012320573325, 0.26242299936635294, 0.262833676005291, 0.2685831606632875, 0.2718685849736114, 0.27104722934581904, 0.2747433267334893, 0.2694045177169404, 0.2739219724764814, 0.2841889139073585, 0.2726899362565066, 0.27392196911071603, 0.26899383910138014, 0.2804928137781194, 0.28090348861790293]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
