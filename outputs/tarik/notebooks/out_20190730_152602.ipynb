{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf32.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 15:26:02 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': 'Front', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '02', '03', '04', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001DEB9798198>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001DED9536EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6132, Accuracy:0.1807, Validation Loss:1.6110, Validation Accuracy:0.1905\n",
    "Epoch #2: Loss:1.6105, Accuracy:0.1914, Validation Loss:1.6097, Validation Accuracy:0.2020\n",
    "Epoch #3: Loss:1.6078, Accuracy:0.2177, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #4: Loss:1.6061, Accuracy:0.2308, Validation Loss:1.6056, Validation Accuracy:0.2266\n",
    "Epoch #5: Loss:1.6047, Accuracy:0.2324, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6029, Accuracy:0.2345, Validation Loss:1.6040, Validation Accuracy:0.2348\n",
    "Epoch #12: Loss:1.6026, Accuracy:0.2378, Validation Loss:1.6040, Validation Accuracy:0.2365\n",
    "Epoch #13: Loss:1.6021, Accuracy:0.2431, Validation Loss:1.6043, Validation Accuracy:0.2299\n",
    "Epoch #14: Loss:1.6018, Accuracy:0.2444, Validation Loss:1.6049, Validation Accuracy:0.2299\n",
    "Epoch #15: Loss:1.6015, Accuracy:0.2448, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6011, Accuracy:0.2456, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #17: Loss:1.6008, Accuracy:0.2489, Validation Loss:1.6054, Validation Accuracy:0.2282\n",
    "Epoch #18: Loss:1.6012, Accuracy:0.2456, Validation Loss:1.6059, Validation Accuracy:0.2315\n",
    "Epoch #19: Loss:1.6014, Accuracy:0.2452, Validation Loss:1.6059, Validation Accuracy:0.2299\n",
    "Epoch #20: Loss:1.6027, Accuracy:0.2357, Validation Loss:1.6056, Validation Accuracy:0.2315\n",
    "Epoch #21: Loss:1.6018, Accuracy:0.2464, Validation Loss:1.6054, Validation Accuracy:0.2266\n",
    "Epoch #22: Loss:1.6013, Accuracy:0.2476, Validation Loss:1.6048, Validation Accuracy:0.2250\n",
    "Epoch #23: Loss:1.6011, Accuracy:0.2456, Validation Loss:1.6050, Validation Accuracy:0.2299\n",
    "Epoch #24: Loss:1.6031, Accuracy:0.2337, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6019, Accuracy:0.2407, Validation Loss:1.6059, Validation Accuracy:0.2282\n",
    "Epoch #27: Loss:1.6031, Accuracy:0.2452, Validation Loss:1.6044, Validation Accuracy:0.2348\n",
    "Epoch #28: Loss:1.6022, Accuracy:0.2324, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #29: Loss:1.6021, Accuracy:0.2324, Validation Loss:1.6041, Validation Accuracy:0.2365\n",
    "Epoch #30: Loss:1.6013, Accuracy:0.2431, Validation Loss:1.6050, Validation Accuracy:0.2266\n",
    "Epoch #31: Loss:1.6015, Accuracy:0.2452, Validation Loss:1.6056, Validation Accuracy:0.2233\n",
    "Epoch #32: Loss:1.6007, Accuracy:0.2431, Validation Loss:1.6059, Validation Accuracy:0.2250\n",
    "Epoch #33: Loss:1.6003, Accuracy:0.2480, Validation Loss:1.6072, Validation Accuracy:0.2250\n",
    "Epoch #34: Loss:1.5999, Accuracy:0.2452, Validation Loss:1.6071, Validation Accuracy:0.2233\n",
    "Epoch #35: Loss:1.6003, Accuracy:0.2435, Validation Loss:1.6066, Validation Accuracy:0.2233\n",
    "Epoch #36: Loss:1.5998, Accuracy:0.2435, Validation Loss:1.6070, Validation Accuracy:0.2250\n",
    "Epoch #37: Loss:1.5998, Accuracy:0.2448, Validation Loss:1.6072, Validation Accuracy:0.2250\n",
    "Epoch #38: Loss:1.5994, Accuracy:0.2448, Validation Loss:1.6076, Validation Accuracy:0.2217\n",
    "Epoch #39: Loss:1.5991, Accuracy:0.2439, Validation Loss:1.6073, Validation Accuracy:0.2233\n",
    "Epoch #40: Loss:1.5992, Accuracy:0.2480, Validation Loss:1.6075, Validation Accuracy:0.2282\n",
    "Epoch #41: Loss:1.5990, Accuracy:0.2444, Validation Loss:1.6080, Validation Accuracy:0.2233\n",
    "Epoch #42: Loss:1.5996, Accuracy:0.2423, Validation Loss:1.6083, Validation Accuracy:0.2233\n",
    "Epoch #43: Loss:1.5992, Accuracy:0.2431, Validation Loss:1.6086, Validation Accuracy:0.2250\n",
    "Epoch #44: Loss:1.5992, Accuracy:0.2448, Validation Loss:1.6077, Validation Accuracy:0.2266\n",
    "Epoch #45: Loss:1.5992, Accuracy:0.2452, Validation Loss:1.6075, Validation Accuracy:0.2282\n",
    "Epoch #46: Loss:1.5994, Accuracy:0.2464, Validation Loss:1.6076, Validation Accuracy:0.2233\n",
    "Epoch #47: Loss:1.5989, Accuracy:0.2456, Validation Loss:1.6071, Validation Accuracy:0.2217\n",
    "Epoch #48: Loss:1.5987, Accuracy:0.2444, Validation Loss:1.6075, Validation Accuracy:0.2250\n",
    "Epoch #49: Loss:1.5987, Accuracy:0.2464, Validation Loss:1.6076, Validation Accuracy:0.2250\n",
    "Epoch #50: Loss:1.5987, Accuracy:0.2448, Validation Loss:1.6077, Validation Accuracy:0.2250\n",
    "Epoch #51: Loss:1.5988, Accuracy:0.2480, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #52: Loss:1.5982, Accuracy:0.2452, Validation Loss:1.6075, Validation Accuracy:0.2233\n",
    "Epoch #53: Loss:1.5986, Accuracy:0.2435, Validation Loss:1.6080, Validation Accuracy:0.2233\n",
    "Epoch #54: Loss:1.5985, Accuracy:0.2505, Validation Loss:1.6080, Validation Accuracy:0.2315\n",
    "Epoch #55: Loss:1.5987, Accuracy:0.2452, Validation Loss:1.6069, Validation Accuracy:0.2282\n",
    "Epoch #56: Loss:1.5984, Accuracy:0.2431, Validation Loss:1.6061, Validation Accuracy:0.2348\n",
    "Epoch #57: Loss:1.5989, Accuracy:0.2435, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #58: Loss:1.5986, Accuracy:0.2431, Validation Loss:1.6066, Validation Accuracy:0.2266\n",
    "Epoch #59: Loss:1.5985, Accuracy:0.2448, Validation Loss:1.6063, Validation Accuracy:0.2282\n",
    "Epoch #60: Loss:1.5984, Accuracy:0.2456, Validation Loss:1.6064, Validation Accuracy:0.2282\n",
    "Epoch #61: Loss:1.5980, Accuracy:0.2444, Validation Loss:1.6078, Validation Accuracy:0.2282\n",
    "Epoch #62: Loss:1.5987, Accuracy:0.2444, Validation Loss:1.6079, Validation Accuracy:0.2365\n",
    "Epoch #63: Loss:1.5978, Accuracy:0.2505, Validation Loss:1.6078, Validation Accuracy:0.2348\n",
    "Epoch #64: Loss:1.5989, Accuracy:0.2464, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #65: Loss:1.5990, Accuracy:0.2419, Validation Loss:1.6053, Validation Accuracy:0.2381\n",
    "Epoch #66: Loss:1.6002, Accuracy:0.2468, Validation Loss:1.6046, Validation Accuracy:0.2381\n",
    "Epoch #67: Loss:1.5996, Accuracy:0.2472, Validation Loss:1.6036, Validation Accuracy:0.2381\n",
    "Epoch #68: Loss:1.5996, Accuracy:0.2394, Validation Loss:1.6040, Validation Accuracy:0.2282\n",
    "Epoch #69: Loss:1.5987, Accuracy:0.2464, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #70: Loss:1.5988, Accuracy:0.2468, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #71: Loss:1.5987, Accuracy:0.2468, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #72: Loss:1.5983, Accuracy:0.2468, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #73: Loss:1.5991, Accuracy:0.2435, Validation Loss:1.6057, Validation Accuracy:0.2365\n",
    "Epoch #74: Loss:1.5980, Accuracy:0.2460, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #75: Loss:1.5980, Accuracy:0.2472, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #76: Loss:1.5979, Accuracy:0.2476, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #77: Loss:1.5977, Accuracy:0.2472, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #78: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.6072, Validation Accuracy:0.2365\n",
    "Epoch #79: Loss:1.5974, Accuracy:0.2448, Validation Loss:1.6067, Validation Accuracy:0.2365\n",
    "Epoch #80: Loss:1.5971, Accuracy:0.2460, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #81: Loss:1.5971, Accuracy:0.2468, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #82: Loss:1.5969, Accuracy:0.2460, Validation Loss:1.6066, Validation Accuracy:0.2348\n",
    "Epoch #83: Loss:1.5972, Accuracy:0.2452, Validation Loss:1.6067, Validation Accuracy:0.2315\n",
    "Epoch #84: Loss:1.5970, Accuracy:0.2444, Validation Loss:1.6068, Validation Accuracy:0.2315\n",
    "Epoch #85: Loss:1.5973, Accuracy:0.2448, Validation Loss:1.6063, Validation Accuracy:0.2315\n",
    "Epoch #86: Loss:1.5990, Accuracy:0.2411, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #87: Loss:1.5989, Accuracy:0.2382, Validation Loss:1.6064, Validation Accuracy:0.2315\n",
    "Epoch #88: Loss:1.5996, Accuracy:0.2472, Validation Loss:1.6070, Validation Accuracy:0.2315\n",
    "Epoch #89: Loss:1.5982, Accuracy:0.2497, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:1.5975, Accuracy:0.2472, Validation Loss:1.6081, Validation Accuracy:0.2332\n",
    "Epoch #91: Loss:1.5979, Accuracy:0.2464, Validation Loss:1.6087, Validation Accuracy:0.2266\n",
    "Epoch #92: Loss:1.5976, Accuracy:0.2448, Validation Loss:1.6078, Validation Accuracy:0.2315\n",
    "Epoch #93: Loss:1.5982, Accuracy:0.2464, Validation Loss:1.6077, Validation Accuracy:0.2315\n",
    "Epoch #94: Loss:1.5979, Accuracy:0.2448, Validation Loss:1.6086, Validation Accuracy:0.2332\n",
    "Epoch #95: Loss:1.5989, Accuracy:0.2444, Validation Loss:1.6078, Validation Accuracy:0.2233\n",
    "Epoch #96: Loss:1.5976, Accuracy:0.2394, Validation Loss:1.6066, Validation Accuracy:0.2299\n",
    "Epoch #97: Loss:1.5976, Accuracy:0.2456, Validation Loss:1.6065, Validation Accuracy:0.2315\n",
    "Epoch #98: Loss:1.5975, Accuracy:0.2456, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #99: Loss:1.5977, Accuracy:0.2427, Validation Loss:1.6061, Validation Accuracy:0.2233\n",
    "Epoch #100: Loss:1.5968, Accuracy:0.2439, Validation Loss:1.6055, Validation Accuracy:0.2348\n",
    "Epoch #101: Loss:1.5979, Accuracy:0.2448, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #102: Loss:1.5970, Accuracy:0.2464, Validation Loss:1.6061, Validation Accuracy:0.2250\n",
    "Epoch #103: Loss:1.5972, Accuracy:0.2419, Validation Loss:1.6059, Validation Accuracy:0.2217\n",
    "Epoch #104: Loss:1.5969, Accuracy:0.2444, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #105: Loss:1.5974, Accuracy:0.2464, Validation Loss:1.6058, Validation Accuracy:0.2266\n",
    "Epoch #106: Loss:1.5963, Accuracy:0.2472, Validation Loss:1.6068, Validation Accuracy:0.2315\n",
    "Epoch #107: Loss:1.5962, Accuracy:0.2427, Validation Loss:1.6060, Validation Accuracy:0.2266\n",
    "Epoch #108: Loss:1.5967, Accuracy:0.2460, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #109: Loss:1.5973, Accuracy:0.2444, Validation Loss:1.6064, Validation Accuracy:0.2315\n",
    "Epoch #110: Loss:1.5962, Accuracy:0.2427, Validation Loss:1.6054, Validation Accuracy:0.2299\n",
    "Epoch #111: Loss:1.5965, Accuracy:0.2452, Validation Loss:1.6062, Validation Accuracy:0.2250\n",
    "Epoch #112: Loss:1.5963, Accuracy:0.2448, Validation Loss:1.6058, Validation Accuracy:0.2299\n",
    "Epoch #113: Loss:1.5962, Accuracy:0.2423, Validation Loss:1.6058, Validation Accuracy:0.2315\n",
    "Epoch #114: Loss:1.5970, Accuracy:0.2378, Validation Loss:1.6061, Validation Accuracy:0.2315\n",
    "Epoch #115: Loss:1.5964, Accuracy:0.2468, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #116: Loss:1.5963, Accuracy:0.2476, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #117: Loss:1.5963, Accuracy:0.2415, Validation Loss:1.6061, Validation Accuracy:0.2266\n",
    "Epoch #118: Loss:1.5964, Accuracy:0.2370, Validation Loss:1.6054, Validation Accuracy:0.2315\n",
    "Epoch #119: Loss:1.5959, Accuracy:0.2452, Validation Loss:1.6058, Validation Accuracy:0.2315\n",
    "Epoch #120: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #121: Loss:1.5958, Accuracy:0.2423, Validation Loss:1.6054, Validation Accuracy:0.2315\n",
    "Epoch #122: Loss:1.5962, Accuracy:0.2460, Validation Loss:1.6056, Validation Accuracy:0.2365\n",
    "Epoch #123: Loss:1.5959, Accuracy:0.2460, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #124: Loss:1.5961, Accuracy:0.2419, Validation Loss:1.6060, Validation Accuracy:0.2299\n",
    "Epoch #125: Loss:1.5963, Accuracy:0.2444, Validation Loss:1.6059, Validation Accuracy:0.2365\n",
    "Epoch #126: Loss:1.5955, Accuracy:0.2460, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #127: Loss:1.5960, Accuracy:0.2419, Validation Loss:1.6068, Validation Accuracy:0.2282\n",
    "Epoch #128: Loss:1.5954, Accuracy:0.2419, Validation Loss:1.6062, Validation Accuracy:0.2414\n",
    "Epoch #129: Loss:1.5959, Accuracy:0.2468, Validation Loss:1.6065, Validation Accuracy:0.2414\n",
    "Epoch #130: Loss:1.5962, Accuracy:0.2444, Validation Loss:1.6066, Validation Accuracy:0.2365\n",
    "Epoch #131: Loss:1.5954, Accuracy:0.2439, Validation Loss:1.6073, Validation Accuracy:0.2365\n",
    "Epoch #132: Loss:1.5954, Accuracy:0.2452, Validation Loss:1.6077, Validation Accuracy:0.2282\n",
    "Epoch #133: Loss:1.5951, Accuracy:0.2460, Validation Loss:1.6085, Validation Accuracy:0.2282\n",
    "Epoch #134: Loss:1.5953, Accuracy:0.2435, Validation Loss:1.6096, Validation Accuracy:0.2348\n",
    "Epoch #135: Loss:1.5957, Accuracy:0.2456, Validation Loss:1.6074, Validation Accuracy:0.2381\n",
    "Epoch #136: Loss:1.5954, Accuracy:0.2444, Validation Loss:1.6060, Validation Accuracy:0.2447\n",
    "Epoch #137: Loss:1.5955, Accuracy:0.2480, Validation Loss:1.6060, Validation Accuracy:0.2430\n",
    "Epoch #138: Loss:1.5955, Accuracy:0.2480, Validation Loss:1.6072, Validation Accuracy:0.2348\n",
    "Epoch #139: Loss:1.5953, Accuracy:0.2452, Validation Loss:1.6074, Validation Accuracy:0.2430\n",
    "Epoch #140: Loss:1.5950, Accuracy:0.2464, Validation Loss:1.6068, Validation Accuracy:0.2414\n",
    "Epoch #141: Loss:1.5953, Accuracy:0.2476, Validation Loss:1.6071, Validation Accuracy:0.2430\n",
    "Epoch #142: Loss:1.5951, Accuracy:0.2460, Validation Loss:1.6075, Validation Accuracy:0.2430\n",
    "Epoch #143: Loss:1.5959, Accuracy:0.2427, Validation Loss:1.6081, Validation Accuracy:0.2397\n",
    "Epoch #144: Loss:1.5962, Accuracy:0.2452, Validation Loss:1.6097, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5958, Accuracy:0.2468, Validation Loss:1.6094, Validation Accuracy:0.2348\n",
    "Epoch #146: Loss:1.5958, Accuracy:0.2427, Validation Loss:1.6099, Validation Accuracy:0.2348\n",
    "Epoch #147: Loss:1.5955, Accuracy:0.2419, Validation Loss:1.6109, Validation Accuracy:0.2414\n",
    "Epoch #148: Loss:1.5959, Accuracy:0.2448, Validation Loss:1.6103, Validation Accuracy:0.2414\n",
    "Epoch #149: Loss:1.5954, Accuracy:0.2427, Validation Loss:1.6113, Validation Accuracy:0.2348\n",
    "Epoch #150: Loss:1.5955, Accuracy:0.2456, Validation Loss:1.6114, Validation Accuracy:0.2381\n",
    "Epoch #151: Loss:1.5958, Accuracy:0.2439, Validation Loss:1.6112, Validation Accuracy:0.2397\n",
    "Epoch #152: Loss:1.5953, Accuracy:0.2411, Validation Loss:1.6107, Validation Accuracy:0.2397\n",
    "Epoch #153: Loss:1.5955, Accuracy:0.2394, Validation Loss:1.6116, Validation Accuracy:0.2332\n",
    "Epoch #154: Loss:1.5959, Accuracy:0.2407, Validation Loss:1.6111, Validation Accuracy:0.2332\n",
    "Epoch #155: Loss:1.5960, Accuracy:0.2407, Validation Loss:1.6099, Validation Accuracy:0.2414\n",
    "Epoch #156: Loss:1.5958, Accuracy:0.2427, Validation Loss:1.6102, Validation Accuracy:0.2414\n",
    "Epoch #157: Loss:1.5951, Accuracy:0.2415, Validation Loss:1.6096, Validation Accuracy:0.2397\n",
    "Epoch #158: Loss:1.5954, Accuracy:0.2398, Validation Loss:1.6086, Validation Accuracy:0.2414\n",
    "Epoch #159: Loss:1.5957, Accuracy:0.2439, Validation Loss:1.6071, Validation Accuracy:0.2414\n",
    "Epoch #160: Loss:1.5958, Accuracy:0.2415, Validation Loss:1.6085, Validation Accuracy:0.2414\n",
    "Epoch #161: Loss:1.5959, Accuracy:0.2411, Validation Loss:1.6083, Validation Accuracy:0.2381\n",
    "Epoch #162: Loss:1.5954, Accuracy:0.2435, Validation Loss:1.6083, Validation Accuracy:0.2414\n",
    "Epoch #163: Loss:1.5955, Accuracy:0.2427, Validation Loss:1.6087, Validation Accuracy:0.2397\n",
    "Epoch #164: Loss:1.5953, Accuracy:0.2435, Validation Loss:1.6070, Validation Accuracy:0.2529\n",
    "Epoch #165: Loss:1.5949, Accuracy:0.2431, Validation Loss:1.6067, Validation Accuracy:0.2529\n",
    "Epoch #166: Loss:1.5946, Accuracy:0.2439, Validation Loss:1.6077, Validation Accuracy:0.2496\n",
    "Epoch #167: Loss:1.5964, Accuracy:0.2452, Validation Loss:1.6067, Validation Accuracy:0.2365\n",
    "Epoch #168: Loss:1.5973, Accuracy:0.2448, Validation Loss:1.6074, Validation Accuracy:0.2397\n",
    "Epoch #169: Loss:1.5958, Accuracy:0.2370, Validation Loss:1.6075, Validation Accuracy:0.2381\n",
    "Epoch #170: Loss:1.5945, Accuracy:0.2460, Validation Loss:1.6082, Validation Accuracy:0.2332\n",
    "Epoch #171: Loss:1.5961, Accuracy:0.2394, Validation Loss:1.6080, Validation Accuracy:0.2332\n",
    "Epoch #172: Loss:1.5947, Accuracy:0.2444, Validation Loss:1.6084, Validation Accuracy:0.2447\n",
    "Epoch #173: Loss:1.5948, Accuracy:0.2460, Validation Loss:1.6085, Validation Accuracy:0.2447\n",
    "Epoch #174: Loss:1.5945, Accuracy:0.2452, Validation Loss:1.6083, Validation Accuracy:0.2414\n",
    "Epoch #175: Loss:1.5944, Accuracy:0.2456, Validation Loss:1.6091, Validation Accuracy:0.2365\n",
    "Epoch #176: Loss:1.5945, Accuracy:0.2419, Validation Loss:1.6105, Validation Accuracy:0.2365\n",
    "Epoch #177: Loss:1.5946, Accuracy:0.2431, Validation Loss:1.6115, Validation Accuracy:0.2447\n",
    "Epoch #178: Loss:1.5945, Accuracy:0.2444, Validation Loss:1.6117, Validation Accuracy:0.2397\n",
    "Epoch #179: Loss:1.5947, Accuracy:0.2394, Validation Loss:1.6111, Validation Accuracy:0.2381\n",
    "Epoch #180: Loss:1.5944, Accuracy:0.2419, Validation Loss:1.6119, Validation Accuracy:0.2397\n",
    "Epoch #181: Loss:1.5945, Accuracy:0.2439, Validation Loss:1.6121, Validation Accuracy:0.2348\n",
    "Epoch #182: Loss:1.5941, Accuracy:0.2390, Validation Loss:1.6121, Validation Accuracy:0.2332\n",
    "Epoch #183: Loss:1.5941, Accuracy:0.2394, Validation Loss:1.6127, Validation Accuracy:0.2414\n",
    "Epoch #184: Loss:1.5940, Accuracy:0.2415, Validation Loss:1.6126, Validation Accuracy:0.2397\n",
    "Epoch #185: Loss:1.5942, Accuracy:0.2353, Validation Loss:1.6126, Validation Accuracy:0.2250\n",
    "Epoch #186: Loss:1.5935, Accuracy:0.2370, Validation Loss:1.6125, Validation Accuracy:0.2381\n",
    "Epoch #187: Loss:1.5936, Accuracy:0.2415, Validation Loss:1.6125, Validation Accuracy:0.2365\n",
    "Epoch #188: Loss:1.5933, Accuracy:0.2390, Validation Loss:1.6122, Validation Accuracy:0.2332\n",
    "Epoch #189: Loss:1.5936, Accuracy:0.2329, Validation Loss:1.6129, Validation Accuracy:0.2365\n",
    "Epoch #190: Loss:1.5931, Accuracy:0.2370, Validation Loss:1.6127, Validation Accuracy:0.2332\n",
    "Epoch #191: Loss:1.5930, Accuracy:0.2390, Validation Loss:1.6134, Validation Accuracy:0.2381\n",
    "Epoch #192: Loss:1.5934, Accuracy:0.2394, Validation Loss:1.6134, Validation Accuracy:0.2299\n",
    "Epoch #193: Loss:1.5936, Accuracy:0.2394, Validation Loss:1.6130, Validation Accuracy:0.2365\n",
    "Epoch #194: Loss:1.5934, Accuracy:0.2394, Validation Loss:1.6134, Validation Accuracy:0.2365\n",
    "Epoch #195: Loss:1.5929, Accuracy:0.2415, Validation Loss:1.6131, Validation Accuracy:0.2365\n",
    "Epoch #196: Loss:1.5928, Accuracy:0.2374, Validation Loss:1.6129, Validation Accuracy:0.2332\n",
    "Epoch #197: Loss:1.5940, Accuracy:0.2378, Validation Loss:1.6128, Validation Accuracy:0.2299\n",
    "Epoch #198: Loss:1.5933, Accuracy:0.2382, Validation Loss:1.6133, Validation Accuracy:0.2397\n",
    "Epoch #199: Loss:1.5931, Accuracy:0.2390, Validation Loss:1.6137, Validation Accuracy:0.2414\n",
    "Epoch #200: Loss:1.5927, Accuracy:0.2378, Validation Loss:1.6138, Validation Accuracy:0.2397\n",
    "Epoch #201: Loss:1.5930, Accuracy:0.2423, Validation Loss:1.6139, Validation Accuracy:0.2365\n",
    "Epoch #202: Loss:1.5929, Accuracy:0.2361, Validation Loss:1.6133, Validation Accuracy:0.2299\n",
    "Epoch #203: Loss:1.5926, Accuracy:0.2402, Validation Loss:1.6133, Validation Accuracy:0.2365\n",
    "Epoch #204: Loss:1.5927, Accuracy:0.2374, Validation Loss:1.6141, Validation Accuracy:0.2365\n",
    "Epoch #205: Loss:1.5925, Accuracy:0.2390, Validation Loss:1.6131, Validation Accuracy:0.2381\n",
    "Epoch #206: Loss:1.5922, Accuracy:0.2489, Validation Loss:1.6130, Validation Accuracy:0.2381\n",
    "Epoch #207: Loss:1.5922, Accuracy:0.2476, Validation Loss:1.6140, Validation Accuracy:0.2397\n",
    "Epoch #208: Loss:1.5922, Accuracy:0.2361, Validation Loss:1.6138, Validation Accuracy:0.2430\n",
    "Epoch #209: Loss:1.5921, Accuracy:0.2345, Validation Loss:1.6134, Validation Accuracy:0.2430\n",
    "Epoch #210: Loss:1.5918, Accuracy:0.2370, Validation Loss:1.6134, Validation Accuracy:0.2365\n",
    "Epoch #211: Loss:1.5920, Accuracy:0.2468, Validation Loss:1.6133, Validation Accuracy:0.2430\n",
    "Epoch #212: Loss:1.5915, Accuracy:0.2423, Validation Loss:1.6137, Validation Accuracy:0.2348\n",
    "Epoch #213: Loss:1.5916, Accuracy:0.2394, Validation Loss:1.6151, Validation Accuracy:0.2381\n",
    "Epoch #214: Loss:1.5915, Accuracy:0.2419, Validation Loss:1.6154, Validation Accuracy:0.2381\n",
    "Epoch #215: Loss:1.5914, Accuracy:0.2402, Validation Loss:1.6155, Validation Accuracy:0.2397\n",
    "Epoch #216: Loss:1.5914, Accuracy:0.2390, Validation Loss:1.6160, Validation Accuracy:0.2463\n",
    "Epoch #217: Loss:1.5912, Accuracy:0.2407, Validation Loss:1.6156, Validation Accuracy:0.2381\n",
    "Epoch #218: Loss:1.5914, Accuracy:0.2448, Validation Loss:1.6160, Validation Accuracy:0.2430\n",
    "Epoch #219: Loss:1.5908, Accuracy:0.2423, Validation Loss:1.6158, Validation Accuracy:0.2430\n",
    "Epoch #220: Loss:1.5907, Accuracy:0.2419, Validation Loss:1.6155, Validation Accuracy:0.2430\n",
    "Epoch #221: Loss:1.5907, Accuracy:0.2427, Validation Loss:1.6150, Validation Accuracy:0.2529\n",
    "Epoch #222: Loss:1.5904, Accuracy:0.2460, Validation Loss:1.6153, Validation Accuracy:0.2479\n",
    "Epoch #223: Loss:1.5903, Accuracy:0.2448, Validation Loss:1.6162, Validation Accuracy:0.2479\n",
    "Epoch #224: Loss:1.5911, Accuracy:0.2423, Validation Loss:1.6172, Validation Accuracy:0.2562\n",
    "Epoch #225: Loss:1.5905, Accuracy:0.2509, Validation Loss:1.6158, Validation Accuracy:0.2562\n",
    "Epoch #226: Loss:1.5908, Accuracy:0.2480, Validation Loss:1.6148, Validation Accuracy:0.2562\n",
    "Epoch #227: Loss:1.5897, Accuracy:0.2497, Validation Loss:1.6149, Validation Accuracy:0.2562\n",
    "Epoch #228: Loss:1.5903, Accuracy:0.2485, Validation Loss:1.6166, Validation Accuracy:0.2545\n",
    "Epoch #229: Loss:1.5897, Accuracy:0.2509, Validation Loss:1.6159, Validation Accuracy:0.2644\n",
    "Epoch #230: Loss:1.5902, Accuracy:0.2534, Validation Loss:1.6152, Validation Accuracy:0.2529\n",
    "Epoch #231: Loss:1.5894, Accuracy:0.2517, Validation Loss:1.6154, Validation Accuracy:0.2545\n",
    "Epoch #232: Loss:1.5891, Accuracy:0.2517, Validation Loss:1.6167, Validation Accuracy:0.2529\n",
    "Epoch #233: Loss:1.5893, Accuracy:0.2505, Validation Loss:1.6167, Validation Accuracy:0.2463\n",
    "Epoch #234: Loss:1.5892, Accuracy:0.2501, Validation Loss:1.6158, Validation Accuracy:0.2545\n",
    "Epoch #235: Loss:1.5891, Accuracy:0.2439, Validation Loss:1.6155, Validation Accuracy:0.2529\n",
    "Epoch #236: Loss:1.5889, Accuracy:0.2526, Validation Loss:1.6155, Validation Accuracy:0.2529\n",
    "Epoch #237: Loss:1.5891, Accuracy:0.2480, Validation Loss:1.6163, Validation Accuracy:0.2463\n",
    "Epoch #238: Loss:1.5887, Accuracy:0.2493, Validation Loss:1.6165, Validation Accuracy:0.2496\n",
    "Epoch #239: Loss:1.5892, Accuracy:0.2493, Validation Loss:1.6180, Validation Accuracy:0.2496\n",
    "Epoch #240: Loss:1.5884, Accuracy:0.2517, Validation Loss:1.6164, Validation Accuracy:0.2529\n",
    "Epoch #241: Loss:1.5886, Accuracy:0.2476, Validation Loss:1.6160, Validation Accuracy:0.2529\n",
    "Epoch #242: Loss:1.5890, Accuracy:0.2534, Validation Loss:1.6165, Validation Accuracy:0.2447\n",
    "Epoch #243: Loss:1.5882, Accuracy:0.2542, Validation Loss:1.6159, Validation Accuracy:0.2496\n",
    "Epoch #244: Loss:1.5883, Accuracy:0.2550, Validation Loss:1.6162, Validation Accuracy:0.2496\n",
    "Epoch #245: Loss:1.5878, Accuracy:0.2513, Validation Loss:1.6167, Validation Accuracy:0.2496\n",
    "Epoch #246: Loss:1.5884, Accuracy:0.2526, Validation Loss:1.6162, Validation Accuracy:0.2512\n",
    "Epoch #247: Loss:1.5880, Accuracy:0.2526, Validation Loss:1.6160, Validation Accuracy:0.2496\n",
    "Epoch #248: Loss:1.5879, Accuracy:0.2456, Validation Loss:1.6164, Validation Accuracy:0.2414\n",
    "Epoch #249: Loss:1.5877, Accuracy:0.2522, Validation Loss:1.6172, Validation Accuracy:0.2463\n",
    "Epoch #250: Loss:1.5878, Accuracy:0.2522, Validation Loss:1.6179, Validation Accuracy:0.2414\n",
    "Epoch #251: Loss:1.5874, Accuracy:0.2476, Validation Loss:1.6171, Validation Accuracy:0.2496\n",
    "Epoch #252: Loss:1.5872, Accuracy:0.2513, Validation Loss:1.6175, Validation Accuracy:0.2496\n",
    "Epoch #253: Loss:1.5874, Accuracy:0.2476, Validation Loss:1.6183, Validation Accuracy:0.2496\n",
    "Epoch #254: Loss:1.5875, Accuracy:0.2517, Validation Loss:1.6188, Validation Accuracy:0.2479\n",
    "Epoch #255: Loss:1.5872, Accuracy:0.2456, Validation Loss:1.6164, Validation Accuracy:0.2414\n",
    "Epoch #256: Loss:1.5870, Accuracy:0.2505, Validation Loss:1.6167, Validation Accuracy:0.2348\n",
    "Epoch #257: Loss:1.5874, Accuracy:0.2501, Validation Loss:1.6174, Validation Accuracy:0.2397\n",
    "Epoch #258: Loss:1.5868, Accuracy:0.2505, Validation Loss:1.6180, Validation Accuracy:0.2381\n",
    "Epoch #259: Loss:1.5868, Accuracy:0.2464, Validation Loss:1.6177, Validation Accuracy:0.2381\n",
    "Epoch #260: Loss:1.5873, Accuracy:0.2489, Validation Loss:1.6167, Validation Accuracy:0.2397\n",
    "Epoch #261: Loss:1.5868, Accuracy:0.2505, Validation Loss:1.6165, Validation Accuracy:0.2348\n",
    "Epoch #262: Loss:1.5867, Accuracy:0.2472, Validation Loss:1.6173, Validation Accuracy:0.2381\n",
    "Epoch #263: Loss:1.5862, Accuracy:0.2509, Validation Loss:1.6178, Validation Accuracy:0.2365\n",
    "Epoch #264: Loss:1.5865, Accuracy:0.2480, Validation Loss:1.6182, Validation Accuracy:0.2381\n",
    "Epoch #265: Loss:1.5873, Accuracy:0.2456, Validation Loss:1.6177, Validation Accuracy:0.2381\n",
    "Epoch #266: Loss:1.5865, Accuracy:0.2476, Validation Loss:1.6178, Validation Accuracy:0.2348\n",
    "Epoch #267: Loss:1.5865, Accuracy:0.2522, Validation Loss:1.6183, Validation Accuracy:0.2282\n",
    "Epoch #268: Loss:1.5862, Accuracy:0.2517, Validation Loss:1.6177, Validation Accuracy:0.2282\n",
    "Epoch #269: Loss:1.5863, Accuracy:0.2509, Validation Loss:1.6191, Validation Accuracy:0.2365\n",
    "Epoch #270: Loss:1.5859, Accuracy:0.2485, Validation Loss:1.6177, Validation Accuracy:0.2381\n",
    "Epoch #271: Loss:1.5864, Accuracy:0.2485, Validation Loss:1.6184, Validation Accuracy:0.2348\n",
    "Epoch #272: Loss:1.5853, Accuracy:0.2497, Validation Loss:1.6188, Validation Accuracy:0.2266\n",
    "Epoch #273: Loss:1.5861, Accuracy:0.2501, Validation Loss:1.6191, Validation Accuracy:0.2299\n",
    "Epoch #274: Loss:1.5858, Accuracy:0.2480, Validation Loss:1.6190, Validation Accuracy:0.2266\n",
    "Epoch #275: Loss:1.5858, Accuracy:0.2554, Validation Loss:1.6204, Validation Accuracy:0.2266\n",
    "Epoch #276: Loss:1.5852, Accuracy:0.2530, Validation Loss:1.6188, Validation Accuracy:0.2282\n",
    "Epoch #277: Loss:1.5854, Accuracy:0.2489, Validation Loss:1.6198, Validation Accuracy:0.2365\n",
    "Epoch #278: Loss:1.5857, Accuracy:0.2497, Validation Loss:1.6191, Validation Accuracy:0.2250\n",
    "Epoch #279: Loss:1.5851, Accuracy:0.2538, Validation Loss:1.6192, Validation Accuracy:0.2233\n",
    "Epoch #280: Loss:1.5851, Accuracy:0.2550, Validation Loss:1.6205, Validation Accuracy:0.2282\n",
    "Epoch #281: Loss:1.5851, Accuracy:0.2567, Validation Loss:1.6192, Validation Accuracy:0.2299\n",
    "Epoch #282: Loss:1.5846, Accuracy:0.2563, Validation Loss:1.6190, Validation Accuracy:0.2299\n",
    "Epoch #283: Loss:1.5859, Accuracy:0.2567, Validation Loss:1.6184, Validation Accuracy:0.2282\n",
    "Epoch #284: Loss:1.5852, Accuracy:0.2559, Validation Loss:1.6207, Validation Accuracy:0.2282\n",
    "Epoch #285: Loss:1.5849, Accuracy:0.2567, Validation Loss:1.6194, Validation Accuracy:0.2315\n",
    "Epoch #286: Loss:1.5847, Accuracy:0.2591, Validation Loss:1.6212, Validation Accuracy:0.2315\n",
    "Epoch #287: Loss:1.5851, Accuracy:0.2554, Validation Loss:1.6197, Validation Accuracy:0.2315\n",
    "Epoch #288: Loss:1.5842, Accuracy:0.2534, Validation Loss:1.6206, Validation Accuracy:0.2250\n",
    "Epoch #289: Loss:1.5847, Accuracy:0.2538, Validation Loss:1.6202, Validation Accuracy:0.2282\n",
    "Epoch #290: Loss:1.5846, Accuracy:0.2546, Validation Loss:1.6209, Validation Accuracy:0.2282\n",
    "Epoch #291: Loss:1.5848, Accuracy:0.2505, Validation Loss:1.6214, Validation Accuracy:0.2282\n",
    "Epoch #292: Loss:1.5836, Accuracy:0.2567, Validation Loss:1.6193, Validation Accuracy:0.2282\n",
    "Epoch #293: Loss:1.5843, Accuracy:0.2526, Validation Loss:1.6200, Validation Accuracy:0.2299\n",
    "Epoch #294: Loss:1.5845, Accuracy:0.2554, Validation Loss:1.6217, Validation Accuracy:0.2250\n",
    "Epoch #295: Loss:1.5842, Accuracy:0.2542, Validation Loss:1.6198, Validation Accuracy:0.2282\n",
    "Epoch #296: Loss:1.5841, Accuracy:0.2571, Validation Loss:1.6204, Validation Accuracy:0.2250\n",
    "Epoch #297: Loss:1.5844, Accuracy:0.2542, Validation Loss:1.6229, Validation Accuracy:0.2282\n",
    "Epoch #298: Loss:1.5839, Accuracy:0.2538, Validation Loss:1.6199, Validation Accuracy:0.2250\n",
    "Epoch #299: Loss:1.5841, Accuracy:0.2550, Validation Loss:1.6205, Validation Accuracy:0.2282\n",
    "Epoch #300: Loss:1.5837, Accuracy:0.2485, Validation Loss:1.6212, Validation Accuracy:0.2184\n",
    "\n",
    "Test:\n",
    "Test Loss:1.62120998, Accuracy:0.2184\n",
    "Labels: ['01', '02', '03', '04', '05']\n",
    "Confusion Matrix:\n",
    "      01  02  03  04  05\n",
    "t:01  51   0  14  12  49\n",
    "t:02  39   0   7  23  45\n",
    "t:03  41   0  10  12  52\n",
    "t:04  43   0  11  20  38\n",
    "t:05  61   0  12  17  52\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.22      0.40      0.28       126\n",
    "          02       0.00      0.00      0.00       114\n",
    "          03       0.19      0.09      0.12       115\n",
    "          04       0.24      0.18      0.20       112\n",
    "          05       0.22      0.37      0.28       142\n",
    "\n",
    "    accuracy                           0.22       609\n",
    "   macro avg       0.17      0.21      0.18       609\n",
    "weighted avg       0.18      0.22      0.18       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 15:41:42 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 40 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.610991436663911, 1.609721771210481, 1.6075550114188484, 1.605648720988695, 1.6054139327141648, 1.6054163170957017, 1.605244731472435, 1.6049104167518553, 1.6046245250795863, 1.6041513740135531, 1.604008767209421, 1.6040367330432135, 1.6043282461479575, 1.604860222006862, 1.6050905842694938, 1.605168367841561, 1.6054253319801368, 1.6059179439137525, 1.60591286548057, 1.6055791004146458, 1.6053748144500557, 1.604809054991686, 1.6050227436134576, 1.6063418607602173, 1.6050814858015339, 1.6058554113009098, 1.6043872003289084, 1.603893202318151, 1.6040768406074035, 1.6049867547400087, 1.6055748807189891, 1.6059373045594039, 1.6072105834832529, 1.607131015099524, 1.6065578290394373, 1.6069502818760613, 1.6072367664628429, 1.6076485727025174, 1.6073095500958572, 1.6075154781733045, 1.6079670061618823, 1.608318855805546, 1.6085866464574152, 1.607743555101855, 1.6074961449320877, 1.6075992742782743, 1.6071138671662029, 1.607536972449918, 1.6075515381025367, 1.60769295007333, 1.6074112935606482, 1.607520048645721, 1.607981786547819, 1.607998368971062, 1.6068864718250844, 1.606060782872593, 1.6057031943488786, 1.6065684061723782, 1.6062728348623942, 1.6064404495831193, 1.6078264867926662, 1.6078556676216313, 1.6077884400419413, 1.6072196557212541, 1.6053382947135637, 1.6046156384087549, 1.6036326626838722, 1.604026934196209, 1.605549957168905, 1.6061656923325387, 1.6073581254345246, 1.6056168504144954, 1.605668734838614, 1.605276502607687, 1.605700170073799, 1.6060028115321068, 1.6062973115244523, 1.6071694466868058, 1.606722818611095, 1.6065680436508605, 1.6065130609596892, 1.606551502138523, 1.606695489343164, 1.6067979605914338, 1.6063249465475724, 1.6058647505364003, 1.6063975866987983, 1.6070479661568828, 1.606657458643608, 1.6080938424970128, 1.6087216365905035, 1.6077686199805223, 1.6077404014386958, 1.608605643407073, 1.6078354486299462, 1.606568655161239, 1.6064881137243436, 1.6063403110394532, 1.6060649391465587, 1.605487536522751, 1.6052291741707838, 1.6060846108325402, 1.6058527734283548, 1.6055470556265419, 1.6057739753049778, 1.6067853878284324, 1.6060106243406023, 1.606315596546054, 1.606394658143493, 1.6053988980542262, 1.6061844436210169, 1.6057672567163979, 1.6058192689626283, 1.6061371981804007, 1.606032254660658, 1.6056185802215426, 1.6060562717112024, 1.6054444483348302, 1.6057766304031773, 1.6057001826015405, 1.6054255809689977, 1.6056144227731013, 1.6054862338333882, 1.6060051293600173, 1.60588459721927, 1.6061719082454937, 1.6068367637045473, 1.6061559704136965, 1.6064613258897378, 1.6066424977798963, 1.607256312284172, 1.6077459686495401, 1.6084804497720377, 1.609620276147313, 1.607438628505212, 1.6060085740974188, 1.6059944214687754, 1.6071751785200021, 1.6074128287962113, 1.6068484851683693, 1.6071253134112053, 1.607463591987472, 1.608148304503931, 1.6096932594412066, 1.6094399684755674, 1.6098533643681818, 1.610911200786459, 1.6103047289088834, 1.6112657339114862, 1.6113815714768784, 1.6111664443180478, 1.6107134956052933, 1.6115764033030995, 1.611062062980702, 1.6099156145196047, 1.610243617607455, 1.6096056429623382, 1.6086253888892814, 1.607100316456386, 1.6084736763745888, 1.6083045070394506, 1.6082809280683645, 1.608736169553547, 1.6070432150109453, 1.6066763929545587, 1.6077318902086155, 1.6067417501816021, 1.6073565295177141, 1.607535583436587, 1.6081783211681446, 1.6080283345455801, 1.6084079147363923, 1.608540722889266, 1.6083099327259658, 1.6091080381365246, 1.6104542490688256, 1.6115471324309927, 1.611703828442077, 1.611118314497185, 1.6118613530458097, 1.6120523584300075, 1.612057597374877, 1.6126806138967253, 1.6126072038766395, 1.612581407691066, 1.6124830414313205, 1.6125271733366993, 1.6122383983264417, 1.6128572932213594, 1.612662223563797, 1.6134369782430589, 1.6133888288476002, 1.6129761155211475, 1.6133946448515593, 1.6130797567430193, 1.612935853317649, 1.6128159894536085, 1.6132604813536595, 1.6136864206473815, 1.6137726118999163, 1.6139388356498505, 1.6133358895485037, 1.613292157160629, 1.6140870351118015, 1.6131375611122019, 1.613006698478423, 1.6139665395755487, 1.6137500560929623, 1.6134453389444963, 1.6134383721500392, 1.6132948065821957, 1.6137115857479802, 1.6151044578192073, 1.6153954580695367, 1.6154776451427166, 1.6160113443490516, 1.615638772842332, 1.616006017905738, 1.6158335189318227, 1.615467515289294, 1.6149659638334377, 1.6153330844024132, 1.6161539057401209, 1.6171806081762454, 1.615787767620118, 1.614793119955141, 1.6149145261015994, 1.6166030023681315, 1.6159494122848135, 1.6151681835036755, 1.615390113030357, 1.616736156991354, 1.6166977870640495, 1.6157674349000302, 1.6154679381005674, 1.615450104077657, 1.6163062160629749, 1.616509477297465, 1.6179654637385277, 1.6164104805399828, 1.6160321092762187, 1.6165194251071449, 1.6158841313986942, 1.6161840044218918, 1.6167104956747471, 1.6161633659466146, 1.6160269120252386, 1.6164190497108673, 1.617231601956247, 1.6178702891166574, 1.6170601758659375, 1.6175163610423924, 1.6182806985131626, 1.6187564449748775, 1.6164449142117805, 1.616729844184149, 1.6174061924757432, 1.6180427853501294, 1.6177298075264115, 1.6166869674018647, 1.616547250199592, 1.6173212657421094, 1.6178315253484816, 1.6182072540417876, 1.617673709670507, 1.6177506033814404, 1.6182761368493142, 1.6177111926728673, 1.6190944739750452, 1.6176913225004825, 1.6183556147984095, 1.6188205936663649, 1.6190932217881402, 1.6190379779718584, 1.6203673686495752, 1.6188103218971215, 1.6198106761440658, 1.6190810882790727, 1.6191558557973902, 1.6205025149879393, 1.6191809220462794, 1.619043279555435, 1.6183630002934748, 1.6206817368568458, 1.6194065819037176, 1.6212015355553338, 1.6196695137493715, 1.620606940955364, 1.6201940297297461, 1.620941478826338, 1.621352045211103, 1.6192504371132561, 1.6200315410084716, 1.621689215082253, 1.6197743723153677, 1.6204329543121538, 1.6229211890638755, 1.619875108471449, 1.6205141665704537, 1.62120995928697], 'val_acc': [0.19047619025597626, 0.20197044104973866, 0.23645320184810212, 0.22660098470784173, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2348111652339425, 0.23645320135873724, 0.22988505685955823, 0.22988505685955823, 0.23316912910914772, 0.2348111652339425, 0.2282430207347635, 0.23152709298435298, 0.22988505685955823, 0.23152709298435298, 0.22660098460996875, 0.224958948485174, 0.22988505685955823, 0.2331691292070207, 0.2331691292070207, 0.22824302083263648, 0.23481116533181545, 0.2331691292070207, 0.23645320135873724, 0.22660098470784173, 0.22331691245825225, 0.224958948583047, 0.224958948583047, 0.22331691245825225, 0.22331691245825225, 0.224958948583047, 0.224958948583047, 0.2216748763334575, 0.22331691245825225, 0.22824302083263648, 0.2233169125561252, 0.2233169126539982, 0.22495894877879294, 0.22660098470784173, 0.22824302083263648, 0.22331691275187118, 0.22167487652920345, 0.22495894877879294, 0.22495894887666593, 0.22495894887666593, 0.2331691292070207, 0.22331691275187118, 0.2233169126539982, 0.23152709308222597, 0.22824302102838243, 0.23481116533181545, 0.2298850570553042, 0.22660098500146067, 0.22824302093050947, 0.22824302093050947, 0.2282430212241284, 0.23645320175022916, 0.23481116542968844, 0.23316912940276668, 0.23809523767927793, 0.23809523767927793, 0.23809523767927793, 0.2282430211262554, 0.23645320155448318, 0.23481116542968844, 0.2331691293048937, 0.2331691293048937, 0.23645320165235617, 0.2331691293048937, 0.2331691293048937, 0.2331691292070207, 0.2331691293048937, 0.23645320165235617, 0.23645320165235617, 0.2331691293048937, 0.2331691293048937, 0.23481116542968844, 0.23152709318009895, 0.23152709318009895, 0.23152709308222597, 0.2331691292070207, 0.23152709308222597, 0.23152709318009895, 0.2331691292070207, 0.2331691293048937, 0.22660098480571472, 0.23152709318009895, 0.23152709308222597, 0.2331691293048937, 0.2233169126539982, 0.2298850570553042, 0.23152709308222597, 0.22660098480571472, 0.2233169126539982, 0.23481116542968844, 0.22824302083263648, 0.22495894868091998, 0.2216748763334575, 0.22824302083263648, 0.22660098460996875, 0.23152709327797194, 0.22660098470784173, 0.22660098470784173, 0.23152709308222597, 0.22988505685955823, 0.224958948583047, 0.2298850571531772, 0.23152709308222597, 0.23152709327797194, 0.23316912910914772, 0.23316912910914772, 0.22660098480571472, 0.23152709308222597, 0.23152709308222597, 0.2298850571531772, 0.23152709308222597, 0.2364532014566102, 0.23645320165235617, 0.2298850571531772, 0.2364532014566102, 0.23481116542968844, 0.22824302093050947, 0.24137930983099445, 0.24137930983099445, 0.23645320165235617, 0.2364532014566102, 0.22824302083263648, 0.22824302093050947, 0.23481116542968844, 0.23809523758140494, 0.24466338198271095, 0.2430213459557892, 0.23481116552756143, 0.2430213459557892, 0.24137930973312147, 0.2430213459557892, 0.2430213459557892, 0.23973727370619968, 0.23973727360832672, 0.23481116552756143, 0.23481116552756143, 0.24137930983099445, 0.24137930983099445, 0.23481116552756143, 0.23809523767927793, 0.23973727380407267, 0.23973727370619968, 0.2331691293048937, 0.2331691293048937, 0.24137930983099445, 0.2413793099288674, 0.23973727370619968, 0.2413793099288674, 0.24137930983099445, 0.2413793099288674, 0.23809523758140494, 0.2413793099288674, 0.23973727370619968, 0.2528735628024306, 0.2528735628024306, 0.24958949055284116, 0.2364532014566102, 0.23973727380407267, 0.23809523758140494, 0.2331691292070207, 0.2331691292070207, 0.24466338208058394, 0.24466338208058394, 0.24137930983099445, 0.2364532014566102, 0.2364532014566102, 0.24466338208058394, 0.23973727370619968, 0.23809523758140494, 0.23973727370619968, 0.23481116533181545, 0.2331691292070207, 0.24137930983099445, 0.23973727370619968, 0.224958948485174, 0.23809523758140494, 0.2364532014566102, 0.2331691292070207, 0.23645320175022916, 0.2331691292070207, 0.23809523758140494, 0.22988505685955823, 0.23645320155448318, 0.2364532014566102, 0.2364532014566102, 0.2331691292070207, 0.22988505695743122, 0.23973727370619968, 0.24137930983099445, 0.23973727370619968, 0.2364532014566102, 0.22988505695743122, 0.2364532014566102, 0.2364532014566102, 0.23809523758140494, 0.23809523758140494, 0.23973727370619968, 0.24302134576004322, 0.24302134576004322, 0.23645320116299126, 0.24302134624940813, 0.2348111652339425, 0.238095237287786, 0.23809523748353198, 0.2397372732168348, 0.24630541791175975, 0.23809523748353198, 0.2430213459557892, 0.24302134556429728, 0.24302134556429728, 0.2528735624109387, 0.2479474540365545, 0.2479474540365545, 0.2561576350520201, 0.2561576352477661, 0.2561576352477661, 0.2561576350520201, 0.25451559882935243, 0.2643678158717398, 0.2528735629981766, 0.25451559892722536, 0.2528735628024306, 0.24630541830325164, 0.25451559892722536, 0.2528735628024306, 0.2528735628024306, 0.24630541830325164, 0.24958949055284116, 0.24958949045496817, 0.2528735628024306, 0.2528735628024306, 0.2446633821784569, 0.2495894907485871, 0.24958949055284116, 0.24958949055284116, 0.2512315265797629, 0.2495894907485871, 0.2413793100267404, 0.24630541830325164, 0.2413793100267404, 0.24958949055284116, 0.24958949055284116, 0.24958949055284116, 0.24794745433017343, 0.24137931022248635, 0.23481116552756143, 0.23973727380407267, 0.23809523777715091, 0.23809523767927793, 0.23973727380407267, 0.23481116552756143, 0.23809523797289686, 0.23645320155448318, 0.23809523767927793, 0.23809523797289686, 0.23481116542968844, 0.2282430207347635, 0.22824302102838243, 0.23645320155448318, 0.23809523767927793, 0.23481116542968844, 0.22660098460996875, 0.2298850571531772, 0.22660098460996875, 0.22660098460996875, 0.22824302093050947, 0.23645320175022916, 0.224958948485174, 0.22331691226250627, 0.22824302063689053, 0.22988505685955823, 0.22988505685955823, 0.22824302063689053, 0.22824302063689053, 0.23152709308222597, 0.23152709288648002, 0.23152709288648002, 0.224958948583047, 0.22824302083263648, 0.22824302063689053, 0.22824302063689053, 0.22824302063689053, 0.22988505676168527, 0.22495894838730102, 0.22824302083263648, 0.22495894838730102, 0.22824302063689053, 0.22495894838730102, 0.22824302083263648, 0.21839080448759404], 'loss': [1.613224399408031, 1.6105356160620155, 1.6077920747243892, 1.6061410154894882, 1.6047368810406946, 1.6047355526526605, 1.6040903021422745, 1.6037494565427182, 1.6034181028910486, 1.6031693412537937, 1.6029411743308974, 1.6026308013673192, 1.6021425529182325, 1.60180368580123, 1.601456428357463, 1.6011334237131984, 1.6008244790580484, 1.601163773810839, 1.601380755867067, 1.6026505318020894, 1.6017820340651996, 1.6012500808958645, 1.6010943727571616, 1.6031160745287822, 1.603776431818028, 1.6018755235711162, 1.6030878548749419, 1.6021548775921612, 1.6021374014124, 1.6012714849605207, 1.601480635774209, 1.6006805122265826, 1.6003057077435252, 1.5998831915414797, 1.6003205169152919, 1.5998429719427527, 1.5998133095872475, 1.5994015133601194, 1.599064880572795, 1.5991706070224363, 1.5990443015735007, 1.5995651667122968, 1.599163781495065, 1.5991774713234246, 1.5991997209172963, 1.5993671962612708, 1.5989361318474677, 1.5987456570905336, 1.5987080937783087, 1.5986798579687944, 1.598783976439333, 1.598249485899046, 1.59862588783554, 1.598538436537161, 1.5987130437788288, 1.5983596890369236, 1.5988500900581877, 1.598571430437374, 1.5984966374520648, 1.5983604635301312, 1.597984285961676, 1.598739825871446, 1.5977805778476002, 1.5989265368949217, 1.599040726616642, 1.6001887035076134, 1.5996065103053068, 1.5996141436653215, 1.598724277014605, 1.5988032552006308, 1.5987180561010843, 1.598325261638884, 1.5991366149463693, 1.5980022054433334, 1.598040577175681, 1.5979261616661808, 1.5976589080489392, 1.597543327911189, 1.5974197494420672, 1.597060632754645, 1.5970677280817678, 1.5969162274190287, 1.5972071697090195, 1.5970344972316735, 1.5972721930646798, 1.5990040897099145, 1.5989380089898864, 1.5996494629055078, 1.598204078664525, 1.5974947957777146, 1.5979048775941194, 1.5976099312917407, 1.5981769635692025, 1.5978926678213006, 1.598883824671563, 1.5975836528644913, 1.5976288498304707, 1.597549790180684, 1.5977074541839975, 1.5968059890324084, 1.5978837938524124, 1.5970453495852022, 1.5971786780523813, 1.5969159569828417, 1.5974217251830523, 1.5962532173191988, 1.5962188625727347, 1.5966624596280483, 1.5973481900882918, 1.5961836839358665, 1.5964582867690915, 1.5962707965036191, 1.596228303360988, 1.596994014344421, 1.5964077498144194, 1.596281285109706, 1.5962945054443956, 1.5963683536165305, 1.5959284340331688, 1.596019406974683, 1.5958492170858678, 1.5961929038809555, 1.5959212955752926, 1.596105597787814, 1.596324505306612, 1.5954871424904105, 1.5959897262604574, 1.5954353816700177, 1.5958551329019377, 1.596203028641687, 1.5954397277420795, 1.5954388122539012, 1.5951227993446209, 1.5953391130944787, 1.5956966417770855, 1.5953864893139755, 1.595474834608591, 1.5954637156374891, 1.595347870742516, 1.5950449192548435, 1.5953299727528003, 1.595082620281948, 1.5959028729178333, 1.596219911908222, 1.5958381089341713, 1.595810482927906, 1.5955006646424592, 1.5959011118514828, 1.5954049436463467, 1.5954908681356441, 1.5958465987407207, 1.5952956853706, 1.5955140565699866, 1.595933486793565, 1.5959567601186293, 1.5957657816963273, 1.595102348954281, 1.5953993263675448, 1.5956550958465012, 1.5957970541360686, 1.5959349280754889, 1.5953783221802917, 1.595457181499724, 1.59527097394579, 1.5949088440783459, 1.5946426701986325, 1.596362452536393, 1.5973157736310235, 1.5958466546001866, 1.5945136769841095, 1.596069585518181, 1.5946819888003307, 1.5947828567982698, 1.5945363356102664, 1.5944114623373293, 1.5945010482408182, 1.5946026942078828, 1.5944659240926315, 1.5946552223738215, 1.5943639676918484, 1.5945069347808494, 1.5940673757137949, 1.5940770548227141, 1.5940015640102128, 1.5941704261474297, 1.5934850892002332, 1.5935913369885706, 1.5933215599040476, 1.5936141398897896, 1.5931302692366331, 1.5930349028330808, 1.5933984757938424, 1.5936281722184324, 1.5934339128725339, 1.5928903348147256, 1.5927870257434413, 1.5939979697644588, 1.5933432499730855, 1.5931018481753936, 1.5926719412421788, 1.5929618519189666, 1.5928950130082744, 1.5926110659291857, 1.5927472983052844, 1.592501846769752, 1.592236758159661, 1.5921711759156025, 1.5921976847325507, 1.5920803272259063, 1.5918177368704542, 1.5920065529292613, 1.5914676820473015, 1.5915534018491082, 1.59147793380142, 1.5913504712635487, 1.5914190618409267, 1.5911832622923645, 1.5914127584355569, 1.5908409656685236, 1.5906959280096284, 1.5907256619396641, 1.5903797339609762, 1.5903464862208592, 1.5910778964569436, 1.5905365113604975, 1.590769227035726, 1.5897437702214203, 1.5903097241321384, 1.5897386822612378, 1.5902220228124693, 1.5893913067831396, 1.589142561693211, 1.5892945834988197, 1.5891703550820477, 1.5891083942546491, 1.588919282400143, 1.5891383653793492, 1.5887222479501055, 1.589150361505622, 1.588405534374151, 1.5886355290422693, 1.5890416498791267, 1.5882107890362123, 1.588279143251188, 1.5878177843054706, 1.5884124884615198, 1.588045880926708, 1.5878924032501127, 1.5876575348558366, 1.5878349376165402, 1.5873898433219236, 1.5871761931041428, 1.5874326180628437, 1.5874598876651553, 1.5872279293972853, 1.5870288114038582, 1.587353270793108, 1.5868070610250047, 1.5867862295565909, 1.5872946574702644, 1.586824342651289, 1.5866726196277312, 1.5862125662073219, 1.5865086777254296, 1.5872791878251813, 1.586511772662952, 1.5864847022160367, 1.5862013863831819, 1.5862916976764216, 1.5859157462384421, 1.5864045303215482, 1.5853497872851958, 1.5861410516488235, 1.5857503397508814, 1.5858170585710656, 1.5851570056449216, 1.585393598388108, 1.5856663386190206, 1.5850700950230905, 1.585053101212582, 1.5850726504100667, 1.5846223757252311, 1.5859169980583738, 1.5851833026267175, 1.5848554699817476, 1.5846615927420113, 1.5851233380531138, 1.5842313739063802, 1.5847200477882089, 1.5845656538890862, 1.5848101406371569, 1.583606908012954, 1.5843078761130143, 1.5844598260013965, 1.584231575754389, 1.5840567165331674, 1.5843794832484188, 1.5839401261762427, 1.5841299901507964, 1.5837214296113784], 'acc': [0.18069815283805682, 0.1913757708895133, 0.2176591393883958, 0.2308008220100305, 0.23244353168064563, 0.23285420929871545, 0.2328542083195837, 0.23285421029620593, 0.2328542083195837, 0.23285420890706277, 0.2344969205543001, 0.23778234169468498, 0.24312115131707163, 0.2443531822313763, 0.2447638614160569, 0.24558521643801146, 0.2488706377742227, 0.24558521467557434, 0.2451745382324626, 0.23572895283938922, 0.24640656993007268, 0.24763860354922881, 0.2455852166338378, 0.23367556435738746, 0.23285421010037957, 0.24065708404204195, 0.24517453686167817, 0.232443531503178, 0.23244353168064563, 0.24312114755965356, 0.24517453883830037, 0.24312115053376623, 0.24804928175477767, 0.2451745394074207, 0.24353182815183605, 0.2435318257652024, 0.24476385963526104, 0.2447638595985436, 0.24394250617991728, 0.24804928098983098, 0.24435318280049662, 0.24229979431849485, 0.24312114998300463, 0.2447638602227401, 0.24517453825082133, 0.2464065701075403, 0.24558521585053242, 0.24435318495458647, 0.24640657169250982, 0.24476386081021914, 0.24804928136312496, 0.24517453784080992, 0.2435318283476624, 0.25051334589658575, 0.2451745382324626, 0.24312115053376623, 0.2435318291309678, 0.24312114955463449, 0.24476386065111022, 0.24558521585053242, 0.24435318242720266, 0.24435318456293376, 0.2505133490298073, 0.24640657069501937, 0.24188911750208916, 0.24681724890056822, 0.24722792651863804, 0.23942505140201756, 0.2464065713008571, 0.246817250467179, 0.24681724733395743, 0.2468172461222819, 0.24353182854348873, 0.24599589248947049, 0.2472279276935961, 0.24763860276592342, 0.24722792575369135, 0.2468172492922209, 0.24476385999019631, 0.24599589307694955, 0.2468172461222819, 0.24599589427026636, 0.2451745382324626, 0.24435318360216074, 0.24476386063275152, 0.24106776109099143, 0.23819301774614401, 0.24722792692864945, 0.24969199320618865, 0.24722792710611707, 0.2464065701075403, 0.2447638602227401, 0.2464065716741511, 0.24476386081021914, 0.24435318281885535, 0.23942505101036488, 0.24558521545887974, 0.24558521545887974, 0.242710471544912, 0.2439425055740795, 0.24476385963526104, 0.24640657169250982, 0.2418891180895682, 0.24435318240884393, 0.2464065720658038, 0.2472279255395063, 0.24271047350317546, 0.24599589407444, 0.24435318360216074, 0.2427104719549234, 0.2451745374491572, 0.2447638598310874, 0.24229979549345296, 0.23778234032390053, 0.24681724890056822, 0.2476386031575761, 0.241478439688193, 0.23696098549777234, 0.24517453883830037, 0.2439425046133065, 0.24229979353518946, 0.24599589127779498, 0.24599589483938666, 0.24188911650459868, 0.24435318377962836, 0.24599589407444, 0.24188911630877236, 0.24188911630877236, 0.24681724733395743, 0.24435318338797568, 0.24394250459494776, 0.24517453864247402, 0.24599589444773398, 0.24353182756435698, 0.2455852156730648, 0.24435318338797568, 0.2480492831255621, 0.24804928157731004, 0.24517453803663625, 0.24640656850421208, 0.24763860474254562, 0.24599589248947049, 0.24271047213239103, 0.24517453686167817, 0.24681724831308918, 0.24271047428648085, 0.24188911709207775, 0.2447638602227401, 0.24271047232821738, 0.2455852148897594, 0.24394250578826457, 0.24106776322672255, 0.23942505239950804, 0.24065708366874797, 0.2406570850211737, 0.24271047174073831, 0.24147843949236664, 0.239835729197555, 0.24394250479077412, 0.24147843829904983, 0.24106776187429682, 0.2435318279560097, 0.24271047312988148, 0.24353182854348873, 0.24312115094377765, 0.24394250616155855, 0.24517453784080992, 0.2447638594394347, 0.23696098606689264, 0.24599589248947049, 0.2394250515978439, 0.24435318360216074, 0.24599589425190763, 0.24517453825082133, 0.2455852178087959, 0.24188911924616757, 0.24312114957299322, 0.2443531837979871, 0.23942505236279057, 0.241889116112946, 0.24394250420329508, 0.23901437533219982, 0.23942505157948518, 0.24147844006148697, 0.23531827541714576, 0.23696098608525137, 0.24147843966983426, 0.23901437378394774, 0.23285420890706277, 0.23696098587106632, 0.23901437339229506, 0.2394250504045271, 0.23942505236279057, 0.23942505020870075, 0.24147844025731333, 0.2373716637033212, 0.23778234247799038, 0.23819301833362305, 0.23901437478143822, 0.23778234112556465, 0.2422997945326799, 0.23613962945996858, 0.24024640605067815, 0.23737166429080023, 0.23901437378394774, 0.2488706374009287, 0.24763860474254562, 0.23613962928250096, 0.23449692037683248, 0.23696098647690406, 0.24681724932893837, 0.24229979353518946, 0.23942505081453852, 0.24188911709207775, 0.24024640720727752, 0.2390143747630795, 0.24065708482534734, 0.24476386122023056, 0.24229979471014756, 0.2418891180895682, 0.24271047115325928, 0.24599589464356034, 0.24476386159352453, 0.24229979570763802, 0.2509240255096365, 0.24804928214643035, 0.249691990232076, 0.24845995956867384, 0.250924024512146, 0.2533880905938589, 0.2517453807090587, 0.25174537972992694, 0.2505133464840648, 0.2501026690618213, 0.24394250576990587, 0.25256673516189293, 0.24804928195060402, 0.2492813150006398, 0.24928131345238774, 0.2517453775391197, 0.24763860413670785, 0.2533880921604697, 0.2542094432475386, 0.2550308028285753, 0.2513347028951625, 0.25256673594519835, 0.25256673596355705, 0.24558521585053242, 0.25215605656469137, 0.25215605813130215, 0.24763860550749228, 0.25133470367846794, 0.2476386039408815, 0.25174538129653773, 0.24558521565470606, 0.2505133446849102, 0.25010267023677946, 0.2505133452723893, 0.24640656973424632, 0.2488706358159592, 0.25051334787320795, 0.2472279267144644, 0.25092402429796096, 0.24804928077564592, 0.24558521467557434, 0.24763860335340246, 0.25215605672380026, 0.251745380317406, 0.25092402568710415, 0.24845995996032652, 0.2484599597828589, 0.24969199144375154, 0.25010266984512675, 0.24804927897649134, 0.255441478310914, 0.25297741356326814, 0.24887063640343823, 0.2496919928328947, 0.2537987691910605, 0.25503080167197595, 0.25667351052256826, 0.25626283411617395, 0.2566735127133755, 0.25585215411147055, 0.2566735117342438, 0.25913757626770456, 0.2554414798591661, 0.25338809078968527, 0.2537987682119287, 0.2546201252105055, 0.2505133472673702, 0.2566735123217228, 0.2525667349660666, 0.25544147629757435, 0.2542094452058021, 0.257084190135619, 0.25420944344336494, 0.25379876840775506, 0.2550308010661382, 0.24845996035197923]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
