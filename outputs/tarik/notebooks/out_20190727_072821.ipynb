{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf32.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 07:28:21 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'All', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '01', '02', '03', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000290010BBE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002904C1E6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6172, Accuracy:0.1823, Validation Loss:1.6119, Validation Accuracy:0.1839\n",
    "Epoch #2: Loss:1.6096, Accuracy:0.2226, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6064, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6019, Accuracy:0.2329, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6001, Accuracy:0.2329, Validation Loss:1.5982, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.5967, Accuracy:0.2333, Validation Loss:1.5930, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.5898, Accuracy:0.2382, Validation Loss:1.5831, Validation Accuracy:0.2365\n",
    "Epoch #20: Loss:1.5765, Accuracy:0.2476, Validation Loss:1.5634, Validation Accuracy:0.2545\n",
    "Epoch #21: Loss:1.5500, Accuracy:0.2908, Validation Loss:1.5255, Validation Accuracy:0.3169\n",
    "Epoch #22: Loss:1.5025, Accuracy:0.3466, Validation Loss:1.4729, Validation Accuracy:0.3136\n",
    "Epoch #23: Loss:1.4363, Accuracy:0.3754, Validation Loss:1.3862, Validation Accuracy:0.4007\n",
    "Epoch #24: Loss:1.3478, Accuracy:0.4123, Validation Loss:1.3089, Validation Accuracy:0.4351\n",
    "Epoch #25: Loss:1.2724, Accuracy:0.4263, Validation Loss:1.2334, Validation Accuracy:0.4499\n",
    "Epoch #26: Loss:1.2052, Accuracy:0.4493, Validation Loss:1.1766, Validation Accuracy:0.4680\n",
    "Epoch #27: Loss:1.1575, Accuracy:0.4632, Validation Loss:1.1299, Validation Accuracy:0.4992\n",
    "Epoch #28: Loss:1.1252, Accuracy:0.4834, Validation Loss:1.1000, Validation Accuracy:0.5107\n",
    "Epoch #29: Loss:1.0893, Accuracy:0.4961, Validation Loss:1.0778, Validation Accuracy:0.5189\n",
    "Epoch #30: Loss:1.0796, Accuracy:0.4961, Validation Loss:1.0560, Validation Accuracy:0.5090\n",
    "Epoch #31: Loss:1.0782, Accuracy:0.5113, Validation Loss:1.0490, Validation Accuracy:0.5090\n",
    "Epoch #32: Loss:1.0571, Accuracy:0.4957, Validation Loss:1.0864, Validation Accuracy:0.4877\n",
    "Epoch #33: Loss:1.0469, Accuracy:0.5092, Validation Loss:1.0213, Validation Accuracy:0.5222\n",
    "Epoch #34: Loss:1.0221, Accuracy:0.5199, Validation Loss:1.0156, Validation Accuracy:0.5172\n",
    "Epoch #35: Loss:1.0041, Accuracy:0.5211, Validation Loss:1.0040, Validation Accuracy:0.5320\n",
    "Epoch #36: Loss:0.9975, Accuracy:0.5261, Validation Loss:1.0035, Validation Accuracy:0.5222\n",
    "Epoch #37: Loss:0.9941, Accuracy:0.5211, Validation Loss:0.9925, Validation Accuracy:0.5304\n",
    "Epoch #38: Loss:0.9790, Accuracy:0.5335, Validation Loss:0.9794, Validation Accuracy:0.5353\n",
    "Epoch #39: Loss:0.9705, Accuracy:0.5429, Validation Loss:0.9722, Validation Accuracy:0.5353\n",
    "Epoch #40: Loss:0.9622, Accuracy:0.5446, Validation Loss:0.9639, Validation Accuracy:0.5468\n",
    "Epoch #41: Loss:0.9520, Accuracy:0.5458, Validation Loss:0.9761, Validation Accuracy:0.5649\n",
    "Epoch #42: Loss:0.9444, Accuracy:0.5561, Validation Loss:0.9495, Validation Accuracy:0.5616\n",
    "Epoch #43: Loss:0.9269, Accuracy:0.5614, Validation Loss:0.9512, Validation Accuracy:0.5452\n",
    "Epoch #44: Loss:0.9282, Accuracy:0.5515, Validation Loss:0.9295, Validation Accuracy:0.5567\n",
    "Epoch #45: Loss:0.9122, Accuracy:0.5618, Validation Loss:0.9221, Validation Accuracy:0.5780\n",
    "Epoch #46: Loss:0.8986, Accuracy:0.5696, Validation Loss:0.9239, Validation Accuracy:0.5681\n",
    "Epoch #47: Loss:0.8953, Accuracy:0.5696, Validation Loss:0.9098, Validation Accuracy:0.5747\n",
    "Epoch #48: Loss:0.8822, Accuracy:0.5766, Validation Loss:0.9002, Validation Accuracy:0.5846\n",
    "Epoch #49: Loss:0.8811, Accuracy:0.5708, Validation Loss:0.8922, Validation Accuracy:0.5813\n",
    "Epoch #50: Loss:0.8807, Accuracy:0.5786, Validation Loss:0.8905, Validation Accuracy:0.5878\n",
    "Epoch #51: Loss:0.8669, Accuracy:0.5811, Validation Loss:0.8837, Validation Accuracy:0.5911\n",
    "Epoch #52: Loss:0.8506, Accuracy:0.5869, Validation Loss:0.8799, Validation Accuracy:0.6043\n",
    "Epoch #53: Loss:0.8492, Accuracy:0.5844, Validation Loss:0.8860, Validation Accuracy:0.5961\n",
    "Epoch #54: Loss:0.8516, Accuracy:0.5828, Validation Loss:0.8647, Validation Accuracy:0.6076\n",
    "Epoch #55: Loss:0.8432, Accuracy:0.5906, Validation Loss:0.8912, Validation Accuracy:0.5928\n",
    "Epoch #56: Loss:0.8533, Accuracy:0.5877, Validation Loss:0.8760, Validation Accuracy:0.5944\n",
    "Epoch #57: Loss:0.8340, Accuracy:0.5971, Validation Loss:0.8360, Validation Accuracy:0.6076\n",
    "Epoch #58: Loss:0.8223, Accuracy:0.5979, Validation Loss:0.8312, Validation Accuracy:0.6059\n",
    "Epoch #59: Loss:0.8142, Accuracy:0.6086, Validation Loss:0.8253, Validation Accuracy:0.6010\n",
    "Epoch #60: Loss:0.8078, Accuracy:0.5996, Validation Loss:0.8161, Validation Accuracy:0.6125\n",
    "Epoch #61: Loss:0.7882, Accuracy:0.6078, Validation Loss:0.8099, Validation Accuracy:0.6240\n",
    "Epoch #62: Loss:0.7797, Accuracy:0.6209, Validation Loss:0.7955, Validation Accuracy:0.6240\n",
    "Epoch #63: Loss:0.7747, Accuracy:0.6156, Validation Loss:0.7864, Validation Accuracy:0.6240\n",
    "Epoch #64: Loss:0.7658, Accuracy:0.6255, Validation Loss:0.7778, Validation Accuracy:0.6289\n",
    "Epoch #65: Loss:0.7521, Accuracy:0.6324, Validation Loss:0.7700, Validation Accuracy:0.6404\n",
    "Epoch #66: Loss:0.7490, Accuracy:0.6337, Validation Loss:0.7564, Validation Accuracy:0.6453\n",
    "Epoch #67: Loss:0.7458, Accuracy:0.6402, Validation Loss:0.7559, Validation Accuracy:0.6470\n",
    "Epoch #68: Loss:0.7258, Accuracy:0.6460, Validation Loss:0.7355, Validation Accuracy:0.6470\n",
    "Epoch #69: Loss:0.7157, Accuracy:0.6509, Validation Loss:0.7278, Validation Accuracy:0.6650\n",
    "Epoch #70: Loss:0.7125, Accuracy:0.6563, Validation Loss:0.7391, Validation Accuracy:0.6502\n",
    "Epoch #71: Loss:0.7298, Accuracy:0.6476, Validation Loss:0.7479, Validation Accuracy:0.6585\n",
    "Epoch #72: Loss:0.7029, Accuracy:0.6698, Validation Loss:0.7190, Validation Accuracy:0.6700\n",
    "Epoch #73: Loss:0.6899, Accuracy:0.6694, Validation Loss:0.6999, Validation Accuracy:0.6765\n",
    "Epoch #74: Loss:0.6939, Accuracy:0.6694, Validation Loss:0.7063, Validation Accuracy:0.6749\n",
    "Epoch #75: Loss:0.6806, Accuracy:0.6727, Validation Loss:0.7091, Validation Accuracy:0.6667\n",
    "Epoch #76: Loss:0.6742, Accuracy:0.6743, Validation Loss:0.6889, Validation Accuracy:0.6831\n",
    "Epoch #77: Loss:0.6648, Accuracy:0.6821, Validation Loss:0.7039, Validation Accuracy:0.6847\n",
    "Epoch #78: Loss:0.6853, Accuracy:0.6702, Validation Loss:0.6822, Validation Accuracy:0.6897\n",
    "Epoch #79: Loss:0.6698, Accuracy:0.6817, Validation Loss:0.6856, Validation Accuracy:0.6765\n",
    "Epoch #80: Loss:0.6496, Accuracy:0.6821, Validation Loss:0.6711, Validation Accuracy:0.6913\n",
    "Epoch #81: Loss:0.6446, Accuracy:0.6945, Validation Loss:0.6675, Validation Accuracy:0.6897\n",
    "Epoch #82: Loss:0.6467, Accuracy:0.6928, Validation Loss:0.6814, Validation Accuracy:0.6962\n",
    "Epoch #83: Loss:0.6514, Accuracy:0.6871, Validation Loss:0.6602, Validation Accuracy:0.6962\n",
    "Epoch #84: Loss:0.6369, Accuracy:0.7006, Validation Loss:0.6997, Validation Accuracy:0.6831\n",
    "Epoch #85: Loss:0.6465, Accuracy:0.6895, Validation Loss:0.6598, Validation Accuracy:0.7061\n",
    "Epoch #86: Loss:0.6336, Accuracy:0.6977, Validation Loss:0.6514, Validation Accuracy:0.6946\n",
    "Epoch #87: Loss:0.6314, Accuracy:0.6924, Validation Loss:0.6532, Validation Accuracy:0.7011\n",
    "Epoch #88: Loss:0.6264, Accuracy:0.7060, Validation Loss:0.6543, Validation Accuracy:0.6995\n",
    "Epoch #89: Loss:0.6276, Accuracy:0.6990, Validation Loss:0.6558, Validation Accuracy:0.7044\n",
    "Epoch #90: Loss:0.6336, Accuracy:0.6916, Validation Loss:0.6469, Validation Accuracy:0.7094\n",
    "Epoch #91: Loss:0.6379, Accuracy:0.6924, Validation Loss:0.6879, Validation Accuracy:0.6929\n",
    "Epoch #92: Loss:0.6303, Accuracy:0.6928, Validation Loss:0.6448, Validation Accuracy:0.7094\n",
    "Epoch #93: Loss:0.6144, Accuracy:0.7055, Validation Loss:0.6357, Validation Accuracy:0.7176\n",
    "Epoch #94: Loss:0.6095, Accuracy:0.7060, Validation Loss:0.6478, Validation Accuracy:0.7011\n",
    "Epoch #95: Loss:0.6181, Accuracy:0.7043, Validation Loss:0.6423, Validation Accuracy:0.7077\n",
    "Epoch #96: Loss:0.6086, Accuracy:0.7117, Validation Loss:0.6276, Validation Accuracy:0.7028\n",
    "Epoch #97: Loss:0.6009, Accuracy:0.7154, Validation Loss:0.6309, Validation Accuracy:0.7110\n",
    "Epoch #98: Loss:0.6003, Accuracy:0.7142, Validation Loss:0.6251, Validation Accuracy:0.7143\n",
    "Epoch #99: Loss:0.6005, Accuracy:0.7191, Validation Loss:0.6509, Validation Accuracy:0.7077\n",
    "Epoch #100: Loss:0.6067, Accuracy:0.7158, Validation Loss:0.6241, Validation Accuracy:0.7094\n",
    "Epoch #101: Loss:0.5965, Accuracy:0.7146, Validation Loss:0.6200, Validation Accuracy:0.7209\n",
    "Epoch #102: Loss:0.5934, Accuracy:0.7220, Validation Loss:0.6182, Validation Accuracy:0.7061\n",
    "Epoch #103: Loss:0.5928, Accuracy:0.7187, Validation Loss:0.6395, Validation Accuracy:0.7077\n",
    "Epoch #104: Loss:0.6096, Accuracy:0.7043, Validation Loss:0.6700, Validation Accuracy:0.6831\n",
    "Epoch #105: Loss:0.6061, Accuracy:0.7138, Validation Loss:0.6151, Validation Accuracy:0.7110\n",
    "Epoch #106: Loss:0.5958, Accuracy:0.7203, Validation Loss:0.6497, Validation Accuracy:0.6880\n",
    "Epoch #107: Loss:0.6000, Accuracy:0.7072, Validation Loss:0.6176, Validation Accuracy:0.7159\n",
    "Epoch #108: Loss:0.5979, Accuracy:0.7191, Validation Loss:0.6114, Validation Accuracy:0.7176\n",
    "Epoch #109: Loss:0.5881, Accuracy:0.7261, Validation Loss:0.6093, Validation Accuracy:0.7225\n",
    "Epoch #110: Loss:0.5959, Accuracy:0.7072, Validation Loss:0.6192, Validation Accuracy:0.7126\n",
    "Epoch #111: Loss:0.6016, Accuracy:0.7158, Validation Loss:0.6132, Validation Accuracy:0.7192\n",
    "Epoch #112: Loss:0.5912, Accuracy:0.7191, Validation Loss:0.6393, Validation Accuracy:0.7028\n",
    "Epoch #113: Loss:0.6163, Accuracy:0.7072, Validation Loss:0.6318, Validation Accuracy:0.7044\n",
    "Epoch #114: Loss:0.6078, Accuracy:0.7101, Validation Loss:0.6028, Validation Accuracy:0.7258\n",
    "Epoch #115: Loss:0.5887, Accuracy:0.7265, Validation Loss:0.6434, Validation Accuracy:0.7061\n",
    "Epoch #116: Loss:0.5797, Accuracy:0.7220, Validation Loss:0.6489, Validation Accuracy:0.7094\n",
    "Epoch #117: Loss:0.6136, Accuracy:0.7117, Validation Loss:0.6050, Validation Accuracy:0.7209\n",
    "Epoch #118: Loss:0.6222, Accuracy:0.7055, Validation Loss:0.6170, Validation Accuracy:0.7192\n",
    "Epoch #119: Loss:0.6242, Accuracy:0.6990, Validation Loss:0.6547, Validation Accuracy:0.6946\n",
    "Epoch #120: Loss:0.6154, Accuracy:0.7150, Validation Loss:0.6407, Validation Accuracy:0.7077\n",
    "Epoch #121: Loss:0.5881, Accuracy:0.7228, Validation Loss:0.6352, Validation Accuracy:0.7028\n",
    "Epoch #122: Loss:0.5967, Accuracy:0.7207, Validation Loss:0.6188, Validation Accuracy:0.7176\n",
    "Epoch #123: Loss:0.5787, Accuracy:0.7195, Validation Loss:0.6084, Validation Accuracy:0.7225\n",
    "Epoch #124: Loss:0.5621, Accuracy:0.7392, Validation Loss:0.6007, Validation Accuracy:0.7258\n",
    "Epoch #125: Loss:0.5616, Accuracy:0.7392, Validation Loss:0.6095, Validation Accuracy:0.7209\n",
    "Epoch #126: Loss:0.5657, Accuracy:0.7326, Validation Loss:0.6054, Validation Accuracy:0.7258\n",
    "Epoch #127: Loss:0.5659, Accuracy:0.7326, Validation Loss:0.5911, Validation Accuracy:0.7274\n",
    "Epoch #128: Loss:0.5635, Accuracy:0.7396, Validation Loss:0.6013, Validation Accuracy:0.7110\n",
    "Epoch #129: Loss:0.5657, Accuracy:0.7351, Validation Loss:0.6495, Validation Accuracy:0.7159\n",
    "Epoch #130: Loss:0.5904, Accuracy:0.7191, Validation Loss:0.6231, Validation Accuracy:0.7159\n",
    "Epoch #131: Loss:0.5659, Accuracy:0.7351, Validation Loss:0.5996, Validation Accuracy:0.7094\n",
    "Epoch #132: Loss:0.5625, Accuracy:0.7413, Validation Loss:0.5871, Validation Accuracy:0.7291\n",
    "Epoch #133: Loss:0.5734, Accuracy:0.7298, Validation Loss:0.5940, Validation Accuracy:0.7340\n",
    "Epoch #134: Loss:0.5649, Accuracy:0.7396, Validation Loss:0.5890, Validation Accuracy:0.7356\n",
    "Epoch #135: Loss:0.5574, Accuracy:0.7417, Validation Loss:0.6136, Validation Accuracy:0.7209\n",
    "Epoch #136: Loss:0.5557, Accuracy:0.7462, Validation Loss:0.5846, Validation Accuracy:0.7258\n",
    "Epoch #137: Loss:0.5522, Accuracy:0.7450, Validation Loss:0.5896, Validation Accuracy:0.7307\n",
    "Epoch #138: Loss:0.5487, Accuracy:0.7487, Validation Loss:0.6242, Validation Accuracy:0.7094\n",
    "Epoch #139: Loss:0.5708, Accuracy:0.7368, Validation Loss:0.6016, Validation Accuracy:0.7258\n",
    "Epoch #140: Loss:0.5610, Accuracy:0.7347, Validation Loss:0.5823, Validation Accuracy:0.7340\n",
    "Epoch #141: Loss:0.5530, Accuracy:0.7413, Validation Loss:0.5859, Validation Accuracy:0.7225\n",
    "Epoch #142: Loss:0.5577, Accuracy:0.7437, Validation Loss:0.6027, Validation Accuracy:0.7192\n",
    "Epoch #143: Loss:0.5535, Accuracy:0.7441, Validation Loss:0.5783, Validation Accuracy:0.7307\n",
    "Epoch #144: Loss:0.5487, Accuracy:0.7520, Validation Loss:0.5815, Validation Accuracy:0.7373\n",
    "Epoch #145: Loss:0.5404, Accuracy:0.7536, Validation Loss:0.5841, Validation Accuracy:0.7274\n",
    "Epoch #146: Loss:0.5426, Accuracy:0.7507, Validation Loss:0.5791, Validation Accuracy:0.7209\n",
    "Epoch #147: Loss:0.5414, Accuracy:0.7520, Validation Loss:0.5828, Validation Accuracy:0.7274\n",
    "Epoch #148: Loss:0.5407, Accuracy:0.7536, Validation Loss:0.5935, Validation Accuracy:0.7241\n",
    "Epoch #149: Loss:0.5429, Accuracy:0.7466, Validation Loss:0.5790, Validation Accuracy:0.7225\n",
    "Epoch #150: Loss:0.5347, Accuracy:0.7585, Validation Loss:0.5761, Validation Accuracy:0.7373\n",
    "Epoch #151: Loss:0.5386, Accuracy:0.7552, Validation Loss:0.5726, Validation Accuracy:0.7307\n",
    "Epoch #152: Loss:0.5409, Accuracy:0.7511, Validation Loss:0.5909, Validation Accuracy:0.7225\n",
    "Epoch #153: Loss:0.5377, Accuracy:0.7528, Validation Loss:0.5723, Validation Accuracy:0.7274\n",
    "Epoch #154: Loss:0.5335, Accuracy:0.7630, Validation Loss:0.5746, Validation Accuracy:0.7356\n",
    "Epoch #155: Loss:0.5381, Accuracy:0.7540, Validation Loss:0.5750, Validation Accuracy:0.7356\n",
    "Epoch #156: Loss:0.5314, Accuracy:0.7569, Validation Loss:0.5718, Validation Accuracy:0.7406\n",
    "Epoch #157: Loss:0.5325, Accuracy:0.7552, Validation Loss:0.5717, Validation Accuracy:0.7422\n",
    "Epoch #158: Loss:0.5311, Accuracy:0.7622, Validation Loss:0.5774, Validation Accuracy:0.7340\n",
    "Epoch #159: Loss:0.5300, Accuracy:0.7573, Validation Loss:0.5797, Validation Accuracy:0.7340\n",
    "Epoch #160: Loss:0.5292, Accuracy:0.7598, Validation Loss:0.5731, Validation Accuracy:0.7323\n",
    "Epoch #161: Loss:0.5291, Accuracy:0.7565, Validation Loss:0.5700, Validation Accuracy:0.7389\n",
    "Epoch #162: Loss:0.5358, Accuracy:0.7581, Validation Loss:0.5779, Validation Accuracy:0.7389\n",
    "Epoch #163: Loss:0.5319, Accuracy:0.7536, Validation Loss:0.5733, Validation Accuracy:0.7389\n",
    "Epoch #164: Loss:0.5358, Accuracy:0.7532, Validation Loss:0.5863, Validation Accuracy:0.7323\n",
    "Epoch #165: Loss:0.5276, Accuracy:0.7602, Validation Loss:0.5755, Validation Accuracy:0.7307\n",
    "Epoch #166: Loss:0.5296, Accuracy:0.7511, Validation Loss:0.5659, Validation Accuracy:0.7356\n",
    "Epoch #167: Loss:0.5216, Accuracy:0.7630, Validation Loss:0.5837, Validation Accuracy:0.7422\n",
    "Epoch #168: Loss:0.5314, Accuracy:0.7569, Validation Loss:0.5710, Validation Accuracy:0.7438\n",
    "Epoch #169: Loss:0.5253, Accuracy:0.7671, Validation Loss:0.5752, Validation Accuracy:0.7373\n",
    "Epoch #170: Loss:0.5301, Accuracy:0.7606, Validation Loss:0.5686, Validation Accuracy:0.7340\n",
    "Epoch #171: Loss:0.5231, Accuracy:0.7565, Validation Loss:0.5786, Validation Accuracy:0.7356\n",
    "Epoch #172: Loss:0.5225, Accuracy:0.7614, Validation Loss:0.5744, Validation Accuracy:0.7340\n",
    "Epoch #173: Loss:0.5253, Accuracy:0.7630, Validation Loss:0.5668, Validation Accuracy:0.7406\n",
    "Epoch #174: Loss:0.5232, Accuracy:0.7581, Validation Loss:0.5657, Validation Accuracy:0.7422\n",
    "Epoch #175: Loss:0.5247, Accuracy:0.7618, Validation Loss:0.5688, Validation Accuracy:0.7356\n",
    "Epoch #176: Loss:0.5273, Accuracy:0.7565, Validation Loss:0.5646, Validation Accuracy:0.7438\n",
    "Epoch #177: Loss:0.5231, Accuracy:0.7585, Validation Loss:0.5730, Validation Accuracy:0.7422\n",
    "Epoch #178: Loss:0.5237, Accuracy:0.7614, Validation Loss:0.5781, Validation Accuracy:0.7356\n",
    "Epoch #179: Loss:0.5199, Accuracy:0.7598, Validation Loss:0.5668, Validation Accuracy:0.7422\n",
    "Epoch #180: Loss:0.5192, Accuracy:0.7634, Validation Loss:0.5775, Validation Accuracy:0.7488\n",
    "Epoch #181: Loss:0.5302, Accuracy:0.7585, Validation Loss:0.5781, Validation Accuracy:0.7340\n",
    "Epoch #182: Loss:0.5270, Accuracy:0.7647, Validation Loss:0.5685, Validation Accuracy:0.7340\n",
    "Epoch #183: Loss:0.5242, Accuracy:0.7634, Validation Loss:0.5715, Validation Accuracy:0.7389\n",
    "Epoch #184: Loss:0.5178, Accuracy:0.7676, Validation Loss:0.5628, Validation Accuracy:0.7323\n",
    "Epoch #185: Loss:0.5158, Accuracy:0.7630, Validation Loss:0.5626, Validation Accuracy:0.7471\n",
    "Epoch #186: Loss:0.5172, Accuracy:0.7622, Validation Loss:0.5668, Validation Accuracy:0.7406\n",
    "Epoch #187: Loss:0.5200, Accuracy:0.7659, Validation Loss:0.5658, Validation Accuracy:0.7521\n",
    "Epoch #188: Loss:0.5252, Accuracy:0.7598, Validation Loss:0.5639, Validation Accuracy:0.7455\n",
    "Epoch #189: Loss:0.5166, Accuracy:0.7639, Validation Loss:0.5653, Validation Accuracy:0.7340\n",
    "Epoch #190: Loss:0.5137, Accuracy:0.7651, Validation Loss:0.5626, Validation Accuracy:0.7455\n",
    "Epoch #191: Loss:0.5130, Accuracy:0.7655, Validation Loss:0.5760, Validation Accuracy:0.7389\n",
    "Epoch #192: Loss:0.5137, Accuracy:0.7667, Validation Loss:0.5727, Validation Accuracy:0.7406\n",
    "Epoch #193: Loss:0.5119, Accuracy:0.7647, Validation Loss:0.6220, Validation Accuracy:0.7241\n",
    "Epoch #194: Loss:0.5504, Accuracy:0.7528, Validation Loss:0.5622, Validation Accuracy:0.7521\n",
    "Epoch #195: Loss:0.5288, Accuracy:0.7602, Validation Loss:0.6282, Validation Accuracy:0.7126\n",
    "Epoch #196: Loss:0.5353, Accuracy:0.7515, Validation Loss:0.5827, Validation Accuracy:0.7455\n",
    "Epoch #197: Loss:0.5232, Accuracy:0.7561, Validation Loss:0.5651, Validation Accuracy:0.7488\n",
    "Epoch #198: Loss:0.5177, Accuracy:0.7606, Validation Loss:0.5872, Validation Accuracy:0.7323\n",
    "Epoch #199: Loss:0.5189, Accuracy:0.7630, Validation Loss:0.5670, Validation Accuracy:0.7504\n",
    "Epoch #200: Loss:0.5129, Accuracy:0.7676, Validation Loss:0.5744, Validation Accuracy:0.7471\n",
    "Epoch #201: Loss:0.5483, Accuracy:0.7466, Validation Loss:0.5896, Validation Accuracy:0.7406\n",
    "Epoch #202: Loss:0.5308, Accuracy:0.7511, Validation Loss:0.5798, Validation Accuracy:0.7323\n",
    "Epoch #203: Loss:0.5232, Accuracy:0.7634, Validation Loss:0.6467, Validation Accuracy:0.7225\n",
    "Epoch #204: Loss:0.5278, Accuracy:0.7577, Validation Loss:0.5779, Validation Accuracy:0.7406\n",
    "Epoch #205: Loss:0.5130, Accuracy:0.7606, Validation Loss:0.5729, Validation Accuracy:0.7422\n",
    "Epoch #206: Loss:0.5084, Accuracy:0.7696, Validation Loss:0.5836, Validation Accuracy:0.7356\n",
    "Epoch #207: Loss:0.5108, Accuracy:0.7692, Validation Loss:0.5622, Validation Accuracy:0.7438\n",
    "Epoch #208: Loss:0.5024, Accuracy:0.7713, Validation Loss:0.5623, Validation Accuracy:0.7438\n",
    "Epoch #209: Loss:0.5048, Accuracy:0.7696, Validation Loss:0.5641, Validation Accuracy:0.7488\n",
    "Epoch #210: Loss:0.5107, Accuracy:0.7696, Validation Loss:0.5657, Validation Accuracy:0.7471\n",
    "Epoch #211: Loss:0.5154, Accuracy:0.7655, Validation Loss:0.5661, Validation Accuracy:0.7471\n",
    "Epoch #212: Loss:0.5017, Accuracy:0.7676, Validation Loss:0.5679, Validation Accuracy:0.7471\n",
    "Epoch #213: Loss:0.5024, Accuracy:0.7700, Validation Loss:0.5624, Validation Accuracy:0.7537\n",
    "Epoch #214: Loss:0.5048, Accuracy:0.7651, Validation Loss:0.5635, Validation Accuracy:0.7521\n",
    "Epoch #215: Loss:0.5004, Accuracy:0.7663, Validation Loss:0.5602, Validation Accuracy:0.7537\n",
    "Epoch #216: Loss:0.5009, Accuracy:0.7696, Validation Loss:0.5602, Validation Accuracy:0.7504\n",
    "Epoch #217: Loss:0.4996, Accuracy:0.7622, Validation Loss:0.5619, Validation Accuracy:0.7471\n",
    "Epoch #218: Loss:0.5005, Accuracy:0.7692, Validation Loss:0.5689, Validation Accuracy:0.7438\n",
    "Epoch #219: Loss:0.5087, Accuracy:0.7696, Validation Loss:0.5980, Validation Accuracy:0.7340\n",
    "Epoch #220: Loss:0.5114, Accuracy:0.7704, Validation Loss:0.5575, Validation Accuracy:0.7504\n",
    "Epoch #221: Loss:0.5019, Accuracy:0.7667, Validation Loss:0.5662, Validation Accuracy:0.7455\n",
    "Epoch #222: Loss:0.5027, Accuracy:0.7647, Validation Loss:0.5587, Validation Accuracy:0.7504\n",
    "Epoch #223: Loss:0.4956, Accuracy:0.7721, Validation Loss:0.5617, Validation Accuracy:0.7488\n",
    "Epoch #224: Loss:0.5008, Accuracy:0.7684, Validation Loss:0.5593, Validation Accuracy:0.7537\n",
    "Epoch #225: Loss:0.5006, Accuracy:0.7708, Validation Loss:0.5678, Validation Accuracy:0.7389\n",
    "Epoch #226: Loss:0.5010, Accuracy:0.7659, Validation Loss:0.5631, Validation Accuracy:0.7438\n",
    "Epoch #227: Loss:0.5008, Accuracy:0.7704, Validation Loss:0.5702, Validation Accuracy:0.7422\n",
    "Epoch #228: Loss:0.5006, Accuracy:0.7700, Validation Loss:0.5645, Validation Accuracy:0.7389\n",
    "Epoch #229: Loss:0.5064, Accuracy:0.7680, Validation Loss:0.5868, Validation Accuracy:0.7340\n",
    "Epoch #230: Loss:0.5142, Accuracy:0.7639, Validation Loss:0.6063, Validation Accuracy:0.7225\n",
    "Epoch #231: Loss:0.5343, Accuracy:0.7528, Validation Loss:0.5650, Validation Accuracy:0.7471\n",
    "Epoch #232: Loss:0.5166, Accuracy:0.7700, Validation Loss:0.5758, Validation Accuracy:0.7406\n",
    "Epoch #233: Loss:0.5085, Accuracy:0.7651, Validation Loss:0.5577, Validation Accuracy:0.7586\n",
    "Epoch #234: Loss:0.5154, Accuracy:0.7651, Validation Loss:0.5759, Validation Accuracy:0.7422\n",
    "Epoch #235: Loss:0.5016, Accuracy:0.7725, Validation Loss:0.5677, Validation Accuracy:0.7471\n",
    "Epoch #236: Loss:0.5021, Accuracy:0.7692, Validation Loss:0.5620, Validation Accuracy:0.7406\n",
    "Epoch #237: Loss:0.5261, Accuracy:0.7556, Validation Loss:0.5850, Validation Accuracy:0.7471\n",
    "Epoch #238: Loss:0.5084, Accuracy:0.7622, Validation Loss:0.5625, Validation Accuracy:0.7504\n",
    "Epoch #239: Loss:0.5015, Accuracy:0.7700, Validation Loss:0.5661, Validation Accuracy:0.7455\n",
    "Epoch #240: Loss:0.5039, Accuracy:0.7655, Validation Loss:0.5684, Validation Accuracy:0.7422\n",
    "Epoch #241: Loss:0.5036, Accuracy:0.7696, Validation Loss:0.5624, Validation Accuracy:0.7422\n",
    "Epoch #242: Loss:0.5025, Accuracy:0.7692, Validation Loss:0.5594, Validation Accuracy:0.7422\n",
    "Epoch #243: Loss:0.5165, Accuracy:0.7544, Validation Loss:0.5900, Validation Accuracy:0.7389\n",
    "Epoch #244: Loss:0.5179, Accuracy:0.7610, Validation Loss:0.5689, Validation Accuracy:0.7521\n",
    "Epoch #245: Loss:0.5009, Accuracy:0.7659, Validation Loss:0.5630, Validation Accuracy:0.7521\n",
    "Epoch #246: Loss:0.5245, Accuracy:0.7548, Validation Loss:0.7094, Validation Accuracy:0.6814\n",
    "Epoch #247: Loss:0.5477, Accuracy:0.7495, Validation Loss:0.5822, Validation Accuracy:0.7406\n",
    "Epoch #248: Loss:0.5263, Accuracy:0.7598, Validation Loss:0.5587, Validation Accuracy:0.7504\n",
    "Epoch #249: Loss:0.5104, Accuracy:0.7717, Validation Loss:0.5606, Validation Accuracy:0.7455\n",
    "Epoch #250: Loss:0.5050, Accuracy:0.7696, Validation Loss:0.5792, Validation Accuracy:0.7373\n",
    "Epoch #251: Loss:0.5080, Accuracy:0.7680, Validation Loss:0.5572, Validation Accuracy:0.7537\n",
    "Epoch #252: Loss:0.4926, Accuracy:0.7696, Validation Loss:0.5663, Validation Accuracy:0.7455\n",
    "Epoch #253: Loss:0.5050, Accuracy:0.7688, Validation Loss:0.5569, Validation Accuracy:0.7586\n",
    "Epoch #254: Loss:0.4917, Accuracy:0.7786, Validation Loss:0.5599, Validation Accuracy:0.7521\n",
    "Epoch #255: Loss:0.4892, Accuracy:0.7770, Validation Loss:0.5558, Validation Accuracy:0.7619\n",
    "Epoch #256: Loss:0.4953, Accuracy:0.7745, Validation Loss:0.6032, Validation Accuracy:0.7373\n",
    "Epoch #257: Loss:0.5238, Accuracy:0.7556, Validation Loss:0.5633, Validation Accuracy:0.7438\n",
    "Epoch #258: Loss:0.5067, Accuracy:0.7680, Validation Loss:0.5570, Validation Accuracy:0.7619\n",
    "Epoch #259: Loss:0.5173, Accuracy:0.7639, Validation Loss:0.6662, Validation Accuracy:0.6913\n",
    "Epoch #260: Loss:0.5383, Accuracy:0.7610, Validation Loss:0.6018, Validation Accuracy:0.7340\n",
    "Epoch #261: Loss:0.5171, Accuracy:0.7647, Validation Loss:0.5594, Validation Accuracy:0.7504\n",
    "Epoch #262: Loss:0.4922, Accuracy:0.7770, Validation Loss:0.5555, Validation Accuracy:0.7488\n",
    "Epoch #263: Loss:0.4963, Accuracy:0.7725, Validation Loss:0.5631, Validation Accuracy:0.7438\n",
    "Epoch #264: Loss:0.4970, Accuracy:0.7762, Validation Loss:0.6076, Validation Accuracy:0.7356\n",
    "Epoch #265: Loss:0.5106, Accuracy:0.7729, Validation Loss:0.6379, Validation Accuracy:0.7241\n",
    "Epoch #266: Loss:0.5192, Accuracy:0.7581, Validation Loss:0.6100, Validation Accuracy:0.7323\n",
    "Epoch #267: Loss:0.5053, Accuracy:0.7700, Validation Loss:0.6115, Validation Accuracy:0.7291\n",
    "Epoch #268: Loss:0.5092, Accuracy:0.7696, Validation Loss:0.5797, Validation Accuracy:0.7356\n",
    "Epoch #269: Loss:0.5035, Accuracy:0.7713, Validation Loss:0.5711, Validation Accuracy:0.7422\n",
    "Epoch #270: Loss:0.5088, Accuracy:0.7680, Validation Loss:0.5670, Validation Accuracy:0.7406\n",
    "Epoch #271: Loss:0.4973, Accuracy:0.7729, Validation Loss:0.5603, Validation Accuracy:0.7471\n",
    "Epoch #272: Loss:0.4931, Accuracy:0.7708, Validation Loss:0.5578, Validation Accuracy:0.7619\n",
    "Epoch #273: Loss:0.4873, Accuracy:0.7733, Validation Loss:0.5794, Validation Accuracy:0.7406\n",
    "Epoch #274: Loss:0.4955, Accuracy:0.7782, Validation Loss:0.5726, Validation Accuracy:0.7422\n",
    "Epoch #275: Loss:0.4999, Accuracy:0.7688, Validation Loss:0.5647, Validation Accuracy:0.7422\n",
    "Epoch #276: Loss:0.4912, Accuracy:0.7758, Validation Loss:0.5554, Validation Accuracy:0.7504\n",
    "Epoch #277: Loss:0.4863, Accuracy:0.7754, Validation Loss:0.5553, Validation Accuracy:0.7553\n",
    "Epoch #278: Loss:0.4848, Accuracy:0.7766, Validation Loss:0.5611, Validation Accuracy:0.7471\n",
    "Epoch #279: Loss:0.4878, Accuracy:0.7696, Validation Loss:0.5677, Validation Accuracy:0.7488\n",
    "Epoch #280: Loss:0.4954, Accuracy:0.7733, Validation Loss:0.5759, Validation Accuracy:0.7438\n",
    "Epoch #281: Loss:0.4934, Accuracy:0.7713, Validation Loss:0.5549, Validation Accuracy:0.7586\n",
    "Epoch #282: Loss:0.4883, Accuracy:0.7799, Validation Loss:0.5824, Validation Accuracy:0.7422\n",
    "Epoch #283: Loss:0.5079, Accuracy:0.7671, Validation Loss:0.5547, Validation Accuracy:0.7521\n",
    "Epoch #284: Loss:0.4922, Accuracy:0.7762, Validation Loss:0.5634, Validation Accuracy:0.7504\n",
    "Epoch #285: Loss:0.5090, Accuracy:0.7684, Validation Loss:0.5937, Validation Accuracy:0.7438\n",
    "Epoch #286: Loss:0.5136, Accuracy:0.7598, Validation Loss:0.6040, Validation Accuracy:0.7307\n",
    "Epoch #287: Loss:0.5153, Accuracy:0.7671, Validation Loss:0.5957, Validation Accuracy:0.7406\n",
    "Epoch #288: Loss:0.4982, Accuracy:0.7680, Validation Loss:0.5745, Validation Accuracy:0.7406\n",
    "Epoch #289: Loss:0.4935, Accuracy:0.7745, Validation Loss:0.5824, Validation Accuracy:0.7455\n",
    "Epoch #290: Loss:0.4903, Accuracy:0.7749, Validation Loss:0.5692, Validation Accuracy:0.7438\n",
    "Epoch #291: Loss:0.4810, Accuracy:0.7749, Validation Loss:0.5539, Validation Accuracy:0.7504\n",
    "Epoch #292: Loss:0.4798, Accuracy:0.7795, Validation Loss:0.5612, Validation Accuracy:0.7471\n",
    "Epoch #293: Loss:0.4839, Accuracy:0.7766, Validation Loss:0.5929, Validation Accuracy:0.7323\n",
    "Epoch #294: Loss:0.4906, Accuracy:0.7729, Validation Loss:0.5566, Validation Accuracy:0.7537\n",
    "Epoch #295: Loss:0.4803, Accuracy:0.7758, Validation Loss:0.5529, Validation Accuracy:0.7553\n",
    "Epoch #296: Loss:0.4795, Accuracy:0.7823, Validation Loss:0.5544, Validation Accuracy:0.7537\n",
    "Epoch #297: Loss:0.4816, Accuracy:0.7762, Validation Loss:0.5533, Validation Accuracy:0.7635\n",
    "Epoch #298: Loss:0.4793, Accuracy:0.7828, Validation Loss:0.5805, Validation Accuracy:0.7422\n",
    "Epoch #299: Loss:0.4881, Accuracy:0.7774, Validation Loss:0.5550, Validation Accuracy:0.7619\n",
    "Epoch #300: Loss:0.4950, Accuracy:0.7688, Validation Loss:0.5541, Validation Accuracy:0.7488\n",
    "\n",
    "Test:\n",
    "Test Loss:0.55409038, Accuracy:0.7488\n",
    "Labels: ['05', '01', '02', '03', '04']\n",
    "Confusion Matrix:\n",
    "       05  01  02  03  04\n",
    "t:05  140   2   0   0   0\n",
    "t:01    2  93  30   1   0\n",
    "t:02    1  33  75   4   1\n",
    "t:03    0   7  10  78  20\n",
    "t:04    0   3   1  38  70\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.98      0.99      0.98       142\n",
    "          01       0.67      0.74      0.70       126\n",
    "          02       0.65      0.66      0.65       114\n",
    "          03       0.64      0.68      0.66       115\n",
    "          04       0.77      0.62      0.69       112\n",
    "\n",
    "    accuracy                           0.75       609\n",
    "   macro avg       0.74      0.74      0.74       609\n",
    "weighted avg       0.75      0.75      0.75       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 08:11:31 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 43 minutes, 10 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6118672181820046, 1.6074106967312165, 1.6058712101530754, 1.6056404744071522, 1.605626866148023, 1.6054085612492803, 1.6052231774933037, 1.605110954572806, 1.6049883017203295, 1.6048134902036444, 1.604606523889626, 1.604288028574538, 1.603864731655528, 1.6032470360961062, 1.602300553290519, 1.600852472245791, 1.5981624774353453, 1.5930377955507176, 1.5830734913376556, 1.5633582683228116, 1.5254634342757352, 1.4728730548974525, 1.386174372460063, 1.3089043657572204, 1.2334166217124325, 1.1766020038053515, 1.1298884756263645, 1.099967896448959, 1.0778023965644523, 1.055967341307153, 1.0490097310546975, 1.0863928704817698, 1.0212987861022573, 1.0156455546960064, 1.0039523855610237, 1.00345361017437, 0.9924645964148009, 0.9794401257300416, 0.9722101267531196, 0.9638837820594925, 0.9760760851877271, 0.9495203459791361, 0.9512411308993259, 0.9295171361251418, 0.922083488826094, 0.9239119863862475, 0.9097849622931582, 0.9001972971878616, 0.8922360645139159, 0.8904820744235723, 0.883669815333606, 0.8798994713620404, 0.8859669171726371, 0.8647195454105759, 0.8912336070745058, 0.8760183571007452, 0.835950040758537, 0.8312046987865555, 0.8252615690818561, 0.8160643928939681, 0.8098747281018149, 0.7954640220147244, 0.7863930659536853, 0.777767046429645, 0.7700306020542514, 0.7564177253759162, 0.75589609469099, 0.7354770482858805, 0.7278376475147817, 0.739098827435661, 0.7478889496064147, 0.7190281766388804, 0.6998788537258781, 0.7062736851436947, 0.7090540622255485, 0.6889167030652364, 0.7039041545590744, 0.682180097537675, 0.6856304107628433, 0.67107647216966, 0.667459883321878, 0.6814053135943922, 0.6602029167176859, 0.6996579576399917, 0.6598056956073529, 0.6513564660044139, 0.653222104598736, 0.6542663823794849, 0.655834007145736, 0.6469190295889655, 0.6879117857842219, 0.6447543677046577, 0.6356834226053923, 0.6478129779959743, 0.6423259022200636, 0.6275602218944255, 0.6309196827642631, 0.6251291506396138, 0.6509476052520702, 0.6241268903163854, 0.619973034694277, 0.618211801514054, 0.6394585767403025, 0.6699645250105897, 0.6150964701116965, 0.6496533528337338, 0.617598319288545, 0.6113604813177989, 0.6093265099086981, 0.61921495150267, 0.6132148253702373, 0.6393398934201459, 0.6317642061972657, 0.6028454513189632, 0.6433943182181059, 0.648902089627114, 0.6050140661950574, 0.6170267034046756, 0.6547327388096326, 0.6406725424851103, 0.6351985934332673, 0.618789543659229, 0.6083617017578413, 0.6007061376555995, 0.6095056563175371, 0.6053824320998293, 0.5911067284778225, 0.601254003975779, 0.6494880669026931, 0.6230567175962264, 0.5995689209654609, 0.5870565565544592, 0.5939857314764376, 0.5889790506394235, 0.6135973124864262, 0.5845850034691822, 0.5896425492853562, 0.6242385415608073, 0.6016286049766102, 0.5822564354671046, 0.5859073985777856, 0.6026851010244273, 0.5782878650428822, 0.5815256342707792, 0.5841154551075401, 0.5790877970568652, 0.5827902200186781, 0.5935122137782217, 0.5790032683726406, 0.5760658124984779, 0.5725652550045884, 0.5909209253361268, 0.5722884187361681, 0.5746058469335434, 0.5750069103413223, 0.5718374753428993, 0.5717358221169959, 0.5774083056277634, 0.5797059920621036, 0.5731396730897462, 0.5699543918686352, 0.5778708789736179, 0.573253654983439, 0.586321828987798, 0.575548899976295, 0.5658659076651524, 0.583688960678276, 0.5709539212421048, 0.5751695275698193, 0.5686418035347474, 0.5786296067566707, 0.5744480741239338, 0.566840688093934, 0.565683848067066, 0.5687647358145815, 0.5646015572038973, 0.5729984522648829, 0.5781190986508024, 0.5668194056144489, 0.5775379202831751, 0.5780980387344736, 0.5685425848013466, 0.5714743900768863, 0.5628390959135221, 0.5626117445173717, 0.5668209130540857, 0.565847415935817, 0.5638595791090102, 0.5653195018055795, 0.5626266538020229, 0.5759618840194101, 0.5727213699437911, 0.6220407438004154, 0.5622206399789194, 0.6281952840354055, 0.582699974084331, 0.5650827159435291, 0.5872126143358415, 0.5670105434207885, 0.574444791464187, 0.5896413606180151, 0.5797505474638665, 0.6466624086909302, 0.5779176315259072, 0.5729251694796708, 0.5836129795350073, 0.5622329008207336, 0.5623088067975538, 0.5641001975790816, 0.5657430368495496, 0.5660818328802613, 0.5679205180388953, 0.5624014148962713, 0.5635071847043406, 0.56018433537585, 0.5601935844703261, 0.5618652369588467, 0.5688643639506573, 0.5979540688650948, 0.5574795220090055, 0.5661644264199268, 0.5587211965731603, 0.5617192564730965, 0.5593380578633013, 0.5677515188069963, 0.5630685735022884, 0.5701800922454872, 0.5644667403059835, 0.5867554159782986, 0.6063141309960527, 0.5649684170392543, 0.5758023738665338, 0.5577469093263248, 0.5759186124175248, 0.5677200834739384, 0.5619507452537273, 0.5850459540810296, 0.5625185704192113, 0.5661073021700816, 0.5683772470954995, 0.562363648943126, 0.5594468787190167, 0.5900112366049943, 0.5688586079428348, 0.5630350308660999, 0.7093747068312759, 0.5822331306382353, 0.5586584024437151, 0.560635931190403, 0.579196776760427, 0.5571988166259427, 0.566324463716673, 0.5568601550727055, 0.559895981415152, 0.55583803534312, 0.6031780622667084, 0.5633392617816017, 0.5569654383095615, 0.6662303723137954, 0.6017524963137747, 0.5594187554075996, 0.5554550936069395, 0.5630706471371142, 0.6076038108866398, 0.6378724276530136, 0.6099646891083428, 0.6115330777732022, 0.5797478147523939, 0.5711022152493545, 0.5669583503052911, 0.5603214610973603, 0.5578230719260981, 0.579438304176863, 0.5726140211955667, 0.5647100271849796, 0.5553722884658914, 0.5553360890676626, 0.561133876516314, 0.5677479785455664, 0.5759042192170969, 0.5548994079207747, 0.5824433458458222, 0.5547061383430594, 0.5634403577187574, 0.5936690594370925, 0.6039506085400511, 0.5956928047836316, 0.5744910446685327, 0.5823820985009518, 0.5691978314827228, 0.5539456063694946, 0.5611915323143131, 0.5929337125302144, 0.5566357631597222, 0.5528807718374068, 0.5544263485617238, 0.5533354668977422, 0.5805307852810827, 0.5549644524436474, 0.554090429898749], 'val_acc': [0.18390804556105134, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.2364532019459751, 0.2545155993187173, 0.31691297027473575, 0.31362889802514626, 0.40065681322650565, 0.4351395725323062, 0.44991789765545886, 0.46798029512607403, 0.49917898188866616, 0.5106732345664834, 0.5188834107372365, 0.5090311984416886, 0.5090311984416886, 0.487684728819357, 0.5221674876357926, 0.5172413746124418, 0.5320196999313405, 0.5221674876357926, 0.5303776681618934, 0.5353037723766759, 0.5353037722788029, 0.5467980250544932, 0.5648604224272353, 0.5615763503733918, 0.5451559889296984, 0.5566502419990076, 0.5779967116213393, 0.5681444947746979, 0.5747126391760038, 0.5845648559247724, 0.5812807834794369, 0.5878489280764888, 0.5911330003260783, 0.6042692897159282, 0.5960591091898274, 0.6075533621612637, 0.5927750367444921, 0.5944170728692868, 0.6075533619655177, 0.605911325938596, 0.6009852174663387, 0.6124794705356479, 0.6239737235070841, 0.6239737234092111, 0.6239737235070841, 0.6288998318814684, 0.6403940847550316, 0.6453201932272887, 0.6469622298414484, 0.6469622294499565, 0.6650246272141906, 0.6502463021889109, 0.6584564827150117, 0.6699507354907018, 0.6765188800877538, 0.674876844256578, 0.6666666637304772, 0.6830870246848058, 0.6847290608096005, 0.6896551694776037, 0.6765188803813728, 0.6912972056023985, 0.6896551694776037, 0.6962233141725287, 0.6962233138789097, 0.6830870245869328, 0.7060755306276781, 0.6945812777541149, 0.701149422351167, 0.6995073860306262, 0.7044334946986294, 0.7093596029751407, 0.6929392412378284, 0.7093596028772676, 0.7175697835991144, 0.7011494225469129, 0.7077175669482189, 0.7027914583780887, 0.7110016390999354, 0.7142857114473978, 0.7077175666546, 0.7093596028772676, 0.7208538558487039, 0.7060755307255512, 0.7077175666546, 0.6830870252720437, 0.7110016390999354, 0.6880131336464279, 0.7159277473764466, 0.7175697836969873, 0.7224958920713715, 0.7126436750289842, 0.7192118195281632, 0.7027914590631995, 0.7044334951879943, 0.7257799642230881, 0.7060755303340592, 0.7093596022900298, 0.7208538556529579, 0.719211820213274, 0.6945812785370988, 0.7077175675354568, 0.7027914591610726, 0.7175697841863523, 0.7224958923649906, 0.7257799642230881, 0.7208538564359418, 0.725779964712453, 0.7274220004457558, 0.7110016387084435, 0.7159277465934628, 0.7159277468870817, 0.7093596024857758, 0.7290640365705505, 0.7339901450428078, 0.7356321811676025, 0.7208538565338147, 0.7257799641252152, 0.7307060727932183, 0.7093596035623785, 0.725779964810326, 0.7339901449449349, 0.7224958916798797, 0.7192118192345442, 0.7307060724995994, 0.7372742173902702, 0.7274220000542639, 0.7208538556529579, 0.7274220000542639, 0.7241379277068014, 0.7224958916798797, 0.7372742169987784, 0.7307060724995994, 0.7224958915820067, 0.7274220001521369, 0.7356321809718566, 0.7356321806782377, 0.7405582890526219, 0.7422003253731626, 0.7339901448470618, 0.7339901451406807, 0.7323481084286482, 0.7389162531235731, 0.738916253515065, 0.738916253221446, 0.7323481084286482, 0.7307060724017264, 0.7356321808739836, 0.7422003258625275, 0.7438423616937033, 0.7372742174881433, 0.7339901445534429, 0.7356321811676025, 0.7339901446513158, 0.7405582892483679, 0.7422003253731626, 0.7356321807761106, 0.7438423614979573, 0.7422003258625275, 0.7356321806782377, 0.7422003254710355, 0.7487684706553254, 0.7339901446513158, 0.7339901446513158, 0.738916253417192, 0.7323481085265211, 0.7471264337475468, 0.7405582893462408, 0.75205254241555, 0.7454843976227521, 0.7339901445534429, 0.7454843976227521, 0.738916253221446, 0.7405582893462408, 0.7241379279025474, 0.752052542317677, 0.712643675714095, 0.745484398307863, 0.7487684699702145, 0.7323481087222671, 0.7504105064865012, 0.7471264344326577, 0.7405582898356057, 0.7323481087222671, 0.7224958915820067, 0.7405582894441138, 0.7422003255689086, 0.7356321810697296, 0.7438423615958303, 0.7438423614979573, 0.7487684703617065, 0.7471264342369117, 0.7471264338454198, 0.7471264345305306, 0.7536945786382178, 0.752052542513423, 0.7536945785403447, 0.7504105062907552, 0.7471264340411657, 0.7438423616937033, 0.7339901444555699, 0.7504105062907552, 0.745484397720625, 0.7504105062907552, 0.7487684701659605, 0.7536945786382178, 0.738916253221446, 0.7438423616937033, 0.7422003255689086, 0.738916253221446, 0.7339901452385538, 0.7224958925607365, 0.7471264338454198, 0.7405582893462408, 0.758620686816856, 0.7422003260582735, 0.7471264338454198, 0.7405582892483679, 0.7471264343347847, 0.7504105065843741, 0.7454843978184981, 0.7422003255689086, 0.7422003257646546, 0.7422003254710355, 0.738916253221446, 0.752052542611296, 0.7520525427091689, 0.6814449892451219, 0.7405582900313517, 0.7504105061928823, 0.745484397916371, 0.7372742170966513, 0.7536945785403447, 0.7454843978184981, 0.758620687012602, 0.752052542611296, 0.7619047593600644, 0.7372742169987784, 0.7438423614979573, 0.7619047593600644, 0.6912972059938903, 0.7339901455321727, 0.7504105064865012, 0.7487684701659605, 0.7438423615958303, 0.7356321809718566, 0.7241379279025474, 0.7323481087222671, 0.7290640363748047, 0.7356321809718566, 0.7422003254710355, 0.7405582894441138, 0.7471264341390388, 0.7619047593600644, 0.7405582895419868, 0.7422003255689086, 0.7422003255689086, 0.7504105060950093, 0.7553366145672665, 0.7471264338454198, 0.7487684700680876, 0.7438423616937033, 0.758620687012602, 0.7422003259604004, 0.752052542317677, 0.7504105061928823, 0.7438423616937033, 0.7307060725974723, 0.7405582894441138, 0.7405582893462408, 0.7454843978184981, 0.7438423614979573, 0.7504105061928823, 0.7471264343347847, 0.732348109211632, 0.7536945786382178, 0.7553366145672665, 0.7536945784424718, 0.7635467955827322, 0.7422003259604004, 0.7619047592621915, 0.7487684700680876], 'loss': [1.6171993338351867, 1.6096490410074316, 1.6063738784750874, 1.60587038039182, 1.6058236515742308, 1.6058113267534322, 1.6055841195754692, 1.60542473460125, 1.6052385242567904, 1.60514469254433, 1.6049695694960608, 1.6046613512097931, 1.6043604588361735, 1.603897301078577, 1.60312364458793, 1.6018587725852793, 1.6001042692568268, 1.5966628397269904, 1.5898371048286954, 1.5765022107952673, 1.5499513630504727, 1.5025408649836234, 1.4362817477396626, 1.34777613635915, 1.2723607870098013, 1.2052031560110605, 1.1575373025400684, 1.125171217487578, 1.089278433846742, 1.0796427862355351, 1.0782297187272527, 1.0571278121192353, 1.0468914740873803, 1.02205495584672, 1.0041074136688968, 0.9974527346769643, 0.9941161165002436, 0.9789591967692365, 0.9705017300112292, 0.9622108417369991, 0.9519809330513345, 0.9443575048348742, 0.9269073755099788, 0.9281587741213413, 0.9121648945113228, 0.8985773506595369, 0.8953387533369984, 0.8821945690766008, 0.8810778462421722, 0.8807334617667619, 0.8668548988610567, 0.8505955468213044, 0.8492445048610288, 0.8516178815761386, 0.8431784636186134, 0.8532601781694307, 0.8340466177928619, 0.8223155318344398, 0.8142120032829425, 0.8078452208938051, 0.7882215286671993, 0.7796595686270226, 0.7746751443806126, 0.7657910609636953, 0.7521117579031285, 0.7490216241969709, 0.745801556526513, 0.7258039212079997, 0.7157387061530315, 0.7125000712318342, 0.7298139211823074, 0.7028894305718753, 0.6899418608119111, 0.6939413243005897, 0.6805883373812729, 0.6741583761982849, 0.6647659614100838, 0.6853468486170994, 0.6698316739569944, 0.6495653318918216, 0.6446471019447217, 0.6467187815623117, 0.6514272026947147, 0.6369051166138855, 0.6464861585129458, 0.633575903683962, 0.6313625411820852, 0.6264081337368709, 0.6276091987102673, 0.6335961108090207, 0.6379087444693156, 0.6302731981022892, 0.6144236061362516, 0.6095320939038569, 0.6181230531580884, 0.6085823798571279, 0.6009299417785551, 0.6002524320839366, 0.6004753502242619, 0.6066859709163955, 0.5965476579734676, 0.5933739123892735, 0.592815524939394, 0.6095924248685582, 0.6061424515820137, 0.5958456330720404, 0.5999858653031336, 0.5978847779777261, 0.5881054938451465, 0.5958752163137009, 0.6016032151128232, 0.5912333839727868, 0.6162973477365544, 0.6078351679768651, 0.5886571540724815, 0.5797298259803647, 0.6135730746100816, 0.6221961417971695, 0.6242145880781405, 0.6154344104398692, 0.5881406905959519, 0.5967446078265227, 0.5787119197649633, 0.5621062948963236, 0.5616489286540225, 0.5656726977418826, 0.565887021381997, 0.5635263442503599, 0.5657277732169603, 0.5904002443720917, 0.5658766400153142, 0.5624689049299737, 0.5734066479259937, 0.5648854446851742, 0.5573763784931426, 0.5556509341547377, 0.5521906874752632, 0.5487447008459965, 0.5707741151844941, 0.5610490783284088, 0.5530117238081946, 0.557668256882035, 0.5535013086741954, 0.548665121864734, 0.540439993669365, 0.5425573873079288, 0.5413555825760232, 0.5407189508972716, 0.5428842064291545, 0.5346657040917163, 0.53863478225849, 0.5409308129757092, 0.5377392926255291, 0.5334702342687446, 0.5381226619900619, 0.531438482248318, 0.5324599148556437, 0.5311302471699411, 0.5300117638933585, 0.5292223703934672, 0.5290774399250195, 0.53579391407526, 0.5319086109588278, 0.5357749131670723, 0.5275583723487306, 0.529586325803087, 0.5215961524593267, 0.5314133477651608, 0.5252613688151695, 0.5300832616475084, 0.5231101748879685, 0.5225342376031425, 0.5253308791889058, 0.5231519905203912, 0.5247290409321168, 0.5273259551495743, 0.5230860840490956, 0.5236670755018199, 0.5199356340529737, 0.5192272062419132, 0.5302031248012362, 0.5270290363251061, 0.5241509172216333, 0.5178444718433357, 0.5158462651211623, 0.5171526041363789, 0.5200324243343831, 0.5252127669430366, 0.5166031327335742, 0.5136879672015228, 0.513043594311395, 0.5136868160485731, 0.5118633527163362, 0.5503987390892217, 0.528752682537024, 0.5353410580809356, 0.5231906606553761, 0.5176886381065087, 0.5188876123643754, 0.5129485384272354, 0.5482803488903711, 0.5307736782322674, 0.5232271483546654, 0.5277585473148729, 0.5129656712866907, 0.5084145714125349, 0.51078524841665, 0.5023674370327035, 0.5047864342494667, 0.5107385647737515, 0.515393515215762, 0.5017458903471302, 0.50243213769102, 0.5047794985942528, 0.5003508961665801, 0.500922858935362, 0.49956410599440276, 0.5004840983747212, 0.508726538427067, 0.5113747774453133, 0.5019145441985473, 0.5027007880886477, 0.4956304666687576, 0.5007730718143667, 0.5005739630125386, 0.5010140418761565, 0.5008172843000972, 0.5005921605798498, 0.5064491564733047, 0.5142192321146782, 0.5342560796032696, 0.5165799536621791, 0.5085323039755929, 0.5153910782914877, 0.5016139868470922, 0.5021309830080067, 0.526131851447926, 0.5084079210028756, 0.5015435432261754, 0.5039383467707546, 0.5036345353851084, 0.5024937902877463, 0.5164803943962042, 0.5178500913741407, 0.5008673430468268, 0.5245240239881637, 0.547687740308793, 0.5263096763612798, 0.510370948182484, 0.5049769098509019, 0.5080215785048091, 0.4926424768915901, 0.505046277357078, 0.4917164322042367, 0.4891830013517971, 0.4952707506547963, 0.5237702087944782, 0.5067262152137209, 0.5173041899346228, 0.5383022672341834, 0.5171169137318277, 0.4921703926102092, 0.4963381309773643, 0.49695380348444473, 0.5106434090426325, 0.5191599295858974, 0.5053182783435257, 0.5092212001645834, 0.5034546842320499, 0.508756103946443, 0.49734303565730303, 0.49309855711778333, 0.48734232900813373, 0.49545749197016015, 0.49991505310031176, 0.4912239984809985, 0.48625880824956563, 0.48477349966703254, 0.48777419347048295, 0.49538300676267494, 0.4934216022736238, 0.4882733760183597, 0.507862878751461, 0.4922299719321899, 0.5090091579381445, 0.5135981370291426, 0.5152851102655673, 0.4981923341506316, 0.49348911876795964, 0.49031129789058675, 0.48098309287300345, 0.4798131845080632, 0.48390855559088614, 0.4905689834569269, 0.480292290412425, 0.479468674875138, 0.4815912876775377, 0.4793097673989909, 0.48807577112616946, 0.4949500439103379], 'acc': [0.1823408615111815, 0.22258727002192816, 0.2328542098861945, 0.2328542106694999, 0.23285421010037957, 0.2328542098861945, 0.2328542083195837, 0.23285421047367355, 0.23285420851541005, 0.23285421031456463, 0.2328542087112364, 0.23285420970872686, 0.23285420970872686, 0.2328542087112364, 0.23285420730373452, 0.2328542094945418, 0.23285420890706277, 0.233264887895917, 0.23819301911692844, 0.24763860354922881, 0.2907597526265366, 0.3466119074723559, 0.3753593432462681, 0.4123203267184616, 0.4262833669077934, 0.4492813158696192, 0.46324435151822757, 0.4833675562111504, 0.4960985650761661, 0.49609856155129184, 0.5112936328323959, 0.49568788514489753, 0.5092402468961367, 0.5199178639868202, 0.5211499001700776, 0.5260780294328254, 0.5211498980159877, 0.5334702211973359, 0.5429158145152568, 0.5445585240084043, 0.5457905566667874, 0.5560574975101855, 0.561396300205215, 0.5515400425364594, 0.5618069843589892, 0.5696098608647528, 0.5696098596897947, 0.5765913724654509, 0.5708418854208208, 0.5786447619265844, 0.581108828026656, 0.5868583156587651, 0.5843942472087774, 0.5827515353657137, 0.5905544108923456, 0.5876796752765193, 0.5971252537605943, 0.5979466121299556, 0.6086242278498546, 0.5995893245604983, 0.6078028708512778, 0.6209445595251706, 0.6156057451295167, 0.6254620131281122, 0.6324435304812093, 0.6336755633354187, 0.6402464079661047, 0.6459958928566449, 0.650924028385836, 0.656262834287522, 0.6476386062663194, 0.6698151988170475, 0.6694045211989776, 0.6694045208073249, 0.67268993979362, 0.674332650657552, 0.6821355283382737, 0.6702258766309437, 0.6817248501327249, 0.682135527946621, 0.6944558476030949, 0.692813137914121, 0.6870636560344109, 0.7006160207597627, 0.6895277246802249, 0.6977412730516594, 0.6924024685941927, 0.7059548218881815, 0.6989733015976892, 0.6915811108123106, 0.6924024612751829, 0.6928131404598635, 0.7055441491657704, 0.7059548230631395, 0.7043121112200759, 0.7117043168393004, 0.7154004090620507, 0.7141683773827994, 0.7190965052747629, 0.7158110855051624, 0.7145790534342583, 0.7219712513428205, 0.718685830985741, 0.7043121118075549, 0.7137577003522085, 0.7203285383247986, 0.7071868568964808, 0.7190965064497209, 0.7260780296776084, 0.7071868614739216, 0.7158110860926414, 0.7190965082121581, 0.7071868574839598, 0.7100616042129313, 0.7264887057290674, 0.7219712511469941, 0.711704315468516, 0.705544144465938, 0.6989733033601263, 0.7149897339897234, 0.7227926069706128, 0.7207392175094792, 0.7195071834803117, 0.7392197109101, 0.7392197099309682, 0.7326488688251566, 0.7326488678460249, 0.7396303904864333, 0.7351129353168808, 0.7190965050789365, 0.7351129335544437, 0.7412730980213174, 0.7297741239320571, 0.7396303871573854, 0.7416837746602554, 0.7462012331588557, 0.7449691977589037, 0.7486652986714483, 0.7367556434392439, 0.734702257111332, 0.7412730982171437, 0.7437371645130416, 0.7441478421311114, 0.7519507213784439, 0.7535934293049806, 0.7507186861743183, 0.7519507200076595, 0.7535934304799388, 0.7466119103852729, 0.758521559938513, 0.755236140364739, 0.7511293630090827, 0.7527720728938829, 0.7630390149122391, 0.7540041065313977, 0.7568788520119764, 0.7552361382106492, 0.7622176583053151, 0.7572895288467407, 0.759753594163507, 0.756468171848164, 0.7581108856494911, 0.7535934291091543, 0.7531827512952581, 0.7601642723690558, 0.7511293630090827, 0.7630390153038917, 0.7568788516203236, 0.7671457926595481, 0.7605749499871256, 0.7564681720439903, 0.7613963034608281, 0.7630390149122391, 0.7581108854536648, 0.7618069836246404, 0.7564681743939065, 0.7585215611134711, 0.7613963034608281, 0.7597535920094171, 0.7634496935094407, 0.7585215601343394, 0.7646817261678237, 0.7634496917470035, 0.767556468711007, 0.7630390158913708, 0.7622176596760994, 0.765913760001165, 0.7597535947509859, 0.7638603721066421, 0.765092405156678, 0.7655030810123107, 0.7667351162164363, 0.7646817287135663, 0.7527720713272721, 0.7601642725648822, 0.7515400421937634, 0.7560574961883576, 0.7605749505746047, 0.7630390160871972, 0.767556472040055, 0.7466119066645722, 0.751129362029951, 0.7634496929219616, 0.7577002070522896, 0.7605749495954729, 0.7696098575846615, 0.769199179574939, 0.77125256649033, 0.7696098591512723, 0.7696098577804878, 0.7655030802290053, 0.7675564696901388, 0.7700205361818631, 0.7650924035900672, 0.7663244368359293, 0.7696098571930088, 0.7622176573261833, 0.7691991801624181, 0.769609855626398, 0.7704312159540228, 0.7667351138665202, 0.7646817294968716, 0.7720739240763859, 0.7683778247304521, 0.7708418902430446, 0.7659137613719493, 0.770431210470885, 0.7700205348110787, 0.7679671433916816, 0.7638603713233368, 0.7527720732855356, 0.7700205379443003, 0.7650924024151091, 0.7650924049608516, 0.7724846040443718, 0.7691991793791126, 0.7556468181786351, 0.7622176610468839, 0.7700205336361206, 0.765503084537185, 0.7696098577804878, 0.7691991770291965, 0.7544147859119047, 0.7609856270177163, 0.7659137570637697, 0.754825464900759, 0.7494866554741987, 0.7597535918135907, 0.7716632466541423, 0.7696098558222244, 0.7679671463290769, 0.7696098603262304, 0.768788504110959, 0.7786447653780239, 0.7770020558848763, 0.7745379919388945, 0.7556468168078507, 0.7679671453499451, 0.7638603707358578, 0.7609856297592852, 0.7646817244053866, 0.7770020560807026, 0.7724846007153239, 0.7761807018236947, 0.7728952797041782, 0.758110883887054, 0.7700205361818631, 0.769609858955446, 0.7712525694277252, 0.7679671445666397, 0.7728952791166991, 0.7708418906346973, 0.7733059577139006, 0.7782340887390857, 0.7687885025443483, 0.7757700228348405, 0.7753593450209443, 0.776591375721064, 0.7696098583679669, 0.7733059553639845, 0.7712525672736354, 0.7798767978405805, 0.7671457916804163, 0.776180702019521, 0.7683778251221047, 0.7597535945551596, 0.7671457865154964, 0.7679671469165559, 0.7745379882181939, 0.7749486658362638, 0.7749486656404374, 0.7794661180684209, 0.7765913782668065, 0.7728952787250464, 0.7757700200932717, 0.7823408633531731, 0.7761807000612576, 0.7827515356104966, 0.7774127352653832, 0.7687885019568692]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
