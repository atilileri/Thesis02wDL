{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf19.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 04:01:42 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '2', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ib', 'eg', 'eo', 'ce', 'my', 'aa', 'ds', 'sg', 'yd', 'ek', 'mb', 'ck', 'by', 'eb', 'sk'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000219250ED240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002191F8D6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7125, Accuracy:0.0448, Validation Loss:2.7075, Validation Accuracy:0.0443\n",
    "Epoch #2: Loss:2.7050, Accuracy:0.0517, Validation Loss:2.7010, Validation Accuracy:0.1018\n",
    "Epoch #3: Loss:2.6992, Accuracy:0.1023, Validation Loss:2.6961, Validation Accuracy:0.1018\n",
    "Epoch #4: Loss:2.6945, Accuracy:0.1023, Validation Loss:2.6915, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6899, Accuracy:0.1023, Validation Loss:2.6871, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6858, Accuracy:0.1023, Validation Loss:2.6828, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6816, Accuracy:0.1023, Validation Loss:2.6789, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6780, Accuracy:0.1023, Validation Loss:2.6752, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6741, Accuracy:0.1023, Validation Loss:2.6720, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6711, Accuracy:0.1023, Validation Loss:2.6693, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6690, Accuracy:0.1023, Validation Loss:2.6671, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6668, Accuracy:0.1023, Validation Loss:2.6654, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6653, Accuracy:0.1023, Validation Loss:2.6641, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6639, Accuracy:0.1023, Validation Loss:2.6630, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6631, Accuracy:0.1023, Validation Loss:2.6620, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6620, Accuracy:0.1023, Validation Loss:2.6612, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6612, Accuracy:0.1023, Validation Loss:2.6604, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6605, Accuracy:0.1023, Validation Loss:2.6596, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6598, Accuracy:0.1023, Validation Loss:2.6588, Validation Accuracy:0.1018\n",
    "Epoch #20: Loss:2.6586, Accuracy:0.1023, Validation Loss:2.6579, Validation Accuracy:0.1018\n",
    "Epoch #21: Loss:2.6578, Accuracy:0.1023, Validation Loss:2.6568, Validation Accuracy:0.1018\n",
    "Epoch #22: Loss:2.6561, Accuracy:0.1023, Validation Loss:2.6550, Validation Accuracy:0.1018\n",
    "Epoch #23: Loss:2.6539, Accuracy:0.1023, Validation Loss:2.6521, Validation Accuracy:0.1034\n",
    "Epoch #24: Loss:2.6500, Accuracy:0.1043, Validation Loss:2.6472, Validation Accuracy:0.1034\n",
    "Epoch #25: Loss:2.6432, Accuracy:0.1105, Validation Loss:2.6384, Validation Accuracy:0.1117\n",
    "Epoch #26: Loss:2.6307, Accuracy:0.1261, Validation Loss:2.6236, Validation Accuracy:0.1264\n",
    "Epoch #27: Loss:2.6116, Accuracy:0.1322, Validation Loss:2.6034, Validation Accuracy:0.1314\n",
    "Epoch #28: Loss:2.5883, Accuracy:0.1363, Validation Loss:2.5895, Validation Accuracy:0.1346\n",
    "Epoch #29: Loss:2.5658, Accuracy:0.1363, Validation Loss:2.5762, Validation Accuracy:0.1215\n",
    "Epoch #30: Loss:2.5521, Accuracy:0.1355, Validation Loss:2.5589, Validation Accuracy:0.1429\n",
    "Epoch #31: Loss:2.5296, Accuracy:0.1552, Validation Loss:2.5444, Validation Accuracy:0.1527\n",
    "Epoch #32: Loss:2.5155, Accuracy:0.1556, Validation Loss:2.5328, Validation Accuracy:0.1494\n",
    "Epoch #33: Loss:2.4985, Accuracy:0.1437, Validation Loss:2.5178, Validation Accuracy:0.1494\n",
    "Epoch #34: Loss:2.4845, Accuracy:0.1704, Validation Loss:2.5061, Validation Accuracy:0.1790\n",
    "Epoch #35: Loss:2.4720, Accuracy:0.1848, Validation Loss:2.4979, Validation Accuracy:0.1642\n",
    "Epoch #36: Loss:2.4592, Accuracy:0.1737, Validation Loss:2.4810, Validation Accuracy:0.1839\n",
    "Epoch #37: Loss:2.4440, Accuracy:0.1823, Validation Loss:2.4729, Validation Accuracy:0.1888\n",
    "Epoch #38: Loss:2.4282, Accuracy:0.1844, Validation Loss:2.4501, Validation Accuracy:0.1856\n",
    "Epoch #39: Loss:2.4181, Accuracy:0.1926, Validation Loss:2.4494, Validation Accuracy:0.1576\n",
    "Epoch #40: Loss:2.4050, Accuracy:0.1795, Validation Loss:2.4310, Validation Accuracy:0.1954\n",
    "Epoch #41: Loss:2.3886, Accuracy:0.2197, Validation Loss:2.4349, Validation Accuracy:0.1494\n",
    "Epoch #42: Loss:2.3912, Accuracy:0.1815, Validation Loss:2.4243, Validation Accuracy:0.2266\n",
    "Epoch #43: Loss:2.3746, Accuracy:0.2140, Validation Loss:2.4000, Validation Accuracy:0.1576\n",
    "Epoch #44: Loss:2.3512, Accuracy:0.2099, Validation Loss:2.3799, Validation Accuracy:0.2167\n",
    "Epoch #45: Loss:2.3381, Accuracy:0.2144, Validation Loss:2.3697, Validation Accuracy:0.1806\n",
    "Epoch #46: Loss:2.3225, Accuracy:0.2283, Validation Loss:2.3506, Validation Accuracy:0.1905\n",
    "Epoch #47: Loss:2.3092, Accuracy:0.2131, Validation Loss:2.3432, Validation Accuracy:0.1905\n",
    "Epoch #48: Loss:2.2965, Accuracy:0.2168, Validation Loss:2.3293, Validation Accuracy:0.1938\n",
    "Epoch #49: Loss:2.2850, Accuracy:0.2398, Validation Loss:2.3234, Validation Accuracy:0.1970\n",
    "Epoch #50: Loss:2.2754, Accuracy:0.2337, Validation Loss:2.3133, Validation Accuracy:0.1823\n",
    "Epoch #51: Loss:2.2704, Accuracy:0.2238, Validation Loss:2.3141, Validation Accuracy:0.2069\n",
    "Epoch #52: Loss:2.2550, Accuracy:0.2382, Validation Loss:2.2958, Validation Accuracy:0.2085\n",
    "Epoch #53: Loss:2.2472, Accuracy:0.2378, Validation Loss:2.2901, Validation Accuracy:0.2200\n",
    "Epoch #54: Loss:2.2369, Accuracy:0.2480, Validation Loss:2.2803, Validation Accuracy:0.2167\n",
    "Epoch #55: Loss:2.2294, Accuracy:0.2563, Validation Loss:2.2758, Validation Accuracy:0.2118\n",
    "Epoch #56: Loss:2.2227, Accuracy:0.2538, Validation Loss:2.2842, Validation Accuracy:0.2217\n",
    "Epoch #57: Loss:2.2209, Accuracy:0.2567, Validation Loss:2.2676, Validation Accuracy:0.2315\n",
    "Epoch #58: Loss:2.2160, Accuracy:0.2563, Validation Loss:2.2625, Validation Accuracy:0.2299\n",
    "Epoch #59: Loss:2.1989, Accuracy:0.2645, Validation Loss:2.2507, Validation Accuracy:0.2217\n",
    "Epoch #60: Loss:2.2017, Accuracy:0.2616, Validation Loss:2.2555, Validation Accuracy:0.2282\n",
    "Epoch #61: Loss:2.1932, Accuracy:0.2669, Validation Loss:2.2428, Validation Accuracy:0.2299\n",
    "Epoch #62: Loss:2.1820, Accuracy:0.2591, Validation Loss:2.2264, Validation Accuracy:0.2447\n",
    "Epoch #63: Loss:2.1690, Accuracy:0.2710, Validation Loss:2.2282, Validation Accuracy:0.2397\n",
    "Epoch #64: Loss:2.1629, Accuracy:0.2698, Validation Loss:2.2162, Validation Accuracy:0.2397\n",
    "Epoch #65: Loss:2.1563, Accuracy:0.2674, Validation Loss:2.2155, Validation Accuracy:0.2447\n",
    "Epoch #66: Loss:2.1539, Accuracy:0.2727, Validation Loss:2.2123, Validation Accuracy:0.2365\n",
    "Epoch #67: Loss:2.1507, Accuracy:0.2723, Validation Loss:2.2143, Validation Accuracy:0.2397\n",
    "Epoch #68: Loss:2.1471, Accuracy:0.2727, Validation Loss:2.2090, Validation Accuracy:0.2365\n",
    "Epoch #69: Loss:2.1360, Accuracy:0.2694, Validation Loss:2.1955, Validation Accuracy:0.2447\n",
    "Epoch #70: Loss:2.1277, Accuracy:0.2776, Validation Loss:2.1969, Validation Accuracy:0.2414\n",
    "Epoch #71: Loss:2.1314, Accuracy:0.2715, Validation Loss:2.1911, Validation Accuracy:0.2447\n",
    "Epoch #72: Loss:2.1324, Accuracy:0.2682, Validation Loss:2.1790, Validation Accuracy:0.2545\n",
    "Epoch #73: Loss:2.1184, Accuracy:0.2747, Validation Loss:2.1846, Validation Accuracy:0.2545\n",
    "Epoch #74: Loss:2.1157, Accuracy:0.2776, Validation Loss:2.1742, Validation Accuracy:0.2644\n",
    "Epoch #75: Loss:2.1088, Accuracy:0.2772, Validation Loss:2.1731, Validation Accuracy:0.2545\n",
    "Epoch #76: Loss:2.1089, Accuracy:0.2760, Validation Loss:2.1664, Validation Accuracy:0.2529\n",
    "Epoch #77: Loss:2.0967, Accuracy:0.2821, Validation Loss:2.1647, Validation Accuracy:0.2479\n",
    "Epoch #78: Loss:2.0898, Accuracy:0.2879, Validation Loss:2.1538, Validation Accuracy:0.2562\n",
    "Epoch #79: Loss:2.0888, Accuracy:0.2838, Validation Loss:2.1536, Validation Accuracy:0.2627\n",
    "Epoch #80: Loss:2.0839, Accuracy:0.2834, Validation Loss:2.1556, Validation Accuracy:0.2529\n",
    "Epoch #81: Loss:2.0785, Accuracy:0.2858, Validation Loss:2.1531, Validation Accuracy:0.2512\n",
    "Epoch #82: Loss:2.0809, Accuracy:0.2871, Validation Loss:2.1565, Validation Accuracy:0.2562\n",
    "Epoch #83: Loss:2.0722, Accuracy:0.2854, Validation Loss:2.1338, Validation Accuracy:0.2693\n",
    "Epoch #84: Loss:2.0658, Accuracy:0.2916, Validation Loss:2.1395, Validation Accuracy:0.2660\n",
    "Epoch #85: Loss:2.0608, Accuracy:0.2903, Validation Loss:2.1291, Validation Accuracy:0.2644\n",
    "Epoch #86: Loss:2.0567, Accuracy:0.2879, Validation Loss:2.1380, Validation Accuracy:0.2660\n",
    "Epoch #87: Loss:2.0612, Accuracy:0.2867, Validation Loss:2.1222, Validation Accuracy:0.2545\n",
    "Epoch #88: Loss:2.0516, Accuracy:0.2945, Validation Loss:2.1480, Validation Accuracy:0.2479\n",
    "Epoch #89: Loss:2.0517, Accuracy:0.2969, Validation Loss:2.1459, Validation Accuracy:0.2496\n",
    "Epoch #90: Loss:2.0588, Accuracy:0.2903, Validation Loss:2.1141, Validation Accuracy:0.2594\n",
    "Epoch #91: Loss:2.0403, Accuracy:0.2949, Validation Loss:2.1048, Validation Accuracy:0.2726\n",
    "Epoch #92: Loss:2.0347, Accuracy:0.2936, Validation Loss:2.1244, Validation Accuracy:0.2660\n",
    "Epoch #93: Loss:2.0376, Accuracy:0.2957, Validation Loss:2.1057, Validation Accuracy:0.2611\n",
    "Epoch #94: Loss:2.0372, Accuracy:0.3055, Validation Loss:2.1198, Validation Accuracy:0.2644\n",
    "Epoch #95: Loss:2.0330, Accuracy:0.2969, Validation Loss:2.1220, Validation Accuracy:0.2578\n",
    "Epoch #96: Loss:2.0252, Accuracy:0.3014, Validation Loss:2.0885, Validation Accuracy:0.2709\n",
    "Epoch #97: Loss:2.0195, Accuracy:0.2998, Validation Loss:2.0903, Validation Accuracy:0.2791\n",
    "Epoch #98: Loss:2.0159, Accuracy:0.3105, Validation Loss:2.0924, Validation Accuracy:0.2742\n",
    "Epoch #99: Loss:2.0127, Accuracy:0.3023, Validation Loss:2.0754, Validation Accuracy:0.2759\n",
    "Epoch #100: Loss:2.0033, Accuracy:0.3023, Validation Loss:2.0768, Validation Accuracy:0.2775\n",
    "Epoch #101: Loss:2.0005, Accuracy:0.3060, Validation Loss:2.0854, Validation Accuracy:0.2791\n",
    "Epoch #102: Loss:2.0090, Accuracy:0.3080, Validation Loss:2.1071, Validation Accuracy:0.2775\n",
    "Epoch #103: Loss:2.0007, Accuracy:0.3133, Validation Loss:2.0678, Validation Accuracy:0.2906\n",
    "Epoch #104: Loss:1.9905, Accuracy:0.3133, Validation Loss:2.0613, Validation Accuracy:0.2956\n",
    "Epoch #105: Loss:1.9890, Accuracy:0.3129, Validation Loss:2.0700, Validation Accuracy:0.2874\n",
    "Epoch #106: Loss:1.9889, Accuracy:0.3080, Validation Loss:2.0659, Validation Accuracy:0.2857\n",
    "Epoch #107: Loss:1.9862, Accuracy:0.3146, Validation Loss:2.0568, Validation Accuracy:0.3038\n",
    "Epoch #108: Loss:1.9790, Accuracy:0.3244, Validation Loss:2.0760, Validation Accuracy:0.2923\n",
    "Epoch #109: Loss:1.9807, Accuracy:0.3166, Validation Loss:2.0490, Validation Accuracy:0.3038\n",
    "Epoch #110: Loss:1.9674, Accuracy:0.3273, Validation Loss:2.0455, Validation Accuracy:0.3054\n",
    "Epoch #111: Loss:1.9637, Accuracy:0.3224, Validation Loss:2.0448, Validation Accuracy:0.3038\n",
    "Epoch #112: Loss:1.9616, Accuracy:0.3253, Validation Loss:2.0427, Validation Accuracy:0.3054\n",
    "Epoch #113: Loss:1.9690, Accuracy:0.3162, Validation Loss:2.0451, Validation Accuracy:0.3087\n",
    "Epoch #114: Loss:1.9673, Accuracy:0.3175, Validation Loss:2.0375, Validation Accuracy:0.3071\n",
    "Epoch #115: Loss:1.9584, Accuracy:0.3240, Validation Loss:2.0381, Validation Accuracy:0.3103\n",
    "Epoch #116: Loss:1.9652, Accuracy:0.3224, Validation Loss:2.0453, Validation Accuracy:0.3038\n",
    "Epoch #117: Loss:1.9573, Accuracy:0.3228, Validation Loss:2.0397, Validation Accuracy:0.3087\n",
    "Epoch #118: Loss:1.9514, Accuracy:0.3265, Validation Loss:2.0310, Validation Accuracy:0.3136\n",
    "Epoch #119: Loss:1.9461, Accuracy:0.3302, Validation Loss:2.0507, Validation Accuracy:0.3021\n",
    "Epoch #120: Loss:1.9554, Accuracy:0.3331, Validation Loss:2.0284, Validation Accuracy:0.3136\n",
    "Epoch #121: Loss:1.9514, Accuracy:0.3265, Validation Loss:2.0218, Validation Accuracy:0.3169\n",
    "Epoch #122: Loss:1.9403, Accuracy:0.3290, Validation Loss:2.0320, Validation Accuracy:0.3120\n",
    "Epoch #123: Loss:1.9412, Accuracy:0.3318, Validation Loss:2.0230, Validation Accuracy:0.3103\n",
    "Epoch #124: Loss:1.9358, Accuracy:0.3335, Validation Loss:2.0195, Validation Accuracy:0.3169\n",
    "Epoch #125: Loss:1.9302, Accuracy:0.3326, Validation Loss:2.0148, Validation Accuracy:0.3202\n",
    "Epoch #126: Loss:1.9300, Accuracy:0.3310, Validation Loss:2.0172, Validation Accuracy:0.3186\n",
    "Epoch #127: Loss:1.9341, Accuracy:0.3290, Validation Loss:2.0075, Validation Accuracy:0.3202\n",
    "Epoch #128: Loss:1.9339, Accuracy:0.3339, Validation Loss:2.0141, Validation Accuracy:0.3120\n",
    "Epoch #129: Loss:1.9364, Accuracy:0.3339, Validation Loss:2.0076, Validation Accuracy:0.3186\n",
    "Epoch #130: Loss:1.9261, Accuracy:0.3355, Validation Loss:2.0057, Validation Accuracy:0.3202\n",
    "Epoch #131: Loss:1.9231, Accuracy:0.3347, Validation Loss:2.0107, Validation Accuracy:0.3103\n",
    "Epoch #132: Loss:1.9213, Accuracy:0.3368, Validation Loss:2.0027, Validation Accuracy:0.3153\n",
    "Epoch #133: Loss:1.9226, Accuracy:0.3376, Validation Loss:2.0125, Validation Accuracy:0.3153\n",
    "Epoch #134: Loss:1.9181, Accuracy:0.3396, Validation Loss:1.9998, Validation Accuracy:0.3186\n",
    "Epoch #135: Loss:1.9115, Accuracy:0.3388, Validation Loss:2.0038, Validation Accuracy:0.3202\n",
    "Epoch #136: Loss:1.9202, Accuracy:0.3331, Validation Loss:2.0163, Validation Accuracy:0.3153\n",
    "Epoch #137: Loss:1.9189, Accuracy:0.3437, Validation Loss:1.9956, Validation Accuracy:0.3235\n",
    "Epoch #138: Loss:1.9064, Accuracy:0.3380, Validation Loss:1.9951, Validation Accuracy:0.3186\n",
    "Epoch #139: Loss:1.9070, Accuracy:0.3413, Validation Loss:2.0010, Validation Accuracy:0.3186\n",
    "Epoch #140: Loss:1.9072, Accuracy:0.3400, Validation Loss:1.9916, Validation Accuracy:0.3300\n",
    "Epoch #141: Loss:1.9025, Accuracy:0.3454, Validation Loss:1.9900, Validation Accuracy:0.3235\n",
    "Epoch #142: Loss:1.9015, Accuracy:0.3355, Validation Loss:2.0033, Validation Accuracy:0.3186\n",
    "Epoch #143: Loss:1.9088, Accuracy:0.3433, Validation Loss:2.0047, Validation Accuracy:0.3218\n",
    "Epoch #144: Loss:1.9103, Accuracy:0.3450, Validation Loss:1.9996, Validation Accuracy:0.3169\n",
    "Epoch #145: Loss:1.9107, Accuracy:0.3425, Validation Loss:2.0245, Validation Accuracy:0.3120\n",
    "Epoch #146: Loss:1.9125, Accuracy:0.3392, Validation Loss:2.0127, Validation Accuracy:0.3087\n",
    "Epoch #147: Loss:1.9100, Accuracy:0.3466, Validation Loss:1.9887, Validation Accuracy:0.3202\n",
    "Epoch #148: Loss:1.9074, Accuracy:0.3499, Validation Loss:1.9915, Validation Accuracy:0.3284\n",
    "Epoch #149: Loss:1.8997, Accuracy:0.3409, Validation Loss:1.9820, Validation Accuracy:0.3251\n",
    "Epoch #150: Loss:1.8875, Accuracy:0.3491, Validation Loss:1.9843, Validation Accuracy:0.3235\n",
    "Epoch #151: Loss:1.8897, Accuracy:0.3470, Validation Loss:1.9731, Validation Accuracy:0.3350\n",
    "Epoch #152: Loss:1.8869, Accuracy:0.3503, Validation Loss:1.9822, Validation Accuracy:0.3284\n",
    "Epoch #153: Loss:1.8890, Accuracy:0.3491, Validation Loss:1.9764, Validation Accuracy:0.3235\n",
    "Epoch #154: Loss:1.8817, Accuracy:0.3532, Validation Loss:1.9737, Validation Accuracy:0.3268\n",
    "Epoch #155: Loss:1.8811, Accuracy:0.3478, Validation Loss:1.9748, Validation Accuracy:0.3251\n",
    "Epoch #156: Loss:1.8843, Accuracy:0.3478, Validation Loss:1.9784, Validation Accuracy:0.3284\n",
    "Epoch #157: Loss:1.8870, Accuracy:0.3474, Validation Loss:1.9672, Validation Accuracy:0.3317\n",
    "Epoch #158: Loss:1.8792, Accuracy:0.3511, Validation Loss:1.9737, Validation Accuracy:0.3350\n",
    "Epoch #159: Loss:1.8765, Accuracy:0.3503, Validation Loss:1.9664, Validation Accuracy:0.3350\n",
    "Epoch #160: Loss:1.8733, Accuracy:0.3499, Validation Loss:1.9903, Validation Accuracy:0.3186\n",
    "Epoch #161: Loss:1.8787, Accuracy:0.3577, Validation Loss:1.9727, Validation Accuracy:0.3251\n",
    "Epoch #162: Loss:1.8741, Accuracy:0.3540, Validation Loss:1.9835, Validation Accuracy:0.3202\n",
    "Epoch #163: Loss:1.8767, Accuracy:0.3515, Validation Loss:1.9733, Validation Accuracy:0.3202\n",
    "Epoch #164: Loss:1.8731, Accuracy:0.3585, Validation Loss:1.9697, Validation Accuracy:0.3317\n",
    "Epoch #165: Loss:1.8655, Accuracy:0.3528, Validation Loss:1.9770, Validation Accuracy:0.3218\n",
    "Epoch #166: Loss:1.8731, Accuracy:0.3536, Validation Loss:1.9811, Validation Accuracy:0.3136\n",
    "Epoch #167: Loss:1.8694, Accuracy:0.3556, Validation Loss:1.9607, Validation Accuracy:0.3284\n",
    "Epoch #168: Loss:1.8614, Accuracy:0.3561, Validation Loss:1.9647, Validation Accuracy:0.3251\n",
    "Epoch #169: Loss:1.8645, Accuracy:0.3581, Validation Loss:1.9623, Validation Accuracy:0.3268\n",
    "Epoch #170: Loss:1.8615, Accuracy:0.3569, Validation Loss:1.9581, Validation Accuracy:0.3333\n",
    "Epoch #171: Loss:1.8557, Accuracy:0.3589, Validation Loss:1.9786, Validation Accuracy:0.3218\n",
    "Epoch #172: Loss:1.8624, Accuracy:0.3630, Validation Loss:1.9696, Validation Accuracy:0.3300\n",
    "Epoch #173: Loss:1.8581, Accuracy:0.3602, Validation Loss:1.9583, Validation Accuracy:0.3202\n",
    "Epoch #174: Loss:1.8626, Accuracy:0.3536, Validation Loss:1.9584, Validation Accuracy:0.3317\n",
    "Epoch #175: Loss:1.8553, Accuracy:0.3589, Validation Loss:1.9552, Validation Accuracy:0.3268\n",
    "Epoch #176: Loss:1.8538, Accuracy:0.3634, Validation Loss:1.9842, Validation Accuracy:0.3153\n",
    "Epoch #177: Loss:1.8597, Accuracy:0.3573, Validation Loss:1.9738, Validation Accuracy:0.3169\n",
    "Epoch #178: Loss:1.8684, Accuracy:0.3573, Validation Loss:1.9969, Validation Accuracy:0.3054\n",
    "Epoch #179: Loss:1.8765, Accuracy:0.3552, Validation Loss:1.9775, Validation Accuracy:0.3186\n",
    "Epoch #180: Loss:1.8661, Accuracy:0.3577, Validation Loss:1.9854, Validation Accuracy:0.3186\n",
    "Epoch #181: Loss:1.8599, Accuracy:0.3610, Validation Loss:1.9746, Validation Accuracy:0.3136\n",
    "Epoch #182: Loss:1.8546, Accuracy:0.3639, Validation Loss:1.9484, Validation Accuracy:0.3251\n",
    "Epoch #183: Loss:1.8459, Accuracy:0.3602, Validation Loss:1.9539, Validation Accuracy:0.3218\n",
    "Epoch #184: Loss:1.8434, Accuracy:0.3639, Validation Loss:1.9519, Validation Accuracy:0.3268\n",
    "Epoch #185: Loss:1.8488, Accuracy:0.3606, Validation Loss:1.9511, Validation Accuracy:0.3235\n",
    "Epoch #186: Loss:1.8465, Accuracy:0.3606, Validation Loss:1.9474, Validation Accuracy:0.3251\n",
    "Epoch #187: Loss:1.8484, Accuracy:0.3573, Validation Loss:1.9559, Validation Accuracy:0.3251\n",
    "Epoch #188: Loss:1.8482, Accuracy:0.3618, Validation Loss:1.9444, Validation Accuracy:0.3350\n",
    "Epoch #189: Loss:1.8394, Accuracy:0.3634, Validation Loss:1.9551, Validation Accuracy:0.3186\n",
    "Epoch #190: Loss:1.8440, Accuracy:0.3626, Validation Loss:1.9686, Validation Accuracy:0.3186\n",
    "Epoch #191: Loss:1.8459, Accuracy:0.3626, Validation Loss:1.9977, Validation Accuracy:0.3087\n",
    "Epoch #192: Loss:1.8593, Accuracy:0.3626, Validation Loss:1.9580, Validation Accuracy:0.3235\n",
    "Epoch #193: Loss:1.8509, Accuracy:0.3598, Validation Loss:1.9469, Validation Accuracy:0.3235\n",
    "Epoch #194: Loss:1.8530, Accuracy:0.3618, Validation Loss:1.9637, Validation Accuracy:0.3218\n",
    "Epoch #195: Loss:1.8513, Accuracy:0.3634, Validation Loss:1.9393, Validation Accuracy:0.3186\n",
    "Epoch #196: Loss:1.8521, Accuracy:0.3581, Validation Loss:1.9434, Validation Accuracy:0.3317\n",
    "Epoch #197: Loss:1.8363, Accuracy:0.3671, Validation Loss:1.9388, Validation Accuracy:0.3350\n",
    "Epoch #198: Loss:1.8318, Accuracy:0.3651, Validation Loss:1.9406, Validation Accuracy:0.3350\n",
    "Epoch #199: Loss:1.8338, Accuracy:0.3602, Validation Loss:1.9382, Validation Accuracy:0.3251\n",
    "Epoch #200: Loss:1.8379, Accuracy:0.3659, Validation Loss:1.9441, Validation Accuracy:0.3235\n",
    "Epoch #201: Loss:1.8385, Accuracy:0.3639, Validation Loss:1.9371, Validation Accuracy:0.3317\n",
    "Epoch #202: Loss:1.8380, Accuracy:0.3639, Validation Loss:1.9561, Validation Accuracy:0.3268\n",
    "Epoch #203: Loss:1.8405, Accuracy:0.3610, Validation Loss:1.9383, Validation Accuracy:0.3251\n",
    "Epoch #204: Loss:1.8383, Accuracy:0.3634, Validation Loss:1.9513, Validation Accuracy:0.3186\n",
    "Epoch #205: Loss:1.8368, Accuracy:0.3643, Validation Loss:1.9748, Validation Accuracy:0.3251\n",
    "Epoch #206: Loss:1.8341, Accuracy:0.3630, Validation Loss:1.9431, Validation Accuracy:0.3186\n",
    "Epoch #207: Loss:1.8232, Accuracy:0.3659, Validation Loss:1.9345, Validation Accuracy:0.3235\n",
    "Epoch #208: Loss:1.8236, Accuracy:0.3692, Validation Loss:1.9438, Validation Accuracy:0.3317\n",
    "Epoch #209: Loss:1.8263, Accuracy:0.3622, Validation Loss:1.9419, Validation Accuracy:0.3333\n",
    "Epoch #210: Loss:1.8293, Accuracy:0.3680, Validation Loss:1.9327, Validation Accuracy:0.3333\n",
    "Epoch #211: Loss:1.8209, Accuracy:0.3692, Validation Loss:1.9266, Validation Accuracy:0.3268\n",
    "Epoch #212: Loss:1.8179, Accuracy:0.3729, Validation Loss:1.9391, Validation Accuracy:0.3169\n",
    "Epoch #213: Loss:1.8216, Accuracy:0.3717, Validation Loss:1.9394, Validation Accuracy:0.3153\n",
    "Epoch #214: Loss:1.8132, Accuracy:0.3717, Validation Loss:1.9308, Validation Accuracy:0.3333\n",
    "Epoch #215: Loss:1.8131, Accuracy:0.3721, Validation Loss:1.9299, Validation Accuracy:0.3317\n",
    "Epoch #216: Loss:1.8176, Accuracy:0.3688, Validation Loss:1.9279, Validation Accuracy:0.3383\n",
    "Epoch #217: Loss:1.8229, Accuracy:0.3671, Validation Loss:1.9270, Validation Accuracy:0.3366\n",
    "Epoch #218: Loss:1.8210, Accuracy:0.3704, Validation Loss:1.9435, Validation Accuracy:0.3169\n",
    "Epoch #219: Loss:1.8166, Accuracy:0.3704, Validation Loss:1.9538, Validation Accuracy:0.3169\n",
    "Epoch #220: Loss:1.8318, Accuracy:0.3671, Validation Loss:1.9675, Validation Accuracy:0.3251\n",
    "Epoch #221: Loss:1.8234, Accuracy:0.3754, Validation Loss:1.9519, Validation Accuracy:0.3071\n",
    "Epoch #222: Loss:1.8265, Accuracy:0.3626, Validation Loss:1.9411, Validation Accuracy:0.3317\n",
    "Epoch #223: Loss:1.8168, Accuracy:0.3684, Validation Loss:1.9377, Validation Accuracy:0.3218\n",
    "Epoch #224: Loss:1.8116, Accuracy:0.3782, Validation Loss:1.9242, Validation Accuracy:0.3268\n",
    "Epoch #225: Loss:1.8087, Accuracy:0.3749, Validation Loss:1.9323, Validation Accuracy:0.3235\n",
    "Epoch #226: Loss:1.8047, Accuracy:0.3745, Validation Loss:1.9242, Validation Accuracy:0.3284\n",
    "Epoch #227: Loss:1.8036, Accuracy:0.3737, Validation Loss:1.9300, Validation Accuracy:0.3235\n",
    "Epoch #228: Loss:1.8041, Accuracy:0.3770, Validation Loss:1.9211, Validation Accuracy:0.3317\n",
    "Epoch #229: Loss:1.8043, Accuracy:0.3725, Validation Loss:1.9254, Validation Accuracy:0.3350\n",
    "Epoch #230: Loss:1.8075, Accuracy:0.3770, Validation Loss:1.9253, Validation Accuracy:0.3366\n",
    "Epoch #231: Loss:1.8056, Accuracy:0.3745, Validation Loss:1.9274, Validation Accuracy:0.3350\n",
    "Epoch #232: Loss:1.8137, Accuracy:0.3713, Validation Loss:1.9525, Validation Accuracy:0.3268\n",
    "Epoch #233: Loss:1.8261, Accuracy:0.3663, Validation Loss:1.9491, Validation Accuracy:0.3333\n",
    "Epoch #234: Loss:1.8226, Accuracy:0.3663, Validation Loss:1.9284, Validation Accuracy:0.3366\n",
    "Epoch #235: Loss:1.8453, Accuracy:0.3614, Validation Loss:1.9316, Validation Accuracy:0.3235\n",
    "Epoch #236: Loss:1.8558, Accuracy:0.3651, Validation Loss:2.0025, Validation Accuracy:0.3120\n",
    "Epoch #237: Loss:1.8550, Accuracy:0.3606, Validation Loss:1.9751, Validation Accuracy:0.3136\n",
    "Epoch #238: Loss:1.8304, Accuracy:0.3659, Validation Loss:1.9219, Validation Accuracy:0.3235\n",
    "Epoch #239: Loss:1.8196, Accuracy:0.3647, Validation Loss:1.9206, Validation Accuracy:0.3465\n",
    "Epoch #240: Loss:1.8070, Accuracy:0.3700, Validation Loss:1.9216, Validation Accuracy:0.3350\n",
    "Epoch #241: Loss:1.8026, Accuracy:0.3717, Validation Loss:1.9208, Validation Accuracy:0.3317\n",
    "Epoch #242: Loss:1.7987, Accuracy:0.3791, Validation Loss:1.9238, Validation Accuracy:0.3268\n",
    "Epoch #243: Loss:1.7968, Accuracy:0.3791, Validation Loss:1.9232, Validation Accuracy:0.3317\n",
    "Epoch #244: Loss:1.7973, Accuracy:0.3778, Validation Loss:1.9314, Validation Accuracy:0.3284\n",
    "Epoch #245: Loss:1.8011, Accuracy:0.3762, Validation Loss:1.9360, Validation Accuracy:0.3186\n",
    "Epoch #246: Loss:1.8009, Accuracy:0.3733, Validation Loss:1.9137, Validation Accuracy:0.3399\n",
    "Epoch #247: Loss:1.7976, Accuracy:0.3762, Validation Loss:1.9271, Validation Accuracy:0.3366\n",
    "Epoch #248: Loss:1.7999, Accuracy:0.3725, Validation Loss:1.9166, Validation Accuracy:0.3333\n",
    "Epoch #249: Loss:1.7952, Accuracy:0.3766, Validation Loss:1.9225, Validation Accuracy:0.3251\n",
    "Epoch #250: Loss:1.8057, Accuracy:0.3754, Validation Loss:1.9137, Validation Accuracy:0.3399\n",
    "Epoch #251: Loss:1.8013, Accuracy:0.3754, Validation Loss:1.9277, Validation Accuracy:0.3202\n",
    "Epoch #252: Loss:1.7889, Accuracy:0.3807, Validation Loss:1.9069, Validation Accuracy:0.3432\n",
    "Epoch #253: Loss:1.7903, Accuracy:0.3766, Validation Loss:1.9190, Validation Accuracy:0.3300\n",
    "Epoch #254: Loss:1.7893, Accuracy:0.3786, Validation Loss:1.9180, Validation Accuracy:0.3268\n",
    "Epoch #255: Loss:1.7854, Accuracy:0.3782, Validation Loss:1.9153, Validation Accuracy:0.3235\n",
    "Epoch #256: Loss:1.7854, Accuracy:0.3799, Validation Loss:1.9128, Validation Accuracy:0.3383\n",
    "Epoch #257: Loss:1.7855, Accuracy:0.3778, Validation Loss:1.9195, Validation Accuracy:0.3350\n",
    "Epoch #258: Loss:1.7872, Accuracy:0.3762, Validation Loss:1.9110, Validation Accuracy:0.3432\n",
    "Epoch #259: Loss:1.7889, Accuracy:0.3758, Validation Loss:1.9174, Validation Accuracy:0.3284\n",
    "Epoch #260: Loss:1.7911, Accuracy:0.3782, Validation Loss:1.9082, Validation Accuracy:0.3448\n",
    "Epoch #261: Loss:1.7913, Accuracy:0.3717, Validation Loss:1.9186, Validation Accuracy:0.3317\n",
    "Epoch #262: Loss:1.7858, Accuracy:0.3823, Validation Loss:1.9140, Validation Accuracy:0.3366\n",
    "Epoch #263: Loss:1.7866, Accuracy:0.3766, Validation Loss:1.9319, Validation Accuracy:0.3202\n",
    "Epoch #264: Loss:1.7870, Accuracy:0.3836, Validation Loss:1.9174, Validation Accuracy:0.3268\n",
    "Epoch #265: Loss:1.7925, Accuracy:0.3688, Validation Loss:1.9291, Validation Accuracy:0.3448\n",
    "Epoch #266: Loss:1.7838, Accuracy:0.3844, Validation Loss:1.9074, Validation Accuracy:0.3383\n",
    "Epoch #267: Loss:1.7886, Accuracy:0.3828, Validation Loss:1.9236, Validation Accuracy:0.3415\n",
    "Epoch #268: Loss:1.7843, Accuracy:0.3869, Validation Loss:1.9070, Validation Accuracy:0.3383\n",
    "Epoch #269: Loss:1.7940, Accuracy:0.3733, Validation Loss:1.9143, Validation Accuracy:0.3366\n",
    "Epoch #270: Loss:1.7875, Accuracy:0.3840, Validation Loss:1.9245, Validation Accuracy:0.3268\n",
    "Epoch #271: Loss:1.7942, Accuracy:0.3708, Validation Loss:1.9546, Validation Accuracy:0.3350\n",
    "Epoch #272: Loss:1.7949, Accuracy:0.3782, Validation Loss:1.9262, Validation Accuracy:0.3218\n",
    "Epoch #273: Loss:1.7945, Accuracy:0.3749, Validation Loss:1.9150, Validation Accuracy:0.3432\n",
    "Epoch #274: Loss:1.7844, Accuracy:0.3749, Validation Loss:1.9129, Validation Accuracy:0.3333\n",
    "Epoch #275: Loss:1.7829, Accuracy:0.3807, Validation Loss:1.9157, Validation Accuracy:0.3563\n",
    "Epoch #276: Loss:1.7888, Accuracy:0.3799, Validation Loss:1.9175, Validation Accuracy:0.3366\n",
    "Epoch #277: Loss:1.7886, Accuracy:0.3791, Validation Loss:1.9040, Validation Accuracy:0.3333\n",
    "Epoch #278: Loss:1.7857, Accuracy:0.3823, Validation Loss:1.9153, Validation Accuracy:0.3284\n",
    "Epoch #279: Loss:1.7779, Accuracy:0.3828, Validation Loss:1.9102, Validation Accuracy:0.3300\n",
    "Epoch #280: Loss:1.7742, Accuracy:0.3844, Validation Loss:1.9194, Validation Accuracy:0.3300\n",
    "Epoch #281: Loss:1.7739, Accuracy:0.3869, Validation Loss:1.9235, Validation Accuracy:0.3284\n",
    "Epoch #282: Loss:1.7818, Accuracy:0.3840, Validation Loss:1.9248, Validation Accuracy:0.3317\n",
    "Epoch #283: Loss:1.7850, Accuracy:0.3832, Validation Loss:1.9027, Validation Accuracy:0.3448\n",
    "Epoch #284: Loss:1.7839, Accuracy:0.3860, Validation Loss:1.9070, Validation Accuracy:0.3530\n",
    "Epoch #285: Loss:1.7706, Accuracy:0.3860, Validation Loss:1.9026, Validation Accuracy:0.3448\n",
    "Epoch #286: Loss:1.7692, Accuracy:0.3869, Validation Loss:1.9056, Validation Accuracy:0.3366\n",
    "Epoch #287: Loss:1.7708, Accuracy:0.3803, Validation Loss:1.9107, Validation Accuracy:0.3399\n",
    "Epoch #288: Loss:1.7686, Accuracy:0.3807, Validation Loss:1.9066, Validation Accuracy:0.3366\n",
    "Epoch #289: Loss:1.7669, Accuracy:0.3840, Validation Loss:1.9071, Validation Accuracy:0.3448\n",
    "Epoch #290: Loss:1.7659, Accuracy:0.3848, Validation Loss:1.9041, Validation Accuracy:0.3415\n",
    "Epoch #291: Loss:1.7686, Accuracy:0.3873, Validation Loss:1.9062, Validation Accuracy:0.3284\n",
    "Epoch #292: Loss:1.7644, Accuracy:0.3852, Validation Loss:1.9025, Validation Accuracy:0.3448\n",
    "Epoch #293: Loss:1.7654, Accuracy:0.3852, Validation Loss:1.9121, Validation Accuracy:0.3530\n",
    "Epoch #294: Loss:1.7680, Accuracy:0.3856, Validation Loss:1.9037, Validation Accuracy:0.3498\n",
    "Epoch #295: Loss:1.7717, Accuracy:0.3848, Validation Loss:1.9037, Validation Accuracy:0.3383\n",
    "Epoch #296: Loss:1.7638, Accuracy:0.3852, Validation Loss:1.9062, Validation Accuracy:0.3547\n",
    "Epoch #297: Loss:1.7672, Accuracy:0.3877, Validation Loss:1.8962, Validation Accuracy:0.3432\n",
    "Epoch #298: Loss:1.7623, Accuracy:0.3901, Validation Loss:1.9063, Validation Accuracy:0.3498\n",
    "Epoch #299: Loss:1.7611, Accuracy:0.3844, Validation Loss:1.9011, Validation Accuracy:0.3415\n",
    "Epoch #300: Loss:1.7673, Accuracy:0.3860, Validation Loss:1.9288, Validation Accuracy:0.3284\n",
    "\n",
    "Test:\n",
    "Test Loss:1.92882514, Accuracy:0.3284\n",
    "Labels: ['ib', 'eg', 'eo', 'ce', 'my', 'aa', 'ds', 'sg', 'yd', 'ek', 'mb', 'ck', 'by', 'eb', 'sk']\n",
    "Confusion Matrix:\n",
    "      ib  eg  eo  ce  my  aa  ds  sg  yd  ek  mb  ck  by  eb  sk\n",
    "t:ib   7   3   1   0   0   0   0   2  29   3   5   0   3   1   0\n",
    "t:eg   1  25   2   0   0   4   5   1   0   4   1   0   3   4   0\n",
    "t:eo   2   2  19   0   0   1   0   2   0   0   1   0   7   0   0\n",
    "t:ce   2   3   2   0   0   1   1   1   2   7   3   0   1   4   0\n",
    "t:my   0   0   0   0   6   0   1   0   0   1   0   0   0  12   0\n",
    "t:aa   0   7   1   0   2  13   3   0   0   2   2   0   0   4   0\n",
    "t:ds   0   1   0   0   5   1   4   0   0   3   4   0   0  13   0\n",
    "t:sg   1   1   2   0   0   0   0  10  23   3   0   0  10   1   0\n",
    "t:yd   4   4   0   0   0   2   0   2  37   3   4   0   4   2   0\n",
    "t:ek   5   4   0   0   2   0   2   0   3   8   5   0   1  18   0\n",
    "t:mb   4   0   1   0   0   0   3   1   2   3  20   0   4  14   0\n",
    "t:ck   0   3   0   0   0   2   3   0   0   4   3   0   0   8   0\n",
    "t:by   3   3   1   0   0   0   0   4   4   3   8   0  14   0   0\n",
    "t:eb   0   1   0   0   4   0   3   0   0   3   2   0   0  37   0\n",
    "t:sk   0   0   0   0   0   0   4   0   0   2   2   0   0  25   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ib       0.24      0.13      0.17        54\n",
    "          eg       0.44      0.50      0.47        50\n",
    "          eo       0.66      0.56      0.60        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          my       0.32      0.30      0.31        20\n",
    "          aa       0.54      0.38      0.45        34\n",
    "          ds       0.14      0.13      0.13        31\n",
    "          sg       0.43      0.20      0.27        51\n",
    "          yd       0.37      0.60      0.46        62\n",
    "          ek       0.16      0.17      0.16        48\n",
    "          mb       0.33      0.38      0.36        52\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          by       0.30      0.35      0.32        40\n",
    "          eb       0.26      0.74      0.38        50\n",
    "          sk       0.00      0.00      0.00        33\n",
    "\n",
    "    accuracy                           0.33       609\n",
    "   macro avg       0.28      0.30      0.27       609\n",
    "weighted avg       0.30      0.33      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 04:17:30 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.707493232388802, 2.701049052631522, 2.696080149492411, 2.6914828794538876, 2.6870687305437913, 2.682843570051522, 2.6789026608803783, 2.675186765996498, 2.6719674919234904, 2.669300163516466, 2.667148534104546, 2.665441992639125, 2.6641412810934786, 2.6630025137038458, 2.661988440014067, 2.661210200665228, 2.660445032840096, 2.6596420625551973, 2.658839186228359, 2.657932970911411, 2.656805534863903, 2.6549964283878973, 2.6521257626012043, 2.64715360616424, 2.6384088002597954, 2.6235881211918173, 2.6033814368381094, 2.589524980836314, 2.576186747386538, 2.558935124298622, 2.5444214058235555, 2.532788663074888, 2.517833456421525, 2.506071480624194, 2.497931831967459, 2.4809649577868984, 2.4729441296682375, 2.4501460451993644, 2.449357081320877, 2.431035852197356, 2.4348758492368, 2.424289872102158, 2.399986295668754, 2.379878108333093, 2.369714603831224, 2.3506272966638573, 2.343164301466668, 2.329275232621993, 2.323431499290153, 2.3133377758740203, 2.314137086883946, 2.2958152591692795, 2.2900634093824865, 2.28034961673818, 2.2758251281794655, 2.284203270973243, 2.2675739787090783, 2.2625477932552593, 2.250724643322047, 2.2555091001325835, 2.24276194705556, 2.2264210958590454, 2.2282015682245513, 2.2161699543249824, 2.2154877960975536, 2.2122506981804255, 2.214323946212117, 2.2089649812732817, 2.1954847004613267, 2.1968553250254863, 2.191114682086387, 2.179029507394299, 2.1845616926309117, 2.174203611164062, 2.173099920275959, 2.166378101104586, 2.164668477031789, 2.153784195973564, 2.15364975216745, 2.155639159855584, 2.153086411737652, 2.1564609808679087, 2.1337544115501865, 2.1394951801581925, 2.129122934317941, 2.138023083237396, 2.1222152713875855, 2.14796898400255, 2.1458713644243814, 2.114054926902007, 2.10482088489877, 2.1244014115952115, 2.105710820219983, 2.1197832511563606, 2.121962734043892, 2.0884719865858457, 2.090275047643627, 2.0923518631454368, 2.0753836475178136, 2.076780573487869, 2.0854153946311214, 2.1071294959151294, 2.0678000416857465, 2.061331331240524, 2.0699560781222064, 2.065869127588319, 2.0568383158917105, 2.0760478511428206, 2.0489612657252594, 2.0455179284946086, 2.044806542850676, 2.0427309677910137, 2.045069421061937, 2.037537192476207, 2.038146250353658, 2.0452922672669485, 2.0397145272475745, 2.03097503388848, 2.0507413894672113, 2.0283548937642517, 2.021753446026193, 2.0320308952300223, 2.0230152393601015, 2.019512752006794, 2.0148186082714687, 2.017168686111964, 2.0074750341609584, 2.0140564568915784, 2.0076339552163684, 2.0056721130615385, 2.0106663126467876, 2.0026543978204083, 2.0124538549648716, 1.9997971813471251, 2.003757327648219, 2.0162950779612623, 1.9956129364583683, 1.995080477889927, 2.0009730093193365, 1.9916455731994804, 1.990046530913054, 2.003268769417686, 2.0046956674218763, 1.9996362010442172, 2.0245233081244485, 2.0127392038335943, 1.9886600047300993, 1.9914616097761884, 1.9819867123523955, 1.9843203516429282, 1.973069790940371, 1.9821531496611722, 1.9763886617322273, 1.9737481859517214, 1.9747866167027766, 1.9783563788105505, 1.9671575247947806, 1.9737064202235055, 1.9663723380303344, 1.9903089820066304, 1.9726756707396609, 1.9834864278536517, 1.9733112469095315, 1.9697297472867668, 1.9770180276657756, 1.9811063071189843, 1.9607323926853624, 1.9646835491575043, 1.9623483272608866, 1.9581061020273294, 1.9786193979589026, 1.969638818003274, 1.9582902686349277, 1.958373847657628, 1.9552033797077748, 1.9842252715663566, 1.973819994965602, 1.9969181278460524, 1.9775310696052213, 1.9854010429679858, 1.9745593861797563, 1.9483818710339675, 1.9538772885239575, 1.9518949083115276, 1.9510918957455012, 1.947365120322442, 1.9559429472890393, 1.9443630708262252, 1.955059362554002, 1.968604384580465, 1.9976698095575343, 1.9579647438866752, 1.9468843247894387, 1.9637406524179017, 1.9392946576837249, 1.943443058551043, 1.9387705887871227, 1.9406095547433362, 1.9381771183562004, 1.944063404706507, 1.937126791927419, 1.9560553620405776, 1.9382594402983466, 1.951271462518789, 1.9748388853762147, 1.9430596583778244, 1.9344735997063773, 1.9438224766642003, 1.9419204147382714, 1.9327321859024624, 1.9266062702842925, 1.93910752865677, 1.9394331236778222, 1.9307786975979608, 1.9299239317576091, 1.9279498449100063, 1.9270069436682464, 1.9434691892664617, 1.9537721551306337, 1.967485331353687, 1.9518876439832114, 1.941072853719464, 1.937723443425935, 1.924201367328124, 1.9323442412910399, 1.924152647333192, 1.930039526989503, 1.921113206052232, 1.925395828554, 1.925252270620249, 1.9273554294175899, 1.9525170745129263, 1.949146096538049, 1.9283806103203684, 1.9315859737067387, 2.002538225137933, 1.975071093327502, 1.9218690058672174, 1.920615237334679, 1.9216316063415828, 1.9207723575272584, 1.9237575116024423, 1.923203736103227, 1.9313831822625522, 1.9360458627710202, 1.9136988776070731, 1.9270723112698258, 1.916637805490854, 1.922518380952782, 1.9136778813081814, 1.9276577383035118, 1.9069014478395334, 1.9190036197405536, 1.918011773396008, 1.915277795642859, 1.9127651081100865, 1.9195465863436119, 1.9110260172234772, 1.917425727805089, 1.908200769980357, 1.9185717399484419, 1.9139640605312653, 1.9319050402085378, 1.917433782359845, 1.929111400848539, 1.9074174854751487, 1.923619333550652, 1.9070133630473822, 1.9142645023922222, 1.92451615775943, 1.9545530550585593, 1.9262180727690899, 1.9150345934239905, 1.9129014802096513, 1.9157425541008635, 1.9175319789078435, 1.9040004983911374, 1.9152721053078061, 1.9101701824144386, 1.9193801177154817, 1.9234617177293023, 1.9247871514024406, 1.9027288780228062, 1.9070270180897955, 1.902550205808555, 1.9055780704777032, 1.9106747969030746, 1.9066001462623208, 1.907115589417456, 1.9041024119591674, 1.906157720852368, 1.9024973986379814, 1.9121186946608946, 1.9036927561846078, 1.9037185602000195, 1.9062079054185714, 1.8961550774441172, 1.9062831419442088, 1.9011454713364149, 1.9288250926288673], 'val_acc': [0.044334975265468084, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10344827565408889, 0.10344827565408889, 0.11165845538497167, 0.12643678050812437, 0.13136288997134551, 0.1346469611320981, 0.12151067322257704, 0.14285714284490875, 0.15270935949580422, 0.1494252862552508, 0.1494252862552508, 0.17898193728454007, 0.16420361157414948, 0.18390804585467027, 0.18883415224712666, 0.18555008178371907, 0.15763546697709752, 0.19540229664843267, 0.14942528645099679, 0.2266009832152788, 0.15763546687922456, 0.21674876844843816, 0.1806239730178429, 0.1904761901581033, 0.19047619025597626, 0.1937602623098198, 0.19704433465728227, 0.18226600924051062, 0.20689655140605073, 0.20853858782446444, 0.2200328405022817, 0.21674876617289138, 0.21182265779850715, 0.2216748745472756, 0.23152709178540898, 0.22988505556274125, 0.2216748746451486, 0.2282430194379465, 0.2298850552691223, 0.2446633809795129, 0.23973727211576376, 0.23973727231150974, 0.24466338068589397, 0.23645319976830131, 0.23973727231150974, 0.23645319967042833, 0.24466338078376695, 0.2413793082405585, 0.24466338058802098, 0.25451559753253544, 0.2545155972389165, 0.2643678145749228, 0.2545155971410435, 0.25287356150561363, 0.24794745303335644, 0.2561576337552032, 0.26272577835225513, 0.2528735614077407, 0.2512315253808189, 0.2561576334615842, 0.26929392304718003, 0.26600985030822566, 0.2643678145749228, 0.26600985021035267, 0.2545155976304084, 0.24794745264186452, 0.24958948915815118, 0.2594417056133007, 0.2725779951010236, 0.26600985030822566, 0.2610837420317144, 0.2643678141834309, 0.25779966978212493, 0.2709359587804829, 0.2791461395023296, 0.27422003112794535, 0.27586206744848607, 0.27750410327966185, 0.2791461395023296, 0.2775041034754078, 0.2906403924737658, 0.295566501043896, 0.2873563202241763, 0.28571428409938154, 0.3037766814721237, 0.29228242850068753, 0.3037766816678697, 0.30541871779266444, 0.3037766816678697, 0.30541871789053743, 0.3087027901401269, 0.30706075401533217, 0.31034482616704867, 0.3037766816678697, 0.3087027901401269, 0.31362889861238413, 0.30213464515158306, 0.31362889822089224, 0.3169129708619736, 0.3119868623897164, 0.31034482646066763, 0.31691297076410063, 0.3201970431115631, 0.3185550068888954, 0.3201970432094361, 0.3119868621939704, 0.31855500698676836, 0.3201970431115631, 0.3103448260691757, 0.3152709347371789, 0.3152709345414329, 0.31855500698676836, 0.3201970430136901, 0.3152709347371789, 0.3234811152632796, 0.31855500698676836, 0.3185550068888954, 0.33004925986033157, 0.3234811152632796, 0.3185550067910224, 0.32183907904061193, 0.31691297066622764, 0.3119868625854624, 0.30870278994438094, 0.3201970430136901, 0.3284072236376639, 0.32512315138807435, 0.3234811152632796, 0.33497536813684287, 0.3284072236376639, 0.3234811152632796, 0.32676518751286915, 0.32512315138807435, 0.3284072236376639, 0.3316912958872534, 0.33497536813684287, 0.33497536813684287, 0.3185550067910224, 0.32512315138807435, 0.3201970431115631, 0.3201970430136901, 0.3316912958872534, 0.32183907913848486, 0.31362889861238413, 0.3284072236376639, 0.32512315138807435, 0.3267651876107421, 0.3333333320120481, 0.32183907933423084, 0.33004925986033157, 0.3201970431115631, 0.3316912958872534, 0.32676518751286915, 0.3152709347371789, 0.3169129709598466, 0.30541871779266444, 0.31855500718251434, 0.31855500698676836, 0.3136288988081301, 0.32512315138807435, 0.32183907923635785, 0.32676518741499616, 0.3234811151654067, 0.32512315138807435, 0.32512315119232843, 0.3349753680389699, 0.31855500698676836, 0.31855500698676836, 0.3087027902379999, 0.3234811154590256, 0.3234811153611526, 0.32183907933423084, 0.3185550067910224, 0.3316912957893804, 0.3349753680389699, 0.3349753679410969, 0.3251231512902014, 0.3234811152632796, 0.3316912958872534, 0.3267651876107421, 0.3251231515838203, 0.31855500698676836, 0.3251231516816933, 0.3185550068888954, 0.3234811152632796, 0.3316912957893804, 0.33333333191417513, 0.33333333191417513, 0.32676518751286915, 0.31691297076410063, 0.3152709346393059, 0.33333333191417513, 0.3316912957893804, 0.33825944028855937, 0.3366174041637646, 0.31691297076410063, 0.3169129705683547, 0.3251231516816933, 0.3070607538195862, 0.3316912962787453, 0.32183907904061193, 0.32676518741499616, 0.3234811152632796, 0.3284072235397909, 0.3234811151654067, 0.3316912958872534, 0.33497536813684287, 0.3366174041637646, 0.3349753679410969, 0.32676518751286915, 0.33333333181630215, 0.3366174042616376, 0.3234811151654067, 0.3119868623897164, 0.31362889861238413, 0.3234811152632796, 0.34646962101040607, 0.33497536813684287, 0.3316912958872534, 0.32676518741499616, 0.33169129598512637, 0.3284072237355368, 0.31855500669314946, 0.3399014766091001, 0.3366174042616376, 0.3333333320120481, 0.3251231512902014, 0.3399014765112271, 0.3201970428179442, 0.3431855487608166, 0.33004925966458565, 0.32676518741499616, 0.3234811151654067, 0.33825944038643235, 0.33497536813684287, 0.34318554895656256, 0.3284072235397909, 0.3448275850813573, 0.3316912956915074, 0.3366174043595106, 0.3201970429158172, 0.3267651876107421, 0.34482758468986535, 0.33825944048430534, 0.34154351244027586, 0.33825944038643235, 0.3366174041637646, 0.3267651873171232, 0.3349753680389699, 0.32183907884486596, 0.3431855487608166, 0.3333333321099211, 0.3563218379549205, 0.3366174043595106, 0.3333333320120481, 0.3284072235397909, 0.33004925986033157, 0.33004925966458565, 0.3284072237355368, 0.3316912957893804, 0.3448275849834843, 0.353037765705331, 0.3448275849834843, 0.3366174041637646, 0.3399014765112271, 0.3366174043595106, 0.3448275848856113, 0.3415435127338948, 0.3284072235397909, 0.3448275849834843, 0.35303776560745803, 0.34975369335786854, 0.33825944048430534, 0.3546798017322528, 0.34318554885868957, 0.34975369325999556, 0.3415435127338948, 0.32840722334404493], 'loss': [2.7125082259795015, 2.7050191796046263, 2.6992188536900517, 2.6944769129860817, 2.6898996825090915, 2.685788410938741, 2.68159063826841, 2.6779733534466317, 2.6741195856912916, 2.671092998516388, 2.6690310385192935, 2.6667588079244937, 2.665268330211757, 2.6638945016528055, 2.6630651637514022, 2.6619607495576205, 2.661238037438363, 2.660540223464339, 2.6598162039104674, 2.658624193849505, 2.6577599448100253, 2.65606310529141, 2.6538770812737624, 2.6500164939637547, 2.643239802648399, 2.6307374430388153, 2.611615646252642, 2.5883479966275256, 2.5657616916867987, 2.552053678794563, 2.529632492085012, 2.5154996367695395, 2.498462050651378, 2.4844655990600586, 2.4720483600726118, 2.4592464617390406, 2.4439892180401688, 2.428153942253066, 2.418104820623535, 2.4049617255248084, 2.3886012556126963, 2.3911535732065627, 2.3746262831364815, 2.351155836234592, 2.338050069848125, 2.3225192740712566, 2.309216075168743, 2.296527533070997, 2.2849903658919755, 2.275354166344206, 2.270368036450302, 2.2549555953768, 2.2472493413537435, 2.236893391560235, 2.229356211264765, 2.222670932863772, 2.2208851643901095, 2.2159704119273034, 2.1988576609006407, 2.2017455797175853, 2.193164904161645, 2.182030200370773, 2.169021782689026, 2.1628808450405113, 2.156262341270212, 2.153866205861681, 2.1507207024513573, 2.147129023197495, 2.136021648442231, 2.1276675850948514, 2.1313523184837013, 2.1324241930943986, 2.118404308432671, 2.115668019132203, 2.108838351600224, 2.1088977287926958, 2.0967057400415565, 2.0898265729939425, 2.0887615447661227, 2.0839295900333097, 2.0784970830842946, 2.0809292001156345, 2.072228963566022, 2.0657819847796244, 2.0608367739761633, 2.056667228107335, 2.0611843894394517, 2.0516119670084616, 2.051650170966585, 2.0587713395300833, 2.040298492560886, 2.0346517850730943, 2.03763040389858, 2.03720339764315, 2.0330433074453773, 2.0252435769388564, 2.0195361489877564, 2.0158918924155422, 2.012697509328932, 2.003311117082161, 2.0005146430992737, 2.0089614356078163, 2.000686071932438, 1.9905322218822503, 1.988997463230235, 1.988850121625395, 1.9862393955920021, 1.9789588411241097, 1.9806709466773627, 1.967430731697004, 1.9637294433444923, 1.9616078598053792, 1.9689944448412322, 1.9673322120486343, 1.9584350518866975, 1.965201885597417, 1.957288179701114, 1.9513745330442394, 1.9460964325272327, 1.9553934273533753, 1.95137358730089, 1.940275613776957, 1.9412109381364355, 1.9358245105958818, 1.93018018372005, 1.9300059572627168, 1.9340559652943385, 1.9338781940374041, 1.936387560254984, 1.9261468185291644, 1.9230611536292326, 1.9213076602751715, 1.9225681780789667, 1.9181377018746408, 1.9114676494128406, 1.920242157168457, 1.9189170378679123, 1.906422745765357, 1.906960143592568, 1.9071932563057181, 1.9024969573872779, 1.9014636473978814, 1.9087714951630734, 1.9103075282529638, 1.9106876177464667, 1.9125052896123647, 1.9099592838933581, 1.9073669585848736, 1.8997169363425253, 1.8874946818949017, 1.8896930469379778, 1.886890403201203, 1.8890245777870351, 1.8816583457668703, 1.8810807511547019, 1.8843211909339168, 1.8869671215022124, 1.8792443575065974, 1.8764599513224263, 1.873343355944514, 1.878741712002294, 1.8740850218022873, 1.87671575526682, 1.8730981787127392, 1.865456037697606, 1.8731012517177104, 1.869394015090911, 1.861372397225006, 1.8645144295643488, 1.861502615676034, 1.8556509233842886, 1.8623944316801349, 1.8580667201008885, 1.8626219768543752, 1.8552990861497132, 1.8537821166079638, 1.8597387305030588, 1.8683631822069078, 1.8765373675974977, 1.86605131097398, 1.859867196797835, 1.8545595668914137, 1.845947609447111, 1.8433988559417411, 1.8488197987574082, 1.8465326265143174, 1.8484081928734906, 1.8482195567301412, 1.8394283840054115, 1.84400154109853, 1.8459327063766104, 1.8593077920545542, 1.8508892644357386, 1.8529941786975586, 1.8512545422117324, 1.8521053906583689, 1.8362597977601038, 1.8318027718600796, 1.8338472332552962, 1.8378956102003061, 1.8385228604996229, 1.83801076984993, 1.8405044867027955, 1.8382633005079547, 1.8368135241267618, 1.8340741835090903, 1.823179516508349, 1.8236345948135093, 1.826340618074797, 1.8292830901958614, 1.8208548232025679, 1.8179055868967358, 1.8215840795936036, 1.81322143900321, 1.8130683346205914, 1.8176137509042478, 1.822942783847237, 1.8210156690413457, 1.8166359325698758, 1.8317530178681047, 1.8233985005952493, 1.8265277911505415, 1.8167638802185686, 1.8115540255266538, 1.8086502264657305, 1.8046915578646336, 1.8035557364046697, 1.8041251161994385, 1.8043202904460367, 1.80748850408276, 1.8055634217585381, 1.8136770317931439, 1.8261393374730919, 1.822648910575334, 1.8453412547493373, 1.8558216303525763, 1.855043835121014, 1.8303615377668971, 1.8196423109062398, 1.8069542266994532, 1.8025501932206829, 1.7987466288787872, 1.796770623234508, 1.7973016388362437, 1.8011441029562354, 1.80092527974068, 1.797598716023032, 1.7998706296728866, 1.7952116694048934, 1.8057097995060916, 1.8012537574376413, 1.788940481728352, 1.7902653266272262, 1.7893210819369714, 1.7854163655510185, 1.785392372417254, 1.7855132927395234, 1.7872049734088185, 1.7888696636262615, 1.7911453419887065, 1.7912686874244736, 1.7858472834377563, 1.7865941250348727, 1.7869689815587821, 1.7924901503557051, 1.7838123947687952, 1.7885863307075578, 1.7842606171445434, 1.7940002591702973, 1.7875208454699976, 1.7941992446382433, 1.794886983983081, 1.7944684509379174, 1.7844415736639034, 1.7829264541426233, 1.788819569679746, 1.7885580783017607, 1.7856651139699948, 1.7778959162670973, 1.7742042036271928, 1.773932274019449, 1.781820313250015, 1.7849623200829758, 1.7838958157161422, 1.7705976740780307, 1.7692398606384558, 1.7708183859408024, 1.7685990274319658, 1.7669065840680007, 1.7659224028460054, 1.768559303175987, 1.764432725720337, 1.7653880021900121, 1.7680295663692622, 1.7717225704839341, 1.7637997877916027, 1.7671856427829125, 1.7623048306980171, 1.7610955228550969, 1.7673427820695253], 'acc': [0.04476386054707748, 0.051745379625894206, 0.10225872683512846, 0.10225872644347576, 0.10225872722678116, 0.10225872702177545, 0.10225872645265513, 0.10225872744096623, 0.10225872644347576, 0.10225872645265513, 0.10225872664848147, 0.10225872645265513, 0.10225872643429641, 0.10225872703095482, 0.10225872641593768, 0.10225872684430783, 0.10225872643429641, 0.10225872644347576, 0.10225872644347576, 0.10225872682594911, 0.10225872644347576, 0.10225872664848147, 0.10225872703095482, 0.10431211513966261, 0.11047227936481303, 0.12607802926147743, 0.13223819194755515, 0.1363449697132228, 0.13634496873409108, 0.13552361465455082, 0.15523613971606418, 0.15564681617753462, 0.14373716664265312, 0.17043121160300606, 0.18480492860874356, 0.17371663215591188, 0.18234086350616244, 0.18439425140068516, 0.19260780176710054, 0.17946611839887788, 0.21971252553884008, 0.1815195068625209, 0.21396303831674235, 0.20985626270516453, 0.2143737173055966, 0.22833675453917446, 0.21314168363136432, 0.21683778240817772, 0.23983572902008737, 0.23367556553234556, 0.22381930132788555, 0.2381930173544913, 0.23778234189051134, 0.24804928157731004, 0.25626283393870636, 0.25379876879940777, 0.25667351055928567, 0.25626283529113203, 0.26447638647757027, 0.2616016433469079, 0.2669404539484263, 0.25913757724683634, 0.2710472275466645, 0.26981519629578327, 0.2673511294124063, 0.2726899374314647, 0.2722792629833339, 0.2726899384473139, 0.26940451589942715, 0.27761806904412883, 0.2714579075513679, 0.26817248641098307, 0.2747433286917528, 0.27761806747751805, 0.27720739400851896, 0.2759753609584832, 0.28213552186376506, 0.2878850085167425, 0.28377823511433065, 0.2833675569087818, 0.2858316204263934, 0.2870636544555609, 0.28542094398328166, 0.2915811084501552, 0.2903490748126404, 0.28788500930004784, 0.28665297883247204, 0.29445585040585953, 0.29691991670175744, 0.2903490748126404, 0.2948665280239293, 0.2936344973238097, 0.295687883847548, 0.3055441496186188, 0.2969199192475, 0.3014373708921781, 0.29979466081155154, 0.3104722772780385, 0.3022587247208159, 0.3022587267157968, 0.30595482390764067, 0.3080082119979898, 0.31334702338281356, 0.3133470218162027, 0.3129363440023066, 0.3080082125854688, 0.31457905525789126, 0.32443532024565663, 0.316632443739893, 0.32731006020638, 0.32238192800623683, 0.3252566728993363, 0.31622176553434417, 0.31745379662611645, 0.3240246390659951, 0.3223819298053914, 0.32279260938172466, 0.3264887052027841, 0.3301848047445442, 0.33305954807103294, 0.3264887043827613, 0.3289527700911802, 0.33182751619595524, 0.33347022545655897, 0.33264887260705295, 0.33100616233060004, 0.3289527704828329, 0.33388090291551986, 0.3338809050696097, 0.3355236133877991, 0.3347022595224439, 0.3367556464378349, 0.33757700304475896, 0.33963038996015, 0.3388090337081612, 0.3330595496376437, 0.34373716672832716, 0.337987680430285, 0.341273098829101, 0.34004106597489153, 0.34537987700478007, 0.33552361436693084, 0.34332649028521545, 0.34496919719590297, 0.34250513246661585, 0.3392197135170382, 0.34661190884314036, 0.34989733017935154, 0.3408624253601019, 0.34907597513903826, 0.3470225880645384, 0.350308010184055, 0.34907597572651733, 0.353182749948952, 0.3478439416973498, 0.3478439448672888, 0.34743326407928, 0.3511293612711239, 0.35030800603498424, 0.34989733256598515, 0.35770020609763614, 0.35400410459761256, 0.3515400392808464, 0.35852156211708114, 0.35277207233088215, 0.3535934303085907, 0.3556468166365026, 0.3560574944503988, 0.35811088492738147, 0.35687884831575395, 0.3589322387560192, 0.363039014740891, 0.36016426945613883, 0.35359342854615355, 0.3589322405551738, 0.36344969079235007, 0.3572895294586981, 0.3572895292995891, 0.35523613803930104, 0.35770020770096433, 0.36098562508393117, 0.36386036782294084, 0.36016427023944425, 0.36386037020957446, 0.3605749488366458, 0.3605749476616877, 0.3572895291037628, 0.3618069830983571, 0.3634496917714818, 0.3626283357520368, 0.36262833810195294, 0.3626283371228212, 0.3597535920338954, 0.36180698012424445, 0.36344969435394175, 0.35811088156161613, 0.3671457911174155, 0.36509240302706647, 0.3601642696519652, 0.3659137598665343, 0.3638603719720116, 0.3638603711887062, 0.3609856274705648, 0.36344969040069736, 0.3642710485742322, 0.36303901434923835, 0.36591375904651147, 0.36919917822863285, 0.36221766009223044, 0.3679671436119863, 0.3691991798319611, 0.37289527718291393, 0.37166324472035717, 0.3716632451120099, 0.3720739219467743, 0.36878850021891035, 0.3671457905299365, 0.3704312110828423, 0.37043121167032134, 0.3671457893549784, 0.3753593407005255, 0.36262833653534216, 0.368377824559104, 0.3782340848470371, 0.3749486637066522, 0.3745379872635405, 0.37371663261487986, 0.3770020549669403, 0.3724845987815387, 0.3770020539510911, 0.37453798648023506, 0.3712525647523712, 0.3663244352937969, 0.3663244337271861, 0.36139630371785014, 0.3650924024395874, 0.3605749496199512, 0.36591375806737975, 0.36468172501734397, 0.37002053561886233, 0.37166324632368536, 0.37905544145396114, 0.37905543988735035, 0.37782340722896723, 0.37618069671997056, 0.37330595421350465, 0.37618070051410607, 0.37248459780240695, 0.37659137457058417, 0.3753593450454226, 0.37535934484959627, 0.38069815212206676, 0.3765913771163267, 0.3786447626609332, 0.3782340868053006, 0.37987679786505885, 0.3778234068373145, 0.37618069871495147, 0.375770018710248, 0.3782340854345161, 0.37166324276209367, 0.38234086318182503, 0.37659137770380574, 0.3835728964276872, 0.36878850319302303, 0.3843942484938878, 0.38275153942911044, 0.3868583149856121, 0.37330595519263643, 0.3839835736541043, 0.3708418910508283, 0.3782340858261688, 0.374948665273263, 0.37494866484489286, 0.3806981507512823, 0.3798767990400169, 0.37905544145396114, 0.38234086239851967, 0.38275154236650566, 0.3843942508805214, 0.3868583185839212, 0.3839835730666253, 0.38316221841796466, 0.3860369613528007, 0.38603695998201626, 0.38685831541398225, 0.3802874741123442, 0.3806981540803302, 0.38398357463323607, 0.3848049290860703, 0.3872689922120292, 0.38521560513752934, 0.38521560729161913, 0.38562628510551533, 0.3848049279111122, 0.38521560807492455, 0.38767967139670984, 0.3901437353426916, 0.3843942484938878, 0.3860369607653216]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
