{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf7.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 14:21:04 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '2', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['sk', 'ib', 'mb', 'sg', 'yd', 'eg', 'ce', 'eb', 'my', 'ck', 'ek', 'by', 'aa', 'eo', 'ds'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001E73F91E240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001E73D076EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7076, Accuracy:0.0567, Validation Loss:2.7005, Validation Accuracy:0.0361\n",
    "Epoch #2: Loss:2.6976, Accuracy:0.0583, Validation Loss:2.6914, Validation Accuracy:0.0690\n",
    "Epoch #3: Loss:2.6895, Accuracy:0.0982, Validation Loss:2.6847, Validation Accuracy:0.1067\n",
    "Epoch #4: Loss:2.6828, Accuracy:0.1092, Validation Loss:2.6780, Validation Accuracy:0.1034\n",
    "Epoch #5: Loss:2.6764, Accuracy:0.1027, Validation Loss:2.6724, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6703, Accuracy:0.1023, Validation Loss:2.6675, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6653, Accuracy:0.1023, Validation Loss:2.6622, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6598, Accuracy:0.1023, Validation Loss:2.6562, Validation Accuracy:0.1002\n",
    "Epoch #9: Loss:2.6535, Accuracy:0.1035, Validation Loss:2.6500, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6459, Accuracy:0.1060, Validation Loss:2.6438, Validation Accuracy:0.1051\n",
    "Epoch #11: Loss:2.6382, Accuracy:0.1105, Validation Loss:2.6351, Validation Accuracy:0.1051\n",
    "Epoch #12: Loss:2.6279, Accuracy:0.1195, Validation Loss:2.6235, Validation Accuracy:0.1084\n",
    "Epoch #13: Loss:2.6150, Accuracy:0.1302, Validation Loss:2.6144, Validation Accuracy:0.1199\n",
    "Epoch #14: Loss:2.6010, Accuracy:0.1425, Validation Loss:2.5985, Validation Accuracy:0.1264\n",
    "Epoch #15: Loss:2.5857, Accuracy:0.1409, Validation Loss:2.5877, Validation Accuracy:0.1379\n",
    "Epoch #16: Loss:2.5716, Accuracy:0.1446, Validation Loss:2.5734, Validation Accuracy:0.1412\n",
    "Epoch #17: Loss:2.5525, Accuracy:0.1409, Validation Loss:2.5696, Validation Accuracy:0.1445\n",
    "Epoch #18: Loss:2.5440, Accuracy:0.1495, Validation Loss:2.5665, Validation Accuracy:0.1593\n",
    "Epoch #19: Loss:2.5309, Accuracy:0.1556, Validation Loss:2.5473, Validation Accuracy:0.1642\n",
    "Epoch #20: Loss:2.5223, Accuracy:0.1667, Validation Loss:2.5399, Validation Accuracy:0.1872\n",
    "Epoch #21: Loss:2.5110, Accuracy:0.1762, Validation Loss:2.5332, Validation Accuracy:0.1708\n",
    "Epoch #22: Loss:2.5021, Accuracy:0.1688, Validation Loss:2.5255, Validation Accuracy:0.1790\n",
    "Epoch #23: Loss:2.4939, Accuracy:0.1684, Validation Loss:2.5152, Validation Accuracy:0.1806\n",
    "Epoch #24: Loss:2.4803, Accuracy:0.1692, Validation Loss:2.5075, Validation Accuracy:0.1724\n",
    "Epoch #25: Loss:2.4771, Accuracy:0.1696, Validation Loss:2.4966, Validation Accuracy:0.1921\n",
    "Epoch #26: Loss:2.4720, Accuracy:0.1741, Validation Loss:2.4931, Validation Accuracy:0.1790\n",
    "Epoch #27: Loss:2.4670, Accuracy:0.1692, Validation Loss:2.4953, Validation Accuracy:0.1741\n",
    "Epoch #28: Loss:2.4657, Accuracy:0.1762, Validation Loss:2.4954, Validation Accuracy:0.1691\n",
    "Epoch #29: Loss:2.4660, Accuracy:0.1725, Validation Loss:2.4908, Validation Accuracy:0.1773\n",
    "Epoch #30: Loss:2.4604, Accuracy:0.1762, Validation Loss:2.4892, Validation Accuracy:0.1790\n",
    "Epoch #31: Loss:2.4604, Accuracy:0.1704, Validation Loss:2.4843, Validation Accuracy:0.1724\n",
    "Epoch #32: Loss:2.4562, Accuracy:0.1741, Validation Loss:2.4787, Validation Accuracy:0.1872\n",
    "Epoch #33: Loss:2.4572, Accuracy:0.1733, Validation Loss:2.4752, Validation Accuracy:0.1773\n",
    "Epoch #34: Loss:2.4542, Accuracy:0.1729, Validation Loss:2.4785, Validation Accuracy:0.1806\n",
    "Epoch #35: Loss:2.4517, Accuracy:0.1733, Validation Loss:2.4824, Validation Accuracy:0.1773\n",
    "Epoch #36: Loss:2.4504, Accuracy:0.1729, Validation Loss:2.4797, Validation Accuracy:0.1905\n",
    "Epoch #37: Loss:2.4545, Accuracy:0.1708, Validation Loss:2.4771, Validation Accuracy:0.1790\n",
    "Epoch #38: Loss:2.4503, Accuracy:0.1721, Validation Loss:2.4791, Validation Accuracy:0.1806\n",
    "Epoch #39: Loss:2.4493, Accuracy:0.1717, Validation Loss:2.4787, Validation Accuracy:0.1806\n",
    "Epoch #40: Loss:2.4492, Accuracy:0.1704, Validation Loss:2.4782, Validation Accuracy:0.1773\n",
    "Epoch #41: Loss:2.4490, Accuracy:0.1700, Validation Loss:2.4745, Validation Accuracy:0.1823\n",
    "Epoch #42: Loss:2.4491, Accuracy:0.1803, Validation Loss:2.4690, Validation Accuracy:0.1724\n",
    "Epoch #43: Loss:2.4488, Accuracy:0.1754, Validation Loss:2.4713, Validation Accuracy:0.1790\n",
    "Epoch #44: Loss:2.4493, Accuracy:0.1717, Validation Loss:2.4772, Validation Accuracy:0.1757\n",
    "Epoch #45: Loss:2.4531, Accuracy:0.1774, Validation Loss:2.4830, Validation Accuracy:0.1724\n",
    "Epoch #46: Loss:2.4504, Accuracy:0.1696, Validation Loss:2.4732, Validation Accuracy:0.1757\n",
    "Epoch #47: Loss:2.4466, Accuracy:0.1671, Validation Loss:2.4834, Validation Accuracy:0.1741\n",
    "Epoch #48: Loss:2.4509, Accuracy:0.1692, Validation Loss:2.4927, Validation Accuracy:0.1724\n",
    "Epoch #49: Loss:2.4539, Accuracy:0.1782, Validation Loss:2.4795, Validation Accuracy:0.1856\n",
    "Epoch #50: Loss:2.4490, Accuracy:0.1704, Validation Loss:2.4709, Validation Accuracy:0.1856\n",
    "Epoch #51: Loss:2.4445, Accuracy:0.1721, Validation Loss:2.4732, Validation Accuracy:0.1790\n",
    "Epoch #52: Loss:2.4430, Accuracy:0.1708, Validation Loss:2.4709, Validation Accuracy:0.1806\n",
    "Epoch #53: Loss:2.4437, Accuracy:0.1700, Validation Loss:2.4712, Validation Accuracy:0.1773\n",
    "Epoch #54: Loss:2.4433, Accuracy:0.1713, Validation Loss:2.4718, Validation Accuracy:0.1790\n",
    "Epoch #55: Loss:2.4424, Accuracy:0.1708, Validation Loss:2.4718, Validation Accuracy:0.1872\n",
    "Epoch #56: Loss:2.4409, Accuracy:0.1725, Validation Loss:2.4717, Validation Accuracy:0.1724\n",
    "Epoch #57: Loss:2.4416, Accuracy:0.1684, Validation Loss:2.4717, Validation Accuracy:0.1839\n",
    "Epoch #58: Loss:2.4414, Accuracy:0.1721, Validation Loss:2.4722, Validation Accuracy:0.1856\n",
    "Epoch #59: Loss:2.4405, Accuracy:0.1725, Validation Loss:2.4716, Validation Accuracy:0.1823\n",
    "Epoch #60: Loss:2.4394, Accuracy:0.1717, Validation Loss:2.4731, Validation Accuracy:0.1790\n",
    "Epoch #61: Loss:2.4397, Accuracy:0.1684, Validation Loss:2.4721, Validation Accuracy:0.1806\n",
    "Epoch #62: Loss:2.4397, Accuracy:0.1688, Validation Loss:2.4728, Validation Accuracy:0.1790\n",
    "Epoch #63: Loss:2.4395, Accuracy:0.1717, Validation Loss:2.4732, Validation Accuracy:0.1741\n",
    "Epoch #64: Loss:2.4392, Accuracy:0.1704, Validation Loss:2.4718, Validation Accuracy:0.1872\n",
    "Epoch #65: Loss:2.4410, Accuracy:0.1671, Validation Loss:2.4806, Validation Accuracy:0.1773\n",
    "Epoch #66: Loss:2.4382, Accuracy:0.1749, Validation Loss:2.4827, Validation Accuracy:0.1806\n",
    "Epoch #67: Loss:2.4450, Accuracy:0.1725, Validation Loss:2.4792, Validation Accuracy:0.1724\n",
    "Epoch #68: Loss:2.4472, Accuracy:0.1671, Validation Loss:2.4704, Validation Accuracy:0.1708\n",
    "Epoch #69: Loss:2.4392, Accuracy:0.1737, Validation Loss:2.4715, Validation Accuracy:0.1757\n",
    "Epoch #70: Loss:2.4423, Accuracy:0.1713, Validation Loss:2.4711, Validation Accuracy:0.1741\n",
    "Epoch #71: Loss:2.4409, Accuracy:0.1770, Validation Loss:2.4744, Validation Accuracy:0.1806\n",
    "Epoch #72: Loss:2.4389, Accuracy:0.1729, Validation Loss:2.4710, Validation Accuracy:0.1757\n",
    "Epoch #73: Loss:2.4374, Accuracy:0.1713, Validation Loss:2.4690, Validation Accuracy:0.1741\n",
    "Epoch #74: Loss:2.4394, Accuracy:0.1717, Validation Loss:2.4697, Validation Accuracy:0.1773\n",
    "Epoch #75: Loss:2.4355, Accuracy:0.1733, Validation Loss:2.4722, Validation Accuracy:0.1806\n",
    "Epoch #76: Loss:2.4350, Accuracy:0.1729, Validation Loss:2.4736, Validation Accuracy:0.1806\n",
    "Epoch #77: Loss:2.4353, Accuracy:0.1770, Validation Loss:2.4751, Validation Accuracy:0.1806\n",
    "Epoch #78: Loss:2.4351, Accuracy:0.1770, Validation Loss:2.4727, Validation Accuracy:0.1757\n",
    "Epoch #79: Loss:2.4336, Accuracy:0.1758, Validation Loss:2.4720, Validation Accuracy:0.1741\n",
    "Epoch #80: Loss:2.4337, Accuracy:0.1692, Validation Loss:2.4733, Validation Accuracy:0.1806\n",
    "Epoch #81: Loss:2.4337, Accuracy:0.1696, Validation Loss:2.4722, Validation Accuracy:0.1856\n",
    "Epoch #82: Loss:2.4329, Accuracy:0.1713, Validation Loss:2.4717, Validation Accuracy:0.1856\n",
    "Epoch #83: Loss:2.4339, Accuracy:0.1725, Validation Loss:2.4734, Validation Accuracy:0.1741\n",
    "Epoch #84: Loss:2.4319, Accuracy:0.1745, Validation Loss:2.4757, Validation Accuracy:0.1856\n",
    "Epoch #85: Loss:2.4328, Accuracy:0.1704, Validation Loss:2.4755, Validation Accuracy:0.1773\n",
    "Epoch #86: Loss:2.4331, Accuracy:0.1725, Validation Loss:2.4759, Validation Accuracy:0.1790\n",
    "Epoch #87: Loss:2.4329, Accuracy:0.1717, Validation Loss:2.4771, Validation Accuracy:0.1823\n",
    "Epoch #88: Loss:2.4309, Accuracy:0.1676, Validation Loss:2.4765, Validation Accuracy:0.1757\n",
    "Epoch #89: Loss:2.4318, Accuracy:0.1729, Validation Loss:2.4731, Validation Accuracy:0.1839\n",
    "Epoch #90: Loss:2.4307, Accuracy:0.1778, Validation Loss:2.4720, Validation Accuracy:0.1823\n",
    "Epoch #91: Loss:2.4314, Accuracy:0.1749, Validation Loss:2.4716, Validation Accuracy:0.1806\n",
    "Epoch #92: Loss:2.4305, Accuracy:0.1725, Validation Loss:2.4702, Validation Accuracy:0.1790\n",
    "Epoch #93: Loss:2.4301, Accuracy:0.1737, Validation Loss:2.4709, Validation Accuracy:0.1856\n",
    "Epoch #94: Loss:2.4308, Accuracy:0.1778, Validation Loss:2.4722, Validation Accuracy:0.1938\n",
    "Epoch #95: Loss:2.4299, Accuracy:0.1770, Validation Loss:2.4729, Validation Accuracy:0.1987\n",
    "Epoch #96: Loss:2.4325, Accuracy:0.1766, Validation Loss:2.4717, Validation Accuracy:0.1938\n",
    "Epoch #97: Loss:2.4300, Accuracy:0.1791, Validation Loss:2.4713, Validation Accuracy:0.1921\n",
    "Epoch #98: Loss:2.4301, Accuracy:0.1774, Validation Loss:2.4711, Validation Accuracy:0.1954\n",
    "Epoch #99: Loss:2.4305, Accuracy:0.1758, Validation Loss:2.4694, Validation Accuracy:0.2036\n",
    "Epoch #100: Loss:2.4328, Accuracy:0.1737, Validation Loss:2.4704, Validation Accuracy:0.2020\n",
    "Epoch #101: Loss:2.4294, Accuracy:0.1721, Validation Loss:2.4717, Validation Accuracy:0.1790\n",
    "Epoch #102: Loss:2.4290, Accuracy:0.1782, Validation Loss:2.4725, Validation Accuracy:0.1839\n",
    "Epoch #103: Loss:2.4334, Accuracy:0.1782, Validation Loss:2.4723, Validation Accuracy:0.1905\n",
    "Epoch #104: Loss:2.4337, Accuracy:0.1737, Validation Loss:2.4747, Validation Accuracy:0.1856\n",
    "Epoch #105: Loss:2.4323, Accuracy:0.1819, Validation Loss:2.4778, Validation Accuracy:0.1773\n",
    "Epoch #106: Loss:2.4310, Accuracy:0.1782, Validation Loss:2.4766, Validation Accuracy:0.1773\n",
    "Epoch #107: Loss:2.4317, Accuracy:0.1774, Validation Loss:2.4762, Validation Accuracy:0.1839\n",
    "Epoch #108: Loss:2.4304, Accuracy:0.1791, Validation Loss:2.4765, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.4294, Accuracy:0.1791, Validation Loss:2.4758, Validation Accuracy:0.1823\n",
    "Epoch #110: Loss:2.4309, Accuracy:0.1795, Validation Loss:2.4764, Validation Accuracy:0.1839\n",
    "Epoch #111: Loss:2.4331, Accuracy:0.1782, Validation Loss:2.4753, Validation Accuracy:0.1839\n",
    "Epoch #112: Loss:2.4316, Accuracy:0.1795, Validation Loss:2.4765, Validation Accuracy:0.1839\n",
    "Epoch #113: Loss:2.4299, Accuracy:0.1786, Validation Loss:2.4734, Validation Accuracy:0.1823\n",
    "Epoch #114: Loss:2.4289, Accuracy:0.1717, Validation Loss:2.4736, Validation Accuracy:0.1839\n",
    "Epoch #115: Loss:2.4288, Accuracy:0.1778, Validation Loss:2.4729, Validation Accuracy:0.1856\n",
    "Epoch #116: Loss:2.4274, Accuracy:0.1778, Validation Loss:2.4743, Validation Accuracy:0.1856\n",
    "Epoch #117: Loss:2.4286, Accuracy:0.1749, Validation Loss:2.4717, Validation Accuracy:0.1905\n",
    "Epoch #118: Loss:2.4292, Accuracy:0.1733, Validation Loss:2.4722, Validation Accuracy:0.1856\n",
    "Epoch #119: Loss:2.4268, Accuracy:0.1762, Validation Loss:2.4715, Validation Accuracy:0.1839\n",
    "Epoch #120: Loss:2.4274, Accuracy:0.1745, Validation Loss:2.4708, Validation Accuracy:0.1839\n",
    "Epoch #121: Loss:2.4262, Accuracy:0.1766, Validation Loss:2.4716, Validation Accuracy:0.1888\n",
    "Epoch #122: Loss:2.4269, Accuracy:0.1803, Validation Loss:2.4709, Validation Accuracy:0.1872\n",
    "Epoch #123: Loss:2.4275, Accuracy:0.1778, Validation Loss:2.4725, Validation Accuracy:0.1856\n",
    "Epoch #124: Loss:2.4259, Accuracy:0.1782, Validation Loss:2.4756, Validation Accuracy:0.1839\n",
    "Epoch #125: Loss:2.4274, Accuracy:0.1786, Validation Loss:2.4757, Validation Accuracy:0.1823\n",
    "Epoch #126: Loss:2.4271, Accuracy:0.1766, Validation Loss:2.4760, Validation Accuracy:0.1806\n",
    "Epoch #127: Loss:2.4263, Accuracy:0.1758, Validation Loss:2.4771, Validation Accuracy:0.1823\n",
    "Epoch #128: Loss:2.4266, Accuracy:0.1778, Validation Loss:2.4754, Validation Accuracy:0.1823\n",
    "Epoch #129: Loss:2.4261, Accuracy:0.1782, Validation Loss:2.4730, Validation Accuracy:0.1888\n",
    "Epoch #130: Loss:2.4279, Accuracy:0.1811, Validation Loss:2.4734, Validation Accuracy:0.1823\n",
    "Epoch #131: Loss:2.4287, Accuracy:0.1762, Validation Loss:2.4744, Validation Accuracy:0.1823\n",
    "Epoch #132: Loss:2.4264, Accuracy:0.1786, Validation Loss:2.4715, Validation Accuracy:0.1806\n",
    "Epoch #133: Loss:2.4264, Accuracy:0.1799, Validation Loss:2.4721, Validation Accuracy:0.1806\n",
    "Epoch #134: Loss:2.4257, Accuracy:0.1766, Validation Loss:2.4746, Validation Accuracy:0.1888\n",
    "Epoch #135: Loss:2.4262, Accuracy:0.1749, Validation Loss:2.4758, Validation Accuracy:0.1856\n",
    "Epoch #136: Loss:2.4279, Accuracy:0.1791, Validation Loss:2.4773, Validation Accuracy:0.1806\n",
    "Epoch #137: Loss:2.4247, Accuracy:0.1791, Validation Loss:2.4786, Validation Accuracy:0.1872\n",
    "Epoch #138: Loss:2.4257, Accuracy:0.1745, Validation Loss:2.4764, Validation Accuracy:0.1757\n",
    "Epoch #139: Loss:2.4255, Accuracy:0.1795, Validation Loss:2.4776, Validation Accuracy:0.1856\n",
    "Epoch #140: Loss:2.4242, Accuracy:0.1823, Validation Loss:2.4758, Validation Accuracy:0.1888\n",
    "Epoch #141: Loss:2.4250, Accuracy:0.1762, Validation Loss:2.4737, Validation Accuracy:0.1806\n",
    "Epoch #142: Loss:2.4262, Accuracy:0.1807, Validation Loss:2.4740, Validation Accuracy:0.1773\n",
    "Epoch #143: Loss:2.4253, Accuracy:0.1815, Validation Loss:2.4789, Validation Accuracy:0.1839\n",
    "Epoch #144: Loss:2.4249, Accuracy:0.1778, Validation Loss:2.4821, Validation Accuracy:0.1741\n",
    "Epoch #145: Loss:2.4242, Accuracy:0.1799, Validation Loss:2.4795, Validation Accuracy:0.1856\n",
    "Epoch #146: Loss:2.4238, Accuracy:0.1815, Validation Loss:2.4810, Validation Accuracy:0.1806\n",
    "Epoch #147: Loss:2.4239, Accuracy:0.1860, Validation Loss:2.4812, Validation Accuracy:0.1790\n",
    "Epoch #148: Loss:2.4252, Accuracy:0.1848, Validation Loss:2.4803, Validation Accuracy:0.1806\n",
    "Epoch #149: Loss:2.4284, Accuracy:0.1803, Validation Loss:2.4771, Validation Accuracy:0.1806\n",
    "Epoch #150: Loss:2.4282, Accuracy:0.1791, Validation Loss:2.4763, Validation Accuracy:0.1790\n",
    "Epoch #151: Loss:2.4275, Accuracy:0.1774, Validation Loss:2.4726, Validation Accuracy:0.1839\n",
    "Epoch #152: Loss:2.4267, Accuracy:0.1799, Validation Loss:2.4724, Validation Accuracy:0.1856\n",
    "Epoch #153: Loss:2.4259, Accuracy:0.1766, Validation Loss:2.4713, Validation Accuracy:0.1790\n",
    "Epoch #154: Loss:2.4248, Accuracy:0.1807, Validation Loss:2.4721, Validation Accuracy:0.1757\n",
    "Epoch #155: Loss:2.4252, Accuracy:0.1795, Validation Loss:2.4767, Validation Accuracy:0.1872\n",
    "Epoch #156: Loss:2.4241, Accuracy:0.1807, Validation Loss:2.4759, Validation Accuracy:0.1741\n",
    "Epoch #157: Loss:2.4245, Accuracy:0.1823, Validation Loss:2.4761, Validation Accuracy:0.1790\n",
    "Epoch #158: Loss:2.4244, Accuracy:0.1803, Validation Loss:2.4743, Validation Accuracy:0.1773\n",
    "Epoch #159: Loss:2.4240, Accuracy:0.1807, Validation Loss:2.4724, Validation Accuracy:0.1773\n",
    "Epoch #160: Loss:2.4251, Accuracy:0.1844, Validation Loss:2.4753, Validation Accuracy:0.1757\n",
    "Epoch #161: Loss:2.4260, Accuracy:0.1844, Validation Loss:2.4759, Validation Accuracy:0.1773\n",
    "Epoch #162: Loss:2.4265, Accuracy:0.1828, Validation Loss:2.4727, Validation Accuracy:0.1856\n",
    "Epoch #163: Loss:2.4256, Accuracy:0.1799, Validation Loss:2.4763, Validation Accuracy:0.1839\n",
    "Epoch #164: Loss:2.4269, Accuracy:0.1811, Validation Loss:2.4742, Validation Accuracy:0.1905\n",
    "Epoch #165: Loss:2.4236, Accuracy:0.1832, Validation Loss:2.4711, Validation Accuracy:0.1921\n",
    "Epoch #166: Loss:2.4263, Accuracy:0.1807, Validation Loss:2.4728, Validation Accuracy:0.1938\n",
    "Epoch #167: Loss:2.4305, Accuracy:0.1762, Validation Loss:2.4712, Validation Accuracy:0.1856\n",
    "Epoch #168: Loss:2.4309, Accuracy:0.1778, Validation Loss:2.4744, Validation Accuracy:0.1675\n",
    "Epoch #169: Loss:2.4257, Accuracy:0.1828, Validation Loss:2.4716, Validation Accuracy:0.1839\n",
    "Epoch #170: Loss:2.4268, Accuracy:0.1758, Validation Loss:2.4707, Validation Accuracy:0.1905\n",
    "Epoch #171: Loss:2.4298, Accuracy:0.1815, Validation Loss:2.4731, Validation Accuracy:0.1773\n",
    "Epoch #172: Loss:2.4246, Accuracy:0.1819, Validation Loss:2.4754, Validation Accuracy:0.1888\n",
    "Epoch #173: Loss:2.4251, Accuracy:0.1803, Validation Loss:2.4735, Validation Accuracy:0.1757\n",
    "Epoch #174: Loss:2.4220, Accuracy:0.1869, Validation Loss:2.4730, Validation Accuracy:0.1921\n",
    "Epoch #175: Loss:2.4229, Accuracy:0.1778, Validation Loss:2.4715, Validation Accuracy:0.1970\n",
    "Epoch #176: Loss:2.4231, Accuracy:0.1823, Validation Loss:2.4758, Validation Accuracy:0.1724\n",
    "Epoch #177: Loss:2.4236, Accuracy:0.1791, Validation Loss:2.4700, Validation Accuracy:0.1921\n",
    "Epoch #178: Loss:2.4255, Accuracy:0.1782, Validation Loss:2.4669, Validation Accuracy:0.1888\n",
    "Epoch #179: Loss:2.4257, Accuracy:0.1778, Validation Loss:2.4652, Validation Accuracy:0.1856\n",
    "Epoch #180: Loss:2.4295, Accuracy:0.1791, Validation Loss:2.4719, Validation Accuracy:0.1921\n",
    "Epoch #181: Loss:2.4346, Accuracy:0.1799, Validation Loss:2.4706, Validation Accuracy:0.1806\n",
    "Epoch #182: Loss:2.4317, Accuracy:0.1791, Validation Loss:2.4675, Validation Accuracy:0.1823\n",
    "Epoch #183: Loss:2.4254, Accuracy:0.1823, Validation Loss:2.4694, Validation Accuracy:0.1938\n",
    "Epoch #184: Loss:2.4227, Accuracy:0.1770, Validation Loss:2.4684, Validation Accuracy:0.1921\n",
    "Epoch #185: Loss:2.4222, Accuracy:0.1791, Validation Loss:2.4660, Validation Accuracy:0.1856\n",
    "Epoch #186: Loss:2.4228, Accuracy:0.1840, Validation Loss:2.4682, Validation Accuracy:0.1823\n",
    "Epoch #187: Loss:2.4214, Accuracy:0.1823, Validation Loss:2.4731, Validation Accuracy:0.1938\n",
    "Epoch #188: Loss:2.4228, Accuracy:0.1762, Validation Loss:2.4744, Validation Accuracy:0.1954\n",
    "Epoch #189: Loss:2.4205, Accuracy:0.1766, Validation Loss:2.4722, Validation Accuracy:0.1970\n",
    "Epoch #190: Loss:2.4216, Accuracy:0.1832, Validation Loss:2.4731, Validation Accuracy:0.1938\n",
    "Epoch #191: Loss:2.4210, Accuracy:0.1807, Validation Loss:2.4727, Validation Accuracy:0.1921\n",
    "Epoch #192: Loss:2.4194, Accuracy:0.1832, Validation Loss:2.4696, Validation Accuracy:0.1888\n",
    "Epoch #193: Loss:2.4185, Accuracy:0.1832, Validation Loss:2.4682, Validation Accuracy:0.1856\n",
    "Epoch #194: Loss:2.4198, Accuracy:0.1786, Validation Loss:2.4726, Validation Accuracy:0.1888\n",
    "Epoch #195: Loss:2.4187, Accuracy:0.1848, Validation Loss:2.4771, Validation Accuracy:0.1872\n",
    "Epoch #196: Loss:2.4191, Accuracy:0.1811, Validation Loss:2.4791, Validation Accuracy:0.1905\n",
    "Epoch #197: Loss:2.4185, Accuracy:0.1815, Validation Loss:2.4794, Validation Accuracy:0.1888\n",
    "Epoch #198: Loss:2.4200, Accuracy:0.1782, Validation Loss:2.4806, Validation Accuracy:0.1905\n",
    "Epoch #199: Loss:2.4197, Accuracy:0.1754, Validation Loss:2.4833, Validation Accuracy:0.1888\n",
    "Epoch #200: Loss:2.4247, Accuracy:0.1770, Validation Loss:2.4802, Validation Accuracy:0.1888\n",
    "Epoch #201: Loss:2.4222, Accuracy:0.1819, Validation Loss:2.4797, Validation Accuracy:0.1856\n",
    "Epoch #202: Loss:2.4213, Accuracy:0.1844, Validation Loss:2.4784, Validation Accuracy:0.1905\n",
    "Epoch #203: Loss:2.4196, Accuracy:0.1799, Validation Loss:2.4763, Validation Accuracy:0.1921\n",
    "Epoch #204: Loss:2.4207, Accuracy:0.1791, Validation Loss:2.4777, Validation Accuracy:0.1856\n",
    "Epoch #205: Loss:2.4205, Accuracy:0.1807, Validation Loss:2.4788, Validation Accuracy:0.1921\n",
    "Epoch #206: Loss:2.4196, Accuracy:0.1799, Validation Loss:2.4778, Validation Accuracy:0.1905\n",
    "Epoch #207: Loss:2.4217, Accuracy:0.1819, Validation Loss:2.4786, Validation Accuracy:0.1806\n",
    "Epoch #208: Loss:2.4221, Accuracy:0.1778, Validation Loss:2.4774, Validation Accuracy:0.1954\n",
    "Epoch #209: Loss:2.4213, Accuracy:0.1774, Validation Loss:2.4789, Validation Accuracy:0.1806\n",
    "Epoch #210: Loss:2.4198, Accuracy:0.1852, Validation Loss:2.4767, Validation Accuracy:0.1888\n",
    "Epoch #211: Loss:2.4185, Accuracy:0.1803, Validation Loss:2.4793, Validation Accuracy:0.1938\n",
    "Epoch #212: Loss:2.4190, Accuracy:0.1786, Validation Loss:2.4793, Validation Accuracy:0.1757\n",
    "Epoch #213: Loss:2.4176, Accuracy:0.1885, Validation Loss:2.4744, Validation Accuracy:0.1905\n",
    "Epoch #214: Loss:2.4186, Accuracy:0.1803, Validation Loss:2.4760, Validation Accuracy:0.1872\n",
    "Epoch #215: Loss:2.4197, Accuracy:0.1819, Validation Loss:2.4734, Validation Accuracy:0.1806\n",
    "Epoch #216: Loss:2.4176, Accuracy:0.1807, Validation Loss:2.4737, Validation Accuracy:0.1888\n",
    "Epoch #217: Loss:2.4200, Accuracy:0.1815, Validation Loss:2.4728, Validation Accuracy:0.1839\n",
    "Epoch #218: Loss:2.4209, Accuracy:0.1811, Validation Loss:2.4697, Validation Accuracy:0.1724\n",
    "Epoch #219: Loss:2.4204, Accuracy:0.1828, Validation Loss:2.4746, Validation Accuracy:0.1872\n",
    "Epoch #220: Loss:2.4214, Accuracy:0.1844, Validation Loss:2.4718, Validation Accuracy:0.1938\n",
    "Epoch #221: Loss:2.4198, Accuracy:0.1844, Validation Loss:2.4743, Validation Accuracy:0.1888\n",
    "Epoch #222: Loss:2.4182, Accuracy:0.1807, Validation Loss:2.4737, Validation Accuracy:0.1839\n",
    "Epoch #223: Loss:2.4193, Accuracy:0.1828, Validation Loss:2.4722, Validation Accuracy:0.1856\n",
    "Epoch #224: Loss:2.4181, Accuracy:0.1869, Validation Loss:2.4750, Validation Accuracy:0.1839\n",
    "Epoch #225: Loss:2.4187, Accuracy:0.1852, Validation Loss:2.4718, Validation Accuracy:0.1905\n",
    "Epoch #226: Loss:2.4193, Accuracy:0.1844, Validation Loss:2.4733, Validation Accuracy:0.1938\n",
    "Epoch #227: Loss:2.4195, Accuracy:0.1869, Validation Loss:2.4708, Validation Accuracy:0.1938\n",
    "Epoch #228: Loss:2.4185, Accuracy:0.1869, Validation Loss:2.4709, Validation Accuracy:0.1806\n",
    "Epoch #229: Loss:2.4173, Accuracy:0.1951, Validation Loss:2.4693, Validation Accuracy:0.1856\n",
    "Epoch #230: Loss:2.4179, Accuracy:0.1860, Validation Loss:2.4687, Validation Accuracy:0.1905\n",
    "Epoch #231: Loss:2.4170, Accuracy:0.1844, Validation Loss:2.4690, Validation Accuracy:0.1905\n",
    "Epoch #232: Loss:2.4177, Accuracy:0.1848, Validation Loss:2.4701, Validation Accuracy:0.1954\n",
    "Epoch #233: Loss:2.4182, Accuracy:0.1881, Validation Loss:2.4726, Validation Accuracy:0.1954\n",
    "Epoch #234: Loss:2.4185, Accuracy:0.1844, Validation Loss:2.4711, Validation Accuracy:0.1888\n",
    "Epoch #235: Loss:2.4193, Accuracy:0.1852, Validation Loss:2.4719, Validation Accuracy:0.1921\n",
    "Epoch #236: Loss:2.4197, Accuracy:0.1836, Validation Loss:2.4707, Validation Accuracy:0.1905\n",
    "Epoch #237: Loss:2.4191, Accuracy:0.1836, Validation Loss:2.4714, Validation Accuracy:0.1888\n",
    "Epoch #238: Loss:2.4199, Accuracy:0.1823, Validation Loss:2.4696, Validation Accuracy:0.1905\n",
    "Epoch #239: Loss:2.4185, Accuracy:0.1885, Validation Loss:2.4721, Validation Accuracy:0.1938\n",
    "Epoch #240: Loss:2.4168, Accuracy:0.1881, Validation Loss:2.4721, Validation Accuracy:0.1938\n",
    "Epoch #241: Loss:2.4166, Accuracy:0.1869, Validation Loss:2.4685, Validation Accuracy:0.1954\n",
    "Epoch #242: Loss:2.4161, Accuracy:0.1828, Validation Loss:2.4695, Validation Accuracy:0.1938\n",
    "Epoch #243: Loss:2.4156, Accuracy:0.1864, Validation Loss:2.4710, Validation Accuracy:0.1970\n",
    "Epoch #244: Loss:2.4151, Accuracy:0.1844, Validation Loss:2.4709, Validation Accuracy:0.1954\n",
    "Epoch #245: Loss:2.4164, Accuracy:0.1860, Validation Loss:2.4736, Validation Accuracy:0.1872\n",
    "Epoch #246: Loss:2.4164, Accuracy:0.1877, Validation Loss:2.4728, Validation Accuracy:0.1872\n",
    "Epoch #247: Loss:2.4150, Accuracy:0.1885, Validation Loss:2.4718, Validation Accuracy:0.1888\n",
    "Epoch #248: Loss:2.4155, Accuracy:0.1869, Validation Loss:2.4722, Validation Accuracy:0.1872\n",
    "Epoch #249: Loss:2.4141, Accuracy:0.1864, Validation Loss:2.4716, Validation Accuracy:0.1921\n",
    "Epoch #250: Loss:2.4145, Accuracy:0.1852, Validation Loss:2.4687, Validation Accuracy:0.1888\n",
    "Epoch #251: Loss:2.4148, Accuracy:0.1848, Validation Loss:2.4710, Validation Accuracy:0.1823\n",
    "Epoch #252: Loss:2.4157, Accuracy:0.1840, Validation Loss:2.4724, Validation Accuracy:0.1888\n",
    "Epoch #253: Loss:2.4147, Accuracy:0.1774, Validation Loss:2.4731, Validation Accuracy:0.1921\n",
    "Epoch #254: Loss:2.4150, Accuracy:0.1840, Validation Loss:2.4750, Validation Accuracy:0.1954\n",
    "Epoch #255: Loss:2.4135, Accuracy:0.1840, Validation Loss:2.4756, Validation Accuracy:0.1954\n",
    "Epoch #256: Loss:2.4139, Accuracy:0.1864, Validation Loss:2.4779, Validation Accuracy:0.1921\n",
    "Epoch #257: Loss:2.4132, Accuracy:0.1893, Validation Loss:2.4788, Validation Accuracy:0.1938\n",
    "Epoch #258: Loss:2.4126, Accuracy:0.1877, Validation Loss:2.4815, Validation Accuracy:0.1987\n",
    "Epoch #259: Loss:2.4137, Accuracy:0.1860, Validation Loss:2.4802, Validation Accuracy:0.1905\n",
    "Epoch #260: Loss:2.4134, Accuracy:0.1856, Validation Loss:2.4803, Validation Accuracy:0.1905\n",
    "Epoch #261: Loss:2.4137, Accuracy:0.1873, Validation Loss:2.4827, Validation Accuracy:0.1921\n",
    "Epoch #262: Loss:2.4155, Accuracy:0.1881, Validation Loss:2.4805, Validation Accuracy:0.1921\n",
    "Epoch #263: Loss:2.4169, Accuracy:0.1852, Validation Loss:2.4802, Validation Accuracy:0.1839\n",
    "Epoch #264: Loss:2.4155, Accuracy:0.1811, Validation Loss:2.4794, Validation Accuracy:0.1856\n",
    "Epoch #265: Loss:2.4164, Accuracy:0.1885, Validation Loss:2.4800, Validation Accuracy:0.1856\n",
    "Epoch #266: Loss:2.4156, Accuracy:0.1848, Validation Loss:2.4776, Validation Accuracy:0.1888\n",
    "Epoch #267: Loss:2.4145, Accuracy:0.1832, Validation Loss:2.4759, Validation Accuracy:0.1872\n",
    "Epoch #268: Loss:2.4133, Accuracy:0.1848, Validation Loss:2.4782, Validation Accuracy:0.1872\n",
    "Epoch #269: Loss:2.4131, Accuracy:0.1864, Validation Loss:2.4807, Validation Accuracy:0.1790\n",
    "Epoch #270: Loss:2.4136, Accuracy:0.1844, Validation Loss:2.4829, Validation Accuracy:0.1806\n",
    "Epoch #271: Loss:2.4135, Accuracy:0.1869, Validation Loss:2.4833, Validation Accuracy:0.1823\n",
    "Epoch #272: Loss:2.4154, Accuracy:0.1819, Validation Loss:2.4828, Validation Accuracy:0.1823\n",
    "Epoch #273: Loss:2.4146, Accuracy:0.1869, Validation Loss:2.4850, Validation Accuracy:0.1888\n",
    "Epoch #274: Loss:2.4150, Accuracy:0.1828, Validation Loss:2.4850, Validation Accuracy:0.1839\n",
    "Epoch #275: Loss:2.4151, Accuracy:0.1860, Validation Loss:2.4844, Validation Accuracy:0.1823\n",
    "Epoch #276: Loss:2.4140, Accuracy:0.1856, Validation Loss:2.4840, Validation Accuracy:0.1806\n",
    "Epoch #277: Loss:2.4142, Accuracy:0.1823, Validation Loss:2.4847, Validation Accuracy:0.1888\n",
    "Epoch #278: Loss:2.4134, Accuracy:0.1873, Validation Loss:2.4877, Validation Accuracy:0.1806\n",
    "Epoch #279: Loss:2.4129, Accuracy:0.1823, Validation Loss:2.4834, Validation Accuracy:0.1839\n",
    "Epoch #280: Loss:2.4133, Accuracy:0.1881, Validation Loss:2.4800, Validation Accuracy:0.1872\n",
    "Epoch #281: Loss:2.4117, Accuracy:0.1869, Validation Loss:2.4816, Validation Accuracy:0.1823\n",
    "Epoch #282: Loss:2.4121, Accuracy:0.1856, Validation Loss:2.4822, Validation Accuracy:0.1823\n",
    "Epoch #283: Loss:2.4120, Accuracy:0.1795, Validation Loss:2.4822, Validation Accuracy:0.1987\n",
    "Epoch #284: Loss:2.4117, Accuracy:0.1811, Validation Loss:2.4836, Validation Accuracy:0.1905\n",
    "Epoch #285: Loss:2.4106, Accuracy:0.1869, Validation Loss:2.4830, Validation Accuracy:0.1938\n",
    "Epoch #286: Loss:2.4154, Accuracy:0.1856, Validation Loss:2.4827, Validation Accuracy:0.1905\n",
    "Epoch #287: Loss:2.4126, Accuracy:0.1848, Validation Loss:2.4824, Validation Accuracy:0.1954\n",
    "Epoch #288: Loss:2.4126, Accuracy:0.1848, Validation Loss:2.4818, Validation Accuracy:0.1954\n",
    "Epoch #289: Loss:2.4115, Accuracy:0.1881, Validation Loss:2.4843, Validation Accuracy:0.1905\n",
    "Epoch #290: Loss:2.4104, Accuracy:0.1848, Validation Loss:2.4826, Validation Accuracy:0.1823\n",
    "Epoch #291: Loss:2.4103, Accuracy:0.1852, Validation Loss:2.4818, Validation Accuracy:0.1954\n",
    "Epoch #292: Loss:2.4092, Accuracy:0.1860, Validation Loss:2.4837, Validation Accuracy:0.1839\n",
    "Epoch #293: Loss:2.4097, Accuracy:0.1885, Validation Loss:2.4825, Validation Accuracy:0.1872\n",
    "Epoch #294: Loss:2.4108, Accuracy:0.1864, Validation Loss:2.4819, Validation Accuracy:0.1905\n",
    "Epoch #295: Loss:2.4103, Accuracy:0.1860, Validation Loss:2.4818, Validation Accuracy:0.1806\n",
    "Epoch #296: Loss:2.4096, Accuracy:0.1901, Validation Loss:2.4828, Validation Accuracy:0.1938\n",
    "Epoch #297: Loss:2.4094, Accuracy:0.1869, Validation Loss:2.4817, Validation Accuracy:0.1888\n",
    "Epoch #298: Loss:2.4122, Accuracy:0.1856, Validation Loss:2.4851, Validation Accuracy:0.1921\n",
    "Epoch #299: Loss:2.4097, Accuracy:0.1881, Validation Loss:2.4797, Validation Accuracy:0.1823\n",
    "Epoch #300: Loss:2.4091, Accuracy:0.1848, Validation Loss:2.4782, Validation Accuracy:0.1938\n",
    "\n",
    "Test:\n",
    "Test Loss:2.47822499, Accuracy:0.1938\n",
    "Labels: ['sk', 'ib', 'mb', 'sg', 'yd', 'eg', 'ce', 'eb', 'my', 'ck', 'ek', 'by', 'aa', 'eo', 'ds']\n",
    "Confusion Matrix:\n",
    "      sk  ib  mb  sg  yd  eg  ce  eb  my  ck  ek  by  aa  eo  ds\n",
    "t:sk   0   1   0   4   5  15   0   0   0   0   0   7   0   0   1\n",
    "t:ib   0   0   0  18  26   4   0   0   0   0   0   6   0   0   0\n",
    "t:mb   0   1   0  20   4  14   0   0   0   0   0  11   0   0   2\n",
    "t:sg   0   1   0  32   5   5   0   0   0   0   0   7   0   1   0\n",
    "t:yd   0   1   0  19  33   4   0   0   0   0   0   5   0   0   0\n",
    "t:eg   0   0   0   8   0  31   0   0   0   0   0   6   1   0   4\n",
    "t:ce   0   0   0  10   3   7   0   0   0   0   0   6   1   0   0\n",
    "t:eb   0   1   0   9  11  14   0   0   0   0   0  14   0   0   1\n",
    "t:my   0   0   0   2   6   5   0   0   0   0   0   4   0   0   3\n",
    "t:ck   0   0   0   3   2  10   0   0   0   0   0   5   0   0   3\n",
    "t:ek   0   2   0  13   5  13   0   0   0   0   0  14   0   0   1\n",
    "t:by   0   0   0  12   2  11   0   0   0   0   0  15   0   0   0\n",
    "t:aa   0   0   0   4   1  20   0   0   0   0   0   5   0   0   4\n",
    "t:eo   0   0   0  10   2   9   0   0   0   0   0  13   0   0   0\n",
    "t:ds   0   0   0   3   0  16   0   0   0   0   0   5   0   0   7\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          sg       0.19      0.63      0.29        51\n",
    "          yd       0.31      0.53      0.40        62\n",
    "          eg       0.17      0.62      0.27        50\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          by       0.12      0.38      0.18        40\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ds       0.27      0.23      0.25        31\n",
    "\n",
    "    accuracy                           0.19       609\n",
    "   macro avg       0.07      0.16      0.09       609\n",
    "weighted avg       0.08      0.19      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 14:36:40 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.700549036019737, 2.6914390221800906, 2.6846650226167075, 2.6779986614076963, 2.67241826981355, 2.6674701298398924, 2.662190965830986, 2.6561654029025625, 2.6499521141177524, 2.643762361827155, 2.6351355492383584, 2.6235417779443297, 2.614426211574786, 2.598484444109286, 2.587699192106626, 2.573430320507983, 2.569609454895671, 2.566538051235656, 2.5472639546605755, 2.539919137954712, 2.5332085796569173, 2.525464672564677, 2.5152116734014554, 2.507485563531885, 2.4966202788360796, 2.4930784811918763, 2.495335266116413, 2.4953601595216197, 2.490825953350474, 2.489180146767001, 2.484269104567655, 2.478728949730032, 2.475166992992407, 2.4785211614787284, 2.482416515084127, 2.479744163835773, 2.4771436255162183, 2.4791243052834946, 2.4786891138612344, 2.478228537711408, 2.4744644904958792, 2.4690450889919386, 2.4713190478840095, 2.477206836193066, 2.4829614659639807, 2.4732223222604133, 2.4834422012072284, 2.492672141941114, 2.4795181633803645, 2.4708586071903875, 2.4732270604871176, 2.4708551370060112, 2.4711758240886117, 2.4717692279659076, 2.471795783254313, 2.471683670930283, 2.471727894249025, 2.47216214843963, 2.4715855062888763, 2.4730501233650544, 2.4721461767437813, 2.4727811519735554, 2.4731915545189516, 2.471767894544429, 2.4805929880032593, 2.4827398173327517, 2.4792071214841895, 2.4704337456739203, 2.4715293929690407, 2.4711472483104084, 2.4744414923030558, 2.471018291655041, 2.4690199712618623, 2.4696789727422406, 2.4722455546186475, 2.473626600306218, 2.4751294144660188, 2.4726873626458428, 2.47204003741197, 2.473330044002564, 2.472151774686741, 2.4716704432013, 2.4733611045799817, 2.4756586277621917, 2.4754736666217423, 2.4758698302145272, 2.477060942422776, 2.4765415320842723, 2.473133789886199, 2.4720136911802495, 2.471600983138938, 2.470210280911676, 2.4708715620495023, 2.472192757626864, 2.472932884062844, 2.4716526620297987, 2.471344466671372, 2.471125168949121, 2.4694065718815246, 2.4703967281554524, 2.471739703602783, 2.4725111975458454, 2.472328037268227, 2.4746669849934446, 2.4777988710231185, 2.4765787778425294, 2.476170272466976, 2.4765182007318254, 2.4758461786216897, 2.47644004837437, 2.4752913934648135, 2.4764509146240936, 2.4734026234725426, 2.473618093578295, 2.4729168188004267, 2.4743204766697877, 2.471720988723053, 2.472231435462563, 2.4714537211043885, 2.4707583073520505, 2.4716200546678064, 2.4708966833030064, 2.4725206665609076, 2.475581125672815, 2.4756953900279277, 2.476030794074774, 2.477077349066147, 2.4753899973601543, 2.472960730100109, 2.4733671331640537, 2.4743618812467076, 2.471501166010138, 2.4720513460475626, 2.474602763875952, 2.475783070124233, 2.477255311114056, 2.478624428825817, 2.4763509741753387, 2.4775660378592357, 2.4757899325860935, 2.473726008325962, 2.4740219331531494, 2.4789470510529767, 2.4821432231878022, 2.4794881825376613, 2.481007545060908, 2.4812293988339027, 2.480298739544472, 2.4770987762018963, 2.476335093697108, 2.4725637549445745, 2.4724008637696064, 2.4713411937989234, 2.472149860291254, 2.476671493307906, 2.4758509599125054, 2.476111753429294, 2.474281258966731, 2.4724316197663105, 2.4752755368675894, 2.475899708877839, 2.472671271544959, 2.476335589325878, 2.474230096453712, 2.4710971353872266, 2.4728371650714593, 2.47122473708906, 2.4744466482516385, 2.4716097756559625, 2.470731771246749, 2.473090991206553, 2.4754393703636084, 2.4735132665273984, 2.473016123074812, 2.4715314565229494, 2.475767581529414, 2.4699676306964142, 2.4668789219190725, 2.465187555463443, 2.4719334416005805, 2.470622907522668, 2.4675445462682566, 2.4693861677141613, 2.468403970079469, 2.4660444925179816, 2.4681773158325546, 2.4730647377584174, 2.474373439654145, 2.472238390316517, 2.4730694802915325, 2.4726914357277754, 2.4695723088112564, 2.468218992104867, 2.472638479007289, 2.47710686285899, 2.479117683197673, 2.479398118647057, 2.4805627906655245, 2.4833231767018638, 2.480174714121325, 2.479719552304749, 2.4783846067481834, 2.4763152982996797, 2.4777419453575495, 2.478812840184555, 2.4778213422678177, 2.478602463388678, 2.477424131042656, 2.4789249160998366, 2.4767072349542074, 2.479309644605139, 2.479346229525035, 2.4744078028573973, 2.4760016843015924, 2.47337214308615, 2.4736561771292602, 2.472824357804798, 2.4696684118562144, 2.4745511260917428, 2.4718333786148547, 2.474266344299066, 2.4737156114750505, 2.4722395416942526, 2.4750121555892117, 2.4718460880085362, 2.4732921291846166, 2.470795156137501, 2.470867815080339, 2.469348015260618, 2.4687254565885697, 2.4689672032404806, 2.47014264128674, 2.472554827362837, 2.471063372732579, 2.4718873700484854, 2.470737668681027, 2.4713535140496363, 2.469558449997299, 2.4721291178748723, 2.472083787808473, 2.468459763941898, 2.4695080853448124, 2.470960582222649, 2.4709115251531744, 2.473573849510481, 2.472820783875063, 2.4717990392926095, 2.472205259529828, 2.4716393873217855, 2.468745577315783, 2.470977723696353, 2.4723746772665893, 2.473069028901349, 2.475004098685504, 2.4755568054117787, 2.477907463443299, 2.478814170082606, 2.4815099936204983, 2.480204096764375, 2.4802750554578057, 2.482651204311202, 2.4804647340758876, 2.4802218380037004, 2.479367416284746, 2.479956123824973, 2.47764090873142, 2.475912390084102, 2.4782195803762854, 2.4806529751356403, 2.4829284087777723, 2.4833439927187264, 2.4828240648279047, 2.484991480759995, 2.4849567280222824, 2.484397491210787, 2.4839825082099303, 2.484708514706842, 2.487679728929241, 2.483405467911894, 2.4799853768841973, 2.4815730073769102, 2.4822308292921345, 2.48215537313953, 2.483635862864101, 2.4830346142717183, 2.4826520616785057, 2.4824431651135774, 2.4818221313025566, 2.484312536289735, 2.4826254183044183, 2.481780938522765, 2.4837072177473547, 2.4824724158238505, 2.4819312397090867, 2.481791293875533, 2.4828332394410433, 2.481742795073536, 2.4850919974848553, 2.4796553444979814, 2.478225161093601], 'val_acc': [0.03612479474242587, 0.06896551683765327, 0.10673234790367837, 0.1034482755562159, 0.10180623943142116, 0.10180623943142116, 0.10180623943142116, 0.10016420330662641, 0.10180623952929414, 0.10509031177888363, 0.10509031168101064, 0.1083743841263461, 0.11986863699990932, 0.12643678159696128, 0.1379310344705245, 0.1412151056312771, 0.1444991789697035, 0.15927750310189226, 0.16420361147627652, 0.1871921173212759, 0.17077175607332848, 0.1789819366973022, 0.18062397291996993, 0.17241379299334117, 0.19211822579353313, 0.17898193679517518, 0.17405582832291797, 0.1691297199485337, 0.17733990057250745, 0.17898193679517518, 0.17241379219812322, 0.1871921174191489, 0.17733990047463447, 0.18062397272422395, 0.17733990037676148, 0.19047618937511945, 0.1789819365994292, 0.18062397272422395, 0.18062397272422395, 0.17733990037676148, 0.18226600875114574, 0.17241379200237725, 0.17898193650155622, 0.17569786434983972, 0.17241379210025023, 0.17569786454558567, 0.1740558285186639, 0.17241379210025023, 0.18555008129435416, 0.18555008119648117, 0.1789819365994292, 0.18062397282209694, 0.17733990047463447, 0.1789819365994292, 0.1871921173212759, 0.17241379200237725, 0.18390804507168643, 0.18555008119648117, 0.18226600894689168, 0.1789819366973022, 0.18062397282209694, 0.1789819365994292, 0.17405582832291797, 0.1871921174191489, 0.17733990057250745, 0.1806239730178429, 0.17241379219812322, 0.1707717559754555, 0.17569786474133164, 0.174055828127172, 0.1806239730178429, 0.17569786425196673, 0.17405582832291797, 0.17733990057250745, 0.18062397272422395, 0.18062397272422395, 0.180623972626351, 0.17569786534080364, 0.1740558292160089, 0.18062397272422395, 0.18555008109860818, 0.18555008109860818, 0.174055828127172, 0.18555008109860818, 0.17733990037676148, 0.17898193759039313, 0.18226600875114574, 0.17569786425196673, 0.18390804497381344, 0.18226600875114574, 0.180623972626351, 0.1789819365994292, 0.18555008109860818, 0.19376026172258193, 0.19868637009696616, 0.19376026162470894, 0.1921182254999142, 0.1954022988383406, 0.2036124784713504, 0.20197044234655565, 0.17898193650155622, 0.18390804497381344, 0.19047618947299244, 0.18555008100073522, 0.17733990047463447, 0.17733990037676148, 0.18390804497381344, 0.18226600894689168, 0.1822660088490187, 0.18390804497381344, 0.18390804497381344, 0.18390804497381344, 0.18226600894689168, 0.18390804497381344, 0.18555008109860818, 0.18555008119648117, 0.19047618947299244, 0.18555008109860818, 0.18390804497381344, 0.18390804507168643, 0.1888341533481977, 0.18719211722340295, 0.18555008109860818, 0.18390804516955941, 0.1822660088490187, 0.18062397272422395, 0.18226600904476467, 0.1822660088490187, 0.1888341533481977, 0.18226600904476467, 0.1822660088490187, 0.18062397282209694, 0.18062397272422395, 0.18883415344607066, 0.18555008109860818, 0.18062397272422395, 0.1871921173212759, 0.17569786434983972, 0.18555008119648117, 0.18883415344607066, 0.18062397272422395, 0.17733990047463447, 0.18390804507168643, 0.17405582822504498, 0.18555008119648117, 0.18062397282209694, 0.17898193650155622, 0.18062397291996993, 0.18062397291996993, 0.1789819365994292, 0.18390804516955941, 0.18555008129435416, 0.1789819366973022, 0.17569786434983972, 0.18719211722340295, 0.1740558292160089, 0.1789819365994292, 0.1773399014655984, 0.1773399014655984, 0.1756978644477127, 0.17733990047463447, 0.18555008129435416, 0.18390804507168643, 0.19047618947299244, 0.19211822579353313, 0.19376026191832788, 0.18555008139222715, 0.16748768461895694, 0.18390804516955941, 0.1904761896687384, 0.17733990037676148, 0.18883415373968962, 0.17569786425196673, 0.19211822589140612, 0.19704433436366334, 0.17241379200237725, 0.19211822579353313, 0.18883415344607066, 0.18555008119648117, 0.1921182259892791, 0.18062397291996993, 0.18226600904476467, 0.19376026201620086, 0.19211822579353313, 0.18555008129435416, 0.18226600904476467, 0.19376026191832788, 0.1954022981409956, 0.1970443340700444, 0.19376026172258193, 0.19211822589140612, 0.18883415364181663, 0.18555008129435416, 0.1888341533481977, 0.18719211712552997, 0.19047618947299244, 0.1888341533481977, 0.1904761896687384, 0.18883415344607066, 0.1888341533481977, 0.18555008109860818, 0.19047618947299244, 0.19211822569566014, 0.18555008109860818, 0.19211822569566014, 0.19047618947299244, 0.18062397272422395, 0.19540229784737667, 0.180623972626351, 0.1888341533481977, 0.19376026182045492, 0.17569786425196673, 0.1904761895708654, 0.18719211722340295, 0.18062397272422395, 0.18883415344607066, 0.18390804507168643, 0.17241379210025023, 0.1871921173212759, 0.19376026191832788, 0.18883415344607066, 0.18390804497381344, 0.18555008109860818, 0.18390804487594048, 0.19047618947299244, 0.19376026182045492, 0.19376026182045492, 0.18062397291996993, 0.18555008109860818, 0.1904761895708654, 0.1904761895708654, 0.19540229794524966, 0.19540229804312262, 0.18883415344607066, 0.19211822579353313, 0.19047618947299244, 0.1888341533481977, 0.1904761895708654, 0.19376026172258193, 0.19376026172258193, 0.19540229804312262, 0.19376026182045492, 0.19704433416791736, 0.19540229794524966, 0.18719211722340295, 0.18719211722340295, 0.1888341533481977, 0.18719211722340295, 0.19211822569566014, 0.18883415344607066, 0.1822660088490187, 0.18883415344607066, 0.19211822579353313, 0.19540229804312262, 0.19540229784737667, 0.19211822569566014, 0.19376026182045492, 0.1986863703905851, 0.1904761895708654, 0.19047618937511945, 0.19211822569566014, 0.1921182254999142, 0.18390804487594048, 0.18555008100073522, 0.18555008119648117, 0.18883415344607066, 0.1871921174191489, 0.18719211722340295, 0.1789819365994292, 0.18062397272422395, 0.1822660088490187, 0.1822660088490187, 0.1888341533481977, 0.18390804497381344, 0.1822660088490187, 0.18062397272422395, 0.1888341533481977, 0.18062397272422395, 0.18390804487594048, 0.18719211722340295, 0.1822660088490187, 0.1822660088490187, 0.19868637019483915, 0.1904761895708654, 0.19376026191832788, 0.1904761895708654, 0.19540229804312262, 0.19540229804312262, 0.19047618947299244, 0.18226600894689168, 0.19540229804312262, 0.18390804507168643, 0.18719211722340295, 0.1904761896687384, 0.18062397282209694, 0.19376026182045492, 0.18883415344607066, 0.19211822579353313, 0.18226600894689168, 0.19376026182045492], 'loss': [2.707568024414031, 2.697643462245714, 2.689471812218856, 2.6827631606703175, 2.6764334242936276, 2.6702810516592415, 2.665261812915058, 2.6597582078812305, 2.653488466920794, 2.6459283846359725, 2.6381881193948233, 2.6278597702480684, 2.615037760940176, 2.600959665329794, 2.5857178383539345, 2.5715512200302655, 2.5525196597316673, 2.5440436363220216, 2.5309087110006345, 2.522292152322538, 2.51099338404207, 2.502063773838646, 2.4938633118811575, 2.480260737475918, 2.477099275393163, 2.4719675603588502, 2.4670006275177, 2.465732049942017, 2.466025038666304, 2.4604023372368156, 2.4604224573170623, 2.4562287186695073, 2.4571686528301826, 2.4542338753138235, 2.4517276965617154, 2.4503922694517604, 2.454512316784085, 2.4502949864467802, 2.449296666660348, 2.4492254396238855, 2.448983146914222, 2.4491284404691975, 2.4488491527353715, 2.449331612557601, 2.453117282973178, 2.4504062306954384, 2.446558440147729, 2.4508793235559483, 2.4538978452310425, 2.448998890667236, 2.4444782001526693, 2.4429659865964855, 2.443708106183908, 2.443259525690725, 2.4424043198140986, 2.440914281535687, 2.4415914999630908, 2.4414209769247006, 2.440512193057082, 2.4393545544367794, 2.439677029223902, 2.439670979364697, 2.439521420026462, 2.4391574764643362, 2.44103269596609, 2.4381910180164312, 2.4449880302319538, 2.447189554296725, 2.4392384046401823, 2.4422898401225126, 2.440860087964569, 2.4389275775063455, 2.4374003808845974, 2.4393715269022165, 2.435508412159444, 2.4349659338134515, 2.435324223281422, 2.4350679402478668, 2.4335957213838486, 2.433669493379534, 2.4337016692151767, 2.432906258032797, 2.4339390533415934, 2.4318923188430817, 2.4327706611131985, 2.433055522260725, 2.4328654480176297, 2.43090355851567, 2.431795017430425, 2.4306613529487313, 2.431364149822102, 2.430536689209987, 2.430075180946679, 2.4308168306487787, 2.4299499322746323, 2.4324761537555797, 2.4300020576257726, 2.430138758320583, 2.4304627453766807, 2.4328109307455574, 2.4294008393062456, 2.429025208338085, 2.4334112881145438, 2.4337003764675385, 2.4322553005061844, 2.4309922404357787, 2.431691643442706, 2.4304183744551953, 2.4294117620103903, 2.4309183231369427, 2.4330737721993447, 2.4315994057077646, 2.4299280688503195, 2.428943144369419, 2.4287983220705507, 2.4274215185177157, 2.4286224624704285, 2.4292409215374895, 2.426799607913352, 2.4274469286509364, 2.426209270880697, 2.4269264181047006, 2.4275347543203365, 2.4258589404809157, 2.427400279877367, 2.427100941483735, 2.4262739745498436, 2.426586421167581, 2.426130706524702, 2.4278911047158056, 2.428737744756303, 2.426358357494127, 2.4264248028428157, 2.4256929721675613, 2.4261997798629853, 2.427919320257293, 2.4247369442142745, 2.425678337477071, 2.4254576073045357, 2.4241744449740805, 2.4250055590204633, 2.4262237918939924, 2.425303407763064, 2.424869479682656, 2.424213905843621, 2.42381773484561, 2.4239077515181084, 2.425236132306485, 2.4284350473533176, 2.428164134524931, 2.427460917995696, 2.4267475071384186, 2.4259097501727345, 2.42483707208653, 2.42520357206372, 2.4241461929109795, 2.4245309098300503, 2.42439367364809, 2.4240037346277883, 2.4250708768010383, 2.4259674313132034, 2.426539727060212, 2.4255610605040125, 2.4268677374665497, 2.4235853782669476, 2.426318773058161, 2.4305025293596962, 2.430915124019803, 2.425665659483453, 2.4267984366759627, 2.42975181603089, 2.4246036927068504, 2.425111620137334, 2.4220446967491136, 2.4229159186752915, 2.4230634684435395, 2.423618638393081, 2.4255110268720124, 2.4257487252997176, 2.4295053206919643, 2.4345773713544654, 2.431700442605929, 2.425429377369812, 2.4227172279749563, 2.422186691413425, 2.422840185087075, 2.4213785483851815, 2.422753002805142, 2.420462456376156, 2.4215879870146453, 2.4210109256376233, 2.419425932533687, 2.418467040228403, 2.4197902563905815, 2.418716068385318, 2.419128735501174, 2.418479606503089, 2.4200327039009735, 2.419748550665697, 2.4247367526961057, 2.4222424085135335, 2.421268450210227, 2.419601647026485, 2.4207114813998496, 2.420488298306475, 2.4196319758280103, 2.421698906583218, 2.42213001476421, 2.4213085710145608, 2.4197761165532734, 2.4184943845384663, 2.4189887774308847, 2.4176258599244105, 2.4185785913369493, 2.4197366774204574, 2.4176305833538456, 2.4199609155282835, 2.4208785671962603, 2.4203670928610426, 2.4214219793401948, 2.4198065807687184, 2.418197466362673, 2.419299153968294, 2.418055011702269, 2.418693566762936, 2.4192868699772894, 2.4195008320974862, 2.418547686708047, 2.4173044231148473, 2.4179070500132975, 2.417002516850309, 2.417714695078636, 2.4181511306175216, 2.418496928812297, 2.419309520427696, 2.419656796817662, 2.4190518846257265, 2.4199300622058844, 2.418471562348352, 2.416812681123706, 2.4165776897015268, 2.416121500277666, 2.415602902661114, 2.415071550580755, 2.416399703427262, 2.4164259662863166, 2.4149685157642717, 2.4154816169268787, 2.4140972746471117, 2.4144501661617896, 2.4147695381783363, 2.4156612055502387, 2.4146683329674254, 2.4149509915580984, 2.4135281325855296, 2.4138824432537542, 2.413188694340982, 2.412552021759002, 2.413670404931603, 2.413436812982422, 2.413725914945348, 2.4155106667865227, 2.4169287098996204, 2.4154913676103282, 2.4163500481317666, 2.415563205331258, 2.4144627081540087, 2.4133194257591293, 2.413132392209658, 2.4136131202415765, 2.413479864132233, 2.415391155630656, 2.414588431019558, 2.414952331989453, 2.4150865911213524, 2.414015978415644, 2.414202232086683, 2.4134125878923482, 2.4129084957208966, 2.413282391765524, 2.41166505177163, 2.4121243798022887, 2.4120083774629313, 2.411698779531083, 2.410594735644926, 2.415362403818714, 2.412558471103958, 2.4126210087378657, 2.411469022449282, 2.4104269232348496, 2.4102702440422417, 2.40915799845905, 2.4097400193341705, 2.4107736566962648, 2.4102705002810185, 2.4095556519604315, 2.4094457745796847, 2.412161456877691, 2.4096674338503297, 2.409087025605188], 'acc': [0.056673511261506736, 0.05831622172001696, 0.09815195065443032, 0.10924024614648897, 0.10266940504985668, 0.10225872703095482, 0.10225872644347576, 0.10225872663930212, 0.10349075988516426, 0.10595482561194187, 0.11047227977482445, 0.11950718578739088, 0.13018480583382827, 0.14250513359261735, 0.14086242351199077, 0.1445585216646077, 0.1408624221228476, 0.1494866530814455, 0.15564681617753462, 0.16673511360949805, 0.17618069825598345, 0.1687885007390741, 0.16837782310264557, 0.16919917776966487, 0.16960985517354962, 0.17412731114476612, 0.16919917853461153, 0.17618069806015713, 0.17248460028083418, 0.1761806992351152, 0.17043121158464733, 0.17412730957815534, 0.17330595412783065, 0.1728952768830548, 0.17330595373617794, 0.17289527631393448, 0.17084188802775907, 0.172073922466938, 0.17166324345972503, 0.17043121199465874, 0.17002053357492483, 0.1802874744366816, 0.17535934202235337, 0.1716632448488682, 0.17741273091436657, 0.1696098555652023, 0.16714579122756787, 0.16919917933627565, 0.17823408632797383, 0.170431211015527, 0.17207392086360979, 0.1708418890068908, 0.170020534964068, 0.1712525658416552, 0.1708418886152381, 0.17248459850003833, 0.16837782410013602, 0.17207392264440563, 0.17248460047666053, 0.17166324541798852, 0.16837782312100427, 0.16878850132655315, 0.1716632448488682, 0.1704312121904851, 0.16714579024843612, 0.17494866638090575, 0.17248459869586466, 0.1671457908542739, 0.1737166327250322, 0.1712525668391457, 0.17700205311882913, 0.17289527731142496, 0.1712525677999187, 0.17166324504469455, 0.17330595412783065, 0.17289527750725128, 0.17700205288628534, 0.177002052690459, 0.17577002101120762, 0.16919917933627565, 0.16960985517354962, 0.17125256642913428, 0.17248459969335514, 0.17453798717786645, 0.17043121119299465, 0.17248459988918147, 0.1716632448305095, 0.1675564684723437, 0.17289527809473035, 0.17782340773077226, 0.17494866639926449, 0.17248460047666053, 0.17371663174590046, 0.1778234095115681, 0.1770020528679266, 0.17659137685318502, 0.17905544236577756, 0.17741273050435516, 0.17577002159868668, 0.1737166323333795, 0.17207392303605834, 0.1782340857588535, 0.1782340861505062, 0.17371663194172682, 0.18193018392982913, 0.17823408650544145, 0.1774127307001815, 0.17905544236577756, 0.1790554405849817, 0.1794661188088893, 0.17823408673798524, 0.1794661193780096, 0.17864476316273825, 0.17166324386973644, 0.17782340931574178, 0.17782340929738305, 0.1749486653834153, 0.17330595571280016, 0.1761806984334511, 0.1745379869820401, 0.17659137606987962, 0.18028747482833432, 0.1778234075349459, 0.17823408730710558, 0.1786447649251754, 0.17659137546404186, 0.1757700208153813, 0.17782340931574178, 0.17823408632797383, 0.18110882926280983, 0.17618069766850442, 0.17864476394604364, 0.17987679697772072, 0.17659137487656282, 0.1749486646001099, 0.1790554409766344, 0.1790554409766344, 0.17453798700039883, 0.17946611957383596, 0.18234086192119292, 0.17618069743596063, 0.18069815164474, 0.18151950705834727, 0.1778234083182513, 0.17987679582112134, 0.18151950666669459, 0.18603696148131174, 0.1848049272196004, 0.18028747363501751, 0.1790554405849817, 0.17741273048599643, 0.17987679582112134, 0.17659137605152092, 0.1806981526422305, 0.179466120161315, 0.18069815225057778, 0.18234086290032467, 0.18028747541581336, 0.18069815301552444, 0.1843942494424217, 0.18439425100903248, 0.1827515397350891, 0.1798767972102645, 0.18110882926280983, 0.18316221696150622, 0.18069815144891366, 0.17618069903928885, 0.1778234075349459, 0.1827515401267418, 0.17577001944459683, 0.18151950609757425, 0.18193018530061358, 0.1802874740266702, 0.18685831669909264, 0.17782340773077226, 0.18234086190283422, 0.1790554405849817, 0.17823408732546428, 0.17782340911991543, 0.17905544117246075, 0.17987679779774354, 0.17905544216995123, 0.182340862135378, 0.17700205386541706, 0.17905544099499313, 0.18398357217928712, 0.18234086231284563, 0.17618069902093014, 0.17659137626570598, 0.18316221696150622, 0.18069815303388317, 0.1831622181364643, 0.18316221835064936, 0.17864476335856458, 0.18480492860874356, 0.18110882847950444, 0.18151950825166407, 0.17823408515301573, 0.17535934200399467, 0.17700205229880628, 0.18193018528225485, 0.18439424979735694, 0.1798767968186118, 0.1790554413682871, 0.18069815281969812, 0.17987679660442674, 0.18193018410729678, 0.17782340970739446, 0.17741273169767197, 0.18521560622681338, 0.1802874732433648, 0.17864476394604364, 0.18850102580058747, 0.1802874740083115, 0.1819301854780812, 0.18069815262387176, 0.1815195082700228, 0.18110883004611522, 0.18275154032256813, 0.18439425038483598, 0.18439425038483598, 0.1806981524280454, 0.1827515401267418, 0.18685831511412312, 0.18521560483767022, 0.18439424961988932, 0.18685831591578725, 0.18685831648490758, 0.1950718676713458, 0.1860369602879949, 0.18439425018900962, 0.18480492704213278, 0.18809034937583446, 0.18439424979735694, 0.18521560544350799, 0.18357289555870776, 0.18357289636037188, 0.18234086309615102, 0.18850102736719826, 0.1880903487699967, 0.1868583165032663, 0.18275154112423225, 0.18644763888519647, 0.18439425081320612, 0.18603696011052728, 0.1876796721126999, 0.18850102658389287, 0.18685831513248186, 0.18644763853026122, 0.1852156062451721, 0.18480492704213278, 0.18398357178763441, 0.1774127298985174, 0.1839835720018195, 0.18398357221600456, 0.18644763908102283, 0.18932238182003247, 0.18767967136611194, 0.1860369598779835, 0.1856262830615778, 0.1872689933380307, 0.18809034857417034, 0.18521560546186672, 0.18110882906698347, 0.18850102738555696, 0.18480492702377405, 0.18316221656985351, 0.18480492782543817, 0.18644763888519647, 0.18439425099067375, 0.1868583157199609, 0.18193018545972248, 0.18685831628908123, 0.18275153914761005, 0.18603696087547397, 0.18562628245574003, 0.18234086135207261, 0.18726899394386848, 0.18234086290032467, 0.18809034818251766, 0.18685831632579866, 0.18562628247409876, 0.17946611920054198, 0.1811088290853422, 0.1868583153283082, 0.1856262822782724, 0.18480492821709085, 0.18480492704213278, 0.18809034976748715, 0.18480492921458133, 0.18521560563933434, 0.18603696148131174, 0.1885010259780551, 0.18644763829771743, 0.18603695989634222, 0.19014373725199846, 0.18685831513248186, 0.18562628347158922, 0.18809034916164938, 0.18480492880456992]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
