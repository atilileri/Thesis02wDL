{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf12.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 18:15:31 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000027D005C9550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000027D41616EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0822, Accuracy:0.3725, Validation Loss:1.0795, Validation Accuracy:0.3563\n",
    "Epoch #2: Loss:1.0779, Accuracy:0.3815, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #3: Loss:1.0746, Accuracy:0.3963, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0734, Accuracy:0.3971, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #22: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0732, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.3859\n",
    "Epoch #25: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #26: Loss:1.0736, Accuracy:0.4025, Validation Loss:1.0741, Validation Accuracy:0.3990\n",
    "Epoch #27: Loss:1.0734, Accuracy:0.4025, Validation Loss:1.0737, Validation Accuracy:0.3957\n",
    "Epoch #28: Loss:1.0733, Accuracy:0.4025, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #29: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #30: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #31: Loss:1.0733, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #32: Loss:1.0731, Accuracy:0.3967, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #33: Loss:1.0729, Accuracy:0.4016, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #34: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #35: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #36: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #37: Loss:1.0730, Accuracy:0.3996, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #38: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0737, Validation Accuracy:0.4039\n",
    "Epoch #39: Loss:1.0733, Accuracy:0.4004, Validation Loss:1.0738, Validation Accuracy:0.4039\n",
    "Epoch #40: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #41: Loss:1.0733, Accuracy:0.4004, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #42: Loss:1.0732, Accuracy:0.4025, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #43: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #44: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #45: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #46: Loss:1.0729, Accuracy:0.3947, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #47: Loss:1.0728, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #48: Loss:1.0728, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #49: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #50: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #51: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.4089\n",
    "Epoch #52: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #53: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #54: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #55: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0730, Validation Accuracy:0.3974\n",
    "Epoch #56: Loss:1.0726, Accuracy:0.4033, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #57: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0727, Validation Accuracy:0.4007\n",
    "Epoch #58: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0726, Validation Accuracy:0.4039\n",
    "Epoch #59: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #60: Loss:1.0721, Accuracy:0.4021, Validation Loss:1.0729, Validation Accuracy:0.4023\n",
    "Epoch #61: Loss:1.0722, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.4056\n",
    "Epoch #62: Loss:1.0723, Accuracy:0.4000, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #63: Loss:1.0723, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #64: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #65: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #66: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #67: Loss:1.0729, Accuracy:0.4033, Validation Loss:1.0754, Validation Accuracy:0.4023\n",
    "Epoch #68: Loss:1.0730, Accuracy:0.4000, Validation Loss:1.0749, Validation Accuracy:0.4007\n",
    "Epoch #69: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.4023\n",
    "Epoch #70: Loss:1.0726, Accuracy:0.3992, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #71: Loss:1.0730, Accuracy:0.4025, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #72: Loss:1.0728, Accuracy:0.4037, Validation Loss:1.0728, Validation Accuracy:0.4007\n",
    "Epoch #73: Loss:1.0722, Accuracy:0.4045, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #74: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #75: Loss:1.0724, Accuracy:0.4049, Validation Loss:1.0731, Validation Accuracy:0.3974\n",
    "Epoch #76: Loss:1.0724, Accuracy:0.4029, Validation Loss:1.0728, Validation Accuracy:0.4007\n",
    "Epoch #77: Loss:1.0721, Accuracy:0.4016, Validation Loss:1.0723, Validation Accuracy:0.4089\n",
    "Epoch #78: Loss:1.0719, Accuracy:0.4033, Validation Loss:1.0724, Validation Accuracy:0.4089\n",
    "Epoch #79: Loss:1.0719, Accuracy:0.4012, Validation Loss:1.0725, Validation Accuracy:0.3990\n",
    "Epoch #80: Loss:1.0721, Accuracy:0.4021, Validation Loss:1.0724, Validation Accuracy:0.4007\n",
    "Epoch #81: Loss:1.0718, Accuracy:0.4012, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #82: Loss:1.0717, Accuracy:0.4033, Validation Loss:1.0725, Validation Accuracy:0.3974\n",
    "Epoch #83: Loss:1.0715, Accuracy:0.4016, Validation Loss:1.0726, Validation Accuracy:0.3990\n",
    "Epoch #84: Loss:1.0713, Accuracy:0.4016, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #85: Loss:1.0712, Accuracy:0.4025, Validation Loss:1.0725, Validation Accuracy:0.3974\n",
    "Epoch #86: Loss:1.0711, Accuracy:0.4045, Validation Loss:1.0719, Validation Accuracy:0.3957\n",
    "Epoch #87: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #88: Loss:1.0716, Accuracy:0.4008, Validation Loss:1.0726, Validation Accuracy:0.3875\n",
    "Epoch #89: Loss:1.0724, Accuracy:0.4053, Validation Loss:1.0720, Validation Accuracy:0.3875\n",
    "Epoch #90: Loss:1.0720, Accuracy:0.4057, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #91: Loss:1.0741, Accuracy:0.3955, Validation Loss:1.0741, Validation Accuracy:0.3612\n",
    "Epoch #92: Loss:1.0755, Accuracy:0.3852, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #93: Loss:1.0741, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #94: Loss:1.0742, Accuracy:0.3955, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #95: Loss:1.0730, Accuracy:0.4025, Validation Loss:1.0740, Validation Accuracy:0.3645\n",
    "Epoch #96: Loss:1.0733, Accuracy:0.3856, Validation Loss:1.0744, Validation Accuracy:0.4105\n",
    "Epoch #97: Loss:1.0732, Accuracy:0.3918, Validation Loss:1.0737, Validation Accuracy:0.3826\n",
    "Epoch #98: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #99: Loss:1.0727, Accuracy:0.3955, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #100: Loss:1.0726, Accuracy:0.4000, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #101: Loss:1.0725, Accuracy:0.4041, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #102: Loss:1.0722, Accuracy:0.4012, Validation Loss:1.0737, Validation Accuracy:0.3859\n",
    "Epoch #103: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #104: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #105: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #106: Loss:1.0721, Accuracy:0.4000, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #107: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #108: Loss:1.0725, Accuracy:0.4016, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #109: Loss:1.0720, Accuracy:0.4025, Validation Loss:1.0732, Validation Accuracy:0.3908\n",
    "Epoch #110: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #111: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #112: Loss:1.0724, Accuracy:0.4000, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #113: Loss:1.0717, Accuracy:0.3988, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #114: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #115: Loss:1.0717, Accuracy:0.4029, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #116: Loss:1.0723, Accuracy:0.4016, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #117: Loss:1.0721, Accuracy:0.4062, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #118: Loss:1.0719, Accuracy:0.3992, Validation Loss:1.0727, Validation Accuracy:0.3957\n",
    "Epoch #119: Loss:1.0718, Accuracy:0.4037, Validation Loss:1.0729, Validation Accuracy:0.3924\n",
    "Epoch #120: Loss:1.0720, Accuracy:0.4086, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #121: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #122: Loss:1.0722, Accuracy:0.4021, Validation Loss:1.0726, Validation Accuracy:0.3924\n",
    "Epoch #123: Loss:1.0713, Accuracy:0.4045, Validation Loss:1.0726, Validation Accuracy:0.3974\n",
    "Epoch #124: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0720, Validation Accuracy:0.4023\n",
    "Epoch #125: Loss:1.0709, Accuracy:0.4049, Validation Loss:1.0717, Validation Accuracy:0.3941\n",
    "Epoch #126: Loss:1.0713, Accuracy:0.4053, Validation Loss:1.0709, Validation Accuracy:0.3924\n",
    "Epoch #127: Loss:1.0711, Accuracy:0.4016, Validation Loss:1.0709, Validation Accuracy:0.4056\n",
    "Epoch #128: Loss:1.0715, Accuracy:0.4070, Validation Loss:1.0716, Validation Accuracy:0.3941\n",
    "Epoch #129: Loss:1.0710, Accuracy:0.4090, Validation Loss:1.0731, Validation Accuracy:0.3974\n",
    "Epoch #130: Loss:1.0715, Accuracy:0.4062, Validation Loss:1.0725, Validation Accuracy:0.3924\n",
    "Epoch #131: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0725, Validation Accuracy:0.3924\n",
    "Epoch #132: Loss:1.0715, Accuracy:0.4037, Validation Loss:1.0719, Validation Accuracy:0.3908\n",
    "Epoch #133: Loss:1.0709, Accuracy:0.4053, Validation Loss:1.0721, Validation Accuracy:0.3892\n",
    "Epoch #134: Loss:1.0714, Accuracy:0.4053, Validation Loss:1.0726, Validation Accuracy:0.3957\n",
    "Epoch #135: Loss:1.0714, Accuracy:0.4066, Validation Loss:1.0727, Validation Accuracy:0.3957\n",
    "Epoch #136: Loss:1.0721, Accuracy:0.4094, Validation Loss:1.0717, Validation Accuracy:0.3990\n",
    "Epoch #137: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0723, Validation Accuracy:0.4039\n",
    "Epoch #138: Loss:1.0716, Accuracy:0.4074, Validation Loss:1.0726, Validation Accuracy:0.3957\n",
    "Epoch #139: Loss:1.0716, Accuracy:0.4066, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #140: Loss:1.0744, Accuracy:0.3922, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #141: Loss:1.0740, Accuracy:0.4016, Validation Loss:1.0757, Validation Accuracy:0.4007\n",
    "Epoch #142: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0722, Validation Accuracy:0.3875\n",
    "Epoch #143: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0754, Validation Accuracy:0.3908\n",
    "Epoch #144: Loss:1.0741, Accuracy:0.3996, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #145: Loss:1.0728, Accuracy:0.3979, Validation Loss:1.0729, Validation Accuracy:0.3974\n",
    "Epoch #146: Loss:1.0732, Accuracy:0.3967, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #147: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0726, Validation Accuracy:0.3924\n",
    "Epoch #148: Loss:1.0726, Accuracy:0.4078, Validation Loss:1.0725, Validation Accuracy:0.3941\n",
    "Epoch #149: Loss:1.0726, Accuracy:0.4033, Validation Loss:1.0726, Validation Accuracy:0.3990\n",
    "Epoch #150: Loss:1.0726, Accuracy:0.4074, Validation Loss:1.0723, Validation Accuracy:0.4105\n",
    "Epoch #151: Loss:1.0723, Accuracy:0.4070, Validation Loss:1.0722, Validation Accuracy:0.4072\n",
    "Epoch #152: Loss:1.0718, Accuracy:0.4033, Validation Loss:1.0720, Validation Accuracy:0.4089\n",
    "Epoch #153: Loss:1.0718, Accuracy:0.4008, Validation Loss:1.0722, Validation Accuracy:0.4089\n",
    "Epoch #154: Loss:1.0718, Accuracy:0.4033, Validation Loss:1.0723, Validation Accuracy:0.4007\n",
    "Epoch #155: Loss:1.0711, Accuracy:0.4062, Validation Loss:1.0737, Validation Accuracy:0.3842\n",
    "Epoch #156: Loss:1.0709, Accuracy:0.4070, Validation Loss:1.0726, Validation Accuracy:0.3892\n",
    "Epoch #157: Loss:1.0710, Accuracy:0.4082, Validation Loss:1.0721, Validation Accuracy:0.3941\n",
    "Epoch #158: Loss:1.0713, Accuracy:0.4045, Validation Loss:1.0721, Validation Accuracy:0.4056\n",
    "Epoch #159: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0718, Validation Accuracy:0.3974\n",
    "Epoch #160: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0772, Validation Accuracy:0.3892\n",
    "Epoch #161: Loss:1.0765, Accuracy:0.3873, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #162: Loss:1.0751, Accuracy:0.3975, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #163: Loss:1.0740, Accuracy:0.3984, Validation Loss:1.0733, Validation Accuracy:0.3793\n",
    "Epoch #164: Loss:1.0730, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #165: Loss:1.0745, Accuracy:0.4008, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #166: Loss:1.0740, Accuracy:0.3984, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #167: Loss:1.0738, Accuracy:0.3988, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #168: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0737, Validation Accuracy:0.3859\n",
    "Epoch #169: Loss:1.0738, Accuracy:0.4037, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #170: Loss:1.0738, Accuracy:0.3984, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #171: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #172: Loss:1.0734, Accuracy:0.3979, Validation Loss:1.0735, Validation Accuracy:0.3974\n",
    "Epoch #173: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #174: Loss:1.0731, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3842\n",
    "Epoch #175: Loss:1.0733, Accuracy:0.4049, Validation Loss:1.0732, Validation Accuracy:0.3892\n",
    "Epoch #176: Loss:1.0731, Accuracy:0.4070, Validation Loss:1.0732, Validation Accuracy:0.3859\n",
    "Epoch #177: Loss:1.0732, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #178: Loss:1.0730, Accuracy:0.4012, Validation Loss:1.0738, Validation Accuracy:0.3793\n",
    "Epoch #179: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #180: Loss:1.0731, Accuracy:0.4045, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #181: Loss:1.0733, Accuracy:0.3996, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #182: Loss:1.0733, Accuracy:0.4021, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #183: Loss:1.0733, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #184: Loss:1.0732, Accuracy:0.4033, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #185: Loss:1.0732, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.4023\n",
    "Epoch #186: Loss:1.0732, Accuracy:0.4000, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #187: Loss:1.0731, Accuracy:0.4004, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #188: Loss:1.0732, Accuracy:0.3988, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #189: Loss:1.0731, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #190: Loss:1.0730, Accuracy:0.4033, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #191: Loss:1.0730, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #192: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0742, Validation Accuracy:0.3826\n",
    "Epoch #193: Loss:1.0732, Accuracy:0.4021, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #194: Loss:1.0730, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #195: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #196: Loss:1.0730, Accuracy:0.4037, Validation Loss:1.0740, Validation Accuracy:0.3826\n",
    "Epoch #197: Loss:1.0731, Accuracy:0.4062, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #198: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #199: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #200: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #201: Loss:1.0726, Accuracy:0.4012, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #202: Loss:1.0726, Accuracy:0.4053, Validation Loss:1.0740, Validation Accuracy:0.3744\n",
    "Epoch #203: Loss:1.0728, Accuracy:0.4057, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #204: Loss:1.0724, Accuracy:0.4008, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #205: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #206: Loss:1.0724, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #207: Loss:1.0722, Accuracy:0.4021, Validation Loss:1.0738, Validation Accuracy:0.3777\n",
    "Epoch #208: Loss:1.0723, Accuracy:0.4012, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #209: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #210: Loss:1.0725, Accuracy:0.3975, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #211: Loss:1.0723, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #212: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #213: Loss:1.0724, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #214: Loss:1.0723, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #215: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0740, Validation Accuracy:0.3793\n",
    "Epoch #216: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0740, Validation Accuracy:0.3892\n",
    "Epoch #217: Loss:1.0722, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3810\n",
    "Epoch #218: Loss:1.0720, Accuracy:0.3992, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #219: Loss:1.0721, Accuracy:0.3996, Validation Loss:1.0740, Validation Accuracy:0.3892\n",
    "Epoch #220: Loss:1.0722, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #221: Loss:1.0720, Accuracy:0.3979, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #222: Loss:1.0720, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #223: Loss:1.0722, Accuracy:0.4016, Validation Loss:1.0736, Validation Accuracy:0.4089\n",
    "Epoch #224: Loss:1.0721, Accuracy:0.3992, Validation Loss:1.0737, Validation Accuracy:0.4039\n",
    "Epoch #225: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.4039\n",
    "Epoch #226: Loss:1.0718, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #227: Loss:1.0719, Accuracy:0.4037, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #228: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #229: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #230: Loss:1.0719, Accuracy:0.4012, Validation Loss:1.0739, Validation Accuracy:0.3826\n",
    "Epoch #231: Loss:1.0721, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #232: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3826\n",
    "Epoch #233: Loss:1.0718, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #234: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0738, Validation Accuracy:0.4089\n",
    "Epoch #235: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #236: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #237: Loss:1.0716, Accuracy:0.4045, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #238: Loss:1.0717, Accuracy:0.4033, Validation Loss:1.0736, Validation Accuracy:0.3892\n",
    "Epoch #239: Loss:1.0715, Accuracy:0.4037, Validation Loss:1.0731, Validation Accuracy:0.4023\n",
    "Epoch #240: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #241: Loss:1.0714, Accuracy:0.4016, Validation Loss:1.0729, Validation Accuracy:0.4089\n",
    "Epoch #242: Loss:1.0712, Accuracy:0.4025, Validation Loss:1.0726, Validation Accuracy:0.3957\n",
    "Epoch #243: Loss:1.0719, Accuracy:0.3975, Validation Loss:1.0725, Validation Accuracy:0.3990\n",
    "Epoch #244: Loss:1.0717, Accuracy:0.3992, Validation Loss:1.0724, Validation Accuracy:0.3859\n",
    "Epoch #245: Loss:1.0717, Accuracy:0.4000, Validation Loss:1.0729, Validation Accuracy:0.3859\n",
    "Epoch #246: Loss:1.0719, Accuracy:0.4000, Validation Loss:1.0725, Validation Accuracy:0.3941\n",
    "Epoch #247: Loss:1.0714, Accuracy:0.3979, Validation Loss:1.0724, Validation Accuracy:0.3974\n",
    "Epoch #248: Loss:1.0715, Accuracy:0.3996, Validation Loss:1.0723, Validation Accuracy:0.3842\n",
    "Epoch #249: Loss:1.0714, Accuracy:0.4041, Validation Loss:1.0723, Validation Accuracy:0.3859\n",
    "Epoch #250: Loss:1.0711, Accuracy:0.4033, Validation Loss:1.0724, Validation Accuracy:0.4023\n",
    "Epoch #251: Loss:1.0722, Accuracy:0.3893, Validation Loss:1.0727, Validation Accuracy:0.4056\n",
    "Epoch #252: Loss:1.0720, Accuracy:0.4008, Validation Loss:1.0729, Validation Accuracy:0.3859\n",
    "Epoch #253: Loss:1.0715, Accuracy:0.4037, Validation Loss:1.0723, Validation Accuracy:0.3859\n",
    "Epoch #254: Loss:1.0713, Accuracy:0.4008, Validation Loss:1.0732, Validation Accuracy:0.3695\n",
    "Epoch #255: Loss:1.0713, Accuracy:0.4029, Validation Loss:1.0725, Validation Accuracy:0.4023\n",
    "Epoch #256: Loss:1.0713, Accuracy:0.4021, Validation Loss:1.0730, Validation Accuracy:0.3826\n",
    "Epoch #257: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0725, Validation Accuracy:0.4023\n",
    "Epoch #258: Loss:1.0710, Accuracy:0.4004, Validation Loss:1.0725, Validation Accuracy:0.3974\n",
    "Epoch #259: Loss:1.0710, Accuracy:0.3996, Validation Loss:1.0727, Validation Accuracy:0.3859\n",
    "Epoch #260: Loss:1.0708, Accuracy:0.4045, Validation Loss:1.0724, Validation Accuracy:0.3990\n",
    "Epoch #261: Loss:1.0707, Accuracy:0.4016, Validation Loss:1.0723, Validation Accuracy:0.3924\n",
    "Epoch #262: Loss:1.0706, Accuracy:0.4049, Validation Loss:1.0721, Validation Accuracy:0.3990\n",
    "Epoch #263: Loss:1.0705, Accuracy:0.4041, Validation Loss:1.0724, Validation Accuracy:0.4007\n",
    "Epoch #264: Loss:1.0704, Accuracy:0.4012, Validation Loss:1.0723, Validation Accuracy:0.3974\n",
    "Epoch #265: Loss:1.0705, Accuracy:0.4000, Validation Loss:1.0723, Validation Accuracy:0.3974\n",
    "Epoch #266: Loss:1.0704, Accuracy:0.4004, Validation Loss:1.0719, Validation Accuracy:0.4007\n",
    "Epoch #267: Loss:1.0709, Accuracy:0.4033, Validation Loss:1.0720, Validation Accuracy:0.3842\n",
    "Epoch #268: Loss:1.0709, Accuracy:0.4004, Validation Loss:1.0711, Validation Accuracy:0.3908\n",
    "Epoch #269: Loss:1.0708, Accuracy:0.3988, Validation Loss:1.0716, Validation Accuracy:0.3892\n",
    "Epoch #270: Loss:1.0716, Accuracy:0.3988, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #271: Loss:1.0713, Accuracy:0.3992, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #272: Loss:1.0718, Accuracy:0.3971, Validation Loss:1.0727, Validation Accuracy:0.4039\n",
    "Epoch #273: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0728, Validation Accuracy:0.3842\n",
    "Epoch #274: Loss:1.0716, Accuracy:0.4033, Validation Loss:1.0734, Validation Accuracy:0.3826\n",
    "Epoch #275: Loss:1.0707, Accuracy:0.4049, Validation Loss:1.0722, Validation Accuracy:0.3892\n",
    "Epoch #276: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0725, Validation Accuracy:0.3892\n",
    "Epoch #277: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0722, Validation Accuracy:0.3875\n",
    "Epoch #278: Loss:1.0708, Accuracy:0.4021, Validation Loss:1.0725, Validation Accuracy:0.3875\n",
    "Epoch #279: Loss:1.0714, Accuracy:0.3885, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #280: Loss:1.0712, Accuracy:0.4008, Validation Loss:1.0723, Validation Accuracy:0.3908\n",
    "Epoch #281: Loss:1.0711, Accuracy:0.4012, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #282: Loss:1.0710, Accuracy:0.4045, Validation Loss:1.0728, Validation Accuracy:0.3793\n",
    "Epoch #283: Loss:1.0708, Accuracy:0.3889, Validation Loss:1.0723, Validation Accuracy:0.4007\n",
    "Epoch #284: Loss:1.0709, Accuracy:0.4021, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #285: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0730, Validation Accuracy:0.3793\n",
    "Epoch #286: Loss:1.0718, Accuracy:0.3938, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #287: Loss:1.0707, Accuracy:0.4049, Validation Loss:1.0731, Validation Accuracy:0.3744\n",
    "Epoch #288: Loss:1.0706, Accuracy:0.3984, Validation Loss:1.0731, Validation Accuracy:0.4056\n",
    "Epoch #289: Loss:1.0701, Accuracy:0.3996, Validation Loss:1.0731, Validation Accuracy:0.4072\n",
    "Epoch #290: Loss:1.0703, Accuracy:0.3984, Validation Loss:1.0724, Validation Accuracy:0.4072\n",
    "Epoch #291: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0728, Validation Accuracy:0.4072\n",
    "Epoch #292: Loss:1.0702, Accuracy:0.3901, Validation Loss:1.0734, Validation Accuracy:0.3695\n",
    "Epoch #293: Loss:1.0695, Accuracy:0.4033, Validation Loss:1.0723, Validation Accuracy:0.4072\n",
    "Epoch #294: Loss:1.0697, Accuracy:0.4008, Validation Loss:1.0720, Validation Accuracy:0.4056\n",
    "Epoch #295: Loss:1.0695, Accuracy:0.4049, Validation Loss:1.0709, Validation Accuracy:0.4007\n",
    "Epoch #296: Loss:1.0692, Accuracy:0.4025, Validation Loss:1.0710, Validation Accuracy:0.3990\n",
    "Epoch #297: Loss:1.0693, Accuracy:0.4082, Validation Loss:1.0711, Validation Accuracy:0.4007\n",
    "Epoch #298: Loss:1.0695, Accuracy:0.4123, Validation Loss:1.0712, Validation Accuracy:0.4122\n",
    "Epoch #299: Loss:1.0700, Accuracy:0.3947, Validation Loss:1.0716, Validation Accuracy:0.3760\n",
    "Epoch #300: Loss:1.0691, Accuracy:0.4025, Validation Loss:1.0721, Validation Accuracy:0.3908\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07211304, Accuracy:0.3908\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  03   01\n",
    "t:02  18   0  209\n",
    "t:03   6   0  136\n",
    "t:01  20   0  220\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.41      0.08      0.13       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.39      0.92      0.55       240\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.27      0.33      0.23       609\n",
    "weighted avg       0.31      0.39      0.26       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 18:56:37 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 41 minutes, 6 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0794823985968904, 1.0749330753567574, 1.0750535701100266, 1.0746461301797325, 1.0739157084369504, 1.0739535827354845, 1.0740288948190624, 1.0740245827313126, 1.0738381333343305, 1.073933950002949, 1.0739661866220935, 1.0739088559581338, 1.0738805908287687, 1.0737858268819223, 1.0737290425449366, 1.0737738439014979, 1.0739669331971844, 1.0741029525625294, 1.0738920687846167, 1.0736356380537813, 1.0735867299469821, 1.0735947937017982, 1.0735514396908639, 1.073368173906173, 1.073995121594133, 1.0740978829379153, 1.0737382394731143, 1.0736786356113228, 1.0739464802890772, 1.0739284293796434, 1.0739953680382965, 1.073881535107279, 1.0738798460153915, 1.073812830820068, 1.0736092062810763, 1.0736647848229495, 1.0735374732166285, 1.0736723707618776, 1.073823536949596, 1.0741659829573482, 1.0741511534391757, 1.0741873967823723, 1.0741824295328952, 1.0743176385099664, 1.0741200631083723, 1.0741005570234727, 1.0739212061579788, 1.0738955397519767, 1.0734730927619245, 1.0733016155819195, 1.07357402446822, 1.0735920295730992, 1.0735460451279564, 1.0734576805080296, 1.072968460851898, 1.0729777225719883, 1.0726610592433385, 1.072610572641119, 1.0730947645622717, 1.0729041937341048, 1.0736850251509442, 1.0734182390673408, 1.0739240446701426, 1.074286597115653, 1.0745259012494768, 1.0746079828155843, 1.0753656316469065, 1.0749294485756133, 1.0747244412871613, 1.0735467766306084, 1.073910775834508, 1.0728473080007117, 1.073083290912835, 1.0735646272919253, 1.0731293254689434, 1.07276814383239, 1.0723056830404623, 1.0723863425121714, 1.0724742588738503, 1.0723765783122021, 1.0725577200574827, 1.0724634190498314, 1.0725778098568344, 1.072812966133769, 1.0724792928726998, 1.071908682829445, 1.07269743510655, 1.072611016005718, 1.0720064211361513, 1.072357105504116, 1.0741043885548909, 1.0749439369085778, 1.0747690752809271, 1.0736562033200694, 1.074002050413874, 1.0743850038948122, 1.0737381905366243, 1.0737591617800333, 1.073855547090665, 1.073685235773597, 1.0738376523865072, 1.0737257849406727, 1.073471700812404, 1.073260384044428, 1.0734878987905818, 1.0732808165949554, 1.0731181141190929, 1.0733177704959864, 1.0731647566621527, 1.0736277238488785, 1.073010523722481, 1.0736663686035106, 1.0729765177556054, 1.073541488749249, 1.0733891809710925, 1.0734580473359583, 1.0731539383701894, 1.0727198897128427, 1.0728801557387428, 1.0726524644297333, 1.0730253510874481, 1.0726326541556122, 1.072593580521582, 1.0719648789498215, 1.0717165930126296, 1.0708939901909413, 1.0709310656502133, 1.0715970904956311, 1.0730637122061844, 1.072485868958221, 1.0725149523056983, 1.071868662176461, 1.0720849416917573, 1.0726389943672519, 1.0726730275428158, 1.071670729146998, 1.072316713716792, 1.0726065171763228, 1.076918324421975, 1.073750240657913, 1.0757345284147215, 1.0722258674295861, 1.0753527867970207, 1.0730587756888228, 1.0728702979722047, 1.0730285728702014, 1.072627374887075, 1.072532637170187, 1.0726152310034716, 1.0723440146015586, 1.0722000133032086, 1.0719635668646526, 1.0721939686679685, 1.0723295106089175, 1.0736768265271617, 1.072568343582216, 1.0721056866528365, 1.0721479971420589, 1.0717655414430967, 1.0771902774159348, 1.0735564523534038, 1.0733845087107767, 1.0733342707059261, 1.074126465958719, 1.0747331672505596, 1.0737534362107075, 1.0736354372184265, 1.0737136749211202, 1.0736893636643985, 1.0737645477301185, 1.073474860739434, 1.0734775138801738, 1.0733201652520592, 1.073278443958176, 1.0731833484176736, 1.0731590144544203, 1.0732184979324466, 1.0737931047167097, 1.0735808174402648, 1.0742198044834856, 1.0742242854999986, 1.0742594958917653, 1.07427122303222, 1.0742052899205625, 1.0740925917288744, 1.0741009489068845, 1.0739738487062, 1.0740430497966573, 1.074239725941312, 1.0740438464827138, 1.0740752229941106, 1.0742314834704345, 1.07425421603599, 1.07416560399317, 1.0741550826478279, 1.0739695211545195, 1.0739396236995953, 1.0740915996883498, 1.0743387066476255, 1.074189163585406, 1.074101223929958, 1.0740095915269774, 1.0739699302635757, 1.073960230659773, 1.0740739269601105, 1.073848086820643, 1.0738198042698877, 1.0739051773043102, 1.0738131494944907, 1.0737829302332085, 1.073756008116874, 1.073829412656073, 1.0739306912242095, 1.0739456019769553, 1.074026287482877, 1.0740479923821435, 1.0740107176534843, 1.073651299883775, 1.073986457290712, 1.0739470694844162, 1.0737731967653548, 1.0738561380477178, 1.0735824041570152, 1.073660560037898, 1.0738839142036751, 1.0737521713003149, 1.073786409030407, 1.0739817443152366, 1.073975024356435, 1.0739298250483371, 1.0741108528694692, 1.0737910482096555, 1.0739191140251598, 1.0737663810867786, 1.073796180081485, 1.073880682046386, 1.073866499077119, 1.0735847820789355, 1.0731254430435757, 1.0729771772237442, 1.0728895491958643, 1.0726299800700547, 1.0724725942502076, 1.072382346749893, 1.072886288459665, 1.0725416563610333, 1.0724108673277355, 1.0722618247879354, 1.0723388825339832, 1.0723660497242593, 1.0726876025912406, 1.0729444156139356, 1.0723298590367258, 1.0731781605624997, 1.0725434118108013, 1.0729748263147665, 1.072468516861864, 1.0725071171821632, 1.0726687348339161, 1.072411226521572, 1.0722965254572225, 1.0720861805679371, 1.0723895210350676, 1.0723276279242755, 1.0722857073610053, 1.071909189811481, 1.0719803041229499, 1.0711203318315579, 1.0716277830706442, 1.0728464316460495, 1.0727894088904846, 1.072687430334796, 1.072830529440017, 1.0733699287687029, 1.0721945496419771, 1.0724915387399483, 1.0722466593696958, 1.0725098848342896, 1.0728383285462955, 1.0722862511432816, 1.0726718303605254, 1.0727527871703475, 1.072338883904205, 1.0725225104487002, 1.0729808521583946, 1.072748050509611, 1.0731454298805525, 1.0730857706226542, 1.0731048540920263, 1.072401840698543, 1.0727832262543426, 1.0733933057299585, 1.07234395979269, 1.071997197586523, 1.0709223604358866, 1.0709955676631584, 1.0711205050667323, 1.0712222440293662, 1.0716456231616793, 1.072113066471269], 'val_acc': [0.35632183736768264, 0.3891625598635775, 0.3973727407811702, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3875205241302747, 0.3940886685315807, 0.3940886685315807, 0.38587848810335296, 0.40065681322650565, 0.3990147771017109, 0.3957307049499944, 0.3990147771995839, 0.3908045965756102, 0.39244663260253193, 0.39737274097691616, 0.3842364519785582, 0.3908045964777372, 0.40065681322650565, 0.40065681322650565, 0.3990147771017109, 0.40065681322650565, 0.40394088547609514, 0.40394088547609514, 0.3957307048521214, 0.3990147771995839, 0.4022988493513004, 0.4022988493513004, 0.4022988494491734, 0.4039408857697141, 0.4006568135201246, 0.40065681361799754, 0.4006568134222516, 0.3990147772974569, 0.40229884954704637, 0.4088669942419713, 0.4006568134222516, 0.4022988494491734, 0.4022988494491734, 0.39737274107478915, 0.3990147775910758, 0.40065681361799754, 0.403940886063333, 0.3990147771995839, 0.40229884954704637, 0.40558292189450884, 0.3957307049499944, 0.39737274097691616, 0.3924466327982779, 0.39737274117266214, 0.39737274107478915, 0.4022988493513004, 0.40065681322650565, 0.4022988493513004, 0.3990147771017109, 0.4022988493513004, 0.4006568135201246, 0.3957307050478674, 0.39737274117266214, 0.39737274117266214, 0.40065681322650565, 0.40886699414409833, 0.40886699414409833, 0.3990147772974569, 0.40065681361799754, 0.3908045967713561, 0.39737274136840806, 0.3990147771017109, 0.4039408857697141, 0.39737274097691616, 0.3957307050478674, 0.3908045962819912, 0.3875205246196396, 0.3875205242281477, 0.3940886695103105, 0.36124794632930474, 0.39901477778682176, 0.3940886685315807, 0.3940886685315807, 0.3645320188725132, 0.41050903075825795, 0.3825944159516364, 0.39244663240678596, 0.3940886685315807, 0.38916256035294244, 0.38587848810335296, 0.38587848810335296, 0.3842364518806852, 0.39244663250465894, 0.3957307049499944, 0.3957307049499944, 0.3908045964777372, 0.3957307048521214, 0.3908045964777372, 0.39408866882519966, 0.3957307048521214, 0.38916256035294244, 0.3957307050478674, 0.39408866892307265, 0.39408866892307265, 0.3842364519785582, 0.3957307050478674, 0.3957307050478674, 0.3924466327982779, 0.39080459667348316, 0.39244663289615084, 0.3924466327982779, 0.3973727412705351, 0.40229884974279234, 0.39408866892307265, 0.3924466327982779, 0.40558292199238183, 0.39408866882519966, 0.3973727412705351, 0.39244663260253193, 0.3924466327982779, 0.3908045965756102, 0.38916256054868836, 0.3957307051457404, 0.3957307051457404, 0.3990147776889488, 0.403940886063333, 0.3957307050478674, 0.39408866931456454, 0.3924466327982779, 0.4006568134222516, 0.3875205245217666, 0.39080459667348316, 0.3957307048521214, 0.39737274097691616, 0.3990147771995839, 0.3924466330918968, 0.39408866921669156, 0.39901477739532987, 0.410509030562512, 0.4072249582150495, 0.40886699443771723, 0.4088669943398443, 0.40065681371587053, 0.3842364520764312, 0.38916256035294244, 0.39408866892307265, 0.40558292199238183, 0.3973727412705351, 0.3891625610380533, 0.39737274107478915, 0.39408866882519966, 0.3793103437999199, 0.3924466329940238, 0.39737274107478915, 0.39408866892307265, 0.39408866921669156, 0.38587848800547997, 0.3924466327004049, 0.3940886685315807, 0.3957307053414863, 0.39737274146628104, 0.39737274136840806, 0.3842364520764312, 0.38916256045081543, 0.38587848820122594, 0.3990147774932028, 0.379310343310555, 0.3908045965756102, 0.40065681322650565, 0.3908045963798642, 0.38916256015719647, 0.39244663260253193, 0.3957307050478674, 0.40229884954704637, 0.3908045965756102, 0.40065681332437864, 0.3875205242281477, 0.3957307049499944, 0.3940886687273267, 0.3957307049499944, 0.38259441585376347, 0.3875205240324017, 0.3875205240324017, 0.38587848810335296, 0.38259441585376347, 0.39737274107478915, 0.39408866882519966, 0.3875205240324017, 0.38916256035294244, 0.3940886687273267, 0.3743842350340438, 0.3875205242281477, 0.3875205242281477, 0.39244663289615084, 0.3957307048521214, 0.37766830728363326, 0.40065681332437864, 0.3940886687273267, 0.40065681332437864, 0.4022988494491734, 0.40065681332437864, 0.3957307051457404, 0.39244663260253193, 0.379310343506301, 0.38916256064656135, 0.38095237992471465, 0.39737274136840806, 0.38916256054868836, 0.3940886690209456, 0.40229884954704637, 0.3842364520764312, 0.4088669943398443, 0.4039408858675871, 0.40394088596546, 0.3908045968692291, 0.39080459667348316, 0.4006568135201246, 0.39901477739532987, 0.38259441585376347, 0.3957307051457404, 0.3825944159516364, 0.3940886690209456, 0.4088669943398443, 0.3875205246196396, 0.3842364522721771, 0.3891625608423073, 0.38916256054868836, 0.40229884954704637, 0.38916256054868836, 0.40886699414409833, 0.3957307050478674, 0.3990147775910758, 0.3858784882990989, 0.38587848810335296, 0.39408866892307265, 0.3973727412705351, 0.38423645217430413, 0.3858784882990989, 0.40229884964491935, 0.40558292179663585, 0.38587848820122594, 0.3858784882990989, 0.3694581272468974, 0.40229884954704637, 0.3825944160495094, 0.40229884954704637, 0.39737274107478915, 0.38587848820122594, 0.3990147772974569, 0.3924466327004049, 0.3990147771995839, 0.4006568134222516, 0.39737274117266214, 0.39737274117266214, 0.4006568134222516, 0.3842364520764312, 0.3908045968692291, 0.38916256054868836, 0.4039408857697141, 0.4039408857697141, 0.4039408857697141, 0.38423645217430413, 0.3825944160495094, 0.38916256074443434, 0.3891625608423073, 0.3875205244238936, 0.38752052540262344, 0.3908045967713561, 0.39080459696710207, 0.3908045967713561, 0.37931034419141185, 0.4006568135201246, 0.3908045967713561, 0.3793103438977929, 0.3908045967713561, 0.37438423571915463, 0.40558292199238183, 0.40722495811717657, 0.40722495811717657, 0.40722495811717657, 0.3694581273447704, 0.4072249582150495, 0.40558292209025476, 0.40065681371587053, 0.3990147776889488, 0.40065681371587053, 0.4121510665894338, 0.37602627194182237, 0.3908045967713561], 'loss': [1.0821763146339745, 1.0778963810609352, 1.0745726474746298, 1.074605172762391, 1.0742426001315735, 1.073581259891972, 1.0736536540534707, 1.0737395564633474, 1.0739239180602087, 1.0737842195087879, 1.0737616462139623, 1.073798831336552, 1.0736884890640541, 1.073553345286626, 1.0735387110367447, 1.0736575538862412, 1.0736745932753327, 1.0735551315656189, 1.0736678629685232, 1.0735578055254489, 1.073401762327864, 1.0733853738655545, 1.0732841455471345, 1.0732320293019195, 1.0733041830865755, 1.0735514293216337, 1.0734072341566459, 1.0733035872849106, 1.073052151296173, 1.0731118510146405, 1.073338444472828, 1.073147339644618, 1.072945800552133, 1.0730418931287418, 1.0730689335163124, 1.0730698931633325, 1.072973146575677, 1.073049479735216, 1.0733265123328144, 1.0733621191439933, 1.0732602300095606, 1.0731758227828103, 1.0730867609106296, 1.0731511301573298, 1.0729289613465265, 1.0729434859336524, 1.072819460197151, 1.0727644573736486, 1.0727477185290453, 1.0728277970633222, 1.0730507260230533, 1.072864278875582, 1.0729508301071073, 1.072890935688293, 1.0727814446239745, 1.0725748613874526, 1.0725783010772612, 1.072486342982345, 1.0725943819453339, 1.0721017024355504, 1.072237358935315, 1.0722960479939987, 1.0723496512955464, 1.0724932775849925, 1.0729807882093552, 1.0731766277270152, 1.0729225644830316, 1.0729893913014468, 1.073372223832524, 1.0725983143342348, 1.0729833282239627, 1.0727554056433926, 1.072159299762342, 1.0727725354063438, 1.072351257659082, 1.0724018664330672, 1.0721153966700028, 1.0719190325335555, 1.0718949093221395, 1.0721154394091033, 1.0717682330270568, 1.0716614075999484, 1.0715176682207863, 1.0713041452411753, 1.0712057900379814, 1.0710868804117002, 1.0711439488604817, 1.0716301036321652, 1.0724201339471022, 1.0719503040920781, 1.074056551833417, 1.0755240045288994, 1.074112585193078, 1.074241152044684, 1.0729593917819265, 1.0732652015999358, 1.073159248726079, 1.0727853341269054, 1.0726946444971606, 1.0726283005865203, 1.0724999651527014, 1.0722201342455415, 1.0723203001081087, 1.0722476487287016, 1.0719543033067205, 1.0720804418626506, 1.0720537261062089, 1.072465609133366, 1.0719846536001876, 1.0724461398330312, 1.071495900653471, 1.072408116426801, 1.0717030732783448, 1.071935392650001, 1.0716844836299668, 1.07229477590604, 1.072086456913723, 1.0718992293492968, 1.0717698618616656, 1.072031299876971, 1.0720064494154538, 1.0722448678476855, 1.071317156286455, 1.0713058404119598, 1.0708948525559976, 1.07125230398511, 1.0711440815817894, 1.0714568178756525, 1.0709607155171263, 1.071536618679211, 1.0711271283073347, 1.071502404682935, 1.0708500527258527, 1.0714096329784981, 1.071366486314386, 1.072053696340604, 1.0735734865650748, 1.0715653333331037, 1.0716123864880822, 1.0743867447733635, 1.0739795785665023, 1.0730247252775658, 1.0735763591907352, 1.0741041787595964, 1.072841969309891, 1.073156467845063, 1.072905671425179, 1.072600812197221, 1.0725551034390803, 1.072556120512177, 1.0723321898027611, 1.0718465312060879, 1.0717883836072573, 1.071771449964394, 1.0711494885920498, 1.0708954244178912, 1.0709988333606133, 1.0712884417304758, 1.0719325450411568, 1.072215578639287, 1.0764894252440278, 1.0751190877303451, 1.0739648090985277, 1.0730336270537955, 1.0745143329338371, 1.073993689029858, 1.0738091343482172, 1.0735378070533643, 1.0737794184342058, 1.0737685410638609, 1.0734413269364125, 1.0733621684432764, 1.0733385820408377, 1.0731152828713952, 1.0732688816176303, 1.0730935243120918, 1.0732481771426035, 1.0730147803833352, 1.0732007819279508, 1.0731279552350055, 1.0733019617793496, 1.073277320509329, 1.073263238734533, 1.0732491157872477, 1.073152586322056, 1.0731961364374023, 1.073136048640069, 1.0732077020394484, 1.0730529300486038, 1.0729917504214652, 1.0729979168952613, 1.0731847815445072, 1.0731927580412408, 1.0730328911383784, 1.073204609598712, 1.0729591257518323, 1.0731069658815984, 1.0729566284273684, 1.072974566610442, 1.0727959735192802, 1.0726275914993129, 1.0725593269728049, 1.0727520706227673, 1.0724224434740979, 1.0723463874088421, 1.072407963584336, 1.0722286311997524, 1.0722955515742056, 1.0722700875887392, 1.0724589681233714, 1.0723073272000103, 1.0722629098187237, 1.072359950929207, 1.0722939249426433, 1.0722778892615004, 1.072372272029305, 1.0722295295041688, 1.0720449030521715, 1.0721110075651008, 1.0722294082387027, 1.0719684153366873, 1.0720379643861273, 1.072195236149265, 1.0720941699261048, 1.0720497862759066, 1.07177571966418, 1.0718505616060763, 1.0723230622877085, 1.0718982952086589, 1.0719088845674016, 1.072075052525718, 1.0719003656806398, 1.0718134703332638, 1.071655250672687, 1.0715576391690076, 1.0714587275742016, 1.0716064676856603, 1.0717031828431867, 1.071452535545067, 1.071227867745276, 1.0714369344515478, 1.0711943220064135, 1.0719250854280695, 1.0717302334626841, 1.0716937149329842, 1.0718600797946938, 1.0714163667612253, 1.0715156248707547, 1.0713639104635564, 1.0711303882285554, 1.0721764356448664, 1.0719630598287562, 1.071464905503839, 1.0713129417607428, 1.071267328321077, 1.0712880941876641, 1.0710334616275294, 1.0710381728178178, 1.0710171754844868, 1.0708313899363335, 1.0706645054983652, 1.0706207180904412, 1.0704588911127015, 1.0704498557829025, 1.0704952317341643, 1.0704263741475601, 1.0708781837193138, 1.0709014326639978, 1.0707647901296127, 1.0715847384513526, 1.071275623965802, 1.0717635532668974, 1.0709308803448687, 1.0715550334057033, 1.0707262118983807, 1.0711976745534972, 1.0717968371369755, 1.0708412146910995, 1.0713969018180267, 1.0712348733839314, 1.0711351742734654, 1.0709872233549427, 1.0708141836542369, 1.070930266625093, 1.070913120853338, 1.0718029766846486, 1.0706642141087588, 1.070563591628104, 1.0700966638216494, 1.070272669263444, 1.0700596399131008, 1.0702367848684167, 1.069527594558512, 1.0697419146982308, 1.0694782142032098, 1.0692381046635904, 1.0693050185268176, 1.0695096969604492, 1.0700165833780653, 1.0690640931256743], 'acc': [0.3724845999564968, 0.3815195073582064, 0.3963039023920251, 0.3942505146933287, 0.39425051391002336, 0.394250514301676, 0.3942505119150424, 0.3942505123434126, 0.3942505123066951, 0.3942505152808078, 0.396303900629588, 0.39507186895033664, 0.39425051449750237, 0.3942505115233897, 0.3942505141058497, 0.3942505142649586, 0.3942505152808078, 0.394250514301676, 0.3942505126983478, 0.39425051211086876, 0.3971252575914473, 0.39589322536143434, 0.39425051492587254, 0.3938398359003008, 0.3954825457483836, 0.4024640643131562, 0.40246406666307233, 0.40246406525557044, 0.40287474428114217, 0.40287474545610025, 0.39589322238732166, 0.39671458118505304, 0.40164271201441176, 0.4016427108394537, 0.40123203420051556, 0.39876796512633134, 0.3995893225165608, 0.40041067681028614, 0.40041067618608966, 0.4004106789276585, 0.4004106783768969, 0.40246406744637775, 0.4008213544283559, 0.4012320306389239, 0.3991786433318802, 0.394661189178177, 0.3963038996137388, 0.3979466112976936, 0.39753593207629556, 0.3967145776234613, 0.3942505117559335, 0.39876796907957573, 0.4008213569740985, 0.3987679657138104, 0.3999999986047373, 0.4032854206875364, 0.3987679663380069, 0.40082135497911753, 0.40082135736575114, 0.4020533862667162, 0.3987679667296596, 0.4000000013095887, 0.4020533902199606, 0.4020533892408289, 0.3991786445435557, 0.39794661364760975, 0.4032854226825174, 0.4000000003671744, 0.39753593309214474, 0.3991786447393821, 0.4024640668588987, 0.40369609752230085, 0.4045174519751351, 0.4000000019337852, 0.4049281305723367, 0.4028747416986822, 0.40164270845282, 0.40328541951257835, 0.4012320343963419, 0.40205339041578697, 0.4012320314222293, 0.4032854212750155, 0.4016427100194308, 0.4016427100194308, 0.40246406505974414, 0.4045174551083567, 0.40328542307417004, 0.4008213569740985, 0.40533880803129757, 0.4057494844376918, 0.3954825485266699, 0.3852156047458766, 0.3987679653588751, 0.39548254441431663, 0.40246406447226507, 0.3856262829514255, 0.39178644542331814, 0.40164270845282, 0.39548254496507823, 0.3999999982130846, 0.4041067775270043, 0.4012320314222293, 0.4008213563866194, 0.40246406744637775, 0.4020533902199606, 0.4000000007588271, 0.4004106799435077, 0.40164270845282, 0.4024640646680914, 0.40205338607088986, 0.40410677435706527, 0.4000000019337852, 0.39876796692548594, 0.40205339041578697, 0.40287474111120314, 0.4016427088811902, 0.4061601624474144, 0.3991786445435557, 0.4036960965431691, 0.4086242279600069, 0.402053387833327, 0.4020533894366552, 0.40451745177930876, 0.40328542107918913, 0.4049281295932049, 0.40533881014866996, 0.40164270884447273, 0.4069815172919013, 0.40903490874801574, 0.4061601658131797, 0.4032854203326012, 0.40369609775484466, 0.4053388079945801, 0.4053388109686928, 0.40657084065296323, 0.4094455843711046, 0.4000000019337852, 0.40739219572999397, 0.40657084006548416, 0.39219712386141076, 0.4016427094319518, 0.40041067896437593, 0.3971252546540521, 0.39958932153742904, 0.3979466102818444, 0.3967145772318086, 0.4045174527584405, 0.407802873702999, 0.4032854217033856, 0.40739219510579744, 0.40698151905433844, 0.4032854220950383, 0.40082135677827213, 0.40328542229086467, 0.4061601658131797, 0.406981520853493, 0.40821355034193707, 0.4045174527584405, 0.4016427127977171, 0.4004106783768969, 0.3872689926403993, 0.39753593325125364, 0.3983572886832196, 0.40287474150285585, 0.40082135482000864, 0.3983572912656796, 0.3987679688837494, 0.40041067739776515, 0.40369609693482184, 0.39835729106985324, 0.4012320326339048, 0.39794661364760975, 0.40082135732903373, 0.40041067775270045, 0.4049281305723367, 0.4069815200701876, 0.39753593579699614, 0.40123203122640294, 0.3983572916573323, 0.404517453382637, 0.3995893231407573, 0.4020533894366552, 0.4028747446727948, 0.40328542092008024, 0.40287474346111934, 0.39999999938804265, 0.400410679551855, 0.3987679653221577, 0.39712525762816475, 0.4032854206875364, 0.4028747433020104, 0.3979466132559571, 0.4020533866583689, 0.40369609693482184, 0.39712525622066286, 0.4036960987339764, 0.4061601628757845, 0.4008213575615775, 0.3991786461101665, 0.4000000005630007, 0.40123203338049274, 0.40533880721127474, 0.4057494870201518, 0.4008213563866194, 0.4004106783768969, 0.40082135716992484, 0.4020533898283079, 0.4012320322422521, 0.40041067818107057, 0.3975359338387327, 0.39876796907957573, 0.39712525821564376, 0.3963039020003724, 0.3983572902865478, 0.3991786433318802, 0.4020533868541952, 0.4028747456519266, 0.399178644151903, 0.3995893205582973, 0.3979466142718063, 0.3979466098901917, 0.39835729048237417, 0.40164271162275905, 0.39917864630599287, 0.40205338763750065, 0.3991786457185138, 0.4036960996763907, 0.3971252584114701, 0.40205338607088986, 0.4012320334172102, 0.40164270962777815, 0.40082135325339785, 0.4008213547832912, 0.4049281313923595, 0.4012320316180556, 0.4041067761195024, 0.40451745334591954, 0.4032854189250993, 0.40369609791395356, 0.4016427113902153, 0.4016427090402991, 0.4024640656839406, 0.3975359328596009, 0.39917864336859765, 0.3999999981763671, 0.39999999935132524, 0.3979466142350888, 0.39958932094995003, 0.40410677615621987, 0.4032854217033856, 0.38932238210153286, 0.4008213569740985, 0.40369609853815003, 0.4008213569740985, 0.40287474150285585, 0.40205339002413426, 0.4032854201367748, 0.40041067618608966, 0.3995893245115417, 0.40451745416594237, 0.4016427122102381, 0.4049281305723367, 0.4041067751770881, 0.4012320310305766, 0.39999999778471446, 0.40041067618608966, 0.4032854211159066, 0.4004106755986106, 0.3987679657138104, 0.3987679692754021, 0.3991786466976456, 0.39712525524153114, 0.4036960967389955, 0.40328542029588377, 0.40492813374227565, 0.40164271021525716, 0.4004106775935915, 0.4020533888491762, 0.38850102467458597, 0.4008213532166804, 0.4012320326339048, 0.40451745177930876, 0.38891170467928937, 0.40205338607088986, 0.40616016205576166, 0.39383983413786366, 0.4049281329589703, 0.39835729048237417, 0.3995893201666446, 0.39835728871993703, 0.40287474490533864, 0.3901437389042833, 0.40328541912092564, 0.4008213547832912, 0.4049281335464493, 0.4024640678380304, 0.4082135535118761, 0.41232032734265806, 0.39466119309470393, 0.40246406447226507]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
