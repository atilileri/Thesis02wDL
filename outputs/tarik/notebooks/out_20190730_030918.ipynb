{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf29.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 03:09:18 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '0Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '03', '02', '04', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001DF0172BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001DF64396EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6112, Accuracy:0.2041, Validation Loss:1.6085, Validation Accuracy:0.2315\n",
    "Epoch #2: Loss:1.6083, Accuracy:0.2320, Validation Loss:1.6073, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6070, Accuracy:0.2320, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6063, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6030, Accuracy:0.2316, Validation Loss:1.6046, Validation Accuracy:0.2315\n",
    "Epoch #13: Loss:1.6027, Accuracy:0.2324, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6021, Accuracy:0.2423, Validation Loss:1.6043, Validation Accuracy:0.2299\n",
    "Epoch #15: Loss:1.6019, Accuracy:0.2402, Validation Loss:1.6046, Validation Accuracy:0.2365\n",
    "Epoch #16: Loss:1.6013, Accuracy:0.2431, Validation Loss:1.6044, Validation Accuracy:0.2348\n",
    "Epoch #17: Loss:1.6009, Accuracy:0.2435, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6010, Accuracy:0.2415, Validation Loss:1.6042, Validation Accuracy:0.2348\n",
    "Epoch #19: Loss:1.6008, Accuracy:0.2435, Validation Loss:1.6038, Validation Accuracy:0.2282\n",
    "Epoch #20: Loss:1.6009, Accuracy:0.2456, Validation Loss:1.6041, Validation Accuracy:0.2365\n",
    "Epoch #21: Loss:1.6008, Accuracy:0.2456, Validation Loss:1.6057, Validation Accuracy:0.2348\n",
    "Epoch #22: Loss:1.6012, Accuracy:0.2448, Validation Loss:1.6058, Validation Accuracy:0.2315\n",
    "Epoch #23: Loss:1.6008, Accuracy:0.2468, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6003, Accuracy:0.2444, Validation Loss:1.6065, Validation Accuracy:0.2233\n",
    "Epoch #25: Loss:1.6006, Accuracy:0.2456, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6008, Accuracy:0.2431, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6013, Accuracy:0.2423, Validation Loss:1.6051, Validation Accuracy:0.2348\n",
    "Epoch #28: Loss:1.6016, Accuracy:0.2407, Validation Loss:1.6042, Validation Accuracy:0.2348\n",
    "Epoch #29: Loss:1.6036, Accuracy:0.2353, Validation Loss:1.6043, Validation Accuracy:0.2348\n",
    "Epoch #30: Loss:1.6021, Accuracy:0.2415, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #31: Loss:1.6025, Accuracy:0.2427, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #32: Loss:1.6028, Accuracy:0.2407, Validation Loss:1.6049, Validation Accuracy:0.2299\n",
    "Epoch #33: Loss:1.6018, Accuracy:0.2390, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #34: Loss:1.6013, Accuracy:0.2411, Validation Loss:1.6034, Validation Accuracy:0.2348\n",
    "Epoch #35: Loss:1.6021, Accuracy:0.2415, Validation Loss:1.6035, Validation Accuracy:0.2299\n",
    "Epoch #36: Loss:1.6017, Accuracy:0.2444, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #37: Loss:1.6011, Accuracy:0.2448, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #38: Loss:1.6007, Accuracy:0.2439, Validation Loss:1.6035, Validation Accuracy:0.2381\n",
    "Epoch #39: Loss:1.6010, Accuracy:0.2452, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #40: Loss:1.6007, Accuracy:0.2448, Validation Loss:1.6039, Validation Accuracy:0.2299\n",
    "Epoch #41: Loss:1.6005, Accuracy:0.2464, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #42: Loss:1.6001, Accuracy:0.2435, Validation Loss:1.6038, Validation Accuracy:0.2348\n",
    "Epoch #43: Loss:1.6001, Accuracy:0.2435, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #44: Loss:1.5999, Accuracy:0.2452, Validation Loss:1.6039, Validation Accuracy:0.2282\n",
    "Epoch #45: Loss:1.5997, Accuracy:0.2452, Validation Loss:1.6031, Validation Accuracy:0.2299\n",
    "Epoch #46: Loss:1.6005, Accuracy:0.2456, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #47: Loss:1.6006, Accuracy:0.2439, Validation Loss:1.6032, Validation Accuracy:0.2299\n",
    "Epoch #48: Loss:1.6007, Accuracy:0.2444, Validation Loss:1.6044, Validation Accuracy:0.2200\n",
    "Epoch #49: Loss:1.6008, Accuracy:0.2435, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #50: Loss:1.6000, Accuracy:0.2464, Validation Loss:1.6042, Validation Accuracy:0.2348\n",
    "Epoch #51: Loss:1.5999, Accuracy:0.2468, Validation Loss:1.6039, Validation Accuracy:0.2365\n",
    "Epoch #52: Loss:1.5998, Accuracy:0.2456, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #53: Loss:1.5995, Accuracy:0.2452, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #54: Loss:1.5993, Accuracy:0.2468, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #55: Loss:1.5995, Accuracy:0.2460, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #56: Loss:1.5994, Accuracy:0.2460, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #57: Loss:1.5991, Accuracy:0.2472, Validation Loss:1.6043, Validation Accuracy:0.2315\n",
    "Epoch #58: Loss:1.5993, Accuracy:0.2476, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #59: Loss:1.5995, Accuracy:0.2480, Validation Loss:1.6042, Validation Accuracy:0.2299\n",
    "Epoch #60: Loss:1.5988, Accuracy:0.2464, Validation Loss:1.6037, Validation Accuracy:0.2299\n",
    "Epoch #61: Loss:1.5989, Accuracy:0.2480, Validation Loss:1.6048, Validation Accuracy:0.2217\n",
    "Epoch #62: Loss:1.5987, Accuracy:0.2468, Validation Loss:1.6049, Validation Accuracy:0.2200\n",
    "Epoch #63: Loss:1.5984, Accuracy:0.2472, Validation Loss:1.6044, Validation Accuracy:0.2250\n",
    "Epoch #64: Loss:1.5986, Accuracy:0.2452, Validation Loss:1.6037, Validation Accuracy:0.2299\n",
    "Epoch #65: Loss:1.5985, Accuracy:0.2464, Validation Loss:1.6047, Validation Accuracy:0.2282\n",
    "Epoch #66: Loss:1.5989, Accuracy:0.2444, Validation Loss:1.6050, Validation Accuracy:0.2381\n",
    "Epoch #67: Loss:1.5998, Accuracy:0.2431, Validation Loss:1.6045, Validation Accuracy:0.2266\n",
    "Epoch #68: Loss:1.5995, Accuracy:0.2456, Validation Loss:1.6045, Validation Accuracy:0.2365\n",
    "Epoch #69: Loss:1.6001, Accuracy:0.2448, Validation Loss:1.6043, Validation Accuracy:0.2348\n",
    "Epoch #70: Loss:1.5991, Accuracy:0.2456, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #71: Loss:1.5994, Accuracy:0.2497, Validation Loss:1.6045, Validation Accuracy:0.2250\n",
    "Epoch #72: Loss:1.5995, Accuracy:0.2476, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #73: Loss:1.5992, Accuracy:0.2456, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #74: Loss:1.5986, Accuracy:0.2456, Validation Loss:1.6046, Validation Accuracy:0.2365\n",
    "Epoch #75: Loss:1.5984, Accuracy:0.2456, Validation Loss:1.6044, Validation Accuracy:0.2299\n",
    "Epoch #76: Loss:1.5980, Accuracy:0.2480, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #77: Loss:1.5980, Accuracy:0.2480, Validation Loss:1.6045, Validation Accuracy:0.2282\n",
    "Epoch #78: Loss:1.5979, Accuracy:0.2485, Validation Loss:1.6051, Validation Accuracy:0.2348\n",
    "Epoch #79: Loss:1.5976, Accuracy:0.2468, Validation Loss:1.6053, Validation Accuracy:0.2282\n",
    "Epoch #80: Loss:1.5981, Accuracy:0.2460, Validation Loss:1.6054, Validation Accuracy:0.2299\n",
    "Epoch #81: Loss:1.5977, Accuracy:0.2476, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #82: Loss:1.5974, Accuracy:0.2472, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #83: Loss:1.5978, Accuracy:0.2472, Validation Loss:1.6048, Validation Accuracy:0.2299\n",
    "Epoch #84: Loss:1.5983, Accuracy:0.2485, Validation Loss:1.6050, Validation Accuracy:0.2299\n",
    "Epoch #85: Loss:1.5978, Accuracy:0.2493, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #86: Loss:1.5980, Accuracy:0.2444, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #87: Loss:1.5977, Accuracy:0.2489, Validation Loss:1.6048, Validation Accuracy:0.2365\n",
    "Epoch #88: Loss:1.5978, Accuracy:0.2485, Validation Loss:1.6044, Validation Accuracy:0.2414\n",
    "Epoch #89: Loss:1.5973, Accuracy:0.2489, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:1.5974, Accuracy:0.2464, Validation Loss:1.6033, Validation Accuracy:0.2348\n",
    "Epoch #91: Loss:1.5969, Accuracy:0.2431, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #92: Loss:1.5967, Accuracy:0.2505, Validation Loss:1.6041, Validation Accuracy:0.2315\n",
    "Epoch #93: Loss:1.5968, Accuracy:0.2501, Validation Loss:1.6042, Validation Accuracy:0.2299\n",
    "Epoch #94: Loss:1.5970, Accuracy:0.2517, Validation Loss:1.6039, Validation Accuracy:0.2348\n",
    "Epoch #95: Loss:1.5965, Accuracy:0.2476, Validation Loss:1.6033, Validation Accuracy:0.2299\n",
    "Epoch #96: Loss:1.5967, Accuracy:0.2452, Validation Loss:1.6033, Validation Accuracy:0.2348\n",
    "Epoch #97: Loss:1.5969, Accuracy:0.2468, Validation Loss:1.6038, Validation Accuracy:0.2266\n",
    "Epoch #98: Loss:1.5966, Accuracy:0.2509, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #99: Loss:1.5968, Accuracy:0.2501, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #100: Loss:1.5958, Accuracy:0.2431, Validation Loss:1.6034, Validation Accuracy:0.2348\n",
    "Epoch #101: Loss:1.5961, Accuracy:0.2460, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #102: Loss:1.5956, Accuracy:0.2501, Validation Loss:1.6043, Validation Accuracy:0.2299\n",
    "Epoch #103: Loss:1.5961, Accuracy:0.2419, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #104: Loss:1.5956, Accuracy:0.2431, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #105: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.6041, Validation Accuracy:0.2282\n",
    "Epoch #106: Loss:1.5956, Accuracy:0.2460, Validation Loss:1.6047, Validation Accuracy:0.2315\n",
    "Epoch #107: Loss:1.5958, Accuracy:0.2439, Validation Loss:1.6050, Validation Accuracy:0.2315\n",
    "Epoch #108: Loss:1.5957, Accuracy:0.2407, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #109: Loss:1.5958, Accuracy:0.2411, Validation Loss:1.6049, Validation Accuracy:0.2282\n",
    "Epoch #110: Loss:1.5956, Accuracy:0.2423, Validation Loss:1.6049, Validation Accuracy:0.2348\n",
    "Epoch #111: Loss:1.5963, Accuracy:0.2460, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #112: Loss:1.5960, Accuracy:0.2501, Validation Loss:1.6037, Validation Accuracy:0.2381\n",
    "Epoch #113: Loss:1.5958, Accuracy:0.2464, Validation Loss:1.6048, Validation Accuracy:0.2365\n",
    "Epoch #114: Loss:1.5960, Accuracy:0.2407, Validation Loss:1.6047, Validation Accuracy:0.2348\n",
    "Epoch #115: Loss:1.5959, Accuracy:0.2456, Validation Loss:1.6036, Validation Accuracy:0.2381\n",
    "Epoch #116: Loss:1.5959, Accuracy:0.2464, Validation Loss:1.6039, Validation Accuracy:0.2365\n",
    "Epoch #117: Loss:1.5960, Accuracy:0.2509, Validation Loss:1.6044, Validation Accuracy:0.2381\n",
    "Epoch #118: Loss:1.5983, Accuracy:0.2386, Validation Loss:1.6038, Validation Accuracy:0.2381\n",
    "Epoch #119: Loss:1.5986, Accuracy:0.2423, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #120: Loss:1.6059, Accuracy:0.2316, Validation Loss:1.6097, Validation Accuracy:0.2332\n",
    "Epoch #121: Loss:1.6035, Accuracy:0.2312, Validation Loss:1.6070, Validation Accuracy:0.2348\n",
    "Epoch #122: Loss:1.6019, Accuracy:0.2378, Validation Loss:1.6066, Validation Accuracy:0.2282\n",
    "Epoch #123: Loss:1.6001, Accuracy:0.2460, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #124: Loss:1.5980, Accuracy:0.2452, Validation Loss:1.6043, Validation Accuracy:0.2381\n",
    "Epoch #125: Loss:1.5974, Accuracy:0.2472, Validation Loss:1.6043, Validation Accuracy:0.2381\n",
    "Epoch #126: Loss:1.5970, Accuracy:0.2501, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #127: Loss:1.5963, Accuracy:0.2493, Validation Loss:1.6036, Validation Accuracy:0.2299\n",
    "Epoch #128: Loss:1.5965, Accuracy:0.2517, Validation Loss:1.6039, Validation Accuracy:0.2299\n",
    "Epoch #129: Loss:1.5964, Accuracy:0.2444, Validation Loss:1.6042, Validation Accuracy:0.2282\n",
    "Epoch #130: Loss:1.5964, Accuracy:0.2423, Validation Loss:1.6045, Validation Accuracy:0.2282\n",
    "Epoch #131: Loss:1.5958, Accuracy:0.2431, Validation Loss:1.6044, Validation Accuracy:0.2282\n",
    "Epoch #132: Loss:1.5957, Accuracy:0.2435, Validation Loss:1.6048, Validation Accuracy:0.2282\n",
    "Epoch #133: Loss:1.5956, Accuracy:0.2439, Validation Loss:1.6053, Validation Accuracy:0.2282\n",
    "Epoch #134: Loss:1.5954, Accuracy:0.2431, Validation Loss:1.6056, Validation Accuracy:0.2299\n",
    "Epoch #135: Loss:1.5955, Accuracy:0.2427, Validation Loss:1.6055, Validation Accuracy:0.2315\n",
    "Epoch #136: Loss:1.5953, Accuracy:0.2419, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #137: Loss:1.5955, Accuracy:0.2423, Validation Loss:1.6053, Validation Accuracy:0.2282\n",
    "Epoch #138: Loss:1.5948, Accuracy:0.2439, Validation Loss:1.6045, Validation Accuracy:0.2266\n",
    "Epoch #139: Loss:1.5947, Accuracy:0.2435, Validation Loss:1.6046, Validation Accuracy:0.2266\n",
    "Epoch #140: Loss:1.5947, Accuracy:0.2419, Validation Loss:1.6046, Validation Accuracy:0.2315\n",
    "Epoch #141: Loss:1.5947, Accuracy:0.2423, Validation Loss:1.6045, Validation Accuracy:0.2266\n",
    "Epoch #142: Loss:1.5945, Accuracy:0.2472, Validation Loss:1.6047, Validation Accuracy:0.2299\n",
    "Epoch #143: Loss:1.5949, Accuracy:0.2522, Validation Loss:1.6051, Validation Accuracy:0.2266\n",
    "Epoch #144: Loss:1.5948, Accuracy:0.2444, Validation Loss:1.6056, Validation Accuracy:0.2299\n",
    "Epoch #145: Loss:1.5948, Accuracy:0.2439, Validation Loss:1.6060, Validation Accuracy:0.2315\n",
    "Epoch #146: Loss:1.5950, Accuracy:0.2435, Validation Loss:1.6052, Validation Accuracy:0.2315\n",
    "Epoch #147: Loss:1.5949, Accuracy:0.2439, Validation Loss:1.6050, Validation Accuracy:0.2315\n",
    "Epoch #148: Loss:1.5945, Accuracy:0.2435, Validation Loss:1.6046, Validation Accuracy:0.2315\n",
    "Epoch #149: Loss:1.5942, Accuracy:0.2444, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #150: Loss:1.5943, Accuracy:0.2439, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #151: Loss:1.5947, Accuracy:0.2411, Validation Loss:1.6050, Validation Accuracy:0.2299\n",
    "Epoch #152: Loss:1.5947, Accuracy:0.2439, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #153: Loss:1.5946, Accuracy:0.2476, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #154: Loss:1.5953, Accuracy:0.2398, Validation Loss:1.6054, Validation Accuracy:0.2315\n",
    "Epoch #155: Loss:1.5958, Accuracy:0.2452, Validation Loss:1.6044, Validation Accuracy:0.2348\n",
    "Epoch #156: Loss:1.5957, Accuracy:0.2452, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #157: Loss:1.5952, Accuracy:0.2411, Validation Loss:1.6046, Validation Accuracy:0.2365\n",
    "Epoch #158: Loss:1.5947, Accuracy:0.2526, Validation Loss:1.6049, Validation Accuracy:0.2414\n",
    "Epoch #159: Loss:1.5941, Accuracy:0.2546, Validation Loss:1.6044, Validation Accuracy:0.2348\n",
    "Epoch #160: Loss:1.5946, Accuracy:0.2493, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #161: Loss:1.5943, Accuracy:0.2497, Validation Loss:1.6056, Validation Accuracy:0.2414\n",
    "Epoch #162: Loss:1.5948, Accuracy:0.2526, Validation Loss:1.6041, Validation Accuracy:0.2200\n",
    "Epoch #163: Loss:1.5949, Accuracy:0.2472, Validation Loss:1.6035, Validation Accuracy:0.2184\n",
    "Epoch #164: Loss:1.5946, Accuracy:0.2505, Validation Loss:1.6038, Validation Accuracy:0.2167\n",
    "Epoch #165: Loss:1.5940, Accuracy:0.2468, Validation Loss:1.6040, Validation Accuracy:0.2200\n",
    "Epoch #166: Loss:1.5941, Accuracy:0.2526, Validation Loss:1.6050, Validation Accuracy:0.2266\n",
    "Epoch #167: Loss:1.5935, Accuracy:0.2587, Validation Loss:1.6063, Validation Accuracy:0.2167\n",
    "Epoch #168: Loss:1.5932, Accuracy:0.2534, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #169: Loss:1.5924, Accuracy:0.2526, Validation Loss:1.6060, Validation Accuracy:0.2184\n",
    "Epoch #170: Loss:1.5923, Accuracy:0.2534, Validation Loss:1.6054, Validation Accuracy:0.2167\n",
    "Epoch #171: Loss:1.5918, Accuracy:0.2542, Validation Loss:1.6052, Validation Accuracy:0.2233\n",
    "Epoch #172: Loss:1.5919, Accuracy:0.2567, Validation Loss:1.6054, Validation Accuracy:0.2184\n",
    "Epoch #173: Loss:1.5928, Accuracy:0.2554, Validation Loss:1.6053, Validation Accuracy:0.2118\n",
    "Epoch #174: Loss:1.5919, Accuracy:0.2563, Validation Loss:1.6058, Validation Accuracy:0.2167\n",
    "Epoch #175: Loss:1.5918, Accuracy:0.2571, Validation Loss:1.6051, Validation Accuracy:0.2036\n",
    "Epoch #176: Loss:1.5916, Accuracy:0.2587, Validation Loss:1.6041, Validation Accuracy:0.2167\n",
    "Epoch #177: Loss:1.5928, Accuracy:0.2563, Validation Loss:1.6060, Validation Accuracy:0.2135\n",
    "Epoch #178: Loss:1.5918, Accuracy:0.2604, Validation Loss:1.6073, Validation Accuracy:0.2020\n",
    "Epoch #179: Loss:1.5908, Accuracy:0.2600, Validation Loss:1.6062, Validation Accuracy:0.2151\n",
    "Epoch #180: Loss:1.5917, Accuracy:0.2575, Validation Loss:1.6047, Validation Accuracy:0.2266\n",
    "Epoch #181: Loss:1.5905, Accuracy:0.2567, Validation Loss:1.6054, Validation Accuracy:0.2102\n",
    "Epoch #182: Loss:1.5908, Accuracy:0.2554, Validation Loss:1.6042, Validation Accuracy:0.2102\n",
    "Epoch #183: Loss:1.5908, Accuracy:0.2513, Validation Loss:1.6042, Validation Accuracy:0.2151\n",
    "Epoch #184: Loss:1.5915, Accuracy:0.2559, Validation Loss:1.6050, Validation Accuracy:0.2085\n",
    "Epoch #185: Loss:1.5955, Accuracy:0.2517, Validation Loss:1.6054, Validation Accuracy:0.2135\n",
    "Epoch #186: Loss:1.5961, Accuracy:0.2563, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #187: Loss:1.5925, Accuracy:0.2620, Validation Loss:1.6053, Validation Accuracy:0.2167\n",
    "Epoch #188: Loss:1.5921, Accuracy:0.2534, Validation Loss:1.6048, Validation Accuracy:0.2118\n",
    "Epoch #189: Loss:1.5922, Accuracy:0.2579, Validation Loss:1.6046, Validation Accuracy:0.2151\n",
    "Epoch #190: Loss:1.5916, Accuracy:0.2559, Validation Loss:1.6054, Validation Accuracy:0.2315\n",
    "Epoch #191: Loss:1.5919, Accuracy:0.2559, Validation Loss:1.6056, Validation Accuracy:0.2217\n",
    "Epoch #192: Loss:1.5922, Accuracy:0.2526, Validation Loss:1.6051, Validation Accuracy:0.2299\n",
    "Epoch #193: Loss:1.5939, Accuracy:0.2513, Validation Loss:1.6054, Validation Accuracy:0.2250\n",
    "Epoch #194: Loss:1.5935, Accuracy:0.2550, Validation Loss:1.6066, Validation Accuracy:0.2266\n",
    "Epoch #195: Loss:1.5930, Accuracy:0.2583, Validation Loss:1.6063, Validation Accuracy:0.2282\n",
    "Epoch #196: Loss:1.5946, Accuracy:0.2546, Validation Loss:1.6072, Validation Accuracy:0.2299\n",
    "Epoch #197: Loss:1.5940, Accuracy:0.2550, Validation Loss:1.6067, Validation Accuracy:0.2266\n",
    "Epoch #198: Loss:1.5945, Accuracy:0.2534, Validation Loss:1.6060, Validation Accuracy:0.2167\n",
    "Epoch #199: Loss:1.5937, Accuracy:0.2513, Validation Loss:1.6065, Validation Accuracy:0.2217\n",
    "Epoch #200: Loss:1.5933, Accuracy:0.2530, Validation Loss:1.6056, Validation Accuracy:0.2151\n",
    "Epoch #201: Loss:1.5942, Accuracy:0.2505, Validation Loss:1.6060, Validation Accuracy:0.2233\n",
    "Epoch #202: Loss:1.5921, Accuracy:0.2575, Validation Loss:1.6051, Validation Accuracy:0.2217\n",
    "Epoch #203: Loss:1.5927, Accuracy:0.2546, Validation Loss:1.6055, Validation Accuracy:0.2200\n",
    "Epoch #204: Loss:1.5920, Accuracy:0.2546, Validation Loss:1.6063, Validation Accuracy:0.2135\n",
    "Epoch #205: Loss:1.5914, Accuracy:0.2567, Validation Loss:1.6068, Validation Accuracy:0.2217\n",
    "Epoch #206: Loss:1.5911, Accuracy:0.2554, Validation Loss:1.6062, Validation Accuracy:0.2282\n",
    "Epoch #207: Loss:1.5912, Accuracy:0.2554, Validation Loss:1.6065, Validation Accuracy:0.2266\n",
    "Epoch #208: Loss:1.5903, Accuracy:0.2571, Validation Loss:1.6071, Validation Accuracy:0.2217\n",
    "Epoch #209: Loss:1.5903, Accuracy:0.2579, Validation Loss:1.6074, Validation Accuracy:0.2233\n",
    "Epoch #210: Loss:1.5901, Accuracy:0.2567, Validation Loss:1.6083, Validation Accuracy:0.2282\n",
    "Epoch #211: Loss:1.5896, Accuracy:0.2571, Validation Loss:1.6078, Validation Accuracy:0.2299\n",
    "Epoch #212: Loss:1.5899, Accuracy:0.2583, Validation Loss:1.6075, Validation Accuracy:0.2217\n",
    "Epoch #213: Loss:1.5896, Accuracy:0.2595, Validation Loss:1.6072, Validation Accuracy:0.2217\n",
    "Epoch #214: Loss:1.5897, Accuracy:0.2600, Validation Loss:1.6071, Validation Accuracy:0.2250\n",
    "Epoch #215: Loss:1.5900, Accuracy:0.2612, Validation Loss:1.6072, Validation Accuracy:0.2217\n",
    "Epoch #216: Loss:1.5894, Accuracy:0.2628, Validation Loss:1.6073, Validation Accuracy:0.2250\n",
    "Epoch #217: Loss:1.5892, Accuracy:0.2583, Validation Loss:1.6082, Validation Accuracy:0.2200\n",
    "Epoch #218: Loss:1.5897, Accuracy:0.2554, Validation Loss:1.6084, Validation Accuracy:0.2167\n",
    "Epoch #219: Loss:1.5889, Accuracy:0.2595, Validation Loss:1.6083, Validation Accuracy:0.2200\n",
    "Epoch #220: Loss:1.5895, Accuracy:0.2567, Validation Loss:1.6087, Validation Accuracy:0.2217\n",
    "Epoch #221: Loss:1.5893, Accuracy:0.2579, Validation Loss:1.6093, Validation Accuracy:0.2151\n",
    "Epoch #222: Loss:1.5893, Accuracy:0.2587, Validation Loss:1.6083, Validation Accuracy:0.2102\n",
    "Epoch #223: Loss:1.5896, Accuracy:0.2595, Validation Loss:1.6095, Validation Accuracy:0.2233\n",
    "Epoch #224: Loss:1.5901, Accuracy:0.2575, Validation Loss:1.6098, Validation Accuracy:0.2266\n",
    "Epoch #225: Loss:1.5894, Accuracy:0.2563, Validation Loss:1.6100, Validation Accuracy:0.2102\n",
    "Epoch #226: Loss:1.5897, Accuracy:0.2587, Validation Loss:1.6082, Validation Accuracy:0.2135\n",
    "Epoch #227: Loss:1.5909, Accuracy:0.2583, Validation Loss:1.6095, Validation Accuracy:0.2332\n",
    "Epoch #228: Loss:1.5900, Accuracy:0.2517, Validation Loss:1.6100, Validation Accuracy:0.2102\n",
    "Epoch #229: Loss:1.5893, Accuracy:0.2587, Validation Loss:1.6069, Validation Accuracy:0.2200\n",
    "Epoch #230: Loss:1.5899, Accuracy:0.2591, Validation Loss:1.6083, Validation Accuracy:0.2266\n",
    "Epoch #231: Loss:1.5877, Accuracy:0.2624, Validation Loss:1.6072, Validation Accuracy:0.2151\n",
    "Epoch #232: Loss:1.5889, Accuracy:0.2595, Validation Loss:1.6062, Validation Accuracy:0.2200\n",
    "Epoch #233: Loss:1.5882, Accuracy:0.2575, Validation Loss:1.6065, Validation Accuracy:0.2053\n",
    "Epoch #234: Loss:1.5881, Accuracy:0.2616, Validation Loss:1.6090, Validation Accuracy:0.2020\n",
    "Epoch #235: Loss:1.5879, Accuracy:0.2595, Validation Loss:1.6065, Validation Accuracy:0.2200\n",
    "Epoch #236: Loss:1.5868, Accuracy:0.2649, Validation Loss:1.6058, Validation Accuracy:0.2184\n",
    "Epoch #237: Loss:1.5870, Accuracy:0.2620, Validation Loss:1.6065, Validation Accuracy:0.2233\n",
    "Epoch #238: Loss:1.5878, Accuracy:0.2632, Validation Loss:1.6065, Validation Accuracy:0.2233\n",
    "Epoch #239: Loss:1.5881, Accuracy:0.2649, Validation Loss:1.6068, Validation Accuracy:0.2200\n",
    "Epoch #240: Loss:1.5877, Accuracy:0.2600, Validation Loss:1.6069, Validation Accuracy:0.2102\n",
    "Epoch #241: Loss:1.5865, Accuracy:0.2620, Validation Loss:1.6056, Validation Accuracy:0.2167\n",
    "Epoch #242: Loss:1.5868, Accuracy:0.2632, Validation Loss:1.6062, Validation Accuracy:0.2200\n",
    "Epoch #243: Loss:1.5855, Accuracy:0.2620, Validation Loss:1.6077, Validation Accuracy:0.2184\n",
    "Epoch #244: Loss:1.5866, Accuracy:0.2637, Validation Loss:1.6067, Validation Accuracy:0.2200\n",
    "Epoch #245: Loss:1.5858, Accuracy:0.2632, Validation Loss:1.6080, Validation Accuracy:0.2233\n",
    "Epoch #246: Loss:1.5858, Accuracy:0.2595, Validation Loss:1.6105, Validation Accuracy:0.2085\n",
    "Epoch #247: Loss:1.5854, Accuracy:0.2657, Validation Loss:1.6110, Validation Accuracy:0.2167\n",
    "Epoch #248: Loss:1.5871, Accuracy:0.2645, Validation Loss:1.6105, Validation Accuracy:0.2200\n",
    "Epoch #249: Loss:1.5856, Accuracy:0.2628, Validation Loss:1.6118, Validation Accuracy:0.2135\n",
    "Epoch #250: Loss:1.5868, Accuracy:0.2641, Validation Loss:1.6095, Validation Accuracy:0.2200\n",
    "Epoch #251: Loss:1.5861, Accuracy:0.2641, Validation Loss:1.6089, Validation Accuracy:0.2315\n",
    "Epoch #252: Loss:1.5845, Accuracy:0.2669, Validation Loss:1.6088, Validation Accuracy:0.2233\n",
    "Epoch #253: Loss:1.5854, Accuracy:0.2674, Validation Loss:1.6084, Validation Accuracy:0.2250\n",
    "Epoch #254: Loss:1.5858, Accuracy:0.2661, Validation Loss:1.6098, Validation Accuracy:0.2217\n",
    "Epoch #255: Loss:1.5853, Accuracy:0.2649, Validation Loss:1.6094, Validation Accuracy:0.2217\n",
    "Epoch #256: Loss:1.5861, Accuracy:0.2641, Validation Loss:1.6100, Validation Accuracy:0.2184\n",
    "Epoch #257: Loss:1.5859, Accuracy:0.2637, Validation Loss:1.6102, Validation Accuracy:0.2217\n",
    "Epoch #258: Loss:1.5856, Accuracy:0.2632, Validation Loss:1.6099, Validation Accuracy:0.2233\n",
    "Epoch #259: Loss:1.5858, Accuracy:0.2649, Validation Loss:1.6120, Validation Accuracy:0.2036\n",
    "Epoch #260: Loss:1.5852, Accuracy:0.2628, Validation Loss:1.6113, Validation Accuracy:0.2184\n",
    "Epoch #261: Loss:1.5844, Accuracy:0.2669, Validation Loss:1.6115, Validation Accuracy:0.2069\n",
    "Epoch #262: Loss:1.5873, Accuracy:0.2534, Validation Loss:1.6115, Validation Accuracy:0.2151\n",
    "Epoch #263: Loss:1.5916, Accuracy:0.2653, Validation Loss:1.6155, Validation Accuracy:0.2102\n",
    "Epoch #264: Loss:1.5881, Accuracy:0.2579, Validation Loss:1.6138, Validation Accuracy:0.2167\n",
    "Epoch #265: Loss:1.5902, Accuracy:0.2493, Validation Loss:1.6066, Validation Accuracy:0.2118\n",
    "Epoch #266: Loss:1.5890, Accuracy:0.2595, Validation Loss:1.6031, Validation Accuracy:0.2266\n",
    "Epoch #267: Loss:1.5876, Accuracy:0.2632, Validation Loss:1.6034, Validation Accuracy:0.2233\n",
    "Epoch #268: Loss:1.5873, Accuracy:0.2641, Validation Loss:1.6036, Validation Accuracy:0.2184\n",
    "Epoch #269: Loss:1.5898, Accuracy:0.2567, Validation Loss:1.6046, Validation Accuracy:0.2282\n",
    "Epoch #270: Loss:1.5862, Accuracy:0.2624, Validation Loss:1.6036, Validation Accuracy:0.2266\n",
    "Epoch #271: Loss:1.5882, Accuracy:0.2624, Validation Loss:1.6041, Validation Accuracy:0.2266\n",
    "Epoch #272: Loss:1.5866, Accuracy:0.2600, Validation Loss:1.6052, Validation Accuracy:0.2217\n",
    "Epoch #273: Loss:1.5861, Accuracy:0.2616, Validation Loss:1.6042, Validation Accuracy:0.2266\n",
    "Epoch #274: Loss:1.5853, Accuracy:0.2612, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #275: Loss:1.5860, Accuracy:0.2637, Validation Loss:1.6052, Validation Accuracy:0.2217\n",
    "Epoch #276: Loss:1.5848, Accuracy:0.2600, Validation Loss:1.6073, Validation Accuracy:0.2217\n",
    "Epoch #277: Loss:1.5861, Accuracy:0.2435, Validation Loss:1.6056, Validation Accuracy:0.2250\n",
    "Epoch #278: Loss:1.5834, Accuracy:0.2595, Validation Loss:1.6069, Validation Accuracy:0.2365\n",
    "Epoch #279: Loss:1.5859, Accuracy:0.2583, Validation Loss:1.6067, Validation Accuracy:0.2299\n",
    "Epoch #280: Loss:1.5865, Accuracy:0.2632, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #281: Loss:1.5961, Accuracy:0.2501, Validation Loss:1.6038, Validation Accuracy:0.2578\n",
    "Epoch #282: Loss:1.5919, Accuracy:0.2505, Validation Loss:1.6086, Validation Accuracy:0.2381\n",
    "Epoch #283: Loss:1.5910, Accuracy:0.2501, Validation Loss:1.6078, Validation Accuracy:0.2365\n",
    "Epoch #284: Loss:1.5924, Accuracy:0.2501, Validation Loss:1.6092, Validation Accuracy:0.2348\n",
    "Epoch #285: Loss:1.6042, Accuracy:0.2378, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #286: Loss:1.5985, Accuracy:0.2390, Validation Loss:1.6044, Validation Accuracy:0.2348\n",
    "Epoch #287: Loss:1.5979, Accuracy:0.2374, Validation Loss:1.6009, Validation Accuracy:0.2233\n",
    "Epoch #288: Loss:1.5954, Accuracy:0.2464, Validation Loss:1.5997, Validation Accuracy:0.2315\n",
    "Epoch #289: Loss:1.5948, Accuracy:0.2505, Validation Loss:1.5999, Validation Accuracy:0.2397\n",
    "Epoch #290: Loss:1.5935, Accuracy:0.2456, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #291: Loss:1.5927, Accuracy:0.2497, Validation Loss:1.6002, Validation Accuracy:0.2430\n",
    "Epoch #292: Loss:1.5918, Accuracy:0.2501, Validation Loss:1.6017, Validation Accuracy:0.2348\n",
    "Epoch #293: Loss:1.5920, Accuracy:0.2439, Validation Loss:1.6042, Validation Accuracy:0.2167\n",
    "Epoch #294: Loss:1.5901, Accuracy:0.2468, Validation Loss:1.6034, Validation Accuracy:0.2430\n",
    "Epoch #295: Loss:1.5897, Accuracy:0.2563, Validation Loss:1.6036, Validation Accuracy:0.2200\n",
    "Epoch #296: Loss:1.5888, Accuracy:0.2571, Validation Loss:1.6050, Validation Accuracy:0.2250\n",
    "Epoch #297: Loss:1.5911, Accuracy:0.2480, Validation Loss:1.6050, Validation Accuracy:0.2397\n",
    "Epoch #298: Loss:1.5909, Accuracy:0.2476, Validation Loss:1.6059, Validation Accuracy:0.2365\n",
    "Epoch #299: Loss:1.5884, Accuracy:0.2480, Validation Loss:1.6061, Validation Accuracy:0.2397\n",
    "Epoch #300: Loss:1.5879, Accuracy:0.2546, Validation Loss:1.6070, Validation Accuracy:0.2102\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60704684, Accuracy:0.2102\n",
    "Labels: ['01', '03', '02', '04', '05']\n",
    "Confusion Matrix:\n",
    "      01  03  02  04  05\n",
    "t:01  10  41   2   8  65\n",
    "t:03   6  27   4   9  69\n",
    "t:02   8  27   2  16  61\n",
    "t:04  11  25   3  12  61\n",
    "t:05   8  45   0  12  77\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.23      0.08      0.12       126\n",
    "          03       0.16      0.23      0.19       115\n",
    "          02       0.18      0.02      0.03       114\n",
    "          04       0.21      0.11      0.14       112\n",
    "          05       0.23      0.54      0.32       142\n",
    "\n",
    "    accuracy                           0.21       609\n",
    "   macro avg       0.20      0.20      0.16       609\n",
    "weighted avg       0.21      0.21      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 03:49:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 30 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.608517820807709, 1.607268964911525, 1.6057707725095827, 1.60564532244734, 1.6054406076033518, 1.6053979643459977, 1.60512276412231, 1.60505243573087, 1.6049077287683347, 1.6047891270742431, 1.6047466012644651, 1.6046255116392238, 1.6044974479769252, 1.6043388422682563, 1.6045582574380834, 1.6044165680952651, 1.6037453397349968, 1.6041581818622908, 1.6037506978891558, 1.604089203726482, 1.6056703415214526, 1.605761726893032, 1.6051682531344285, 1.6065171976590586, 1.6066630137182025, 1.6062515038379113, 1.6050901133047144, 1.6041515813085245, 1.6042976305011067, 1.6044239201177712, 1.6052514972357914, 1.6048541818737787, 1.6040512143293233, 1.6034323940136161, 1.6034590246642164, 1.6038084685900333, 1.6030816906582936, 1.6034992653356592, 1.6038313291734467, 1.6039465199941876, 1.6036240598446825, 1.6038356064184154, 1.6040364452966525, 1.603891709559461, 1.6030574422360249, 1.6051365935743735, 1.6032299991507444, 1.6044211978787075, 1.60408183115065, 1.6041672407895669, 1.6038604579339866, 1.6037327614911083, 1.6040017933681094, 1.6043210657946583, 1.6038245136906164, 1.60381288305292, 1.6043415100899432, 1.6036167728098352, 1.604230659935862, 1.6037266422766574, 1.6047548235735087, 1.6048515854993672, 1.604405973932426, 1.603693123129984, 1.6046927885468958, 1.6050047989940799, 1.6045379646501714, 1.604450143421029, 1.6042805722194353, 1.6045053371263451, 1.6045312208103624, 1.6041353903772013, 1.6042494971568166, 1.6046469031492085, 1.6044384354636783, 1.6043643123410605, 1.6045197422672766, 1.6051257210607794, 1.6053221337313723, 1.605398159308974, 1.605169034943792, 1.605195939051498, 1.6047961379115414, 1.6049617607213789, 1.604375060947462, 1.604173783598275, 1.6048277140838172, 1.6043728141753348, 1.6034944280614993, 1.6032757242324904, 1.6032429935505432, 1.6040850086948164, 1.604161857384179, 1.6038976469259152, 1.6032809334239742, 1.603281479946694, 1.6038164152887655, 1.6030548834448377, 1.6037182796177605, 1.60341294058438, 1.6044635414489972, 1.6043199326212967, 1.6038451621489376, 1.6037907392912114, 1.6041408172381924, 1.6047155502785995, 1.6050126166962246, 1.6052097702652754, 1.604888784865832, 1.60493693817621, 1.6052971788619343, 1.603749970888661, 1.604835205085955, 1.6046646653333516, 1.6036490908592989, 1.603901774033733, 1.604378518408351, 1.6037527524387503, 1.6030137468441366, 1.609686757347658, 1.6069965573954466, 1.6065587891733706, 1.6043523820162995, 1.6043413572123486, 1.604286652676186, 1.6037044899021267, 1.6035625018509738, 1.603926337411251, 1.604200698472009, 1.6045236783270374, 1.6043620516709702, 1.6048007956866561, 1.6052728873755544, 1.6055510325972082, 1.605487975580939, 1.6057409547232642, 1.6052648745146878, 1.604456126787784, 1.6046318142676392, 1.6045749156150129, 1.6045410644831917, 1.6047295130336618, 1.6050830292584273, 1.6055835589203733, 1.6060422149980793, 1.6052444335470841, 1.6050213847450043, 1.6045509721649496, 1.6044120025164976, 1.6040135324490676, 1.605045233845515, 1.6048765372368698, 1.6054201701591755, 1.6053584036960196, 1.6043913638454743, 1.6044337111349372, 1.6045800866360342, 1.6048652648142798, 1.6043794092482142, 1.605639782836676, 1.6055579991959195, 1.6041317630088192, 1.6035183515454747, 1.6037851251013369, 1.6040361802566228, 1.60499676401392, 1.6062708431472528, 1.606650334077907, 1.606039719824329, 1.6054477047646183, 1.605168036443651, 1.6053799191131968, 1.6053294105874298, 1.605766353544539, 1.605056458506091, 1.6040715278663071, 1.606049880801359, 1.607314710350851, 1.6062241209355872, 1.6047376531294022, 1.605366431433579, 1.6041966388965476, 1.6041613141891404, 1.6049573499776655, 1.6053789448855547, 1.6055073293754816, 1.6052826621458056, 1.6048058741198385, 1.6045909534729956, 1.6053980673083728, 1.6056291534395641, 1.6051170172166747, 1.6054270782298448, 1.606564042994933, 1.6063220925714778, 1.607196203789296, 1.6066678566689954, 1.6060462616543072, 1.6065240892870674, 1.6056291223159564, 1.606008465066919, 1.6051499673298426, 1.6055219095133013, 1.606318625127545, 1.6067829188846408, 1.6062201359393367, 1.606478877646974, 1.6071071548414935, 1.6074344802568308, 1.60831727515692, 1.6077578929061764, 1.6074611921420043, 1.6072178029857442, 1.6071105293060954, 1.607205415202675, 1.6072827947550807, 1.608183166663635, 1.608407335719843, 1.6083218370165144, 1.608739718819291, 1.6092691826702925, 1.608257629601239, 1.6094838737071246, 1.6097828527585234, 1.6100190014674747, 1.6081996946695012, 1.609519396509443, 1.6099783540359272, 1.6069490835192952, 1.6083420712763845, 1.6071996289521016, 1.606216608009902, 1.6065308838250798, 1.6089997369863325, 1.6064956472033547, 1.6057660624702967, 1.6065420102211838, 1.6065498985876199, 1.606800392147747, 1.606907304480354, 1.605578475006304, 1.6062487520412077, 1.6076832530142247, 1.6067118437223638, 1.6079797280833052, 1.6105083763501522, 1.610977819596214, 1.6104772617468497, 1.6117579226423366, 1.6094900567347585, 1.608924100747445, 1.608801022147506, 1.6084366717753544, 1.6097926774439946, 1.6094388378469031, 1.6099583946033846, 1.610170791889059, 1.6098584112862648, 1.6119674960967942, 1.6112542927558786, 1.6114836508417365, 1.61153287899318, 1.6155328913079499, 1.6137797482103746, 1.6066021018819072, 1.6031447500235145, 1.6034496126112288, 1.6035883775094069, 1.6045581118030892, 1.603642132677664, 1.60405602850546, 1.6051809161363173, 1.6042243982183522, 1.6044158970781148, 1.6052206228127817, 1.607265352028344, 1.6056181787465789, 1.6068849902239144, 1.6066891248590254, 1.606565573141101, 1.6037819223059417, 1.608593464289197, 1.6078115963975002, 1.6091992314813173, 1.604434978981519, 1.6044378736727736, 1.6008836440069139, 1.599678504447436, 1.59994128948362, 1.6002643828713052, 1.6002344585991846, 1.6017003854115803, 1.6042228614168215, 1.6034090796910678, 1.6036093651954764, 1.605010615585277, 1.6049836752645683, 1.6058897345719862, 1.60614273994427, 1.6070467940300752], 'val_acc': [0.23152709139391706, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.23152709149179004, 0.23316912781233076, 0.2298850554648683, 0.23645319996404726, 0.23481116374137953, 0.23316912771445777, 0.23481116364350654, 0.22824301914432757, 0.23645319996404726, 0.23481116374137953, 0.23152709139391706, 0.2331691276165848, 0.22331691076994334, 0.23316912771445777, 0.23316912781233076, 0.2348111639371255, 0.23481116364350654, 0.23481116383925252, 0.23152709168753602, 0.23316912771445777, 0.22988505556274125, 0.23152709149179004, 0.23481116374137953, 0.2298850553669953, 0.23152709158966303, 0.23316912781233076, 0.238095236186715, 0.23152709158966303, 0.2298850553669953, 0.23481116383925252, 0.23481116383925252, 0.23316912771445777, 0.22824301934007354, 0.22988505556274125, 0.23316912771445777, 0.2298850554648683, 0.22003283852035385, 0.23152709158966303, 0.23481116374137953, 0.23645319986617427, 0.23316912771445777, 0.23316912771445777, 0.23316912771445777, 0.23316912771445777, 0.23316912771445777, 0.23152709158966303, 0.23316912781233076, 0.2298850554648683, 0.2298850554648683, 0.22167487484089454, 0.2200328387160998, 0.22495894718835702, 0.22988505556274125, 0.22824301934007354, 0.238095236088842, 0.2266009831174058, 0.23645319996404726, 0.23481116383925252, 0.23316912771445777, 0.22495894709048403, 0.23316912771445777, 0.23316912771445777, 0.23645320006192025, 0.22988505556274125, 0.23152709168753602, 0.22824301934007354, 0.23481116383925252, 0.22824301934007354, 0.22988505556274125, 0.22824301934007354, 0.22824301934007354, 0.2298850554648683, 0.2298850554648683, 0.23645319996404726, 0.23316912771445777, 0.23645319996404726, 0.2413793083384315, 0.23316912771445777, 0.23481116383925252, 0.23316912781233076, 0.23152709168753602, 0.2298850554648683, 0.23481116383925252, 0.22988505556274125, 0.23481116403499847, 0.2266009832152788, 0.23316912771445777, 0.23316912771445777, 0.23481116383925252, 0.23316912781233076, 0.22988505556274125, 0.23152709158966303, 0.23152709168753602, 0.2282430194379465, 0.23152709158966303, 0.23152709158966303, 0.2282430194379465, 0.2282430194379465, 0.23481116383925252, 0.23316912781233076, 0.238095236186715, 0.23645320006192025, 0.23481116383925252, 0.238095236186715, 0.23645319996404726, 0.238095236088842, 0.238095236088842, 0.24302134436535328, 0.2331691276165848, 0.23481116374137953, 0.22824301924220056, 0.23316912771445777, 0.238095236088842, 0.238095236088842, 0.23316912771445777, 0.2298850554648683, 0.2298850553669953, 0.22824301934007354, 0.22824301934007354, 0.22824301934007354, 0.22824301934007354, 0.22824301934007354, 0.2298850554648683, 0.23152709158966303, 0.2298850554648683, 0.22824301934007354, 0.2266009832152788, 0.2266009832152788, 0.23152709158966303, 0.2266009832152788, 0.2298850554648683, 0.2266009832152788, 0.2298850554648683, 0.23152709158966303, 0.23152709158966303, 0.23152709158966303, 0.23152709158966303, 0.23316912771445777, 0.23316912771445777, 0.2298850554648683, 0.23316912771445777, 0.23316912771445777, 0.23152709158966303, 0.23481116364350654, 0.23316912781233076, 0.23645320006192025, 0.24137930853417747, 0.23481116383925252, 0.23481116383925252, 0.2413793083384315, 0.22003283842248086, 0.2183908044753599, 0.21674876844843816, 0.22003283842248086, 0.22660098519720664, 0.21674876617289138, 0.2331691275187118, 0.2183908045732329, 0.2167487682526922, 0.22331691067207035, 0.21839080229768612, 0.21182265779850715, 0.21674876844843816, 0.20361247925433423, 0.21674876844843816, 0.2134646939233019, 0.2019704431295395, 0.21510673004809663, 0.2266009832152788, 0.2101806238513862, 0.21018062394925918, 0.2151067323236434, 0.20853858772659145, 0.21346469600310272, 0.22824301904645458, 0.2167487682526922, 0.21182265997618094, 0.21510673222577043, 0.23152709357159088, 0.22167487672494943, 0.22988505734892314, 0.2249589489745389, 0.2266009832152788, 0.2282430213220014, 0.2298850553669953, 0.2266009831174058, 0.21674876617289138, 0.2216748768228224, 0.21510673222577043, 0.22331691086781633, 0.2216748768228224, 0.22003284069802764, 0.21346469610097568, 0.2216748768228224, 0.22824301914432757, 0.22660098301953283, 0.2216748746451486, 0.22331691284974417, 0.2282430213220014, 0.22988505517124935, 0.2216748746451486, 0.2216748746451486, 0.22495894689473808, 0.2216748746451486, 0.2249589489745389, 0.2200328405022817, 0.21674876844843816, 0.22003284069802764, 0.2216748768228224, 0.21510673004809663, 0.2101806237535132, 0.22331691294761713, 0.22660098292165984, 0.2101806238513862, 0.2134646939233019, 0.2331691275187118, 0.2101806238513862, 0.22003283852035385, 0.2266009831174058, 0.21510673004809663, 0.22003283861822684, 0.20525451537912898, 0.2019704430316665, 0.22003283861822684, 0.21839080259130506, 0.22331691086781633, 0.22331691086781633, 0.22003283861822684, 0.2101806238513862, 0.2167487664665103, 0.22003283861822684, 0.2183908023955591, 0.22003283852035385, 0.22331691067207035, 0.20853858762871846, 0.21674876627076436, 0.22003283852035385, 0.2134646939233019, 0.22003283842248086, 0.2315270912960441, 0.22331691067207035, 0.22495894689473808, 0.2216748746451486, 0.2216748746451486, 0.21839080229768612, 0.2216748746451486, 0.22331691076994334, 0.20361247915646125, 0.21839080259130506, 0.2068965516017967, 0.21510673014596962, 0.2101806238513862, 0.21674876617289138, 0.21182265779850715, 0.2266009831174058, 0.22331691086781633, 0.2183908023955591, 0.22824301904645458, 0.2266009831174058, 0.2266009831174058, 0.2216748746451486, 0.2266009831174058, 0.23152709149179004, 0.2216748746451486, 0.22167487474302158, 0.2249589467968651, 0.2364532019459751, 0.2298850554648683, 0.23316912742083884, 0.25779966958637895, 0.23809523589309606, 0.23645320006192025, 0.23481116364350654, 0.23152709357159088, 0.23481116364350654, 0.22331691275187118, 0.2315270912960441, 0.23973727211576376, 0.2430213442674803, 0.24302134436535328, 0.23481116364350654, 0.21674876636863732, 0.2430213442674803, 0.22003283852035385, 0.2249589467968651, 0.2397372720178908, 0.2364532001597932, 0.2397372719200178, 0.2101806216737124], 'loss': [1.611192646604299, 1.608277487852735, 1.606963351815633, 1.6062621481854322, 1.60556421627499, 1.6052316203499233, 1.6049121354884435, 1.6043420155190344, 1.6038846211266957, 1.6036265225870654, 1.6033972590366183, 1.603009219336069, 1.6026534213667287, 1.6020758190194195, 1.6018683472697985, 1.6012646767148246, 1.6009298893460502, 1.6010242853810899, 1.6007991938130812, 1.600894841619096, 1.6007747777433612, 1.6012284504559495, 1.600841792065505, 1.600267983462042, 1.60062652642722, 1.6008241171709565, 1.6012857774444673, 1.6016050363712977, 1.6035718382751183, 1.6020706377969385, 1.602540919815001, 1.6028143836242708, 1.6017672604848716, 1.6012932403866025, 1.602133330035748, 1.601749335569033, 1.6010924695698387, 1.600670085599535, 1.6009606927327307, 1.6006712568369246, 1.6005460239288989, 1.6001175508362067, 1.6000769802677068, 1.5998955724665271, 1.5997472420365413, 1.6005242536689712, 1.6006031153383196, 1.6006556874672735, 1.600786480179068, 1.6000352384128609, 1.5998929612690418, 1.5997668902732018, 1.5995087689198018, 1.599260256716358, 1.59949703255718, 1.5994344068503723, 1.5990554051722345, 1.5993232434779956, 1.5995239495251947, 1.5987768893858736, 1.5989477428812267, 1.598739709452682, 1.5984437990971905, 1.5986147460017117, 1.5984784509611816, 1.598948336479845, 1.5998288344553608, 1.5995188291557516, 1.6001371543265466, 1.5991142205389128, 1.5994414562562163, 1.5994696000273467, 1.599159839021107, 1.5985814307994177, 1.5983734881363856, 1.597957940121206, 1.59799413842587, 1.5978897924051148, 1.5975765232677577, 1.5981117250982986, 1.597668538348141, 1.5974111581974695, 1.5977537794524395, 1.5983289180595037, 1.59781669042928, 1.5979865932856252, 1.5976630476221167, 1.5977695566428025, 1.597260029996445, 1.5973749358061649, 1.59694716362493, 1.5967438287558742, 1.5967831019258598, 1.5969689744698683, 1.5965342065881654, 1.596686171848916, 1.5968530169257882, 1.5965640854786554, 1.5968221197872436, 1.5958330124555427, 1.5960622232797455, 1.5956429185808563, 1.5961068296334582, 1.5956313283046903, 1.5959755339906445, 1.5956380585135865, 1.5958052090795622, 1.5957459699936227, 1.5958440158401426, 1.5956053865518902, 1.5962557074960007, 1.5960280445811685, 1.5958029953606074, 1.5960164791259923, 1.5958892086937686, 1.5959459899142538, 1.5960058190249808, 1.5982808955641008, 1.598633793492092, 1.60592973824644, 1.6034937623589924, 1.6018774485441203, 1.6001202568626012, 1.597999001920101, 1.5974085574277372, 1.59696877247499, 1.5962669980599404, 1.5965063373655755, 1.5964270976045047, 1.5963659605695972, 1.5958207837365246, 1.5956808611108046, 1.595593759558284, 1.595438429070694, 1.5955124244552863, 1.5953241566123415, 1.5954856234654264, 1.59481850174663, 1.5947429994782873, 1.5947174444335688, 1.5946752702431022, 1.5945284933524944, 1.5949102696452053, 1.5948328157225184, 1.5948235931337738, 1.5949951148375838, 1.5948542173393454, 1.594488564751721, 1.59417323307335, 1.5943486779132663, 1.5946666499182918, 1.594657483189013, 1.5946323134815914, 1.5952666453022732, 1.595782876308449, 1.5956597381548716, 1.5952147598873663, 1.5947013529908731, 1.5941471911553728, 1.5945713276246245, 1.5943338331500607, 1.5948051893246002, 1.5949252090904502, 1.5946311843958234, 1.5940284713337798, 1.594051692030513, 1.5935213541348123, 1.593209126010323, 1.5924226369701127, 1.5923334279833878, 1.5917540678498192, 1.591939018786076, 1.5928319180525794, 1.591887379524889, 1.591808455093196, 1.5916442706599618, 1.592773065576808, 1.5918278562459613, 1.5907699352906715, 1.5916783468924018, 1.5904976870734588, 1.5907928632270139, 1.5907731701461196, 1.5914917169899911, 1.5954982530899362, 1.5961073308509968, 1.5924982927173559, 1.5920585139331387, 1.5921615898731554, 1.5916151150540894, 1.5919030447025808, 1.5921537950053597, 1.5939259148721088, 1.593461536773666, 1.592966723393121, 1.5946390290035115, 1.5940485527872794, 1.594481543202175, 1.5936522161201774, 1.5932711723159225, 1.594227595838433, 1.592142924146241, 1.5927423794411535, 1.5919742147046192, 1.5914361088182891, 1.5911287508461265, 1.5911708819547963, 1.59032648853208, 1.5903051314167909, 1.5900641116273477, 1.5895909172308764, 1.5899319087700188, 1.5895654521194083, 1.5897063228873503, 1.589969938589562, 1.5893976245817463, 1.5891925853380677, 1.5896842646647773, 1.588922607972147, 1.589514254054984, 1.5893489803866439, 1.5893376505595214, 1.5895663136084712, 1.5901214086544342, 1.5893895036630807, 1.5897372457770595, 1.5909270407972396, 1.589963483663555, 1.5892810139567946, 1.5899250109337684, 1.5876859541546393, 1.588886692685513, 1.5881529179441856, 1.5881457247528452, 1.5878986871218046, 1.5867889558510124, 1.5870324774689253, 1.5878054963979389, 1.5881470771785635, 1.5877495624691065, 1.5865122282529514, 1.5868132902611451, 1.5854790781557682, 1.58662475403819, 1.5858295085738572, 1.585792711089524, 1.5854375193496015, 1.587060173287284, 1.5855875905289543, 1.5868004903656256, 1.5861111311452345, 1.584483013946173, 1.5854302370572726, 1.5857775136920216, 1.5852545540435603, 1.586125033787878, 1.5858931166924979, 1.5855775769975886, 1.5857616059344408, 1.585175494932296, 1.5843830732839064, 1.5873176305935368, 1.591627627084877, 1.5881355449649097, 1.5901711861945276, 1.588969023712362, 1.5875902823109402, 1.5872652746568716, 1.5897766219517044, 1.5862085347302886, 1.5882070843933545, 1.5866251864717236, 1.58607267545234, 1.5852994831190952, 1.5860028402026918, 1.5848040346737025, 1.5861178765796293, 1.583414954424394, 1.5859160946135158, 1.5864916959582414, 1.5960871029683452, 1.5918688591011252, 1.5910325518378976, 1.592361099176583, 1.6042061124249405, 1.598471965731047, 1.5978648796218622, 1.5954332454003837, 1.5948008480992406, 1.593469989617992, 1.5926525517900376, 1.5918045127661076, 1.5920133438443256, 1.5900671063017797, 1.5896982386371683, 1.5887843009627576, 1.5911447638114131, 1.5908797656730949, 1.5884080365453168, 1.5879157904971553], 'acc': [0.2041067760521871, 0.2320328530834441, 0.2320328538851082, 0.2328542083195837, 0.23285421051039099, 0.23285421029620593, 0.23285420833794243, 0.23285421029620593, 0.23285421010037957, 0.23285421010037957, 0.23285421031456463, 0.23162217724617012, 0.23244353089734024, 0.24229979431849485, 0.24024640701145117, 0.24312114898551415, 0.2435318291309678, 0.24147844027567203, 0.24353182736853066, 0.2455852166338378, 0.24558521626054383, 0.2447638598310874, 0.24681724792143647, 0.24435318340633438, 0.245585214479748, 0.24312114896715545, 0.24229979431849485, 0.24065708482534734, 0.23531827422382895, 0.24147843947400793, 0.24271047291569642, 0.24065708523535875, 0.23901437337393633, 0.2410677626392435, 0.24147844002476954, 0.24435318377962836, 0.24476386043692516, 0.24394250402582746, 0.2451745382324626, 0.24476386120187182, 0.24640657108667205, 0.24353182893514144, 0.24353182817019475, 0.24517453864247402, 0.24517453803663625, 0.24558521528141208, 0.2439425055740795, 0.2443531818213649, 0.2435318287393151, 0.24640657265328283, 0.2468172492922209, 0.24558521626054383, 0.24517453883830037, 0.24681724811726283, 0.24599589248947049, 0.24599589231200286, 0.2472279251478536, 0.2476386031575761, 0.24804928116729863, 0.24640657110503078, 0.24804928097147227, 0.2468172485089155, 0.24722792730194343, 0.24517453746751594, 0.24640656928751747, 0.24435318319214933, 0.24312115053376623, 0.24558521487140067, 0.24476386002691375, 0.24558521506722703, 0.24969199066044614, 0.24763860296174975, 0.24558521487140067, 0.245585214479748, 0.24558521761296956, 0.24804928292973574, 0.24804928214643035, 0.24845995917702113, 0.24681724772561012, 0.24599589268529684, 0.24763860135842153, 0.24722792691029072, 0.24722792828107518, 0.24845995737786655, 0.24928131462734582, 0.24435318162553854, 0.2488706377742227, 0.2484599598012176, 0.2488706377742227, 0.24640657149668346, 0.24312115055212494, 0.2505133490298073, 0.25010266984512675, 0.2517453814923641, 0.2476386039408815, 0.2451745374491572, 0.2468172496838736, 0.2509240264520508, 0.25010267043260576, 0.24312114814713262, 0.24599589386025494, 0.25010267023677946, 0.2418891168962514, 0.24312115190455066, 0.24394250559243824, 0.24599589348696096, 0.2439425046133065, 0.24065708325873655, 0.24106776283506984, 0.2422997968642374, 0.2459958936827873, 0.25010266947183274, 0.24640657149668346, 0.24065708386457432, 0.2455852166338378, 0.24640656991171397, 0.2509240241021346, 0.238603696753357, 0.2422997949059739, 0.23162217859859585, 0.2312114984347835, 0.23778234151721736, 0.2459958920978178, 0.24517453803663625, 0.24722792635952912, 0.25010266984512675, 0.24928131304237633, 0.2517453807274174, 0.24435318397545472, 0.2422997952976266, 0.24312114916298178, 0.24353182697687795, 0.24394250459494776, 0.24312115055212494, 0.24271047193656467, 0.2418891173062628, 0.24229979353518946, 0.2439425055740795, 0.2435318283660211, 0.2418891168962514, 0.24229979627675835, 0.24722792632281168, 0.25215605754382314, 0.2443531811971684, 0.24394250559243824, 0.2435318283476624, 0.2439425053966119, 0.24353182776018334, 0.2443531818213649, 0.24394250480913285, 0.2410677622475908, 0.24394250598409092, 0.24763860474254562, 0.23983572960756644, 0.2451745382324626, 0.24517453705750453, 0.24106776126845905, 0.25256673653267736, 0.2546201248188528, 0.24928131323820266, 0.24969199263706834, 0.2525667349660666, 0.2472279267144644, 0.2505133476590229, 0.2468172479397952, 0.25256673653267736, 0.2587268980254383, 0.2533880902022062, 0.2525667373159828, 0.2533880896147272, 0.2542094444592141, 0.2566735133008546, 0.25544147731342354, 0.2562628347220117, 0.25708418954813994, 0.25872689825798206, 0.2562628334919775, 0.2603696075185858, 0.2599589315038442, 0.25749486438792346, 0.2566735095434365, 0.25544147809672896, 0.25133470172020445, 0.2558521566939305, 0.2517453795341006, 0.25626283313704223, 0.26201232135663044, 0.25338808801139895, 0.25790554302184243, 0.25585215411147055, 0.2558521582605413, 0.2525667341827612, 0.25133470269933617, 0.2550308018678023, 0.2583162225981757, 0.2546201240539061, 0.25503080128032324, 0.253388091181338, 0.2513347044617733, 0.2529774129757891, 0.2505133476590229, 0.2574948651712288, 0.25462012423137376, 0.25462012344806834, 0.25667351212589645, 0.25544147811508766, 0.25544147727670613, 0.25708418954813994, 0.25790554537175864, 0.25667351212589645, 0.25708418915648723, 0.25831622142321764, 0.2595482548649061, 0.25995893365793404, 0.2611909641622273, 0.2628336744386802, 0.2583162220106967, 0.2554414784883816, 0.2595482550607325, 0.25667351116512344, 0.257905545567585, 0.2587269004120719, 0.2595482550607325, 0.25749486638290436, 0.2562628333328686, 0.25872690023460426, 0.25831622124575004, 0.2517453787507952, 0.25872690139120363, 0.25913757724683634, 0.2624229979955685, 0.2595482552198414, 0.2574948677536888, 0.2616016417802971, 0.25954825368994805, 0.2648870637039874, 0.2620123215524568, 0.26324435284005543, 0.2648870648789455, 0.25995893130801784, 0.26201232037749866, 0.2632443512367272, 0.2620123221399358, 0.26365503065395157, 0.2632443534275345, 0.2595482542774271, 0.26570841796099526, 0.264476384286763, 0.26283367659277007, 0.2640657062770405, 0.26406570608121416, 0.2669404539484263, 0.2673511315664961, 0.26611909636237047, 0.26488706409564006, 0.2640657098386322, 0.2636550330038678, 0.26324435421083986, 0.2648870642914664, 0.26283367621947606, 0.2669404539484263, 0.2533880902022062, 0.2652977421053626, 0.2579055449801059, 0.24928131482317217, 0.25954825564821155, 0.2632443540150135, 0.2640657102302849, 0.256673510755112, 0.26242299600058755, 0.26242299819139486, 0.25995893365793404, 0.2616016417802971, 0.26119096594302316, 0.26365502865897067, 0.2599589328746286, 0.24353182658522526, 0.25954825662734327, 0.25831622040736846, 0.26324435460249257, 0.25010267023677946, 0.2505133472673702, 0.25010267023677946, 0.25010266886599497, 0.23778234051972688, 0.23901437317810997, 0.23737166290165707, 0.24640657304493555, 0.25051334728572894, 0.24558521467557434, 0.24969199146211024, 0.2501026692576477, 0.24394250598409092, 0.24681724831308918, 0.2562628323170194, 0.257084189762325, 0.2480492805981783, 0.24763860335340246, 0.24804928214643035, 0.2546201230564157]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
