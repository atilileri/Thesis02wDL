{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf8.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 01:07:19 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '2', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '02', '03', '04', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000020397A1BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000020395186EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6086, Accuracy:0.2337, Validation Loss:1.6069, Validation Accuracy:0.2348\n",
    "Epoch #2: Loss:1.6059, Accuracy:0.2324, Validation Loss:1.6068, Validation Accuracy:0.2233\n",
    "Epoch #3: Loss:1.6055, Accuracy:0.2320, Validation Loss:1.6061, Validation Accuracy:0.2250\n",
    "Epoch #4: Loss:1.6046, Accuracy:0.2333, Validation Loss:1.6066, Validation Accuracy:0.2233\n",
    "Epoch #5: Loss:1.6042, Accuracy:0.2300, Validation Loss:1.6060, Validation Accuracy:0.2315\n",
    "Epoch #6: Loss:1.6041, Accuracy:0.2320, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6044, Accuracy:0.2324, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6031, Accuracy:0.2333, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6021, Accuracy:0.2324, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6015, Accuracy:0.2320, Validation Loss:1.6051, Validation Accuracy:0.2381\n",
    "Epoch #19: Loss:1.6014, Accuracy:0.2390, Validation Loss:1.6053, Validation Accuracy:0.2397\n",
    "Epoch #20: Loss:1.6011, Accuracy:0.2394, Validation Loss:1.6054, Validation Accuracy:0.2397\n",
    "Epoch #21: Loss:1.6007, Accuracy:0.2390, Validation Loss:1.6051, Validation Accuracy:0.2365\n",
    "Epoch #22: Loss:1.6001, Accuracy:0.2423, Validation Loss:1.6050, Validation Accuracy:0.2381\n",
    "Epoch #23: Loss:1.6011, Accuracy:0.2423, Validation Loss:1.6039, Validation Accuracy:0.2365\n",
    "Epoch #24: Loss:1.6003, Accuracy:0.2439, Validation Loss:1.6039, Validation Accuracy:0.2315\n",
    "Epoch #25: Loss:1.5998, Accuracy:0.2472, Validation Loss:1.6035, Validation Accuracy:0.2348\n",
    "Epoch #26: Loss:1.6032, Accuracy:0.2324, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6018, Accuracy:0.2370, Validation Loss:1.6072, Validation Accuracy:0.2365\n",
    "Epoch #28: Loss:1.6015, Accuracy:0.2444, Validation Loss:1.6054, Validation Accuracy:0.2365\n",
    "Epoch #29: Loss:1.6007, Accuracy:0.2378, Validation Loss:1.6059, Validation Accuracy:0.2299\n",
    "Epoch #30: Loss:1.6002, Accuracy:0.2398, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #31: Loss:1.6003, Accuracy:0.2476, Validation Loss:1.6052, Validation Accuracy:0.2299\n",
    "Epoch #32: Loss:1.5997, Accuracy:0.2448, Validation Loss:1.6054, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.5994, Accuracy:0.2423, Validation Loss:1.6057, Validation Accuracy:0.2365\n",
    "Epoch #34: Loss:1.5997, Accuracy:0.2452, Validation Loss:1.6052, Validation Accuracy:0.2381\n",
    "Epoch #35: Loss:1.5996, Accuracy:0.2452, Validation Loss:1.6049, Validation Accuracy:0.2365\n",
    "Epoch #36: Loss:1.5991, Accuracy:0.2464, Validation Loss:1.6053, Validation Accuracy:0.2315\n",
    "Epoch #37: Loss:1.5987, Accuracy:0.2460, Validation Loss:1.6054, Validation Accuracy:0.2315\n",
    "Epoch #38: Loss:1.5992, Accuracy:0.2476, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #39: Loss:1.5993, Accuracy:0.2431, Validation Loss:1.6059, Validation Accuracy:0.2381\n",
    "Epoch #40: Loss:1.5993, Accuracy:0.2468, Validation Loss:1.6064, Validation Accuracy:0.2365\n",
    "Epoch #41: Loss:1.5993, Accuracy:0.2456, Validation Loss:1.6060, Validation Accuracy:0.2381\n",
    "Epoch #42: Loss:1.5993, Accuracy:0.2448, Validation Loss:1.6062, Validation Accuracy:0.2381\n",
    "Epoch #43: Loss:1.5989, Accuracy:0.2456, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #44: Loss:1.5982, Accuracy:0.2456, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #45: Loss:1.5978, Accuracy:0.2456, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #46: Loss:1.5979, Accuracy:0.2464, Validation Loss:1.6058, Validation Accuracy:0.2397\n",
    "Epoch #47: Loss:1.5978, Accuracy:0.2476, Validation Loss:1.6061, Validation Accuracy:0.2381\n",
    "Epoch #48: Loss:1.5974, Accuracy:0.2485, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #49: Loss:1.5971, Accuracy:0.2476, Validation Loss:1.6049, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.5973, Accuracy:0.2509, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #51: Loss:1.5974, Accuracy:0.2480, Validation Loss:1.6071, Validation Accuracy:0.2348\n",
    "Epoch #52: Loss:1.5978, Accuracy:0.2485, Validation Loss:1.6069, Validation Accuracy:0.2365\n",
    "Epoch #53: Loss:1.5970, Accuracy:0.2493, Validation Loss:1.6070, Validation Accuracy:0.2315\n",
    "Epoch #54: Loss:1.5972, Accuracy:0.2460, Validation Loss:1.6060, Validation Accuracy:0.2299\n",
    "Epoch #55: Loss:1.5971, Accuracy:0.2538, Validation Loss:1.6059, Validation Accuracy:0.2282\n",
    "Epoch #56: Loss:1.5965, Accuracy:0.2526, Validation Loss:1.6068, Validation Accuracy:0.2348\n",
    "Epoch #57: Loss:1.5960, Accuracy:0.2522, Validation Loss:1.6077, Validation Accuracy:0.2233\n",
    "Epoch #58: Loss:1.5945, Accuracy:0.2513, Validation Loss:1.6085, Validation Accuracy:0.2315\n",
    "Epoch #59: Loss:1.5946, Accuracy:0.2534, Validation Loss:1.6090, Validation Accuracy:0.2315\n",
    "Epoch #60: Loss:1.5954, Accuracy:0.2550, Validation Loss:1.6102, Validation Accuracy:0.2167\n",
    "Epoch #61: Loss:1.5947, Accuracy:0.2517, Validation Loss:1.6086, Validation Accuracy:0.2200\n",
    "Epoch #62: Loss:1.5955, Accuracy:0.2394, Validation Loss:1.6098, Validation Accuracy:0.2135\n",
    "Epoch #63: Loss:1.5937, Accuracy:0.2534, Validation Loss:1.6100, Validation Accuracy:0.2167\n",
    "Epoch #64: Loss:1.5956, Accuracy:0.2538, Validation Loss:1.6111, Validation Accuracy:0.2266\n",
    "Epoch #65: Loss:1.5966, Accuracy:0.2476, Validation Loss:1.6106, Validation Accuracy:0.2085\n",
    "Epoch #66: Loss:1.5969, Accuracy:0.2415, Validation Loss:1.6098, Validation Accuracy:0.2036\n",
    "Epoch #67: Loss:1.5966, Accuracy:0.2407, Validation Loss:1.6075, Validation Accuracy:0.2365\n",
    "Epoch #68: Loss:1.5952, Accuracy:0.2468, Validation Loss:1.6072, Validation Accuracy:0.2184\n",
    "Epoch #69: Loss:1.5958, Accuracy:0.2497, Validation Loss:1.6056, Validation Accuracy:0.2184\n",
    "Epoch #70: Loss:1.5946, Accuracy:0.2485, Validation Loss:1.6080, Validation Accuracy:0.2299\n",
    "Epoch #71: Loss:1.5953, Accuracy:0.2489, Validation Loss:1.6086, Validation Accuracy:0.2250\n",
    "Epoch #72: Loss:1.5936, Accuracy:0.2513, Validation Loss:1.6071, Validation Accuracy:0.2233\n",
    "Epoch #73: Loss:1.5945, Accuracy:0.2464, Validation Loss:1.6092, Validation Accuracy:0.2135\n",
    "Epoch #74: Loss:1.5928, Accuracy:0.2485, Validation Loss:1.6096, Validation Accuracy:0.2151\n",
    "Epoch #75: Loss:1.5924, Accuracy:0.2501, Validation Loss:1.6112, Validation Accuracy:0.2184\n",
    "Epoch #76: Loss:1.5920, Accuracy:0.2480, Validation Loss:1.6127, Validation Accuracy:0.2102\n",
    "Epoch #77: Loss:1.5921, Accuracy:0.2493, Validation Loss:1.6113, Validation Accuracy:0.2151\n",
    "Epoch #78: Loss:1.5924, Accuracy:0.2505, Validation Loss:1.6098, Validation Accuracy:0.2184\n",
    "Epoch #79: Loss:1.5917, Accuracy:0.2456, Validation Loss:1.6104, Validation Accuracy:0.2167\n",
    "Epoch #80: Loss:1.5910, Accuracy:0.2489, Validation Loss:1.6113, Validation Accuracy:0.2184\n",
    "Epoch #81: Loss:1.5909, Accuracy:0.2493, Validation Loss:1.6102, Validation Accuracy:0.2151\n",
    "Epoch #82: Loss:1.5896, Accuracy:0.2489, Validation Loss:1.6118, Validation Accuracy:0.2151\n",
    "Epoch #83: Loss:1.5901, Accuracy:0.2546, Validation Loss:1.6121, Validation Accuracy:0.2085\n",
    "Epoch #84: Loss:1.5913, Accuracy:0.2472, Validation Loss:1.6119, Validation Accuracy:0.2135\n",
    "Epoch #85: Loss:1.5892, Accuracy:0.2526, Validation Loss:1.6205, Validation Accuracy:0.2053\n",
    "Epoch #86: Loss:1.6098, Accuracy:0.2382, Validation Loss:1.6204, Validation Accuracy:0.2299\n",
    "Epoch #87: Loss:1.6124, Accuracy:0.2345, Validation Loss:1.6164, Validation Accuracy:0.2184\n",
    "Epoch #88: Loss:1.6087, Accuracy:0.2300, Validation Loss:1.6084, Validation Accuracy:0.2348\n",
    "Epoch #89: Loss:1.6000, Accuracy:0.2464, Validation Loss:1.6033, Validation Accuracy:0.2299\n",
    "Epoch #90: Loss:1.5964, Accuracy:0.2513, Validation Loss:1.6043, Validation Accuracy:0.2233\n",
    "Epoch #91: Loss:1.5960, Accuracy:0.2353, Validation Loss:1.6058, Validation Accuracy:0.2020\n",
    "Epoch #92: Loss:1.5967, Accuracy:0.2452, Validation Loss:1.6063, Validation Accuracy:0.2200\n",
    "Epoch #93: Loss:1.5957, Accuracy:0.2476, Validation Loss:1.6063, Validation Accuracy:0.2250\n",
    "Epoch #94: Loss:1.5939, Accuracy:0.2501, Validation Loss:1.6055, Validation Accuracy:0.2233\n",
    "Epoch #95: Loss:1.5928, Accuracy:0.2497, Validation Loss:1.6049, Validation Accuracy:0.2184\n",
    "Epoch #96: Loss:1.5928, Accuracy:0.2575, Validation Loss:1.6058, Validation Accuracy:0.2184\n",
    "Epoch #97: Loss:1.5911, Accuracy:0.2472, Validation Loss:1.6071, Validation Accuracy:0.2200\n",
    "Epoch #98: Loss:1.5912, Accuracy:0.2513, Validation Loss:1.6087, Validation Accuracy:0.2250\n",
    "Epoch #99: Loss:1.5907, Accuracy:0.2407, Validation Loss:1.6091, Validation Accuracy:0.2184\n",
    "Epoch #100: Loss:1.5893, Accuracy:0.2485, Validation Loss:1.6114, Validation Accuracy:0.2217\n",
    "Epoch #101: Loss:1.5898, Accuracy:0.2522, Validation Loss:1.6120, Validation Accuracy:0.2250\n",
    "Epoch #102: Loss:1.5885, Accuracy:0.2554, Validation Loss:1.6130, Validation Accuracy:0.2036\n",
    "Epoch #103: Loss:1.5893, Accuracy:0.2538, Validation Loss:1.6143, Validation Accuracy:0.1987\n",
    "Epoch #104: Loss:1.5900, Accuracy:0.2554, Validation Loss:1.6134, Validation Accuracy:0.2250\n",
    "Epoch #105: Loss:1.5895, Accuracy:0.2489, Validation Loss:1.6132, Validation Accuracy:0.2069\n",
    "Epoch #106: Loss:1.5897, Accuracy:0.2493, Validation Loss:1.6134, Validation Accuracy:0.2233\n",
    "Epoch #107: Loss:1.5904, Accuracy:0.2538, Validation Loss:1.6136, Validation Accuracy:0.2250\n",
    "Epoch #108: Loss:1.5896, Accuracy:0.2542, Validation Loss:1.6147, Validation Accuracy:0.2069\n",
    "Epoch #109: Loss:1.5904, Accuracy:0.2501, Validation Loss:1.6133, Validation Accuracy:0.2069\n",
    "Epoch #110: Loss:1.5907, Accuracy:0.2419, Validation Loss:1.6128, Validation Accuracy:0.2299\n",
    "Epoch #111: Loss:1.5897, Accuracy:0.2534, Validation Loss:1.6137, Validation Accuracy:0.2299\n",
    "Epoch #112: Loss:1.5889, Accuracy:0.2517, Validation Loss:1.6133, Validation Accuracy:0.2036\n",
    "Epoch #113: Loss:1.5876, Accuracy:0.2571, Validation Loss:1.6149, Validation Accuracy:0.2102\n",
    "Epoch #114: Loss:1.5866, Accuracy:0.2637, Validation Loss:1.6147, Validation Accuracy:0.2250\n",
    "Epoch #115: Loss:1.5863, Accuracy:0.2591, Validation Loss:1.6179, Validation Accuracy:0.2118\n",
    "Epoch #116: Loss:1.5847, Accuracy:0.2583, Validation Loss:1.6141, Validation Accuracy:0.1954\n",
    "Epoch #117: Loss:1.5850, Accuracy:0.2641, Validation Loss:1.6117, Validation Accuracy:0.2299\n",
    "Epoch #118: Loss:1.5855, Accuracy:0.2624, Validation Loss:1.6121, Validation Accuracy:0.2315\n",
    "Epoch #119: Loss:1.5863, Accuracy:0.2595, Validation Loss:1.6140, Validation Accuracy:0.2315\n",
    "Epoch #120: Loss:1.5866, Accuracy:0.2612, Validation Loss:1.6149, Validation Accuracy:0.2184\n",
    "Epoch #121: Loss:1.5819, Accuracy:0.2612, Validation Loss:1.6191, Validation Accuracy:0.2135\n",
    "Epoch #122: Loss:1.5831, Accuracy:0.2559, Validation Loss:1.6186, Validation Accuracy:0.2151\n",
    "Epoch #123: Loss:1.5819, Accuracy:0.2653, Validation Loss:1.6201, Validation Accuracy:0.1987\n",
    "Epoch #124: Loss:1.5833, Accuracy:0.2591, Validation Loss:1.6211, Validation Accuracy:0.2184\n",
    "Epoch #125: Loss:1.5828, Accuracy:0.2637, Validation Loss:1.6228, Validation Accuracy:0.2118\n",
    "Epoch #126: Loss:1.5820, Accuracy:0.2616, Validation Loss:1.6259, Validation Accuracy:0.2118\n",
    "Epoch #127: Loss:1.5833, Accuracy:0.2583, Validation Loss:1.6272, Validation Accuracy:0.2020\n",
    "Epoch #128: Loss:1.5800, Accuracy:0.2571, Validation Loss:1.6260, Validation Accuracy:0.2053\n",
    "Epoch #129: Loss:1.5802, Accuracy:0.2628, Validation Loss:1.6229, Validation Accuracy:0.2069\n",
    "Epoch #130: Loss:1.5802, Accuracy:0.2649, Validation Loss:1.6260, Validation Accuracy:0.2053\n",
    "Epoch #131: Loss:1.5806, Accuracy:0.2682, Validation Loss:1.6218, Validation Accuracy:0.2463\n",
    "Epoch #132: Loss:1.5788, Accuracy:0.2657, Validation Loss:1.6254, Validation Accuracy:0.2102\n",
    "Epoch #133: Loss:1.5789, Accuracy:0.2624, Validation Loss:1.6217, Validation Accuracy:0.2299\n",
    "Epoch #134: Loss:1.5761, Accuracy:0.2735, Validation Loss:1.6228, Validation Accuracy:0.2348\n",
    "Epoch #135: Loss:1.5743, Accuracy:0.2698, Validation Loss:1.6263, Validation Accuracy:0.2053\n",
    "Epoch #136: Loss:1.5745, Accuracy:0.2768, Validation Loss:1.6286, Validation Accuracy:0.2299\n",
    "Epoch #137: Loss:1.5756, Accuracy:0.2706, Validation Loss:1.6365, Validation Accuracy:0.2003\n",
    "Epoch #138: Loss:1.5764, Accuracy:0.2637, Validation Loss:1.6271, Validation Accuracy:0.2397\n",
    "Epoch #139: Loss:1.5764, Accuracy:0.2735, Validation Loss:1.6297, Validation Accuracy:0.2299\n",
    "Epoch #140: Loss:1.5767, Accuracy:0.2616, Validation Loss:1.6288, Validation Accuracy:0.2315\n",
    "Epoch #141: Loss:1.5762, Accuracy:0.2661, Validation Loss:1.6283, Validation Accuracy:0.2184\n",
    "Epoch #142: Loss:1.5766, Accuracy:0.2665, Validation Loss:1.6287, Validation Accuracy:0.2217\n",
    "Epoch #143: Loss:1.5782, Accuracy:0.2649, Validation Loss:1.6260, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5771, Accuracy:0.2554, Validation Loss:1.6286, Validation Accuracy:0.2348\n",
    "Epoch #145: Loss:1.5794, Accuracy:0.2698, Validation Loss:1.6259, Validation Accuracy:0.2414\n",
    "Epoch #146: Loss:1.5921, Accuracy:0.2522, Validation Loss:1.6257, Validation Accuracy:0.2381\n",
    "Epoch #147: Loss:1.5932, Accuracy:0.2542, Validation Loss:1.6176, Validation Accuracy:0.2282\n",
    "Epoch #148: Loss:1.5873, Accuracy:0.2534, Validation Loss:1.6216, Validation Accuracy:0.2151\n",
    "Epoch #149: Loss:1.5832, Accuracy:0.2600, Validation Loss:1.6239, Validation Accuracy:0.2200\n",
    "Epoch #150: Loss:1.5832, Accuracy:0.2546, Validation Loss:1.6256, Validation Accuracy:0.2282\n",
    "Epoch #151: Loss:1.5814, Accuracy:0.2669, Validation Loss:1.6294, Validation Accuracy:0.2135\n",
    "Epoch #152: Loss:1.5818, Accuracy:0.2595, Validation Loss:1.6306, Validation Accuracy:0.2266\n",
    "Epoch #153: Loss:1.5785, Accuracy:0.2632, Validation Loss:1.6260, Validation Accuracy:0.2282\n",
    "Epoch #154: Loss:1.5767, Accuracy:0.2661, Validation Loss:1.6214, Validation Accuracy:0.2200\n",
    "Epoch #155: Loss:1.5738, Accuracy:0.2616, Validation Loss:1.6266, Validation Accuracy:0.2332\n",
    "Epoch #156: Loss:1.5742, Accuracy:0.2678, Validation Loss:1.6265, Validation Accuracy:0.2250\n",
    "Epoch #157: Loss:1.5724, Accuracy:0.2743, Validation Loss:1.6246, Validation Accuracy:0.2463\n",
    "Epoch #158: Loss:1.5743, Accuracy:0.2674, Validation Loss:1.6223, Validation Accuracy:0.2381\n",
    "Epoch #159: Loss:1.5810, Accuracy:0.2682, Validation Loss:1.6188, Validation Accuracy:0.2381\n",
    "Epoch #160: Loss:1.5858, Accuracy:0.2526, Validation Loss:1.6285, Validation Accuracy:0.2184\n",
    "Epoch #161: Loss:1.5754, Accuracy:0.2682, Validation Loss:1.6310, Validation Accuracy:0.2529\n",
    "Epoch #162: Loss:1.5817, Accuracy:0.2608, Validation Loss:1.6276, Validation Accuracy:0.2282\n",
    "Epoch #163: Loss:1.5762, Accuracy:0.2678, Validation Loss:1.6333, Validation Accuracy:0.2348\n",
    "Epoch #164: Loss:1.5769, Accuracy:0.2591, Validation Loss:1.6262, Validation Accuracy:0.2332\n",
    "Epoch #165: Loss:1.5738, Accuracy:0.2760, Validation Loss:1.6261, Validation Accuracy:0.2250\n",
    "Epoch #166: Loss:1.5749, Accuracy:0.2682, Validation Loss:1.6295, Validation Accuracy:0.2250\n",
    "Epoch #167: Loss:1.5738, Accuracy:0.2731, Validation Loss:1.6268, Validation Accuracy:0.2397\n",
    "Epoch #168: Loss:1.5744, Accuracy:0.2756, Validation Loss:1.6281, Validation Accuracy:0.2266\n",
    "Epoch #169: Loss:1.5762, Accuracy:0.2698, Validation Loss:1.6327, Validation Accuracy:0.2200\n",
    "Epoch #170: Loss:1.5756, Accuracy:0.2657, Validation Loss:1.6297, Validation Accuracy:0.2332\n",
    "Epoch #171: Loss:1.5757, Accuracy:0.2645, Validation Loss:1.6304, Validation Accuracy:0.2266\n",
    "Epoch #172: Loss:1.5751, Accuracy:0.2641, Validation Loss:1.6307, Validation Accuracy:0.2250\n",
    "Epoch #173: Loss:1.5742, Accuracy:0.2661, Validation Loss:1.6312, Validation Accuracy:0.2315\n",
    "Epoch #174: Loss:1.5729, Accuracy:0.2682, Validation Loss:1.6318, Validation Accuracy:0.2250\n",
    "Epoch #175: Loss:1.5735, Accuracy:0.2657, Validation Loss:1.6298, Validation Accuracy:0.2282\n",
    "Epoch #176: Loss:1.5711, Accuracy:0.2591, Validation Loss:1.6325, Validation Accuracy:0.2430\n",
    "Epoch #177: Loss:1.5686, Accuracy:0.2723, Validation Loss:1.6336, Validation Accuracy:0.2348\n",
    "Epoch #178: Loss:1.5700, Accuracy:0.2821, Validation Loss:1.6341, Validation Accuracy:0.2365\n",
    "Epoch #179: Loss:1.5678, Accuracy:0.2801, Validation Loss:1.6379, Validation Accuracy:0.2135\n",
    "Epoch #180: Loss:1.5679, Accuracy:0.2817, Validation Loss:1.6327, Validation Accuracy:0.2332\n",
    "Epoch #181: Loss:1.5695, Accuracy:0.2690, Validation Loss:1.6429, Validation Accuracy:0.2282\n",
    "Epoch #182: Loss:1.5695, Accuracy:0.2706, Validation Loss:1.6410, Validation Accuracy:0.2217\n",
    "Epoch #183: Loss:1.5695, Accuracy:0.2735, Validation Loss:1.6340, Validation Accuracy:0.2447\n",
    "Epoch #184: Loss:1.5699, Accuracy:0.2719, Validation Loss:1.6254, Validation Accuracy:0.2315\n",
    "Epoch #185: Loss:1.5701, Accuracy:0.2776, Validation Loss:1.6363, Validation Accuracy:0.2069\n",
    "Epoch #186: Loss:1.5752, Accuracy:0.2645, Validation Loss:1.6235, Validation Accuracy:0.2463\n",
    "Epoch #187: Loss:1.5728, Accuracy:0.2682, Validation Loss:1.6330, Validation Accuracy:0.1938\n",
    "Epoch #188: Loss:1.5731, Accuracy:0.2600, Validation Loss:1.6339, Validation Accuracy:0.2217\n",
    "Epoch #189: Loss:1.5676, Accuracy:0.2768, Validation Loss:1.6325, Validation Accuracy:0.2282\n",
    "Epoch #190: Loss:1.5672, Accuracy:0.2842, Validation Loss:1.6284, Validation Accuracy:0.2348\n",
    "Epoch #191: Loss:1.5654, Accuracy:0.2674, Validation Loss:1.6333, Validation Accuracy:0.2512\n",
    "Epoch #192: Loss:1.5639, Accuracy:0.2768, Validation Loss:1.6280, Validation Accuracy:0.2365\n",
    "Epoch #193: Loss:1.5626, Accuracy:0.2825, Validation Loss:1.6332, Validation Accuracy:0.2365\n",
    "Epoch #194: Loss:1.5615, Accuracy:0.2813, Validation Loss:1.6457, Validation Accuracy:0.2184\n",
    "Epoch #195: Loss:1.5786, Accuracy:0.2678, Validation Loss:1.6352, Validation Accuracy:0.2250\n",
    "Epoch #196: Loss:1.5852, Accuracy:0.2559, Validation Loss:1.6490, Validation Accuracy:0.2233\n",
    "Epoch #197: Loss:1.5798, Accuracy:0.2567, Validation Loss:1.6367, Validation Accuracy:0.2167\n",
    "Epoch #198: Loss:1.5771, Accuracy:0.2645, Validation Loss:1.6304, Validation Accuracy:0.2365\n",
    "Epoch #199: Loss:1.5693, Accuracy:0.2727, Validation Loss:1.6364, Validation Accuracy:0.2167\n",
    "Epoch #200: Loss:1.5674, Accuracy:0.2727, Validation Loss:1.6414, Validation Accuracy:0.2135\n",
    "Epoch #201: Loss:1.5660, Accuracy:0.2710, Validation Loss:1.6403, Validation Accuracy:0.2447\n",
    "Epoch #202: Loss:1.5642, Accuracy:0.2838, Validation Loss:1.6392, Validation Accuracy:0.2430\n",
    "Epoch #203: Loss:1.5648, Accuracy:0.2735, Validation Loss:1.6357, Validation Accuracy:0.2217\n",
    "Epoch #204: Loss:1.5633, Accuracy:0.2698, Validation Loss:1.6373, Validation Accuracy:0.2381\n",
    "Epoch #205: Loss:1.5619, Accuracy:0.2793, Validation Loss:1.6413, Validation Accuracy:0.2184\n",
    "Epoch #206: Loss:1.5648, Accuracy:0.2735, Validation Loss:1.6386, Validation Accuracy:0.2348\n",
    "Epoch #207: Loss:1.5732, Accuracy:0.2760, Validation Loss:1.6399, Validation Accuracy:0.2184\n",
    "Epoch #208: Loss:1.5692, Accuracy:0.2793, Validation Loss:1.6408, Validation Accuracy:0.2250\n",
    "Epoch #209: Loss:1.5758, Accuracy:0.2710, Validation Loss:1.6437, Validation Accuracy:0.1905\n",
    "Epoch #210: Loss:1.5716, Accuracy:0.2723, Validation Loss:1.6408, Validation Accuracy:0.2299\n",
    "Epoch #211: Loss:1.5789, Accuracy:0.2653, Validation Loss:1.6422, Validation Accuracy:0.2184\n",
    "Epoch #212: Loss:1.5805, Accuracy:0.2571, Validation Loss:1.6440, Validation Accuracy:0.1970\n",
    "Epoch #213: Loss:1.5759, Accuracy:0.2682, Validation Loss:1.6416, Validation Accuracy:0.2167\n",
    "Epoch #214: Loss:1.5750, Accuracy:0.2698, Validation Loss:1.6403, Validation Accuracy:0.2167\n",
    "Epoch #215: Loss:1.5725, Accuracy:0.2698, Validation Loss:1.6420, Validation Accuracy:0.1970\n",
    "Epoch #216: Loss:1.5683, Accuracy:0.2797, Validation Loss:1.6436, Validation Accuracy:0.2217\n",
    "Epoch #217: Loss:1.5683, Accuracy:0.2862, Validation Loss:1.6427, Validation Accuracy:0.2414\n",
    "Epoch #218: Loss:1.5652, Accuracy:0.2805, Validation Loss:1.6461, Validation Accuracy:0.2118\n",
    "Epoch #219: Loss:1.5640, Accuracy:0.2805, Validation Loss:1.6443, Validation Accuracy:0.2430\n",
    "Epoch #220: Loss:1.5628, Accuracy:0.2756, Validation Loss:1.6450, Validation Accuracy:0.2085\n",
    "Epoch #221: Loss:1.5594, Accuracy:0.2862, Validation Loss:1.6422, Validation Accuracy:0.2463\n",
    "Epoch #222: Loss:1.5623, Accuracy:0.2784, Validation Loss:1.6453, Validation Accuracy:0.2217\n",
    "Epoch #223: Loss:1.5585, Accuracy:0.2862, Validation Loss:1.6449, Validation Accuracy:0.2479\n",
    "Epoch #224: Loss:1.5578, Accuracy:0.2838, Validation Loss:1.6528, Validation Accuracy:0.2069\n",
    "Epoch #225: Loss:1.5548, Accuracy:0.2817, Validation Loss:1.6469, Validation Accuracy:0.2529\n",
    "Epoch #226: Loss:1.5549, Accuracy:0.2809, Validation Loss:1.6492, Validation Accuracy:0.2151\n",
    "Epoch #227: Loss:1.5594, Accuracy:0.2809, Validation Loss:1.6442, Validation Accuracy:0.2479\n",
    "Epoch #228: Loss:1.5633, Accuracy:0.2825, Validation Loss:1.6484, Validation Accuracy:0.2332\n",
    "Epoch #229: Loss:1.5627, Accuracy:0.2669, Validation Loss:1.6481, Validation Accuracy:0.2200\n",
    "Epoch #230: Loss:1.5620, Accuracy:0.2772, Validation Loss:1.6538, Validation Accuracy:0.2233\n",
    "Epoch #231: Loss:1.5601, Accuracy:0.2784, Validation Loss:1.6600, Validation Accuracy:0.2118\n",
    "Epoch #232: Loss:1.5599, Accuracy:0.2719, Validation Loss:1.6525, Validation Accuracy:0.2348\n",
    "Epoch #233: Loss:1.5606, Accuracy:0.2760, Validation Loss:1.6506, Validation Accuracy:0.1987\n",
    "Epoch #234: Loss:1.5573, Accuracy:0.2752, Validation Loss:1.6463, Validation Accuracy:0.2315\n",
    "Epoch #235: Loss:1.5604, Accuracy:0.2747, Validation Loss:1.6449, Validation Accuracy:0.2479\n",
    "Epoch #236: Loss:1.5584, Accuracy:0.2756, Validation Loss:1.6515, Validation Accuracy:0.2167\n",
    "Epoch #237: Loss:1.5570, Accuracy:0.2801, Validation Loss:1.6507, Validation Accuracy:0.2447\n",
    "Epoch #238: Loss:1.5571, Accuracy:0.2698, Validation Loss:1.6510, Validation Accuracy:0.2167\n",
    "Epoch #239: Loss:1.5588, Accuracy:0.2776, Validation Loss:1.6523, Validation Accuracy:0.2397\n",
    "Epoch #240: Loss:1.5574, Accuracy:0.2780, Validation Loss:1.6515, Validation Accuracy:0.2365\n",
    "Epoch #241: Loss:1.5583, Accuracy:0.2772, Validation Loss:1.6579, Validation Accuracy:0.2102\n",
    "Epoch #242: Loss:1.5566, Accuracy:0.2895, Validation Loss:1.6540, Validation Accuracy:0.2479\n",
    "Epoch #243: Loss:1.5543, Accuracy:0.2871, Validation Loss:1.6502, Validation Accuracy:0.2479\n",
    "Epoch #244: Loss:1.5544, Accuracy:0.2891, Validation Loss:1.6482, Validation Accuracy:0.2200\n",
    "Epoch #245: Loss:1.5544, Accuracy:0.2858, Validation Loss:1.6472, Validation Accuracy:0.2332\n",
    "Epoch #246: Loss:1.5546, Accuracy:0.2850, Validation Loss:1.6455, Validation Accuracy:0.2135\n",
    "Epoch #247: Loss:1.5551, Accuracy:0.2784, Validation Loss:1.6459, Validation Accuracy:0.2332\n",
    "Epoch #248: Loss:1.5584, Accuracy:0.2801, Validation Loss:1.6465, Validation Accuracy:0.2463\n",
    "Epoch #249: Loss:1.5583, Accuracy:0.2756, Validation Loss:1.6566, Validation Accuracy:0.2233\n",
    "Epoch #250: Loss:1.5572, Accuracy:0.2756, Validation Loss:1.6514, Validation Accuracy:0.2299\n",
    "Epoch #251: Loss:1.5521, Accuracy:0.2797, Validation Loss:1.6563, Validation Accuracy:0.2233\n",
    "Epoch #252: Loss:1.5488, Accuracy:0.2903, Validation Loss:1.6575, Validation Accuracy:0.2233\n",
    "Epoch #253: Loss:1.5488, Accuracy:0.2908, Validation Loss:1.6559, Validation Accuracy:0.2496\n",
    "Epoch #254: Loss:1.5451, Accuracy:0.2969, Validation Loss:1.6591, Validation Accuracy:0.2250\n",
    "Epoch #255: Loss:1.5450, Accuracy:0.2891, Validation Loss:1.6608, Validation Accuracy:0.2594\n",
    "Epoch #256: Loss:1.5453, Accuracy:0.2945, Validation Loss:1.6603, Validation Accuracy:0.2496\n",
    "Epoch #257: Loss:1.5400, Accuracy:0.2982, Validation Loss:1.6690, Validation Accuracy:0.2102\n",
    "Epoch #258: Loss:1.5409, Accuracy:0.2908, Validation Loss:1.6651, Validation Accuracy:0.2397\n",
    "Epoch #259: Loss:1.5403, Accuracy:0.2908, Validation Loss:1.6693, Validation Accuracy:0.2397\n",
    "Epoch #260: Loss:1.5389, Accuracy:0.2973, Validation Loss:1.6714, Validation Accuracy:0.2381\n",
    "Epoch #261: Loss:1.5385, Accuracy:0.2949, Validation Loss:1.6704, Validation Accuracy:0.2365\n",
    "Epoch #262: Loss:1.5351, Accuracy:0.3051, Validation Loss:1.6737, Validation Accuracy:0.2217\n",
    "Epoch #263: Loss:1.5337, Accuracy:0.3047, Validation Loss:1.6795, Validation Accuracy:0.2299\n",
    "Epoch #264: Loss:1.5332, Accuracy:0.2982, Validation Loss:1.6761, Validation Accuracy:0.2414\n",
    "Epoch #265: Loss:1.5312, Accuracy:0.3035, Validation Loss:1.6794, Validation Accuracy:0.2167\n",
    "Epoch #266: Loss:1.5321, Accuracy:0.3088, Validation Loss:1.6793, Validation Accuracy:0.2315\n",
    "Epoch #267: Loss:1.5333, Accuracy:0.3064, Validation Loss:1.6748, Validation Accuracy:0.2414\n",
    "Epoch #268: Loss:1.5374, Accuracy:0.2961, Validation Loss:1.6745, Validation Accuracy:0.2414\n",
    "Epoch #269: Loss:1.5324, Accuracy:0.3088, Validation Loss:1.6756, Validation Accuracy:0.2463\n",
    "Epoch #270: Loss:1.5307, Accuracy:0.3084, Validation Loss:1.6709, Validation Accuracy:0.2332\n",
    "Epoch #271: Loss:1.5303, Accuracy:0.3014, Validation Loss:1.6641, Validation Accuracy:0.2463\n",
    "Epoch #272: Loss:1.5310, Accuracy:0.3047, Validation Loss:1.6629, Validation Accuracy:0.2447\n",
    "Epoch #273: Loss:1.5305, Accuracy:0.3101, Validation Loss:1.6608, Validation Accuracy:0.2397\n",
    "Epoch #274: Loss:1.5340, Accuracy:0.3047, Validation Loss:1.6710, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5391, Accuracy:0.3039, Validation Loss:1.6848, Validation Accuracy:0.2282\n",
    "Epoch #276: Loss:1.5379, Accuracy:0.3060, Validation Loss:1.6639, Validation Accuracy:0.2496\n",
    "Epoch #277: Loss:1.5365, Accuracy:0.3031, Validation Loss:1.6701, Validation Accuracy:0.2348\n",
    "Epoch #278: Loss:1.5362, Accuracy:0.2957, Validation Loss:1.6756, Validation Accuracy:0.2381\n",
    "Epoch #279: Loss:1.5358, Accuracy:0.3039, Validation Loss:1.6721, Validation Accuracy:0.2348\n",
    "Epoch #280: Loss:1.5362, Accuracy:0.3068, Validation Loss:1.6714, Validation Accuracy:0.2332\n",
    "Epoch #281: Loss:1.5324, Accuracy:0.3047, Validation Loss:1.6843, Validation Accuracy:0.2069\n",
    "Epoch #282: Loss:1.5346, Accuracy:0.3043, Validation Loss:1.6834, Validation Accuracy:0.2332\n",
    "Epoch #283: Loss:1.5356, Accuracy:0.2998, Validation Loss:1.6837, Validation Accuracy:0.2397\n",
    "Epoch #284: Loss:1.5332, Accuracy:0.2973, Validation Loss:1.6906, Validation Accuracy:0.2463\n",
    "Epoch #285: Loss:1.5280, Accuracy:0.3055, Validation Loss:1.6829, Validation Accuracy:0.2003\n",
    "Epoch #286: Loss:1.5352, Accuracy:0.2875, Validation Loss:1.6739, Validation Accuracy:0.2365\n",
    "Epoch #287: Loss:1.5352, Accuracy:0.3072, Validation Loss:1.6674, Validation Accuracy:0.2414\n",
    "Epoch #288: Loss:1.5307, Accuracy:0.2969, Validation Loss:1.6722, Validation Accuracy:0.2282\n",
    "Epoch #289: Loss:1.5312, Accuracy:0.3018, Validation Loss:1.6800, Validation Accuracy:0.2496\n",
    "Epoch #290: Loss:1.5378, Accuracy:0.2903, Validation Loss:1.6711, Validation Accuracy:0.2250\n",
    "Epoch #291: Loss:1.5294, Accuracy:0.2940, Validation Loss:1.6712, Validation Accuracy:0.2414\n",
    "Epoch #292: Loss:1.5323, Accuracy:0.2986, Validation Loss:1.6700, Validation Accuracy:0.2365\n",
    "Epoch #293: Loss:1.5336, Accuracy:0.2969, Validation Loss:1.6616, Validation Accuracy:0.2545\n",
    "Epoch #294: Loss:1.5355, Accuracy:0.3018, Validation Loss:1.6596, Validation Accuracy:0.2644\n",
    "Epoch #295: Loss:1.5366, Accuracy:0.3014, Validation Loss:1.6574, Validation Accuracy:0.2397\n",
    "Epoch #296: Loss:1.5294, Accuracy:0.3088, Validation Loss:1.6603, Validation Accuracy:0.2430\n",
    "Epoch #297: Loss:1.5296, Accuracy:0.3060, Validation Loss:1.6560, Validation Accuracy:0.2332\n",
    "Epoch #298: Loss:1.5258, Accuracy:0.3097, Validation Loss:1.6595, Validation Accuracy:0.2397\n",
    "Epoch #299: Loss:1.5271, Accuracy:0.3060, Validation Loss:1.6605, Validation Accuracy:0.2250\n",
    "Epoch #300: Loss:1.5331, Accuracy:0.3018, Validation Loss:1.6674, Validation Accuracy:0.2315\n",
    "\n",
    "Test:\n",
    "Test Loss:1.66738021, Accuracy:0.2315\n",
    "Labels: ['05', '02', '03', '04', '01']\n",
    "Confusion Matrix:\n",
    "      05  02  03  04  01\n",
    "t:05  69   6  38  19  10\n",
    "t:02  62   5  25  15   7\n",
    "t:03  60   3  32  12   8\n",
    "t:04  56   8  22  20   6\n",
    "t:01  56   6  38  11  15\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.23      0.49      0.31       142\n",
    "          02       0.18      0.04      0.07       114\n",
    "          03       0.21      0.28      0.24       115\n",
    "          04       0.26      0.18      0.21       112\n",
    "          01       0.33      0.12      0.17       126\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.24      0.22      0.20       609\n",
    "weighted avg       0.24      0.23      0.21       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 01:23:02 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 43 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6069465689666949, 1.6068474459530684, 1.6060973255113624, 1.6065773991332657, 1.6060396665814278, 1.6058722881260763, 1.6057803705212321, 1.6064317907605852, 1.6051144139911546, 1.6051296761078984, 1.6051554396038963, 1.6050928678418614, 1.6046707915946572, 1.604740214269541, 1.604734592249828, 1.604828256887364, 1.6050649840256264, 1.6051207215132188, 1.6052606693042324, 1.6054260515422851, 1.6050726230117096, 1.6050174982089715, 1.6038826964367394, 1.6038986152811787, 1.6035090909998602, 1.606269422618822, 1.6071728949476345, 1.6054156163251654, 1.6058975440528005, 1.6048008974745551, 1.6052399114239195, 1.6054158889992876, 1.6056737809737132, 1.6052283718080944, 1.6049373809535712, 1.6053314201154536, 1.6053797452907843, 1.6061082215144717, 1.6059361412411643, 1.6064148170411685, 1.606001107955018, 1.6062398383574337, 1.6059921392666294, 1.6062078197992886, 1.6063844014466886, 1.6058487962619425, 1.6060794537094818, 1.6054780265967834, 1.6048622775351864, 1.605586427185923, 1.6071427647507641, 1.6069180119801036, 1.607011898789304, 1.6059554273076049, 1.6059406175597744, 1.6068413351556938, 1.6077107322235609, 1.6085073697351666, 1.6089782773567538, 1.610225137818623, 1.6085617452223704, 1.6097574551117244, 1.6099570842799296, 1.6111036605631386, 1.6106486629774222, 1.6098356628652863, 1.6074869023951013, 1.6072239648728144, 1.6056374744045714, 1.608040890474429, 1.6086459784280687, 1.6070829768877704, 1.6092135931666458, 1.6095806974887066, 1.6112495488525416, 1.61268111852981, 1.6112955941746778, 1.6097743053154405, 1.610447898286904, 1.6113221065947183, 1.610201036988808, 1.6117807274381515, 1.6120744595190966, 1.6119444062948618, 1.620486333452422, 1.620384839172238, 1.6164370890712894, 1.6083794246949195, 1.603335601551388, 1.6042976258032036, 1.6057949040715134, 1.6062537190949389, 1.606285541907124, 1.6055146782660523, 1.6049281478123907, 1.605766490762457, 1.6071415198064594, 1.6087018267078743, 1.609061106476682, 1.611415903556523, 1.611966268965372, 1.6129544176687356, 1.6142809310765884, 1.6133899631954374, 1.6132266098642585, 1.6133735066368466, 1.613564923283306, 1.6147416835935244, 1.6132879239585012, 1.6128044911401809, 1.6136854996626404, 1.6132736086649653, 1.6149407711326587, 1.6147476663730416, 1.6178708941674194, 1.6141367471472579, 1.6117176207024084, 1.6120604272742185, 1.6139994965398252, 1.6149201030997415, 1.6191057305422127, 1.6185519984986003, 1.6200988421886426, 1.6211300522627305, 1.6228379800010393, 1.6259263289973067, 1.6271565551632534, 1.6259502562004553, 1.6228817578019767, 1.6260469410024057, 1.6217862821760631, 1.6253521837820168, 1.621743210235057, 1.622807190531776, 1.6262529052928556, 1.628614545455707, 1.6364719744386345, 1.627055731117236, 1.6296718968154957, 1.6287660876714145, 1.6283094542367118, 1.6287308066153565, 1.6259552860886397, 1.6285746890335835, 1.6258683970017582, 1.6257088951680851, 1.6176416781931284, 1.6215509168424433, 1.6239300034512048, 1.625637567689266, 1.6293546902918072, 1.6305511620244368, 1.6259644888891962, 1.6214195272605407, 1.6266429886246354, 1.6265190354317476, 1.6246403122966122, 1.622289990752397, 1.6187742061802906, 1.628536937859258, 1.631007773927084, 1.6276172256626322, 1.633256284669898, 1.6261643543227748, 1.626087560050789, 1.6295432562898533, 1.6268197305879766, 1.6281334202865074, 1.6327420373268315, 1.6297237307371568, 1.6303780533018566, 1.630682032488054, 1.6311788768408138, 1.6318251434805358, 1.6297689032280582, 1.632470497944085, 1.633628653775295, 1.6340800874143202, 1.6379098582933298, 1.6326504932052788, 1.6429420807482966, 1.6409868066534032, 1.6340367146117738, 1.6254231784927042, 1.636315097558283, 1.6234917930390056, 1.6330484897632318, 1.6338668908978917, 1.632509552786503, 1.6284029787201404, 1.6333343548140502, 1.628043512601179, 1.633190193199759, 1.6457106883106951, 1.635213896167298, 1.648995354845019, 1.6367160105352918, 1.630413469618373, 1.6363912697496086, 1.6413582684762764, 1.6402723763768112, 1.639227612852463, 1.635726794233463, 1.6373417036873954, 1.6412587300897232, 1.6385500820595251, 1.6399035359838325, 1.6407998124954148, 1.6437232946527416, 1.640817403793335, 1.6421691255616437, 1.6440376518982385, 1.6416256163507847, 1.6403453772878411, 1.642017736419277, 1.6435922370559868, 1.6426921613110697, 1.6461444271021877, 1.6443421243642546, 1.6450268409913789, 1.6421964511104015, 1.6453000189635554, 1.6449168458556502, 1.6527607313713613, 1.646850829053982, 1.6491949464299995, 1.6441833554034555, 1.6483565702031202, 1.6481321558772246, 1.6537504151145421, 1.6599619419899676, 1.652504554131544, 1.6506446603875247, 1.6462568888327562, 1.6449115937958014, 1.6515404205212647, 1.650709565245655, 1.6510255761530208, 1.652286250603023, 1.6514539753862203, 1.6578541608475308, 1.6540234648731151, 1.6501699913115728, 1.6482012177923042, 1.6471989409285421, 1.6455018567334254, 1.645934926660973, 1.6464773598562907, 1.6565533152159015, 1.6514295077284764, 1.6563410576928426, 1.6574788757145698, 1.6558880962566007, 1.6590805263159114, 1.6607941894108438, 1.6602750223845684, 1.6690073091604047, 1.6651222028560044, 1.6693156842136228, 1.6713561510609092, 1.6704496023885917, 1.673668047085967, 1.6795302962238956, 1.6761462093378328, 1.679360845797559, 1.6792606205384328, 1.6747730899914144, 1.6744500817532217, 1.675588427701803, 1.6709327476560971, 1.6641300699393737, 1.6629402968292362, 1.6608420759194786, 1.6709848336985547, 1.6847580365946728, 1.663900520609713, 1.6701387603490419, 1.6755947964923528, 1.6721452674254995, 1.6714242352249196, 1.6842505516872812, 1.6834069842775468, 1.6837157400566565, 1.6905710297852314, 1.6829130240457595, 1.6739490229899465, 1.667399744682124, 1.6721654233869856, 1.6799660339731302, 1.671145952589602, 1.6711524427426467, 1.67003832799069, 1.6616404643786953, 1.6595500801584404, 1.6574279257816633, 1.6602567052606292, 1.6559543768173368, 1.6595440932682581, 1.660508009404776, 1.667380354674579], 'val_acc': [0.2348111656254344, 0.22331691275187118, 0.22495894877879294, 0.2233169126539982, 0.23152709327797194, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23316912940276668, 0.23809523797289686, 0.2397372740976916, 0.2397372740976916, 0.23645320175022916, 0.2380952378750239, 0.2364532014566102, 0.23152709318009895, 0.23481116552756143, 0.23316912940276668, 0.23645320165235617, 0.23645320175022916, 0.2298850570553042, 0.2331691293048937, 0.22988505695743122, 0.23645320175022916, 0.23645320165235617, 0.2380952378750239, 0.23645320175022916, 0.23152709318009895, 0.23152709327797194, 0.23316912950063964, 0.2380952378750239, 0.23645320175022916, 0.23809523797289686, 0.23809523797289686, 0.2348111656254344, 0.23481116572330737, 0.23316912959851263, 0.2397372740976916, 0.23809523797289686, 0.23316912950063964, 0.23645320175022916, 0.2348111656254344, 0.2348111656254344, 0.23645320165235617, 0.23152709327797194, 0.2298850570553042, 0.22824302102838243, 0.23481116572330737, 0.2233169125561252, 0.2315270933758449, 0.2315270934737179, 0.21674876835056517, 0.2200328404044087, 0.21346469600310272, 0.21674876835056517, 0.22660098292165984, 0.2085385875308455, 0.20361247935220722, 0.23645320184810212, 0.2183908044753599, 0.21839080437748695, 0.22988505725105016, 0.22495894877879294, 0.22331691294761713, 0.21346469580735675, 0.21510673203002448, 0.21839080437748695, 0.21018062355776726, 0.21510673212789747, 0.21839080437748695, 0.2167487682526922, 0.2183908044753599, 0.2151067319321515, 0.21510673212789747, 0.20853858743297252, 0.21346469590522973, 0.20525451547700196, 0.2298850571531772, 0.2183908044753599, 0.23481116364350654, 0.22988505725105016, 0.22331691284974417, 0.20197044273804757, 0.2200328405022817, 0.22495894868091998, 0.2233169126539982, 0.2183908044753599, 0.2183908044753599, 0.22003284030653572, 0.22495894887666593, 0.2183908044753599, 0.22167487662707644, 0.22495894887666593, 0.20361247935220722, 0.198686370977823, 0.22495894887666593, 0.20689655130817777, 0.22331691284974417, 0.22495894887666593, 0.20689655130817777, 0.20689655121030479, 0.2298850570553042, 0.2298850570553042, 0.20361247905858829, 0.21018062345989427, 0.224958948583047, 0.211822659780435, 0.19540229784737667, 0.22988505685955823, 0.23152709269073404, 0.23152709298435298, 0.21839080418174098, 0.21346469580735675, 0.21510673203002448, 0.19868637068420403, 0.2183908045732329, 0.21182265958468902, 0.21182265958468902, 0.20197044264017458, 0.20525451498763705, 0.2068965511124318, 0.20525451498763705, 0.24630541830325164, 0.21018062326414833, 0.2298850570553042, 0.23481116552756143, 0.20525451469401812, 0.22988505695743122, 0.2003284067111258, 0.23973727390194566, 0.22988505725105016, 0.23152709318009895, 0.21839080418174098, 0.22167487623558452, 0.22988505725105016, 0.23481116572330737, 0.24137930814268554, 0.23809523579522307, 0.22824302093050947, 0.2151067318342785, 0.22003284020866276, 0.22824301904645458, 0.21346469600310272, 0.22660098292165984, 0.2282430211262554, 0.22003284011078977, 0.2331691293048937, 0.2249589489745389, 0.2463054185968706, 0.23809523797289686, 0.23809523579522307, 0.21839080427961396, 0.2528735630960496, 0.22824302102838243, 0.2348111656254344, 0.23316912950063964, 0.224958948583047, 0.22495894877879294, 0.23973727390194566, 0.22660098490358768, 0.2200328404044087, 0.23316912950063964, 0.22660098519720664, 0.2249589489745389, 0.23152709357159088, 0.2249589489745389, 0.2282430211262554, 0.24302134436535328, 0.23481116582118036, 0.23645320184810212, 0.21346469590522973, 0.23316912969638562, 0.2282430212241284, 0.22167487474302158, 0.24466338049014802, 0.2315270912960441, 0.20689655130817777, 0.24630541869474357, 0.19376026191832788, 0.22167487623558452, 0.2282430211262554, 0.23481116582118036, 0.251231524891454, 0.23645320165235617, 0.23645320155448318, 0.21839080418174098, 0.22495894868091998, 0.2233169126539982, 0.21674876805694623, 0.23645320184810212, 0.21674876795907325, 0.2134646956116108, 0.24466338256994882, 0.24302134644515408, 0.2216748763334575, 0.23809523807076985, 0.21839080249343207, 0.2348111639371255, 0.2183908044753599, 0.2249589467968651, 0.19047618986448436, 0.2298850571531772, 0.21839080437748695, 0.19704433465728227, 0.2167487682526922, 0.2167487682526922, 0.19704433455940928, 0.22167487672494943, 0.24137930814268554, 0.21182266007405393, 0.2430213441696073, 0.20853858782446444, 0.24630541651706978, 0.2216748768228224, 0.24794745264186452, 0.2068965516017967, 0.2528735630960496, 0.2151067318342785, 0.2479474548195383, 0.2331691293048937, 0.22003284030653572, 0.22331691284974417, 0.21182265958468902, 0.23481116582118036, 0.19868637068420403, 0.23152709308222597, 0.2479474548195383, 0.2167487682526922, 0.24466338208058394, 0.21674876805694623, 0.23973727399981865, 0.23645320165235617, 0.21018062345989427, 0.2479474548195383, 0.24794745472166535, 0.22003284020866276, 0.23316912950063964, 0.21346469570948376, 0.2331691293048937, 0.24630541869474357, 0.2233169125561252, 0.22988505685955823, 0.2233169125561252, 0.22331691245825225, 0.24958948886453225, 0.22495894907241187, 0.25944170541755474, 0.24958948876665926, 0.2101806238513862, 0.2397372740976916, 0.2397372741955646, 0.23809523807076985, 0.2364532019459751, 0.2216748745472756, 0.22988505725105016, 0.24137931022248635, 0.2167487682526922, 0.23152709318009895, 0.24137931032035934, 0.24137931032035934, 0.2463054164191968, 0.23316912959851263, 0.2463054164191968, 0.24466338039227503, 0.2397372719200178, 0.23481116354563358, 0.2282430213220014, 0.24958949094433308, 0.23481116533181545, 0.23809523777715091, 0.23481116552756143, 0.23316912959851263, 0.20689655140605073, 0.2331691275187118, 0.2397372741955646, 0.2463054164191968, 0.20032840641750688, 0.23645320155448318, 0.2413793099288674, 0.22824302053901754, 0.24958949094433308, 0.22495894877879294, 0.24137930804481256, 0.23645319976830131, 0.2545155992208443, 0.26436781398768494, 0.23973727399981865, 0.24302134624940813, 0.23316912969638562, 0.2397372720178908, 0.22495894907241187, 0.23152709327797194], 'loss': [1.6086494304316246, 1.6058578904894099, 1.6055099767336365, 1.6045745365918294, 1.604166941329439, 1.6040534534003945, 1.6039733310989286, 1.604360390247995, 1.6037012674480493, 1.6034342442205065, 1.6030744364129443, 1.6029785501393938, 1.6029131346904277, 1.6032362933521154, 1.6027072035556456, 1.6021210650888558, 1.6020827064768735, 1.6015245978102792, 1.6013753658936989, 1.6010666477117206, 1.6006873788285305, 1.6000850909054891, 1.6010989364902097, 1.600334268575821, 1.5998225308541645, 1.603212406209362, 1.6017691436489505, 1.6015118985695027, 1.6006766036795395, 1.6002131298582167, 1.600342894041073, 1.5997086267941296, 1.5993945916330545, 1.5997120025466356, 1.599552330686816, 1.599087130607276, 1.5986512027481987, 1.5992040908801728, 1.599304172880106, 1.599347768720905, 1.5992531875320528, 1.5993450611278996, 1.598930493664203, 1.5982269643023763, 1.5977646338131883, 1.5978857982819574, 1.5977506811858693, 1.5974490048704206, 1.5971058088161618, 1.5972843550069131, 1.5974005514101817, 1.5978475304844444, 1.5969939904536066, 1.5971574911591453, 1.5970866761413198, 1.5965082171516496, 1.5960349494671675, 1.5945416031432103, 1.5946499611073206, 1.5954019402576423, 1.59466128882931, 1.5955296648111676, 1.5936557541637695, 1.5955822103566948, 1.5966126176610864, 1.5969312288922695, 1.5966357784349572, 1.5952020642693772, 1.5958029917867766, 1.594616430447087, 1.5953413023840965, 1.5935605868177003, 1.5945355831964794, 1.5927801308935428, 1.592414440070824, 1.592028484207404, 1.59214791776708, 1.592381277710995, 1.5917462618199216, 1.5909625921895616, 1.5909323605179053, 1.5896063427660745, 1.5900565170409497, 1.591311170971614, 1.5892261060111577, 1.6097979387953052, 1.6123571044855294, 1.6086554416640828, 1.5999605749177248, 1.5963669160063507, 1.5959699830969747, 1.5966500164791788, 1.5956643777706294, 1.5939405013893175, 1.5928191418030913, 1.5928268086494117, 1.5911417057382007, 1.591193243610296, 1.5906909566150798, 1.5893454906632034, 1.5897586260488146, 1.5884511901123077, 1.589281457209734, 1.5900410355483727, 1.589484765984929, 1.5896900307226476, 1.5903622150910708, 1.589601862210268, 1.5903767736540684, 1.5907148298052058, 1.5896697846771022, 1.588907490569708, 1.5875971034322187, 1.5866135039123912, 1.5862543519272696, 1.584678277244803, 1.5850275425450757, 1.5855419449737673, 1.5863219431048792, 1.5865847076968247, 1.5818881027507585, 1.5831286251667345, 1.5819042987647243, 1.5833321674648497, 1.5828431632239717, 1.5820322706469276, 1.5832973211942512, 1.5799819715703538, 1.5802215775425184, 1.5802213332491488, 1.580565074532918, 1.5788256683388775, 1.5788779091786065, 1.5760948941937707, 1.5742792061466946, 1.5744821359489487, 1.5756495023899744, 1.5764282469387172, 1.5764324531418097, 1.5766690565085755, 1.5761658351768948, 1.5766035618478513, 1.5782270719383287, 1.577068424518593, 1.5794087644475197, 1.592108451414402, 1.5932281280689906, 1.5873478465990853, 1.5832087857032948, 1.5832268826036238, 1.5814446324440488, 1.58181345090729, 1.578529106811821, 1.576683043699245, 1.5737643532194885, 1.574247857971113, 1.572441708429638, 1.574292598514831, 1.580955659829126, 1.5858012439778697, 1.5754480507829105, 1.5816702010450423, 1.5761888994573323, 1.5768554135759263, 1.5737730755208699, 1.5749436088656008, 1.5738349635498234, 1.5744425909719917, 1.5762165091610543, 1.5756347876554642, 1.575718461414627, 1.5751089190065983, 1.5741647618017647, 1.5728627846715877, 1.5735013890315375, 1.5711096597648009, 1.5686020088881194, 1.5699743673786735, 1.5677992508886287, 1.5678688933961935, 1.5694767090818966, 1.569538199093797, 1.5694845565780233, 1.5698959838193545, 1.57011302888271, 1.5752467144686093, 1.5728408808091339, 1.5731005638776618, 1.5676032335116878, 1.5671762369007056, 1.565413837021626, 1.5639428109358957, 1.562573789962753, 1.5614793719697047, 1.578573347753568, 1.5852468680062577, 1.5797965530497338, 1.577053544359775, 1.5693178630707445, 1.5674428749378213, 1.5659763257361536, 1.564242150406573, 1.5647705091832844, 1.5632562617746466, 1.5619369663497016, 1.5647941484588372, 1.5731944083677916, 1.5692223378030672, 1.5758345812497931, 1.5716481029130596, 1.5788741997869598, 1.580532500729179, 1.5759333940012499, 1.5750369685386485, 1.5725478654035063, 1.5682875456506467, 1.5682707833558382, 1.565237334572559, 1.5640155795663289, 1.5627661403444513, 1.5593785390227237, 1.562316748346881, 1.5584564440059465, 1.557849496590773, 1.5548480792212045, 1.5549417902067213, 1.559350210930043, 1.5633306021563083, 1.5627318534518169, 1.5620306126146102, 1.5601030334554904, 1.5598645088853778, 1.560625763056949, 1.5572561214591933, 1.5603885583074675, 1.5584229240672054, 1.5569919711020939, 1.5571148477785397, 1.5588306513165546, 1.5573528757330328, 1.558331133550687, 1.556555683705841, 1.5543444579142076, 1.5543755240508907, 1.554428655166156, 1.554611077103037, 1.5550615776246088, 1.5584491612240519, 1.5582964705735507, 1.5572109168559864, 1.5520767470404842, 1.548792751907568, 1.5488326199979998, 1.5451312024000978, 1.5449927293789216, 1.545279060790671, 1.5399647696551846, 1.5409141536610818, 1.54026865851463, 1.5388636713400023, 1.5385252086044092, 1.5350767402433516, 1.5336587516189355, 1.5332177208189601, 1.5312180570997986, 1.5320791057003107, 1.533275556809114, 1.5374033421216804, 1.5323882043728838, 1.5307298272052585, 1.5303468860395146, 1.5310238240925438, 1.5304975042108149, 1.5339824193312157, 1.539147981725924, 1.537909474412029, 1.5364658601964523, 1.536222074604622, 1.5357622353203242, 1.536223996493361, 1.5323913162983418, 1.534585808826423, 1.5356420724543702, 1.5332111550552399, 1.5279922858400756, 1.5352439480885343, 1.5351737776331345, 1.5307445015016277, 1.5311973411689304, 1.5377936255026157, 1.5294459985266966, 1.532318930116767, 1.533591285672276, 1.5355068006554669, 1.5366454919505659, 1.5293735272096167, 1.5296146283649077, 1.525809905073726, 1.5270900095267952, 1.533141305559225], 'acc': [0.23367556396573477, 0.2324435310931666, 0.2320328542767609, 0.23326488907087511, 0.22997946638223815, 0.23203285226342124, 0.23285421047367355, 0.23244353089734024, 0.2328542085337688, 0.2328542091028891, 0.23326488632930623, 0.23285420890706277, 0.23285420990455322, 0.2328542102778472, 0.2328542110611526, 0.2324435310931666, 0.23285420833794243, 0.2320328542767609, 0.23901437415724172, 0.23942505179367027, 0.23901437317810997, 0.24229979512015898, 0.24229979471014756, 0.24394250479077412, 0.24722792749776978, 0.23244353171736307, 0.23696098450028186, 0.244353183583802, 0.23778234192722877, 0.2398357284142496, 0.24763860454671927, 0.2447638588152382, 0.24229979353518946, 0.24517453725333085, 0.24517453883830037, 0.24640657051755172, 0.24599589327277588, 0.24763860296174975, 0.24312115055212494, 0.2468172487231006, 0.24558521467557434, 0.2447638598310874, 0.24558521545887974, 0.24558521506722703, 0.24558521665219654, 0.24640657069501937, 0.24763860258845577, 0.24845995659456116, 0.24763860454671927, 0.2509240235146556, 0.24804927956397038, 0.24845995996032652, 0.24928131362985537, 0.24599589127779498, 0.2537987689952341, 0.2525667347702402, 0.25215605754382314, 0.2513347025035098, 0.2533880896147272, 0.25503080088867053, 0.25174537972992694, 0.2394250519711379, 0.2533880921604697, 0.25379876562946874, 0.24763860413670785, 0.24147843988401935, 0.24065708560865273, 0.2468172487231006, 0.24969199203123058, 0.24845995898119477, 0.24887063836170173, 0.2513347028951625, 0.24640657149668346, 0.24845995976450017, 0.25010266986348545, 0.24804928175477767, 0.24928131280983254, 0.25051334589658575, 0.24558521526305338, 0.24887063642179696, 0.24928131519646615, 0.2488706358159592, 0.254620123252242, 0.24722792497038595, 0.25256673437858757, 0.23819301990023384, 0.23449692018100612, 0.22997946558057406, 0.2464065708908457, 0.25133470132855174, 0.23531827541714576, 0.245174539015768, 0.2476386047241869, 0.2501026676543195, 0.2496919928328947, 0.2574948685369942, 0.2472279271244758, 0.2513347027176949, 0.2406570838462156, 0.24845995659456116, 0.2521560587187812, 0.2554414790758607, 0.2537987701701922, 0.2554414804466451, 0.24887063759675507, 0.2492813153922925, 0.2537987686035814, 0.25420944680913027, 0.25010266927600644, 0.24188911924616757, 0.2533880892230745, 0.25174538129653773, 0.2570841893523136, 0.2636550302622989, 0.2591375758760519, 0.258316221619044, 0.2640657086636741, 0.2624229987788739, 0.25954825370830675, 0.2611909661388495, 0.26119096651214346, 0.2558521563022778, 0.265297742888668, 0.25913757466437637, 0.26365503006647256, 0.261601643561093, 0.25831622001571575, 0.25708418837318187, 0.2628336750261593, 0.26488706507477183, 0.2681724834368704, 0.26570842109421683, 0.2624229991705266, 0.2735112940751062, 0.2698151945333461, 0.27679671501966474, 0.2706365491452893, 0.2636550330038678, 0.27351129188429896, 0.26160164336526665, 0.26611909694984953, 0.2665297735887876, 0.2648870637039874, 0.2554414782925553, 0.269815197274915, 0.2521560573663555, 0.2542094452425195, 0.25338809039803256, 0.2599589308796477, 0.2546201252105055, 0.26694045336094724, 0.25954825228244616, 0.26324435143255354, 0.2661190967540232, 0.2616016427594289, 0.2677618066388234, 0.2743326506820303, 0.2673511299998853, 0.2681724848443723, 0.2525667341827612, 0.2681724854318513, 0.26078028793330066, 0.26776180820543416, 0.25913757763848905, 0.2759753583760232, 0.2681724862151567, 0.2731006168670478, 0.2755646839278924, 0.26981519590413056, 0.2657084207025642, 0.26447638549843855, 0.26406570964280585, 0.2661190965581968, 0.26817248562767765, 0.26570842089839053, 0.25913757861762077, 0.2722792614167231, 0.28213552542535675, 0.2800821349850915, 0.2817248476114606, 0.26899384145129634, 0.2706365503753235, 0.2735112936834535, 0.27186858258697777, 0.27761807103910974, 0.26447638708340804, 0.2681724852543837, 0.2599589322871496, 0.2767967165862755, 0.28418891194909507, 0.2673511290207536, 0.27679671263303113, 0.2825462018868272, 0.2813141685858889, 0.2677618050354952, 0.25585215806471495, 0.256673510167633, 0.26447638369928395, 0.272689940209751, 0.27268993903479294, 0.27104722637170636, 0.2837782335110024, 0.2735112912968199, 0.26981519547576044, 0.27926078209886807, 0.27351129149264625, 0.27597536154596225, 0.27926078170721536, 0.27104722738755554, 0.2722792629833339, 0.2652977413220572, 0.2570841897439663, 0.26817248601933036, 0.26981519688326233, 0.26981519590413056, 0.2796714583461534, 0.28624230043109683, 0.2804928153447302, 0.2804928141697721, 0.2755646839278924, 0.2862423012144022, 0.27843942349696305, 0.28624229980690036, 0.2837782319443916, 0.28172484444152157, 0.280903489792861, 0.2809034901845137, 0.28254620128098945, 0.2669404519901628, 0.27720739044692727, 0.2784394255103027, 0.2718685853652641, 0.27597535919604604, 0.27515400176909915, 0.27474332434685567, 0.2755646805621271, 0.28008213596422327, 0.2698151929300179, 0.27761807103910974, 0.2780287480697005, 0.2772073916586028, 0.28952771980904457, 0.2870636544555609, 0.28911704493254364, 0.2858316239879851, 0.2850102661693855, 0.2784394266669021, 0.280082134360895, 0.27556468373206605, 0.275564679974648, 0.2796714555678671, 0.2903490742251614, 0.2907597550131702, 0.29691991670175744, 0.2891170429375627, 0.29445585240084043, 0.29815195151423035, 0.2907597520390575, 0.2907597534465594, 0.2973305945156536, 0.294866531585521, 0.3051334716456137, 0.30472279206928043, 0.2981519501434459, 0.30349075839504813, 0.30882956958404556, 0.3063655021131895, 0.2960985630322286, 0.3088295672341294, 0.3084188913784967, 0.3014373724587889, 0.30472279164091026, 0.3100616002841652, 0.30472279320752105, 0.30390143875468684, 0.30595482488677245, 0.3030800831268945, 0.2956878850225061, 0.3039014369922497, 0.30677618090621744, 0.30472279477413183, 0.3043121146103195, 0.29979465963659346, 0.29733059651063454, 0.30554414848037814, 0.287474334264438, 0.3071868598950717, 0.2969199200675228, 0.3018480507010552, 0.290349074029335, 0.29404517552935855, 0.298562629952323, 0.29691991866002093, 0.30184805128853426, 0.30143736952139366, 0.3088295688007402, 0.30595482547425146, 0.30965092207861633, 0.30595482390764067, 0.30184804753111616]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
