{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf9.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 01:23:09 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '2', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001DF0C9B9550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001DF0A0F7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0810, Accuracy:0.3951, Validation Loss:1.0766, Validation Accuracy:0.4236\n",
    "Epoch #2: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0721, Validation Accuracy:0.4105\n",
    "Epoch #3: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.4138\n",
    "Epoch #4: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0727, Validation Accuracy:0.4154\n",
    "Epoch #5: Loss:1.0727, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #6: Loss:1.0720, Accuracy:0.4172, Validation Loss:1.0744, Validation Accuracy:0.4089\n",
    "Epoch #7: Loss:1.0734, Accuracy:0.4127, Validation Loss:1.0736, Validation Accuracy:0.4089\n",
    "Epoch #8: Loss:1.0737, Accuracy:0.4049, Validation Loss:1.0742, Validation Accuracy:0.4122\n",
    "Epoch #9: Loss:1.0735, Accuracy:0.4025, Validation Loss:1.0734, Validation Accuracy:0.4122\n",
    "Epoch #10: Loss:1.0732, Accuracy:0.4094, Validation Loss:1.0747, Validation Accuracy:0.4007\n",
    "Epoch #11: Loss:1.0744, Accuracy:0.3967, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #12: Loss:1.0732, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #13: Loss:1.0743, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #14: Loss:1.0726, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #15: Loss:1.0728, Accuracy:0.4094, Validation Loss:1.0755, Validation Accuracy:0.3760\n",
    "Epoch #16: Loss:1.0729, Accuracy:0.4164, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #17: Loss:1.0728, Accuracy:0.4111, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #18: Loss:1.0729, Accuracy:0.4049, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #19: Loss:1.0726, Accuracy:0.4103, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #20: Loss:1.0725, Accuracy:0.4078, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #21: Loss:1.0722, Accuracy:0.4049, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #22: Loss:1.0715, Accuracy:0.4168, Validation Loss:1.0742, Validation Accuracy:0.4105\n",
    "Epoch #23: Loss:1.0714, Accuracy:0.4214, Validation Loss:1.0744, Validation Accuracy:0.4105\n",
    "Epoch #24: Loss:1.0722, Accuracy:0.4090, Validation Loss:1.0739, Validation Accuracy:0.4105\n",
    "Epoch #25: Loss:1.0726, Accuracy:0.4070, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #26: Loss:1.0724, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.4023\n",
    "Epoch #27: Loss:1.0721, Accuracy:0.4041, Validation Loss:1.0731, Validation Accuracy:0.4138\n",
    "Epoch #28: Loss:1.0723, Accuracy:0.4094, Validation Loss:1.0726, Validation Accuracy:0.4122\n",
    "Epoch #29: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #30: Loss:1.0712, Accuracy:0.4090, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #31: Loss:1.0714, Accuracy:0.4115, Validation Loss:1.0739, Validation Accuracy:0.3859\n",
    "Epoch #32: Loss:1.0714, Accuracy:0.4016, Validation Loss:1.0754, Validation Accuracy:0.3793\n",
    "Epoch #33: Loss:1.0714, Accuracy:0.4057, Validation Loss:1.0779, Validation Accuracy:0.3957\n",
    "Epoch #34: Loss:1.0729, Accuracy:0.4074, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #35: Loss:1.0721, Accuracy:0.4053, Validation Loss:1.0764, Validation Accuracy:0.3793\n",
    "Epoch #36: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #37: Loss:1.0752, Accuracy:0.3889, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0748, Accuracy:0.3877, Validation Loss:1.0760, Validation Accuracy:0.3547\n",
    "Epoch #39: Loss:1.0746, Accuracy:0.3844, Validation Loss:1.0761, Validation Accuracy:0.3547\n",
    "Epoch #40: Loss:1.0739, Accuracy:0.3914, Validation Loss:1.0756, Validation Accuracy:0.4007\n",
    "Epoch #41: Loss:1.0748, Accuracy:0.3938, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #42: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.3596\n",
    "Epoch #43: Loss:1.0734, Accuracy:0.3955, Validation Loss:1.0758, Validation Accuracy:0.3695\n",
    "Epoch #44: Loss:1.0735, Accuracy:0.4016, Validation Loss:1.0755, Validation Accuracy:0.3777\n",
    "Epoch #45: Loss:1.0732, Accuracy:0.4025, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #46: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #47: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #48: Loss:1.0733, Accuracy:0.4037, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #49: Loss:1.0733, Accuracy:0.4053, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #50: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #51: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #52: Loss:1.0729, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #53: Loss:1.0730, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3892\n",
    "Epoch #54: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #55: Loss:1.0729, Accuracy:0.4074, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #56: Loss:1.0727, Accuracy:0.4062, Validation Loss:1.0741, Validation Accuracy:0.3810\n",
    "Epoch #57: Loss:1.0726, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #58: Loss:1.0727, Accuracy:0.3979, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #59: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #60: Loss:1.0727, Accuracy:0.4057, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #61: Loss:1.0727, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #62: Loss:1.0731, Accuracy:0.3967, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #63: Loss:1.0729, Accuracy:0.3967, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #64: Loss:1.0726, Accuracy:0.4049, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #65: Loss:1.0724, Accuracy:0.4115, Validation Loss:1.0739, Validation Accuracy:0.4122\n",
    "Epoch #66: Loss:1.0721, Accuracy:0.4107, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #67: Loss:1.0725, Accuracy:0.4062, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #68: Loss:1.0724, Accuracy:0.4070, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #69: Loss:1.0721, Accuracy:0.4082, Validation Loss:1.0771, Validation Accuracy:0.3941\n",
    "Epoch #70: Loss:1.0741, Accuracy:0.4012, Validation Loss:1.0756, Validation Accuracy:0.3810\n",
    "Epoch #71: Loss:1.0732, Accuracy:0.3934, Validation Loss:1.0766, Validation Accuracy:0.3892\n",
    "Epoch #72: Loss:1.0736, Accuracy:0.3873, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #73: Loss:1.0733, Accuracy:0.4000, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #74: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.3727\n",
    "Epoch #75: Loss:1.0725, Accuracy:0.3934, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #76: Loss:1.0722, Accuracy:0.4082, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #77: Loss:1.0744, Accuracy:0.3967, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #78: Loss:1.0731, Accuracy:0.3864, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #79: Loss:1.0727, Accuracy:0.3959, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #80: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #81: Loss:1.0722, Accuracy:0.4086, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #82: Loss:1.0720, Accuracy:0.4074, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #83: Loss:1.0723, Accuracy:0.4000, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #84: Loss:1.0718, Accuracy:0.4070, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #85: Loss:1.0720, Accuracy:0.4025, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #86: Loss:1.0720, Accuracy:0.3971, Validation Loss:1.0729, Validation Accuracy:0.3875\n",
    "Epoch #87: Loss:1.0717, Accuracy:0.3959, Validation Loss:1.0709, Validation Accuracy:0.4007\n",
    "Epoch #88: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #89: Loss:1.0730, Accuracy:0.3926, Validation Loss:1.0766, Validation Accuracy:0.3777\n",
    "Epoch #90: Loss:1.0743, Accuracy:0.3836, Validation Loss:1.0781, Validation Accuracy:0.3727\n",
    "Epoch #91: Loss:1.0731, Accuracy:0.3852, Validation Loss:1.0763, Validation Accuracy:0.4039\n",
    "Epoch #92: Loss:1.0718, Accuracy:0.4012, Validation Loss:1.0718, Validation Accuracy:0.3990\n",
    "Epoch #93: Loss:1.0741, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #94: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0764, Validation Accuracy:0.3908\n",
    "Epoch #95: Loss:1.0735, Accuracy:0.4029, Validation Loss:1.0750, Validation Accuracy:0.3892\n",
    "Epoch #96: Loss:1.0723, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.4023\n",
    "Epoch #97: Loss:1.0715, Accuracy:0.4107, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #98: Loss:1.0715, Accuracy:0.4033, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #99: Loss:1.0714, Accuracy:0.4021, Validation Loss:1.0761, Validation Accuracy:0.3695\n",
    "Epoch #100: Loss:1.0718, Accuracy:0.4049, Validation Loss:1.0755, Validation Accuracy:0.3875\n",
    "Epoch #101: Loss:1.0712, Accuracy:0.4041, Validation Loss:1.0754, Validation Accuracy:0.3777\n",
    "Epoch #102: Loss:1.0713, Accuracy:0.4041, Validation Loss:1.0762, Validation Accuracy:0.3760\n",
    "Epoch #103: Loss:1.0709, Accuracy:0.4066, Validation Loss:1.0761, Validation Accuracy:0.3727\n",
    "Epoch #104: Loss:1.0708, Accuracy:0.4090, Validation Loss:1.0764, Validation Accuracy:0.3760\n",
    "Epoch #105: Loss:1.0708, Accuracy:0.4094, Validation Loss:1.0755, Validation Accuracy:0.3744\n",
    "Epoch #106: Loss:1.0711, Accuracy:0.4066, Validation Loss:1.0761, Validation Accuracy:0.3810\n",
    "Epoch #107: Loss:1.0706, Accuracy:0.4074, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #108: Loss:1.0705, Accuracy:0.4111, Validation Loss:1.0780, Validation Accuracy:0.3760\n",
    "Epoch #109: Loss:1.0702, Accuracy:0.4074, Validation Loss:1.0769, Validation Accuracy:0.3596\n",
    "Epoch #110: Loss:1.0707, Accuracy:0.4066, Validation Loss:1.0768, Validation Accuracy:0.3727\n",
    "Epoch #111: Loss:1.0703, Accuracy:0.4012, Validation Loss:1.0758, Validation Accuracy:0.3892\n",
    "Epoch #112: Loss:1.0705, Accuracy:0.4103, Validation Loss:1.0763, Validation Accuracy:0.3908\n",
    "Epoch #113: Loss:1.0702, Accuracy:0.4131, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #114: Loss:1.0706, Accuracy:0.4115, Validation Loss:1.0768, Validation Accuracy:0.3629\n",
    "Epoch #115: Loss:1.0700, Accuracy:0.4012, Validation Loss:1.0765, Validation Accuracy:0.3875\n",
    "Epoch #116: Loss:1.0704, Accuracy:0.4086, Validation Loss:1.0778, Validation Accuracy:0.3793\n",
    "Epoch #117: Loss:1.0703, Accuracy:0.4021, Validation Loss:1.0764, Validation Accuracy:0.3563\n",
    "Epoch #118: Loss:1.0718, Accuracy:0.3918, Validation Loss:1.0770, Validation Accuracy:0.3892\n",
    "Epoch #119: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0776, Validation Accuracy:0.3777\n",
    "Epoch #120: Loss:1.0700, Accuracy:0.4041, Validation Loss:1.0773, Validation Accuracy:0.3826\n",
    "Epoch #121: Loss:1.0696, Accuracy:0.4090, Validation Loss:1.0763, Validation Accuracy:0.3810\n",
    "Epoch #122: Loss:1.0689, Accuracy:0.4115, Validation Loss:1.0757, Validation Accuracy:0.3695\n",
    "Epoch #123: Loss:1.0694, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #124: Loss:1.0695, Accuracy:0.4029, Validation Loss:1.0756, Validation Accuracy:0.3810\n",
    "Epoch #125: Loss:1.0699, Accuracy:0.4053, Validation Loss:1.0766, Validation Accuracy:0.3760\n",
    "Epoch #126: Loss:1.0687, Accuracy:0.4029, Validation Loss:1.0773, Validation Accuracy:0.3810\n",
    "Epoch #127: Loss:1.0691, Accuracy:0.4082, Validation Loss:1.0769, Validation Accuracy:0.3711\n",
    "Epoch #128: Loss:1.0684, Accuracy:0.4057, Validation Loss:1.0765, Validation Accuracy:0.3678\n",
    "Epoch #129: Loss:1.0687, Accuracy:0.4049, Validation Loss:1.0773, Validation Accuracy:0.3678\n",
    "Epoch #130: Loss:1.0698, Accuracy:0.3984, Validation Loss:1.0780, Validation Accuracy:0.3580\n",
    "Epoch #131: Loss:1.0696, Accuracy:0.4070, Validation Loss:1.0770, Validation Accuracy:0.3842\n",
    "Epoch #132: Loss:1.0691, Accuracy:0.4008, Validation Loss:1.0788, Validation Accuracy:0.3924\n",
    "Epoch #133: Loss:1.0674, Accuracy:0.4115, Validation Loss:1.0776, Validation Accuracy:0.3810\n",
    "Epoch #134: Loss:1.0678, Accuracy:0.4066, Validation Loss:1.0769, Validation Accuracy:0.3662\n",
    "Epoch #135: Loss:1.0666, Accuracy:0.4099, Validation Loss:1.0755, Validation Accuracy:0.3547\n",
    "Epoch #136: Loss:1.0677, Accuracy:0.4074, Validation Loss:1.0748, Validation Accuracy:0.3629\n",
    "Epoch #137: Loss:1.0669, Accuracy:0.4053, Validation Loss:1.0771, Validation Accuracy:0.3777\n",
    "Epoch #138: Loss:1.0697, Accuracy:0.3963, Validation Loss:1.0770, Validation Accuracy:0.3662\n",
    "Epoch #139: Loss:1.0719, Accuracy:0.4049, Validation Loss:1.0807, Validation Accuracy:0.3892\n",
    "Epoch #140: Loss:1.0687, Accuracy:0.3938, Validation Loss:1.0778, Validation Accuracy:0.3777\n",
    "Epoch #141: Loss:1.0669, Accuracy:0.4062, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #142: Loss:1.0674, Accuracy:0.4111, Validation Loss:1.0800, Validation Accuracy:0.3629\n",
    "Epoch #143: Loss:1.0668, Accuracy:0.4201, Validation Loss:1.0736, Validation Accuracy:0.4072\n",
    "Epoch #144: Loss:1.0684, Accuracy:0.4029, Validation Loss:1.0816, Validation Accuracy:0.3924\n",
    "Epoch #145: Loss:1.0700, Accuracy:0.4119, Validation Loss:1.0765, Validation Accuracy:0.3974\n",
    "Epoch #146: Loss:1.0720, Accuracy:0.4004, Validation Loss:1.0767, Validation Accuracy:0.4007\n",
    "Epoch #147: Loss:1.0716, Accuracy:0.4062, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #148: Loss:1.0721, Accuracy:0.3836, Validation Loss:1.0761, Validation Accuracy:0.3957\n",
    "Epoch #149: Loss:1.0707, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.3859\n",
    "Epoch #150: Loss:1.0684, Accuracy:0.3996, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #151: Loss:1.0684, Accuracy:0.3926, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #152: Loss:1.0673, Accuracy:0.4016, Validation Loss:1.0751, Validation Accuracy:0.3727\n",
    "Epoch #153: Loss:1.0671, Accuracy:0.4045, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #154: Loss:1.0670, Accuracy:0.3979, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #155: Loss:1.0663, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3645\n",
    "Epoch #156: Loss:1.0654, Accuracy:0.3996, Validation Loss:1.0745, Validation Accuracy:0.3760\n",
    "Epoch #157: Loss:1.0645, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3777\n",
    "Epoch #158: Loss:1.0641, Accuracy:0.4090, Validation Loss:1.0755, Validation Accuracy:0.3612\n",
    "Epoch #159: Loss:1.0647, Accuracy:0.4053, Validation Loss:1.0763, Validation Accuracy:0.3842\n",
    "Epoch #160: Loss:1.0617, Accuracy:0.4119, Validation Loss:1.0759, Validation Accuracy:0.3678\n",
    "Epoch #161: Loss:1.0616, Accuracy:0.4140, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "Epoch #162: Loss:1.0627, Accuracy:0.4037, Validation Loss:1.0770, Validation Accuracy:0.3842\n",
    "Epoch #163: Loss:1.0624, Accuracy:0.4168, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #164: Loss:1.0625, Accuracy:0.4123, Validation Loss:1.0757, Validation Accuracy:0.3957\n",
    "Epoch #165: Loss:1.0618, Accuracy:0.4152, Validation Loss:1.0774, Validation Accuracy:0.3695\n",
    "Epoch #166: Loss:1.0611, Accuracy:0.4074, Validation Loss:1.0790, Validation Accuracy:0.3810\n",
    "Epoch #167: Loss:1.0614, Accuracy:0.4070, Validation Loss:1.0787, Validation Accuracy:0.3695\n",
    "Epoch #168: Loss:1.0612, Accuracy:0.4123, Validation Loss:1.0765, Validation Accuracy:0.3760\n",
    "Epoch #169: Loss:1.0630, Accuracy:0.4131, Validation Loss:1.0794, Validation Accuracy:0.3530\n",
    "Epoch #170: Loss:1.0629, Accuracy:0.4004, Validation Loss:1.0801, Validation Accuracy:0.3875\n",
    "Epoch #171: Loss:1.0623, Accuracy:0.4082, Validation Loss:1.0778, Validation Accuracy:0.3563\n",
    "Epoch #172: Loss:1.0607, Accuracy:0.4090, Validation Loss:1.0837, Validation Accuracy:0.3908\n",
    "Epoch #173: Loss:1.0644, Accuracy:0.4111, Validation Loss:1.0792, Validation Accuracy:0.3727\n",
    "Epoch #174: Loss:1.0690, Accuracy:0.4008, Validation Loss:1.0818, Validation Accuracy:0.3399\n",
    "Epoch #175: Loss:1.0710, Accuracy:0.4012, Validation Loss:1.0863, Validation Accuracy:0.3892\n",
    "Epoch #176: Loss:1.0626, Accuracy:0.4127, Validation Loss:1.0857, Validation Accuracy:0.3760\n",
    "Epoch #177: Loss:1.0670, Accuracy:0.3988, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #178: Loss:1.0631, Accuracy:0.4152, Validation Loss:1.0770, Validation Accuracy:0.3924\n",
    "Epoch #179: Loss:1.0602, Accuracy:0.4205, Validation Loss:1.0767, Validation Accuracy:0.3810\n",
    "Epoch #180: Loss:1.0577, Accuracy:0.4267, Validation Loss:1.0785, Validation Accuracy:0.3777\n",
    "Epoch #181: Loss:1.0592, Accuracy:0.4164, Validation Loss:1.0786, Validation Accuracy:0.3645\n",
    "Epoch #182: Loss:1.0590, Accuracy:0.4107, Validation Loss:1.0779, Validation Accuracy:0.3662\n",
    "Epoch #183: Loss:1.0587, Accuracy:0.4148, Validation Loss:1.0777, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0577, Accuracy:0.4144, Validation Loss:1.0790, Validation Accuracy:0.3810\n",
    "Epoch #185: Loss:1.0573, Accuracy:0.4238, Validation Loss:1.0781, Validation Accuracy:0.3875\n",
    "Epoch #186: Loss:1.0581, Accuracy:0.4148, Validation Loss:1.0813, Validation Accuracy:0.3859\n",
    "Epoch #187: Loss:1.0571, Accuracy:0.4316, Validation Loss:1.0785, Validation Accuracy:0.3941\n",
    "Epoch #188: Loss:1.0571, Accuracy:0.4168, Validation Loss:1.0808, Validation Accuracy:0.3859\n",
    "Epoch #189: Loss:1.0560, Accuracy:0.4275, Validation Loss:1.0795, Validation Accuracy:0.3990\n",
    "Epoch #190: Loss:1.0556, Accuracy:0.4267, Validation Loss:1.0784, Validation Accuracy:0.3892\n",
    "Epoch #191: Loss:1.0542, Accuracy:0.4271, Validation Loss:1.0742, Validation Accuracy:0.3744\n",
    "Epoch #192: Loss:1.0540, Accuracy:0.4251, Validation Loss:1.0759, Validation Accuracy:0.3974\n",
    "Epoch #193: Loss:1.0515, Accuracy:0.4222, Validation Loss:1.0785, Validation Accuracy:0.3974\n",
    "Epoch #194: Loss:1.0499, Accuracy:0.4308, Validation Loss:1.0822, Validation Accuracy:0.3892\n",
    "Epoch #195: Loss:1.0491, Accuracy:0.4312, Validation Loss:1.0774, Validation Accuracy:0.3826\n",
    "Epoch #196: Loss:1.0479, Accuracy:0.4283, Validation Loss:1.0782, Validation Accuracy:0.3924\n",
    "Epoch #197: Loss:1.0483, Accuracy:0.4324, Validation Loss:1.0837, Validation Accuracy:0.4056\n",
    "Epoch #198: Loss:1.0511, Accuracy:0.4304, Validation Loss:1.0947, Validation Accuracy:0.3826\n",
    "Epoch #199: Loss:1.0607, Accuracy:0.4168, Validation Loss:1.0893, Validation Accuracy:0.3777\n",
    "Epoch #200: Loss:1.0551, Accuracy:0.4251, Validation Loss:1.0855, Validation Accuracy:0.3760\n",
    "Epoch #201: Loss:1.0547, Accuracy:0.4308, Validation Loss:1.0851, Validation Accuracy:0.3711\n",
    "Epoch #202: Loss:1.0497, Accuracy:0.4320, Validation Loss:1.0822, Validation Accuracy:0.3924\n",
    "Epoch #203: Loss:1.0468, Accuracy:0.4427, Validation Loss:1.0855, Validation Accuracy:0.3760\n",
    "Epoch #204: Loss:1.0459, Accuracy:0.4444, Validation Loss:1.0863, Validation Accuracy:0.3695\n",
    "Epoch #205: Loss:1.0440, Accuracy:0.4427, Validation Loss:1.0886, Validation Accuracy:0.3711\n",
    "Epoch #206: Loss:1.0443, Accuracy:0.4423, Validation Loss:1.0999, Validation Accuracy:0.3596\n",
    "Epoch #207: Loss:1.0447, Accuracy:0.4452, Validation Loss:1.0913, Validation Accuracy:0.3842\n",
    "Epoch #208: Loss:1.0423, Accuracy:0.4398, Validation Loss:1.1036, Validation Accuracy:0.3744\n",
    "Epoch #209: Loss:1.0398, Accuracy:0.4460, Validation Loss:1.1016, Validation Accuracy:0.3695\n",
    "Epoch #210: Loss:1.0392, Accuracy:0.4534, Validation Loss:1.0969, Validation Accuracy:0.3744\n",
    "Epoch #211: Loss:1.0384, Accuracy:0.4460, Validation Loss:1.1093, Validation Accuracy:0.3727\n",
    "Epoch #212: Loss:1.0427, Accuracy:0.4345, Validation Loss:1.1076, Validation Accuracy:0.3875\n",
    "Epoch #213: Loss:1.0398, Accuracy:0.4456, Validation Loss:1.1097, Validation Accuracy:0.3892\n",
    "Epoch #214: Loss:1.0404, Accuracy:0.4460, Validation Loss:1.1090, Validation Accuracy:0.3859\n",
    "Epoch #215: Loss:1.0449, Accuracy:0.4493, Validation Loss:1.1078, Validation Accuracy:0.3744\n",
    "Epoch #216: Loss:1.0438, Accuracy:0.4468, Validation Loss:1.1029, Validation Accuracy:0.3744\n",
    "Epoch #217: Loss:1.0392, Accuracy:0.4316, Validation Loss:1.1025, Validation Accuracy:0.3777\n",
    "Epoch #218: Loss:1.0443, Accuracy:0.4390, Validation Loss:1.1011, Validation Accuracy:0.3793\n",
    "Epoch #219: Loss:1.0426, Accuracy:0.4402, Validation Loss:1.1014, Validation Accuracy:0.3842\n",
    "Epoch #220: Loss:1.0445, Accuracy:0.4341, Validation Loss:1.1062, Validation Accuracy:0.3826\n",
    "Epoch #221: Loss:1.0417, Accuracy:0.4464, Validation Loss:1.1053, Validation Accuracy:0.3662\n",
    "Epoch #222: Loss:1.0375, Accuracy:0.4370, Validation Loss:1.1037, Validation Accuracy:0.3777\n",
    "Epoch #223: Loss:1.0333, Accuracy:0.4587, Validation Loss:1.1105, Validation Accuracy:0.3842\n",
    "Epoch #224: Loss:1.0321, Accuracy:0.4538, Validation Loss:1.1110, Validation Accuracy:0.3892\n",
    "Epoch #225: Loss:1.0328, Accuracy:0.4472, Validation Loss:1.1179, Validation Accuracy:0.3892\n",
    "Epoch #226: Loss:1.0304, Accuracy:0.4468, Validation Loss:1.1233, Validation Accuracy:0.3810\n",
    "Epoch #227: Loss:1.0330, Accuracy:0.4431, Validation Loss:1.1270, Validation Accuracy:0.3678\n",
    "Epoch #228: Loss:1.0338, Accuracy:0.4427, Validation Loss:1.1217, Validation Accuracy:0.3711\n",
    "Epoch #229: Loss:1.0338, Accuracy:0.4390, Validation Loss:1.1039, Validation Accuracy:0.3924\n",
    "Epoch #230: Loss:1.0396, Accuracy:0.4300, Validation Loss:1.0914, Validation Accuracy:0.4089\n",
    "Epoch #231: Loss:1.0443, Accuracy:0.4201, Validation Loss:1.0976, Validation Accuracy:0.3777\n",
    "Epoch #232: Loss:1.0399, Accuracy:0.4337, Validation Loss:1.1047, Validation Accuracy:0.3892\n",
    "Epoch #233: Loss:1.0424, Accuracy:0.4324, Validation Loss:1.1009, Validation Accuracy:0.3810\n",
    "Epoch #234: Loss:1.0440, Accuracy:0.4283, Validation Loss:1.1032, Validation Accuracy:0.3727\n",
    "Epoch #235: Loss:1.0567, Accuracy:0.4287, Validation Loss:1.1056, Validation Accuracy:0.3892\n",
    "Epoch #236: Loss:1.0520, Accuracy:0.4271, Validation Loss:1.0978, Validation Accuracy:0.3957\n",
    "Epoch #237: Loss:1.0436, Accuracy:0.4345, Validation Loss:1.0945, Validation Accuracy:0.3744\n",
    "Epoch #238: Loss:1.0425, Accuracy:0.4296, Validation Loss:1.0964, Validation Accuracy:0.3727\n",
    "Epoch #239: Loss:1.0387, Accuracy:0.4366, Validation Loss:1.1005, Validation Accuracy:0.3793\n",
    "Epoch #240: Loss:1.0346, Accuracy:0.4337, Validation Loss:1.0990, Validation Accuracy:0.3793\n",
    "Epoch #241: Loss:1.0327, Accuracy:0.4378, Validation Loss:1.0987, Validation Accuracy:0.3793\n",
    "Epoch #242: Loss:1.0296, Accuracy:0.4349, Validation Loss:1.0991, Validation Accuracy:0.3859\n",
    "Epoch #243: Loss:1.0288, Accuracy:0.4357, Validation Loss:1.0996, Validation Accuracy:0.3859\n",
    "Epoch #244: Loss:1.0270, Accuracy:0.4452, Validation Loss:1.1036, Validation Accuracy:0.3957\n",
    "Epoch #245: Loss:1.0271, Accuracy:0.4390, Validation Loss:1.1117, Validation Accuracy:0.3974\n",
    "Epoch #246: Loss:1.0294, Accuracy:0.4366, Validation Loss:1.1241, Validation Accuracy:0.3826\n",
    "Epoch #247: Loss:1.0337, Accuracy:0.4431, Validation Loss:1.1186, Validation Accuracy:0.3826\n",
    "Epoch #248: Loss:1.0261, Accuracy:0.4439, Validation Loss:1.1129, Validation Accuracy:0.3924\n",
    "Epoch #249: Loss:1.0265, Accuracy:0.4509, Validation Loss:1.1102, Validation Accuracy:0.3793\n",
    "Epoch #250: Loss:1.0244, Accuracy:0.4476, Validation Loss:1.1168, Validation Accuracy:0.3760\n",
    "Epoch #251: Loss:1.0241, Accuracy:0.4476, Validation Loss:1.1165, Validation Accuracy:0.3777\n",
    "Epoch #252: Loss:1.0295, Accuracy:0.4485, Validation Loss:1.1163, Validation Accuracy:0.3760\n",
    "Epoch #253: Loss:1.0259, Accuracy:0.4489, Validation Loss:1.1212, Validation Accuracy:0.4056\n",
    "Epoch #254: Loss:1.0265, Accuracy:0.4300, Validation Loss:1.1158, Validation Accuracy:0.3744\n",
    "Epoch #255: Loss:1.0271, Accuracy:0.4493, Validation Loss:1.1125, Validation Accuracy:0.3974\n",
    "Epoch #256: Loss:1.0264, Accuracy:0.4304, Validation Loss:1.1213, Validation Accuracy:0.3924\n",
    "Epoch #257: Loss:1.0254, Accuracy:0.4505, Validation Loss:1.1211, Validation Accuracy:0.3711\n",
    "Epoch #258: Loss:1.0190, Accuracy:0.4476, Validation Loss:1.1224, Validation Accuracy:0.3875\n",
    "Epoch #259: Loss:1.0189, Accuracy:0.4559, Validation Loss:1.1254, Validation Accuracy:0.3777\n",
    "Epoch #260: Loss:1.0222, Accuracy:0.4538, Validation Loss:1.1293, Validation Accuracy:0.3711\n",
    "Epoch #261: Loss:1.0301, Accuracy:0.4419, Validation Loss:1.1302, Validation Accuracy:0.3695\n",
    "Epoch #262: Loss:1.0304, Accuracy:0.4370, Validation Loss:1.1214, Validation Accuracy:0.3695\n",
    "Epoch #263: Loss:1.0276, Accuracy:0.4271, Validation Loss:1.1244, Validation Accuracy:0.3695\n",
    "Epoch #264: Loss:1.0295, Accuracy:0.4427, Validation Loss:1.1244, Validation Accuracy:0.3695\n",
    "Epoch #265: Loss:1.0413, Accuracy:0.4296, Validation Loss:1.1238, Validation Accuracy:0.3793\n",
    "Epoch #266: Loss:1.0418, Accuracy:0.4292, Validation Loss:1.1059, Validation Accuracy:0.3859\n",
    "Epoch #267: Loss:1.0316, Accuracy:0.4415, Validation Loss:1.1069, Validation Accuracy:0.4039\n",
    "Epoch #268: Loss:1.0303, Accuracy:0.4337, Validation Loss:1.1022, Validation Accuracy:0.3908\n",
    "Epoch #269: Loss:1.0278, Accuracy:0.4472, Validation Loss:1.1036, Validation Accuracy:0.3777\n",
    "Epoch #270: Loss:1.0256, Accuracy:0.4493, Validation Loss:1.1074, Validation Accuracy:0.3941\n",
    "Epoch #271: Loss:1.0262, Accuracy:0.4411, Validation Loss:1.1093, Validation Accuracy:0.3941\n",
    "Epoch #272: Loss:1.0238, Accuracy:0.4538, Validation Loss:1.1128, Validation Accuracy:0.3990\n",
    "Epoch #273: Loss:1.0212, Accuracy:0.4538, Validation Loss:1.1187, Validation Accuracy:0.4089\n",
    "Epoch #274: Loss:1.0157, Accuracy:0.4653, Validation Loss:1.1261, Validation Accuracy:0.3744\n",
    "Epoch #275: Loss:1.0147, Accuracy:0.4645, Validation Loss:1.1263, Validation Accuracy:0.3826\n",
    "Epoch #276: Loss:1.0132, Accuracy:0.4694, Validation Loss:1.1359, Validation Accuracy:0.3875\n",
    "Epoch #277: Loss:1.0113, Accuracy:0.4657, Validation Loss:1.1271, Validation Accuracy:0.3875\n",
    "Epoch #278: Loss:1.0161, Accuracy:0.4727, Validation Loss:1.1275, Validation Accuracy:0.3810\n",
    "Epoch #279: Loss:1.0113, Accuracy:0.4595, Validation Loss:1.1255, Validation Accuracy:0.3727\n",
    "Epoch #280: Loss:1.0084, Accuracy:0.4698, Validation Loss:1.1217, Validation Accuracy:0.3957\n",
    "Epoch #281: Loss:1.0072, Accuracy:0.4575, Validation Loss:1.1268, Validation Accuracy:0.3957\n",
    "Epoch #282: Loss:1.0070, Accuracy:0.4715, Validation Loss:1.1432, Validation Accuracy:0.3810\n",
    "Epoch #283: Loss:1.0013, Accuracy:0.4587, Validation Loss:1.1516, Validation Accuracy:0.3727\n",
    "Epoch #284: Loss:1.0037, Accuracy:0.4690, Validation Loss:1.1664, Validation Accuracy:0.3941\n",
    "Epoch #285: Loss:1.0072, Accuracy:0.4723, Validation Loss:1.1529, Validation Accuracy:0.4072\n",
    "Epoch #286: Loss:0.9972, Accuracy:0.4834, Validation Loss:1.1455, Validation Accuracy:0.3908\n",
    "Epoch #287: Loss:0.9981, Accuracy:0.4883, Validation Loss:1.1507, Validation Accuracy:0.3908\n",
    "Epoch #288: Loss:1.0016, Accuracy:0.4702, Validation Loss:1.1455, Validation Accuracy:0.3990\n",
    "Epoch #289: Loss:1.0000, Accuracy:0.4797, Validation Loss:1.1416, Validation Accuracy:0.3957\n",
    "Epoch #290: Loss:1.0015, Accuracy:0.4686, Validation Loss:1.1570, Validation Accuracy:0.3826\n",
    "Epoch #291: Loss:1.0055, Accuracy:0.4608, Validation Loss:1.1531, Validation Accuracy:0.3777\n",
    "Epoch #292: Loss:1.0001, Accuracy:0.4706, Validation Loss:1.1636, Validation Accuracy:0.3810\n",
    "Epoch #293: Loss:0.9911, Accuracy:0.4752, Validation Loss:1.1572, Validation Accuracy:0.3990\n",
    "Epoch #294: Loss:0.9975, Accuracy:0.4875, Validation Loss:1.1457, Validation Accuracy:0.3908\n",
    "Epoch #295: Loss:1.0054, Accuracy:0.4674, Validation Loss:1.1441, Validation Accuracy:0.3924\n",
    "Epoch #296: Loss:1.0062, Accuracy:0.4682, Validation Loss:1.1294, Validation Accuracy:0.4269\n",
    "Epoch #297: Loss:1.0087, Accuracy:0.4567, Validation Loss:1.1420, Validation Accuracy:0.4138\n",
    "Epoch #298: Loss:1.0095, Accuracy:0.4612, Validation Loss:1.1501, Validation Accuracy:0.3892\n",
    "Epoch #299: Loss:1.0155, Accuracy:0.4632, Validation Loss:1.1691, Validation Accuracy:0.3924\n",
    "Epoch #300: Loss:1.0406, Accuracy:0.4534, Validation Loss:1.1378, Validation Accuracy:0.3941\n",
    "\n",
    "Test:\n",
    "Test Loss:1.13775289, Accuracy:0.3941\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  57  145  25\n",
    "t:01  59  167  14\n",
    "t:03  30   96  16\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.39      0.25      0.31       227\n",
    "          01       0.41      0.70      0.52       240\n",
    "          03       0.29      0.11      0.16       142\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.36      0.35      0.33       609\n",
    "weighted avg       0.37      0.39      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 01:38:50 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 40 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0765713517889013, 1.0720764071679076, 1.073056171876065, 1.0727488053060321, 1.0741248293267487, 1.074393367140947, 1.0735722885930479, 1.0742320894999262, 1.0733615210882352, 1.0747106359118508, 1.0749872132083662, 1.0746246153497931, 1.0740713574028955, 1.0746659700114936, 1.075462032812961, 1.0747250257845975, 1.0748479086385767, 1.0755106382965063, 1.073929581344617, 1.0739842870552552, 1.0749456890306646, 1.0742269280704566, 1.0744214300647354, 1.0739356692397144, 1.074048326520497, 1.0732330289380303, 1.0730547401901145, 1.072606023700758, 1.073207579614298, 1.0738886849242086, 1.0738818494752906, 1.0753733138928467, 1.077874149790734, 1.0756867718814043, 1.0763761954158788, 1.0741507783899167, 1.0748783798249093, 1.0760270314067846, 1.0761069720993293, 1.0755832424304757, 1.0761172356472422, 1.0754651635738428, 1.0758143172084014, 1.0755196721682996, 1.0751650863876092, 1.0748850346003065, 1.0746093360074047, 1.0749717463413482, 1.075139187239661, 1.075093735615021, 1.074456347229054, 1.0748604119117624, 1.0744353127597002, 1.074515192770997, 1.0745732694228098, 1.0740662032160266, 1.0745203729920787, 1.0747944555063358, 1.0744026611591209, 1.0741025808409517, 1.074180597546457, 1.073829471575607, 1.073547601895575, 1.073554009835317, 1.0738788036681552, 1.0738243397037774, 1.0736098338426237, 1.073501809281473, 1.077081380806533, 1.0755761345031813, 1.0765655974449195, 1.0753401762550492, 1.075930031062347, 1.075181767662562, 1.0746697340105555, 1.0738080383717328, 1.0734153324355828, 1.0741791398262939, 1.0747661490745732, 1.0747777842144268, 1.073913374166379, 1.0737877661371467, 1.0738765490661897, 1.074353594889586, 1.0735344364138073, 1.0728643048181519, 1.0708982860317762, 1.0747484514865968, 1.076622821623077, 1.078122575686287, 1.076339348196396, 1.071833166387085, 1.0738808294430937, 1.0764161487322528, 1.075032875651405, 1.074907495274724, 1.0749741041014347, 1.0759000895645818, 1.0760593290986686, 1.0754902920699472, 1.0753713793355255, 1.0762056288460793, 1.076067323559415, 1.076445197432695, 1.075478366638835, 1.0760704320052574, 1.0767060394944816, 1.0779943121673634, 1.076864427925135, 1.0768161685204467, 1.0757534476532333, 1.0762587233717218, 1.0760245372117643, 1.0767821715578854, 1.0764764048195825, 1.0778390135866864, 1.0764166079522746, 1.0770322728431088, 1.0776014198810595, 1.077261433812785, 1.0762678717549015, 1.0756581542135655, 1.075311630230232, 1.0755540858740094, 1.0765561200127813, 1.0773101446076567, 1.0768954317362243, 1.0765189521614162, 1.0773298423278508, 1.0780404226729043, 1.0770248301902232, 1.0788463857177835, 1.0776348656229982, 1.0769389022160045, 1.075453066473524, 1.0748437915138032, 1.077124750868636, 1.0770406247359778, 1.0807194541436307, 1.077774743141212, 1.0744494659755812, 1.0799851715075364, 1.0735509988709624, 1.0816160743851184, 1.0764747950048086, 1.0767091068336725, 1.0747495952302404, 1.076122307229316, 1.0760937085488356, 1.0740480473867582, 1.07441524370942, 1.0750907786765513, 1.074083919008377, 1.0742451608278873, 1.073924608027015, 1.0744687451909132, 1.0746050597411658, 1.0754943269814177, 1.0762762001582555, 1.0759055976601462, 1.077981949048285, 1.07702930278966, 1.0753258480422798, 1.0756962882669885, 1.0774241668250173, 1.07896200698389, 1.0787148250539118, 1.0765008061194459, 1.0794338111219735, 1.0801420998690752, 1.0778070974036782, 1.083659949169566, 1.0792307722548937, 1.0818002864057794, 1.086334705940021, 1.0857411926407337, 1.0755744597007488, 1.0769732395807903, 1.0767243834356173, 1.078451941949002, 1.0785605713651685, 1.0778767187607112, 1.0777075627362982, 1.0789820488254815, 1.0781177777570652, 1.0812777787789531, 1.0785274994980134, 1.0808417624832178, 1.079507671553513, 1.0783580453525037, 1.074220129030287, 1.0758889018999924, 1.078523239478689, 1.082169498128844, 1.0773524707565558, 1.07817303645004, 1.0837400155310168, 1.0947179743417574, 1.0893235868225348, 1.085455425462895, 1.0851358388640806, 1.0821905950411592, 1.0855221116092602, 1.0862670913705685, 1.0885651577478168, 1.0999350416640734, 1.0912591526269522, 1.1035624754252693, 1.1016046237475767, 1.0968504991437413, 1.1093293354037557, 1.107564121044328, 1.1097293634132799, 1.109041865823304, 1.1077968745396054, 1.1029279739007183, 1.102468076597881, 1.1011085161826097, 1.101364127716603, 1.1062302291882644, 1.1052850890042158, 1.103742695990063, 1.1105158738119065, 1.1109663880321583, 1.11791159425463, 1.1233467957852117, 1.126968561526394, 1.1217033917876496, 1.103859368020482, 1.0913614897892392, 1.0975976943578234, 1.104747732088875, 1.1008600619038924, 1.1032444448110896, 1.105581668014401, 1.0978104036625578, 1.0944689365443339, 1.0964288243715008, 1.1004943440504653, 1.0990303102972472, 1.0987266691643225, 1.0990900222108086, 1.0996359997782215, 1.1035692617419515, 1.1117255235540455, 1.1240507473890808, 1.1186027771537919, 1.1129190191651017, 1.1102341846096495, 1.1168380233846078, 1.1164858366664017, 1.116331486670646, 1.121245791955143, 1.115767975941863, 1.1125390975933356, 1.1213141683678713, 1.1210803002950984, 1.1223592364729331, 1.125416572262305, 1.1292576551045885, 1.1302073005776492, 1.1213853633266755, 1.1244303579205166, 1.1243990755629265, 1.1238491687868617, 1.1058766922144272, 1.1068760669485884, 1.102176903308123, 1.1036398085858825, 1.1073677990041146, 1.109335306047023, 1.1128230932702377, 1.1187451956503105, 1.1261289383977504, 1.1262922866395346, 1.1358673186920742, 1.1270644701956136, 1.1274780465660033, 1.125471513651079, 1.1217260014247425, 1.1268319376975249, 1.1431880686279197, 1.1516350580162211, 1.166446664846198, 1.1529435823703635, 1.1455283368553826, 1.1506609550642066, 1.1455029055402783, 1.14162116352169, 1.1569938810392357, 1.1531187636511666, 1.1636090507648262, 1.1571938776226074, 1.1457091172536213, 1.1440567657082343, 1.1294362525438832, 1.141994741358389, 1.150131373961375, 1.1690998141988744, 1.1377527619817573], 'val_acc': [0.4236453189736321, 0.41050903075825795, 0.41379310300784744, 0.4154351391326422, 0.4039408856718411, 0.40886699414409833, 0.40886699443771723, 0.41215106580644995, 0.4121510665894338, 0.40065681371587053, 0.40065681361799754, 0.39573070426488355, 0.40394088508460324, 0.38916256064656135, 0.37602627194182237, 0.39080459667348316, 0.3908045958904993, 0.3908045963798642, 0.4006568138137435, 0.4006568135201246, 0.3990147770038379, 0.4105090299752741, 0.41050903026889307, 0.41050903036676606, 0.39573070455850246, 0.4022988494491734, 0.41379310271422853, 0.41215106619794184, 0.39244663211316705, 0.39244663221104, 0.3858784882990989, 0.37931034419141185, 0.39573070455850246, 0.38916256074443434, 0.379310343408428, 0.3908045961841182, 0.3940886683358347, 0.3546798016343798, 0.35467980212374467, 0.40065681283501375, 0.39244663201529406, 0.3596059098130181, 0.36945812695327845, 0.3776683069900143, 0.38752052383665575, 0.3891625598635775, 0.3875205241302747, 0.38752052383665575, 0.3891625598635775, 0.39737274117266214, 0.3973727405854242, 0.3891625598635775, 0.3891625598635775, 0.3891625598635775, 0.3908045964777372, 0.3809523793374768, 0.38095237943534976, 0.3891625600593235, 0.3908045961841182, 0.39244663211316705, 0.3973727405854242, 0.3891625597657046, 0.39573070426488355, 0.39244663201529406, 0.4121510662958148, 0.39244663240678596, 0.3940886685315807, 0.4022988493513004, 0.3940886685315807, 0.3809523793374768, 0.3891625609401803, 0.38095237943534976, 0.3940886682379618, 0.37274219910499495, 0.385878487809734, 0.38752052383665575, 0.40229884954704637, 0.3924466327004049, 0.39080459608624524, 0.3891625597657046, 0.4022988491555544, 0.3908045958904993, 0.3891625597657046, 0.3940886685315807, 0.4006568130307597, 0.3875205240324017, 0.40065681312863266, 0.3940886685315807, 0.3776683070878873, 0.372742199007122, 0.40394088528034916, 0.3990147771995839, 0.4006568138137435, 0.3908045962819912, 0.38916256045081543, 0.40229884974279234, 0.3908045964777372, 0.38752052373878276, 0.36945812685540547, 0.38752052364090983, 0.37766830738150625, 0.37602627106096553, 0.37274219861563007, 0.37602627096309255, 0.37438423513191676, 0.38095237943534976, 0.37931034321268203, 0.37602627096309255, 0.35960590942152615, 0.37274219861563007, 0.3891625600593235, 0.3908045961841182, 0.3825944155601445, 0.3628899822583535, 0.38752052393452874, 0.379310343310555, 0.35632183766130154, 0.3891625599614505, 0.37766830757725217, 0.3825944154622715, 0.3809523793374768, 0.3694581267575325, 0.38095237953322275, 0.3809523793374768, 0.37602627145245743, 0.3809523792396038, 0.3711001627844543, 0.3678160904369918, 0.36781609024124584, 0.35796387359035037, 0.3842364514891933, 0.3924466327982779, 0.3809523791417308, 0.3661740543121971, 0.3546798013407609, 0.3628899818668616, 0.3776683070878873, 0.36617405441007006, 0.3891625599614505, 0.37766830777299815, 0.39080459598837225, 0.3628899817689886, 0.40722495762781163, 0.39244663348338876, 0.39737274146628104, 0.40065681371587053, 0.39408866941243753, 0.39573070475424843, 0.385878487907607, 0.3924466327982779, 0.38587848820122594, 0.37274219920286794, 0.3973727408790432, 0.39080459608624524, 0.36453201828527526, 0.37602627076734657, 0.3776683071857603, 0.36124794583993985, 0.3842364513913203, 0.3678160907306107, 0.37110016249083533, 0.38423645178281224, 0.39244663240678596, 0.39573070446062947, 0.36945812646391357, 0.38095237943534976, 0.36945812665965955, 0.3760262711588385, 0.353037765705331, 0.3875205241302747, 0.3563218379549205, 0.39080459598837225, 0.3727421994964869, 0.33990147670697307, 0.3891625597657046, 0.3760262716482034, 0.38587848839697186, 0.39244663211316705, 0.38095237982684166, 0.37766830777299815, 0.36453201818740233, 0.3661740542143241, 0.39573070475424843, 0.38095237943534976, 0.38752052393452874, 0.38587848800547997, 0.3940886686294537, 0.38587848800547997, 0.39901477651447304, 0.3891625608423073, 0.37438423542553567, 0.39737274146628104, 0.39737274136840806, 0.3891625609401803, 0.3825944160495094, 0.3924466330918968, 0.40558292199238183, 0.3825944154622715, 0.3776683067942683, 0.37602627096309255, 0.3711001626865813, 0.39244663211316705, 0.37602627106096553, 0.36945812646391357, 0.37110016249083533, 0.3596059097151451, 0.38423645168493925, 0.3743842346425518, 0.36945812695327845, 0.3743842346425518, 0.37274219910499495, 0.38752052383665575, 0.3891625595699586, 0.38587848800547997, 0.3743842346425518, 0.3743842346425518, 0.3776683071857603, 0.37931034311480905, 0.38423645119557437, 0.3825944154622715, 0.3661740539207051, 0.3776683069900143, 0.3842364513913203, 0.3891625598635775, 0.3891625595699586, 0.3809523788481119, 0.36781609014337285, 0.3711001626865813, 0.39244663250465894, 0.4088669943398443, 0.3776683068921413, 0.3891625596678316, 0.3809523792396038, 0.37274219861563007, 0.38916256045081543, 0.39573070446062947, 0.3743842346425518, 0.37274219871350306, 0.37931034301693606, 0.3793103439956659, 0.379310343310555, 0.385878487613988, 0.385878487613988, 0.3957307051457404, 0.3973727407811702, 0.3825944155601445, 0.3825944157558905, 0.3924466327004049, 0.37931034321268203, 0.37602627096309255, 0.37766830728363326, 0.3760262711588385, 0.4055829215030169, 0.3743842350340438, 0.3973727406832972, 0.39244663221104, 0.3711001626865813, 0.38752052393452874, 0.37766830728363326, 0.37110016239296234, 0.3694581262681676, 0.36945812665965955, 0.3694581260724216, 0.36945812656178656, 0.37931034301693606, 0.38587848800547997, 0.4039408858675871, 0.3908045961841182, 0.3776683068921413, 0.39408866911881857, 0.39408866892307265, 0.39901477778682176, 0.40886699443771723, 0.37438423513191676, 0.3825944154622715, 0.3875205243260207, 0.3875205245217666, 0.38095237953322275, 0.37274219920286794, 0.3957307049499944, 0.39573070465637544, 0.38095237982684166, 0.37274219871350306, 0.39408866882519966, 0.4072249578235576, 0.3908045961841182, 0.3908045963798642, 0.3990147771017109, 0.39573070475424843, 0.3825944154622715, 0.3776683070878873, 0.38095237982684166, 0.39901477680809194, 0.3908045963798642, 0.39244663230891297, 0.4269293910274756, 0.4137931023227366, 0.3891625600593235, 0.39244663250465894, 0.3940886683358347], 'loss': [1.0810181971203374, 1.0731461364385773, 1.0730312172637093, 1.0729489222689086, 1.0726790646018434, 1.0720038775790643, 1.0734249040086656, 1.0736807725757544, 1.0735469918965803, 1.073173350181423, 1.074427622691317, 1.0731664920979211, 1.0742953930057784, 1.0726136562515822, 1.07284891923595, 1.07285840124565, 1.0728167877549752, 1.0729313993845633, 1.0726417710404132, 1.0725075911202715, 1.0722367758623628, 1.071464165427112, 1.0713516186884542, 1.0722243130819018, 1.0726337169474889, 1.0724085904734335, 1.0721412540706032, 1.072335903943197, 1.0716075417442243, 1.0712188318769544, 1.0714123499222115, 1.0713863175018123, 1.0713902723128301, 1.0728601117888026, 1.072093707873836, 1.0730893560013977, 1.0752112345039477, 1.0747661027575421, 1.0745976677665476, 1.0738689957213352, 1.0747817739568941, 1.0737217323491215, 1.0734099267689354, 1.0734567573183127, 1.0732363443844617, 1.0734182537948327, 1.0734065238944803, 1.073323376859238, 1.0732998042625568, 1.0735215532216693, 1.0731048198696034, 1.0728747281695294, 1.072960390838999, 1.0728494930071508, 1.0728806312568868, 1.0726667952978146, 1.0725710005241254, 1.0726930021505336, 1.0728271427095792, 1.0726685122542803, 1.0727397237715046, 1.0730725300140693, 1.0729278453321673, 1.0726202637752713, 1.0724278392733, 1.0721050959592973, 1.0724503433924681, 1.0723875672420682, 1.0721421833645393, 1.074112419083378, 1.0732378572409158, 1.0736234796610211, 1.0732998660457698, 1.0733639661781107, 1.0724980260802002, 1.0721769463110264, 1.0743826722217535, 1.0730523723841203, 1.0726920986077624, 1.0725407496614867, 1.072184147188551, 1.0720108810636297, 1.072326179745261, 1.0717618574107208, 1.0719509955059576, 1.071956301861475, 1.0717106525902875, 1.0725677220973147, 1.0730431777495868, 1.0742572747706387, 1.0730614551528523, 1.0718330434705197, 1.0740887175840028, 1.073047780696861, 1.0735453516550868, 1.0722652086731834, 1.0715031446617487, 1.0715334961791303, 1.071437153630188, 1.0718174642605949, 1.0711981587830999, 1.0712876930373896, 1.0708601480636752, 1.0707799104204903, 1.0707803131863323, 1.0711243072329606, 1.070562774983275, 1.070537395692704, 1.0702349899241077, 1.0707470994710433, 1.070324355473998, 1.0704983969243889, 1.070216338835213, 1.0706371752388424, 1.0700094390943555, 1.0704007378838636, 1.0702528754298937, 1.0718271457194304, 1.070905501250124, 1.0700401065775502, 1.0696450300529996, 1.0689402803013701, 1.0694068928763607, 1.0695441537813974, 1.0698623437901051, 1.0686934986643233, 1.0690528883826316, 1.0683611692099602, 1.0687052226898852, 1.0697847307585104, 1.0696449786485833, 1.06905388592205, 1.0673500491853123, 1.0677797950023988, 1.0666210463893977, 1.0677406727655712, 1.0669119961697462, 1.069651523263058, 1.0719112908815702, 1.06866563149791, 1.0669484569306735, 1.0674112687120692, 1.0668340057807781, 1.068363736003821, 1.0699808580919459, 1.071957921932855, 1.0715935587148646, 1.0721250244234621, 1.0707443544752053, 1.0683580327082953, 1.0683889604936636, 1.0673410926266618, 1.0670862883268195, 1.0670150884123064, 1.0663086572466935, 1.0654495056160176, 1.064520620663308, 1.0641086134822462, 1.0646669273748535, 1.0616890203781442, 1.0616257276378374, 1.0626816234549459, 1.062368423395333, 1.0625123664338976, 1.061847313669428, 1.0611050145582006, 1.0614218816130558, 1.0612294913317388, 1.0629792646705738, 1.0628705317969196, 1.062313265917972, 1.0606648112714168, 1.0644457632511304, 1.069039025933346, 1.0709776932209178, 1.0625954089468266, 1.0669913008472514, 1.0630777512242906, 1.0601560686158449, 1.0577381450292755, 1.059219949788871, 1.0589994997948837, 1.05868878149154, 1.0577358396635899, 1.0572888918236296, 1.0580716204104728, 1.0571046192298434, 1.0570953855279535, 1.0560095901606754, 1.0556377619443733, 1.054220617870041, 1.054044396529697, 1.0514722771223566, 1.0498849674416764, 1.0491432319676361, 1.047902204515508, 1.0482976854214678, 1.0511143622701908, 1.0607106581850463, 1.0551231745576957, 1.0546660821295861, 1.0496789047605448, 1.0468378353412635, 1.0458525242991517, 1.0440127613118542, 1.0442627181262696, 1.0446624608010482, 1.0423124346155406, 1.0397554142029624, 1.0391832995953256, 1.0384363656171294, 1.0426666856546423, 1.039826154757819, 1.0403964183658545, 1.0449441735015024, 1.0438306778608162, 1.0391860916384437, 1.044275736808777, 1.0426005555863742, 1.0444822774530682, 1.0416813270267276, 1.0375471854846336, 1.033311567903789, 1.032121756532109, 1.0327673487105165, 1.0304462789754847, 1.032970907702828, 1.0338203309253011, 1.0337971963432047, 1.039596746051091, 1.0443127176355287, 1.0398691163170755, 1.0423630406479572, 1.043983392989611, 1.0567435379145815, 1.0519870238627251, 1.043621455423641, 1.0425418237396333, 1.0387278687048251, 1.0346062655321626, 1.0327077043129922, 1.0295730712722215, 1.0288206565551445, 1.0269536494719174, 1.027062835683568, 1.0294319500913365, 1.0336562844517294, 1.0261187590123202, 1.026452752550035, 1.0244134097128679, 1.024065862495062, 1.0295483619525447, 1.0258622089695393, 1.0265101399020249, 1.027118935771057, 1.0264341429273696, 1.0254317509320237, 1.018986950371055, 1.0188595435457797, 1.0221611131143276, 1.0301014044446377, 1.030350874238925, 1.0276294021391037, 1.0295289393078375, 1.0412977857021826, 1.041818402386299, 1.0315632607657808, 1.0303323875462496, 1.0278096910374854, 1.025582592834927, 1.0262372632291037, 1.0238029645943298, 1.0212344365443047, 1.0156671522089589, 1.0146899774578808, 1.0132494720345404, 1.0113070008691087, 1.0161390583617975, 1.0112849433808846, 1.008397577332765, 1.0071866414140627, 1.0070097141686896, 1.0013432714973387, 1.0036926875124232, 1.007219995928496, 0.9971699628741835, 0.9980795169023517, 1.0016492058609057, 0.9999567033573832, 1.0015121205141901, 1.0054698623670935, 1.0000874043979684, 0.9911062857453583, 0.997520134091622, 1.0054175520824455, 1.0061622344982453, 1.0087077878583872, 1.0094701286214087, 1.0155345687141653, 1.040600152231095], 'acc': [0.39507186636787667, 0.3942505123066951, 0.4004106754395017, 0.39917864708929823, 0.40041067638191596, 0.41724845891860474, 0.4127310053523806, 0.40492813119653315, 0.4024640660755933, 0.4094455849953011, 0.3967145776234613, 0.4020533866583689, 0.3987679684920967, 0.4012320334172102, 0.4094455871126735, 0.41642710685240414, 0.4110882968383648, 0.4049281305723367, 0.4102669421897042, 0.40780287569797996, 0.40492813315479664, 0.416837781888014, 0.4213552368984575, 0.4090349085521894, 0.4069815216367984, 0.4012320343963419, 0.40410677592367605, 0.40944558358779926, 0.403696099284738, 0.40903490773216655, 0.41149897226562737, 0.40164270923612544, 0.4057494850251709, 0.4073921984715628, 0.4053388090104293, 0.4016427086486464, 0.3889117042876367, 0.3876796726083853, 0.3843942510763478, 0.3913757711710137, 0.393839833746211, 0.3946611913322668, 0.3954825485266699, 0.401642710447801, 0.40246406368895965, 0.3983572902865478, 0.40205338724584794, 0.4036961004964135, 0.40533880959790836, 0.3967145776234613, 0.4028747456519266, 0.40041067579443695, 0.3999999981763671, 0.40082135521166135, 0.4073921966724082, 0.4061601638549162, 0.4045174539333986, 0.3979466138434361, 0.40164270884447273, 0.40574948858676263, 0.39917864391935926, 0.39671458118505304, 0.3967145770359823, 0.4049281295932049, 0.41149897504391364, 0.41067761784951057, 0.4061601624474144, 0.4069815186626857, 0.4082135505377634, 0.4012320322422521, 0.3934291568747291, 0.38726899538196824, 0.40000000173795885, 0.40041067579443695, 0.39342915808640466, 0.40821355132106885, 0.3967145804017476, 0.38644763717171593, 0.39589322418647627, 0.40082135736575114, 0.4086242301140967, 0.40739219784736636, 0.39999999993880425, 0.40698152144097205, 0.40246406744637775, 0.3971252564532067, 0.3958932247739553, 0.39876796947122845, 0.3926078018711333, 0.3835728972109926, 0.385215605529182, 0.4012320310305766, 0.39753593602953996, 0.4008213532166804, 0.4028747422861612, 0.39425051195175986, 0.41067761605035596, 0.40328542150755925, 0.4020533878700444, 0.40492813315479664, 0.4041067741612389, 0.404106778506136, 0.4065708442145549, 0.4090349079647103, 0.4094455855460627, 0.40657084401872856, 0.4073921968682346, 0.41108829781749656, 0.4073921953016238, 0.40657084343124955, 0.4012320320097083, 0.41026694117385504, 0.41314168512454025, 0.41149897543556635, 0.4012320329888401, 0.4086242283516596, 0.40205339002413426, 0.3917864480424955, 0.39999999899638994, 0.404106777918657, 0.40903490913966845, 0.41149897109066924, 0.4053388074071011, 0.402874745101165, 0.4053388079945801, 0.40287474248198757, 0.40821355370770246, 0.4057494889784153, 0.40492813018068397, 0.3983572911065707, 0.4069815172919013, 0.40082135521166135, 0.4114989742238908, 0.4065708438229022, 0.40985626159752175, 0.4073921976882574, 0.405338809402082, 0.39630390337115684, 0.4049281337789931, 0.3938398339053199, 0.40616016561735335, 0.41108829464755753, 0.42012320244091983, 0.4028747452602739, 0.41190965285780984, 0.4004106755986106, 0.406160164834048, 0.383572894077771, 0.3942505128941742, 0.3995893225165608, 0.3926078030460914, 0.4016427098236045, 0.4045174559283795, 0.39794661208099896, 0.3979466105143882, 0.3995893227491046, 0.4008213563866194, 0.4090349089438421, 0.4053388103812137, 0.4119096508628289, 0.41396303719074085, 0.4036961001047608, 0.41683778286714573, 0.41232032691428794, 0.4151950702407766, 0.4073921967091257, 0.40698152144097205, 0.4123203267184616, 0.4131416845370612, 0.4004106791602023, 0.40821355409935517, 0.4090349087112983, 0.41108829507592765, 0.40082135720664225, 0.40123203083475023, 0.41273100570731586, 0.39876796512633134, 0.41519507086497315, 0.42053388045064233, 0.42669404652084414, 0.41642710329081245, 0.41067761784951057, 0.41478439301435954, 0.41437371582465987, 0.42381930061189543, 0.41478439203522777, 0.4316221775460292, 0.41683778114142606, 0.42751539858704474, 0.4266940471083232, 0.4271047243347403, 0.4250513334661049, 0.4221765924895324, 0.43080082109821405, 0.4312115000870683, 0.4283367571522323, 0.43244353176631967, 0.43039014308849155, 0.416837784470474, 0.4250513354243684, 0.4308008224689985, 0.43203285336494446, 0.44271047025980154, 0.44435318249451794, 0.4427104743721549, 0.4422997953833006, 0.44517453691063474, 0.43983573088655725, 0.44599589433758163, 0.4533880904837066, 0.4459958923793182, 0.43449692204747603, 0.4455852161320328, 0.44599589155929537, 0.4492813153188576, 0.4468172487904159, 0.4316221755510483, 0.43901437306795765, 0.4402464067054725, 0.4340862444294062, 0.44640657254313054, 0.4369609865442194, 0.458726901672704, 0.4537987692767345, 0.4472279238260257, 0.4468172507486794, 0.4431211494811996, 0.44271047065145425, 0.4390143752220475, 0.42997946801616427, 0.4201232050233798, 0.43367556321302725, 0.4324435300038825, 0.4283367567605796, 0.4287474322245596, 0.4271047248855019, 0.4344969202483214, 0.4295687880481783, 0.43655030837538794, 0.433675563445571, 0.4377823394304428, 0.43490759747473856, 0.43572895333507466, 0.4451745367148084, 0.43901437463456844, 0.43655030833867053, 0.443121152186051, 0.4439425072263643, 0.4509240247752877, 0.4476386046140346, 0.44763860402655553, 0.44845995926269516, 0.44887063590163323, 0.42997946605790077, 0.4492813154779665, 0.43039014406762327, 0.45051334754887057, 0.4476386024232273, 0.45585215442968835, 0.45379876575186023, 0.44188911756940447, 0.43696098677676315, 0.4271047217522803, 0.44271047143475967, 0.42956878945568017, 0.42915811262091574, 0.44147843838472384, 0.4336755650121818, 0.4472279283667492, 0.4492813158696192, 0.44106776370404926, 0.4537987684934291, 0.453798768652538, 0.4652977433659947, 0.4644763851924599, 0.46940451974251923, 0.46570841785084294, 0.472689936538007, 0.45954825592971194, 0.46981519716476267, 0.45749486682351365, 0.47145790466292925, 0.45872690049774595, 0.4689938405578386, 0.47227926009489524, 0.48336755460782216, 0.48829568641631266, 0.4702258723961989, 0.4796714574159783, 0.4685831645063796, 0.4607802895672267, 0.47063655103011787, 0.4751540059671265, 0.48747433157182574, 0.4673511310646911, 0.46817248355926183, 0.45667350943328416, 0.46119096679364385, 0.46324435370903483, 0.45338809263779645]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
