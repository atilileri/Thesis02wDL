{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf8.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 06:48:55 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '2', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '03', '01', '04', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000027708E38198>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000277045D6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6079, Accuracy:0.2226, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6062, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6036, Accuracy:0.2341, Validation Loss:1.6014, Validation Accuracy:0.2430\n",
    "Epoch #10: Loss:1.6035, Accuracy:0.2378, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #11: Loss:1.6031, Accuracy:0.2390, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #12: Loss:1.6029, Accuracy:0.2366, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #13: Loss:1.6026, Accuracy:0.2374, Validation Loss:1.6000, Validation Accuracy:0.2430\n",
    "Epoch #14: Loss:1.6021, Accuracy:0.2415, Validation Loss:1.5996, Validation Accuracy:0.2496\n",
    "Epoch #15: Loss:1.6019, Accuracy:0.2411, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #16: Loss:1.6015, Accuracy:0.2407, Validation Loss:1.5990, Validation Accuracy:0.2512\n",
    "Epoch #17: Loss:1.6016, Accuracy:0.2407, Validation Loss:1.5989, Validation Accuracy:0.2512\n",
    "Epoch #18: Loss:1.6007, Accuracy:0.2415, Validation Loss:1.5986, Validation Accuracy:0.2529\n",
    "Epoch #19: Loss:1.6004, Accuracy:0.2423, Validation Loss:1.5985, Validation Accuracy:0.2529\n",
    "Epoch #20: Loss:1.6002, Accuracy:0.2435, Validation Loss:1.5984, Validation Accuracy:0.2545\n",
    "Epoch #21: Loss:1.5999, Accuracy:0.2427, Validation Loss:1.5986, Validation Accuracy:0.2545\n",
    "Epoch #22: Loss:1.5996, Accuracy:0.2423, Validation Loss:1.5986, Validation Accuracy:0.2529\n",
    "Epoch #23: Loss:1.5999, Accuracy:0.2394, Validation Loss:1.5987, Validation Accuracy:0.2562\n",
    "Epoch #24: Loss:1.5994, Accuracy:0.2423, Validation Loss:1.5995, Validation Accuracy:0.2545\n",
    "Epoch #25: Loss:1.5988, Accuracy:0.2427, Validation Loss:1.5994, Validation Accuracy:0.2479\n",
    "Epoch #26: Loss:1.5988, Accuracy:0.2366, Validation Loss:1.5995, Validation Accuracy:0.2545\n",
    "Epoch #27: Loss:1.5981, Accuracy:0.2431, Validation Loss:1.5998, Validation Accuracy:0.2512\n",
    "Epoch #28: Loss:1.5979, Accuracy:0.2456, Validation Loss:1.6003, Validation Accuracy:0.2545\n",
    "Epoch #29: Loss:1.5982, Accuracy:0.2431, Validation Loss:1.6006, Validation Accuracy:0.2529\n",
    "Epoch #30: Loss:1.5975, Accuracy:0.2407, Validation Loss:1.6007, Validation Accuracy:0.2496\n",
    "Epoch #31: Loss:1.5968, Accuracy:0.2337, Validation Loss:1.6011, Validation Accuracy:0.2447\n",
    "Epoch #32: Loss:1.5966, Accuracy:0.2386, Validation Loss:1.6014, Validation Accuracy:0.2447\n",
    "Epoch #33: Loss:1.5965, Accuracy:0.2402, Validation Loss:1.6020, Validation Accuracy:0.2430\n",
    "Epoch #34: Loss:1.5962, Accuracy:0.2419, Validation Loss:1.6030, Validation Accuracy:0.2627\n",
    "Epoch #35: Loss:1.5958, Accuracy:0.2407, Validation Loss:1.6033, Validation Accuracy:0.2496\n",
    "Epoch #36: Loss:1.5953, Accuracy:0.2439, Validation Loss:1.6027, Validation Accuracy:0.2594\n",
    "Epoch #37: Loss:1.5946, Accuracy:0.2505, Validation Loss:1.6026, Validation Accuracy:0.2594\n",
    "Epoch #38: Loss:1.5947, Accuracy:0.2526, Validation Loss:1.6031, Validation Accuracy:0.2381\n",
    "Epoch #39: Loss:1.5944, Accuracy:0.2480, Validation Loss:1.6052, Validation Accuracy:0.2562\n",
    "Epoch #40: Loss:1.5947, Accuracy:0.2407, Validation Loss:1.6062, Validation Accuracy:0.2512\n",
    "Epoch #41: Loss:1.5935, Accuracy:0.2534, Validation Loss:1.6054, Validation Accuracy:0.2365\n",
    "Epoch #42: Loss:1.5930, Accuracy:0.2637, Validation Loss:1.6032, Validation Accuracy:0.2447\n",
    "Epoch #43: Loss:1.5931, Accuracy:0.2587, Validation Loss:1.6042, Validation Accuracy:0.2496\n",
    "Epoch #44: Loss:1.5925, Accuracy:0.2628, Validation Loss:1.6048, Validation Accuracy:0.2414\n",
    "Epoch #45: Loss:1.5914, Accuracy:0.2616, Validation Loss:1.6049, Validation Accuracy:0.2578\n",
    "Epoch #46: Loss:1.5913, Accuracy:0.2632, Validation Loss:1.6051, Validation Accuracy:0.2447\n",
    "Epoch #47: Loss:1.5901, Accuracy:0.2604, Validation Loss:1.6065, Validation Accuracy:0.2594\n",
    "Epoch #48: Loss:1.5907, Accuracy:0.2579, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #49: Loss:1.5933, Accuracy:0.2526, Validation Loss:1.6063, Validation Accuracy:0.2447\n",
    "Epoch #50: Loss:1.5927, Accuracy:0.2571, Validation Loss:1.6067, Validation Accuracy:0.2397\n",
    "Epoch #51: Loss:1.5895, Accuracy:0.2682, Validation Loss:1.6069, Validation Accuracy:0.2463\n",
    "Epoch #52: Loss:1.5878, Accuracy:0.2665, Validation Loss:1.6077, Validation Accuracy:0.2315\n",
    "Epoch #53: Loss:1.5891, Accuracy:0.2600, Validation Loss:1.6068, Validation Accuracy:0.2447\n",
    "Epoch #54: Loss:1.5867, Accuracy:0.2674, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #55: Loss:1.5859, Accuracy:0.2690, Validation Loss:1.6074, Validation Accuracy:0.2463\n",
    "Epoch #56: Loss:1.5842, Accuracy:0.2702, Validation Loss:1.6067, Validation Accuracy:0.2365\n",
    "Epoch #57: Loss:1.5838, Accuracy:0.2694, Validation Loss:1.6091, Validation Accuracy:0.2348\n",
    "Epoch #58: Loss:1.5844, Accuracy:0.2694, Validation Loss:1.6104, Validation Accuracy:0.2397\n",
    "Epoch #59: Loss:1.5826, Accuracy:0.2682, Validation Loss:1.6104, Validation Accuracy:0.2381\n",
    "Epoch #60: Loss:1.5811, Accuracy:0.2661, Validation Loss:1.6131, Validation Accuracy:0.2332\n",
    "Epoch #61: Loss:1.5791, Accuracy:0.2686, Validation Loss:1.6105, Validation Accuracy:0.2381\n",
    "Epoch #62: Loss:1.5803, Accuracy:0.2649, Validation Loss:1.6160, Validation Accuracy:0.2414\n",
    "Epoch #63: Loss:1.5787, Accuracy:0.2756, Validation Loss:1.6131, Validation Accuracy:0.2348\n",
    "Epoch #64: Loss:1.5769, Accuracy:0.2817, Validation Loss:1.6116, Validation Accuracy:0.2397\n",
    "Epoch #65: Loss:1.5745, Accuracy:0.2789, Validation Loss:1.6136, Validation Accuracy:0.2381\n",
    "Epoch #66: Loss:1.5718, Accuracy:0.2809, Validation Loss:1.6147, Validation Accuracy:0.2562\n",
    "Epoch #67: Loss:1.5716, Accuracy:0.2842, Validation Loss:1.6187, Validation Accuracy:0.2430\n",
    "Epoch #68: Loss:1.5737, Accuracy:0.2719, Validation Loss:1.6165, Validation Accuracy:0.2594\n",
    "Epoch #69: Loss:1.5706, Accuracy:0.2834, Validation Loss:1.6230, Validation Accuracy:0.2365\n",
    "Epoch #70: Loss:1.5697, Accuracy:0.2809, Validation Loss:1.6214, Validation Accuracy:0.2545\n",
    "Epoch #71: Loss:1.5633, Accuracy:0.2953, Validation Loss:1.6243, Validation Accuracy:0.2365\n",
    "Epoch #72: Loss:1.5656, Accuracy:0.2871, Validation Loss:1.6237, Validation Accuracy:0.2463\n",
    "Epoch #73: Loss:1.5628, Accuracy:0.2957, Validation Loss:1.6243, Validation Accuracy:0.2348\n",
    "Epoch #74: Loss:1.5588, Accuracy:0.2969, Validation Loss:1.6316, Validation Accuracy:0.2463\n",
    "Epoch #75: Loss:1.5562, Accuracy:0.2990, Validation Loss:1.6354, Validation Accuracy:0.2479\n",
    "Epoch #76: Loss:1.5566, Accuracy:0.2986, Validation Loss:1.6335, Validation Accuracy:0.2479\n",
    "Epoch #77: Loss:1.5511, Accuracy:0.3010, Validation Loss:1.6364, Validation Accuracy:0.2463\n",
    "Epoch #78: Loss:1.5494, Accuracy:0.2998, Validation Loss:1.6490, Validation Accuracy:0.2250\n",
    "Epoch #79: Loss:1.5534, Accuracy:0.2977, Validation Loss:1.6517, Validation Accuracy:0.2282\n",
    "Epoch #80: Loss:1.5579, Accuracy:0.2903, Validation Loss:1.6524, Validation Accuracy:0.2315\n",
    "Epoch #81: Loss:1.5530, Accuracy:0.2969, Validation Loss:1.6398, Validation Accuracy:0.2414\n",
    "Epoch #82: Loss:1.5456, Accuracy:0.3117, Validation Loss:1.6426, Validation Accuracy:0.2332\n",
    "Epoch #83: Loss:1.5449, Accuracy:0.3113, Validation Loss:1.6465, Validation Accuracy:0.2397\n",
    "Epoch #84: Loss:1.5380, Accuracy:0.3203, Validation Loss:1.6587, Validation Accuracy:0.2233\n",
    "Epoch #85: Loss:1.5394, Accuracy:0.3199, Validation Loss:1.6637, Validation Accuracy:0.2266\n",
    "Epoch #86: Loss:1.5438, Accuracy:0.3133, Validation Loss:1.6535, Validation Accuracy:0.2414\n",
    "Epoch #87: Loss:1.5423, Accuracy:0.3097, Validation Loss:1.6676, Validation Accuracy:0.2167\n",
    "Epoch #88: Loss:1.5361, Accuracy:0.3228, Validation Loss:1.6564, Validation Accuracy:0.2282\n",
    "Epoch #89: Loss:1.5331, Accuracy:0.3261, Validation Loss:1.6788, Validation Accuracy:0.2233\n",
    "Epoch #90: Loss:1.5396, Accuracy:0.3228, Validation Loss:1.6577, Validation Accuracy:0.2315\n",
    "Epoch #91: Loss:1.5301, Accuracy:0.3322, Validation Loss:1.6649, Validation Accuracy:0.2266\n",
    "Epoch #92: Loss:1.5257, Accuracy:0.3298, Validation Loss:1.6749, Validation Accuracy:0.2233\n",
    "Epoch #93: Loss:1.5210, Accuracy:0.3474, Validation Loss:1.6805, Validation Accuracy:0.2315\n",
    "Epoch #94: Loss:1.5205, Accuracy:0.3368, Validation Loss:1.6861, Validation Accuracy:0.2167\n",
    "Epoch #95: Loss:1.5151, Accuracy:0.3368, Validation Loss:1.6902, Validation Accuracy:0.2217\n",
    "Epoch #96: Loss:1.5173, Accuracy:0.3409, Validation Loss:1.6998, Validation Accuracy:0.2266\n",
    "Epoch #97: Loss:1.5219, Accuracy:0.3322, Validation Loss:1.6999, Validation Accuracy:0.2250\n",
    "Epoch #98: Loss:1.5220, Accuracy:0.3281, Validation Loss:1.7004, Validation Accuracy:0.2282\n",
    "Epoch #99: Loss:1.5150, Accuracy:0.3368, Validation Loss:1.6962, Validation Accuracy:0.2184\n",
    "Epoch #100: Loss:1.5108, Accuracy:0.3458, Validation Loss:1.6934, Validation Accuracy:0.2118\n",
    "Epoch #101: Loss:1.5093, Accuracy:0.3532, Validation Loss:1.7013, Validation Accuracy:0.2151\n",
    "Epoch #102: Loss:1.5056, Accuracy:0.3470, Validation Loss:1.7036, Validation Accuracy:0.2167\n",
    "Epoch #103: Loss:1.5019, Accuracy:0.3487, Validation Loss:1.7209, Validation Accuracy:0.2217\n",
    "Epoch #104: Loss:1.5037, Accuracy:0.3450, Validation Loss:1.7089, Validation Accuracy:0.2118\n",
    "Epoch #105: Loss:1.5026, Accuracy:0.3413, Validation Loss:1.7133, Validation Accuracy:0.2102\n",
    "Epoch #106: Loss:1.4998, Accuracy:0.3544, Validation Loss:1.7343, Validation Accuracy:0.2266\n",
    "Epoch #107: Loss:1.5030, Accuracy:0.3544, Validation Loss:1.7150, Validation Accuracy:0.2069\n",
    "Epoch #108: Loss:1.4921, Accuracy:0.3598, Validation Loss:1.7433, Validation Accuracy:0.2184\n",
    "Epoch #109: Loss:1.5025, Accuracy:0.3417, Validation Loss:1.7283, Validation Accuracy:0.2020\n",
    "Epoch #110: Loss:1.4932, Accuracy:0.3540, Validation Loss:1.7228, Validation Accuracy:0.2217\n",
    "Epoch #111: Loss:1.4943, Accuracy:0.3520, Validation Loss:1.7295, Validation Accuracy:0.2151\n",
    "Epoch #112: Loss:1.4945, Accuracy:0.3499, Validation Loss:1.7253, Validation Accuracy:0.2085\n",
    "Epoch #113: Loss:1.4877, Accuracy:0.3676, Validation Loss:1.7481, Validation Accuracy:0.2184\n",
    "Epoch #114: Loss:1.4796, Accuracy:0.3655, Validation Loss:1.7331, Validation Accuracy:0.2217\n",
    "Epoch #115: Loss:1.4710, Accuracy:0.3766, Validation Loss:1.7547, Validation Accuracy:0.2102\n",
    "Epoch #116: Loss:1.4788, Accuracy:0.3536, Validation Loss:1.7476, Validation Accuracy:0.2184\n",
    "Epoch #117: Loss:1.4767, Accuracy:0.3647, Validation Loss:1.7470, Validation Accuracy:0.2217\n",
    "Epoch #118: Loss:1.4804, Accuracy:0.3655, Validation Loss:1.7773, Validation Accuracy:0.2118\n",
    "Epoch #119: Loss:1.4685, Accuracy:0.3676, Validation Loss:1.7464, Validation Accuracy:0.2167\n",
    "Epoch #120: Loss:1.4632, Accuracy:0.3819, Validation Loss:1.7626, Validation Accuracy:0.2102\n",
    "Epoch #121: Loss:1.4595, Accuracy:0.3762, Validation Loss:1.7692, Validation Accuracy:0.2053\n",
    "Epoch #122: Loss:1.4730, Accuracy:0.3725, Validation Loss:1.7685, Validation Accuracy:0.2151\n",
    "Epoch #123: Loss:1.4636, Accuracy:0.3791, Validation Loss:1.7634, Validation Accuracy:0.2053\n",
    "Epoch #124: Loss:1.4514, Accuracy:0.3836, Validation Loss:1.7830, Validation Accuracy:0.2069\n",
    "Epoch #125: Loss:1.4495, Accuracy:0.3869, Validation Loss:1.7639, Validation Accuracy:0.2167\n",
    "Epoch #126: Loss:1.4459, Accuracy:0.3897, Validation Loss:1.7836, Validation Accuracy:0.2118\n",
    "Epoch #127: Loss:1.4551, Accuracy:0.3852, Validation Loss:1.7813, Validation Accuracy:0.2151\n",
    "Epoch #128: Loss:1.4449, Accuracy:0.3836, Validation Loss:1.8022, Validation Accuracy:0.2167\n",
    "Epoch #129: Loss:1.4357, Accuracy:0.3947, Validation Loss:1.8027, Validation Accuracy:0.1872\n",
    "Epoch #130: Loss:1.4467, Accuracy:0.3766, Validation Loss:1.8087, Validation Accuracy:0.2233\n",
    "Epoch #131: Loss:1.4380, Accuracy:0.3979, Validation Loss:1.8066, Validation Accuracy:0.2266\n",
    "Epoch #132: Loss:1.4294, Accuracy:0.3988, Validation Loss:1.8111, Validation Accuracy:0.2151\n",
    "Epoch #133: Loss:1.4320, Accuracy:0.3975, Validation Loss:1.7965, Validation Accuracy:0.2135\n",
    "Epoch #134: Loss:1.4299, Accuracy:0.3930, Validation Loss:1.8066, Validation Accuracy:0.2167\n",
    "Epoch #135: Loss:1.4274, Accuracy:0.4004, Validation Loss:1.8028, Validation Accuracy:0.2102\n",
    "Epoch #136: Loss:1.4244, Accuracy:0.4029, Validation Loss:1.8244, Validation Accuracy:0.2200\n",
    "Epoch #137: Loss:1.4183, Accuracy:0.4111, Validation Loss:1.8240, Validation Accuracy:0.2200\n",
    "Epoch #138: Loss:1.4149, Accuracy:0.4094, Validation Loss:1.8293, Validation Accuracy:0.2233\n",
    "Epoch #139: Loss:1.4069, Accuracy:0.4168, Validation Loss:1.8334, Validation Accuracy:0.2184\n",
    "Epoch #140: Loss:1.4110, Accuracy:0.4172, Validation Loss:1.8162, Validation Accuracy:0.2151\n",
    "Epoch #141: Loss:1.4097, Accuracy:0.4119, Validation Loss:1.8367, Validation Accuracy:0.2053\n",
    "Epoch #142: Loss:1.4136, Accuracy:0.4049, Validation Loss:1.8318, Validation Accuracy:0.2102\n",
    "Epoch #143: Loss:1.4056, Accuracy:0.4049, Validation Loss:1.8544, Validation Accuracy:0.2167\n",
    "Epoch #144: Loss:1.4063, Accuracy:0.4111, Validation Loss:1.8533, Validation Accuracy:0.2085\n",
    "Epoch #145: Loss:1.4001, Accuracy:0.4201, Validation Loss:1.8234, Validation Accuracy:0.2151\n",
    "Epoch #146: Loss:1.4022, Accuracy:0.4107, Validation Loss:1.8478, Validation Accuracy:0.1987\n",
    "Epoch #147: Loss:1.3946, Accuracy:0.4115, Validation Loss:1.8748, Validation Accuracy:0.2053\n",
    "Epoch #148: Loss:1.3967, Accuracy:0.4251, Validation Loss:1.8705, Validation Accuracy:0.2118\n",
    "Epoch #149: Loss:1.3832, Accuracy:0.4283, Validation Loss:1.8487, Validation Accuracy:0.1970\n",
    "Epoch #150: Loss:1.3883, Accuracy:0.4234, Validation Loss:1.8369, Validation Accuracy:0.2036\n",
    "Epoch #151: Loss:1.3990, Accuracy:0.4127, Validation Loss:1.8790, Validation Accuracy:0.1954\n",
    "Epoch #152: Loss:1.3889, Accuracy:0.4152, Validation Loss:1.9147, Validation Accuracy:0.2102\n",
    "Epoch #153: Loss:1.3908, Accuracy:0.4168, Validation Loss:1.8802, Validation Accuracy:0.1921\n",
    "Epoch #154: Loss:1.3801, Accuracy:0.4279, Validation Loss:1.8742, Validation Accuracy:0.2085\n",
    "Epoch #155: Loss:1.3695, Accuracy:0.4374, Validation Loss:1.8684, Validation Accuracy:0.2135\n",
    "Epoch #156: Loss:1.3664, Accuracy:0.4345, Validation Loss:1.8865, Validation Accuracy:0.2020\n",
    "Epoch #157: Loss:1.3736, Accuracy:0.4218, Validation Loss:1.8727, Validation Accuracy:0.2151\n",
    "Epoch #158: Loss:1.3900, Accuracy:0.4218, Validation Loss:1.8639, Validation Accuracy:0.2118\n",
    "Epoch #159: Loss:1.4078, Accuracy:0.4004, Validation Loss:1.8393, Validation Accuracy:0.1970\n",
    "Epoch #160: Loss:1.3890, Accuracy:0.4078, Validation Loss:1.8760, Validation Accuracy:0.2184\n",
    "Epoch #161: Loss:1.3719, Accuracy:0.4292, Validation Loss:1.8654, Validation Accuracy:0.2135\n",
    "Epoch #162: Loss:1.3587, Accuracy:0.4353, Validation Loss:1.8665, Validation Accuracy:0.2118\n",
    "Epoch #163: Loss:1.3565, Accuracy:0.4390, Validation Loss:1.8529, Validation Accuracy:0.2069\n",
    "Epoch #164: Loss:1.3579, Accuracy:0.4407, Validation Loss:1.8850, Validation Accuracy:0.2036\n",
    "Epoch #165: Loss:1.3531, Accuracy:0.4386, Validation Loss:1.9064, Validation Accuracy:0.2053\n",
    "Epoch #166: Loss:1.3554, Accuracy:0.4390, Validation Loss:1.9204, Validation Accuracy:0.2085\n",
    "Epoch #167: Loss:1.3488, Accuracy:0.4501, Validation Loss:1.8951, Validation Accuracy:0.1905\n",
    "Epoch #168: Loss:1.3406, Accuracy:0.4554, Validation Loss:1.8997, Validation Accuracy:0.2135\n",
    "Epoch #169: Loss:1.3371, Accuracy:0.4452, Validation Loss:1.8947, Validation Accuracy:0.1954\n",
    "Epoch #170: Loss:1.3365, Accuracy:0.4489, Validation Loss:1.9354, Validation Accuracy:0.2102\n",
    "Epoch #171: Loss:1.3434, Accuracy:0.4411, Validation Loss:1.9475, Validation Accuracy:0.1823\n",
    "Epoch #172: Loss:1.3376, Accuracy:0.4546, Validation Loss:1.9629, Validation Accuracy:0.2003\n",
    "Epoch #173: Loss:1.3297, Accuracy:0.4579, Validation Loss:1.9212, Validation Accuracy:0.2069\n",
    "Epoch #174: Loss:1.3271, Accuracy:0.4604, Validation Loss:1.9309, Validation Accuracy:0.1921\n",
    "Epoch #175: Loss:1.3330, Accuracy:0.4546, Validation Loss:1.9450, Validation Accuracy:0.2053\n",
    "Epoch #176: Loss:1.3309, Accuracy:0.4546, Validation Loss:1.9592, Validation Accuracy:0.1938\n",
    "Epoch #177: Loss:1.3206, Accuracy:0.4600, Validation Loss:1.9543, Validation Accuracy:0.2053\n",
    "Epoch #178: Loss:1.3190, Accuracy:0.4563, Validation Loss:1.9117, Validation Accuracy:0.2069\n",
    "Epoch #179: Loss:1.3178, Accuracy:0.4669, Validation Loss:1.9311, Validation Accuracy:0.1921\n",
    "Epoch #180: Loss:1.3079, Accuracy:0.4743, Validation Loss:1.9629, Validation Accuracy:0.2036\n",
    "Epoch #181: Loss:1.3131, Accuracy:0.4715, Validation Loss:1.9918, Validation Accuracy:0.1856\n",
    "Epoch #182: Loss:1.3162, Accuracy:0.4628, Validation Loss:1.9518, Validation Accuracy:0.2020\n",
    "Epoch #183: Loss:1.3077, Accuracy:0.4645, Validation Loss:1.9439, Validation Accuracy:0.1954\n",
    "Epoch #184: Loss:1.3040, Accuracy:0.4694, Validation Loss:1.9682, Validation Accuracy:0.2020\n",
    "Epoch #185: Loss:1.2927, Accuracy:0.4784, Validation Loss:1.9638, Validation Accuracy:0.1872\n",
    "Epoch #186: Loss:1.2985, Accuracy:0.4780, Validation Loss:1.9582, Validation Accuracy:0.1921\n",
    "Epoch #187: Loss:1.3061, Accuracy:0.4657, Validation Loss:1.9880, Validation Accuracy:0.1970\n",
    "Epoch #188: Loss:1.2891, Accuracy:0.4784, Validation Loss:2.0120, Validation Accuracy:0.1823\n",
    "Epoch #189: Loss:1.2885, Accuracy:0.4797, Validation Loss:1.9962, Validation Accuracy:0.1987\n",
    "Epoch #190: Loss:1.2903, Accuracy:0.4830, Validation Loss:1.9866, Validation Accuracy:0.2020\n",
    "Epoch #191: Loss:1.2794, Accuracy:0.4850, Validation Loss:1.9896, Validation Accuracy:0.1954\n",
    "Epoch #192: Loss:1.2874, Accuracy:0.4793, Validation Loss:1.9727, Validation Accuracy:0.2085\n",
    "Epoch #193: Loss:1.2782, Accuracy:0.4846, Validation Loss:2.0129, Validation Accuracy:0.2069\n",
    "Epoch #194: Loss:1.2793, Accuracy:0.4817, Validation Loss:2.0036, Validation Accuracy:0.1905\n",
    "Epoch #195: Loss:1.2694, Accuracy:0.4875, Validation Loss:2.0245, Validation Accuracy:0.2003\n",
    "Epoch #196: Loss:1.2622, Accuracy:0.5051, Validation Loss:2.0153, Validation Accuracy:0.1888\n",
    "Epoch #197: Loss:1.2641, Accuracy:0.4977, Validation Loss:2.0266, Validation Accuracy:0.1905\n",
    "Epoch #198: Loss:1.2607, Accuracy:0.4945, Validation Loss:2.0278, Validation Accuracy:0.2102\n",
    "Epoch #199: Loss:1.2527, Accuracy:0.5018, Validation Loss:2.0391, Validation Accuracy:0.1905\n",
    "Epoch #200: Loss:1.2679, Accuracy:0.4953, Validation Loss:2.0892, Validation Accuracy:0.1938\n",
    "Epoch #201: Loss:1.3036, Accuracy:0.4715, Validation Loss:2.0406, Validation Accuracy:0.1856\n",
    "Epoch #202: Loss:1.3043, Accuracy:0.4657, Validation Loss:2.0120, Validation Accuracy:0.2069\n",
    "Epoch #203: Loss:1.2959, Accuracy:0.4669, Validation Loss:1.9658, Validation Accuracy:0.1888\n",
    "Epoch #204: Loss:1.2818, Accuracy:0.4768, Validation Loss:1.9683, Validation Accuracy:0.2036\n",
    "Epoch #205: Loss:1.2804, Accuracy:0.4739, Validation Loss:1.9880, Validation Accuracy:0.1872\n",
    "Epoch #206: Loss:1.2564, Accuracy:0.5010, Validation Loss:2.0273, Validation Accuracy:0.1856\n",
    "Epoch #207: Loss:1.2492, Accuracy:0.5076, Validation Loss:2.0412, Validation Accuracy:0.2003\n",
    "Epoch #208: Loss:1.2400, Accuracy:0.5162, Validation Loss:2.0311, Validation Accuracy:0.1938\n",
    "Epoch #209: Loss:1.2325, Accuracy:0.5158, Validation Loss:2.0368, Validation Accuracy:0.1970\n",
    "Epoch #210: Loss:1.2402, Accuracy:0.5121, Validation Loss:2.0203, Validation Accuracy:0.1856\n",
    "Epoch #211: Loss:1.2336, Accuracy:0.5133, Validation Loss:2.0595, Validation Accuracy:0.1954\n",
    "Epoch #212: Loss:1.2354, Accuracy:0.5076, Validation Loss:2.0492, Validation Accuracy:0.2003\n",
    "Epoch #213: Loss:1.2319, Accuracy:0.5125, Validation Loss:2.0469, Validation Accuracy:0.1856\n",
    "Epoch #214: Loss:1.2211, Accuracy:0.5203, Validation Loss:2.0990, Validation Accuracy:0.1970\n",
    "Epoch #215: Loss:1.2206, Accuracy:0.5166, Validation Loss:2.0731, Validation Accuracy:0.1987\n",
    "Epoch #216: Loss:1.2241, Accuracy:0.5175, Validation Loss:2.0953, Validation Accuracy:0.1987\n",
    "Epoch #217: Loss:1.2289, Accuracy:0.5060, Validation Loss:2.1099, Validation Accuracy:0.1888\n",
    "Epoch #218: Loss:1.2196, Accuracy:0.5142, Validation Loss:2.1159, Validation Accuracy:0.1823\n",
    "Epoch #219: Loss:1.2155, Accuracy:0.5211, Validation Loss:2.1114, Validation Accuracy:0.2020\n",
    "Epoch #220: Loss:1.2229, Accuracy:0.5281, Validation Loss:2.1136, Validation Accuracy:0.2003\n",
    "Epoch #221: Loss:1.2186, Accuracy:0.5211, Validation Loss:2.1408, Validation Accuracy:0.1938\n",
    "Epoch #222: Loss:1.2268, Accuracy:0.5121, Validation Loss:2.1164, Validation Accuracy:0.1905\n",
    "Epoch #223: Loss:1.2103, Accuracy:0.5244, Validation Loss:2.0650, Validation Accuracy:0.1970\n",
    "Epoch #224: Loss:1.2045, Accuracy:0.5335, Validation Loss:2.0916, Validation Accuracy:0.1987\n",
    "Epoch #225: Loss:1.2196, Accuracy:0.5199, Validation Loss:2.0950, Validation Accuracy:0.1987\n",
    "Epoch #226: Loss:1.2200, Accuracy:0.5285, Validation Loss:2.1442, Validation Accuracy:0.1888\n",
    "Epoch #227: Loss:1.2035, Accuracy:0.5302, Validation Loss:2.1398, Validation Accuracy:0.1954\n",
    "Epoch #228: Loss:1.2100, Accuracy:0.5294, Validation Loss:2.1155, Validation Accuracy:0.1839\n",
    "Epoch #229: Loss:1.1917, Accuracy:0.5368, Validation Loss:2.0960, Validation Accuracy:0.1905\n",
    "Epoch #230: Loss:1.1916, Accuracy:0.5372, Validation Loss:2.0949, Validation Accuracy:0.1938\n",
    "Epoch #231: Loss:1.2009, Accuracy:0.5265, Validation Loss:2.1510, Validation Accuracy:0.1790\n",
    "Epoch #232: Loss:1.1900, Accuracy:0.5290, Validation Loss:2.1327, Validation Accuracy:0.1987\n",
    "Epoch #233: Loss:1.1873, Accuracy:0.5335, Validation Loss:2.1640, Validation Accuracy:0.1954\n",
    "Epoch #234: Loss:1.1938, Accuracy:0.5298, Validation Loss:2.1936, Validation Accuracy:0.1839\n",
    "Epoch #235: Loss:1.2061, Accuracy:0.5203, Validation Loss:2.1625, Validation Accuracy:0.1823\n",
    "Epoch #236: Loss:1.1911, Accuracy:0.5281, Validation Loss:2.1322, Validation Accuracy:0.1790\n",
    "Epoch #237: Loss:1.1661, Accuracy:0.5454, Validation Loss:2.1230, Validation Accuracy:0.1970\n",
    "Epoch #238: Loss:1.1683, Accuracy:0.5458, Validation Loss:2.1459, Validation Accuracy:0.1839\n",
    "Epoch #239: Loss:1.1752, Accuracy:0.5409, Validation Loss:2.1484, Validation Accuracy:0.1905\n",
    "Epoch #240: Loss:1.1769, Accuracy:0.5446, Validation Loss:2.1662, Validation Accuracy:0.1856\n",
    "Epoch #241: Loss:1.1587, Accuracy:0.5598, Validation Loss:2.1633, Validation Accuracy:0.1823\n",
    "Epoch #242: Loss:1.1498, Accuracy:0.5618, Validation Loss:2.1880, Validation Accuracy:0.1856\n",
    "Epoch #243: Loss:1.1560, Accuracy:0.5507, Validation Loss:2.1527, Validation Accuracy:0.1954\n",
    "Epoch #244: Loss:1.1468, Accuracy:0.5602, Validation Loss:2.1609, Validation Accuracy:0.1806\n",
    "Epoch #245: Loss:1.1486, Accuracy:0.5684, Validation Loss:2.1938, Validation Accuracy:0.1921\n",
    "Epoch #246: Loss:1.1555, Accuracy:0.5466, Validation Loss:2.1891, Validation Accuracy:0.1872\n",
    "Epoch #247: Loss:1.1446, Accuracy:0.5532, Validation Loss:2.1846, Validation Accuracy:0.1823\n",
    "Epoch #248: Loss:1.1522, Accuracy:0.5495, Validation Loss:2.1868, Validation Accuracy:0.1938\n",
    "Epoch #249: Loss:1.1378, Accuracy:0.5704, Validation Loss:2.2252, Validation Accuracy:0.1905\n",
    "Epoch #250: Loss:1.1381, Accuracy:0.5610, Validation Loss:2.1859, Validation Accuracy:0.1872\n",
    "Epoch #251: Loss:1.1423, Accuracy:0.5573, Validation Loss:2.2250, Validation Accuracy:0.1938\n",
    "Epoch #252: Loss:1.1443, Accuracy:0.5610, Validation Loss:2.2354, Validation Accuracy:0.1691\n",
    "Epoch #253: Loss:1.1393, Accuracy:0.5556, Validation Loss:2.2079, Validation Accuracy:0.1905\n",
    "Epoch #254: Loss:1.1353, Accuracy:0.5663, Validation Loss:2.1992, Validation Accuracy:0.1888\n",
    "Epoch #255: Loss:1.1257, Accuracy:0.5639, Validation Loss:2.2349, Validation Accuracy:0.1806\n",
    "Epoch #256: Loss:1.1273, Accuracy:0.5696, Validation Loss:2.2118, Validation Accuracy:0.1823\n",
    "Epoch #257: Loss:1.1189, Accuracy:0.5791, Validation Loss:2.2234, Validation Accuracy:0.1970\n",
    "Epoch #258: Loss:1.1385, Accuracy:0.5598, Validation Loss:2.2333, Validation Accuracy:0.1856\n",
    "Epoch #259: Loss:1.1296, Accuracy:0.5655, Validation Loss:2.2876, Validation Accuracy:0.1905\n",
    "Epoch #260: Loss:1.1301, Accuracy:0.5610, Validation Loss:2.2289, Validation Accuracy:0.1938\n",
    "Epoch #261: Loss:1.1263, Accuracy:0.5610, Validation Loss:2.2516, Validation Accuracy:0.1757\n",
    "Epoch #262: Loss:1.1064, Accuracy:0.5733, Validation Loss:2.2516, Validation Accuracy:0.1872\n",
    "Epoch #263: Loss:1.1079, Accuracy:0.5754, Validation Loss:2.2335, Validation Accuracy:0.1823\n",
    "Epoch #264: Loss:1.1072, Accuracy:0.5778, Validation Loss:2.2528, Validation Accuracy:0.1872\n",
    "Epoch #265: Loss:1.1000, Accuracy:0.5803, Validation Loss:2.2398, Validation Accuracy:0.1856\n",
    "Epoch #266: Loss:1.0920, Accuracy:0.5914, Validation Loss:2.2495, Validation Accuracy:0.1790\n",
    "Epoch #267: Loss:1.1008, Accuracy:0.5725, Validation Loss:2.2472, Validation Accuracy:0.1872\n",
    "Epoch #268: Loss:1.0928, Accuracy:0.5852, Validation Loss:2.2574, Validation Accuracy:0.1872\n",
    "Epoch #269: Loss:1.1089, Accuracy:0.5840, Validation Loss:2.2321, Validation Accuracy:0.1741\n",
    "Epoch #270: Loss:1.1083, Accuracy:0.5708, Validation Loss:2.2611, Validation Accuracy:0.1839\n",
    "Epoch #271: Loss:1.0951, Accuracy:0.5844, Validation Loss:2.2647, Validation Accuracy:0.1773\n",
    "Epoch #272: Loss:1.0815, Accuracy:0.5864, Validation Loss:2.2691, Validation Accuracy:0.1806\n",
    "Epoch #273: Loss:1.0838, Accuracy:0.5910, Validation Loss:2.2839, Validation Accuracy:0.1806\n",
    "Epoch #274: Loss:1.0807, Accuracy:0.5877, Validation Loss:2.2792, Validation Accuracy:0.1823\n",
    "Epoch #275: Loss:1.0805, Accuracy:0.5943, Validation Loss:2.2868, Validation Accuracy:0.1856\n",
    "Epoch #276: Loss:1.1117, Accuracy:0.5708, Validation Loss:2.2337, Validation Accuracy:0.1823\n",
    "Epoch #277: Loss:1.1208, Accuracy:0.5610, Validation Loss:2.3139, Validation Accuracy:0.1823\n",
    "Epoch #278: Loss:1.1172, Accuracy:0.5737, Validation Loss:2.3014, Validation Accuracy:0.1773\n",
    "Epoch #279: Loss:1.0821, Accuracy:0.5885, Validation Loss:2.2500, Validation Accuracy:0.1839\n",
    "Epoch #280: Loss:1.0867, Accuracy:0.5856, Validation Loss:2.2623, Validation Accuracy:0.1856\n",
    "Epoch #281: Loss:1.0759, Accuracy:0.5918, Validation Loss:2.3043, Validation Accuracy:0.1691\n",
    "Epoch #282: Loss:1.0685, Accuracy:0.5877, Validation Loss:2.3084, Validation Accuracy:0.1888\n",
    "Epoch #283: Loss:1.0586, Accuracy:0.5975, Validation Loss:2.3056, Validation Accuracy:0.1806\n",
    "Epoch #284: Loss:1.0595, Accuracy:0.5947, Validation Loss:2.3532, Validation Accuracy:0.1872\n",
    "Epoch #285: Loss:1.0674, Accuracy:0.5922, Validation Loss:2.2980, Validation Accuracy:0.1806\n",
    "Epoch #286: Loss:1.0655, Accuracy:0.5893, Validation Loss:2.3166, Validation Accuracy:0.1757\n",
    "Epoch #287: Loss:1.0564, Accuracy:0.6004, Validation Loss:2.3279, Validation Accuracy:0.1823\n",
    "Epoch #288: Loss:1.0512, Accuracy:0.6012, Validation Loss:2.3225, Validation Accuracy:0.1888\n",
    "Epoch #289: Loss:1.0579, Accuracy:0.5930, Validation Loss:2.3385, Validation Accuracy:0.1741\n",
    "Epoch #290: Loss:1.0797, Accuracy:0.5766, Validation Loss:2.3553, Validation Accuracy:0.1905\n",
    "Epoch #291: Loss:1.0637, Accuracy:0.5922, Validation Loss:2.3117, Validation Accuracy:0.1806\n",
    "Epoch #292: Loss:1.0963, Accuracy:0.5729, Validation Loss:2.3083, Validation Accuracy:0.1790\n",
    "Epoch #293: Loss:1.0599, Accuracy:0.5947, Validation Loss:2.3698, Validation Accuracy:0.1741\n",
    "Epoch #294: Loss:1.0944, Accuracy:0.5737, Validation Loss:2.3545, Validation Accuracy:0.1856\n",
    "Epoch #295: Loss:1.0737, Accuracy:0.5918, Validation Loss:2.3412, Validation Accuracy:0.1905\n",
    "Epoch #296: Loss:1.0658, Accuracy:0.5951, Validation Loss:2.3499, Validation Accuracy:0.1806\n",
    "Epoch #297: Loss:1.0704, Accuracy:0.5901, Validation Loss:2.3683, Validation Accuracy:0.1856\n",
    "Epoch #298: Loss:1.0637, Accuracy:0.5860, Validation Loss:2.3510, Validation Accuracy:0.1872\n",
    "Epoch #299: Loss:1.0412, Accuracy:0.6053, Validation Loss:2.3342, Validation Accuracy:0.1773\n",
    "Epoch #300: Loss:1.0311, Accuracy:0.6115, Validation Loss:2.3631, Validation Accuracy:0.1806\n",
    "\n",
    "Test:\n",
    "Test Loss:2.36308527, Accuracy:0.1806\n",
    "Labels: ['05', '03', '01', '04', '02']\n",
    "Confusion Matrix:\n",
    "      05  03  01  04  02\n",
    "t:05  30  37  33  23  19\n",
    "t:03  33  24  18  20  20\n",
    "t:01  37  34  14  19  22\n",
    "t:04  30  21  24  27  10\n",
    "t:02  29  23  27  20  15\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.19      0.21      0.20       142\n",
    "          03       0.17      0.21      0.19       115\n",
    "          01       0.12      0.11      0.12       126\n",
    "          04       0.25      0.24      0.24       112\n",
    "          02       0.17      0.13      0.15       114\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.18      0.18      0.18       609\n",
    "weighted avg       0.18      0.18      0.18       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 07:04:32 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.605567572347832, 1.6046956435017201, 1.6041025657371935, 1.6035520053653687, 1.6031449075990123, 1.6028018369659023, 1.602593558958207, 1.6019478558711036, 1.601386576254771, 1.6010554692232355, 1.6006940558234655, 1.6004074430230804, 1.6000109679984733, 1.5995805864459385, 1.5992487672906008, 1.5989771940438031, 1.5989115349764895, 1.5986170026860604, 1.5984821025961138, 1.5984478860065854, 1.5986187293611724, 1.598632111142226, 1.5986681648075873, 1.5995194502847732, 1.5993664648341037, 1.5995257628962325, 1.5997566068896716, 1.6003176431937758, 1.6005937816278493, 1.6006567233497482, 1.6010982258175002, 1.6014418513904065, 1.6019618072729, 1.6029801448969223, 1.6032783249133131, 1.6027002481404196, 1.6025647153995308, 1.6030779023867328, 1.605150511895103, 1.6061620264022025, 1.6054242816073163, 1.6031938045482916, 1.6041562672710574, 1.6047877648781086, 1.6049010377799349, 1.6050574601381675, 1.6064727036236541, 1.605469149321758, 1.6062648431420914, 1.6067146105915064, 1.6068906292735259, 1.6077442703575924, 1.6068157325628747, 1.6066281509712608, 1.6074123280780461, 1.6067360013185072, 1.6091064042449976, 1.6104314822477268, 1.6103676496859647, 1.6131467341593726, 1.610464954806862, 1.6160332215047626, 1.6130680438920195, 1.6116399430289057, 1.6135653327838542, 1.6147139505016783, 1.6186971022381962, 1.6165058135203345, 1.6230499292242115, 1.6213961285714837, 1.6243089114503908, 1.6236828409000765, 1.624288471265771, 1.631556037415816, 1.635368794447487, 1.6334779753865083, 1.6364140334387718, 1.6489778661179817, 1.6516609340661461, 1.6523818433382633, 1.6398484381940368, 1.642584974542627, 1.6464581734245438, 1.658666734820712, 1.6637141708474246, 1.6535274495044952, 1.6676492493336619, 1.6563731463280413, 1.6787664314796185, 1.6576565169348505, 1.6649497858996463, 1.6749046823661315, 1.68049591652474, 1.6861162878609643, 1.6901904056812156, 1.6998353386160188, 1.699880332390859, 1.7004275801538051, 1.6962338120283555, 1.6934095290298337, 1.7012845823917482, 1.7036397400356473, 1.7209410782909549, 1.708933394139232, 1.7132819198035254, 1.7342883892638734, 1.7150339369703396, 1.7432730053054484, 1.7283180057513108, 1.7227809104230407, 1.729506571500368, 1.7253309000889068, 1.748120241564483, 1.733139937342877, 1.7547495372972661, 1.747615446010834, 1.7470493888228593, 1.7773480111938, 1.7464346316060408, 1.7625745280427103, 1.769156622964956, 1.7685204953787166, 1.7633901805126022, 1.7830271840291265, 1.7638587683488192, 1.783598653397145, 1.7813325899183652, 1.8021834724642374, 1.8026932840081076, 1.8086764742000936, 1.8066087070552783, 1.8110639203358165, 1.7964518585032823, 1.8066007531139454, 1.8028395886491673, 1.8244041560710162, 1.8240465491471816, 1.82928417038252, 1.8333570931736862, 1.8162189056524893, 1.8366868374578667, 1.8318269015924489, 1.8544085264597425, 1.8532686204158615, 1.8233538928681798, 1.8478123799137685, 1.874833589116928, 1.8705350659750952, 1.8486913000225824, 1.8368651586996119, 1.878989919065842, 1.9147388363510909, 1.880157879616435, 1.8742024117502674, 1.86840853452291, 1.8865088412327131, 1.8726799354960375, 1.8639114527475267, 1.8392765116809038, 1.8759963807997053, 1.865413077163383, 1.866491207739794, 1.8529371614330898, 1.8849895630759754, 1.906440969366941, 1.920430634409336, 1.8950584341935532, 1.899747166531818, 1.8946644492533016, 1.9353897665521782, 1.9474517516118943, 1.9628806098537102, 1.921228329340617, 1.930856451808134, 1.9449508452454616, 1.9591770935528383, 1.9543107957479793, 1.911659397319424, 1.9310527399842963, 1.9628695780029046, 1.991769386629753, 1.9517511197890358, 1.9439432354787691, 1.9681671926344948, 1.9638060458579478, 1.9581833101062744, 1.9879748641172261, 2.011963129826563, 1.9961866169727494, 1.9865741800204875, 1.9895802609047475, 1.9727139664792466, 2.012917745289544, 2.0036205540736907, 2.0245372975009612, 2.0153361737043007, 2.026618870608325, 2.027788520838044, 2.0391332934838404, 2.089203163125049, 2.040560327335727, 2.012043602556626, 1.9657919559572719, 1.9683411274050258, 1.9879869870560118, 2.027266874689187, 2.0411526319037128, 2.0311023331628055, 2.036790450414022, 2.0202641216992157, 2.0595195880664394, 2.049174925376629, 2.04693315769064, 2.09896115912201, 2.0731476852655017, 2.0952566053675508, 2.109921585358618, 2.1159177233628648, 2.1113705791667567, 2.113602497307538, 2.140787169263868, 2.1164294874726846, 2.064991581420397, 2.091614760005807, 2.095036862910479, 2.1441997068464658, 2.139819495979397, 2.1154987322677337, 2.096042143300249, 2.094867360219971, 2.151031523111027, 2.132671053186426, 2.1640445015504834, 2.1936456933984614, 2.162518720908705, 2.1322307367434448, 2.1230174661269916, 2.1458625867840495, 2.148361618687171, 2.16623009132047, 2.1633080751046365, 2.187959011356623, 2.1526985685226365, 2.1609037161264903, 2.1938220886957076, 2.189109996817578, 2.1845704471732206, 2.1868134367054908, 2.2252213669136434, 2.185895688036588, 2.224950636548949, 2.235415139221793, 2.2078673185777586, 2.1992427558930245, 2.234891536004829, 2.2117658620593192, 2.223404259908767, 2.233295724113978, 2.2875726316950002, 2.228936260361194, 2.2515811375992247, 2.251562686976541, 2.2335254292574227, 2.2528173716001714, 2.2397935410047007, 2.2494653770684803, 2.2472456104453955, 2.2574311676871015, 2.232114896789952, 2.26113781983825, 2.2646858727403463, 2.269080798614201, 2.283927400319643, 2.2792400358541456, 2.2868002478908043, 2.233650231400538, 2.3138649815996293, 2.301447039950266, 2.2499958047725883, 2.2623420642514533, 2.304268880039209, 2.3083691397323984, 2.3056032493196685, 2.3532019976911873, 2.298034842182654, 2.3166289212081232, 2.327889076790395, 2.3225277127890753, 2.338452778817789, 2.355307411090494, 2.311687561483023, 2.308276910108494, 2.3698098228874267, 2.354480871034569, 2.3411937315867255, 2.3499180789064305, 2.368272874938639, 2.3510101745868552, 2.3341976637127755, 2.3630851336887906], 'val_acc': [0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.24302134605366216, 0.2413793099288674, 0.24302134605366216, 0.2413793099288674, 0.2430213459557892, 0.24958949045496817, 0.24958949045496817, 0.2512315265797629, 0.2512315265797629, 0.2528735627045577, 0.2528735629003036, 0.25451559882935243, 0.25451559882935243, 0.2528735627045577, 0.2561576352477661, 0.25451559882935243, 0.24794745452591938, 0.25451559912297134, 0.2512315265797629, 0.25451559892722536, 0.2528735629003036, 0.24958949065071412, 0.2446633821784569, 0.2446633821784569, 0.24302134605366216, 0.26272577994269103, 0.24958949045496817, 0.25944170749735557, 0.25944170769310154, 0.23809523748353198, 0.2561576351498931, 0.25123152687338185, 0.2364532014566102, 0.24466338208058394, 0.24958949065071412, 0.24137930983099445, 0.2577996711768149, 0.2446633821784569, 0.25944170759522855, 0.23645320155448318, 0.24466338237420288, 0.2397372740976916, 0.24630541840112463, 0.23152709318009895, 0.24466338247207586, 0.2331691293048937, 0.2463054164191968, 0.2364532014566102, 0.2348111651360695, 0.23973727390194566, 0.23809523797289686, 0.23316912950063964, 0.2380952378750239, 0.24137931032035934, 0.2348111651360695, 0.23973727390194566, 0.2380952378750239, 0.25615763326583824, 0.24302134605366216, 0.2594417057111737, 0.23645320155448318, 0.25451559912297134, 0.23645320155448318, 0.24630541869474357, 0.23481116572330737, 0.2463054164191968, 0.24794745264186452, 0.24794745254399153, 0.24630541651706978, 0.22495894868091998, 0.22824302102838243, 0.23152709308222597, 0.2413793082405585, 0.23316912969638562, 0.2397372719200178, 0.22331691275187118, 0.22660098509933366, 0.24137930804481256, 0.21674876835056517, 0.22824301904645458, 0.22331691226250627, 0.2315270912960441, 0.22660098500146067, 0.22331691275187118, 0.23152709357159088, 0.2167487682526922, 0.22167487652920345, 0.22660098480571472, 0.22495894887666593, 0.2282430211262554, 0.2183908044753599, 0.21182265987830795, 0.21510673203002448, 0.21674876844843816, 0.22167487662707644, 0.211822659780435, 0.2101806237535132, 0.22660098500146067, 0.20689655150392372, 0.21839080427961396, 0.20197044283592056, 0.2216748768228224, 0.21510673222577043, 0.20853858762871846, 0.21839080418174098, 0.22167487672494943, 0.21018062365564025, 0.2183908044753599, 0.2216748768228224, 0.21182266007405393, 0.21674876835056517, 0.21018062365564025, 0.20525451537912898, 0.21510673004809663, 0.20525451557487495, 0.20689655130817777, 0.21674876617289138, 0.2118226578963801, 0.21510673212789747, 0.21674876835056517, 0.18719211761489485, 0.22331691284974417, 0.22660098490358768, 0.2151067319321515, 0.21346469619884867, 0.21674876627076436, 0.2101806237535132, 0.22003284060015468, 0.2200328405022817, 0.22331691275187118, 0.2183908044753599, 0.21510673014596962, 0.20525451329932815, 0.21018062394925918, 0.21674876805694623, 0.20853858762871846, 0.21510673014596962, 0.19868637058633107, 0.20525451537912898, 0.211822659682562, 0.19704433446153632, 0.20361247925433423, 0.19540229853248753, 0.21018062355776726, 0.19211822608715207, 0.20853858743297252, 0.21346469600310272, 0.20197044293379354, 0.21510673004809663, 0.2118226579942531, 0.19704433485302825, 0.21839080229768612, 0.21346469402117488, 0.21182265987830795, 0.20689655140605073, 0.20361247915646125, 0.20525451537912898, 0.2085385875308455, 0.19047618996235732, 0.21346469610097568, 0.19540229863036052, 0.2101806237535132, 0.18226600924051062, 0.20032840661325282, 0.20689655150392372, 0.19211822638077103, 0.20525451508551004, 0.1937602623098198, 0.20525451508551004, 0.2068965516996697, 0.19211822628289804, 0.20361247915646125, 0.1855500814901001, 0.20197044322741248, 0.19540229833674158, 0.20197044332528544, 0.18719211761489485, 0.19211822618502505, 0.19704433455940928, 0.18226600924051062, 0.19868637058633107, 0.2019704430316665, 0.19540229833674158, 0.2085385875308455, 0.20689655150392372, 0.1904761900602303, 0.20032840700474475, 0.18883415354394364, 0.19047618986448436, 0.21018062365564025, 0.19047618976661138, 0.19376026211407385, 0.18555008168584608, 0.20689655140605073, 0.18883415373968962, 0.2036124788628423, 0.18719211771276784, 0.1855500814901001, 0.2003284067111258, 0.1937602622119468, 0.19704433426579035, 0.18555008139222715, 0.19540229843461454, 0.20032840651537984, 0.1855500814901001, 0.19704433446153632, 0.19868637087995, 0.19868637058633107, 0.18883415364181663, 0.18226600914263763, 0.20197044293379354, 0.20032840700474475, 0.19376026201620086, 0.19047618976661138, 0.19704433416791736, 0.19868637078207702, 0.1986863703905851, 0.18883415373968962, 0.19540229833674158, 0.18390804516955941, 0.1904761896687384, 0.19376026191832788, 0.17898193708879412, 0.19868637068420403, 0.19540229833674158, 0.1839080452674324, 0.1822660093383836, 0.1789819366973022, 0.19704433436366334, 0.1839080452674324, 0.19047618976661138, 0.18555008129435416, 0.18226600914263763, 0.1855500814901001, 0.1954022981409956, 0.18062397282209694, 0.1921182259892791, 0.1871921174191489, 0.18226600904476467, 0.19376026201620086, 0.19047618976661138, 0.18719211761489485, 0.19376026211407385, 0.1691297200464067, 0.19047618986448436, 0.18883415354394364, 0.18062397291996993, 0.18226600904476467, 0.19704433416791736, 0.18555008129435416, 0.19047618976661138, 0.19376026211407385, 0.17569786474133164, 0.18719211761489485, 0.18226600904476467, 0.1871921174191489, 0.18555008139222715, 0.17898193679517518, 0.1871921175170219, 0.1871921174191489, 0.17405582842079093, 0.18390804536530536, 0.17733990057250745, 0.18062397291996993, 0.18062397291996993, 0.18226600894689168, 0.1855500815879731, 0.18226600904476467, 0.1822660093383836, 0.17733990067038044, 0.18390804536530536, 0.18555008139222715, 0.1691297200464067, 0.18883415364181663, 0.18062397291996993, 0.18719211761489485, 0.18062397311571587, 0.17569786474133164, 0.18226600904476467, 0.18883415364181663, 0.1740558285186639, 0.19047618976661138, 0.18062397272422395, 0.17898193699092113, 0.1740558285186639, 0.1855500815879731, 0.1904761896687384, 0.1806239730178429, 0.18555008139222715, 0.1871921175170219, 0.17733990067038044, 0.1806239730178429], 'loss': [1.6078842689858814, 1.6062053864007124, 1.6052778557830278, 1.6049561378647415, 1.6045656354520355, 1.6042559656519175, 1.6041065164660038, 1.6039646337164501, 1.6035892370545155, 1.6034689802898274, 1.603112417618597, 1.602862442249635, 1.6025657381120404, 1.602134092534592, 1.6018716881652142, 1.6014963689036439, 1.6015765658149483, 1.6007477963484777, 1.600409366169015, 1.6001570444577038, 1.5999141916357271, 1.599589520998804, 1.5999289672722317, 1.5993842571912604, 1.5987912541786993, 1.59878651689455, 1.5981116380534868, 1.5979320569694409, 1.5981809326265872, 1.597464520975305, 1.5967722799743715, 1.5965620280780832, 1.5964882358633272, 1.5961545289664298, 1.595762405062603, 1.595292551757374, 1.5946115175556599, 1.594660586351242, 1.594353584097641, 1.5946857766693867, 1.5934609600161136, 1.592989347749667, 1.5930809886548554, 1.592527360152415, 1.591436786328498, 1.591300140660891, 1.5901182464016046, 1.5907473271877124, 1.5933109773013137, 1.592710295007459, 1.5895492697153737, 1.5878241161546178, 1.5890721430768713, 1.5866626479542476, 1.5858521709207147, 1.5841708325752242, 1.5837570024466858, 1.5844450621634294, 1.5826214266508756, 1.5810522333552461, 1.5790712305652532, 1.5803345056529896, 1.5787368468435394, 1.5768727092037944, 1.5745261981991527, 1.571813580681411, 1.5716453966908386, 1.5736778802205895, 1.5705579407161265, 1.5697216756534773, 1.5632804043728712, 1.5656396154995083, 1.5627779333498444, 1.5588328614616787, 1.5561840354539531, 1.5565531484889787, 1.5510870455716426, 1.5493771620599641, 1.5534250411164834, 1.557876313932133, 1.5530281667591854, 1.545641186154109, 1.5449131718406441, 1.5380088872733302, 1.5393907054004239, 1.5437741701118266, 1.5423375883631147, 1.5360616925805501, 1.533070168553926, 1.5395784276711622, 1.5301048679273477, 1.5256624208093914, 1.520959045950637, 1.5205495206237574, 1.5151076255637763, 1.5172580853624755, 1.5218761541515404, 1.5219526227739557, 1.514964031438808, 1.5107632658075258, 1.5092987159439182, 1.5056485188815139, 1.501935756328904, 1.5036993844308402, 1.502620709652284, 1.4997626056416569, 1.5029524666572742, 1.4920830587097262, 1.5024543427343975, 1.4931992492636617, 1.4943121021043593, 1.4945335080736226, 1.4877085532495864, 1.4795764886378262, 1.4710386538162858, 1.4787788073874597, 1.4766534955594575, 1.48038187766222, 1.4685006293428018, 1.463161113815386, 1.4595101542051814, 1.4730422595198394, 1.4635567641600935, 1.4513667607454304, 1.449511921136531, 1.445915524132198, 1.4550739859163884, 1.4448782343149675, 1.4357124421630796, 1.446705902430556, 1.437982346340861, 1.4294310747475594, 1.4319942796499578, 1.4299459055463881, 1.4274248061483645, 1.4244353690920915, 1.4182781596937708, 1.4149081349617647, 1.406918301474632, 1.4110023543575216, 1.4096651380311782, 1.413569640427889, 1.4056236377242164, 1.4062622394405107, 1.40011806145341, 1.402178015454349, 1.3946393783087603, 1.3967186352555512, 1.3832082647562516, 1.388307276153956, 1.3989516827604853, 1.388927187958782, 1.390805180998064, 1.3801150641646962, 1.3694703623009903, 1.366415708510538, 1.3735516530042802, 1.390046632167495, 1.4078392514947504, 1.3889750256430686, 1.371894321304572, 1.35873787212176, 1.3565328911344618, 1.3578889491866502, 1.3531398054510662, 1.3554248547896712, 1.3488427592988377, 1.3406354875779984, 1.3370536252948049, 1.336544149563298, 1.3434115283053514, 1.337555475009785, 1.3296893858566912, 1.3271356009359967, 1.3329842821528535, 1.3309312748468387, 1.3205889040929335, 1.3190308111159463, 1.3177945086598641, 1.307878490150342, 1.313095980748014, 1.3162428224845588, 1.3076866254179875, 1.3040221564823598, 1.292742725955877, 1.2984968122760374, 1.306050480758385, 1.2891029872933453, 1.288469477404804, 1.2902610907564418, 1.2793910158733077, 1.2873655754437927, 1.2782034637502087, 1.2793068055989072, 1.2693748551962067, 1.2622454611917295, 1.2640654594746459, 1.2607150241334826, 1.2526902125356623, 1.267855313720155, 1.3036215720480229, 1.3043028126996645, 1.2959189308252668, 1.2817750995408828, 1.280368819226964, 1.2564213339063421, 1.2492090914038907, 1.2400319654593968, 1.2324711504902928, 1.2402378961046128, 1.233565734005562, 1.2353640793774896, 1.2319131594174206, 1.2210872806317996, 1.2205551899923681, 1.2241153380219696, 1.2289326107232723, 1.219618342397639, 1.2155491338373454, 1.2229049072618112, 1.218576153249956, 1.226780665336938, 1.2103173492870292, 1.2044988375669632, 1.2196194884713425, 1.2200044231493126, 1.203547554779836, 1.2099521961545063, 1.1917339027295122, 1.1915966106880862, 1.2008727281734928, 1.190003446484983, 1.1873149067469446, 1.193820215985026, 1.2060849219621819, 1.1910910107516655, 1.1660509362602625, 1.1683028728320612, 1.1751707630725368, 1.1768778216422706, 1.158685231845237, 1.1498140847658476, 1.1560068807562764, 1.1467887216035346, 1.1486449008604829, 1.1554743853437828, 1.1446323053547978, 1.1522272855594173, 1.137842890858895, 1.1381131793439265, 1.1422858256334152, 1.1442760865056785, 1.1393100773774134, 1.1353368666137758, 1.1256781568272647, 1.1272796005194192, 1.118918598310169, 1.138547541669262, 1.1296205666520511, 1.1301098083323766, 1.1263443473917747, 1.1064493071127233, 1.107887074002495, 1.107177385559317, 1.1000186177984155, 1.0919960994250475, 1.1007990263815532, 1.0928177371896513, 1.108872197247139, 1.1083241082314839, 1.0950512063087134, 1.081545927979863, 1.083777449605891, 1.0807060676433713, 1.0804745152256083, 1.111700956385728, 1.1208274920618264, 1.117186440530499, 1.082112427511744, 1.0867046011057233, 1.075905604186244, 1.0684662872760937, 1.0585990568450834, 1.0594817007346808, 1.0674317500919288, 1.0654961507667995, 1.0563974457355005, 1.0512009051301396, 1.0578980889898062, 1.0797151093120692, 1.0637145206424001, 1.0962920626575696, 1.059946656325025, 1.094435896716813, 1.0736630357511234, 1.0658073333744151, 1.0703798886442086, 1.0636681817150704, 1.041218736822845, 1.031051750937037], 'acc': [0.2225872710010599, 0.23285420970872686, 0.23285421029620593, 0.23285421029620593, 0.2328542083195837, 0.23285421049203225, 0.2328542091028891, 0.2328542087112364, 0.23408624315041537, 0.2377823401280742, 0.2390143743714268, 0.2365503074696911, 0.2373716637033212, 0.24147844047149838, 0.24106776267596094, 0.24065708482534734, 0.2406570830629102, 0.24147843986566062, 0.2422997953159853, 0.2435318287393151, 0.24271047193656467, 0.24229979392684217, 0.23942505081453852, 0.2422997939635596, 0.24271047193656467, 0.23655030807552885, 0.24312115151289798, 0.24558521702549052, 0.2431211503562986, 0.2406570838462156, 0.23367556474904017, 0.23860369614751922, 0.24024640603231942, 0.24188911789374185, 0.24065708521700002, 0.24394250617991728, 0.25051334628823846, 0.252566735749372, 0.24804928175477767, 0.240657084256227, 0.253388090808044, 0.26365503065395157, 0.25872689982459285, 0.2628336736186574, 0.26160164432603966, 0.26324435362336085, 0.26036960771441214, 0.2579055436093215, 0.25256673516189293, 0.2570841897439663, 0.26817248660680937, 0.26652977417626667, 0.2599589328746286, 0.26735112980405895, 0.26899383990304426, 0.27022587371802675, 0.2694045183227782, 0.26940451511612173, 0.268172485823504, 0.26611909515069493, 0.26858316246244207, 0.2648870652705982, 0.2755646839278924, 0.28172484444152157, 0.2788501046766246, 0.28090349061288383, 0.2841889123407477, 0.27186858458195867, 0.2833675545221481, 0.28090349038034007, 0.2952772066211309, 0.2870636570380209, 0.2956878846308534, 0.29691991850091204, 0.29897330381297477, 0.2985626305398021, 0.30102669546491556, 0.2997946610073779, 0.2977412743245307, 0.2903490752042931, 0.2969199184641946, 0.3117043099364216, 0.3112936331016572, 0.3203285432816531, 0.319917866055236, 0.3133470242028364, 0.30965092384105347, 0.32279260801094023, 0.32607803091376225, 0.3227926091858983, 0.3322381939731339, 0.32977412513149346, 0.34743326685756626, 0.33675564898357746, 0.3367556448345067, 0.34086242418514384, 0.3322381944015041, 0.3281314190041113, 0.33675564702531396, 0.34579055442702356, 0.35318275014477835, 0.34702258884784376, 0.348665298732644, 0.34496919899505757, 0.34127310019988544, 0.354414784602316, 0.35441478675640586, 0.35975359379633254, 0.34168377664299715, 0.354004105968397, 0.3519507168989162, 0.34989733295763786, 0.36755646795217994, 0.36550308005765725, 0.37659137672467397, 0.3535934307369608, 0.3646817259964757, 0.36550308244429086, 0.36755646756052723, 0.3819301841929708, 0.37618069593666514, 0.37248460116817234, 0.3790554408664821, 0.38357289404105355, 0.3868583165889403, 0.38973305874047093, 0.38521560729161913, 0.3835728946652501, 0.394661189765656, 0.37659137574554225, 0.3979466112976936, 0.3987679677087913, 0.39753593661701897, 0.39301848203494566, 0.4004106771652214, 0.40287474150285585, 0.4110882962508858, 0.40944558319614655, 0.41683778407882127, 0.41724846169689106, 0.41190965246615713, 0.40492813018068397, 0.40492813217566487, 0.41108829703419114, 0.420123204790836, 0.4106776182411632, 0.4114989736364118, 0.4250513354243684, 0.42833675718894976, 0.42340862495208914, 0.4127310084856022, 0.4151950720399312, 0.4168377814963613, 0.4279260787508571, 0.4373716610290676, 0.4344969208358005, 0.42176591271737274, 0.42176591608313807, 0.4004106775935915, 0.4078028764812853, 0.4291581106259348, 0.4353182762677665, 0.4390143738512631, 0.44065708569432677, 0.43860369505823515, 0.43901437600535287, 0.45010266754416717, 0.4554414779498592, 0.4451745369473522, 0.4488706386432021, 0.44106776174578577, 0.45462012271371954, 0.45790554565325897, 0.4603696105783725, 0.45462012412122144, 0.4546201221262405, 0.45995893393943443, 0.4562628336143689, 0.46694044968920323, 0.47433265014350784, 0.47145790763704193, 0.4628336772659231, 0.46447638715072337, 0.46940451915504017, 0.4784394273400551, 0.4780287493303326, 0.4657084209840645, 0.4784394239742898, 0.4796714562410202, 0.4829568787521895, 0.48501026684253856, 0.47926077901460307, 0.48459958746203163, 0.48172484393971654, 0.48747433157182574, 0.505133472673702, 0.4977412716319184, 0.49445585225397065, 0.5018480474209639, 0.4952772072942839, 0.47145790407545024, 0.4657084188299747, 0.46694045246748955, 0.47679671291453146, 0.4739219723663291, 0.5010266953180458, 0.507597535053073, 0.5162217673824553, 0.5158110905476909, 0.5121149913975835, 0.5133470269933619, 0.5075975356405521, 0.5125256699947851, 0.5203285435631535, 0.5166324446088725, 0.5174537994533593, 0.5059548265390572, 0.5141683785088008, 0.5211498958618979, 0.528131419481438, 0.5211498991909458, 0.5121149906142781, 0.5244353183730672, 0.5334702267539085, 0.5199178649659519, 0.5285420974911605, 0.5301848056135237, 0.5293634501815576, 0.5367556498525569, 0.537166327666453, 0.5264887095966378, 0.5289527731509669, 0.5334702304746092, 0.5297741307370227, 0.5203285437589799, 0.5281314214397015, 0.5453798731004923, 0.5457905515018675, 0.5408624242708179, 0.5445585232250989, 0.5597535889496303, 0.5618069861214263, 0.5507186904335414, 0.5601642725159255, 0.5683778193207492, 0.54661190497557, 0.5531827559461339, 0.5494866552294158, 0.5704312159050662, 0.5609856219996662, 0.5572895293852632, 0.5609856245454087, 0.5556468214587265, 0.5663244318178792, 0.5638603738201228, 0.5696098533499168, 0.5790554373905644, 0.5597535974435982, 0.5655030829216176, 0.5609856221954925, 0.560985630493634, 0.5733059521083714, 0.5753593398070678, 0.577823412051191, 0.5802874706364265, 0.5913757667159643, 0.572484602232978, 0.5852156049906595, 0.583983576713891, 0.5708418869874315, 0.5843942545277871, 0.5864476372573899, 0.5909650962210778, 0.5876796667825515, 0.5942505121965428, 0.5708418856166472, 0.5609856301019813, 0.5737166346955348, 0.5885010263268707, 0.5856262802588131, 0.5917864486422137, 0.5876796689366414, 0.5975359311828379, 0.5946611868772174, 0.5921971252811518, 0.5893223857487986, 0.6004106747051529, 0.6012320326828614, 0.5930184829406425, 0.5765913803719397, 0.5921971215604512, 0.572895281026006, 0.5946611892271335, 0.5737166295306149, 0.5917864435507286, 0.5950718701742513, 0.5901437358200183, 0.5860369576810567, 0.6053388057303379, 0.6114989705888643]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
