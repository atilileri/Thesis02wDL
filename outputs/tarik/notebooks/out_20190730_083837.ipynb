{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf15.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 08:38:37 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': 'Front', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001D85C494E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001D853D36EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0848, Accuracy:0.3713, Validation Loss:1.0753, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0746, Accuracy:0.3799, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0740, Accuracy:0.3885, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #7: Loss:1.0738, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #11: Loss:1.0736, Accuracy:0.3926, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #12: Loss:1.0734, Accuracy:0.3934, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #13: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.4039\n",
    "Epoch #14: Loss:1.0733, Accuracy:0.4025, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #15: Loss:1.0731, Accuracy:0.4025, Validation Loss:1.0740, Validation Accuracy:0.4056\n",
    "Epoch #16: Loss:1.0730, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #17: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #18: Loss:1.0730, Accuracy:0.4021, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #19: Loss:1.0730, Accuracy:0.4074, Validation Loss:1.0740, Validation Accuracy:0.4039\n",
    "Epoch #20: Loss:1.0725, Accuracy:0.4090, Validation Loss:1.0738, Validation Accuracy:0.4039\n",
    "Epoch #21: Loss:1.0727, Accuracy:0.3984, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #22: Loss:1.0723, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.4056\n",
    "Epoch #23: Loss:1.0721, Accuracy:0.4021, Validation Loss:1.0737, Validation Accuracy:0.4122\n",
    "Epoch #24: Loss:1.0720, Accuracy:0.4078, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #25: Loss:1.0720, Accuracy:0.4025, Validation Loss:1.0733, Validation Accuracy:0.4039\n",
    "Epoch #26: Loss:1.0716, Accuracy:0.4115, Validation Loss:1.0733, Validation Accuracy:0.4154\n",
    "Epoch #27: Loss:1.0716, Accuracy:0.4062, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #28: Loss:1.0713, Accuracy:0.4066, Validation Loss:1.0734, Validation Accuracy:0.4105\n",
    "Epoch #29: Loss:1.0713, Accuracy:0.4160, Validation Loss:1.0731, Validation Accuracy:0.4089\n",
    "Epoch #30: Loss:1.0712, Accuracy:0.4057, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #31: Loss:1.0709, Accuracy:0.4107, Validation Loss:1.0734, Validation Accuracy:0.4072\n",
    "Epoch #32: Loss:1.0706, Accuracy:0.4123, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #33: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #34: Loss:1.0710, Accuracy:0.4074, Validation Loss:1.0747, Validation Accuracy:0.4105\n",
    "Epoch #35: Loss:1.0702, Accuracy:0.4136, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #36: Loss:1.0698, Accuracy:0.4168, Validation Loss:1.0735, Validation Accuracy:0.4138\n",
    "Epoch #37: Loss:1.0685, Accuracy:0.4197, Validation Loss:1.0735, Validation Accuracy:0.4072\n",
    "Epoch #38: Loss:1.0683, Accuracy:0.4144, Validation Loss:1.0737, Validation Accuracy:0.4056\n",
    "Epoch #39: Loss:1.0675, Accuracy:0.4242, Validation Loss:1.0743, Validation Accuracy:0.4122\n",
    "Epoch #40: Loss:1.0662, Accuracy:0.4263, Validation Loss:1.0749, Validation Accuracy:0.4007\n",
    "Epoch #41: Loss:1.0665, Accuracy:0.4214, Validation Loss:1.0759, Validation Accuracy:0.4007\n",
    "Epoch #42: Loss:1.0663, Accuracy:0.4259, Validation Loss:1.0769, Validation Accuracy:0.3859\n",
    "Epoch #43: Loss:1.0648, Accuracy:0.4292, Validation Loss:1.0770, Validation Accuracy:0.3924\n",
    "Epoch #44: Loss:1.0635, Accuracy:0.4341, Validation Loss:1.0778, Validation Accuracy:0.3859\n",
    "Epoch #45: Loss:1.0624, Accuracy:0.4279, Validation Loss:1.0791, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0618, Accuracy:0.4353, Validation Loss:1.0816, Validation Accuracy:0.3842\n",
    "Epoch #47: Loss:1.0628, Accuracy:0.4337, Validation Loss:1.0847, Validation Accuracy:0.3924\n",
    "Epoch #48: Loss:1.0638, Accuracy:0.4324, Validation Loss:1.0855, Validation Accuracy:0.3760\n",
    "Epoch #49: Loss:1.0630, Accuracy:0.4300, Validation Loss:1.0821, Validation Accuracy:0.3908\n",
    "Epoch #50: Loss:1.0609, Accuracy:0.4267, Validation Loss:1.0834, Validation Accuracy:0.3892\n",
    "Epoch #51: Loss:1.0601, Accuracy:0.4271, Validation Loss:1.0831, Validation Accuracy:0.3875\n",
    "Epoch #52: Loss:1.0607, Accuracy:0.4324, Validation Loss:1.0797, Validation Accuracy:0.3842\n",
    "Epoch #53: Loss:1.0610, Accuracy:0.4345, Validation Loss:1.0794, Validation Accuracy:0.3875\n",
    "Epoch #54: Loss:1.0578, Accuracy:0.4394, Validation Loss:1.0823, Validation Accuracy:0.3924\n",
    "Epoch #55: Loss:1.0590, Accuracy:0.4370, Validation Loss:1.0827, Validation Accuracy:0.3760\n",
    "Epoch #56: Loss:1.0582, Accuracy:0.4341, Validation Loss:1.0826, Validation Accuracy:0.3810\n",
    "Epoch #57: Loss:1.0571, Accuracy:0.4423, Validation Loss:1.0825, Validation Accuracy:0.3810\n",
    "Epoch #58: Loss:1.0557, Accuracy:0.4439, Validation Loss:1.0840, Validation Accuracy:0.3826\n",
    "Epoch #59: Loss:1.0546, Accuracy:0.4489, Validation Loss:1.0851, Validation Accuracy:0.3678\n",
    "Epoch #60: Loss:1.0566, Accuracy:0.4509, Validation Loss:1.0843, Validation Accuracy:0.3727\n",
    "Epoch #61: Loss:1.0557, Accuracy:0.4382, Validation Loss:1.0858, Validation Accuracy:0.3957\n",
    "Epoch #62: Loss:1.0564, Accuracy:0.4370, Validation Loss:1.0860, Validation Accuracy:0.3892\n",
    "Epoch #63: Loss:1.0546, Accuracy:0.4394, Validation Loss:1.0880, Validation Accuracy:0.4007\n",
    "Epoch #64: Loss:1.0536, Accuracy:0.4505, Validation Loss:1.0873, Validation Accuracy:0.3875\n",
    "Epoch #65: Loss:1.0554, Accuracy:0.4398, Validation Loss:1.0866, Validation Accuracy:0.4072\n",
    "Epoch #66: Loss:1.0557, Accuracy:0.4448, Validation Loss:1.0855, Validation Accuracy:0.3810\n",
    "Epoch #67: Loss:1.0504, Accuracy:0.4439, Validation Loss:1.0881, Validation Accuracy:0.3892\n",
    "Epoch #68: Loss:1.0488, Accuracy:0.4554, Validation Loss:1.0896, Validation Accuracy:0.3760\n",
    "Epoch #69: Loss:1.0492, Accuracy:0.4513, Validation Loss:1.0868, Validation Accuracy:0.4023\n",
    "Epoch #70: Loss:1.0492, Accuracy:0.4600, Validation Loss:1.0886, Validation Accuracy:0.3908\n",
    "Epoch #71: Loss:1.0439, Accuracy:0.4628, Validation Loss:1.0954, Validation Accuracy:0.3842\n",
    "Epoch #72: Loss:1.0451, Accuracy:0.4624, Validation Loss:1.0909, Validation Accuracy:0.3727\n",
    "Epoch #73: Loss:1.0424, Accuracy:0.4608, Validation Loss:1.0946, Validation Accuracy:0.3793\n",
    "Epoch #74: Loss:1.0403, Accuracy:0.4694, Validation Loss:1.0937, Validation Accuracy:0.3842\n",
    "Epoch #75: Loss:1.0371, Accuracy:0.4752, Validation Loss:1.0956, Validation Accuracy:0.3842\n",
    "Epoch #76: Loss:1.0351, Accuracy:0.4727, Validation Loss:1.0998, Validation Accuracy:0.3826\n",
    "Epoch #77: Loss:1.0356, Accuracy:0.4747, Validation Loss:1.1010, Validation Accuracy:0.3892\n",
    "Epoch #78: Loss:1.0336, Accuracy:0.4731, Validation Loss:1.1075, Validation Accuracy:0.3957\n",
    "Epoch #79: Loss:1.0364, Accuracy:0.4710, Validation Loss:1.1042, Validation Accuracy:0.3760\n",
    "Epoch #80: Loss:1.0327, Accuracy:0.4805, Validation Loss:1.1017, Validation Accuracy:0.3744\n",
    "Epoch #81: Loss:1.0286, Accuracy:0.4797, Validation Loss:1.0988, Validation Accuracy:0.3924\n",
    "Epoch #82: Loss:1.0276, Accuracy:0.4809, Validation Loss:1.1049, Validation Accuracy:0.3924\n",
    "Epoch #83: Loss:1.0271, Accuracy:0.4834, Validation Loss:1.1087, Validation Accuracy:0.4007\n",
    "Epoch #84: Loss:1.0243, Accuracy:0.4834, Validation Loss:1.1099, Validation Accuracy:0.3678\n",
    "Epoch #85: Loss:1.0272, Accuracy:0.4924, Validation Loss:1.1174, Validation Accuracy:0.3924\n",
    "Epoch #86: Loss:1.0202, Accuracy:0.4916, Validation Loss:1.1112, Validation Accuracy:0.3760\n",
    "Epoch #87: Loss:1.0160, Accuracy:0.4949, Validation Loss:1.1126, Validation Accuracy:0.3793\n",
    "Epoch #88: Loss:1.0211, Accuracy:0.4936, Validation Loss:1.1312, Validation Accuracy:0.3908\n",
    "Epoch #89: Loss:1.0141, Accuracy:0.4998, Validation Loss:1.1178, Validation Accuracy:0.3810\n",
    "Epoch #90: Loss:1.0105, Accuracy:0.5023, Validation Loss:1.1216, Validation Accuracy:0.3793\n",
    "Epoch #91: Loss:1.0056, Accuracy:0.5138, Validation Loss:1.1235, Validation Accuracy:0.3777\n",
    "Epoch #92: Loss:1.0024, Accuracy:0.5113, Validation Loss:1.1384, Validation Accuracy:0.3875\n",
    "Epoch #93: Loss:1.0037, Accuracy:0.5101, Validation Loss:1.1328, Validation Accuracy:0.3793\n",
    "Epoch #94: Loss:0.9973, Accuracy:0.5240, Validation Loss:1.1377, Validation Accuracy:0.3875\n",
    "Epoch #95: Loss:0.9989, Accuracy:0.5129, Validation Loss:1.1436, Validation Accuracy:0.3826\n",
    "Epoch #96: Loss:0.9932, Accuracy:0.5191, Validation Loss:1.1384, Validation Accuracy:0.3777\n",
    "Epoch #97: Loss:0.9946, Accuracy:0.5158, Validation Loss:1.1521, Validation Accuracy:0.3924\n",
    "Epoch #98: Loss:0.9920, Accuracy:0.5146, Validation Loss:1.1540, Validation Accuracy:0.3957\n",
    "Epoch #99: Loss:0.9914, Accuracy:0.5220, Validation Loss:1.1521, Validation Accuracy:0.3580\n",
    "Epoch #100: Loss:0.9931, Accuracy:0.5228, Validation Loss:1.1560, Validation Accuracy:0.3941\n",
    "Epoch #101: Loss:0.9928, Accuracy:0.5257, Validation Loss:1.1513, Validation Accuracy:0.4007\n",
    "Epoch #102: Loss:1.0115, Accuracy:0.5047, Validation Loss:1.1287, Validation Accuracy:0.3810\n",
    "Epoch #103: Loss:0.9890, Accuracy:0.5228, Validation Loss:1.1291, Validation Accuracy:0.3744\n",
    "Epoch #104: Loss:0.9856, Accuracy:0.5216, Validation Loss:1.1260, Validation Accuracy:0.3777\n",
    "Epoch #105: Loss:0.9774, Accuracy:0.5339, Validation Loss:1.1387, Validation Accuracy:0.3842\n",
    "Epoch #106: Loss:0.9741, Accuracy:0.5347, Validation Loss:1.1575, Validation Accuracy:0.4007\n",
    "Epoch #107: Loss:0.9732, Accuracy:0.5388, Validation Loss:1.1806, Validation Accuracy:0.3924\n",
    "Epoch #108: Loss:0.9852, Accuracy:0.5216, Validation Loss:1.1932, Validation Accuracy:0.3924\n",
    "Epoch #109: Loss:0.9733, Accuracy:0.5409, Validation Loss:1.1677, Validation Accuracy:0.3875\n",
    "Epoch #110: Loss:0.9754, Accuracy:0.5199, Validation Loss:1.1528, Validation Accuracy:0.3711\n",
    "Epoch #111: Loss:0.9742, Accuracy:0.5376, Validation Loss:1.1639, Validation Accuracy:0.3875\n",
    "Epoch #112: Loss:0.9688, Accuracy:0.5437, Validation Loss:1.1507, Validation Accuracy:0.3793\n",
    "Epoch #113: Loss:0.9615, Accuracy:0.5413, Validation Loss:1.1591, Validation Accuracy:0.3760\n",
    "Epoch #114: Loss:0.9538, Accuracy:0.5483, Validation Loss:1.1691, Validation Accuracy:0.3727\n",
    "Epoch #115: Loss:0.9561, Accuracy:0.5524, Validation Loss:1.1833, Validation Accuracy:0.3908\n",
    "Epoch #116: Loss:0.9554, Accuracy:0.5437, Validation Loss:1.1954, Validation Accuracy:0.3941\n",
    "Epoch #117: Loss:0.9616, Accuracy:0.5491, Validation Loss:1.1808, Validation Accuracy:0.3760\n",
    "Epoch #118: Loss:0.9592, Accuracy:0.5478, Validation Loss:1.1988, Validation Accuracy:0.3284\n",
    "Epoch #119: Loss:0.9671, Accuracy:0.5483, Validation Loss:1.1726, Validation Accuracy:0.3859\n",
    "Epoch #120: Loss:0.9715, Accuracy:0.5351, Validation Loss:1.1848, Validation Accuracy:0.3810\n",
    "Epoch #121: Loss:0.9681, Accuracy:0.5359, Validation Loss:1.1689, Validation Accuracy:0.3547\n",
    "Epoch #122: Loss:0.9585, Accuracy:0.5507, Validation Loss:1.1603, Validation Accuracy:0.3777\n",
    "Epoch #123: Loss:0.9543, Accuracy:0.5478, Validation Loss:1.1719, Validation Accuracy:0.3826\n",
    "Epoch #124: Loss:0.9494, Accuracy:0.5483, Validation Loss:1.1803, Validation Accuracy:0.3678\n",
    "Epoch #125: Loss:0.9542, Accuracy:0.5544, Validation Loss:1.1859, Validation Accuracy:0.3727\n",
    "Epoch #126: Loss:0.9536, Accuracy:0.5405, Validation Loss:1.1963, Validation Accuracy:0.3760\n",
    "Epoch #127: Loss:0.9561, Accuracy:0.5450, Validation Loss:1.2135, Validation Accuracy:0.3892\n",
    "Epoch #128: Loss:0.9463, Accuracy:0.5540, Validation Loss:1.1680, Validation Accuracy:0.3727\n",
    "Epoch #129: Loss:0.9367, Accuracy:0.5708, Validation Loss:1.1819, Validation Accuracy:0.3793\n",
    "Epoch #130: Loss:0.9314, Accuracy:0.5659, Validation Loss:1.2190, Validation Accuracy:0.4007\n",
    "Epoch #131: Loss:0.9302, Accuracy:0.5659, Validation Loss:1.2066, Validation Accuracy:0.3612\n",
    "Epoch #132: Loss:0.9230, Accuracy:0.5795, Validation Loss:1.2388, Validation Accuracy:0.3744\n",
    "Epoch #133: Loss:0.9237, Accuracy:0.5692, Validation Loss:1.2301, Validation Accuracy:0.3695\n",
    "Epoch #134: Loss:0.9256, Accuracy:0.5807, Validation Loss:1.2516, Validation Accuracy:0.3924\n",
    "Epoch #135: Loss:0.9346, Accuracy:0.5700, Validation Loss:1.2395, Validation Accuracy:0.3990\n",
    "Epoch #136: Loss:0.9227, Accuracy:0.5741, Validation Loss:1.2004, Validation Accuracy:0.3678\n",
    "Epoch #137: Loss:0.9281, Accuracy:0.5811, Validation Loss:1.2270, Validation Accuracy:0.4089\n",
    "Epoch #138: Loss:0.9241, Accuracy:0.5770, Validation Loss:1.2292, Validation Accuracy:0.3941\n",
    "Epoch #139: Loss:0.9208, Accuracy:0.5766, Validation Loss:1.2197, Validation Accuracy:0.3842\n",
    "Epoch #140: Loss:0.9115, Accuracy:0.5864, Validation Loss:1.2308, Validation Accuracy:0.3744\n",
    "Epoch #141: Loss:0.9187, Accuracy:0.5828, Validation Loss:1.2648, Validation Accuracy:0.3826\n",
    "Epoch #142: Loss:0.9314, Accuracy:0.5700, Validation Loss:1.2422, Validation Accuracy:0.3892\n",
    "Epoch #143: Loss:0.9209, Accuracy:0.5717, Validation Loss:1.2321, Validation Accuracy:0.3645\n",
    "Epoch #144: Loss:0.9166, Accuracy:0.5758, Validation Loss:1.2226, Validation Accuracy:0.3842\n",
    "Epoch #145: Loss:0.9063, Accuracy:0.5984, Validation Loss:1.2256, Validation Accuracy:0.3645\n",
    "Epoch #146: Loss:0.9136, Accuracy:0.5823, Validation Loss:1.2409, Validation Accuracy:0.3695\n",
    "Epoch #147: Loss:0.9004, Accuracy:0.5971, Validation Loss:1.2339, Validation Accuracy:0.3645\n",
    "Epoch #148: Loss:0.8949, Accuracy:0.6086, Validation Loss:1.2527, Validation Accuracy:0.3662\n",
    "Epoch #149: Loss:0.8948, Accuracy:0.6016, Validation Loss:1.2664, Validation Accuracy:0.3875\n",
    "Epoch #150: Loss:0.8964, Accuracy:0.5979, Validation Loss:1.2812, Validation Accuracy:0.3924\n",
    "Epoch #151: Loss:0.8963, Accuracy:0.6033, Validation Loss:1.2691, Validation Accuracy:0.3793\n",
    "Epoch #152: Loss:0.9003, Accuracy:0.5901, Validation Loss:1.2716, Validation Accuracy:0.3957\n",
    "Epoch #153: Loss:0.9045, Accuracy:0.5897, Validation Loss:1.2521, Validation Accuracy:0.3826\n",
    "Epoch #154: Loss:0.8973, Accuracy:0.5930, Validation Loss:1.2416, Validation Accuracy:0.3596\n",
    "Epoch #155: Loss:0.8969, Accuracy:0.6012, Validation Loss:1.2459, Validation Accuracy:0.3760\n",
    "Epoch #156: Loss:0.9069, Accuracy:0.5914, Validation Loss:1.2759, Validation Accuracy:0.3612\n",
    "Epoch #157: Loss:0.9320, Accuracy:0.5655, Validation Loss:1.2335, Validation Accuracy:0.3924\n",
    "Epoch #158: Loss:0.9088, Accuracy:0.5803, Validation Loss:1.2522, Validation Accuracy:0.3875\n",
    "Epoch #159: Loss:0.8994, Accuracy:0.5918, Validation Loss:1.2063, Validation Accuracy:0.3810\n",
    "Epoch #160: Loss:0.8868, Accuracy:0.6078, Validation Loss:1.2328, Validation Accuracy:0.3744\n",
    "Epoch #161: Loss:0.8806, Accuracy:0.6119, Validation Loss:1.2693, Validation Accuracy:0.3711\n",
    "Epoch #162: Loss:0.8767, Accuracy:0.6144, Validation Loss:1.2803, Validation Accuracy:0.3481\n",
    "Epoch #163: Loss:0.8782, Accuracy:0.6189, Validation Loss:1.2999, Validation Accuracy:0.3662\n",
    "Epoch #164: Loss:0.8784, Accuracy:0.6136, Validation Loss:1.2969, Validation Accuracy:0.3875\n",
    "Epoch #165: Loss:0.8824, Accuracy:0.6107, Validation Loss:1.2900, Validation Accuracy:0.3941\n",
    "Epoch #166: Loss:0.8704, Accuracy:0.6168, Validation Loss:1.2721, Validation Accuracy:0.3941\n",
    "Epoch #167: Loss:0.8732, Accuracy:0.6160, Validation Loss:1.2767, Validation Accuracy:0.3662\n",
    "Epoch #168: Loss:0.8918, Accuracy:0.6016, Validation Loss:1.2973, Validation Accuracy:0.3662\n",
    "Epoch #169: Loss:0.8892, Accuracy:0.5959, Validation Loss:1.2631, Validation Accuracy:0.3711\n",
    "Epoch #170: Loss:0.8755, Accuracy:0.6062, Validation Loss:1.2752, Validation Accuracy:0.3695\n",
    "Epoch #171: Loss:0.8851, Accuracy:0.6062, Validation Loss:1.2754, Validation Accuracy:0.3695\n",
    "Epoch #172: Loss:0.8761, Accuracy:0.6086, Validation Loss:1.2659, Validation Accuracy:0.3711\n",
    "Epoch #173: Loss:0.8611, Accuracy:0.6275, Validation Loss:1.2775, Validation Accuracy:0.3793\n",
    "Epoch #174: Loss:0.8605, Accuracy:0.6234, Validation Loss:1.2859, Validation Accuracy:0.3563\n",
    "Epoch #175: Loss:0.8510, Accuracy:0.6292, Validation Loss:1.2970, Validation Accuracy:0.3744\n",
    "Epoch #176: Loss:0.8477, Accuracy:0.6320, Validation Loss:1.3298, Validation Accuracy:0.3957\n",
    "Epoch #177: Loss:0.8728, Accuracy:0.6148, Validation Loss:1.3113, Validation Accuracy:0.3810\n",
    "Epoch #178: Loss:0.8612, Accuracy:0.6279, Validation Loss:1.3144, Validation Accuracy:0.3941\n",
    "Epoch #179: Loss:0.8577, Accuracy:0.6251, Validation Loss:1.3027, Validation Accuracy:0.3908\n",
    "Epoch #180: Loss:0.8450, Accuracy:0.6415, Validation Loss:1.3005, Validation Accuracy:0.3777\n",
    "Epoch #181: Loss:0.8537, Accuracy:0.6378, Validation Loss:1.2999, Validation Accuracy:0.3629\n",
    "Epoch #182: Loss:0.8542, Accuracy:0.6382, Validation Loss:1.3323, Validation Accuracy:0.3432\n",
    "Epoch #183: Loss:0.8495, Accuracy:0.6366, Validation Loss:1.3108, Validation Accuracy:0.3662\n",
    "Epoch #184: Loss:0.8384, Accuracy:0.6435, Validation Loss:1.3097, Validation Accuracy:0.3645\n",
    "Epoch #185: Loss:0.8344, Accuracy:0.6489, Validation Loss:1.3223, Validation Accuracy:0.3777\n",
    "Epoch #186: Loss:0.8406, Accuracy:0.6427, Validation Loss:1.3267, Validation Accuracy:0.3777\n",
    "Epoch #187: Loss:0.8381, Accuracy:0.6456, Validation Loss:1.3444, Validation Accuracy:0.3941\n",
    "Epoch #188: Loss:0.8451, Accuracy:0.6366, Validation Loss:1.3340, Validation Accuracy:0.3826\n",
    "Epoch #189: Loss:0.8485, Accuracy:0.6366, Validation Loss:1.3199, Validation Accuracy:0.3744\n",
    "Epoch #190: Loss:0.8427, Accuracy:0.6263, Validation Loss:1.3220, Validation Accuracy:0.3612\n",
    "Epoch #191: Loss:0.8343, Accuracy:0.6538, Validation Loss:1.3311, Validation Accuracy:0.3415\n",
    "Epoch #192: Loss:0.8361, Accuracy:0.6501, Validation Loss:1.3108, Validation Accuracy:0.3744\n",
    "Epoch #193: Loss:0.8227, Accuracy:0.6563, Validation Loss:1.3300, Validation Accuracy:0.3695\n",
    "Epoch #194: Loss:0.8177, Accuracy:0.6554, Validation Loss:1.3420, Validation Accuracy:0.3662\n",
    "Epoch #195: Loss:0.8166, Accuracy:0.6641, Validation Loss:1.3452, Validation Accuracy:0.3678\n",
    "Epoch #196: Loss:0.8186, Accuracy:0.6530, Validation Loss:1.3669, Validation Accuracy:0.3563\n",
    "Epoch #197: Loss:0.8219, Accuracy:0.6476, Validation Loss:1.3763, Validation Accuracy:0.3481\n",
    "Epoch #198: Loss:0.8085, Accuracy:0.6637, Validation Loss:1.3785, Validation Accuracy:0.3547\n",
    "Epoch #199: Loss:0.8277, Accuracy:0.6538, Validation Loss:1.3477, Validation Accuracy:0.3826\n",
    "Epoch #200: Loss:0.8110, Accuracy:0.6632, Validation Loss:1.3589, Validation Accuracy:0.3744\n",
    "Epoch #201: Loss:0.8104, Accuracy:0.6641, Validation Loss:1.3533, Validation Accuracy:0.3793\n",
    "Epoch #202: Loss:0.8093, Accuracy:0.6690, Validation Loss:1.3571, Validation Accuracy:0.3810\n",
    "Epoch #203: Loss:0.8254, Accuracy:0.6563, Validation Loss:1.3657, Validation Accuracy:0.3842\n",
    "Epoch #204: Loss:0.8317, Accuracy:0.6398, Validation Loss:1.3553, Validation Accuracy:0.3842\n",
    "Epoch #205: Loss:0.8188, Accuracy:0.6538, Validation Loss:1.3211, Validation Accuracy:0.3875\n",
    "Epoch #206: Loss:0.8087, Accuracy:0.6653, Validation Loss:1.3442, Validation Accuracy:0.3547\n",
    "Epoch #207: Loss:0.8024, Accuracy:0.6661, Validation Loss:1.3544, Validation Accuracy:0.3612\n",
    "Epoch #208: Loss:0.8040, Accuracy:0.6645, Validation Loss:1.3669, Validation Accuracy:0.3793\n",
    "Epoch #209: Loss:0.8015, Accuracy:0.6669, Validation Loss:1.3766, Validation Accuracy:0.3924\n",
    "Epoch #210: Loss:0.8151, Accuracy:0.6583, Validation Loss:1.3727, Validation Accuracy:0.3760\n",
    "Epoch #211: Loss:0.7924, Accuracy:0.6789, Validation Loss:1.3719, Validation Accuracy:0.3629\n",
    "Epoch #212: Loss:0.7858, Accuracy:0.6784, Validation Loss:1.3801, Validation Accuracy:0.3727\n",
    "Epoch #213: Loss:0.7817, Accuracy:0.6838, Validation Loss:1.3804, Validation Accuracy:0.3810\n",
    "Epoch #214: Loss:0.7904, Accuracy:0.6821, Validation Loss:1.4366, Validation Accuracy:0.3268\n",
    "Epoch #215: Loss:0.8025, Accuracy:0.6628, Validation Loss:1.4268, Validation Accuracy:0.3448\n",
    "Epoch #216: Loss:0.8035, Accuracy:0.6694, Validation Loss:1.3871, Validation Accuracy:0.3514\n",
    "Epoch #217: Loss:0.7957, Accuracy:0.6743, Validation Loss:1.3895, Validation Accuracy:0.3530\n",
    "Epoch #218: Loss:0.7837, Accuracy:0.6797, Validation Loss:1.3764, Validation Accuracy:0.3695\n",
    "Epoch #219: Loss:0.7802, Accuracy:0.6842, Validation Loss:1.3800, Validation Accuracy:0.3793\n",
    "Epoch #220: Loss:0.7723, Accuracy:0.6932, Validation Loss:1.4042, Validation Accuracy:0.3842\n",
    "Epoch #221: Loss:0.7811, Accuracy:0.6854, Validation Loss:1.4030, Validation Accuracy:0.3662\n",
    "Epoch #222: Loss:0.7819, Accuracy:0.6854, Validation Loss:1.4017, Validation Accuracy:0.3793\n",
    "Epoch #223: Loss:0.7861, Accuracy:0.6764, Validation Loss:1.4046, Validation Accuracy:0.3826\n",
    "Epoch #224: Loss:0.7780, Accuracy:0.6797, Validation Loss:1.3819, Validation Accuracy:0.3826\n",
    "Epoch #225: Loss:0.7617, Accuracy:0.6982, Validation Loss:1.3933, Validation Accuracy:0.3563\n",
    "Epoch #226: Loss:0.7623, Accuracy:0.6965, Validation Loss:1.4145, Validation Accuracy:0.3678\n",
    "Epoch #227: Loss:0.7596, Accuracy:0.6977, Validation Loss:1.4095, Validation Accuracy:0.3810\n",
    "Epoch #228: Loss:0.7611, Accuracy:0.6916, Validation Loss:1.4295, Validation Accuracy:0.3629\n",
    "Epoch #229: Loss:0.7625, Accuracy:0.6936, Validation Loss:1.4278, Validation Accuracy:0.3662\n",
    "Epoch #230: Loss:0.7559, Accuracy:0.6961, Validation Loss:1.4362, Validation Accuracy:0.3629\n",
    "Epoch #231: Loss:0.7466, Accuracy:0.7105, Validation Loss:1.4359, Validation Accuracy:0.3481\n",
    "Epoch #232: Loss:0.7450, Accuracy:0.7117, Validation Loss:1.4350, Validation Accuracy:0.3662\n",
    "Epoch #233: Loss:0.7545, Accuracy:0.6998, Validation Loss:1.4852, Validation Accuracy:0.3530\n",
    "Epoch #234: Loss:0.7542, Accuracy:0.6920, Validation Loss:1.4294, Validation Accuracy:0.3580\n",
    "Epoch #235: Loss:0.7484, Accuracy:0.7018, Validation Loss:1.4207, Validation Accuracy:0.3826\n",
    "Epoch #236: Loss:0.7584, Accuracy:0.6998, Validation Loss:1.4177, Validation Accuracy:0.3859\n",
    "Epoch #237: Loss:0.7506, Accuracy:0.7072, Validation Loss:1.4205, Validation Accuracy:0.3629\n",
    "Epoch #238: Loss:0.7492, Accuracy:0.7006, Validation Loss:1.4350, Validation Accuracy:0.3580\n",
    "Epoch #239: Loss:0.7366, Accuracy:0.7031, Validation Loss:1.4424, Validation Accuracy:0.3629\n",
    "Epoch #240: Loss:0.7394, Accuracy:0.7072, Validation Loss:1.4607, Validation Accuracy:0.3498\n",
    "Epoch #241: Loss:0.7330, Accuracy:0.7084, Validation Loss:1.4467, Validation Accuracy:0.3695\n",
    "Epoch #242: Loss:0.7356, Accuracy:0.7228, Validation Loss:1.4817, Validation Accuracy:0.3514\n",
    "Epoch #243: Loss:0.7476, Accuracy:0.7010, Validation Loss:1.4635, Validation Accuracy:0.3859\n",
    "Epoch #244: Loss:0.7656, Accuracy:0.6891, Validation Loss:1.4814, Validation Accuracy:0.3415\n",
    "Epoch #245: Loss:0.7541, Accuracy:0.6969, Validation Loss:1.4377, Validation Accuracy:0.3744\n",
    "Epoch #246: Loss:0.7647, Accuracy:0.6784, Validation Loss:1.4058, Validation Accuracy:0.3777\n",
    "Epoch #247: Loss:0.7458, Accuracy:0.6982, Validation Loss:1.4145, Validation Accuracy:0.3826\n",
    "Epoch #248: Loss:0.7380, Accuracy:0.7092, Validation Loss:1.4180, Validation Accuracy:0.3908\n",
    "Epoch #249: Loss:0.7517, Accuracy:0.6936, Validation Loss:1.4311, Validation Accuracy:0.3760\n",
    "Epoch #250: Loss:0.7464, Accuracy:0.6982, Validation Loss:1.4271, Validation Accuracy:0.3727\n",
    "Epoch #251: Loss:0.7226, Accuracy:0.7158, Validation Loss:1.4369, Validation Accuracy:0.3695\n",
    "Epoch #252: Loss:0.7205, Accuracy:0.7150, Validation Loss:1.4383, Validation Accuracy:0.3810\n",
    "Epoch #253: Loss:0.7317, Accuracy:0.7101, Validation Loss:1.4343, Validation Accuracy:0.3859\n",
    "Epoch #254: Loss:0.7200, Accuracy:0.7175, Validation Loss:1.4528, Validation Accuracy:0.3777\n",
    "Epoch #255: Loss:0.7320, Accuracy:0.7121, Validation Loss:1.4470, Validation Accuracy:0.3760\n",
    "Epoch #256: Loss:0.7350, Accuracy:0.7097, Validation Loss:1.4513, Validation Accuracy:0.3924\n",
    "Epoch #257: Loss:0.7306, Accuracy:0.7105, Validation Loss:1.4399, Validation Accuracy:0.3810\n",
    "Epoch #258: Loss:0.7333, Accuracy:0.7035, Validation Loss:1.4329, Validation Accuracy:0.3842\n",
    "Epoch #259: Loss:0.7340, Accuracy:0.7060, Validation Loss:1.4327, Validation Accuracy:0.3842\n",
    "Epoch #260: Loss:0.7122, Accuracy:0.7314, Validation Loss:1.4590, Validation Accuracy:0.3875\n",
    "Epoch #261: Loss:0.7214, Accuracy:0.7142, Validation Loss:1.4556, Validation Accuracy:0.3695\n",
    "Epoch #262: Loss:0.7082, Accuracy:0.7294, Validation Loss:1.5051, Validation Accuracy:0.3448\n",
    "Epoch #263: Loss:0.7522, Accuracy:0.6986, Validation Loss:1.4747, Validation Accuracy:0.3645\n",
    "Epoch #264: Loss:0.7285, Accuracy:0.7109, Validation Loss:1.4857, Validation Accuracy:0.3711\n",
    "Epoch #265: Loss:0.7499, Accuracy:0.6867, Validation Loss:1.4767, Validation Accuracy:0.3662\n",
    "Epoch #266: Loss:0.7101, Accuracy:0.7228, Validation Loss:1.4908, Validation Accuracy:0.3629\n",
    "Epoch #267: Loss:0.7005, Accuracy:0.7277, Validation Loss:1.4768, Validation Accuracy:0.3563\n",
    "Epoch #268: Loss:0.6939, Accuracy:0.7372, Validation Loss:1.4824, Validation Accuracy:0.3596\n",
    "Epoch #269: Loss:0.6866, Accuracy:0.7425, Validation Loss:1.5045, Validation Accuracy:0.3678\n",
    "Epoch #270: Loss:0.6833, Accuracy:0.7380, Validation Loss:1.5225, Validation Accuracy:0.3563\n",
    "Epoch #271: Loss:0.6833, Accuracy:0.7491, Validation Loss:1.5209, Validation Accuracy:0.3744\n",
    "Epoch #272: Loss:0.6781, Accuracy:0.7499, Validation Loss:1.5393, Validation Accuracy:0.3662\n",
    "Epoch #273: Loss:0.6864, Accuracy:0.7372, Validation Loss:1.5025, Validation Accuracy:0.3744\n",
    "Epoch #274: Loss:0.6806, Accuracy:0.7437, Validation Loss:1.5118, Validation Accuracy:0.3695\n",
    "Epoch #275: Loss:0.6780, Accuracy:0.7450, Validation Loss:1.5107, Validation Accuracy:0.3645\n",
    "Epoch #276: Loss:0.6745, Accuracy:0.7483, Validation Loss:1.5436, Validation Accuracy:0.3629\n",
    "Epoch #277: Loss:0.6809, Accuracy:0.7417, Validation Loss:1.5142, Validation Accuracy:0.3810\n",
    "Epoch #278: Loss:0.7118, Accuracy:0.7216, Validation Loss:1.5056, Validation Accuracy:0.3695\n",
    "Epoch #279: Loss:0.7049, Accuracy:0.7170, Validation Loss:1.5008, Validation Accuracy:0.3826\n",
    "Epoch #280: Loss:0.6923, Accuracy:0.7195, Validation Loss:1.4887, Validation Accuracy:0.3678\n",
    "Epoch #281: Loss:0.6769, Accuracy:0.7302, Validation Loss:1.5118, Validation Accuracy:0.3612\n",
    "Epoch #282: Loss:0.6759, Accuracy:0.7368, Validation Loss:1.5648, Validation Accuracy:0.3563\n",
    "Epoch #283: Loss:0.6598, Accuracy:0.7577, Validation Loss:1.5468, Validation Accuracy:0.3530\n",
    "Epoch #284: Loss:0.6595, Accuracy:0.7524, Validation Loss:1.5639, Validation Accuracy:0.3563\n",
    "Epoch #285: Loss:0.6600, Accuracy:0.7528, Validation Loss:1.5577, Validation Accuracy:0.3629\n",
    "Epoch #286: Loss:0.6587, Accuracy:0.7511, Validation Loss:1.5297, Validation Accuracy:0.3678\n",
    "Epoch #287: Loss:0.6499, Accuracy:0.7548, Validation Loss:1.5574, Validation Accuracy:0.3711\n",
    "Epoch #288: Loss:0.6578, Accuracy:0.7524, Validation Loss:1.5276, Validation Accuracy:0.3842\n",
    "Epoch #289: Loss:0.6747, Accuracy:0.7347, Validation Loss:1.5277, Validation Accuracy:0.3793\n",
    "Epoch #290: Loss:0.6577, Accuracy:0.7540, Validation Loss:1.6624, Validation Accuracy:0.3300\n",
    "Epoch #291: Loss:0.7005, Accuracy:0.7244, Validation Loss:1.5547, Validation Accuracy:0.3662\n",
    "Epoch #292: Loss:0.6543, Accuracy:0.7569, Validation Loss:1.5464, Validation Accuracy:0.3662\n",
    "Epoch #293: Loss:0.6480, Accuracy:0.7561, Validation Loss:1.6304, Validation Accuracy:0.3448\n",
    "Epoch #294: Loss:0.6553, Accuracy:0.7540, Validation Loss:1.5659, Validation Accuracy:0.3596\n",
    "Epoch #295: Loss:0.6392, Accuracy:0.7598, Validation Loss:1.5785, Validation Accuracy:0.3744\n",
    "Epoch #296: Loss:0.6591, Accuracy:0.7487, Validation Loss:1.6061, Validation Accuracy:0.3547\n",
    "Epoch #297: Loss:0.6783, Accuracy:0.7396, Validation Loss:1.5521, Validation Accuracy:0.3777\n",
    "Epoch #298: Loss:0.6905, Accuracy:0.7314, Validation Loss:1.5091, Validation Accuracy:0.3892\n",
    "Epoch #299: Loss:0.6815, Accuracy:0.7253, Validation Loss:1.5158, Validation Accuracy:0.3859\n",
    "Epoch #300: Loss:0.6667, Accuracy:0.7450, Validation Loss:1.4988, Validation Accuracy:0.3859\n",
    "\n",
    "Test:\n",
    "Test Loss:1.49884129, Accuracy:0.3859\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  103  111  26\n",
    "t:02   93  112  22\n",
    "t:03   48   74  20\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.42      0.43      0.43       240\n",
    "          02       0.38      0.49      0.43       227\n",
    "          03       0.29      0.14      0.19       142\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.36      0.35      0.35       609\n",
    "weighted avg       0.38      0.39      0.37       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 08:54:14 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0753381500886188, 1.0757373309096288, 1.0752759563120324, 1.0744310366892071, 1.0744264168888087, 1.0745135371516688, 1.0744037307150454, 1.074340941870741, 1.0743487237513751, 1.074357310930888, 1.0743087442050427, 1.0742116647792372, 1.0741963443301974, 1.074364053008983, 1.0740105847419776, 1.0739473069242655, 1.0739924406574668, 1.0741726285326854, 1.0740319324048673, 1.073777037496833, 1.0736706885210987, 1.0736830181676178, 1.07369519650251, 1.0734731021577306, 1.0733208801163046, 1.07329292504854, 1.0733065344821449, 1.073396521248841, 1.0730634777025245, 1.0731845391403474, 1.0734468015348186, 1.0731805130374452, 1.073234551450106, 1.0746636606006592, 1.0736644821996955, 1.0735432422415572, 1.073484246170971, 1.073748001715624, 1.0742687695523592, 1.0748612438320917, 1.0759411588286727, 1.0768939691224122, 1.0770306890625476, 1.0778258760965909, 1.079139340295776, 1.081615988452642, 1.0846822148277646, 1.0855078554310038, 1.0821286430108332, 1.0834039361606091, 1.0830683514402417, 1.0796739399335262, 1.079353719509294, 1.082331155908519, 1.0827464522986576, 1.0825705348172994, 1.0825060698003408, 1.0840081720320853, 1.0851458780871237, 1.0843269975706078, 1.085838428663307, 1.085967271590272, 1.0880249077071893, 1.0872594288417272, 1.0865901350387799, 1.0854861297826657, 1.0881051976105263, 1.08955645913561, 1.086812182208783, 1.0886413726117614, 1.0953621697934781, 1.0908790376581778, 1.0945548063820023, 1.0936667668604108, 1.0955863713435157, 1.0998274375652444, 1.1010182158308859, 1.1074979777015097, 1.1042236406814876, 1.1016719169021632, 1.0988438495469994, 1.1049223044039973, 1.1086936920930208, 1.1099299857964853, 1.1173639473656716, 1.11116629692134, 1.112552153065874, 1.1312054861551044, 1.1177587313409314, 1.121643837645332, 1.123535472771217, 1.138422166772664, 1.1327844163271399, 1.1376673638918522, 1.143642256608346, 1.1384261484412332, 1.152144730188968, 1.15399611642208, 1.1521218247797298, 1.1559604271292099, 1.151298858262048, 1.1287389494515405, 1.1291456958538988, 1.1260180541838722, 1.1386505989801317, 1.1574565170237976, 1.1806054984407472, 1.1931751956689143, 1.1677322910337025, 1.1527731209161443, 1.1639252762097638, 1.150699735666535, 1.1590829766638369, 1.1690642549878074, 1.1833171783801175, 1.1954369543025452, 1.180844345116263, 1.1987873619217395, 1.1725598231129262, 1.1848480969618498, 1.1689222559748809, 1.1602643199742133, 1.1718814093099634, 1.180265607700755, 1.1859248258014423, 1.1963323628765412, 1.2134787124170263, 1.1679882558891534, 1.1818907395959488, 1.2189689291326087, 1.2065532929791605, 1.238833277487794, 1.2300669250425642, 1.2516171914603322, 1.2394630742581998, 1.200353312374923, 1.227044003741886, 1.229222981017603, 1.2197459896992775, 1.2308373590212542, 1.2647561936934397, 1.2421538694738754, 1.232115880609146, 1.2225944140470282, 1.2255725604168495, 1.2408785191662792, 1.2339265072482757, 1.2526577409656567, 1.2663717986327674, 1.2811690142197758, 1.2691022067625926, 1.2715918957110501, 1.2521464331396694, 1.2416327260006432, 1.2458968422878747, 1.275887166142268, 1.2335294849179648, 1.2521803406463272, 1.2063263100747796, 1.2327720343773, 1.2693309241719237, 1.2802881883282966, 1.2999468546587063, 1.2969299272950647, 1.289977403892868, 1.272108310940622, 1.2766514114166911, 1.2973342145409295, 1.2630834385679273, 1.2752378852105102, 1.2754169878701271, 1.2658979724389188, 1.2775425891375112, 1.2858956531546581, 1.2969857924090231, 1.3297996066865467, 1.3112995244795074, 1.3143952724773114, 1.3026504886561427, 1.3005076521527394, 1.2999202220506465, 1.3322639704142103, 1.3108350954619534, 1.3096975935699513, 1.3223082267592106, 1.3267173796451737, 1.344394984307939, 1.3340167770244804, 1.3198619468263022, 1.321966441394073, 1.3311370910682114, 1.3107602418154136, 1.3300441288204223, 1.341975323280873, 1.3452188498868143, 1.3668550558278125, 1.3762547098748594, 1.3784570472776791, 1.3476765862435152, 1.3588908274576974, 1.3533212733386186, 1.3571055003966408, 1.3657461782590117, 1.3552995293794203, 1.3211116289661826, 1.3442324901057778, 1.3544130307700246, 1.366930998800619, 1.3765522067378504, 1.372714068110549, 1.371941386185256, 1.3800661894683963, 1.3804185764347194, 1.4366009875471368, 1.4267571373721846, 1.3871214454397192, 1.389485122926521, 1.3763616353224455, 1.3799796734733143, 1.4041693643200377, 1.402951751045014, 1.4017063306861715, 1.40460800967976, 1.381937457031413, 1.3932650138200406, 1.4145363962709023, 1.4095068702165325, 1.4294669904145114, 1.4277609651311864, 1.4361513939201342, 1.4359214027918423, 1.4350224228328086, 1.4852370276239706, 1.4293984063153196, 1.420701504928138, 1.417661637899715, 1.4204802902657019, 1.4349694179588155, 1.442394962060236, 1.4606873253100416, 1.4467296692342397, 1.4816807394935971, 1.4635053618592386, 1.4814113779804, 1.4376867659181993, 1.4058106795124623, 1.4145229612469477, 1.4179698943308814, 1.4310600673428113, 1.4271371358721128, 1.4369006233262311, 1.4382583989298403, 1.4343199994176479, 1.452826537913681, 1.4470407025175924, 1.4512653992876827, 1.4399318027574637, 1.4329091835100272, 1.432688261683547, 1.4590490179500362, 1.455582876706554, 1.5050971901475503, 1.474683234257064, 1.4857362557710294, 1.4766992837533184, 1.4907637889357819, 1.4768372310206221, 1.4823871825520432, 1.504511750194631, 1.5224517000524085, 1.5209241588714675, 1.5392635864968762, 1.5025384201009089, 1.5117915121009589, 1.510702932800957, 1.543570373837388, 1.5141897005792126, 1.5055666261510112, 1.5008487760139804, 1.4886563043484742, 1.5118316008735369, 1.56478651286346, 1.5467707224080127, 1.563901824904193, 1.5576784687089216, 1.5296988076177136, 1.5574241579068313, 1.5275723493745175, 1.5276612299807946, 1.6624317887577125, 1.5547497967389612, 1.5464160640055715, 1.6303862979259398, 1.5659116533980972, 1.5784676642644972, 1.6061262837771713, 1.5520898064564796, 1.5090598187031612, 1.5157837045603786, 1.4988413831870544], 'val_acc': [0.37274219920286794, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.40229885013428424, 0.39408866892307265, 0.39408866882519966, 0.39408866882519966, 0.39080459667348316, 0.39080459667348316, 0.3990147772974569, 0.4039408858675871, 0.39901477788469475, 0.40558292189450884, 0.40229884954704637, 0.4006568135201246, 0.4006568140094895, 0.403940886161206, 0.4039408857697141, 0.3990147772974569, 0.40558292228600074, 0.4121510665894338, 0.3990147774932028, 0.4039408857697141, 0.4154351388390233, 0.3957307049499944, 0.41050903026889307, 0.40886699414409833, 0.39737274097691616, 0.4072249577256846, 0.3990147770038379, 0.40394088537822215, 0.410509030562512, 0.38916256054868836, 0.4137931021269906, 0.40722495762781163, 0.4055829214051439, 0.41215106619794184, 0.4006568129328867, 0.40065681322650565, 0.385878487907607, 0.39244663240678596, 0.385878487613988, 0.3940886685315807, 0.38423645158706626, 0.39244663221104, 0.3760262712567115, 0.3908045961841182, 0.38916256015719647, 0.38752052373878276, 0.38423645178281224, 0.3875205241302747, 0.39244663260253193, 0.37602627135458444, 0.38095237953322275, 0.3809523797289687, 0.3825944157558905, 0.3678160905348648, 0.37274219910499495, 0.39573070475424843, 0.3891625600593235, 0.4006568130307597, 0.3875205240324017, 0.4072249577256846, 0.38095237953322275, 0.38916256035294244, 0.3760262711588385, 0.4022988491555544, 0.3908045964777372, 0.38423645168493925, 0.37274219890924903, 0.37931034321268203, 0.38423645217430413, 0.3842364519785582, 0.3825944157558905, 0.38916256064656135, 0.39573070446062947, 0.37602627106096553, 0.3743842348382978, 0.39244663250465894, 0.39244663250465894, 0.40065681312863266, 0.36781609014337285, 0.39244663250465894, 0.37602627096309255, 0.379310343408428, 0.3908045964777372, 0.3809523797289687, 0.379310343310555, 0.37766830738150625, 0.38752052393452874, 0.379310343604174, 0.3875205245217666, 0.38259441585376347, 0.37766830767512516, 0.39244663250465894, 0.3957307050478674, 0.3579638737860963, 0.39408866892307265, 0.4006568130307597, 0.38095237963109574, 0.3743842353276627, 0.37766830757725217, 0.3842364523700501, 0.40065681361799754, 0.39244663260253193, 0.3924466327982779, 0.3875205241302747, 0.3711001628823273, 0.3875205243260207, 0.3793103438977929, 0.3760262716482034, 0.37274219920286794, 0.39080459667348316, 0.39408866892307265, 0.37602627135458444, 0.32840722305042597, 0.38587848810335296, 0.38095237963109574, 0.3546798017322528, 0.37766830747937924, 0.3825944159516364, 0.3678160908284837, 0.37274219910499495, 0.37602627096309255, 0.38916256054868836, 0.37274219920286794, 0.3793103437020469, 0.40065681312863266, 0.3612479455463209, 0.3743842350340438, 0.36945812656178656, 0.39244663250465894, 0.3990147771995839, 0.36781609033911883, 0.4088669942419713, 0.3940886686294537, 0.3842364520764312, 0.37438423513191676, 0.38259441536439853, 0.38916256074443434, 0.36453201818740233, 0.3842364518806852, 0.36453201848102124, 0.36945812685540547, 0.36453201848102124, 0.366174054507943, 0.3875205241302747, 0.39244663260253193, 0.379310343408428, 0.39573070475424843, 0.38259441585376347, 0.3596059096172721, 0.3760262712567115, 0.36124794613355876, 0.39244663240678596, 0.3875205242281477, 0.38095237953322275, 0.3743842352297897, 0.3711001628823273, 0.34811165664583593, 0.36617405441007006, 0.3875205241302747, 0.3940886687273267, 0.3940886687273267, 0.36617405441007006, 0.3661740542143241, 0.3711001626865813, 0.36945812646391357, 0.36945812665965955, 0.3711001630780732, 0.379310343604174, 0.35632183756342856, 0.3743842352297897, 0.3957307049499944, 0.3809523797289687, 0.39408866882519966, 0.3908045963798642, 0.37766830728363326, 0.3628899822583535, 0.3431855482714517, 0.366174054507943, 0.36453201838314825, 0.37766830777299815, 0.37766830738150625, 0.3940886687273267, 0.3825944157558905, 0.3743842350340438, 0.36124794593781284, 0.34154351214665696, 0.3743842350340438, 0.36945812656178656, 0.3661740543121971, 0.3678160905348648, 0.35632183756342856, 0.34811165664583593, 0.3546798013407609, 0.3825944157558905, 0.3743842352297897, 0.379310343604174, 0.38095237943534976, 0.3842364519785582, 0.3842364519785582, 0.3875205241302747, 0.3546798013407609, 0.36124794593781284, 0.379310343506301, 0.3924466327982779, 0.3760262715503304, 0.3628899820626076, 0.37274219871350306, 0.38095237953322275, 0.3267651866320123, 0.3448275844941194, 0.3513957289932984, 0.35303776531383907, 0.36945812665965955, 0.3793103438977929, 0.3842364520764312, 0.36617405441007006, 0.379310343310555, 0.3825944156580175, 0.3825944156580175, 0.3563218374655556, 0.3678160904369918, 0.38095237963109574, 0.3628899821604805, 0.3661740542143241, 0.3628899820626076, 0.3481116567437089, 0.36617405441007006, 0.35303776511809315, 0.3579638737860963, 0.3825944159516364, 0.385878487809734, 0.3628899817689886, 0.3579638736882233, 0.3628899819647346, 0.34975369286850366, 0.36945812685540547, 0.35139572879755243, 0.3858784882990989, 0.341543511853038, 0.3743842347404248, 0.37766830747937924, 0.3825944159516364, 0.3908045965756102, 0.37602627106096553, 0.372742199007122, 0.36945812665965955, 0.3809523797289687, 0.38587848800547997, 0.37766830728363326, 0.37602627135458444, 0.3924466329940238, 0.38095237953322275, 0.3842364518806852, 0.3842364520764312, 0.3875205245217666, 0.36945812685540547, 0.3448275844941194, 0.36453201828527526, 0.3711001628823273, 0.366174054605816, 0.3628899820626076, 0.35632183756342856, 0.3596059097151451, 0.3678160907306107, 0.35632183736768264, 0.3743842352297897, 0.366174054605816, 0.3743842353276627, 0.36945812685540547, 0.36453201828527526, 0.3628899822583535, 0.3809523797289687, 0.36945812656178656, 0.3825944160495094, 0.3678160907306107, 0.36124794593781284, 0.35632183756342856, 0.35303776521596614, 0.35632183775917453, 0.3628899821604805, 0.3678160905348648, 0.3711001632738192, 0.38423645217430413, 0.379310343506301, 0.3300492590773478, 0.36617405441007006, 0.36617405470368897, 0.34482758459199236, 0.3596059098130181, 0.3743842353276627, 0.3546798016343798, 0.37766830747937924, 0.38916256054868836, 0.38587848839697186, 0.38587848820122594], 'loss': [1.084795119385455, 1.0746013777456733, 1.0750376405657194, 1.0745308864777583, 1.0739599656764975, 1.0739752701910126, 1.073826842386375, 1.07364196831196, 1.0736701600115892, 1.0737706420357958, 1.0735892827995503, 1.0734394105797676, 1.0732546125838889, 1.0733452594255766, 1.0730500154181917, 1.0729792814235177, 1.073266143084062, 1.0729891380000653, 1.0730244122981045, 1.0725247866808756, 1.072671694285571, 1.0722781628309088, 1.072115060876772, 1.071996471230744, 1.072029256184243, 1.0715793917556073, 1.071598090870914, 1.071278916443153, 1.0713161663353077, 1.0712174489512825, 1.0708762519413442, 1.0705579132514813, 1.0709466939589327, 1.070957759175702, 1.07023435084482, 1.0698179433967543, 1.0684589407527227, 1.0682857124222866, 1.067503011300089, 1.0661643566781736, 1.0664921874138364, 1.0662742608871303, 1.064833393380872, 1.0634991294794258, 1.062408842785892, 1.0618431093266858, 1.0628140701160784, 1.0638434801258345, 1.0630124381925046, 1.0609324569330079, 1.060112894389174, 1.0606601948610812, 1.0610151275717012, 1.0577540260076033, 1.059025454765962, 1.0582146375331056, 1.0570621250101673, 1.0556575867674434, 1.054632323527483, 1.0566109538323092, 1.0557460192537407, 1.0563570103850943, 1.0546063812361606, 1.0536107716374328, 1.055448513158293, 1.0557017431122078, 1.0503807793407713, 1.0488440663907561, 1.0491593251238125, 1.0492025367533158, 1.043895777148143, 1.0450873433197303, 1.0424185898759282, 1.0402653856688702, 1.0370858232098683, 1.0350611568231602, 1.0355681131018262, 1.0335847975047463, 1.0364295249112578, 1.032725452446595, 1.028583330148544, 1.0276063242976916, 1.0270747034456695, 1.0243214726203276, 1.0272141174124496, 1.020230831649514, 1.016036311899612, 1.0211371503081899, 1.0140964180047507, 1.0104617550142982, 1.0056355551772538, 1.0024455614403287, 1.0036956499734209, 0.9972974876358769, 0.9989245685708597, 0.9931659596656627, 0.994627719787112, 0.9920019548531676, 0.9914490213629157, 0.9930688935383634, 0.9928047956627253, 1.011451846520269, 0.9890066803603201, 0.9855838998387236, 0.9773853583502329, 0.9741444777659077, 0.9731825268978456, 0.9852451128146976, 0.9732592405969357, 0.9754144540312844, 0.9741958637746697, 0.9687830585718644, 0.9614890477006196, 0.9537650278216759, 0.956071227360555, 0.9554012773218096, 0.9615563103795296, 0.9591866443778945, 0.9671119723231886, 0.9714631556485468, 0.9680577466375284, 0.9584752028482896, 0.9543382744524758, 0.9493982884918151, 0.9542216684783998, 0.9535988815511277, 0.9560610326653388, 0.9462674315949975, 0.9366960993048102, 0.9313669819361865, 0.9301762217858489, 0.9230016419530159, 0.9236652011254485, 0.925567650305417, 0.9346192660517761, 0.9227267616583337, 0.928055081132501, 0.924072094595163, 0.9207659030597068, 0.9114996401436275, 0.9186586232890339, 0.9314421061372855, 0.9208994561641858, 0.916608316452841, 0.9062681017715094, 0.9136228985120628, 0.9004310930778848, 0.8949433031268188, 0.8947873570101462, 0.8963656026969455, 0.8963291541262084, 0.9002680915826645, 0.9045099702459096, 0.8972895714781367, 0.8968696880634316, 0.9069137131898555, 0.9319659521692343, 0.9088300617568547, 0.8994460131598204, 0.8867934851431014, 0.8806371756158081, 0.8767167377765663, 0.8782193628914302, 0.8783951118007088, 0.8823610069325817, 0.8704004965278892, 0.8731728905035485, 0.8918311870073635, 0.8891719985791545, 0.8755013807108759, 0.8850942370093579, 0.8760533545051512, 0.861063346187192, 0.860452990365469, 0.851028977625179, 0.8476828342345706, 0.872805182938703, 0.8612175095007895, 0.8577334340103353, 0.845047619112708, 0.8537038313289932, 0.8542145692592285, 0.8494575891161846, 0.8383687289588505, 0.8343828926341, 0.8406177700912194, 0.8381365589537415, 0.8450961684054663, 0.8485026485376534, 0.8426864688156567, 0.8343055404676794, 0.8361084485690452, 0.8226774524369523, 0.8176802923302386, 0.816601464342043, 0.8186204098822889, 0.8218816896238855, 0.8084871978240826, 0.827681734062563, 0.8109743810532274, 0.8104057908792516, 0.8092782997007977, 0.8254284616123724, 0.8317382042657668, 0.8188173882035994, 0.8087292008086642, 0.802448526030938, 0.8039933810733427, 0.8014559180095211, 0.8150514223737149, 0.7924381122941598, 0.7858475936266921, 0.7817374134455374, 0.7903966972960094, 0.8024831621798647, 0.8034971395312394, 0.7957253355265154, 0.7836995020049798, 0.7802188934976315, 0.772300406109381, 0.7810913832035887, 0.7819038877496974, 0.7860799587972355, 0.7779643676853767, 0.761743364197028, 0.7623425644770785, 0.7595993617966434, 0.7610635642398309, 0.762467723827832, 0.755948111849399, 0.746611691916503, 0.7449792670763004, 0.7545390105590194, 0.7542076115490719, 0.7483972046654327, 0.7584049157048642, 0.7505674539160679, 0.7491975403419511, 0.7365824886416018, 0.7394451553571885, 0.7329829196176, 0.7356131392827514, 0.7475542938195214, 0.7655735906634242, 0.7541054357003872, 0.7647396739993008, 0.7458374416314111, 0.7380291721659274, 0.7516807230835822, 0.7463798734441675, 0.7225784992045691, 0.7205147026500662, 0.7316534177233796, 0.7199984246700452, 0.7319738831852985, 0.7349664606108068, 0.7306448580303231, 0.7332898729146139, 0.7340104890799866, 0.7122144325068355, 0.7214132365994385, 0.708243283721211, 0.7522301885381616, 0.7285173575491386, 0.7499433650128405, 0.7101036954464609, 0.700516784558306, 0.6938796824743126, 0.6866088614326727, 0.6833407718298127, 0.6833278897117051, 0.6780865703030533, 0.6864377510865856, 0.6805719172195732, 0.6780038781479398, 0.6744523046197832, 0.6808770255631245, 0.7117849363683431, 0.7048835623925227, 0.692319975716867, 0.6768847198701737, 0.6758626539359592, 0.6597973117593378, 0.6594599346115848, 0.6600150817228784, 0.6587053509953086, 0.6498763542645276, 0.6577854744462751, 0.6746571221146006, 0.6576727905312603, 0.7005293623133116, 0.6543195516911375, 0.6479584560501992, 0.6553322302976917, 0.639171374039973, 0.6590532812249734, 0.6783276989964244, 0.6905131093775222, 0.6815362206229929, 0.6667158216421609], 'acc': [0.3712525661231556, 0.3798767968859271, 0.3942505157091779, 0.394250514301676, 0.394250514301676, 0.3885010280403513, 0.4045174541292249, 0.39712525821564376, 0.3942505123066951, 0.39548254833084356, 0.3926078024586123, 0.3934291594571891, 0.3954825471558855, 0.4024640672505514, 0.4024640662347022, 0.39794661364760975, 0.3987679681371614, 0.4020533872825654, 0.4073921992548682, 0.4090349095313211, 0.3983572906782005, 0.39466119152809315, 0.4020533874416743, 0.4078028755021536, 0.4024640676422041, 0.41149897543556635, 0.40616016264324073, 0.4065708408487896, 0.41601642704352704, 0.4057494878034572, 0.4106776197710566, 0.4123203276975933, 0.4061601630348934, 0.4073921976882574, 0.4135523599643237, 0.416837781888014, 0.41971252541032905, 0.4143737159470513, 0.4242299782299653, 0.42628336553700896, 0.4213552350993029, 0.4258726902688553, 0.4291581100384558, 0.4340862408678145, 0.4279260791425098, 0.43531827509280835, 0.4336755663829662, 0.4324435325863425, 0.42997946746540266, 0.42669404315507875, 0.4271047233188911, 0.4324435296122298, 0.4344969218149322, 0.43942505146933286, 0.4369609845859559, 0.4340862424344253, 0.44229979518747425, 0.44394250546392716, 0.4488706366849386, 0.45092402692937755, 0.43819301642431613, 0.43696098517343496, 0.43942505104096274, 0.4505133459455424, 0.43983572869575, 0.44476386030841414, 0.44394250546392716, 0.4554414789289909, 0.45133470258918384, 0.45995893135697447, 0.4628336753076596, 0.462422996869567, 0.4607802897630531, 0.46940451778425574, 0.47515400420468934, 0.47268993990377234, 0.4747433254483789, 0.4731006175218422, 0.4710472274365122, 0.4804928110855071, 0.4796714587867627, 0.48090348889940326, 0.48336755460782216, 0.48336755836524026, 0.49240246279283717, 0.4915811077525239, 0.49486652771795064, 0.49363449662617837, 0.49979466011392015, 0.50225872781732, 0.5137576997157729, 0.5112936359656175, 0.5100615981423144, 0.5240246361775565, 0.5129363483961603, 0.5190965085548542, 0.5158110899969293, 0.5145790565185233, 0.521971253056301, 0.5227926071174825, 0.5256673527938874, 0.5047227930973688, 0.5227926106423568, 0.5215605779839737, 0.5338809051552837, 0.5347022609789024, 0.5388090387262113, 0.5215605772006683, 0.5408624248582969, 0.5199178665325627, 0.5375770039137384, 0.5437371628240394, 0.5412731016930613, 0.5482546247251224, 0.5523613918243737, 0.5437371679889593, 0.5490759764363878, 0.5478439431905257, 0.5482546221793799, 0.5351129330403995, 0.5359342944205909, 0.5507186900418887, 0.5478439443654838, 0.5482546196336374, 0.5544147868420799, 0.5404517462610954, 0.544969203193085, 0.5540041072657466, 0.5708418933273096, 0.5659137530248513, 0.5659137601480347, 0.5794661163794187, 0.5691991759276733, 0.5806981551818534, 0.5700205371120383, 0.574127307344511, 0.5811088333874023, 0.577002050279347, 0.5765913803719397, 0.5864476352991265, 0.5827515400655461, 0.570020532338771, 0.5716632408527867, 0.575770018208443, 0.5983572877897619, 0.5823408642833483, 0.5971252545438998, 0.6086242258915912, 0.6016427087343205, 0.5979466125216083, 0.6032854186191207, 0.5901437407891119, 0.5897330637585211, 0.5930184783632017, 0.6012320374561286, 0.5913757667159643, 0.5655030754067815, 0.5802874702447739, 0.5917864472714293, 0.6078028739844994, 0.6119096493818922, 0.6143737148944847, 0.6188911661475102, 0.6135523622040876, 0.6106776194650779, 0.616837781973688, 0.6160164286958119, 0.6016427095176258, 0.5958932214938639, 0.6061601607706513, 0.6061601658621363, 0.6086242294164653, 0.6275154002393296, 0.6234086236669787, 0.6291581081658663, 0.6320328556047083, 0.6147843923167282, 0.6279260774657467, 0.6250513343350843, 0.6414784425827512, 0.6377823408869013, 0.6381930185049711, 0.6365503080326919, 0.6435318279315314, 0.6488706377497444, 0.6427104740661762, 0.645585217980144, 0.6365503113617398, 0.6365503074452128, 0.6262833658185093, 0.6537987660333606, 0.650102672366391, 0.656262834287522, 0.6554414814012985, 0.6640657123598964, 0.6529774166720114, 0.6476386056788404, 0.6636550276186431, 0.653798771908151, 0.6632443543821879, 0.6640657096183276, 0.6689938420142971, 0.6562628368332646, 0.6398357272148132, 0.6537987724956301, 0.6652977382867488, 0.6661191012335509, 0.6644763899779662, 0.6669404558822115, 0.6583162249236136, 0.678850105631278, 0.6784394212816774, 0.6837782374397685, 0.6821355283382737, 0.6628336735574617, 0.669404521394804, 0.6743326498742466, 0.6796714620423757, 0.6841889144703592, 0.6932238222637216, 0.6854209486953532, 0.6854209415721697, 0.6763860379645956, 0.679671461063244, 0.6981519496905975, 0.6965092360850966, 0.6977412699184379, 0.6915811044724325, 0.6936345000776177, 0.6960985673526474, 0.7104722758827757, 0.7117043110869014, 0.6997946639570123, 0.6919917844404185, 0.7018480530264931, 0.699794664936144, 0.7071868578756125, 0.7006160189973255, 0.7030800837266127, 0.7071868630405325, 0.7084188899465166, 0.7227926073622655, 0.7010266904713437, 0.6891170472579815, 0.696919916248909, 0.6784394230441146, 0.698151953484733, 0.7092402436160454, 0.693634499098486, 0.6981519463615495, 0.7158110847218571, 0.7149897373922062, 0.7100615984605323, 0.7174537959774416, 0.7121149879216658, 0.7096509212341152, 0.7104722778410393, 0.7034907600962895, 0.705954825021403, 0.7314168404749531, 0.7141683760120149, 0.7293634482722507, 0.6985626305153237, 0.710882960624029, 0.6866529821370416, 0.7227926079497445, 0.7277207377999715, 0.737166324582188, 0.7425051330296166, 0.7379876774684115, 0.7490759737437755, 0.7498973293715678, 0.7371663239947089, 0.7437371662754787, 0.74496919795473, 0.7482546228158156, 0.7416837785767824, 0.7215605709831817, 0.7170431258742079, 0.7195071904076688, 0.730184802725085, 0.7367556438308966, 0.7577002070522896, 0.7523613986048611, 0.7527720721105775, 0.7511293669256097, 0.7548254623550165, 0.7523613952758131, 0.7347022563280267, 0.7540041079021822, 0.7244353152888022, 0.7568788533827607, 0.7560574950133996, 0.7540041096646193, 0.759753595338465, 0.7486652965173585, 0.7396303922488704, 0.7314168359709471, 0.7252566762039059, 0.7449692010879516]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
