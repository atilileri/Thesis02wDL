{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf6.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 00:35:36 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '1', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000022254E64E10>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000022251616EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0781, Accuracy:0.3807, Validation Loss:1.0748, Validation Accuracy:0.4089\n",
    "Epoch #2: Loss:1.0753, Accuracy:0.3873, Validation Loss:1.0743, Validation Accuracy:0.4072\n",
    "Epoch #3: Loss:1.0749, Accuracy:0.3914, Validation Loss:1.0758, Validation Accuracy:0.3990\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3955, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0735, Accuracy:0.3988, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #6: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #7: Loss:1.0729, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #8: Loss:1.0730, Accuracy:0.4021, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #9: Loss:1.0728, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.4122\n",
    "Epoch #10: Loss:1.0727, Accuracy:0.3996, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #11: Loss:1.0726, Accuracy:0.3996, Validation Loss:1.0754, Validation Accuracy:0.3990\n",
    "Epoch #12: Loss:1.0726, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.4072\n",
    "Epoch #13: Loss:1.0727, Accuracy:0.3934, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0725, Accuracy:0.3938, Validation Loss:1.0751, Validation Accuracy:0.3744\n",
    "Epoch #15: Loss:1.0724, Accuracy:0.3951, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #16: Loss:1.0725, Accuracy:0.3947, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #17: Loss:1.0723, Accuracy:0.3926, Validation Loss:1.0752, Validation Accuracy:0.4039\n",
    "Epoch #18: Loss:1.0721, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.4023\n",
    "Epoch #19: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0752, Validation Accuracy:0.4023\n",
    "Epoch #20: Loss:1.0718, Accuracy:0.3955, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0712, Accuracy:0.3943, Validation Loss:1.0760, Validation Accuracy:0.3974\n",
    "Epoch #22: Loss:1.0709, Accuracy:0.3984, Validation Loss:1.0754, Validation Accuracy:0.3908\n",
    "Epoch #23: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #24: Loss:1.0706, Accuracy:0.4033, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #25: Loss:1.0707, Accuracy:0.3947, Validation Loss:1.0757, Validation Accuracy:0.4039\n",
    "Epoch #26: Loss:1.0706, Accuracy:0.3967, Validation Loss:1.0762, Validation Accuracy:0.4089\n",
    "Epoch #27: Loss:1.0704, Accuracy:0.4025, Validation Loss:1.0753, Validation Accuracy:0.4039\n",
    "Epoch #28: Loss:1.0709, Accuracy:0.4041, Validation Loss:1.0757, Validation Accuracy:0.4056\n",
    "Epoch #29: Loss:1.0707, Accuracy:0.4066, Validation Loss:1.0755, Validation Accuracy:0.4154\n",
    "Epoch #30: Loss:1.0713, Accuracy:0.4053, Validation Loss:1.0742, Validation Accuracy:0.4335\n",
    "Epoch #31: Loss:1.0719, Accuracy:0.4037, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #32: Loss:1.0714, Accuracy:0.4008, Validation Loss:1.0759, Validation Accuracy:0.4122\n",
    "Epoch #33: Loss:1.0709, Accuracy:0.4004, Validation Loss:1.0764, Validation Accuracy:0.4007\n",
    "Epoch #34: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0769, Validation Accuracy:0.3924\n",
    "Epoch #35: Loss:1.0710, Accuracy:0.3984, Validation Loss:1.0771, Validation Accuracy:0.4105\n",
    "Epoch #36: Loss:1.0705, Accuracy:0.3926, Validation Loss:1.0770, Validation Accuracy:0.4007\n",
    "Epoch #37: Loss:1.0704, Accuracy:0.4119, Validation Loss:1.0762, Validation Accuracy:0.3974\n",
    "Epoch #38: Loss:1.0697, Accuracy:0.4111, Validation Loss:1.0773, Validation Accuracy:0.3892\n",
    "Epoch #39: Loss:1.0688, Accuracy:0.4037, Validation Loss:1.0759, Validation Accuracy:0.3974\n",
    "Epoch #40: Loss:1.0711, Accuracy:0.4057, Validation Loss:1.0773, Validation Accuracy:0.3924\n",
    "Epoch #41: Loss:1.0711, Accuracy:0.4029, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #42: Loss:1.0704, Accuracy:0.4062, Validation Loss:1.0763, Validation Accuracy:0.3957\n",
    "Epoch #43: Loss:1.0687, Accuracy:0.4000, Validation Loss:1.0756, Validation Accuracy:0.4007\n",
    "Epoch #44: Loss:1.0684, Accuracy:0.4057, Validation Loss:1.0776, Validation Accuracy:0.4023\n",
    "Epoch #45: Loss:1.0675, Accuracy:0.4049, Validation Loss:1.0769, Validation Accuracy:0.3908\n",
    "Epoch #46: Loss:1.0671, Accuracy:0.4090, Validation Loss:1.0765, Validation Accuracy:0.3957\n",
    "Epoch #47: Loss:1.0669, Accuracy:0.4131, Validation Loss:1.0765, Validation Accuracy:0.3957\n",
    "Epoch #48: Loss:1.0658, Accuracy:0.4148, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #49: Loss:1.0655, Accuracy:0.4107, Validation Loss:1.0766, Validation Accuracy:0.4056\n",
    "Epoch #50: Loss:1.0650, Accuracy:0.4168, Validation Loss:1.0768, Validation Accuracy:0.4122\n",
    "Epoch #51: Loss:1.0652, Accuracy:0.4082, Validation Loss:1.0783, Validation Accuracy:0.3777\n",
    "Epoch #52: Loss:1.0642, Accuracy:0.4136, Validation Loss:1.0795, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0629, Accuracy:0.4164, Validation Loss:1.0765, Validation Accuracy:0.3957\n",
    "Epoch #54: Loss:1.0634, Accuracy:0.4164, Validation Loss:1.0763, Validation Accuracy:0.3990\n",
    "Epoch #55: Loss:1.0666, Accuracy:0.3967, Validation Loss:1.0771, Validation Accuracy:0.3974\n",
    "Epoch #56: Loss:1.0658, Accuracy:0.4094, Validation Loss:1.0759, Validation Accuracy:0.4056\n",
    "Epoch #57: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0770, Validation Accuracy:0.4138\n",
    "Epoch #58: Loss:1.0715, Accuracy:0.3996, Validation Loss:1.0770, Validation Accuracy:0.3859\n",
    "Epoch #59: Loss:1.0691, Accuracy:0.4062, Validation Loss:1.0813, Validation Accuracy:0.3777\n",
    "Epoch #60: Loss:1.0654, Accuracy:0.4045, Validation Loss:1.0821, Validation Accuracy:0.3892\n",
    "Epoch #61: Loss:1.0678, Accuracy:0.4033, Validation Loss:1.0817, Validation Accuracy:0.4023\n",
    "Epoch #62: Loss:1.0691, Accuracy:0.4090, Validation Loss:1.0853, Validation Accuracy:0.3645\n",
    "Epoch #63: Loss:1.0655, Accuracy:0.4234, Validation Loss:1.0770, Validation Accuracy:0.3957\n",
    "Epoch #64: Loss:1.0647, Accuracy:0.4131, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #65: Loss:1.0642, Accuracy:0.4103, Validation Loss:1.0739, Validation Accuracy:0.4072\n",
    "Epoch #66: Loss:1.0639, Accuracy:0.4148, Validation Loss:1.0746, Validation Accuracy:0.4154\n",
    "Epoch #67: Loss:1.0636, Accuracy:0.4164, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #68: Loss:1.0641, Accuracy:0.4181, Validation Loss:1.0745, Validation Accuracy:0.4138\n",
    "Epoch #69: Loss:1.0657, Accuracy:0.4164, Validation Loss:1.0769, Validation Accuracy:0.4072\n",
    "Epoch #70: Loss:1.0647, Accuracy:0.4037, Validation Loss:1.0805, Validation Accuracy:0.4039\n",
    "Epoch #71: Loss:1.0640, Accuracy:0.4086, Validation Loss:1.0815, Validation Accuracy:0.3875\n",
    "Epoch #72: Loss:1.0640, Accuracy:0.4127, Validation Loss:1.0808, Validation Accuracy:0.4007\n",
    "Epoch #73: Loss:1.0633, Accuracy:0.4152, Validation Loss:1.0788, Validation Accuracy:0.4056\n",
    "Epoch #74: Loss:1.0644, Accuracy:0.4086, Validation Loss:1.0807, Validation Accuracy:0.3875\n",
    "Epoch #75: Loss:1.0620, Accuracy:0.4214, Validation Loss:1.0792, Validation Accuracy:0.3892\n",
    "Epoch #76: Loss:1.0629, Accuracy:0.4136, Validation Loss:1.0796, Validation Accuracy:0.3924\n",
    "Epoch #77: Loss:1.0613, Accuracy:0.4193, Validation Loss:1.0846, Validation Accuracy:0.3908\n",
    "Epoch #78: Loss:1.0618, Accuracy:0.4193, Validation Loss:1.0870, Validation Accuracy:0.3810\n",
    "Epoch #79: Loss:1.0617, Accuracy:0.4156, Validation Loss:1.0867, Validation Accuracy:0.3842\n",
    "Epoch #80: Loss:1.0617, Accuracy:0.4111, Validation Loss:1.0881, Validation Accuracy:0.3777\n",
    "Epoch #81: Loss:1.0625, Accuracy:0.4033, Validation Loss:1.0865, Validation Accuracy:0.3777\n",
    "Epoch #82: Loss:1.0592, Accuracy:0.4271, Validation Loss:1.0865, Validation Accuracy:0.3727\n",
    "Epoch #83: Loss:1.0642, Accuracy:0.4099, Validation Loss:1.0846, Validation Accuracy:0.3810\n",
    "Epoch #84: Loss:1.0782, Accuracy:0.4168, Validation Loss:1.0947, Validation Accuracy:0.3941\n",
    "Epoch #85: Loss:1.0890, Accuracy:0.3938, Validation Loss:1.0874, Validation Accuracy:0.4204\n",
    "Epoch #86: Loss:1.0833, Accuracy:0.3951, Validation Loss:1.0936, Validation Accuracy:0.3875\n",
    "Epoch #87: Loss:1.0834, Accuracy:0.3984, Validation Loss:1.0760, Validation Accuracy:0.4007\n",
    "Epoch #88: Loss:1.0774, Accuracy:0.3791, Validation Loss:1.0785, Validation Accuracy:0.3842\n",
    "Epoch #89: Loss:1.0754, Accuracy:0.4062, Validation Loss:1.0790, Validation Accuracy:0.3842\n",
    "Epoch #90: Loss:1.0742, Accuracy:0.3910, Validation Loss:1.0769, Validation Accuracy:0.3826\n",
    "Epoch #91: Loss:1.0729, Accuracy:0.3901, Validation Loss:1.0770, Validation Accuracy:0.3711\n",
    "Epoch #92: Loss:1.0729, Accuracy:0.3906, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #93: Loss:1.0728, Accuracy:0.3934, Validation Loss:1.0769, Validation Accuracy:0.3695\n",
    "Epoch #94: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0767, Validation Accuracy:0.3744\n",
    "Epoch #95: Loss:1.0726, Accuracy:0.3992, Validation Loss:1.0765, Validation Accuracy:0.3826\n",
    "Epoch #96: Loss:1.0724, Accuracy:0.3967, Validation Loss:1.0772, Validation Accuracy:0.3908\n",
    "Epoch #97: Loss:1.0725, Accuracy:0.3955, Validation Loss:1.0771, Validation Accuracy:0.3859\n",
    "Epoch #98: Loss:1.0719, Accuracy:0.3996, Validation Loss:1.0768, Validation Accuracy:0.3826\n",
    "Epoch #99: Loss:1.0719, Accuracy:0.3992, Validation Loss:1.0767, Validation Accuracy:0.3892\n",
    "Epoch #100: Loss:1.0718, Accuracy:0.4086, Validation Loss:1.0775, Validation Accuracy:0.3711\n",
    "Epoch #101: Loss:1.0718, Accuracy:0.3967, Validation Loss:1.0777, Validation Accuracy:0.3892\n",
    "Epoch #102: Loss:1.0714, Accuracy:0.3996, Validation Loss:1.0775, Validation Accuracy:0.3924\n",
    "Epoch #103: Loss:1.0714, Accuracy:0.4012, Validation Loss:1.0775, Validation Accuracy:0.3974\n",
    "Epoch #104: Loss:1.0714, Accuracy:0.4111, Validation Loss:1.0781, Validation Accuracy:0.3793\n",
    "Epoch #105: Loss:1.0716, Accuracy:0.3971, Validation Loss:1.0779, Validation Accuracy:0.3974\n",
    "Epoch #106: Loss:1.0705, Accuracy:0.4008, Validation Loss:1.0779, Validation Accuracy:0.3957\n",
    "Epoch #107: Loss:1.0705, Accuracy:0.3992, Validation Loss:1.0775, Validation Accuracy:0.4039\n",
    "Epoch #108: Loss:1.0700, Accuracy:0.4168, Validation Loss:1.0779, Validation Accuracy:0.3859\n",
    "Epoch #109: Loss:1.0698, Accuracy:0.4144, Validation Loss:1.0778, Validation Accuracy:0.3826\n",
    "Epoch #110: Loss:1.0693, Accuracy:0.4164, Validation Loss:1.0771, Validation Accuracy:0.4089\n",
    "Epoch #111: Loss:1.0695, Accuracy:0.4160, Validation Loss:1.0771, Validation Accuracy:0.3924\n",
    "Epoch #112: Loss:1.0689, Accuracy:0.4025, Validation Loss:1.0785, Validation Accuracy:0.4023\n",
    "Epoch #113: Loss:1.0689, Accuracy:0.4029, Validation Loss:1.0779, Validation Accuracy:0.4039\n",
    "Epoch #114: Loss:1.0695, Accuracy:0.4062, Validation Loss:1.0796, Validation Accuracy:0.3777\n",
    "Epoch #115: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0804, Validation Accuracy:0.3924\n",
    "Epoch #116: Loss:1.0696, Accuracy:0.4074, Validation Loss:1.0792, Validation Accuracy:0.3875\n",
    "Epoch #117: Loss:1.0674, Accuracy:0.4049, Validation Loss:1.0784, Validation Accuracy:0.3908\n",
    "Epoch #118: Loss:1.0675, Accuracy:0.4152, Validation Loss:1.0788, Validation Accuracy:0.3859\n",
    "Epoch #119: Loss:1.0665, Accuracy:0.4193, Validation Loss:1.0773, Validation Accuracy:0.3974\n",
    "Epoch #120: Loss:1.0667, Accuracy:0.4148, Validation Loss:1.0755, Validation Accuracy:0.4007\n",
    "Epoch #121: Loss:1.0673, Accuracy:0.4099, Validation Loss:1.0822, Validation Accuracy:0.3810\n",
    "Epoch #122: Loss:1.0692, Accuracy:0.3967, Validation Loss:1.0850, Validation Accuracy:0.3957\n",
    "Epoch #123: Loss:1.0702, Accuracy:0.4008, Validation Loss:1.0828, Validation Accuracy:0.3777\n",
    "Epoch #124: Loss:1.0678, Accuracy:0.4140, Validation Loss:1.0771, Validation Accuracy:0.4007\n",
    "Epoch #125: Loss:1.0694, Accuracy:0.4107, Validation Loss:1.0805, Validation Accuracy:0.3974\n",
    "Epoch #126: Loss:1.0694, Accuracy:0.4127, Validation Loss:1.0834, Validation Accuracy:0.3695\n",
    "Epoch #127: Loss:1.0697, Accuracy:0.4148, Validation Loss:1.0833, Validation Accuracy:0.3826\n",
    "Epoch #128: Loss:1.0714, Accuracy:0.4103, Validation Loss:1.0840, Validation Accuracy:0.3826\n",
    "Epoch #129: Loss:1.0681, Accuracy:0.4144, Validation Loss:1.0811, Validation Accuracy:0.3924\n",
    "Epoch #130: Loss:1.0670, Accuracy:0.4115, Validation Loss:1.0835, Validation Accuracy:0.3662\n",
    "Epoch #131: Loss:1.0667, Accuracy:0.4127, Validation Loss:1.0812, Validation Accuracy:0.3810\n",
    "Epoch #132: Loss:1.0693, Accuracy:0.4021, Validation Loss:1.0797, Validation Accuracy:0.3974\n",
    "Epoch #133: Loss:1.0680, Accuracy:0.3947, Validation Loss:1.0811, Validation Accuracy:0.3990\n",
    "Epoch #134: Loss:1.0665, Accuracy:0.4062, Validation Loss:1.0814, Validation Accuracy:0.4039\n",
    "Epoch #135: Loss:1.0680, Accuracy:0.4086, Validation Loss:1.0862, Validation Accuracy:0.3727\n",
    "Epoch #136: Loss:1.0678, Accuracy:0.4090, Validation Loss:1.0856, Validation Accuracy:0.3875\n",
    "Epoch #137: Loss:1.0668, Accuracy:0.4127, Validation Loss:1.0813, Validation Accuracy:0.3990\n",
    "Epoch #138: Loss:1.0661, Accuracy:0.4140, Validation Loss:1.0845, Validation Accuracy:0.3760\n",
    "Epoch #139: Loss:1.0645, Accuracy:0.4185, Validation Loss:1.0845, Validation Accuracy:0.3859\n",
    "Epoch #140: Loss:1.0647, Accuracy:0.4136, Validation Loss:1.0821, Validation Accuracy:0.3957\n",
    "Epoch #141: Loss:1.0639, Accuracy:0.4131, Validation Loss:1.0826, Validation Accuracy:0.4023\n",
    "Epoch #142: Loss:1.0631, Accuracy:0.4201, Validation Loss:1.0809, Validation Accuracy:0.3941\n",
    "Epoch #143: Loss:1.0649, Accuracy:0.4123, Validation Loss:1.0867, Validation Accuracy:0.3875\n",
    "Epoch #144: Loss:1.0685, Accuracy:0.4074, Validation Loss:1.0859, Validation Accuracy:0.3810\n",
    "Epoch #145: Loss:1.0670, Accuracy:0.4136, Validation Loss:1.0813, Validation Accuracy:0.3842\n",
    "Epoch #146: Loss:1.0709, Accuracy:0.3975, Validation Loss:1.0818, Validation Accuracy:0.3662\n",
    "Epoch #147: Loss:1.0853, Accuracy:0.3832, Validation Loss:1.0980, Validation Accuracy:0.3974\n",
    "Epoch #148: Loss:1.0847, Accuracy:0.4033, Validation Loss:1.0943, Validation Accuracy:0.3826\n",
    "Epoch #149: Loss:1.0795, Accuracy:0.3918, Validation Loss:1.0980, Validation Accuracy:0.3793\n",
    "Epoch #150: Loss:1.0702, Accuracy:0.4115, Validation Loss:1.0871, Validation Accuracy:0.3842\n",
    "Epoch #151: Loss:1.0725, Accuracy:0.4025, Validation Loss:1.0866, Validation Accuracy:0.3875\n",
    "Epoch #152: Loss:1.0728, Accuracy:0.4037, Validation Loss:1.0816, Validation Accuracy:0.3924\n",
    "Epoch #153: Loss:1.0700, Accuracy:0.4049, Validation Loss:1.0816, Validation Accuracy:0.3924\n",
    "Epoch #154: Loss:1.0698, Accuracy:0.3988, Validation Loss:1.0796, Validation Accuracy:0.3974\n",
    "Epoch #155: Loss:1.0691, Accuracy:0.4131, Validation Loss:1.0791, Validation Accuracy:0.4007\n",
    "Epoch #156: Loss:1.0689, Accuracy:0.4094, Validation Loss:1.0839, Validation Accuracy:0.3892\n",
    "Epoch #157: Loss:1.0717, Accuracy:0.4037, Validation Loss:1.0794, Validation Accuracy:0.3875\n",
    "Epoch #158: Loss:1.0698, Accuracy:0.4057, Validation Loss:1.0792, Validation Accuracy:0.3957\n",
    "Epoch #159: Loss:1.0696, Accuracy:0.4021, Validation Loss:1.0782, Validation Accuracy:0.4007\n",
    "Epoch #160: Loss:1.0682, Accuracy:0.4107, Validation Loss:1.0779, Validation Accuracy:0.4023\n",
    "Epoch #161: Loss:1.0678, Accuracy:0.4144, Validation Loss:1.0803, Validation Accuracy:0.3859\n",
    "Epoch #162: Loss:1.0680, Accuracy:0.4140, Validation Loss:1.0807, Validation Accuracy:0.3842\n",
    "Epoch #163: Loss:1.0679, Accuracy:0.4172, Validation Loss:1.0801, Validation Accuracy:0.3908\n",
    "Epoch #164: Loss:1.0676, Accuracy:0.4193, Validation Loss:1.0814, Validation Accuracy:0.3810\n",
    "Epoch #165: Loss:1.0673, Accuracy:0.4193, Validation Loss:1.0812, Validation Accuracy:0.3859\n",
    "Epoch #166: Loss:1.0668, Accuracy:0.4177, Validation Loss:1.0821, Validation Accuracy:0.3908\n",
    "Epoch #167: Loss:1.0668, Accuracy:0.4148, Validation Loss:1.0828, Validation Accuracy:0.3875\n",
    "Epoch #168: Loss:1.0661, Accuracy:0.4164, Validation Loss:1.0844, Validation Accuracy:0.3810\n",
    "Epoch #169: Loss:1.0661, Accuracy:0.4168, Validation Loss:1.0840, Validation Accuracy:0.3908\n",
    "Epoch #170: Loss:1.0656, Accuracy:0.4131, Validation Loss:1.0853, Validation Accuracy:0.3892\n",
    "Epoch #171: Loss:1.0653, Accuracy:0.4148, Validation Loss:1.0833, Validation Accuracy:0.3924\n",
    "Epoch #172: Loss:1.0647, Accuracy:0.4148, Validation Loss:1.0832, Validation Accuracy:0.4007\n",
    "Epoch #173: Loss:1.0649, Accuracy:0.4168, Validation Loss:1.0837, Validation Accuracy:0.3974\n",
    "Epoch #174: Loss:1.0642, Accuracy:0.4172, Validation Loss:1.0841, Validation Accuracy:0.3924\n",
    "Epoch #175: Loss:1.0649, Accuracy:0.4164, Validation Loss:1.0850, Validation Accuracy:0.3875\n",
    "Epoch #176: Loss:1.0658, Accuracy:0.4164, Validation Loss:1.0841, Validation Accuracy:0.4007\n",
    "Epoch #177: Loss:1.0648, Accuracy:0.4164, Validation Loss:1.0848, Validation Accuracy:0.3941\n",
    "Epoch #178: Loss:1.0653, Accuracy:0.4144, Validation Loss:1.0850, Validation Accuracy:0.4007\n",
    "Epoch #179: Loss:1.0650, Accuracy:0.4181, Validation Loss:1.0864, Validation Accuracy:0.3875\n",
    "Epoch #180: Loss:1.0641, Accuracy:0.4156, Validation Loss:1.0858, Validation Accuracy:0.3908\n",
    "Epoch #181: Loss:1.0640, Accuracy:0.4201, Validation Loss:1.0856, Validation Accuracy:0.3892\n",
    "Epoch #182: Loss:1.0636, Accuracy:0.4197, Validation Loss:1.0858, Validation Accuracy:0.3974\n",
    "Epoch #183: Loss:1.0633, Accuracy:0.4168, Validation Loss:1.0880, Validation Accuracy:0.3924\n",
    "Epoch #184: Loss:1.0639, Accuracy:0.4168, Validation Loss:1.0878, Validation Accuracy:0.3859\n",
    "Epoch #185: Loss:1.0622, Accuracy:0.4189, Validation Loss:1.0872, Validation Accuracy:0.3908\n",
    "Epoch #186: Loss:1.0627, Accuracy:0.4172, Validation Loss:1.0868, Validation Accuracy:0.4023\n",
    "Epoch #187: Loss:1.0623, Accuracy:0.4131, Validation Loss:1.0861, Validation Accuracy:0.3957\n",
    "Epoch #188: Loss:1.0625, Accuracy:0.4152, Validation Loss:1.0835, Validation Accuracy:0.4023\n",
    "Epoch #189: Loss:1.0627, Accuracy:0.4111, Validation Loss:1.0842, Validation Accuracy:0.3957\n",
    "Epoch #190: Loss:1.0632, Accuracy:0.4140, Validation Loss:1.0841, Validation Accuracy:0.3974\n",
    "Epoch #191: Loss:1.0690, Accuracy:0.4000, Validation Loss:1.0927, Validation Accuracy:0.4056\n",
    "Epoch #192: Loss:1.0703, Accuracy:0.3979, Validation Loss:1.0850, Validation Accuracy:0.4089\n",
    "Epoch #193: Loss:1.0723, Accuracy:0.4086, Validation Loss:1.0848, Validation Accuracy:0.4056\n",
    "Epoch #194: Loss:1.0682, Accuracy:0.4041, Validation Loss:1.0815, Validation Accuracy:0.3563\n",
    "Epoch #195: Loss:1.0681, Accuracy:0.4053, Validation Loss:1.0796, Validation Accuracy:0.3727\n",
    "Epoch #196: Loss:1.0672, Accuracy:0.4025, Validation Loss:1.0804, Validation Accuracy:0.3711\n",
    "Epoch #197: Loss:1.0666, Accuracy:0.4082, Validation Loss:1.0798, Validation Accuracy:0.3924\n",
    "Epoch #198: Loss:1.0665, Accuracy:0.4103, Validation Loss:1.0805, Validation Accuracy:0.3892\n",
    "Epoch #199: Loss:1.0647, Accuracy:0.4103, Validation Loss:1.0822, Validation Accuracy:0.3875\n",
    "Epoch #200: Loss:1.0643, Accuracy:0.4201, Validation Loss:1.0828, Validation Accuracy:0.4007\n",
    "Epoch #201: Loss:1.0637, Accuracy:0.4131, Validation Loss:1.0832, Validation Accuracy:0.3793\n",
    "Epoch #202: Loss:1.0634, Accuracy:0.4160, Validation Loss:1.0857, Validation Accuracy:0.3908\n",
    "Epoch #203: Loss:1.0624, Accuracy:0.4193, Validation Loss:1.0861, Validation Accuracy:0.3777\n",
    "Epoch #204: Loss:1.0641, Accuracy:0.4127, Validation Loss:1.0844, Validation Accuracy:0.3957\n",
    "Epoch #205: Loss:1.0634, Accuracy:0.4164, Validation Loss:1.0868, Validation Accuracy:0.3908\n",
    "Epoch #206: Loss:1.0637, Accuracy:0.4144, Validation Loss:1.0860, Validation Accuracy:0.4122\n",
    "Epoch #207: Loss:1.0615, Accuracy:0.4160, Validation Loss:1.0850, Validation Accuracy:0.3957\n",
    "Epoch #208: Loss:1.0627, Accuracy:0.4255, Validation Loss:1.0858, Validation Accuracy:0.4072\n",
    "Epoch #209: Loss:1.0621, Accuracy:0.4279, Validation Loss:1.0877, Validation Accuracy:0.4039\n",
    "Epoch #210: Loss:1.0614, Accuracy:0.4242, Validation Loss:1.0895, Validation Accuracy:0.4023\n",
    "Epoch #211: Loss:1.0624, Accuracy:0.4234, Validation Loss:1.0856, Validation Accuracy:0.4023\n",
    "Epoch #212: Loss:1.0617, Accuracy:0.4205, Validation Loss:1.0900, Validation Accuracy:0.3957\n",
    "Epoch #213: Loss:1.0631, Accuracy:0.4160, Validation Loss:1.0891, Validation Accuracy:0.3875\n",
    "Epoch #214: Loss:1.0639, Accuracy:0.4152, Validation Loss:1.0876, Validation Accuracy:0.3990\n",
    "Epoch #215: Loss:1.0629, Accuracy:0.4127, Validation Loss:1.0902, Validation Accuracy:0.3859\n",
    "Epoch #216: Loss:1.0598, Accuracy:0.4160, Validation Loss:1.0889, Validation Accuracy:0.3941\n",
    "Epoch #217: Loss:1.0630, Accuracy:0.4099, Validation Loss:1.0899, Validation Accuracy:0.3826\n",
    "Epoch #218: Loss:1.0614, Accuracy:0.4136, Validation Loss:1.0907, Validation Accuracy:0.3842\n",
    "Epoch #219: Loss:1.0638, Accuracy:0.4127, Validation Loss:1.0872, Validation Accuracy:0.3859\n",
    "Epoch #220: Loss:1.0653, Accuracy:0.4140, Validation Loss:1.0848, Validation Accuracy:0.3859\n",
    "Epoch #221: Loss:1.0652, Accuracy:0.4148, Validation Loss:1.0812, Validation Accuracy:0.4039\n",
    "Epoch #222: Loss:1.0646, Accuracy:0.4164, Validation Loss:1.0809, Validation Accuracy:0.3793\n",
    "Epoch #223: Loss:1.0633, Accuracy:0.4148, Validation Loss:1.0817, Validation Accuracy:0.3826\n",
    "Epoch #224: Loss:1.0619, Accuracy:0.4197, Validation Loss:1.0813, Validation Accuracy:0.4023\n",
    "Epoch #225: Loss:1.0639, Accuracy:0.4189, Validation Loss:1.0815, Validation Accuracy:0.4056\n",
    "Epoch #226: Loss:1.0639, Accuracy:0.4140, Validation Loss:1.0813, Validation Accuracy:0.4039\n",
    "Epoch #227: Loss:1.0638, Accuracy:0.4082, Validation Loss:1.0812, Validation Accuracy:0.3974\n",
    "Epoch #228: Loss:1.0640, Accuracy:0.4156, Validation Loss:1.0825, Validation Accuracy:0.4039\n",
    "Epoch #229: Loss:1.0650, Accuracy:0.4119, Validation Loss:1.0815, Validation Accuracy:0.3957\n",
    "Epoch #230: Loss:1.0639, Accuracy:0.4140, Validation Loss:1.0827, Validation Accuracy:0.3892\n",
    "Epoch #231: Loss:1.0652, Accuracy:0.4094, Validation Loss:1.0819, Validation Accuracy:0.3990\n",
    "Epoch #232: Loss:1.0610, Accuracy:0.4193, Validation Loss:1.0845, Validation Accuracy:0.3990\n",
    "Epoch #233: Loss:1.0627, Accuracy:0.4168, Validation Loss:1.0815, Validation Accuracy:0.3859\n",
    "Epoch #234: Loss:1.0614, Accuracy:0.4152, Validation Loss:1.0831, Validation Accuracy:0.4007\n",
    "Epoch #235: Loss:1.0613, Accuracy:0.4148, Validation Loss:1.0833, Validation Accuracy:0.3974\n",
    "Epoch #236: Loss:1.0599, Accuracy:0.4197, Validation Loss:1.0829, Validation Accuracy:0.4023\n",
    "Epoch #237: Loss:1.0592, Accuracy:0.4172, Validation Loss:1.0861, Validation Accuracy:0.4039\n",
    "Epoch #238: Loss:1.0597, Accuracy:0.4103, Validation Loss:1.0846, Validation Accuracy:0.4072\n",
    "Epoch #239: Loss:1.0594, Accuracy:0.4177, Validation Loss:1.0832, Validation Accuracy:0.4056\n",
    "Epoch #240: Loss:1.0584, Accuracy:0.4193, Validation Loss:1.0821, Validation Accuracy:0.4171\n",
    "Epoch #241: Loss:1.0581, Accuracy:0.4226, Validation Loss:1.0826, Validation Accuracy:0.4072\n",
    "Epoch #242: Loss:1.0585, Accuracy:0.4127, Validation Loss:1.0835, Validation Accuracy:0.3974\n",
    "Epoch #243: Loss:1.0583, Accuracy:0.4205, Validation Loss:1.0875, Validation Accuracy:0.3924\n",
    "Epoch #244: Loss:1.0582, Accuracy:0.4242, Validation Loss:1.0846, Validation Accuracy:0.3974\n",
    "Epoch #245: Loss:1.0577, Accuracy:0.4226, Validation Loss:1.0866, Validation Accuracy:0.3875\n",
    "Epoch #246: Loss:1.0581, Accuracy:0.4226, Validation Loss:1.0853, Validation Accuracy:0.4007\n",
    "Epoch #247: Loss:1.0579, Accuracy:0.4214, Validation Loss:1.0869, Validation Accuracy:0.3990\n",
    "Epoch #248: Loss:1.0609, Accuracy:0.4201, Validation Loss:1.0900, Validation Accuracy:0.3990\n",
    "Epoch #249: Loss:1.0584, Accuracy:0.4222, Validation Loss:1.0877, Validation Accuracy:0.3924\n",
    "Epoch #250: Loss:1.0577, Accuracy:0.4222, Validation Loss:1.0871, Validation Accuracy:0.4007\n",
    "Epoch #251: Loss:1.0573, Accuracy:0.4234, Validation Loss:1.0856, Validation Accuracy:0.4023\n",
    "Epoch #252: Loss:1.0559, Accuracy:0.4255, Validation Loss:1.0871, Validation Accuracy:0.4007\n",
    "Epoch #253: Loss:1.0595, Accuracy:0.4181, Validation Loss:1.0916, Validation Accuracy:0.3908\n",
    "Epoch #254: Loss:1.0603, Accuracy:0.4201, Validation Loss:1.0876, Validation Accuracy:0.3941\n",
    "Epoch #255: Loss:1.0625, Accuracy:0.4136, Validation Loss:1.0908, Validation Accuracy:0.3842\n",
    "Epoch #256: Loss:1.0635, Accuracy:0.4160, Validation Loss:1.0896, Validation Accuracy:0.3908\n",
    "Epoch #257: Loss:1.0615, Accuracy:0.4238, Validation Loss:1.0863, Validation Accuracy:0.3990\n",
    "Epoch #258: Loss:1.0595, Accuracy:0.4242, Validation Loss:1.0887, Validation Accuracy:0.3875\n",
    "Epoch #259: Loss:1.0558, Accuracy:0.4263, Validation Loss:1.0916, Validation Accuracy:0.3826\n",
    "Epoch #260: Loss:1.0591, Accuracy:0.4156, Validation Loss:1.0842, Validation Accuracy:0.3859\n",
    "Epoch #261: Loss:1.0572, Accuracy:0.4041, Validation Loss:1.0892, Validation Accuracy:0.3810\n",
    "Epoch #262: Loss:1.0579, Accuracy:0.4078, Validation Loss:1.0950, Validation Accuracy:0.3760\n",
    "Epoch #263: Loss:1.0583, Accuracy:0.4074, Validation Loss:1.0962, Validation Accuracy:0.3727\n",
    "Epoch #264: Loss:1.0584, Accuracy:0.4136, Validation Loss:1.0954, Validation Accuracy:0.3826\n",
    "Epoch #265: Loss:1.0602, Accuracy:0.4177, Validation Loss:1.0961, Validation Accuracy:0.3645\n",
    "Epoch #266: Loss:1.0605, Accuracy:0.4189, Validation Loss:1.0899, Validation Accuracy:0.3842\n",
    "Epoch #267: Loss:1.0580, Accuracy:0.4177, Validation Loss:1.0922, Validation Accuracy:0.3892\n",
    "Epoch #268: Loss:1.0590, Accuracy:0.4185, Validation Loss:1.0910, Validation Accuracy:0.3760\n",
    "Epoch #269: Loss:1.0591, Accuracy:0.4144, Validation Loss:1.0930, Validation Accuracy:0.3793\n",
    "Epoch #270: Loss:1.0618, Accuracy:0.4131, Validation Loss:1.0891, Validation Accuracy:0.3760\n",
    "Epoch #271: Loss:1.0596, Accuracy:0.4140, Validation Loss:1.0882, Validation Accuracy:0.3793\n",
    "Epoch #272: Loss:1.0593, Accuracy:0.4242, Validation Loss:1.0902, Validation Accuracy:0.3744\n",
    "Epoch #273: Loss:1.0562, Accuracy:0.4209, Validation Loss:1.0954, Validation Accuracy:0.3727\n",
    "Epoch #274: Loss:1.0596, Accuracy:0.4193, Validation Loss:1.0972, Validation Accuracy:0.3678\n",
    "Epoch #275: Loss:1.0572, Accuracy:0.4177, Validation Loss:1.0948, Validation Accuracy:0.3875\n",
    "Epoch #276: Loss:1.0567, Accuracy:0.4090, Validation Loss:1.0975, Validation Accuracy:0.3859\n",
    "Epoch #277: Loss:1.0581, Accuracy:0.4185, Validation Loss:1.0991, Validation Accuracy:0.3908\n",
    "Epoch #278: Loss:1.0576, Accuracy:0.4189, Validation Loss:1.0976, Validation Accuracy:0.3892\n",
    "Epoch #279: Loss:1.0614, Accuracy:0.4008, Validation Loss:1.0979, Validation Accuracy:0.3859\n",
    "Epoch #280: Loss:1.0606, Accuracy:0.4115, Validation Loss:1.0950, Validation Accuracy:0.3941\n",
    "Epoch #281: Loss:1.0605, Accuracy:0.4119, Validation Loss:1.0962, Validation Accuracy:0.3892\n",
    "Epoch #282: Loss:1.0602, Accuracy:0.3897, Validation Loss:1.0993, Validation Accuracy:0.3859\n",
    "Epoch #283: Loss:1.0576, Accuracy:0.4189, Validation Loss:1.1073, Validation Accuracy:0.3596\n",
    "Epoch #284: Loss:1.0627, Accuracy:0.4004, Validation Loss:1.0977, Validation Accuracy:0.3908\n",
    "Epoch #285: Loss:1.0585, Accuracy:0.4057, Validation Loss:1.0983, Validation Accuracy:0.3695\n",
    "Epoch #286: Loss:1.0577, Accuracy:0.4164, Validation Loss:1.0927, Validation Accuracy:0.3892\n",
    "Epoch #287: Loss:1.0581, Accuracy:0.4177, Validation Loss:1.0865, Validation Accuracy:0.3875\n",
    "Epoch #288: Loss:1.0582, Accuracy:0.4156, Validation Loss:1.0857, Validation Accuracy:0.3924\n",
    "Epoch #289: Loss:1.0615, Accuracy:0.4181, Validation Loss:1.0848, Validation Accuracy:0.3941\n",
    "Epoch #290: Loss:1.0588, Accuracy:0.4172, Validation Loss:1.0902, Validation Accuracy:0.3924\n",
    "Epoch #291: Loss:1.0624, Accuracy:0.4008, Validation Loss:1.0960, Validation Accuracy:0.3695\n",
    "Epoch #292: Loss:1.0609, Accuracy:0.4160, Validation Loss:1.0936, Validation Accuracy:0.3908\n",
    "Epoch #293: Loss:1.0584, Accuracy:0.4152, Validation Loss:1.0942, Validation Accuracy:0.3727\n",
    "Epoch #294: Loss:1.0585, Accuracy:0.3971, Validation Loss:1.0919, Validation Accuracy:0.3875\n",
    "Epoch #295: Loss:1.0569, Accuracy:0.4156, Validation Loss:1.0920, Validation Accuracy:0.3875\n",
    "Epoch #296: Loss:1.0586, Accuracy:0.4197, Validation Loss:1.0951, Validation Accuracy:0.4072\n",
    "Epoch #297: Loss:1.0612, Accuracy:0.4131, Validation Loss:1.0943, Validation Accuracy:0.3924\n",
    "Epoch #298: Loss:1.0579, Accuracy:0.4123, Validation Loss:1.0959, Validation Accuracy:0.3924\n",
    "Epoch #299: Loss:1.0579, Accuracy:0.4144, Validation Loss:1.1003, Validation Accuracy:0.3957\n",
    "Epoch #300: Loss:1.0562, Accuracy:0.4189, Validation Loss:1.0980, Validation Accuracy:0.3957\n",
    "\n",
    "Test:\n",
    "Test Loss:1.09802878, Accuracy:0.3957\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02  03\n",
    "t:01  206  34   0\n",
    "t:02  192  35   0\n",
    "t:03  120  22   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.40      0.86      0.54       240\n",
    "          02       0.38      0.15      0.22       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.40       609\n",
    "   macro avg       0.26      0.34      0.25       609\n",
    "weighted avg       0.30      0.40      0.30       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 00:51:22 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 46 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0748476625859051, 1.0742778462925175, 1.0757744239860372, 1.0748662582563453, 1.0742966519983728, 1.0750548191649965, 1.075005839415176, 1.074839052699861, 1.0739302159530189, 1.0743902150437554, 1.0754200411939072, 1.0750184734466628, 1.0747827884598906, 1.0750558137502184, 1.0748687555833012, 1.075016326504975, 1.0752402947258284, 1.075309338045042, 1.0752331409939795, 1.0754656141810426, 1.0760372761630856, 1.0753739152244355, 1.0752756482078916, 1.0749319926858536, 1.0757349248002903, 1.076207721957628, 1.0752695573766047, 1.0757166164849192, 1.0755447260851931, 1.0742416109749053, 1.0753121250760183, 1.075915140079943, 1.0763879472203246, 1.0769112178649025, 1.0771022424322043, 1.0770183654841532, 1.0762349306460477, 1.0772910953938275, 1.0758568659204568, 1.0773483367976298, 1.0756420916915919, 1.0762970161751182, 1.0756386013453818, 1.0776032052799593, 1.0768755646957748, 1.076487217043421, 1.076537793101544, 1.0750183007987262, 1.076592110256452, 1.076785717691694, 1.0783182658585422, 1.07954157377503, 1.0764847770700314, 1.0762992770409545, 1.077112951497922, 1.075910411640537, 1.0769720042280375, 1.0770334751147943, 1.0812534084069514, 1.0820525275858361, 1.0816840604804028, 1.085281053004398, 1.076960563659668, 1.0748563658427723, 1.0739117434067875, 1.0745819096494778, 1.0734850630188615, 1.0744577818511938, 1.0768977223554463, 1.08054038848, 1.0814504255410682, 1.0807719541887932, 1.0787645251488647, 1.0806865656904399, 1.079235417502267, 1.0795532672471797, 1.084573790553364, 1.0870023494087808, 1.0866816789645868, 1.0881105626157939, 1.0864677452688734, 1.0864645955206333, 1.0846115832258327, 1.0946898928221027, 1.0874199186052596, 1.0935643811531255, 1.0759961152898854, 1.0785008739368083, 1.0790297691457964, 1.0768724599691055, 1.077003533421283, 1.0767267202508861, 1.0769265852929728, 1.0767011139388938, 1.0764887164574735, 1.0771534775669742, 1.0770895759068881, 1.076829535620553, 1.0767260171313984, 1.077456061671716, 1.0777297313577436, 1.0775216095553244, 1.0774943341175323, 1.0781000335815505, 1.077894058525073, 1.0778861905162167, 1.0775044931371027, 1.0778931426297267, 1.077783617088556, 1.077099357332502, 1.0770802717099244, 1.0785108031506216, 1.0779141720096856, 1.0795594029042912, 1.0804272424215557, 1.0792045072577465, 1.0783796177317553, 1.0788319623724776, 1.0773284037907918, 1.075526998156593, 1.0822496059884383, 1.0850024708777617, 1.0827564329936588, 1.0770766194818056, 1.080541403227056, 1.0834014370719396, 1.0833478858709726, 1.083954383782761, 1.0810710559729089, 1.083484674909432, 1.0811566944388529, 1.0797189280317334, 1.081117683052038, 1.0813976659367628, 1.0862161003310105, 1.085638609621521, 1.0812695989467827, 1.0844762472096334, 1.0844590297864967, 1.082107473281021, 1.0826228189546683, 1.0808930980356652, 1.086734107562474, 1.0859089698306055, 1.0812715832235777, 1.0818415365391372, 1.0980098791701844, 1.094295990486646, 1.0979632679464781, 1.087067801572615, 1.0866124258057042, 1.081610961696393, 1.0816146949633394, 1.0795909565657817, 1.0791487364933408, 1.0838712256138743, 1.0793743010224968, 1.079180736064128, 1.0781651337941487, 1.0778677886342767, 1.080251892799227, 1.0806649189277235, 1.080099812869368, 1.0814369766191505, 1.081217666369158, 1.0821198266128014, 1.0828146437314539, 1.084424039413189, 1.084014360540606, 1.085250272735195, 1.0832783150163974, 1.0831640827636217, 1.0837460020297072, 1.0840826893870663, 1.085010421882905, 1.0840596759260581, 1.0847512536448212, 1.084983697665736, 1.086415371871347, 1.0857816971777303, 1.0856177734428243, 1.0857807697333726, 1.088039711778387, 1.0877946458622347, 1.0872278773334423, 1.0867955594618723, 1.0860961207811077, 1.0835004117101283, 1.0841616906947495, 1.084111296876115, 1.09269570834531, 1.084969952971673, 1.0847639871152555, 1.0815441708259395, 1.0796243858650596, 1.0803605768285165, 1.0798145728353992, 1.0805200596748314, 1.0822131823632126, 1.0827585490074847, 1.083153705683052, 1.0857241807508546, 1.0860673310525704, 1.0843939881019404, 1.0867538072401275, 1.0860150469152015, 1.0850488652149444, 1.0858483919369175, 1.087664597923141, 1.0895049885184502, 1.0855705099935797, 1.0899533608864094, 1.0890602208123419, 1.0875665622783217, 1.0901960633658423, 1.088872931665192, 1.0898751124176878, 1.0906570584120225, 1.087197472896482, 1.08475198044957, 1.081155407017675, 1.0809438009371704, 1.0817082619236411, 1.0813083034039326, 1.0815381794336003, 1.081277248111656, 1.0811627298740332, 1.0825343703597246, 1.081530438268126, 1.0826513941456335, 1.081925294473645, 1.0844650826430673, 1.081507404645284, 1.0831038536892343, 1.0833235385969942, 1.0829352856856849, 1.0860887166901763, 1.0845575775027472, 1.0832313835523006, 1.0821088981158629, 1.0825645516462905, 1.083542838080959, 1.087492111672713, 1.0846429667840842, 1.0866097712947427, 1.0853125722145995, 1.0869295150775629, 1.0899741741628286, 1.0876619714038518, 1.0870892026741517, 1.0855737601594972, 1.087144477614041, 1.0915579014811023, 1.087579173994769, 1.0908226884644607, 1.0896205673076835, 1.0863210604891598, 1.0887098901573269, 1.0916328804050563, 1.0841704708797786, 1.089233501204129, 1.09498551580902, 1.0961639628621744, 1.095395164928217, 1.0961352604363352, 1.0898826697777058, 1.0921834200278095, 1.090966752988756, 1.0930365357297198, 1.0890928806342515, 1.0882169452598334, 1.090170720919404, 1.0953854551456246, 1.097228920714217, 1.0948484337388589, 1.0975171893296767, 1.0991199126188782, 1.097649379512555, 1.0979494285113707, 1.0950167875963284, 1.0962418836521592, 1.0992526440393358, 1.107282675741537, 1.0977342778630248, 1.0982624533141188, 1.092662990778342, 1.0865075116478555, 1.0856677608928462, 1.0848297528641173, 1.0902290416664286, 1.095965081639282, 1.093606617063137, 1.0942046258641385, 1.091911670805394, 1.0919508851807693, 1.095104577701863, 1.0942854930223112, 1.095927358065137, 1.1003337064987333, 1.098028847540932], 'val_acc': [0.4088669943398443, 0.4072249584107955, 0.3990147775910758, 0.3940886690209456, 0.3875205247175126, 0.39080459706497506, 0.3957307052436133, 0.3908045967713561, 0.4121510664915608, 0.3990147776889488, 0.39901477739532987, 0.4072249578235576, 0.39408866911881857, 0.37438423552340866, 0.39244663289615084, 0.3842364523700501, 0.4039408857697141, 0.40229884964491935, 0.40229884954704637, 0.39408866911881857, 0.39737274117266214, 0.39080459667348316, 0.39737274136840806, 0.3990147772974569, 0.4039408857697141, 0.4088669943398443, 0.4039408856718411, 0.40558292199238183, 0.4154351390347692, 0.4334975360160195, 0.3957307052436133, 0.4121510663936878, 0.40065681322650565, 0.3924466327004049, 0.410509030562512, 0.40065681312863266, 0.39737274097691616, 0.38916256045081543, 0.39737274117266214, 0.3924466327982779, 0.39244663240678596, 0.3957307050478674, 0.40065681332437864, 0.4022988493513004, 0.3908045965756102, 0.3957307053414863, 0.3957307048521214, 0.38752052383665575, 0.4055829215030169, 0.41215106610006885, 0.3776683071857603, 0.3940886687273267, 0.39573070465637544, 0.3990147772974569, 0.3973727408790432, 0.4055829216008899, 0.41379310271422853, 0.38587848820122594, 0.37766830787087113, 0.38916256035294244, 0.4022988493513004, 0.36453201808952934, 0.3957307052436133, 0.3924466327004049, 0.4072249584107955, 0.4154351387411503, 0.40065681361799754, 0.41379310261635555, 0.4072249578235576, 0.4039408856718411, 0.3875205244238936, 0.40065681332437864, 0.4055829215030169, 0.3875205243260207, 0.38916256045081543, 0.3924466327982779, 0.3908045965756102, 0.38095237963109574, 0.38423645168493925, 0.3776683067942683, 0.3776683071857603, 0.37274219890924903, 0.38095237963109574, 0.39408866921669156, 0.4203612477027724, 0.3875205245217666, 0.40065681361799754, 0.3842364518806852, 0.38423645266366907, 0.38259441644100134, 0.37110016337169216, 0.3793103439956659, 0.36945812685540547, 0.3743842353276627, 0.3825944161473824, 0.3908045968692291, 0.38587848820122594, 0.3825944160495094, 0.3891625608423073, 0.3711001632738192, 0.38916256025506946, 0.39244663240678596, 0.39737274107478915, 0.379310343408428, 0.3973727407811702, 0.39573070446062947, 0.4039408856718411, 0.3858784882990989, 0.3825944159516364, 0.40886699414409833, 0.3924466327982779, 0.40229884905768143, 0.4039408856718411, 0.37766830728363326, 0.39244663289615084, 0.38752052393452874, 0.39080459696710207, 0.385878487809734, 0.39737274156415403, 0.40065681332437864, 0.38095237992471465, 0.3957307052436133, 0.3776683079687441, 0.4006568138137435, 0.39737274136840806, 0.36945812685540547, 0.38259441624525536, 0.3825944160495094, 0.3924466329940238, 0.3661740543121971, 0.38095237943534976, 0.39737274097691616, 0.3990147776889488, 0.4039408858675871, 0.37274219890924903, 0.3875205241302747, 0.3990147772974569, 0.3760262716482034, 0.38587848839697186, 0.3957307050478674, 0.4022988498406653, 0.39408866911881857, 0.3875205247175126, 0.3809523802183336, 0.3842364522721771, 0.36617405441007006, 0.3973727412705351, 0.3825944160495094, 0.3793103438977929, 0.3842364519785582, 0.3875205242281477, 0.3924466329940238, 0.3924466329940238, 0.39737274136840806, 0.4006568135201246, 0.38916256025506946, 0.3875205243260207, 0.3957307051457404, 0.40065681322650565, 0.40229884954704637, 0.38587848810335296, 0.38423645217430413, 0.3908045967713561, 0.38095238002258763, 0.38587848839697186, 0.39080459667348316, 0.3875205245217666, 0.38095238002258763, 0.3908045965756102, 0.38916256045081543, 0.39244663289615084, 0.40065681361799754, 0.3973727412705351, 0.39244663289615084, 0.3875205245217666, 0.4006568134222516, 0.3940886690209456, 0.40065681361799754, 0.3875205242281477, 0.3908045968692291, 0.38916256074443434, 0.39737274146628104, 0.3924466327004049, 0.3858784882990989, 0.3908045965756102, 0.40229884974279234, 0.3957307053414863, 0.4022988498406653, 0.3957307054393593, 0.39737274146628104, 0.40558292189450884, 0.4088669942419713, 0.40558292189450884, 0.35632183775917453, 0.37274219930074093, 0.3711001630780732, 0.39244663230891297, 0.38916256045081543, 0.38752052383665575, 0.40065681283501375, 0.379310343604174, 0.3908045965756102, 0.37766830757725217, 0.39573070475424843, 0.3908045963798642, 0.41215106590432293, 0.3957307048521214, 0.40722495811717657, 0.4039408855739681, 0.4022988492534274, 0.40229884954704637, 0.3957307050478674, 0.3875205243260207, 0.39901477739532987, 0.38587848800547997, 0.3940886690209456, 0.3825944161473824, 0.3842364520764312, 0.38587848859271784, 0.3858784882990989, 0.40394088547609514, 0.379310343408428, 0.3825944155601445, 0.4022988492534274, 0.40558292169876287, 0.40394088547609514, 0.3973727412705351, 0.40394088528034916, 0.3957307050478674, 0.38916256064656135, 0.39901477739532987, 0.3990147771017109, 0.38587848849484485, 0.40065681312863266, 0.3973727412705351, 0.4022988491555544, 0.40394088537822215, 0.4072249578235576, 0.40558292179663585, 0.41707717467019906, 0.4072249579214306, 0.39737274117266214, 0.39244663240678596, 0.3973727407811702, 0.3875205246196396, 0.40065681332437864, 0.3990147771995839, 0.3990147771017109, 0.39244663260253193, 0.40065681332437864, 0.4022988494491734, 0.4006568134222516, 0.3908045965756102, 0.39408866911881857, 0.38423645178281224, 0.3908045964777372, 0.39901477778682176, 0.3875205240324017, 0.3825944160495094, 0.38587848820122594, 0.38095237982684166, 0.3760262712567115, 0.37274219910499495, 0.38259441585376347, 0.36453201818740233, 0.38423645178281224, 0.38916256025506946, 0.3760262711588385, 0.379310343408428, 0.37602627145245743, 0.3793103437020469, 0.37438423542553567, 0.37274219910499495, 0.3678160908284837, 0.3875205241302747, 0.385878487907607, 0.3908045962819912, 0.38916256025506946, 0.38587848800547997, 0.3940886687273267, 0.38916256015719647, 0.38587848800547997, 0.3596059096172721, 0.3908045964777372, 0.36945812665965955, 0.38916256035294244, 0.3875205241302747, 0.39244663260253193, 0.3940886687273267, 0.39244663260253193, 0.36945812656178656, 0.3908045963798642, 0.37274219881137605, 0.3875205240324017, 0.3875205241302747, 0.4072249578235576, 0.39244663250465894, 0.39244663260253193, 0.39573070475424843, 0.39573070475424843], 'loss': [1.0781253922891323, 1.0752954521707931, 1.0749402403097132, 1.0745798303850866, 1.0735288713502198, 1.0732122893206149, 1.07291556481708, 1.0730374555568185, 1.0728153346255576, 1.0727236277758463, 1.0725730369712783, 1.0725982728680057, 1.072677559823226, 1.0725185067257108, 1.0723814619150496, 1.0724708849399731, 1.0723333436605622, 1.07205648138293, 1.0721718579591912, 1.071812413458462, 1.0712060410384034, 1.070915521145846, 1.0710993846583905, 1.0706212855951986, 1.0706559987528368, 1.0705935910007547, 1.0703962366194206, 1.0709021349462395, 1.07067165286634, 1.0713071737935655, 1.071903844535718, 1.0714012270345825, 1.07085169477874, 1.0710599697590852, 1.0709518680827084, 1.070538752769298, 1.070384193739607, 1.0697004136608366, 1.068834666109183, 1.0710701806344536, 1.0710902152364994, 1.0703557058036695, 1.0687401042581828, 1.0683720660650264, 1.067533178153224, 1.0671240571587972, 1.0669270161975335, 1.0657652771693236, 1.0655412637232755, 1.064968480760312, 1.0651674304409928, 1.0641625933578616, 1.062898079668472, 1.0633604701784358, 1.0665881195107525, 1.065776251571624, 1.0735217744564862, 1.0714920629466094, 1.0691251745458992, 1.0654254300883175, 1.067841410734815, 1.069113826996492, 1.065517819197026, 1.0646961970985303, 1.064157690502535, 1.0639472469901647, 1.0636155292973137, 1.0640581063421355, 1.0656772835788295, 1.0646893653047158, 1.0640312445971511, 1.0639565485458844, 1.0632948282562977, 1.0644335150473907, 1.0619548992944203, 1.062891344221221, 1.0613089063573913, 1.061782199107646, 1.0616529361912848, 1.0616964144383612, 1.0625321720659855, 1.0591950572246889, 1.0641812991312642, 1.0782350677239576, 1.0890377554805373, 1.0833031675898808, 1.083401593535343, 1.0773703886987738, 1.0754435852567763, 1.0741957893606573, 1.0728636135555636, 1.0728690773554652, 1.072792215905395, 1.0728771998407416, 1.072556292790407, 1.0724231846768264, 1.072470204884022, 1.0719211873577361, 1.0718731733318227, 1.0717845283739376, 1.0718219670916485, 1.071412286915084, 1.0713803216906788, 1.0713831249448553, 1.071585912773007, 1.070460300181191, 1.0705365190760556, 1.069998907896038, 1.0698453782275472, 1.0693463594272152, 1.069505937339344, 1.0688917737721908, 1.0689001748204476, 1.0695328532303139, 1.0699689397087333, 1.0696221477441965, 1.0674427427060795, 1.0675331476043137, 1.06646061668161, 1.0666587576484288, 1.0672673314993386, 1.0692125118243865, 1.0702003877510524, 1.0678294161261963, 1.0694383896841406, 1.0693692674382267, 1.06965568055852, 1.0713869121285189, 1.0681119727402986, 1.0670060526908545, 1.066708999639664, 1.0692879742420673, 1.0680044676978486, 1.0665018575637002, 1.0679932344131156, 1.0678200850986113, 1.0668437676752862, 1.066086748591194, 1.0645332950341384, 1.064679964171298, 1.0638798371967104, 1.0630737890208282, 1.0649188818138482, 1.068459480645965, 1.066979218704255, 1.0708893838114808, 1.0852722523393572, 1.0846853589130379, 1.0794564502684731, 1.0701670867951254, 1.0724517940740566, 1.0728028646974348, 1.0699685302847954, 1.069776494938735, 1.0691019550730805, 1.068917883984607, 1.0716665463770685, 1.0697983597338323, 1.0696352261537398, 1.068242916042555, 1.0677927135686855, 1.0680017459563895, 1.0678505208702793, 1.067600110424128, 1.0673323836904287, 1.0668052922039306, 1.066762028435662, 1.0660580294332955, 1.0661338916304666, 1.0655671042338535, 1.0652559777794433, 1.0647074987266587, 1.0648592046643675, 1.064193142317159, 1.064946902606032, 1.0658152167557202, 1.0647971369647393, 1.0652852040786274, 1.0649519299579597, 1.0641279675877315, 1.0639731999540232, 1.0635586653401965, 1.0633348612325146, 1.063872651006162, 1.0622371327460913, 1.0626636201596114, 1.0622824258628079, 1.0624737072284707, 1.062687140176918, 1.063187404383869, 1.069034036914426, 1.0702894326352974, 1.072331709195946, 1.0681580299224698, 1.0681103919810584, 1.067224423057979, 1.0665947055424998, 1.0665368099721795, 1.0647312460983558, 1.064323882204796, 1.0636644021686343, 1.0634025774452476, 1.0624282949024646, 1.0640893671791656, 1.0634268066966313, 1.0636973962646734, 1.0614852332970934, 1.062723769446418, 1.0620633231051404, 1.0613853422278496, 1.0623927622115588, 1.0617307028486498, 1.063061753680329, 1.063871673343118, 1.06291435009645, 1.0597771116350712, 1.0629912226596652, 1.061439083684886, 1.0637537710965292, 1.0653054313248433, 1.0651722387121934, 1.064580390046999, 1.0632798049484191, 1.0618846151128687, 1.0638717423229491, 1.0639256526312544, 1.0638206613626813, 1.0639807384361721, 1.064994568942264, 1.063914120025948, 1.0651647522708962, 1.0609509441642055, 1.0627482715328616, 1.0613908640413068, 1.0612626840936086, 1.0598803456559074, 1.0591849935128215, 1.059662009656307, 1.0594144077516434, 1.0584035492041273, 1.0580738458790084, 1.0585130310156508, 1.0583196196957536, 1.0582067395627377, 1.0576795422810548, 1.0581300657143093, 1.057867124095345, 1.0608986922113313, 1.0583952204647495, 1.057739330610945, 1.057340090328663, 1.055855009981739, 1.0594774222716659, 1.0602618848029104, 1.062503907959564, 1.0635432301605507, 1.0614741501132565, 1.059515200748091, 1.0557621969579427, 1.0590736218301666, 1.0572270274407076, 1.0578583772667134, 1.058277711682251, 1.0583848459275107, 1.0601698764785359, 1.0604931867098173, 1.0580113309609571, 1.0590323319425328, 1.0590901620089397, 1.0617795327850437, 1.059557954878288, 1.0593210091091525, 1.0561625644166857, 1.0596061672762924, 1.0572226015694088, 1.0567256340500755, 1.0580856994926562, 1.057637830436597, 1.061377686690501, 1.060593352033862, 1.0604541398171772, 1.0602003095575916, 1.0575941610630044, 1.062664075602741, 1.05850971357044, 1.0576935923809387, 1.0580981114561798, 1.05815556607452, 1.0615184436343779, 1.0587862177795944, 1.0623802079312366, 1.0609354783866929, 1.0584396909149765, 1.0585489285800003, 1.0568723006414926, 1.058561848566028, 1.0611594143834202, 1.0579151499197958, 1.0578792300801991, 1.0561659939235242], 'acc': [0.380698154312874, 0.3872689947944892, 0.39137576940857655, 0.3954825475475382, 0.3987679692754021, 0.40082135560331406, 0.39835729146150595, 0.4020533902199606, 0.4020533893999378, 0.3995893219657992, 0.39958932036247097, 0.40123203517964734, 0.3934291588697101, 0.3938398368794326, 0.3950718687545103, 0.39466119231139857, 0.39260780183441585, 0.39425051351837065, 0.402464067801313, 0.39548254637258007, 0.3942505152808078, 0.39835728750826155, 0.3979466103185618, 0.4032854189250993, 0.39466119191974586, 0.3967145804017476, 0.40246406447226507, 0.4041067759603935, 0.40657084124044224, 0.405338809402082, 0.4036960995172818, 0.40082135341250674, 0.40041067896437593, 0.40492813374227565, 0.3983572916573323, 0.39260780183441585, 0.4119096487087391, 0.4110882968383648, 0.40369609752230085, 0.40574948760763085, 0.40287474529699135, 0.40616016264324073, 0.4000000001346306, 0.4057494844376918, 0.4049281329589703, 0.40903490972714746, 0.4131416839128647, 0.4147843937976649, 0.41067761800861946, 0.41683778329551585, 0.4082135539035288, 0.4135523609434555, 0.4164271030949861, 0.4164271027033334, 0.39671458079340033, 0.40944558319614655, 0.4004106765777423, 0.3995893245115417, 0.40616016561735335, 0.40451745334591954, 0.4032854220950383, 0.4090349085521894, 0.4234086235813047, 0.41314168473288754, 0.41026694199387786, 0.4147843937976649, 0.41642710583655496, 0.4180698145463971, 0.41642710489414064, 0.40369609693482184, 0.4086242285474859, 0.41273100731064405, 0.41519507141573475, 0.40862423030992306, 0.4213552345118239, 0.41355236211841356, 0.4193018488081084, 0.4193018472047802, 0.41560574942545725, 0.4110882968383648, 0.40328541912092564, 0.4271047251180457, 0.4098562617933481, 0.4168377814963613, 0.39383983766273795, 0.3950718693419893, 0.3983572873124352, 0.3790554416497875, 0.4061601642098515, 0.3909650901871785, 0.3901437359301706, 0.3905544128016525, 0.39342915609142376, 0.399589324119889, 0.39917864391935926, 0.39671458020592126, 0.3954825485266699, 0.39958932392406266, 0.3991786427811186, 0.4086242285474859, 0.3967145804017476, 0.39958932094995003, 0.4012320344330594, 0.41108829781749656, 0.39712525762816475, 0.4008213559949667, 0.3991786461101665, 0.4168377811047086, 0.41437371895788144, 0.4164271040741178, 0.4160164280226588, 0.40246406349313335, 0.40287474451368593, 0.4061601664006588, 0.4020533894366552, 0.40739219843484537, 0.4049281306090541, 0.4151950708282557, 0.4193018488081084, 0.4147843963801249, 0.40985626457163443, 0.39671457961844225, 0.4008213565457283, 0.4139630407156151, 0.41067761800861946, 0.41273100492401044, 0.41478439461768773, 0.41026694156550775, 0.41437371895788144, 0.411498974652261, 0.41273100672316504, 0.40205338708673904, 0.39466119113644044, 0.4061601664006588, 0.40862423210907767, 0.4090349089438421, 0.4127310084856022, 0.41396303973648335, 0.4184804913811615, 0.41355236172676085, 0.4131416825420803, 0.4201232046317271, 0.412320330671706, 0.40739219592582027, 0.4135523623142399, 0.3975359342671028, 0.38316221645970117, 0.40328541974512216, 0.3917864464391673, 0.4114989736364118, 0.4024640649006352, 0.40369609634734277, 0.4049281319798385, 0.39876796551798405, 0.41314168332538564, 0.4094455830003202, 0.40369609830560627, 0.4057494854168236, 0.4020533866583689, 0.4106776164420087, 0.41437371699961795, 0.41396303914900434, 0.4172484613052384, 0.41930184798808556, 0.41930184978724017, 0.4176591359491955, 0.4147843928185332, 0.41642710646075143, 0.4168377826713194, 0.41314168610367197, 0.4147843932101859, 0.41478439265942424, 0.4168377811047086, 0.41724846009356287, 0.41642710426994417, 0.4164271054816197, 0.41642710622820767, 0.41437371598376876, 0.4180698143505707, 0.41560575165298197, 0.4201232022450935, 0.41971252541032905, 0.4168377814963613, 0.41683778051722953, 0.41889117197334397, 0.41724845774364666, 0.4131416823462539, 0.41519507341071565, 0.4110882946108401, 0.4139630413398116, 0.40000000134630614, 0.3979466132559571, 0.4086242305424669, 0.4041067741612389, 0.4053388085820592, 0.40246406564722315, 0.4082135517127215, 0.4102669382364598, 0.4102669427771833, 0.4201232032242252, 0.4131416843412348, 0.4160164260643953, 0.4193018460298221, 0.41273100574403326, 0.41642710387829146, 0.4143737162163125, 0.4160164256727426, 0.4254620110841747, 0.42792607698841995, 0.42422998081242524, 0.423408623189652, 0.42053387986316326, 0.41601642743517975, 0.4151950728232366, 0.41273100414070507, 0.41601642547691625, 0.40985626515911344, 0.41355235918101835, 0.412731006135686, 0.4139630385615253, 0.4147843948135141, 0.41642710387829146, 0.41478439203522777, 0.4197125262303519, 0.4188911707983859, 0.41396303797404627, 0.4082135546868342, 0.4156057506371328, 0.41190965223361337, 0.41396303679908814, 0.4094455853869538, 0.4193018491997611, 0.4168377842746476, 0.4151950722357576, 0.41478439399349126, 0.4197125266220046, 0.41724846071775934, 0.4102669427771833, 0.41765913673250094, 0.419301848771391, 0.42258726795351237, 0.41273100574403326, 0.42053387986316326, 0.42422997940492335, 0.4225872697159495, 0.42258726795351237, 0.42135523768176286, 0.4201232022450935, 0.42217659213459713, 0.42217659350538156, 0.423408626359591, 0.42546201167165376, 0.41806981709213964, 0.4201232045950096, 0.41355236156765196, 0.41601642766772357, 0.42381930378183447, 0.42422997940492335, 0.42628336867023053, 0.41560574844632553, 0.40410677831030967, 0.40780287429047807, 0.40739219706406093, 0.41355236211841356, 0.41765913814000283, 0.4188911697825367, 0.4176591382991117, 0.41848049357196876, 0.4143737148088107, 0.41314168273790663, 0.41396304075233253, 0.42422997940492335, 0.42094455728540675, 0.4193018491997611, 0.4176591363408482, 0.40903490577390306, 0.41848049314359864, 0.4188911704067332, 0.40082135380415945, 0.4114989732447591, 0.41190964890456544, 0.3897330579204481, 0.4188911697825367, 0.40041067540278424, 0.4057494881951099, 0.41642710329081245, 0.4176591359491955, 0.41560574864215183, 0.41806981712885705, 0.4172484622843701, 0.40082135482000864, 0.416016426456048, 0.4151950741940211, 0.3971252588031228, 0.41560574942545725, 0.4197125272094836, 0.41314168567530185, 0.412320328089246, 0.41437371598376876, 0.4188911691950577]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
