{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf71.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 22:16:53 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '01', '05', '04', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001A20F1BBE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001A20C896EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6186, Accuracy:0.1873, Validation Loss:1.6135, Validation Accuracy:0.1856\n",
    "Epoch #2: Loss:1.6105, Accuracy:0.2123, Validation Loss:1.6092, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6075, Accuracy:0.2329, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6062, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6023, Validation Accuracy:0.2397\n",
    "Epoch #23: Loss:1.6033, Accuracy:0.2370, Validation Loss:1.6018, Validation Accuracy:0.2397\n",
    "Epoch #24: Loss:1.6034, Accuracy:0.2398, Validation Loss:1.6016, Validation Accuracy:0.2381\n",
    "Epoch #25: Loss:1.6031, Accuracy:0.2386, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #26: Loss:1.6029, Accuracy:0.2390, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #27: Loss:1.6028, Accuracy:0.2398, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #28: Loss:1.6027, Accuracy:0.2411, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #29: Loss:1.6026, Accuracy:0.2435, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #30: Loss:1.6025, Accuracy:0.2431, Validation Loss:1.6006, Validation Accuracy:0.2397\n",
    "Epoch #31: Loss:1.6025, Accuracy:0.2435, Validation Loss:1.6006, Validation Accuracy:0.2397\n",
    "Epoch #32: Loss:1.6025, Accuracy:0.2435, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #33: Loss:1.6022, Accuracy:0.2435, Validation Loss:1.6007, Validation Accuracy:0.2332\n",
    "Epoch #34: Loss:1.6022, Accuracy:0.2444, Validation Loss:1.6007, Validation Accuracy:0.2315\n",
    "Epoch #35: Loss:1.6023, Accuracy:0.2435, Validation Loss:1.6008, Validation Accuracy:0.2315\n",
    "Epoch #36: Loss:1.6021, Accuracy:0.2431, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #37: Loss:1.6021, Accuracy:0.2427, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #38: Loss:1.6018, Accuracy:0.2431, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #39: Loss:1.6025, Accuracy:0.2439, Validation Loss:1.6005, Validation Accuracy:0.2299\n",
    "Epoch #40: Loss:1.6017, Accuracy:0.2435, Validation Loss:1.6005, Validation Accuracy:0.2365\n",
    "Epoch #41: Loss:1.6016, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2365\n",
    "Epoch #42: Loss:1.6014, Accuracy:0.2419, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #43: Loss:1.6012, Accuracy:0.2423, Validation Loss:1.6004, Validation Accuracy:0.2315\n",
    "Epoch #44: Loss:1.6012, Accuracy:0.2435, Validation Loss:1.6003, Validation Accuracy:0.2315\n",
    "Epoch #45: Loss:1.6011, Accuracy:0.2435, Validation Loss:1.6004, Validation Accuracy:0.2315\n",
    "Epoch #46: Loss:1.6010, Accuracy:0.2435, Validation Loss:1.6003, Validation Accuracy:0.2315\n",
    "Epoch #47: Loss:1.6013, Accuracy:0.2435, Validation Loss:1.6004, Validation Accuracy:0.2315\n",
    "Epoch #48: Loss:1.6011, Accuracy:0.2435, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #49: Loss:1.6011, Accuracy:0.2415, Validation Loss:1.6005, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.6009, Accuracy:0.2407, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #51: Loss:1.6007, Accuracy:0.2423, Validation Loss:1.6003, Validation Accuracy:0.2299\n",
    "Epoch #52: Loss:1.6008, Accuracy:0.2435, Validation Loss:1.6002, Validation Accuracy:0.2299\n",
    "Epoch #53: Loss:1.6006, Accuracy:0.2431, Validation Loss:1.6004, Validation Accuracy:0.2299\n",
    "Epoch #54: Loss:1.6004, Accuracy:0.2423, Validation Loss:1.6004, Validation Accuracy:0.2365\n",
    "Epoch #55: Loss:1.6002, Accuracy:0.2407, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #56: Loss:1.6001, Accuracy:0.2435, Validation Loss:1.6002, Validation Accuracy:0.2381\n",
    "Epoch #57: Loss:1.6000, Accuracy:0.2435, Validation Loss:1.5999, Validation Accuracy:0.2282\n",
    "Epoch #58: Loss:1.5999, Accuracy:0.2435, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #59: Loss:1.5996, Accuracy:0.2415, Validation Loss:1.5999, Validation Accuracy:0.2397\n",
    "Epoch #60: Loss:1.5997, Accuracy:0.2423, Validation Loss:1.5999, Validation Accuracy:0.2233\n",
    "Epoch #61: Loss:1.5997, Accuracy:0.2460, Validation Loss:1.6000, Validation Accuracy:0.2397\n",
    "Epoch #62: Loss:1.5999, Accuracy:0.2415, Validation Loss:1.6000, Validation Accuracy:0.2397\n",
    "Epoch #63: Loss:1.5989, Accuracy:0.2464, Validation Loss:1.6001, Validation Accuracy:0.2217\n",
    "Epoch #64: Loss:1.5995, Accuracy:0.2468, Validation Loss:1.6000, Validation Accuracy:0.2282\n",
    "Epoch #65: Loss:1.5989, Accuracy:0.2452, Validation Loss:1.6021, Validation Accuracy:0.2365\n",
    "Epoch #66: Loss:1.6024, Accuracy:0.2345, Validation Loss:1.6034, Validation Accuracy:0.2348\n",
    "Epoch #67: Loss:1.5997, Accuracy:0.2415, Validation Loss:1.6035, Validation Accuracy:0.2200\n",
    "Epoch #68: Loss:1.6009, Accuracy:0.2435, Validation Loss:1.6014, Validation Accuracy:0.2250\n",
    "Epoch #69: Loss:1.6015, Accuracy:0.2411, Validation Loss:1.6027, Validation Accuracy:0.2365\n",
    "Epoch #70: Loss:1.6005, Accuracy:0.2398, Validation Loss:1.6002, Validation Accuracy:0.2233\n",
    "Epoch #71: Loss:1.5992, Accuracy:0.2485, Validation Loss:1.6005, Validation Accuracy:0.2233\n",
    "Epoch #72: Loss:1.5996, Accuracy:0.2452, Validation Loss:1.6032, Validation Accuracy:0.2250\n",
    "Epoch #73: Loss:1.6008, Accuracy:0.2419, Validation Loss:1.6009, Validation Accuracy:0.2365\n",
    "Epoch #74: Loss:1.5985, Accuracy:0.2464, Validation Loss:1.6013, Validation Accuracy:0.2250\n",
    "Epoch #75: Loss:1.6010, Accuracy:0.2407, Validation Loss:1.6008, Validation Accuracy:0.2299\n",
    "Epoch #76: Loss:1.5994, Accuracy:0.2427, Validation Loss:1.6010, Validation Accuracy:0.2282\n",
    "Epoch #77: Loss:1.5991, Accuracy:0.2448, Validation Loss:1.6006, Validation Accuracy:0.2315\n",
    "Epoch #78: Loss:1.5984, Accuracy:0.2493, Validation Loss:1.5998, Validation Accuracy:0.2266\n",
    "Epoch #79: Loss:1.5986, Accuracy:0.2472, Validation Loss:1.5996, Validation Accuracy:0.2315\n",
    "Epoch #80: Loss:1.5978, Accuracy:0.2476, Validation Loss:1.6000, Validation Accuracy:0.2315\n",
    "Epoch #81: Loss:1.5980, Accuracy:0.2472, Validation Loss:1.5997, Validation Accuracy:0.2315\n",
    "Epoch #82: Loss:1.5980, Accuracy:0.2464, Validation Loss:1.6000, Validation Accuracy:0.2282\n",
    "Epoch #83: Loss:1.5977, Accuracy:0.2485, Validation Loss:1.6001, Validation Accuracy:0.2282\n",
    "Epoch #84: Loss:1.5976, Accuracy:0.2472, Validation Loss:1.6001, Validation Accuracy:0.2282\n",
    "Epoch #85: Loss:1.5978, Accuracy:0.2485, Validation Loss:1.5997, Validation Accuracy:0.2299\n",
    "Epoch #86: Loss:1.5973, Accuracy:0.2456, Validation Loss:1.5999, Validation Accuracy:0.2299\n",
    "Epoch #87: Loss:1.5974, Accuracy:0.2468, Validation Loss:1.5999, Validation Accuracy:0.2299\n",
    "Epoch #88: Loss:1.5973, Accuracy:0.2468, Validation Loss:1.5995, Validation Accuracy:0.2348\n",
    "Epoch #89: Loss:1.5972, Accuracy:0.2452, Validation Loss:1.5990, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:1.5970, Accuracy:0.2460, Validation Loss:1.5991, Validation Accuracy:0.2315\n",
    "Epoch #91: Loss:1.5969, Accuracy:0.2480, Validation Loss:1.5990, Validation Accuracy:0.2299\n",
    "Epoch #92: Loss:1.5971, Accuracy:0.2456, Validation Loss:1.5992, Validation Accuracy:0.2315\n",
    "Epoch #93: Loss:1.5975, Accuracy:0.2444, Validation Loss:1.6000, Validation Accuracy:0.2332\n",
    "Epoch #94: Loss:1.5974, Accuracy:0.2439, Validation Loss:1.6003, Validation Accuracy:0.2315\n",
    "Epoch #95: Loss:1.5979, Accuracy:0.2448, Validation Loss:1.6003, Validation Accuracy:0.2282\n",
    "Epoch #96: Loss:1.5983, Accuracy:0.2419, Validation Loss:1.6002, Validation Accuracy:0.2332\n",
    "Epoch #97: Loss:1.5973, Accuracy:0.2427, Validation Loss:1.6000, Validation Accuracy:0.2299\n",
    "Epoch #98: Loss:1.5974, Accuracy:0.2468, Validation Loss:1.5995, Validation Accuracy:0.2315\n",
    "Epoch #99: Loss:1.5970, Accuracy:0.2431, Validation Loss:1.5990, Validation Accuracy:0.2315\n",
    "Epoch #100: Loss:1.5970, Accuracy:0.2448, Validation Loss:1.5991, Validation Accuracy:0.2332\n",
    "Epoch #101: Loss:1.5969, Accuracy:0.2423, Validation Loss:1.5993, Validation Accuracy:0.2332\n",
    "Epoch #102: Loss:1.5969, Accuracy:0.2423, Validation Loss:1.5997, Validation Accuracy:0.2332\n",
    "Epoch #103: Loss:1.5968, Accuracy:0.2423, Validation Loss:1.5994, Validation Accuracy:0.2315\n",
    "Epoch #104: Loss:1.5970, Accuracy:0.2448, Validation Loss:1.5996, Validation Accuracy:0.2315\n",
    "Epoch #105: Loss:1.5968, Accuracy:0.2444, Validation Loss:1.5995, Validation Accuracy:0.2332\n",
    "Epoch #106: Loss:1.5970, Accuracy:0.2431, Validation Loss:1.5996, Validation Accuracy:0.2332\n",
    "Epoch #107: Loss:1.5974, Accuracy:0.2415, Validation Loss:1.5999, Validation Accuracy:0.2315\n",
    "Epoch #108: Loss:1.5968, Accuracy:0.2427, Validation Loss:1.6003, Validation Accuracy:0.2414\n",
    "Epoch #109: Loss:1.5974, Accuracy:0.2468, Validation Loss:1.5999, Validation Accuracy:0.2315\n",
    "Epoch #110: Loss:1.5965, Accuracy:0.2468, Validation Loss:1.6000, Validation Accuracy:0.2315\n",
    "Epoch #111: Loss:1.5974, Accuracy:0.2472, Validation Loss:1.6002, Validation Accuracy:0.2332\n",
    "Epoch #112: Loss:1.5970, Accuracy:0.2468, Validation Loss:1.5993, Validation Accuracy:0.2479\n",
    "Epoch #113: Loss:1.5970, Accuracy:0.2505, Validation Loss:1.5991, Validation Accuracy:0.2414\n",
    "Epoch #114: Loss:1.5971, Accuracy:0.2476, Validation Loss:1.5996, Validation Accuracy:0.2315\n",
    "Epoch #115: Loss:1.5968, Accuracy:0.2485, Validation Loss:1.5990, Validation Accuracy:0.2430\n",
    "Epoch #116: Loss:1.5972, Accuracy:0.2480, Validation Loss:1.5997, Validation Accuracy:0.2414\n",
    "Epoch #117: Loss:1.5963, Accuracy:0.2452, Validation Loss:1.5994, Validation Accuracy:0.2299\n",
    "Epoch #118: Loss:1.5965, Accuracy:0.2452, Validation Loss:1.5986, Validation Accuracy:0.2430\n",
    "Epoch #119: Loss:1.5968, Accuracy:0.2497, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #120: Loss:1.5965, Accuracy:0.2468, Validation Loss:1.5998, Validation Accuracy:0.2315\n",
    "Epoch #121: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.5997, Validation Accuracy:0.2299\n",
    "Epoch #122: Loss:1.5969, Accuracy:0.2444, Validation Loss:1.5992, Validation Accuracy:0.2447\n",
    "Epoch #123: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.5990, Validation Accuracy:0.2414\n",
    "Epoch #124: Loss:1.5978, Accuracy:0.2448, Validation Loss:1.5993, Validation Accuracy:0.2365\n",
    "Epoch #125: Loss:1.5968, Accuracy:0.2386, Validation Loss:1.5984, Validation Accuracy:0.2397\n",
    "Epoch #126: Loss:1.5968, Accuracy:0.2398, Validation Loss:1.5985, Validation Accuracy:0.2332\n",
    "Epoch #127: Loss:1.5970, Accuracy:0.2390, Validation Loss:1.5998, Validation Accuracy:0.2315\n",
    "Epoch #128: Loss:1.5974, Accuracy:0.2435, Validation Loss:1.5996, Validation Accuracy:0.2282\n",
    "Epoch #129: Loss:1.5971, Accuracy:0.2448, Validation Loss:1.6001, Validation Accuracy:0.2266\n",
    "Epoch #130: Loss:1.5974, Accuracy:0.2419, Validation Loss:1.5997, Validation Accuracy:0.2282\n",
    "Epoch #131: Loss:1.5972, Accuracy:0.2431, Validation Loss:1.5996, Validation Accuracy:0.2250\n",
    "Epoch #132: Loss:1.5976, Accuracy:0.2419, Validation Loss:1.5999, Validation Accuracy:0.2250\n",
    "Epoch #133: Loss:1.5971, Accuracy:0.2415, Validation Loss:1.5998, Validation Accuracy:0.2250\n",
    "Epoch #134: Loss:1.5969, Accuracy:0.2419, Validation Loss:1.5997, Validation Accuracy:0.2250\n",
    "Epoch #135: Loss:1.5970, Accuracy:0.2411, Validation Loss:1.5998, Validation Accuracy:0.2250\n",
    "Epoch #136: Loss:1.5971, Accuracy:0.2419, Validation Loss:1.5992, Validation Accuracy:0.2250\n",
    "Epoch #137: Loss:1.5968, Accuracy:0.2423, Validation Loss:1.5985, Validation Accuracy:0.2299\n",
    "Epoch #138: Loss:1.5968, Accuracy:0.2411, Validation Loss:1.5985, Validation Accuracy:0.2282\n",
    "Epoch #139: Loss:1.5967, Accuracy:0.2464, Validation Loss:1.5986, Validation Accuracy:0.2250\n",
    "Epoch #140: Loss:1.5962, Accuracy:0.2448, Validation Loss:1.5987, Validation Accuracy:0.2332\n",
    "Epoch #141: Loss:1.5963, Accuracy:0.2476, Validation Loss:1.5987, Validation Accuracy:0.2332\n",
    "Epoch #142: Loss:1.5961, Accuracy:0.2439, Validation Loss:1.5989, Validation Accuracy:0.2233\n",
    "Epoch #143: Loss:1.5961, Accuracy:0.2411, Validation Loss:1.5983, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5957, Accuracy:0.2423, Validation Loss:1.5979, Validation Accuracy:0.2381\n",
    "Epoch #145: Loss:1.5957, Accuracy:0.2468, Validation Loss:1.5983, Validation Accuracy:0.2315\n",
    "Epoch #146: Loss:1.5961, Accuracy:0.2464, Validation Loss:1.5983, Validation Accuracy:0.2332\n",
    "Epoch #147: Loss:1.5963, Accuracy:0.2480, Validation Loss:1.5974, Validation Accuracy:0.2397\n",
    "Epoch #148: Loss:1.5964, Accuracy:0.2480, Validation Loss:1.5978, Validation Accuracy:0.2332\n",
    "Epoch #149: Loss:1.5960, Accuracy:0.2448, Validation Loss:1.5974, Validation Accuracy:0.2315\n",
    "Epoch #150: Loss:1.5965, Accuracy:0.2452, Validation Loss:1.5974, Validation Accuracy:0.2397\n",
    "Epoch #151: Loss:1.5959, Accuracy:0.2501, Validation Loss:1.5978, Validation Accuracy:0.2299\n",
    "Epoch #152: Loss:1.5964, Accuracy:0.2489, Validation Loss:1.5983, Validation Accuracy:0.2332\n",
    "Epoch #153: Loss:1.5969, Accuracy:0.2480, Validation Loss:1.5997, Validation Accuracy:0.2348\n",
    "Epoch #154: Loss:1.5967, Accuracy:0.2468, Validation Loss:1.5998, Validation Accuracy:0.2299\n",
    "Epoch #155: Loss:1.5972, Accuracy:0.2468, Validation Loss:1.5993, Validation Accuracy:0.2250\n",
    "Epoch #156: Loss:1.5965, Accuracy:0.2427, Validation Loss:1.6001, Validation Accuracy:0.2299\n",
    "Epoch #157: Loss:1.5970, Accuracy:0.2435, Validation Loss:1.6003, Validation Accuracy:0.2315\n",
    "Epoch #158: Loss:1.5976, Accuracy:0.2419, Validation Loss:1.6010, Validation Accuracy:0.2282\n",
    "Epoch #159: Loss:1.5980, Accuracy:0.2423, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #160: Loss:1.5978, Accuracy:0.2448, Validation Loss:1.6010, Validation Accuracy:0.2348\n",
    "Epoch #161: Loss:1.5977, Accuracy:0.2448, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #162: Loss:1.5975, Accuracy:0.2423, Validation Loss:1.6008, Validation Accuracy:0.2282\n",
    "Epoch #163: Loss:1.5972, Accuracy:0.2464, Validation Loss:1.6006, Validation Accuracy:0.2282\n",
    "Epoch #164: Loss:1.5977, Accuracy:0.2456, Validation Loss:1.6009, Validation Accuracy:0.2348\n",
    "Epoch #165: Loss:1.5972, Accuracy:0.2444, Validation Loss:1.6006, Validation Accuracy:0.2282\n",
    "Epoch #166: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.6003, Validation Accuracy:0.2217\n",
    "Epoch #167: Loss:1.5972, Accuracy:0.2452, Validation Loss:1.6011, Validation Accuracy:0.2250\n",
    "Epoch #168: Loss:1.5973, Accuracy:0.2431, Validation Loss:1.6010, Validation Accuracy:0.2315\n",
    "Epoch #169: Loss:1.5970, Accuracy:0.2448, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #170: Loss:1.5972, Accuracy:0.2460, Validation Loss:1.6010, Validation Accuracy:0.2282\n",
    "Epoch #171: Loss:1.5972, Accuracy:0.2431, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #172: Loss:1.5968, Accuracy:0.2456, Validation Loss:1.6001, Validation Accuracy:0.2381\n",
    "Epoch #173: Loss:1.5970, Accuracy:0.2439, Validation Loss:1.5998, Validation Accuracy:0.2217\n",
    "Epoch #174: Loss:1.5968, Accuracy:0.2366, Validation Loss:1.5991, Validation Accuracy:0.2397\n",
    "Epoch #175: Loss:1.5966, Accuracy:0.2415, Validation Loss:1.5992, Validation Accuracy:0.2348\n",
    "Epoch #176: Loss:1.5967, Accuracy:0.2456, Validation Loss:1.5992, Validation Accuracy:0.2315\n",
    "Epoch #177: Loss:1.5969, Accuracy:0.2427, Validation Loss:1.5996, Validation Accuracy:0.2381\n",
    "Epoch #178: Loss:1.5964, Accuracy:0.2411, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #179: Loss:1.5978, Accuracy:0.2378, Validation Loss:1.6020, Validation Accuracy:0.2233\n",
    "Epoch #180: Loss:1.5982, Accuracy:0.2349, Validation Loss:1.6000, Validation Accuracy:0.2397\n",
    "Epoch #181: Loss:1.5970, Accuracy:0.2464, Validation Loss:1.5993, Validation Accuracy:0.2414\n",
    "Epoch #182: Loss:1.5979, Accuracy:0.2493, Validation Loss:1.5986, Validation Accuracy:0.2414\n",
    "Epoch #183: Loss:1.5973, Accuracy:0.2468, Validation Loss:1.5994, Validation Accuracy:0.2315\n",
    "Epoch #184: Loss:1.5963, Accuracy:0.2448, Validation Loss:1.5995, Validation Accuracy:0.2365\n",
    "Epoch #185: Loss:1.5970, Accuracy:0.2493, Validation Loss:1.5991, Validation Accuracy:0.2299\n",
    "Epoch #186: Loss:1.5969, Accuracy:0.2439, Validation Loss:1.5987, Validation Accuracy:0.2315\n",
    "Epoch #187: Loss:1.5970, Accuracy:0.2456, Validation Loss:1.5995, Validation Accuracy:0.2365\n",
    "Epoch #188: Loss:1.5962, Accuracy:0.2439, Validation Loss:1.6004, Validation Accuracy:0.2381\n",
    "Epoch #189: Loss:1.5965, Accuracy:0.2448, Validation Loss:1.5994, Validation Accuracy:0.2332\n",
    "Epoch #190: Loss:1.5957, Accuracy:0.2485, Validation Loss:1.5989, Validation Accuracy:0.2365\n",
    "Epoch #191: Loss:1.5955, Accuracy:0.2489, Validation Loss:1.5991, Validation Accuracy:0.2381\n",
    "Epoch #192: Loss:1.5957, Accuracy:0.2468, Validation Loss:1.5988, Validation Accuracy:0.2365\n",
    "Epoch #193: Loss:1.5960, Accuracy:0.2476, Validation Loss:1.5992, Validation Accuracy:0.2315\n",
    "Epoch #194: Loss:1.5959, Accuracy:0.2476, Validation Loss:1.5989, Validation Accuracy:0.2365\n",
    "Epoch #195: Loss:1.5953, Accuracy:0.2460, Validation Loss:1.5988, Validation Accuracy:0.2315\n",
    "Epoch #196: Loss:1.5967, Accuracy:0.2476, Validation Loss:1.5990, Validation Accuracy:0.2365\n",
    "Epoch #197: Loss:1.5959, Accuracy:0.2485, Validation Loss:1.5990, Validation Accuracy:0.2381\n",
    "Epoch #198: Loss:1.5971, Accuracy:0.2435, Validation Loss:1.5985, Validation Accuracy:0.2365\n",
    "Epoch #199: Loss:1.5954, Accuracy:0.2439, Validation Loss:1.5995, Validation Accuracy:0.2315\n",
    "Epoch #200: Loss:1.5951, Accuracy:0.2394, Validation Loss:1.5987, Validation Accuracy:0.2365\n",
    "Epoch #201: Loss:1.5954, Accuracy:0.2501, Validation Loss:1.5985, Validation Accuracy:0.2365\n",
    "Epoch #202: Loss:1.5960, Accuracy:0.2489, Validation Loss:1.5989, Validation Accuracy:0.2332\n",
    "Epoch #203: Loss:1.5954, Accuracy:0.2452, Validation Loss:1.5990, Validation Accuracy:0.2381\n",
    "Epoch #204: Loss:1.5952, Accuracy:0.2489, Validation Loss:1.5988, Validation Accuracy:0.2365\n",
    "Epoch #205: Loss:1.5955, Accuracy:0.2497, Validation Loss:1.5985, Validation Accuracy:0.2365\n",
    "Epoch #206: Loss:1.5962, Accuracy:0.2419, Validation Loss:1.5992, Validation Accuracy:0.2381\n",
    "Epoch #207: Loss:1.5956, Accuracy:0.2493, Validation Loss:1.5995, Validation Accuracy:0.2414\n",
    "Epoch #208: Loss:1.5955, Accuracy:0.2485, Validation Loss:1.5988, Validation Accuracy:0.2397\n",
    "Epoch #209: Loss:1.5953, Accuracy:0.2435, Validation Loss:1.5989, Validation Accuracy:0.2365\n",
    "Epoch #210: Loss:1.5949, Accuracy:0.2489, Validation Loss:1.5982, Validation Accuracy:0.2430\n",
    "Epoch #211: Loss:1.5949, Accuracy:0.2505, Validation Loss:1.5982, Validation Accuracy:0.2381\n",
    "Epoch #212: Loss:1.5945, Accuracy:0.2480, Validation Loss:1.5986, Validation Accuracy:0.2365\n",
    "Epoch #213: Loss:1.5951, Accuracy:0.2464, Validation Loss:1.5987, Validation Accuracy:0.2365\n",
    "Epoch #214: Loss:1.5959, Accuracy:0.2452, Validation Loss:1.5986, Validation Accuracy:0.2381\n",
    "Epoch #215: Loss:1.5964, Accuracy:0.2489, Validation Loss:1.5992, Validation Accuracy:0.2397\n",
    "Epoch #216: Loss:1.5952, Accuracy:0.2472, Validation Loss:1.5991, Validation Accuracy:0.2397\n",
    "Epoch #217: Loss:1.5954, Accuracy:0.2468, Validation Loss:1.5981, Validation Accuracy:0.2430\n",
    "Epoch #218: Loss:1.5949, Accuracy:0.2522, Validation Loss:1.5980, Validation Accuracy:0.2430\n",
    "Epoch #219: Loss:1.5960, Accuracy:0.2509, Validation Loss:1.5987, Validation Accuracy:0.2365\n",
    "Epoch #220: Loss:1.5954, Accuracy:0.2361, Validation Loss:1.5983, Validation Accuracy:0.2332\n",
    "Epoch #221: Loss:1.5948, Accuracy:0.2489, Validation Loss:1.5979, Validation Accuracy:0.2430\n",
    "Epoch #222: Loss:1.5948, Accuracy:0.2497, Validation Loss:1.5979, Validation Accuracy:0.2430\n",
    "Epoch #223: Loss:1.5946, Accuracy:0.2522, Validation Loss:1.5980, Validation Accuracy:0.2430\n",
    "Epoch #224: Loss:1.5946, Accuracy:0.2526, Validation Loss:1.5981, Validation Accuracy:0.2365\n",
    "Epoch #225: Loss:1.5947, Accuracy:0.2489, Validation Loss:1.5977, Validation Accuracy:0.2430\n",
    "Epoch #226: Loss:1.5942, Accuracy:0.2522, Validation Loss:1.5985, Validation Accuracy:0.2430\n",
    "Epoch #227: Loss:1.5944, Accuracy:0.2489, Validation Loss:1.5991, Validation Accuracy:0.2365\n",
    "Epoch #228: Loss:1.5943, Accuracy:0.2464, Validation Loss:1.5981, Validation Accuracy:0.2430\n",
    "Epoch #229: Loss:1.5951, Accuracy:0.2501, Validation Loss:1.5984, Validation Accuracy:0.2381\n",
    "Epoch #230: Loss:1.5944, Accuracy:0.2501, Validation Loss:1.5976, Validation Accuracy:0.2430\n",
    "Epoch #231: Loss:1.5948, Accuracy:0.2522, Validation Loss:1.5974, Validation Accuracy:0.2430\n",
    "Epoch #232: Loss:1.5942, Accuracy:0.2554, Validation Loss:1.5983, Validation Accuracy:0.2365\n",
    "Epoch #233: Loss:1.5942, Accuracy:0.2476, Validation Loss:1.5983, Validation Accuracy:0.2365\n",
    "Epoch #234: Loss:1.5942, Accuracy:0.2497, Validation Loss:1.5972, Validation Accuracy:0.2430\n",
    "Epoch #235: Loss:1.5944, Accuracy:0.2522, Validation Loss:1.5973, Validation Accuracy:0.2430\n",
    "Epoch #236: Loss:1.5947, Accuracy:0.2456, Validation Loss:1.5986, Validation Accuracy:0.2365\n",
    "Epoch #237: Loss:1.5938, Accuracy:0.2476, Validation Loss:1.5983, Validation Accuracy:0.2381\n",
    "Epoch #238: Loss:1.5941, Accuracy:0.2485, Validation Loss:1.5978, Validation Accuracy:0.2447\n",
    "Epoch #239: Loss:1.5942, Accuracy:0.2456, Validation Loss:1.5978, Validation Accuracy:0.2414\n",
    "Epoch #240: Loss:1.5942, Accuracy:0.2522, Validation Loss:1.5977, Validation Accuracy:0.2479\n",
    "Epoch #241: Loss:1.5938, Accuracy:0.2505, Validation Loss:1.5978, Validation Accuracy:0.2365\n",
    "Epoch #242: Loss:1.5946, Accuracy:0.2435, Validation Loss:1.5977, Validation Accuracy:0.2365\n",
    "Epoch #243: Loss:1.5940, Accuracy:0.2489, Validation Loss:1.5978, Validation Accuracy:0.2463\n",
    "Epoch #244: Loss:1.5937, Accuracy:0.2480, Validation Loss:1.5975, Validation Accuracy:0.2365\n",
    "Epoch #245: Loss:1.5935, Accuracy:0.2472, Validation Loss:1.5975, Validation Accuracy:0.2430\n",
    "Epoch #246: Loss:1.5937, Accuracy:0.2538, Validation Loss:1.5971, Validation Accuracy:0.2430\n",
    "Epoch #247: Loss:1.5937, Accuracy:0.2522, Validation Loss:1.5974, Validation Accuracy:0.2430\n",
    "Epoch #248: Loss:1.5933, Accuracy:0.2522, Validation Loss:1.5974, Validation Accuracy:0.2430\n",
    "Epoch #249: Loss:1.5937, Accuracy:0.2522, Validation Loss:1.5984, Validation Accuracy:0.2381\n",
    "Epoch #250: Loss:1.5934, Accuracy:0.2489, Validation Loss:1.5976, Validation Accuracy:0.2447\n",
    "Epoch #251: Loss:1.5940, Accuracy:0.2534, Validation Loss:1.5970, Validation Accuracy:0.2447\n",
    "Epoch #252: Loss:1.5936, Accuracy:0.2485, Validation Loss:1.5982, Validation Accuracy:0.2463\n",
    "Epoch #253: Loss:1.5934, Accuracy:0.2489, Validation Loss:1.5977, Validation Accuracy:0.2479\n",
    "Epoch #254: Loss:1.5941, Accuracy:0.2517, Validation Loss:1.5974, Validation Accuracy:0.2447\n",
    "Epoch #255: Loss:1.5931, Accuracy:0.2493, Validation Loss:1.5985, Validation Accuracy:0.2414\n",
    "Epoch #256: Loss:1.5934, Accuracy:0.2468, Validation Loss:1.5972, Validation Accuracy:0.2447\n",
    "Epoch #257: Loss:1.5937, Accuracy:0.2526, Validation Loss:1.5978, Validation Accuracy:0.2496\n",
    "Epoch #258: Loss:1.5929, Accuracy:0.2526, Validation Loss:1.5977, Validation Accuracy:0.2463\n",
    "Epoch #259: Loss:1.5936, Accuracy:0.2472, Validation Loss:1.5979, Validation Accuracy:0.2430\n",
    "Epoch #260: Loss:1.5927, Accuracy:0.2542, Validation Loss:1.5978, Validation Accuracy:0.2479\n",
    "Epoch #261: Loss:1.5935, Accuracy:0.2517, Validation Loss:1.5977, Validation Accuracy:0.2447\n",
    "Epoch #262: Loss:1.5924, Accuracy:0.2526, Validation Loss:1.5975, Validation Accuracy:0.2479\n",
    "Epoch #263: Loss:1.5935, Accuracy:0.2493, Validation Loss:1.5971, Validation Accuracy:0.2496\n",
    "Epoch #264: Loss:1.5928, Accuracy:0.2493, Validation Loss:1.5973, Validation Accuracy:0.2447\n",
    "Epoch #265: Loss:1.5928, Accuracy:0.2534, Validation Loss:1.5987, Validation Accuracy:0.2381\n",
    "Epoch #266: Loss:1.5930, Accuracy:0.2489, Validation Loss:1.5984, Validation Accuracy:0.2447\n",
    "Epoch #267: Loss:1.5925, Accuracy:0.2530, Validation Loss:1.5973, Validation Accuracy:0.2496\n",
    "Epoch #268: Loss:1.5927, Accuracy:0.2464, Validation Loss:1.5968, Validation Accuracy:0.2447\n",
    "Epoch #269: Loss:1.5924, Accuracy:0.2534, Validation Loss:1.5978, Validation Accuracy:0.2447\n",
    "Epoch #270: Loss:1.5930, Accuracy:0.2468, Validation Loss:1.5981, Validation Accuracy:0.2447\n",
    "Epoch #271: Loss:1.5923, Accuracy:0.2448, Validation Loss:1.5974, Validation Accuracy:0.2447\n",
    "Epoch #272: Loss:1.5924, Accuracy:0.2583, Validation Loss:1.5975, Validation Accuracy:0.2447\n",
    "Epoch #273: Loss:1.5924, Accuracy:0.2534, Validation Loss:1.5978, Validation Accuracy:0.2447\n",
    "Epoch #274: Loss:1.5932, Accuracy:0.2485, Validation Loss:1.5985, Validation Accuracy:0.2479\n",
    "Epoch #275: Loss:1.5923, Accuracy:0.2497, Validation Loss:1.5978, Validation Accuracy:0.2414\n",
    "Epoch #276: Loss:1.5929, Accuracy:0.2591, Validation Loss:1.5977, Validation Accuracy:0.2479\n",
    "Epoch #277: Loss:1.5918, Accuracy:0.2538, Validation Loss:1.5984, Validation Accuracy:0.2496\n",
    "Epoch #278: Loss:1.5926, Accuracy:0.2480, Validation Loss:1.5979, Validation Accuracy:0.2512\n",
    "Epoch #279: Loss:1.5924, Accuracy:0.2493, Validation Loss:1.5974, Validation Accuracy:0.2479\n",
    "Epoch #280: Loss:1.5917, Accuracy:0.2550, Validation Loss:1.5980, Validation Accuracy:0.2479\n",
    "Epoch #281: Loss:1.5921, Accuracy:0.2476, Validation Loss:1.5981, Validation Accuracy:0.2529\n",
    "Epoch #282: Loss:1.5921, Accuracy:0.2522, Validation Loss:1.5976, Validation Accuracy:0.2479\n",
    "Epoch #283: Loss:1.5919, Accuracy:0.2550, Validation Loss:1.5980, Validation Accuracy:0.2479\n",
    "Epoch #284: Loss:1.5916, Accuracy:0.2542, Validation Loss:1.5983, Validation Accuracy:0.2512\n",
    "Epoch #285: Loss:1.5917, Accuracy:0.2517, Validation Loss:1.5985, Validation Accuracy:0.2447\n",
    "Epoch #286: Loss:1.5920, Accuracy:0.2526, Validation Loss:1.5978, Validation Accuracy:0.2479\n",
    "Epoch #287: Loss:1.5919, Accuracy:0.2522, Validation Loss:1.5985, Validation Accuracy:0.2496\n",
    "Epoch #288: Loss:1.5913, Accuracy:0.2517, Validation Loss:1.5974, Validation Accuracy:0.2479\n",
    "Epoch #289: Loss:1.5919, Accuracy:0.2546, Validation Loss:1.5985, Validation Accuracy:0.2479\n",
    "Epoch #290: Loss:1.5913, Accuracy:0.2538, Validation Loss:1.5992, Validation Accuracy:0.2512\n",
    "Epoch #291: Loss:1.5914, Accuracy:0.2501, Validation Loss:1.5980, Validation Accuracy:0.2529\n",
    "Epoch #292: Loss:1.5913, Accuracy:0.2501, Validation Loss:1.5980, Validation Accuracy:0.2479\n",
    "Epoch #293: Loss:1.5916, Accuracy:0.2583, Validation Loss:1.5989, Validation Accuracy:0.2479\n",
    "Epoch #294: Loss:1.5911, Accuracy:0.2550, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #295: Loss:1.5912, Accuracy:0.2509, Validation Loss:1.5989, Validation Accuracy:0.2512\n",
    "Epoch #296: Loss:1.5913, Accuracy:0.2505, Validation Loss:1.5975, Validation Accuracy:0.2529\n",
    "Epoch #297: Loss:1.5915, Accuracy:0.2534, Validation Loss:1.5986, Validation Accuracy:0.2479\n",
    "Epoch #298: Loss:1.5909, Accuracy:0.2550, Validation Loss:1.5991, Validation Accuracy:0.2479\n",
    "Epoch #299: Loss:1.5910, Accuracy:0.2517, Validation Loss:1.5988, Validation Accuracy:0.2447\n",
    "Epoch #300: Loss:1.5911, Accuracy:0.2460, Validation Loss:1.5994, Validation Accuracy:0.2447\n",
    "\n",
    "Test:\n",
    "Test Loss:1.59935379, Accuracy:0.2447\n",
    "Labels: ['03', '01', '05', '04', '02']\n",
    "Confusion Matrix:\n",
    "      03  01   05  04  02\n",
    "t:03   4   4   88  15   4\n",
    "t:01   2   5   98  19   2\n",
    "t:05   3   6  115  17   1\n",
    "t:04   0   5   82  22   3\n",
    "t:02   2   6   82  21   3\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.36      0.03      0.06       115\n",
    "          01       0.19      0.04      0.07       126\n",
    "          05       0.25      0.81      0.38       142\n",
    "          04       0.23      0.20      0.21       112\n",
    "          02       0.23      0.03      0.05       114\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.25      0.22      0.15       609\n",
    "weighted avg       0.25      0.24      0.16       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 22:57:27 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6135005199263248, 1.609208326621596, 1.60627550855646, 1.6059829092573845, 1.6054117182401209, 1.6050219265698211, 1.6046020070516025, 1.6048689628469532, 1.6049731725150924, 1.6049815428080818, 1.6048936693147682, 1.604592053369544, 1.6045784617488217, 1.6045373550972524, 1.6042660351457267, 1.6038251925376052, 1.6035952675714478, 1.603415369008758, 1.6032073356835126, 1.6030358641801405, 1.6025023998689574, 1.6022575175625153, 1.6018443728119673, 1.6016303729541197, 1.601401442964676, 1.601242389584997, 1.601108939776867, 1.6009169918758723, 1.6006872683323075, 1.6006092892100268, 1.60061264762346, 1.6006669140801641, 1.6006667232278533, 1.6007185385536482, 1.6007872510621897, 1.60079987765533, 1.6008097150642884, 1.6002720429979522, 1.6005255768843276, 1.6005203995994355, 1.6005924570149388, 1.6006127562624677, 1.6003995325373508, 1.6003283091953822, 1.6003605135164434, 1.6002740306024286, 1.60037890247915, 1.6006736612476542, 1.6004959579759044, 1.6003424234578174, 1.6002923673009637, 1.600190964434143, 1.6003905726575303, 1.6003884433329791, 1.6003377881934882, 1.6001577833407423, 1.599876796866481, 1.5999058625968219, 1.5999136393880609, 1.5998542389063217, 1.5999533685753107, 1.5999930333621397, 1.6000833290159604, 1.5999801466226187, 1.6020706580777473, 1.6034447956946487, 1.6034518067276928, 1.6013511397764209, 1.6027452998560638, 1.6002093813885216, 1.6005037148010555, 1.6031579744248163, 1.600924941119302, 1.601303727168755, 1.600760798736159, 1.6009973984438015, 1.6006495837116086, 1.5997951576862428, 1.5995756661754914, 1.600013617028548, 1.5996540933602745, 1.5999725058748218, 1.6000836954523974, 1.600055719048323, 1.5997414332501014, 1.5999046376186052, 1.59992853448113, 1.5994526991507494, 1.5989962223128145, 1.5990604665283303, 1.5990012743202924, 1.5991906763493329, 1.5999683856181128, 1.6003147921538705, 1.6002804395209, 1.600157336844208, 1.599990998387141, 1.5994958983266294, 1.5990298005747678, 1.599126627292539, 1.5993377555571557, 1.5996595695492473, 1.599415391928261, 1.5995588224313921, 1.5994701506860542, 1.599623174502932, 1.599908236799569, 1.6002585322203111, 1.5998882348901533, 1.6000073012851534, 1.6001816488839136, 1.5993257430190915, 1.5990501344693313, 1.5995918529961497, 1.5990368370547867, 1.5997064941622354, 1.5994141646010926, 1.598609207886193, 1.5996152056849062, 1.599751392608793, 1.5997419102830057, 1.599198516562263, 1.5990153785996837, 1.5993048494868287, 1.5984397596130622, 1.5984885173869643, 1.5997702803322051, 1.5996056910610355, 1.6000579168839604, 1.5996630491294297, 1.5996009557705206, 1.5999207664984592, 1.5997616798419672, 1.599673575172675, 1.5997716628859195, 1.5991849507800073, 1.5984754621101718, 1.5985203204288077, 1.598593927369329, 1.5987092816379467, 1.5986706118278315, 1.5989442081091243, 1.5983295638377248, 1.5979126255304748, 1.5982697051146935, 1.5982584563773645, 1.5973877035729795, 1.5977505652971065, 1.597445332553782, 1.597425931780209, 1.5977870364886004, 1.5983271657539706, 1.5996829562978008, 1.5998068067240598, 1.5993035769423436, 1.6001112897603578, 1.6003462021378265, 1.600964635463771, 1.6007143474369019, 1.6009644712329107, 1.6009912807953182, 1.6007674944224617, 1.6006141165011427, 1.6009297410059837, 1.6005521934412188, 1.600289665615226, 1.6010892999974768, 1.6009953981158378, 1.6011786227938773, 1.6010065656185932, 1.60038869232184, 1.600138900706725, 1.5997654138918973, 1.5991079349235948, 1.599186053808491, 1.5991555779242554, 1.5995567496774232, 1.5993696985573604, 1.6019991740021604, 1.5999813263835188, 1.5992843750466659, 1.5986133512409253, 1.599410359691125, 1.599469581065311, 1.5991187606539046, 1.5987171287019852, 1.5995068534450188, 1.6004298988038488, 1.5994051144823849, 1.5988923279914167, 1.5990630125960301, 1.5988342145393635, 1.5991920440263545, 1.5989119096342566, 1.5988264461652006, 1.5989621073154394, 1.5990332566654348, 1.598525804056127, 1.5994915813452308, 1.598732185481217, 1.5984887683332847, 1.5989427558698481, 1.5989799499511719, 1.5987705373998933, 1.5985066281946618, 1.5991904134625088, 1.5995054225420522, 1.5987527785434317, 1.5989198479159126, 1.5982303198530952, 1.5981565821542725, 1.5985969375506999, 1.5987117699605882, 1.5985620221480947, 1.5992117380273754, 1.5991232285554382, 1.598135115282093, 1.5979875656967288, 1.5986802423333104, 1.5982550284741155, 1.5979405285298138, 1.5978514251646345, 1.5979567419719227, 1.5981051645842679, 1.597745296990343, 1.5984613511754178, 1.5990650475710289, 1.5980670669395936, 1.5983980118934744, 1.5976257537581846, 1.5973784117080112, 1.5982764518906918, 1.5982744431456517, 1.5972421323920314, 1.597331147084291, 1.5986478526408254, 1.598308821225597, 1.5978389089722156, 1.597773181589562, 1.5977221936819392, 1.5977715512214623, 1.5976593373052788, 1.5978430908889019, 1.597540250552699, 1.5975220727998831, 1.5971286034544896, 1.597404595862077, 1.597368471140932, 1.5983726375404446, 1.597571543089079, 1.5970211430331953, 1.5981836610631206, 1.5976882225578446, 1.5974385597435712, 1.5984921465170598, 1.5972151249304585, 1.597777486630457, 1.5977458016234274, 1.5979494966309646, 1.5978135755300913, 1.5976891554830892, 1.5975460538331707, 1.5971378806385108, 1.5973051894083008, 1.5986804750752566, 1.5984405674566384, 1.5972920260797385, 1.5967528328715483, 1.597773956743563, 1.5980973286777491, 1.5973893086898503, 1.5974975978995387, 1.597768602895815, 1.598522147130105, 1.597795614272307, 1.5977473082800804, 1.598443851682353, 1.5979026518823283, 1.5973922423345506, 1.5979722164730328, 1.5981378386956326, 1.5975606212475029, 1.5980359186679858, 1.5983024843416387, 1.5984943128375977, 1.5977885490176322, 1.598480131630044, 1.5973915560492153, 1.5984993707174542, 1.599210812931969, 1.5980307840557129, 1.5979757289385366, 1.5988907254192433, 1.599775692512249, 1.5988615725819504, 1.597483646497742, 1.5986427262498828, 1.5991145538774814, 1.598820349657281, 1.5993537552446764], 'val_acc': [0.18555008139222715, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.2397372740976916, 0.23973727399981865, 0.2380952378750239, 0.23973727399981865, 0.2380952378750239, 0.23645320175022916, 0.23973727399981865, 0.23973727399981865, 0.23973727399981865, 0.23973727399981865, 0.23973727399981865, 0.23316912950063964, 0.23152709327797194, 0.23152709327797194, 0.23645320165235617, 0.23645320165235617, 0.2380952378750239, 0.2298850571531772, 0.23645320165235617, 0.23645320165235617, 0.23481116552756143, 0.23152709327797194, 0.23152709327797194, 0.23152709327797194, 0.23152709327797194, 0.23152709327797194, 0.23645320175022916, 0.23645320175022916, 0.23809523777715091, 0.2298850571531772, 0.2298850571531772, 0.2298850571531772, 0.23645320175022916, 0.23645320175022916, 0.23809523777715091, 0.2282430211262554, 0.23809523777715091, 0.23973727399981865, 0.22331691275187118, 0.23973727399981865, 0.23973727399981865, 0.22167487643133046, 0.22824302093050947, 0.23645320175022916, 0.23481116572330737, 0.22003284011078977, 0.2249589489745389, 0.23645320175022916, 0.22331691284974417, 0.2233169125561252, 0.224958948583047, 0.23645320165235617, 0.22495894887666593, 0.22988505734892314, 0.2282430207347635, 0.23152709298435298, 0.22660098470784173, 0.2315270934737179, 0.2315270934737179, 0.2315270934737179, 0.22824302083263648, 0.22824302093050947, 0.22824302093050947, 0.22988505695743122, 0.2298850571531772, 0.2298850571531772, 0.23481116552756143, 0.23316912940276668, 0.23152709318009895, 0.2298850571531772, 0.23152709318009895, 0.23316912950063964, 0.2315270933758449, 0.2282430211262554, 0.23316912950063964, 0.2298850571531772, 0.23152709327797194, 0.23152709318009895, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23152709318009895, 0.23152709318009895, 0.23316912950063964, 0.23316912950063964, 0.2315270933758449, 0.24137931022248635, 0.23152709318009895, 0.23152709318009895, 0.23316912940276668, 0.24794745462379236, 0.2413793100267404, 0.23152709318009895, 0.24302134615153514, 0.2413793099288674, 0.2298850571531772, 0.24302134624940813, 0.24794745462379236, 0.23152709327797194, 0.2298850570553042, 0.24466338237420288, 0.2413793101246134, 0.23645320184810212, 0.2397372740976916, 0.23316912950063964, 0.2315270933758449, 0.22824302102838243, 0.22660098500146067, 0.2282430211262554, 0.22495894887666593, 0.22495894887666593, 0.22495894887666593, 0.22495894887666593, 0.22495894887666593, 0.22495894887666593, 0.22988505725105016, 0.22824302102838243, 0.22495894868091998, 0.23316912940276668, 0.23316912940276668, 0.2233169125561252, 0.2298850571531772, 0.23809523777715091, 0.23152709327797194, 0.23316912940276668, 0.23973727390194566, 0.23316912950063964, 0.23152709327797194, 0.23973727390194566, 0.2298850570553042, 0.2331691293048937, 0.23481116542968844, 0.22988505695743122, 0.224958948583047, 0.2298850570553042, 0.23152709327797194, 0.22824302083263648, 0.23481116542968844, 0.23481116552756143, 0.23645320175022916, 0.22824302093050947, 0.2282430211262554, 0.23481116552756143, 0.22824302093050947, 0.2216748763334575, 0.22495894868091998, 0.23152709318009895, 0.23316912950063964, 0.2282430211262554, 0.23316912940276668, 0.2380952378750239, 0.2216748763334575, 0.2397372741955646, 0.23481116582118036, 0.23152709318009895, 0.23809523807076985, 0.24302134436535328, 0.22331691116143526, 0.2397372740976916, 0.24137931022248635, 0.24137931022248635, 0.23152709318009895, 0.23645320165235617, 0.2298850570553042, 0.23152709318009895, 0.23645320165235617, 0.23809523777715091, 0.2331691293048937, 0.23645320165235617, 0.23809523777715091, 0.23645320165235617, 0.23152709318009895, 0.23645320165235617, 0.23152709318009895, 0.23645320165235617, 0.23809523777715091, 0.23645320165235617, 0.23152709357159088, 0.23645320165235617, 0.23645320165235617, 0.2331691293048937, 0.23809523777715091, 0.23645320165235617, 0.23645320165235617, 0.23809523777715091, 0.2413793101246134, 0.23973727390194566, 0.23645319967042833, 0.24302134624940813, 0.23809523777715091, 0.23645319967042833, 0.23645319967042833, 0.23809523777715091, 0.23973727380407267, 0.2397372719200178, 0.24302134624940813, 0.24302134624940813, 0.23645319967042833, 0.2331691276165848, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.23645319967042833, 0.24302134624940813, 0.24302134624940813, 0.23645319967042833, 0.24302134624940813, 0.23809523589309606, 0.24302134624940813, 0.24302134624940813, 0.23645319967042833, 0.23645319967042833, 0.24302134624940813, 0.24302134624940813, 0.23645319967042833, 0.23809523589309606, 0.24466338247207586, 0.2413793101246134, 0.24794745462379236, 0.23645319967042833, 0.23645319967042833, 0.2463054164191968, 0.23645319967042833, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.24302134624940813, 0.23809523589309606, 0.24466338247207586, 0.24466338247207586, 0.24630541869474357, 0.24794745462379236, 0.24466338247207586, 0.2413793083384315, 0.24466338247207586, 0.24958949094433308, 0.24630541869474357, 0.24302134624940813, 0.24794745462379236, 0.24466338247207586, 0.24794745254399153, 0.24958948866878627, 0.24466338247207586, 0.23809523589309606, 0.24466338247207586, 0.24958948866878627, 0.24466338247207586, 0.24466338247207586, 0.24466338247207586, 0.24466338247207586, 0.24466338247207586, 0.24466338247207586, 0.24794745254399153, 0.24137931032035934, 0.24794745472166535, 0.24958948866878627, 0.251231524793581, 0.24794745472166535, 0.24794745472166535, 0.25287356091837576, 0.24794745472166535, 0.24794745472166535, 0.251231524793581, 0.24466338247207586, 0.24794745472166535, 0.24958949094433308, 0.24794745472166535, 0.24794745472166535, 0.251231524793581, 0.25287356091837576, 0.24794745472166535, 0.24794745472166535, 0.24794745472166535, 0.251231524793581, 0.25287356091837576, 0.24794745472166535, 0.24794745472166535, 0.24466338247207586, 0.24466338247207586], 'loss': [1.6186010101737427, 1.6104906528147829, 1.607465569684148, 1.6062322516216145, 1.6058785975591359, 1.6053071560066583, 1.6053006885967216, 1.605083896541008, 1.6049174800790555, 1.6052500325306729, 1.6054899072255442, 1.60494450592652, 1.6050424870034752, 1.6047273491931893, 1.60465128485427, 1.60450978132244, 1.6043310583494528, 1.6041989721556709, 1.6039402723312377, 1.6038510553156327, 1.6036184192438145, 1.6036369952333047, 1.6033298547263017, 1.6033818515174443, 1.6031326892194806, 1.6029421277604308, 1.6028238643121426, 1.6027466571306546, 1.6025711700412038, 1.6025016978046487, 1.6025094878257422, 1.602482092502915, 1.6021576105446786, 1.6021682585044563, 1.6023224468838262, 1.6021387814496333, 1.6020659022752264, 1.6017679806852243, 1.6025021982878385, 1.6016523612842912, 1.6015747657791546, 1.6014292765936569, 1.6012059194106585, 1.601208800799548, 1.6011408597292107, 1.6010151063637077, 1.6012511486879855, 1.6010934723965686, 1.6011046939806772, 1.6009266726534959, 1.6006717688738687, 1.6008158688182947, 1.6005539069675077, 1.60037769199152, 1.600162986367635, 1.6000903447795454, 1.600030007058835, 1.5998902597466533, 1.5996042537493382, 1.5996663471022181, 1.599657603064112, 1.5999080042085119, 1.5989163076118766, 1.599518594115177, 1.5988872284272369, 1.6024320080050207, 1.5996781251268954, 1.6009463415008796, 1.6015076869811855, 1.6005447613874746, 1.5992455464368973, 1.599577621170138, 1.6007881457311173, 1.5985418167447163, 1.6009602430664782, 1.5994437163860156, 1.5990515111652976, 1.5984049104322398, 1.5986489455557946, 1.5977823554612773, 1.5980319693348002, 1.5979707290014937, 1.5977001640586148, 1.597611236670179, 1.5977613312018235, 1.5973394002268202, 1.597430514408088, 1.5973185085907609, 1.5972234805751386, 1.5970240129337663, 1.5969241632817952, 1.597084930153598, 1.5974899639583955, 1.597410699767988, 1.5978808918038432, 1.5982745241090748, 1.597323175573251, 1.597413813015274, 1.5969792897206803, 1.5970154976208353, 1.596933745113976, 1.5969493449835808, 1.5968048714514387, 1.5970489616021972, 1.5967569310562322, 1.5969984511819952, 1.597356761946081, 1.5968141947928396, 1.5974288203633051, 1.5964660191193254, 1.5974279467819652, 1.597023386583191, 1.5969570672977143, 1.5971418022864654, 1.596772284527334, 1.5971964595743764, 1.5963174696086124, 1.5964957996070752, 1.596784983963937, 1.5964952556993928, 1.5972419034284242, 1.5968693638239553, 1.5974696672427826, 1.5977710029672547, 1.5968242999219797, 1.5968254676834515, 1.596996711656543, 1.597394644161514, 1.5970968258699108, 1.5973966151537102, 1.5972297075103197, 1.5975832523506526, 1.5971248222327574, 1.5969175370077333, 1.5970315699215052, 1.5970531097917342, 1.5967562666663888, 1.596787848462804, 1.5967050569992536, 1.5961900468234897, 1.596273376124106, 1.5961470398325206, 1.596071615307238, 1.595670105005926, 1.5957103169674256, 1.5960576003581837, 1.5963290394698815, 1.5964110351930654, 1.5960449920787458, 1.5964539023150655, 1.5959341722836975, 1.5964415787181816, 1.596871159013047, 1.5966682346939307, 1.5972324685639179, 1.596528180668731, 1.5970350883335058, 1.5976350387263836, 1.5980019811732078, 1.5977739613648558, 1.5976608628365048, 1.5974969572599909, 1.5972321381069552, 1.5976640432032716, 1.5971622869464162, 1.5971706163222295, 1.597164331716189, 1.5973053318763906, 1.5970403541529692, 1.597237147198076, 1.5972319458054811, 1.5967785713853777, 1.5970178485651036, 1.5968110248048693, 1.596563806768805, 1.5967454637100564, 1.596857052857871, 1.596440024542368, 1.5977751600668906, 1.598230128660339, 1.596999083652144, 1.5979289487646835, 1.597256574004093, 1.5963263096505855, 1.5970132433168698, 1.596940952796466, 1.597029679120199, 1.5961791956449192, 1.5964897458802993, 1.595666170511892, 1.5955093397496907, 1.5956942684107003, 1.5959602066623602, 1.5958685148423213, 1.5952762785388703, 1.5966913623241918, 1.595851412837755, 1.5971472088071599, 1.5953996949127323, 1.5950897378843178, 1.5954047385672034, 1.596010724670833, 1.5953800525998187, 1.5951624114900154, 1.5955179412751717, 1.596245971303701, 1.5955873421330227, 1.5955442171566785, 1.5953092854615354, 1.5948793918934692, 1.5949433109353945, 1.5944522702473634, 1.5950823458313208, 1.5958973382777502, 1.5963789450804067, 1.5951911048477925, 1.5953614107147625, 1.5949114133200362, 1.5960115033253508, 1.595429080614564, 1.594814317525045, 1.5947847085322198, 1.5945642784146068, 1.5945930184280113, 1.5947352866617317, 1.5942105131227622, 1.5944429480809206, 1.5942727043888163, 1.5951436154406664, 1.5944208837387743, 1.5948038639718747, 1.594243803699893, 1.5941774556768993, 1.5941618763201046, 1.5943509465125552, 1.5946983361880638, 1.5938006031929346, 1.5940911150566117, 1.5942474817103673, 1.5941559352424355, 1.5938313628613827, 1.5946073529656664, 1.594045279892563, 1.5937071310666062, 1.5934868358243908, 1.5937255126984458, 1.5937472413942309, 1.5932942512343797, 1.5937213753283146, 1.5933647448032544, 1.594049557914969, 1.5936176201646088, 1.593441482637942, 1.5941131092929253, 1.59311548669725, 1.593412819143683, 1.593657586755694, 1.5928557860043504, 1.5935601184500316, 1.5927237911146035, 1.5935184946295173, 1.5923554226603107, 1.5934663867558787, 1.5927844885683158, 1.5928426885996512, 1.5929911323151793, 1.592490813129981, 1.592706262551294, 1.592399397963616, 1.59304903376029, 1.5923451909294364, 1.5924146526403251, 1.5924131463440536, 1.5932136959608576, 1.592307268520645, 1.5929033090936084, 1.5918018312180067, 1.5926198415442903, 1.5924344170020102, 1.5917050546199634, 1.5921120452195467, 1.5921244651140374, 1.5919250114742491, 1.5916030331069195, 1.5917475930474376, 1.5920191250787379, 1.591851667554961, 1.5912912071118364, 1.591908165316807, 1.5912692093996053, 1.5913825135945785, 1.5913218274008811, 1.591628901277969, 1.591111101702743, 1.5912153733584427, 1.5913051013339472, 1.5914756883096401, 1.590908586709651, 1.5909759685488942, 1.591126985961162], 'acc': [0.18726899275055167, 0.21232032900106246, 0.23285420970872686, 0.23285420833794243, 0.23285421010037957, 0.23285421010037957, 0.23285420929871545, 0.23285420851541005, 0.23285420951290053, 0.23285420890706277, 0.23285421010037957, 0.23285421010037957, 0.23285421051039099, 0.23285420969036816, 0.2328542091028891, 0.2328542083195837, 0.23285420890706277, 0.23285421029620593, 0.2328542083195837, 0.2328542083195837, 0.2328542094945418, 0.2328542097270856, 0.23696098567523996, 0.23983573076416578, 0.2386036957558665, 0.23901437454889443, 0.23983572804095563, 0.24106776167847047, 0.24353182797436843, 0.24312114896715545, 0.24353182719106303, 0.24353182777854207, 0.2435318283660211, 0.24435318299632297, 0.24353182797436843, 0.24312115094377765, 0.24271047134908563, 0.2431211499646459, 0.2439425059657322, 0.24353182854348873, 0.24271047311152277, 0.2418891168962514, 0.24229979551181166, 0.2435318283476624, 0.24353182699523668, 0.24353182756435698, 0.2435318283476624, 0.2435318279560097, 0.24147844084479236, 0.24065708523535875, 0.24229979627675835, 0.24353182893514144, 0.2431211487713291, 0.24229979551181166, 0.2406570834545629, 0.24353182658522526, 0.24353182679941032, 0.2435318267810516, 0.24147843966983426, 0.2422997949059739, 0.24599589346860223, 0.24147843888652887, 0.2464065712824984, 0.24681725027135265, 0.24517453725333085, 0.2344969207868439, 0.24147843966983426, 0.24353182517772337, 0.24106776089516507, 0.23983572960756644, 0.24845995800206302, 0.2451745394074207, 0.24188911787538314, 0.2464065712824984, 0.24065708462952098, 0.2427104725240437, 0.24476386081021914, 0.24928131501899853, 0.2472279271244758, 0.2476386031575761, 0.24722792653699674, 0.2464065712824984, 0.24845995958703254, 0.24722792632281168, 0.24845995839371574, 0.24558521626054383, 0.2468172496838736, 0.24681724815398026, 0.2451745374491572, 0.24599589346860223, 0.24804928253808306, 0.24558521586889115, 0.2443531826046703, 0.2439425046133065, 0.24476386082857787, 0.24188911750208916, 0.242710471544912, 0.24681724833144789, 0.24312115014211352, 0.24476386082857787, 0.2422997941226685, 0.24229979431849485, 0.24229979492433262, 0.24476386002691375, 0.24435318456293376, 0.24312115033793988, 0.24147843888652887, 0.24271047254240244, 0.2468172475297838, 0.24681724670976093, 0.24722792534367993, 0.24681724811726283, 0.2505133480506756, 0.24763860214172692, 0.2484599578062367, 0.24804928136312496, 0.24517453960324703, 0.2451745382324626, 0.24969199085627247, 0.2468172485089155, 0.24271047291569642, 0.24435318342469312, 0.24681725066300536, 0.24476385984944612, 0.2386036957558665, 0.23983572861007596, 0.2390143747630795, 0.24353182756435698, 0.24476385963526104, 0.24188911709207775, 0.24312114975046084, 0.24188911846286218, 0.24147843829904983, 0.24188911728790408, 0.24106776087680637, 0.24188911750208916, 0.24229979471014756, 0.24106776068098001, 0.2464065688958648, 0.24476386159352453, 0.24763860413670785, 0.24394250598409092, 0.24106776205176444, 0.24229979588510564, 0.2468172485089155, 0.2464065688958648, 0.2480492831255621, 0.24804928214643035, 0.24476385842358553, 0.24517453686167817, 0.2501026704509645, 0.2488706367583735, 0.2480492813814837, 0.246817249506406, 0.24681724733395743, 0.24271047115325928, 0.24353182756435698, 0.2418891161313047, 0.24229979647258468, 0.24476385924360836, 0.2447638614160569, 0.2422997952976266, 0.246406570499193, 0.24558521565470606, 0.24435318338797568, 0.24271047134908563, 0.24517453960324703, 0.24312114937716686, 0.2447638606143928, 0.24599589248947049, 0.2431211503562986, 0.24558521528141208, 0.24394250400746872, 0.23655030864464918, 0.24147843966983426, 0.2455852156730648, 0.24271047033323645, 0.24106776185593812, 0.23778234149885863, 0.23490759758489088, 0.24640656991171397, 0.24928131241817983, 0.24681724931057963, 0.24476386218100357, 0.2492813142173344, 0.24394250498660047, 0.24558521506722703, 0.24394250420329508, 0.24476386257265628, 0.24845996093945827, 0.2488706365992646, 0.24681724891892692, 0.24763860395924023, 0.24763860296174975, 0.24599589231200286, 0.24763860335340246, 0.24845995976450017, 0.24353182817019475, 0.24394250537825315, 0.23942505179367027, 0.25010267141173753, 0.24887063738256998, 0.24517453883830037, 0.2488706377742227, 0.24969199301036232, 0.2418891176795568, 0.2492813142173344, 0.24845996035197923, 0.2435318263893989, 0.2488706365992646, 0.2505133474815553, 0.24804928195060402, 0.24640657030336666, 0.24517453864247402, 0.248870637009276, 0.24722792495202725, 0.24681724733395743, 0.25215605854131357, 0.25092402527709273, 0.23613963122240572, 0.24887063642179696, 0.2496919920495893, 0.25215605814966086, 0.25256673594519835, 0.24887063480011, 0.25215605854131357, 0.24887063642179696, 0.24640657169250982, 0.25010267043260576, 0.2501026700593118, 0.2521560569747028, 0.2554414782925553, 0.2476386031575761, 0.24969199124792518, 0.2521560579354758, 0.24558521508558576, 0.24763860296174975, 0.24845996035197923, 0.24558521604635877, 0.25215605773964944, 0.2505133470715439, 0.24353182856184746, 0.24887063679509094, 0.24804928177313637, 0.24722792495202725, 0.2537987691910605, 0.25215605715217043, 0.2521560573663555, 0.2521560579354758, 0.2488706365992646, 0.2533880921604697, 0.24845995937284748, 0.24887063720510236, 0.2517453789466216, 0.24928131519646615, 0.2468172496838736, 0.25256673398693485, 0.2525667353577193, 0.247227925931159, 0.2542094470049566, 0.2517453809232438, 0.2525667353577193, 0.24928131423569314, 0.24928131382568172, 0.25338809157299064, 0.2488706377742227, 0.2529774121924837, 0.2464065701075403, 0.2533880882072253, 0.24681724911475328, 0.24476386257265628, 0.25831622103156493, 0.2533880921604697, 0.2484599585895421, 0.2496919916579366, 0.25913757724683634, 0.2537987689952341, 0.24804928116729863, 0.24928131460898711, 0.2550308028285753, 0.24763860257009707, 0.25215605773964944, 0.25503080128032324, 0.2542094448508668, 0.2517453807090587, 0.25256673437858757, 0.25215605717052914, 0.2517453807274174, 0.25462012086560837, 0.2537987695827132, 0.2501026712159112, 0.25010267043260576, 0.25831622142321764, 0.2550308018678023, 0.2509240250812664, 0.2505133464840648, 0.25338809078968527, 0.2550308004786591, 0.251745380904885, 0.24599589248947049]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
