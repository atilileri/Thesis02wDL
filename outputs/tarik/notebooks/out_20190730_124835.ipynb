{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf22.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 12:48:35 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '1', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['sg', 'sk', 'eo', 'yd', 'eg', 'ck', 'ds', 'ib', 'my', 'ce', 'aa', 'eb', 'ek', 'by', 'mb'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001EE4E18D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001EE49946EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7106, Accuracy:0.0378, Validation Loss:2.7070, Validation Accuracy:0.0312\n",
    "Epoch #2: Loss:2.7007, Accuracy:0.0595, Validation Loss:2.6956, Validation Accuracy:0.0821\n",
    "Epoch #3: Loss:2.6912, Accuracy:0.0928, Validation Loss:2.6885, Validation Accuracy:0.0854\n",
    "Epoch #4: Loss:2.6835, Accuracy:0.1084, Validation Loss:2.6824, Validation Accuracy:0.1002\n",
    "Epoch #5: Loss:2.6779, Accuracy:0.1158, Validation Loss:2.6766, Validation Accuracy:0.1067\n",
    "Epoch #6: Loss:2.6711, Accuracy:0.1269, Validation Loss:2.6718, Validation Accuracy:0.1166\n",
    "Epoch #7: Loss:2.6664, Accuracy:0.1298, Validation Loss:2.6665, Validation Accuracy:0.1084\n",
    "Epoch #8: Loss:2.6593, Accuracy:0.1388, Validation Loss:2.6591, Validation Accuracy:0.1363\n",
    "Epoch #9: Loss:2.6510, Accuracy:0.1421, Validation Loss:2.6508, Validation Accuracy:0.1330\n",
    "Epoch #10: Loss:2.6415, Accuracy:0.1396, Validation Loss:2.6387, Validation Accuracy:0.1297\n",
    "Epoch #11: Loss:2.6297, Accuracy:0.1339, Validation Loss:2.6265, Validation Accuracy:0.1297\n",
    "Epoch #12: Loss:2.6151, Accuracy:0.1351, Validation Loss:2.6117, Validation Accuracy:0.1297\n",
    "Epoch #13: Loss:2.5991, Accuracy:0.1368, Validation Loss:2.5948, Validation Accuracy:0.1429\n",
    "Epoch #14: Loss:2.5826, Accuracy:0.1421, Validation Loss:2.5958, Validation Accuracy:0.1478\n",
    "Epoch #15: Loss:2.5816, Accuracy:0.1368, Validation Loss:2.5725, Validation Accuracy:0.1314\n",
    "Epoch #16: Loss:2.5624, Accuracy:0.1606, Validation Loss:2.5704, Validation Accuracy:0.1642\n",
    "Epoch #17: Loss:2.5591, Accuracy:0.1544, Validation Loss:2.5478, Validation Accuracy:0.1642\n",
    "Epoch #18: Loss:2.5428, Accuracy:0.1405, Validation Loss:2.5474, Validation Accuracy:0.1281\n",
    "Epoch #19: Loss:2.5312, Accuracy:0.1458, Validation Loss:2.5349, Validation Accuracy:0.1560\n",
    "Epoch #20: Loss:2.5220, Accuracy:0.1487, Validation Loss:2.5207, Validation Accuracy:0.1412\n",
    "Epoch #21: Loss:2.5117, Accuracy:0.1470, Validation Loss:2.5118, Validation Accuracy:0.1527\n",
    "Epoch #22: Loss:2.5051, Accuracy:0.1544, Validation Loss:2.5071, Validation Accuracy:0.1708\n",
    "Epoch #23: Loss:2.5017, Accuracy:0.1585, Validation Loss:2.4960, Validation Accuracy:0.1576\n",
    "Epoch #24: Loss:2.4990, Accuracy:0.1561, Validation Loss:2.4922, Validation Accuracy:0.1790\n",
    "Epoch #25: Loss:2.4935, Accuracy:0.1630, Validation Loss:2.4868, Validation Accuracy:0.1757\n",
    "Epoch #26: Loss:2.4900, Accuracy:0.1606, Validation Loss:2.4817, Validation Accuracy:0.1790\n",
    "Epoch #27: Loss:2.4843, Accuracy:0.1725, Validation Loss:2.4790, Validation Accuracy:0.1724\n",
    "Epoch #28: Loss:2.4819, Accuracy:0.1696, Validation Loss:2.4763, Validation Accuracy:0.1724\n",
    "Epoch #29: Loss:2.4780, Accuracy:0.1692, Validation Loss:2.4715, Validation Accuracy:0.1741\n",
    "Epoch #30: Loss:2.4752, Accuracy:0.1692, Validation Loss:2.4724, Validation Accuracy:0.1741\n",
    "Epoch #31: Loss:2.4706, Accuracy:0.1721, Validation Loss:2.4738, Validation Accuracy:0.1823\n",
    "Epoch #32: Loss:2.4706, Accuracy:0.1713, Validation Loss:2.4742, Validation Accuracy:0.1691\n",
    "Epoch #33: Loss:2.4723, Accuracy:0.1721, Validation Loss:2.4720, Validation Accuracy:0.1691\n",
    "Epoch #34: Loss:2.4708, Accuracy:0.1667, Validation Loss:2.4687, Validation Accuracy:0.1691\n",
    "Epoch #35: Loss:2.4680, Accuracy:0.1692, Validation Loss:2.4607, Validation Accuracy:0.1724\n",
    "Epoch #36: Loss:2.4921, Accuracy:0.1655, Validation Loss:2.4937, Validation Accuracy:0.1626\n",
    "Epoch #37: Loss:2.5011, Accuracy:0.1593, Validation Loss:2.5192, Validation Accuracy:0.1642\n",
    "Epoch #38: Loss:2.4837, Accuracy:0.1651, Validation Loss:2.4706, Validation Accuracy:0.1839\n",
    "Epoch #39: Loss:2.4857, Accuracy:0.1659, Validation Loss:2.4731, Validation Accuracy:0.1773\n",
    "Epoch #40: Loss:2.4679, Accuracy:0.1688, Validation Loss:2.4681, Validation Accuracy:0.1675\n",
    "Epoch #41: Loss:2.4712, Accuracy:0.1692, Validation Loss:2.4620, Validation Accuracy:0.1773\n",
    "Epoch #42: Loss:2.4643, Accuracy:0.1655, Validation Loss:2.4616, Validation Accuracy:0.1675\n",
    "Epoch #43: Loss:2.4614, Accuracy:0.1667, Validation Loss:2.4542, Validation Accuracy:0.1757\n",
    "Epoch #44: Loss:2.4597, Accuracy:0.1688, Validation Loss:2.4518, Validation Accuracy:0.1806\n",
    "Epoch #45: Loss:2.4569, Accuracy:0.1696, Validation Loss:2.4635, Validation Accuracy:0.1741\n",
    "Epoch #46: Loss:2.4666, Accuracy:0.1634, Validation Loss:2.4537, Validation Accuracy:0.1757\n",
    "Epoch #47: Loss:2.4648, Accuracy:0.1717, Validation Loss:2.4519, Validation Accuracy:0.1741\n",
    "Epoch #48: Loss:2.4596, Accuracy:0.1680, Validation Loss:2.4514, Validation Accuracy:0.1741\n",
    "Epoch #49: Loss:2.4566, Accuracy:0.1704, Validation Loss:2.4456, Validation Accuracy:0.1741\n",
    "Epoch #50: Loss:2.4558, Accuracy:0.1725, Validation Loss:2.4444, Validation Accuracy:0.1724\n",
    "Epoch #51: Loss:2.4535, Accuracy:0.1717, Validation Loss:2.4440, Validation Accuracy:0.1741\n",
    "Epoch #52: Loss:2.4533, Accuracy:0.1696, Validation Loss:2.4426, Validation Accuracy:0.1741\n",
    "Epoch #53: Loss:2.4530, Accuracy:0.1717, Validation Loss:2.4460, Validation Accuracy:0.1724\n",
    "Epoch #54: Loss:2.4520, Accuracy:0.1688, Validation Loss:2.4438, Validation Accuracy:0.1757\n",
    "Epoch #55: Loss:2.4515, Accuracy:0.1696, Validation Loss:2.4589, Validation Accuracy:0.1773\n",
    "Epoch #56: Loss:2.4570, Accuracy:0.1655, Validation Loss:2.4527, Validation Accuracy:0.1773\n",
    "Epoch #57: Loss:2.4522, Accuracy:0.1721, Validation Loss:2.4584, Validation Accuracy:0.1757\n",
    "Epoch #58: Loss:2.4562, Accuracy:0.1639, Validation Loss:2.4467, Validation Accuracy:0.1691\n",
    "Epoch #59: Loss:2.4517, Accuracy:0.1688, Validation Loss:2.4434, Validation Accuracy:0.1708\n",
    "Epoch #60: Loss:2.4517, Accuracy:0.1717, Validation Loss:2.4421, Validation Accuracy:0.1724\n",
    "Epoch #61: Loss:2.4478, Accuracy:0.1713, Validation Loss:2.4437, Validation Accuracy:0.1724\n",
    "Epoch #62: Loss:2.4513, Accuracy:0.1725, Validation Loss:2.4494, Validation Accuracy:0.1675\n",
    "Epoch #63: Loss:2.4498, Accuracy:0.1676, Validation Loss:2.4478, Validation Accuracy:0.1724\n",
    "Epoch #64: Loss:2.4500, Accuracy:0.1680, Validation Loss:2.4433, Validation Accuracy:0.1724\n",
    "Epoch #65: Loss:2.4478, Accuracy:0.1721, Validation Loss:2.4445, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4496, Accuracy:0.1680, Validation Loss:2.4423, Validation Accuracy:0.1741\n",
    "Epoch #67: Loss:2.4491, Accuracy:0.1667, Validation Loss:2.4431, Validation Accuracy:0.1675\n",
    "Epoch #68: Loss:2.4476, Accuracy:0.1684, Validation Loss:2.4427, Validation Accuracy:0.1741\n",
    "Epoch #69: Loss:2.4472, Accuracy:0.1671, Validation Loss:2.4419, Validation Accuracy:0.1741\n",
    "Epoch #70: Loss:2.4462, Accuracy:0.1676, Validation Loss:2.4409, Validation Accuracy:0.1708\n",
    "Epoch #71: Loss:2.4461, Accuracy:0.1729, Validation Loss:2.4387, Validation Accuracy:0.1790\n",
    "Epoch #72: Loss:2.4461, Accuracy:0.1733, Validation Loss:2.4388, Validation Accuracy:0.1773\n",
    "Epoch #73: Loss:2.4452, Accuracy:0.1717, Validation Loss:2.4399, Validation Accuracy:0.1741\n",
    "Epoch #74: Loss:2.4457, Accuracy:0.1704, Validation Loss:2.4376, Validation Accuracy:0.1773\n",
    "Epoch #75: Loss:2.4459, Accuracy:0.1696, Validation Loss:2.4405, Validation Accuracy:0.1724\n",
    "Epoch #76: Loss:2.4457, Accuracy:0.1684, Validation Loss:2.4417, Validation Accuracy:0.1708\n",
    "Epoch #77: Loss:2.4453, Accuracy:0.1684, Validation Loss:2.4422, Validation Accuracy:0.1757\n",
    "Epoch #78: Loss:2.4445, Accuracy:0.1692, Validation Loss:2.4403, Validation Accuracy:0.1757\n",
    "Epoch #79: Loss:2.4453, Accuracy:0.1713, Validation Loss:2.4414, Validation Accuracy:0.1741\n",
    "Epoch #80: Loss:2.4438, Accuracy:0.1737, Validation Loss:2.4408, Validation Accuracy:0.1806\n",
    "Epoch #81: Loss:2.4436, Accuracy:0.1725, Validation Loss:2.4395, Validation Accuracy:0.1773\n",
    "Epoch #82: Loss:2.4424, Accuracy:0.1721, Validation Loss:2.4399, Validation Accuracy:0.1741\n",
    "Epoch #83: Loss:2.4427, Accuracy:0.1733, Validation Loss:2.4407, Validation Accuracy:0.1790\n",
    "Epoch #84: Loss:2.4428, Accuracy:0.1729, Validation Loss:2.4397, Validation Accuracy:0.1741\n",
    "Epoch #85: Loss:2.4432, Accuracy:0.1745, Validation Loss:2.4388, Validation Accuracy:0.1773\n",
    "Epoch #86: Loss:2.4421, Accuracy:0.1721, Validation Loss:2.4392, Validation Accuracy:0.1790\n",
    "Epoch #87: Loss:2.4421, Accuracy:0.1766, Validation Loss:2.4372, Validation Accuracy:0.1773\n",
    "Epoch #88: Loss:2.4422, Accuracy:0.1717, Validation Loss:2.4393, Validation Accuracy:0.1823\n",
    "Epoch #89: Loss:2.4419, Accuracy:0.1749, Validation Loss:2.4412, Validation Accuracy:0.1757\n",
    "Epoch #90: Loss:2.4426, Accuracy:0.1745, Validation Loss:2.4412, Validation Accuracy:0.1839\n",
    "Epoch #91: Loss:2.4421, Accuracy:0.1758, Validation Loss:2.4409, Validation Accuracy:0.1839\n",
    "Epoch #92: Loss:2.4422, Accuracy:0.1758, Validation Loss:2.4409, Validation Accuracy:0.1790\n",
    "Epoch #93: Loss:2.4426, Accuracy:0.1733, Validation Loss:2.4408, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4421, Accuracy:0.1737, Validation Loss:2.4409, Validation Accuracy:0.1773\n",
    "Epoch #95: Loss:2.4418, Accuracy:0.1741, Validation Loss:2.4407, Validation Accuracy:0.1790\n",
    "Epoch #96: Loss:2.4414, Accuracy:0.1770, Validation Loss:2.4391, Validation Accuracy:0.1790\n",
    "Epoch #97: Loss:2.4409, Accuracy:0.1766, Validation Loss:2.4405, Validation Accuracy:0.1823\n",
    "Epoch #98: Loss:2.4411, Accuracy:0.1754, Validation Loss:2.4416, Validation Accuracy:0.1806\n",
    "Epoch #99: Loss:2.4415, Accuracy:0.1754, Validation Loss:2.4428, Validation Accuracy:0.1790\n",
    "Epoch #100: Loss:2.4411, Accuracy:0.1741, Validation Loss:2.4436, Validation Accuracy:0.1790\n",
    "Epoch #101: Loss:2.4412, Accuracy:0.1754, Validation Loss:2.4416, Validation Accuracy:0.1806\n",
    "Epoch #102: Loss:2.4408, Accuracy:0.1721, Validation Loss:2.4417, Validation Accuracy:0.1839\n",
    "Epoch #103: Loss:2.4398, Accuracy:0.1754, Validation Loss:2.4410, Validation Accuracy:0.1823\n",
    "Epoch #104: Loss:2.4403, Accuracy:0.1762, Validation Loss:2.4405, Validation Accuracy:0.1790\n",
    "Epoch #105: Loss:2.4406, Accuracy:0.1782, Validation Loss:2.4409, Validation Accuracy:0.1839\n",
    "Epoch #106: Loss:2.4405, Accuracy:0.1758, Validation Loss:2.4399, Validation Accuracy:0.1839\n",
    "Epoch #107: Loss:2.4399, Accuracy:0.1762, Validation Loss:2.4401, Validation Accuracy:0.1872\n",
    "Epoch #108: Loss:2.4414, Accuracy:0.1758, Validation Loss:2.4391, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.4407, Accuracy:0.1741, Validation Loss:2.4408, Validation Accuracy:0.1757\n",
    "Epoch #110: Loss:2.4409, Accuracy:0.1721, Validation Loss:2.4396, Validation Accuracy:0.1806\n",
    "Epoch #111: Loss:2.4409, Accuracy:0.1745, Validation Loss:2.4402, Validation Accuracy:0.1823\n",
    "Epoch #112: Loss:2.4396, Accuracy:0.1754, Validation Loss:2.4409, Validation Accuracy:0.1741\n",
    "Epoch #113: Loss:2.4393, Accuracy:0.1786, Validation Loss:2.4398, Validation Accuracy:0.1790\n",
    "Epoch #114: Loss:2.4396, Accuracy:0.1754, Validation Loss:2.4399, Validation Accuracy:0.1741\n",
    "Epoch #115: Loss:2.4393, Accuracy:0.1754, Validation Loss:2.4393, Validation Accuracy:0.1757\n",
    "Epoch #116: Loss:2.4388, Accuracy:0.1774, Validation Loss:2.4400, Validation Accuracy:0.1839\n",
    "Epoch #117: Loss:2.4385, Accuracy:0.1754, Validation Loss:2.4406, Validation Accuracy:0.1773\n",
    "Epoch #118: Loss:2.4392, Accuracy:0.1786, Validation Loss:2.4424, Validation Accuracy:0.1757\n",
    "Epoch #119: Loss:2.4392, Accuracy:0.1766, Validation Loss:2.4417, Validation Accuracy:0.1773\n",
    "Epoch #120: Loss:2.4391, Accuracy:0.1778, Validation Loss:2.4409, Validation Accuracy:0.1806\n",
    "Epoch #121: Loss:2.4397, Accuracy:0.1782, Validation Loss:2.4403, Validation Accuracy:0.1839\n",
    "Epoch #122: Loss:2.4387, Accuracy:0.1745, Validation Loss:2.4407, Validation Accuracy:0.1790\n",
    "Epoch #123: Loss:2.4384, Accuracy:0.1766, Validation Loss:2.4407, Validation Accuracy:0.1839\n",
    "Epoch #124: Loss:2.4389, Accuracy:0.1778, Validation Loss:2.4410, Validation Accuracy:0.1806\n",
    "Epoch #125: Loss:2.4389, Accuracy:0.1778, Validation Loss:2.4405, Validation Accuracy:0.1823\n",
    "Epoch #126: Loss:2.4383, Accuracy:0.1782, Validation Loss:2.4398, Validation Accuracy:0.1823\n",
    "Epoch #127: Loss:2.4382, Accuracy:0.1778, Validation Loss:2.4399, Validation Accuracy:0.1823\n",
    "Epoch #128: Loss:2.4382, Accuracy:0.1803, Validation Loss:2.4410, Validation Accuracy:0.1773\n",
    "Epoch #129: Loss:2.4377, Accuracy:0.1745, Validation Loss:2.4403, Validation Accuracy:0.1839\n",
    "Epoch #130: Loss:2.4374, Accuracy:0.1774, Validation Loss:2.4402, Validation Accuracy:0.1839\n",
    "Epoch #131: Loss:2.4375, Accuracy:0.1766, Validation Loss:2.4406, Validation Accuracy:0.1757\n",
    "Epoch #132: Loss:2.4367, Accuracy:0.1770, Validation Loss:2.4408, Validation Accuracy:0.1839\n",
    "Epoch #133: Loss:2.4370, Accuracy:0.1729, Validation Loss:2.4405, Validation Accuracy:0.1839\n",
    "Epoch #134: Loss:2.4367, Accuracy:0.1762, Validation Loss:2.4409, Validation Accuracy:0.1741\n",
    "Epoch #135: Loss:2.4379, Accuracy:0.1766, Validation Loss:2.4404, Validation Accuracy:0.1806\n",
    "Epoch #136: Loss:2.4363, Accuracy:0.1778, Validation Loss:2.4420, Validation Accuracy:0.1741\n",
    "Epoch #137: Loss:2.4362, Accuracy:0.1762, Validation Loss:2.4428, Validation Accuracy:0.1773\n",
    "Epoch #138: Loss:2.4382, Accuracy:0.1729, Validation Loss:2.4422, Validation Accuracy:0.1773\n",
    "Epoch #139: Loss:2.4375, Accuracy:0.1749, Validation Loss:2.4427, Validation Accuracy:0.1741\n",
    "Epoch #140: Loss:2.4382, Accuracy:0.1745, Validation Loss:2.4422, Validation Accuracy:0.1806\n",
    "Epoch #141: Loss:2.4370, Accuracy:0.1729, Validation Loss:2.4427, Validation Accuracy:0.1773\n",
    "Epoch #142: Loss:2.4366, Accuracy:0.1766, Validation Loss:2.4430, Validation Accuracy:0.1708\n",
    "Epoch #143: Loss:2.4364, Accuracy:0.1766, Validation Loss:2.4423, Validation Accuracy:0.1790\n",
    "Epoch #144: Loss:2.4367, Accuracy:0.1754, Validation Loss:2.4423, Validation Accuracy:0.1790\n",
    "Epoch #145: Loss:2.4363, Accuracy:0.1754, Validation Loss:2.4438, Validation Accuracy:0.1708\n",
    "Epoch #146: Loss:2.4367, Accuracy:0.1774, Validation Loss:2.4428, Validation Accuracy:0.1790\n",
    "Epoch #147: Loss:2.4370, Accuracy:0.1778, Validation Loss:2.4422, Validation Accuracy:0.1790\n",
    "Epoch #148: Loss:2.4358, Accuracy:0.1758, Validation Loss:2.4426, Validation Accuracy:0.1741\n",
    "Epoch #149: Loss:2.4369, Accuracy:0.1766, Validation Loss:2.4421, Validation Accuracy:0.1773\n",
    "Epoch #150: Loss:2.4354, Accuracy:0.1762, Validation Loss:2.4420, Validation Accuracy:0.1724\n",
    "Epoch #151: Loss:2.4354, Accuracy:0.1791, Validation Loss:2.4417, Validation Accuracy:0.1757\n",
    "Epoch #152: Loss:2.4356, Accuracy:0.1762, Validation Loss:2.4415, Validation Accuracy:0.1724\n",
    "Epoch #153: Loss:2.4354, Accuracy:0.1758, Validation Loss:2.4403, Validation Accuracy:0.1773\n",
    "Epoch #154: Loss:2.4352, Accuracy:0.1782, Validation Loss:2.4411, Validation Accuracy:0.1773\n",
    "Epoch #155: Loss:2.4354, Accuracy:0.1770, Validation Loss:2.4422, Validation Accuracy:0.1724\n",
    "Epoch #156: Loss:2.4361, Accuracy:0.1770, Validation Loss:2.4421, Validation Accuracy:0.1773\n",
    "Epoch #157: Loss:2.4357, Accuracy:0.1770, Validation Loss:2.4414, Validation Accuracy:0.1741\n",
    "Epoch #158: Loss:2.4353, Accuracy:0.1778, Validation Loss:2.4412, Validation Accuracy:0.1691\n",
    "Epoch #159: Loss:2.4356, Accuracy:0.1749, Validation Loss:2.4418, Validation Accuracy:0.1757\n",
    "Epoch #160: Loss:2.4352, Accuracy:0.1754, Validation Loss:2.4428, Validation Accuracy:0.1757\n",
    "Epoch #161: Loss:2.4353, Accuracy:0.1786, Validation Loss:2.4426, Validation Accuracy:0.1757\n",
    "Epoch #162: Loss:2.4347, Accuracy:0.1770, Validation Loss:2.4441, Validation Accuracy:0.1724\n",
    "Epoch #163: Loss:2.4347, Accuracy:0.1774, Validation Loss:2.4433, Validation Accuracy:0.1757\n",
    "Epoch #164: Loss:2.4349, Accuracy:0.1774, Validation Loss:2.4434, Validation Accuracy:0.1757\n",
    "Epoch #165: Loss:2.4350, Accuracy:0.1766, Validation Loss:2.4446, Validation Accuracy:0.1741\n",
    "Epoch #166: Loss:2.4348, Accuracy:0.1766, Validation Loss:2.4433, Validation Accuracy:0.1757\n",
    "Epoch #167: Loss:2.4342, Accuracy:0.1766, Validation Loss:2.4430, Validation Accuracy:0.1741\n",
    "Epoch #168: Loss:2.4342, Accuracy:0.1770, Validation Loss:2.4432, Validation Accuracy:0.1741\n",
    "Epoch #169: Loss:2.4342, Accuracy:0.1749, Validation Loss:2.4441, Validation Accuracy:0.1741\n",
    "Epoch #170: Loss:2.4340, Accuracy:0.1766, Validation Loss:2.4438, Validation Accuracy:0.1741\n",
    "Epoch #171: Loss:2.4337, Accuracy:0.1762, Validation Loss:2.4432, Validation Accuracy:0.1741\n",
    "Epoch #172: Loss:2.4343, Accuracy:0.1741, Validation Loss:2.4432, Validation Accuracy:0.1708\n",
    "Epoch #173: Loss:2.4343, Accuracy:0.1729, Validation Loss:2.4426, Validation Accuracy:0.1741\n",
    "Epoch #174: Loss:2.4345, Accuracy:0.1749, Validation Loss:2.4420, Validation Accuracy:0.1741\n",
    "Epoch #175: Loss:2.4345, Accuracy:0.1770, Validation Loss:2.4424, Validation Accuracy:0.1757\n",
    "Epoch #176: Loss:2.4348, Accuracy:0.1762, Validation Loss:2.4415, Validation Accuracy:0.1724\n",
    "Epoch #177: Loss:2.4338, Accuracy:0.1725, Validation Loss:2.4413, Validation Accuracy:0.1757\n",
    "Epoch #178: Loss:2.4338, Accuracy:0.1782, Validation Loss:2.4423, Validation Accuracy:0.1724\n",
    "Epoch #179: Loss:2.4341, Accuracy:0.1762, Validation Loss:2.4418, Validation Accuracy:0.1708\n",
    "Epoch #180: Loss:2.4340, Accuracy:0.1749, Validation Loss:2.4416, Validation Accuracy:0.1708\n",
    "Epoch #181: Loss:2.4338, Accuracy:0.1770, Validation Loss:2.4423, Validation Accuracy:0.1741\n",
    "Epoch #182: Loss:2.4335, Accuracy:0.1766, Validation Loss:2.4431, Validation Accuracy:0.1757\n",
    "Epoch #183: Loss:2.4324, Accuracy:0.1799, Validation Loss:2.4447, Validation Accuracy:0.1724\n",
    "Epoch #184: Loss:2.4329, Accuracy:0.1791, Validation Loss:2.4435, Validation Accuracy:0.1773\n",
    "Epoch #185: Loss:2.4333, Accuracy:0.1778, Validation Loss:2.4432, Validation Accuracy:0.1741\n",
    "Epoch #186: Loss:2.4324, Accuracy:0.1745, Validation Loss:2.4438, Validation Accuracy:0.1741\n",
    "Epoch #187: Loss:2.4335, Accuracy:0.1758, Validation Loss:2.4436, Validation Accuracy:0.1773\n",
    "Epoch #188: Loss:2.4325, Accuracy:0.1791, Validation Loss:2.4438, Validation Accuracy:0.1773\n",
    "Epoch #189: Loss:2.4322, Accuracy:0.1778, Validation Loss:2.4429, Validation Accuracy:0.1773\n",
    "Epoch #190: Loss:2.4322, Accuracy:0.1770, Validation Loss:2.4429, Validation Accuracy:0.1724\n",
    "Epoch #191: Loss:2.4324, Accuracy:0.1770, Validation Loss:2.4445, Validation Accuracy:0.1806\n",
    "Epoch #192: Loss:2.4317, Accuracy:0.1811, Validation Loss:2.4452, Validation Accuracy:0.1790\n",
    "Epoch #193: Loss:2.4330, Accuracy:0.1791, Validation Loss:2.4491, Validation Accuracy:0.1757\n",
    "Epoch #194: Loss:2.4348, Accuracy:0.1770, Validation Loss:2.4459, Validation Accuracy:0.1757\n",
    "Epoch #195: Loss:2.4327, Accuracy:0.1795, Validation Loss:2.4447, Validation Accuracy:0.1790\n",
    "Epoch #196: Loss:2.4323, Accuracy:0.1770, Validation Loss:2.4440, Validation Accuracy:0.1806\n",
    "Epoch #197: Loss:2.4314, Accuracy:0.1786, Validation Loss:2.4436, Validation Accuracy:0.1790\n",
    "Epoch #198: Loss:2.4319, Accuracy:0.1795, Validation Loss:2.4442, Validation Accuracy:0.1757\n",
    "Epoch #199: Loss:2.4309, Accuracy:0.1807, Validation Loss:2.4437, Validation Accuracy:0.1773\n",
    "Epoch #200: Loss:2.4311, Accuracy:0.1799, Validation Loss:2.4449, Validation Accuracy:0.1757\n",
    "Epoch #201: Loss:2.4324, Accuracy:0.1782, Validation Loss:2.4469, Validation Accuracy:0.1741\n",
    "Epoch #202: Loss:2.4313, Accuracy:0.1807, Validation Loss:2.4439, Validation Accuracy:0.1773\n",
    "Epoch #203: Loss:2.4318, Accuracy:0.1791, Validation Loss:2.4442, Validation Accuracy:0.1773\n",
    "Epoch #204: Loss:2.4329, Accuracy:0.1819, Validation Loss:2.4445, Validation Accuracy:0.1773\n",
    "Epoch #205: Loss:2.4486, Accuracy:0.1725, Validation Loss:2.4492, Validation Accuracy:0.1823\n",
    "Epoch #206: Loss:2.4435, Accuracy:0.1733, Validation Loss:2.4393, Validation Accuracy:0.1839\n",
    "Epoch #207: Loss:2.4339, Accuracy:0.1819, Validation Loss:2.4398, Validation Accuracy:0.1823\n",
    "Epoch #208: Loss:2.4377, Accuracy:0.1774, Validation Loss:2.4387, Validation Accuracy:0.1724\n",
    "Epoch #209: Loss:2.4342, Accuracy:0.1815, Validation Loss:2.4383, Validation Accuracy:0.1691\n",
    "Epoch #210: Loss:2.4317, Accuracy:0.1803, Validation Loss:2.4413, Validation Accuracy:0.1691\n",
    "Epoch #211: Loss:2.4326, Accuracy:0.1815, Validation Loss:2.4413, Validation Accuracy:0.1658\n",
    "Epoch #212: Loss:2.4316, Accuracy:0.1774, Validation Loss:2.4452, Validation Accuracy:0.1708\n",
    "Epoch #213: Loss:2.4331, Accuracy:0.1766, Validation Loss:2.4450, Validation Accuracy:0.1691\n",
    "Epoch #214: Loss:2.4322, Accuracy:0.1762, Validation Loss:2.4434, Validation Accuracy:0.1806\n",
    "Epoch #215: Loss:2.4324, Accuracy:0.1803, Validation Loss:2.4458, Validation Accuracy:0.1839\n",
    "Epoch #216: Loss:2.4323, Accuracy:0.1758, Validation Loss:2.4473, Validation Accuracy:0.1773\n",
    "Epoch #217: Loss:2.4330, Accuracy:0.1754, Validation Loss:2.4466, Validation Accuracy:0.1724\n",
    "Epoch #218: Loss:2.4327, Accuracy:0.1749, Validation Loss:2.4441, Validation Accuracy:0.1757\n",
    "Epoch #219: Loss:2.4326, Accuracy:0.1745, Validation Loss:2.4465, Validation Accuracy:0.1708\n",
    "Epoch #220: Loss:2.4322, Accuracy:0.1778, Validation Loss:2.4462, Validation Accuracy:0.1724\n",
    "Epoch #221: Loss:2.4335, Accuracy:0.1803, Validation Loss:2.4452, Validation Accuracy:0.1691\n",
    "Epoch #222: Loss:2.4313, Accuracy:0.1811, Validation Loss:2.4453, Validation Accuracy:0.1757\n",
    "Epoch #223: Loss:2.4311, Accuracy:0.1811, Validation Loss:2.4433, Validation Accuracy:0.1773\n",
    "Epoch #224: Loss:2.4312, Accuracy:0.1803, Validation Loss:2.4450, Validation Accuracy:0.1658\n",
    "Epoch #225: Loss:2.4313, Accuracy:0.1807, Validation Loss:2.4462, Validation Accuracy:0.1675\n",
    "Epoch #226: Loss:2.4309, Accuracy:0.1799, Validation Loss:2.4464, Validation Accuracy:0.1741\n",
    "Epoch #227: Loss:2.4315, Accuracy:0.1762, Validation Loss:2.4451, Validation Accuracy:0.1691\n",
    "Epoch #228: Loss:2.4326, Accuracy:0.1807, Validation Loss:2.4480, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4304, Accuracy:0.1799, Validation Loss:2.4476, Validation Accuracy:0.1823\n",
    "Epoch #230: Loss:2.4300, Accuracy:0.1803, Validation Loss:2.4467, Validation Accuracy:0.1806\n",
    "Epoch #231: Loss:2.4296, Accuracy:0.1799, Validation Loss:2.4480, Validation Accuracy:0.1757\n",
    "Epoch #232: Loss:2.4302, Accuracy:0.1823, Validation Loss:2.4491, Validation Accuracy:0.1757\n",
    "Epoch #233: Loss:2.4295, Accuracy:0.1807, Validation Loss:2.4476, Validation Accuracy:0.1823\n",
    "Epoch #234: Loss:2.4290, Accuracy:0.1819, Validation Loss:2.4486, Validation Accuracy:0.1790\n",
    "Epoch #235: Loss:2.4292, Accuracy:0.1844, Validation Loss:2.4482, Validation Accuracy:0.1790\n",
    "Epoch #236: Loss:2.4291, Accuracy:0.1819, Validation Loss:2.4481, Validation Accuracy:0.1757\n",
    "Epoch #237: Loss:2.4283, Accuracy:0.1786, Validation Loss:2.4473, Validation Accuracy:0.1790\n",
    "Epoch #238: Loss:2.4290, Accuracy:0.1836, Validation Loss:2.4474, Validation Accuracy:0.1757\n",
    "Epoch #239: Loss:2.4293, Accuracy:0.1823, Validation Loss:2.4722, Validation Accuracy:0.1593\n",
    "Epoch #240: Loss:2.4554, Accuracy:0.1762, Validation Loss:2.4675, Validation Accuracy:0.1560\n",
    "Epoch #241: Loss:2.4453, Accuracy:0.1700, Validation Loss:2.4505, Validation Accuracy:0.1773\n",
    "Epoch #242: Loss:2.4421, Accuracy:0.1811, Validation Loss:2.4551, Validation Accuracy:0.1708\n",
    "Epoch #243: Loss:2.4423, Accuracy:0.1749, Validation Loss:2.4483, Validation Accuracy:0.1741\n",
    "Epoch #244: Loss:2.4436, Accuracy:0.1766, Validation Loss:2.4439, Validation Accuracy:0.1773\n",
    "Epoch #245: Loss:2.4321, Accuracy:0.1791, Validation Loss:2.4439, Validation Accuracy:0.1856\n",
    "Epoch #246: Loss:2.4357, Accuracy:0.1786, Validation Loss:2.4438, Validation Accuracy:0.1708\n",
    "Epoch #247: Loss:2.4346, Accuracy:0.1811, Validation Loss:2.4455, Validation Accuracy:0.1757\n",
    "Epoch #248: Loss:2.4339, Accuracy:0.1799, Validation Loss:2.4431, Validation Accuracy:0.1741\n",
    "Epoch #249: Loss:2.4328, Accuracy:0.1803, Validation Loss:2.4422, Validation Accuracy:0.1790\n",
    "Epoch #250: Loss:2.4316, Accuracy:0.1791, Validation Loss:2.4438, Validation Accuracy:0.1773\n",
    "Epoch #251: Loss:2.4316, Accuracy:0.1795, Validation Loss:2.4425, Validation Accuracy:0.1773\n",
    "Epoch #252: Loss:2.4326, Accuracy:0.1803, Validation Loss:2.4423, Validation Accuracy:0.1773\n",
    "Epoch #253: Loss:2.4343, Accuracy:0.1799, Validation Loss:2.4431, Validation Accuracy:0.1790\n",
    "Epoch #254: Loss:2.4324, Accuracy:0.1762, Validation Loss:2.4410, Validation Accuracy:0.1839\n",
    "Epoch #255: Loss:2.4343, Accuracy:0.1811, Validation Loss:2.4422, Validation Accuracy:0.1806\n",
    "Epoch #256: Loss:2.4329, Accuracy:0.1758, Validation Loss:2.4443, Validation Accuracy:0.1839\n",
    "Epoch #257: Loss:2.4334, Accuracy:0.1799, Validation Loss:2.4450, Validation Accuracy:0.1806\n",
    "Epoch #258: Loss:2.4329, Accuracy:0.1774, Validation Loss:2.4461, Validation Accuracy:0.1773\n",
    "Epoch #259: Loss:2.4334, Accuracy:0.1733, Validation Loss:2.4443, Validation Accuracy:0.1823\n",
    "Epoch #260: Loss:2.4327, Accuracy:0.1786, Validation Loss:2.4451, Validation Accuracy:0.1839\n",
    "Epoch #261: Loss:2.4322, Accuracy:0.1803, Validation Loss:2.4437, Validation Accuracy:0.1839\n",
    "Epoch #262: Loss:2.4317, Accuracy:0.1795, Validation Loss:2.4455, Validation Accuracy:0.1823\n",
    "Epoch #263: Loss:2.4322, Accuracy:0.1795, Validation Loss:2.4458, Validation Accuracy:0.1839\n",
    "Epoch #264: Loss:2.4312, Accuracy:0.1795, Validation Loss:2.4452, Validation Accuracy:0.1806\n",
    "Epoch #265: Loss:2.4308, Accuracy:0.1795, Validation Loss:2.4450, Validation Accuracy:0.1823\n",
    "Epoch #266: Loss:2.4300, Accuracy:0.1791, Validation Loss:2.4460, Validation Accuracy:0.1823\n",
    "Epoch #267: Loss:2.4305, Accuracy:0.1778, Validation Loss:2.4460, Validation Accuracy:0.1823\n",
    "Epoch #268: Loss:2.4302, Accuracy:0.1778, Validation Loss:2.4456, Validation Accuracy:0.1823\n",
    "Epoch #269: Loss:2.4304, Accuracy:0.1774, Validation Loss:2.4446, Validation Accuracy:0.1806\n",
    "Epoch #270: Loss:2.4307, Accuracy:0.1786, Validation Loss:2.4445, Validation Accuracy:0.1839\n",
    "Epoch #271: Loss:2.4303, Accuracy:0.1791, Validation Loss:2.4461, Validation Accuracy:0.1856\n",
    "Epoch #272: Loss:2.4303, Accuracy:0.1774, Validation Loss:2.4450, Validation Accuracy:0.1806\n",
    "Epoch #273: Loss:2.4307, Accuracy:0.1799, Validation Loss:2.4451, Validation Accuracy:0.1823\n",
    "Epoch #274: Loss:2.4300, Accuracy:0.1811, Validation Loss:2.4470, Validation Accuracy:0.1806\n",
    "Epoch #275: Loss:2.4299, Accuracy:0.1807, Validation Loss:2.4461, Validation Accuracy:0.1823\n",
    "Epoch #276: Loss:2.4295, Accuracy:0.1778, Validation Loss:2.4448, Validation Accuracy:0.1806\n",
    "Epoch #277: Loss:2.4297, Accuracy:0.1782, Validation Loss:2.4453, Validation Accuracy:0.1806\n",
    "Epoch #278: Loss:2.4292, Accuracy:0.1791, Validation Loss:2.4465, Validation Accuracy:0.1839\n",
    "Epoch #279: Loss:2.4292, Accuracy:0.1774, Validation Loss:2.4458, Validation Accuracy:0.1823\n",
    "Epoch #280: Loss:2.4292, Accuracy:0.1811, Validation Loss:2.4449, Validation Accuracy:0.1823\n",
    "Epoch #281: Loss:2.4295, Accuracy:0.1803, Validation Loss:2.4460, Validation Accuracy:0.1806\n",
    "Epoch #282: Loss:2.4286, Accuracy:0.1774, Validation Loss:2.4466, Validation Accuracy:0.1839\n",
    "Epoch #283: Loss:2.4297, Accuracy:0.1791, Validation Loss:2.4458, Validation Accuracy:0.1806\n",
    "Epoch #284: Loss:2.4289, Accuracy:0.1811, Validation Loss:2.4458, Validation Accuracy:0.1872\n",
    "Epoch #285: Loss:2.4290, Accuracy:0.1762, Validation Loss:2.4464, Validation Accuracy:0.1839\n",
    "Epoch #286: Loss:2.4285, Accuracy:0.1815, Validation Loss:2.4466, Validation Accuracy:0.1839\n",
    "Epoch #287: Loss:2.4288, Accuracy:0.1815, Validation Loss:2.4470, Validation Accuracy:0.1823\n",
    "Epoch #288: Loss:2.4292, Accuracy:0.1758, Validation Loss:2.4466, Validation Accuracy:0.1806\n",
    "Epoch #289: Loss:2.4289, Accuracy:0.1795, Validation Loss:2.4460, Validation Accuracy:0.1823\n",
    "Epoch #290: Loss:2.4297, Accuracy:0.1762, Validation Loss:2.4481, Validation Accuracy:0.1806\n",
    "Epoch #291: Loss:2.4307, Accuracy:0.1799, Validation Loss:2.4466, Validation Accuracy:0.1823\n",
    "Epoch #292: Loss:2.4285, Accuracy:0.1803, Validation Loss:2.4478, Validation Accuracy:0.1773\n",
    "Epoch #293: Loss:2.4292, Accuracy:0.1733, Validation Loss:2.4471, Validation Accuracy:0.1823\n",
    "Epoch #294: Loss:2.4298, Accuracy:0.1803, Validation Loss:2.4466, Validation Accuracy:0.1823\n",
    "Epoch #295: Loss:2.4290, Accuracy:0.1778, Validation Loss:2.4491, Validation Accuracy:0.1773\n",
    "Epoch #296: Loss:2.4285, Accuracy:0.1778, Validation Loss:2.4468, Validation Accuracy:0.1823\n",
    "Epoch #297: Loss:2.4284, Accuracy:0.1803, Validation Loss:2.4476, Validation Accuracy:0.1790\n",
    "Epoch #298: Loss:2.4279, Accuracy:0.1795, Validation Loss:2.4470, Validation Accuracy:0.1806\n",
    "Epoch #299: Loss:2.4278, Accuracy:0.1778, Validation Loss:2.4458, Validation Accuracy:0.1806\n",
    "Epoch #300: Loss:2.4278, Accuracy:0.1782, Validation Loss:2.4460, Validation Accuracy:0.1823\n",
    "\n",
    "Test:\n",
    "Test Loss:2.44597197, Accuracy:0.1823\n",
    "Labels: ['sg', 'sk', 'eo', 'yd', 'eg', 'ck', 'ds', 'ib', 'my', 'ce', 'aa', 'eb', 'ek', 'by', 'mb']\n",
    "Confusion Matrix:\n",
    "      sg  sk  eo  yd  eg  ck  ds  ib  my  ce  aa  eb  ek  by  mb\n",
    "t:sg  28   0   0  16   2   0   0   0   0   0   0   5   0   0   0\n",
    "t:sk  13   0   1   2  13   0   2   0   0   0   0   2   0   0   0\n",
    "t:eo  20   0   0   4   6   0   0   0   0   0   0   4   0   0   0\n",
    "t:yd  20   0   0  40   2   0   0   0   0   0   0   0   0   0   0\n",
    "t:eg   9   0   3   0  27   0   7   0   0   0   0   4   0   0   0\n",
    "t:ck   4   0   2   3   6   0   0   0   0   0   0   7   0   1   0\n",
    "t:ds   8   0   1   1  11   0   7   0   0   0   0   2   0   1   0\n",
    "t:ib  14   0   1  36   3   0   0   0   0   0   0   0   0   0   0\n",
    "t:my   5   0   1   4   9   0   0   0   0   0   0   1   0   0   0\n",
    "t:ce  10   0   2   3   8   0   3   0   0   0   0   0   0   1   0\n",
    "t:aa   0   0   0   5  21   0   5   0   0   0   0   3   0   0   0\n",
    "t:eb  13   0   1  13  17   0   1   0   0   0   0   5   0   0   0\n",
    "t:ek  25   0   1   9   8   0   1   0   0   0   0   4   0   0   0\n",
    "t:by  16   0   2   4   5   0   0   0   0   0   0   9   0   4   0\n",
    "t:mb  23   0   1   9  11   0   0   0   0   0   0   8   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          sg       0.13      0.55      0.22        51\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          yd       0.27      0.65      0.38        62\n",
    "          eg       0.18      0.54      0.27        50\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ds       0.27      0.23      0.25        31\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          eb       0.09      0.10      0.10        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          by       0.57      0.10      0.17        40\n",
    "          mb       0.00      0.00      0.00        52\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.10      0.14      0.09       609\n",
    "weighted avg       0.11      0.18      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 13:04:12 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 37 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7070169675917852, 2.6955715079221427, 2.6884823871167813, 2.6824421831735443, 2.676649619401578, 2.671770535470621, 2.666545408308408, 2.6590849691619622, 2.65082750492691, 2.638652941276287, 2.6265013268819977, 2.611746171425129, 2.59483892968527, 2.595793391291927, 2.5724501069543395, 2.5704421394172754, 2.5478389149620417, 2.5474185908369242, 2.5348683593699888, 2.520716872708551, 2.5117784270707806, 2.5070995387968367, 2.4960424535967447, 2.492212757492692, 2.486810931626995, 2.481715074705177, 2.4790158428386317, 2.476331528380195, 2.4715070133334507, 2.4723794393743006, 2.473780006023463, 2.4742327096622763, 2.4719546064367437, 2.4686735255769126, 2.460684952477516, 2.493705090630818, 2.519198831470533, 2.4706323491332958, 2.473141040708044, 2.4680953855780743, 2.4620017523836033, 2.461597269196033, 2.4542489513779313, 2.45177281509675, 2.463513457716392, 2.4536762640785503, 2.451896232924438, 2.451398751222833, 2.4456203058239665, 2.4444128691856495, 2.4439991514866772, 2.4425826358481975, 2.44599953034437, 2.4437761792212678, 2.45894045391302, 2.452653417837835, 2.4583744309805886, 2.4466749306382805, 2.4434095176765678, 2.44211374519298, 2.443688403600934, 2.4494236400366223, 2.4478206043368687, 2.4433104498633025, 2.4444823993250653, 2.4423239747878953, 2.4431173335546736, 2.4427026042405804, 2.4418972673870267, 2.440888942756089, 2.4387432217402214, 2.438768924359226, 2.439873340290364, 2.4375658039193238, 2.4404858783352354, 2.441690439465402, 2.4422400314819637, 2.4402753030529554, 2.441363089973312, 2.4407934320384057, 2.4395020219492793, 2.4399289908667505, 2.4406914773637243, 2.4396765204681747, 2.438760151416797, 2.4392288157896846, 2.4372458230881464, 2.4392536919692467, 2.4411543578349897, 2.44119203658331, 2.4408550360324153, 2.440855060696406, 2.440798517126951, 2.4408926415717467, 2.440705901883506, 2.439088105763904, 2.4404536875206455, 2.4416401617241217, 2.442845723899127, 2.4435727803773677, 2.441574566469991, 2.441725564903422, 2.4409854560845785, 2.440459327744733, 2.4408683236596618, 2.4398740156139254, 2.440121093211307, 2.439066083168944, 2.4408376960723075, 2.4395820984895202, 2.440151673428139, 2.4408616025263843, 2.439798164837466, 2.439943483114634, 2.4392612931763598, 2.439972962456188, 2.440566205821797, 2.442367317249818, 2.4416840800706585, 2.440884345074984, 2.4403340612922007, 2.4406637472080677, 2.440666568690333, 2.4409911507260427, 2.440514306520985, 2.439804083020816, 2.439938796564863, 2.441041475446353, 2.4403358265292665, 2.4401571151658232, 2.440608142436236, 2.4408215638647723, 2.4404652592388083, 2.440928942660001, 2.440411212604817, 2.442038538029237, 2.442787745511786, 2.442171880960073, 2.442743698364408, 2.442230405478642, 2.442714957376615, 2.4430125636615974, 2.4422827048841955, 2.442287958705758, 2.4437906945671743, 2.4428006980219497, 2.442151065334702, 2.4426391856815233, 2.442146077336153, 2.4419942231013856, 2.4416691657944853, 2.4414664395337033, 2.4402738255624503, 2.44112100468089, 2.442183568168352, 2.4420981790827607, 2.441390117009481, 2.4411829927284727, 2.441753879556515, 2.4427894207057106, 2.4425903963925215, 2.4441418240614516, 2.4433301982817, 2.4433716439652717, 2.444644082747461, 2.4432886108780534, 2.4430326029584912, 2.443204879760742, 2.444077151553776, 2.4438293395175528, 2.443191866569331, 2.443240191940408, 2.4426405746948543, 2.4419681767524755, 2.4423738469435468, 2.4415377971574004, 2.441319118775366, 2.4423138723389073, 2.441821511743104, 2.4415908447040127, 2.442346930308099, 2.443067373704832, 2.4447493189074136, 2.443508581966406, 2.443241225479076, 2.443810312618763, 2.443607893287646, 2.4438238648945476, 2.4429185323918787, 2.442861980209601, 2.4445339623343183, 2.4451818787209896, 2.4491139092468863, 2.445888807816654, 2.444684572407765, 2.4439693147130006, 2.4435963442760147, 2.4442047391619, 2.4436879670874436, 2.4449134076561636, 2.446878731544382, 2.4438960759706294, 2.444169609417469, 2.444458993197662, 2.4491527726497555, 2.4393158550137173, 2.4397511396110545, 2.438732848770317, 2.4383353916882293, 2.4413078731699724, 2.441271394344386, 2.4451943238576255, 2.44502768493051, 2.4434316960853115, 2.445842381964372, 2.4472506731406027, 2.446581625586073, 2.4441294192484837, 2.446535534459382, 2.4461581836193065, 2.445218449938669, 2.4452815455168926, 2.4432568487471156, 2.444960229306777, 2.4462358575736363, 2.4463629922256094, 2.445062937603404, 2.448007920692707, 2.4476357500737134, 2.446701706728129, 2.4480037638315033, 2.449106104463975, 2.4476387132760533, 2.448612463689594, 2.448228860723561, 2.4481486420717538, 2.447276560935285, 2.447387910633056, 2.4722098489896025, 2.467490232636776, 2.4504710856721124, 2.4551188742194467, 2.4482787865136055, 2.4438669853805517, 2.443901196293447, 2.4437566872300773, 2.4454688340768045, 2.4431298049212677, 2.44215630388808, 2.44382018643647, 2.4425280837981376, 2.4423118463682227, 2.443071136333672, 2.4410362983572074, 2.4421971235760718, 2.444347367106596, 2.4449633357951597, 2.446102660277794, 2.444334990481046, 2.4451385472208407, 2.4437441132926003, 2.4454688888856735, 2.4457688229816106, 2.445226758180189, 2.4450441103654934, 2.445955577938036, 2.446043581406667, 2.445565763169713, 2.4446038616506143, 2.444466491833892, 2.4460590645205995, 2.4450486054757152, 2.445064028299892, 2.4470394836075005, 2.4460585426618704, 2.4448043920332183, 2.44526774072882, 2.4464989922121045, 2.4458236803953675, 2.444850270188305, 2.4460074200810276, 2.4466483769158422, 2.4457671736261526, 2.4458252260054665, 2.446384963143635, 2.446629106117587, 2.4470051865663827, 2.446563490505876, 2.4459818075051643, 2.4480857696439244, 2.4465636408387734, 2.4478476266751343, 2.4470940403554633, 2.446646081598717, 2.4490717565289075, 2.446768985789006, 2.447625457751144, 2.4470025349916105, 2.445832619721862, 2.4459719462151988], 'val_acc': [0.031198686368041635, 0.0821018060317572, 0.08538587808560072, 0.10016420340449939, 0.10673234761005944, 0.11658456455457386, 0.10837438402847312, 0.13628899764838476, 0.1330049253009223, 0.1297208530513328, 0.12972085324707877, 0.1297208530513328, 0.14285714214756376, 0.14778325061982098, 0.13136288997134551, 0.1642036118677684, 0.16420361176989545, 0.12807881762388304, 0.15599343104804875, 0.1412151056312771, 0.15270935879845923, 0.17077175626907443, 0.1576354671728435, 0.17898193708879412, 0.17569786474133164, 0.17898193699092113, 0.17241379229599618, 0.17241379239386917, 0.1740558285186639, 0.17405582842079093, 0.18226600914263763, 0.16912972024215267, 0.16912972014427968, 0.1691297199485337, 0.17241379239386917, 0.16256157554722772, 0.16420361246724044, 0.18390804556105134, 0.17733990106187233, 0.16748768392161195, 0.17733990067038044, 0.16748768401948494, 0.17569786454558567, 0.18062397272422395, 0.1740558286165369, 0.17569786454558567, 0.17405582842079093, 0.1740558286165369, 0.17405582842079093, 0.17241379229599618, 0.17405582842079093, 0.17405582842079093, 0.17241379229599618, 0.17569786464345866, 0.1773399008661264, 0.17733990067038044, 0.17569786474133164, 0.16912972014427968, 0.17077175617120144, 0.17241379239386917, 0.17241379239386917, 0.16748768401948494, 0.17241379200237725, 0.17241379219812322, 0.17077175626907443, 0.1740558285186639, 0.16748768382373896, 0.1740558285186639, 0.1740558285186639, 0.17077175626907443, 0.17898193689304814, 0.1773399007682534, 0.1740558285186639, 0.17733990047463447, 0.17241379239386917, 0.17077175617120144, 0.1756978644477127, 0.17569786464345866, 0.1740558285186639, 0.18062397311571587, 0.1773399008661264, 0.1740558286165369, 0.17898193699092113, 0.1740558285186639, 0.1773399008661264, 0.17898193699092113, 0.1773399007682534, 0.18226600924051062, 0.17569786474133164, 0.18390804536530536, 0.18390804536530536, 0.17898193699092113, 0.1773399008661264, 0.1773399008661264, 0.17898193699092113, 0.17898193689304814, 0.18226600924051062, 0.1806239730178429, 0.17898193689304814, 0.17898193699092113, 0.1806239730178429, 0.18390804536530536, 0.18226600924051062, 0.17898193699092113, 0.1839080452674324, 0.1839080452674324, 0.1871921175170219, 0.18226600914263763, 0.17569786454558567, 0.1806239730178429, 0.18226600914263763, 0.17405582842079093, 0.17898193689304814, 0.1740558285186639, 0.17569786464345866, 0.1839080452674324, 0.1773399007682534, 0.17569786464345866, 0.1773399007682534, 0.1806239730178429, 0.1839080452674324, 0.17898193689304814, 0.1839080452674324, 0.1806239730178429, 0.18226600914263763, 0.18226600914263763, 0.18226600914263763, 0.1773399007682534, 0.1839080452674324, 0.1839080452674324, 0.17569786464345866, 0.1839080452674324, 0.1839080452674324, 0.1740558285186639, 0.1806239730178429, 0.1740558285186639, 0.1773399007682534, 0.1773399007682534, 0.1740558285186639, 0.1806239730178429, 0.1773399007682534, 0.17077175626907443, 0.17898193689304814, 0.17898193689304814, 0.17077175626907443, 0.17898193689304814, 0.17898193689304814, 0.1740558285186639, 0.1773399007682534, 0.17241379239386917, 0.17569786464345866, 0.17241379239386917, 0.1773399007682534, 0.1773399007682534, 0.17241379239386917, 0.1773399007682534, 0.1740558285186639, 0.16912972014427968, 0.17569786464345866, 0.17569786464345866, 0.17569786464345866, 0.17241379239386917, 0.17569786464345866, 0.17569786464345866, 0.1740558285186639, 0.17569786464345866, 0.1740558285186639, 0.1740558285186639, 0.1740558285186639, 0.1740558285186639, 0.1740558285186639, 0.17077175626907443, 0.1740558285186639, 0.1740558285186639, 0.17569786464345866, 0.17241379239386917, 0.17569786464345866, 0.17241379239386917, 0.17077175626907443, 0.17077175626907443, 0.1740558285186639, 0.17569786464345866, 0.17241379239386917, 0.1773399007682534, 0.1740558285186639, 0.1740558285186639, 0.1773399007682534, 0.1773399007682534, 0.1773399007682534, 0.17241379249174216, 0.1806239730178429, 0.17898193689304814, 0.17569786454558567, 0.17569786464345866, 0.17898193689304814, 0.1806239730178429, 0.17898193699092113, 0.17569786464345866, 0.1773399008661264, 0.17569786464345866, 0.1740558285186639, 0.17733990096399938, 0.1773399007682534, 0.1773399007682534, 0.18226600894689168, 0.18390804516955941, 0.18226600924051062, 0.17241379239386917, 0.16912972014427968, 0.16912972014427968, 0.1658456478946902, 0.17077175626907443, 0.16912972024215267, 0.18062397311571587, 0.18390804536530536, 0.1773399008661264, 0.17241379239386917, 0.17569786464345866, 0.17077175617120144, 0.17241379229599618, 0.1691297200464067, 0.17569786464345866, 0.1773399007682534, 0.1658456477968172, 0.16748768392161195, 0.1740558285186639, 0.16912972014427968, 0.1691297200464067, 0.1822660093383836, 0.18062397311571587, 0.17569786464345866, 0.17569786464345866, 0.1822660093383836, 0.17898193699092113, 0.17898193699092113, 0.17569786474133164, 0.17898193699092113, 0.17569786464345866, 0.15927750368913016, 0.1559934318432667, 0.17733990057250745, 0.17077175646482037, 0.17405582832291797, 0.1773399007682534, 0.1855500815879731, 0.1707717563669474, 0.1756978644477127, 0.1740558285186639, 0.17898193689304814, 0.1773399007682534, 0.1773399007682534, 0.1773399008661264, 0.1789819366973022, 0.18390804536530536, 0.18062397291996993, 0.18390804507168643, 0.18062397311571587, 0.17733990057250745, 0.18226600924051062, 0.18390804536530536, 0.18390804546317835, 0.18226600914263763, 0.18390804536530536, 0.1806239730178429, 0.18226600924051062, 0.18226600914263763, 0.18226600914263763, 0.18226600924051062, 0.1806239730178429, 0.18390804546317835, 0.1855500814901001, 0.1806239730178429, 0.18226600924051062, 0.1806239730178429, 0.18226600924051062, 0.1806239730178429, 0.1806239730178429, 0.18390804546317835, 0.18226600924051062, 0.18226600924051062, 0.1806239730178429, 0.18390804536530536, 0.1806239730178429, 0.18719211771276784, 0.18390804546317835, 0.18390804536530536, 0.18226600924051062, 0.1806239730178429, 0.18226600924051062, 0.1806239730178429, 0.18226600924051062, 0.17733990067038044, 0.18226600914263763, 0.18226600914263763, 0.17733990067038044, 0.18226600924051062, 0.17898193679517518, 0.1806239730178429, 0.1806239730178429, 0.18226600924051062], 'loss': [2.710607595854961, 2.700654000374326, 2.6911735341779015, 2.6835318055241015, 2.677851125885574, 2.671063362842223, 2.666448041496825, 2.6593104879469354, 2.6510367719544523, 2.6415273517553812, 2.6296676795340663, 2.6150666002864957, 2.599122640778152, 2.5825846564353614, 2.581562927567249, 2.56239324947647, 2.559114027414968, 2.54277310126616, 2.5312317857996884, 2.521998890126755, 2.511698209480583, 2.5050671516747447, 2.5017201686052326, 2.4989933986193833, 2.4934964917278877, 2.4900356258454996, 2.4842947990252986, 2.4818568078889003, 2.4779844908743667, 2.4752266354139825, 2.4705573853526026, 2.4705501093267173, 2.4723396270916447, 2.4707758332669614, 2.467959637122967, 2.4921134782278074, 2.5011209161864167, 2.4837466525835667, 2.4856951671459346, 2.4679360036242914, 2.4711969967005922, 2.4643492683982458, 2.461449200318824, 2.459748446721071, 2.456928518910183, 2.4666193661503724, 2.464767689577608, 2.459581796246632, 2.45657267120095, 2.4558257550429516, 2.4534662579608892, 2.453292118401498, 2.452955207981368, 2.451984439297623, 2.4515309526690223, 2.4570222054663624, 2.4521765117527767, 2.4561697519290617, 2.451652607927577, 2.451703995698776, 2.447781473606274, 2.451342526010909, 2.4497747015904108, 2.450042777286663, 2.4477944516058576, 2.4496325379279607, 2.4490544666744603, 2.4476364998357254, 2.447153939260839, 2.4461621932670075, 2.4461008847371755, 2.4460619569069553, 2.445212482084239, 2.4456681682343846, 2.445873584982306, 2.4456805588528363, 2.4452621200491027, 2.444467242591435, 2.44532308509952, 2.443768223991629, 2.4435528170646337, 2.442359609329725, 2.442723701915702, 2.442846186694668, 2.4432246628238437, 2.4421037948107083, 2.442149838285035, 2.442153692637136, 2.4418754874313637, 2.4426119770112713, 2.442055702601125, 2.442224139109774, 2.4425739730897624, 2.442088035685326, 2.4418334953104446, 2.4414133414595525, 2.4409052000887828, 2.441088905471551, 2.441465513123624, 2.4411490852583118, 2.4412140313604773, 2.4407828472967754, 2.439775588135455, 2.440280168012427, 2.4405503414984357, 2.4405065520832916, 2.4399348720143217, 2.441352278985527, 2.440709362969996, 2.4408641418637194, 2.440865038356742, 2.439590325639478, 2.4393411431714007, 2.439630623960397, 2.439316723429936, 2.4388364268034635, 2.438516404251788, 2.439246281852957, 2.4391501813453815, 2.4391470832746376, 2.4397081809856562, 2.4386767943536967, 2.438405931656855, 2.4388885768287234, 2.438880658884068, 2.4382674889887626, 2.4381702402533936, 2.4382363776651497, 2.4377467128040853, 2.43744015429299, 2.437532861325775, 2.43674856534484, 2.4370137060936963, 2.4366942687690627, 2.4379349582249135, 2.4362545654759025, 2.4362280037858404, 2.438202101787747, 2.437452734177607, 2.4381926033286345, 2.437044253829079, 2.436626817362509, 2.4364306382330048, 2.436707102934193, 2.4363139568657846, 2.436651976740091, 2.4369511662077854, 2.435792028243047, 2.436852741241455, 2.4353967205454925, 2.4353762791142084, 2.435595300016462, 2.4353649993207176, 2.4351726543732, 2.4353736241005772, 2.436052837117252, 2.4357376591136077, 2.435266162237837, 2.4356481220198365, 2.435216797842382, 2.4353073228800812, 2.434699945723986, 2.4346707536944128, 2.4349262094595594, 2.435012935074448, 2.4348160641883676, 2.43422339133903, 2.434178974594179, 2.4341511834083884, 2.433970372339049, 2.4337309524998285, 2.4342748452016214, 2.434281111840595, 2.43454743861173, 2.4344630037734643, 2.4347970463167226, 2.4338464010422722, 2.43375633120292, 2.4341170531278764, 2.4339700190683167, 2.4337827956652003, 2.4335496668453334, 2.4324417768317814, 2.4328876435634292, 2.4333282718423455, 2.4323972494450437, 2.433486338366718, 2.4324763319575564, 2.4321548277837297, 2.4322105403798315, 2.432379658755825, 2.431724858724606, 2.433040914300531, 2.434799569539221, 2.432653450329446, 2.432285857249579, 2.431351683712593, 2.4319067990265832, 2.430870868635863, 2.4311175645009695, 2.43236218105841, 2.431278705694837, 2.431793538060276, 2.4328648296959345, 2.4486105115017116, 2.4434940792451894, 2.43385521792778, 2.437718513907838, 2.4342291948486894, 2.431693921784356, 2.4325629628904055, 2.4316247917543445, 2.4330707464864365, 2.4322163609263834, 2.432385327536957, 2.4322741618146644, 2.4330044301873115, 2.432676204469904, 2.432568102352918, 2.4321913160093023, 2.4334540478747484, 2.4312747260138727, 2.4310921757128203, 2.431171410871972, 2.4312512606321173, 2.4309145964636207, 2.431538779241104, 2.4325787468857345, 2.430386250611448, 2.429973992576834, 2.4295811039221604, 2.430206379606494, 2.4295037005226714, 2.428967296807918, 2.4291696464256582, 2.4290544915248238, 2.428275952936443, 2.4289905487389536, 2.4292904682472747, 2.4554366663006544, 2.445348031711774, 2.4420524151663026, 2.4422805528621163, 2.4435977097654247, 2.432097332286639, 2.435699277443073, 2.434605882887478, 2.4339142739650406, 2.432783033128147, 2.431612508840385, 2.431617364060952, 2.4326380088344, 2.4342766276130443, 2.4323946355549464, 2.4342733189310626, 2.4328853836294564, 2.433396712759437, 2.432930221400956, 2.4333959023320944, 2.4327089321441964, 2.432158796106765, 2.4317275417414046, 2.4321602697000366, 2.4312212745267017, 2.430826941456883, 2.4300365750549755, 2.430451305299324, 2.4301808579501674, 2.4303898227777814, 2.4307061903285785, 2.43031220553592, 2.4303254644484, 2.430674445898381, 2.429989046923189, 2.429885416011301, 2.4294620867382575, 2.429729509941117, 2.429167092850076, 2.429233700832547, 2.429203962349549, 2.429453752956351, 2.4286003794268662, 2.4297346482286706, 2.42892528559393, 2.428991764331011, 2.42845711590573, 2.4287647248293585, 2.429246855075844, 2.4288882624686865, 2.4297493443107214, 2.430748150823542, 2.4285369890671245, 2.429150259029694, 2.4297691862196404, 2.4289998560715507, 2.428454859007066, 2.4284231134019105, 2.427940582984282, 2.4277645394053056, 2.427840663275435], 'acc': [0.03778234103530095, 0.05954825516170545, 0.09281314123704938, 0.10841889168447538, 0.11581108783977967, 0.12689938391013802, 0.12977412821575846, 0.1388090358132944, 0.14209445636620022, 0.13963038907281183, 0.13388090341732486, 0.13511293527404386, 0.13675564791877168, 0.14209445556453612, 0.1367556457279644, 0.16057494777184003, 0.15441478408827183, 0.14045174648140002, 0.14579055491046983, 0.14866529727618552, 0.147022587783038, 0.1544147839108042, 0.15852156108899282, 0.15605749397307206, 0.1630390140860967, 0.16057494757601368, 0.17248459889169102, 0.16960985615268134, 0.1691991783387852, 0.1691991779287738, 0.17207392266276436, 0.17125256701661332, 0.17207392109615358, 0.16673511284455136, 0.16919917894462297, 0.1655030807736473, 0.15934291671678516, 0.16509240215808704, 0.16591375778587936, 0.1687885016998471, 0.16919917972792836, 0.1655030807736473, 0.1667351134320304, 0.16878849991905126, 0.1696098551551909, 0.16344969148998142, 0.17166324541798852, 0.1679671452887494, 0.1704312117988324, 0.17248459928334373, 0.17166324504469455, 0.16960985615268134, 0.17166324504469455, 0.16878850072071538, 0.16960985715017182, 0.16550307999034192, 0.17207392209364403, 0.16386036949970395, 0.1687885011307268, 0.17166324543634723, 0.17125256760409235, 0.17248459988918147, 0.16755646747485323, 0.16796714685536018, 0.1720739224485793, 0.16796714628623985, 0.16673511360949805, 0.16837782427760364, 0.16714579026679482, 0.1675564672973856, 0.17289527652811956, 0.1733059558902678, 0.17166324543634723, 0.17043121236795272, 0.1696098557793874, 0.16837782451014743, 0.16837782449178873, 0.1691991779287738, 0.17125256703497202, 0.1737166323333795, 0.17248459969335514, 0.17207392088196852, 0.1733059539320043, 0.1728952771155986, 0.17453798676785504, 0.17207392284023199, 0.1765913756598682, 0.1716632448488682, 0.17494866440428355, 0.17453798678621374, 0.17577002159868668, 0.1757700196404232, 0.17330595412783065, 0.17371663313504362, 0.17412731114476612, 0.17700205249463263, 0.17659137587405327, 0.17535934341149653, 0.17535934280565876, 0.1741273099330906, 0.1753593426098324, 0.17207392266276436, 0.17535934321567018, 0.1761806976501457, 0.17823408632797383, 0.1757700200320759, 0.17618069784597204, 0.17577002022790222, 0.17412730975562296, 0.17207392227111165, 0.1745379885670096, 0.17535934221817973, 0.1786447641602287, 0.17535934300148512, 0.17535934398061687, 0.17741273148348688, 0.17535934300148512, 0.17864476433769633, 0.17659137587405327, 0.17782340892408907, 0.17823408515301573, 0.17453798737369278, 0.17659137626570598, 0.17782340851407766, 0.17782340890573037, 0.17823408730710558, 0.17782340814078368, 0.18028747343919116, 0.17453798876283594, 0.1774127303085288, 0.17659137507238917, 0.17700205290464405, 0.17289527611810812, 0.17618069902093014, 0.17659137606987962, 0.17782340890573037, 0.17618069706266665, 0.1728952763322932, 0.1749486642084572, 0.17453798876283594, 0.1728952767055872, 0.17659137646153233, 0.1765913752498568, 0.17535934301984385, 0.1753593426098324, 0.17741273208932465, 0.17782340892408907, 0.17577002142121906, 0.17659137509074788, 0.17618069763178698, 0.1790554409766344, 0.17618069903928885, 0.17577001983624954, 0.17823408534884208, 0.17700205407960215, 0.1770020531004704, 0.17700205368794944, 0.17782340771241353, 0.17494866438592485, 0.17535934280565876, 0.17864476318109695, 0.1770020538837758, 0.17741273091436657, 0.17741273210768338, 0.17659137585569457, 0.17659137466237776, 0.17659137567822694, 0.17700205288628534, 0.1749486641900985, 0.17659137528657423, 0.17618069823762475, 0.17412731075311344, 0.1728952767055872, 0.17494866522430638, 0.17700205229880628, 0.1761806992351152, 0.17248459889169102, 0.17823408515301573, 0.17618069884346252, 0.17494866597089434, 0.17700205229880628, 0.17659137546404186, 0.17987679582112134, 0.17905544138664584, 0.177823408709904, 0.17453798719622515, 0.17577002101120762, 0.17905544218830993, 0.17782340812242498, 0.177002052690459, 0.17700205427542848, 0.18110883026030028, 0.17905544197412487, 0.17700205267210026, 0.1794661195921947, 0.177002052690459, 0.17864476334020588, 0.179466120161315, 0.18069815184056637, 0.17987679662278547, 0.1782340865421589, 0.18069815225057778, 0.17905544117246075, 0.18193018408893805, 0.17248459908751737, 0.17330595451948333, 0.181930185710625, 0.17741273050435516, 0.18151950727253235, 0.18028747384920257, 0.1815195082700228, 0.17741273029017007, 0.1765913756598682, 0.17618069925347393, 0.18028747422249655, 0.1757700208153813, 0.17535934221817973, 0.17494866518758895, 0.17453798835282452, 0.17782340931574178, 0.18028747482833432, 0.18110882887115715, 0.18110882906698347, 0.1802874752016283, 0.1806981526422305, 0.1798767976019172, 0.17618069806015713, 0.18069815283805682, 0.17987679740609086, 0.18028747541581336, 0.17987679621277403, 0.1823408617253666, 0.18069815303388317, 0.18193018392982913, 0.18439425059902104, 0.18193018371564407, 0.17864476277108554, 0.18357289636037188, 0.18234086211701928, 0.1761806980417984, 0.17002053320163085, 0.18110883006447395, 0.17494866518758895, 0.1765913756598682, 0.17905544257996264, 0.17864476355439093, 0.18110882945863618, 0.1798767972102645, 0.180287475219987, 0.1790554417599398, 0.1794661184172366, 0.18028747463250797, 0.17987679740609086, 0.17618069806015713, 0.18110883004611522, 0.17577002102956635, 0.17987679599858897, 0.17741272993523483, 0.17330595592698522, 0.1786447649435341, 0.18028747345754986, 0.17946611798886647, 0.17946611800722517, 0.17946611879053057, 0.17946611861306294, 0.17905544195576614, 0.17782340812242498, 0.1778234094932094, 0.1774127299168761, 0.17864476373185856, 0.17905544236577756, 0.1774127318934983, 0.17987679640860038, 0.18110883045612663, 0.18069815225057778, 0.17782340812242498, 0.17823408556302714, 0.1790554413682871, 0.17741273011270245, 0.18110882926280983, 0.1802874740266702, 0.17741273208932465, 0.17905544177829852, 0.1811088298686476, 0.1761806984334511, 0.18151950609757425, 0.18151950688087964, 0.17577001983624954, 0.17946611898635692, 0.17618069786433077, 0.1798767960169477, 0.180287475219987, 0.17330595412783065, 0.18028747363501751, 0.17782340792659862, 0.17782340812242498, 0.18028747461414923, 0.17946611939636833, 0.1778234083182513, 0.1782340869338116]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
