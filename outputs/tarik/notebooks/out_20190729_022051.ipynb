{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf77.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 02:20:51 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '0Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '01', '05', '02', '04'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001A3026FCE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001A350146EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6080, Accuracy:0.2259, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6064, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6022, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6017, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2348\n",
    "Epoch #11: Loss:1.6014, Accuracy:0.2390, Validation Loss:1.6051, Validation Accuracy:0.2348\n",
    "Epoch #12: Loss:1.6009, Accuracy:0.2452, Validation Loss:1.6049, Validation Accuracy:0.2348\n",
    "Epoch #13: Loss:1.6009, Accuracy:0.2452, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #14: Loss:1.6004, Accuracy:0.2448, Validation Loss:1.6056, Validation Accuracy:0.2348\n",
    "Epoch #15: Loss:1.6002, Accuracy:0.2448, Validation Loss:1.6070, Validation Accuracy:0.2348\n",
    "Epoch #16: Loss:1.6003, Accuracy:0.2456, Validation Loss:1.6079, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6014, Accuracy:0.2411, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #18: Loss:1.6002, Accuracy:0.2439, Validation Loss:1.6073, Validation Accuracy:0.2299\n",
    "Epoch #19: Loss:1.5998, Accuracy:0.2464, Validation Loss:1.6068, Validation Accuracy:0.2381\n",
    "Epoch #20: Loss:1.5994, Accuracy:0.2435, Validation Loss:1.6071, Validation Accuracy:0.2348\n",
    "Epoch #21: Loss:1.5992, Accuracy:0.2444, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.5988, Accuracy:0.2480, Validation Loss:1.6079, Validation Accuracy:0.2365\n",
    "Epoch #23: Loss:1.5989, Accuracy:0.2476, Validation Loss:1.6079, Validation Accuracy:0.2381\n",
    "Epoch #24: Loss:1.5986, Accuracy:0.2468, Validation Loss:1.6075, Validation Accuracy:0.2381\n",
    "Epoch #25: Loss:1.5985, Accuracy:0.2468, Validation Loss:1.6076, Validation Accuracy:0.2381\n",
    "Epoch #26: Loss:1.5981, Accuracy:0.2480, Validation Loss:1.6075, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.5987, Accuracy:0.2456, Validation Loss:1.6074, Validation Accuracy:0.2365\n",
    "Epoch #28: Loss:1.5980, Accuracy:0.2472, Validation Loss:1.6080, Validation Accuracy:0.2348\n",
    "Epoch #29: Loss:1.5976, Accuracy:0.2456, Validation Loss:1.6075, Validation Accuracy:0.2414\n",
    "Epoch #30: Loss:1.5972, Accuracy:0.2476, Validation Loss:1.6076, Validation Accuracy:0.2348\n",
    "Epoch #31: Loss:1.5973, Accuracy:0.2480, Validation Loss:1.6093, Validation Accuracy:0.2365\n",
    "Epoch #32: Loss:1.5965, Accuracy:0.2472, Validation Loss:1.6079, Validation Accuracy:0.2315\n",
    "Epoch #33: Loss:1.5969, Accuracy:0.2439, Validation Loss:1.6087, Validation Accuracy:0.2282\n",
    "Epoch #34: Loss:1.5964, Accuracy:0.2444, Validation Loss:1.6086, Validation Accuracy:0.2299\n",
    "Epoch #35: Loss:1.5959, Accuracy:0.2468, Validation Loss:1.6086, Validation Accuracy:0.2348\n",
    "Epoch #36: Loss:1.5967, Accuracy:0.2435, Validation Loss:1.6080, Validation Accuracy:0.2332\n",
    "Epoch #37: Loss:1.5964, Accuracy:0.2439, Validation Loss:1.6086, Validation Accuracy:0.2250\n",
    "Epoch #38: Loss:1.5964, Accuracy:0.2464, Validation Loss:1.6079, Validation Accuracy:0.2299\n",
    "Epoch #39: Loss:1.5961, Accuracy:0.2472, Validation Loss:1.6080, Validation Accuracy:0.2250\n",
    "Epoch #40: Loss:1.5961, Accuracy:0.2485, Validation Loss:1.6087, Validation Accuracy:0.2299\n",
    "Epoch #41: Loss:1.5957, Accuracy:0.2489, Validation Loss:1.6086, Validation Accuracy:0.2315\n",
    "Epoch #42: Loss:1.5958, Accuracy:0.2468, Validation Loss:1.6092, Validation Accuracy:0.2299\n",
    "Epoch #43: Loss:1.5958, Accuracy:0.2456, Validation Loss:1.6095, Validation Accuracy:0.2315\n",
    "Epoch #44: Loss:1.5956, Accuracy:0.2476, Validation Loss:1.6100, Validation Accuracy:0.2315\n",
    "Epoch #45: Loss:1.5958, Accuracy:0.2476, Validation Loss:1.6096, Validation Accuracy:0.2332\n",
    "Epoch #46: Loss:1.5959, Accuracy:0.2509, Validation Loss:1.6102, Validation Accuracy:0.2365\n",
    "Epoch #47: Loss:1.5956, Accuracy:0.2493, Validation Loss:1.6094, Validation Accuracy:0.2365\n",
    "Epoch #48: Loss:1.5955, Accuracy:0.2505, Validation Loss:1.6094, Validation Accuracy:0.2315\n",
    "Epoch #49: Loss:1.5956, Accuracy:0.2501, Validation Loss:1.6094, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.5955, Accuracy:0.2513, Validation Loss:1.6089, Validation Accuracy:0.2332\n",
    "Epoch #51: Loss:1.5961, Accuracy:0.2522, Validation Loss:1.6087, Validation Accuracy:0.2332\n",
    "Epoch #52: Loss:1.5952, Accuracy:0.2546, Validation Loss:1.6093, Validation Accuracy:0.2381\n",
    "Epoch #53: Loss:1.5949, Accuracy:0.2534, Validation Loss:1.6084, Validation Accuracy:0.2332\n",
    "Epoch #54: Loss:1.5950, Accuracy:0.2517, Validation Loss:1.6084, Validation Accuracy:0.2332\n",
    "Epoch #55: Loss:1.5943, Accuracy:0.2534, Validation Loss:1.6096, Validation Accuracy:0.2381\n",
    "Epoch #56: Loss:1.5949, Accuracy:0.2526, Validation Loss:1.6092, Validation Accuracy:0.2332\n",
    "Epoch #57: Loss:1.5945, Accuracy:0.2522, Validation Loss:1.6091, Validation Accuracy:0.2332\n",
    "Epoch #58: Loss:1.5945, Accuracy:0.2538, Validation Loss:1.6097, Validation Accuracy:0.2381\n",
    "Epoch #59: Loss:1.5947, Accuracy:0.2530, Validation Loss:1.6092, Validation Accuracy:0.2332\n",
    "Epoch #60: Loss:1.5939, Accuracy:0.2538, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #61: Loss:1.5940, Accuracy:0.2546, Validation Loss:1.6098, Validation Accuracy:0.2315\n",
    "Epoch #62: Loss:1.5936, Accuracy:0.2546, Validation Loss:1.6102, Validation Accuracy:0.2266\n",
    "Epoch #63: Loss:1.5937, Accuracy:0.2559, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #64: Loss:1.5947, Accuracy:0.2509, Validation Loss:1.6093, Validation Accuracy:0.2299\n",
    "Epoch #65: Loss:1.5942, Accuracy:0.2526, Validation Loss:1.6102, Validation Accuracy:0.2348\n",
    "Epoch #66: Loss:1.5940, Accuracy:0.2542, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #67: Loss:1.5939, Accuracy:0.2542, Validation Loss:1.6100, Validation Accuracy:0.2315\n",
    "Epoch #68: Loss:1.5934, Accuracy:0.2542, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #69: Loss:1.5944, Accuracy:0.2505, Validation Loss:1.6103, Validation Accuracy:0.2282\n",
    "Epoch #70: Loss:1.5934, Accuracy:0.2546, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #71: Loss:1.5939, Accuracy:0.2534, Validation Loss:1.6100, Validation Accuracy:0.2299\n",
    "Epoch #72: Loss:1.5933, Accuracy:0.2546, Validation Loss:1.6104, Validation Accuracy:0.2282\n",
    "Epoch #73: Loss:1.5933, Accuracy:0.2550, Validation Loss:1.6101, Validation Accuracy:0.2299\n",
    "Epoch #74: Loss:1.5930, Accuracy:0.2546, Validation Loss:1.6097, Validation Accuracy:0.2315\n",
    "Epoch #75: Loss:1.5933, Accuracy:0.2522, Validation Loss:1.6098, Validation Accuracy:0.2315\n",
    "Epoch #76: Loss:1.5934, Accuracy:0.2546, Validation Loss:1.6107, Validation Accuracy:0.2282\n",
    "Epoch #77: Loss:1.5930, Accuracy:0.2559, Validation Loss:1.6098, Validation Accuracy:0.2332\n",
    "Epoch #78: Loss:1.5931, Accuracy:0.2546, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #79: Loss:1.5931, Accuracy:0.2559, Validation Loss:1.6102, Validation Accuracy:0.2299\n",
    "Epoch #80: Loss:1.5929, Accuracy:0.2526, Validation Loss:1.6095, Validation Accuracy:0.2315\n",
    "Epoch #81: Loss:1.5927, Accuracy:0.2546, Validation Loss:1.6096, Validation Accuracy:0.2299\n",
    "Epoch #82: Loss:1.5926, Accuracy:0.2550, Validation Loss:1.6101, Validation Accuracy:0.2282\n",
    "Epoch #83: Loss:1.5928, Accuracy:0.2550, Validation Loss:1.6101, Validation Accuracy:0.2282\n",
    "Epoch #84: Loss:1.5926, Accuracy:0.2542, Validation Loss:1.6097, Validation Accuracy:0.2332\n",
    "Epoch #85: Loss:1.5935, Accuracy:0.2509, Validation Loss:1.6103, Validation Accuracy:0.2315\n",
    "Epoch #86: Loss:1.5934, Accuracy:0.2526, Validation Loss:1.6093, Validation Accuracy:0.2365\n",
    "Epoch #87: Loss:1.5940, Accuracy:0.2526, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #88: Loss:1.5924, Accuracy:0.2542, Validation Loss:1.6092, Validation Accuracy:0.2332\n",
    "Epoch #89: Loss:1.5934, Accuracy:0.2505, Validation Loss:1.6091, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:1.5924, Accuracy:0.2559, Validation Loss:1.6109, Validation Accuracy:0.2250\n",
    "Epoch #91: Loss:1.5924, Accuracy:0.2509, Validation Loss:1.6091, Validation Accuracy:0.2299\n",
    "Epoch #92: Loss:1.5923, Accuracy:0.2530, Validation Loss:1.6091, Validation Accuracy:0.2365\n",
    "Epoch #93: Loss:1.5926, Accuracy:0.2513, Validation Loss:1.6096, Validation Accuracy:0.2315\n",
    "Epoch #94: Loss:1.5920, Accuracy:0.2550, Validation Loss:1.6100, Validation Accuracy:0.2315\n",
    "Epoch #95: Loss:1.5921, Accuracy:0.2550, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #96: Loss:1.5923, Accuracy:0.2497, Validation Loss:1.6094, Validation Accuracy:0.2365\n",
    "Epoch #97: Loss:1.5917, Accuracy:0.2542, Validation Loss:1.6102, Validation Accuracy:0.2299\n",
    "Epoch #98: Loss:1.5920, Accuracy:0.2567, Validation Loss:1.6099, Validation Accuracy:0.2299\n",
    "Epoch #99: Loss:1.5917, Accuracy:0.2554, Validation Loss:1.6099, Validation Accuracy:0.2315\n",
    "Epoch #100: Loss:1.5918, Accuracy:0.2550, Validation Loss:1.6099, Validation Accuracy:0.2266\n",
    "Epoch #101: Loss:1.5920, Accuracy:0.2485, Validation Loss:1.6096, Validation Accuracy:0.2282\n",
    "Epoch #102: Loss:1.5915, Accuracy:0.2517, Validation Loss:1.6098, Validation Accuracy:0.2315\n",
    "Epoch #103: Loss:1.5918, Accuracy:0.2546, Validation Loss:1.6101, Validation Accuracy:0.2282\n",
    "Epoch #104: Loss:1.5919, Accuracy:0.2550, Validation Loss:1.6101, Validation Accuracy:0.2299\n",
    "Epoch #105: Loss:1.5913, Accuracy:0.2534, Validation Loss:1.6097, Validation Accuracy:0.2250\n",
    "Epoch #106: Loss:1.5926, Accuracy:0.2439, Validation Loss:1.6099, Validation Accuracy:0.2315\n",
    "Epoch #107: Loss:1.5917, Accuracy:0.2505, Validation Loss:1.6112, Validation Accuracy:0.2217\n",
    "Epoch #108: Loss:1.5920, Accuracy:0.2546, Validation Loss:1.6101, Validation Accuracy:0.2200\n",
    "Epoch #109: Loss:1.5918, Accuracy:0.2587, Validation Loss:1.6098, Validation Accuracy:0.2282\n",
    "Epoch #110: Loss:1.5910, Accuracy:0.2559, Validation Loss:1.6106, Validation Accuracy:0.2282\n",
    "Epoch #111: Loss:1.5923, Accuracy:0.2542, Validation Loss:1.6104, Validation Accuracy:0.2299\n",
    "Epoch #112: Loss:1.5912, Accuracy:0.2448, Validation Loss:1.6099, Validation Accuracy:0.2250\n",
    "Epoch #113: Loss:1.5920, Accuracy:0.2485, Validation Loss:1.6095, Validation Accuracy:0.2299\n",
    "Epoch #114: Loss:1.5918, Accuracy:0.2534, Validation Loss:1.6114, Validation Accuracy:0.2348\n",
    "Epoch #115: Loss:1.5912, Accuracy:0.2554, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #116: Loss:1.5923, Accuracy:0.2493, Validation Loss:1.6100, Validation Accuracy:0.2200\n",
    "Epoch #117: Loss:1.5915, Accuracy:0.2542, Validation Loss:1.6103, Validation Accuracy:0.2299\n",
    "Epoch #118: Loss:1.5917, Accuracy:0.2505, Validation Loss:1.6100, Validation Accuracy:0.2266\n",
    "Epoch #119: Loss:1.5919, Accuracy:0.2460, Validation Loss:1.6097, Validation Accuracy:0.2365\n",
    "Epoch #120: Loss:1.5912, Accuracy:0.2509, Validation Loss:1.6100, Validation Accuracy:0.2200\n",
    "Epoch #121: Loss:1.5922, Accuracy:0.2571, Validation Loss:1.6099, Validation Accuracy:0.2184\n",
    "Epoch #122: Loss:1.5912, Accuracy:0.2480, Validation Loss:1.6093, Validation Accuracy:0.2299\n",
    "Epoch #123: Loss:1.5915, Accuracy:0.2513, Validation Loss:1.6097, Validation Accuracy:0.2299\n",
    "Epoch #124: Loss:1.5908, Accuracy:0.2554, Validation Loss:1.6098, Validation Accuracy:0.2266\n",
    "Epoch #125: Loss:1.5909, Accuracy:0.2505, Validation Loss:1.6102, Validation Accuracy:0.2184\n",
    "Epoch #126: Loss:1.5908, Accuracy:0.2485, Validation Loss:1.6103, Validation Accuracy:0.2151\n",
    "Epoch #127: Loss:1.5910, Accuracy:0.2517, Validation Loss:1.6102, Validation Accuracy:0.2315\n",
    "Epoch #128: Loss:1.5906, Accuracy:0.2526, Validation Loss:1.6104, Validation Accuracy:0.2102\n",
    "Epoch #129: Loss:1.5909, Accuracy:0.2517, Validation Loss:1.6107, Validation Accuracy:0.2233\n",
    "Epoch #130: Loss:1.5907, Accuracy:0.2476, Validation Loss:1.6106, Validation Accuracy:0.2085\n",
    "Epoch #131: Loss:1.5909, Accuracy:0.2464, Validation Loss:1.6105, Validation Accuracy:0.2217\n",
    "Epoch #132: Loss:1.5907, Accuracy:0.2567, Validation Loss:1.6114, Validation Accuracy:0.2266\n",
    "Epoch #133: Loss:1.5904, Accuracy:0.2583, Validation Loss:1.6114, Validation Accuracy:0.2151\n",
    "Epoch #134: Loss:1.5914, Accuracy:0.2538, Validation Loss:1.6117, Validation Accuracy:0.2282\n",
    "Epoch #135: Loss:1.5918, Accuracy:0.2575, Validation Loss:1.6113, Validation Accuracy:0.2250\n",
    "Epoch #136: Loss:1.5919, Accuracy:0.2439, Validation Loss:1.6107, Validation Accuracy:0.2266\n",
    "Epoch #137: Loss:1.5909, Accuracy:0.2530, Validation Loss:1.6103, Validation Accuracy:0.2151\n",
    "Epoch #138: Loss:1.5906, Accuracy:0.2641, Validation Loss:1.6109, Validation Accuracy:0.2217\n",
    "Epoch #139: Loss:1.5912, Accuracy:0.2587, Validation Loss:1.6104, Validation Accuracy:0.2184\n",
    "Epoch #140: Loss:1.5908, Accuracy:0.2571, Validation Loss:1.6104, Validation Accuracy:0.2332\n",
    "Epoch #141: Loss:1.5910, Accuracy:0.2538, Validation Loss:1.6104, Validation Accuracy:0.2250\n",
    "Epoch #142: Loss:1.5922, Accuracy:0.2554, Validation Loss:1.6130, Validation Accuracy:0.2069\n",
    "Epoch #143: Loss:1.5911, Accuracy:0.2534, Validation Loss:1.6106, Validation Accuracy:0.2315\n",
    "Epoch #144: Loss:1.5907, Accuracy:0.2550, Validation Loss:1.6104, Validation Accuracy:0.2282\n",
    "Epoch #145: Loss:1.5907, Accuracy:0.2526, Validation Loss:1.6105, Validation Accuracy:0.2053\n",
    "Epoch #146: Loss:1.5902, Accuracy:0.2595, Validation Loss:1.6103, Validation Accuracy:0.2200\n",
    "Epoch #147: Loss:1.5899, Accuracy:0.2538, Validation Loss:1.6099, Validation Accuracy:0.2332\n",
    "Epoch #148: Loss:1.5902, Accuracy:0.2550, Validation Loss:1.6101, Validation Accuracy:0.2299\n",
    "Epoch #149: Loss:1.5903, Accuracy:0.2534, Validation Loss:1.6109, Validation Accuracy:0.2299\n",
    "Epoch #150: Loss:1.5905, Accuracy:0.2526, Validation Loss:1.6102, Validation Accuracy:0.2332\n",
    "Epoch #151: Loss:1.5901, Accuracy:0.2456, Validation Loss:1.6108, Validation Accuracy:0.2053\n",
    "Epoch #152: Loss:1.5898, Accuracy:0.2534, Validation Loss:1.6111, Validation Accuracy:0.2299\n",
    "Epoch #153: Loss:1.5896, Accuracy:0.2550, Validation Loss:1.6108, Validation Accuracy:0.2233\n",
    "Epoch #154: Loss:1.5895, Accuracy:0.2534, Validation Loss:1.6108, Validation Accuracy:0.2233\n",
    "Epoch #155: Loss:1.5897, Accuracy:0.2513, Validation Loss:1.6110, Validation Accuracy:0.2348\n",
    "Epoch #156: Loss:1.5898, Accuracy:0.2559, Validation Loss:1.6111, Validation Accuracy:0.2299\n",
    "Epoch #157: Loss:1.5890, Accuracy:0.2604, Validation Loss:1.6114, Validation Accuracy:0.2053\n",
    "Epoch #158: Loss:1.5894, Accuracy:0.2559, Validation Loss:1.6115, Validation Accuracy:0.2118\n",
    "Epoch #159: Loss:1.5899, Accuracy:0.2522, Validation Loss:1.6109, Validation Accuracy:0.2250\n",
    "Epoch #160: Loss:1.5897, Accuracy:0.2489, Validation Loss:1.6115, Validation Accuracy:0.2085\n",
    "Epoch #161: Loss:1.5893, Accuracy:0.2526, Validation Loss:1.6111, Validation Accuracy:0.2299\n",
    "Epoch #162: Loss:1.5894, Accuracy:0.2563, Validation Loss:1.6115, Validation Accuracy:0.2348\n",
    "Epoch #163: Loss:1.5892, Accuracy:0.2546, Validation Loss:1.6116, Validation Accuracy:0.2085\n",
    "Epoch #164: Loss:1.5902, Accuracy:0.2559, Validation Loss:1.6124, Validation Accuracy:0.2085\n",
    "Epoch #165: Loss:1.5892, Accuracy:0.2505, Validation Loss:1.6108, Validation Accuracy:0.2266\n",
    "Epoch #166: Loss:1.5888, Accuracy:0.2542, Validation Loss:1.6110, Validation Accuracy:0.2299\n",
    "Epoch #167: Loss:1.5890, Accuracy:0.2575, Validation Loss:1.6114, Validation Accuracy:0.2266\n",
    "Epoch #168: Loss:1.5894, Accuracy:0.2538, Validation Loss:1.6118, Validation Accuracy:0.2053\n",
    "Epoch #169: Loss:1.5885, Accuracy:0.2587, Validation Loss:1.6116, Validation Accuracy:0.2299\n",
    "Epoch #170: Loss:1.5893, Accuracy:0.2559, Validation Loss:1.6110, Validation Accuracy:0.2348\n",
    "Epoch #171: Loss:1.5885, Accuracy:0.2587, Validation Loss:1.6123, Validation Accuracy:0.2102\n",
    "Epoch #172: Loss:1.5891, Accuracy:0.2554, Validation Loss:1.6120, Validation Accuracy:0.2085\n",
    "Epoch #173: Loss:1.5897, Accuracy:0.2554, Validation Loss:1.6121, Validation Accuracy:0.2332\n",
    "Epoch #174: Loss:1.5886, Accuracy:0.2604, Validation Loss:1.6117, Validation Accuracy:0.2085\n",
    "Epoch #175: Loss:1.5887, Accuracy:0.2542, Validation Loss:1.6117, Validation Accuracy:0.2167\n",
    "Epoch #176: Loss:1.5885, Accuracy:0.2637, Validation Loss:1.6122, Validation Accuracy:0.2184\n",
    "Epoch #177: Loss:1.5881, Accuracy:0.2571, Validation Loss:1.6121, Validation Accuracy:0.2266\n",
    "Epoch #178: Loss:1.5886, Accuracy:0.2600, Validation Loss:1.6120, Validation Accuracy:0.2266\n",
    "Epoch #179: Loss:1.5881, Accuracy:0.2645, Validation Loss:1.6128, Validation Accuracy:0.2069\n",
    "Epoch #180: Loss:1.5884, Accuracy:0.2559, Validation Loss:1.6124, Validation Accuracy:0.2118\n",
    "Epoch #181: Loss:1.5881, Accuracy:0.2641, Validation Loss:1.6119, Validation Accuracy:0.2167\n",
    "Epoch #182: Loss:1.5888, Accuracy:0.2583, Validation Loss:1.6118, Validation Accuracy:0.2200\n",
    "Epoch #183: Loss:1.5892, Accuracy:0.2554, Validation Loss:1.6137, Validation Accuracy:0.2151\n",
    "Epoch #184: Loss:1.5886, Accuracy:0.2571, Validation Loss:1.6120, Validation Accuracy:0.2282\n",
    "Epoch #185: Loss:1.5885, Accuracy:0.2571, Validation Loss:1.6122, Validation Accuracy:0.2085\n",
    "Epoch #186: Loss:1.5886, Accuracy:0.2595, Validation Loss:1.6139, Validation Accuracy:0.2102\n",
    "Epoch #187: Loss:1.5880, Accuracy:0.2645, Validation Loss:1.6129, Validation Accuracy:0.2282\n",
    "Epoch #188: Loss:1.5886, Accuracy:0.2575, Validation Loss:1.6123, Validation Accuracy:0.2250\n",
    "Epoch #189: Loss:1.5894, Accuracy:0.2608, Validation Loss:1.6138, Validation Accuracy:0.2102\n",
    "Epoch #190: Loss:1.5882, Accuracy:0.2657, Validation Loss:1.6123, Validation Accuracy:0.2348\n",
    "Epoch #191: Loss:1.5886, Accuracy:0.2559, Validation Loss:1.6111, Validation Accuracy:0.2282\n",
    "Epoch #192: Loss:1.5880, Accuracy:0.2600, Validation Loss:1.6136, Validation Accuracy:0.2085\n",
    "Epoch #193: Loss:1.5887, Accuracy:0.2559, Validation Loss:1.6120, Validation Accuracy:0.2250\n",
    "Epoch #194: Loss:1.5875, Accuracy:0.2604, Validation Loss:1.6118, Validation Accuracy:0.2184\n",
    "Epoch #195: Loss:1.5871, Accuracy:0.2682, Validation Loss:1.6131, Validation Accuracy:0.2184\n",
    "Epoch #196: Loss:1.5873, Accuracy:0.2632, Validation Loss:1.6121, Validation Accuracy:0.2151\n",
    "Epoch #197: Loss:1.5873, Accuracy:0.2567, Validation Loss:1.6116, Validation Accuracy:0.2217\n",
    "Epoch #198: Loss:1.5871, Accuracy:0.2542, Validation Loss:1.6139, Validation Accuracy:0.2200\n",
    "Epoch #199: Loss:1.5869, Accuracy:0.2682, Validation Loss:1.6135, Validation Accuracy:0.2184\n",
    "Epoch #200: Loss:1.5873, Accuracy:0.2624, Validation Loss:1.6130, Validation Accuracy:0.2135\n",
    "Epoch #201: Loss:1.5871, Accuracy:0.2678, Validation Loss:1.6131, Validation Accuracy:0.2184\n",
    "Epoch #202: Loss:1.5873, Accuracy:0.2669, Validation Loss:1.6128, Validation Accuracy:0.2184\n",
    "Epoch #203: Loss:1.5868, Accuracy:0.2600, Validation Loss:1.6134, Validation Accuracy:0.2020\n",
    "Epoch #204: Loss:1.5866, Accuracy:0.2616, Validation Loss:1.6132, Validation Accuracy:0.2233\n",
    "Epoch #205: Loss:1.5871, Accuracy:0.2620, Validation Loss:1.6145, Validation Accuracy:0.2184\n",
    "Epoch #206: Loss:1.5870, Accuracy:0.2682, Validation Loss:1.6128, Validation Accuracy:0.2266\n",
    "Epoch #207: Loss:1.5869, Accuracy:0.2686, Validation Loss:1.6131, Validation Accuracy:0.2184\n",
    "Epoch #208: Loss:1.5867, Accuracy:0.2682, Validation Loss:1.6142, Validation Accuracy:0.2184\n",
    "Epoch #209: Loss:1.5866, Accuracy:0.2682, Validation Loss:1.6146, Validation Accuracy:0.2167\n",
    "Epoch #210: Loss:1.5861, Accuracy:0.2678, Validation Loss:1.6129, Validation Accuracy:0.2184\n",
    "Epoch #211: Loss:1.5859, Accuracy:0.2682, Validation Loss:1.6133, Validation Accuracy:0.2184\n",
    "Epoch #212: Loss:1.5865, Accuracy:0.2682, Validation Loss:1.6140, Validation Accuracy:0.2184\n",
    "Epoch #213: Loss:1.5863, Accuracy:0.2706, Validation Loss:1.6130, Validation Accuracy:0.2266\n",
    "Epoch #214: Loss:1.5867, Accuracy:0.2608, Validation Loss:1.6148, Validation Accuracy:0.2167\n",
    "Epoch #215: Loss:1.5873, Accuracy:0.2624, Validation Loss:1.6150, Validation Accuracy:0.2102\n",
    "Epoch #216: Loss:1.5863, Accuracy:0.2719, Validation Loss:1.6129, Validation Accuracy:0.2282\n",
    "Epoch #217: Loss:1.5859, Accuracy:0.2645, Validation Loss:1.6138, Validation Accuracy:0.2184\n",
    "Epoch #218: Loss:1.5860, Accuracy:0.2674, Validation Loss:1.6154, Validation Accuracy:0.2135\n",
    "Epoch #219: Loss:1.5854, Accuracy:0.2678, Validation Loss:1.6132, Validation Accuracy:0.2184\n",
    "Epoch #220: Loss:1.5854, Accuracy:0.2632, Validation Loss:1.6138, Validation Accuracy:0.2167\n",
    "Epoch #221: Loss:1.5852, Accuracy:0.2649, Validation Loss:1.6147, Validation Accuracy:0.2200\n",
    "Epoch #222: Loss:1.5852, Accuracy:0.2620, Validation Loss:1.6143, Validation Accuracy:0.2184\n",
    "Epoch #223: Loss:1.5845, Accuracy:0.2661, Validation Loss:1.6147, Validation Accuracy:0.2085\n",
    "Epoch #224: Loss:1.5859, Accuracy:0.2616, Validation Loss:1.6143, Validation Accuracy:0.2184\n",
    "Epoch #225: Loss:1.5867, Accuracy:0.2550, Validation Loss:1.6141, Validation Accuracy:0.2266\n",
    "Epoch #226: Loss:1.5842, Accuracy:0.2669, Validation Loss:1.6163, Validation Accuracy:0.2085\n",
    "Epoch #227: Loss:1.5873, Accuracy:0.2563, Validation Loss:1.6135, Validation Accuracy:0.2200\n",
    "Epoch #228: Loss:1.5849, Accuracy:0.2600, Validation Loss:1.6136, Validation Accuracy:0.2282\n",
    "Epoch #229: Loss:1.5856, Accuracy:0.2624, Validation Loss:1.6148, Validation Accuracy:0.2085\n",
    "Epoch #230: Loss:1.5845, Accuracy:0.2641, Validation Loss:1.6149, Validation Accuracy:0.2200\n",
    "Epoch #231: Loss:1.5850, Accuracy:0.2661, Validation Loss:1.6142, Validation Accuracy:0.2184\n",
    "Epoch #232: Loss:1.5860, Accuracy:0.2686, Validation Loss:1.6139, Validation Accuracy:0.2167\n",
    "Epoch #233: Loss:1.5839, Accuracy:0.2665, Validation Loss:1.6152, Validation Accuracy:0.2184\n",
    "Epoch #234: Loss:1.5851, Accuracy:0.2657, Validation Loss:1.6149, Validation Accuracy:0.2184\n",
    "Epoch #235: Loss:1.5835, Accuracy:0.2657, Validation Loss:1.6134, Validation Accuracy:0.2184\n",
    "Epoch #236: Loss:1.5842, Accuracy:0.2665, Validation Loss:1.6146, Validation Accuracy:0.2200\n",
    "Epoch #237: Loss:1.5856, Accuracy:0.2632, Validation Loss:1.6152, Validation Accuracy:0.2085\n",
    "Epoch #238: Loss:1.5841, Accuracy:0.2637, Validation Loss:1.6135, Validation Accuracy:0.2282\n",
    "Epoch #239: Loss:1.5841, Accuracy:0.2735, Validation Loss:1.6154, Validation Accuracy:0.2184\n",
    "Epoch #240: Loss:1.5838, Accuracy:0.2657, Validation Loss:1.6145, Validation Accuracy:0.2102\n",
    "Epoch #241: Loss:1.5843, Accuracy:0.2653, Validation Loss:1.6146, Validation Accuracy:0.2200\n",
    "Epoch #242: Loss:1.5834, Accuracy:0.2665, Validation Loss:1.6143, Validation Accuracy:0.2167\n",
    "Epoch #243: Loss:1.5843, Accuracy:0.2674, Validation Loss:1.6150, Validation Accuracy:0.2200\n",
    "Epoch #244: Loss:1.5840, Accuracy:0.2682, Validation Loss:1.6146, Validation Accuracy:0.2020\n",
    "Epoch #245: Loss:1.5836, Accuracy:0.2645, Validation Loss:1.6148, Validation Accuracy:0.2200\n",
    "Epoch #246: Loss:1.5843, Accuracy:0.2641, Validation Loss:1.6133, Validation Accuracy:0.2266\n",
    "Epoch #247: Loss:1.5864, Accuracy:0.2641, Validation Loss:1.6163, Validation Accuracy:0.2036\n",
    "Epoch #248: Loss:1.5836, Accuracy:0.2661, Validation Loss:1.6152, Validation Accuracy:0.2167\n",
    "Epoch #249: Loss:1.5842, Accuracy:0.2616, Validation Loss:1.6149, Validation Accuracy:0.2184\n",
    "Epoch #250: Loss:1.5836, Accuracy:0.2637, Validation Loss:1.6147, Validation Accuracy:0.2053\n",
    "Epoch #251: Loss:1.5834, Accuracy:0.2674, Validation Loss:1.6134, Validation Accuracy:0.2184\n",
    "Epoch #252: Loss:1.5830, Accuracy:0.2620, Validation Loss:1.6147, Validation Accuracy:0.2184\n",
    "Epoch #253: Loss:1.5833, Accuracy:0.2628, Validation Loss:1.6135, Validation Accuracy:0.2184\n",
    "Epoch #254: Loss:1.5828, Accuracy:0.2624, Validation Loss:1.6157, Validation Accuracy:0.2200\n",
    "Epoch #255: Loss:1.5829, Accuracy:0.2649, Validation Loss:1.6137, Validation Accuracy:0.2167\n",
    "Epoch #256: Loss:1.5834, Accuracy:0.2674, Validation Loss:1.6153, Validation Accuracy:0.2102\n",
    "Epoch #257: Loss:1.5837, Accuracy:0.2624, Validation Loss:1.6151, Validation Accuracy:0.2200\n",
    "Epoch #258: Loss:1.5825, Accuracy:0.2678, Validation Loss:1.6139, Validation Accuracy:0.2233\n",
    "Epoch #259: Loss:1.5824, Accuracy:0.2645, Validation Loss:1.6155, Validation Accuracy:0.2102\n",
    "Epoch #260: Loss:1.5820, Accuracy:0.2637, Validation Loss:1.6152, Validation Accuracy:0.2085\n",
    "Epoch #261: Loss:1.5819, Accuracy:0.2653, Validation Loss:1.6150, Validation Accuracy:0.2184\n",
    "Epoch #262: Loss:1.5828, Accuracy:0.2669, Validation Loss:1.6140, Validation Accuracy:0.2282\n",
    "Epoch #263: Loss:1.5825, Accuracy:0.2686, Validation Loss:1.6174, Validation Accuracy:0.2003\n",
    "Epoch #264: Loss:1.5821, Accuracy:0.2657, Validation Loss:1.6139, Validation Accuracy:0.2184\n",
    "Epoch #265: Loss:1.5829, Accuracy:0.2674, Validation Loss:1.6139, Validation Accuracy:0.2282\n",
    "Epoch #266: Loss:1.5815, Accuracy:0.2694, Validation Loss:1.6173, Validation Accuracy:0.2003\n",
    "Epoch #267: Loss:1.5818, Accuracy:0.2665, Validation Loss:1.6132, Validation Accuracy:0.2266\n",
    "Epoch #268: Loss:1.5819, Accuracy:0.2702, Validation Loss:1.6142, Validation Accuracy:0.2266\n",
    "Epoch #269: Loss:1.5812, Accuracy:0.2686, Validation Loss:1.6160, Validation Accuracy:0.2085\n",
    "Epoch #270: Loss:1.5808, Accuracy:0.2645, Validation Loss:1.6160, Validation Accuracy:0.2184\n",
    "Epoch #271: Loss:1.5812, Accuracy:0.2665, Validation Loss:1.6154, Validation Accuracy:0.2102\n",
    "Epoch #272: Loss:1.5815, Accuracy:0.2669, Validation Loss:1.6136, Validation Accuracy:0.2167\n",
    "Epoch #273: Loss:1.5825, Accuracy:0.2661, Validation Loss:1.6170, Validation Accuracy:0.2102\n",
    "Epoch #274: Loss:1.5818, Accuracy:0.2632, Validation Loss:1.6141, Validation Accuracy:0.2184\n",
    "Epoch #275: Loss:1.5823, Accuracy:0.2669, Validation Loss:1.6153, Validation Accuracy:0.2184\n",
    "Epoch #276: Loss:1.5803, Accuracy:0.2669, Validation Loss:1.6162, Validation Accuracy:0.2118\n",
    "Epoch #277: Loss:1.5809, Accuracy:0.2702, Validation Loss:1.6152, Validation Accuracy:0.2266\n",
    "Epoch #278: Loss:1.5804, Accuracy:0.2669, Validation Loss:1.6147, Validation Accuracy:0.2102\n",
    "Epoch #279: Loss:1.5796, Accuracy:0.2674, Validation Loss:1.6145, Validation Accuracy:0.2184\n",
    "Epoch #280: Loss:1.5803, Accuracy:0.2665, Validation Loss:1.6149, Validation Accuracy:0.2151\n",
    "Epoch #281: Loss:1.5799, Accuracy:0.2669, Validation Loss:1.6148, Validation Accuracy:0.2184\n",
    "Epoch #282: Loss:1.5810, Accuracy:0.2690, Validation Loss:1.6154, Validation Accuracy:0.2085\n",
    "Epoch #283: Loss:1.5803, Accuracy:0.2653, Validation Loss:1.6143, Validation Accuracy:0.2167\n",
    "Epoch #284: Loss:1.5797, Accuracy:0.2694, Validation Loss:1.6152, Validation Accuracy:0.2167\n",
    "Epoch #285: Loss:1.5805, Accuracy:0.2686, Validation Loss:1.6144, Validation Accuracy:0.2266\n",
    "Epoch #286: Loss:1.5813, Accuracy:0.2649, Validation Loss:1.6156, Validation Accuracy:0.2053\n",
    "Epoch #287: Loss:1.5803, Accuracy:0.2702, Validation Loss:1.6142, Validation Accuracy:0.2184\n",
    "Epoch #288: Loss:1.5803, Accuracy:0.2674, Validation Loss:1.6165, Validation Accuracy:0.2184\n",
    "Epoch #289: Loss:1.5794, Accuracy:0.2616, Validation Loss:1.6149, Validation Accuracy:0.2167\n",
    "Epoch #290: Loss:1.5806, Accuracy:0.2674, Validation Loss:1.6165, Validation Accuracy:0.2036\n",
    "Epoch #291: Loss:1.5795, Accuracy:0.2661, Validation Loss:1.6144, Validation Accuracy:0.2184\n",
    "Epoch #292: Loss:1.5805, Accuracy:0.2727, Validation Loss:1.6142, Validation Accuracy:0.2233\n",
    "Epoch #293: Loss:1.5799, Accuracy:0.2686, Validation Loss:1.6190, Validation Accuracy:0.2053\n",
    "Epoch #294: Loss:1.5801, Accuracy:0.2669, Validation Loss:1.6139, Validation Accuracy:0.2282\n",
    "Epoch #295: Loss:1.5798, Accuracy:0.2690, Validation Loss:1.6153, Validation Accuracy:0.2167\n",
    "Epoch #296: Loss:1.5810, Accuracy:0.2678, Validation Loss:1.6155, Validation Accuracy:0.2102\n",
    "Epoch #297: Loss:1.5800, Accuracy:0.2686, Validation Loss:1.6163, Validation Accuracy:0.2200\n",
    "Epoch #298: Loss:1.5788, Accuracy:0.2678, Validation Loss:1.6148, Validation Accuracy:0.2184\n",
    "Epoch #299: Loss:1.5801, Accuracy:0.2669, Validation Loss:1.6154, Validation Accuracy:0.2053\n",
    "Epoch #300: Loss:1.5798, Accuracy:0.2674, Validation Loss:1.6145, Validation Accuracy:0.2135\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61453092, Accuracy:0.2135\n",
    "Labels: ['03', '01', '05', '02', '04']\n",
    "Confusion Matrix:\n",
    "      03  01  05  02  04\n",
    "t:03  22  20  46   0  27\n",
    "t:01  19  31  54   0  22\n",
    "t:05  28  35  54   0  25\n",
    "t:02  14  22  50   0  28\n",
    "t:04  19  26  44   0  23\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.22      0.19      0.20       115\n",
    "          01       0.23      0.25      0.24       126\n",
    "          05       0.22      0.38      0.28       142\n",
    "          02       0.00      0.00      0.00       114\n",
    "          04       0.18      0.21      0.19       112\n",
    "\n",
    "    accuracy                           0.21       609\n",
    "   macro avg       0.17      0.20      0.18       609\n",
    "weighted avg       0.17      0.21      0.19       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 03:01:17 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 25 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6060489147950472, 1.605477462456927, 1.6055419576187635, 1.6053511444570983, 1.60510525033979, 1.6052475852527837, 1.6050116315068086, 1.6051077517774108, 1.6047913198204855, 1.605005295993072, 1.6050823650923856, 1.6048946570488816, 1.6052073265727127, 1.6056380156421506, 1.6070406094364735, 1.6078975609762132, 1.6061983644864437, 1.607255406567616, 1.6067885136956652, 1.6071039166160797, 1.607388749302706, 1.6078972524805806, 1.6079399556361984, 1.6075206172877345, 1.6076327863780933, 1.607521975373204, 1.6074137595682505, 1.6079634702068635, 1.607540430498045, 1.6075940506015896, 1.6092779802767123, 1.6078577090562467, 1.6087383059249527, 1.6085972096923928, 1.608604742779912, 1.6079918384943495, 1.608554964778067, 1.607922311486869, 1.6080460648231318, 1.6087141853247957, 1.6085535501220152, 1.6092452719098045, 1.6094536973142075, 1.6099808231754646, 1.609559382906884, 1.6101527803245632, 1.6093507412031953, 1.6093765732102794, 1.6093690313141922, 1.6088519211864627, 1.608652749672312, 1.6092940216581222, 1.6083649038681256, 1.6084161446795284, 1.609566151214938, 1.6091789189230632, 1.6090686628579702, 1.6096865661038553, 1.609217278280086, 1.6093982203644877, 1.609814006315272, 1.610215284358496, 1.6096886727218753, 1.609313959558609, 1.6101557374587787, 1.6097460468414382, 1.6100312103387366, 1.6100403154620593, 1.6102521137650965, 1.6096741045245593, 1.610010174694907, 1.6104444404345233, 1.6100773793723195, 1.6097158731889647, 1.6097820734938573, 1.6106999165123124, 1.6097711567416764, 1.609983552265637, 1.6102119218343975, 1.6095365131234105, 1.6095978978819447, 1.6100928213796004, 1.610076812100528, 1.6097230204611968, 1.6102640871539688, 1.6092815499000361, 1.610028102871624, 1.6091586686120245, 1.6090695468467249, 1.610920918790382, 1.6090559344769306, 1.6091402044828693, 1.6096482263214287, 1.6100045096110829, 1.6094171767947318, 1.6094374400250038, 1.6101545228550977, 1.60992221076696, 1.609943313160162, 1.6098995355549703, 1.6096432600506811, 1.6097728340888062, 1.6100791610520462, 1.6100738698430053, 1.6097375792627069, 1.609855192635447, 1.6112062048246512, 1.6100567115351485, 1.6097956788167969, 1.6106498901088446, 1.610431067657784, 1.609922585424727, 1.6094941957830795, 1.6113870560829275, 1.6096916089112732, 1.6100273263474012, 1.6102565211811284, 1.6100043440100007, 1.6096512705625963, 1.6099963256682472, 1.6099128231822173, 1.6093041268475536, 1.6097274887542223, 1.6097604876081344, 1.610172991877902, 1.6103325440182865, 1.610155556393766, 1.6104071551355823, 1.6106832812376601, 1.6106396424163543, 1.6105455212992401, 1.6113895393161743, 1.6114383248860025, 1.6117129004843325, 1.6112867129847335, 1.6106818023769336, 1.6102725153877622, 1.6109076957593018, 1.6103528078357965, 1.6104491357928623, 1.610418613908326, 1.612988643458324, 1.6106056501516959, 1.6103923211152527, 1.6104694068529728, 1.6102950937055014, 1.6099316275178506, 1.6100909641419334, 1.6109492686777476, 1.6101907276363403, 1.6107965641225304, 1.6110592630304923, 1.6107602493320583, 1.61077362974289, 1.6110193377057906, 1.6110763622230693, 1.611351152554717, 1.6114841527343775, 1.6108905388216668, 1.6115187383050402, 1.6110666526362227, 1.6114566631505054, 1.611597112051176, 1.612368882387534, 1.6108097243191573, 1.611037975657358, 1.6114103535713233, 1.611822169598296, 1.6116257428339942, 1.6110084906391713, 1.6122618933225108, 1.6119900340908657, 1.6120919147735746, 1.6117218918792524, 1.611697519354045, 1.6121843419051523, 1.6121258731741819, 1.6120149754538324, 1.6128485929006817, 1.6123538408764868, 1.6119374453727835, 1.6118149007678229, 1.6136724363602637, 1.612045310988215, 1.6122162862755787, 1.6138701198136278, 1.6128937698937402, 1.612266861354972, 1.6137808835369416, 1.612299879979226, 1.6110929518889128, 1.613611375169801, 1.6120042172558788, 1.611842612914851, 1.6130571563059866, 1.6121420292627244, 1.6116376563246027, 1.6138777022291286, 1.613452579783297, 1.613002598970786, 1.6130926111844568, 1.6128265738291498, 1.6134118859599573, 1.6131799577296466, 1.614530032295703, 1.6128499891566135, 1.6131015009872236, 1.614171237193892, 1.6146121138618106, 1.6128792063943271, 1.6133027646341935, 1.6140043670907984, 1.6129531535413268, 1.614812789683663, 1.614995524409565, 1.612854983419033, 1.6137859410253064, 1.6154371451078768, 1.6132302117856656, 1.6138373842380318, 1.6147228497002513, 1.6143410906611602, 1.6147248772369034, 1.61429806785239, 1.6141082285268749, 1.6163060700364889, 1.6134707410934523, 1.6135933087964363, 1.6147852141673147, 1.6148833559064442, 1.6141786459827268, 1.6138801449429616, 1.6151516093017628, 1.6149306945221373, 1.613396333551955, 1.6145642986047053, 1.6152386271894859, 1.613522279438714, 1.6154324329153853, 1.6144848832943168, 1.6146079358600436, 1.6143337836602247, 1.6149786174395206, 1.6146174299305882, 1.6147555455394176, 1.6132836557178467, 1.6162782604079724, 1.6152009678200157, 1.6148959736910165, 1.614725921737345, 1.613356921668906, 1.6147397337680185, 1.6135185089800355, 1.6157287962135227, 1.6137286491190468, 1.6152531963655319, 1.6150667487302632, 1.6139341686746758, 1.6154846299458019, 1.6151608060341947, 1.6150190468100687, 1.6139862300531422, 1.6174400271649039, 1.6139106766147957, 1.6138783941910968, 1.6172990522948392, 1.6131865363598652, 1.614158395867434, 1.615996563375877, 1.6160253562363498, 1.6153841474764845, 1.6136069244939117, 1.6170450419627975, 1.6141258270673955, 1.615303197126279, 1.616205101921445, 1.6151874186761666, 1.614729867192912, 1.6144777186006944, 1.6148757819080197, 1.6147931908151787, 1.6153755542288468, 1.6143136296561982, 1.6151982109339171, 1.6144314021704036, 1.615614935486579, 1.6141751853899025, 1.616458651663243, 1.6148862738914678, 1.6164948846319038, 1.614419089162291, 1.6142477057446007, 1.6190216686142294, 1.6139184404868014, 1.6153108194739556, 1.6155326166763682, 1.616337961164014, 1.6147766373623376, 1.6153905935866884, 1.614530842488231], 'val_acc': [0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23481116572330737, 0.23481116572330737, 0.23481116572330737, 0.23481116572330737, 0.23481116572330737, 0.23481116572330737, 0.23316912950063964, 0.23481116572330737, 0.22988505725105016, 0.23809523807076985, 0.23481116572330737, 0.23316912969638562, 0.2364532019459751, 0.23809523807076985, 0.23809523807076985, 0.23809523807076985, 0.23316912950063964, 0.23645320184810212, 0.23481116552756143, 0.24137931032035934, 0.2348111656254344, 0.2364532019459751, 0.23152709327797194, 0.22824302093050947, 0.2298850570553042, 0.23481116572330737, 0.23316912969638562, 0.22495894877879294, 0.2298850571531772, 0.22495894877879294, 0.2298850571531772, 0.23152709327797194, 0.2298850571531772, 0.23152709327797194, 0.23152709327797194, 0.23316912940276668, 0.23645320165235617, 0.23645320165235617, 0.23152709327797194, 0.23645320165235617, 0.23316912940276668, 0.23316912940276668, 0.23809523777715091, 0.23316912940276668, 0.23316912940276668, 0.23809523777715091, 0.23316912940276668, 0.2331691293048937, 0.23809523777715091, 0.23316912940276668, 0.23316912940276668, 0.23152709327797194, 0.22660098490358768, 0.2298850571531772, 0.2298850571531772, 0.23481116552756143, 0.2298850571531772, 0.2315270933758449, 0.22824302102838243, 0.22824302102838243, 0.22988505734892314, 0.22988505725105016, 0.22824302102838243, 0.2298850571531772, 0.2315270933758449, 0.2315270934737179, 0.22824302102838243, 0.23316912940276668, 0.22824302102838243, 0.2298850571531772, 0.2315270933758449, 0.2298850571531772, 0.22824302102838243, 0.22824302102838243, 0.23316912950063964, 0.2315270934737179, 0.23645320184810212, 0.22824302102838243, 0.23316912940276668, 0.23316912969638562, 0.22495894868091998, 0.22988505725105016, 0.23645320184810212, 0.2315270934737179, 0.2315270934737179, 0.23316912969638562, 0.2364532019459751, 0.22988505734892314, 0.22988505734892314, 0.2315270934737179, 0.22660098509933366, 0.2282430213220014, 0.23152709357159088, 0.22824302102838243, 0.22988505734892314, 0.22495894907241187, 0.2315270934737179, 0.22167487652920345, 0.22003284069802764, 0.22824301904645458, 0.22824302102838243, 0.22988505734892314, 0.22495894907241187, 0.22988505744679613, 0.23481116572330737, 0.22988505744679613, 0.2200328405022817, 0.22988505734892314, 0.22660098509933366, 0.23645320184810212, 0.2200328404044087, 0.21839080418174098, 0.22988505744679613, 0.22988505725105016, 0.22660098509933366, 0.21839080418174098, 0.21510673203002448, 0.23152709357159088, 0.21018062355776726, 0.22331691275187118, 0.2085385875308455, 0.22167487672494943, 0.22660098480571472, 0.21510673212789747, 0.2282430211262554, 0.22495894877879294, 0.22660098509933366, 0.21510673212789747, 0.22167487643133046, 0.21839080418174098, 0.23316912950063964, 0.2249589489745389, 0.20689655140605073, 0.23152709327797194, 0.22824302102838243, 0.20525451508551004, 0.2200328405022817, 0.23316912950063964, 0.22988505734892314, 0.22988505725105016, 0.23316912950063964, 0.20525451508551004, 0.22988505734892314, 0.22331691275187118, 0.22331691294761713, 0.23481116572330737, 0.22988505734892314, 0.20525451508551004, 0.211822659780435, 0.2249589489745389, 0.2085385875308455, 0.22988505734892314, 0.23481116572330737, 0.2085385875308455, 0.20853858733509953, 0.22660098509933366, 0.22988505734892314, 0.22660098500146067, 0.20525451508551004, 0.22988505734892314, 0.23481116572330737, 0.21018062355776726, 0.2085385875308455, 0.23316912940276668, 0.2085385875308455, 0.21674876815481922, 0.21839080427961396, 0.22660098500146067, 0.22660098500146067, 0.20689655130817777, 0.211822659780435, 0.21674876815481922, 0.2200328404044087, 0.2151067319321515, 0.22824302102838243, 0.20853858743297252, 0.21018062355776726, 0.22824302102838243, 0.22495894868091998, 0.21018062365564025, 0.23481116552756143, 0.22824302102838243, 0.20853858733509953, 0.22495894868091998, 0.21839080408386802, 0.21839080408386802, 0.21510673203002448, 0.22167487672494943, 0.2200328404044087, 0.21839080408386802, 0.21346469580735675, 0.21839080408386802, 0.21839080408386802, 0.20197044273804757, 0.22331691236037926, 0.21839080408386802, 0.22660098500146067, 0.21839080408386802, 0.21839080408386802, 0.2167487677633273, 0.21839080408386802, 0.21839080408386802, 0.21839080408386802, 0.22660098500146067, 0.2167487678612003, 0.21018062336202128, 0.22824302102838243, 0.21839080388812204, 0.2134646956116108, 0.21839080408386802, 0.21674876795907325, 0.22003284020866276, 0.21839080398599503, 0.2085385870414806, 0.21839080408386802, 0.22660098470784173, 0.2085385870414806, 0.22003284020866276, 0.22824302063689053, 0.2085385870414806, 0.22003284020866276, 0.21839080398599503, 0.21674876795907325, 0.21839080398599503, 0.21839080398599503, 0.21839080408386802, 0.22003284020866276, 0.2085385870414806, 0.2282430211262554, 0.21839080398599503, 0.21018062316627534, 0.22003284020866276, 0.2167487678612003, 0.22003284020866276, 0.20197044234655565, 0.22003284020866276, 0.22660098490358768, 0.2036124784713504, 0.2167487677633273, 0.21839080408386802, 0.20525451459614513, 0.21839080408386802, 0.21839080398599503, 0.21839080408386802, 0.22003284020866276, 0.2167487678612003, 0.21018062316627534, 0.22003284020866276, 0.2233169125561252, 0.21018062316627534, 0.2085385869436076, 0.21839080408386802, 0.22824302102838243, 0.2003284062217609, 0.21839080398599503, 0.22824302102838243, 0.2003284062217609, 0.22660098480571472, 0.22660098490358768, 0.2085385869436076, 0.21839080408386802, 0.21018062316627534, 0.2167487678612003, 0.21018062316627534, 0.21839080408386802, 0.21839080388812204, 0.21182265938894307, 0.22660098490358768, 0.21018062316627534, 0.21839080408386802, 0.21510673163853256, 0.21839080408386802, 0.2085385869436076, 0.2167487678612003, 0.2167487678612003, 0.22660098490358768, 0.20525451459614513, 0.21839080408386802, 0.21839080398599503, 0.2167487678612003, 0.2036124784713504, 0.21839080398599503, 0.22331691245825225, 0.20525451459614513, 0.22824302102838243, 0.2167487677633273, 0.21018062326414833, 0.22003284020866276, 0.21839080398599503, 0.20525451449827217, 0.21346469541586482], 'loss': [1.6079882593859882, 1.60639452204812, 1.6054591984719466, 1.6051313899136177, 1.6041364363821136, 1.6037637669447755, 1.6033881821426768, 1.6027643311439843, 1.6022287704127036, 1.6017440470337134, 1.601393786201242, 1.6009254949048803, 1.6008624217348666, 1.6003944547758944, 1.6002320528519962, 1.6002688587568623, 1.601431502943411, 1.6001924496167004, 1.599833219252083, 1.5994327902549101, 1.5992108588345977, 1.598820523314897, 1.5989199249651398, 1.5986094025860578, 1.5984559504648008, 1.5981184704347802, 1.598735276139982, 1.597983717086134, 1.597641007513481, 1.5971757873127836, 1.5972965361890852, 1.5965093429083697, 1.5968938192058149, 1.5963632765247102, 1.5958521916391424, 1.5966781619637898, 1.5963771611023732, 1.5963743341532086, 1.5960798363421242, 1.5961212166525744, 1.595737818036481, 1.5957805324628858, 1.5958480141735665, 1.5955901010325313, 1.595800562511969, 1.5958664734015964, 1.5956276550430046, 1.5955328459122833, 1.5955628524325955, 1.5954766531989315, 1.5960681054626402, 1.5952220363538612, 1.5949093692846121, 1.5950293247704632, 1.5942954563262282, 1.594919569291618, 1.5944996248280487, 1.5944965575999548, 1.5947187141226546, 1.5938965022441542, 1.5939573963075202, 1.593596430187108, 1.5937262142952953, 1.5946633930323795, 1.5942061898155133, 1.594023645927774, 1.5938692308304492, 1.593404197692871, 1.5943591930538232, 1.5933910077602222, 1.5939101741054464, 1.5932598414607118, 1.5932857757721104, 1.592950203874028, 1.593271615911558, 1.5933676278566677, 1.592951913095353, 1.5930678913970258, 1.5930737487099744, 1.5928587870920954, 1.59271410231228, 1.5926414413373817, 1.5927500058003765, 1.592571232108365, 1.5935286197819014, 1.5933985288138262, 1.5939959936807777, 1.5923985604632807, 1.5933782045356546, 1.5924359607500707, 1.5924068098929873, 1.592257956068129, 1.592602156564685, 1.5920455544881018, 1.5920572274029867, 1.5923203998522593, 1.5917370640521666, 1.5920241654531178, 1.5917204297788334, 1.5917976686841897, 1.5919732925093883, 1.5914843142644581, 1.591804311064969, 1.5919272129540571, 1.591251051352499, 1.592564487848928, 1.5916703226629958, 1.5920061766978897, 1.5917510387589064, 1.5910465638985134, 1.592250722046995, 1.5912066211445866, 1.592036807218861, 1.591763914537136, 1.5911822956445525, 1.5922826171655675, 1.5915467990742083, 1.5917330612636935, 1.5918631660375262, 1.591201529267877, 1.592160580828939, 1.591231167830481, 1.5915398422942268, 1.5908246563200588, 1.590853648900496, 1.5907559390430333, 1.5910162542390138, 1.5905797065895442, 1.5909365695604798, 1.5906718808767488, 1.5909301685846318, 1.5907208930295595, 1.5904445289341576, 1.5914343487800269, 1.5917927618633796, 1.5919275425298014, 1.5909324227417274, 1.5906055762292912, 1.5912186096825884, 1.5907908770093193, 1.5910323640404296, 1.5921704533163772, 1.5911004932509312, 1.5907000535322655, 1.590716368119085, 1.590226153818244, 1.5899279728562434, 1.5901573535598035, 1.5903157564159291, 1.5904977634457347, 1.590103289625728, 1.5897857726232227, 1.5896173548159902, 1.5895116299329597, 1.5896577894810044, 1.5897501992004364, 1.5890184261960416, 1.5894473319180937, 1.5898964546054786, 1.5897497864964072, 1.5893090152642566, 1.5893574767533758, 1.5892024701136094, 1.5902346734883115, 1.5891592313132001, 1.5887910624059562, 1.5890077156697455, 1.5894460028936241, 1.5885122572372092, 1.589318103075517, 1.5884771575183594, 1.5890647815727845, 1.5896524156633098, 1.5885591541717186, 1.588692383697635, 1.5885483736864596, 1.588135272374633, 1.5886458481606516, 1.5880701794516625, 1.5884315452046953, 1.5880806746668885, 1.5887549707287392, 1.5891545942431848, 1.5885798912028757, 1.5884974865942765, 1.5886246534343618, 1.5880185094946953, 1.5885623163755915, 1.589448794006567, 1.588165774041867, 1.588606981183469, 1.5879758568025468, 1.5887000814355619, 1.5875493758023398, 1.5871246350619337, 1.587289902659657, 1.5872949582105789, 1.587147700321503, 1.5869483972721765, 1.5873425195838882, 1.587068740200458, 1.5873104445009016, 1.5868469645600054, 1.5865889316222017, 1.587103712534268, 1.5870445621576643, 1.5869221780823977, 1.5866761825412696, 1.586575921653967, 1.5861451889210414, 1.5859234192532925, 1.5865142669521073, 1.5863482100762871, 1.5867385819217752, 1.5872797716324825, 1.5862598036348943, 1.5859342601999364, 1.5859855479038716, 1.5853902705151441, 1.5854143471198894, 1.5852133693146755, 1.5851741906798595, 1.5845083086397613, 1.5858685975691622, 1.5867185035525406, 1.5841973032060346, 1.5872772657895724, 1.584891331416136, 1.5856272579952921, 1.5845307162654962, 1.5849748997717668, 1.585957472084484, 1.583890630870874, 1.5851023816964465, 1.583515374028952, 1.584223130008768, 1.5856413928390283, 1.5840690051260915, 1.5840598432435147, 1.5837708428165507, 1.5842645431690883, 1.5833868574068042, 1.5842701931019338, 1.5840072602951552, 1.5836128824790157, 1.584301555670752, 1.5863695000720954, 1.583576788451882, 1.5842167851371687, 1.583597413470368, 1.5834310803814835, 1.5830345057364117, 1.5832999294543413, 1.5828208208084107, 1.582889955097645, 1.5833523636236329, 1.5836768101862568, 1.5824616345047215, 1.5823925642507033, 1.581959658922356, 1.5818878923353474, 1.5828207779224404, 1.5825317928678446, 1.5821081213393007, 1.5829074017565843, 1.58150510097676, 1.5817662512252464, 1.5818883741660774, 1.5811665506578323, 1.5807946112121645, 1.5812194417879077, 1.581490976169124, 1.5824508093220986, 1.5817941729782543, 1.5822661215274976, 1.5802882843683388, 1.5809278592926277, 1.5803905657919035, 1.5796274346737402, 1.580300982581027, 1.5799476349378268, 1.5809724286351605, 1.5802767178850743, 1.5796969840169197, 1.5804842881353485, 1.5813131909595624, 1.5802649465184926, 1.580322444169673, 1.5794035052861521, 1.5805838015045228, 1.5794847009118334, 1.5805328435232018, 1.5799013705224227, 1.5801414131873441, 1.5798025728006382, 1.5810298306251698, 1.5799685452263459, 1.5788345566520456, 1.580084413718394, 1.5798047892122054], 'acc': [0.22587268881239686, 0.2328542071079082, 0.23285420970872686, 0.2328542083195837, 0.23285420951290053, 0.2328542102778472, 0.2328542098861945, 0.23285420970872686, 0.23285421031456463, 0.23285421011873828, 0.23901437376558904, 0.24517453960324703, 0.24517453727168959, 0.2447638592619671, 0.24476386122023056, 0.24558521626054383, 0.2410677626392435, 0.24394250441748014, 0.24640657108667205, 0.24353182779690077, 0.24435318242720266, 0.24804927956397038, 0.2476386047241869, 0.2468172475297838, 0.24681724870474187, 0.24804928175477767, 0.24558521682966417, 0.2472279263411704, 0.24558521647472892, 0.24763860474254562, 0.24804928038399324, 0.24722792692864945, 0.2439425067490376, 0.24435318417128107, 0.24681724813562153, 0.24353182776018334, 0.24394250500495918, 0.24640657108667205, 0.24722792730194343, 0.24845996015615288, 0.24887063640343823, 0.24681724890056822, 0.24558521487140067, 0.2476386043325342, 0.24763860374505514, 0.25092402429796096, 0.24928131384404043, 0.25051334746319653, 0.25010266945347404, 0.2513347027176949, 0.2521560573479968, 0.2546201236438947, 0.253388091181338, 0.2517453807274174, 0.2533880902022062, 0.2525667347702402, 0.25215605715217043, 0.2537987666086005, 0.2529774121924837, 0.2537987666086005, 0.2546201250146791, 0.25462012344806834, 0.2558521549314934, 0.2509240250812664, 0.252566736336851, 0.2542094460441836, 0.2542094450466931, 0.2542094475924357, 0.25051334746319653, 0.2546201236438947, 0.2533880902022062, 0.2546201234664271, 0.25503080224109625, 0.25462012227311026, 0.25215605969791294, 0.25462012344806834, 0.2558521566939305, 0.25462012344806834, 0.2558521563022778, 0.25256673555354564, 0.25462012383972105, 0.25503080024611535, 0.2550308008703118, 0.2542094436391913, 0.25092402488544, 0.2525667373159828, 0.2525667349660666, 0.25420944543834584, 0.25051334785484924, 0.2558521563022778, 0.25092402664787716, 0.2529774127799628, 0.25133470132855174, 0.25503080167197595, 0.2550308004786591, 0.249691992441242, 0.25420944680913027, 0.2566735089559575, 0.2554414784883816, 0.2550308000870064, 0.2484599607436319, 0.2517453787507952, 0.25462012540633183, 0.2550307984836782, 0.2533880911996967, 0.24394250479077412, 0.2505133480506756, 0.2546201252105055, 0.25872689782961195, 0.25585215512731974, 0.2542094452425195, 0.24476386063275152, 0.24845995956867384, 0.25338809000637985, 0.2554414794858721, 0.24928131402150805, 0.25420944663166267, 0.2505133459149445, 0.24599589346860223, 0.2509240239063083, 0.25708418915648723, 0.24804928116729863, 0.2513347019160308, 0.2554414790758607, 0.2505133484423283, 0.24845995976450017, 0.25174538053159107, 0.2525667339502174, 0.2517453787691539, 0.24763860492001324, 0.24640657030336666, 0.25667351310502823, 0.25831622181487035, 0.25379876703697063, 0.25749486755786244, 0.2439425043991214, 0.2529774121924837, 0.26406570725617223, 0.25872690139120363, 0.2570841897439663, 0.2537987689952341, 0.25544148064247146, 0.25338809000637985, 0.2550307992669836, 0.252566736336851, 0.25954825623569056, 0.2537987682119287, 0.25503080184944354, 0.25338809157299064, 0.25256673457441386, 0.2455852166338378, 0.25338809041639127, 0.2550308000870064, 0.2533880902205649, 0.25133470172020445, 0.25585215728140953, 0.26036961166765654, 0.25585215571479875, 0.25215605656469137, 0.2488706358159592, 0.2525667371201564, 0.25626283313704223, 0.254620123252242, 0.25585215649810417, 0.2505133492256337, 0.25420944624000996, 0.25749486640126307, 0.25379876879940777, 0.25872689886381983, 0.2558521582605413, 0.2587269009995509, 0.2554414790758607, 0.25544148064247146, 0.26036960951356675, 0.2542094466133039, 0.2636550318289097, 0.25708418798152916, 0.25995893111219154, 0.264476386869223, 0.25585215371981784, 0.2640657088595005, 0.25831622279400207, 0.2554414790758607, 0.25708418995815135, 0.25708418995815135, 0.25954825564821155, 0.26447638765252834, 0.25749486794951515, 0.2607802877191156, 0.26570841991925875, 0.2558521559106251, 0.2599589326788023, 0.2558521566939305, 0.2603696088893702, 0.2681724828493913, 0.26324435285841413, 0.2566735129092019, 0.2542094460441836, 0.26817248505855734, 0.262422999953832, 0.2677618089887396, 0.26694044940770284, 0.2599589318954969, 0.2616016427594289, 0.2620123207691514, 0.2681724862151567, 0.2685831608591138, 0.26817248660680937, 0.26817248406106686, 0.26776180859708687, 0.26817248545021, 0.2681724844710783, 0.27063654836198386, 0.2607802867583426, 0.26242299740808944, 0.27186858121619334, 0.2644763862817439, 0.267351127221599, 0.26776180761795515, 0.2632443526442291, 0.2648870646831191, 0.2620123185783441, 0.26611909436738956, 0.26160164295525523, 0.25503080088867053, 0.2669404519901628, 0.25626283529113203, 0.2599589334621077, 0.2624229965880666, 0.2640657070603459, 0.2661190967540232, 0.26858316246244207, 0.2665297751553984, 0.2657084165534934, 0.26570841991925875, 0.26652977237711206, 0.2632443524484027, 0.26365503124143064, 0.27351129348762715, 0.26570841796099526, 0.26529774034292547, 0.26652977417626667, 0.26735113137066974, 0.2681724850401986, 0.264476386869223, 0.26406570964280585, 0.2640657098386322, 0.2661190959707178, 0.26160164375691936, 0.2636550318289097, 0.2673511309790171, 0.26201231838251776, 0.2628336771802491, 0.26242299682061043, 0.26488706445057536, 0.2673511284332745, 0.2624229985830475, 0.26776180820543416, 0.2644763858900912, 0.26365502967481985, 0.265297742888668, 0.2669404523818155, 0.26858316306827984, 0.26570841894012703, 0.2673511307831907, 0.26940451828606077, 0.2665297745679193, 0.27022587391385305, 0.2685831632457475, 0.2644763862817439, 0.2665297745862781, 0.2669404525776419, 0.2661190945632159, 0.2632443510409009, 0.2669404515985101, 0.26694045161686886, 0.2702258713313931, 0.2669404515985101, 0.26735113058736437, 0.2665297755470511, 0.26694045140268374, 0.26899383990304426, 0.2652977413220572, 0.2694045153119481, 0.2685831634415738, 0.264887062920682, 0.2702258746971585, 0.2673511286291009, 0.2616016427594289, 0.26735112980405895, 0.26611909436738956, 0.27268993864314023, 0.26858316207078936, 0.2669404496035292, 0.2689938402763382, 0.26776180859708687, 0.26858316109165764, 0.2677618056229742, 0.2669404510110311, 0.2673511291798625]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
