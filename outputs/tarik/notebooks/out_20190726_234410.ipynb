{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf20.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 23:44:11 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '1Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '02', '01', '05', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000192165ACE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000019212787EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6142, Accuracy:0.2094, Validation Loss:1.6110, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6097, Accuracy:0.2329, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6070, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6061, Accuracy:0.2329, Validation Loss:1.6063, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2299\n",
    "Epoch #20: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #21: Loss:1.6032, Accuracy:0.2361, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6026, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2397\n",
    "Epoch #23: Loss:1.6023, Accuracy:0.2382, Validation Loss:1.6051, Validation Accuracy:0.2479\n",
    "Epoch #24: Loss:1.6022, Accuracy:0.2431, Validation Loss:1.6053, Validation Accuracy:0.2479\n",
    "Epoch #25: Loss:1.6021, Accuracy:0.2427, Validation Loss:1.6046, Validation Accuracy:0.2496\n",
    "Epoch #26: Loss:1.6026, Accuracy:0.2374, Validation Loss:1.6038, Validation Accuracy:0.2496\n",
    "Epoch #27: Loss:1.6020, Accuracy:0.2411, Validation Loss:1.6057, Validation Accuracy:0.2348\n",
    "Epoch #28: Loss:1.6023, Accuracy:0.2345, Validation Loss:1.6028, Validation Accuracy:0.2463\n",
    "Epoch #29: Loss:1.6030, Accuracy:0.2374, Validation Loss:1.6036, Validation Accuracy:0.2414\n",
    "Epoch #30: Loss:1.6036, Accuracy:0.2374, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #31: Loss:1.6030, Accuracy:0.2390, Validation Loss:1.6036, Validation Accuracy:0.2529\n",
    "Epoch #32: Loss:1.6026, Accuracy:0.2390, Validation Loss:1.6034, Validation Accuracy:0.2529\n",
    "Epoch #33: Loss:1.6031, Accuracy:0.2353, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #34: Loss:1.6026, Accuracy:0.2370, Validation Loss:1.6036, Validation Accuracy:0.2512\n",
    "Epoch #35: Loss:1.6026, Accuracy:0.2398, Validation Loss:1.6037, Validation Accuracy:0.2479\n",
    "Epoch #36: Loss:1.6023, Accuracy:0.2415, Validation Loss:1.6042, Validation Accuracy:0.2496\n",
    "Epoch #37: Loss:1.6019, Accuracy:0.2390, Validation Loss:1.6040, Validation Accuracy:0.2447\n",
    "Epoch #38: Loss:1.6015, Accuracy:0.2370, Validation Loss:1.6037, Validation Accuracy:0.2430\n",
    "Epoch #39: Loss:1.6018, Accuracy:0.2407, Validation Loss:1.6035, Validation Accuracy:0.2512\n",
    "Epoch #40: Loss:1.6023, Accuracy:0.2435, Validation Loss:1.6033, Validation Accuracy:0.2496\n",
    "Epoch #41: Loss:1.6018, Accuracy:0.2419, Validation Loss:1.6032, Validation Accuracy:0.2512\n",
    "Epoch #42: Loss:1.6025, Accuracy:0.2374, Validation Loss:1.6038, Validation Accuracy:0.2447\n",
    "Epoch #43: Loss:1.6017, Accuracy:0.2382, Validation Loss:1.6032, Validation Accuracy:0.2512\n",
    "Epoch #44: Loss:1.6016, Accuracy:0.2407, Validation Loss:1.6032, Validation Accuracy:0.2381\n",
    "Epoch #45: Loss:1.6016, Accuracy:0.2394, Validation Loss:1.6029, Validation Accuracy:0.2512\n",
    "Epoch #46: Loss:1.6014, Accuracy:0.2419, Validation Loss:1.6029, Validation Accuracy:0.2496\n",
    "Epoch #47: Loss:1.6013, Accuracy:0.2419, Validation Loss:1.6032, Validation Accuracy:0.2479\n",
    "Epoch #48: Loss:1.6013, Accuracy:0.2415, Validation Loss:1.6030, Validation Accuracy:0.2496\n",
    "Epoch #49: Loss:1.6020, Accuracy:0.2439, Validation Loss:1.6029, Validation Accuracy:0.2512\n",
    "Epoch #50: Loss:1.6019, Accuracy:0.2439, Validation Loss:1.6021, Validation Accuracy:0.2496\n",
    "Epoch #51: Loss:1.6020, Accuracy:0.2394, Validation Loss:1.6022, Validation Accuracy:0.2463\n",
    "Epoch #52: Loss:1.6019, Accuracy:0.2419, Validation Loss:1.6019, Validation Accuracy:0.2479\n",
    "Epoch #53: Loss:1.6015, Accuracy:0.2431, Validation Loss:1.6017, Validation Accuracy:0.2479\n",
    "Epoch #54: Loss:1.6014, Accuracy:0.2423, Validation Loss:1.6024, Validation Accuracy:0.2496\n",
    "Epoch #55: Loss:1.6014, Accuracy:0.2439, Validation Loss:1.6019, Validation Accuracy:0.2512\n",
    "Epoch #56: Loss:1.6013, Accuracy:0.2427, Validation Loss:1.6024, Validation Accuracy:0.2512\n",
    "Epoch #57: Loss:1.6012, Accuracy:0.2435, Validation Loss:1.6025, Validation Accuracy:0.2512\n",
    "Epoch #58: Loss:1.6009, Accuracy:0.2431, Validation Loss:1.6022, Validation Accuracy:0.2529\n",
    "Epoch #59: Loss:1.6010, Accuracy:0.2402, Validation Loss:1.6023, Validation Accuracy:0.2365\n",
    "Epoch #60: Loss:1.6004, Accuracy:0.2489, Validation Loss:1.6021, Validation Accuracy:0.2512\n",
    "Epoch #61: Loss:1.6002, Accuracy:0.2411, Validation Loss:1.6025, Validation Accuracy:0.2463\n",
    "Epoch #62: Loss:1.6006, Accuracy:0.2402, Validation Loss:1.6018, Validation Accuracy:0.2529\n",
    "Epoch #63: Loss:1.6000, Accuracy:0.2444, Validation Loss:1.6022, Validation Accuracy:0.2496\n",
    "Epoch #64: Loss:1.5996, Accuracy:0.2456, Validation Loss:1.6019, Validation Accuracy:0.2496\n",
    "Epoch #65: Loss:1.5993, Accuracy:0.2427, Validation Loss:1.6019, Validation Accuracy:0.2512\n",
    "Epoch #66: Loss:1.5995, Accuracy:0.2435, Validation Loss:1.6020, Validation Accuracy:0.2479\n",
    "Epoch #67: Loss:1.5998, Accuracy:0.2444, Validation Loss:1.6023, Validation Accuracy:0.2496\n",
    "Epoch #68: Loss:1.5997, Accuracy:0.2394, Validation Loss:1.6027, Validation Accuracy:0.2381\n",
    "Epoch #69: Loss:1.6003, Accuracy:0.2435, Validation Loss:1.6026, Validation Accuracy:0.2348\n",
    "Epoch #70: Loss:1.6001, Accuracy:0.2415, Validation Loss:1.6032, Validation Accuracy:0.2512\n",
    "Epoch #71: Loss:1.5992, Accuracy:0.2427, Validation Loss:1.6028, Validation Accuracy:0.2348\n",
    "Epoch #72: Loss:1.5989, Accuracy:0.2402, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #73: Loss:1.5986, Accuracy:0.2415, Validation Loss:1.6018, Validation Accuracy:0.2397\n",
    "Epoch #74: Loss:1.5978, Accuracy:0.2427, Validation Loss:1.6026, Validation Accuracy:0.2562\n",
    "Epoch #75: Loss:1.5983, Accuracy:0.2427, Validation Loss:1.6024, Validation Accuracy:0.2381\n",
    "Epoch #76: Loss:1.5981, Accuracy:0.2472, Validation Loss:1.6009, Validation Accuracy:0.2430\n",
    "Epoch #77: Loss:1.5994, Accuracy:0.2390, Validation Loss:1.6015, Validation Accuracy:0.2348\n",
    "Epoch #78: Loss:1.5986, Accuracy:0.2427, Validation Loss:1.5990, Validation Accuracy:0.2430\n",
    "Epoch #79: Loss:1.6004, Accuracy:0.2361, Validation Loss:1.5995, Validation Accuracy:0.2365\n",
    "Epoch #80: Loss:1.6006, Accuracy:0.2337, Validation Loss:1.5984, Validation Accuracy:0.2381\n",
    "Epoch #81: Loss:1.6003, Accuracy:0.2386, Validation Loss:1.6007, Validation Accuracy:0.2250\n",
    "Epoch #82: Loss:1.6075, Accuracy:0.2324, Validation Loss:1.6084, Validation Accuracy:0.2200\n",
    "Epoch #83: Loss:1.6040, Accuracy:0.2296, Validation Loss:1.6078, Validation Accuracy:0.2414\n",
    "Epoch #84: Loss:1.6053, Accuracy:0.2337, Validation Loss:1.6062, Validation Accuracy:0.2184\n",
    "Epoch #85: Loss:1.6019, Accuracy:0.2370, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #86: Loss:1.6014, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2167\n",
    "Epoch #87: Loss:1.6019, Accuracy:0.2308, Validation Loss:1.6042, Validation Accuracy:0.2282\n",
    "Epoch #88: Loss:1.6001, Accuracy:0.2407, Validation Loss:1.6035, Validation Accuracy:0.2479\n",
    "Epoch #89: Loss:1.6006, Accuracy:0.2435, Validation Loss:1.6030, Validation Accuracy:0.2496\n",
    "Epoch #90: Loss:1.5999, Accuracy:0.2419, Validation Loss:1.6026, Validation Accuracy:0.2348\n",
    "Epoch #91: Loss:1.5995, Accuracy:0.2419, Validation Loss:1.6022, Validation Accuracy:0.2512\n",
    "Epoch #92: Loss:1.6000, Accuracy:0.2386, Validation Loss:1.6027, Validation Accuracy:0.2479\n",
    "Epoch #93: Loss:1.5991, Accuracy:0.2394, Validation Loss:1.6023, Validation Accuracy:0.2299\n",
    "Epoch #94: Loss:1.5989, Accuracy:0.2423, Validation Loss:1.6020, Validation Accuracy:0.2348\n",
    "Epoch #95: Loss:1.5981, Accuracy:0.2439, Validation Loss:1.6021, Validation Accuracy:0.2512\n",
    "Epoch #96: Loss:1.5989, Accuracy:0.2460, Validation Loss:1.6009, Validation Accuracy:0.2496\n",
    "Epoch #97: Loss:1.5979, Accuracy:0.2452, Validation Loss:1.6003, Validation Accuracy:0.2299\n",
    "Epoch #98: Loss:1.5982, Accuracy:0.2390, Validation Loss:1.6012, Validation Accuracy:0.2348\n",
    "Epoch #99: Loss:1.5982, Accuracy:0.2419, Validation Loss:1.6013, Validation Accuracy:0.2381\n",
    "Epoch #100: Loss:1.5979, Accuracy:0.2411, Validation Loss:1.6005, Validation Accuracy:0.2512\n",
    "Epoch #101: Loss:1.5987, Accuracy:0.2415, Validation Loss:1.6006, Validation Accuracy:0.2315\n",
    "Epoch #102: Loss:1.5976, Accuracy:0.2419, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #103: Loss:1.5977, Accuracy:0.2435, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #104: Loss:1.5989, Accuracy:0.2329, Validation Loss:1.6022, Validation Accuracy:0.2348\n",
    "Epoch #105: Loss:1.6027, Accuracy:0.2345, Validation Loss:1.6042, Validation Accuracy:0.2447\n",
    "Epoch #106: Loss:1.5997, Accuracy:0.2398, Validation Loss:1.6065, Validation Accuracy:0.2315\n",
    "Epoch #107: Loss:1.5997, Accuracy:0.2419, Validation Loss:1.6060, Validation Accuracy:0.1921\n",
    "Epoch #108: Loss:1.6005, Accuracy:0.2341, Validation Loss:1.6028, Validation Accuracy:0.2414\n",
    "Epoch #109: Loss:1.6002, Accuracy:0.2361, Validation Loss:1.6019, Validation Accuracy:0.2250\n",
    "Epoch #110: Loss:1.5985, Accuracy:0.2415, Validation Loss:1.6027, Validation Accuracy:0.2053\n",
    "Epoch #111: Loss:1.5987, Accuracy:0.2349, Validation Loss:1.6013, Validation Accuracy:0.2463\n",
    "Epoch #112: Loss:1.5979, Accuracy:0.2439, Validation Loss:1.6025, Validation Accuracy:0.2479\n",
    "Epoch #113: Loss:1.5987, Accuracy:0.2390, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #114: Loss:1.6021, Accuracy:0.2337, Validation Loss:1.6167, Validation Accuracy:0.2200\n",
    "Epoch #115: Loss:1.6064, Accuracy:0.2312, Validation Loss:1.6056, Validation Accuracy:0.2250\n",
    "Epoch #116: Loss:1.6021, Accuracy:0.2341, Validation Loss:1.6111, Validation Accuracy:0.2217\n",
    "Epoch #117: Loss:1.6012, Accuracy:0.2386, Validation Loss:1.6058, Validation Accuracy:0.2184\n",
    "Epoch #118: Loss:1.6019, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2184\n",
    "Epoch #119: Loss:1.6018, Accuracy:0.2308, Validation Loss:1.6047, Validation Accuracy:0.2348\n",
    "Epoch #120: Loss:1.6011, Accuracy:0.2296, Validation Loss:1.6038, Validation Accuracy:0.2184\n",
    "Epoch #121: Loss:1.6008, Accuracy:0.2349, Validation Loss:1.6038, Validation Accuracy:0.2282\n",
    "Epoch #122: Loss:1.5996, Accuracy:0.2394, Validation Loss:1.6032, Validation Accuracy:0.2217\n",
    "Epoch #123: Loss:1.5996, Accuracy:0.2394, Validation Loss:1.6026, Validation Accuracy:0.2233\n",
    "Epoch #124: Loss:1.5993, Accuracy:0.2370, Validation Loss:1.6020, Validation Accuracy:0.2299\n",
    "Epoch #125: Loss:1.5991, Accuracy:0.2370, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #126: Loss:1.5990, Accuracy:0.2431, Validation Loss:1.6013, Validation Accuracy:0.2430\n",
    "Epoch #127: Loss:1.5987, Accuracy:0.2423, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #128: Loss:1.5983, Accuracy:0.2419, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #129: Loss:1.5979, Accuracy:0.2407, Validation Loss:1.6015, Validation Accuracy:0.2348\n",
    "Epoch #130: Loss:1.5976, Accuracy:0.2423, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #131: Loss:1.5984, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2447\n",
    "Epoch #132: Loss:1.5983, Accuracy:0.2419, Validation Loss:1.6008, Validation Accuracy:0.2447\n",
    "Epoch #133: Loss:1.5987, Accuracy:0.2357, Validation Loss:1.6011, Validation Accuracy:0.2332\n",
    "Epoch #134: Loss:1.5987, Accuracy:0.2390, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #135: Loss:1.5987, Accuracy:0.2394, Validation Loss:1.6011, Validation Accuracy:0.2447\n",
    "Epoch #136: Loss:1.5986, Accuracy:0.2407, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #137: Loss:1.5982, Accuracy:0.2423, Validation Loss:1.6000, Validation Accuracy:0.2397\n",
    "Epoch #138: Loss:1.5980, Accuracy:0.2407, Validation Loss:1.5999, Validation Accuracy:0.2397\n",
    "Epoch #139: Loss:1.5980, Accuracy:0.2357, Validation Loss:1.5996, Validation Accuracy:0.2299\n",
    "Epoch #140: Loss:1.5983, Accuracy:0.2394, Validation Loss:1.5988, Validation Accuracy:0.2299\n",
    "Epoch #141: Loss:1.5991, Accuracy:0.2386, Validation Loss:1.5992, Validation Accuracy:0.2315\n",
    "Epoch #142: Loss:1.5983, Accuracy:0.2398, Validation Loss:1.5982, Validation Accuracy:0.2282\n",
    "Epoch #143: Loss:1.5985, Accuracy:0.2382, Validation Loss:1.5980, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5981, Accuracy:0.2452, Validation Loss:1.5988, Validation Accuracy:0.2365\n",
    "Epoch #145: Loss:1.5987, Accuracy:0.2415, Validation Loss:1.5980, Validation Accuracy:0.2315\n",
    "Epoch #146: Loss:1.5977, Accuracy:0.2419, Validation Loss:1.5995, Validation Accuracy:0.2332\n",
    "Epoch #147: Loss:1.5981, Accuracy:0.2415, Validation Loss:1.5994, Validation Accuracy:0.2332\n",
    "Epoch #148: Loss:1.5967, Accuracy:0.2444, Validation Loss:1.6017, Validation Accuracy:0.2430\n",
    "Epoch #149: Loss:1.5971, Accuracy:0.2448, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #150: Loss:1.5964, Accuracy:0.2448, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #151: Loss:1.5965, Accuracy:0.2419, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #152: Loss:1.5955, Accuracy:0.2398, Validation Loss:1.6002, Validation Accuracy:0.2397\n",
    "Epoch #153: Loss:1.5961, Accuracy:0.2448, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #154: Loss:1.5956, Accuracy:0.2444, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #155: Loss:1.5968, Accuracy:0.2415, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #156: Loss:1.5954, Accuracy:0.2460, Validation Loss:1.5999, Validation Accuracy:0.2463\n",
    "Epoch #157: Loss:1.5956, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #158: Loss:1.5961, Accuracy:0.2382, Validation Loss:1.6012, Validation Accuracy:0.2430\n",
    "Epoch #159: Loss:1.5946, Accuracy:0.2415, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #160: Loss:1.5952, Accuracy:0.2452, Validation Loss:1.6027, Validation Accuracy:0.2479\n",
    "Epoch #161: Loss:1.5949, Accuracy:0.2431, Validation Loss:1.6022, Validation Accuracy:0.2381\n",
    "Epoch #162: Loss:1.5959, Accuracy:0.2419, Validation Loss:1.6052, Validation Accuracy:0.2414\n",
    "Epoch #163: Loss:1.5971, Accuracy:0.2439, Validation Loss:1.6084, Validation Accuracy:0.2102\n",
    "Epoch #164: Loss:1.5971, Accuracy:0.2435, Validation Loss:1.6075, Validation Accuracy:0.2315\n",
    "Epoch #165: Loss:1.5967, Accuracy:0.2456, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #166: Loss:1.5963, Accuracy:0.2460, Validation Loss:1.6078, Validation Accuracy:0.2332\n",
    "Epoch #167: Loss:1.5975, Accuracy:0.2423, Validation Loss:1.6070, Validation Accuracy:0.2332\n",
    "Epoch #168: Loss:1.5974, Accuracy:0.2460, Validation Loss:1.6072, Validation Accuracy:0.2397\n",
    "Epoch #169: Loss:1.5969, Accuracy:0.2431, Validation Loss:1.6045, Validation Accuracy:0.2381\n",
    "Epoch #170: Loss:1.5956, Accuracy:0.2505, Validation Loss:1.6060, Validation Accuracy:0.2118\n",
    "Epoch #171: Loss:1.5964, Accuracy:0.2472, Validation Loss:1.6051, Validation Accuracy:0.2282\n",
    "Epoch #172: Loss:1.5949, Accuracy:0.2485, Validation Loss:1.6045, Validation Accuracy:0.2381\n",
    "Epoch #173: Loss:1.5947, Accuracy:0.2476, Validation Loss:1.6041, Validation Accuracy:0.2381\n",
    "Epoch #174: Loss:1.5963, Accuracy:0.2476, Validation Loss:1.6047, Validation Accuracy:0.2299\n",
    "Epoch #175: Loss:1.5954, Accuracy:0.2452, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #176: Loss:1.5951, Accuracy:0.2407, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #177: Loss:1.5951, Accuracy:0.2513, Validation Loss:1.6040, Validation Accuracy:0.2414\n",
    "Epoch #178: Loss:1.5943, Accuracy:0.2476, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #179: Loss:1.5954, Accuracy:0.2419, Validation Loss:1.6041, Validation Accuracy:0.2381\n",
    "Epoch #180: Loss:1.5946, Accuracy:0.2460, Validation Loss:1.6024, Validation Accuracy:0.2365\n",
    "Epoch #181: Loss:1.5939, Accuracy:0.2472, Validation Loss:1.6029, Validation Accuracy:0.2397\n",
    "Epoch #182: Loss:1.5935, Accuracy:0.2493, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #183: Loss:1.5954, Accuracy:0.2407, Validation Loss:1.6040, Validation Accuracy:0.2167\n",
    "Epoch #184: Loss:1.5970, Accuracy:0.2353, Validation Loss:1.6099, Validation Accuracy:0.2414\n",
    "Epoch #185: Loss:1.5970, Accuracy:0.2419, Validation Loss:1.6070, Validation Accuracy:0.2036\n",
    "Epoch #186: Loss:1.5967, Accuracy:0.2501, Validation Loss:1.6059, Validation Accuracy:0.2036\n",
    "Epoch #187: Loss:1.5965, Accuracy:0.2390, Validation Loss:1.6049, Validation Accuracy:0.2348\n",
    "Epoch #188: Loss:1.5951, Accuracy:0.2501, Validation Loss:1.6051, Validation Accuracy:0.2135\n",
    "Epoch #189: Loss:1.5950, Accuracy:0.2415, Validation Loss:1.6037, Validation Accuracy:0.2299\n",
    "Epoch #190: Loss:1.5957, Accuracy:0.2448, Validation Loss:1.6044, Validation Accuracy:0.2266\n",
    "Epoch #191: Loss:1.5947, Accuracy:0.2452, Validation Loss:1.6043, Validation Accuracy:0.2118\n",
    "Epoch #192: Loss:1.5951, Accuracy:0.2444, Validation Loss:1.6030, Validation Accuracy:0.2430\n",
    "Epoch #193: Loss:1.5945, Accuracy:0.2476, Validation Loss:1.6031, Validation Accuracy:0.2365\n",
    "Epoch #194: Loss:1.5939, Accuracy:0.2472, Validation Loss:1.6045, Validation Accuracy:0.2118\n",
    "Epoch #195: Loss:1.5942, Accuracy:0.2472, Validation Loss:1.6033, Validation Accuracy:0.2381\n",
    "Epoch #196: Loss:1.5945, Accuracy:0.2452, Validation Loss:1.6040, Validation Accuracy:0.2381\n",
    "Epoch #197: Loss:1.5944, Accuracy:0.2460, Validation Loss:1.6027, Validation Accuracy:0.2053\n",
    "Epoch #198: Loss:1.5939, Accuracy:0.2439, Validation Loss:1.6018, Validation Accuracy:0.2397\n",
    "Epoch #199: Loss:1.5943, Accuracy:0.2423, Validation Loss:1.6023, Validation Accuracy:0.2414\n",
    "Epoch #200: Loss:1.5935, Accuracy:0.2460, Validation Loss:1.6033, Validation Accuracy:0.2102\n",
    "Epoch #201: Loss:1.5941, Accuracy:0.2423, Validation Loss:1.6019, Validation Accuracy:0.2365\n",
    "Epoch #202: Loss:1.5955, Accuracy:0.2530, Validation Loss:1.6020, Validation Accuracy:0.2332\n",
    "Epoch #203: Loss:1.5957, Accuracy:0.2394, Validation Loss:1.6028, Validation Accuracy:0.2348\n",
    "Epoch #204: Loss:1.5944, Accuracy:0.2411, Validation Loss:1.6029, Validation Accuracy:0.2036\n",
    "Epoch #205: Loss:1.5948, Accuracy:0.2468, Validation Loss:1.6017, Validation Accuracy:0.2381\n",
    "Epoch #206: Loss:1.5943, Accuracy:0.2448, Validation Loss:1.6017, Validation Accuracy:0.2397\n",
    "Epoch #207: Loss:1.5946, Accuracy:0.2444, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #208: Loss:1.5953, Accuracy:0.2394, Validation Loss:1.6028, Validation Accuracy:0.2299\n",
    "Epoch #209: Loss:1.5951, Accuracy:0.2407, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #210: Loss:1.5957, Accuracy:0.2435, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #211: Loss:1.5943, Accuracy:0.2435, Validation Loss:1.6019, Validation Accuracy:0.2430\n",
    "Epoch #212: Loss:1.5944, Accuracy:0.2366, Validation Loss:1.6020, Validation Accuracy:0.2447\n",
    "Epoch #213: Loss:1.5937, Accuracy:0.2444, Validation Loss:1.6016, Validation Accuracy:0.2381\n",
    "Epoch #214: Loss:1.5936, Accuracy:0.2460, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #215: Loss:1.5939, Accuracy:0.2480, Validation Loss:1.6009, Validation Accuracy:0.2463\n",
    "Epoch #216: Loss:1.5941, Accuracy:0.2439, Validation Loss:1.6017, Validation Accuracy:0.2430\n",
    "Epoch #217: Loss:1.5934, Accuracy:0.2439, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #218: Loss:1.5933, Accuracy:0.2517, Validation Loss:1.6012, Validation Accuracy:0.2200\n",
    "Epoch #219: Loss:1.5950, Accuracy:0.2435, Validation Loss:1.6006, Validation Accuracy:0.2332\n",
    "Epoch #220: Loss:1.5940, Accuracy:0.2407, Validation Loss:1.6005, Validation Accuracy:0.2135\n",
    "Epoch #221: Loss:1.5941, Accuracy:0.2427, Validation Loss:1.6001, Validation Accuracy:0.2315\n",
    "Epoch #222: Loss:1.5944, Accuracy:0.2439, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #223: Loss:1.5956, Accuracy:0.2460, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #224: Loss:1.5956, Accuracy:0.2390, Validation Loss:1.6024, Validation Accuracy:0.2250\n",
    "Epoch #225: Loss:1.5946, Accuracy:0.2415, Validation Loss:1.6013, Validation Accuracy:0.2365\n",
    "Epoch #226: Loss:1.5955, Accuracy:0.2427, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #227: Loss:1.5950, Accuracy:0.2411, Validation Loss:1.6047, Validation Accuracy:0.2315\n",
    "Epoch #228: Loss:1.5948, Accuracy:0.2415, Validation Loss:1.6027, Validation Accuracy:0.2381\n",
    "Epoch #229: Loss:1.5958, Accuracy:0.2452, Validation Loss:1.6008, Validation Accuracy:0.2381\n",
    "Epoch #230: Loss:1.5952, Accuracy:0.2431, Validation Loss:1.5987, Validation Accuracy:0.2397\n",
    "Epoch #231: Loss:1.5953, Accuracy:0.2402, Validation Loss:1.6011, Validation Accuracy:0.2266\n",
    "Epoch #232: Loss:1.5967, Accuracy:0.2431, Validation Loss:1.5998, Validation Accuracy:0.2348\n",
    "Epoch #233: Loss:1.5962, Accuracy:0.2476, Validation Loss:1.6021, Validation Accuracy:0.2381\n",
    "Epoch #234: Loss:1.5951, Accuracy:0.2476, Validation Loss:1.6000, Validation Accuracy:0.2282\n",
    "Epoch #235: Loss:1.5939, Accuracy:0.2427, Validation Loss:1.6000, Validation Accuracy:0.2365\n",
    "Epoch #236: Loss:1.5953, Accuracy:0.2464, Validation Loss:1.6016, Validation Accuracy:0.2167\n",
    "Epoch #237: Loss:1.5956, Accuracy:0.2423, Validation Loss:1.6024, Validation Accuracy:0.2282\n",
    "Epoch #238: Loss:1.5969, Accuracy:0.2427, Validation Loss:1.6013, Validation Accuracy:0.2332\n",
    "Epoch #239: Loss:1.5948, Accuracy:0.2431, Validation Loss:1.6013, Validation Accuracy:0.2200\n",
    "Epoch #240: Loss:1.5949, Accuracy:0.2435, Validation Loss:1.6009, Validation Accuracy:0.2381\n",
    "Epoch #241: Loss:1.5952, Accuracy:0.2472, Validation Loss:1.6013, Validation Accuracy:0.2217\n",
    "Epoch #242: Loss:1.5940, Accuracy:0.2444, Validation Loss:1.5998, Validation Accuracy:0.2282\n",
    "Epoch #243: Loss:1.5960, Accuracy:0.2472, Validation Loss:1.5993, Validation Accuracy:0.2332\n",
    "Epoch #244: Loss:1.5961, Accuracy:0.2423, Validation Loss:1.6016, Validation Accuracy:0.2266\n",
    "Epoch #245: Loss:1.5936, Accuracy:0.2394, Validation Loss:1.6017, Validation Accuracy:0.2036\n",
    "Epoch #246: Loss:1.5958, Accuracy:0.2370, Validation Loss:1.5991, Validation Accuracy:0.2414\n",
    "Epoch #247: Loss:1.5974, Accuracy:0.2398, Validation Loss:1.5998, Validation Accuracy:0.2332\n",
    "Epoch #248: Loss:1.5957, Accuracy:0.2468, Validation Loss:1.6047, Validation Accuracy:0.2250\n",
    "Epoch #249: Loss:1.5965, Accuracy:0.2390, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #250: Loss:1.5946, Accuracy:0.2407, Validation Loss:1.6010, Validation Accuracy:0.2135\n",
    "Epoch #251: Loss:1.5946, Accuracy:0.2427, Validation Loss:1.6002, Validation Accuracy:0.2545\n",
    "Epoch #252: Loss:1.5940, Accuracy:0.2493, Validation Loss:1.6002, Validation Accuracy:0.2447\n",
    "Epoch #253: Loss:1.5929, Accuracy:0.2476, Validation Loss:1.6002, Validation Accuracy:0.2430\n",
    "Epoch #254: Loss:1.5935, Accuracy:0.2464, Validation Loss:1.6013, Validation Accuracy:0.2397\n",
    "Epoch #255: Loss:1.5924, Accuracy:0.2476, Validation Loss:1.6006, Validation Accuracy:0.2381\n",
    "Epoch #256: Loss:1.5937, Accuracy:0.2554, Validation Loss:1.6016, Validation Accuracy:0.2315\n",
    "Epoch #257: Loss:1.5927, Accuracy:0.2456, Validation Loss:1.6011, Validation Accuracy:0.2397\n",
    "Epoch #258: Loss:1.5929, Accuracy:0.2501, Validation Loss:1.6013, Validation Accuracy:0.2102\n",
    "Epoch #259: Loss:1.5927, Accuracy:0.2501, Validation Loss:1.6015, Validation Accuracy:0.2135\n",
    "Epoch #260: Loss:1.5920, Accuracy:0.2480, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #261: Loss:1.5912, Accuracy:0.2517, Validation Loss:1.6013, Validation Accuracy:0.2266\n",
    "Epoch #262: Loss:1.5913, Accuracy:0.2513, Validation Loss:1.6023, Validation Accuracy:0.2135\n",
    "Epoch #263: Loss:1.5910, Accuracy:0.2468, Validation Loss:1.6029, Validation Accuracy:0.2365\n",
    "Epoch #264: Loss:1.5912, Accuracy:0.2567, Validation Loss:1.6030, Validation Accuracy:0.2299\n",
    "Epoch #265: Loss:1.5912, Accuracy:0.2554, Validation Loss:1.6032, Validation Accuracy:0.2167\n",
    "Epoch #266: Loss:1.5923, Accuracy:0.2456, Validation Loss:1.6034, Validation Accuracy:0.2266\n",
    "Epoch #267: Loss:1.5917, Accuracy:0.2472, Validation Loss:1.6030, Validation Accuracy:0.2151\n",
    "Epoch #268: Loss:1.5914, Accuracy:0.2501, Validation Loss:1.6030, Validation Accuracy:0.2167\n",
    "Epoch #269: Loss:1.5916, Accuracy:0.2480, Validation Loss:1.6032, Validation Accuracy:0.2118\n",
    "Epoch #270: Loss:1.5905, Accuracy:0.2468, Validation Loss:1.6037, Validation Accuracy:0.2036\n",
    "Epoch #271: Loss:1.5902, Accuracy:0.2497, Validation Loss:1.6049, Validation Accuracy:0.2233\n",
    "Epoch #272: Loss:1.5900, Accuracy:0.2505, Validation Loss:1.6038, Validation Accuracy:0.2315\n",
    "Epoch #273: Loss:1.5901, Accuracy:0.2517, Validation Loss:1.6037, Validation Accuracy:0.2118\n",
    "Epoch #274: Loss:1.5900, Accuracy:0.2489, Validation Loss:1.6026, Validation Accuracy:0.2118\n",
    "Epoch #275: Loss:1.5900, Accuracy:0.2497, Validation Loss:1.6027, Validation Accuracy:0.1970\n",
    "Epoch #276: Loss:1.5910, Accuracy:0.2513, Validation Loss:1.6027, Validation Accuracy:0.2332\n",
    "Epoch #277: Loss:1.5914, Accuracy:0.2476, Validation Loss:1.6025, Validation Accuracy:0.2266\n",
    "Epoch #278: Loss:1.5907, Accuracy:0.2493, Validation Loss:1.6022, Validation Accuracy:0.2118\n",
    "Epoch #279: Loss:1.5894, Accuracy:0.2501, Validation Loss:1.6032, Validation Accuracy:0.2365\n",
    "Epoch #280: Loss:1.5906, Accuracy:0.2489, Validation Loss:1.6026, Validation Accuracy:0.2266\n",
    "Epoch #281: Loss:1.5902, Accuracy:0.2501, Validation Loss:1.6038, Validation Accuracy:0.2020\n",
    "Epoch #282: Loss:1.5896, Accuracy:0.2464, Validation Loss:1.6041, Validation Accuracy:0.2348\n",
    "Epoch #283: Loss:1.5903, Accuracy:0.2370, Validation Loss:1.6033, Validation Accuracy:0.2118\n",
    "Epoch #284: Loss:1.5892, Accuracy:0.2415, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #285: Loss:1.5902, Accuracy:0.2452, Validation Loss:1.6016, Validation Accuracy:0.2118\n",
    "Epoch #286: Loss:1.5898, Accuracy:0.2456, Validation Loss:1.6026, Validation Accuracy:0.2151\n",
    "Epoch #287: Loss:1.5890, Accuracy:0.2472, Validation Loss:1.6035, Validation Accuracy:0.2381\n",
    "Epoch #288: Loss:1.5896, Accuracy:0.2522, Validation Loss:1.6026, Validation Accuracy:0.2036\n",
    "Epoch #289: Loss:1.5889, Accuracy:0.2522, Validation Loss:1.6018, Validation Accuracy:0.2315\n",
    "Epoch #290: Loss:1.5901, Accuracy:0.2472, Validation Loss:1.6012, Validation Accuracy:0.2102\n",
    "Epoch #291: Loss:1.5920, Accuracy:0.2497, Validation Loss:1.6024, Validation Accuracy:0.2036\n",
    "Epoch #292: Loss:1.5905, Accuracy:0.2415, Validation Loss:1.6030, Validation Accuracy:0.2365\n",
    "Epoch #293: Loss:1.5892, Accuracy:0.2480, Validation Loss:1.6007, Validation Accuracy:0.2036\n",
    "Epoch #294: Loss:1.5886, Accuracy:0.2452, Validation Loss:1.6011, Validation Accuracy:0.2102\n",
    "Epoch #295: Loss:1.5886, Accuracy:0.2489, Validation Loss:1.6027, Validation Accuracy:0.2282\n",
    "Epoch #296: Loss:1.5885, Accuracy:0.2505, Validation Loss:1.6024, Validation Accuracy:0.2118\n",
    "Epoch #297: Loss:1.5871, Accuracy:0.2423, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #298: Loss:1.5883, Accuracy:0.2497, Validation Loss:1.6017, Validation Accuracy:0.2151\n",
    "Epoch #299: Loss:1.5875, Accuracy:0.2505, Validation Loss:1.6024, Validation Accuracy:0.2118\n",
    "Epoch #300: Loss:1.5874, Accuracy:0.2493, Validation Loss:1.6024, Validation Accuracy:0.2299\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60237229, Accuracy:0.2299\n",
    "Labels: ['04', '02', '01', '05', '03']\n",
    "Confusion Matrix:\n",
    "      04  02  01  05  03\n",
    "t:04  23   4  23  50  12\n",
    "t:02  20   1  20  59  14\n",
    "t:01   8   4  24  76  14\n",
    "t:05  13   0  30  71  28\n",
    "t:03  15   2  29  48  21\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.29      0.21      0.24       112\n",
    "          02       0.09      0.01      0.02       114\n",
    "          01       0.19      0.19      0.19       126\n",
    "          05       0.23      0.50      0.32       142\n",
    "          03       0.24      0.18      0.21       115\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.21      0.22      0.19       609\n",
    "weighted avg       0.21      0.23      0.20       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 00:25:00 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 49 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6109996315685204, 1.6072188698012253, 1.606093865701522, 1.6063409528904555, 1.6061873134525342, 1.6056992522209932, 1.6052622879275744, 1.605275571835648, 1.6051217869584784, 1.605370931633196, 1.6050936976089853, 1.6050687822802314, 1.6048405111716886, 1.6047800002231192, 1.604680972929267, 1.6045841589349832, 1.604479886236645, 1.6045024592692434, 1.6042512344022102, 1.604436841895819, 1.6045581065179484, 1.6049943952920598, 1.6051484853371807, 1.6052943401540245, 1.6045661532428661, 1.6038352601438124, 1.605715942891752, 1.6028175802262155, 1.6036119789912784, 1.6028428263656416, 1.6036282186633457, 1.6033527913743444, 1.6041456042056406, 1.6035840532854078, 1.6036814435557976, 1.6042103358285964, 1.6040320952341867, 1.6036519608865623, 1.6035479046832557, 1.6033385252130443, 1.6031615405247128, 1.6038195685604326, 1.603174948731471, 1.6032164406110891, 1.6028672689679024, 1.6028866554520205, 1.603153001890198, 1.6029684197139271, 1.6028589117898926, 1.6021417559465556, 1.6022229458898158, 1.6018653951450716, 1.6017208596559973, 1.6024085950773141, 1.6018838663210815, 1.6023924043417368, 1.6024681552877567, 1.6021836956929298, 1.6022943945353842, 1.602125000875376, 1.6024520673188083, 1.6017866667072564, 1.6021628315225611, 1.601865139500848, 1.6019146712542756, 1.6020114460993675, 1.602266310862524, 1.6026753095178965, 1.6025687736047705, 1.6032039420358066, 1.602767741347377, 1.6016551855162446, 1.6017990952054855, 1.6026023735945252, 1.602435300894363, 1.6009387617627975, 1.601493552009069, 1.5990376026172357, 1.5995255563842448, 1.5984472198830841, 1.6006710036047573, 1.6084282546990807, 1.6078016636602592, 1.6062481767438315, 1.6056293327428632, 1.6055244442277354, 1.6042263928696832, 1.6035408063475134, 1.6029609169670318, 1.6025793019969672, 1.6022076835773262, 1.6026777671084225, 1.602312479504615, 1.6019898472944112, 1.6020818316486278, 1.600939928995956, 1.6003236655139768, 1.6012376623200666, 1.6012760574985998, 1.6004891319227923, 1.6006264954756437, 1.6004645988858979, 1.6011698020894343, 1.6021723520188105, 1.6042424657661927, 1.606470345472076, 1.6060097712797092, 1.6027540428493605, 1.6018508224455985, 1.6027379282589616, 1.6012975793753939, 1.6025294449137546, 1.6026140981902826, 1.6166897798798159, 1.6055556140313987, 1.611138462041595, 1.605788032213847, 1.605851021893506, 1.604738877716127, 1.603790127585087, 1.6037816641170208, 1.6031741751434376, 1.6025814499173845, 1.6020491301328286, 1.601664872592306, 1.601319153907851, 1.6009990840122617, 1.6010228262354784, 1.601539501024193, 1.6012502168786937, 1.6006090087060663, 1.6007906642844916, 1.601099416931666, 1.6007137044114237, 1.6011217961757642, 1.601134156162907, 1.5999954764674647, 1.5998786030144527, 1.5995880320350133, 1.5988238276714957, 1.599210740505964, 1.5981972571860001, 1.5980109863093335, 1.5988185711094898, 1.598027219326038, 1.5994807558106672, 1.5994270799195238, 1.6017272695531986, 1.6010506174638746, 1.6010711678534697, 1.601074277082296, 1.6001588988969675, 1.6003678598623166, 1.601183528383377, 1.6005512387881726, 1.5999194289663154, 1.600609038850944, 1.6011911769610125, 1.6030011190765205, 1.602731910440918, 1.6022012149563367, 1.6051828624383961, 1.608407837416738, 1.6075249839886068, 1.6072245111997883, 1.6078421012521378, 1.607019554022302, 1.6072435099111597, 1.604536186493872, 1.6060049179543807, 1.605068730407552, 1.6044897526160053, 1.6040569917713283, 1.6046815605586386, 1.6052083959328913, 1.602782208148286, 1.6040294430721764, 1.6044355145424654, 1.6040773877173613, 1.6024317441902725, 1.6029271635124445, 1.60026983909419, 1.6039962670681707, 1.6098597039925837, 1.6069830513157084, 1.6059037570295662, 1.6049335308262866, 1.6051335777163702, 1.6036685613184336, 1.6043715641416352, 1.6042624224582915, 1.603027510329812, 1.6031083142620393, 1.60451760061073, 1.6032929874601818, 1.603985310188068, 1.6026753106923721, 1.6017788033963032, 1.6023221323251333, 1.6033000401871154, 1.6019217167386084, 1.6020448170663493, 1.6027588309912846, 1.6028580604907132, 1.6016599501686535, 1.6016639521948026, 1.6019414935401703, 1.602791448336321, 1.6030471845604908, 1.6012134301447125, 1.601935605110206, 1.6020414130441074, 1.6015995979700575, 1.6007811864608614, 1.6008976983710854, 1.6016681415498355, 1.6013486712241212, 1.601171227120022, 1.6005561038582587, 1.6005193952269154, 1.600055187989534, 1.6019215740398038, 1.6035973072443495, 1.6024141429093084, 1.601342002355015, 1.6037770329633565, 1.6047459801625343, 1.6026581018820576, 1.6007559135042388, 1.5986867843590347, 1.6011131496852256, 1.5997509658826004, 1.602148624672287, 1.599958619069192, 1.5999706678202588, 1.6015802375201522, 1.6023999497612513, 1.6012707948684692, 1.601289889495361, 1.600886573736695, 1.6013159127462477, 1.5998031957983383, 1.59929393018995, 1.6016381172515293, 1.6017135534380458, 1.5991499619726672, 1.5998467232401932, 1.6046600866396048, 1.600791652214351, 1.6010202470867114, 1.600179418162955, 1.6002272471222776, 1.600155785753222, 1.6012769499044308, 1.600577692680171, 1.6015664178553866, 1.6010529527131756, 1.6013054986696917, 1.6014705557737052, 1.6010442602223363, 1.6013412575416377, 1.602296423638004, 1.6029245154610996, 1.6029518183033258, 1.6031709860502596, 1.6033885261695373, 1.602950617597608, 1.6029652085014556, 1.6031866377014636, 1.6036789360500516, 1.6048664715881222, 1.6038431344165396, 1.6037370577234353, 1.602564976916133, 1.6026958186050941, 1.602739477000996, 1.6025354024420426, 1.6021916652939394, 1.603165201757146, 1.6025951323642325, 1.6038105023905562, 1.604107741064626, 1.6033287146212825, 1.6040975730407414, 1.601626382672728, 1.6026281069456454, 1.6035317115987267, 1.602550783963822, 1.6017654574367604, 1.6011823795503388, 1.602438455340506, 1.6030496219891828, 1.6007463333054717, 1.601118524673537, 1.602721865541242, 1.6023849299780057, 1.6016776375778399, 1.6017438459083169, 1.6024290164703219, 1.6023722440738397], 'val_acc': [0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.2331691275187118, 0.22988505517124935, 0.23645319976830131, 0.2331691275187118, 0.23973727211576376, 0.2479474527397375, 0.2479474527397375, 0.24958948896240524, 0.2495894890602782, 0.23481116374137953, 0.24630541661494276, 0.24137930814268554, 0.24302134436535328, 0.2528735612119947, 0.25287356111412174, 0.24302134446322624, 0.251231524989327, 0.24794745264186452, 0.24958948886453225, 0.24466338058802098, 0.24302134446322624, 0.251231524989327, 0.24958948886453225, 0.251231524989327, 0.24466338058802098, 0.251231524989327, 0.23809523589309606, 0.251231524989327, 0.24958948886453225, 0.2479474527397375, 0.24958948886453225, 0.251231524989327, 0.24958948886453225, 0.24630541671281572, 0.2479474527397375, 0.2479474527397375, 0.24958948886453225, 0.251231524989327, 0.251231524989327, 0.251231524989327, 0.25287356111412174, 0.23645319996404726, 0.251231524989327, 0.24630541671281572, 0.25287356111412174, 0.24958948886453225, 0.24958948886453225, 0.251231524989327, 0.2479474527397375, 0.24958948886453225, 0.238095236088842, 0.23481116383925252, 0.251231524989327, 0.23481116383925252, 0.23645319996404726, 0.23973727221363675, 0.2561576333637112, 0.23809523599096902, 0.2430213442674803, 0.23481116383925252, 0.24302134436535328, 0.23645319976830131, 0.238095236186715, 0.22495894718835702, 0.22003283861822684, 0.24137930804481256, 0.21839080229768612, 0.2331691275187118, 0.21674876636863732, 0.2282430194379465, 0.2479474527397375, 0.24958948886453225, 0.23481116383925252, 0.251231524989327, 0.2479474527397375, 0.2298850554648683, 0.23481116383925252, 0.251231524989327, 0.24958948886453225, 0.2298850554648683, 0.2348111639371255, 0.238095236088842, 0.251231524989327, 0.23152709158966303, 0.238095236088842, 0.2413793083384315, 0.23481116383925252, 0.24466338078376695, 0.23152709158966303, 0.19211822589140612, 0.2413793082405585, 0.22495894709048403, 0.205254515281256, 0.24630541661494276, 0.2479474527397375, 0.23316912940276668, 0.22003283861822684, 0.22495894699261107, 0.22167487652920345, 0.21839080249343207, 0.21839080249343207, 0.23481116364350654, 0.21839080249343207, 0.22824301934007354, 0.22167487484089454, 0.22331691096568929, 0.22988505556274125, 0.23645320006192025, 0.2430213446589722, 0.23645320006192025, 0.23645320006192025, 0.23481116383925252, 0.24137930843630448, 0.24466338068589397, 0.24466338068589397, 0.23316912781233076, 0.24302134456109922, 0.24466338058802098, 0.2413793083384315, 0.23973727231150974, 0.23973727221363675, 0.22988505556274125, 0.2298850553669953, 0.23152709168753602, 0.22824301924220056, 0.2298850553669953, 0.23645320006192025, 0.23152709149179004, 0.23316912781233076, 0.23316912781233076, 0.24302134446322624, 0.2413793083384315, 0.238095236088842, 0.2364532001597932, 0.23973727221363675, 0.24302134446322624, 0.23645320006192025, 0.24302134446322624, 0.24630541671281572, 0.24302134446322624, 0.24302134446322624, 0.24302134446322624, 0.24794745293548345, 0.238095236186715, 0.2413793083384315, 0.21018062345989427, 0.23152709149179004, 0.23316912969638562, 0.2331691275187118, 0.2331691275187118, 0.23973727231150974, 0.238095236186715, 0.211822659682562, 0.22824301904645458, 0.238095236186715, 0.23809523579522307, 0.22988505734892314, 0.22824301934007354, 0.2430213441696073, 0.24137930853417747, 0.23645319976830131, 0.238095236186715, 0.23645319976830131, 0.23973727221363675, 0.24302134446322624, 0.21674876795907325, 0.2413793082405585, 0.20361247905858829, 0.2036124788628423, 0.23481116383925252, 0.21346469570948376, 0.22988505744679613, 0.22660098509933366, 0.21182265948681606, 0.24302134446322624, 0.23645319986617427, 0.21182265958468902, 0.238095236088842, 0.238095236088842, 0.205254515281256, 0.23973727231150974, 0.2413793083384315, 0.21018062336202128, 0.23645319986617427, 0.23316912959851263, 0.23481116383925252, 0.20361247876496932, 0.23809523599096902, 0.23973727211576376, 0.23316912969638562, 0.2298850552691223, 0.2331691275187118, 0.23645319986617427, 0.24302134456109922, 0.24466338068589397, 0.238095236088842, 0.24302134456109922, 0.24630541671281572, 0.24302134456109922, 0.2413793083384315, 0.22003284060015468, 0.23316912771445777, 0.21346469600310272, 0.23152709149179004, 0.23973727231150974, 0.23481116383925252, 0.22495894709048403, 0.23645319996404726, 0.2331691276165848, 0.23152709149179004, 0.23809523599096902, 0.238095236088842, 0.23973727221363675, 0.22660098331315176, 0.23481116383925252, 0.23809523589309606, 0.22824301904645458, 0.23645319986617427, 0.21674876844843816, 0.22824301904645458, 0.23316912742083884, 0.22003284069802764, 0.23809523589309606, 0.2216748768228224, 0.2282430213220014, 0.23316912950063964, 0.2266009832152788, 0.20361247876496932, 0.2413793082405585, 0.23316912950063964, 0.22495894709048403, 0.23645319986617427, 0.21346469590522973, 0.2545155972389165, 0.24466338039227503, 0.24302134446322624, 0.23973727211576376, 0.23809523599096902, 0.23152709149179004, 0.23973727211576376, 0.21018062355776726, 0.21346469590522973, 0.23645319976830131, 0.22660098301953283, 0.21346469590522973, 0.23645319976830131, 0.2298850553669953, 0.2167487682526922, 0.2266009831174058, 0.2151067323236434, 0.21674876844843816, 0.21182265997618094, 0.20361247925433423, 0.22331691106356227, 0.23152709149179004, 0.21182265997618094, 0.21182265997618094, 0.19704433436366334, 0.2331691276165848, 0.22660098331315176, 0.21182265997618094, 0.23645319996404726, 0.22660098331315176, 0.20197044283592056, 0.23481116374137953, 0.21182265997618094, 0.2331691276165848, 0.21182265997618094, 0.21510673222577043, 0.238095236088842, 0.2036124789607153, 0.23152709149179004, 0.2101806238513862, 0.2036124789607153, 0.23645319976830131, 0.2036124789607153, 0.2101806238513862, 0.2282430194379465, 0.21182265997618094, 0.2331691276165848, 0.21510673203002448, 0.21182265997618094, 0.22988505556274125], 'loss': [1.6141555879639893, 1.6097480527674148, 1.6069996605174008, 1.606096880832492, 1.6057059936210116, 1.6051538439012405, 1.6050901539271862, 1.6046770926128913, 1.6047010764938605, 1.6046471018076434, 1.6045669765198256, 1.6042679467974748, 1.604158460873598, 1.604223635407199, 1.6039510410669158, 1.6036821395709528, 1.6036194182519306, 1.6034010525846383, 1.6031078538855488, 1.6032110313125705, 1.6031804735900441, 1.602561023739574, 1.60230086035797, 1.6022354109820889, 1.60207183439384, 1.602624239960735, 1.6020307022932863, 1.602349243271767, 1.6030413758828166, 1.603638229673648, 1.602964825600814, 1.602602370514762, 1.6030851167330262, 1.602552723933539, 1.6025702218010685, 1.6023237240143136, 1.6019002452278528, 1.6015383281257363, 1.6018475388599371, 1.6023171606005095, 1.6018307715226003, 1.602546964974374, 1.6016590836601337, 1.601613634322948, 1.6015826347672228, 1.6013681158148043, 1.6012727832892102, 1.6013324464860639, 1.601951754754084, 1.6018572896413, 1.6020100808486313, 1.6018624442803542, 1.601473616231883, 1.6014094438885762, 1.601381766869547, 1.6013213613439634, 1.6012164981947787, 1.6008706911388608, 1.6010339326682277, 1.6003549886190427, 1.6001522985571954, 1.6006460186881941, 1.600012225634753, 1.5995514640083548, 1.5993163397179981, 1.599495319566198, 1.5998143808063297, 1.5997363305434553, 1.600288037940462, 1.6001158858716364, 1.5992136346241288, 1.5989046069386068, 1.5985690226055513, 1.5977907112246912, 1.5982855187304457, 1.5980799931030742, 1.5994229549255214, 1.598634987788034, 1.6004161510134625, 1.6005671377789068, 1.600250989308837, 1.6074568451797204, 1.6040475163371657, 1.6052772144517369, 1.6019102286019609, 1.6013597109968902, 1.6018928186115053, 1.6000555998001256, 1.600592861723851, 1.5998558729825814, 1.5995427917406055, 1.6000004591148738, 1.5990649205703265, 1.598908161578482, 1.5981481497292647, 1.5989367290688736, 1.5979333227909567, 1.5982105385351475, 1.5981554079349525, 1.5979016728469724, 1.5987053212199123, 1.5976373672485351, 1.5977439709023038, 1.598904204172765, 1.6027249497799414, 1.5996748808717827, 1.5996885282058246, 1.600498159220576, 1.6002103733086244, 1.598465106619457, 1.598705935184471, 1.5979276596398324, 1.5986949011530476, 1.602139260000272, 1.606372380354566, 1.6021110601248927, 1.6012112987604474, 1.6018503962111423, 1.6018414267279528, 1.601084128836097, 1.600793105131302, 1.5996411660368683, 1.5995694202074524, 1.5992957040269762, 1.5990658293514526, 1.5990078053680044, 1.5986650040996637, 1.5982963576698694, 1.5979286935539951, 1.5976295779617904, 1.5983600981181652, 1.598273411717503, 1.598681669068777, 1.5986902692724303, 1.598666754542435, 1.5985851558571724, 1.598185009682203, 1.597997919881613, 1.598027227399775, 1.598270303219006, 1.5991268349868806, 1.5982654266533665, 1.5984948457878474, 1.5981050008131494, 1.5987207119959337, 1.5976634412820334, 1.5980944238893795, 1.596717529913728, 1.597064902943997, 1.5964240591139274, 1.5965453649693202, 1.5954830764989834, 1.596103629047621, 1.5956187747097603, 1.596763439834485, 1.5953794705059983, 1.5956268758499648, 1.5960889860345109, 1.5946466237367791, 1.5952237979587343, 1.5949377461380538, 1.5959100767817096, 1.5971453355323117, 1.597080964229435, 1.5967006839031557, 1.596343302433006, 1.5974786927323077, 1.5974137854527155, 1.5969233285230289, 1.5956127796329758, 1.596437109667173, 1.5949421804298856, 1.594718244580028, 1.596337519289287, 1.5954225627303857, 1.5950629264177483, 1.595079733360964, 1.594345919454367, 1.5953618819708697, 1.5945776481648, 1.5938623167406119, 1.5934786371137082, 1.5954329342323161, 1.5970497622382225, 1.5969551087404914, 1.5966813196147003, 1.59647334708815, 1.5951472228067856, 1.5950156766041592, 1.5956524716755203, 1.5946770518222628, 1.5950705481260954, 1.594497694225037, 1.5938913448145746, 1.5941686064800442, 1.5945416096544365, 1.5944141753645158, 1.5938721979423225, 1.5942692562295182, 1.5934683580907707, 1.5941222495366905, 1.595527436845846, 1.5957234563768767, 1.5943776943355614, 1.5947510067687143, 1.5943318441907972, 1.5946375465980545, 1.5952847237949255, 1.5950801463097761, 1.5956853562067177, 1.5942831572565945, 1.5943836700744942, 1.5937384156965377, 1.5935916412048026, 1.5939278918370083, 1.5941190214372514, 1.5934425994845631, 1.5932894451662256, 1.5949907901595506, 1.59401594628054, 1.5940900637628606, 1.5944412339149805, 1.5956162193227843, 1.5955847629531452, 1.5945850440853675, 1.5955487202325151, 1.5949752146703262, 1.5948407452209286, 1.5957683305231207, 1.5951829833416478, 1.5953111287749524, 1.5966677640253024, 1.596159875906958, 1.5950573548154419, 1.5939412000487718, 1.5952580915094645, 1.595614886822397, 1.5968521715923991, 1.5947504471459673, 1.5949052724015786, 1.5952315639421435, 1.5940411052175125, 1.5959916798730651, 1.5961465110524233, 1.5935874317705754, 1.5958251882627514, 1.5974344973691434, 1.5956648225901797, 1.5965404407689214, 1.5946399937420166, 1.5945813111945588, 1.5940470387069108, 1.5928982658797466, 1.5934607197861408, 1.5924233007235205, 1.5937246355922314, 1.592664556630583, 1.5929362452740052, 1.5926788431907826, 1.592022245522642, 1.5912294137649223, 1.5912977194639202, 1.5910138626607782, 1.5911973438223774, 1.5912172123636799, 1.5923061298883425, 1.591683299634491, 1.5914299148798479, 1.591600030060911, 1.590450888982299, 1.590161966200482, 1.58996486467992, 1.5901365492622954, 1.5899757394555658, 1.5900472636095553, 1.590956396880336, 1.5913835424662126, 1.59067440850534, 1.589379297830241, 1.5905656967319748, 1.5901709605536176, 1.5896014427991862, 1.590347597535386, 1.5891513836212472, 1.5902058749717853, 1.5898455527773627, 1.589027304522066, 1.5895984140020132, 1.5888865150710152, 1.590114267748729, 1.591982760076895, 1.5905229310969797, 1.589172640718229, 1.5885824635777874, 1.5886429560502697, 1.5885291696329136, 1.5870667703342634, 1.5883432701627822, 1.5874715159805892, 1.5874068292014654], 'acc': [0.20944558428543059, 0.2328542097270856, 0.23285420912124782, 0.23285421010037957, 0.2328542083195837, 0.23285421047367355, 0.23285420851541005, 0.2328542094945418, 0.2328542110611526, 0.23285420890706277, 0.2328542085337688, 0.23285421029620593, 0.23285420851541005, 0.23285421051039099, 0.23285420851541005, 0.23285420851541005, 0.2328542091028891, 0.2328542087112364, 0.2328542094945418, 0.23285420969036816, 0.23613962926414223, 0.23285420931707418, 0.23819301815615543, 0.2431211517087243, 0.242710471171618, 0.23737166270583074, 0.24106776166011176, 0.23449691978935344, 0.23737166329330978, 0.2373716637033212, 0.23901437591967886, 0.2390143739797741, 0.23531827600462482, 0.23696098590778375, 0.23983572882426105, 0.24147843787067969, 0.23901437356976268, 0.23696098606689264, 0.24065708484370604, 0.2435318279560097, 0.2418891176795568, 0.2373716629200158, 0.2381930173544913, 0.24065708560865273, 0.2394250511878325, 0.24188911750208916, 0.2418891176979155, 0.24147843888652887, 0.24394250475405668, 0.24394250617991728, 0.23942505239950804, 0.24188911750208916, 0.24312114937716686, 0.242299796080932, 0.2439425043991214, 0.24271047312988148, 0.24353182815183605, 0.24312115055212494, 0.2402464068156248, 0.24887063738256998, 0.24106776126845905, 0.24024640642397213, 0.2443531818213649, 0.245585214479748, 0.24271047134908563, 0.24353182815183605, 0.244353182623029, 0.23942505099200614, 0.24353182854348873, 0.24147843988401935, 0.24271047174073831, 0.2402464074398213, 0.24147844162809776, 0.24271047174073831, 0.24271047271987006, 0.2472279267144644, 0.23901437456725314, 0.24271047332570783, 0.23613963124076442, 0.2336755649265078, 0.23860369556004016, 0.23244353265977738, 0.22956878798086294, 0.2336755645532138, 0.23696098608525137, 0.23285421031456463, 0.23080082140419272, 0.2406570850395324, 0.2435318283660211, 0.2418891176795568, 0.24188911670042504, 0.23860369499091985, 0.23942505216696425, 0.24229979549345296, 0.2439425067490376, 0.24599589346860223, 0.24517453883830037, 0.23901437435306808, 0.24188911789374185, 0.24106776285342857, 0.24147843927818158, 0.2418891173062628, 0.24353182736853066, 0.23285420970872686, 0.23449692096431152, 0.2398357284142496, 0.24188911593547838, 0.23408624117379315, 0.23613962986998, 0.24147844029403076, 0.234907598386555, 0.2439425055740795, 0.23901437378394774, 0.23367556376990842, 0.23121150003811172, 0.234086243327883, 0.23860369714500967, 0.23285420890706277, 0.23080082218749812, 0.22956878835415692, 0.23490759738906453, 0.23942505218532295, 0.23942505120619123, 0.23696098588942502, 0.23696098508776092, 0.24312114916298178, 0.24229979570763802, 0.24188911750208916, 0.2406570830629102, 0.24229979510180025, 0.24271047291569642, 0.24188911652295741, 0.23572895340850955, 0.23901437552802618, 0.23942505198949662, 0.24065708325873655, 0.24229979570763802, 0.24065708639195812, 0.23572895303521557, 0.23942505099200614, 0.23860369694918332, 0.23983572902008737, 0.23819301755031766, 0.24517453903412673, 0.24147843947400793, 0.2418891169146101, 0.24147843829904983, 0.24435318221301758, 0.2447638594394347, 0.2447638610060455, 0.24188911826703582, 0.23983572960756644, 0.24476386081021914, 0.24435318242720266, 0.24147844047149838, 0.24599589248947049, 0.24271047234657608, 0.23819301950858116, 0.24147843829904983, 0.2451745384466477, 0.24312114896715545, 0.24188911750208916, 0.24394250520078553, 0.2435318271727043, 0.2455852166338378, 0.24599589407444, 0.24229979551181166, 0.24599589425190763, 0.2431211503746573, 0.2505133464840648, 0.2472279267144644, 0.2484599585895421, 0.24763860452836053, 0.24763860296174975, 0.245174539015768, 0.2406570830629102, 0.25133470328681523, 0.24763860395924023, 0.24188911750208916, 0.2459958928994819, 0.24722792694700818, 0.2492813150006398, 0.24065708404204195, 0.2353182744196553, 0.24188911885451486, 0.25010266886599497, 0.23901437513637347, 0.25010267141173753, 0.24147844025731333, 0.24476386043692516, 0.24517453883830037, 0.244353183583802, 0.24763860276592342, 0.24722792612698533, 0.247227925931159, 0.24517453805499498, 0.24599589464356034, 0.24394250598409092, 0.24229979551181166, 0.2459958932911346, 0.24229979510180025, 0.2529774137590945, 0.23942505218532295, 0.2410677626576022, 0.2468172479397952, 0.24476386081021914, 0.24435318060968936, 0.23942505179367027, 0.24065708600030541, 0.24353182815183605, 0.24353182856184746, 0.23655030844882283, 0.2443531847587601, 0.24599589268529684, 0.24804928077564592, 0.24394250402582746, 0.24394250521914426, 0.2517453794973832, 0.2435318274052481, 0.24065708406040065, 0.24271047213239103, 0.24394250420329508, 0.24599589427026636, 0.23901437259063094, 0.24147843869070254, 0.24271047234657608, 0.24106776246177586, 0.24147843949236664, 0.2451745394074207, 0.24312114916298178, 0.2402464058548518, 0.24312115092541892, 0.2476386039408815, 0.24763860276592342, 0.2427104727382288, 0.24640657110503078, 0.24229979549345296, 0.24271047136744434, 0.24312115094377765, 0.24353182815183605, 0.24722792788942247, 0.24435318221301758, 0.24722792413200442, 0.2422997941226685, 0.2394250507961798, 0.23696098508776092, 0.23983572782677057, 0.2468172487231006, 0.2390143747630795, 0.24065708466623842, 0.24271047232821738, 0.24928131597977154, 0.24763860452836053, 0.24640656971588762, 0.24763860413670785, 0.2554414784883816, 0.24558521565470606, 0.2501026704509645, 0.25010266827851596, 0.24804928292973574, 0.25174538129653773, 0.25133470172020445, 0.246817249506406, 0.25667351055928567, 0.2554414790758607, 0.24558521586889115, 0.24722792573533264, 0.2501026692576477, 0.24804928175477767, 0.2468172475297838, 0.2496919926187096, 0.2505133476590229, 0.2517453775391197, 0.24887063640343823, 0.24969199066044614, 0.25133470328681523, 0.2476386051158396, 0.24928131521482488, 0.2501026712159112, 0.2488706369909173, 0.25010267023677946, 0.2464065708908457, 0.23696098569359866, 0.241478439688193, 0.24517453803663625, 0.2455852164563702, 0.2472279259495177, 0.2521560593062602, 0.252156056368865, 0.24722792612698533, 0.24969199281453597, 0.24147843869070254, 0.24804928175477767, 0.2451745386241153, 0.24887063738256998, 0.2505133474815553, 0.24229979232351392, 0.2496919912662839, 0.25051334628823846, 0.2492813142173344]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
