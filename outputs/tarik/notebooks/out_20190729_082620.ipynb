{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf86.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 08:26:20 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '3Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '05', '01', '04', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000195003CBE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001955AF46EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6069, Accuracy:0.2238, Validation Loss:1.6072, Validation Accuracy:0.2348\n",
    "Epoch #2: Loss:1.6064, Accuracy:0.2333, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6025, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6019, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6019, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2381\n",
    "Epoch #17: Loss:1.6018, Accuracy:0.2398, Validation Loss:1.6051, Validation Accuracy:0.2397\n",
    "Epoch #18: Loss:1.6015, Accuracy:0.2382, Validation Loss:1.6048, Validation Accuracy:0.2381\n",
    "Epoch #19: Loss:1.6014, Accuracy:0.2394, Validation Loss:1.6049, Validation Accuracy:0.2397\n",
    "Epoch #20: Loss:1.6012, Accuracy:0.2407, Validation Loss:1.6051, Validation Accuracy:0.2414\n",
    "Epoch #21: Loss:1.6017, Accuracy:0.2452, Validation Loss:1.6048, Validation Accuracy:0.2430\n",
    "Epoch #22: Loss:1.6017, Accuracy:0.2439, Validation Loss:1.6055, Validation Accuracy:0.2430\n",
    "Epoch #23: Loss:1.6015, Accuracy:0.2444, Validation Loss:1.6052, Validation Accuracy:0.2430\n",
    "Epoch #24: Loss:1.6017, Accuracy:0.2423, Validation Loss:1.6050, Validation Accuracy:0.2397\n",
    "Epoch #25: Loss:1.6016, Accuracy:0.2407, Validation Loss:1.6045, Validation Accuracy:0.2463\n",
    "Epoch #26: Loss:1.6018, Accuracy:0.2320, Validation Loss:1.6043, Validation Accuracy:0.2430\n",
    "Epoch #27: Loss:1.6006, Accuracy:0.2439, Validation Loss:1.6064, Validation Accuracy:0.2512\n",
    "Epoch #28: Loss:1.6012, Accuracy:0.2394, Validation Loss:1.6046, Validation Accuracy:0.2430\n",
    "Epoch #29: Loss:1.6012, Accuracy:0.2411, Validation Loss:1.6054, Validation Accuracy:0.2397\n",
    "Epoch #30: Loss:1.6013, Accuracy:0.2411, Validation Loss:1.6051, Validation Accuracy:0.2496\n",
    "Epoch #31: Loss:1.6008, Accuracy:0.2480, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #32: Loss:1.6026, Accuracy:0.2333, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #33: Loss:1.6025, Accuracy:0.2353, Validation Loss:1.6043, Validation Accuracy:0.2430\n",
    "Epoch #34: Loss:1.6023, Accuracy:0.2370, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #35: Loss:1.6021, Accuracy:0.2357, Validation Loss:1.6044, Validation Accuracy:0.2397\n",
    "Epoch #36: Loss:1.6014, Accuracy:0.2357, Validation Loss:1.6042, Validation Accuracy:0.2430\n",
    "Epoch #37: Loss:1.6010, Accuracy:0.2378, Validation Loss:1.6047, Validation Accuracy:0.2381\n",
    "Epoch #38: Loss:1.6013, Accuracy:0.2402, Validation Loss:1.6035, Validation Accuracy:0.2479\n",
    "Epoch #39: Loss:1.6007, Accuracy:0.2444, Validation Loss:1.6043, Validation Accuracy:0.2430\n",
    "Epoch #40: Loss:1.6005, Accuracy:0.2407, Validation Loss:1.6037, Validation Accuracy:0.2447\n",
    "Epoch #41: Loss:1.6014, Accuracy:0.2382, Validation Loss:1.6050, Validation Accuracy:0.2315\n",
    "Epoch #42: Loss:1.6002, Accuracy:0.2460, Validation Loss:1.6044, Validation Accuracy:0.2447\n",
    "Epoch #43: Loss:1.6006, Accuracy:0.2419, Validation Loss:1.6046, Validation Accuracy:0.2430\n",
    "Epoch #44: Loss:1.6005, Accuracy:0.2419, Validation Loss:1.6041, Validation Accuracy:0.2447\n",
    "Epoch #45: Loss:1.5999, Accuracy:0.2398, Validation Loss:1.6043, Validation Accuracy:0.2447\n",
    "Epoch #46: Loss:1.5993, Accuracy:0.2444, Validation Loss:1.6043, Validation Accuracy:0.2447\n",
    "Epoch #47: Loss:1.5997, Accuracy:0.2444, Validation Loss:1.6043, Validation Accuracy:0.2430\n",
    "Epoch #48: Loss:1.5996, Accuracy:0.2427, Validation Loss:1.6040, Validation Accuracy:0.2430\n",
    "Epoch #49: Loss:1.5995, Accuracy:0.2439, Validation Loss:1.6043, Validation Accuracy:0.2414\n",
    "Epoch #50: Loss:1.5994, Accuracy:0.2394, Validation Loss:1.6042, Validation Accuracy:0.2463\n",
    "Epoch #51: Loss:1.5991, Accuracy:0.2435, Validation Loss:1.6042, Validation Accuracy:0.2430\n",
    "Epoch #52: Loss:1.5993, Accuracy:0.2419, Validation Loss:1.6045, Validation Accuracy:0.2430\n",
    "Epoch #53: Loss:1.5995, Accuracy:0.2439, Validation Loss:1.6043, Validation Accuracy:0.2430\n",
    "Epoch #54: Loss:1.5997, Accuracy:0.2411, Validation Loss:1.6058, Validation Accuracy:0.2381\n",
    "Epoch #55: Loss:1.5997, Accuracy:0.2431, Validation Loss:1.6046, Validation Accuracy:0.2414\n",
    "Epoch #56: Loss:1.5998, Accuracy:0.2407, Validation Loss:1.6039, Validation Accuracy:0.2463\n",
    "Epoch #57: Loss:1.5992, Accuracy:0.2444, Validation Loss:1.6044, Validation Accuracy:0.2447\n",
    "Epoch #58: Loss:1.5989, Accuracy:0.2448, Validation Loss:1.6040, Validation Accuracy:0.2414\n",
    "Epoch #59: Loss:1.5985, Accuracy:0.2439, Validation Loss:1.6045, Validation Accuracy:0.2447\n",
    "Epoch #60: Loss:1.5991, Accuracy:0.2456, Validation Loss:1.6041, Validation Accuracy:0.2447\n",
    "Epoch #61: Loss:1.5990, Accuracy:0.2435, Validation Loss:1.6041, Validation Accuracy:0.2414\n",
    "Epoch #62: Loss:1.5991, Accuracy:0.2448, Validation Loss:1.6042, Validation Accuracy:0.2447\n",
    "Epoch #63: Loss:1.5990, Accuracy:0.2423, Validation Loss:1.6042, Validation Accuracy:0.2397\n",
    "Epoch #64: Loss:1.5994, Accuracy:0.2394, Validation Loss:1.6040, Validation Accuracy:0.2463\n",
    "Epoch #65: Loss:1.5988, Accuracy:0.2423, Validation Loss:1.6031, Validation Accuracy:0.2447\n",
    "Epoch #66: Loss:1.5992, Accuracy:0.2452, Validation Loss:1.6033, Validation Accuracy:0.2430\n",
    "Epoch #67: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6034, Validation Accuracy:0.2430\n",
    "Epoch #68: Loss:1.5983, Accuracy:0.2419, Validation Loss:1.6036, Validation Accuracy:0.2430\n",
    "Epoch #69: Loss:1.5985, Accuracy:0.2390, Validation Loss:1.6029, Validation Accuracy:0.2463\n",
    "Epoch #70: Loss:1.5973, Accuracy:0.2407, Validation Loss:1.6038, Validation Accuracy:0.2463\n",
    "Epoch #71: Loss:1.5983, Accuracy:0.2439, Validation Loss:1.6074, Validation Accuracy:0.2348\n",
    "Epoch #72: Loss:1.5980, Accuracy:0.2460, Validation Loss:1.6045, Validation Accuracy:0.2447\n",
    "Epoch #73: Loss:1.5977, Accuracy:0.2476, Validation Loss:1.6036, Validation Accuracy:0.2315\n",
    "Epoch #74: Loss:1.5972, Accuracy:0.2448, Validation Loss:1.6049, Validation Accuracy:0.2315\n",
    "Epoch #75: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.6045, Validation Accuracy:0.2381\n",
    "Epoch #76: Loss:1.5969, Accuracy:0.2468, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #77: Loss:1.5972, Accuracy:0.2427, Validation Loss:1.6045, Validation Accuracy:0.2463\n",
    "Epoch #78: Loss:1.5969, Accuracy:0.2489, Validation Loss:1.6035, Validation Accuracy:0.2430\n",
    "Epoch #79: Loss:1.5962, Accuracy:0.2476, Validation Loss:1.6035, Validation Accuracy:0.2233\n",
    "Epoch #80: Loss:1.5969, Accuracy:0.2402, Validation Loss:1.6043, Validation Accuracy:0.2266\n",
    "Epoch #81: Loss:1.5962, Accuracy:0.2423, Validation Loss:1.6053, Validation Accuracy:0.2414\n",
    "Epoch #82: Loss:1.5967, Accuracy:0.2456, Validation Loss:1.6037, Validation Accuracy:0.2348\n",
    "Epoch #83: Loss:1.5964, Accuracy:0.2448, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #84: Loss:1.5947, Accuracy:0.2476, Validation Loss:1.6069, Validation Accuracy:0.2348\n",
    "Epoch #85: Loss:1.5962, Accuracy:0.2485, Validation Loss:1.6054, Validation Accuracy:0.2381\n",
    "Epoch #86: Loss:1.5950, Accuracy:0.2452, Validation Loss:1.6057, Validation Accuracy:0.2365\n",
    "Epoch #87: Loss:1.5952, Accuracy:0.2448, Validation Loss:1.6051, Validation Accuracy:0.2315\n",
    "Epoch #88: Loss:1.5945, Accuracy:0.2444, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #89: Loss:1.5946, Accuracy:0.2476, Validation Loss:1.6050, Validation Accuracy:0.2381\n",
    "Epoch #90: Loss:1.5952, Accuracy:0.2435, Validation Loss:1.6062, Validation Accuracy:0.2332\n",
    "Epoch #91: Loss:1.5956, Accuracy:0.2448, Validation Loss:1.6058, Validation Accuracy:0.2282\n",
    "Epoch #92: Loss:1.5952, Accuracy:0.2485, Validation Loss:1.6055, Validation Accuracy:0.2299\n",
    "Epoch #93: Loss:1.5954, Accuracy:0.2444, Validation Loss:1.6052, Validation Accuracy:0.2397\n",
    "Epoch #94: Loss:1.5947, Accuracy:0.2431, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #95: Loss:1.5942, Accuracy:0.2505, Validation Loss:1.6036, Validation Accuracy:0.2414\n",
    "Epoch #96: Loss:1.5950, Accuracy:0.2456, Validation Loss:1.6054, Validation Accuracy:0.2299\n",
    "Epoch #97: Loss:1.5969, Accuracy:0.2472, Validation Loss:1.6057, Validation Accuracy:0.2348\n",
    "Epoch #98: Loss:1.5960, Accuracy:0.2464, Validation Loss:1.6058, Validation Accuracy:0.2299\n",
    "Epoch #99: Loss:1.5958, Accuracy:0.2493, Validation Loss:1.6094, Validation Accuracy:0.2233\n",
    "Epoch #100: Loss:1.5957, Accuracy:0.2464, Validation Loss:1.6057, Validation Accuracy:0.2315\n",
    "Epoch #101: Loss:1.5964, Accuracy:0.2444, Validation Loss:1.6054, Validation Accuracy:0.2414\n",
    "Epoch #102: Loss:1.5938, Accuracy:0.2480, Validation Loss:1.6066, Validation Accuracy:0.2299\n",
    "Epoch #103: Loss:1.5942, Accuracy:0.2505, Validation Loss:1.6057, Validation Accuracy:0.2414\n",
    "Epoch #104: Loss:1.5948, Accuracy:0.2444, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #105: Loss:1.5947, Accuracy:0.2444, Validation Loss:1.6062, Validation Accuracy:0.2315\n",
    "Epoch #106: Loss:1.5952, Accuracy:0.2468, Validation Loss:1.6057, Validation Accuracy:0.2397\n",
    "Epoch #107: Loss:1.5948, Accuracy:0.2464, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #108: Loss:1.5953, Accuracy:0.2419, Validation Loss:1.6066, Validation Accuracy:0.2315\n",
    "Epoch #109: Loss:1.5952, Accuracy:0.2476, Validation Loss:1.6058, Validation Accuracy:0.2365\n",
    "Epoch #110: Loss:1.5954, Accuracy:0.2476, Validation Loss:1.6057, Validation Accuracy:0.2250\n",
    "Epoch #111: Loss:1.5971, Accuracy:0.2435, Validation Loss:1.6068, Validation Accuracy:0.2266\n",
    "Epoch #112: Loss:1.5982, Accuracy:0.2366, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #113: Loss:1.6016, Accuracy:0.2411, Validation Loss:1.6170, Validation Accuracy:0.2217\n",
    "Epoch #114: Loss:1.6014, Accuracy:0.2394, Validation Loss:1.6054, Validation Accuracy:0.2299\n",
    "Epoch #115: Loss:1.5996, Accuracy:0.2444, Validation Loss:1.6030, Validation Accuracy:0.2250\n",
    "Epoch #116: Loss:1.5989, Accuracy:0.2370, Validation Loss:1.6069, Validation Accuracy:0.2233\n",
    "Epoch #117: Loss:1.5998, Accuracy:0.2353, Validation Loss:1.6043, Validation Accuracy:0.2217\n",
    "Epoch #118: Loss:1.5996, Accuracy:0.2411, Validation Loss:1.6058, Validation Accuracy:0.2315\n",
    "Epoch #119: Loss:1.6000, Accuracy:0.2357, Validation Loss:1.6033, Validation Accuracy:0.2315\n",
    "Epoch #120: Loss:1.5975, Accuracy:0.2435, Validation Loss:1.6037, Validation Accuracy:0.2315\n",
    "Epoch #121: Loss:1.5973, Accuracy:0.2427, Validation Loss:1.6029, Validation Accuracy:0.2266\n",
    "Epoch #122: Loss:1.5963, Accuracy:0.2382, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #123: Loss:1.5962, Accuracy:0.2435, Validation Loss:1.6027, Validation Accuracy:0.2479\n",
    "Epoch #124: Loss:1.5961, Accuracy:0.2468, Validation Loss:1.6026, Validation Accuracy:0.2282\n",
    "Epoch #125: Loss:1.5955, Accuracy:0.2431, Validation Loss:1.6029, Validation Accuracy:0.2315\n",
    "Epoch #126: Loss:1.5958, Accuracy:0.2472, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #127: Loss:1.5958, Accuracy:0.2423, Validation Loss:1.6030, Validation Accuracy:0.2282\n",
    "Epoch #128: Loss:1.5956, Accuracy:0.2439, Validation Loss:1.6022, Validation Accuracy:0.2447\n",
    "Epoch #129: Loss:1.5954, Accuracy:0.2485, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #130: Loss:1.5949, Accuracy:0.2431, Validation Loss:1.6032, Validation Accuracy:0.2365\n",
    "Epoch #131: Loss:1.5948, Accuracy:0.2468, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #132: Loss:1.5951, Accuracy:0.2476, Validation Loss:1.6039, Validation Accuracy:0.2266\n",
    "Epoch #133: Loss:1.5951, Accuracy:0.2448, Validation Loss:1.6035, Validation Accuracy:0.2266\n",
    "Epoch #134: Loss:1.5947, Accuracy:0.2460, Validation Loss:1.6040, Validation Accuracy:0.2348\n",
    "Epoch #135: Loss:1.5949, Accuracy:0.2464, Validation Loss:1.6045, Validation Accuracy:0.2348\n",
    "Epoch #136: Loss:1.5945, Accuracy:0.2472, Validation Loss:1.6053, Validation Accuracy:0.2348\n",
    "Epoch #137: Loss:1.5940, Accuracy:0.2435, Validation Loss:1.6054, Validation Accuracy:0.2299\n",
    "Epoch #138: Loss:1.5937, Accuracy:0.2427, Validation Loss:1.6059, Validation Accuracy:0.2266\n",
    "Epoch #139: Loss:1.5937, Accuracy:0.2460, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #140: Loss:1.5939, Accuracy:0.2468, Validation Loss:1.6059, Validation Accuracy:0.2299\n",
    "Epoch #141: Loss:1.5936, Accuracy:0.2460, Validation Loss:1.6061, Validation Accuracy:0.2299\n",
    "Epoch #142: Loss:1.5936, Accuracy:0.2448, Validation Loss:1.6063, Validation Accuracy:0.2299\n",
    "Epoch #143: Loss:1.5932, Accuracy:0.2485, Validation Loss:1.6071, Validation Accuracy:0.2250\n",
    "Epoch #144: Loss:1.5937, Accuracy:0.2419, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #145: Loss:1.5933, Accuracy:0.2480, Validation Loss:1.6072, Validation Accuracy:0.2282\n",
    "Epoch #146: Loss:1.5931, Accuracy:0.2464, Validation Loss:1.6068, Validation Accuracy:0.2365\n",
    "Epoch #147: Loss:1.5925, Accuracy:0.2480, Validation Loss:1.6071, Validation Accuracy:0.2315\n",
    "Epoch #148: Loss:1.5940, Accuracy:0.2472, Validation Loss:1.6071, Validation Accuracy:0.2332\n",
    "Epoch #149: Loss:1.5947, Accuracy:0.2464, Validation Loss:1.6081, Validation Accuracy:0.2282\n",
    "Epoch #150: Loss:1.5926, Accuracy:0.2439, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #151: Loss:1.5930, Accuracy:0.2439, Validation Loss:1.6064, Validation Accuracy:0.2315\n",
    "Epoch #152: Loss:1.5919, Accuracy:0.2435, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #153: Loss:1.5921, Accuracy:0.2452, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #154: Loss:1.5917, Accuracy:0.2464, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #155: Loss:1.5925, Accuracy:0.2423, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #156: Loss:1.5919, Accuracy:0.2476, Validation Loss:1.6083, Validation Accuracy:0.2282\n",
    "Epoch #157: Loss:1.5919, Accuracy:0.2452, Validation Loss:1.6078, Validation Accuracy:0.2332\n",
    "Epoch #158: Loss:1.5924, Accuracy:0.2423, Validation Loss:1.6078, Validation Accuracy:0.2332\n",
    "Epoch #159: Loss:1.5923, Accuracy:0.2444, Validation Loss:1.6083, Validation Accuracy:0.2217\n",
    "Epoch #160: Loss:1.5919, Accuracy:0.2472, Validation Loss:1.6074, Validation Accuracy:0.2315\n",
    "Epoch #161: Loss:1.5914, Accuracy:0.2460, Validation Loss:1.6075, Validation Accuracy:0.2250\n",
    "Epoch #162: Loss:1.5916, Accuracy:0.2460, Validation Loss:1.6074, Validation Accuracy:0.2250\n",
    "Epoch #163: Loss:1.5914, Accuracy:0.2452, Validation Loss:1.6075, Validation Accuracy:0.2299\n",
    "Epoch #164: Loss:1.5913, Accuracy:0.2460, Validation Loss:1.6079, Validation Accuracy:0.2233\n",
    "Epoch #165: Loss:1.5913, Accuracy:0.2489, Validation Loss:1.6083, Validation Accuracy:0.2233\n",
    "Epoch #166: Loss:1.5906, Accuracy:0.2501, Validation Loss:1.6080, Validation Accuracy:0.2282\n",
    "Epoch #167: Loss:1.5907, Accuracy:0.2542, Validation Loss:1.6088, Validation Accuracy:0.2332\n",
    "Epoch #168: Loss:1.5912, Accuracy:0.2480, Validation Loss:1.6087, Validation Accuracy:0.2233\n",
    "Epoch #169: Loss:1.5901, Accuracy:0.2554, Validation Loss:1.6094, Validation Accuracy:0.2397\n",
    "Epoch #170: Loss:1.5907, Accuracy:0.2550, Validation Loss:1.6089, Validation Accuracy:0.2299\n",
    "Epoch #171: Loss:1.5898, Accuracy:0.2546, Validation Loss:1.6082, Validation Accuracy:0.2332\n",
    "Epoch #172: Loss:1.5914, Accuracy:0.2563, Validation Loss:1.6084, Validation Accuracy:0.2299\n",
    "Epoch #173: Loss:1.5926, Accuracy:0.2456, Validation Loss:1.6094, Validation Accuracy:0.2315\n",
    "Epoch #174: Loss:1.5917, Accuracy:0.2559, Validation Loss:1.6108, Validation Accuracy:0.2332\n",
    "Epoch #175: Loss:1.5908, Accuracy:0.2579, Validation Loss:1.6070, Validation Accuracy:0.2397\n",
    "Epoch #176: Loss:1.5907, Accuracy:0.2505, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #177: Loss:1.5904, Accuracy:0.2534, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #178: Loss:1.5896, Accuracy:0.2530, Validation Loss:1.6074, Validation Accuracy:0.2266\n",
    "Epoch #179: Loss:1.5897, Accuracy:0.2591, Validation Loss:1.6073, Validation Accuracy:0.2365\n",
    "Epoch #180: Loss:1.5893, Accuracy:0.2587, Validation Loss:1.6067, Validation Accuracy:0.2348\n",
    "Epoch #181: Loss:1.5895, Accuracy:0.2497, Validation Loss:1.6081, Validation Accuracy:0.2217\n",
    "Epoch #182: Loss:1.5878, Accuracy:0.2554, Validation Loss:1.6072, Validation Accuracy:0.2397\n",
    "Epoch #183: Loss:1.5892, Accuracy:0.2579, Validation Loss:1.6071, Validation Accuracy:0.2397\n",
    "Epoch #184: Loss:1.5886, Accuracy:0.2632, Validation Loss:1.6081, Validation Accuracy:0.2414\n",
    "Epoch #185: Loss:1.5882, Accuracy:0.2628, Validation Loss:1.6071, Validation Accuracy:0.2266\n",
    "Epoch #186: Loss:1.5880, Accuracy:0.2526, Validation Loss:1.6077, Validation Accuracy:0.2200\n",
    "Epoch #187: Loss:1.5879, Accuracy:0.2620, Validation Loss:1.6084, Validation Accuracy:0.2479\n",
    "Epoch #188: Loss:1.5876, Accuracy:0.2579, Validation Loss:1.6073, Validation Accuracy:0.2381\n",
    "Epoch #189: Loss:1.5879, Accuracy:0.2526, Validation Loss:1.6080, Validation Accuracy:0.2512\n",
    "Epoch #190: Loss:1.5875, Accuracy:0.2612, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #191: Loss:1.5872, Accuracy:0.2591, Validation Loss:1.6078, Validation Accuracy:0.2250\n",
    "Epoch #192: Loss:1.5871, Accuracy:0.2485, Validation Loss:1.6080, Validation Accuracy:0.2299\n",
    "Epoch #193: Loss:1.5867, Accuracy:0.2604, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #194: Loss:1.5871, Accuracy:0.2600, Validation Loss:1.6075, Validation Accuracy:0.2529\n",
    "Epoch #195: Loss:1.5866, Accuracy:0.2468, Validation Loss:1.6075, Validation Accuracy:0.2496\n",
    "Epoch #196: Loss:1.5888, Accuracy:0.2542, Validation Loss:1.6076, Validation Accuracy:0.2529\n",
    "Epoch #197: Loss:1.5894, Accuracy:0.2526, Validation Loss:1.6082, Validation Accuracy:0.2315\n",
    "Epoch #198: Loss:1.5861, Accuracy:0.2579, Validation Loss:1.6090, Validation Accuracy:0.2529\n",
    "Epoch #199: Loss:1.5882, Accuracy:0.2542, Validation Loss:1.6063, Validation Accuracy:0.2365\n",
    "Epoch #200: Loss:1.5861, Accuracy:0.2554, Validation Loss:1.6060, Validation Accuracy:0.2529\n",
    "Epoch #201: Loss:1.5863, Accuracy:0.2628, Validation Loss:1.6070, Validation Accuracy:0.2479\n",
    "Epoch #202: Loss:1.5866, Accuracy:0.2501, Validation Loss:1.6068, Validation Accuracy:0.2594\n",
    "Epoch #203: Loss:1.5862, Accuracy:0.2583, Validation Loss:1.6070, Validation Accuracy:0.2529\n",
    "Epoch #204: Loss:1.5859, Accuracy:0.2702, Validation Loss:1.6065, Validation Accuracy:0.2545\n",
    "Epoch #205: Loss:1.5856, Accuracy:0.2608, Validation Loss:1.6068, Validation Accuracy:0.2463\n",
    "Epoch #206: Loss:1.5858, Accuracy:0.2538, Validation Loss:1.6068, Validation Accuracy:0.2463\n",
    "Epoch #207: Loss:1.5850, Accuracy:0.2657, Validation Loss:1.6067, Validation Accuracy:0.2529\n",
    "Epoch #208: Loss:1.5856, Accuracy:0.2649, Validation Loss:1.6063, Validation Accuracy:0.2545\n",
    "Epoch #209: Loss:1.5847, Accuracy:0.2563, Validation Loss:1.6088, Validation Accuracy:0.2562\n",
    "Epoch #210: Loss:1.5863, Accuracy:0.2550, Validation Loss:1.6074, Validation Accuracy:0.2496\n",
    "Epoch #211: Loss:1.5857, Accuracy:0.2645, Validation Loss:1.6068, Validation Accuracy:0.2529\n",
    "Epoch #212: Loss:1.5853, Accuracy:0.2632, Validation Loss:1.6066, Validation Accuracy:0.2529\n",
    "Epoch #213: Loss:1.5862, Accuracy:0.2641, Validation Loss:1.6076, Validation Accuracy:0.2381\n",
    "Epoch #214: Loss:1.5881, Accuracy:0.2452, Validation Loss:1.6054, Validation Accuracy:0.2545\n",
    "Epoch #215: Loss:1.5839, Accuracy:0.2649, Validation Loss:1.6068, Validation Accuracy:0.2496\n",
    "Epoch #216: Loss:1.5844, Accuracy:0.2645, Validation Loss:1.6060, Validation Accuracy:0.2463\n",
    "Epoch #217: Loss:1.5848, Accuracy:0.2509, Validation Loss:1.6055, Validation Accuracy:0.2496\n",
    "Epoch #218: Loss:1.5837, Accuracy:0.2661, Validation Loss:1.6059, Validation Accuracy:0.2529\n",
    "Epoch #219: Loss:1.5839, Accuracy:0.2665, Validation Loss:1.6067, Validation Accuracy:0.2529\n",
    "Epoch #220: Loss:1.5850, Accuracy:0.2591, Validation Loss:1.6068, Validation Accuracy:0.2496\n",
    "Epoch #221: Loss:1.5849, Accuracy:0.2501, Validation Loss:1.6066, Validation Accuracy:0.2594\n",
    "Epoch #222: Loss:1.5849, Accuracy:0.2587, Validation Loss:1.6064, Validation Accuracy:0.2496\n",
    "Epoch #223: Loss:1.5834, Accuracy:0.2661, Validation Loss:1.6060, Validation Accuracy:0.2463\n",
    "Epoch #224: Loss:1.5835, Accuracy:0.2620, Validation Loss:1.6060, Validation Accuracy:0.2594\n",
    "Epoch #225: Loss:1.5844, Accuracy:0.2641, Validation Loss:1.6058, Validation Accuracy:0.2512\n",
    "Epoch #226: Loss:1.5837, Accuracy:0.2624, Validation Loss:1.6068, Validation Accuracy:0.2496\n",
    "Epoch #227: Loss:1.5838, Accuracy:0.2653, Validation Loss:1.6061, Validation Accuracy:0.2529\n",
    "Epoch #228: Loss:1.5842, Accuracy:0.2604, Validation Loss:1.6058, Validation Accuracy:0.2578\n",
    "Epoch #229: Loss:1.5829, Accuracy:0.2604, Validation Loss:1.6063, Validation Accuracy:0.2463\n",
    "Epoch #230: Loss:1.5827, Accuracy:0.2653, Validation Loss:1.6067, Validation Accuracy:0.2430\n",
    "Epoch #231: Loss:1.5845, Accuracy:0.2526, Validation Loss:1.6057, Validation Accuracy:0.2496\n",
    "Epoch #232: Loss:1.5827, Accuracy:0.2649, Validation Loss:1.6051, Validation Accuracy:0.2529\n",
    "Epoch #233: Loss:1.5825, Accuracy:0.2624, Validation Loss:1.6059, Validation Accuracy:0.2348\n",
    "Epoch #234: Loss:1.5818, Accuracy:0.2612, Validation Loss:1.6054, Validation Accuracy:0.2496\n",
    "Epoch #235: Loss:1.5830, Accuracy:0.2649, Validation Loss:1.6055, Validation Accuracy:0.2463\n",
    "Epoch #236: Loss:1.5827, Accuracy:0.2600, Validation Loss:1.6073, Validation Accuracy:0.2447\n",
    "Epoch #237: Loss:1.5826, Accuracy:0.2665, Validation Loss:1.6062, Validation Accuracy:0.2496\n",
    "Epoch #238: Loss:1.5825, Accuracy:0.2665, Validation Loss:1.6056, Validation Accuracy:0.2627\n",
    "Epoch #239: Loss:1.5820, Accuracy:0.2608, Validation Loss:1.6060, Validation Accuracy:0.2496\n",
    "Epoch #240: Loss:1.5808, Accuracy:0.2624, Validation Loss:1.6058, Validation Accuracy:0.2578\n",
    "Epoch #241: Loss:1.5818, Accuracy:0.2542, Validation Loss:1.6055, Validation Accuracy:0.2496\n",
    "Epoch #242: Loss:1.5822, Accuracy:0.2665, Validation Loss:1.6057, Validation Accuracy:0.2529\n",
    "Epoch #243: Loss:1.5805, Accuracy:0.2669, Validation Loss:1.6054, Validation Accuracy:0.2529\n",
    "Epoch #244: Loss:1.5811, Accuracy:0.2665, Validation Loss:1.6057, Validation Accuracy:0.2463\n",
    "Epoch #245: Loss:1.5813, Accuracy:0.2661, Validation Loss:1.6071, Validation Accuracy:0.2496\n",
    "Epoch #246: Loss:1.5812, Accuracy:0.2604, Validation Loss:1.6060, Validation Accuracy:0.2627\n",
    "Epoch #247: Loss:1.5816, Accuracy:0.2653, Validation Loss:1.6058, Validation Accuracy:0.2496\n",
    "Epoch #248: Loss:1.5814, Accuracy:0.2604, Validation Loss:1.6058, Validation Accuracy:0.2529\n",
    "Epoch #249: Loss:1.5837, Accuracy:0.2637, Validation Loss:1.6054, Validation Accuracy:0.2496\n",
    "Epoch #250: Loss:1.5837, Accuracy:0.2501, Validation Loss:1.6057, Validation Accuracy:0.2611\n",
    "Epoch #251: Loss:1.5826, Accuracy:0.2620, Validation Loss:1.6058, Validation Accuracy:0.2496\n",
    "Epoch #252: Loss:1.5812, Accuracy:0.2641, Validation Loss:1.6053, Validation Accuracy:0.2529\n",
    "Epoch #253: Loss:1.5812, Accuracy:0.2637, Validation Loss:1.6043, Validation Accuracy:0.2463\n",
    "Epoch #254: Loss:1.5810, Accuracy:0.2649, Validation Loss:1.6051, Validation Accuracy:0.2496\n",
    "Epoch #255: Loss:1.5811, Accuracy:0.2653, Validation Loss:1.6058, Validation Accuracy:0.2512\n",
    "Epoch #256: Loss:1.5812, Accuracy:0.2678, Validation Loss:1.6049, Validation Accuracy:0.2529\n",
    "Epoch #257: Loss:1.5815, Accuracy:0.2669, Validation Loss:1.6056, Validation Accuracy:0.2529\n",
    "Epoch #258: Loss:1.5819, Accuracy:0.2600, Validation Loss:1.6080, Validation Accuracy:0.2479\n",
    "Epoch #259: Loss:1.5824, Accuracy:0.2612, Validation Loss:1.6047, Validation Accuracy:0.2627\n",
    "Epoch #260: Loss:1.5815, Accuracy:0.2645, Validation Loss:1.6055, Validation Accuracy:0.2348\n",
    "Epoch #261: Loss:1.5812, Accuracy:0.2669, Validation Loss:1.6071, Validation Accuracy:0.2496\n",
    "Epoch #262: Loss:1.5810, Accuracy:0.2608, Validation Loss:1.6061, Validation Accuracy:0.2529\n",
    "Epoch #263: Loss:1.5817, Accuracy:0.2657, Validation Loss:1.6054, Validation Accuracy:0.2496\n",
    "Epoch #264: Loss:1.5806, Accuracy:0.2616, Validation Loss:1.6059, Validation Accuracy:0.2512\n",
    "Epoch #265: Loss:1.5810, Accuracy:0.2637, Validation Loss:1.6063, Validation Accuracy:0.2529\n",
    "Epoch #266: Loss:1.5809, Accuracy:0.2616, Validation Loss:1.6058, Validation Accuracy:0.2479\n",
    "Epoch #267: Loss:1.5803, Accuracy:0.2669, Validation Loss:1.6061, Validation Accuracy:0.2512\n",
    "Epoch #268: Loss:1.5806, Accuracy:0.2620, Validation Loss:1.6062, Validation Accuracy:0.2644\n",
    "Epoch #269: Loss:1.5799, Accuracy:0.2624, Validation Loss:1.6069, Validation Accuracy:0.2512\n",
    "Epoch #270: Loss:1.5800, Accuracy:0.2665, Validation Loss:1.6056, Validation Accuracy:0.2529\n",
    "Epoch #271: Loss:1.5804, Accuracy:0.2616, Validation Loss:1.6053, Validation Accuracy:0.2512\n",
    "Epoch #272: Loss:1.5801, Accuracy:0.2657, Validation Loss:1.6052, Validation Accuracy:0.2479\n",
    "Epoch #273: Loss:1.5799, Accuracy:0.2624, Validation Loss:1.6053, Validation Accuracy:0.2529\n",
    "Epoch #274: Loss:1.5802, Accuracy:0.2653, Validation Loss:1.6061, Validation Accuracy:0.2512\n",
    "Epoch #275: Loss:1.5802, Accuracy:0.2649, Validation Loss:1.6063, Validation Accuracy:0.2512\n",
    "Epoch #276: Loss:1.5795, Accuracy:0.2661, Validation Loss:1.6057, Validation Accuracy:0.2529\n",
    "Epoch #277: Loss:1.5795, Accuracy:0.2653, Validation Loss:1.6050, Validation Accuracy:0.2496\n",
    "Epoch #278: Loss:1.5794, Accuracy:0.2608, Validation Loss:1.6053, Validation Accuracy:0.2512\n",
    "Epoch #279: Loss:1.5790, Accuracy:0.2727, Validation Loss:1.6053, Validation Accuracy:0.2529\n",
    "Epoch #280: Loss:1.5799, Accuracy:0.2645, Validation Loss:1.6060, Validation Accuracy:0.2529\n",
    "Epoch #281: Loss:1.5794, Accuracy:0.2678, Validation Loss:1.6061, Validation Accuracy:0.2512\n",
    "Epoch #282: Loss:1.5785, Accuracy:0.2678, Validation Loss:1.6058, Validation Accuracy:0.2562\n",
    "Epoch #283: Loss:1.5793, Accuracy:0.2657, Validation Loss:1.6060, Validation Accuracy:0.2529\n",
    "Epoch #284: Loss:1.5805, Accuracy:0.2669, Validation Loss:1.6058, Validation Accuracy:0.2529\n",
    "Epoch #285: Loss:1.5799, Accuracy:0.2653, Validation Loss:1.6076, Validation Accuracy:0.2299\n",
    "Epoch #286: Loss:1.5801, Accuracy:0.2690, Validation Loss:1.6071, Validation Accuracy:0.2512\n",
    "Epoch #287: Loss:1.5796, Accuracy:0.2682, Validation Loss:1.6071, Validation Accuracy:0.2463\n",
    "Epoch #288: Loss:1.5790, Accuracy:0.2661, Validation Loss:1.6053, Validation Accuracy:0.2512\n",
    "Epoch #289: Loss:1.5804, Accuracy:0.2694, Validation Loss:1.6051, Validation Accuracy:0.2479\n",
    "Epoch #290: Loss:1.5800, Accuracy:0.2653, Validation Loss:1.6075, Validation Accuracy:0.2463\n",
    "Epoch #291: Loss:1.5794, Accuracy:0.2612, Validation Loss:1.6053, Validation Accuracy:0.2512\n",
    "Epoch #292: Loss:1.5808, Accuracy:0.2649, Validation Loss:1.6051, Validation Accuracy:0.2463\n",
    "Epoch #293: Loss:1.5790, Accuracy:0.2698, Validation Loss:1.6053, Validation Accuracy:0.2512\n",
    "Epoch #294: Loss:1.5804, Accuracy:0.2595, Validation Loss:1.6039, Validation Accuracy:0.2512\n",
    "Epoch #295: Loss:1.5787, Accuracy:0.2690, Validation Loss:1.6049, Validation Accuracy:0.2512\n",
    "Epoch #296: Loss:1.5787, Accuracy:0.2674, Validation Loss:1.6046, Validation Accuracy:0.2430\n",
    "Epoch #297: Loss:1.5787, Accuracy:0.2653, Validation Loss:1.6049, Validation Accuracy:0.2496\n",
    "Epoch #298: Loss:1.5784, Accuracy:0.2624, Validation Loss:1.6046, Validation Accuracy:0.2430\n",
    "Epoch #299: Loss:1.5785, Accuracy:0.2587, Validation Loss:1.6045, Validation Accuracy:0.2447\n",
    "Epoch #300: Loss:1.5786, Accuracy:0.2649, Validation Loss:1.6052, Validation Accuracy:0.2529\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60516286, Accuracy:0.2529\n",
    "Labels: ['03', '05', '01', '04', '02']\n",
    "Confusion Matrix:\n",
    "      03  05  01  04  02\n",
    "t:03  26  46  14  25   4\n",
    "t:05  24  81  10  27   0\n",
    "t:01  13  68  15  29   1\n",
    "t:04  17  54  11  29   1\n",
    "t:02  24  56  10  21   3\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.25      0.23      0.24       115\n",
    "          05       0.27      0.57      0.36       142\n",
    "          01       0.25      0.12      0.16       126\n",
    "          04       0.22      0.26      0.24       112\n",
    "          02       0.33      0.03      0.05       114\n",
    "\n",
    "    accuracy                           0.25       609\n",
    "   macro avg       0.26      0.24      0.21       609\n",
    "weighted avg       0.26      0.25      0.22       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 09:06:47 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 26 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6071579127476132, 1.6063786472984527, 1.6065141416731334, 1.6059961099734252, 1.6056337654101243, 1.605342428476744, 1.605058612494633, 1.604925338270629, 1.6047288563059665, 1.6048368517009692, 1.6047909250008845, 1.604876532147475, 1.604964768162306, 1.6048506030503948, 1.604960599947837, 1.604984910030083, 1.6050954231096215, 1.6047672254502872, 1.604856076890416, 1.605072771387147, 1.6047844074434052, 1.6055265704203514, 1.605242795740638, 1.6049937538325494, 1.6044776606050815, 1.6043052236826354, 1.6063888632800976, 1.604562160221031, 1.6053796045494393, 1.6051438144470866, 1.6043554628619616, 1.6043394682638359, 1.6042903370066426, 1.6040808733656684, 1.604358517673411, 1.6042404630892775, 1.6047116529765388, 1.603537502547203, 1.6042844256744009, 1.6036985611484946, 1.604972252900573, 1.6044007627834826, 1.6045772440132053, 1.6041442236093857, 1.6042697038165064, 1.6042649260491182, 1.6042917506839647, 1.6040211534265227, 1.6043068857615805, 1.6041704594403847, 1.6041762372738817, 1.6045152040929433, 1.6043487977120285, 1.6058025814238048, 1.6045576731363933, 1.6038787229895004, 1.6043662827198923, 1.603976689731742, 1.6044862563974165, 1.604080846548472, 1.6040527391903505, 1.6041681414167281, 1.6042248396254917, 1.60396462669122, 1.6031306674719248, 1.6033319851447796, 1.60340765563921, 1.6035629612667415, 1.6028860345458358, 1.6037586555496617, 1.6073620125577954, 1.6044608421122108, 1.6035877661947742, 1.6049114747587683, 1.6045372030026415, 1.6041140192247965, 1.6045122117244552, 1.6035043016834603, 1.6035032231232216, 1.6043203333132765, 1.6052647975865255, 1.6036805985204887, 1.606427598077871, 1.606901072124738, 1.6054299813381752, 1.6056594018669943, 1.605065122222274, 1.6052403951121865, 1.6049827489946864, 1.6061659894749056, 1.6058145330848757, 1.6054532954649776, 1.605245345919003, 1.6053410862466972, 1.603584778328443, 1.6053889647297475, 1.6056981049539225, 1.6058189925693331, 1.6093915168483466, 1.6056959742591494, 1.6053971825366342, 1.6065959934334841, 1.6057317787399041, 1.6059766232673758, 1.606234897337915, 1.6057041897170845, 1.607139157935708, 1.606613111809165, 1.6057785951053762, 1.60572579243696, 1.6068055052279644, 1.60329742365087, 1.6170201360298495, 1.6054447198344766, 1.6029740474102727, 1.6068676723831001, 1.6043234096567816, 1.6057559271359874, 1.6033114149848424, 1.6036731511697002, 1.602860830883283, 1.6021852201624653, 1.6026723410304153, 1.602629679960179, 1.602883565993536, 1.60293233433772, 1.6029945371186205, 1.6022209768812057, 1.602762129506454, 1.6032017864812966, 1.604090749336581, 1.6038719061364486, 1.6034925404832086, 1.6039968427570386, 1.6044955237941398, 1.6052552224771535, 1.6053924766080132, 1.6058770743105408, 1.6052579811249656, 1.605854001147015, 1.6060572866540042, 1.6063458109136872, 1.6071176728591543, 1.6069336988655805, 1.6072375245869452, 1.606778923905346, 1.6071446628993369, 1.6071007882041493, 1.6081146843523424, 1.60715252718902, 1.6064473641134052, 1.606711404664176, 1.6071487490962488, 1.6068376310548955, 1.6072281252574452, 1.6082553499437906, 1.607804170774513, 1.6077692101545913, 1.6082783774985077, 1.6074092914709708, 1.6074805537663852, 1.6074479056892332, 1.6074615564252355, 1.607870735558383, 1.6083170862620688, 1.6080364417559996, 1.6087602069616709, 1.6087342894136025, 1.6093945395574585, 1.6089331530193585, 1.6082233446963707, 1.608353229187588, 1.609441222424186, 1.6107785827029124, 1.6070426344284283, 1.6065570787451733, 1.6067483840121817, 1.6073875760014225, 1.6072627957818544, 1.6066997200005944, 1.6080749741524507, 1.6071616745934698, 1.6070627988069908, 1.6081023731059434, 1.6071416627010102, 1.6076558472096234, 1.60840092445242, 1.6073464546689062, 1.608042125239944, 1.6076388114387374, 1.6077575423251624, 1.6080171253489353, 1.607564374731092, 1.607454849777159, 1.6075222151620048, 1.607564389803531, 1.6081842865262712, 1.609036620027326, 1.6063033761257803, 1.6060160639250807, 1.6069941581372165, 1.606783545271712, 1.606971859148962, 1.6065495942026524, 1.6068061813345096, 1.606847536779194, 1.6066738983680462, 1.606282207374698, 1.6087521212833074, 1.6074263378121387, 1.606779806915371, 1.6065773038049833, 1.6076471762508397, 1.6053578722457384, 1.606829417162928, 1.606048437762143, 1.605517483501403, 1.6058835161143337, 1.6066859745235473, 1.6067999016083716, 1.606563530923502, 1.6064220788247872, 1.606004560130766, 1.606015481189358, 1.6058080681830595, 1.6068218513858339, 1.606096819899548, 1.6058038737386318, 1.6062987717893127, 1.6067021801358177, 1.6056939398713888, 1.6050503326362773, 1.605932409148694, 1.6053844292958577, 1.6055030948031321, 1.6072718339600587, 1.6062317642280817, 1.6056108666562485, 1.6059937817709786, 1.6058462315983764, 1.6054808294831826, 1.605700671183456, 1.6053601843970162, 1.6056923858442134, 1.6071392282085075, 1.6060040351401017, 1.6057779105817547, 1.605811127888158, 1.6054440173022266, 1.605681773281254, 1.6058078769392568, 1.6052785871064135, 1.6043228848618631, 1.6050872131325733, 1.6057918664857085, 1.60489291921625, 1.6055968007430654, 1.6080366882001629, 1.604664326692841, 1.6055354132440878, 1.607142175163933, 1.6061439465223666, 1.605434774960986, 1.605877702655072, 1.6063283991148123, 1.6057853675240954, 1.606139488408131, 1.6061778383693477, 1.6068670745749387, 1.605611732244883, 1.6053419610353918, 1.6052329743828484, 1.6053475325526472, 1.6061100689648407, 1.6062937101902828, 1.605729473048243, 1.605016846766417, 1.6052911500821168, 1.6052884176642632, 1.605962160577132, 1.6060769432675466, 1.605788566991809, 1.6060183486719242, 1.6057902576496643, 1.6075770592650365, 1.6071117288373373, 1.6070816557982872, 1.6053496552218358, 1.6051087492988223, 1.6074747373709342, 1.605340098121092, 1.6050719915352432, 1.605287351044528, 1.60394500726941, 1.6049311346999922, 1.6046152938958655, 1.6048650424468693, 1.604576816308283, 1.604483084334137, 1.6051628160946474], 'val_acc': [0.23481116572330737, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23809523589309606, 0.2397372720178908, 0.23809523589309606, 0.2397372720178908, 0.24137930814268554, 0.24302134436535328, 0.24302134436535328, 0.24302134436535328, 0.2397372720178908, 0.24630541651706978, 0.2430213442674803, 0.2512315250872, 0.2430213442674803, 0.2397372720178908, 0.24958948886453225, 0.23645319967042833, 0.23316912969638562, 0.24302134436535328, 0.2430213442674803, 0.2397372741955646, 0.2430213441696073, 0.23809523599096902, 0.24794745283761047, 0.2430213442674803, 0.24466338039227503, 0.23152709139391706, 0.24466338049014802, 0.2430213442674803, 0.24466338039227503, 0.24466338039227503, 0.24466338039227503, 0.2430213442674803, 0.24302134436535328, 0.24137930814268554, 0.24630541651706978, 0.2430213442674803, 0.2430213442674803, 0.2430213442674803, 0.23809523807076985, 0.2413793082405585, 0.24630541661494276, 0.24466338039227503, 0.24137930814268554, 0.24466338039227503, 0.24466338039227503, 0.2413793082405585, 0.24466338039227503, 0.2397372720178908, 0.24630541661494276, 0.24466338039227503, 0.24302134436535328, 0.2430213442674803, 0.2430213442674803, 0.24630541671281572, 0.24630541661494276, 0.23481116354563358, 0.24466338058802098, 0.23152709139391706, 0.2315270912960441, 0.23809523579522307, 0.24302134436535328, 0.24630541661494276, 0.24302134436535328, 0.22331691067207035, 0.22660098292165984, 0.2413793082405585, 0.23481116354563358, 0.23316912742083884, 0.23481116354563358, 0.23809523579522307, 0.2364532019459751, 0.2315270912960441, 0.23481116354563358, 0.23809523579522307, 0.23316912742083884, 0.22824301904645458, 0.22988505744679613, 0.2397372720178908, 0.23645319976830131, 0.24137930814268554, 0.22988505517124935, 0.23481116582118036, 0.22988505744679613, 0.22331691275187118, 0.23152709357159088, 0.24137930804481256, 0.22988505744679613, 0.24137930804481256, 0.23481116354563358, 0.23152709357159088, 0.2397372719200178, 0.2282430213220014, 0.2315270912960441, 0.23645319976830131, 0.2249589467968651, 0.22660098519720664, 0.2331691275187118, 0.22167487662707644, 0.22988505517124935, 0.2249589489745389, 0.22331691275187118, 0.2216748745472756, 0.2315270912960441, 0.2315270912960441, 0.2315270912960441, 0.22660098292165984, 0.24302134446322624, 0.2479474527397375, 0.22824301904645458, 0.2315270912960441, 0.23316912742083884, 0.22824301904645458, 0.24466338058802098, 0.2331691275187118, 0.23645319976830131, 0.2331691275187118, 0.22660098292165984, 0.22660098519720664, 0.23481116364350654, 0.23481116354563358, 0.23481116582118036, 0.22988505517124935, 0.22660098292165984, 0.23645319967042833, 0.22988505744679613, 0.22988505517124935, 0.22988505517124935, 0.2249589467968651, 0.23316912969638562, 0.2282430213220014, 0.23645319967042833, 0.2315270912960441, 0.23316912969638562, 0.22824301904645458, 0.23316912742083884, 0.2315270912960441, 0.22824301904645458, 0.2282430213220014, 0.23316912742083884, 0.23316912742083884, 0.22824301904645458, 0.23316912742083884, 0.23316912742083884, 0.2216748745472756, 0.2315270912960441, 0.22495894907241187, 0.22495894907241187, 0.22988505517124935, 0.22331691294761713, 0.22331691294761713, 0.22824301904645458, 0.23316912969638562, 0.22331691294761713, 0.2397372719200178, 0.22988505744679613, 0.23316912969638562, 0.22988505734892314, 0.23152709168753602, 0.2331691275187118, 0.2397372719200178, 0.23316912969638562, 0.2282430213220014, 0.22660098509933366, 0.2364532019459751, 0.23481116572330737, 0.2216748746451486, 0.23973727211576376, 0.23973727211576376, 0.24137930814268554, 0.2266009831174058, 0.22003283852035385, 0.24794745303335644, 0.23809523589309606, 0.251231524891454, 0.23645320006192025, 0.22495894689473808, 0.2298850554648683, 0.23645320006192025, 0.2528735613098677, 0.24958948896240524, 0.2528735614077407, 0.23152709158966303, 0.2528735614077407, 0.23645319996404726, 0.2528735613098677, 0.24794745283761047, 0.25944170600479266, 0.2528735614077407, 0.25451559743466246, 0.24630541671281572, 0.2463054168106887, 0.2528735613098677, 0.25451559743466246, 0.2561576337552032, 0.24958948915815118, 0.2528735613098677, 0.2528735613098677, 0.23809523628458798, 0.25451559743466246, 0.2495894890602782, 0.24630541700643468, 0.2495894890602782, 0.2528735614077407, 0.2528735614077407, 0.2495894890602782, 0.2594417061026656, 0.2495894890602782, 0.24630541700643468, 0.2594417061026656, 0.25123152518507297, 0.2495894890602782, 0.2528735614077407, 0.2577996698799979, 0.2463054168106887, 0.24302134456109922, 0.2495894890602782, 0.2528735614077407, 0.23481116374137953, 0.2495894890602782, 0.2463054169085617, 0.24466338068589397, 0.2495894890602782, 0.26272577835225513, 0.2495894890602782, 0.2577996698799979, 0.24958948925602417, 0.2528735614077407, 0.2528735614077407, 0.2463054169085617, 0.2495894890602782, 0.26272577835225513, 0.2495894890602782, 0.2528735614077407, 0.2495894890602782, 0.2610837421295874, 0.2495894890602782, 0.2528735612119947, 0.2463054169085617, 0.2495894890602782, 0.2512315250872, 0.2528735614077407, 0.2528735614077407, 0.2479474527397375, 0.26272577835225513, 0.23481116413287145, 0.24958948886453225, 0.2528735614077407, 0.24958948925602417, 0.25123152518507297, 0.2528735614077407, 0.2479474527397375, 0.2512315254786919, 0.2643678144770499, 0.2512315250872, 0.2528735614077407, 0.25123152518507297, 0.24794745313122943, 0.2528735614077407, 0.25123152528294596, 0.2512315254786919, 0.2528735614077407, 0.2495894890602782, 0.25123152528294596, 0.2528735614077407, 0.2528735614077407, 0.2512315254786919, 0.2561576336573302, 0.2528735614077407, 0.2528735614077407, 0.2298850554648683, 0.25123152528294596, 0.2463054169085617, 0.2512315254786919, 0.24794745313122943, 0.24630541671281572, 0.2512315253808189, 0.2463054168106887, 0.25123152518507297, 0.25123152518507297, 0.2512315253808189, 0.2430213446589722, 0.2495894890602782, 0.24302134446322624, 0.2446633809795129, 0.2528735614077407], 'loss': [1.606857703206965, 1.6063529214330277, 1.6051227643504524, 1.6052648419472226, 1.60475900750875, 1.6046389662999148, 1.60413033198527, 1.6038717634624036, 1.6037272488556849, 1.603410673337306, 1.6029800114445618, 1.602940882500682, 1.6024913038806015, 1.6021424928974566, 1.601948354621198, 1.6018735136094768, 1.601836371177031, 1.6014799057335825, 1.6013735723201743, 1.6012296180705514, 1.6016623554288485, 1.6016877287467157, 1.6015132456099963, 1.6016721323530287, 1.60162617425899, 1.6017604388740274, 1.6006440660057617, 1.6012361186240978, 1.6011977152168384, 1.6012977262786772, 1.600844732300212, 1.6026380127705098, 1.6025407746587201, 1.602284868639842, 1.602125223167623, 1.6013514427678541, 1.6010288440716096, 1.6012777571805448, 1.6006754817903899, 1.6005315992132105, 1.601433808994489, 1.6001689429155854, 1.600612313301901, 1.6005462754188866, 1.5999301702334896, 1.5993263498224026, 1.5996543680128377, 1.5996184272687783, 1.5995474071228528, 1.5994193443282674, 1.5991168161682034, 1.5992776340527701, 1.5995272500803828, 1.599712267010119, 1.599734449582423, 1.5997559317817922, 1.5992204106563905, 1.5988689807406196, 1.5985145072917428, 1.599077654912976, 1.5989554817426865, 1.5991283644396177, 1.5989797697909314, 1.5993915282235742, 1.5988449988668705, 1.5991735131344023, 1.5984153109164698, 1.5982753558814893, 1.5985292937966098, 1.5973182579819916, 1.598323503265146, 1.5980080724007295, 1.5976778087674715, 1.5971975918912789, 1.5972090682944233, 1.596886584695115, 1.5971571726965463, 1.5969062393450884, 1.5961751475226462, 1.5968579994334822, 1.5961924061393347, 1.5966933242594192, 1.596431993067387, 1.5946661508548432, 1.596190503147838, 1.5950263814025347, 1.5951694778348386, 1.5945397960576677, 1.5946487835545315, 1.5952094127510117, 1.5956047408634633, 1.5951565616184682, 1.5954141687318775, 1.5947301470523498, 1.5942461969426525, 1.5949813637645338, 1.596859137086653, 1.5960051738750762, 1.5958315606969093, 1.5957428101396658, 1.5964230603505942, 1.5937921109875124, 1.5941545884467248, 1.594792075666314, 1.5946829517274421, 1.5952155882817765, 1.5948012419060271, 1.5952675860520507, 1.5951516832903916, 1.5954030206805627, 1.5971283488694648, 1.5981761468754168, 1.6016349181502263, 1.601365815491647, 1.5996188085426786, 1.5989188674049455, 1.5998104027898894, 1.5996493965203757, 1.5999648133342517, 1.5975432248086165, 1.597268452144991, 1.5963209480230813, 1.5962459834449345, 1.596063901707377, 1.5954652678060826, 1.5958400505034587, 1.5957559646277457, 1.5956074197189518, 1.5954223225983262, 1.5949117660032894, 1.594807658792766, 1.595135991039707, 1.5951235865665412, 1.5946591336134768, 1.5948802229804915, 1.5944772540666239, 1.5939777643528807, 1.5937235602118396, 1.5936688091231077, 1.5939320464888147, 1.5935520490336956, 1.5936276092666375, 1.5931881251521178, 1.5937059319729188, 1.593251419654862, 1.593051142075713, 1.5925323063343213, 1.594046227936872, 1.5946852862222973, 1.5925582412331991, 1.5929675290716747, 1.5918898978517286, 1.5921043991308192, 1.5917293981360214, 1.5925498058663745, 1.5919217458251076, 1.5919378994916253, 1.5924130602783735, 1.5922627000103742, 1.5918735183484745, 1.5914450539210983, 1.5915614916313845, 1.5914014196983353, 1.5913453708683931, 1.5912972418434566, 1.5906132852761896, 1.5906542058842872, 1.591201467386751, 1.5901004683555273, 1.5907187632711517, 1.5898496570038845, 1.5914111308248626, 1.5926176542618926, 1.5917428814165402, 1.5908283883786054, 1.590655884165049, 1.590361616449924, 1.58958919038518, 1.589725237313727, 1.5892667670024738, 1.5894651367923807, 1.5878475635203493, 1.5892342808310256, 1.588584416505003, 1.5882480795623342, 1.5879706424364564, 1.5879328859904953, 1.5875689475198547, 1.5879343609055945, 1.5874635508907404, 1.5872287340477507, 1.5870509183871917, 1.5867055652567494, 1.5871241187168097, 1.5865959349598973, 1.5888391391942143, 1.5894043928299106, 1.5861198859537897, 1.5881953957144486, 1.5860650121798505, 1.586254650072885, 1.5866097443402425, 1.5861738270557881, 1.5858515964641218, 1.5856321668233224, 1.5858263258082177, 1.5849566072409158, 1.585627278165406, 1.5847397155585474, 1.5862532590204195, 1.5857014400023943, 1.5852795786926144, 1.586241094385574, 1.5880851671681022, 1.5839230053233904, 1.5844092305925592, 1.5847815661949298, 1.5836916299326464, 1.583916740495811, 1.5850367119180104, 1.5849376022938095, 1.5848907460422241, 1.5833849835444769, 1.5835114048246974, 1.5843608377895315, 1.58374783180088, 1.5837592116616346, 1.5841980024529678, 1.5828721658405092, 1.582671032500218, 1.584508796443195, 1.5826814295085303, 1.5825089094330398, 1.5818222807173368, 1.5830090107124688, 1.582668375968933, 1.5825522628408193, 1.5825163153897075, 1.58201032595468, 1.580828166546518, 1.58176530371946, 1.5821627655558028, 1.5804996470406314, 1.581126934938607, 1.5812693718277697, 1.581165607313356, 1.5816186387925666, 1.5814261135379393, 1.583673893989234, 1.5836569480093108, 1.5825922724647443, 1.5812255835386273, 1.5811543288416932, 1.5809601824875974, 1.5810526113490548, 1.5812131823455529, 1.581493344835677, 1.5819134707813145, 1.5824137742514484, 1.5814632172946812, 1.5812253636256381, 1.580975006982776, 1.5817077461454168, 1.5806050485164478, 1.5810417879288692, 1.5809004292595803, 1.5803047863609736, 1.5805759240959214, 1.579867296493029, 1.5799621885072523, 1.5803624887486014, 1.5801044138060458, 1.5799080463895072, 1.5802225350844052, 1.5801646272749381, 1.5794999112828312, 1.5794942912135037, 1.5794104933493927, 1.5789823303966797, 1.5798971963858948, 1.5793976953631799, 1.578524496129406, 1.579315966306526, 1.5804687065265506, 1.579936098709733, 1.5801441067787656, 1.5796449181969896, 1.5790384742024008, 1.5803723022433522, 1.580039418992076, 1.5793841073645214, 1.5808132465859948, 1.57902906665567, 1.580361643659995, 1.5787093490545754, 1.5786681321122562, 1.5786533433064298, 1.578388784308698, 1.5784914851433443, 1.5786266329352125], 'acc': [0.22381930189700586, 0.2332648861518386, 0.23285420929871545, 0.23285420990455322, 0.23285420969036816, 0.23285420887034533, 0.23285421008202084, 0.23285420929871545, 0.23285421029620593, 0.23285420912124782, 0.23285420912124782, 0.2328542071079082, 0.23285420970872686, 0.23285421047367355, 0.23285420890706277, 0.23285420890706277, 0.23983572978503406, 0.2381930185294494, 0.2394250513469414, 0.2406570838462156, 0.24517453903412673, 0.24394250694486394, 0.2443531837979871, 0.2422997941226685, 0.2406570838462156, 0.23203285327927042, 0.24394250576990587, 0.2394250515978439, 0.24106776381420159, 0.24106776126845905, 0.24804928253808306, 0.233264886935144, 0.23531827539878705, 0.23696098587106632, 0.2357289536043359, 0.2357289516460724, 0.23778234169468498, 0.24024640642397213, 0.24435318360216074, 0.24065708464787972, 0.2381930173544913, 0.24599589309530825, 0.241889116112946, 0.24188911671878377, 0.23983572843260834, 0.24435318280049662, 0.24435318221301758, 0.24271047213239103, 0.24394250479077412, 0.23942505236279057, 0.24353182776018334, 0.24188911709207775, 0.2439425054149706, 0.2410677622475908, 0.2431211487713291, 0.24065708600030541, 0.24435318240884393, 0.2447638598310874, 0.24394250459494776, 0.245585214479748, 0.24353182758271572, 0.24476386041856643, 0.24229979492433262, 0.23942505255861693, 0.24229979414102723, 0.24517453784080992, 0.2443531818213649, 0.24188911632713106, 0.23901437298228365, 0.2406570834545629, 0.24394250420329508, 0.24599589427026636, 0.24763860474254562, 0.2447638610060455, 0.242710471544912, 0.2468172479397952, 0.2427104725240437, 0.24887063720510236, 0.24763860492001324, 0.2402464058364931, 0.24229979471014756, 0.24558521604635877, 0.24476386122023056, 0.24763860135842153, 0.24845995997868525, 0.24517453960324703, 0.24476386159352453, 0.2443531837979871, 0.24763860278428212, 0.24353182758271572, 0.2447638602227401, 0.24845995956867384, 0.24435318319214933, 0.24312115016047225, 0.2505133452723893, 0.24558521665219654, 0.24722792555786502, 0.246406570499193, 0.2492813153922925, 0.24640656993007268, 0.2443531818213649, 0.24804928273390942, 0.25051334728572894, 0.24435318203554995, 0.24435318340633438, 0.24681724852727424, 0.24640656991171397, 0.2418891169146101, 0.24763860374505514, 0.24763860415506656, 0.2435318263893989, 0.2365503074880498, 0.24106776069933875, 0.23942505138365885, 0.24435318319214933, 0.23696098450028186, 0.235318274811308, 0.24106776107263272, 0.23572895203772512, 0.24353182736853066, 0.24271047175909705, 0.23819301952693986, 0.24353182777854207, 0.24681724891892692, 0.24312115014211352, 0.247227925931159, 0.24229979392684217, 0.24394250498660047, 0.24845995839371574, 0.24312115053376623, 0.24681724909639458, 0.24763860454671927, 0.24476385924360836, 0.24599589270365557, 0.2464065720658038, 0.24722792495202725, 0.2435318267810516, 0.2427104719549234, 0.24599589464356034, 0.24681724886385079, 0.2459958920978178, 0.2447638598310874, 0.24845995819788938, 0.24188911728790408, 0.24804928292973574, 0.2464065708908457, 0.24804928157731004, 0.24722792710611707, 0.24640657304493555, 0.24394250520078553, 0.24394250400746872, 0.2435318267810516, 0.24517453725333085, 0.24640657069501937, 0.2422997941226685, 0.24763860531166593, 0.24517453803663625, 0.24229979551181166, 0.24435318221301758, 0.2472279271244758, 0.24599589523103937, 0.24599589428862506, 0.24517453864247402, 0.24599589248947049, 0.24887063757839634, 0.25010267025513816, 0.2542094448508668, 0.24804928175477767, 0.25544147864749056, 0.2550308016536172, 0.2546201252105055, 0.25626283133788763, 0.24558521528141208, 0.2558521569081156, 0.2579055441968005, 0.2505133484423283, 0.25338809041639127, 0.2529774125841364, 0.25913757763848905, 0.2587268986496348, 0.24969199107045756, 0.2554414776683588, 0.2579055447842796, 0.26324435362336085, 0.26283367737607544, 0.2525667361593834, 0.26201232037749866, 0.2579055457634113, 0.2525667342011199, 0.2611909647497064, 0.25913757705101, 0.24845995819788938, 0.2603696114718302, 0.2599589322871496, 0.24681725066300536, 0.2542094460258249, 0.2525667357677307, 0.2579055422018196, 0.25420944486922553, 0.25544147629757435, 0.26283367582782335, 0.2501026700409531, 0.2583162225981757, 0.2702258725430686, 0.2607802888940737, 0.2537987682119287, 0.26570842089839053, 0.26488706311650834, 0.25626283509530573, 0.25503080067448547, 0.26447638706504933, 0.26324435381918715, 0.2640657094469795, 0.2451745374491572, 0.2648870619048328, 0.26447638369928395, 0.2509240230862854, 0.26611909832063396, 0.26652977574287745, 0.2591375768551836, 0.25010267043260576, 0.25872690023460426, 0.2661190981248076, 0.26201232096497773, 0.2640657062770405, 0.2624229971755457, 0.26529774347614704, 0.26036960953192545, 0.26036960970939305, 0.2652977403612842, 0.2525667349660666, 0.2648870627248556, 0.26242299897470023, 0.2611909641622273, 0.264887062920682, 0.2599589315038442, 0.26652977574287745, 0.2665297761345301, 0.26078028832495337, 0.26242299936635294, 0.2542094448508668, 0.26652977198545935, 0.26694045296929453, 0.2665297747637457, 0.2661190967723819, 0.2603696107068835, 0.26529774230118897, 0.26036961049269847, 0.2636550308497779, 0.25010266827851596, 0.2620123211608041, 0.2640657088595005, 0.26365503124143064, 0.2648870642914664, 0.2652977393270763, 0.2677618070304761, 0.26694045218598916, 0.2599589318954969, 0.2611909637338572, 0.26447638706504933, 0.26694045179433645, 0.2607802868990928, 0.26570841792427785, 0.26160164491351867, 0.26365503222056236, 0.26160164471769237, 0.2669404510110311, 0.2620123207691514, 0.2624229983872212, 0.26652977335624384, 0.26160164236777617, 0.2657084165534934, 0.26242299897470023, 0.26529774308449433, 0.26488706389981376, 0.26611909792898125, 0.265297742888668, 0.2607802886982473, 0.2726899362565066, 0.26447638804418105, 0.2677618048396688, 0.2677618084012605, 0.2657084183159305, 0.2669404525776419, 0.26529773913124993, 0.26899383951139155, 0.26817248505855734, 0.26611909538323875, 0.26940451828606077, 0.2652977411445896, 0.2611909641622273, 0.2648870622964855, 0.26981519492499884, 0.25954825662734327, 0.2689938392972065, 0.26735112820073076, 0.2652977407345782, 0.26242299897470023, 0.2587268990412875, 0.2648870627248556]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
