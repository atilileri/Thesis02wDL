{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf25.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 00:26:31 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'AllShfRnd', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['by', 'ce', 'mb', 'sg', 'eo', 'eb', 'yd', 'my', 'ds', 'ek', 'sk', 'ck', 'ib', 'aa', 'eg'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000013897FEE278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000013895506EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7057, Accuracy:0.0653, Validation Loss:2.6972, Validation Accuracy:0.0969\n",
    "Epoch #2: Loss:2.6946, Accuracy:0.1060, Validation Loss:2.6880, Validation Accuracy:0.1182\n",
    "Epoch #3: Loss:2.6864, Accuracy:0.1195, Validation Loss:2.6809, Validation Accuracy:0.1117\n",
    "Epoch #4: Loss:2.6803, Accuracy:0.1076, Validation Loss:2.6756, Validation Accuracy:0.1051\n",
    "Epoch #5: Loss:2.6743, Accuracy:0.1117, Validation Loss:2.6688, Validation Accuracy:0.1461\n",
    "Epoch #6: Loss:2.6678, Accuracy:0.1072, Validation Loss:2.6637, Validation Accuracy:0.0985\n",
    "Epoch #7: Loss:2.6622, Accuracy:0.1142, Validation Loss:2.6578, Validation Accuracy:0.1182\n",
    "Epoch #8: Loss:2.6554, Accuracy:0.1248, Validation Loss:2.6508, Validation Accuracy:0.1445\n",
    "Epoch #9: Loss:2.6466, Accuracy:0.1318, Validation Loss:2.6411, Validation Accuracy:0.1396\n",
    "Epoch #10: Loss:2.6364, Accuracy:0.1331, Validation Loss:2.6304, Validation Accuracy:0.1412\n",
    "Epoch #11: Loss:2.6234, Accuracy:0.1326, Validation Loss:2.6172, Validation Accuracy:0.1412\n",
    "Epoch #12: Loss:2.6077, Accuracy:0.1331, Validation Loss:2.6021, Validation Accuracy:0.1412\n",
    "Epoch #13: Loss:2.5894, Accuracy:0.1536, Validation Loss:2.5837, Validation Accuracy:0.1560\n",
    "Epoch #14: Loss:2.5703, Accuracy:0.1639, Validation Loss:2.5645, Validation Accuracy:0.1675\n",
    "Epoch #15: Loss:2.5520, Accuracy:0.1634, Validation Loss:2.5492, Validation Accuracy:0.1626\n",
    "Epoch #16: Loss:2.5447, Accuracy:0.1593, Validation Loss:2.6026, Validation Accuracy:0.1297\n",
    "Epoch #17: Loss:2.5677, Accuracy:0.1499, Validation Loss:2.5463, Validation Accuracy:0.1445\n",
    "Epoch #18: Loss:2.5366, Accuracy:0.1577, Validation Loss:2.5369, Validation Accuracy:0.1461\n",
    "Epoch #19: Loss:2.5131, Accuracy:0.1630, Validation Loss:2.5270, Validation Accuracy:0.1642\n",
    "Epoch #20: Loss:2.5185, Accuracy:0.1602, Validation Loss:2.5138, Validation Accuracy:0.1642\n",
    "Epoch #21: Loss:2.5029, Accuracy:0.1676, Validation Loss:2.5150, Validation Accuracy:0.1560\n",
    "Epoch #22: Loss:2.5005, Accuracy:0.1647, Validation Loss:2.5064, Validation Accuracy:0.1675\n",
    "Epoch #23: Loss:2.4932, Accuracy:0.1713, Validation Loss:2.5024, Validation Accuracy:0.1658\n",
    "Epoch #24: Loss:2.4907, Accuracy:0.1704, Validation Loss:2.5007, Validation Accuracy:0.1609\n",
    "Epoch #25: Loss:2.4877, Accuracy:0.1667, Validation Loss:2.4971, Validation Accuracy:0.1708\n",
    "Epoch #26: Loss:2.4840, Accuracy:0.1782, Validation Loss:2.4940, Validation Accuracy:0.1626\n",
    "Epoch #27: Loss:2.4788, Accuracy:0.1749, Validation Loss:2.4934, Validation Accuracy:0.1626\n",
    "Epoch #28: Loss:2.4793, Accuracy:0.1733, Validation Loss:2.4877, Validation Accuracy:0.1658\n",
    "Epoch #29: Loss:2.4788, Accuracy:0.1717, Validation Loss:2.4865, Validation Accuracy:0.1658\n",
    "Epoch #30: Loss:2.4767, Accuracy:0.1721, Validation Loss:2.4865, Validation Accuracy:0.1741\n",
    "Epoch #31: Loss:2.4783, Accuracy:0.1749, Validation Loss:2.5175, Validation Accuracy:0.1576\n",
    "Epoch #32: Loss:2.4936, Accuracy:0.1643, Validation Loss:2.4800, Validation Accuracy:0.1609\n",
    "Epoch #33: Loss:2.4854, Accuracy:0.1651, Validation Loss:2.5034, Validation Accuracy:0.1494\n",
    "Epoch #34: Loss:2.4842, Accuracy:0.1696, Validation Loss:2.4816, Validation Accuracy:0.1724\n",
    "Epoch #35: Loss:2.4762, Accuracy:0.1741, Validation Loss:2.4855, Validation Accuracy:0.1741\n",
    "Epoch #36: Loss:2.4732, Accuracy:0.1729, Validation Loss:2.4803, Validation Accuracy:0.1691\n",
    "Epoch #37: Loss:2.4692, Accuracy:0.1754, Validation Loss:2.4836, Validation Accuracy:0.1675\n",
    "Epoch #38: Loss:2.4660, Accuracy:0.1762, Validation Loss:2.4780, Validation Accuracy:0.1609\n",
    "Epoch #39: Loss:2.4638, Accuracy:0.1799, Validation Loss:2.4774, Validation Accuracy:0.1724\n",
    "Epoch #40: Loss:2.4618, Accuracy:0.1758, Validation Loss:2.4744, Validation Accuracy:0.1741\n",
    "Epoch #41: Loss:2.4629, Accuracy:0.1770, Validation Loss:2.4848, Validation Accuracy:0.1658\n",
    "Epoch #42: Loss:2.4646, Accuracy:0.1770, Validation Loss:2.4749, Validation Accuracy:0.1675\n",
    "Epoch #43: Loss:2.4641, Accuracy:0.1770, Validation Loss:2.4723, Validation Accuracy:0.1675\n",
    "Epoch #44: Loss:2.4582, Accuracy:0.1741, Validation Loss:2.4742, Validation Accuracy:0.1691\n",
    "Epoch #45: Loss:2.4571, Accuracy:0.1737, Validation Loss:2.4700, Validation Accuracy:0.1773\n",
    "Epoch #46: Loss:2.4541, Accuracy:0.1786, Validation Loss:2.4695, Validation Accuracy:0.1741\n",
    "Epoch #47: Loss:2.4532, Accuracy:0.1815, Validation Loss:2.4688, Validation Accuracy:0.1741\n",
    "Epoch #48: Loss:2.4519, Accuracy:0.1745, Validation Loss:2.4670, Validation Accuracy:0.1741\n",
    "Epoch #49: Loss:2.4506, Accuracy:0.1737, Validation Loss:2.4630, Validation Accuracy:0.1675\n",
    "Epoch #50: Loss:2.4521, Accuracy:0.1762, Validation Loss:2.4618, Validation Accuracy:0.1691\n",
    "Epoch #51: Loss:2.4512, Accuracy:0.1729, Validation Loss:2.4610, Validation Accuracy:0.1773\n",
    "Epoch #52: Loss:2.4506, Accuracy:0.1786, Validation Loss:2.4626, Validation Accuracy:0.1823\n",
    "Epoch #53: Loss:2.4495, Accuracy:0.1770, Validation Loss:2.4615, Validation Accuracy:0.1806\n",
    "Epoch #54: Loss:2.4489, Accuracy:0.1749, Validation Loss:2.4603, Validation Accuracy:0.1790\n",
    "Epoch #55: Loss:2.4483, Accuracy:0.1754, Validation Loss:2.4605, Validation Accuracy:0.1806\n",
    "Epoch #56: Loss:2.4483, Accuracy:0.1713, Validation Loss:2.4618, Validation Accuracy:0.1823\n",
    "Epoch #57: Loss:2.4486, Accuracy:0.1745, Validation Loss:2.4632, Validation Accuracy:0.1806\n",
    "Epoch #58: Loss:2.4479, Accuracy:0.1762, Validation Loss:2.4633, Validation Accuracy:0.1773\n",
    "Epoch #59: Loss:2.4464, Accuracy:0.1754, Validation Loss:2.4630, Validation Accuracy:0.1790\n",
    "Epoch #60: Loss:2.4463, Accuracy:0.1741, Validation Loss:2.4607, Validation Accuracy:0.1757\n",
    "Epoch #61: Loss:2.4451, Accuracy:0.1733, Validation Loss:2.4580, Validation Accuracy:0.1658\n",
    "Epoch #62: Loss:2.4453, Accuracy:0.1749, Validation Loss:2.4587, Validation Accuracy:0.1691\n",
    "Epoch #63: Loss:2.4442, Accuracy:0.1762, Validation Loss:2.4600, Validation Accuracy:0.1724\n",
    "Epoch #64: Loss:2.4442, Accuracy:0.1774, Validation Loss:2.4582, Validation Accuracy:0.1806\n",
    "Epoch #65: Loss:2.4442, Accuracy:0.1795, Validation Loss:2.4610, Validation Accuracy:0.1773\n",
    "Epoch #66: Loss:2.4446, Accuracy:0.1778, Validation Loss:2.4603, Validation Accuracy:0.1708\n",
    "Epoch #67: Loss:2.4455, Accuracy:0.1799, Validation Loss:2.4614, Validation Accuracy:0.1741\n",
    "Epoch #68: Loss:2.4456, Accuracy:0.1811, Validation Loss:2.4615, Validation Accuracy:0.1724\n",
    "Epoch #69: Loss:2.4460, Accuracy:0.1791, Validation Loss:2.4632, Validation Accuracy:0.1691\n",
    "Epoch #70: Loss:2.4462, Accuracy:0.1791, Validation Loss:2.4623, Validation Accuracy:0.1675\n",
    "Epoch #71: Loss:2.4443, Accuracy:0.1778, Validation Loss:2.4634, Validation Accuracy:0.1675\n",
    "Epoch #72: Loss:2.4450, Accuracy:0.1766, Validation Loss:2.4630, Validation Accuracy:0.1642\n",
    "Epoch #73: Loss:2.4451, Accuracy:0.1758, Validation Loss:2.4632, Validation Accuracy:0.1626\n",
    "Epoch #74: Loss:2.4434, Accuracy:0.1754, Validation Loss:2.4650, Validation Accuracy:0.1626\n",
    "Epoch #75: Loss:2.4853, Accuracy:0.1708, Validation Loss:2.4835, Validation Accuracy:0.1658\n",
    "Epoch #76: Loss:2.4934, Accuracy:0.1618, Validation Loss:2.4914, Validation Accuracy:0.1609\n",
    "Epoch #77: Loss:2.4547, Accuracy:0.1717, Validation Loss:2.4740, Validation Accuracy:0.1560\n",
    "Epoch #78: Loss:2.4604, Accuracy:0.1704, Validation Loss:2.4742, Validation Accuracy:0.1544\n",
    "Epoch #79: Loss:2.4485, Accuracy:0.1803, Validation Loss:2.4687, Validation Accuracy:0.1626\n",
    "Epoch #80: Loss:2.4486, Accuracy:0.1791, Validation Loss:2.4689, Validation Accuracy:0.1626\n",
    "Epoch #81: Loss:2.4445, Accuracy:0.1786, Validation Loss:2.4642, Validation Accuracy:0.1675\n",
    "Epoch #82: Loss:2.4449, Accuracy:0.1749, Validation Loss:2.4666, Validation Accuracy:0.1576\n",
    "Epoch #83: Loss:2.4434, Accuracy:0.1745, Validation Loss:2.4644, Validation Accuracy:0.1642\n",
    "Epoch #84: Loss:2.4418, Accuracy:0.1795, Validation Loss:2.4629, Validation Accuracy:0.1626\n",
    "Epoch #85: Loss:2.4399, Accuracy:0.1774, Validation Loss:2.4620, Validation Accuracy:0.1675\n",
    "Epoch #86: Loss:2.4401, Accuracy:0.1762, Validation Loss:2.4613, Validation Accuracy:0.1675\n",
    "Epoch #87: Loss:2.4389, Accuracy:0.1782, Validation Loss:2.4605, Validation Accuracy:0.1642\n",
    "Epoch #88: Loss:2.4387, Accuracy:0.1786, Validation Loss:2.4606, Validation Accuracy:0.1642\n",
    "Epoch #89: Loss:2.4381, Accuracy:0.1791, Validation Loss:2.4605, Validation Accuracy:0.1642\n",
    "Epoch #90: Loss:2.4372, Accuracy:0.1786, Validation Loss:2.4590, Validation Accuracy:0.1658\n",
    "Epoch #91: Loss:2.4370, Accuracy:0.1758, Validation Loss:2.4595, Validation Accuracy:0.1642\n",
    "Epoch #92: Loss:2.4362, Accuracy:0.1774, Validation Loss:2.4585, Validation Accuracy:0.1642\n",
    "Epoch #93: Loss:2.4359, Accuracy:0.1749, Validation Loss:2.4575, Validation Accuracy:0.1675\n",
    "Epoch #94: Loss:2.4354, Accuracy:0.1778, Validation Loss:2.4552, Validation Accuracy:0.1691\n",
    "Epoch #95: Loss:2.4352, Accuracy:0.1754, Validation Loss:2.4560, Validation Accuracy:0.1675\n",
    "Epoch #96: Loss:2.4348, Accuracy:0.1778, Validation Loss:2.4539, Validation Accuracy:0.1691\n",
    "Epoch #97: Loss:2.4336, Accuracy:0.1754, Validation Loss:2.4541, Validation Accuracy:0.1642\n",
    "Epoch #98: Loss:2.4337, Accuracy:0.1745, Validation Loss:2.4539, Validation Accuracy:0.1675\n",
    "Epoch #99: Loss:2.4339, Accuracy:0.1758, Validation Loss:2.4541, Validation Accuracy:0.1675\n",
    "Epoch #100: Loss:2.4337, Accuracy:0.1733, Validation Loss:2.4538, Validation Accuracy:0.1658\n",
    "Epoch #101: Loss:2.4372, Accuracy:0.1762, Validation Loss:2.4616, Validation Accuracy:0.1609\n",
    "Epoch #102: Loss:2.4396, Accuracy:0.1749, Validation Loss:2.4572, Validation Accuracy:0.1724\n",
    "Epoch #103: Loss:2.4359, Accuracy:0.1749, Validation Loss:2.4563, Validation Accuracy:0.1675\n",
    "Epoch #104: Loss:2.4340, Accuracy:0.1791, Validation Loss:2.4558, Validation Accuracy:0.1658\n",
    "Epoch #105: Loss:2.4345, Accuracy:0.1754, Validation Loss:2.4527, Validation Accuracy:0.1658\n",
    "Epoch #106: Loss:2.4331, Accuracy:0.1758, Validation Loss:2.4535, Validation Accuracy:0.1675\n",
    "Epoch #107: Loss:2.4339, Accuracy:0.1754, Validation Loss:2.4530, Validation Accuracy:0.1626\n",
    "Epoch #108: Loss:2.4322, Accuracy:0.1807, Validation Loss:2.4536, Validation Accuracy:0.1675\n",
    "Epoch #109: Loss:2.4329, Accuracy:0.1799, Validation Loss:2.4530, Validation Accuracy:0.1675\n",
    "Epoch #110: Loss:2.4318, Accuracy:0.1782, Validation Loss:2.4547, Validation Accuracy:0.1691\n",
    "Epoch #111: Loss:2.4316, Accuracy:0.1795, Validation Loss:2.4524, Validation Accuracy:0.1675\n",
    "Epoch #112: Loss:2.4309, Accuracy:0.1778, Validation Loss:2.4522, Validation Accuracy:0.1658\n",
    "Epoch #113: Loss:2.4303, Accuracy:0.1782, Validation Loss:2.4534, Validation Accuracy:0.1642\n",
    "Epoch #114: Loss:2.4306, Accuracy:0.1807, Validation Loss:2.4524, Validation Accuracy:0.1675\n",
    "Epoch #115: Loss:2.4296, Accuracy:0.1811, Validation Loss:2.4506, Validation Accuracy:0.1691\n",
    "Epoch #116: Loss:2.4303, Accuracy:0.1786, Validation Loss:2.4521, Validation Accuracy:0.1658\n",
    "Epoch #117: Loss:2.4299, Accuracy:0.1799, Validation Loss:2.4523, Validation Accuracy:0.1675\n",
    "Epoch #118: Loss:2.4303, Accuracy:0.1774, Validation Loss:2.4526, Validation Accuracy:0.1609\n",
    "Epoch #119: Loss:2.4304, Accuracy:0.1782, Validation Loss:2.4526, Validation Accuracy:0.1675\n",
    "Epoch #120: Loss:2.4297, Accuracy:0.1774, Validation Loss:2.4524, Validation Accuracy:0.1626\n",
    "Epoch #121: Loss:2.4299, Accuracy:0.1770, Validation Loss:2.4503, Validation Accuracy:0.1675\n",
    "Epoch #122: Loss:2.4309, Accuracy:0.1782, Validation Loss:2.4511, Validation Accuracy:0.1675\n",
    "Epoch #123: Loss:2.4299, Accuracy:0.1803, Validation Loss:2.4526, Validation Accuracy:0.1642\n",
    "Epoch #124: Loss:2.4301, Accuracy:0.1758, Validation Loss:2.4537, Validation Accuracy:0.1675\n",
    "Epoch #125: Loss:2.4300, Accuracy:0.1786, Validation Loss:2.4538, Validation Accuracy:0.1658\n",
    "Epoch #126: Loss:2.4305, Accuracy:0.1758, Validation Loss:2.4538, Validation Accuracy:0.1724\n",
    "Epoch #127: Loss:2.4299, Accuracy:0.1754, Validation Loss:2.4562, Validation Accuracy:0.1609\n",
    "Epoch #128: Loss:2.4302, Accuracy:0.1770, Validation Loss:2.4573, Validation Accuracy:0.1642\n",
    "Epoch #129: Loss:2.4314, Accuracy:0.1758, Validation Loss:2.4557, Validation Accuracy:0.1642\n",
    "Epoch #130: Loss:2.4313, Accuracy:0.1778, Validation Loss:2.4564, Validation Accuracy:0.1626\n",
    "Epoch #131: Loss:2.4312, Accuracy:0.1799, Validation Loss:2.4578, Validation Accuracy:0.1609\n",
    "Epoch #132: Loss:2.4328, Accuracy:0.1725, Validation Loss:2.4613, Validation Accuracy:0.1658\n",
    "Epoch #133: Loss:2.4302, Accuracy:0.1803, Validation Loss:2.4537, Validation Accuracy:0.1626\n",
    "Epoch #134: Loss:2.4314, Accuracy:0.1786, Validation Loss:2.4534, Validation Accuracy:0.1642\n",
    "Epoch #135: Loss:2.4310, Accuracy:0.1762, Validation Loss:2.4511, Validation Accuracy:0.1642\n",
    "Epoch #136: Loss:2.4311, Accuracy:0.1782, Validation Loss:2.4524, Validation Accuracy:0.1642\n",
    "Epoch #137: Loss:2.4302, Accuracy:0.1766, Validation Loss:2.4531, Validation Accuracy:0.1642\n",
    "Epoch #138: Loss:2.4303, Accuracy:0.1774, Validation Loss:2.4534, Validation Accuracy:0.1708\n",
    "Epoch #139: Loss:2.4306, Accuracy:0.1770, Validation Loss:2.4544, Validation Accuracy:0.1675\n",
    "Epoch #140: Loss:2.4293, Accuracy:0.1791, Validation Loss:2.4537, Validation Accuracy:0.1642\n",
    "Epoch #141: Loss:2.4293, Accuracy:0.1782, Validation Loss:2.4532, Validation Accuracy:0.1642\n",
    "Epoch #142: Loss:2.4295, Accuracy:0.1778, Validation Loss:2.4526, Validation Accuracy:0.1691\n",
    "Epoch #143: Loss:2.4296, Accuracy:0.1725, Validation Loss:2.4534, Validation Accuracy:0.1642\n",
    "Epoch #144: Loss:2.4305, Accuracy:0.1749, Validation Loss:2.4537, Validation Accuracy:0.1658\n",
    "Epoch #145: Loss:2.4295, Accuracy:0.1758, Validation Loss:2.4505, Validation Accuracy:0.1658\n",
    "Epoch #146: Loss:2.4307, Accuracy:0.1786, Validation Loss:2.4553, Validation Accuracy:0.1642\n",
    "Epoch #147: Loss:2.4297, Accuracy:0.1782, Validation Loss:2.4542, Validation Accuracy:0.1642\n",
    "Epoch #148: Loss:2.4299, Accuracy:0.1774, Validation Loss:2.4540, Validation Accuracy:0.1626\n",
    "Epoch #149: Loss:2.4295, Accuracy:0.1778, Validation Loss:2.4554, Validation Accuracy:0.1642\n",
    "Epoch #150: Loss:2.4293, Accuracy:0.1762, Validation Loss:2.4522, Validation Accuracy:0.1691\n",
    "Epoch #151: Loss:2.4286, Accuracy:0.1770, Validation Loss:2.4512, Validation Accuracy:0.1691\n",
    "Epoch #152: Loss:2.4291, Accuracy:0.1754, Validation Loss:2.4523, Validation Accuracy:0.1642\n",
    "Epoch #153: Loss:2.4284, Accuracy:0.1758, Validation Loss:2.4516, Validation Accuracy:0.1675\n",
    "Epoch #154: Loss:2.4279, Accuracy:0.1737, Validation Loss:2.4512, Validation Accuracy:0.1642\n",
    "Epoch #155: Loss:2.4287, Accuracy:0.1754, Validation Loss:2.4502, Validation Accuracy:0.1626\n",
    "Epoch #156: Loss:2.4287, Accuracy:0.1770, Validation Loss:2.4545, Validation Accuracy:0.1626\n",
    "Epoch #157: Loss:2.4319, Accuracy:0.1749, Validation Loss:2.4581, Validation Accuracy:0.1708\n",
    "Epoch #158: Loss:2.4325, Accuracy:0.1745, Validation Loss:2.4554, Validation Accuracy:0.1658\n",
    "Epoch #159: Loss:2.4298, Accuracy:0.1782, Validation Loss:2.4576, Validation Accuracy:0.1626\n",
    "Epoch #160: Loss:2.4305, Accuracy:0.1778, Validation Loss:2.4569, Validation Accuracy:0.1626\n",
    "Epoch #161: Loss:2.4307, Accuracy:0.1799, Validation Loss:2.4586, Validation Accuracy:0.1675\n",
    "Epoch #162: Loss:2.4285, Accuracy:0.1782, Validation Loss:2.4573, Validation Accuracy:0.1675\n",
    "Epoch #163: Loss:2.4269, Accuracy:0.1774, Validation Loss:2.4574, Validation Accuracy:0.1741\n",
    "Epoch #164: Loss:2.4273, Accuracy:0.1799, Validation Loss:2.4561, Validation Accuracy:0.1675\n",
    "Epoch #165: Loss:2.4265, Accuracy:0.1803, Validation Loss:2.4570, Validation Accuracy:0.1626\n",
    "Epoch #166: Loss:2.4265, Accuracy:0.1795, Validation Loss:2.4565, Validation Accuracy:0.1658\n",
    "Epoch #167: Loss:2.4265, Accuracy:0.1807, Validation Loss:2.4559, Validation Accuracy:0.1626\n",
    "Epoch #168: Loss:2.4275, Accuracy:0.1791, Validation Loss:2.4547, Validation Accuracy:0.1642\n",
    "Epoch #169: Loss:2.4275, Accuracy:0.1782, Validation Loss:2.4542, Validation Accuracy:0.1691\n",
    "Epoch #170: Loss:2.4291, Accuracy:0.1786, Validation Loss:2.4553, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4283, Accuracy:0.1786, Validation Loss:2.4568, Validation Accuracy:0.1626\n",
    "Epoch #172: Loss:2.4271, Accuracy:0.1770, Validation Loss:2.4551, Validation Accuracy:0.1658\n",
    "Epoch #173: Loss:2.4275, Accuracy:0.1786, Validation Loss:2.4554, Validation Accuracy:0.1708\n",
    "Epoch #174: Loss:2.4270, Accuracy:0.1799, Validation Loss:2.4558, Validation Accuracy:0.1626\n",
    "Epoch #175: Loss:2.4268, Accuracy:0.1799, Validation Loss:2.4548, Validation Accuracy:0.1626\n",
    "Epoch #176: Loss:2.4270, Accuracy:0.1807, Validation Loss:2.4552, Validation Accuracy:0.1675\n",
    "Epoch #177: Loss:2.4259, Accuracy:0.1782, Validation Loss:2.4545, Validation Accuracy:0.1658\n",
    "Epoch #178: Loss:2.4272, Accuracy:0.1741, Validation Loss:2.4539, Validation Accuracy:0.1658\n",
    "Epoch #179: Loss:2.4275, Accuracy:0.1803, Validation Loss:2.4534, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.4286, Accuracy:0.1770, Validation Loss:2.4530, Validation Accuracy:0.1675\n",
    "Epoch #181: Loss:2.4280, Accuracy:0.1774, Validation Loss:2.4542, Validation Accuracy:0.1609\n",
    "Epoch #182: Loss:2.4275, Accuracy:0.1815, Validation Loss:2.4536, Validation Accuracy:0.1544\n",
    "Epoch #183: Loss:2.4283, Accuracy:0.1762, Validation Loss:2.4537, Validation Accuracy:0.1626\n",
    "Epoch #184: Loss:2.4268, Accuracy:0.1807, Validation Loss:2.4540, Validation Accuracy:0.1642\n",
    "Epoch #185: Loss:2.4272, Accuracy:0.1799, Validation Loss:2.4544, Validation Accuracy:0.1576\n",
    "Epoch #186: Loss:2.4272, Accuracy:0.1774, Validation Loss:2.4544, Validation Accuracy:0.1658\n",
    "Epoch #187: Loss:2.4269, Accuracy:0.1762, Validation Loss:2.4544, Validation Accuracy:0.1560\n",
    "Epoch #188: Loss:2.4266, Accuracy:0.1786, Validation Loss:2.4543, Validation Accuracy:0.1544\n",
    "Epoch #189: Loss:2.4266, Accuracy:0.1782, Validation Loss:2.4532, Validation Accuracy:0.1593\n",
    "Epoch #190: Loss:2.4273, Accuracy:0.1791, Validation Loss:2.4523, Validation Accuracy:0.1593\n",
    "Epoch #191: Loss:2.4262, Accuracy:0.1786, Validation Loss:2.4540, Validation Accuracy:0.1576\n",
    "Epoch #192: Loss:2.4273, Accuracy:0.1749, Validation Loss:2.4548, Validation Accuracy:0.1708\n",
    "Epoch #193: Loss:2.4276, Accuracy:0.1778, Validation Loss:2.4550, Validation Accuracy:0.1724\n",
    "Epoch #194: Loss:2.4268, Accuracy:0.1778, Validation Loss:2.4553, Validation Accuracy:0.1708\n",
    "Epoch #195: Loss:2.4275, Accuracy:0.1758, Validation Loss:2.4546, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4272, Accuracy:0.1762, Validation Loss:2.4551, Validation Accuracy:0.1724\n",
    "Epoch #197: Loss:2.4262, Accuracy:0.1766, Validation Loss:2.4553, Validation Accuracy:0.1658\n",
    "Epoch #198: Loss:2.4272, Accuracy:0.1745, Validation Loss:2.4552, Validation Accuracy:0.1642\n",
    "Epoch #199: Loss:2.4272, Accuracy:0.1762, Validation Loss:2.4558, Validation Accuracy:0.1658\n",
    "Epoch #200: Loss:2.4282, Accuracy:0.1762, Validation Loss:2.4544, Validation Accuracy:0.1658\n",
    "Epoch #201: Loss:2.4283, Accuracy:0.1758, Validation Loss:2.4546, Validation Accuracy:0.1675\n",
    "Epoch #202: Loss:2.4283, Accuracy:0.1778, Validation Loss:2.4553, Validation Accuracy:0.1658\n",
    "Epoch #203: Loss:2.4280, Accuracy:0.1749, Validation Loss:2.4564, Validation Accuracy:0.1658\n",
    "Epoch #204: Loss:2.4280, Accuracy:0.1758, Validation Loss:2.4554, Validation Accuracy:0.1691\n",
    "Epoch #205: Loss:2.4265, Accuracy:0.1766, Validation Loss:2.4541, Validation Accuracy:0.1691\n",
    "Epoch #206: Loss:2.4258, Accuracy:0.1741, Validation Loss:2.4543, Validation Accuracy:0.1691\n",
    "Epoch #207: Loss:2.4258, Accuracy:0.1791, Validation Loss:2.4563, Validation Accuracy:0.1691\n",
    "Epoch #208: Loss:2.4252, Accuracy:0.1758, Validation Loss:2.4562, Validation Accuracy:0.1691\n",
    "Epoch #209: Loss:2.4251, Accuracy:0.1770, Validation Loss:2.4553, Validation Accuracy:0.1691\n",
    "Epoch #210: Loss:2.4262, Accuracy:0.1795, Validation Loss:2.4544, Validation Accuracy:0.1675\n",
    "Epoch #211: Loss:2.4265, Accuracy:0.1725, Validation Loss:2.4565, Validation Accuracy:0.1675\n",
    "Epoch #212: Loss:2.4260, Accuracy:0.1795, Validation Loss:2.4581, Validation Accuracy:0.1741\n",
    "Epoch #213: Loss:2.4269, Accuracy:0.1786, Validation Loss:2.4570, Validation Accuracy:0.1741\n",
    "Epoch #214: Loss:2.4263, Accuracy:0.1766, Validation Loss:2.4553, Validation Accuracy:0.1642\n",
    "Epoch #215: Loss:2.4261, Accuracy:0.1770, Validation Loss:2.4569, Validation Accuracy:0.1741\n",
    "Epoch #216: Loss:2.4272, Accuracy:0.1799, Validation Loss:2.4578, Validation Accuracy:0.1626\n",
    "Epoch #217: Loss:2.4267, Accuracy:0.1807, Validation Loss:2.4568, Validation Accuracy:0.1741\n",
    "Epoch #218: Loss:2.4268, Accuracy:0.1803, Validation Loss:2.4557, Validation Accuracy:0.1642\n",
    "Epoch #219: Loss:2.4266, Accuracy:0.1766, Validation Loss:2.4559, Validation Accuracy:0.1724\n",
    "Epoch #220: Loss:2.4269, Accuracy:0.1774, Validation Loss:2.4556, Validation Accuracy:0.1741\n",
    "Epoch #221: Loss:2.4266, Accuracy:0.1774, Validation Loss:2.4568, Validation Accuracy:0.1757\n",
    "Epoch #222: Loss:2.4276, Accuracy:0.1774, Validation Loss:2.4564, Validation Accuracy:0.1642\n",
    "Epoch #223: Loss:2.4275, Accuracy:0.1741, Validation Loss:2.4563, Validation Accuracy:0.1642\n",
    "Epoch #224: Loss:2.4275, Accuracy:0.1745, Validation Loss:2.4551, Validation Accuracy:0.1658\n",
    "Epoch #225: Loss:2.4252, Accuracy:0.1754, Validation Loss:2.4572, Validation Accuracy:0.1741\n",
    "Epoch #226: Loss:2.4255, Accuracy:0.1799, Validation Loss:2.4585, Validation Accuracy:0.1741\n",
    "Epoch #227: Loss:2.4255, Accuracy:0.1791, Validation Loss:2.4579, Validation Accuracy:0.1741\n",
    "Epoch #228: Loss:2.4245, Accuracy:0.1811, Validation Loss:2.4580, Validation Accuracy:0.1741\n",
    "Epoch #229: Loss:2.4248, Accuracy:0.1815, Validation Loss:2.4592, Validation Accuracy:0.1691\n",
    "Epoch #230: Loss:2.4250, Accuracy:0.1770, Validation Loss:2.4572, Validation Accuracy:0.1741\n",
    "Epoch #231: Loss:2.4241, Accuracy:0.1770, Validation Loss:2.4563, Validation Accuracy:0.1741\n",
    "Epoch #232: Loss:2.4247, Accuracy:0.1823, Validation Loss:2.4568, Validation Accuracy:0.1708\n",
    "Epoch #233: Loss:2.4232, Accuracy:0.1819, Validation Loss:2.4558, Validation Accuracy:0.1708\n",
    "Epoch #234: Loss:2.4229, Accuracy:0.1799, Validation Loss:2.4558, Validation Accuracy:0.1741\n",
    "Epoch #235: Loss:2.4232, Accuracy:0.1860, Validation Loss:2.4566, Validation Accuracy:0.1708\n",
    "Epoch #236: Loss:2.4236, Accuracy:0.1828, Validation Loss:2.4564, Validation Accuracy:0.1691\n",
    "Epoch #237: Loss:2.4241, Accuracy:0.1778, Validation Loss:2.4568, Validation Accuracy:0.1658\n",
    "Epoch #238: Loss:2.4236, Accuracy:0.1848, Validation Loss:2.4567, Validation Accuracy:0.1658\n",
    "Epoch #239: Loss:2.4234, Accuracy:0.1869, Validation Loss:2.4576, Validation Accuracy:0.1593\n",
    "Epoch #240: Loss:2.4229, Accuracy:0.1815, Validation Loss:2.4582, Validation Accuracy:0.1658\n",
    "Epoch #241: Loss:2.4231, Accuracy:0.1782, Validation Loss:2.4567, Validation Accuracy:0.1642\n",
    "Epoch #242: Loss:2.4227, Accuracy:0.1819, Validation Loss:2.4575, Validation Accuracy:0.1658\n",
    "Epoch #243: Loss:2.4227, Accuracy:0.1832, Validation Loss:2.4569, Validation Accuracy:0.1741\n",
    "Epoch #244: Loss:2.4230, Accuracy:0.1770, Validation Loss:2.4577, Validation Accuracy:0.1658\n",
    "Epoch #245: Loss:2.4224, Accuracy:0.1766, Validation Loss:2.4590, Validation Accuracy:0.1675\n",
    "Epoch #246: Loss:2.4227, Accuracy:0.1815, Validation Loss:2.4566, Validation Accuracy:0.1757\n",
    "Epoch #247: Loss:2.4222, Accuracy:0.1815, Validation Loss:2.4568, Validation Accuracy:0.1757\n",
    "Epoch #248: Loss:2.4216, Accuracy:0.1836, Validation Loss:2.4575, Validation Accuracy:0.1560\n",
    "Epoch #249: Loss:2.4219, Accuracy:0.1840, Validation Loss:2.4573, Validation Accuracy:0.1642\n",
    "Epoch #250: Loss:2.4216, Accuracy:0.1823, Validation Loss:2.4569, Validation Accuracy:0.1576\n",
    "Epoch #251: Loss:2.4218, Accuracy:0.1832, Validation Loss:2.4568, Validation Accuracy:0.1741\n",
    "Epoch #252: Loss:2.4208, Accuracy:0.1819, Validation Loss:2.4577, Validation Accuracy:0.1593\n",
    "Epoch #253: Loss:2.4216, Accuracy:0.1823, Validation Loss:2.4571, Validation Accuracy:0.1691\n",
    "Epoch #254: Loss:2.4218, Accuracy:0.1819, Validation Loss:2.4568, Validation Accuracy:0.1691\n",
    "Epoch #255: Loss:2.4210, Accuracy:0.1840, Validation Loss:2.4581, Validation Accuracy:0.1609\n",
    "Epoch #256: Loss:2.4207, Accuracy:0.1836, Validation Loss:2.4567, Validation Accuracy:0.1757\n",
    "Epoch #257: Loss:2.4215, Accuracy:0.1852, Validation Loss:2.4565, Validation Accuracy:0.1675\n",
    "Epoch #258: Loss:2.4204, Accuracy:0.1840, Validation Loss:2.4563, Validation Accuracy:0.1757\n",
    "Epoch #259: Loss:2.4202, Accuracy:0.1856, Validation Loss:2.4559, Validation Accuracy:0.1773\n",
    "Epoch #260: Loss:2.4204, Accuracy:0.1836, Validation Loss:2.4557, Validation Accuracy:0.1675\n",
    "Epoch #261: Loss:2.4198, Accuracy:0.1832, Validation Loss:2.4551, Validation Accuracy:0.1773\n",
    "Epoch #262: Loss:2.4203, Accuracy:0.1811, Validation Loss:2.4556, Validation Accuracy:0.1773\n",
    "Epoch #263: Loss:2.4198, Accuracy:0.1848, Validation Loss:2.4549, Validation Accuracy:0.1708\n",
    "Epoch #264: Loss:2.4195, Accuracy:0.1807, Validation Loss:2.4554, Validation Accuracy:0.1691\n",
    "Epoch #265: Loss:2.4196, Accuracy:0.1844, Validation Loss:2.4553, Validation Accuracy:0.1691\n",
    "Epoch #266: Loss:2.4193, Accuracy:0.1873, Validation Loss:2.4553, Validation Accuracy:0.1757\n",
    "Epoch #267: Loss:2.4201, Accuracy:0.1860, Validation Loss:2.4561, Validation Accuracy:0.1708\n",
    "Epoch #268: Loss:2.4209, Accuracy:0.1795, Validation Loss:2.4551, Validation Accuracy:0.1757\n",
    "Epoch #269: Loss:2.4205, Accuracy:0.1852, Validation Loss:2.4569, Validation Accuracy:0.1790\n",
    "Epoch #270: Loss:2.4190, Accuracy:0.1836, Validation Loss:2.4566, Validation Accuracy:0.1773\n",
    "Epoch #271: Loss:2.4199, Accuracy:0.1832, Validation Loss:2.4557, Validation Accuracy:0.1773\n",
    "Epoch #272: Loss:2.4189, Accuracy:0.1844, Validation Loss:2.4556, Validation Accuracy:0.1691\n",
    "Epoch #273: Loss:2.4196, Accuracy:0.1828, Validation Loss:2.4560, Validation Accuracy:0.1691\n",
    "Epoch #274: Loss:2.4189, Accuracy:0.1856, Validation Loss:2.4547, Validation Accuracy:0.1675\n",
    "Epoch #275: Loss:2.4187, Accuracy:0.1848, Validation Loss:2.4555, Validation Accuracy:0.1691\n",
    "Epoch #276: Loss:2.4190, Accuracy:0.1819, Validation Loss:2.4562, Validation Accuracy:0.1691\n",
    "Epoch #277: Loss:2.4185, Accuracy:0.1819, Validation Loss:2.4562, Validation Accuracy:0.1757\n",
    "Epoch #278: Loss:2.4185, Accuracy:0.1832, Validation Loss:2.4558, Validation Accuracy:0.1691\n",
    "Epoch #279: Loss:2.4182, Accuracy:0.1832, Validation Loss:2.4552, Validation Accuracy:0.1675\n",
    "Epoch #280: Loss:2.4183, Accuracy:0.1832, Validation Loss:2.4554, Validation Accuracy:0.1691\n",
    "Epoch #281: Loss:2.4188, Accuracy:0.1848, Validation Loss:2.4560, Validation Accuracy:0.1757\n",
    "Epoch #282: Loss:2.4180, Accuracy:0.1840, Validation Loss:2.4551, Validation Accuracy:0.1658\n",
    "Epoch #283: Loss:2.4182, Accuracy:0.1840, Validation Loss:2.4556, Validation Accuracy:0.1675\n",
    "Epoch #284: Loss:2.4193, Accuracy:0.1811, Validation Loss:2.4560, Validation Accuracy:0.1757\n",
    "Epoch #285: Loss:2.4196, Accuracy:0.1774, Validation Loss:2.4557, Validation Accuracy:0.1757\n",
    "Epoch #286: Loss:2.4176, Accuracy:0.1758, Validation Loss:2.4579, Validation Accuracy:0.1708\n",
    "Epoch #287: Loss:2.4179, Accuracy:0.1807, Validation Loss:2.4547, Validation Accuracy:0.1675\n",
    "Epoch #288: Loss:2.4182, Accuracy:0.1848, Validation Loss:2.4553, Validation Accuracy:0.1757\n",
    "Epoch #289: Loss:2.4184, Accuracy:0.1799, Validation Loss:2.4557, Validation Accuracy:0.1757\n",
    "Epoch #290: Loss:2.4168, Accuracy:0.1844, Validation Loss:2.4556, Validation Accuracy:0.1741\n",
    "Epoch #291: Loss:2.4179, Accuracy:0.1852, Validation Loss:2.4555, Validation Accuracy:0.1741\n",
    "Epoch #292: Loss:2.4189, Accuracy:0.1811, Validation Loss:2.4553, Validation Accuracy:0.1675\n",
    "Epoch #293: Loss:2.4171, Accuracy:0.1811, Validation Loss:2.4561, Validation Accuracy:0.1658\n",
    "Epoch #294: Loss:2.4184, Accuracy:0.1848, Validation Loss:2.4568, Validation Accuracy:0.1741\n",
    "Epoch #295: Loss:2.4167, Accuracy:0.1852, Validation Loss:2.4552, Validation Accuracy:0.1757\n",
    "Epoch #296: Loss:2.4172, Accuracy:0.1840, Validation Loss:2.4560, Validation Accuracy:0.1757\n",
    "Epoch #297: Loss:2.4173, Accuracy:0.1791, Validation Loss:2.4556, Validation Accuracy:0.1757\n",
    "Epoch #298: Loss:2.4172, Accuracy:0.1844, Validation Loss:2.4558, Validation Accuracy:0.1757\n",
    "Epoch #299: Loss:2.4165, Accuracy:0.1828, Validation Loss:2.4563, Validation Accuracy:0.1658\n",
    "Epoch #300: Loss:2.4161, Accuracy:0.1852, Validation Loss:2.4553, Validation Accuracy:0.1658\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45527625, Accuracy:0.1658\n",
    "Labels: ['by', 'ce', 'mb', 'sg', 'eo', 'eb', 'yd', 'my', 'ds', 'ek', 'sk', 'ck', 'ib', 'aa', 'eg']\n",
    "Confusion Matrix:\n",
    "      by  ce  mb  sg  eo  eb  yd  my  ds  ek  sk  ck  ib  aa  eg\n",
    "t:by   1   0   0  10  10  14   1   0   1   0   0   0   0   0   3\n",
    "t:ce   0   0   0   8   3   4   2   0   0   0   0   0   0   0  10\n",
    "t:mb   0   0   0  15  13   7   4   0   1   0   0   0   0   0  12\n",
    "t:sg   0   0   0  27  10   4   8   0   0   0   0   0   0   0   2\n",
    "t:eo   1   0   0  16   9   3   3   0   0   0   0   0   0   0   2\n",
    "t:eb   1   0   0   8  11   9   4   0   1   0   0   0   0   0  16\n",
    "t:yd   0   0   0  23  10   1  26   0   0   0   0   0   1   0   1\n",
    "t:my   0   0   0   3   4   4   2   0   2   0   0   0   0   0   5\n",
    "t:ds   2   0   0   4   7   3   1   0   7   0   0   0   0   0   7\n",
    "t:ek   0   0   0   5  16   9   6   0   0   0   0   0   0   0  12\n",
    "t:sk   1   0   0   9   5   6   1   0   2   0   0   0   0   0   9\n",
    "t:ck   1   0   0   5   2   7   1   0   1   0   0   0   0   0   6\n",
    "t:ib   0   0   0  18   4   1  28   0   0   0   0   0   0   0   3\n",
    "t:aa   1   0   0   4   5   4   1   0   6   0   0   0   0   0  13\n",
    "t:eg   0   0   0   3   8  16   0   0   1   0   0   0   0   0  22\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          by       0.12      0.03      0.04        40\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          sg       0.17      0.53      0.26        51\n",
    "          eo       0.08      0.26      0.12        34\n",
    "          eb       0.10      0.18      0.13        50\n",
    "          yd       0.30      0.42      0.35        62\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ds       0.32      0.23      0.26        31\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          eg       0.18      0.44      0.25        50\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.08      0.14      0.09       609\n",
    "weighted avg       0.10      0.17      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 01:07:08 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 37 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6972451773770336, 2.688040515276403, 2.6809493426618904, 2.6755524750413566, 2.6687854765279737, 2.6637359953474724, 2.6578177548394413, 2.6508042988518774, 2.6410851537300446, 2.6304094967583715, 2.6171880202927613, 2.6021278174639924, 2.5837463749257608, 2.5645293809705962, 2.549243797026636, 2.602576610490019, 2.5463253613959003, 2.5368956047521634, 2.527032601226531, 2.5138477746684758, 2.5149517904948717, 2.5063653082291677, 2.502388776816758, 2.5006686451008364, 2.497070853150341, 2.4940467432801947, 2.4934481431306486, 2.4876791859299483, 2.486455077608231, 2.486495194176735, 2.5175455677489733, 2.479962058450984, 2.5034418771615363, 2.4815900282711034, 2.4854644687696434, 2.4803046491150003, 2.4835530247398587, 2.478008274570083, 2.477423423225265, 2.474421170348996, 2.4848217365189726, 2.474911991598571, 2.472256882045852, 2.474226123202219, 2.4700168983885415, 2.469534168493963, 2.4687779908892753, 2.466967719333317, 2.4630390129653104, 2.461814519415544, 2.461005117701388, 2.4626017262782955, 2.4614903132120767, 2.46032678748195, 2.4605443873037456, 2.461761723989728, 2.46315168590577, 2.463301216244502, 2.462969628460889, 2.460661798079417, 2.4580087078420205, 2.458708196242259, 2.4599807904467403, 2.458235968509918, 2.4610019806766354, 2.460306494498292, 2.4613733890608613, 2.461477361876389, 2.463222359984575, 2.4622895255660384, 2.4633732773791785, 2.462995767201892, 2.4632017017389556, 2.4649525992388797, 2.4835003737745613, 2.4914325927865915, 2.4739762159012417, 2.4741937562162652, 2.4686690324241503, 2.468885278466887, 2.464183791321878, 2.4665798029093122, 2.464375046086429, 2.462874276297433, 2.4619718044262213, 2.4613059753267637, 2.4604923145720132, 2.460639290425969, 2.4604593552587852, 2.4589937837253064, 2.4594861995214705, 2.4584576754734435, 2.457475409327665, 2.455154753279412, 2.455998074244983, 2.4538942546092817, 2.454090021318207, 2.4539119359503436, 2.4540796424759237, 2.4537972339072645, 2.4616024435447357, 2.457236135730211, 2.4562758111405647, 2.4557550932190493, 2.4526888964015665, 2.453510460595192, 2.45304057163558, 2.4535768623226772, 2.452994778042748, 2.454657910884112, 2.452448751734591, 2.452206931873691, 2.4534128444339647, 2.4524368468568047, 2.450646690155681, 2.4521280583881198, 2.45228770957596, 2.452586117244902, 2.452618839705519, 2.4523590538889315, 2.450319222041539, 2.4510957517451644, 2.4526254026760608, 2.453702853426753, 2.4537975243942687, 2.45384540816246, 2.4561697399283475, 2.4573313457820998, 2.4556505026292723, 2.456443600662432, 2.4577692420220334, 2.461267424334446, 2.4537304254196743, 2.453357915377186, 2.451098571661462, 2.4523631406730813, 2.453061840608594, 2.4533912493481815, 2.4543972539980032, 2.453665343019958, 2.453222107613224, 2.4525665238573047, 2.4533939510339193, 2.4536885913761184, 2.450493730739224, 2.4553285377170457, 2.4541532625314244, 2.453992198057754, 2.455395500843944, 2.4522038238193407, 2.4511609891756807, 2.4522800970155814, 2.4515747773431986, 2.451219930241652, 2.450234260465124, 2.454467154097283, 2.45814940534006, 2.4553644578836624, 2.457576222411909, 2.456890061179601, 2.4586234421565614, 2.457318117270133, 2.457446252966945, 2.456086531844241, 2.457033609130308, 2.456454361209337, 2.4559102723946906, 2.4546903236746203, 2.4542209675355107, 2.4552824626415233, 2.4567601109177413, 2.4550839288677095, 2.4553720199415836, 2.455833590089394, 2.454753966558547, 2.4552120802241983, 2.454453784648225, 2.4539488892641366, 2.4533881483406854, 2.453007439674415, 2.454200795131364, 2.4535826485732506, 2.453688993046828, 2.454008321260975, 2.4543723570693694, 2.4543525710677474, 2.454391955546362, 2.454293244773727, 2.453235179919915, 2.4522654830137105, 2.454012901716436, 2.454785235801158, 2.4549868905485557, 2.4552565020293438, 2.454590803296695, 2.4550979364485968, 2.4552671337754073, 2.455166172315726, 2.4558060255348195, 2.454388723389073, 2.4545707675232284, 2.455284327709029, 2.4563925767375525, 2.455449982033966, 2.4540961822265475, 2.4543136364133487, 2.4563470857679746, 2.456214097137326, 2.455270508044263, 2.4544051265090165, 2.4564803171236136, 2.4580988547289118, 2.4569684997176497, 2.45534996994219, 2.4568941366104853, 2.4577587304639894, 2.4567511868594316, 2.455658899739458, 2.4559018095138625, 2.4555585983351533, 2.4568490751075434, 2.456400552210941, 2.45633056011106, 2.4550545853738517, 2.4572226022460386, 2.458457013460607, 2.457850445667511, 2.4580282166673633, 2.4591524342597997, 2.457165154721741, 2.45629979276109, 2.456772457789905, 2.4557781086375168, 2.4558297742176527, 2.4565523622071215, 2.4564180726488236, 2.456764382094585, 2.4567365595468353, 2.4576209949938144, 2.4581639872395935, 2.456696697056587, 2.457473253773155, 2.4569250566423038, 2.4576715522603254, 2.4590368165171204, 2.4566307013062225, 2.4567605908868346, 2.45750310033413, 2.4573167744528486, 2.4568951157317764, 2.4567672172790678, 2.457653607445202, 2.4570662947906845, 2.456758175773182, 2.458115387041189, 2.456670914181739, 2.456518190051926, 2.456266013663782, 2.455943974759583, 2.4556583653529875, 2.4550789901971424, 2.455618285193232, 2.454864083839755, 2.4554350732386796, 2.4553299772328345, 2.45534302605001, 2.4561182837963886, 2.4550518030407784, 2.456870773547193, 2.456609107786407, 2.455676438185969, 2.4556254146525816, 2.456002547040166, 2.4546821849491014, 2.455454754711959, 2.456178061089101, 2.456171126592727, 2.455759745709023, 2.4551658880925924, 2.4553566417474855, 2.456035418659204, 2.455108677029414, 2.455606938582923, 2.455965231596347, 2.455710847585268, 2.4578805459152497, 2.454708989226368, 2.455253305889311, 2.455724943643329, 2.455644475611168, 2.4554952905683094, 2.455273693614014, 2.456050129359579, 2.456778083920283, 2.4551675879505077, 2.4559589144827307, 2.455570156351099, 2.4558028497523665, 2.456314027015799, 2.4552763318780606], 'val_acc': [0.09688013105703693, 0.11822660097910462, 0.11165845598444367, 0.1050903114852647, 0.14614121499662525, 0.09852216718183167, 0.11822660067936862, 0.14449917867608453, 0.1395730703017003, 0.14121510642649504, 0.14121510642649504, 0.14121510642649504, 0.1559934318432667, 0.16748768471682993, 0.16256157614669972, 0.12972085365080482, 0.14449917788086658, 0.14614121509449823, 0.16420361227149446, 0.16420361227149446, 0.1559934317453937, 0.16748768392161195, 0.16584564760107126, 0.16091954002190498, 0.1707717559754555, 0.1625615756451007, 0.1625615752536088, 0.16584564750319827, 0.16584564760107126, 0.17405582832291797, 0.15763546787018845, 0.16091954011977797, 0.14942528724621473, 0.17241379219812322, 0.1740558292160089, 0.1691297199485337, 0.16748768362799302, 0.16091953912881404, 0.17241379210025023, 0.174055828127172, 0.16584564760107126, 0.16748768471682993, 0.16748768471682993, 0.16912971975278776, 0.17733990037676148, 0.17405582832291797, 0.17405582822504498, 0.174055828127172, 0.16748768362799302, 0.16912971975278776, 0.17733990037676148, 0.1822660088490187, 0.180623972626351, 0.17898193650155622, 0.180623972626351, 0.18226600983998262, 0.18062397272422395, 0.17733990047463447, 0.17898193650155622, 0.17569786425196673, 0.16584564760107126, 0.16912971985066075, 0.17241379200237725, 0.18062397272422395, 0.17733990047463447, 0.1707717558775825, 0.174055828127172, 0.17241379200237725, 0.16912971975278776, 0.16748768362799302, 0.16748768362799302, 0.16420361246724044, 0.16256157634244567, 0.16256157634244567, 0.16584564769894422, 0.16091954011977797, 0.15599343095017576, 0.15435139482538102, 0.1625615752536088, 0.1625615752536088, 0.16748768362799302, 0.15763546697709752, 0.16420361147627652, 0.1625615752536088, 0.16748768362799302, 0.16748768362799302, 0.16420361147627652, 0.16420361147627652, 0.16420361157414948, 0.16584564769894422, 0.16420361157414948, 0.16420361157414948, 0.16748768382373896, 0.1691297199485337, 0.16748768392161195, 0.1691297199485337, 0.16420361176989545, 0.16748768392161195, 0.16748768392161195, 0.1658456477968172, 0.16091953932456, 0.17241379219812322, 0.167487683725866, 0.16584564769894422, 0.16584564769894422, 0.16748768382373896, 0.1625615756451007, 0.16748768392161195, 0.16748768392161195, 0.1691297199485337, 0.16748768392161195, 0.1658456478946902, 0.16420361176989545, 0.16748768392161195, 0.1691297199485337, 0.1658456477968172, 0.16748768382373896, 0.16091953952030597, 0.16748768392161195, 0.16256157574297367, 0.16748768392161195, 0.16748768382373896, 0.16420361157414948, 0.16748768392161195, 0.16584564769894422, 0.17241379229599618, 0.16091953942243298, 0.16420361167202246, 0.16420361167202246, 0.16256157544935473, 0.16091953932456, 0.16584564769894422, 0.16256157544935473, 0.16420361167202246, 0.16420361157414948, 0.16420361167202246, 0.16420361167202246, 0.17077175607332848, 0.16748768392161195, 0.16420361167202246, 0.16420361167202246, 0.1691297200464067, 0.16420361157414948, 0.1658456478946902, 0.16584564760107126, 0.16420361157414948, 0.16420361157414948, 0.16256157544935473, 0.16420361167202246, 0.1691297200464067, 0.16912972024215267, 0.16420361167202246, 0.16748768382373896, 0.16420361167202246, 0.16256157554722772, 0.16256157544935473, 0.17077175617120144, 0.1658456477968172, 0.16256157554722772, 0.16256157554722772, 0.16748768382373896, 0.16748768382373896, 0.17405582832291797, 0.16748768392161195, 0.16256157554722772, 0.1658456477968172, 0.16256157554722772, 0.16420361167202246, 0.1691297200464067, 0.17077175617120144, 0.16256157554722772, 0.1658456477968172, 0.17077175607332848, 0.16256157554722772, 0.16256157554722772, 0.16748768392161195, 0.1658456478946902, 0.1658456477968172, 0.1756978644477127, 0.16748768382373896, 0.16091953942243298, 0.15435139482538102, 0.16256157544935473, 0.16420361147627652, 0.1576354670749705, 0.16584564769894422, 0.15599343104804875, 0.15435139482538102, 0.15927750319976525, 0.15927750319976525, 0.1576354670749705, 0.1707717559754555, 0.17241379210025023, 0.1707717559754555, 0.17241379210025023, 0.17241379210025023, 0.16584564769894422, 0.16420361176989545, 0.16584564769894422, 0.16584564769894422, 0.16748768382373896, 0.1658456478946902, 0.1658456478946902, 0.1691297199485337, 0.1691297199485337, 0.1691297199485337, 0.1691297199485337, 0.1691297199485337, 0.1691297199485337, 0.16748768382373896, 0.16748768382373896, 0.17405582832291797, 0.17405582832291797, 0.16420361157414948, 0.17405582832291797, 0.1625615756451007, 0.17405582832291797, 0.16420361157414948, 0.17241379239386917, 0.1740558285186639, 0.1756978644477127, 0.16420361176989545, 0.16420361157414948, 0.16584564769894422, 0.17405582832291797, 0.17405582832291797, 0.17405582832291797, 0.17405582832291797, 0.16912971985066075, 0.17405582822504498, 0.17405582822504498, 0.1707717559754555, 0.1707717559754555, 0.17405582822504498, 0.1707717559754555, 0.1691297200464067, 0.1658456477968172, 0.1658456477968172, 0.15927750310189226, 0.16584564760107126, 0.16420361167202246, 0.1658456478946902, 0.1740558285186639, 0.1658456478946902, 0.167487683725866, 0.17569786464345866, 0.1756978644477127, 0.15599343104804875, 0.16420361167202246, 0.15763546727071645, 0.1740558285186639, 0.15927750310189226, 0.1691297199485337, 0.1691297199485337, 0.16091953932456, 0.17569786464345866, 0.16748768411735793, 0.17569786464345866, 0.17733990057250745, 0.16748768401948494, 0.17733990057250745, 0.17733990057250745, 0.17077175617120144, 0.16912972024215267, 0.16912972024215267, 0.17569786464345866, 0.17077175617120144, 0.17569786464345866, 0.17898193679517518, 0.17733990057250745, 0.1773399008661264, 0.16912972024215267, 0.1691297199485337, 0.16748768401948494, 0.16912972024215267, 0.16912972024215267, 0.1756978644477127, 0.16912972024215267, 0.16748768411735793, 0.16912972024215267, 0.17569786464345866, 0.1658456478946902, 0.16748768411735793, 0.17569786464345866, 0.1756978644477127, 0.17077175617120144, 0.16748768401948494, 0.17569786474133164, 0.17569786474133164, 0.1740558285186639, 0.1740558285186639, 0.16748768411735793, 0.1658456478946902, 0.1740558285186639, 0.17569786474133164, 0.17569786474133164, 0.1756978644477127, 0.17569786474133164, 0.1658456478946902, 0.1658456478946902], 'loss': [2.705669339530522, 2.6945785915092766, 2.6863635609526897, 2.6803431849704875, 2.674263230780067, 2.667768074501711, 2.662211836045283, 2.6553939777233273, 2.6466277338885673, 2.6364159372552955, 2.623401176856039, 2.607727516358393, 2.5893945894691734, 2.570256590892157, 2.5519632196524307, 2.5446933821731035, 2.567719477353889, 2.5366277908152868, 2.513110805781715, 2.5185440666621717, 2.502864368152814, 2.5005443628808557, 2.4932203576794887, 2.490745984847051, 2.487691448748234, 2.4839671472749183, 2.4788104676123273, 2.4792599466547096, 2.4787712467279768, 2.4766717805999505, 2.478311470646633, 2.493583064696138, 2.4854237097244734, 2.4842408845067268, 2.476187981227585, 2.4731523473649544, 2.4691600287474644, 2.4660438346666966, 2.463783160665931, 2.4618128582682206, 2.462941740474662, 2.4645736463260848, 2.4640761572232726, 2.458195934941881, 2.4571164167392423, 2.454130128962303, 2.4532142446271203, 2.451880256547086, 2.4506286893292373, 2.4521361115042435, 2.4512442174633424, 2.450641194799842, 2.4495359745848106, 2.44893154530065, 2.4483305035185765, 2.44833564611431, 2.448567409339137, 2.447901937653152, 2.4463909470325134, 2.4463343771086583, 2.4450967437187994, 2.445273762415077, 2.4441674775901028, 2.4441752404402903, 2.444184826531694, 2.4446492784566702, 2.445478572199232, 2.4455753766046167, 2.4460196409871693, 2.446249323752871, 2.4443019899254708, 2.444984386197351, 2.4451184296265276, 2.4434184822458507, 2.485257145903194, 2.4934485949530005, 2.454709952763708, 2.460426718498402, 2.4485057710377345, 2.448624940868276, 2.444497398674121, 2.444921190293173, 2.4434120447483885, 2.441774026569155, 2.439864761775524, 2.4401014688323412, 2.438900001533712, 2.4386676012857738, 2.438128708618133, 2.4372032064676774, 2.436969572513745, 2.436235902049948, 2.4359017753013594, 2.435387647127469, 2.4351857484978083, 2.4348271882019983, 2.43362937042111, 2.433682069347624, 2.4339335709871452, 2.4336795586580124, 2.4372143449724577, 2.439560084176504, 2.4358621185075577, 2.4339973716030867, 2.4345194113572766, 2.4330524205671935, 2.433860315189714, 2.4321635998250035, 2.4328552620122075, 2.4317836881907815, 2.4316046000016542, 2.430874797695716, 2.430314433550198, 2.4306205488083545, 2.4295741898812797, 2.4302634695472167, 2.429868843521181, 2.4303397461618976, 2.430369472503662, 2.4297487699520417, 2.4299202809343594, 2.4308984788781074, 2.429927439954002, 2.430149254514941, 2.430026161009771, 2.4305151047403073, 2.4298974432739633, 2.430204321765312, 2.4313504640081827, 2.431278264889727, 2.4312022301205864, 2.4328169554410772, 2.430165284859816, 2.4314130428635363, 2.4310419383235047, 2.431067210888716, 2.430196673375625, 2.4303143639339315, 2.4305708741260506, 2.429340204516965, 2.429260769321199, 2.4294607913469632, 2.4295699503387516, 2.430517886159846, 2.4294762310795717, 2.4307078128477877, 2.4296590320383498, 2.4298757029265103, 2.429508943087756, 2.4292937017319383, 2.4285799558157795, 2.4291142261982945, 2.428426534781955, 2.4279240339933237, 2.428692774214539, 2.428651295354479, 2.431896583549296, 2.43246436686976, 2.429760753447515, 2.430500310451343, 2.430717587128312, 2.428518899118631, 2.426938774453541, 2.4272817487344605, 2.4264958435505077, 2.4265342762338062, 2.4264949663463806, 2.427504163209418, 2.427469752897227, 2.4290625641722943, 2.4283194711320943, 2.4271289871458643, 2.4275211568241, 2.4269919962364055, 2.426783263756754, 2.426958201993907, 2.42591373514101, 2.427191740580408, 2.427467617410899, 2.428609233421467, 2.428024391765712, 2.427539679646737, 2.4282793375990472, 2.4268094485790086, 2.4271743412135316, 2.4271753691060343, 2.426921751366993, 2.4265985828650316, 2.4265800050151913, 2.4273209618836704, 2.4262048547028026, 2.427345839662963, 2.4276394147892506, 2.426845975480285, 2.427530741153067, 2.4271963206649563, 2.4262313628343586, 2.4272430257386004, 2.427212340337295, 2.428248445992597, 2.428296443911793, 2.4282880450176263, 2.428024474306518, 2.4280130215004485, 2.4264516723718974, 2.425762946307047, 2.425800258961546, 2.4252128745985715, 2.4250691936735747, 2.426165873803642, 2.42653107692084, 2.425965592405879, 2.4269399505865894, 2.426260298331415, 2.4260825463633764, 2.4271628374926117, 2.426670676726825, 2.426848776090806, 2.426602072294733, 2.4269267177190135, 2.426597547041562, 2.4276203267138596, 2.4274833727176675, 2.4275047144605884, 2.4252027469494015, 2.425549580184341, 2.4254709684383697, 2.4245425544480277, 2.4248412559164625, 2.424964790520482, 2.424120734506564, 2.4246839561501567, 2.423234984419429, 2.4229095983798987, 2.423244958344916, 2.423620363721123, 2.4241418910956725, 2.423584306509343, 2.4233709321619306, 2.422917619720866, 2.423052523219365, 2.4226882508648004, 2.422711906491853, 2.423000705413505, 2.4224080154293617, 2.4227120327019347, 2.422191970108471, 2.421560132772771, 2.421870074281947, 2.421618853653236, 2.421753074598998, 2.420793497097321, 2.421638422286486, 2.4217860267881983, 2.42102077648625, 2.4206705253961394, 2.4214530621710746, 2.4204234233871866, 2.4201915351272363, 2.420378983534827, 2.4197869109911596, 2.420316380103266, 2.419848500238062, 2.4194599246587107, 2.4195549673123526, 2.4192844420243094, 2.420117946816666, 2.4208611275870697, 2.4205275320174513, 2.419009644784477, 2.4198900545891795, 2.418891503042264, 2.4195736979067446, 2.41894053678493, 2.4186788161432475, 2.419036689284401, 2.418506124817615, 2.4185181649069034, 2.418237448228213, 2.418285666745791, 2.4188087143202828, 2.4179576075786926, 2.418182807585542, 2.419310583568941, 2.419577685518676, 2.417577877475496, 2.417882584546381, 2.418199698685131, 2.4184480603470693, 2.4167509203329223, 2.417937986072329, 2.4188852632315005, 2.4171339654824573, 2.4183894228886285, 2.4167290142184656, 2.4171937515603443, 2.4173243836945333, 2.4171582583284477, 2.416478524315773, 2.4160885409408035], 'acc': [0.06529774104055683, 0.1059548257894095, 0.11950718640240801, 0.10759753548756272, 0.11170431263821326, 0.10718685806531926, 0.11416837753578868, 0.12484599660309433, 0.1318275157186285, 0.13305954877784365, 0.13264887115059448, 0.1330595485820173, 0.1535934290663173, 0.1638603704971944, 0.16344969309330964, 0.1593429157376534, 0.14989732970202482, 0.15770020544284177, 0.1630390138719116, 0.16016427034959657, 0.16755646827651735, 0.16468172375671183, 0.17125256742662473, 0.1704312114071797, 0.16673511321784534, 0.17823408613214747, 0.17494866442264229, 0.1733059559086265, 0.17166324504469455, 0.17207392285859072, 0.17494866638090575, 0.16427104654865343, 0.16509240178479306, 0.16960985736435688, 0.17412731114476612, 0.17289527829055668, 0.17535934301984385, 0.17618069862927743, 0.17987679619441532, 0.17577001965878192, 0.17700205288628534, 0.17700205366959074, 0.17700205445289613, 0.17412730955979663, 0.17371663313504362, 0.17864476335856458, 0.18151950805583775, 0.17453798700039883, 0.1737166317642592, 0.1761806980417984, 0.17289527731142496, 0.17864476394604364, 0.17700205228044757, 0.17494866461846864, 0.17535934221817973, 0.17125256625166663, 0.1745379869820401, 0.17618069823762475, 0.17535934300148512, 0.1741273097739817, 0.1733059555169738, 0.17494866579342672, 0.17618069823762475, 0.1774127303085288, 0.1794661181663341, 0.17782340812242498, 0.17987679740609086, 0.181108830651953, 0.17905544060334044, 0.1790554405849817, 0.17782340849571893, 0.17659137622898854, 0.17577002120703397, 0.17535934202235337, 0.1708418882235854, 0.1618069826210304, 0.17166324465304184, 0.17043121039133052, 0.18028747342083243, 0.17905544117246075, 0.17864476353603223, 0.17494866440428355, 0.17453798876283594, 0.179466120161315, 0.1774127315018456, 0.1761806984334511, 0.17823408714799666, 0.17864476396440235, 0.17905544156411346, 0.17864476474770774, 0.17577001944459683, 0.17741273071854022, 0.17494866461846864, 0.17782340851407766, 0.17535934380314924, 0.17782340892408907, 0.17535934241400608, 0.1745379869820401, 0.17577001965878192, 0.1733059559086265, 0.1761806992167565, 0.17494866440428355, 0.17494866481429497, 0.1790554421515925, 0.17535934262819114, 0.17577002102956635, 0.17535934399897557, 0.1806981512347286, 0.1798767973693734, 0.1782340865421589, 0.1794661197696623, 0.17782340792659862, 0.1782340865421589, 0.18069815283805682, 0.18110882887115715, 0.17864476316273825, 0.1798767976019172, 0.1774127311101929, 0.1782340857404948, 0.1774127318934983, 0.17700205407960215, 0.1782340855263097, 0.18028747343919116, 0.17577002042372858, 0.17864476474770774, 0.17577002101120762, 0.17535934182652702, 0.17700205349212308, 0.17577002159868668, 0.17782340851407766, 0.17987679582112134, 0.17248460065412816, 0.18028747343919116, 0.17864476414187, 0.17618069706266665, 0.17823408534884208, 0.17659137505403044, 0.1774127318751396, 0.1770020538837758, 0.17905544037079665, 0.1782340857404948, 0.17782340773077226, 0.17248459869586466, 0.17494866579342672, 0.17577002142121906, 0.17864476412351127, 0.1782340855263097, 0.17741273169767197, 0.17782340968903576, 0.17618069747267806, 0.17700205427542848, 0.1753593426098324, 0.1757700196404232, 0.17371663293921727, 0.175359342199821, 0.17700205447125483, 0.17494866440428355, 0.17453798756951913, 0.17823408534884208, 0.17782340931574178, 0.1798767960169477, 0.17823408613214747, 0.17741273011270245, 0.17987679699607942, 0.1802874732433648, 0.17946611879053057, 0.18069815125308733, 0.1790554409766344, 0.1782340857404948, 0.1786447649435341, 0.17864476435605506, 0.17700205407960215, 0.1786447649435341, 0.17987679738773213, 0.17987679640860038, 0.1806981526422305, 0.17823408556302714, 0.174127309382329, 0.18028747383084384, 0.177002052690459, 0.1774127310918342, 0.18151950825166407, 0.17618069884346252, 0.18069815260551303, 0.17987679779774354, 0.1774127307001815, 0.17618069704430794, 0.17864476433769633, 0.17823408691545287, 0.17905544195576614, 0.17864476376857602, 0.1749486642084572, 0.1778234094932094, 0.1778234095115681, 0.1757700204420873, 0.1761806980417984, 0.17659137466237776, 0.17453798854865088, 0.17618069864763616, 0.17618069825598345, 0.17577002024626096, 0.17782340773077226, 0.17494866598925307, 0.17577002061955493, 0.17659137546404186, 0.17412730996980805, 0.17905544216995123, 0.17577002042372858, 0.17700205368794944, 0.17946611978802104, 0.17248459850003833, 0.179466120161315, 0.1786447649435341, 0.17659137646153233, 0.177002052690459, 0.179876795625295, 0.18069815223221905, 0.1802874744183229, 0.1765913766757174, 0.17741273011270245, 0.1774127318934983, 0.1774127310918342, 0.1741273089906763, 0.17453798837118326, 0.17535934221817973, 0.1798767977793848, 0.17905544156411346, 0.18110883022358285, 0.18151950805583775, 0.1770020531004704, 0.17700205228044757, 0.18234086350616244, 0.18193018391147042, 0.179876795625295, 0.18603696165877937, 0.18275153916596876, 0.17782340911991543, 0.18480492900039625, 0.18685831709074532, 0.1815195078783701, 0.1782340867196265, 0.18193018391147042, 0.18316221656985351, 0.1770020531004704, 0.17659137546404186, 0.1815195066850533, 0.18151950768254377, 0.18357289497122872, 0.18398357239347218, 0.1823408625270307, 0.1831622169431475, 0.18193018588809262, 0.1823408627044983, 0.18193018449894946, 0.1839835720018195, 0.18357289555870776, 0.18521560603098702, 0.183983573586789, 0.18562628405906825, 0.18357289538124014, 0.18316221733480018, 0.18110882906698347, 0.1848049284129172, 0.18069815143055495, 0.18439425159651152, 0.18726899275055167, 0.18603696108965903, 0.17946611838051915, 0.18521560681429242, 0.18357289497122872, 0.18316221754898526, 0.18439424961988932, 0.18275154051839448, 0.18562628304321907, 0.18480492802126453, 0.18193018371564407, 0.18193018371564407, 0.18316221835064936, 0.1831622185464757, 0.183162218528117, 0.18480492723795913, 0.1839835725709398, 0.18398357219764583, 0.18110882906698347, 0.1774127310918342, 0.1757700200320759, 0.18069815164474, 0.18480492723795913, 0.17987679623113276, 0.18439425140068516, 0.18521560487438765, 0.18110882965446254, 0.1811088284611457, 0.18480492704213278, 0.18521560603098702, 0.18398357337260393, 0.17905544078080807, 0.18439425140068516, 0.18275154112423225, 0.18521560683265115]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
