{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf18.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 22:22:13 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '0Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000029619B69550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000029615EA6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1020, Accuracy:0.3088, Validation Loss:1.0824, Validation Accuracy:0.3892\n",
    "Epoch #2: Loss:1.0790, Accuracy:0.3996, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #3: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0751, Accuracy:0.3947, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0748, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #10: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0737, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0733, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3826\n",
    "Epoch #14: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0741, Validation Accuracy:0.3810\n",
    "Epoch #15: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0744, Validation Accuracy:0.3810\n",
    "Epoch #16: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.3810\n",
    "Epoch #17: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #18: Loss:1.0738, Accuracy:0.3984, Validation Loss:1.0745, Validation Accuracy:0.3859\n",
    "Epoch #19: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #20: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #21: Loss:1.0736, Accuracy:0.4016, Validation Loss:1.0746, Validation Accuracy:0.3826\n",
    "Epoch #22: Loss:1.0734, Accuracy:0.4037, Validation Loss:1.0746, Validation Accuracy:0.3793\n",
    "Epoch #23: Loss:1.0734, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.3760\n",
    "Epoch #24: Loss:1.0734, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3826\n",
    "Epoch #25: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #26: Loss:1.0733, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #27: Loss:1.0732, Accuracy:0.4057, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #28: Loss:1.0731, Accuracy:0.4025, Validation Loss:1.0742, Validation Accuracy:0.3826\n",
    "Epoch #29: Loss:1.0731, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.3826\n",
    "Epoch #30: Loss:1.0731, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #31: Loss:1.0731, Accuracy:0.4037, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #32: Loss:1.0730, Accuracy:0.4045, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #33: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #34: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.3810\n",
    "Epoch #35: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #36: Loss:1.0730, Accuracy:0.3971, Validation Loss:1.0744, Validation Accuracy:0.4023\n",
    "Epoch #37: Loss:1.0724, Accuracy:0.4090, Validation Loss:1.0751, Validation Accuracy:0.3793\n",
    "Epoch #38: Loss:1.0723, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #39: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.3711\n",
    "Epoch #40: Loss:1.0747, Accuracy:0.3938, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #41: Loss:1.0731, Accuracy:0.3992, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #42: Loss:1.0732, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.3810\n",
    "Epoch #43: Loss:1.0731, Accuracy:0.4053, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #44: Loss:1.0731, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #45: Loss:1.0730, Accuracy:0.4066, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #46: Loss:1.0728, Accuracy:0.4099, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #47: Loss:1.0727, Accuracy:0.4012, Validation Loss:1.0744, Validation Accuracy:0.3810\n",
    "Epoch #48: Loss:1.0725, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #49: Loss:1.0727, Accuracy:0.4115, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #50: Loss:1.0726, Accuracy:0.4049, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #51: Loss:1.0728, Accuracy:0.4033, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #52: Loss:1.0727, Accuracy:0.4049, Validation Loss:1.0748, Validation Accuracy:0.3793\n",
    "Epoch #53: Loss:1.0724, Accuracy:0.4033, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #54: Loss:1.0721, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #55: Loss:1.0723, Accuracy:0.4045, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #56: Loss:1.0721, Accuracy:0.4037, Validation Loss:1.0754, Validation Accuracy:0.3842\n",
    "Epoch #57: Loss:1.0722, Accuracy:0.4185, Validation Loss:1.0750, Validation Accuracy:0.4105\n",
    "Epoch #58: Loss:1.0717, Accuracy:0.4164, Validation Loss:1.0750, Validation Accuracy:0.3744\n",
    "Epoch #59: Loss:1.0718, Accuracy:0.4045, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #60: Loss:1.0716, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0721, Accuracy:0.4144, Validation Loss:1.0760, Validation Accuracy:0.3957\n",
    "Epoch #62: Loss:1.0713, Accuracy:0.4168, Validation Loss:1.0759, Validation Accuracy:0.3760\n",
    "Epoch #63: Loss:1.0718, Accuracy:0.4074, Validation Loss:1.0765, Validation Accuracy:0.3744\n",
    "Epoch #64: Loss:1.0715, Accuracy:0.4082, Validation Loss:1.0768, Validation Accuracy:0.3826\n",
    "Epoch #65: Loss:1.0709, Accuracy:0.4082, Validation Loss:1.0764, Validation Accuracy:0.3777\n",
    "Epoch #66: Loss:1.0704, Accuracy:0.4049, Validation Loss:1.0759, Validation Accuracy:0.3810\n",
    "Epoch #67: Loss:1.0718, Accuracy:0.4074, Validation Loss:1.0757, Validation Accuracy:0.3793\n",
    "Epoch #68: Loss:1.0721, Accuracy:0.4029, Validation Loss:1.0762, Validation Accuracy:0.3892\n",
    "Epoch #69: Loss:1.0719, Accuracy:0.3926, Validation Loss:1.0769, Validation Accuracy:0.3842\n",
    "Epoch #70: Loss:1.0722, Accuracy:0.4029, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #71: Loss:1.0716, Accuracy:0.4090, Validation Loss:1.0741, Validation Accuracy:0.3793\n",
    "Epoch #72: Loss:1.0710, Accuracy:0.4115, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #73: Loss:1.0708, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #74: Loss:1.0703, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #75: Loss:1.0699, Accuracy:0.4136, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0698, Accuracy:0.4111, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #77: Loss:1.0686, Accuracy:0.4136, Validation Loss:1.0737, Validation Accuracy:0.3826\n",
    "Epoch #78: Loss:1.0689, Accuracy:0.4123, Validation Loss:1.0731, Validation Accuracy:0.3777\n",
    "Epoch #79: Loss:1.0684, Accuracy:0.4189, Validation Loss:1.0734, Validation Accuracy:0.3777\n",
    "Epoch #80: Loss:1.0698, Accuracy:0.4107, Validation Loss:1.0743, Validation Accuracy:0.3711\n",
    "Epoch #81: Loss:1.0710, Accuracy:0.3889, Validation Loss:1.0759, Validation Accuracy:0.3695\n",
    "Epoch #82: Loss:1.0719, Accuracy:0.4066, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #83: Loss:1.0664, Accuracy:0.4177, Validation Loss:1.0787, Validation Accuracy:0.3810\n",
    "Epoch #84: Loss:1.0712, Accuracy:0.4086, Validation Loss:1.0758, Validation Accuracy:0.3875\n",
    "Epoch #85: Loss:1.0681, Accuracy:0.4144, Validation Loss:1.0766, Validation Accuracy:0.3826\n",
    "Epoch #86: Loss:1.0684, Accuracy:0.4070, Validation Loss:1.0763, Validation Accuracy:0.3941\n",
    "Epoch #87: Loss:1.0682, Accuracy:0.4021, Validation Loss:1.0765, Validation Accuracy:0.3974\n",
    "Epoch #88: Loss:1.0683, Accuracy:0.4090, Validation Loss:1.0837, Validation Accuracy:0.3777\n",
    "Epoch #89: Loss:1.0791, Accuracy:0.4078, Validation Loss:1.0834, Validation Accuracy:0.3810\n",
    "Epoch #90: Loss:1.0728, Accuracy:0.4078, Validation Loss:1.0821, Validation Accuracy:0.3826\n",
    "Epoch #91: Loss:1.0767, Accuracy:0.4029, Validation Loss:1.0780, Validation Accuracy:0.4056\n",
    "Epoch #92: Loss:1.0714, Accuracy:0.4144, Validation Loss:1.0775, Validation Accuracy:0.3662\n",
    "Epoch #93: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0761, Validation Accuracy:0.4007\n",
    "Epoch #94: Loss:1.0733, Accuracy:0.3934, Validation Loss:1.0732, Validation Accuracy:0.4105\n",
    "Epoch #95: Loss:1.0722, Accuracy:0.4074, Validation Loss:1.0763, Validation Accuracy:0.3810\n",
    "Epoch #96: Loss:1.0727, Accuracy:0.4021, Validation Loss:1.0772, Validation Accuracy:0.3842\n",
    "Epoch #97: Loss:1.0726, Accuracy:0.4041, Validation Loss:1.0764, Validation Accuracy:0.3727\n",
    "Epoch #98: Loss:1.0713, Accuracy:0.4045, Validation Loss:1.0760, Validation Accuracy:0.3711\n",
    "Epoch #99: Loss:1.0707, Accuracy:0.4099, Validation Loss:1.0762, Validation Accuracy:0.3793\n",
    "Epoch #100: Loss:1.0704, Accuracy:0.4078, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #101: Loss:1.0707, Accuracy:0.4029, Validation Loss:1.0773, Validation Accuracy:0.3892\n",
    "Epoch #102: Loss:1.0698, Accuracy:0.4021, Validation Loss:1.0786, Validation Accuracy:0.3892\n",
    "Epoch #103: Loss:1.0701, Accuracy:0.4045, Validation Loss:1.0789, Validation Accuracy:0.3875\n",
    "Epoch #104: Loss:1.0692, Accuracy:0.4156, Validation Loss:1.0784, Validation Accuracy:0.3777\n",
    "Epoch #105: Loss:1.0700, Accuracy:0.3992, Validation Loss:1.0776, Validation Accuracy:0.3924\n",
    "Epoch #106: Loss:1.0691, Accuracy:0.4103, Validation Loss:1.0778, Validation Accuracy:0.3957\n",
    "Epoch #107: Loss:1.0690, Accuracy:0.4107, Validation Loss:1.0771, Validation Accuracy:0.4056\n",
    "Epoch #108: Loss:1.0691, Accuracy:0.4140, Validation Loss:1.0769, Validation Accuracy:0.3990\n",
    "Epoch #109: Loss:1.0687, Accuracy:0.4119, Validation Loss:1.0785, Validation Accuracy:0.3924\n",
    "Epoch #110: Loss:1.0690, Accuracy:0.4037, Validation Loss:1.0792, Validation Accuracy:0.3695\n",
    "Epoch #111: Loss:1.0679, Accuracy:0.4136, Validation Loss:1.0795, Validation Accuracy:0.3645\n",
    "Epoch #112: Loss:1.0679, Accuracy:0.4144, Validation Loss:1.0795, Validation Accuracy:0.3612\n",
    "Epoch #113: Loss:1.0685, Accuracy:0.4074, Validation Loss:1.0803, Validation Accuracy:0.3695\n",
    "Epoch #114: Loss:1.0685, Accuracy:0.4016, Validation Loss:1.0790, Validation Accuracy:0.3678\n",
    "Epoch #115: Loss:1.0679, Accuracy:0.4181, Validation Loss:1.0781, Validation Accuracy:0.3662\n",
    "Epoch #116: Loss:1.0677, Accuracy:0.4156, Validation Loss:1.0768, Validation Accuracy:0.3678\n",
    "Epoch #117: Loss:1.0675, Accuracy:0.4156, Validation Loss:1.0756, Validation Accuracy:0.3612\n",
    "Epoch #118: Loss:1.0664, Accuracy:0.4107, Validation Loss:1.0751, Validation Accuracy:0.4105\n",
    "Epoch #119: Loss:1.0660, Accuracy:0.4070, Validation Loss:1.0754, Validation Accuracy:0.3645\n",
    "Epoch #120: Loss:1.0669, Accuracy:0.4140, Validation Loss:1.0752, Validation Accuracy:0.3711\n",
    "Epoch #121: Loss:1.0662, Accuracy:0.4123, Validation Loss:1.0751, Validation Accuracy:0.3777\n",
    "Epoch #122: Loss:1.0668, Accuracy:0.4062, Validation Loss:1.0750, Validation Accuracy:0.3777\n",
    "Epoch #123: Loss:1.0665, Accuracy:0.4123, Validation Loss:1.0756, Validation Accuracy:0.3662\n",
    "Epoch #124: Loss:1.0650, Accuracy:0.4152, Validation Loss:1.0792, Validation Accuracy:0.3924\n",
    "Epoch #125: Loss:1.0653, Accuracy:0.4209, Validation Loss:1.0802, Validation Accuracy:0.3842\n",
    "Epoch #126: Loss:1.0646, Accuracy:0.4037, Validation Loss:1.0800, Validation Accuracy:0.3744\n",
    "Epoch #127: Loss:1.0668, Accuracy:0.4131, Validation Loss:1.0798, Validation Accuracy:0.3727\n",
    "Epoch #128: Loss:1.0662, Accuracy:0.4140, Validation Loss:1.0817, Validation Accuracy:0.3842\n",
    "Epoch #129: Loss:1.0638, Accuracy:0.4177, Validation Loss:1.0805, Validation Accuracy:0.3563\n",
    "Epoch #130: Loss:1.0655, Accuracy:0.4214, Validation Loss:1.0796, Validation Accuracy:0.3612\n",
    "Epoch #131: Loss:1.0662, Accuracy:0.4177, Validation Loss:1.0801, Validation Accuracy:0.3810\n",
    "Epoch #132: Loss:1.0658, Accuracy:0.4115, Validation Loss:1.0791, Validation Accuracy:0.3892\n",
    "Epoch #133: Loss:1.0665, Accuracy:0.4037, Validation Loss:1.0796, Validation Accuracy:0.3826\n",
    "Epoch #134: Loss:1.0654, Accuracy:0.4177, Validation Loss:1.0821, Validation Accuracy:0.3908\n",
    "Epoch #135: Loss:1.0625, Accuracy:0.4214, Validation Loss:1.0799, Validation Accuracy:0.4039\n",
    "Epoch #136: Loss:1.0641, Accuracy:0.4136, Validation Loss:1.0781, Validation Accuracy:0.3793\n",
    "Epoch #137: Loss:1.0636, Accuracy:0.4201, Validation Loss:1.0799, Validation Accuracy:0.3810\n",
    "Epoch #138: Loss:1.0619, Accuracy:0.4271, Validation Loss:1.0797, Validation Accuracy:0.4007\n",
    "Epoch #139: Loss:1.0624, Accuracy:0.4144, Validation Loss:1.0797, Validation Accuracy:0.3793\n",
    "Epoch #140: Loss:1.0617, Accuracy:0.4177, Validation Loss:1.0797, Validation Accuracy:0.3875\n",
    "Epoch #141: Loss:1.0644, Accuracy:0.4099, Validation Loss:1.0797, Validation Accuracy:0.3892\n",
    "Epoch #142: Loss:1.0601, Accuracy:0.4230, Validation Loss:1.0787, Validation Accuracy:0.3990\n",
    "Epoch #143: Loss:1.0613, Accuracy:0.4168, Validation Loss:1.0790, Validation Accuracy:0.4023\n",
    "Epoch #144: Loss:1.0592, Accuracy:0.4238, Validation Loss:1.0775, Validation Accuracy:0.3859\n",
    "Epoch #145: Loss:1.0598, Accuracy:0.4218, Validation Loss:1.0791, Validation Accuracy:0.4007\n",
    "Epoch #146: Loss:1.0623, Accuracy:0.4144, Validation Loss:1.0772, Validation Accuracy:0.3859\n",
    "Epoch #147: Loss:1.0602, Accuracy:0.4197, Validation Loss:1.0772, Validation Accuracy:0.3777\n",
    "Epoch #148: Loss:1.0584, Accuracy:0.4214, Validation Loss:1.0748, Validation Accuracy:0.4122\n",
    "Epoch #149: Loss:1.0585, Accuracy:0.4156, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #150: Loss:1.0590, Accuracy:0.4271, Validation Loss:1.0752, Validation Accuracy:0.4204\n",
    "Epoch #151: Loss:1.0589, Accuracy:0.4234, Validation Loss:1.0760, Validation Accuracy:0.3711\n",
    "Epoch #152: Loss:1.0581, Accuracy:0.4185, Validation Loss:1.0772, Validation Accuracy:0.3974\n",
    "Epoch #153: Loss:1.0565, Accuracy:0.4275, Validation Loss:1.0762, Validation Accuracy:0.3744\n",
    "Epoch #154: Loss:1.0572, Accuracy:0.4242, Validation Loss:1.0759, Validation Accuracy:0.3727\n",
    "Epoch #155: Loss:1.0582, Accuracy:0.4152, Validation Loss:1.0787, Validation Accuracy:0.3793\n",
    "Epoch #156: Loss:1.0587, Accuracy:0.4201, Validation Loss:1.0789, Validation Accuracy:0.4269\n",
    "Epoch #157: Loss:1.0587, Accuracy:0.4308, Validation Loss:1.0767, Validation Accuracy:0.4023\n",
    "Epoch #158: Loss:1.0602, Accuracy:0.4205, Validation Loss:1.0761, Validation Accuracy:0.4253\n",
    "Epoch #159: Loss:1.0596, Accuracy:0.4193, Validation Loss:1.0803, Validation Accuracy:0.4023\n",
    "Epoch #160: Loss:1.0559, Accuracy:0.4329, Validation Loss:1.0817, Validation Accuracy:0.3826\n",
    "Epoch #161: Loss:1.0586, Accuracy:0.4296, Validation Loss:1.0791, Validation Accuracy:0.3990\n",
    "Epoch #162: Loss:1.0562, Accuracy:0.4267, Validation Loss:1.0801, Validation Accuracy:0.3892\n",
    "Epoch #163: Loss:1.0558, Accuracy:0.4197, Validation Loss:1.0802, Validation Accuracy:0.4056\n",
    "Epoch #164: Loss:1.0564, Accuracy:0.4267, Validation Loss:1.0807, Validation Accuracy:0.3711\n",
    "Epoch #165: Loss:1.0556, Accuracy:0.4279, Validation Loss:1.0800, Validation Accuracy:0.3990\n",
    "Epoch #166: Loss:1.0572, Accuracy:0.4177, Validation Loss:1.0805, Validation Accuracy:0.3892\n",
    "Epoch #167: Loss:1.0558, Accuracy:0.4205, Validation Loss:1.0799, Validation Accuracy:0.3777\n",
    "Epoch #168: Loss:1.0549, Accuracy:0.4226, Validation Loss:1.0780, Validation Accuracy:0.4056\n",
    "Epoch #169: Loss:1.0550, Accuracy:0.4259, Validation Loss:1.0786, Validation Accuracy:0.3793\n",
    "Epoch #170: Loss:1.0524, Accuracy:0.4197, Validation Loss:1.0796, Validation Accuracy:0.3990\n",
    "Epoch #171: Loss:1.0534, Accuracy:0.4279, Validation Loss:1.0782, Validation Accuracy:0.3875\n",
    "Epoch #172: Loss:1.0528, Accuracy:0.4255, Validation Loss:1.0783, Validation Accuracy:0.3941\n",
    "Epoch #173: Loss:1.0511, Accuracy:0.4308, Validation Loss:1.0810, Validation Accuracy:0.3842\n",
    "Epoch #174: Loss:1.0525, Accuracy:0.4238, Validation Loss:1.0797, Validation Accuracy:0.3908\n",
    "Epoch #175: Loss:1.0520, Accuracy:0.4209, Validation Loss:1.0796, Validation Accuracy:0.3941\n",
    "Epoch #176: Loss:1.0535, Accuracy:0.4172, Validation Loss:1.0845, Validation Accuracy:0.3875\n",
    "Epoch #177: Loss:1.0543, Accuracy:0.4242, Validation Loss:1.0832, Validation Accuracy:0.3908\n",
    "Epoch #178: Loss:1.0517, Accuracy:0.4242, Validation Loss:1.0818, Validation Accuracy:0.3941\n",
    "Epoch #179: Loss:1.0526, Accuracy:0.4238, Validation Loss:1.0804, Validation Accuracy:0.3875\n",
    "Epoch #180: Loss:1.0553, Accuracy:0.4057, Validation Loss:1.0796, Validation Accuracy:0.4023\n",
    "Epoch #181: Loss:1.0521, Accuracy:0.4324, Validation Loss:1.0849, Validation Accuracy:0.3810\n",
    "Epoch #182: Loss:1.0543, Accuracy:0.4177, Validation Loss:1.0810, Validation Accuracy:0.3777\n",
    "Epoch #183: Loss:1.0534, Accuracy:0.4189, Validation Loss:1.0837, Validation Accuracy:0.3974\n",
    "Epoch #184: Loss:1.0531, Accuracy:0.4283, Validation Loss:1.0814, Validation Accuracy:0.4039\n",
    "Epoch #185: Loss:1.0514, Accuracy:0.4267, Validation Loss:1.0820, Validation Accuracy:0.3810\n",
    "Epoch #186: Loss:1.0514, Accuracy:0.4197, Validation Loss:1.0828, Validation Accuracy:0.3974\n",
    "Epoch #187: Loss:1.0502, Accuracy:0.4246, Validation Loss:1.0817, Validation Accuracy:0.3645\n",
    "Epoch #188: Loss:1.0538, Accuracy:0.4197, Validation Loss:1.0787, Validation Accuracy:0.4007\n",
    "Epoch #189: Loss:1.0528, Accuracy:0.4218, Validation Loss:1.0819, Validation Accuracy:0.3924\n",
    "Epoch #190: Loss:1.0559, Accuracy:0.4177, Validation Loss:1.0887, Validation Accuracy:0.4056\n",
    "Epoch #191: Loss:1.0684, Accuracy:0.3934, Validation Loss:1.0836, Validation Accuracy:0.3744\n",
    "Epoch #192: Loss:1.0606, Accuracy:0.4168, Validation Loss:1.0819, Validation Accuracy:0.4072\n",
    "Epoch #193: Loss:1.0561, Accuracy:0.4333, Validation Loss:1.0757, Validation Accuracy:0.3974\n",
    "Epoch #194: Loss:1.0532, Accuracy:0.4053, Validation Loss:1.0739, Validation Accuracy:0.3875\n",
    "Epoch #195: Loss:1.0518, Accuracy:0.4234, Validation Loss:1.0754, Validation Accuracy:0.3974\n",
    "Epoch #196: Loss:1.0512, Accuracy:0.4287, Validation Loss:1.0756, Validation Accuracy:0.3744\n",
    "Epoch #197: Loss:1.0508, Accuracy:0.4251, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #198: Loss:1.0485, Accuracy:0.4341, Validation Loss:1.0771, Validation Accuracy:0.3842\n",
    "Epoch #199: Loss:1.0474, Accuracy:0.4329, Validation Loss:1.0814, Validation Accuracy:0.3842\n",
    "Epoch #200: Loss:1.0479, Accuracy:0.4345, Validation Loss:1.0843, Validation Accuracy:0.3924\n",
    "Epoch #201: Loss:1.0511, Accuracy:0.4312, Validation Loss:1.0805, Validation Accuracy:0.3810\n",
    "Epoch #202: Loss:1.0523, Accuracy:0.4251, Validation Loss:1.0852, Validation Accuracy:0.3727\n",
    "Epoch #203: Loss:1.0504, Accuracy:0.4246, Validation Loss:1.0903, Validation Accuracy:0.3875\n",
    "Epoch #204: Loss:1.0464, Accuracy:0.4349, Validation Loss:1.0896, Validation Accuracy:0.3744\n",
    "Epoch #205: Loss:1.0439, Accuracy:0.4333, Validation Loss:1.0857, Validation Accuracy:0.3924\n",
    "Epoch #206: Loss:1.0471, Accuracy:0.4226, Validation Loss:1.0867, Validation Accuracy:0.3908\n",
    "Epoch #207: Loss:1.0479, Accuracy:0.4242, Validation Loss:1.0901, Validation Accuracy:0.3990\n",
    "Epoch #208: Loss:1.0459, Accuracy:0.4292, Validation Loss:1.0909, Validation Accuracy:0.3760\n",
    "Epoch #209: Loss:1.0470, Accuracy:0.4312, Validation Loss:1.0905, Validation Accuracy:0.3908\n",
    "Epoch #210: Loss:1.0482, Accuracy:0.4329, Validation Loss:1.0876, Validation Accuracy:0.3859\n",
    "Epoch #211: Loss:1.0457, Accuracy:0.4246, Validation Loss:1.0930, Validation Accuracy:0.3990\n",
    "Epoch #212: Loss:1.0465, Accuracy:0.4300, Validation Loss:1.0932, Validation Accuracy:0.3924\n",
    "Epoch #213: Loss:1.0461, Accuracy:0.4259, Validation Loss:1.0877, Validation Accuracy:0.3941\n",
    "Epoch #214: Loss:1.0446, Accuracy:0.4283, Validation Loss:1.0876, Validation Accuracy:0.3957\n",
    "Epoch #215: Loss:1.0443, Accuracy:0.4329, Validation Loss:1.0881, Validation Accuracy:0.4072\n",
    "Epoch #216: Loss:1.0446, Accuracy:0.4324, Validation Loss:1.0852, Validation Accuracy:0.3777\n",
    "Epoch #217: Loss:1.0459, Accuracy:0.4333, Validation Loss:1.0892, Validation Accuracy:0.3826\n",
    "Epoch #218: Loss:1.0415, Accuracy:0.4324, Validation Loss:1.0875, Validation Accuracy:0.3777\n",
    "Epoch #219: Loss:1.0425, Accuracy:0.4316, Validation Loss:1.0889, Validation Accuracy:0.3924\n",
    "Epoch #220: Loss:1.0454, Accuracy:0.4275, Validation Loss:1.0865, Validation Accuracy:0.3760\n",
    "Epoch #221: Loss:1.0426, Accuracy:0.4407, Validation Loss:1.0848, Validation Accuracy:0.3957\n",
    "Epoch #222: Loss:1.0468, Accuracy:0.4292, Validation Loss:1.0813, Validation Accuracy:0.3859\n",
    "Epoch #223: Loss:1.0467, Accuracy:0.4349, Validation Loss:1.0801, Validation Accuracy:0.3974\n",
    "Epoch #224: Loss:1.0448, Accuracy:0.4378, Validation Loss:1.0892, Validation Accuracy:0.3777\n",
    "Epoch #225: Loss:1.0458, Accuracy:0.4304, Validation Loss:1.0824, Validation Accuracy:0.4056\n",
    "Epoch #226: Loss:1.0472, Accuracy:0.4296, Validation Loss:1.0808, Validation Accuracy:0.4056\n",
    "Epoch #227: Loss:1.0451, Accuracy:0.4308, Validation Loss:1.0773, Validation Accuracy:0.3875\n",
    "Epoch #228: Loss:1.0444, Accuracy:0.4419, Validation Loss:1.0793, Validation Accuracy:0.3957\n",
    "Epoch #229: Loss:1.0445, Accuracy:0.4386, Validation Loss:1.0791, Validation Accuracy:0.3777\n",
    "Epoch #230: Loss:1.0448, Accuracy:0.4386, Validation Loss:1.0802, Validation Accuracy:0.3941\n",
    "Epoch #231: Loss:1.0461, Accuracy:0.4353, Validation Loss:1.0817, Validation Accuracy:0.3875\n",
    "Epoch #232: Loss:1.0476, Accuracy:0.4345, Validation Loss:1.0790, Validation Accuracy:0.3908\n",
    "Epoch #233: Loss:1.0445, Accuracy:0.4324, Validation Loss:1.0869, Validation Accuracy:0.3892\n",
    "Epoch #234: Loss:1.0434, Accuracy:0.4353, Validation Loss:1.0887, Validation Accuracy:0.3908\n",
    "Epoch #235: Loss:1.0427, Accuracy:0.4341, Validation Loss:1.0915, Validation Accuracy:0.3826\n",
    "Epoch #236: Loss:1.0415, Accuracy:0.4386, Validation Loss:1.0903, Validation Accuracy:0.3908\n",
    "Epoch #237: Loss:1.0418, Accuracy:0.4398, Validation Loss:1.0915, Validation Accuracy:0.4023\n",
    "Epoch #238: Loss:1.0412, Accuracy:0.4378, Validation Loss:1.0925, Validation Accuracy:0.3678\n",
    "Epoch #239: Loss:1.0428, Accuracy:0.4353, Validation Loss:1.0923, Validation Accuracy:0.3810\n",
    "Epoch #240: Loss:1.0428, Accuracy:0.4398, Validation Loss:1.0918, Validation Accuracy:0.3826\n",
    "Epoch #241: Loss:1.0421, Accuracy:0.4357, Validation Loss:1.1000, Validation Accuracy:0.3760\n",
    "Epoch #242: Loss:1.0454, Accuracy:0.4320, Validation Loss:1.0950, Validation Accuracy:0.3662\n",
    "Epoch #243: Loss:1.0430, Accuracy:0.4345, Validation Loss:1.0939, Validation Accuracy:0.3974\n",
    "Epoch #244: Loss:1.0439, Accuracy:0.4386, Validation Loss:1.0927, Validation Accuracy:0.3563\n",
    "Epoch #245: Loss:1.0431, Accuracy:0.4292, Validation Loss:1.0964, Validation Accuracy:0.3941\n",
    "Epoch #246: Loss:1.0515, Accuracy:0.4251, Validation Loss:1.0888, Validation Accuracy:0.3859\n",
    "Epoch #247: Loss:1.0424, Accuracy:0.4353, Validation Loss:1.0909, Validation Accuracy:0.4072\n",
    "Epoch #248: Loss:1.0426, Accuracy:0.4349, Validation Loss:1.0914, Validation Accuracy:0.3974\n",
    "Epoch #249: Loss:1.0474, Accuracy:0.4127, Validation Loss:1.0902, Validation Accuracy:0.3760\n",
    "Epoch #250: Loss:1.0526, Accuracy:0.4242, Validation Loss:1.0951, Validation Accuracy:0.3892\n",
    "Epoch #251: Loss:1.0448, Accuracy:0.4181, Validation Loss:1.0912, Validation Accuracy:0.3612\n",
    "Epoch #252: Loss:1.0426, Accuracy:0.4209, Validation Loss:1.0870, Validation Accuracy:0.3974\n",
    "Epoch #253: Loss:1.0436, Accuracy:0.4222, Validation Loss:1.0890, Validation Accuracy:0.3974\n",
    "Epoch #254: Loss:1.0428, Accuracy:0.4214, Validation Loss:1.0900, Validation Accuracy:0.3924\n",
    "Epoch #255: Loss:1.0400, Accuracy:0.4402, Validation Loss:1.0924, Validation Accuracy:0.3990\n",
    "Epoch #256: Loss:1.0409, Accuracy:0.4337, Validation Loss:1.0953, Validation Accuracy:0.3974\n",
    "Epoch #257: Loss:1.0409, Accuracy:0.4337, Validation Loss:1.0996, Validation Accuracy:0.3908\n",
    "Epoch #258: Loss:1.0414, Accuracy:0.4370, Validation Loss:1.1010, Validation Accuracy:0.3826\n",
    "Epoch #259: Loss:1.0426, Accuracy:0.4361, Validation Loss:1.0927, Validation Accuracy:0.3842\n",
    "Epoch #260: Loss:1.0433, Accuracy:0.4324, Validation Loss:1.0904, Validation Accuracy:0.3727\n",
    "Epoch #261: Loss:1.0438, Accuracy:0.4341, Validation Loss:1.0817, Validation Accuracy:0.4007\n",
    "Epoch #262: Loss:1.0479, Accuracy:0.4296, Validation Loss:1.0783, Validation Accuracy:0.3892\n",
    "Epoch #263: Loss:1.0474, Accuracy:0.4226, Validation Loss:1.0769, Validation Accuracy:0.3957\n",
    "Epoch #264: Loss:1.0482, Accuracy:0.4255, Validation Loss:1.0811, Validation Accuracy:0.3859\n",
    "Epoch #265: Loss:1.0493, Accuracy:0.4255, Validation Loss:1.0811, Validation Accuracy:0.3826\n",
    "Epoch #266: Loss:1.0488, Accuracy:0.4144, Validation Loss:1.0759, Validation Accuracy:0.3810\n",
    "Epoch #267: Loss:1.0449, Accuracy:0.4271, Validation Loss:1.0796, Validation Accuracy:0.3974\n",
    "Epoch #268: Loss:1.0451, Accuracy:0.4251, Validation Loss:1.0828, Validation Accuracy:0.3924\n",
    "Epoch #269: Loss:1.0425, Accuracy:0.4246, Validation Loss:1.0826, Validation Accuracy:0.3810\n",
    "Epoch #270: Loss:1.0411, Accuracy:0.4333, Validation Loss:1.0834, Validation Accuracy:0.3990\n",
    "Epoch #271: Loss:1.0394, Accuracy:0.4320, Validation Loss:1.0864, Validation Accuracy:0.4007\n",
    "Epoch #272: Loss:1.0390, Accuracy:0.4353, Validation Loss:1.0870, Validation Accuracy:0.3842\n",
    "Epoch #273: Loss:1.0402, Accuracy:0.4168, Validation Loss:1.0900, Validation Accuracy:0.4138\n",
    "Epoch #274: Loss:1.0397, Accuracy:0.4374, Validation Loss:1.0931, Validation Accuracy:0.4056\n",
    "Epoch #275: Loss:1.0401, Accuracy:0.4382, Validation Loss:1.0962, Validation Accuracy:0.4039\n",
    "Epoch #276: Loss:1.0431, Accuracy:0.4316, Validation Loss:1.0984, Validation Accuracy:0.4039\n",
    "Epoch #277: Loss:1.0422, Accuracy:0.4366, Validation Loss:1.0984, Validation Accuracy:0.3974\n",
    "Epoch #278: Loss:1.0394, Accuracy:0.4324, Validation Loss:1.0916, Validation Accuracy:0.4056\n",
    "Epoch #279: Loss:1.0379, Accuracy:0.4382, Validation Loss:1.0893, Validation Accuracy:0.3826\n",
    "Epoch #280: Loss:1.0391, Accuracy:0.4308, Validation Loss:1.0901, Validation Accuracy:0.4154\n",
    "Epoch #281: Loss:1.0401, Accuracy:0.4423, Validation Loss:1.0860, Validation Accuracy:0.3941\n",
    "Epoch #282: Loss:1.0374, Accuracy:0.4255, Validation Loss:1.0894, Validation Accuracy:0.4039\n",
    "Epoch #283: Loss:1.0376, Accuracy:0.4382, Validation Loss:1.0855, Validation Accuracy:0.3892\n",
    "Epoch #284: Loss:1.0369, Accuracy:0.4366, Validation Loss:1.0817, Validation Accuracy:0.4138\n",
    "Epoch #285: Loss:1.0385, Accuracy:0.4304, Validation Loss:1.0866, Validation Accuracy:0.3777\n",
    "Epoch #286: Loss:1.0375, Accuracy:0.4320, Validation Loss:1.0923, Validation Accuracy:0.4023\n",
    "Epoch #287: Loss:1.0400, Accuracy:0.4390, Validation Loss:1.0908, Validation Accuracy:0.3908\n",
    "Epoch #288: Loss:1.0470, Accuracy:0.4197, Validation Loss:1.0982, Validation Accuracy:0.3941\n",
    "Epoch #289: Loss:1.0516, Accuracy:0.4234, Validation Loss:1.0966, Validation Accuracy:0.3859\n",
    "Epoch #290: Loss:1.0502, Accuracy:0.4185, Validation Loss:1.0938, Validation Accuracy:0.3892\n",
    "Epoch #291: Loss:1.0489, Accuracy:0.4189, Validation Loss:1.0885, Validation Accuracy:0.3826\n",
    "Epoch #292: Loss:1.0493, Accuracy:0.4259, Validation Loss:1.0950, Validation Accuracy:0.3941\n",
    "Epoch #293: Loss:1.0547, Accuracy:0.4131, Validation Loss:1.0904, Validation Accuracy:0.3777\n",
    "Epoch #294: Loss:1.0513, Accuracy:0.3992, Validation Loss:1.0806, Validation Accuracy:0.3810\n",
    "Epoch #295: Loss:1.0469, Accuracy:0.4193, Validation Loss:1.0796, Validation Accuracy:0.4023\n",
    "Epoch #296: Loss:1.0453, Accuracy:0.4259, Validation Loss:1.0794, Validation Accuracy:0.4023\n",
    "Epoch #297: Loss:1.0447, Accuracy:0.4255, Validation Loss:1.0779, Validation Accuracy:0.3957\n",
    "Epoch #298: Loss:1.0424, Accuracy:0.4366, Validation Loss:1.0800, Validation Accuracy:0.3908\n",
    "Epoch #299: Loss:1.0407, Accuracy:0.4382, Validation Loss:1.0833, Validation Accuracy:0.3990\n",
    "Epoch #300: Loss:1.0391, Accuracy:0.4333, Validation Loss:1.0825, Validation Accuracy:0.3908\n",
    "\n",
    "Test:\n",
    "Test Loss:1.08254850, Accuracy:0.3908\n",
    "Labels: ['03', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03  02   01\n",
    "t:03   0  37  105\n",
    "t:02   0  67  160\n",
    "t:01   1  68  171\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       142\n",
    "          02       0.39      0.30      0.34       227\n",
    "          01       0.39      0.71      0.51       240\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.26      0.34      0.28       609\n",
    "weighted avg       0.30      0.39      0.32       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 23:03:05 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 51 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0823906795144669, 1.0745449528122575, 1.0746204091606077, 1.0754093039407715, 1.0753760378936241, 1.0743026310587165, 1.0740125494441768, 1.0740091395495561, 1.07409235017836, 1.0739599245130917, 1.0741347518851996, 1.0742262656661286, 1.0740368254666257, 1.074103403365475, 1.0743682404065562, 1.0744197756198828, 1.0742582204110909, 1.074476322517019, 1.0745482881276673, 1.0744793481623207, 1.0745561287320893, 1.0746131781091048, 1.0745101360656162, 1.0744078392269967, 1.0743979888988051, 1.0744039785294306, 1.074318353178466, 1.074187076346236, 1.074390353436149, 1.0744946427728939, 1.0743719548818904, 1.074536588195901, 1.0741894885236993, 1.0743903945428006, 1.074519210652569, 1.0744140171652357, 1.0750897300654445, 1.074909395967994, 1.0749557247302803, 1.0745116947906945, 1.0745854074340344, 1.0746830335783057, 1.07449939156988, 1.0741061716048392, 1.073791254721643, 1.073736276728375, 1.0743751862561957, 1.0745189317145762, 1.0747810032567366, 1.0746412682415816, 1.0746613267215797, 1.0747600418006258, 1.0743847202589163, 1.0741565335168823, 1.0746626779559407, 1.0754496963153333, 1.0749768829110808, 1.075038472811381, 1.0752862879795393, 1.0753246516429733, 1.0759635922944017, 1.0759405205010977, 1.0765222050677772, 1.0768064571718865, 1.0764464547089951, 1.0758974244833384, 1.075665442031397, 1.0762168427406273, 1.076871955727513, 1.0751159776412011, 1.0741347080381045, 1.074451593342673, 1.0747260366167342, 1.0745858282878482, 1.0745352935321226, 1.0737536556419285, 1.0737352690281734, 1.0730539292146029, 1.0733504019347317, 1.074277654657223, 1.0758636097602656, 1.0746940456587692, 1.0786923205323995, 1.0757588739269863, 1.0766223164027549, 1.0763260852331402, 1.0764768229329527, 1.083652001100612, 1.0834094771414946, 1.0820601080438774, 1.078010828428472, 1.077535935419142, 1.0761179454220926, 1.0731641921308044, 1.076345331758897, 1.077173026715985, 1.076394430913753, 1.076015717681797, 1.0762259729194328, 1.0766899922406927, 1.0772808091393833, 1.0785652438212303, 1.0788891906613003, 1.0784001914151196, 1.0776119310476118, 1.0777501265207927, 1.0771228589839341, 1.0769023474409858, 1.0785487620114105, 1.0792465025959734, 1.0795435474815431, 1.079511991862593, 1.080282187226958, 1.0790287121176132, 1.0781458945109927, 1.0767923757947724, 1.0755718507985959, 1.0750932437054237, 1.0754020521401968, 1.075242671277527, 1.0750914690725517, 1.0750158815744084, 1.0755574566194381, 1.079202496555247, 1.0801514122873692, 1.0799525903755025, 1.079758009887094, 1.0817242567175127, 1.0805388567678642, 1.0795694157016298, 1.0801244116769049, 1.0791267630306176, 1.0795653140407868, 1.082089358949896, 1.079916570574192, 1.0780908192319822, 1.0799400001911108, 1.0797045217163261, 1.0797283635742363, 1.0797261011424324, 1.0797289040288314, 1.078736657971036, 1.0790347966850293, 1.0775177672578784, 1.0790862457701333, 1.0772019871349992, 1.077177044793303, 1.0747508415447666, 1.075239267255285, 1.0751573863287864, 1.075991305811652, 1.0772414540226627, 1.0762057805491982, 1.0759198869194695, 1.078670010778117, 1.0788838816393773, 1.0767388610025541, 1.0761390601472902, 1.0803057286148197, 1.0816819303728678, 1.079071317400251, 1.0800863386962214, 1.080210141556212, 1.0807192024143262, 1.0799841718329193, 1.0805212590103275, 1.0799160718134864, 1.077985991202356, 1.0786065115717245, 1.0796031904925267, 1.0781995905639699, 1.0783250087196212, 1.0809605084420817, 1.0797453795747805, 1.0795905202480371, 1.0844966907219347, 1.0832367908386957, 1.0817578371326715, 1.0804416988479288, 1.0796377281054292, 1.0848830219951562, 1.0809552481609026, 1.0837312615759462, 1.081436847818309, 1.0819625173296248, 1.0828280587893206, 1.0816646712558415, 1.0786506751879488, 1.081874985804503, 1.0887374592140586, 1.0836420346950661, 1.0819246048606284, 1.0757041713482836, 1.0739088074131355, 1.0753842357344228, 1.0755717762193853, 1.074588686178862, 1.0771264489648378, 1.0813518836971965, 1.0842823510491006, 1.080468802616514, 1.0851915538408878, 1.0903428019756949, 1.0896027845702148, 1.0856920684303948, 1.0866527555415588, 1.0900720590832589, 1.0909372497661947, 1.0904616213392937, 1.0876021911749503, 1.0929911913738657, 1.0932082299919943, 1.0876799677001627, 1.0875931007325748, 1.0880836559633904, 1.0852049348389574, 1.0891790076821113, 1.087498948491853, 1.0888605250905103, 1.0864562054573021, 1.084769334111895, 1.081306073270212, 1.080149262605238, 1.089242003821387, 1.0823923746744792, 1.0807608328820841, 1.0773140419097174, 1.0793261253970794, 1.0790744552079876, 1.080249119470468, 1.0817018160091831, 1.0789653397546026, 1.0868816504924756, 1.0887474469559142, 1.0914578578742267, 1.090347448984782, 1.0914547603901579, 1.0924886827202658, 1.0922835282308518, 1.0918286422203327, 1.1000439069541217, 1.0949942394234669, 1.093874239373481, 1.092725639272793, 1.0963849567231678, 1.0887555917495577, 1.090895282615386, 1.0914301598209075, 1.0902220915103782, 1.0951447907731255, 1.0912385196325618, 1.0870176514577004, 1.0889730218596059, 1.09000856163858, 1.0923867460542125, 1.095299180113819, 1.0996364783770933, 1.1010241488909291, 1.0926689532002791, 1.090386391860511, 1.0817028810629508, 1.0783403949393036, 1.0769269777636223, 1.0811281227713148, 1.0810810155273463, 1.0759384003766064, 1.0795509819130984, 1.0828089661198883, 1.0826080452240943, 1.0834372938168655, 1.0863878513596132, 1.0870440523025438, 1.0900031334073672, 1.093136850249004, 1.096243801179582, 1.0983566697595155, 1.098360666109032, 1.0916198266942316, 1.0892852161122464, 1.0901124904112667, 1.0860124595451042, 1.0893827862731733, 1.0854874575275115, 1.0816812186405576, 1.086555248019339, 1.0923180229753893, 1.090838899557618, 1.0982287589748114, 1.0965933731232567, 1.0938496572043508, 1.0885445161406042, 1.0949919208125725, 1.0903934103319015, 1.080633609752937, 1.0795615625694663, 1.0794136263858314, 1.0779025799339432, 1.0800202897029558, 1.0833458820195816, 1.0825485901292322], 'val_acc': [0.38916256035294244, 0.3908045964777372, 0.3957307048521214, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3908045965756102, 0.39408866892307265, 0.39408866892307265, 0.3940886687273267, 0.3825944160495094, 0.3809523797289687, 0.38095238002258763, 0.3809523797289687, 0.3842364520764312, 0.38587848820122594, 0.3825944159516364, 0.38095237982684166, 0.3825944160495094, 0.379310343604174, 0.37602627135458444, 0.38259441585376347, 0.3842364520764312, 0.3842364519785582, 0.3858784882990989, 0.38259441585376347, 0.3825944159516364, 0.3809523797289687, 0.38423645217430413, 0.38259441585376347, 0.38587848820122594, 0.38095237963109574, 0.38095237992471465, 0.40229884964491935, 0.379310343506301, 0.379310343506301, 0.3711001626865813, 0.3842364520764312, 0.3891625600593235, 0.38095237953322275, 0.3875205243260207, 0.38587848800547997, 0.3842364519785582, 0.3875205241302747, 0.38095237953322275, 0.3990147771017109, 0.3875205244238936, 0.3842364520764312, 0.38587848820122594, 0.379310343604174, 0.3842364518806852, 0.37602627135458444, 0.3875205243260207, 0.3842364518806852, 0.41050903026889307, 0.37438423513191676, 0.379310343604174, 0.3940886685315807, 0.3957307048521214, 0.37602627135458444, 0.37438423513191676, 0.38259441585376347, 0.37766830757725217, 0.38095237982684166, 0.3793103437020469, 0.38916256054868836, 0.3842364520764312, 0.3842364518806852, 0.379310343604174, 0.3825944160495094, 0.39244663260253193, 0.3842364520764312, 0.39408866882519966, 0.39080459667348316, 0.38259441624525536, 0.37766830757725217, 0.37766830767512516, 0.3711001631759462, 0.36945812695327845, 0.39244663260253193, 0.38095237982684166, 0.3875205242281477, 0.3825944159516364, 0.39408866882519966, 0.3973727412705351, 0.37766830728363326, 0.38095237992471465, 0.38259441634312835, 0.4055829223838737, 0.366174054605816, 0.40065681371587053, 0.41050903026889307, 0.38095237953322275, 0.3842364518806852, 0.37274219920286794, 0.3711001630780732, 0.379310343604174, 0.3875205244238936, 0.38916256045081543, 0.38916256054868836, 0.3875205245217666, 0.37766830757725217, 0.39244663289615084, 0.3957307054393593, 0.40558292189450884, 0.3990147772974569, 0.39244663260253193, 0.36945812695327845, 0.3645320187746402, 0.36124794623143175, 0.3694581275405163, 0.3678160909263567, 0.36617405499730793, 0.36781609121997566, 0.3612479464271777, 0.41050903085613094, 0.3645320186767672, 0.37110016346956515, 0.37766830787087113, 0.3776683079687441, 0.36617405499730793, 0.3924466333855158, 0.3842364522721771, 0.37438423562128165, 0.372742199007122, 0.3842364522721771, 0.3563218379549205, 0.36124794632930474, 0.38095237982684166, 0.38916256064656135, 0.38259441624525536, 0.3908045965756102, 0.403940886063333, 0.3793103439956659, 0.3809523802183336, 0.40065681361799754, 0.3793103438977929, 0.3875205243260207, 0.38916256054868836, 0.3990147774932028, 0.40229884993853826, 0.38587848859271784, 0.4006568135201246, 0.38587848839697186, 0.37766830777299815, 0.4121510668830527, 0.38587848839697186, 0.4203612475070264, 0.3711001631759462, 0.39737274136840806, 0.37438423542553567, 0.372742199007122, 0.3793103437020469, 0.4269293921040784, 0.40229884954704637, 0.4252873557835377, 0.40229884954704637, 0.3825944160495094, 0.3990147775910758, 0.38916256074443434, 0.40558292199238183, 0.3711001632738192, 0.3990147774932028, 0.38916256054868836, 0.37766830787087113, 0.40558292199238183, 0.3793103438977929, 0.39901477739532987, 0.3875205242281477, 0.3940886690209456, 0.3842364520764312, 0.39080459667348316, 0.39408866931456454, 0.3875205247175126, 0.39080459696710207, 0.39408866882519966, 0.3875205244238936, 0.4022988498406653, 0.38095238002258763, 0.37766830777299815, 0.39737274146628104, 0.4039408858675871, 0.38095237992471465, 0.39737274136840806, 0.3645320185788942, 0.40065681361799754, 0.39244663289615084, 0.40558292179663585, 0.37438423552340866, 0.4072249583129225, 0.3973727417599, 0.3875205246196396, 0.39737274136840806, 0.3743842358170276, 0.397372741662027, 0.3842364522721771, 0.38423645217430413, 0.3924466329940238, 0.38095237992471465, 0.37274219930074093, 0.3875205242281477, 0.37438423562128165, 0.3924466329940238, 0.39080459696710207, 0.3990147775910758, 0.3760262718439494, 0.39080459696710207, 0.38587848820122594, 0.39901477739532987, 0.3924466329940238, 0.39408866921669156, 0.3957307051457404, 0.4072249580193036, 0.37766830777299815, 0.3825944160495094, 0.37766830777299815, 0.39244663289615084, 0.3760262717460764, 0.3957307051457404, 0.38587848859271784, 0.397372741662027, 0.37766830787087113, 0.40558292189450884, 0.40558292199238183, 0.38752052481538557, 0.3957307054393593, 0.3776683079687441, 0.39408866931456454, 0.3875205247175126, 0.390804597358594, 0.3891625611359263, 0.39080459716284804, 0.38259441624525536, 0.39080459696710207, 0.40229884974279234, 0.3678160908284837, 0.38095237992471465, 0.3825944157558905, 0.37602627135458444, 0.366174054605816, 0.39737274117266214, 0.3563218378570475, 0.3940886690209456, 0.38587848800547997, 0.4072249580193036, 0.39737274097691616, 0.3760262715503304, 0.38916256074443434, 0.3612479466229237, 0.39737274146628104, 0.39737274117266214, 0.3924466330918968, 0.3990147776889488, 0.39737274156415403, 0.39080459696710207, 0.38259441624525536, 0.3842364522721771, 0.3727421993986139, 0.4006568135201246, 0.38916256074443434, 0.3957307050478674, 0.38587848859271784, 0.3825944160495094, 0.38095238002258763, 0.39737274146628104, 0.3924466329940238, 0.3809523803162066, 0.3990147776889488, 0.40065681371587053, 0.38423645276154206, 0.41379310310572043, 0.4055829223838737, 0.40394088596546, 0.4039408857697141, 0.3973727412705351, 0.40558292199238183, 0.38259441634312835, 0.41543513923051517, 0.39408866941243753, 0.4039408857697141, 0.3891625610380533, 0.41379310261635555, 0.37766830757725217, 0.40229884964491935, 0.39080459667348316, 0.3940886690209456, 0.38587848820122594, 0.38916256045081543, 0.38259441585376347, 0.3940886686294537, 0.3776683080666171, 0.3809523801204606, 0.40229884993853826, 0.4022988498406653, 0.3957307053414863, 0.39080459696710207, 0.3990147775910758, 0.3908045968692291], 'loss': [1.1020496483945748, 1.0789682530769333, 1.0741698293470505, 1.0746047121298632, 1.0750936478315192, 1.074786262296798, 1.074040264711243, 1.0737543607394553, 1.0738438211672114, 1.0738141338928036, 1.0736517887095895, 1.0734519223168155, 1.073259054415035, 1.0735393183432076, 1.0736010107905958, 1.0736225693132844, 1.0735555830922214, 1.073752301821229, 1.0735896162918217, 1.0736654698726333, 1.0735641741899495, 1.0733571355592544, 1.0734333681130066, 1.07339989257789, 1.0734681017834549, 1.0733040518339654, 1.0731694792819952, 1.073135466252509, 1.073056947083444, 1.073052636211168, 1.0731409002868058, 1.072953090432733, 1.0730118502336852, 1.0733233092012346, 1.0730443252919881, 1.0730357281236433, 1.0724158625338356, 1.0722710910519047, 1.073075417571489, 1.0746557130950678, 1.0730503921880858, 1.0731715993959556, 1.07306778455417, 1.0730754755850445, 1.0730051978657622, 1.0727909083728673, 1.0726641306886928, 1.072465959956269, 1.0727245626508333, 1.072619215712655, 1.0727922761219972, 1.0727195720163458, 1.0724496788067983, 1.0721378259835057, 1.072304796144458, 1.0721333731371274, 1.0721873610416233, 1.0717047442645753, 1.0718147452117481, 1.0715681197462141, 1.072114374407508, 1.0713371590177627, 1.071761401624895, 1.0714608701102788, 1.070918541424573, 1.0703765051565621, 1.071764844105229, 1.072121464936885, 1.0719145988781593, 1.072215176460924, 1.0715964525387272, 1.0710422235347896, 1.0707504423736791, 1.070266236025205, 1.0698817146877, 1.0698267820189866, 1.0686161621884889, 1.068899451193134, 1.068434289642428, 1.0697948340762566, 1.0710317564206446, 1.0719443894509662, 1.0664030931323951, 1.071162661683633, 1.0681292752710456, 1.068449419235057, 1.068173759771813, 1.068315660047825, 1.079124021677021, 1.0727734362565027, 1.0767226350380898, 1.0713777384963614, 1.0731108172963042, 1.0733058799218838, 1.0721631443720823, 1.0726872580252145, 1.0726462671154577, 1.0712641968619407, 1.0706708178138342, 1.0704431577874405, 1.0706733195933473, 1.0698410874274722, 1.0701333203110117, 1.0692408282653996, 1.0699944355159814, 1.0691154310101112, 1.0690207181769964, 1.0691443575970692, 1.0686716981002682, 1.0690173679798292, 1.0678809119935397, 1.0679120713925214, 1.068511574958629, 1.06848244397792, 1.0678693932429477, 1.0677444724821212, 1.067523601216702, 1.0664151044841665, 1.0660306947187232, 1.0668653210575332, 1.0661622686307777, 1.0668214426393137, 1.0664790103567698, 1.064953718684782, 1.0652588106033982, 1.0645896341276855, 1.06683120972322, 1.066217416855344, 1.063841803117944, 1.0654569629281454, 1.0661561882716184, 1.0657710637400037, 1.0664640694428273, 1.0653731447470507, 1.062486361429187, 1.0641161946545392, 1.0636431443373036, 1.061907336403457, 1.0623903128645504, 1.0617007811211463, 1.0643870768361023, 1.0601318917969658, 1.061300496692775, 1.0592365301610018, 1.059771232340615, 1.0623454003852986, 1.060200093952782, 1.058387890586618, 1.05846170164966, 1.0590289765070107, 1.058889467515495, 1.0581053186980607, 1.0564676410118903, 1.057204815151755, 1.0581542151664562, 1.0586720715313231, 1.0586540446878703, 1.0602320167807828, 1.0595908762737956, 1.055934728783014, 1.0585992927668766, 1.0562267860592758, 1.0557598325016562, 1.056440519601168, 1.0556190121100424, 1.05721450522695, 1.055796653534108, 1.0549227108456025, 1.0549693933502604, 1.0524009042696787, 1.053399533853394, 1.0527973420810894, 1.0510777373578268, 1.052478954385683, 1.0520478924686658, 1.0534766321064755, 1.0542828588759874, 1.0517042627569586, 1.0526275464886268, 1.055336690977124, 1.052096546650912, 1.054299106588109, 1.0533808960806907, 1.0531325622260939, 1.0513726024412278, 1.0513757749749404, 1.05016433453413, 1.053819414428617, 1.052823491360862, 1.0558724329946467, 1.068423552434792, 1.0605815185413714, 1.0560540072482225, 1.0532003901088018, 1.0518379708824706, 1.0511765791894965, 1.0508138871535628, 1.0485183531253979, 1.0474410276882946, 1.0479016530195546, 1.0511419782893123, 1.0523442280121162, 1.0504139331821543, 1.0463752228131773, 1.0438543267808167, 1.0471371953247508, 1.0479260661029228, 1.0458803799118104, 1.0469554024310572, 1.0481953574891452, 1.0457413829082827, 1.0465263146884143, 1.0461300249217227, 1.0445959827004028, 1.044300457584295, 1.044554368279553, 1.0459304865870387, 1.041495972835063, 1.0425423598632186, 1.045403895975383, 1.0426213638004092, 1.046845688565311, 1.0466672391617322, 1.0448337365959215, 1.0457962960922742, 1.0472452249370316, 1.0450693053631321, 1.044367947666552, 1.0445481424703735, 1.0448172398416413, 1.0461131344096126, 1.0476067176344948, 1.0445213267935376, 1.043427857236451, 1.0426550376097035, 1.0415372164097654, 1.04182479557805, 1.0411827578926478, 1.0428310554375149, 1.0428006627966002, 1.042061213105611, 1.045430070614668, 1.0430247038052067, 1.043876098607355, 1.0430892419521325, 1.051536980448807, 1.0424428911424515, 1.0425710376038444, 1.0474403322110186, 1.0525636709201507, 1.0447610732711072, 1.0425972199293132, 1.0436239107923095, 1.0428140353862754, 1.040014709486364, 1.0408712027254046, 1.0409044913442718, 1.0414201320319205, 1.0426057584966233, 1.0432981470037535, 1.0438248616714008, 1.0478883459337929, 1.047434577217337, 1.048210494963785, 1.0493392851318422, 1.04876015269536, 1.0448567956380042, 1.0451417467187807, 1.0425036460711972, 1.0410896824125881, 1.0393902514749485, 1.0390317745522062, 1.040182658681145, 1.0396718821731192, 1.040061455781455, 1.0431144402991575, 1.0422207923885243, 1.0393636867985343, 1.0378727604476334, 1.039117786722751, 1.0400656383385158, 1.0373704546041311, 1.0375946139408088, 1.036909995431528, 1.0385465443746265, 1.03753967015895, 1.0399860931372986, 1.0470139626849604, 1.0516042531638174, 1.050224888104433, 1.0489308920239522, 1.049265557099172, 1.0547025393656393, 1.0513405247145855, 1.0468640545799992, 1.0452828404350203, 1.0446531101418717, 1.0424315258218033, 1.0407200974360629, 1.0391498506925925], 'acc': [0.3088295689598491, 0.3995893247073681, 0.39466118953311224, 0.3942505128941742, 0.3946611928988776, 0.3942505152808078, 0.3942505117559335, 0.39425051449750237, 0.39425051492587254, 0.394250513714197, 0.40205338861663237, 0.39630390121706704, 0.3958932255572607, 0.3999999985680198, 0.40041067974768135, 0.40082135497911753, 0.3999999985680198, 0.3983572912656796, 0.4004106785727233, 0.3979466098901917, 0.40164270845282, 0.40369610088806623, 0.4041067759603935, 0.39999999993880425, 0.39835728750826155, 0.4000000005630007, 0.4057494854168236, 0.40246406744637775, 0.4045174555367268, 0.404928133350623, 0.4036960985014326, 0.4045174515834824, 0.4016427126018908, 0.40082135403670327, 0.40164270923612544, 0.39712525762816475, 0.4090349085154719, 0.4008213569740985, 0.4012320334172102, 0.3938398358635834, 0.3991786437235329, 0.40041067876854963, 0.40533880881460294, 0.4004106771652214, 0.4065708437861848, 0.40985626414326426, 0.40123203122640294, 0.39917864493520844, 0.4114989746155435, 0.4049281295932049, 0.4032854189250993, 0.4049281305723367, 0.4032854189250993, 0.40287474306946663, 0.4045174545575951, 0.40369609791395356, 0.41848049494275325, 0.4164271054816197, 0.40451745217096147, 0.4053388103812137, 0.41437371660796524, 0.41683778090888224, 0.40739219745571364, 0.4082135507335898, 0.4082135531202234, 0.404928129434096, 0.4073921992548682, 0.40287474310618404, 0.3926078024586123, 0.40287474545610025, 0.4090349055780767, 0.4114989716781483, 0.40287474111120314, 0.4045174523667878, 0.41355235957267106, 0.4110882976216702, 0.4135523605518028, 0.4123203294967479, 0.41889116899923134, 0.41067761980777406, 0.3889117054625948, 0.40657084303959684, 0.41765913810328537, 0.40862422893913863, 0.414373715004637, 0.40698151909105584, 0.40205339041578697, 0.40903490577390306, 0.40780287452302183, 0.4078028764812853, 0.4028747446727948, 0.41437371852951127, 0.40287474111120314, 0.3934291569114466, 0.40739219765154, 0.4020533862667162, 0.40410677831030967, 0.40451745217096147, 0.40985626261337094, 0.4078028770687644, 0.4028747444769685, 0.40205339002413426, 0.40451745240350523, 0.4156057506371328, 0.39917864708929823, 0.4102669401947233, 0.41067761800861946, 0.41396304055650623, 0.4119096504711762, 0.4036960995172818, 0.41355236274261004, 0.41437371739127066, 0.4073921986673892, 0.4016427108027362, 0.4180698153297025, 0.4156057512246118, 0.4156057488379782, 0.41067762000360036, 0.40698151787938036, 0.41396303797404627, 0.4123203287134425, 0.406160162251588, 0.4123203284808987, 0.415195070436603, 0.4209445575179505, 0.40369609752230085, 0.4131416831295593, 0.4139630377782199, 0.41765913771163266, 0.42135523607843467, 0.4176591369283273, 0.41149897265728, 0.40369609853815003, 0.4176591359491955, 0.42135523490347654, 0.41355235976849736, 0.42012320146178805, 0.4271047237105438, 0.41437371758709696, 0.4176591369283273, 0.4098562629683062, 0.4229979447882768, 0.4168377825122105, 0.42381930339018176, 0.42176591369650446, 0.4143737148088107, 0.41971252384371827, 0.42135523748593656, 0.41560574785884646, 0.42710472057732224, 0.4234086224063466, 0.4184804916137053, 0.427515400777852, 0.42422998136318685, 0.41519507380236836, 0.42012320342005155, 0.430800823093195, 0.42053387986316326, 0.41930184798808556, 0.43285421036352123, 0.4295687864815675, 0.426694044758407, 0.4197125236478919, 0.42669404354673146, 0.4279260795708799, 0.41765913555754286, 0.4205338806464687, 0.4225872669743806, 0.42587269226383623, 0.4197125261936344, 0.4279260779675517, 0.4254620110841747, 0.43080082051073504, 0.42381930041606913, 0.42094455767705946, 0.4172484618927174, 0.42422998120407795, 0.42422998179155696, 0.42381930061189543, 0.4057494858451937, 0.4324435331371041, 0.4176591377483501, 0.41889117177751767, 0.42833675480231614, 0.4266940459333651, 0.41971252501867634, 0.4246406574513633, 0.419712524235371, 0.4217659142839835, 0.41765913872748184, 0.3934291584780574, 0.4168377826713194, 0.433264887981591, 0.4053388109686928, 0.4234086222105202, 0.42874743339951765, 0.4250513370276966, 0.4340862440377535, 0.43285420781777867, 0.4344969216558233, 0.4312115003196121, 0.4250513360118474, 0.4246406570229932, 0.43490759904134935, 0.43326488778576466, 0.4225872713192777, 0.42422997885416175, 0.429158109059324, 0.4312114971496731, 0.432854208013605, 0.42464065584803506, 0.4299794648829427, 0.4258726894855499, 0.4283367561731006, 0.43285420918856315, 0.4324435303588178, 0.43326488880161385, 0.4324435315704933, 0.43162217496356925, 0.427515400777852, 0.44065708373606327, 0.4291581124250894, 0.4349075992371757, 0.43778234080122724, 0.43039014308849155, 0.42956878824400463, 0.4308008220773458, 0.44188911756940447, 0.4386036958415405, 0.43860369466658244, 0.43531827509280835, 0.4344969177025789, 0.4324435296122298, 0.43531827450532934, 0.43408624106364085, 0.43860369701649865, 0.4398357290874027, 0.43778233861042, 0.4353182747011557, 0.4398357277166183, 0.4357289546691417, 0.4320328535607708, 0.4344969214232795, 0.43860369760397766, 0.4291581124250894, 0.4250513326827995, 0.4353182756802874, 0.4349075953206487, 0.4127310055482069, 0.4242299800291198, 0.4180698169330307, 0.42094455826453847, 0.42217659111874795, 0.42135523529512925, 0.4402464055305144, 0.43367556422887643, 0.4336755640330501, 0.43696098439012954, 0.436139630328948, 0.43244353094629684, 0.43408624067198814, 0.429568788439831, 0.4225872685409914, 0.4254620144499401, 0.4254620104966957, 0.4143737165712478, 0.4271047209322575, 0.4250513354610858, 0.4246406560438614, 0.4332648858275012, 0.4320328539524235, 0.43531827352619756, 0.41683778466630034, 0.43737166118817655, 0.438193017244339, 0.43162217731348546, 0.4365503081428442, 0.4324435311788406, 0.4381930201817342, 0.4308008193357769, 0.44229979616660603, 0.42546201167165376, 0.43819301626520724, 0.436550308962867, 0.43039014207264237, 0.43203285199416, 0.43901437166045576, 0.4197125266220046, 0.4234086241687837, 0.4184804913811615, 0.41889117138586496, 0.4258726877231128, 0.41314168332538564, 0.39917864258529223, 0.4193018491997611, 0.4258726902688553, 0.4254620110841747, 0.4365503065762334, 0.43819301822347073, 0.4332648862191539]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
