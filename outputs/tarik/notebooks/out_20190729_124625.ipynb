{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf1.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 12:46:25 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '0', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['eg', 'mb', 'ce', 'sk', 'yd', 'eo', 'sg', 'ck', 'eb', 'by', 'ds', 'ek', 'ib', 'aa', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001738F20D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001738B997EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7077, Accuracy:0.0538, Validation Loss:2.7014, Validation Accuracy:0.0476\n",
    "Epoch #2: Loss:2.6978, Accuracy:0.0542, Validation Loss:2.6926, Validation Accuracy:0.0821\n",
    "Epoch #3: Loss:2.6893, Accuracy:0.0961, Validation Loss:2.6853, Validation Accuracy:0.1215\n",
    "Epoch #4: Loss:2.6820, Accuracy:0.1142, Validation Loss:2.6786, Validation Accuracy:0.1429\n",
    "Epoch #5: Loss:2.6755, Accuracy:0.1495, Validation Loss:2.6727, Validation Accuracy:0.1544\n",
    "Epoch #6: Loss:2.6694, Accuracy:0.1602, Validation Loss:2.6664, Validation Accuracy:0.1626\n",
    "Epoch #7: Loss:2.6625, Accuracy:0.1515, Validation Loss:2.6588, Validation Accuracy:0.1560\n",
    "Epoch #8: Loss:2.6554, Accuracy:0.1454, Validation Loss:2.6520, Validation Accuracy:0.1396\n",
    "Epoch #9: Loss:2.6478, Accuracy:0.1380, Validation Loss:2.6439, Validation Accuracy:0.1379\n",
    "Epoch #10: Loss:2.6391, Accuracy:0.1396, Validation Loss:2.6339, Validation Accuracy:0.1445\n",
    "Epoch #11: Loss:2.6293, Accuracy:0.1491, Validation Loss:2.6239, Validation Accuracy:0.1461\n",
    "Epoch #12: Loss:2.6188, Accuracy:0.1474, Validation Loss:2.6079, Validation Accuracy:0.1724\n",
    "Epoch #13: Loss:2.6024, Accuracy:0.1577, Validation Loss:2.5927, Validation Accuracy:0.1527\n",
    "Epoch #14: Loss:2.5934, Accuracy:0.1478, Validation Loss:2.5804, Validation Accuracy:0.1609\n",
    "Epoch #15: Loss:2.5749, Accuracy:0.1565, Validation Loss:2.5717, Validation Accuracy:0.1593\n",
    "Epoch #16: Loss:2.5636, Accuracy:0.1589, Validation Loss:2.5552, Validation Accuracy:0.1593\n",
    "Epoch #17: Loss:2.5486, Accuracy:0.1618, Validation Loss:2.5352, Validation Accuracy:0.1691\n",
    "Epoch #18: Loss:2.5351, Accuracy:0.1663, Validation Loss:2.5233, Validation Accuracy:0.1724\n",
    "Epoch #19: Loss:2.5231, Accuracy:0.1663, Validation Loss:2.5117, Validation Accuracy:0.1708\n",
    "Epoch #20: Loss:2.5148, Accuracy:0.1659, Validation Loss:2.5048, Validation Accuracy:0.1675\n",
    "Epoch #21: Loss:2.5067, Accuracy:0.1684, Validation Loss:2.4991, Validation Accuracy:0.1658\n",
    "Epoch #22: Loss:2.4949, Accuracy:0.1680, Validation Loss:2.4938, Validation Accuracy:0.1675\n",
    "Epoch #23: Loss:2.4869, Accuracy:0.1700, Validation Loss:2.4921, Validation Accuracy:0.1642\n",
    "Epoch #24: Loss:2.4829, Accuracy:0.1688, Validation Loss:2.4917, Validation Accuracy:0.1724\n",
    "Epoch #25: Loss:2.4807, Accuracy:0.1745, Validation Loss:2.4845, Validation Accuracy:0.1642\n",
    "Epoch #26: Loss:2.4796, Accuracy:0.1655, Validation Loss:2.4771, Validation Accuracy:0.1708\n",
    "Epoch #27: Loss:2.4746, Accuracy:0.1659, Validation Loss:2.4756, Validation Accuracy:0.1691\n",
    "Epoch #28: Loss:2.4733, Accuracy:0.1663, Validation Loss:2.4750, Validation Accuracy:0.1773\n",
    "Epoch #29: Loss:2.4687, Accuracy:0.1684, Validation Loss:2.4730, Validation Accuracy:0.1741\n",
    "Epoch #30: Loss:2.4663, Accuracy:0.1717, Validation Loss:2.4695, Validation Accuracy:0.1773\n",
    "Epoch #31: Loss:2.4661, Accuracy:0.1713, Validation Loss:2.4663, Validation Accuracy:0.1773\n",
    "Epoch #32: Loss:2.4638, Accuracy:0.1700, Validation Loss:2.4661, Validation Accuracy:0.1741\n",
    "Epoch #33: Loss:2.4646, Accuracy:0.1684, Validation Loss:2.4643, Validation Accuracy:0.1658\n",
    "Epoch #34: Loss:2.4650, Accuracy:0.1708, Validation Loss:2.4677, Validation Accuracy:0.1691\n",
    "Epoch #35: Loss:2.4623, Accuracy:0.1696, Validation Loss:2.4651, Validation Accuracy:0.1757\n",
    "Epoch #36: Loss:2.4586, Accuracy:0.1774, Validation Loss:2.4632, Validation Accuracy:0.1806\n",
    "Epoch #37: Loss:2.4571, Accuracy:0.1786, Validation Loss:2.4638, Validation Accuracy:0.1823\n",
    "Epoch #38: Loss:2.4567, Accuracy:0.1803, Validation Loss:2.4636, Validation Accuracy:0.1741\n",
    "Epoch #39: Loss:2.4565, Accuracy:0.1791, Validation Loss:2.4633, Validation Accuracy:0.1839\n",
    "Epoch #40: Loss:2.4545, Accuracy:0.1758, Validation Loss:2.4683, Validation Accuracy:0.1691\n",
    "Epoch #41: Loss:2.4527, Accuracy:0.1774, Validation Loss:2.4636, Validation Accuracy:0.1823\n",
    "Epoch #42: Loss:2.4525, Accuracy:0.1745, Validation Loss:2.4643, Validation Accuracy:0.1790\n",
    "Epoch #43: Loss:2.4535, Accuracy:0.1725, Validation Loss:2.4628, Validation Accuracy:0.1823\n",
    "Epoch #44: Loss:2.4523, Accuracy:0.1803, Validation Loss:2.4618, Validation Accuracy:0.1823\n",
    "Epoch #45: Loss:2.4518, Accuracy:0.1807, Validation Loss:2.4609, Validation Accuracy:0.1773\n",
    "Epoch #46: Loss:2.4520, Accuracy:0.1762, Validation Loss:2.4618, Validation Accuracy:0.1823\n",
    "Epoch #47: Loss:2.4514, Accuracy:0.1782, Validation Loss:2.4624, Validation Accuracy:0.1790\n",
    "Epoch #48: Loss:2.4524, Accuracy:0.1737, Validation Loss:2.4631, Validation Accuracy:0.1773\n",
    "Epoch #49: Loss:2.4506, Accuracy:0.1749, Validation Loss:2.4593, Validation Accuracy:0.1790\n",
    "Epoch #50: Loss:2.4507, Accuracy:0.1770, Validation Loss:2.4613, Validation Accuracy:0.1790\n",
    "Epoch #51: Loss:2.4489, Accuracy:0.1807, Validation Loss:2.4611, Validation Accuracy:0.1823\n",
    "Epoch #52: Loss:2.4509, Accuracy:0.1807, Validation Loss:2.4619, Validation Accuracy:0.1806\n",
    "Epoch #53: Loss:2.4467, Accuracy:0.1828, Validation Loss:2.4600, Validation Accuracy:0.1741\n",
    "Epoch #54: Loss:2.4466, Accuracy:0.1770, Validation Loss:2.4581, Validation Accuracy:0.1839\n",
    "Epoch #55: Loss:2.4447, Accuracy:0.1840, Validation Loss:2.4616, Validation Accuracy:0.1773\n",
    "Epoch #56: Loss:2.4432, Accuracy:0.1807, Validation Loss:2.4683, Validation Accuracy:0.1708\n",
    "Epoch #57: Loss:2.4591, Accuracy:0.1758, Validation Loss:2.4698, Validation Accuracy:0.1757\n",
    "Epoch #58: Loss:2.4555, Accuracy:0.1786, Validation Loss:2.4696, Validation Accuracy:0.1823\n",
    "Epoch #59: Loss:2.4577, Accuracy:0.1741, Validation Loss:2.4628, Validation Accuracy:0.1741\n",
    "Epoch #60: Loss:2.4442, Accuracy:0.1828, Validation Loss:2.4629, Validation Accuracy:0.1658\n",
    "Epoch #61: Loss:2.4438, Accuracy:0.1815, Validation Loss:2.4625, Validation Accuracy:0.1658\n",
    "Epoch #62: Loss:2.4451, Accuracy:0.1795, Validation Loss:2.4601, Validation Accuracy:0.1691\n",
    "Epoch #63: Loss:2.4438, Accuracy:0.1803, Validation Loss:2.4621, Validation Accuracy:0.1757\n",
    "Epoch #64: Loss:2.4433, Accuracy:0.1774, Validation Loss:2.4627, Validation Accuracy:0.1724\n",
    "Epoch #65: Loss:2.4437, Accuracy:0.1749, Validation Loss:2.4630, Validation Accuracy:0.1691\n",
    "Epoch #66: Loss:2.4426, Accuracy:0.1745, Validation Loss:2.4628, Validation Accuracy:0.1741\n",
    "Epoch #67: Loss:2.4433, Accuracy:0.1770, Validation Loss:2.4590, Validation Accuracy:0.1658\n",
    "Epoch #68: Loss:2.4417, Accuracy:0.1741, Validation Loss:2.4596, Validation Accuracy:0.1675\n",
    "Epoch #69: Loss:2.4419, Accuracy:0.1766, Validation Loss:2.4590, Validation Accuracy:0.1642\n",
    "Epoch #70: Loss:2.4422, Accuracy:0.1811, Validation Loss:2.4560, Validation Accuracy:0.1675\n",
    "Epoch #71: Loss:2.4402, Accuracy:0.1803, Validation Loss:2.4571, Validation Accuracy:0.1642\n",
    "Epoch #72: Loss:2.4396, Accuracy:0.1799, Validation Loss:2.4585, Validation Accuracy:0.1806\n",
    "Epoch #73: Loss:2.4409, Accuracy:0.1778, Validation Loss:2.4591, Validation Accuracy:0.1658\n",
    "Epoch #74: Loss:2.4413, Accuracy:0.1811, Validation Loss:2.4591, Validation Accuracy:0.1691\n",
    "Epoch #75: Loss:2.4412, Accuracy:0.1770, Validation Loss:2.4615, Validation Accuracy:0.1757\n",
    "Epoch #76: Loss:2.4412, Accuracy:0.1782, Validation Loss:2.4566, Validation Accuracy:0.1741\n",
    "Epoch #77: Loss:2.4411, Accuracy:0.1766, Validation Loss:2.4606, Validation Accuracy:0.1724\n",
    "Epoch #78: Loss:2.4412, Accuracy:0.1791, Validation Loss:2.4595, Validation Accuracy:0.1757\n",
    "Epoch #79: Loss:2.4370, Accuracy:0.1778, Validation Loss:2.4624, Validation Accuracy:0.1658\n",
    "Epoch #80: Loss:2.4381, Accuracy:0.1754, Validation Loss:2.4593, Validation Accuracy:0.1724\n",
    "Epoch #81: Loss:2.4362, Accuracy:0.1774, Validation Loss:2.4629, Validation Accuracy:0.1658\n",
    "Epoch #82: Loss:2.4374, Accuracy:0.1799, Validation Loss:2.4614, Validation Accuracy:0.1773\n",
    "Epoch #83: Loss:2.4404, Accuracy:0.1778, Validation Loss:2.4599, Validation Accuracy:0.1757\n",
    "Epoch #84: Loss:2.4450, Accuracy:0.1749, Validation Loss:2.4708, Validation Accuracy:0.1675\n",
    "Epoch #85: Loss:2.4455, Accuracy:0.1819, Validation Loss:2.4621, Validation Accuracy:0.1741\n",
    "Epoch #86: Loss:2.4404, Accuracy:0.1791, Validation Loss:2.4606, Validation Accuracy:0.1757\n",
    "Epoch #87: Loss:2.4371, Accuracy:0.1799, Validation Loss:2.4601, Validation Accuracy:0.1773\n",
    "Epoch #88: Loss:2.4347, Accuracy:0.1860, Validation Loss:2.4571, Validation Accuracy:0.1724\n",
    "Epoch #89: Loss:2.4347, Accuracy:0.1840, Validation Loss:2.4614, Validation Accuracy:0.1724\n",
    "Epoch #90: Loss:2.4365, Accuracy:0.1782, Validation Loss:2.4596, Validation Accuracy:0.1675\n",
    "Epoch #91: Loss:2.4350, Accuracy:0.1749, Validation Loss:2.4595, Validation Accuracy:0.1708\n",
    "Epoch #92: Loss:2.4349, Accuracy:0.1815, Validation Loss:2.4596, Validation Accuracy:0.1691\n",
    "Epoch #93: Loss:2.4323, Accuracy:0.1803, Validation Loss:2.4614, Validation Accuracy:0.1642\n",
    "Epoch #94: Loss:2.4328, Accuracy:0.1815, Validation Loss:2.4593, Validation Accuracy:0.1741\n",
    "Epoch #95: Loss:2.4319, Accuracy:0.1770, Validation Loss:2.4606, Validation Accuracy:0.1790\n",
    "Epoch #96: Loss:2.4327, Accuracy:0.1815, Validation Loss:2.4601, Validation Accuracy:0.1806\n",
    "Epoch #97: Loss:2.4329, Accuracy:0.1836, Validation Loss:2.4598, Validation Accuracy:0.1806\n",
    "Epoch #98: Loss:2.4327, Accuracy:0.1848, Validation Loss:2.4586, Validation Accuracy:0.1872\n",
    "Epoch #99: Loss:2.4334, Accuracy:0.1828, Validation Loss:2.4605, Validation Accuracy:0.1675\n",
    "Epoch #100: Loss:2.4307, Accuracy:0.1807, Validation Loss:2.4584, Validation Accuracy:0.1839\n",
    "Epoch #101: Loss:2.4320, Accuracy:0.1815, Validation Loss:2.4576, Validation Accuracy:0.1675\n",
    "Epoch #102: Loss:2.4365, Accuracy:0.1832, Validation Loss:2.4564, Validation Accuracy:0.1823\n",
    "Epoch #103: Loss:2.4366, Accuracy:0.1803, Validation Loss:2.4580, Validation Accuracy:0.1839\n",
    "Epoch #104: Loss:2.4260, Accuracy:0.1889, Validation Loss:2.4598, Validation Accuracy:0.1675\n",
    "Epoch #105: Loss:2.4315, Accuracy:0.1836, Validation Loss:2.4569, Validation Accuracy:0.1806\n",
    "Epoch #106: Loss:2.4300, Accuracy:0.1828, Validation Loss:2.4578, Validation Accuracy:0.1790\n",
    "Epoch #107: Loss:2.4269, Accuracy:0.1815, Validation Loss:2.4585, Validation Accuracy:0.1806\n",
    "Epoch #108: Loss:2.4272, Accuracy:0.1840, Validation Loss:2.4583, Validation Accuracy:0.1856\n",
    "Epoch #109: Loss:2.4273, Accuracy:0.1819, Validation Loss:2.4585, Validation Accuracy:0.1839\n",
    "Epoch #110: Loss:2.4260, Accuracy:0.1819, Validation Loss:2.4574, Validation Accuracy:0.1806\n",
    "Epoch #111: Loss:2.4257, Accuracy:0.1807, Validation Loss:2.4589, Validation Accuracy:0.1757\n",
    "Epoch #112: Loss:2.4261, Accuracy:0.1803, Validation Loss:2.4603, Validation Accuracy:0.1790\n",
    "Epoch #113: Loss:2.4259, Accuracy:0.1823, Validation Loss:2.4583, Validation Accuracy:0.1757\n",
    "Epoch #114: Loss:2.4245, Accuracy:0.1823, Validation Loss:2.4585, Validation Accuracy:0.1741\n",
    "Epoch #115: Loss:2.4251, Accuracy:0.1815, Validation Loss:2.4589, Validation Accuracy:0.1691\n",
    "Epoch #116: Loss:2.4275, Accuracy:0.1815, Validation Loss:2.4590, Validation Accuracy:0.1741\n",
    "Epoch #117: Loss:2.4266, Accuracy:0.1869, Validation Loss:2.4574, Validation Accuracy:0.1741\n",
    "Epoch #118: Loss:2.4259, Accuracy:0.1860, Validation Loss:2.4586, Validation Accuracy:0.1741\n",
    "Epoch #119: Loss:2.4270, Accuracy:0.1864, Validation Loss:2.4557, Validation Accuracy:0.1757\n",
    "Epoch #120: Loss:2.4265, Accuracy:0.1828, Validation Loss:2.4538, Validation Accuracy:0.1790\n",
    "Epoch #121: Loss:2.4260, Accuracy:0.1856, Validation Loss:2.4537, Validation Accuracy:0.1823\n",
    "Epoch #122: Loss:2.4246, Accuracy:0.1852, Validation Loss:2.4532, Validation Accuracy:0.1806\n",
    "Epoch #123: Loss:2.4251, Accuracy:0.1844, Validation Loss:2.4537, Validation Accuracy:0.1806\n",
    "Epoch #124: Loss:2.4263, Accuracy:0.1832, Validation Loss:2.4530, Validation Accuracy:0.1806\n",
    "Epoch #125: Loss:2.4272, Accuracy:0.1803, Validation Loss:2.4534, Validation Accuracy:0.1773\n",
    "Epoch #126: Loss:2.4262, Accuracy:0.1840, Validation Loss:2.4544, Validation Accuracy:0.1806\n",
    "Epoch #127: Loss:2.4257, Accuracy:0.1819, Validation Loss:2.4553, Validation Accuracy:0.1773\n",
    "Epoch #128: Loss:2.4252, Accuracy:0.1811, Validation Loss:2.4566, Validation Accuracy:0.1741\n",
    "Epoch #129: Loss:2.4251, Accuracy:0.1823, Validation Loss:2.4580, Validation Accuracy:0.1806\n",
    "Epoch #130: Loss:2.4250, Accuracy:0.1799, Validation Loss:2.4594, Validation Accuracy:0.1757\n",
    "Epoch #131: Loss:2.4254, Accuracy:0.1828, Validation Loss:2.4588, Validation Accuracy:0.1773\n",
    "Epoch #132: Loss:2.4244, Accuracy:0.1807, Validation Loss:2.4598, Validation Accuracy:0.1741\n",
    "Epoch #133: Loss:2.4238, Accuracy:0.1836, Validation Loss:2.4617, Validation Accuracy:0.1691\n",
    "Epoch #134: Loss:2.4252, Accuracy:0.1832, Validation Loss:2.4606, Validation Accuracy:0.1626\n",
    "Epoch #135: Loss:2.4260, Accuracy:0.1844, Validation Loss:2.4638, Validation Accuracy:0.1626\n",
    "Epoch #136: Loss:2.4268, Accuracy:0.1811, Validation Loss:2.4612, Validation Accuracy:0.1675\n",
    "Epoch #137: Loss:2.4225, Accuracy:0.1860, Validation Loss:2.4590, Validation Accuracy:0.1675\n",
    "Epoch #138: Loss:2.4214, Accuracy:0.1889, Validation Loss:2.4609, Validation Accuracy:0.1658\n",
    "Epoch #139: Loss:2.4203, Accuracy:0.1877, Validation Loss:2.4582, Validation Accuracy:0.1691\n",
    "Epoch #140: Loss:2.4212, Accuracy:0.1852, Validation Loss:2.4609, Validation Accuracy:0.1708\n",
    "Epoch #141: Loss:2.4218, Accuracy:0.1848, Validation Loss:2.4609, Validation Accuracy:0.1708\n",
    "Epoch #142: Loss:2.4217, Accuracy:0.1864, Validation Loss:2.4616, Validation Accuracy:0.1757\n",
    "Epoch #143: Loss:2.4329, Accuracy:0.1807, Validation Loss:2.4702, Validation Accuracy:0.1724\n",
    "Epoch #144: Loss:2.4454, Accuracy:0.1811, Validation Loss:2.5166, Validation Accuracy:0.1691\n",
    "Epoch #145: Loss:2.4853, Accuracy:0.1671, Validation Loss:2.5201, Validation Accuracy:0.1576\n",
    "Epoch #146: Loss:2.4918, Accuracy:0.1659, Validation Loss:2.5352, Validation Accuracy:0.1576\n",
    "Epoch #147: Loss:2.4853, Accuracy:0.1717, Validation Loss:2.4983, Validation Accuracy:0.1658\n",
    "Epoch #148: Loss:2.4598, Accuracy:0.1762, Validation Loss:2.4875, Validation Accuracy:0.1593\n",
    "Epoch #149: Loss:2.4478, Accuracy:0.1754, Validation Loss:2.4696, Validation Accuracy:0.1708\n",
    "Epoch #150: Loss:2.4482, Accuracy:0.1869, Validation Loss:2.4593, Validation Accuracy:0.1790\n",
    "Epoch #151: Loss:2.4335, Accuracy:0.1803, Validation Loss:2.4571, Validation Accuracy:0.1691\n",
    "Epoch #152: Loss:2.4338, Accuracy:0.1815, Validation Loss:2.4592, Validation Accuracy:0.1741\n",
    "Epoch #153: Loss:2.4310, Accuracy:0.1811, Validation Loss:2.4640, Validation Accuracy:0.1675\n",
    "Epoch #154: Loss:2.4303, Accuracy:0.1819, Validation Loss:2.4638, Validation Accuracy:0.1790\n",
    "Epoch #155: Loss:2.4291, Accuracy:0.1803, Validation Loss:2.4640, Validation Accuracy:0.1708\n",
    "Epoch #156: Loss:2.4282, Accuracy:0.1819, Validation Loss:2.4602, Validation Accuracy:0.1741\n",
    "Epoch #157: Loss:2.4285, Accuracy:0.1856, Validation Loss:2.4608, Validation Accuracy:0.1691\n",
    "Epoch #158: Loss:2.4279, Accuracy:0.1864, Validation Loss:2.4600, Validation Accuracy:0.1724\n",
    "Epoch #159: Loss:2.4296, Accuracy:0.1852, Validation Loss:2.4536, Validation Accuracy:0.1757\n",
    "Epoch #160: Loss:2.4303, Accuracy:0.1840, Validation Loss:2.4541, Validation Accuracy:0.1773\n",
    "Epoch #161: Loss:2.4310, Accuracy:0.1836, Validation Loss:2.4530, Validation Accuracy:0.1626\n",
    "Epoch #162: Loss:2.4340, Accuracy:0.1852, Validation Loss:2.4548, Validation Accuracy:0.1757\n",
    "Epoch #163: Loss:2.4344, Accuracy:0.1807, Validation Loss:2.4652, Validation Accuracy:0.1642\n",
    "Epoch #164: Loss:2.4616, Accuracy:0.1733, Validation Loss:2.4728, Validation Accuracy:0.1675\n",
    "Epoch #165: Loss:2.4426, Accuracy:0.1762, Validation Loss:2.4605, Validation Accuracy:0.1576\n",
    "Epoch #166: Loss:2.4365, Accuracy:0.1844, Validation Loss:2.4578, Validation Accuracy:0.1593\n",
    "Epoch #167: Loss:2.4298, Accuracy:0.1852, Validation Loss:2.4593, Validation Accuracy:0.1724\n",
    "Epoch #168: Loss:2.4271, Accuracy:0.1803, Validation Loss:2.4567, Validation Accuracy:0.1675\n",
    "Epoch #169: Loss:2.4265, Accuracy:0.1836, Validation Loss:2.4561, Validation Accuracy:0.1708\n",
    "Epoch #170: Loss:2.4256, Accuracy:0.1840, Validation Loss:2.4563, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4252, Accuracy:0.1836, Validation Loss:2.4559, Validation Accuracy:0.1708\n",
    "Epoch #172: Loss:2.4249, Accuracy:0.1823, Validation Loss:2.4582, Validation Accuracy:0.1675\n",
    "Epoch #173: Loss:2.4263, Accuracy:0.1828, Validation Loss:2.4558, Validation Accuracy:0.1773\n",
    "Epoch #174: Loss:2.4239, Accuracy:0.1844, Validation Loss:2.4589, Validation Accuracy:0.1773\n",
    "Epoch #175: Loss:2.4228, Accuracy:0.1795, Validation Loss:2.4564, Validation Accuracy:0.1708\n",
    "Epoch #176: Loss:2.4237, Accuracy:0.1828, Validation Loss:2.4583, Validation Accuracy:0.1691\n",
    "Epoch #177: Loss:2.4288, Accuracy:0.1807, Validation Loss:2.4613, Validation Accuracy:0.1757\n",
    "Epoch #178: Loss:2.4384, Accuracy:0.1770, Validation Loss:2.4718, Validation Accuracy:0.1626\n",
    "Epoch #179: Loss:2.4755, Accuracy:0.1741, Validation Loss:2.4844, Validation Accuracy:0.1544\n",
    "Epoch #180: Loss:2.4512, Accuracy:0.1745, Validation Loss:2.4603, Validation Accuracy:0.1675\n",
    "Epoch #181: Loss:2.4320, Accuracy:0.1844, Validation Loss:2.4610, Validation Accuracy:0.1593\n",
    "Epoch #182: Loss:2.4383, Accuracy:0.1819, Validation Loss:2.4577, Validation Accuracy:0.1593\n",
    "Epoch #183: Loss:2.4304, Accuracy:0.1823, Validation Loss:2.4575, Validation Accuracy:0.1658\n",
    "Epoch #184: Loss:2.4296, Accuracy:0.1844, Validation Loss:2.4550, Validation Accuracy:0.1773\n",
    "Epoch #185: Loss:2.4295, Accuracy:0.1881, Validation Loss:2.4535, Validation Accuracy:0.1609\n",
    "Epoch #186: Loss:2.4255, Accuracy:0.1864, Validation Loss:2.4567, Validation Accuracy:0.1724\n",
    "Epoch #187: Loss:2.4268, Accuracy:0.1852, Validation Loss:2.4548, Validation Accuracy:0.1609\n",
    "Epoch #188: Loss:2.4290, Accuracy:0.1823, Validation Loss:2.4539, Validation Accuracy:0.1741\n",
    "Epoch #189: Loss:2.4269, Accuracy:0.1836, Validation Loss:2.4542, Validation Accuracy:0.1675\n",
    "Epoch #190: Loss:2.4279, Accuracy:0.1856, Validation Loss:2.4551, Validation Accuracy:0.1724\n",
    "Epoch #191: Loss:2.4241, Accuracy:0.1877, Validation Loss:2.4570, Validation Accuracy:0.1609\n",
    "Epoch #192: Loss:2.4243, Accuracy:0.1852, Validation Loss:2.4581, Validation Accuracy:0.1757\n",
    "Epoch #193: Loss:2.4241, Accuracy:0.1836, Validation Loss:2.4543, Validation Accuracy:0.1642\n",
    "Epoch #194: Loss:2.4230, Accuracy:0.1836, Validation Loss:2.4515, Validation Accuracy:0.1773\n",
    "Epoch #195: Loss:2.4248, Accuracy:0.1823, Validation Loss:2.4507, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4256, Accuracy:0.1844, Validation Loss:2.4540, Validation Accuracy:0.1773\n",
    "Epoch #197: Loss:2.4242, Accuracy:0.1836, Validation Loss:2.4531, Validation Accuracy:0.1741\n",
    "Epoch #198: Loss:2.4263, Accuracy:0.1877, Validation Loss:2.4524, Validation Accuracy:0.1708\n",
    "Epoch #199: Loss:2.4252, Accuracy:0.1848, Validation Loss:2.4530, Validation Accuracy:0.1757\n",
    "Epoch #200: Loss:2.4271, Accuracy:0.1885, Validation Loss:2.4532, Validation Accuracy:0.1741\n",
    "Epoch #201: Loss:2.4242, Accuracy:0.1877, Validation Loss:2.4537, Validation Accuracy:0.1790\n",
    "Epoch #202: Loss:2.4230, Accuracy:0.1832, Validation Loss:2.4549, Validation Accuracy:0.1691\n",
    "Epoch #203: Loss:2.4223, Accuracy:0.1815, Validation Loss:2.4567, Validation Accuracy:0.1724\n",
    "Epoch #204: Loss:2.4215, Accuracy:0.1815, Validation Loss:2.4572, Validation Accuracy:0.1609\n",
    "Epoch #205: Loss:2.4233, Accuracy:0.1852, Validation Loss:2.4568, Validation Accuracy:0.1593\n",
    "Epoch #206: Loss:2.4239, Accuracy:0.1840, Validation Loss:2.4587, Validation Accuracy:0.1576\n",
    "Epoch #207: Loss:2.4234, Accuracy:0.1823, Validation Loss:2.4591, Validation Accuracy:0.1609\n",
    "Epoch #208: Loss:2.4238, Accuracy:0.1828, Validation Loss:2.4599, Validation Accuracy:0.1675\n",
    "Epoch #209: Loss:2.4237, Accuracy:0.1819, Validation Loss:2.4598, Validation Accuracy:0.1675\n",
    "Epoch #210: Loss:2.4245, Accuracy:0.1848, Validation Loss:2.4594, Validation Accuracy:0.1642\n",
    "Epoch #211: Loss:2.4216, Accuracy:0.1877, Validation Loss:2.4587, Validation Accuracy:0.1741\n",
    "Epoch #212: Loss:2.4231, Accuracy:0.1836, Validation Loss:2.4570, Validation Accuracy:0.1658\n",
    "Epoch #213: Loss:2.4233, Accuracy:0.1877, Validation Loss:2.4579, Validation Accuracy:0.1691\n",
    "Epoch #214: Loss:2.4233, Accuracy:0.1852, Validation Loss:2.4591, Validation Accuracy:0.1724\n",
    "Epoch #215: Loss:2.4232, Accuracy:0.1836, Validation Loss:2.4535, Validation Accuracy:0.1675\n",
    "Epoch #216: Loss:2.4233, Accuracy:0.1832, Validation Loss:2.4550, Validation Accuracy:0.1741\n",
    "Epoch #217: Loss:2.4225, Accuracy:0.1877, Validation Loss:2.4534, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.4233, Accuracy:0.1848, Validation Loss:2.4557, Validation Accuracy:0.1757\n",
    "Epoch #219: Loss:2.4217, Accuracy:0.1856, Validation Loss:2.4563, Validation Accuracy:0.1675\n",
    "Epoch #220: Loss:2.4222, Accuracy:0.1844, Validation Loss:2.4576, Validation Accuracy:0.1708\n",
    "Epoch #221: Loss:2.4207, Accuracy:0.1848, Validation Loss:2.4589, Validation Accuracy:0.1691\n",
    "Epoch #222: Loss:2.4204, Accuracy:0.1832, Validation Loss:2.4587, Validation Accuracy:0.1675\n",
    "Epoch #223: Loss:2.4205, Accuracy:0.1840, Validation Loss:2.4581, Validation Accuracy:0.1675\n",
    "Epoch #224: Loss:2.4200, Accuracy:0.1848, Validation Loss:2.4597, Validation Accuracy:0.1675\n",
    "Epoch #225: Loss:2.4193, Accuracy:0.1819, Validation Loss:2.4574, Validation Accuracy:0.1724\n",
    "Epoch #226: Loss:2.4201, Accuracy:0.1856, Validation Loss:2.4548, Validation Accuracy:0.1708\n",
    "Epoch #227: Loss:2.4206, Accuracy:0.1807, Validation Loss:2.4645, Validation Accuracy:0.1708\n",
    "Epoch #228: Loss:2.4265, Accuracy:0.1823, Validation Loss:2.4653, Validation Accuracy:0.1724\n",
    "Epoch #229: Loss:2.4279, Accuracy:0.1836, Validation Loss:2.4617, Validation Accuracy:0.1708\n",
    "Epoch #230: Loss:2.4265, Accuracy:0.1856, Validation Loss:2.4559, Validation Accuracy:0.1593\n",
    "Epoch #231: Loss:2.4281, Accuracy:0.1811, Validation Loss:2.4579, Validation Accuracy:0.1560\n",
    "Epoch #232: Loss:2.4247, Accuracy:0.1906, Validation Loss:2.4590, Validation Accuracy:0.1691\n",
    "Epoch #233: Loss:2.4256, Accuracy:0.1869, Validation Loss:2.4565, Validation Accuracy:0.1560\n",
    "Epoch #234: Loss:2.4246, Accuracy:0.1852, Validation Loss:2.4546, Validation Accuracy:0.1806\n",
    "Epoch #235: Loss:2.4220, Accuracy:0.1799, Validation Loss:2.4513, Validation Accuracy:0.1576\n",
    "Epoch #236: Loss:2.4226, Accuracy:0.1864, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #237: Loss:2.4200, Accuracy:0.1832, Validation Loss:2.4551, Validation Accuracy:0.1675\n",
    "Epoch #238: Loss:2.4185, Accuracy:0.1873, Validation Loss:2.4546, Validation Accuracy:0.1724\n",
    "Epoch #239: Loss:2.4183, Accuracy:0.1885, Validation Loss:2.4556, Validation Accuracy:0.1724\n",
    "Epoch #240: Loss:2.4175, Accuracy:0.1934, Validation Loss:2.4592, Validation Accuracy:0.1626\n",
    "Epoch #241: Loss:2.4201, Accuracy:0.1873, Validation Loss:2.4611, Validation Accuracy:0.1741\n",
    "Epoch #242: Loss:2.4178, Accuracy:0.1906, Validation Loss:2.4609, Validation Accuracy:0.1626\n",
    "Epoch #243: Loss:2.4184, Accuracy:0.1860, Validation Loss:2.4608, Validation Accuracy:0.1741\n",
    "Epoch #244: Loss:2.4206, Accuracy:0.1873, Validation Loss:2.4608, Validation Accuracy:0.1593\n",
    "Epoch #245: Loss:2.4198, Accuracy:0.1893, Validation Loss:2.4609, Validation Accuracy:0.1658\n",
    "Epoch #246: Loss:2.4199, Accuracy:0.1869, Validation Loss:2.4574, Validation Accuracy:0.1544\n",
    "Epoch #247: Loss:2.4177, Accuracy:0.1934, Validation Loss:2.4581, Validation Accuracy:0.1741\n",
    "Epoch #248: Loss:2.4165, Accuracy:0.1906, Validation Loss:2.4588, Validation Accuracy:0.1593\n",
    "Epoch #249: Loss:2.4167, Accuracy:0.1922, Validation Loss:2.4581, Validation Accuracy:0.1658\n",
    "Epoch #250: Loss:2.4180, Accuracy:0.1938, Validation Loss:2.4547, Validation Accuracy:0.1626\n",
    "Epoch #251: Loss:2.4166, Accuracy:0.1897, Validation Loss:2.4576, Validation Accuracy:0.1593\n",
    "Epoch #252: Loss:2.4161, Accuracy:0.1901, Validation Loss:2.4597, Validation Accuracy:0.1658\n",
    "Epoch #253: Loss:2.4144, Accuracy:0.1943, Validation Loss:2.4619, Validation Accuracy:0.1675\n",
    "Epoch #254: Loss:2.4152, Accuracy:0.1918, Validation Loss:2.4613, Validation Accuracy:0.1609\n",
    "Epoch #255: Loss:2.4162, Accuracy:0.1889, Validation Loss:2.4626, Validation Accuracy:0.1527\n",
    "Epoch #256: Loss:2.4150, Accuracy:0.1930, Validation Loss:2.4605, Validation Accuracy:0.1658\n",
    "Epoch #257: Loss:2.4161, Accuracy:0.1951, Validation Loss:2.4590, Validation Accuracy:0.1626\n",
    "Epoch #258: Loss:2.4156, Accuracy:0.1930, Validation Loss:2.4600, Validation Accuracy:0.1576\n",
    "Epoch #259: Loss:2.4160, Accuracy:0.1963, Validation Loss:2.4569, Validation Accuracy:0.1593\n",
    "Epoch #260: Loss:2.4172, Accuracy:0.1914, Validation Loss:2.4587, Validation Accuracy:0.1609\n",
    "Epoch #261: Loss:2.4155, Accuracy:0.1963, Validation Loss:2.4625, Validation Accuracy:0.1560\n",
    "Epoch #262: Loss:2.4174, Accuracy:0.1943, Validation Loss:2.4643, Validation Accuracy:0.1642\n",
    "Epoch #263: Loss:2.4163, Accuracy:0.1959, Validation Loss:2.4626, Validation Accuracy:0.1593\n",
    "Epoch #264: Loss:2.4160, Accuracy:0.2000, Validation Loss:2.4633, Validation Accuracy:0.1560\n",
    "Epoch #265: Loss:2.4150, Accuracy:0.1984, Validation Loss:2.4678, Validation Accuracy:0.1544\n",
    "Epoch #266: Loss:2.4149, Accuracy:0.1979, Validation Loss:2.4667, Validation Accuracy:0.1576\n",
    "Epoch #267: Loss:2.4155, Accuracy:0.2016, Validation Loss:2.4652, Validation Accuracy:0.1527\n",
    "Epoch #268: Loss:2.4149, Accuracy:0.1975, Validation Loss:2.4631, Validation Accuracy:0.1494\n",
    "Epoch #269: Loss:2.4153, Accuracy:0.1967, Validation Loss:2.4704, Validation Accuracy:0.1527\n",
    "Epoch #270: Loss:2.4131, Accuracy:0.1984, Validation Loss:2.4698, Validation Accuracy:0.1560\n",
    "Epoch #271: Loss:2.4153, Accuracy:0.1959, Validation Loss:2.4662, Validation Accuracy:0.1593\n",
    "Epoch #272: Loss:2.4143, Accuracy:0.1938, Validation Loss:2.4666, Validation Accuracy:0.1511\n",
    "Epoch #273: Loss:2.4119, Accuracy:0.2008, Validation Loss:2.4666, Validation Accuracy:0.1511\n",
    "Epoch #274: Loss:2.4126, Accuracy:0.2021, Validation Loss:2.4654, Validation Accuracy:0.1511\n",
    "Epoch #275: Loss:2.4123, Accuracy:0.1996, Validation Loss:2.4643, Validation Accuracy:0.1511\n",
    "Epoch #276: Loss:2.4128, Accuracy:0.2025, Validation Loss:2.4660, Validation Accuracy:0.1478\n",
    "Epoch #277: Loss:2.4123, Accuracy:0.2008, Validation Loss:2.4671, Validation Accuracy:0.1461\n",
    "Epoch #278: Loss:2.4128, Accuracy:0.2004, Validation Loss:2.4719, Validation Accuracy:0.1494\n",
    "Epoch #279: Loss:2.4113, Accuracy:0.1967, Validation Loss:2.4697, Validation Accuracy:0.1494\n",
    "Epoch #280: Loss:2.4170, Accuracy:0.1943, Validation Loss:2.4726, Validation Accuracy:0.1478\n",
    "Epoch #281: Loss:2.4260, Accuracy:0.2004, Validation Loss:2.4783, Validation Accuracy:0.1511\n",
    "Epoch #282: Loss:2.4250, Accuracy:0.1852, Validation Loss:2.4672, Validation Accuracy:0.1445\n",
    "Epoch #283: Loss:2.4215, Accuracy:0.1914, Validation Loss:2.4650, Validation Accuracy:0.1560\n",
    "Epoch #284: Loss:2.4212, Accuracy:0.1897, Validation Loss:2.4731, Validation Accuracy:0.1527\n",
    "Epoch #285: Loss:2.4190, Accuracy:0.1922, Validation Loss:2.4694, Validation Accuracy:0.1511\n",
    "Epoch #286: Loss:2.4168, Accuracy:0.1910, Validation Loss:2.4651, Validation Accuracy:0.1560\n",
    "Epoch #287: Loss:2.4220, Accuracy:0.1910, Validation Loss:2.4657, Validation Accuracy:0.1642\n",
    "Epoch #288: Loss:2.4209, Accuracy:0.1922, Validation Loss:2.4619, Validation Accuracy:0.1642\n",
    "Epoch #289: Loss:2.4191, Accuracy:0.1914, Validation Loss:2.4571, Validation Accuracy:0.1642\n",
    "Epoch #290: Loss:2.4240, Accuracy:0.1930, Validation Loss:2.4592, Validation Accuracy:0.1494\n",
    "Epoch #291: Loss:2.4211, Accuracy:0.1992, Validation Loss:2.4606, Validation Accuracy:0.1642\n",
    "Epoch #292: Loss:2.4199, Accuracy:0.1955, Validation Loss:2.4630, Validation Accuracy:0.1576\n",
    "Epoch #293: Loss:2.4185, Accuracy:0.1967, Validation Loss:2.4642, Validation Accuracy:0.1576\n",
    "Epoch #294: Loss:2.4162, Accuracy:0.1951, Validation Loss:2.4620, Validation Accuracy:0.1494\n",
    "Epoch #295: Loss:2.4188, Accuracy:0.1951, Validation Loss:2.4607, Validation Accuracy:0.1544\n",
    "Epoch #296: Loss:2.4203, Accuracy:0.1955, Validation Loss:2.4596, Validation Accuracy:0.1527\n",
    "Epoch #297: Loss:2.4190, Accuracy:0.1955, Validation Loss:2.4547, Validation Accuracy:0.1544\n",
    "Epoch #298: Loss:2.4219, Accuracy:0.1943, Validation Loss:2.4560, Validation Accuracy:0.1511\n",
    "Epoch #299: Loss:2.4212, Accuracy:0.1918, Validation Loss:2.4578, Validation Accuracy:0.1544\n",
    "Epoch #300: Loss:2.4210, Accuracy:0.1906, Validation Loss:2.4558, Validation Accuracy:0.1511\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45578694, Accuracy:0.1511\n",
    "Labels: ['eg', 'mb', 'ce', 'sk', 'yd', 'eo', 'sg', 'ck', 'eb', 'by', 'ds', 'ek', 'ib', 'aa', 'my']\n",
    "Confusion Matrix:\n",
    "      eg  mb  ce  sk  yd  eo  sg  ck  eb  by  ds  ek  ib  aa  my\n",
    "t:eg  24   0   0   0   0   0   2   0  13   0   8   3   0   0   0\n",
    "t:mb   5   0   0   0  11   0  15   0   7   0   1   7   6   0   0\n",
    "t:ce   6   0   0   0   4   0  10   0   3   1   2   0   1   0   0\n",
    "t:sk  11   0   0   0   3   0   5   0   4   0   3   7   0   0   0\n",
    "t:yd   2   0   0   0  28   0  19   0   1   0   1   6   5   0   0\n",
    "t:eo   2   0   0   0   2   0  13   0   6   1   0   9   1   0   0\n",
    "t:sg   2   0   0   0   9   0  22   0   2   1   1   7   7   0   0\n",
    "t:ck   9   0   0   0   0   0   3   0   4   1   3   2   1   0   0\n",
    "t:eb  17   0   0   0   6   0   8   0   5   2   2   8   2   0   0\n",
    "t:by   8   0   0   0   1   0  11   0   7   0   0  12   1   0   0\n",
    "t:ds  10   0   0   0   0   0   6   0   3   0   5   6   1   0   0\n",
    "t:ek  18   0   0   0   7   0  11   0   7   0   0   4   1   0   0\n",
    "t:ib   2   0   0   0  28   0  12   0   0   0   0   8   4   0   0\n",
    "t:aa  14   0   0   0   2   0   5   0   3   0   7   2   1   0   0\n",
    "t:my   4   0   0   0   4   0   3   0   2   0   2   3   2   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          eg       0.18      0.48      0.26        50\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          yd       0.27      0.45      0.34        62\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          sg       0.15      0.43      0.22        51\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eb       0.07      0.10      0.09        50\n",
    "          by       0.00      0.00      0.00        40\n",
    "          ds       0.14      0.16      0.15        31\n",
    "          ek       0.05      0.08      0.06        48\n",
    "          ib       0.12      0.07      0.09        54\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.15       609\n",
    "   macro avg       0.07      0.12      0.08       609\n",
    "weighted avg       0.08      0.15      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 13:02:30 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 16 minutes, 4 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7013736765568677, 2.692627503954131, 2.6853131551069187, 2.678574974705237, 2.672703626316365, 2.6664315319217877, 2.6588372197644463, 2.6519659458122815, 2.6439169764714485, 2.6339054890649853, 2.623850821274255, 2.6078612882711227, 2.5927100099366287, 2.580424735503048, 2.571691070675654, 2.5551653491648154, 2.535207730012966, 2.523311943060463, 2.5116524477114623, 2.5048129390221705, 2.4991334142356085, 2.493793776469865, 2.4920618142596216, 2.491684309563222, 2.484504067056089, 2.4771245128806982, 2.4756010205091905, 2.4750481454413906, 2.472986551341165, 2.4694978216011534, 2.466345280066304, 2.466067572532616, 2.4642587620245022, 2.4677380693369897, 2.4651008572288724, 2.4632462147617185, 2.463768868219285, 2.4636381725568097, 2.4633360003015676, 2.4682937302612906, 2.4636406988541677, 2.4642619241047377, 2.46278763052278, 2.4617637296028323, 2.460861867676032, 2.461803212345919, 2.462358793405868, 2.4631233348439285, 2.4593019814326844, 2.461275148470022, 2.461147324009286, 2.461869126665964, 2.460011054729593, 2.458108191811197, 2.4615891578749483, 2.4682576828597997, 2.469774750457413, 2.4696041034360237, 2.4627898170051514, 2.462862852954708, 2.4624745665708394, 2.4601061947044287, 2.4620566473805847, 2.4626784144559712, 2.462977897161725, 2.4628039847062335, 2.4590290140831605, 2.45956088991588, 2.4590055731129765, 2.4560042077488893, 2.4570648901176764, 2.4584769587994404, 2.459110992882639, 2.459130074590298, 2.461498996111364, 2.4566130352333455, 2.4606394008266905, 2.459538868103904, 2.4623776053755937, 2.4592818023731753, 2.4628722937823517, 2.4613984265350943, 2.459936851351132, 2.4707923060763255, 2.4621182490256426, 2.46059546133959, 2.460141204652332, 2.4571142126186727, 2.4613537181578637, 2.459647492626422, 2.4594832605916293, 2.459576447021785, 2.4613958653949557, 2.459278098468123, 2.4605500917325074, 2.460134495655304, 2.459805207886719, 2.4585786914982037, 2.4605125488318835, 2.4584339733781486, 2.4575735911947167, 2.456423768856255, 2.457976808688911, 2.459808952898423, 2.456857698108567, 2.4577748027732613, 2.4585314153254716, 2.4582936744188832, 2.4585035426667563, 2.4574282200661393, 2.458878823689052, 2.4602786852612675, 2.4583107567773075, 2.4584673919113986, 2.458914855039374, 2.4590410602895303, 2.457413250589606, 2.4586338671948913, 2.4557437407363616, 2.4538458442844586, 2.4537331251479526, 2.4532250152237114, 2.453659562641764, 2.452950919985967, 2.4534018795282773, 2.454405850769068, 2.455251825658363, 2.456558108525519, 2.457991747629075, 2.459362770340517, 2.458769362939794, 2.4597950302712825, 2.461744673733641, 2.460643416750803, 2.463758126855484, 2.461194380163559, 2.4590101758834764, 2.4609287188362408, 2.4582445488383224, 2.460867557228101, 2.46093322764868, 2.4616418495554053, 2.470236014850034, 2.516582536775686, 2.520099649679876, 2.535185514803982, 2.4983334658768377, 2.4874827305867364, 2.4695680043575994, 2.45929388029039, 2.4570746269132115, 2.459169933557119, 2.463961373017535, 2.4637946373919157, 2.4639874718263624, 2.460243567653086, 2.4608300260722342, 2.46001268959985, 2.4536019769208184, 2.454093752627694, 2.4530289243594767, 2.454794338771275, 2.465202623987433, 2.472816917109372, 2.460543964492472, 2.457755119342522, 2.4592514249491573, 2.4567435805629234, 2.45611324686135, 2.456319380667801, 2.4559360912867954, 2.4582390060957233, 2.4558311604905403, 2.4589287154192996, 2.4563682079315186, 2.4582933064164787, 2.461251223224333, 2.471807965699871, 2.484439102103949, 2.460253373351199, 2.460980108806065, 2.4576541880277185, 2.4575334554431083, 2.454979516798248, 2.453480260908506, 2.456700408399986, 2.454750286925994, 2.4538519496009465, 2.4541688118074916, 2.455061544142725, 2.457033610304784, 2.4580555326245688, 2.4542925248200866, 2.4515371142545552, 2.4506871708116704, 2.454017542461652, 2.453050596177676, 2.452419155336953, 2.45304387543589, 2.453209319921158, 2.453715701017082, 2.4548606625918685, 2.4567123316778927, 2.4572459617859037, 2.456811234281568, 2.458689614469782, 2.4591401891755353, 2.459948015134714, 2.4598488009034707, 2.4594442264982828, 2.4587165655565184, 2.4569581204838746, 2.4578613565473133, 2.4591013375174238, 2.453511415835476, 2.4550121824925366, 2.4534081109051633, 2.455677233306058, 2.456289793665969, 2.4576152850841653, 2.4588549203669103, 2.4587038397201764, 2.4581380180145915, 2.4596915577824285, 2.457431512513184, 2.454752230683375, 2.464471459192987, 2.4652854550648207, 2.4616739612886276, 2.4559242686223124, 2.457925177951556, 2.4590383186716163, 2.456543579477395, 2.454635358209093, 2.451301345684258, 2.4552725884323245, 2.4551194211336584, 2.4546368392230256, 2.455648222580332, 2.45917789337083, 2.461089497129318, 2.460867126978481, 2.460761389708871, 2.460808930138649, 2.46089692342849, 2.4574155854474147, 2.4580821251047067, 2.4587569804418656, 2.458063796627502, 2.4547147050083957, 2.457565433286094, 2.4596734195702963, 2.461897212687776, 2.461319118102, 2.4625793235446825, 2.4604565157678913, 2.4590404601324174, 2.4600317881416607, 2.456921739531268, 2.458685110355246, 2.4625102518423048, 2.4643063517822617, 2.4626334308599214, 2.463342415679656, 2.4678213106979094, 2.466697701483916, 2.4652349897989105, 2.4630854630900916, 2.470405526153364, 2.469764753319751, 2.466201696489832, 2.466557907549227, 2.4665666573936327, 2.4654089832932296, 2.4642603413029063, 2.4660202208019437, 2.4670966513246935, 2.4718705323725105, 2.469730033858852, 2.472618955305253, 2.478261434777421, 2.46718041415285, 2.464951255247119, 2.4730697194930955, 2.46935469646172, 2.4650924699059846, 2.4656822673597163, 2.461854679439651, 2.457130929323644, 2.459186130752313, 2.4605691699167385, 2.463023826993745, 2.464189785063169, 2.462019300617412, 2.4606597728917166, 2.459645733261735, 2.4546898816802427, 2.45604635067957, 2.4578239522348286, 2.4557865499862896], 'val_acc': [0.0476190474171846, 0.08210180603787426, 0.12151067322257704, 0.14285714185394482, 0.15435139462963504, 0.16256157634244567, 0.15599343075442978, 0.13957307059531923, 0.1379310344705245, 0.14449917788086658, 0.14614121400566132, 0.17241379219812322, 0.15270935860271329, 0.160919539226687, 0.1592775030040193, 0.15927750310189226, 0.16912971985066075, 0.17241379229599618, 0.17077175617120144, 0.16748768401948494, 0.1658456477968172, 0.16748768401948494, 0.16420361157414948, 0.17241379258961514, 0.16420361157414948, 0.1707717563669474, 0.16912972024215267, 0.1773399008661264, 0.1740558286165369, 0.1773399007682534, 0.1773399007682534, 0.17405582832291797, 0.16584564760107126, 0.16912971985066075, 0.17569786425196673, 0.180623972626351, 0.18226600914263763, 0.17405582842079093, 0.18390804546317835, 0.16912972014427968, 0.1822660094362566, 0.17898193699092113, 0.18226600924051062, 0.18226600914263763, 0.17733990067038044, 0.18226600924051062, 0.17898193689304814, 0.17733990057250745, 0.17898193699092113, 0.17898193679517518, 0.1822660093383836, 0.18062397321358886, 0.1740558285186639, 0.18390804546317835, 0.1773399008661264, 0.17077175646482037, 0.17569786474133164, 0.1822660093383836, 0.1740558285186639, 0.16584564769894422, 0.1658456477968172, 0.16912972014427968, 0.1756978644477127, 0.17241379210025023, 0.16912971975278776, 0.17405582832291797, 0.1658456477968172, 0.16748768401948494, 0.16420361147627652, 0.16748768411735793, 0.16420361167202246, 0.1806239730178429, 0.16584564769894422, 0.16912971975278776, 0.17569786425196673, 0.174055828127172, 0.17241379210025023, 0.1756978644477127, 0.16584564760107126, 0.17241379219812322, 0.1658456477968172, 0.17733990067038044, 0.17569786454558567, 0.16748768382373896, 0.17405582832291797, 0.17569786464345866, 0.17733990057250745, 0.17241379219812322, 0.17241379219812322, 0.16748768382373896, 0.17077175607332848, 0.1691297199485337, 0.16420361157414948, 0.17405582842079093, 0.17898193689304814, 0.18062397311571587, 0.18062397311571587, 0.18719211761489485, 0.16748768392161195, 0.1839080452674324, 0.16748768392161195, 0.18226600904476467, 0.1839080452674324, 0.16748768382373896, 0.1806239730178429, 0.17898193679517518, 0.18062397291996993, 0.18555008129435416, 0.18390804507168643, 0.18062397282209694, 0.1756978644477127, 0.1789819366973022, 0.1756978644477127, 0.17405582832291797, 0.16912971985066075, 0.17405582842079093, 0.17405582822504498, 0.17405582842079093, 0.17569786454558567, 0.1789819366973022, 0.18226600904476467, 0.1806239730178429, 0.1806239730178429, 0.18062397311571587, 0.17733990067038044, 0.18062397291996993, 0.17733990057250745, 0.174055828127172, 0.18062397291996993, 0.17569786434983972, 0.17733990057250745, 0.17405582822504498, 0.16912971985066075, 0.1625615762445727, 0.16256157634244567, 0.16748768362799302, 0.16748768362799302, 0.16584564859203518, 0.16912971975278776, 0.1707717559754555, 0.1707717558775825, 0.17569786425196673, 0.17241379309121416, 0.1691297200464067, 0.15763546697709752, 0.15763546746646243, 0.1658456477968172, 0.15927750349338418, 0.17077175617120144, 0.17898193699092113, 0.1691297199485337, 0.1740558287144099, 0.16748768401948494, 0.17898193699092113, 0.17077175617120144, 0.17405582842079093, 0.16912971985066075, 0.17241379210025023, 0.17569786454558567, 0.1773399008661264, 0.16256157554722772, 0.17569786474133164, 0.16420361176989545, 0.16748768382373896, 0.15763546697709752, 0.15927750319976525, 0.17241379219812322, 0.16748768382373896, 0.17077175607332848, 0.17077175607332848, 0.17077175607332848, 0.167487683725866, 0.17733990067038044, 0.1773399008661264, 0.17077175626907443, 0.16912972014427968, 0.17569786483920463, 0.16256157544935473, 0.15435139472750803, 0.16748768382373896, 0.15927750310189226, 0.15927750310189226, 0.1658456477968172, 0.1773399007682534, 0.16091953961817893, 0.17241379229599618, 0.16091953961817893, 0.17405582842079093, 0.16748768392161195, 0.17241379229599618, 0.16091953961817893, 0.17569786464345866, 0.16420361157414948, 0.1773399007682534, 0.17241379239386917, 0.17733990067038044, 0.17405582842079093, 0.17077175617120144, 0.17569786454558567, 0.17405582842079093, 0.17898193679517518, 0.1691297200464067, 0.17241379239386917, 0.16091953932456, 0.15927750319976525, 0.1576354670749705, 0.16091953932456, 0.16748768392161195, 0.16748768392161195, 0.16420361167202246, 0.17405582832291797, 0.16584564769894422, 0.1691297199485337, 0.17241379229599618, 0.16748768392161195, 0.1740558285186639, 0.1691297199485337, 0.1756978644477127, 0.16748768392161195, 0.17077175626907443, 0.1691297200464067, 0.16748768392161195, 0.16748768392161195, 0.16748768392161195, 0.17241379229599618, 0.17077175617120144, 0.1707717563669474, 0.17241379249174216, 0.17077175626907443, 0.15927750329763823, 0.15599343085230277, 0.1691297199485337, 0.15599343104804875, 0.1806239730178429, 0.1576354671728435, 0.1740558285186639, 0.16748768392161195, 0.17241379239386917, 0.17241379239386917, 0.16256157535148175, 0.17405582842079093, 0.16256157544935473, 0.1740558285186639, 0.15927750319976525, 0.1658456477968172, 0.154351394923254, 0.1740558285186639, 0.15927750319976525, 0.1658456478946902, 0.16256157544935473, 0.15927750319976525, 0.1658456477968172, 0.16748768382373896, 0.16091953942243298, 0.15270935860271329, 0.1658456478946902, 0.1625615756451007, 0.1576354671728435, 0.15927750329763823, 0.16091953961817893, 0.15599343104804875, 0.16420361167202246, 0.15927750319976525, 0.15599343095017576, 0.15435139482538102, 0.1576354670749705, 0.1527093595936772, 0.1494252863531238, 0.1527093585048403, 0.15599343104804875, 0.15927750329763823, 0.15106732257579153, 0.15106732257579153, 0.15106732257579153, 0.15106732257579153, 0.14778325022832905, 0.14614121400566132, 0.1494252863531238, 0.1494252863531238, 0.14778325032620204, 0.15106732257579153, 0.14449917788086658, 0.15599343104804875, 0.15270935870058627, 0.15106732257579153, 0.15599343075442978, 0.1642036118677684, 0.16420361167202246, 0.16420361167202246, 0.1494252863531238, 0.16420361167202246, 0.1576354671728435, 0.1576354671728435, 0.14942528645099679, 0.15435139472750803, 0.1527093585048403, 0.15435139472750803, 0.15106732346888246, 0.15435139462963504, 0.15106732238004555], 'loss': [2.7077202099794233, 2.6978130444364137, 2.6892602307595754, 2.682014338534471, 2.6755110404819433, 2.66943631074267, 2.6624930654952657, 2.655351651718484, 2.6478082866394543, 2.6391153974944315, 2.6293262475814663, 2.6187780094342554, 2.602392220448175, 2.593364010111752, 2.574903684573007, 2.5636341018598428, 2.548600672598492, 2.535118173329003, 2.5230908822719567, 2.5148072371003076, 2.50666384158438, 2.494923240007561, 2.486905652443731, 2.482930817985926, 2.480728515217681, 2.4796222308822724, 2.4745634940615426, 2.473256969843557, 2.468694511918806, 2.466290925807287, 2.466122327154422, 2.463847804216389, 2.4645720042242405, 2.465022224665178, 2.462286052468866, 2.458599677076085, 2.457068962336076, 2.4566766279678816, 2.456470234731874, 2.4545092637534016, 2.452653194208165, 2.4524777482912037, 2.453533498070813, 2.452332622559408, 2.4517749010904613, 2.451956602386381, 2.4514167520299828, 2.4523690509600318, 2.4506247710398337, 2.4507028277160208, 2.448924004223802, 2.45090743991139, 2.446735944003785, 2.446602059291863, 2.444749137067697, 2.443221366576835, 2.459143245783185, 2.4555057172168206, 2.457737355261613, 2.4442357342346, 2.4438166718218604, 2.445106843462715, 2.4438132791303757, 2.443308534367618, 2.443692154757051, 2.442611505265598, 2.443308756434697, 2.441691626562475, 2.4418938238273165, 2.4421774059350487, 2.4402016185392346, 2.439577479920593, 2.440864317128301, 2.441262400468517, 2.441206860003775, 2.4411930487630795, 2.441098577139069, 2.4411676528272688, 2.436957511275211, 2.438144686775286, 2.4362026964614523, 2.437436114689163, 2.440409017539367, 2.4450338684802673, 2.445549119424526, 2.440365831367289, 2.437138711549418, 2.434718382872595, 2.434653702408871, 2.4364636804044126, 2.4350257735477583, 2.43486816183008, 2.4322982039050154, 2.4327734944267196, 2.4319036038259707, 2.4326961834572667, 2.432915693533739, 2.432703226939364, 2.4333985818240187, 2.4307492785874825, 2.4320068669759762, 2.43650884168104, 2.436570537457476, 2.4260071910137513, 2.4315381002132406, 2.4299998865969616, 2.426858321450329, 2.427206203629104, 2.4273019521388184, 2.4260001680934207, 2.425703189505199, 2.426063250026664, 2.425892497186054, 2.4244997644326527, 2.4251325709129508, 2.4275042081515648, 2.4266364946502437, 2.4258590431918354, 2.426966290992878, 2.426517232585492, 2.4260145190315323, 2.4246034189416155, 2.425107366789048, 2.426309823891955, 2.427184274896704, 2.426208657356748, 2.4257242261506695, 2.4252146999938775, 2.425073101682095, 2.425004453580727, 2.4253591273110016, 2.4243674519125684, 2.4238311964383605, 2.4252482608113692, 2.426016548184154, 2.4268067318800783, 2.4224656064897103, 2.421387166115293, 2.4202785020980992, 2.421159883299403, 2.4218084490029965, 2.4217398419272484, 2.4328870636236988, 2.445391558034219, 2.485252458701633, 2.491816233511578, 2.4853193677671146, 2.459772610125845, 2.447803655248403, 2.44815335283534, 2.43351245739132, 2.4337843671716457, 2.4310476812249093, 2.430274843533181, 2.4290600469224994, 2.4282039221307334, 2.428455257317858, 2.427879513558421, 2.429554544288275, 2.4302845084446902, 2.431015021796099, 2.4340264854979465, 2.434360612736101, 2.461641411223206, 2.4426123746366715, 2.4365179734553153, 2.4298043132562657, 2.427070825349624, 2.4265495171047577, 2.4255875769581885, 2.42517609243765, 2.424873311514727, 2.426254495409235, 2.423899046351533, 2.4228476472459044, 2.423686898562453, 2.4288096733406586, 2.4384048499121067, 2.4754703953525614, 2.4512413942348785, 2.432008526995931, 2.4382941736577717, 2.430410890657554, 2.4295678040819735, 2.4294955140021792, 2.425468455790494, 2.4268430284895692, 2.4289645148009003, 2.4269415592021275, 2.427870189384758, 2.424146763059393, 2.4242511780599796, 2.424103610177794, 2.4229938032201184, 2.424765177575959, 2.4256359820003626, 2.4241777786239216, 2.4263422417689644, 2.4251924682202035, 2.427109351344177, 2.424208387359212, 2.423017391482907, 2.4222530067334187, 2.4214713407982544, 2.4233155362170336, 2.423921859631548, 2.4233538369133734, 2.423816312069276, 2.423686140322832, 2.424475988666135, 2.4215718693312187, 2.423055692668813, 2.4232655979524647, 2.4233226584213226, 2.423234784089075, 2.4233128631873786, 2.4224501743943296, 2.4232988975376073, 2.42167114506512, 2.4222002718727693, 2.420661879564947, 2.4203604941984955, 2.420526420507098, 2.4200153776752384, 2.419349618516174, 2.4200749521627563, 2.420558474342926, 2.4264932333811107, 2.427895395075271, 2.4264656681789263, 2.428074800698909, 2.4246676810223464, 2.425598893958685, 2.4245714885743004, 2.4219658890788804, 2.422564795668365, 2.4200495149075865, 2.418472159716628, 2.418315943359594, 2.4174817097994827, 2.4200648181492297, 2.4178489994464223, 2.4184193647372894, 2.4206057056019685, 2.4198093572925985, 2.41988028324605, 2.4177127815614736, 2.4164798306733433, 2.416681061730982, 2.417999345125359, 2.416585564270646, 2.4161317824338253, 2.4143828289219975, 2.4152312771250823, 2.4161641043559237, 2.4149766653714972, 2.416056498364991, 2.41561555529522, 2.416031004907659, 2.4171646451068853, 2.415503825299304, 2.417397010008168, 2.4163385347174424, 2.415968384635032, 2.415032268990237, 2.4148767763094736, 2.415479786244261, 2.4149398189795335, 2.415288853204715, 2.413079617008781, 2.4152599721473833, 2.4142670341585695, 2.4118678347530795, 2.412557006714525, 2.412281191471421, 2.412755386442619, 2.412315097726591, 2.412809418554913, 2.411320563263472, 2.4170261887309485, 2.4259920901586387, 2.425047825102444, 2.4214821764575873, 2.4211935753205474, 2.4190210744830374, 2.4167736693818958, 2.421998155288383, 2.4209228743762696, 2.419063789790661, 2.4240104697813, 2.4210724177546568, 2.4199359769449096, 2.418476171023547, 2.4162467652032995, 2.418847843851642, 2.4202864081952606, 2.4190402937620816, 2.421890772735314, 2.4211948565144312, 2.4210024509586594], 'acc': [0.05379876814002374, 0.05420944594015086, 0.09609856256408124, 0.11416837714413598, 0.14948665388310958, 0.1601642717387397, 0.15154003999683646, 0.14537987690074733, 0.13798767957966432, 0.13963039104943403, 0.1490759748575379, 0.14743326497273768, 0.15770020583449448, 0.14784394182586083, 0.1564681729619263, 0.15893223927618297, 0.16180698181936629, 0.16632443640143965, 0.1663244350122965, 0.16591375858754348, 0.1683778225151665, 0.1679671452887494, 0.17002053416240387, 0.16878850013323632, 0.17453798659038738, 0.1655030809878324, 0.16591375878336984, 0.1663244350306552, 0.16837782230098144, 0.17166324504469455, 0.1712525666249606, 0.17002053396657751, 0.16837782351265698, 0.1708418893985435, 0.1696098557793874, 0.17741273169767197, 0.17864476474770774, 0.18028747363501751, 0.1790554423841363, 0.17577002101120762, 0.17741273050435516, 0.17453798678621374, 0.17248459930170243, 0.18028747422249655, 0.18069815184056637, 0.17618069823762475, 0.1782340857404948, 0.1737166315500741, 0.17494866479593627, 0.17700205229880628, 0.18069815084307592, 0.18069815164474, 0.18275154032256813, 0.177002052317165, 0.1839835727851249, 0.18069815303388317, 0.17577001983624954, 0.1786447649251754, 0.17412731093058106, 0.18275154071422084, 0.18151950727253235, 0.17946611918218328, 0.18028747463250797, 0.1774127311101929, 0.17494866638090575, 0.17453798678621374, 0.17700205251299136, 0.1741273111080487, 0.1765913746990952, 0.181108830278659, 0.18028747345754986, 0.17987679662278547, 0.1778234075349459, 0.18110882847950444, 0.17700205308211167, 0.17823408732546428, 0.17659137626570598, 0.1790554425616039, 0.17782340851407766, 0.17535934319731147, 0.1774127303085288, 0.17987679582112134, 0.17782340929738305, 0.1749486642084572, 0.1819301843031231, 0.17905544079916677, 0.1798767968002531, 0.18603696009216858, 0.1839835727851249, 0.17823408515301573, 0.17494866481429497, 0.1815195074867174, 0.18028747420413782, 0.18151950807419645, 0.17700205308211167, 0.18151950729089106, 0.18357289518541378, 0.18480492702377405, 0.18275154071422084, 0.18069815223221905, 0.18151950609757425, 0.1831622185464757, 0.18028747502416068, 0.18891170459361536, 0.18357289575453412, 0.18275154051839448, 0.18151950609757425, 0.1839835720018195, 0.1819301843031231, 0.18193018487224344, 0.18069815164474, 0.18028747424085528, 0.18234086133371388, 0.1823408615111815, 0.18151950727253235, 0.1815195062934006, 0.18685831630743993, 0.18603696009216858, 0.18644763870772885, 0.1827515389517837, 0.1856262822782724, 0.18521560620845465, 0.1843942512048588, 0.1831622173531589, 0.18028747502416068, 0.18398357396008297, 0.1819301843031231, 0.18110883024194158, 0.18234086133371388, 0.1798767968186118, 0.18275154112423225, 0.18069815262387176, 0.18357289477540237, 0.18316221656985351, 0.18439424961988932, 0.18110883008283266, 0.1860369598779835, 0.18891170459361536, 0.18767967056444784, 0.18521560524768163, 0.1848049272196004, 0.18644763753277074, 0.18069815303388317, 0.18110882847950444, 0.16714579163757928, 0.16591375800006444, 0.17166324543634723, 0.17618069727685173, 0.17535934300148512, 0.186858316894919, 0.1802874732433648, 0.18151950766418504, 0.18110882906698347, 0.1819301850864285, 0.18028747342083243, 0.18193018490896087, 0.18562628404070955, 0.1864476384935438, 0.18521560524768163, 0.18398357239347218, 0.18357289438374969, 0.18521560464184386, 0.18069815144891366, 0.1733059558902678, 0.17618069862927743, 0.18439425157815278, 0.18521560563933434, 0.18028747461414923, 0.1835728963787306, 0.18398357198346077, 0.18357289614618683, 0.18234086192119292, 0.18275154071422084, 0.18439425140068516, 0.17946611898635692, 0.18275153994927415, 0.18069815205475143, 0.17700205329629676, 0.17412731055728708, 0.17453798680457247, 0.18439425059902104, 0.18193018569226627, 0.18234086331033608, 0.184394250011542, 0.1880903495533021, 0.18644763868937012, 0.18521560546186672, 0.18234086290032467, 0.18357289595036047, 0.1856262830615778, 0.18767967115192688, 0.18521560583516067, 0.18357289596871917, 0.18357289477540237, 0.18234086209866054, 0.18439424981571564, 0.18357289595036047, 0.18767967254107004, 0.18480492882292862, 0.18850102619224016, 0.18767967232688496, 0.18316221655149478, 0.1815195062934006, 0.18151950688087964, 0.18521560563933434, 0.18398357221600456, 0.18234086211701928, 0.1827515393434364, 0.1819301850864285, 0.18480492780707944, 0.18767967214941733, 0.183572894579576, 0.1876796717210472, 0.18521560544350799, 0.18357289536288143, 0.18316221794063794, 0.18767967074191547, 0.18480492782543817, 0.18562628304321907, 0.18439424942406296, 0.18480492821709085, 0.18316221774481162, 0.1839835735684303, 0.18480492900039625, 0.1819301838931117, 0.18562628225991368, 0.18069815084307592, 0.18234086152954024, 0.18357289636037188, 0.1856262822782724, 0.18110882887115715, 0.19055441487006827, 0.18685831552413454, 0.18521560563933434, 0.17987679662278547, 0.18644763868937012, 0.1831622185464757, 0.18726899413969483, 0.18850102580058747, 0.19342915919404743, 0.18726899472717387, 0.19055441387257782, 0.18603696167713807, 0.18726899492300023, 0.18932238162420614, 0.1868583153283082, 0.19342915919404743, 0.19055441408676288, 0.19219712434485708, 0.1938398365979322, 0.18973306002558135, 0.1901437378211188, 0.19425051264939122, 0.1917864483301155, 0.18891170500362678, 0.19301847979518177, 0.19507186923795658, 0.19301848098849858, 0.19630390191469838, 0.19137576971455522, 0.19630390212888346, 0.19425051402017565, 0.19589322312167048, 0.20000000049568545, 0.19835728881173065, 0.19794661238697764, 0.2016427103437682, 0.19753593375305864, 0.19671457972859455, 0.19835729041505887, 0.19589322369079082, 0.19383983601045315, 0.2008213547159759, 0.20205338837184944, 0.19958932227177786, 0.20246406657739832, 0.2008213543243232, 0.20041067848704924, 0.1967145779661574, 0.19425051382434932, 0.20041067729373244, 0.18521560544350799, 0.19137576893124983, 0.1897330586547969, 0.19219712414903073, 0.1909650920964854, 0.1909650917048327, 0.19219712414903073, 0.19137577106698092, 0.19301848077431352, 0.199178644439523, 0.1954825472843965, 0.19671457994277963, 0.19507186806299848, 0.1950718688646626, 0.1954825472660378, 0.1954825472660378, 0.19425051321851156, 0.19178644752845136, 0.19055441446005686]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
