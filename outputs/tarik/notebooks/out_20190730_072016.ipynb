{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf10.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 07:20:16 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '3', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['sk', 'eb', 'mb', 'eo', 'ek', 'yd', 'ib', 'sg', 'by', 'ck', 'my', 'ce', 'ds', 'eg', 'aa'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023FA427D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023FA0A06EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7017, Accuracy:0.0743, Validation Loss:2.6916, Validation Accuracy:0.0525\n",
    "Epoch #2: Loss:2.6889, Accuracy:0.0698, Validation Loss:2.6816, Validation Accuracy:0.0936\n",
    "Epoch #3: Loss:2.6807, Accuracy:0.0879, Validation Loss:2.6738, Validation Accuracy:0.0903\n",
    "Epoch #4: Loss:2.6735, Accuracy:0.0768, Validation Loss:2.6665, Validation Accuracy:0.0657\n",
    "Epoch #5: Loss:2.6669, Accuracy:0.0669, Validation Loss:2.6584, Validation Accuracy:0.0706\n",
    "Epoch #6: Loss:2.6593, Accuracy:0.0665, Validation Loss:2.6493, Validation Accuracy:0.0772\n",
    "Epoch #7: Loss:2.6509, Accuracy:0.0624, Validation Loss:2.6390, Validation Accuracy:0.0706\n",
    "Epoch #8: Loss:2.6412, Accuracy:0.0661, Validation Loss:2.6271, Validation Accuracy:0.0690\n",
    "Epoch #9: Loss:2.6302, Accuracy:0.0805, Validation Loss:2.6127, Validation Accuracy:0.1002\n",
    "Epoch #10: Loss:2.6165, Accuracy:0.0998, Validation Loss:2.5951, Validation Accuracy:0.1363\n",
    "Epoch #11: Loss:2.6013, Accuracy:0.1368, Validation Loss:2.5752, Validation Accuracy:0.1675\n",
    "Epoch #12: Loss:2.5859, Accuracy:0.1614, Validation Loss:2.5555, Validation Accuracy:0.1708\n",
    "Epoch #13: Loss:2.5716, Accuracy:0.1618, Validation Loss:2.5374, Validation Accuracy:0.1724\n",
    "Epoch #14: Loss:2.5582, Accuracy:0.1618, Validation Loss:2.5231, Validation Accuracy:0.1691\n",
    "Epoch #15: Loss:2.5519, Accuracy:0.1626, Validation Loss:2.5104, Validation Accuracy:0.1708\n",
    "Epoch #16: Loss:2.5403, Accuracy:0.1589, Validation Loss:2.5054, Validation Accuracy:0.1675\n",
    "Epoch #17: Loss:2.5322, Accuracy:0.1634, Validation Loss:2.4917, Validation Accuracy:0.1724\n",
    "Epoch #18: Loss:2.5249, Accuracy:0.1630, Validation Loss:2.4876, Validation Accuracy:0.1724\n",
    "Epoch #19: Loss:2.5201, Accuracy:0.1610, Validation Loss:2.4760, Validation Accuracy:0.1773\n",
    "Epoch #20: Loss:2.5118, Accuracy:0.1598, Validation Loss:2.4743, Validation Accuracy:0.1691\n",
    "Epoch #21: Loss:2.5081, Accuracy:0.1655, Validation Loss:2.4625, Validation Accuracy:0.1773\n",
    "Epoch #22: Loss:2.5016, Accuracy:0.1655, Validation Loss:2.4585, Validation Accuracy:0.1741\n",
    "Epoch #23: Loss:2.4958, Accuracy:0.1651, Validation Loss:2.4527, Validation Accuracy:0.1757\n",
    "Epoch #24: Loss:2.4916, Accuracy:0.1643, Validation Loss:2.4509, Validation Accuracy:0.1741\n",
    "Epoch #25: Loss:2.4882, Accuracy:0.1708, Validation Loss:2.4464, Validation Accuracy:0.1757\n",
    "Epoch #26: Loss:2.4852, Accuracy:0.1688, Validation Loss:2.4470, Validation Accuracy:0.1757\n",
    "Epoch #27: Loss:2.4821, Accuracy:0.1696, Validation Loss:2.4377, Validation Accuracy:0.1773\n",
    "Epoch #28: Loss:2.4801, Accuracy:0.1713, Validation Loss:2.4361, Validation Accuracy:0.1839\n",
    "Epoch #29: Loss:2.4759, Accuracy:0.1717, Validation Loss:2.4412, Validation Accuracy:0.1773\n",
    "Epoch #30: Loss:2.4763, Accuracy:0.1733, Validation Loss:2.4321, Validation Accuracy:0.1839\n",
    "Epoch #31: Loss:2.4744, Accuracy:0.1717, Validation Loss:2.4302, Validation Accuracy:0.1806\n",
    "Epoch #32: Loss:2.4703, Accuracy:0.1725, Validation Loss:2.4313, Validation Accuracy:0.1823\n",
    "Epoch #33: Loss:2.4695, Accuracy:0.1713, Validation Loss:2.4264, Validation Accuracy:0.1806\n",
    "Epoch #34: Loss:2.4700, Accuracy:0.1704, Validation Loss:2.4287, Validation Accuracy:0.1954\n",
    "Epoch #35: Loss:2.4697, Accuracy:0.1762, Validation Loss:2.4320, Validation Accuracy:0.1806\n",
    "Epoch #36: Loss:2.4651, Accuracy:0.1745, Validation Loss:2.4235, Validation Accuracy:0.1888\n",
    "Epoch #37: Loss:2.4640, Accuracy:0.1729, Validation Loss:2.4223, Validation Accuracy:0.1839\n",
    "Epoch #38: Loss:2.4611, Accuracy:0.1729, Validation Loss:2.4206, Validation Accuracy:0.1839\n",
    "Epoch #39: Loss:2.4597, Accuracy:0.1733, Validation Loss:2.4195, Validation Accuracy:0.1856\n",
    "Epoch #40: Loss:2.4589, Accuracy:0.1749, Validation Loss:2.4175, Validation Accuracy:0.1888\n",
    "Epoch #41: Loss:2.4573, Accuracy:0.1737, Validation Loss:2.4171, Validation Accuracy:0.1872\n",
    "Epoch #42: Loss:2.4571, Accuracy:0.1749, Validation Loss:2.4194, Validation Accuracy:0.1790\n",
    "Epoch #43: Loss:2.4570, Accuracy:0.1741, Validation Loss:2.4181, Validation Accuracy:0.1839\n",
    "Epoch #44: Loss:2.4539, Accuracy:0.1782, Validation Loss:2.4143, Validation Accuracy:0.1970\n",
    "Epoch #45: Loss:2.4524, Accuracy:0.1791, Validation Loss:2.4187, Validation Accuracy:0.1823\n",
    "Epoch #46: Loss:2.4540, Accuracy:0.1766, Validation Loss:2.4123, Validation Accuracy:0.1921\n",
    "Epoch #47: Loss:2.4551, Accuracy:0.1774, Validation Loss:2.4149, Validation Accuracy:0.1987\n",
    "Epoch #48: Loss:2.4530, Accuracy:0.1737, Validation Loss:2.4143, Validation Accuracy:0.1905\n",
    "Epoch #49: Loss:2.4497, Accuracy:0.1754, Validation Loss:2.4125, Validation Accuracy:0.1905\n",
    "Epoch #50: Loss:2.4470, Accuracy:0.1766, Validation Loss:2.4109, Validation Accuracy:0.1921\n",
    "Epoch #51: Loss:2.4458, Accuracy:0.1791, Validation Loss:2.4142, Validation Accuracy:0.1888\n",
    "Epoch #52: Loss:2.4454, Accuracy:0.1782, Validation Loss:2.4153, Validation Accuracy:0.2003\n",
    "Epoch #53: Loss:2.4472, Accuracy:0.1774, Validation Loss:2.4140, Validation Accuracy:0.1888\n",
    "Epoch #54: Loss:2.4425, Accuracy:0.1766, Validation Loss:2.4088, Validation Accuracy:0.1905\n",
    "Epoch #55: Loss:2.4422, Accuracy:0.1766, Validation Loss:2.4092, Validation Accuracy:0.1938\n",
    "Epoch #56: Loss:2.4415, Accuracy:0.1766, Validation Loss:2.4095, Validation Accuracy:0.1938\n",
    "Epoch #57: Loss:2.4388, Accuracy:0.1778, Validation Loss:2.4075, Validation Accuracy:0.1888\n",
    "Epoch #58: Loss:2.4409, Accuracy:0.1766, Validation Loss:2.4083, Validation Accuracy:0.1905\n",
    "Epoch #59: Loss:2.4373, Accuracy:0.1770, Validation Loss:2.4094, Validation Accuracy:0.1938\n",
    "Epoch #60: Loss:2.4380, Accuracy:0.1786, Validation Loss:2.4137, Validation Accuracy:0.1938\n",
    "Epoch #61: Loss:2.4383, Accuracy:0.1836, Validation Loss:2.4068, Validation Accuracy:0.1938\n",
    "Epoch #62: Loss:2.4352, Accuracy:0.1799, Validation Loss:2.4132, Validation Accuracy:0.1856\n",
    "Epoch #63: Loss:2.4341, Accuracy:0.1844, Validation Loss:2.4107, Validation Accuracy:0.1872\n",
    "Epoch #64: Loss:2.4333, Accuracy:0.1807, Validation Loss:2.4100, Validation Accuracy:0.2003\n",
    "Epoch #65: Loss:2.4362, Accuracy:0.1762, Validation Loss:2.4091, Validation Accuracy:0.1938\n",
    "Epoch #66: Loss:2.4312, Accuracy:0.1832, Validation Loss:2.4079, Validation Accuracy:0.1905\n",
    "Epoch #67: Loss:2.4290, Accuracy:0.1819, Validation Loss:2.4085, Validation Accuracy:0.1921\n",
    "Epoch #68: Loss:2.4288, Accuracy:0.1823, Validation Loss:2.4155, Validation Accuracy:0.1938\n",
    "Epoch #69: Loss:2.4309, Accuracy:0.1844, Validation Loss:2.4208, Validation Accuracy:0.1888\n",
    "Epoch #70: Loss:2.4340, Accuracy:0.1799, Validation Loss:2.4345, Validation Accuracy:0.1675\n",
    "Epoch #71: Loss:2.4289, Accuracy:0.1832, Validation Loss:2.4120, Validation Accuracy:0.2003\n",
    "Epoch #72: Loss:2.4262, Accuracy:0.1860, Validation Loss:2.4111, Validation Accuracy:0.1938\n",
    "Epoch #73: Loss:2.4264, Accuracy:0.1848, Validation Loss:2.4221, Validation Accuracy:0.1872\n",
    "Epoch #74: Loss:2.4266, Accuracy:0.1869, Validation Loss:2.4227, Validation Accuracy:0.1872\n",
    "Epoch #75: Loss:2.4293, Accuracy:0.1819, Validation Loss:2.4372, Validation Accuracy:0.1823\n",
    "Epoch #76: Loss:2.4311, Accuracy:0.1836, Validation Loss:2.4137, Validation Accuracy:0.1921\n",
    "Epoch #77: Loss:2.4238, Accuracy:0.1832, Validation Loss:2.4146, Validation Accuracy:0.1938\n",
    "Epoch #78: Loss:2.4204, Accuracy:0.1869, Validation Loss:2.4177, Validation Accuracy:0.1872\n",
    "Epoch #79: Loss:2.4193, Accuracy:0.1832, Validation Loss:2.4175, Validation Accuracy:0.1921\n",
    "Epoch #80: Loss:2.4166, Accuracy:0.1893, Validation Loss:2.4152, Validation Accuracy:0.1938\n",
    "Epoch #81: Loss:2.4178, Accuracy:0.1893, Validation Loss:2.4181, Validation Accuracy:0.1905\n",
    "Epoch #82: Loss:2.4278, Accuracy:0.1823, Validation Loss:2.4151, Validation Accuracy:0.1954\n",
    "Epoch #83: Loss:2.4251, Accuracy:0.1856, Validation Loss:2.4577, Validation Accuracy:0.1626\n",
    "Epoch #84: Loss:2.4325, Accuracy:0.1881, Validation Loss:2.4167, Validation Accuracy:0.1872\n",
    "Epoch #85: Loss:2.4192, Accuracy:0.1877, Validation Loss:2.4144, Validation Accuracy:0.1954\n",
    "Epoch #86: Loss:2.4189, Accuracy:0.1852, Validation Loss:2.4210, Validation Accuracy:0.1856\n",
    "Epoch #87: Loss:2.4107, Accuracy:0.1947, Validation Loss:2.4183, Validation Accuracy:0.1888\n",
    "Epoch #88: Loss:2.4099, Accuracy:0.1951, Validation Loss:2.4292, Validation Accuracy:0.1839\n",
    "Epoch #89: Loss:2.4122, Accuracy:0.1922, Validation Loss:2.4258, Validation Accuracy:0.1872\n",
    "Epoch #90: Loss:2.4080, Accuracy:0.1963, Validation Loss:2.4254, Validation Accuracy:0.1888\n",
    "Epoch #91: Loss:2.4105, Accuracy:0.1889, Validation Loss:2.4205, Validation Accuracy:0.1872\n",
    "Epoch #92: Loss:2.4108, Accuracy:0.1943, Validation Loss:2.4307, Validation Accuracy:0.1856\n",
    "Epoch #93: Loss:2.4192, Accuracy:0.1873, Validation Loss:2.4230, Validation Accuracy:0.1823\n",
    "Epoch #94: Loss:2.4215, Accuracy:0.1815, Validation Loss:2.4485, Validation Accuracy:0.1741\n",
    "Epoch #95: Loss:2.4133, Accuracy:0.1918, Validation Loss:2.4200, Validation Accuracy:0.1921\n",
    "Epoch #96: Loss:2.4085, Accuracy:0.1951, Validation Loss:2.4232, Validation Accuracy:0.1905\n",
    "Epoch #97: Loss:2.4044, Accuracy:0.2012, Validation Loss:2.4320, Validation Accuracy:0.1856\n",
    "Epoch #98: Loss:2.4070, Accuracy:0.1959, Validation Loss:2.4300, Validation Accuracy:0.1872\n",
    "Epoch #99: Loss:2.4024, Accuracy:0.1934, Validation Loss:2.4216, Validation Accuracy:0.1905\n",
    "Epoch #100: Loss:2.4021, Accuracy:0.2041, Validation Loss:2.4303, Validation Accuracy:0.1888\n",
    "Epoch #101: Loss:2.4113, Accuracy:0.2012, Validation Loss:2.4321, Validation Accuracy:0.1938\n",
    "Epoch #102: Loss:2.4018, Accuracy:0.2012, Validation Loss:2.4323, Validation Accuracy:0.1872\n",
    "Epoch #103: Loss:2.3981, Accuracy:0.1984, Validation Loss:2.4287, Validation Accuracy:0.1839\n",
    "Epoch #104: Loss:2.4021, Accuracy:0.2029, Validation Loss:2.4264, Validation Accuracy:0.1823\n",
    "Epoch #105: Loss:2.3977, Accuracy:0.1975, Validation Loss:2.4416, Validation Accuracy:0.1773\n",
    "Epoch #106: Loss:2.3942, Accuracy:0.2012, Validation Loss:2.4268, Validation Accuracy:0.1872\n",
    "Epoch #107: Loss:2.3904, Accuracy:0.2045, Validation Loss:2.4304, Validation Accuracy:0.1872\n",
    "Epoch #108: Loss:2.3882, Accuracy:0.2049, Validation Loss:2.4285, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.3873, Accuracy:0.2049, Validation Loss:2.4357, Validation Accuracy:0.1872\n",
    "Epoch #110: Loss:2.3879, Accuracy:0.2094, Validation Loss:2.4388, Validation Accuracy:0.1839\n",
    "Epoch #111: Loss:2.3855, Accuracy:0.2062, Validation Loss:2.4426, Validation Accuracy:0.1921\n",
    "Epoch #112: Loss:2.3867, Accuracy:0.2066, Validation Loss:2.4386, Validation Accuracy:0.1872\n",
    "Epoch #113: Loss:2.3873, Accuracy:0.2049, Validation Loss:2.4356, Validation Accuracy:0.1872\n",
    "Epoch #114: Loss:2.3829, Accuracy:0.2033, Validation Loss:2.4364, Validation Accuracy:0.1888\n",
    "Epoch #115: Loss:2.3834, Accuracy:0.2057, Validation Loss:2.4357, Validation Accuracy:0.1823\n",
    "Epoch #116: Loss:2.3821, Accuracy:0.2021, Validation Loss:2.4401, Validation Accuracy:0.1856\n",
    "Epoch #117: Loss:2.3784, Accuracy:0.2057, Validation Loss:2.4421, Validation Accuracy:0.1954\n",
    "Epoch #118: Loss:2.3798, Accuracy:0.2033, Validation Loss:2.4481, Validation Accuracy:0.1872\n",
    "Epoch #119: Loss:2.3859, Accuracy:0.1979, Validation Loss:2.4618, Validation Accuracy:0.1773\n",
    "Epoch #120: Loss:2.3817, Accuracy:0.2045, Validation Loss:2.4433, Validation Accuracy:0.1938\n",
    "Epoch #121: Loss:2.3771, Accuracy:0.2086, Validation Loss:2.4577, Validation Accuracy:0.1823\n",
    "Epoch #122: Loss:2.3763, Accuracy:0.2119, Validation Loss:2.4483, Validation Accuracy:0.1921\n",
    "Epoch #123: Loss:2.3702, Accuracy:0.2131, Validation Loss:2.4475, Validation Accuracy:0.1872\n",
    "Epoch #124: Loss:2.3716, Accuracy:0.2107, Validation Loss:2.4473, Validation Accuracy:0.1905\n",
    "Epoch #125: Loss:2.3756, Accuracy:0.2103, Validation Loss:2.4500, Validation Accuracy:0.1921\n",
    "Epoch #126: Loss:2.3708, Accuracy:0.2172, Validation Loss:2.4492, Validation Accuracy:0.1872\n",
    "Epoch #127: Loss:2.3691, Accuracy:0.2037, Validation Loss:2.4847, Validation Accuracy:0.1691\n",
    "Epoch #128: Loss:2.3796, Accuracy:0.2078, Validation Loss:2.4822, Validation Accuracy:0.1856\n",
    "Epoch #129: Loss:2.3851, Accuracy:0.2025, Validation Loss:2.4801, Validation Accuracy:0.1593\n",
    "Epoch #130: Loss:2.3814, Accuracy:0.2070, Validation Loss:2.4430, Validation Accuracy:0.1839\n",
    "Epoch #131: Loss:2.3693, Accuracy:0.2148, Validation Loss:2.4543, Validation Accuracy:0.1987\n",
    "Epoch #132: Loss:2.3658, Accuracy:0.2156, Validation Loss:2.4501, Validation Accuracy:0.1987\n",
    "Epoch #133: Loss:2.3612, Accuracy:0.2119, Validation Loss:2.4630, Validation Accuracy:0.1741\n",
    "Epoch #134: Loss:2.3667, Accuracy:0.2049, Validation Loss:2.4574, Validation Accuracy:0.1905\n",
    "Epoch #135: Loss:2.3647, Accuracy:0.2164, Validation Loss:2.4582, Validation Accuracy:0.1921\n",
    "Epoch #136: Loss:2.3548, Accuracy:0.2189, Validation Loss:2.4555, Validation Accuracy:0.1856\n",
    "Epoch #137: Loss:2.3520, Accuracy:0.2222, Validation Loss:2.4539, Validation Accuracy:0.1839\n",
    "Epoch #138: Loss:2.3530, Accuracy:0.2168, Validation Loss:2.4689, Validation Accuracy:0.1790\n",
    "Epoch #139: Loss:2.3533, Accuracy:0.2193, Validation Loss:2.4560, Validation Accuracy:0.1921\n",
    "Epoch #140: Loss:2.3509, Accuracy:0.2164, Validation Loss:2.4659, Validation Accuracy:0.1806\n",
    "Epoch #141: Loss:2.3490, Accuracy:0.2234, Validation Loss:2.4636, Validation Accuracy:0.1839\n",
    "Epoch #142: Loss:2.3473, Accuracy:0.2214, Validation Loss:2.4641, Validation Accuracy:0.1905\n",
    "Epoch #143: Loss:2.3493, Accuracy:0.2255, Validation Loss:2.4675, Validation Accuracy:0.1823\n",
    "Epoch #144: Loss:2.3473, Accuracy:0.2164, Validation Loss:2.4761, Validation Accuracy:0.1724\n",
    "Epoch #145: Loss:2.3541, Accuracy:0.2152, Validation Loss:2.4836, Validation Accuracy:0.1773\n",
    "Epoch #146: Loss:2.3496, Accuracy:0.2189, Validation Loss:2.4868, Validation Accuracy:0.1724\n",
    "Epoch #147: Loss:2.3440, Accuracy:0.2209, Validation Loss:2.4657, Validation Accuracy:0.1856\n",
    "Epoch #148: Loss:2.3601, Accuracy:0.2156, Validation Loss:2.4628, Validation Accuracy:0.1806\n",
    "Epoch #149: Loss:2.3651, Accuracy:0.2099, Validation Loss:2.4765, Validation Accuracy:0.1773\n",
    "Epoch #150: Loss:2.3615, Accuracy:0.2127, Validation Loss:2.4600, Validation Accuracy:0.1823\n",
    "Epoch #151: Loss:2.3464, Accuracy:0.2127, Validation Loss:2.4681, Validation Accuracy:0.1839\n",
    "Epoch #152: Loss:2.3356, Accuracy:0.2296, Validation Loss:2.4629, Validation Accuracy:0.1872\n",
    "Epoch #153: Loss:2.3358, Accuracy:0.2230, Validation Loss:2.4723, Validation Accuracy:0.1773\n",
    "Epoch #154: Loss:2.3325, Accuracy:0.2234, Validation Loss:2.4754, Validation Accuracy:0.1806\n",
    "Epoch #155: Loss:2.3337, Accuracy:0.2275, Validation Loss:2.4903, Validation Accuracy:0.1773\n",
    "Epoch #156: Loss:2.3327, Accuracy:0.2271, Validation Loss:2.4745, Validation Accuracy:0.1790\n",
    "Epoch #157: Loss:2.3372, Accuracy:0.2263, Validation Loss:2.4784, Validation Accuracy:0.1823\n",
    "Epoch #158: Loss:2.3390, Accuracy:0.2234, Validation Loss:2.4772, Validation Accuracy:0.1757\n",
    "Epoch #159: Loss:2.3298, Accuracy:0.2275, Validation Loss:2.4783, Validation Accuracy:0.1856\n",
    "Epoch #160: Loss:2.3269, Accuracy:0.2300, Validation Loss:2.4947, Validation Accuracy:0.1806\n",
    "Epoch #161: Loss:2.3308, Accuracy:0.2234, Validation Loss:2.4780, Validation Accuracy:0.1773\n",
    "Epoch #162: Loss:2.3238, Accuracy:0.2283, Validation Loss:2.4844, Validation Accuracy:0.1790\n",
    "Epoch #163: Loss:2.3225, Accuracy:0.2304, Validation Loss:2.4849, Validation Accuracy:0.1839\n",
    "Epoch #164: Loss:2.3218, Accuracy:0.2292, Validation Loss:2.4892, Validation Accuracy:0.1823\n",
    "Epoch #165: Loss:2.3216, Accuracy:0.2320, Validation Loss:2.5149, Validation Accuracy:0.1708\n",
    "Epoch #166: Loss:2.3337, Accuracy:0.2205, Validation Loss:2.5208, Validation Accuracy:0.1642\n",
    "Epoch #167: Loss:2.3344, Accuracy:0.2222, Validation Loss:2.4925, Validation Accuracy:0.1741\n",
    "Epoch #168: Loss:2.3330, Accuracy:0.2238, Validation Loss:2.5090, Validation Accuracy:0.1724\n",
    "Epoch #169: Loss:2.3234, Accuracy:0.2279, Validation Loss:2.4911, Validation Accuracy:0.1773\n",
    "Epoch #170: Loss:2.3170, Accuracy:0.2341, Validation Loss:2.4875, Validation Accuracy:0.1773\n",
    "Epoch #171: Loss:2.3123, Accuracy:0.2374, Validation Loss:2.4903, Validation Accuracy:0.1757\n",
    "Epoch #172: Loss:2.3117, Accuracy:0.2316, Validation Loss:2.4911, Validation Accuracy:0.1806\n",
    "Epoch #173: Loss:2.3130, Accuracy:0.2345, Validation Loss:2.4913, Validation Accuracy:0.1757\n",
    "Epoch #174: Loss:2.3113, Accuracy:0.2337, Validation Loss:2.5071, Validation Accuracy:0.1773\n",
    "Epoch #175: Loss:2.3091, Accuracy:0.2366, Validation Loss:2.4955, Validation Accuracy:0.1823\n",
    "Epoch #176: Loss:2.3090, Accuracy:0.2349, Validation Loss:2.5038, Validation Accuracy:0.1790\n",
    "Epoch #177: Loss:2.3113, Accuracy:0.2415, Validation Loss:2.5024, Validation Accuracy:0.1790\n",
    "Epoch #178: Loss:2.3115, Accuracy:0.2324, Validation Loss:2.4958, Validation Accuracy:0.1806\n",
    "Epoch #179: Loss:2.3060, Accuracy:0.2361, Validation Loss:2.5075, Validation Accuracy:0.1741\n",
    "Epoch #180: Loss:2.3082, Accuracy:0.2316, Validation Loss:2.5086, Validation Accuracy:0.1741\n",
    "Epoch #181: Loss:2.3052, Accuracy:0.2378, Validation Loss:2.5052, Validation Accuracy:0.1839\n",
    "Epoch #182: Loss:2.3045, Accuracy:0.2271, Validation Loss:2.5074, Validation Accuracy:0.1708\n",
    "Epoch #183: Loss:2.3099, Accuracy:0.2329, Validation Loss:2.5076, Validation Accuracy:0.1691\n",
    "Epoch #184: Loss:2.3072, Accuracy:0.2283, Validation Loss:2.5032, Validation Accuracy:0.1856\n",
    "Epoch #185: Loss:2.3042, Accuracy:0.2353, Validation Loss:2.5129, Validation Accuracy:0.1708\n",
    "Epoch #186: Loss:2.2953, Accuracy:0.2407, Validation Loss:2.5130, Validation Accuracy:0.1691\n",
    "Epoch #187: Loss:2.2915, Accuracy:0.2427, Validation Loss:2.5188, Validation Accuracy:0.1708\n",
    "Epoch #188: Loss:2.2940, Accuracy:0.2366, Validation Loss:2.5370, Validation Accuracy:0.1691\n",
    "Epoch #189: Loss:2.2951, Accuracy:0.2370, Validation Loss:2.5193, Validation Accuracy:0.1741\n",
    "Epoch #190: Loss:2.2974, Accuracy:0.2337, Validation Loss:2.5259, Validation Accuracy:0.1757\n",
    "Epoch #191: Loss:2.2920, Accuracy:0.2419, Validation Loss:2.5212, Validation Accuracy:0.1741\n",
    "Epoch #192: Loss:2.2922, Accuracy:0.2366, Validation Loss:2.5201, Validation Accuracy:0.1773\n",
    "Epoch #193: Loss:2.2886, Accuracy:0.2415, Validation Loss:2.5214, Validation Accuracy:0.1724\n",
    "Epoch #194: Loss:2.2859, Accuracy:0.2427, Validation Loss:2.5398, Validation Accuracy:0.1724\n",
    "Epoch #195: Loss:2.2835, Accuracy:0.2374, Validation Loss:2.5402, Validation Accuracy:0.1708\n",
    "Epoch #196: Loss:2.2850, Accuracy:0.2366, Validation Loss:2.5334, Validation Accuracy:0.1708\n",
    "Epoch #197: Loss:2.2827, Accuracy:0.2415, Validation Loss:2.5452, Validation Accuracy:0.1741\n",
    "Epoch #198: Loss:2.2952, Accuracy:0.2361, Validation Loss:2.5448, Validation Accuracy:0.1724\n",
    "Epoch #199: Loss:2.2924, Accuracy:0.2398, Validation Loss:2.5560, Validation Accuracy:0.1724\n",
    "Epoch #200: Loss:2.2816, Accuracy:0.2407, Validation Loss:2.5307, Validation Accuracy:0.1691\n",
    "Epoch #201: Loss:2.2767, Accuracy:0.2444, Validation Loss:2.5413, Validation Accuracy:0.1741\n",
    "Epoch #202: Loss:2.2751, Accuracy:0.2370, Validation Loss:2.5389, Validation Accuracy:0.1675\n",
    "Epoch #203: Loss:2.2796, Accuracy:0.2419, Validation Loss:2.5465, Validation Accuracy:0.1724\n",
    "Epoch #204: Loss:2.2751, Accuracy:0.2390, Validation Loss:2.5539, Validation Accuracy:0.1741\n",
    "Epoch #205: Loss:2.2717, Accuracy:0.2485, Validation Loss:2.5522, Validation Accuracy:0.1741\n",
    "Epoch #206: Loss:2.2793, Accuracy:0.2386, Validation Loss:2.5494, Validation Accuracy:0.1823\n",
    "Epoch #207: Loss:2.2828, Accuracy:0.2468, Validation Loss:2.5415, Validation Accuracy:0.1741\n",
    "Epoch #208: Loss:2.2768, Accuracy:0.2407, Validation Loss:2.5655, Validation Accuracy:0.1658\n",
    "Epoch #209: Loss:2.2703, Accuracy:0.2522, Validation Loss:2.5422, Validation Accuracy:0.1675\n",
    "Epoch #210: Loss:2.2756, Accuracy:0.2444, Validation Loss:2.5437, Validation Accuracy:0.1790\n",
    "Epoch #211: Loss:2.2643, Accuracy:0.2476, Validation Loss:2.5463, Validation Accuracy:0.1708\n",
    "Epoch #212: Loss:2.2773, Accuracy:0.2427, Validation Loss:2.5490, Validation Accuracy:0.1691\n",
    "Epoch #213: Loss:2.2796, Accuracy:0.2394, Validation Loss:2.5488, Validation Accuracy:0.1675\n",
    "Epoch #214: Loss:2.2875, Accuracy:0.2320, Validation Loss:2.5493, Validation Accuracy:0.1691\n",
    "Epoch #215: Loss:2.2621, Accuracy:0.2546, Validation Loss:2.5494, Validation Accuracy:0.1675\n",
    "Epoch #216: Loss:2.2593, Accuracy:0.2530, Validation Loss:2.5499, Validation Accuracy:0.1724\n",
    "Epoch #217: Loss:2.2568, Accuracy:0.2505, Validation Loss:2.5533, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.2593, Accuracy:0.2493, Validation Loss:2.5708, Validation Accuracy:0.1658\n",
    "Epoch #219: Loss:2.2659, Accuracy:0.2448, Validation Loss:2.5529, Validation Accuracy:0.1675\n",
    "Epoch #220: Loss:2.2704, Accuracy:0.2489, Validation Loss:2.5649, Validation Accuracy:0.1609\n",
    "Epoch #221: Loss:2.2672, Accuracy:0.2435, Validation Loss:2.5643, Validation Accuracy:0.1708\n",
    "Epoch #222: Loss:2.2603, Accuracy:0.2489, Validation Loss:2.5799, Validation Accuracy:0.1708\n",
    "Epoch #223: Loss:2.2556, Accuracy:0.2472, Validation Loss:2.5669, Validation Accuracy:0.1675\n",
    "Epoch #224: Loss:2.2564, Accuracy:0.2407, Validation Loss:2.5929, Validation Accuracy:0.1658\n",
    "Epoch #225: Loss:2.2552, Accuracy:0.2542, Validation Loss:2.5829, Validation Accuracy:0.1691\n",
    "Epoch #226: Loss:2.2559, Accuracy:0.2448, Validation Loss:2.5748, Validation Accuracy:0.1741\n",
    "Epoch #227: Loss:2.2522, Accuracy:0.2534, Validation Loss:2.5898, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.2635, Accuracy:0.2431, Validation Loss:2.5948, Validation Accuracy:0.1741\n",
    "Epoch #229: Loss:2.2586, Accuracy:0.2427, Validation Loss:2.5840, Validation Accuracy:0.1806\n",
    "Epoch #230: Loss:2.2652, Accuracy:0.2439, Validation Loss:2.5695, Validation Accuracy:0.1757\n",
    "Epoch #231: Loss:2.2503, Accuracy:0.2530, Validation Loss:2.5747, Validation Accuracy:0.1658\n",
    "Epoch #232: Loss:2.2529, Accuracy:0.2423, Validation Loss:2.5786, Validation Accuracy:0.1544\n",
    "Epoch #233: Loss:2.2595, Accuracy:0.2460, Validation Loss:2.5679, Validation Accuracy:0.1658\n",
    "Epoch #234: Loss:2.2560, Accuracy:0.2505, Validation Loss:2.5788, Validation Accuracy:0.1675\n",
    "Epoch #235: Loss:2.2658, Accuracy:0.2431, Validation Loss:2.5698, Validation Accuracy:0.1642\n",
    "Epoch #236: Loss:2.2525, Accuracy:0.2468, Validation Loss:2.5921, Validation Accuracy:0.1691\n",
    "Epoch #237: Loss:2.2489, Accuracy:0.2468, Validation Loss:2.5902, Validation Accuracy:0.1757\n",
    "Epoch #238: Loss:2.2392, Accuracy:0.2542, Validation Loss:2.5931, Validation Accuracy:0.1757\n",
    "Epoch #239: Loss:2.2408, Accuracy:0.2550, Validation Loss:2.6007, Validation Accuracy:0.1741\n",
    "Epoch #240: Loss:2.2450, Accuracy:0.2480, Validation Loss:2.5883, Validation Accuracy:0.1626\n",
    "Epoch #241: Loss:2.2296, Accuracy:0.2554, Validation Loss:2.5919, Validation Accuracy:0.1691\n",
    "Epoch #242: Loss:2.2263, Accuracy:0.2583, Validation Loss:2.5896, Validation Accuracy:0.1675\n",
    "Epoch #243: Loss:2.2247, Accuracy:0.2604, Validation Loss:2.5953, Validation Accuracy:0.1691\n",
    "Epoch #244: Loss:2.2256, Accuracy:0.2641, Validation Loss:2.5968, Validation Accuracy:0.1675\n",
    "Epoch #245: Loss:2.2339, Accuracy:0.2637, Validation Loss:2.5986, Validation Accuracy:0.1658\n",
    "Epoch #246: Loss:2.2344, Accuracy:0.2628, Validation Loss:2.5954, Validation Accuracy:0.1658\n",
    "Epoch #247: Loss:2.2276, Accuracy:0.2608, Validation Loss:2.6112, Validation Accuracy:0.1691\n",
    "Epoch #248: Loss:2.2290, Accuracy:0.2608, Validation Loss:2.6052, Validation Accuracy:0.1708\n",
    "Epoch #249: Loss:2.2249, Accuracy:0.2567, Validation Loss:2.6038, Validation Accuracy:0.1708\n",
    "Epoch #250: Loss:2.2201, Accuracy:0.2600, Validation Loss:2.6099, Validation Accuracy:0.1576\n",
    "Epoch #251: Loss:2.2262, Accuracy:0.2497, Validation Loss:2.6124, Validation Accuracy:0.1708\n",
    "Epoch #252: Loss:2.2185, Accuracy:0.2624, Validation Loss:2.6149, Validation Accuracy:0.1642\n",
    "Epoch #253: Loss:2.2202, Accuracy:0.2637, Validation Loss:2.6104, Validation Accuracy:0.1708\n",
    "Epoch #254: Loss:2.2120, Accuracy:0.2669, Validation Loss:2.6349, Validation Accuracy:0.1658\n",
    "Epoch #255: Loss:2.2274, Accuracy:0.2595, Validation Loss:2.6297, Validation Accuracy:0.1675\n",
    "Epoch #256: Loss:2.2277, Accuracy:0.2517, Validation Loss:2.6375, Validation Accuracy:0.1675\n",
    "Epoch #257: Loss:2.2213, Accuracy:0.2563, Validation Loss:2.6345, Validation Accuracy:0.1790\n",
    "Epoch #258: Loss:2.2115, Accuracy:0.2686, Validation Loss:2.6201, Validation Accuracy:0.1658\n",
    "Epoch #259: Loss:2.2155, Accuracy:0.2641, Validation Loss:2.6338, Validation Accuracy:0.1691\n",
    "Epoch #260: Loss:2.2127, Accuracy:0.2682, Validation Loss:2.6349, Validation Accuracy:0.1593\n",
    "Epoch #261: Loss:2.2139, Accuracy:0.2628, Validation Loss:2.6258, Validation Accuracy:0.1658\n",
    "Epoch #262: Loss:2.2169, Accuracy:0.2583, Validation Loss:2.6495, Validation Accuracy:0.1544\n",
    "Epoch #263: Loss:2.2163, Accuracy:0.2600, Validation Loss:2.6324, Validation Accuracy:0.1724\n",
    "Epoch #264: Loss:2.2142, Accuracy:0.2616, Validation Loss:2.6418, Validation Accuracy:0.1626\n",
    "Epoch #265: Loss:2.2214, Accuracy:0.2632, Validation Loss:2.6328, Validation Accuracy:0.1626\n",
    "Epoch #266: Loss:2.2142, Accuracy:0.2612, Validation Loss:2.6355, Validation Accuracy:0.1544\n",
    "Epoch #267: Loss:2.2130, Accuracy:0.2665, Validation Loss:2.6304, Validation Accuracy:0.1724\n",
    "Epoch #268: Loss:2.2040, Accuracy:0.2637, Validation Loss:2.6410, Validation Accuracy:0.1642\n",
    "Epoch #269: Loss:2.2038, Accuracy:0.2686, Validation Loss:2.6325, Validation Accuracy:0.1658\n",
    "Epoch #270: Loss:2.2022, Accuracy:0.2690, Validation Loss:2.6432, Validation Accuracy:0.1724\n",
    "Epoch #271: Loss:2.1963, Accuracy:0.2674, Validation Loss:2.6430, Validation Accuracy:0.1691\n",
    "Epoch #272: Loss:2.1958, Accuracy:0.2702, Validation Loss:2.6539, Validation Accuracy:0.1724\n",
    "Epoch #273: Loss:2.1976, Accuracy:0.2665, Validation Loss:2.6526, Validation Accuracy:0.1708\n",
    "Epoch #274: Loss:2.1917, Accuracy:0.2756, Validation Loss:2.6468, Validation Accuracy:0.1560\n",
    "Epoch #275: Loss:2.1891, Accuracy:0.2764, Validation Loss:2.6504, Validation Accuracy:0.1593\n",
    "Epoch #276: Loss:2.1930, Accuracy:0.2768, Validation Loss:2.6575, Validation Accuracy:0.1658\n",
    "Epoch #277: Loss:2.1948, Accuracy:0.2669, Validation Loss:2.6452, Validation Accuracy:0.1445\n",
    "Epoch #278: Loss:2.1950, Accuracy:0.2706, Validation Loss:2.6497, Validation Accuracy:0.1527\n",
    "Epoch #279: Loss:2.1987, Accuracy:0.2690, Validation Loss:2.6655, Validation Accuracy:0.1658\n",
    "Epoch #280: Loss:2.1858, Accuracy:0.2789, Validation Loss:2.6627, Validation Accuracy:0.1626\n",
    "Epoch #281: Loss:2.1815, Accuracy:0.2760, Validation Loss:2.6640, Validation Accuracy:0.1724\n",
    "Epoch #282: Loss:2.1810, Accuracy:0.2797, Validation Loss:2.6758, Validation Accuracy:0.1724\n",
    "Epoch #283: Loss:2.1893, Accuracy:0.2776, Validation Loss:2.6656, Validation Accuracy:0.1560\n",
    "Epoch #284: Loss:2.1797, Accuracy:0.2743, Validation Loss:2.6713, Validation Accuracy:0.1675\n",
    "Epoch #285: Loss:2.1739, Accuracy:0.2756, Validation Loss:2.6791, Validation Accuracy:0.1609\n",
    "Epoch #286: Loss:2.1774, Accuracy:0.2883, Validation Loss:2.6709, Validation Accuracy:0.1675\n",
    "Epoch #287: Loss:2.1759, Accuracy:0.2805, Validation Loss:2.6802, Validation Accuracy:0.1658\n",
    "Epoch #288: Loss:2.1730, Accuracy:0.2801, Validation Loss:2.6888, Validation Accuracy:0.1675\n",
    "Epoch #289: Loss:2.1702, Accuracy:0.2784, Validation Loss:2.6912, Validation Accuracy:0.1609\n",
    "Epoch #290: Loss:2.1736, Accuracy:0.2789, Validation Loss:2.6994, Validation Accuracy:0.1593\n",
    "Epoch #291: Loss:2.1921, Accuracy:0.2715, Validation Loss:2.6901, Validation Accuracy:0.1576\n",
    "Epoch #292: Loss:2.1902, Accuracy:0.2719, Validation Loss:2.6761, Validation Accuracy:0.1642\n",
    "Epoch #293: Loss:2.1766, Accuracy:0.2789, Validation Loss:2.6872, Validation Accuracy:0.1626\n",
    "Epoch #294: Loss:2.1722, Accuracy:0.2780, Validation Loss:2.6928, Validation Accuracy:0.1708\n",
    "Epoch #295: Loss:2.1802, Accuracy:0.2731, Validation Loss:2.6796, Validation Accuracy:0.1642\n",
    "Epoch #296: Loss:2.1738, Accuracy:0.2809, Validation Loss:2.7096, Validation Accuracy:0.1626\n",
    "Epoch #297: Loss:2.1699, Accuracy:0.2805, Validation Loss:2.6860, Validation Accuracy:0.1675\n",
    "Epoch #298: Loss:2.1616, Accuracy:0.2809, Validation Loss:2.7014, Validation Accuracy:0.1658\n",
    "Epoch #299: Loss:2.1640, Accuracy:0.2830, Validation Loss:2.7083, Validation Accuracy:0.1576\n",
    "Epoch #300: Loss:2.1634, Accuracy:0.2821, Validation Loss:2.6879, Validation Accuracy:0.1642\n",
    "\n",
    "Test:\n",
    "Test Loss:2.68789315, Accuracy:0.1642\n",
    "Labels: ['sk', 'eb', 'mb', 'eo', 'ek', 'yd', 'ib', 'sg', 'by', 'ck', 'my', 'ce', 'ds', 'eg', 'aa']\n",
    "Confusion Matrix:\n",
    "      sk  eb  mb  eo  ek  yd  ib  sg  by  ck  my  ce  ds  eg  aa\n",
    "t:sk   0   2   1   6   0   1   1   3   3   0   0   0   6  10   0\n",
    "t:eb   0  13   2   2   1   7   0   3   3   0   0   0   3  16   0\n",
    "t:mb   0   5   5   4   0  10   1  10   5   0   0   0   3   9   0\n",
    "t:eo   0   2   0   3   5   4   0  13   1   0   0   0   1   5   0\n",
    "t:ek   0   3   1   7   0  11   2   4   5   0   0   0   1  12   2\n",
    "t:yd   0   2   9   5   0  31   3   4   5   0   0   0   0   3   0\n",
    "t:ib   0   2   4   5   0  29   1   9   2   0   0   0   0   2   0\n",
    "t:sg   0   5   1   5   1  16   0  14   7   0   0   0   0   2   0\n",
    "t:by   0   4   3   7   1   9   0   4   3   0   0   0   0   9   0\n",
    "t:ck   0   2   0   4   1   1   0   2   2   0   0   0   4   7   0\n",
    "t:my   0   1   1   1   0   5   0   3   1   0   0   0   2   6   0\n",
    "t:ce   0   1   0   5   0   3   0   6   3   0   0   0   2   7   0\n",
    "t:ds   0   1   0   2   1   5   1   6   1   0   0   0   9   5   0\n",
    "t:eg   0   9   1   2   1   0   0   5   2   0   0   0   8  19   3\n",
    "t:aa   0   1   1   2   0   3   0   2   1   0   0   0   8  14   2\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eb       0.25      0.26      0.25        50\n",
    "          mb       0.17      0.10      0.12        52\n",
    "          eo       0.05      0.09      0.06        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          yd       0.23      0.50      0.31        62\n",
    "          ib       0.11      0.02      0.03        54\n",
    "          sg       0.16      0.27      0.20        51\n",
    "          by       0.07      0.07      0.07        40\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ds       0.19      0.29      0.23        31\n",
    "          eg       0.15      0.38      0.22        50\n",
    "          aa       0.29      0.06      0.10        34\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.11      0.14      0.11       609\n",
    "weighted avg       0.13      0.16      0.13       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 07:35:53 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 36 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6916460337114256, 2.6815993899390813, 2.673815558892361, 2.666489117251241, 2.658442747025263, 2.6492710927828584, 2.63901514137907, 2.6271045979216376, 2.6126608872061294, 2.5950964828234393, 2.5751808476565508, 2.555494811147305, 2.537420942669823, 2.5230779518634816, 2.5103823022889387, 2.505388368722449, 2.491714513947811, 2.487558477226345, 2.476016128787462, 2.4743339060171095, 2.4624765929330157, 2.458526546340466, 2.4526549180348716, 2.4509413555533626, 2.4464189254591617, 2.4469727581161975, 2.4377159105341617, 2.4361314303769266, 2.4411703228754753, 2.432062439534856, 2.430221305887883, 2.43126580398071, 2.426439174877599, 2.428668192454747, 2.4320458880394744, 2.423458663113599, 2.422349965435335, 2.4205731613491164, 2.4195039565927288, 2.4175011500936425, 2.4170644306784195, 2.4193940334915136, 2.4180583491896956, 2.41432119942651, 2.418744284139674, 2.412320930969539, 2.414868200549547, 2.414319704318869, 2.4125049063333344, 2.410850086039902, 2.414209070268327, 2.4153200225485567, 2.414032836265752, 2.4088095103578615, 2.4092086124889955, 2.409518832252139, 2.407482772429393, 2.4082730219673443, 2.4094096110959358, 2.4136952566983076, 2.40684603548598, 2.4131920967978995, 2.4106507454012416, 2.409999692772801, 2.409102035077726, 2.4078761117994687, 2.4085303139804033, 2.415514900570824, 2.420776225858917, 2.434542846992881, 2.4119522912161693, 2.4110974408135624, 2.422068016086697, 2.422713983235101, 2.437216730540609, 2.4137299593250545, 2.414606180097082, 2.417736433213959, 2.4174914606686295, 2.41524992122243, 2.418069697365972, 2.415146359081926, 2.457689941418778, 2.4166793349536966, 2.4143881050040963, 2.4210404265298826, 2.418310949172097, 2.429232874918845, 2.4258069620148106, 2.4254370056741146, 2.420475272709513, 2.4306734610465166, 2.42300375576677, 2.448498975271466, 2.419969613916181, 2.4231501623914746, 2.4319801479333334, 2.429999881190032, 2.421564583316421, 2.430276489414409, 2.432101235992607, 2.432253547098445, 2.428717533747355, 2.4263956167036285, 2.44161816105271, 2.4268136705671037, 2.4304279333656447, 2.4284899007706415, 2.4356834884543335, 2.438764297316227, 2.4426152921466797, 2.438568072561756, 2.4356014489736073, 2.4364197669162344, 2.4357068131514175, 2.4400841911829554, 2.4420785207075046, 2.448125046462261, 2.461832973561655, 2.4433270362014645, 2.4577220253560736, 2.4482893336974145, 2.4474936995795993, 2.4473489164718853, 2.450034185387622, 2.4491850993120416, 2.4846623566350328, 2.48216230685292, 2.480056380990691, 2.44302590139981, 2.4543096357574212, 2.4500681700181883, 2.463016103249661, 2.457432623958744, 2.458157445801107, 2.455489545424388, 2.4538761477165036, 2.468867852378557, 2.4559895295423435, 2.465944581431121, 2.463608141603141, 2.464097739048975, 2.467492865811428, 2.4760510749221827, 2.4835691315004196, 2.486832228787427, 2.4656597467870354, 2.4628308547541424, 2.4765362528157353, 2.459953529689895, 2.4681317587008422, 2.4629431436410285, 2.4722687834002115, 2.475437030416404, 2.4902840384904583, 2.474533162485007, 2.4783672186345695, 2.4772403142330877, 2.478288731160031, 2.4946768084183115, 2.4779514636116464, 2.484395033033023, 2.4848956071293022, 2.489229690460932, 2.514888164445097, 2.520792798651459, 2.492452465254685, 2.5090493322006, 2.491060118369868, 2.487450956711041, 2.4902604338766516, 2.491076400127317, 2.491272738414445, 2.5071330544200827, 2.4955077026473673, 2.503784395399548, 2.5023766617078107, 2.4957502080106186, 2.5074531910650446, 2.508649187526484, 2.5051736941282776, 2.507449445270357, 2.5075646965765994, 2.503193780119196, 2.512893158031019, 2.5129537993463975, 2.5187862890303037, 2.536989762865264, 2.5193351180291135, 2.525905981439675, 2.5211700195162168, 2.5201002134282406, 2.52139529492859, 2.53983183995452, 2.5401971242306463, 2.5334254665719267, 2.5451889695792365, 2.5447684841594476, 2.5560143792570518, 2.5306671014168773, 2.5412865914343223, 2.5388612105145634, 2.546518759187219, 2.553888016342138, 2.5522152610208795, 2.5494289194617563, 2.541538817150448, 2.5654670295652693, 2.5422190887783156, 2.543748834450257, 2.546343708664717, 2.548960927671987, 2.5488436175097386, 2.549291962669009, 2.549415169482552, 2.5499423415398557, 2.5533090131036165, 2.5708058444149975, 2.552890673059548, 2.564881214367345, 2.564263464782038, 2.5798599790469767, 2.5668712128167863, 2.5929427984704327, 2.582924431376465, 2.574809716840096, 2.5897641874886497, 2.594797202519008, 2.584002263048795, 2.5695046082701785, 2.5746562336074503, 2.5785536390220005, 2.5679020220031488, 2.57877292617397, 2.5698114846923277, 2.592112563905262, 2.5902131860479347, 2.593134533204077, 2.600728506329416, 2.5882950040507198, 2.591902107636525, 2.5896116258279833, 2.5952517598720606, 2.596775531377307, 2.598616372188324, 2.5953806049522314, 2.611157941113552, 2.60518257058117, 2.6038175012873506, 2.6099000697456947, 2.612350002689706, 2.6148768262127153, 2.6104476248297983, 2.634889660210445, 2.6297287893999974, 2.6374518851732778, 2.634479694961523, 2.6200642151198363, 2.6337549365408512, 2.63492015743099, 2.6257806462411613, 2.6495293228105568, 2.6323683845194297, 2.6417847192541917, 2.6327790681560246, 2.6355475882199793, 2.6303675515311107, 2.641010553770269, 2.6324673391915305, 2.6432128191385753, 2.642974290941736, 2.653854276550619, 2.652584642808034, 2.6468459722052264, 2.6503725686096793, 2.6574992628520344, 2.6452304064150907, 2.6496587210688096, 2.6654512279335107, 2.66274436980437, 2.6639654804724584, 2.675757695105667, 2.6656050310150547, 2.6712557271196338, 2.6790810899781476, 2.6709074425971373, 2.6802163308085674, 2.688769105619985, 2.6912498039565063, 2.6993841107059975, 2.690121810424504, 2.6761229688115113, 2.687185554081583, 2.692848075199597, 2.679587766650471, 2.7096373882199742, 2.6859887405765077, 2.701374765687388, 2.708306673516585, 2.687893286910159], 'val_acc': [0.052545155987314796, 0.09359605861170148, 0.09031198645998496, 0.06568144498567276, 0.07060755336005699, 0.07717569755949998, 0.07060755296244801, 0.06896551673978028, 0.10016420340449939, 0.1362889975505118, 0.1674876842152309, 0.1707717563669474, 0.17241379249174216, 0.16912972024215267, 0.17077175607332848, 0.16748768401948494, 0.17241379239386917, 0.17241379249174216, 0.1773399007682534, 0.16912972014427968, 0.1773399007682534, 0.17405582842079093, 0.17569786464345866, 0.17405582842079093, 0.17569786454558567, 0.17569786454558567, 0.1773399007682534, 0.18390804536530536, 0.1773399007682534, 0.18390804516955941, 0.1806239730178429, 0.18226600924051062, 0.18062397291996993, 0.1954022981409956, 0.1806239730178429, 0.18883415364181663, 0.18390804516955941, 0.1839080452674324, 0.18555008139222715, 0.18883415364181663, 0.18719211761489485, 0.17898193679517518, 0.18390804516955941, 0.19704433416791736, 0.18226600904476467, 0.19211822579353313, 0.1986863702927121, 0.19047618976661138, 0.19047618976661138, 0.19211822579353313, 0.18883415373968962, 0.20032840641750688, 0.18883415364181663, 0.1904761896687384, 0.19376026201620086, 0.19376026211407385, 0.18883415354394364, 0.1904761896687384, 0.19376026201620086, 0.19376026211407385, 0.19376026191832788, 0.18555008129435416, 0.1871921175170219, 0.20032840641750688, 0.19376026191832788, 0.19047618976661138, 0.19211822589140612, 0.19376026191832788, 0.18883415354394364, 0.16748768461895694, 0.20032840651537984, 0.19376026201620086, 0.1871921174191489, 0.18719211761489485, 0.1822660088490187, 0.19211822589140612, 0.19376026191832788, 0.1871921174191489, 0.19211822589140612, 0.19376026201620086, 0.19047618976661138, 0.19540229804312262, 0.1625615752536088, 0.1871921175170219, 0.1954022981409956, 0.18555008129435416, 0.18883415364181663, 0.18390804516955941, 0.1871921175170219, 0.18883415354394364, 0.1871921174191489, 0.1855500815879731, 0.18226600924051062, 0.17405582822504498, 0.19211822589140612, 0.1904761896687384, 0.18555008119648117, 0.1871921174191489, 0.1904761896687384, 0.18883415383756258, 0.19376026191832788, 0.1871921174191489, 0.18390804516955941, 0.18226600904476467, 0.17733990037676148, 0.1871921173212759, 0.1871921174191489, 0.18226600904476467, 0.18719211761489485, 0.1839080452674324, 0.19211822589140612, 0.1871921175170219, 0.1871921174191489, 0.18883415354394364, 0.18226600894689168, 0.1855500814901001, 0.1954022981409956, 0.1871921174191489, 0.1773399013677254, 0.19376026182045492, 0.18226600875114574, 0.19211822589140612, 0.18719211722340295, 0.1904761895708654, 0.19211822559778718, 0.1871921174191489, 0.16912972084162467, 0.1855500820895721, 0.15927750399498322, 0.18390804516955941, 0.19868637019483915, 0.19868637019483915, 0.17405582822504498, 0.1904761896687384, 0.19211822559778718, 0.18555008109860818, 0.18390804516955941, 0.17898193749252014, 0.19211822589140612, 0.18062397282209694, 0.18390804507168643, 0.1904761896687384, 0.18226600875114574, 0.17241379309121416, 0.1773399014655984, 0.17241379289546818, 0.18555008119648117, 0.18062397282209694, 0.17733990067038044, 0.1822660088490187, 0.18390804487594048, 0.1871921173212759, 0.1773399014655984, 0.18062397272422395, 0.17733990047463447, 0.1789819366973022, 0.18226600875114574, 0.17569786534080364, 0.18555008139222715, 0.18062397342156894, 0.17733990047463447, 0.17898193650155622, 0.18390804497381344, 0.18226600894689168, 0.1707717569664194, 0.16420361227149446, 0.17405582822504498, 0.17241379309121416, 0.1773399013677254, 0.1773399007682534, 0.17569786425196673, 0.18062397282209694, 0.17569786434983972, 0.17733990126985244, 0.18226600894689168, 0.17898193749252014, 0.17898193650155622, 0.1806239730178429, 0.17405582822504498, 0.1740558291181359, 0.1839080452674324, 0.17077175686854643, 0.1691297206458787, 0.1855500814901001, 0.17077175677067344, 0.16912971975278776, 0.1707717569664194, 0.16912972084162467, 0.17405582832291797, 0.17569786425196673, 0.17405582822504498, 0.17733990037676148, 0.17241379200237725, 0.17241379210025023, 0.1707717558775825, 0.1707717558775825, 0.174055828127172, 0.17241379219812322, 0.17241379289546818, 0.16912972084162467, 0.17405582822504498, 0.16748768461895694, 0.17241379309121416, 0.17405582822504498, 0.1740558292160089, 0.18226600894689168, 0.1740558292160089, 0.1658456483962892, 0.16748768471682993, 0.1789819365994292, 0.1707717569664194, 0.16912971985066075, 0.16748768461895694, 0.1691297206458787, 0.16748768471682993, 0.17241379299334117, 0.16912972074375168, 0.1658456484941622, 0.16748768461895694, 0.16091954021765092, 0.17077175686854643, 0.17077175686854643, 0.16748768471682993, 0.16584564859203518, 0.16912971985066075, 0.17405582822504498, 0.17241379210025023, 0.17405582822504498, 0.180623972626351, 0.17569786425196673, 0.16584564750319827, 0.15435139562059896, 0.16584564859203518, 0.16748768362799302, 0.16420361227149446, 0.16912971975278776, 0.17569786534080364, 0.17569786425196673, 0.174055828127172, 0.16256157614669972, 0.16912972074375168, 0.16748768461895694, 0.1691297206458787, 0.16748768452108395, 0.16584564750319827, 0.1658456483962892, 0.16912972084162467, 0.17077175677067344, 0.1707717569664194, 0.15763546787018845, 0.17077175686854643, 0.16420361217362148, 0.17077175677067344, 0.1658456484941622, 0.16748768461895694, 0.16748768452108395, 0.17898193759039313, 0.1658456483962892, 0.16912971975278776, 0.15927750399498322, 0.1658456483962892, 0.154351395522726, 0.17241379309121416, 0.16256157614669972, 0.1625615762445727, 0.15435139562059896, 0.17241379200237725, 0.16420361227149446, 0.1658456484941622, 0.17241379289546818, 0.16912972084162467, 0.17241379309121416, 0.17077175686854643, 0.15599343164752075, 0.15927750399498322, 0.1658456483962892, 0.1444991788718305, 0.15270935930005827, 0.16584564829841622, 0.16256157614669972, 0.17241379309121416, 0.17241379299334117, 0.15599343164752075, 0.16748768461895694, 0.160919539924032, 0.16748768471682993, 0.16584564859203518, 0.16748768452108395, 0.16091954011977797, 0.15927750379923725, 0.15763546796806144, 0.16420361236936745, 0.1625615762445727, 0.1707717569664194, 0.16420361227149446, 0.16256157614669972, 0.16748768461895694, 0.1658456483962892, 0.1576354677723155, 0.16420361227149446], 'loss': [2.7017383547044633, 2.688935970329896, 2.680691647774385, 2.673481377145348, 2.6669090654815735, 2.659303565779261, 2.650884079394644, 2.6411905264217994, 2.630152165865262, 2.6164644340225314, 2.6012522327336933, 2.585890514600938, 2.5715547250281614, 2.5582118894040464, 2.5518802433288075, 2.5402959703175196, 2.5322487890842758, 2.5249179002930253, 2.520109257609937, 2.51179089898691, 2.5081063555496184, 2.5015732199259606, 2.4958169698225645, 2.4916025021727326, 2.4881744461137902, 2.485176099741973, 2.4820529715481237, 2.4801392384867893, 2.475940230348027, 2.4763473450036018, 2.474412812638332, 2.4703494609503776, 2.4695322216903404, 2.4699623793302377, 2.4696506395477043, 2.4650614253794143, 2.464011683943825, 2.4610598147037828, 2.4597012007750525, 2.4589496083817686, 2.4573355615016617, 2.4571180058700595, 2.457003614691983, 2.4538694403254766, 2.4523692317077512, 2.4539522349222485, 2.4551311804283817, 2.4529726833288676, 2.4497156940201714, 2.446996487337461, 2.445789731846208, 2.445412723043861, 2.4472295394913126, 2.4425433563255923, 2.4421802713640908, 2.4414580428380006, 2.4388029386375476, 2.4409336343683012, 2.4372851415826067, 2.437961151173962, 2.4382658274021973, 2.435210248430162, 2.43407597169739, 2.4333314194571556, 2.4362218890102003, 2.4311885843531553, 2.4290429565696012, 2.4287898667783954, 2.430893675355696, 2.434046421990992, 2.428909309982519, 2.4261555093025033, 2.4263914023091906, 2.426569952759165, 2.429293612728863, 2.431113678522913, 2.423761691498805, 2.4203651094338734, 2.4193414593134572, 2.4165653009434256, 2.4177551329258287, 2.427751663039597, 2.4251471630112102, 2.432519820191777, 2.4191522430835075, 2.4188886778555365, 2.410705848494105, 2.4099153984743467, 2.4121860365113683, 2.408016612642355, 2.410482973925142, 2.4108180286947953, 2.419212895790899, 2.4215026409963807, 2.4132949409054048, 2.4085486217201124, 2.404411023745057, 2.407046120759153, 2.4023740131017854, 2.4020859477456344, 2.411348376675553, 2.401765794186132, 2.3981039195090106, 2.4021051188513973, 2.397678450245632, 2.3941781608965362, 2.390383102467907, 2.388247896170959, 2.387349392599149, 2.387930467241354, 2.385476425100401, 2.386725491517868, 2.387290864118071, 2.3829431454503807, 2.3834499326819514, 2.3821329681780306, 2.3783904073664295, 2.379760499264915, 2.385874183466792, 2.381699018654637, 2.3771417101795422, 2.3763464048413034, 2.3701681716730953, 2.3715750559154722, 2.3755724372315457, 2.370840998107158, 2.369073190728252, 2.379597995217576, 2.385096291888666, 2.381441898561356, 2.3692810110487734, 2.365798598001625, 2.3611543110019126, 2.366660273001669, 2.3647395125159982, 2.3547748969076108, 2.351986458561014, 2.352994122201657, 2.3533085515611716, 2.350864755740156, 2.3489925540203433, 2.3473278580260226, 2.349297721968539, 2.347293223786403, 2.354105961445176, 2.349579610765837, 2.3439952506666555, 2.3601363662821555, 2.36506522959997, 2.361473703286486, 2.3464388553611553, 2.3355598434040923, 2.3358444432213568, 2.3325372805585607, 2.333707429936779, 2.332688457716172, 2.337180048631202, 2.3389929764569417, 2.329806694798401, 2.3268856881825095, 2.33077348282205, 2.323805854648535, 2.3225393868080153, 2.321760659638861, 2.3215991450041473, 2.3336937883796143, 2.334377666861125, 2.3330270788752814, 2.323379238626061, 2.3169793299336208, 2.3123293896230583, 2.3117238898541648, 2.3130223125402933, 2.3112808396928854, 2.309130446083492, 2.308974915314504, 2.311326765083924, 2.311518982842228, 2.306023407667814, 2.30818248597993, 2.305156268329346, 2.3045180369696334, 2.309853073214114, 2.3072437951207405, 2.304180520369042, 2.2953407747789574, 2.291482034405154, 2.2939770875770207, 2.295076336165473, 2.297375387822333, 2.29196533530155, 2.292162166728621, 2.2886013141647745, 2.285900066520644, 2.28345535305736, 2.2850214580246067, 2.2827172335168417, 2.2952046012486766, 2.292421568786339, 2.281593094518297, 2.2767271396315807, 2.275101372642439, 2.2796428481656177, 2.2750504698351914, 2.2716859426831317, 2.279299525117972, 2.2828013146927226, 2.2768088040165835, 2.270319655886421, 2.27559788633421, 2.26429495860419, 2.277278594559468, 2.2796408174463854, 2.2874577753352923, 2.262109761659125, 2.2592745068136915, 2.256780144027616, 2.2592988483225294, 2.2658791638987266, 2.2703634667445503, 2.2672463660857027, 2.2603375758968096, 2.2556303279845378, 2.256387378253976, 2.2552308706287487, 2.255855341564703, 2.252236695847717, 2.263475112895456, 2.258553488886087, 2.265191673302308, 2.2502501477940617, 2.2529367923736574, 2.259466631946133, 2.25599842619847, 2.2657728783648605, 2.25251826554598, 2.248889502460707, 2.2392040292829947, 2.2408066576266434, 2.2450244090885105, 2.229566040176141, 2.2262950119786193, 2.2247473247731735, 2.22558447536257, 2.2339044201790186, 2.234386595367651, 2.227604334750949, 2.228958720495079, 2.2248611772329654, 2.2201020129162674, 2.226199766057228, 2.2185282266605073, 2.220230824061243, 2.2120319887353164, 2.2274244688374796, 2.227695912942749, 2.2212826480121337, 2.211476958214135, 2.215479993428538, 2.2127011615392855, 2.2139294118117503, 2.2168878978282764, 2.2162984683528326, 2.214248032344685, 2.2213900442730474, 2.214179992969521, 2.212965065938491, 2.2039777715592903, 2.2038489186053893, 2.202228839000882, 2.1963440440763438, 2.195848410819835, 2.1976310682003013, 2.1917273792642833, 2.1891448591768863, 2.193016326794634, 2.1947707690252662, 2.1950160567031016, 2.1987064474172415, 2.1858277042788403, 2.1815372162531044, 2.181040758959322, 2.1892739420309204, 2.179720762769789, 2.1738787679456832, 2.1773699398158266, 2.175940913194504, 2.173044566207353, 2.1702021918012866, 2.173563721831085, 2.1921434472963304, 2.190187217469578, 2.17660309509575, 2.172151541367204, 2.180164235915981, 2.1738491203261106, 2.16989863589559, 2.161552577341851, 2.164010112241553, 2.163382950064093], 'acc': [0.07433264863809276, 0.06981519561345083, 0.08788501021186429, 0.07679671492481134, 0.06694045172243147, 0.06652977467807167, 0.062422997723247484, 0.06611909666834916, 0.08049281270413428, 0.09979466051169245, 0.13675564711710755, 0.16139630439712283, 0.16180698242520405, 0.16180698181936629, 0.16262833746551733, 0.15893223827869252, 0.16344969189999284, 0.16303901369444398, 0.16098562579992126, 0.15975359433485498, 0.16550308057782095, 0.16550308018616827, 0.16509240295975114, 0.16427104613864202, 0.17084188902524952, 0.16878850209149981, 0.16960985734599818, 0.17125256603748157, 0.17166324385137774, 0.173305954323657, 0.17166324543634723, 0.17248459949752878, 0.17125256564582889, 0.17043121040968925, 0.17618069825598345, 0.1745379873920515, 0.1728952783089154, 0.17289527788054526, 0.17330595532114745, 0.17494866442264229, 0.1737166323333795, 0.17494866598925307, 0.17412730936397028, 0.1782340861505062, 0.17905544060334044, 0.17659137507238917, 0.17741273128766055, 0.17371663156843284, 0.17535934184488575, 0.17659137585569457, 0.17905544078080807, 0.17823408673798524, 0.17741273148348688, 0.17659137646153233, 0.17659137626570598, 0.17659137487656282, 0.17782340773077226, 0.17659137585569457, 0.17700205445289613, 0.17864476396440235, 0.18357289536288143, 0.17987679640860038, 0.1843942507948474, 0.18069815225057778, 0.17618069708102538, 0.18316221676567987, 0.18193018449894946, 0.18234086190283422, 0.18439425020736835, 0.1798767976019172, 0.18316221754898526, 0.18603696005545112, 0.1848049286271023, 0.18685831709074532, 0.18193018448059073, 0.18357289577289285, 0.18316221674732114, 0.18685831570160216, 0.18316221874230207, 0.18932238299499057, 0.18932238201585883, 0.18234086133371388, 0.18562628445072096, 0.1880903495533021, 0.1876796721126999, 0.18521560661846606, 0.19466119124659279, 0.19507186927467401, 0.19219712612565293, 0.19630390170051332, 0.18891170379195124, 0.19425051264939122, 0.18726899469045644, 0.18151950828838154, 0.19178644735098374, 0.19507186865047751, 0.20123203196075173, 0.1958932244740962, 0.19342915878403602, 0.20410677624801346, 0.2012320331357098, 0.20123203333153616, 0.1983572902008738, 0.2028747425921399, 0.19753593516056053, 0.20123203235240442, 0.20451745406190963, 0.20492813167997945, 0.2049281318941645, 0.209445584695442, 0.20616016355505715, 0.20657084078147425, 0.20492813050502134, 0.2032854212260589, 0.20574948713030414, 0.202053387001065, 0.20574948593698733, 0.2032854216177116, 0.19794661158531354, 0.20451745367025692, 0.20862423024260776, 0.21190965077715487, 0.21314168484303986, 0.21067761754965145, 0.2102669397357553, 0.2172484606137266, 0.20369609943160777, 0.2078028755939472, 0.20246406557990784, 0.20698151898702313, 0.21478439353452325, 0.2156057483790102, 0.211909651578819, 0.2049281314841531, 0.21642710381097618, 0.21889116951939505, 0.22217659024976852, 0.21683778240817772, 0.2193018477065852, 0.2164271055734133, 0.22340862331816302, 0.22135523718607744, 0.22546201354424322, 0.21642710381097618, 0.21519507215008354, 0.2188911704801681, 0.2209445581972232, 0.21560574996397971, 0.20985626231351182, 0.21273100642330592, 0.21273100503416276, 0.22956878635917602, 0.2229979468750513, 0.22340862331816302, 0.22751539949886118, 0.22710472266409676, 0.22628336860291523, 0.2234086240831097, 0.2275154012429396, 0.22997946636387945, 0.22340862470730619, 0.22833675649743795, 0.23039014359029655, 0.22915811036279315, 0.23203285582501296, 0.2205338799733156, 0.22217659105143264, 0.22381930091787414, 0.22792607848771543, 0.2340862413696195, 0.23737166350749483, 0.23162217546537428, 0.23449692057265883, 0.2336755645532138, 0.2365503082713552, 0.23490759699741184, 0.24147843870906124, 0.23244353269649481, 0.2361396298516213, 0.2316221766586911, 0.2377823409297383, 0.22710472344740215, 0.23285420951290053, 0.2283367563016116, 0.23531827696539784, 0.2406570842378683, 0.2427104733073491, 0.2365503090546606, 0.2369609846961082, 0.23367556394737604, 0.24188911529292317, 0.23655030807552885, 0.24147843888652887, 0.24271047193656467, 0.23737166329330978, 0.2365503078613438, 0.24147844027567203, 0.23613963043910033, 0.23983572880590232, 0.24065708619613177, 0.2443531826046703, 0.23696098489193457, 0.2418891168962514, 0.23901437417560045, 0.24845995839371574, 0.23860369497256112, 0.24681724852727424, 0.24065708619613177, 0.25215605656469137, 0.24435318299632297, 0.24763860296174975, 0.24271047293405512, 0.23942505060035346, 0.2320328538667495, 0.25462012344806834, 0.25297741240666877, 0.2505133474815553, 0.24928131501899853, 0.2447638598310874, 0.24887063601178555, 0.24353182736853066, 0.24887063816587537, 0.24722792694700818, 0.24065708404204195, 0.2542094466133039, 0.2447638614344156, 0.25338809235629606, 0.24312115014211352, 0.24271047134908563, 0.2439425059657322, 0.25297741160500464, 0.2422997955301704, 0.24599589407444, 0.25051334824650195, 0.24312115131707163, 0.24681724772561012, 0.24681724987969997, 0.2542094460258249, 0.25503080126196453, 0.24804928097147227, 0.25544147868420797, 0.2583162220106967, 0.2603696106885248, 0.2640657082720214, 0.2636550330038678, 0.26283367381448375, 0.260780285919961, 0.2607802873274629, 0.2566735113425911, 0.25995893365793404, 0.2496919898404233, 0.2624229987788739, 0.26365503124143064, 0.26694044940770284, 0.25954825623569056, 0.25174537972992694, 0.25626283411617395, 0.26858316246244207, 0.2640657094653382, 0.26817248601933036, 0.2628336766111288, 0.25831622181487035, 0.25995893130801784, 0.2616016431510816, 0.2632443547983189, 0.26119096709962253, 0.2665297745679193, 0.2636550324163887, 0.2685831614465929, 0.26899383769387825, 0.26735112976734154, 0.27022587152721944, 0.266529774372093, 0.2755646805621271, 0.2763860368141159, 0.27679671462801203, 0.26694045161686886, 0.27063655016113847, 0.26899383965214174, 0.2788501046766246, 0.2759753609584832, 0.27967145873780613, 0.277618070059978, 0.27433264970289856, 0.2755646805988445, 0.2882956869181177, 0.28049281080400673, 0.28008213534002674, 0.2784394227136577, 0.27885010131085924, 0.2714579059847571, 0.2718685831744568, 0.2788501027183611, 0.2780287480697005, 0.2731006166528627, 0.2809034898295784, 0.2804928143655985, 0.280903492375321, 0.282956879504897, 0.28213552382202856]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
