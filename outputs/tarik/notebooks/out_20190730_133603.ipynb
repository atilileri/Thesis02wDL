{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf25.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 13:36:03 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '2', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['mb', 'ce', 'sk', 'ib', 'my', 'eg', 'sg', 'ck', 'ds', 'by', 'eo', 'ek', 'eb', 'yd', 'aa'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001BDC78FD240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001BDC4086EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7055, Accuracy:0.0747, Validation Loss:2.6982, Validation Accuracy:0.0952\n",
    "Epoch #2: Loss:2.6945, Accuracy:0.0891, Validation Loss:2.6896, Validation Accuracy:0.0887\n",
    "Epoch #3: Loss:2.6865, Accuracy:0.0891, Validation Loss:2.6830, Validation Accuracy:0.0887\n",
    "Epoch #4: Loss:2.6805, Accuracy:0.0891, Validation Loss:2.6770, Validation Accuracy:0.0887\n",
    "Epoch #5: Loss:2.6745, Accuracy:0.0891, Validation Loss:2.6708, Validation Accuracy:0.0887\n",
    "Epoch #6: Loss:2.6687, Accuracy:0.0908, Validation Loss:2.6647, Validation Accuracy:0.0870\n",
    "Epoch #7: Loss:2.6625, Accuracy:0.0945, Validation Loss:2.6577, Validation Accuracy:0.0936\n",
    "Epoch #8: Loss:2.6552, Accuracy:0.0994, Validation Loss:2.6499, Validation Accuracy:0.1067\n",
    "Epoch #9: Loss:2.6467, Accuracy:0.1088, Validation Loss:2.6400, Validation Accuracy:0.1117\n",
    "Epoch #10: Loss:2.6366, Accuracy:0.1105, Validation Loss:2.6271, Validation Accuracy:0.1199\n",
    "Epoch #11: Loss:2.6238, Accuracy:0.1244, Validation Loss:2.6127, Validation Accuracy:0.1363\n",
    "Epoch #12: Loss:2.6083, Accuracy:0.1359, Validation Loss:2.5963, Validation Accuracy:0.1478\n",
    "Epoch #13: Loss:2.5907, Accuracy:0.1417, Validation Loss:2.5851, Validation Accuracy:0.1429\n",
    "Epoch #14: Loss:2.5862, Accuracy:0.1339, Validation Loss:2.5681, Validation Accuracy:0.1527\n",
    "Epoch #15: Loss:2.5676, Accuracy:0.1544, Validation Loss:2.5460, Validation Accuracy:0.1560\n",
    "Epoch #16: Loss:2.5438, Accuracy:0.1589, Validation Loss:2.5346, Validation Accuracy:0.1576\n",
    "Epoch #17: Loss:2.5339, Accuracy:0.1540, Validation Loss:2.5257, Validation Accuracy:0.1527\n",
    "Epoch #18: Loss:2.5224, Accuracy:0.1532, Validation Loss:2.5141, Validation Accuracy:0.1593\n",
    "Epoch #19: Loss:2.5120, Accuracy:0.1671, Validation Loss:2.4968, Validation Accuracy:0.1478\n",
    "Epoch #20: Loss:2.5056, Accuracy:0.1680, Validation Loss:2.4905, Validation Accuracy:0.1593\n",
    "Epoch #21: Loss:2.4986, Accuracy:0.1643, Validation Loss:2.4874, Validation Accuracy:0.1675\n",
    "Epoch #22: Loss:2.4940, Accuracy:0.1663, Validation Loss:2.4965, Validation Accuracy:0.1626\n",
    "Epoch #23: Loss:2.5179, Accuracy:0.1643, Validation Loss:2.5810, Validation Accuracy:0.1626\n",
    "Epoch #24: Loss:2.5797, Accuracy:0.1495, Validation Loss:2.5947, Validation Accuracy:0.1478\n",
    "Epoch #25: Loss:2.5551, Accuracy:0.1565, Validation Loss:2.4919, Validation Accuracy:0.1560\n",
    "Epoch #26: Loss:2.4999, Accuracy:0.1634, Validation Loss:2.5048, Validation Accuracy:0.1691\n",
    "Epoch #27: Loss:2.5139, Accuracy:0.1548, Validation Loss:2.4878, Validation Accuracy:0.1741\n",
    "Epoch #28: Loss:2.4889, Accuracy:0.1676, Validation Loss:2.4822, Validation Accuracy:0.1691\n",
    "Epoch #29: Loss:2.4881, Accuracy:0.1647, Validation Loss:2.4849, Validation Accuracy:0.1691\n",
    "Epoch #30: Loss:2.4834, Accuracy:0.1684, Validation Loss:2.4713, Validation Accuracy:0.1658\n",
    "Epoch #31: Loss:2.4770, Accuracy:0.1696, Validation Loss:2.4687, Validation Accuracy:0.1658\n",
    "Epoch #32: Loss:2.4733, Accuracy:0.1671, Validation Loss:2.4662, Validation Accuracy:0.1691\n",
    "Epoch #33: Loss:2.4687, Accuracy:0.1684, Validation Loss:2.4681, Validation Accuracy:0.1658\n",
    "Epoch #34: Loss:2.4674, Accuracy:0.1692, Validation Loss:2.4653, Validation Accuracy:0.1642\n",
    "Epoch #35: Loss:2.4653, Accuracy:0.1692, Validation Loss:2.4636, Validation Accuracy:0.1609\n",
    "Epoch #36: Loss:2.4637, Accuracy:0.1717, Validation Loss:2.4632, Validation Accuracy:0.1642\n",
    "Epoch #37: Loss:2.4639, Accuracy:0.1733, Validation Loss:2.4633, Validation Accuracy:0.1593\n",
    "Epoch #38: Loss:2.4626, Accuracy:0.1700, Validation Loss:2.4620, Validation Accuracy:0.1560\n",
    "Epoch #39: Loss:2.4609, Accuracy:0.1676, Validation Loss:2.4636, Validation Accuracy:0.1626\n",
    "Epoch #40: Loss:2.4607, Accuracy:0.1655, Validation Loss:2.4629, Validation Accuracy:0.1593\n",
    "Epoch #41: Loss:2.4594, Accuracy:0.1651, Validation Loss:2.4628, Validation Accuracy:0.1626\n",
    "Epoch #42: Loss:2.4587, Accuracy:0.1667, Validation Loss:2.4625, Validation Accuracy:0.1658\n",
    "Epoch #43: Loss:2.4579, Accuracy:0.1680, Validation Loss:2.4629, Validation Accuracy:0.1724\n",
    "Epoch #44: Loss:2.4576, Accuracy:0.1671, Validation Loss:2.4629, Validation Accuracy:0.1724\n",
    "Epoch #45: Loss:2.4573, Accuracy:0.1680, Validation Loss:2.4619, Validation Accuracy:0.1658\n",
    "Epoch #46: Loss:2.4560, Accuracy:0.1696, Validation Loss:2.4629, Validation Accuracy:0.1609\n",
    "Epoch #47: Loss:2.4552, Accuracy:0.1659, Validation Loss:2.4613, Validation Accuracy:0.1593\n",
    "Epoch #48: Loss:2.4541, Accuracy:0.1684, Validation Loss:2.4603, Validation Accuracy:0.1642\n",
    "Epoch #49: Loss:2.4541, Accuracy:0.1676, Validation Loss:2.4612, Validation Accuracy:0.1691\n",
    "Epoch #50: Loss:2.4534, Accuracy:0.1659, Validation Loss:2.4607, Validation Accuracy:0.1675\n",
    "Epoch #51: Loss:2.4531, Accuracy:0.1667, Validation Loss:2.4595, Validation Accuracy:0.1675\n",
    "Epoch #52: Loss:2.4527, Accuracy:0.1659, Validation Loss:2.4589, Validation Accuracy:0.1626\n",
    "Epoch #53: Loss:2.4519, Accuracy:0.1676, Validation Loss:2.4596, Validation Accuracy:0.1642\n",
    "Epoch #54: Loss:2.4518, Accuracy:0.1676, Validation Loss:2.4601, Validation Accuracy:0.1609\n",
    "Epoch #55: Loss:2.4507, Accuracy:0.1643, Validation Loss:2.4594, Validation Accuracy:0.1658\n",
    "Epoch #56: Loss:2.4515, Accuracy:0.1700, Validation Loss:2.4591, Validation Accuracy:0.1675\n",
    "Epoch #57: Loss:2.4493, Accuracy:0.1700, Validation Loss:2.4584, Validation Accuracy:0.1642\n",
    "Epoch #58: Loss:2.4491, Accuracy:0.1688, Validation Loss:2.4591, Validation Accuracy:0.1642\n",
    "Epoch #59: Loss:2.4482, Accuracy:0.1708, Validation Loss:2.4573, Validation Accuracy:0.1642\n",
    "Epoch #60: Loss:2.4469, Accuracy:0.1713, Validation Loss:2.4579, Validation Accuracy:0.1642\n",
    "Epoch #61: Loss:2.4478, Accuracy:0.1713, Validation Loss:2.4584, Validation Accuracy:0.1609\n",
    "Epoch #62: Loss:2.4467, Accuracy:0.1708, Validation Loss:2.4564, Validation Accuracy:0.1658\n",
    "Epoch #63: Loss:2.4471, Accuracy:0.1708, Validation Loss:2.4570, Validation Accuracy:0.1658\n",
    "Epoch #64: Loss:2.4468, Accuracy:0.1680, Validation Loss:2.4587, Validation Accuracy:0.1658\n",
    "Epoch #65: Loss:2.4453, Accuracy:0.1688, Validation Loss:2.4563, Validation Accuracy:0.1741\n",
    "Epoch #66: Loss:2.4454, Accuracy:0.1737, Validation Loss:2.4571, Validation Accuracy:0.1691\n",
    "Epoch #67: Loss:2.4455, Accuracy:0.1704, Validation Loss:2.4580, Validation Accuracy:0.1658\n",
    "Epoch #68: Loss:2.4451, Accuracy:0.1688, Validation Loss:2.4562, Validation Accuracy:0.1626\n",
    "Epoch #69: Loss:2.4464, Accuracy:0.1729, Validation Loss:2.4558, Validation Accuracy:0.1609\n",
    "Epoch #70: Loss:2.4451, Accuracy:0.1684, Validation Loss:2.4582, Validation Accuracy:0.1741\n",
    "Epoch #71: Loss:2.4448, Accuracy:0.1688, Validation Loss:2.4544, Validation Accuracy:0.1626\n",
    "Epoch #72: Loss:2.4456, Accuracy:0.1721, Validation Loss:2.4564, Validation Accuracy:0.1609\n",
    "Epoch #73: Loss:2.4442, Accuracy:0.1700, Validation Loss:2.4578, Validation Accuracy:0.1741\n",
    "Epoch #74: Loss:2.4452, Accuracy:0.1749, Validation Loss:2.4563, Validation Accuracy:0.1675\n",
    "Epoch #75: Loss:2.4441, Accuracy:0.1680, Validation Loss:2.4553, Validation Accuracy:0.1642\n",
    "Epoch #76: Loss:2.4432, Accuracy:0.1671, Validation Loss:2.4571, Validation Accuracy:0.1757\n",
    "Epoch #77: Loss:2.4440, Accuracy:0.1733, Validation Loss:2.4588, Validation Accuracy:0.1724\n",
    "Epoch #78: Loss:2.4434, Accuracy:0.1688, Validation Loss:2.4576, Validation Accuracy:0.1658\n",
    "Epoch #79: Loss:2.4423, Accuracy:0.1655, Validation Loss:2.4583, Validation Accuracy:0.1724\n",
    "Epoch #80: Loss:2.4428, Accuracy:0.1758, Validation Loss:2.4568, Validation Accuracy:0.1724\n",
    "Epoch #81: Loss:2.4421, Accuracy:0.1733, Validation Loss:2.4566, Validation Accuracy:0.1741\n",
    "Epoch #82: Loss:2.4418, Accuracy:0.1786, Validation Loss:2.4572, Validation Accuracy:0.1757\n",
    "Epoch #83: Loss:2.4419, Accuracy:0.1737, Validation Loss:2.4554, Validation Accuracy:0.1675\n",
    "Epoch #84: Loss:2.4412, Accuracy:0.1700, Validation Loss:2.4554, Validation Accuracy:0.1691\n",
    "Epoch #85: Loss:2.4411, Accuracy:0.1680, Validation Loss:2.4549, Validation Accuracy:0.1675\n",
    "Epoch #86: Loss:2.4408, Accuracy:0.1725, Validation Loss:2.4565, Validation Accuracy:0.1741\n",
    "Epoch #87: Loss:2.4407, Accuracy:0.1786, Validation Loss:2.4555, Validation Accuracy:0.1658\n",
    "Epoch #88: Loss:2.4409, Accuracy:0.1766, Validation Loss:2.4555, Validation Accuracy:0.1691\n",
    "Epoch #89: Loss:2.4405, Accuracy:0.1766, Validation Loss:2.4563, Validation Accuracy:0.1757\n",
    "Epoch #90: Loss:2.4405, Accuracy:0.1713, Validation Loss:2.4568, Validation Accuracy:0.1626\n",
    "Epoch #91: Loss:2.4412, Accuracy:0.1692, Validation Loss:2.4554, Validation Accuracy:0.1741\n",
    "Epoch #92: Loss:2.4408, Accuracy:0.1721, Validation Loss:2.4544, Validation Accuracy:0.1708\n",
    "Epoch #93: Loss:2.4411, Accuracy:0.1754, Validation Loss:2.4561, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4406, Accuracy:0.1770, Validation Loss:2.4545, Validation Accuracy:0.1773\n",
    "Epoch #95: Loss:2.4405, Accuracy:0.1770, Validation Loss:2.4550, Validation Accuracy:0.1757\n",
    "Epoch #96: Loss:2.4389, Accuracy:0.1770, Validation Loss:2.4545, Validation Accuracy:0.1724\n",
    "Epoch #97: Loss:2.4399, Accuracy:0.1795, Validation Loss:2.4557, Validation Accuracy:0.1675\n",
    "Epoch #98: Loss:2.4394, Accuracy:0.1799, Validation Loss:2.4560, Validation Accuracy:0.1626\n",
    "Epoch #99: Loss:2.4396, Accuracy:0.1786, Validation Loss:2.4555, Validation Accuracy:0.1626\n",
    "Epoch #100: Loss:2.4394, Accuracy:0.1791, Validation Loss:2.4555, Validation Accuracy:0.1626\n",
    "Epoch #101: Loss:2.4399, Accuracy:0.1737, Validation Loss:2.4559, Validation Accuracy:0.1609\n",
    "Epoch #102: Loss:2.4391, Accuracy:0.1782, Validation Loss:2.4556, Validation Accuracy:0.1626\n",
    "Epoch #103: Loss:2.4393, Accuracy:0.1786, Validation Loss:2.4556, Validation Accuracy:0.1642\n",
    "Epoch #104: Loss:2.4395, Accuracy:0.1778, Validation Loss:2.4555, Validation Accuracy:0.1642\n",
    "Epoch #105: Loss:2.4391, Accuracy:0.1754, Validation Loss:2.4550, Validation Accuracy:0.1675\n",
    "Epoch #106: Loss:2.4388, Accuracy:0.1819, Validation Loss:2.4566, Validation Accuracy:0.1675\n",
    "Epoch #107: Loss:2.4384, Accuracy:0.1819, Validation Loss:2.4552, Validation Accuracy:0.1642\n",
    "Epoch #108: Loss:2.4385, Accuracy:0.1786, Validation Loss:2.4548, Validation Accuracy:0.1658\n",
    "Epoch #109: Loss:2.4380, Accuracy:0.1786, Validation Loss:2.4544, Validation Accuracy:0.1691\n",
    "Epoch #110: Loss:2.4388, Accuracy:0.1811, Validation Loss:2.4539, Validation Accuracy:0.1609\n",
    "Epoch #111: Loss:2.4391, Accuracy:0.1766, Validation Loss:2.4557, Validation Accuracy:0.1691\n",
    "Epoch #112: Loss:2.4437, Accuracy:0.1762, Validation Loss:2.4573, Validation Accuracy:0.1691\n",
    "Epoch #113: Loss:2.4429, Accuracy:0.1758, Validation Loss:2.4552, Validation Accuracy:0.1658\n",
    "Epoch #114: Loss:2.4436, Accuracy:0.1717, Validation Loss:2.4537, Validation Accuracy:0.1675\n",
    "Epoch #115: Loss:2.4414, Accuracy:0.1791, Validation Loss:2.4557, Validation Accuracy:0.1658\n",
    "Epoch #116: Loss:2.4399, Accuracy:0.1786, Validation Loss:2.4535, Validation Accuracy:0.1626\n",
    "Epoch #117: Loss:2.4394, Accuracy:0.1799, Validation Loss:2.4555, Validation Accuracy:0.1642\n",
    "Epoch #118: Loss:2.4407, Accuracy:0.1766, Validation Loss:2.4533, Validation Accuracy:0.1642\n",
    "Epoch #119: Loss:2.4397, Accuracy:0.1729, Validation Loss:2.4553, Validation Accuracy:0.1675\n",
    "Epoch #120: Loss:2.4414, Accuracy:0.1762, Validation Loss:2.4529, Validation Accuracy:0.1691\n",
    "Epoch #121: Loss:2.4408, Accuracy:0.1729, Validation Loss:2.4532, Validation Accuracy:0.1642\n",
    "Epoch #122: Loss:2.4389, Accuracy:0.1770, Validation Loss:2.4581, Validation Accuracy:0.1675\n",
    "Epoch #123: Loss:2.4394, Accuracy:0.1774, Validation Loss:2.4538, Validation Accuracy:0.1658\n",
    "Epoch #124: Loss:2.4402, Accuracy:0.1782, Validation Loss:2.4545, Validation Accuracy:0.1642\n",
    "Epoch #125: Loss:2.4397, Accuracy:0.1803, Validation Loss:2.4525, Validation Accuracy:0.1626\n",
    "Epoch #126: Loss:2.4384, Accuracy:0.1725, Validation Loss:2.4533, Validation Accuracy:0.1642\n",
    "Epoch #127: Loss:2.4381, Accuracy:0.1778, Validation Loss:2.4532, Validation Accuracy:0.1658\n",
    "Epoch #128: Loss:2.4381, Accuracy:0.1815, Validation Loss:2.4535, Validation Accuracy:0.1658\n",
    "Epoch #129: Loss:2.4383, Accuracy:0.1848, Validation Loss:2.4531, Validation Accuracy:0.1642\n",
    "Epoch #130: Loss:2.4386, Accuracy:0.1774, Validation Loss:2.4541, Validation Accuracy:0.1642\n",
    "Epoch #131: Loss:2.4383, Accuracy:0.1799, Validation Loss:2.4532, Validation Accuracy:0.1609\n",
    "Epoch #132: Loss:2.4382, Accuracy:0.1786, Validation Loss:2.4527, Validation Accuracy:0.1658\n",
    "Epoch #133: Loss:2.4384, Accuracy:0.1791, Validation Loss:2.4532, Validation Accuracy:0.1708\n",
    "Epoch #134: Loss:2.4376, Accuracy:0.1791, Validation Loss:2.4520, Validation Accuracy:0.1675\n",
    "Epoch #135: Loss:2.4382, Accuracy:0.1774, Validation Loss:2.4528, Validation Accuracy:0.1626\n",
    "Epoch #136: Loss:2.4376, Accuracy:0.1807, Validation Loss:2.4523, Validation Accuracy:0.1658\n",
    "Epoch #137: Loss:2.4375, Accuracy:0.1811, Validation Loss:2.4524, Validation Accuracy:0.1658\n",
    "Epoch #138: Loss:2.4385, Accuracy:0.1832, Validation Loss:2.4541, Validation Accuracy:0.1691\n",
    "Epoch #139: Loss:2.4409, Accuracy:0.1815, Validation Loss:2.4528, Validation Accuracy:0.1642\n",
    "Epoch #140: Loss:2.4409, Accuracy:0.1704, Validation Loss:2.4524, Validation Accuracy:0.1642\n",
    "Epoch #141: Loss:2.4381, Accuracy:0.1819, Validation Loss:2.4530, Validation Accuracy:0.1658\n",
    "Epoch #142: Loss:2.4376, Accuracy:0.1823, Validation Loss:2.4524, Validation Accuracy:0.1626\n",
    "Epoch #143: Loss:2.4372, Accuracy:0.1786, Validation Loss:2.4531, Validation Accuracy:0.1724\n",
    "Epoch #144: Loss:2.4375, Accuracy:0.1795, Validation Loss:2.4522, Validation Accuracy:0.1609\n",
    "Epoch #145: Loss:2.4388, Accuracy:0.1778, Validation Loss:2.4521, Validation Accuracy:0.1609\n",
    "Epoch #146: Loss:2.4368, Accuracy:0.1836, Validation Loss:2.4541, Validation Accuracy:0.1658\n",
    "Epoch #147: Loss:2.4373, Accuracy:0.1807, Validation Loss:2.4520, Validation Accuracy:0.1609\n",
    "Epoch #148: Loss:2.4375, Accuracy:0.1811, Validation Loss:2.4525, Validation Accuracy:0.1642\n",
    "Epoch #149: Loss:2.4371, Accuracy:0.1828, Validation Loss:2.4522, Validation Accuracy:0.1642\n",
    "Epoch #150: Loss:2.4384, Accuracy:0.1807, Validation Loss:2.4541, Validation Accuracy:0.1642\n",
    "Epoch #151: Loss:2.4369, Accuracy:0.1791, Validation Loss:2.4529, Validation Accuracy:0.1626\n",
    "Epoch #152: Loss:2.4394, Accuracy:0.1791, Validation Loss:2.4548, Validation Accuracy:0.1691\n",
    "Epoch #153: Loss:2.4371, Accuracy:0.1836, Validation Loss:2.4522, Validation Accuracy:0.1609\n",
    "Epoch #154: Loss:2.4379, Accuracy:0.1807, Validation Loss:2.4535, Validation Accuracy:0.1691\n",
    "Epoch #155: Loss:2.4368, Accuracy:0.1725, Validation Loss:2.4524, Validation Accuracy:0.1609\n",
    "Epoch #156: Loss:2.4375, Accuracy:0.1770, Validation Loss:2.4536, Validation Accuracy:0.1642\n",
    "Epoch #157: Loss:2.4360, Accuracy:0.1799, Validation Loss:2.4521, Validation Accuracy:0.1609\n",
    "Epoch #158: Loss:2.4361, Accuracy:0.1823, Validation Loss:2.4533, Validation Accuracy:0.1642\n",
    "Epoch #159: Loss:2.4363, Accuracy:0.1823, Validation Loss:2.4521, Validation Accuracy:0.1658\n",
    "Epoch #160: Loss:2.4362, Accuracy:0.1782, Validation Loss:2.4525, Validation Accuracy:0.1675\n",
    "Epoch #161: Loss:2.4358, Accuracy:0.1807, Validation Loss:2.4522, Validation Accuracy:0.1675\n",
    "Epoch #162: Loss:2.4359, Accuracy:0.1823, Validation Loss:2.4518, Validation Accuracy:0.1675\n",
    "Epoch #163: Loss:2.4355, Accuracy:0.1811, Validation Loss:2.4521, Validation Accuracy:0.1675\n",
    "Epoch #164: Loss:2.4356, Accuracy:0.1819, Validation Loss:2.4527, Validation Accuracy:0.1675\n",
    "Epoch #165: Loss:2.4356, Accuracy:0.1815, Validation Loss:2.4522, Validation Accuracy:0.1658\n",
    "Epoch #166: Loss:2.4349, Accuracy:0.1815, Validation Loss:2.4523, Validation Accuracy:0.1658\n",
    "Epoch #167: Loss:2.4356, Accuracy:0.1819, Validation Loss:2.4529, Validation Accuracy:0.1626\n",
    "Epoch #168: Loss:2.4352, Accuracy:0.1836, Validation Loss:2.4528, Validation Accuracy:0.1675\n",
    "Epoch #169: Loss:2.4364, Accuracy:0.1786, Validation Loss:2.4528, Validation Accuracy:0.1675\n",
    "Epoch #170: Loss:2.4351, Accuracy:0.1795, Validation Loss:2.4530, Validation Accuracy:0.1675\n",
    "Epoch #171: Loss:2.4350, Accuracy:0.1811, Validation Loss:2.4523, Validation Accuracy:0.1609\n",
    "Epoch #172: Loss:2.4348, Accuracy:0.1828, Validation Loss:2.4528, Validation Accuracy:0.1626\n",
    "Epoch #173: Loss:2.4358, Accuracy:0.1828, Validation Loss:2.4526, Validation Accuracy:0.1658\n",
    "Epoch #174: Loss:2.4354, Accuracy:0.1807, Validation Loss:2.4528, Validation Accuracy:0.1626\n",
    "Epoch #175: Loss:2.4353, Accuracy:0.1828, Validation Loss:2.4515, Validation Accuracy:0.1642\n",
    "Epoch #176: Loss:2.4354, Accuracy:0.1782, Validation Loss:2.4537, Validation Accuracy:0.1691\n",
    "Epoch #177: Loss:2.4366, Accuracy:0.1713, Validation Loss:2.4531, Validation Accuracy:0.1626\n",
    "Epoch #178: Loss:2.4351, Accuracy:0.1852, Validation Loss:2.4545, Validation Accuracy:0.1691\n",
    "Epoch #179: Loss:2.4350, Accuracy:0.1807, Validation Loss:2.4524, Validation Accuracy:0.1642\n",
    "Epoch #180: Loss:2.4353, Accuracy:0.1828, Validation Loss:2.4540, Validation Accuracy:0.1642\n",
    "Epoch #181: Loss:2.4355, Accuracy:0.1815, Validation Loss:2.4531, Validation Accuracy:0.1626\n",
    "Epoch #182: Loss:2.4349, Accuracy:0.1832, Validation Loss:2.4523, Validation Accuracy:0.1626\n",
    "Epoch #183: Loss:2.4347, Accuracy:0.1823, Validation Loss:2.4541, Validation Accuracy:0.1642\n",
    "Epoch #184: Loss:2.4359, Accuracy:0.1782, Validation Loss:2.4526, Validation Accuracy:0.1626\n",
    "Epoch #185: Loss:2.4346, Accuracy:0.1745, Validation Loss:2.4553, Validation Accuracy:0.1691\n",
    "Epoch #186: Loss:2.4348, Accuracy:0.1815, Validation Loss:2.4517, Validation Accuracy:0.1658\n",
    "Epoch #187: Loss:2.4344, Accuracy:0.1807, Validation Loss:2.4531, Validation Accuracy:0.1691\n",
    "Epoch #188: Loss:2.4336, Accuracy:0.1811, Validation Loss:2.4517, Validation Accuracy:0.1658\n",
    "Epoch #189: Loss:2.4348, Accuracy:0.1778, Validation Loss:2.4525, Validation Accuracy:0.1626\n",
    "Epoch #190: Loss:2.4377, Accuracy:0.1766, Validation Loss:2.4536, Validation Accuracy:0.1691\n",
    "Epoch #191: Loss:2.4342, Accuracy:0.1758, Validation Loss:2.4535, Validation Accuracy:0.1609\n",
    "Epoch #192: Loss:2.4355, Accuracy:0.1803, Validation Loss:2.4556, Validation Accuracy:0.1691\n",
    "Epoch #193: Loss:2.4354, Accuracy:0.1815, Validation Loss:2.4527, Validation Accuracy:0.1626\n",
    "Epoch #194: Loss:2.4341, Accuracy:0.1807, Validation Loss:2.4534, Validation Accuracy:0.1691\n",
    "Epoch #195: Loss:2.4351, Accuracy:0.1811, Validation Loss:2.4517, Validation Accuracy:0.1609\n",
    "Epoch #196: Loss:2.4338, Accuracy:0.1836, Validation Loss:2.4519, Validation Accuracy:0.1658\n",
    "Epoch #197: Loss:2.4336, Accuracy:0.1828, Validation Loss:2.4529, Validation Accuracy:0.1626\n",
    "Epoch #198: Loss:2.4344, Accuracy:0.1828, Validation Loss:2.4533, Validation Accuracy:0.1626\n",
    "Epoch #199: Loss:2.4336, Accuracy:0.1823, Validation Loss:2.4520, Validation Accuracy:0.1626\n",
    "Epoch #200: Loss:2.4335, Accuracy:0.1856, Validation Loss:2.4541, Validation Accuracy:0.1691\n",
    "Epoch #201: Loss:2.4331, Accuracy:0.1791, Validation Loss:2.4520, Validation Accuracy:0.1626\n",
    "Epoch #202: Loss:2.4336, Accuracy:0.1828, Validation Loss:2.4528, Validation Accuracy:0.1626\n",
    "Epoch #203: Loss:2.4331, Accuracy:0.1782, Validation Loss:2.4521, Validation Accuracy:0.1626\n",
    "Epoch #204: Loss:2.4344, Accuracy:0.1786, Validation Loss:2.4531, Validation Accuracy:0.1626\n",
    "Epoch #205: Loss:2.4337, Accuracy:0.1778, Validation Loss:2.4526, Validation Accuracy:0.1626\n",
    "Epoch #206: Loss:2.4329, Accuracy:0.1844, Validation Loss:2.4543, Validation Accuracy:0.1626\n",
    "Epoch #207: Loss:2.4336, Accuracy:0.1770, Validation Loss:2.4523, Validation Accuracy:0.1658\n",
    "Epoch #208: Loss:2.4339, Accuracy:0.1819, Validation Loss:2.4521, Validation Accuracy:0.1658\n",
    "Epoch #209: Loss:2.4333, Accuracy:0.1803, Validation Loss:2.4541, Validation Accuracy:0.1691\n",
    "Epoch #210: Loss:2.4322, Accuracy:0.1856, Validation Loss:2.4521, Validation Accuracy:0.1626\n",
    "Epoch #211: Loss:2.4331, Accuracy:0.1832, Validation Loss:2.4521, Validation Accuracy:0.1658\n",
    "Epoch #212: Loss:2.4324, Accuracy:0.1823, Validation Loss:2.4531, Validation Accuracy:0.1626\n",
    "Epoch #213: Loss:2.4352, Accuracy:0.1795, Validation Loss:2.4530, Validation Accuracy:0.1658\n",
    "Epoch #214: Loss:2.4336, Accuracy:0.1828, Validation Loss:2.4553, Validation Accuracy:0.1658\n",
    "Epoch #215: Loss:2.4331, Accuracy:0.1860, Validation Loss:2.4524, Validation Accuracy:0.1626\n",
    "Epoch #216: Loss:2.4332, Accuracy:0.1860, Validation Loss:2.4536, Validation Accuracy:0.1675\n",
    "Epoch #217: Loss:2.4337, Accuracy:0.1811, Validation Loss:2.4523, Validation Accuracy:0.1626\n",
    "Epoch #218: Loss:2.4320, Accuracy:0.1836, Validation Loss:2.4535, Validation Accuracy:0.1626\n",
    "Epoch #219: Loss:2.4322, Accuracy:0.1815, Validation Loss:2.4524, Validation Accuracy:0.1658\n",
    "Epoch #220: Loss:2.4320, Accuracy:0.1819, Validation Loss:2.4536, Validation Accuracy:0.1626\n",
    "Epoch #221: Loss:2.4321, Accuracy:0.1823, Validation Loss:2.4528, Validation Accuracy:0.1658\n",
    "Epoch #222: Loss:2.4318, Accuracy:0.1832, Validation Loss:2.4526, Validation Accuracy:0.1642\n",
    "Epoch #223: Loss:2.4319, Accuracy:0.1811, Validation Loss:2.4548, Validation Accuracy:0.1626\n",
    "Epoch #224: Loss:2.4317, Accuracy:0.1823, Validation Loss:2.4533, Validation Accuracy:0.1626\n",
    "Epoch #225: Loss:2.4320, Accuracy:0.1828, Validation Loss:2.4528, Validation Accuracy:0.1626\n",
    "Epoch #226: Loss:2.4317, Accuracy:0.1844, Validation Loss:2.4534, Validation Accuracy:0.1658\n",
    "Epoch #227: Loss:2.4321, Accuracy:0.1832, Validation Loss:2.4526, Validation Accuracy:0.1626\n",
    "Epoch #228: Loss:2.4314, Accuracy:0.1803, Validation Loss:2.4550, Validation Accuracy:0.1626\n",
    "Epoch #229: Loss:2.4321, Accuracy:0.1807, Validation Loss:2.4531, Validation Accuracy:0.1658\n",
    "Epoch #230: Loss:2.4312, Accuracy:0.1811, Validation Loss:2.4534, Validation Accuracy:0.1593\n",
    "Epoch #231: Loss:2.4317, Accuracy:0.1754, Validation Loss:2.4530, Validation Accuracy:0.1626\n",
    "Epoch #232: Loss:2.4349, Accuracy:0.1823, Validation Loss:2.4529, Validation Accuracy:0.1658\n",
    "Epoch #233: Loss:2.4334, Accuracy:0.1778, Validation Loss:2.4558, Validation Accuracy:0.1675\n",
    "Epoch #234: Loss:2.4319, Accuracy:0.1811, Validation Loss:2.4539, Validation Accuracy:0.1658\n",
    "Epoch #235: Loss:2.4331, Accuracy:0.1762, Validation Loss:2.4566, Validation Accuracy:0.1658\n",
    "Epoch #236: Loss:2.4309, Accuracy:0.1819, Validation Loss:2.4534, Validation Accuracy:0.1626\n",
    "Epoch #237: Loss:2.4324, Accuracy:0.1832, Validation Loss:2.4535, Validation Accuracy:0.1658\n",
    "Epoch #238: Loss:2.4304, Accuracy:0.1840, Validation Loss:2.4553, Validation Accuracy:0.1626\n",
    "Epoch #239: Loss:2.4309, Accuracy:0.1848, Validation Loss:2.4535, Validation Accuracy:0.1626\n",
    "Epoch #240: Loss:2.4320, Accuracy:0.1823, Validation Loss:2.4535, Validation Accuracy:0.1527\n",
    "Epoch #241: Loss:2.4328, Accuracy:0.1795, Validation Loss:2.4546, Validation Accuracy:0.1626\n",
    "Epoch #242: Loss:2.4325, Accuracy:0.1815, Validation Loss:2.4533, Validation Accuracy:0.1626\n",
    "Epoch #243: Loss:2.4316, Accuracy:0.1770, Validation Loss:2.4563, Validation Accuracy:0.1675\n",
    "Epoch #244: Loss:2.4305, Accuracy:0.1828, Validation Loss:2.4542, Validation Accuracy:0.1658\n",
    "Epoch #245: Loss:2.4323, Accuracy:0.1844, Validation Loss:2.4556, Validation Accuracy:0.1527\n",
    "Epoch #246: Loss:2.4305, Accuracy:0.1803, Validation Loss:2.4535, Validation Accuracy:0.1658\n",
    "Epoch #247: Loss:2.4308, Accuracy:0.1844, Validation Loss:2.4546, Validation Accuracy:0.1626\n",
    "Epoch #248: Loss:2.4308, Accuracy:0.1791, Validation Loss:2.4537, Validation Accuracy:0.1626\n",
    "Epoch #249: Loss:2.4311, Accuracy:0.1811, Validation Loss:2.4543, Validation Accuracy:0.1658\n",
    "Epoch #250: Loss:2.4299, Accuracy:0.1840, Validation Loss:2.4535, Validation Accuracy:0.1658\n",
    "Epoch #251: Loss:2.4299, Accuracy:0.1848, Validation Loss:2.4546, Validation Accuracy:0.1626\n",
    "Epoch #252: Loss:2.4301, Accuracy:0.1828, Validation Loss:2.4540, Validation Accuracy:0.1593\n",
    "Epoch #253: Loss:2.4296, Accuracy:0.1828, Validation Loss:2.4545, Validation Accuracy:0.1626\n",
    "Epoch #254: Loss:2.4305, Accuracy:0.1828, Validation Loss:2.4539, Validation Accuracy:0.1658\n",
    "Epoch #255: Loss:2.4295, Accuracy:0.1791, Validation Loss:2.4543, Validation Accuracy:0.1658\n",
    "Epoch #256: Loss:2.4296, Accuracy:0.1832, Validation Loss:2.4534, Validation Accuracy:0.1658\n",
    "Epoch #257: Loss:2.4293, Accuracy:0.1803, Validation Loss:2.4552, Validation Accuracy:0.1626\n",
    "Epoch #258: Loss:2.4301, Accuracy:0.1807, Validation Loss:2.4545, Validation Accuracy:0.1658\n",
    "Epoch #259: Loss:2.4295, Accuracy:0.1795, Validation Loss:2.4544, Validation Accuracy:0.1658\n",
    "Epoch #260: Loss:2.4292, Accuracy:0.1840, Validation Loss:2.4539, Validation Accuracy:0.1626\n",
    "Epoch #261: Loss:2.4295, Accuracy:0.1823, Validation Loss:2.4539, Validation Accuracy:0.1658\n",
    "Epoch #262: Loss:2.4303, Accuracy:0.1819, Validation Loss:2.4543, Validation Accuracy:0.1626\n",
    "Epoch #263: Loss:2.4325, Accuracy:0.1745, Validation Loss:2.4553, Validation Accuracy:0.1412\n",
    "Epoch #264: Loss:2.4311, Accuracy:0.1807, Validation Loss:2.4539, Validation Accuracy:0.1642\n",
    "Epoch #265: Loss:2.4296, Accuracy:0.1778, Validation Loss:2.4565, Validation Accuracy:0.1494\n",
    "Epoch #266: Loss:2.4314, Accuracy:0.1819, Validation Loss:2.4537, Validation Accuracy:0.1658\n",
    "Epoch #267: Loss:2.4280, Accuracy:0.1803, Validation Loss:2.4577, Validation Accuracy:0.1675\n",
    "Epoch #268: Loss:2.4308, Accuracy:0.1803, Validation Loss:2.4543, Validation Accuracy:0.1626\n",
    "Epoch #269: Loss:2.4301, Accuracy:0.1828, Validation Loss:2.4545, Validation Accuracy:0.1511\n",
    "Epoch #270: Loss:2.4284, Accuracy:0.1860, Validation Loss:2.4544, Validation Accuracy:0.1691\n",
    "Epoch #271: Loss:2.4297, Accuracy:0.1778, Validation Loss:2.4555, Validation Accuracy:0.1560\n",
    "Epoch #272: Loss:2.4301, Accuracy:0.1766, Validation Loss:2.4551, Validation Accuracy:0.1511\n",
    "Epoch #273: Loss:2.4283, Accuracy:0.1836, Validation Loss:2.4539, Validation Accuracy:0.1658\n",
    "Epoch #274: Loss:2.4302, Accuracy:0.1815, Validation Loss:2.4554, Validation Accuracy:0.1626\n",
    "Epoch #275: Loss:2.4283, Accuracy:0.1848, Validation Loss:2.4549, Validation Accuracy:0.1658\n",
    "Epoch #276: Loss:2.4301, Accuracy:0.1840, Validation Loss:2.4560, Validation Accuracy:0.1445\n",
    "Epoch #277: Loss:2.4297, Accuracy:0.1766, Validation Loss:2.4541, Validation Accuracy:0.1658\n",
    "Epoch #278: Loss:2.4283, Accuracy:0.1786, Validation Loss:2.4556, Validation Accuracy:0.1511\n",
    "Epoch #279: Loss:2.4291, Accuracy:0.1836, Validation Loss:2.4554, Validation Accuracy:0.1691\n",
    "Epoch #280: Loss:2.4285, Accuracy:0.1803, Validation Loss:2.4550, Validation Accuracy:0.1658\n",
    "Epoch #281: Loss:2.4279, Accuracy:0.1819, Validation Loss:2.4561, Validation Accuracy:0.1494\n",
    "Epoch #282: Loss:2.4291, Accuracy:0.1799, Validation Loss:2.4545, Validation Accuracy:0.1658\n",
    "Epoch #283: Loss:2.4303, Accuracy:0.1823, Validation Loss:2.4542, Validation Accuracy:0.1658\n",
    "Epoch #284: Loss:2.4273, Accuracy:0.1811, Validation Loss:2.4577, Validation Accuracy:0.1494\n",
    "Epoch #285: Loss:2.4284, Accuracy:0.1836, Validation Loss:2.4549, Validation Accuracy:0.1658\n",
    "Epoch #286: Loss:2.4281, Accuracy:0.1832, Validation Loss:2.4559, Validation Accuracy:0.1576\n",
    "Epoch #287: Loss:2.4285, Accuracy:0.1844, Validation Loss:2.4552, Validation Accuracy:0.1511\n",
    "Epoch #288: Loss:2.4290, Accuracy:0.1815, Validation Loss:2.4546, Validation Accuracy:0.1658\n",
    "Epoch #289: Loss:2.4297, Accuracy:0.1786, Validation Loss:2.4570, Validation Accuracy:0.1675\n",
    "Epoch #290: Loss:2.4293, Accuracy:0.1774, Validation Loss:2.4551, Validation Accuracy:0.1658\n",
    "Epoch #291: Loss:2.4291, Accuracy:0.1786, Validation Loss:2.4576, Validation Accuracy:0.1445\n",
    "Epoch #292: Loss:2.4279, Accuracy:0.1832, Validation Loss:2.4546, Validation Accuracy:0.1658\n",
    "Epoch #293: Loss:2.4303, Accuracy:0.1811, Validation Loss:2.4559, Validation Accuracy:0.1626\n",
    "Epoch #294: Loss:2.4313, Accuracy:0.1791, Validation Loss:2.4567, Validation Accuracy:0.1445\n",
    "Epoch #295: Loss:2.4312, Accuracy:0.1766, Validation Loss:2.4566, Validation Accuracy:0.1478\n",
    "Epoch #296: Loss:2.4287, Accuracy:0.1844, Validation Loss:2.4554, Validation Accuracy:0.1642\n",
    "Epoch #297: Loss:2.4284, Accuracy:0.1844, Validation Loss:2.4603, Validation Accuracy:0.1494\n",
    "Epoch #298: Loss:2.4302, Accuracy:0.1774, Validation Loss:2.4553, Validation Accuracy:0.1626\n",
    "Epoch #299: Loss:2.4292, Accuracy:0.1786, Validation Loss:2.4557, Validation Accuracy:0.1511\n",
    "Epoch #300: Loss:2.4299, Accuracy:0.1791, Validation Loss:2.4583, Validation Accuracy:0.1494\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45833898, Accuracy:0.1494\n",
    "Labels: ['mb', 'ce', 'sk', 'ib', 'my', 'eg', 'sg', 'ck', 'ds', 'by', 'eo', 'ek', 'eb', 'yd', 'aa']\n",
    "Confusion Matrix:\n",
    "      mb  ce  sk  ib  my  eg  sg  ck  ds  by  eo  ek  eb  yd  aa\n",
    "t:mb   0   0   0   6   0  17  16   0   1   1   0   0   9   2   0\n",
    "t:ce   0   0   0   2   0   6   8   0   2   3   0   0   6   0   0\n",
    "t:sk   0   0   0   1   0  15   5   0   1   5   0   0   6   0   0\n",
    "t:ib   0   0   0  21   0   6  16   0   1   2   0   0   4   4   0\n",
    "t:my   0   0   0   3   0   8   3   0   3   0   0   0   3   0   0\n",
    "t:eg   0   0   0   0   0  28   4   0   8   4   0   0   6   0   0\n",
    "t:sg   0   0   0   7   0   6  20   0   0   3   0   0  13   2   0\n",
    "t:ck   0   0   0   0   0  13   4   0   1   1   0   0   4   0   0\n",
    "t:ds   0   0   0   1   0  11   4   0   8   3   0   0   4   0   0\n",
    "t:by   0   0   0   4   0   9  11   0   3   4   0   0   9   0   0\n",
    "t:eo   0   0   0   1   0   6   9   0   0   2   0   0  15   1   0\n",
    "t:ek   0   0   0   3   0  20  10   0   0   1   0   0  12   2   0\n",
    "t:eb   0   0   0   4   0  24   7   0   0   4   0   0   7   4   0\n",
    "t:yd   0   0   0  31   0   3  19   0   0   0   0   0   6   3   0\n",
    "t:aa   0   0   0   1   0  17   1   0   7   3   0   0   4   1   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ib       0.25      0.39      0.30        54\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eg       0.15      0.56      0.23        50\n",
    "          sg       0.15      0.39      0.21        51\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ds       0.23      0.26      0.24        31\n",
    "          by       0.11      0.10      0.11        40\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          eb       0.06      0.14      0.09        50\n",
    "          yd       0.16      0.05      0.07        62\n",
    "          aa       0.00      0.00      0.00        34\n",
    "\n",
    "    accuracy                           0.15       609\n",
    "   macro avg       0.07      0.13      0.08       609\n",
    "weighted avg       0.09      0.15      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 13:51:37 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.698224435298901, 2.6895596010148624, 2.6830225711189857, 2.677013264892528, 2.670826170049082, 2.664702799519882, 2.6577423452743756, 2.6498956966086955, 2.6399884541046443, 2.6271329607282365, 2.612693340711797, 2.596300365106617, 2.5851395596032853, 2.5680503516361632, 2.5460251516896517, 2.5345823024881295, 2.525672932955236, 2.5140996680079617, 2.496842695182963, 2.4905169155014364, 2.487424686428753, 2.4965221733099527, 2.5810262835867497, 2.5946813534046043, 2.491894409574311, 2.5047591852241355, 2.487806981811774, 2.4821620128424886, 2.4849092854654846, 2.4713270276638086, 2.468670105894994, 2.466239006452764, 2.468118838684508, 2.4653098007728316, 2.4636448059958975, 2.4632049601262036, 2.4633465435704576, 2.4619535049194186, 2.463565335485148, 2.462859165492316, 2.4627817025521312, 2.4624710822927542, 2.4629162285715487, 2.4629186464256447, 2.461884305199183, 2.4629465069481107, 2.4612816938234277, 2.460341887716785, 2.4612342540070733, 2.460697460644351, 2.459549736311087, 2.4588583330019746, 2.4595710608759536, 2.4600549977401207, 2.4593945927612104, 2.4590858144713152, 2.458368908204077, 2.4591135430610045, 2.4572515068774545, 2.4579175348548077, 2.45844188425537, 2.4563901792410365, 2.457028349044875, 2.4587320776408528, 2.456310201748251, 2.457116129363112, 2.458001942470156, 2.456173986832692, 2.4557721462155797, 2.4581859659874574, 2.4543757595256435, 2.4563573397243355, 2.457811972582086, 2.456252946054994, 2.455316213942905, 2.45707981457264, 2.458816497392451, 2.45756796741329, 2.458318928388148, 2.456816978251014, 2.4566313128166013, 2.45715613866283, 2.455384047356341, 2.455407689944864, 2.4548765967044925, 2.456486633454246, 2.455459867204939, 2.4554833340135898, 2.45633828307216, 2.4567661770850373, 2.4553894327191883, 2.454399484718962, 2.4561422293996578, 2.454491003002048, 2.455016870608275, 2.4545125789047266, 2.455679066271226, 2.45602140559743, 2.455489261984238, 2.4555368431291753, 2.4559441916461062, 2.4555582076262175, 2.455579370113429, 2.4555118530254645, 2.4550001636906016, 2.456649801414001, 2.4552101771819768, 2.454794455827359, 2.4544159857118855, 2.45394570447737, 2.455660875207685, 2.457275820091636, 2.455182276727335, 2.4536846744994616, 2.4556916864047498, 2.4534961331654066, 2.455497970721992, 2.4532904311745427, 2.4552883134882633, 2.452886758375246, 2.4531934014682113, 2.4580530673999506, 2.453770614414184, 2.454509623532225, 2.4524836712478613, 2.453287681335299, 2.4532073663764793, 2.453530722650988, 2.45307510161439, 2.4541129885831685, 2.4531728716319416, 2.452749135263252, 2.4531697627946074, 2.4520386745189797, 2.4527627948078226, 2.4523287277503556, 2.4523902491395697, 2.4540861408502987, 2.452823301449981, 2.452350533458791, 2.45301274908783, 2.4524356790364084, 2.4531305055508668, 2.4521938834480075, 2.4520645619221706, 2.454052721534065, 2.45200760532874, 2.4525277469741495, 2.4522286898201124, 2.4541118959292207, 2.4528988633054034, 2.4548490129668137, 2.4522062808226286, 2.4534821639507274, 2.452372690335479, 2.4535943784541487, 2.452093641550474, 2.453274585930585, 2.4520969770616303, 2.4525198098669696, 2.452222675721242, 2.4517944180123714, 2.4521143812264126, 2.4527198391399163, 2.4521968732717982, 2.4522916417208016, 2.452884854158549, 2.4528247703276636, 2.452790291634295, 2.4530008782698407, 2.452288398210247, 2.4527934842509, 2.452583864991888, 2.45284046014933, 2.451520001359761, 2.4536678712747757, 2.453106608101104, 2.4545079436403974, 2.452414859104626, 2.454038201881747, 2.4531119793702425, 2.452292395734239, 2.454057481292825, 2.452596171540384, 2.455250645701717, 2.4516932889941487, 2.4531182005683387, 2.451699850398723, 2.4525387862633012, 2.453603086408919, 2.4535156462971606, 2.4556099843900583, 2.452727916792696, 2.4533945860338133, 2.4517498149464676, 2.4519187300076037, 2.452906675135169, 2.4533385392676044, 2.452034441316852, 2.454127175075863, 2.4519721800079095, 2.452836611392267, 2.452134954909777, 2.453124322327487, 2.4525662169276394, 2.4542794677815807, 2.452250624720882, 2.4521197321379713, 2.4541295087591966, 2.4521472422751693, 2.4520784379617724, 2.453068532771469, 2.452978503723646, 2.455291786804575, 2.4524133123200516, 2.453644306984637, 2.4523230820453814, 2.4534503062957613, 2.4524111873019114, 2.4535698691025156, 2.4528023167001005, 2.452563151154416, 2.454808039422497, 2.4533061076854836, 2.4528279355398346, 2.453395845463319, 2.4526218017333834, 2.4550112605290657, 2.453057724071058, 2.453378301927413, 2.45296092965137, 2.4528855118649737, 2.455815275705898, 2.4538913493477454, 2.4565669225745994, 2.4534187097659057, 2.4534976255326044, 2.4552583177688674, 2.453474209226411, 2.4535464414430566, 2.4545679632666078, 2.453267636557518, 2.4562626382204504, 2.4541979168827703, 2.4556399284325208, 2.453461980584807, 2.4545663252644156, 2.453658890058646, 2.4543040519081702, 2.453478587672041, 2.4545776847939575, 2.454011305603879, 2.4545069927065244, 2.45389079147176, 2.454308076445105, 2.4534499547360173, 2.455235036527386, 2.4545200289959586, 2.4543975789362964, 2.453917881538128, 2.4539409216205867, 2.454288407891059, 2.4553487046403055, 2.4538565120477784, 2.4565451904666444, 2.4536922220721817, 2.457735943676803, 2.4543476641080257, 2.454527087203779, 2.4544415626619838, 2.455529619711765, 2.4551375691330883, 2.4539355585727787, 2.4554035632285385, 2.454859665070457, 2.4559666037755257, 2.454093928407566, 2.455590434458064, 2.4554401144801297, 2.4550323028282577, 2.4561440396582945, 2.4545316641358124, 2.4542153531498903, 2.4577123429779153, 2.4549269202503274, 2.455861006660023, 2.4551795135773657, 2.454616066270274, 2.4569545418562364, 2.4550624473145835, 2.457628110750947, 2.4545598852223365, 2.4558654649700045, 2.4566795853362686, 2.456589436491918, 2.4553873452842723, 2.4602866388111084, 2.455271571336317, 2.4556986201181394, 2.4583389794297994], 'val_acc': [0.09523809473649622, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.0886699507327992, 0.08702791460800445, 0.09359605910718344, 0.10673234761005944, 0.11165845618018963, 0.11986863690203634, 0.13628899735476582, 0.14778325013045607, 0.14285714175607184, 0.15270935860271329, 0.15599343075442978, 0.15763546687922456, 0.1527093595936772, 0.15927750409285618, 0.147783251023547, 0.15927750310189226, 0.16748768452108395, 0.1625615762445727, 0.16256157634244567, 0.14778325121929298, 0.15599343164752075, 0.16912971985066075, 0.1740558285186639, 0.16912972074375168, 0.16912972074375168, 0.1658456484941622, 0.16584564859203518, 0.16912972084162467, 0.16584564769894422, 0.16420361167202246, 0.16091953932456, 0.16420361147627652, 0.15927750310189226, 0.15599343164752075, 0.1625615752536088, 0.15927750399498322, 0.16256157634244567, 0.16584564750319827, 0.17241379200237725, 0.17241379200237725, 0.16584564750319827, 0.16091953912881404, 0.15927750409285618, 0.16420361246724044, 0.16912972084162467, 0.16748768461895694, 0.16748768461895694, 0.1625615762445727, 0.16420361236936745, 0.16091954011977797, 0.16584564750319827, 0.16748768382373896, 0.16420361137840353, 0.16420361137840353, 0.16420361137840353, 0.16420361147627652, 0.16091953912881404, 0.16584564859203518, 0.16584564750319827, 0.16584564750319827, 0.1740558285186639, 0.16912971985066075, 0.16584564760107126, 0.16256157544935473, 0.16091954021765092, 0.17405582842079093, 0.16256157634244567, 0.16091953912881404, 0.17405582842079093, 0.16748768362799302, 0.16420361137840353, 0.17569786454558567, 0.17241379309121416, 0.16584564760107126, 0.17241379229599618, 0.17241379229599618, 0.1740558285186639, 0.17569786454558567, 0.16748768401948494, 0.16912971975278776, 0.16748768362799302, 0.17405582842079093, 0.1658456478946902, 0.16912972034002563, 0.17569786454558567, 0.1625615752536088, 0.17405582842079093, 0.1707717559754555, 0.1773399007682534, 0.1773399007682534, 0.17569786464345866, 0.17241379210025023, 0.16748768411735793, 0.1625615756451007, 0.1625615756451007, 0.1625615756451007, 0.16091953912881404, 0.1625615756451007, 0.16420361176989545, 0.16420361176989545, 0.1674876842152309, 0.16748768392161195, 0.1642036118677684, 0.16584564799256318, 0.1691297200464067, 0.16091953912881404, 0.16912972034002563, 0.1691297200464067, 0.16584564750319827, 0.1674876842152309, 0.16584564769894422, 0.16256157544935473, 0.16420361147627652, 0.16420361157414948, 0.16748768382373896, 0.16912972034002563, 0.16420361157414948, 0.167487683725866, 0.16584564769894422, 0.16420361147627652, 0.16256157544935473, 0.1642036119656414, 0.16584564799256318, 0.16584564799256318, 0.1642036118677684, 0.16420361167202246, 0.16091953961817893, 0.16584564799256318, 0.17077175626907443, 0.1674876842152309, 0.16256157574297367, 0.16584564799256318, 0.16584564799256318, 0.1691297199485337, 0.1642036118677684, 0.1642036118677684, 0.16584564799256318, 0.16256157574297367, 0.17241379239386917, 0.160919539226687, 0.16091953961817893, 0.16584564760107126, 0.16091953961817893, 0.1642036118677684, 0.1642036118677684, 0.16420361167202246, 0.16256157535148175, 0.1691297199485337, 0.16091953961817893, 0.1691297199485337, 0.160919539226687, 0.16420361167202246, 0.16091953961817893, 0.16420361167202246, 0.16584564799256318, 0.16748768411735793, 0.16748768411735793, 0.16748768411735793, 0.16748768411735793, 0.16748768411735793, 0.16584564799256318, 0.16584564799256318, 0.16256157554722772, 0.16748768411735793, 0.16748768411735793, 0.16748768411735793, 0.16091953961817893, 0.16256157554722772, 0.16584564799256318, 0.16256157554722772, 0.1642036118677684, 0.1691297199485337, 0.16256157535148175, 0.1691297199485337, 0.1642036118677684, 0.16420361167202246, 0.16256157574297367, 0.16256157574297367, 0.16420361167202246, 0.16256157535148175, 0.1691297199485337, 0.16584564799256318, 0.1691297199485337, 0.16584564799256318, 0.16256157574297367, 0.1691297199485337, 0.16091953961817893, 0.1691297199485337, 0.16256157574297367, 0.1691297199485337, 0.16091953961817893, 0.16584564799256318, 0.16256157554722772, 0.16256157554722772, 0.16256157574297367, 0.1691297199485337, 0.16256157574297367, 0.16256157554722772, 0.16256157574297367, 0.16256157554722772, 0.16256157574297367, 0.16256157535148175, 0.16584564799256318, 0.16584564799256318, 0.1691297199485337, 0.16256157574297367, 0.16584564799256318, 0.16256157554722772, 0.16584564799256318, 0.16584564769894422, 0.16256157574297367, 0.16748768382373896, 0.16256157574297367, 0.16256157554722772, 0.16584564799256318, 0.16256157554722772, 0.16584564799256318, 0.1642036118677684, 0.16256157554722772, 0.16256157574297367, 0.16256157574297367, 0.16584564799256318, 0.16256157574297367, 0.16256157554722772, 0.16584564799256318, 0.15927750329763823, 0.16256157574297367, 0.16584564799256318, 0.16748768382373896, 0.16584564760107126, 0.16584564769894422, 0.16256157574297367, 0.16584564799256318, 0.16256157554722772, 0.16256157574297367, 0.15270935870058627, 0.16256157554722772, 0.16256157574297367, 0.16748768382373896, 0.16584564799256318, 0.15270935860271329, 0.16584564799256318, 0.16256157554722772, 0.16256157574297367, 0.16584564799256318, 0.16584564799256318, 0.16256157554722772, 0.15927750329763823, 0.16256157554722772, 0.16584564799256318, 0.16584564799256318, 0.16584564799256318, 0.16256157554722772, 0.16584564799256318, 0.16584564799256318, 0.16256157574297367, 0.16584564799256318, 0.16256157554722772, 0.14121510582702304, 0.1642036118677684, 0.1494252863531238, 0.16584564799256318, 0.16748768382373896, 0.16256157574297367, 0.15106732277153748, 0.16912972024215267, 0.15599343095017576, 0.15106732277153748, 0.16584564799256318, 0.16256157554722772, 0.16584564799256318, 0.14449917807661253, 0.16584564799256318, 0.15106732277153748, 0.16912972024215267, 0.16584564799256318, 0.1494252863531238, 0.16584564799256318, 0.16584564799256318, 0.1494252863531238, 0.16584564799256318, 0.1576354671728435, 0.15106732277153748, 0.16584564799256318, 0.16748768382373896, 0.16584564799256318, 0.14449917807661253, 0.16584564799256318, 0.16256157554722772, 0.14449917807661253, 0.14778325032620204, 0.1642036118677684, 0.1494252863531238, 0.16256157574297367, 0.15106732277153748, 0.1494252863531238], 'loss': [2.705461426094572, 2.6945076982588247, 2.686527949632805, 2.680472428402127, 2.674499743968799, 2.6687123109672592, 2.66248346608767, 2.655243277304961, 2.6467057325022423, 2.636589103263996, 2.6237817756449173, 2.608342077404077, 2.590725936439248, 2.5861622555789516, 2.5676374878971484, 2.543766596968414, 2.5339277992992675, 2.5223913232893427, 2.5120015668183626, 2.505590052281562, 2.4985647734185754, 2.494005630735989, 2.5179436199963705, 2.5797260986461286, 2.555148591740665, 2.499875335184211, 2.513861888439014, 2.4888660094086883, 2.4880512301192392, 2.4833833349803633, 2.477029989877031, 2.473325413404304, 2.4686797506265816, 2.467361053059478, 2.465272685291831, 2.463671572546205, 2.463854759576629, 2.462552470886732, 2.4608994317495356, 2.4606860789430214, 2.4594017597684137, 2.458662000965534, 2.4579005448969973, 2.4576024418738833, 2.4572852800514173, 2.455959641419397, 2.455179354885031, 2.45412081125091, 2.454101487204769, 2.453408300509443, 2.453141001656315, 2.4526964379531893, 2.4519388132271582, 2.451844878461082, 2.4507167899388307, 2.4514802194473924, 2.4492742312272715, 2.4490556768812928, 2.4481998110698724, 2.446928439052198, 2.447832935497746, 2.4466508642114406, 2.447144724945758, 2.4468357800947813, 2.4453338830622804, 2.445398777270464, 2.445518255625417, 2.445069904640715, 2.4463865750134604, 2.445132236950696, 2.4447907940318205, 2.445591169216305, 2.4441755459293937, 2.4451816043814594, 2.4440681718947705, 2.4432247695492033, 2.4439773742667947, 2.443383842621006, 2.442332229673006, 2.4428458498733487, 2.4421428806727916, 2.4418059054341406, 2.4418884562271086, 2.441224784381091, 2.441097408447422, 2.4407910983420495, 2.440700137982378, 2.4408873761703838, 2.4405344037794237, 2.440541771158301, 2.441231818953089, 2.44078888139196, 2.4410988110536422, 2.4405805420337026, 2.440455865272506, 2.4388832637661535, 2.43991940995751, 2.439387139353664, 2.4396494749879936, 2.4393599692311376, 2.439890283921416, 2.4391036507530135, 2.4392831609479213, 2.4394564241844034, 2.4391390627169756, 2.438807381251999, 2.438368075975892, 2.438464452304879, 2.438016640528027, 2.4388288878807054, 2.4390845466198616, 2.443681141679047, 2.4428507552254617, 2.4435996154495334, 2.4413859886310427, 2.4398558679302615, 2.4394393722134695, 2.440741639656208, 2.4397466778020838, 2.441443834657297, 2.440787844295619, 2.438937269148151, 2.439382680093973, 2.440154379838791, 2.4397006835780837, 2.43843982048348, 2.4380945660005606, 2.43812456796791, 2.438271856112157, 2.4385585381509833, 2.4383235694446603, 2.438247578638535, 2.4384109757519354, 2.4375818373975813, 2.438207980396811, 2.437642024821569, 2.4374912864128913, 2.4384566274756523, 2.4408853787416307, 2.4408934172174033, 2.4381121294699164, 2.437593053645422, 2.437235747715286, 2.4375018372428, 2.4388034717747806, 2.4367839696715743, 2.4372766612246783, 2.437462092131315, 2.437058807545374, 2.4383962115223157, 2.4369394962302957, 2.439422818228939, 2.4371123980692526, 2.437879613194867, 2.436806595937427, 2.4375403126162425, 2.435974207648996, 2.4361473916737206, 2.436291709132263, 2.4361722566263877, 2.435772225009832, 2.435855890935941, 2.4355379651948903, 2.435644017989141, 2.4355946388087966, 2.434940671137471, 2.4355610966927217, 2.435150324196786, 2.4363897429354626, 2.4350857273509123, 2.434978317675894, 2.4347813884335623, 2.435842215011252, 2.435406080359551, 2.4352653516146683, 2.435435616358105, 2.436571427488229, 2.4350908419434782, 2.435008469156661, 2.4352549685100264, 2.4354610185603587, 2.434928158812944, 2.4346998469295933, 2.4359021643104004, 2.4346372711095476, 2.4347612051014047, 2.4344122820076755, 2.433620218182981, 2.434773492617284, 2.4376950506801722, 2.4341648659911734, 2.435512014678861, 2.4353893609996695, 2.434134428701851, 2.4351355655482174, 2.4337871492765766, 2.433587096799815, 2.4343598518528244, 2.433563555045784, 2.4334555118725287, 2.4330947871080904, 2.4335535049438475, 2.433054979430087, 2.4344149062765696, 2.4336844753680533, 2.432927396507968, 2.43360030156631, 2.4339406630341767, 2.4332729751814073, 2.4321897623230546, 2.4330766845287974, 2.432351842700089, 2.4351983947675575, 2.433605685322192, 2.433088058318935, 2.4331993697360312, 2.4336861207989453, 2.4320015718315173, 2.43222679200848, 2.431961966735871, 2.4320988948829854, 2.4318266106826814, 2.4318817502908883, 2.431710801526017, 2.43200269381858, 2.4317388284867305, 2.4320933852107616, 2.431377546302592, 2.4321440757422477, 2.4312346081469336, 2.431723398643353, 2.4348878692062974, 2.433358194891677, 2.4318816804787953, 2.4330717856389543, 2.4309337373142124, 2.4323513023172807, 2.4304426822819014, 2.430931894000796, 2.431953808218547, 2.4327601258514844, 2.4324763522255837, 2.431575913301973, 2.4305320978654237, 2.4322861339522093, 2.4305404509362254, 2.4307791543447506, 2.430823266481717, 2.4310647810753854, 2.4299432227743725, 2.4298864428267586, 2.430111957624463, 2.4296218217031176, 2.430518531407664, 2.4295381685057214, 2.4296486467796186, 2.429296131251529, 2.4300719486369733, 2.429456853866577, 2.429227751432258, 2.429493960609671, 2.430260837934835, 2.432532110449225, 2.431103199071708, 2.429629306088238, 2.4313564936972742, 2.4279637434644132, 2.430787470893938, 2.430146418264025, 2.4283629324401916, 2.4296929224315855, 2.4301328674723726, 2.428346466969171, 2.4301625163648164, 2.428316839323886, 2.4300889662403837, 2.4296832485120645, 2.4282844768657332, 2.429059635001776, 2.428495457040211, 2.427933720054078, 2.4290518149702947, 2.4302743085355973, 2.427267576194152, 2.428447128076573, 2.428077538742911, 2.4284707728352637, 2.428984845688211, 2.42966304706597, 2.429290775596728, 2.4290679296184123, 2.427940576228273, 2.4303050830378914, 2.431274427084952, 2.4311645331568785, 2.4286637056534786, 2.4283887780912115, 2.430232833983717, 2.4292248545730875, 2.429921863896646], 'acc': [0.07474332602361879, 0.08911704328025881, 0.08911704308443245, 0.08911704366273214, 0.08911704286106803, 0.09075975334252664, 0.09445585190515499, 0.0993839837044661, 0.10882956892007185, 0.11047227900069842, 0.12443531800589278, 0.13593429109766253, 0.1416837787297717, 0.13388090420063026, 0.154414785477415, 0.15893223769121345, 0.15400410727186614, 0.15318275046911572, 0.167145791655938, 0.1679671460720548, 0.16427104713613247, 0.16632443462064378, 0.16427104813362295, 0.1494866530998042, 0.1564681729619263, 0.16344969092086112, 0.15482546170634165, 0.16755646884563768, 0.16468172495002864, 0.1683778229068192, 0.16960985536937595, 0.16714578966095708, 0.16837782427760364, 0.169199179532102, 0.16919917796549122, 0.17166324465304184, 0.173305954323657, 0.17002053435823022, 0.16755646727902687, 0.16550308016780954, 0.1650924035472302, 0.16673511225707233, 0.16796714667789256, 0.16714579065844754, 0.16796714687371891, 0.16960985615268134, 0.16591375739422667, 0.16837782271099286, 0.16755646708320054, 0.1659137582142495, 0.16673511304037772, 0.16591375759005303, 0.1675564684723437, 0.16755646866817003, 0.16427104672612106, 0.17002053357492483, 0.17002053377075116, 0.1687885011307268, 0.1708418882235854, 0.17125256723079837, 0.171252567408266, 0.17084188961272856, 0.1708418886152381, 0.16796714687371891, 0.16878850072071538, 0.1737166325475646, 0.17043121060551558, 0.1687884999557687, 0.17289527731142496, 0.16837782253352523, 0.16878850171820584, 0.1720739212736212, 0.17002053435823022, 0.17494866501012132, 0.16796714548457575, 0.16714579163757928, 0.1733059559086265, 0.16878850171820584, 0.16550307999034192, 0.17577002140286033, 0.17330595395036302, 0.1786447649435341, 0.17371663174590046, 0.17002053457241528, 0.16796714589458717, 0.17248460026247545, 0.1786447637502173, 0.17659137646153233, 0.17659137468073646, 0.1712525658416552, 0.16919917853461153, 0.17207392107779484, 0.17535934221817973, 0.17700205329629676, 0.17700205249463263, 0.17700205288628534, 0.17946612017967373, 0.1798767976019172, 0.17864476453352268, 0.17905544156411346, 0.17371663156843284, 0.17823408554466844, 0.17864476353603223, 0.17782340773077226, 0.17535934241400608, 0.18193018588809262, 0.18193018371564407, 0.17864476394604364, 0.1786447649435341, 0.18110882965446254, 0.17659137526821553, 0.17618069864763616, 0.1757700204420873, 0.17166324345972503, 0.17905544156411346, 0.17864476474770774, 0.17987679560693628, 0.17659137487656282, 0.17289527768471893, 0.17618069784597204, 0.172895277898904, 0.17700205229880628, 0.17741273152020434, 0.17823408534884208, 0.18028747541581336, 0.17248460028083418, 0.17782340968903576, 0.18151950648922696, 0.18480492860874356, 0.17741272993523483, 0.1798767968186118, 0.17864476355439093, 0.17905544037079665, 0.17905544056662298, 0.17741273128766055, 0.1806981526422305, 0.18110882887115715, 0.18316221794063794, 0.18151950729089106, 0.17043121138882097, 0.1819301854780812, 0.18234086329197738, 0.1786447629669119, 0.17946611998384737, 0.17782340771241353, 0.18357289555870776, 0.18069815301552444, 0.1811088284611457, 0.18275154051839448, 0.18069815223221905, 0.17905544156411346, 0.17905544236577756, 0.18357289514869635, 0.18069815262387176, 0.1724845999075402, 0.17700205327793803, 0.1798767960169477, 0.18234086272285704, 0.18234086250867199, 0.17823408652380018, 0.18069815125308733, 0.182340862135378, 0.18110882967282124, 0.18193018528225485, 0.18151950648922696, 0.18151950670341202, 0.18193018410729678, 0.18357289477540237, 0.17864476298527063, 0.17946611941472707, 0.1811088284611457, 0.18275154032256813, 0.18275153914761005, 0.18069815105726098, 0.18275153994927415, 0.17823408593632112, 0.171252567408266, 0.18521560524768163, 0.1806981514672724, 0.18275153953926274, 0.18151950607921552, 0.18316221696150622, 0.18234086209866054, 0.17823408554466844, 0.17453798737369278, 0.18151950807419645, 0.18069815246476284, 0.18110882906698347, 0.17782340792659862, 0.17659137567822694, 0.1757700196404232, 0.18028747439996418, 0.1815195066850533, 0.18069815244640414, 0.18110883006447395, 0.18357289418792333, 0.18275154053675322, 0.18275153994927415, 0.18234086133371388, 0.18562628323904543, 0.1790554425616039, 0.18275153994927415, 0.17823408730710558, 0.17864476277108554, 0.17782340773077226, 0.18439425159651152, 0.17700205366959074, 0.18193018588809262, 0.1802874740083115, 0.1856262842548946, 0.18316221754898526, 0.18234086249031325, 0.17946611918218328, 0.18275154114259098, 0.1860369610713003, 0.1860369618546057, 0.18110882906698347, 0.18357289454285858, 0.18151950647086823, 0.18193018569226627, 0.1823408629186834, 0.1831622181364643, 0.18110883063359426, 0.18234086211701928, 0.18275154053675322, 0.1843942504031947, 0.18316221655149478, 0.1802874744366816, 0.18069815262387176, 0.18110882887115715, 0.17535934321567018, 0.18234086192119292, 0.17782340892408907, 0.18110882945863618, 0.17618069745431936, 0.18193018410729678, 0.18316221717569128, 0.18398357239347218, 0.18480492743378546, 0.1823408629186834, 0.17946611820305153, 0.181519507076706, 0.17700205368794944, 0.1827515389517837, 0.1843942511865001, 0.18028747381248514, 0.18439424960153059, 0.1790554413682871, 0.18110883024194158, 0.1839835720018195, 0.18480492702377405, 0.1827515397350891, 0.1827515409100472, 0.18275154110587352, 0.17905544117246075, 0.18316221696150622, 0.1802874732433648, 0.18069815303388317, 0.1794661193780096, 0.1839835725709398, 0.1823408625270307, 0.18193018489060217, 0.17453798835282452, 0.1806981514672724, 0.1778234095115681, 0.18193018471313452, 0.18028747502416068, 0.18028747345754986, 0.18275153993091545, 0.18603696126712665, 0.17782340814078368, 0.17659137526821553, 0.18357289438374969, 0.1815195062934006, 0.18480492723795913, 0.1839835739784417, 0.17659137507238917, 0.17864476435605506, 0.1835728955770665, 0.18028747361665878, 0.1819301838931117, 0.17987679779774354, 0.18234086231284563, 0.18110882985028887, 0.18357289575453412, 0.18316221715733255, 0.18439425099067375, 0.18151950807419645, 0.1786447629669119, 0.1774127310918342, 0.17864476474770774, 0.18316221753062653, 0.18110883045612663, 0.17905544158247216, 0.17659137585569457, 0.184394250011542, 0.1843942507948474, 0.1774127299168761, 0.17864476298527063, 0.17905544216995123]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
