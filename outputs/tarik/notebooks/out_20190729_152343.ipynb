{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf11.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 15:23:43 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '3', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '04', '03', '05', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000289001EBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002895A246EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6152, Accuracy:0.1823, Validation Loss:1.6092, Validation Accuracy:0.1872\n",
    "Epoch #2: Loss:1.6076, Accuracy:0.2140, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6066, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2299\n",
    "Epoch #20: Loss:1.6032, Accuracy:0.2341, Validation Loss:1.6058, Validation Accuracy:0.2266\n",
    "Epoch #21: Loss:1.6029, Accuracy:0.2320, Validation Loss:1.6059, Validation Accuracy:0.2299\n",
    "Epoch #22: Loss:1.6028, Accuracy:0.2366, Validation Loss:1.6062, Validation Accuracy:0.2299\n",
    "Epoch #23: Loss:1.6021, Accuracy:0.2370, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6032, Accuracy:0.2324, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #26: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6023, Accuracy:0.2324, Validation Loss:1.6060, Validation Accuracy:0.2299\n",
    "Epoch #28: Loss:1.6021, Accuracy:0.2378, Validation Loss:1.6060, Validation Accuracy:0.2282\n",
    "Epoch #29: Loss:1.6011, Accuracy:0.2419, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #30: Loss:1.6005, Accuracy:0.2357, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #31: Loss:1.6004, Accuracy:0.2353, Validation Loss:1.6065, Validation Accuracy:0.2332\n",
    "Epoch #32: Loss:1.5997, Accuracy:0.2435, Validation Loss:1.6060, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.5987, Accuracy:0.2452, Validation Loss:1.6071, Validation Accuracy:0.2332\n",
    "Epoch #34: Loss:1.5982, Accuracy:0.2439, Validation Loss:1.6086, Validation Accuracy:0.2266\n",
    "Epoch #35: Loss:1.5991, Accuracy:0.2456, Validation Loss:1.6087, Validation Accuracy:0.2250\n",
    "Epoch #36: Loss:1.6008, Accuracy:0.2398, Validation Loss:1.6092, Validation Accuracy:0.2315\n",
    "Epoch #37: Loss:1.6018, Accuracy:0.2378, Validation Loss:1.6094, Validation Accuracy:0.2167\n",
    "Epoch #38: Loss:1.6008, Accuracy:0.2378, Validation Loss:1.6090, Validation Accuracy:0.2085\n",
    "Epoch #39: Loss:1.5994, Accuracy:0.2427, Validation Loss:1.6093, Validation Accuracy:0.2348\n",
    "Epoch #40: Loss:1.5991, Accuracy:0.2452, Validation Loss:1.6103, Validation Accuracy:0.2348\n",
    "Epoch #41: Loss:1.5992, Accuracy:0.2394, Validation Loss:1.6092, Validation Accuracy:0.2299\n",
    "Epoch #42: Loss:1.5970, Accuracy:0.2493, Validation Loss:1.6119, Validation Accuracy:0.2102\n",
    "Epoch #43: Loss:1.5981, Accuracy:0.2472, Validation Loss:1.6107, Validation Accuracy:0.2151\n",
    "Epoch #44: Loss:1.5977, Accuracy:0.2456, Validation Loss:1.6124, Validation Accuracy:0.2200\n",
    "Epoch #45: Loss:1.5976, Accuracy:0.2480, Validation Loss:1.6119, Validation Accuracy:0.2217\n",
    "Epoch #46: Loss:1.5982, Accuracy:0.2493, Validation Loss:1.6126, Validation Accuracy:0.2250\n",
    "Epoch #47: Loss:1.5972, Accuracy:0.2493, Validation Loss:1.6126, Validation Accuracy:0.2233\n",
    "Epoch #48: Loss:1.5971, Accuracy:0.2431, Validation Loss:1.6139, Validation Accuracy:0.2233\n",
    "Epoch #49: Loss:1.5973, Accuracy:0.2480, Validation Loss:1.6112, Validation Accuracy:0.2397\n",
    "Epoch #50: Loss:1.5981, Accuracy:0.2378, Validation Loss:1.6104, Validation Accuracy:0.2381\n",
    "Epoch #51: Loss:1.5980, Accuracy:0.2423, Validation Loss:1.6101, Validation Accuracy:0.2332\n",
    "Epoch #52: Loss:1.5971, Accuracy:0.2439, Validation Loss:1.6099, Validation Accuracy:0.2200\n",
    "Epoch #53: Loss:1.5975, Accuracy:0.2439, Validation Loss:1.6109, Validation Accuracy:0.2200\n",
    "Epoch #54: Loss:1.5964, Accuracy:0.2460, Validation Loss:1.6114, Validation Accuracy:0.2348\n",
    "Epoch #55: Loss:1.5961, Accuracy:0.2513, Validation Loss:1.6119, Validation Accuracy:0.2233\n",
    "Epoch #56: Loss:1.5951, Accuracy:0.2501, Validation Loss:1.6104, Validation Accuracy:0.2151\n",
    "Epoch #57: Loss:1.5946, Accuracy:0.2505, Validation Loss:1.6104, Validation Accuracy:0.2151\n",
    "Epoch #58: Loss:1.5945, Accuracy:0.2468, Validation Loss:1.6127, Validation Accuracy:0.2102\n",
    "Epoch #59: Loss:1.5942, Accuracy:0.2485, Validation Loss:1.6135, Validation Accuracy:0.2053\n",
    "Epoch #60: Loss:1.5938, Accuracy:0.2456, Validation Loss:1.6141, Validation Accuracy:0.2102\n",
    "Epoch #61: Loss:1.5950, Accuracy:0.2480, Validation Loss:1.6152, Validation Accuracy:0.2003\n",
    "Epoch #62: Loss:1.5945, Accuracy:0.2448, Validation Loss:1.6149, Validation Accuracy:0.2135\n",
    "Epoch #63: Loss:1.5948, Accuracy:0.2448, Validation Loss:1.6147, Validation Accuracy:0.2085\n",
    "Epoch #64: Loss:1.5946, Accuracy:0.2468, Validation Loss:1.6148, Validation Accuracy:0.2151\n",
    "Epoch #65: Loss:1.5935, Accuracy:0.2460, Validation Loss:1.6149, Validation Accuracy:0.2135\n",
    "Epoch #66: Loss:1.5936, Accuracy:0.2431, Validation Loss:1.6152, Validation Accuracy:0.2151\n",
    "Epoch #67: Loss:1.5924, Accuracy:0.2439, Validation Loss:1.6162, Validation Accuracy:0.2167\n",
    "Epoch #68: Loss:1.5925, Accuracy:0.2468, Validation Loss:1.6172, Validation Accuracy:0.2167\n",
    "Epoch #69: Loss:1.5935, Accuracy:0.2444, Validation Loss:1.6172, Validation Accuracy:0.2102\n",
    "Epoch #70: Loss:1.5932, Accuracy:0.2460, Validation Loss:1.6181, Validation Accuracy:0.2151\n",
    "Epoch #71: Loss:1.5939, Accuracy:0.2464, Validation Loss:1.6186, Validation Accuracy:0.2069\n",
    "Epoch #72: Loss:1.5940, Accuracy:0.2439, Validation Loss:1.6192, Validation Accuracy:0.2102\n",
    "Epoch #73: Loss:1.5942, Accuracy:0.2427, Validation Loss:1.6191, Validation Accuracy:0.2053\n",
    "Epoch #74: Loss:1.5940, Accuracy:0.2435, Validation Loss:1.6178, Validation Accuracy:0.2069\n",
    "Epoch #75: Loss:1.5943, Accuracy:0.2378, Validation Loss:1.6172, Validation Accuracy:0.2135\n",
    "Epoch #76: Loss:1.5950, Accuracy:0.2419, Validation Loss:1.6162, Validation Accuracy:0.2020\n",
    "Epoch #77: Loss:1.5927, Accuracy:0.2456, Validation Loss:1.6157, Validation Accuracy:0.2102\n",
    "Epoch #78: Loss:1.5938, Accuracy:0.2452, Validation Loss:1.6156, Validation Accuracy:0.2332\n",
    "Epoch #79: Loss:1.5929, Accuracy:0.2431, Validation Loss:1.6179, Validation Accuracy:0.2003\n",
    "Epoch #80: Loss:1.5928, Accuracy:0.2419, Validation Loss:1.6154, Validation Accuracy:0.2118\n",
    "Epoch #81: Loss:1.5939, Accuracy:0.2435, Validation Loss:1.6181, Validation Accuracy:0.2282\n",
    "Epoch #82: Loss:1.5937, Accuracy:0.2493, Validation Loss:1.6179, Validation Accuracy:0.2036\n",
    "Epoch #83: Loss:1.5933, Accuracy:0.2476, Validation Loss:1.6188, Validation Accuracy:0.2069\n",
    "Epoch #84: Loss:1.5937, Accuracy:0.2415, Validation Loss:1.6197, Validation Accuracy:0.2036\n",
    "Epoch #85: Loss:1.5925, Accuracy:0.2480, Validation Loss:1.6175, Validation Accuracy:0.2200\n",
    "Epoch #86: Loss:1.5927, Accuracy:0.2485, Validation Loss:1.6182, Validation Accuracy:0.2135\n",
    "Epoch #87: Loss:1.5930, Accuracy:0.2456, Validation Loss:1.6161, Validation Accuracy:0.2085\n",
    "Epoch #88: Loss:1.5926, Accuracy:0.2505, Validation Loss:1.6146, Validation Accuracy:0.2200\n",
    "Epoch #89: Loss:1.5925, Accuracy:0.2489, Validation Loss:1.6155, Validation Accuracy:0.2200\n",
    "Epoch #90: Loss:1.5925, Accuracy:0.2448, Validation Loss:1.6186, Validation Accuracy:0.2118\n",
    "Epoch #91: Loss:1.5930, Accuracy:0.2468, Validation Loss:1.6171, Validation Accuracy:0.2217\n",
    "Epoch #92: Loss:1.5915, Accuracy:0.2509, Validation Loss:1.6183, Validation Accuracy:0.2250\n",
    "Epoch #93: Loss:1.5912, Accuracy:0.2501, Validation Loss:1.6176, Validation Accuracy:0.2217\n",
    "Epoch #94: Loss:1.5920, Accuracy:0.2501, Validation Loss:1.6172, Validation Accuracy:0.2266\n",
    "Epoch #95: Loss:1.5916, Accuracy:0.2501, Validation Loss:1.6180, Validation Accuracy:0.2102\n",
    "Epoch #96: Loss:1.5918, Accuracy:0.2415, Validation Loss:1.6167, Validation Accuracy:0.2184\n",
    "Epoch #97: Loss:1.5914, Accuracy:0.2472, Validation Loss:1.6161, Validation Accuracy:0.2233\n",
    "Epoch #98: Loss:1.5908, Accuracy:0.2468, Validation Loss:1.6174, Validation Accuracy:0.2085\n",
    "Epoch #99: Loss:1.5919, Accuracy:0.2419, Validation Loss:1.6177, Validation Accuracy:0.2233\n",
    "Epoch #100: Loss:1.5928, Accuracy:0.2398, Validation Loss:1.6193, Validation Accuracy:0.2217\n",
    "Epoch #101: Loss:1.5917, Accuracy:0.2480, Validation Loss:1.6177, Validation Accuracy:0.2200\n",
    "Epoch #102: Loss:1.5926, Accuracy:0.2472, Validation Loss:1.6190, Validation Accuracy:0.2167\n",
    "Epoch #103: Loss:1.5913, Accuracy:0.2468, Validation Loss:1.6187, Validation Accuracy:0.2167\n",
    "Epoch #104: Loss:1.5927, Accuracy:0.2476, Validation Loss:1.6169, Validation Accuracy:0.2200\n",
    "Epoch #105: Loss:1.5926, Accuracy:0.2480, Validation Loss:1.6184, Validation Accuracy:0.2151\n",
    "Epoch #106: Loss:1.5912, Accuracy:0.2501, Validation Loss:1.6175, Validation Accuracy:0.2266\n",
    "Epoch #107: Loss:1.5913, Accuracy:0.2476, Validation Loss:1.6185, Validation Accuracy:0.2118\n",
    "Epoch #108: Loss:1.5918, Accuracy:0.2485, Validation Loss:1.6180, Validation Accuracy:0.2118\n",
    "Epoch #109: Loss:1.5909, Accuracy:0.2464, Validation Loss:1.6211, Validation Accuracy:0.1987\n",
    "Epoch #110: Loss:1.5922, Accuracy:0.2485, Validation Loss:1.6176, Validation Accuracy:0.2200\n",
    "Epoch #111: Loss:1.5917, Accuracy:0.2476, Validation Loss:1.6186, Validation Accuracy:0.2184\n",
    "Epoch #112: Loss:1.5914, Accuracy:0.2480, Validation Loss:1.6217, Validation Accuracy:0.1970\n",
    "Epoch #113: Loss:1.5904, Accuracy:0.2517, Validation Loss:1.6197, Validation Accuracy:0.2118\n",
    "Epoch #114: Loss:1.5918, Accuracy:0.2444, Validation Loss:1.6189, Validation Accuracy:0.2184\n",
    "Epoch #115: Loss:1.5924, Accuracy:0.2431, Validation Loss:1.6214, Validation Accuracy:0.2069\n",
    "Epoch #116: Loss:1.5913, Accuracy:0.2460, Validation Loss:1.6193, Validation Accuracy:0.2085\n",
    "Epoch #117: Loss:1.5914, Accuracy:0.2452, Validation Loss:1.6201, Validation Accuracy:0.2102\n",
    "Epoch #118: Loss:1.5904, Accuracy:0.2427, Validation Loss:1.6227, Validation Accuracy:0.2069\n",
    "Epoch #119: Loss:1.5899, Accuracy:0.2509, Validation Loss:1.6214, Validation Accuracy:0.2167\n",
    "Epoch #120: Loss:1.5896, Accuracy:0.2534, Validation Loss:1.6204, Validation Accuracy:0.2184\n",
    "Epoch #121: Loss:1.5895, Accuracy:0.2517, Validation Loss:1.6207, Validation Accuracy:0.2184\n",
    "Epoch #122: Loss:1.5889, Accuracy:0.2509, Validation Loss:1.6198, Validation Accuracy:0.2200\n",
    "Epoch #123: Loss:1.5887, Accuracy:0.2538, Validation Loss:1.6198, Validation Accuracy:0.2167\n",
    "Epoch #124: Loss:1.5893, Accuracy:0.2526, Validation Loss:1.6226, Validation Accuracy:0.2085\n",
    "Epoch #125: Loss:1.5885, Accuracy:0.2534, Validation Loss:1.6182, Validation Accuracy:0.2053\n",
    "Epoch #126: Loss:1.5874, Accuracy:0.2538, Validation Loss:1.6214, Validation Accuracy:0.2069\n",
    "Epoch #127: Loss:1.5859, Accuracy:0.2526, Validation Loss:1.6230, Validation Accuracy:0.2053\n",
    "Epoch #128: Loss:1.5864, Accuracy:0.2534, Validation Loss:1.6220, Validation Accuracy:0.2102\n",
    "Epoch #129: Loss:1.5860, Accuracy:0.2559, Validation Loss:1.6240, Validation Accuracy:0.2102\n",
    "Epoch #130: Loss:1.5873, Accuracy:0.2554, Validation Loss:1.6199, Validation Accuracy:0.2102\n",
    "Epoch #131: Loss:1.5863, Accuracy:0.2538, Validation Loss:1.6216, Validation Accuracy:0.2053\n",
    "Epoch #132: Loss:1.5884, Accuracy:0.2517, Validation Loss:1.6222, Validation Accuracy:0.2200\n",
    "Epoch #133: Loss:1.5893, Accuracy:0.2546, Validation Loss:1.6199, Validation Accuracy:0.2135\n",
    "Epoch #134: Loss:1.5897, Accuracy:0.2522, Validation Loss:1.6178, Validation Accuracy:0.2233\n",
    "Epoch #135: Loss:1.5896, Accuracy:0.2546, Validation Loss:1.6202, Validation Accuracy:0.2036\n",
    "Epoch #136: Loss:1.5890, Accuracy:0.2546, Validation Loss:1.6195, Validation Accuracy:0.2102\n",
    "Epoch #137: Loss:1.5881, Accuracy:0.2550, Validation Loss:1.6198, Validation Accuracy:0.2167\n",
    "Epoch #138: Loss:1.5879, Accuracy:0.2517, Validation Loss:1.6211, Validation Accuracy:0.2200\n",
    "Epoch #139: Loss:1.5887, Accuracy:0.2583, Validation Loss:1.6241, Validation Accuracy:0.2250\n",
    "Epoch #140: Loss:1.5921, Accuracy:0.2493, Validation Loss:1.6225, Validation Accuracy:0.2053\n",
    "Epoch #141: Loss:1.5902, Accuracy:0.2571, Validation Loss:1.6220, Validation Accuracy:0.2102\n",
    "Epoch #142: Loss:1.5873, Accuracy:0.2628, Validation Loss:1.6214, Validation Accuracy:0.2135\n",
    "Epoch #143: Loss:1.5928, Accuracy:0.2538, Validation Loss:1.6370, Validation Accuracy:0.1970\n",
    "Epoch #144: Loss:1.5947, Accuracy:0.2505, Validation Loss:1.6170, Validation Accuracy:0.2348\n",
    "Epoch #145: Loss:1.5954, Accuracy:0.2497, Validation Loss:1.6217, Validation Accuracy:0.2184\n",
    "Epoch #146: Loss:1.5962, Accuracy:0.2546, Validation Loss:1.6176, Validation Accuracy:0.2200\n",
    "Epoch #147: Loss:1.5909, Accuracy:0.2616, Validation Loss:1.6181, Validation Accuracy:0.2200\n",
    "Epoch #148: Loss:1.5901, Accuracy:0.2530, Validation Loss:1.6184, Validation Accuracy:0.2184\n",
    "Epoch #149: Loss:1.5899, Accuracy:0.2513, Validation Loss:1.6181, Validation Accuracy:0.2200\n",
    "Epoch #150: Loss:1.5886, Accuracy:0.2579, Validation Loss:1.6196, Validation Accuracy:0.2217\n",
    "Epoch #151: Loss:1.5890, Accuracy:0.2497, Validation Loss:1.6201, Validation Accuracy:0.2315\n",
    "Epoch #152: Loss:1.5880, Accuracy:0.2587, Validation Loss:1.6219, Validation Accuracy:0.2167\n",
    "Epoch #153: Loss:1.5880, Accuracy:0.2628, Validation Loss:1.6194, Validation Accuracy:0.2184\n",
    "Epoch #154: Loss:1.5866, Accuracy:0.2546, Validation Loss:1.6208, Validation Accuracy:0.2085\n",
    "Epoch #155: Loss:1.5868, Accuracy:0.2620, Validation Loss:1.6200, Validation Accuracy:0.2151\n",
    "Epoch #156: Loss:1.5869, Accuracy:0.2624, Validation Loss:1.6185, Validation Accuracy:0.2167\n",
    "Epoch #157: Loss:1.5866, Accuracy:0.2542, Validation Loss:1.6197, Validation Accuracy:0.2118\n",
    "Epoch #158: Loss:1.5858, Accuracy:0.2616, Validation Loss:1.6206, Validation Accuracy:0.2102\n",
    "Epoch #159: Loss:1.5862, Accuracy:0.2612, Validation Loss:1.6226, Validation Accuracy:0.2102\n",
    "Epoch #160: Loss:1.5879, Accuracy:0.2604, Validation Loss:1.6227, Validation Accuracy:0.2085\n",
    "Epoch #161: Loss:1.5851, Accuracy:0.2637, Validation Loss:1.6254, Validation Accuracy:0.2135\n",
    "Epoch #162: Loss:1.5855, Accuracy:0.2591, Validation Loss:1.6246, Validation Accuracy:0.2036\n",
    "Epoch #163: Loss:1.5858, Accuracy:0.2595, Validation Loss:1.6239, Validation Accuracy:0.2053\n",
    "Epoch #164: Loss:1.5849, Accuracy:0.2583, Validation Loss:1.6254, Validation Accuracy:0.1987\n",
    "Epoch #165: Loss:1.5848, Accuracy:0.2575, Validation Loss:1.6239, Validation Accuracy:0.2053\n",
    "Epoch #166: Loss:1.5850, Accuracy:0.2604, Validation Loss:1.6247, Validation Accuracy:0.2053\n",
    "Epoch #167: Loss:1.5848, Accuracy:0.2604, Validation Loss:1.6235, Validation Accuracy:0.2069\n",
    "Epoch #168: Loss:1.5851, Accuracy:0.2579, Validation Loss:1.6252, Validation Accuracy:0.2135\n",
    "Epoch #169: Loss:1.5850, Accuracy:0.2583, Validation Loss:1.6240, Validation Accuracy:0.1987\n",
    "Epoch #170: Loss:1.5849, Accuracy:0.2612, Validation Loss:1.6262, Validation Accuracy:0.2085\n",
    "Epoch #171: Loss:1.5850, Accuracy:0.2567, Validation Loss:1.6236, Validation Accuracy:0.2053\n",
    "Epoch #172: Loss:1.5836, Accuracy:0.2632, Validation Loss:1.6234, Validation Accuracy:0.2085\n",
    "Epoch #173: Loss:1.5845, Accuracy:0.2653, Validation Loss:1.6237, Validation Accuracy:0.2118\n",
    "Epoch #174: Loss:1.5830, Accuracy:0.2674, Validation Loss:1.6218, Validation Accuracy:0.2085\n",
    "Epoch #175: Loss:1.5838, Accuracy:0.2641, Validation Loss:1.6245, Validation Accuracy:0.2102\n",
    "Epoch #176: Loss:1.5839, Accuracy:0.2665, Validation Loss:1.6270, Validation Accuracy:0.1970\n",
    "Epoch #177: Loss:1.5871, Accuracy:0.2628, Validation Loss:1.6288, Validation Accuracy:0.2085\n",
    "Epoch #178: Loss:1.5857, Accuracy:0.2604, Validation Loss:1.6276, Validation Accuracy:0.2085\n",
    "Epoch #179: Loss:1.5833, Accuracy:0.2579, Validation Loss:1.6312, Validation Accuracy:0.1987\n",
    "Epoch #180: Loss:1.5858, Accuracy:0.2637, Validation Loss:1.6294, Validation Accuracy:0.1938\n",
    "Epoch #181: Loss:1.5827, Accuracy:0.2628, Validation Loss:1.6275, Validation Accuracy:0.2053\n",
    "Epoch #182: Loss:1.5835, Accuracy:0.2542, Validation Loss:1.6265, Validation Accuracy:0.2036\n",
    "Epoch #183: Loss:1.5828, Accuracy:0.2620, Validation Loss:1.6254, Validation Accuracy:0.2069\n",
    "Epoch #184: Loss:1.5822, Accuracy:0.2595, Validation Loss:1.6212, Validation Accuracy:0.2282\n",
    "Epoch #185: Loss:1.5820, Accuracy:0.2665, Validation Loss:1.6241, Validation Accuracy:0.2233\n",
    "Epoch #186: Loss:1.5822, Accuracy:0.2649, Validation Loss:1.6236, Validation Accuracy:0.2266\n",
    "Epoch #187: Loss:1.5813, Accuracy:0.2678, Validation Loss:1.6203, Validation Accuracy:0.2299\n",
    "Epoch #188: Loss:1.5825, Accuracy:0.2669, Validation Loss:1.6267, Validation Accuracy:0.2233\n",
    "Epoch #189: Loss:1.5838, Accuracy:0.2661, Validation Loss:1.6263, Validation Accuracy:0.2250\n",
    "Epoch #190: Loss:1.5846, Accuracy:0.2620, Validation Loss:1.6248, Validation Accuracy:0.2250\n",
    "Epoch #191: Loss:1.5825, Accuracy:0.2637, Validation Loss:1.6217, Validation Accuracy:0.2299\n",
    "Epoch #192: Loss:1.5827, Accuracy:0.2542, Validation Loss:1.6192, Validation Accuracy:0.2299\n",
    "Epoch #193: Loss:1.5812, Accuracy:0.2632, Validation Loss:1.6200, Validation Accuracy:0.2315\n",
    "Epoch #194: Loss:1.5828, Accuracy:0.2669, Validation Loss:1.6265, Validation Accuracy:0.2282\n",
    "Epoch #195: Loss:1.5885, Accuracy:0.2530, Validation Loss:1.6272, Validation Accuracy:0.2266\n",
    "Epoch #196: Loss:1.5848, Accuracy:0.2612, Validation Loss:1.6267, Validation Accuracy:0.2315\n",
    "Epoch #197: Loss:1.5836, Accuracy:0.2608, Validation Loss:1.6285, Validation Accuracy:0.2299\n",
    "Epoch #198: Loss:1.5857, Accuracy:0.2575, Validation Loss:1.6261, Validation Accuracy:0.2233\n",
    "Epoch #199: Loss:1.5853, Accuracy:0.2546, Validation Loss:1.6263, Validation Accuracy:0.2250\n",
    "Epoch #200: Loss:1.5859, Accuracy:0.2550, Validation Loss:1.6263, Validation Accuracy:0.2003\n",
    "Epoch #201: Loss:1.5816, Accuracy:0.2620, Validation Loss:1.6253, Validation Accuracy:0.2151\n",
    "Epoch #202: Loss:1.5837, Accuracy:0.2620, Validation Loss:1.6223, Validation Accuracy:0.2250\n",
    "Epoch #203: Loss:1.5825, Accuracy:0.2509, Validation Loss:1.6275, Validation Accuracy:0.1970\n",
    "Epoch #204: Loss:1.5816, Accuracy:0.2604, Validation Loss:1.6238, Validation Accuracy:0.2135\n",
    "Epoch #205: Loss:1.5826, Accuracy:0.2595, Validation Loss:1.6243, Validation Accuracy:0.2200\n",
    "Epoch #206: Loss:1.5826, Accuracy:0.2583, Validation Loss:1.6236, Validation Accuracy:0.2217\n",
    "Epoch #207: Loss:1.5812, Accuracy:0.2645, Validation Loss:1.6224, Validation Accuracy:0.2332\n",
    "Epoch #208: Loss:1.5806, Accuracy:0.2534, Validation Loss:1.6234, Validation Accuracy:0.2135\n",
    "Epoch #209: Loss:1.5808, Accuracy:0.2620, Validation Loss:1.6246, Validation Accuracy:0.2365\n",
    "Epoch #210: Loss:1.5812, Accuracy:0.2661, Validation Loss:1.6263, Validation Accuracy:0.2217\n",
    "Epoch #211: Loss:1.5809, Accuracy:0.2620, Validation Loss:1.6268, Validation Accuracy:0.2167\n",
    "Epoch #212: Loss:1.5822, Accuracy:0.2571, Validation Loss:1.6254, Validation Accuracy:0.2200\n",
    "Epoch #213: Loss:1.5814, Accuracy:0.2608, Validation Loss:1.6253, Validation Accuracy:0.2365\n",
    "Epoch #214: Loss:1.5814, Accuracy:0.2604, Validation Loss:1.6286, Validation Accuracy:0.2266\n",
    "Epoch #215: Loss:1.5807, Accuracy:0.2624, Validation Loss:1.6286, Validation Accuracy:0.2167\n",
    "Epoch #216: Loss:1.5792, Accuracy:0.2653, Validation Loss:1.6316, Validation Accuracy:0.2003\n",
    "Epoch #217: Loss:1.5804, Accuracy:0.2612, Validation Loss:1.6359, Validation Accuracy:0.1938\n",
    "Epoch #218: Loss:1.5825, Accuracy:0.2682, Validation Loss:1.6347, Validation Accuracy:0.2118\n",
    "Epoch #219: Loss:1.5832, Accuracy:0.2538, Validation Loss:1.6304, Validation Accuracy:0.2266\n",
    "Epoch #220: Loss:1.5804, Accuracy:0.2583, Validation Loss:1.6291, Validation Accuracy:0.2200\n",
    "Epoch #221: Loss:1.5811, Accuracy:0.2513, Validation Loss:1.6345, Validation Accuracy:0.2069\n",
    "Epoch #222: Loss:1.5799, Accuracy:0.2559, Validation Loss:1.6326, Validation Accuracy:0.2102\n",
    "Epoch #223: Loss:1.5787, Accuracy:0.2608, Validation Loss:1.6301, Validation Accuracy:0.2250\n",
    "Epoch #224: Loss:1.5761, Accuracy:0.2608, Validation Loss:1.6286, Validation Accuracy:0.2151\n",
    "Epoch #225: Loss:1.5780, Accuracy:0.2591, Validation Loss:1.6271, Validation Accuracy:0.2102\n",
    "Epoch #226: Loss:1.5799, Accuracy:0.2550, Validation Loss:1.6265, Validation Accuracy:0.1954\n",
    "Epoch #227: Loss:1.5791, Accuracy:0.2661, Validation Loss:1.6287, Validation Accuracy:0.2315\n",
    "Epoch #228: Loss:1.5793, Accuracy:0.2661, Validation Loss:1.6287, Validation Accuracy:0.2397\n",
    "Epoch #229: Loss:1.5771, Accuracy:0.2632, Validation Loss:1.6322, Validation Accuracy:0.1905\n",
    "Epoch #230: Loss:1.5771, Accuracy:0.2604, Validation Loss:1.6310, Validation Accuracy:0.2348\n",
    "Epoch #231: Loss:1.5783, Accuracy:0.2624, Validation Loss:1.6328, Validation Accuracy:0.2282\n",
    "Epoch #232: Loss:1.5778, Accuracy:0.2764, Validation Loss:1.6324, Validation Accuracy:0.2200\n",
    "Epoch #233: Loss:1.5775, Accuracy:0.2669, Validation Loss:1.6326, Validation Accuracy:0.2233\n",
    "Epoch #234: Loss:1.5766, Accuracy:0.2604, Validation Loss:1.6352, Validation Accuracy:0.2118\n",
    "Epoch #235: Loss:1.5756, Accuracy:0.2665, Validation Loss:1.6332, Validation Accuracy:0.2069\n",
    "Epoch #236: Loss:1.5760, Accuracy:0.2702, Validation Loss:1.6319, Validation Accuracy:0.2151\n",
    "Epoch #237: Loss:1.5759, Accuracy:0.2715, Validation Loss:1.6329, Validation Accuracy:0.2184\n",
    "Epoch #238: Loss:1.5766, Accuracy:0.2702, Validation Loss:1.6355, Validation Accuracy:0.2167\n",
    "Epoch #239: Loss:1.5792, Accuracy:0.2706, Validation Loss:1.6345, Validation Accuracy:0.2233\n",
    "Epoch #240: Loss:1.5777, Accuracy:0.2608, Validation Loss:1.6319, Validation Accuracy:0.2365\n",
    "Epoch #241: Loss:1.5796, Accuracy:0.2595, Validation Loss:1.6285, Validation Accuracy:0.2397\n",
    "Epoch #242: Loss:1.5779, Accuracy:0.2608, Validation Loss:1.6325, Validation Accuracy:0.2184\n",
    "Epoch #243: Loss:1.5782, Accuracy:0.2657, Validation Loss:1.6305, Validation Accuracy:0.2135\n",
    "Epoch #244: Loss:1.5775, Accuracy:0.2604, Validation Loss:1.6257, Validation Accuracy:0.2282\n",
    "Epoch #245: Loss:1.5781, Accuracy:0.2559, Validation Loss:1.6219, Validation Accuracy:0.2315\n",
    "Epoch #246: Loss:1.5773, Accuracy:0.2653, Validation Loss:1.6266, Validation Accuracy:0.2282\n",
    "Epoch #247: Loss:1.5750, Accuracy:0.2616, Validation Loss:1.6278, Validation Accuracy:0.2217\n",
    "Epoch #248: Loss:1.5771, Accuracy:0.2628, Validation Loss:1.6310, Validation Accuracy:0.2200\n",
    "Epoch #249: Loss:1.5760, Accuracy:0.2686, Validation Loss:1.6345, Validation Accuracy:0.2200\n",
    "Epoch #250: Loss:1.5758, Accuracy:0.2669, Validation Loss:1.6314, Validation Accuracy:0.2217\n",
    "Epoch #251: Loss:1.5752, Accuracy:0.2645, Validation Loss:1.6351, Validation Accuracy:0.2118\n",
    "Epoch #252: Loss:1.5768, Accuracy:0.2604, Validation Loss:1.6334, Validation Accuracy:0.2299\n",
    "Epoch #253: Loss:1.5766, Accuracy:0.2628, Validation Loss:1.6287, Validation Accuracy:0.2282\n",
    "Epoch #254: Loss:1.5762, Accuracy:0.2628, Validation Loss:1.6287, Validation Accuracy:0.2282\n",
    "Epoch #255: Loss:1.5774, Accuracy:0.2669, Validation Loss:1.6255, Validation Accuracy:0.2250\n",
    "Epoch #256: Loss:1.5775, Accuracy:0.2612, Validation Loss:1.6324, Validation Accuracy:0.2217\n",
    "Epoch #257: Loss:1.5757, Accuracy:0.2641, Validation Loss:1.6305, Validation Accuracy:0.2167\n",
    "Epoch #258: Loss:1.5735, Accuracy:0.2706, Validation Loss:1.6369, Validation Accuracy:0.2036\n",
    "Epoch #259: Loss:1.5747, Accuracy:0.2674, Validation Loss:1.6313, Validation Accuracy:0.2217\n",
    "Epoch #260: Loss:1.5749, Accuracy:0.2604, Validation Loss:1.6330, Validation Accuracy:0.2151\n",
    "Epoch #261: Loss:1.5761, Accuracy:0.2653, Validation Loss:1.6293, Validation Accuracy:0.2184\n",
    "Epoch #262: Loss:1.5793, Accuracy:0.2612, Validation Loss:1.6276, Validation Accuracy:0.2233\n",
    "Epoch #263: Loss:1.5798, Accuracy:0.2579, Validation Loss:1.6270, Validation Accuracy:0.2184\n",
    "Epoch #264: Loss:1.5764, Accuracy:0.2637, Validation Loss:1.6243, Validation Accuracy:0.2266\n",
    "Epoch #265: Loss:1.5777, Accuracy:0.2628, Validation Loss:1.6262, Validation Accuracy:0.2250\n",
    "Epoch #266: Loss:1.5790, Accuracy:0.2579, Validation Loss:1.6281, Validation Accuracy:0.2135\n",
    "Epoch #267: Loss:1.5815, Accuracy:0.2513, Validation Loss:1.6198, Validation Accuracy:0.2315\n",
    "Epoch #268: Loss:1.5776, Accuracy:0.2624, Validation Loss:1.6306, Validation Accuracy:0.2003\n",
    "Epoch #269: Loss:1.5799, Accuracy:0.2591, Validation Loss:1.6251, Validation Accuracy:0.2365\n",
    "Epoch #270: Loss:1.5781, Accuracy:0.2649, Validation Loss:1.6254, Validation Accuracy:0.2299\n",
    "Epoch #271: Loss:1.5786, Accuracy:0.2587, Validation Loss:1.6274, Validation Accuracy:0.2069\n",
    "Epoch #272: Loss:1.5752, Accuracy:0.2604, Validation Loss:1.6225, Validation Accuracy:0.2414\n",
    "Epoch #273: Loss:1.5755, Accuracy:0.2710, Validation Loss:1.6264, Validation Accuracy:0.2266\n",
    "Epoch #274: Loss:1.5730, Accuracy:0.2657, Validation Loss:1.6330, Validation Accuracy:0.2200\n",
    "Epoch #275: Loss:1.5748, Accuracy:0.2690, Validation Loss:1.6262, Validation Accuracy:0.2250\n",
    "Epoch #276: Loss:1.5756, Accuracy:0.2669, Validation Loss:1.6241, Validation Accuracy:0.2266\n",
    "Epoch #277: Loss:1.5744, Accuracy:0.2715, Validation Loss:1.6266, Validation Accuracy:0.2053\n",
    "Epoch #278: Loss:1.5723, Accuracy:0.2674, Validation Loss:1.6243, Validation Accuracy:0.2266\n",
    "Epoch #279: Loss:1.5724, Accuracy:0.2678, Validation Loss:1.6287, Validation Accuracy:0.2250\n",
    "Epoch #280: Loss:1.5748, Accuracy:0.2665, Validation Loss:1.6254, Validation Accuracy:0.2266\n",
    "Epoch #281: Loss:1.5750, Accuracy:0.2678, Validation Loss:1.6243, Validation Accuracy:0.2282\n",
    "Epoch #282: Loss:1.5734, Accuracy:0.2690, Validation Loss:1.6261, Validation Accuracy:0.2332\n",
    "Epoch #283: Loss:1.5717, Accuracy:0.2747, Validation Loss:1.6316, Validation Accuracy:0.2151\n",
    "Epoch #284: Loss:1.5716, Accuracy:0.2735, Validation Loss:1.6310, Validation Accuracy:0.2200\n",
    "Epoch #285: Loss:1.5710, Accuracy:0.2669, Validation Loss:1.6308, Validation Accuracy:0.2200\n",
    "Epoch #286: Loss:1.5714, Accuracy:0.2661, Validation Loss:1.6280, Validation Accuracy:0.2250\n",
    "Epoch #287: Loss:1.5690, Accuracy:0.2743, Validation Loss:1.6290, Validation Accuracy:0.2151\n",
    "Epoch #288: Loss:1.5718, Accuracy:0.2723, Validation Loss:1.6335, Validation Accuracy:0.2167\n",
    "Epoch #289: Loss:1.5740, Accuracy:0.2686, Validation Loss:1.6309, Validation Accuracy:0.2233\n",
    "Epoch #290: Loss:1.5726, Accuracy:0.2612, Validation Loss:1.6271, Validation Accuracy:0.2184\n",
    "Epoch #291: Loss:1.5748, Accuracy:0.2575, Validation Loss:1.6245, Validation Accuracy:0.2266\n",
    "Epoch #292: Loss:1.5736, Accuracy:0.2608, Validation Loss:1.6241, Validation Accuracy:0.2282\n",
    "Epoch #293: Loss:1.5726, Accuracy:0.2678, Validation Loss:1.6258, Validation Accuracy:0.2315\n",
    "Epoch #294: Loss:1.5717, Accuracy:0.2608, Validation Loss:1.6291, Validation Accuracy:0.2167\n",
    "Epoch #295: Loss:1.5711, Accuracy:0.2604, Validation Loss:1.6314, Validation Accuracy:0.2135\n",
    "Epoch #296: Loss:1.5700, Accuracy:0.2620, Validation Loss:1.6310, Validation Accuracy:0.2217\n",
    "Epoch #297: Loss:1.5711, Accuracy:0.2715, Validation Loss:1.6392, Validation Accuracy:0.2250\n",
    "Epoch #298: Loss:1.5713, Accuracy:0.2657, Validation Loss:1.6388, Validation Accuracy:0.2266\n",
    "Epoch #299: Loss:1.5711, Accuracy:0.2669, Validation Loss:1.6380, Validation Accuracy:0.2315\n",
    "Epoch #300: Loss:1.5728, Accuracy:0.2591, Validation Loss:1.6301, Validation Accuracy:0.2397\n",
    "\n",
    "Test:\n",
    "Test Loss:1.63009393, Accuracy:0.2397\n",
    "Labels: ['01', '04', '03', '05', '02']\n",
    "Confusion Matrix:\n",
    "      01  04  03   05  02\n",
    "t:01   9  16   1   92   8\n",
    "t:04   7  12   3   82   8\n",
    "t:03   3  14   5   91   2\n",
    "t:05   6  13   1  112  10\n",
    "t:02   7  12   3   84   8\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.28      0.07      0.11       126\n",
    "          04       0.18      0.11      0.13       112\n",
    "          03       0.38      0.04      0.08       115\n",
    "          05       0.24      0.79      0.37       142\n",
    "          02       0.22      0.07      0.11       114\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.26      0.22      0.16       609\n",
    "weighted avg       0.26      0.24      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 15:39:16 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 32 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6092181984818432, 1.6058459278006467, 1.60493020940884, 1.6048835100994516, 1.6049466162479569, 1.6048477484870622, 1.6042326559574145, 1.6048337003867614, 1.6046689474719695, 1.6053696878633672, 1.6052184735221424, 1.6054593518449756, 1.6052765979359693, 1.6049626249397917, 1.604376514948452, 1.6054193997030775, 1.6050815862192114, 1.6049780812365277, 1.605339546900469, 1.605849939418348, 1.6058524443793962, 1.6061741853582447, 1.6046902005895605, 1.6054034503222687, 1.6057466816628116, 1.6055386751547627, 1.6060112241062232, 1.606038714668825, 1.606454089161602, 1.6058520415342108, 1.6065178923614702, 1.6059622707821073, 1.6071185580223848, 1.6085643290690406, 1.6086696063356447, 1.6091820166028779, 1.609357652797292, 1.6090277043860925, 1.6093104997487686, 1.610268834972225, 1.6091550116860025, 1.6118582671107526, 1.6107290812901087, 1.6123750910578887, 1.611890639969085, 1.6125687736595793, 1.6125687033867797, 1.6138851910780607, 1.6111980463287905, 1.61041776632832, 1.610090308197222, 1.609865226377603, 1.6108735016805589, 1.6113908970101518, 1.6119170092988289, 1.6103804062544222, 1.6104176394849379, 1.6127096181628349, 1.6134692919861116, 1.6140959045569885, 1.6152141368252106, 1.6149100008464994, 1.6147245203920186, 1.6147741997378997, 1.6148825637225448, 1.6151756983868202, 1.6162119724088897, 1.6171618764623632, 1.6172492551098903, 1.6181132033932188, 1.6186067770267356, 1.6191969029421878, 1.6191209229733947, 1.6178220312779368, 1.6171918655263966, 1.6162419982731635, 1.6157448888803743, 1.6155861946945316, 1.6179337521100474, 1.6153681928105346, 1.6180753402522046, 1.617855462926166, 1.6187971571983375, 1.6196832504178502, 1.6175155062197857, 1.6181844353480097, 1.6160826808321849, 1.6146412079753156, 1.6154557867786177, 1.6186196692471433, 1.617072251238455, 1.6182910145209928, 1.617590235371895, 1.617161584213645, 1.6179515232985047, 1.616666029631015, 1.6160739505623753, 1.617433951014564, 1.617746820982258, 1.6193189628801519, 1.6177328290610478, 1.6189643614397848, 1.6187353723350613, 1.6168949114669524, 1.6183901185472611, 1.6175278736452752, 1.6185096093195022, 1.6180378498114976, 1.621068161305144, 1.6176161075069009, 1.6186065198165442, 1.6217271782494531, 1.6197418481454082, 1.6188852131268856, 1.6214039580183859, 1.6193049065585208, 1.620098912657188, 1.622720899644548, 1.6213864559806235, 1.6204434930788865, 1.6206630157132453, 1.619842261712148, 1.6197916741050131, 1.6226204250050686, 1.6181998665892627, 1.6213777724940985, 1.6229643831503606, 1.6220168127802206, 1.6240472135872677, 1.6199436792599156, 1.6216326977427566, 1.6221731907041201, 1.6199139096271034, 1.6178423741768146, 1.6202317920615912, 1.6195028047452027, 1.6198010123617739, 1.6211421094308738, 1.6241394982157866, 1.6225197272151952, 1.6220367583147999, 1.621360196855855, 1.6369870762128156, 1.6169721295289414, 1.6217292306458422, 1.6175637513350187, 1.6181184382274234, 1.6184468110793917, 1.618119035252601, 1.6196269734543924, 1.620117465263517, 1.6219075109766818, 1.6194444767555776, 1.62084193437166, 1.619956480458452, 1.618544155545227, 1.6197078380678676, 1.6206467823050488, 1.622617576509861, 1.6226877120915304, 1.6253557038816129, 1.624554898742776, 1.6239367997509309, 1.6253690641306109, 1.623853804051191, 1.6246854468127974, 1.6234663967624283, 1.6251634588382515, 1.6240049878560459, 1.6262310011242018, 1.6236355617911553, 1.6233504395962544, 1.6236574810322477, 1.6217849767462569, 1.6245464526961002, 1.626956727704391, 1.6287722344860458, 1.6276331102515285, 1.631218624036692, 1.6293983974284532, 1.6274673136192785, 1.626504010754853, 1.6253659656678123, 1.621230559591785, 1.624130593340581, 1.6235578308747516, 1.6203049098329592, 1.626692724149607, 1.6263010059475702, 1.6247689894267492, 1.621710322760596, 1.6191671654117128, 1.6199950185315362, 1.6265460556168079, 1.6272080817637578, 1.6266592925013776, 1.6284951944460815, 1.6261060711589745, 1.626287141065488, 1.6262679650082768, 1.6253304473676509, 1.6223051982560182, 1.6275248404206901, 1.6237756188084143, 1.6243414964973437, 1.623618283491025, 1.6224000694716505, 1.623388580109294, 1.624573427076606, 1.6263203176567316, 1.6268041194561862, 1.6253672294037291, 1.6252639792822852, 1.628557528572521, 1.6286180076144992, 1.6315673268683046, 1.6358944326394493, 1.634698099690705, 1.6304164114844035, 1.6290913542307461, 1.6345128433653482, 1.6326444789106622, 1.630121818512727, 1.6286322395202562, 1.6271060943995008, 1.6265214108089703, 1.628699647969213, 1.6286624807050858, 1.6322166569322984, 1.6310442772209155, 1.6328070304663898, 1.6324425270208975, 1.6326089026696968, 1.635177190080652, 1.633156004014665, 1.6318975978688457, 1.6329235989471962, 1.6354642458541444, 1.6345441043866287, 1.631940162436324, 1.628512495844235, 1.6325127430541566, 1.6305183027373942, 1.6257495842935221, 1.621897812155863, 1.6266403002496228, 1.6277756401274983, 1.6310280747405805, 1.6345431554102154, 1.6314377029149598, 1.6351396739972244, 1.6334133020958486, 1.6286718990219442, 1.6286949355809754, 1.6255316693207313, 1.6324240123892848, 1.6304644193555333, 1.636890070191745, 1.6312741158631048, 1.6329831990898145, 1.6293414602138725, 1.6275788923398222, 1.6269568042410614, 1.6242573331729533, 1.6262341420638737, 1.6280967134168778, 1.6198363462692411, 1.6306433667886042, 1.6251267028363858, 1.625374353577938, 1.6274067331808932, 1.6224621387538065, 1.626408284716614, 1.6329591225324984, 1.6261641219723204, 1.6240930942870517, 1.6265860911464847, 1.6242614659574035, 1.628706654695846, 1.6254370698004912, 1.624278928454482, 1.6260570853410292, 1.6316205112413429, 1.6309668364000243, 1.630832421955804, 1.6280163869090465, 1.6289723285509057, 1.6334527222002277, 1.6309119382711075, 1.627145620793936, 1.624526209431916, 1.6241069166922608, 1.625814457636553, 1.6291217261738768, 1.6313852841043708, 1.630980192734103, 1.6392342173211485, 1.6387743315673227, 1.6380259855627426, 1.6300940104501784], 'val_acc': [0.18719211722340295, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2298850552691223, 0.22660098301953283, 0.2298850553669953, 0.2298850552691223, 0.2331691275187118, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2298850553669953, 0.22824301914432757, 0.2331691276165848, 0.2331691275187118, 0.23316912742083884, 0.23645319967042833, 0.2331691275187118, 0.22660098301953283, 0.2249589467968651, 0.23152709149179004, 0.21674876844843816, 0.20853858772659145, 0.23481116354563358, 0.23481116354563358, 0.22988505517124935, 0.2101806238513862, 0.2151067323236434, 0.22003283842248086, 0.2216748745472756, 0.2249589467968651, 0.22331691067207035, 0.22331691294761713, 0.2397372720178908, 0.23809523579522307, 0.2331691275187118, 0.22003283852035385, 0.22003283842248086, 0.23481116364350654, 0.22331691086781633, 0.21510673034171557, 0.21510673034171557, 0.21018062177158536, 0.20525451329932815, 0.21018062177158536, 0.20032840492494391, 0.21346469402117488, 0.20853858564679062, 0.21510673004809663, 0.2134646939233019, 0.21510673014596962, 0.2167487682526922, 0.21674876844843816, 0.21018062394925918, 0.21510673004809663, 0.2068965516996697, 0.21018062394925918, 0.20525451557487495, 0.20689654942412292, 0.2134646939233019, 0.20197044104973866, 0.2101806238513862, 0.23316912959851263, 0.2003284072004907, 0.21182265997618094, 0.2282430213220014, 0.2036124771745334, 0.2068965516996697, 0.2036124771745334, 0.22003283852035385, 0.2134646939233019, 0.20853858564679062, 0.22003283861822684, 0.22003283852035385, 0.2118226579942531, 0.2216748746451486, 0.22495894699261107, 0.2216748768228224, 0.22660098292165984, 0.21018062177158536, 0.21839080229768612, 0.22331691294761713, 0.20853858554891766, 0.22331691076994334, 0.22167487474302158, 0.22003283852035385, 0.21674876844843816, 0.21674876617289138, 0.22003283852035385, 0.2151067323236434, 0.22660098301953283, 0.21182266007405393, 0.21182266007405393, 0.19868637087995, 0.2200328405022817, 0.21839080437748695, 0.19704433465728227, 0.211822659682562, 0.21839080408386802, 0.20689655121030479, 0.20853858733509953, 0.21018062345989427, 0.20689655140605073, 0.2167487678612003, 0.21839080427961396, 0.21839080437748695, 0.22003284030653572, 0.21674876805694623, 0.20853858743297252, 0.20525451498763705, 0.20689655130817777, 0.20525451518338303, 0.21018062365564025, 0.21018062365564025, 0.21018062355776726, 0.20525451508551004, 0.2200328405022817, 0.21346469580735675, 0.22331691067207035, 0.20361247905858829, 0.21018062394925918, 0.21674876617289138, 0.2200328405022817, 0.22495894907241187, 0.20525451557487495, 0.2101806238513862, 0.21346469590522973, 0.19704433475515526, 0.23481116572330737, 0.21839080229768612, 0.2200328405022817, 0.22003283842248086, 0.2183908045732329, 0.22003284060015468, 0.22167487662707644, 0.23152709357159088, 0.21674876815481922, 0.2183908044753599, 0.20853858772659145, 0.21510673004809663, 0.21674876627076436, 0.21182266007405393, 0.2101806238513862, 0.2101806238513862, 0.20853858772659145, 0.21346469600310272, 0.20361247915646125, 0.20525451537912898, 0.19868637087995, 0.20525451537912898, 0.20525451537912898, 0.2068965516017967, 0.21346469600310272, 0.19868637087995, 0.20853858762871846, 0.20525451537912898, 0.20853858762871846, 0.21182265987830795, 0.20853858772659145, 0.2101806237535132, 0.19704433465728227, 0.20853858762871846, 0.20853858723722654, 0.19868637078207702, 0.1937602623098198, 0.20525451537912898, 0.20361247905858829, 0.20689655140605073, 0.2282430212241284, 0.22331691294761713, 0.22660098500146067, 0.22988505734892314, 0.22331691275187118, 0.22495894887666593, 0.22495894877879294, 0.22988505725105016, 0.22988505725105016, 0.23152709327797194, 0.22824302093050947, 0.22660098480571472, 0.23152709298435298, 0.2298850570553042, 0.2233169125561252, 0.22495894877879294, 0.20032840700474475, 0.21510673203002448, 0.22495894868091998, 0.1970443349509012, 0.21346469570948376, 0.22003284030653572, 0.22167487652920345, 0.23316912950063964, 0.21346469610097568, 0.23645320165235617, 0.22167487672494943, 0.21674876844843816, 0.2200328404044087, 0.23645320175022916, 0.22660098480571472, 0.2167487682526922, 0.20032840690687176, 0.19376026250556577, 0.21182265958468902, 0.22660098470784173, 0.22003284030653572, 0.20689655130817777, 0.21018062365564025, 0.2249589489745389, 0.21510673222577043, 0.2101806237535132, 0.19540229655055968, 0.2315270933758449, 0.23973727399981865, 0.1904761900602303, 0.2348111656254344, 0.2282430212241284, 0.2200328404044087, 0.22331691284974417, 0.21182265987830795, 0.20689654942412292, 0.2151067319321515, 0.2183908044753599, 0.21674876844843816, 0.2233169126539982, 0.23645320184810212, 0.2397372740976916, 0.21839080427961396, 0.21346469600310272, 0.2282430213220014, 0.2315270933758449, 0.2282430212241284, 0.22167487662707644, 0.2200328404044087, 0.2200328404044087, 0.22167487672494943, 0.21182265997618094, 0.22988505725105016, 0.22824302102838243, 0.22824302102838243, 0.2249589489745389, 0.22167487652920345, 0.21674876617289138, 0.20361247925433423, 0.22167487652920345, 0.2151067319321515, 0.21839080437748695, 0.2233169126539982, 0.21839080388812204, 0.22660098500146067, 0.2249589489745389, 0.21346469580735675, 0.23152709139391706, 0.20032840690687176, 0.23645320184810212, 0.2298850553669953, 0.2068965516017967, 0.2413793100267404, 0.22660098509933366, 0.2200328404044087, 0.2249589467968651, 0.2266009831174058, 0.20525451339720113, 0.22660098490358768, 0.22495894689473808, 0.22660098490358768, 0.22824301904645458, 0.23316912969638562, 0.21510673222577043, 0.22003283861822684, 0.22003283861822684, 0.22495894887666593, 0.21510673024384258, 0.21674876627076436, 0.22331691067207035, 0.2183908023955591, 0.22660098292165984, 0.22824302102838243, 0.2315270912960441, 0.2167487682526922, 0.2134646956116108, 0.22167487623558452, 0.22495894877879294, 0.22660098509933366, 0.23152709327797194, 0.2397372740976916], 'loss': [1.6151544330056442, 1.607616742288797, 1.6065850721492414, 1.6057290494319594, 1.6052109566067767, 1.6049081819013404, 1.6046782516600904, 1.6048576197340259, 1.6050934858635466, 1.6044054355954243, 1.6044386031446516, 1.6041664657161956, 1.603903239316764, 1.60365389685856, 1.6035266447850567, 1.6037521287401109, 1.6045523005099758, 1.6046548580976483, 1.603340757309289, 1.6032169109007661, 1.6028639231374375, 1.602761161400797, 1.6020897658209048, 1.6032163086857885, 1.6030681326648784, 1.602403149222936, 1.6022962676426225, 1.602065195440022, 1.601098455442785, 1.600482591025883, 1.600351823526731, 1.599656467124422, 1.5986838485670776, 1.598219481679693, 1.5990564764892297, 1.6007677175670678, 1.6018222342771182, 1.6008052131233763, 1.5994040066701432, 1.5990859699445095, 1.5992094340510437, 1.5970370667671032, 1.5981075839095538, 1.5976872989529212, 1.5976273722227594, 1.5982257872881096, 1.597189226777157, 1.5971021982189078, 1.5973429531532146, 1.598052613446355, 1.597982223077966, 1.5971065089443137, 1.5974805645874148, 1.5964049143468084, 1.5960731114205393, 1.595113505279259, 1.5946400037291604, 1.5945348522746343, 1.594183613877032, 1.593779984979414, 1.5950003469259588, 1.5945122561170826, 1.5948270772761632, 1.5945870496898706, 1.5935237265220659, 1.5935608705211224, 1.5923604856526337, 1.5924934501765444, 1.5935315181587266, 1.593244747361608, 1.593941532463998, 1.5940430757201427, 1.5941709380375042, 1.5939628093883977, 1.5943262055179666, 1.594977364843631, 1.5927310680217077, 1.5937810281953282, 1.592942210340402, 1.5927818818748365, 1.593859912972186, 1.5936668116943546, 1.5933170112496284, 1.5937378386452457, 1.5924670031428092, 1.5926982730810648, 1.5929885057453257, 1.5926100180623957, 1.5924794895693017, 1.5925418549249795, 1.5930038835478515, 1.5915184880184197, 1.5912183122223653, 1.592049917939752, 1.5915901619795656, 1.591807028743031, 1.5913881827673628, 1.5908062729747388, 1.5919069593202406, 1.5927720317605585, 1.5916753742973908, 1.5926231207054498, 1.5913013513572896, 1.592656637804709, 1.5926310624430067, 1.591201880825129, 1.59133710273727, 1.5918002793921093, 1.5908555251127396, 1.5921874985313025, 1.5917135284177086, 1.5914482419250926, 1.5903981613182678, 1.5918496830997035, 1.592413392204034, 1.5912928483814184, 1.5913954730396154, 1.5904328422624718, 1.5898529655389961, 1.5895834984965393, 1.5895045516427293, 1.5888811449740212, 1.5887187956294975, 1.5892558654965316, 1.5885101710991203, 1.5873684721071373, 1.585924946747766, 1.586370087942793, 1.5860376144581507, 1.5872894525038388, 1.5862918777877055, 1.588419113168971, 1.589269510284831, 1.589738855469643, 1.5895506762870772, 1.5890265896090248, 1.5880594848852139, 1.5878748013987922, 1.5886988232023171, 1.592145857869722, 1.5902163618643916, 1.5872787143660276, 1.5927957232727896, 1.5946617098070024, 1.5953602530383477, 1.5961519614871766, 1.5908962119531338, 1.5901404790075409, 1.5899153418609493, 1.5886352245812543, 1.5889509782654059, 1.5879757424889158, 1.5879686389370866, 1.5866190361046448, 1.5867616073306825, 1.586865715030772, 1.5866485234403511, 1.5858355881986677, 1.5862246979922974, 1.5878558506955847, 1.5851387630497895, 1.5854775990304026, 1.5858056590787195, 1.5849448379305109, 1.5847730659606276, 1.5849700888079539, 1.5847679486264927, 1.585061125441988, 1.5850471697793604, 1.5849439731124, 1.5849550454768313, 1.583625379333261, 1.5845212540342577, 1.5829551672788615, 1.583805624121758, 1.5839398746372984, 1.5870564198836654, 1.5856914632374257, 1.5833486660305234, 1.5858157810978821, 1.5827282104159284, 1.5834730079286643, 1.5827679991477324, 1.5822179636671314, 1.5819574564144596, 1.5822053569053478, 1.5813358305416068, 1.582482211692622, 1.5838250833370358, 1.584606937510277, 1.582461282555817, 1.5827231835535664, 1.5811614007675672, 1.5827792917188923, 1.5884805774297068, 1.5848073128067737, 1.5835594635969314, 1.5856989472798497, 1.5853492759336436, 1.5859349913665646, 1.5816314992963412, 1.5837467231789653, 1.5825368209051645, 1.5816429626280768, 1.5826137530975029, 1.5825886085048104, 1.581177582779949, 1.5806370165313783, 1.5808485237235161, 1.581182693798684, 1.5808558352429274, 1.5822152356102726, 1.5814403248518645, 1.5813869436173957, 1.580664437850153, 1.579176985409715, 1.5804171316432758, 1.5824536357816974, 1.5832465219301854, 1.5803878853208475, 1.5811421514291784, 1.5799051624548754, 1.5786628791683752, 1.5760772004998929, 1.577969013151447, 1.5799288406019583, 1.5790847219236088, 1.5792909814102205, 1.577068927694395, 1.5770810895387153, 1.578259320033894, 1.5778074631211205, 1.5775357562168912, 1.5765633059722932, 1.5756341697743785, 1.5759765177536793, 1.5759154094562884, 1.5765514200962545, 1.5791898168822334, 1.5776889250263786, 1.5796008386651104, 1.577859560567006, 1.5782486435813825, 1.5775363304287011, 1.578070088039923, 1.5773211065014285, 1.5749668938912895, 1.5770602151353745, 1.5759782676579281, 1.575752799320025, 1.575230759565835, 1.5768037630547243, 1.576551892723146, 1.5762012276071788, 1.5773769440836976, 1.5774559447897534, 1.575688841945092, 1.5735279672199696, 1.5746678613294567, 1.574918090734149, 1.5760752451738047, 1.5793429944549497, 1.5797716901042869, 1.576432683825248, 1.5777315922586335, 1.5790486397439694, 1.5814752933670608, 1.5776093717473243, 1.5799058254739342, 1.5780533314240786, 1.5786076108043443, 1.5751713248493735, 1.5755183835783533, 1.572990571106239, 1.5748251732859524, 1.5756411900510534, 1.5743571301994872, 1.5723311680298322, 1.572382076613957, 1.574757310937807, 1.5750357647940854, 1.5733578584522192, 1.5717424857298208, 1.571613915744993, 1.5709511350067733, 1.5714464924908271, 1.5689786937936865, 1.5717694711881007, 1.5739515943938458, 1.572648007962738, 1.5748425689810845, 1.5735684011995914, 1.572649286904619, 1.5717212485581697, 1.5710754729394305, 1.570012742195286, 1.5710502173131986, 1.5713028451010922, 1.5710768604180652, 1.5728409023989887], 'acc': [0.1823408622944869, 0.21396303869003633, 0.23285420970872686, 0.23285420851541005, 0.2328542085337688, 0.23285420951290053, 0.2328542094945418, 0.23285420990455322, 0.23285420951290053, 0.23285420931707418, 0.23285420912124782, 0.2328542094945418, 0.23285421047367355, 0.23285421047367355, 0.23285420951290053, 0.23285420912124782, 0.23285421010037957, 0.23285420970872686, 0.23285420931707418, 0.23408623996211764, 0.23203285466841359, 0.23655030884047554, 0.23696098489193457, 0.232443531876472, 0.23285420990455322, 0.2328542106694999, 0.23244353168064563, 0.2377823409113796, 0.24188911750208916, 0.23572895223355148, 0.23531827716122417, 0.24353182797436843, 0.24517453725333085, 0.24394250537825315, 0.24558521702549052, 0.23983572823678198, 0.2377823417130437, 0.23778234306546941, 0.24271047350317546, 0.24517453881994164, 0.23942505177531154, 0.2492813142173344, 0.24722792614534406, 0.24558521487140067, 0.24804928273390942, 0.2492813150006398, 0.24928131402150805, 0.24312114975046084, 0.24804928077564592, 0.23778234247799038, 0.2422997941226685, 0.24394250498660047, 0.2439425063573849, 0.24599589327277588, 0.25133470252186857, 0.25010267082425847, 0.2505133464840648, 0.24681724909639458, 0.24845995956867384, 0.24558521586889115, 0.2480492815589513, 0.24476386063275152, 0.24476386159352453, 0.24681724811726283, 0.24599589231200286, 0.2431211491813405, 0.24394250598409092, 0.24681724852727424, 0.2443531837979871, 0.24599589327277588, 0.24640657245745648, 0.24394250559243824, 0.242710471544912, 0.2435318291309678, 0.23778234032390053, 0.24188911711043645, 0.24558521547723844, 0.24517453686167817, 0.24312114896715545, 0.241889116112946, 0.2435318263893989, 0.24928131501899853, 0.24763860354922881, 0.24147843988401935, 0.24804928292973574, 0.24845995800206302, 0.24558521604635877, 0.2505133466982499, 0.24887063562013284, 0.2447638610060455, 0.24681724870474187, 0.25092402470797237, 0.2501026692576477, 0.2501026696676591, 0.2501026700593118, 0.24147843869070254, 0.2472279267144644, 0.2468172469055873, 0.24188911650459868, 0.23983572821842328, 0.24804928038399324, 0.24722792413200442, 0.24681724792143647, 0.24763860356758752, 0.24804928216478908, 0.2501026700409531, 0.24763860374505514, 0.2484599586079008, 0.24640657245745648, 0.24845995919537986, 0.24763860155424788, 0.2480492791723177, 0.25174537988903584, 0.24435318201719122, 0.2431211517087243, 0.24599589307694955, 0.2451745374491572, 0.24271047293405512, 0.25092402468961367, 0.253388091181338, 0.2517453789466216, 0.2509240254729191, 0.2537987686035814, 0.25256673555354564, 0.25338809078968527, 0.2537987686035814, 0.25256673594519835, 0.25338809235629606, 0.25585215532314604, 0.25544147966333974, 0.25379876562946874, 0.2517453807274174, 0.25462012344806834, 0.25215605852295486, 0.25462012540633183, 0.2546201230564157, 0.25503080067448547, 0.2517453789649803, 0.25831622083573863, 0.24928131304237633, 0.25708418839154057, 0.26283367620111736, 0.2537987676244497, 0.25051334667989117, 0.24969199144375154, 0.25462012423137376, 0.26160164295525523, 0.2529774127799628, 0.25133470328681523, 0.25790554380514785, 0.24969199301036232, 0.2587268972421329, 0.26283367582782335, 0.254620123252242, 0.2620123215524568, 0.2624229976039158, 0.254209447200783, 0.261601644521866, 0.26119096455388, 0.26036961010104576, 0.2636550304581253, 0.25913757626770456, 0.2595482550607325, 0.25831622220652306, 0.25749486873282057, 0.2603696107068835, 0.26036960810606485, 0.2579055432176688, 0.25831622142321764, 0.2611909649455327, 0.256673510167633, 0.26324435284005543, 0.265297739718729, 0.2673511309790171, 0.26406571003445856, 0.26652977276876477, 0.2628336750261593, 0.26036960951356675, 0.2579055431809513, 0.26365503006647256, 0.26283367483033293, 0.2542094460258249, 0.262012320573325, 0.2595482564315169, 0.26652977495957203, 0.2648870619048328, 0.26776180464384247, 0.2669404525776419, 0.26611909734150224, 0.26201232135663044, 0.2636550298706462, 0.2542094464358363, 0.26324435518997163, 0.2669404519901628, 0.2529774129757891, 0.2611909649455327, 0.2607802883065946, 0.2574948671662098, 0.25462012344806834, 0.2550308014577909, 0.2620123201816723, 0.26201231838251776, 0.25092402529545144, 0.26036961108017753, 0.2595482550790912, 0.2583162220106967, 0.2644763872608757, 0.2533880902022062, 0.26201232137498914, 0.26611909832063396, 0.2620123217482831, 0.25708418954813994, 0.26078028850242096, 0.26036960951356675, 0.26242299740808944, 0.26529774231954767, 0.2611909641622273, 0.2681724854318513, 0.25379876938688684, 0.25831622338148114, 0.25133470172020445, 0.2558521566939305, 0.26078028850242096, 0.2607802867399838, 0.2591375784217944, 0.25503080067448547, 0.2661190987122867, 0.26611909554234764, 0.26324435460249257, 0.2603696089260877, 0.2624229995621793, 0.2763860350149613, 0.26694045218598916, 0.26036961010104576, 0.26652977198545935, 0.2702258705480877, 0.27145790735554154, 0.2702258741096794, 0.2706365519235756, 0.26078028850242096, 0.25954825603986426, 0.26078028713163653, 0.2657084207025642, 0.26036961049269847, 0.25585215612481016, 0.2652977407345782, 0.26160164115610063, 0.2628336766111288, 0.26858316442070557, 0.26694045336094724, 0.26447638647757027, 0.2603696106885248, 0.2628336744386802, 0.2628336775719018, 0.2669404506193784, 0.2611909649455327, 0.26406570925115314, 0.2706365481661575, 0.2673511299998853, 0.2603696093177404, 0.26529773913124993, 0.2611909661204908, 0.25790554439262686, 0.2636550308681367, 0.26283367620111736, 0.25790554341349514, 0.25133470132855174, 0.2624229985830475, 0.2591375774610214, 0.26488706252902927, 0.2587269008037246, 0.26036960992357816, 0.2710472295416454, 0.26570842031091146, 0.2689938400805119, 0.26694044979935555, 0.2714579069638889, 0.2673511294124063, 0.26776180581880055, 0.266529774372093, 0.267761805855518, 0.26899384125547, 0.2747433257176401, 0.2735112938425624, 0.26694045181269516, 0.2661190971456759, 0.2743326500945512, 0.27227926004593866, 0.2685831616791367, 0.2611909653371854, 0.2574948665787307, 0.2607802877191156, 0.2677618084012605, 0.26078028967737904, 0.2603696102968721, 0.26201231939836694, 0.2714579059847571, 0.26570842109421683, 0.26694045273675077, 0.25913757626770456]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
