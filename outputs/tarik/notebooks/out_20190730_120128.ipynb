{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf19.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 12:01:28 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '0', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ib', 'aa', 'eo', 'eb', 'mb', 'ck', 'eg', 'ek', 'sg', 'by', 'yd', 'ce', 'my', 'sk', 'ds'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000216822DE240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000216FCF87EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7046, Accuracy:0.0612, Validation Loss:2.6986, Validation Accuracy:0.1034\n",
    "Epoch #2: Loss:2.6953, Accuracy:0.0949, Validation Loss:2.6881, Validation Accuracy:0.1149\n",
    "Epoch #3: Loss:2.6854, Accuracy:0.1027, Validation Loss:2.6812, Validation Accuracy:0.1084\n",
    "Epoch #4: Loss:2.6774, Accuracy:0.0994, Validation Loss:2.6714, Validation Accuracy:0.1248\n",
    "Epoch #5: Loss:2.6675, Accuracy:0.1150, Validation Loss:2.6601, Validation Accuracy:0.1330\n",
    "Epoch #6: Loss:2.6569, Accuracy:0.1224, Validation Loss:2.6495, Validation Accuracy:0.1330\n",
    "Epoch #7: Loss:2.6457, Accuracy:0.1236, Validation Loss:2.6402, Validation Accuracy:0.1363\n",
    "Epoch #8: Loss:2.6332, Accuracy:0.1322, Validation Loss:2.6263, Validation Accuracy:0.1478\n",
    "Epoch #9: Loss:2.6161, Accuracy:0.1536, Validation Loss:2.6101, Validation Accuracy:0.1593\n",
    "Epoch #10: Loss:2.5978, Accuracy:0.1556, Validation Loss:2.5941, Validation Accuracy:0.1544\n",
    "Epoch #11: Loss:2.5814, Accuracy:0.1561, Validation Loss:2.5701, Validation Accuracy:0.1642\n",
    "Epoch #12: Loss:2.5601, Accuracy:0.1602, Validation Loss:2.5574, Validation Accuracy:0.1675\n",
    "Epoch #13: Loss:2.5416, Accuracy:0.1602, Validation Loss:2.5411, Validation Accuracy:0.1675\n",
    "Epoch #14: Loss:2.5309, Accuracy:0.1614, Validation Loss:2.5397, Validation Accuracy:0.1691\n",
    "Epoch #15: Loss:2.5208, Accuracy:0.1655, Validation Loss:2.5254, Validation Accuracy:0.1724\n",
    "Epoch #16: Loss:2.5104, Accuracy:0.1647, Validation Loss:2.5085, Validation Accuracy:0.1724\n",
    "Epoch #17: Loss:2.5018, Accuracy:0.1663, Validation Loss:2.5291, Validation Accuracy:0.1609\n",
    "Epoch #18: Loss:2.4969, Accuracy:0.1733, Validation Loss:2.5137, Validation Accuracy:0.1642\n",
    "Epoch #19: Loss:2.4930, Accuracy:0.1651, Validation Loss:2.5142, Validation Accuracy:0.1593\n",
    "Epoch #20: Loss:2.4892, Accuracy:0.1688, Validation Loss:2.5054, Validation Accuracy:0.1675\n",
    "Epoch #21: Loss:2.4838, Accuracy:0.1626, Validation Loss:2.5015, Validation Accuracy:0.1609\n",
    "Epoch #22: Loss:2.4716, Accuracy:0.1684, Validation Loss:2.4943, Validation Accuracy:0.1576\n",
    "Epoch #23: Loss:2.4638, Accuracy:0.1561, Validation Loss:2.4953, Validation Accuracy:0.1675\n",
    "Epoch #24: Loss:2.4696, Accuracy:0.1684, Validation Loss:2.4938, Validation Accuracy:0.1609\n",
    "Epoch #25: Loss:2.4644, Accuracy:0.1692, Validation Loss:2.4919, Validation Accuracy:0.1609\n",
    "Epoch #26: Loss:2.4610, Accuracy:0.1717, Validation Loss:2.4924, Validation Accuracy:0.1642\n",
    "Epoch #27: Loss:2.4634, Accuracy:0.1688, Validation Loss:2.4935, Validation Accuracy:0.1626\n",
    "Epoch #28: Loss:2.4634, Accuracy:0.1634, Validation Loss:2.5038, Validation Accuracy:0.1642\n",
    "Epoch #29: Loss:2.4609, Accuracy:0.1700, Validation Loss:2.4854, Validation Accuracy:0.1494\n",
    "Epoch #30: Loss:2.4619, Accuracy:0.1540, Validation Loss:2.4878, Validation Accuracy:0.1658\n",
    "Epoch #31: Loss:2.4579, Accuracy:0.1733, Validation Loss:2.4900, Validation Accuracy:0.1642\n",
    "Epoch #32: Loss:2.4542, Accuracy:0.1684, Validation Loss:2.4809, Validation Accuracy:0.1576\n",
    "Epoch #33: Loss:2.4514, Accuracy:0.1630, Validation Loss:2.4856, Validation Accuracy:0.1642\n",
    "Epoch #34: Loss:2.4502, Accuracy:0.1713, Validation Loss:2.4806, Validation Accuracy:0.1544\n",
    "Epoch #35: Loss:2.4477, Accuracy:0.1663, Validation Loss:2.4778, Validation Accuracy:0.1544\n",
    "Epoch #36: Loss:2.4476, Accuracy:0.1663, Validation Loss:2.4775, Validation Accuracy:0.1658\n",
    "Epoch #37: Loss:2.4473, Accuracy:0.1729, Validation Loss:2.4861, Validation Accuracy:0.1691\n",
    "Epoch #38: Loss:2.4502, Accuracy:0.1676, Validation Loss:2.4858, Validation Accuracy:0.1642\n",
    "Epoch #39: Loss:2.4499, Accuracy:0.1704, Validation Loss:2.4820, Validation Accuracy:0.1560\n",
    "Epoch #40: Loss:2.4465, Accuracy:0.1655, Validation Loss:2.4808, Validation Accuracy:0.1544\n",
    "Epoch #41: Loss:2.4459, Accuracy:0.1634, Validation Loss:2.4842, Validation Accuracy:0.1691\n",
    "Epoch #42: Loss:2.4448, Accuracy:0.1713, Validation Loss:2.4815, Validation Accuracy:0.1626\n",
    "Epoch #43: Loss:2.4430, Accuracy:0.1717, Validation Loss:2.4807, Validation Accuracy:0.1609\n",
    "Epoch #44: Loss:2.4435, Accuracy:0.1729, Validation Loss:2.4825, Validation Accuracy:0.1576\n",
    "Epoch #45: Loss:2.4416, Accuracy:0.1721, Validation Loss:2.4833, Validation Accuracy:0.1691\n",
    "Epoch #46: Loss:2.4411, Accuracy:0.1737, Validation Loss:2.4817, Validation Accuracy:0.1691\n",
    "Epoch #47: Loss:2.4416, Accuracy:0.1729, Validation Loss:2.4799, Validation Accuracy:0.1642\n",
    "Epoch #48: Loss:2.4426, Accuracy:0.1749, Validation Loss:2.4803, Validation Accuracy:0.1609\n",
    "Epoch #49: Loss:2.4422, Accuracy:0.1713, Validation Loss:2.4781, Validation Accuracy:0.1576\n",
    "Epoch #50: Loss:2.4417, Accuracy:0.1704, Validation Loss:2.4821, Validation Accuracy:0.1708\n",
    "Epoch #51: Loss:2.4407, Accuracy:0.1708, Validation Loss:2.5130, Validation Accuracy:0.1576\n",
    "Epoch #52: Loss:2.4602, Accuracy:0.1589, Validation Loss:2.4874, Validation Accuracy:0.1675\n",
    "Epoch #53: Loss:2.4562, Accuracy:0.1684, Validation Loss:2.5374, Validation Accuracy:0.1527\n",
    "Epoch #54: Loss:2.4833, Accuracy:0.1667, Validation Loss:2.4840, Validation Accuracy:0.1708\n",
    "Epoch #55: Loss:2.4732, Accuracy:0.1655, Validation Loss:2.5044, Validation Accuracy:0.1675\n",
    "Epoch #56: Loss:2.4669, Accuracy:0.1651, Validation Loss:2.4857, Validation Accuracy:0.1741\n",
    "Epoch #57: Loss:2.4572, Accuracy:0.1708, Validation Loss:2.5033, Validation Accuracy:0.1560\n",
    "Epoch #58: Loss:2.4583, Accuracy:0.1700, Validation Loss:2.4867, Validation Accuracy:0.1806\n",
    "Epoch #59: Loss:2.4918, Accuracy:0.1569, Validation Loss:2.4878, Validation Accuracy:0.1708\n",
    "Epoch #60: Loss:2.4577, Accuracy:0.1733, Validation Loss:2.5066, Validation Accuracy:0.1658\n",
    "Epoch #61: Loss:2.4524, Accuracy:0.1692, Validation Loss:2.4794, Validation Accuracy:0.1675\n",
    "Epoch #62: Loss:2.4517, Accuracy:0.1717, Validation Loss:2.4767, Validation Accuracy:0.1790\n",
    "Epoch #63: Loss:2.4421, Accuracy:0.1639, Validation Loss:2.4867, Validation Accuracy:0.1790\n",
    "Epoch #64: Loss:2.4445, Accuracy:0.1708, Validation Loss:2.4787, Validation Accuracy:0.1823\n",
    "Epoch #65: Loss:2.4401, Accuracy:0.1778, Validation Loss:2.4778, Validation Accuracy:0.1790\n",
    "Epoch #66: Loss:2.4406, Accuracy:0.1737, Validation Loss:2.4792, Validation Accuracy:0.1757\n",
    "Epoch #67: Loss:2.4377, Accuracy:0.1791, Validation Loss:2.4812, Validation Accuracy:0.1708\n",
    "Epoch #68: Loss:2.4379, Accuracy:0.1717, Validation Loss:2.4816, Validation Accuracy:0.1691\n",
    "Epoch #69: Loss:2.4375, Accuracy:0.1758, Validation Loss:2.4799, Validation Accuracy:0.1773\n",
    "Epoch #70: Loss:2.4368, Accuracy:0.1807, Validation Loss:2.4794, Validation Accuracy:0.1773\n",
    "Epoch #71: Loss:2.4369, Accuracy:0.1791, Validation Loss:2.4811, Validation Accuracy:0.1724\n",
    "Epoch #72: Loss:2.4359, Accuracy:0.1774, Validation Loss:2.4788, Validation Accuracy:0.1708\n",
    "Epoch #73: Loss:2.4358, Accuracy:0.1749, Validation Loss:2.4785, Validation Accuracy:0.1724\n",
    "Epoch #74: Loss:2.4358, Accuracy:0.1754, Validation Loss:2.4802, Validation Accuracy:0.1691\n",
    "Epoch #75: Loss:2.4354, Accuracy:0.1786, Validation Loss:2.4770, Validation Accuracy:0.1741\n",
    "Epoch #76: Loss:2.4360, Accuracy:0.1778, Validation Loss:2.4768, Validation Accuracy:0.1741\n",
    "Epoch #77: Loss:2.4347, Accuracy:0.1791, Validation Loss:2.4793, Validation Accuracy:0.1741\n",
    "Epoch #78: Loss:2.4343, Accuracy:0.1786, Validation Loss:2.4778, Validation Accuracy:0.1741\n",
    "Epoch #79: Loss:2.4338, Accuracy:0.1799, Validation Loss:2.4774, Validation Accuracy:0.1757\n",
    "Epoch #80: Loss:2.4343, Accuracy:0.1786, Validation Loss:2.4768, Validation Accuracy:0.1741\n",
    "Epoch #81: Loss:2.4337, Accuracy:0.1803, Validation Loss:2.4790, Validation Accuracy:0.1741\n",
    "Epoch #82: Loss:2.4339, Accuracy:0.1791, Validation Loss:2.4778, Validation Accuracy:0.1757\n",
    "Epoch #83: Loss:2.4331, Accuracy:0.1795, Validation Loss:2.4772, Validation Accuracy:0.1757\n",
    "Epoch #84: Loss:2.4333, Accuracy:0.1774, Validation Loss:2.4776, Validation Accuracy:0.1757\n",
    "Epoch #85: Loss:2.4333, Accuracy:0.1807, Validation Loss:2.4790, Validation Accuracy:0.1757\n",
    "Epoch #86: Loss:2.4334, Accuracy:0.1774, Validation Loss:2.4765, Validation Accuracy:0.1757\n",
    "Epoch #87: Loss:2.4325, Accuracy:0.1791, Validation Loss:2.4772, Validation Accuracy:0.1790\n",
    "Epoch #88: Loss:2.4317, Accuracy:0.1791, Validation Loss:2.4746, Validation Accuracy:0.1790\n",
    "Epoch #89: Loss:2.4317, Accuracy:0.1766, Validation Loss:2.4747, Validation Accuracy:0.1823\n",
    "Epoch #90: Loss:2.4309, Accuracy:0.1791, Validation Loss:2.4763, Validation Accuracy:0.1823\n",
    "Epoch #91: Loss:2.4314, Accuracy:0.1803, Validation Loss:2.4751, Validation Accuracy:0.1823\n",
    "Epoch #92: Loss:2.4325, Accuracy:0.1782, Validation Loss:2.4722, Validation Accuracy:0.1823\n",
    "Epoch #93: Loss:2.4316, Accuracy:0.1762, Validation Loss:2.4718, Validation Accuracy:0.1823\n",
    "Epoch #94: Loss:2.4329, Accuracy:0.1791, Validation Loss:2.4746, Validation Accuracy:0.1823\n",
    "Epoch #95: Loss:2.4311, Accuracy:0.1778, Validation Loss:2.4718, Validation Accuracy:0.1839\n",
    "Epoch #96: Loss:2.4326, Accuracy:0.1749, Validation Loss:2.4728, Validation Accuracy:0.1806\n",
    "Epoch #97: Loss:2.4323, Accuracy:0.1799, Validation Loss:2.4767, Validation Accuracy:0.1757\n",
    "Epoch #98: Loss:2.4327, Accuracy:0.1799, Validation Loss:2.4732, Validation Accuracy:0.1790\n",
    "Epoch #99: Loss:2.4321, Accuracy:0.1758, Validation Loss:2.4758, Validation Accuracy:0.1806\n",
    "Epoch #100: Loss:2.4300, Accuracy:0.1791, Validation Loss:2.4772, Validation Accuracy:0.1757\n",
    "Epoch #101: Loss:2.4316, Accuracy:0.1729, Validation Loss:2.4741, Validation Accuracy:0.1757\n",
    "Epoch #102: Loss:2.4323, Accuracy:0.1754, Validation Loss:2.4759, Validation Accuracy:0.1773\n",
    "Epoch #103: Loss:2.4331, Accuracy:0.1708, Validation Loss:2.4729, Validation Accuracy:0.1806\n",
    "Epoch #104: Loss:2.4319, Accuracy:0.1754, Validation Loss:2.4767, Validation Accuracy:0.1741\n",
    "Epoch #105: Loss:2.4311, Accuracy:0.1799, Validation Loss:2.4767, Validation Accuracy:0.1790\n",
    "Epoch #106: Loss:2.4324, Accuracy:0.1733, Validation Loss:2.4758, Validation Accuracy:0.1773\n",
    "Epoch #107: Loss:2.4307, Accuracy:0.1766, Validation Loss:2.4788, Validation Accuracy:0.1724\n",
    "Epoch #108: Loss:2.4309, Accuracy:0.1819, Validation Loss:2.4758, Validation Accuracy:0.1773\n",
    "Epoch #109: Loss:2.4299, Accuracy:0.1770, Validation Loss:2.4761, Validation Accuracy:0.1790\n",
    "Epoch #110: Loss:2.4306, Accuracy:0.1762, Validation Loss:2.4779, Validation Accuracy:0.1757\n",
    "Epoch #111: Loss:2.4303, Accuracy:0.1754, Validation Loss:2.4784, Validation Accuracy:0.1741\n",
    "Epoch #112: Loss:2.4301, Accuracy:0.1782, Validation Loss:2.4768, Validation Accuracy:0.1790\n",
    "Epoch #113: Loss:2.4304, Accuracy:0.1713, Validation Loss:2.4781, Validation Accuracy:0.1790\n",
    "Epoch #114: Loss:2.4300, Accuracy:0.1754, Validation Loss:2.4781, Validation Accuracy:0.1790\n",
    "Epoch #115: Loss:2.4299, Accuracy:0.1770, Validation Loss:2.4803, Validation Accuracy:0.1691\n",
    "Epoch #116: Loss:2.4298, Accuracy:0.1774, Validation Loss:2.4779, Validation Accuracy:0.1806\n",
    "Epoch #117: Loss:2.4294, Accuracy:0.1774, Validation Loss:2.4791, Validation Accuracy:0.1691\n",
    "Epoch #118: Loss:2.4293, Accuracy:0.1795, Validation Loss:2.4780, Validation Accuracy:0.1790\n",
    "Epoch #119: Loss:2.4289, Accuracy:0.1782, Validation Loss:2.4780, Validation Accuracy:0.1741\n",
    "Epoch #120: Loss:2.4286, Accuracy:0.1778, Validation Loss:2.4789, Validation Accuracy:0.1691\n",
    "Epoch #121: Loss:2.4287, Accuracy:0.1786, Validation Loss:2.4790, Validation Accuracy:0.1691\n",
    "Epoch #122: Loss:2.4286, Accuracy:0.1782, Validation Loss:2.4791, Validation Accuracy:0.1757\n",
    "Epoch #123: Loss:2.4282, Accuracy:0.1786, Validation Loss:2.4796, Validation Accuracy:0.1691\n",
    "Epoch #124: Loss:2.4282, Accuracy:0.1786, Validation Loss:2.4787, Validation Accuracy:0.1741\n",
    "Epoch #125: Loss:2.4277, Accuracy:0.1770, Validation Loss:2.4782, Validation Accuracy:0.1757\n",
    "Epoch #126: Loss:2.4289, Accuracy:0.1749, Validation Loss:2.4788, Validation Accuracy:0.1757\n",
    "Epoch #127: Loss:2.4283, Accuracy:0.1791, Validation Loss:2.4833, Validation Accuracy:0.1708\n",
    "Epoch #128: Loss:2.4284, Accuracy:0.1807, Validation Loss:2.4784, Validation Accuracy:0.1806\n",
    "Epoch #129: Loss:2.4274, Accuracy:0.1741, Validation Loss:2.4791, Validation Accuracy:0.1757\n",
    "Epoch #130: Loss:2.4273, Accuracy:0.1778, Validation Loss:2.4807, Validation Accuracy:0.1691\n",
    "Epoch #131: Loss:2.4281, Accuracy:0.1758, Validation Loss:2.4788, Validation Accuracy:0.1757\n",
    "Epoch #132: Loss:2.4274, Accuracy:0.1828, Validation Loss:2.4803, Validation Accuracy:0.1658\n",
    "Epoch #133: Loss:2.4270, Accuracy:0.1828, Validation Loss:2.4796, Validation Accuracy:0.1757\n",
    "Epoch #134: Loss:2.4271, Accuracy:0.1778, Validation Loss:2.4797, Validation Accuracy:0.1757\n",
    "Epoch #135: Loss:2.4269, Accuracy:0.1807, Validation Loss:2.4812, Validation Accuracy:0.1658\n",
    "Epoch #136: Loss:2.4266, Accuracy:0.1856, Validation Loss:2.4789, Validation Accuracy:0.1724\n",
    "Epoch #137: Loss:2.4265, Accuracy:0.1828, Validation Loss:2.4800, Validation Accuracy:0.1724\n",
    "Epoch #138: Loss:2.4263, Accuracy:0.1803, Validation Loss:2.4815, Validation Accuracy:0.1675\n",
    "Epoch #139: Loss:2.4263, Accuracy:0.1791, Validation Loss:2.4801, Validation Accuracy:0.1724\n",
    "Epoch #140: Loss:2.4261, Accuracy:0.1828, Validation Loss:2.4797, Validation Accuracy:0.1708\n",
    "Epoch #141: Loss:2.4258, Accuracy:0.1832, Validation Loss:2.4806, Validation Accuracy:0.1658\n",
    "Epoch #142: Loss:2.4257, Accuracy:0.1828, Validation Loss:2.4801, Validation Accuracy:0.1708\n",
    "Epoch #143: Loss:2.4255, Accuracy:0.1832, Validation Loss:2.4809, Validation Accuracy:0.1724\n",
    "Epoch #144: Loss:2.4265, Accuracy:0.1823, Validation Loss:2.4816, Validation Accuracy:0.1658\n",
    "Epoch #145: Loss:2.4267, Accuracy:0.1799, Validation Loss:2.4801, Validation Accuracy:0.1724\n",
    "Epoch #146: Loss:2.4250, Accuracy:0.1832, Validation Loss:2.4830, Validation Accuracy:0.1658\n",
    "Epoch #147: Loss:2.4265, Accuracy:0.1840, Validation Loss:2.4810, Validation Accuracy:0.1658\n",
    "Epoch #148: Loss:2.4277, Accuracy:0.1803, Validation Loss:2.4796, Validation Accuracy:0.1773\n",
    "Epoch #149: Loss:2.4256, Accuracy:0.1848, Validation Loss:2.4851, Validation Accuracy:0.1658\n",
    "Epoch #150: Loss:2.4259, Accuracy:0.1840, Validation Loss:2.4806, Validation Accuracy:0.1724\n",
    "Epoch #151: Loss:2.4259, Accuracy:0.1828, Validation Loss:2.4800, Validation Accuracy:0.1724\n",
    "Epoch #152: Loss:2.4249, Accuracy:0.1832, Validation Loss:2.4809, Validation Accuracy:0.1658\n",
    "Epoch #153: Loss:2.4260, Accuracy:0.1844, Validation Loss:2.4818, Validation Accuracy:0.1658\n",
    "Epoch #154: Loss:2.4258, Accuracy:0.1836, Validation Loss:2.4804, Validation Accuracy:0.1724\n",
    "Epoch #155: Loss:2.4248, Accuracy:0.1852, Validation Loss:2.4836, Validation Accuracy:0.1658\n",
    "Epoch #156: Loss:2.4253, Accuracy:0.1873, Validation Loss:2.4803, Validation Accuracy:0.1724\n",
    "Epoch #157: Loss:2.4250, Accuracy:0.1832, Validation Loss:2.4805, Validation Accuracy:0.1675\n",
    "Epoch #158: Loss:2.4241, Accuracy:0.1836, Validation Loss:2.4815, Validation Accuracy:0.1658\n",
    "Epoch #159: Loss:2.4245, Accuracy:0.1836, Validation Loss:2.4821, Validation Accuracy:0.1675\n",
    "Epoch #160: Loss:2.4239, Accuracy:0.1848, Validation Loss:2.4808, Validation Accuracy:0.1724\n",
    "Epoch #161: Loss:2.4238, Accuracy:0.1852, Validation Loss:2.4813, Validation Accuracy:0.1708\n",
    "Epoch #162: Loss:2.4237, Accuracy:0.1848, Validation Loss:2.4810, Validation Accuracy:0.1708\n",
    "Epoch #163: Loss:2.4238, Accuracy:0.1823, Validation Loss:2.4819, Validation Accuracy:0.1675\n",
    "Epoch #164: Loss:2.4239, Accuracy:0.1799, Validation Loss:2.4813, Validation Accuracy:0.1724\n",
    "Epoch #165: Loss:2.4240, Accuracy:0.1807, Validation Loss:2.4828, Validation Accuracy:0.1675\n",
    "Epoch #166: Loss:2.4241, Accuracy:0.1832, Validation Loss:2.4815, Validation Accuracy:0.1724\n",
    "Epoch #167: Loss:2.4233, Accuracy:0.1828, Validation Loss:2.4829, Validation Accuracy:0.1675\n",
    "Epoch #168: Loss:2.4236, Accuracy:0.1832, Validation Loss:2.4823, Validation Accuracy:0.1658\n",
    "Epoch #169: Loss:2.4237, Accuracy:0.1877, Validation Loss:2.4817, Validation Accuracy:0.1724\n",
    "Epoch #170: Loss:2.4232, Accuracy:0.1832, Validation Loss:2.4844, Validation Accuracy:0.1658\n",
    "Epoch #171: Loss:2.4235, Accuracy:0.1795, Validation Loss:2.4829, Validation Accuracy:0.1675\n",
    "Epoch #172: Loss:2.4234, Accuracy:0.1823, Validation Loss:2.4822, Validation Accuracy:0.1724\n",
    "Epoch #173: Loss:2.4236, Accuracy:0.1823, Validation Loss:2.4849, Validation Accuracy:0.1675\n",
    "Epoch #174: Loss:2.4240, Accuracy:0.1795, Validation Loss:2.4813, Validation Accuracy:0.1724\n",
    "Epoch #175: Loss:2.4237, Accuracy:0.1856, Validation Loss:2.4848, Validation Accuracy:0.1658\n",
    "Epoch #176: Loss:2.4230, Accuracy:0.1811, Validation Loss:2.4817, Validation Accuracy:0.1675\n",
    "Epoch #177: Loss:2.4229, Accuracy:0.1823, Validation Loss:2.4835, Validation Accuracy:0.1675\n",
    "Epoch #178: Loss:2.4225, Accuracy:0.1832, Validation Loss:2.4838, Validation Accuracy:0.1658\n",
    "Epoch #179: Loss:2.4228, Accuracy:0.1836, Validation Loss:2.4826, Validation Accuracy:0.1724\n",
    "Epoch #180: Loss:2.4229, Accuracy:0.1823, Validation Loss:2.4835, Validation Accuracy:0.1658\n",
    "Epoch #181: Loss:2.4228, Accuracy:0.1803, Validation Loss:2.4826, Validation Accuracy:0.1724\n",
    "Epoch #182: Loss:2.4224, Accuracy:0.1836, Validation Loss:2.4845, Validation Accuracy:0.1658\n",
    "Epoch #183: Loss:2.4223, Accuracy:0.1873, Validation Loss:2.4824, Validation Accuracy:0.1626\n",
    "Epoch #184: Loss:2.4223, Accuracy:0.1811, Validation Loss:2.4829, Validation Accuracy:0.1675\n",
    "Epoch #185: Loss:2.4234, Accuracy:0.1832, Validation Loss:2.4847, Validation Accuracy:0.1675\n",
    "Epoch #186: Loss:2.4239, Accuracy:0.1815, Validation Loss:2.4827, Validation Accuracy:0.1675\n",
    "Epoch #187: Loss:2.4236, Accuracy:0.1832, Validation Loss:2.4869, Validation Accuracy:0.1658\n",
    "Epoch #188: Loss:2.4226, Accuracy:0.1848, Validation Loss:2.4819, Validation Accuracy:0.1724\n",
    "Epoch #189: Loss:2.4226, Accuracy:0.1836, Validation Loss:2.4844, Validation Accuracy:0.1675\n",
    "Epoch #190: Loss:2.4230, Accuracy:0.1832, Validation Loss:2.4860, Validation Accuracy:0.1626\n",
    "Epoch #191: Loss:2.4221, Accuracy:0.1836, Validation Loss:2.4833, Validation Accuracy:0.1675\n",
    "Epoch #192: Loss:2.4217, Accuracy:0.1877, Validation Loss:2.4850, Validation Accuracy:0.1609\n",
    "Epoch #193: Loss:2.4227, Accuracy:0.1869, Validation Loss:2.4843, Validation Accuracy:0.1658\n",
    "Epoch #194: Loss:2.4215, Accuracy:0.1836, Validation Loss:2.4835, Validation Accuracy:0.1626\n",
    "Epoch #195: Loss:2.4216, Accuracy:0.1860, Validation Loss:2.4838, Validation Accuracy:0.1626\n",
    "Epoch #196: Loss:2.4213, Accuracy:0.1823, Validation Loss:2.4831, Validation Accuracy:0.1658\n",
    "Epoch #197: Loss:2.4213, Accuracy:0.1869, Validation Loss:2.4850, Validation Accuracy:0.1609\n",
    "Epoch #198: Loss:2.4215, Accuracy:0.1828, Validation Loss:2.4835, Validation Accuracy:0.1675\n",
    "Epoch #199: Loss:2.4213, Accuracy:0.1840, Validation Loss:2.4848, Validation Accuracy:0.1609\n",
    "Epoch #200: Loss:2.4210, Accuracy:0.1877, Validation Loss:2.4843, Validation Accuracy:0.1626\n",
    "Epoch #201: Loss:2.4216, Accuracy:0.1856, Validation Loss:2.4845, Validation Accuracy:0.1609\n",
    "Epoch #202: Loss:2.4222, Accuracy:0.1860, Validation Loss:2.4836, Validation Accuracy:0.1675\n",
    "Epoch #203: Loss:2.4211, Accuracy:0.1840, Validation Loss:2.4860, Validation Accuracy:0.1609\n",
    "Epoch #204: Loss:2.4216, Accuracy:0.1881, Validation Loss:2.4834, Validation Accuracy:0.1609\n",
    "Epoch #205: Loss:2.4209, Accuracy:0.1864, Validation Loss:2.4857, Validation Accuracy:0.1609\n",
    "Epoch #206: Loss:2.4207, Accuracy:0.1840, Validation Loss:2.4835, Validation Accuracy:0.1675\n",
    "Epoch #207: Loss:2.4204, Accuracy:0.1844, Validation Loss:2.4848, Validation Accuracy:0.1609\n",
    "Epoch #208: Loss:2.4206, Accuracy:0.1807, Validation Loss:2.4845, Validation Accuracy:0.1658\n",
    "Epoch #209: Loss:2.4204, Accuracy:0.1852, Validation Loss:2.4854, Validation Accuracy:0.1609\n",
    "Epoch #210: Loss:2.4204, Accuracy:0.1906, Validation Loss:2.4858, Validation Accuracy:0.1609\n",
    "Epoch #211: Loss:2.4208, Accuracy:0.1848, Validation Loss:2.4844, Validation Accuracy:0.1626\n",
    "Epoch #212: Loss:2.4207, Accuracy:0.1799, Validation Loss:2.4871, Validation Accuracy:0.1609\n",
    "Epoch #213: Loss:2.4214, Accuracy:0.1840, Validation Loss:2.4843, Validation Accuracy:0.1626\n",
    "Epoch #214: Loss:2.4205, Accuracy:0.1860, Validation Loss:2.4855, Validation Accuracy:0.1609\n",
    "Epoch #215: Loss:2.4202, Accuracy:0.1848, Validation Loss:2.4854, Validation Accuracy:0.1609\n",
    "Epoch #216: Loss:2.4207, Accuracy:0.1864, Validation Loss:2.4851, Validation Accuracy:0.1609\n",
    "Epoch #217: Loss:2.4219, Accuracy:0.1823, Validation Loss:2.4845, Validation Accuracy:0.1675\n",
    "Epoch #218: Loss:2.4201, Accuracy:0.1910, Validation Loss:2.4879, Validation Accuracy:0.1609\n",
    "Epoch #219: Loss:2.4202, Accuracy:0.1864, Validation Loss:2.4840, Validation Accuracy:0.1675\n",
    "Epoch #220: Loss:2.4200, Accuracy:0.1864, Validation Loss:2.4837, Validation Accuracy:0.1675\n",
    "Epoch #221: Loss:2.4200, Accuracy:0.1848, Validation Loss:2.4867, Validation Accuracy:0.1609\n",
    "Epoch #222: Loss:2.4213, Accuracy:0.1860, Validation Loss:2.4853, Validation Accuracy:0.1609\n",
    "Epoch #223: Loss:2.4209, Accuracy:0.1844, Validation Loss:2.4839, Validation Accuracy:0.1675\n",
    "Epoch #224: Loss:2.4195, Accuracy:0.1852, Validation Loss:2.4885, Validation Accuracy:0.1609\n",
    "Epoch #225: Loss:2.4207, Accuracy:0.1844, Validation Loss:2.4851, Validation Accuracy:0.1675\n",
    "Epoch #226: Loss:2.4200, Accuracy:0.1844, Validation Loss:2.4853, Validation Accuracy:0.1675\n",
    "Epoch #227: Loss:2.4198, Accuracy:0.1811, Validation Loss:2.4872, Validation Accuracy:0.1609\n",
    "Epoch #228: Loss:2.4199, Accuracy:0.1873, Validation Loss:2.4839, Validation Accuracy:0.1675\n",
    "Epoch #229: Loss:2.4198, Accuracy:0.1889, Validation Loss:2.4855, Validation Accuracy:0.1609\n",
    "Epoch #230: Loss:2.4193, Accuracy:0.1885, Validation Loss:2.4860, Validation Accuracy:0.1609\n",
    "Epoch #231: Loss:2.4196, Accuracy:0.1885, Validation Loss:2.4858, Validation Accuracy:0.1609\n",
    "Epoch #232: Loss:2.4191, Accuracy:0.1840, Validation Loss:2.4845, Validation Accuracy:0.1658\n",
    "Epoch #233: Loss:2.4198, Accuracy:0.1881, Validation Loss:2.4863, Validation Accuracy:0.1609\n",
    "Epoch #234: Loss:2.4190, Accuracy:0.1864, Validation Loss:2.4861, Validation Accuracy:0.1609\n",
    "Epoch #235: Loss:2.4190, Accuracy:0.1840, Validation Loss:2.4848, Validation Accuracy:0.1675\n",
    "Epoch #236: Loss:2.4195, Accuracy:0.1885, Validation Loss:2.4867, Validation Accuracy:0.1609\n",
    "Epoch #237: Loss:2.4187, Accuracy:0.1885, Validation Loss:2.4851, Validation Accuracy:0.1609\n",
    "Epoch #238: Loss:2.4188, Accuracy:0.1860, Validation Loss:2.4850, Validation Accuracy:0.1609\n",
    "Epoch #239: Loss:2.4189, Accuracy:0.1889, Validation Loss:2.4869, Validation Accuracy:0.1609\n",
    "Epoch #240: Loss:2.4188, Accuracy:0.1885, Validation Loss:2.4864, Validation Accuracy:0.1609\n",
    "Epoch #241: Loss:2.4195, Accuracy:0.1840, Validation Loss:2.4858, Validation Accuracy:0.1658\n",
    "Epoch #242: Loss:2.4185, Accuracy:0.1889, Validation Loss:2.4874, Validation Accuracy:0.1609\n",
    "Epoch #243: Loss:2.4183, Accuracy:0.1885, Validation Loss:2.4856, Validation Accuracy:0.1609\n",
    "Epoch #244: Loss:2.4188, Accuracy:0.1869, Validation Loss:2.4851, Validation Accuracy:0.1609\n",
    "Epoch #245: Loss:2.4192, Accuracy:0.1901, Validation Loss:2.4877, Validation Accuracy:0.1609\n",
    "Epoch #246: Loss:2.4192, Accuracy:0.1869, Validation Loss:2.4851, Validation Accuracy:0.1675\n",
    "Epoch #247: Loss:2.4182, Accuracy:0.1856, Validation Loss:2.4877, Validation Accuracy:0.1609\n",
    "Epoch #248: Loss:2.4187, Accuracy:0.1815, Validation Loss:2.4867, Validation Accuracy:0.1609\n",
    "Epoch #249: Loss:2.4184, Accuracy:0.1918, Validation Loss:2.4873, Validation Accuracy:0.1609\n",
    "Epoch #250: Loss:2.4191, Accuracy:0.1856, Validation Loss:2.4843, Validation Accuracy:0.1609\n",
    "Epoch #251: Loss:2.4190, Accuracy:0.1864, Validation Loss:2.4873, Validation Accuracy:0.1609\n",
    "Epoch #252: Loss:2.4182, Accuracy:0.1893, Validation Loss:2.4866, Validation Accuracy:0.1609\n",
    "Epoch #253: Loss:2.4179, Accuracy:0.1885, Validation Loss:2.4864, Validation Accuracy:0.1609\n",
    "Epoch #254: Loss:2.4186, Accuracy:0.1893, Validation Loss:2.4854, Validation Accuracy:0.1658\n",
    "Epoch #255: Loss:2.4180, Accuracy:0.1864, Validation Loss:2.4873, Validation Accuracy:0.1609\n",
    "Epoch #256: Loss:2.4184, Accuracy:0.1828, Validation Loss:2.4870, Validation Accuracy:0.1675\n",
    "Epoch #257: Loss:2.4187, Accuracy:0.1856, Validation Loss:2.4886, Validation Accuracy:0.1609\n",
    "Epoch #258: Loss:2.4188, Accuracy:0.1869, Validation Loss:2.4850, Validation Accuracy:0.1658\n",
    "Epoch #259: Loss:2.4187, Accuracy:0.1864, Validation Loss:2.4894, Validation Accuracy:0.1609\n",
    "Epoch #260: Loss:2.4180, Accuracy:0.1881, Validation Loss:2.4856, Validation Accuracy:0.1658\n",
    "Epoch #261: Loss:2.4188, Accuracy:0.1840, Validation Loss:2.4866, Validation Accuracy:0.1609\n",
    "Epoch #262: Loss:2.4203, Accuracy:0.1852, Validation Loss:2.4889, Validation Accuracy:0.1560\n",
    "Epoch #263: Loss:2.4184, Accuracy:0.1844, Validation Loss:2.4860, Validation Accuracy:0.1675\n",
    "Epoch #264: Loss:2.4195, Accuracy:0.1840, Validation Loss:2.4887, Validation Accuracy:0.1609\n",
    "Epoch #265: Loss:2.4180, Accuracy:0.1832, Validation Loss:2.4852, Validation Accuracy:0.1609\n",
    "Epoch #266: Loss:2.4174, Accuracy:0.1885, Validation Loss:2.4868, Validation Accuracy:0.1609\n",
    "Epoch #267: Loss:2.4172, Accuracy:0.1885, Validation Loss:2.4876, Validation Accuracy:0.1609\n",
    "Epoch #268: Loss:2.4173, Accuracy:0.1897, Validation Loss:2.4857, Validation Accuracy:0.1609\n",
    "Epoch #269: Loss:2.4169, Accuracy:0.1893, Validation Loss:2.4872, Validation Accuracy:0.1609\n",
    "Epoch #270: Loss:2.4171, Accuracy:0.1877, Validation Loss:2.4873, Validation Accuracy:0.1609\n",
    "Epoch #271: Loss:2.4176, Accuracy:0.1881, Validation Loss:2.4854, Validation Accuracy:0.1609\n",
    "Epoch #272: Loss:2.4178, Accuracy:0.1869, Validation Loss:2.4864, Validation Accuracy:0.1609\n",
    "Epoch #273: Loss:2.4164, Accuracy:0.1893, Validation Loss:2.4892, Validation Accuracy:0.1609\n",
    "Epoch #274: Loss:2.4177, Accuracy:0.1844, Validation Loss:2.4866, Validation Accuracy:0.1609\n",
    "Epoch #275: Loss:2.4174, Accuracy:0.1799, Validation Loss:2.4855, Validation Accuracy:0.1658\n",
    "Epoch #276: Loss:2.4171, Accuracy:0.1852, Validation Loss:2.4888, Validation Accuracy:0.1609\n",
    "Epoch #277: Loss:2.4171, Accuracy:0.1889, Validation Loss:2.4874, Validation Accuracy:0.1609\n",
    "Epoch #278: Loss:2.4171, Accuracy:0.1873, Validation Loss:2.4868, Validation Accuracy:0.1609\n",
    "Epoch #279: Loss:2.4170, Accuracy:0.1881, Validation Loss:2.4857, Validation Accuracy:0.1560\n",
    "Epoch #280: Loss:2.4166, Accuracy:0.1893, Validation Loss:2.4868, Validation Accuracy:0.1609\n",
    "Epoch #281: Loss:2.4166, Accuracy:0.1897, Validation Loss:2.4867, Validation Accuracy:0.1658\n",
    "Epoch #282: Loss:2.4171, Accuracy:0.1869, Validation Loss:2.4871, Validation Accuracy:0.1609\n",
    "Epoch #283: Loss:2.4164, Accuracy:0.1885, Validation Loss:2.4882, Validation Accuracy:0.1560\n",
    "Epoch #284: Loss:2.4166, Accuracy:0.1873, Validation Loss:2.4862, Validation Accuracy:0.1609\n",
    "Epoch #285: Loss:2.4166, Accuracy:0.1873, Validation Loss:2.4861, Validation Accuracy:0.1609\n",
    "Epoch #286: Loss:2.4165, Accuracy:0.1885, Validation Loss:2.4866, Validation Accuracy:0.1609\n",
    "Epoch #287: Loss:2.4160, Accuracy:0.1885, Validation Loss:2.4867, Validation Accuracy:0.1609\n",
    "Epoch #288: Loss:2.4160, Accuracy:0.1860, Validation Loss:2.4873, Validation Accuracy:0.1609\n",
    "Epoch #289: Loss:2.4162, Accuracy:0.1869, Validation Loss:2.4865, Validation Accuracy:0.1658\n",
    "Epoch #290: Loss:2.4161, Accuracy:0.1873, Validation Loss:2.4878, Validation Accuracy:0.1560\n",
    "Epoch #291: Loss:2.4159, Accuracy:0.1922, Validation Loss:2.4864, Validation Accuracy:0.1658\n",
    "Epoch #292: Loss:2.4154, Accuracy:0.1901, Validation Loss:2.4879, Validation Accuracy:0.1609\n",
    "Epoch #293: Loss:2.4155, Accuracy:0.1885, Validation Loss:2.4887, Validation Accuracy:0.1609\n",
    "Epoch #294: Loss:2.4153, Accuracy:0.1881, Validation Loss:2.4865, Validation Accuracy:0.1609\n",
    "Epoch #295: Loss:2.4162, Accuracy:0.1893, Validation Loss:2.4859, Validation Accuracy:0.1609\n",
    "Epoch #296: Loss:2.4155, Accuracy:0.1897, Validation Loss:2.4868, Validation Accuracy:0.1609\n",
    "Epoch #297: Loss:2.4150, Accuracy:0.1885, Validation Loss:2.4878, Validation Accuracy:0.1609\n",
    "Epoch #298: Loss:2.4151, Accuracy:0.1877, Validation Loss:2.4878, Validation Accuracy:0.1609\n",
    "Epoch #299: Loss:2.4151, Accuracy:0.1893, Validation Loss:2.4878, Validation Accuracy:0.1609\n",
    "Epoch #300: Loss:2.4151, Accuracy:0.1877, Validation Loss:2.4878, Validation Accuracy:0.1609\n",
    "\n",
    "Test:\n",
    "Test Loss:2.48781371, Accuracy:0.1609\n",
    "Labels: ['ib', 'aa', 'eo', 'eb', 'mb', 'ck', 'eg', 'ek', 'sg', 'by', 'yd', 'ce', 'my', 'sk', 'ds']\n",
    "Confusion Matrix:\n",
    "      ib  aa  eo  eb  mb  ck  eg  ek  sg  by  yd  ce  my  sk  ds\n",
    "t:ib   1   0   6   0   0   0   5   0  21   0  21   0   0   0   0\n",
    "t:aa   0   0   6   1   0   0  15   0   4   2   1   0   0   0   5\n",
    "t:eo   1   0  12   1   0   0   5   0  13   1   1   0   0   0   0\n",
    "t:eb   1   0  12   4   0   0  15   0   6   1   8   0   0   0   3\n",
    "t:mb   0   0  12   6   0   0  12   0  12   2   8   0   0   0   0\n",
    "t:ck   0   1   6   3   0   0   6   0   3   0   0   0   0   0   4\n",
    "t:eg   0   0   4   7   0   0  27   0   3   1   0   0   0   0   8\n",
    "t:ek   0   0  12   4   0   0  10   0  11   3   5   0   0   0   3\n",
    "t:sg   2   0  15   3   0   0   7   0  20   1   3   0   0   0   0\n",
    "t:by   0   0  12   3   0   0  10   0   8   3   3   0   0   0   1\n",
    "t:yd   4   0  12   0   0   0   3   0  17   1  25   0   0   0   0\n",
    "t:ce   1   0   7   2   0   0  10   0   5   0   1   0   0   0   1\n",
    "t:my   0   0   4   1   0   0   4   0   2   0   5   0   0   0   4\n",
    "t:sk   1   0   4   2   0   0  14   0   2   2   2   0   0   0   6\n",
    "t:ds   0   0   6   1   0   0  14   0   4   0   0   0   0   0   6\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ib       0.09      0.02      0.03        54\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          eo       0.09      0.35      0.15        34\n",
    "          eb       0.11      0.08      0.09        50\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eg       0.17      0.54      0.26        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          sg       0.15      0.39      0.22        51\n",
    "          by       0.18      0.07      0.11        40\n",
    "          yd       0.30      0.40      0.34        62\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          my       0.00      0.00      0.00        20\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ds       0.15      0.19      0.17        31\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.08      0.14      0.09       609\n",
    "weighted avg       0.10      0.16      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 12:17:09 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 40 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6985889174080833, 2.6880506705768004, 2.681235069907553, 2.671355859790921, 2.660114069877587, 2.649461566912521, 2.640246702923955, 2.6262689447168057, 2.6100528831356655, 2.5940979873801298, 2.5700539685235233, 2.5574114459684525, 2.541072282493604, 2.53973478401823, 2.525360570556816, 2.50848951324062, 2.5290644865709377, 2.513730895147339, 2.514230294767859, 2.5053919504820223, 2.501487033903501, 2.494279240152519, 2.495297543520998, 2.4938197668354305, 2.491891877013083, 2.492369995915831, 2.493530011137914, 2.5038334019665647, 2.4854272433689664, 2.4878231234151156, 2.4900413643941897, 2.480863752427751, 2.4856385589624663, 2.480636355520665, 2.4778422870855223, 2.4775353828674467, 2.4860835169336477, 2.4858215371963426, 2.48198476879076, 2.480761434448568, 2.4842137844104486, 2.481477634072891, 2.4806824102386074, 2.4824983200611936, 2.4833499880259846, 2.4816527687661556, 2.479948892969216, 2.480275873675918, 2.4780834938700758, 2.482093241022921, 2.513031041093648, 2.487416700385083, 2.5373620407530435, 2.483952317527558, 2.50443082724886, 2.4856829286991866, 2.5033173619819977, 2.486681157536499, 2.4877736736792455, 2.5065865285682367, 2.4793986025310697, 2.4767128117566037, 2.4866537628894174, 2.4786511074341773, 2.4777980567199256, 2.479191121209431, 2.4811551754893535, 2.481564039080014, 2.479876327592947, 2.479443160575403, 2.481099097012299, 2.478801132227204, 2.478471017627685, 2.4801832705687223, 2.4770326954977855, 2.476760296203037, 2.47926320113572, 2.4777744908638186, 2.47742590606702, 2.4768021224167547, 2.4790076289466647, 2.4777521656456054, 2.4772270293462846, 2.477564110544515, 2.4789554591249363, 2.4764965073815706, 2.477217860214033, 2.4746472134770237, 2.474677933456471, 2.4762764094498357, 2.47505282964221, 2.4721677029269866, 2.471849223075829, 2.474560510936042, 2.471771971541281, 2.472819120034404, 2.476698436173312, 2.473249492582625, 2.475811969274762, 2.4772417592297633, 2.4740649318851666, 2.475850698787395, 2.472899477274351, 2.4767498018706373, 2.47665636840908, 2.47580342613809, 2.47881632446264, 2.4758287905080762, 2.476074782889856, 2.477858387973704, 2.4784121838304993, 2.476829498663716, 2.4780681728337983, 2.4780735605455972, 2.480262773181809, 2.477930495695919, 2.479083020894594, 2.477981550940152, 2.478019143951742, 2.4788818680398372, 2.479028798480731, 2.479077818358473, 2.479629153297061, 2.4787081192279685, 2.47823054684794, 2.478759205008571, 2.4832713682271774, 2.4784454547712955, 2.4790699951754416, 2.4807318736766946, 2.478761924311445, 2.4802866600612896, 2.479572817609815, 2.4796943750679006, 2.4811839521029118, 2.4789483034356277, 2.4800182164008984, 2.481516098936986, 2.480100388988877, 2.4796803561337475, 2.4805675805691623, 2.4800566221497133, 2.4808734637763115, 2.4815988446691355, 2.4800626820531386, 2.482952332066002, 2.480998032198751, 2.479587196325042, 2.4851499472932863, 2.4805782780858685, 2.48001708694671, 2.480891516643205, 2.481766402819278, 2.4803778917722905, 2.4835954702937935, 2.48026868549278, 2.4805120313891833, 2.4815477698503066, 2.4820610834851444, 2.480805929462702, 2.481335993470817, 2.480969103294836, 2.481862957645911, 2.481259408647008, 2.4827785554582067, 2.481475586178659, 2.4828909731459343, 2.4823388274275806, 2.48170014946723, 2.484359899765165, 2.482898698064494, 2.482237783363104, 2.4848600514416623, 2.481295196097864, 2.484843945464086, 2.481679775053253, 2.483540820370754, 2.4837706515745968, 2.4825701177218082, 2.483456824996397, 2.4825859567018957, 2.4844681339702386, 2.482367670594765, 2.4828817488133224, 2.484686527346155, 2.482702432203371, 2.4868532405502495, 2.481920551196695, 2.4844137804065825, 2.4860181773237406, 2.4833185015053583, 2.485036378227823, 2.484287121808783, 2.483457269339726, 2.4837555176714567, 2.483059344033302, 2.4849894704489874, 2.4834578601010326, 2.484772888114691, 2.48432880512795, 2.484479847408476, 2.483574166477999, 2.4860063887190544, 2.483410188521462, 2.485732790284556, 2.4834966385501556, 2.4848311824360114, 2.48452210308883, 2.485423310049649, 2.48577789289415, 2.484353830465934, 2.4871302355686433, 2.484321248159424, 2.485512751076609, 2.4853889769912745, 2.485134062900136, 2.484483882515693, 2.487852203826403, 2.484001885102496, 2.4836890959778835, 2.4866808729218732, 2.485269990460626, 2.4839269081360014, 2.4884902992467772, 2.4850734834404804, 2.485307323130089, 2.4871802952489244, 2.4839288053058444, 2.485546286274451, 2.4859778051110126, 2.485769221348128, 2.484483256911605, 2.4862641469989897, 2.4860618505963354, 2.4847843318149962, 2.486676456501527, 2.4851310668124746, 2.484994509341486, 2.4868825801291883, 2.4864146533270777, 2.485814642632145, 2.4873598952990252, 2.4856488328849156, 2.4851334028447596, 2.487679313947806, 2.4850735809219686, 2.4877173673538935, 2.486693623813698, 2.4873347439006435, 2.4842684139759084, 2.487349783062739, 2.4866012097970995, 2.4864054532669644, 2.4853849003859145, 2.487260999742204, 2.4869509107373617, 2.488568780457445, 2.4850427799036936, 2.48942519878519, 2.4855883955368268, 2.486615772122037, 2.48890200820071, 2.4859605168278387, 2.488685247346098, 2.4852044629345973, 2.4867553111954863, 2.487613887035201, 2.4856689829740226, 2.4871988206465647, 2.4873088545399935, 2.4853739699315165, 2.486375176847862, 2.489220579660976, 2.4865621275502474, 2.4854827913744697, 2.4888123675128706, 2.4873601039642184, 2.486799327023511, 2.485725169894339, 2.4867925409025746, 2.4866950738997686, 2.4870744373998033, 2.4881577342993326, 2.486236294306362, 2.486107608563403, 2.486622164402102, 2.4866919889434413, 2.487297949141078, 2.4865442661229027, 2.4877738384973433, 2.4863746330655854, 2.487876687731062, 2.488726834358253, 2.486500173953954, 2.485933342590708, 2.486789589836484, 2.487820277268859, 2.4878378579964973, 2.4878343905525644, 2.487813521292801], 'val_acc': [0.10344827545834293, 0.11494252842977912, 0.10837438373485418, 0.12479474517854759, 0.13300492609614026, 0.13300492609614026, 0.13628899725689286, 0.14778325032620204, 0.1592775033955112, 0.15435139502112696, 0.16420361176989545, 0.16748768411735793, 0.16748768401948494, 0.16912972024215267, 0.17241379249174216, 0.17241379249174216, 0.16091953961817893, 0.16420361176989545, 0.15927750349338418, 0.16748768431310387, 0.16091953942243298, 0.1576354675643354, 0.1674876842152309, 0.1609195397160519, 0.1609195397160519, 0.1642036119656414, 0.16256157574297367, 0.1642036120635144, 0.14942528654886975, 0.16584564818830913, 0.1642036120635144, 0.15763546736858944, 0.1642036119656414, 0.15435139502112696, 0.15435139511899995, 0.16584564818830913, 0.1691297205357716, 0.1642036120635144, 0.1559934312437947, 0.15435139502112696, 0.1691297206336446, 0.16256157603659263, 0.16091953991179786, 0.15763546766220837, 0.16912972082939054, 0.1691297206336446, 0.1642036119656414, 0.16091953961817893, 0.15763546736858944, 0.17077175675843934, 0.15763546727071645, 0.16748768401948494, 0.15270935889633222, 0.17077175666056635, 0.16748768401948494, 0.1740558287144099, 0.15599343095017576, 0.1806239734093348, 0.17077175666056635, 0.16584564818830913, 0.16748768411735793, 0.17898193699092113, 0.178981937578159, 0.1822660093383836, 0.17898193699092113, 0.17569786503495058, 0.17077175675843934, 0.1691297206336446, 0.17733990096399938, 0.17733990096399938, 0.1724137927853611, 0.17077175666056635, 0.17241379258961514, 0.1691297206336446, 0.1740558287144099, 0.1740558287144099, 0.17405582891015584, 0.17405582891015584, 0.17569786483920463, 0.1740558287144099, 0.17405582891015584, 0.17569786483920463, 0.17569786483920463, 0.17569786483920463, 0.17569786483920463, 0.17569786483920463, 0.17898193708879412, 0.17898193708879412, 0.1822660093383836, 0.1822660093383836, 0.1822660093383836, 0.1822660093383836, 0.1822660093383836, 0.1822660093383836, 0.18390804546317835, 0.18062397321358886, 0.17569786474133164, 0.17898193708879412, 0.18062397321358886, 0.17569786483920463, 0.1756978649370776, 0.17733990106187233, 0.18062397331146185, 0.17405582881228285, 0.1789819371866671, 0.17733990106187233, 0.1724137926874881, 0.17733990115974532, 0.1789819371866671, 0.17569786503495058, 0.17405582891015584, 0.1789819371866671, 0.1789819371866671, 0.1789819371866671, 0.16912972043789862, 0.18062397331146185, 0.16912972043789862, 0.1789819371866671, 0.1740558287144099, 0.16912972043789862, 0.16912972043789862, 0.17569786483920463, 0.16912972043789862, 0.1740558287144099, 0.17569786483920463, 0.17569786483920463, 0.17077175656269336, 0.18062397331146185, 0.17569786483920463, 0.16912972043789862, 0.17569786483920463, 0.16584564818830913, 0.17569786483920463, 0.17569786483920463, 0.16584564818830913, 0.17241379258961514, 0.17241379258961514, 0.16748768431310387, 0.17241379258961514, 0.17077175646482037, 0.16584564818830913, 0.17077175646482037, 0.17241379258961514, 0.16584564818830913, 0.17241379258961514, 0.16584564828618212, 0.16584564828618212, 0.17733990106187233, 0.16584564828618212, 0.17241379258961514, 0.1724137926874881, 0.16584564828618212, 0.16584564828618212, 0.1724137926874881, 0.16584564828618212, 0.1724137926874881, 0.16748768441097686, 0.16584564828618212, 0.16748768441097686, 0.1724137926874881, 0.17077175656269336, 0.17077175656269336, 0.16748768450884982, 0.1724137926874881, 0.16748768441097686, 0.1724137926874881, 0.16748768441097686, 0.16584564828618212, 0.1724137926874881, 0.16584564828618212, 0.16748768450884982, 0.1724137926874881, 0.16748768441097686, 0.1724137926874881, 0.16584564828618212, 0.16748768431310387, 0.16748768441097686, 0.16584564828618212, 0.1724137926874881, 0.16584564838405508, 0.1724137926874881, 0.16584564828618212, 0.1625615761344656, 0.16748768431310387, 0.16748768441097686, 0.16748768431310387, 0.16584564828618212, 0.1724137926874881, 0.16748768441097686, 0.16256157603659263, 0.16748768431310387, 0.16091953991179786, 0.16584564828618212, 0.1625615761344656, 0.1625615761344656, 0.16584564818830913, 0.16091953991179786, 0.16748768431310387, 0.16091953991179786, 0.16256157603659263, 0.16091953991179786, 0.16748768431310387, 0.16091954000967085, 0.16091954000967085, 0.16091953991179786, 0.16748768431310387, 0.16091954000967085, 0.16584564818830913, 0.16091954000967085, 0.16091953991179786, 0.1625615761344656, 0.16091953991179786, 0.1625615761344656, 0.16091954000967085, 0.16091953991179786, 0.16091953991179786, 0.16748768431310387, 0.16091953991179786, 0.16748768431310387, 0.16748768431310387, 0.16091953991179786, 0.16091954000967085, 0.16748768431310387, 0.16091953991179786, 0.16748768431310387, 0.16748768431310387, 0.16091953991179786, 0.16748768431310387, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16584564818830913, 0.16091953991179786, 0.16091954000967085, 0.16748768431310387, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16584564818830913, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16748768431310387, 0.16091953991179786, 0.16091954000967085, 0.16091953991179786, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16584564818830913, 0.16091953991179786, 0.16748768431310387, 0.16091953991179786, 0.16584564818830913, 0.16091954000967085, 0.16584564818830913, 0.16091953991179786, 0.15599343163528662, 0.16748768431310387, 0.16091953991179786, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091953991179786, 0.16091953991179786, 0.16584564818830913, 0.16091953991179786, 0.16091954000967085, 0.16091954000967085, 0.1559934317331596, 0.16091954000967085, 0.16584564818830913, 0.16091954000967085, 0.15599343163528662, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091953991179786, 0.16584564818830913, 0.15599343163528662, 0.16584564818830913, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085, 0.16091954000967085], 'loss': [2.704577855946347, 2.6952831081296384, 2.685401309833879, 2.677413483175164, 2.667512234329443, 2.656930976432941, 2.6457311326718185, 2.633229691683634, 2.6160928274816557, 2.597839868240043, 2.5813517930816086, 2.560092072907904, 2.541623535587068, 2.530871578702202, 2.520783899158423, 2.510439507329733, 2.5018028620576955, 2.4968542141101686, 2.493020075400507, 2.4892042128701966, 2.4837977933198276, 2.471602418829039, 2.4638018800003083, 2.4695833909193348, 2.464445194424545, 2.460990129408161, 2.46341701601565, 2.463371627776285, 2.46088139447833, 2.4619225690007456, 2.457899817793766, 2.4541749976743663, 2.451365133966998, 2.4502091608497887, 2.44769057238616, 2.4476140304267773, 2.4473267938077328, 2.450247314186801, 2.4498592979854137, 2.4465410603634874, 2.4459359100467126, 2.444783935018144, 2.4430002309458456, 2.443531421469467, 2.4415728364392226, 2.4410620659039006, 2.4416188895090403, 2.4425703892717614, 2.4422438030125426, 2.4417486385643117, 2.440748297458312, 2.4601603113405512, 2.4561514919053846, 2.4832936156701746, 2.4732239272804963, 2.4669236284996203, 2.457181071451802, 2.458277324778343, 2.491776955348021, 2.4576593525355848, 2.4523709668271105, 2.4517082200647624, 2.4420733397011882, 2.4445153238347426, 2.4400745994990856, 2.440560717749155, 2.437723942214214, 2.4379285805524007, 2.4375410176890098, 2.4368458730239397, 2.4368706489735317, 2.4358785641022043, 2.4358494536343054, 2.4357612279895884, 2.4353694670009416, 2.4359996550871363, 2.4347201425681613, 2.4342713346226748, 2.4338035523769057, 2.4343442302954514, 2.4336527626617244, 2.4339166793979903, 2.433120795686632, 2.4333284090187024, 2.43334457732324, 2.4333722264370143, 2.4324639921560425, 2.4316516407216597, 2.4316932295382143, 2.430932568328826, 2.431388872996493, 2.432451869330122, 2.431641695974299, 2.4329386978423573, 2.431111046127225, 2.4326224886171626, 2.432268691797276, 2.4327174326722383, 2.432086190159071, 2.4299553168138193, 2.43164832283584, 2.4323223034704, 2.4330961533885227, 2.4318526000702407, 2.431098505505791, 2.4323916258018854, 2.430659339462218, 2.4309105711551173, 2.4298854547849182, 2.4305648825251835, 2.4302861856973634, 2.430118243748158, 2.430383012916518, 2.4300046369525194, 2.4298901272015896, 2.429803885767347, 2.429432897939819, 2.429298339878999, 2.428893573572993, 2.428556753037157, 2.428672449397845, 2.428553090007398, 2.4282340006661856, 2.4281517497812697, 2.427743949733476, 2.4288570089751444, 2.4282622805366283, 2.4283980227593767, 2.427443615073296, 2.4272798813344028, 2.428076562548565, 2.4273640177333133, 2.427012062758146, 2.4271108814333497, 2.4268729244169513, 2.4265665097403084, 2.4265170857157305, 2.4262714177431266, 2.4262755709262356, 2.4261324679827054, 2.4257695198059084, 2.425747118740356, 2.4254966470495143, 2.4264635775368317, 2.4266592020861175, 2.424969868003955, 2.4265287618617504, 2.427694008237772, 2.4255975394278337, 2.4258617944541165, 2.4259125266966146, 2.4249329099909724, 2.4259583819818205, 2.4257787309877683, 2.4247581726716527, 2.425340273248097, 2.4249916340046593, 2.4240979466839736, 2.4244979985685564, 2.4239045370286005, 2.4238379728133186, 2.4237480994857066, 2.4238105167843234, 2.4239497216085635, 2.423985423885087, 2.424112327877256, 2.4232794304403193, 2.4236276378866584, 2.4237448601262526, 2.4231565587084885, 2.4234980698728465, 2.423370483817506, 2.423630601130961, 2.424014382489653, 2.423702254285558, 2.4230287633147816, 2.422866025548696, 2.422521297447001, 2.4228464927516677, 2.422850615287953, 2.4228343299771726, 2.4224015930105285, 2.4223409889659844, 2.4222958391451983, 2.4234169450873466, 2.4239071983086746, 2.423562235411188, 2.4225734011593296, 2.4225786635028754, 2.423017950273392, 2.4221221370618693, 2.4217190017935186, 2.4226903602083114, 2.421458628437113, 2.421631087121043, 2.421259690212273, 2.4213064954510948, 2.421545627719323, 2.421263154869941, 2.421040974309557, 2.4215770837952224, 2.422178229071521, 2.4211192037045834, 2.421593856028218, 2.420898526796815, 2.4207257156254576, 2.4203901434825923, 2.4205969644033445, 2.4204477126103896, 2.420378066773777, 2.420792253795835, 2.420660719783399, 2.421407908923327, 2.4205118962626684, 2.4202004446386067, 2.4207382769065715, 2.42193041856284, 2.4200938300185624, 2.4201638552687252, 2.41997720965125, 2.41997810575262, 2.4213048158485053, 2.420940285101074, 2.4194873596363733, 2.4206911823343202, 2.420001366985407, 2.419795790149446, 2.419896419728806, 2.419757635931215, 2.4192928443454376, 2.4196206212288547, 2.419135397807284, 2.419765067639047, 2.419029042559238, 2.418978212697305, 2.4195461488602343, 2.4187245260273897, 2.4187540440099196, 2.418900681619037, 2.418800901534376, 2.4195405858253305, 2.4185334181149147, 2.4183417713862423, 2.418843968937774, 2.4191828228364978, 2.4192288288100787, 2.4182246891624875, 2.418703115255681, 2.4184268320855176, 2.4190640922444557, 2.4189968898311043, 2.4181956002354865, 2.417941015212198, 2.4185565178888777, 2.4180076595204567, 2.418441266151914, 2.4186752060845156, 2.418780991477888, 2.418683474068769, 2.4179824272954735, 2.41876668136957, 2.420258767599932, 2.4184406919890606, 2.419531962318342, 2.4180388506433066, 2.4173752584006998, 2.417169440600417, 2.4173267044325875, 2.4168927742960027, 2.417086624756486, 2.4176132021009065, 2.417770069678461, 2.4164347582039647, 2.417720136701204, 2.4173637494903817, 2.4171198726434726, 2.417100877585597, 2.417061450691928, 2.4169620254446103, 2.4166471263955995, 2.416627168851222, 2.4171330577294197, 2.4164213175646334, 2.4165936974284583, 2.416550930324766, 2.416485615971152, 2.416034887752494, 2.4160023167392803, 2.416215601691965, 2.4160911847923328, 2.4159218877247963, 2.4154233543779817, 2.415546777165156, 2.4152925210322196, 2.4161710138927983, 2.4154853295986167, 2.4149911980364602, 2.4151159040736956, 2.4150604726842295, 2.4151071482860087], 'acc': [0.06119096563398471, 0.0948665301198832, 0.10266940406154558, 0.09938398311698705, 0.11498973260364004, 0.1223819303071964, 0.12361396196808903, 0.1322381925350342, 0.15359343024127536, 0.15564681676501366, 0.15605749536221522, 0.16016427138380446, 0.16016427056378163, 0.16139630379128506, 0.16550307999034192, 0.16468172434419087, 0.16632443641979835, 0.17330595571280016, 0.1650924031555775, 0.16878850111236807, 0.16262833725133227, 0.1683778232984719, 0.15605749438308347, 0.16837782312100427, 0.1691991779471325, 0.1716632452588796, 0.16878850210985855, 0.16344969188163413, 0.17002053437658893, 0.15400410568689663, 0.17330595395036302, 0.16837782449178873, 0.16303901467357573, 0.17125256682078696, 0.16632443520812284, 0.1663244362056133, 0.17289527709723987, 0.167556468080691, 0.17043121040968925, 0.16550307999034192, 0.16344969148998142, 0.1712525658232965, 0.1716632444572155, 0.17289527786218656, 0.17207392225275295, 0.17371663293921727, 0.1728952767055872, 0.17494866501012132, 0.1712525678182774, 0.17043121160300606, 0.17084188841941175, 0.1589322381195836, 0.16837782451014743, 0.16673511401950947, 0.1655030799719832, 0.16509240137478165, 0.17084189000438127, 0.17002053357492483, 0.15687884981504946, 0.1733059551253211, 0.16919917776966487, 0.1716632444572155, 0.1638603693038776, 0.1708418902002076, 0.1778234091015567, 0.1737166323333795, 0.17905544138664584, 0.17166324345972503, 0.17577001983624954, 0.18069815225057778, 0.17905544119081948, 0.17741273089600784, 0.17494866518758895, 0.1753593420407121, 0.1786447637502173, 0.1778234085324364, 0.17905544117246075, 0.17864476355439093, 0.17987679623113276, 0.17864476435605506, 0.18028747461414923, 0.17905544234741885, 0.1794661185947042, 0.17741273071854022, 0.18069815103890224, 0.17741273052271386, 0.1790554409766344, 0.17905544236577756, 0.17659137646153233, 0.1790554413682871, 0.18028747381248514, 0.17823408730710558, 0.1761806984518098, 0.17905544177829852, 0.1778234091015567, 0.1749486646001099, 0.179876795625295, 0.17987679582112134, 0.17577002063791364, 0.17905544236577756, 0.1728952767055872, 0.17535934399897557, 0.17084188882942317, 0.17535934300148512, 0.17987679560693628, 0.17330595453784206, 0.17659137587405327, 0.18193018448059073, 0.17700205366959074, 0.17618069862927743, 0.17535934398061687, 0.17823408533048335, 0.17125256603748157, 0.17535934182652702, 0.17700205347376438, 0.17741273130601926, 0.17741273071854022, 0.17946611939636833, 0.1782340853672008, 0.17782340851407766, 0.17864476355439093, 0.1782340857404948, 0.17864476277108554, 0.17864476414187, 0.17700205427542848, 0.17494866579342672, 0.1790554409766344, 0.18069815244640414, 0.1741273109489398, 0.17782340968903576, 0.17577002083374, 0.1827515409284059, 0.18275154051839448, 0.1778234083182513, 0.18069815244640414, 0.1856262842548946, 0.1827515397350891, 0.18028747383084384, 0.1790554417599398, 0.1827515397350891, 0.18316221794063794, 0.18275154053675322, 0.1831622185464757, 0.1823408627044983, 0.17987679701443815, 0.1831622173531589, 0.183983573586789, 0.1802874732433648, 0.1848049291962226, 0.18398357298095122, 0.18275154034092686, 0.18316221754898526, 0.18439425061737977, 0.18357289636037188, 0.18521560544350799, 0.18726899470881514, 0.18316221815482303, 0.18357289436539095, 0.18357289438374969, 0.1848049276112531, 0.18521560485602895, 0.18480492704213278, 0.18234086211701928, 0.17987679740609086, 0.18069815225057778, 0.18316221833229065, 0.18275154032256813, 0.1831622181364643, 0.18767967152522086, 0.18316221874230207, 0.1794661193780096, 0.18234086211701928, 0.18234086309615102, 0.17946611978802104, 0.18562628325740416, 0.1811088286753308, 0.1823408625270307, 0.18316221795899668, 0.18357289438374969, 0.18234086331033608, 0.18028747383084384, 0.18357289636037188, 0.18726899392550975, 0.18110882847950444, 0.1831622173531589, 0.1815195082700228, 0.1831622173531589, 0.1848049272196004, 0.18357289536288143, 0.18316221874230207, 0.18357289614618683, 0.18767967134775324, 0.186858316894919, 0.183572894579576, 0.1860369618546057, 0.18234086250867199, 0.18685831710910406, 0.18275154032256813, 0.18398357317677758, 0.18767967056444784, 0.18562628404070955, 0.18603696009216858, 0.183983573586789, 0.1880903499633135, 0.18644763810189108, 0.18398357378261535, 0.1843942511865001, 0.18069815184056637, 0.1852156058535194, 0.19055441446005686, 0.18480492704213278, 0.17987679560693628, 0.18398357258929854, 0.18603696126712665, 0.1848049286271023, 0.18644763927684918, 0.18234086250867199, 0.1909650928797908, 0.18644763908102283, 0.1864476382793587, 0.1848049280029058, 0.18603696165877937, 0.18439424979735694, 0.18521560622681338, 0.18439425100903248, 0.18439425159651152, 0.1811088294769949, 0.18726899276891038, 0.18891170381030997, 0.1885010277404922, 0.18850102580058747, 0.18398357298095122, 0.18809034818251766, 0.18644763751441204, 0.1839835725709398, 0.18850102758138332, 0.18850102736719826, 0.18603696165877937, 0.18891170400613633, 0.18850102699390428, 0.18398357217928712, 0.18891170478944172, 0.18850102658389287, 0.18685831669909264, 0.19014373666451942, 0.18685831554249327, 0.18562628325740416, 0.18151950768254377, 0.19178644811593043, 0.18562628284739274, 0.18644763868937012, 0.18932238279916422, 0.18850102736719826, 0.18932238103672708, 0.18644763888519647, 0.18275153953926274, 0.18562628345323048, 0.18685831728657168, 0.18644763927684918, 0.18809034994495477, 0.18398357337260393, 0.18521560681429242, 0.18439425018900962, 0.18398357180599315, 0.18316221756734397, 0.18850102658389287, 0.18850102734883953, 0.18973305844061183, 0.18932238221168518, 0.18767967173940592, 0.18809034994495477, 0.1868583165032663, 0.18932238103672708, 0.18439425081320612, 0.17987679582112134, 0.1852156062451721, 0.1889117032044722, 0.18726899312384565, 0.18809035014078113, 0.18932238297663184, 0.18973306002558135, 0.18685831669909264, 0.1885010259964138, 0.18726899351549833, 0.1872689937113247, 0.18850102697554555, 0.18850102736719826, 0.1860369602879949, 0.1868583165032663, 0.18726899469045644, 0.19219712455904214, 0.19014373645033436, 0.1885010267613605, 0.1880903479866913, 0.18932238180167377, 0.18973305845897057, 0.18850102560476112, 0.18767967173940592, 0.18932238101836837, 0.18767967193523227]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
