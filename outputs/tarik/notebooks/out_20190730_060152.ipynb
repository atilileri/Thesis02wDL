{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf5.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 06:01:52 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '1', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '01', '05', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002700224BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000027071396EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6163, Accuracy:0.2329, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6060, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6063, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6040, Accuracy:0.2345, Validation Loss:1.6009, Validation Accuracy:0.2479\n",
    "Epoch #12: Loss:1.6036, Accuracy:0.2370, Validation Loss:1.6005, Validation Accuracy:0.2463\n",
    "Epoch #13: Loss:1.6034, Accuracy:0.2337, Validation Loss:1.6003, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6000, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6035, Accuracy:0.2337, Validation Loss:1.5993, Validation Accuracy:0.2512\n",
    "Epoch #16: Loss:1.6030, Accuracy:0.2370, Validation Loss:1.5990, Validation Accuracy:0.2496\n",
    "Epoch #17: Loss:1.6026, Accuracy:0.2382, Validation Loss:1.5988, Validation Accuracy:0.2479\n",
    "Epoch #18: Loss:1.6026, Accuracy:0.2378, Validation Loss:1.5985, Validation Accuracy:0.2479\n",
    "Epoch #19: Loss:1.6024, Accuracy:0.2382, Validation Loss:1.5981, Validation Accuracy:0.2496\n",
    "Epoch #20: Loss:1.6021, Accuracy:0.2390, Validation Loss:1.5976, Validation Accuracy:0.2529\n",
    "Epoch #21: Loss:1.6019, Accuracy:0.2407, Validation Loss:1.5972, Validation Accuracy:0.2496\n",
    "Epoch #22: Loss:1.6018, Accuracy:0.2402, Validation Loss:1.5969, Validation Accuracy:0.2496\n",
    "Epoch #23: Loss:1.6016, Accuracy:0.2382, Validation Loss:1.5963, Validation Accuracy:0.2512\n",
    "Epoch #24: Loss:1.6016, Accuracy:0.2407, Validation Loss:1.5961, Validation Accuracy:0.2512\n",
    "Epoch #25: Loss:1.6009, Accuracy:0.2411, Validation Loss:1.5957, Validation Accuracy:0.2496\n",
    "Epoch #26: Loss:1.6010, Accuracy:0.2402, Validation Loss:1.5956, Validation Accuracy:0.2512\n",
    "Epoch #27: Loss:1.6010, Accuracy:0.2415, Validation Loss:1.5956, Validation Accuracy:0.2479\n",
    "Epoch #28: Loss:1.6004, Accuracy:0.2419, Validation Loss:1.5951, Validation Accuracy:0.2479\n",
    "Epoch #29: Loss:1.6004, Accuracy:0.2402, Validation Loss:1.5948, Validation Accuracy:0.2496\n",
    "Epoch #30: Loss:1.5999, Accuracy:0.2407, Validation Loss:1.5946, Validation Accuracy:0.2512\n",
    "Epoch #31: Loss:1.5995, Accuracy:0.2398, Validation Loss:1.5945, Validation Accuracy:0.2512\n",
    "Epoch #32: Loss:1.5997, Accuracy:0.2415, Validation Loss:1.5945, Validation Accuracy:0.2545\n",
    "Epoch #33: Loss:1.5991, Accuracy:0.2402, Validation Loss:1.5948, Validation Accuracy:0.2463\n",
    "Epoch #34: Loss:1.5988, Accuracy:0.2435, Validation Loss:1.5941, Validation Accuracy:0.2529\n",
    "Epoch #35: Loss:1.5986, Accuracy:0.2431, Validation Loss:1.5941, Validation Accuracy:0.2512\n",
    "Epoch #36: Loss:1.5981, Accuracy:0.2435, Validation Loss:1.5939, Validation Accuracy:0.2512\n",
    "Epoch #37: Loss:1.5981, Accuracy:0.2427, Validation Loss:1.5940, Validation Accuracy:0.2496\n",
    "Epoch #38: Loss:1.5974, Accuracy:0.2423, Validation Loss:1.5940, Validation Accuracy:0.2463\n",
    "Epoch #39: Loss:1.5978, Accuracy:0.2349, Validation Loss:1.5941, Validation Accuracy:0.2463\n",
    "Epoch #40: Loss:1.5987, Accuracy:0.2361, Validation Loss:1.5939, Validation Accuracy:0.2562\n",
    "Epoch #41: Loss:1.5968, Accuracy:0.2402, Validation Loss:1.5963, Validation Accuracy:0.2414\n",
    "Epoch #42: Loss:1.5962, Accuracy:0.2390, Validation Loss:1.5939, Validation Accuracy:0.2529\n",
    "Epoch #43: Loss:1.5960, Accuracy:0.2460, Validation Loss:1.5942, Validation Accuracy:0.2463\n",
    "Epoch #44: Loss:1.5954, Accuracy:0.2435, Validation Loss:1.5941, Validation Accuracy:0.2479\n",
    "Epoch #45: Loss:1.5948, Accuracy:0.2452, Validation Loss:1.5952, Validation Accuracy:0.2414\n",
    "Epoch #46: Loss:1.5945, Accuracy:0.2444, Validation Loss:1.5942, Validation Accuracy:0.2496\n",
    "Epoch #47: Loss:1.5954, Accuracy:0.2448, Validation Loss:1.5946, Validation Accuracy:0.2381\n",
    "Epoch #48: Loss:1.5934, Accuracy:0.2501, Validation Loss:1.5962, Validation Accuracy:0.2299\n",
    "Epoch #49: Loss:1.5927, Accuracy:0.2517, Validation Loss:1.5949, Validation Accuracy:0.2562\n",
    "Epoch #50: Loss:1.5939, Accuracy:0.2526, Validation Loss:1.5946, Validation Accuracy:0.2381\n",
    "Epoch #51: Loss:1.5919, Accuracy:0.2517, Validation Loss:1.5964, Validation Accuracy:0.2266\n",
    "Epoch #52: Loss:1.5914, Accuracy:0.2554, Validation Loss:1.5957, Validation Accuracy:0.2365\n",
    "Epoch #53: Loss:1.5921, Accuracy:0.2513, Validation Loss:1.5964, Validation Accuracy:0.2299\n",
    "Epoch #54: Loss:1.5918, Accuracy:0.2493, Validation Loss:1.5955, Validation Accuracy:0.2414\n",
    "Epoch #55: Loss:1.5919, Accuracy:0.2530, Validation Loss:1.5958, Validation Accuracy:0.2332\n",
    "Epoch #56: Loss:1.5960, Accuracy:0.2501, Validation Loss:1.5983, Validation Accuracy:0.2266\n",
    "Epoch #57: Loss:1.5926, Accuracy:0.2460, Validation Loss:1.5985, Validation Accuracy:0.2233\n",
    "Epoch #58: Loss:1.5911, Accuracy:0.2583, Validation Loss:1.5994, Validation Accuracy:0.2282\n",
    "Epoch #59: Loss:1.5902, Accuracy:0.2509, Validation Loss:1.5978, Validation Accuracy:0.2299\n",
    "Epoch #60: Loss:1.5894, Accuracy:0.2550, Validation Loss:1.5967, Validation Accuracy:0.2430\n",
    "Epoch #61: Loss:1.5882, Accuracy:0.2595, Validation Loss:1.5989, Validation Accuracy:0.2282\n",
    "Epoch #62: Loss:1.5881, Accuracy:0.2591, Validation Loss:1.5969, Validation Accuracy:0.2348\n",
    "Epoch #63: Loss:1.5870, Accuracy:0.2591, Validation Loss:1.5985, Validation Accuracy:0.2250\n",
    "Epoch #64: Loss:1.5854, Accuracy:0.2641, Validation Loss:1.5984, Validation Accuracy:0.2447\n",
    "Epoch #65: Loss:1.5859, Accuracy:0.2612, Validation Loss:1.5998, Validation Accuracy:0.2266\n",
    "Epoch #66: Loss:1.5847, Accuracy:0.2641, Validation Loss:1.5988, Validation Accuracy:0.2332\n",
    "Epoch #67: Loss:1.5846, Accuracy:0.2575, Validation Loss:1.6020, Validation Accuracy:0.2250\n",
    "Epoch #68: Loss:1.5841, Accuracy:0.2620, Validation Loss:1.6010, Validation Accuracy:0.2381\n",
    "Epoch #69: Loss:1.5815, Accuracy:0.2702, Validation Loss:1.6015, Validation Accuracy:0.2250\n",
    "Epoch #70: Loss:1.5810, Accuracy:0.2715, Validation Loss:1.6050, Validation Accuracy:0.2217\n",
    "Epoch #71: Loss:1.5795, Accuracy:0.2682, Validation Loss:1.6038, Validation Accuracy:0.2381\n",
    "Epoch #72: Loss:1.5787, Accuracy:0.2669, Validation Loss:1.6075, Validation Accuracy:0.2348\n",
    "Epoch #73: Loss:1.5781, Accuracy:0.2649, Validation Loss:1.6077, Validation Accuracy:0.2447\n",
    "Epoch #74: Loss:1.5796, Accuracy:0.2776, Validation Loss:1.6117, Validation Accuracy:0.2282\n",
    "Epoch #75: Loss:1.5781, Accuracy:0.2690, Validation Loss:1.6083, Validation Accuracy:0.2348\n",
    "Epoch #76: Loss:1.5828, Accuracy:0.2682, Validation Loss:1.6082, Validation Accuracy:0.2217\n",
    "Epoch #77: Loss:1.5746, Accuracy:0.2797, Validation Loss:1.6151, Validation Accuracy:0.2266\n",
    "Epoch #78: Loss:1.5756, Accuracy:0.2715, Validation Loss:1.6138, Validation Accuracy:0.2282\n",
    "Epoch #79: Loss:1.5735, Accuracy:0.2797, Validation Loss:1.6108, Validation Accuracy:0.2200\n",
    "Epoch #80: Loss:1.5735, Accuracy:0.2825, Validation Loss:1.6119, Validation Accuracy:0.2365\n",
    "Epoch #81: Loss:1.5736, Accuracy:0.2719, Validation Loss:1.6113, Validation Accuracy:0.2266\n",
    "Epoch #82: Loss:1.5717, Accuracy:0.2789, Validation Loss:1.6145, Validation Accuracy:0.2365\n",
    "Epoch #83: Loss:1.5737, Accuracy:0.2752, Validation Loss:1.6134, Validation Accuracy:0.2496\n",
    "Epoch #84: Loss:1.5672, Accuracy:0.2854, Validation Loss:1.6151, Validation Accuracy:0.2250\n",
    "Epoch #85: Loss:1.5665, Accuracy:0.2854, Validation Loss:1.6176, Validation Accuracy:0.2282\n",
    "Epoch #86: Loss:1.5630, Accuracy:0.2838, Validation Loss:1.6142, Validation Accuracy:0.2365\n",
    "Epoch #87: Loss:1.5603, Accuracy:0.2977, Validation Loss:1.6183, Validation Accuracy:0.2332\n",
    "Epoch #88: Loss:1.5609, Accuracy:0.3010, Validation Loss:1.6229, Validation Accuracy:0.2348\n",
    "Epoch #89: Loss:1.5619, Accuracy:0.2887, Validation Loss:1.6203, Validation Accuracy:0.2348\n",
    "Epoch #90: Loss:1.5578, Accuracy:0.2982, Validation Loss:1.6218, Validation Accuracy:0.2200\n",
    "Epoch #91: Loss:1.5552, Accuracy:0.3027, Validation Loss:1.6226, Validation Accuracy:0.2315\n",
    "Epoch #92: Loss:1.5570, Accuracy:0.2961, Validation Loss:1.6294, Validation Accuracy:0.2266\n",
    "Epoch #93: Loss:1.5588, Accuracy:0.3035, Validation Loss:1.6310, Validation Accuracy:0.2217\n",
    "Epoch #94: Loss:1.5527, Accuracy:0.2990, Validation Loss:1.6269, Validation Accuracy:0.2233\n",
    "Epoch #95: Loss:1.5482, Accuracy:0.3138, Validation Loss:1.6295, Validation Accuracy:0.2250\n",
    "Epoch #96: Loss:1.5459, Accuracy:0.3051, Validation Loss:1.6324, Validation Accuracy:0.2282\n",
    "Epoch #97: Loss:1.5434, Accuracy:0.3080, Validation Loss:1.6367, Validation Accuracy:0.2250\n",
    "Epoch #98: Loss:1.5418, Accuracy:0.3080, Validation Loss:1.6384, Validation Accuracy:0.2315\n",
    "Epoch #99: Loss:1.5438, Accuracy:0.3121, Validation Loss:1.6436, Validation Accuracy:0.2332\n",
    "Epoch #100: Loss:1.5413, Accuracy:0.3055, Validation Loss:1.6438, Validation Accuracy:0.2250\n",
    "Epoch #101: Loss:1.5387, Accuracy:0.3076, Validation Loss:1.6387, Validation Accuracy:0.2233\n",
    "Epoch #102: Loss:1.5369, Accuracy:0.3117, Validation Loss:1.6490, Validation Accuracy:0.2250\n",
    "Epoch #103: Loss:1.5467, Accuracy:0.3097, Validation Loss:1.6500, Validation Accuracy:0.2332\n",
    "Epoch #104: Loss:1.5336, Accuracy:0.3253, Validation Loss:1.6402, Validation Accuracy:0.2414\n",
    "Epoch #105: Loss:1.5316, Accuracy:0.3257, Validation Loss:1.6492, Validation Accuracy:0.2282\n",
    "Epoch #106: Loss:1.5285, Accuracy:0.3207, Validation Loss:1.6501, Validation Accuracy:0.2348\n",
    "Epoch #107: Loss:1.5275, Accuracy:0.3248, Validation Loss:1.6544, Validation Accuracy:0.2167\n",
    "Epoch #108: Loss:1.5250, Accuracy:0.3240, Validation Loss:1.6590, Validation Accuracy:0.2233\n",
    "Epoch #109: Loss:1.5226, Accuracy:0.3281, Validation Loss:1.6724, Validation Accuracy:0.2315\n",
    "Epoch #110: Loss:1.5240, Accuracy:0.3195, Validation Loss:1.6629, Validation Accuracy:0.2167\n",
    "Epoch #111: Loss:1.5262, Accuracy:0.3183, Validation Loss:1.6689, Validation Accuracy:0.2348\n",
    "Epoch #112: Loss:1.5238, Accuracy:0.3154, Validation Loss:1.6647, Validation Accuracy:0.2069\n",
    "Epoch #113: Loss:1.5188, Accuracy:0.3220, Validation Loss:1.6678, Validation Accuracy:0.2365\n",
    "Epoch #114: Loss:1.5195, Accuracy:0.3281, Validation Loss:1.6632, Validation Accuracy:0.2315\n",
    "Epoch #115: Loss:1.5157, Accuracy:0.3306, Validation Loss:1.6677, Validation Accuracy:0.2315\n",
    "Epoch #116: Loss:1.5136, Accuracy:0.3343, Validation Loss:1.6765, Validation Accuracy:0.2233\n",
    "Epoch #117: Loss:1.5102, Accuracy:0.3388, Validation Loss:1.6783, Validation Accuracy:0.2184\n",
    "Epoch #118: Loss:1.5039, Accuracy:0.3363, Validation Loss:1.6798, Validation Accuracy:0.2250\n",
    "Epoch #119: Loss:1.5040, Accuracy:0.3446, Validation Loss:1.6894, Validation Accuracy:0.2233\n",
    "Epoch #120: Loss:1.5002, Accuracy:0.3487, Validation Loss:1.6819, Validation Accuracy:0.2381\n",
    "Epoch #121: Loss:1.4994, Accuracy:0.3507, Validation Loss:1.6819, Validation Accuracy:0.2233\n",
    "Epoch #122: Loss:1.4953, Accuracy:0.3437, Validation Loss:1.6952, Validation Accuracy:0.2332\n",
    "Epoch #123: Loss:1.4945, Accuracy:0.3520, Validation Loss:1.7069, Validation Accuracy:0.2414\n",
    "Epoch #124: Loss:1.5019, Accuracy:0.3450, Validation Loss:1.6992, Validation Accuracy:0.2381\n",
    "Epoch #125: Loss:1.4958, Accuracy:0.3466, Validation Loss:1.6951, Validation Accuracy:0.1921\n",
    "Epoch #126: Loss:1.5024, Accuracy:0.3384, Validation Loss:1.7093, Validation Accuracy:0.2430\n",
    "Epoch #127: Loss:1.4890, Accuracy:0.3528, Validation Loss:1.6862, Validation Accuracy:0.2151\n",
    "Epoch #128: Loss:1.4901, Accuracy:0.3520, Validation Loss:1.7243, Validation Accuracy:0.2282\n",
    "Epoch #129: Loss:1.4882, Accuracy:0.3548, Validation Loss:1.6968, Validation Accuracy:0.2167\n",
    "Epoch #130: Loss:1.4899, Accuracy:0.3507, Validation Loss:1.7142, Validation Accuracy:0.2332\n",
    "Epoch #131: Loss:1.4850, Accuracy:0.3507, Validation Loss:1.6983, Validation Accuracy:0.2348\n",
    "Epoch #132: Loss:1.4813, Accuracy:0.3618, Validation Loss:1.7087, Validation Accuracy:0.2397\n",
    "Epoch #133: Loss:1.4802, Accuracy:0.3610, Validation Loss:1.7034, Validation Accuracy:0.2250\n",
    "Epoch #134: Loss:1.4769, Accuracy:0.3688, Validation Loss:1.7000, Validation Accuracy:0.2381\n",
    "Epoch #135: Loss:1.4727, Accuracy:0.3733, Validation Loss:1.7224, Validation Accuracy:0.2397\n",
    "Epoch #136: Loss:1.4683, Accuracy:0.3667, Validation Loss:1.7202, Validation Accuracy:0.2397\n",
    "Epoch #137: Loss:1.4635, Accuracy:0.3795, Validation Loss:1.7276, Validation Accuracy:0.2332\n",
    "Epoch #138: Loss:1.4704, Accuracy:0.3663, Validation Loss:1.7183, Validation Accuracy:0.2365\n",
    "Epoch #139: Loss:1.4717, Accuracy:0.3610, Validation Loss:1.7527, Validation Accuracy:0.2003\n",
    "Epoch #140: Loss:1.4745, Accuracy:0.3626, Validation Loss:1.7159, Validation Accuracy:0.2250\n",
    "Epoch #141: Loss:1.4621, Accuracy:0.3741, Validation Loss:1.7333, Validation Accuracy:0.2381\n",
    "Epoch #142: Loss:1.4538, Accuracy:0.3762, Validation Loss:1.7211, Validation Accuracy:0.2167\n",
    "Epoch #143: Loss:1.4510, Accuracy:0.3823, Validation Loss:1.7427, Validation Accuracy:0.2397\n",
    "Epoch #144: Loss:1.4511, Accuracy:0.3762, Validation Loss:1.7385, Validation Accuracy:0.2282\n",
    "Epoch #145: Loss:1.4462, Accuracy:0.3828, Validation Loss:1.7358, Validation Accuracy:0.2250\n",
    "Epoch #146: Loss:1.4441, Accuracy:0.3910, Validation Loss:1.7542, Validation Accuracy:0.2184\n",
    "Epoch #147: Loss:1.4461, Accuracy:0.3819, Validation Loss:1.7625, Validation Accuracy:0.2233\n",
    "Epoch #148: Loss:1.4448, Accuracy:0.3807, Validation Loss:1.7486, Validation Accuracy:0.2167\n",
    "Epoch #149: Loss:1.4418, Accuracy:0.3840, Validation Loss:1.7585, Validation Accuracy:0.2282\n",
    "Epoch #150: Loss:1.4368, Accuracy:0.3984, Validation Loss:1.7557, Validation Accuracy:0.2381\n",
    "Epoch #151: Loss:1.4331, Accuracy:0.3963, Validation Loss:1.7596, Validation Accuracy:0.2200\n",
    "Epoch #152: Loss:1.4350, Accuracy:0.3967, Validation Loss:1.7902, Validation Accuracy:0.2332\n",
    "Epoch #153: Loss:1.4420, Accuracy:0.3856, Validation Loss:1.7556, Validation Accuracy:0.2151\n",
    "Epoch #154: Loss:1.4380, Accuracy:0.3930, Validation Loss:1.7563, Validation Accuracy:0.2135\n",
    "Epoch #155: Loss:1.4241, Accuracy:0.4086, Validation Loss:1.7834, Validation Accuracy:0.2217\n",
    "Epoch #156: Loss:1.4311, Accuracy:0.3996, Validation Loss:1.7622, Validation Accuracy:0.2315\n",
    "Epoch #157: Loss:1.4278, Accuracy:0.3934, Validation Loss:1.7538, Validation Accuracy:0.2069\n",
    "Epoch #158: Loss:1.4234, Accuracy:0.4062, Validation Loss:1.7648, Validation Accuracy:0.2118\n",
    "Epoch #159: Loss:1.4223, Accuracy:0.3971, Validation Loss:1.7859, Validation Accuracy:0.2233\n",
    "Epoch #160: Loss:1.4231, Accuracy:0.3984, Validation Loss:1.7882, Validation Accuracy:0.2315\n",
    "Epoch #161: Loss:1.4228, Accuracy:0.3926, Validation Loss:1.7635, Validation Accuracy:0.2003\n",
    "Epoch #162: Loss:1.4153, Accuracy:0.4045, Validation Loss:1.7858, Validation Accuracy:0.2036\n",
    "Epoch #163: Loss:1.4221, Accuracy:0.3984, Validation Loss:1.7829, Validation Accuracy:0.2282\n",
    "Epoch #164: Loss:1.4130, Accuracy:0.4021, Validation Loss:1.7689, Validation Accuracy:0.2102\n",
    "Epoch #165: Loss:1.4161, Accuracy:0.4045, Validation Loss:1.7886, Validation Accuracy:0.2102\n",
    "Epoch #166: Loss:1.4144, Accuracy:0.4099, Validation Loss:1.7935, Validation Accuracy:0.2184\n",
    "Epoch #167: Loss:1.4178, Accuracy:0.4090, Validation Loss:1.7779, Validation Accuracy:0.2085\n",
    "Epoch #168: Loss:1.4132, Accuracy:0.4045, Validation Loss:1.7902, Validation Accuracy:0.2085\n",
    "Epoch #169: Loss:1.3999, Accuracy:0.4090, Validation Loss:1.7778, Validation Accuracy:0.2200\n",
    "Epoch #170: Loss:1.4010, Accuracy:0.4131, Validation Loss:1.7999, Validation Accuracy:0.2069\n",
    "Epoch #171: Loss:1.4045, Accuracy:0.4123, Validation Loss:1.8090, Validation Accuracy:0.2135\n",
    "Epoch #172: Loss:1.3959, Accuracy:0.4193, Validation Loss:1.7853, Validation Accuracy:0.2135\n",
    "Epoch #173: Loss:1.3906, Accuracy:0.4218, Validation Loss:1.8030, Validation Accuracy:0.2151\n",
    "Epoch #174: Loss:1.3884, Accuracy:0.4189, Validation Loss:1.8062, Validation Accuracy:0.2118\n",
    "Epoch #175: Loss:1.3876, Accuracy:0.4259, Validation Loss:1.8300, Validation Accuracy:0.2135\n",
    "Epoch #176: Loss:1.3895, Accuracy:0.4246, Validation Loss:1.8206, Validation Accuracy:0.2217\n",
    "Epoch #177: Loss:1.3831, Accuracy:0.4238, Validation Loss:1.8109, Validation Accuracy:0.2102\n",
    "Epoch #178: Loss:1.3839, Accuracy:0.4230, Validation Loss:1.8195, Validation Accuracy:0.2036\n",
    "Epoch #179: Loss:1.3850, Accuracy:0.4172, Validation Loss:1.8295, Validation Accuracy:0.1987\n",
    "Epoch #180: Loss:1.3857, Accuracy:0.4209, Validation Loss:1.8091, Validation Accuracy:0.2036\n",
    "Epoch #181: Loss:1.3838, Accuracy:0.4197, Validation Loss:1.8494, Validation Accuracy:0.2118\n",
    "Epoch #182: Loss:1.3852, Accuracy:0.4193, Validation Loss:1.8106, Validation Accuracy:0.2135\n",
    "Epoch #183: Loss:1.3816, Accuracy:0.4341, Validation Loss:1.8036, Validation Accuracy:0.2053\n",
    "Epoch #184: Loss:1.3959, Accuracy:0.4131, Validation Loss:1.8134, Validation Accuracy:0.2003\n",
    "Epoch #185: Loss:1.3796, Accuracy:0.4271, Validation Loss:1.8298, Validation Accuracy:0.2102\n",
    "Epoch #186: Loss:1.3706, Accuracy:0.4287, Validation Loss:1.8011, Validation Accuracy:0.2069\n",
    "Epoch #187: Loss:1.3738, Accuracy:0.4279, Validation Loss:1.8072, Validation Accuracy:0.2299\n",
    "Epoch #188: Loss:1.3676, Accuracy:0.4361, Validation Loss:1.8568, Validation Accuracy:0.2102\n",
    "Epoch #189: Loss:1.3721, Accuracy:0.4345, Validation Loss:1.8207, Validation Accuracy:0.2003\n",
    "Epoch #190: Loss:1.3698, Accuracy:0.4423, Validation Loss:1.8206, Validation Accuracy:0.2102\n",
    "Epoch #191: Loss:1.3661, Accuracy:0.4411, Validation Loss:1.8337, Validation Accuracy:0.2200\n",
    "Epoch #192: Loss:1.3540, Accuracy:0.4472, Validation Loss:1.8562, Validation Accuracy:0.2053\n",
    "Epoch #193: Loss:1.3581, Accuracy:0.4378, Validation Loss:1.8227, Validation Accuracy:0.2085\n",
    "Epoch #194: Loss:1.3528, Accuracy:0.4431, Validation Loss:1.8499, Validation Accuracy:0.2151\n",
    "Epoch #195: Loss:1.3495, Accuracy:0.4485, Validation Loss:1.8359, Validation Accuracy:0.2053\n",
    "Epoch #196: Loss:1.3519, Accuracy:0.4448, Validation Loss:1.8377, Validation Accuracy:0.1970\n",
    "Epoch #197: Loss:1.3581, Accuracy:0.4353, Validation Loss:1.8962, Validation Accuracy:0.2085\n",
    "Epoch #198: Loss:1.3619, Accuracy:0.4398, Validation Loss:1.8528, Validation Accuracy:0.2036\n",
    "Epoch #199: Loss:1.3660, Accuracy:0.4300, Validation Loss:1.8272, Validation Accuracy:0.2167\n",
    "Epoch #200: Loss:1.3558, Accuracy:0.4439, Validation Loss:1.8590, Validation Accuracy:0.2036\n",
    "Epoch #201: Loss:1.3505, Accuracy:0.4600, Validation Loss:1.8621, Validation Accuracy:0.2003\n",
    "Epoch #202: Loss:1.3411, Accuracy:0.4550, Validation Loss:1.8454, Validation Accuracy:0.2053\n",
    "Epoch #203: Loss:1.3432, Accuracy:0.4468, Validation Loss:1.8621, Validation Accuracy:0.2036\n",
    "Epoch #204: Loss:1.3380, Accuracy:0.4550, Validation Loss:1.8559, Validation Accuracy:0.2003\n",
    "Epoch #205: Loss:1.3321, Accuracy:0.4600, Validation Loss:1.8636, Validation Accuracy:0.2135\n",
    "Epoch #206: Loss:1.3301, Accuracy:0.4575, Validation Loss:1.8730, Validation Accuracy:0.2053\n",
    "Epoch #207: Loss:1.3291, Accuracy:0.4620, Validation Loss:1.8731, Validation Accuracy:0.2200\n",
    "Epoch #208: Loss:1.3242, Accuracy:0.4637, Validation Loss:1.8764, Validation Accuracy:0.1921\n",
    "Epoch #209: Loss:1.3229, Accuracy:0.4674, Validation Loss:1.8746, Validation Accuracy:0.2020\n",
    "Epoch #210: Loss:1.3220, Accuracy:0.4628, Validation Loss:1.9031, Validation Accuracy:0.2102\n",
    "Epoch #211: Loss:1.3308, Accuracy:0.4612, Validation Loss:1.8840, Validation Accuracy:0.2053\n",
    "Epoch #212: Loss:1.3174, Accuracy:0.4665, Validation Loss:1.8864, Validation Accuracy:0.2053\n",
    "Epoch #213: Loss:1.3328, Accuracy:0.4641, Validation Loss:1.8776, Validation Accuracy:0.2036\n",
    "Epoch #214: Loss:1.3280, Accuracy:0.4649, Validation Loss:1.8802, Validation Accuracy:0.1954\n",
    "Epoch #215: Loss:1.3225, Accuracy:0.4624, Validation Loss:1.8979, Validation Accuracy:0.2020\n",
    "Epoch #216: Loss:1.3184, Accuracy:0.4645, Validation Loss:1.8813, Validation Accuracy:0.2036\n",
    "Epoch #217: Loss:1.3132, Accuracy:0.4789, Validation Loss:1.9026, Validation Accuracy:0.2200\n",
    "Epoch #218: Loss:1.3163, Accuracy:0.4702, Validation Loss:1.8727, Validation Accuracy:0.2102\n",
    "Epoch #219: Loss:1.3167, Accuracy:0.4649, Validation Loss:1.9054, Validation Accuracy:0.2003\n",
    "Epoch #220: Loss:1.3138, Accuracy:0.4715, Validation Loss:1.9242, Validation Accuracy:0.2003\n",
    "Epoch #221: Loss:1.3212, Accuracy:0.4772, Validation Loss:1.9092, Validation Accuracy:0.2151\n",
    "Epoch #222: Loss:1.3198, Accuracy:0.4641, Validation Loss:1.8973, Validation Accuracy:0.1905\n",
    "Epoch #223: Loss:1.3107, Accuracy:0.4776, Validation Loss:1.8913, Validation Accuracy:0.2036\n",
    "Epoch #224: Loss:1.3078, Accuracy:0.4887, Validation Loss:1.9019, Validation Accuracy:0.2036\n",
    "Epoch #225: Loss:1.2978, Accuracy:0.4719, Validation Loss:1.9003, Validation Accuracy:0.1954\n",
    "Epoch #226: Loss:1.3005, Accuracy:0.4858, Validation Loss:1.9206, Validation Accuracy:0.2020\n",
    "Epoch #227: Loss:1.2930, Accuracy:0.4862, Validation Loss:1.9330, Validation Accuracy:0.2003\n",
    "Epoch #228: Loss:1.2928, Accuracy:0.4830, Validation Loss:1.9150, Validation Accuracy:0.2085\n",
    "Epoch #229: Loss:1.2952, Accuracy:0.4871, Validation Loss:1.8933, Validation Accuracy:0.2003\n",
    "Epoch #230: Loss:1.2985, Accuracy:0.4883, Validation Loss:1.9327, Validation Accuracy:0.2069\n",
    "Epoch #231: Loss:1.2843, Accuracy:0.4871, Validation Loss:1.9238, Validation Accuracy:0.1921\n",
    "Epoch #232: Loss:1.2828, Accuracy:0.4928, Validation Loss:1.9408, Validation Accuracy:0.2036\n",
    "Epoch #233: Loss:1.2812, Accuracy:0.4895, Validation Loss:1.9179, Validation Accuracy:0.1970\n",
    "Epoch #234: Loss:1.2787, Accuracy:0.4949, Validation Loss:1.9247, Validation Accuracy:0.2003\n",
    "Epoch #235: Loss:1.2818, Accuracy:0.4965, Validation Loss:1.9463, Validation Accuracy:0.2085\n",
    "Epoch #236: Loss:1.2894, Accuracy:0.4768, Validation Loss:1.9456, Validation Accuracy:0.1954\n",
    "Epoch #237: Loss:1.2861, Accuracy:0.4871, Validation Loss:1.9334, Validation Accuracy:0.2053\n",
    "Epoch #238: Loss:1.2885, Accuracy:0.4858, Validation Loss:1.9294, Validation Accuracy:0.1888\n",
    "Epoch #239: Loss:1.2779, Accuracy:0.4965, Validation Loss:1.9409, Validation Accuracy:0.2053\n",
    "Epoch #240: Loss:1.2739, Accuracy:0.4953, Validation Loss:1.9342, Validation Accuracy:0.1970\n",
    "Epoch #241: Loss:1.2736, Accuracy:0.4928, Validation Loss:1.9573, Validation Accuracy:0.1921\n",
    "Epoch #242: Loss:1.2760, Accuracy:0.4973, Validation Loss:1.9593, Validation Accuracy:0.2069\n",
    "Epoch #243: Loss:1.2695, Accuracy:0.4998, Validation Loss:1.9647, Validation Accuracy:0.1970\n",
    "Epoch #244: Loss:1.2642, Accuracy:0.5018, Validation Loss:1.9626, Validation Accuracy:0.1954\n",
    "Epoch #245: Loss:1.2726, Accuracy:0.4961, Validation Loss:1.9404, Validation Accuracy:0.2085\n",
    "Epoch #246: Loss:1.2607, Accuracy:0.5101, Validation Loss:1.9478, Validation Accuracy:0.1921\n",
    "Epoch #247: Loss:1.2610, Accuracy:0.5088, Validation Loss:1.9438, Validation Accuracy:0.2020\n",
    "Epoch #248: Loss:1.2628, Accuracy:0.5072, Validation Loss:1.9950, Validation Accuracy:0.2003\n",
    "Epoch #249: Loss:1.2618, Accuracy:0.5051, Validation Loss:2.0063, Validation Accuracy:0.2069\n",
    "Epoch #250: Loss:1.2556, Accuracy:0.5146, Validation Loss:1.9596, Validation Accuracy:0.1954\n",
    "Epoch #251: Loss:1.2612, Accuracy:0.5035, Validation Loss:1.9682, Validation Accuracy:0.2053\n",
    "Epoch #252: Loss:1.2610, Accuracy:0.5043, Validation Loss:1.9503, Validation Accuracy:0.1905\n",
    "Epoch #253: Loss:1.2554, Accuracy:0.5183, Validation Loss:1.9778, Validation Accuracy:0.2118\n",
    "Epoch #254: Loss:1.2573, Accuracy:0.5043, Validation Loss:2.0029, Validation Accuracy:0.1954\n",
    "Epoch #255: Loss:1.2545, Accuracy:0.5117, Validation Loss:2.0013, Validation Accuracy:0.2053\n",
    "Epoch #256: Loss:1.2547, Accuracy:0.5084, Validation Loss:1.9702, Validation Accuracy:0.2020\n",
    "Epoch #257: Loss:1.2540, Accuracy:0.5039, Validation Loss:1.9604, Validation Accuracy:0.2118\n",
    "Epoch #258: Loss:1.2594, Accuracy:0.4994, Validation Loss:1.9720, Validation Accuracy:0.2003\n",
    "Epoch #259: Loss:1.2525, Accuracy:0.5129, Validation Loss:1.9946, Validation Accuracy:0.2069\n",
    "Epoch #260: Loss:1.2462, Accuracy:0.5101, Validation Loss:1.9842, Validation Accuracy:0.1954\n",
    "Epoch #261: Loss:1.2487, Accuracy:0.5060, Validation Loss:2.0191, Validation Accuracy:0.2085\n",
    "Epoch #262: Loss:1.2590, Accuracy:0.5068, Validation Loss:2.0019, Validation Accuracy:0.2020\n",
    "Epoch #263: Loss:1.2337, Accuracy:0.5232, Validation Loss:2.0134, Validation Accuracy:0.2085\n",
    "Epoch #264: Loss:1.2336, Accuracy:0.5207, Validation Loss:2.0289, Validation Accuracy:0.1938\n",
    "Epoch #265: Loss:1.2322, Accuracy:0.5240, Validation Loss:2.0042, Validation Accuracy:0.2036\n",
    "Epoch #266: Loss:1.2297, Accuracy:0.5253, Validation Loss:1.9832, Validation Accuracy:0.2003\n",
    "Epoch #267: Loss:1.2439, Accuracy:0.5187, Validation Loss:1.9993, Validation Accuracy:0.2069\n",
    "Epoch #268: Loss:1.2447, Accuracy:0.5150, Validation Loss:2.0116, Validation Accuracy:0.1921\n",
    "Epoch #269: Loss:1.2268, Accuracy:0.5253, Validation Loss:2.0165, Validation Accuracy:0.2069\n",
    "Epoch #270: Loss:1.2224, Accuracy:0.5290, Validation Loss:2.0126, Validation Accuracy:0.2036\n",
    "Epoch #271: Loss:1.2214, Accuracy:0.5269, Validation Loss:2.0187, Validation Accuracy:0.2003\n",
    "Epoch #272: Loss:1.2219, Accuracy:0.5232, Validation Loss:1.9964, Validation Accuracy:0.2003\n",
    "Epoch #273: Loss:1.2181, Accuracy:0.5244, Validation Loss:2.0094, Validation Accuracy:0.2053\n",
    "Epoch #274: Loss:1.2226, Accuracy:0.5261, Validation Loss:2.0260, Validation Accuracy:0.2053\n",
    "Epoch #275: Loss:1.2153, Accuracy:0.5355, Validation Loss:2.0298, Validation Accuracy:0.2036\n",
    "Epoch #276: Loss:1.2113, Accuracy:0.5376, Validation Loss:2.0423, Validation Accuracy:0.2102\n",
    "Epoch #277: Loss:1.2139, Accuracy:0.5335, Validation Loss:2.0046, Validation Accuracy:0.2053\n",
    "Epoch #278: Loss:1.2105, Accuracy:0.5302, Validation Loss:1.9917, Validation Accuracy:0.2102\n",
    "Epoch #279: Loss:1.2091, Accuracy:0.5306, Validation Loss:2.0361, Validation Accuracy:0.2069\n",
    "Epoch #280: Loss:1.2112, Accuracy:0.5273, Validation Loss:2.0585, Validation Accuracy:0.2036\n",
    "Epoch #281: Loss:1.2208, Accuracy:0.5302, Validation Loss:2.0217, Validation Accuracy:0.2069\n",
    "Epoch #282: Loss:1.2117, Accuracy:0.5310, Validation Loss:2.0321, Validation Accuracy:0.2200\n",
    "Epoch #283: Loss:1.2202, Accuracy:0.5232, Validation Loss:2.0185, Validation Accuracy:0.1888\n",
    "Epoch #284: Loss:1.2059, Accuracy:0.5380, Validation Loss:2.0152, Validation Accuracy:0.2036\n",
    "Epoch #285: Loss:1.2130, Accuracy:0.5318, Validation Loss:2.0237, Validation Accuracy:0.2069\n",
    "Epoch #286: Loss:1.2036, Accuracy:0.5384, Validation Loss:2.0230, Validation Accuracy:0.2118\n",
    "Epoch #287: Loss:1.1984, Accuracy:0.5372, Validation Loss:2.0827, Validation Accuracy:0.2085\n",
    "Epoch #288: Loss:1.1952, Accuracy:0.5392, Validation Loss:2.0678, Validation Accuracy:0.2085\n",
    "Epoch #289: Loss:1.1943, Accuracy:0.5462, Validation Loss:2.0526, Validation Accuracy:0.2102\n",
    "Epoch #290: Loss:1.2048, Accuracy:0.5326, Validation Loss:2.0080, Validation Accuracy:0.2053\n",
    "Epoch #291: Loss:1.1916, Accuracy:0.5470, Validation Loss:2.0322, Validation Accuracy:0.2184\n",
    "Epoch #292: Loss:1.2080, Accuracy:0.5396, Validation Loss:2.0536, Validation Accuracy:0.2020\n",
    "Epoch #293: Loss:1.1853, Accuracy:0.5429, Validation Loss:2.0567, Validation Accuracy:0.2036\n",
    "Epoch #294: Loss:1.1952, Accuracy:0.5372, Validation Loss:2.0745, Validation Accuracy:0.2217\n",
    "Epoch #295: Loss:1.2061, Accuracy:0.5322, Validation Loss:2.0683, Validation Accuracy:0.2102\n",
    "Epoch #296: Loss:1.1831, Accuracy:0.5474, Validation Loss:2.0286, Validation Accuracy:0.2085\n",
    "Epoch #297: Loss:1.1835, Accuracy:0.5470, Validation Loss:2.0590, Validation Accuracy:0.2036\n",
    "Epoch #298: Loss:1.1795, Accuracy:0.5454, Validation Loss:2.0609, Validation Accuracy:0.2102\n",
    "Epoch #299: Loss:1.1731, Accuracy:0.5544, Validation Loss:2.0644, Validation Accuracy:0.2118\n",
    "Epoch #300: Loss:1.1732, Accuracy:0.5540, Validation Loss:2.0720, Validation Accuracy:0.2184\n",
    "\n",
    "Test:\n",
    "Test Loss:2.07203555, Accuracy:0.2184\n",
    "Labels: ['04', '01', '05', '02', '03']\n",
    "Confusion Matrix:\n",
    "      04  01  05  02  03\n",
    "t:04  18  20  27  27  20\n",
    "t:01  15  32  30  29  20\n",
    "t:05  17  28  38  30  29\n",
    "t:02  17  20  32  21  24\n",
    "t:03  18  18  29  26  24\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.21      0.16      0.18       112\n",
    "          01       0.27      0.25      0.26       126\n",
    "          05       0.24      0.27      0.26       142\n",
    "          02       0.16      0.18      0.17       114\n",
    "          03       0.21      0.21      0.21       115\n",
    "\n",
    "    accuracy                           0.22       609\n",
    "   macro avg       0.22      0.22      0.22       609\n",
    "weighted avg       0.22      0.22      0.22       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 06:17:28 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.606887874344887, 1.6052225315316362, 1.6047221905687956, 1.6042165192477222, 1.6035988383692474, 1.603090187990411, 1.6028007916824767, 1.602279845521172, 1.6017861618784262, 1.6014076316689427, 1.6008745803817348, 1.600518378718146, 1.600338330997035, 1.5999525070973413, 1.599341026276399, 1.598994454139559, 1.598777213903092, 1.5984833283573145, 1.5981092609599699, 1.597595258103607, 1.5971553075098248, 1.5969108080824803, 1.5963224444679047, 1.5960839168583034, 1.5956701999423148, 1.5955750934400386, 1.5955632124432593, 1.5950603003572361, 1.5947793776961579, 1.5945736982160796, 1.594529653221907, 1.5944684160558265, 1.5947691824635848, 1.594112032152749, 1.5941143564402764, 1.5939061220839301, 1.593995310598602, 1.5939994529746044, 1.594071080727726, 1.5938838228999297, 1.5963175778318508, 1.5939199202166403, 1.5941611526439148, 1.5940887583496144, 1.595161131645854, 1.5942264323555582, 1.5945961667203354, 1.596150762537626, 1.5948810735946806, 1.5946205997310445, 1.5963629601624212, 1.5956737308079385, 1.5963771893277348, 1.5954778215959546, 1.595777184113689, 1.5982635900109077, 1.5985497026803654, 1.5993541073916582, 1.597797604029989, 1.5966941911011494, 1.598854124448178, 1.5969006570884943, 1.5985153760816075, 1.5983923209711837, 1.599767750315674, 1.5988135404383217, 1.6020283487630007, 1.6009710852931482, 1.601498237188618, 1.604985910291938, 1.603758765363145, 1.6074893548962321, 1.607683280810151, 1.6117496065709782, 1.6082649172233243, 1.6082244242353392, 1.6150777224445187, 1.6137875007291145, 1.6108497345976054, 1.6119101603434396, 1.611307485545993, 1.6145406918376928, 1.6134462873336717, 1.6150849996920682, 1.6175933312899962, 1.6142021073104909, 1.6182803183745085, 1.6229444720670703, 1.620337622310532, 1.6217563209079562, 1.6226179167163393, 1.6294083851703087, 1.6310435836929797, 1.626884926520349, 1.6294603488715411, 1.6324347897703424, 1.6367424699081772, 1.6383906984564118, 1.6436248477456605, 1.6438440871356157, 1.6387387140239598, 1.6489668724376385, 1.6499882006684352, 1.6401529126175127, 1.6492092683788981, 1.6500906333547507, 1.654358295188553, 1.6590270252259103, 1.672371546035917, 1.6628551747411342, 1.6689204562865259, 1.6647275404389856, 1.6678166802489307, 1.6632192575285587, 1.667680240225518, 1.6765399057485395, 1.6782570869856084, 1.6797819053402478, 1.6893645797065522, 1.6819033986829184, 1.6818832421341945, 1.6951520940157385, 1.7068544322829724, 1.69923611697305, 1.6951175376112237, 1.7092702104931785, 1.6861837305654641, 1.7242682491030012, 1.6968079319924165, 1.7142147559837755, 1.6983445976755303, 1.7086626183614746, 1.7033807871181195, 1.7000439748388205, 1.722375209304108, 1.7202324810482206, 1.7275665794883064, 1.7182965789522444, 1.7526511470672532, 1.7159261249360585, 1.7332915206652362, 1.7211370935972492, 1.7426906048957937, 1.7384677044863772, 1.7357947892939125, 1.754243669447249, 1.7625284146009799, 1.748645241037378, 1.7584945918695485, 1.7557123670437065, 1.7596050424528826, 1.7902430129560147, 1.755550357704288, 1.7562604111012174, 1.7834467692132459, 1.7622305677442127, 1.7537733590465852, 1.76483253716248, 1.7858769664623466, 1.7881528612819604, 1.7634521889177646, 1.7857853086123914, 1.7829430819732215, 1.7688812099654099, 1.7886205377249882, 1.7935093003149298, 1.7778852007463453, 1.7901507406594914, 1.7777915993347544, 1.7999075244017226, 1.8089608055813167, 1.7853021001189409, 1.8029624339199222, 1.8062408142684911, 1.830022005024802, 1.8205797922826557, 1.8108675301760093, 1.81948477607251, 1.8294699315367073, 1.8091102413747502, 1.849384552152286, 1.8105955374456195, 1.8036301670403316, 1.8133576673827148, 1.82975794078877, 1.8010732426823457, 1.8072017899091999, 1.8567870013623793, 1.8207403006420542, 1.8206293244275749, 1.833741190202522, 1.8561751820966723, 1.8227129745953188, 1.8499395081953853, 1.8358724497026215, 1.8377044608048814, 1.8962378047761463, 1.8528079495249907, 1.8272215542926382, 1.8590193900764478, 1.8620546153809245, 1.8453994186836706, 1.8620583453201895, 1.855867437736937, 1.8636017346812783, 1.8730418388479448, 1.8730621175421478, 1.8763743940441089, 1.8745733301823557, 1.9030732585878796, 1.8840372198320963, 1.8864326459433645, 1.8776453576847445, 1.8802376903336624, 1.897873576442988, 1.8812518665943239, 1.9026046014575928, 1.872693722471228, 1.9053593744785327, 1.9241997716070591, 1.909184543761518, 1.8972867572640355, 1.8912941266358976, 1.9018875062955032, 1.9003090533521179, 1.9205600148547068, 1.9330169941208437, 1.914999153813705, 1.8933185393782868, 1.9326555398101681, 1.9238484404944434, 1.9408288922020172, 1.9178903461089862, 1.9247084566329304, 1.9462547049733805, 1.945560658702318, 1.9333948648621884, 1.9294353655014915, 1.9409002331873075, 1.9342362450065675, 1.9572625451878765, 1.9593190455867349, 1.964680785616043, 1.9625861826788615, 1.9404010234403688, 1.947826799305006, 1.943756050076978, 1.9949613394604135, 2.0062956248206656, 1.9595848229914072, 1.9681947288059054, 1.9503403171921403, 1.9777787880748754, 2.002899700589172, 2.0012597352608865, 1.9701759736917681, 1.960406365261485, 1.972010500325358, 1.9945539831136443, 1.984182918796007, 2.0190765023818744, 2.0019029320167205, 2.0134325454192012, 2.0289326119305464, 2.004199420681532, 1.983197608800553, 1.9992939877784115, 2.011607524600914, 2.016487507201572, 2.012599848565601, 2.018675328866993, 1.996356355145647, 2.0093739559301995, 2.0260388835506093, 2.0298396750232466, 2.0423458855727623, 2.004561097946856, 1.9916613792942466, 2.036087307045221, 2.0585473495946927, 2.021659290262044, 2.032104925569055, 2.0184571598159464, 2.015179961968721, 2.023723527715711, 2.023038850042033, 2.0827227425692705, 2.0677625510492934, 2.052644825138286, 2.008030828584004, 2.032166437953955, 2.0536032200642604, 2.0567365597034324, 2.074465986934593, 2.068273216632787, 2.028590232084929, 2.059025647409248, 2.060892275793016, 2.0644095914900205, 2.0720353772487545], 'val_acc': [0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.24794745264186452, 0.24630541651706978, 0.23316912959851263, 0.23316912959851263, 0.251231524891454, 0.24958948876665926, 0.24794745264186452, 0.24794745264186452, 0.24958948876665926, 0.25287356101624875, 0.24958948876665926, 0.24958948876665926, 0.251231524891454, 0.251231524891454, 0.24958948876665926, 0.251231524891454, 0.24794745254399153, 0.24794745264186452, 0.24958948876665926, 0.251231524891454, 0.251231524891454, 0.2545155971410435, 0.24630541651706978, 0.25287356101624875, 0.251231524793581, 0.2512315270691278, 0.24958948866878627, 0.2463054164191968, 0.2463054164191968, 0.25615763534563907, 0.24137930804481256, 0.2528735630960496, 0.2463054185968706, 0.2479474548195383, 0.24137931032035934, 0.2495894908464601, 0.2380952378750239, 0.22988505734892314, 0.2561576352477661, 0.23809523807076985, 0.22660098509933366, 0.23645320184810212, 0.22988505725105016, 0.2413793101246134, 0.23316912950063964, 0.22660098509933366, 0.22331691245825225, 0.2282430213220014, 0.22988505734892314, 0.24302134624940813, 0.2282430213220014, 0.2348111656254344, 0.22495894887666593, 0.24466338247207586, 0.22660098509933366, 0.23316912940276668, 0.2249589489745389, 0.23809523797289686, 0.22495894887666593, 0.22167487662707644, 0.23809523807076985, 0.23481116582118036, 0.24466338237420288, 0.2282430212241284, 0.23481116542968844, 0.22167487662707644, 0.22660098470784173, 0.22824302102838243, 0.2200328405022817, 0.23645320165235617, 0.22660098500146067, 0.23645319967042833, 0.24958948866878627, 0.2249589489745389, 0.2282430213220014, 0.2364532019459751, 0.23316912959851263, 0.23481116572330737, 0.23481116552756143, 0.2200328404044087, 0.23152709318009895, 0.22660098519720664, 0.22167487672494943, 0.2233169126539982, 0.22495894887666593, 0.2282430211262554, 0.22495894877879294, 0.23152709318009895, 0.23316912959851263, 0.2249589489745389, 0.2233169121646333, 0.22495894887666593, 0.23316912959851263, 0.2413793100267404, 0.2282430211262554, 0.23481116542968844, 0.21674876795907325, 0.2233169125561252, 0.2315270933758449, 0.21674876795907325, 0.23481116582118036, 0.20689655121030479, 0.2364532019459751, 0.2315270934737179, 0.23152709318009895, 0.22331691245825225, 0.21839080408386802, 0.22495894877879294, 0.22331691275187118, 0.23809523797289686, 0.2233169125561252, 0.23316912959851263, 0.24137930814268554, 0.23809523797289686, 0.19211822628289804, 0.24302134624940813, 0.21510673173640554, 0.2282430212241284, 0.21674876795907325, 0.23316912940276668, 0.23481116542968844, 0.23973727399981865, 0.224958948485174, 0.23809523777715091, 0.23973727390194566, 0.23973727390194566, 0.2331691293048937, 0.23645320155448318, 0.20032840680899877, 0.224958948583047, 0.2380952378750239, 0.21674876815481922, 0.2397372740976916, 0.2282430211262554, 0.22495894868091998, 0.21839080437748695, 0.2233169126539982, 0.21674876815481922, 0.2282430212241284, 0.2380952378750239, 0.22003284030653572, 0.2331691293048937, 0.2151067318342785, 0.21346469570948376, 0.22167487652920345, 0.23152709327797194, 0.20689655130817777, 0.211822659780435, 0.22331691275187118, 0.23152709327797194, 0.2003284067111258, 0.2036124788628423, 0.22824302093050947, 0.21018062336202128, 0.21018062336202128, 0.21839080418174098, 0.20853858733509953, 0.20853858723722654, 0.2200328404044087, 0.2068965511124318, 0.21346469570948376, 0.21346469580735675, 0.2151067318342785, 0.211822659682562, 0.21346469590522973, 0.22167487662707644, 0.21018062345989427, 0.2036124789607153, 0.1986863703905851, 0.2036124788628423, 0.21182265987830795, 0.21346469570948376, 0.205254515281256, 0.20032840690687176, 0.21018062355776726, 0.20689655081881286, 0.22988505685955823, 0.2101806216737124, 0.20032840661325282, 0.21018062336202128, 0.2200328405022817, 0.205254515281256, 0.20853858723722654, 0.21510673212789747, 0.20525451469401812, 0.19704433436366334, 0.2085385875308455, 0.20361247905858829, 0.21674876746970836, 0.20361247905858829, 0.2003284067111258, 0.2052545148897641, 0.2036124789607153, 0.20032840680899877, 0.21346469570948376, 0.205254515281256, 0.22003284030653572, 0.19211822589140612, 0.20197044273804757, 0.21018062365564025, 0.20525451537912898, 0.20525451508551004, 0.20361247905858829, 0.19540229843461454, 0.2019704430316665, 0.20361247905858829, 0.2200328404044087, 0.21018062336202128, 0.20032840651537984, 0.20032840700474475, 0.21510673173640554, 0.19047618976661138, 0.20361247876496932, 0.20361247905858829, 0.1954022982388686, 0.20197044273804757, 0.2003284067111258, 0.20853858743297252, 0.20032840661325282, 0.2068965511124318, 0.1921182259892791, 0.2036124788628423, 0.19704433446153632, 0.20032840651537984, 0.20853858723722654, 0.19540229833674158, 0.20525451498763705, 0.18883415373968962, 0.20525451498763705, 0.19704433436366334, 0.19211822618502505, 0.20689655121030479, 0.19704433455940928, 0.1954022981409956, 0.20853858713935358, 0.1921182259892791, 0.20197044264017458, 0.20032840680899877, 0.20689655130817777, 0.1954022981409956, 0.2052545148897641, 0.19047618986448436, 0.21182265958468902, 0.1954022981409956, 0.20525451508551004, 0.20197044283592056, 0.21182265929107008, 0.2003284067111258, 0.20689655101455884, 0.19540229804312262, 0.20853858713935358, 0.20197044273804757, 0.20853858733509953, 0.19376026172258193, 0.2036124789607153, 0.20032840651537984, 0.20689655130817777, 0.19211822579353313, 0.20689655130817777, 0.2036124788628423, 0.2003284067111258, 0.20032840661325282, 0.20525451498763705, 0.20525451508551004, 0.2036124789607153, 0.21018062336202128, 0.20525451508551004, 0.2101806229705294, 0.20689655121030479, 0.20361247876496932, 0.2068965511124318, 0.22003284001291679, 0.18883415354394364, 0.20361247856922338, 0.20689655101455884, 0.21182265948681606, 0.20853858713935358, 0.20853858713935358, 0.21018062345989427, 0.2052545148897641, 0.21839080398599503, 0.20197044254230162, 0.20361247915646125, 0.22167487652920345, 0.21018062345989427, 0.2085385870414806, 0.20361247905858829, 0.21018062326414833, 0.211822659682562, 0.21839080398599503], 'loss': [1.6163353075481783, 1.6060125231498077, 1.6062691790856864, 1.6054857634910569, 1.6051356168743032, 1.604627936919367, 1.604878923878288, 1.6044096700464676, 1.6041555843803672, 1.604017097112824, 1.6040344682807062, 1.6035982127551915, 1.6034414109752897, 1.6032906311003825, 1.6035421926138094, 1.602988138678627, 1.6026258815240566, 1.6025550592116997, 1.6024370641434218, 1.602126747430962, 1.6019012098194882, 1.6018428050517057, 1.6015521046072552, 1.6016117635448854, 1.600944711540269, 1.6010254129981603, 1.6009724680158393, 1.6003550392891102, 1.600367814020944, 1.5999245537869495, 1.599546138853508, 1.5996695045573022, 1.5991303476709604, 1.5988020987481306, 1.5986305228983353, 1.598130516741555, 1.598111544938058, 1.5974115804969897, 1.597804439190232, 1.5987303913006792, 1.5967634241683772, 1.5961917281395601, 1.59595365308883, 1.595437897891724, 1.594751350982478, 1.594467473421743, 1.595375173096784, 1.5933624629367305, 1.592691599123287, 1.5939350641728427, 1.5919458619378186, 1.5914393792162196, 1.5920540915377577, 1.5918166990887213, 1.5919486219143721, 1.5959801826633713, 1.592598025118301, 1.5910954669270916, 1.590173422139773, 1.5894077883608777, 1.5881654424099463, 1.5881299958826336, 1.5870006389931242, 1.5854395650005928, 1.585854434869128, 1.5846864720389584, 1.584608818520266, 1.5840523452974198, 1.5814753721382093, 1.5809986814091583, 1.579512087863084, 1.5786875302297134, 1.5781358533816172, 1.5795876435430634, 1.5780624339712719, 1.5828270805934617, 1.5745667807620165, 1.575648667875991, 1.5735432097554451, 1.573529241119322, 1.573604795183734, 1.5716582264009198, 1.5737376578779436, 1.5671543400390437, 1.5665200054278363, 1.5629997163337848, 1.5602674430400685, 1.560874892945652, 1.5619095066489626, 1.5577593429867003, 1.5552475675175568, 1.5570083198116544, 1.5588097339782871, 1.552718964151778, 1.5482170461384424, 1.545874276102446, 1.5433515716627149, 1.541797822554743, 1.5437535303084513, 1.5413037395085643, 1.538668245309677, 1.5368628700655833, 1.5467229962104154, 1.5336436217325669, 1.5315947050920993, 1.5284898322710512, 1.5274665499125173, 1.524961794181526, 1.5226262714338987, 1.5239814435187307, 1.52622266545188, 1.5238013501529577, 1.5188367560169291, 1.519501297723586, 1.5157174849167496, 1.5135561492164031, 1.510234372131144, 1.5039374866035196, 1.5040364095562537, 1.5001949307854905, 1.4994184932180008, 1.4952594939198582, 1.4945054865470901, 1.501949141500422, 1.4957849228896154, 1.5024246782737591, 1.4889997919482127, 1.4900676855071615, 1.488190446105581, 1.48988807294403, 1.4849631092631597, 1.481281514236325, 1.4801937538005978, 1.476899783077671, 1.4726606778295623, 1.4682558500791232, 1.4635240898974378, 1.4703833311734993, 1.47174797351845, 1.4744578940667656, 1.4621245347988434, 1.4537929536380807, 1.4510352185129876, 1.4511072992054588, 1.4462075706380104, 1.444142047531551, 1.446138496565378, 1.4448242189947829, 1.4418350496820846, 1.4368467101326223, 1.4331479744744742, 1.4350330046314967, 1.44202172398812, 1.4379662203348147, 1.4240572566124448, 1.4311485423199695, 1.427849510416113, 1.4233979026884513, 1.4223177556384516, 1.4231134474889453, 1.4228042926142104, 1.4152545305248159, 1.4221175324500708, 1.4129769575424507, 1.4160726805242425, 1.4144167236234129, 1.417773910322718, 1.413230384252889, 1.3999396006429465, 1.4009822507658534, 1.404474410139315, 1.3958914950642987, 1.3905640118420737, 1.3884128667490683, 1.3875910939132408, 1.3894690860713041, 1.3831396368249975, 1.383912683855092, 1.3849805049582917, 1.385685343076561, 1.3837726283611949, 1.385230976600177, 1.3815835573834805, 1.3958507572111407, 1.3795934864138186, 1.370649907182619, 1.3738455945216654, 1.3675848094344873, 1.3720595630042607, 1.3698428982337152, 1.3661294709975225, 1.3539688558304335, 1.3581125687279985, 1.3528396854165643, 1.3494561511633087, 1.3519090345997584, 1.3581253485513174, 1.3619151694573906, 1.3660174122581248, 1.355831998035893, 1.3505008679885395, 1.3411480229493284, 1.3432222669863847, 1.3380253806985625, 1.3320946461366188, 1.3300619036754788, 1.329091808291676, 1.324208690743182, 1.3228650496480892, 1.3219685178517806, 1.3308339377937866, 1.3173539631665365, 1.3328368414598815, 1.3280480770115, 1.3225117750481168, 1.318402790239949, 1.3132175486680173, 1.3162711843572847, 1.3166511706992585, 1.3138021705576526, 1.3212035087589367, 1.3197789925079815, 1.3106831665156558, 1.3077584368492299, 1.2978429867746404, 1.3005327442588257, 1.2930297020769217, 1.292801166461968, 1.295222978131727, 1.2984891391632738, 1.2842825236996096, 1.2828168704034857, 1.2812282193123192, 1.2786730653696237, 1.2818427616076302, 1.2894405602429682, 1.2860782147922556, 1.2885197229209133, 1.2778825930746185, 1.2739390192090607, 1.2735603627238186, 1.27603475141819, 1.2695050365381417, 1.2641790862445714, 1.272587625495707, 1.2606557529320217, 1.2609647682804836, 1.2628231629698674, 1.2618248655565956, 1.2555635162447512, 1.2612139058064142, 1.2610220157145475, 1.2553742483166455, 1.2573387498973088, 1.2545300785765756, 1.2547462340008306, 1.2540494738173436, 1.2593783774170297, 1.2525407703015838, 1.2461619076542785, 1.2487231370115182, 1.2589723860703455, 1.2336799815450117, 1.233643749313433, 1.2321926978579292, 1.2296508884527846, 1.2438677621328367, 1.2447320673744782, 1.2268078352146814, 1.2224445632840573, 1.221398307949121, 1.2218623215168163, 1.2181051497097133, 1.2226439146045787, 1.2153108140036801, 1.211274517586099, 1.2139147334519842, 1.2105037230975328, 1.2090642011630706, 1.2112101817767478, 1.2208056029353054, 1.21174099876161, 1.220213973155012, 1.2059381278388555, 1.213037855277561, 1.2036338386594392, 1.1983997097739938, 1.1952343206386058, 1.1942532864439415, 1.2048043778300042, 1.1915703996251006, 1.2079782681298696, 1.185328180050703, 1.1952232549322703, 1.2060815521823798, 1.1831460311427497, 1.1834941565868056, 1.1794640894543218, 1.1731012617538108, 1.1731824437695606], 'acc': [0.23285420872959514, 0.23285420890706277, 0.23285421049203225, 0.23285421010037957, 0.23285421010037957, 0.23285420929871545, 0.2328542083195837, 0.2328542091028891, 0.23285421047367355, 0.23285420872959514, 0.23449692035847375, 0.23696098328860635, 0.23367556476739887, 0.2328542091028891, 0.23367556553234556, 0.23696098628107773, 0.2381930193311135, 0.23778234149885863, 0.23819301970440748, 0.23901437474472076, 0.24065708523535875, 0.24024640742146258, 0.23819301833362305, 0.2406570838462156, 0.2410677626576022, 0.24024640603231942, 0.241478440648966, 0.24188911791210058, 0.24024640546319911, 0.24065708464787972, 0.23983572861007596, 0.241478439688193, 0.24024640818640927, 0.24353182856184746, 0.24312115151289798, 0.24353182854348873, 0.24271047428648085, 0.24229979551181166, 0.234907598386555, 0.23613963043910033, 0.24024640622814578, 0.23901437376558904, 0.24599589268529684, 0.24353182758271572, 0.2451745386241153, 0.244353183583802, 0.2447638610060455, 0.25010267141173753, 0.25174537914244793, 0.252566735376078, 0.2517453809232438, 0.25544147927168703, 0.2513347011327254, 0.2492813144315195, 0.25297741199665735, 0.25010266986348545, 0.24599589444773398, 0.2583162239689602, 0.2509240230862854, 0.25503080184944354, 0.2595482524782725, 0.25913757804850046, 0.2591375788134471, 0.26406570964280585, 0.26119096535554415, 0.26406570964280585, 0.257494866187078, 0.2620123211608041, 0.2702258731305477, 0.27145790633969236, 0.26817248406106686, 0.26694045218598916, 0.2648870641139988, 0.2776180712349361, 0.26899383949303285, 0.2681724850401986, 0.27967145576369346, 0.27145790559310445, 0.2796714563511725, 0.2825461992860085, 0.2718685814120197, 0.27885010385660175, 0.2751540031398836, 0.28542094656574163, 0.2854209461373715, 0.28377823413519887, 0.2977412747161834, 0.301026696248221, 0.28870636473201383, 0.2981519491643142, 0.3026694035505612, 0.29609856322805495, 0.3034907580033954, 0.29897330577123826, 0.31375769900590245, 0.30513346808402203, 0.3080082121938161, 0.3080082137604269, 0.31211498876616695, 0.30554415001027146, 0.3075975353957691, 0.3117043134980133, 0.30965092501601155, 0.3252566717243782, 0.3256673523165607, 0.3207392218788546, 0.32484599473050485, 0.3240246414526287, 0.32813141626254244, 0.3195071846797481, 0.3182751532330405, 0.31540041166898897, 0.321971253558106, 0.3281314164583688, 0.3305954807592858, 0.33429158249185315, 0.3388090360947948, 0.33634496823228605, 0.34455852036113854, 0.3486652989284703, 0.3507186858071439, 0.34373716590830433, 0.351950719053006, 0.3449691999741893, 0.3466119106422949, 0.33839835550261227, 0.3527720755008212, 0.3519507170947425, 0.3548254630036912, 0.3507186852563823, 0.35071868682299306, 0.36180697973259174, 0.3609856278622175, 0.36878850221389126, 0.37330595323437293, 0.3667351133035194, 0.37946612145866454, 0.3663244347063178, 0.36098562844969656, 0.3626283390810847, 0.3741273108204287, 0.37618069930243053, 0.38234086318182503, 0.3761806965608616, 0.38275153802160855, 0.39096509155796294, 0.38193018654288696, 0.38069815270954577, 0.3839835720874935, 0.3983572912656796, 0.39630390278367783, 0.39671457883513683, 0.3856262817764674, 0.39301848262242467, 0.40862422972244405, 0.3995893219657992, 0.3934291602404945, 0.406160165421527, 0.39712525801981746, 0.39835729048237417, 0.39260780382939675, 0.40451745295426683, 0.3983572902865478, 0.4020533884575235, 0.40451745612420587, 0.40985626359250266, 0.4090349095313211, 0.4045174541292249, 0.40903490718140495, 0.41314168473288754, 0.4123203263268089, 0.41930184740060655, 0.42176591271737274, 0.41889117138586496, 0.4258726892897236, 0.42464065843049503, 0.4238193011993745, 0.42299794596323487, 0.4172484595060838, 0.4209445604553458, 0.41971252345206556, 0.4193018495914138, 0.43408624321773065, 0.41314168215042757, 0.4271047227314121, 0.42874743242038593, 0.42792607581346187, 0.436139630328948, 0.4344969198566687, 0.44229979358414606, 0.4410677615499594, 0.4472279282076403, 0.43778234158453266, 0.44312115202694213, 0.44845996043765324, 0.44476386128754586, 0.4353182751295258, 0.43983573084983985, 0.4299794640996373, 0.44394250409314273, 0.4599589319444535, 0.45503080291424935, 0.44681724820293683, 0.45503079958520143, 0.4599589321769973, 0.45749486666440475, 0.4620123204631727, 0.46365502952795007, 0.46735112969390663, 0.46283367354522253, 0.4611909634278785, 0.46652977406611434, 0.4640657071460199, 0.46488706199050683, 0.46242299866872155, 0.46447638734654967, 0.47885010217983864, 0.4702258739995271, 0.4648870653562722, 0.47145790662119275, 0.4772073946449546, 0.4640657079293253, 0.4776180675631921, 0.48870636583353705, 0.4718685826726518, 0.4858316244653118, 0.4862422989134426, 0.4829568797680387, 0.4870636527787978, 0.488295686612139, 0.4870636547737787, 0.49281313982342795, 0.48952771946634843, 0.49486652810960335, 0.4965092399893845, 0.4767967131470753, 0.48706365571619303, 0.48583162368200644, 0.49650924230258325, 0.49527720651097856, 0.4928131396276016, 0.4973305975754403, 0.4997946612888783, 0.5018480485959219, 0.4960985637053817, 0.5100616033072344, 0.5088295675156298, 0.5071868588057876, 0.5051334724778757, 0.5145790574976551, 0.5034907610264647, 0.5043121150876463, 0.5182751562561098, 0.5043121168500834, 0.5117043155419508, 0.5084188922474762, 0.5039014378612291, 0.4993839828875031, 0.5129363480045076, 0.5100616015447973, 0.5059548251682728, 0.5067761813835442, 0.5232032864979895, 0.5207392211812233, 0.5240246419299555, 0.5252566771340811, 0.518685835244964, 0.5149897349198985, 0.525256675959123, 0.5289527747175776, 0.5268993858439232, 0.5232032837564206, 0.5244353179814145, 0.5260780302161309, 0.5355236175858265, 0.5375770046970438, 0.5334702273413876, 0.5301848063968291, 0.5305954855815096, 0.527310062874514, 0.5301848014277354, 0.5310061564680487, 0.5232032807455905, 0.5379876829025927, 0.5318275184357191, 0.5383983607164888, 0.5371663268831476, 0.5392197143860177, 0.5462012275533265, 0.5326488715177689, 0.5470225910876076, 0.5396303937665247, 0.5429158119695142, 0.5371663251207105, 0.5322381942913518, 0.5474332689015038, 0.5470225897168232, 0.5453798768946277, 0.5544147866462535, 0.5540041094198364]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
