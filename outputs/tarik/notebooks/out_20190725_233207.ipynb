{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf2.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.25 23:32:07 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '0', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '04', '02', '05', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001035473BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000010351EA6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6196, Accuracy:0.1996, Validation Loss:1.6143, Validation Accuracy:0.1856\n",
    "Epoch #2: Loss:1.6123, Accuracy:0.1877, Validation Loss:1.6092, Validation Accuracy:0.2085\n",
    "Epoch #3: Loss:1.6105, Accuracy:0.2312, Validation Loss:1.6087, Validation Accuracy:0.2397\n",
    "Epoch #4: Loss:1.6093, Accuracy:0.2324, Validation Loss:1.6072, Validation Accuracy:0.2381\n",
    "Epoch #5: Loss:1.6080, Accuracy:0.2337, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6061, Accuracy:0.2333, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6056, Accuracy:0.2271, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #8: Loss:1.6061, Accuracy:0.2197, Validation Loss:1.6072, Validation Accuracy:0.2250\n",
    "Epoch #9: Loss:1.6052, Accuracy:0.2345, Validation Loss:1.6069, Validation Accuracy:0.2315\n",
    "Epoch #10: Loss:1.6048, Accuracy:0.2333, Validation Loss:1.6069, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6048, Accuracy:0.2333, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6046, Accuracy:0.2337, Validation Loss:1.6051, Validation Accuracy:0.2348\n",
    "Epoch #13: Loss:1.6048, Accuracy:0.2333, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2348\n",
    "Epoch #15: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2348\n",
    "Epoch #16: Loss:1.6040, Accuracy:0.2333, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6037, Accuracy:0.2333, Validation Loss:1.6034, Validation Accuracy:0.2315\n",
    "Epoch #20: Loss:1.6033, Accuracy:0.2333, Validation Loss:1.6030, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6025, Accuracy:0.2345, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6022, Accuracy:0.2337, Validation Loss:1.6024, Validation Accuracy:0.2315\n",
    "Epoch #23: Loss:1.6035, Accuracy:0.2341, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6031, Accuracy:0.2333, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #25: Loss:1.6021, Accuracy:0.2333, Validation Loss:1.6045, Validation Accuracy:0.2315\n",
    "Epoch #26: Loss:1.6011, Accuracy:0.2366, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #27: Loss:1.6013, Accuracy:0.2382, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #28: Loss:1.6014, Accuracy:0.2345, Validation Loss:1.5995, Validation Accuracy:0.2463\n",
    "Epoch #29: Loss:1.6007, Accuracy:0.2435, Validation Loss:1.6017, Validation Accuracy:0.2315\n",
    "Epoch #30: Loss:1.6003, Accuracy:0.2411, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #31: Loss:1.5998, Accuracy:0.2411, Validation Loss:1.5992, Validation Accuracy:0.2447\n",
    "Epoch #32: Loss:1.6010, Accuracy:0.2394, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.6025, Accuracy:0.2341, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #34: Loss:1.6014, Accuracy:0.2370, Validation Loss:1.6009, Validation Accuracy:0.2381\n",
    "Epoch #35: Loss:1.6011, Accuracy:0.2402, Validation Loss:1.6023, Validation Accuracy:0.2167\n",
    "Epoch #36: Loss:1.6015, Accuracy:0.2361, Validation Loss:1.6018, Validation Accuracy:0.2233\n",
    "Epoch #37: Loss:1.6012, Accuracy:0.2386, Validation Loss:1.6016, Validation Accuracy:0.2250\n",
    "Epoch #38: Loss:1.6022, Accuracy:0.2345, Validation Loss:1.6017, Validation Accuracy:0.2102\n",
    "Epoch #39: Loss:1.6021, Accuracy:0.2394, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #40: Loss:1.6011, Accuracy:0.2476, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #41: Loss:1.6009, Accuracy:0.2468, Validation Loss:1.6001, Validation Accuracy:0.2315\n",
    "Epoch #42: Loss:1.6013, Accuracy:0.2411, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #43: Loss:1.6006, Accuracy:0.2394, Validation Loss:1.6005, Validation Accuracy:0.2299\n",
    "Epoch #44: Loss:1.6013, Accuracy:0.2501, Validation Loss:1.6014, Validation Accuracy:0.2348\n",
    "Epoch #45: Loss:1.6010, Accuracy:0.2522, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #46: Loss:1.6009, Accuracy:0.2452, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #47: Loss:1.6012, Accuracy:0.2407, Validation Loss:1.6000, Validation Accuracy:0.2381\n",
    "Epoch #48: Loss:1.6007, Accuracy:0.2480, Validation Loss:1.6002, Validation Accuracy:0.2397\n",
    "Epoch #49: Loss:1.6009, Accuracy:0.2501, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #50: Loss:1.6005, Accuracy:0.2509, Validation Loss:1.5998, Validation Accuracy:0.2381\n",
    "Epoch #51: Loss:1.6002, Accuracy:0.2468, Validation Loss:1.6003, Validation Accuracy:0.2414\n",
    "Epoch #52: Loss:1.5998, Accuracy:0.2489, Validation Loss:1.6001, Validation Accuracy:0.2332\n",
    "Epoch #53: Loss:1.5998, Accuracy:0.2431, Validation Loss:1.6001, Validation Accuracy:0.2365\n",
    "Epoch #54: Loss:1.5994, Accuracy:0.2464, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #55: Loss:1.5995, Accuracy:0.2505, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #56: Loss:1.6006, Accuracy:0.2460, Validation Loss:1.5999, Validation Accuracy:0.2365\n",
    "Epoch #57: Loss:1.6016, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2184\n",
    "Epoch #58: Loss:1.6025, Accuracy:0.2415, Validation Loss:1.5995, Validation Accuracy:0.2430\n",
    "Epoch #59: Loss:1.6020, Accuracy:0.2427, Validation Loss:1.6015, Validation Accuracy:0.2463\n",
    "Epoch #60: Loss:1.6003, Accuracy:0.2452, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #61: Loss:1.6008, Accuracy:0.2386, Validation Loss:1.6003, Validation Accuracy:0.2250\n",
    "Epoch #62: Loss:1.5995, Accuracy:0.2439, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #63: Loss:1.5995, Accuracy:0.2456, Validation Loss:1.6030, Validation Accuracy:0.2397\n",
    "Epoch #64: Loss:1.6003, Accuracy:0.2444, Validation Loss:1.6118, Validation Accuracy:0.2053\n",
    "Epoch #65: Loss:1.6042, Accuracy:0.2316, Validation Loss:1.6072, Validation Accuracy:0.2167\n",
    "Epoch #66: Loss:1.6007, Accuracy:0.2456, Validation Loss:1.6079, Validation Accuracy:0.2151\n",
    "Epoch #67: Loss:1.6032, Accuracy:0.2316, Validation Loss:1.6064, Validation Accuracy:0.2250\n",
    "Epoch #68: Loss:1.6048, Accuracy:0.2341, Validation Loss:1.6067, Validation Accuracy:0.2381\n",
    "Epoch #69: Loss:1.6028, Accuracy:0.2456, Validation Loss:1.6033, Validation Accuracy:0.2463\n",
    "Epoch #70: Loss:1.6020, Accuracy:0.2308, Validation Loss:1.6028, Validation Accuracy:0.2315\n",
    "Epoch #71: Loss:1.6021, Accuracy:0.2345, Validation Loss:1.6029, Validation Accuracy:0.2414\n",
    "Epoch #72: Loss:1.6010, Accuracy:0.2452, Validation Loss:1.6026, Validation Accuracy:0.2463\n",
    "Epoch #73: Loss:1.6007, Accuracy:0.2439, Validation Loss:1.6028, Validation Accuracy:0.2447\n",
    "Epoch #74: Loss:1.6004, Accuracy:0.2448, Validation Loss:1.6025, Validation Accuracy:0.2463\n",
    "Epoch #75: Loss:1.6003, Accuracy:0.2456, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #76: Loss:1.6001, Accuracy:0.2439, Validation Loss:1.6020, Validation Accuracy:0.2479\n",
    "Epoch #77: Loss:1.5995, Accuracy:0.2456, Validation Loss:1.6026, Validation Accuracy:0.2463\n",
    "Epoch #78: Loss:1.5994, Accuracy:0.2448, Validation Loss:1.6036, Validation Accuracy:0.2447\n",
    "Epoch #79: Loss:1.5990, Accuracy:0.2411, Validation Loss:1.6033, Validation Accuracy:0.2479\n",
    "Epoch #80: Loss:1.5983, Accuracy:0.2485, Validation Loss:1.6029, Validation Accuracy:0.2479\n",
    "Epoch #81: Loss:1.5982, Accuracy:0.2497, Validation Loss:1.6027, Validation Accuracy:0.2430\n",
    "Epoch #82: Loss:1.5981, Accuracy:0.2501, Validation Loss:1.6030, Validation Accuracy:0.2447\n",
    "Epoch #83: Loss:1.5978, Accuracy:0.2509, Validation Loss:1.6031, Validation Accuracy:0.2479\n",
    "Epoch #84: Loss:1.5979, Accuracy:0.2522, Validation Loss:1.6030, Validation Accuracy:0.2414\n",
    "Epoch #85: Loss:1.5973, Accuracy:0.2522, Validation Loss:1.6031, Validation Accuracy:0.2381\n",
    "Epoch #86: Loss:1.5966, Accuracy:0.2517, Validation Loss:1.6032, Validation Accuracy:0.2479\n",
    "Epoch #87: Loss:1.5965, Accuracy:0.2550, Validation Loss:1.6041, Validation Accuracy:0.2463\n",
    "Epoch #88: Loss:1.5958, Accuracy:0.2554, Validation Loss:1.6045, Validation Accuracy:0.2397\n",
    "Epoch #89: Loss:1.5966, Accuracy:0.2497, Validation Loss:1.6055, Validation Accuracy:0.2365\n",
    "Epoch #90: Loss:1.5955, Accuracy:0.2526, Validation Loss:1.6061, Validation Accuracy:0.2348\n",
    "Epoch #91: Loss:1.5943, Accuracy:0.2567, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #92: Loss:1.5945, Accuracy:0.2509, Validation Loss:1.6053, Validation Accuracy:0.2414\n",
    "Epoch #93: Loss:1.5945, Accuracy:0.2517, Validation Loss:1.6065, Validation Accuracy:0.2397\n",
    "Epoch #94: Loss:1.5932, Accuracy:0.2563, Validation Loss:1.6055, Validation Accuracy:0.2463\n",
    "Epoch #95: Loss:1.5925, Accuracy:0.2620, Validation Loss:1.6076, Validation Accuracy:0.2397\n",
    "Epoch #96: Loss:1.5931, Accuracy:0.2575, Validation Loss:1.6085, Validation Accuracy:0.2299\n",
    "Epoch #97: Loss:1.5930, Accuracy:0.2587, Validation Loss:1.6090, Validation Accuracy:0.2397\n",
    "Epoch #98: Loss:1.5926, Accuracy:0.2604, Validation Loss:1.6093, Validation Accuracy:0.2299\n",
    "Epoch #99: Loss:1.5950, Accuracy:0.2538, Validation Loss:1.6094, Validation Accuracy:0.2315\n",
    "Epoch #100: Loss:1.5947, Accuracy:0.2452, Validation Loss:1.6077, Validation Accuracy:0.2414\n",
    "Epoch #101: Loss:1.5945, Accuracy:0.2513, Validation Loss:1.6085, Validation Accuracy:0.2512\n",
    "Epoch #102: Loss:1.5929, Accuracy:0.2567, Validation Loss:1.6096, Validation Accuracy:0.2414\n",
    "Epoch #103: Loss:1.5920, Accuracy:0.2559, Validation Loss:1.6089, Validation Accuracy:0.2414\n",
    "Epoch #104: Loss:1.5913, Accuracy:0.2583, Validation Loss:1.6115, Validation Accuracy:0.2332\n",
    "Epoch #105: Loss:1.5944, Accuracy:0.2517, Validation Loss:1.6106, Validation Accuracy:0.2282\n",
    "Epoch #106: Loss:1.5928, Accuracy:0.2452, Validation Loss:1.6107, Validation Accuracy:0.2315\n",
    "Epoch #107: Loss:1.5916, Accuracy:0.2476, Validation Loss:1.6124, Validation Accuracy:0.2414\n",
    "Epoch #108: Loss:1.5895, Accuracy:0.2575, Validation Loss:1.6090, Validation Accuracy:0.2447\n",
    "Epoch #109: Loss:1.5900, Accuracy:0.2567, Validation Loss:1.6089, Validation Accuracy:0.2250\n",
    "Epoch #110: Loss:1.5908, Accuracy:0.2559, Validation Loss:1.6094, Validation Accuracy:0.2299\n",
    "Epoch #111: Loss:1.5920, Accuracy:0.2472, Validation Loss:1.6087, Validation Accuracy:0.2282\n",
    "Epoch #112: Loss:1.5911, Accuracy:0.2513, Validation Loss:1.6079, Validation Accuracy:0.2233\n",
    "Epoch #113: Loss:1.5926, Accuracy:0.2517, Validation Loss:1.6114, Validation Accuracy:0.2233\n",
    "Epoch #114: Loss:1.5896, Accuracy:0.2542, Validation Loss:1.6094, Validation Accuracy:0.2135\n",
    "Epoch #115: Loss:1.5900, Accuracy:0.2485, Validation Loss:1.6098, Validation Accuracy:0.2217\n",
    "Epoch #116: Loss:1.5885, Accuracy:0.2554, Validation Loss:1.6099, Validation Accuracy:0.2118\n",
    "Epoch #117: Loss:1.5870, Accuracy:0.2575, Validation Loss:1.6090, Validation Accuracy:0.2184\n",
    "Epoch #118: Loss:1.5862, Accuracy:0.2600, Validation Loss:1.6128, Validation Accuracy:0.2233\n",
    "Epoch #119: Loss:1.5856, Accuracy:0.2546, Validation Loss:1.6146, Validation Accuracy:0.2233\n",
    "Epoch #120: Loss:1.5846, Accuracy:0.2612, Validation Loss:1.6165, Validation Accuracy:0.2151\n",
    "Epoch #121: Loss:1.5841, Accuracy:0.2616, Validation Loss:1.6190, Validation Accuracy:0.2184\n",
    "Epoch #122: Loss:1.5891, Accuracy:0.2579, Validation Loss:1.6183, Validation Accuracy:0.2085\n",
    "Epoch #123: Loss:1.5890, Accuracy:0.2509, Validation Loss:1.6272, Validation Accuracy:0.2118\n",
    "Epoch #124: Loss:1.5885, Accuracy:0.2571, Validation Loss:1.6215, Validation Accuracy:0.2266\n",
    "Epoch #125: Loss:1.5885, Accuracy:0.2522, Validation Loss:1.6154, Validation Accuracy:0.2135\n",
    "Epoch #126: Loss:1.5875, Accuracy:0.2559, Validation Loss:1.6161, Validation Accuracy:0.2118\n",
    "Epoch #127: Loss:1.5863, Accuracy:0.2620, Validation Loss:1.6150, Validation Accuracy:0.2282\n",
    "Epoch #128: Loss:1.5888, Accuracy:0.2587, Validation Loss:1.6160, Validation Accuracy:0.2167\n",
    "Epoch #129: Loss:1.5902, Accuracy:0.2485, Validation Loss:1.6135, Validation Accuracy:0.2250\n",
    "Epoch #130: Loss:1.5875, Accuracy:0.2550, Validation Loss:1.6170, Validation Accuracy:0.2020\n",
    "Epoch #131: Loss:1.5880, Accuracy:0.2480, Validation Loss:1.6158, Validation Accuracy:0.2250\n",
    "Epoch #132: Loss:1.5870, Accuracy:0.2509, Validation Loss:1.6185, Validation Accuracy:0.2184\n",
    "Epoch #133: Loss:1.5867, Accuracy:0.2517, Validation Loss:1.6162, Validation Accuracy:0.2200\n",
    "Epoch #134: Loss:1.5878, Accuracy:0.2554, Validation Loss:1.6161, Validation Accuracy:0.2233\n",
    "Epoch #135: Loss:1.5877, Accuracy:0.2554, Validation Loss:1.6195, Validation Accuracy:0.2200\n",
    "Epoch #136: Loss:1.5836, Accuracy:0.2563, Validation Loss:1.6232, Validation Accuracy:0.2184\n",
    "Epoch #137: Loss:1.5819, Accuracy:0.2723, Validation Loss:1.6226, Validation Accuracy:0.2250\n",
    "Epoch #138: Loss:1.5847, Accuracy:0.2505, Validation Loss:1.6203, Validation Accuracy:0.2250\n",
    "Epoch #139: Loss:1.5867, Accuracy:0.2583, Validation Loss:1.6232, Validation Accuracy:0.2135\n",
    "Epoch #140: Loss:1.5829, Accuracy:0.2579, Validation Loss:1.6229, Validation Accuracy:0.2184\n",
    "Epoch #141: Loss:1.5834, Accuracy:0.2571, Validation Loss:1.6184, Validation Accuracy:0.2233\n",
    "Epoch #142: Loss:1.5840, Accuracy:0.2571, Validation Loss:1.6214, Validation Accuracy:0.2217\n",
    "Epoch #143: Loss:1.5827, Accuracy:0.2587, Validation Loss:1.6253, Validation Accuracy:0.2282\n",
    "Epoch #144: Loss:1.5791, Accuracy:0.2632, Validation Loss:1.6284, Validation Accuracy:0.2266\n",
    "Epoch #145: Loss:1.5786, Accuracy:0.2669, Validation Loss:1.6247, Validation Accuracy:0.2282\n",
    "Epoch #146: Loss:1.5781, Accuracy:0.2637, Validation Loss:1.6290, Validation Accuracy:0.2233\n",
    "Epoch #147: Loss:1.5759, Accuracy:0.2587, Validation Loss:1.6285, Validation Accuracy:0.2299\n",
    "Epoch #148: Loss:1.5766, Accuracy:0.2678, Validation Loss:1.6317, Validation Accuracy:0.2069\n",
    "Epoch #149: Loss:1.5775, Accuracy:0.2522, Validation Loss:1.6317, Validation Accuracy:0.2151\n",
    "Epoch #150: Loss:1.5821, Accuracy:0.2756, Validation Loss:1.6387, Validation Accuracy:0.2085\n",
    "Epoch #151: Loss:1.5832, Accuracy:0.2554, Validation Loss:1.6329, Validation Accuracy:0.2217\n",
    "Epoch #152: Loss:1.5782, Accuracy:0.2706, Validation Loss:1.6335, Validation Accuracy:0.2003\n",
    "Epoch #153: Loss:1.5781, Accuracy:0.2612, Validation Loss:1.6315, Validation Accuracy:0.2069\n",
    "Epoch #154: Loss:1.5802, Accuracy:0.2682, Validation Loss:1.6261, Validation Accuracy:0.2118\n",
    "Epoch #155: Loss:1.5764, Accuracy:0.2661, Validation Loss:1.6312, Validation Accuracy:0.2135\n",
    "Epoch #156: Loss:1.5783, Accuracy:0.2649, Validation Loss:1.6256, Validation Accuracy:0.2135\n",
    "Epoch #157: Loss:1.5770, Accuracy:0.2604, Validation Loss:1.6302, Validation Accuracy:0.2069\n",
    "Epoch #158: Loss:1.5775, Accuracy:0.2690, Validation Loss:1.6205, Validation Accuracy:0.2250\n",
    "Epoch #159: Loss:1.5765, Accuracy:0.2702, Validation Loss:1.6299, Validation Accuracy:0.2020\n",
    "Epoch #160: Loss:1.5781, Accuracy:0.2608, Validation Loss:1.6290, Validation Accuracy:0.2135\n",
    "Epoch #161: Loss:1.5753, Accuracy:0.2678, Validation Loss:1.6272, Validation Accuracy:0.2200\n",
    "Epoch #162: Loss:1.5737, Accuracy:0.2674, Validation Loss:1.6296, Validation Accuracy:0.2135\n",
    "Epoch #163: Loss:1.5743, Accuracy:0.2678, Validation Loss:1.6259, Validation Accuracy:0.2315\n",
    "Epoch #164: Loss:1.5808, Accuracy:0.2563, Validation Loss:1.6287, Validation Accuracy:0.2167\n",
    "Epoch #165: Loss:1.5762, Accuracy:0.2678, Validation Loss:1.6305, Validation Accuracy:0.2102\n",
    "Epoch #166: Loss:1.5727, Accuracy:0.2789, Validation Loss:1.6269, Validation Accuracy:0.2167\n",
    "Epoch #167: Loss:1.5743, Accuracy:0.2698, Validation Loss:1.6272, Validation Accuracy:0.2200\n",
    "Epoch #168: Loss:1.5738, Accuracy:0.2747, Validation Loss:1.6229, Validation Accuracy:0.2151\n",
    "Epoch #169: Loss:1.5696, Accuracy:0.2702, Validation Loss:1.6242, Validation Accuracy:0.2184\n",
    "Epoch #170: Loss:1.5693, Accuracy:0.2674, Validation Loss:1.6260, Validation Accuracy:0.2167\n",
    "Epoch #171: Loss:1.5721, Accuracy:0.2661, Validation Loss:1.6295, Validation Accuracy:0.2102\n",
    "Epoch #172: Loss:1.5715, Accuracy:0.2632, Validation Loss:1.6255, Validation Accuracy:0.2102\n",
    "Epoch #173: Loss:1.5698, Accuracy:0.2645, Validation Loss:1.6355, Validation Accuracy:0.2217\n",
    "Epoch #174: Loss:1.5741, Accuracy:0.2674, Validation Loss:1.6254, Validation Accuracy:0.2217\n",
    "Epoch #175: Loss:1.5753, Accuracy:0.2698, Validation Loss:1.6354, Validation Accuracy:0.2332\n",
    "Epoch #176: Loss:1.5706, Accuracy:0.2645, Validation Loss:1.6241, Validation Accuracy:0.2414\n",
    "Epoch #177: Loss:1.5709, Accuracy:0.2780, Validation Loss:1.6326, Validation Accuracy:0.2135\n",
    "Epoch #178: Loss:1.5675, Accuracy:0.2678, Validation Loss:1.6287, Validation Accuracy:0.2151\n",
    "Epoch #179: Loss:1.5667, Accuracy:0.2735, Validation Loss:1.6290, Validation Accuracy:0.2200\n",
    "Epoch #180: Loss:1.5641, Accuracy:0.2784, Validation Loss:1.6365, Validation Accuracy:0.2036\n",
    "Epoch #181: Loss:1.5702, Accuracy:0.2747, Validation Loss:1.6394, Validation Accuracy:0.2282\n",
    "Epoch #182: Loss:1.5858, Accuracy:0.2624, Validation Loss:1.6609, Validation Accuracy:0.2184\n",
    "Epoch #183: Loss:1.6143, Accuracy:0.2374, Validation Loss:1.6424, Validation Accuracy:0.2332\n",
    "Epoch #184: Loss:1.6116, Accuracy:0.2423, Validation Loss:1.6185, Validation Accuracy:0.2200\n",
    "Epoch #185: Loss:1.5948, Accuracy:0.2402, Validation Loss:1.6220, Validation Accuracy:0.2332\n",
    "Epoch #186: Loss:1.5940, Accuracy:0.2505, Validation Loss:1.6169, Validation Accuracy:0.2167\n",
    "Epoch #187: Loss:1.5905, Accuracy:0.2608, Validation Loss:1.6148, Validation Accuracy:0.2266\n",
    "Epoch #188: Loss:1.5917, Accuracy:0.2522, Validation Loss:1.6121, Validation Accuracy:0.2282\n",
    "Epoch #189: Loss:1.5909, Accuracy:0.2550, Validation Loss:1.6165, Validation Accuracy:0.2447\n",
    "Epoch #190: Loss:1.5885, Accuracy:0.2489, Validation Loss:1.6232, Validation Accuracy:0.1905\n",
    "Epoch #191: Loss:1.5884, Accuracy:0.2604, Validation Loss:1.6173, Validation Accuracy:0.1987\n",
    "Epoch #192: Loss:1.5851, Accuracy:0.2628, Validation Loss:1.6162, Validation Accuracy:0.2250\n",
    "Epoch #193: Loss:1.5853, Accuracy:0.2575, Validation Loss:1.6172, Validation Accuracy:0.2250\n",
    "Epoch #194: Loss:1.5818, Accuracy:0.2604, Validation Loss:1.6205, Validation Accuracy:0.2151\n",
    "Epoch #195: Loss:1.5795, Accuracy:0.2624, Validation Loss:1.6217, Validation Accuracy:0.2085\n",
    "Epoch #196: Loss:1.5763, Accuracy:0.2706, Validation Loss:1.6225, Validation Accuracy:0.2217\n",
    "Epoch #197: Loss:1.5758, Accuracy:0.2702, Validation Loss:1.6259, Validation Accuracy:0.2020\n",
    "Epoch #198: Loss:1.5745, Accuracy:0.2595, Validation Loss:1.6334, Validation Accuracy:0.2036\n",
    "Epoch #199: Loss:1.5719, Accuracy:0.2686, Validation Loss:1.6355, Validation Accuracy:0.2053\n",
    "Epoch #200: Loss:1.5703, Accuracy:0.2760, Validation Loss:1.6392, Validation Accuracy:0.2167\n",
    "Epoch #201: Loss:1.5747, Accuracy:0.2727, Validation Loss:1.6332, Validation Accuracy:0.2053\n",
    "Epoch #202: Loss:1.5752, Accuracy:0.2686, Validation Loss:1.6334, Validation Accuracy:0.2135\n",
    "Epoch #203: Loss:1.5725, Accuracy:0.2764, Validation Loss:1.6392, Validation Accuracy:0.2102\n",
    "Epoch #204: Loss:1.5693, Accuracy:0.2756, Validation Loss:1.6367, Validation Accuracy:0.2020\n",
    "Epoch #205: Loss:1.5667, Accuracy:0.2739, Validation Loss:1.6406, Validation Accuracy:0.2200\n",
    "Epoch #206: Loss:1.5659, Accuracy:0.2682, Validation Loss:1.6369, Validation Accuracy:0.2200\n",
    "Epoch #207: Loss:1.5650, Accuracy:0.2694, Validation Loss:1.6451, Validation Accuracy:0.2200\n",
    "Epoch #208: Loss:1.5653, Accuracy:0.2669, Validation Loss:1.6353, Validation Accuracy:0.2167\n",
    "Epoch #209: Loss:1.5636, Accuracy:0.2747, Validation Loss:1.6413, Validation Accuracy:0.2151\n",
    "Epoch #210: Loss:1.5619, Accuracy:0.2719, Validation Loss:1.6406, Validation Accuracy:0.2184\n",
    "Epoch #211: Loss:1.5773, Accuracy:0.2669, Validation Loss:1.6421, Validation Accuracy:0.2200\n",
    "Epoch #212: Loss:1.5828, Accuracy:0.2530, Validation Loss:1.6412, Validation Accuracy:0.2003\n",
    "Epoch #213: Loss:1.5748, Accuracy:0.2657, Validation Loss:1.6311, Validation Accuracy:0.2020\n",
    "Epoch #214: Loss:1.5703, Accuracy:0.2702, Validation Loss:1.6395, Validation Accuracy:0.2003\n",
    "Epoch #215: Loss:1.5705, Accuracy:0.2579, Validation Loss:1.6246, Validation Accuracy:0.2315\n",
    "Epoch #216: Loss:1.5721, Accuracy:0.2719, Validation Loss:1.6280, Validation Accuracy:0.2545\n",
    "Epoch #217: Loss:1.5669, Accuracy:0.2747, Validation Loss:1.6335, Validation Accuracy:0.2266\n",
    "Epoch #218: Loss:1.5663, Accuracy:0.2719, Validation Loss:1.6307, Validation Accuracy:0.2250\n",
    "Epoch #219: Loss:1.5619, Accuracy:0.2805, Validation Loss:1.6311, Validation Accuracy:0.2217\n",
    "Epoch #220: Loss:1.5607, Accuracy:0.2764, Validation Loss:1.6366, Validation Accuracy:0.2200\n",
    "Epoch #221: Loss:1.5597, Accuracy:0.2789, Validation Loss:1.6341, Validation Accuracy:0.2250\n",
    "Epoch #222: Loss:1.5604, Accuracy:0.2789, Validation Loss:1.6375, Validation Accuracy:0.2200\n",
    "Epoch #223: Loss:1.5622, Accuracy:0.2756, Validation Loss:1.6332, Validation Accuracy:0.2233\n",
    "Epoch #224: Loss:1.5605, Accuracy:0.2764, Validation Loss:1.6303, Validation Accuracy:0.2233\n",
    "Epoch #225: Loss:1.5576, Accuracy:0.2854, Validation Loss:1.6260, Validation Accuracy:0.2299\n",
    "Epoch #226: Loss:1.5569, Accuracy:0.2830, Validation Loss:1.6299, Validation Accuracy:0.2233\n",
    "Epoch #227: Loss:1.5597, Accuracy:0.2760, Validation Loss:1.6269, Validation Accuracy:0.2233\n",
    "Epoch #228: Loss:1.5636, Accuracy:0.2702, Validation Loss:1.6356, Validation Accuracy:0.2217\n",
    "Epoch #229: Loss:1.5609, Accuracy:0.2735, Validation Loss:1.6340, Validation Accuracy:0.2200\n",
    "Epoch #230: Loss:1.5575, Accuracy:0.2825, Validation Loss:1.6470, Validation Accuracy:0.2200\n",
    "Epoch #231: Loss:1.5574, Accuracy:0.2669, Validation Loss:1.6415, Validation Accuracy:0.2167\n",
    "Epoch #232: Loss:1.5531, Accuracy:0.2850, Validation Loss:1.6514, Validation Accuracy:0.2184\n",
    "Epoch #233: Loss:1.5522, Accuracy:0.2764, Validation Loss:1.6405, Validation Accuracy:0.2381\n",
    "Epoch #234: Loss:1.5480, Accuracy:0.2883, Validation Loss:1.6450, Validation Accuracy:0.2299\n",
    "Epoch #235: Loss:1.5534, Accuracy:0.2838, Validation Loss:1.6339, Validation Accuracy:0.2315\n",
    "Epoch #236: Loss:1.5562, Accuracy:0.2821, Validation Loss:1.6307, Validation Accuracy:0.2266\n",
    "Epoch #237: Loss:1.5490, Accuracy:0.2805, Validation Loss:1.6489, Validation Accuracy:0.2102\n",
    "Epoch #238: Loss:1.5497, Accuracy:0.2780, Validation Loss:1.6426, Validation Accuracy:0.1954\n",
    "Epoch #239: Loss:1.5509, Accuracy:0.2793, Validation Loss:1.6436, Validation Accuracy:0.2118\n",
    "Epoch #240: Loss:1.5578, Accuracy:0.2690, Validation Loss:1.6331, Validation Accuracy:0.2184\n",
    "Epoch #241: Loss:1.5634, Accuracy:0.2739, Validation Loss:1.6462, Validation Accuracy:0.2167\n",
    "Epoch #242: Loss:1.5627, Accuracy:0.2743, Validation Loss:1.6451, Validation Accuracy:0.2184\n",
    "Epoch #243: Loss:1.5622, Accuracy:0.2715, Validation Loss:1.6383, Validation Accuracy:0.2151\n",
    "Epoch #244: Loss:1.5598, Accuracy:0.2842, Validation Loss:1.6354, Validation Accuracy:0.2200\n",
    "Epoch #245: Loss:1.5609, Accuracy:0.2801, Validation Loss:1.6302, Validation Accuracy:0.2233\n",
    "Epoch #246: Loss:1.5617, Accuracy:0.2731, Validation Loss:1.6337, Validation Accuracy:0.2250\n",
    "Epoch #247: Loss:1.5596, Accuracy:0.2805, Validation Loss:1.6319, Validation Accuracy:0.2217\n",
    "Epoch #248: Loss:1.5568, Accuracy:0.2764, Validation Loss:1.6463, Validation Accuracy:0.2250\n",
    "Epoch #249: Loss:1.5545, Accuracy:0.2780, Validation Loss:1.6337, Validation Accuracy:0.2299\n",
    "Epoch #250: Loss:1.5551, Accuracy:0.2789, Validation Loss:1.6403, Validation Accuracy:0.2315\n",
    "Epoch #251: Loss:1.5563, Accuracy:0.2871, Validation Loss:1.6395, Validation Accuracy:0.2299\n",
    "Epoch #252: Loss:1.5517, Accuracy:0.2887, Validation Loss:1.6432, Validation Accuracy:0.2315\n",
    "Epoch #253: Loss:1.5507, Accuracy:0.2842, Validation Loss:1.6410, Validation Accuracy:0.2250\n",
    "Epoch #254: Loss:1.5491, Accuracy:0.2920, Validation Loss:1.6593, Validation Accuracy:0.2266\n",
    "Epoch #255: Loss:1.5487, Accuracy:0.2846, Validation Loss:1.6685, Validation Accuracy:0.2184\n",
    "Epoch #256: Loss:1.5503, Accuracy:0.2842, Validation Loss:1.6547, Validation Accuracy:0.2233\n",
    "Epoch #257: Loss:1.5540, Accuracy:0.2903, Validation Loss:1.6515, Validation Accuracy:0.2069\n",
    "Epoch #258: Loss:1.5540, Accuracy:0.2850, Validation Loss:1.6485, Validation Accuracy:0.2250\n",
    "Epoch #259: Loss:1.5497, Accuracy:0.2895, Validation Loss:1.6562, Validation Accuracy:0.2118\n",
    "Epoch #260: Loss:1.5500, Accuracy:0.2899, Validation Loss:1.6501, Validation Accuracy:0.2151\n",
    "Epoch #261: Loss:1.5459, Accuracy:0.2875, Validation Loss:1.6743, Validation Accuracy:0.2167\n",
    "Epoch #262: Loss:1.5401, Accuracy:0.2887, Validation Loss:1.6653, Validation Accuracy:0.2036\n",
    "Epoch #263: Loss:1.5427, Accuracy:0.2936, Validation Loss:1.6691, Validation Accuracy:0.2184\n",
    "Epoch #264: Loss:1.5413, Accuracy:0.2850, Validation Loss:1.6529, Validation Accuracy:0.2020\n",
    "Epoch #265: Loss:1.5481, Accuracy:0.2825, Validation Loss:1.6469, Validation Accuracy:0.2102\n",
    "Epoch #266: Loss:1.5493, Accuracy:0.2846, Validation Loss:1.6525, Validation Accuracy:0.2053\n",
    "Epoch #267: Loss:1.5456, Accuracy:0.2858, Validation Loss:1.6473, Validation Accuracy:0.2102\n",
    "Epoch #268: Loss:1.5465, Accuracy:0.2776, Validation Loss:1.6476, Validation Accuracy:0.2003\n",
    "Epoch #269: Loss:1.5448, Accuracy:0.2846, Validation Loss:1.6476, Validation Accuracy:0.2167\n",
    "Epoch #270: Loss:1.5383, Accuracy:0.2899, Validation Loss:1.6565, Validation Accuracy:0.2069\n",
    "Epoch #271: Loss:1.5381, Accuracy:0.2879, Validation Loss:1.6569, Validation Accuracy:0.2200\n",
    "Epoch #272: Loss:1.5386, Accuracy:0.2945, Validation Loss:1.6587, Validation Accuracy:0.2167\n",
    "Epoch #273: Loss:1.5381, Accuracy:0.2895, Validation Loss:1.6698, Validation Accuracy:0.2233\n",
    "Epoch #274: Loss:1.5349, Accuracy:0.2982, Validation Loss:1.6613, Validation Accuracy:0.2266\n",
    "Epoch #275: Loss:1.5326, Accuracy:0.2920, Validation Loss:1.6752, Validation Accuracy:0.2102\n",
    "Epoch #276: Loss:1.5310, Accuracy:0.3006, Validation Loss:1.6638, Validation Accuracy:0.2217\n",
    "Epoch #277: Loss:1.5294, Accuracy:0.2977, Validation Loss:1.6673, Validation Accuracy:0.2217\n",
    "Epoch #278: Loss:1.5255, Accuracy:0.3080, Validation Loss:1.6620, Validation Accuracy:0.2184\n",
    "Epoch #279: Loss:1.5281, Accuracy:0.2961, Validation Loss:1.6762, Validation Accuracy:0.2102\n",
    "Epoch #280: Loss:1.5315, Accuracy:0.2920, Validation Loss:1.6860, Validation Accuracy:0.1954\n",
    "Epoch #281: Loss:1.5314, Accuracy:0.2862, Validation Loss:1.6688, Validation Accuracy:0.2135\n",
    "Epoch #282: Loss:1.5308, Accuracy:0.2879, Validation Loss:1.6670, Validation Accuracy:0.2184\n",
    "Epoch #283: Loss:1.5298, Accuracy:0.2891, Validation Loss:1.6773, Validation Accuracy:0.2135\n",
    "Epoch #284: Loss:1.5373, Accuracy:0.2875, Validation Loss:1.6835, Validation Accuracy:0.1954\n",
    "Epoch #285: Loss:1.5451, Accuracy:0.2817, Validation Loss:1.6673, Validation Accuracy:0.2135\n",
    "Epoch #286: Loss:1.5314, Accuracy:0.2908, Validation Loss:1.6675, Validation Accuracy:0.2167\n",
    "Epoch #287: Loss:1.5368, Accuracy:0.2813, Validation Loss:1.6588, Validation Accuracy:0.2167\n",
    "Epoch #288: Loss:1.5348, Accuracy:0.2945, Validation Loss:1.6636, Validation Accuracy:0.2217\n",
    "Epoch #289: Loss:1.5313, Accuracy:0.2977, Validation Loss:1.6713, Validation Accuracy:0.2250\n",
    "Epoch #290: Loss:1.5318, Accuracy:0.3002, Validation Loss:1.6551, Validation Accuracy:0.2397\n",
    "Epoch #291: Loss:1.5283, Accuracy:0.2940, Validation Loss:1.6695, Validation Accuracy:0.2167\n",
    "Epoch #292: Loss:1.5173, Accuracy:0.3051, Validation Loss:1.6744, Validation Accuracy:0.2250\n",
    "Epoch #293: Loss:1.5171, Accuracy:0.3072, Validation Loss:1.6915, Validation Accuracy:0.2167\n",
    "Epoch #294: Loss:1.5202, Accuracy:0.2969, Validation Loss:1.6861, Validation Accuracy:0.2085\n",
    "Epoch #295: Loss:1.5211, Accuracy:0.3002, Validation Loss:1.6824, Validation Accuracy:0.2315\n",
    "Epoch #296: Loss:1.5211, Accuracy:0.2990, Validation Loss:1.6900, Validation Accuracy:0.2217\n",
    "Epoch #297: Loss:1.5125, Accuracy:0.3088, Validation Loss:1.6874, Validation Accuracy:0.1954\n",
    "Epoch #298: Loss:1.5142, Accuracy:0.3097, Validation Loss:1.6855, Validation Accuracy:0.2184\n",
    "Epoch #299: Loss:1.5184, Accuracy:0.2998, Validation Loss:1.6874, Validation Accuracy:0.2299\n",
    "Epoch #300: Loss:1.5279, Accuracy:0.2977, Validation Loss:1.6844, Validation Accuracy:0.2118\n",
    "\n",
    "Test:\n",
    "Test Loss:1.68437409, Accuracy:0.2118\n",
    "Labels: ['01', '04', '02', '05', '03']\n",
    "Confusion Matrix:\n",
    "      01  04  02  05  03\n",
    "t:01  13  17   2  78  16\n",
    "t:04   8  20   5  72   7\n",
    "t:02  13  17   3  64  17\n",
    "t:05  12  19   6  87  18\n",
    "t:03  14   9   6  80   6\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.22      0.10      0.14       126\n",
    "          04       0.24      0.18      0.21       112\n",
    "          02       0.14      0.03      0.04       114\n",
    "          05       0.23      0.61      0.33       142\n",
    "          03       0.09      0.05      0.07       115\n",
    "\n",
    "    accuracy                           0.21       609\n",
    "   macro avg       0.18      0.19      0.16       609\n",
    "weighted avg       0.19      0.21      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.25 23:47:51 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 43 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6142839197259036, 1.6091736950506326, 1.608682583901291, 1.6072311563836335, 1.6056668427581662, 1.6060399821239153, 1.6062580994980284, 1.6071797204135088, 1.6068915185474215, 1.6068626877122325, 1.6064021495371226, 1.60509266974695, 1.6050556905946904, 1.6045219761201706, 1.6042518741000071, 1.6041802890195047, 1.6038729886116065, 1.6035839969105712, 1.603395607083889, 1.6029740096313025, 1.6028289614835591, 1.6024457241709793, 1.6048691750355737, 1.6044807424294734, 1.6044682591223756, 1.6037800766172863, 1.6004426481297058, 1.5994791072381933, 1.6016807150958208, 1.6003359145131604, 1.59919275791187, 1.6014423673767566, 1.6030662883874427, 1.6008565866301212, 1.602292918219355, 1.6017716976222147, 1.6015699753424608, 1.6017239992254473, 1.6012582684972603, 1.6009731733153019, 1.6001141601791131, 1.600667036225643, 1.600548169883014, 1.6014011221370479, 1.6009386689792127, 1.600785805674022, 1.6000276670863085, 1.600225163210789, 1.5998709338834916, 1.599755853267726, 1.600285276012076, 1.6001439550631544, 1.6001494680523676, 1.6006658871968586, 1.6009848685491652, 1.599943507872583, 1.6005530073529197, 1.5995255910312796, 1.6014789062963526, 1.6011653236176189, 1.6003292683505856, 1.6003108075491117, 1.6029786386317613, 1.611791281473069, 1.6072499761831975, 1.6078520608065752, 1.6064255233664426, 1.6067120292895338, 1.6033216924307185, 1.6028378536352774, 1.6029251950910721, 1.6026319981795814, 1.6027557597371744, 1.602450216540758, 1.6028316118838557, 1.6020447262402238, 1.6025561847906002, 1.6035903400584004, 1.603280245964163, 1.6028817514284883, 1.6026572084974968, 1.6029559804496702, 1.6031412408856922, 1.6030366360064603, 1.603147483420098, 1.603157390710364, 1.604138610398241, 1.604519203770141, 1.6055290317300506, 1.6061035617818973, 1.6056299307467707, 1.605302524292606, 1.6064951942472034, 1.6055363218968333, 1.6076208636874245, 1.608497538394333, 1.6090255523550099, 1.6093479316614336, 1.6093895122139716, 1.6076602969067828, 1.6085207867504927, 1.609595956082023, 1.6088548920228956, 1.6114742538611877, 1.6105776686582267, 1.6107138712417903, 1.6123524722207356, 1.6090259066551973, 1.6089309702561603, 1.60935768098471, 1.6087046699179413, 1.6079182231367515, 1.6113531299804036, 1.6093840700847957, 1.6097840524855114, 1.6099085807800293, 1.6090183128864306, 1.6128074022740957, 1.6146297323684191, 1.6164559235518006, 1.6189846843725746, 1.6183076243486703, 1.627169059219423, 1.6214813927711524, 1.615434667159771, 1.616106873662601, 1.6149961509923825, 1.6159513587826382, 1.6135020633832182, 1.617028252244583, 1.6158479266174517, 1.6185485437781548, 1.6162149191685693, 1.6161003770499394, 1.6195297705128862, 1.6231752177960375, 1.6226199009931341, 1.6202623082694945, 1.6231852801171038, 1.6229050116390233, 1.6184052657611265, 1.6213821816718441, 1.6253306579903037, 1.6283619176773798, 1.6246925197015647, 1.628985178881678, 1.6284729167941365, 1.6317372881915966, 1.6317359710170327, 1.6386591793085359, 1.63292306436498, 1.6334563581814319, 1.6314694018199527, 1.62613490943251, 1.6312139014696645, 1.6255950861180748, 1.6302238238856124, 1.6204633270382685, 1.629886362352982, 1.6290351099568634, 1.627230932559873, 1.6296450602401458, 1.6258700676935256, 1.6287268290574524, 1.6305006842307856, 1.6269002782887425, 1.6271557543665318, 1.622939709959359, 1.6241616619435828, 1.6260288247138213, 1.629500710905479, 1.6255068403159456, 1.6354716057064889, 1.6254393792113255, 1.6354177992527903, 1.62414121706106, 1.632646385867803, 1.6286530428136314, 1.6290148445733859, 1.6365481902813088, 1.639393170870388, 1.6609284276837002, 1.6424490833908858, 1.618468995164768, 1.6220373146248177, 1.6168531514153692, 1.6148392286989686, 1.6120745836220352, 1.616464039375042, 1.6232455665450574, 1.6173032768841447, 1.616245858383492, 1.6171823796771823, 1.6204537814865363, 1.6217446434869751, 1.6225371055415112, 1.6259298915737759, 1.6334047822529458, 1.6355428284612195, 1.6392010501257108, 1.6331749087679759, 1.6333963510829632, 1.6391692214411469, 1.6367258432463472, 1.6405947517683157, 1.6369130008522121, 1.6450684859443376, 1.635315897625264, 1.641295036463119, 1.6405793998041764, 1.6421045947740427, 1.6411656644348245, 1.6310670142886283, 1.6395125075905586, 1.6245686500922016, 1.6280346985520988, 1.6335114970778792, 1.6307095013228543, 1.6311455086142754, 1.636635870964852, 1.6340976742100832, 1.6374566273149012, 1.633151181421452, 1.6303035183297394, 1.6260470735224206, 1.6298685790282752, 1.62686884246632, 1.63561366168149, 1.6340007872025564, 1.6469855608024033, 1.6415471455146526, 1.6514282316605642, 1.640492256834785, 1.6450219283550245, 1.633938900747127, 1.6306576235540982, 1.6489379642827953, 1.6426294349097266, 1.6436452286192544, 1.633074606972179, 1.646232638061536, 1.6450735720116125, 1.6382819930908128, 1.6354085240262286, 1.630171123005095, 1.633714545536511, 1.631905964442662, 1.6462535100617433, 1.6336821098437255, 1.6402816876206296, 1.639534914826329, 1.6431627496709964, 1.6410437466084271, 1.6592890529209756, 1.6684769767845793, 1.6547410022253277, 1.6514749442806775, 1.6485149801658292, 1.6562139455516547, 1.6500961970421677, 1.6742919828308431, 1.6653305842175663, 1.6690954155914106, 1.6528666144717112, 1.6469119808748243, 1.6524757479603458, 1.6473286245844048, 1.6476127480834184, 1.6475878092651493, 1.6565052952085222, 1.6569033921841525, 1.6587484270481054, 1.6697793124344549, 1.6612934768689285, 1.6752193324475844, 1.6638388747260684, 1.6672673017912114, 1.6619510746550286, 1.67620949028748, 1.6860446223288725, 1.6688295992332922, 1.666957066955629, 1.6772938309044674, 1.6835064128506163, 1.6673038851451405, 1.6675080542493923, 1.6587524380785688, 1.6636390946377282, 1.6712692521867298, 1.6550943861258245, 1.6694889894651466, 1.6744144528565932, 1.691488533184446, 1.686116527845511, 1.6824242264179174, 1.6900494425559083, 1.6873984996516913, 1.6855091206937391, 1.687366378914155, 1.6843741358990347], 'val_acc': [0.18555008178371907, 0.20853858762871846, 0.2397372720178908, 0.23809523589309606, 0.23316912742083884, 0.2331691275187118, 0.22660098509933366, 0.2249589489745389, 0.2315270912960441, 0.23316912742083884, 0.23316912742083884, 0.23481116354563358, 0.23316912742083884, 0.23481116354563358, 0.23481116354563358, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.2315270912960441, 0.23316912742083884, 0.23316912742083884, 0.2315270934737179, 0.23316912742083884, 0.23316912742083884, 0.23152709357159088, 0.23316912959851263, 0.24302134436535328, 0.24630541661494276, 0.23152709149179004, 0.23645319986617427, 0.24466338039227503, 0.23645320184810212, 0.23316912742083884, 0.23809523767927793, 0.2167487682526922, 0.22331691076994334, 0.2249589467968651, 0.21018062394925918, 0.2413793082405585, 0.2413793082405585, 0.2315270912960441, 0.23481116582118036, 0.22988505517124935, 0.23481116374137953, 0.23316912742083884, 0.23481116354563358, 0.23809523589309606, 0.2397372720178908, 0.23809523589309606, 0.23809523579522307, 0.2413793082405585, 0.23316912742083884, 0.2364532019459751, 0.23481116582118036, 0.2364532019459751, 0.23645320175022916, 0.2183908045732329, 0.2430213442674803, 0.24630541661494276, 0.23316912742083884, 0.2249589489745389, 0.23809523579522307, 0.2397372720178908, 0.20525451508551004, 0.2167487677633273, 0.2151067318342785, 0.2249589489745389, 0.23809523589309606, 0.24630541661494276, 0.2315270933758449, 0.24137930814268554, 0.24630541661494276, 0.24466338049014802, 0.24630541661494276, 0.24302134436535328, 0.2479474527397375, 0.24630541661494276, 0.24466338058802098, 0.2479474527397375, 0.2479474527397375, 0.2430213442674803, 0.24466338039227503, 0.2479474527397375, 0.24137930814268554, 0.23809523589309606, 0.2479474527397375, 0.2463054164191968, 0.2397372720178908, 0.23645319967042833, 0.2348111656254344, 0.2331691275187118, 0.24137930814268554, 0.2397372719200178, 0.24630541651706978, 0.2397372741955646, 0.2298850553669953, 0.2397372719200178, 0.2298850552691223, 0.23152709357159088, 0.24137930814268554, 0.251231524891454, 0.2413793082405585, 0.2413793082405585, 0.23316912969638562, 0.22824301904645458, 0.2315270912960441, 0.24137930814268554, 0.24466338049014802, 0.22495894887666593, 0.22988505744679613, 0.2282430213220014, 0.22331691284974417, 0.22331691284974417, 0.21346469610097568, 0.22167487672494943, 0.21182265987830795, 0.21839080437748695, 0.22331691284974417, 0.22331691294761713, 0.2151067323236434, 0.2183908044753599, 0.20853858772659145, 0.21182265997618094, 0.22660098519720664, 0.21346469610097568, 0.211822659780435, 0.22824301914432757, 0.21674876835056517, 0.22495894689473808, 0.20197044293379354, 0.22495894689473808, 0.21839080418174098, 0.2200328404044087, 0.22331691275187118, 0.2200328404044087, 0.21839080418174098, 0.22495894907241187, 0.2249589489745389, 0.21346469600310272, 0.2183908023955591, 0.22331691294761713, 0.22167487672494943, 0.22824301914432757, 0.22660098301953283, 0.22824302093050947, 0.22331691067207035, 0.2298850552691223, 0.20689655140605073, 0.2151067318342785, 0.20853858772659145, 0.22167487672494943, 0.20032840661325282, 0.20689655140605073, 0.21182265987830795, 0.2134646939233019, 0.21346469580735675, 0.20689655140605073, 0.2249589489745389, 0.20197044273804757, 0.21346469619884867, 0.22003284060015468, 0.21346469590522973, 0.2315270912960441, 0.2167487682526922, 0.21018062355776726, 0.21674876835056517, 0.2200328404044087, 0.21510673203002448, 0.21839080408386802, 0.21674876805694623, 0.21018062345989427, 0.21018062355776726, 0.22167487652920345, 0.22167487672494943, 0.23316912950063964, 0.24137931032035934, 0.21346469590522973, 0.21510673212789747, 0.22003284069802764, 0.20361247905858829, 0.22824301904645458, 0.21839080418174098, 0.2331691275187118, 0.22003283842248086, 0.2331691275187118, 0.21674876617289138, 0.22660098301953283, 0.22824301904645458, 0.24466338049014802, 0.1904761896687384, 0.19868637068420403, 0.22495894689473808, 0.2249589467968651, 0.2151067318342785, 0.20853858772659145, 0.2216748745472756, 0.2019704430316665, 0.20361247925433423, 0.20525451547700196, 0.21674876844843816, 0.20525451547700196, 0.21346469610097568, 0.21018062365564025, 0.20197044322741248, 0.2200328404044087, 0.22003284069802764, 0.22003283861822684, 0.21674876835056517, 0.21510673222577043, 0.21839080437748695, 0.2200328405022817, 0.20032840700474475, 0.20197044322741248, 0.2003284067111258, 0.2315270912960441, 0.2545155971410435, 0.22660098500146067, 0.22495894907241187, 0.22167487662707644, 0.2200328405022817, 0.2249589467968651, 0.22003283842248086, 0.22331691067207035, 0.22331691067207035, 0.22988505744679613, 0.22331691067207035, 0.22331691294761713, 0.22167487652920345, 0.22003284060015468, 0.2200328405022817, 0.21674876844843816, 0.2183908044753599, 0.23809523797289686, 0.22988505517124935, 0.23152709139391706, 0.22660098490358768, 0.21018062345989427, 0.19540229843461454, 0.21182265987830795, 0.21839080418174098, 0.21674876844843816, 0.2183908045732329, 0.21510673004809663, 0.22003283852035385, 0.2233169126539982, 0.22495894887666593, 0.22167487623558452, 0.2249589489745389, 0.22988505517124935, 0.23152709357159088, 0.22988505734892314, 0.2315270934737179, 0.22495894877879294, 0.22660098509933366, 0.21839080437748695, 0.2233169126539982, 0.20689655140605073, 0.22495894907241187, 0.211822659780435, 0.21510673222577043, 0.21674876844843816, 0.20361247915646125, 0.21839080229768612, 0.20197044293379354, 0.21018062355776726, 0.205254515281256, 0.21018062345989427, 0.2003284067111258, 0.21674876617289138, 0.20689655150392372, 0.22003283842248086, 0.21674876844843816, 0.22331691294761713, 0.22660098519720664, 0.2101806237535132, 0.2216748768228224, 0.2216748745472756, 0.2183908044753599, 0.2101806237535132, 0.19540229843461454, 0.21346469600310272, 0.2183908044753599, 0.21346469619884867, 0.19540229872823348, 0.21346469610097568, 0.21674876815481922, 0.2167487682526922, 0.2216748768228224, 0.22495894868091998, 0.2397372720178908, 0.21674876805694623, 0.2249589489745389, 0.2167487682526922, 0.20853858762871846, 0.23152709149179004, 0.2216748746451486, 0.19540229843461454, 0.21839080427961396, 0.22988505517124935, 0.21182265997618094], 'loss': [1.6196111532207387, 1.6122820225094867, 1.6105364272237068, 1.609269595537832, 1.6079890711841152, 1.6061373028177501, 1.6056357587876995, 1.6060780796427012, 1.6052167443034586, 1.6048251567190432, 1.6047588327337339, 1.604585833226386, 1.6048019066973145, 1.6043396660434637, 1.6041969621940315, 1.6040035482794353, 1.6043748742501105, 1.6038249063295995, 1.6037126920551246, 1.603317155289699, 1.6024839365996375, 1.6021803258136067, 1.6034930119524258, 1.6030873233532759, 1.602108958320696, 1.6011048699306512, 1.6012925713458834, 1.6013802428999475, 1.6006613012701578, 1.6002545504599381, 1.5997934894150532, 1.600998679029868, 1.6025151432417257, 1.6014448664271612, 1.6011073087519934, 1.6014534058267331, 1.6012423075200106, 1.6022362690441907, 1.6021322975902832, 1.6011304162610973, 1.6008958340180728, 1.6012968113779777, 1.6006385810566144, 1.601304722468711, 1.6009559016942487, 1.6008551489401157, 1.6011838814561128, 1.600692157334126, 1.6008655235263112, 1.6005005653389182, 1.6001744119048853, 1.5998395195731883, 1.599812113479912, 1.5993571642732718, 1.599483628880072, 1.600635718760794, 1.6015622715685647, 1.6024752283487966, 1.6020208299527179, 1.6002815692086974, 1.600751337916944, 1.5995334572860593, 1.5994550611448974, 1.60025782413796, 1.6042404068079328, 1.6007244218791046, 1.6031701275944954, 1.6047873069128706, 1.6028424822084713, 1.6019870961716043, 1.6021097789310088, 1.6009577328664322, 1.6007029684172518, 1.6004314323715116, 1.600329072911147, 1.6001468038656874, 1.599517278896465, 1.5994258249564828, 1.5989747776877463, 1.5982500663283425, 1.5982254213865779, 1.5980590352287527, 1.5977764855175292, 1.5979171359318727, 1.5972920707119074, 1.5965659846025815, 1.5965316546771071, 1.5958013286336001, 1.5965721219472082, 1.5955348665464586, 1.5942546766151882, 1.5944762211805497, 1.594469733110933, 1.5932353545997666, 1.5924992368451378, 1.5931028003810124, 1.5930254000168316, 1.59260686780393, 1.5949937642722158, 1.5947386398452508, 1.5944638458855098, 1.592886685884464, 1.5920359578710317, 1.591343679809962, 1.5944231661438208, 1.5927860500876174, 1.5916114329312616, 1.5895260092658918, 1.5900373516631077, 1.5908487431077742, 1.5920482541011833, 1.5911493235789775, 1.5926099004686736, 1.589571738487886, 1.5899816897860297, 1.5885085502933918, 1.5869524172933684, 1.5862252390604978, 1.5855532120385454, 1.5846031973738934, 1.5841076553235063, 1.5891025064907034, 1.5890165003907755, 1.5885476208320632, 1.5884517902221522, 1.587481446138887, 1.5862894000458767, 1.5888053466651963, 1.590226834021065, 1.5874715414869711, 1.5880224622495365, 1.5870269889949038, 1.5866784251446107, 1.5877988641511733, 1.5877456003146004, 1.5835913359996474, 1.581913607908715, 1.5847343750313323, 1.5866767357507037, 1.5829206083344727, 1.5833659243534723, 1.5839861218199838, 1.582693792025901, 1.5790584504971514, 1.5786299907696075, 1.5780877555909831, 1.5759191948775149, 1.5765646018531534, 1.577474401227258, 1.5820896303874021, 1.583165800008441, 1.578199193413987, 1.5780504955158585, 1.5801568876791294, 1.5763541723913237, 1.5782888086914282, 1.576983277068246, 1.5775128917282857, 1.5765181146363214, 1.5781027634285802, 1.5752660808132415, 1.5736719467801479, 1.5743119649573762, 1.580788157265289, 1.5762239667669213, 1.572727057625381, 1.5742721309896857, 1.5737917606345926, 1.5695806543929867, 1.5693251277876585, 1.5721442756222015, 1.5714931771006182, 1.5698313152520809, 1.5741093846561973, 1.575308613757578, 1.5705693766321736, 1.5708853626153307, 1.5674716771750479, 1.5667006501916498, 1.5641305827017438, 1.5702391998479008, 1.5858328929427223, 1.6142959386661067, 1.6115939543232536, 1.5948322708846607, 1.593976768380073, 1.5904510610157458, 1.5917203931593062, 1.5908835281336822, 1.5885293825451108, 1.5883656788166054, 1.5850788484608613, 1.5852773889134306, 1.5817868568569238, 1.5795182317678933, 1.576267133894887, 1.5757665588625647, 1.5744915081490236, 1.5719390975865986, 1.570338974449424, 1.574654351612381, 1.5751512444239621, 1.572457937244517, 1.569259289011084, 1.5666806909827482, 1.5659007052866096, 1.5649571569058929, 1.5653013986238953, 1.5635706089360513, 1.5619148108503902, 1.5772941625093777, 1.5827721259432408, 1.5747530858864285, 1.5703348603336718, 1.570508439829707, 1.5720813413420252, 1.5668942013315597, 1.566330441752988, 1.561860664571335, 1.5607302149708022, 1.5597461276964975, 1.5604179399459024, 1.5622167068340451, 1.5605465778335164, 1.5575530619102338, 1.5569197330631515, 1.5597457045157588, 1.563649849627297, 1.5608821870854748, 1.5574869445217219, 1.557415190516556, 1.5530810412929779, 1.5521502125679345, 1.5480472231792473, 1.5534323270805563, 1.5562448933384012, 1.5489537376642717, 1.5496519196939174, 1.5509144709095573, 1.5578270210622518, 1.5634289291115513, 1.5626621315366678, 1.5622029505715966, 1.5597593084742647, 1.560874035471029, 1.5616834563151523, 1.5595968015874435, 1.5568391483667205, 1.5544984322553788, 1.5551447020908646, 1.5562793538310935, 1.551671433252965, 1.55066598142197, 1.5491440982544447, 1.5487363848598097, 1.5502953249326232, 1.5539668269715514, 1.5539960057828461, 1.5496988067881527, 1.549957974099036, 1.5458810467005266, 1.5401280990616253, 1.542696242655572, 1.5413047013096741, 1.5481324017659839, 1.5492770853962987, 1.5456164725262527, 1.5464920292155209, 1.5447933630776847, 1.5383322097927148, 1.5381172651138149, 1.5386358333074581, 1.5381039616997971, 1.5348738671817819, 1.5325957309049258, 1.5310419111036422, 1.529360032815953, 1.5254574282213402, 1.5280672032240725, 1.5315450204716081, 1.5313701597327325, 1.5308017144212978, 1.529754377831179, 1.537257016659762, 1.545105510227979, 1.531405483427968, 1.5367593632096872, 1.5348053876869, 1.5313213296984256, 1.5318456089716916, 1.5283392637417301, 1.5172941737596015, 1.5170502741968364, 1.5202134713010376, 1.5211134784275502, 1.5210801689042204, 1.5124682230136723, 1.5141507495355313, 1.5184135728792978, 1.527937717310457], 'acc': [0.19958932246760422, 0.18767967113356815, 0.23121149943227395, 0.23244353344308277, 0.23367556355572333, 0.23326488752262303, 0.2271047216666063, 0.21971252573466643, 0.23449692037683248, 0.23326488811010207, 0.23326488711261162, 0.23367556572817189, 0.23326488754098176, 0.2328542083195837, 0.23285420872959514, 0.23326488595601225, 0.23285421008202084, 0.23285421010037957, 0.23326488732679668, 0.2332648877184494, 0.23449692096431152, 0.23367556474904017, 0.23408624217128363, 0.23326488613347987, 0.23326488652513258, 0.23655030787970252, 0.23819301774614401, 0.23449692018100612, 0.2435318279560097, 0.24106776226594953, 0.24106776107263272, 0.23942505216696425, 0.23408624217128363, 0.23696098450028186, 0.24024640742146258, 0.23613963006580635, 0.23860369556004016, 0.23449691879186296, 0.23942505138365885, 0.24763860335340246, 0.24681724909639458, 0.2410677630308962, 0.23942505140201756, 0.2501026716075639, 0.2521560579354758, 0.24517453883830037, 0.24065708404204195, 0.24804928175477767, 0.2501026696676591, 0.2509240254912778, 0.24681724987969997, 0.2488706369909173, 0.24312115055212494, 0.24640657147832476, 0.2505133461107708, 0.24599589386025494, 0.24271047291569642, 0.24147843966983426, 0.24271047232821738, 0.245174539015768, 0.23860369694918332, 0.2439425059657322, 0.24558521667055525, 0.2443531818213649, 0.2316221768361587, 0.24558521604635877, 0.23162217765618154, 0.23408624234875125, 0.2455852148897594, 0.2308008204434197, 0.23449691895097188, 0.2451745401907261, 0.24394250598409092, 0.2447638610244042, 0.2455852166338378, 0.24394250578826457, 0.2455852148897594, 0.24476386081021914, 0.24106776068098001, 0.2484599585895421, 0.24969199144375154, 0.25010266984512675, 0.25092402527709273, 0.2521560583271285, 0.25215605773964944, 0.2517453787691539, 0.2550308010844969, 0.2554414800549924, 0.24969199105209883, 0.252566735749372, 0.25667351036345937, 0.2509240256687454, 0.25174538051323236, 0.2562628327453895, 0.2620123211608041, 0.25749486638290436, 0.25872689884546113, 0.26036960912191404, 0.2537987676244497, 0.24517453705750453, 0.2513347040701206, 0.25667351153841744, 0.2558521570855832, 0.2583162212273913, 0.25174537914244793, 0.24517453903412673, 0.24763860452836053, 0.25749486698874213, 0.25667351036345937, 0.25585215532314604, 0.24722792612698533, 0.25133470132855174, 0.25174537773494604, 0.2542094470049566, 0.24845995976450017, 0.2554414790942194, 0.25749486794951515, 0.2599589322871496, 0.25462012424973246, 0.26119096553301174, 0.261601643561093, 0.257905545567585, 0.25092402468961367, 0.25708418915648723, 0.2521560583454872, 0.25585215532314604, 0.2620123225315885, 0.25872689880874367, 0.24845995878536845, 0.25503080224109625, 0.24804928196896273, 0.25092402664787716, 0.25174538207984315, 0.2554414782925553, 0.2554414787025667, 0.25626283254956317, 0.2722792618267345, 0.2505133480506756, 0.25831622281236083, 0.2579055449801059, 0.25708418915648723, 0.257084189762325, 0.2587269004120719, 0.26324435323170814, 0.26694045277346823, 0.26365503006647256, 0.2587268996287665, 0.2677618070488348, 0.2521560559404949, 0.27556468314458704, 0.2554414779009026, 0.2706365481661575, 0.26119096396640096, 0.26817248406106686, 0.26611909832063396, 0.2648870646831191, 0.2603696089260877, 0.26899384145129634, 0.27022587152721944, 0.26078028908990003, 0.2677618050354952, 0.26735113019571166, 0.2677618048396688, 0.25626283313704223, 0.2677618089887396, 0.27885010189833825, 0.26981519449662866, 0.2747433255218138, 0.27022587489298483, 0.26735113019571166, 0.2661190955790651, 0.2632443544066662, 0.26447638608591756, 0.2673511276132517, 0.2698151927341915, 0.264476387456702, 0.27802874826552687, 0.2677618084012605, 0.2735112944667589, 0.2784394270585548, 0.2747433241510293, 0.2624229983872212, 0.23737166368496246, 0.24229979551181166, 0.2402464066381572, 0.2505133492256337, 0.2607802883065946, 0.25215605656469137, 0.25503080128032324, 0.2488706374009287, 0.2603696089260877, 0.26283367483033293, 0.2574948671662098, 0.26036961049269847, 0.2624229995621793, 0.2706365487536366, 0.2702258735405591, 0.25954825384905694, 0.2685831616791367, 0.2759753597835251, 0.2726899384473139, 0.26858316007580846, 0.2763860387723793, 0.27556468275293433, 0.273921971105697, 0.26817248564603635, 0.26940451711110264, 0.2669404525776419, 0.2747433279084474, 0.27186858458195867, 0.26694044940770284, 0.2529774137590945, 0.2657084201150851, 0.27022587293472133, 0.2579055457634113, 0.27186858281952153, 0.2747433261092928, 0.271868581020367, 0.28049281475725113, 0.2763860362266368, 0.2788501031100138, 0.2788501011150329, 0.2755646815412588, 0.276386035994093, 0.28542094300414994, 0.28295688026984367, 0.27597536013846036, 0.2702258705480877, 0.27351129448511763, 0.28254620343507925, 0.2669404496035292, 0.285010266793582, 0.2763860377932476, 0.28829568910892495, 0.28377823472267794, 0.2821355252295304, 0.28049281319064034, 0.2780287486571796, 0.27926077974895186, 0.268993841842949, 0.2739219703223916, 0.27433264970289856, 0.27145790614386606, 0.2841889109332459, 0.2800821345567214, 0.27310061667122143, 0.28049281354557565, 0.276386038576553, 0.27802874705385133, 0.27885010091920653, 0.2870636570380209, 0.28870636394870847, 0.28418891112907224, 0.29199178630076883, 0.28459958995881757, 0.28418891014994047, 0.2903490761834248, 0.28501026597355916, 0.2895277201639798, 0.2899383995812042, 0.2874743303111936, 0.28870636355705576, 0.2936344953655462, 0.2850102683601928, 0.28254620088933674, 0.2845995909379493, 0.28583162062221973, 0.27761806947249895, 0.2845995869847049, 0.28993839699874424, 0.2878850083209161, 0.2944558525599493, 0.2895277227464398, 0.29815194936014056, 0.29199178865068504, 0.30061601878926003, 0.297741271350418, 0.3080082114105107, 0.29609856068231244, 0.29199178528491965, 0.28624229925613875, 0.2878850098875269, 0.28911704411252076, 0.2874743346560907, 0.28172484780728696, 0.29075975282236294, 0.28131416721510444, 0.29445584981838047, 0.29774127393287797, 0.3002053403878848, 0.29404517298361604, 0.30513347203726643, 0.30718685676185015, 0.2969199168975838, 0.30020533921292675, 0.29897330776621917, 0.3088295674299557, 0.3096509240368798, 0.2997946602240725, 0.2977412737003342]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
