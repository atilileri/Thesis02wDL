{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf62.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 16:17:49 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'All', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '03', '04', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000026F82CABE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000026FFE8B6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6095, Accuracy:0.2329, Validation Loss:1.6084, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6063, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6057, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6041, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6027, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6031, Accuracy:0.2374, Validation Loss:1.6025, Validation Accuracy:0.2365\n",
    "Epoch #18: Loss:1.6028, Accuracy:0.2390, Validation Loss:1.6023, Validation Accuracy:0.2381\n",
    "Epoch #19: Loss:1.6025, Accuracy:0.2394, Validation Loss:1.6020, Validation Accuracy:0.2348\n",
    "Epoch #20: Loss:1.6026, Accuracy:0.2394, Validation Loss:1.6019, Validation Accuracy:0.2381\n",
    "Epoch #21: Loss:1.6024, Accuracy:0.2394, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #22: Loss:1.6021, Accuracy:0.2419, Validation Loss:1.6015, Validation Accuracy:0.2447\n",
    "Epoch #23: Loss:1.6022, Accuracy:0.2427, Validation Loss:1.6014, Validation Accuracy:0.2463\n",
    "Epoch #24: Loss:1.6017, Accuracy:0.2435, Validation Loss:1.6011, Validation Accuracy:0.2463\n",
    "Epoch #25: Loss:1.6017, Accuracy:0.2431, Validation Loss:1.6014, Validation Accuracy:0.2447\n",
    "Epoch #26: Loss:1.6017, Accuracy:0.2427, Validation Loss:1.6011, Validation Accuracy:0.2463\n",
    "Epoch #27: Loss:1.6013, Accuracy:0.2431, Validation Loss:1.6005, Validation Accuracy:0.2463\n",
    "Epoch #28: Loss:1.6012, Accuracy:0.2431, Validation Loss:1.6003, Validation Accuracy:0.2463\n",
    "Epoch #29: Loss:1.6012, Accuracy:0.2444, Validation Loss:1.6005, Validation Accuracy:0.2447\n",
    "Epoch #30: Loss:1.6014, Accuracy:0.2419, Validation Loss:1.6006, Validation Accuracy:0.2447\n",
    "Epoch #31: Loss:1.6008, Accuracy:0.2448, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #32: Loss:1.6009, Accuracy:0.2435, Validation Loss:1.6006, Validation Accuracy:0.2447\n",
    "Epoch #33: Loss:1.6004, Accuracy:0.2456, Validation Loss:1.6003, Validation Accuracy:0.2447\n",
    "Epoch #34: Loss:1.6005, Accuracy:0.2444, Validation Loss:1.6003, Validation Accuracy:0.2447\n",
    "Epoch #35: Loss:1.6002, Accuracy:0.2435, Validation Loss:1.6008, Validation Accuracy:0.2447\n",
    "Epoch #36: Loss:1.6009, Accuracy:0.2452, Validation Loss:1.6003, Validation Accuracy:0.2430\n",
    "Epoch #37: Loss:1.5998, Accuracy:0.2452, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #38: Loss:1.5996, Accuracy:0.2448, Validation Loss:1.6006, Validation Accuracy:0.2414\n",
    "Epoch #39: Loss:1.5996, Accuracy:0.2398, Validation Loss:1.6009, Validation Accuracy:0.2315\n",
    "Epoch #40: Loss:1.5994, Accuracy:0.2419, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #41: Loss:1.5992, Accuracy:0.2468, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #42: Loss:1.5986, Accuracy:0.2439, Validation Loss:1.6009, Validation Accuracy:0.2414\n",
    "Epoch #43: Loss:1.5986, Accuracy:0.2439, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #44: Loss:1.5987, Accuracy:0.2411, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #45: Loss:1.5987, Accuracy:0.2444, Validation Loss:1.6011, Validation Accuracy:0.2348\n",
    "Epoch #46: Loss:1.5982, Accuracy:0.2439, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #47: Loss:1.5985, Accuracy:0.2460, Validation Loss:1.6015, Validation Accuracy:0.2430\n",
    "Epoch #48: Loss:1.5980, Accuracy:0.2460, Validation Loss:1.6014, Validation Accuracy:0.2348\n",
    "Epoch #49: Loss:1.5981, Accuracy:0.2472, Validation Loss:1.6014, Validation Accuracy:0.2430\n",
    "Epoch #50: Loss:1.5982, Accuracy:0.2480, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #51: Loss:1.5985, Accuracy:0.2423, Validation Loss:1.6024, Validation Accuracy:0.2447\n",
    "Epoch #52: Loss:1.5978, Accuracy:0.2480, Validation Loss:1.6017, Validation Accuracy:0.2315\n",
    "Epoch #53: Loss:1.5972, Accuracy:0.2485, Validation Loss:1.6014, Validation Accuracy:0.2348\n",
    "Epoch #54: Loss:1.5982, Accuracy:0.2468, Validation Loss:1.6024, Validation Accuracy:0.2447\n",
    "Epoch #55: Loss:1.5975, Accuracy:0.2468, Validation Loss:1.6025, Validation Accuracy:0.2365\n",
    "Epoch #56: Loss:1.5973, Accuracy:0.2464, Validation Loss:1.6020, Validation Accuracy:0.2447\n",
    "Epoch #57: Loss:1.5973, Accuracy:0.2468, Validation Loss:1.6008, Validation Accuracy:0.2447\n",
    "Epoch #58: Loss:1.5971, Accuracy:0.2460, Validation Loss:1.6026, Validation Accuracy:0.2381\n",
    "Epoch #59: Loss:1.5994, Accuracy:0.2464, Validation Loss:1.6035, Validation Accuracy:0.2365\n",
    "Epoch #60: Loss:1.6085, Accuracy:0.2337, Validation Loss:1.6066, Validation Accuracy:0.2348\n",
    "Epoch #61: Loss:1.6028, Accuracy:0.2275, Validation Loss:1.6086, Validation Accuracy:0.1987\n",
    "Epoch #62: Loss:1.6041, Accuracy:0.2341, Validation Loss:1.6025, Validation Accuracy:0.2299\n",
    "Epoch #63: Loss:1.5997, Accuracy:0.2357, Validation Loss:1.6018, Validation Accuracy:0.2250\n",
    "Epoch #64: Loss:1.5995, Accuracy:0.2357, Validation Loss:1.6009, Validation Accuracy:0.2381\n",
    "Epoch #65: Loss:1.6003, Accuracy:0.2407, Validation Loss:1.6016, Validation Accuracy:0.2447\n",
    "Epoch #66: Loss:1.5995, Accuracy:0.2444, Validation Loss:1.6006, Validation Accuracy:0.2299\n",
    "Epoch #67: Loss:1.5987, Accuracy:0.2411, Validation Loss:1.6009, Validation Accuracy:0.2299\n",
    "Epoch #68: Loss:1.5988, Accuracy:0.2411, Validation Loss:1.6006, Validation Accuracy:0.2282\n",
    "Epoch #69: Loss:1.5981, Accuracy:0.2439, Validation Loss:1.6003, Validation Accuracy:0.2447\n",
    "Epoch #70: Loss:1.5981, Accuracy:0.2497, Validation Loss:1.6004, Validation Accuracy:0.2430\n",
    "Epoch #71: Loss:1.5974, Accuracy:0.2485, Validation Loss:1.6005, Validation Accuracy:0.2332\n",
    "Epoch #72: Loss:1.5980, Accuracy:0.2444, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #73: Loss:1.5973, Accuracy:0.2464, Validation Loss:1.6009, Validation Accuracy:0.2250\n",
    "Epoch #74: Loss:1.5970, Accuracy:0.2476, Validation Loss:1.6011, Validation Accuracy:0.2315\n",
    "Epoch #75: Loss:1.5965, Accuracy:0.2472, Validation Loss:1.6009, Validation Accuracy:0.2250\n",
    "Epoch #76: Loss:1.5968, Accuracy:0.2460, Validation Loss:1.6009, Validation Accuracy:0.2233\n",
    "Epoch #77: Loss:1.5965, Accuracy:0.2423, Validation Loss:1.6018, Validation Accuracy:0.2299\n",
    "Epoch #78: Loss:1.5979, Accuracy:0.2489, Validation Loss:1.6014, Validation Accuracy:0.2250\n",
    "Epoch #79: Loss:1.5971, Accuracy:0.2415, Validation Loss:1.6019, Validation Accuracy:0.2299\n",
    "Epoch #80: Loss:1.5964, Accuracy:0.2444, Validation Loss:1.6014, Validation Accuracy:0.2250\n",
    "Epoch #81: Loss:1.5958, Accuracy:0.2472, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #82: Loss:1.5961, Accuracy:0.2456, Validation Loss:1.6013, Validation Accuracy:0.2233\n",
    "Epoch #83: Loss:1.5956, Accuracy:0.2460, Validation Loss:1.6015, Validation Accuracy:0.2299\n",
    "Epoch #84: Loss:1.5954, Accuracy:0.2464, Validation Loss:1.6015, Validation Accuracy:0.2233\n",
    "Epoch #85: Loss:1.5959, Accuracy:0.2468, Validation Loss:1.6015, Validation Accuracy:0.2299\n",
    "Epoch #86: Loss:1.5963, Accuracy:0.2460, Validation Loss:1.6015, Validation Accuracy:0.2299\n",
    "Epoch #87: Loss:1.5967, Accuracy:0.2419, Validation Loss:1.6022, Validation Accuracy:0.2348\n",
    "Epoch #88: Loss:1.5964, Accuracy:0.2448, Validation Loss:1.6020, Validation Accuracy:0.2250\n",
    "Epoch #89: Loss:1.5955, Accuracy:0.2448, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:1.5955, Accuracy:0.2448, Validation Loss:1.6013, Validation Accuracy:0.2266\n",
    "Epoch #91: Loss:1.5957, Accuracy:0.2472, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #92: Loss:1.5957, Accuracy:0.2427, Validation Loss:1.6021, Validation Accuracy:0.2332\n",
    "Epoch #93: Loss:1.5952, Accuracy:0.2468, Validation Loss:1.6015, Validation Accuracy:0.2299\n",
    "Epoch #94: Loss:1.5956, Accuracy:0.2464, Validation Loss:1.6014, Validation Accuracy:0.2348\n",
    "Epoch #95: Loss:1.5971, Accuracy:0.2407, Validation Loss:1.6025, Validation Accuracy:0.2315\n",
    "Epoch #96: Loss:1.5969, Accuracy:0.2460, Validation Loss:1.6024, Validation Accuracy:0.2200\n",
    "Epoch #97: Loss:1.5947, Accuracy:0.2448, Validation Loss:1.6023, Validation Accuracy:0.2365\n",
    "Epoch #98: Loss:1.5963, Accuracy:0.2427, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #99: Loss:1.5950, Accuracy:0.2456, Validation Loss:1.6015, Validation Accuracy:0.2282\n",
    "Epoch #100: Loss:1.5959, Accuracy:0.2431, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #101: Loss:1.5949, Accuracy:0.2456, Validation Loss:1.6011, Validation Accuracy:0.2332\n",
    "Epoch #102: Loss:1.5948, Accuracy:0.2489, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #103: Loss:1.5949, Accuracy:0.2460, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #104: Loss:1.5950, Accuracy:0.2444, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #105: Loss:1.5944, Accuracy:0.2444, Validation Loss:1.6015, Validation Accuracy:0.2299\n",
    "Epoch #106: Loss:1.5949, Accuracy:0.2444, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #107: Loss:1.5944, Accuracy:0.2444, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #108: Loss:1.5953, Accuracy:0.2448, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #109: Loss:1.5948, Accuracy:0.2419, Validation Loss:1.6024, Validation Accuracy:0.2365\n",
    "Epoch #110: Loss:1.5948, Accuracy:0.2423, Validation Loss:1.6020, Validation Accuracy:0.2332\n",
    "Epoch #111: Loss:1.5949, Accuracy:0.2472, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #112: Loss:1.5946, Accuracy:0.2472, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #113: Loss:1.5955, Accuracy:0.2456, Validation Loss:1.6015, Validation Accuracy:0.2348\n",
    "Epoch #114: Loss:1.5948, Accuracy:0.2480, Validation Loss:1.6012, Validation Accuracy:0.2348\n",
    "Epoch #115: Loss:1.5956, Accuracy:0.2464, Validation Loss:1.6020, Validation Accuracy:0.2299\n",
    "Epoch #116: Loss:1.5969, Accuracy:0.2480, Validation Loss:1.6029, Validation Accuracy:0.2315\n",
    "Epoch #117: Loss:1.5960, Accuracy:0.2472, Validation Loss:1.6018, Validation Accuracy:0.2282\n",
    "Epoch #118: Loss:1.5948, Accuracy:0.2460, Validation Loss:1.6009, Validation Accuracy:0.2365\n",
    "Epoch #119: Loss:1.5947, Accuracy:0.2444, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #120: Loss:1.5944, Accuracy:0.2448, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #121: Loss:1.5949, Accuracy:0.2460, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #122: Loss:1.5946, Accuracy:0.2456, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #123: Loss:1.5941, Accuracy:0.2456, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #124: Loss:1.5938, Accuracy:0.2444, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #125: Loss:1.5938, Accuracy:0.2444, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #126: Loss:1.5940, Accuracy:0.2456, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #127: Loss:1.5942, Accuracy:0.2460, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #128: Loss:1.5939, Accuracy:0.2452, Validation Loss:1.6016, Validation Accuracy:0.2365\n",
    "Epoch #129: Loss:1.5937, Accuracy:0.2452, Validation Loss:1.6018, Validation Accuracy:0.2365\n",
    "Epoch #130: Loss:1.5946, Accuracy:0.2460, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #131: Loss:1.5943, Accuracy:0.2452, Validation Loss:1.6016, Validation Accuracy:0.2365\n",
    "Epoch #132: Loss:1.5941, Accuracy:0.2435, Validation Loss:1.6022, Validation Accuracy:0.2365\n",
    "Epoch #133: Loss:1.5958, Accuracy:0.2460, Validation Loss:1.6014, Validation Accuracy:0.2381\n",
    "Epoch #134: Loss:1.5951, Accuracy:0.2485, Validation Loss:1.6018, Validation Accuracy:0.2397\n",
    "Epoch #135: Loss:1.5947, Accuracy:0.2497, Validation Loss:1.6015, Validation Accuracy:0.2381\n",
    "Epoch #136: Loss:1.5942, Accuracy:0.2460, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #137: Loss:1.5938, Accuracy:0.2435, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #138: Loss:1.5940, Accuracy:0.2460, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #139: Loss:1.5933, Accuracy:0.2448, Validation Loss:1.6017, Validation Accuracy:0.2414\n",
    "Epoch #140: Loss:1.5933, Accuracy:0.2476, Validation Loss:1.6009, Validation Accuracy:0.2381\n",
    "Epoch #141: Loss:1.5937, Accuracy:0.2476, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #142: Loss:1.5946, Accuracy:0.2435, Validation Loss:1.6022, Validation Accuracy:0.2397\n",
    "Epoch #143: Loss:1.5943, Accuracy:0.2448, Validation Loss:1.6017, Validation Accuracy:0.2381\n",
    "Epoch #144: Loss:1.5934, Accuracy:0.2468, Validation Loss:1.6017, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5934, Accuracy:0.2452, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #146: Loss:1.5932, Accuracy:0.2480, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #147: Loss:1.5929, Accuracy:0.2485, Validation Loss:1.6014, Validation Accuracy:0.2430\n",
    "Epoch #148: Loss:1.5931, Accuracy:0.2493, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #149: Loss:1.5927, Accuracy:0.2493, Validation Loss:1.6017, Validation Accuracy:0.2414\n",
    "Epoch #150: Loss:1.5931, Accuracy:0.2464, Validation Loss:1.6014, Validation Accuracy:0.2381\n",
    "Epoch #151: Loss:1.5931, Accuracy:0.2476, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #152: Loss:1.5934, Accuracy:0.2497, Validation Loss:1.6016, Validation Accuracy:0.2414\n",
    "Epoch #153: Loss:1.5930, Accuracy:0.2505, Validation Loss:1.6017, Validation Accuracy:0.2397\n",
    "Epoch #154: Loss:1.5938, Accuracy:0.2505, Validation Loss:1.6021, Validation Accuracy:0.2430\n",
    "Epoch #155: Loss:1.5929, Accuracy:0.2501, Validation Loss:1.6011, Validation Accuracy:0.2397\n",
    "Epoch #156: Loss:1.5929, Accuracy:0.2509, Validation Loss:1.6013, Validation Accuracy:0.2397\n",
    "Epoch #157: Loss:1.5927, Accuracy:0.2513, Validation Loss:1.6010, Validation Accuracy:0.2397\n",
    "Epoch #158: Loss:1.5935, Accuracy:0.2489, Validation Loss:1.6013, Validation Accuracy:0.2397\n",
    "Epoch #159: Loss:1.5941, Accuracy:0.2452, Validation Loss:1.6024, Validation Accuracy:0.2299\n",
    "Epoch #160: Loss:1.5932, Accuracy:0.2460, Validation Loss:1.6016, Validation Accuracy:0.2332\n",
    "Epoch #161: Loss:1.5928, Accuracy:0.2472, Validation Loss:1.6008, Validation Accuracy:0.2381\n",
    "Epoch #162: Loss:1.5945, Accuracy:0.2431, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #163: Loss:1.5920, Accuracy:0.2501, Validation Loss:1.6010, Validation Accuracy:0.2397\n",
    "Epoch #164: Loss:1.5932, Accuracy:0.2505, Validation Loss:1.6003, Validation Accuracy:0.2397\n",
    "Epoch #165: Loss:1.5938, Accuracy:0.2472, Validation Loss:1.6012, Validation Accuracy:0.2430\n",
    "Epoch #166: Loss:1.5919, Accuracy:0.2476, Validation Loss:1.6005, Validation Accuracy:0.2397\n",
    "Epoch #167: Loss:1.5929, Accuracy:0.2513, Validation Loss:1.6005, Validation Accuracy:0.2397\n",
    "Epoch #168: Loss:1.5921, Accuracy:0.2476, Validation Loss:1.6006, Validation Accuracy:0.2414\n",
    "Epoch #169: Loss:1.5929, Accuracy:0.2513, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #170: Loss:1.5918, Accuracy:0.2509, Validation Loss:1.6021, Validation Accuracy:0.2463\n",
    "Epoch #171: Loss:1.5922, Accuracy:0.2501, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #172: Loss:1.5921, Accuracy:0.2534, Validation Loss:1.6011, Validation Accuracy:0.2414\n",
    "Epoch #173: Loss:1.5918, Accuracy:0.2542, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #174: Loss:1.5918, Accuracy:0.2493, Validation Loss:1.6007, Validation Accuracy:0.2381\n",
    "Epoch #175: Loss:1.5934, Accuracy:0.2509, Validation Loss:1.6011, Validation Accuracy:0.2381\n",
    "Epoch #176: Loss:1.5924, Accuracy:0.2476, Validation Loss:1.6025, Validation Accuracy:0.2447\n",
    "Epoch #177: Loss:1.5936, Accuracy:0.2505, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #178: Loss:1.5915, Accuracy:0.2501, Validation Loss:1.6004, Validation Accuracy:0.2348\n",
    "Epoch #179: Loss:1.5916, Accuracy:0.2464, Validation Loss:1.6003, Validation Accuracy:0.2348\n",
    "Epoch #180: Loss:1.5916, Accuracy:0.2550, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #181: Loss:1.5910, Accuracy:0.2505, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #182: Loss:1.5913, Accuracy:0.2485, Validation Loss:1.6004, Validation Accuracy:0.2414\n",
    "Epoch #183: Loss:1.5907, Accuracy:0.2538, Validation Loss:1.6006, Validation Accuracy:0.2381\n",
    "Epoch #184: Loss:1.5907, Accuracy:0.2493, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #185: Loss:1.5910, Accuracy:0.2530, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #186: Loss:1.5904, Accuracy:0.2587, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #187: Loss:1.5914, Accuracy:0.2587, Validation Loss:1.6013, Validation Accuracy:0.2381\n",
    "Epoch #188: Loss:1.5908, Accuracy:0.2575, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #189: Loss:1.5919, Accuracy:0.2538, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #190: Loss:1.5897, Accuracy:0.2534, Validation Loss:1.6011, Validation Accuracy:0.2348\n",
    "Epoch #191: Loss:1.5926, Accuracy:0.2452, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #192: Loss:1.5911, Accuracy:0.2595, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #193: Loss:1.5907, Accuracy:0.2526, Validation Loss:1.6010, Validation Accuracy:0.2315\n",
    "Epoch #194: Loss:1.5906, Accuracy:0.2567, Validation Loss:1.6004, Validation Accuracy:0.2365\n",
    "Epoch #195: Loss:1.5911, Accuracy:0.2509, Validation Loss:1.6013, Validation Accuracy:0.2299\n",
    "Epoch #196: Loss:1.5896, Accuracy:0.2538, Validation Loss:1.6003, Validation Accuracy:0.2348\n",
    "Epoch #197: Loss:1.5908, Accuracy:0.2567, Validation Loss:1.6002, Validation Accuracy:0.2397\n",
    "Epoch #198: Loss:1.5914, Accuracy:0.2505, Validation Loss:1.6012, Validation Accuracy:0.2348\n",
    "Epoch #199: Loss:1.5899, Accuracy:0.2538, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #200: Loss:1.5903, Accuracy:0.2571, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #201: Loss:1.5896, Accuracy:0.2509, Validation Loss:1.6009, Validation Accuracy:0.2381\n",
    "Epoch #202: Loss:1.5897, Accuracy:0.2526, Validation Loss:1.6005, Validation Accuracy:0.2365\n",
    "Epoch #203: Loss:1.5895, Accuracy:0.2579, Validation Loss:1.6010, Validation Accuracy:0.2299\n",
    "Epoch #204: Loss:1.5898, Accuracy:0.2538, Validation Loss:1.6005, Validation Accuracy:0.2365\n",
    "Epoch #205: Loss:1.5892, Accuracy:0.2534, Validation Loss:1.6010, Validation Accuracy:0.2348\n",
    "Epoch #206: Loss:1.5893, Accuracy:0.2575, Validation Loss:1.6011, Validation Accuracy:0.2299\n",
    "Epoch #207: Loss:1.5893, Accuracy:0.2579, Validation Loss:1.6005, Validation Accuracy:0.2529\n",
    "Epoch #208: Loss:1.5890, Accuracy:0.2608, Validation Loss:1.6009, Validation Accuracy:0.2496\n",
    "Epoch #209: Loss:1.5894, Accuracy:0.2497, Validation Loss:1.6010, Validation Accuracy:0.2512\n",
    "Epoch #210: Loss:1.5894, Accuracy:0.2583, Validation Loss:1.6009, Validation Accuracy:0.2479\n",
    "Epoch #211: Loss:1.5890, Accuracy:0.2608, Validation Loss:1.6007, Validation Accuracy:0.2496\n",
    "Epoch #212: Loss:1.5896, Accuracy:0.2595, Validation Loss:1.6008, Validation Accuracy:0.2529\n",
    "Epoch #213: Loss:1.5888, Accuracy:0.2591, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #214: Loss:1.5887, Accuracy:0.2600, Validation Loss:1.6004, Validation Accuracy:0.2529\n",
    "Epoch #215: Loss:1.5900, Accuracy:0.2534, Validation Loss:1.6014, Validation Accuracy:0.2512\n",
    "Epoch #216: Loss:1.5898, Accuracy:0.2546, Validation Loss:1.6010, Validation Accuracy:0.2381\n",
    "Epoch #217: Loss:1.5898, Accuracy:0.2579, Validation Loss:1.6004, Validation Accuracy:0.2512\n",
    "Epoch #218: Loss:1.5881, Accuracy:0.2501, Validation Loss:1.6008, Validation Accuracy:0.2529\n",
    "Epoch #219: Loss:1.5894, Accuracy:0.2530, Validation Loss:1.6002, Validation Accuracy:0.2529\n",
    "Epoch #220: Loss:1.5892, Accuracy:0.2583, Validation Loss:1.6009, Validation Accuracy:0.2447\n",
    "Epoch #221: Loss:1.5888, Accuracy:0.2579, Validation Loss:1.6019, Validation Accuracy:0.2479\n",
    "Epoch #222: Loss:1.5890, Accuracy:0.2517, Validation Loss:1.6013, Validation Accuracy:0.2479\n",
    "Epoch #223: Loss:1.5882, Accuracy:0.2583, Validation Loss:1.6010, Validation Accuracy:0.2447\n",
    "Epoch #224: Loss:1.5882, Accuracy:0.2563, Validation Loss:1.6008, Validation Accuracy:0.2529\n",
    "Epoch #225: Loss:1.5880, Accuracy:0.2604, Validation Loss:1.6009, Validation Accuracy:0.2529\n",
    "Epoch #226: Loss:1.5876, Accuracy:0.2600, Validation Loss:1.6011, Validation Accuracy:0.2529\n",
    "Epoch #227: Loss:1.5875, Accuracy:0.2624, Validation Loss:1.6012, Validation Accuracy:0.2414\n",
    "Epoch #228: Loss:1.5881, Accuracy:0.2575, Validation Loss:1.6016, Validation Accuracy:0.2496\n",
    "Epoch #229: Loss:1.5884, Accuracy:0.2616, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #230: Loss:1.5882, Accuracy:0.2575, Validation Loss:1.6015, Validation Accuracy:0.2365\n",
    "Epoch #231: Loss:1.5879, Accuracy:0.2600, Validation Loss:1.6013, Validation Accuracy:0.2529\n",
    "Epoch #232: Loss:1.5883, Accuracy:0.2546, Validation Loss:1.6022, Validation Accuracy:0.2545\n",
    "Epoch #233: Loss:1.5871, Accuracy:0.2587, Validation Loss:1.6019, Validation Accuracy:0.2365\n",
    "Epoch #234: Loss:1.5894, Accuracy:0.2600, Validation Loss:1.6013, Validation Accuracy:0.2397\n",
    "Epoch #235: Loss:1.5875, Accuracy:0.2595, Validation Loss:1.6017, Validation Accuracy:0.2479\n",
    "Epoch #236: Loss:1.5898, Accuracy:0.2517, Validation Loss:1.6015, Validation Accuracy:0.2529\n",
    "Epoch #237: Loss:1.5912, Accuracy:0.2616, Validation Loss:1.6019, Validation Accuracy:0.2447\n",
    "Epoch #238: Loss:1.5884, Accuracy:0.2600, Validation Loss:1.6036, Validation Accuracy:0.2496\n",
    "Epoch #239: Loss:1.5891, Accuracy:0.2591, Validation Loss:1.6003, Validation Accuracy:0.2529\n",
    "Epoch #240: Loss:1.5881, Accuracy:0.2583, Validation Loss:1.6002, Validation Accuracy:0.2529\n",
    "Epoch #241: Loss:1.5874, Accuracy:0.2591, Validation Loss:1.6013, Validation Accuracy:0.2479\n",
    "Epoch #242: Loss:1.5877, Accuracy:0.2575, Validation Loss:1.6001, Validation Accuracy:0.2512\n",
    "Epoch #243: Loss:1.5881, Accuracy:0.2583, Validation Loss:1.6006, Validation Accuracy:0.2529\n",
    "Epoch #244: Loss:1.5881, Accuracy:0.2608, Validation Loss:1.6022, Validation Accuracy:0.2496\n",
    "Epoch #245: Loss:1.5875, Accuracy:0.2587, Validation Loss:1.6016, Validation Accuracy:0.2447\n",
    "Epoch #246: Loss:1.5877, Accuracy:0.2641, Validation Loss:1.6013, Validation Accuracy:0.2529\n",
    "Epoch #247: Loss:1.5870, Accuracy:0.2612, Validation Loss:1.6009, Validation Accuracy:0.2529\n",
    "Epoch #248: Loss:1.5873, Accuracy:0.2604, Validation Loss:1.6009, Validation Accuracy:0.2529\n",
    "Epoch #249: Loss:1.5877, Accuracy:0.2600, Validation Loss:1.6017, Validation Accuracy:0.2529\n",
    "Epoch #250: Loss:1.5889, Accuracy:0.2600, Validation Loss:1.6014, Validation Accuracy:0.2463\n",
    "Epoch #251: Loss:1.5887, Accuracy:0.2600, Validation Loss:1.6025, Validation Accuracy:0.2529\n",
    "Epoch #252: Loss:1.5872, Accuracy:0.2616, Validation Loss:1.6010, Validation Accuracy:0.2545\n",
    "Epoch #253: Loss:1.5869, Accuracy:0.2604, Validation Loss:1.6005, Validation Accuracy:0.2512\n",
    "Epoch #254: Loss:1.5872, Accuracy:0.2608, Validation Loss:1.6016, Validation Accuracy:0.2529\n",
    "Epoch #255: Loss:1.5871, Accuracy:0.2608, Validation Loss:1.6014, Validation Accuracy:0.2545\n",
    "Epoch #256: Loss:1.5868, Accuracy:0.2591, Validation Loss:1.6011, Validation Accuracy:0.2529\n",
    "Epoch #257: Loss:1.5871, Accuracy:0.2608, Validation Loss:1.6017, Validation Accuracy:0.2529\n",
    "Epoch #258: Loss:1.5870, Accuracy:0.2604, Validation Loss:1.6011, Validation Accuracy:0.2529\n",
    "Epoch #259: Loss:1.5874, Accuracy:0.2608, Validation Loss:1.6011, Validation Accuracy:0.2430\n",
    "Epoch #260: Loss:1.5876, Accuracy:0.2608, Validation Loss:1.6021, Validation Accuracy:0.2529\n",
    "Epoch #261: Loss:1.5872, Accuracy:0.2591, Validation Loss:1.6018, Validation Accuracy:0.2512\n",
    "Epoch #262: Loss:1.5871, Accuracy:0.2595, Validation Loss:1.6011, Validation Accuracy:0.2512\n",
    "Epoch #263: Loss:1.5873, Accuracy:0.2604, Validation Loss:1.6017, Validation Accuracy:0.2529\n",
    "Epoch #264: Loss:1.5864, Accuracy:0.2501, Validation Loss:1.6020, Validation Accuracy:0.2496\n",
    "Epoch #265: Loss:1.5865, Accuracy:0.2546, Validation Loss:1.6008, Validation Accuracy:0.2512\n",
    "Epoch #266: Loss:1.5870, Accuracy:0.2600, Validation Loss:1.6010, Validation Accuracy:0.2545\n",
    "Epoch #267: Loss:1.5860, Accuracy:0.2616, Validation Loss:1.6003, Validation Accuracy:0.2578\n",
    "Epoch #268: Loss:1.5870, Accuracy:0.2604, Validation Loss:1.5996, Validation Accuracy:0.2562\n",
    "Epoch #269: Loss:1.5873, Accuracy:0.2595, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #270: Loss:1.5870, Accuracy:0.2600, Validation Loss:1.6006, Validation Accuracy:0.2463\n",
    "Epoch #271: Loss:1.5878, Accuracy:0.2480, Validation Loss:1.6018, Validation Accuracy:0.2562\n",
    "Epoch #272: Loss:1.5870, Accuracy:0.2608, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #273: Loss:1.5879, Accuracy:0.2600, Validation Loss:1.6013, Validation Accuracy:0.2496\n",
    "Epoch #274: Loss:1.5885, Accuracy:0.2579, Validation Loss:1.6004, Validation Accuracy:0.2562\n",
    "Epoch #275: Loss:1.5909, Accuracy:0.2600, Validation Loss:1.5979, Validation Accuracy:0.2529\n",
    "Epoch #276: Loss:1.5926, Accuracy:0.2579, Validation Loss:1.6000, Validation Accuracy:0.2562\n",
    "Epoch #277: Loss:1.5934, Accuracy:0.2485, Validation Loss:1.6026, Validation Accuracy:0.2660\n",
    "Epoch #278: Loss:1.5914, Accuracy:0.2624, Validation Loss:1.6201, Validation Accuracy:0.2085\n",
    "Epoch #279: Loss:1.6027, Accuracy:0.2197, Validation Loss:1.6172, Validation Accuracy:0.2463\n",
    "Epoch #280: Loss:1.6096, Accuracy:0.2444, Validation Loss:1.6059, Validation Accuracy:0.2397\n",
    "Epoch #281: Loss:1.6022, Accuracy:0.2398, Validation Loss:1.6102, Validation Accuracy:0.2135\n",
    "Epoch #282: Loss:1.6026, Accuracy:0.2283, Validation Loss:1.6049, Validation Accuracy:0.2184\n",
    "Epoch #283: Loss:1.6008, Accuracy:0.2287, Validation Loss:1.6006, Validation Accuracy:0.2315\n",
    "Epoch #284: Loss:1.5982, Accuracy:0.2435, Validation Loss:1.5984, Validation Accuracy:0.2348\n",
    "Epoch #285: Loss:1.5949, Accuracy:0.2444, Validation Loss:1.5995, Validation Accuracy:0.2315\n",
    "Epoch #286: Loss:1.5947, Accuracy:0.2448, Validation Loss:1.5998, Validation Accuracy:0.2332\n",
    "Epoch #287: Loss:1.5938, Accuracy:0.2464, Validation Loss:1.5992, Validation Accuracy:0.2512\n",
    "Epoch #288: Loss:1.5929, Accuracy:0.2542, Validation Loss:1.5988, Validation Accuracy:0.2430\n",
    "Epoch #289: Loss:1.5935, Accuracy:0.2542, Validation Loss:1.6002, Validation Accuracy:0.2414\n",
    "Epoch #290: Loss:1.5943, Accuracy:0.2550, Validation Loss:1.6020, Validation Accuracy:0.2463\n",
    "Epoch #291: Loss:1.5938, Accuracy:0.2546, Validation Loss:1.6026, Validation Accuracy:0.2611\n",
    "Epoch #292: Loss:1.5934, Accuracy:0.2571, Validation Loss:1.6028, Validation Accuracy:0.2611\n",
    "Epoch #293: Loss:1.5933, Accuracy:0.2583, Validation Loss:1.6030, Validation Accuracy:0.2578\n",
    "Epoch #294: Loss:1.5930, Accuracy:0.2571, Validation Loss:1.6023, Validation Accuracy:0.2611\n",
    "Epoch #295: Loss:1.5928, Accuracy:0.2575, Validation Loss:1.6006, Validation Accuracy:0.2644\n",
    "Epoch #296: Loss:1.5926, Accuracy:0.2567, Validation Loss:1.6004, Validation Accuracy:0.2529\n",
    "Epoch #297: Loss:1.5924, Accuracy:0.2600, Validation Loss:1.6007, Validation Accuracy:0.2627\n",
    "Epoch #298: Loss:1.5921, Accuracy:0.2575, Validation Loss:1.6006, Validation Accuracy:0.2644\n",
    "Epoch #299: Loss:1.5919, Accuracy:0.2571, Validation Loss:1.6007, Validation Accuracy:0.2611\n",
    "Epoch #300: Loss:1.5923, Accuracy:0.2575, Validation Loss:1.6004, Validation Accuracy:0.2611\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60037541, Accuracy:0.2611\n",
    "Labels: ['05', '03', '04', '01', '02']\n",
    "Confusion Matrix:\n",
    "       05  03  04  01  02\n",
    "t:05  125   0  12   5   0\n",
    "t:03   92   4   7  12   0\n",
    "t:04   83   3  20   6   0\n",
    "t:01  102   2  12  10   0\n",
    "t:02   78   4  20  12   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.26      0.88      0.40       142\n",
    "          03       0.31      0.03      0.06       115\n",
    "          04       0.28      0.18      0.22       112\n",
    "          01       0.22      0.08      0.12       126\n",
    "          02       0.00      0.00      0.00       114\n",
    "\n",
    "    accuracy                           0.26       609\n",
    "   macro avg       0.21      0.23      0.16       609\n",
    "weighted avg       0.22      0.26      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 16:58:21 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 31 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6084125202473356, 1.6060326725782823, 1.6054660700420635, 1.6052047237386844, 1.6046278650928991, 1.604181892178916, 1.6040085898635814, 1.6037575411679121, 1.6040804992951392, 1.603748912098764, 1.6035570213555899, 1.6034083464267024, 1.6031461646795664, 1.6029233061425596, 1.6026532221310243, 1.6025199477112744, 1.6025409209121428, 1.602258065846949, 1.6019810761136961, 1.601922430624124, 1.6016667315916866, 1.6015368750921415, 1.601401876346231, 1.6011318739607612, 1.601357765581416, 1.6011248231912873, 1.600506086654851, 1.6003256566418802, 1.60047529561962, 1.600633437214618, 1.600633760195452, 1.600649011350422, 1.60025681083034, 1.6003165951699068, 1.6007822789190633, 1.6002675165683764, 1.6006850607093723, 1.6005964656964506, 1.6009064985222026, 1.6007690682199789, 1.600755117796912, 1.6008796566617116, 1.6010292607966707, 1.6010640374153902, 1.6010822864197354, 1.6012366076408349, 1.6014962484096658, 1.6013955535559818, 1.6013985829204564, 1.6016672540376535, 1.6023983695041175, 1.6017259672553277, 1.601403720077427, 1.6024134010321205, 1.602519569138588, 1.6019500386343017, 1.6008298753321857, 1.6026159822451462, 1.6035186109088717, 1.6065722694146418, 1.6085985143392152, 1.6024592165485, 1.6017658542138211, 1.6009210076042388, 1.601608503041009, 1.6006442136169459, 1.6009101072947185, 1.6006307445331942, 1.600310958273501, 1.6004065641237206, 1.6005359690373362, 1.600506477363787, 1.6009285489130882, 1.6010809919517028, 1.600913893021582, 1.6009021938727994, 1.6017650798428038, 1.601352065850557, 1.6018660048937368, 1.6013836537675905, 1.601667178675459, 1.6013131938348655, 1.6015423647875857, 1.6014571649883376, 1.60148886585079, 1.6015226909483986, 1.6021505331953954, 1.601959838851528, 1.6017032662048716, 1.6012636127534563, 1.6014004378091722, 1.6021316763998448, 1.6014758499189354, 1.6014039473384862, 1.6024527729829936, 1.6023985734714077, 1.602282425061431, 1.6017245604291142, 1.601494780119221, 1.6013803957718347, 1.6011425894860956, 1.60119843443822, 1.6014093642164333, 1.6015491485595703, 1.601474174529265, 1.6014252186604516, 1.601415017946992, 1.601516516337841, 1.6023502608238183, 1.6020274966808374, 1.6014977556535568, 1.601546740884264, 1.6014791985450707, 1.6011759598658395, 1.6019541767038932, 1.6029112523020979, 1.6018004280397262, 1.6009003636480748, 1.6011657053222406, 1.6007338092832142, 1.601114253692439, 1.6006995705743925, 1.601106672059922, 1.6010642049739319, 1.6011863255931436, 1.6012413732719735, 1.6016591601379595, 1.6015568805249845, 1.6018016060389126, 1.6013734839819922, 1.601567678263622, 1.6022014950688053, 1.6013647595845615, 1.6017740373736729, 1.6014588364630888, 1.6014899949134864, 1.601025041883998, 1.600753230610113, 1.6016809893359105, 1.6009418040465055, 1.6010626235423222, 1.6021727078849655, 1.6017163727671055, 1.6016704719054875, 1.6012304205025358, 1.601074388853239, 1.6013819206328619, 1.6011740335298486, 1.6016658532795647, 1.6013622332872037, 1.6014061273612412, 1.6016390437171573, 1.6017294403758935, 1.6021309086841902, 1.6011212929129013, 1.6013179122912278, 1.6009932844509631, 1.601281883877095, 1.602351729701501, 1.6016441560143908, 1.6007581776977564, 1.602551256690315, 1.6010476266613538, 1.6002802341833882, 1.6012229805900937, 1.600523806166375, 1.6004659182136673, 1.6006098930863129, 1.6007556208640288, 1.6020970080286412, 1.6007794752496805, 1.601119939329589, 1.6013696555825094, 1.60073875480489, 1.6010827107969763, 1.6024740889350377, 1.6009077323089875, 1.6003607899097387, 1.600264442378077, 1.6005927715786963, 1.6007811226476785, 1.6004459278532632, 1.6006217826959144, 1.6008143886948258, 1.6007727605760196, 1.6009115004187147, 1.6013263814359266, 1.6012125181642856, 1.6015016419938437, 1.601072561173212, 1.60057584483831, 1.6017843702156556, 1.601008419332833, 1.6003546767634125, 1.6012728059624608, 1.6002996010929103, 1.6002015200350281, 1.6011777530945777, 1.6005246451335589, 1.6006072875118411, 1.6009234328966815, 1.60048896730044, 1.6010451336408091, 1.6005219448180426, 1.6010201153496804, 1.601050356926002, 1.6005421454096076, 1.6009320793872202, 1.6009925868123622, 1.6008533867709156, 1.6007325788241107, 1.6007842050593084, 1.601653175596729, 1.6004493823779629, 1.6013726724192427, 1.600975425959808, 1.6004236815206718, 1.6008136252855825, 1.600164303247173, 1.6009004562359137, 1.6019140816674444, 1.601319612149143, 1.6009768071433006, 1.600779039127682, 1.6009339642250675, 1.6010763461170916, 1.6011719259330988, 1.6015601486995303, 1.6012684259508632, 1.601467236509464, 1.6012665708664016, 1.6022002033412164, 1.6018997448418528, 1.6012696178480126, 1.6016598436828513, 1.6014961217620298, 1.6018963921050524, 1.603630020700652, 1.600274540324908, 1.6001746278678255, 1.6013006146122473, 1.600114892660495, 1.6006235453882829, 1.602186088883035, 1.6015882623215223, 1.6013004922710226, 1.600865373666259, 1.6008989646516998, 1.6017072666650531, 1.601375138622591, 1.602538638709997, 1.6009716359265331, 1.6005454460779827, 1.6015665928522747, 1.6014162049504923, 1.6011399065919698, 1.6017195790859278, 1.6011081779335912, 1.6011214217137428, 1.6020512989980638, 1.6017907264784639, 1.6010759317228946, 1.6016693275746061, 1.6020175251858966, 1.60082439248785, 1.6009818213718083, 1.6003324480479575, 1.5995564321774762, 1.6009915775461934, 1.6005953847872605, 1.601845846191807, 1.6007663197509565, 1.6012641866806105, 1.6004145409673305, 1.5978663705644154, 1.6000050079254877, 1.6026184091035565, 1.6200732703279392, 1.6171555378166913, 1.6059295252234673, 1.6102015568900774, 1.6048786002035407, 1.6006140258707633, 1.598357500309623, 1.5995024100117299, 1.599784469369597, 1.5991881733457443, 1.5988003540117361, 1.6002318579183619, 1.6019643200637868, 1.6025522336584006, 1.6027623062650558, 1.6029721457382728, 1.6022592957188146, 1.6005957394789396, 1.6004184754611237, 1.6007038899047425, 1.6006099349759482, 1.6007138533740992, 1.6003755194017257], 'val_acc': [0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.2364532019459751, 0.23809523807076985, 0.23481116582118036, 0.23809523807076985, 0.2364532019459751, 0.24466338247207586, 0.24630541869474357, 0.2463054185968706, 0.24466338247207586, 0.24630541869474357, 0.2463054185968706, 0.2463054185968706, 0.24466338256994882, 0.24466338256994882, 0.24302134634728112, 0.24466338247207586, 0.24466338256994882, 0.24466338256994882, 0.24466338247207586, 0.24302134624940813, 0.24466338247207586, 0.2413793100267404, 0.23152709357159088, 0.24302134615153514, 0.2413793100267404, 0.2413793100267404, 0.2413793100267404, 0.23809523797289686, 0.23481116552756143, 0.23316912940276668, 0.24302134615153514, 0.2348111656254344, 0.24302134615153514, 0.23316912950063964, 0.24466338237420288, 0.2315270933758449, 0.2348111656254344, 0.24466338237420288, 0.23645320184810212, 0.24466338237420288, 0.24466338237420288, 0.23809523797289686, 0.23645320175022916, 0.23481116572330737, 0.19868637078207702, 0.22988505725105016, 0.22495894877879294, 0.23809523807076985, 0.24466338029440204, 0.2298850570553042, 0.2298850570553042, 0.22824302102838243, 0.24466338029440204, 0.24302134634728112, 0.2331691293048937, 0.23481116542968844, 0.224958948583047, 0.23152709298435298, 0.224958948583047, 0.22331691236037926, 0.22988505685955823, 0.224958948583047, 0.22988505685955823, 0.224958948583047, 0.2331691292070207, 0.22331691236037926, 0.22988505685955823, 0.22331691236037926, 0.22988505685955823, 0.22988505685955823, 0.23481116533181545, 0.224958948583047, 0.2331691292070207, 0.22660098470784173, 0.2331691292070207, 0.2331691292070207, 0.2298850570553042, 0.23481116533181545, 0.23152709308222597, 0.22003284011078977, 0.2364532014566102, 0.2364532014566102, 0.22824302093050947, 0.2364532014566102, 0.2331691292070207, 0.2331691292070207, 0.2364532014566102, 0.2364532014566102, 0.22988505695743122, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.23645320155448318, 0.2331691292070207, 0.2364532014566102, 0.2364532014566102, 0.23481116533181545, 0.23481116533181545, 0.22988505695743122, 0.23152709318009895, 0.22824302083263648, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.23809523758140494, 0.23973727370619968, 0.23809523758140494, 0.2364532014566102, 0.2364532014566102, 0.2364532014566102, 0.2413793099288674, 0.23809523758140494, 0.23809523758140494, 0.23973727380407267, 0.23809523758140494, 0.23973727380407267, 0.23809523758140494, 0.23809523758140494, 0.24302134605366216, 0.23809523758140494, 0.2413793099288674, 0.23809523758140494, 0.23973727370619968, 0.2413793099288674, 0.23973727370619968, 0.24302134605366216, 0.23973727370619968, 0.23973727370619968, 0.23973727370619968, 0.23973727370619968, 0.22988505695743122, 0.23316912910914772, 0.23809523758140494, 0.24466338237420288, 0.23973727370619968, 0.23973727370619968, 0.24302134605366216, 0.23973727370619968, 0.23973727370619968, 0.2413793099288674, 0.23973727370619968, 0.2463054185968706, 0.23973727370619968, 0.24137930983099445, 0.23973727380407267, 0.23809523758140494, 0.23809523758140494, 0.2446633821784569, 0.23973727370619968, 0.2348111652339425, 0.2348111652339425, 0.24302134605366216, 0.2364532014566102, 0.24137930983099445, 0.23809523758140494, 0.2348111652339425, 0.24137930983099445, 0.23973727370619968, 0.23809523758140494, 0.23645320135873724, 0.23645320155448318, 0.2348111652339425, 0.2348111652339425, 0.2331691275187118, 0.23152709288648002, 0.23645320135873724, 0.2298850571531772, 0.2348111652339425, 0.23973727370619968, 0.2348111656254344, 0.2348111652339425, 0.24302134605366216, 0.23809523777715091, 0.23645320135873724, 0.22988505695743122, 0.23645320135873724, 0.23481116533181545, 0.22988505695743122, 0.2528735629981766, 0.24958949065071412, 0.25123152687338185, 0.24794745452591938, 0.24958949065071412, 0.2528735629981766, 0.2364532014566102, 0.2528735629981766, 0.25123152697125484, 0.23809523777715091, 0.25123152687338185, 0.25287356319392257, 0.2528735629981766, 0.24466338247207586, 0.24794745452591938, 0.24794745452591938, 0.24466338247207586, 0.2528735629981766, 0.2528735629981766, 0.2528735629981766, 0.2413793099288674, 0.24958949065071412, 0.2413793099288674, 0.23645320165235617, 0.2528735629981766, 0.2545155993187173, 0.2364532014566102, 0.23973727380407267, 0.24794745452591938, 0.2528735629981766, 0.24466338247207586, 0.24958949065071412, 0.2528735629981766, 0.2528735629981766, 0.24794745452591938, 0.25123152687338185, 0.2528735629981766, 0.24958949065071412, 0.24466338247207586, 0.2528735629981766, 0.2528735629981766, 0.2528735629981766, 0.2528735629981766, 0.2463054185968706, 0.2528735629981766, 0.2545155992208443, 0.25123152687338185, 0.2528735629981766, 0.2545155992208443, 0.2528735629981766, 0.2528735629981766, 0.2528735629981766, 0.24302134624940813, 0.2528735629981766, 0.25123152687338185, 0.25123152687338185, 0.2528735629981766, 0.2495894907485871, 0.25123152687338185, 0.25451559912297134, 0.2577996714704338, 0.2561576352477661, 0.2413793099288674, 0.24630541840112463, 0.25615763544351206, 0.24302134624940813, 0.2495894907485871, 0.25615763534563907, 0.25287356319392257, 0.25615763326583824, 0.26600985030822566, 0.20853858594040955, 0.24630541869474357, 0.2397372719200178, 0.21346469402117488, 0.2183908044753599, 0.2315270912960441, 0.23481116354563358, 0.23152709357159088, 0.23316912969638562, 0.251231524793581, 0.2430213441696073, 0.24137931032035934, 0.24630541869474357, 0.2610837438178963, 0.2610837438178963, 0.2577996715683068, 0.2610837438178963, 0.2643678160674858, 0.25287356319392257, 0.26272577994269103, 0.2643678160674858, 0.2610837438178963, 0.2610837438178963], 'loss': [1.609469070179996, 1.6062614142772353, 1.6057007839057968, 1.6053775542081015, 1.604946226848469, 1.6047578429294562, 1.6045007592598761, 1.6042697631847687, 1.6040295793780066, 1.6042895085023414, 1.6042507704278526, 1.6037237653497307, 1.6036693305695082, 1.6036781036388703, 1.6034053699191835, 1.6033123646428697, 1.6031017729389105, 1.6027658992724252, 1.602477083460751, 1.6026288180870196, 1.602412188224479, 1.6021422409668595, 1.6021569171235792, 1.6016570819232008, 1.601711467893706, 1.6016942905449525, 1.6013395711381821, 1.601210810222665, 1.6012120729599155, 1.6014113444812, 1.6008250576269945, 1.6008709842909044, 1.6004475222475965, 1.600476673692159, 1.600151423605071, 1.600909369290487, 1.5998024075427828, 1.5995710810107127, 1.5995996835540207, 1.5994452490698876, 1.59919420683898, 1.5986390356655238, 1.598595054732211, 1.5986822661922697, 1.598720297088858, 1.5981785863821512, 1.598517686189812, 1.5979650436730355, 1.5981447158163333, 1.5981642536069334, 1.5984724704734599, 1.5977661115188129, 1.5971684279138303, 1.5981544658143907, 1.597468100729909, 1.5973182807468047, 1.5972608056646107, 1.5970772896459215, 1.5993940407245801, 1.608468473275829, 1.602848464948196, 1.604077621162305, 1.5997366315775094, 1.599545566991614, 1.6002783385635158, 1.5995048285509772, 1.5986929785299595, 1.598794359495018, 1.598135892615426, 1.5980591655511875, 1.5973803182891753, 1.5980189289155682, 1.597298688467523, 1.5970463816390146, 1.596474429567247, 1.5968322636410441, 1.5964643491611834, 1.5979257355970033, 1.5971054307733963, 1.5964155021389408, 1.5958143724308367, 1.5961233350040975, 1.5956285375344434, 1.5953851465816615, 1.595907815134256, 1.5963252252132252, 1.5966808209918608, 1.5964069709151187, 1.5955288426832006, 1.5955246238982652, 1.5957479651214161, 1.5956576292519697, 1.5951712758144558, 1.5956141676990894, 1.597119243678616, 1.5969080199451173, 1.5947056300831037, 1.5963499893153228, 1.5950133194424043, 1.5959277717974152, 1.594912203675178, 1.5947600821450016, 1.5948682251896946, 1.5949868475876794, 1.594440459862382, 1.5949164852224582, 1.594436358524299, 1.595300768043471, 1.594752082051193, 1.594811705446341, 1.594919424429077, 1.5946426686809783, 1.5955411514951954, 1.5948439724881056, 1.5955705347981541, 1.5969356763534233, 1.5959546166034204, 1.5948209793415893, 1.5947481725739747, 1.594441470864862, 1.594923763255564, 1.5945694294308734, 1.5941436876261748, 1.5938124298804595, 1.593827103540393, 1.5940114084944832, 1.59424073989876, 1.5939276068117585, 1.5937101463028047, 1.594617221144925, 1.5943283452145618, 1.5940505202056445, 1.5957974755054136, 1.595052234886608, 1.5947248622866872, 1.594198603845475, 1.5938103078571924, 1.5940162460417229, 1.5932713486085928, 1.593337019317204, 1.5936520487865629, 1.594553215841493, 1.5942525603687983, 1.5934489433770307, 1.5934051428976979, 1.5932355818562438, 1.592895360551086, 1.5930707491888403, 1.5927343334260662, 1.5930526960557, 1.593050008828635, 1.593394439372194, 1.5930486810770368, 1.5937526997109948, 1.5928915203474385, 1.5928708364341784, 1.5926758795058702, 1.593488695929917, 1.5940523988657174, 1.5932285834631637, 1.5928177482048833, 1.5945136216631661, 1.5919649831568192, 1.5931835625450714, 1.5937928140530595, 1.5918558979426076, 1.592877087456, 1.5920971387710414, 1.5928938229225988, 1.591785715345974, 1.5921970811467885, 1.5921379776216384, 1.5918021385185037, 1.5917961414834556, 1.5934116092305899, 1.592390174102, 1.5935953715988254, 1.5914740256460296, 1.5916421620507994, 1.5916448482497763, 1.5909785204599525, 1.5913325225058523, 1.5906552307904378, 1.5907197942968756, 1.5909726666718782, 1.5904020668300025, 1.5913646633864917, 1.590769680667462, 1.591867562730699, 1.5897240519768403, 1.5925983045625, 1.5911115322759264, 1.5907322350468724, 1.59056978842561, 1.591097894341549, 1.5895585268185124, 1.5907586447267317, 1.59136188387626, 1.5898692327358395, 1.5902672895415852, 1.5895662257313974, 1.589748684336762, 1.589485957294519, 1.589761860414697, 1.5892461364518935, 1.5893350083235598, 1.5893055557959868, 1.588954122649081, 1.5894034185938277, 1.5893648258714461, 1.5890288703495472, 1.5895798094708327, 1.5888068777824573, 1.58868814626024, 1.5900446772330596, 1.589774960903661, 1.5898014054406104, 1.5881295800943394, 1.5894107138106954, 1.5892453636232098, 1.5887505994929916, 1.5889809185964126, 1.5881866097205473, 1.5881694047602786, 1.5879763463684176, 1.5875889617070036, 1.5874612928171177, 1.5881467235161784, 1.588364163610235, 1.588171010536335, 1.587914356020197, 1.5882900039273367, 1.5871333035110693, 1.5894218357681493, 1.5874773942959137, 1.589830423135777, 1.5911834444108686, 1.5884455537404367, 1.5890637872155442, 1.5881104061490945, 1.5874338942141992, 1.5877421017789743, 1.5881275261696848, 1.5881244857698005, 1.5875173448781947, 1.5877017467663272, 1.5870436956750293, 1.58727518802306, 1.5877083786703967, 1.5889256134659848, 1.5886864718960052, 1.5871789348688459, 1.5869060356269382, 1.5871612313346941, 1.587109877245627, 1.5868372177441261, 1.5871330455098553, 1.5870120352543355, 1.587380027183517, 1.5875738717692098, 1.5871764783741757, 1.587114107584317, 1.587344742555638, 1.586366449146545, 1.5865044672631141, 1.5869984571938642, 1.5860480992456236, 1.5869706609165888, 1.5873293443871719, 1.587003178762949, 1.5878143204800648, 1.5869519029554646, 1.587890808293462, 1.5884769122458582, 1.5909159620194955, 1.592627727814034, 1.5934427164908063, 1.5914308458383077, 1.602729992063628, 1.6096303365069005, 1.6021702023257465, 1.6026120837464224, 1.6008136615616095, 1.598156891711194, 1.5949278445704027, 1.594667259917367, 1.5937597917580262, 1.5928725048257095, 1.5934854123626647, 1.5943150611383958, 1.5937791732302438, 1.5934358477837252, 1.5933483448851036, 1.5930327638708346, 1.5927500270964918, 1.5925787227599284, 1.5923784698548993, 1.592101733395696, 1.5919143281188588, 1.5922719901102524], 'acc': [0.23285420912124782, 0.23285420953125924, 0.23285420970872686, 0.2328542098861945, 0.23285420951290053, 0.23285420892542147, 0.23285421047367355, 0.23285420872959514, 0.23285420833794243, 0.23285421031456463, 0.23285421164863163, 0.23285421049203225, 0.2328542102778472, 0.23285420931707418, 0.23285420851541005, 0.23285421010037957, 0.23737166211835167, 0.23901437337393633, 0.23942505314609597, 0.2394250504045271, 0.23942505218532295, 0.24188911630877236, 0.24271047216910846, 0.2435318271727043, 0.2431211487713291, 0.2427104727382288, 0.24312114878968782, 0.24312114916298178, 0.24435318377962836, 0.24188911750208916, 0.24476386063275152, 0.24353182736853066, 0.24558521545887974, 0.2443531815888211, 0.24353182679941032, 0.2451745382324626, 0.2451745374491572, 0.24476386082857787, 0.23983573001757785, 0.24188911750208916, 0.246817249506406, 0.24394250537825315, 0.24394250479077412, 0.2410677614642854, 0.2443531818213649, 0.24394250400746872, 0.24599589348696096, 0.24599589427026636, 0.2472279267144644, 0.24804928116729863, 0.2422997941226685, 0.24804928195060402, 0.2484599578062367, 0.2468172469055873, 0.24681724948804726, 0.2464065708908457, 0.24681724774396885, 0.24599589268529684, 0.24640656991171397, 0.23367556433902872, 0.22751540147548338, 0.23408624274040393, 0.23572895184189877, 0.2357289536043359, 0.2406570842378683, 0.244353183583802, 0.2410677614642854, 0.24106776128681778, 0.24394250520078553, 0.24969199320618865, 0.24845995997868525, 0.2443531837979871, 0.24640657149668346, 0.2476386031575761, 0.247227925931159, 0.2459958920978178, 0.24229979551181166, 0.24887063738256998, 0.241478439688193, 0.24435318164389727, 0.2472279251662123, 0.24558521626054383, 0.24599589387861365, 0.24640657090920443, 0.24681724749306633, 0.24599589387861365, 0.24188911791210058, 0.2447638606143928, 0.24476386218100357, 0.24476385924360836, 0.24722792614534406, 0.24271047389482817, 0.24681724909639458, 0.24640656850421208, 0.240657084256227, 0.24599589503521302, 0.24476386002691375, 0.24271047291569642, 0.24558521643801146, 0.2431211507295926, 0.2455852178087959, 0.24887063718674365, 0.24599589248947049, 0.2443531818213649, 0.24435318240884393, 0.2443531822313763, 0.24435318340633438, 0.2447638594394347, 0.24188911846286218, 0.2422997941226685, 0.24722792691029072, 0.2472279251478536, 0.24558521467557434, 0.2480492805981783, 0.2464065708908457, 0.24804928097147227, 0.2472279267144644, 0.24599589387861365, 0.24435318162553854, 0.2447638594394347, 0.2459958932911346, 0.2455852160647175, 0.24558521565470606, 0.24435318280049662, 0.2443531811971684, 0.24558521627890256, 0.2459958928811232, 0.24517453803663625, 0.2451745374491572, 0.24599589425190763, 0.24517453746751594, 0.24353182797436843, 0.24599589346860223, 0.2484599597828589, 0.24969199359784136, 0.24599589229364416, 0.2435318283660211, 0.24599589348696096, 0.2447638614160569, 0.2476386039408815, 0.2476386039408815, 0.24353182658522526, 0.2447638598310874, 0.2468172475297838, 0.24517453623748167, 0.24804928175477767, 0.24845995878536845, 0.2492813150006398, 0.2492813153922925, 0.2464065708908457, 0.24763860570331864, 0.24969199242288326, 0.2505133474815553, 0.2505133464840648, 0.25010267025513816, 0.2509240268437035, 0.2513347030909889, 0.24887063718674365, 0.24517453903412673, 0.24599589427026636, 0.24722792632281168, 0.2431211507479513, 0.25010266867016867, 0.2505133468757175, 0.24722792788942247, 0.2476386051158396, 0.2513347027176949, 0.24763860492001324, 0.2513347023260422, 0.25092402371048195, 0.25010266888435373, 0.2533880911996967, 0.2542094450466931, 0.24928131480481344, 0.25092402527709273, 0.2476386039408815, 0.25051334728572894, 0.2501026704509645, 0.24640657186997744, 0.2550307998911801, 0.2505133480690343, 0.24845996035197923, 0.2537987680161024, 0.24928131423569314, 0.2529774137774532, 0.25872690023460426, 0.25872689982459285, 0.25749486794951515, 0.2537987686035814, 0.2533880909855116, 0.24517453825082133, 0.25954825368994805, 0.25256673594519835, 0.2566735113058736, 0.25092402488544, 0.2537987686035814, 0.25667351013091555, 0.2505133476590229, 0.2537987662169478, 0.257084190135619, 0.2509240243163197, 0.2525667361593834, 0.25790554537175864, 0.25379876997436585, 0.2533880905938589, 0.2574948651712288, 0.2579055422018196, 0.26078028908990003, 0.24969199281453597, 0.25831622181487035, 0.26078028850242096, 0.2595482542957858, 0.25913757466437637, 0.2599589342454131, 0.25338809039803256, 0.254620123252242, 0.2579055446068119, 0.2501026696493004, 0.2529774127799628, 0.2583162224023494, 0.2579055440009742, 0.2517453807090587, 0.25831621942823674, 0.25626283529113203, 0.2603696108843512, 0.2599589308796477, 0.26242299600058755, 0.25749486755786244, 0.2616016415844708, 0.25749486873282057, 0.25995893404958675, 0.25462012327060074, 0.25872689884546113, 0.2599589326788023, 0.25954825525655884, 0.25174538168819044, 0.26160164491351867, 0.2599589314671268, 0.25913757763848905, 0.25831622142321764, 0.2591375778526741, 0.257494866187078, 0.25831622279400207, 0.260780288129127, 0.2587269008037246, 0.26406570768454235, 0.2611909641622273, 0.2603696118634829, 0.2599589328746286, 0.2599589334621077, 0.2599589318954969, 0.2616016437385606, 0.26036960912191404, 0.2607802877374743, 0.26078028908990003, 0.2591375774426627, 0.26078028948155274, 0.26036960951356675, 0.26078028811076825, 0.2607802873274629, 0.2591375764635309, 0.25954825228244616, 0.2603696075185858, 0.25010266945347404, 0.2546201236438947, 0.259958932697161, 0.2616016441302133, 0.2603696099052194, 0.2595482544732534, 0.259958929900516, 0.2480492815589513, 0.26078028811076825, 0.25995893130801784, 0.25790554595923765, 0.2599589334621077, 0.25790554595923765, 0.24845995958703254, 0.2624229976039158, 0.2197125272829185, 0.2443531826046703, 0.23983572802259692, 0.22833675590995892, 0.2287474327263646, 0.2435318283660211, 0.24435318360216074, 0.24476386002691375, 0.24640657030336666, 0.25420944622165126, 0.2542094448508668, 0.25503080128032324, 0.2546201228605893, 0.2570841897439663, 0.2583162229898284, 0.25708418915648723, 0.25749486697038343, 0.25667351234008157, 0.25995893130801784, 0.2574948645837498, 0.25708418954813994, 0.2574948657954253]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
