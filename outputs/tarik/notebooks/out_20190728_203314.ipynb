{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf69.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 20:33:14 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'Split', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000027D2760CE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000027D24CD6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0783, Accuracy:0.3775, Validation Loss:1.0751, Validation Accuracy:0.3998\n",
    "Epoch #2: Loss:1.0748, Accuracy:0.3961, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #3: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.4052\n",
    "Epoch #4: Loss:1.0748, Accuracy:0.3917, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #5: Loss:1.0746, Accuracy:0.3998, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #6: Loss:1.0746, Accuracy:0.3948, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0744, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.4031\n",
    "Epoch #8: Loss:1.0743, Accuracy:0.4007, Validation Loss:1.0740, Validation Accuracy:0.4011\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.3978, Validation Loss:1.0739, Validation Accuracy:0.3875\n",
    "Epoch #10: Loss:1.0741, Accuracy:0.3946, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #11: Loss:1.0739, Accuracy:0.3944, Validation Loss:1.0738, Validation Accuracy:0.3883\n",
    "Epoch #12: Loss:1.0737, Accuracy:0.3987, Validation Loss:1.0738, Validation Accuracy:0.3970\n",
    "Epoch #13: Loss:1.0738, Accuracy:0.3982, Validation Loss:1.0738, Validation Accuracy:0.3982\n",
    "Epoch #14: Loss:1.0737, Accuracy:0.4011, Validation Loss:1.0737, Validation Accuracy:0.3912\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3940, Validation Loss:1.0737, Validation Accuracy:0.3883\n",
    "Epoch #16: Loss:1.0735, Accuracy:0.4009, Validation Loss:1.0736, Validation Accuracy:0.4048\n",
    "Epoch #17: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.4035\n",
    "Epoch #18: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #19: Loss:1.0735, Accuracy:0.3996, Validation Loss:1.0736, Validation Accuracy:0.4031\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.3971, Validation Loss:1.0738, Validation Accuracy:0.3904\n",
    "Epoch #21: Loss:1.0739, Accuracy:0.3945, Validation Loss:1.0735, Validation Accuracy:0.3892\n",
    "Epoch #22: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0737, Validation Accuracy:0.3970\n",
    "Epoch #23: Loss:1.0738, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.3998\n",
    "Epoch #24: Loss:1.0734, Accuracy:0.3994, Validation Loss:1.0736, Validation Accuracy:0.4019\n",
    "Epoch #25: Loss:1.0737, Accuracy:0.3945, Validation Loss:1.0734, Validation Accuracy:0.4011\n",
    "Epoch #26: Loss:1.0733, Accuracy:0.3983, Validation Loss:1.0734, Validation Accuracy:0.3916\n",
    "Epoch #27: Loss:1.0738, Accuracy:0.3991, Validation Loss:1.0733, Validation Accuracy:0.4052\n",
    "Epoch #28: Loss:1.0732, Accuracy:0.4006, Validation Loss:1.0733, Validation Accuracy:0.4052\n",
    "Epoch #29: Loss:1.0735, Accuracy:0.3993, Validation Loss:1.0731, Validation Accuracy:0.4056\n",
    "Epoch #30: Loss:1.0732, Accuracy:0.3997, Validation Loss:1.0735, Validation Accuracy:0.4015\n",
    "Epoch #31: Loss:1.0744, Accuracy:0.3986, Validation Loss:1.0739, Validation Accuracy:0.4056\n",
    "Epoch #32: Loss:1.0736, Accuracy:0.3923, Validation Loss:1.0733, Validation Accuracy:0.4068\n",
    "Epoch #33: Loss:1.0736, Accuracy:0.3953, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #34: Loss:1.0734, Accuracy:0.3988, Validation Loss:1.0731, Validation Accuracy:0.4007\n",
    "Epoch #35: Loss:1.0737, Accuracy:0.3975, Validation Loss:1.0733, Validation Accuracy:0.3994\n",
    "Epoch #36: Loss:1.0730, Accuracy:0.4012, Validation Loss:1.0729, Validation Accuracy:0.4039\n",
    "Epoch #37: Loss:1.0735, Accuracy:0.3990, Validation Loss:1.0728, Validation Accuracy:0.4011\n",
    "Epoch #38: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0728, Validation Accuracy:0.3994\n",
    "Epoch #39: Loss:1.0728, Accuracy:0.3989, Validation Loss:1.0730, Validation Accuracy:0.3966\n",
    "Epoch #40: Loss:1.0732, Accuracy:0.3980, Validation Loss:1.0732, Validation Accuracy:0.3867\n",
    "Epoch #41: Loss:1.0731, Accuracy:0.3960, Validation Loss:1.0729, Validation Accuracy:0.3937\n",
    "Epoch #42: Loss:1.0728, Accuracy:0.4003, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #43: Loss:1.0731, Accuracy:0.3974, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0731, Accuracy:0.3976, Validation Loss:1.0732, Validation Accuracy:0.4011\n",
    "Epoch #45: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0731, Validation Accuracy:0.3982\n",
    "Epoch #46: Loss:1.0732, Accuracy:0.3991, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #47: Loss:1.0730, Accuracy:0.3978, Validation Loss:1.0731, Validation Accuracy:0.3970\n",
    "Epoch #48: Loss:1.0735, Accuracy:0.3976, Validation Loss:1.0731, Validation Accuracy:0.3912\n",
    "Epoch #49: Loss:1.0728, Accuracy:0.4007, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #50: Loss:1.0728, Accuracy:0.4017, Validation Loss:1.0727, Validation Accuracy:0.4126\n",
    "Epoch #51: Loss:1.0726, Accuracy:0.3995, Validation Loss:1.0726, Validation Accuracy:0.4011\n",
    "Epoch #52: Loss:1.0723, Accuracy:0.4003, Validation Loss:1.0725, Validation Accuracy:0.4019\n",
    "Epoch #53: Loss:1.0727, Accuracy:0.3996, Validation Loss:1.0731, Validation Accuracy:0.4002\n",
    "Epoch #54: Loss:1.0731, Accuracy:0.4021, Validation Loss:1.0732, Validation Accuracy:0.4183\n",
    "Epoch #55: Loss:1.0725, Accuracy:0.4028, Validation Loss:1.0726, Validation Accuracy:0.4007\n",
    "Epoch #56: Loss:1.0722, Accuracy:0.4028, Validation Loss:1.0735, Validation Accuracy:0.4044\n",
    "Epoch #57: Loss:1.0727, Accuracy:0.4052, Validation Loss:1.0735, Validation Accuracy:0.3929\n",
    "Epoch #58: Loss:1.0735, Accuracy:0.3995, Validation Loss:1.0740, Validation Accuracy:0.3834\n",
    "Epoch #59: Loss:1.0741, Accuracy:0.3902, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0746, Accuracy:0.3941, Validation Loss:1.0741, Validation Accuracy:0.3896\n",
    "Epoch #61: Loss:1.0738, Accuracy:0.3961, Validation Loss:1.0736, Validation Accuracy:0.3883\n",
    "Epoch #62: Loss:1.0730, Accuracy:0.3976, Validation Loss:1.0743, Validation Accuracy:0.3896\n",
    "Epoch #63: Loss:1.0737, Accuracy:0.3983, Validation Loss:1.0740, Validation Accuracy:0.3879\n",
    "Epoch #64: Loss:1.0742, Accuracy:0.3949, Validation Loss:1.0738, Validation Accuracy:0.3855\n",
    "Epoch #65: Loss:1.0725, Accuracy:0.3972, Validation Loss:1.0738, Validation Accuracy:0.3916\n",
    "Epoch #66: Loss:1.0727, Accuracy:0.3962, Validation Loss:1.0732, Validation Accuracy:0.3929\n",
    "Epoch #67: Loss:1.0725, Accuracy:0.3972, Validation Loss:1.0732, Validation Accuracy:0.3863\n",
    "Epoch #68: Loss:1.0724, Accuracy:0.3964, Validation Loss:1.0734, Validation Accuracy:0.3867\n",
    "Epoch #69: Loss:1.0727, Accuracy:0.3953, Validation Loss:1.0733, Validation Accuracy:0.3900\n",
    "Epoch #70: Loss:1.0728, Accuracy:0.3999, Validation Loss:1.0739, Validation Accuracy:0.3879\n",
    "Epoch #71: Loss:1.0728, Accuracy:0.3965, Validation Loss:1.0736, Validation Accuracy:0.3937\n",
    "Epoch #72: Loss:1.0727, Accuracy:0.3979, Validation Loss:1.0735, Validation Accuracy:0.3912\n",
    "Epoch #73: Loss:1.0726, Accuracy:0.3966, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #74: Loss:1.0727, Accuracy:0.3974, Validation Loss:1.0732, Validation Accuracy:0.3912\n",
    "Epoch #75: Loss:1.0722, Accuracy:0.3998, Validation Loss:1.0731, Validation Accuracy:0.3929\n",
    "Epoch #76: Loss:1.0721, Accuracy:0.3970, Validation Loss:1.0730, Validation Accuracy:0.3900\n",
    "Epoch #77: Loss:1.0720, Accuracy:0.3975, Validation Loss:1.0737, Validation Accuracy:0.3961\n",
    "Epoch #78: Loss:1.0728, Accuracy:0.3978, Validation Loss:1.0730, Validation Accuracy:0.3912\n",
    "Epoch #79: Loss:1.0718, Accuracy:0.3978, Validation Loss:1.0729, Validation Accuracy:0.3793\n",
    "Epoch #80: Loss:1.0723, Accuracy:0.3961, Validation Loss:1.0730, Validation Accuracy:0.3883\n",
    "Epoch #81: Loss:1.0734, Accuracy:0.3968, Validation Loss:1.0730, Validation Accuracy:0.3916\n",
    "Epoch #82: Loss:1.0726, Accuracy:0.3964, Validation Loss:1.0724, Validation Accuracy:0.3933\n",
    "Epoch #83: Loss:1.0727, Accuracy:0.3987, Validation Loss:1.0737, Validation Accuracy:0.3937\n",
    "Epoch #84: Loss:1.0722, Accuracy:0.3989, Validation Loss:1.0728, Validation Accuracy:0.3916\n",
    "Epoch #85: Loss:1.0719, Accuracy:0.3986, Validation Loss:1.0729, Validation Accuracy:0.3916\n",
    "Epoch #86: Loss:1.0722, Accuracy:0.3979, Validation Loss:1.0727, Validation Accuracy:0.3916\n",
    "Epoch #87: Loss:1.0720, Accuracy:0.3980, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #88: Loss:1.0722, Accuracy:0.3985, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #89: Loss:1.0721, Accuracy:0.3987, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #90: Loss:1.0724, Accuracy:0.3986, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #91: Loss:1.0722, Accuracy:0.4000, Validation Loss:1.0724, Validation Accuracy:0.3994\n",
    "Epoch #92: Loss:1.0718, Accuracy:0.4006, Validation Loss:1.0723, Validation Accuracy:0.3994\n",
    "Epoch #93: Loss:1.0716, Accuracy:0.4031, Validation Loss:1.0724, Validation Accuracy:0.3961\n",
    "Epoch #94: Loss:1.0717, Accuracy:0.4011, Validation Loss:1.0724, Validation Accuracy:0.3994\n",
    "Epoch #95: Loss:1.0720, Accuracy:0.4046, Validation Loss:1.0725, Validation Accuracy:0.4007\n",
    "Epoch #96: Loss:1.0721, Accuracy:0.4031, Validation Loss:1.0727, Validation Accuracy:0.3978\n",
    "Epoch #97: Loss:1.0722, Accuracy:0.3915, Validation Loss:1.0724, Validation Accuracy:0.3961\n",
    "Epoch #98: Loss:1.0725, Accuracy:0.3986, Validation Loss:1.0720, Validation Accuracy:0.3929\n",
    "Epoch #99: Loss:1.0714, Accuracy:0.4011, Validation Loss:1.0721, Validation Accuracy:0.3961\n",
    "Epoch #100: Loss:1.0714, Accuracy:0.3983, Validation Loss:1.0725, Validation Accuracy:0.3961\n",
    "Epoch #101: Loss:1.0717, Accuracy:0.3988, Validation Loss:1.0720, Validation Accuracy:0.3842\n",
    "Epoch #102: Loss:1.0714, Accuracy:0.3951, Validation Loss:1.0721, Validation Accuracy:0.3822\n",
    "Epoch #103: Loss:1.0717, Accuracy:0.4014, Validation Loss:1.0719, Validation Accuracy:0.3773\n",
    "Epoch #104: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0720, Validation Accuracy:0.3838\n",
    "Epoch #105: Loss:1.0711, Accuracy:0.3952, Validation Loss:1.0725, Validation Accuracy:0.3801\n",
    "Epoch #106: Loss:1.0720, Accuracy:0.3950, Validation Loss:1.0717, Validation Accuracy:0.3863\n",
    "Epoch #107: Loss:1.0720, Accuracy:0.3973, Validation Loss:1.0721, Validation Accuracy:0.3793\n",
    "Epoch #108: Loss:1.0741, Accuracy:0.3779, Validation Loss:1.0726, Validation Accuracy:0.3916\n",
    "Epoch #109: Loss:1.0726, Accuracy:0.3947, Validation Loss:1.0721, Validation Accuracy:0.3892\n",
    "Epoch #110: Loss:1.0714, Accuracy:0.3970, Validation Loss:1.0716, Validation Accuracy:0.3974\n",
    "Epoch #111: Loss:1.0714, Accuracy:0.4034, Validation Loss:1.0716, Validation Accuracy:0.4002\n",
    "Epoch #112: Loss:1.0718, Accuracy:0.4015, Validation Loss:1.0715, Validation Accuracy:0.3920\n",
    "Epoch #113: Loss:1.0712, Accuracy:0.3992, Validation Loss:1.0714, Validation Accuracy:0.3863\n",
    "Epoch #114: Loss:1.0712, Accuracy:0.3959, Validation Loss:1.0713, Validation Accuracy:0.3875\n",
    "Epoch #115: Loss:1.0716, Accuracy:0.4016, Validation Loss:1.0714, Validation Accuracy:0.3867\n",
    "Epoch #116: Loss:1.0724, Accuracy:0.3971, Validation Loss:1.0714, Validation Accuracy:0.3929\n",
    "Epoch #117: Loss:1.0718, Accuracy:0.3991, Validation Loss:1.0714, Validation Accuracy:0.3875\n",
    "Epoch #118: Loss:1.0715, Accuracy:0.3994, Validation Loss:1.0719, Validation Accuracy:0.3937\n",
    "Epoch #119: Loss:1.0713, Accuracy:0.4001, Validation Loss:1.0714, Validation Accuracy:0.3805\n",
    "Epoch #120: Loss:1.0709, Accuracy:0.4004, Validation Loss:1.0714, Validation Accuracy:0.3961\n",
    "Epoch #121: Loss:1.0713, Accuracy:0.3948, Validation Loss:1.0715, Validation Accuracy:0.3998\n",
    "Epoch #122: Loss:1.0712, Accuracy:0.4021, Validation Loss:1.0711, Validation Accuracy:0.3986\n",
    "Epoch #123: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0709, Validation Accuracy:0.3834\n",
    "Epoch #124: Loss:1.0714, Accuracy:0.3970, Validation Loss:1.0715, Validation Accuracy:0.3929\n",
    "Epoch #125: Loss:1.0705, Accuracy:0.3954, Validation Loss:1.0709, Validation Accuracy:0.3838\n",
    "Epoch #126: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0711, Validation Accuracy:0.3838\n",
    "Epoch #127: Loss:1.0703, Accuracy:0.4006, Validation Loss:1.0707, Validation Accuracy:0.3851\n",
    "Epoch #128: Loss:1.0708, Accuracy:0.4013, Validation Loss:1.0710, Validation Accuracy:0.3937\n",
    "Epoch #129: Loss:1.0702, Accuracy:0.4008, Validation Loss:1.0702, Validation Accuracy:0.3834\n",
    "Epoch #130: Loss:1.0711, Accuracy:0.3998, Validation Loss:1.0703, Validation Accuracy:0.3875\n",
    "Epoch #131: Loss:1.0702, Accuracy:0.4026, Validation Loss:1.0702, Validation Accuracy:0.3986\n",
    "Epoch #132: Loss:1.0703, Accuracy:0.4013, Validation Loss:1.0698, Validation Accuracy:0.3957\n",
    "Epoch #133: Loss:1.0697, Accuracy:0.4027, Validation Loss:1.0696, Validation Accuracy:0.3883\n",
    "Epoch #134: Loss:1.0702, Accuracy:0.4014, Validation Loss:1.0693, Validation Accuracy:0.3924\n",
    "Epoch #135: Loss:1.0694, Accuracy:0.3969, Validation Loss:1.0694, Validation Accuracy:0.3953\n",
    "Epoch #136: Loss:1.0706, Accuracy:0.4001, Validation Loss:1.0698, Validation Accuracy:0.3945\n",
    "Epoch #137: Loss:1.0698, Accuracy:0.4004, Validation Loss:1.0696, Validation Accuracy:0.3953\n",
    "Epoch #138: Loss:1.0701, Accuracy:0.4068, Validation Loss:1.0695, Validation Accuracy:0.3900\n",
    "Epoch #139: Loss:1.0698, Accuracy:0.3995, Validation Loss:1.0695, Validation Accuracy:0.3982\n",
    "Epoch #140: Loss:1.0708, Accuracy:0.4079, Validation Loss:1.0694, Validation Accuracy:0.3900\n",
    "Epoch #141: Loss:1.0704, Accuracy:0.4002, Validation Loss:1.0700, Validation Accuracy:0.3867\n",
    "Epoch #142: Loss:1.0701, Accuracy:0.3958, Validation Loss:1.0697, Validation Accuracy:0.3838\n",
    "Epoch #143: Loss:1.0698, Accuracy:0.3989, Validation Loss:1.0695, Validation Accuracy:0.3768\n",
    "Epoch #144: Loss:1.0698, Accuracy:0.3973, Validation Loss:1.0698, Validation Accuracy:0.3801\n",
    "Epoch #145: Loss:1.0698, Accuracy:0.4003, Validation Loss:1.0688, Validation Accuracy:0.3883\n",
    "Epoch #146: Loss:1.0689, Accuracy:0.3993, Validation Loss:1.0683, Validation Accuracy:0.3883\n",
    "Epoch #147: Loss:1.0690, Accuracy:0.4055, Validation Loss:1.0687, Validation Accuracy:0.4035\n",
    "Epoch #148: Loss:1.0689, Accuracy:0.4016, Validation Loss:1.0682, Validation Accuracy:0.3814\n",
    "Epoch #149: Loss:1.0694, Accuracy:0.3978, Validation Loss:1.0680, Validation Accuracy:0.3842\n",
    "Epoch #150: Loss:1.0689, Accuracy:0.4008, Validation Loss:1.0679, Validation Accuracy:0.4002\n",
    "Epoch #151: Loss:1.0699, Accuracy:0.3999, Validation Loss:1.0679, Validation Accuracy:0.4048\n",
    "Epoch #152: Loss:1.0692, Accuracy:0.4001, Validation Loss:1.0686, Validation Accuracy:0.3974\n",
    "Epoch #153: Loss:1.0694, Accuracy:0.4007, Validation Loss:1.0681, Validation Accuracy:0.3842\n",
    "Epoch #154: Loss:1.0693, Accuracy:0.4000, Validation Loss:1.0682, Validation Accuracy:0.3838\n",
    "Epoch #155: Loss:1.0692, Accuracy:0.4022, Validation Loss:1.0681, Validation Accuracy:0.3974\n",
    "Epoch #156: Loss:1.0702, Accuracy:0.3988, Validation Loss:1.0682, Validation Accuracy:0.3859\n",
    "Epoch #157: Loss:1.0691, Accuracy:0.3996, Validation Loss:1.0679, Validation Accuracy:0.3871\n",
    "Epoch #158: Loss:1.0688, Accuracy:0.4014, Validation Loss:1.0679, Validation Accuracy:0.3883\n",
    "Epoch #159: Loss:1.0687, Accuracy:0.4009, Validation Loss:1.0678, Validation Accuracy:0.3978\n",
    "Epoch #160: Loss:1.0687, Accuracy:0.4012, Validation Loss:1.0678, Validation Accuracy:0.3974\n",
    "Epoch #161: Loss:1.0691, Accuracy:0.4039, Validation Loss:1.0676, Validation Accuracy:0.3883\n",
    "Epoch #162: Loss:1.0687, Accuracy:0.3999, Validation Loss:1.0675, Validation Accuracy:0.3867\n",
    "Epoch #163: Loss:1.0690, Accuracy:0.3991, Validation Loss:1.0674, Validation Accuracy:0.3978\n",
    "Epoch #164: Loss:1.0685, Accuracy:0.4016, Validation Loss:1.0675, Validation Accuracy:0.4027\n",
    "Epoch #165: Loss:1.0685, Accuracy:0.4060, Validation Loss:1.0672, Validation Accuracy:0.4027\n",
    "Epoch #166: Loss:1.0684, Accuracy:0.4006, Validation Loss:1.0676, Validation Accuracy:0.3900\n",
    "Epoch #167: Loss:1.0686, Accuracy:0.4003, Validation Loss:1.0679, Validation Accuracy:0.3937\n",
    "Epoch #168: Loss:1.0686, Accuracy:0.4014, Validation Loss:1.0673, Validation Accuracy:0.3908\n",
    "Epoch #169: Loss:1.0681, Accuracy:0.3991, Validation Loss:1.0675, Validation Accuracy:0.3855\n",
    "Epoch #170: Loss:1.0682, Accuracy:0.4040, Validation Loss:1.0674, Validation Accuracy:0.3855\n",
    "Epoch #171: Loss:1.0685, Accuracy:0.4038, Validation Loss:1.0670, Validation Accuracy:0.3978\n",
    "Epoch #172: Loss:1.0685, Accuracy:0.4008, Validation Loss:1.0670, Validation Accuracy:0.3883\n",
    "Epoch #173: Loss:1.0682, Accuracy:0.4011, Validation Loss:1.0675, Validation Accuracy:0.3937\n",
    "Epoch #174: Loss:1.0686, Accuracy:0.4028, Validation Loss:1.0671, Validation Accuracy:0.3912\n",
    "Epoch #175: Loss:1.0679, Accuracy:0.4004, Validation Loss:1.0670, Validation Accuracy:0.3871\n",
    "Epoch #176: Loss:1.0688, Accuracy:0.3957, Validation Loss:1.0673, Validation Accuracy:0.3867\n",
    "Epoch #177: Loss:1.0685, Accuracy:0.3990, Validation Loss:1.0672, Validation Accuracy:0.3949\n",
    "Epoch #178: Loss:1.0679, Accuracy:0.4036, Validation Loss:1.0669, Validation Accuracy:0.3949\n",
    "Epoch #179: Loss:1.0678, Accuracy:0.4026, Validation Loss:1.0666, Validation Accuracy:0.3937\n",
    "Epoch #180: Loss:1.0682, Accuracy:0.4012, Validation Loss:1.0668, Validation Accuracy:0.3937\n",
    "Epoch #181: Loss:1.0688, Accuracy:0.3996, Validation Loss:1.0674, Validation Accuracy:0.3933\n",
    "Epoch #182: Loss:1.0689, Accuracy:0.4032, Validation Loss:1.0670, Validation Accuracy:0.4056\n",
    "Epoch #183: Loss:1.0694, Accuracy:0.3954, Validation Loss:1.0668, Validation Accuracy:0.4048\n",
    "Epoch #184: Loss:1.0680, Accuracy:0.4002, Validation Loss:1.0663, Validation Accuracy:0.4044\n",
    "Epoch #185: Loss:1.0688, Accuracy:0.4029, Validation Loss:1.0663, Validation Accuracy:0.4044\n",
    "Epoch #186: Loss:1.0681, Accuracy:0.4001, Validation Loss:1.0664, Validation Accuracy:0.4060\n",
    "Epoch #187: Loss:1.0683, Accuracy:0.4034, Validation Loss:1.0667, Validation Accuracy:0.3908\n",
    "Epoch #188: Loss:1.0678, Accuracy:0.3979, Validation Loss:1.0664, Validation Accuracy:0.3871\n",
    "Epoch #189: Loss:1.0684, Accuracy:0.4024, Validation Loss:1.0667, Validation Accuracy:0.3937\n",
    "Epoch #190: Loss:1.0681, Accuracy:0.4001, Validation Loss:1.0663, Validation Accuracy:0.4019\n",
    "Epoch #191: Loss:1.0678, Accuracy:0.4077, Validation Loss:1.0665, Validation Accuracy:0.4068\n",
    "Epoch #192: Loss:1.0681, Accuracy:0.4034, Validation Loss:1.0665, Validation Accuracy:0.4044\n",
    "Epoch #193: Loss:1.0679, Accuracy:0.4072, Validation Loss:1.0665, Validation Accuracy:0.3937\n",
    "Epoch #194: Loss:1.0692, Accuracy:0.3940, Validation Loss:1.0665, Validation Accuracy:0.3924\n",
    "Epoch #195: Loss:1.0676, Accuracy:0.4042, Validation Loss:1.0660, Validation Accuracy:0.4019\n",
    "Epoch #196: Loss:1.0676, Accuracy:0.4056, Validation Loss:1.0662, Validation Accuracy:0.4056\n",
    "Epoch #197: Loss:1.0677, Accuracy:0.4066, Validation Loss:1.0661, Validation Accuracy:0.4044\n",
    "Epoch #198: Loss:1.0679, Accuracy:0.4015, Validation Loss:1.0659, Validation Accuracy:0.3924\n",
    "Epoch #199: Loss:1.0677, Accuracy:0.4051, Validation Loss:1.0656, Validation Accuracy:0.4007\n",
    "Epoch #200: Loss:1.0674, Accuracy:0.4042, Validation Loss:1.0654, Validation Accuracy:0.4060\n",
    "Epoch #201: Loss:1.0674, Accuracy:0.4022, Validation Loss:1.0658, Validation Accuracy:0.4068\n",
    "Epoch #202: Loss:1.0674, Accuracy:0.4054, Validation Loss:1.0656, Validation Accuracy:0.3978\n",
    "Epoch #203: Loss:1.0673, Accuracy:0.4030, Validation Loss:1.0656, Validation Accuracy:0.4068\n",
    "Epoch #204: Loss:1.0678, Accuracy:0.4063, Validation Loss:1.0656, Validation Accuracy:0.3978\n",
    "Epoch #205: Loss:1.0672, Accuracy:0.3970, Validation Loss:1.0654, Validation Accuracy:0.4015\n",
    "Epoch #206: Loss:1.0680, Accuracy:0.4008, Validation Loss:1.0655, Validation Accuracy:0.3937\n",
    "Epoch #207: Loss:1.0674, Accuracy:0.4050, Validation Loss:1.0658, Validation Accuracy:0.3949\n",
    "Epoch #208: Loss:1.0670, Accuracy:0.4008, Validation Loss:1.0655, Validation Accuracy:0.3978\n",
    "Epoch #209: Loss:1.0671, Accuracy:0.4034, Validation Loss:1.0655, Validation Accuracy:0.4068\n",
    "Epoch #210: Loss:1.0673, Accuracy:0.4015, Validation Loss:1.0659, Validation Accuracy:0.3937\n",
    "Epoch #211: Loss:1.0680, Accuracy:0.4011, Validation Loss:1.0661, Validation Accuracy:0.3937\n",
    "Epoch #212: Loss:1.0673, Accuracy:0.4043, Validation Loss:1.0659, Validation Accuracy:0.3924\n",
    "Epoch #213: Loss:1.0686, Accuracy:0.3964, Validation Loss:1.0662, Validation Accuracy:0.3990\n",
    "Epoch #214: Loss:1.0690, Accuracy:0.4036, Validation Loss:1.0659, Validation Accuracy:0.3978\n",
    "Epoch #215: Loss:1.0684, Accuracy:0.4081, Validation Loss:1.0662, Validation Accuracy:0.4056\n",
    "Epoch #216: Loss:1.0686, Accuracy:0.4057, Validation Loss:1.0661, Validation Accuracy:0.4085\n",
    "Epoch #217: Loss:1.0674, Accuracy:0.3985, Validation Loss:1.0662, Validation Accuracy:0.3949\n",
    "Epoch #218: Loss:1.0670, Accuracy:0.4046, Validation Loss:1.0658, Validation Accuracy:0.3978\n",
    "Epoch #219: Loss:1.0677, Accuracy:0.4054, Validation Loss:1.0657, Validation Accuracy:0.3929\n",
    "Epoch #220: Loss:1.0688, Accuracy:0.3941, Validation Loss:1.0654, Validation Accuracy:0.3978\n",
    "Epoch #221: Loss:1.0686, Accuracy:0.4040, Validation Loss:1.0659, Validation Accuracy:0.4031\n",
    "Epoch #222: Loss:1.0670, Accuracy:0.4001, Validation Loss:1.0659, Validation Accuracy:0.3978\n",
    "Epoch #223: Loss:1.0668, Accuracy:0.4036, Validation Loss:1.0656, Validation Accuracy:0.4068\n",
    "Epoch #224: Loss:1.0666, Accuracy:0.4024, Validation Loss:1.0654, Validation Accuracy:0.3937\n",
    "Epoch #225: Loss:1.0672, Accuracy:0.4002, Validation Loss:1.0653, Validation Accuracy:0.3937\n",
    "Epoch #226: Loss:1.0668, Accuracy:0.4063, Validation Loss:1.0652, Validation Accuracy:0.4109\n",
    "Epoch #227: Loss:1.0667, Accuracy:0.4015, Validation Loss:1.0654, Validation Accuracy:0.3949\n",
    "Epoch #228: Loss:1.0666, Accuracy:0.4022, Validation Loss:1.0652, Validation Accuracy:0.3990\n",
    "Epoch #229: Loss:1.0668, Accuracy:0.4000, Validation Loss:1.0654, Validation Accuracy:0.3933\n",
    "Epoch #230: Loss:1.0675, Accuracy:0.4021, Validation Loss:1.0657, Validation Accuracy:0.3978\n",
    "Epoch #231: Loss:1.0669, Accuracy:0.4069, Validation Loss:1.0650, Validation Accuracy:0.4072\n",
    "Epoch #232: Loss:1.0672, Accuracy:0.4030, Validation Loss:1.0655, Validation Accuracy:0.3912\n",
    "Epoch #233: Loss:1.0677, Accuracy:0.4017, Validation Loss:1.0652, Validation Accuracy:0.3924\n",
    "Epoch #234: Loss:1.0701, Accuracy:0.3952, Validation Loss:1.0656, Validation Accuracy:0.3924\n",
    "Epoch #235: Loss:1.0673, Accuracy:0.4020, Validation Loss:1.0656, Validation Accuracy:0.3978\n",
    "Epoch #236: Loss:1.0666, Accuracy:0.4004, Validation Loss:1.0654, Validation Accuracy:0.3949\n",
    "Epoch #237: Loss:1.0673, Accuracy:0.3982, Validation Loss:1.0657, Validation Accuracy:0.3990\n",
    "Epoch #238: Loss:1.0667, Accuracy:0.4034, Validation Loss:1.0651, Validation Accuracy:0.3949\n",
    "Epoch #239: Loss:1.0666, Accuracy:0.4041, Validation Loss:1.0654, Validation Accuracy:0.3937\n",
    "Epoch #240: Loss:1.0662, Accuracy:0.4030, Validation Loss:1.0649, Validation Accuracy:0.3961\n",
    "Epoch #241: Loss:1.0663, Accuracy:0.4001, Validation Loss:1.0650, Validation Accuracy:0.3953\n",
    "Epoch #242: Loss:1.0669, Accuracy:0.4015, Validation Loss:1.0648, Validation Accuracy:0.3953\n",
    "Epoch #243: Loss:1.0662, Accuracy:0.4042, Validation Loss:1.0648, Validation Accuracy:0.3961\n",
    "Epoch #244: Loss:1.0659, Accuracy:0.4068, Validation Loss:1.0648, Validation Accuracy:0.4080\n",
    "Epoch #245: Loss:1.0659, Accuracy:0.4087, Validation Loss:1.0654, Validation Accuracy:0.3961\n",
    "Epoch #246: Loss:1.0664, Accuracy:0.4072, Validation Loss:1.0654, Validation Accuracy:0.3990\n",
    "Epoch #247: Loss:1.0666, Accuracy:0.4021, Validation Loss:1.0652, Validation Accuracy:0.3998\n",
    "Epoch #248: Loss:1.0659, Accuracy:0.4064, Validation Loss:1.0655, Validation Accuracy:0.3998\n",
    "Epoch #249: Loss:1.0663, Accuracy:0.4009, Validation Loss:1.0651, Validation Accuracy:0.3978\n",
    "Epoch #250: Loss:1.0659, Accuracy:0.4044, Validation Loss:1.0649, Validation Accuracy:0.4105\n",
    "Epoch #251: Loss:1.0656, Accuracy:0.4048, Validation Loss:1.0645, Validation Accuracy:0.3924\n",
    "Epoch #252: Loss:1.0663, Accuracy:0.3998, Validation Loss:1.0643, Validation Accuracy:0.3933\n",
    "Epoch #253: Loss:1.0660, Accuracy:0.4054, Validation Loss:1.0656, Validation Accuracy:0.4080\n",
    "Epoch #254: Loss:1.0653, Accuracy:0.4042, Validation Loss:1.0647, Validation Accuracy:0.3937\n",
    "Epoch #255: Loss:1.0667, Accuracy:0.3973, Validation Loss:1.0649, Validation Accuracy:0.3924\n",
    "Epoch #256: Loss:1.0657, Accuracy:0.4004, Validation Loss:1.0642, Validation Accuracy:0.4097\n",
    "Epoch #257: Loss:1.0657, Accuracy:0.4074, Validation Loss:1.0642, Validation Accuracy:0.4105\n",
    "Epoch #258: Loss:1.0652, Accuracy:0.4049, Validation Loss:1.0637, Validation Accuracy:0.4101\n",
    "Epoch #259: Loss:1.0654, Accuracy:0.4039, Validation Loss:1.0653, Validation Accuracy:0.4080\n",
    "Epoch #260: Loss:1.0657, Accuracy:0.4075, Validation Loss:1.0646, Validation Accuracy:0.4105\n",
    "Epoch #261: Loss:1.0653, Accuracy:0.4060, Validation Loss:1.0646, Validation Accuracy:0.4101\n",
    "Epoch #262: Loss:1.0655, Accuracy:0.4036, Validation Loss:1.0642, Validation Accuracy:0.3933\n",
    "Epoch #263: Loss:1.0647, Accuracy:0.4071, Validation Loss:1.0640, Validation Accuracy:0.3953\n",
    "Epoch #264: Loss:1.0651, Accuracy:0.4010, Validation Loss:1.0643, Validation Accuracy:0.4109\n",
    "Epoch #265: Loss:1.0654, Accuracy:0.4064, Validation Loss:1.0642, Validation Accuracy:0.4080\n",
    "Epoch #266: Loss:1.0651, Accuracy:0.4065, Validation Loss:1.0649, Validation Accuracy:0.4031\n",
    "Epoch #267: Loss:1.0653, Accuracy:0.4047, Validation Loss:1.0642, Validation Accuracy:0.4060\n",
    "Epoch #268: Loss:1.0653, Accuracy:0.4070, Validation Loss:1.0638, Validation Accuracy:0.4072\n",
    "Epoch #269: Loss:1.0641, Accuracy:0.4078, Validation Loss:1.0633, Validation Accuracy:0.4093\n",
    "Epoch #270: Loss:1.0646, Accuracy:0.4004, Validation Loss:1.0642, Validation Accuracy:0.4060\n",
    "Epoch #271: Loss:1.0644, Accuracy:0.4035, Validation Loss:1.0635, Validation Accuracy:0.4093\n",
    "Epoch #272: Loss:1.0648, Accuracy:0.4050, Validation Loss:1.0640, Validation Accuracy:0.4035\n",
    "Epoch #273: Loss:1.0645, Accuracy:0.4044, Validation Loss:1.0638, Validation Accuracy:0.3933\n",
    "Epoch #274: Loss:1.0649, Accuracy:0.3987, Validation Loss:1.0640, Validation Accuracy:0.4064\n",
    "Epoch #275: Loss:1.0647, Accuracy:0.4057, Validation Loss:1.0646, Validation Accuracy:0.4044\n",
    "Epoch #276: Loss:1.0650, Accuracy:0.4032, Validation Loss:1.0638, Validation Accuracy:0.3978\n",
    "Epoch #277: Loss:1.0660, Accuracy:0.4075, Validation Loss:1.0669, Validation Accuracy:0.4011\n",
    "Epoch #278: Loss:1.0657, Accuracy:0.4073, Validation Loss:1.0639, Validation Accuracy:0.3933\n",
    "Epoch #279: Loss:1.0649, Accuracy:0.4016, Validation Loss:1.0639, Validation Accuracy:0.4068\n",
    "Epoch #280: Loss:1.0637, Accuracy:0.4065, Validation Loss:1.0630, Validation Accuracy:0.3978\n",
    "Epoch #281: Loss:1.0642, Accuracy:0.3946, Validation Loss:1.0630, Validation Accuracy:0.3912\n",
    "Epoch #282: Loss:1.0649, Accuracy:0.4031, Validation Loss:1.0643, Validation Accuracy:0.4080\n",
    "Epoch #283: Loss:1.0645, Accuracy:0.4065, Validation Loss:1.0635, Validation Accuracy:0.4064\n",
    "Epoch #284: Loss:1.0647, Accuracy:0.4028, Validation Loss:1.0635, Validation Accuracy:0.3888\n",
    "Epoch #285: Loss:1.0642, Accuracy:0.4066, Validation Loss:1.0633, Validation Accuracy:0.4093\n",
    "Epoch #286: Loss:1.0636, Accuracy:0.4049, Validation Loss:1.0628, Validation Accuracy:0.3924\n",
    "Epoch #287: Loss:1.0646, Accuracy:0.4008, Validation Loss:1.0630, Validation Accuracy:0.4064\n",
    "Epoch #288: Loss:1.0641, Accuracy:0.4087, Validation Loss:1.0633, Validation Accuracy:0.4068\n",
    "Epoch #289: Loss:1.0641, Accuracy:0.4057, Validation Loss:1.0637, Validation Accuracy:0.4101\n",
    "Epoch #290: Loss:1.0635, Accuracy:0.4018, Validation Loss:1.0635, Validation Accuracy:0.3990\n",
    "Epoch #291: Loss:1.0634, Accuracy:0.4047, Validation Loss:1.0634, Validation Accuracy:0.4089\n",
    "Epoch #292: Loss:1.0640, Accuracy:0.4029, Validation Loss:1.0639, Validation Accuracy:0.3920\n",
    "Epoch #293: Loss:1.0646, Accuracy:0.4054, Validation Loss:1.0657, Validation Accuracy:0.4056\n",
    "Epoch #294: Loss:1.0655, Accuracy:0.4073, Validation Loss:1.0660, Validation Accuracy:0.4031\n",
    "Epoch #295: Loss:1.0636, Accuracy:0.4068, Validation Loss:1.0644, Validation Accuracy:0.4089\n",
    "Epoch #296: Loss:1.0633, Accuracy:0.4028, Validation Loss:1.0633, Validation Accuracy:0.4064\n",
    "Epoch #297: Loss:1.0635, Accuracy:0.4042, Validation Loss:1.0635, Validation Accuracy:0.4039\n",
    "Epoch #298: Loss:1.0640, Accuracy:0.3951, Validation Loss:1.0630, Validation Accuracy:0.4052\n",
    "Epoch #299: Loss:1.0645, Accuracy:0.4057, Validation Loss:1.0638, Validation Accuracy:0.4019\n",
    "Epoch #300: Loss:1.0628, Accuracy:0.4077, Validation Loss:1.0638, Validation Accuracy:0.3929\n",
    "\n",
    "Test:\n",
    "Test Loss:1.06378186, Accuracy:0.3929\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  389  571   0\n",
    "t:02  340  568   0\n",
    "t:03  275  293   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.39      0.41      0.40       960\n",
    "          02       0.40      0.63      0.49       908\n",
    "          03       0.00      0.00      0.00       568\n",
    "\n",
    "    accuracy                           0.39      2436\n",
    "   macro avg       0.26      0.34      0.29      2436\n",
    "weighted avg       0.30      0.39      0.34      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 21:35:50 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0750589386387215, 1.073937353829445, 1.07393577846596, 1.0737788387511555, 1.0737838382987162, 1.0741948513757615, 1.0738700119341145, 1.0739872759003162, 1.073905904500551, 1.0739119632295004, 1.0738355943134852, 1.073798676038219, 1.073788051926248, 1.0737350552735854, 1.0737122301201907, 1.0736495407148339, 1.0736379014643151, 1.0737643981802052, 1.0735961936769032, 1.0738149981193355, 1.0734765269290443, 1.0737062101489414, 1.073668799181094, 1.0735596334209974, 1.0733794987886802, 1.0733721164255503, 1.0733057321194552, 1.0733155638517808, 1.073140473592849, 1.0735102373195204, 1.0739386486889693, 1.0733293502397334, 1.0729555568867324, 1.0730659392079696, 1.0732866523692566, 1.0728962343118853, 1.07276408660588, 1.072762687022267, 1.0730045941858652, 1.0731859365707548, 1.072934486204376, 1.072763540866144, 1.0735293104143566, 1.0731960321686342, 1.0731371834947558, 1.0732070847685113, 1.0731303032200128, 1.0730610606314122, 1.0729650989150374, 1.072690393928628, 1.0726350128944284, 1.0724809490792662, 1.0730940549831671, 1.0732032949309827, 1.072627813162279, 1.073458336452741, 1.0735002446840158, 1.0740251204454645, 1.0739251839116288, 1.0740737022437485, 1.0735709610439481, 1.0743143376458455, 1.073968739345156, 1.0738463268687182, 1.0737945428622768, 1.073190830611243, 1.0731906047204054, 1.073387324320663, 1.0733489109377556, 1.0738909444198232, 1.073615004863645, 1.0735402279495214, 1.0734775350207375, 1.0732000482884925, 1.073082592295504, 1.0730363230399897, 1.0736630123432829, 1.0729907878318248, 1.072914329851398, 1.072984897052909, 1.0730084041852277, 1.072417043308515, 1.073683636333359, 1.0728325491468307, 1.0728824332429858, 1.072717030451607, 1.0727090954976324, 1.0725614359030387, 1.0725640130943461, 1.0726107861999612, 1.0723700131884546, 1.0723216112807075, 1.0723977086970764, 1.072351492488717, 1.0724536975224812, 1.0726531407320246, 1.0723984488125504, 1.0719502661224265, 1.0721139945028646, 1.0725079788558785, 1.0719635737157611, 1.072145212264288, 1.071912969078728, 1.0720347581042837, 1.0724555402749474, 1.0717129740613238, 1.0720976598940068, 1.0726213911288283, 1.0720966332064474, 1.0715867993475376, 1.071645858644069, 1.0715311141241164, 1.071438515127586, 1.0713443529038202, 1.0713683936396257, 1.0714077875140462, 1.0714469265272268, 1.0718699421592925, 1.0713719932120813, 1.0714175284202463, 1.0714884863306933, 1.071098965768548, 1.0708631602022645, 1.0714550356951058, 1.070880003359126, 1.0710932039862195, 1.0706739204466245, 1.0709599109706034, 1.0702188739244183, 1.0703033745190975, 1.070161448519414, 1.0697629040685193, 1.069581677565238, 1.069265540988966, 1.0694437596598283, 1.0697609575706946, 1.0695957362358206, 1.0694569102649032, 1.0695490169603445, 1.069440021303487, 1.0700351806305508, 1.0697054265950896, 1.0695425790714708, 1.0697525357965179, 1.0687513075438626, 1.0682988587662896, 1.0686921174890303, 1.068192013574547, 1.0680292100937692, 1.0678556983302576, 1.0679026656158648, 1.0685668753089967, 1.068131548822024, 1.0681951764377662, 1.0681207473642134, 1.0682230179728742, 1.0679481096064125, 1.0678641482918525, 1.0678359095881922, 1.0677739902474415, 1.0675607637818811, 1.0674750200046108, 1.0674455163905578, 1.0674929383940297, 1.0671830913311937, 1.0676457286859773, 1.0678629305562362, 1.067323674317847, 1.067460264282665, 1.067373143432567, 1.0670235098289151, 1.0670263405112406, 1.0675155599716262, 1.0670839418918627, 1.0669769528268398, 1.0672600472893425, 1.067210867682897, 1.0669228506009958, 1.0666142629676656, 1.066779506813325, 1.067397517523742, 1.0669970610263118, 1.0668265659037874, 1.0663098488339453, 1.0662810782885122, 1.0664169316612833, 1.0667067811211146, 1.0664092961986273, 1.0666951058533392, 1.0662671127930063, 1.0665092151153264, 1.0664709199629785, 1.0665133058143954, 1.0665007412727243, 1.0659907093189034, 1.0661602486139057, 1.0661444452595827, 1.065936682259508, 1.0656459674067882, 1.065405030947405, 1.0657668757712704, 1.0656223586823161, 1.0655764357014046, 1.065635279285888, 1.0653547289336256, 1.0655469651684188, 1.0657934770599766, 1.0654672147409474, 1.0654936547349827, 1.0658972306400294, 1.066091848515916, 1.0659289107534098, 1.0662426559012903, 1.0658600866696712, 1.0662170585936122, 1.066100952660509, 1.066248366985415, 1.0657616838054313, 1.0656877084710132, 1.0653569430161776, 1.0659047427827306, 1.065855344136556, 1.065605459933602, 1.065384309084349, 1.0653439012458563, 1.0651810924799376, 1.0653898398864445, 1.0652075242526426, 1.0654234064036403, 1.0656602727172801, 1.0649863232924237, 1.0655271478474433, 1.0652389038959746, 1.065584167666819, 1.065621106103919, 1.0654372442727802, 1.0656745572787005, 1.0650607040167246, 1.0654242199238493, 1.064889810746918, 1.0649721802553325, 1.064766048602087, 1.0647884836337838, 1.0648418338036498, 1.0654178622908193, 1.0654356886796372, 1.0652302419415052, 1.0654779700027115, 1.065128105614573, 1.0649081503816427, 1.0644802843604377, 1.064291014068428, 1.0655870063747288, 1.064701926336304, 1.06493416149628, 1.0642330204129022, 1.0641575429239885, 1.0637303093579797, 1.065308212255218, 1.0646155387505718, 1.0646186493496197, 1.064248263738034, 1.0640278169869986, 1.0643001914220098, 1.0642422106857174, 1.0649235277927567, 1.0642077388434574, 1.0638192506455044, 1.063298059606004, 1.064163112092292, 1.063470585592862, 1.0639832135296023, 1.0637632070112306, 1.0640485241691076, 1.0646442047676625, 1.063820047527307, 1.066929676262616, 1.0638871490465989, 1.0639188172194758, 1.0630170009014837, 1.0629751089171235, 1.0642757149556978, 1.0635445912679036, 1.0634944350848645, 1.0632765751166884, 1.062837293191105, 1.0629917729664318, 1.0632641231289441, 1.0636518510495891, 1.0634689309522631, 1.0634027304516245, 1.0639194849089448, 1.0656914495677978, 1.0659656324997324, 1.0643758478227312, 1.0633036501106174, 1.0635358486660986, 1.0630368747930417, 1.0637541839054652, 1.0637817697963496], 'val_acc': [0.39983579565347316, 0.39080459936499007, 0.4051724130590561, 0.3875205247175126, 0.3891625608423073, 0.39408867161457956, 0.4031198702520142, 0.4010673226981327, 0.3875205247175126, 0.3908045993160536, 0.38834154277990995, 0.3969622348329704, 0.3981937618286935, 0.3912151083961888, 0.38834154277990995, 0.4047619063768089, 0.40353037688532484, 0.3875205247175126, 0.4031198702520142, 0.3903940902848549, 0.3891625608423073, 0.3969622324350824, 0.3998357980024247, 0.4018883431094816, 0.40106732264919626, 0.39162561742738744, 0.4051724130590561, 0.40517241540800764, 0.40558292209025476, 0.4014778316803949, 0.4055829244392063, 0.4068144492327873, 0.3957307053414863, 0.4006568159669491, 0.39942528897122603, 0.4039408882654751, 0.4010673250960208, 0.39942528897122603, 0.39655172340388367, 0.38669950900406674, 0.3936781601854929, 0.3940886692656281, 0.39408867161457956, 0.4010673251938937, 0.3981937595286784, 0.4039408858675871, 0.3969622347350974, 0.3912151083961888, 0.3990147774932028, 0.412561575473823, 0.40106732274706924, 0.4018883432562911, 0.40024630713149634, 0.41830870450423857, 0.40065681606482206, 0.4043513948498493, 0.392857142074159, 0.38341543401403383, 0.39408867161457956, 0.3895730698245695, 0.3883415450309885, 0.38957307212458453, 0.3879310360976628, 0.38546798191047066, 0.391625617378451, 0.392857142074159, 0.38628899757498003, 0.3866995066551152, 0.3899835789047047, 0.3879310360976628, 0.3936781625344444, 0.39121510834725226, 0.39408866921669156, 0.39121510834725226, 0.3928571421230955, 0.38998357885576823, 0.396141214372685, 0.39121510594936426, 0.37931034614887144, 0.3883415427309735, 0.391625617378451, 0.3932676511053577, 0.3936781625344444, 0.39162561732951445, 0.39162561732951445, 0.39162561732951445, 0.3908045969181656, 0.39080459936499007, 0.39080459936499007, 0.3908045969181656, 0.39942528897122603, 0.39942528897122603, 0.3961412167216365, 0.39942528897122603, 0.40065681371587053, 0.39778325049747976, 0.3961412167216365, 0.3928571445209835, 0.3961412167216365, 0.3961412167216365, 0.38423645222324065, 0.3821839070672472, 0.3772577986439265, 0.38382594319204194, 0.3801313643091418, 0.3862889997771221, 0.3793103437020469, 0.391625614686944, 0.38916256319125886, 0.3973727438152326, 0.4002463070336234, 0.39203612645858615, 0.3862889997771221, 0.38752052447283014, 0.38669950636149625, 0.3928571445209835, 0.38752052447283014, 0.3936781625344444, 0.380541870991389, 0.3961412167216365, 0.3998357980024247, 0.39860427090882866, 0.3834154340629703, 0.3928571421230955, 0.38382594309416895, 0.38382594319204194, 0.385057470285638, 0.3936781625344444, 0.3834154365097948, 0.3875205268217817, 0.39860427090882866, 0.39573070769043783, 0.3883415424373545, 0.39244663544084835, 0.3953201962613511, 0.3944991802542863, 0.3953201962613511, 0.38998357856214927, 0.39819375947974195, 0.38998357856214927, 0.38669950636149625, 0.38382594319204194, 0.3768472919616793, 0.38013136416233234, 0.3883415424373545, 0.388341542486291, 0.4035303792832129, 0.38136289125592837, 0.3842364544743192, 0.4002463069846869, 0.40476190383211147, 0.3973727414173446, 0.3842364544743192, 0.38382594309416895, 0.3973727437662961, 0.38587849059911394, 0.38711001539269496, 0.38834154478630606, 0.39778325279749477, 0.3973727437662961, 0.38834154478630606, 0.3866995086615113, 0.39778325279749477, 0.40270936112294253, 0.40270936112294253, 0.3899835786110858, 0.3936781621429525, 0.39080459667348316, 0.3854679792679002, 0.3854679792679002, 0.39778325039960677, 0.3883415448352425, 0.3936781621429525, 0.39121510555787237, 0.3871100152458855, 0.3866995063125598, 0.3949096868386605, 0.394909689383358, 0.3936781621429525, 0.3936781621429525, 0.3932676511053577, 0.40558292189450884, 0.40476190383211147, 0.4043513949477222, 0.4043513949477222, 0.4059934310235805, 0.3908045964777372, 0.3871100153437585, 0.3936781621429525, 0.40188834301160864, 0.4068144491349144, 0.4043513949477222, 0.3936781621429525, 0.3924466351961659, 0.4018883407605301, 0.4055829220413183, 0.4043513949477222, 0.3924466326514684, 0.4006568159669491, 0.4059934310235805, 0.4068144491349144, 0.39778325279749477, 0.4068144491349144, 0.39778325015492433, 0.40147783402934645, 0.3936781621429525, 0.394909689383358, 0.3977832525038758, 0.4068144491349144, 0.3936781621429525, 0.39367816224082547, 0.3924466350493564, 0.3990147771995839, 0.39778325015492433, 0.40558292199238183, 0.40845648755972414, 0.394909689383358, 0.39778325015492433, 0.3928571440805551, 0.39778325015492433, 0.4031198678051897, 0.39778325015492433, 0.4068144491349144, 0.3936781621429525, 0.3936781621429525, 0.41091954169797973, 0.39490968918761205, 0.3990147771995839, 0.3932676507628023, 0.39778325015492433, 0.40722495806824005, 0.39121510550893585, 0.3924466350493564, 0.3924466326514684, 0.39778325035067025, 0.39490968918761205, 0.3990147771995839, 0.39490968918761205, 0.39367815984293747, 0.3961412164280176, 0.3953201983166837, 0.3953201983166837, 0.3961412164280176, 0.40804597617957394, 0.3961412164280176, 0.3990147771995839, 0.39983579531091773, 0.39983579531091773, 0.3977832525038758, 0.41050903266678107, 0.3924466326514684, 0.39326765071386577, 0.40804597617957394, 0.393678162094016, 0.39244663260253193, 0.4096880146043837, 0.41050903266678107, 0.4100985212866309, 0.40804597617957394, 0.41050903031782954, 0.4100985235866459, 0.3932676506649293, 0.3953201983166837, 0.41091954169797973, 0.40804597617957394, 0.4031198676583802, 0.4059934333235955, 0.40722495806824005, 0.409277503175297, 0.4059934309257075, 0.409277503175297, 0.40353037688532484, 0.3932676506649293, 0.4064039424037307, 0.4043513972477373, 0.39778325245493934, 0.4010673226981327, 0.3932676506649293, 0.4068144490370414, 0.39778325245493934, 0.39121510790682384, 0.40804597617957394, 0.4064039424037307, 0.38875205137068025, 0.4092775032242335, 0.39244663260253193, 0.4064039424037307, 0.4068144514349294, 0.4100985212866309, 0.39901477964640836, 0.4088669965909228, 0.3920361259692212, 0.40558292199238183, 0.4031198678541262, 0.4088669965909228, 0.4064039424037307, 0.4039408857207776, 0.4051724129611831, 0.40188834315841815, 0.3928571440805551], 'loss': [1.078280114099475, 1.0747629599894342, 1.074514241189193, 1.0747551357476863, 1.0745742716094062, 1.0745997040668307, 1.0744326474485457, 1.0743158517676947, 1.0737683280537504, 1.0741118633282014, 1.0738693319062187, 1.0737407254487337, 1.0738081954098335, 1.0737278201986387, 1.0740961298071139, 1.0735071194000558, 1.073615972364218, 1.0734046385273552, 1.0734824300546666, 1.0734041029422925, 1.0738663465335383, 1.0737375234431554, 1.0737644826117483, 1.0734420160982887, 1.0736805450255376, 1.0733423360319354, 1.073756540923148, 1.0731857213151528, 1.0735094890457404, 1.0732357411413957, 1.0743927900306498, 1.0736486609466755, 1.073622179520938, 1.0733923849873475, 1.0736750188059876, 1.0730100841737626, 1.0735096276418383, 1.0736283658221517, 1.0728325356203428, 1.0732129441149671, 1.0731032609450009, 1.0728387577577783, 1.0731096802306126, 1.0731192481101661, 1.0734121659942721, 1.0731968879699707, 1.0729784153570139, 1.073542303128409, 1.0728162337132792, 1.0727802695190147, 1.07261044641295, 1.0722543681426704, 1.0726512414474016, 1.0730694294465395, 1.0724967472851887, 1.0722461297037176, 1.0726757975329608, 1.0734696983556729, 1.0741068548735162, 1.074556077628165, 1.0737602535459294, 1.0730399989494308, 1.0736707806342436, 1.0741727261083083, 1.072526114187691, 1.0727022082409086, 1.072499997904658, 1.0724365889903702, 1.072741031940468, 1.0728091089632477, 1.0728122825005706, 1.0727339506638858, 1.0725941173839373, 1.072682523482634, 1.072222969517326, 1.0720644224840512, 1.0720041796167283, 1.072841204461131, 1.0718240522995621, 1.0723331841600015, 1.0733996453471253, 1.0725530327223165, 1.0726896387840443, 1.0721842587116563, 1.0719350677251327, 1.072220890869595, 1.0720183245210433, 1.0722141357417958, 1.0720966518782002, 1.0724492607175447, 1.072218634852149, 1.0717518820165364, 1.071604549958231, 1.0716841562083126, 1.0720159747517328, 1.0721099653283184, 1.0722088769231244, 1.072535155000628, 1.0713713152452662, 1.0713832645200851, 1.0716879055485344, 1.0714337228015218, 1.0716943880371, 1.0710985234141106, 1.0711380299601467, 1.0719735443714462, 1.0720396384566226, 1.0740611542911256, 1.0725627680823544, 1.0714009335887995, 1.0713727324650273, 1.0718009672615318, 1.0711854916578445, 1.0712189392877065, 1.0715864556526011, 1.0723616349378895, 1.0718124530153843, 1.0714971413602574, 1.0712679296793144, 1.070932552824275, 1.0712562367656637, 1.0712052765323397, 1.07087457914372, 1.0713713718880373, 1.0705277709745529, 1.0715745414306985, 1.0703037446039658, 1.0708496681718611, 1.0701695263508164, 1.0710868054591656, 1.0701594855506318, 1.0702938982103885, 1.0697437411461033, 1.070208969400159, 1.069386233196611, 1.0706068995062576, 1.0698198299388377, 1.0700728160399922, 1.0698390810396636, 1.0707606235813556, 1.0703709532837604, 1.070117843518267, 1.0698445818017885, 1.0698212432665501, 1.0697878925707307, 1.0688760958902646, 1.0690011159595278, 1.0689056920319857, 1.0693602934999877, 1.0689060673331823, 1.069864628545068, 1.0691962822262022, 1.069404041081728, 1.0693343765681773, 1.0691996564365756, 1.0701941446601977, 1.06910657329481, 1.0688135754645973, 1.068699781850623, 1.068651824075828, 1.069053086558896, 1.0687417809968123, 1.0690444097381842, 1.068490659676538, 1.068482294635851, 1.0684032516557822, 1.0685506613592348, 1.0686054520048889, 1.0680531708856382, 1.0682340824138947, 1.068451130953168, 1.0685142352106145, 1.068202535915179, 1.0685611929002483, 1.06793165740536, 1.0688451627931066, 1.0684814825684628, 1.0679268120740228, 1.0677696102698482, 1.0682452966055587, 1.0688343674984802, 1.0688851440222111, 1.0694380454214203, 1.0680493673259963, 1.068802431378766, 1.0680739329825681, 1.0682504836049167, 1.0677730348320713, 1.0684453513832797, 1.0680949687468198, 1.0677782161524654, 1.0680901699731973, 1.0678871376803278, 1.0692001936127273, 1.0676152462832003, 1.0676337699870555, 1.067687213347433, 1.0679464267509429, 1.0676555307983617, 1.067359450808296, 1.0673935825085492, 1.0673948731510545, 1.0672809753085064, 1.0678113338149304, 1.067249325609305, 1.0679952812880216, 1.067432018961505, 1.0670408704197627, 1.0670801743344849, 1.0673428749156928, 1.0679789505455284, 1.06730342670143, 1.068566481337655, 1.0690420484885543, 1.0683679360873402, 1.068611268243261, 1.0673877942243886, 1.067047446709149, 1.0676501582045819, 1.06880435033011, 1.0685974023670142, 1.066988691901769, 1.066800599666102, 1.0666381894685404, 1.067196568767148, 1.0667650807809537, 1.0667148343836257, 1.0666061643702294, 1.066789663058287, 1.0674903034919097, 1.0669099690243449, 1.067230610534151, 1.0676976876092397, 1.0701432532108786, 1.0672876491194143, 1.0666176496100377, 1.0672806362351843, 1.0666535093554237, 1.0665794646225915, 1.0662341143316312, 1.0662724925262483, 1.0668975873404705, 1.0662001498670794, 1.065942691679608, 1.0659189750526474, 1.0664342874374233, 1.0665773595383035, 1.065932508685016, 1.0662703239942233, 1.0658933357536426, 1.0655548010029097, 1.0662571252004323, 1.0660258581506152, 1.0653487057166913, 1.0667154705989532, 1.0656552773236738, 1.065741281832513, 1.0651894762775491, 1.065416323183988, 1.0657124237358202, 1.0653125479480814, 1.0654829387057734, 1.0646938503645285, 1.0650848202636845, 1.0654071631617614, 1.0651184454590878, 1.0652613345847237, 1.0653201299526363, 1.0641311186295026, 1.0646495133210012, 1.0643778670250268, 1.0648360127541074, 1.0645324077939107, 1.0649100646835579, 1.064745833496783, 1.064983939316728, 1.0660482844777666, 1.0656697061272373, 1.0649402011346523, 1.063700097787062, 1.064158713866553, 1.0649447668749203, 1.0645012404639618, 1.0647062965487064, 1.0641583877912046, 1.0636380019863527, 1.0645681075736482, 1.0640753725226166, 1.0641107445869602, 1.0635049261841196, 1.0633880927577402, 1.063953605202434, 1.064635971635274, 1.0655004065384366, 1.0636095642553953, 1.0632574238081978, 1.0634851326443087, 1.0639547831223974, 1.0645165520771818, 1.062812769878082], 'acc': [0.3775154004106776, 0.39609856262833676, 0.39425051335926176, 0.39168377823408623, 0.3997946611909651, 0.39476386036960986, 0.3950718685892818, 0.400718685819383, 0.39784394252961175, 0.39455852156057497, 0.39435318273930087, 0.398665297729034, 0.39815195073092496, 0.40112936344969197, 0.39404517453798765, 0.4009240246406571, 0.39630390143737165, 0.39917864475162124, 0.3995893223819302, 0.397125256649033, 0.39445585214381834, 0.39671457904320234, 0.39876796712131224, 0.39938398358513444, 0.39445585214381834, 0.3982546201476816, 0.3990759753593429, 0.40061601643934386, 0.39928131418061696, 0.3996919917925672, 0.3985626283489948, 0.3922997946673105, 0.3952772074044363, 0.3987679671335514, 0.3975359343038203, 0.4012320328664486, 0.39897330595482544, 0.39712525668575044, 0.3988706365258297, 0.39804928132640754, 0.39599589321158013, 0.4003080081890741, 0.3974332648993028, 0.3976386036838594, 0.3975359343038203, 0.39907597534710376, 0.3978439424806552, 0.3976386036716203, 0.4007186858316222, 0.40174537986455755, 0.3994866529774127, 0.40030800821355234, 0.399589322369691, 0.4020533880903491, 0.40277207392197123, 0.4027720739342104, 0.40523613963650973, 0.3994866529774127, 0.39024640657084186, 0.39414784394862473, 0.39609856262833676, 0.39763860369609855, 0.39825462013544244, 0.3948665297741273, 0.3972279260902679, 0.3962012320206151, 0.3972279260902679, 0.39640657082965, 0.3952772073921971, 0.3998973306077217, 0.39650924027088486, 0.3979466119096509, 0.39661190963868487, 0.3974332648931832, 0.3997946611787259, 0.3970225872567547, 0.3975359342671028, 0.39784394249289434, 0.3978439425051335, 0.3960985626160976, 0.3968172484599589, 0.3964065708174108, 0.398665297729034, 0.3988706365625472, 0.3985626283489948, 0.3979466119096509, 0.39804928132640754, 0.39845995894447733, 0.3986652977657514, 0.39856262833675565, 0.39999999998776087, 0.4006160164271047, 0.40308008211104535, 0.4011293634619311, 0.40462012320328544, 0.40308008214776275, 0.3914784394372905, 0.39856262833675565, 0.40112936344969197, 0.39825462013544244, 0.3987679671335514, 0.3950718685709231, 0.4014373716754835, 0.3979466118851726, 0.39517453801215796, 0.3949691991908839, 0.3973305954825462, 0.377926078034867, 0.39466119095285324, 0.39702258728123296, 0.4033880903368368, 0.40154004108000096, 0.3991786447638604, 0.39589322381930186, 0.40164271049675754, 0.3971252566735113, 0.39907597534710376, 0.39938398356065613, 0.40010266938003913, 0.40041067760583066, 0.39476386036960986, 0.4020533881025882, 0.40123203285420944, 0.3970225872751134, 0.3953798767967146, 0.39589322381930186, 0.4006160164271047, 0.40133470224648776, 0.4008213552239005, 0.3997946612032042, 0.40256673511293634, 0.4013347022587269, 0.4026694045174538, 0.4014373716510052, 0.39691991785223724, 0.4001026693922783, 0.4004106776180698, 0.40677618067367366, 0.3994866529774127, 0.4079055441723222, 0.4002053387845566, 0.39579055440254524, 0.3988706365625472, 0.39733059549478533, 0.40030800821355234, 0.39928131414389956, 0.40554414785618165, 0.40164271047839883, 0.3978439424806552, 0.4008213552239005, 0.39989733059548255, 0.4001026694167566, 0.40071868584386133, 0.3999999999755217, 0.40215605749486655, 0.39876796714579055, 0.3995893223819302, 0.40143737168772264, 0.4009240246284179, 0.4012320328664486, 0.403901437374723, 0.39989733057100424, 0.3990759753593429, 0.40164271047839883, 0.40595482543753403, 0.40061601643016453, 0.40030800821355234, 0.4014373716387661, 0.39907597536546247, 0.40400410676394155, 0.40379876795490666, 0.4008213552239005, 0.40112936343745287, 0.402772073897493, 0.40041067760583066, 0.3956878850163865, 0.39897330594258634, 0.40359342916423047, 0.40256673511293634, 0.40123203287868775, 0.39958932235745187, 0.4031827515400411, 0.3953798767722363, 0.4002053387845566, 0.40287474333260825, 0.4001026694167566, 0.4033880903368368, 0.3979466119096509, 0.4023613963161406, 0.40010266940451744, 0.407700205338809, 0.40338809032459766, 0.4071868583039826, 0.39404517453798765, 0.4042094455852156, 0.40564681722398166, 0.4065708419013562, 0.40154004105552266, 0.40513347023811186, 0.4042094455852156, 0.40215605749486655, 0.40544147841494677, 0.4029774127310062, 0.40626283366332555, 0.39702258728123296, 0.40082135521166135, 0.40503080084583354, 0.40082135524837875, 0.40338809032459766, 0.4015400410677618, 0.40112936344969197, 0.4043121149897331, 0.3964065708541283, 0.40359342915811086, 0.40811088295687886, 0.4057494866529774, 0.3984599589567165, 0.40462012320328544, 0.4054414784394251, 0.394147843930266, 0.40400410678841986, 0.4001026693922783, 0.40359342917035, 0.40236139632837975, 0.40020533879679576, 0.40626283366332555, 0.40154004108000096, 0.40215605749486655, 0.4, 0.4020533880964686, 0.4068788501026694, 0.40297741274324533, 0.40174537986455755, 0.39517453798767965, 0.4019507186919512, 0.4004106776425481, 0.3981519507248054, 0.4033880903735543, 0.40410677615621987, 0.40297741270652787, 0.4001026693922783, 0.4015400410922401, 0.4042094455852156, 0.4067761806859128, 0.4087268993962227, 0.4071868583284609, 0.4020533880903491, 0.4063655030556038, 0.4009240246284179, 0.4044147843820114, 0.4048254620153801, 0.3997946611787259, 0.4054414784516642, 0.4042094455882754, 0.3973305954580679, 0.4004106775935915, 0.4073921971374958, 0.40492813140459866, 0.4039014373594241, 0.40749486651753497, 0.40595482546201234, 0.40359342914587176, 0.40708418891170434, 0.40102669404517455, 0.4063655030800821, 0.40646817249071915, 0.40472279260780286, 0.4069815194949477, 0.40780287475556565, 0.40041067760583066, 0.4034907597413543, 0.4050308008335944, 0.4044147843820114, 0.398665297729034, 0.4057494866529774, 0.4031827515400411, 0.40749486651753497, 0.4072895276962609, 0.4016427104845184, 0.40646817247236045, 0.39455852156057497, 0.4030800821355236, 0.40646817247236045, 0.4027720739342104, 0.4065708419013562, 0.40492813141683776, 0.40082135523613965, 0.4087268993839836, 0.4057494866529774, 0.40184804925683587, 0.40472279261392247, 0.4028747433020104, 0.40544147841494677, 0.40728952772073923, 0.40677618069815197, 0.40277207392197123, 0.4042094455913352, 0.39507186858316223, 0.4057494866529774, 0.407700205338809]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
