{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf11.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 07:35:58 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '3', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['05', '03', '02', '04', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000018AB190BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000018AAA166EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6199, Accuracy:0.1881, Validation Loss:1.6110, Validation Accuracy:0.1938\n",
    "Epoch #2: Loss:1.6089, Accuracy:0.2115, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6065, Accuracy:0.2329, Validation Loss:1.6064, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6065, Accuracy:0.2329, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6027, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6022, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6020, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6016, Validation Accuracy:0.2332\n",
    "Epoch #20: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6013, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6019, Accuracy:0.2329, Validation Loss:1.6007, Validation Accuracy:0.2332\n",
    "Epoch #23: Loss:1.6016, Accuracy:0.2337, Validation Loss:1.6004, Validation Accuracy:0.2463\n",
    "Epoch #24: Loss:1.6013, Accuracy:0.2390, Validation Loss:1.6002, Validation Accuracy:0.2447\n",
    "Epoch #25: Loss:1.6013, Accuracy:0.2423, Validation Loss:1.6000, Validation Accuracy:0.2463\n",
    "Epoch #26: Loss:1.6012, Accuracy:0.2382, Validation Loss:1.5997, Validation Accuracy:0.2447\n",
    "Epoch #27: Loss:1.6006, Accuracy:0.2407, Validation Loss:1.5997, Validation Accuracy:0.2430\n",
    "Epoch #28: Loss:1.6006, Accuracy:0.2427, Validation Loss:1.6001, Validation Accuracy:0.2381\n",
    "Epoch #29: Loss:1.6004, Accuracy:0.2431, Validation Loss:1.5996, Validation Accuracy:0.2365\n",
    "Epoch #30: Loss:1.6001, Accuracy:0.2435, Validation Loss:1.5993, Validation Accuracy:0.2414\n",
    "Epoch #31: Loss:1.6002, Accuracy:0.2427, Validation Loss:1.5992, Validation Accuracy:0.2365\n",
    "Epoch #32: Loss:1.6000, Accuracy:0.2435, Validation Loss:1.5992, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.5999, Accuracy:0.2435, Validation Loss:1.5994, Validation Accuracy:0.2348\n",
    "Epoch #34: Loss:1.5996, Accuracy:0.2419, Validation Loss:1.5993, Validation Accuracy:0.2365\n",
    "Epoch #35: Loss:1.5995, Accuracy:0.2427, Validation Loss:1.5992, Validation Accuracy:0.2348\n",
    "Epoch #36: Loss:1.5994, Accuracy:0.2435, Validation Loss:1.5990, Validation Accuracy:0.2414\n",
    "Epoch #37: Loss:1.5992, Accuracy:0.2444, Validation Loss:1.5994, Validation Accuracy:0.2348\n",
    "Epoch #38: Loss:1.5994, Accuracy:0.2423, Validation Loss:1.5991, Validation Accuracy:0.2414\n",
    "Epoch #39: Loss:1.5989, Accuracy:0.2419, Validation Loss:1.5993, Validation Accuracy:0.2348\n",
    "Epoch #40: Loss:1.5988, Accuracy:0.2415, Validation Loss:1.5994, Validation Accuracy:0.2365\n",
    "Epoch #41: Loss:1.5988, Accuracy:0.2444, Validation Loss:1.5991, Validation Accuracy:0.2282\n",
    "Epoch #42: Loss:1.5985, Accuracy:0.2476, Validation Loss:1.5992, Validation Accuracy:0.2299\n",
    "Epoch #43: Loss:1.5984, Accuracy:0.2431, Validation Loss:1.5995, Validation Accuracy:0.2365\n",
    "Epoch #44: Loss:1.5985, Accuracy:0.2439, Validation Loss:1.5992, Validation Accuracy:0.2348\n",
    "Epoch #45: Loss:1.5981, Accuracy:0.2452, Validation Loss:1.5993, Validation Accuracy:0.2299\n",
    "Epoch #46: Loss:1.5982, Accuracy:0.2452, Validation Loss:1.5993, Validation Accuracy:0.2315\n",
    "Epoch #47: Loss:1.5976, Accuracy:0.2439, Validation Loss:1.5990, Validation Accuracy:0.2365\n",
    "Epoch #48: Loss:1.5981, Accuracy:0.2444, Validation Loss:1.5990, Validation Accuracy:0.2365\n",
    "Epoch #49: Loss:1.5973, Accuracy:0.2439, Validation Loss:1.5997, Validation Accuracy:0.2332\n",
    "Epoch #50: Loss:1.5976, Accuracy:0.2419, Validation Loss:1.5997, Validation Accuracy:0.2365\n",
    "Epoch #51: Loss:1.5975, Accuracy:0.2431, Validation Loss:1.5994, Validation Accuracy:0.2365\n",
    "Epoch #52: Loss:1.5972, Accuracy:0.2423, Validation Loss:1.5995, Validation Accuracy:0.2381\n",
    "Epoch #53: Loss:1.5968, Accuracy:0.2435, Validation Loss:1.5993, Validation Accuracy:0.2397\n",
    "Epoch #54: Loss:1.5972, Accuracy:0.2476, Validation Loss:1.5995, Validation Accuracy:0.2381\n",
    "Epoch #55: Loss:1.5964, Accuracy:0.2460, Validation Loss:1.5994, Validation Accuracy:0.2447\n",
    "Epoch #56: Loss:1.5961, Accuracy:0.2402, Validation Loss:1.5996, Validation Accuracy:0.2348\n",
    "Epoch #57: Loss:1.5958, Accuracy:0.2431, Validation Loss:1.5994, Validation Accuracy:0.2397\n",
    "Epoch #58: Loss:1.5954, Accuracy:0.2439, Validation Loss:1.5995, Validation Accuracy:0.2463\n",
    "Epoch #59: Loss:1.5954, Accuracy:0.2439, Validation Loss:1.5998, Validation Accuracy:0.2463\n",
    "Epoch #60: Loss:1.5947, Accuracy:0.2439, Validation Loss:1.5993, Validation Accuracy:0.2381\n",
    "Epoch #61: Loss:1.5947, Accuracy:0.2415, Validation Loss:1.5995, Validation Accuracy:0.2430\n",
    "Epoch #62: Loss:1.5948, Accuracy:0.2402, Validation Loss:1.6000, Validation Accuracy:0.2430\n",
    "Epoch #63: Loss:1.5941, Accuracy:0.2476, Validation Loss:1.5990, Validation Accuracy:0.2414\n",
    "Epoch #64: Loss:1.5939, Accuracy:0.2419, Validation Loss:1.5994, Validation Accuracy:0.2381\n",
    "Epoch #65: Loss:1.5934, Accuracy:0.2411, Validation Loss:1.5995, Validation Accuracy:0.2299\n",
    "Epoch #66: Loss:1.5931, Accuracy:0.2435, Validation Loss:1.5994, Validation Accuracy:0.2365\n",
    "Epoch #67: Loss:1.5929, Accuracy:0.2431, Validation Loss:1.5993, Validation Accuracy:0.2348\n",
    "Epoch #68: Loss:1.5923, Accuracy:0.2431, Validation Loss:1.6006, Validation Accuracy:0.2414\n",
    "Epoch #69: Loss:1.5916, Accuracy:0.2431, Validation Loss:1.6005, Validation Accuracy:0.2250\n",
    "Epoch #70: Loss:1.5923, Accuracy:0.2423, Validation Loss:1.6007, Validation Accuracy:0.2381\n",
    "Epoch #71: Loss:1.5908, Accuracy:0.2439, Validation Loss:1.6003, Validation Accuracy:0.2266\n",
    "Epoch #72: Loss:1.5905, Accuracy:0.2472, Validation Loss:1.6004, Validation Accuracy:0.2266\n",
    "Epoch #73: Loss:1.5897, Accuracy:0.2472, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #74: Loss:1.5898, Accuracy:0.2468, Validation Loss:1.6018, Validation Accuracy:0.2299\n",
    "Epoch #75: Loss:1.5889, Accuracy:0.2501, Validation Loss:1.6018, Validation Accuracy:0.2266\n",
    "Epoch #76: Loss:1.5892, Accuracy:0.2489, Validation Loss:1.6016, Validation Accuracy:0.2266\n",
    "Epoch #77: Loss:1.5885, Accuracy:0.2493, Validation Loss:1.6034, Validation Accuracy:0.2315\n",
    "Epoch #78: Loss:1.5898, Accuracy:0.2501, Validation Loss:1.6030, Validation Accuracy:0.2299\n",
    "Epoch #79: Loss:1.5876, Accuracy:0.2513, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #80: Loss:1.5885, Accuracy:0.2563, Validation Loss:1.6015, Validation Accuracy:0.2430\n",
    "Epoch #81: Loss:1.5865, Accuracy:0.2620, Validation Loss:1.6025, Validation Accuracy:0.2414\n",
    "Epoch #82: Loss:1.5865, Accuracy:0.2591, Validation Loss:1.6033, Validation Accuracy:0.2365\n",
    "Epoch #83: Loss:1.5861, Accuracy:0.2563, Validation Loss:1.6041, Validation Accuracy:0.2348\n",
    "Epoch #84: Loss:1.5856, Accuracy:0.2657, Validation Loss:1.6046, Validation Accuracy:0.2348\n",
    "Epoch #85: Loss:1.5852, Accuracy:0.2665, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #86: Loss:1.5845, Accuracy:0.2678, Validation Loss:1.6049, Validation Accuracy:0.2348\n",
    "Epoch #87: Loss:1.5843, Accuracy:0.2645, Validation Loss:1.6073, Validation Accuracy:0.2282\n",
    "Epoch #88: Loss:1.5859, Accuracy:0.2710, Validation Loss:1.6060, Validation Accuracy:0.2365\n",
    "Epoch #89: Loss:1.5838, Accuracy:0.2702, Validation Loss:1.6068, Validation Accuracy:0.2381\n",
    "Epoch #90: Loss:1.5840, Accuracy:0.2698, Validation Loss:1.6064, Validation Accuracy:0.2365\n",
    "Epoch #91: Loss:1.5831, Accuracy:0.2653, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #92: Loss:1.5822, Accuracy:0.2698, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #93: Loss:1.5830, Accuracy:0.2682, Validation Loss:1.6092, Validation Accuracy:0.2299\n",
    "Epoch #94: Loss:1.5836, Accuracy:0.2669, Validation Loss:1.6088, Validation Accuracy:0.2266\n",
    "Epoch #95: Loss:1.5818, Accuracy:0.2710, Validation Loss:1.6081, Validation Accuracy:0.2299\n",
    "Epoch #96: Loss:1.5812, Accuracy:0.2690, Validation Loss:1.6078, Validation Accuracy:0.2463\n",
    "Epoch #97: Loss:1.5817, Accuracy:0.2723, Validation Loss:1.6120, Validation Accuracy:0.2282\n",
    "Epoch #98: Loss:1.5807, Accuracy:0.2702, Validation Loss:1.6139, Validation Accuracy:0.2250\n",
    "Epoch #99: Loss:1.5803, Accuracy:0.2723, Validation Loss:1.6101, Validation Accuracy:0.2282\n",
    "Epoch #100: Loss:1.5804, Accuracy:0.2698, Validation Loss:1.6097, Validation Accuracy:0.2365\n",
    "Epoch #101: Loss:1.5819, Accuracy:0.2674, Validation Loss:1.6128, Validation Accuracy:0.2217\n",
    "Epoch #102: Loss:1.5797, Accuracy:0.2727, Validation Loss:1.6123, Validation Accuracy:0.2200\n",
    "Epoch #103: Loss:1.5787, Accuracy:0.2797, Validation Loss:1.6114, Validation Accuracy:0.2315\n",
    "Epoch #104: Loss:1.5784, Accuracy:0.2743, Validation Loss:1.6126, Validation Accuracy:0.2250\n",
    "Epoch #105: Loss:1.5790, Accuracy:0.2735, Validation Loss:1.6143, Validation Accuracy:0.2233\n",
    "Epoch #106: Loss:1.5779, Accuracy:0.2710, Validation Loss:1.6138, Validation Accuracy:0.2315\n",
    "Epoch #107: Loss:1.5774, Accuracy:0.2752, Validation Loss:1.6134, Validation Accuracy:0.2332\n",
    "Epoch #108: Loss:1.5779, Accuracy:0.2739, Validation Loss:1.6182, Validation Accuracy:0.2299\n",
    "Epoch #109: Loss:1.5779, Accuracy:0.2760, Validation Loss:1.6142, Validation Accuracy:0.2233\n",
    "Epoch #110: Loss:1.5766, Accuracy:0.2813, Validation Loss:1.6154, Validation Accuracy:0.2299\n",
    "Epoch #111: Loss:1.5770, Accuracy:0.2776, Validation Loss:1.6178, Validation Accuracy:0.2250\n",
    "Epoch #112: Loss:1.5763, Accuracy:0.2727, Validation Loss:1.6172, Validation Accuracy:0.2184\n",
    "Epoch #113: Loss:1.5752, Accuracy:0.2772, Validation Loss:1.6180, Validation Accuracy:0.2233\n",
    "Epoch #114: Loss:1.5747, Accuracy:0.2780, Validation Loss:1.6163, Validation Accuracy:0.2365\n",
    "Epoch #115: Loss:1.5743, Accuracy:0.2809, Validation Loss:1.6183, Validation Accuracy:0.2299\n",
    "Epoch #116: Loss:1.5767, Accuracy:0.2743, Validation Loss:1.6189, Validation Accuracy:0.2266\n",
    "Epoch #117: Loss:1.5758, Accuracy:0.2731, Validation Loss:1.6182, Validation Accuracy:0.2315\n",
    "Epoch #118: Loss:1.5747, Accuracy:0.2760, Validation Loss:1.6175, Validation Accuracy:0.2250\n",
    "Epoch #119: Loss:1.5726, Accuracy:0.2825, Validation Loss:1.6198, Validation Accuracy:0.2250\n",
    "Epoch #120: Loss:1.5728, Accuracy:0.2731, Validation Loss:1.6185, Validation Accuracy:0.2184\n",
    "Epoch #121: Loss:1.5731, Accuracy:0.2743, Validation Loss:1.6189, Validation Accuracy:0.2250\n",
    "Epoch #122: Loss:1.5723, Accuracy:0.2805, Validation Loss:1.6211, Validation Accuracy:0.2266\n",
    "Epoch #123: Loss:1.5724, Accuracy:0.2813, Validation Loss:1.6240, Validation Accuracy:0.2167\n",
    "Epoch #124: Loss:1.5701, Accuracy:0.2825, Validation Loss:1.6211, Validation Accuracy:0.2266\n",
    "Epoch #125: Loss:1.5718, Accuracy:0.2809, Validation Loss:1.6201, Validation Accuracy:0.2266\n",
    "Epoch #126: Loss:1.5729, Accuracy:0.2776, Validation Loss:1.6235, Validation Accuracy:0.2184\n",
    "Epoch #127: Loss:1.5725, Accuracy:0.2817, Validation Loss:1.6200, Validation Accuracy:0.2332\n",
    "Epoch #128: Loss:1.5715, Accuracy:0.2789, Validation Loss:1.6223, Validation Accuracy:0.2250\n",
    "Epoch #129: Loss:1.5719, Accuracy:0.2780, Validation Loss:1.6233, Validation Accuracy:0.2250\n",
    "Epoch #130: Loss:1.5717, Accuracy:0.2838, Validation Loss:1.6186, Validation Accuracy:0.2332\n",
    "Epoch #131: Loss:1.5760, Accuracy:0.2752, Validation Loss:1.6201, Validation Accuracy:0.2217\n",
    "Epoch #132: Loss:1.5716, Accuracy:0.2776, Validation Loss:1.6255, Validation Accuracy:0.2200\n",
    "Epoch #133: Loss:1.5712, Accuracy:0.2854, Validation Loss:1.6254, Validation Accuracy:0.2102\n",
    "Epoch #134: Loss:1.5705, Accuracy:0.2887, Validation Loss:1.6182, Validation Accuracy:0.2233\n",
    "Epoch #135: Loss:1.5684, Accuracy:0.2813, Validation Loss:1.6204, Validation Accuracy:0.2348\n",
    "Epoch #136: Loss:1.5681, Accuracy:0.2834, Validation Loss:1.6225, Validation Accuracy:0.2299\n",
    "Epoch #137: Loss:1.5667, Accuracy:0.2830, Validation Loss:1.6246, Validation Accuracy:0.2365\n",
    "Epoch #138: Loss:1.5654, Accuracy:0.2879, Validation Loss:1.6210, Validation Accuracy:0.2430\n",
    "Epoch #139: Loss:1.5661, Accuracy:0.2842, Validation Loss:1.6252, Validation Accuracy:0.2200\n",
    "Epoch #140: Loss:1.5643, Accuracy:0.2871, Validation Loss:1.6238, Validation Accuracy:0.2315\n",
    "Epoch #141: Loss:1.5642, Accuracy:0.2858, Validation Loss:1.6281, Validation Accuracy:0.2151\n",
    "Epoch #142: Loss:1.5642, Accuracy:0.2895, Validation Loss:1.6246, Validation Accuracy:0.2266\n",
    "Epoch #143: Loss:1.5634, Accuracy:0.2895, Validation Loss:1.6261, Validation Accuracy:0.2217\n",
    "Epoch #144: Loss:1.5627, Accuracy:0.2871, Validation Loss:1.6271, Validation Accuracy:0.2184\n",
    "Epoch #145: Loss:1.5607, Accuracy:0.2899, Validation Loss:1.6282, Validation Accuracy:0.2151\n",
    "Epoch #146: Loss:1.5616, Accuracy:0.2854, Validation Loss:1.6294, Validation Accuracy:0.2282\n",
    "Epoch #147: Loss:1.5614, Accuracy:0.2850, Validation Loss:1.6276, Validation Accuracy:0.2184\n",
    "Epoch #148: Loss:1.5592, Accuracy:0.2920, Validation Loss:1.6290, Validation Accuracy:0.2200\n",
    "Epoch #149: Loss:1.5595, Accuracy:0.2924, Validation Loss:1.6298, Validation Accuracy:0.2332\n",
    "Epoch #150: Loss:1.5625, Accuracy:0.2920, Validation Loss:1.6288, Validation Accuracy:0.2266\n",
    "Epoch #151: Loss:1.5594, Accuracy:0.2862, Validation Loss:1.6285, Validation Accuracy:0.2167\n",
    "Epoch #152: Loss:1.5599, Accuracy:0.2949, Validation Loss:1.6358, Validation Accuracy:0.2250\n",
    "Epoch #153: Loss:1.5581, Accuracy:0.2916, Validation Loss:1.6298, Validation Accuracy:0.2184\n",
    "Epoch #154: Loss:1.5615, Accuracy:0.2895, Validation Loss:1.6339, Validation Accuracy:0.2151\n",
    "Epoch #155: Loss:1.5628, Accuracy:0.2973, Validation Loss:1.6257, Validation Accuracy:0.2299\n",
    "Epoch #156: Loss:1.5639, Accuracy:0.2887, Validation Loss:1.6332, Validation Accuracy:0.2200\n",
    "Epoch #157: Loss:1.5623, Accuracy:0.2945, Validation Loss:1.6265, Validation Accuracy:0.2282\n",
    "Epoch #158: Loss:1.5582, Accuracy:0.2945, Validation Loss:1.6278, Validation Accuracy:0.2053\n",
    "Epoch #159: Loss:1.5571, Accuracy:0.2932, Validation Loss:1.6284, Validation Accuracy:0.2085\n",
    "Epoch #160: Loss:1.5555, Accuracy:0.2990, Validation Loss:1.6264, Validation Accuracy:0.2151\n",
    "Epoch #161: Loss:1.5564, Accuracy:0.2957, Validation Loss:1.6296, Validation Accuracy:0.2020\n",
    "Epoch #162: Loss:1.5567, Accuracy:0.2949, Validation Loss:1.6326, Validation Accuracy:0.2233\n",
    "Epoch #163: Loss:1.5557, Accuracy:0.2891, Validation Loss:1.6304, Validation Accuracy:0.2250\n",
    "Epoch #164: Loss:1.5531, Accuracy:0.3039, Validation Loss:1.6329, Validation Accuracy:0.2069\n",
    "Epoch #165: Loss:1.5523, Accuracy:0.2973, Validation Loss:1.6365, Validation Accuracy:0.2151\n",
    "Epoch #166: Loss:1.5556, Accuracy:0.2977, Validation Loss:1.6333, Validation Accuracy:0.2151\n",
    "Epoch #167: Loss:1.5610, Accuracy:0.2908, Validation Loss:1.6327, Validation Accuracy:0.2167\n",
    "Epoch #168: Loss:1.5543, Accuracy:0.3055, Validation Loss:1.6359, Validation Accuracy:0.2184\n",
    "Epoch #169: Loss:1.5511, Accuracy:0.3006, Validation Loss:1.6352, Validation Accuracy:0.2085\n",
    "Epoch #170: Loss:1.5530, Accuracy:0.3018, Validation Loss:1.6364, Validation Accuracy:0.2200\n",
    "Epoch #171: Loss:1.5540, Accuracy:0.3023, Validation Loss:1.6380, Validation Accuracy:0.2036\n",
    "Epoch #172: Loss:1.5538, Accuracy:0.2994, Validation Loss:1.6366, Validation Accuracy:0.2102\n",
    "Epoch #173: Loss:1.5520, Accuracy:0.3002, Validation Loss:1.6337, Validation Accuracy:0.2184\n",
    "Epoch #174: Loss:1.5536, Accuracy:0.2994, Validation Loss:1.6406, Validation Accuracy:0.2184\n",
    "Epoch #175: Loss:1.5483, Accuracy:0.3084, Validation Loss:1.6375, Validation Accuracy:0.2135\n",
    "Epoch #176: Loss:1.5472, Accuracy:0.3051, Validation Loss:1.6395, Validation Accuracy:0.1987\n",
    "Epoch #177: Loss:1.5444, Accuracy:0.3080, Validation Loss:1.6387, Validation Accuracy:0.2233\n",
    "Epoch #178: Loss:1.5453, Accuracy:0.3170, Validation Loss:1.6445, Validation Accuracy:0.2085\n",
    "Epoch #179: Loss:1.5437, Accuracy:0.3117, Validation Loss:1.6443, Validation Accuracy:0.2118\n",
    "Epoch #180: Loss:1.5455, Accuracy:0.3129, Validation Loss:1.6433, Validation Accuracy:0.2151\n",
    "Epoch #181: Loss:1.5437, Accuracy:0.3080, Validation Loss:1.6476, Validation Accuracy:0.2151\n",
    "Epoch #182: Loss:1.5430, Accuracy:0.3105, Validation Loss:1.6473, Validation Accuracy:0.2085\n",
    "Epoch #183: Loss:1.5419, Accuracy:0.3138, Validation Loss:1.6464, Validation Accuracy:0.2053\n",
    "Epoch #184: Loss:1.5482, Accuracy:0.3076, Validation Loss:1.6484, Validation Accuracy:0.2167\n",
    "Epoch #185: Loss:1.5403, Accuracy:0.3105, Validation Loss:1.6508, Validation Accuracy:0.2167\n",
    "Epoch #186: Loss:1.5397, Accuracy:0.3150, Validation Loss:1.6525, Validation Accuracy:0.2200\n",
    "Epoch #187: Loss:1.5371, Accuracy:0.3224, Validation Loss:1.6524, Validation Accuracy:0.2118\n",
    "Epoch #188: Loss:1.5375, Accuracy:0.3138, Validation Loss:1.6518, Validation Accuracy:0.2167\n",
    "Epoch #189: Loss:1.5376, Accuracy:0.3129, Validation Loss:1.6639, Validation Accuracy:0.2003\n",
    "Epoch #190: Loss:1.5338, Accuracy:0.3179, Validation Loss:1.6551, Validation Accuracy:0.2151\n",
    "Epoch #191: Loss:1.5339, Accuracy:0.3175, Validation Loss:1.6601, Validation Accuracy:0.2200\n",
    "Epoch #192: Loss:1.5329, Accuracy:0.3175, Validation Loss:1.6595, Validation Accuracy:0.2184\n",
    "Epoch #193: Loss:1.5311, Accuracy:0.3187, Validation Loss:1.6584, Validation Accuracy:0.2118\n",
    "Epoch #194: Loss:1.5335, Accuracy:0.3162, Validation Loss:1.6636, Validation Accuracy:0.2102\n",
    "Epoch #195: Loss:1.5344, Accuracy:0.3101, Validation Loss:1.6698, Validation Accuracy:0.2020\n",
    "Epoch #196: Loss:1.5356, Accuracy:0.3068, Validation Loss:1.6763, Validation Accuracy:0.1954\n",
    "Epoch #197: Loss:1.5373, Accuracy:0.3092, Validation Loss:1.6644, Validation Accuracy:0.2118\n",
    "Epoch #198: Loss:1.5284, Accuracy:0.3179, Validation Loss:1.6629, Validation Accuracy:0.2020\n",
    "Epoch #199: Loss:1.5225, Accuracy:0.3236, Validation Loss:1.6636, Validation Accuracy:0.2135\n",
    "Epoch #200: Loss:1.5262, Accuracy:0.3253, Validation Loss:1.6640, Validation Accuracy:0.2020\n",
    "Epoch #201: Loss:1.5258, Accuracy:0.3183, Validation Loss:1.6732, Validation Accuracy:0.2102\n",
    "Epoch #202: Loss:1.5225, Accuracy:0.3232, Validation Loss:1.6634, Validation Accuracy:0.2036\n",
    "Epoch #203: Loss:1.5226, Accuracy:0.3224, Validation Loss:1.6769, Validation Accuracy:0.1987\n",
    "Epoch #204: Loss:1.5161, Accuracy:0.3326, Validation Loss:1.6734, Validation Accuracy:0.2053\n",
    "Epoch #205: Loss:1.5182, Accuracy:0.3236, Validation Loss:1.6717, Validation Accuracy:0.2184\n",
    "Epoch #206: Loss:1.5157, Accuracy:0.3257, Validation Loss:1.6898, Validation Accuracy:0.2151\n",
    "Epoch #207: Loss:1.5214, Accuracy:0.3187, Validation Loss:1.6829, Validation Accuracy:0.2020\n",
    "Epoch #208: Loss:1.5217, Accuracy:0.3244, Validation Loss:1.6752, Validation Accuracy:0.2085\n",
    "Epoch #209: Loss:1.5200, Accuracy:0.3142, Validation Loss:1.6701, Validation Accuracy:0.2020\n",
    "Epoch #210: Loss:1.5082, Accuracy:0.3318, Validation Loss:1.6804, Validation Accuracy:0.2020\n",
    "Epoch #211: Loss:1.5117, Accuracy:0.3290, Validation Loss:1.6865, Validation Accuracy:0.2085\n",
    "Epoch #212: Loss:1.5100, Accuracy:0.3392, Validation Loss:1.6870, Validation Accuracy:0.2069\n",
    "Epoch #213: Loss:1.5072, Accuracy:0.3343, Validation Loss:1.6808, Validation Accuracy:0.2167\n",
    "Epoch #214: Loss:1.5055, Accuracy:0.3347, Validation Loss:1.6849, Validation Accuracy:0.2069\n",
    "Epoch #215: Loss:1.5039, Accuracy:0.3359, Validation Loss:1.6868, Validation Accuracy:0.2102\n",
    "Epoch #216: Loss:1.5012, Accuracy:0.3347, Validation Loss:1.6909, Validation Accuracy:0.1987\n",
    "Epoch #217: Loss:1.5033, Accuracy:0.3359, Validation Loss:1.6966, Validation Accuracy:0.2053\n",
    "Epoch #218: Loss:1.4997, Accuracy:0.3314, Validation Loss:1.6998, Validation Accuracy:0.1954\n",
    "Epoch #219: Loss:1.5005, Accuracy:0.3290, Validation Loss:1.7051, Validation Accuracy:0.1905\n",
    "Epoch #220: Loss:1.5036, Accuracy:0.3343, Validation Loss:1.7046, Validation Accuracy:0.2020\n",
    "Epoch #221: Loss:1.5027, Accuracy:0.3417, Validation Loss:1.6945, Validation Accuracy:0.2020\n",
    "Epoch #222: Loss:1.4971, Accuracy:0.3281, Validation Loss:1.6960, Validation Accuracy:0.1954\n",
    "Epoch #223: Loss:1.4918, Accuracy:0.3400, Validation Loss:1.6934, Validation Accuracy:0.2151\n",
    "Epoch #224: Loss:1.4927, Accuracy:0.3405, Validation Loss:1.6969, Validation Accuracy:0.2085\n",
    "Epoch #225: Loss:1.4874, Accuracy:0.3487, Validation Loss:1.7086, Validation Accuracy:0.1938\n",
    "Epoch #226: Loss:1.4868, Accuracy:0.3409, Validation Loss:1.7027, Validation Accuracy:0.2053\n",
    "Epoch #227: Loss:1.4888, Accuracy:0.3450, Validation Loss:1.6957, Validation Accuracy:0.2135\n",
    "Epoch #228: Loss:1.4834, Accuracy:0.3458, Validation Loss:1.7038, Validation Accuracy:0.2135\n",
    "Epoch #229: Loss:1.4822, Accuracy:0.3548, Validation Loss:1.7126, Validation Accuracy:0.2036\n",
    "Epoch #230: Loss:1.4741, Accuracy:0.3589, Validation Loss:1.7037, Validation Accuracy:0.2184\n",
    "Epoch #231: Loss:1.4806, Accuracy:0.3499, Validation Loss:1.7172, Validation Accuracy:0.1987\n",
    "Epoch #232: Loss:1.4776, Accuracy:0.3507, Validation Loss:1.7150, Validation Accuracy:0.2118\n",
    "Epoch #233: Loss:1.4711, Accuracy:0.3524, Validation Loss:1.7223, Validation Accuracy:0.2151\n",
    "Epoch #234: Loss:1.4750, Accuracy:0.3487, Validation Loss:1.7169, Validation Accuracy:0.2003\n",
    "Epoch #235: Loss:1.4739, Accuracy:0.3552, Validation Loss:1.7456, Validation Accuracy:0.1970\n",
    "Epoch #236: Loss:1.4853, Accuracy:0.3441, Validation Loss:1.7339, Validation Accuracy:0.2036\n",
    "Epoch #237: Loss:1.4872, Accuracy:0.3425, Validation Loss:1.7109, Validation Accuracy:0.2299\n",
    "Epoch #238: Loss:1.4883, Accuracy:0.3405, Validation Loss:1.7047, Validation Accuracy:0.2020\n",
    "Epoch #239: Loss:1.4802, Accuracy:0.3569, Validation Loss:1.7250, Validation Accuracy:0.2102\n",
    "Epoch #240: Loss:1.4825, Accuracy:0.3458, Validation Loss:1.7175, Validation Accuracy:0.2003\n",
    "Epoch #241: Loss:1.4762, Accuracy:0.3450, Validation Loss:1.7121, Validation Accuracy:0.2020\n",
    "Epoch #242: Loss:1.4670, Accuracy:0.3565, Validation Loss:1.7070, Validation Accuracy:0.2135\n",
    "Epoch #243: Loss:1.4653, Accuracy:0.3618, Validation Loss:1.7227, Validation Accuracy:0.2299\n",
    "Epoch #244: Loss:1.4671, Accuracy:0.3606, Validation Loss:1.7261, Validation Accuracy:0.2053\n",
    "Epoch #245: Loss:1.4575, Accuracy:0.3614, Validation Loss:1.7316, Validation Accuracy:0.1970\n",
    "Epoch #246: Loss:1.4600, Accuracy:0.3663, Validation Loss:1.7357, Validation Accuracy:0.2085\n",
    "Epoch #247: Loss:1.4538, Accuracy:0.3618, Validation Loss:1.7316, Validation Accuracy:0.2036\n",
    "Epoch #248: Loss:1.4485, Accuracy:0.3766, Validation Loss:1.7374, Validation Accuracy:0.2069\n",
    "Epoch #249: Loss:1.4544, Accuracy:0.3589, Validation Loss:1.7396, Validation Accuracy:0.2036\n",
    "Epoch #250: Loss:1.4537, Accuracy:0.3581, Validation Loss:1.7381, Validation Accuracy:0.2102\n",
    "Epoch #251: Loss:1.4484, Accuracy:0.3725, Validation Loss:1.7392, Validation Accuracy:0.2200\n",
    "Epoch #252: Loss:1.4551, Accuracy:0.3708, Validation Loss:1.7376, Validation Accuracy:0.1954\n",
    "Epoch #253: Loss:1.4532, Accuracy:0.3610, Validation Loss:1.7436, Validation Accuracy:0.2118\n",
    "Epoch #254: Loss:1.4413, Accuracy:0.3737, Validation Loss:1.7442, Validation Accuracy:0.2053\n",
    "Epoch #255: Loss:1.4444, Accuracy:0.3704, Validation Loss:1.7548, Validation Accuracy:0.1938\n",
    "Epoch #256: Loss:1.4451, Accuracy:0.3741, Validation Loss:1.7486, Validation Accuracy:0.2184\n",
    "Epoch #257: Loss:1.4479, Accuracy:0.3671, Validation Loss:1.7510, Validation Accuracy:0.2151\n",
    "Epoch #258: Loss:1.4349, Accuracy:0.3749, Validation Loss:1.7516, Validation Accuracy:0.2200\n",
    "Epoch #259: Loss:1.4388, Accuracy:0.3749, Validation Loss:1.7553, Validation Accuracy:0.2085\n",
    "Epoch #260: Loss:1.4361, Accuracy:0.3721, Validation Loss:1.7498, Validation Accuracy:0.1954\n",
    "Epoch #261: Loss:1.4413, Accuracy:0.3745, Validation Loss:1.7843, Validation Accuracy:0.2069\n",
    "Epoch #262: Loss:1.4390, Accuracy:0.3745, Validation Loss:1.7583, Validation Accuracy:0.2200\n",
    "Epoch #263: Loss:1.4335, Accuracy:0.3836, Validation Loss:1.7622, Validation Accuracy:0.2003\n",
    "Epoch #264: Loss:1.4349, Accuracy:0.3819, Validation Loss:1.7756, Validation Accuracy:0.2102\n",
    "Epoch #265: Loss:1.4291, Accuracy:0.3791, Validation Loss:1.7715, Validation Accuracy:0.2200\n",
    "Epoch #266: Loss:1.4345, Accuracy:0.3836, Validation Loss:1.7601, Validation Accuracy:0.2003\n",
    "Epoch #267: Loss:1.4252, Accuracy:0.3778, Validation Loss:1.7801, Validation Accuracy:0.2020\n",
    "Epoch #268: Loss:1.4345, Accuracy:0.3864, Validation Loss:1.7672, Validation Accuracy:0.2069\n",
    "Epoch #269: Loss:1.4253, Accuracy:0.3782, Validation Loss:1.7627, Validation Accuracy:0.2184\n",
    "Epoch #270: Loss:1.4189, Accuracy:0.3860, Validation Loss:1.7670, Validation Accuracy:0.2118\n",
    "Epoch #271: Loss:1.4148, Accuracy:0.3910, Validation Loss:1.7669, Validation Accuracy:0.2085\n",
    "Epoch #272: Loss:1.4181, Accuracy:0.3906, Validation Loss:1.7726, Validation Accuracy:0.2118\n",
    "Epoch #273: Loss:1.4239, Accuracy:0.3811, Validation Loss:1.7728, Validation Accuracy:0.2250\n",
    "Epoch #274: Loss:1.4133, Accuracy:0.3901, Validation Loss:1.7768, Validation Accuracy:0.2151\n",
    "Epoch #275: Loss:1.4164, Accuracy:0.3860, Validation Loss:1.7812, Validation Accuracy:0.2003\n",
    "Epoch #276: Loss:1.4218, Accuracy:0.3774, Validation Loss:1.8025, Validation Accuracy:0.2020\n",
    "Epoch #277: Loss:1.4368, Accuracy:0.3696, Validation Loss:1.7858, Validation Accuracy:0.2200\n",
    "Epoch #278: Loss:1.4239, Accuracy:0.3881, Validation Loss:1.7679, Validation Accuracy:0.2217\n",
    "Epoch #279: Loss:1.4092, Accuracy:0.3934, Validation Loss:1.7774, Validation Accuracy:0.2036\n",
    "Epoch #280: Loss:1.4019, Accuracy:0.3893, Validation Loss:1.7769, Validation Accuracy:0.2085\n",
    "Epoch #281: Loss:1.4002, Accuracy:0.3975, Validation Loss:1.7982, Validation Accuracy:0.2036\n",
    "Epoch #282: Loss:1.3987, Accuracy:0.3971, Validation Loss:1.7966, Validation Accuracy:0.2053\n",
    "Epoch #283: Loss:1.3996, Accuracy:0.3947, Validation Loss:1.7983, Validation Accuracy:0.2135\n",
    "Epoch #284: Loss:1.3982, Accuracy:0.3963, Validation Loss:1.8006, Validation Accuracy:0.2053\n",
    "Epoch #285: Loss:1.4061, Accuracy:0.4033, Validation Loss:1.8117, Validation Accuracy:0.1987\n",
    "Epoch #286: Loss:1.3958, Accuracy:0.3992, Validation Loss:1.8004, Validation Accuracy:0.2102\n",
    "Epoch #287: Loss:1.3938, Accuracy:0.4016, Validation Loss:1.8030, Validation Accuracy:0.2053\n",
    "Epoch #288: Loss:1.3942, Accuracy:0.3955, Validation Loss:1.8202, Validation Accuracy:0.1987\n",
    "Epoch #289: Loss:1.3934, Accuracy:0.3979, Validation Loss:1.7934, Validation Accuracy:0.2102\n",
    "Epoch #290: Loss:1.3952, Accuracy:0.4000, Validation Loss:1.7978, Validation Accuracy:0.2217\n",
    "Epoch #291: Loss:1.3979, Accuracy:0.3947, Validation Loss:1.7932, Validation Accuracy:0.2053\n",
    "Epoch #292: Loss:1.3973, Accuracy:0.3996, Validation Loss:1.7868, Validation Accuracy:0.2020\n",
    "Epoch #293: Loss:1.3903, Accuracy:0.4049, Validation Loss:1.8098, Validation Accuracy:0.2053\n",
    "Epoch #294: Loss:1.3883, Accuracy:0.4037, Validation Loss:1.8029, Validation Accuracy:0.2167\n",
    "Epoch #295: Loss:1.3921, Accuracy:0.4049, Validation Loss:1.8017, Validation Accuracy:0.2151\n",
    "Epoch #296: Loss:1.3844, Accuracy:0.4000, Validation Loss:1.8029, Validation Accuracy:0.1987\n",
    "Epoch #297: Loss:1.3779, Accuracy:0.4177, Validation Loss:1.8169, Validation Accuracy:0.2069\n",
    "Epoch #298: Loss:1.3741, Accuracy:0.4119, Validation Loss:1.8095, Validation Accuracy:0.2184\n",
    "Epoch #299: Loss:1.3738, Accuracy:0.4148, Validation Loss:1.8157, Validation Accuracy:0.2003\n",
    "Epoch #300: Loss:1.3637, Accuracy:0.4214, Validation Loss:1.8268, Validation Accuracy:0.2003\n",
    "\n",
    "Test:\n",
    "Test Loss:1.82676101, Accuracy:0.2003\n",
    "Labels: ['05', '03', '02', '04', '01']\n",
    "Confusion Matrix:\n",
    "      05  03  02  04  01\n",
    "t:05  28  26  23  45  20\n",
    "t:03  28  23  10  37  17\n",
    "t:02  30  18  14  31  21\n",
    "t:04  27  21  19  39   6\n",
    "t:01  29  20  24  35  18\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          05       0.20      0.20      0.20       142\n",
    "          03       0.21      0.20      0.21       115\n",
    "          02       0.16      0.12      0.14       114\n",
    "          04       0.21      0.35      0.26       112\n",
    "          01       0.22      0.14      0.17       126\n",
    "\n",
    "    accuracy                           0.20       609\n",
    "   macro avg       0.20      0.20      0.19       609\n",
    "weighted avg       0.20      0.20      0.19       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 07:51:33 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.611008590273865, 1.6067014259266343, 1.6063984828238025, 1.6061366313001009, 1.6054216635051033, 1.6050068748799842, 1.6047186395413378, 1.6045142880018513, 1.604243512028348, 1.6040021055829152, 1.603785642066417, 1.6036174309077522, 1.6033310729686068, 1.603091894307943, 1.6027433169494905, 1.6024753530624465, 1.6021998392537309, 1.6019787980222153, 1.6016000900754004, 1.601349029047736, 1.601001817604591, 1.6006531852415238, 1.6004352469749639, 1.6001615169991805, 1.5999709328602882, 1.5996727728099853, 1.599697140050052, 1.6001486380894978, 1.5996360252251962, 1.5992881811311093, 1.5992206454472784, 1.5991660701034496, 1.5994343424861264, 1.5993034430521071, 1.5991917940587637, 1.5990409144431303, 1.5993984520728952, 1.5990970919676406, 1.5993234713872273, 1.59938462027188, 1.5991234235184142, 1.5992416605377824, 1.599470307674314, 1.5991935242573028, 1.5992807300611473, 1.5992940182756321, 1.5990438790156924, 1.5990455301328637, 1.599704828168371, 1.599736613201586, 1.5993914449547704, 1.599475603385512, 1.599280819712797, 1.5994627923996774, 1.5994115827118822, 1.5996024587080004, 1.5993660036566222, 1.5995315370105563, 1.5997683853155678, 1.5993099024730364, 1.5995231645643613, 1.6000015624051023, 1.5989914480688536, 1.5993968704455397, 1.599518227459762, 1.5993627368522982, 1.599307450950635, 1.6006374204491551, 1.6004601125842441, 1.600743727535254, 1.6003437093130277, 1.6003783029092749, 1.6006375833097937, 1.6018344038617238, 1.6018124343139197, 1.6015741181099552, 1.6034419951572012, 1.6029693929628395, 1.6039249237339288, 1.6014789840074988, 1.602530788905515, 1.6033328166736172, 1.6041313556614767, 1.6045661963069773, 1.605050991712924, 1.6049106201319077, 1.6072599429802354, 1.6059551043267712, 1.6067795097730038, 1.6064199275766884, 1.6053024283770858, 1.6076283811152667, 1.6091677678629683, 1.608785336827997, 1.6080789765700918, 1.607788159733727, 1.611977390272081, 1.6138786958356208, 1.6100945960124726, 1.6096543991702728, 1.6128448887998834, 1.6123466127611734, 1.6113937711480804, 1.6126214328462072, 1.614288262154277, 1.6138030635116527, 1.6134038053709885, 1.6181655119988327, 1.614172477048802, 1.6153783909792971, 1.6178289518763476, 1.6172470265421375, 1.6180331201976157, 1.6162672956980313, 1.6182771930945135, 1.6188885650807021, 1.6181610519271374, 1.617468477078455, 1.6198435984613078, 1.6185218602761455, 1.6189109961974797, 1.6210966388188754, 1.6240019655384257, 1.6210924778470068, 1.620065630950364, 1.6234698277976125, 1.6199696101187093, 1.6223124463374197, 1.6232542887892825, 1.618609783293187, 1.6200578762784184, 1.6254994256547324, 1.6253698895913236, 1.6181948897482334, 1.6204315662775526, 1.6225417531378359, 1.6245780351322467, 1.6209643546779364, 1.625227279655256, 1.6237846906549238, 1.6281488568129014, 1.6246435626582756, 1.6260994050303117, 1.6271288467353984, 1.628178943162677, 1.6293588076123267, 1.6276031879368673, 1.6289974002806815, 1.6298195190226112, 1.6288218723337835, 1.628532529855988, 1.6358368294971135, 1.6297976403009324, 1.6339260989613524, 1.6257154597045949, 1.633164659509518, 1.6265029807396123, 1.627765729118059, 1.6284276333152758, 1.6264105943231943, 1.629649067159944, 1.632642568821586, 1.630432187238546, 1.6329411604917303, 1.6364835453738134, 1.633290950887896, 1.6326889247925607, 1.635851546852851, 1.635177010385861, 1.6363532102753964, 1.6380195104821367, 1.6366043707420086, 1.6336843515264576, 1.6405506267140455, 1.6374847149026805, 1.639499228967626, 1.6386995648319889, 1.644477406550315, 1.6442975506602446, 1.6432738239542017, 1.6475533115844225, 1.647276686330147, 1.6463987696151232, 1.648354548147355, 1.650778006450296, 1.6525157006894817, 1.6523528230209852, 1.6518392343630737, 1.6639167939500856, 1.6551277940887927, 1.6600674200919265, 1.659529120464043, 1.6584301732835316, 1.6635561856534484, 1.6698266342159953, 1.6762972978144053, 1.6643667712391694, 1.6628633886331017, 1.6635986390372215, 1.6640378628262549, 1.6732404999349308, 1.663436254061306, 1.6769191003198107, 1.6733730408945695, 1.6716766936829917, 1.6898063380142738, 1.682943944077578, 1.6751714666880215, 1.6701163484153685, 1.6804267929496828, 1.6865147694774059, 1.6869993863630373, 1.680765005755307, 1.684904826098475, 1.686805721182737, 1.6908785766372931, 1.6966079007620098, 1.6997726672192903, 1.7051268150457999, 1.7046045354630168, 1.694452863412929, 1.6959507672853267, 1.6933962989518991, 1.696944212482872, 1.708620144620122, 1.7027124043168693, 1.6956775621044615, 1.7038060084156605, 1.7126046869359384, 1.7037337785479667, 1.71723810479363, 1.7150464429839687, 1.7222734492009104, 1.7169279077370179, 1.7456414153423216, 1.7339277674607652, 1.7109007369512799, 1.7047318065499242, 1.7250388232357983, 1.7174818633225164, 1.712066455231903, 1.7070357327782266, 1.7226950084830348, 1.7260724601682966, 1.7316454774248973, 1.7357281892757697, 1.7316261872477916, 1.7374106641669187, 1.7396420813937885, 1.738083946098052, 1.7391526107913364, 1.737636821023349, 1.743602030970193, 1.7442173679865445, 1.754757138700125, 1.74862139859223, 1.750957127862376, 1.7516089938152795, 1.755326768251867, 1.7497884810264475, 1.7842562296511897, 1.758305359356509, 1.7621848591051275, 1.7756300524537787, 1.7715377474849057, 1.7600982308583502, 1.7800575255956164, 1.7672313025040776, 1.7627192238477258, 1.7669792187037727, 1.766885670730829, 1.7725690945811656, 1.7727755570450832, 1.7767959917315905, 1.7811909994272568, 1.8024564924694242, 1.7857786117516128, 1.7678811628438764, 1.7774143258143333, 1.7768850776753793, 1.7981622671258861, 1.7966035776733373, 1.7983149123700772, 1.8005917647789265, 1.8117211720430597, 1.800434454321274, 1.802970690680255, 1.8201631470071076, 1.79337234900307, 1.7977854449956483, 1.7931605322999125, 1.7867827685595734, 1.809755578025417, 1.802930879475448, 1.8016925089073494, 1.8029488311416801, 1.8168544923926417, 1.8094991517967387, 1.815724516541304, 1.8267607806351385], 'val_acc': [0.19376026191832788, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.2463054181075057, 0.24466338208058394, 0.2463054181075057, 0.24466338198271095, 0.2430213459557892, 0.23809523748353198, 0.23645320135873724, 0.24137930973312147, 0.23645320135873724, 0.23645320135873724, 0.2348111652339425, 0.23645320135873724, 0.2348111652339425, 0.24137930973312147, 0.2348111652339425, 0.24137930973312147, 0.2348111652339425, 0.2364532014566102, 0.2282430207347635, 0.22988505685955823, 0.2364532014566102, 0.2348111652339425, 0.22988505685955823, 0.23152709308222597, 0.23645320135873724, 0.23645320126086425, 0.2331691293048937, 0.23645320155448318, 0.2364532014566102, 0.23809523758140494, 0.23973727360832672, 0.23809523767927793, 0.24466338198271095, 0.23481116503819652, 0.23973727360832672, 0.2463054181075057, 0.2463054181075057, 0.238095237385659, 0.24302134576004322, 0.24302134624940813, 0.24137930963524848, 0.23809523758140494, 0.22988505676168527, 0.23645320126086425, 0.2348111651360695, 0.24137930963524848, 0.224958948485174, 0.23809523748353198, 0.22660098460996875, 0.22660098470784173, 0.23481116533181545, 0.22988505685955823, 0.22660098470784173, 0.22660098470784173, 0.23152709298435298, 0.2298850570553042, 0.23316912910914772, 0.24302134615153514, 0.2413793099288674, 0.23645320155448318, 0.23481116533181545, 0.23481116533181545, 0.2331691292070207, 0.23481116533181545, 0.2282430207347635, 0.23645320155448318, 0.23809523758140494, 0.23645320155448318, 0.2364532014566102, 0.23645320135873724, 0.22988505685955823, 0.22660098460996875, 0.22988505685955823, 0.24630541840112463, 0.2282430207347635, 0.224958948485174, 0.2282430207347635, 0.23645320155448318, 0.22167487623558452, 0.22003284020866276, 0.23152709298435298, 0.22495894838730102, 0.22331691236037926, 0.23152709308222597, 0.2331691293048937, 0.22988505685955823, 0.22331691245825225, 0.22988505695743122, 0.22495894838730102, 0.21839080427961396, 0.22331691245825225, 0.2364532014566102, 0.22988505695743122, 0.22660098470784173, 0.2315270933758449, 0.22495894877879294, 0.22495894868091998, 0.21839080398599503, 0.22495894868091998, 0.22660098480571472, 0.21674876795907325, 0.22660098480571472, 0.22660098480571472, 0.21839080418174098, 0.23316912940276668, 0.22495894877879294, 0.22495894877879294, 0.23316912940276668, 0.22167487672494943, 0.2200328399150438, 0.21018062355776726, 0.22331691275187118, 0.23481116552756143, 0.22988505734892314, 0.23645320155448318, 0.24302134624940813, 0.2200328404044087, 0.2315270934737179, 0.21510673212789747, 0.22660098500146067, 0.22167487652920345, 0.21839080418174098, 0.21510673203002448, 0.22824302102838243, 0.21839080418174098, 0.2200328405022817, 0.23316912950063964, 0.22660098500146067, 0.21674876617289138, 0.22495894887666593, 0.2183908045732329, 0.2151067319321515, 0.22988505725105016, 0.22003284020866276, 0.2282430212241284, 0.20525451518338303, 0.20853858743297252, 0.21510673203002448, 0.20197044293379354, 0.22331691245825225, 0.22495894887666593, 0.20689655150392372, 0.2151067319321515, 0.21510673203002448, 0.2167487682526922, 0.21839080427961396, 0.2085385875308455, 0.22003284069802764, 0.20361247905858829, 0.21018062355776726, 0.21839080418174098, 0.21839080437748695, 0.21346469610097568, 0.19868637058633107, 0.2233169126539982, 0.2085385875308455, 0.211822659682562, 0.2151067319321515, 0.21510673203002448, 0.20853858743297252, 0.20525451537912898, 0.21674876815481922, 0.2167487682526922, 0.2200328404044087, 0.211822659780435, 0.2167487682526922, 0.2003284067111258, 0.2151067319321515, 0.2200328404044087, 0.21839080427961396, 0.21182265987830795, 0.2101806237535132, 0.20197044264017458, 0.19540229863036052, 0.211822659780435, 0.2019704431295395, 0.21346469590522973, 0.2019704431295395, 0.2101806237535132, 0.20361247925433423, 0.198686370977823, 0.20525451518338303, 0.2183908045732329, 0.2151067323236434, 0.20197044322741248, 0.20853858782446444, 0.20197044332528544, 0.20197044322741248, 0.20853858782446444, 0.20689655150392372, 0.2167487682526922, 0.20689655091668585, 0.21018062394925918, 0.19868636880014917, 0.20525451557487495, 0.19540229872823348, 0.19047619025597626, 0.20197044104973866, 0.20197044104973866, 0.19540229882610646, 0.21510673024384258, 0.2085385857446636, 0.19376026052363793, 0.20525451557487495, 0.21346469619884867, 0.21346469402117488, 0.2036124794500802, 0.21839080229768612, 0.19868636880014917, 0.21182265997618094, 0.21510673004809663, 0.2003284050228169, 0.19704433446153632, 0.2036124771745334, 0.22988505734892314, 0.20197044104973866, 0.2101806216737124, 0.20032840521856285, 0.2019704430316665, 0.2134646939233019, 0.2298850553669953, 0.20525451557487495, 0.19704433485302825, 0.20853858554891766, 0.2036124771745334, 0.20689654942412292, 0.2036124794500802, 0.2101806238513862, 0.22003284069802764, 0.19540229833674158, 0.21182266007405393, 0.20525451547700196, 0.19376026270131172, 0.21839080229768612, 0.21510673212789747, 0.22003283852035385, 0.20853858772659145, 0.19540229843461454, 0.2068965516017967, 0.22003284069802764, 0.20032840700474475, 0.21018062394925918, 0.22003284069802764, 0.20032840700474475, 0.20197044293379354, 0.2068965516996697, 0.21839080437748695, 0.21182266007405393, 0.2085385875308455, 0.21182265948681606, 0.22495894907241187, 0.21510673222577043, 0.20032840710261773, 0.20197044332528544, 0.22003283842248086, 0.2216748746451486, 0.20361247935220722, 0.20853858762871846, 0.20361247935220722, 0.20525451547700196, 0.2134646939233019, 0.20525451349507412, 0.19868637068420403, 0.21018062394925918, 0.20525451547700196, 0.19868637087995, 0.2101806238513862, 0.22167487662707644, 0.205254515281256, 0.20197044332528544, 0.20525451518338303, 0.2167487682526922, 0.21510673212789747, 0.19868637107569595, 0.2068965516017967, 0.2183908045732329, 0.20032840700474475, 0.2003284072004907], 'loss': [1.6198581590789545, 1.6088828544597116, 1.6065009056420296, 1.6064575645223536, 1.6059079362136872, 1.6054754499537254, 1.605112184538244, 1.6050275335566464, 1.6047688411736145, 1.6044879826187353, 1.6042290647416633, 1.6040632137772484, 1.6040471480367609, 1.6038005241867943, 1.6036450429618725, 1.6033303555521876, 1.6031452241129944, 1.6030614847030482, 1.6027969396089872, 1.6024108180275198, 1.6021300646803462, 1.6018779017842035, 1.6016466169631456, 1.6013236277402059, 1.601264294657619, 1.6012083779125488, 1.600607865547008, 1.6006324565875703, 1.6004322098021144, 1.600050039849487, 1.6002088505629397, 1.5999570033878272, 1.599902526798679, 1.5995846888857457, 1.5995221790591794, 1.5993590006348533, 1.5992306543816286, 1.5993861646378065, 1.5989056074154206, 1.5987987033640334, 1.5987987470333092, 1.5984811199763962, 1.598356517041733, 1.5984953220375264, 1.5980542009616046, 1.5981622456035576, 1.597645378357576, 1.5981014678610423, 1.5973079685313012, 1.5975943950656992, 1.5974774051740674, 1.5972132854637913, 1.596758937052388, 1.5971683397919736, 1.596351746171407, 1.5960779283080992, 1.5957516097434983, 1.5953578105452615, 1.5953649108169994, 1.5947487474711768, 1.594692621191914, 1.594820928035086, 1.5940832709384896, 1.5938561966776603, 1.5934266307270748, 1.5930569573349531, 1.5929457980259734, 1.5922991253267325, 1.5916363073815065, 1.5922999136256977, 1.5907777706455646, 1.5904952300402664, 1.5897223723742506, 1.5898261769841093, 1.5889378167275776, 1.589183510353433, 1.588492604790282, 1.5898137678111113, 1.5876240578520224, 1.5885282938974838, 1.5865286854503091, 1.586457468107251, 1.5861221516156834, 1.5856053721488623, 1.585174464640921, 1.5845370743553742, 1.584331778970832, 1.5859154659619812, 1.583753600110753, 1.5839777611609112, 1.583072942336237, 1.5822177909972486, 1.5830167140804032, 1.5836354835811826, 1.5818439504204345, 1.5812217562595188, 1.5817013010107273, 1.580679346256922, 1.580334846488749, 1.5804154415150198, 1.5819060735878758, 1.5796685076836932, 1.5787404357040686, 1.5784046579924942, 1.5790137890672782, 1.5779391882600726, 1.5773519374017109, 1.5779302463883982, 1.5778502360996034, 1.5765569427909303, 1.57699423477635, 1.576328208412233, 1.575178845951934, 1.5747252091734807, 1.5742624726873158, 1.5767143063476687, 1.5757673605266782, 1.574697605589332, 1.5725538250357218, 1.5728158964023942, 1.5731452073404677, 1.572252423954206, 1.5723505952764585, 1.5700873335773695, 1.5718346664792948, 1.5728958057427063, 1.5725226153093688, 1.571497828368044, 1.571919413660586, 1.5716880680354468, 1.576036261679945, 1.571564601138387, 1.5712389511249394, 1.5704790205926131, 1.568374706293768, 1.5680697443059337, 1.5666590719007614, 1.5654477855263305, 1.5660724302581692, 1.5642519971918032, 1.564244460031482, 1.5641783446501902, 1.563432989668797, 1.5627459265612968, 1.5606868673399, 1.5615969288275717, 1.5614131437434797, 1.5591648528218514, 1.5594999970841457, 1.5625383548912817, 1.5593948514554534, 1.55985441663182, 1.5580703080312428, 1.5615008315021741, 1.5628023617566245, 1.5639345562678344, 1.5622549381589008, 1.5581845032361008, 1.557095373729416, 1.5554590818083995, 1.5564047011996196, 1.556664414817058, 1.5557421756720886, 1.553063723194036, 1.552301435744738, 1.5555698604309582, 1.5610470944606303, 1.5543310356335962, 1.551097774309789, 1.5529741753298154, 1.5539916208882107, 1.5537544270560482, 1.551963524064489, 1.5535948537458384, 1.548271080502739, 1.547154488945399, 1.544405318187737, 1.545262141590001, 1.5437372994863523, 1.545490125219435, 1.543748871746494, 1.543048813602518, 1.541874078509744, 1.548168337075862, 1.5403466449381145, 1.5396989531096001, 1.5370975197707848, 1.5375053086564772, 1.5376370285081178, 1.5338279571376543, 1.5338597537066168, 1.532879633335607, 1.5310575634547083, 1.5334832885182124, 1.534440817529416, 1.5356365874562665, 1.5373005030825888, 1.528416044364475, 1.5225285467915466, 1.5261639235200823, 1.5258005823687606, 1.5224784627832182, 1.5226381551558477, 1.516113339948948, 1.518192806968454, 1.5157259688974651, 1.5214172004919033, 1.5216617455962258, 1.5200254798669834, 1.5082155795067977, 1.5116869727199327, 1.5099902187774314, 1.5072317340290766, 1.5054683557526043, 1.5039004008628014, 1.5012000449629046, 1.5033043010523677, 1.4996891349248083, 1.5005063071143212, 1.5036468394238356, 1.5027079512696002, 1.4971115786926457, 1.4917603105979778, 1.4927079323625663, 1.4874299330388252, 1.4867703763856046, 1.4888156611327028, 1.4833822042790281, 1.4821504451900538, 1.4740975841114898, 1.4806167852217658, 1.4776063450552845, 1.4711151027091964, 1.474958970365583, 1.47393362869717, 1.4853367473555297, 1.4872135669543758, 1.4882674608876818, 1.4802407080632705, 1.4825094310165186, 1.4762365458192765, 1.466995693525984, 1.465309119518288, 1.4670743784130966, 1.4575463775247028, 1.4599735270780214, 1.4537788678488448, 1.4485037973039694, 1.4543696821103105, 1.4536896024151749, 1.4483590612666073, 1.4550653406237186, 1.4532496565910824, 1.4412673621696612, 1.4443560729526153, 1.4451079676038676, 1.4478940293039875, 1.434873986782724, 1.4388375592182794, 1.4361362372580495, 1.4413387139475076, 1.438961201722617, 1.4334821860158713, 1.4349179688420384, 1.4291311985168613, 1.4344994004991756, 1.4251801947059084, 1.4344731758262588, 1.4252716795374971, 1.4189249751504196, 1.4148479078339844, 1.4180855567450397, 1.4238777012306072, 1.413296407938493, 1.4164143189757268, 1.4218494087763636, 1.436825499103789, 1.4238604021268213, 1.409242703146024, 1.401908232837732, 1.4001796677861615, 1.398651320439834, 1.3995770456854568, 1.3982308778919479, 1.4060834300591472, 1.3957569889953738, 1.3937524279529798, 1.3941600799070981, 1.3934025390926572, 1.395175033428341, 1.3979212767779214, 1.3973339711371389, 1.3902668265101845, 1.3883184056507243, 1.392075687903888, 1.3844121230456374, 1.3779173822128796, 1.3741127958532722, 1.373847555722544, 1.3636617406437774], 'acc': [0.18809034994495477, 0.21149897394239045, 0.23285420969036816, 0.23285420872959514, 0.2328542091028891, 0.23285420933543288, 0.23285421029620593, 0.23285421010037957, 0.23285420929871545, 0.23285420892542147, 0.23285420890706277, 0.2328542087112364, 0.23285420912124782, 0.23285421049203225, 0.2328542106694999, 0.23285420851541005, 0.23285420990455322, 0.23285420912124782, 0.2328542091028891, 0.23285421010037957, 0.23285420912124782, 0.23285420969036816, 0.23367556396573477, 0.2390143727864573, 0.2422997949059739, 0.23819301934947223, 0.24065708523535875, 0.24271047175909705, 0.2431211487713291, 0.24353182797436843, 0.24271047254240244, 0.24353182776018334, 0.24353182838437984, 0.24188911787538314, 0.24271047174073831, 0.24353182797436843, 0.24435318456293376, 0.24229979392684217, 0.24188911748373043, 0.24147843910071395, 0.2443531811971684, 0.24763860335340246, 0.24312114795130627, 0.24394250616155855, 0.2451745382324626, 0.24517453785916862, 0.24394250459494776, 0.24435318319214933, 0.24394250420329508, 0.24188911709207775, 0.24312114935880813, 0.24229979647258468, 0.2435318263893989, 0.24763860356758752, 0.24599589464356034, 0.24024640544484038, 0.2431211499646459, 0.24394250617991728, 0.24394250498660047, 0.24394250480913285, 0.241478439688193, 0.24024640722563625, 0.24763860276592342, 0.24188911750208916, 0.24106776205176444, 0.24353182777854207, 0.24312114937716686, 0.24312114935880813, 0.24312115016047225, 0.24229979570763802, 0.24394250617991728, 0.24722792710611707, 0.2472279267144644, 0.246817249506406, 0.2501026676543195, 0.2488706358159592, 0.24928131558811886, 0.25010267043260576, 0.2513347028951625, 0.25626283133788763, 0.26201232137498914, 0.25913757783431535, 0.2562628348994794, 0.2657084207025642, 0.2665297751737571, 0.26776180800960786, 0.2644763857126236, 0.2710472283666873, 0.2702258721146985, 0.26981519572666296, 0.26529774108951343, 0.26981519492499884, 0.26817248464854593, 0.2669404531651209, 0.27104722934581904, 0.26899384203877535, 0.27227926200420216, 0.2702258717230458, 0.27227925863843677, 0.2698151933216706, 0.2673511307831907, 0.2726899366481593, 0.2796714595211115, 0.27433264986200745, 0.27351129290014814, 0.27104722973747175, 0.2751540027482309, 0.2739219689148897, 0.27597535720106514, 0.2813141664317991, 0.2776180716265888, 0.2726899406014037, 0.27720739263773453, 0.2780287484613532, 0.280903492375321, 0.27433265087785663, 0.2731006148537082, 0.2759753609584832, 0.28254619987348756, 0.27310061645703637, 0.274332648491223, 0.28049281299481404, 0.2813141692100854, 0.28254620343507925, 0.28090349057616637, 0.27761807103910974, 0.28172484561647965, 0.2788501038933192, 0.2780287475005802, 0.28377823570180966, 0.275154004510668, 0.27761807103910974, 0.2854209424166709, 0.2887063637528821, 0.28131416901425904, 0.2833675555012799, 0.2829568794865383, 0.2878850104750059, 0.2841889105415932, 0.2870636526931238, 0.2858316232046797, 0.2895277187931954, 0.28952772294226614, 0.2870636570380209, 0.2899383977820496, 0.2854209424166709, 0.28501026914349814, 0.2919917864598778, 0.29240246329464215, 0.2919917882590323, 0.2862422986319422, 0.294866531585521, 0.2915811090376343, 0.2895277184015427, 0.29733059745304885, 0.2887063649278402, 0.2944558539674512, 0.2944558507975122, 0.2932238189224345, 0.2989733061996084, 0.29568788423920067, 0.29486652900306104, 0.2891170423500837, 0.30390143562146527, 0.2973305941240009, 0.29774127491200975, 0.29075975575975815, 0.30554414785618167, 0.3006160144810804, 0.3018480473352898, 0.3022587267157968, 0.2993839829976554, 0.3002053364713579, 0.2993839826060027, 0.30841889298182495, 0.3051334688673274, 0.30800821317294785, 0.31704312053794, 0.3117043115397498, 0.312936344785612, 0.30800821356460056, 0.31047227731475596, 0.3137576988100761, 0.30759753532233425, 0.3104722802521512, 0.31498973467511565, 0.32238192882625966, 0.3137577023716678, 0.3129363463522228, 0.31786447541914437, 0.3174537983885536, 0.31745379936768536, 0.3186858330052002, 0.3162217679209778, 0.31006160087164425, 0.30677618168952286, 0.3092402477895944, 0.31786447420746883, 0.3236139630512535, 0.32525667450266454, 0.31827515577878307, 0.32320328382985547, 0.32238193098034945, 0.3326488690454612, 0.32361396265960085, 0.32566735309986605, 0.3186858316344157, 0.3244353173082614, 0.3141683799897376, 0.33182751678343425, 0.328952770911203, 0.3392197135170382, 0.3342915834709849, 0.33470225850659474, 0.3359342919850007, 0.33470225733163667, 0.3359342939432642, 0.33141683779458003, 0.32895277345694557, 0.33429158249185315, 0.3416837772304762, 0.328131414891758, 0.34004106597489153, 0.34045174519628957, 0.348665295562705, 0.3408624234018385, 0.34496919700007667, 0.34579055360700073, 0.3548254612045366, 0.35893224035934745, 0.3498973306077217, 0.3507186856113175, 0.35236139827440405, 0.34866529971177573, 0.35523614179671914, 0.3441478429388951, 0.34250513446159675, 0.3404517467629004, 0.35687885246482476, 0.34579055223621624, 0.34496919876251375, 0.35646817406344955, 0.3618069812992025, 0.3605749474658614, 0.361396305284461, 0.36632443353135974, 0.3618069812992025, 0.3765913743747578, 0.3589322366019294, 0.3581108823449215, 0.37248459979738785, 0.37084189089171943, 0.3609856254755839, 0.3737166318315745, 0.37043121307782323, 0.3741273094496443, 0.36714579272074377, 0.37494866586074205, 0.37494866351082584, 0.37207392175094794, 0.37453798648023506, 0.3745379878510195, 0.3835728948610764, 0.3819301847804498, 0.37905544188233126, 0.38357289584020815, 0.37782341094966787, 0.38644763955834954, 0.37823408523868973, 0.38603696037366897, 0.39096509355294384, 0.3905544149557423, 0.381108828528461, 0.390143736713476, 0.3860369593945372, 0.37741273156916094, 0.36960985663000806, 0.38809034689740723, 0.3934291564830764, 0.3893223810856837, 0.3975359350504082, 0.39712525582901015, 0.3946611927030512, 0.3963039007886969, 0.4032854220950383, 0.3991786466609281, 0.4016427086486464, 0.3954825477433645, 0.3979466106734971, 0.4000000019337852, 0.3946611921155722, 0.3995893229082135, 0.4049281319798385, 0.40369609830560627, 0.4049281323714912, 0.4000000019337852, 0.41765913555754286, 0.4119096510953727, 0.41478439262270683, 0.42135523807341557]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
