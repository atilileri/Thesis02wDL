{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf11.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 01:54:46 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '3', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '02', '05', '04', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023FEEA28198>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023FE5286EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6080, Accuracy:0.2279, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6046, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6037, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6022, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6015, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6028, Accuracy:0.2329, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6027, Accuracy:0.2329, Validation Loss:1.6011, Validation Accuracy:0.2315\n",
    "Epoch #16: Loss:1.6025, Accuracy:0.2366, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #17: Loss:1.6026, Accuracy:0.2316, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6022, Accuracy:0.2320, Validation Loss:1.6010, Validation Accuracy:0.2447\n",
    "Epoch #19: Loss:1.6024, Accuracy:0.2415, Validation Loss:1.6010, Validation Accuracy:0.2414\n",
    "Epoch #20: Loss:1.6021, Accuracy:0.2407, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #21: Loss:1.6022, Accuracy:0.2394, Validation Loss:1.6006, Validation Accuracy:0.2381\n",
    "Epoch #22: Loss:1.6022, Accuracy:0.2402, Validation Loss:1.6006, Validation Accuracy:0.2365\n",
    "Epoch #23: Loss:1.6024, Accuracy:0.2386, Validation Loss:1.6005, Validation Accuracy:0.2447\n",
    "Epoch #24: Loss:1.6021, Accuracy:0.2419, Validation Loss:1.6001, Validation Accuracy:0.2463\n",
    "Epoch #25: Loss:1.6014, Accuracy:0.2419, Validation Loss:1.6018, Validation Accuracy:0.2397\n",
    "Epoch #26: Loss:1.6023, Accuracy:0.2361, Validation Loss:1.6001, Validation Accuracy:0.2479\n",
    "Epoch #27: Loss:1.6021, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2463\n",
    "Epoch #28: Loss:1.6021, Accuracy:0.2398, Validation Loss:1.6001, Validation Accuracy:0.2447\n",
    "Epoch #29: Loss:1.6016, Accuracy:0.2390, Validation Loss:1.5997, Validation Accuracy:0.2414\n",
    "Epoch #30: Loss:1.6016, Accuracy:0.2415, Validation Loss:1.5996, Validation Accuracy:0.2463\n",
    "Epoch #31: Loss:1.6016, Accuracy:0.2411, Validation Loss:1.5997, Validation Accuracy:0.2430\n",
    "Epoch #32: Loss:1.6016, Accuracy:0.2402, Validation Loss:1.6008, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.6018, Accuracy:0.2386, Validation Loss:1.6003, Validation Accuracy:0.2365\n",
    "Epoch #34: Loss:1.6015, Accuracy:0.2411, Validation Loss:1.6002, Validation Accuracy:0.2381\n",
    "Epoch #35: Loss:1.6017, Accuracy:0.2394, Validation Loss:1.6005, Validation Accuracy:0.2365\n",
    "Epoch #36: Loss:1.6019, Accuracy:0.2415, Validation Loss:1.6002, Validation Accuracy:0.2529\n",
    "Epoch #37: Loss:1.6019, Accuracy:0.2444, Validation Loss:1.5998, Validation Accuracy:0.2414\n",
    "Epoch #38: Loss:1.6017, Accuracy:0.2419, Validation Loss:1.5994, Validation Accuracy:0.2397\n",
    "Epoch #39: Loss:1.6019, Accuracy:0.2415, Validation Loss:1.5991, Validation Accuracy:0.2397\n",
    "Epoch #40: Loss:1.6017, Accuracy:0.2407, Validation Loss:1.5991, Validation Accuracy:0.2381\n",
    "Epoch #41: Loss:1.6018, Accuracy:0.2415, Validation Loss:1.5991, Validation Accuracy:0.2496\n",
    "Epoch #42: Loss:1.6015, Accuracy:0.2448, Validation Loss:1.5990, Validation Accuracy:0.2447\n",
    "Epoch #43: Loss:1.6015, Accuracy:0.2431, Validation Loss:1.5989, Validation Accuracy:0.2463\n",
    "Epoch #44: Loss:1.6012, Accuracy:0.2439, Validation Loss:1.5987, Validation Accuracy:0.2414\n",
    "Epoch #45: Loss:1.6013, Accuracy:0.2435, Validation Loss:1.5988, Validation Accuracy:0.2397\n",
    "Epoch #46: Loss:1.6011, Accuracy:0.2415, Validation Loss:1.6000, Validation Accuracy:0.2381\n",
    "Epoch #47: Loss:1.6024, Accuracy:0.2427, Validation Loss:1.5990, Validation Accuracy:0.2529\n",
    "Epoch #48: Loss:1.6017, Accuracy:0.2427, Validation Loss:1.5988, Validation Accuracy:0.2397\n",
    "Epoch #49: Loss:1.6017, Accuracy:0.2411, Validation Loss:1.5988, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.6015, Accuracy:0.2407, Validation Loss:1.5986, Validation Accuracy:0.2397\n",
    "Epoch #51: Loss:1.6014, Accuracy:0.2407, Validation Loss:1.5986, Validation Accuracy:0.2381\n",
    "Epoch #52: Loss:1.6013, Accuracy:0.2394, Validation Loss:1.5986, Validation Accuracy:0.2512\n",
    "Epoch #53: Loss:1.6012, Accuracy:0.2423, Validation Loss:1.5984, Validation Accuracy:0.2496\n",
    "Epoch #54: Loss:1.6012, Accuracy:0.2423, Validation Loss:1.5982, Validation Accuracy:0.2381\n",
    "Epoch #55: Loss:1.6008, Accuracy:0.2427, Validation Loss:1.5980, Validation Accuracy:0.2381\n",
    "Epoch #56: Loss:1.6006, Accuracy:0.2411, Validation Loss:1.5979, Validation Accuracy:0.2496\n",
    "Epoch #57: Loss:1.6009, Accuracy:0.2435, Validation Loss:1.5979, Validation Accuracy:0.2496\n",
    "Epoch #58: Loss:1.6002, Accuracy:0.2423, Validation Loss:1.5974, Validation Accuracy:0.2397\n",
    "Epoch #59: Loss:1.6006, Accuracy:0.2427, Validation Loss:1.5973, Validation Accuracy:0.2414\n",
    "Epoch #60: Loss:1.6007, Accuracy:0.2415, Validation Loss:1.5971, Validation Accuracy:0.2414\n",
    "Epoch #61: Loss:1.6001, Accuracy:0.2435, Validation Loss:1.5973, Validation Accuracy:0.2479\n",
    "Epoch #62: Loss:1.6006, Accuracy:0.2398, Validation Loss:1.5971, Validation Accuracy:0.2463\n",
    "Epoch #63: Loss:1.6006, Accuracy:0.2468, Validation Loss:1.5970, Validation Accuracy:0.2529\n",
    "Epoch #64: Loss:1.5999, Accuracy:0.2439, Validation Loss:1.5970, Validation Accuracy:0.2397\n",
    "Epoch #65: Loss:1.6010, Accuracy:0.2407, Validation Loss:1.5970, Validation Accuracy:0.2397\n",
    "Epoch #66: Loss:1.5997, Accuracy:0.2448, Validation Loss:1.5967, Validation Accuracy:0.2496\n",
    "Epoch #67: Loss:1.5997, Accuracy:0.2439, Validation Loss:1.5973, Validation Accuracy:0.2496\n",
    "Epoch #68: Loss:1.5999, Accuracy:0.2407, Validation Loss:1.5961, Validation Accuracy:0.2479\n",
    "Epoch #69: Loss:1.5995, Accuracy:0.2402, Validation Loss:1.5964, Validation Accuracy:0.2414\n",
    "Epoch #70: Loss:1.5997, Accuracy:0.2419, Validation Loss:1.5968, Validation Accuracy:0.2414\n",
    "Epoch #71: Loss:1.5991, Accuracy:0.2435, Validation Loss:1.5956, Validation Accuracy:0.2414\n",
    "Epoch #72: Loss:1.5992, Accuracy:0.2407, Validation Loss:1.5960, Validation Accuracy:0.2447\n",
    "Epoch #73: Loss:1.5995, Accuracy:0.2353, Validation Loss:1.5956, Validation Accuracy:0.2479\n",
    "Epoch #74: Loss:1.5988, Accuracy:0.2415, Validation Loss:1.5961, Validation Accuracy:0.2414\n",
    "Epoch #75: Loss:1.5998, Accuracy:0.2427, Validation Loss:1.5951, Validation Accuracy:0.2397\n",
    "Epoch #76: Loss:1.5986, Accuracy:0.2423, Validation Loss:1.5956, Validation Accuracy:0.2463\n",
    "Epoch #77: Loss:1.5988, Accuracy:0.2394, Validation Loss:1.5943, Validation Accuracy:0.2430\n",
    "Epoch #78: Loss:1.5993, Accuracy:0.2419, Validation Loss:1.5946, Validation Accuracy:0.2414\n",
    "Epoch #79: Loss:1.5988, Accuracy:0.2411, Validation Loss:1.5952, Validation Accuracy:0.2496\n",
    "Epoch #80: Loss:1.5989, Accuracy:0.2361, Validation Loss:1.5943, Validation Accuracy:0.2545\n",
    "Epoch #81: Loss:1.5988, Accuracy:0.2452, Validation Loss:1.5947, Validation Accuracy:0.2447\n",
    "Epoch #82: Loss:1.5995, Accuracy:0.2353, Validation Loss:1.5944, Validation Accuracy:0.2479\n",
    "Epoch #83: Loss:1.5986, Accuracy:0.2402, Validation Loss:1.5946, Validation Accuracy:0.2365\n",
    "Epoch #84: Loss:1.5997, Accuracy:0.2435, Validation Loss:1.5947, Validation Accuracy:0.2315\n",
    "Epoch #85: Loss:1.6004, Accuracy:0.2201, Validation Loss:1.5947, Validation Accuracy:0.2529\n",
    "Epoch #86: Loss:1.6005, Accuracy:0.2382, Validation Loss:1.5945, Validation Accuracy:0.2430\n",
    "Epoch #87: Loss:1.5991, Accuracy:0.2456, Validation Loss:1.5943, Validation Accuracy:0.2447\n",
    "Epoch #88: Loss:1.5994, Accuracy:0.2341, Validation Loss:1.5941, Validation Accuracy:0.2512\n",
    "Epoch #89: Loss:1.5998, Accuracy:0.2448, Validation Loss:1.5943, Validation Accuracy:0.2348\n",
    "Epoch #90: Loss:1.5991, Accuracy:0.2464, Validation Loss:1.5942, Validation Accuracy:0.2397\n",
    "Epoch #91: Loss:1.5991, Accuracy:0.2398, Validation Loss:1.5944, Validation Accuracy:0.2381\n",
    "Epoch #92: Loss:1.5993, Accuracy:0.2357, Validation Loss:1.5943, Validation Accuracy:0.2348\n",
    "Epoch #93: Loss:1.5996, Accuracy:0.2456, Validation Loss:1.5934, Validation Accuracy:0.2348\n",
    "Epoch #94: Loss:1.6007, Accuracy:0.2394, Validation Loss:1.5972, Validation Accuracy:0.2365\n",
    "Epoch #95: Loss:1.5998, Accuracy:0.2419, Validation Loss:1.5976, Validation Accuracy:0.2332\n",
    "Epoch #96: Loss:1.6001, Accuracy:0.2419, Validation Loss:1.5966, Validation Accuracy:0.2299\n",
    "Epoch #97: Loss:1.5996, Accuracy:0.2390, Validation Loss:1.5959, Validation Accuracy:0.2365\n",
    "Epoch #98: Loss:1.5990, Accuracy:0.2390, Validation Loss:1.5946, Validation Accuracy:0.2479\n",
    "Epoch #99: Loss:1.5991, Accuracy:0.2497, Validation Loss:1.5948, Validation Accuracy:0.2365\n",
    "Epoch #100: Loss:1.5989, Accuracy:0.2493, Validation Loss:1.5958, Validation Accuracy:0.2348\n",
    "Epoch #101: Loss:1.5987, Accuracy:0.2439, Validation Loss:1.5941, Validation Accuracy:0.2430\n",
    "Epoch #102: Loss:1.5980, Accuracy:0.2501, Validation Loss:1.5987, Validation Accuracy:0.2348\n",
    "Epoch #103: Loss:1.5993, Accuracy:0.2468, Validation Loss:1.5960, Validation Accuracy:0.2332\n",
    "Epoch #104: Loss:1.6011, Accuracy:0.2419, Validation Loss:1.5958, Validation Accuracy:0.2348\n",
    "Epoch #105: Loss:1.6011, Accuracy:0.2349, Validation Loss:1.5985, Validation Accuracy:0.2479\n",
    "Epoch #106: Loss:1.5993, Accuracy:0.2263, Validation Loss:1.5966, Validation Accuracy:0.2332\n",
    "Epoch #107: Loss:1.5985, Accuracy:0.2480, Validation Loss:1.5963, Validation Accuracy:0.2365\n",
    "Epoch #108: Loss:1.5986, Accuracy:0.2468, Validation Loss:1.5959, Validation Accuracy:0.2381\n",
    "Epoch #109: Loss:1.5985, Accuracy:0.2476, Validation Loss:1.5961, Validation Accuracy:0.2365\n",
    "Epoch #110: Loss:1.5979, Accuracy:0.2489, Validation Loss:1.5960, Validation Accuracy:0.2381\n",
    "Epoch #111: Loss:1.5975, Accuracy:0.2501, Validation Loss:1.5959, Validation Accuracy:0.2348\n",
    "Epoch #112: Loss:1.5972, Accuracy:0.2456, Validation Loss:1.5959, Validation Accuracy:0.2365\n",
    "Epoch #113: Loss:1.5961, Accuracy:0.2472, Validation Loss:1.5950, Validation Accuracy:0.2365\n",
    "Epoch #114: Loss:1.5958, Accuracy:0.2501, Validation Loss:1.5949, Validation Accuracy:0.2381\n",
    "Epoch #115: Loss:1.5957, Accuracy:0.2534, Validation Loss:1.5951, Validation Accuracy:0.2479\n",
    "Epoch #116: Loss:1.5950, Accuracy:0.2505, Validation Loss:1.5949, Validation Accuracy:0.2365\n",
    "Epoch #117: Loss:1.5953, Accuracy:0.2480, Validation Loss:1.5952, Validation Accuracy:0.2381\n",
    "Epoch #118: Loss:1.5946, Accuracy:0.2534, Validation Loss:1.5960, Validation Accuracy:0.2447\n",
    "Epoch #119: Loss:1.5955, Accuracy:0.2460, Validation Loss:1.5957, Validation Accuracy:0.2397\n",
    "Epoch #120: Loss:1.5945, Accuracy:0.2497, Validation Loss:1.5968, Validation Accuracy:0.2332\n",
    "Epoch #121: Loss:1.5946, Accuracy:0.2530, Validation Loss:1.5935, Validation Accuracy:0.2479\n",
    "Epoch #122: Loss:1.5955, Accuracy:0.2468, Validation Loss:1.5933, Validation Accuracy:0.2381\n",
    "Epoch #123: Loss:1.5948, Accuracy:0.2456, Validation Loss:1.5939, Validation Accuracy:0.2414\n",
    "Epoch #124: Loss:1.5942, Accuracy:0.2476, Validation Loss:1.5951, Validation Accuracy:0.2430\n",
    "Epoch #125: Loss:1.5933, Accuracy:0.2509, Validation Loss:1.5953, Validation Accuracy:0.2332\n",
    "Epoch #126: Loss:1.5930, Accuracy:0.2476, Validation Loss:1.5950, Validation Accuracy:0.2496\n",
    "Epoch #127: Loss:1.5937, Accuracy:0.2464, Validation Loss:1.5940, Validation Accuracy:0.2562\n",
    "Epoch #128: Loss:1.5934, Accuracy:0.2485, Validation Loss:1.5945, Validation Accuracy:0.2496\n",
    "Epoch #129: Loss:1.5932, Accuracy:0.2509, Validation Loss:1.5955, Validation Accuracy:0.2381\n",
    "Epoch #130: Loss:1.5943, Accuracy:0.2435, Validation Loss:1.5939, Validation Accuracy:0.2447\n",
    "Epoch #131: Loss:1.5954, Accuracy:0.2407, Validation Loss:1.5930, Validation Accuracy:0.2397\n",
    "Epoch #132: Loss:1.5954, Accuracy:0.2456, Validation Loss:1.5925, Validation Accuracy:0.2365\n",
    "Epoch #133: Loss:1.5945, Accuracy:0.2419, Validation Loss:1.5934, Validation Accuracy:0.2479\n",
    "Epoch #134: Loss:1.5920, Accuracy:0.2493, Validation Loss:1.5924, Validation Accuracy:0.2365\n",
    "Epoch #135: Loss:1.5932, Accuracy:0.2534, Validation Loss:1.5919, Validation Accuracy:0.2463\n",
    "Epoch #136: Loss:1.5923, Accuracy:0.2501, Validation Loss:1.5924, Validation Accuracy:0.2463\n",
    "Epoch #137: Loss:1.5923, Accuracy:0.2517, Validation Loss:1.5951, Validation Accuracy:0.2447\n",
    "Epoch #138: Loss:1.5912, Accuracy:0.2522, Validation Loss:1.5953, Validation Accuracy:0.2365\n",
    "Epoch #139: Loss:1.5918, Accuracy:0.2489, Validation Loss:1.5966, Validation Accuracy:0.2332\n",
    "Epoch #140: Loss:1.5923, Accuracy:0.2476, Validation Loss:1.5948, Validation Accuracy:0.2282\n",
    "Epoch #141: Loss:1.5932, Accuracy:0.2501, Validation Loss:1.5949, Validation Accuracy:0.2282\n",
    "Epoch #142: Loss:1.5913, Accuracy:0.2480, Validation Loss:1.5955, Validation Accuracy:0.2381\n",
    "Epoch #143: Loss:1.5929, Accuracy:0.2448, Validation Loss:1.5937, Validation Accuracy:0.2414\n",
    "Epoch #144: Loss:1.5933, Accuracy:0.2439, Validation Loss:1.5952, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5939, Accuracy:0.2382, Validation Loss:1.5945, Validation Accuracy:0.2365\n",
    "Epoch #146: Loss:1.5934, Accuracy:0.2468, Validation Loss:1.5901, Validation Accuracy:0.2545\n",
    "Epoch #147: Loss:1.5913, Accuracy:0.2534, Validation Loss:1.5918, Validation Accuracy:0.2496\n",
    "Epoch #148: Loss:1.5923, Accuracy:0.2501, Validation Loss:1.5932, Validation Accuracy:0.2496\n",
    "Epoch #149: Loss:1.5930, Accuracy:0.2513, Validation Loss:1.5945, Validation Accuracy:0.2447\n",
    "Epoch #150: Loss:1.5932, Accuracy:0.2513, Validation Loss:1.5940, Validation Accuracy:0.2479\n",
    "Epoch #151: Loss:1.5950, Accuracy:0.2526, Validation Loss:1.5928, Validation Accuracy:0.2496\n",
    "Epoch #152: Loss:1.5916, Accuracy:0.2456, Validation Loss:1.5932, Validation Accuracy:0.2512\n",
    "Epoch #153: Loss:1.5929, Accuracy:0.2480, Validation Loss:1.5941, Validation Accuracy:0.2479\n",
    "Epoch #154: Loss:1.5912, Accuracy:0.2480, Validation Loss:1.5957, Validation Accuracy:0.2479\n",
    "Epoch #155: Loss:1.5923, Accuracy:0.2468, Validation Loss:1.5948, Validation Accuracy:0.2447\n",
    "Epoch #156: Loss:1.5948, Accuracy:0.2345, Validation Loss:1.5925, Validation Accuracy:0.2562\n",
    "Epoch #157: Loss:1.5908, Accuracy:0.2489, Validation Loss:1.5906, Validation Accuracy:0.2463\n",
    "Epoch #158: Loss:1.5919, Accuracy:0.2501, Validation Loss:1.5899, Validation Accuracy:0.2660\n",
    "Epoch #159: Loss:1.5978, Accuracy:0.2415, Validation Loss:1.6095, Validation Accuracy:0.2414\n",
    "Epoch #160: Loss:1.5985, Accuracy:0.2300, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #161: Loss:1.5987, Accuracy:0.2283, Validation Loss:1.5990, Validation Accuracy:0.2397\n",
    "Epoch #162: Loss:1.5973, Accuracy:0.2472, Validation Loss:1.5983, Validation Accuracy:0.2414\n",
    "Epoch #163: Loss:1.5953, Accuracy:0.2464, Validation Loss:1.5964, Validation Accuracy:0.2365\n",
    "Epoch #164: Loss:1.5953, Accuracy:0.2394, Validation Loss:1.5952, Validation Accuracy:0.2365\n",
    "Epoch #165: Loss:1.5931, Accuracy:0.2398, Validation Loss:1.5934, Validation Accuracy:0.2627\n",
    "Epoch #166: Loss:1.5924, Accuracy:0.2517, Validation Loss:1.5926, Validation Accuracy:0.2545\n",
    "Epoch #167: Loss:1.5926, Accuracy:0.2497, Validation Loss:1.5932, Validation Accuracy:0.2479\n",
    "Epoch #168: Loss:1.5903, Accuracy:0.2513, Validation Loss:1.5944, Validation Accuracy:0.2562\n",
    "Epoch #169: Loss:1.5895, Accuracy:0.2587, Validation Loss:1.5941, Validation Accuracy:0.2496\n",
    "Epoch #170: Loss:1.5907, Accuracy:0.2522, Validation Loss:1.5936, Validation Accuracy:0.2611\n",
    "Epoch #171: Loss:1.5923, Accuracy:0.2456, Validation Loss:1.5965, Validation Accuracy:0.2578\n",
    "Epoch #172: Loss:1.5870, Accuracy:0.2665, Validation Loss:1.5952, Validation Accuracy:0.2529\n",
    "Epoch #173: Loss:1.5886, Accuracy:0.2550, Validation Loss:1.5919, Validation Accuracy:0.2545\n",
    "Epoch #174: Loss:1.5886, Accuracy:0.2476, Validation Loss:1.5930, Validation Accuracy:0.2529\n",
    "Epoch #175: Loss:1.5890, Accuracy:0.2530, Validation Loss:1.5936, Validation Accuracy:0.2512\n",
    "Epoch #176: Loss:1.5884, Accuracy:0.2550, Validation Loss:1.5932, Validation Accuracy:0.2562\n",
    "Epoch #177: Loss:1.5881, Accuracy:0.2530, Validation Loss:1.5954, Validation Accuracy:0.2463\n",
    "Epoch #178: Loss:1.5869, Accuracy:0.2546, Validation Loss:1.5956, Validation Accuracy:0.2430\n",
    "Epoch #179: Loss:1.5873, Accuracy:0.2538, Validation Loss:1.5953, Validation Accuracy:0.2578\n",
    "Epoch #180: Loss:1.5884, Accuracy:0.2472, Validation Loss:1.5945, Validation Accuracy:0.2447\n",
    "Epoch #181: Loss:1.5905, Accuracy:0.2501, Validation Loss:1.5915, Validation Accuracy:0.2562\n",
    "Epoch #182: Loss:1.5921, Accuracy:0.2452, Validation Loss:1.5919, Validation Accuracy:0.2479\n",
    "Epoch #183: Loss:1.5901, Accuracy:0.2628, Validation Loss:1.5941, Validation Accuracy:0.2529\n",
    "Epoch #184: Loss:1.5874, Accuracy:0.2579, Validation Loss:1.5947, Validation Accuracy:0.2479\n",
    "Epoch #185: Loss:1.5864, Accuracy:0.2583, Validation Loss:1.5933, Validation Accuracy:0.2463\n",
    "Epoch #186: Loss:1.5882, Accuracy:0.2472, Validation Loss:1.5910, Validation Accuracy:0.2611\n",
    "Epoch #187: Loss:1.5865, Accuracy:0.2542, Validation Loss:1.5937, Validation Accuracy:0.2512\n",
    "Epoch #188: Loss:1.5845, Accuracy:0.2579, Validation Loss:1.5921, Validation Accuracy:0.2677\n",
    "Epoch #189: Loss:1.5858, Accuracy:0.2501, Validation Loss:1.5898, Validation Accuracy:0.2529\n",
    "Epoch #190: Loss:1.5847, Accuracy:0.2591, Validation Loss:1.5945, Validation Accuracy:0.2496\n",
    "Epoch #191: Loss:1.5843, Accuracy:0.2559, Validation Loss:1.6006, Validation Accuracy:0.2562\n",
    "Epoch #192: Loss:1.5839, Accuracy:0.2526, Validation Loss:1.5971, Validation Accuracy:0.2479\n",
    "Epoch #193: Loss:1.5850, Accuracy:0.2550, Validation Loss:1.5926, Validation Accuracy:0.2660\n",
    "Epoch #194: Loss:1.5859, Accuracy:0.2522, Validation Loss:1.5953, Validation Accuracy:0.2611\n",
    "Epoch #195: Loss:1.5869, Accuracy:0.2530, Validation Loss:1.5964, Validation Accuracy:0.2512\n",
    "Epoch #196: Loss:1.5842, Accuracy:0.2546, Validation Loss:1.5987, Validation Accuracy:0.2414\n",
    "Epoch #197: Loss:1.5838, Accuracy:0.2591, Validation Loss:1.5979, Validation Accuracy:0.2512\n",
    "Epoch #198: Loss:1.5847, Accuracy:0.2530, Validation Loss:1.5992, Validation Accuracy:0.2512\n",
    "Epoch #199: Loss:1.5844, Accuracy:0.2575, Validation Loss:1.5997, Validation Accuracy:0.2512\n",
    "Epoch #200: Loss:1.5848, Accuracy:0.2501, Validation Loss:1.5991, Validation Accuracy:0.2397\n",
    "Epoch #201: Loss:1.5822, Accuracy:0.2641, Validation Loss:1.5974, Validation Accuracy:0.2529\n",
    "Epoch #202: Loss:1.5824, Accuracy:0.2538, Validation Loss:1.5991, Validation Accuracy:0.2414\n",
    "Epoch #203: Loss:1.5811, Accuracy:0.2710, Validation Loss:1.5992, Validation Accuracy:0.2660\n",
    "Epoch #204: Loss:1.5816, Accuracy:0.2641, Validation Loss:1.5980, Validation Accuracy:0.2627\n",
    "Epoch #205: Loss:1.5823, Accuracy:0.2620, Validation Loss:1.5950, Validation Accuracy:0.2644\n",
    "Epoch #206: Loss:1.5810, Accuracy:0.2628, Validation Loss:1.5909, Validation Accuracy:0.2660\n",
    "Epoch #207: Loss:1.5812, Accuracy:0.2591, Validation Loss:1.5915, Validation Accuracy:0.2709\n",
    "Epoch #208: Loss:1.5800, Accuracy:0.2563, Validation Loss:1.5922, Validation Accuracy:0.2562\n",
    "Epoch #209: Loss:1.5787, Accuracy:0.2657, Validation Loss:1.5927, Validation Accuracy:0.2693\n",
    "Epoch #210: Loss:1.5788, Accuracy:0.2653, Validation Loss:1.5882, Validation Accuracy:0.2677\n",
    "Epoch #211: Loss:1.5785, Accuracy:0.2661, Validation Loss:1.5977, Validation Accuracy:0.2496\n",
    "Epoch #212: Loss:1.5805, Accuracy:0.2604, Validation Loss:1.5955, Validation Accuracy:0.2578\n",
    "Epoch #213: Loss:1.5813, Accuracy:0.2612, Validation Loss:1.5946, Validation Accuracy:0.2578\n",
    "Epoch #214: Loss:1.5811, Accuracy:0.2534, Validation Loss:1.5965, Validation Accuracy:0.2726\n",
    "Epoch #215: Loss:1.5804, Accuracy:0.2612, Validation Loss:1.5975, Validation Accuracy:0.2709\n",
    "Epoch #216: Loss:1.5828, Accuracy:0.2579, Validation Loss:1.6009, Validation Accuracy:0.2430\n",
    "Epoch #217: Loss:1.5798, Accuracy:0.2583, Validation Loss:1.5992, Validation Accuracy:0.2529\n",
    "Epoch #218: Loss:1.5815, Accuracy:0.2472, Validation Loss:1.5984, Validation Accuracy:0.2463\n",
    "Epoch #219: Loss:1.5813, Accuracy:0.2563, Validation Loss:1.5964, Validation Accuracy:0.2545\n",
    "Epoch #220: Loss:1.5800, Accuracy:0.2608, Validation Loss:1.5990, Validation Accuracy:0.2562\n",
    "Epoch #221: Loss:1.5774, Accuracy:0.2678, Validation Loss:1.5984, Validation Accuracy:0.2545\n",
    "Epoch #222: Loss:1.5783, Accuracy:0.2616, Validation Loss:1.5981, Validation Accuracy:0.2677\n",
    "Epoch #223: Loss:1.5789, Accuracy:0.2628, Validation Loss:1.6004, Validation Accuracy:0.2562\n",
    "Epoch #224: Loss:1.5829, Accuracy:0.2583, Validation Loss:1.5985, Validation Accuracy:0.2578\n",
    "Epoch #225: Loss:1.5778, Accuracy:0.2641, Validation Loss:1.6008, Validation Accuracy:0.2578\n",
    "Epoch #226: Loss:1.5777, Accuracy:0.2645, Validation Loss:1.6030, Validation Accuracy:0.2496\n",
    "Epoch #227: Loss:1.5765, Accuracy:0.2612, Validation Loss:1.5995, Validation Accuracy:0.2545\n",
    "Epoch #228: Loss:1.5764, Accuracy:0.2637, Validation Loss:1.5979, Validation Accuracy:0.2644\n",
    "Epoch #229: Loss:1.5753, Accuracy:0.2715, Validation Loss:1.5949, Validation Accuracy:0.2677\n",
    "Epoch #230: Loss:1.5760, Accuracy:0.2702, Validation Loss:1.5939, Validation Accuracy:0.2693\n",
    "Epoch #231: Loss:1.5752, Accuracy:0.2723, Validation Loss:1.5965, Validation Accuracy:0.2677\n",
    "Epoch #232: Loss:1.5750, Accuracy:0.2686, Validation Loss:1.5953, Validation Accuracy:0.2759\n",
    "Epoch #233: Loss:1.5756, Accuracy:0.2645, Validation Loss:1.5939, Validation Accuracy:0.2677\n",
    "Epoch #234: Loss:1.5754, Accuracy:0.2661, Validation Loss:1.5967, Validation Accuracy:0.2775\n",
    "Epoch #235: Loss:1.5766, Accuracy:0.2641, Validation Loss:1.5987, Validation Accuracy:0.2644\n",
    "Epoch #236: Loss:1.5749, Accuracy:0.2645, Validation Loss:1.5999, Validation Accuracy:0.2496\n",
    "Epoch #237: Loss:1.5732, Accuracy:0.2743, Validation Loss:1.5990, Validation Accuracy:0.2496\n",
    "Epoch #238: Loss:1.5753, Accuracy:0.2583, Validation Loss:1.5981, Validation Accuracy:0.2545\n",
    "Epoch #239: Loss:1.5721, Accuracy:0.2682, Validation Loss:1.5977, Validation Accuracy:0.2644\n",
    "Epoch #240: Loss:1.5719, Accuracy:0.2702, Validation Loss:1.5949, Validation Accuracy:0.2463\n",
    "Epoch #241: Loss:1.5732, Accuracy:0.2698, Validation Loss:1.5990, Validation Accuracy:0.2545\n",
    "Epoch #242: Loss:1.5735, Accuracy:0.2600, Validation Loss:1.5981, Validation Accuracy:0.2742\n",
    "Epoch #243: Loss:1.5744, Accuracy:0.2719, Validation Loss:1.6007, Validation Accuracy:0.2512\n",
    "Epoch #244: Loss:1.5751, Accuracy:0.2608, Validation Loss:1.5991, Validation Accuracy:0.2611\n",
    "Epoch #245: Loss:1.5724, Accuracy:0.2682, Validation Loss:1.6007, Validation Accuracy:0.2463\n",
    "Epoch #246: Loss:1.5731, Accuracy:0.2653, Validation Loss:1.6020, Validation Accuracy:0.2414\n",
    "Epoch #247: Loss:1.5700, Accuracy:0.2710, Validation Loss:1.6004, Validation Accuracy:0.2496\n",
    "Epoch #248: Loss:1.5712, Accuracy:0.2678, Validation Loss:1.5980, Validation Accuracy:0.2545\n",
    "Epoch #249: Loss:1.5735, Accuracy:0.2710, Validation Loss:1.5979, Validation Accuracy:0.2479\n",
    "Epoch #250: Loss:1.5749, Accuracy:0.2653, Validation Loss:1.5996, Validation Accuracy:0.2529\n",
    "Epoch #251: Loss:1.5698, Accuracy:0.2706, Validation Loss:1.5995, Validation Accuracy:0.2496\n",
    "Epoch #252: Loss:1.5698, Accuracy:0.2743, Validation Loss:1.5998, Validation Accuracy:0.2709\n",
    "Epoch #253: Loss:1.5696, Accuracy:0.2731, Validation Loss:1.5994, Validation Accuracy:0.2677\n",
    "Epoch #254: Loss:1.5689, Accuracy:0.2747, Validation Loss:1.5996, Validation Accuracy:0.2677\n",
    "Epoch #255: Loss:1.5708, Accuracy:0.2727, Validation Loss:1.6009, Validation Accuracy:0.2496\n",
    "Epoch #256: Loss:1.5670, Accuracy:0.2706, Validation Loss:1.6045, Validation Accuracy:0.2562\n",
    "Epoch #257: Loss:1.5670, Accuracy:0.2797, Validation Loss:1.6013, Validation Accuracy:0.2496\n",
    "Epoch #258: Loss:1.5711, Accuracy:0.2678, Validation Loss:1.5998, Validation Accuracy:0.2562\n",
    "Epoch #259: Loss:1.5686, Accuracy:0.2706, Validation Loss:1.5982, Validation Accuracy:0.2529\n",
    "Epoch #260: Loss:1.5682, Accuracy:0.2719, Validation Loss:1.6006, Validation Accuracy:0.2644\n",
    "Epoch #261: Loss:1.5662, Accuracy:0.2731, Validation Loss:1.5976, Validation Accuracy:0.2479\n",
    "Epoch #262: Loss:1.5711, Accuracy:0.2756, Validation Loss:1.5985, Validation Accuracy:0.2791\n",
    "Epoch #263: Loss:1.5717, Accuracy:0.2669, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #264: Loss:1.5703, Accuracy:0.2780, Validation Loss:1.6087, Validation Accuracy:0.2479\n",
    "Epoch #265: Loss:1.5676, Accuracy:0.2838, Validation Loss:1.6049, Validation Accuracy:0.2430\n",
    "Epoch #266: Loss:1.5680, Accuracy:0.2764, Validation Loss:1.6055, Validation Accuracy:0.2644\n",
    "Epoch #267: Loss:1.5675, Accuracy:0.2793, Validation Loss:1.6080, Validation Accuracy:0.2512\n",
    "Epoch #268: Loss:1.5661, Accuracy:0.2694, Validation Loss:1.6041, Validation Accuracy:0.2709\n",
    "Epoch #269: Loss:1.5625, Accuracy:0.2760, Validation Loss:1.6028, Validation Accuracy:0.2759\n",
    "Epoch #270: Loss:1.5636, Accuracy:0.2797, Validation Loss:1.6054, Validation Accuracy:0.2742\n",
    "Epoch #271: Loss:1.5626, Accuracy:0.2793, Validation Loss:1.6073, Validation Accuracy:0.2660\n",
    "Epoch #272: Loss:1.5640, Accuracy:0.2825, Validation Loss:1.6085, Validation Accuracy:0.2611\n",
    "Epoch #273: Loss:1.5613, Accuracy:0.2801, Validation Loss:1.6083, Validation Accuracy:0.2644\n",
    "Epoch #274: Loss:1.5613, Accuracy:0.2727, Validation Loss:1.6071, Validation Accuracy:0.2430\n",
    "Epoch #275: Loss:1.5582, Accuracy:0.2715, Validation Loss:1.6077, Validation Accuracy:0.2463\n",
    "Epoch #276: Loss:1.5599, Accuracy:0.2825, Validation Loss:1.6077, Validation Accuracy:0.2479\n",
    "Epoch #277: Loss:1.5597, Accuracy:0.2780, Validation Loss:1.6094, Validation Accuracy:0.2644\n",
    "Epoch #278: Loss:1.5605, Accuracy:0.2793, Validation Loss:1.6082, Validation Accuracy:0.2447\n",
    "Epoch #279: Loss:1.5593, Accuracy:0.2752, Validation Loss:1.6065, Validation Accuracy:0.2594\n",
    "Epoch #280: Loss:1.5580, Accuracy:0.2776, Validation Loss:1.6114, Validation Accuracy:0.2578\n",
    "Epoch #281: Loss:1.5585, Accuracy:0.2838, Validation Loss:1.6137, Validation Accuracy:0.2430\n",
    "Epoch #282: Loss:1.5594, Accuracy:0.2756, Validation Loss:1.6153, Validation Accuracy:0.2611\n",
    "Epoch #283: Loss:1.5572, Accuracy:0.2858, Validation Loss:1.6031, Validation Accuracy:0.2594\n",
    "Epoch #284: Loss:1.5588, Accuracy:0.2756, Validation Loss:1.6044, Validation Accuracy:0.2496\n",
    "Epoch #285: Loss:1.5578, Accuracy:0.2715, Validation Loss:1.6072, Validation Accuracy:0.2594\n",
    "Epoch #286: Loss:1.5571, Accuracy:0.2743, Validation Loss:1.6092, Validation Accuracy:0.2578\n",
    "Epoch #287: Loss:1.5613, Accuracy:0.2616, Validation Loss:1.6187, Validation Accuracy:0.2463\n",
    "Epoch #288: Loss:1.5629, Accuracy:0.2665, Validation Loss:1.6094, Validation Accuracy:0.2250\n",
    "Epoch #289: Loss:1.5584, Accuracy:0.2706, Validation Loss:1.6105, Validation Accuracy:0.2512\n",
    "Epoch #290: Loss:1.5634, Accuracy:0.2665, Validation Loss:1.6057, Validation Accuracy:0.2496\n",
    "Epoch #291: Loss:1.5598, Accuracy:0.2706, Validation Loss:1.6051, Validation Accuracy:0.2627\n",
    "Epoch #292: Loss:1.5612, Accuracy:0.2682, Validation Loss:1.6089, Validation Accuracy:0.2726\n",
    "Epoch #293: Loss:1.5592, Accuracy:0.2702, Validation Loss:1.6082, Validation Accuracy:0.2562\n",
    "Epoch #294: Loss:1.5589, Accuracy:0.2719, Validation Loss:1.6092, Validation Accuracy:0.2578\n",
    "Epoch #295: Loss:1.5572, Accuracy:0.2637, Validation Loss:1.6045, Validation Accuracy:0.2709\n",
    "Epoch #296: Loss:1.5536, Accuracy:0.2780, Validation Loss:1.6109, Validation Accuracy:0.2709\n",
    "Epoch #297: Loss:1.5514, Accuracy:0.2752, Validation Loss:1.6139, Validation Accuracy:0.2512\n",
    "Epoch #298: Loss:1.5572, Accuracy:0.2760, Validation Loss:1.6101, Validation Accuracy:0.2381\n",
    "Epoch #299: Loss:1.5558, Accuracy:0.2661, Validation Loss:1.6004, Validation Accuracy:0.2759\n",
    "Epoch #300: Loss:1.5537, Accuracy:0.2702, Validation Loss:1.5997, Validation Accuracy:0.2677\n",
    "\n",
    "Test:\n",
    "Test Loss:1.59974992, Accuracy:0.2677\n",
    "Labels: ['01', '02', '05', '04', '03']\n",
    "Confusion Matrix:\n",
    "      01  02  05  04  03\n",
    "t:01  37   1  53  18  17\n",
    "t:02  24   1  59  23   7\n",
    "t:05  28   0  79  15  20\n",
    "t:04  23   2  46  27  14\n",
    "t:03  30   0  51  15  19\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.26      0.29      0.28       126\n",
    "          02       0.25      0.01      0.02       114\n",
    "          05       0.27      0.56      0.37       142\n",
    "          04       0.28      0.24      0.26       112\n",
    "          03       0.25      0.17      0.20       115\n",
    "\n",
    "    accuracy                           0.27       609\n",
    "   macro avg       0.26      0.25      0.22       609\n",
    "weighted avg       0.26      0.27      0.23       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 02:10:29 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 42 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6061141530085472, 1.60481824193682, 1.6042396014155622, 1.603504747201265, 1.6036910133800288, 1.6035431560820155, 1.6033182355570677, 1.602766562369461, 1.6029179804822298, 1.6025635941666727, 1.602177757152, 1.6016909661159922, 1.601501430783953, 1.6012107695656261, 1.6011238814574744, 1.6007157057181172, 1.6013514255655223, 1.6010139673605732, 1.600950368127995, 1.6005346526457562, 1.6006168379572225, 1.6005518910137109, 1.6005055640131383, 1.6001476168828253, 1.6017681101860084, 1.600060098081191, 1.6005730352965482, 1.6000825307639361, 1.5997482884693615, 1.59964333085591, 1.5997045574517086, 1.6008485888416935, 1.6003124561215856, 1.6001743692874126, 1.600505116342128, 1.6001965494578696, 1.599760227406945, 1.5993954467851736, 1.5991095461085905, 1.599129353250776, 1.5990865207070788, 1.5990324926689536, 1.5989260824247338, 1.598723931461328, 1.5988255056058636, 1.5999577225526957, 1.5990430196909287, 1.5987587857911936, 1.5988272675152482, 1.598612943502091, 1.5985740380138402, 1.598611788405182, 1.59839949662658, 1.5982403882422862, 1.5979880386189678, 1.5978980524395094, 1.597864812622321, 1.5973790061884914, 1.5972742210272302, 1.5970817371737978, 1.597318625606731, 1.5970959790625987, 1.5969988403257673, 1.5969937898842572, 1.596989549439529, 1.596748241062822, 1.5972571249665886, 1.5961384869169915, 1.5964042281086612, 1.596839642877062, 1.5955700854754018, 1.5959844301486839, 1.5955520155786098, 1.5960926111108564, 1.5951304392665868, 1.5955877466546295, 1.5943151495139587, 1.5945602469452105, 1.5952323299323397, 1.594315226050629, 1.5946961443608225, 1.5943576995962359, 1.5945733542904281, 1.594726635904735, 1.5947336348015295, 1.5944846173616858, 1.594277433965398, 1.5941366826372194, 1.5943074400593298, 1.5941611179968798, 1.5943574993481189, 1.5942714085132617, 1.59338319360329, 1.5971705711925364, 1.5975525882248025, 1.596620807311022, 1.5958993188266097, 1.5945780112825592, 1.5948047771046705, 1.595784527131881, 1.5941294668539012, 1.5986843559346566, 1.5959773758557825, 1.5957774195960786, 1.598507266326491, 1.5965544349454306, 1.5963332149978537, 1.5959191745137933, 1.596109828338247, 1.5960122325345996, 1.5958626086292986, 1.5958812692873976, 1.5950008712965866, 1.5949167555384645, 1.5950984203169498, 1.594852913189404, 1.5951844445981807, 1.5959772135823818, 1.595679677765945, 1.5967637660663898, 1.5935194206550987, 1.5932964123724325, 1.5938647484348716, 1.595112242722159, 1.5953159590660058, 1.5950302132244767, 1.5939512503362445, 1.5944662798801665, 1.5954995155334473, 1.5939095272806478, 1.593039456063695, 1.592485155965307, 1.5933784514616667, 1.5923556890002222, 1.5919206212894084, 1.5924205292621856, 1.5951342641426425, 1.595259423522135, 1.5965659536164383, 1.5948050856003033, 1.594874012059179, 1.5955347362997496, 1.5936655473630807, 1.5951900167026738, 1.5945255247438679, 1.590128988663747, 1.5918426466692845, 1.5932485612938165, 1.5945067697362163, 1.5940143639230964, 1.5928467281150505, 1.5931934112398496, 1.594119964953518, 1.5956816360085273, 1.5948413935396668, 1.5924601114442196, 1.5906034886151894, 1.589890104992245, 1.6095102796413627, 1.6007096779170296, 1.5989962369937616, 1.5983193744775306, 1.5963613773605898, 1.59523951557078, 1.5934430991096058, 1.5926075858631352, 1.5931805691304073, 1.594447423671854, 1.5941185716337758, 1.5935527295706111, 1.5965295023910322, 1.5952226268051097, 1.59188126558545, 1.5930157234320304, 1.593630000875501, 1.5931501275017148, 1.5954222373774487, 1.595632387890996, 1.5953331780551103, 1.5945224916602199, 1.5915398022224163, 1.5919388648128665, 1.594129322197637, 1.5947202283564852, 1.5933141784714948, 1.590977342453692, 1.5937316265012242, 1.5921016434339075, 1.5898230273539602, 1.5945468641854272, 1.6005656754442037, 1.59713284331198, 1.5925902748734297, 1.595263899449253, 1.5963791293659428, 1.5986953167296787, 1.597941288219884, 1.599228038967928, 1.599663720342326, 1.59912632153735, 1.5973919735753477, 1.5990851232766714, 1.5992302307354405, 1.598017170902935, 1.5950283279951374, 1.5909462820720204, 1.5914853087003986, 1.5922316569217125, 1.5926792834975645, 1.5881915691450899, 1.597717962828763, 1.5954657021805962, 1.594594360376618, 1.5965416901217306, 1.5974780244780291, 1.6009081024645977, 1.5992143574998101, 1.5983845541629884, 1.5963694186046207, 1.5989524026222417, 1.5983528908837605, 1.5981338507631926, 1.6003690308146485, 1.5984735894085738, 1.6008346448782433, 1.6030407901272201, 1.5994983467170953, 1.5978695935216443, 1.5948827799122125, 1.5939159906164961, 1.5965276690343722, 1.5953151473075102, 1.5939358866273476, 1.5966827955543506, 1.5987241485435975, 1.5998948244821458, 1.5990219533149832, 1.5981379727816152, 1.5977119236744095, 1.5949114451463196, 1.5990176948616266, 1.5980832623730739, 1.600698065875199, 1.599052194695559, 1.6007175964283433, 1.601970120603815, 1.6003562121947215, 1.5979688245870405, 1.5979476202101934, 1.5995530541894472, 1.5994626412837964, 1.5997955638591097, 1.5993682977992718, 1.5995894059759055, 1.6009300044800456, 1.6045293030871939, 1.6012518298254028, 1.5998253691176867, 1.598178080933043, 1.6005876698517447, 1.5976045102321457, 1.5984626797032473, 1.6044340523201452, 1.6087319845049253, 1.6049083406702052, 1.6054834331001946, 1.6079519818764798, 1.6041170129635063, 1.6028152737515704, 1.6054077819846142, 1.6072717217976236, 1.6085173598259737, 1.608330537337192, 1.6071187465257442, 1.6076988334138993, 1.607707235221988, 1.6094394509232495, 1.6081795802061585, 1.606461544342229, 1.6114498237866681, 1.6137138632522232, 1.6153280400290277, 1.6030853806653829, 1.6043705200326854, 1.6071839450028143, 1.609160675203859, 1.6186694394191499, 1.6094098510021841, 1.6104946287198998, 1.605652274365104, 1.6051179399631295, 1.6089235242755933, 1.6082316501974472, 1.6091827299011556, 1.6045391863007068, 1.6108785973393858, 1.6138712289102364, 1.6100552861130688, 1.6003990739045668, 1.5997498579604676], 'val_acc': [0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.2331691293048937, 0.23152709327797194, 0.23645320165235617, 0.2331691293048937, 0.24466338247207586, 0.2413793101246134, 0.23809523777715091, 0.2380952378750239, 0.23645320175022916, 0.24466338247207586, 0.24630541840112463, 0.23973727390194566, 0.24794745472166535, 0.24630541869474357, 0.24466338237420288, 0.2413793100267404, 0.2463054185968706, 0.24302134634728112, 0.23645320155448318, 0.23645320155448318, 0.23809523767927793, 0.23645320155448318, 0.2528735630960496, 0.2413793100267404, 0.23973727390194566, 0.23973727390194566, 0.23809523767927793, 0.2495894907485871, 0.2446633822763299, 0.24630541840112463, 0.2413793100267404, 0.23973727380407267, 0.23809523767927793, 0.2528735630960496, 0.23973727380407267, 0.23645320155448318, 0.23973727390194566, 0.23809523777715091, 0.25123152697125484, 0.2495894908464601, 0.23809523777715091, 0.23809523777715091, 0.2495894908464601, 0.2495894908464601, 0.23973727390194566, 0.2413793100267404, 0.2413793100267404, 0.24794745462379236, 0.2463054185968706, 0.2528735630960496, 0.23973727390194566, 0.23973727390194566, 0.2495894908464601, 0.2495894907485871, 0.24794745472166535, 0.2413793100267404, 0.2413793100267404, 0.2413793100267404, 0.24466338247207586, 0.2479474548195383, 0.2413793100267404, 0.23973727390194566, 0.24630541840112463, 0.24302134634728112, 0.2413793100267404, 0.2495894907485871, 0.2545155992208443, 0.24466338237420288, 0.24794745452591938, 0.23645320155448318, 0.23152709298435298, 0.2528735629003036, 0.24302134605366216, 0.2446633822763299, 0.25123152687338185, 0.2348111652339425, 0.23973727390194566, 0.23809523777715091, 0.23481116542968844, 0.23481116542968844, 0.23645320165235617, 0.23316912940276668, 0.22988505685955823, 0.23645320126086425, 0.24794745433017343, 0.23645320135873724, 0.2348111652339425, 0.2430213459557892, 0.2348111652339425, 0.23316912910914772, 0.2348111651360695, 0.24794745452591938, 0.23316912910914772, 0.23645320155448318, 0.23809523777715091, 0.2364532014566102, 0.23809523767927793, 0.23481116542968844, 0.23645320155448318, 0.2364532014566102, 0.23809523758140494, 0.24794745462379236, 0.2364532014566102, 0.23809523758140494, 0.24466338237420288, 0.23973727380407267, 0.2331691292070207, 0.24794745462379236, 0.23809523748353198, 0.24137930973312147, 0.24302134624940813, 0.2331691292070207, 0.24958949065071412, 0.2561576347584012, 0.24958949065071412, 0.23809523767927793, 0.24466338247207586, 0.23973727380407267, 0.23645320175022916, 0.24794745462379236, 0.23645320155448318, 0.24630541869474357, 0.24630541869474357, 0.2446633822763299, 0.2364532014566102, 0.2331691292070207, 0.22824302063689053, 0.22824302063689053, 0.238095237385659, 0.2413793101246134, 0.23973727370619968, 0.23645320155448318, 0.25451559912297134, 0.24958949065071412, 0.2495894908464601, 0.2446633821784569, 0.24794745423230044, 0.24958949065071412, 0.25123152697125484, 0.24794745462379236, 0.24794745462379236, 0.2446633821784569, 0.2561576348562742, 0.24630541820537868, 0.26600985160504265, 0.2413793100267404, 0.23973727370619968, 0.23973727390194566, 0.2413793100267404, 0.23645320116299126, 0.2364532014566102, 0.26272577955119913, 0.25451559863360645, 0.24794745413442745, 0.2561576350520201, 0.2495894907485871, 0.2610837435242773, 0.2577996713725608, 0.2528735629003036, 0.25451559863360645, 0.2528735625088117, 0.2512315266776359, 0.2561576349541472, 0.24630541820537868, 0.24302134566217024, 0.2577996710789419, 0.24466338188483797, 0.2561576350520201, 0.24794745462379236, 0.2528735627045577, 0.24794745442804642, 0.24630541820537868, 0.2610837435242773, 0.2512315266776359, 0.26765188792558336, 0.2528735625088117, 0.24958949065071412, 0.2561576351498931, 0.24794745452591938, 0.26600985199653454, 0.2610837436221503, 0.2512315266776359, 0.24137930983099445, 0.25123152648188996, 0.25123152677550886, 0.25123152677550886, 0.23973727360832672, 0.2528735628024306, 0.2413793100267404, 0.2660098518007886, 0.26272577935545316, 0.2643678154802479, 0.26600985160504265, 0.27093596007729986, 0.2561576352477661, 0.2692939240503781, 0.26765188821920227, 0.24958949065071412, 0.25779967127468784, 0.2577996714704338, 0.27257799441591274, 0.27093596046879176, 0.24302134634728112, 0.2528735628024306, 0.24630541849899762, 0.2545155992208443, 0.25615763534563907, 0.25451559892722536, 0.26765188802345635, 0.2561576348562742, 0.25779967088319594, 0.25779967127468784, 0.24958949045496817, 0.25451559892722536, 0.2643678154802479, 0.26765188792558336, 0.2692939240503781, 0.26765188792558336, 0.27586206864743007, 0.26765188792558336, 0.2775041049679708, 0.2643678156759939, 0.24958949065071412, 0.2495894907485871, 0.25451559892722536, 0.2643678160674858, 0.24630541840112463, 0.25451559902509835, 0.27422003232688935, 0.2512315265797629, 0.26108374313278543, 0.24630541820537868, 0.24137930963524848, 0.24958949055284116, 0.25451559912297134, 0.24794745442804642, 0.2528735629981766, 0.24958949035709518, 0.27093596007729986, 0.2676518878277104, 0.2676518877298374, 0.24958949045496817, 0.2561576349541472, 0.24958949055284116, 0.2561576350520201, 0.2528735629003036, 0.2643678159696128, 0.24794745452591938, 0.27914614099489254, 0.23152709298435298, 0.24794745413442745, 0.24302134615153514, 0.26436781577386687, 0.2512315265797629, 0.27093596017517285, 0.2758620685495571, 0.27422003232688935, 0.26600985160504265, 0.2610837435242773, 0.2643678156759939, 0.24302134615153514, 0.24630541830325164, 0.24794745433017343, 0.2643678155781209, 0.24466338208058394, 0.2594417073994826, 0.2577996715683068, 0.2430213459557892, 0.2610837437200233, 0.2594417073994826, 0.2495894907485871, 0.2594417071058637, 0.25779967127468784, 0.24630541849899762, 0.22495894868091998, 0.2512315266776359, 0.24958949045496817, 0.26272577984481804, 0.2725779963978406, 0.2561576348562742, 0.25779967088319594, 0.27093596007729986, 0.27093596017517285, 0.25123152638401697, 0.23809523758140494, 0.275862068843176, 0.2676518877298374], 'loss': [1.6079543217986028, 1.605857149090855, 1.6050752946728308, 1.604591021204876, 1.6041157822344583, 1.6039794495463127, 1.6036194998136046, 1.6035137019363028, 1.60363615729236, 1.6034568173195058, 1.6031188605991966, 1.6030272687485085, 1.6027846537576318, 1.6028376011388257, 1.6027353949615353, 1.60245454022527, 1.602586269035966, 1.602238528684424, 1.6023699514185379, 1.6021289087663686, 1.6022045907543425, 1.602215793490165, 1.6023561343520085, 1.6021227962427316, 1.6013788021075896, 1.6023308647731491, 1.6020811366839085, 1.602094213673711, 1.6015557531948208, 1.6016219237991427, 1.6016031139929927, 1.6015994880723268, 1.6017793925146302, 1.6014516584192704, 1.60173542847134, 1.6018982536249338, 1.6018721387616417, 1.6017162717588138, 1.6018554163664518, 1.6017000814727689, 1.6017580184114053, 1.6015163725651265, 1.601537790484497, 1.6012335144274044, 1.6012890044668617, 1.601110096829628, 1.6024066564728348, 1.6017414204638596, 1.6016598799390225, 1.60150299253405, 1.6014220558887144, 1.6012549161911012, 1.601244047975638, 1.6011914420176825, 1.600771777732661, 1.6005960623096882, 1.6008971581958402, 1.6002448101063285, 1.6005510431050765, 1.6007241956507157, 1.600141945658768, 1.6006326335166758, 1.6006095952811428, 1.5998912351087378, 1.6010015509211797, 1.5997021383328605, 1.5997260763904642, 1.5998931955752675, 1.599515827921137, 1.5997485348331366, 1.5990508805554995, 1.599232252079848, 1.599493522859452, 1.5987814060716414, 1.5997787250875202, 1.5985655147681737, 1.5988433463372733, 1.5993235529325827, 1.5987942629526284, 1.5988573541386661, 1.598841139325371, 1.5994540893566438, 1.5986175273722936, 1.59970656175633, 1.6003996927880164, 1.6004860757557517, 1.599098991172759, 1.5993744027688028, 1.5998280158522682, 1.59906726010771, 1.5990657215490478, 1.5992825372997495, 1.5995912981229152, 1.6007288571990246, 1.5997803986194932, 1.6000679975172822, 1.5996089770319035, 1.598989392061253, 1.5990902199147907, 1.5988924511649036, 1.5987063992439599, 1.5979687503231135, 1.59932682724704, 1.6010642182410866, 1.6010639690520583, 1.5993468560722084, 1.5984549547367761, 1.598568703114864, 1.5985394761302878, 1.5978865003683729, 1.5975446390175476, 1.5971722340926497, 1.5961072869369382, 1.5958305113614217, 1.595745053820052, 1.5949921755820085, 1.5952998604862596, 1.594604224590795, 1.5955396916097684, 1.5944764844201185, 1.5945762593153812, 1.5955412838248502, 1.5947648848841078, 1.5942344244500695, 1.5933312790104985, 1.5930143918834427, 1.5936529740660588, 1.5934165822896624, 1.593236455878193, 1.5943145564938963, 1.59536220170634, 1.5954127953037833, 1.594503810567288, 1.592028523813283, 1.593160270616504, 1.59234754897241, 1.5922871125062632, 1.5911947877010526, 1.5917549620908389, 1.5922632788730597, 1.5931735903330653, 1.591266953039463, 1.5929065284787751, 1.5932571577095642, 1.5938798326241652, 1.5933609588435054, 1.5913183688627865, 1.5923026091264258, 1.593049413516536, 1.5931694608939013, 1.594969799190576, 1.591586197181404, 1.5928943749570748, 1.591205018110099, 1.592279825905755, 1.5947744623591522, 1.5908389935013694, 1.591884939381719, 1.5978114114404949, 1.5985470783049567, 1.5986519396427477, 1.597274431996277, 1.5953304031791138, 1.5953367885867673, 1.5930886905051356, 1.592434618458366, 1.5926396455118543, 1.5902509212493896, 1.5895359682596195, 1.5907215021474161, 1.5923432338898675, 1.5870188402199403, 1.5885709137887192, 1.588559646381245, 1.5889706338455545, 1.5883545173511857, 1.5881012119551703, 1.5869211129339325, 1.5872797994398238, 1.5883777094083156, 1.5905139363521912, 1.5921337884064817, 1.5900830474967096, 1.5874473144386338, 1.5863655667040628, 1.5882490597711207, 1.586510250553703, 1.5844747867917133, 1.5857602571804665, 1.584676373470001, 1.5843382534305173, 1.58392664539251, 1.5850430571812624, 1.5858808795039903, 1.5868693158367086, 1.5842201176120516, 1.5837507930379628, 1.5846569306551799, 1.584381174992242, 1.5847629411019828, 1.582210896636916, 1.5824473962157168, 1.581089175408381, 1.5816450611521822, 1.5823192789814067, 1.5810375697803694, 1.5811711192865392, 1.5799878304009565, 1.5787051213105845, 1.5788202224570869, 1.5785265068253942, 1.5805381589846446, 1.5813218190684701, 1.581134299722785, 1.5804033476713992, 1.5828305144084798, 1.5797900964592027, 1.5815158154685394, 1.5812771135287118, 1.5799806750041014, 1.5774029303869916, 1.578343544260922, 1.5789150550869702, 1.58286937252452, 1.5778161819465841, 1.5777261051553966, 1.5764743769682898, 1.5764416912987491, 1.575313751457653, 1.576015668679067, 1.5752153189520082, 1.575049546513959, 1.5756051896289145, 1.5753853812110008, 1.5766031348484988, 1.5749322276830184, 1.5732029852191525, 1.5753271518546699, 1.5721099507392555, 1.5719013454977737, 1.5731620697025401, 1.5734658825813623, 1.574403412777785, 1.57513643045445, 1.5724117294229276, 1.573103925481714, 1.5700339141078063, 1.571155087610045, 1.573512348110426, 1.5749361252148293, 1.5698299508809554, 1.5698129723938583, 1.569566639590802, 1.568934777434112, 1.5707979033370283, 1.567040220765852, 1.5669539131423043, 1.5711059995745242, 1.5685942649351743, 1.5681928044227114, 1.5662411936989065, 1.5710876596047403, 1.5717182728788937, 1.5703008494582753, 1.5675848742530087, 1.5680019106953051, 1.5675074217990195, 1.5660774824310866, 1.5625137936163243, 1.563637610725309, 1.5625991258777876, 1.5639601907690936, 1.5612993670684845, 1.5612592921364723, 1.55822073732803, 1.559949249653356, 1.5597171663993192, 1.560491830612355, 1.5592975888653702, 1.5580271859433372, 1.5584815229968123, 1.5594007869520716, 1.5571825127337258, 1.558832454142874, 1.55784308836935, 1.5570884576323585, 1.5613443933718014, 1.5629001722198737, 1.5583751186942663, 1.5633883538921756, 1.5597560531060064, 1.5611579202283823, 1.559185381395861, 1.558896238554185, 1.5571900089663402, 1.5535857565349132, 1.5514228028193637, 1.5572327091953349, 1.5557742568256918, 1.553720255505133], 'acc': [0.22792607866518305, 0.23285421010037957, 0.23285420931707418, 0.2328542083195837, 0.2328542094945418, 0.23285421011873828, 0.23285420874795384, 0.23285420931707418, 0.23285420851541005, 0.23285420892542147, 0.2328542083195837, 0.2328542083195837, 0.23285420892542147, 0.23285420929871545, 0.23285420970872686, 0.23655030805717014, 0.23162217705034377, 0.23203285347509678, 0.24147843888652887, 0.24065708325873655, 0.23942505099200614, 0.24024640603231942, 0.238603696380063, 0.24188911789374185, 0.24188911630877236, 0.23613963024327397, 0.24271047134908563, 0.23983573095999214, 0.23901437478143822, 0.24147843988401935, 0.2410677626576022, 0.24024640720727752, 0.23860369556004016, 0.24106776107263272, 0.23942505179367027, 0.2414784414322714, 0.24435318221301758, 0.24188911728790408, 0.24147843888652887, 0.24065708486206477, 0.24147843829904983, 0.2447638617893509, 0.24312114916298178, 0.24394250576990587, 0.24353182738688936, 0.24147844027567203, 0.24271047134908563, 0.2427104719549234, 0.24106776185593812, 0.24065708523535875, 0.24065708482534734, 0.23942505138365885, 0.24229979431849485, 0.2422997949059739, 0.24271047254240244, 0.24106776283506984, 0.24353182776018334, 0.24229979510180025, 0.24271047291569642, 0.24147843951072537, 0.24353182817019475, 0.23983572960756644, 0.24681724811726283, 0.2439425043991214, 0.24065708325873655, 0.2447638602227401, 0.24394250480913285, 0.240657084256227, 0.24024640564066674, 0.2418891169146101, 0.24353182658522526, 0.24065708462952098, 0.2353182759862661, 0.24147843927818158, 0.24271047232821738, 0.24229979414102723, 0.23942505020870075, 0.24188911709207775, 0.24106776089516507, 0.23613963004744762, 0.24517453803663625, 0.23531827461548166, 0.24024640683398354, 0.24353182856184746, 0.22012320258778958, 0.2381930173544913, 0.24558521545887974, 0.23408624275876266, 0.2447638594394347, 0.246406570499193, 0.239835729197555, 0.23572895381852096, 0.24558521585053242, 0.23942505101036488, 0.24188911750208916, 0.2418891176979155, 0.23901437456725314, 0.2390143743714268, 0.24969199105209883, 0.24928131382568172, 0.2439425055740795, 0.25010267023677946, 0.24681724811726283, 0.241889116112946, 0.2349075964282915, 0.22628336760542475, 0.24804928038399324, 0.2468172465139346, 0.24763860335340246, 0.2488706365992646, 0.2501026700409531, 0.24558521487140067, 0.2472279251478536, 0.2501026696676591, 0.2533880921604697, 0.2505133490298073, 0.24804928018816688, 0.2533880892230745, 0.24599589088614227, 0.2496919898404233, 0.2529774137590945, 0.24681724987969997, 0.24558521565470606, 0.24763860374505514, 0.25092402429796096, 0.24763860335340246, 0.24640657149668346, 0.24845995841207447, 0.2509240244937873, 0.24353182679941032, 0.24065708464787972, 0.24558521665219654, 0.2418891176795568, 0.24928131222235347, 0.2533880913771643, 0.2501026692576477, 0.2517453799257533, 0.25215605656469137, 0.24887063718674365, 0.2476386047241869, 0.2501026712159112, 0.24804928015144945, 0.2447638610060455, 0.2439425055740795, 0.23819301815615543, 0.24681724987969997, 0.25338809039803256, 0.25010266984512675, 0.2513347031093476, 0.25133470172020445, 0.25256673437858757, 0.2455852174171432, 0.24804928057981956, 0.24804928116729863, 0.24681724772561012, 0.2344969199851798, 0.24887063640343823, 0.2501026690618213, 0.24147843986566062, 0.22997946559893276, 0.22833675530412115, 0.24722792432783075, 0.24640657071337807, 0.2394250515978439, 0.2398357299808604, 0.2517453807274174, 0.24969199066044614, 0.2513347023076835, 0.2587269000204192, 0.2521560573479968, 0.2455852174171432, 0.2665297751553984, 0.2550308010844969, 0.24763860296174975, 0.2529774131899742, 0.2550308014761496, 0.25297741238831006, 0.25462012385807975, 0.2537987682119287, 0.2472279251478536, 0.2501026692576477, 0.24517453805499498, 0.26283367582782335, 0.257905546155064, 0.2583162225981757, 0.24722792651863804, 0.2542094452425195, 0.25790554200599325, 0.2501026696493004, 0.2591375760718782, 0.25585215610645146, 0.25256673398693485, 0.2550308008703118, 0.25215605854131357, 0.25297741356326814, 0.2546201250146791, 0.25913757466437637, 0.2529774121924837, 0.25749486542213135, 0.25010266867016867, 0.26406570729288964, 0.2537987701701922, 0.27104722719172925, 0.26406570729288964, 0.26201232135663044, 0.26283367420613646, 0.2591375780301417, 0.2562628341345327, 0.26570841796099526, 0.26529774308449433, 0.26611909832063396, 0.26036960849771756, 0.26119096709962253, 0.2533880887947043, 0.26119096594302316, 0.257905546155064, 0.25831622220652306, 0.2472279260902679, 0.25626283568278474, 0.26078028908990003, 0.267761805855518, 0.26160164297361393, 0.26283367538109453, 0.25831622197397924, 0.26406571062193757, 0.26447638510678584, 0.2611909661388495, 0.26365503065395157, 0.27145790715971524, 0.27022587332637404, 0.2722792627875075, 0.2685831642248792, 0.26447638647757027, 0.26611909734150224, 0.26406570925115314, 0.2644763878483547, 0.27433265087785663, 0.2583162224023494, 0.26817248660680937, 0.27022587371802675, 0.26981519551247785, 0.2599589338537604, 0.2718685837986533, 0.2607802883065946, 0.26817248562767765, 0.2652977432803207, 0.2710472267633591, 0.26776180800960786, 0.27104722895416633, 0.2652977426928416, 0.2706365511402702, 0.27433264833211407, 0.27310061446205547, 0.2747433268925982, 0.2726899376640085, 0.2706365485578102, 0.2796714579545007, 0.2677618084012605, 0.2706365525110546, 0.27186858399447966, 0.27310061626121, 0.2755646835362397, 0.26694045336094724, 0.27802874944048495, 0.2837782325318707, 0.27638603661828953, 0.27926077814562367, 0.2694045186777134, 0.27597535958769875, 0.2796714571344779, 0.2792607819030417, 0.28254620226012117, 0.2800821363558759, 0.27268993645233297, 0.27145790676806253, 0.2825461992860085, 0.2780287484613532, 0.27926078131556265, 0.27515400533069084, 0.27761806947249895, 0.2837782325318707, 0.2755646833404134, 0.28583162101387244, 0.2755646805621271, 0.2714579063947685, 0.2743326473162649, 0.2616016437385606, 0.2665297755470511, 0.27063655231522826, 0.2665297761345301, 0.2706365499653121, 0.26817248641098307, 0.27022587093974043, 0.27186858258697777, 0.26365503261221507, 0.27802874944048495, 0.2751540049390382, 0.2759753597835251, 0.2661190985164603, 0.2702258719555896]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
