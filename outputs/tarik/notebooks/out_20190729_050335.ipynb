{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf81.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 05:03:35 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '1Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001AE94677E48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001AE900F7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0951, Accuracy:0.3614, Validation Loss:1.0808, Validation Accuracy:0.4138\n",
    "Epoch #2: Loss:1.0787, Accuracy:0.3725, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #3: Loss:1.0744, Accuracy:0.3729, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0753, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0752, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0744, Accuracy:0.3951, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #8: Loss:1.0740, Accuracy:0.3951, Validation Loss:1.0731, Validation Accuracy:0.4138\n",
    "Epoch #9: Loss:1.0747, Accuracy:0.3873, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #10: Loss:1.0745, Accuracy:0.3930, Validation Loss:1.0731, Validation Accuracy:0.4023\n",
    "Epoch #11: Loss:1.0745, Accuracy:0.3934, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.4023\n",
    "Epoch #14: Loss:1.0748, Accuracy:0.3766, Validation Loss:1.0734, Validation Accuracy:0.4154\n",
    "Epoch #15: Loss:1.0742, Accuracy:0.3795, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0741, Accuracy:0.3938, Validation Loss:1.0734, Validation Accuracy:0.4089\n",
    "Epoch #19: Loss:1.0740, Accuracy:0.3955, Validation Loss:1.0729, Validation Accuracy:0.4039\n",
    "Epoch #20: Loss:1.0740, Accuracy:0.3930, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #26: Loss:1.0740, Accuracy:0.3955, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #27: Loss:1.0739, Accuracy:0.3922, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #35: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0737, Accuracy:0.3955, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #43: Loss:1.0738, Accuracy:0.3979, Validation Loss:1.0742, Validation Accuracy:0.4039\n",
    "Epoch #44: Loss:1.0736, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #45: Loss:1.0732, Accuracy:0.4107, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #46: Loss:1.0735, Accuracy:0.3934, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #48: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #49: Loss:1.0737, Accuracy:0.3975, Validation Loss:1.0727, Validation Accuracy:0.3990\n",
    "Epoch #50: Loss:1.0736, Accuracy:0.3938, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #51: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #52: Loss:1.0735, Accuracy:0.3914, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #54: Loss:1.0734, Accuracy:0.3926, Validation Loss:1.0731, Validation Accuracy:0.3957\n",
    "Epoch #55: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0730, Validation Accuracy:0.4056\n",
    "Epoch #56: Loss:1.0734, Accuracy:0.4049, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #57: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #58: Loss:1.0741, Accuracy:0.3930, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #59: Loss:1.0739, Accuracy:0.3926, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #60: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #64: Loss:1.0740, Accuracy:0.3971, Validation Loss:1.0737, Validation Accuracy:0.4072\n",
    "Epoch #65: Loss:1.0740, Accuracy:0.3955, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #66: Loss:1.0739, Accuracy:0.3967, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #67: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #68: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #69: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #70: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #72: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #73: Loss:1.0739, Accuracy:0.3938, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #74: Loss:1.0741, Accuracy:0.3934, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #75: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #77: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #78: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #79: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #80: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #81: Loss:1.0742, Accuracy:0.3844, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #82: Loss:1.0736, Accuracy:0.4016, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #83: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #84: Loss:1.0731, Accuracy:0.3967, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #85: Loss:1.0732, Accuracy:0.3996, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #86: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #87: Loss:1.0737, Accuracy:0.4008, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #88: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #89: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #90: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #91: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.4138\n",
    "Epoch #92: Loss:1.0743, Accuracy:0.3922, Validation Loss:1.0735, Validation Accuracy:0.4171\n",
    "Epoch #93: Loss:1.0742, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #94: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #95: Loss:1.0747, Accuracy:0.3737, Validation Loss:1.0740, Validation Accuracy:0.4105\n",
    "Epoch #96: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #97: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #98: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #99: Loss:1.0738, Accuracy:0.3910, Validation Loss:1.0743, Validation Accuracy:0.4072\n",
    "Epoch #100: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #101: Loss:1.0736, Accuracy:0.3926, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #102: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #103: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #104: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #105: Loss:1.0733, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #106: Loss:1.0733, Accuracy:0.3959, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #107: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0745, Validation Accuracy:0.4072\n",
    "Epoch #108: Loss:1.0737, Accuracy:0.3897, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #109: Loss:1.0741, Accuracy:0.3984, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #110: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #111: Loss:1.0742, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.4039\n",
    "Epoch #112: Loss:1.0729, Accuracy:0.3955, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #113: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #114: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #115: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.4056\n",
    "Epoch #116: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #117: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #118: Loss:1.0729, Accuracy:0.3979, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #119: Loss:1.0727, Accuracy:0.3967, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #120: Loss:1.0731, Accuracy:0.3967, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #121: Loss:1.0729, Accuracy:0.3951, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #122: Loss:1.0730, Accuracy:0.3975, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #123: Loss:1.0732, Accuracy:0.3930, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #124: Loss:1.0732, Accuracy:0.3975, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #125: Loss:1.0727, Accuracy:0.3955, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #126: Loss:1.0730, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.4072\n",
    "Epoch #127: Loss:1.0726, Accuracy:0.3951, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #128: Loss:1.0727, Accuracy:0.3971, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #129: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0759, Validation Accuracy:0.3924\n",
    "Epoch #130: Loss:1.0724, Accuracy:0.3959, Validation Loss:1.0758, Validation Accuracy:0.3957\n",
    "Epoch #131: Loss:1.0723, Accuracy:0.3975, Validation Loss:1.0762, Validation Accuracy:0.3990\n",
    "Epoch #132: Loss:1.0734, Accuracy:0.3979, Validation Loss:1.0768, Validation Accuracy:0.3941\n",
    "Epoch #133: Loss:1.0725, Accuracy:0.3975, Validation Loss:1.0773, Validation Accuracy:0.4007\n",
    "Epoch #134: Loss:1.0728, Accuracy:0.3963, Validation Loss:1.0762, Validation Accuracy:0.3974\n",
    "Epoch #135: Loss:1.0731, Accuracy:0.3947, Validation Loss:1.0756, Validation Accuracy:0.3957\n",
    "Epoch #136: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0768, Validation Accuracy:0.3941\n",
    "Epoch #137: Loss:1.0743, Accuracy:0.4012, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #138: Loss:1.0724, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #139: Loss:1.0723, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #140: Loss:1.0723, Accuracy:0.3906, Validation Loss:1.0746, Validation Accuracy:0.4056\n",
    "Epoch #141: Loss:1.0721, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.4056\n",
    "Epoch #142: Loss:1.0721, Accuracy:0.3951, Validation Loss:1.0747, Validation Accuracy:0.4007\n",
    "Epoch #143: Loss:1.0721, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #144: Loss:1.0726, Accuracy:0.3951, Validation Loss:1.0752, Validation Accuracy:0.4039\n",
    "Epoch #145: Loss:1.0716, Accuracy:0.3988, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #146: Loss:1.0724, Accuracy:0.3963, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #147: Loss:1.0720, Accuracy:0.3955, Validation Loss:1.0751, Validation Accuracy:0.4007\n",
    "Epoch #148: Loss:1.0722, Accuracy:0.3947, Validation Loss:1.0752, Validation Accuracy:0.4039\n",
    "Epoch #149: Loss:1.0717, Accuracy:0.3984, Validation Loss:1.0754, Validation Accuracy:0.3859\n",
    "Epoch #150: Loss:1.0721, Accuracy:0.3984, Validation Loss:1.0748, Validation Accuracy:0.4056\n",
    "Epoch #151: Loss:1.0719, Accuracy:0.3955, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #152: Loss:1.0725, Accuracy:0.3967, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #153: Loss:1.0724, Accuracy:0.3922, Validation Loss:1.0740, Validation Accuracy:0.4122\n",
    "Epoch #154: Loss:1.0732, Accuracy:0.3955, Validation Loss:1.0740, Validation Accuracy:0.4138\n",
    "Epoch #155: Loss:1.0725, Accuracy:0.3918, Validation Loss:1.0742, Validation Accuracy:0.4122\n",
    "Epoch #156: Loss:1.0725, Accuracy:0.3963, Validation Loss:1.0752, Validation Accuracy:0.3957\n",
    "Epoch #157: Loss:1.0730, Accuracy:0.3947, Validation Loss:1.0759, Validation Accuracy:0.3859\n",
    "Epoch #158: Loss:1.0720, Accuracy:0.3938, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #159: Loss:1.0729, Accuracy:0.3938, Validation Loss:1.0748, Validation Accuracy:0.4056\n",
    "Epoch #160: Loss:1.0725, Accuracy:0.3975, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #161: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #162: Loss:1.0730, Accuracy:0.3951, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #163: Loss:1.0727, Accuracy:0.3934, Validation Loss:1.0743, Validation Accuracy:0.4056\n",
    "Epoch #164: Loss:1.0728, Accuracy:0.3963, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #165: Loss:1.0723, Accuracy:0.3955, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #166: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #167: Loss:1.0721, Accuracy:0.3959, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #168: Loss:1.0722, Accuracy:0.3934, Validation Loss:1.0742, Validation Accuracy:0.4072\n",
    "Epoch #169: Loss:1.0720, Accuracy:0.3979, Validation Loss:1.0742, Validation Accuracy:0.4056\n",
    "Epoch #170: Loss:1.0723, Accuracy:0.3914, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #171: Loss:1.0721, Accuracy:0.3967, Validation Loss:1.0743, Validation Accuracy:0.4056\n",
    "Epoch #172: Loss:1.0723, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #173: Loss:1.0725, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #174: Loss:1.0727, Accuracy:0.3930, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #175: Loss:1.0719, Accuracy:0.3951, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #176: Loss:1.0727, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #177: Loss:1.0727, Accuracy:0.3963, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #178: Loss:1.0723, Accuracy:0.3914, Validation Loss:1.0748, Validation Accuracy:0.4056\n",
    "Epoch #179: Loss:1.0719, Accuracy:0.3959, Validation Loss:1.0750, Validation Accuracy:0.3859\n",
    "Epoch #180: Loss:1.0719, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3859\n",
    "Epoch #181: Loss:1.0719, Accuracy:0.3971, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #182: Loss:1.0719, Accuracy:0.3984, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #183: Loss:1.0718, Accuracy:0.3967, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #184: Loss:1.0716, Accuracy:0.3971, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #185: Loss:1.0721, Accuracy:0.3967, Validation Loss:1.0744, Validation Accuracy:0.4072\n",
    "Epoch #186: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #187: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #188: Loss:1.0715, Accuracy:0.3971, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #189: Loss:1.0717, Accuracy:0.3975, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #190: Loss:1.0713, Accuracy:0.3975, Validation Loss:1.0751, Validation Accuracy:0.4072\n",
    "Epoch #191: Loss:1.0714, Accuracy:0.3975, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #192: Loss:1.0711, Accuracy:0.3963, Validation Loss:1.0750, Validation Accuracy:0.4072\n",
    "Epoch #193: Loss:1.0713, Accuracy:0.3951, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #194: Loss:1.0712, Accuracy:0.3979, Validation Loss:1.0752, Validation Accuracy:0.3908\n",
    "Epoch #195: Loss:1.0714, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #196: Loss:1.0715, Accuracy:0.3955, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #197: Loss:1.0714, Accuracy:0.3963, Validation Loss:1.0765, Validation Accuracy:0.3859\n",
    "Epoch #198: Loss:1.0717, Accuracy:0.3955, Validation Loss:1.0761, Validation Accuracy:0.3908\n",
    "Epoch #199: Loss:1.0716, Accuracy:0.3975, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #200: Loss:1.0712, Accuracy:0.3930, Validation Loss:1.0754, Validation Accuracy:0.4056\n",
    "Epoch #201: Loss:1.0715, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3908\n",
    "Epoch #202: Loss:1.0713, Accuracy:0.3971, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #203: Loss:1.0712, Accuracy:0.3979, Validation Loss:1.0759, Validation Accuracy:0.4072\n",
    "Epoch #204: Loss:1.0713, Accuracy:0.3959, Validation Loss:1.0757, Validation Accuracy:0.4056\n",
    "Epoch #205: Loss:1.0716, Accuracy:0.3984, Validation Loss:1.0763, Validation Accuracy:0.3924\n",
    "Epoch #206: Loss:1.0719, Accuracy:0.3963, Validation Loss:1.0752, Validation Accuracy:0.3924\n",
    "Epoch #207: Loss:1.0718, Accuracy:0.3959, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #208: Loss:1.0718, Accuracy:0.3934, Validation Loss:1.0756, Validation Accuracy:0.3908\n",
    "Epoch #209: Loss:1.0718, Accuracy:0.3947, Validation Loss:1.0758, Validation Accuracy:0.3990\n",
    "Epoch #210: Loss:1.0716, Accuracy:0.3922, Validation Loss:1.0754, Validation Accuracy:0.4072\n",
    "Epoch #211: Loss:1.0717, Accuracy:0.3910, Validation Loss:1.0754, Validation Accuracy:0.4072\n",
    "Epoch #212: Loss:1.0719, Accuracy:0.3959, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #213: Loss:1.0714, Accuracy:0.3955, Validation Loss:1.0753, Validation Accuracy:0.3859\n",
    "Epoch #214: Loss:1.0719, Accuracy:0.3926, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #215: Loss:1.0721, Accuracy:0.3910, Validation Loss:1.0749, Validation Accuracy:0.4039\n",
    "Epoch #216: Loss:1.0716, Accuracy:0.3971, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #217: Loss:1.0715, Accuracy:0.3959, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #218: Loss:1.0716, Accuracy:0.3951, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #219: Loss:1.0719, Accuracy:0.3975, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #220: Loss:1.0721, Accuracy:0.3938, Validation Loss:1.0753, Validation Accuracy:0.3908\n",
    "Epoch #221: Loss:1.0714, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #222: Loss:1.0723, Accuracy:0.3975, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #223: Loss:1.0719, Accuracy:0.3971, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #224: Loss:1.0724, Accuracy:0.3918, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #225: Loss:1.0724, Accuracy:0.3934, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #226: Loss:1.0716, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #227: Loss:1.0717, Accuracy:0.3967, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #228: Loss:1.0716, Accuracy:0.3967, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #229: Loss:1.0716, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #230: Loss:1.0714, Accuracy:0.3967, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #231: Loss:1.0714, Accuracy:0.3963, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #232: Loss:1.0713, Accuracy:0.3947, Validation Loss:1.0752, Validation Accuracy:0.3990\n",
    "Epoch #233: Loss:1.0715, Accuracy:0.3938, Validation Loss:1.0750, Validation Accuracy:0.4023\n",
    "Epoch #234: Loss:1.0725, Accuracy:0.3906, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #235: Loss:1.0720, Accuracy:0.3967, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #236: Loss:1.0718, Accuracy:0.3959, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #237: Loss:1.0713, Accuracy:0.3959, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #238: Loss:1.0714, Accuracy:0.3930, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #239: Loss:1.0713, Accuracy:0.3963, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #240: Loss:1.0716, Accuracy:0.3963, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #241: Loss:1.0712, Accuracy:0.3967, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #242: Loss:1.0717, Accuracy:0.3967, Validation Loss:1.0752, Validation Accuracy:0.3974\n",
    "Epoch #243: Loss:1.0713, Accuracy:0.3951, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #244: Loss:1.0710, Accuracy:0.3959, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #245: Loss:1.0713, Accuracy:0.3947, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #246: Loss:1.0714, Accuracy:0.3959, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #247: Loss:1.0723, Accuracy:0.3959, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #248: Loss:1.0710, Accuracy:0.3938, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #249: Loss:1.0719, Accuracy:0.3947, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #250: Loss:1.0714, Accuracy:0.3918, Validation Loss:1.0759, Validation Accuracy:0.3957\n",
    "Epoch #251: Loss:1.0719, Accuracy:0.3951, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #252: Loss:1.0714, Accuracy:0.3934, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #253: Loss:1.0713, Accuracy:0.3934, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #254: Loss:1.0711, Accuracy:0.3918, Validation Loss:1.0751, Validation Accuracy:0.3908\n",
    "Epoch #255: Loss:1.0711, Accuracy:0.3971, Validation Loss:1.0747, Validation Accuracy:0.4039\n",
    "Epoch #256: Loss:1.0711, Accuracy:0.3984, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #257: Loss:1.0711, Accuracy:0.4008, Validation Loss:1.0763, Validation Accuracy:0.3941\n",
    "Epoch #258: Loss:1.0713, Accuracy:0.3975, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #259: Loss:1.0718, Accuracy:0.3979, Validation Loss:1.0760, Validation Accuracy:0.4039\n",
    "Epoch #260: Loss:1.0714, Accuracy:0.3959, Validation Loss:1.0766, Validation Accuracy:0.3924\n",
    "Epoch #261: Loss:1.0723, Accuracy:0.3897, Validation Loss:1.0764, Validation Accuracy:0.3892\n",
    "Epoch #262: Loss:1.0707, Accuracy:0.3988, Validation Loss:1.0760, Validation Accuracy:0.4039\n",
    "Epoch #263: Loss:1.0713, Accuracy:0.3996, Validation Loss:1.0761, Validation Accuracy:0.4007\n",
    "Epoch #264: Loss:1.0711, Accuracy:0.3947, Validation Loss:1.0766, Validation Accuracy:0.4023\n",
    "Epoch #265: Loss:1.0714, Accuracy:0.3955, Validation Loss:1.0761, Validation Accuracy:0.3974\n",
    "Epoch #266: Loss:1.0708, Accuracy:0.3963, Validation Loss:1.0758, Validation Accuracy:0.4089\n",
    "Epoch #267: Loss:1.0711, Accuracy:0.3992, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #268: Loss:1.0709, Accuracy:0.4008, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #269: Loss:1.0705, Accuracy:0.3963, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #270: Loss:1.0711, Accuracy:0.3971, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #271: Loss:1.0707, Accuracy:0.3963, Validation Loss:1.0755, Validation Accuracy:0.4007\n",
    "Epoch #272: Loss:1.0709, Accuracy:0.4045, Validation Loss:1.0756, Validation Accuracy:0.4039\n",
    "Epoch #273: Loss:1.0707, Accuracy:0.4037, Validation Loss:1.0753, Validation Accuracy:0.4039\n",
    "Epoch #274: Loss:1.0714, Accuracy:0.4029, Validation Loss:1.0753, Validation Accuracy:0.4023\n",
    "Epoch #275: Loss:1.0711, Accuracy:0.4041, Validation Loss:1.0773, Validation Accuracy:0.4023\n",
    "Epoch #276: Loss:1.0715, Accuracy:0.4053, Validation Loss:1.0759, Validation Accuracy:0.3974\n",
    "Epoch #277: Loss:1.0708, Accuracy:0.4049, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "Epoch #278: Loss:1.0705, Accuracy:0.4025, Validation Loss:1.0756, Validation Accuracy:0.4105\n",
    "Epoch #279: Loss:1.0712, Accuracy:0.3971, Validation Loss:1.0761, Validation Accuracy:0.4154\n",
    "Epoch #280: Loss:1.0719, Accuracy:0.3934, Validation Loss:1.0753, Validation Accuracy:0.4089\n",
    "Epoch #281: Loss:1.0715, Accuracy:0.3906, Validation Loss:1.0761, Validation Accuracy:0.4154\n",
    "Epoch #282: Loss:1.0712, Accuracy:0.3963, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #283: Loss:1.0713, Accuracy:0.3926, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #284: Loss:1.0713, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.4072\n",
    "Epoch #285: Loss:1.0710, Accuracy:0.3947, Validation Loss:1.0757, Validation Accuracy:0.4072\n",
    "Epoch #286: Loss:1.0720, Accuracy:0.3955, Validation Loss:1.0762, Validation Accuracy:0.4072\n",
    "Epoch #287: Loss:1.0711, Accuracy:0.3934, Validation Loss:1.0754, Validation Accuracy:0.4089\n",
    "Epoch #288: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.4122\n",
    "Epoch #289: Loss:1.0718, Accuracy:0.3963, Validation Loss:1.0758, Validation Accuracy:0.4122\n",
    "Epoch #290: Loss:1.0721, Accuracy:0.3963, Validation Loss:1.0755, Validation Accuracy:0.4122\n",
    "Epoch #291: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #292: Loss:1.0718, Accuracy:0.3951, Validation Loss:1.0766, Validation Accuracy:0.4072\n",
    "Epoch #293: Loss:1.0720, Accuracy:0.3938, Validation Loss:1.0752, Validation Accuracy:0.4056\n",
    "Epoch #294: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0751, Validation Accuracy:0.4089\n",
    "Epoch #295: Loss:1.0713, Accuracy:0.3947, Validation Loss:1.0754, Validation Accuracy:0.4007\n",
    "Epoch #296: Loss:1.0717, Accuracy:0.3947, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #297: Loss:1.0713, Accuracy:0.3967, Validation Loss:1.0754, Validation Accuracy:0.4056\n",
    "Epoch #298: Loss:1.0717, Accuracy:0.3959, Validation Loss:1.0757, Validation Accuracy:0.4007\n",
    "Epoch #299: Loss:1.0712, Accuracy:0.3955, Validation Loss:1.0760, Validation Accuracy:0.3892\n",
    "Epoch #300: Loss:1.0713, Accuracy:0.3951, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07579494, Accuracy:0.4007\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  22  205   0\n",
    "t:01  18  222   0\n",
    "t:03  18  124   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.38      0.10      0.15       227\n",
    "          01       0.40      0.93      0.56       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.40       609\n",
    "   macro avg       0.26      0.34      0.24       609\n",
    "weighted avg       0.30      0.40      0.28       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 05:44:02 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 27 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0808174878309904, 1.0740433913733571, 1.0737194474694765, 1.0747820383613724, 1.0744251096972888, 1.0733907197300827, 1.0731379400528906, 1.0731080574746594, 1.0729666135972749, 1.0731425332318385, 1.0728932092538217, 1.073017657879734, 1.0731190161164759, 1.0734118692980612, 1.0733043675743692, 1.0736350001176982, 1.0738324148118594, 1.073429451004429, 1.0729452868792029, 1.0731641711859867, 1.0729861402355, 1.0730278260993644, 1.073015511916776, 1.073315899947594, 1.0732927024853836, 1.0734310377211798, 1.0734079377404575, 1.0735263215692956, 1.0737447683838592, 1.0738900821588702, 1.0740826852215921, 1.0741986345579275, 1.0740254647626077, 1.0741876168008313, 1.0741125981404471, 1.0739643924146254, 1.074047536685549, 1.074012298693602, 1.0740749614775082, 1.0740256757767526, 1.0746489242575634, 1.074130376180013, 1.0742382273102433, 1.0740779912334748, 1.0737314120497805, 1.0734429997567865, 1.0735232088170419, 1.0734444097149352, 1.072694309826555, 1.0731349330034554, 1.0730316008644543, 1.0730701522482635, 1.0728318871339946, 1.0730800442703448, 1.073001176852898, 1.0729588477678096, 1.074332760081111, 1.0743033085354834, 1.074018676096974, 1.0735778109780674, 1.0740197264697948, 1.0736165340310835, 1.0738053460818011, 1.073658706519404, 1.0734904421178382, 1.0735981427194254, 1.073590104215838, 1.0735418700623787, 1.0737151696372698, 1.0736629385470562, 1.0738242285200723, 1.0740760641145002, 1.0738213275649473, 1.0737868007180726, 1.0737901853614644, 1.0737979877954242, 1.07387293853196, 1.073637331647826, 1.0736099068558667, 1.0735484081731836, 1.0735392537218793, 1.0740404246475896, 1.0737338189421028, 1.0735924627589084, 1.0737805358686274, 1.0739730526073812, 1.0739751423912487, 1.0741226678998599, 1.0739703935942626, 1.0739802650630181, 1.0737825542052195, 1.0735159899017885, 1.0740328083680377, 1.0736715310117098, 1.073954681653303, 1.0742546130087967, 1.0742587146696394, 1.0741379360847285, 1.074277590648294, 1.0740891379871587, 1.074290145989905, 1.074106748076691, 1.0741826475938945, 1.074025868195031, 1.07419166972093, 1.0742371011837362, 1.0744756967171856, 1.0744352141037363, 1.0752349340269718, 1.0747658462555734, 1.0751729585071308, 1.0746253556610132, 1.0747191220864483, 1.0746311894778549, 1.0750123902494684, 1.0746986173056616, 1.0747887559712226, 1.0747486583900765, 1.0748271491922965, 1.074964672865343, 1.075264973006225, 1.0752885153728167, 1.0756267565616051, 1.0756392856732573, 1.0754332518929919, 1.0754825646066901, 1.075559587118465, 1.0758803785336624, 1.075944492382369, 1.0757586476446568, 1.0761713821116732, 1.0767890318665403, 1.0773174792087723, 1.0762427487396842, 1.0755697455507978, 1.0768413252039692, 1.0755938127123077, 1.0747040876222558, 1.0746702854269243, 1.0745553263693999, 1.0746240613887268, 1.0746518389149056, 1.0747112278476334, 1.0751623112971365, 1.0749105275753879, 1.0750662070777028, 1.0751171951810714, 1.0752309413966288, 1.0753624237621164, 1.074795736467897, 1.0752508019774614, 1.0747597517051133, 1.0739860910500212, 1.0740050850634897, 1.0741769357267859, 1.0751581990660117, 1.07593204450529, 1.0763773722405896, 1.074750908489885, 1.0747954473511143, 1.0744094899526762, 1.0746627537096272, 1.0743241950208917, 1.074375930090843, 1.0743047997282056, 1.0749184239674083, 1.0742044844259377, 1.0741561185354473, 1.074170079920288, 1.0742166171520215, 1.0742986871691174, 1.0741015688343392, 1.0740838951273701, 1.0750701400055283, 1.0745917895353094, 1.0747085396683667, 1.0750791435366978, 1.0748326251855234, 1.0749999278871885, 1.0750256042762343, 1.0749657672810045, 1.0747881816525764, 1.074485041042071, 1.0748444920885936, 1.0743779312017907, 1.0746660240373784, 1.0746423579593403, 1.0745801594848507, 1.0746094651997382, 1.0750663881427158, 1.0749739798027502, 1.0749701316329254, 1.0749971633669975, 1.0751511535816787, 1.0749298641442862, 1.0753109879877376, 1.0764849812330675, 1.0761422578532904, 1.0748903044730376, 1.0753997607379908, 1.0753132216448855, 1.0756107371037424, 1.0758798919092063, 1.0756945747068558, 1.0762886114308399, 1.075155687253855, 1.075732137573568, 1.0755607392791848, 1.075804942738638, 1.075399139244568, 1.0753740657530786, 1.0759793269418927, 1.0753408815277425, 1.0747852959656363, 1.0749454294715217, 1.0749819077098703, 1.0749115476075848, 1.0748606432834868, 1.0751072499160892, 1.075259672401378, 1.0748516387735878, 1.0751471997090356, 1.07511844090836, 1.0754139069070174, 1.075329367554638, 1.0747371221019326, 1.0748534821132916, 1.07490329143449, 1.0749170032432318, 1.074875012407162, 1.0748250601914129, 1.0752281109100492, 1.074957919433982, 1.0749214770171442, 1.0755628795655099, 1.074892121387037, 1.0749903267436036, 1.0749411504648394, 1.0750853324367105, 1.0754633942261118, 1.0751456053581927, 1.0752329106009848, 1.0753118907681043, 1.0749652880948948, 1.0750199023921698, 1.0754901701202142, 1.0755392148576934, 1.0750194143974918, 1.0749781082807894, 1.0758567938859436, 1.0757290254085523, 1.0750035096467618, 1.0750455731045827, 1.0751285104720267, 1.0747347880271072, 1.074917929513114, 1.0763201355346905, 1.075946769103628, 1.0760391377071636, 1.0765660799587107, 1.0764053139976288, 1.0759927913277412, 1.07614068855793, 1.0765640772817953, 1.0761061996857717, 1.0757829557694434, 1.075765785129591, 1.0764552084683197, 1.0760696361021846, 1.0762483821126627, 1.0755116675287633, 1.0755695016513318, 1.0752956184064617, 1.075318382682863, 1.0773372096185418, 1.0758598275568294, 1.0757726419148186, 1.0756351652208025, 1.0761359532674153, 1.0752523323193754, 1.0760904174720125, 1.0754630414918922, 1.075911410336424, 1.0755131086105196, 1.0757469608278698, 1.076243134750717, 1.0753613056611937, 1.0748924882149657, 1.0758415190457122, 1.075505963687239, 1.0758578282075955, 1.0766204483990598, 1.0751730460055748, 1.0751041817939144, 1.0753874144530648, 1.0760950261148914, 1.0753798653143771, 1.0756877447388247, 1.076040438439067, 1.0757948731749711], 'val_acc': [0.41379309904399175, 0.3957307053414863, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.39901477817831366, 0.41379309875037285, 0.4072249589001604, 0.40229885052577613, 0.3940886696081835, 0.3940886696081835, 0.4022988504279032, 0.41543513923051517, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.40886699502495516, 0.4039408820994773, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.39573070583085124, 0.397372741955646, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.403940886161206, 0.40065681430310846, 0.38752052510900453, 0.3940886696081835, 0.390804597358594, 0.39901477798256774, 0.3990147780804407, 0.390804597358594, 0.39244663348338876, 0.3940886696081835, 0.3940886696081835, 0.39573070573297825, 0.4055829226774927, 0.3940886696081835, 0.3940886696081835, 0.3875205247175126, 0.39080459706497506, 0.3940886697060565, 0.3940886696081835, 0.3940886696081835, 0.3940886697060565, 0.40722495860654145, 0.3940886697060565, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.39244663348338876, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.40065681420523547, 0.39573070592872417, 0.3940886696081835, 0.3940886696081835, 0.39901477817831366, 0.39901477817831366, 0.39901477817831366, 0.39573070583085124, 0.3908045930032464, 0.3940886696081835, 0.3990147780804407, 0.397372741955646, 0.40229884626830154, 0.3990147780804407, 0.3957307014755036, 0.3940886696081835, 0.3940886696081835, 0.3940886696081835, 0.41379309894611876, 0.4170771716850732, 0.3940886696081835, 0.39737273750242535, 0.41050902728376715, 0.3940886696081835, 0.3940886696081835, 0.38752052491325856, 0.40722495474055875, 0.3940886699018024, 0.3940886696081835, 0.3940886696081835, 0.3924466292259141, 0.3940886696081835, 0.3940886696081835, 0.38916255687845164, 0.40722495474055875, 0.3875205207536569, 0.3940886696081835, 0.39737273760029834, 0.4039408823930963, 0.3940886696081835, 0.3940886696081835, 0.39408866535070886, 0.405582918517891, 0.39737273760029834, 0.39080459310111937, 0.39737273760029834, 0.39737273760029834, 0.3875205207536569, 0.3990147738229661, 0.39737273760029834, 0.3924466292259141, 0.39737273760029834, 0.3875205207536569, 0.40722495474055875, 0.39737273760029834, 0.3875205207536569, 0.3924466292259141, 0.3957307014755036, 0.3990147737250931, 0.39408866544858184, 0.40065680984988783, 0.39737273760029834, 0.3957307013776306, 0.3940886696081835, 0.3924466292259141, 0.39737273760029834, 0.3957307015733766, 0.405582918517891, 0.405582918517891, 0.40065680984988783, 0.3990147736272201, 0.4039408822952233, 0.38587848462886215, 0.39737273750242535, 0.40065680984988783, 0.4039408822952233, 0.38587848462886215, 0.40558291832214505, 0.3973727377960443, 0.39408866525283587, 0.41215106727454465, 0.41379309904399175, 0.412151063114943, 0.39573070583085124, 0.3858784892778287, 0.4022988459746826, 0.4055829181263991, 0.384236452859415, 0.38752052530475045, 0.40558291842001803, 0.40558291832214505, 0.3858784891799557, 0.38423645305516096, 0.3973727420535189, 0.3858784891799557, 0.4072249543490668, 0.40558291822427206, 0.3908045976522129, 0.40558291822427206, 0.3924466336791347, 0.3973727420535189, 0.40558291842001803, 0.3891625614295452, 0.3924466336791347, 0.40558291832214505, 0.40558291842001803, 0.3858784891799557, 0.3858784891799557, 0.3973727421513919, 0.3990147736272201, 0.3990147736272201, 0.3990147736272201, 0.4072249545448128, 0.39408866525283587, 0.38752052540262344, 0.39408866525283587, 0.3990147736272201, 0.4072249545448128, 0.3990147736272201, 0.4072249545448128, 0.3990147736272201, 0.3908045930032464, 0.40558291842001803, 0.3990147736272201, 0.3858784892778287, 0.3908045930032464, 0.405582918517891, 0.405582918517891, 0.3908045930032464, 0.39408866525283587, 0.40722495464268577, 0.405582918517891, 0.39244663377700767, 0.3924466336791347, 0.39901477817831366, 0.39080459755433994, 0.39901477817831366, 0.4072249544469398, 0.4072249543490668, 0.39901477817831366, 0.3858784891799557, 0.3891625614295452, 0.4039408823930963, 0.3973727420535189, 0.3891625614295452, 0.3973727420535189, 0.3858784891799557, 0.39080459755433994, 0.3973727420535189, 0.3973727421513919, 0.40558291832214505, 0.38752052530475045, 0.3973727421513919, 0.3973727420535189, 0.38752052530475045, 0.3924466336791347, 0.3973727420535189, 0.3973727420535189, 0.3973727420535189, 0.3990147736272201, 0.40229884617042855, 0.3973727420535189, 0.3924466336791347, 0.3973727420535189, 0.3973727421513919, 0.39901477401871205, 0.3957307014755036, 0.3973727420535189, 0.3973727420535189, 0.3973727420535189, 0.39901477392083906, 0.3957307014755036, 0.3957307013776306, 0.3973727420535189, 0.3973727420535189, 0.3973727377960443, 0.4006568102413798, 0.3957307013776306, 0.3891625615274182, 0.3973727420535189, 0.3973727377960443, 0.39080459319899236, 0.4039408823930963, 0.39737273750242535, 0.3940886697060565, 0.4022988459746826, 0.4039408823930963, 0.3924466293237871, 0.3891625570741976, 0.4039408823930963, 0.40065681420523547, 0.40229884626830154, 0.3973727377960443, 0.4088669909632265, 0.39408866554645483, 0.39080459329686534, 0.40229884626830154, 0.4022988459746826, 0.4006568102413798, 0.40394088249096927, 0.4039408822952233, 0.40229884626830154, 0.4022988463661745, 0.397372741955646, 0.4006568101435068, 0.41050902708802123, 0.4154351352666595, 0.40886699502495516, 0.4154351351687865, 0.40886699047386155, 0.4022988458768096, 0.4072249543490668, 0.4072249543490668, 0.4072249543490668, 0.40886699057173453, 0.41215106301707, 0.41215106301707, 0.41215106301707, 0.40229885052577613, 0.40722495464268577, 0.40558291842001803, 0.4088669906696075, 0.40065680975201484, 0.41050902689227525, 0.40558291842001803, 0.4006568144009814, 0.3891625569763246, 0.4006568144009814], 'loss': [1.0951183725921037, 1.078737409452638, 1.07444660502048, 1.0752795997341555, 1.0751630024743521, 1.0743502107733818, 1.0744421173659684, 1.0740200095108157, 1.0746521360330759, 1.0745060601028817, 1.074481821598703, 1.0744185774233308, 1.0742365270669456, 1.0747680711060823, 1.074246291409283, 1.074529969422969, 1.074275931391628, 1.074119364505431, 1.0740238626879588, 1.073966361267121, 1.0740021629744732, 1.0741835766504433, 1.0741647527447962, 1.073989977288295, 1.0739060347574692, 1.0739658424252112, 1.0738670485709971, 1.0738143965448932, 1.073950047806303, 1.0738324337671425, 1.0739112838337799, 1.07383433416394, 1.0738689120055713, 1.0737049934555618, 1.0738185280891903, 1.0736496221847847, 1.0737061729666144, 1.0736775610725982, 1.0735037955415323, 1.073662693642493, 1.0739782110621552, 1.0739484970574507, 1.0737587969895506, 1.0735860200878042, 1.0732453294848026, 1.0734612482039592, 1.0741483998739254, 1.0738813325854542, 1.0737022749941942, 1.0735889940535999, 1.0734715333954266, 1.073455177048638, 1.0736398892725763, 1.0734073126340549, 1.0735445465150555, 1.0733806728582362, 1.0738957722329017, 1.0741281185306808, 1.073860398504034, 1.073660577396103, 1.074108574081985, 1.0739072234723601, 1.0738787234441456, 1.0740053478452458, 1.0739737097977122, 1.0738682316558807, 1.0739207119912337, 1.0738384219410484, 1.0737993895395581, 1.0738142177064804, 1.0737903523494086, 1.0743457827479932, 1.0738947836035821, 1.0741115906400112, 1.0738407553099019, 1.0734990974226526, 1.073692773352903, 1.0733288682216982, 1.073535276242595, 1.0735589899811167, 1.0742250439077923, 1.0736303525783688, 1.0736248697833115, 1.0731383343741634, 1.0732425918814092, 1.0736687998017735, 1.0737072293518506, 1.0736220242796004, 1.0740289639153764, 1.0740573910961895, 1.0740207913475115, 1.0742995793324965, 1.074179916019557, 1.0741594532921575, 1.0747403341641906, 1.0740310732588876, 1.074094902318606, 1.0738983965507523, 1.0737574663984701, 1.073621890334378, 1.0736327431285162, 1.0736234586586453, 1.073612061565172, 1.073331392423328, 1.0733388307403, 1.0733236779912052, 1.0734626747009934, 1.0737022917373469, 1.0740611662365327, 1.0733694675766712, 1.074185637184237, 1.0729193636034549, 1.0732264061972836, 1.0734036353089726, 1.0730669650209024, 1.0734127141611778, 1.072941071443734, 1.0728858082691013, 1.072739513747746, 1.0731306284604865, 1.072851441332447, 1.0729563594109224, 1.073184287278804, 1.07323987846257, 1.0726993451617826, 1.0729835757484671, 1.0725764442028696, 1.072668185028452, 1.0727802428866313, 1.07236430243545, 1.0722602158846062, 1.0733890204459, 1.072472639446141, 1.072818109044304, 1.0730810391584706, 1.0742427194877326, 1.0742817105698634, 1.072416820956941, 1.0722768249452972, 1.0722650762456154, 1.0720873056740732, 1.0720998307273129, 1.072052775467201, 1.0726098701449636, 1.071574143952168, 1.0723735798555722, 1.0719824904044306, 1.072160389487014, 1.0716653460594663, 1.0720668247348228, 1.0719404533903212, 1.0724706028521183, 1.0723653015904357, 1.0732420862577778, 1.072451356549038, 1.0724794102890045, 1.0730444011257414, 1.0720320438702249, 1.0728850037165492, 1.0724558983005783, 1.0727715057024476, 1.073009678274699, 1.0726937791894837, 1.0728211330437318, 1.0723381594221206, 1.072783465943542, 1.0720794893143357, 1.072192580107546, 1.0720169078642827, 1.0722951771052711, 1.072088095931302, 1.0723433219921417, 1.07246069550759, 1.0726565350742066, 1.0719369561275662, 1.072742848915241, 1.07271068223448, 1.0722853485808481, 1.0718513962669294, 1.071939513325936, 1.0718890541632806, 1.0718933815828828, 1.0717520180668918, 1.0715630353598624, 1.072111319124821, 1.07160598399458, 1.0716104706210032, 1.071504344000219, 1.0716699355436792, 1.0713392170058138, 1.0714459480446221, 1.0711438730267284, 1.0712935828575119, 1.071222228191227, 1.0713927797223508, 1.0714935240069943, 1.071436657699961, 1.0716916869553208, 1.0715628769853032, 1.0711677593861761, 1.071492176330065, 1.071253302160964, 1.0711909225589196, 1.0712941813517889, 1.071599325604997, 1.071920639239787, 1.071756744237896, 1.0718313100646408, 1.0718380492815491, 1.0715758825964017, 1.07172719923133, 1.071944808127699, 1.071449793192885, 1.0719336620346476, 1.0720632015067695, 1.0716244852273615, 1.0714767224490032, 1.07164333708722, 1.071854019997301, 1.0721319889875407, 1.0713987319621217, 1.0723381232921592, 1.071869593283479, 1.0723928424122398, 1.0724382398064867, 1.0716418977635598, 1.0717366882907782, 1.0715692686593998, 1.0716032130027944, 1.0714349478911571, 1.0714424709030246, 1.0712734683093594, 1.0715337715599327, 1.0725296756325315, 1.0719967297215236, 1.0718082421124593, 1.0712841686526853, 1.0714442405857345, 1.0712506051425816, 1.0715738514365603, 1.0712219057631445, 1.0717196146810324, 1.0713448137228494, 1.071025590387458, 1.071259794587717, 1.0713729939666372, 1.0723414960093567, 1.0710471141999263, 1.0719408603174732, 1.0714404792511487, 1.0718810477051157, 1.0713588817898008, 1.0713051593279201, 1.0710852600955376, 1.0711374471319774, 1.0710934184170358, 1.0711339766484755, 1.0712969150876117, 1.0718198388998514, 1.0713748684654, 1.0722982182395042, 1.0707001851569455, 1.0712804067795771, 1.071074415085497, 1.0714083935935395, 1.0707640039847373, 1.0710750207274358, 1.0709068536758424, 1.0704922268278054, 1.071072740966045, 1.0707238489597486, 1.0708727734779186, 1.0706565556340149, 1.071382928041462, 1.0710509857847461, 1.0715007807439847, 1.070823531865584, 1.0704535988077246, 1.0711707682090619, 1.0718772233144458, 1.071497698779958, 1.0712416788880585, 1.0713411561272717, 1.0713144732696565, 1.0709791631914016, 1.072012179294406, 1.0711245256282955, 1.0716307911784742, 1.0717826296416642, 1.072053882130852, 1.072050843591318, 1.0717809717757991, 1.071985851861613, 1.0716045921098525, 1.071328569535602, 1.071653510266016, 1.071336567573234, 1.0717103609558982, 1.071212093737091, 1.071283487176993], 'acc': [0.36139630172286924, 0.37248459819405966, 0.37289527816204565, 0.3942505125392389, 0.39425051547663414, 0.3942505153175252, 0.39507187090860013, 0.39507186953781565, 0.38726899499031553, 0.3930184814474666, 0.3934291588697101, 0.39425051273506523, 0.39425051508498143, 0.3765913747664105, 0.37946612024698906, 0.39425051449750237, 0.3942505119150424, 0.39383983805439066, 0.39548254437759917, 0.3930184814474666, 0.39425051449750237, 0.39425051113173704, 0.3942505153175252, 0.39425051116845444, 0.39383983766273795, 0.39548254833084356, 0.39219712699463233, 0.3942505113275634, 0.3942505123434126, 0.39425051113173704, 0.394250513714197, 0.3942505141058497, 0.3942505152808078, 0.3942505148891551, 0.3942505133225443, 0.3942505152808078, 0.3954825447692519, 0.39630390337115684, 0.3942505152808078, 0.3942505123066951, 0.3942505123066951, 0.39425051508498143, 0.39794661008601806, 0.40287474506444754, 0.4106776184369896, 0.3934291573030993, 0.3950718701252947, 0.3942505129308916, 0.3975359358337136, 0.39383983766273795, 0.39671457977755115, 0.39137577136684004, 0.39425051250252147, 0.39260780324191774, 0.3967145772318086, 0.4049281310007068, 0.3942505125392389, 0.39301848262242467, 0.39260780285026503, 0.39260780422104946, 0.3950718707127738, 0.39466118996148236, 0.3950718707127738, 0.3971252570406857, 0.3954825485266699, 0.39671457883513683, 0.3942505152808078, 0.39425051250252147, 0.395071866563703, 0.395482546531689, 0.39466119309470393, 0.3942505148891551, 0.3938398342969726, 0.39342915769475195, 0.3950718677386611, 0.3942505115233897, 0.39630390023793527, 0.3967145790309632, 0.3983572916573323, 0.39753593661701897, 0.38439425225130586, 0.40164270966449556, 0.3942505115233897, 0.3967145770726997, 0.3995893205582973, 0.39589322222821277, 0.40082135341250674, 0.39753593642119267, 0.3942505113275634, 0.394250513714197, 0.3942505148891551, 0.39219712542802154, 0.3958932243823026, 0.3946611928988776, 0.3737166332390764, 0.39466119309470393, 0.3942505133225443, 0.3942505148891551, 0.39096509198633306, 0.3987679653221577, 0.3926078048085285, 0.39425051171921605, 0.3975359328596009, 0.39876796551798405, 0.3954825455892747, 0.3958932247739553, 0.40000000212961156, 0.389733059915429, 0.39835728848739327, 0.3946611887498068, 0.4004106759902633, 0.3954825475475382, 0.39425051250252147, 0.3958932232073445, 0.3942505113642808, 0.3975359324679482, 0.3995893231407573, 0.39794661266847803, 0.3967145804017476, 0.3967145800100949, 0.3950718681670312, 0.3975359358337136, 0.3930184810558139, 0.39753593266377457, 0.39548254793919085, 0.39466118914145953, 0.395071869146163, 0.39712525762816475, 0.39671457922678954, 0.39589322536143434, 0.39753593309214474, 0.39794661364760975, 0.39753593207629556, 0.39630390219619877, 0.3946611909406141, 0.39466119191974586, 0.40123203517964734, 0.39671457860259307, 0.3963039004337616, 0.39055441354824044, 0.39589322340317085, 0.39507186895033664, 0.39671457684015593, 0.3950718707127738, 0.3987679657505278, 0.3963038997728477, 0.39548254539344835, 0.394661189765656, 0.39835728852411073, 0.3983572882915669, 0.39548254555255724, 0.3967145774643524, 0.3921971264071533, 0.39548254500179564, 0.3917864471857553, 0.39630390278367783, 0.39466118914145953, 0.3938398342969726, 0.39383983648777987, 0.3975359338754501, 0.395071869146163, 0.39507186636787667, 0.39342915965301545, 0.3963039004337616, 0.3954825467642328, 0.40000000212961156, 0.3958932255572607, 0.3934291602404945, 0.3979466106734971, 0.391375768196901, 0.3967145772318086, 0.3975359358337136, 0.3963039025878515, 0.3930184794892031, 0.395071869146163, 0.39425051113173704, 0.39630390219619877, 0.39137576800107465, 0.39589322536143434, 0.39425051508498143, 0.3971252562573803, 0.39835728770408785, 0.3967145799733775, 0.3971252570406857, 0.39671457742763494, 0.39425051113173704, 0.395893225165608, 0.39712525524153114, 0.39753593563788725, 0.3975359362253663, 0.39753593465875553, 0.3963039009845232, 0.39507186836285757, 0.3979466128643044, 0.39630390020121786, 0.3954825441817728, 0.3963039023920251, 0.39548254793919085, 0.3975359354420609, 0.3930184804683349, 0.3942505115233897, 0.39712525566990126, 0.39794661126097614, 0.39589322199566895, 0.3983572916573323, 0.3963039009845232, 0.39589322536143434, 0.393429158282231, 0.3946611928988776, 0.39219712480382507, 0.3909650941404229, 0.395893225165608, 0.3954825465684064, 0.3926078030460914, 0.3909650933571175, 0.397125256649033, 0.39589322575308705, 0.395071866563703, 0.3975359358337136, 0.3938398368794326, 0.39630390121706704, 0.39753593465875553, 0.39712525778727364, 0.39178644585168826, 0.39342915726638183, 0.3963038992220861, 0.39671457981426855, 0.39671458020592126, 0.39630390297950413, 0.3967145809892267, 0.3963039025878515, 0.3946611917239195, 0.3938398378585643, 0.3905544145640896, 0.3967145786393105, 0.39589322395393245, 0.3958932235989972, 0.3930184824265983, 0.39630390297950413, 0.3963039023920251, 0.39671457805183147, 0.3967145778560051, 0.3950718707127738, 0.39589322536143434, 0.3946611909406141, 0.3958932232073445, 0.3958932245781289, 0.39383983805439066, 0.39466119191974586, 0.3917864471857553, 0.3950718675795522, 0.39342915926136274, 0.3934291559323148, 0.39178644702664633, 0.3971252589989492, 0.3983572878999142, 0.40082135576242295, 0.39753593207629556, 0.3979466114568025, 0.3958932245781289, 0.38973305772462175, 0.39876796610546306, 0.3995893221249081, 0.39466119149137574, 0.3954825459809274, 0.3963039023920251, 0.3991786445435557, 0.4008213543916385, 0.3963039007886969, 0.3971252570406857, 0.39630389941791244, 0.40451745573255316, 0.4036960991256291, 0.40287474111120314, 0.40410677811448337, 0.4053388074071011, 0.40492812978903125, 0.4024640672505514, 0.3971252562573803, 0.3934291558955974, 0.3905544129607614, 0.3963039005928705, 0.39260780461270217, 0.39425051547663414, 0.3946611893372859, 0.39548254696005913, 0.3934291594571891, 0.3942505141058497, 0.39630389941791244, 0.39630389980956515, 0.400410679551855, 0.39507186738372585, 0.3938398372710853, 0.39589322536143434, 0.3946611913322668, 0.3946611917239195, 0.39671458118505304, 0.39589322301151814, 0.3954825475475382, 0.39507186636787667]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
