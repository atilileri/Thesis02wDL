{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf16.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 17:56:54 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'Front', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['yd', 'ds', 'mb', 'by', 'eg', 'ek', 'aa', 'sg', 'ib', 'sk', 'eb', 'ck', 'ce', 'my', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000220F1009F98>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000220ED7A7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7179, Accuracy:0.0727, Validation Loss:2.7104, Validation Accuracy:0.0887\n",
    "Epoch #2: Loss:2.7038, Accuracy:0.0891, Validation Loss:2.6986, Validation Accuracy:0.0887\n",
    "Epoch #3: Loss:2.6938, Accuracy:0.0908, Validation Loss:2.6908, Validation Accuracy:0.0903\n",
    "Epoch #4: Loss:2.6877, Accuracy:0.0969, Validation Loss:2.6842, Validation Accuracy:0.1100\n",
    "Epoch #5: Loss:2.6819, Accuracy:0.1035, Validation Loss:2.6781, Validation Accuracy:0.1182\n",
    "Epoch #6: Loss:2.6769, Accuracy:0.1051, Validation Loss:2.6738, Validation Accuracy:0.1149\n",
    "Epoch #7: Loss:2.6733, Accuracy:0.0973, Validation Loss:2.6706, Validation Accuracy:0.0969\n",
    "Epoch #8: Loss:2.6688, Accuracy:0.0936, Validation Loss:2.6658, Validation Accuracy:0.0969\n",
    "Epoch #9: Loss:2.6642, Accuracy:0.0953, Validation Loss:2.6608, Validation Accuracy:0.0969\n",
    "Epoch #10: Loss:2.6591, Accuracy:0.0957, Validation Loss:2.6548, Validation Accuracy:0.0985\n",
    "Epoch #11: Loss:2.6526, Accuracy:0.1076, Validation Loss:2.6464, Validation Accuracy:0.1182\n",
    "Epoch #12: Loss:2.6443, Accuracy:0.1166, Validation Loss:2.6374, Validation Accuracy:0.1117\n",
    "Epoch #13: Loss:2.6327, Accuracy:0.1154, Validation Loss:2.6244, Validation Accuracy:0.1133\n",
    "Epoch #14: Loss:2.6238, Accuracy:0.1092, Validation Loss:2.6141, Validation Accuracy:0.1133\n",
    "Epoch #15: Loss:2.6113, Accuracy:0.1117, Validation Loss:2.5999, Validation Accuracy:0.1215\n",
    "Epoch #16: Loss:2.5973, Accuracy:0.1216, Validation Loss:2.5860, Validation Accuracy:0.1100\n",
    "Epoch #17: Loss:2.5809, Accuracy:0.1179, Validation Loss:2.5688, Validation Accuracy:0.1149\n",
    "Epoch #18: Loss:2.5635, Accuracy:0.1265, Validation Loss:2.5501, Validation Accuracy:0.1232\n",
    "Epoch #19: Loss:2.5510, Accuracy:0.1363, Validation Loss:2.5366, Validation Accuracy:0.1494\n",
    "Epoch #20: Loss:2.5383, Accuracy:0.1565, Validation Loss:2.5262, Validation Accuracy:0.1346\n",
    "Epoch #21: Loss:2.5268, Accuracy:0.1520, Validation Loss:2.5171, Validation Accuracy:0.1429\n",
    "Epoch #22: Loss:2.5199, Accuracy:0.1634, Validation Loss:2.5100, Validation Accuracy:0.1429\n",
    "Epoch #23: Loss:2.5132, Accuracy:0.1606, Validation Loss:2.4952, Validation Accuracy:0.1494\n",
    "Epoch #24: Loss:2.5046, Accuracy:0.1585, Validation Loss:2.4907, Validation Accuracy:0.1576\n",
    "Epoch #25: Loss:2.5010, Accuracy:0.1606, Validation Loss:2.4902, Validation Accuracy:0.1494\n",
    "Epoch #26: Loss:2.5017, Accuracy:0.1536, Validation Loss:2.4780, Validation Accuracy:0.1478\n",
    "Epoch #27: Loss:2.4940, Accuracy:0.1610, Validation Loss:2.4714, Validation Accuracy:0.1560\n",
    "Epoch #28: Loss:2.4898, Accuracy:0.1618, Validation Loss:2.4663, Validation Accuracy:0.1511\n",
    "Epoch #29: Loss:2.4863, Accuracy:0.1610, Validation Loss:2.4612, Validation Accuracy:0.1642\n",
    "Epoch #30: Loss:2.4839, Accuracy:0.1643, Validation Loss:2.4557, Validation Accuracy:0.1593\n",
    "Epoch #31: Loss:2.4831, Accuracy:0.1639, Validation Loss:2.4537, Validation Accuracy:0.1658\n",
    "Epoch #32: Loss:2.4811, Accuracy:0.1774, Validation Loss:2.4551, Validation Accuracy:0.1658\n",
    "Epoch #33: Loss:2.4810, Accuracy:0.1684, Validation Loss:2.4496, Validation Accuracy:0.1856\n",
    "Epoch #34: Loss:2.4764, Accuracy:0.1733, Validation Loss:2.4459, Validation Accuracy:0.1658\n",
    "Epoch #35: Loss:2.4891, Accuracy:0.1659, Validation Loss:2.4485, Validation Accuracy:0.1839\n",
    "Epoch #36: Loss:2.4907, Accuracy:0.1598, Validation Loss:2.4402, Validation Accuracy:0.1773\n",
    "Epoch #37: Loss:2.4819, Accuracy:0.1680, Validation Loss:2.4462, Validation Accuracy:0.1609\n",
    "Epoch #38: Loss:2.4718, Accuracy:0.1721, Validation Loss:2.4427, Validation Accuracy:0.1839\n",
    "Epoch #39: Loss:2.4705, Accuracy:0.1766, Validation Loss:2.4395, Validation Accuracy:0.1741\n",
    "Epoch #40: Loss:2.4690, Accuracy:0.1774, Validation Loss:2.4391, Validation Accuracy:0.1823\n",
    "Epoch #41: Loss:2.4658, Accuracy:0.1791, Validation Loss:2.4372, Validation Accuracy:0.1691\n",
    "Epoch #42: Loss:2.4641, Accuracy:0.1774, Validation Loss:2.4349, Validation Accuracy:0.1691\n",
    "Epoch #43: Loss:2.4633, Accuracy:0.1770, Validation Loss:2.4348, Validation Accuracy:0.1691\n",
    "Epoch #44: Loss:2.4634, Accuracy:0.1737, Validation Loss:2.4351, Validation Accuracy:0.1790\n",
    "Epoch #45: Loss:2.4629, Accuracy:0.1725, Validation Loss:2.4346, Validation Accuracy:0.1839\n",
    "Epoch #46: Loss:2.4618, Accuracy:0.1791, Validation Loss:2.4314, Validation Accuracy:0.1839\n",
    "Epoch #47: Loss:2.4636, Accuracy:0.1770, Validation Loss:2.4354, Validation Accuracy:0.1856\n",
    "Epoch #48: Loss:2.4617, Accuracy:0.1791, Validation Loss:2.4359, Validation Accuracy:0.1790\n",
    "Epoch #49: Loss:2.4623, Accuracy:0.1770, Validation Loss:2.4353, Validation Accuracy:0.1790\n",
    "Epoch #50: Loss:2.4638, Accuracy:0.1721, Validation Loss:2.4328, Validation Accuracy:0.1741\n",
    "Epoch #51: Loss:2.4619, Accuracy:0.1782, Validation Loss:2.4312, Validation Accuracy:0.1757\n",
    "Epoch #52: Loss:2.4623, Accuracy:0.1758, Validation Loss:2.4319, Validation Accuracy:0.1741\n",
    "Epoch #53: Loss:2.4603, Accuracy:0.1782, Validation Loss:2.4309, Validation Accuracy:0.1823\n",
    "Epoch #54: Loss:2.4614, Accuracy:0.1774, Validation Loss:2.4302, Validation Accuracy:0.1741\n",
    "Epoch #55: Loss:2.4586, Accuracy:0.1844, Validation Loss:2.4295, Validation Accuracy:0.1790\n",
    "Epoch #56: Loss:2.4582, Accuracy:0.1803, Validation Loss:2.4276, Validation Accuracy:0.1790\n",
    "Epoch #57: Loss:2.4582, Accuracy:0.1807, Validation Loss:2.4285, Validation Accuracy:0.1856\n",
    "Epoch #58: Loss:2.4556, Accuracy:0.1774, Validation Loss:2.4282, Validation Accuracy:0.1823\n",
    "Epoch #59: Loss:2.4555, Accuracy:0.1795, Validation Loss:2.4275, Validation Accuracy:0.1806\n",
    "Epoch #60: Loss:2.4575, Accuracy:0.1778, Validation Loss:2.4319, Validation Accuracy:0.1773\n",
    "Epoch #61: Loss:2.4558, Accuracy:0.1807, Validation Loss:2.4315, Validation Accuracy:0.1708\n",
    "Epoch #62: Loss:2.4534, Accuracy:0.1823, Validation Loss:2.4314, Validation Accuracy:0.1806\n",
    "Epoch #63: Loss:2.4538, Accuracy:0.1782, Validation Loss:2.4311, Validation Accuracy:0.1658\n",
    "Epoch #64: Loss:2.4525, Accuracy:0.1799, Validation Loss:2.4321, Validation Accuracy:0.1708\n",
    "Epoch #65: Loss:2.4517, Accuracy:0.1778, Validation Loss:2.4309, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4529, Accuracy:0.1749, Validation Loss:2.4270, Validation Accuracy:0.1790\n",
    "Epoch #67: Loss:2.4517, Accuracy:0.1766, Validation Loss:2.4256, Validation Accuracy:0.1658\n",
    "Epoch #68: Loss:2.4553, Accuracy:0.1766, Validation Loss:2.4245, Validation Accuracy:0.1773\n",
    "Epoch #69: Loss:2.4536, Accuracy:0.1774, Validation Loss:2.4234, Validation Accuracy:0.1757\n",
    "Epoch #70: Loss:2.4549, Accuracy:0.1774, Validation Loss:2.4233, Validation Accuracy:0.1806\n",
    "Epoch #71: Loss:2.4525, Accuracy:0.1741, Validation Loss:2.4209, Validation Accuracy:0.1626\n",
    "Epoch #72: Loss:2.4549, Accuracy:0.1778, Validation Loss:2.4187, Validation Accuracy:0.1806\n",
    "Epoch #73: Loss:2.4537, Accuracy:0.1749, Validation Loss:2.4179, Validation Accuracy:0.1675\n",
    "Epoch #74: Loss:2.4555, Accuracy:0.1770, Validation Loss:2.4188, Validation Accuracy:0.1823\n",
    "Epoch #75: Loss:2.4559, Accuracy:0.1725, Validation Loss:2.4176, Validation Accuracy:0.1658\n",
    "Epoch #76: Loss:2.4529, Accuracy:0.1791, Validation Loss:2.4195, Validation Accuracy:0.1823\n",
    "Epoch #77: Loss:2.4521, Accuracy:0.1795, Validation Loss:2.4181, Validation Accuracy:0.1642\n",
    "Epoch #78: Loss:2.4527, Accuracy:0.1791, Validation Loss:2.4234, Validation Accuracy:0.1905\n",
    "Epoch #79: Loss:2.4526, Accuracy:0.1774, Validation Loss:2.4231, Validation Accuracy:0.1626\n",
    "Epoch #80: Loss:2.4509, Accuracy:0.1786, Validation Loss:2.4236, Validation Accuracy:0.1773\n",
    "Epoch #81: Loss:2.4522, Accuracy:0.1749, Validation Loss:2.4244, Validation Accuracy:0.1626\n",
    "Epoch #82: Loss:2.4489, Accuracy:0.1762, Validation Loss:2.4224, Validation Accuracy:0.1626\n",
    "Epoch #83: Loss:2.4513, Accuracy:0.1803, Validation Loss:2.4232, Validation Accuracy:0.1757\n",
    "Epoch #84: Loss:2.4509, Accuracy:0.1795, Validation Loss:2.4221, Validation Accuracy:0.1642\n",
    "Epoch #85: Loss:2.4515, Accuracy:0.1758, Validation Loss:2.4224, Validation Accuracy:0.1658\n",
    "Epoch #86: Loss:2.4511, Accuracy:0.1749, Validation Loss:2.4215, Validation Accuracy:0.1675\n",
    "Epoch #87: Loss:2.4503, Accuracy:0.1807, Validation Loss:2.4209, Validation Accuracy:0.1658\n",
    "Epoch #88: Loss:2.4505, Accuracy:0.1778, Validation Loss:2.4195, Validation Accuracy:0.1773\n",
    "Epoch #89: Loss:2.4500, Accuracy:0.1782, Validation Loss:2.4195, Validation Accuracy:0.1724\n",
    "Epoch #90: Loss:2.4490, Accuracy:0.1766, Validation Loss:2.4188, Validation Accuracy:0.1691\n",
    "Epoch #91: Loss:2.4492, Accuracy:0.1836, Validation Loss:2.4204, Validation Accuracy:0.1823\n",
    "Epoch #92: Loss:2.4503, Accuracy:0.1786, Validation Loss:2.4190, Validation Accuracy:0.1773\n",
    "Epoch #93: Loss:2.4491, Accuracy:0.1774, Validation Loss:2.4194, Validation Accuracy:0.1691\n",
    "Epoch #94: Loss:2.4496, Accuracy:0.1733, Validation Loss:2.4208, Validation Accuracy:0.1691\n",
    "Epoch #95: Loss:2.4486, Accuracy:0.1741, Validation Loss:2.4190, Validation Accuracy:0.1741\n",
    "Epoch #96: Loss:2.4504, Accuracy:0.1782, Validation Loss:2.4171, Validation Accuracy:0.1823\n",
    "Epoch #97: Loss:2.4505, Accuracy:0.1786, Validation Loss:2.4184, Validation Accuracy:0.1806\n",
    "Epoch #98: Loss:2.4517, Accuracy:0.1762, Validation Loss:2.4177, Validation Accuracy:0.1806\n",
    "Epoch #99: Loss:2.4511, Accuracy:0.1754, Validation Loss:2.4203, Validation Accuracy:0.1773\n",
    "Epoch #100: Loss:2.4511, Accuracy:0.1803, Validation Loss:2.4232, Validation Accuracy:0.1724\n",
    "Epoch #101: Loss:2.4502, Accuracy:0.1799, Validation Loss:2.4209, Validation Accuracy:0.1724\n",
    "Epoch #102: Loss:2.4497, Accuracy:0.1811, Validation Loss:2.4202, Validation Accuracy:0.1724\n",
    "Epoch #103: Loss:2.4508, Accuracy:0.1774, Validation Loss:2.4266, Validation Accuracy:0.1790\n",
    "Epoch #104: Loss:2.4545, Accuracy:0.1782, Validation Loss:2.4272, Validation Accuracy:0.1691\n",
    "Epoch #105: Loss:2.4529, Accuracy:0.1754, Validation Loss:2.4246, Validation Accuracy:0.1741\n",
    "Epoch #106: Loss:2.4523, Accuracy:0.1791, Validation Loss:2.4240, Validation Accuracy:0.1790\n",
    "Epoch #107: Loss:2.4520, Accuracy:0.1778, Validation Loss:2.4195, Validation Accuracy:0.1823\n",
    "Epoch #108: Loss:2.4498, Accuracy:0.1786, Validation Loss:2.4269, Validation Accuracy:0.1856\n",
    "Epoch #109: Loss:2.4547, Accuracy:0.1778, Validation Loss:2.4241, Validation Accuracy:0.1773\n",
    "Epoch #110: Loss:2.4526, Accuracy:0.1758, Validation Loss:2.4230, Validation Accuracy:0.1823\n",
    "Epoch #111: Loss:2.4514, Accuracy:0.1819, Validation Loss:2.4195, Validation Accuracy:0.1773\n",
    "Epoch #112: Loss:2.4526, Accuracy:0.1791, Validation Loss:2.4227, Validation Accuracy:0.1790\n",
    "Epoch #113: Loss:2.4499, Accuracy:0.1811, Validation Loss:2.4213, Validation Accuracy:0.1757\n",
    "Epoch #114: Loss:2.4499, Accuracy:0.1852, Validation Loss:2.4249, Validation Accuracy:0.1724\n",
    "Epoch #115: Loss:2.4498, Accuracy:0.1811, Validation Loss:2.4201, Validation Accuracy:0.1773\n",
    "Epoch #116: Loss:2.4480, Accuracy:0.1823, Validation Loss:2.4197, Validation Accuracy:0.1806\n",
    "Epoch #117: Loss:2.4479, Accuracy:0.1840, Validation Loss:2.4203, Validation Accuracy:0.1790\n",
    "Epoch #118: Loss:2.4473, Accuracy:0.1815, Validation Loss:2.4189, Validation Accuracy:0.1741\n",
    "Epoch #119: Loss:2.4475, Accuracy:0.1811, Validation Loss:2.4203, Validation Accuracy:0.1773\n",
    "Epoch #120: Loss:2.4491, Accuracy:0.1778, Validation Loss:2.4205, Validation Accuracy:0.1724\n",
    "Epoch #121: Loss:2.4500, Accuracy:0.1786, Validation Loss:2.4197, Validation Accuracy:0.1806\n",
    "Epoch #122: Loss:2.4482, Accuracy:0.1807, Validation Loss:2.4204, Validation Accuracy:0.1806\n",
    "Epoch #123: Loss:2.4482, Accuracy:0.1807, Validation Loss:2.4219, Validation Accuracy:0.1773\n",
    "Epoch #124: Loss:2.4483, Accuracy:0.1774, Validation Loss:2.4228, Validation Accuracy:0.1724\n",
    "Epoch #125: Loss:2.4491, Accuracy:0.1807, Validation Loss:2.4194, Validation Accuracy:0.1773\n",
    "Epoch #126: Loss:2.4471, Accuracy:0.1758, Validation Loss:2.4209, Validation Accuracy:0.1823\n",
    "Epoch #127: Loss:2.4451, Accuracy:0.1803, Validation Loss:2.4172, Validation Accuracy:0.1806\n",
    "Epoch #128: Loss:2.4448, Accuracy:0.1795, Validation Loss:2.4210, Validation Accuracy:0.1790\n",
    "Epoch #129: Loss:2.4431, Accuracy:0.1823, Validation Loss:2.4169, Validation Accuracy:0.1823\n",
    "Epoch #130: Loss:2.4445, Accuracy:0.1844, Validation Loss:2.4171, Validation Accuracy:0.1806\n",
    "Epoch #131: Loss:2.4439, Accuracy:0.1848, Validation Loss:2.4166, Validation Accuracy:0.1806\n",
    "Epoch #132: Loss:2.4427, Accuracy:0.1815, Validation Loss:2.4175, Validation Accuracy:0.1823\n",
    "Epoch #133: Loss:2.4423, Accuracy:0.1860, Validation Loss:2.4144, Validation Accuracy:0.1823\n",
    "Epoch #134: Loss:2.4419, Accuracy:0.1856, Validation Loss:2.4159, Validation Accuracy:0.1790\n",
    "Epoch #135: Loss:2.4418, Accuracy:0.1864, Validation Loss:2.4156, Validation Accuracy:0.1773\n",
    "Epoch #136: Loss:2.4408, Accuracy:0.1856, Validation Loss:2.4161, Validation Accuracy:0.1691\n",
    "Epoch #137: Loss:2.4410, Accuracy:0.1844, Validation Loss:2.4145, Validation Accuracy:0.1757\n",
    "Epoch #138: Loss:2.4407, Accuracy:0.1885, Validation Loss:2.4164, Validation Accuracy:0.1741\n",
    "Epoch #139: Loss:2.4428, Accuracy:0.1840, Validation Loss:2.4141, Validation Accuracy:0.1806\n",
    "Epoch #140: Loss:2.4436, Accuracy:0.1828, Validation Loss:2.4151, Validation Accuracy:0.1806\n",
    "Epoch #141: Loss:2.4447, Accuracy:0.1852, Validation Loss:2.4170, Validation Accuracy:0.1790\n",
    "Epoch #142: Loss:2.4464, Accuracy:0.1782, Validation Loss:2.4167, Validation Accuracy:0.1773\n",
    "Epoch #143: Loss:2.4458, Accuracy:0.1795, Validation Loss:2.4196, Validation Accuracy:0.1675\n",
    "Epoch #144: Loss:2.4453, Accuracy:0.1778, Validation Loss:2.4167, Validation Accuracy:0.1773\n",
    "Epoch #145: Loss:2.4438, Accuracy:0.1770, Validation Loss:2.4188, Validation Accuracy:0.1790\n",
    "Epoch #146: Loss:2.4433, Accuracy:0.1799, Validation Loss:2.4160, Validation Accuracy:0.1757\n",
    "Epoch #147: Loss:2.4421, Accuracy:0.1815, Validation Loss:2.4196, Validation Accuracy:0.1741\n",
    "Epoch #148: Loss:2.4448, Accuracy:0.1795, Validation Loss:2.4181, Validation Accuracy:0.1741\n",
    "Epoch #149: Loss:2.4434, Accuracy:0.1811, Validation Loss:2.4175, Validation Accuracy:0.1741\n",
    "Epoch #150: Loss:2.4438, Accuracy:0.1799, Validation Loss:2.4180, Validation Accuracy:0.1708\n",
    "Epoch #151: Loss:2.4431, Accuracy:0.1844, Validation Loss:2.4166, Validation Accuracy:0.1757\n",
    "Epoch #152: Loss:2.4455, Accuracy:0.1807, Validation Loss:2.4191, Validation Accuracy:0.1708\n",
    "Epoch #153: Loss:2.4449, Accuracy:0.1778, Validation Loss:2.4159, Validation Accuracy:0.1773\n",
    "Epoch #154: Loss:2.4456, Accuracy:0.1815, Validation Loss:2.4161, Validation Accuracy:0.1773\n",
    "Epoch #155: Loss:2.4440, Accuracy:0.1758, Validation Loss:2.4201, Validation Accuracy:0.1691\n",
    "Epoch #156: Loss:2.4443, Accuracy:0.1754, Validation Loss:2.4195, Validation Accuracy:0.1773\n",
    "Epoch #157: Loss:2.4462, Accuracy:0.1799, Validation Loss:2.4194, Validation Accuracy:0.1741\n",
    "Epoch #158: Loss:2.4442, Accuracy:0.1799, Validation Loss:2.4205, Validation Accuracy:0.1691\n",
    "Epoch #159: Loss:2.4425, Accuracy:0.1832, Validation Loss:2.4197, Validation Accuracy:0.1773\n",
    "Epoch #160: Loss:2.4446, Accuracy:0.1766, Validation Loss:2.4201, Validation Accuracy:0.1757\n",
    "Epoch #161: Loss:2.4456, Accuracy:0.1819, Validation Loss:2.4187, Validation Accuracy:0.1757\n",
    "Epoch #162: Loss:2.4441, Accuracy:0.1786, Validation Loss:2.4233, Validation Accuracy:0.1790\n",
    "Epoch #163: Loss:2.4435, Accuracy:0.1807, Validation Loss:2.4209, Validation Accuracy:0.1757\n",
    "Epoch #164: Loss:2.4424, Accuracy:0.1774, Validation Loss:2.4253, Validation Accuracy:0.1773\n",
    "Epoch #165: Loss:2.4430, Accuracy:0.1725, Validation Loss:2.4212, Validation Accuracy:0.1675\n",
    "Epoch #166: Loss:2.4411, Accuracy:0.1782, Validation Loss:2.4235, Validation Accuracy:0.1626\n",
    "Epoch #167: Loss:2.4399, Accuracy:0.1791, Validation Loss:2.4220, Validation Accuracy:0.1626\n",
    "Epoch #168: Loss:2.4391, Accuracy:0.1815, Validation Loss:2.4194, Validation Accuracy:0.1691\n",
    "Epoch #169: Loss:2.4379, Accuracy:0.1819, Validation Loss:2.4196, Validation Accuracy:0.1675\n",
    "Epoch #170: Loss:2.4399, Accuracy:0.1823, Validation Loss:2.4165, Validation Accuracy:0.1724\n",
    "Epoch #171: Loss:2.4403, Accuracy:0.1807, Validation Loss:2.4177, Validation Accuracy:0.1642\n",
    "Epoch #172: Loss:2.4404, Accuracy:0.1844, Validation Loss:2.4205, Validation Accuracy:0.1724\n",
    "Epoch #173: Loss:2.4396, Accuracy:0.1823, Validation Loss:2.4207, Validation Accuracy:0.1708\n",
    "Epoch #174: Loss:2.4386, Accuracy:0.1844, Validation Loss:2.4207, Validation Accuracy:0.1823\n",
    "Epoch #175: Loss:2.4367, Accuracy:0.1832, Validation Loss:2.4187, Validation Accuracy:0.1642\n",
    "Epoch #176: Loss:2.4391, Accuracy:0.1799, Validation Loss:2.4247, Validation Accuracy:0.1724\n",
    "Epoch #177: Loss:2.4398, Accuracy:0.1828, Validation Loss:2.4195, Validation Accuracy:0.1708\n",
    "Epoch #178: Loss:2.4371, Accuracy:0.1828, Validation Loss:2.4198, Validation Accuracy:0.1675\n",
    "Epoch #179: Loss:2.4356, Accuracy:0.1844, Validation Loss:2.4221, Validation Accuracy:0.1658\n",
    "Epoch #180: Loss:2.4362, Accuracy:0.1795, Validation Loss:2.4208, Validation Accuracy:0.1708\n",
    "Epoch #181: Loss:2.4369, Accuracy:0.1782, Validation Loss:2.4199, Validation Accuracy:0.1708\n",
    "Epoch #182: Loss:2.4369, Accuracy:0.1758, Validation Loss:2.4171, Validation Accuracy:0.1642\n",
    "Epoch #183: Loss:2.4375, Accuracy:0.1782, Validation Loss:2.4182, Validation Accuracy:0.1576\n",
    "Epoch #184: Loss:2.4370, Accuracy:0.1754, Validation Loss:2.4232, Validation Accuracy:0.1544\n",
    "Epoch #185: Loss:2.4387, Accuracy:0.1729, Validation Loss:2.4250, Validation Accuracy:0.1691\n",
    "Epoch #186: Loss:2.4421, Accuracy:0.1786, Validation Loss:2.4261, Validation Accuracy:0.1708\n",
    "Epoch #187: Loss:2.4413, Accuracy:0.1754, Validation Loss:2.4284, Validation Accuracy:0.1658\n",
    "Epoch #188: Loss:2.4411, Accuracy:0.1803, Validation Loss:2.4335, Validation Accuracy:0.1658\n",
    "Epoch #189: Loss:2.4395, Accuracy:0.1832, Validation Loss:2.4251, Validation Accuracy:0.1658\n",
    "Epoch #190: Loss:2.4377, Accuracy:0.1782, Validation Loss:2.4235, Validation Accuracy:0.1675\n",
    "Epoch #191: Loss:2.4363, Accuracy:0.1799, Validation Loss:2.4262, Validation Accuracy:0.1708\n",
    "Epoch #192: Loss:2.4347, Accuracy:0.1828, Validation Loss:2.4260, Validation Accuracy:0.1691\n",
    "Epoch #193: Loss:2.4330, Accuracy:0.1823, Validation Loss:2.4255, Validation Accuracy:0.1675\n",
    "Epoch #194: Loss:2.4322, Accuracy:0.1819, Validation Loss:2.4250, Validation Accuracy:0.1708\n",
    "Epoch #195: Loss:2.4347, Accuracy:0.1844, Validation Loss:2.4203, Validation Accuracy:0.1708\n",
    "Epoch #196: Loss:2.4336, Accuracy:0.1823, Validation Loss:2.4199, Validation Accuracy:0.1658\n",
    "Epoch #197: Loss:2.4350, Accuracy:0.1823, Validation Loss:2.4199, Validation Accuracy:0.1642\n",
    "Epoch #198: Loss:2.4366, Accuracy:0.1844, Validation Loss:2.4228, Validation Accuracy:0.1658\n",
    "Epoch #199: Loss:2.4369, Accuracy:0.1819, Validation Loss:2.4250, Validation Accuracy:0.1658\n",
    "Epoch #200: Loss:2.4351, Accuracy:0.1799, Validation Loss:2.4218, Validation Accuracy:0.1626\n",
    "Epoch #201: Loss:2.4339, Accuracy:0.1828, Validation Loss:2.4258, Validation Accuracy:0.1724\n",
    "Epoch #202: Loss:2.4344, Accuracy:0.1819, Validation Loss:2.4205, Validation Accuracy:0.1576\n",
    "Epoch #203: Loss:2.4332, Accuracy:0.1815, Validation Loss:2.4243, Validation Accuracy:0.1626\n",
    "Epoch #204: Loss:2.4346, Accuracy:0.1828, Validation Loss:2.4210, Validation Accuracy:0.1675\n",
    "Epoch #205: Loss:2.4348, Accuracy:0.1807, Validation Loss:2.4200, Validation Accuracy:0.1675\n",
    "Epoch #206: Loss:2.4316, Accuracy:0.1836, Validation Loss:2.4240, Validation Accuracy:0.1708\n",
    "Epoch #207: Loss:2.4342, Accuracy:0.1836, Validation Loss:2.4186, Validation Accuracy:0.1675\n",
    "Epoch #208: Loss:2.4361, Accuracy:0.1786, Validation Loss:2.4240, Validation Accuracy:0.1675\n",
    "Epoch #209: Loss:2.4347, Accuracy:0.1840, Validation Loss:2.4231, Validation Accuracy:0.1626\n",
    "Epoch #210: Loss:2.4331, Accuracy:0.1819, Validation Loss:2.4219, Validation Accuracy:0.1609\n",
    "Epoch #211: Loss:2.4316, Accuracy:0.1807, Validation Loss:2.4270, Validation Accuracy:0.1609\n",
    "Epoch #212: Loss:2.4333, Accuracy:0.1823, Validation Loss:2.4239, Validation Accuracy:0.1626\n",
    "Epoch #213: Loss:2.4331, Accuracy:0.1836, Validation Loss:2.4249, Validation Accuracy:0.1593\n",
    "Epoch #214: Loss:2.4320, Accuracy:0.1828, Validation Loss:2.4235, Validation Accuracy:0.1609\n",
    "Epoch #215: Loss:2.4341, Accuracy:0.1795, Validation Loss:2.4219, Validation Accuracy:0.1658\n",
    "Epoch #216: Loss:2.4345, Accuracy:0.1799, Validation Loss:2.4233, Validation Accuracy:0.1642\n",
    "Epoch #217: Loss:2.4361, Accuracy:0.1778, Validation Loss:2.4204, Validation Accuracy:0.1642\n",
    "Epoch #218: Loss:2.4367, Accuracy:0.1807, Validation Loss:2.4235, Validation Accuracy:0.1609\n",
    "Epoch #219: Loss:2.4372, Accuracy:0.1795, Validation Loss:2.4240, Validation Accuracy:0.1675\n",
    "Epoch #220: Loss:2.4337, Accuracy:0.1811, Validation Loss:2.4282, Validation Accuracy:0.1609\n",
    "Epoch #221: Loss:2.4319, Accuracy:0.1832, Validation Loss:2.4227, Validation Accuracy:0.1626\n",
    "Epoch #222: Loss:2.4323, Accuracy:0.1823, Validation Loss:2.4240, Validation Accuracy:0.1642\n",
    "Epoch #223: Loss:2.4335, Accuracy:0.1741, Validation Loss:2.4243, Validation Accuracy:0.1609\n",
    "Epoch #224: Loss:2.4352, Accuracy:0.1811, Validation Loss:2.4268, Validation Accuracy:0.1609\n",
    "Epoch #225: Loss:2.4349, Accuracy:0.1770, Validation Loss:2.4245, Validation Accuracy:0.1576\n",
    "Epoch #226: Loss:2.4315, Accuracy:0.1782, Validation Loss:2.4279, Validation Accuracy:0.1609\n",
    "Epoch #227: Loss:2.4342, Accuracy:0.1807, Validation Loss:2.4243, Validation Accuracy:0.1576\n",
    "Epoch #228: Loss:2.4346, Accuracy:0.1807, Validation Loss:2.4261, Validation Accuracy:0.1642\n",
    "Epoch #229: Loss:2.4351, Accuracy:0.1770, Validation Loss:2.4260, Validation Accuracy:0.1626\n",
    "Epoch #230: Loss:2.4341, Accuracy:0.1766, Validation Loss:2.4269, Validation Accuracy:0.1658\n",
    "Epoch #231: Loss:2.4330, Accuracy:0.1803, Validation Loss:2.4323, Validation Accuracy:0.1675\n",
    "Epoch #232: Loss:2.4317, Accuracy:0.1815, Validation Loss:2.4336, Validation Accuracy:0.1691\n",
    "Epoch #233: Loss:2.4316, Accuracy:0.1844, Validation Loss:2.4319, Validation Accuracy:0.1691\n",
    "Epoch #234: Loss:2.4318, Accuracy:0.1811, Validation Loss:2.4289, Validation Accuracy:0.1708\n",
    "Epoch #235: Loss:2.4308, Accuracy:0.1856, Validation Loss:2.4320, Validation Accuracy:0.1691\n",
    "Epoch #236: Loss:2.4299, Accuracy:0.1811, Validation Loss:2.4299, Validation Accuracy:0.1675\n",
    "Epoch #237: Loss:2.4324, Accuracy:0.1791, Validation Loss:2.4321, Validation Accuracy:0.1658\n",
    "Epoch #238: Loss:2.4331, Accuracy:0.1823, Validation Loss:2.4290, Validation Accuracy:0.1544\n",
    "Epoch #239: Loss:2.4336, Accuracy:0.1749, Validation Loss:2.4320, Validation Accuracy:0.1708\n",
    "Epoch #240: Loss:2.4343, Accuracy:0.1823, Validation Loss:2.4257, Validation Accuracy:0.1708\n",
    "Epoch #241: Loss:2.4329, Accuracy:0.1791, Validation Loss:2.4252, Validation Accuracy:0.1691\n",
    "Epoch #242: Loss:2.4337, Accuracy:0.1791, Validation Loss:2.4252, Validation Accuracy:0.1691\n",
    "Epoch #243: Loss:2.4360, Accuracy:0.1799, Validation Loss:2.4223, Validation Accuracy:0.1609\n",
    "Epoch #244: Loss:2.4364, Accuracy:0.1762, Validation Loss:2.4220, Validation Accuracy:0.1642\n",
    "Epoch #245: Loss:2.4364, Accuracy:0.1782, Validation Loss:2.4252, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4343, Accuracy:0.1811, Validation Loss:2.4253, Validation Accuracy:0.1691\n",
    "Epoch #247: Loss:2.4347, Accuracy:0.1795, Validation Loss:2.4269, Validation Accuracy:0.1675\n",
    "Epoch #248: Loss:2.4341, Accuracy:0.1786, Validation Loss:2.4215, Validation Accuracy:0.1708\n",
    "Epoch #249: Loss:2.4368, Accuracy:0.1815, Validation Loss:2.4225, Validation Accuracy:0.1576\n",
    "Epoch #250: Loss:2.4355, Accuracy:0.1786, Validation Loss:2.4210, Validation Accuracy:0.1609\n",
    "Epoch #251: Loss:2.4351, Accuracy:0.1811, Validation Loss:2.4211, Validation Accuracy:0.1724\n",
    "Epoch #252: Loss:2.4353, Accuracy:0.1836, Validation Loss:2.4239, Validation Accuracy:0.1675\n",
    "Epoch #253: Loss:2.4348, Accuracy:0.1819, Validation Loss:2.4282, Validation Accuracy:0.1658\n",
    "Epoch #254: Loss:2.4361, Accuracy:0.1786, Validation Loss:2.4275, Validation Accuracy:0.1790\n",
    "Epoch #255: Loss:2.4356, Accuracy:0.1873, Validation Loss:2.4352, Validation Accuracy:0.1741\n",
    "Epoch #256: Loss:2.4378, Accuracy:0.1815, Validation Loss:2.4306, Validation Accuracy:0.1593\n",
    "Epoch #257: Loss:2.4387, Accuracy:0.1807, Validation Loss:2.4339, Validation Accuracy:0.1708\n",
    "Epoch #258: Loss:2.4361, Accuracy:0.1795, Validation Loss:2.4228, Validation Accuracy:0.1741\n",
    "Epoch #259: Loss:2.4341, Accuracy:0.1836, Validation Loss:2.4156, Validation Accuracy:0.1773\n",
    "Epoch #260: Loss:2.4380, Accuracy:0.1749, Validation Loss:2.4167, Validation Accuracy:0.1757\n",
    "Epoch #261: Loss:2.4369, Accuracy:0.1811, Validation Loss:2.4141, Validation Accuracy:0.1823\n",
    "Epoch #262: Loss:2.4357, Accuracy:0.1758, Validation Loss:2.4172, Validation Accuracy:0.1724\n",
    "Epoch #263: Loss:2.4348, Accuracy:0.1770, Validation Loss:2.4142, Validation Accuracy:0.1806\n",
    "Epoch #264: Loss:2.4345, Accuracy:0.1823, Validation Loss:2.4205, Validation Accuracy:0.1741\n",
    "Epoch #265: Loss:2.4367, Accuracy:0.1782, Validation Loss:2.4142, Validation Accuracy:0.1790\n",
    "Epoch #266: Loss:2.4359, Accuracy:0.1828, Validation Loss:2.4180, Validation Accuracy:0.1790\n",
    "Epoch #267: Loss:2.4333, Accuracy:0.1819, Validation Loss:2.4200, Validation Accuracy:0.1691\n",
    "Epoch #268: Loss:2.4318, Accuracy:0.1807, Validation Loss:2.4191, Validation Accuracy:0.1757\n",
    "Epoch #269: Loss:2.4353, Accuracy:0.1819, Validation Loss:2.4247, Validation Accuracy:0.1724\n",
    "Epoch #270: Loss:2.4372, Accuracy:0.1795, Validation Loss:2.4230, Validation Accuracy:0.1724\n",
    "Epoch #271: Loss:2.4369, Accuracy:0.1795, Validation Loss:2.4183, Validation Accuracy:0.1790\n",
    "Epoch #272: Loss:2.4323, Accuracy:0.1811, Validation Loss:2.4266, Validation Accuracy:0.1724\n",
    "Epoch #273: Loss:2.4329, Accuracy:0.1786, Validation Loss:2.4151, Validation Accuracy:0.1806\n",
    "Epoch #274: Loss:2.4341, Accuracy:0.1815, Validation Loss:2.4164, Validation Accuracy:0.1823\n",
    "Epoch #275: Loss:2.4340, Accuracy:0.1807, Validation Loss:2.4154, Validation Accuracy:0.1757\n",
    "Epoch #276: Loss:2.4346, Accuracy:0.1799, Validation Loss:2.4185, Validation Accuracy:0.1757\n",
    "Epoch #277: Loss:2.4345, Accuracy:0.1807, Validation Loss:2.4181, Validation Accuracy:0.1773\n",
    "Epoch #278: Loss:2.4349, Accuracy:0.1774, Validation Loss:2.4152, Validation Accuracy:0.1823\n",
    "Epoch #279: Loss:2.4334, Accuracy:0.1811, Validation Loss:2.4154, Validation Accuracy:0.1790\n",
    "Epoch #280: Loss:2.4341, Accuracy:0.1815, Validation Loss:2.4159, Validation Accuracy:0.1691\n",
    "Epoch #281: Loss:2.4351, Accuracy:0.1811, Validation Loss:2.4173, Validation Accuracy:0.1708\n",
    "Epoch #282: Loss:2.4338, Accuracy:0.1811, Validation Loss:2.4169, Validation Accuracy:0.1839\n",
    "Epoch #283: Loss:2.4367, Accuracy:0.1766, Validation Loss:2.4213, Validation Accuracy:0.1708\n",
    "Epoch #284: Loss:2.4363, Accuracy:0.1836, Validation Loss:2.4175, Validation Accuracy:0.1872\n",
    "Epoch #285: Loss:2.4375, Accuracy:0.1791, Validation Loss:2.4236, Validation Accuracy:0.1724\n",
    "Epoch #286: Loss:2.4378, Accuracy:0.1848, Validation Loss:2.4195, Validation Accuracy:0.1856\n",
    "Epoch #287: Loss:2.4366, Accuracy:0.1836, Validation Loss:2.4284, Validation Accuracy:0.1839\n",
    "Epoch #288: Loss:2.4374, Accuracy:0.1819, Validation Loss:2.4265, Validation Accuracy:0.1856\n",
    "Epoch #289: Loss:2.4378, Accuracy:0.1815, Validation Loss:2.4283, Validation Accuracy:0.1872\n",
    "Epoch #290: Loss:2.4384, Accuracy:0.1852, Validation Loss:2.4231, Validation Accuracy:0.1839\n",
    "Epoch #291: Loss:2.4408, Accuracy:0.1856, Validation Loss:2.4188, Validation Accuracy:0.1839\n",
    "Epoch #292: Loss:2.4419, Accuracy:0.1856, Validation Loss:2.4220, Validation Accuracy:0.1905\n",
    "Epoch #293: Loss:2.4421, Accuracy:0.1873, Validation Loss:2.4193, Validation Accuracy:0.1872\n",
    "Epoch #294: Loss:2.4417, Accuracy:0.1889, Validation Loss:2.4201, Validation Accuracy:0.1856\n",
    "Epoch #295: Loss:2.4426, Accuracy:0.1860, Validation Loss:2.4174, Validation Accuracy:0.1888\n",
    "Epoch #296: Loss:2.4394, Accuracy:0.1852, Validation Loss:2.4169, Validation Accuracy:0.1872\n",
    "Epoch #297: Loss:2.4387, Accuracy:0.1860, Validation Loss:2.4176, Validation Accuracy:0.1872\n",
    "Epoch #298: Loss:2.4383, Accuracy:0.1844, Validation Loss:2.4196, Validation Accuracy:0.1872\n",
    "Epoch #299: Loss:2.4370, Accuracy:0.1848, Validation Loss:2.4236, Validation Accuracy:0.1888\n",
    "Epoch #300: Loss:2.4377, Accuracy:0.1864, Validation Loss:2.4224, Validation Accuracy:0.1823\n",
    "\n",
    "Test:\n",
    "Test Loss:2.42239213, Accuracy:0.1823\n",
    "Labels: ['yd', 'ds', 'mb', 'by', 'eg', 'ek', 'aa', 'sg', 'ib', 'sk', 'eb', 'ck', 'ce', 'my', 'eo']\n",
    "Confusion Matrix:\n",
    "      yd  ds  mb  by  eg  ek  aa  sg  ib  sk  eb  ck  ce  my  eo\n",
    "t:yd  38   0   0   0   0   0   0  22   2   0   0   0   0   0   0\n",
    "t:ds   0   7   0   0  11   0   0  11   0   0   2   0   0   0   0\n",
    "t:mb   7   1   0   1   8   0   0  33   0   0   2   0   0   0   0\n",
    "t:by   3   2   0   1  13   0   0  15   0   0   6   0   0   0   0\n",
    "t:eg   1   5   0   0  25   0   0  10   0   0   9   0   0   0   0\n",
    "t:ek   6   2   0   0  13   0   0  17   4   0   6   0   0   0   0\n",
    "t:aa   1   6   0   1  16   0   0   6   0   0   4   0   0   0   0\n",
    "t:sg  12   0   0   0   2   0   0  34   0   0   3   0   0   0   0\n",
    "t:ib  32   0   0   0   1   0   0  18   2   0   1   0   0   0   0\n",
    "t:sk   5   2   0   0  13   0   0   9   0   0   4   0   0   0   0\n",
    "t:eb   9   2   0   2  13   0   0  19   1   0   4   0   0   0   0\n",
    "t:ck   1   2   0   0  14   0   0   4   0   0   2   0   0   0   0\n",
    "t:ce   3   0   0   1   5   0   0  16   0   0   2   0   0   0   0\n",
    "t:my   6   4   0   0   3   0   0   6   0   0   1   0   0   0   0\n",
    "t:eo   4   0   0   0   0   0   0  26   0   0   4   0   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.30      0.61      0.40        62\n",
    "          ds       0.21      0.23      0.22        31\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          by       0.17      0.03      0.04        40\n",
    "          eg       0.18      0.50      0.27        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          sg       0.14      0.67      0.23        51\n",
    "          ib       0.22      0.04      0.06        54\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          eb       0.08      0.08      0.08        50\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eo       0.00      0.00      0.00        34\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.09      0.14      0.09       609\n",
    "weighted avg       0.10      0.18      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 18:12:28 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7104201704410498, 2.6985507841376446, 2.6908003951136896, 2.6841720861362903, 2.678058199498845, 2.6738311394877816, 2.670565606729542, 2.6658231366444105, 2.660761480065206, 2.654812508615954, 2.646417875791027, 2.637352062955083, 2.624420751687537, 2.614119046623092, 2.5999295766326203, 2.586022413814401, 2.568836017195227, 2.550056242198975, 2.536566410941639, 2.526161624097276, 2.5171442842248624, 2.5100228394976587, 2.495226006594002, 2.4907338423486216, 2.490236974506347, 2.478008079215615, 2.471368718812814, 2.4662898087932166, 2.461227283493443, 2.4557396934928954, 2.4536702719032277, 2.4550946717974784, 2.449560650855254, 2.445871076364627, 2.4485432886333496, 2.4402102304405373, 2.446164932940002, 2.4426866196254986, 2.439494365150314, 2.439112439335665, 2.4371562822307467, 2.434949015161674, 2.4347524901329005, 2.435050285116988, 2.434589762210063, 2.4313776363880177, 2.4354084587253766, 2.435945890219928, 2.435311687795204, 2.4328410680266632, 2.4312068214165947, 2.431937570838114, 2.430868251766086, 2.430232648191781, 2.4295394397134262, 2.4276400552007367, 2.428475735810003, 2.4282462224975987, 2.427467303518787, 2.4318861338892592, 2.4314873257685568, 2.4314417357515232, 2.4310573858189075, 2.432092298623572, 2.430895725494535, 2.4269804140225615, 2.425611889421059, 2.4245029059536938, 2.4234301515400705, 2.4232712105185725, 2.420861328372423, 2.4186692226109248, 2.417865285340984, 2.418835789112035, 2.4175588556111154, 2.419492430287629, 2.41807398968338, 2.4233835514738837, 2.423149009839263, 2.423594362825791, 2.4244228074899055, 2.4223652974333865, 2.423151940547774, 2.4221168766272285, 2.4224172010406093, 2.421501363243767, 2.4208751715267036, 2.4195098826059174, 2.419484282166304, 2.4187814582549096, 2.4203928068940863, 2.4190390423209407, 2.41935715142925, 2.420803364078791, 2.4189579341994913, 2.4170605282869637, 2.4183823013149066, 2.4177267469208816, 2.420297363513013, 2.423242269086916, 2.420940520532417, 2.4202319675282697, 2.4266268549294305, 2.4271794089738568, 2.424583244402029, 2.4239648836978356, 2.419485451552668, 2.426879545737957, 2.424070389204229, 2.4229992493032824, 2.4195393274961825, 2.4227103094749265, 2.4213384595410576, 2.42486393001475, 2.4200983756085725, 2.419747881897173, 2.4203368016260205, 2.4189101937173425, 2.420315439086438, 2.420464901305576, 2.419722050085835, 2.4204122104081027, 2.4219362230723713, 2.422768440152624, 2.4194404229350472, 2.420907360383834, 2.4172132978298393, 2.421047445588511, 2.4169249914354096, 2.417098138524198, 2.416560675710293, 2.4174512724571042, 2.4144420087435368, 2.415874324604404, 2.4156341259115437, 2.4161336660776622, 2.41449158767174, 2.416353019391766, 2.4141139326424432, 2.415093320148136, 2.416983932109889, 2.4167374006437354, 2.419625400518157, 2.4167014170554277, 2.418766749903486, 2.415967130895906, 2.419598091999298, 2.418137267696838, 2.4174750082207037, 2.4179558362475366, 2.416560782979079, 2.419101021364209, 2.4158539905140946, 2.416112830486204, 2.4200785672919114, 2.4194918117303956, 2.419422266714287, 2.4204802333036275, 2.4197070735624466, 2.4201371787216863, 2.4186884080639417, 2.4232907471398413, 2.4209460002448173, 2.4252715149928, 2.4212220979637307, 2.423470866308228, 2.422016626116873, 2.4193960234449414, 2.4195659696957943, 2.416453066717815, 2.417697316124326, 2.4205307823488083, 2.4207051160496054, 2.4206774649753164, 2.4187132525326582, 2.424728435052831, 2.419460561670889, 2.4198265420196483, 2.422070357599869, 2.4207707637636533, 2.419877202248534, 2.41706135746685, 2.418249885045445, 2.4232217302463326, 2.4250384990021905, 2.4261114010082676, 2.4283585567975474, 2.433546756875926, 2.4251361519636583, 2.423506468974898, 2.426231334949362, 2.42598877242829, 2.4255136096810275, 2.424966538480937, 2.42028750261454, 2.419922066831041, 2.4199490500201146, 2.422819544333347, 2.425022914491851, 2.421820209922853, 2.4258210110938414, 2.4205405485062372, 2.424285201212064, 2.4209973678996017, 2.4199987389575477, 2.423990513890835, 2.4185596060478822, 2.423962178880162, 2.4230857102937495, 2.4219246290391694, 2.4270083708520396, 2.4238602786228576, 2.4248954462887617, 2.423468568250659, 2.421853092112173, 2.423336245939258, 2.420378369846563, 2.423484001645118, 2.4240126343587742, 2.4282203621073504, 2.4227199918530844, 2.42396062857216, 2.424302543912615, 2.4268493472257466, 2.424517196974731, 2.4278901018728374, 2.4243059107431244, 2.4260607509581718, 2.426021971334573, 2.426916491221912, 2.4322910774713273, 2.433616919274792, 2.431868222742441, 2.428922171271689, 2.4320221886846234, 2.429912404278033, 2.4321188049754876, 2.4290485248972824, 2.431975133313334, 2.4257335580628494, 2.425175215027407, 2.425187291770146, 2.4223443228622963, 2.4220198124696077, 2.425173014451326, 2.4253409742721783, 2.426926026399108, 2.4215088254712485, 2.4224620495719473, 2.421023213804649, 2.4210914962593164, 2.423941569962525, 2.4281727351578586, 2.4274968913036026, 2.435199968529061, 2.4306353806274865, 2.4339330893236233, 2.4228261844277967, 2.4156252276917005, 2.4166760855707627, 2.4140872861364207, 2.417243890965905, 2.414198242776304, 2.4205014678253525, 2.414246770548703, 2.4179873677897334, 2.4200171159797508, 2.419066568509307, 2.424666537831374, 2.423048554970126, 2.4183191368341053, 2.426624981249103, 2.4150784109613577, 2.4163631590324863, 2.4153525484802296, 2.418451009712783, 2.4180604178329994, 2.4151812104756023, 2.4154265381041027, 2.4159430905515924, 2.41732404384707, 2.416866733131346, 2.421251903418054, 2.41752765683705, 2.423639544516753, 2.4194744889959328, 2.428410329646469, 2.426481156122117, 2.4282815143196843, 2.423106965173054, 2.418825471342491, 2.4219761893079785, 2.419313470326817, 2.420093835085288, 2.4174040542251762, 2.4169460446963758, 2.417592600257134, 2.419606635918954, 2.423592454694175, 2.422392110323475], 'val_acc': [0.08866995033519022, 0.08866995033519022, 0.09031198645998496, 0.11001642015326786, 0.11822659998202363, 0.11494252872339807, 0.09688013105703693, 0.09688013125278289, 0.0968801311549099, 0.09852216727970464, 0.11822659988415066, 0.11165845538497167, 0.11330049259860332, 0.11330049250073033, 0.12151067322257704, 0.11001642025114085, 0.11494252862552508, 0.12315270924949881, 0.1494252863531238, 0.13464696212306201, 0.14285714185394482, 0.14285714185394482, 0.1494252862552508, 0.1576354670749705, 0.1494252862552508, 0.14778325013045607, 0.15599343075442978, 0.15106732238004555, 0.16420361236936745, 0.15927750399498322, 0.1658456484941622, 0.1658456484941622, 0.1855500820895721, 0.1658456484941622, 0.18390804596477736, 0.1773399013677254, 0.160919539924032, 0.18390804596477736, 0.1740558291181359, 0.18226600983998262, 0.16912972054800574, 0.16912972084162467, 0.16912972084162467, 0.17898193759039313, 0.18390804497381344, 0.18390804497381344, 0.1855500820895721, 0.1789819366973022, 0.17898193679517518, 0.1740558291181359, 0.17569786434983972, 0.17405582832291797, 0.18226600974210966, 0.17405582832291797, 0.17898193749252014, 0.1789819366973022, 0.18555008199169915, 0.18226600974210966, 0.18062397272422395, 0.17733990047463447, 0.1707717558775825, 0.18062397272422395, 0.16584564829841622, 0.1707717558775825, 0.1707717558775825, 0.17898193739464718, 0.1658456483962892, 0.17733990047463447, 0.17569786434983972, 0.18062397282209694, 0.1625615752536088, 0.18062397272422395, 0.16748768362799302, 0.1822660088490187, 0.16584564859203518, 0.18226600875114574, 0.16420361246724044, 0.1904761895708654, 0.1625615752536088, 0.17733990057250745, 0.16256157535148175, 0.16256157614669972, 0.1756978644477127, 0.16420361157414948, 0.1658456484941622, 0.16748768461895694, 0.16584564829841622, 0.1773399013677254, 0.17241379210025023, 0.1691297206458787, 0.18226600894689168, 0.1773399013677254, 0.1691297200464067, 0.1691297200464067, 0.1740558291181359, 0.18226600875114574, 0.180623972626351, 0.180623972626351, 0.1773399014655984, 0.17241379309121416, 0.17241379299334117, 0.17241379309121416, 0.17898193759039313, 0.16912972074375168, 0.1740558291181359, 0.17898193749252014, 0.18226600974210966, 0.18555008199169915, 0.1773399013677254, 0.18226600964423667, 0.17733990126985244, 0.1789819366973022, 0.1756978651450577, 0.17241379210025023, 0.17733990117197945, 0.18062397351944193, 0.17898193739464718, 0.17405582902026293, 0.17733990126985244, 0.17241379210025023, 0.18062397361731491, 0.18062397361731491, 0.17733990126985244, 0.17241379219812322, 0.17733990126985244, 0.1822660088490187, 0.18062397361731491, 0.17898193739464718, 0.18226600964423667, 0.18062397361731491, 0.18062397361731491, 0.18226600964423667, 0.18226600974210966, 0.17898193739464718, 0.17733990126985244, 0.16912971985066075, 0.1756978651450577, 0.17405582822504498, 0.18062397351944193, 0.18062397361731491, 0.17898193739464718, 0.17733990047463447, 0.16748768362799302, 0.17733990126985244, 0.17898193679517518, 0.1756978651450577, 0.17405582832291797, 0.17405582902026293, 0.17405582902026293, 0.17077175607332848, 0.1756978651450577, 0.17077175617120144, 0.1773399013677254, 0.17733990126985244, 0.1691297199485337, 0.17733990126985244, 0.17405582902026293, 0.16912971985066075, 0.1773399013677254, 0.1756978651450577, 0.1756978651450577, 0.17898193679517518, 0.1756978651450577, 0.17733990067038044, 0.16748768461895694, 0.16256157535148175, 0.16256157535148175, 0.1691297206458787, 0.167487683725866, 0.17241379299334117, 0.16420361137840353, 0.17241379210025023, 0.1707717558775825, 0.1822660088490187, 0.16420361246724044, 0.17241379200237725, 0.1707717558775825, 0.167487683725866, 0.16584564760107126, 0.1707717559754555, 0.1707717559754555, 0.16420361137840353, 0.15763546796806144, 0.15435139472750803, 0.1691297199485337, 0.17077175617120144, 0.16584564769894422, 0.16584564769894422, 0.16584564750319827, 0.16748768362799302, 0.1707717559754555, 0.16912971985066075, 0.167487683725866, 0.17077175607332848, 0.1707717559754555, 0.16584564760107126, 0.16420361147627652, 0.16584564760107126, 0.16584564760107126, 0.16256157604882673, 0.17241379219812322, 0.15763546687922456, 0.16256157535148175, 0.16748768362799302, 0.16748768362799302, 0.1707717559754555, 0.16748768362799302, 0.16748768362799302, 0.1625615752536088, 0.16091953912881404, 0.160919539226687, 0.1625615752536088, 0.1592775030040193, 0.16091953912881404, 0.16584564769894422, 0.16420361157414948, 0.16420361147627652, 0.160919539226687, 0.16748768452108395, 0.16091953912881404, 0.16256157634244567, 0.16420361147627652, 0.16091953912881404, 0.16091953912881404, 0.15763546787018845, 0.16091953912881404, 0.15763546687922456, 0.16420361147627652, 0.1625615752536088, 0.16584564750319827, 0.16748768362799302, 0.16912971975278776, 0.16912971975278776, 0.1707717559754555, 0.16912971975278776, 0.167487683725866, 0.16584564750319827, 0.15435139571847195, 0.17077175607332848, 0.17077175607332848, 0.16912971985066075, 0.16912971985066075, 0.160919539226687, 0.16420361157414948, 0.1691297199485337, 0.16912972054800574, 0.16748768382373896, 0.17077175677067344, 0.15763546697709752, 0.160919539226687, 0.17241379279759522, 0.16748768382373896, 0.16584564769894422, 0.17898193739464718, 0.17405582832291797, 0.15927750310189226, 0.17077175607332848, 0.1740558291181359, 0.1773399014655984, 0.17569786534080364, 0.18226600974210966, 0.17241379229599618, 0.18062397361731491, 0.17405582842079093, 0.17898193650155622, 0.17898193650155622, 0.1691297199485337, 0.17569786524293066, 0.17241379229599618, 0.17241379229599618, 0.17898193749252014, 0.17241379229599618, 0.18062397351944193, 0.18226600983998262, 0.17569786524293066, 0.17569786524293066, 0.1773399013677254, 0.18226600974210966, 0.17898193749252014, 0.1691297200464067, 0.17077175617120144, 0.18390804596477736, 0.17077175617120144, 0.1871921180186209, 0.17241379229599618, 0.18555008189382616, 0.18390804596477736, 0.18555008199169915, 0.1871921181164939, 0.1839080458669044, 0.18390804596477736, 0.19047619046395636, 0.18719211712552997, 0.18555008100073522, 0.18883415433916162, 0.18719211821436688, 0.1871921181164939, 0.18719211712552997, 0.18883415424128863, 0.18226600974210966], 'loss': [2.717884329554971, 2.703795746268678, 2.6937856418640953, 2.6876675521568596, 2.6818559214809348, 2.6768932603957474, 2.673285733894646, 2.6687889957819633, 2.66416863729332, 2.659132344619939, 2.6526148082294503, 2.644279580928951, 2.632708262320172, 2.623765908912956, 2.6112598305610173, 2.5973456516892512, 2.580892113542655, 2.563529944370904, 2.550962479452333, 2.5382637114985034, 2.526758845190248, 2.519949494641909, 2.513178227031011, 2.5045799317056394, 2.5010359844387926, 2.501712035936986, 2.4939717947824778, 2.489750874948208, 2.4863052471952027, 2.483870910276378, 2.4830943105646717, 2.4810669315424296, 2.4809743706940135, 2.476402612192675, 2.489106655904155, 2.4907371521975223, 2.4818937887156522, 2.4717732945506823, 2.4705124288124227, 2.468995561149331, 2.46580198237049, 2.464112439929093, 2.463297428585421, 2.4633965340483117, 2.462879431468016, 2.4618269081233217, 2.4636140582497847, 2.461735044416706, 2.462253534818332, 2.463767333784632, 2.4619268398754897, 2.4622611424761387, 2.4602924558416284, 2.461418873769302, 2.4586078918934846, 2.4582220503436956, 2.458202205693208, 2.4555756233066504, 2.4555232590473652, 2.457509691367649, 2.455843008421285, 2.4534398508757294, 2.4538486696121873, 2.4524717104753186, 2.451733511233477, 2.452925167044575, 2.4517159993643634, 2.4553193719969637, 2.4536320469952217, 2.4549406822212423, 2.4524838330074994, 2.454852575345206, 2.453714893094323, 2.455493759080859, 2.4559478296636312, 2.4529488481290533, 2.4521190707933243, 2.45274444527205, 2.4525862512647247, 2.4509374727213897, 2.4521910596921948, 2.4488817919940673, 2.4512668259089976, 2.4509457743877747, 2.4514503455504744, 2.4510857649652373, 2.4502741767640477, 2.45051951124438, 2.4500205596124855, 2.4490403564069307, 2.4492088620422803, 2.4502890410609313, 2.4491039367182297, 2.4495938452851846, 2.4485748005109156, 2.450407970318804, 2.45052596470169, 2.451659419061712, 2.451147475624476, 2.4511323871064237, 2.450152560866589, 2.4497422918401948, 2.450777715138586, 2.4544596136473045, 2.4529144384043415, 2.452330303387965, 2.4520107330972407, 2.4498018382266316, 2.4547366645546664, 2.452601883102981, 2.4514197106723667, 2.4526457999031646, 2.449856320935353, 2.449924386061682, 2.4498357831575053, 2.4480301883431186, 2.447940555686089, 2.4473103734746853, 2.4475376920288836, 2.449127385552659, 2.4500209683999876, 2.4481692605929206, 2.448168804121703, 2.4483007050148027, 2.4491433954336803, 2.4470985552613493, 2.4450661053158176, 2.4448052968332656, 2.4431487780576857, 2.44448703162724, 2.443949470382941, 2.4426999093081183, 2.4423404329366507, 2.441918980071677, 2.441781426063553, 2.4407855378528884, 2.4409966338586515, 2.44070916166051, 2.442763057920233, 2.443592795503213, 2.444725492798572, 2.4463802847774123, 2.445824899125148, 2.445315673072235, 2.443799638846082, 2.4433103324451486, 2.4421140157711334, 2.4448078417924886, 2.4434045066089354, 2.443774322905335, 2.4431027572992154, 2.445480296450229, 2.444856375100921, 2.4455841530519833, 2.444012705746128, 2.444328529095503, 2.4462434099929777, 2.444194755123381, 2.4424590103924886, 2.44455648643525, 2.445632476532484, 2.444089070972231, 2.443536404075074, 2.4423527712694675, 2.4430042098435045, 2.4410913742543245, 2.4399253301796726, 2.439129275511912, 2.437946785157221, 2.439949359149659, 2.4403450414630177, 2.440429422742777, 2.4396007726814224, 2.438649796166704, 2.4367461191800097, 2.439079852760205, 2.4397782590110197, 2.4371338049244344, 2.4355632593989127, 2.436153822121434, 2.436931424072391, 2.4369281351688707, 2.437522420158621, 2.4369950130979627, 2.438737206977985, 2.44212632972357, 2.441258500782616, 2.441063964734087, 2.439530091804645, 2.437681149848922, 2.4362563007910882, 2.4346858335961064, 2.4330288763653325, 2.432185712144605, 2.434741951161097, 2.433607160579987, 2.4350462786226057, 2.436584386688483, 2.4368515495402123, 2.4351447127927743, 2.4338622829507752, 2.434410695912167, 2.43322336208649, 2.434560478394526, 2.434795768353973, 2.431642712998439, 2.434241544539434, 2.4360579031448832, 2.434654165636098, 2.433077308431543, 2.4316278895313492, 2.4332837891040153, 2.4331395384222576, 2.432046964476975, 2.434065686016357, 2.434458471960111, 2.4360906002702656, 2.4366514372385013, 2.4372351468221365, 2.4337484462550045, 2.4319312890697065, 2.4323414989565433, 2.433450105302877, 2.4351781975317297, 2.434919893766086, 2.4315075814601577, 2.4342420922657304, 2.4346209928485156, 2.435116106283983, 2.434142969765947, 2.4330333905543147, 2.431712603911727, 2.431588118472873, 2.431847143369044, 2.4307744910340046, 2.4298509739752423, 2.432410109752992, 2.4330524214484117, 2.43362830806317, 2.434305010096493, 2.432941073901355, 2.433682700006379, 2.4360091695060966, 2.436438846098569, 2.436437573814784, 2.4343130672736826, 2.4347211770208466, 2.434068218540607, 2.436837245404598, 2.4354752291888917, 2.4350831591862674, 2.43530969188933, 2.4348074452832984, 2.4361055958687157, 2.435583758501057, 2.43777522316214, 2.4387321088348326, 2.436068633645467, 2.434134266655548, 2.4380032297522134, 2.436879512074057, 2.435661183245618, 2.4348008311504703, 2.4345131189671387, 2.4366769892479114, 2.435900498807308, 2.433296058310131, 2.431816268701573, 2.435328493177034, 2.437222373559, 2.4369451885105895, 2.4323256569966154, 2.4329341269616473, 2.4340749578064718, 2.43403243010049, 2.4346263283332026, 2.4344651215375084, 2.434931448108117, 2.433405558333505, 2.434113517972723, 2.435074585419171, 2.4337914760597434, 2.4366796101877575, 2.4362768333795377, 2.437496772683866, 2.4378038910624915, 2.436563596490472, 2.4374226508444097, 2.437759484598524, 2.4383677238801176, 2.4408395890582515, 2.44191038006385, 2.442130622628778, 2.4416813019119985, 2.4426462105901825, 2.4394394564187993, 2.438748227595304, 2.4382891751902305, 2.437024584149433, 2.437673721568051], 'acc': [0.07268993814745478, 0.08911704305689437, 0.09075975393918505, 0.09691991780022087, 0.10349076029517568, 0.10513347016161717, 0.09733059601494909, 0.09363449668737407, 0.09527720792460001, 0.09568788455435873, 0.1075975358608567, 0.11663244385004533, 0.11540041121002095, 0.10924024654732103, 0.11170431205073421, 0.12156057508023613, 0.11786447729173383, 0.12648870746702628, 0.13634496832407966, 0.15646817337193772, 0.15195071781073263, 0.16344969129415507, 0.16057494935680952, 0.1585215614622868, 0.16057494876933048, 0.15359342926214364, 0.16098562620993268, 0.1618069826210304, 0.16098562538990985, 0.1642710479377966, 0.16386036871639856, 0.1774127318751396, 0.16837782253352523, 0.17330595491113604, 0.16591375680674764, 0.15975359394320227, 0.1679671456987608, 0.17207392088196852, 0.17659137507238917, 0.17741273208932465, 0.17905544177829852, 0.17741273071854022, 0.17700205288628534, 0.17371663213755315, 0.17248460049501924, 0.1790554405849817, 0.17700205288628534, 0.17905544117246075, 0.17700205427542848, 0.1720739216836326, 0.17823408712963792, 0.17577002161704539, 0.17823408712963792, 0.1774127299168761, 0.18439424940570423, 0.18028747363501751, 0.18069815103890224, 0.1774127303085288, 0.17946611939636833, 0.1778234075349459, 0.18069815203639272, 0.18234086311450973, 0.17823408515301573, 0.17987679758355848, 0.17782340771241353, 0.17494866598925307, 0.17659137487656282, 0.17659137507238917, 0.1774127310918342, 0.1774127315018456, 0.17412731093058106, 0.17782340812242498, 0.17494866518758895, 0.17700205445289613, 0.17248459969335514, 0.1790554409766344, 0.17946611918218328, 0.17905544138664584, 0.17741273048599643, 0.1786447629669119, 0.1749486641900985, 0.1761806984334511, 0.18028747384920257, 0.17946611820305153, 0.17577002102956635, 0.17494866598925307, 0.18069815164474, 0.17782340890573037, 0.1782340857404948, 0.17659137526821553, 0.183572894579576, 0.17864476433769633, 0.1774127318934983, 0.17330595473366842, 0.17412730936397028, 0.17823408711127922, 0.17864476316273825, 0.17618069823762475, 0.17535934239564735, 0.1802874744183229, 0.17987679560693628, 0.18110882887115715, 0.17741273171603067, 0.17823408732546428, 0.17535934380314924, 0.17905544076244934, 0.1778234085324364, 0.17864476433769633, 0.17782340792659862, 0.17577002022790222, 0.1819301838931117, 0.1790554405849817, 0.18110883061523556, 0.18521560622681338, 0.18110882926280983, 0.18234086352452117, 0.18398357178763441, 0.18151950746835868, 0.18110882985028887, 0.17782340773077226, 0.17864476414187, 0.18069815086143462, 0.18069815223221905, 0.17741273068182278, 0.18069815086143462, 0.17577002102956635, 0.1802874740266702, 0.1794661197696623, 0.1823408622944869, 0.1843942512048588, 0.1848049280029058, 0.1815195063117593, 0.18603696167713807, 0.1856262826515664, 0.18644763925849045, 0.18562628364905684, 0.18439425100903248, 0.18850102619224016, 0.18398357296259252, 0.18275153994927415, 0.1852156062451721, 0.17823408712963792, 0.17946611879053057, 0.17782340773077226, 0.17700205427542848, 0.1798767960169477, 0.1815195062934006, 0.17946611879053057, 0.18110883006447395, 0.179876796586068, 0.18439425020736835, 0.1806981526422305, 0.17782340812242498, 0.1815195082700228, 0.17577002102956635, 0.1753593433931378, 0.1798767960169477, 0.1798767977793848, 0.18316221794063794, 0.17659137546404186, 0.18193018369728534, 0.1786447641602287, 0.18069815225057778, 0.1774127307001815, 0.1724846004583018, 0.17823408515301573, 0.17905544236577756, 0.18151950729089106, 0.1819301843031231, 0.18234086249031325, 0.18069815184056637, 0.18439425059902104, 0.18234086152954024, 0.18439424981571564, 0.1831622173531589, 0.17987679738773213, 0.1827515409284059, 0.18275153914761005, 0.18439425159651152, 0.1794661181846928, 0.1782340867196265, 0.17577002022790222, 0.1782340855263097, 0.17535934184488575, 0.17289527750725128, 0.1786447645518814, 0.17535934223653843, 0.18028747541581336, 0.1831622185464757, 0.1782340869338116, 0.17987679660442674, 0.18275154053675322, 0.18234086311450973, 0.18193018467641708, 0.1843942511865001, 0.18234086311450973, 0.18234086133371388, 0.18439425020736835, 0.1819301854780812, 0.17987679779774354, 0.1827515393434364, 0.18193018489060217, 0.18151950609757425, 0.18275153912925132, 0.18069815303388317, 0.18357289418792333, 0.18357289634201315, 0.1786447649435341, 0.18398357221600456, 0.18193018449894946, 0.18069815203639272, 0.18234086152954024, 0.1835728941695646, 0.18275153932507768, 0.1794661193780096, 0.17987679738773213, 0.17782340911991543, 0.18069815283805682, 0.17946611998384737, 0.18110882906698347, 0.18316221676567987, 0.18234086309615102, 0.1741273107347547, 0.18110883024194158, 0.17700205386541706, 0.1782340857404948, 0.18069815223221905, 0.18069815283805682, 0.17700205229880628, 0.17659137526821553, 0.18028747539745463, 0.18151950625668317, 0.18439425099067375, 0.18110882967282124, 0.18562628288411018, 0.181108830651953, 0.17905544197412487, 0.18234086331033608, 0.17494866579342672, 0.18234086192119292, 0.17905544076244934, 0.17905544115410205, 0.17987679560693628, 0.17618069862927743, 0.1782340857404948, 0.18110882985028887, 0.17946611978802104, 0.17864476433769633, 0.18151950807419645, 0.17864476316273825, 0.18110882985028887, 0.18357289498958745, 0.18193018489060217, 0.1786447649251754, 0.18726899433552116, 0.1815195078600114, 0.18069815184056637, 0.1794661195921947, 0.18357289477540237, 0.17494866577506799, 0.18110882967282124, 0.17577002042372858, 0.1770020527088177, 0.1823408627044983, 0.17823408632797383, 0.18275153914761005, 0.18193018510478723, 0.18069815184056637, 0.1819301850864285, 0.1794661188088893, 0.17946611998384737, 0.18110882865697206, 0.1786447641602287, 0.18151950746835868, 0.18069815203639272, 0.1798767972102645, 0.18069815144891366, 0.1774127303268875, 0.18110882985028887, 0.18151950607921552, 0.18110882945863618, 0.1811088286936895, 0.17659137526821553, 0.18357289634201315, 0.1790554425616039, 0.1848049278437969, 0.1835728941695646, 0.1819301850864285, 0.18151950725417362, 0.18521560563933434, 0.18562628423653588, 0.18562628225991368, 0.18726899353385706, 0.18891170361448362, 0.18603696187296442, 0.18521560524768163, 0.18603696126712665, 0.18439425140068516, 0.1848049276112531, 0.18644763769187966]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
