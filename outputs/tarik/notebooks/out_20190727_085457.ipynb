{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf34.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 08:54:57 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'Front', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['by', 'eb', 'aa', 'ds', 'yd', 'ib', 'ck', 'ek', 'sg', 'eo', 'sk', 'mb', 'my', 'eg', 'ce'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000226A4419F98>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000226A1B86EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7153, Accuracy:0.0891, Validation Loss:2.7095, Validation Accuracy:0.0887\n",
    "Epoch #2: Loss:2.7064, Accuracy:0.0891, Validation Loss:2.7017, Validation Accuracy:0.0509\n",
    "Epoch #3: Loss:2.6992, Accuracy:0.0690, Validation Loss:2.6955, Validation Accuracy:0.0821\n",
    "Epoch #4: Loss:2.6935, Accuracy:0.0957, Validation Loss:2.6903, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6886, Accuracy:0.1023, Validation Loss:2.6855, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6839, Accuracy:0.1023, Validation Loss:2.6815, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6801, Accuracy:0.1023, Validation Loss:2.6777, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6763, Accuracy:0.1023, Validation Loss:2.6744, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6731, Accuracy:0.1023, Validation Loss:2.6713, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6702, Accuracy:0.1023, Validation Loss:2.6687, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6678, Accuracy:0.1023, Validation Loss:2.6664, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6656, Accuracy:0.1023, Validation Loss:2.6646, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6641, Accuracy:0.1023, Validation Loss:2.6631, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6628, Accuracy:0.1023, Validation Loss:2.6621, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6621, Accuracy:0.1023, Validation Loss:2.6614, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6612, Accuracy:0.1023, Validation Loss:2.6609, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6609, Accuracy:0.1023, Validation Loss:2.6606, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6605, Accuracy:0.1023, Validation Loss:2.6603, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6602, Accuracy:0.1023, Validation Loss:2.6600, Validation Accuracy:0.1018\n",
    "Epoch #20: Loss:2.6599, Accuracy:0.1023, Validation Loss:2.6597, Validation Accuracy:0.1018\n",
    "Epoch #21: Loss:2.6595, Accuracy:0.1023, Validation Loss:2.6594, Validation Accuracy:0.1018\n",
    "Epoch #22: Loss:2.6591, Accuracy:0.1023, Validation Loss:2.6590, Validation Accuracy:0.1018\n",
    "Epoch #23: Loss:2.6585, Accuracy:0.1023, Validation Loss:2.6585, Validation Accuracy:0.1018\n",
    "Epoch #24: Loss:2.6578, Accuracy:0.1023, Validation Loss:2.6578, Validation Accuracy:0.1018\n",
    "Epoch #25: Loss:2.6569, Accuracy:0.1023, Validation Loss:2.6569, Validation Accuracy:0.1018\n",
    "Epoch #26: Loss:2.6555, Accuracy:0.1023, Validation Loss:2.6556, Validation Accuracy:0.1018\n",
    "Epoch #27: Loss:2.6538, Accuracy:0.1023, Validation Loss:2.6539, Validation Accuracy:0.1018\n",
    "Epoch #28: Loss:2.6517, Accuracy:0.1023, Validation Loss:2.6516, Validation Accuracy:0.1018\n",
    "Epoch #29: Loss:2.6484, Accuracy:0.1023, Validation Loss:2.6483, Validation Accuracy:0.1018\n",
    "Epoch #30: Loss:2.6441, Accuracy:0.1043, Validation Loss:2.6435, Validation Accuracy:0.1018\n",
    "Epoch #31: Loss:2.6369, Accuracy:0.1080, Validation Loss:2.6363, Validation Accuracy:0.1084\n",
    "Epoch #32: Loss:2.6275, Accuracy:0.1166, Validation Loss:2.6260, Validation Accuracy:0.1133\n",
    "Epoch #33: Loss:2.6130, Accuracy:0.1224, Validation Loss:2.6128, Validation Accuracy:0.1264\n",
    "Epoch #34: Loss:2.5966, Accuracy:0.1314, Validation Loss:2.5991, Validation Accuracy:0.1379\n",
    "Epoch #35: Loss:2.5820, Accuracy:0.1483, Validation Loss:2.5907, Validation Accuracy:0.1346\n",
    "Epoch #36: Loss:2.5683, Accuracy:0.1552, Validation Loss:2.5812, Validation Accuracy:0.1412\n",
    "Epoch #37: Loss:2.5577, Accuracy:0.1552, Validation Loss:2.5676, Validation Accuracy:0.1461\n",
    "Epoch #38: Loss:2.5487, Accuracy:0.1483, Validation Loss:2.5578, Validation Accuracy:0.1511\n",
    "Epoch #39: Loss:2.5357, Accuracy:0.1581, Validation Loss:2.5478, Validation Accuracy:0.1494\n",
    "Epoch #40: Loss:2.5262, Accuracy:0.1556, Validation Loss:2.5383, Validation Accuracy:0.1429\n",
    "Epoch #41: Loss:2.5156, Accuracy:0.1721, Validation Loss:2.5285, Validation Accuracy:0.1773\n",
    "Epoch #42: Loss:2.5063, Accuracy:0.1840, Validation Loss:2.5193, Validation Accuracy:0.1741\n",
    "Epoch #43: Loss:2.5002, Accuracy:0.1836, Validation Loss:2.5079, Validation Accuracy:0.1675\n",
    "Epoch #44: Loss:2.4884, Accuracy:0.1799, Validation Loss:2.4979, Validation Accuracy:0.1691\n",
    "Epoch #45: Loss:2.4800, Accuracy:0.1749, Validation Loss:2.4879, Validation Accuracy:0.1691\n",
    "Epoch #46: Loss:2.4763, Accuracy:0.1770, Validation Loss:2.4894, Validation Accuracy:0.1576\n",
    "Epoch #47: Loss:2.4686, Accuracy:0.1655, Validation Loss:2.4774, Validation Accuracy:0.1757\n",
    "Epoch #48: Loss:2.4648, Accuracy:0.1708, Validation Loss:2.4601, Validation Accuracy:0.1609\n",
    "Epoch #49: Loss:2.4424, Accuracy:0.1762, Validation Loss:2.4491, Validation Accuracy:0.1823\n",
    "Epoch #50: Loss:2.4307, Accuracy:0.1803, Validation Loss:2.4349, Validation Accuracy:0.1741\n",
    "Epoch #51: Loss:2.4178, Accuracy:0.1758, Validation Loss:2.4164, Validation Accuracy:0.1806\n",
    "Epoch #52: Loss:2.4020, Accuracy:0.1869, Validation Loss:2.3950, Validation Accuracy:0.1806\n",
    "Epoch #53: Loss:2.3853, Accuracy:0.1869, Validation Loss:2.3754, Validation Accuracy:0.1954\n",
    "Epoch #54: Loss:2.3653, Accuracy:0.1988, Validation Loss:2.3530, Validation Accuracy:0.2315\n",
    "Epoch #55: Loss:2.3513, Accuracy:0.2004, Validation Loss:2.3323, Validation Accuracy:0.2414\n",
    "Epoch #56: Loss:2.3353, Accuracy:0.2275, Validation Loss:2.3179, Validation Accuracy:0.2348\n",
    "Epoch #57: Loss:2.3185, Accuracy:0.2333, Validation Loss:2.3006, Validation Accuracy:0.2512\n",
    "Epoch #58: Loss:2.3089, Accuracy:0.2296, Validation Loss:2.2913, Validation Accuracy:0.2644\n",
    "Epoch #59: Loss:2.3087, Accuracy:0.2333, Validation Loss:2.2909, Validation Accuracy:0.2365\n",
    "Epoch #60: Loss:2.2881, Accuracy:0.2361, Validation Loss:2.2694, Validation Accuracy:0.2594\n",
    "Epoch #61: Loss:2.2743, Accuracy:0.2493, Validation Loss:2.2572, Validation Accuracy:0.2529\n",
    "Epoch #62: Loss:2.2590, Accuracy:0.2480, Validation Loss:2.2346, Validation Accuracy:0.2693\n",
    "Epoch #63: Loss:2.2438, Accuracy:0.2534, Validation Loss:2.2305, Validation Accuracy:0.2611\n",
    "Epoch #64: Loss:2.2379, Accuracy:0.2546, Validation Loss:2.2129, Validation Accuracy:0.2693\n",
    "Epoch #65: Loss:2.2283, Accuracy:0.2575, Validation Loss:2.2041, Validation Accuracy:0.2726\n",
    "Epoch #66: Loss:2.2176, Accuracy:0.2632, Validation Loss:2.1944, Validation Accuracy:0.2791\n",
    "Epoch #67: Loss:2.2071, Accuracy:0.2624, Validation Loss:2.1841, Validation Accuracy:0.2874\n",
    "Epoch #68: Loss:2.2005, Accuracy:0.2637, Validation Loss:2.1768, Validation Accuracy:0.2824\n",
    "Epoch #69: Loss:2.1917, Accuracy:0.2739, Validation Loss:2.1665, Validation Accuracy:0.2841\n",
    "Epoch #70: Loss:2.1837, Accuracy:0.2760, Validation Loss:2.1593, Validation Accuracy:0.2808\n",
    "Epoch #71: Loss:2.1753, Accuracy:0.2817, Validation Loss:2.1538, Validation Accuracy:0.2890\n",
    "Epoch #72: Loss:2.1720, Accuracy:0.2702, Validation Loss:2.1585, Validation Accuracy:0.2874\n",
    "Epoch #73: Loss:2.1646, Accuracy:0.2776, Validation Loss:2.1376, Validation Accuracy:0.2906\n",
    "Epoch #74: Loss:2.1538, Accuracy:0.2875, Validation Loss:2.1315, Validation Accuracy:0.2841\n",
    "Epoch #75: Loss:2.1474, Accuracy:0.2867, Validation Loss:2.1246, Validation Accuracy:0.2906\n",
    "Epoch #76: Loss:2.1409, Accuracy:0.2821, Validation Loss:2.1183, Validation Accuracy:0.2939\n",
    "Epoch #77: Loss:2.1369, Accuracy:0.2842, Validation Loss:2.1170, Validation Accuracy:0.2857\n",
    "Epoch #78: Loss:2.1319, Accuracy:0.2809, Validation Loss:2.1043, Validation Accuracy:0.2989\n",
    "Epoch #79: Loss:2.1273, Accuracy:0.2838, Validation Loss:2.1068, Validation Accuracy:0.2874\n",
    "Epoch #80: Loss:2.1244, Accuracy:0.2887, Validation Loss:2.0921, Validation Accuracy:0.3054\n",
    "Epoch #81: Loss:2.1177, Accuracy:0.2838, Validation Loss:2.0881, Validation Accuracy:0.2906\n",
    "Epoch #82: Loss:2.1140, Accuracy:0.2821, Validation Loss:2.0854, Validation Accuracy:0.2906\n",
    "Epoch #83: Loss:2.1117, Accuracy:0.2879, Validation Loss:2.0793, Validation Accuracy:0.3021\n",
    "Epoch #84: Loss:2.1062, Accuracy:0.2891, Validation Loss:2.0736, Validation Accuracy:0.2923\n",
    "Epoch #85: Loss:2.1041, Accuracy:0.2891, Validation Loss:2.0681, Validation Accuracy:0.2989\n",
    "Epoch #86: Loss:2.1001, Accuracy:0.2924, Validation Loss:2.0672, Validation Accuracy:0.3021\n",
    "Epoch #87: Loss:2.1020, Accuracy:0.2899, Validation Loss:2.0659, Validation Accuracy:0.2874\n",
    "Epoch #88: Loss:2.0958, Accuracy:0.2867, Validation Loss:2.0620, Validation Accuracy:0.2956\n",
    "Epoch #89: Loss:2.0937, Accuracy:0.2920, Validation Loss:2.0524, Validation Accuracy:0.2989\n",
    "Epoch #90: Loss:2.0882, Accuracy:0.2961, Validation Loss:2.0567, Validation Accuracy:0.2989\n",
    "Epoch #91: Loss:2.0856, Accuracy:0.2961, Validation Loss:2.0492, Validation Accuracy:0.2989\n",
    "Epoch #92: Loss:2.0816, Accuracy:0.2969, Validation Loss:2.0424, Validation Accuracy:0.2972\n",
    "Epoch #93: Loss:2.0783, Accuracy:0.2973, Validation Loss:2.0382, Validation Accuracy:0.3136\n",
    "Epoch #94: Loss:2.0757, Accuracy:0.2949, Validation Loss:2.0400, Validation Accuracy:0.3054\n",
    "Epoch #95: Loss:2.0739, Accuracy:0.2953, Validation Loss:2.0345, Validation Accuracy:0.3071\n",
    "Epoch #96: Loss:2.0784, Accuracy:0.2940, Validation Loss:2.0422, Validation Accuracy:0.2874\n",
    "Epoch #97: Loss:2.0753, Accuracy:0.2932, Validation Loss:2.0281, Validation Accuracy:0.3038\n",
    "Epoch #98: Loss:2.0664, Accuracy:0.2973, Validation Loss:2.0237, Validation Accuracy:0.3038\n",
    "Epoch #99: Loss:2.0641, Accuracy:0.3002, Validation Loss:2.0261, Validation Accuracy:0.2989\n",
    "Epoch #100: Loss:2.0633, Accuracy:0.2949, Validation Loss:2.0202, Validation Accuracy:0.3087\n",
    "Epoch #101: Loss:2.0687, Accuracy:0.2961, Validation Loss:2.0195, Validation Accuracy:0.2972\n",
    "Epoch #102: Loss:2.0555, Accuracy:0.2986, Validation Loss:2.0173, Validation Accuracy:0.3054\n",
    "Epoch #103: Loss:2.0546, Accuracy:0.2945, Validation Loss:2.0169, Validation Accuracy:0.2989\n",
    "Epoch #104: Loss:2.0567, Accuracy:0.3002, Validation Loss:2.0098, Validation Accuracy:0.3038\n",
    "Epoch #105: Loss:2.0540, Accuracy:0.3014, Validation Loss:2.0151, Validation Accuracy:0.2939\n",
    "Epoch #106: Loss:2.0518, Accuracy:0.3006, Validation Loss:2.0081, Validation Accuracy:0.3021\n",
    "Epoch #107: Loss:2.0476, Accuracy:0.3002, Validation Loss:2.0119, Validation Accuracy:0.2906\n",
    "Epoch #108: Loss:2.0432, Accuracy:0.3010, Validation Loss:2.0232, Validation Accuracy:0.2841\n",
    "Epoch #109: Loss:2.0471, Accuracy:0.3039, Validation Loss:2.0139, Validation Accuracy:0.2841\n",
    "Epoch #110: Loss:2.0438, Accuracy:0.3080, Validation Loss:2.0030, Validation Accuracy:0.2956\n",
    "Epoch #111: Loss:2.0364, Accuracy:0.3051, Validation Loss:2.0000, Validation Accuracy:0.3021\n",
    "Epoch #112: Loss:2.0368, Accuracy:0.3051, Validation Loss:2.0017, Validation Accuracy:0.3071\n",
    "Epoch #113: Loss:2.0380, Accuracy:0.3002, Validation Loss:1.9971, Validation Accuracy:0.2956\n",
    "Epoch #114: Loss:2.0324, Accuracy:0.3084, Validation Loss:1.9953, Validation Accuracy:0.2890\n",
    "Epoch #115: Loss:2.0310, Accuracy:0.3084, Validation Loss:1.9946, Validation Accuracy:0.3087\n",
    "Epoch #116: Loss:2.0315, Accuracy:0.3039, Validation Loss:1.9957, Validation Accuracy:0.2939\n",
    "Epoch #117: Loss:2.0300, Accuracy:0.3088, Validation Loss:2.0004, Validation Accuracy:0.2972\n",
    "Epoch #118: Loss:2.0319, Accuracy:0.3080, Validation Loss:1.9952, Validation Accuracy:0.2890\n",
    "Epoch #119: Loss:2.0295, Accuracy:0.3117, Validation Loss:2.0031, Validation Accuracy:0.3087\n",
    "Epoch #120: Loss:2.0368, Accuracy:0.3055, Validation Loss:1.9970, Validation Accuracy:0.2923\n",
    "Epoch #121: Loss:2.0285, Accuracy:0.3117, Validation Loss:1.9894, Validation Accuracy:0.3021\n",
    "Epoch #122: Loss:2.0233, Accuracy:0.3154, Validation Loss:1.9960, Validation Accuracy:0.2989\n",
    "Epoch #123: Loss:2.0226, Accuracy:0.3113, Validation Loss:1.9861, Validation Accuracy:0.3103\n",
    "Epoch #124: Loss:2.0232, Accuracy:0.3101, Validation Loss:1.9972, Validation Accuracy:0.2989\n",
    "Epoch #125: Loss:2.0220, Accuracy:0.3175, Validation Loss:2.0047, Validation Accuracy:0.2939\n",
    "Epoch #126: Loss:2.0279, Accuracy:0.3088, Validation Loss:1.9858, Validation Accuracy:0.2939\n",
    "Epoch #127: Loss:2.0186, Accuracy:0.3080, Validation Loss:1.9831, Validation Accuracy:0.3038\n",
    "Epoch #128: Loss:2.0171, Accuracy:0.3142, Validation Loss:1.9855, Validation Accuracy:0.3021\n",
    "Epoch #129: Loss:2.0130, Accuracy:0.3129, Validation Loss:1.9799, Validation Accuracy:0.3153\n",
    "Epoch #130: Loss:2.0083, Accuracy:0.3142, Validation Loss:1.9764, Validation Accuracy:0.3120\n",
    "Epoch #131: Loss:2.0068, Accuracy:0.3183, Validation Loss:1.9822, Validation Accuracy:0.3103\n",
    "Epoch #132: Loss:2.0144, Accuracy:0.3097, Validation Loss:1.9778, Validation Accuracy:0.3087\n",
    "Epoch #133: Loss:2.0076, Accuracy:0.3158, Validation Loss:1.9729, Validation Accuracy:0.3235\n",
    "Epoch #134: Loss:2.0018, Accuracy:0.3170, Validation Loss:1.9742, Validation Accuracy:0.3054\n",
    "Epoch #135: Loss:2.0008, Accuracy:0.3162, Validation Loss:1.9725, Validation Accuracy:0.3218\n",
    "Epoch #136: Loss:2.0024, Accuracy:0.3162, Validation Loss:1.9779, Validation Accuracy:0.2956\n",
    "Epoch #137: Loss:2.0032, Accuracy:0.3129, Validation Loss:1.9729, Validation Accuracy:0.3218\n",
    "Epoch #138: Loss:2.0048, Accuracy:0.3125, Validation Loss:1.9918, Validation Accuracy:0.3054\n",
    "Epoch #139: Loss:2.0180, Accuracy:0.3158, Validation Loss:1.9734, Validation Accuracy:0.3054\n",
    "Epoch #140: Loss:1.9965, Accuracy:0.3133, Validation Loss:1.9676, Validation Accuracy:0.3120\n",
    "Epoch #141: Loss:1.9953, Accuracy:0.3236, Validation Loss:1.9739, Validation Accuracy:0.3136\n",
    "Epoch #142: Loss:1.9983, Accuracy:0.3150, Validation Loss:1.9777, Validation Accuracy:0.3038\n",
    "Epoch #143: Loss:1.9983, Accuracy:0.3133, Validation Loss:1.9794, Validation Accuracy:0.3087\n",
    "Epoch #144: Loss:2.0010, Accuracy:0.3150, Validation Loss:1.9689, Validation Accuracy:0.3103\n",
    "Epoch #145: Loss:1.9944, Accuracy:0.3175, Validation Loss:1.9636, Validation Accuracy:0.3120\n",
    "Epoch #146: Loss:1.9888, Accuracy:0.3195, Validation Loss:1.9676, Validation Accuracy:0.3021\n",
    "Epoch #147: Loss:1.9883, Accuracy:0.3158, Validation Loss:1.9616, Validation Accuracy:0.3120\n",
    "Epoch #148: Loss:1.9896, Accuracy:0.3162, Validation Loss:1.9624, Validation Accuracy:0.3202\n",
    "Epoch #149: Loss:1.9877, Accuracy:0.3195, Validation Loss:1.9661, Validation Accuracy:0.3169\n",
    "Epoch #150: Loss:1.9891, Accuracy:0.3228, Validation Loss:1.9598, Validation Accuracy:0.3186\n",
    "Epoch #151: Loss:1.9851, Accuracy:0.3207, Validation Loss:1.9664, Validation Accuracy:0.3136\n",
    "Epoch #152: Loss:1.9854, Accuracy:0.3257, Validation Loss:1.9707, Validation Accuracy:0.3120\n",
    "Epoch #153: Loss:1.9909, Accuracy:0.3220, Validation Loss:1.9582, Validation Accuracy:0.3169\n",
    "Epoch #154: Loss:1.9852, Accuracy:0.3166, Validation Loss:1.9703, Validation Accuracy:0.3153\n",
    "Epoch #155: Loss:1.9835, Accuracy:0.3175, Validation Loss:1.9702, Validation Accuracy:0.3021\n",
    "Epoch #156: Loss:1.9881, Accuracy:0.3125, Validation Loss:1.9655, Validation Accuracy:0.3153\n",
    "Epoch #157: Loss:1.9880, Accuracy:0.3105, Validation Loss:1.9657, Validation Accuracy:0.2972\n",
    "Epoch #158: Loss:1.9851, Accuracy:0.3195, Validation Loss:1.9558, Validation Accuracy:0.3153\n",
    "Epoch #159: Loss:1.9824, Accuracy:0.3133, Validation Loss:1.9616, Validation Accuracy:0.3186\n",
    "Epoch #160: Loss:1.9786, Accuracy:0.3211, Validation Loss:1.9650, Validation Accuracy:0.3103\n",
    "Epoch #161: Loss:1.9868, Accuracy:0.3203, Validation Loss:1.9579, Validation Accuracy:0.3005\n",
    "Epoch #162: Loss:1.9761, Accuracy:0.3240, Validation Loss:1.9554, Validation Accuracy:0.3153\n",
    "Epoch #163: Loss:1.9768, Accuracy:0.3224, Validation Loss:1.9551, Validation Accuracy:0.3054\n",
    "Epoch #164: Loss:1.9744, Accuracy:0.3211, Validation Loss:1.9630, Validation Accuracy:0.3103\n",
    "Epoch #165: Loss:1.9746, Accuracy:0.3170, Validation Loss:1.9519, Validation Accuracy:0.3087\n",
    "Epoch #166: Loss:1.9764, Accuracy:0.3220, Validation Loss:1.9548, Validation Accuracy:0.3251\n",
    "Epoch #167: Loss:1.9740, Accuracy:0.3183, Validation Loss:1.9612, Validation Accuracy:0.2989\n",
    "Epoch #168: Loss:1.9731, Accuracy:0.3265, Validation Loss:1.9519, Validation Accuracy:0.3103\n",
    "Epoch #169: Loss:1.9756, Accuracy:0.3224, Validation Loss:1.9557, Validation Accuracy:0.3186\n",
    "Epoch #170: Loss:1.9733, Accuracy:0.3191, Validation Loss:1.9545, Validation Accuracy:0.2906\n",
    "Epoch #171: Loss:1.9782, Accuracy:0.3224, Validation Loss:1.9552, Validation Accuracy:0.3235\n",
    "Epoch #172: Loss:1.9773, Accuracy:0.3195, Validation Loss:1.9636, Validation Accuracy:0.3169\n",
    "Epoch #173: Loss:1.9762, Accuracy:0.3220, Validation Loss:1.9587, Validation Accuracy:0.3186\n",
    "Epoch #174: Loss:1.9700, Accuracy:0.3175, Validation Loss:1.9549, Validation Accuracy:0.2939\n",
    "Epoch #175: Loss:1.9723, Accuracy:0.3207, Validation Loss:1.9549, Validation Accuracy:0.3153\n",
    "Epoch #176: Loss:1.9752, Accuracy:0.3150, Validation Loss:1.9502, Validation Accuracy:0.2906\n",
    "Epoch #177: Loss:1.9669, Accuracy:0.3191, Validation Loss:1.9486, Validation Accuracy:0.3284\n",
    "Epoch #178: Loss:1.9665, Accuracy:0.3269, Validation Loss:1.9442, Validation Accuracy:0.3120\n",
    "Epoch #179: Loss:1.9652, Accuracy:0.3207, Validation Loss:1.9428, Validation Accuracy:0.3136\n",
    "Epoch #180: Loss:1.9672, Accuracy:0.3257, Validation Loss:1.9437, Validation Accuracy:0.3251\n",
    "Epoch #181: Loss:1.9642, Accuracy:0.3203, Validation Loss:1.9460, Validation Accuracy:0.2989\n",
    "Epoch #182: Loss:1.9671, Accuracy:0.3187, Validation Loss:1.9790, Validation Accuracy:0.3186\n",
    "Epoch #183: Loss:1.9816, Accuracy:0.3269, Validation Loss:1.9463, Validation Accuracy:0.3169\n",
    "Epoch #184: Loss:1.9622, Accuracy:0.3248, Validation Loss:1.9401, Validation Accuracy:0.3251\n",
    "Epoch #185: Loss:1.9591, Accuracy:0.3257, Validation Loss:1.9412, Validation Accuracy:0.3136\n",
    "Epoch #186: Loss:1.9563, Accuracy:0.3240, Validation Loss:1.9403, Validation Accuracy:0.3005\n",
    "Epoch #187: Loss:1.9609, Accuracy:0.3203, Validation Loss:1.9412, Validation Accuracy:0.3284\n",
    "Epoch #188: Loss:1.9607, Accuracy:0.3281, Validation Loss:1.9493, Validation Accuracy:0.3071\n",
    "Epoch #189: Loss:1.9592, Accuracy:0.3216, Validation Loss:1.9667, Validation Accuracy:0.3186\n",
    "Epoch #190: Loss:1.9702, Accuracy:0.3220, Validation Loss:1.9396, Validation Accuracy:0.2989\n",
    "Epoch #191: Loss:1.9615, Accuracy:0.3240, Validation Loss:1.9429, Validation Accuracy:0.3350\n",
    "Epoch #192: Loss:1.9575, Accuracy:0.3269, Validation Loss:1.9404, Validation Accuracy:0.2956\n",
    "Epoch #193: Loss:1.9507, Accuracy:0.3363, Validation Loss:1.9419, Validation Accuracy:0.3300\n",
    "Epoch #194: Loss:1.9547, Accuracy:0.3257, Validation Loss:1.9419, Validation Accuracy:0.2956\n",
    "Epoch #195: Loss:1.9565, Accuracy:0.3228, Validation Loss:1.9391, Validation Accuracy:0.3284\n",
    "Epoch #196: Loss:1.9535, Accuracy:0.3269, Validation Loss:1.9376, Validation Accuracy:0.3136\n",
    "Epoch #197: Loss:1.9619, Accuracy:0.3314, Validation Loss:1.9449, Validation Accuracy:0.2874\n",
    "Epoch #198: Loss:1.9556, Accuracy:0.3248, Validation Loss:1.9343, Validation Accuracy:0.3300\n",
    "Epoch #199: Loss:1.9536, Accuracy:0.3302, Validation Loss:1.9343, Validation Accuracy:0.3005\n",
    "Epoch #200: Loss:1.9488, Accuracy:0.3240, Validation Loss:1.9376, Validation Accuracy:0.3350\n",
    "Epoch #201: Loss:1.9500, Accuracy:0.3216, Validation Loss:1.9392, Validation Accuracy:0.2939\n",
    "Epoch #202: Loss:1.9494, Accuracy:0.3248, Validation Loss:1.9312, Validation Accuracy:0.3284\n",
    "Epoch #203: Loss:1.9479, Accuracy:0.3298, Validation Loss:1.9373, Validation Accuracy:0.3054\n",
    "Epoch #204: Loss:1.9507, Accuracy:0.3224, Validation Loss:1.9363, Validation Accuracy:0.3350\n",
    "Epoch #205: Loss:1.9444, Accuracy:0.3269, Validation Loss:1.9342, Validation Accuracy:0.3054\n",
    "Epoch #206: Loss:1.9476, Accuracy:0.3253, Validation Loss:1.9308, Validation Accuracy:0.3284\n",
    "Epoch #207: Loss:1.9441, Accuracy:0.3322, Validation Loss:1.9302, Validation Accuracy:0.3103\n",
    "Epoch #208: Loss:1.9416, Accuracy:0.3285, Validation Loss:1.9309, Validation Accuracy:0.3186\n",
    "Epoch #209: Loss:1.9431, Accuracy:0.3310, Validation Loss:1.9263, Validation Accuracy:0.3103\n",
    "Epoch #210: Loss:1.9404, Accuracy:0.3306, Validation Loss:1.9325, Validation Accuracy:0.3202\n",
    "Epoch #211: Loss:1.9395, Accuracy:0.3306, Validation Loss:1.9348, Validation Accuracy:0.3136\n",
    "Epoch #212: Loss:1.9497, Accuracy:0.3261, Validation Loss:1.9253, Validation Accuracy:0.3087\n",
    "Epoch #213: Loss:1.9425, Accuracy:0.3261, Validation Loss:1.9339, Validation Accuracy:0.3071\n",
    "Epoch #214: Loss:1.9437, Accuracy:0.3285, Validation Loss:1.9245, Validation Accuracy:0.3317\n",
    "Epoch #215: Loss:1.9414, Accuracy:0.3347, Validation Loss:1.9307, Validation Accuracy:0.3169\n",
    "Epoch #216: Loss:1.9411, Accuracy:0.3277, Validation Loss:1.9260, Validation Accuracy:0.3235\n",
    "Epoch #217: Loss:1.9413, Accuracy:0.3281, Validation Loss:1.9406, Validation Accuracy:0.3153\n",
    "Epoch #218: Loss:1.9377, Accuracy:0.3400, Validation Loss:1.9318, Validation Accuracy:0.3153\n",
    "Epoch #219: Loss:1.9379, Accuracy:0.3322, Validation Loss:1.9258, Validation Accuracy:0.3284\n",
    "Epoch #220: Loss:1.9383, Accuracy:0.3281, Validation Loss:1.9234, Validation Accuracy:0.3038\n",
    "Epoch #221: Loss:1.9346, Accuracy:0.3290, Validation Loss:1.9196, Validation Accuracy:0.3218\n",
    "Epoch #222: Loss:1.9325, Accuracy:0.3318, Validation Loss:1.9219, Validation Accuracy:0.3218\n",
    "Epoch #223: Loss:1.9319, Accuracy:0.3326, Validation Loss:1.9211, Validation Accuracy:0.3169\n",
    "Epoch #224: Loss:1.9334, Accuracy:0.3298, Validation Loss:1.9187, Validation Accuracy:0.3186\n",
    "Epoch #225: Loss:1.9335, Accuracy:0.3339, Validation Loss:1.9260, Validation Accuracy:0.3087\n",
    "Epoch #226: Loss:1.9304, Accuracy:0.3372, Validation Loss:1.9253, Validation Accuracy:0.3153\n",
    "Epoch #227: Loss:1.9426, Accuracy:0.3306, Validation Loss:1.9169, Validation Accuracy:0.3333\n",
    "Epoch #228: Loss:1.9385, Accuracy:0.3310, Validation Loss:1.9309, Validation Accuracy:0.2956\n",
    "Epoch #229: Loss:1.9327, Accuracy:0.3347, Validation Loss:1.9168, Validation Accuracy:0.3120\n",
    "Epoch #230: Loss:1.9342, Accuracy:0.3253, Validation Loss:1.9200, Validation Accuracy:0.3317\n",
    "Epoch #231: Loss:1.9345, Accuracy:0.3363, Validation Loss:1.9182, Validation Accuracy:0.3153\n",
    "Epoch #232: Loss:1.9365, Accuracy:0.3335, Validation Loss:1.9310, Validation Accuracy:0.3038\n",
    "Epoch #233: Loss:1.9335, Accuracy:0.3277, Validation Loss:1.9183, Validation Accuracy:0.3366\n",
    "Epoch #234: Loss:1.9277, Accuracy:0.3355, Validation Loss:1.9170, Validation Accuracy:0.3235\n",
    "Epoch #235: Loss:1.9359, Accuracy:0.3281, Validation Loss:1.9135, Validation Accuracy:0.3268\n",
    "Epoch #236: Loss:1.9316, Accuracy:0.3314, Validation Loss:1.9279, Validation Accuracy:0.3087\n",
    "Epoch #237: Loss:1.9291, Accuracy:0.3339, Validation Loss:1.9125, Validation Accuracy:0.3251\n",
    "Epoch #238: Loss:1.9250, Accuracy:0.3355, Validation Loss:1.9124, Validation Accuracy:0.3268\n",
    "Epoch #239: Loss:1.9245, Accuracy:0.3339, Validation Loss:1.9138, Validation Accuracy:0.3202\n",
    "Epoch #240: Loss:1.9249, Accuracy:0.3388, Validation Loss:1.9111, Validation Accuracy:0.3169\n",
    "Epoch #241: Loss:1.9252, Accuracy:0.3347, Validation Loss:1.9108, Validation Accuracy:0.3300\n",
    "Epoch #242: Loss:1.9252, Accuracy:0.3331, Validation Loss:1.9146, Validation Accuracy:0.3432\n",
    "Epoch #243: Loss:1.9285, Accuracy:0.3347, Validation Loss:1.9184, Validation Accuracy:0.3153\n",
    "Epoch #244: Loss:1.9331, Accuracy:0.3359, Validation Loss:1.9240, Validation Accuracy:0.3350\n",
    "Epoch #245: Loss:1.9320, Accuracy:0.3355, Validation Loss:1.9120, Validation Accuracy:0.3333\n",
    "Epoch #246: Loss:1.9310, Accuracy:0.3314, Validation Loss:1.9228, Validation Accuracy:0.2956\n",
    "Epoch #247: Loss:1.9341, Accuracy:0.3326, Validation Loss:1.9160, Validation Accuracy:0.3366\n",
    "Epoch #248: Loss:1.9328, Accuracy:0.3318, Validation Loss:1.9079, Validation Accuracy:0.3202\n",
    "Epoch #249: Loss:1.9169, Accuracy:0.3359, Validation Loss:1.9152, Validation Accuracy:0.3202\n",
    "Epoch #250: Loss:1.9292, Accuracy:0.3335, Validation Loss:1.9080, Validation Accuracy:0.3366\n",
    "Epoch #251: Loss:1.9407, Accuracy:0.3302, Validation Loss:1.9409, Validation Accuracy:0.3038\n",
    "Epoch #252: Loss:1.9356, Accuracy:0.3384, Validation Loss:1.9174, Validation Accuracy:0.3432\n",
    "Epoch #253: Loss:1.9260, Accuracy:0.3359, Validation Loss:1.9275, Validation Accuracy:0.3005\n",
    "Epoch #254: Loss:1.9309, Accuracy:0.3372, Validation Loss:1.9079, Validation Accuracy:0.3432\n",
    "Epoch #255: Loss:1.9232, Accuracy:0.3347, Validation Loss:1.9148, Validation Accuracy:0.3169\n",
    "Epoch #256: Loss:1.9228, Accuracy:0.3310, Validation Loss:1.9207, Validation Accuracy:0.3153\n",
    "Epoch #257: Loss:1.9265, Accuracy:0.3318, Validation Loss:1.9085, Validation Accuracy:0.3251\n",
    "Epoch #258: Loss:1.9314, Accuracy:0.3355, Validation Loss:1.9347, Validation Accuracy:0.3120\n",
    "Epoch #259: Loss:1.9288, Accuracy:0.3347, Validation Loss:1.9277, Validation Accuracy:0.3350\n",
    "Epoch #260: Loss:1.9316, Accuracy:0.3355, Validation Loss:1.9070, Validation Accuracy:0.3169\n",
    "Epoch #261: Loss:1.9165, Accuracy:0.3396, Validation Loss:1.9087, Validation Accuracy:0.3399\n",
    "Epoch #262: Loss:1.9183, Accuracy:0.3368, Validation Loss:1.9143, Validation Accuracy:0.3186\n",
    "Epoch #263: Loss:1.9148, Accuracy:0.3384, Validation Loss:1.9015, Validation Accuracy:0.3284\n",
    "Epoch #264: Loss:1.9157, Accuracy:0.3343, Validation Loss:1.9013, Validation Accuracy:0.3350\n",
    "Epoch #265: Loss:1.9122, Accuracy:0.3355, Validation Loss:1.9127, Validation Accuracy:0.3284\n",
    "Epoch #266: Loss:1.9148, Accuracy:0.3392, Validation Loss:1.9025, Validation Accuracy:0.3235\n",
    "Epoch #267: Loss:1.9136, Accuracy:0.3384, Validation Loss:1.9059, Validation Accuracy:0.3465\n",
    "Epoch #268: Loss:1.9163, Accuracy:0.3326, Validation Loss:1.9160, Validation Accuracy:0.3087\n",
    "Epoch #269: Loss:1.9110, Accuracy:0.3384, Validation Loss:1.9031, Validation Accuracy:0.3399\n",
    "Epoch #270: Loss:1.9136, Accuracy:0.3405, Validation Loss:1.9020, Validation Accuracy:0.3300\n",
    "Epoch #271: Loss:1.9089, Accuracy:0.3396, Validation Loss:1.8955, Validation Accuracy:0.3350\n",
    "Epoch #272: Loss:1.9065, Accuracy:0.3441, Validation Loss:1.8964, Validation Accuracy:0.3317\n",
    "Epoch #273: Loss:1.9081, Accuracy:0.3441, Validation Loss:1.9084, Validation Accuracy:0.3218\n",
    "Epoch #274: Loss:1.9093, Accuracy:0.3405, Validation Loss:1.8954, Validation Accuracy:0.3300\n",
    "Epoch #275: Loss:1.9067, Accuracy:0.3425, Validation Loss:1.8972, Validation Accuracy:0.3300\n",
    "Epoch #276: Loss:1.9052, Accuracy:0.3458, Validation Loss:1.8952, Validation Accuracy:0.3284\n",
    "Epoch #277: Loss:1.9044, Accuracy:0.3396, Validation Loss:1.8984, Validation Accuracy:0.3284\n",
    "Epoch #278: Loss:1.9038, Accuracy:0.3446, Validation Loss:1.8985, Validation Accuracy:0.3448\n",
    "Epoch #279: Loss:1.9114, Accuracy:0.3405, Validation Loss:1.9044, Validation Accuracy:0.3284\n",
    "Epoch #280: Loss:1.9211, Accuracy:0.3359, Validation Loss:1.8992, Validation Accuracy:0.3465\n",
    "Epoch #281: Loss:1.9103, Accuracy:0.3400, Validation Loss:1.8983, Validation Accuracy:0.3399\n",
    "Epoch #282: Loss:1.9067, Accuracy:0.3413, Validation Loss:1.9020, Validation Accuracy:0.3202\n",
    "Epoch #283: Loss:1.9107, Accuracy:0.3429, Validation Loss:1.8966, Validation Accuracy:0.3415\n",
    "Epoch #284: Loss:1.9074, Accuracy:0.3454, Validation Loss:1.8955, Validation Accuracy:0.3317\n",
    "Epoch #285: Loss:1.9066, Accuracy:0.3413, Validation Loss:1.8926, Validation Accuracy:0.3415\n",
    "Epoch #286: Loss:1.9025, Accuracy:0.3405, Validation Loss:1.8951, Validation Accuracy:0.3235\n",
    "Epoch #287: Loss:1.9019, Accuracy:0.3495, Validation Loss:1.8888, Validation Accuracy:0.3415\n",
    "Epoch #288: Loss:1.8988, Accuracy:0.3433, Validation Loss:1.8900, Validation Accuracy:0.3415\n",
    "Epoch #289: Loss:1.8970, Accuracy:0.3454, Validation Loss:1.8993, Validation Accuracy:0.3300\n",
    "Epoch #290: Loss:1.8985, Accuracy:0.3466, Validation Loss:1.8949, Validation Accuracy:0.3333\n",
    "Epoch #291: Loss:1.8999, Accuracy:0.3491, Validation Loss:1.8937, Validation Accuracy:0.3465\n",
    "Epoch #292: Loss:1.9016, Accuracy:0.3470, Validation Loss:1.8909, Validation Accuracy:0.3300\n",
    "Epoch #293: Loss:1.8989, Accuracy:0.3470, Validation Loss:1.8906, Validation Accuracy:0.3350\n",
    "Epoch #294: Loss:1.9004, Accuracy:0.3466, Validation Loss:1.9167, Validation Accuracy:0.3415\n",
    "Epoch #295: Loss:1.9030, Accuracy:0.3466, Validation Loss:1.8914, Validation Accuracy:0.3383\n",
    "Epoch #296: Loss:1.9011, Accuracy:0.3462, Validation Loss:1.8942, Validation Accuracy:0.3383\n",
    "Epoch #297: Loss:1.9065, Accuracy:0.3458, Validation Loss:1.8987, Validation Accuracy:0.3448\n",
    "Epoch #298: Loss:1.9004, Accuracy:0.3446, Validation Loss:1.8924, Validation Accuracy:0.3317\n",
    "Epoch #299: Loss:1.8970, Accuracy:0.3499, Validation Loss:1.8859, Validation Accuracy:0.3317\n",
    "Epoch #300: Loss:1.8962, Accuracy:0.3458, Validation Loss:1.8957, Validation Accuracy:0.3251\n",
    "\n",
    "Test:\n",
    "Test Loss:1.89573467, Accuracy:0.3251\n",
    "Labels: ['by', 'eb', 'aa', 'ds', 'yd', 'ib', 'ck', 'ek', 'sg', 'eo', 'sk', 'mb', 'my', 'eg', 'ce']\n",
    "Confusion Matrix:\n",
    "      by  eb  aa  ds  yd  ib  ck  ek  sg  eo  sk  mb  my  eg  ce\n",
    "t:by  23   0   1   0   0   1   0   0   3   4   0   6   0   2   0\n",
    "t:eb   0  41   0   2   3   0   0   4   0   0   0   0   0   0   0\n",
    "t:aa   1   1  11   1   2   1   0   3   1   1   0   6   0   6   0\n",
    "t:ds   1   6   1   4   1   0   0  10   2   0   0   4   1   1   0\n",
    "t:yd   5   0   0   0  29  10   0   2   7   0   0   9   0   0   0\n",
    "t:ib   3   0   0   0  13  18   0   1   2   1   0  12   0   4   0\n",
    "t:ck   2   4   0   3   1   0   0   4   0   0   0   6   0   3   0\n",
    "t:ek   0  18   2   4   5   0   0  10   0   0   0   5   0   4   0\n",
    "t:sg  13   0   0   0   4  17   0   0   4   3   0   7   1   2   0\n",
    "t:eo   6   0   0   0   0   0   0   0   0  24   0   0   0   4   0\n",
    "t:sk   0  14   0   5   2   0   0  10   0   0   0   0   1   1   0\n",
    "t:mb   8   4   0   1   6   9   0   5   2   0   0  15   0   2   0\n",
    "t:my   0  10   0   0   4   0   0   6   0   0   0   0   0   0   0\n",
    "t:eg   9   1   8   3   2   1   0   2   0   4   0   1   0  19   0\n",
    "t:ce   7   1   0   0   4   1   0   2   4   1   0   5   0   2   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          by       0.29      0.57      0.39        40\n",
    "          eb       0.41      0.82      0.55        50\n",
    "          aa       0.48      0.32      0.39        34\n",
    "          ds       0.17      0.13      0.15        31\n",
    "          yd       0.38      0.47      0.42        62\n",
    "          ib       0.31      0.33      0.32        54\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ek       0.17      0.21      0.19        48\n",
    "          sg       0.16      0.08      0.11        51\n",
    "          eo       0.63      0.71      0.67        34\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          mb       0.20      0.29      0.23        52\n",
    "          my       0.00      0.00      0.00        20\n",
    "          eg       0.38      0.38      0.38        50\n",
    "          ce       0.00      0.00      0.00        27\n",
    "\n",
    "    accuracy                           0.33       609\n",
    "   macro avg       0.24      0.29      0.25       609\n",
    "weighted avg       0.27      0.33      0.28       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 09:11:30 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 16 minutes, 33 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.709460491421579, 2.7017358653063845, 2.695508132035705, 2.6902582222604985, 2.6855370144930184, 2.681470494747945, 2.6777012633964152, 2.674373846335951, 2.6713078601411215, 2.668666479818535, 2.6663553319345357, 2.664577992678863, 2.6631225376880816, 2.662086004889853, 2.6613731720960394, 2.660867911058498, 2.6605597791217623, 2.6602931547243216, 2.659981404227772, 2.659708950515647, 2.6593978119209676, 2.6590192372771515, 2.6584622331440744, 2.6577754192947363, 2.656856027925739, 2.655569909632891, 2.653927186831269, 2.651648349949879, 2.648326365622785, 2.6434553374210603, 2.636330456569277, 2.626033297900496, 2.6127541217897914, 2.599069307981845, 2.590721051289726, 2.581152021395553, 2.5676153767089342, 2.5578111584354897, 2.547809250836302, 2.5383190114314136, 2.5285280976193683, 2.5193201509015313, 2.507880845093375, 2.4978550098995465, 2.48785395496976, 2.4893684140567123, 2.4773863313233324, 2.4601024755311913, 2.4491337434020144, 2.434854906376555, 2.4164395528082387, 2.3950442731478336, 2.375415280926208, 2.3529659785660617, 2.3323186653588204, 2.3178609869946007, 2.30057422320048, 2.291252831520118, 2.290922536834316, 2.269448131959035, 2.257158289597735, 2.234594601519981, 2.2304703112697757, 2.2129280492785726, 2.2040893142838, 2.1943621956460384, 2.184071934281899, 2.1767769376632615, 2.16647189278125, 2.1592737821914096, 2.1538233373356963, 2.1584580747169033, 2.1375809240419485, 2.1315367535025813, 2.124636652434401, 2.118344719578284, 2.1170493753868564, 2.104262284653136, 2.106824512356412, 2.092071168724148, 2.0881359401007593, 2.085417301392516, 2.0792908881881162, 2.0735966849992624, 2.0680986482325836, 2.067157740271933, 2.065931349356578, 2.062003040548616, 2.0523986019720195, 2.056747858747473, 2.0491725650718453, 2.0423564268841923, 2.0382425037315133, 2.040021574164455, 2.03454607382588, 2.0422212259327055, 2.028133300529129, 2.023728086639116, 2.0261200883705626, 2.020247656723549, 2.019539671186939, 2.0173282386438403, 2.01685633115189, 2.009754804946323, 2.015105139445789, 2.008141229501107, 2.0118658755996153, 2.02320544238161, 2.0138794614372193, 2.0029530953891173, 2.0000171301204386, 2.00171267046717, 1.9971120320321696, 1.9953350502086196, 1.994616477947517, 1.9957253566907935, 2.000370399705295, 1.9952327469103834, 2.0030509147346507, 1.9969656236457511, 1.9893683580733676, 1.9959595115313977, 1.986116837593918, 1.9972346420162808, 2.0046870281739384, 1.9857506438820625, 1.983055322824049, 1.985451706720299, 1.979927723239404, 1.9763503213625628, 1.9821706883034291, 1.9777690450154697, 1.9728611862326686, 1.9742313721301326, 1.9724853514450524, 1.9778918855883219, 1.9729236699090216, 1.991806179040367, 1.9734472995516898, 1.9675780556276319, 1.9739054185024818, 1.9776927234699768, 1.9794213118028563, 1.968913533613208, 1.963570310955956, 1.9676315293131987, 1.9616015258876758, 1.9624273765263298, 1.966116219514305, 1.9598205265740456, 1.966436226966933, 1.9707175548049225, 1.958218133704024, 1.9702658224575624, 1.9701899074764282, 1.965455330651382, 1.96567365806091, 1.9558098181127914, 1.9616226952259959, 1.9650475182165261, 1.9579249379670092, 1.9553654293708613, 1.955146283938967, 1.9629572988143695, 1.9518533580996134, 1.9548283787979477, 1.961237888226564, 1.951856972940254, 1.9557488641911147, 1.9545316114801492, 1.9552488738092884, 1.9636456541631413, 1.95869753180662, 1.9549328247314604, 1.9549354880509902, 1.950199865746772, 1.9486428922033074, 1.9442069524614682, 1.942813436973271, 1.9436887105305989, 1.9459854152989504, 1.978981033530337, 1.9462837499546495, 1.9401294253338341, 1.9412078669505755, 1.940334950566096, 1.941231915907711, 1.9492779138248737, 1.9666529093274145, 1.9396129568613614, 1.94285443753053, 1.9404080379968403, 1.9418672687314413, 1.9419309392155488, 1.9391016913165013, 1.937565643407637, 1.9448904184676548, 1.9343019019206757, 1.9343253785166248, 1.9375591469907212, 1.9392006984485195, 1.9311923855435476, 1.9373264651384652, 1.9362597244322202, 1.9342040859028231, 1.9307693883116022, 1.9302283541126595, 1.9309213560790264, 1.9263499155028896, 1.9324944183744233, 1.9347754736447764, 1.9252583980560303, 1.9338700049029196, 1.9245327029909407, 1.9307206137035475, 1.925984170440774, 1.9405967508043562, 1.9318373426427982, 1.9258028047621152, 1.923441538278301, 1.9196475517182123, 1.921852220846906, 1.9210530597783857, 1.918698081046294, 1.925959523871223, 1.9252983335595217, 1.9168775060102465, 1.9309195700928887, 1.9168471123393143, 1.9200397647660354, 1.918183709991781, 1.9309641946908485, 1.918346741320856, 1.9170131818414322, 1.913472914343397, 1.9278655594401368, 1.9125078119863625, 1.9123578711683527, 1.913783663012124, 1.9110913235565712, 1.9108028601738816, 1.9145623349595344, 1.918374177857573, 1.924042118006739, 1.9120165025463636, 1.9227724437447409, 1.9160224567297448, 1.9079039627303827, 1.9152336418139329, 1.9079637691892426, 1.9409195815009632, 1.917355096594649, 1.9274688614608815, 1.9079399263526027, 1.9148318354523632, 1.9206664049370927, 1.908486759721352, 1.9347044321508047, 1.9276762131986946, 1.9070211576515035, 1.9087279825570744, 1.9142642974462023, 1.901524353888626, 1.9013379442280736, 1.9126731206239347, 1.9025033953154615, 1.9058790747168028, 1.916013445172991, 1.9030783975065635, 1.9020108591355323, 1.8955306145553714, 1.896360402428262, 1.9083617669216713, 1.8953625881808929, 1.897177468966968, 1.8952369595983345, 1.8984214971805442, 1.8984830217016937, 1.9043588456261922, 1.8992030248657628, 1.8982556801907142, 1.9020231676414878, 1.8965873937497193, 1.8954786970501853, 1.892590933441137, 1.8950989553689566, 1.8887565615533413, 1.8899621092431456, 1.8993346642195101, 1.89489677876283, 1.893660321024251, 1.8909141942981038, 1.8905572787489993, 1.9166901843692674, 1.891449383914177, 1.8942208928231927, 1.8986900860844378, 1.8923945873241705, 1.8858994893448302, 1.8957346386118672], 'val_acc': [0.0886699504330632, 0.05090311976464707, 0.08210180623362022, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.1018062386362032, 0.10837438313538218, 0.1133004916076394, 0.1264367808017433, 0.13793103357743355, 0.13464696142571705, 0.14121510582702304, 0.14614121420140727, 0.1510673226736645, 0.14942528645099679, 0.14285714204969077, 0.17733990096399938, 0.1740558287144099, 0.1674876842152309, 0.16912972024215267, 0.16912972024215267, 0.15763546736858944, 0.1756978653285695, 0.16091953942243298, 0.18226600963200254, 0.1740558287144099, 0.1806239734093348, 0.1806239735072078, 0.19540229853248753, 0.2315270912960441, 0.2413793083384315, 0.23481116383925252, 0.25123152528294596, 0.2643678144770499, 0.23645319986617427, 0.2594417058090467, 0.2528735612119947, 0.2692939228514341, 0.2610837423253333, 0.2692939227535611, 0.2725779951988966, 0.27914613979594854, 0.2873563203220493, 0.28243021184979206, 0.2840722479745868, 0.2807881756271243, 0.28899835625109804, 0.28735632051779525, 0.2906403923758928, 0.2840722478767138, 0.2906403924737658, 0.2939244646254823, 0.2857142842951275, 0.2988505729998665, 0.2873563203220493, 0.30541871759691847, 0.2906403924737658, 0.29064039257163876, 0.30213464544520197, 0.2922824285985605, 0.2988505731956125, 0.30213464524945605, 0.2873563203220493, 0.29556650084815, 0.29885057290199357, 0.2988505729998665, 0.29885057290199357, 0.29720853677719883, 0.31362889802514626, 0.30541871769479145, 0.30706075372171326, 0.2873563198326844, 0.3037766811785048, 0.3037766812763778, 0.29885057290199357, 0.308702789748635, 0.29720853677719883, 0.30541871749904553, 0.29885057290199357, 0.3037766812763778, 0.2939244646254823, 0.302134645347329, 0.29064039218014687, 0.2840722480724598, 0.2840722475830949, 0.2955665006524041, 0.302134645347329, 0.3070607538195862, 0.29556650084815, 0.2889983560553521, 0.3087027900422539, 0.29392446433186337, 0.29720853707081774, 0.2889983560553521, 0.30870278994438094, 0.29228242801132265, 0.302134645347329, 0.2988505729998665, 0.31034482616704867, 0.29885057339135845, 0.29392446472335526, 0.2939244646254823, 0.3037766816678697, 0.30213464515158306, 0.3152709345414329, 0.3119868622918434, 0.31034482626492166, 0.308702789846508, 0.3234811152632796, 0.30541871759691847, 0.32183907933423084, 0.295566500750277, 0.32183907913848486, 0.30541871779266444, 0.30541871769479145, 0.3119868620960975, 0.31362889861238413, 0.3037766813742508, 0.3087027901401269, 0.31034482587342976, 0.3119868622918434, 0.302134645347329, 0.3119868622918434, 0.3201970429158172, 0.3169129704704817, 0.31855500669314946, 0.31362889851451115, 0.3119868620960975, 0.3169129705683547, 0.3152709346393059, 0.30213464515158306, 0.3152709346393059, 0.29720853687507176, 0.315270934247814, 0.31855500669314946, 0.3103448260691757, 0.30049260912466125, 0.31527093444355997, 0.30541871759691847, 0.31034482616704867, 0.308702789748635, 0.32512315119232843, 0.2988505729998665, 0.31034482587342976, 0.31855500669314946, 0.2906403923758928, 0.3234811151654067, 0.3169129704704817, 0.3185550068888954, 0.29392446472335526, 0.3152709345414329, 0.2906403923758928, 0.3284072237355368, 0.3119868619982245, 0.31362889812301925, 0.3251231512902014, 0.2988505729998665, 0.31855500698676836, 0.31691297037260874, 0.32512315119232843, 0.3136288983187652, 0.30049260922253423, 0.3284072236376639, 0.3070607534280943, 0.3185550072803873, 0.2988505730977395, 0.33497536823471585, 0.29556650084815, 0.33004925976245864, 0.29556650084815, 0.3284072237355368, 0.31362889802514626, 0.2873563202241763, 0.33004925976245864, 0.30049260922253423, 0.3349753683325888, 0.29392446472335526, 0.3284072234419179, 0.30541871759691847, 0.3349753684304618, 0.30541871769479145, 0.3284072236376639, 0.31034482587342976, 0.3185550064974035, 0.31034482587342976, 0.3201970428179442, 0.31362889802514626, 0.30870278965076203, 0.30706075372171326, 0.3316912956915074, 0.31691297027473575, 0.3234811151654067, 0.3152709346393059, 0.315270934247814, 0.3284072236376639, 0.3037766814721237, 0.32183907874699297, 0.32183907884486596, 0.3169129704704817, 0.31855500669314946, 0.30870278994438094, 0.315270934247814, 0.33333333191417513, 0.295566501043896, 0.3119868619982245, 0.3316912957893804, 0.31527093444355997, 0.3037766816678697, 0.33661740445738353, 0.3234811148717877, 0.32676518741499616, 0.3087027900422539, 0.3251231512902014, 0.32676518751286915, 0.3201970428179442, 0.3169129705683547, 0.33004925966458565, 0.34318554925018147, 0.315270934345687, 0.3349753680389699, 0.3333333321099211, 0.295566500946023, 0.3366174046531295, 0.3201970429158172, 0.3201970430136901, 0.33661740445738353, 0.3037766812763778, 0.3431855491523085, 0.3004926094182802, 0.3431855490544355, 0.3169129704704817, 0.3152709345414329, 0.32512315138807435, 0.3119868619982245, 0.33497536852833476, 0.31691297076410063, 0.339901476804846, 0.3185550068888954, 0.3284072236376639, 0.33497536823471585, 0.3284072237355368, 0.3234811151654067, 0.34646962159764394, 0.3087027901401269, 0.339901476902719, 0.33004925995820455, 0.33497536823471585, 0.33169129598512637, 0.32183907923635785, 0.33004925976245864, 0.33004925995820455, 0.3284072236376639, 0.3284072237355368, 0.3448275852771032, 0.3284072235397909, 0.34646962130402503, 0.33990147670697307, 0.3201970429158172, 0.34154351302751373, 0.3316912958872534, 0.3415435128317678, 0.3234811152632796, 0.3415435127338948, 0.34154351263602184, 0.33004925976245864, 0.3333333320120481, 0.34646962140189796, 0.33004925986033157, 0.3349753683325888, 0.3415435131253867, 0.33825944048430534, 0.33825944068005126, 0.3448275852771032, 0.3316912960829993, 0.33169129598512637, 0.3251231515838203], 'loss': [2.715278298654106, 2.7064098130995733, 2.699160343420824, 2.693548262927077, 2.6886018543517567, 2.683873847671603, 2.680094935125394, 2.676270473370562, 2.6730685981147344, 2.670174022572731, 2.6677646032838607, 2.665557797292909, 2.6641111811573257, 2.66280644914208, 2.662083896817123, 2.6612123546169526, 2.6609419549515114, 2.660544026020371, 2.6601861426962476, 2.659906308411083, 2.659478931701159, 2.659117769656485, 2.6585388229612943, 2.657805723527129, 2.6569009682970615, 2.6554636296305567, 2.653821180978105, 2.65165739607762, 2.648351655407853, 2.644111248991573, 2.6369207711190414, 2.62749937182824, 2.6130264773750698, 2.596619053787764, 2.5819790097967066, 2.568295611982718, 2.5576612919997386, 2.5487157399649494, 2.535697887663479, 2.526249839441977, 2.5156424079832354, 2.5063493952858864, 2.500244709598455, 2.488388555299575, 2.479996599847531, 2.476255226918559, 2.468562022763356, 2.4648323768952545, 2.442447275694391, 2.4306755282306085, 2.417829218145758, 2.4019899352620024, 2.3853118900400903, 2.3653289279898577, 2.351284876790135, 2.3352509494679663, 2.31853456732184, 2.308905760708286, 2.3086999634697696, 2.2881428302435904, 2.274279091441411, 2.2590357989990735, 2.243757854692745, 2.237914624204381, 2.2282716353571144, 2.217636922154828, 2.2070613354872872, 2.200457476296709, 2.191698881634941, 2.1837101190241945, 2.1753252808807813, 2.172034691980977, 2.164557111140884, 2.1538010546314155, 2.1474496431174463, 2.140908602865325, 2.1369465653656445, 2.1319138059870664, 2.1272580378843773, 2.1243882996835257, 2.11772310004342, 2.113972525919732, 2.111738715964911, 2.106169762112032, 2.1041473957547416, 2.100121331459688, 2.102018111148654, 2.095781027169198, 2.093707532216881, 2.0882283732631612, 2.085577064177339, 2.081644434312041, 2.078308939688994, 2.0757091594672548, 2.0739069969012753, 2.078382416966025, 2.0753026439423925, 2.0664140843268046, 2.0641043062817146, 2.063325726276061, 2.068655984455555, 2.0554922327123872, 2.054599422204176, 2.056653743798728, 2.0539579376792516, 2.0518463330591974, 2.047602069255508, 2.0431625245777734, 2.0471388433992983, 2.0438474137190674, 2.0364405547813713, 2.0368122856719784, 2.037998832177822, 2.032380772224442, 2.0310114032189213, 2.031479978463488, 2.0299990184008463, 2.0318655300923685, 2.0295156252702404, 2.0367643319116238, 2.028485150209932, 2.023294280637706, 2.022598689684388, 2.0232476853247294, 2.0219691380338256, 2.0278837722919314, 2.018581404872009, 2.017102098807662, 2.012975230207189, 2.0083069995688216, 2.006810637029534, 2.0144358647677443, 2.0075869395747565, 2.0018443992740074, 2.000837316552227, 2.0023947068553194, 2.003240485945277, 2.00483409110036, 2.0180046190226593, 1.9965273188369719, 1.9952510467055398, 1.9982551144868197, 1.9983084826498796, 2.0009846860623215, 1.994387126801195, 1.9888357346552354, 1.98830079797357, 1.9895779865723127, 1.9876978863436094, 1.989127202200449, 1.9850936909231072, 1.9854477316447108, 1.990948839892597, 1.9852199671939168, 1.9835375041687513, 1.9880805945249553, 1.9879910588998815, 1.9851408370956014, 1.9824269635476615, 1.9786128457811578, 1.9867667988830033, 1.9760647613654636, 1.9767724473863166, 1.9743627015570107, 1.9746380230239775, 1.9763578067325225, 1.9739654530245176, 1.9730993865206992, 1.9756109037438456, 1.9733247729542318, 1.9782025884553882, 1.9773470858039308, 1.9762313151506428, 1.969965837770419, 1.972275967274848, 1.9751566470770865, 1.9669345614846483, 1.9664908748877368, 1.9651595994921924, 1.9672169087114275, 1.964237804138685, 1.9671220542469063, 1.9816155599617615, 1.9622278179231365, 1.9591367682392347, 1.9563242713528737, 1.9609079679669297, 1.9607196724145564, 1.9592403902899802, 1.9702470544427326, 1.961532641436285, 1.9574777448446599, 1.9506781342582782, 1.9546523430019433, 1.9564791058123234, 1.9535175866414878, 1.961937123989912, 1.9556150022718206, 1.9535501946658813, 1.9488436346915714, 1.9499985961209088, 1.9493889394971624, 1.9478807589846225, 1.9506963591800823, 1.9443777631685228, 1.9476211528758494, 1.9441377232941268, 1.9416435524668292, 1.9431265935271183, 1.9404112299854506, 1.939516542090038, 1.9497336225587973, 1.9424757097780827, 1.943744010063657, 1.941396102180716, 1.9410787571137447, 1.9413292515693994, 1.9376551822960009, 1.9378951662619746, 1.9382750162598534, 1.934553509962877, 1.932484514414652, 1.9319326342988064, 1.93339987280922, 1.9334902264499076, 1.9303645338610702, 1.9426088245008026, 1.9384819996675182, 1.9326598807771593, 1.934162612617383, 1.934519630535917, 1.9365044634934567, 1.9334885925727703, 1.927748778664356, 1.9358739533708325, 1.9315567748992106, 1.929056966476127, 1.9250376188289948, 1.9244830845807368, 1.9249233703593698, 1.9251871939802072, 1.925221628574865, 1.9284536113974005, 1.9331304766069448, 1.9320390502040636, 1.9310213201099842, 1.9341132924297262, 1.9328043331110991, 1.9168906456635963, 1.929194455469903, 1.9406783120098545, 1.935562718232799, 1.9259738791894618, 1.9308615830889473, 1.923172195834056, 1.9227688470660294, 1.9265109472451023, 1.9314043834224128, 1.9288291295695843, 1.9316430046328283, 1.9165077068967251, 1.9183153779110136, 1.9148023728227714, 1.9156559133921316, 1.912189333198986, 1.9147922640708437, 1.9136114911621847, 1.9162952659556018, 1.9110209190380403, 1.9135671773730363, 1.9088798309988064, 1.906498452525364, 1.9081207256297557, 1.9093040107456811, 1.906691311957655, 1.9051814983023265, 1.9043517949889572, 1.9037772446442434, 1.9114238058517112, 1.921118898558176, 1.9103086842648547, 1.9066606061904092, 1.9107081772120826, 1.9074204660783802, 1.9065779618903598, 1.902480440218101, 1.9019434502971735, 1.8988464307491295, 1.8970208275734277, 1.8984938438912926, 1.8999072776927595, 1.9016228121164154, 1.8989109948920029, 1.900386671021244, 1.9030189168037086, 1.9011297171610337, 1.9065287846559371, 1.9004093958367068, 1.8970315405965097, 1.8962071002631218], 'acc': [0.08911704327107944, 0.08911704265606232, 0.0689938395940058, 0.09568788475018507, 0.10225872666684019, 0.10225872722678116, 0.10225872704931353, 0.10225872664848147, 0.10225872743178686, 0.10225872744096623, 0.10225872664848147, 0.10225872644347576, 0.10225872645265513, 0.10225872723596052, 0.10225872703095482, 0.10225872683512846, 0.10225872663930212, 0.10225872663012275, 0.10225872744096623, 0.10225872663012275, 0.10225872645265513, 0.10225872644347576, 0.10225872663012275, 0.10225872684430783, 0.10225872743178686, 0.10225872722678116, 0.10225872683512846, 0.10225872723596052, 0.10225872704013417, 0.10431211553131531, 0.10800821408476428, 0.11663244443752438, 0.12238193089467544, 0.13141683849221136, 0.1482546192481043, 0.15523614051772827, 0.1552361385594648, 0.14825461905227794, 0.15811088325673794, 0.1556468181357981, 0.17207392186110024, 0.1839835720018195, 0.18357289555870776, 0.1798767968002531, 0.17494866638090575, 0.17700205288628534, 0.16550308016780954, 0.17084188841941175, 0.17618069806015713, 0.1802874732433648, 0.17577001962206448, 0.1868583153283082, 0.1868583161116136, 0.19876796781894362, 0.20041067809539653, 0.22751540145712468, 0.2332648875042643, 0.22956878758921026, 0.23326488613347987, 0.23613962965579494, 0.24928131402150805, 0.24804928177313637, 0.25338808941890084, 0.25462012462302647, 0.2574948671662098, 0.2632443544066662, 0.26242299682061043, 0.2636550304581253, 0.2739219696981951, 0.2759753613501359, 0.28172484702398154, 0.2702258746971585, 0.27761807182241516, 0.28747433328530625, 0.28665297585835936, 0.2821355252295304, 0.2841889123040303, 0.2809034917878419, 0.28377823609346237, 0.28870636473201383, 0.2837782323360443, 0.28213552225541777, 0.2878850109033761, 0.28911704313338904, 0.2891170407834729, 0.29240246211968407, 0.28993839938537785, 0.28665297726586125, 0.2919917882590323, 0.2960985640480778, 0.2960985626405759, 0.29691991866002093, 0.2973305945156536, 0.2948665282197557, 0.29527720959524356, 0.2940451751744233, 0.2932238185307818, 0.2973305943198273, 0.3002053404246023, 0.294866530998042, 0.2960985622489232, 0.2985626275656894, 0.29445585295160204, 0.3002053364713579, 0.3014373701088727, 0.30061601506855945, 0.3002053400329496, 0.30102669503654544, 0.3039014362089443, 0.30800821278129514, 0.30513347200054897, 0.3051334702381118, 0.3002053386254477, 0.3084188925534548, 0.30841889079101764, 0.30390143875468684, 0.30882957095483, 0.3080082125854688, 0.3117043105606181, 0.3055441482845518, 0.31170431114809716, 0.3154004089274201, 0.31129363529246445, 0.3100616012632969, 0.31745379858437994, 0.3088295678216084, 0.3080082139562532, 0.31416837744399506, 0.31293634697641926, 0.3141683768197986, 0.3182751526455615, 0.30965092305774805, 0.31581108674131625, 0.317043119791352, 0.3162217639677334, 0.31622176631764953, 0.3129363467805929, 0.31252566951745836, 0.31581108634966354, 0.3133470231869872, 0.3236139620354043, 0.3149897320926557, 0.3133470238111837, 0.314989732288482, 0.3174537987802063, 0.31950718788640453, 0.31581108830792703, 0.31622176416355974, 0.3195071864789027, 0.3227926101650301, 0.32073921815815404, 0.32566735290403975, 0.32197125336227966, 0.31663244550233016, 0.31745379936768536, 0.3125256669717158, 0.3104722812312829, 0.31950718471646555, 0.31334702142455007, 0.3211498957395064, 0.3203285413233896, 0.3240246418442814, 0.32238192937702126, 0.3211498983219664, 0.3170431188122203, 0.32197125257897424, 0.31827515401634593, 0.3264887042236524, 0.32238192820206313, 0.3190965100357909, 0.3223819301970441, 0.31950718608725, 0.32197125257897424, 0.317453799171859, 0.3207392189414594, 0.31498973091769755, 0.3190965102316173, 0.32689938477911745, 0.3207392212913756, 0.32566734937916547, 0.32032854128667215, 0.3186858322218948, 0.3268993849749438, 0.32484599708042106, 0.3256673507499499, 0.32402463847851604, 0.3203285419108686, 0.3281314164583688, 0.3215605751567308, 0.32197125042488445, 0.3240246405102144, 0.3268993855624228, 0.3363449676080896, 0.32566734898751276, 0.3227926085984193, 0.3268993855624228, 0.33141683994866983, 0.32484599708042106, 0.33018480494037056, 0.3240246424317605, 0.3215605737859464, 0.3248459968845947, 0.32977412513149346, 0.32238193199619863, 0.32689938477911745, 0.3252566728993363, 0.3322381940098514, 0.328542094859744, 0.33100616177983844, 0.33059548095511215, 0.33059548079600326, 0.32607802696051785, 0.32607802696051785, 0.3285420952513967, 0.3347022583107684, 0.32772074119021516, 0.3281314184166323, 0.340041069928136, 0.33223819263906695, 0.3281314184166323, 0.3289527720494437, 0.33182751478845335, 0.33264887064878945, 0.3297741257189725, 0.33388090546126237, 0.33716632562251553, 0.3305954807592858, 0.3310061625264264, 0.3347022585433122, 0.3252566743068382, 0.3363449709738549, 0.33347022506490626, 0.3277207409943888, 0.335523615541889, 0.3281314178291532, 0.33141683638707814, 0.33388090409047794, 0.3355236121761236, 0.33388090464123954, 0.33880903531148937, 0.33470226089322835, 0.33305954627187834, 0.33470225873913856, 0.3359342919850007, 0.33552361436693084, 0.3314168375987537, 0.33264887162792117, 0.33182751639178154, 0.3359342894025407, 0.3334702262765818, 0.3301848043528915, 0.33839835788924594, 0.33593429315995876, 0.33716632562251553, 0.3347022595224439, 0.33100616193894733, 0.33182751701597807, 0.3355236157377153, 0.33470225932661757, 0.3355236117844709, 0.33963039074345536, 0.3367556472211403, 0.33839835886837766, 0.33429158010521953, 0.33552361397527813, 0.3392197111304046, 0.338398359651683, 0.33264887162792117, 0.33839835706922305, 0.3404517467629004, 0.3396303885526481, 0.3441478421555897, 0.3441478421555897, 0.34045174774203213, 0.34250513148748407, 0.34579055262786895, 0.3396303917225871, 0.3445585205569649, 0.34045174476791945, 0.3359342895983671, 0.34004106714984966, 0.3412731023906927, 0.34291581008468563, 0.34537987461814645, 0.3412731012157346, 0.34045174715455306, 0.34948665416460994, 0.3433264910685208, 0.3453798754014519, 0.34661190884314036, 0.34907597654654016, 0.34702258802782093, 0.3470225854820784, 0.34661190766818223, 0.3466119078640086, 0.346201233183334, 0.34579055301952166, 0.34455852173192303, 0.34989733213761504, 0.3457905528236953]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
