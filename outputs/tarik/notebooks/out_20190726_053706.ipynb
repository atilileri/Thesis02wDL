{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf25.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 05:37:06 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '0', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ds', 'aa', 'ck', 'ek', 'yd', 'eo', 'eb', 'eg', 'ce', 'ib', 'sk', 'by', 'sg', 'mb', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001818E16D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001818B8C7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7189, Accuracy:0.0641, Validation Loss:2.7136, Validation Accuracy:0.0657\n",
    "Epoch #2: Loss:2.7117, Accuracy:0.0637, Validation Loss:2.7077, Validation Accuracy:0.0509\n",
    "Epoch #3: Loss:2.7049, Accuracy:0.0661, Validation Loss:2.7006, Validation Accuracy:0.1232\n",
    "Epoch #4: Loss:2.6985, Accuracy:0.1031, Validation Loss:2.6933, Validation Accuracy:0.0952\n",
    "Epoch #5: Loss:2.6925, Accuracy:0.1055, Validation Loss:2.6879, Validation Accuracy:0.1067\n",
    "Epoch #6: Loss:2.6872, Accuracy:0.1101, Validation Loss:2.6833, Validation Accuracy:0.1166\n",
    "Epoch #7: Loss:2.6827, Accuracy:0.1121, Validation Loss:2.6784, Validation Accuracy:0.1182\n",
    "Epoch #8: Loss:2.6778, Accuracy:0.1138, Validation Loss:2.6734, Validation Accuracy:0.1182\n",
    "Epoch #9: Loss:2.6724, Accuracy:0.1150, Validation Loss:2.6673, Validation Accuracy:0.1248\n",
    "Epoch #10: Loss:2.6678, Accuracy:0.1158, Validation Loss:2.6632, Validation Accuracy:0.1149\n",
    "Epoch #11: Loss:2.6612, Accuracy:0.1142, Validation Loss:2.6542, Validation Accuracy:0.1182\n",
    "Epoch #12: Loss:2.6545, Accuracy:0.1183, Validation Loss:2.6448, Validation Accuracy:0.1330\n",
    "Epoch #13: Loss:2.6460, Accuracy:0.1372, Validation Loss:2.6339, Validation Accuracy:0.1527\n",
    "Epoch #14: Loss:2.6349, Accuracy:0.1413, Validation Loss:2.6186, Validation Accuracy:0.1478\n",
    "Epoch #15: Loss:2.6230, Accuracy:0.1474, Validation Loss:2.6004, Validation Accuracy:0.1544\n",
    "Epoch #16: Loss:2.6060, Accuracy:0.1503, Validation Loss:2.5769, Validation Accuracy:0.1708\n",
    "Epoch #17: Loss:2.5860, Accuracy:0.1577, Validation Loss:2.5520, Validation Accuracy:0.1773\n",
    "Epoch #18: Loss:2.5645, Accuracy:0.1577, Validation Loss:2.5228, Validation Accuracy:0.1757\n",
    "Epoch #19: Loss:2.5525, Accuracy:0.1614, Validation Loss:2.5108, Validation Accuracy:0.1773\n",
    "Epoch #20: Loss:2.5423, Accuracy:0.1548, Validation Loss:2.4923, Validation Accuracy:0.1757\n",
    "Epoch #21: Loss:2.5283, Accuracy:0.1524, Validation Loss:2.4985, Validation Accuracy:0.1741\n",
    "Epoch #22: Loss:2.5324, Accuracy:0.1552, Validation Loss:2.4853, Validation Accuracy:0.1757\n",
    "Epoch #23: Loss:2.5211, Accuracy:0.1614, Validation Loss:2.4844, Validation Accuracy:0.1724\n",
    "Epoch #24: Loss:2.5093, Accuracy:0.1663, Validation Loss:2.4731, Validation Accuracy:0.1626\n",
    "Epoch #25: Loss:2.5058, Accuracy:0.1589, Validation Loss:2.4614, Validation Accuracy:0.1823\n",
    "Epoch #26: Loss:2.4987, Accuracy:0.1634, Validation Loss:2.4570, Validation Accuracy:0.1823\n",
    "Epoch #27: Loss:2.5227, Accuracy:0.1593, Validation Loss:2.4770, Validation Accuracy:0.1658\n",
    "Epoch #28: Loss:2.5260, Accuracy:0.1483, Validation Loss:2.4515, Validation Accuracy:0.1806\n",
    "Epoch #29: Loss:2.4944, Accuracy:0.1667, Validation Loss:2.4561, Validation Accuracy:0.1790\n",
    "Epoch #30: Loss:2.4834, Accuracy:0.1643, Validation Loss:2.4455, Validation Accuracy:0.1741\n",
    "Epoch #31: Loss:2.4774, Accuracy:0.1643, Validation Loss:2.4477, Validation Accuracy:0.1691\n",
    "Epoch #32: Loss:2.4780, Accuracy:0.1602, Validation Loss:2.4466, Validation Accuracy:0.1757\n",
    "Epoch #33: Loss:2.4819, Accuracy:0.1655, Validation Loss:2.4401, Validation Accuracy:0.1724\n",
    "Epoch #34: Loss:2.4767, Accuracy:0.1602, Validation Loss:2.4454, Validation Accuracy:0.1790\n",
    "Epoch #35: Loss:2.4802, Accuracy:0.1651, Validation Loss:2.4471, Validation Accuracy:0.1773\n",
    "Epoch #36: Loss:2.4765, Accuracy:0.1618, Validation Loss:2.4403, Validation Accuracy:0.1741\n",
    "Epoch #37: Loss:2.4746, Accuracy:0.1671, Validation Loss:2.4379, Validation Accuracy:0.1675\n",
    "Epoch #38: Loss:2.4766, Accuracy:0.1684, Validation Loss:2.4270, Validation Accuracy:0.1856\n",
    "Epoch #39: Loss:2.4665, Accuracy:0.1676, Validation Loss:2.4338, Validation Accuracy:0.1658\n",
    "Epoch #40: Loss:2.4755, Accuracy:0.1684, Validation Loss:2.4405, Validation Accuracy:0.1708\n",
    "Epoch #41: Loss:2.4683, Accuracy:0.1721, Validation Loss:2.4343, Validation Accuracy:0.1806\n",
    "Epoch #42: Loss:2.4651, Accuracy:0.1651, Validation Loss:2.4260, Validation Accuracy:0.1741\n",
    "Epoch #43: Loss:2.4718, Accuracy:0.1680, Validation Loss:2.4325, Validation Accuracy:0.1724\n",
    "Epoch #44: Loss:2.4690, Accuracy:0.1692, Validation Loss:2.4353, Validation Accuracy:0.1856\n",
    "Epoch #45: Loss:2.4752, Accuracy:0.1692, Validation Loss:2.4408, Validation Accuracy:0.1675\n",
    "Epoch #46: Loss:2.4738, Accuracy:0.1692, Validation Loss:2.4474, Validation Accuracy:0.1839\n",
    "Epoch #47: Loss:2.4760, Accuracy:0.1717, Validation Loss:2.4288, Validation Accuracy:0.1708\n",
    "Epoch #48: Loss:2.4881, Accuracy:0.1581, Validation Loss:2.4486, Validation Accuracy:0.1691\n",
    "Epoch #49: Loss:2.4674, Accuracy:0.1692, Validation Loss:2.4408, Validation Accuracy:0.1675\n",
    "Epoch #50: Loss:2.4720, Accuracy:0.1692, Validation Loss:2.4334, Validation Accuracy:0.1856\n",
    "Epoch #51: Loss:2.4687, Accuracy:0.1655, Validation Loss:2.4304, Validation Accuracy:0.1724\n",
    "Epoch #52: Loss:2.4676, Accuracy:0.1684, Validation Loss:2.4296, Validation Accuracy:0.1757\n",
    "Epoch #53: Loss:2.4632, Accuracy:0.1725, Validation Loss:2.4189, Validation Accuracy:0.1790\n",
    "Epoch #54: Loss:2.4587, Accuracy:0.1725, Validation Loss:2.4113, Validation Accuracy:0.1691\n",
    "Epoch #55: Loss:2.4560, Accuracy:0.1733, Validation Loss:2.4193, Validation Accuracy:0.1691\n",
    "Epoch #56: Loss:2.4560, Accuracy:0.1741, Validation Loss:2.4211, Validation Accuracy:0.1724\n",
    "Epoch #57: Loss:2.4625, Accuracy:0.1713, Validation Loss:2.4246, Validation Accuracy:0.1773\n",
    "Epoch #58: Loss:2.4620, Accuracy:0.1700, Validation Loss:2.4234, Validation Accuracy:0.1741\n",
    "Epoch #59: Loss:2.4621, Accuracy:0.1676, Validation Loss:2.4240, Validation Accuracy:0.1642\n",
    "Epoch #60: Loss:2.4607, Accuracy:0.1696, Validation Loss:2.4146, Validation Accuracy:0.1823\n",
    "Epoch #61: Loss:2.4514, Accuracy:0.1655, Validation Loss:2.4157, Validation Accuracy:0.1675\n",
    "Epoch #62: Loss:2.4541, Accuracy:0.1692, Validation Loss:2.4144, Validation Accuracy:0.1675\n",
    "Epoch #63: Loss:2.4513, Accuracy:0.1639, Validation Loss:2.4161, Validation Accuracy:0.1593\n",
    "Epoch #64: Loss:2.4518, Accuracy:0.1643, Validation Loss:2.4133, Validation Accuracy:0.1593\n",
    "Epoch #65: Loss:2.4473, Accuracy:0.1634, Validation Loss:2.4128, Validation Accuracy:0.1773\n",
    "Epoch #66: Loss:2.4493, Accuracy:0.1676, Validation Loss:2.4129, Validation Accuracy:0.1872\n",
    "Epoch #67: Loss:2.4495, Accuracy:0.1713, Validation Loss:2.4009, Validation Accuracy:0.1658\n",
    "Epoch #68: Loss:2.4568, Accuracy:0.1647, Validation Loss:2.4223, Validation Accuracy:0.1642\n",
    "Epoch #69: Loss:2.4593, Accuracy:0.1725, Validation Loss:2.4222, Validation Accuracy:0.1839\n",
    "Epoch #70: Loss:2.4583, Accuracy:0.1692, Validation Loss:2.4256, Validation Accuracy:0.1642\n",
    "Epoch #71: Loss:2.4578, Accuracy:0.1692, Validation Loss:2.4271, Validation Accuracy:0.1642\n",
    "Epoch #72: Loss:2.4571, Accuracy:0.1692, Validation Loss:2.4250, Validation Accuracy:0.1658\n",
    "Epoch #73: Loss:2.4567, Accuracy:0.1700, Validation Loss:2.4258, Validation Accuracy:0.1658\n",
    "Epoch #74: Loss:2.4562, Accuracy:0.1700, Validation Loss:2.4245, Validation Accuracy:0.1675\n",
    "Epoch #75: Loss:2.4557, Accuracy:0.1700, Validation Loss:2.4250, Validation Accuracy:0.1658\n",
    "Epoch #76: Loss:2.4559, Accuracy:0.1692, Validation Loss:2.4261, Validation Accuracy:0.1773\n",
    "Epoch #77: Loss:2.4632, Accuracy:0.1692, Validation Loss:2.4221, Validation Accuracy:0.1856\n",
    "Epoch #78: Loss:2.4559, Accuracy:0.1713, Validation Loss:2.4333, Validation Accuracy:0.1724\n",
    "Epoch #79: Loss:2.4596, Accuracy:0.1708, Validation Loss:2.4237, Validation Accuracy:0.1708\n",
    "Epoch #80: Loss:2.4565, Accuracy:0.1749, Validation Loss:2.4230, Validation Accuracy:0.1708\n",
    "Epoch #81: Loss:2.4556, Accuracy:0.1704, Validation Loss:2.4228, Validation Accuracy:0.1823\n",
    "Epoch #82: Loss:2.4543, Accuracy:0.1704, Validation Loss:2.4225, Validation Accuracy:0.1741\n",
    "Epoch #83: Loss:2.4542, Accuracy:0.1713, Validation Loss:2.4215, Validation Accuracy:0.1806\n",
    "Epoch #84: Loss:2.4558, Accuracy:0.1692, Validation Loss:2.4201, Validation Accuracy:0.1757\n",
    "Epoch #85: Loss:2.4546, Accuracy:0.1704, Validation Loss:2.4212, Validation Accuracy:0.1757\n",
    "Epoch #86: Loss:2.4542, Accuracy:0.1704, Validation Loss:2.4197, Validation Accuracy:0.1790\n",
    "Epoch #87: Loss:2.4553, Accuracy:0.1696, Validation Loss:2.4184, Validation Accuracy:0.1757\n",
    "Epoch #88: Loss:2.4547, Accuracy:0.1684, Validation Loss:2.4190, Validation Accuracy:0.1888\n",
    "Epoch #89: Loss:2.4544, Accuracy:0.1704, Validation Loss:2.4225, Validation Accuracy:0.1806\n",
    "Epoch #90: Loss:2.4523, Accuracy:0.1696, Validation Loss:2.4191, Validation Accuracy:0.1773\n",
    "Epoch #91: Loss:2.4522, Accuracy:0.1700, Validation Loss:2.4213, Validation Accuracy:0.1708\n",
    "Epoch #92: Loss:2.4520, Accuracy:0.1680, Validation Loss:2.4218, Validation Accuracy:0.1708\n",
    "Epoch #93: Loss:2.4520, Accuracy:0.1655, Validation Loss:2.4207, Validation Accuracy:0.1839\n",
    "Epoch #94: Loss:2.4515, Accuracy:0.1692, Validation Loss:2.4207, Validation Accuracy:0.1741\n",
    "Epoch #95: Loss:2.4515, Accuracy:0.1700, Validation Loss:2.4200, Validation Accuracy:0.1741\n",
    "Epoch #96: Loss:2.4514, Accuracy:0.1688, Validation Loss:2.4201, Validation Accuracy:0.1757\n",
    "Epoch #97: Loss:2.4511, Accuracy:0.1680, Validation Loss:2.4193, Validation Accuracy:0.1773\n",
    "Epoch #98: Loss:2.4505, Accuracy:0.1684, Validation Loss:2.4191, Validation Accuracy:0.1708\n",
    "Epoch #99: Loss:2.4502, Accuracy:0.1671, Validation Loss:2.4201, Validation Accuracy:0.1724\n",
    "Epoch #100: Loss:2.4500, Accuracy:0.1671, Validation Loss:2.4199, Validation Accuracy:0.1691\n",
    "Epoch #101: Loss:2.4514, Accuracy:0.1708, Validation Loss:2.4190, Validation Accuracy:0.1839\n",
    "Epoch #102: Loss:2.4491, Accuracy:0.1741, Validation Loss:2.4221, Validation Accuracy:0.1806\n",
    "Epoch #103: Loss:2.4518, Accuracy:0.1680, Validation Loss:2.4195, Validation Accuracy:0.1806\n",
    "Epoch #104: Loss:2.4492, Accuracy:0.1708, Validation Loss:2.4197, Validation Accuracy:0.1708\n",
    "Epoch #105: Loss:2.4497, Accuracy:0.1684, Validation Loss:2.4201, Validation Accuracy:0.1691\n",
    "Epoch #106: Loss:2.4485, Accuracy:0.1680, Validation Loss:2.4176, Validation Accuracy:0.1806\n",
    "Epoch #107: Loss:2.4490, Accuracy:0.1717, Validation Loss:2.4193, Validation Accuracy:0.1708\n",
    "Epoch #108: Loss:2.4484, Accuracy:0.1700, Validation Loss:2.4215, Validation Accuracy:0.1790\n",
    "Epoch #109: Loss:2.4481, Accuracy:0.1733, Validation Loss:2.4189, Validation Accuracy:0.1839\n",
    "Epoch #110: Loss:2.4483, Accuracy:0.1680, Validation Loss:2.4181, Validation Accuracy:0.1773\n",
    "Epoch #111: Loss:2.4480, Accuracy:0.1708, Validation Loss:2.4199, Validation Accuracy:0.1757\n",
    "Epoch #112: Loss:2.4486, Accuracy:0.1737, Validation Loss:2.4196, Validation Accuracy:0.1856\n",
    "Epoch #113: Loss:2.4487, Accuracy:0.1700, Validation Loss:2.4209, Validation Accuracy:0.1773\n",
    "Epoch #114: Loss:2.4484, Accuracy:0.1704, Validation Loss:2.4184, Validation Accuracy:0.1839\n",
    "Epoch #115: Loss:2.4483, Accuracy:0.1655, Validation Loss:2.4173, Validation Accuracy:0.1823\n",
    "Epoch #116: Loss:2.4490, Accuracy:0.1729, Validation Loss:2.4166, Validation Accuracy:0.1773\n",
    "Epoch #117: Loss:2.4479, Accuracy:0.1733, Validation Loss:2.4169, Validation Accuracy:0.1839\n",
    "Epoch #118: Loss:2.4481, Accuracy:0.1721, Validation Loss:2.4173, Validation Accuracy:0.1773\n",
    "Epoch #119: Loss:2.4472, Accuracy:0.1725, Validation Loss:2.4158, Validation Accuracy:0.1790\n",
    "Epoch #120: Loss:2.4469, Accuracy:0.1745, Validation Loss:2.4165, Validation Accuracy:0.1806\n",
    "Epoch #121: Loss:2.4479, Accuracy:0.1733, Validation Loss:2.4167, Validation Accuracy:0.1790\n",
    "Epoch #122: Loss:2.4489, Accuracy:0.1688, Validation Loss:2.4176, Validation Accuracy:0.1921\n",
    "Epoch #123: Loss:2.4484, Accuracy:0.1799, Validation Loss:2.4186, Validation Accuracy:0.1790\n",
    "Epoch #124: Loss:2.4476, Accuracy:0.1729, Validation Loss:2.4164, Validation Accuracy:0.1806\n",
    "Epoch #125: Loss:2.4476, Accuracy:0.1729, Validation Loss:2.4172, Validation Accuracy:0.1790\n",
    "Epoch #126: Loss:2.4467, Accuracy:0.1741, Validation Loss:2.4171, Validation Accuracy:0.1790\n",
    "Epoch #127: Loss:2.4462, Accuracy:0.1745, Validation Loss:2.4181, Validation Accuracy:0.1823\n",
    "Epoch #128: Loss:2.4459, Accuracy:0.1803, Validation Loss:2.4183, Validation Accuracy:0.1790\n",
    "Epoch #129: Loss:2.4464, Accuracy:0.1803, Validation Loss:2.4178, Validation Accuracy:0.1888\n",
    "Epoch #130: Loss:2.4465, Accuracy:0.1828, Validation Loss:2.4178, Validation Accuracy:0.1823\n",
    "Epoch #131: Loss:2.4458, Accuracy:0.1823, Validation Loss:2.4183, Validation Accuracy:0.1856\n",
    "Epoch #132: Loss:2.4461, Accuracy:0.1799, Validation Loss:2.4159, Validation Accuracy:0.1954\n",
    "Epoch #133: Loss:2.4470, Accuracy:0.1766, Validation Loss:2.4180, Validation Accuracy:0.1823\n",
    "Epoch #134: Loss:2.4447, Accuracy:0.1811, Validation Loss:2.4175, Validation Accuracy:0.1888\n",
    "Epoch #135: Loss:2.4451, Accuracy:0.1799, Validation Loss:2.4196, Validation Accuracy:0.1790\n",
    "Epoch #136: Loss:2.4444, Accuracy:0.1815, Validation Loss:2.4194, Validation Accuracy:0.1741\n",
    "Epoch #137: Loss:2.4439, Accuracy:0.1828, Validation Loss:2.4193, Validation Accuracy:0.1773\n",
    "Epoch #138: Loss:2.4433, Accuracy:0.1807, Validation Loss:2.4179, Validation Accuracy:0.1757\n",
    "Epoch #139: Loss:2.4412, Accuracy:0.1782, Validation Loss:2.4122, Validation Accuracy:0.1757\n",
    "Epoch #140: Loss:2.4343, Accuracy:0.1758, Validation Loss:2.3988, Validation Accuracy:0.1806\n",
    "Epoch #141: Loss:2.4451, Accuracy:0.1774, Validation Loss:2.4100, Validation Accuracy:0.1790\n",
    "Epoch #142: Loss:2.4473, Accuracy:0.1762, Validation Loss:2.4172, Validation Accuracy:0.1741\n",
    "Epoch #143: Loss:2.4447, Accuracy:0.1807, Validation Loss:2.4209, Validation Accuracy:0.1806\n",
    "Epoch #144: Loss:2.4434, Accuracy:0.1782, Validation Loss:2.4031, Validation Accuracy:0.1806\n",
    "Epoch #145: Loss:2.4358, Accuracy:0.1807, Validation Loss:2.4180, Validation Accuracy:0.1856\n",
    "Epoch #146: Loss:2.4445, Accuracy:0.1741, Validation Loss:2.4194, Validation Accuracy:0.1790\n",
    "Epoch #147: Loss:2.4469, Accuracy:0.1754, Validation Loss:2.4173, Validation Accuracy:0.1839\n",
    "Epoch #148: Loss:2.4467, Accuracy:0.1737, Validation Loss:2.4165, Validation Accuracy:0.1790\n",
    "Epoch #149: Loss:2.4467, Accuracy:0.1708, Validation Loss:2.4174, Validation Accuracy:0.1856\n",
    "Epoch #150: Loss:2.4450, Accuracy:0.1762, Validation Loss:2.4177, Validation Accuracy:0.1839\n",
    "Epoch #151: Loss:2.4442, Accuracy:0.1778, Validation Loss:2.4177, Validation Accuracy:0.1839\n",
    "Epoch #152: Loss:2.4441, Accuracy:0.1795, Validation Loss:2.4196, Validation Accuracy:0.1839\n",
    "Epoch #153: Loss:2.4442, Accuracy:0.1778, Validation Loss:2.4177, Validation Accuracy:0.1757\n",
    "Epoch #154: Loss:2.4430, Accuracy:0.1762, Validation Loss:2.4170, Validation Accuracy:0.1757\n",
    "Epoch #155: Loss:2.4422, Accuracy:0.1758, Validation Loss:2.4184, Validation Accuracy:0.1757\n",
    "Epoch #156: Loss:2.4424, Accuracy:0.1786, Validation Loss:2.4186, Validation Accuracy:0.1741\n",
    "Epoch #157: Loss:2.4422, Accuracy:0.1778, Validation Loss:2.4182, Validation Accuracy:0.1741\n",
    "Epoch #158: Loss:2.4418, Accuracy:0.1778, Validation Loss:2.4183, Validation Accuracy:0.1757\n",
    "Epoch #159: Loss:2.4415, Accuracy:0.1774, Validation Loss:2.4189, Validation Accuracy:0.1741\n",
    "Epoch #160: Loss:2.4422, Accuracy:0.1774, Validation Loss:2.4191, Validation Accuracy:0.1806\n",
    "Epoch #161: Loss:2.4430, Accuracy:0.1770, Validation Loss:2.4177, Validation Accuracy:0.1921\n",
    "Epoch #162: Loss:2.4421, Accuracy:0.1811, Validation Loss:2.4196, Validation Accuracy:0.1823\n",
    "Epoch #163: Loss:2.4403, Accuracy:0.1823, Validation Loss:2.4167, Validation Accuracy:0.1888\n",
    "Epoch #164: Loss:2.4412, Accuracy:0.1828, Validation Loss:2.4171, Validation Accuracy:0.1888\n",
    "Epoch #165: Loss:2.4402, Accuracy:0.1836, Validation Loss:2.4205, Validation Accuracy:0.1757\n",
    "Epoch #166: Loss:2.4401, Accuracy:0.1819, Validation Loss:2.4181, Validation Accuracy:0.1806\n",
    "Epoch #167: Loss:2.4419, Accuracy:0.1782, Validation Loss:2.4179, Validation Accuracy:0.1839\n",
    "Epoch #168: Loss:2.4405, Accuracy:0.1823, Validation Loss:2.4201, Validation Accuracy:0.1757\n",
    "Epoch #169: Loss:2.4399, Accuracy:0.1852, Validation Loss:2.4183, Validation Accuracy:0.1872\n",
    "Epoch #170: Loss:2.4394, Accuracy:0.1840, Validation Loss:2.4186, Validation Accuracy:0.1806\n",
    "Epoch #171: Loss:2.4391, Accuracy:0.1807, Validation Loss:2.4185, Validation Accuracy:0.1757\n",
    "Epoch #172: Loss:2.4393, Accuracy:0.1864, Validation Loss:2.4189, Validation Accuracy:0.1757\n",
    "Epoch #173: Loss:2.4388, Accuracy:0.1832, Validation Loss:2.4191, Validation Accuracy:0.1741\n",
    "Epoch #174: Loss:2.4386, Accuracy:0.1836, Validation Loss:2.4193, Validation Accuracy:0.1741\n",
    "Epoch #175: Loss:2.4384, Accuracy:0.1840, Validation Loss:2.4190, Validation Accuracy:0.1839\n",
    "Epoch #176: Loss:2.4388, Accuracy:0.1860, Validation Loss:2.4193, Validation Accuracy:0.1741\n",
    "Epoch #177: Loss:2.4385, Accuracy:0.1840, Validation Loss:2.4197, Validation Accuracy:0.1741\n",
    "Epoch #178: Loss:2.4388, Accuracy:0.1819, Validation Loss:2.4198, Validation Accuracy:0.1741\n",
    "Epoch #179: Loss:2.4395, Accuracy:0.1815, Validation Loss:2.4191, Validation Accuracy:0.1839\n",
    "Epoch #180: Loss:2.4379, Accuracy:0.1823, Validation Loss:2.4207, Validation Accuracy:0.1675\n",
    "Epoch #181: Loss:2.4383, Accuracy:0.1844, Validation Loss:2.4200, Validation Accuracy:0.1741\n",
    "Epoch #182: Loss:2.4379, Accuracy:0.1864, Validation Loss:2.4186, Validation Accuracy:0.1806\n",
    "Epoch #183: Loss:2.4378, Accuracy:0.1848, Validation Loss:2.4200, Validation Accuracy:0.1741\n",
    "Epoch #184: Loss:2.4387, Accuracy:0.1836, Validation Loss:2.4210, Validation Accuracy:0.1741\n",
    "Epoch #185: Loss:2.4379, Accuracy:0.1860, Validation Loss:2.4203, Validation Accuracy:0.1839\n",
    "Epoch #186: Loss:2.4384, Accuracy:0.1823, Validation Loss:2.4198, Validation Accuracy:0.1675\n",
    "Epoch #187: Loss:2.4373, Accuracy:0.1803, Validation Loss:2.4202, Validation Accuracy:0.1839\n",
    "Epoch #188: Loss:2.4376, Accuracy:0.1848, Validation Loss:2.4206, Validation Accuracy:0.1658\n",
    "Epoch #189: Loss:2.4369, Accuracy:0.1848, Validation Loss:2.4203, Validation Accuracy:0.1823\n",
    "Epoch #190: Loss:2.4369, Accuracy:0.1840, Validation Loss:2.4200, Validation Accuracy:0.1741\n",
    "Epoch #191: Loss:2.4365, Accuracy:0.1836, Validation Loss:2.4212, Validation Accuracy:0.1675\n",
    "Epoch #192: Loss:2.4367, Accuracy:0.1815, Validation Loss:2.4204, Validation Accuracy:0.1823\n",
    "Epoch #193: Loss:2.4361, Accuracy:0.1832, Validation Loss:2.4210, Validation Accuracy:0.1675\n",
    "Epoch #194: Loss:2.4363, Accuracy:0.1860, Validation Loss:2.4210, Validation Accuracy:0.1741\n",
    "Epoch #195: Loss:2.4362, Accuracy:0.1864, Validation Loss:2.4217, Validation Accuracy:0.1708\n",
    "Epoch #196: Loss:2.4359, Accuracy:0.1848, Validation Loss:2.4209, Validation Accuracy:0.1757\n",
    "Epoch #197: Loss:2.4361, Accuracy:0.1807, Validation Loss:2.4199, Validation Accuracy:0.1675\n",
    "Epoch #198: Loss:2.4358, Accuracy:0.1832, Validation Loss:2.4205, Validation Accuracy:0.1708\n",
    "Epoch #199: Loss:2.4357, Accuracy:0.1860, Validation Loss:2.4210, Validation Accuracy:0.1806\n",
    "Epoch #200: Loss:2.4356, Accuracy:0.1873, Validation Loss:2.4228, Validation Accuracy:0.1658\n",
    "Epoch #201: Loss:2.4356, Accuracy:0.1823, Validation Loss:2.4210, Validation Accuracy:0.1708\n",
    "Epoch #202: Loss:2.4359, Accuracy:0.1885, Validation Loss:2.4213, Validation Accuracy:0.1691\n",
    "Epoch #203: Loss:2.4354, Accuracy:0.1848, Validation Loss:2.4216, Validation Accuracy:0.1609\n",
    "Epoch #204: Loss:2.4349, Accuracy:0.1844, Validation Loss:2.4211, Validation Accuracy:0.1708\n",
    "Epoch #205: Loss:2.4350, Accuracy:0.1860, Validation Loss:2.4217, Validation Accuracy:0.1675\n",
    "Epoch #206: Loss:2.4347, Accuracy:0.1864, Validation Loss:2.4214, Validation Accuracy:0.1708\n",
    "Epoch #207: Loss:2.4345, Accuracy:0.1856, Validation Loss:2.4216, Validation Accuracy:0.1708\n",
    "Epoch #208: Loss:2.4344, Accuracy:0.1844, Validation Loss:2.4220, Validation Accuracy:0.1708\n",
    "Epoch #209: Loss:2.4346, Accuracy:0.1856, Validation Loss:2.4225, Validation Accuracy:0.1675\n",
    "Epoch #210: Loss:2.4341, Accuracy:0.1852, Validation Loss:2.4217, Validation Accuracy:0.1708\n",
    "Epoch #211: Loss:2.4341, Accuracy:0.1864, Validation Loss:2.4225, Validation Accuracy:0.1691\n",
    "Epoch #212: Loss:2.4343, Accuracy:0.1848, Validation Loss:2.4227, Validation Accuracy:0.1691\n",
    "Epoch #213: Loss:2.4346, Accuracy:0.1848, Validation Loss:2.4214, Validation Accuracy:0.1708\n",
    "Epoch #214: Loss:2.4338, Accuracy:0.1860, Validation Loss:2.4218, Validation Accuracy:0.1724\n",
    "Epoch #215: Loss:2.4339, Accuracy:0.1901, Validation Loss:2.4228, Validation Accuracy:0.1691\n",
    "Epoch #216: Loss:2.4333, Accuracy:0.1877, Validation Loss:2.4239, Validation Accuracy:0.1675\n",
    "Epoch #217: Loss:2.4343, Accuracy:0.1844, Validation Loss:2.4222, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.4331, Accuracy:0.1873, Validation Loss:2.4229, Validation Accuracy:0.1691\n",
    "Epoch #219: Loss:2.4337, Accuracy:0.1881, Validation Loss:2.4245, Validation Accuracy:0.1741\n",
    "Epoch #220: Loss:2.4360, Accuracy:0.1852, Validation Loss:2.4223, Validation Accuracy:0.1691\n",
    "Epoch #221: Loss:2.4338, Accuracy:0.1873, Validation Loss:2.4228, Validation Accuracy:0.1691\n",
    "Epoch #222: Loss:2.4340, Accuracy:0.1828, Validation Loss:2.4243, Validation Accuracy:0.1593\n",
    "Epoch #223: Loss:2.4336, Accuracy:0.1873, Validation Loss:2.4232, Validation Accuracy:0.1691\n",
    "Epoch #224: Loss:2.4331, Accuracy:0.1877, Validation Loss:2.4235, Validation Accuracy:0.1675\n",
    "Epoch #225: Loss:2.4329, Accuracy:0.1848, Validation Loss:2.4229, Validation Accuracy:0.1708\n",
    "Epoch #226: Loss:2.4327, Accuracy:0.1860, Validation Loss:2.4221, Validation Accuracy:0.1691\n",
    "Epoch #227: Loss:2.4333, Accuracy:0.1848, Validation Loss:2.4232, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.4341, Accuracy:0.1881, Validation Loss:2.4235, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4324, Accuracy:0.1860, Validation Loss:2.4241, Validation Accuracy:0.1675\n",
    "Epoch #230: Loss:2.4331, Accuracy:0.1791, Validation Loss:2.4236, Validation Accuracy:0.1724\n",
    "Epoch #231: Loss:2.4328, Accuracy:0.1864, Validation Loss:2.4230, Validation Accuracy:0.1708\n",
    "Epoch #232: Loss:2.4347, Accuracy:0.1815, Validation Loss:2.4241, Validation Accuracy:0.1691\n",
    "Epoch #233: Loss:2.4345, Accuracy:0.1795, Validation Loss:2.4249, Validation Accuracy:0.1773\n",
    "Epoch #234: Loss:2.4310, Accuracy:0.1893, Validation Loss:2.4262, Validation Accuracy:0.1658\n",
    "Epoch #235: Loss:2.4339, Accuracy:0.1811, Validation Loss:2.4238, Validation Accuracy:0.1708\n",
    "Epoch #236: Loss:2.4342, Accuracy:0.1869, Validation Loss:2.4248, Validation Accuracy:0.1773\n",
    "Epoch #237: Loss:2.4343, Accuracy:0.1860, Validation Loss:2.4272, Validation Accuracy:0.1658\n",
    "Epoch #238: Loss:2.4336, Accuracy:0.1869, Validation Loss:2.4235, Validation Accuracy:0.1708\n",
    "Epoch #239: Loss:2.4331, Accuracy:0.1889, Validation Loss:2.4238, Validation Accuracy:0.1658\n",
    "Epoch #240: Loss:2.4320, Accuracy:0.1836, Validation Loss:2.4238, Validation Accuracy:0.1741\n",
    "Epoch #241: Loss:2.4313, Accuracy:0.1860, Validation Loss:2.4242, Validation Accuracy:0.1691\n",
    "Epoch #242: Loss:2.4310, Accuracy:0.1885, Validation Loss:2.4243, Validation Accuracy:0.1691\n",
    "Epoch #243: Loss:2.4322, Accuracy:0.1889, Validation Loss:2.4240, Validation Accuracy:0.1691\n",
    "Epoch #244: Loss:2.4312, Accuracy:0.1877, Validation Loss:2.4249, Validation Accuracy:0.1675\n",
    "Epoch #245: Loss:2.4316, Accuracy:0.1811, Validation Loss:2.4255, Validation Accuracy:0.1724\n",
    "Epoch #246: Loss:2.4314, Accuracy:0.1869, Validation Loss:2.4242, Validation Accuracy:0.1691\n",
    "Epoch #247: Loss:2.4324, Accuracy:0.1848, Validation Loss:2.4249, Validation Accuracy:0.1658\n",
    "Epoch #248: Loss:2.4309, Accuracy:0.1840, Validation Loss:2.4255, Validation Accuracy:0.1741\n",
    "Epoch #249: Loss:2.4317, Accuracy:0.1828, Validation Loss:2.4245, Validation Accuracy:0.1691\n",
    "Epoch #250: Loss:2.4330, Accuracy:0.1836, Validation Loss:2.4249, Validation Accuracy:0.1691\n",
    "Epoch #251: Loss:2.4317, Accuracy:0.1897, Validation Loss:2.4247, Validation Accuracy:0.1708\n",
    "Epoch #252: Loss:2.4312, Accuracy:0.1823, Validation Loss:2.4249, Validation Accuracy:0.1691\n",
    "Epoch #253: Loss:2.4311, Accuracy:0.1828, Validation Loss:2.4247, Validation Accuracy:0.1691\n",
    "Epoch #254: Loss:2.4319, Accuracy:0.1852, Validation Loss:2.4236, Validation Accuracy:0.1691\n",
    "Epoch #255: Loss:2.4308, Accuracy:0.1856, Validation Loss:2.4253, Validation Accuracy:0.1708\n",
    "Epoch #256: Loss:2.4304, Accuracy:0.1836, Validation Loss:2.4251, Validation Accuracy:0.1724\n",
    "Epoch #257: Loss:2.4305, Accuracy:0.1864, Validation Loss:2.4246, Validation Accuracy:0.1691\n",
    "Epoch #258: Loss:2.4299, Accuracy:0.1877, Validation Loss:2.4251, Validation Accuracy:0.1691\n",
    "Epoch #259: Loss:2.4305, Accuracy:0.1823, Validation Loss:2.4249, Validation Accuracy:0.1741\n",
    "Epoch #260: Loss:2.4303, Accuracy:0.1844, Validation Loss:2.4255, Validation Accuracy:0.1691\n",
    "Epoch #261: Loss:2.4299, Accuracy:0.1869, Validation Loss:2.4252, Validation Accuracy:0.1691\n",
    "Epoch #262: Loss:2.4304, Accuracy:0.1922, Validation Loss:2.4241, Validation Accuracy:0.1724\n",
    "Epoch #263: Loss:2.4301, Accuracy:0.1873, Validation Loss:2.4246, Validation Accuracy:0.1691\n",
    "Epoch #264: Loss:2.4297, Accuracy:0.1877, Validation Loss:2.4251, Validation Accuracy:0.1691\n",
    "Epoch #265: Loss:2.4291, Accuracy:0.1881, Validation Loss:2.4256, Validation Accuracy:0.1691\n",
    "Epoch #266: Loss:2.4299, Accuracy:0.1828, Validation Loss:2.4250, Validation Accuracy:0.1691\n",
    "Epoch #267: Loss:2.4294, Accuracy:0.1860, Validation Loss:2.4252, Validation Accuracy:0.1724\n",
    "Epoch #268: Loss:2.4303, Accuracy:0.1877, Validation Loss:2.4260, Validation Accuracy:0.1691\n",
    "Epoch #269: Loss:2.4301, Accuracy:0.1877, Validation Loss:2.4262, Validation Accuracy:0.1675\n",
    "Epoch #270: Loss:2.4304, Accuracy:0.1864, Validation Loss:2.4263, Validation Accuracy:0.1741\n",
    "Epoch #271: Loss:2.4297, Accuracy:0.1795, Validation Loss:2.4279, Validation Accuracy:0.1658\n",
    "Epoch #272: Loss:2.4303, Accuracy:0.1836, Validation Loss:2.4254, Validation Accuracy:0.1708\n",
    "Epoch #273: Loss:2.4315, Accuracy:0.1906, Validation Loss:2.4257, Validation Accuracy:0.1724\n",
    "Epoch #274: Loss:2.4295, Accuracy:0.1774, Validation Loss:2.4279, Validation Accuracy:0.1609\n",
    "Epoch #275: Loss:2.4301, Accuracy:0.1819, Validation Loss:2.4253, Validation Accuracy:0.1724\n",
    "Epoch #276: Loss:2.4296, Accuracy:0.1848, Validation Loss:2.4259, Validation Accuracy:0.1741\n",
    "Epoch #277: Loss:2.4295, Accuracy:0.1873, Validation Loss:2.4259, Validation Accuracy:0.1691\n",
    "Epoch #278: Loss:2.4287, Accuracy:0.1852, Validation Loss:2.4262, Validation Accuracy:0.1708\n",
    "Epoch #279: Loss:2.4298, Accuracy:0.1815, Validation Loss:2.4269, Validation Accuracy:0.1741\n",
    "Epoch #280: Loss:2.4297, Accuracy:0.1869, Validation Loss:2.4273, Validation Accuracy:0.1708\n",
    "Epoch #281: Loss:2.4299, Accuracy:0.1832, Validation Loss:2.4283, Validation Accuracy:0.1741\n",
    "Epoch #282: Loss:2.4320, Accuracy:0.1786, Validation Loss:2.4270, Validation Accuracy:0.1724\n",
    "Epoch #283: Loss:2.4297, Accuracy:0.1807, Validation Loss:2.4252, Validation Accuracy:0.1724\n",
    "Epoch #284: Loss:2.4300, Accuracy:0.1881, Validation Loss:2.4264, Validation Accuracy:0.1691\n",
    "Epoch #285: Loss:2.4287, Accuracy:0.1860, Validation Loss:2.4263, Validation Accuracy:0.1724\n",
    "Epoch #286: Loss:2.4286, Accuracy:0.1860, Validation Loss:2.4263, Validation Accuracy:0.1691\n",
    "Epoch #287: Loss:2.4284, Accuracy:0.1840, Validation Loss:2.4258, Validation Accuracy:0.1691\n",
    "Epoch #288: Loss:2.4280, Accuracy:0.1885, Validation Loss:2.4253, Validation Accuracy:0.1691\n",
    "Epoch #289: Loss:2.4283, Accuracy:0.1828, Validation Loss:2.4255, Validation Accuracy:0.1708\n",
    "Epoch #290: Loss:2.4280, Accuracy:0.1828, Validation Loss:2.4262, Validation Accuracy:0.1691\n",
    "Epoch #291: Loss:2.4279, Accuracy:0.1877, Validation Loss:2.4264, Validation Accuracy:0.1708\n",
    "Epoch #292: Loss:2.4283, Accuracy:0.1873, Validation Loss:2.4257, Validation Accuracy:0.1724\n",
    "Epoch #293: Loss:2.4283, Accuracy:0.1836, Validation Loss:2.4278, Validation Accuracy:0.1708\n",
    "Epoch #294: Loss:2.4287, Accuracy:0.1823, Validation Loss:2.4274, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4277, Accuracy:0.1864, Validation Loss:2.4268, Validation Accuracy:0.1691\n",
    "Epoch #296: Loss:2.4278, Accuracy:0.1885, Validation Loss:2.4264, Validation Accuracy:0.1691\n",
    "Epoch #297: Loss:2.4289, Accuracy:0.1832, Validation Loss:2.4270, Validation Accuracy:0.1691\n",
    "Epoch #298: Loss:2.4278, Accuracy:0.1856, Validation Loss:2.4268, Validation Accuracy:0.1741\n",
    "Epoch #299: Loss:2.4275, Accuracy:0.1881, Validation Loss:2.4271, Validation Accuracy:0.1708\n",
    "Epoch #300: Loss:2.4289, Accuracy:0.1848, Validation Loss:2.4268, Validation Accuracy:0.1724\n",
    "\n",
    "Test:\n",
    "Test Loss:2.42676115, Accuracy:0.1724\n",
    "Labels: ['ds', 'aa', 'ck', 'ek', 'yd', 'eo', 'eb', 'eg', 'ce', 'ib', 'sk', 'by', 'sg', 'mb', 'my']\n",
    "Confusion Matrix:\n",
    "      ds  aa  ck  ek  yd  eo  eb  eg  ce  ib  sk  by  sg  mb  my\n",
    "t:ds   8   0   0   0   0   0   5   9   0   0   0   2   7   0   0\n",
    "t:aa   8   0   0   0   2   0   2  15   0   0   0   4   3   0   0\n",
    "t:ck   3   0   0   0   0   0   2   7   0   2   0   3   6   0   0\n",
    "t:ek   1   0   0   0   1   0   6   8   0   6   0   8  18   0   0\n",
    "t:yd   0   0   0   0  12   0   1   0   0  27   0   5  17   0   0\n",
    "t:eo   0   0   0   0   1   0   0   4   0   2   0  11  16   0   0\n",
    "t:eb   2   0   0   1   3   0   6  18   0   4   0  10   6   0   0\n",
    "t:eg   7   0   0   0   0   0  11  28   0   0   0   3   1   0   0\n",
    "t:ce   0   0   0   0   0   0   3   8   0   2   0   4  10   0   0\n",
    "t:ib   0   0   0   0   7   0   1   3   0  22   0   5  16   0   0\n",
    "t:sk   2   0   0   0   0   0   5  11   0   1   0   7   7   0   0\n",
    "t:by   2   0   0   0   0   0   3  12   0   2   0   7  14   0   0\n",
    "t:sg   1   0   0   1   0   0   3   4   0  10   0  10  22   0   0\n",
    "t:mb   5   0   0   0   2   0   2   6   0   7   0   9  21   0   0\n",
    "t:my   3   0   0   0   3   0   2   3   0   3   0   1   5   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ds       0.19      0.26      0.22        31\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          yd       0.39      0.19      0.26        62\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          eb       0.12      0.12      0.12        50\n",
    "          eg       0.21      0.56      0.30        50\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ib       0.25      0.41      0.31        54\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          by       0.08      0.17      0.11        40\n",
    "          sg       0.13      0.43      0.20        51\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.09      0.14      0.10       609\n",
    "weighted avg       0.11      0.17      0.12       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 05:52:53 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 47 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.713554279753336, 2.7077232472023547, 2.700569709925033, 2.6933219452405406, 2.687914354264834, 2.6832641623486047, 2.6783746377196413, 2.6733594711973945, 2.6672891063251716, 2.663150517224091, 2.6541894335660636, 2.6447853302133493, 2.633864412166802, 2.618575712338653, 2.600376268521514, 2.5769089771609, 2.5520375910259427, 2.52280620048786, 2.5108184571728134, 2.4923343897257335, 2.498475788066344, 2.4852921293286854, 2.4844426294461455, 2.4730706101372126, 2.461446367461106, 2.456972326942657, 2.476970915332412, 2.45153478017973, 2.4560787658190297, 2.445525889326199, 2.4476974488087673, 2.4466375278917636, 2.4400599989397773, 2.4454078044014413, 2.447056810256883, 2.440318889805836, 2.4378540433686355, 2.4270378169167803, 2.433786549200174, 2.4405429848700715, 2.4343377287164696, 2.426040232475168, 2.432487209442214, 2.435335017581683, 2.4408308911597594, 2.447378660461977, 2.428831571428647, 2.448593659158215, 2.4407781485853524, 2.4334068533234996, 2.430364732476095, 2.4295788155792186, 2.4189015096435798, 2.4113227498942407, 2.419283073719695, 2.421091825895513, 2.4245940895111886, 2.423391190264221, 2.42396483162941, 2.4145844366358613, 2.415677139128762, 2.414414038211841, 2.4160870190324455, 2.4132961592650766, 2.4127620210005536, 2.412917884112579, 2.400861246049502, 2.422290147818955, 2.4221673736039837, 2.4255715207317583, 2.4270764004029273, 2.4250328532972163, 2.4258243583497547, 2.4244841300012245, 2.425043850305241, 2.4260725317330194, 2.422146153175968, 2.4333214595400054, 2.4236783856045827, 2.4230099264624085, 2.422832834701037, 2.4225240188279176, 2.4214702830917534, 2.4201222061132173, 2.421195213039129, 2.419677252056955, 2.41839805377528, 2.4190150336874727, 2.4225047077059942, 2.4191263810362917, 2.421266691242337, 2.421787870341334, 2.4207024127978998, 2.4206681897487545, 2.4199834386703416, 2.4200537345679525, 2.4192549814340123, 2.4190547344915583, 2.420087733683719, 2.4198566031181947, 2.419030245497505, 2.4220927779506187, 2.419454088351997, 2.4196869900269657, 2.4201358801429884, 2.4176218356992223, 2.419304360309845, 2.421521685589319, 2.4188888143436076, 2.418052111157447, 2.419872093278982, 2.419609342694087, 2.42088280601063, 2.4183988367591196, 2.4172955166138648, 2.416616449215142, 2.41692407182089, 2.417303179676701, 2.415824586338989, 2.41651954791816, 2.4167254589657086, 2.4176311093598164, 2.4186430509846004, 2.4164370312087837, 2.417207833385624, 2.417115168422705, 2.418058982623622, 2.4182949579016526, 2.417786878905273, 2.4178209758940197, 2.4182932427755524, 2.4158808954047846, 2.417998418432151, 2.417456304302748, 2.4196121528230865, 2.41941366328786, 2.4192941517665467, 2.4179350903077275, 2.4121795890757993, 2.3988355920819813, 2.4100381016535515, 2.417225138894443, 2.4208981419236006, 2.4030645570927263, 2.418000496471261, 2.4194220874109877, 2.417315677664746, 2.4165435307131613, 2.4174167503081323, 2.4177133653355742, 2.417730455132345, 2.4196326881402426, 2.417714492440811, 2.417034970911461, 2.4183622561456337, 2.4185610138528255, 2.418215192205996, 2.4183041763618855, 2.418907111109967, 2.4190890186134424, 2.4176627145024945, 2.4195552710046124, 2.4167385751194947, 2.4171312830131044, 2.4204653186359626, 2.418137058640153, 2.417892896091605, 2.4200661534746293, 2.4182720380072134, 2.4186180561829866, 2.4185405611404645, 2.4188832136602043, 2.4190865921465243, 2.419325463681777, 2.4189904710929384, 2.419313995708973, 2.4196775961783525, 2.4198009959974116, 2.419123716933778, 2.420691464334873, 2.419968984005682, 2.4186343900088607, 2.420014174309466, 2.4210172928808555, 2.4203015164592974, 2.419848479660861, 2.4202266354083233, 2.4206154522637426, 2.4203005357720384, 2.420035889191776, 2.4211520400932076, 2.4203512394565276, 2.420953313705369, 2.4210363092093634, 2.421677140375272, 2.4209326668130156, 2.419906373485947, 2.420477096278875, 2.4209933472775864, 2.4228082513574307, 2.4209905813871737, 2.4213456958777018, 2.421627232202364, 2.421068212668884, 2.421723552525337, 2.4214462169089734, 2.421579956029632, 2.4219866057334865, 2.4225398358844576, 2.4216560307394697, 2.4225147091500667, 2.4226883432548036, 2.4213558805399926, 2.4217995215323564, 2.42283849645718, 2.423851933972589, 2.422206885708964, 2.4228721510600573, 2.424509989608489, 2.4223430599093634, 2.422831453909036, 2.424324330046455, 2.4232445444379533, 2.4235373251935335, 2.4229161923350566, 2.4221176725303013, 2.423219635764562, 2.423504740146581, 2.424115936548643, 2.4235840618904003, 2.423006622270606, 2.424120107895048, 2.4249003669507005, 2.426172874635468, 2.4237650110216564, 2.424814077825186, 2.427227434070631, 2.42354000417274, 2.423756113584797, 2.423803727223564, 2.4241705050413636, 2.4242641095848896, 2.4239660550416593, 2.424933963221282, 2.4254697477093274, 2.4242404290216504, 2.4249085577446445, 2.4255024157525673, 2.4245386135401983, 2.4248928925869695, 2.4247271145505858, 2.4249498084652403, 2.424697862665837, 2.423623620582919, 2.425294427448893, 2.425094396218486, 2.424569869472084, 2.4251344477993317, 2.4248700780038566, 2.4254540860750797, 2.4252040562371313, 2.424071520224385, 2.4246141640423553, 2.42509729482466, 2.425562240807294, 2.4250385240576735, 2.425196860224155, 2.4259712269349247, 2.426205220089366, 2.4262507455102327, 2.427899182136423, 2.4254316876478774, 2.425738261838265, 2.4279152008113014, 2.425335055697336, 2.425888103021581, 2.425916954410096, 2.426233902744863, 2.426913258673131, 2.4273388636327535, 2.428297386576585, 2.4269970602589876, 2.42520312800979, 2.4264246673615304, 2.426341531703429, 2.426332150382557, 2.4257620430149274, 2.425308302314019, 2.425525304719145, 2.4261903657114563, 2.42636746608565, 2.4256936196231687, 2.4278161145979156, 2.427389107314237, 2.4268107163690775, 2.4263560345215947, 2.426958009331489, 2.4267629206865684, 2.4270625032227615, 2.426761327314455], 'val_acc': [0.0656814444901908, 0.05090311986557858, 0.12315270934737178, 0.09523809523197818, 0.10673234800155136, 0.11658456484819281, 0.11822660087511458, 0.11822660087511458, 0.12479474537429355, 0.11494252862552508, 0.11822660087511458, 0.13300492500730335, 0.15270935870058627, 0.14778325022832905, 0.15435139462963504, 0.17077175617120144, 0.17733990067038044, 0.17569786454558567, 0.17733990067038044, 0.17569786454558567, 0.17405582842079093, 0.17569786474133164, 0.17241379239386917, 0.16256157544935473, 0.18226600894689168, 0.18226600894689168, 0.1658456477968172, 0.18062397282209694, 0.1789819366973022, 0.1740558285186639, 0.16912972024215267, 0.17569786454558567, 0.17241379219812322, 0.17898193679517518, 0.1773399007682534, 0.1740558285186639, 0.16748768411735793, 0.18555008129435416, 0.16584564769894422, 0.1707717569664194, 0.18062397291996993, 0.17405582822504498, 0.17241379219812322, 0.18555008139222715, 0.16748768382373896, 0.1839080452674324, 0.17077175617120144, 0.1691297199485337, 0.16748768392161195, 0.18555008129435416, 0.17241379219812322, 0.17569786454558567, 0.17898193689304814, 0.16912972024215267, 0.1691297200464067, 0.17241379249174216, 0.1773399008661264, 0.1740558287144099, 0.16420361137840353, 0.18226600924051062, 0.16748768392161195, 0.16748768401948494, 0.15927750349338418, 0.15927750349338418, 0.17733990106187233, 0.1871921174191489, 0.1658456478946902, 0.16420361157414948, 0.18390804536530536, 0.16420361167202246, 0.16420361176989545, 0.1658456478946902, 0.1658456478946902, 0.16748768401948494, 0.1658456478946902, 0.1773399007682534, 0.18555008139222715, 0.17241379200237725, 0.17077175607332848, 0.1707717563669474, 0.18226600914263763, 0.1740558285186639, 0.18062397311571587, 0.17569786464345866, 0.17569786454558567, 0.17898193679517518, 0.17569786464345866, 0.18883415364181663, 0.18062397282209694, 0.1773399008661264, 0.1707717563669474, 0.1707717563669474, 0.18390804536530536, 0.1740558286165369, 0.1740558286165369, 0.17569786474133164, 0.17733990096399938, 0.1707717563669474, 0.17241379249174216, 0.16912972024215267, 0.18390804536530536, 0.18062397291996993, 0.18062397311571587, 0.1707717563669474, 0.16912972024215267, 0.18062397311571587, 0.1707717563669474, 0.17898193679517518, 0.18390804546317835, 0.17733990096399938, 0.17569786464345866, 0.18555008129435416, 0.1773399007682534, 0.18390804536530536, 0.18226600914263763, 0.17733990067038044, 0.18390804536530536, 0.17733990067038044, 0.17898193679517518, 0.18062397291996993, 0.17898193679517518, 0.19211822579353313, 0.17898193699092113, 0.1806239730178429, 0.17898193689304814, 0.17898193689304814, 0.1822660093383836, 0.17898193679517518, 0.18883415383756258, 0.1822660093383836, 0.1855500815879731, 0.19540229843461454, 0.1822660093383836, 0.18883415383756258, 0.17898193708879412, 0.1740558287144099, 0.17733990096399938, 0.17569786483920463, 0.17569786474133164, 0.18062397311571587, 0.17898193708879412, 0.17405582842079093, 0.18062397282209694, 0.18062397321358886, 0.1855500815879731, 0.17898193699092113, 0.18390804536530536, 0.17898193728454007, 0.1855500815879731, 0.18390804536530536, 0.18390804536530536, 0.18390804536530536, 0.1756978649370776, 0.1756978649370776, 0.1756978649370776, 0.1740558287144099, 0.1740558287144099, 0.1756978649370776, 0.17405582881228285, 0.18062397311571587, 0.19211822579353313, 0.18226600914263763, 0.18883415354394364, 0.18883415354394364, 0.17569786454558567, 0.1806239730178429, 0.1839080452674324, 0.17569786464345866, 0.1871921174191489, 0.18062397291996993, 0.17569786464345866, 0.17569786464345866, 0.1740558285186639, 0.1740558285186639, 0.1839080452674324, 0.1740558285186639, 0.1740558285186639, 0.1740558285186639, 0.1839080452674324, 0.16748768392161195, 0.1740558285186639, 0.1806239730178429, 0.1740558285186639, 0.1740558285186639, 0.1839080452674324, 0.16748768392161195, 0.18390804536530536, 0.1658456477968172, 0.18226600914263763, 0.17405582842079093, 0.16748768392161195, 0.18226600914263763, 0.16748768392161195, 0.17405582842079093, 0.17077175607332848, 0.17569786454558567, 0.16748768392161195, 0.17077175607332848, 0.18062397291996993, 0.1658456478946902, 0.17077175607332848, 0.1691297199485337, 0.16091953932456, 0.17077175607332848, 0.16748768382373896, 0.17077175607332848, 0.1707717559754555, 0.1707717559754555, 0.16748768382373896, 0.17077175607332848, 0.1691297200464067, 0.1691297199485337, 0.17077175607332848, 0.17241379219812322, 0.1691297200464067, 0.16748768392161195, 0.1691297199485337, 0.1691297199485337, 0.17405582822504498, 0.1691297200464067, 0.1691297199485337, 0.15927750329763823, 0.1691297199485337, 0.16748768382373896, 0.1707717559754555, 0.1691297200464067, 0.17241379210025023, 0.1691297200464067, 0.16748768392161195, 0.17241379309121416, 0.17077175617120144, 0.1691297200464067, 0.17733990047463447, 0.1658456477968172, 0.17077175617120144, 0.17733990037676148, 0.16584564799256318, 0.17077175607332848, 0.1658456477968172, 0.17405582832291797, 0.1691297200464067, 0.1691297200464067, 0.1691297200464067, 0.16748768401948494, 0.17241379309121416, 0.1691297200464067, 0.1658456477968172, 0.17405582822504498, 0.1691297199485337, 0.1691297200464067, 0.1707717558775825, 0.16912972014427968, 0.1691297200464067, 0.1691297200464067, 0.17077175617120144, 0.17241379309121416, 0.1691297200464067, 0.1691297199485337, 0.17405582822504498, 0.1691297200464067, 0.1691297200464067, 0.17241379200237725, 0.1691297200464067, 0.1691297200464067, 0.1691297199485337, 0.1691297199485337, 0.17241379210025023, 0.1691297200464067, 0.16748768382373896, 0.174055828127172, 0.1658456478946902, 0.1707717559754555, 0.17241379309121416, 0.16091953942243298, 0.17241379200237725, 0.17405582832291797, 0.1691297200464067, 0.17077175617120144, 0.17405582842079093, 0.17077175617120144, 0.1740558285186639, 0.17241379219812322, 0.17241379210025023, 0.1691297200464067, 0.17241379210025023, 0.1691297200464067, 0.1691297200464067, 0.1691297200464067, 0.17077175617120144, 0.16912972014427968, 0.17077175617120144, 0.17241379210025023, 0.17077175617120144, 0.16912971985066075, 0.1691297200464067, 0.16912971985066075, 0.16912971985066075, 0.17405582822504498, 0.17077175617120144, 0.17241379200237725], 'loss': [2.718879503391117, 2.7116823679123083, 2.704947573937919, 2.6984733970258272, 2.692503714610419, 2.687192941446324, 2.682717091836479, 2.6778306892520347, 2.6724406790684383, 2.6678292806143635, 2.6612041836646547, 2.654512990720463, 2.645961536372222, 2.6349297203322455, 2.6229502537901643, 2.605992734995221, 2.5860178422634115, 2.564469300843852, 2.552499197738616, 2.542262911160134, 2.5283232342291173, 2.532406311309313, 2.5211151966079304, 2.5093209580963887, 2.5058120667812025, 2.498731165793887, 2.5226926005596497, 2.525964190093399, 2.494417222765192, 2.4833791822868205, 2.4774131950166924, 2.4779934360261326, 2.481914700717652, 2.4767041859440737, 2.480212457077214, 2.476456809092841, 2.474622613644453, 2.476636994692824, 2.466499738379915, 2.4755173232765904, 2.4682605259717123, 2.465076801272633, 2.4717757778246057, 2.468958832351089, 2.475229788903583, 2.4738354740691135, 2.475980565778039, 2.488122928999288, 2.4674137616793965, 2.472032651372514, 2.468743672654859, 2.4676203743388276, 2.4631762643614343, 2.458714045636218, 2.4560248929127533, 2.455988602275966, 2.4625073014833108, 2.461992282201622, 2.462121784613607, 2.4607441822850973, 2.4513542460220306, 2.4540682176789708, 2.451296686489724, 2.451767128742696, 2.4473084695530134, 2.4492632052247285, 2.449466288310057, 2.4567835601203494, 2.459267674824051, 2.4582888876388207, 2.4577524826511956, 2.4571080123619375, 2.456725831981557, 2.4562458487751546, 2.4557261653994145, 2.455875867005491, 2.463189519063648, 2.4558715952495285, 2.459578769260853, 2.4565227690663427, 2.4556366763810114, 2.4543247113237636, 2.4541675261158717, 2.455767735710379, 2.4546386307514667, 2.454191831298922, 2.4553148994210807, 2.4546831822248456, 2.4543589245367343, 2.4522970642152506, 2.4521875457842004, 2.4520198661443877, 2.4520456788965808, 2.451452008801564, 2.4515133646234593, 2.451373867822134, 2.4510597199629953, 2.4505375330453045, 2.4502488711041837, 2.45001762897327, 2.4514399036000154, 2.449124267115975, 2.451754863257281, 2.449207081882861, 2.4496613532855527, 2.4484678705125376, 2.4490173848992254, 2.4483985064700398, 2.4480529016537833, 2.4483341620443295, 2.4480373093724497, 2.4485957538322745, 2.4486818045316534, 2.4483502714051357, 2.448304320458759, 2.4489870095889428, 2.447887687859349, 2.4481121785831648, 2.447247962736251, 2.4469074440198266, 2.447899532416028, 2.4489161439500062, 2.448368106485637, 2.4476158925395235, 2.4475713692651393, 2.446694038536025, 2.4461516704402664, 2.445911859340002, 2.446351034293674, 2.4465467731076345, 2.445775412534052, 2.4460615477278003, 2.4469840759614163, 2.4446728098319053, 2.44506448994427, 2.4443620564266886, 2.443944573745101, 2.4432512238285136, 2.441177365716233, 2.43429448443027, 2.445069444448796, 2.4473010747584474, 2.4446734943429056, 2.4434334101862976, 2.43581432444359, 2.444482234665011, 2.446917200186414, 2.446685913211266, 2.4466970663051097, 2.445004940718351, 2.444224438285436, 2.4441016136498424, 2.444150596281831, 2.4429824217144223, 2.442239770850117, 2.4424134219206826, 2.442230169680084, 2.4418387458064963, 2.441452353886755, 2.4421779883226087, 2.4429663816761433, 2.4421036697755847, 2.4402839190661294, 2.441196767309608, 2.440175632970289, 2.440112100636445, 2.441852971611571, 2.440469700697756, 2.4399319918003903, 2.4393846173061235, 2.439115105517346, 2.4392878535347062, 2.4387905803304433, 2.438558619418918, 2.438399522945866, 2.4387561473023966, 2.4384753736382394, 2.438760833720652, 2.4395382312289007, 2.4378846977280886, 2.438286882357431, 2.4379395754185547, 2.437758209475257, 2.4387320756422666, 2.4378886595888547, 2.4384042904362295, 2.437315572801312, 2.4375945034457915, 2.4368971780585067, 2.436933198454933, 2.436475123223338, 2.4366696347935735, 2.4360704429830125, 2.4363292653947397, 2.4362265269614345, 2.435936579126597, 2.436065271698719, 2.43579401578257, 2.4357171558991104, 2.435596889832671, 2.4355803100969755, 2.435927643766149, 2.4353820201552625, 2.4348718902658386, 2.4350010212931545, 2.434737720332841, 2.434465916298743, 2.4344073321050685, 2.4345968642029185, 2.4340521446243693, 2.434101413358653, 2.434273032437115, 2.4345959715285095, 2.4338381770210344, 2.433881041939988, 2.433255633191651, 2.4342804154821, 2.433149948355109, 2.4337473894781154, 2.43599464721993, 2.4338222006263184, 2.434042739280685, 2.4336156052975193, 2.4331083143026677, 2.432877731812808, 2.43274053303368, 2.4332759010228777, 2.4340617662582553, 2.4324123957318693, 2.4331413164275872, 2.432755441685232, 2.4346846482592195, 2.4345432526277078, 2.430958912064163, 2.433936960937061, 2.434215059417474, 2.4343240915137883, 2.4336293229332204, 2.4331207762998233, 2.4319888007224706, 2.4313135096179876, 2.4309547901153565, 2.432203232825904, 2.4311819525959555, 2.431589020253207, 2.4314076195507326, 2.432381269231714, 2.430923592530237, 2.431713895288581, 2.432959920816598, 2.431735779078834, 2.431190913218003, 2.4310614434111044, 2.431886273781622, 2.430786285654965, 2.430413143737605, 2.430542368761568, 2.429864371434864, 2.430490874754575, 2.430279274104312, 2.4298758318781606, 2.430373752190592, 2.4301350566151205, 2.4297318316583025, 2.429068377864924, 2.4299173037863855, 2.429389104020669, 2.430292890791531, 2.430068680980612, 2.430422994684145, 2.429671094745581, 2.430260985979554, 2.4314694666030228, 2.42945861160388, 2.4300870851324814, 2.4296346179758497, 2.429537639774581, 2.428659520452762, 2.4297753370273285, 2.429720049178576, 2.429893927211879, 2.4319887574448478, 2.4297251295015307, 2.4299743386999046, 2.4286990355662006, 2.4286437523193674, 2.42836385008246, 2.4279658982396373, 2.428344455538834, 2.4279721058369663, 2.4278722606400445, 2.4283110156441126, 2.428314711670611, 2.4287194250055895, 2.4276938139780344, 2.427843150270059, 2.4288672898584323, 2.4278140274650997, 2.4274695679392413, 2.4289113149505868], 'acc': [0.06406570838676341, 0.06365503114657725, 0.06611909706000185, 0.1030800822762738, 0.10554414759304, 0.11006160215675463, 0.11211498985545101, 0.11375770032773028, 0.11498973299529273, 0.11581108824061173, 0.11416837836499087, 0.11827515490980364, 0.13716632395187198, 0.1412731007384079, 0.1474332645994437, 0.15030800854094953, 0.1577002050511891, 0.15770020424952497, 0.16139630380964376, 0.15482546272219083, 0.1523613964262929, 0.15523613993024923, 0.1613963035954587, 0.16632443600978694, 0.15893223829705122, 0.16344969092086112, 0.15934291671678516, 0.1482546204230624, 0.16673511282619266, 0.16427104752778518, 0.16427104772361154, 0.1601642713287283, 0.16550308095111493, 0.1601642699763026, 0.1650924015889667, 0.16180698043022312, 0.16714578966095708, 0.1683778225151665, 0.16755646827651735, 0.16837782410013602, 0.17207392285859072, 0.16509240256809846, 0.16796714628623985, 0.16919917894462297, 0.16919917931791692, 0.1691991791220906, 0.1716632432638987, 0.15811088265090018, 0.1691991775738385, 0.1691991795504607, 0.16550307899285147, 0.16837782411849475, 0.17248459869586466, 0.17248459988918147, 0.17330595373617794, 0.17412730975562296, 0.17125256564582889, 0.17002053280997814, 0.16755646827651735, 0.16960985595685502, 0.16550307938450415, 0.1691991775738385, 0.16386037030136805, 0.16427104791943786, 0.16344969307495094, 0.16755646747485323, 0.17125256703497202, 0.16468172414836452, 0.17248460067248686, 0.16919917855297026, 0.16919917775130613, 0.16919917853461153, 0.17002053437658893, 0.1700205330058045, 0.17002053298744577, 0.16919917776966487, 0.16919917873043788, 0.17125256762245109, 0.17084188980855491, 0.17494866638090575, 0.17043121236795272, 0.17043121217212637, 0.17125256564582889, 0.16919917873043788, 0.17043121240467016, 0.17043121119299465, 0.16960985715017182, 0.16837782292517794, 0.17043121199465874, 0.16960985715017182, 0.1700205341807626, 0.16796714530710813, 0.1655030799719832, 0.16919917853461153, 0.17002053279161944, 0.1687885007390741, 0.16796714667789256, 0.16837782271099286, 0.167145789446772, 0.16714579103174151, 0.1708418901818489, 0.17412730975562296, 0.16796714667789256, 0.1708418886152381, 0.16837782372684204, 0.1679671464820662, 0.17166324345972503, 0.17002053298744577, 0.173305954323657, 0.16796714491545542, 0.17084188841941175, 0.17371663194172682, 0.17002053298744577, 0.17043121121135335, 0.1655030807736473, 0.1728952763322932, 0.1733059543420157, 0.17207392146944755, 0.17248459928334373, 0.17453798737369278, 0.17330595395036302, 0.16878850032906267, 0.1798767968002531, 0.17289527611810812, 0.1728952783089154, 0.1741273109672985, 0.17453798796117184, 0.18028747541581336, 0.1802874732433648, 0.18275153952090403, 0.18234086192119292, 0.1798767972102645, 0.17659137663899996, 0.18110882985028887, 0.1798767968002531, 0.1815195074867174, 0.18275153953926274, 0.18069815184056637, 0.1782340857404948, 0.17577001942623813, 0.17741273128766055, 0.17618069745431936, 0.18069815283805682, 0.17823408673798524, 0.18069815303388317, 0.17412731075311344, 0.17535934380314924, 0.17371663313504362, 0.17084189000438127, 0.17618069862927743, 0.1778234075349459, 0.17946611839887788, 0.17782340911991543, 0.17618069864763616, 0.17577002063791364, 0.17864476376857602, 0.1778234075349459, 0.17782340970739446, 0.17741273130601926, 0.1774127318934983, 0.17700205288628534, 0.1811088290853422, 0.1823408617253666, 0.18275154053675322, 0.18357289616454553, 0.1819301858697339, 0.17823408593632112, 0.18234086131535515, 0.18521560604934575, 0.18398357239347218, 0.18069815086143462, 0.18644763769187966, 0.1831622181364643, 0.18357289516705508, 0.18398357239347218, 0.1860369602696362, 0.18398357180599315, 0.1819301850864285, 0.18151950609757425, 0.18234086290032467, 0.1843942507948474, 0.18644763908102283, 0.18480492843127594, 0.18357289438374969, 0.18603696126712665, 0.18234086309615102, 0.18028747461414923, 0.184804928645461, 0.18480492901875498, 0.1839835727851249, 0.1835728945612173, 0.18151950805583775, 0.18316221656985351, 0.186036961462953, 0.18644763908102283, 0.1848049284129172, 0.18069815225057778, 0.1831622185464757, 0.18603696087547397, 0.18726899372968342, 0.1823408627044983, 0.18850102738555696, 0.18480492782543817, 0.18439425081320612, 0.18603696067964762, 0.18644763888519647, 0.18562628364905684, 0.18439424942406296, 0.18562628225991368, 0.18521560483767022, 0.1864476390626641, 0.18480492882292862, 0.18480492921458133, 0.18603696187296442, 0.19014373824948894, 0.18767967213105863, 0.18439425059902104, 0.18726899273219294, 0.18809034935747573, 0.18521560485602895, 0.18726899492300023, 0.18275154114259098, 0.18726899314220435, 0.18767967232688496, 0.18480492704213278, 0.18603696048382126, 0.1848049278437969, 0.1880903485558116, 0.18603696205043205, 0.17905544038915536, 0.18644763927684918, 0.18151950727253235, 0.17946611898635692, 0.18932238281752295, 0.18110882887115715, 0.18685831530994948, 0.18603696108965903, 0.18685831550577583, 0.18891170381030997, 0.18357289538124014, 0.186036961462953, 0.18850102560476112, 0.1889117046119741, 0.18767967115192688, 0.18110883026030028, 0.18685831513248186, 0.1848049272196004, 0.18398357337260393, 0.18275153993091545, 0.18357289595036047, 0.18973305924227596, 0.18234086170700786, 0.18275153953926274, 0.1852156046602026, 0.1856262826699251, 0.18357289595036047, 0.1864476374960533, 0.18767967134775324, 0.18234086211701928, 0.18439425159651152, 0.1868583157199609, 0.19219712592982657, 0.18726899312384565, 0.18767967193523227, 0.18809034857417034, 0.18275154112423225, 0.18603696206879078, 0.1876796703686215, 0.18767967232688496, 0.1864476384935438, 0.17946611939636833, 0.18357289554034906, 0.19055441487006827, 0.17741273167931323, 0.18193018526389615, 0.18480492743378546, 0.18726899372968342, 0.1852156066368248, 0.1815195074867174, 0.18685831669909264, 0.18316221753062653, 0.17864476472934904, 0.18069815223221905, 0.18809034935747573, 0.18603696128548539, 0.18603696126712665, 0.18398357317677758, 0.18850102619224016, 0.1827515409284059, 0.18275154051839448, 0.1876796709377418, 0.1872689941213361, 0.18357289498958745, 0.18234086211701928, 0.18644763847518506, 0.1885010259964138, 0.18316221656985351, 0.18562628386324193, 0.18809035015913986, 0.18480492843127594]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
