{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf17.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 18:12:33 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'Front', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '05', '04', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F43451BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F42DD16EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6086, Accuracy:0.2119, Validation Loss:1.6070, Validation Accuracy:0.2233\n",
    "Epoch #2: Loss:1.6066, Accuracy:0.2185, Validation Loss:1.6058, Validation Accuracy:0.2282\n",
    "Epoch #3: Loss:1.6054, Accuracy:0.2246, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6048, Accuracy:0.2341, Validation Loss:1.6035, Validation Accuracy:0.2348\n",
    "Epoch #5: Loss:1.6045, Accuracy:0.2333, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6023, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6033, Accuracy:0.2324, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6028, Accuracy:0.2320, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6025, Accuracy:0.2333, Validation Loss:1.6013, Validation Accuracy:0.2315\n",
    "Epoch #12: Loss:1.6023, Accuracy:0.2341, Validation Loss:1.6012, Validation Accuracy:0.2397\n",
    "Epoch #13: Loss:1.6017, Accuracy:0.2357, Validation Loss:1.6001, Validation Accuracy:0.2430\n",
    "Epoch #14: Loss:1.6015, Accuracy:0.2386, Validation Loss:1.6001, Validation Accuracy:0.2447\n",
    "Epoch #15: Loss:1.6014, Accuracy:0.2353, Validation Loss:1.5995, Validation Accuracy:0.2529\n",
    "Epoch #16: Loss:1.6008, Accuracy:0.2415, Validation Loss:1.5990, Validation Accuracy:0.2660\n",
    "Epoch #17: Loss:1.6005, Accuracy:0.2411, Validation Loss:1.5987, Validation Accuracy:0.2447\n",
    "Epoch #18: Loss:1.6003, Accuracy:0.2407, Validation Loss:1.5989, Validation Accuracy:0.2447\n",
    "Epoch #19: Loss:1.5995, Accuracy:0.2394, Validation Loss:1.6010, Validation Accuracy:0.2529\n",
    "Epoch #20: Loss:1.5996, Accuracy:0.2407, Validation Loss:1.6014, Validation Accuracy:0.2529\n",
    "Epoch #21: Loss:1.5988, Accuracy:0.2427, Validation Loss:1.6004, Validation Accuracy:0.2545\n",
    "Epoch #22: Loss:1.5999, Accuracy:0.2378, Validation Loss:1.6009, Validation Accuracy:0.2282\n",
    "Epoch #23: Loss:1.6006, Accuracy:0.2361, Validation Loss:1.6014, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.6002, Accuracy:0.2361, Validation Loss:1.6015, Validation Accuracy:0.2644\n",
    "Epoch #25: Loss:1.6000, Accuracy:0.2439, Validation Loss:1.5994, Validation Accuracy:0.2611\n",
    "Epoch #26: Loss:1.6006, Accuracy:0.2308, Validation Loss:1.5995, Validation Accuracy:0.2479\n",
    "Epoch #27: Loss:1.6018, Accuracy:0.2329, Validation Loss:1.6011, Validation Accuracy:0.2562\n",
    "Epoch #28: Loss:1.6003, Accuracy:0.2390, Validation Loss:1.5988, Validation Accuracy:0.2512\n",
    "Epoch #29: Loss:1.6010, Accuracy:0.2374, Validation Loss:1.6026, Validation Accuracy:0.2299\n",
    "Epoch #30: Loss:1.6026, Accuracy:0.2374, Validation Loss:1.6012, Validation Accuracy:0.2479\n",
    "Epoch #31: Loss:1.6026, Accuracy:0.2287, Validation Loss:1.5974, Validation Accuracy:0.2512\n",
    "Epoch #32: Loss:1.6025, Accuracy:0.2378, Validation Loss:1.5970, Validation Accuracy:0.2430\n",
    "Epoch #33: Loss:1.6003, Accuracy:0.2407, Validation Loss:1.5963, Validation Accuracy:0.2545\n",
    "Epoch #34: Loss:1.5992, Accuracy:0.2415, Validation Loss:1.5982, Validation Accuracy:0.2315\n",
    "Epoch #35: Loss:1.6002, Accuracy:0.2341, Validation Loss:1.6008, Validation Accuracy:0.2332\n",
    "Epoch #36: Loss:1.6006, Accuracy:0.2398, Validation Loss:1.5980, Validation Accuracy:0.2512\n",
    "Epoch #37: Loss:1.5996, Accuracy:0.2411, Validation Loss:1.5984, Validation Accuracy:0.2430\n",
    "Epoch #38: Loss:1.5985, Accuracy:0.2427, Validation Loss:1.5965, Validation Accuracy:0.2578\n",
    "Epoch #39: Loss:1.5972, Accuracy:0.2485, Validation Loss:1.5949, Validation Accuracy:0.2677\n",
    "Epoch #40: Loss:1.5988, Accuracy:0.2378, Validation Loss:1.6013, Validation Accuracy:0.2479\n",
    "Epoch #41: Loss:1.5989, Accuracy:0.2411, Validation Loss:1.6000, Validation Accuracy:0.2397\n",
    "Epoch #42: Loss:1.5986, Accuracy:0.2456, Validation Loss:1.5985, Validation Accuracy:0.2594\n",
    "Epoch #43: Loss:1.5963, Accuracy:0.2464, Validation Loss:1.5980, Validation Accuracy:0.2611\n",
    "Epoch #44: Loss:1.5963, Accuracy:0.2435, Validation Loss:1.5959, Validation Accuracy:0.2644\n",
    "Epoch #45: Loss:1.5975, Accuracy:0.2415, Validation Loss:1.5968, Validation Accuracy:0.2562\n",
    "Epoch #46: Loss:1.5972, Accuracy:0.2485, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #47: Loss:1.5976, Accuracy:0.2427, Validation Loss:1.5996, Validation Accuracy:0.2348\n",
    "Epoch #48: Loss:1.5965, Accuracy:0.2460, Validation Loss:1.5990, Validation Accuracy:0.2463\n",
    "Epoch #49: Loss:1.6005, Accuracy:0.2337, Validation Loss:1.6030, Validation Accuracy:0.2397\n",
    "Epoch #50: Loss:1.6032, Accuracy:0.2304, Validation Loss:1.6038, Validation Accuracy:0.2529\n",
    "Epoch #51: Loss:1.6008, Accuracy:0.2378, Validation Loss:1.6040, Validation Accuracy:0.2611\n",
    "Epoch #52: Loss:1.6000, Accuracy:0.2345, Validation Loss:1.6010, Validation Accuracy:0.2430\n",
    "Epoch #53: Loss:1.5981, Accuracy:0.2370, Validation Loss:1.6014, Validation Accuracy:0.2299\n",
    "Epoch #54: Loss:1.5987, Accuracy:0.2390, Validation Loss:1.6025, Validation Accuracy:0.2233\n",
    "Epoch #55: Loss:1.5992, Accuracy:0.2439, Validation Loss:1.6033, Validation Accuracy:0.2266\n",
    "Epoch #56: Loss:1.5992, Accuracy:0.2427, Validation Loss:1.6068, Validation Accuracy:0.2348\n",
    "Epoch #57: Loss:1.5994, Accuracy:0.2407, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #58: Loss:1.5989, Accuracy:0.2587, Validation Loss:1.6020, Validation Accuracy:0.2381\n",
    "Epoch #59: Loss:1.5979, Accuracy:0.2427, Validation Loss:1.5938, Validation Accuracy:0.2479\n",
    "Epoch #60: Loss:1.5972, Accuracy:0.2419, Validation Loss:1.5939, Validation Accuracy:0.2479\n",
    "Epoch #61: Loss:1.5968, Accuracy:0.2522, Validation Loss:1.5972, Validation Accuracy:0.2545\n",
    "Epoch #62: Loss:1.5949, Accuracy:0.2480, Validation Loss:1.5926, Validation Accuracy:0.2578\n",
    "Epoch #63: Loss:1.5927, Accuracy:0.2559, Validation Loss:1.5929, Validation Accuracy:0.2545\n",
    "Epoch #64: Loss:1.5934, Accuracy:0.2517, Validation Loss:1.5936, Validation Accuracy:0.2677\n",
    "Epoch #65: Loss:1.5920, Accuracy:0.2550, Validation Loss:1.5921, Validation Accuracy:0.2562\n",
    "Epoch #66: Loss:1.5900, Accuracy:0.2550, Validation Loss:1.5912, Validation Accuracy:0.2578\n",
    "Epoch #67: Loss:1.5890, Accuracy:0.2567, Validation Loss:1.5917, Validation Accuracy:0.2660\n",
    "Epoch #68: Loss:1.5881, Accuracy:0.2522, Validation Loss:1.5933, Validation Accuracy:0.2693\n",
    "Epoch #69: Loss:1.5899, Accuracy:0.2542, Validation Loss:1.5967, Validation Accuracy:0.2611\n",
    "Epoch #70: Loss:1.5902, Accuracy:0.2501, Validation Loss:1.6017, Validation Accuracy:0.2447\n",
    "Epoch #71: Loss:1.5964, Accuracy:0.2415, Validation Loss:1.5958, Validation Accuracy:0.2463\n",
    "Epoch #72: Loss:1.5901, Accuracy:0.2567, Validation Loss:1.6048, Validation Accuracy:0.2266\n",
    "Epoch #73: Loss:1.5931, Accuracy:0.2464, Validation Loss:1.5974, Validation Accuracy:0.2627\n",
    "Epoch #74: Loss:1.5935, Accuracy:0.2530, Validation Loss:1.6066, Validation Accuracy:0.2430\n",
    "Epoch #75: Loss:1.5942, Accuracy:0.2415, Validation Loss:1.6047, Validation Accuracy:0.2282\n",
    "Epoch #76: Loss:1.5943, Accuracy:0.2493, Validation Loss:1.6040, Validation Accuracy:0.2414\n",
    "Epoch #77: Loss:1.5930, Accuracy:0.2480, Validation Loss:1.6057, Validation Accuracy:0.2184\n",
    "Epoch #78: Loss:1.5969, Accuracy:0.2550, Validation Loss:1.6076, Validation Accuracy:0.2397\n",
    "Epoch #79: Loss:1.5979, Accuracy:0.2370, Validation Loss:1.6102, Validation Accuracy:0.2348\n",
    "Epoch #80: Loss:1.5936, Accuracy:0.2427, Validation Loss:1.6060, Validation Accuracy:0.2315\n",
    "Epoch #81: Loss:1.6018, Accuracy:0.2341, Validation Loss:1.6099, Validation Accuracy:0.2381\n",
    "Epoch #82: Loss:1.6105, Accuracy:0.2271, Validation Loss:1.6075, Validation Accuracy:0.2102\n",
    "Epoch #83: Loss:1.6058, Accuracy:0.2172, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #84: Loss:1.6077, Accuracy:0.2234, Validation Loss:1.6068, Validation Accuracy:0.2250\n",
    "Epoch #85: Loss:1.6054, Accuracy:0.2246, Validation Loss:1.6053, Validation Accuracy:0.2282\n",
    "Epoch #86: Loss:1.6049, Accuracy:0.2292, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #87: Loss:1.6048, Accuracy:0.2304, Validation Loss:1.6039, Validation Accuracy:0.2397\n",
    "Epoch #88: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2430\n",
    "Epoch #89: Loss:1.6028, Accuracy:0.2337, Validation Loss:1.6028, Validation Accuracy:0.2397\n",
    "Epoch #90: Loss:1.6022, Accuracy:0.2349, Validation Loss:1.6028, Validation Accuracy:0.2430\n",
    "Epoch #91: Loss:1.6020, Accuracy:0.2316, Validation Loss:1.6031, Validation Accuracy:0.2397\n",
    "Epoch #92: Loss:1.6012, Accuracy:0.2345, Validation Loss:1.6025, Validation Accuracy:0.2381\n",
    "Epoch #93: Loss:1.6011, Accuracy:0.2353, Validation Loss:1.6010, Validation Accuracy:0.2381\n",
    "Epoch #94: Loss:1.6010, Accuracy:0.2366, Validation Loss:1.6017, Validation Accuracy:0.2381\n",
    "Epoch #95: Loss:1.6007, Accuracy:0.2353, Validation Loss:1.6026, Validation Accuracy:0.2348\n",
    "Epoch #96: Loss:1.6008, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2299\n",
    "Epoch #97: Loss:1.6014, Accuracy:0.2312, Validation Loss:1.6014, Validation Accuracy:0.2430\n",
    "Epoch #98: Loss:1.6008, Accuracy:0.2370, Validation Loss:1.6016, Validation Accuracy:0.2315\n",
    "Epoch #99: Loss:1.6003, Accuracy:0.2361, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #100: Loss:1.5997, Accuracy:0.2386, Validation Loss:1.6008, Validation Accuracy:0.2463\n",
    "Epoch #101: Loss:1.5996, Accuracy:0.2398, Validation Loss:1.6010, Validation Accuracy:0.2512\n",
    "Epoch #102: Loss:1.5997, Accuracy:0.2382, Validation Loss:1.6004, Validation Accuracy:0.2529\n",
    "Epoch #103: Loss:1.5993, Accuracy:0.2411, Validation Loss:1.6004, Validation Accuracy:0.2479\n",
    "Epoch #104: Loss:1.5994, Accuracy:0.2407, Validation Loss:1.5991, Validation Accuracy:0.2479\n",
    "Epoch #105: Loss:1.5991, Accuracy:0.2444, Validation Loss:1.5995, Validation Accuracy:0.2594\n",
    "Epoch #106: Loss:1.5996, Accuracy:0.2407, Validation Loss:1.5996, Validation Accuracy:0.2479\n",
    "Epoch #107: Loss:1.5983, Accuracy:0.2452, Validation Loss:1.5997, Validation Accuracy:0.2594\n",
    "Epoch #108: Loss:1.5985, Accuracy:0.2472, Validation Loss:1.5991, Validation Accuracy:0.2627\n",
    "Epoch #109: Loss:1.5976, Accuracy:0.2472, Validation Loss:1.5996, Validation Accuracy:0.2529\n",
    "Epoch #110: Loss:1.5977, Accuracy:0.2448, Validation Loss:1.5996, Validation Accuracy:0.2529\n",
    "Epoch #111: Loss:1.5972, Accuracy:0.2497, Validation Loss:1.5998, Validation Accuracy:0.2562\n",
    "Epoch #112: Loss:1.5972, Accuracy:0.2493, Validation Loss:1.5990, Validation Accuracy:0.2512\n",
    "Epoch #113: Loss:1.5975, Accuracy:0.2452, Validation Loss:1.5982, Validation Accuracy:0.2512\n",
    "Epoch #114: Loss:1.5970, Accuracy:0.2476, Validation Loss:1.5983, Validation Accuracy:0.2611\n",
    "Epoch #115: Loss:1.5981, Accuracy:0.2439, Validation Loss:1.5988, Validation Accuracy:0.2529\n",
    "Epoch #116: Loss:1.5976, Accuracy:0.2452, Validation Loss:1.5999, Validation Accuracy:0.2463\n",
    "Epoch #117: Loss:1.5980, Accuracy:0.2419, Validation Loss:1.5994, Validation Accuracy:0.2430\n",
    "Epoch #118: Loss:1.5971, Accuracy:0.2398, Validation Loss:1.5994, Validation Accuracy:0.2414\n",
    "Epoch #119: Loss:1.5977, Accuracy:0.2411, Validation Loss:1.5961, Validation Accuracy:0.2479\n",
    "Epoch #120: Loss:1.5966, Accuracy:0.2415, Validation Loss:1.5977, Validation Accuracy:0.2562\n",
    "Epoch #121: Loss:1.5990, Accuracy:0.2370, Validation Loss:1.6006, Validation Accuracy:0.2479\n",
    "Epoch #122: Loss:1.5975, Accuracy:0.2382, Validation Loss:1.5978, Validation Accuracy:0.2479\n",
    "Epoch #123: Loss:1.5988, Accuracy:0.2374, Validation Loss:1.5954, Validation Accuracy:0.2545\n",
    "Epoch #124: Loss:1.5967, Accuracy:0.2370, Validation Loss:1.5963, Validation Accuracy:0.2562\n",
    "Epoch #125: Loss:1.5975, Accuracy:0.2431, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #126: Loss:1.5960, Accuracy:0.2423, Validation Loss:1.6032, Validation Accuracy:0.2282\n",
    "Epoch #127: Loss:1.5969, Accuracy:0.2357, Validation Loss:1.6029, Validation Accuracy:0.2430\n",
    "Epoch #128: Loss:1.5961, Accuracy:0.2398, Validation Loss:1.6018, Validation Accuracy:0.2512\n",
    "Epoch #129: Loss:1.5954, Accuracy:0.2427, Validation Loss:1.6022, Validation Accuracy:0.2332\n",
    "Epoch #130: Loss:1.5985, Accuracy:0.2386, Validation Loss:1.6106, Validation Accuracy:0.2282\n",
    "Epoch #131: Loss:1.5999, Accuracy:0.2324, Validation Loss:1.6038, Validation Accuracy:0.2529\n",
    "Epoch #132: Loss:1.5974, Accuracy:0.2407, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #133: Loss:1.5973, Accuracy:0.2370, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #134: Loss:1.5986, Accuracy:0.2333, Validation Loss:1.6012, Validation Accuracy:0.2250\n",
    "Epoch #135: Loss:1.5990, Accuracy:0.2333, Validation Loss:1.5992, Validation Accuracy:0.2594\n",
    "Epoch #136: Loss:1.5978, Accuracy:0.2370, Validation Loss:1.5981, Validation Accuracy:0.2479\n",
    "Epoch #137: Loss:1.5988, Accuracy:0.2382, Validation Loss:1.5974, Validation Accuracy:0.2562\n",
    "Epoch #138: Loss:1.5971, Accuracy:0.2407, Validation Loss:1.5965, Validation Accuracy:0.2627\n",
    "Epoch #139: Loss:1.5952, Accuracy:0.2431, Validation Loss:1.5964, Validation Accuracy:0.2726\n",
    "Epoch #140: Loss:1.5960, Accuracy:0.2419, Validation Loss:1.5985, Validation Accuracy:0.2512\n",
    "Epoch #141: Loss:1.5945, Accuracy:0.2407, Validation Loss:1.5983, Validation Accuracy:0.2644\n",
    "Epoch #142: Loss:1.5955, Accuracy:0.2472, Validation Loss:1.5986, Validation Accuracy:0.2693\n",
    "Epoch #143: Loss:1.5942, Accuracy:0.2439, Validation Loss:1.5991, Validation Accuracy:0.2447\n",
    "Epoch #144: Loss:1.5946, Accuracy:0.2423, Validation Loss:1.5989, Validation Accuracy:0.2414\n",
    "Epoch #145: Loss:1.5939, Accuracy:0.2398, Validation Loss:1.5962, Validation Accuracy:0.2709\n",
    "Epoch #146: Loss:1.5941, Accuracy:0.2402, Validation Loss:1.5978, Validation Accuracy:0.2512\n",
    "Epoch #147: Loss:1.5929, Accuracy:0.2349, Validation Loss:1.5962, Validation Accuracy:0.2709\n",
    "Epoch #148: Loss:1.5941, Accuracy:0.2435, Validation Loss:1.5964, Validation Accuracy:0.2578\n",
    "Epoch #149: Loss:1.5938, Accuracy:0.2407, Validation Loss:1.5962, Validation Accuracy:0.2496\n",
    "Epoch #150: Loss:1.5940, Accuracy:0.2513, Validation Loss:1.5976, Validation Accuracy:0.2742\n",
    "Epoch #151: Loss:1.5914, Accuracy:0.2493, Validation Loss:1.5968, Validation Accuracy:0.2562\n",
    "Epoch #152: Loss:1.5914, Accuracy:0.2460, Validation Loss:1.5961, Validation Accuracy:0.2578\n",
    "Epoch #153: Loss:1.5914, Accuracy:0.2444, Validation Loss:1.5938, Validation Accuracy:0.2660\n",
    "Epoch #154: Loss:1.5912, Accuracy:0.2505, Validation Loss:1.5948, Validation Accuracy:0.2775\n",
    "Epoch #155: Loss:1.5910, Accuracy:0.2431, Validation Loss:1.5943, Validation Accuracy:0.2562\n",
    "Epoch #156: Loss:1.5917, Accuracy:0.2444, Validation Loss:1.5965, Validation Accuracy:0.2627\n",
    "Epoch #157: Loss:1.5925, Accuracy:0.2448, Validation Loss:1.5967, Validation Accuracy:0.2315\n",
    "Epoch #158: Loss:1.5930, Accuracy:0.2493, Validation Loss:1.5957, Validation Accuracy:0.2529\n",
    "Epoch #159: Loss:1.5945, Accuracy:0.2370, Validation Loss:1.5948, Validation Accuracy:0.2529\n",
    "Epoch #160: Loss:1.5935, Accuracy:0.2493, Validation Loss:1.5973, Validation Accuracy:0.2529\n",
    "Epoch #161: Loss:1.5937, Accuracy:0.2448, Validation Loss:1.5972, Validation Accuracy:0.2562\n",
    "Epoch #162: Loss:1.5930, Accuracy:0.2394, Validation Loss:1.5987, Validation Accuracy:0.2578\n",
    "Epoch #163: Loss:1.5928, Accuracy:0.2493, Validation Loss:1.6011, Validation Accuracy:0.2496\n",
    "Epoch #164: Loss:1.5925, Accuracy:0.2501, Validation Loss:1.5984, Validation Accuracy:0.2562\n",
    "Epoch #165: Loss:1.5924, Accuracy:0.2476, Validation Loss:1.6012, Validation Accuracy:0.2447\n",
    "Epoch #166: Loss:1.5934, Accuracy:0.2444, Validation Loss:1.6005, Validation Accuracy:0.2430\n",
    "Epoch #167: Loss:1.5945, Accuracy:0.2546, Validation Loss:1.6011, Validation Accuracy:0.2479\n",
    "Epoch #168: Loss:1.5940, Accuracy:0.2530, Validation Loss:1.5996, Validation Accuracy:0.2381\n",
    "Epoch #169: Loss:1.5941, Accuracy:0.2398, Validation Loss:1.6012, Validation Accuracy:0.2447\n",
    "Epoch #170: Loss:1.5934, Accuracy:0.2505, Validation Loss:1.6008, Validation Accuracy:0.2545\n",
    "Epoch #171: Loss:1.5921, Accuracy:0.2530, Validation Loss:1.6013, Validation Accuracy:0.2463\n",
    "Epoch #172: Loss:1.5929, Accuracy:0.2480, Validation Loss:1.5995, Validation Accuracy:0.2447\n",
    "Epoch #173: Loss:1.5934, Accuracy:0.2435, Validation Loss:1.5979, Validation Accuracy:0.2365\n",
    "Epoch #174: Loss:1.5958, Accuracy:0.2361, Validation Loss:1.5962, Validation Accuracy:0.2348\n",
    "Epoch #175: Loss:1.5994, Accuracy:0.2349, Validation Loss:1.5985, Validation Accuracy:0.2611\n",
    "Epoch #176: Loss:1.5947, Accuracy:0.2464, Validation Loss:1.5978, Validation Accuracy:0.2611\n",
    "Epoch #177: Loss:1.5964, Accuracy:0.2366, Validation Loss:1.5992, Validation Accuracy:0.2496\n",
    "Epoch #178: Loss:1.5957, Accuracy:0.2333, Validation Loss:1.6028, Validation Accuracy:0.2315\n",
    "Epoch #179: Loss:1.5958, Accuracy:0.2361, Validation Loss:1.5996, Validation Accuracy:0.2545\n",
    "Epoch #180: Loss:1.5940, Accuracy:0.2411, Validation Loss:1.6015, Validation Accuracy:0.2479\n",
    "Epoch #181: Loss:1.5930, Accuracy:0.2538, Validation Loss:1.6036, Validation Accuracy:0.2562\n",
    "Epoch #182: Loss:1.5923, Accuracy:0.2522, Validation Loss:1.6038, Validation Accuracy:0.2545\n",
    "Epoch #183: Loss:1.5930, Accuracy:0.2575, Validation Loss:1.6007, Validation Accuracy:0.2529\n",
    "Epoch #184: Loss:1.5918, Accuracy:0.2583, Validation Loss:1.6029, Validation Accuracy:0.2365\n",
    "Epoch #185: Loss:1.5910, Accuracy:0.2509, Validation Loss:1.6009, Validation Accuracy:0.2545\n",
    "Epoch #186: Loss:1.5896, Accuracy:0.2497, Validation Loss:1.5992, Validation Accuracy:0.2447\n",
    "Epoch #187: Loss:1.5922, Accuracy:0.2534, Validation Loss:1.5994, Validation Accuracy:0.2512\n",
    "Epoch #188: Loss:1.5924, Accuracy:0.2583, Validation Loss:1.6012, Validation Accuracy:0.2677\n",
    "Epoch #189: Loss:1.5904, Accuracy:0.2505, Validation Loss:1.6020, Validation Accuracy:0.2611\n",
    "Epoch #190: Loss:1.5907, Accuracy:0.2493, Validation Loss:1.6041, Validation Accuracy:0.2479\n",
    "Epoch #191: Loss:1.5904, Accuracy:0.2517, Validation Loss:1.6048, Validation Accuracy:0.2233\n",
    "Epoch #192: Loss:1.5902, Accuracy:0.2489, Validation Loss:1.6043, Validation Accuracy:0.2250\n",
    "Epoch #193: Loss:1.5895, Accuracy:0.2489, Validation Loss:1.6038, Validation Accuracy:0.2365\n",
    "Epoch #194: Loss:1.5897, Accuracy:0.2554, Validation Loss:1.6043, Validation Accuracy:0.2479\n",
    "Epoch #195: Loss:1.5899, Accuracy:0.2534, Validation Loss:1.6026, Validation Accuracy:0.2447\n",
    "Epoch #196: Loss:1.5886, Accuracy:0.2534, Validation Loss:1.6017, Validation Accuracy:0.2348\n",
    "Epoch #197: Loss:1.5895, Accuracy:0.2464, Validation Loss:1.6007, Validation Accuracy:0.2332\n",
    "Epoch #198: Loss:1.5864, Accuracy:0.2509, Validation Loss:1.5960, Validation Accuracy:0.2430\n",
    "Epoch #199: Loss:1.5894, Accuracy:0.2476, Validation Loss:1.5973, Validation Accuracy:0.2299\n",
    "Epoch #200: Loss:1.5911, Accuracy:0.2513, Validation Loss:1.5981, Validation Accuracy:0.2545\n",
    "Epoch #201: Loss:1.5908, Accuracy:0.2550, Validation Loss:1.5963, Validation Accuracy:0.2578\n",
    "Epoch #202: Loss:1.5888, Accuracy:0.2513, Validation Loss:1.5944, Validation Accuracy:0.2430\n",
    "Epoch #203: Loss:1.5894, Accuracy:0.2448, Validation Loss:1.5930, Validation Accuracy:0.2184\n",
    "Epoch #204: Loss:1.5893, Accuracy:0.2423, Validation Loss:1.5950, Validation Accuracy:0.2562\n",
    "Epoch #205: Loss:1.5887, Accuracy:0.2501, Validation Loss:1.5976, Validation Accuracy:0.2217\n",
    "Epoch #206: Loss:1.5903, Accuracy:0.2394, Validation Loss:1.5947, Validation Accuracy:0.2266\n",
    "Epoch #207: Loss:1.5902, Accuracy:0.2378, Validation Loss:1.5958, Validation Accuracy:0.2447\n",
    "Epoch #208: Loss:1.5874, Accuracy:0.2497, Validation Loss:1.5965, Validation Accuracy:0.2381\n",
    "Epoch #209: Loss:1.5877, Accuracy:0.2538, Validation Loss:1.5971, Validation Accuracy:0.2381\n",
    "Epoch #210: Loss:1.5878, Accuracy:0.2419, Validation Loss:1.6039, Validation Accuracy:0.2381\n",
    "Epoch #211: Loss:1.5878, Accuracy:0.2522, Validation Loss:1.5992, Validation Accuracy:0.2332\n",
    "Epoch #212: Loss:1.5862, Accuracy:0.2571, Validation Loss:1.6006, Validation Accuracy:0.2282\n",
    "Epoch #213: Loss:1.5882, Accuracy:0.2476, Validation Loss:1.6002, Validation Accuracy:0.2250\n",
    "Epoch #214: Loss:1.5857, Accuracy:0.2587, Validation Loss:1.5989, Validation Accuracy:0.2250\n",
    "Epoch #215: Loss:1.5881, Accuracy:0.2571, Validation Loss:1.6001, Validation Accuracy:0.2217\n",
    "Epoch #216: Loss:1.5886, Accuracy:0.2513, Validation Loss:1.5996, Validation Accuracy:0.2365\n",
    "Epoch #217: Loss:1.5857, Accuracy:0.2513, Validation Loss:1.5976, Validation Accuracy:0.2463\n",
    "Epoch #218: Loss:1.5856, Accuracy:0.2505, Validation Loss:1.5965, Validation Accuracy:0.2463\n",
    "Epoch #219: Loss:1.5858, Accuracy:0.2489, Validation Loss:1.5958, Validation Accuracy:0.2479\n",
    "Epoch #220: Loss:1.5853, Accuracy:0.2513, Validation Loss:1.5943, Validation Accuracy:0.2496\n",
    "Epoch #221: Loss:1.5852, Accuracy:0.2513, Validation Loss:1.5952, Validation Accuracy:0.2200\n",
    "Epoch #222: Loss:1.5861, Accuracy:0.2439, Validation Loss:1.5942, Validation Accuracy:0.2463\n",
    "Epoch #223: Loss:1.5877, Accuracy:0.2546, Validation Loss:1.5932, Validation Accuracy:0.2529\n",
    "Epoch #224: Loss:1.5864, Accuracy:0.2464, Validation Loss:1.5949, Validation Accuracy:0.2479\n",
    "Epoch #225: Loss:1.5880, Accuracy:0.2509, Validation Loss:1.5993, Validation Accuracy:0.2479\n",
    "Epoch #226: Loss:1.5879, Accuracy:0.2464, Validation Loss:1.5982, Validation Accuracy:0.2611\n",
    "Epoch #227: Loss:1.5875, Accuracy:0.2505, Validation Loss:1.5993, Validation Accuracy:0.2447\n",
    "Epoch #228: Loss:1.5886, Accuracy:0.2501, Validation Loss:1.5997, Validation Accuracy:0.2447\n",
    "Epoch #229: Loss:1.5904, Accuracy:0.2444, Validation Loss:1.5950, Validation Accuracy:0.2512\n",
    "Epoch #230: Loss:1.5906, Accuracy:0.2530, Validation Loss:1.5973, Validation Accuracy:0.2479\n",
    "Epoch #231: Loss:1.5882, Accuracy:0.2476, Validation Loss:1.5951, Validation Accuracy:0.2463\n",
    "Epoch #232: Loss:1.5866, Accuracy:0.2526, Validation Loss:1.5968, Validation Accuracy:0.2496\n",
    "Epoch #233: Loss:1.5861, Accuracy:0.2591, Validation Loss:1.5968, Validation Accuracy:0.2463\n",
    "Epoch #234: Loss:1.5858, Accuracy:0.2513, Validation Loss:1.6006, Validation Accuracy:0.2463\n",
    "Epoch #235: Loss:1.5834, Accuracy:0.2550, Validation Loss:1.6002, Validation Accuracy:0.2545\n",
    "Epoch #236: Loss:1.5838, Accuracy:0.2587, Validation Loss:1.6004, Validation Accuracy:0.2463\n",
    "Epoch #237: Loss:1.5825, Accuracy:0.2575, Validation Loss:1.5997, Validation Accuracy:0.2479\n",
    "Epoch #238: Loss:1.5838, Accuracy:0.2513, Validation Loss:1.5996, Validation Accuracy:0.2529\n",
    "Epoch #239: Loss:1.5845, Accuracy:0.2538, Validation Loss:1.6005, Validation Accuracy:0.2529\n",
    "Epoch #240: Loss:1.5845, Accuracy:0.2559, Validation Loss:1.5978, Validation Accuracy:0.2496\n",
    "Epoch #241: Loss:1.5874, Accuracy:0.2472, Validation Loss:1.5952, Validation Accuracy:0.2545\n",
    "Epoch #242: Loss:1.5894, Accuracy:0.2485, Validation Loss:1.5928, Validation Accuracy:0.2594\n",
    "Epoch #243: Loss:1.5888, Accuracy:0.2517, Validation Loss:1.5935, Validation Accuracy:0.2562\n",
    "Epoch #244: Loss:1.5894, Accuracy:0.2337, Validation Loss:1.5948, Validation Accuracy:0.2627\n",
    "Epoch #245: Loss:1.5874, Accuracy:0.2526, Validation Loss:1.5942, Validation Accuracy:0.2578\n",
    "Epoch #246: Loss:1.5876, Accuracy:0.2517, Validation Loss:1.5970, Validation Accuracy:0.2496\n",
    "Epoch #247: Loss:1.5843, Accuracy:0.2550, Validation Loss:1.5995, Validation Accuracy:0.2594\n",
    "Epoch #248: Loss:1.5823, Accuracy:0.2595, Validation Loss:1.6028, Validation Accuracy:0.2496\n",
    "Epoch #249: Loss:1.5824, Accuracy:0.2464, Validation Loss:1.6065, Validation Accuracy:0.2447\n",
    "Epoch #250: Loss:1.5830, Accuracy:0.2604, Validation Loss:1.6040, Validation Accuracy:0.2447\n",
    "Epoch #251: Loss:1.5828, Accuracy:0.2550, Validation Loss:1.6070, Validation Accuracy:0.2299\n",
    "Epoch #252: Loss:1.5842, Accuracy:0.2571, Validation Loss:1.6078, Validation Accuracy:0.2479\n",
    "Epoch #253: Loss:1.5882, Accuracy:0.2583, Validation Loss:1.6082, Validation Accuracy:0.2299\n",
    "Epoch #254: Loss:1.5828, Accuracy:0.2546, Validation Loss:1.6037, Validation Accuracy:0.2496\n",
    "Epoch #255: Loss:1.5824, Accuracy:0.2616, Validation Loss:1.6049, Validation Accuracy:0.2545\n",
    "Epoch #256: Loss:1.5825, Accuracy:0.2628, Validation Loss:1.6040, Validation Accuracy:0.2529\n",
    "Epoch #257: Loss:1.5836, Accuracy:0.2587, Validation Loss:1.6026, Validation Accuracy:0.2430\n",
    "Epoch #258: Loss:1.5806, Accuracy:0.2595, Validation Loss:1.6012, Validation Accuracy:0.2578\n",
    "Epoch #259: Loss:1.5800, Accuracy:0.2554, Validation Loss:1.6035, Validation Accuracy:0.2414\n",
    "Epoch #260: Loss:1.5799, Accuracy:0.2645, Validation Loss:1.6032, Validation Accuracy:0.2512\n",
    "Epoch #261: Loss:1.5795, Accuracy:0.2550, Validation Loss:1.6033, Validation Accuracy:0.2381\n",
    "Epoch #262: Loss:1.5789, Accuracy:0.2669, Validation Loss:1.6043, Validation Accuracy:0.2611\n",
    "Epoch #263: Loss:1.5791, Accuracy:0.2657, Validation Loss:1.6052, Validation Accuracy:0.2381\n",
    "Epoch #264: Loss:1.5779, Accuracy:0.2567, Validation Loss:1.6025, Validation Accuracy:0.2709\n",
    "Epoch #265: Loss:1.5823, Accuracy:0.2637, Validation Loss:1.6036, Validation Accuracy:0.2545\n",
    "Epoch #266: Loss:1.5821, Accuracy:0.2587, Validation Loss:1.6025, Validation Accuracy:0.2529\n",
    "Epoch #267: Loss:1.5811, Accuracy:0.2554, Validation Loss:1.6043, Validation Accuracy:0.2545\n",
    "Epoch #268: Loss:1.5799, Accuracy:0.2665, Validation Loss:1.6090, Validation Accuracy:0.2627\n",
    "Epoch #269: Loss:1.5860, Accuracy:0.2509, Validation Loss:1.6090, Validation Accuracy:0.2578\n",
    "Epoch #270: Loss:1.5842, Accuracy:0.2538, Validation Loss:1.6115, Validation Accuracy:0.2332\n",
    "Epoch #271: Loss:1.5895, Accuracy:0.2509, Validation Loss:1.6047, Validation Accuracy:0.2594\n",
    "Epoch #272: Loss:1.5851, Accuracy:0.2517, Validation Loss:1.6089, Validation Accuracy:0.2250\n",
    "Epoch #273: Loss:1.5869, Accuracy:0.2476, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #274: Loss:1.5849, Accuracy:0.2583, Validation Loss:1.6067, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5880, Accuracy:0.2542, Validation Loss:1.6057, Validation Accuracy:0.2266\n",
    "Epoch #276: Loss:1.5873, Accuracy:0.2485, Validation Loss:1.6066, Validation Accuracy:0.2250\n",
    "Epoch #277: Loss:1.5860, Accuracy:0.2587, Validation Loss:1.6054, Validation Accuracy:0.2463\n",
    "Epoch #278: Loss:1.5874, Accuracy:0.2468, Validation Loss:1.6087, Validation Accuracy:0.2151\n",
    "Epoch #279: Loss:1.5851, Accuracy:0.2526, Validation Loss:1.6062, Validation Accuracy:0.2151\n",
    "Epoch #280: Loss:1.5839, Accuracy:0.2538, Validation Loss:1.6053, Validation Accuracy:0.2200\n",
    "Epoch #281: Loss:1.5852, Accuracy:0.2489, Validation Loss:1.6043, Validation Accuracy:0.2447\n",
    "Epoch #282: Loss:1.5853, Accuracy:0.2579, Validation Loss:1.6082, Validation Accuracy:0.2250\n",
    "Epoch #283: Loss:1.5850, Accuracy:0.2583, Validation Loss:1.6057, Validation Accuracy:0.2282\n",
    "Epoch #284: Loss:1.5869, Accuracy:0.2509, Validation Loss:1.6115, Validation Accuracy:0.2233\n",
    "Epoch #285: Loss:1.5909, Accuracy:0.2509, Validation Loss:1.6104, Validation Accuracy:0.2479\n",
    "Epoch #286: Loss:1.5935, Accuracy:0.2579, Validation Loss:1.6327, Validation Accuracy:0.2266\n",
    "Epoch #287: Loss:1.6041, Accuracy:0.2407, Validation Loss:1.6197, Validation Accuracy:0.2036\n",
    "Epoch #288: Loss:1.5960, Accuracy:0.2308, Validation Loss:1.6125, Validation Accuracy:0.2085\n",
    "Epoch #289: Loss:1.5921, Accuracy:0.2419, Validation Loss:1.6060, Validation Accuracy:0.2233\n",
    "Epoch #290: Loss:1.5898, Accuracy:0.2468, Validation Loss:1.6042, Validation Accuracy:0.2200\n",
    "Epoch #291: Loss:1.5881, Accuracy:0.2546, Validation Loss:1.6033, Validation Accuracy:0.2315\n",
    "Epoch #292: Loss:1.5866, Accuracy:0.2641, Validation Loss:1.6067, Validation Accuracy:0.2266\n",
    "Epoch #293: Loss:1.5867, Accuracy:0.2583, Validation Loss:1.6065, Validation Accuracy:0.2282\n",
    "Epoch #294: Loss:1.5887, Accuracy:0.2559, Validation Loss:1.6089, Validation Accuracy:0.2496\n",
    "Epoch #295: Loss:1.5899, Accuracy:0.2563, Validation Loss:1.6097, Validation Accuracy:0.2479\n",
    "Epoch #296: Loss:1.5909, Accuracy:0.2505, Validation Loss:1.6113, Validation Accuracy:0.2250\n",
    "Epoch #297: Loss:1.5905, Accuracy:0.2431, Validation Loss:1.6084, Validation Accuracy:0.2496\n",
    "Epoch #298: Loss:1.5890, Accuracy:0.2559, Validation Loss:1.6088, Validation Accuracy:0.2397\n",
    "Epoch #299: Loss:1.5903, Accuracy:0.2513, Validation Loss:1.6082, Validation Accuracy:0.2463\n",
    "Epoch #300: Loss:1.5906, Accuracy:0.2571, Validation Loss:1.6081, Validation Accuracy:0.2365\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60814333, Accuracy:0.2365\n",
    "Labels: ['02', '05', '04', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02   05  04  03  01\n",
    "t:02   6   89  12   1   6\n",
    "t:05   7  113  13   4   5\n",
    "t:04  15   72  13   4   8\n",
    "t:03   5   83  18   2   7\n",
    "t:01   1  100  15   0  10\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.18      0.05      0.08       114\n",
    "          05       0.25      0.80      0.38       142\n",
    "          04       0.18      0.12      0.14       112\n",
    "          03       0.18      0.02      0.03       115\n",
    "          01       0.28      0.08      0.12       126\n",
    "\n",
    "    accuracy                           0.24       609\n",
    "   macro avg       0.21      0.21      0.15       609\n",
    "weighted avg       0.22      0.24      0.16       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 18:34:58 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 22 minutes, 24 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6069933671277927, 1.6057604547400388, 1.6044869328954536, 1.603505030249923, 1.6031263379627847, 1.6029019733563628, 1.6023200441072336, 1.6025692614037024, 1.601764406476702, 1.601381929832922, 1.6013126940954299, 1.6011877157809504, 1.6001159445992832, 1.6001144667172862, 1.5995152151251857, 1.5990112546238013, 1.5986746089603319, 1.5988725122364087, 1.6009784290943239, 1.6013824550193323, 1.600410375101813, 1.6008654887648834, 1.6013601562268236, 1.6014880840414263, 1.5993952216773197, 1.5995398986907232, 1.6010867488403822, 1.5987684299988896, 1.6026369848079087, 1.6012101629488966, 1.5973998343416036, 1.5970011008001117, 1.5963442881510568, 1.598231076019738, 1.600756497414437, 1.5979803458027455, 1.598434367594852, 1.5965428350398498, 1.5949126949842731, 1.6012930286733191, 1.5999534063542808, 1.5985250457362785, 1.598041656373561, 1.5958932158590733, 1.5968345292096067, 1.5992447169151995, 1.5996176895053906, 1.598961884556537, 1.602963704780992, 1.6037658633073955, 1.6040405105487467, 1.6009535010420826, 1.601377243478897, 1.602523899626458, 1.6032789462109895, 1.6067920502379218, 1.6050595861350374, 1.6020367204262118, 1.5938158025491023, 1.5938977991614631, 1.597235742265172, 1.5925781460622652, 1.5928554370485504, 1.5936476640121886, 1.5921345459808074, 1.5912271851585025, 1.5916995575471073, 1.5932890810989981, 1.5966688200758008, 1.601691381293173, 1.5958398555104172, 1.6047515902417437, 1.5974148658696066, 1.60660859262219, 1.6046959134353989, 1.6039883536462518, 1.605658047696444, 1.6075917950208942, 1.6102353371618612, 1.605950122592093, 1.609945082703639, 1.6075423441105483, 1.6094317763114014, 1.6067852603978123, 1.6053127214826386, 1.604065122275517, 1.603890378486934, 1.6033352082977546, 1.6028497767174381, 1.6027974549968451, 1.6030670743074715, 1.602460396700892, 1.6010261646828237, 1.6017426919858835, 1.6025702843720886, 1.6024719367082092, 1.601440234724524, 1.6016458789703294, 1.6010433263183619, 1.6007709499258909, 1.6009503906387805, 1.6004347029969415, 1.6004414850072124, 1.5991265540835502, 1.5994977269853865, 1.5995746527986574, 1.5997011841615825, 1.5990789065807325, 1.5995901267125296, 1.5996313226242567, 1.599762606111849, 1.5990137562571685, 1.5981805813919343, 1.5983414700857328, 1.598805049957313, 1.5998527964543434, 1.5994154271625338, 1.5993825473221652, 1.5961435491032592, 1.5977236054018018, 1.60056122790025, 1.5978481687348465, 1.5954395661800367, 1.5962708362413354, 1.6010204498795257, 1.6032207720776888, 1.6028878416725372, 1.601766409740855, 1.6021667853952042, 1.6106286428636323, 1.6038283207537898, 1.60058068211247, 1.5999378241928928, 1.6011650029857367, 1.5992031344052018, 1.5980692679071662, 1.5973826184844344, 1.596512272244408, 1.596414620457416, 1.5984752156660083, 1.5982958146895485, 1.5985876317877683, 1.5991462498462845, 1.5988851963788613, 1.5961539193327203, 1.5977635138923507, 1.5961727645792594, 1.5964337442504557, 1.5961740717707793, 1.5975810825726864, 1.596797052471117, 1.596146007672515, 1.5938493422491014, 1.594794034370648, 1.59426326411111, 1.5965309595239574, 1.5966559775748668, 1.5956879358964993, 1.5948355360375641, 1.5973239649692779, 1.5971855806012458, 1.598685645900532, 1.6010854369509593, 1.5984195580427674, 1.6011523470307023, 1.6004512679987941, 1.6011303209123158, 1.5995513232079241, 1.6012198365184864, 1.6007619254499037, 1.601295433412436, 1.5995349927097315, 1.5978910901472094, 1.596195769818937, 1.5984576314149428, 1.5978358289095373, 1.5991869311418832, 1.6027566679984282, 1.599647235987809, 1.6015169763408468, 1.6035888390783801, 1.6038046928462137, 1.6006561711503955, 1.602945972154489, 1.6008676930601373, 1.5992183401471092, 1.5994236649355082, 1.6011862606054847, 1.6019759203608597, 1.6041195331927396, 1.6048256333042639, 1.6042831300318927, 1.603804145736256, 1.604324899675028, 1.6025613710798066, 1.601658045951956, 1.600733947675608, 1.596017867473546, 1.5973080300736702, 1.5981130312229026, 1.5963356980353545, 1.5944188970258866, 1.5930199100466198, 1.59496932761814, 1.5975632526604413, 1.5947291755128181, 1.5958150819017383, 1.5965391213474993, 1.597123739558879, 1.6039348267177838, 1.5992124875386555, 1.6006036209942671, 1.600243281652579, 1.5989221528245898, 1.6000500819561712, 1.5996079546673152, 1.597630720224678, 1.5964535660735883, 1.595780202124898, 1.5942592526891548, 1.5952337870652649, 1.5941881125392194, 1.5931773485221299, 1.5949119137621475, 1.599277410796906, 1.5982181233138286, 1.599302161699054, 1.599741118686344, 1.5949813021814883, 1.597279517325666, 1.5951438149795156, 1.5968482451290136, 1.596834396493846, 1.6006279099359497, 1.6001950447586761, 1.6003772754387315, 1.5997057076549686, 1.5995700932880146, 1.6005066934673267, 1.597830286949922, 1.5952114921876754, 1.5928454802345564, 1.5935492605607107, 1.5947729433307116, 1.5942075090063812, 1.5970392105810356, 1.599475734339559, 1.6028462405666732, 1.6064900933032358, 1.603973690316399, 1.6070054350619638, 1.607815029781636, 1.6082083832454213, 1.6037261116093602, 1.6049453742398416, 1.604021293972122, 1.602555356980936, 1.6012289300927975, 1.6035346910479817, 1.6032252397834765, 1.603282707469608, 1.6042797081967684, 1.60517101471843, 1.60245812448179, 1.6036485284811561, 1.6024542684821268, 1.6043425620287315, 1.6090095900549677, 1.6090343904808433, 1.6114999316204552, 1.604660328973103, 1.6088817581558854, 1.6066567980009934, 1.6067184918423982, 1.6057198249256277, 1.6066054610783242, 1.6053529897542618, 1.6086857307133415, 1.6062085920171, 1.6052800070476063, 1.604269661143887, 1.6081882981439726, 1.6056965822461007, 1.611496705922783, 1.6104137296551357, 1.6326801015434202, 1.6197196123830986, 1.612534191220852, 1.6060418693107141, 1.6042064633862725, 1.6033481014968922, 1.6067037813377694, 1.606535933875098, 1.6089096118272428, 1.609655290010136, 1.6113402318876169, 1.6083743638788734, 1.6088168906852334, 1.6082217010175457, 1.6081433957824958], 'val_acc': [0.22331691076994334, 0.2282430195358195, 0.2331691280080767, 0.23481116403499847, 0.23316912791020372, 0.23316912791020372, 0.23316912791020372, 0.23316912791020372, 0.23316912791020372, 0.23316912791020372, 0.23152709178540898, 0.23973727260512867, 0.24302134505046413, 0.2446633810773859, 0.2528735617013596, 0.26600985079759054, 0.24466338117525888, 0.2446633810773859, 0.2528735617013596, 0.2528735617013596, 0.25451559782615435, 0.2282430195358195, 0.23316912791020372, 0.2643678146727958, 0.2610837423253333, 0.24794745342484836, 0.2561576339509491, 0.2512315254786919, 0.22988505556274125, 0.24794745332697538, 0.2512315254786919, 0.24302134495259115, 0.25451559782615435, 0.23152709188328197, 0.2331691280080767, 0.25123152567443785, 0.24302134485471816, 0.2577996702714898, 0.26765188682451235, 0.24794745303335644, 0.23973727240938272, 0.25944170639628455, 0.26108374222746034, 0.2643678146727958, 0.2561576335594572, 0.2495894895496431, 0.23481116423074444, 0.2463054169085617, 0.23973727240938272, 0.25287356319392257, 0.2610837423253333, 0.24302134485471816, 0.2298850553669953, 0.22331691086781633, 0.22660098292165984, 0.23481116374137953, 0.2331691276165848, 0.238095236186715, 0.24794745332697538, 0.24794745303335644, 0.25451559792402734, 0.25779967007574384, 0.25451559782615435, 0.26765188682451235, 0.2561576335594572, 0.25779967007574384, 0.26600985050397163, 0.26929392304718003, 0.2610837420317144, 0.24466338068589397, 0.24630541661494276, 0.22660098509933366, 0.26272577815650916, 0.24302134436535328, 0.2282430213220014, 0.2413793082405585, 0.2183908023955591, 0.23973727240938272, 0.23481116582118036, 0.23152709168753602, 0.23809523638246094, 0.2101806216737124, 0.23316912781233076, 0.2249589467968651, 0.2282430195358195, 0.24302134475684517, 0.23973727270300166, 0.24302134495259115, 0.23973727260512867, 0.24302134495259115, 0.23973727250725568, 0.23809523638246094, 0.23809523638246094, 0.23809523638246094, 0.23481116403499847, 0.22988505575848722, 0.2430213451483371, 0.23152709188328197, 0.23316912791020372, 0.2463054173979266, 0.25123152577231084, 0.2528735617992326, 0.24794745342484836, 0.24794745342484836, 0.2594417062005386, 0.24794745352272135, 0.25944170629841157, 0.26272577854800105, 0.2528735617992326, 0.2528735617992326, 0.25615763414669507, 0.25123152567443785, 0.25123152567443785, 0.2610837424232063, 0.2528735617013596, 0.24630541710430764, 0.24302134495259115, 0.2413793088277964, 0.24794745342484836, 0.2561576339509491, 0.24794745313122943, 0.24794745342484836, 0.25451559772828136, 0.2561576339509491, 0.23645319976830131, 0.2282430195358195, 0.24302134446322624, 0.25123152557656486, 0.23316912771445777, 0.2282430213220014, 0.25287356150561363, 0.23481116383925252, 0.2413793089256694, 0.224958947384103, 0.25944170541755474, 0.2479474532291024, 0.25615763414669507, 0.26272577864587404, 0.2725779952967695, 0.2512315253808189, 0.2643678146727958, 0.26929392304718003, 0.24466338088163994, 0.24137930863205043, 0.27093595936772075, 0.2512315253808189, 0.2709359586826099, 0.25779966968425194, 0.24958948925602417, 0.27422003122581834, 0.2561576335594572, 0.25779966968425194, 0.26600985030822566, 0.27750410279029697, 0.2561576334615842, 0.2627257779607632, 0.2315270934737179, 0.25287356091837576, 0.2528735614077407, 0.25287356111412174, 0.2561576335594572, 0.25779966958637895, 0.24958948896240524, 0.2561576335594572, 0.24466338058802098, 0.2430213442674803, 0.24794745254399153, 0.23809523589309606, 0.24466338049014802, 0.25451559743466246, 0.24630541671281572, 0.24466338049014802, 0.23645319996404726, 0.23481116364350654, 0.2610837423253333, 0.26108374222746034, 0.24958948925602417, 0.23152709139391706, 0.25451559733678947, 0.24794745293548345, 0.2561576338530761, 0.2545155976304084, 0.2528735613098677, 0.23645319967042833, 0.25451559733678947, 0.24466338058802098, 0.2512315250872, 0.2676518870202583, 0.2610837421295874, 0.24794745313122943, 0.22331691294761713, 0.22495894887666593, 0.23645319986617427, 0.24794745293548345, 0.24466338078376695, 0.23481116364350654, 0.2331691275187118, 0.2430213442674803, 0.2298850552691223, 0.2545155976304084, 0.25779966997787085, 0.24302134475684517, 0.2183908044753599, 0.2561576336573302, 0.22167487672494943, 0.22660098509933366, 0.24466338256994882, 0.23809523579522307, 0.23809523589309606, 0.23809523807076985, 0.23316912742083884, 0.2282430211262554, 0.22495894887666593, 0.22495894887666593, 0.2216748746451486, 0.23645319976830131, 0.24630541651706978, 0.24630541671281572, 0.24794745293548345, 0.2495894890602782, 0.22003284060015468, 0.2463054168106887, 0.25287356111412174, 0.24794745283761047, 0.24794745303335644, 0.2610837420317144, 0.24466338078376695, 0.24466338029440204, 0.25123152528294596, 0.24794745264186452, 0.24630541671281572, 0.2495894890602782, 0.2463054168106887, 0.2463054168106887, 0.25451559753253544, 0.2463054168106887, 0.24794745293548345, 0.25287356150561363, 0.2528735614077407, 0.24958948915815118, 0.25451559772828136, 0.25944170600479266, 0.2561576337552032, 0.26272577815650916, 0.2577996698799979, 0.2495894890602782, 0.2594417059069197, 0.2495894890602782, 0.24466338068589397, 0.24466338078376695, 0.22988505517124935, 0.24794745303335644, 0.22988505734892314, 0.24958948886453225, 0.25451559733678947, 0.2528735612119947, 0.2430213441696073, 0.25779966968425194, 0.2413793101246134, 0.251231524891454, 0.23809523797289686, 0.2610837420317144, 0.23809523579522307, 0.2709359585847369, 0.2545155970431705, 0.25287356091837576, 0.25451559743466246, 0.26272577984481804, 0.25779966948850597, 0.23316912969638562, 0.25944170600479266, 0.22495894887666593, 0.22824301924220056, 0.23481116383925252, 0.2266009832152788, 0.22495894887666593, 0.24630541700643468, 0.21510673014596962, 0.2151067323236434, 0.22003284069802764, 0.24466338068589397, 0.22495894907241187, 0.2282430195358195, 0.22331691067207035, 0.24794745332697538, 0.22660098341102475, 0.2036124788628423, 0.20853858743297252, 0.22331691284974417, 0.22003283852035385, 0.23152709178540898, 0.22660098331315176, 0.22824301963369248, 0.24958948915815118, 0.24794745313122943, 0.22495894699261107, 0.24958948925602417, 0.23973727250725568, 0.2463054169085617, 0.23645320006192025], 'loss': [1.6085873281196892, 1.6065582072220788, 1.6053976615596357, 1.6047564590246526, 1.6044900979839067, 1.6042321315291481, 1.6039159340535345, 1.6036477454633928, 1.6032700864686125, 1.6027766180234277, 1.6024808091549414, 1.6023201471971045, 1.6017107678634674, 1.6014956262322178, 1.6014114092997211, 1.6008255186022184, 1.6004856500782272, 1.6003033661989217, 1.5995480224092393, 1.5996217008488869, 1.5987862126783179, 1.5999163590417504, 1.6006034111829754, 1.6002394018231965, 1.6000140653253825, 1.6005630770747912, 1.601802199724029, 1.6002906353321897, 1.6010390478972292, 1.6026246046383523, 1.60262019516751, 1.602528086531089, 1.600319908923437, 1.5992130104276434, 1.600195830509648, 1.6005564890358237, 1.5995958580373493, 1.598502903258776, 1.5972205841565768, 1.5988298228633966, 1.5988784659324975, 1.5986061708148744, 1.5963326001803733, 1.5962822633602292, 1.5974613194592924, 1.5971862660296399, 1.5976065939211992, 1.5964963851768132, 1.6004919326770477, 1.6031796917043917, 1.6008245649768587, 1.5999881685147295, 1.598116505415288, 1.5986612685651995, 1.5992397874287756, 1.599154935969954, 1.5993754848072905, 1.5988876800517526, 1.5979111425685686, 1.5971936150987536, 1.5968005248408543, 1.5948656458629475, 1.5926951088699717, 1.5934426700310051, 1.5920352816336942, 1.58999542837515, 1.588993093218402, 1.5880605465088047, 1.5899072503162361, 1.590158599700771, 1.5964305641714798, 1.590069890120191, 1.5931249424172622, 1.593509227637148, 1.5941940605762803, 1.594290564437177, 1.5930065414009642, 1.5969006485028434, 1.5979226033545617, 1.5936389139300744, 1.6017929833527709, 1.6104699786438834, 1.6058304681915032, 1.6077053644329127, 1.6054086752251189, 1.604899008709792, 1.6047543432678286, 1.6037600336623143, 1.6028395215099107, 1.6021552626846751, 1.6019663157159543, 1.6012018746663903, 1.6010903403009966, 1.6010094562839923, 1.6006723148866846, 1.600806663658095, 1.6013958562326138, 1.600836488843209, 1.6003416469699303, 1.5997231669494503, 1.5996396376122195, 1.5996514132380242, 1.5992619359762517, 1.5993522572076786, 1.599122773303633, 1.59963244182618, 1.598265701201907, 1.5984617173059765, 1.5976236808471367, 1.5976798599016004, 1.5972380534334594, 1.5971848083962161, 1.597498330198519, 1.5970275836803585, 1.598112440696732, 1.5975530216581277, 1.5979964593597507, 1.5970546133953933, 1.597660703727597, 1.5965913808810883, 1.598958840213517, 1.5974927264806915, 1.5987751365442295, 1.5966953210517367, 1.597501689452655, 1.5960035351022803, 1.5969254607782226, 1.5960877874793458, 1.5953660880270926, 1.5984993792167679, 1.5999130214753827, 1.5973698064287096, 1.5973221437152652, 1.5986460695031732, 1.5990274807755707, 1.5978400813480667, 1.598834442651737, 1.5970824941717379, 1.5951517054677253, 1.5960180514646995, 1.5944580914793074, 1.5954633511556982, 1.5941791197602508, 1.5945500934393255, 1.5939234488309042, 1.5940708748369001, 1.5929486774076427, 1.594061121362925, 1.5937555139804034, 1.5939522068113763, 1.5914369024045658, 1.5914293905058436, 1.5913504022347609, 1.5912250794424414, 1.5910412762444122, 1.5917434557262633, 1.5924940723168532, 1.5930114173791248, 1.5944616514554504, 1.593537138130141, 1.5937238118486972, 1.5930457602291381, 1.5927892485683215, 1.5924744459148306, 1.5923645737724383, 1.5934224364204328, 1.5945481681725817, 1.5939927041897783, 1.5941374483049773, 1.5933617299587086, 1.5920963305467453, 1.592947501225638, 1.5933722651225095, 1.5958197602990716, 1.5993641657995736, 1.594733663604, 1.5963562799919802, 1.5957426274336828, 1.5957508277109762, 1.593959016868466, 1.5930126332649215, 1.5923064853621214, 1.592958922063056, 1.5917579564715314, 1.5909904005101574, 1.589623563637234, 1.592178851035586, 1.5924065408275847, 1.5903700553905793, 1.5906980461653253, 1.5903554611382298, 1.5901665760016783, 1.5894635270018842, 1.5897374244686024, 1.5899143694852167, 1.58860507980754, 1.5894728355094392, 1.5863962822136692, 1.5893810885153268, 1.5911160068590293, 1.5907891109983534, 1.5888264893995907, 1.5893706539083556, 1.5893301817915524, 1.5887414761392487, 1.5902760129200115, 1.5902055254217535, 1.5874332534703874, 1.5876958877398983, 1.5878204866111645, 1.5878398145738324, 1.5862250099436703, 1.5881742689399014, 1.5857355876624952, 1.5880727436018676, 1.5885664292184725, 1.5856993059358069, 1.5855698648174685, 1.5857981248557935, 1.5852504978434505, 1.5851796600608121, 1.5860607509005975, 1.587748151736093, 1.5863633768759224, 1.588026259665127, 1.5879463237903446, 1.587547643914115, 1.5885675432256114, 1.5904235625413898, 1.59064675444695, 1.5881548646049577, 1.586603797632566, 1.5861088821775369, 1.5857625880035777, 1.5834341396786105, 1.5838176186814201, 1.5825055434228947, 1.5838275679817435, 1.5845095798465016, 1.584484830186597, 1.5873891926399246, 1.5894451457127408, 1.5887724612038239, 1.5893716042046675, 1.5874480692512936, 1.5875603500577704, 1.5842969130196856, 1.582262927844539, 1.5823533044948226, 1.5830392865919234, 1.5828481916529442, 1.5842451376591864, 1.5882344705612996, 1.5827868248647734, 1.58238057042539, 1.5824665446056232, 1.5836385739167858, 1.5806463440830458, 1.5800450605533451, 1.5799353214749565, 1.5795251897717892, 1.5789022187677497, 1.5790572807284595, 1.5779333808828429, 1.582322169035612, 1.5821468483985572, 1.581118322251024, 1.5798718427485754, 1.585971543431527, 1.584165091191474, 1.5894935097782519, 1.585123821303585, 1.586895794643269, 1.5849088667844111, 1.5879666987385839, 1.5873465798473945, 1.5860301713923899, 1.587412587964804, 1.5850582638805162, 1.5839077018859204, 1.5852272131115015, 1.5852776218978286, 1.5849617058736343, 1.5869385482839, 1.5908602125590832, 1.5934676324562371, 1.6040998659094745, 1.5960170803618383, 1.5921455680467265, 1.58984368229304, 1.5880841779513035, 1.5866358340888054, 1.5867203308571536, 1.5886685142282098, 1.5898748552529964, 1.590932882933646, 1.5904875896794595, 1.589033675046917, 1.590272852086923, 1.5906135220302449], 'acc': [0.2119096511871663, 0.21848049288045698, 0.2246406571331455, 0.23408624391536204, 0.233264886935144, 0.23285420912124782, 0.23285421051039099, 0.2328542102778472, 0.2324435322864834, 0.23203285327927042, 0.23326488691678526, 0.23408624177963092, 0.23572895244773653, 0.2386036951867462, 0.2353182752029607, 0.24147844047149838, 0.24106776244341716, 0.24065708484370604, 0.2394250515978439, 0.24065708325873655, 0.24271047271987006, 0.2377823401280742, 0.2361396306532854, 0.2361396302616327, 0.2439425059657322, 0.23080082024759335, 0.23285421029620593, 0.2390143747630795, 0.23737166350749483, 0.23737166352585357, 0.2287474327263646, 0.23778234189051134, 0.24065708365038924, 0.241478439688193, 0.23408624295458902, 0.2398357298217515, 0.24106776068098001, 0.24271047254240244, 0.24845996093945827, 0.2377823405380856, 0.24106776109099143, 0.24558521467557434, 0.24640657069501937, 0.2435318271727043, 0.24147843927818158, 0.24845995958703254, 0.242710471544912, 0.24599589268529684, 0.23367556474904017, 0.23039014302117625, 0.2377823409297383, 0.23449692057265883, 0.2369609866543717, 0.2390143747630795, 0.24394250733651665, 0.24271047389482817, 0.24065708560865273, 0.25872690139120363, 0.2427104744823072, 0.24188911846286218, 0.2521560579354758, 0.24804928214643035, 0.25585215728140953, 0.2517453803357647, 0.2550308024369226, 0.2550307998911801, 0.256673510167633, 0.25215605656469137, 0.25420944622165126, 0.2501026696676591, 0.24147843888652887, 0.2566735133008546, 0.24640657108667205, 0.25297741356326814, 0.24147844104061872, 0.24928131382568172, 0.24804928273390942, 0.2550308014761496, 0.23696098606689264, 0.24271047193656467, 0.23408624313205664, 0.22710472344740215, 0.21724845963459485, 0.22340862329980432, 0.22464065617237247, 0.22915810975695539, 0.23039014302117625, 0.2328542098861945, 0.23367556514069285, 0.23490759836819627, 0.23162217605285332, 0.23449691879186296, 0.235318274811308, 0.23655030807552885, 0.23531827600462482, 0.23285421049203225, 0.23121149823895715, 0.2369609864952628, 0.23613962926414223, 0.23860369556004016, 0.23983572821842328, 0.23819301833362305, 0.24106776246177586, 0.2406570830629102, 0.2443531815888211, 0.24065708404204195, 0.24517453960324703, 0.24722792573533264, 0.24722792534367993, 0.24476386043692516, 0.24969199105209883, 0.24928131519646615, 0.24517453864247402, 0.247638605899145, 0.2439425055740795, 0.2451745384466477, 0.24188911885451486, 0.2398357311558185, 0.24106776285342857, 0.24147843831740856, 0.2369609846961082, 0.23819301755031766, 0.23737166231417803, 0.2369609861036101, 0.2431211499646459, 0.24229979353518946, 0.2357289536226946, 0.23983572821842328, 0.24271047134908563, 0.23860369655753064, 0.23244353070151388, 0.24065708445205336, 0.23696098628107773, 0.23326488713097035, 0.23326488595601225, 0.23696098647690406, 0.23819301952693986, 0.2406570850395324, 0.24312114878968782, 0.24188911789374185, 0.24065708462952098, 0.2472279271244758, 0.2439425058066233, 0.24229979570763802, 0.23983572900172867, 0.24024640605067815, 0.23490759858238133, 0.24353182893514144, 0.2406570830629102, 0.25133470211185716, 0.2492813136482141, 0.24599589387861365, 0.24435318340633438, 0.2505133460924121, 0.24312114955463449, 0.24435318360216074, 0.2447638610244042, 0.2492813144315195, 0.23696098530194598, 0.2492813157839452, 0.24476386139769818, 0.23942505157948518, 0.24928131304237633, 0.25010266827851596, 0.24763860413670785, 0.24435318319214933, 0.25462012423137376, 0.2529774133858005, 0.23983572902008737, 0.2505133472673702, 0.25297741317161543, 0.24804928196896273, 0.2435318283660211, 0.23613963143659078, 0.2349075968015855, 0.24640657110503078, 0.2365503086630079, 0.23326488811010207, 0.23613963004744762, 0.2410677626392435, 0.2537987689952341, 0.2521560569563441, 0.2574948677536888, 0.2583162235773075, 0.25092402625622445, 0.2496919928328947, 0.2533880882072253, 0.25831622083573863, 0.25051334589658575, 0.24928131460898711, 0.25174537974828565, 0.24887063718674365, 0.24887063642179696, 0.2554414798591661, 0.25338809139552304, 0.25338809196464335, 0.2464065708908457, 0.25092402529545144, 0.24763860296174975, 0.25133470269933617, 0.2550308008703118, 0.25133470213021586, 0.24476386139769818, 0.24229979570763802, 0.2501026700409531, 0.2394250504045271, 0.2377823401280742, 0.24969199203123058, 0.25379876938688684, 0.24188911591711965, 0.252156056368865, 0.25708418915648723, 0.24763860374505514, 0.2587268996287665, 0.25708418993979265, 0.2513347025035098, 0.25133470367846794, 0.25051334746319653, 0.24887063562013284, 0.2513347031093476, 0.25133470211185716, 0.24394250617991728, 0.2546201224689366, 0.24640657090920443, 0.2509240254912778, 0.24640656991171397, 0.25051334628823846, 0.25010267023677946, 0.24435318319214933, 0.25297741238831006, 0.24763860296174975, 0.25256673614102465, 0.2591375764635309, 0.2513347015243781, 0.2550308018678023, 0.2587268990412875, 0.25749486638290436, 0.25133470031270255, 0.25379876840775506, 0.25585215728140953, 0.24722792495202725, 0.24845995996032652, 0.251745380904885, 0.23367556376990842, 0.2525667341827612, 0.25174537974828565, 0.25503080128032324, 0.25954825409995946, 0.24640657188833617, 0.2603696107068835, 0.25503079969535375, 0.2570841881773555, 0.2583162235773075, 0.2546201230564157, 0.26160164354273424, 0.262833676005291, 0.25872689825798206, 0.2595482534941217, 0.2554414798591661, 0.2644763858900912, 0.25503079867950457, 0.26694045296929453, 0.2657084201150851, 0.256673510755112, 0.26365503125978934, 0.2587269004304306, 0.25544147731342354, 0.26652977178963305, 0.25092402412049336, 0.2537987701701922, 0.2509240239063083, 0.25174537914244793, 0.24763860454671927, 0.2583162224023494, 0.2542094470049566, 0.24845995819788938, 0.25872690023460426, 0.24681724987969997, 0.2525667353577193, 0.25379876997436585, 0.24887063718674365, 0.25790554439262686, 0.2583162225981757, 0.2509240244937873, 0.250924023924667, 0.257905545567585, 0.24065708406040065, 0.23080082161837778, 0.2418891168962514, 0.2468172475297838, 0.25462012462302647, 0.2640657088595005, 0.2583162220106967, 0.2558521555189724, 0.2562628347220117, 0.25051334746319653, 0.24312114955463449, 0.25585215767306224, 0.25133470330517393, 0.2570841879448117]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
