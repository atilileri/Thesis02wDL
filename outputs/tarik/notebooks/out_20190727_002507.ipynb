{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf21.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 00:25:07 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '1Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001838121A550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000183FCAC6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1139, Accuracy:0.2468, Validation Loss:1.0936, Validation Accuracy:0.4023\n",
    "Epoch #2: Loss:1.0866, Accuracy:0.3955, Validation Loss:1.0788, Validation Accuracy:0.3924\n",
    "Epoch #3: Loss:1.0760, Accuracy:0.3943, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0752, Accuracy:0.3943, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0753, Accuracy:0.3943, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0742, Accuracy:0.3963, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0737, Accuracy:0.3951, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0737, Accuracy:0.3955, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0736, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #26: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #27: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #28: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #29: Loss:1.0734, Accuracy:0.3971, Validation Loss:1.0745, Validation Accuracy:0.3859\n",
    "Epoch #30: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0745, Validation Accuracy:0.3875\n",
    "Epoch #31: Loss:1.0734, Accuracy:0.3951, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #32: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #33: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #34: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0746, Validation Accuracy:0.3793\n",
    "Epoch #35: Loss:1.0735, Accuracy:0.3955, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #36: Loss:1.0733, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #37: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #38: Loss:1.0733, Accuracy:0.3992, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #39: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #40: Loss:1.0734, Accuracy:0.3996, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #41: Loss:1.0733, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #42: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #43: Loss:1.0733, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #44: Loss:1.0732, Accuracy:0.3996, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #45: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #46: Loss:1.0742, Accuracy:0.3741, Validation Loss:1.0759, Validation Accuracy:0.3842\n",
    "Epoch #47: Loss:1.0743, Accuracy:0.3721, Validation Loss:1.0753, Validation Accuracy:0.4007\n",
    "Epoch #48: Loss:1.0740, Accuracy:0.3926, Validation Loss:1.0751, Validation Accuracy:0.3859\n",
    "Epoch #49: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #50: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #51: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0733, Accuracy:0.3951, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #53: Loss:1.0735, Accuracy:0.3979, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #54: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #55: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3842\n",
    "Epoch #56: Loss:1.0736, Accuracy:0.3947, Validation Loss:1.0746, Validation Accuracy:0.3826\n",
    "Epoch #57: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #58: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #59: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #60: Loss:1.0733, Accuracy:0.4016, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #61: Loss:1.0733, Accuracy:0.3979, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #62: Loss:1.0731, Accuracy:0.3979, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #63: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #64: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #65: Loss:1.0737, Accuracy:0.3906, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #66: Loss:1.0729, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.3957\n",
    "Epoch #67: Loss:1.0730, Accuracy:0.3984, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #68: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #69: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #70: Loss:1.0734, Accuracy:0.3951, Validation Loss:1.0751, Validation Accuracy:0.3810\n",
    "Epoch #71: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #72: Loss:1.0729, Accuracy:0.3951, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #73: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #74: Loss:1.0724, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #75: Loss:1.0727, Accuracy:0.3967, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #76: Loss:1.0725, Accuracy:0.3992, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #77: Loss:1.0724, Accuracy:0.4016, Validation Loss:1.0751, Validation Accuracy:0.3924\n",
    "Epoch #78: Loss:1.0724, Accuracy:0.4033, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #80: Loss:1.0724, Accuracy:0.4045, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #81: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #82: Loss:1.0724, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #83: Loss:1.0721, Accuracy:0.4033, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #84: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #85: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #86: Loss:1.0713, Accuracy:0.3988, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #87: Loss:1.0710, Accuracy:0.4041, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #88: Loss:1.0709, Accuracy:0.4025, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #89: Loss:1.0712, Accuracy:0.4103, Validation Loss:1.0750, Validation Accuracy:0.3810\n",
    "Epoch #90: Loss:1.0707, Accuracy:0.4090, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #91: Loss:1.0709, Accuracy:0.4057, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #92: Loss:1.0707, Accuracy:0.4021, Validation Loss:1.0764, Validation Accuracy:0.3793\n",
    "Epoch #93: Loss:1.0700, Accuracy:0.4070, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #94: Loss:1.0707, Accuracy:0.4029, Validation Loss:1.0754, Validation Accuracy:0.3908\n",
    "Epoch #95: Loss:1.0703, Accuracy:0.4082, Validation Loss:1.0757, Validation Accuracy:0.3957\n",
    "Epoch #96: Loss:1.0705, Accuracy:0.4070, Validation Loss:1.0750, Validation Accuracy:0.3760\n",
    "Epoch #97: Loss:1.0704, Accuracy:0.4062, Validation Loss:1.0745, Validation Accuracy:0.4056\n",
    "Epoch #98: Loss:1.0689, Accuracy:0.4123, Validation Loss:1.0766, Validation Accuracy:0.3760\n",
    "Epoch #99: Loss:1.0704, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #100: Loss:1.0694, Accuracy:0.4127, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #101: Loss:1.0701, Accuracy:0.4094, Validation Loss:1.0762, Validation Accuracy:0.3908\n",
    "Epoch #102: Loss:1.0697, Accuracy:0.4090, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #103: Loss:1.0694, Accuracy:0.4066, Validation Loss:1.0767, Validation Accuracy:0.3760\n",
    "Epoch #104: Loss:1.0685, Accuracy:0.4066, Validation Loss:1.0766, Validation Accuracy:0.3793\n",
    "Epoch #105: Loss:1.0686, Accuracy:0.4033, Validation Loss:1.0769, Validation Accuracy:0.3678\n",
    "Epoch #106: Loss:1.0687, Accuracy:0.4037, Validation Loss:1.0762, Validation Accuracy:0.3645\n",
    "Epoch #107: Loss:1.0679, Accuracy:0.4090, Validation Loss:1.0762, Validation Accuracy:0.3612\n",
    "Epoch #108: Loss:1.0682, Accuracy:0.4033, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #109: Loss:1.0691, Accuracy:0.3996, Validation Loss:1.0765, Validation Accuracy:0.3711\n",
    "Epoch #110: Loss:1.0707, Accuracy:0.4037, Validation Loss:1.0764, Validation Accuracy:0.3678\n",
    "Epoch #111: Loss:1.0692, Accuracy:0.4021, Validation Loss:1.0775, Validation Accuracy:0.3678\n",
    "Epoch #112: Loss:1.0679, Accuracy:0.4029, Validation Loss:1.0781, Validation Accuracy:0.3826\n",
    "Epoch #113: Loss:1.0682, Accuracy:0.4041, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #114: Loss:1.0681, Accuracy:0.4123, Validation Loss:1.0781, Validation Accuracy:0.3662\n",
    "Epoch #115: Loss:1.0680, Accuracy:0.4062, Validation Loss:1.0782, Validation Accuracy:0.3826\n",
    "Epoch #116: Loss:1.0689, Accuracy:0.4119, Validation Loss:1.0853, Validation Accuracy:0.3662\n",
    "Epoch #117: Loss:1.0702, Accuracy:0.4016, Validation Loss:1.0808, Validation Accuracy:0.3629\n",
    "Epoch #118: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0796, Validation Accuracy:0.3711\n",
    "Epoch #119: Loss:1.0679, Accuracy:0.4004, Validation Loss:1.0809, Validation Accuracy:0.3645\n",
    "Epoch #120: Loss:1.0682, Accuracy:0.4016, Validation Loss:1.0783, Validation Accuracy:0.3596\n",
    "Epoch #121: Loss:1.0677, Accuracy:0.4136, Validation Loss:1.0775, Validation Accuracy:0.3727\n",
    "Epoch #122: Loss:1.0666, Accuracy:0.4111, Validation Loss:1.0767, Validation Accuracy:0.3645\n",
    "Epoch #123: Loss:1.0659, Accuracy:0.4094, Validation Loss:1.0767, Validation Accuracy:0.3859\n",
    "Epoch #124: Loss:1.0663, Accuracy:0.4127, Validation Loss:1.0774, Validation Accuracy:0.3859\n",
    "Epoch #125: Loss:1.0665, Accuracy:0.4078, Validation Loss:1.0796, Validation Accuracy:0.3662\n",
    "Epoch #126: Loss:1.0667, Accuracy:0.4086, Validation Loss:1.0803, Validation Accuracy:0.3612\n",
    "Epoch #127: Loss:1.0691, Accuracy:0.4041, Validation Loss:1.0781, Validation Accuracy:0.3662\n",
    "Epoch #128: Loss:1.0676, Accuracy:0.4070, Validation Loss:1.0786, Validation Accuracy:0.3645\n",
    "Epoch #129: Loss:1.0665, Accuracy:0.4070, Validation Loss:1.0811, Validation Accuracy:0.3612\n",
    "Epoch #130: Loss:1.0671, Accuracy:0.4123, Validation Loss:1.0818, Validation Accuracy:0.3826\n",
    "Epoch #131: Loss:1.0649, Accuracy:0.4107, Validation Loss:1.0795, Validation Accuracy:0.3645\n",
    "Epoch #132: Loss:1.0704, Accuracy:0.4070, Validation Loss:1.0785, Validation Accuracy:0.3842\n",
    "Epoch #133: Loss:1.0688, Accuracy:0.4094, Validation Loss:1.0805, Validation Accuracy:0.3777\n",
    "Epoch #134: Loss:1.0789, Accuracy:0.3930, Validation Loss:1.0979, Validation Accuracy:0.3941\n",
    "Epoch #135: Loss:1.0994, Accuracy:0.3955, Validation Loss:1.0939, Validation Accuracy:0.3990\n",
    "Epoch #136: Loss:1.0817, Accuracy:0.3910, Validation Loss:1.0798, Validation Accuracy:0.3612\n",
    "Epoch #137: Loss:1.0799, Accuracy:0.3754, Validation Loss:1.0798, Validation Accuracy:0.3711\n",
    "Epoch #138: Loss:1.0797, Accuracy:0.3745, Validation Loss:1.0767, Validation Accuracy:0.3793\n",
    "Epoch #139: Loss:1.0752, Accuracy:0.3782, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #140: Loss:1.0747, Accuracy:0.3910, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #141: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #142: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #143: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #144: Loss:1.0740, Accuracy:0.3897, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #145: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3760\n",
    "Epoch #146: Loss:1.0744, Accuracy:0.3955, Validation Loss:1.0743, Validation Accuracy:0.3777\n",
    "Epoch #147: Loss:1.0741, Accuracy:0.3959, Validation Loss:1.0738, Validation Accuracy:0.3777\n",
    "Epoch #148: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #149: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #150: Loss:1.0734, Accuracy:0.3971, Validation Loss:1.0740, Validation Accuracy:0.3760\n",
    "Epoch #151: Loss:1.0733, Accuracy:0.3963, Validation Loss:1.0738, Validation Accuracy:0.3744\n",
    "Epoch #152: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #153: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #154: Loss:1.0733, Accuracy:0.3971, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #155: Loss:1.0731, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #156: Loss:1.0732, Accuracy:0.3975, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #157: Loss:1.0736, Accuracy:0.3975, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #158: Loss:1.0730, Accuracy:0.3979, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #159: Loss:1.0731, Accuracy:0.3975, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #160: Loss:1.0728, Accuracy:0.3975, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #161: Loss:1.0729, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #162: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #163: Loss:1.0725, Accuracy:0.3988, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #164: Loss:1.0731, Accuracy:0.3959, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #165: Loss:1.0728, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #166: Loss:1.0730, Accuracy:0.3988, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #167: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #168: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #169: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #170: Loss:1.0725, Accuracy:0.3996, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #171: Loss:1.0725, Accuracy:0.3971, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #172: Loss:1.0724, Accuracy:0.3975, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #173: Loss:1.0722, Accuracy:0.3971, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #174: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #175: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0733, Validation Accuracy:0.3875\n",
    "Epoch #176: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #177: Loss:1.0722, Accuracy:0.3984, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #178: Loss:1.0723, Accuracy:0.3996, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #179: Loss:1.0723, Accuracy:0.3959, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #180: Loss:1.0747, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #181: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #182: Loss:1.0724, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #183: Loss:1.0725, Accuracy:0.3975, Validation Loss:1.0740, Validation Accuracy:0.3810\n",
    "Epoch #184: Loss:1.0725, Accuracy:0.4033, Validation Loss:1.0743, Validation Accuracy:0.3810\n",
    "Epoch #185: Loss:1.0729, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #186: Loss:1.0725, Accuracy:0.4016, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #187: Loss:1.0730, Accuracy:0.4012, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #188: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #189: Loss:1.0720, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #190: Loss:1.0718, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #191: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #192: Loss:1.0714, Accuracy:0.4012, Validation Loss:1.0748, Validation Accuracy:0.3810\n",
    "Epoch #193: Loss:1.0716, Accuracy:0.4008, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #194: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #195: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.3826\n",
    "Epoch #196: Loss:1.0715, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #197: Loss:1.0720, Accuracy:0.3959, Validation Loss:1.0753, Validation Accuracy:0.3859\n",
    "Epoch #198: Loss:1.0718, Accuracy:0.4008, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #199: Loss:1.0715, Accuracy:0.4029, Validation Loss:1.0753, Validation Accuracy:0.3859\n",
    "Epoch #200: Loss:1.0712, Accuracy:0.4062, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #201: Loss:1.0710, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.3842\n",
    "Epoch #202: Loss:1.0710, Accuracy:0.4062, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #203: Loss:1.0711, Accuracy:0.4074, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #204: Loss:1.0711, Accuracy:0.4057, Validation Loss:1.0750, Validation Accuracy:0.3875\n",
    "Epoch #205: Loss:1.0709, Accuracy:0.4066, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #206: Loss:1.0717, Accuracy:0.4057, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #207: Loss:1.0707, Accuracy:0.4057, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #208: Loss:1.0714, Accuracy:0.4025, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #209: Loss:1.0716, Accuracy:0.4062, Validation Loss:1.0736, Validation Accuracy:0.3810\n",
    "Epoch #210: Loss:1.0713, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3629\n",
    "Epoch #211: Loss:1.0712, Accuracy:0.4082, Validation Loss:1.0740, Validation Accuracy:0.3580\n",
    "Epoch #212: Loss:1.0717, Accuracy:0.4066, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #213: Loss:1.0705, Accuracy:0.4078, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #214: Loss:1.0704, Accuracy:0.4070, Validation Loss:1.0749, Validation Accuracy:0.3760\n",
    "Epoch #215: Loss:1.0719, Accuracy:0.4016, Validation Loss:1.0739, Validation Accuracy:0.3793\n",
    "Epoch #216: Loss:1.0698, Accuracy:0.4066, Validation Loss:1.0744, Validation Accuracy:0.3777\n",
    "Epoch #217: Loss:1.0714, Accuracy:0.4057, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #218: Loss:1.0715, Accuracy:0.4053, Validation Loss:1.0744, Validation Accuracy:0.3760\n",
    "Epoch #219: Loss:1.0713, Accuracy:0.4025, Validation Loss:1.0740, Validation Accuracy:0.3777\n",
    "Epoch #220: Loss:1.0704, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3760\n",
    "Epoch #221: Loss:1.0705, Accuracy:0.4057, Validation Loss:1.0739, Validation Accuracy:0.3810\n",
    "Epoch #222: Loss:1.0702, Accuracy:0.4037, Validation Loss:1.0740, Validation Accuracy:0.3777\n",
    "Epoch #223: Loss:1.0717, Accuracy:0.4053, Validation Loss:1.0744, Validation Accuracy:0.3793\n",
    "Epoch #224: Loss:1.0702, Accuracy:0.4041, Validation Loss:1.0741, Validation Accuracy:0.3777\n",
    "Epoch #225: Loss:1.0700, Accuracy:0.4049, Validation Loss:1.0743, Validation Accuracy:0.3777\n",
    "Epoch #226: Loss:1.0698, Accuracy:0.3996, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #227: Loss:1.0715, Accuracy:0.4037, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #228: Loss:1.0704, Accuracy:0.4053, Validation Loss:1.0750, Validation Accuracy:0.3810\n",
    "Epoch #229: Loss:1.0704, Accuracy:0.4029, Validation Loss:1.0751, Validation Accuracy:0.3777\n",
    "Epoch #230: Loss:1.0701, Accuracy:0.4045, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #231: Loss:1.0693, Accuracy:0.4053, Validation Loss:1.0759, Validation Accuracy:0.3859\n",
    "Epoch #232: Loss:1.0694, Accuracy:0.4049, Validation Loss:1.0764, Validation Accuracy:0.3777\n",
    "Epoch #233: Loss:1.0694, Accuracy:0.4033, Validation Loss:1.0765, Validation Accuracy:0.3662\n",
    "Epoch #234: Loss:1.0689, Accuracy:0.4086, Validation Loss:1.0762, Validation Accuracy:0.3777\n",
    "Epoch #235: Loss:1.0696, Accuracy:0.4074, Validation Loss:1.0760, Validation Accuracy:0.3695\n",
    "Epoch #236: Loss:1.0696, Accuracy:0.4016, Validation Loss:1.0752, Validation Accuracy:0.3793\n",
    "Epoch #237: Loss:1.0696, Accuracy:0.3996, Validation Loss:1.0756, Validation Accuracy:0.3695\n",
    "Epoch #238: Loss:1.0691, Accuracy:0.4029, Validation Loss:1.0757, Validation Accuracy:0.3777\n",
    "Epoch #239: Loss:1.0694, Accuracy:0.4041, Validation Loss:1.0757, Validation Accuracy:0.3793\n",
    "Epoch #240: Loss:1.0695, Accuracy:0.4037, Validation Loss:1.0759, Validation Accuracy:0.3810\n",
    "Epoch #241: Loss:1.0693, Accuracy:0.4070, Validation Loss:1.0760, Validation Accuracy:0.3810\n",
    "Epoch #242: Loss:1.0692, Accuracy:0.4041, Validation Loss:1.0761, Validation Accuracy:0.3810\n",
    "Epoch #243: Loss:1.0692, Accuracy:0.4029, Validation Loss:1.0765, Validation Accuracy:0.3744\n",
    "Epoch #244: Loss:1.0689, Accuracy:0.4037, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #245: Loss:1.0690, Accuracy:0.4086, Validation Loss:1.0761, Validation Accuracy:0.3810\n",
    "Epoch #246: Loss:1.0704, Accuracy:0.4025, Validation Loss:1.0762, Validation Accuracy:0.3662\n",
    "Epoch #247: Loss:1.0719, Accuracy:0.3963, Validation Loss:1.0767, Validation Accuracy:0.3645\n",
    "Epoch #248: Loss:1.0694, Accuracy:0.4160, Validation Loss:1.0779, Validation Accuracy:0.3810\n",
    "Epoch #249: Loss:1.0699, Accuracy:0.4033, Validation Loss:1.0786, Validation Accuracy:0.3695\n",
    "Epoch #250: Loss:1.0698, Accuracy:0.4078, Validation Loss:1.0774, Validation Accuracy:0.3810\n",
    "Epoch #251: Loss:1.0698, Accuracy:0.3996, Validation Loss:1.0768, Validation Accuracy:0.3629\n",
    "Epoch #252: Loss:1.0691, Accuracy:0.4041, Validation Loss:1.0767, Validation Accuracy:0.3629\n",
    "Epoch #253: Loss:1.0704, Accuracy:0.3984, Validation Loss:1.0770, Validation Accuracy:0.3629\n",
    "Epoch #254: Loss:1.0687, Accuracy:0.4066, Validation Loss:1.0779, Validation Accuracy:0.3695\n",
    "Epoch #255: Loss:1.0689, Accuracy:0.4057, Validation Loss:1.0774, Validation Accuracy:0.3629\n",
    "Epoch #256: Loss:1.0712, Accuracy:0.4004, Validation Loss:1.0775, Validation Accuracy:0.3793\n",
    "Epoch #257: Loss:1.0723, Accuracy:0.4025, Validation Loss:1.0782, Validation Accuracy:0.3695\n",
    "Epoch #258: Loss:1.0699, Accuracy:0.4057, Validation Loss:1.0784, Validation Accuracy:0.3826\n",
    "Epoch #259: Loss:1.0697, Accuracy:0.4016, Validation Loss:1.0787, Validation Accuracy:0.3645\n",
    "Epoch #260: Loss:1.0695, Accuracy:0.4057, Validation Loss:1.0777, Validation Accuracy:0.3793\n",
    "Epoch #261: Loss:1.0699, Accuracy:0.4041, Validation Loss:1.0772, Validation Accuracy:0.3612\n",
    "Epoch #262: Loss:1.0693, Accuracy:0.4041, Validation Loss:1.0773, Validation Accuracy:0.3629\n",
    "Epoch #263: Loss:1.0684, Accuracy:0.4029, Validation Loss:1.0778, Validation Accuracy:0.3810\n",
    "Epoch #264: Loss:1.0690, Accuracy:0.3984, Validation Loss:1.0777, Validation Accuracy:0.3629\n",
    "Epoch #265: Loss:1.0685, Accuracy:0.4045, Validation Loss:1.0779, Validation Accuracy:0.3629\n",
    "Epoch #266: Loss:1.0692, Accuracy:0.4037, Validation Loss:1.0781, Validation Accuracy:0.3629\n",
    "Epoch #267: Loss:1.0689, Accuracy:0.4045, Validation Loss:1.0776, Validation Accuracy:0.3793\n",
    "Epoch #268: Loss:1.0686, Accuracy:0.4033, Validation Loss:1.0772, Validation Accuracy:0.3645\n",
    "Epoch #269: Loss:1.0681, Accuracy:0.4049, Validation Loss:1.0772, Validation Accuracy:0.3645\n",
    "Epoch #270: Loss:1.0683, Accuracy:0.4057, Validation Loss:1.0771, Validation Accuracy:0.3645\n",
    "Epoch #271: Loss:1.0687, Accuracy:0.4000, Validation Loss:1.0772, Validation Accuracy:0.3645\n",
    "Epoch #272: Loss:1.0682, Accuracy:0.4066, Validation Loss:1.0768, Validation Accuracy:0.3645\n",
    "Epoch #273: Loss:1.0682, Accuracy:0.4062, Validation Loss:1.0777, Validation Accuracy:0.3760\n",
    "Epoch #274: Loss:1.0689, Accuracy:0.4041, Validation Loss:1.0829, Validation Accuracy:0.3941\n",
    "Epoch #275: Loss:1.0729, Accuracy:0.3885, Validation Loss:1.0819, Validation Accuracy:0.3596\n",
    "Epoch #276: Loss:1.0683, Accuracy:0.3918, Validation Loss:1.0787, Validation Accuracy:0.3842\n",
    "Epoch #277: Loss:1.0713, Accuracy:0.3992, Validation Loss:1.0788, Validation Accuracy:0.3612\n",
    "Epoch #278: Loss:1.0708, Accuracy:0.3996, Validation Loss:1.0771, Validation Accuracy:0.3777\n",
    "Epoch #279: Loss:1.0698, Accuracy:0.4033, Validation Loss:1.0765, Validation Accuracy:0.3760\n",
    "Epoch #280: Loss:1.0697, Accuracy:0.4021, Validation Loss:1.0770, Validation Accuracy:0.3629\n",
    "Epoch #281: Loss:1.0681, Accuracy:0.4066, Validation Loss:1.0776, Validation Accuracy:0.3777\n",
    "Epoch #282: Loss:1.0683, Accuracy:0.4123, Validation Loss:1.0784, Validation Accuracy:0.3678\n",
    "Epoch #283: Loss:1.0684, Accuracy:0.4029, Validation Loss:1.0762, Validation Accuracy:0.3810\n",
    "Epoch #284: Loss:1.0674, Accuracy:0.4016, Validation Loss:1.0763, Validation Accuracy:0.3842\n",
    "Epoch #285: Loss:1.0675, Accuracy:0.4045, Validation Loss:1.0749, Validation Accuracy:0.3695\n",
    "Epoch #286: Loss:1.0679, Accuracy:0.4062, Validation Loss:1.0748, Validation Accuracy:0.3662\n",
    "Epoch #287: Loss:1.0696, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.3629\n",
    "Epoch #288: Loss:1.0677, Accuracy:0.4037, Validation Loss:1.0758, Validation Accuracy:0.3760\n",
    "Epoch #289: Loss:1.0681, Accuracy:0.4025, Validation Loss:1.0771, Validation Accuracy:0.3580\n",
    "Epoch #290: Loss:1.0673, Accuracy:0.4008, Validation Loss:1.0774, Validation Accuracy:0.3711\n",
    "Epoch #291: Loss:1.0673, Accuracy:0.4152, Validation Loss:1.0773, Validation Accuracy:0.3662\n",
    "Epoch #292: Loss:1.0691, Accuracy:0.4033, Validation Loss:1.0771, Validation Accuracy:0.3760\n",
    "Epoch #293: Loss:1.0685, Accuracy:0.4131, Validation Loss:1.0772, Validation Accuracy:0.3596\n",
    "Epoch #294: Loss:1.0674, Accuracy:0.4057, Validation Loss:1.0785, Validation Accuracy:0.3563\n",
    "Epoch #295: Loss:1.0669, Accuracy:0.4086, Validation Loss:1.0790, Validation Accuracy:0.3727\n",
    "Epoch #296: Loss:1.0671, Accuracy:0.4119, Validation Loss:1.0783, Validation Accuracy:0.3580\n",
    "Epoch #297: Loss:1.0688, Accuracy:0.4029, Validation Loss:1.0774, Validation Accuracy:0.3580\n",
    "Epoch #298: Loss:1.0686, Accuracy:0.4123, Validation Loss:1.0772, Validation Accuracy:0.3711\n",
    "Epoch #299: Loss:1.0674, Accuracy:0.4049, Validation Loss:1.0792, Validation Accuracy:0.3629\n",
    "Epoch #300: Loss:1.0689, Accuracy:0.4082, Validation Loss:1.0780, Validation Accuracy:0.3711\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07795656, Accuracy:0.3711\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02  03\n",
    "t:01  195  44   1\n",
    "t:02  196  31   0\n",
    "t:03  127  15   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.38      0.81      0.51       240\n",
    "          02       0.34      0.14      0.20       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.37       609\n",
    "   macro avg       0.24      0.32      0.24       609\n",
    "weighted avg       0.28      0.37      0.28       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 01:06:00 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 52 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0935545805444076, 1.0787542083580506, 1.0749983458683408, 1.0754974980659673, 1.0759985989146241, 1.0754159291585286, 1.074924136226009, 1.0747612513149118, 1.074659536429031, 1.0746798200168828, 1.074671612976024, 1.0746016110888452, 1.074527577030639, 1.0745105291235035, 1.0745351719738814, 1.0747426417465085, 1.0747479484194802, 1.0747171671715472, 1.0745779814195555, 1.0746972051943073, 1.0746824598468974, 1.0745377338970041, 1.0746513677543803, 1.074636907217342, 1.0747121388493304, 1.0747642276322313, 1.0745062749765582, 1.0744456814231935, 1.0745309060821784, 1.0744671590614006, 1.074578008236752, 1.0746981561281803, 1.074719376947688, 1.074647531720805, 1.0743997130292193, 1.074573927324981, 1.074701213680073, 1.0746452242674303, 1.074447941897538, 1.0746784818975013, 1.0746340636157834, 1.0748310754647592, 1.0748947081698965, 1.0743097806799, 1.0752106632896636, 1.0758775885664966, 1.0752581054549695, 1.0751418262866919, 1.075062339333282, 1.0747900310604053, 1.0748989893297844, 1.0747580829708057, 1.0748407975793473, 1.0747651832640073, 1.0749183006474536, 1.0746246674182185, 1.0747008615330913, 1.0748025850317944, 1.0748463093940848, 1.074809087321089, 1.074916328506908, 1.074808979465065, 1.0746046711854356, 1.0747696499910653, 1.0750608266085044, 1.0749054285888797, 1.0750869943199095, 1.0751757081506288, 1.0749746735264318, 1.075135261945928, 1.074594112452615, 1.0741347765491904, 1.074103959284001, 1.0747807619019683, 1.0750911083127477, 1.075037210445686, 1.075067703555566, 1.0749959597250902, 1.0749904794254521, 1.0748040503860499, 1.0749238510241454, 1.0748417021214276, 1.0747731855545921, 1.0750125078927903, 1.075388837330447, 1.075607618674856, 1.075148368508162, 1.075091340663202, 1.0750437603012486, 1.0755811661530794, 1.0752165683580346, 1.0763599302968367, 1.075336418128366, 1.075396799688856, 1.0757432841315058, 1.074989685675585, 1.0744534813124558, 1.0766202228997142, 1.0749419218996672, 1.0753376844089801, 1.0762257840245815, 1.0758586489704052, 1.076662092569035, 1.0765896099933068, 1.0768962981078425, 1.0761817503836746, 1.0761797340045423, 1.0751060449039602, 1.076516403549019, 1.0763828237655715, 1.077468713907577, 1.078069811384079, 1.0765317705855972, 1.0781458909875654, 1.0781888955919614, 1.0853362737226564, 1.080843210220337, 1.0796488415822998, 1.0808977614874127, 1.0783118348207772, 1.0775425561347423, 1.0767431987330245, 1.076741113842806, 1.0774059875062338, 1.079581087445978, 1.0802756553800235, 1.0781477830680133, 1.0786002494627227, 1.0811395660801277, 1.081825794649046, 1.0794777537410092, 1.0785147834489694, 1.0804790935688613, 1.0978942586870617, 1.0938828224423287, 1.0798466493343484, 1.0798388001171044, 1.0767404001530363, 1.0755182935295042, 1.073973547257422, 1.0746493177069427, 1.074666684484247, 1.074323511476, 1.0743673585710072, 1.0740342653052168, 1.0742686031682933, 1.0737896779879366, 1.0739411796842302, 1.073830295470352, 1.0739790762978039, 1.0738159427893377, 1.073851271215918, 1.0739423185342247, 1.073917679794512, 1.0738835002009701, 1.0738402702929744, 1.074005370461099, 1.0735868953523182, 1.0737695551075175, 1.0736982671693824, 1.0741028405958404, 1.0744079073465909, 1.0744020093250743, 1.0742774127152166, 1.0741772311074393, 1.0741074684218233, 1.0738891392505814, 1.0739729087341008, 1.0739337219589058, 1.0740099317334555, 1.0738469242853876, 1.0738555116606463, 1.0737220585248348, 1.0737125272625576, 1.073328181832099, 1.0731683270684604, 1.0730582109617286, 1.072990572902761, 1.075912204282037, 1.0745154398023984, 1.0741370896791982, 1.07397295140672, 1.0740265791443573, 1.074252528901562, 1.0743059900593874, 1.0744275528026137, 1.0743823626945759, 1.074303296203488, 1.074257990213842, 1.0744620332576957, 1.0746722278140841, 1.074833650307115, 1.0749706713045368, 1.075041827897133, 1.0750085221135557, 1.0750649748568857, 1.0753459274670956, 1.0751626098097251, 1.0752772345331503, 1.0750770463144839, 1.0752616173332352, 1.0751890533271877, 1.0750568331951773, 1.0750043948099923, 1.0751336666163553, 1.0749462539535046, 1.0736061336567444, 1.0737717942455522, 1.0735723342018566, 1.074285932558119, 1.0739946108929237, 1.0740462480898954, 1.0738491481552375, 1.074861053175527, 1.073851627865057, 1.0744037808260112, 1.0741687463030636, 1.07435031849371, 1.0739508849646657, 1.0739203229522079, 1.0738654130785337, 1.0740301826317322, 1.0743607734811718, 1.0741297589930017, 1.0743087784605856, 1.0747693385592432, 1.0747600279026626, 1.0749840219619826, 1.0751493699444925, 1.075174337145926, 1.075928368396164, 1.0763642335760182, 1.0764698992026067, 1.076228883270364, 1.0760400164107775, 1.075193781375102, 1.0756291442708232, 1.0756849565333726, 1.0757241008316942, 1.07590629999665, 1.0759733580603388, 1.0761248603438704, 1.0764913014786193, 1.076008701167866, 1.0761192167921019, 1.0762033317672404, 1.0766877893156606, 1.0778615580403745, 1.0785868387112671, 1.077402617348043, 1.0767571119643589, 1.0766664776700274, 1.0770437817268184, 1.077873472118221, 1.077356005341353, 1.077506691364232, 1.0781901244850973, 1.0784414480080942, 1.0787471217670659, 1.077716740677118, 1.077226713960394, 1.0772609834013314, 1.077811935852314, 1.077748872376428, 1.0778750821287408, 1.0781175506917517, 1.0775814627974687, 1.0772007114585789, 1.0771821850822085, 1.0771414885184252, 1.077159052020419, 1.076792899219469, 1.0776717686300794, 1.0828609842384977, 1.0819043784305966, 1.0787361186909166, 1.0788120191868498, 1.0770620779059399, 1.0765134556148634, 1.077012589020878, 1.0776087605503002, 1.0783518180862828, 1.0762259116509474, 1.076253705228295, 1.074862623253871, 1.0747978129410392, 1.0750023150091688, 1.0758235801029674, 1.0771463122861138, 1.0774488748587998, 1.0773459232499447, 1.0771101192496289, 1.0771950089872764, 1.0785464647368257, 1.0789810270315712, 1.0783099323657934, 1.0773503038487802, 1.0772468732495613, 1.0792373162380775, 1.0779565232140678], 'val_acc': [0.4022988491555544, 0.39244663260253193, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.38752052383665575, 0.385878487809734, 0.39244663260253193, 0.39244663260253193, 0.385878487711861, 0.38752052383665575, 0.38752052383665575, 0.385878487613988, 0.385878487711861, 0.37931034311480905, 0.39244663221104, 0.38752052373878276, 0.38752052373878276, 0.39244663221104, 0.39244663211316705, 0.385878487711861, 0.38752052373878276, 0.39080459608624524, 0.38423645158706626, 0.39244663260253193, 0.385878487809734, 0.3842364518806852, 0.40065681283501375, 0.38587848800547997, 0.3940886687273267, 0.3940886687273267, 0.3940886687273267, 0.3809523793374768, 0.39080459608624524, 0.3809523792396038, 0.38423645158706626, 0.3825944156580175, 0.3940886683358347, 0.38423645178281224, 0.39244663240678596, 0.385878487711861, 0.385878487711861, 0.385878487711861, 0.38752052393452874, 0.39080459598837225, 0.38916256025506946, 0.39573070475424843, 0.3940886687273267, 0.3940886687273267, 0.38752052373878276, 0.38095237963109574, 0.3891625599614505, 0.38423645158706626, 0.3940886684337077, 0.39244663221104, 0.3825944155601445, 0.38752052393452874, 0.39244663221104, 0.39080459608624524, 0.3875205240324017, 0.3825944156580175, 0.39244663230891297, 0.39573070446062947, 0.3973727407811702, 0.39244663230891297, 0.3940886684337077, 0.385878487809734, 0.39573070455850246, 0.3891625600593235, 0.3809523797289687, 0.3875205242281477, 0.38095237953322275, 0.3793103438977929, 0.3990147771017109, 0.3908045965756102, 0.39573070475424843, 0.37602627135458444, 0.40558292179663585, 0.3760262716482034, 0.3793103437999199, 0.3825944159516364, 0.3908045965756102, 0.3940886690209456, 0.3760262717460764, 0.37931034428928484, 0.3678160910242297, 0.3645320188725132, 0.3612479466229237, 0.3875205244238936, 0.3711001627844543, 0.36781609063273774, 0.3678160908284837, 0.3825944161473824, 0.39408866911881857, 0.36617405441007006, 0.38259441644100134, 0.3661740542143241, 0.3628899823562265, 0.3711001627844543, 0.3645320185788942, 0.35960590991089103, 0.3727421994964869, 0.3645320185788942, 0.38587848849484485, 0.38587848820122594, 0.36617405470368897, 0.3612479464271777, 0.366174054605816, 0.36453201838314825, 0.36124794632930474, 0.38259441624525536, 0.36453201848102124, 0.38423645266366907, 0.37766830747937924, 0.3940886687273267, 0.3990147776889488, 0.36124794632930474, 0.37110016337169216, 0.37931034419141185, 0.3842364522721771, 0.3957307049499944, 0.3940886687273267, 0.3940886687273267, 0.3957307049499944, 0.39408866931456454, 0.3760262718439494, 0.3776683079687441, 0.3776683079687441, 0.3940886690209456, 0.39408866882519966, 0.3760262715503304, 0.37438423571915463, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3924466327004049, 0.39244663260253193, 0.39408866882519966, 0.3924466327004049, 0.3924466327004049, 0.3924466327004049, 0.3924466327004049, 0.3908045965756102, 0.3908045965756102, 0.3924466327004049, 0.3924466327004049, 0.39408866882519966, 0.38916256015719647, 0.3875205240324017, 0.39244663240678596, 0.38916256015719647, 0.3908045962819912, 0.3940886687273267, 0.3957307049499944, 0.39408866882519966, 0.39408866882519966, 0.38095237943534976, 0.38095237943534976, 0.385878487809734, 0.385878487809734, 0.385878487809734, 0.3908045962819912, 0.38423645168493925, 0.38423645168493925, 0.38423645168493925, 0.38095237943534976, 0.3825944155601445, 0.3825944155601445, 0.3825944155601445, 0.38423645178281224, 0.38587848800547997, 0.3825944155601445, 0.38587848800547997, 0.38423645168493925, 0.38423645178281224, 0.385878487907607, 0.3825944156580175, 0.38752052393452874, 0.3842364514891933, 0.38752052393452874, 0.385878487907607, 0.3842364519785582, 0.38095237963109574, 0.3628899824540995, 0.3579638738839693, 0.3875205240324017, 0.38916256015719647, 0.37602627106096553, 0.37931034321268203, 0.37766830728363326, 0.385878487809734, 0.37602627106096553, 0.3776683071857603, 0.37602627106096553, 0.3809523793374768, 0.3776683071857603, 0.379310343310555, 0.3776683071857603, 0.3776683071857603, 0.385878487809734, 0.37602627106096553, 0.3809523793374768, 0.3776683071857603, 0.38095237943534976, 0.385878487809734, 0.3776683071857603, 0.366174054605816, 0.37766830738150625, 0.36945812705115144, 0.379310343506301, 0.36945812705115144, 0.37766830738150625, 0.379310343506301, 0.3809523797289687, 0.3809523797289687, 0.3809523797289687, 0.37438423542553567, 0.38259441585376347, 0.38095237963109574, 0.36617405499730793, 0.3645320188725132, 0.38095237953322275, 0.36945812695327845, 0.38095237953322275, 0.36288998264984546, 0.36288998264984546, 0.36288998264984546, 0.36945812695327845, 0.36288998264984546, 0.379310343506301, 0.36945812695327845, 0.3825944155601445, 0.3645320188725132, 0.37931034321268203, 0.3612479465250507, 0.36288998274771844, 0.38095237953322275, 0.36288998264984546, 0.36288998264984546, 0.36288998264984546, 0.379310343506301, 0.3645320187746402, 0.3645320187746402, 0.3645320187746402, 0.3645320187746402, 0.3645320185788942, 0.3760262717460764, 0.3940886690209456, 0.359605910106637, 0.3842364520764312, 0.3612479466229237, 0.37766830738150625, 0.3760262712567115, 0.36288998274771844, 0.37766830738150625, 0.3678160908284837, 0.3809523797289687, 0.3842364520764312, 0.3694581273447704, 0.36617405499730793, 0.36288998264984546, 0.3760262712567115, 0.3579638743733342, 0.3711001628823273, 0.36617405499730793, 0.37602627135458444, 0.35960591049812896, 0.3563218382485394, 0.372742199007122, 0.3579638743733342, 0.3579638743733342, 0.3711001628823273, 0.36288998274771844, 0.3711001628823273], 'loss': [1.1139227127881999, 1.08662415314504, 1.0760293196358965, 1.073945877586302, 1.0751963467079022, 1.07525842488424, 1.0745446083726824, 1.0741152876456415, 1.0740992562726783, 1.0739865144420209, 1.0739937756340607, 1.0739042826991307, 1.0738870629539725, 1.074234429375102, 1.0739231978108996, 1.0738270184342622, 1.0738238894719119, 1.07377193326578, 1.0736973982327283, 1.0737360507800593, 1.0736695852123002, 1.0736471281404123, 1.073615570577508, 1.0735760876775033, 1.073488368586593, 1.0735038509114323, 1.073449863055893, 1.0733839536349632, 1.073350319529461, 1.0733552646343223, 1.073356271034883, 1.0734795728992876, 1.0735708186758617, 1.0735429416691742, 1.073466731880235, 1.0733426687898577, 1.0734776444993224, 1.0732750315930564, 1.073162726210373, 1.073366355895996, 1.0732725930164972, 1.0731962718023658, 1.0732635371738881, 1.0731787662976087, 1.0742229004904964, 1.0741643665752372, 1.074286486921369, 1.0740381939455224, 1.074134754008581, 1.074028339131412, 1.0737375196734982, 1.073291273381431, 1.073483353131116, 1.0736420062043583, 1.0737397945392304, 1.0735604639660405, 1.0734944692137796, 1.0735922871673866, 1.0734159110263144, 1.073279936014994, 1.0732932883366422, 1.073130436697535, 1.0732124461285633, 1.0730148558743926, 1.0736619581676852, 1.072867775550858, 1.0729783271127658, 1.0733671010152515, 1.0734126821435697, 1.073393745441946, 1.0729435198116106, 1.0729356900377685, 1.0728024289348532, 1.0723838803704515, 1.0726642996378748, 1.0725419156605214, 1.0723952281646416, 1.0723707484513583, 1.0724681271175094, 1.0723549663163798, 1.0724654057187466, 1.0724438797521885, 1.0720566551788142, 1.0719228201578286, 1.071548924504854, 1.0713228356422095, 1.0709906519316061, 1.0708645302656985, 1.0711625839405725, 1.0706954831704956, 1.0709157379256138, 1.0707011882284583, 1.0700491605108524, 1.0706802703027118, 1.0702909841184989, 1.070537074978102, 1.0704472749385012, 1.0688534030679315, 1.070380101801189, 1.0693612717015542, 1.0700802574901855, 1.0697201486485695, 1.069444707629617, 1.0685013158610224, 1.0686320949139292, 1.0687484083234406, 1.067918755680139, 1.0681997048536611, 1.069084857964173, 1.0706564194367896, 1.0692096148673025, 1.0678653653397452, 1.0682435746065646, 1.0680693139775332, 1.068037815554186, 1.068903300111054, 1.0701767126882344, 1.070902978713018, 1.067908684575827, 1.0681520338665533, 1.0676783876497398, 1.0666278850860909, 1.0659461882569707, 1.0663356474537624, 1.0665213026794809, 1.0667044237653822, 1.0691142446940929, 1.067585037229487, 1.0664555381210923, 1.0671498727015156, 1.0649105499902056, 1.0704002303509252, 1.0687617540359498, 1.078941420950684, 1.0993766600591202, 1.0816622979342325, 1.0799284174701762, 1.0797280008053634, 1.0751576403572818, 1.0747095945189866, 1.0739646228187139, 1.0742980361229586, 1.0739905970786876, 1.0739650841855906, 1.07444098240541, 1.074350727803898, 1.0740514049784604, 1.0737039816697764, 1.0737609183274255, 1.0733685680972966, 1.073311722107247, 1.0731193149848641, 1.073480193766725, 1.0733169375993388, 1.0730989809643317, 1.0731626640355072, 1.0736277834345918, 1.0730305631547492, 1.0730622894710093, 1.072836516818961, 1.0728744658601357, 1.0726701909266947, 1.0725195200291502, 1.0730971144454924, 1.072817893292625, 1.0729970317112103, 1.072689804942701, 1.0726918974941027, 1.0726746690836286, 1.0725007582494122, 1.0725017757631181, 1.0724159964301014, 1.0721873976121938, 1.0723061650195895, 1.0723183783172827, 1.0723790908006672, 1.0721968902944294, 1.072313930758216, 1.072327109626676, 1.0747232304951004, 1.0733621624216163, 1.0724218805712595, 1.0724683695994852, 1.0725143743491516, 1.0729355519801929, 1.072464568022585, 1.0729636277506238, 1.072532700953787, 1.0720421348019546, 1.0717964606608208, 1.0716170241945333, 1.071368820564458, 1.071583606477146, 1.0719354311298785, 1.071597930244352, 1.0715078178127688, 1.0720128085823764, 1.0718452437947172, 1.0715129951187228, 1.0712387377231762, 1.07099148414463, 1.0709766364929858, 1.071086350456645, 1.0710724306302393, 1.0709057089239664, 1.0716504867071979, 1.0707361157669912, 1.0713837590305713, 1.0715710320756666, 1.0712603222418124, 1.0711808127789038, 1.0717030934974154, 1.0705362708172024, 1.070353553329405, 1.0718791920056823, 1.0698493002866083, 1.071449674620031, 1.0715297365580252, 1.0713465719986746, 1.0704319023253737, 1.0705197288270358, 1.0702096917546016, 1.0717272557272315, 1.0701611906596034, 1.070014161642572, 1.069772717595345, 1.0715256824140922, 1.0704346382152863, 1.0704030703714986, 1.0700756167973826, 1.0693310250002257, 1.0694071478912228, 1.069416729184881, 1.0688944106229277, 1.069638090163041, 1.0695976138359713, 1.0696289753277444, 1.0691455432766517, 1.0693986426633486, 1.0694620297429986, 1.0692923068021112, 1.069201575510311, 1.0691750358017562, 1.0689234227370432, 1.0690112157523999, 1.0704406123876082, 1.0719488537042292, 1.0693959583736787, 1.0699067634723514, 1.069768145833417, 1.0697636922526899, 1.0690845546291594, 1.0703979080952168, 1.0687456817352796, 1.068912686165843, 1.0712123957992334, 1.0723377268906735, 1.0699098373095848, 1.0696663890286393, 1.069526996896497, 1.0698946653695076, 1.0693392165142896, 1.0684414870440349, 1.0689632121542396, 1.0684563704339876, 1.0691742495100112, 1.0688546780443289, 1.0686052030606437, 1.0681099619463974, 1.0682936826036207, 1.0686947726126323, 1.0681589790927801, 1.0681840814359378, 1.0688808376539414, 1.0729387822826786, 1.0683231618614901, 1.0712970529004044, 1.0708322086373394, 1.0697596310590083, 1.06970753322147, 1.068086374320044, 1.0683057888822145, 1.0684143348885757, 1.0674198337159362, 1.0675432800022728, 1.0678765152024536, 1.0695952336156638, 1.0676976183846256, 1.0681135757747862, 1.0673247690318302, 1.067265483535046, 1.0690842343551668, 1.0685145018281879, 1.0673779538524715, 1.066901657370816, 1.067143279130454, 1.068799297863453, 1.0685750990188096, 1.067366160406469, 1.068926841179693], 'acc': [0.24681724964715618, 0.39548254833084356, 0.3942505148891551, 0.3942505152808078, 0.3942505129308916, 0.3942505138733059, 0.3942505115233897, 0.39425051391002336, 0.3942505156724605, 0.39425051312671794, 0.394250514301676, 0.39425051391002336, 0.39425051391002336, 0.3963039020003724, 0.39425051547663414, 0.3942505146933287, 0.39425051449750237, 0.394250513714197, 0.39425051391002336, 0.3950718687177928, 0.39548254735171184, 0.3946611905489614, 0.39425051250252147, 0.3942505146933287, 0.3963039005928705, 0.3946611925072249, 0.3946611921155722, 0.3963039005928705, 0.3971252568081419, 0.39589322356227974, 0.3950718685586839, 0.3975359332879711, 0.39794660969436535, 0.39753593665373643, 0.39548254539344835, 0.395893225165608, 0.39589322496978163, 0.39917864708929823, 0.4008213565824458, 0.39958932176997286, 0.395893225165608, 0.3942505125392389, 0.39917864708929823, 0.39958932349569254, 0.394661189765656, 0.37412730807885985, 0.3720739235133851, 0.39260780281354757, 0.3946611927030512, 0.39425051449750237, 0.3942505117559335, 0.3950718685586839, 0.39794661208099896, 0.3963039025878515, 0.3963039000421089, 0.39466118953311224, 0.3975359330554273, 0.3983572906782005, 0.39917864689347193, 0.40164270845282, 0.3979466122768253, 0.39794661403926246, 0.3979466134517834, 0.4016427082569937, 0.39055441315658773, 0.4004106767735687, 0.39835728927069863, 0.394250514301676, 0.3975359328596009, 0.39507186832614016, 0.3987679653221577, 0.3950718673470084, 0.39507187032112107, 0.40041067540278424, 0.39671458138087934, 0.3991786461101665, 0.40164271064362733, 0.4032854226825174, 0.40205339002413426, 0.4045174515834824, 0.4008213575615775, 0.4024640668588987, 0.40328542229086467, 0.40164271060690987, 0.40205338806587076, 0.39876796712131224, 0.4041067775270043, 0.40246406509646154, 0.4102669390197652, 0.40903490913966845, 0.40574948620012896, 0.40205338747839175, 0.40698151787938036, 0.40287474365694564, 0.40821355190854786, 0.40698151948270855, 0.40616016561735335, 0.4123203276975933, 0.39630390337115684, 0.4127310060989685, 0.40944558754104365, 0.40903490812381926, 0.40657083967383145, 0.40657084303959684, 0.40328542092008024, 0.40369609892980274, 0.409034907768884, 0.40328542307417004, 0.3995893211457764, 0.40369610030058717, 0.4020533882616971, 0.4028747440853158, 0.40410677533619704, 0.4123203290683778, 0.40616016205576166, 0.41190965324946255, 0.4016427118185854, 0.40369609990893446, 0.40041067896437593, 0.40164270923612544, 0.41355235957267106, 0.411088295271754, 0.40944558417527827, 0.41273100672316504, 0.40780287389882536, 0.40862422792328945, 0.40410677494454433, 0.40698151846685937, 0.40698152104931934, 0.4123203285176162, 0.41067762000360036, 0.40698152144097205, 0.4094455857786065, 0.3930184824265983, 0.39548254735171184, 0.39096509155796294, 0.37535934230385376, 0.37453798765519314, 0.37823408719695323, 0.3909650931612912, 0.39425051328582683, 0.394250513714197, 0.3942505133225443, 0.38973306050290807, 0.3942505119150424, 0.39548254696005913, 0.3958932231706271, 0.3979466098901917, 0.39630390141289334, 0.39712525524153114, 0.3963039025878515, 0.3942505113275634, 0.3975359344629292, 0.39712525524153114, 0.3975359328596009, 0.39753593266377457, 0.3975359354420609, 0.39794661403926246, 0.3975359348178644, 0.3975359342671028, 0.3975359350504082, 0.39753593563788725, 0.39876796610546306, 0.39589322536143434, 0.39712525524153114, 0.39876796907957573, 0.39753593309214474, 0.39753593642119267, 0.3975359328596009, 0.39958932353241, 0.39712525661231557, 0.39753593367962375, 0.3971252578239911, 0.39712525622066286, 0.39917864431101197, 0.3983572898581777, 0.39835729122896213, 0.39958932176997286, 0.3958932237581061, 0.3946611927030512, 0.3958932241497588, 0.3958932255572607, 0.39753593270049203, 0.40328542107918913, 0.40123203322138384, 0.4016427094686692, 0.401232034983821, 0.4020533862667162, 0.402464066467246, 0.4008213555665966, 0.40123203361303655, 0.40123203420051556, 0.4008213536083331, 0.4016427126018908, 0.40123203517964734, 0.4012320310305766, 0.39589322199566895, 0.40082135576242295, 0.40287474388948946, 0.40616016600900606, 0.4053388070154484, 0.40616016283906703, 0.40739219628075557, 0.40574948858676263, 0.4065708404204194, 0.40574948858676263, 0.40574948662849913, 0.4024640668588987, 0.4061601642098515, 0.39917864313605383, 0.4082135527285707, 0.4065708428437705, 0.40780287291969364, 0.40698151846685937, 0.40164271256517337, 0.40657084186463877, 0.4057494844376918, 0.4053388103812137, 0.40246406447226507, 0.40287474150285585, 0.4057494854168236, 0.40369609971310816, 0.4053388076029274, 0.404106777918657, 0.4049281319798385, 0.3995893221249081, 0.40369609892980274, 0.4053388109319753, 0.40287474111120314, 0.4045174531500932, 0.40533880998956107, 0.4049281313923595, 0.4032854189250993, 0.40862423210907767, 0.4073921982757365, 0.4016427126018908, 0.39958932192908175, 0.4028747428736403, 0.40410677831030967, 0.40369609990893446, 0.4069815186626857, 0.40410677811448337, 0.40287474150285585, 0.4036960971673656, 0.40862422815583327, 0.4024640656839406, 0.3963039035669832, 0.4160164252810899, 0.4032854203326012, 0.407802873702999, 0.39958932294493094, 0.40410677693952524, 0.3983572882915669, 0.4065708414362686, 0.4057494878034572, 0.40041067579443695, 0.40246406408061236, 0.40574948561264995, 0.4016427127977171, 0.4057494844376918, 0.4041067755320234, 0.40410677772283066, 0.40287474506444754, 0.3983572896990688, 0.40451745432505126, 0.4036960987339764, 0.4045174551450741, 0.40328541912092564, 0.4049281329589703, 0.4057494844376918, 0.4000000003671744, 0.40657084401872856, 0.4061601634265461, 0.4041067755320234, 0.38850102529878244, 0.39178644542331814, 0.39917864591434016, 0.39958932036247097, 0.40328541912092564, 0.4020533894366552, 0.4065708398696578, 0.4123203284808987, 0.4028747421270523, 0.40164270923612544, 0.4045174543617687, 0.40616016362237245, 0.40451745416594237, 0.4036960965431691, 0.40246406748309516, 0.40082135325339785, 0.41519507102408204, 0.4032854232699964, 0.41314168273790663, 0.4057494879625661, 0.4086242279600069, 0.41190965324946255, 0.40287474306946663, 0.4123203294967479, 0.40492813315479664, 0.40821355233691803]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
