{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf7.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 00:51:29 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '2', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ib', 'ds', 'my', 'by', 'mb', 'yd', 'eg', 'eb', 'eo', 'ek', 'aa', 'sk', 'sg', 'ce', 'ck'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001DBA803D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001DBA5796EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7100, Accuracy:0.0591, Validation Loss:2.7036, Validation Accuracy:0.0755\n",
    "Epoch #2: Loss:2.7010, Accuracy:0.0793, Validation Loss:2.6950, Validation Accuracy:0.0805\n",
    "Epoch #3: Loss:2.6927, Accuracy:0.0797, Validation Loss:2.6878, Validation Accuracy:0.0805\n",
    "Epoch #4: Loss:2.6855, Accuracy:0.1084, Validation Loss:2.6811, Validation Accuracy:0.1100\n",
    "Epoch #5: Loss:2.6794, Accuracy:0.1117, Validation Loss:2.6738, Validation Accuracy:0.1018\n",
    "Epoch #6: Loss:2.6725, Accuracy:0.1023, Validation Loss:2.6674, Validation Accuracy:0.1034\n",
    "Epoch #7: Loss:2.6651, Accuracy:0.1027, Validation Loss:2.6610, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6576, Accuracy:0.1092, Validation Loss:2.6508, Validation Accuracy:0.1248\n",
    "Epoch #9: Loss:2.6476, Accuracy:0.1273, Validation Loss:2.6384, Validation Accuracy:0.1379\n",
    "Epoch #10: Loss:2.6352, Accuracy:0.1363, Validation Loss:2.6249, Validation Accuracy:0.1297\n",
    "Epoch #11: Loss:2.6228, Accuracy:0.1392, Validation Loss:2.6133, Validation Accuracy:0.1379\n",
    "Epoch #12: Loss:2.6113, Accuracy:0.1433, Validation Loss:2.6030, Validation Accuracy:0.1494\n",
    "Epoch #13: Loss:2.5990, Accuracy:0.1569, Validation Loss:2.5832, Validation Accuracy:0.1527\n",
    "Epoch #14: Loss:2.5801, Accuracy:0.1561, Validation Loss:2.5638, Validation Accuracy:0.1511\n",
    "Epoch #15: Loss:2.5581, Accuracy:0.1598, Validation Loss:2.5470, Validation Accuracy:0.1560\n",
    "Epoch #16: Loss:2.5426, Accuracy:0.1602, Validation Loss:2.5299, Validation Accuracy:0.1560\n",
    "Epoch #17: Loss:2.5307, Accuracy:0.1577, Validation Loss:2.5186, Validation Accuracy:0.1593\n",
    "Epoch #18: Loss:2.5184, Accuracy:0.1622, Validation Loss:2.5090, Validation Accuracy:0.1544\n",
    "Epoch #19: Loss:2.5069, Accuracy:0.1577, Validation Loss:2.4996, Validation Accuracy:0.1626\n",
    "Epoch #20: Loss:2.4970, Accuracy:0.1639, Validation Loss:2.4894, Validation Accuracy:0.1675\n",
    "Epoch #21: Loss:2.4914, Accuracy:0.1630, Validation Loss:2.4852, Validation Accuracy:0.1675\n",
    "Epoch #22: Loss:2.4873, Accuracy:0.1651, Validation Loss:2.4840, Validation Accuracy:0.1691\n",
    "Epoch #23: Loss:2.4832, Accuracy:0.1659, Validation Loss:2.4783, Validation Accuracy:0.1708\n",
    "Epoch #24: Loss:2.4780, Accuracy:0.1704, Validation Loss:2.4745, Validation Accuracy:0.1741\n",
    "Epoch #25: Loss:2.4751, Accuracy:0.1713, Validation Loss:2.4678, Validation Accuracy:0.1708\n",
    "Epoch #26: Loss:2.4720, Accuracy:0.1696, Validation Loss:2.4699, Validation Accuracy:0.1658\n",
    "Epoch #27: Loss:2.4688, Accuracy:0.1704, Validation Loss:2.4697, Validation Accuracy:0.1626\n",
    "Epoch #28: Loss:2.4667, Accuracy:0.1729, Validation Loss:2.4685, Validation Accuracy:0.1658\n",
    "Epoch #29: Loss:2.4640, Accuracy:0.1815, Validation Loss:2.4666, Validation Accuracy:0.1741\n",
    "Epoch #30: Loss:2.4638, Accuracy:0.1795, Validation Loss:2.4705, Validation Accuracy:0.1593\n",
    "Epoch #31: Loss:2.4638, Accuracy:0.1811, Validation Loss:2.4717, Validation Accuracy:0.1708\n",
    "Epoch #32: Loss:2.4636, Accuracy:0.1774, Validation Loss:2.4752, Validation Accuracy:0.1461\n",
    "Epoch #33: Loss:2.4621, Accuracy:0.1799, Validation Loss:2.4707, Validation Accuracy:0.1642\n",
    "Epoch #34: Loss:2.4612, Accuracy:0.1819, Validation Loss:2.4738, Validation Accuracy:0.1429\n",
    "Epoch #35: Loss:2.4622, Accuracy:0.1811, Validation Loss:2.4777, Validation Accuracy:0.1593\n",
    "Epoch #36: Loss:2.4596, Accuracy:0.1786, Validation Loss:2.4700, Validation Accuracy:0.1576\n",
    "Epoch #37: Loss:2.4569, Accuracy:0.1881, Validation Loss:2.4649, Validation Accuracy:0.1642\n",
    "Epoch #38: Loss:2.4576, Accuracy:0.1807, Validation Loss:2.4639, Validation Accuracy:0.1642\n",
    "Epoch #39: Loss:2.4553, Accuracy:0.1828, Validation Loss:2.4639, Validation Accuracy:0.1560\n",
    "Epoch #40: Loss:2.4535, Accuracy:0.1782, Validation Loss:2.4620, Validation Accuracy:0.1626\n",
    "Epoch #41: Loss:2.4528, Accuracy:0.1791, Validation Loss:2.4591, Validation Accuracy:0.1593\n",
    "Epoch #42: Loss:2.4513, Accuracy:0.1807, Validation Loss:2.4584, Validation Accuracy:0.1609\n",
    "Epoch #43: Loss:2.4516, Accuracy:0.1778, Validation Loss:2.4597, Validation Accuracy:0.1609\n",
    "Epoch #44: Loss:2.4515, Accuracy:0.1811, Validation Loss:2.4609, Validation Accuracy:0.1675\n",
    "Epoch #45: Loss:2.4503, Accuracy:0.1819, Validation Loss:2.4579, Validation Accuracy:0.1626\n",
    "Epoch #46: Loss:2.4491, Accuracy:0.1836, Validation Loss:2.4602, Validation Accuracy:0.1511\n",
    "Epoch #47: Loss:2.4474, Accuracy:0.1840, Validation Loss:2.4605, Validation Accuracy:0.1642\n",
    "Epoch #48: Loss:2.4469, Accuracy:0.1823, Validation Loss:2.4582, Validation Accuracy:0.1511\n",
    "Epoch #49: Loss:2.4458, Accuracy:0.1856, Validation Loss:2.4569, Validation Accuracy:0.1691\n",
    "Epoch #50: Loss:2.4472, Accuracy:0.1881, Validation Loss:2.4584, Validation Accuracy:0.1609\n",
    "Epoch #51: Loss:2.4479, Accuracy:0.1823, Validation Loss:2.4626, Validation Accuracy:0.1560\n",
    "Epoch #52: Loss:2.4471, Accuracy:0.1823, Validation Loss:2.4606, Validation Accuracy:0.1642\n",
    "Epoch #53: Loss:2.4454, Accuracy:0.1840, Validation Loss:2.4576, Validation Accuracy:0.1576\n",
    "Epoch #54: Loss:2.4442, Accuracy:0.1901, Validation Loss:2.4575, Validation Accuracy:0.1609\n",
    "Epoch #55: Loss:2.4426, Accuracy:0.1869, Validation Loss:2.4608, Validation Accuracy:0.1576\n",
    "Epoch #56: Loss:2.4418, Accuracy:0.1889, Validation Loss:2.4589, Validation Accuracy:0.1609\n",
    "Epoch #57: Loss:2.4410, Accuracy:0.1856, Validation Loss:2.4609, Validation Accuracy:0.1511\n",
    "Epoch #58: Loss:2.4434, Accuracy:0.1815, Validation Loss:2.4566, Validation Accuracy:0.1609\n",
    "Epoch #59: Loss:2.4435, Accuracy:0.1869, Validation Loss:2.4559, Validation Accuracy:0.1560\n",
    "Epoch #60: Loss:2.4436, Accuracy:0.1852, Validation Loss:2.4542, Validation Accuracy:0.1544\n",
    "Epoch #61: Loss:2.4429, Accuracy:0.1819, Validation Loss:2.4534, Validation Accuracy:0.1609\n",
    "Epoch #62: Loss:2.4432, Accuracy:0.1811, Validation Loss:2.4513, Validation Accuracy:0.1593\n",
    "Epoch #63: Loss:2.4429, Accuracy:0.1832, Validation Loss:2.4526, Validation Accuracy:0.1576\n",
    "Epoch #64: Loss:2.4436, Accuracy:0.1803, Validation Loss:2.4518, Validation Accuracy:0.1609\n",
    "Epoch #65: Loss:2.4447, Accuracy:0.1815, Validation Loss:2.4567, Validation Accuracy:0.1609\n",
    "Epoch #66: Loss:2.4438, Accuracy:0.1803, Validation Loss:2.4536, Validation Accuracy:0.1593\n",
    "Epoch #67: Loss:2.4411, Accuracy:0.1840, Validation Loss:2.4555, Validation Accuracy:0.1593\n",
    "Epoch #68: Loss:2.4400, Accuracy:0.1799, Validation Loss:2.4550, Validation Accuracy:0.1609\n",
    "Epoch #69: Loss:2.4395, Accuracy:0.1869, Validation Loss:2.4551, Validation Accuracy:0.1609\n",
    "Epoch #70: Loss:2.4394, Accuracy:0.1869, Validation Loss:2.4539, Validation Accuracy:0.1576\n",
    "Epoch #71: Loss:2.4396, Accuracy:0.1885, Validation Loss:2.4536, Validation Accuracy:0.1560\n",
    "Epoch #72: Loss:2.4386, Accuracy:0.1856, Validation Loss:2.4527, Validation Accuracy:0.1560\n",
    "Epoch #73: Loss:2.4384, Accuracy:0.1885, Validation Loss:2.4524, Validation Accuracy:0.1593\n",
    "Epoch #74: Loss:2.4383, Accuracy:0.1836, Validation Loss:2.4535, Validation Accuracy:0.1593\n",
    "Epoch #75: Loss:2.4368, Accuracy:0.1881, Validation Loss:2.4524, Validation Accuracy:0.1560\n",
    "Epoch #76: Loss:2.4384, Accuracy:0.1836, Validation Loss:2.4539, Validation Accuracy:0.1511\n",
    "Epoch #77: Loss:2.4362, Accuracy:0.1864, Validation Loss:2.4527, Validation Accuracy:0.1658\n",
    "Epoch #78: Loss:2.4372, Accuracy:0.1852, Validation Loss:2.4527, Validation Accuracy:0.1494\n",
    "Epoch #79: Loss:2.4365, Accuracy:0.1897, Validation Loss:2.4501, Validation Accuracy:0.1560\n",
    "Epoch #80: Loss:2.4361, Accuracy:0.1881, Validation Loss:2.4523, Validation Accuracy:0.1511\n",
    "Epoch #81: Loss:2.4352, Accuracy:0.1881, Validation Loss:2.4508, Validation Accuracy:0.1544\n",
    "Epoch #82: Loss:2.4354, Accuracy:0.1906, Validation Loss:2.4497, Validation Accuracy:0.1593\n",
    "Epoch #83: Loss:2.4351, Accuracy:0.1786, Validation Loss:2.4513, Validation Accuracy:0.1593\n",
    "Epoch #84: Loss:2.4358, Accuracy:0.1864, Validation Loss:2.4491, Validation Accuracy:0.1560\n",
    "Epoch #85: Loss:2.4372, Accuracy:0.1893, Validation Loss:2.4508, Validation Accuracy:0.1593\n",
    "Epoch #86: Loss:2.4363, Accuracy:0.1877, Validation Loss:2.4517, Validation Accuracy:0.1576\n",
    "Epoch #87: Loss:2.4338, Accuracy:0.1836, Validation Loss:2.4518, Validation Accuracy:0.1494\n",
    "Epoch #88: Loss:2.4331, Accuracy:0.1860, Validation Loss:2.4503, Validation Accuracy:0.1626\n",
    "Epoch #89: Loss:2.4324, Accuracy:0.1881, Validation Loss:2.4521, Validation Accuracy:0.1560\n",
    "Epoch #90: Loss:2.4321, Accuracy:0.1889, Validation Loss:2.4516, Validation Accuracy:0.1593\n",
    "Epoch #91: Loss:2.4322, Accuracy:0.1877, Validation Loss:2.4517, Validation Accuracy:0.1626\n",
    "Epoch #92: Loss:2.4322, Accuracy:0.1877, Validation Loss:2.4531, Validation Accuracy:0.1527\n",
    "Epoch #93: Loss:2.4333, Accuracy:0.1889, Validation Loss:2.4498, Validation Accuracy:0.1642\n",
    "Epoch #94: Loss:2.4317, Accuracy:0.1819, Validation Loss:2.4511, Validation Accuracy:0.1593\n",
    "Epoch #95: Loss:2.4309, Accuracy:0.1873, Validation Loss:2.4503, Validation Accuracy:0.1527\n",
    "Epoch #96: Loss:2.4309, Accuracy:0.1901, Validation Loss:2.4516, Validation Accuracy:0.1527\n",
    "Epoch #97: Loss:2.4309, Accuracy:0.1881, Validation Loss:2.4518, Validation Accuracy:0.1527\n",
    "Epoch #98: Loss:2.4311, Accuracy:0.1877, Validation Loss:2.4527, Validation Accuracy:0.1544\n",
    "Epoch #99: Loss:2.4310, Accuracy:0.1901, Validation Loss:2.4524, Validation Accuracy:0.1544\n",
    "Epoch #100: Loss:2.4313, Accuracy:0.1901, Validation Loss:2.4516, Validation Accuracy:0.1511\n",
    "Epoch #101: Loss:2.4309, Accuracy:0.1901, Validation Loss:2.4520, Validation Accuracy:0.1527\n",
    "Epoch #102: Loss:2.4315, Accuracy:0.1881, Validation Loss:2.4503, Validation Accuracy:0.1527\n",
    "Epoch #103: Loss:2.4294, Accuracy:0.1893, Validation Loss:2.4529, Validation Accuracy:0.1478\n",
    "Epoch #104: Loss:2.4309, Accuracy:0.1844, Validation Loss:2.4524, Validation Accuracy:0.1527\n",
    "Epoch #105: Loss:2.4306, Accuracy:0.1914, Validation Loss:2.4541, Validation Accuracy:0.1478\n",
    "Epoch #106: Loss:2.4305, Accuracy:0.1864, Validation Loss:2.4518, Validation Accuracy:0.1560\n",
    "Epoch #107: Loss:2.4294, Accuracy:0.1893, Validation Loss:2.4534, Validation Accuracy:0.1527\n",
    "Epoch #108: Loss:2.4283, Accuracy:0.1893, Validation Loss:2.4542, Validation Accuracy:0.1511\n",
    "Epoch #109: Loss:2.4290, Accuracy:0.1897, Validation Loss:2.4515, Validation Accuracy:0.1544\n",
    "Epoch #110: Loss:2.4287, Accuracy:0.1947, Validation Loss:2.4562, Validation Accuracy:0.1544\n",
    "Epoch #111: Loss:2.4288, Accuracy:0.1926, Validation Loss:2.4567, Validation Accuracy:0.1511\n",
    "Epoch #112: Loss:2.4286, Accuracy:0.1889, Validation Loss:2.4566, Validation Accuracy:0.1478\n",
    "Epoch #113: Loss:2.4282, Accuracy:0.1844, Validation Loss:2.4565, Validation Accuracy:0.1560\n",
    "Epoch #114: Loss:2.4269, Accuracy:0.1922, Validation Loss:2.4572, Validation Accuracy:0.1494\n",
    "Epoch #115: Loss:2.4270, Accuracy:0.1918, Validation Loss:2.4583, Validation Accuracy:0.1478\n",
    "Epoch #116: Loss:2.4274, Accuracy:0.1918, Validation Loss:2.4584, Validation Accuracy:0.1511\n",
    "Epoch #117: Loss:2.4260, Accuracy:0.1930, Validation Loss:2.4579, Validation Accuracy:0.1478\n",
    "Epoch #118: Loss:2.4262, Accuracy:0.1881, Validation Loss:2.4568, Validation Accuracy:0.1527\n",
    "Epoch #119: Loss:2.4255, Accuracy:0.1934, Validation Loss:2.4574, Validation Accuracy:0.1560\n",
    "Epoch #120: Loss:2.4246, Accuracy:0.1889, Validation Loss:2.4544, Validation Accuracy:0.1593\n",
    "Epoch #121: Loss:2.4242, Accuracy:0.1938, Validation Loss:2.4544, Validation Accuracy:0.1494\n",
    "Epoch #122: Loss:2.4238, Accuracy:0.1934, Validation Loss:2.4563, Validation Accuracy:0.1511\n",
    "Epoch #123: Loss:2.4240, Accuracy:0.1934, Validation Loss:2.4563, Validation Accuracy:0.1527\n",
    "Epoch #124: Loss:2.4240, Accuracy:0.1930, Validation Loss:2.4561, Validation Accuracy:0.1560\n",
    "Epoch #125: Loss:2.4227, Accuracy:0.1951, Validation Loss:2.4577, Validation Accuracy:0.1511\n",
    "Epoch #126: Loss:2.4226, Accuracy:0.1930, Validation Loss:2.4582, Validation Accuracy:0.1494\n",
    "Epoch #127: Loss:2.4220, Accuracy:0.1934, Validation Loss:2.4586, Validation Accuracy:0.1511\n",
    "Epoch #128: Loss:2.4239, Accuracy:0.1910, Validation Loss:2.4558, Validation Accuracy:0.1560\n",
    "Epoch #129: Loss:2.4230, Accuracy:0.1930, Validation Loss:2.4533, Validation Accuracy:0.1576\n",
    "Epoch #130: Loss:2.4201, Accuracy:0.1943, Validation Loss:2.4539, Validation Accuracy:0.1461\n",
    "Epoch #131: Loss:2.4220, Accuracy:0.1951, Validation Loss:2.4529, Validation Accuracy:0.1560\n",
    "Epoch #132: Loss:2.4208, Accuracy:0.1984, Validation Loss:2.4528, Validation Accuracy:0.1560\n",
    "Epoch #133: Loss:2.4214, Accuracy:0.1959, Validation Loss:2.4550, Validation Accuracy:0.1544\n",
    "Epoch #134: Loss:2.4200, Accuracy:0.1955, Validation Loss:2.4563, Validation Accuracy:0.1511\n",
    "Epoch #135: Loss:2.4198, Accuracy:0.1971, Validation Loss:2.4569, Validation Accuracy:0.1494\n",
    "Epoch #136: Loss:2.4211, Accuracy:0.1967, Validation Loss:2.4541, Validation Accuracy:0.1593\n",
    "Epoch #137: Loss:2.4186, Accuracy:0.1943, Validation Loss:2.4531, Validation Accuracy:0.1527\n",
    "Epoch #138: Loss:2.4185, Accuracy:0.1922, Validation Loss:2.4510, Validation Accuracy:0.1609\n",
    "Epoch #139: Loss:2.4189, Accuracy:0.1955, Validation Loss:2.4499, Validation Accuracy:0.1560\n",
    "Epoch #140: Loss:2.4190, Accuracy:0.1955, Validation Loss:2.4532, Validation Accuracy:0.1494\n",
    "Epoch #141: Loss:2.4217, Accuracy:0.1938, Validation Loss:2.4548, Validation Accuracy:0.1560\n",
    "Epoch #142: Loss:2.4195, Accuracy:0.1934, Validation Loss:2.4562, Validation Accuracy:0.1478\n",
    "Epoch #143: Loss:2.4188, Accuracy:0.1959, Validation Loss:2.4528, Validation Accuracy:0.1609\n",
    "Epoch #144: Loss:2.4178, Accuracy:0.1971, Validation Loss:2.4529, Validation Accuracy:0.1527\n",
    "Epoch #145: Loss:2.4168, Accuracy:0.1967, Validation Loss:2.4520, Validation Accuracy:0.1544\n",
    "Epoch #146: Loss:2.4173, Accuracy:0.1975, Validation Loss:2.4529, Validation Accuracy:0.1560\n",
    "Epoch #147: Loss:2.4189, Accuracy:0.1926, Validation Loss:2.4535, Validation Accuracy:0.1576\n",
    "Epoch #148: Loss:2.4174, Accuracy:0.1951, Validation Loss:2.4522, Validation Accuracy:0.1527\n",
    "Epoch #149: Loss:2.4153, Accuracy:0.1959, Validation Loss:2.4537, Validation Accuracy:0.1511\n",
    "Epoch #150: Loss:2.4157, Accuracy:0.1996, Validation Loss:2.4550, Validation Accuracy:0.1560\n",
    "Epoch #151: Loss:2.4161, Accuracy:0.1943, Validation Loss:2.4558, Validation Accuracy:0.1560\n",
    "Epoch #152: Loss:2.4172, Accuracy:0.1975, Validation Loss:2.4573, Validation Accuracy:0.1626\n",
    "Epoch #153: Loss:2.4188, Accuracy:0.1930, Validation Loss:2.4538, Validation Accuracy:0.1576\n",
    "Epoch #154: Loss:2.4167, Accuracy:0.1979, Validation Loss:2.4513, Validation Accuracy:0.1527\n",
    "Epoch #155: Loss:2.4154, Accuracy:0.1930, Validation Loss:2.4553, Validation Accuracy:0.1494\n",
    "Epoch #156: Loss:2.4155, Accuracy:0.1992, Validation Loss:2.4545, Validation Accuracy:0.1626\n",
    "Epoch #157: Loss:2.4145, Accuracy:0.2008, Validation Loss:2.4561, Validation Accuracy:0.1511\n",
    "Epoch #158: Loss:2.4148, Accuracy:0.1984, Validation Loss:2.4547, Validation Accuracy:0.1609\n",
    "Epoch #159: Loss:2.4144, Accuracy:0.2000, Validation Loss:2.4562, Validation Accuracy:0.1544\n",
    "Epoch #160: Loss:2.4145, Accuracy:0.1955, Validation Loss:2.4549, Validation Accuracy:0.1626\n",
    "Epoch #161: Loss:2.4166, Accuracy:0.1947, Validation Loss:2.4533, Validation Accuracy:0.1609\n",
    "Epoch #162: Loss:2.4162, Accuracy:0.1906, Validation Loss:2.4533, Validation Accuracy:0.1576\n",
    "Epoch #163: Loss:2.4153, Accuracy:0.1955, Validation Loss:2.4520, Validation Accuracy:0.1511\n",
    "Epoch #164: Loss:2.4134, Accuracy:0.1967, Validation Loss:2.4534, Validation Accuracy:0.1511\n",
    "Epoch #165: Loss:2.4144, Accuracy:0.1934, Validation Loss:2.4570, Validation Accuracy:0.1527\n",
    "Epoch #166: Loss:2.4140, Accuracy:0.1959, Validation Loss:2.4563, Validation Accuracy:0.1560\n",
    "Epoch #167: Loss:2.4156, Accuracy:0.1934, Validation Loss:2.4574, Validation Accuracy:0.1511\n",
    "Epoch #168: Loss:2.4170, Accuracy:0.1959, Validation Loss:2.4552, Validation Accuracy:0.1527\n",
    "Epoch #169: Loss:2.4136, Accuracy:0.1992, Validation Loss:2.4531, Validation Accuracy:0.1511\n",
    "Epoch #170: Loss:2.4130, Accuracy:0.1979, Validation Loss:2.4525, Validation Accuracy:0.1576\n",
    "Epoch #171: Loss:2.4145, Accuracy:0.1897, Validation Loss:2.4532, Validation Accuracy:0.1576\n",
    "Epoch #172: Loss:2.4143, Accuracy:0.1901, Validation Loss:2.4603, Validation Accuracy:0.1527\n",
    "Epoch #173: Loss:2.4200, Accuracy:0.1877, Validation Loss:2.4589, Validation Accuracy:0.1511\n",
    "Epoch #174: Loss:2.4194, Accuracy:0.1951, Validation Loss:2.4597, Validation Accuracy:0.1527\n",
    "Epoch #175: Loss:2.4201, Accuracy:0.1893, Validation Loss:2.4450, Validation Accuracy:0.1544\n",
    "Epoch #176: Loss:2.4273, Accuracy:0.1947, Validation Loss:2.4653, Validation Accuracy:0.1544\n",
    "Epoch #177: Loss:2.4385, Accuracy:0.1869, Validation Loss:2.4653, Validation Accuracy:0.1741\n",
    "Epoch #178: Loss:2.4288, Accuracy:0.1897, Validation Loss:2.4601, Validation Accuracy:0.1478\n",
    "Epoch #179: Loss:2.4213, Accuracy:0.1947, Validation Loss:2.4653, Validation Accuracy:0.1494\n",
    "Epoch #180: Loss:2.4227, Accuracy:0.2016, Validation Loss:2.4599, Validation Accuracy:0.1445\n",
    "Epoch #181: Loss:2.4253, Accuracy:0.1885, Validation Loss:2.4634, Validation Accuracy:0.1412\n",
    "Epoch #182: Loss:2.4229, Accuracy:0.1947, Validation Loss:2.4644, Validation Accuracy:0.1461\n",
    "Epoch #183: Loss:2.4195, Accuracy:0.1955, Validation Loss:2.4671, Validation Accuracy:0.1461\n",
    "Epoch #184: Loss:2.4203, Accuracy:0.1951, Validation Loss:2.4681, Validation Accuracy:0.1445\n",
    "Epoch #185: Loss:2.4210, Accuracy:0.1967, Validation Loss:2.4676, Validation Accuracy:0.1412\n",
    "Epoch #186: Loss:2.4194, Accuracy:0.1959, Validation Loss:2.4659, Validation Accuracy:0.1461\n",
    "Epoch #187: Loss:2.4234, Accuracy:0.1996, Validation Loss:2.4661, Validation Accuracy:0.1494\n",
    "Epoch #188: Loss:2.4233, Accuracy:0.1922, Validation Loss:2.4687, Validation Accuracy:0.1461\n",
    "Epoch #189: Loss:2.4233, Accuracy:0.1959, Validation Loss:2.4648, Validation Accuracy:0.1478\n",
    "Epoch #190: Loss:2.4206, Accuracy:0.1979, Validation Loss:2.4648, Validation Accuracy:0.1494\n",
    "Epoch #191: Loss:2.4202, Accuracy:0.1996, Validation Loss:2.4637, Validation Accuracy:0.1478\n",
    "Epoch #192: Loss:2.4198, Accuracy:0.1971, Validation Loss:2.4667, Validation Accuracy:0.1396\n",
    "Epoch #193: Loss:2.4193, Accuracy:0.1992, Validation Loss:2.4648, Validation Accuracy:0.1494\n",
    "Epoch #194: Loss:2.4203, Accuracy:0.2008, Validation Loss:2.4655, Validation Accuracy:0.1461\n",
    "Epoch #195: Loss:2.4203, Accuracy:0.1996, Validation Loss:2.4655, Validation Accuracy:0.1461\n",
    "Epoch #196: Loss:2.4178, Accuracy:0.1959, Validation Loss:2.4619, Validation Accuracy:0.1478\n",
    "Epoch #197: Loss:2.4180, Accuracy:0.2004, Validation Loss:2.4634, Validation Accuracy:0.1494\n",
    "Epoch #198: Loss:2.4171, Accuracy:0.1975, Validation Loss:2.4641, Validation Accuracy:0.1511\n",
    "Epoch #199: Loss:2.4162, Accuracy:0.2016, Validation Loss:2.4659, Validation Accuracy:0.1494\n",
    "Epoch #200: Loss:2.4155, Accuracy:0.1988, Validation Loss:2.4665, Validation Accuracy:0.1494\n",
    "Epoch #201: Loss:2.4125, Accuracy:0.1996, Validation Loss:2.4705, Validation Accuracy:0.1494\n",
    "Epoch #202: Loss:2.4124, Accuracy:0.2029, Validation Loss:2.4649, Validation Accuracy:0.1478\n",
    "Epoch #203: Loss:2.4132, Accuracy:0.2016, Validation Loss:2.4633, Validation Accuracy:0.1527\n",
    "Epoch #204: Loss:2.4145, Accuracy:0.2029, Validation Loss:2.4625, Validation Accuracy:0.1494\n",
    "Epoch #205: Loss:2.4128, Accuracy:0.2033, Validation Loss:2.4590, Validation Accuracy:0.1478\n",
    "Epoch #206: Loss:2.4131, Accuracy:0.2029, Validation Loss:2.4527, Validation Accuracy:0.1461\n",
    "Epoch #207: Loss:2.4117, Accuracy:0.2000, Validation Loss:2.4574, Validation Accuracy:0.1461\n",
    "Epoch #208: Loss:2.4131, Accuracy:0.2033, Validation Loss:2.4563, Validation Accuracy:0.1478\n",
    "Epoch #209: Loss:2.4155, Accuracy:0.1996, Validation Loss:2.4569, Validation Accuracy:0.1527\n",
    "Epoch #210: Loss:2.4163, Accuracy:0.1988, Validation Loss:2.4607, Validation Accuracy:0.1478\n",
    "Epoch #211: Loss:2.4163, Accuracy:0.1943, Validation Loss:2.4570, Validation Accuracy:0.1544\n",
    "Epoch #212: Loss:2.4177, Accuracy:0.1943, Validation Loss:2.4598, Validation Accuracy:0.1527\n",
    "Epoch #213: Loss:2.4168, Accuracy:0.1963, Validation Loss:2.4504, Validation Accuracy:0.1511\n",
    "Epoch #214: Loss:2.4149, Accuracy:0.2012, Validation Loss:2.4529, Validation Accuracy:0.1527\n",
    "Epoch #215: Loss:2.4217, Accuracy:0.1951, Validation Loss:2.4513, Validation Accuracy:0.1527\n",
    "Epoch #216: Loss:2.4201, Accuracy:0.1934, Validation Loss:2.4544, Validation Accuracy:0.1560\n",
    "Epoch #217: Loss:2.4158, Accuracy:0.1988, Validation Loss:2.4486, Validation Accuracy:0.1544\n",
    "Epoch #218: Loss:2.4160, Accuracy:0.2025, Validation Loss:2.4475, Validation Accuracy:0.1593\n",
    "Epoch #219: Loss:2.4151, Accuracy:0.1979, Validation Loss:2.4491, Validation Accuracy:0.1511\n",
    "Epoch #220: Loss:2.4141, Accuracy:0.1992, Validation Loss:2.4538, Validation Accuracy:0.1544\n",
    "Epoch #221: Loss:2.4247, Accuracy:0.1885, Validation Loss:2.4561, Validation Accuracy:0.1560\n",
    "Epoch #222: Loss:2.4266, Accuracy:0.1951, Validation Loss:2.4567, Validation Accuracy:0.1527\n",
    "Epoch #223: Loss:2.4246, Accuracy:0.1967, Validation Loss:2.4573, Validation Accuracy:0.1461\n",
    "Epoch #224: Loss:2.4224, Accuracy:0.1992, Validation Loss:2.4583, Validation Accuracy:0.1478\n",
    "Epoch #225: Loss:2.4215, Accuracy:0.1984, Validation Loss:2.4614, Validation Accuracy:0.1494\n",
    "Epoch #226: Loss:2.4200, Accuracy:0.2021, Validation Loss:2.4598, Validation Accuracy:0.1494\n",
    "Epoch #227: Loss:2.4222, Accuracy:0.1975, Validation Loss:2.4677, Validation Accuracy:0.1478\n",
    "Epoch #228: Loss:2.4248, Accuracy:0.1951, Validation Loss:2.4654, Validation Accuracy:0.1461\n",
    "Epoch #229: Loss:2.4206, Accuracy:0.1967, Validation Loss:2.4661, Validation Accuracy:0.1527\n",
    "Epoch #230: Loss:2.4200, Accuracy:0.2000, Validation Loss:2.4663, Validation Accuracy:0.1576\n",
    "Epoch #231: Loss:2.4201, Accuracy:0.1955, Validation Loss:2.4648, Validation Accuracy:0.1527\n",
    "Epoch #232: Loss:2.4190, Accuracy:0.1975, Validation Loss:2.4639, Validation Accuracy:0.1445\n",
    "Epoch #233: Loss:2.4177, Accuracy:0.2021, Validation Loss:2.4623, Validation Accuracy:0.1527\n",
    "Epoch #234: Loss:2.4174, Accuracy:0.1996, Validation Loss:2.4629, Validation Accuracy:0.1576\n",
    "Epoch #235: Loss:2.4195, Accuracy:0.1979, Validation Loss:2.4636, Validation Accuracy:0.1576\n",
    "Epoch #236: Loss:2.4203, Accuracy:0.1979, Validation Loss:2.4575, Validation Accuracy:0.1511\n",
    "Epoch #237: Loss:2.4164, Accuracy:0.2012, Validation Loss:2.4638, Validation Accuracy:0.1494\n",
    "Epoch #238: Loss:2.4169, Accuracy:0.2016, Validation Loss:2.4556, Validation Accuracy:0.1494\n",
    "Epoch #239: Loss:2.4148, Accuracy:0.1984, Validation Loss:2.4607, Validation Accuracy:0.1511\n",
    "Epoch #240: Loss:2.4119, Accuracy:0.2033, Validation Loss:2.4529, Validation Accuracy:0.1544\n",
    "Epoch #241: Loss:2.4117, Accuracy:0.2045, Validation Loss:2.4603, Validation Accuracy:0.1461\n",
    "Epoch #242: Loss:2.4138, Accuracy:0.1971, Validation Loss:2.4646, Validation Accuracy:0.1412\n",
    "Epoch #243: Loss:2.4266, Accuracy:0.1922, Validation Loss:2.5181, Validation Accuracy:0.1593\n",
    "Epoch #244: Loss:2.4549, Accuracy:0.1823, Validation Loss:2.4692, Validation Accuracy:0.1527\n",
    "Epoch #245: Loss:2.4295, Accuracy:0.1856, Validation Loss:2.4705, Validation Accuracy:0.1609\n",
    "Epoch #246: Loss:2.4228, Accuracy:0.1992, Validation Loss:2.4556, Validation Accuracy:0.1560\n",
    "Epoch #247: Loss:2.4267, Accuracy:0.1943, Validation Loss:2.4598, Validation Accuracy:0.1511\n",
    "Epoch #248: Loss:2.4267, Accuracy:0.1979, Validation Loss:2.4579, Validation Accuracy:0.1560\n",
    "Epoch #249: Loss:2.4232, Accuracy:0.1959, Validation Loss:2.4552, Validation Accuracy:0.1576\n",
    "Epoch #250: Loss:2.4217, Accuracy:0.1930, Validation Loss:2.4519, Validation Accuracy:0.1527\n",
    "Epoch #251: Loss:2.4215, Accuracy:0.1988, Validation Loss:2.4494, Validation Accuracy:0.1560\n",
    "Epoch #252: Loss:2.4204, Accuracy:0.1984, Validation Loss:2.4482, Validation Accuracy:0.1675\n",
    "Epoch #253: Loss:2.4387, Accuracy:0.1881, Validation Loss:2.4995, Validation Accuracy:0.1494\n",
    "Epoch #254: Loss:2.4518, Accuracy:0.1860, Validation Loss:2.4778, Validation Accuracy:0.1741\n",
    "Epoch #255: Loss:2.4566, Accuracy:0.1729, Validation Loss:2.4667, Validation Accuracy:0.1691\n",
    "Epoch #256: Loss:2.4441, Accuracy:0.1836, Validation Loss:2.4618, Validation Accuracy:0.1642\n",
    "Epoch #257: Loss:2.4324, Accuracy:0.1881, Validation Loss:2.4565, Validation Accuracy:0.1478\n",
    "Epoch #258: Loss:2.4378, Accuracy:0.1906, Validation Loss:2.4561, Validation Accuracy:0.1445\n",
    "Epoch #259: Loss:2.4287, Accuracy:0.1893, Validation Loss:2.4670, Validation Accuracy:0.1527\n",
    "Epoch #260: Loss:2.4263, Accuracy:0.1881, Validation Loss:2.4669, Validation Accuracy:0.1346\n",
    "Epoch #261: Loss:2.4282, Accuracy:0.1893, Validation Loss:2.4666, Validation Accuracy:0.1412\n",
    "Epoch #262: Loss:2.4245, Accuracy:0.1893, Validation Loss:2.4698, Validation Accuracy:0.1527\n",
    "Epoch #263: Loss:2.4236, Accuracy:0.1873, Validation Loss:2.4649, Validation Accuracy:0.1544\n",
    "Epoch #264: Loss:2.4237, Accuracy:0.2000, Validation Loss:2.4672, Validation Accuracy:0.1478\n",
    "Epoch #265: Loss:2.4214, Accuracy:0.1930, Validation Loss:2.4678, Validation Accuracy:0.1478\n",
    "Epoch #266: Loss:2.4211, Accuracy:0.1918, Validation Loss:2.4669, Validation Accuracy:0.1560\n",
    "Epoch #267: Loss:2.4183, Accuracy:0.1955, Validation Loss:2.4652, Validation Accuracy:0.1544\n",
    "Epoch #268: Loss:2.4180, Accuracy:0.1897, Validation Loss:2.4687, Validation Accuracy:0.1609\n",
    "Epoch #269: Loss:2.4191, Accuracy:0.1926, Validation Loss:2.4665, Validation Accuracy:0.1544\n",
    "Epoch #270: Loss:2.4187, Accuracy:0.1897, Validation Loss:2.4694, Validation Accuracy:0.1609\n",
    "Epoch #271: Loss:2.4173, Accuracy:0.1943, Validation Loss:2.4699, Validation Accuracy:0.1494\n",
    "Epoch #272: Loss:2.4173, Accuracy:0.1938, Validation Loss:2.4660, Validation Accuracy:0.1527\n",
    "Epoch #273: Loss:2.4162, Accuracy:0.1963, Validation Loss:2.4660, Validation Accuracy:0.1560\n",
    "Epoch #274: Loss:2.4179, Accuracy:0.1922, Validation Loss:2.4629, Validation Accuracy:0.1527\n",
    "Epoch #275: Loss:2.4132, Accuracy:0.1967, Validation Loss:2.4659, Validation Accuracy:0.1494\n",
    "Epoch #276: Loss:2.4123, Accuracy:0.2033, Validation Loss:2.4762, Validation Accuracy:0.1576\n",
    "Epoch #277: Loss:2.4231, Accuracy:0.1963, Validation Loss:2.5042, Validation Accuracy:0.1478\n",
    "Epoch #278: Loss:2.4453, Accuracy:0.1901, Validation Loss:2.4718, Validation Accuracy:0.1478\n",
    "Epoch #279: Loss:2.4244, Accuracy:0.1823, Validation Loss:2.4750, Validation Accuracy:0.1527\n",
    "Epoch #280: Loss:2.4249, Accuracy:0.1832, Validation Loss:2.4676, Validation Accuracy:0.1412\n",
    "Epoch #281: Loss:2.4156, Accuracy:0.1893, Validation Loss:2.4663, Validation Accuracy:0.1511\n",
    "Epoch #282: Loss:2.4167, Accuracy:0.1906, Validation Loss:2.4695, Validation Accuracy:0.1429\n",
    "Epoch #283: Loss:2.4160, Accuracy:0.1906, Validation Loss:2.4643, Validation Accuracy:0.1461\n",
    "Epoch #284: Loss:2.4158, Accuracy:0.1943, Validation Loss:2.4619, Validation Accuracy:0.1544\n",
    "Epoch #285: Loss:2.4147, Accuracy:0.1914, Validation Loss:2.4653, Validation Accuracy:0.1494\n",
    "Epoch #286: Loss:2.4139, Accuracy:0.1918, Validation Loss:2.4653, Validation Accuracy:0.1445\n",
    "Epoch #287: Loss:2.4144, Accuracy:0.1901, Validation Loss:2.4623, Validation Accuracy:0.1494\n",
    "Epoch #288: Loss:2.4174, Accuracy:0.1943, Validation Loss:2.4664, Validation Accuracy:0.1494\n",
    "Epoch #289: Loss:2.4179, Accuracy:0.1955, Validation Loss:2.4671, Validation Accuracy:0.1461\n",
    "Epoch #290: Loss:2.4160, Accuracy:0.1910, Validation Loss:2.4677, Validation Accuracy:0.1544\n",
    "Epoch #291: Loss:2.4150, Accuracy:0.1918, Validation Loss:2.4660, Validation Accuracy:0.1412\n",
    "Epoch #292: Loss:2.4168, Accuracy:0.1959, Validation Loss:2.4675, Validation Accuracy:0.1544\n",
    "Epoch #293: Loss:2.4153, Accuracy:0.1947, Validation Loss:2.4662, Validation Accuracy:0.1511\n",
    "Epoch #294: Loss:2.4153, Accuracy:0.1922, Validation Loss:2.4636, Validation Accuracy:0.1494\n",
    "Epoch #295: Loss:2.4140, Accuracy:0.1918, Validation Loss:2.4636, Validation Accuracy:0.1511\n",
    "Epoch #296: Loss:2.4133, Accuracy:0.1914, Validation Loss:2.4612, Validation Accuracy:0.1461\n",
    "Epoch #297: Loss:2.4141, Accuracy:0.1906, Validation Loss:2.4599, Validation Accuracy:0.1576\n",
    "Epoch #298: Loss:2.4136, Accuracy:0.1914, Validation Loss:2.4602, Validation Accuracy:0.1511\n",
    "Epoch #299: Loss:2.4141, Accuracy:0.1893, Validation Loss:2.4575, Validation Accuracy:0.1494\n",
    "Epoch #300: Loss:2.4145, Accuracy:0.1930, Validation Loss:2.4578, Validation Accuracy:0.1478\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45783424, Accuracy:0.1478\n",
    "Labels: ['ib', 'ds', 'my', 'by', 'mb', 'yd', 'eg', 'eb', 'eo', 'ek', 'aa', 'sk', 'sg', 'ce', 'ck']\n",
    "Confusion Matrix:\n",
    "      ib  ds  my  by  mb  yd  eg  eb  eo  ek  aa  sk  sg  ce  ck\n",
    "t:ib   1   1   0   5   0  26   2   0   0   0   0   0  19   0   0\n",
    "t:ds   0   7   0   4   0   2  13   2   0   0   1   0   2   0   0\n",
    "t:my   0   2   0   1   0   8   5   0   0   0   0   0   4   0   0\n",
    "t:by   0   1   0   7   0   0   9   6   0   0   0   0  17   0   0\n",
    "t:mb   2   2   0   6   0   6  11   5   0   0   0   0  20   0   0\n",
    "t:yd   3   0   0   5   0  27   4   0   0   0   0   0  23   0   0\n",
    "t:eg   0   7   0   6   0   0  22  11   0   0   1   0   3   0   0\n",
    "t:eb   0   0   0  14   1   8  15   3   0   0   1   0   8   0   0\n",
    "t:eo   0   0   0   7   0   1   3   4   0   0   0   0  19   0   0\n",
    "t:ek   0   1   0  11   0   2  14   5   0   0   0   0  15   0   0\n",
    "t:aa   0   4   0   2   0   4  17   4   0   0   0   0   3   0   0\n",
    "t:sk   2   3   0   9   0   0   8   4   0   0   1   0   6   0   0\n",
    "t:sg   0   0   0  10   0  12   3   3   0   0   0   0  23   0   0\n",
    "t:ce   0   2   0   3   0   4   6   0   0   0   1   0  11   0   0\n",
    "t:ck   1   0   0   4   0   0   8   4   0   0   0   0   6   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ib       0.11      0.02      0.03        54\n",
    "          ds       0.23      0.23      0.23        31\n",
    "          my       0.00      0.00      0.00        20\n",
    "          by       0.07      0.17      0.10        40\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          yd       0.27      0.44      0.33        62\n",
    "          eg       0.16      0.44      0.23        50\n",
    "          eb       0.06      0.06      0.06        50\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          sg       0.13      0.45      0.20        51\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ck       0.00      0.00      0.00        23\n",
    "\n",
    "    accuracy                           0.15       609\n",
    "   macro avg       0.07      0.12      0.08       609\n",
    "weighted avg       0.08      0.15      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 01:07:12 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 42 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7035558967559012, 2.6949638740965494, 2.6877668988332766, 2.6810932225977453, 2.6737622314290266, 2.6673828972188516, 2.661031455241988, 2.6507907837678255, 2.6383787144972577, 2.624897812779118, 2.6132700075265416, 2.602955026579608, 2.583233824308674, 2.5637616191200046, 2.5469827464061416, 2.529871357289833, 2.518618676854276, 2.508964968040855, 2.4996267934933867, 2.489444551013765, 2.485206708140757, 2.4839947971412895, 2.4782787626012794, 2.474530196542223, 2.4678261487550532, 2.469906426807147, 2.4696942538463422, 2.4684618243638714, 2.466641785476008, 2.470472066468989, 2.4716701570207067, 2.475191705528347, 2.4707417311926783, 2.473787794363714, 2.477711986829886, 2.470004068806841, 2.4649242047214353, 2.463902985521138, 2.4638588267985626, 2.46197129626971, 2.459060521744351, 2.4583729644518573, 2.4597414671298123, 2.4609204755823795, 2.457908571255814, 2.460184428883695, 2.4604592178451212, 2.458198349659862, 2.4568955095726475, 2.458427390050027, 2.4625768892479254, 2.4605823017302013, 2.45760924553832, 2.457483216068036, 2.46084870141128, 2.4588951610383534, 2.460902147496667, 2.4566483572003093, 2.4559231943684847, 2.4541585241828257, 2.4534435569750657, 2.4512777751302486, 2.452612602456254, 2.4517648114359436, 2.4566636285171133, 2.453586435083098, 2.4554815241464447, 2.4550031832677783, 2.455061463103897, 2.453949768163496, 2.4536332626060897, 2.4526816068220216, 2.452443515921657, 2.4534771062666167, 2.4524456628633446, 2.453937893039096, 2.452688928112412, 2.452670756623467, 2.4501248999377974, 2.452267182480134, 2.4508288114137446, 2.449678728341665, 2.4513241935441843, 2.4490531285603843, 2.4508464868824276, 2.4517122407264895, 2.451810291052256, 2.4503281699808555, 2.452076066303723, 2.4515908144181977, 2.45171737788346, 2.453121283958698, 2.449773239971969, 2.4510858924126584, 2.450271395039676, 2.451551379045634, 2.451808965460616, 2.4526982847692933, 2.4524141023507453, 2.4516388222893273, 2.4519594252011654, 2.450274616235191, 2.4528707342194807, 2.452383027679619, 2.4540833569512577, 2.4518102064900016, 2.45343535365338, 2.45416085395124, 2.451511823093558, 2.4562255666760975, 2.4566717785958976, 2.456627217028137, 2.456476390851151, 2.4571971443094838, 2.4583136675197306, 2.4584347829834385, 2.4579461341225257, 2.4568400253803273, 2.4573557153711176, 2.4544429097856795, 2.4543937274387906, 2.4563322118154693, 2.4563351624900682, 2.456121343305741, 2.4576956579837894, 2.4582100196424963, 2.4585994147314816, 2.455766075741873, 2.4532601011210473, 2.453867261241418, 2.452860079375394, 2.452776469229086, 2.4549648879196844, 2.4563489340013276, 2.456902528631276, 2.454052388765933, 2.453100566597799, 2.451029852693304, 2.449946198361652, 2.453185912227787, 2.4548435857143307, 2.456207605809805, 2.4528338216208474, 2.452939984050682, 2.451996189032869, 2.452904380601028, 2.4534980827951665, 2.4521504141427024, 2.453709630543375, 2.454966539232602, 2.4557714834197597, 2.457348634065274, 2.453753454540359, 2.4513306840887212, 2.455297521769707, 2.454487524987833, 2.4561291347779273, 2.4546782927364355, 2.4562153311198567, 2.4549326266365488, 2.4532581808531813, 2.4533017679975537, 2.451994699406115, 2.453444500862084, 2.457039244265, 2.456322003272171, 2.457419108874692, 2.455204992654484, 2.453135854113474, 2.4524606327313703, 2.4531589674049217, 2.460334252449875, 2.4588634967803955, 2.45967714618188, 2.4449958644672765, 2.4652634164186926, 2.465285244050676, 2.4601302510999106, 2.4652875926107023, 2.4598903346727243, 2.46344479708053, 2.464401529731813, 2.467082382227204, 2.4681178423375725, 2.4676189023285664, 2.465907989855862, 2.466099159666666, 2.468680447154053, 2.4648363257472345, 2.4647884364981567, 2.4636591588726575, 2.4667200192637826, 2.464767386760618, 2.465547141574678, 2.4654989070297266, 2.461904207865397, 2.4633862702130096, 2.4641019949576344, 2.465888300552744, 2.466482766547618, 2.470458417886192, 2.4649383614607436, 2.4633081311662797, 2.462549313340085, 2.458982761270307, 2.4527333401302593, 2.457359745780431, 2.4562693260769146, 2.4568601402351615, 2.460683252228109, 2.457039261099153, 2.4598472271059535, 2.4503561095846895, 2.452860882325321, 2.4513036245587227, 2.4543985036402107, 2.448649922027964, 2.4475164143322723, 2.449142971649546, 2.453842902614174, 2.456121670984478, 2.456674444655871, 2.4572524217940708, 2.458308293118657, 2.461361689324841, 2.459799457261911, 2.467738346513269, 2.465391021643953, 2.466147361717788, 2.4663480185522824, 2.4647999220880967, 2.4639106384051845, 2.4623482552263734, 2.4629178242926137, 2.4636180228591944, 2.4575241465482414, 2.4638035767184103, 2.455572286458634, 2.460720023498159, 2.4528983077783693, 2.460344672790302, 2.464580748468784, 2.518134115560497, 2.4692069806880355, 2.4705455816046555, 2.455627430052984, 2.4597787344201247, 2.4579469695662826, 2.4552434477312812, 2.451887869482557, 2.4493593112588514, 2.4482486107079264, 2.499500278573122, 2.4777550290175063, 2.466661533111422, 2.4618241207548746, 2.4564812242103913, 2.456064668977985, 2.466989045464151, 2.4668952992005497, 2.466612882802052, 2.4697508988122046, 2.4648773572323552, 2.4672311201862906, 2.46784077215273, 2.466871191519626, 2.4651897579969835, 2.468722952997743, 2.4664505902182294, 2.4693574134156426, 2.469931926633337, 2.4659752148908543, 2.4659994901303195, 2.462873130792076, 2.4658718328366334, 2.476160146136981, 2.504178407744234, 2.4717509394208785, 2.474979441741417, 2.467563313999395, 2.4662941837154193, 2.4695404617265724, 2.46427044492637, 2.4619032013396716, 2.4653071632917682, 2.4652562004396286, 2.4623141574546428, 2.466440623616937, 2.467088227984549, 2.4676710420054166, 2.4659521168675917, 2.4674989719108993, 2.466245267191544, 2.4636340736364106, 2.4636418126486794, 2.4611610387542173, 2.4599086917288395, 2.4602262026374953, 2.4574528472568407, 2.4578343678773527], 'val_acc': [0.07553366123895927, 0.08045977010882546, 0.08045977010882546, 0.11001641955379586, 0.10180623883194916, 0.1034482749567439, 0.10180623883194916, 0.12479474487269453, 0.13793103396892548, 0.12972085314920578, 0.1379310338710525, 0.1494252868424887, 0.1527093589942052, 0.1510673226736645, 0.15599343134166768, 0.15599343134166768, 0.15927750368913016, 0.15435139511899995, 0.16256157574297367, 0.16748768431310387, 0.16748768431310387, 0.16912972034002563, 0.17077175646482037, 0.17405582881228285, 0.17077175646482037, 0.16584564799256318, 0.1625615756451007, 0.16584564769894422, 0.17405582842079093, 0.1592775033955112, 0.17077175626907443, 0.14614121509449823, 0.16420361137840353, 0.14285714175607184, 0.1592775030040193, 0.15763546687922456, 0.16420361147627652, 0.16420361246724044, 0.15599343085230277, 0.16256157554722772, 0.15927750319976525, 0.16091953932456, 0.16091953932456, 0.16748768362799302, 0.16256157544935473, 0.15106732247791854, 0.16420361137840353, 0.15106732257579153, 0.1691297199485337, 0.16091953932456, 0.15599343085230277, 0.16420361147627652, 0.1576354671728435, 0.16091953912881404, 0.1576354670749705, 0.16091953912881404, 0.15106732247791854, 0.160919539226687, 0.15599343085230277, 0.15435139502112696, 0.16091953932456, 0.15927750319976525, 0.1576354670749705, 0.160919539226687, 0.16091953932456, 0.15927750329763823, 0.15927750329763823, 0.16091953932456, 0.16091953932456, 0.1576354670749705, 0.15599343095017576, 0.15599343095017576, 0.15927750329763823, 0.15927750319976525, 0.15599343085230277, 0.15106732247791854, 0.16584564799256318, 0.14942528645099679, 0.15599343104804875, 0.15106732257579153, 0.15435139482538102, 0.15927750319976525, 0.15927750319976525, 0.15599343095017576, 0.15927750319976525, 0.1576354670749705, 0.14942528645099679, 0.16256157544935473, 0.15599343085230277, 0.15927750319976525, 0.16256157554722772, 0.15270935879845923, 0.16420361167202246, 0.15927750329763823, 0.1527093585048403, 0.1527093585048403, 0.15270935860271329, 0.15435139472750803, 0.15435139462963504, 0.15106732238004555, 0.15270935879845923, 0.15270935870058627, 0.14778325032620204, 0.15270935870058627, 0.14778325013045607, 0.15599343095017576, 0.15270935860271329, 0.15106732247791854, 0.15435139571847195, 0.15435139571847195, 0.15106732337100948, 0.14778325121929298, 0.1559934317453937, 0.14942528724621473, 0.14778325112142, 0.15106732337100948, 0.14778325112142, 0.15270935949580422, 0.1559934318432667, 0.1592775030040193, 0.14942528734408772, 0.15106732346888246, 0.1527093585048403, 0.1559934318432667, 0.15106732238004555, 0.1494252862552508, 0.15106732346888246, 0.1559934318432667, 0.15763546687922456, 0.14614121509449823, 0.15599343075442978, 0.15599343075442978, 0.15435139462963504, 0.15106732238004555, 0.14942528734408772, 0.1592775030040193, 0.1527093585048403, 0.160919539226687, 0.15599343095017576, 0.1494252862552508, 0.1559934318432667, 0.14778325112142, 0.160919539226687, 0.1527093585048403, 0.15435139472750803, 0.15599343095017576, 0.15763546697709752, 0.15270935870058627, 0.15106732238004555, 0.1559934318432667, 0.15599343095017576, 0.16256157544935473, 0.1576354670749705, 0.1527093595936772, 0.1494252863531238, 0.1625615752536088, 0.15106732247791854, 0.16091954021765092, 0.15435139462963504, 0.16256157535148175, 0.160919539226687, 0.15763546697709752, 0.15106732337100948, 0.15106732346888246, 0.1527093585048403, 0.15599343085230277, 0.15106732238004555, 0.1527093595936772, 0.15106732346888246, 0.15763546687922456, 0.15763546697709752, 0.1527093585048403, 0.15106732247791854, 0.1527093595936772, 0.15435139571847195, 0.15435139571847195, 0.17405582842079093, 0.14778325013045607, 0.14942528705046879, 0.14449917867608453, 0.14121510662224102, 0.14614121489875226, 0.14614121509449823, 0.14449917867608453, 0.14121510642649504, 0.14614121489875226, 0.14942528705046879, 0.14614121499662525, 0.14778325092567404, 0.14942528724621473, 0.14778325092567404, 0.13957307039957328, 0.14942528734408772, 0.14614121509449823, 0.14614121499662525, 0.147783251023547, 0.1494252869525958, 0.15106732307739054, 0.14942528705046879, 0.14942528705046879, 0.14942528705046879, 0.14778325092567404, 0.15270935930005827, 0.14942528705046879, 0.14778325092567404, 0.1461412148008793, 0.1461412148008793, 0.14778325112142, 0.15270935939793126, 0.147783251023547, 0.154351395522726, 0.15270935949580422, 0.15106732346888246, 0.15270935949580422, 0.15270935949580422, 0.1559934318432667, 0.15435139571847195, 0.15927750409285618, 0.15106732346888246, 0.15435139571847195, 0.1559934317453937, 0.15270935949580422, 0.14614121489875226, 0.147783251023547, 0.14942528724621473, 0.14942528734408772, 0.14778325112142, 0.14614121489875226, 0.15270935949580422, 0.1576354677723155, 0.15270935949580422, 0.14449917867608453, 0.15270935949580422, 0.15763546787018845, 0.15763546787018845, 0.1510673232731365, 0.14942528724621473, 0.14942528724621473, 0.1510673232731365, 0.15435139562059896, 0.14614121509449823, 0.14121510662224102, 0.15927750319976525, 0.15270935939793126, 0.160919539226687, 0.15599343085230277, 0.15106732337100948, 0.15599343075442978, 0.1576354670749705, 0.15270935949580422, 0.1559934318432667, 0.167487683725866, 0.14942528654886975, 0.17405582842079093, 0.1691297199485337, 0.16420361157414948, 0.14778325112142, 0.14449917797873957, 0.15270935870058627, 0.134646962220935, 0.14121510672011398, 0.15270935860271329, 0.15435139472750803, 0.14778325013045607, 0.14778325013045607, 0.1559934318432667, 0.15435139571847195, 0.16091953912881404, 0.15435139571847195, 0.16091953912881404, 0.1494252862552508, 0.1527093585048403, 0.15599343075442978, 0.1527093595936772, 0.1494252862552508, 0.15763546687922456, 0.147783251023547, 0.14778325022832905, 0.1527093585048403, 0.1412151056312771, 0.15106732238004555, 0.14285714175607184, 0.1461412141035343, 0.15435139462963504, 0.1494252862552508, 0.14449917788086658, 0.1494252862552508, 0.1494252862552508, 0.14614121400566132, 0.15435139472750803, 0.14121510672011398, 0.15435139472750803, 0.15106732346888246, 0.14942528734408772, 0.15106732346888246, 0.14614121509449823, 0.15763546687922456, 0.15106732346888246, 0.14942528724621473, 0.14778325121929298], 'loss': [2.709980075609023, 2.700970480819013, 2.6926601755545616, 2.685503911286654, 2.679429259094614, 2.672526661620248, 2.665121268051116, 2.657607066117273, 2.647622603410568, 2.635159209817342, 2.6227606071339005, 2.6113069616548823, 2.59895889568133, 2.580066054459715, 2.5581105889236166, 2.5425838216863865, 2.5307411474858466, 2.518434008631618, 2.5068531510276717, 2.496952242038578, 2.491430391570136, 2.487318614669894, 2.4831881438926993, 2.4779916402006053, 2.4750745642111776, 2.4720087109160374, 2.4687573101975833, 2.4667044454531504, 2.464035387646246, 2.463760027699402, 2.463815641305285, 2.463624694557895, 2.4621346540274804, 2.4611882430082472, 2.462210862592505, 2.45964249477739, 2.4568975065767886, 2.4575604521028804, 2.4552832516311867, 2.4534670478264653, 2.4527942468498276, 2.4512613469815108, 2.4515611027790047, 2.451549676899058, 2.4502888784271493, 2.4490605978016, 2.447419626267294, 2.4468849406350075, 2.4457980062927307, 2.4471735110273105, 2.447944452140855, 2.4471258627560593, 2.445394009486361, 2.4441523572502684, 2.442601994763165, 2.441773355765999, 2.441021645974819, 2.443423484238266, 2.443494617082255, 2.4436231670927953, 2.4428547764216115, 2.443163051056911, 2.442932117744148, 2.443646204202327, 2.4447308829188104, 2.4438479697679836, 2.441106222983014, 2.440019647394607, 2.4394579972574597, 2.4393993388455995, 2.4396477459392507, 2.4386007379457446, 2.4383524512852977, 2.4382715521896645, 2.4368103531596597, 2.438393663821524, 2.436208813391182, 2.4371662470839106, 2.4364959469076544, 2.4360803833242803, 2.4352249573388383, 2.435426871047128, 2.435083285983828, 2.4358159589082065, 2.4372224138013148, 2.436270662989215, 2.4337805718611887, 2.4331374787207256, 2.4324474811553953, 2.432081869250695, 2.432169397166133, 2.432187046015777, 2.4332941057256114, 2.431738675448439, 2.430917394234659, 2.430927742090558, 2.4309452833336236, 2.431124819181783, 2.4310311385003938, 2.431265623417723, 2.430900672720688, 2.431468349805358, 2.429361659839168, 2.430938137140607, 2.4305624100217096, 2.430544334760192, 2.4293681442370403, 2.4283293037688707, 2.4290090268152693, 2.4287168421539684, 2.4287805900926216, 2.4285536953066407, 2.4281760906047154, 2.4269406861103535, 2.427010941260649, 2.427437906637329, 2.4259968774274636, 2.4262020886556326, 2.425548507937171, 2.4246453618122077, 2.4241778106415297, 2.4237503981932966, 2.423988083402724, 2.4240230733609054, 2.4227158828437694, 2.4225641093949273, 2.422013980015592, 2.4239296369728858, 2.4230033028541893, 2.4201188760126886, 2.4220066949816945, 2.4207959750839327, 2.421350629912265, 2.4199669283763092, 2.419770903753794, 2.4211256141780093, 2.4185888474482042, 2.41850982103994, 2.418947930600364, 2.419041454914414, 2.42166142091614, 2.419503882288688, 2.4187908007134156, 2.41779417893725, 2.416757964892064, 2.417290004518732, 2.418941333602341, 2.417408424432273, 2.415332063659261, 2.4156734162042763, 2.4161486100856773, 2.4172219434068434, 2.418762537586126, 2.416679134114322, 2.415387432775948, 2.4155011754750717, 2.414522444198264, 2.414777130659601, 2.414424815755605, 2.414484496087264, 2.4165809750801728, 2.4162327613673904, 2.4153119311440405, 2.4133761647790366, 2.4143694895248884, 2.4140131171968684, 2.4156331162188334, 2.417011638833267, 2.413627227180058, 2.4129725577650127, 2.414547512222854, 2.4143006689005073, 2.4199975143957433, 2.4194297972645846, 2.4201094129002314, 2.4272822022682834, 2.4385196872805177, 2.428780558075014, 2.4212659311979947, 2.4226745293125727, 2.4252820918692213, 2.422890803750291, 2.419454472167781, 2.4203242399854092, 2.420955366079812, 2.419438713531964, 2.4233720194877297, 2.4233351887618735, 2.4233003895385554, 2.4206194132505257, 2.4201688790957787, 2.4198136235654233, 2.419292959393417, 2.42034671164636, 2.4203187322714492, 2.4177944624938026, 2.4180401795209066, 2.4170785998906443, 2.416219371251257, 2.4155451294822616, 2.4124552389924285, 2.4124476849910415, 2.413190797515963, 2.4145283760720946, 2.412808917141548, 2.4130888886030695, 2.4117191849303197, 2.41305063696123, 2.4154650159440245, 2.4163386730687573, 2.416339089493487, 2.4177304178782313, 2.4167630430609295, 2.4149139820427865, 2.4217136839332034, 2.420087019765646, 2.4158410447334115, 2.415959209191481, 2.415079655833313, 2.4141365696517347, 2.4246788749459833, 2.426619381679402, 2.424605673098711, 2.422425991798573, 2.4215431741130917, 2.4200279372918287, 2.4221690043776434, 2.42480124467697, 2.4206138161418376, 2.4199972938952747, 2.420115068071432, 2.4189578328534074, 2.4177211427590684, 2.4173967625815767, 2.419464769010916, 2.42027715442117, 2.4164253353338223, 2.4168592061350234, 2.4147504280724807, 2.4119239777754955, 2.4117245130715186, 2.413766945167244, 2.4265927615351743, 2.454870520969681, 2.4295381717368563, 2.422824501354836, 2.426730035022054, 2.42672196658485, 2.4231722226622656, 2.4217198967199307, 2.4215272443250466, 2.4204494671165575, 2.438682467443008, 2.4517568147647553, 2.456556037221356, 2.4441211175624837, 2.4324106326093418, 2.4377566212256587, 2.428689426465201, 2.426310044392423, 2.4282477648106444, 2.4245175719016387, 2.423572694986018, 2.4237224277284843, 2.42144656876519, 2.421103609367073, 2.4182982073672252, 2.4179836758842703, 2.4191314887217183, 2.418694771486631, 2.417317024246623, 2.4172525494005646, 2.4162461681287635, 2.4179373602113197, 2.413177812956197, 2.412289852283329, 2.4231329047459598, 2.4452880506887573, 2.424392068459513, 2.4249179481725673, 2.4156164376887452, 2.416667138967181, 2.416045156102895, 2.415817795641858, 2.4146732498732923, 2.4138912980316602, 2.4143771021762666, 2.417423858290091, 2.417889087156104, 2.4159768667553974, 2.4149793487799487, 2.4167865092259904, 2.4153117720351327, 2.415272721421792, 2.4139527656704005, 2.4133124153716854, 2.414149556463504, 2.4136443924365345, 2.4140853503891084, 2.4144563492808255], 'acc': [0.05913757654614517, 0.07926078005952022, 0.07967145745881529, 0.10841889090116998, 0.11170431264739261, 0.10225872643429641, 0.10266940407072495, 0.10924024595066262, 0.12731006053071736, 0.1363449681282533, 0.1392197130397115, 0.14332648802709286, 0.1568788496192231, 0.1560574947930949, 0.15975359314153817, 0.16016426995794386, 0.15770020622614717, 0.16221765924160975, 0.15770020622614717, 0.16386037069302076, 0.1630390148694021, 0.16509240295975114, 0.16591375778587936, 0.1704312125821378, 0.17125256564582889, 0.16960985715017182, 0.17043121199465874, 0.17289527650976083, 0.1815195082700228, 0.1794661185947042, 0.18110883006447395, 0.17741273013106118, 0.17987679738773213, 0.18193018369728534, 0.181108830651953, 0.17864476277108554, 0.18809034976748715, 0.18069815303388317, 0.1827515393434364, 0.17823408593632112, 0.1790554417599398, 0.18069815084307592, 0.1778234094932094, 0.18110883024194158, 0.18193018412565548, 0.18357289420628206, 0.18398357298095122, 0.18234086250867199, 0.1856262822966311, 0.1880903495533021, 0.18234086350616244, 0.1823408634878037, 0.18398357221600456, 0.19014373707453083, 0.18685831628908123, 0.18891170361448362, 0.18562628245574003, 0.181519507076706, 0.18685831511412312, 0.18521560505185528, 0.18193018449894946, 0.18110882985028887, 0.18316221656985351, 0.1802874744366816, 0.18151950609757425, 0.180287475219987, 0.18398357258929854, 0.17987679619441532, 0.18685831530994948, 0.18685831671745134, 0.18850102578222874, 0.18562628245574003, 0.18850102736719826, 0.1835728955770665, 0.1880903495533021, 0.1835728941695646, 0.18644763868937012, 0.1852156046602026, 0.18973305885062325, 0.1880903479866913, 0.1880903479866913, 0.19055441487006827, 0.17864476316273825, 0.18644763810189108, 0.18932238299499057, 0.18767967095610052, 0.18357289595036047, 0.18603696128548539, 0.18809034894746432, 0.18891170496690934, 0.1876796705460891, 0.18767967095610052, 0.18891170341865726, 0.18193018530061358, 0.18726899312384565, 0.19014373805366258, 0.1880903487699967, 0.1876796703686215, 0.1901437382311302, 0.1901437372703572, 0.1901437370561721, 0.18809034896582305, 0.18932238121419473, 0.18439425020736835, 0.1913757689128911, 0.1864476374960533, 0.18932238203421756, 0.18932238240751154, 0.1897330590464496, 0.19466119044492866, 0.192607802764591, 0.1889117053952795, 0.18439424961988932, 0.19219712612565293, 0.19178644752845136, 0.19178644672678727, 0.19301848057848717, 0.18809034976748715, 0.19342915800073063, 0.18891170519945313, 0.1938398365979322, 0.19342915897986238, 0.19342915760907795, 0.19301848155761892, 0.19507186827718354, 0.1930184811476075, 0.19342915778654557, 0.1909650917048327, 0.1930184807559548, 0.19425051402017565, 0.1950718684730099, 0.198357289007557, 0.19589322349496446, 0.1954825452894156, 0.1971252559575212, 0.19671457933694186, 0.19425051441182836, 0.19219712493233612, 0.1954825468743851, 0.19548254509358926, 0.19383983462131, 0.19342915800073063, 0.19589322290748543, 0.19712525734666436, 0.19671457835781012, 0.1975359337897761, 0.1926078039579078, 0.19507186927467401, 0.19589322449245491, 0.1995893224492455, 0.1942505122577385, 0.19753593337976466, 0.1930184794035291, 0.19794661158531354, 0.19301848157597762, 0.19917864424369663, 0.20082135412849686, 0.1983572905741678, 0.20000000106480578, 0.19548254550360067, 0.1946611916566042, 0.1905544144784156, 0.19548254628690606, 0.19671458012024726, 0.19342915878403602, 0.19589322370914952, 0.19342915880239475, 0.19589322429662856, 0.19917864504536076, 0.197946612582804, 0.189733059829755, 0.19014373666451942, 0.18767967193523227, 0.19507186866883625, 0.18932238281752295, 0.19466119007163468, 0.18685831728657168, 0.18973305845897057, 0.19466119065911372, 0.20164270934627776, 0.18850102697554555, 0.19466119163824547, 0.19548254650109112, 0.1950718680997159, 0.19671457972859455, 0.1958932232991381, 0.19958932303672453, 0.19219712534234754, 0.19589322470664, 0.19794661099783448, 0.1995893224492455, 0.1971252573650231, 0.19917864387040266, 0.20082135592765143, 0.1995893232509096, 0.19589322449245491, 0.20041067690207973, 0.19753593357559102, 0.20164271077213836, 0.19876796742729094, 0.1995893220759515, 0.202874743393804, 0.20164271036212694, 0.20287474399964178, 0.2032854212260589, 0.2028747430021513, 0.20000000028150036, 0.20328542120770018, 0.19958932188012515, 0.1987679664481592, 0.1942505122577385, 0.1942505122393798, 0.19630390150468696, 0.20123203176492538, 0.1950718694337829, 0.19342915739489286, 0.19876796662562682, 0.20246406638157197, 0.19794661238697764, 0.19917864404787028, 0.1885010275630246, 0.1950718696663267, 0.1967145779477987, 0.19917864543701344, 0.19835728939920969, 0.20205338758854405, 0.19753593359394975, 0.19507186786717212, 0.19671457912275678, 0.19999999947983627, 0.1954825460910797, 0.19753593337976466, 0.2020533881760231, 0.19958932246760422, 0.19794661140784592, 0.1979466125644453, 0.20123203333153616, 0.20164270956046282, 0.198357289007557, 0.2032854214035265, 0.2045174546677474, 0.1971252573650231, 0.19219712536070627, 0.18234086154789894, 0.18562628347158922, 0.19917864504536076, 0.19425051321851156, 0.19794661238697764, 0.19589322308495305, 0.19301847938517036, 0.19876796781894362, 0.19835729039670016, 0.18809034916164938, 0.18603696009216858, 0.1728952767239459, 0.18357289516705508, 0.1880903479866913, 0.19055441406840418, 0.1893223816058474, 0.1880903495533021, 0.18932238201585883, 0.18932238084090075, 0.18726899314220435, 0.19999999946147753, 0.1930184803826608, 0.19178644674514597, 0.19548254587689465, 0.18973306022140768, 0.19260780296041735, 0.18973305941974358, 0.19425051243520616, 0.19383983561880044, 0.1963039015230457, 0.1921971243632158, 0.19671457812526633, 0.20328542159935287, 0.19630390111303428, 0.1901437374478248, 0.1823408627044983, 0.18316221774481162, 0.18932238199750012, 0.19055441584920002, 0.1905544144784156, 0.19425051284521758, 0.19137577108533965, 0.19178644774263645, 0.19014373766200987, 0.1942505126310325, 0.19548254628690606, 0.19096509286143207, 0.19178644694097233, 0.19589322372750825, 0.19466119185243055, 0.1921971247548685, 0.19178644831175676, 0.19137576971455522, 0.19055441563501496, 0.19137576991038155, 0.18932238182003247, 0.1930184811659662]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
