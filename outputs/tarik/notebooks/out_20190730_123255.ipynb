{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf21.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 12:32:55 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nPhases', 'channelMode': '0', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nPhases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001CECCFA5E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001CEC9757EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1176, Accuracy:0.3105, Validation Loss:1.1014, Validation Accuracy:0.3596\n",
    "Epoch #2: Loss:1.0934, Accuracy:0.3745, Validation Loss:1.0861, Validation Accuracy:0.3612\n",
    "Epoch #3: Loss:1.0808, Accuracy:0.3725, Validation Loss:1.0785, Validation Accuracy:0.3580\n",
    "Epoch #4: Loss:1.0756, Accuracy:0.3828, Validation Loss:1.0763, Validation Accuracy:0.3826\n",
    "Epoch #5: Loss:1.0742, Accuracy:0.3984, Validation Loss:1.0765, Validation Accuracy:0.4007\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.4004, Validation Loss:1.0768, Validation Accuracy:0.3957\n",
    "Epoch #7: Loss:1.0746, Accuracy:0.3947, Validation Loss:1.0764, Validation Accuracy:0.3957\n",
    "Epoch #8: Loss:1.0740, Accuracy:0.3947, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3957\n",
    "Epoch #10: Loss:1.0744, Accuracy:0.3873, Validation Loss:1.0749, Validation Accuracy:0.3744\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.4037, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #12: Loss:1.0737, Accuracy:0.3893, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0743, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0754, Validation Accuracy:0.3924\n",
    "Epoch #19: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #20: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #35: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #43: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #45: Loss:1.0731, Accuracy:0.3951, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0732, Accuracy:0.3947, Validation Loss:1.0762, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0730, Accuracy:0.3951, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #49: Loss:1.0730, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #50: Loss:1.0730, Accuracy:0.3947, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #51: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #54: Loss:1.0727, Accuracy:0.3947, Validation Loss:1.0762, Validation Accuracy:0.3941\n",
    "Epoch #55: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0762, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0728, Accuracy:0.3930, Validation Loss:1.0765, Validation Accuracy:0.4072\n",
    "Epoch #57: Loss:1.0726, Accuracy:0.3971, Validation Loss:1.0762, Validation Accuracy:0.3892\n",
    "Epoch #58: Loss:1.0725, Accuracy:0.3959, Validation Loss:1.0761, Validation Accuracy:0.3892\n",
    "Epoch #59: Loss:1.0724, Accuracy:0.3947, Validation Loss:1.0762, Validation Accuracy:0.3892\n",
    "Epoch #60: Loss:1.0724, Accuracy:0.3943, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #61: Loss:1.0723, Accuracy:0.3938, Validation Loss:1.0764, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0723, Accuracy:0.3951, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0724, Accuracy:0.3967, Validation Loss:1.0765, Validation Accuracy:0.3892\n",
    "Epoch #64: Loss:1.0720, Accuracy:0.3951, Validation Loss:1.0766, Validation Accuracy:0.3892\n",
    "Epoch #65: Loss:1.0720, Accuracy:0.3955, Validation Loss:1.0766, Validation Accuracy:0.3924\n",
    "Epoch #66: Loss:1.0719, Accuracy:0.3947, Validation Loss:1.0764, Validation Accuracy:0.3875\n",
    "Epoch #67: Loss:1.0720, Accuracy:0.3959, Validation Loss:1.0772, Validation Accuracy:0.4023\n",
    "Epoch #68: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0763, Validation Accuracy:0.3892\n",
    "Epoch #69: Loss:1.0724, Accuracy:0.3975, Validation Loss:1.0762, Validation Accuracy:0.3908\n",
    "Epoch #70: Loss:1.0719, Accuracy:0.3984, Validation Loss:1.0762, Validation Accuracy:0.3875\n",
    "Epoch #71: Loss:1.0721, Accuracy:0.3963, Validation Loss:1.0760, Validation Accuracy:0.3875\n",
    "Epoch #72: Loss:1.0717, Accuracy:0.3988, Validation Loss:1.0769, Validation Accuracy:0.3924\n",
    "Epoch #73: Loss:1.0718, Accuracy:0.3959, Validation Loss:1.0779, Validation Accuracy:0.4056\n",
    "Epoch #74: Loss:1.0728, Accuracy:0.3975, Validation Loss:1.0767, Validation Accuracy:0.4039\n",
    "Epoch #75: Loss:1.0734, Accuracy:0.3914, Validation Loss:1.0781, Validation Accuracy:0.3957\n",
    "Epoch #76: Loss:1.0727, Accuracy:0.3947, Validation Loss:1.0771, Validation Accuracy:0.3974\n",
    "Epoch #77: Loss:1.0726, Accuracy:0.3938, Validation Loss:1.0777, Validation Accuracy:0.3974\n",
    "Epoch #78: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0774, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0767, Validation Accuracy:0.4072\n",
    "Epoch #80: Loss:1.0725, Accuracy:0.3873, Validation Loss:1.0771, Validation Accuracy:0.4105\n",
    "Epoch #81: Loss:1.0745, Accuracy:0.3951, Validation Loss:1.0776, Validation Accuracy:0.3826\n",
    "Epoch #82: Loss:1.0755, Accuracy:0.3897, Validation Loss:1.0752, Validation Accuracy:0.4023\n",
    "Epoch #83: Loss:1.0735, Accuracy:0.3992, Validation Loss:1.0760, Validation Accuracy:0.3793\n",
    "Epoch #84: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #85: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #86: Loss:1.0735, Accuracy:0.3938, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #87: Loss:1.0729, Accuracy:0.3955, Validation Loss:1.0754, Validation Accuracy:0.4039\n",
    "Epoch #88: Loss:1.0731, Accuracy:0.3930, Validation Loss:1.0753, Validation Accuracy:0.4007\n",
    "Epoch #89: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0753, Validation Accuracy:0.4039\n",
    "Epoch #90: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #91: Loss:1.0729, Accuracy:0.3975, Validation Loss:1.0752, Validation Accuracy:0.4023\n",
    "Epoch #92: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #93: Loss:1.0728, Accuracy:0.3959, Validation Loss:1.0756, Validation Accuracy:0.4039\n",
    "Epoch #94: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0758, Validation Accuracy:0.4056\n",
    "Epoch #95: Loss:1.0724, Accuracy:0.3967, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "Epoch #96: Loss:1.0724, Accuracy:0.3963, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #97: Loss:1.0723, Accuracy:0.3914, Validation Loss:1.0761, Validation Accuracy:0.3908\n",
    "Epoch #98: Loss:1.0722, Accuracy:0.3938, Validation Loss:1.0760, Validation Accuracy:0.3826\n",
    "Epoch #99: Loss:1.0721, Accuracy:0.3959, Validation Loss:1.0761, Validation Accuracy:0.4056\n",
    "Epoch #100: Loss:1.0722, Accuracy:0.3955, Validation Loss:1.0760, Validation Accuracy:0.4072\n",
    "Epoch #101: Loss:1.0724, Accuracy:0.3947, Validation Loss:1.0757, Validation Accuracy:0.4039\n",
    "Epoch #102: Loss:1.0717, Accuracy:0.3947, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #103: Loss:1.0719, Accuracy:0.3955, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #104: Loss:1.0720, Accuracy:0.3947, Validation Loss:1.0762, Validation Accuracy:0.4007\n",
    "Epoch #105: Loss:1.0717, Accuracy:0.3951, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #106: Loss:1.0716, Accuracy:0.3955, Validation Loss:1.0763, Validation Accuracy:0.4007\n",
    "Epoch #107: Loss:1.0721, Accuracy:0.3934, Validation Loss:1.0762, Validation Accuracy:0.4039\n",
    "Epoch #108: Loss:1.0721, Accuracy:0.3951, Validation Loss:1.0766, Validation Accuracy:0.4007\n",
    "Epoch #109: Loss:1.0719, Accuracy:0.3947, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #110: Loss:1.0719, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.3941\n",
    "Epoch #111: Loss:1.0718, Accuracy:0.3955, Validation Loss:1.0759, Validation Accuracy:0.4007\n",
    "Epoch #112: Loss:1.0717, Accuracy:0.3947, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "Epoch #113: Loss:1.0718, Accuracy:0.3922, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #114: Loss:1.0717, Accuracy:0.3930, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #115: Loss:1.0717, Accuracy:0.3930, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #116: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0759, Validation Accuracy:0.4007\n",
    "Epoch #117: Loss:1.0722, Accuracy:0.3926, Validation Loss:1.0760, Validation Accuracy:0.4007\n",
    "Epoch #118: Loss:1.0715, Accuracy:0.3938, Validation Loss:1.0765, Validation Accuracy:0.3990\n",
    "Epoch #119: Loss:1.0720, Accuracy:0.3930, Validation Loss:1.0760, Validation Accuracy:0.4007\n",
    "Epoch #120: Loss:1.0718, Accuracy:0.3943, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #121: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0758, Validation Accuracy:0.4023\n",
    "Epoch #122: Loss:1.0719, Accuracy:0.3938, Validation Loss:1.0758, Validation Accuracy:0.3990\n",
    "Epoch #123: Loss:1.0718, Accuracy:0.3930, Validation Loss:1.0759, Validation Accuracy:0.4007\n",
    "Epoch #124: Loss:1.0717, Accuracy:0.3934, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #125: Loss:1.0720, Accuracy:0.3938, Validation Loss:1.0758, Validation Accuracy:0.3990\n",
    "Epoch #126: Loss:1.0720, Accuracy:0.3971, Validation Loss:1.0763, Validation Accuracy:0.4039\n",
    "Epoch #127: Loss:1.0717, Accuracy:0.3951, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #128: Loss:1.0720, Accuracy:0.3889, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #129: Loss:1.0718, Accuracy:0.3934, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #130: Loss:1.0715, Accuracy:0.3938, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #131: Loss:1.0716, Accuracy:0.3938, Validation Loss:1.0760, Validation Accuracy:0.4007\n",
    "Epoch #132: Loss:1.0718, Accuracy:0.3938, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #133: Loss:1.0715, Accuracy:0.3922, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #134: Loss:1.0718, Accuracy:0.3926, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #135: Loss:1.0714, Accuracy:0.3959, Validation Loss:1.0765, Validation Accuracy:0.4023\n",
    "Epoch #136: Loss:1.0718, Accuracy:0.3934, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #137: Loss:1.0716, Accuracy:0.3934, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #138: Loss:1.0723, Accuracy:0.3963, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #139: Loss:1.0725, Accuracy:0.3955, Validation Loss:1.0763, Validation Accuracy:0.3990\n",
    "Epoch #140: Loss:1.0718, Accuracy:0.3947, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #141: Loss:1.0726, Accuracy:0.3922, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #142: Loss:1.0720, Accuracy:0.3922, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #143: Loss:1.0715, Accuracy:0.3930, Validation Loss:1.0762, Validation Accuracy:0.4007\n",
    "Epoch #144: Loss:1.0713, Accuracy:0.3938, Validation Loss:1.0767, Validation Accuracy:0.4023\n",
    "Epoch #145: Loss:1.0718, Accuracy:0.3934, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #146: Loss:1.0718, Accuracy:0.3897, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #147: Loss:1.0714, Accuracy:0.3930, Validation Loss:1.0760, Validation Accuracy:0.4039\n",
    "Epoch #148: Loss:1.0715, Accuracy:0.3971, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #149: Loss:1.0717, Accuracy:0.3947, Validation Loss:1.0759, Validation Accuracy:0.4007\n",
    "Epoch #150: Loss:1.0715, Accuracy:0.3934, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #151: Loss:1.0726, Accuracy:0.3910, Validation Loss:1.0760, Validation Accuracy:0.4039\n",
    "Epoch #152: Loss:1.0717, Accuracy:0.3947, Validation Loss:1.0770, Validation Accuracy:0.4138\n",
    "Epoch #153: Loss:1.0717, Accuracy:0.3943, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #154: Loss:1.0721, Accuracy:0.3901, Validation Loss:1.0764, Validation Accuracy:0.4007\n",
    "Epoch #155: Loss:1.0720, Accuracy:0.3889, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #156: Loss:1.0715, Accuracy:0.3934, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #157: Loss:1.0713, Accuracy:0.3971, Validation Loss:1.0763, Validation Accuracy:0.4039\n",
    "Epoch #158: Loss:1.0713, Accuracy:0.3938, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #159: Loss:1.0715, Accuracy:0.3906, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #160: Loss:1.0715, Accuracy:0.3943, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #161: Loss:1.0716, Accuracy:0.3963, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #162: Loss:1.0714, Accuracy:0.3918, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #163: Loss:1.0715, Accuracy:0.3926, Validation Loss:1.0761, Validation Accuracy:0.4039\n",
    "Epoch #164: Loss:1.0711, Accuracy:0.3959, Validation Loss:1.0765, Validation Accuracy:0.4056\n",
    "Epoch #165: Loss:1.0713, Accuracy:0.3959, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #166: Loss:1.0712, Accuracy:0.3934, Validation Loss:1.0761, Validation Accuracy:0.4039\n",
    "Epoch #167: Loss:1.0714, Accuracy:0.3922, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #168: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0769, Validation Accuracy:0.4056\n",
    "Epoch #169: Loss:1.0720, Accuracy:0.3897, Validation Loss:1.0763, Validation Accuracy:0.4007\n",
    "Epoch #170: Loss:1.0711, Accuracy:0.3979, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #171: Loss:1.0712, Accuracy:0.3955, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #172: Loss:1.0711, Accuracy:0.3963, Validation Loss:1.0762, Validation Accuracy:0.4056\n",
    "Epoch #173: Loss:1.0712, Accuracy:0.3889, Validation Loss:1.0763, Validation Accuracy:0.3990\n",
    "Epoch #174: Loss:1.0711, Accuracy:0.3938, Validation Loss:1.0765, Validation Accuracy:0.4023\n",
    "Epoch #175: Loss:1.0710, Accuracy:0.3926, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #176: Loss:1.0710, Accuracy:0.3947, Validation Loss:1.0763, Validation Accuracy:0.4039\n",
    "Epoch #177: Loss:1.0713, Accuracy:0.3959, Validation Loss:1.0766, Validation Accuracy:0.4056\n",
    "Epoch #178: Loss:1.0715, Accuracy:0.3947, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #179: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0770, Validation Accuracy:0.4056\n",
    "Epoch #180: Loss:1.0711, Accuracy:0.3943, Validation Loss:1.0764, Validation Accuracy:0.4039\n",
    "Epoch #181: Loss:1.0709, Accuracy:0.3943, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #182: Loss:1.0714, Accuracy:0.3955, Validation Loss:1.0762, Validation Accuracy:0.4039\n",
    "Epoch #183: Loss:1.0709, Accuracy:0.3955, Validation Loss:1.0771, Validation Accuracy:0.4056\n",
    "Epoch #184: Loss:1.0714, Accuracy:0.3963, Validation Loss:1.0767, Validation Accuracy:0.4039\n",
    "Epoch #185: Loss:1.0712, Accuracy:0.3959, Validation Loss:1.0765, Validation Accuracy:0.4007\n",
    "Epoch #186: Loss:1.0718, Accuracy:0.3951, Validation Loss:1.0763, Validation Accuracy:0.4023\n",
    "Epoch #187: Loss:1.0713, Accuracy:0.3930, Validation Loss:1.0764, Validation Accuracy:0.4007\n",
    "Epoch #188: Loss:1.0709, Accuracy:0.3938, Validation Loss:1.0761, Validation Accuracy:0.4039\n",
    "Epoch #189: Loss:1.0708, Accuracy:0.3918, Validation Loss:1.0761, Validation Accuracy:0.4039\n",
    "Epoch #190: Loss:1.0709, Accuracy:0.3959, Validation Loss:1.0762, Validation Accuracy:0.4056\n",
    "Epoch #191: Loss:1.0709, Accuracy:0.3947, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #192: Loss:1.0709, Accuracy:0.3951, Validation Loss:1.0763, Validation Accuracy:0.4039\n",
    "Epoch #193: Loss:1.0710, Accuracy:0.3955, Validation Loss:1.0762, Validation Accuracy:0.4105\n",
    "Epoch #194: Loss:1.0708, Accuracy:0.3938, Validation Loss:1.0765, Validation Accuracy:0.4072\n",
    "Epoch #195: Loss:1.0707, Accuracy:0.3963, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #196: Loss:1.0716, Accuracy:0.3947, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #197: Loss:1.0707, Accuracy:0.3951, Validation Loss:1.0762, Validation Accuracy:0.4039\n",
    "Epoch #198: Loss:1.0713, Accuracy:0.3881, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #199: Loss:1.0708, Accuracy:0.3955, Validation Loss:1.0765, Validation Accuracy:0.4056\n",
    "Epoch #200: Loss:1.0710, Accuracy:0.3975, Validation Loss:1.0766, Validation Accuracy:0.4089\n",
    "Epoch #201: Loss:1.0707, Accuracy:0.3959, Validation Loss:1.0763, Validation Accuracy:0.4007\n",
    "Epoch #202: Loss:1.0712, Accuracy:0.3926, Validation Loss:1.0763, Validation Accuracy:0.4056\n",
    "Epoch #203: Loss:1.0707, Accuracy:0.3947, Validation Loss:1.0767, Validation Accuracy:0.3941\n",
    "Epoch #204: Loss:1.0708, Accuracy:0.3955, Validation Loss:1.0762, Validation Accuracy:0.4089\n",
    "Epoch #205: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0765, Validation Accuracy:0.4072\n",
    "Epoch #206: Loss:1.0705, Accuracy:0.3881, Validation Loss:1.0763, Validation Accuracy:0.4007\n",
    "Epoch #207: Loss:1.0707, Accuracy:0.3926, Validation Loss:1.0762, Validation Accuracy:0.4089\n",
    "Epoch #208: Loss:1.0710, Accuracy:0.3926, Validation Loss:1.0768, Validation Accuracy:0.3957\n",
    "Epoch #209: Loss:1.0706, Accuracy:0.3988, Validation Loss:1.0759, Validation Accuracy:0.4056\n",
    "Epoch #210: Loss:1.0707, Accuracy:0.3951, Validation Loss:1.0760, Validation Accuracy:0.4023\n",
    "Epoch #211: Loss:1.0708, Accuracy:0.3938, Validation Loss:1.0761, Validation Accuracy:0.4007\n",
    "Epoch #212: Loss:1.0705, Accuracy:0.3947, Validation Loss:1.0766, Validation Accuracy:0.4039\n",
    "Epoch #213: Loss:1.0707, Accuracy:0.3959, Validation Loss:1.0762, Validation Accuracy:0.4039\n",
    "Epoch #214: Loss:1.0706, Accuracy:0.3947, Validation Loss:1.0761, Validation Accuracy:0.4056\n",
    "Epoch #215: Loss:1.0704, Accuracy:0.3959, Validation Loss:1.0763, Validation Accuracy:0.4056\n",
    "Epoch #216: Loss:1.0704, Accuracy:0.3947, Validation Loss:1.0765, Validation Accuracy:0.4023\n",
    "Epoch #217: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0768, Validation Accuracy:0.4056\n",
    "Epoch #218: Loss:1.0705, Accuracy:0.3918, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #219: Loss:1.0706, Accuracy:0.3967, Validation Loss:1.0762, Validation Accuracy:0.4039\n",
    "Epoch #220: Loss:1.0709, Accuracy:0.3959, Validation Loss:1.0760, Validation Accuracy:0.4039\n",
    "Epoch #221: Loss:1.0710, Accuracy:0.3955, Validation Loss:1.0767, Validation Accuracy:0.4269\n",
    "Epoch #222: Loss:1.0703, Accuracy:0.3979, Validation Loss:1.0757, Validation Accuracy:0.4039\n",
    "Epoch #223: Loss:1.0702, Accuracy:0.3959, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #224: Loss:1.0704, Accuracy:0.3943, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #225: Loss:1.0702, Accuracy:0.3959, Validation Loss:1.0761, Validation Accuracy:0.4072\n",
    "Epoch #226: Loss:1.0703, Accuracy:0.4000, Validation Loss:1.0760, Validation Accuracy:0.4089\n",
    "Epoch #227: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0762, Validation Accuracy:0.4007\n",
    "Epoch #228: Loss:1.0708, Accuracy:0.3967, Validation Loss:1.0759, Validation Accuracy:0.4023\n",
    "Epoch #229: Loss:1.0704, Accuracy:0.3971, Validation Loss:1.0758, Validation Accuracy:0.3990\n",
    "Epoch #230: Loss:1.0702, Accuracy:0.3967, Validation Loss:1.0760, Validation Accuracy:0.4007\n",
    "Epoch #231: Loss:1.0700, Accuracy:0.3988, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "Epoch #232: Loss:1.0700, Accuracy:0.3979, Validation Loss:1.0756, Validation Accuracy:0.4023\n",
    "Epoch #233: Loss:1.0702, Accuracy:0.3947, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #234: Loss:1.0702, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3908\n",
    "Epoch #235: Loss:1.0702, Accuracy:0.3943, Validation Loss:1.0759, Validation Accuracy:0.3941\n",
    "Epoch #236: Loss:1.0700, Accuracy:0.3971, Validation Loss:1.0757, Validation Accuracy:0.4023\n",
    "Epoch #237: Loss:1.0701, Accuracy:0.3947, Validation Loss:1.0756, Validation Accuracy:0.4023\n",
    "Epoch #238: Loss:1.0702, Accuracy:0.3988, Validation Loss:1.0757, Validation Accuracy:0.4204\n",
    "Epoch #239: Loss:1.0699, Accuracy:0.4057, Validation Loss:1.0763, Validation Accuracy:0.4204\n",
    "Epoch #240: Loss:1.0699, Accuracy:0.4025, Validation Loss:1.0753, Validation Accuracy:0.4039\n",
    "Epoch #241: Loss:1.0699, Accuracy:0.3959, Validation Loss:1.0753, Validation Accuracy:0.4039\n",
    "Epoch #242: Loss:1.0697, Accuracy:0.3955, Validation Loss:1.0755, Validation Accuracy:0.4122\n",
    "Epoch #243: Loss:1.0697, Accuracy:0.3955, Validation Loss:1.0752, Validation Accuracy:0.4171\n",
    "Epoch #244: Loss:1.0696, Accuracy:0.4057, Validation Loss:1.0751, Validation Accuracy:0.4220\n",
    "Epoch #245: Loss:1.0697, Accuracy:0.3938, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #246: Loss:1.0699, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.4171\n",
    "Epoch #247: Loss:1.0696, Accuracy:0.4086, Validation Loss:1.0748, Validation Accuracy:0.4154\n",
    "Epoch #248: Loss:1.0698, Accuracy:0.3988, Validation Loss:1.0746, Validation Accuracy:0.4269\n",
    "Epoch #249: Loss:1.0696, Accuracy:0.3984, Validation Loss:1.0750, Validation Accuracy:0.4138\n",
    "Epoch #250: Loss:1.0695, Accuracy:0.4008, Validation Loss:1.0755, Validation Accuracy:0.3974\n",
    "Epoch #251: Loss:1.0698, Accuracy:0.4025, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #252: Loss:1.0695, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.4171\n",
    "Epoch #253: Loss:1.0694, Accuracy:0.4041, Validation Loss:1.0752, Validation Accuracy:0.4269\n",
    "Epoch #254: Loss:1.0694, Accuracy:0.3979, Validation Loss:1.0750, Validation Accuracy:0.4122\n",
    "Epoch #255: Loss:1.0704, Accuracy:0.3856, Validation Loss:1.0757, Validation Accuracy:0.3974\n",
    "Epoch #256: Loss:1.0698, Accuracy:0.3963, Validation Loss:1.0748, Validation Accuracy:0.4286\n",
    "Epoch #257: Loss:1.0694, Accuracy:0.3992, Validation Loss:1.0747, Validation Accuracy:0.4269\n",
    "Epoch #258: Loss:1.0694, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.4154\n",
    "Epoch #259: Loss:1.0693, Accuracy:0.4016, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #260: Loss:1.0693, Accuracy:0.4025, Validation Loss:1.0753, Validation Accuracy:0.4122\n",
    "Epoch #261: Loss:1.0691, Accuracy:0.4062, Validation Loss:1.0748, Validation Accuracy:0.4122\n",
    "Epoch #262: Loss:1.0695, Accuracy:0.4049, Validation Loss:1.0747, Validation Accuracy:0.4171\n",
    "Epoch #263: Loss:1.0690, Accuracy:0.4053, Validation Loss:1.0753, Validation Accuracy:0.4089\n",
    "Epoch #264: Loss:1.0694, Accuracy:0.4041, Validation Loss:1.0752, Validation Accuracy:0.4138\n",
    "Epoch #265: Loss:1.0692, Accuracy:0.4016, Validation Loss:1.0746, Validation Accuracy:0.4138\n",
    "Epoch #266: Loss:1.0698, Accuracy:0.4033, Validation Loss:1.0745, Validation Accuracy:0.4122\n",
    "Epoch #267: Loss:1.0688, Accuracy:0.4033, Validation Loss:1.0754, Validation Accuracy:0.4138\n",
    "Epoch #268: Loss:1.0693, Accuracy:0.4037, Validation Loss:1.0750, Validation Accuracy:0.4138\n",
    "Epoch #269: Loss:1.0689, Accuracy:0.4045, Validation Loss:1.0744, Validation Accuracy:0.4138\n",
    "Epoch #270: Loss:1.0693, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.4122\n",
    "Epoch #271: Loss:1.0693, Accuracy:0.4004, Validation Loss:1.0743, Validation Accuracy:0.4122\n",
    "Epoch #272: Loss:1.0689, Accuracy:0.4057, Validation Loss:1.0744, Validation Accuracy:0.4072\n",
    "Epoch #273: Loss:1.0691, Accuracy:0.4070, Validation Loss:1.0745, Validation Accuracy:0.4138\n",
    "Epoch #274: Loss:1.0691, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.4122\n",
    "Epoch #275: Loss:1.0689, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.4072\n",
    "Epoch #276: Loss:1.0692, Accuracy:0.3975, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #277: Loss:1.0693, Accuracy:0.3967, Validation Loss:1.0743, Validation Accuracy:0.4286\n",
    "Epoch #278: Loss:1.0692, Accuracy:0.4012, Validation Loss:1.0744, Validation Accuracy:0.4154\n",
    "Epoch #279: Loss:1.0686, Accuracy:0.4045, Validation Loss:1.0743, Validation Accuracy:0.4138\n",
    "Epoch #280: Loss:1.0689, Accuracy:0.4045, Validation Loss:1.0742, Validation Accuracy:0.4154\n",
    "Epoch #281: Loss:1.0691, Accuracy:0.4016, Validation Loss:1.0743, Validation Accuracy:0.4122\n",
    "Epoch #282: Loss:1.0686, Accuracy:0.4025, Validation Loss:1.0740, Validation Accuracy:0.4122\n",
    "Epoch #283: Loss:1.0691, Accuracy:0.4049, Validation Loss:1.0744, Validation Accuracy:0.4056\n",
    "Epoch #284: Loss:1.0687, Accuracy:0.4049, Validation Loss:1.0746, Validation Accuracy:0.4187\n",
    "Epoch #285: Loss:1.0688, Accuracy:0.4021, Validation Loss:1.0742, Validation Accuracy:0.4039\n",
    "Epoch #286: Loss:1.0688, Accuracy:0.4016, Validation Loss:1.0745, Validation Accuracy:0.4056\n",
    "Epoch #287: Loss:1.0688, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.4138\n",
    "Epoch #288: Loss:1.0689, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.4138\n",
    "Epoch #289: Loss:1.0700, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.4039\n",
    "Epoch #290: Loss:1.0689, Accuracy:0.4070, Validation Loss:1.0759, Validation Accuracy:0.4039\n",
    "Epoch #291: Loss:1.0692, Accuracy:0.4070, Validation Loss:1.0742, Validation Accuracy:0.4122\n",
    "Epoch #292: Loss:1.0685, Accuracy:0.4037, Validation Loss:1.0743, Validation Accuracy:0.4138\n",
    "Epoch #293: Loss:1.0686, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.4138\n",
    "Epoch #294: Loss:1.0689, Accuracy:0.4053, Validation Loss:1.0741, Validation Accuracy:0.4138\n",
    "Epoch #295: Loss:1.0689, Accuracy:0.4033, Validation Loss:1.0747, Validation Accuracy:0.3859\n",
    "Epoch #296: Loss:1.0687, Accuracy:0.4086, Validation Loss:1.0741, Validation Accuracy:0.4138\n",
    "Epoch #297: Loss:1.0688, Accuracy:0.4057, Validation Loss:1.0744, Validation Accuracy:0.4187\n",
    "Epoch #298: Loss:1.0686, Accuracy:0.4033, Validation Loss:1.0748, Validation Accuracy:0.4056\n",
    "Epoch #299: Loss:1.0693, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.4056\n",
    "Epoch #300: Loss:1.0684, Accuracy:0.4070, Validation Loss:1.0746, Validation Accuracy:0.4138\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07460153, Accuracy:0.4138\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  03   01\n",
    "t:02  74   0  153\n",
    "t:03  38   0  104\n",
    "t:01  62   0  178\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.43      0.33      0.37       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.41      0.74      0.53       240\n",
    "\n",
    "    accuracy                           0.41       609\n",
    "   macro avg       0.28      0.36      0.30       609\n",
    "weighted avg       0.32      0.41      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 12:48:30 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.1014080769910013, 1.08608604495357, 1.078494073330671, 1.076251228063173, 1.076481526904114, 1.0767723347361648, 1.0763748496624048, 1.0757628358252138, 1.0755464993478434, 1.0748936853972562, 1.074809404233798, 1.0747708624415406, 1.074849390239747, 1.0755597372556163, 1.0752819140360665, 1.0754648640825244, 1.0754441610110805, 1.075356433152761, 1.075301643662852, 1.0753806831409973, 1.0754594411364526, 1.0754938887062135, 1.0754924339222398, 1.0754859987738097, 1.0755083498304896, 1.0755639134956698, 1.0755943151921865, 1.0756237054693287, 1.0756140052782883, 1.0757072817515858, 1.075712775166203, 1.075754419531924, 1.0756706361504416, 1.075784118108953, 1.0758151904311282, 1.0757898726486808, 1.0758368272108005, 1.075953614731336, 1.0759619634922697, 1.0761275559614836, 1.0760721911741986, 1.0758313506303359, 1.07585923934022, 1.0758910461012365, 1.0759650081249292, 1.0762030841486012, 1.0760425679593641, 1.0761249106505821, 1.0761264165242512, 1.0761065375432983, 1.0761059326882825, 1.076083393323989, 1.0761000802755747, 1.076224386594174, 1.0761698394377635, 1.0764538071229932, 1.076196069396384, 1.0761478148853445, 1.0762132257467811, 1.0764881059258247, 1.0763705452087478, 1.0765403808631333, 1.076493896091317, 1.076625884264365, 1.0766453022635825, 1.0763729315477444, 1.077240262712751, 1.076269191474163, 1.0761585286489652, 1.076187253781336, 1.0760217891342339, 1.076871326795744, 1.077945931595926, 1.0767142890122137, 1.078084014906672, 1.0771100472151156, 1.077738680471536, 1.0773700309308682, 1.0766732191608848, 1.0771286256599113, 1.0775537046501398, 1.0752475533775117, 1.0759988159968936, 1.0755693569950673, 1.0755932268446498, 1.075029335977213, 1.0753711113593065, 1.0752965660126534, 1.075262316537804, 1.0753128599063517, 1.0752420609416242, 1.075230506449106, 1.075572708948884, 1.0757792191748157, 1.0758338951320678, 1.0759629081622721, 1.0760642744245983, 1.076038482545436, 1.0760803596531032, 1.0759972707782863, 1.0757353272539838, 1.076264037678786, 1.0763771091580194, 1.076153171082044, 1.076244563108986, 1.0763049785335272, 1.0762409566854216, 1.0765817423759423, 1.0762979015340945, 1.0761212171200656, 1.075882053531841, 1.0757822422754197, 1.075688825060777, 1.0758537995599957, 1.0759212532262692, 1.0758506159477046, 1.0759577492775, 1.0765014843791967, 1.0759541387432705, 1.0762440107138873, 1.0758333163112646, 1.0757946212499208, 1.0758961707304655, 1.0759021977485694, 1.0758300101620026, 1.0762512108375286, 1.0758763256135637, 1.0760553897112266, 1.0761448287807271, 1.0759563974559014, 1.0760077868384876, 1.076114687034845, 1.0760018557359041, 1.0760712112699236, 1.0765209299785945, 1.076012753304981, 1.0762165499046714, 1.0762797480537778, 1.0763156237860618, 1.0762117048006732, 1.0760890051649121, 1.075999831722679, 1.0761918136834707, 1.0766841186874214, 1.0763721495426346, 1.0760060456781748, 1.0760448511402398, 1.0760975939103927, 1.0759388731030994, 1.0761738383319774, 1.0760443504220747, 1.0770454694484841, 1.0762500553491277, 1.0763641006645115, 1.076385771699727, 1.0761051191680733, 1.0762845420680807, 1.0760111382050663, 1.0759943103163896, 1.0763481467815454, 1.0763302630391614, 1.0761348443665528, 1.0761317672400639, 1.0765004815726444, 1.0763768615393803, 1.0760542197376246, 1.0760139058571927, 1.0769005759400492, 1.0762693234069398, 1.0762925932951553, 1.0763344555261296, 1.076225225561358, 1.0763457694468632, 1.0765208661654118, 1.0763122070403326, 1.076340065213847, 1.0765655633851225, 1.0764398651170026, 1.0769793768038696, 1.0764205042756054, 1.0762117417966595, 1.0761747505081503, 1.07709187866236, 1.0767395106833948, 1.0764510014961506, 1.0762549844281426, 1.0763506742533793, 1.0760639312819307, 1.0761015509149712, 1.0761960077364066, 1.0760033316604414, 1.0762906618697694, 1.0762024176336078, 1.0765457623110617, 1.0764292642988007, 1.0764230730498365, 1.0762400100579599, 1.0760869453301767, 1.0765091129907443, 1.0766220640862125, 1.076322298136055, 1.0762746494587614, 1.076668093944418, 1.0762258486207483, 1.076537690726407, 1.0763006022411028, 1.0761899061390918, 1.0767993292785043, 1.075870020244705, 1.0760016114449462, 1.0760874566186238, 1.0765823718949492, 1.076176155768396, 1.0761122652658297, 1.0762737468741406, 1.0765071956590675, 1.0768345498490608, 1.0761452952433495, 1.0762184077295764, 1.0760025584639, 1.076695306156265, 1.0757155897973598, 1.075868054172284, 1.0760894590997931, 1.0760691299031324, 1.0760106247634136, 1.0762127440159739, 1.0758794685106952, 1.0757709424483952, 1.0759518381410045, 1.0757849500292824, 1.075582600970965, 1.0755013485847436, 1.0752590986699697, 1.075903584608695, 1.0757065559255665, 1.0756129460968995, 1.0756748613269849, 1.076261033174048, 1.0753492400759743, 1.0753273184859302, 1.075508820795269, 1.0751565467743647, 1.0751142507703433, 1.0752136607475469, 1.0750906406756497, 1.0747567748005558, 1.0745839677224998, 1.0750049481838209, 1.075529796932327, 1.0753368524886509, 1.0749838109478378, 1.0751925747970055, 1.0749615415172233, 1.0756723679149485, 1.0748084262869824, 1.074664309890008, 1.0748627021394928, 1.075648634891792, 1.0752798021329055, 1.074780636233062, 1.0747294422049436, 1.0753346720744041, 1.0751929752932394, 1.0745837688446045, 1.0744816765605132, 1.0754289104433483, 1.0750213424951964, 1.0744019175202193, 1.0742851669956701, 1.0743393375368542, 1.074374721946779, 1.07453971425888, 1.0741419800005132, 1.0744582950971004, 1.074853695672134, 1.0743009355072122, 1.0743525646785992, 1.0743284718743686, 1.0742353504318713, 1.0742766821912944, 1.074023669967902, 1.0743962590917577, 1.0746070704436654, 1.0742130968566794, 1.0744520654819283, 1.0744696227200512, 1.074225458997028, 1.0742232813232246, 1.07594637996066, 1.0742244830076721, 1.0743428523513092, 1.0741678840421103, 1.074076195851531, 1.0747207982591025, 1.0740864983528902, 1.0744339809041892, 1.0747789996010917, 1.0742744788747702, 1.0746014644751212], 'val_acc': [0.3596059098130181, 0.3612479460356858, 0.3579638736882233, 0.3825944160495094, 0.4006568140094895, 0.39573070563510526, 0.3957307055372323, 0.39408866941243753, 0.39573070563510526, 0.37438423552340866, 0.3957307052436133, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3924466332876428, 0.3957307055372323, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.40722495880228743, 0.3891625609401803, 0.3891625609401803, 0.3891625609401803, 0.39080459706497506, 0.39408866941243753, 0.39408866941243753, 0.3891625610380533, 0.3891625610380533, 0.3924466332876428, 0.38752052491325856, 0.4022988503300302, 0.3891625610380533, 0.39080459716284804, 0.38752052491325856, 0.38752052481538557, 0.3924466333855158, 0.4055829227753656, 0.40394088655269794, 0.39573070563510526, 0.3973727420535189, 0.39737274136840806, 0.39080459726072103, 0.40722495425119387, 0.41050903046463905, 0.38259441644100134, 0.4022988493513004, 0.37931034428928484, 0.3924466332876428, 0.39408866941243753, 0.39080459716284804, 0.40394088635695197, 0.4006568141073625, 0.40394088655269794, 0.39901477788469475, 0.4022988504279032, 0.3858784886905908, 0.40394088655269794, 0.4055829224817467, 0.4006568141073625, 0.4022988502321572, 0.39080459716284804, 0.38259441644100134, 0.4055829224817467, 0.40722495870441444, 0.403940886259079, 0.4022988502321572, 0.4022988502321572, 0.4006568141073625, 0.4022988502321572, 0.4006568141073625, 0.403940886259079, 0.4006568141073625, 0.40229885013428424, 0.3940886695103105, 0.4006568141073625, 0.4006568141073625, 0.39901477788469475, 0.39901477788469475, 0.39901477788469475, 0.4006568140094895, 0.4006568140094895, 0.39901477788469475, 0.4006568140094895, 0.4022988502321572, 0.40229885013428424, 0.39901477788469475, 0.4006568140094895, 0.39901477788469475, 0.39901477788469475, 0.403940886259079, 0.40229885013428424, 0.40229885013428424, 0.39901477788469475, 0.39901477788469475, 0.4006568140094895, 0.39901477788469475, 0.40229885013428424, 0.39901477788469475, 0.40229885003641125, 0.40229885013428424, 0.40229885013428424, 0.40229885003641125, 0.3990147780804407, 0.40229885003641125, 0.40229885003641125, 0.3990147780804407, 0.4006568140094895, 0.40229885013428424, 0.40229885013428424, 0.3990147780804407, 0.403940886259079, 0.40229885003641125, 0.4006568140094895, 0.40229885013428424, 0.40394088635695197, 0.41379310271422853, 0.40229885013428424, 0.4006568140094895, 0.40229885013428424, 0.3990147780804407, 0.40394088635695197, 0.40229885013428424, 0.40229885013428424, 0.40229885003641125, 0.40229885013428424, 0.3990147780804407, 0.40394088635695197, 0.40558292228600074, 0.40229885013428424, 0.40394088635695197, 0.40229885013428424, 0.40558292228600074, 0.4006568140094895, 0.40229885013428424, 0.40229885003641125, 0.40558292228600074, 0.3990147780804407, 0.40229885013428424, 0.40229885013428424, 0.403940886161206, 0.40558292228600074, 0.4022988503300302, 0.40558292228600074, 0.40394088635695197, 0.40229885013428424, 0.40394088635695197, 0.40558292228600074, 0.403940886161206, 0.4006568140094895, 0.40229885013428424, 0.4006568140094895, 0.403940886161206, 0.403940886161206, 0.40558292228600074, 0.40229885013428424, 0.403940886161206, 0.41050903085613094, 0.40722495860654145, 0.40229885003641125, 0.40229885003641125, 0.40394088635695197, 0.40229885013428424, 0.40558292228600074, 0.4088669946334632, 0.4006568140094895, 0.40558292228600074, 0.39408866931456454, 0.4088669946334632, 0.4072249584107955, 0.4006568140094895, 0.4088669946334632, 0.3957307054393593, 0.40558292228600074, 0.40229885003641125, 0.4006568140094895, 0.403940886161206, 0.403940886161206, 0.40558292228600074, 0.40558292228600074, 0.4022988498406653, 0.40558292228600074, 0.4022988498406653, 0.403940886161206, 0.403940886161206, 0.4269293921040784, 0.403940886161206, 0.40229885003641125, 0.4022988498406653, 0.4072249584107955, 0.4088669945355902, 0.4006568138137435, 0.4022988498406653, 0.39901477778682176, 0.4006568138137435, 0.4006568138137435, 0.4022988498406653, 0.39737274156415403, 0.3908045968692291, 0.39408866911881857, 0.4022988498406653, 0.4022988498406653, 0.4203612478006454, 0.4203612478006454, 0.40394088596546, 0.40394088596546, 0.4121510668830527, 0.4170771755510559, 0.4220032839254401, 0.39408866911881857, 0.4170771755510559, 0.4154351391326422, 0.42692939200620544, 0.41379310310572043, 0.39737274136840806, 0.39408866911881857, 0.4170771755510559, 0.4269293921040784, 0.4121510669809257, 0.39737274136840806, 0.4285714281310002, 0.4269293921040784, 0.4154351391326422, 0.3858784882990989, 0.4121510669809257, 0.4121510669809257, 0.4170771755510559, 0.4088669945355902, 0.41379310310572043, 0.41379310310572043, 0.4121510669809257, 0.41379310310572043, 0.41379310310572043, 0.41379310310572043, 0.4121510669809257, 0.4121510669809257, 0.4072249583129225, 0.41379310310572043, 0.4121510669809257, 0.4072249583129225, 0.40558292228600074, 0.4285714282288731, 0.41543513923051517, 0.41379310310572043, 0.41543513923051517, 0.4121510669809257, 0.4121510669809257, 0.40558292228600074, 0.41871921167585063, 0.403940886161206, 0.40558292228600074, 0.41379310310572043, 0.41379310310572043, 0.403940886161206, 0.403940886161206, 0.4121510669809257, 0.41379310310572043, 0.41379310310572043, 0.41379310300784744, 0.38587848849484485, 0.41379310310572043, 0.41871921167585063, 0.40558292228600074, 0.40558292228600074, 0.41379310310572043], 'loss': [1.1176399955024954, 1.093434823171314, 1.0807885479878105, 1.0755916636092953, 1.0742342694339322, 1.074529493515986, 1.0746135680827271, 1.074044635408468, 1.0738623922120865, 1.0743604419657338, 1.0739541638313623, 1.0737010538700424, 1.0737452351337096, 1.0740527383600662, 1.0744790765539087, 1.0742888199475267, 1.0739316224072748, 1.0738281413515, 1.0738990944758577, 1.0737864817437206, 1.0737752299533978, 1.073752185500378, 1.0737463663735674, 1.0736571268868886, 1.0736579456858077, 1.0736491428508406, 1.0735722039024933, 1.0736539420650726, 1.0735169996226348, 1.0735365398610641, 1.073535887906194, 1.0735104837456768, 1.07347285791589, 1.073389972994215, 1.0733731618896891, 1.0734176713093595, 1.0733144509963677, 1.0733536975339697, 1.0732385537462803, 1.0732369603562404, 1.073327693606304, 1.073268998085351, 1.0731464585239638, 1.0731036321828007, 1.0731056666716903, 1.0732001283575132, 1.0730033663019263, 1.0730826257435448, 1.0730261545161692, 1.0730106095758551, 1.0728230438193256, 1.0727989733341539, 1.0727663057785504, 1.072655222058541, 1.0729216282862168, 1.0728103262687856, 1.0726480236778024, 1.0724778413772582, 1.07244179332036, 1.0724224664347373, 1.072273645557662, 1.072286572985091, 1.0723875107951233, 1.0720291677196903, 1.0719979184364146, 1.0719010349171851, 1.0719780838709838, 1.0722891200004907, 1.0723984809382006, 1.071903580610757, 1.0721400634953617, 1.0716654228723514, 1.071826021431408, 1.0727843655697864, 1.0733689366424843, 1.0727477890754873, 1.072582108039386, 1.0722744742947683, 1.0724026197280727, 1.0725127199102475, 1.074523402435334, 1.0754867001480635, 1.0734662073104044, 1.0736873773578746, 1.073091544165014, 1.073459074825232, 1.072893356248828, 1.0731497948174604, 1.073152306045595, 1.0728635478558237, 1.0729052361032068, 1.0728135735102502, 1.0727655470493638, 1.072682576600531, 1.0724474239643105, 1.072430805014389, 1.0722981901384232, 1.072201862178544, 1.0720724906764725, 1.0721845433452535, 1.0723796266305128, 1.0717018793740556, 1.0719371663961077, 1.0719878400865277, 1.0717171157410013, 1.0716014230520574, 1.0721361572003218, 1.0720710474362853, 1.0718902907087573, 1.071864917488803, 1.0718441377675019, 1.071732945510739, 1.0718079461698904, 1.0717192606759511, 1.0717294533884256, 1.0716018491701913, 1.0722261220277947, 1.0715219823731534, 1.0720063398016553, 1.0718413900790518, 1.0715599451221725, 1.071936264224121, 1.0718138700148407, 1.0716709448326784, 1.0719728439985114, 1.0719519805614464, 1.071695763717197, 1.071953362556943, 1.071824451981139, 1.071478845794098, 1.0716417904507207, 1.0717720204065468, 1.0714916095596565, 1.0717845093298253, 1.0714082461362993, 1.071844848078624, 1.0715567327867543, 1.072323617357493, 1.0724962713782058, 1.0718380147181987, 1.0726481740234814, 1.072022170012002, 1.0714815639127695, 1.0713249647641818, 1.0717725429691574, 1.0718019752287034, 1.071409022391944, 1.071521787379067, 1.0716547788291007, 1.0715030274596793, 1.0726371338724845, 1.0717151501829865, 1.071736547344764, 1.0721357922779216, 1.0719891720483925, 1.0715487296086808, 1.0713169143430017, 1.0712521441418532, 1.07152558317909, 1.0714543440993072, 1.0716145835617974, 1.0714367481227773, 1.0714594650072728, 1.0710954091387364, 1.0713082590631882, 1.0712232053157484, 1.071393633133577, 1.0715907718611448, 1.0720304531727973, 1.0711477317849223, 1.071206249887204, 1.0711111541646217, 1.0712409909500968, 1.0711236576280065, 1.070973292268522, 1.0709644180058944, 1.0713266675721937, 1.0714702317846874, 1.0715802692045175, 1.0711060908296024, 1.0709347146737258, 1.0713970634237207, 1.0709282271426317, 1.0714033387769664, 1.0711913961159865, 1.071775456571481, 1.071251602094521, 1.0709020388934156, 1.0708478256417495, 1.0708700026330027, 1.070923649505913, 1.0708592958763639, 1.0709880423007314, 1.0707704152904252, 1.0706783917405522, 1.07161090932098, 1.0707063268097519, 1.0713066315014503, 1.0708154633304667, 1.0710038469067833, 1.0706666841644037, 1.0712342033640805, 1.0706522564623635, 1.0707842318673888, 1.0708955590485059, 1.0704534419508196, 1.070708623950731, 1.071049173313979, 1.0706449729950767, 1.0706802649664438, 1.070768778769632, 1.0704978794532636, 1.070747887721052, 1.0705604841087388, 1.0703833287256699, 1.0703663601278035, 1.070602205059122, 1.0705032772596856, 1.0705635971601983, 1.070912393211584, 1.070950592861528, 1.0703382084746624, 1.0702249019297732, 1.070366272691339, 1.0701952876006797, 1.0703251722167404, 1.0712621777943763, 1.0708102343753134, 1.0704231620079683, 1.0701906483765744, 1.0700417104932562, 1.0699639735525395, 1.0702253507637636, 1.0702374547903544, 1.0701838250522495, 1.0699578422785294, 1.0701178806763165, 1.0702125485672842, 1.069870731326344, 1.069889045789746, 1.069941129429874, 1.069726765229227, 1.0697322107683218, 1.0696256670374156, 1.0697021417304475, 1.0699050661474772, 1.0696008117291962, 1.0698292044888287, 1.0696039902356127, 1.0695147119263604, 1.069811961929901, 1.0694560859237607, 1.069419624624311, 1.0693780470677714, 1.0704448389076844, 1.0697895891122995, 1.0693894729966744, 1.0693612965715005, 1.0692839045788962, 1.0692878082792372, 1.069141535347737, 1.0695047485754965, 1.0690013790032702, 1.0694349947896091, 1.0692038445012526, 1.0697761182667538, 1.0688159225902518, 1.0692748499602018, 1.0688955018652537, 1.0692961127361478, 1.0693480956236194, 1.0689495049952482, 1.0691154873591429, 1.0691470176042717, 1.0688544148537167, 1.0692005160408098, 1.0692508352365826, 1.0691997106559956, 1.0686146377293235, 1.0688581031450746, 1.069066125558387, 1.0685868001816454, 1.0691027248174994, 1.0687331135023301, 1.0687732820882934, 1.0687713876641995, 1.0688147938961365, 1.0688958781945388, 1.0699891516315374, 1.0688536844214376, 1.0691801445684883, 1.0685415944034804, 1.0686313187561975, 1.0688669350602544, 1.068905015256126, 1.068661031830727, 1.068824365937, 1.0686123555201035, 1.0692626988373743, 1.0683593904702815], 'acc': [0.3104722782571703, 0.37453798566021224, 0.3724845979982333, 0.3827515413873739, 0.3983572908740269, 0.4004106771652214, 0.3946611893740033, 0.3946611927030512, 0.39425051171921605, 0.387268992444573, 0.403696097950671, 0.38932238030237826, 0.3942505125392389, 0.39425051508498143, 0.39425051547663414, 0.39466119191974586, 0.3958932224240391, 0.39507186836285757, 0.3946611903164176, 0.3963039004337616, 0.3942505133225443, 0.394250513714197, 0.39425051351837065, 0.3942505141058497, 0.39425051171921605, 0.394250514301676, 0.3942505129308916, 0.394250514301676, 0.39425051309000053, 0.39425051391002336, 0.3942505119150424, 0.3942505113275634, 0.3942505152808078, 0.39425051449750237, 0.39425051508498143, 0.3942505119150424, 0.39425051171921605, 0.3942505156724605, 0.3942505126983478, 0.39425051446078496, 0.3942505148891551, 0.3942505146933287, 0.3942505119150424, 0.3942505148891551, 0.39507186832614016, 0.39466118953311224, 0.3950718667595293, 0.3942505119150424, 0.3942505117559335, 0.3946611912955494, 0.39548254793919085, 0.39548254793919085, 0.39548254833084356, 0.3946611907447878, 0.39425051351837065, 0.3930184789017241, 0.3971252578239911, 0.39589322199566895, 0.39466119231139857, 0.3942505128941742, 0.3938398362919535, 0.3950718681670312, 0.3967145799733775, 0.395071866563703, 0.3954825465684064, 0.3946611917239195, 0.3958932243823026, 0.3971252584114701, 0.397535934034559, 0.39835729146150595, 0.3963039000053915, 0.39876796806372655, 0.3958932216040163, 0.3975359323088393, 0.39137577097518733, 0.3946611913322668, 0.39383983707525894, 0.39712525684485933, 0.39835728852411073, 0.3872689928362257, 0.3950718667595293, 0.38973305913212364, 0.39917864352770654, 0.3946611913322668, 0.39425051449750237, 0.39383983472534273, 0.3954825471558855, 0.39301847866918027, 0.3983572900907215, 0.3991786461101665, 0.3975359350504082, 0.39671457981426855, 0.39589322179984265, 0.39753593266377457, 0.39671458079340033, 0.39630390219619877, 0.3913757715626664, 0.39383983413786366, 0.3958932221914953, 0.3954825475475382, 0.3946611928988776, 0.39466119113644044, 0.3954825477433645, 0.3946611912955494, 0.3950718667595293, 0.3954825475475382, 0.39342916004466816, 0.3950718681303138, 0.3946611917239195, 0.39425051351837065, 0.3954825453567309, 0.3946611907447878, 0.39219712581967425, 0.39301847827752756, 0.39301848262242467, 0.39425051508498143, 0.3926078044168758, 0.3938398357044745, 0.39301848125164024, 0.3942505141058497, 0.394250513714197, 0.39383983370949355, 0.3930184798808558, 0.3934291598488418, 0.393839835080278, 0.39712525860729647, 0.39507186895033664, 0.38891170526676844, 0.3934291602404945, 0.3938398368794326, 0.3938398374669116, 0.3938398374669116, 0.3921971275821114, 0.3926078048085285, 0.3958932239906499, 0.3934291568747291, 0.39342915808640466, 0.39630389984628256, 0.3954825467642328, 0.39466118914145953, 0.39219712304138793, 0.39219712581967425, 0.39301847827752756, 0.3938398374669116, 0.3934291574989256, 0.3897330583488182, 0.3930184824265983, 0.3971252562573803, 0.3946611921522896, 0.39342915710727294, 0.390965094727902, 0.3946611897289386, 0.3942505125392389, 0.3901437355385179, 0.38891170624590016, 0.3934291558955974, 0.397125256649033, 0.39383983766273795, 0.39055441593487406, 0.3942505133225443, 0.3963039035669832, 0.3917864499640416, 0.39260780363357045, 0.39589322496978163, 0.3958932261447397, 0.39342916043632087, 0.3921971246079987, 0.39425051113173704, 0.3897330577613392, 0.3979466112976936, 0.39548254496507823, 0.39630390023793527, 0.38891170272102593, 0.39383983394203737, 0.3926078030460914, 0.3946611913322668, 0.3958932247739553, 0.39466119152809315, 0.39425051211086876, 0.3942505129308916, 0.39425051391002336, 0.3954825453567309, 0.39548254398594646, 0.3963038996137388, 0.3958932243823026, 0.395071869733642, 0.39301848007668216, 0.39383983648777987, 0.39178644937656254, 0.39589322238732166, 0.39466118953311224, 0.3950718693419893, 0.3954825448059693, 0.39383983609612716, 0.39630390360370066, 0.39466118992476495, 0.39507186992946836, 0.38809034924732344, 0.39548254751082074, 0.3975359344629292, 0.3958932226198655, 0.39260780520018124, 0.39466119149137574, 0.39548254637258007, 0.3995893207541237, 0.3880903500306288, 0.39260780461270217, 0.3926078032052003, 0.3987679663012894, 0.3950718685586839, 0.39383983550864815, 0.3946611893372859, 0.39589322297480073, 0.3946611893372859, 0.39589322238732166, 0.3946611927030512, 0.3979466098901917, 0.39178644780995175, 0.39671457922678954, 0.3958932255572607, 0.39548254696005913, 0.39794661008601806, 0.39589322418647627, 0.3942505117559335, 0.3958932221914953, 0.40000000212961156, 0.40041067618608966, 0.39671457977755115, 0.39712525860729647, 0.3967145784434841, 0.3987679688837494, 0.3979466098901917, 0.39466119152809315, 0.4004106785360058, 0.3942505113275634, 0.3971252570406857, 0.39466118914145953, 0.3987679653221577, 0.4057494889784153, 0.4024640642764387, 0.3958932259489134, 0.39548254696005913, 0.3954825455892747, 0.4057494883909363, 0.39383983805439066, 0.4012320338088629, 0.40862422838837703, 0.3987679688837494, 0.39835728907487233, 0.4008213575615775, 0.40246406349313335, 0.401232034983821, 0.4041067749812618, 0.39794661126097614, 0.38562628255977277, 0.3963039011803496, 0.39917864708929823, 0.400410679551855, 0.40164270923612544, 0.4024640668588987, 0.4061601638181988, 0.40492813018068397, 0.4053388070154484, 0.40410677831030967, 0.40164271256517337, 0.40328542049171007, 0.4032854208833628, 0.40369609971310816, 0.4045174559283795, 0.39999999935132524, 0.40041067896437593, 0.40574948561264995, 0.4069815172919013, 0.39630390180454605, 0.4028747426778139, 0.39753593465875553, 0.39671457981426855, 0.4012320343963419, 0.4045174519751351, 0.4045174549492478, 0.40164270923612544, 0.40246406368895965, 0.4049281325673176, 0.4049281323714912, 0.40205338963248155, 0.40164271201441176, 0.4041067767436989, 0.4028747436936631, 0.40123203420051556, 0.40698151987436126, 0.4069815212451457, 0.4036961001047608, 0.40369609932145545, 0.40533881116451914, 0.40328542189921196, 0.40862422776418056, 0.40574948760763085, 0.4032854212750155, 0.4028747416986822, 0.40698152046184033]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
