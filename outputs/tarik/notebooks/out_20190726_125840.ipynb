{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf4.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 12:58:40 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'Front', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ds', 'ck', 'eo', 'by', 'my', 'yd', 'eb', 'sg', 'aa', 'ce', 'ib', 'mb', 'eg', 'ek', 'sk'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000017D24D6D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000017D19676EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7156, Accuracy:0.0661, Validation Loss:2.7075, Validation Accuracy:0.0640\n",
    "Epoch #2: Loss:2.7056, Accuracy:0.0661, Validation Loss:2.7008, Validation Accuracy:0.0837\n",
    "Epoch #3: Loss:2.6990, Accuracy:0.0727, Validation Loss:2.6958, Validation Accuracy:0.0722\n",
    "Epoch #4: Loss:2.6943, Accuracy:0.0838, Validation Loss:2.6911, Validation Accuracy:0.0854\n",
    "Epoch #5: Loss:2.6895, Accuracy:0.0834, Validation Loss:2.6864, Validation Accuracy:0.0837\n",
    "Epoch #6: Loss:2.6852, Accuracy:0.0830, Validation Loss:2.6821, Validation Accuracy:0.0837\n",
    "Epoch #7: Loss:2.6807, Accuracy:0.0830, Validation Loss:2.6772, Validation Accuracy:0.0837\n",
    "Epoch #8: Loss:2.6761, Accuracy:0.0830, Validation Loss:2.6733, Validation Accuracy:0.0837\n",
    "Epoch #9: Loss:2.6713, Accuracy:0.0830, Validation Loss:2.6685, Validation Accuracy:0.0837\n",
    "Epoch #10: Loss:2.6662, Accuracy:0.0830, Validation Loss:2.6631, Validation Accuracy:0.0837\n",
    "Epoch #11: Loss:2.6607, Accuracy:0.0899, Validation Loss:2.6573, Validation Accuracy:0.1084\n",
    "Epoch #12: Loss:2.6544, Accuracy:0.1125, Validation Loss:2.6512, Validation Accuracy:0.1149\n",
    "Epoch #13: Loss:2.6469, Accuracy:0.1146, Validation Loss:2.6428, Validation Accuracy:0.1281\n",
    "Epoch #14: Loss:2.6372, Accuracy:0.1191, Validation Loss:2.6356, Validation Accuracy:0.1232\n",
    "Epoch #15: Loss:2.6286, Accuracy:0.1175, Validation Loss:2.6213, Validation Accuracy:0.1297\n",
    "Epoch #16: Loss:2.6177, Accuracy:0.1248, Validation Loss:2.6096, Validation Accuracy:0.1232\n",
    "Epoch #17: Loss:2.6067, Accuracy:0.1146, Validation Loss:2.5979, Validation Accuracy:0.1396\n",
    "Epoch #18: Loss:2.5944, Accuracy:0.1294, Validation Loss:2.5859, Validation Accuracy:0.1412\n",
    "Epoch #19: Loss:2.5826, Accuracy:0.1372, Validation Loss:2.5751, Validation Accuracy:0.1346\n",
    "Epoch #20: Loss:2.5709, Accuracy:0.1561, Validation Loss:2.5610, Validation Accuracy:0.1363\n",
    "Epoch #21: Loss:2.5588, Accuracy:0.1569, Validation Loss:2.5492, Validation Accuracy:0.1527\n",
    "Epoch #22: Loss:2.5489, Accuracy:0.1556, Validation Loss:2.5410, Validation Accuracy:0.1527\n",
    "Epoch #23: Loss:2.5404, Accuracy:0.1466, Validation Loss:2.5385, Validation Accuracy:0.1511\n",
    "Epoch #24: Loss:2.5327, Accuracy:0.1520, Validation Loss:2.5288, Validation Accuracy:0.1527\n",
    "Epoch #25: Loss:2.5241, Accuracy:0.1565, Validation Loss:2.5212, Validation Accuracy:0.1560\n",
    "Epoch #26: Loss:2.5162, Accuracy:0.1573, Validation Loss:2.5143, Validation Accuracy:0.1544\n",
    "Epoch #27: Loss:2.5117, Accuracy:0.1569, Validation Loss:2.5113, Validation Accuracy:0.1626\n",
    "Epoch #28: Loss:2.5045, Accuracy:0.1556, Validation Loss:2.5044, Validation Accuracy:0.1511\n",
    "Epoch #29: Loss:2.4991, Accuracy:0.1569, Validation Loss:2.4959, Validation Accuracy:0.1560\n",
    "Epoch #30: Loss:2.4932, Accuracy:0.1556, Validation Loss:2.4924, Validation Accuracy:0.1593\n",
    "Epoch #31: Loss:2.4891, Accuracy:0.1544, Validation Loss:2.4888, Validation Accuracy:0.1527\n",
    "Epoch #32: Loss:2.4885, Accuracy:0.1569, Validation Loss:2.4817, Validation Accuracy:0.1527\n",
    "Epoch #33: Loss:2.4822, Accuracy:0.1556, Validation Loss:2.4881, Validation Accuracy:0.1658\n",
    "Epoch #34: Loss:2.5010, Accuracy:0.1581, Validation Loss:2.5628, Validation Accuracy:0.1429\n",
    "Epoch #35: Loss:2.5180, Accuracy:0.1548, Validation Loss:2.4973, Validation Accuracy:0.1658\n",
    "Epoch #36: Loss:2.5556, Accuracy:0.1446, Validation Loss:2.6401, Validation Accuracy:0.1281\n",
    "Epoch #37: Loss:2.6093, Accuracy:0.1199, Validation Loss:2.5061, Validation Accuracy:0.1642\n",
    "Epoch #38: Loss:2.4919, Accuracy:0.1663, Validation Loss:2.4881, Validation Accuracy:0.1609\n",
    "Epoch #39: Loss:2.4921, Accuracy:0.1598, Validation Loss:2.4948, Validation Accuracy:0.1609\n",
    "Epoch #40: Loss:2.4801, Accuracy:0.1696, Validation Loss:2.4818, Validation Accuracy:0.1708\n",
    "Epoch #41: Loss:2.4850, Accuracy:0.1606, Validation Loss:2.5509, Validation Accuracy:0.1461\n",
    "Epoch #42: Loss:2.5186, Accuracy:0.1577, Validation Loss:2.4911, Validation Accuracy:0.1724\n",
    "Epoch #43: Loss:2.4881, Accuracy:0.1692, Validation Loss:2.4985, Validation Accuracy:0.1773\n",
    "Epoch #44: Loss:2.4854, Accuracy:0.1725, Validation Loss:2.4794, Validation Accuracy:0.1724\n",
    "Epoch #45: Loss:2.4732, Accuracy:0.1733, Validation Loss:2.4831, Validation Accuracy:0.1691\n",
    "Epoch #46: Loss:2.4729, Accuracy:0.1688, Validation Loss:2.4752, Validation Accuracy:0.1708\n",
    "Epoch #47: Loss:2.4669, Accuracy:0.1758, Validation Loss:2.4748, Validation Accuracy:0.1724\n",
    "Epoch #48: Loss:2.4663, Accuracy:0.1762, Validation Loss:2.4707, Validation Accuracy:0.1757\n",
    "Epoch #49: Loss:2.4630, Accuracy:0.1749, Validation Loss:2.4708, Validation Accuracy:0.1741\n",
    "Epoch #50: Loss:2.4632, Accuracy:0.1745, Validation Loss:2.4711, Validation Accuracy:0.1708\n",
    "Epoch #51: Loss:2.4636, Accuracy:0.1717, Validation Loss:2.4681, Validation Accuracy:0.1691\n",
    "Epoch #52: Loss:2.4634, Accuracy:0.1721, Validation Loss:2.4663, Validation Accuracy:0.1708\n",
    "Epoch #53: Loss:2.4617, Accuracy:0.1729, Validation Loss:2.4637, Validation Accuracy:0.1691\n",
    "Epoch #54: Loss:2.4612, Accuracy:0.1745, Validation Loss:2.4633, Validation Accuracy:0.1708\n",
    "Epoch #55: Loss:2.4598, Accuracy:0.1737, Validation Loss:2.4606, Validation Accuracy:0.1724\n",
    "Epoch #56: Loss:2.4577, Accuracy:0.1749, Validation Loss:2.4604, Validation Accuracy:0.1708\n",
    "Epoch #57: Loss:2.4561, Accuracy:0.1754, Validation Loss:2.4613, Validation Accuracy:0.1724\n",
    "Epoch #58: Loss:2.4562, Accuracy:0.1749, Validation Loss:2.4598, Validation Accuracy:0.1724\n",
    "Epoch #59: Loss:2.4544, Accuracy:0.1729, Validation Loss:2.4578, Validation Accuracy:0.1724\n",
    "Epoch #60: Loss:2.4545, Accuracy:0.1741, Validation Loss:2.4602, Validation Accuracy:0.1708\n",
    "Epoch #61: Loss:2.4531, Accuracy:0.1721, Validation Loss:2.4595, Validation Accuracy:0.1724\n",
    "Epoch #62: Loss:2.4546, Accuracy:0.1704, Validation Loss:2.4566, Validation Accuracy:0.1790\n",
    "Epoch #63: Loss:2.4515, Accuracy:0.1721, Validation Loss:2.4591, Validation Accuracy:0.1708\n",
    "Epoch #64: Loss:2.4512, Accuracy:0.1737, Validation Loss:2.4592, Validation Accuracy:0.1691\n",
    "Epoch #65: Loss:2.4502, Accuracy:0.1737, Validation Loss:2.4578, Validation Accuracy:0.1708\n",
    "Epoch #66: Loss:2.4508, Accuracy:0.1729, Validation Loss:2.4569, Validation Accuracy:0.1691\n",
    "Epoch #67: Loss:2.4492, Accuracy:0.1729, Validation Loss:2.4581, Validation Accuracy:0.1691\n",
    "Epoch #68: Loss:2.4493, Accuracy:0.1733, Validation Loss:2.4600, Validation Accuracy:0.1675\n",
    "Epoch #69: Loss:2.4492, Accuracy:0.1758, Validation Loss:2.4550, Validation Accuracy:0.1691\n",
    "Epoch #70: Loss:2.4500, Accuracy:0.1799, Validation Loss:2.4576, Validation Accuracy:0.1658\n",
    "Epoch #71: Loss:2.4504, Accuracy:0.1778, Validation Loss:2.4567, Validation Accuracy:0.1609\n",
    "Epoch #72: Loss:2.4506, Accuracy:0.1749, Validation Loss:2.4556, Validation Accuracy:0.1708\n",
    "Epoch #73: Loss:2.4469, Accuracy:0.1799, Validation Loss:2.4562, Validation Accuracy:0.1691\n",
    "Epoch #74: Loss:2.4463, Accuracy:0.1786, Validation Loss:2.4560, Validation Accuracy:0.1691\n",
    "Epoch #75: Loss:2.4465, Accuracy:0.1741, Validation Loss:2.4548, Validation Accuracy:0.1724\n",
    "Epoch #76: Loss:2.4462, Accuracy:0.1782, Validation Loss:2.4558, Validation Accuracy:0.1708\n",
    "Epoch #77: Loss:2.4457, Accuracy:0.1758, Validation Loss:2.4547, Validation Accuracy:0.1724\n",
    "Epoch #78: Loss:2.4441, Accuracy:0.1778, Validation Loss:2.4524, Validation Accuracy:0.1708\n",
    "Epoch #79: Loss:2.4446, Accuracy:0.1778, Validation Loss:2.4509, Validation Accuracy:0.1691\n",
    "Epoch #80: Loss:2.4443, Accuracy:0.1778, Validation Loss:2.4507, Validation Accuracy:0.1708\n",
    "Epoch #81: Loss:2.4449, Accuracy:0.1774, Validation Loss:2.4488, Validation Accuracy:0.1708\n",
    "Epoch #82: Loss:2.4441, Accuracy:0.1766, Validation Loss:2.4450, Validation Accuracy:0.1741\n",
    "Epoch #83: Loss:2.4440, Accuracy:0.1745, Validation Loss:2.4470, Validation Accuracy:0.1642\n",
    "Epoch #84: Loss:2.4460, Accuracy:0.1766, Validation Loss:2.4482, Validation Accuracy:0.1691\n",
    "Epoch #85: Loss:2.4451, Accuracy:0.1745, Validation Loss:2.4424, Validation Accuracy:0.1724\n",
    "Epoch #86: Loss:2.4823, Accuracy:0.1593, Validation Loss:2.4608, Validation Accuracy:0.1806\n",
    "Epoch #87: Loss:2.4715, Accuracy:0.1770, Validation Loss:2.4612, Validation Accuracy:0.1658\n",
    "Epoch #88: Loss:2.4475, Accuracy:0.1836, Validation Loss:2.4668, Validation Accuracy:0.1642\n",
    "Epoch #89: Loss:2.4571, Accuracy:0.1717, Validation Loss:2.4605, Validation Accuracy:0.1642\n",
    "Epoch #90: Loss:2.4474, Accuracy:0.1754, Validation Loss:2.4582, Validation Accuracy:0.1741\n",
    "Epoch #91: Loss:2.4510, Accuracy:0.1782, Validation Loss:2.4560, Validation Accuracy:0.1724\n",
    "Epoch #92: Loss:2.4438, Accuracy:0.1733, Validation Loss:2.4598, Validation Accuracy:0.1675\n",
    "Epoch #93: Loss:2.4465, Accuracy:0.1766, Validation Loss:2.4576, Validation Accuracy:0.1675\n",
    "Epoch #94: Loss:2.4449, Accuracy:0.1745, Validation Loss:2.4545, Validation Accuracy:0.1593\n",
    "Epoch #95: Loss:2.4445, Accuracy:0.1725, Validation Loss:2.4544, Validation Accuracy:0.1642\n",
    "Epoch #96: Loss:2.4451, Accuracy:0.1745, Validation Loss:2.4576, Validation Accuracy:0.1593\n",
    "Epoch #97: Loss:2.4433, Accuracy:0.1717, Validation Loss:2.4517, Validation Accuracy:0.1626\n",
    "Epoch #98: Loss:2.4424, Accuracy:0.1762, Validation Loss:2.4524, Validation Accuracy:0.1658\n",
    "Epoch #99: Loss:2.4427, Accuracy:0.1766, Validation Loss:2.4524, Validation Accuracy:0.1642\n",
    "Epoch #100: Loss:2.4435, Accuracy:0.1741, Validation Loss:2.4530, Validation Accuracy:0.1691\n",
    "Epoch #101: Loss:2.4445, Accuracy:0.1770, Validation Loss:2.4534, Validation Accuracy:0.1642\n",
    "Epoch #102: Loss:2.4439, Accuracy:0.1700, Validation Loss:2.4563, Validation Accuracy:0.1658\n",
    "Epoch #103: Loss:2.4431, Accuracy:0.1766, Validation Loss:2.4581, Validation Accuracy:0.1658\n",
    "Epoch #104: Loss:2.4441, Accuracy:0.1745, Validation Loss:2.4577, Validation Accuracy:0.1642\n",
    "Epoch #105: Loss:2.4434, Accuracy:0.1754, Validation Loss:2.4567, Validation Accuracy:0.1609\n",
    "Epoch #106: Loss:2.4434, Accuracy:0.1754, Validation Loss:2.4557, Validation Accuracy:0.1642\n",
    "Epoch #107: Loss:2.4436, Accuracy:0.1741, Validation Loss:2.4572, Validation Accuracy:0.1593\n",
    "Epoch #108: Loss:2.4442, Accuracy:0.1717, Validation Loss:2.4560, Validation Accuracy:0.1609\n",
    "Epoch #109: Loss:2.4431, Accuracy:0.1770, Validation Loss:2.4572, Validation Accuracy:0.1609\n",
    "Epoch #110: Loss:2.4441, Accuracy:0.1766, Validation Loss:2.4595, Validation Accuracy:0.1642\n",
    "Epoch #111: Loss:2.4434, Accuracy:0.1754, Validation Loss:2.4552, Validation Accuracy:0.1691\n",
    "Epoch #112: Loss:2.4435, Accuracy:0.1729, Validation Loss:2.4520, Validation Accuracy:0.1708\n",
    "Epoch #113: Loss:2.4419, Accuracy:0.1745, Validation Loss:2.4509, Validation Accuracy:0.1626\n",
    "Epoch #114: Loss:2.4415, Accuracy:0.1754, Validation Loss:2.4476, Validation Accuracy:0.1642\n",
    "Epoch #115: Loss:2.4410, Accuracy:0.1778, Validation Loss:2.4460, Validation Accuracy:0.1593\n",
    "Epoch #116: Loss:2.4403, Accuracy:0.1799, Validation Loss:2.4459, Validation Accuracy:0.1658\n",
    "Epoch #117: Loss:2.4384, Accuracy:0.1807, Validation Loss:2.4466, Validation Accuracy:0.1708\n",
    "Epoch #118: Loss:2.4384, Accuracy:0.1778, Validation Loss:2.4467, Validation Accuracy:0.1658\n",
    "Epoch #119: Loss:2.4381, Accuracy:0.1741, Validation Loss:2.4475, Validation Accuracy:0.1642\n",
    "Epoch #120: Loss:2.4375, Accuracy:0.1819, Validation Loss:2.4480, Validation Accuracy:0.1675\n",
    "Epoch #121: Loss:2.4378, Accuracy:0.1754, Validation Loss:2.4506, Validation Accuracy:0.1675\n",
    "Epoch #122: Loss:2.4385, Accuracy:0.1778, Validation Loss:2.4504, Validation Accuracy:0.1642\n",
    "Epoch #123: Loss:2.4369, Accuracy:0.1815, Validation Loss:2.4520, Validation Accuracy:0.1708\n",
    "Epoch #124: Loss:2.4364, Accuracy:0.1749, Validation Loss:2.4523, Validation Accuracy:0.1773\n",
    "Epoch #125: Loss:2.4355, Accuracy:0.1778, Validation Loss:2.4525, Validation Accuracy:0.1724\n",
    "Epoch #126: Loss:2.4349, Accuracy:0.1766, Validation Loss:2.4518, Validation Accuracy:0.1724\n",
    "Epoch #127: Loss:2.4349, Accuracy:0.1770, Validation Loss:2.4533, Validation Accuracy:0.1675\n",
    "Epoch #128: Loss:2.4350, Accuracy:0.1770, Validation Loss:2.4545, Validation Accuracy:0.1658\n",
    "Epoch #129: Loss:2.4351, Accuracy:0.1758, Validation Loss:2.4526, Validation Accuracy:0.1757\n",
    "Epoch #130: Loss:2.4370, Accuracy:0.1725, Validation Loss:2.4553, Validation Accuracy:0.1691\n",
    "Epoch #131: Loss:2.4355, Accuracy:0.1766, Validation Loss:2.4551, Validation Accuracy:0.1642\n",
    "Epoch #132: Loss:2.4357, Accuracy:0.1786, Validation Loss:2.4540, Validation Accuracy:0.1658\n",
    "Epoch #133: Loss:2.4351, Accuracy:0.1770, Validation Loss:2.4538, Validation Accuracy:0.1658\n",
    "Epoch #134: Loss:2.4353, Accuracy:0.1762, Validation Loss:2.4536, Validation Accuracy:0.1642\n",
    "Epoch #135: Loss:2.4345, Accuracy:0.1758, Validation Loss:2.4533, Validation Accuracy:0.1626\n",
    "Epoch #136: Loss:2.4342, Accuracy:0.1758, Validation Loss:2.4534, Validation Accuracy:0.1642\n",
    "Epoch #137: Loss:2.4340, Accuracy:0.1770, Validation Loss:2.4528, Validation Accuracy:0.1642\n",
    "Epoch #138: Loss:2.4333, Accuracy:0.1828, Validation Loss:2.4525, Validation Accuracy:0.1626\n",
    "Epoch #139: Loss:2.4338, Accuracy:0.1795, Validation Loss:2.4547, Validation Accuracy:0.1642\n",
    "Epoch #140: Loss:2.4382, Accuracy:0.1737, Validation Loss:2.4536, Validation Accuracy:0.1691\n",
    "Epoch #141: Loss:2.4369, Accuracy:0.1754, Validation Loss:2.4540, Validation Accuracy:0.1626\n",
    "Epoch #142: Loss:2.4346, Accuracy:0.1807, Validation Loss:2.4493, Validation Accuracy:0.1626\n",
    "Epoch #143: Loss:2.4342, Accuracy:0.1819, Validation Loss:2.4473, Validation Accuracy:0.1626\n",
    "Epoch #144: Loss:2.4338, Accuracy:0.1823, Validation Loss:2.4490, Validation Accuracy:0.1675\n",
    "Epoch #145: Loss:2.4345, Accuracy:0.1807, Validation Loss:2.4508, Validation Accuracy:0.1675\n",
    "Epoch #146: Loss:2.4340, Accuracy:0.1832, Validation Loss:2.4501, Validation Accuracy:0.1658\n",
    "Epoch #147: Loss:2.4338, Accuracy:0.1819, Validation Loss:2.4530, Validation Accuracy:0.1675\n",
    "Epoch #148: Loss:2.4335, Accuracy:0.1786, Validation Loss:2.4550, Validation Accuracy:0.1708\n",
    "Epoch #149: Loss:2.4323, Accuracy:0.1823, Validation Loss:2.4536, Validation Accuracy:0.1757\n",
    "Epoch #150: Loss:2.4323, Accuracy:0.1840, Validation Loss:2.4529, Validation Accuracy:0.1691\n",
    "Epoch #151: Loss:2.4327, Accuracy:0.1811, Validation Loss:2.4547, Validation Accuracy:0.1691\n",
    "Epoch #152: Loss:2.4347, Accuracy:0.1786, Validation Loss:2.4553, Validation Accuracy:0.1691\n",
    "Epoch #153: Loss:2.4340, Accuracy:0.1782, Validation Loss:2.4545, Validation Accuracy:0.1675\n",
    "Epoch #154: Loss:2.4333, Accuracy:0.1832, Validation Loss:2.4541, Validation Accuracy:0.1691\n",
    "Epoch #155: Loss:2.4331, Accuracy:0.1832, Validation Loss:2.4527, Validation Accuracy:0.1724\n",
    "Epoch #156: Loss:2.4320, Accuracy:0.1828, Validation Loss:2.4529, Validation Accuracy:0.1741\n",
    "Epoch #157: Loss:2.4308, Accuracy:0.1795, Validation Loss:2.4506, Validation Accuracy:0.1741\n",
    "Epoch #158: Loss:2.4313, Accuracy:0.1819, Validation Loss:2.4511, Validation Accuracy:0.1757\n",
    "Epoch #159: Loss:2.4302, Accuracy:0.1745, Validation Loss:2.4509, Validation Accuracy:0.1675\n",
    "Epoch #160: Loss:2.4300, Accuracy:0.1795, Validation Loss:2.4505, Validation Accuracy:0.1790\n",
    "Epoch #161: Loss:2.4292, Accuracy:0.1795, Validation Loss:2.4524, Validation Accuracy:0.1790\n",
    "Epoch #162: Loss:2.4293, Accuracy:0.1795, Validation Loss:2.4527, Validation Accuracy:0.1773\n",
    "Epoch #163: Loss:2.4291, Accuracy:0.1782, Validation Loss:2.4547, Validation Accuracy:0.1773\n",
    "Epoch #164: Loss:2.4311, Accuracy:0.1758, Validation Loss:2.4559, Validation Accuracy:0.1806\n",
    "Epoch #165: Loss:2.4303, Accuracy:0.1791, Validation Loss:2.4579, Validation Accuracy:0.1757\n",
    "Epoch #166: Loss:2.4304, Accuracy:0.1774, Validation Loss:2.4580, Validation Accuracy:0.1773\n",
    "Epoch #167: Loss:2.4308, Accuracy:0.1811, Validation Loss:2.4587, Validation Accuracy:0.1790\n",
    "Epoch #168: Loss:2.4309, Accuracy:0.1840, Validation Loss:2.4610, Validation Accuracy:0.1741\n",
    "Epoch #169: Loss:2.4308, Accuracy:0.1770, Validation Loss:2.4626, Validation Accuracy:0.1741\n",
    "Epoch #170: Loss:2.4305, Accuracy:0.1766, Validation Loss:2.4596, Validation Accuracy:0.1790\n",
    "Epoch #171: Loss:2.4294, Accuracy:0.1774, Validation Loss:2.4581, Validation Accuracy:0.1675\n",
    "Epoch #172: Loss:2.4294, Accuracy:0.1762, Validation Loss:2.4574, Validation Accuracy:0.1724\n",
    "Epoch #173: Loss:2.4296, Accuracy:0.1754, Validation Loss:2.4571, Validation Accuracy:0.1741\n",
    "Epoch #174: Loss:2.4296, Accuracy:0.1766, Validation Loss:2.4565, Validation Accuracy:0.1691\n",
    "Epoch #175: Loss:2.4299, Accuracy:0.1799, Validation Loss:2.4575, Validation Accuracy:0.1806\n",
    "Epoch #176: Loss:2.4298, Accuracy:0.1770, Validation Loss:2.4574, Validation Accuracy:0.1724\n",
    "Epoch #177: Loss:2.4293, Accuracy:0.1778, Validation Loss:2.4569, Validation Accuracy:0.1741\n",
    "Epoch #178: Loss:2.4289, Accuracy:0.1815, Validation Loss:2.4567, Validation Accuracy:0.1757\n",
    "Epoch #179: Loss:2.4288, Accuracy:0.1770, Validation Loss:2.4554, Validation Accuracy:0.1790\n",
    "Epoch #180: Loss:2.4301, Accuracy:0.1823, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #181: Loss:2.4302, Accuracy:0.1774, Validation Loss:2.4576, Validation Accuracy:0.1741\n",
    "Epoch #182: Loss:2.4306, Accuracy:0.1799, Validation Loss:2.4577, Validation Accuracy:0.1757\n",
    "Epoch #183: Loss:2.4297, Accuracy:0.1836, Validation Loss:2.4594, Validation Accuracy:0.1675\n",
    "Epoch #184: Loss:2.4293, Accuracy:0.1774, Validation Loss:2.4587, Validation Accuracy:0.1708\n",
    "Epoch #185: Loss:2.4296, Accuracy:0.1795, Validation Loss:2.4564, Validation Accuracy:0.1757\n",
    "Epoch #186: Loss:2.4289, Accuracy:0.1869, Validation Loss:2.4555, Validation Accuracy:0.1741\n",
    "Epoch #187: Loss:2.4282, Accuracy:0.1795, Validation Loss:2.4580, Validation Accuracy:0.1691\n",
    "Epoch #188: Loss:2.4291, Accuracy:0.1774, Validation Loss:2.4562, Validation Accuracy:0.1790\n",
    "Epoch #189: Loss:2.4280, Accuracy:0.1869, Validation Loss:2.4560, Validation Accuracy:0.1609\n",
    "Epoch #190: Loss:2.4319, Accuracy:0.1758, Validation Loss:2.4565, Validation Accuracy:0.1658\n",
    "Epoch #191: Loss:2.4301, Accuracy:0.1782, Validation Loss:2.4522, Validation Accuracy:0.1658\n",
    "Epoch #192: Loss:2.4321, Accuracy:0.1815, Validation Loss:2.4516, Validation Accuracy:0.1675\n",
    "Epoch #193: Loss:2.4302, Accuracy:0.1754, Validation Loss:2.4555, Validation Accuracy:0.1593\n",
    "Epoch #194: Loss:2.4302, Accuracy:0.1807, Validation Loss:2.4528, Validation Accuracy:0.1773\n",
    "Epoch #195: Loss:2.4307, Accuracy:0.1815, Validation Loss:2.4530, Validation Accuracy:0.1741\n",
    "Epoch #196: Loss:2.4286, Accuracy:0.1840, Validation Loss:2.4524, Validation Accuracy:0.1708\n",
    "Epoch #197: Loss:2.4286, Accuracy:0.1749, Validation Loss:2.4525, Validation Accuracy:0.1626\n",
    "Epoch #198: Loss:2.4275, Accuracy:0.1815, Validation Loss:2.4528, Validation Accuracy:0.1823\n",
    "Epoch #199: Loss:2.4274, Accuracy:0.1836, Validation Loss:2.4548, Validation Accuracy:0.1741\n",
    "Epoch #200: Loss:2.4282, Accuracy:0.1819, Validation Loss:2.4574, Validation Accuracy:0.1642\n",
    "Epoch #201: Loss:2.4276, Accuracy:0.1828, Validation Loss:2.4577, Validation Accuracy:0.1576\n",
    "Epoch #202: Loss:2.4271, Accuracy:0.1889, Validation Loss:2.4600, Validation Accuracy:0.1626\n",
    "Epoch #203: Loss:2.4270, Accuracy:0.1836, Validation Loss:2.4598, Validation Accuracy:0.1642\n",
    "Epoch #204: Loss:2.4268, Accuracy:0.1864, Validation Loss:2.4568, Validation Accuracy:0.1773\n",
    "Epoch #205: Loss:2.4262, Accuracy:0.1852, Validation Loss:2.4563, Validation Accuracy:0.1576\n",
    "Epoch #206: Loss:2.4256, Accuracy:0.1885, Validation Loss:2.4575, Validation Accuracy:0.1609\n",
    "Epoch #207: Loss:2.4258, Accuracy:0.1864, Validation Loss:2.4580, Validation Accuracy:0.1626\n",
    "Epoch #208: Loss:2.4257, Accuracy:0.1832, Validation Loss:2.4581, Validation Accuracy:0.1757\n",
    "Epoch #209: Loss:2.4251, Accuracy:0.1840, Validation Loss:2.4549, Validation Accuracy:0.1708\n",
    "Epoch #210: Loss:2.4269, Accuracy:0.1832, Validation Loss:2.4551, Validation Accuracy:0.1724\n",
    "Epoch #211: Loss:2.4260, Accuracy:0.1815, Validation Loss:2.4537, Validation Accuracy:0.1691\n",
    "Epoch #212: Loss:2.4279, Accuracy:0.1791, Validation Loss:2.4541, Validation Accuracy:0.1741\n",
    "Epoch #213: Loss:2.4277, Accuracy:0.1786, Validation Loss:2.4527, Validation Accuracy:0.1757\n",
    "Epoch #214: Loss:2.4257, Accuracy:0.1848, Validation Loss:2.4556, Validation Accuracy:0.1708\n",
    "Epoch #215: Loss:2.4278, Accuracy:0.1897, Validation Loss:2.4516, Validation Accuracy:0.1609\n",
    "Epoch #216: Loss:2.4290, Accuracy:0.1893, Validation Loss:2.4501, Validation Accuracy:0.1741\n",
    "Epoch #217: Loss:2.4279, Accuracy:0.1860, Validation Loss:2.4524, Validation Accuracy:0.1658\n",
    "Epoch #218: Loss:2.4283, Accuracy:0.1881, Validation Loss:2.4545, Validation Accuracy:0.1658\n",
    "Epoch #219: Loss:2.4264, Accuracy:0.1897, Validation Loss:2.4566, Validation Accuracy:0.1609\n",
    "Epoch #220: Loss:2.4269, Accuracy:0.1889, Validation Loss:2.4580, Validation Accuracy:0.1609\n",
    "Epoch #221: Loss:2.4273, Accuracy:0.1881, Validation Loss:2.4546, Validation Accuracy:0.1593\n",
    "Epoch #222: Loss:2.4289, Accuracy:0.1885, Validation Loss:2.4528, Validation Accuracy:0.1593\n",
    "Epoch #223: Loss:2.4293, Accuracy:0.1889, Validation Loss:2.4510, Validation Accuracy:0.1560\n",
    "Epoch #224: Loss:2.4294, Accuracy:0.1877, Validation Loss:2.4512, Validation Accuracy:0.1642\n",
    "Epoch #225: Loss:2.4293, Accuracy:0.1881, Validation Loss:2.4523, Validation Accuracy:0.1544\n",
    "Epoch #226: Loss:2.4281, Accuracy:0.1906, Validation Loss:2.4515, Validation Accuracy:0.1609\n",
    "Epoch #227: Loss:2.4270, Accuracy:0.1877, Validation Loss:2.4515, Validation Accuracy:0.1642\n",
    "Epoch #228: Loss:2.4261, Accuracy:0.1897, Validation Loss:2.4502, Validation Accuracy:0.1593\n",
    "Epoch #229: Loss:2.4253, Accuracy:0.1914, Validation Loss:2.4514, Validation Accuracy:0.1576\n",
    "Epoch #230: Loss:2.4257, Accuracy:0.1901, Validation Loss:2.4483, Validation Accuracy:0.1544\n",
    "Epoch #231: Loss:2.4257, Accuracy:0.1893, Validation Loss:2.4476, Validation Accuracy:0.1576\n",
    "Epoch #232: Loss:2.4263, Accuracy:0.1889, Validation Loss:2.4485, Validation Accuracy:0.1576\n",
    "Epoch #233: Loss:2.4246, Accuracy:0.1885, Validation Loss:2.4478, Validation Accuracy:0.1593\n",
    "Epoch #234: Loss:2.4258, Accuracy:0.1856, Validation Loss:2.4474, Validation Accuracy:0.1658\n",
    "Epoch #235: Loss:2.4252, Accuracy:0.1869, Validation Loss:2.4484, Validation Accuracy:0.1691\n",
    "Epoch #236: Loss:2.4266, Accuracy:0.1856, Validation Loss:2.4465, Validation Accuracy:0.1658\n",
    "Epoch #237: Loss:2.4265, Accuracy:0.1852, Validation Loss:2.4490, Validation Accuracy:0.1675\n",
    "Epoch #238: Loss:2.4275, Accuracy:0.1873, Validation Loss:2.4461, Validation Accuracy:0.1741\n",
    "Epoch #239: Loss:2.4272, Accuracy:0.1885, Validation Loss:2.4447, Validation Accuracy:0.1708\n",
    "Epoch #240: Loss:2.4276, Accuracy:0.1828, Validation Loss:2.4488, Validation Accuracy:0.1773\n",
    "Epoch #241: Loss:2.4268, Accuracy:0.1864, Validation Loss:2.4510, Validation Accuracy:0.1724\n",
    "Epoch #242: Loss:2.4269, Accuracy:0.1844, Validation Loss:2.4495, Validation Accuracy:0.1593\n",
    "Epoch #243: Loss:2.4278, Accuracy:0.1873, Validation Loss:2.4492, Validation Accuracy:0.1626\n",
    "Epoch #244: Loss:2.4299, Accuracy:0.1819, Validation Loss:2.4521, Validation Accuracy:0.1609\n",
    "Epoch #245: Loss:2.4276, Accuracy:0.1869, Validation Loss:2.4533, Validation Accuracy:0.1642\n",
    "Epoch #246: Loss:2.4265, Accuracy:0.1869, Validation Loss:2.4537, Validation Accuracy:0.1609\n",
    "Epoch #247: Loss:2.4260, Accuracy:0.1864, Validation Loss:2.4561, Validation Accuracy:0.1708\n",
    "Epoch #248: Loss:2.4263, Accuracy:0.1869, Validation Loss:2.4562, Validation Accuracy:0.1741\n",
    "Epoch #249: Loss:2.4263, Accuracy:0.1856, Validation Loss:2.4555, Validation Accuracy:0.1626\n",
    "Epoch #250: Loss:2.4262, Accuracy:0.1873, Validation Loss:2.4538, Validation Accuracy:0.1593\n",
    "Epoch #251: Loss:2.4272, Accuracy:0.1840, Validation Loss:2.4536, Validation Accuracy:0.1708\n",
    "Epoch #252: Loss:2.4264, Accuracy:0.1844, Validation Loss:2.4548, Validation Accuracy:0.1609\n",
    "Epoch #253: Loss:2.4269, Accuracy:0.1848, Validation Loss:2.4551, Validation Accuracy:0.1626\n",
    "Epoch #254: Loss:2.4272, Accuracy:0.1840, Validation Loss:2.4563, Validation Accuracy:0.1691\n",
    "Epoch #255: Loss:2.4276, Accuracy:0.1782, Validation Loss:2.4578, Validation Accuracy:0.1560\n",
    "Epoch #256: Loss:2.4272, Accuracy:0.1815, Validation Loss:2.4572, Validation Accuracy:0.1757\n",
    "Epoch #257: Loss:2.4275, Accuracy:0.1856, Validation Loss:2.4572, Validation Accuracy:0.1741\n",
    "Epoch #258: Loss:2.4282, Accuracy:0.1836, Validation Loss:2.4571, Validation Accuracy:0.1593\n",
    "Epoch #259: Loss:2.4273, Accuracy:0.1782, Validation Loss:2.4549, Validation Accuracy:0.1609\n",
    "Epoch #260: Loss:2.4271, Accuracy:0.1840, Validation Loss:2.4526, Validation Accuracy:0.1724\n",
    "Epoch #261: Loss:2.4268, Accuracy:0.1869, Validation Loss:2.4527, Validation Accuracy:0.1593\n",
    "Epoch #262: Loss:2.4268, Accuracy:0.1864, Validation Loss:2.4535, Validation Accuracy:0.1724\n",
    "Epoch #263: Loss:2.4269, Accuracy:0.1856, Validation Loss:2.4538, Validation Accuracy:0.1741\n",
    "Epoch #264: Loss:2.4264, Accuracy:0.1873, Validation Loss:2.4531, Validation Accuracy:0.1708\n",
    "Epoch #265: Loss:2.4255, Accuracy:0.1860, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #266: Loss:2.4252, Accuracy:0.1881, Validation Loss:2.4553, Validation Accuracy:0.1609\n",
    "Epoch #267: Loss:2.4245, Accuracy:0.1873, Validation Loss:2.4558, Validation Accuracy:0.1724\n",
    "Epoch #268: Loss:2.4251, Accuracy:0.1885, Validation Loss:2.4549, Validation Accuracy:0.1691\n",
    "Epoch #269: Loss:2.4271, Accuracy:0.1856, Validation Loss:2.4546, Validation Accuracy:0.1642\n",
    "Epoch #270: Loss:2.4263, Accuracy:0.1791, Validation Loss:2.4558, Validation Accuracy:0.1626\n",
    "Epoch #271: Loss:2.4252, Accuracy:0.1869, Validation Loss:2.4550, Validation Accuracy:0.1658\n",
    "Epoch #272: Loss:2.4252, Accuracy:0.1881, Validation Loss:2.4548, Validation Accuracy:0.1642\n",
    "Epoch #273: Loss:2.4254, Accuracy:0.1848, Validation Loss:2.4556, Validation Accuracy:0.1609\n",
    "Epoch #274: Loss:2.4248, Accuracy:0.1860, Validation Loss:2.4544, Validation Accuracy:0.1691\n",
    "Epoch #275: Loss:2.4245, Accuracy:0.1885, Validation Loss:2.4563, Validation Accuracy:0.1708\n",
    "Epoch #276: Loss:2.4238, Accuracy:0.1881, Validation Loss:2.4561, Validation Accuracy:0.1741\n",
    "Epoch #277: Loss:2.4232, Accuracy:0.1893, Validation Loss:2.4552, Validation Accuracy:0.1724\n",
    "Epoch #278: Loss:2.4237, Accuracy:0.1881, Validation Loss:2.4552, Validation Accuracy:0.1724\n",
    "Epoch #279: Loss:2.4236, Accuracy:0.1819, Validation Loss:2.4553, Validation Accuracy:0.1741\n",
    "Epoch #280: Loss:2.4229, Accuracy:0.1889, Validation Loss:2.4547, Validation Accuracy:0.1757\n",
    "Epoch #281: Loss:2.4234, Accuracy:0.1885, Validation Loss:2.4565, Validation Accuracy:0.1724\n",
    "Epoch #282: Loss:2.4225, Accuracy:0.1885, Validation Loss:2.4564, Validation Accuracy:0.1724\n",
    "Epoch #283: Loss:2.4241, Accuracy:0.1881, Validation Loss:2.4559, Validation Accuracy:0.1757\n",
    "Epoch #284: Loss:2.4225, Accuracy:0.1881, Validation Loss:2.4554, Validation Accuracy:0.1741\n",
    "Epoch #285: Loss:2.4236, Accuracy:0.1885, Validation Loss:2.4563, Validation Accuracy:0.1691\n",
    "Epoch #286: Loss:2.4219, Accuracy:0.1881, Validation Loss:2.4565, Validation Accuracy:0.1691\n",
    "Epoch #287: Loss:2.4217, Accuracy:0.1893, Validation Loss:2.4554, Validation Accuracy:0.1708\n",
    "Epoch #288: Loss:2.4221, Accuracy:0.1877, Validation Loss:2.4563, Validation Accuracy:0.1724\n",
    "Epoch #289: Loss:2.4231, Accuracy:0.1852, Validation Loss:2.4580, Validation Accuracy:0.1576\n",
    "Epoch #290: Loss:2.4230, Accuracy:0.1881, Validation Loss:2.4548, Validation Accuracy:0.1757\n",
    "Epoch #291: Loss:2.4233, Accuracy:0.1881, Validation Loss:2.4534, Validation Accuracy:0.1708\n",
    "Epoch #292: Loss:2.4229, Accuracy:0.1844, Validation Loss:2.4537, Validation Accuracy:0.1576\n",
    "Epoch #293: Loss:2.4226, Accuracy:0.1852, Validation Loss:2.4521, Validation Accuracy:0.1691\n",
    "Epoch #294: Loss:2.4226, Accuracy:0.1869, Validation Loss:2.4527, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4219, Accuracy:0.1869, Validation Loss:2.4551, Validation Accuracy:0.1724\n",
    "Epoch #296: Loss:2.4216, Accuracy:0.1881, Validation Loss:2.4559, Validation Accuracy:0.1675\n",
    "Epoch #297: Loss:2.4217, Accuracy:0.1885, Validation Loss:2.4564, Validation Accuracy:0.1741\n",
    "Epoch #298: Loss:2.4224, Accuracy:0.1864, Validation Loss:2.4556, Validation Accuracy:0.1675\n",
    "Epoch #299: Loss:2.4226, Accuracy:0.1852, Validation Loss:2.4574, Validation Accuracy:0.1544\n",
    "Epoch #300: Loss:2.4217, Accuracy:0.1832, Validation Loss:2.4554, Validation Accuracy:0.1741\n",
    "\n",
    "Test:\n",
    "Test Loss:2.45539117, Accuracy:0.1741\n",
    "Labels: ['ds', 'ck', 'eo', 'by', 'my', 'yd', 'eb', 'sg', 'aa', 'ce', 'ib', 'mb', 'eg', 'ek', 'sk']\n",
    "Confusion Matrix:\n",
    "      ds  ck  eo  by  my  yd  eb  sg  aa  ce  ib  mb  eg  ek  sk\n",
    "t:ds   7   0   0   3   0   0   0   9   0   0   0   0  12   0   0\n",
    "t:ck   1   0   0   4   0   0   1   8   0   0   0   0   9   0   0\n",
    "t:eo   0   0   0   2   0   1   1  26   0   0   1   0   3   0   0\n",
    "t:by   3   0   0   5   0   1   2  21   0   0   0   0   7   1   0\n",
    "t:my   3   0   0   1   0   2   1   7   0   0   0   0   6   0   0\n",
    "t:yd   0   0   0   0   0  31   0  27   0   0   4   0   0   0   0\n",
    "t:eb   2   0   0   6   0   4   4  19   0   0   0   0  13   2   0\n",
    "t:sg   0   0   0   3   0  10   1  30   0   0   2   0   4   1   0\n",
    "t:aa   4   0   0   3   0   1   2  11   0   0   0   0  13   0   0\n",
    "t:ce   2   0   0   1   0   0   1  13   0   0   0   0  10   0   0\n",
    "t:ib   1   0   0   2   0  29   0  15   0   0   2   0   5   0   0\n",
    "t:mb   2   0   0   4   0   7   0  25   0   0   1   0  12   1   0\n",
    "t:eg   3   0   0   7   0   0   1  11   0   0   1   0  27   0   0\n",
    "t:ek   2   0   0   8   0   6   0  14   0   0   1   0  17   0   0\n",
    "t:sk   2   0   0   6   0   2   0  11   0   0   0   0  12   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ds       0.22      0.23      0.22        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          by       0.09      0.12      0.11        40\n",
    "          my       0.00      0.00      0.00        20\n",
    "          yd       0.33      0.50      0.40        62\n",
    "          eb       0.29      0.08      0.12        50\n",
    "          sg       0.12      0.59      0.20        51\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ib       0.17      0.04      0.06        54\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eg       0.18      0.54      0.27        50\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          sk       0.00      0.00      0.00        33\n",
    "\n",
    "    accuracy                           0.17       609\n",
    "   macro avg       0.09      0.14      0.09       609\n",
    "weighted avg       0.11      0.17      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 13:14:32 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 52 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7074882115049315, 2.7008486549646786, 2.6957960195337805, 2.691135004823431, 2.686438145504405, 2.6821150599637837, 2.6772177873182375, 2.6732757698334693, 2.6684838793743615, 2.663090065390801, 2.6573495712186315, 2.651244268041526, 2.642782883886829, 2.6356368969226707, 2.6213325635944487, 2.6096258672391643, 2.5979064877201576, 2.5858693103289174, 2.575144894996104, 2.561029301488341, 2.5492291798928295, 2.5410352708475146, 2.538549439660434, 2.528780967731194, 2.5211955592745827, 2.5143042135316946, 2.5112694425535906, 2.504441196303845, 2.4958769601749866, 2.492374649188789, 2.4887882649213418, 2.481670331485166, 2.48807804690206, 2.5627806437230856, 2.4973031096466265, 2.640131922191, 2.5061426256677786, 2.4880859225450087, 2.49478189698581, 2.4818033964567388, 2.5508759393676357, 2.491122721451257, 2.4985059525187574, 2.4793746252169555, 2.483074037508033, 2.475222013266803, 2.4748433321371848, 2.4706814152070846, 2.470776120234397, 2.471076538214347, 2.4681123042928763, 2.466277042241715, 2.463650575020826, 2.4633082407840172, 2.4606139742094895, 2.4604188796921904, 2.461346310347759, 2.459808437303565, 2.4577566770888706, 2.4601824565474035, 2.4594999514581337, 2.4565624486049407, 2.4590715085735853, 2.4591614620634683, 2.457759849347896, 2.456898833339046, 2.4580810113101954, 2.4599905847915875, 2.4550087459764653, 2.457586898788051, 2.456666812520896, 2.4555839180750603, 2.4562140803031736, 2.456034907370757, 2.454793959807097, 2.4557761010669528, 2.4547378609724624, 2.452428492419238, 2.4508580867880085, 2.450714031855265, 2.4488255402137495, 2.4449768238662695, 2.4469565742317285, 2.4481652053118927, 2.4424492334105894, 2.460786348493228, 2.4611713259873915, 2.4667790174875743, 2.4604557445288098, 2.458167024825398, 2.45600244799271, 2.4598375482512225, 2.4575719089539376, 2.4544519818279347, 2.4543567353672975, 2.457568322104969, 2.451703019134321, 2.452368634479191, 2.452363722430074, 2.4529843212935725, 2.4533996601605845, 2.4562902885117555, 2.458132329636998, 2.4576878896096264, 2.4567324230432117, 2.455712192360012, 2.4572138833294948, 2.4560355321918608, 2.4571670577639626, 2.4595362732954604, 2.4551939564972676, 2.4520094355534647, 2.4509491231445413, 2.4476427292001657, 2.4459526366592432, 2.4458756466412974, 2.4466223842013255, 2.4466916974542174, 2.4475263656653796, 2.448043962221819, 2.450554593247537, 2.450359489725924, 2.4520124750967294, 2.452316452129721, 2.4524881953284856, 2.4518342386129848, 2.453276068118993, 2.4545173778126785, 2.4526438247198348, 2.4552533004084243, 2.4550508814687997, 2.453954076140581, 2.453810380205928, 2.4536074217904376, 2.4533004717678075, 2.4534386226109097, 2.4528093674695746, 2.4525286497545165, 2.4546991373322085, 2.4535900892686766, 2.4539872918810164, 2.4493124602463445, 2.4472515700485906, 2.44899875503064, 2.450820911889789, 2.45011913326182, 2.453016027049674, 2.454957726749489, 2.4536148546560255, 2.452878641573275, 2.454693069207453, 2.4553242371783077, 2.4544641184689375, 2.4541025200892355, 2.452699642071779, 2.452864682145894, 2.4506243997802484, 2.451123048910758, 2.4508947226018547, 2.45045848002379, 2.4523543101813408, 2.452720709426454, 2.454680371558529, 2.4559266911743114, 2.457870762140684, 2.457957508920253, 2.458693631176878, 2.461016721521888, 2.4625593969974613, 2.4595933160171133, 2.458067529894448, 2.4574010579652583, 2.457085633317042, 2.4565496123678776, 2.457498500695565, 2.457440149216425, 2.4568790348096825, 2.4567003359739807, 2.45542377947978, 2.455260485851119, 2.457562608671893, 2.4576649975111136, 2.4594158656491434, 2.4587436517079673, 2.4563705674533187, 2.455541779842283, 2.4580218267362497, 2.456155221841997, 2.4559857845306396, 2.4564771710945466, 2.452236338397748, 2.4516451918628612, 2.455482882231914, 2.452800000633904, 2.4529914946000173, 2.4523618538391414, 2.4525172827866277, 2.4527589025951566, 2.4548062741854313, 2.457442801182689, 2.45769007648349, 2.4600377689637183, 2.4598394622552178, 2.45681691835275, 2.4562667821624204, 2.457480201580254, 2.4580427261408913, 2.4580989040568935, 2.4548675426708653, 2.4550814933964773, 2.453738935279533, 2.4541466991693905, 2.452695516529929, 2.4556470982155383, 2.451620246389229, 2.450070545199665, 2.452440122861189, 2.454459086623294, 2.4566156112501774, 2.4579578440373364, 2.4545538245359273, 2.452849565076906, 2.4510354662959406, 2.4512150080137456, 2.4523488978055505, 2.451492004206615, 2.4514564203315574, 2.4502225427204753, 2.4514433139650693, 2.448343983620454, 2.4476088988174163, 2.4484978035361507, 2.447820223415231, 2.4473648267035024, 2.4484427159251445, 2.4464860608425045, 2.448977719778302, 2.4460526475765434, 2.444693271358221, 2.4488070860676383, 2.450969755551694, 2.4494877605406913, 2.449169577440409, 2.452088197855331, 2.4533210528895184, 2.4536737741899413, 2.4560688195753175, 2.4562154336907396, 2.455510817529337, 2.4537810813421492, 2.453577912695498, 2.4547694312723594, 2.455122132998186, 2.456314846408387, 2.4577850943128463, 2.4572322873646404, 2.457162107348638, 2.4570957005317577, 2.4548945736219534, 2.4526251008357907, 2.452743376417113, 2.4534838900386013, 2.453753154657549, 2.4531006335429173, 2.4553397226411917, 2.4552555957255495, 2.4557589259249433, 2.4548558781691177, 2.4546249014599177, 2.455773725885476, 2.4550488668513806, 2.4547953997143774, 2.4555677772547027, 2.4543740432250676, 2.4562690258026123, 2.4560871684101024, 2.4552198211939267, 2.4552444229376533, 2.4553294898253943, 2.4547151881094247, 2.456498626809206, 2.4563813577536098, 2.4558563858808946, 2.45536972697341, 2.456326142516238, 2.4565306304906587, 2.4553632438672195, 2.4562576169450883, 2.4580295410845276, 2.454796036671731, 2.453359880666623, 2.453731392404716, 2.452105004994936, 2.452734809790926, 2.4550577948246097, 2.4559427024108436, 2.4564311617896673, 2.455622512131489, 2.457381593378502, 2.4553910339211398], 'val_acc': [0.06403940866513205, 0.08374384215655194, 0.07224958918511575, 0.0853858781834737, 0.08374384196080598, 0.08374384196080598, 0.08374384196080598, 0.08374384196080598, 0.08374384196080598, 0.08374384196080598, 0.10837438313538218, 0.11494252872339807, 0.1280788168286651, 0.12315270835640786, 0.12972085275771386, 0.12315270825853489, 0.13957307059531923, 0.14121510662224102, 0.1346469611320981, 0.1362889981499838, 0.15270935860271329, 0.15270935879845923, 0.15106732277153748, 0.1527093589942052, 0.15599343143954067, 0.15435139502112696, 0.1625615761344656, 0.15106732277153748, 0.15599343163528662, 0.1592775038848761, 0.1527093589942052, 0.15270935918995115, 0.1658456477968172, 0.14285714195181778, 0.16584564838405508, 0.1280788167307921, 0.1642036120635144, 0.16091953932456, 0.16091953942243298, 0.17077175617120144, 0.14614121449502623, 0.17241379258961514, 0.17733990096399938, 0.17241379249174216, 0.16912972034002563, 0.17077175646482037, 0.17241379249174216, 0.17569786474133164, 0.1740558286165369, 0.1707717563669474, 0.16912972024215267, 0.1707717563669474, 0.16912972024215267, 0.17077175646482037, 0.17241379258961514, 0.1707717563669474, 0.17241379249174216, 0.17241379249174216, 0.17241379249174216, 0.1707717563669474, 0.17241379249174216, 0.17898193708879412, 0.17077175626907443, 0.16912972014427968, 0.1707717563669474, 0.16912972024215267, 0.16912972024215267, 0.16748768411735793, 0.16912972024215267, 0.16584564809043614, 0.16091953952030597, 0.17077175646482037, 0.16912972034002563, 0.16912972024215267, 0.17241379258961514, 0.17077175646482037, 0.1724137926874881, 0.17077175646482037, 0.16912972034002563, 0.1707717563669474, 0.1707717563669474, 0.1740558286165369, 0.1642036118677684, 0.16912972024215267, 0.17241379239386917, 0.18062397311571587, 0.1658456477968172, 0.1642036119656414, 0.1642036119656414, 0.1740558287144099, 0.17241379258961514, 0.16748768431310387, 0.16748768431310387, 0.15927750359125717, 0.1642036119656414, 0.15927750349338418, 0.16256157574297367, 0.16584564809043614, 0.1642036118677684, 0.16912972034002563, 0.1642036118677684, 0.16584564799256318, 0.1658456478946902, 0.16420361167202246, 0.16091953952030597, 0.16420361176989545, 0.1592775033955112, 0.16091953952030597, 0.16091953942243298, 0.16420361167202246, 0.1691297200464067, 0.17077175617120144, 0.16256157544935473, 0.16420361167202246, 0.15927750319976525, 0.1658456477968172, 0.17077175626907443, 0.1658456477968172, 0.16420361176989545, 0.16748768401948494, 0.16748768401948494, 0.16420361176989545, 0.17077175646482037, 0.17733990115974532, 0.17241379258961514, 0.17241379258961514, 0.16748768411735793, 0.1658456478946902, 0.17569786474133164, 0.16912972014427968, 0.16420361167202246, 0.1658456478946902, 0.1658456478946902, 0.1642036118677684, 0.16256157574297367, 0.16420361176989545, 0.16420361176989545, 0.16256157554722772, 0.16420361167202246, 0.16912972014427968, 0.1625615756451007, 0.16256157574297367, 0.16256157554722772, 0.16748768401948494, 0.16748768392161195, 0.1658456477968172, 0.16748768401948494, 0.1707717563669474, 0.17569786474133164, 0.16912972024215267, 0.16912972024215267, 0.16912972024215267, 0.16748768411735793, 0.16912972024215267, 0.17241379249174216, 0.1740558286165369, 0.1740558286165369, 0.17569786483920463, 0.16748768411735793, 0.17898193708879412, 0.17898193708879412, 0.17733990106187233, 0.17733990115974532, 0.18062397331146185, 0.17569786483920463, 0.17733990106187233, 0.1789819371866671, 0.1740558287144099, 0.17405582881228285, 0.1789819371866671, 0.1674876842152309, 0.17241379258961514, 0.17405582881228285, 0.16912972024215267, 0.18062397331146185, 0.1724137926874881, 0.17405582881228285, 0.1756978649370776, 0.17898193708879412, 0.17405582881228285, 0.17405582881228285, 0.1756978649370776, 0.1674876842152309, 0.17077175646482037, 0.17569786483920463, 0.1740558287144099, 0.16912972034002563, 0.17898193708879412, 0.16091953961817893, 0.16584564799256318, 0.1658456478946902, 0.16748768401948494, 0.15927750359125717, 0.1773399007682534, 0.1740558286165369, 0.1707717563669474, 0.16256157574297367, 0.1822660093383836, 0.1740558286165369, 0.1642036118677684, 0.15763546727071645, 0.16256157584084666, 0.1642036119656414, 0.17733990115974532, 0.15763546736858944, 0.16091953961817893, 0.16256157574297367, 0.17569786503495058, 0.1707717563669474, 0.17241379258961514, 0.16912972024215267, 0.1740558287144099, 0.17569786483920463, 0.17077175626907443, 0.1609195397160519, 0.17405582881228285, 0.16584564809043614, 0.16584564809043614, 0.16091953961817893, 0.16091953991179786, 0.15927750368913016, 0.15927750359125717, 0.1559934312437947, 0.16420361216138737, 0.15435139511899995, 0.1609195397160519, 0.1642036120635144, 0.15927750359125717, 0.1576354675643354, 0.15435139521687294, 0.15763546736858944, 0.15763546736858944, 0.15927750349338418, 0.16584564809043614, 0.16912972043789862, 0.16584564818830913, 0.16748768441097686, 0.17405582891015584, 0.17077175666056635, 0.17733990115974532, 0.17241379258961514, 0.15927750359125717, 0.16256157584084666, 0.1609195397160519, 0.1642036119656414, 0.16091953961817893, 0.1707717563669474, 0.1740558287144099, 0.16256157584084666, 0.15927750359125717, 0.17077175656269336, 0.1609195397160519, 0.16256157584084666, 0.16912972024215267, 0.1559934311459217, 0.17569786483920463, 0.1740558287144099, 0.15927750359125717, 0.1609195397160519, 0.17241379258961514, 0.15927750349338418, 0.17241379239386917, 0.1740558286165369, 0.17077175646482037, 0.1740558286165369, 0.1609195397160519, 0.17241379249174216, 0.16912972024215267, 0.1642036118677684, 0.16256157593871964, 0.16584564799256318, 0.1642036118677684, 0.1609195398139249, 0.16912972024215267, 0.1707717563669474, 0.1740558287144099, 0.17241379258961514, 0.17241379258961514, 0.1740558287144099, 0.17569786483920463, 0.17241379258961514, 0.17241379249174216, 0.17569786483920463, 0.1740558286165369, 0.16912972024215267, 0.16912972024215267, 0.17077175646482037, 0.17241379258961514, 0.15763546746646243, 0.17569786483920463, 0.17077175646482037, 0.15763546736858944, 0.16912972024215267, 0.16912972024215267, 0.17241379258961514, 0.16748768401948494, 0.1740558286165369, 0.16748768411735793, 0.15435139502112696, 0.1740558286165369], 'loss': [2.715625994993676, 2.705601891061364, 2.6990222382594427, 2.69432492579278, 2.689527671978459, 2.6852217196439083, 2.680732125765979, 2.67614631868241, 2.6712785246925432, 2.6661645050166323, 2.6606913438323097, 2.6543546745174966, 2.64691590479512, 2.637203587937404, 2.628611992712628, 2.617707899904349, 2.606693772270939, 2.5943739014729337, 2.582602115039708, 2.5709385204119357, 2.558808746279143, 2.5489268158005984, 2.5403617643477734, 2.532747365121234, 2.524101862427635, 2.516177181835292, 2.5117444681680667, 2.504521647126278, 2.4990958283324507, 2.4931609928240768, 2.4891421411071715, 2.488529967772153, 2.482217038779288, 2.5010499527322194, 2.518035166464302, 2.5556172663671037, 2.6093145240258875, 2.4919097196394904, 2.492077392033728, 2.480066177047009, 2.485036651603495, 2.5185926280716853, 2.4881046559531588, 2.4853552884879297, 2.47318014334849, 2.472885911910196, 2.4669413394262167, 2.466342915940334, 2.4630031324265182, 2.4631592841608567, 2.4636464735810515, 2.46343459564068, 2.4616769820023365, 2.4611786100164332, 2.4598278550886277, 2.457658293996259, 2.456099656964719, 2.4562270502290198, 2.45442001168488, 2.4544585399314363, 2.453070777250756, 2.4546295407861165, 2.4514619540384905, 2.4512433044229933, 2.4502097977749866, 2.450814431697681, 2.4491713916496574, 2.4493287203003495, 2.4491850800093196, 2.450048721203814, 2.4504302420410533, 2.450637560752383, 2.4469475107760887, 2.4462640578252333, 2.446545895903507, 2.446169218439341, 2.4457440487902757, 2.444119389492873, 2.4445922010488332, 2.4443399662354643, 2.444944330407364, 2.444140350059807, 2.444004553788986, 2.4459906619187497, 2.4450969726398006, 2.4823420231836777, 2.4715187205915825, 2.447534073353793, 2.4570559150139655, 2.4473831477351258, 2.4509717905056307, 2.4438185678125652, 2.4464609384047176, 2.4448765900590335, 2.4445183066616805, 2.4451114113081163, 2.443320624294712, 2.4424088260721133, 2.442697438958734, 2.443451877785904, 2.4444503401339177, 2.4439377370556277, 2.443138283429939, 2.4441094337792366, 2.4434307752448676, 2.443410543543602, 2.4436120520871767, 2.4442353678435027, 2.4430835118773535, 2.4441363622520496, 2.4434459837065585, 2.443512737579659, 2.4419405330133146, 2.4414584452611465, 2.440990189507267, 2.440272186130469, 2.4383987594189342, 2.4384219015893014, 2.438141200185067, 2.437489333828372, 2.4378224581418833, 2.4385171808990855, 2.4368626480964175, 2.4363688209463192, 2.4354501747742328, 2.434901758287966, 2.434916511747137, 2.4350484230190332, 2.435061693387355, 2.4370287629857934, 2.435490190419818, 2.4356752472002157, 2.4351132239159616, 2.4352549104475143, 2.4344577267429424, 2.434214714763101, 2.433952523354877, 2.4333459255876484, 2.4337854075970347, 2.438184886203899, 2.4368928969028794, 2.4346480568331614, 2.4342165481872873, 2.433826640499201, 2.4345461685799474, 2.434038956503114, 2.4338101324359496, 2.4335129984595203, 2.4323296389295823, 2.4323032522103625, 2.4327115761915517, 2.434749812805677, 2.4340352844653435, 2.433346492798666, 2.433103927107073, 2.4320017112598773, 2.4307687729046332, 2.4312626403949587, 2.4302115942663236, 2.4300165803035916, 2.4291599590920323, 2.4292960133640675, 2.429127839752291, 2.431130972633127, 2.4303028647170173, 2.4304031093017766, 2.430758430238132, 2.430871586143603, 2.4307769416049276, 2.430486044110214, 2.4294204882282986, 2.429432286178307, 2.429624318391146, 2.429625014945467, 2.4298926317226717, 2.4298058157339235, 2.4293128703898716, 2.428881564228442, 2.4288256728428834, 2.430140166018288, 2.430160949949856, 2.4306378625012033, 2.4296708240156546, 2.4293342185950624, 2.429566698446411, 2.4288687639412694, 2.4282207627071246, 2.4291288377812754, 2.4279652956819633, 2.4318957524622733, 2.4300989326265556, 2.4321035663205253, 2.4302119947312057, 2.4302044523814863, 2.430730171321109, 2.428619206465735, 2.4285698497075074, 2.4275213240598017, 2.4274169992372485, 2.428238160899043, 2.4275629741700033, 2.4270881678289458, 2.427029349329046, 2.42683274016488, 2.426153525189942, 2.4255582479480844, 2.425762144398151, 2.4256776883127262, 2.425148439701088, 2.4268847492930825, 2.425999352574593, 2.427897243480173, 2.427658633578729, 2.4256614088767363, 2.4278247304520812, 2.428969261631584, 2.4279464933172146, 2.428250664313471, 2.4263935164504473, 2.4269145788353326, 2.4272890164377263, 2.4289065056512977, 2.429298180574264, 2.4293750378140677, 2.4292838219989252, 2.4281180136012837, 2.4269519817657783, 2.426133834361051, 2.4252636073306357, 2.425730956310609, 2.425673986705177, 2.4262921998143443, 2.42455239589699, 2.4258402092990443, 2.425211164642898, 2.426596830219214, 2.426460593143283, 2.427469398451537, 2.4272309134873034, 2.427572408691814, 2.4268297123957954, 2.42686780688699, 2.427795837449342, 2.429904210934649, 2.427620610270412, 2.426501571667023, 2.425963150745055, 2.4262595974689147, 2.4262973180296976, 2.426189781361292, 2.4271697838448403, 2.4264057758652453, 2.42690085806641, 2.4271717889108206, 2.4276484345508553, 2.4272051211989636, 2.4274748761061526, 2.428216584164504, 2.4273238116466045, 2.4270840554756306, 2.4268451054238196, 2.426795007071211, 2.4269446437608533, 2.4264163766308733, 2.4254763788266347, 2.4251855996110354, 2.424502951604385, 2.42507814097943, 2.427131730868831, 2.4263261115526515, 2.425161222853455, 2.4252099748999187, 2.425373339701972, 2.42475105066319, 2.424503259149665, 2.4238382602863977, 2.4232047791843296, 2.4236664611456087, 2.423630286829672, 2.4229060226886916, 2.423384287518887, 2.4224758162880335, 2.4241380334145233, 2.4224771666086187, 2.423644374772998, 2.421873883006509, 2.421692075768535, 2.4220806540894557, 2.4231408532395253, 2.423020418077034, 2.4232660497238503, 2.4228562257128328, 2.4226213533530734, 2.422603254494481, 2.421913803578402, 2.4216024989220153, 2.421708702502554, 2.4223777775891753, 2.4225583984132175, 2.421732256887385], 'acc': [0.06611909646334345, 0.06611909667293883, 0.0726899381658135, 0.08377823462782455, 0.08336755682310774, 0.08295687840337382, 0.08295687860837953, 0.08295687881338523, 0.08295687880420587, 0.08295687859920016, 0.089938398293034, 0.1125256672960532, 0.11457905536804355, 0.11909651034176962, 0.11745379889953797, 0.12484599641644735, 0.1145790547622058, 0.1293634504018623, 0.13716632534101514, 0.15605749477473618, 0.15687884999251708, 0.15564681754831905, 0.14661190997832121, 0.15195071859403803, 0.1564681716095006, 0.15728952746065736, 0.1568788504025285, 0.15564681635500224, 0.15687885020670214, 0.15564681833162444, 0.15441478510412102, 0.1568788507941812, 0.15564681674665495, 0.1581108830609116, 0.1548254629180172, 0.1445585220746191, 0.1199178649904302, 0.16632443442481745, 0.15975359353319085, 0.16960985595685502, 0.16057494898351557, 0.1577002050511891, 0.1691991791220906, 0.1724846000482904, 0.17330595592698522, 0.16878850150402078, 0.1757700200320759, 0.1761806976501457, 0.17494866540177403, 0.1745379881753569, 0.1716632432638987, 0.17207392109615358, 0.17289527631393448, 0.17453798758787786, 0.17371663274339091, 0.17494866518758895, 0.17535934399897557, 0.17494866520594768, 0.17289527829055668, 0.17412730996980805, 0.1720739212736212, 0.1704312109971683, 0.17207392285859072, 0.17371663331251125, 0.1737166335266963, 0.17289527750725128, 0.17289527829055668, 0.17330595373617794, 0.17577002042372858, 0.17987679640860038, 0.17782340812242498, 0.17494866598925307, 0.1798767968002531, 0.17864476335856458, 0.174127309382329, 0.1782340865421589, 0.17577001944459683, 0.17782340931574178, 0.17782340931574178, 0.1778234079082399, 0.17741273208932465, 0.17659137509074788, 0.1745379885670096, 0.17659137546404186, 0.17453798858536831, 0.1593429161109474, 0.17700205366959074, 0.18357289596871917, 0.1716632440472041, 0.17535934182652702, 0.1782340869338116, 0.17330595410947192, 0.1765913768348263, 0.17453798837118326, 0.17248459889169102, 0.17453798778370422, 0.1716632448488682, 0.17618069786433077, 0.17659137606987962, 0.17412730897231757, 0.17700205308211167, 0.17002053298744577, 0.17659137526821553, 0.17453798717786645, 0.17535934399897557, 0.17535934202235337, 0.17412731075311344, 0.1716632452405209, 0.17700205349212308, 0.17659137466237776, 0.17535934380314924, 0.17289527709723987, 0.17453798876283594, 0.17535934341149653, 0.1778234094932094, 0.17987679640860038, 0.18069815303388317, 0.17782340931574178, 0.1741273109489398, 0.1819301843031231, 0.17535934319731147, 0.17782340812242498, 0.1815195078783701, 0.17494866440428355, 0.1778234085324364, 0.1765913756598682, 0.177002052317165, 0.1770020538837758, 0.17577002083374, 0.17248460047666053, 0.17659137587405327, 0.1786447641602287, 0.177002052690459, 0.17618069825598345, 0.17577002102956635, 0.17577002022790222, 0.17700205327793803, 0.1827515393434364, 0.17946611898635692, 0.17371663333086998, 0.17535934300148512, 0.18069815301552444, 0.1819301838931117, 0.18234086190283422, 0.18069815283805682, 0.18316221774481162, 0.1819301854964399, 0.17864476453352268, 0.1823408625270307, 0.18398357317677758, 0.181108830651953, 0.17864476472934904, 0.17823408712963792, 0.18316221737151764, 0.18316221831393192, 0.1827515409284059, 0.179466120161315, 0.18193018371564407, 0.17453798815699817, 0.17946611900471565, 0.17946611980637975, 0.17946612017967373, 0.17823408534884208, 0.17577001983624954, 0.17905544156411346, 0.17741273011270245, 0.18110882945863618, 0.18398357239347218, 0.1770020527088177, 0.17659137626570598, 0.17741273210768338, 0.1761806992167565, 0.1753593426098324, 0.17659137626570598, 0.17987679582112134, 0.17700205386541706, 0.17782340794495732, 0.1815195070950647, 0.17700205249463263, 0.18234086209866054, 0.1774127311101929, 0.17987679758355848, 0.18357289459793474, 0.17741273011270245, 0.17946611879053057, 0.18685831530994948, 0.17946611978802104, 0.17741273169767197, 0.1868583157199609, 0.1757700196404232, 0.17823408650544145, 0.18151950705834727, 0.17535934300148512, 0.18069815184056637, 0.18151950746835868, 0.18398357376425664, 0.17494866557924166, 0.18151950744999998, 0.18357289418792333, 0.1819301850864285, 0.18275154110587352, 0.18891170361448362, 0.18357289536288143, 0.18644763751441204, 0.18521560483767022, 0.18850102658389287, 0.1864476374960533, 0.18316221776317032, 0.18398357241183091, 0.1831622185464757, 0.18151950607921552, 0.17905544037079665, 0.17864476472934904, 0.18480492880456992, 0.18973305844061183, 0.18932238121419473, 0.18603696206879078, 0.18809034816415893, 0.1897330594381023, 0.1889117035961249, 0.18809034896582305, 0.18850102736719826, 0.18891170420196268, 0.18767967097445926, 0.18809034896582305, 0.19055441467424192, 0.1876796723452437, 0.18973305924227596, 0.19137576950037014, 0.19014373686034577, 0.18932238084090075, 0.1889117035961249, 0.1885010255864024, 0.1856262826515664, 0.18685831552413454, 0.18562628441400353, 0.18521560583516067, 0.18726899372968342, 0.18850102636970778, 0.18275154032256813, 0.18644763927684918, 0.1843942504031947, 0.18726899353385706, 0.18193018469477582, 0.1868583157199609, 0.1868583153283082, 0.18644763790606472, 0.18685831728657168, 0.18562628225991368, 0.18726899273219294, 0.18398357298095122, 0.18439424942406296, 0.18480492882292862, 0.1839835727851249, 0.17823408534884208, 0.18151950768254377, 0.1856262826699251, 0.18357289516705508, 0.17823408513465702, 0.18398357296259252, 0.18685831589742852, 0.1864476384935438, 0.18562628247409876, 0.18726899372968342, 0.18603696087547397, 0.1880903491800081, 0.1872689941213361, 0.18850102619224016, 0.18562628225991368, 0.17905544234741885, 0.1868583153283082, 0.18809034835998528, 0.1848049284129172, 0.18603696087547397, 0.18850102775885094, 0.18809034837834399, 0.18932238103672708, 0.18809034974912844, 0.18193018391147042, 0.18891170478944172, 0.18850102619224016, 0.1885010263880665, 0.18809035014078113, 0.18809034974912844, 0.18850102697554555, 0.18809035014078113, 0.18932238201585883, 0.1876796705460891, 0.18521560526604036, 0.18809034914329067, 0.18809034974912844, 0.18439424942406296, 0.18521560505185528, 0.18685831669909264, 0.18685831709074532, 0.1880903487883554, 0.18850102758138332, 0.18644763751441204, 0.18521560563933434, 0.18316221754898526]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
