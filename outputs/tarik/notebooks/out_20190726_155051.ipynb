{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf9.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 15:50:51 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'Split', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000023B0131CE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000023B54D07EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0763, Accuracy:0.3924, Validation Loss:1.0750, Validation Accuracy:0.3896\n",
    "Epoch #2: Loss:1.0742, Accuracy:0.3972, Validation Loss:1.0744, Validation Accuracy:0.3904\n",
    "Epoch #3: Loss:1.0743, Accuracy:0.3958, Validation Loss:1.0746, Validation Accuracy:0.3949\n",
    "Epoch #4: Loss:1.0736, Accuracy:0.3989, Validation Loss:1.0744, Validation Accuracy:0.3883\n",
    "Epoch #5: Loss:1.0736, Accuracy:0.4001, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #6: Loss:1.0737, Accuracy:0.3966, Validation Loss:1.0742, Validation Accuracy:0.3867\n",
    "Epoch #7: Loss:1.0736, Accuracy:0.3950, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #8: Loss:1.0739, Accuracy:0.3965, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #9: Loss:1.0731, Accuracy:0.3980, Validation Loss:1.0739, Validation Accuracy:0.3888\n",
    "Epoch #10: Loss:1.0727, Accuracy:0.4014, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #11: Loss:1.0727, Accuracy:0.4021, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #12: Loss:1.0729, Accuracy:0.3989, Validation Loss:1.0740, Validation Accuracy:0.3961\n",
    "Epoch #13: Loss:1.0733, Accuracy:0.4020, Validation Loss:1.0741, Validation Accuracy:0.3945\n",
    "Epoch #14: Loss:1.0734, Accuracy:0.3945, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #15: Loss:1.0732, Accuracy:0.3990, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #16: Loss:1.0727, Accuracy:0.4041, Validation Loss:1.0735, Validation Accuracy:0.3937\n",
    "Epoch #17: Loss:1.0734, Accuracy:0.3970, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #18: Loss:1.0721, Accuracy:0.4018, Validation Loss:1.0742, Validation Accuracy:0.3805\n",
    "Epoch #19: Loss:1.0729, Accuracy:0.4021, Validation Loss:1.0754, Validation Accuracy:0.3920\n",
    "Epoch #20: Loss:1.0735, Accuracy:0.3973, Validation Loss:1.0746, Validation Accuracy:0.3949\n",
    "Epoch #21: Loss:1.0723, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0732, Accuracy:0.3990, Validation Loss:1.0764, Validation Accuracy:0.3863\n",
    "Epoch #23: Loss:1.0731, Accuracy:0.4006, Validation Loss:1.0740, Validation Accuracy:0.3945\n",
    "Epoch #24: Loss:1.0722, Accuracy:0.4045, Validation Loss:1.0740, Validation Accuracy:0.3945\n",
    "Epoch #25: Loss:1.0720, Accuracy:0.3974, Validation Loss:1.0739, Validation Accuracy:0.3945\n",
    "Epoch #26: Loss:1.0717, Accuracy:0.4020, Validation Loss:1.0730, Validation Accuracy:0.3830\n",
    "Epoch #27: Loss:1.0726, Accuracy:0.4002, Validation Loss:1.0745, Validation Accuracy:0.3949\n",
    "Epoch #28: Loss:1.0720, Accuracy:0.4054, Validation Loss:1.0738, Validation Accuracy:0.3846\n",
    "Epoch #29: Loss:1.0726, Accuracy:0.4006, Validation Loss:1.0739, Validation Accuracy:0.3888\n",
    "Epoch #30: Loss:1.0722, Accuracy:0.4065, Validation Loss:1.0757, Validation Accuracy:0.3719\n",
    "Epoch #31: Loss:1.0744, Accuracy:0.3888, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #32: Loss:1.0723, Accuracy:0.4053, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #33: Loss:1.0726, Accuracy:0.4038, Validation Loss:1.0737, Validation Accuracy:0.3793\n",
    "Epoch #34: Loss:1.0718, Accuracy:0.3955, Validation Loss:1.0733, Validation Accuracy:0.3834\n",
    "Epoch #35: Loss:1.0713, Accuracy:0.4072, Validation Loss:1.0730, Validation Accuracy:0.3863\n",
    "Epoch #36: Loss:1.0715, Accuracy:0.4078, Validation Loss:1.0733, Validation Accuracy:0.3863\n",
    "Epoch #37: Loss:1.0718, Accuracy:0.4055, Validation Loss:1.0730, Validation Accuracy:0.3883\n",
    "Epoch #38: Loss:1.0722, Accuracy:0.4056, Validation Loss:1.0736, Validation Accuracy:0.3842\n",
    "Epoch #39: Loss:1.0727, Accuracy:0.4035, Validation Loss:1.0734, Validation Accuracy:0.3867\n",
    "Epoch #40: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0731, Validation Accuracy:0.3846\n",
    "Epoch #41: Loss:1.0712, Accuracy:0.4036, Validation Loss:1.0733, Validation Accuracy:0.3846\n",
    "Epoch #42: Loss:1.0720, Accuracy:0.3978, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #43: Loss:1.0731, Accuracy:0.4052, Validation Loss:1.0736, Validation Accuracy:0.3793\n",
    "Epoch #44: Loss:1.0714, Accuracy:0.4067, Validation Loss:1.0743, Validation Accuracy:0.3871\n",
    "Epoch #45: Loss:1.0750, Accuracy:0.3992, Validation Loss:1.0737, Validation Accuracy:0.3920\n",
    "Epoch #46: Loss:1.0737, Accuracy:0.4033, Validation Loss:1.0737, Validation Accuracy:0.3830\n",
    "Epoch #47: Loss:1.0724, Accuracy:0.4029, Validation Loss:1.0734, Validation Accuracy:0.3970\n",
    "Epoch #48: Loss:1.0727, Accuracy:0.4007, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #49: Loss:1.0718, Accuracy:0.4061, Validation Loss:1.0731, Validation Accuracy:0.3879\n",
    "Epoch #50: Loss:1.0715, Accuracy:0.4066, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #51: Loss:1.0718, Accuracy:0.4012, Validation Loss:1.0737, Validation Accuracy:0.3764\n",
    "Epoch #52: Loss:1.0718, Accuracy:0.3995, Validation Loss:1.0733, Validation Accuracy:0.3863\n",
    "Epoch #53: Loss:1.0718, Accuracy:0.4043, Validation Loss:1.0736, Validation Accuracy:0.3867\n",
    "Epoch #54: Loss:1.0720, Accuracy:0.4005, Validation Loss:1.0737, Validation Accuracy:0.3814\n",
    "Epoch #55: Loss:1.0727, Accuracy:0.4029, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #56: Loss:1.0715, Accuracy:0.4027, Validation Loss:1.0736, Validation Accuracy:0.3789\n",
    "Epoch #57: Loss:1.0709, Accuracy:0.4031, Validation Loss:1.0738, Validation Accuracy:0.3826\n",
    "Epoch #58: Loss:1.0716, Accuracy:0.4033, Validation Loss:1.0734, Validation Accuracy:0.3851\n",
    "Epoch #59: Loss:1.0711, Accuracy:0.4034, Validation Loss:1.0733, Validation Accuracy:0.3871\n",
    "Epoch #60: Loss:1.0730, Accuracy:0.4005, Validation Loss:1.0756, Validation Accuracy:0.3736\n",
    "Epoch #61: Loss:1.0723, Accuracy:0.3961, Validation Loss:1.0741, Validation Accuracy:0.3883\n",
    "Epoch #62: Loss:1.0712, Accuracy:0.4024, Validation Loss:1.0732, Validation Accuracy:0.3768\n",
    "Epoch #63: Loss:1.0713, Accuracy:0.4047, Validation Loss:1.0723, Validation Accuracy:0.3879\n",
    "Epoch #64: Loss:1.0712, Accuracy:0.4085, Validation Loss:1.0734, Validation Accuracy:0.3851\n",
    "Epoch #65: Loss:1.0703, Accuracy:0.4053, Validation Loss:1.0729, Validation Accuracy:0.3826\n",
    "Epoch #66: Loss:1.0701, Accuracy:0.4087, Validation Loss:1.0739, Validation Accuracy:0.3867\n",
    "Epoch #67: Loss:1.0705, Accuracy:0.4025, Validation Loss:1.0740, Validation Accuracy:0.3797\n",
    "Epoch #68: Loss:1.0704, Accuracy:0.4038, Validation Loss:1.0726, Validation Accuracy:0.3810\n",
    "Epoch #69: Loss:1.0702, Accuracy:0.4061, Validation Loss:1.0727, Validation Accuracy:0.3867\n",
    "Epoch #70: Loss:1.0697, Accuracy:0.4073, Validation Loss:1.0728, Validation Accuracy:0.3896\n",
    "Epoch #71: Loss:1.0693, Accuracy:0.4057, Validation Loss:1.0725, Validation Accuracy:0.3838\n",
    "Epoch #72: Loss:1.0700, Accuracy:0.4015, Validation Loss:1.0720, Validation Accuracy:0.3875\n",
    "Epoch #73: Loss:1.0746, Accuracy:0.4006, Validation Loss:1.0750, Validation Accuracy:0.3785\n",
    "Epoch #74: Loss:1.0707, Accuracy:0.4067, Validation Loss:1.0735, Validation Accuracy:0.3912\n",
    "Epoch #75: Loss:1.0697, Accuracy:0.4065, Validation Loss:1.0727, Validation Accuracy:0.3863\n",
    "Epoch #76: Loss:1.0689, Accuracy:0.4056, Validation Loss:1.0726, Validation Accuracy:0.3920\n",
    "Epoch #77: Loss:1.0692, Accuracy:0.4041, Validation Loss:1.0733, Validation Accuracy:0.3855\n",
    "Epoch #78: Loss:1.0685, Accuracy:0.4054, Validation Loss:1.0719, Validation Accuracy:0.3929\n",
    "Epoch #79: Loss:1.0684, Accuracy:0.4044, Validation Loss:1.0717, Validation Accuracy:0.3916\n",
    "Epoch #80: Loss:1.0680, Accuracy:0.4090, Validation Loss:1.0731, Validation Accuracy:0.3875\n",
    "Epoch #81: Loss:1.0686, Accuracy:0.4070, Validation Loss:1.0752, Validation Accuracy:0.3732\n",
    "Epoch #82: Loss:1.0696, Accuracy:0.4003, Validation Loss:1.0745, Validation Accuracy:0.3789\n",
    "Epoch #83: Loss:1.0704, Accuracy:0.4022, Validation Loss:1.0739, Validation Accuracy:0.3851\n",
    "Epoch #84: Loss:1.0694, Accuracy:0.4031, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #85: Loss:1.0689, Accuracy:0.4049, Validation Loss:1.0746, Validation Accuracy:0.3842\n",
    "Epoch #86: Loss:1.0682, Accuracy:0.4062, Validation Loss:1.0725, Validation Accuracy:0.3834\n",
    "Epoch #87: Loss:1.0686, Accuracy:0.4013, Validation Loss:1.0743, Validation Accuracy:0.3904\n",
    "Epoch #88: Loss:1.0675, Accuracy:0.4070, Validation Loss:1.0742, Validation Accuracy:0.3871\n",
    "Epoch #89: Loss:1.0681, Accuracy:0.4093, Validation Loss:1.0734, Validation Accuracy:0.3900\n",
    "Epoch #90: Loss:1.0675, Accuracy:0.4083, Validation Loss:1.0748, Validation Accuracy:0.3945\n",
    "Epoch #91: Loss:1.0691, Accuracy:0.4025, Validation Loss:1.0735, Validation Accuracy:0.3855\n",
    "Epoch #92: Loss:1.0679, Accuracy:0.4031, Validation Loss:1.0730, Validation Accuracy:0.3879\n",
    "Epoch #93: Loss:1.0676, Accuracy:0.4097, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #94: Loss:1.0679, Accuracy:0.4061, Validation Loss:1.0747, Validation Accuracy:0.3855\n",
    "Epoch #95: Loss:1.0697, Accuracy:0.4036, Validation Loss:1.0759, Validation Accuracy:0.3855\n",
    "Epoch #96: Loss:1.0698, Accuracy:0.4047, Validation Loss:1.0726, Validation Accuracy:0.3892\n",
    "Epoch #97: Loss:1.0684, Accuracy:0.4101, Validation Loss:1.0734, Validation Accuracy:0.3773\n",
    "Epoch #98: Loss:1.0682, Accuracy:0.4071, Validation Loss:1.0745, Validation Accuracy:0.3822\n",
    "Epoch #99: Loss:1.0681, Accuracy:0.4078, Validation Loss:1.0741, Validation Accuracy:0.3855\n",
    "Epoch #100: Loss:1.0670, Accuracy:0.4103, Validation Loss:1.0742, Validation Accuracy:0.3875\n",
    "Epoch #101: Loss:1.0680, Accuracy:0.4057, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #102: Loss:1.0672, Accuracy:0.4103, Validation Loss:1.0735, Validation Accuracy:0.3896\n",
    "Epoch #103: Loss:1.0676, Accuracy:0.4092, Validation Loss:1.0725, Validation Accuracy:0.3863\n",
    "Epoch #104: Loss:1.0682, Accuracy:0.4079, Validation Loss:1.0738, Validation Accuracy:0.3888\n",
    "Epoch #105: Loss:1.0671, Accuracy:0.4084, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #106: Loss:1.0660, Accuracy:0.4095, Validation Loss:1.0740, Validation Accuracy:0.3883\n",
    "Epoch #107: Loss:1.0663, Accuracy:0.4137, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #108: Loss:1.0653, Accuracy:0.4162, Validation Loss:1.0743, Validation Accuracy:0.3834\n",
    "Epoch #109: Loss:1.0673, Accuracy:0.4081, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #110: Loss:1.0656, Accuracy:0.4138, Validation Loss:1.0742, Validation Accuracy:0.3863\n",
    "Epoch #111: Loss:1.0667, Accuracy:0.4083, Validation Loss:1.0753, Validation Accuracy:0.3838\n",
    "Epoch #112: Loss:1.0655, Accuracy:0.4052, Validation Loss:1.0726, Validation Accuracy:0.3937\n",
    "Epoch #113: Loss:1.0657, Accuracy:0.4069, Validation Loss:1.0756, Validation Accuracy:0.3834\n",
    "Epoch #114: Loss:1.0652, Accuracy:0.4128, Validation Loss:1.0761, Validation Accuracy:0.3818\n",
    "Epoch #115: Loss:1.0662, Accuracy:0.4050, Validation Loss:1.0771, Validation Accuracy:0.3883\n",
    "Epoch #116: Loss:1.0662, Accuracy:0.4040, Validation Loss:1.0753, Validation Accuracy:0.3801\n",
    "Epoch #117: Loss:1.0656, Accuracy:0.4090, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #118: Loss:1.0664, Accuracy:0.4093, Validation Loss:1.0744, Validation Accuracy:0.3896\n",
    "Epoch #119: Loss:1.0653, Accuracy:0.4149, Validation Loss:1.0758, Validation Accuracy:0.3846\n",
    "Epoch #120: Loss:1.0659, Accuracy:0.4099, Validation Loss:1.0737, Validation Accuracy:0.3920\n",
    "Epoch #121: Loss:1.0663, Accuracy:0.4092, Validation Loss:1.0727, Validation Accuracy:0.4035\n",
    "Epoch #122: Loss:1.0650, Accuracy:0.4128, Validation Loss:1.0724, Validation Accuracy:0.3957\n",
    "Epoch #123: Loss:1.0656, Accuracy:0.4113, Validation Loss:1.0749, Validation Accuracy:0.3920\n",
    "Epoch #124: Loss:1.0660, Accuracy:0.4011, Validation Loss:1.0769, Validation Accuracy:0.3805\n",
    "Epoch #125: Loss:1.0652, Accuracy:0.4134, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #126: Loss:1.0673, Accuracy:0.4077, Validation Loss:1.0830, Validation Accuracy:0.3842\n",
    "Epoch #127: Loss:1.0666, Accuracy:0.4158, Validation Loss:1.0751, Validation Accuracy:0.3879\n",
    "Epoch #128: Loss:1.0636, Accuracy:0.4163, Validation Loss:1.0739, Validation Accuracy:0.3945\n",
    "Epoch #129: Loss:1.0633, Accuracy:0.4107, Validation Loss:1.0744, Validation Accuracy:0.3842\n",
    "Epoch #130: Loss:1.0629, Accuracy:0.4090, Validation Loss:1.0736, Validation Accuracy:0.3953\n",
    "Epoch #131: Loss:1.0641, Accuracy:0.4089, Validation Loss:1.0732, Validation Accuracy:0.3912\n",
    "Epoch #132: Loss:1.0646, Accuracy:0.4097, Validation Loss:1.0726, Validation Accuracy:0.3978\n",
    "Epoch #133: Loss:1.0644, Accuracy:0.4113, Validation Loss:1.0767, Validation Accuracy:0.3966\n",
    "Epoch #134: Loss:1.0633, Accuracy:0.4075, Validation Loss:1.0761, Validation Accuracy:0.3916\n",
    "Epoch #135: Loss:1.0621, Accuracy:0.4143, Validation Loss:1.0741, Validation Accuracy:0.3998\n",
    "Epoch #136: Loss:1.0637, Accuracy:0.4066, Validation Loss:1.0740, Validation Accuracy:0.4027\n",
    "Epoch #137: Loss:1.0625, Accuracy:0.4119, Validation Loss:1.0749, Validation Accuracy:0.3998\n",
    "Epoch #138: Loss:1.0616, Accuracy:0.4191, Validation Loss:1.0747, Validation Accuracy:0.3933\n",
    "Epoch #139: Loss:1.0620, Accuracy:0.4165, Validation Loss:1.0754, Validation Accuracy:0.3978\n",
    "Epoch #140: Loss:1.0659, Accuracy:0.4141, Validation Loss:1.0785, Validation Accuracy:0.3859\n",
    "Epoch #141: Loss:1.0645, Accuracy:0.4086, Validation Loss:1.0769, Validation Accuracy:0.4048\n",
    "Epoch #142: Loss:1.0630, Accuracy:0.4182, Validation Loss:1.0755, Validation Accuracy:0.3920\n",
    "Epoch #143: Loss:1.0619, Accuracy:0.4137, Validation Loss:1.0778, Validation Accuracy:0.3871\n",
    "Epoch #144: Loss:1.0618, Accuracy:0.4122, Validation Loss:1.0761, Validation Accuracy:0.3982\n",
    "Epoch #145: Loss:1.0620, Accuracy:0.4129, Validation Loss:1.0773, Validation Accuracy:0.3949\n",
    "Epoch #146: Loss:1.0624, Accuracy:0.4133, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #147: Loss:1.0612, Accuracy:0.4148, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #148: Loss:1.0604, Accuracy:0.4170, Validation Loss:1.0763, Validation Accuracy:0.3867\n",
    "Epoch #149: Loss:1.0618, Accuracy:0.4125, Validation Loss:1.0778, Validation Accuracy:0.3933\n",
    "Epoch #150: Loss:1.0629, Accuracy:0.4126, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #151: Loss:1.0615, Accuracy:0.4101, Validation Loss:1.0769, Validation Accuracy:0.3961\n",
    "Epoch #152: Loss:1.0606, Accuracy:0.4241, Validation Loss:1.0775, Validation Accuracy:0.3957\n",
    "Epoch #153: Loss:1.0611, Accuracy:0.4220, Validation Loss:1.0791, Validation Accuracy:0.3912\n",
    "Epoch #154: Loss:1.0655, Accuracy:0.4095, Validation Loss:1.0820, Validation Accuracy:0.3924\n",
    "Epoch #155: Loss:1.0616, Accuracy:0.4165, Validation Loss:1.0788, Validation Accuracy:0.3900\n",
    "Epoch #156: Loss:1.0624, Accuracy:0.4131, Validation Loss:1.0804, Validation Accuracy:0.3957\n",
    "Epoch #157: Loss:1.0613, Accuracy:0.4129, Validation Loss:1.0793, Validation Accuracy:0.3957\n",
    "Epoch #158: Loss:1.0604, Accuracy:0.4240, Validation Loss:1.0777, Validation Accuracy:0.3900\n",
    "Epoch #159: Loss:1.0600, Accuracy:0.4179, Validation Loss:1.0775, Validation Accuracy:0.3879\n",
    "Epoch #160: Loss:1.0594, Accuracy:0.4230, Validation Loss:1.0802, Validation Accuracy:0.3994\n",
    "Epoch #161: Loss:1.0605, Accuracy:0.4148, Validation Loss:1.0792, Validation Accuracy:0.3912\n",
    "Epoch #162: Loss:1.0610, Accuracy:0.4130, Validation Loss:1.0801, Validation Accuracy:0.3929\n",
    "Epoch #163: Loss:1.0606, Accuracy:0.4204, Validation Loss:1.0791, Validation Accuracy:0.3924\n",
    "Epoch #164: Loss:1.0624, Accuracy:0.4208, Validation Loss:1.0789, Validation Accuracy:0.3953\n",
    "Epoch #165: Loss:1.0621, Accuracy:0.4184, Validation Loss:1.0799, Validation Accuracy:0.3970\n",
    "Epoch #166: Loss:1.0625, Accuracy:0.4143, Validation Loss:1.0788, Validation Accuracy:0.3949\n",
    "Epoch #167: Loss:1.0609, Accuracy:0.4108, Validation Loss:1.0836, Validation Accuracy:0.3846\n",
    "Epoch #168: Loss:1.0613, Accuracy:0.4103, Validation Loss:1.0782, Validation Accuracy:0.3908\n",
    "Epoch #169: Loss:1.0608, Accuracy:0.4177, Validation Loss:1.0818, Validation Accuracy:0.3941\n",
    "Epoch #170: Loss:1.0614, Accuracy:0.4154, Validation Loss:1.0799, Validation Accuracy:0.3900\n",
    "Epoch #171: Loss:1.0600, Accuracy:0.4126, Validation Loss:1.0773, Validation Accuracy:0.3982\n",
    "Epoch #172: Loss:1.0592, Accuracy:0.4197, Validation Loss:1.0787, Validation Accuracy:0.3920\n",
    "Epoch #173: Loss:1.0578, Accuracy:0.4233, Validation Loss:1.0773, Validation Accuracy:0.3929\n",
    "Epoch #174: Loss:1.0589, Accuracy:0.4236, Validation Loss:1.0789, Validation Accuracy:0.3961\n",
    "Epoch #175: Loss:1.0583, Accuracy:0.4202, Validation Loss:1.0778, Validation Accuracy:0.3896\n",
    "Epoch #176: Loss:1.0584, Accuracy:0.4203, Validation Loss:1.0771, Validation Accuracy:0.3966\n",
    "Epoch #177: Loss:1.0582, Accuracy:0.4192, Validation Loss:1.0780, Validation Accuracy:0.3994\n",
    "Epoch #178: Loss:1.0589, Accuracy:0.4193, Validation Loss:1.0834, Validation Accuracy:0.3970\n",
    "Epoch #179: Loss:1.0617, Accuracy:0.4163, Validation Loss:1.0799, Validation Accuracy:0.3883\n",
    "Epoch #180: Loss:1.0584, Accuracy:0.4147, Validation Loss:1.0777, Validation Accuracy:0.3966\n",
    "Epoch #181: Loss:1.0581, Accuracy:0.4171, Validation Loss:1.0758, Validation Accuracy:0.3986\n",
    "Epoch #182: Loss:1.0577, Accuracy:0.4208, Validation Loss:1.0799, Validation Accuracy:0.3916\n",
    "Epoch #183: Loss:1.0584, Accuracy:0.4163, Validation Loss:1.0794, Validation Accuracy:0.3883\n",
    "Epoch #184: Loss:1.0610, Accuracy:0.4148, Validation Loss:1.0797, Validation Accuracy:0.3924\n",
    "Epoch #185: Loss:1.0586, Accuracy:0.4215, Validation Loss:1.0796, Validation Accuracy:0.3892\n",
    "Epoch #186: Loss:1.0560, Accuracy:0.4275, Validation Loss:1.0766, Validation Accuracy:0.3978\n",
    "Epoch #187: Loss:1.0558, Accuracy:0.4257, Validation Loss:1.0768, Validation Accuracy:0.3982\n",
    "Epoch #188: Loss:1.0564, Accuracy:0.4205, Validation Loss:1.0760, Validation Accuracy:0.3982\n",
    "Epoch #189: Loss:1.0568, Accuracy:0.4246, Validation Loss:1.0758, Validation Accuracy:0.3982\n",
    "Epoch #190: Loss:1.0587, Accuracy:0.4227, Validation Loss:1.0746, Validation Accuracy:0.4019\n",
    "Epoch #191: Loss:1.0559, Accuracy:0.4218, Validation Loss:1.0745, Validation Accuracy:0.4035\n",
    "Epoch #192: Loss:1.0561, Accuracy:0.4232, Validation Loss:1.0749, Validation Accuracy:0.4019\n",
    "Epoch #193: Loss:1.0548, Accuracy:0.4218, Validation Loss:1.0774, Validation Accuracy:0.4002\n",
    "Epoch #194: Loss:1.0550, Accuracy:0.4252, Validation Loss:1.0732, Validation Accuracy:0.3978\n",
    "Epoch #195: Loss:1.0563, Accuracy:0.4186, Validation Loss:1.0769, Validation Accuracy:0.4007\n",
    "Epoch #196: Loss:1.0562, Accuracy:0.4186, Validation Loss:1.0755, Validation Accuracy:0.4007\n",
    "Epoch #197: Loss:1.0560, Accuracy:0.4241, Validation Loss:1.0728, Validation Accuracy:0.4048\n",
    "Epoch #198: Loss:1.0556, Accuracy:0.4273, Validation Loss:1.0732, Validation Accuracy:0.3970\n",
    "Epoch #199: Loss:1.0575, Accuracy:0.4232, Validation Loss:1.0772, Validation Accuracy:0.3974\n",
    "Epoch #200: Loss:1.0571, Accuracy:0.4243, Validation Loss:1.0810, Validation Accuracy:0.3957\n",
    "Epoch #201: Loss:1.0584, Accuracy:0.4160, Validation Loss:1.0808, Validation Accuracy:0.3978\n",
    "Epoch #202: Loss:1.0600, Accuracy:0.4178, Validation Loss:1.0788, Validation Accuracy:0.3949\n",
    "Epoch #203: Loss:1.0612, Accuracy:0.4115, Validation Loss:1.0784, Validation Accuracy:0.3896\n",
    "Epoch #204: Loss:1.0642, Accuracy:0.4074, Validation Loss:1.0749, Validation Accuracy:0.3879\n",
    "Epoch #205: Loss:1.0622, Accuracy:0.4130, Validation Loss:1.0781, Validation Accuracy:0.3867\n",
    "Epoch #206: Loss:1.0599, Accuracy:0.4139, Validation Loss:1.0749, Validation Accuracy:0.3879\n",
    "Epoch #207: Loss:1.0576, Accuracy:0.4174, Validation Loss:1.0771, Validation Accuracy:0.3883\n",
    "Epoch #208: Loss:1.0574, Accuracy:0.4217, Validation Loss:1.0762, Validation Accuracy:0.3871\n",
    "Epoch #209: Loss:1.0591, Accuracy:0.4102, Validation Loss:1.0780, Validation Accuracy:0.3810\n",
    "Epoch #210: Loss:1.0583, Accuracy:0.4152, Validation Loss:1.0793, Validation Accuracy:0.3871\n",
    "Epoch #211: Loss:1.0562, Accuracy:0.4189, Validation Loss:1.0813, Validation Accuracy:0.3924\n",
    "Epoch #212: Loss:1.0566, Accuracy:0.4188, Validation Loss:1.0791, Validation Accuracy:0.3834\n",
    "Epoch #213: Loss:1.0580, Accuracy:0.4123, Validation Loss:1.0802, Validation Accuracy:0.3892\n",
    "Epoch #214: Loss:1.0583, Accuracy:0.4105, Validation Loss:1.0795, Validation Accuracy:0.3879\n",
    "Epoch #215: Loss:1.0582, Accuracy:0.4143, Validation Loss:1.0779, Validation Accuracy:0.3859\n",
    "Epoch #216: Loss:1.0582, Accuracy:0.4127, Validation Loss:1.0771, Validation Accuracy:0.4002\n",
    "Epoch #217: Loss:1.0586, Accuracy:0.4144, Validation Loss:1.0789, Validation Accuracy:0.3998\n",
    "Epoch #218: Loss:1.0590, Accuracy:0.4180, Validation Loss:1.0802, Validation Accuracy:0.3929\n",
    "Epoch #219: Loss:1.0581, Accuracy:0.4162, Validation Loss:1.0783, Validation Accuracy:0.4039\n",
    "Epoch #220: Loss:1.0567, Accuracy:0.4224, Validation Loss:1.0830, Validation Accuracy:0.3978\n",
    "Epoch #221: Loss:1.0590, Accuracy:0.4213, Validation Loss:1.0795, Validation Accuracy:0.3953\n",
    "Epoch #222: Loss:1.0601, Accuracy:0.4128, Validation Loss:1.0827, Validation Accuracy:0.3900\n",
    "Epoch #223: Loss:1.0578, Accuracy:0.4165, Validation Loss:1.0828, Validation Accuracy:0.3912\n",
    "Epoch #224: Loss:1.0585, Accuracy:0.4194, Validation Loss:1.0789, Validation Accuracy:0.3916\n",
    "Epoch #225: Loss:1.0583, Accuracy:0.4157, Validation Loss:1.0797, Validation Accuracy:0.4019\n",
    "Epoch #226: Loss:1.0590, Accuracy:0.4170, Validation Loss:1.0840, Validation Accuracy:0.3851\n",
    "Epoch #227: Loss:1.0590, Accuracy:0.4171, Validation Loss:1.0830, Validation Accuracy:0.3896\n",
    "Epoch #228: Loss:1.0587, Accuracy:0.4140, Validation Loss:1.0836, Validation Accuracy:0.3879\n",
    "Epoch #229: Loss:1.0583, Accuracy:0.4213, Validation Loss:1.0845, Validation Accuracy:0.3900\n",
    "Epoch #230: Loss:1.0573, Accuracy:0.4224, Validation Loss:1.0825, Validation Accuracy:0.3904\n",
    "Epoch #231: Loss:1.0576, Accuracy:0.4160, Validation Loss:1.0851, Validation Accuracy:0.3908\n",
    "Epoch #232: Loss:1.0562, Accuracy:0.4172, Validation Loss:1.0835, Validation Accuracy:0.3822\n",
    "Epoch #233: Loss:1.0571, Accuracy:0.4190, Validation Loss:1.0828, Validation Accuracy:0.3888\n",
    "Epoch #234: Loss:1.0605, Accuracy:0.4159, Validation Loss:1.0754, Validation Accuracy:0.3953\n",
    "Epoch #235: Loss:1.0605, Accuracy:0.4247, Validation Loss:1.0765, Validation Accuracy:0.3846\n",
    "Epoch #236: Loss:1.0590, Accuracy:0.4158, Validation Loss:1.0830, Validation Accuracy:0.3871\n",
    "Epoch #237: Loss:1.0587, Accuracy:0.4222, Validation Loss:1.0799, Validation Accuracy:0.3883\n",
    "Epoch #238: Loss:1.0574, Accuracy:0.4150, Validation Loss:1.0817, Validation Accuracy:0.3855\n",
    "Epoch #239: Loss:1.0569, Accuracy:0.4143, Validation Loss:1.0841, Validation Accuracy:0.3797\n",
    "Epoch #240: Loss:1.0558, Accuracy:0.4175, Validation Loss:1.0827, Validation Accuracy:0.3822\n",
    "Epoch #241: Loss:1.0557, Accuracy:0.4175, Validation Loss:1.0815, Validation Accuracy:0.3892\n",
    "Epoch #242: Loss:1.0545, Accuracy:0.4220, Validation Loss:1.0828, Validation Accuracy:0.3859\n",
    "Epoch #243: Loss:1.0561, Accuracy:0.4240, Validation Loss:1.0819, Validation Accuracy:0.3994\n",
    "Epoch #244: Loss:1.0558, Accuracy:0.4273, Validation Loss:1.0804, Validation Accuracy:0.3949\n",
    "Epoch #245: Loss:1.0557, Accuracy:0.4175, Validation Loss:1.0799, Validation Accuracy:0.3974\n",
    "Epoch #246: Loss:1.0563, Accuracy:0.4164, Validation Loss:1.0829, Validation Accuracy:0.3924\n",
    "Epoch #247: Loss:1.0582, Accuracy:0.4160, Validation Loss:1.0803, Validation Accuracy:0.3916\n",
    "Epoch #248: Loss:1.0568, Accuracy:0.4171, Validation Loss:1.0838, Validation Accuracy:0.3933\n",
    "Epoch #249: Loss:1.0562, Accuracy:0.4183, Validation Loss:1.0805, Validation Accuracy:0.4023\n",
    "Epoch #250: Loss:1.0547, Accuracy:0.4206, Validation Loss:1.0785, Validation Accuracy:0.4007\n",
    "Epoch #251: Loss:1.0544, Accuracy:0.4226, Validation Loss:1.0791, Validation Accuracy:0.4011\n",
    "Epoch #252: Loss:1.0558, Accuracy:0.4228, Validation Loss:1.0818, Validation Accuracy:0.3842\n",
    "Epoch #253: Loss:1.0548, Accuracy:0.4249, Validation Loss:1.0800, Validation Accuracy:0.3908\n",
    "Epoch #254: Loss:1.0543, Accuracy:0.4228, Validation Loss:1.0803, Validation Accuracy:0.3904\n",
    "Epoch #255: Loss:1.0551, Accuracy:0.4132, Validation Loss:1.0868, Validation Accuracy:0.3933\n",
    "Epoch #256: Loss:1.0539, Accuracy:0.4292, Validation Loss:1.0828, Validation Accuracy:0.3998\n",
    "Epoch #257: Loss:1.0534, Accuracy:0.4279, Validation Loss:1.0841, Validation Accuracy:0.3978\n",
    "Epoch #258: Loss:1.0539, Accuracy:0.4262, Validation Loss:1.0815, Validation Accuracy:0.3937\n",
    "Epoch #259: Loss:1.0552, Accuracy:0.4141, Validation Loss:1.0843, Validation Accuracy:0.4011\n",
    "Epoch #260: Loss:1.0534, Accuracy:0.4263, Validation Loss:1.0830, Validation Accuracy:0.3986\n",
    "Epoch #261: Loss:1.0541, Accuracy:0.4246, Validation Loss:1.0844, Validation Accuracy:0.3986\n",
    "Epoch #262: Loss:1.0544, Accuracy:0.4272, Validation Loss:1.0883, Validation Accuracy:0.3994\n",
    "Epoch #263: Loss:1.0536, Accuracy:0.4306, Validation Loss:1.0882, Validation Accuracy:0.3941\n",
    "Epoch #264: Loss:1.0518, Accuracy:0.4158, Validation Loss:1.0844, Validation Accuracy:0.4039\n",
    "Epoch #265: Loss:1.0534, Accuracy:0.4238, Validation Loss:1.0810, Validation Accuracy:0.3912\n",
    "Epoch #266: Loss:1.0527, Accuracy:0.4189, Validation Loss:1.0859, Validation Accuracy:0.4023\n",
    "Epoch #267: Loss:1.0532, Accuracy:0.4240, Validation Loss:1.0817, Validation Accuracy:0.3961\n",
    "Epoch #268: Loss:1.0558, Accuracy:0.4138, Validation Loss:1.0829, Validation Accuracy:0.3998\n",
    "Epoch #269: Loss:1.0520, Accuracy:0.4287, Validation Loss:1.0832, Validation Accuracy:0.3908\n",
    "Epoch #270: Loss:1.0513, Accuracy:0.4296, Validation Loss:1.0849, Validation Accuracy:0.3871\n",
    "Epoch #271: Loss:1.0502, Accuracy:0.4251, Validation Loss:1.0873, Validation Accuracy:0.3912\n",
    "Epoch #272: Loss:1.0534, Accuracy:0.4211, Validation Loss:1.0841, Validation Accuracy:0.3908\n",
    "Epoch #273: Loss:1.0521, Accuracy:0.4240, Validation Loss:1.0825, Validation Accuracy:0.3982\n",
    "Epoch #274: Loss:1.0530, Accuracy:0.4247, Validation Loss:1.0828, Validation Accuracy:0.4011\n",
    "Epoch #275: Loss:1.0512, Accuracy:0.4295, Validation Loss:1.0841, Validation Accuracy:0.3937\n",
    "Epoch #276: Loss:1.0507, Accuracy:0.4299, Validation Loss:1.0872, Validation Accuracy:0.3949\n",
    "Epoch #277: Loss:1.0516, Accuracy:0.4248, Validation Loss:1.0807, Validation Accuracy:0.3929\n",
    "Epoch #278: Loss:1.0522, Accuracy:0.4238, Validation Loss:1.0790, Validation Accuracy:0.3879\n",
    "Epoch #279: Loss:1.0517, Accuracy:0.4269, Validation Loss:1.0835, Validation Accuracy:0.3970\n",
    "Epoch #280: Loss:1.0511, Accuracy:0.4247, Validation Loss:1.0883, Validation Accuracy:0.3937\n",
    "Epoch #281: Loss:1.0536, Accuracy:0.4267, Validation Loss:1.0816, Validation Accuracy:0.3912\n",
    "Epoch #282: Loss:1.0540, Accuracy:0.4179, Validation Loss:1.0841, Validation Accuracy:0.3900\n",
    "Epoch #283: Loss:1.0520, Accuracy:0.4243, Validation Loss:1.0828, Validation Accuracy:0.3924\n",
    "Epoch #284: Loss:1.0517, Accuracy:0.4206, Validation Loss:1.0832, Validation Accuracy:0.3875\n",
    "Epoch #285: Loss:1.0510, Accuracy:0.4271, Validation Loss:1.0829, Validation Accuracy:0.3863\n",
    "Epoch #286: Loss:1.0528, Accuracy:0.4228, Validation Loss:1.0840, Validation Accuracy:0.3896\n",
    "Epoch #287: Loss:1.0525, Accuracy:0.4235, Validation Loss:1.0920, Validation Accuracy:0.3929\n",
    "Epoch #288: Loss:1.0527, Accuracy:0.4222, Validation Loss:1.0897, Validation Accuracy:0.3896\n",
    "Epoch #289: Loss:1.0523, Accuracy:0.4258, Validation Loss:1.0825, Validation Accuracy:0.3953\n",
    "Epoch #290: Loss:1.0499, Accuracy:0.4248, Validation Loss:1.0874, Validation Accuracy:0.3859\n",
    "Epoch #291: Loss:1.0516, Accuracy:0.4246, Validation Loss:1.0883, Validation Accuracy:0.3896\n",
    "Epoch #292: Loss:1.0519, Accuracy:0.4249, Validation Loss:1.0858, Validation Accuracy:0.3875\n",
    "Epoch #293: Loss:1.0512, Accuracy:0.4249, Validation Loss:1.0856, Validation Accuracy:0.3851\n",
    "Epoch #294: Loss:1.0517, Accuracy:0.4196, Validation Loss:1.0912, Validation Accuracy:0.3855\n",
    "Epoch #295: Loss:1.0516, Accuracy:0.4175, Validation Loss:1.0882, Validation Accuracy:0.3801\n",
    "Epoch #296: Loss:1.0502, Accuracy:0.4229, Validation Loss:1.0880, Validation Accuracy:0.3892\n",
    "Epoch #297: Loss:1.0490, Accuracy:0.4261, Validation Loss:1.0854, Validation Accuracy:0.3888\n",
    "Epoch #298: Loss:1.0513, Accuracy:0.4194, Validation Loss:1.0897, Validation Accuracy:0.3781\n",
    "Epoch #299: Loss:1.0485, Accuracy:0.4222, Validation Loss:1.0915, Validation Accuracy:0.3818\n",
    "Epoch #300: Loss:1.0487, Accuracy:0.4185, Validation Loss:1.0930, Validation Accuracy:0.3822\n",
    "\n",
    "Test:\n",
    "Test Loss:1.09301889, Accuracy:0.3822\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01   02  03\n",
    "t:01  500  457   3\n",
    "t:02  475  428   5\n",
    "t:03  335  230   3\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.38      0.52      0.44       960\n",
    "          02       0.38      0.47      0.42       908\n",
    "          03       0.27      0.01      0.01       568\n",
    "\n",
    "    accuracy                           0.38      2436\n",
    "   macro avg       0.35      0.33      0.29      2436\n",
    "weighted avg       0.36      0.38      0.33      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 16:53:04 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 12 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0749652460095134, 1.074433733481296, 1.0746216875774715, 1.0743500475813015, 1.0742630749108952, 1.0742070937195827, 1.0741772974653196, 1.0742497528323596, 1.073895699284934, 1.0740600963335711, 1.0734844524872127, 1.0739857138475566, 1.074086435714183, 1.0741798441202575, 1.0738245593307445, 1.0734932698639743, 1.0735579572483431, 1.0742440615185767, 1.0754077878883124, 1.0745606101400942, 1.0748132805910409, 1.076362767047287, 1.073978822415294, 1.0739773069500727, 1.073856141375399, 1.0730408105161193, 1.074466966055884, 1.0737678198195835, 1.0738998128862804, 1.0757161572648974, 1.074496933000624, 1.0737481287547521, 1.0736948255639163, 1.0733401622678258, 1.0730239358441582, 1.0733168346345523, 1.072997398955873, 1.0735724375557234, 1.073390644955126, 1.0730756221733657, 1.073305197145747, 1.0749991668268966, 1.0736418774562517, 1.0742521223372035, 1.073654226481621, 1.073743140756203, 1.0734333665108642, 1.0729555439674991, 1.073063494340931, 1.073576023425962, 1.0736849766059462, 1.0733002569092123, 1.0736062002103708, 1.0736578796884697, 1.0731411675122766, 1.0735560436358398, 1.0738149186464758, 1.0734493340960474, 1.0733325919885746, 1.0755745732334057, 1.0740909055731762, 1.0731622311477786, 1.072339133088812, 1.073412615285914, 1.0729088616880094, 1.0739428202311199, 1.0740461212465133, 1.0725975148196292, 1.0726742073037159, 1.0727615978917464, 1.0725430226678332, 1.071970151171504, 1.0750291171332298, 1.0735136239203718, 1.0726604964737039, 1.072557172947525, 1.0732834965528917, 1.0718561281711596, 1.0716632279660705, 1.0731443282222903, 1.0751644770304363, 1.0744934264074992, 1.073919868430089, 1.0740874798231328, 1.0746422138903138, 1.0725368778106614, 1.0743211429498858, 1.0742212185522997, 1.073374687157241, 1.0747873103873091, 1.0734972681709503, 1.072966818347549, 1.0745173755341955, 1.074712828266601, 1.0759027466202409, 1.0725510273073695, 1.0733847187461916, 1.0744862879438353, 1.0741008426168281, 1.0741500715512555, 1.0733880699170242, 1.073462865818506, 1.0724774992524697, 1.0738434349179071, 1.0734022528862914, 1.0739842271569915, 1.0743844235080413, 1.0742692820152822, 1.0751257246155261, 1.074154315910903, 1.0753286771586377, 1.0726477678969184, 1.075644479988048, 1.07607398910084, 1.0771227100212586, 1.0752822616808912, 1.0755498777273644, 1.0743566694713773, 1.0757735088736748, 1.073681261347628, 1.0726577182512957, 1.0724451289388346, 1.0748673293782376, 1.0768967798386497, 1.0751825375314221, 1.0829537750660687, 1.0750932354840934, 1.0739094608131496, 1.0743999305029808, 1.073579928362115, 1.0732251456609891, 1.0725967633508893, 1.076716574737787, 1.0761406045829134, 1.0741193337589259, 1.0740112966700337, 1.0748680156635728, 1.0746716803126344, 1.075440773627245, 1.0785404457443062, 1.076879874043081, 1.0755210382793532, 1.0777893769134246, 1.0761366387697668, 1.077276315007891, 1.0743704640806602, 1.0764773498810767, 1.0763096478576535, 1.0777557538256466, 1.0764533526008744, 1.076911712710689, 1.077502839475234, 1.0790606772371114, 1.0819875518676683, 1.0787776162471678, 1.0804206461742007, 1.0792622517286654, 1.0777092600495162, 1.0774601872135656, 1.0802420162410766, 1.0791714923526659, 1.0801284781034748, 1.0790690780664705, 1.078920602211224, 1.0798924525187326, 1.0788441465797487, 1.0835540737033087, 1.0781870565586684, 1.081779264268421, 1.0798632325405753, 1.0773259007872031, 1.0786557661488725, 1.0772662836146865, 1.078930514590885, 1.077838429676488, 1.077132005801146, 1.0780361305512427, 1.0834070900195143, 1.079888408407202, 1.0776966489203066, 1.0758324920250277, 1.079885975480667, 1.079405519175412, 1.0796599920551568, 1.0796195384121097, 1.0766190858114333, 1.076844051357952, 1.0759876163918005, 1.0757648279318472, 1.0746465512292922, 1.0745166181930768, 1.0748520899680252, 1.077442296620073, 1.0731824622757133, 1.0769381241258142, 1.0754702991648457, 1.0727586560257159, 1.0732471157960313, 1.077217693399326, 1.0809839184844043, 1.0808127382510206, 1.0788061779316618, 1.0783506138571377, 1.0748527326020114, 1.0780996728217465, 1.0748779431156728, 1.0771014199076812, 1.0761686091744058, 1.0779670572828972, 1.079297159692924, 1.0812886499223255, 1.0791492017814874, 1.080203476602025, 1.0795423669376591, 1.0779332327725264, 1.0770999729535458, 1.0788663453460718, 1.0801786428993363, 1.0782507607110812, 1.0829704802220286, 1.0795085216782168, 1.0826598840394044, 1.0828347480160065, 1.0788737605945231, 1.079730559060922, 1.0840140301214258, 1.0830349291878185, 1.0836451779836895, 1.0845215475226466, 1.08245579499525, 1.0850990987176379, 1.0834619362757516, 1.0827744799881733, 1.0754101683549302, 1.0764507296050123, 1.082990298325988, 1.0799291599756, 1.0816922046868085, 1.0840659979333236, 1.0827481823014509, 1.0814991126506788, 1.0828183255172128, 1.0818642988580789, 1.0803911155472052, 1.0799048729913772, 1.082854523251601, 1.0803455315982962, 1.083827072567932, 1.0805346799405728, 1.0784600211677489, 1.0791488609877713, 1.0817661811957022, 1.0799528806667609, 1.0803067230042958, 1.0868187669071265, 1.082785690359294, 1.084098727049303, 1.0814839010363926, 1.08433557848625, 1.0829838416455022, 1.084416862191825, 1.0882798327601015, 1.0882054310909828, 1.0843510433958081, 1.0810257721025565, 1.085866396650305, 1.08169593188563, 1.0829379627074318, 1.0832135542272934, 1.0849004245939708, 1.087276577949524, 1.0841409253760903, 1.082471012873407, 1.0828021441774416, 1.0841120657662453, 1.0872271397626654, 1.0807299146119793, 1.0789671110597934, 1.0835138293126925, 1.0882826303613597, 1.0815825374255628, 1.0841200210777997, 1.0827818670492062, 1.083181932250463, 1.0829096899439745, 1.0839670839763822, 1.0920241239231403, 1.0896562690218095, 1.0825030670573168, 1.0874468049000832, 1.0883049876819104, 1.0858033883747795, 1.0856188506328415, 1.0912382596819272, 1.088208096759464, 1.0879927020159066, 1.0854122568233846, 1.0896686485835485, 1.0915282260020966, 1.0930187565156784], 'val_acc': [0.3895730717820291, 0.39039408994229946, 0.394909689383358, 0.388341542486291, 0.39244663284721437, 0.3866995087104478, 0.3875205246196396, 0.39080459902243464, 0.38875205156642617, 0.39080459672241963, 0.3924466327982779, 0.39614121407906605, 0.39449918040109583, 0.3875205244238936, 0.3957307049499944, 0.3936781599897469, 0.3924466327982779, 0.380541870893516, 0.3920361262628402, 0.3949096871322795, 0.3940886689720091, 0.3862889972324246, 0.39449917810108076, 0.39449917800320783, 0.3944991779542713, 0.3830049250317716, 0.3949096870344065, 0.38464696120550285, 0.3887520538175047, 0.37192118123834356, 0.39080459902243464, 0.3924466351472294, 0.37931034605099845, 0.38341543401403383, 0.3862889996303126, 0.3862889996303126, 0.3883415424373545, 0.3842364520764312, 0.3866995086615113, 0.38464696350551786, 0.3846469635544544, 0.3842364519785582, 0.37931034375098344, 0.3871100177416465, 0.3920361261160307, 0.38300492733178665, 0.3969622321414634, 0.38916256054868836, 0.38793103335721935, 0.38587849059911394, 0.3764367805325926, 0.3862889971345516, 0.3866995062146868, 0.3813628912069919, 0.3891625628976399, 0.3788998347197847, 0.3825944160005729, 0.38505747253671657, 0.38711001769271, 0.3735632173631383, 0.38834154468843307, 0.37684729191274285, 0.3879310357061709, 0.3850574703345745, 0.38259441844739744, 0.3866995063125598, 0.37972085518007015, 0.38095237997365117, 0.3866995063125598, 0.3895730695798871, 0.38382594558992994, 0.3875205243260207, 0.37848932813541053, 0.39121510565574535, 0.3862889997281856, 0.3920361261649672, 0.38546798156791523, 0.3928571418784131, 0.391625614784817, 0.3875205243260207, 0.37315271082770063, 0.37889983486659423, 0.38505747258565304, 0.3891625629465764, 0.38423645232111364, 0.3834154341608433, 0.390394087593348, 0.38711001539269496, 0.3899835811068468, 0.39449917800320783, 0.3854679792189637, 0.38793103580404387, 0.3908045965756102, 0.38546797917002723, 0.3854679793657732, 0.38916256054868836, 0.3772578010418145, 0.38218390692043774, 0.3854679792189637, 0.38752052677284515, 0.3875205269196546, 0.3895730720267116, 0.3862889973302976, 0.3887520515174897, 0.3875205244238936, 0.388341542584164, 0.38752052687071814, 0.38341543420977975, 0.38259441844739744, 0.38628899737923406, 0.3838259455409935, 0.39367816233869846, 0.38341543420977975, 0.381773400385, 0.3883415448352425, 0.38013136435807826, 0.38587848849484485, 0.38957306962882354, 0.38464696360339085, 0.3920361238649522, 0.4035303791853399, 0.39573070519467685, 0.39203612391388865, 0.38054187074670653, 0.3858784906480504, 0.38423645222324065, 0.38793103345509233, 0.39449918040109583, 0.3842364522721771, 0.3953201984634932, 0.3912151059004278, 0.3977832503017338, 0.3965517232570742, 0.39162561473588053, 0.39983579775774225, 0.402709358676118, 0.39983579785561524, 0.3932676509585482, 0.39778325044854324, 0.38587848839697186, 0.40476190388104794, 0.3920361262139037, 0.3871100178395195, 0.3981937616329475, 0.3949096894322945, 0.3924466329940238, 0.3908045968692291, 0.38669950650830576, 0.3932676535032457, 0.3908045967713561, 0.39614121417693904, 0.39573070749469186, 0.3912151082983158, 0.3924466351961659, 0.38998358105791026, 0.39573070749469186, 0.3957307051457404, 0.38998358105791026, 0.3879310336508383, 0.3994252890201625, 0.3912151082004428, 0.39285714192734955, 0.3924466352940389, 0.39532019851242967, 0.3969622322393364, 0.39490968708334295, 0.3846469612544393, 0.39080459672241963, 0.3940886690698821, 0.38998358115578324, 0.3981937618286935, 0.3920361237181427, 0.39285714202522254, 0.39614121652589057, 0.3895730698245695, 0.39655172580177167, 0.3994252866222745, 0.39696223478403386, 0.38834154277990995, 0.3965517233549472, 0.39860426855987713, 0.391625614980563, 0.38834154507992497, 0.39244663304296035, 0.38916256069549787, 0.39778325289536776, 0.39819376187762995, 0.39819376187762995, 0.39819376187762995, 0.40188834315841815, 0.40353037693426136, 0.4018883431094816, 0.40024630708255987, 0.39778325289536776, 0.40065681606482206, 0.40065681611375853, 0.40476190642574544, 0.3969622323372094, 0.3973727436684231, 0.39573070519467685, 0.39778325044854324, 0.3949096895301675, 0.38957307212458453, 0.38793103585298033, 0.38669950880832077, 0.3879310359997898, 0.38834154268203697, 0.38711001558844094, 0.38095238237153917, 0.3871100178395195, 0.3924466352940389, 0.38341543665660427, 0.3891625630444494, 0.3879310360976628, 0.38587849079485986, 0.40024630463573535, 0.3998357980024247, 0.392857141976286, 0.4039408882654751, 0.3977832503017338, 0.39532019861030265, 0.38998358105791026, 0.3912151081025698, 0.391625614784817, 0.40188834306054516, 0.3850574702367015, 0.38957306967776006, 0.3879310359508533, 0.3899835788068317, 0.3903940878380304, 0.39080459672241963, 0.3821839071161837, 0.38875205166429916, 0.39532019851242967, 0.38464696130337583, 0.38711001539269496, 0.388341544982052, 0.38546797941470967, 0.3797208528800551, 0.38218390941619873, 0.3891625604997519, 0.3858784882501624, 0.39942528877548006, 0.394909689383358, 0.3973727412705351, 0.39244663284721437, 0.391625614784817, 0.39326765090961174, 0.4022988496938558, 0.4006568158690761, 0.40106732494921127, 0.38423645452325567, 0.39080459672241963, 0.3903940901869819, 0.3932676508606753, 0.3998357955066637, 0.3977832527485583, 0.39367815994081046, 0.4010673226981327, 0.3986042684130676, 0.3986042684130676, 0.3994252866222745, 0.3940886715656431, 0.40394088581865056, 0.3912151081025698, 0.40229884959598283, 0.39614121657482704, 0.3998357978066787, 0.3908045968202926, 0.38711001549056795, 0.3912151057536183, 0.39080459667348316, 0.39819376177975696, 0.4010673249002748, 0.39367815994081046, 0.39490968708334295, 0.39285714182947656, 0.3879310335040288, 0.3969622322882729, 0.39367815994081046, 0.3912151057536183, 0.38998357885576823, 0.3924466354897848, 0.3875205270175276, 0.3862889974281706, 0.38957307207564806, 0.392857141976286, 0.38957306962882354, 0.39532019611454167, 0.38587848839697186, 0.38957306977563305, 0.38752052447283014, 0.3850574727324625, 0.3854679793657732, 0.38013136435807826, 0.3891625630444494, 0.3887520516153627, 0.3780788169020698, 0.38177340033606355, 0.3821839070672472], 'loss': [1.0763047447439582, 1.0742325814108096, 1.0743147607701515, 1.0736399199683564, 1.0735504721713995, 1.0736576777953633, 1.0736281483080352, 1.0738910841011657, 1.0730992190402147, 1.0727371858375518, 1.0727194828174442, 1.0728895408172137, 1.073285662369072, 1.0733515172034074, 1.0731528493168419, 1.0727405722625936, 1.0734272663108622, 1.0720707772448812, 1.0728623032325102, 1.0734923491487758, 1.072299497670951, 1.0732143709057411, 1.0730705048024531, 1.0721879766707059, 1.0719798956563584, 1.0716636628830458, 1.0725541207335079, 1.0719720559932857, 1.0725794603692433, 1.0722085135183785, 1.0743529562587855, 1.0722629943667497, 1.0726006547038804, 1.0717549410199239, 1.0713042586735877, 1.071482518174565, 1.0717692364902223, 1.0721666655746085, 1.0727212849094148, 1.0717015593938024, 1.0711833865001217, 1.0719642934368376, 1.0730716044897906, 1.0713830877378492, 1.0750114791936698, 1.0736661562929408, 1.072438916043824, 1.0726967652475565, 1.0718241920216618, 1.0715083307309317, 1.0718360909201525, 1.071793379431143, 1.07175316350416, 1.0720437226109436, 1.0726620112601246, 1.0714987199164514, 1.0708978235354414, 1.0716231798978801, 1.0710596259859309, 1.0729870455465766, 1.0723478257044141, 1.0711821125273342, 1.0712838476933004, 1.0711862420644114, 1.0702900594754385, 1.0701093460255335, 1.0704754185627618, 1.070406230127542, 1.070225226560902, 1.069703559806949, 1.0693469277642347, 1.0699839250752567, 1.0746356525460308, 1.0707159674877504, 1.0697319553128504, 1.0688817364968803, 1.0692082435932981, 1.0684759893456524, 1.0683971835847264, 1.0680451367670016, 1.0686369623246867, 1.069640880786418, 1.0704251520932333, 1.0693740798217806, 1.0688587677307442, 1.0682074145614733, 1.0686366314271147, 1.067534717348322, 1.0680988370025917, 1.0675106102925795, 1.0690644999059564, 1.0678518001548563, 1.0676024943161795, 1.0679364610256845, 1.0696712610902728, 1.0698204697035176, 1.0684341611313868, 1.0682360350963271, 1.06810587558903, 1.0669739783911734, 1.068021672119595, 1.0672297291442354, 1.0675895176873804, 1.068197841761783, 1.0671013879580176, 1.0660046360086366, 1.0663276358551559, 1.065287320398452, 1.067337227895764, 1.0655675022019497, 1.0667464691021113, 1.0654826989898447, 1.0657445135547394, 1.0651516824042773, 1.0661905727836876, 1.06623421562281, 1.0656389095455225, 1.066439958813254, 1.0652632379923512, 1.065912201712998, 1.0663065229842794, 1.0650051540417838, 1.0656429950216713, 1.0660253293705182, 1.0651587378073033, 1.0673111415007277, 1.066606121973825, 1.0636487204925724, 1.063340591429685, 1.062920970201982, 1.0640791699626853, 1.0646190112621143, 1.0643690636025807, 1.0633087551324518, 1.062120326441661, 1.0637159183040048, 1.0624567845518829, 1.0615940264363064, 1.0620228740468896, 1.065944035758228, 1.0645216723976683, 1.0629741802352655, 1.0618552143079298, 1.061788820388136, 1.0619984552356008, 1.0623564345146352, 1.0612255701538964, 1.0604121363383299, 1.0617994002003444, 1.062890904688982, 1.0615332508968376, 1.060631140668779, 1.061124049613608, 1.0655036892489487, 1.061626819173903, 1.0623801487916793, 1.0612982726439804, 1.0604106475685167, 1.0600364215075357, 1.0594251114484956, 1.0605016342668319, 1.0609842873696673, 1.060558268568599, 1.0623977577417048, 1.0620874475404711, 1.0625477201884777, 1.0608919649643085, 1.0612797527097824, 1.0607660397367067, 1.0614133039294327, 1.0600360805249067, 1.059189018111454, 1.057829529108208, 1.0588771219860602, 1.0583090737125467, 1.058359621584538, 1.0582258181895075, 1.0589220753930189, 1.0617478625240757, 1.0584103943141334, 1.05809342366714, 1.057737760303936, 1.0583958870086827, 1.060983563008005, 1.0585576057434083, 1.0560353091610042, 1.0557599012611827, 1.0563696029984242, 1.0568410710142868, 1.058705023549176, 1.0559058264785235, 1.056136209592682, 1.0547860282402508, 1.054995102069706, 1.0563408776475174, 1.0561867056441259, 1.0560060508197338, 1.0556232144945212, 1.0574648557991952, 1.0571412036551098, 1.0584447258796534, 1.0599988825756912, 1.061213457902599, 1.0641711714331374, 1.062225145334091, 1.0598972112980711, 1.057624468323631, 1.0574230087366436, 1.0590626251036626, 1.0582957225169, 1.0562295589114117, 1.0565997928564554, 1.0579641297123026, 1.0582633465956859, 1.0581638739094352, 1.0581890188693022, 1.0586145727541412, 1.0589688034762592, 1.0581337059547768, 1.0567488182741513, 1.0590216270462443, 1.060112769549877, 1.0577512641951778, 1.0585086904267267, 1.058301311596708, 1.0590144262176764, 1.0590187409575225, 1.0587125907933197, 1.058331322767896, 1.0573077533768922, 1.057637815847534, 1.0562208389599466, 1.0570662346708701, 1.0605435907473555, 1.0604611685143848, 1.0590126380048983, 1.058675844223837, 1.0574211755082836, 1.0569367405570262, 1.0557581650403, 1.055671831127065, 1.0544614716966538, 1.0560598199617202, 1.055847290213348, 1.0557352804305373, 1.0562888890076467, 1.058181422006423, 1.0567589257042511, 1.0562256143078421, 1.0547176983322206, 1.0544173360360476, 1.0557806782164367, 1.0547817478434507, 1.054259728626549, 1.0550921282973866, 1.0538896299730336, 1.0533936413161809, 1.0539051175117493, 1.0552096037894059, 1.0534124457126282, 1.0540560790890297, 1.0543836969124953, 1.0535915089828523, 1.0517846638172315, 1.0534382732007537, 1.0527266459543358, 1.053191539100553, 1.0557745600627924, 1.0519608679738133, 1.0513263793941396, 1.0502197405151272, 1.0534011267049113, 1.0521018547688665, 1.0529525465054679, 1.0511572807966072, 1.0507205726429667, 1.0515755765002366, 1.052186657075275, 1.0517046577387033, 1.0511216371455967, 1.0535952300261668, 1.0540377231593985, 1.0519558257880397, 1.0517076100167309, 1.0510308862956397, 1.0528355789869963, 1.0524677572309113, 1.052745018406815, 1.052334460340731, 1.0498612344632159, 1.0516292933810663, 1.0518961953676211, 1.0511782850083384, 1.0517203225492207, 1.0516148746380816, 1.0501723503430032, 1.0489847374647794, 1.0512683261836089, 1.0485279974017054, 1.048698665718768], 'acc': [0.39240246406570845, 0.39722792608108853, 0.3957905544392627, 0.398870636550308, 0.40010266940451744, 0.3966119096631632, 0.39496919920312307, 0.3965092402219283, 0.3980492813141684, 0.40143737168772264, 0.4020533881025882, 0.3988706365625472, 0.4019507186735925, 0.39445585216829665, 0.39897330595482544, 0.40410677615621987, 0.39702258726899387, 0.4018480492813142, 0.4020533881025882, 0.3973305954703071, 0.40041067760583066, 0.39897330594258634, 0.40061601641486555, 0.40451745377428966, 0.39743326486258534, 0.4019507186858316, 0.4002053388090349, 0.4054414784394251, 0.4006160164271047, 0.4064681724845996, 0.3888090349198367, 0.40533880904714675, 0.40379876795490666, 0.39548254618899287, 0.4071868583284609, 0.4078028747188482, 0.40554414785618165, 0.40564681724845997, 0.4034907597658326, 0.40492813140459866, 0.40359342915811086, 0.39784394249289434, 0.4052361396303901, 0.4066735112691562, 0.3991786447393821, 0.40328542094455855, 0.402874743350967, 0.40071868584386133, 0.40605749486652976, 0.4065708418952366, 0.4012320328664486, 0.39948665298965186, 0.40431211499279285, 0.4005133470225873, 0.4028747433264887, 0.4026694045419321, 0.4030800821355236, 0.40328542095679765, 0.40338809035213574, 0.40051334703482644, 0.3960985626160976, 0.40236139630390144, 0.40472279260780286, 0.40852156057494865, 0.40533880904714675, 0.40872689937174445, 0.40246406569617976, 0.40379876797938496, 0.40605749486652976, 0.4072895277085001, 0.4057494866529774, 0.40154004108000096, 0.40061601643934386, 0.4066735113181128, 0.40646817249683875, 0.40564681724845997, 0.404106776168459, 0.4054414784516642, 0.4044147843942505, 0.40903490760977507, 0.4069815194949477, 0.40030800821967194, 0.40215605749486655, 0.40308008214776275, 0.4049281314290769, 0.40616016427104723, 0.4013347022342486, 0.406981519519426, 0.4093429158110883, 0.4083162217781529, 0.4024640656839406, 0.4030800821355236, 0.4096509240368798, 0.40605749486652976, 0.40359342917035, 0.404722792620042, 0.41006160163047134, 0.4070841888994652, 0.4078028747188482, 0.41026694043950623, 0.4057494866407383, 0.4102669404517454, 0.4092402464126904, 0.40790554412336566, 0.4084188911581921, 0.40954825463236233, 0.41365503078858223, 0.4162217658892794, 0.4081108829446397, 0.4137577002053388, 0.40831622174143545, 0.40523613960591187, 0.40687885011490854, 0.4128336755646817, 0.4050308008091161, 0.4040041067761807, 0.40903490760977507, 0.40934291582332744, 0.4148870636795091, 0.40985626283979515, 0.40924024638209255, 0.41283367557692086, 0.41129363450915907, 0.40112936344969197, 0.41344969199178644, 0.40770020532656986, 0.415811088307927, 0.4163244353305143, 0.410677618057576, 0.40903490760977507, 0.4089322381930185, 0.40965092402464065, 0.4112936344969199, 0.4074948665297741, 0.4142710472401652, 0.40657084186463877, 0.4119096509117855, 0.4190965092280073, 0.4165297741334296, 0.4140657084433695, 0.4086242299917053, 0.418172484575111, 0.4136550307763431, 0.412217659137577, 0.41293634494472087, 0.41334702259338857, 0.4147843942627525, 0.4170431211560169, 0.4125256673266511, 0.4126283367311685, 0.41006160163047134, 0.42412731006160165, 0.4219712525697949, 0.40954825462012323, 0.41652977412731007, 0.4131416837659949, 0.41293634498143833, 0.4240246406570842, 0.4178644763737978, 0.4229979466241488, 0.41478439425051333, 0.4130390143614775, 0.4204312114989733, 0.4208418891170431, 0.4183778234086242, 0.4142710472156869, 0.41078028744985434, 0.41026694045786494, 0.4176591375647629, 0.41540041067761807, 0.4126283367556468, 0.41971252565511197, 0.423305954825462, 0.4236139630145361, 0.42022587266546013, 0.42032854209445586, 0.419199178657003, 0.41930184804928133, 0.41632443534275343, 0.4146817248215176, 0.4171457905666539, 0.42084188914152143, 0.4163244353305143, 0.4147843942382742, 0.4214579055502674, 0.4275154003984385, 0.42566735112936344, 0.42053388090349075, 0.4246406570597106, 0.42268993837387897, 0.4217659137699393, 0.42320328544542285, 0.42176591375770023, 0.4251540041128958, 0.41858316224213743, 0.4185831622298983, 0.42412731003712334, 0.42731006161388185, 0.4232032854331837, 0.42433264889511485, 0.41601642712920106, 0.41776180695704124, 0.41149897331819396, 0.40739219711301755, 0.4130390143859558, 0.413860369585378, 0.4173511293634497, 0.42166324435318275, 0.4101642710472279, 0.4151950718685832, 0.41889117044345064, 0.4187885010144549, 0.4123203285543336, 0.41047227927301944, 0.41427104722792607, 0.412731006135686, 0.41437371663244355, 0.41796714579055444, 0.4162217659137577, 0.4223819302092832, 0.4212525667351129, 0.41283367557692086, 0.41652977410283176, 0.4194045174660379, 0.4157084188789313, 0.4170431211376582, 0.4171457905299365, 0.41396303902661286, 0.42125256671063466, 0.42238193019704406, 0.41601642709248365, 0.4172484599466931, 0.41899383983572897, 0.4159137577124445, 0.4247433264887064, 0.415811088307927, 0.4221765914002483, 0.4149897330595482, 0.4142710472156869, 0.4174537987679671, 0.417453798755728, 0.4219712525697949, 0.4240246406570842, 0.4273100616016427, 0.41745379874348887, 0.41642710471055344, 0.4160164271047228, 0.4171457905544148, 0.4182751539796285, 0.4206365503202474, 0.42258726899383986, 0.4227926078151139, 0.4249486652855021, 0.42279260777839645, 0.4132443531705124, 0.42915811088295686, 0.4279260780165083, 0.42618069812747245, 0.4140657084188912, 0.4262833675687073, 0.4246406570841889, 0.42720739217264697, 0.4305954825462012, 0.41581108828344876, 0.4238193018358101, 0.41889117041897234, 0.424024640644845, 0.4137577001808605, 0.4287474332526479, 0.4295687885010267, 0.42505133471449785, 0.42114989731835634, 0.4240246406570842, 0.42474332647646723, 0.4294661190965092, 0.42987679669010076, 0.42484599590546296, 0.4238193018480493, 0.4268993839835729, 0.42474332647646723, 0.42669404519901627, 0.4178644763982761, 0.4243326488828757, 0.4206365503141278, 0.42710472278036865, 0.4227926078151139, 0.42351129362225776, 0.42217659137577, 0.42577002054612006, 0.4248459958932238, 0.42464065709030846, 0.4249486653222196, 0.42494866527326297, 0.4196098562628337, 0.4174537987679671, 0.4228952772135118, 0.4260780287351941, 0.4194045174568585, 0.4221765914002483, 0.41848049280090255]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
