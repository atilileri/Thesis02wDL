{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf64.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 17:39:03 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'Front', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['mb', 'eg', 'sg', 'ek', 'eo', 'aa', 'ib', 'yd', 'eb', 'ce', 'ds', 'by', 'sk', 'ck', 'my'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001A00027D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001A067BB6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7077, Accuracy:0.0419, Validation Loss:2.7012, Validation Accuracy:0.0575\n",
    "Epoch #2: Loss:2.7006, Accuracy:0.0575, Validation Loss:2.6945, Validation Accuracy:0.0608\n",
    "Epoch #3: Loss:2.6935, Accuracy:0.0595, Validation Loss:2.6880, Validation Accuracy:0.0575\n",
    "Epoch #4: Loss:2.6869, Accuracy:0.0620, Validation Loss:2.6821, Validation Accuracy:0.0788\n",
    "Epoch #5: Loss:2.6810, Accuracy:0.0932, Validation Loss:2.6757, Validation Accuracy:0.1248\n",
    "Epoch #6: Loss:2.6745, Accuracy:0.1331, Validation Loss:2.6693, Validation Accuracy:0.1363\n",
    "Epoch #7: Loss:2.6684, Accuracy:0.1429, Validation Loss:2.6627, Validation Accuracy:0.1396\n",
    "Epoch #8: Loss:2.6624, Accuracy:0.1446, Validation Loss:2.6555, Validation Accuracy:0.1396\n",
    "Epoch #9: Loss:2.6550, Accuracy:0.1446, Validation Loss:2.6475, Validation Accuracy:0.1396\n",
    "Epoch #10: Loss:2.6464, Accuracy:0.1433, Validation Loss:2.6370, Validation Accuracy:0.1379\n",
    "Epoch #11: Loss:2.6368, Accuracy:0.1437, Validation Loss:2.6246, Validation Accuracy:0.1379\n",
    "Epoch #12: Loss:2.6247, Accuracy:0.1433, Validation Loss:2.6099, Validation Accuracy:0.1379\n",
    "Epoch #13: Loss:2.6108, Accuracy:0.1437, Validation Loss:2.5921, Validation Accuracy:0.1396\n",
    "Epoch #14: Loss:2.5956, Accuracy:0.1433, Validation Loss:2.5729, Validation Accuracy:0.1511\n",
    "Epoch #15: Loss:2.5807, Accuracy:0.1376, Validation Loss:2.5558, Validation Accuracy:0.1478\n",
    "Epoch #16: Loss:2.5648, Accuracy:0.1339, Validation Loss:2.5397, Validation Accuracy:0.1494\n",
    "Epoch #17: Loss:2.5522, Accuracy:0.1384, Validation Loss:2.5272, Validation Accuracy:0.1494\n",
    "Epoch #18: Loss:2.5400, Accuracy:0.1372, Validation Loss:2.5169, Validation Accuracy:0.1576\n",
    "Epoch #19: Loss:2.5290, Accuracy:0.1421, Validation Loss:2.5103, Validation Accuracy:0.1593\n",
    "Epoch #20: Loss:2.5210, Accuracy:0.1446, Validation Loss:2.5055, Validation Accuracy:0.1724\n",
    "Epoch #21: Loss:2.5165, Accuracy:0.1544, Validation Loss:2.5404, Validation Accuracy:0.1297\n",
    "Epoch #22: Loss:2.5188, Accuracy:0.1569, Validation Loss:2.5019, Validation Accuracy:0.1708\n",
    "Epoch #23: Loss:2.5106, Accuracy:0.1573, Validation Loss:2.5173, Validation Accuracy:0.1461\n",
    "Epoch #24: Loss:2.5129, Accuracy:0.1532, Validation Loss:2.4984, Validation Accuracy:0.1724\n",
    "Epoch #25: Loss:2.5054, Accuracy:0.1532, Validation Loss:2.4911, Validation Accuracy:0.1741\n",
    "Epoch #26: Loss:2.5045, Accuracy:0.1569, Validation Loss:2.4921, Validation Accuracy:0.1593\n",
    "Epoch #27: Loss:2.4972, Accuracy:0.1618, Validation Loss:2.4895, Validation Accuracy:0.1691\n",
    "Epoch #28: Loss:2.4951, Accuracy:0.1659, Validation Loss:2.4919, Validation Accuracy:0.1511\n",
    "Epoch #29: Loss:2.4913, Accuracy:0.1663, Validation Loss:2.4829, Validation Accuracy:0.1708\n",
    "Epoch #30: Loss:2.4851, Accuracy:0.1717, Validation Loss:2.4807, Validation Accuracy:0.1708\n",
    "Epoch #31: Loss:2.4831, Accuracy:0.1692, Validation Loss:2.4798, Validation Accuracy:0.1576\n",
    "Epoch #32: Loss:2.4797, Accuracy:0.1725, Validation Loss:2.4774, Validation Accuracy:0.1626\n",
    "Epoch #33: Loss:2.4790, Accuracy:0.1733, Validation Loss:2.4729, Validation Accuracy:0.1609\n",
    "Epoch #34: Loss:2.4746, Accuracy:0.1749, Validation Loss:2.4709, Validation Accuracy:0.1642\n",
    "Epoch #35: Loss:2.4722, Accuracy:0.1741, Validation Loss:2.4709, Validation Accuracy:0.1626\n",
    "Epoch #36: Loss:2.4716, Accuracy:0.1737, Validation Loss:2.4670, Validation Accuracy:0.1626\n",
    "Epoch #37: Loss:2.4755, Accuracy:0.1741, Validation Loss:2.5139, Validation Accuracy:0.1461\n",
    "Epoch #38: Loss:2.4823, Accuracy:0.1721, Validation Loss:2.4648, Validation Accuracy:0.1576\n",
    "Epoch #39: Loss:2.4659, Accuracy:0.1766, Validation Loss:2.4720, Validation Accuracy:0.1527\n",
    "Epoch #40: Loss:2.4623, Accuracy:0.1749, Validation Loss:2.4633, Validation Accuracy:0.1658\n",
    "Epoch #41: Loss:2.4630, Accuracy:0.1741, Validation Loss:2.4685, Validation Accuracy:0.1478\n",
    "Epoch #42: Loss:2.4596, Accuracy:0.1786, Validation Loss:2.4667, Validation Accuracy:0.1494\n",
    "Epoch #43: Loss:2.4585, Accuracy:0.1795, Validation Loss:2.4682, Validation Accuracy:0.1494\n",
    "Epoch #44: Loss:2.4572, Accuracy:0.1786, Validation Loss:2.4675, Validation Accuracy:0.1494\n",
    "Epoch #45: Loss:2.4558, Accuracy:0.1799, Validation Loss:2.4697, Validation Accuracy:0.1511\n",
    "Epoch #46: Loss:2.4548, Accuracy:0.1795, Validation Loss:2.4656, Validation Accuracy:0.1494\n",
    "Epoch #47: Loss:2.4535, Accuracy:0.1786, Validation Loss:2.4682, Validation Accuracy:0.1461\n",
    "Epoch #48: Loss:2.4532, Accuracy:0.1791, Validation Loss:2.4641, Validation Accuracy:0.1494\n",
    "Epoch #49: Loss:2.4532, Accuracy:0.1774, Validation Loss:2.4621, Validation Accuracy:0.1494\n",
    "Epoch #50: Loss:2.4528, Accuracy:0.1795, Validation Loss:2.4625, Validation Accuracy:0.1461\n",
    "Epoch #51: Loss:2.4529, Accuracy:0.1786, Validation Loss:2.4644, Validation Accuracy:0.1429\n",
    "Epoch #52: Loss:2.4524, Accuracy:0.1778, Validation Loss:2.4627, Validation Accuracy:0.1429\n",
    "Epoch #53: Loss:2.4511, Accuracy:0.1754, Validation Loss:2.4660, Validation Accuracy:0.1396\n",
    "Epoch #54: Loss:2.4502, Accuracy:0.1774, Validation Loss:2.4633, Validation Accuracy:0.1445\n",
    "Epoch #55: Loss:2.4494, Accuracy:0.1770, Validation Loss:2.4635, Validation Accuracy:0.1429\n",
    "Epoch #56: Loss:2.4493, Accuracy:0.1754, Validation Loss:2.4613, Validation Accuracy:0.1396\n",
    "Epoch #57: Loss:2.4491, Accuracy:0.1774, Validation Loss:2.4627, Validation Accuracy:0.1396\n",
    "Epoch #58: Loss:2.4499, Accuracy:0.1778, Validation Loss:2.4609, Validation Accuracy:0.1429\n",
    "Epoch #59: Loss:2.4497, Accuracy:0.1786, Validation Loss:2.4639, Validation Accuracy:0.1396\n",
    "Epoch #60: Loss:2.4494, Accuracy:0.1807, Validation Loss:2.4599, Validation Accuracy:0.1494\n",
    "Epoch #61: Loss:2.4476, Accuracy:0.1766, Validation Loss:2.4684, Validation Accuracy:0.1412\n",
    "Epoch #62: Loss:2.4472, Accuracy:0.1758, Validation Loss:2.4588, Validation Accuracy:0.1445\n",
    "Epoch #63: Loss:2.4466, Accuracy:0.1754, Validation Loss:2.4613, Validation Accuracy:0.1412\n",
    "Epoch #64: Loss:2.4461, Accuracy:0.1741, Validation Loss:2.4615, Validation Accuracy:0.1412\n",
    "Epoch #65: Loss:2.4466, Accuracy:0.1786, Validation Loss:2.4596, Validation Accuracy:0.1461\n",
    "Epoch #66: Loss:2.4446, Accuracy:0.1749, Validation Loss:2.4651, Validation Accuracy:0.1412\n",
    "Epoch #67: Loss:2.4438, Accuracy:0.1749, Validation Loss:2.4577, Validation Accuracy:0.1478\n",
    "Epoch #68: Loss:2.4443, Accuracy:0.1811, Validation Loss:2.4586, Validation Accuracy:0.1429\n",
    "Epoch #69: Loss:2.4429, Accuracy:0.1749, Validation Loss:2.4597, Validation Accuracy:0.1412\n",
    "Epoch #70: Loss:2.4429, Accuracy:0.1766, Validation Loss:2.4565, Validation Accuracy:0.1544\n",
    "Epoch #71: Loss:2.4426, Accuracy:0.1774, Validation Loss:2.4596, Validation Accuracy:0.1412\n",
    "Epoch #72: Loss:2.4419, Accuracy:0.1766, Validation Loss:2.4581, Validation Accuracy:0.1429\n",
    "Epoch #73: Loss:2.4418, Accuracy:0.1786, Validation Loss:2.4589, Validation Accuracy:0.1445\n",
    "Epoch #74: Loss:2.4421, Accuracy:0.1770, Validation Loss:2.4599, Validation Accuracy:0.1445\n",
    "Epoch #75: Loss:2.4435, Accuracy:0.1770, Validation Loss:2.4564, Validation Accuracy:0.1511\n",
    "Epoch #76: Loss:2.4422, Accuracy:0.1762, Validation Loss:2.4620, Validation Accuracy:0.1396\n",
    "Epoch #77: Loss:2.4425, Accuracy:0.1807, Validation Loss:2.4560, Validation Accuracy:0.1576\n",
    "Epoch #78: Loss:2.4417, Accuracy:0.1819, Validation Loss:2.4614, Validation Accuracy:0.1396\n",
    "Epoch #79: Loss:2.4405, Accuracy:0.1758, Validation Loss:2.4562, Validation Accuracy:0.1576\n",
    "Epoch #80: Loss:2.4410, Accuracy:0.1778, Validation Loss:2.4587, Validation Accuracy:0.1445\n",
    "Epoch #81: Loss:2.4398, Accuracy:0.1762, Validation Loss:2.4581, Validation Accuracy:0.1445\n",
    "Epoch #82: Loss:2.4395, Accuracy:0.1828, Validation Loss:2.4568, Validation Accuracy:0.1445\n",
    "Epoch #83: Loss:2.4404, Accuracy:0.1766, Validation Loss:2.4576, Validation Accuracy:0.1445\n",
    "Epoch #84: Loss:2.4388, Accuracy:0.1786, Validation Loss:2.4557, Validation Accuracy:0.1527\n",
    "Epoch #85: Loss:2.4387, Accuracy:0.1803, Validation Loss:2.4582, Validation Accuracy:0.1412\n",
    "Epoch #86: Loss:2.4386, Accuracy:0.1745, Validation Loss:2.4581, Validation Accuracy:0.1412\n",
    "Epoch #87: Loss:2.4388, Accuracy:0.1770, Validation Loss:2.4579, Validation Accuracy:0.1429\n",
    "Epoch #88: Loss:2.4386, Accuracy:0.1778, Validation Loss:2.4570, Validation Accuracy:0.1527\n",
    "Epoch #89: Loss:2.4394, Accuracy:0.1778, Validation Loss:2.4597, Validation Accuracy:0.1429\n",
    "Epoch #90: Loss:2.4392, Accuracy:0.1778, Validation Loss:2.4555, Validation Accuracy:0.1544\n",
    "Epoch #91: Loss:2.4402, Accuracy:0.1762, Validation Loss:2.4629, Validation Accuracy:0.1412\n",
    "Epoch #92: Loss:2.4381, Accuracy:0.1758, Validation Loss:2.4554, Validation Accuracy:0.1576\n",
    "Epoch #93: Loss:2.4381, Accuracy:0.1770, Validation Loss:2.4591, Validation Accuracy:0.1429\n",
    "Epoch #94: Loss:2.4381, Accuracy:0.1766, Validation Loss:2.4576, Validation Accuracy:0.1511\n",
    "Epoch #95: Loss:2.4389, Accuracy:0.1795, Validation Loss:2.4576, Validation Accuracy:0.1527\n",
    "Epoch #96: Loss:2.4376, Accuracy:0.1778, Validation Loss:2.4597, Validation Accuracy:0.1412\n",
    "Epoch #97: Loss:2.4381, Accuracy:0.1737, Validation Loss:2.4548, Validation Accuracy:0.1511\n",
    "Epoch #98: Loss:2.4367, Accuracy:0.1774, Validation Loss:2.4596, Validation Accuracy:0.1429\n",
    "Epoch #99: Loss:2.4375, Accuracy:0.1770, Validation Loss:2.4578, Validation Accuracy:0.1511\n",
    "Epoch #100: Loss:2.4364, Accuracy:0.1811, Validation Loss:2.4575, Validation Accuracy:0.1445\n",
    "Epoch #101: Loss:2.4363, Accuracy:0.1807, Validation Loss:2.4565, Validation Accuracy:0.1429\n",
    "Epoch #102: Loss:2.4356, Accuracy:0.1791, Validation Loss:2.4548, Validation Accuracy:0.1560\n",
    "Epoch #103: Loss:2.4355, Accuracy:0.1791, Validation Loss:2.4579, Validation Accuracy:0.1511\n",
    "Epoch #104: Loss:2.4365, Accuracy:0.1762, Validation Loss:2.4617, Validation Accuracy:0.1494\n",
    "Epoch #105: Loss:2.4351, Accuracy:0.1803, Validation Loss:2.4590, Validation Accuracy:0.1511\n",
    "Epoch #106: Loss:2.4359, Accuracy:0.1778, Validation Loss:2.4620, Validation Accuracy:0.1461\n",
    "Epoch #107: Loss:2.4352, Accuracy:0.1754, Validation Loss:2.4593, Validation Accuracy:0.1511\n",
    "Epoch #108: Loss:2.4345, Accuracy:0.1754, Validation Loss:2.4622, Validation Accuracy:0.1461\n",
    "Epoch #109: Loss:2.4346, Accuracy:0.1762, Validation Loss:2.4597, Validation Accuracy:0.1494\n",
    "Epoch #110: Loss:2.4342, Accuracy:0.1786, Validation Loss:2.4602, Validation Accuracy:0.1544\n",
    "Epoch #111: Loss:2.4346, Accuracy:0.1737, Validation Loss:2.4613, Validation Accuracy:0.1544\n",
    "Epoch #112: Loss:2.4340, Accuracy:0.1762, Validation Loss:2.4597, Validation Accuracy:0.1494\n",
    "Epoch #113: Loss:2.4344, Accuracy:0.1803, Validation Loss:2.4627, Validation Accuracy:0.1544\n",
    "Epoch #114: Loss:2.4343, Accuracy:0.1762, Validation Loss:2.4614, Validation Accuracy:0.1511\n",
    "Epoch #115: Loss:2.4344, Accuracy:0.1807, Validation Loss:2.4601, Validation Accuracy:0.1527\n",
    "Epoch #116: Loss:2.4338, Accuracy:0.1762, Validation Loss:2.4625, Validation Accuracy:0.1544\n",
    "Epoch #117: Loss:2.4338, Accuracy:0.1762, Validation Loss:2.4616, Validation Accuracy:0.1494\n",
    "Epoch #118: Loss:2.4333, Accuracy:0.1791, Validation Loss:2.4613, Validation Accuracy:0.1527\n",
    "Epoch #119: Loss:2.4338, Accuracy:0.1782, Validation Loss:2.4621, Validation Accuracy:0.1527\n",
    "Epoch #120: Loss:2.4336, Accuracy:0.1741, Validation Loss:2.4628, Validation Accuracy:0.1527\n",
    "Epoch #121: Loss:2.4332, Accuracy:0.1766, Validation Loss:2.4592, Validation Accuracy:0.1511\n",
    "Epoch #122: Loss:2.4328, Accuracy:0.1819, Validation Loss:2.4643, Validation Accuracy:0.1445\n",
    "Epoch #123: Loss:2.4333, Accuracy:0.1766, Validation Loss:2.4618, Validation Accuracy:0.1511\n",
    "Epoch #124: Loss:2.4337, Accuracy:0.1819, Validation Loss:2.4601, Validation Accuracy:0.1494\n",
    "Epoch #125: Loss:2.4339, Accuracy:0.1766, Validation Loss:2.4622, Validation Accuracy:0.1445\n",
    "Epoch #126: Loss:2.4325, Accuracy:0.1762, Validation Loss:2.4591, Validation Accuracy:0.1511\n",
    "Epoch #127: Loss:2.4326, Accuracy:0.1807, Validation Loss:2.4624, Validation Accuracy:0.1544\n",
    "Epoch #128: Loss:2.4334, Accuracy:0.1774, Validation Loss:2.4621, Validation Accuracy:0.1511\n",
    "Epoch #129: Loss:2.4324, Accuracy:0.1807, Validation Loss:2.4595, Validation Accuracy:0.1511\n",
    "Epoch #130: Loss:2.4330, Accuracy:0.1774, Validation Loss:2.4643, Validation Accuracy:0.1445\n",
    "Epoch #131: Loss:2.4320, Accuracy:0.1758, Validation Loss:2.4612, Validation Accuracy:0.1527\n",
    "Epoch #132: Loss:2.4325, Accuracy:0.1807, Validation Loss:2.4638, Validation Accuracy:0.1412\n",
    "Epoch #133: Loss:2.4320, Accuracy:0.1807, Validation Loss:2.4630, Validation Accuracy:0.1461\n",
    "Epoch #134: Loss:2.4325, Accuracy:0.1819, Validation Loss:2.4604, Validation Accuracy:0.1461\n",
    "Epoch #135: Loss:2.4342, Accuracy:0.1815, Validation Loss:2.4620, Validation Accuracy:0.1461\n",
    "Epoch #136: Loss:2.4315, Accuracy:0.1811, Validation Loss:2.4599, Validation Accuracy:0.1527\n",
    "Epoch #137: Loss:2.4327, Accuracy:0.1819, Validation Loss:2.4653, Validation Accuracy:0.1429\n",
    "Epoch #138: Loss:2.4329, Accuracy:0.1766, Validation Loss:2.4641, Validation Accuracy:0.1461\n",
    "Epoch #139: Loss:2.4319, Accuracy:0.1799, Validation Loss:2.4584, Validation Accuracy:0.1527\n",
    "Epoch #140: Loss:2.4332, Accuracy:0.1807, Validation Loss:2.4617, Validation Accuracy:0.1511\n",
    "Epoch #141: Loss:2.4316, Accuracy:0.1836, Validation Loss:2.4630, Validation Accuracy:0.1461\n",
    "Epoch #142: Loss:2.4316, Accuracy:0.1811, Validation Loss:2.4620, Validation Accuracy:0.1461\n",
    "Epoch #143: Loss:2.4314, Accuracy:0.1786, Validation Loss:2.4610, Validation Accuracy:0.1478\n",
    "Epoch #144: Loss:2.4317, Accuracy:0.1795, Validation Loss:2.4599, Validation Accuracy:0.1494\n",
    "Epoch #145: Loss:2.4307, Accuracy:0.1840, Validation Loss:2.4628, Validation Accuracy:0.1494\n",
    "Epoch #146: Loss:2.4315, Accuracy:0.1811, Validation Loss:2.4634, Validation Accuracy:0.1494\n",
    "Epoch #147: Loss:2.4315, Accuracy:0.1823, Validation Loss:2.4636, Validation Accuracy:0.1494\n",
    "Epoch #148: Loss:2.4324, Accuracy:0.1852, Validation Loss:2.4645, Validation Accuracy:0.1461\n",
    "Epoch #149: Loss:2.4318, Accuracy:0.1836, Validation Loss:2.4613, Validation Accuracy:0.1511\n",
    "Epoch #150: Loss:2.4314, Accuracy:0.1803, Validation Loss:2.4642, Validation Accuracy:0.1478\n",
    "Epoch #151: Loss:2.4316, Accuracy:0.1807, Validation Loss:2.4638, Validation Accuracy:0.1494\n",
    "Epoch #152: Loss:2.4335, Accuracy:0.1864, Validation Loss:2.4621, Validation Accuracy:0.1478\n",
    "Epoch #153: Loss:2.4316, Accuracy:0.1770, Validation Loss:2.4642, Validation Accuracy:0.1461\n",
    "Epoch #154: Loss:2.4308, Accuracy:0.1807, Validation Loss:2.4635, Validation Accuracy:0.1461\n",
    "Epoch #155: Loss:2.4330, Accuracy:0.1811, Validation Loss:2.4658, Validation Accuracy:0.1461\n",
    "Epoch #156: Loss:2.4319, Accuracy:0.1778, Validation Loss:2.4683, Validation Accuracy:0.1445\n",
    "Epoch #157: Loss:2.4310, Accuracy:0.1803, Validation Loss:2.4633, Validation Accuracy:0.1461\n",
    "Epoch #158: Loss:2.4319, Accuracy:0.1782, Validation Loss:2.4648, Validation Accuracy:0.1494\n",
    "Epoch #159: Loss:2.4298, Accuracy:0.1823, Validation Loss:2.4631, Validation Accuracy:0.1511\n",
    "Epoch #160: Loss:2.4300, Accuracy:0.1811, Validation Loss:2.4654, Validation Accuracy:0.1511\n",
    "Epoch #161: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4599, Validation Accuracy:0.1527\n",
    "Epoch #162: Loss:2.4315, Accuracy:0.1811, Validation Loss:2.4630, Validation Accuracy:0.1511\n",
    "Epoch #163: Loss:2.4344, Accuracy:0.1786, Validation Loss:2.4675, Validation Accuracy:0.1494\n",
    "Epoch #164: Loss:2.4326, Accuracy:0.1823, Validation Loss:2.4616, Validation Accuracy:0.1544\n",
    "Epoch #165: Loss:2.4377, Accuracy:0.1803, Validation Loss:2.4613, Validation Accuracy:0.1461\n",
    "Epoch #166: Loss:2.4356, Accuracy:0.1819, Validation Loss:2.4651, Validation Accuracy:0.1461\n",
    "Epoch #167: Loss:2.4328, Accuracy:0.1832, Validation Loss:2.4590, Validation Accuracy:0.1544\n",
    "Epoch #168: Loss:2.4339, Accuracy:0.1795, Validation Loss:2.4597, Validation Accuracy:0.1527\n",
    "Epoch #169: Loss:2.4338, Accuracy:0.1766, Validation Loss:2.4629, Validation Accuracy:0.1560\n",
    "Epoch #170: Loss:2.4326, Accuracy:0.1803, Validation Loss:2.4580, Validation Accuracy:0.1576\n",
    "Epoch #171: Loss:2.4327, Accuracy:0.1819, Validation Loss:2.4610, Validation Accuracy:0.1544\n",
    "Epoch #172: Loss:2.4319, Accuracy:0.1819, Validation Loss:2.4594, Validation Accuracy:0.1494\n",
    "Epoch #173: Loss:2.4314, Accuracy:0.1786, Validation Loss:2.4590, Validation Accuracy:0.1576\n",
    "Epoch #174: Loss:2.4318, Accuracy:0.1795, Validation Loss:2.4602, Validation Accuracy:0.1527\n",
    "Epoch #175: Loss:2.4336, Accuracy:0.1774, Validation Loss:2.4584, Validation Accuracy:0.1527\n",
    "Epoch #176: Loss:2.4339, Accuracy:0.1795, Validation Loss:2.4633, Validation Accuracy:0.1494\n",
    "Epoch #177: Loss:2.4329, Accuracy:0.1836, Validation Loss:2.4580, Validation Accuracy:0.1527\n",
    "Epoch #178: Loss:2.4322, Accuracy:0.1807, Validation Loss:2.4594, Validation Accuracy:0.1511\n",
    "Epoch #179: Loss:2.4331, Accuracy:0.1811, Validation Loss:2.4606, Validation Accuracy:0.1593\n",
    "Epoch #180: Loss:2.4301, Accuracy:0.1844, Validation Loss:2.4593, Validation Accuracy:0.1544\n",
    "Epoch #181: Loss:2.4321, Accuracy:0.1811, Validation Loss:2.4596, Validation Accuracy:0.1576\n",
    "Epoch #182: Loss:2.4314, Accuracy:0.1832, Validation Loss:2.4589, Validation Accuracy:0.1494\n",
    "Epoch #183: Loss:2.4316, Accuracy:0.1811, Validation Loss:2.4566, Validation Accuracy:0.1560\n",
    "Epoch #184: Loss:2.4302, Accuracy:0.1819, Validation Loss:2.4605, Validation Accuracy:0.1576\n",
    "Epoch #185: Loss:2.4306, Accuracy:0.1799, Validation Loss:2.4594, Validation Accuracy:0.1576\n",
    "Epoch #186: Loss:2.4300, Accuracy:0.1815, Validation Loss:2.4579, Validation Accuracy:0.1527\n",
    "Epoch #187: Loss:2.4306, Accuracy:0.1811, Validation Loss:2.4579, Validation Accuracy:0.1576\n",
    "Epoch #188: Loss:2.4296, Accuracy:0.1803, Validation Loss:2.4604, Validation Accuracy:0.1527\n",
    "Epoch #189: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4571, Validation Accuracy:0.1527\n",
    "Epoch #190: Loss:2.4303, Accuracy:0.1791, Validation Loss:2.4600, Validation Accuracy:0.1560\n",
    "Epoch #191: Loss:2.4299, Accuracy:0.1803, Validation Loss:2.4591, Validation Accuracy:0.1544\n",
    "Epoch #192: Loss:2.4297, Accuracy:0.1819, Validation Loss:2.4602, Validation Accuracy:0.1494\n",
    "Epoch #193: Loss:2.4291, Accuracy:0.1832, Validation Loss:2.4578, Validation Accuracy:0.1511\n",
    "Epoch #194: Loss:2.4288, Accuracy:0.1832, Validation Loss:2.4577, Validation Accuracy:0.1593\n",
    "Epoch #195: Loss:2.4295, Accuracy:0.1840, Validation Loss:2.4594, Validation Accuracy:0.1560\n",
    "Epoch #196: Loss:2.4323, Accuracy:0.1832, Validation Loss:2.4644, Validation Accuracy:0.1511\n",
    "Epoch #197: Loss:2.4362, Accuracy:0.1803, Validation Loss:2.4735, Validation Accuracy:0.1429\n",
    "Epoch #198: Loss:2.4400, Accuracy:0.1885, Validation Loss:2.4630, Validation Accuracy:0.1626\n",
    "Epoch #199: Loss:2.4324, Accuracy:0.1864, Validation Loss:2.4631, Validation Accuracy:0.1511\n",
    "Epoch #200: Loss:2.4314, Accuracy:0.1823, Validation Loss:2.4601, Validation Accuracy:0.1560\n",
    "Epoch #201: Loss:2.4318, Accuracy:0.1840, Validation Loss:2.4629, Validation Accuracy:0.1511\n",
    "Epoch #202: Loss:2.4321, Accuracy:0.1823, Validation Loss:2.4670, Validation Accuracy:0.1593\n",
    "Epoch #203: Loss:2.4293, Accuracy:0.1803, Validation Loss:2.4654, Validation Accuracy:0.1527\n",
    "Epoch #204: Loss:2.4308, Accuracy:0.1803, Validation Loss:2.4691, Validation Accuracy:0.1478\n",
    "Epoch #205: Loss:2.4297, Accuracy:0.1803, Validation Loss:2.4654, Validation Accuracy:0.1544\n",
    "Epoch #206: Loss:2.4287, Accuracy:0.1811, Validation Loss:2.4635, Validation Accuracy:0.1560\n",
    "Epoch #207: Loss:2.4295, Accuracy:0.1766, Validation Loss:2.4656, Validation Accuracy:0.1544\n",
    "Epoch #208: Loss:2.4281, Accuracy:0.1799, Validation Loss:2.4652, Validation Accuracy:0.1576\n",
    "Epoch #209: Loss:2.4284, Accuracy:0.1819, Validation Loss:2.4656, Validation Accuracy:0.1494\n",
    "Epoch #210: Loss:2.4272, Accuracy:0.1815, Validation Loss:2.4662, Validation Accuracy:0.1544\n",
    "Epoch #211: Loss:2.4271, Accuracy:0.1811, Validation Loss:2.4651, Validation Accuracy:0.1527\n",
    "Epoch #212: Loss:2.4280, Accuracy:0.1828, Validation Loss:2.4692, Validation Accuracy:0.1461\n",
    "Epoch #213: Loss:2.4294, Accuracy:0.1828, Validation Loss:2.4693, Validation Accuracy:0.1461\n",
    "Epoch #214: Loss:2.4292, Accuracy:0.1852, Validation Loss:2.4687, Validation Accuracy:0.1494\n",
    "Epoch #215: Loss:2.4282, Accuracy:0.1832, Validation Loss:2.4671, Validation Accuracy:0.1511\n",
    "Epoch #216: Loss:2.4276, Accuracy:0.1811, Validation Loss:2.4706, Validation Accuracy:0.1478\n",
    "Epoch #217: Loss:2.4289, Accuracy:0.1832, Validation Loss:2.4736, Validation Accuracy:0.1494\n",
    "Epoch #218: Loss:2.4306, Accuracy:0.1799, Validation Loss:2.4731, Validation Accuracy:0.1494\n",
    "Epoch #219: Loss:2.4300, Accuracy:0.1869, Validation Loss:2.4735, Validation Accuracy:0.1560\n",
    "Epoch #220: Loss:2.4300, Accuracy:0.1836, Validation Loss:2.4704, Validation Accuracy:0.1560\n",
    "Epoch #221: Loss:2.4304, Accuracy:0.1832, Validation Loss:2.4685, Validation Accuracy:0.1560\n",
    "Epoch #222: Loss:2.4308, Accuracy:0.1823, Validation Loss:2.4707, Validation Accuracy:0.1494\n",
    "Epoch #223: Loss:2.4296, Accuracy:0.1823, Validation Loss:2.4716, Validation Accuracy:0.1511\n",
    "Epoch #224: Loss:2.4297, Accuracy:0.1832, Validation Loss:2.4693, Validation Accuracy:0.1527\n",
    "Epoch #225: Loss:2.4283, Accuracy:0.1852, Validation Loss:2.4709, Validation Accuracy:0.1576\n",
    "Epoch #226: Loss:2.4287, Accuracy:0.1848, Validation Loss:2.4703, Validation Accuracy:0.1527\n",
    "Epoch #227: Loss:2.4284, Accuracy:0.1811, Validation Loss:2.4706, Validation Accuracy:0.1511\n",
    "Epoch #228: Loss:2.4286, Accuracy:0.1828, Validation Loss:2.4704, Validation Accuracy:0.1494\n",
    "Epoch #229: Loss:2.4295, Accuracy:0.1799, Validation Loss:2.4707, Validation Accuracy:0.1494\n",
    "Epoch #230: Loss:2.4291, Accuracy:0.1840, Validation Loss:2.4684, Validation Accuracy:0.1511\n",
    "Epoch #231: Loss:2.4293, Accuracy:0.1864, Validation Loss:2.4683, Validation Accuracy:0.1609\n",
    "Epoch #232: Loss:2.4293, Accuracy:0.1815, Validation Loss:2.4683, Validation Accuracy:0.1609\n",
    "Epoch #233: Loss:2.4295, Accuracy:0.1840, Validation Loss:2.4695, Validation Accuracy:0.1642\n",
    "Epoch #234: Loss:2.4298, Accuracy:0.1844, Validation Loss:2.4721, Validation Accuracy:0.1494\n",
    "Epoch #235: Loss:2.4289, Accuracy:0.1852, Validation Loss:2.4699, Validation Accuracy:0.1544\n",
    "Epoch #236: Loss:2.4298, Accuracy:0.1836, Validation Loss:2.4693, Validation Accuracy:0.1527\n",
    "Epoch #237: Loss:2.4274, Accuracy:0.1832, Validation Loss:2.4712, Validation Accuracy:0.1478\n",
    "Epoch #238: Loss:2.4286, Accuracy:0.1848, Validation Loss:2.4686, Validation Accuracy:0.1593\n",
    "Epoch #239: Loss:2.4268, Accuracy:0.1840, Validation Loss:2.4647, Validation Accuracy:0.1626\n",
    "Epoch #240: Loss:2.4283, Accuracy:0.1836, Validation Loss:2.4670, Validation Accuracy:0.1511\n",
    "Epoch #241: Loss:2.4274, Accuracy:0.1869, Validation Loss:2.4694, Validation Accuracy:0.1544\n",
    "Epoch #242: Loss:2.4267, Accuracy:0.1844, Validation Loss:2.4668, Validation Accuracy:0.1593\n",
    "Epoch #243: Loss:2.4273, Accuracy:0.1807, Validation Loss:2.4679, Validation Accuracy:0.1511\n",
    "Epoch #244: Loss:2.4267, Accuracy:0.1864, Validation Loss:2.4653, Validation Accuracy:0.1658\n",
    "Epoch #245: Loss:2.4266, Accuracy:0.1815, Validation Loss:2.4684, Validation Accuracy:0.1527\n",
    "Epoch #246: Loss:2.4275, Accuracy:0.1852, Validation Loss:2.4644, Validation Accuracy:0.1560\n",
    "Epoch #247: Loss:2.4289, Accuracy:0.1832, Validation Loss:2.4665, Validation Accuracy:0.1511\n",
    "Epoch #248: Loss:2.4284, Accuracy:0.1807, Validation Loss:2.4643, Validation Accuracy:0.1626\n",
    "Epoch #249: Loss:2.4283, Accuracy:0.1906, Validation Loss:2.4653, Validation Accuracy:0.1576\n",
    "Epoch #250: Loss:2.4288, Accuracy:0.1860, Validation Loss:2.4644, Validation Accuracy:0.1658\n",
    "Epoch #251: Loss:2.4286, Accuracy:0.1852, Validation Loss:2.4654, Validation Accuracy:0.1609\n",
    "Epoch #252: Loss:2.4294, Accuracy:0.1893, Validation Loss:2.4658, Validation Accuracy:0.1593\n",
    "Epoch #253: Loss:2.4294, Accuracy:0.1823, Validation Loss:2.4648, Validation Accuracy:0.1593\n",
    "Epoch #254: Loss:2.4291, Accuracy:0.1852, Validation Loss:2.4656, Validation Accuracy:0.1576\n",
    "Epoch #255: Loss:2.4281, Accuracy:0.1844, Validation Loss:2.4639, Validation Accuracy:0.1642\n",
    "Epoch #256: Loss:2.4276, Accuracy:0.1828, Validation Loss:2.4663, Validation Accuracy:0.1511\n",
    "Epoch #257: Loss:2.4284, Accuracy:0.1782, Validation Loss:2.4674, Validation Accuracy:0.1527\n",
    "Epoch #258: Loss:2.4281, Accuracy:0.1828, Validation Loss:2.4651, Validation Accuracy:0.1626\n",
    "Epoch #259: Loss:2.4296, Accuracy:0.1852, Validation Loss:2.4699, Validation Accuracy:0.1544\n",
    "Epoch #260: Loss:2.4300, Accuracy:0.1856, Validation Loss:2.4667, Validation Accuracy:0.1626\n",
    "Epoch #261: Loss:2.4294, Accuracy:0.1828, Validation Loss:2.4690, Validation Accuracy:0.1609\n",
    "Epoch #262: Loss:2.4299, Accuracy:0.1856, Validation Loss:2.4693, Validation Accuracy:0.1511\n",
    "Epoch #263: Loss:2.4295, Accuracy:0.1844, Validation Loss:2.4687, Validation Accuracy:0.1609\n",
    "Epoch #264: Loss:2.4290, Accuracy:0.1881, Validation Loss:2.4693, Validation Accuracy:0.1609\n",
    "Epoch #265: Loss:2.4292, Accuracy:0.1864, Validation Loss:2.4693, Validation Accuracy:0.1576\n",
    "Epoch #266: Loss:2.4305, Accuracy:0.1852, Validation Loss:2.4699, Validation Accuracy:0.1560\n",
    "Epoch #267: Loss:2.4306, Accuracy:0.1819, Validation Loss:2.4713, Validation Accuracy:0.1544\n",
    "Epoch #268: Loss:2.4309, Accuracy:0.1869, Validation Loss:2.4719, Validation Accuracy:0.1511\n",
    "Epoch #269: Loss:2.4309, Accuracy:0.1840, Validation Loss:2.4728, Validation Accuracy:0.1544\n",
    "Epoch #270: Loss:2.4308, Accuracy:0.1836, Validation Loss:2.4712, Validation Accuracy:0.1544\n",
    "Epoch #271: Loss:2.4297, Accuracy:0.1864, Validation Loss:2.4714, Validation Accuracy:0.1461\n",
    "Epoch #272: Loss:2.4287, Accuracy:0.1897, Validation Loss:2.4691, Validation Accuracy:0.1478\n",
    "Epoch #273: Loss:2.4287, Accuracy:0.1873, Validation Loss:2.4688, Validation Accuracy:0.1593\n",
    "Epoch #274: Loss:2.4285, Accuracy:0.1877, Validation Loss:2.4699, Validation Accuracy:0.1593\n",
    "Epoch #275: Loss:2.4296, Accuracy:0.1869, Validation Loss:2.4723, Validation Accuracy:0.1609\n",
    "Epoch #276: Loss:2.4290, Accuracy:0.1897, Validation Loss:2.4705, Validation Accuracy:0.1593\n",
    "Epoch #277: Loss:2.4293, Accuracy:0.1881, Validation Loss:2.4715, Validation Accuracy:0.1609\n",
    "Epoch #278: Loss:2.4304, Accuracy:0.1852, Validation Loss:2.4721, Validation Accuracy:0.1626\n",
    "Epoch #279: Loss:2.4333, Accuracy:0.1897, Validation Loss:2.4752, Validation Accuracy:0.1429\n",
    "Epoch #280: Loss:2.4303, Accuracy:0.1951, Validation Loss:2.4746, Validation Accuracy:0.1511\n",
    "Epoch #281: Loss:2.4329, Accuracy:0.1836, Validation Loss:2.4778, Validation Accuracy:0.1560\n",
    "Epoch #282: Loss:2.4345, Accuracy:0.1873, Validation Loss:2.4788, Validation Accuracy:0.1544\n",
    "Epoch #283: Loss:2.4340, Accuracy:0.1864, Validation Loss:2.4757, Validation Accuracy:0.1544\n",
    "Epoch #284: Loss:2.4313, Accuracy:0.1856, Validation Loss:2.4808, Validation Accuracy:0.1412\n",
    "Epoch #285: Loss:2.4334, Accuracy:0.1864, Validation Loss:2.4748, Validation Accuracy:0.1544\n",
    "Epoch #286: Loss:2.4313, Accuracy:0.1844, Validation Loss:2.4781, Validation Accuracy:0.1560\n",
    "Epoch #287: Loss:2.4325, Accuracy:0.1881, Validation Loss:2.4758, Validation Accuracy:0.1544\n",
    "Epoch #288: Loss:2.4318, Accuracy:0.1852, Validation Loss:2.4744, Validation Accuracy:0.1527\n",
    "Epoch #289: Loss:2.4315, Accuracy:0.1877, Validation Loss:2.4789, Validation Accuracy:0.1429\n",
    "Epoch #290: Loss:2.4300, Accuracy:0.1897, Validation Loss:2.4745, Validation Accuracy:0.1560\n",
    "Epoch #291: Loss:2.4300, Accuracy:0.1860, Validation Loss:2.4749, Validation Accuracy:0.1544\n",
    "Epoch #292: Loss:2.4296, Accuracy:0.1897, Validation Loss:2.4760, Validation Accuracy:0.1511\n",
    "Epoch #293: Loss:2.4306, Accuracy:0.1856, Validation Loss:2.4767, Validation Accuracy:0.1544\n",
    "Epoch #294: Loss:2.4296, Accuracy:0.1885, Validation Loss:2.4762, Validation Accuracy:0.1511\n",
    "Epoch #295: Loss:2.4305, Accuracy:0.1877, Validation Loss:2.4783, Validation Accuracy:0.1511\n",
    "Epoch #296: Loss:2.4296, Accuracy:0.1914, Validation Loss:2.4756, Validation Accuracy:0.1544\n",
    "Epoch #297: Loss:2.4291, Accuracy:0.1926, Validation Loss:2.4750, Validation Accuracy:0.1527\n",
    "Epoch #298: Loss:2.4295, Accuracy:0.1943, Validation Loss:2.4764, Validation Accuracy:0.1560\n",
    "Epoch #299: Loss:2.4290, Accuracy:0.1918, Validation Loss:2.4767, Validation Accuracy:0.1560\n",
    "Epoch #300: Loss:2.4288, Accuracy:0.1864, Validation Loss:2.4745, Validation Accuracy:0.1560\n",
    "\n",
    "Test:\n",
    "Test Loss:2.47449279, Accuracy:0.1560\n",
    "Labels: ['mb', 'eg', 'sg', 'ek', 'eo', 'aa', 'ib', 'yd', 'eb', 'ce', 'ds', 'by', 'sk', 'ck', 'my']\n",
    "Confusion Matrix:\n",
    "      mb  eg  sg  ek  eo  aa  ib  yd  eb  ce  ds  by  sk  ck  my\n",
    "t:mb   0  13  23   0   0   0   3   5   1   0   0   7   0   0   0\n",
    "t:eg   0  22  11   0   0   0   0   0   9   0   1   7   0   0   0\n",
    "t:sg   0   5  29   0   0   0   7   4   0   0   0   6   0   0   0\n",
    "t:ek   0  24  14   0   0   0   1   1   0   0   1   7   0   0   0\n",
    "t:eo   0   2  20   0   0   0   1   0   2   0   0   9   0   0   0\n",
    "t:aa   0  19   8   0   0   0   2   0   2   0   1   2   0   0   0\n",
    "t:ib   0   4  14   0   0   0  13  19   0   0   0   4   0   0   0\n",
    "t:yd   0   1  29   0   0   0  10  19   0   0   0   3   0   0   0\n",
    "t:eb   0  11  16   0   0   0   3   6   2   0   0  12   0   0   0\n",
    "t:ce   0   7  14   0   0   0   2   2   0   0   0   2   0   0   0\n",
    "t:ds   0  20   5   0   0   0   0   0   1   0   3   2   0   0   0\n",
    "t:by   0  11  17   0   0   0   2   1   2   0   0   7   0   0   0\n",
    "t:sk   0  19   8   0   0   0   1   2   0   0   0   3   0   0   0\n",
    "t:ck   0  12   7   0   0   0   0   0   1   0   1   2   0   0   0\n",
    "t:my   0   9   5   0   0   0   2   1   0   0   1   2   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eg       0.12      0.44      0.19        50\n",
    "          sg       0.13      0.57      0.21        51\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ib       0.28      0.24      0.26        54\n",
    "          yd       0.32      0.31      0.31        62\n",
    "          eb       0.10      0.04      0.06        50\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ds       0.38      0.10      0.15        31\n",
    "          by       0.09      0.17      0.12        40\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          my       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.16       609\n",
    "   macro avg       0.09      0.12      0.09       609\n",
    "weighted avg       0.11      0.16      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 17:54:38 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.70123828377434, 2.6945470917988295, 2.6880252764534283, 2.682129564739409, 2.6756702455981025, 2.6693336098456424, 2.6626916362342774, 2.655476639423464, 2.6475263726339358, 2.6369576622504125, 2.6246313289272765, 2.6098814805348716, 2.5921400446805656, 2.572932058562982, 2.5558240687710114, 2.539656679031297, 2.5271897245510457, 2.5168830287476087, 2.5102978214646012, 2.505520587288492, 2.540377521358296, 2.5019074206673255, 2.5172606454106976, 2.4984112952534594, 2.49111167511525, 2.492094677657329, 2.4894798475337536, 2.4918579730298522, 2.482855444471237, 2.4807096803912585, 2.4798067477340573, 2.477376488824979, 2.472909816575951, 2.470900024295049, 2.470940590296277, 2.46697014071084, 2.5138882709841424, 2.46482361165565, 2.471955555804649, 2.4632542293842987, 2.4685120085385828, 2.4667341403773264, 2.4681539617735764, 2.4674616263222027, 2.4696748722558732, 2.4656038495707393, 2.4682360940378874, 2.464069153091982, 2.46206541758257, 2.4625076824808354, 2.4643545847612454, 2.4627110621416315, 2.4660404769853614, 2.46330787630504, 2.4634996337452155, 2.4612540490130095, 2.462663065232275, 2.460900579571528, 2.463857001663233, 2.4598547304400866, 2.468366601784241, 2.458769318701207, 2.46130337856086, 2.461536599301744, 2.4596239103276547, 2.4651412552800673, 2.457731267697314, 2.458603596648168, 2.4597434476874342, 2.4564831503506364, 2.4595874639958977, 2.458055605050574, 2.4588994498323338, 2.4598739515188686, 2.456368193054826, 2.4620007012277987, 2.4559927080652395, 2.4614204475640857, 2.4562229370248727, 2.4587084949505935, 2.4581321131419664, 2.4567913879901906, 2.457595968481355, 2.45571269777608, 2.4581534059959873, 2.458144124505555, 2.45786803461648, 2.4570017874925987, 2.45968398202229, 2.4555389634494125, 2.4629325839294784, 2.45539777188857, 2.4591294031816555, 2.457580732790316, 2.4575925789443143, 2.4596920487132956, 2.4547877256897674, 2.4595917336067736, 2.4577877819048752, 2.457543279932833, 2.456511037886045, 2.454796844711053, 2.4579394172956595, 2.4616673583859097, 2.4589652987732284, 2.462026405021279, 2.4592508236175687, 2.462164409446403, 2.459686918994672, 2.4601645277834487, 2.4612628834196695, 2.4596953842244518, 2.4626640102937696, 2.4614215073327124, 2.460056434515466, 2.462530025707677, 2.4616393925521174, 2.4612542905635237, 2.46210634493084, 2.462779967459943, 2.4591562677486776, 2.4643005643572127, 2.4618327809476304, 2.46011312254544, 2.4622214897512804, 2.459128153147956, 2.4624010537841245, 2.462136564192122, 2.4594754479788796, 2.4642871260055768, 2.4612310441648235, 2.4638430195293206, 2.4629921901402216, 2.4604383607216067, 2.4619520031564144, 2.459909714305734, 2.4652695068584873, 2.464145429420158, 2.4583707313819474, 2.461745312648454, 2.4630035923423828, 2.4620443506193865, 2.4609554248490357, 2.4598755151375955, 2.462778868150633, 2.463357164746239, 2.4636428966898047, 2.464480040695867, 2.4613093664297723, 2.464151715214421, 2.4637665729021596, 2.462050579255829, 2.464216707570995, 2.463535622031426, 2.465835301942622, 2.4683345057106956, 2.463313329787481, 2.464812765372015, 2.4630726957556064, 2.465449232186003, 2.4599116365310594, 2.4629532503964278, 2.467466125347344, 2.4616028045002856, 2.461286695915686, 2.4651135834567066, 2.4590388111684516, 2.4596584931578738, 2.462914964052648, 2.4580177607011717, 2.46102751612859, 2.4594166732969738, 2.459028252239885, 2.460160065167056, 2.458395302980796, 2.463278020739751, 2.4580005319247693, 2.4593927108595524, 2.4606107855078037, 2.4593093833704103, 2.4595540215816403, 2.4589362496812943, 2.45657061395191, 2.4604672179825005, 2.4593648491626108, 2.4578506488518173, 2.4579074214440455, 2.460445259983708, 2.457092767474295, 2.4600308145012564, 2.459103867729701, 2.4601787365911827, 2.4578429799166024, 2.4577393433926336, 2.459420832898621, 2.464439698628017, 2.4735245136987594, 2.4629536411053636, 2.463142479190294, 2.460075026466733, 2.4628643864285573, 2.4670281809539043, 2.4653967123704983, 2.469126874002917, 2.4654357217998535, 2.4634552385615205, 2.465593604227201, 2.4651536961102916, 2.4655711721316935, 2.466235159261669, 2.4650964584256627, 2.4691908453485647, 2.4693143078063313, 2.468712105931124, 2.4671164510285326, 2.4705625242004645, 2.473592958622574, 2.4731430267465524, 2.473524914977977, 2.4704288868677047, 2.4684840043385825, 2.470653308436201, 2.4716062659309026, 2.4692976811445013, 2.470876027014847, 2.4703250988363634, 2.4706145354679654, 2.4703790205844323, 2.470694398253618, 2.468400361698445, 2.46832193607963, 2.468336424804086, 2.469488546374592, 2.472060252488736, 2.469869339211625, 2.4692941745513766, 2.4711662513281913, 2.468572247008776, 2.464683453241984, 2.4669533566692583, 2.469441825728894, 2.4667659862875353, 2.4679139556947405, 2.4652660879595527, 2.468417157876276, 2.464444556259757, 2.4665307779421752, 2.4643393032656515, 2.4652944425448213, 2.464419063871913, 2.465357756184044, 2.4658093237133056, 2.4647593701805777, 2.465636111636859, 2.4639266061861136, 2.466272555744315, 2.4674070374719026, 2.465074379064375, 2.469903135143086, 2.466669017262451, 2.4689723537081765, 2.4693418523948183, 2.4686944915351807, 2.4693271687073857, 2.4693417185045816, 2.469881549453109, 2.471318488833548, 2.4719323285890527, 2.472751719611032, 2.4711530310375545, 2.4713533179121847, 2.469123714271633, 2.4687940792497156, 2.4698794992099256, 2.472308347573617, 2.4704556649150127, 2.4714593280516626, 2.47213086510331, 2.4752484886908572, 2.4745656950720427, 2.4778405884020827, 2.4788048991624554, 2.475695168834993, 2.4808010749628977, 2.4748034837406454, 2.4781226669430536, 2.4758110661029034, 2.474430167224803, 2.478934352621069, 2.474458880025178, 2.4748678610634136, 2.4759899587271055, 2.4767089070161967, 2.4762166756127266, 2.478254448604114, 2.475556233833576, 2.474998146833849, 2.476397936763043, 2.476681024961675, 2.4744927522975626], 'val_acc': [0.05747126415983601, 0.06075533650729848, 0.05747126406196303, 0.07881773388004068, 0.12479474487269453, 0.1362889975505118, 0.13957306989797427, 0.13957306980010128, 0.13957306980010128, 0.13793103357743355, 0.13793103357743355, 0.13793103357743355, 0.13957306980010128, 0.15106732277153748, 0.147783250424075, 0.14942528664674273, 0.14942528654886975, 0.15763546697709752, 0.15927750310189226, 0.17241379249174216, 0.12972085285558685, 0.1707717558775825, 0.1461412141035343, 0.17241379210025023, 0.17405582832291797, 0.1592775030040193, 0.16912972084162467, 0.1510673232731365, 0.1707717569664194, 0.1707717569664194, 0.1576354676744425, 0.16256157614669972, 0.16091954002190498, 0.16420361227149446, 0.16256157614669972, 0.16256157614669972, 0.1461412148008793, 0.1576354676744425, 0.15270935939793126, 0.1658456483962892, 0.14778325082780105, 0.14942528705046879, 0.14942528705046879, 0.1494252869525958, 0.15106732307739054, 0.1494252869525958, 0.14614121460513332, 0.14942528705046879, 0.14942528705046879, 0.1461412148008793, 0.14285714245341682, 0.14285714255128978, 0.13957307020382734, 0.14449917867608453, 0.14285714255128978, 0.13957307020382734, 0.13957307020382734, 0.14285714255128978, 0.13957307020382734, 0.14942528714834175, 0.14121510632862208, 0.14449917877395752, 0.14121510632862208, 0.14121510632862208, 0.1461412148008793, 0.14121510632862208, 0.14778325092567404, 0.14285714245341682, 0.14121510632862208, 0.154351395522726, 0.14121510632862208, 0.14285714245341682, 0.14449917848033858, 0.14449917857821157, 0.15106732317526353, 0.13957307020382734, 0.1576354676744425, 0.13957307020382734, 0.1576354676744425, 0.14449917857821157, 0.14449917857821157, 0.14449917857821157, 0.14449917857821157, 0.15270935920218529, 0.14121510632862208, 0.14121510632862208, 0.14285714245341682, 0.15270935930005827, 0.14285714245341682, 0.15435139532698003, 0.14121510632862208, 0.1576354676744425, 0.14285714245341682, 0.15106732307739054, 0.15270935920218529, 0.14121510632862208, 0.15106732337100948, 0.14285714245341682, 0.15106732307739054, 0.14449917848033858, 0.14285714235554384, 0.15599343154964776, 0.15106732307739054, 0.1494252869525958, 0.15106732307739054, 0.14614121460513332, 0.15106732307739054, 0.14614121460513332, 0.1494252869525958, 0.15435139532698003, 0.15435139532698003, 0.1494252869525958, 0.15435139532698003, 0.15106732307739054, 0.15270935920218529, 0.15435139532698003, 0.1494252869525958, 0.15270935920218529, 0.15270935920218529, 0.15270935920218529, 0.15106732337100948, 0.14449917848033858, 0.15106732307739054, 0.1494252869525958, 0.14449917848033858, 0.15106732307739054, 0.15435139532698003, 0.15106732307739054, 0.15106732317526353, 0.14449917848033858, 0.15270935920218529, 0.1412151062307491, 0.1461412147030063, 0.14614121489875226, 0.1461412148008793, 0.15270935939793126, 0.14285714235554384, 0.1461412147030063, 0.15270935939793126, 0.15106732317526353, 0.14614121489875226, 0.14614121489875226, 0.14778325082780105, 0.14942528705046879, 0.14942528714834175, 0.14942528724621473, 0.14942528724621473, 0.14614121489875226, 0.15106732337100948, 0.14778325092567404, 0.14942528705046879, 0.147783251023547, 0.1461412148008793, 0.1461412148008793, 0.1461412148008793, 0.14449917877395752, 0.14614121489875226, 0.14942528724621473, 0.1510673232731365, 0.15106732307739054, 0.15270935920218529, 0.15106732307739054, 0.14942528705046879, 0.15435139542485302, 0.14614121460513332, 0.1461412147030063, 0.15435139542485302, 0.15270935920218529, 0.15599343154964776, 0.1576354676744425, 0.15435139542485302, 0.1494252869525958, 0.1576354676744425, 0.15270935920218529, 0.15270935920218529, 0.1494252869525958, 0.15270935930005827, 0.15106732317526353, 0.15927750389711023, 0.15435139542485302, 0.1576354676744425, 0.14942528705046879, 0.15599343154964776, 0.1576354676744425, 0.1576354676744425, 0.15270935920218529, 0.1576354676744425, 0.15270935920218529, 0.15270935920218529, 0.15599343154964776, 0.15435139542485302, 0.14942528705046879, 0.15106732317526353, 0.15927750399498322, 0.15599343164752075, 0.1510673232731365, 0.1428571428510258, 0.1625615752536088, 0.1510673232731365, 0.1559934317453937, 0.15106732317526353, 0.15927750389711023, 0.15270935939793126, 0.14778325092567404, 0.154351395522726, 0.15599343154964776, 0.154351395522726, 0.1576354676744425, 0.14942528714834175, 0.154351395522726, 0.15270935939793126, 0.14614121499662525, 0.14614121499662525, 0.14942528724621473, 0.15106732337100948, 0.147783251023547, 0.14942528724621473, 0.14942528714834175, 0.1559934318432667, 0.15599343075442978, 0.15599343075442978, 0.14942528734408772, 0.15106732337100948, 0.1527093595936772, 0.15763546796806144, 0.15270935939793126, 0.15106732337100948, 0.14942528714834175, 0.14942528714834175, 0.15106732337100948, 0.16091954011977797, 0.16091954021765092, 0.16420361236936745, 0.14942528714834175, 0.154351395522726, 0.15270935939793126, 0.14778325092567404, 0.15927750399498322, 0.16256157614669972, 0.1510673232731365, 0.15435139542485302, 0.15927750389711023, 0.15106732337100948, 0.1658456484941622, 0.15270935939793126, 0.15599343154964776, 0.1510673232731365, 0.1625615762445727, 0.1576354676744425, 0.1658456484941622, 0.160919539924032, 0.15927750399498322, 0.15927750389711023, 0.15763546787018845, 0.16420361227149446, 0.1510673232731365, 0.15270935949580422, 0.1625615762445727, 0.154351395522726, 0.1625615762445727, 0.16091954021765092, 0.15106732337100948, 0.16091954002190498, 0.16091954002190498, 0.15763546796806144, 0.1559934318432667, 0.15435139571847195, 0.1510673232731365, 0.15435139571847195, 0.15435139571847195, 0.14614121489875226, 0.147783251023547, 0.15927750409285618, 0.15927750399498322, 0.16091954011977797, 0.15927750399498322, 0.16091954021765092, 0.16256157634244567, 0.14285714264916277, 0.15106732337100948, 0.1559934318432667, 0.15435139562059896, 0.15435139562059896, 0.14121510652436803, 0.15435139562059896, 0.1559934318432667, 0.15435139562059896, 0.15270935939793126, 0.14285714264916277, 0.1559934317453937, 0.15435139562059896, 0.15106732337100948, 0.15435139562059896, 0.1510673232731365, 0.15106732337100948, 0.15435139562059896, 0.15270935949580422, 0.1559934317453937, 0.15599343164752075, 0.15599343164752075], 'loss': [2.7077186978083616, 2.700610539849534, 2.6934980361123837, 2.686918730608492, 2.680999846820714, 2.674481892830537, 2.668398077140354, 2.6624224140903543, 2.6550160132884, 2.6463661975194785, 2.6367946256602326, 2.624704951672094, 2.6108089893505557, 2.595554639377633, 2.580686466111295, 2.5647527817093616, 2.552198529879905, 2.540006059493862, 2.5289919958956677, 2.5210417149248063, 2.516489829370863, 2.518771731829007, 2.51057795538305, 2.5129188428914033, 2.505374512878042, 2.504491288657061, 2.4972390772136084, 2.4951364066811315, 2.4912502984001894, 2.485114782889521, 2.483124352774336, 2.4797473460007495, 2.4789900532003792, 2.4745577351023775, 2.472232228676641, 2.471619491068, 2.4754803013263054, 2.482281323282136, 2.4658695016308734, 2.462310086726163, 2.4630094768085518, 2.4595953688729226, 2.458499546070608, 2.45721345319885, 2.4557576174608737, 2.454827829650785, 2.453486130712458, 2.4531726288844427, 2.453163855276558, 2.4528094058653656, 2.4529314162550033, 2.452372383141175, 2.4510929030314608, 2.4502136228510487, 2.4494470186057278, 2.4493486095013317, 2.449125621059347, 2.4498735068515094, 2.4496619980438044, 2.449408870849766, 2.4475690642911063, 2.447215763107707, 2.446550756509299, 2.446111314546401, 2.446647135481942, 2.44458852431123, 2.4438349166689957, 2.4442730221170663, 2.4428596875506017, 2.442864005913235, 2.4426451366784883, 2.4419382769958684, 2.4418338259632337, 2.442133475818673, 2.443522248277919, 2.442169048751894, 2.4425303343629934, 2.441669976686795, 2.440540730439172, 2.4409684555241706, 2.4397603059451436, 2.4395258058512725, 2.440434243008341, 2.438841250495989, 2.438682250565327, 2.438627367097984, 2.438766462846948, 2.4386340609321357, 2.4393894242554963, 2.4392115834802084, 2.4401523019254086, 2.4381398202947033, 2.4380849599348693, 2.4380962389450542, 2.4389165522871075, 2.4375736843633944, 2.4381002416356146, 2.436696237117603, 2.437532667261864, 2.4364359591286284, 2.436297058031055, 2.4355601507535463, 2.4354917406791046, 2.4365338392081446, 2.435131901539327, 2.435862986214107, 2.4351671126833687, 2.434543620585416, 2.4346177315565107, 2.434221714967575, 2.4346224190518106, 2.4340208280747433, 2.4343593561184234, 2.4343235054055277, 2.43435841301873, 2.4337840933084975, 2.433836727024838, 2.4332880372629027, 2.4338422697917146, 2.4336039020295503, 2.4332436426464294, 2.4328206966055492, 2.4332712567072874, 2.4336506816151204, 2.4339047291929963, 2.4324914397645045, 2.4326233142210474, 2.433432564255638, 2.4324025574161285, 2.433033499433764, 2.4320265020922713, 2.4325355810795966, 2.4320229021185966, 2.432454344966818, 2.434233446042885, 2.431493320308427, 2.4326918708715106, 2.432922037719946, 2.4318536754506326, 2.433213292989398, 2.431555517694054, 2.431619379897382, 2.431364468627397, 2.4317229545091945, 2.43068061677827, 2.4315457392032633, 2.4315459299381263, 2.4324201135419967, 2.4317817932771217, 2.431362438201904, 2.4316291446803286, 2.433459026857568, 2.431614622296249, 2.4308186559461715, 2.4330012082564023, 2.4319458836157954, 2.4309516001041422, 2.43192128430157, 2.4298029707687347, 2.42996901653141, 2.4308128168940297, 2.431455981021544, 2.434405718887611, 2.432553167656462, 2.437708453525018, 2.435580606480154, 2.432802036998208, 2.4338557517504054, 2.433757059774849, 2.4325520588876777, 2.4327001196647817, 2.43194268616318, 2.4314315236814212, 2.431786519154386, 2.433592008320458, 2.4339432239532472, 2.4328785259865637, 2.4322050381490095, 2.433077203566534, 2.4300701013091164, 2.4321476364527395, 2.4313649806153848, 2.4316219726382338, 2.4302340602483103, 2.430568209614842, 2.4300032721407847, 2.4305610422725796, 2.429598127400361, 2.430805950047299, 2.430322919489177, 2.429893524788733, 2.4296685499821846, 2.4291323533537943, 2.4288234628446292, 2.4295057199818886, 2.432256512083802, 2.4362481478548146, 2.440031840228447, 2.4323628735003773, 2.43139007840558, 2.4318494198503435, 2.4321278445774523, 2.4292719215338234, 2.430816112945212, 2.4296707796609867, 2.428667342638333, 2.429480579795289, 2.428051646876874, 2.428386702331919, 2.427234740619542, 2.4270619002700586, 2.4279787536519266, 2.429399120734213, 2.4291668070415207, 2.4282435153788855, 2.427634856930993, 2.4288744696356677, 2.4306323867069377, 2.4300238682748847, 2.4300142403745553, 2.4303594284723427, 2.4307910645032567, 2.429565358308796, 2.4296724035509802, 2.4283366945489964, 2.4287367893195495, 2.4284427681987535, 2.4286245920819667, 2.4295185740233936, 2.429088120391971, 2.4292782869671896, 2.429330826980622, 2.4294822377590672, 2.429842310813418, 2.4288651279355467, 2.4298086653989444, 2.427394215135359, 2.428610617815836, 2.4267985837415504, 2.42833074798819, 2.4273914676916917, 2.4266858423025455, 2.427250339265232, 2.4266794149880537, 2.4265758941305737, 2.4275124769191234, 2.4288939739399624, 2.428428460146612, 2.4283365690243075, 2.4288492037285523, 2.4285516870095254, 2.42944407717648, 2.4294276856299053, 2.429050535049282, 2.4280941565668313, 2.427610250567019, 2.4283613122708987, 2.428129399630568, 2.4295985728073903, 2.4299664933089113, 2.429419458475446, 2.4298557633981566, 2.4294628538879772, 2.4289978524742675, 2.4291680356560303, 2.430526114195524, 2.4306180135425355, 2.430857409686768, 2.4309025267066406, 2.4308191828169616, 2.429659570169155, 2.4287195981160816, 2.4286899502027697, 2.4284916003381936, 2.429595538576036, 2.4289830605352196, 2.4292899244375055, 2.4303993111518376, 2.4332503151355094, 2.4302788545463607, 2.4328742476704184, 2.4344592837582377, 2.433992958852153, 2.4312825482973572, 2.4334085764091853, 2.431252834488479, 2.4325033257384563, 2.4317673550004586, 2.4314946488922873, 2.4300087320731163, 2.4300366277322634, 2.4296446850167652, 2.4305831356949383, 2.4296167828953488, 2.4305305285130685, 2.429631202960161, 2.4290979551828373, 2.4295250522527363, 2.4289886747787133, 2.4287709829498856], 'acc': [0.0418891172159991, 0.057494866483877326, 0.05954825459258512, 0.06201232047847164, 0.09322381883676047, 0.13305954718369478, 0.14291581119232843, 0.14455852246627182, 0.14455852148714007, 0.14332648800873413, 0.14373716523515126, 0.14332648922958902, 0.14373716684765883, 0.14332648922040966, 0.13757700176576815, 0.13388090341732486, 0.1383983571977341, 0.1371663249585418, 0.14209445558289482, 0.14455852068547595, 0.15441478549577373, 0.15687885059835485, 0.15728952704146656, 0.15318275125242112, 0.15318275105659476, 0.1568788492092117, 0.16180698125024595, 0.16591375682510634, 0.16632443599142824, 0.1716632432638987, 0.16919917775130613, 0.17248459947917005, 0.1733059559086265, 0.1749486646001099, 0.17412731034310203, 0.17371663194172682, 0.1741273101656344, 0.17207392225275295, 0.17659137606987962, 0.17494866422681593, 0.17412730936397028, 0.17864476277108554, 0.17946611900471565, 0.17864476435605506, 0.17987679558857755, 0.17946611800722517, 0.17864476453352268, 0.17905544236577756, 0.17741273011270245, 0.17946611879053057, 0.17864476414187, 0.17782340970739446, 0.17535934317895274, 0.17741273052271386, 0.17700205386541706, 0.17535934358896416, 0.17741273169767197, 0.17782340872826272, 0.1786447629669119, 0.18069815303388317, 0.1765913756598682, 0.17577002083374, 0.17535934301984385, 0.1741273089906763, 0.1786447645518814, 0.17494866559760036, 0.17494866481429497, 0.18110882965446254, 0.17494866638090575, 0.17659137605152092, 0.1774127315018456, 0.17659137468073646, 0.17864476474770774, 0.17700205425706977, 0.17700205249463263, 0.17618069902093014, 0.18069815223221905, 0.1819301843031231, 0.1757700198178908, 0.17782340833661003, 0.1761806992351152, 0.18275153993091545, 0.17659137567822694, 0.17864476277108554, 0.18028747361665878, 0.17453798756951913, 0.1770020531004704, 0.17782340833661003, 0.177823408709904, 0.17782340773077226, 0.1761806984518098, 0.17577002022790222, 0.17700205366959074, 0.17659137606987962, 0.1794661196105534, 0.1778234075165872, 0.1737166331166849, 0.17741273167931323, 0.17700205445289613, 0.1811088284611457, 0.18069815225057778, 0.1790554413682871, 0.17905544195576614, 0.17618069903928885, 0.1802874748099756, 0.1778234094932094, 0.17535934221817973, 0.17535934399897557, 0.176180697258493, 0.17864476355439093, 0.17371663292085854, 0.17618069784597204, 0.18028747463250797, 0.1761806976501457, 0.18069815184056637, 0.1761806988251038, 0.17618069903928885, 0.17905544099499313, 0.17823408554466844, 0.1741273097739817, 0.1765913748582041, 0.18193018567390756, 0.17659137624734725, 0.1819301850864285, 0.17659137624734725, 0.1761806976501457, 0.18069815301552444, 0.1774127311101929, 0.1806981524280454, 0.17741273210768338, 0.17577001965878192, 0.1806981514672724, 0.18069815283805682, 0.1819301854964399, 0.1815195066850533, 0.18110882985028887, 0.1819301850680698, 0.17659137587405327, 0.1798767976019172, 0.18069815164474, 0.1835728941512059, 0.18110882847950444, 0.17864476275272684, 0.17946611879053057, 0.18398357317677758, 0.18110883047448537, 0.18234086309615102, 0.18521560485602895, 0.1835728955770665, 0.18028747463250797, 0.18069815184056637, 0.18644763751441204, 0.17700205249463263, 0.1806981514672724, 0.18110883026030028, 0.17782340929738305, 0.18028747539745463, 0.17823408732546428, 0.1823408617253666, 0.18110883004611522, 0.1823408617253666, 0.18110883024194158, 0.17864476394604364, 0.1823408617253666, 0.18028747539745463, 0.18193018528225485, 0.18316221754898526, 0.17946611955547725, 0.17659137624734725, 0.18028747539745463, 0.1819301838931117, 0.1819301858697339, 0.17864476414187, 0.17946611957383596, 0.17741273052271386, 0.1794661184172366, 0.18357289614618683, 0.1806981526422305, 0.1811088286753308, 0.18439425138232643, 0.18110882926280983, 0.18316221833229065, 0.18110882865697206, 0.1819301854780812, 0.17987679582112134, 0.18151950627504188, 0.18110882847950444, 0.18028747541581336, 0.18234086131535515, 0.17905544078080807, 0.18028747343919116, 0.1819301850864285, 0.18316221696150622, 0.18316221715733255, 0.18398357239347218, 0.1831622185464757, 0.18028747463250797, 0.1885010267797192, 0.18644763966850186, 0.18234086152954024, 0.18398357198346077, 0.18234086211701928, 0.18028747482833432, 0.18028747342083243, 0.18028747363501751, 0.181108830651953, 0.1765913754824006, 0.17987679640860038, 0.18193018567390756, 0.1815195078600114, 0.18110882983193016, 0.18275153994927415, 0.18275154034092686, 0.18521560681429242, 0.18316221754898526, 0.181108830651953, 0.18316221715733255, 0.1798767968002531, 0.18685831669909264, 0.18357289477540237, 0.18316221774481162, 0.18234086250867199, 0.18234086331033608, 0.18316221655149478, 0.18521560464184386, 0.1848049272196004, 0.18110882945863618, 0.18275153914761005, 0.17987679740609086, 0.1839835739784417, 0.18644763769187966, 0.1815195062934006, 0.18398357178763441, 0.18439425022572709, 0.1852156046602026, 0.18357289497122872, 0.18316221874230207, 0.18480492782543817, 0.18398357298095122, 0.18357289616454553, 0.1868583161116136, 0.1843942511865001, 0.18069815107561968, 0.18644763751441204, 0.1815195078783701, 0.18521560622681338, 0.18316221774481162, 0.1806981526422305, 0.1905544142642305, 0.18603695991470093, 0.18521560644099844, 0.18932238084090075, 0.18234086309615102, 0.18521560681429242, 0.184394250011542, 0.18275154051839448, 0.1782340867196265, 0.18275154051839448, 0.18521560622681338, 0.18562628386324193, 0.1827515409284059, 0.18562628345323048, 0.184394250011542, 0.18809034898418175, 0.1864476394726755, 0.1852156066368248, 0.18193018448059073, 0.18685831513248186, 0.18398357378261535, 0.18357289614618683, 0.18644763908102283, 0.18973306022140768, 0.18726899314220435, 0.1876796705460891, 0.18685831609325487, 0.18973306064977782, 0.18809034896582305, 0.18521560683265115, 0.1897330594381023, 0.1950718688646626, 0.18357289596871917, 0.18726899275055167, 0.18644763929520788, 0.18562628404070955, 0.1864476374960533, 0.18439425038483598, 0.18809034894746432, 0.18521560622681338, 0.18767967035026276, 0.1897330590464496, 0.18603696087547397, 0.1897330594381023, 0.18562628445072096, 0.18850102697554555, 0.18767967056444784, 0.19137577030203426, 0.19260780372536404, 0.19425051321851156, 0.19178644672678727, 0.18644763810189108]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
