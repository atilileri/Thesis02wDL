{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf24.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 05:21:16 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '3', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '03', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000025FB5CC4E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000025FAF4F6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0924, Accuracy:0.3729, Validation Loss:1.0818, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0777, Accuracy:0.3749, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0758, Accuracy:0.3943, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0756, Accuracy:0.3943, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0729, Accuracy:0.3943, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0724, Accuracy:0.3943, Validation Loss:1.0721, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0721, Accuracy:0.3943, Validation Loss:1.0715, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0716, Accuracy:0.3943, Validation Loss:1.0705, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0705, Accuracy:0.3943, Validation Loss:1.0693, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0692, Accuracy:0.3943, Validation Loss:1.0678, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0677, Accuracy:0.3943, Validation Loss:1.0655, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0654, Accuracy:0.3943, Validation Loss:1.0622, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0626, Accuracy:0.3963, Validation Loss:1.0572, Validation Accuracy:0.3957\n",
    "Epoch #31: Loss:1.0572, Accuracy:0.4012, Validation Loss:1.0501, Validation Accuracy:0.4122\n",
    "Epoch #32: Loss:1.0504, Accuracy:0.4066, Validation Loss:1.0403, Validation Accuracy:0.4122\n",
    "Epoch #33: Loss:1.0418, Accuracy:0.4193, Validation Loss:1.0301, Validation Accuracy:0.4105\n",
    "Epoch #34: Loss:1.0338, Accuracy:0.4242, Validation Loss:1.0250, Validation Accuracy:0.4089\n",
    "Epoch #35: Loss:1.0313, Accuracy:0.4275, Validation Loss:1.0251, Validation Accuracy:0.4122\n",
    "Epoch #36: Loss:1.0301, Accuracy:0.4279, Validation Loss:1.0235, Validation Accuracy:0.4220\n",
    "Epoch #37: Loss:1.0284, Accuracy:0.4271, Validation Loss:1.0273, Validation Accuracy:0.4220\n",
    "Epoch #38: Loss:1.0298, Accuracy:0.4234, Validation Loss:1.0261, Validation Accuracy:0.4220\n",
    "Epoch #39: Loss:1.0260, Accuracy:0.4300, Validation Loss:1.0250, Validation Accuracy:0.4171\n",
    "Epoch #40: Loss:1.0262, Accuracy:0.4259, Validation Loss:1.0213, Validation Accuracy:0.4269\n",
    "Epoch #41: Loss:1.0255, Accuracy:0.4255, Validation Loss:1.0196, Validation Accuracy:0.4204\n",
    "Epoch #42: Loss:1.0243, Accuracy:0.4296, Validation Loss:1.0166, Validation Accuracy:0.4302\n",
    "Epoch #43: Loss:1.0236, Accuracy:0.4337, Validation Loss:1.0160, Validation Accuracy:0.4220\n",
    "Epoch #44: Loss:1.0232, Accuracy:0.4287, Validation Loss:1.0153, Validation Accuracy:0.4335\n",
    "Epoch #45: Loss:1.0232, Accuracy:0.4361, Validation Loss:1.0187, Validation Accuracy:0.4236\n",
    "Epoch #46: Loss:1.0268, Accuracy:0.4304, Validation Loss:1.0225, Validation Accuracy:0.4269\n",
    "Epoch #47: Loss:1.0292, Accuracy:0.4361, Validation Loss:1.0241, Validation Accuracy:0.4253\n",
    "Epoch #48: Loss:1.0271, Accuracy:0.4271, Validation Loss:1.0151, Validation Accuracy:0.4319\n",
    "Epoch #49: Loss:1.0221, Accuracy:0.4296, Validation Loss:1.0140, Validation Accuracy:0.4253\n",
    "Epoch #50: Loss:1.0204, Accuracy:0.4312, Validation Loss:1.0130, Validation Accuracy:0.4335\n",
    "Epoch #51: Loss:1.0198, Accuracy:0.4345, Validation Loss:1.0120, Validation Accuracy:0.4302\n",
    "Epoch #52: Loss:1.0195, Accuracy:0.4345, Validation Loss:1.0123, Validation Accuracy:0.4302\n",
    "Epoch #53: Loss:1.0204, Accuracy:0.4316, Validation Loss:1.0108, Validation Accuracy:0.4302\n",
    "Epoch #54: Loss:1.0213, Accuracy:0.4292, Validation Loss:1.0108, Validation Accuracy:0.4286\n",
    "Epoch #55: Loss:1.0222, Accuracy:0.4345, Validation Loss:1.0124, Validation Accuracy:0.4286\n",
    "Epoch #56: Loss:1.0193, Accuracy:0.4370, Validation Loss:1.0105, Validation Accuracy:0.4368\n",
    "Epoch #57: Loss:1.0181, Accuracy:0.4386, Validation Loss:1.0098, Validation Accuracy:0.4302\n",
    "Epoch #58: Loss:1.0177, Accuracy:0.4345, Validation Loss:1.0101, Validation Accuracy:0.4335\n",
    "Epoch #59: Loss:1.0181, Accuracy:0.4390, Validation Loss:1.0085, Validation Accuracy:0.4286\n",
    "Epoch #60: Loss:1.0162, Accuracy:0.4394, Validation Loss:1.0089, Validation Accuracy:0.4384\n",
    "Epoch #61: Loss:1.0166, Accuracy:0.4435, Validation Loss:1.0074, Validation Accuracy:0.4319\n",
    "Epoch #62: Loss:1.0158, Accuracy:0.4374, Validation Loss:1.0105, Validation Accuracy:0.4302\n",
    "Epoch #63: Loss:1.0168, Accuracy:0.4312, Validation Loss:1.0075, Validation Accuracy:0.4335\n",
    "Epoch #64: Loss:1.0164, Accuracy:0.4353, Validation Loss:1.0062, Validation Accuracy:0.4319\n",
    "Epoch #65: Loss:1.0150, Accuracy:0.4283, Validation Loss:1.0060, Validation Accuracy:0.4220\n",
    "Epoch #66: Loss:1.0146, Accuracy:0.4439, Validation Loss:1.0059, Validation Accuracy:0.4401\n",
    "Epoch #67: Loss:1.0145, Accuracy:0.4439, Validation Loss:1.0053, Validation Accuracy:0.4368\n",
    "Epoch #68: Loss:1.0131, Accuracy:0.4353, Validation Loss:1.0045, Validation Accuracy:0.4335\n",
    "Epoch #69: Loss:1.0129, Accuracy:0.4337, Validation Loss:1.0034, Validation Accuracy:0.4466\n",
    "Epoch #70: Loss:1.0142, Accuracy:0.4390, Validation Loss:1.0036, Validation Accuracy:0.4417\n",
    "Epoch #71: Loss:1.0138, Accuracy:0.4390, Validation Loss:1.0030, Validation Accuracy:0.4417\n",
    "Epoch #72: Loss:1.0115, Accuracy:0.4444, Validation Loss:1.0032, Validation Accuracy:0.4417\n",
    "Epoch #73: Loss:1.0121, Accuracy:0.4407, Validation Loss:1.0031, Validation Accuracy:0.4401\n",
    "Epoch #74: Loss:1.0109, Accuracy:0.4333, Validation Loss:1.0021, Validation Accuracy:0.4319\n",
    "Epoch #75: Loss:1.0108, Accuracy:0.4390, Validation Loss:1.0046, Validation Accuracy:0.4417\n",
    "Epoch #76: Loss:1.0107, Accuracy:0.4411, Validation Loss:1.0036, Validation Accuracy:0.4433\n",
    "Epoch #77: Loss:1.0114, Accuracy:0.4390, Validation Loss:1.0085, Validation Accuracy:0.4351\n",
    "Epoch #78: Loss:1.0145, Accuracy:0.4374, Validation Loss:1.0026, Validation Accuracy:0.4450\n",
    "Epoch #79: Loss:1.0090, Accuracy:0.4394, Validation Loss:1.0046, Validation Accuracy:0.4401\n",
    "Epoch #80: Loss:1.0098, Accuracy:0.4439, Validation Loss:1.0041, Validation Accuracy:0.4548\n",
    "Epoch #81: Loss:1.0123, Accuracy:0.4460, Validation Loss:1.0052, Validation Accuracy:0.4450\n",
    "Epoch #82: Loss:1.0110, Accuracy:0.4476, Validation Loss:0.9999, Validation Accuracy:0.4548\n",
    "Epoch #83: Loss:1.0078, Accuracy:0.4448, Validation Loss:0.9981, Validation Accuracy:0.4483\n",
    "Epoch #84: Loss:1.0065, Accuracy:0.4505, Validation Loss:0.9967, Validation Accuracy:0.4565\n",
    "Epoch #85: Loss:1.0055, Accuracy:0.4444, Validation Loss:0.9960, Validation Accuracy:0.4466\n",
    "Epoch #86: Loss:1.0048, Accuracy:0.4415, Validation Loss:0.9950, Validation Accuracy:0.4499\n",
    "Epoch #87: Loss:1.0041, Accuracy:0.4534, Validation Loss:0.9949, Validation Accuracy:0.4614\n",
    "Epoch #88: Loss:1.0032, Accuracy:0.4530, Validation Loss:0.9937, Validation Accuracy:0.4565\n",
    "Epoch #89: Loss:1.0020, Accuracy:0.4542, Validation Loss:0.9925, Validation Accuracy:0.4516\n",
    "Epoch #90: Loss:1.0025, Accuracy:0.4513, Validation Loss:0.9924, Validation Accuracy:0.4631\n",
    "Epoch #91: Loss:1.0017, Accuracy:0.4517, Validation Loss:0.9977, Validation Accuracy:0.4466\n",
    "Epoch #92: Loss:1.0033, Accuracy:0.4480, Validation Loss:0.9929, Validation Accuracy:0.4565\n",
    "Epoch #93: Loss:1.0005, Accuracy:0.4563, Validation Loss:0.9898, Validation Accuracy:0.4466\n",
    "Epoch #94: Loss:0.9986, Accuracy:0.4587, Validation Loss:0.9925, Validation Accuracy:0.4483\n",
    "Epoch #95: Loss:0.9999, Accuracy:0.4493, Validation Loss:0.9878, Validation Accuracy:0.4663\n",
    "Epoch #96: Loss:1.0006, Accuracy:0.4460, Validation Loss:0.9934, Validation Accuracy:0.4614\n",
    "Epoch #97: Loss:0.9955, Accuracy:0.4509, Validation Loss:0.9882, Validation Accuracy:0.4581\n",
    "Epoch #98: Loss:0.9942, Accuracy:0.4542, Validation Loss:0.9874, Validation Accuracy:0.4631\n",
    "Epoch #99: Loss:0.9943, Accuracy:0.4534, Validation Loss:0.9836, Validation Accuracy:0.4647\n",
    "Epoch #100: Loss:0.9968, Accuracy:0.4485, Validation Loss:0.9865, Validation Accuracy:0.4614\n",
    "Epoch #101: Loss:0.9895, Accuracy:0.4583, Validation Loss:0.9830, Validation Accuracy:0.4663\n",
    "Epoch #102: Loss:0.9870, Accuracy:0.4501, Validation Loss:0.9862, Validation Accuracy:0.4696\n",
    "Epoch #103: Loss:0.9881, Accuracy:0.4485, Validation Loss:0.9795, Validation Accuracy:0.4828\n",
    "Epoch #104: Loss:0.9852, Accuracy:0.4632, Validation Loss:0.9830, Validation Accuracy:0.4745\n",
    "Epoch #105: Loss:0.9852, Accuracy:0.4624, Validation Loss:0.9766, Validation Accuracy:0.4729\n",
    "Epoch #106: Loss:0.9820, Accuracy:0.4653, Validation Loss:0.9777, Validation Accuracy:0.4729\n",
    "Epoch #107: Loss:0.9871, Accuracy:0.4604, Validation Loss:0.9739, Validation Accuracy:0.4844\n",
    "Epoch #108: Loss:0.9831, Accuracy:0.4600, Validation Loss:0.9802, Validation Accuracy:0.4795\n",
    "Epoch #109: Loss:0.9865, Accuracy:0.4624, Validation Loss:1.0021, Validation Accuracy:0.4713\n",
    "Epoch #110: Loss:0.9919, Accuracy:0.4665, Validation Loss:0.9953, Validation Accuracy:0.4844\n",
    "Epoch #111: Loss:0.9959, Accuracy:0.4489, Validation Loss:0.9793, Validation Accuracy:0.4745\n",
    "Epoch #112: Loss:0.9798, Accuracy:0.4694, Validation Loss:0.9769, Validation Accuracy:0.4729\n",
    "Epoch #113: Loss:0.9780, Accuracy:0.4575, Validation Loss:0.9759, Validation Accuracy:0.4811\n",
    "Epoch #114: Loss:0.9719, Accuracy:0.4793, Validation Loss:0.9689, Validation Accuracy:0.4893\n",
    "Epoch #115: Loss:0.9697, Accuracy:0.4793, Validation Loss:0.9689, Validation Accuracy:0.4811\n",
    "Epoch #116: Loss:0.9684, Accuracy:0.4739, Validation Loss:0.9878, Validation Accuracy:0.4729\n",
    "Epoch #117: Loss:0.9758, Accuracy:0.4789, Validation Loss:0.9796, Validation Accuracy:0.4762\n",
    "Epoch #118: Loss:0.9827, Accuracy:0.4752, Validation Loss:0.9662, Validation Accuracy:0.4828\n",
    "Epoch #119: Loss:0.9758, Accuracy:0.4784, Validation Loss:0.9791, Validation Accuracy:0.4893\n",
    "Epoch #120: Loss:0.9740, Accuracy:0.4846, Validation Loss:0.9838, Validation Accuracy:0.4860\n",
    "Epoch #121: Loss:0.9803, Accuracy:0.4797, Validation Loss:0.9849, Validation Accuracy:0.4663\n",
    "Epoch #122: Loss:0.9838, Accuracy:0.4661, Validation Loss:0.9737, Validation Accuracy:0.4828\n",
    "Epoch #123: Loss:0.9799, Accuracy:0.4825, Validation Loss:0.9773, Validation Accuracy:0.4844\n",
    "Epoch #124: Loss:0.9732, Accuracy:0.4817, Validation Loss:0.9627, Validation Accuracy:0.4959\n",
    "Epoch #125: Loss:0.9661, Accuracy:0.4871, Validation Loss:0.9689, Validation Accuracy:0.4844\n",
    "Epoch #126: Loss:0.9625, Accuracy:0.4912, Validation Loss:0.9601, Validation Accuracy:0.4877\n",
    "Epoch #127: Loss:0.9590, Accuracy:0.4895, Validation Loss:0.9583, Validation Accuracy:0.4893\n",
    "Epoch #128: Loss:0.9583, Accuracy:0.4895, Validation Loss:0.9587, Validation Accuracy:0.5090\n",
    "Epoch #129: Loss:0.9552, Accuracy:0.4928, Validation Loss:0.9576, Validation Accuracy:0.5107\n",
    "Epoch #130: Loss:0.9556, Accuracy:0.4977, Validation Loss:0.9563, Validation Accuracy:0.5074\n",
    "Epoch #131: Loss:0.9541, Accuracy:0.5002, Validation Loss:0.9559, Validation Accuracy:0.5074\n",
    "Epoch #132: Loss:0.9541, Accuracy:0.4961, Validation Loss:0.9635, Validation Accuracy:0.4975\n",
    "Epoch #133: Loss:0.9526, Accuracy:0.5047, Validation Loss:0.9572, Validation Accuracy:0.4959\n",
    "Epoch #134: Loss:0.9532, Accuracy:0.4961, Validation Loss:0.9560, Validation Accuracy:0.4992\n",
    "Epoch #135: Loss:0.9612, Accuracy:0.4940, Validation Loss:0.9542, Validation Accuracy:0.5008\n",
    "Epoch #136: Loss:0.9566, Accuracy:0.4986, Validation Loss:0.9546, Validation Accuracy:0.5008\n",
    "Epoch #137: Loss:0.9459, Accuracy:0.4986, Validation Loss:0.9522, Validation Accuracy:0.4943\n",
    "Epoch #138: Loss:0.9469, Accuracy:0.5105, Validation Loss:0.9526, Validation Accuracy:0.5025\n",
    "Epoch #139: Loss:0.9501, Accuracy:0.5055, Validation Loss:0.9523, Validation Accuracy:0.5057\n",
    "Epoch #140: Loss:0.9458, Accuracy:0.5039, Validation Loss:0.9520, Validation Accuracy:0.5107\n",
    "Epoch #141: Loss:0.9445, Accuracy:0.5142, Validation Loss:0.9528, Validation Accuracy:0.5074\n",
    "Epoch #142: Loss:0.9436, Accuracy:0.5092, Validation Loss:0.9508, Validation Accuracy:0.5057\n",
    "Epoch #143: Loss:0.9428, Accuracy:0.5092, Validation Loss:0.9497, Validation Accuracy:0.5008\n",
    "Epoch #144: Loss:0.9431, Accuracy:0.5060, Validation Loss:0.9573, Validation Accuracy:0.5140\n",
    "Epoch #145: Loss:0.9454, Accuracy:0.5138, Validation Loss:0.9625, Validation Accuracy:0.5074\n",
    "Epoch #146: Loss:0.9550, Accuracy:0.4994, Validation Loss:0.9635, Validation Accuracy:0.5057\n",
    "Epoch #147: Loss:0.9561, Accuracy:0.4924, Validation Loss:0.9694, Validation Accuracy:0.4860\n",
    "Epoch #148: Loss:0.9532, Accuracy:0.5027, Validation Loss:0.9577, Validation Accuracy:0.5057\n",
    "Epoch #149: Loss:0.9459, Accuracy:0.5035, Validation Loss:0.9471, Validation Accuracy:0.5057\n",
    "Epoch #150: Loss:0.9427, Accuracy:0.5047, Validation Loss:0.9494, Validation Accuracy:0.5057\n",
    "Epoch #151: Loss:0.9433, Accuracy:0.5175, Validation Loss:0.9526, Validation Accuracy:0.5107\n",
    "Epoch #152: Loss:0.9416, Accuracy:0.5039, Validation Loss:0.9461, Validation Accuracy:0.5041\n",
    "Epoch #153: Loss:0.9388, Accuracy:0.5170, Validation Loss:0.9520, Validation Accuracy:0.5090\n",
    "Epoch #154: Loss:0.9391, Accuracy:0.5109, Validation Loss:0.9478, Validation Accuracy:0.5107\n",
    "Epoch #155: Loss:0.9387, Accuracy:0.5142, Validation Loss:0.9456, Validation Accuracy:0.5008\n",
    "Epoch #156: Loss:0.9378, Accuracy:0.5150, Validation Loss:0.9475, Validation Accuracy:0.5057\n",
    "Epoch #157: Loss:0.9416, Accuracy:0.5133, Validation Loss:0.9467, Validation Accuracy:0.5156\n",
    "Epoch #158: Loss:0.9383, Accuracy:0.5129, Validation Loss:0.9482, Validation Accuracy:0.5090\n",
    "Epoch #159: Loss:0.9396, Accuracy:0.5175, Validation Loss:0.9546, Validation Accuracy:0.5107\n",
    "Epoch #160: Loss:0.9469, Accuracy:0.5138, Validation Loss:0.9557, Validation Accuracy:0.5025\n",
    "Epoch #161: Loss:0.9546, Accuracy:0.4990, Validation Loss:0.9530, Validation Accuracy:0.5057\n",
    "Epoch #162: Loss:0.9417, Accuracy:0.5097, Validation Loss:0.9464, Validation Accuracy:0.5140\n",
    "Epoch #163: Loss:0.9398, Accuracy:0.5166, Validation Loss:0.9457, Validation Accuracy:0.5107\n",
    "Epoch #164: Loss:0.9397, Accuracy:0.5179, Validation Loss:0.9446, Validation Accuracy:0.4943\n",
    "Epoch #165: Loss:0.9379, Accuracy:0.5133, Validation Loss:0.9436, Validation Accuracy:0.5041\n",
    "Epoch #166: Loss:0.9407, Accuracy:0.5170, Validation Loss:0.9434, Validation Accuracy:0.5090\n",
    "Epoch #167: Loss:0.9402, Accuracy:0.5101, Validation Loss:0.9500, Validation Accuracy:0.5255\n",
    "Epoch #168: Loss:0.9429, Accuracy:0.5187, Validation Loss:0.9575, Validation Accuracy:0.5189\n",
    "Epoch #169: Loss:0.9406, Accuracy:0.5187, Validation Loss:0.9525, Validation Accuracy:0.5172\n",
    "Epoch #170: Loss:0.9434, Accuracy:0.5138, Validation Loss:0.9449, Validation Accuracy:0.5123\n",
    "Epoch #171: Loss:0.9447, Accuracy:0.5101, Validation Loss:0.9448, Validation Accuracy:0.5123\n",
    "Epoch #172: Loss:0.9478, Accuracy:0.5092, Validation Loss:0.9422, Validation Accuracy:0.5123\n",
    "Epoch #173: Loss:0.9406, Accuracy:0.5175, Validation Loss:0.9468, Validation Accuracy:0.5123\n",
    "Epoch #174: Loss:0.9368, Accuracy:0.5097, Validation Loss:0.9435, Validation Accuracy:0.4975\n",
    "Epoch #175: Loss:0.9357, Accuracy:0.5199, Validation Loss:0.9456, Validation Accuracy:0.5172\n",
    "Epoch #176: Loss:0.9361, Accuracy:0.5133, Validation Loss:0.9447, Validation Accuracy:0.5189\n",
    "Epoch #177: Loss:0.9379, Accuracy:0.5244, Validation Loss:0.9476, Validation Accuracy:0.5041\n",
    "Epoch #178: Loss:0.9412, Accuracy:0.5109, Validation Loss:0.9613, Validation Accuracy:0.5074\n",
    "Epoch #179: Loss:0.9406, Accuracy:0.5154, Validation Loss:0.9422, Validation Accuracy:0.5140\n",
    "Epoch #180: Loss:0.9364, Accuracy:0.5170, Validation Loss:0.9512, Validation Accuracy:0.5123\n",
    "Epoch #181: Loss:0.9451, Accuracy:0.5105, Validation Loss:0.9455, Validation Accuracy:0.5156\n",
    "Epoch #182: Loss:0.9397, Accuracy:0.5039, Validation Loss:0.9415, Validation Accuracy:0.5074\n",
    "Epoch #183: Loss:0.9377, Accuracy:0.5133, Validation Loss:0.9529, Validation Accuracy:0.5090\n",
    "Epoch #184: Loss:0.9380, Accuracy:0.5166, Validation Loss:0.9437, Validation Accuracy:0.5107\n",
    "Epoch #185: Loss:0.9386, Accuracy:0.5199, Validation Loss:0.9412, Validation Accuracy:0.5140\n",
    "Epoch #186: Loss:0.9367, Accuracy:0.5195, Validation Loss:0.9503, Validation Accuracy:0.5205\n",
    "Epoch #187: Loss:0.9344, Accuracy:0.5207, Validation Loss:0.9434, Validation Accuracy:0.5172\n",
    "Epoch #188: Loss:0.9388, Accuracy:0.5142, Validation Loss:0.9427, Validation Accuracy:0.5074\n",
    "Epoch #189: Loss:0.9437, Accuracy:0.5076, Validation Loss:0.9646, Validation Accuracy:0.5057\n",
    "Epoch #190: Loss:0.9482, Accuracy:0.5043, Validation Loss:0.9486, Validation Accuracy:0.5074\n",
    "Epoch #191: Loss:0.9401, Accuracy:0.5101, Validation Loss:0.9469, Validation Accuracy:0.5090\n",
    "Epoch #192: Loss:0.9441, Accuracy:0.5138, Validation Loss:0.9452, Validation Accuracy:0.5205\n",
    "Epoch #193: Loss:0.9331, Accuracy:0.5248, Validation Loss:0.9426, Validation Accuracy:0.5140\n",
    "Epoch #194: Loss:0.9325, Accuracy:0.5203, Validation Loss:0.9441, Validation Accuracy:0.5090\n",
    "Epoch #195: Loss:0.9332, Accuracy:0.5133, Validation Loss:0.9416, Validation Accuracy:0.5074\n",
    "Epoch #196: Loss:0.9318, Accuracy:0.5158, Validation Loss:0.9431, Validation Accuracy:0.5090\n",
    "Epoch #197: Loss:0.9329, Accuracy:0.5179, Validation Loss:0.9463, Validation Accuracy:0.5156\n",
    "Epoch #198: Loss:0.9355, Accuracy:0.5187, Validation Loss:0.9422, Validation Accuracy:0.5107\n",
    "Epoch #199: Loss:0.9449, Accuracy:0.5117, Validation Loss:0.9700, Validation Accuracy:0.5123\n",
    "Epoch #200: Loss:0.9420, Accuracy:0.5228, Validation Loss:0.9472, Validation Accuracy:0.5189\n",
    "Epoch #201: Loss:0.9409, Accuracy:0.5125, Validation Loss:0.9470, Validation Accuracy:0.5074\n",
    "Epoch #202: Loss:0.9462, Accuracy:0.5060, Validation Loss:0.9527, Validation Accuracy:0.5090\n",
    "Epoch #203: Loss:0.9385, Accuracy:0.5129, Validation Loss:0.9444, Validation Accuracy:0.5140\n",
    "Epoch #204: Loss:0.9340, Accuracy:0.5166, Validation Loss:0.9440, Validation Accuracy:0.5140\n",
    "Epoch #205: Loss:0.9388, Accuracy:0.5191, Validation Loss:0.9580, Validation Accuracy:0.5172\n",
    "Epoch #206: Loss:0.9364, Accuracy:0.5158, Validation Loss:0.9406, Validation Accuracy:0.5172\n",
    "Epoch #207: Loss:0.9340, Accuracy:0.5195, Validation Loss:0.9419, Validation Accuracy:0.5172\n",
    "Epoch #208: Loss:0.9309, Accuracy:0.5191, Validation Loss:0.9404, Validation Accuracy:0.5123\n",
    "Epoch #209: Loss:0.9328, Accuracy:0.5216, Validation Loss:0.9411, Validation Accuracy:0.5090\n",
    "Epoch #210: Loss:0.9321, Accuracy:0.5158, Validation Loss:0.9463, Validation Accuracy:0.5107\n",
    "Epoch #211: Loss:0.9305, Accuracy:0.5150, Validation Loss:0.9407, Validation Accuracy:0.5140\n",
    "Epoch #212: Loss:0.9316, Accuracy:0.5203, Validation Loss:0.9503, Validation Accuracy:0.5025\n",
    "Epoch #213: Loss:0.9341, Accuracy:0.5179, Validation Loss:0.9404, Validation Accuracy:0.5205\n",
    "Epoch #214: Loss:0.9310, Accuracy:0.5162, Validation Loss:0.9403, Validation Accuracy:0.5205\n",
    "Epoch #215: Loss:0.9301, Accuracy:0.5191, Validation Loss:0.9411, Validation Accuracy:0.5074\n",
    "Epoch #216: Loss:0.9295, Accuracy:0.5187, Validation Loss:0.9408, Validation Accuracy:0.5074\n",
    "Epoch #217: Loss:0.9290, Accuracy:0.5199, Validation Loss:0.9413, Validation Accuracy:0.5074\n",
    "Epoch #218: Loss:0.9295, Accuracy:0.5179, Validation Loss:0.9413, Validation Accuracy:0.5074\n",
    "Epoch #219: Loss:0.9280, Accuracy:0.5187, Validation Loss:0.9414, Validation Accuracy:0.5074\n",
    "Epoch #220: Loss:0.9290, Accuracy:0.5179, Validation Loss:0.9422, Validation Accuracy:0.5123\n",
    "Epoch #221: Loss:0.9283, Accuracy:0.5195, Validation Loss:0.9414, Validation Accuracy:0.5172\n",
    "Epoch #222: Loss:0.9290, Accuracy:0.5187, Validation Loss:0.9405, Validation Accuracy:0.5156\n",
    "Epoch #223: Loss:0.9298, Accuracy:0.5195, Validation Loss:0.9420, Validation Accuracy:0.5156\n",
    "Epoch #224: Loss:0.9315, Accuracy:0.5224, Validation Loss:0.9470, Validation Accuracy:0.5205\n",
    "Epoch #225: Loss:0.9317, Accuracy:0.5240, Validation Loss:0.9414, Validation Accuracy:0.5140\n",
    "Epoch #226: Loss:0.9279, Accuracy:0.5150, Validation Loss:0.9401, Validation Accuracy:0.5172\n",
    "Epoch #227: Loss:0.9282, Accuracy:0.5244, Validation Loss:0.9458, Validation Accuracy:0.5107\n",
    "Epoch #228: Loss:0.9311, Accuracy:0.5170, Validation Loss:0.9553, Validation Accuracy:0.5057\n",
    "Epoch #229: Loss:0.9351, Accuracy:0.5228, Validation Loss:0.9400, Validation Accuracy:0.5172\n",
    "Epoch #230: Loss:0.9306, Accuracy:0.5273, Validation Loss:0.9607, Validation Accuracy:0.5189\n",
    "Epoch #231: Loss:0.9387, Accuracy:0.5117, Validation Loss:0.9522, Validation Accuracy:0.5287\n",
    "Epoch #232: Loss:0.9320, Accuracy:0.5162, Validation Loss:0.9395, Validation Accuracy:0.5172\n",
    "Epoch #233: Loss:0.9293, Accuracy:0.5187, Validation Loss:0.9392, Validation Accuracy:0.5140\n",
    "Epoch #234: Loss:0.9276, Accuracy:0.5236, Validation Loss:0.9472, Validation Accuracy:0.5008\n",
    "Epoch #235: Loss:0.9325, Accuracy:0.5129, Validation Loss:0.9420, Validation Accuracy:0.5074\n",
    "Epoch #236: Loss:0.9284, Accuracy:0.5240, Validation Loss:0.9464, Validation Accuracy:0.5074\n",
    "Epoch #237: Loss:0.9280, Accuracy:0.5187, Validation Loss:0.9454, Validation Accuracy:0.5205\n",
    "Epoch #238: Loss:0.9285, Accuracy:0.5257, Validation Loss:0.9431, Validation Accuracy:0.5222\n",
    "Epoch #239: Loss:0.9302, Accuracy:0.5253, Validation Loss:0.9400, Validation Accuracy:0.5205\n",
    "Epoch #240: Loss:0.9329, Accuracy:0.5253, Validation Loss:0.9451, Validation Accuracy:0.5140\n",
    "Epoch #241: Loss:0.9305, Accuracy:0.5265, Validation Loss:0.9414, Validation Accuracy:0.5172\n",
    "Epoch #242: Loss:0.9278, Accuracy:0.5211, Validation Loss:0.9402, Validation Accuracy:0.5205\n",
    "Epoch #243: Loss:0.9281, Accuracy:0.5179, Validation Loss:0.9407, Validation Accuracy:0.5123\n",
    "Epoch #244: Loss:0.9265, Accuracy:0.5240, Validation Loss:0.9434, Validation Accuracy:0.5238\n",
    "Epoch #245: Loss:0.9272, Accuracy:0.5207, Validation Loss:0.9392, Validation Accuracy:0.5140\n",
    "Epoch #246: Loss:0.9345, Accuracy:0.5146, Validation Loss:0.9395, Validation Accuracy:0.5205\n",
    "Epoch #247: Loss:0.9471, Accuracy:0.5084, Validation Loss:0.9505, Validation Accuracy:0.5090\n",
    "Epoch #248: Loss:0.9445, Accuracy:0.5170, Validation Loss:0.9674, Validation Accuracy:0.4877\n",
    "Epoch #249: Loss:0.9407, Accuracy:0.5051, Validation Loss:0.9413, Validation Accuracy:0.5205\n",
    "Epoch #250: Loss:0.9316, Accuracy:0.5150, Validation Loss:0.9453, Validation Accuracy:0.5271\n",
    "Epoch #251: Loss:0.9265, Accuracy:0.5306, Validation Loss:0.9464, Validation Accuracy:0.5057\n",
    "Epoch #252: Loss:0.9319, Accuracy:0.5158, Validation Loss:0.9453, Validation Accuracy:0.5123\n",
    "Epoch #253: Loss:0.9346, Accuracy:0.5203, Validation Loss:0.9462, Validation Accuracy:0.5255\n",
    "Epoch #254: Loss:0.9326, Accuracy:0.5170, Validation Loss:0.9503, Validation Accuracy:0.5320\n",
    "Epoch #255: Loss:0.9299, Accuracy:0.5183, Validation Loss:0.9384, Validation Accuracy:0.5140\n",
    "Epoch #256: Loss:0.9254, Accuracy:0.5211, Validation Loss:0.9429, Validation Accuracy:0.5107\n",
    "Epoch #257: Loss:0.9338, Accuracy:0.5187, Validation Loss:0.9390, Validation Accuracy:0.5238\n",
    "Epoch #258: Loss:0.9412, Accuracy:0.5060, Validation Loss:0.9523, Validation Accuracy:0.5304\n",
    "Epoch #259: Loss:0.9462, Accuracy:0.5043, Validation Loss:0.9732, Validation Accuracy:0.5041\n",
    "Epoch #260: Loss:0.9496, Accuracy:0.5051, Validation Loss:0.9544, Validation Accuracy:0.4926\n",
    "Epoch #261: Loss:0.9444, Accuracy:0.5125, Validation Loss:0.9457, Validation Accuracy:0.5140\n",
    "Epoch #262: Loss:0.9325, Accuracy:0.5187, Validation Loss:0.9550, Validation Accuracy:0.5107\n",
    "Epoch #263: Loss:0.9379, Accuracy:0.5203, Validation Loss:0.9394, Validation Accuracy:0.5156\n",
    "Epoch #264: Loss:0.9311, Accuracy:0.5195, Validation Loss:0.9578, Validation Accuracy:0.5107\n",
    "Epoch #265: Loss:0.9456, Accuracy:0.5060, Validation Loss:0.9434, Validation Accuracy:0.5255\n",
    "Epoch #266: Loss:0.9295, Accuracy:0.5236, Validation Loss:0.9394, Validation Accuracy:0.5140\n",
    "Epoch #267: Loss:0.9252, Accuracy:0.5203, Validation Loss:0.9378, Validation Accuracy:0.5140\n",
    "Epoch #268: Loss:0.9242, Accuracy:0.5248, Validation Loss:0.9377, Validation Accuracy:0.5107\n",
    "Epoch #269: Loss:0.9232, Accuracy:0.5195, Validation Loss:0.9406, Validation Accuracy:0.5140\n",
    "Epoch #270: Loss:0.9247, Accuracy:0.5203, Validation Loss:0.9394, Validation Accuracy:0.5140\n",
    "Epoch #271: Loss:0.9237, Accuracy:0.5203, Validation Loss:0.9443, Validation Accuracy:0.5107\n",
    "Epoch #272: Loss:0.9287, Accuracy:0.5203, Validation Loss:0.9451, Validation Accuracy:0.5008\n",
    "Epoch #273: Loss:0.9250, Accuracy:0.5216, Validation Loss:0.9426, Validation Accuracy:0.5205\n",
    "Epoch #274: Loss:0.9239, Accuracy:0.5257, Validation Loss:0.9374, Validation Accuracy:0.5140\n",
    "Epoch #275: Loss:0.9254, Accuracy:0.5224, Validation Loss:0.9412, Validation Accuracy:0.5090\n",
    "Epoch #276: Loss:0.9251, Accuracy:0.5179, Validation Loss:0.9395, Validation Accuracy:0.5189\n",
    "Epoch #277: Loss:0.9244, Accuracy:0.5211, Validation Loss:0.9412, Validation Accuracy:0.5255\n",
    "Epoch #278: Loss:0.9284, Accuracy:0.5294, Validation Loss:0.9451, Validation Accuracy:0.5222\n",
    "Epoch #279: Loss:0.9254, Accuracy:0.5195, Validation Loss:0.9364, Validation Accuracy:0.5205\n",
    "Epoch #280: Loss:0.9289, Accuracy:0.5199, Validation Loss:0.9380, Validation Accuracy:0.5222\n",
    "Epoch #281: Loss:0.9232, Accuracy:0.5211, Validation Loss:0.9404, Validation Accuracy:0.5107\n",
    "Epoch #282: Loss:0.9247, Accuracy:0.5166, Validation Loss:0.9408, Validation Accuracy:0.5041\n",
    "Epoch #283: Loss:0.9298, Accuracy:0.5138, Validation Loss:0.9470, Validation Accuracy:0.5041\n",
    "Epoch #284: Loss:0.9263, Accuracy:0.5244, Validation Loss:0.9368, Validation Accuracy:0.5189\n",
    "Epoch #285: Loss:0.9213, Accuracy:0.5191, Validation Loss:0.9376, Validation Accuracy:0.5205\n",
    "Epoch #286: Loss:0.9224, Accuracy:0.5220, Validation Loss:0.9354, Validation Accuracy:0.5271\n",
    "Epoch #287: Loss:0.9207, Accuracy:0.5207, Validation Loss:0.9375, Validation Accuracy:0.5090\n",
    "Epoch #288: Loss:0.9206, Accuracy:0.5220, Validation Loss:0.9347, Validation Accuracy:0.5222\n",
    "Epoch #289: Loss:0.9188, Accuracy:0.5244, Validation Loss:0.9350, Validation Accuracy:0.5189\n",
    "Epoch #290: Loss:0.9197, Accuracy:0.5290, Validation Loss:0.9381, Validation Accuracy:0.5238\n",
    "Epoch #291: Loss:0.9203, Accuracy:0.5207, Validation Loss:0.9409, Validation Accuracy:0.5057\n",
    "Epoch #292: Loss:0.9197, Accuracy:0.5199, Validation Loss:0.9357, Validation Accuracy:0.5238\n",
    "Epoch #293: Loss:0.9206, Accuracy:0.5211, Validation Loss:0.9341, Validation Accuracy:0.5172\n",
    "Epoch #294: Loss:0.9240, Accuracy:0.5281, Validation Loss:0.9470, Validation Accuracy:0.5025\n",
    "Epoch #295: Loss:0.9263, Accuracy:0.5207, Validation Loss:0.9399, Validation Accuracy:0.5057\n",
    "Epoch #296: Loss:0.9212, Accuracy:0.5368, Validation Loss:0.9402, Validation Accuracy:0.5304\n",
    "Epoch #297: Loss:0.9196, Accuracy:0.5306, Validation Loss:0.9322, Validation Accuracy:0.5255\n",
    "Epoch #298: Loss:0.9175, Accuracy:0.5285, Validation Loss:0.9321, Validation Accuracy:0.5205\n",
    "Epoch #299: Loss:0.9176, Accuracy:0.5195, Validation Loss:0.9326, Validation Accuracy:0.5222\n",
    "Epoch #300: Loss:0.9172, Accuracy:0.5199, Validation Loss:0.9342, Validation Accuracy:0.5255\n",
    "\n",
    "Test:\n",
    "Test Loss:0.93416083, Accuracy:0.5255\n",
    "Labels: ['01', '03', '02']\n",
    "Confusion Matrix:\n",
    "       01  03   02\n",
    "t:01  120  30   90\n",
    "t:03   58  76    8\n",
    "t:02   84  19  124\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.46      0.50      0.48       240\n",
    "          03       0.61      0.54      0.57       142\n",
    "          02       0.56      0.55      0.55       227\n",
    "\n",
    "    accuracy                           0.53       609\n",
    "   macro avg       0.54      0.53      0.53       609\n",
    "weighted avg       0.53      0.53      0.53       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 05:36:59 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 42 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0818125089792587, 1.0745511023673322, 1.0756706815635042, 1.075488021025321, 1.0747043420920035, 1.0743109117000562, 1.0744472270332925, 1.0745209782385865, 1.074330922222294, 1.0742748942476972, 1.0742591284765985, 1.0742795443887194, 1.0742295802324668, 1.074148875934933, 1.0741249146719871, 1.0740168127911822, 1.0738790375845773, 1.0737512904434956, 1.0735981478088203, 1.0733100038835373, 1.0729658781797036, 1.0725659944349517, 1.0720707890631138, 1.0714510246646425, 1.0704820823591135, 1.0693215742487039, 1.0677887451864032, 1.0654560247274063, 1.062224770414418, 1.057236476484778, 1.0501032016547442, 1.0403402910639696, 1.0301067437640161, 1.0250239325274388, 1.025134094438725, 1.0234756647855385, 1.0273377957993932, 1.0261425490449803, 1.025020099234307, 1.021300860226448, 1.019607146581014, 1.016603186016991, 1.0159580049843624, 1.015302880252719, 1.0187239188866075, 1.0224913358688354, 1.0241343642299008, 1.0151170652683927, 1.0139930076004053, 1.0129980516355417, 1.0120119108942343, 1.0122960598402226, 1.010842404733542, 1.0107565623003079, 1.012406431199686, 1.0105057790361602, 1.0098407147161674, 1.0100993601167927, 1.0085127655116992, 1.0088697826529567, 1.007422860228565, 1.010490805644707, 1.0075476118692233, 1.006179158714996, 1.0059979484586292, 1.0059074643014492, 1.0053141267820336, 1.004517888592186, 1.0034110293599772, 1.003648154841268, 1.0030068607361642, 1.0031907258949844, 1.0031253596636267, 1.0021081593236312, 1.0046026221245576, 1.003592448281537, 1.0084862493724853, 1.002593686819468, 1.0046428475278155, 1.0041091500831942, 1.0052119370164543, 0.9998649681730224, 0.9981006817473175, 0.9967177108003589, 0.9960160183984853, 0.9949863002022303, 0.9948772531620583, 0.9937220324436432, 0.9925241463485805, 0.9924071462479327, 0.9976597798281702, 0.9929320228902382, 0.9898091851196853, 0.9924731600069255, 0.9877676028140464, 0.9933910148680112, 0.9882165702497235, 0.9874221857741157, 0.983553541215574, 0.9865366953542862, 0.983035858922404, 0.9861753086738398, 0.9794722833656913, 0.9829798456287541, 0.9766328094040819, 0.9776809601165195, 0.9738823105940482, 0.9801772309250041, 1.0020828182474146, 0.995338528810072, 0.9792874844986426, 0.976868592263834, 0.9758930181830583, 0.9689148175109588, 0.9689321002153731, 0.9877847590665708, 0.9796311736889857, 0.9661627114308488, 0.9791314268151332, 0.9838147297477096, 0.9848617665677627, 0.9737498569371078, 0.9772963776377034, 0.9627473822172443, 0.9689303227441847, 0.960098605535692, 0.9583093467212859, 0.9586511915931952, 0.9575768916673457, 0.9562712951833978, 0.9559040586349412, 0.9635462980160768, 0.9571736295430727, 0.9559787330760549, 0.9542313534246485, 0.9545844297103694, 0.952197346389783, 0.9525780806987744, 0.9523320689381441, 0.9519869664619709, 0.9528114567640771, 0.9507561511985578, 0.9497015719930527, 0.9573497966984027, 0.9625156658819352, 0.9635096580915654, 0.9693931524193737, 0.9577050156194001, 0.9470831371097533, 0.9494439897866085, 0.9525876132334002, 0.9461178792325539, 0.9519866124554025, 0.9478108220303979, 0.9456220962143884, 0.9475032095055667, 0.9466886000680219, 0.9482017686997337, 0.9546185539078047, 0.9556882017351723, 0.9529575016502481, 0.9463673739010477, 0.9457443078554714, 0.9445692495563739, 0.9436055704682136, 0.9434274961599967, 0.9500463187009439, 0.9574739756842552, 0.9524933490259894, 0.9449161973297107, 0.9448486117306601, 0.9421876458503147, 0.9467719295146234, 0.9434676676352427, 0.945553091457129, 0.9447474064693858, 0.947632991328028, 0.9613464223144481, 0.9422234974079727, 0.9512299697583141, 0.9455292276756713, 0.9414948361064804, 0.952923932392609, 0.9436551048642113, 0.9412238932595465, 0.9503304772384844, 0.9434237702335239, 0.9426522796181427, 0.9645603880506431, 0.9486181709566727, 0.9468846966088895, 0.9451853696544378, 0.9426388375473336, 0.9441255630530747, 0.9416011422138496, 0.9430698679977254, 0.9462571759725048, 0.9421935079524474, 0.9700001571174521, 0.9471931040580637, 0.9469722610193324, 0.9527205713276793, 0.9444303593807816, 0.943971833082647, 0.9579930560929435, 0.940572714081343, 0.9419489765206386, 0.9403682907030891, 0.9411309311542605, 0.9462586997569292, 0.9406677940600415, 0.9503263510115236, 0.9403982520690692, 0.9402720132484812, 0.9411021549321944, 0.940758744679844, 0.941286749636207, 0.9413361228354067, 0.941350188454971, 0.9421722584561566, 0.941356043510249, 0.9404939251580262, 0.941982944023433, 0.9470456723117672, 0.9413855627839789, 0.940114993865071, 0.9457759750496186, 0.9552886935290444, 0.9399778025686643, 0.9606652100293703, 0.9521772876161659, 0.9395263414273317, 0.9392037007021786, 0.9471752160092685, 0.942036762907, 0.9463525666979146, 0.9453858112662492, 0.9430746554545385, 0.9399597865020113, 0.9451231397627218, 0.9414175832995836, 0.9402233101855749, 0.940658240091233, 0.9434091603423183, 0.9391609167034795, 0.9395122339181321, 0.950451101575579, 0.9673980771027175, 0.9413099328089621, 0.9452966739391458, 0.9464385670002654, 0.945271255249656, 0.9461870458717221, 0.95025070175553, 0.938376003885504, 0.9429383539213922, 0.9389523427278929, 0.9522594285911723, 0.9732058780338181, 0.9544411017781212, 0.9456891810952737, 0.9550235632605153, 0.9393997273617386, 0.9578116404011919, 0.943366298828219, 0.9394033127426122, 0.937808018208333, 0.9377461163868458, 0.9406486719308425, 0.939358666123232, 0.9443405815728976, 0.9450514507411149, 0.9426000907111833, 0.937413620635598, 0.941197059228894, 0.9394694340640101, 0.9411669566322038, 0.9451293578288825, 0.9364130412807997, 0.9380050169423296, 0.9403504137139407, 0.9407957389045427, 0.9469788419985027, 0.9367663900448967, 0.9376362035622934, 0.9353970697360673, 0.9375120797768015, 0.9346832533188054, 0.9350239808727759, 0.9380840650332972, 0.940910303631831, 0.9356951400368476, 0.9341107220289546, 0.9470337229017749, 0.9399129919622137, 0.9401684960316751, 0.9322423087356517, 0.9320586907061058, 0.9325738050080286, 0.9341608546246057], 'val_acc': [0.37274219920286794, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.3957307052436133, 0.4121510664915608, 0.4121510664915608, 0.41050903026889307, 0.40886699404622534, 0.4121510662958148, 0.42200328324032926, 0.42200328324032926, 0.4220032831424563, 0.417077174963818, 0.4269293916147135, 0.4203612471155345, 0.430213463864303, 0.42200328324032926, 0.43349753611389247, 0.423645319462997, 0.4269293916147135, 0.4252873557835377, 0.4318554999890977, 0.42528735539204576, 0.43349753621176546, 0.430213463864303, 0.430213463864303, 0.430213463864303, 0.42857142773950824, 0.42857142773950824, 0.43678160846135494, 0.43021346406004896, 0.43349753611389247, 0.42857142773950824, 0.4384236446840227, 0.4318555001848437, 0.43021346396217597, 0.43349753611389247, 0.4318555000869707, 0.42200328372969415, 0.44006568120030937, 0.43678160846135494, 0.43349753621176546, 0.44663382560161535, 0.44170771693361216, 0.44170771693361216, 0.4417077172272311, 0.4400656812981823, 0.4318554999890977, 0.4417077173251041, 0.4433497529605339, 0.43513957272805215, 0.4449917893789476, 0.4400656810045634, 0.45484400622558907, 0.4449917895746936, 0.4548440061277161, 0.4482758617264101, 0.4564860423503838, 0.44663382579736133, 0.44991789785120484, 0.461412150920514, 0.4564860425461298, 0.4515599339759996, 0.4630541868495628, 0.44663382569948834, 0.4564860423503838, 0.44663382560161535, 0.44827586192215607, 0.4663382590991523, 0.461412150920514, 0.45812807867092453, 0.46305418704530876, 0.4646962231701035, 0.461412150920514, 0.46633825929489825, 0.46962233154448774, 0.48275862064071867, 0.4745484353677784, 0.47290639934085665, 0.4729064038919502, 0.48440065666764043, 0.47947454374216264, 0.4712643671799176, 0.4844006563740215, 0.4745484354656514, 0.47290639924298367, 0.48111658451592393, 0.48932676484627874, 0.48111658441805094, 0.47290639943872964, 0.47619047188406505, 0.4827586159917521, 0.48932676484627874, 0.48604269259668925, 0.4663382547438047, 0.4827586162853711, 0.48440065666764043, 0.49589490528372904, 0.48440065270378474, 0.48768472436613636, 0.48932676513989765, 0.5090311986374346, 0.5106732302111358, 0.5073891580594193, 0.5073891579615463, 0.4975369414085238, 0.4958949095412037, 0.4991789772396996, 0.5008210133644944, 0.5008210180134609, 0.49425287331853596, 0.5024630541382556, 0.5057471217388786, 0.5106732347622294, 0.5073891578636733, 0.5057471260942262, 0.5008210178177149, 0.5139573023628523, 0.5073891623168939, 0.5057471263878451, 0.48604269259668925, 0.5057471263878451, 0.5057471262899721, 0.5057471218367515, 0.5106732302111358, 0.5041050900673044, 0.509031194184214, 0.5106732346643563, 0.5008210177198419, 0.5057471262899721, 0.5155993430387407, 0.5090311985395616, 0.5106732347622294, 0.5024630539425097, 0.5057471260942262, 0.5139573069139459, 0.5106732301132628, 0.494252872927044, 0.5041050897736855, 0.5090311983438157, 0.5254515557257804, 0.5188834110308554, 0.5172413749060607, 0.5123152663359305, 0.5123152708870241, 0.5123152706912781, 0.5123152662380576, 0.49753694537237947, 0.5172413750039337, 0.5188834152883302, 0.5041050902630504, 0.5073891624147668, 0.5139573069139459, 0.5123152666295495, 0.515599338683393, 0.507389162219021, 0.5090311984416886, 0.5106732345664834, 0.5139573069139459, 0.5205254473513962, 0.5172413746124418, 0.5073891621211479, 0.5057471263878451, 0.5073891623168939, 0.5090311984416886, 0.5205254472535232, 0.5139573023628523, 0.5090311984416886, 0.507389162219021, 0.5090311984416886, 0.5155993430387407, 0.5106732346643563, 0.5123152669231684, 0.5188834110308554, 0.5073891624147668, 0.5090311985395616, 0.5139573070118189, 0.5139573027543443, 0.5172413751018067, 0.5172413791635354, 0.5172413791635354, 0.5123152706912781, 0.5090311984416886, 0.5106732304068817, 0.5139573069139459, 0.5024630539425097, 0.5205254515109978, 0.5205254515109978, 0.5073891623168939, 0.507389162219021, 0.5073891623168939, 0.5073891623168939, 0.5073891623168939, 0.5123152708870241, 0.5172413791635354, 0.5155993431366136, 0.5155993429408676, 0.5205254471556502, 0.5139573023628523, 0.5172413746124418, 0.5106732345664834, 0.5057471263878451, 0.5172413792614083, 0.5188834113244745, 0.5287356279753699, 0.5172413792614083, 0.5139573070118189, 0.5008210177198419, 0.5073891624147668, 0.507389162219021, 0.5205254471556502, 0.522167483182572, 0.5205254515109978, 0.5139573068160729, 0.5172413790656624, 0.5205254515109978, 0.5123152662380576, 0.5238095193073667, 0.5139573070118189, 0.5205254468620313, 0.5090311986374346, 0.487684728721484, 0.5205254468620313, 0.5270935918505751, 0.5057471261920992, 0.5123152707891511, 0.5254515556279075, 0.5320197002249594, 0.5139573023628523, 0.5106732345664834, 0.5238095237605873, 0.5303776641001646, 0.5041050862991947, 0.4926108371937412, 0.5139573023628523, 0.5106732307005006, 0.5155993429408676, 0.5106732347622294, 0.5254515556279075, 0.5139573070118189, 0.5139573024607252, 0.5106732346643563, 0.5139573023628523, 0.5139573023628523, 0.5106732345664834, 0.5008210178177149, 0.5205254470577773, 0.5139573023628523, 0.5090311986374346, 0.5188834108351095, 0.5254515553342884, 0.5221674832804449, 0.5205254468620313, 0.5221674876357926, 0.5106732346643563, 0.5041050901651773, 0.5041050901651773, 0.5188834108351095, 0.5205254470577773, 0.5270935960101768, 0.5090311985395616, 0.5221674876357926, 0.5188834107372365, 0.5238095193073667, 0.5057471262899721, 0.5238095193073667, 0.5172413746124418, 0.5024630540403826, 0.5057471263878451, 0.5303776639044187, 0.5254515552364155, 0.5205254469599042, 0.5221674876357926, 0.5254515556279075], 'loss': [1.0924244104224798, 1.0776868374685487, 1.0758055818154337, 1.0756471441511746, 1.0749472936320843, 1.0745722684527326, 1.0742271279407478, 1.0744262683073353, 1.0744233334578528, 1.0741791567028915, 1.0741474759162575, 1.0741392680017365, 1.0741501009684569, 1.074151202051057, 1.0742885129897257, 1.073975076420841, 1.0739678964967356, 1.0737263913027315, 1.0736406051647491, 1.073461103096635, 1.0732626047467304, 1.0728749730014213, 1.07239638363801, 1.0720596540145562, 1.0715840445406872, 1.0705228291008262, 1.0692453481823023, 1.0676664990810887, 1.0654330382846464, 1.0625723728164265, 1.0571797693044989, 1.050353055274462, 1.0418260072535803, 1.0338469328087214, 1.031262867896219, 1.0300860749622636, 1.0283998925093507, 1.029773352327288, 1.0260281675405325, 1.026159570985751, 1.0255302077690924, 1.0242708787291446, 1.0235746634324718, 1.0231994114862086, 1.0231641917258072, 1.0267841489897616, 1.0292257045083957, 1.0271465630991503, 1.022125024570332, 1.020409316595575, 1.0198323192048122, 1.019484518196059, 1.0204063995173336, 1.0213379997981893, 1.0222134297388534, 1.019284833432223, 1.0181314284307021, 1.0176536137563248, 1.0181491775189582, 1.0161971499542926, 1.0165532151286851, 1.0157671279241416, 1.0168045479658938, 1.016448619625162, 1.0149737324802783, 1.0145625944744634, 1.0145123906693665, 1.0131100942466782, 1.0128785800395315, 1.0142318231613974, 1.0138487826627383, 1.0114558645831975, 1.0120551308077708, 1.0109204327546106, 1.0108496092917738, 1.010747518480681, 1.0113878279985589, 1.0144925765677888, 1.0089629148800514, 1.0098066782314918, 1.0122641034684388, 1.0110221744073244, 1.0077856738464543, 1.0064690250635637, 1.0054627605777011, 1.0047996273275763, 1.0041015615208682, 1.0032137048562695, 1.0019575902813513, 1.0025361885525117, 1.0017203578224416, 1.003343758911078, 1.0005396556315727, 0.9985756505686155, 0.9999273583874321, 1.000592346137554, 0.9954519871813561, 0.9941927518443161, 0.9942687638731218, 0.9968298696149791, 0.9894733798332528, 0.9869851790414453, 0.9881198977053288, 0.9851770598296022, 0.9852377554229642, 0.9820138262282652, 0.9870785418721929, 0.9831105283643186, 0.9865387784871722, 0.991884177127658, 0.9959091649163185, 0.9798434703501833, 0.9779591366250902, 0.9718688548223194, 0.9696725478896859, 0.9684434821228717, 0.9757712348530669, 0.9827456465492014, 0.9758058884060603, 0.9739648429764859, 0.9802567883438643, 0.9837946938782992, 0.9799148252367729, 0.9731690415611502, 0.9660778027785143, 0.9625469632706848, 0.9589525027196755, 0.9582557485823269, 0.9552329601937986, 0.9556260400239447, 0.9541495299192425, 0.9541382984948599, 0.9525933759902782, 0.9531888692775546, 0.9611558578831949, 0.9565723378800268, 0.9459142571357242, 0.9469109347468774, 0.9500846826075529, 0.9457853020828607, 0.9444819838359371, 0.9435787279258274, 0.9428263349944317, 0.9430833712495573, 0.9454087635574889, 0.9549787781811347, 0.9560743992578322, 0.9532196892115614, 0.9459151403370334, 0.9427346862317111, 0.9432547291691054, 0.9415885058027028, 0.9387656570949594, 0.9391464282599807, 0.9386839050531877, 0.9378097841627054, 0.9415596266057212, 0.9382754755705534, 0.9396477864263484, 0.9469499100894654, 0.9545994224734375, 0.9417336951291047, 0.9398410208905746, 0.9397082461958304, 0.9378589546411189, 0.9407111482943352, 0.9401514467762236, 0.9429424842280284, 0.9405730526794888, 0.9434030165172945, 0.9447427159951698, 0.9478408976257214, 0.9406312181964303, 0.936789837240928, 0.9356597355014244, 0.9361034462339335, 0.9379146513508085, 0.9411579887969783, 0.9406066189801179, 0.9364202551528413, 0.9450894600312079, 0.9396959150596321, 0.9376607188943475, 0.9380375727980533, 0.9386187901976661, 0.9366930037553306, 0.9343519244840258, 0.9388265526759796, 0.9436858055038374, 0.9481874107580165, 0.9401434330724837, 0.944074414373668, 0.9331087619127434, 0.9324772499669993, 0.9332076966884935, 0.9317919070715777, 0.9329476765538632, 0.9354833460196822, 0.9448879244880755, 0.9420484234909747, 0.9408992221223256, 0.9462138984482391, 0.938475919651055, 0.933979377144416, 0.938827079228552, 0.9364246053862131, 0.9340289176856713, 0.930904965454548, 0.9327798010142677, 0.9321200068726432, 0.930528185328419, 0.9315565430163358, 0.9340807407054079, 0.9310390927218803, 0.9300717081622176, 0.9294877026115355, 0.9290046745501994, 0.9295246584214714, 0.9280158114629115, 0.929027557887091, 0.9283267547952077, 0.92902195637231, 0.9298111088222057, 0.931485647832099, 0.93169804706221, 0.9279409554704748, 0.9281507947361689, 0.9310869982110401, 0.9351430052359736, 0.9306360148061718, 0.9386670585285711, 0.9319874681486486, 0.9292814140691894, 0.9275936663028396, 0.9325225625684374, 0.9283561172671386, 0.9280363093411408, 0.9285395542209398, 0.9302476606574637, 0.9329383801141069, 0.930518555274, 0.9278465967158762, 0.928112755859657, 0.9264723518301085, 0.9272076293673114, 0.9345443813463011, 0.9470669296243107, 0.9444777683066147, 0.9407042617915348, 0.9316047808717653, 0.9264715879115236, 0.9319202499467979, 0.934632052459756, 0.9326116719774642, 0.9298899837832676, 0.9254464829482092, 0.9338441831620077, 0.9411538608020336, 0.9461741702267766, 0.9495718105372952, 0.9444248667487863, 0.9324633375819948, 0.9379183769226074, 0.9310698813481497, 0.9455731586264389, 0.9294960430270592, 0.9251850457406876, 0.9242075841529658, 0.9232133726319738, 0.9246582366847405, 0.9237030074336935, 0.928693251957394, 0.9249780323960698, 0.9238816045148172, 0.925371851225898, 0.9250880168693512, 0.9243828849381245, 0.9284068944028271, 0.9253695855640043, 0.9288557701531867, 0.9232096415525589, 0.9247447406242026, 0.9298378342720517, 0.9262975091562134, 0.9213007185248624, 0.9224332359781989, 0.9207303116943313, 0.9205817554031309, 0.9188203466256786, 0.9197361881483263, 0.9202503392094215, 0.9196853822995995, 0.9205541117724941, 0.9240479165034128, 0.9262672921225765, 0.9212110159578265, 0.919602741597859, 0.9174546229276325, 0.9175603275915926, 0.9172031819698012], 'acc': [0.3728952775745666, 0.3749486670357001, 0.3942505157091779, 0.3942505115233897, 0.3942505151216989, 0.3942505141058497, 0.3942505133225443, 0.394250514301676, 0.39425051351837065, 0.3942505113275634, 0.3942505128941742, 0.39425051351837065, 0.39425051449750237, 0.39425051351837065, 0.3942505146933287, 0.394250514301676, 0.39425051547663414, 0.39425051171921605, 0.3942505152808078, 0.3942505148891551, 0.39425051328582683, 0.39425051309000053, 0.3942505148891551, 0.3942505123434126, 0.3942505119150424, 0.3942505141058497, 0.3942505123066951, 0.39425051250252147, 0.3942505146933287, 0.39630389980956515, 0.4012320338088629, 0.40657084163209495, 0.41930185033800177, 0.4242299796007497, 0.42751540132861365, 0.42792607933833615, 0.4271047207731486, 0.4234086222105202, 0.4299794654704217, 0.42587268968137626, 0.4254620126507855, 0.42956878765652556, 0.43367556677461894, 0.42874743242038593, 0.43613963169973247, 0.4303901422684687, 0.4361396311122534, 0.4271047223397594, 0.42956878726487285, 0.43121150067454733, 0.4344969218149322, 0.43449692161910586, 0.4316221777051381, 0.42915811043010843, 0.4344969184858843, 0.4369609875233511, 0.43860369623319323, 0.43449691868171064, 0.43901437146462946, 0.43942505264429094, 0.44353182663418184, 0.4373716639664628, 0.4312115011029175, 0.4353182739178503, 0.4283367542148371, 0.4439425060514062, 0.4439425066388853, 0.4353182760719401, 0.43367556677461894, 0.43901437502622115, 0.4390143718929996, 0.4443531840611287, 0.4406570854985004, 0.4332648881774174, 0.43901437263958754, 0.44106775998334863, 0.43901437345961036, 0.4373716649455946, 0.43942505107768015, 0.4439425046806218, 0.4459958937501026, 0.4476386042223819, 0.4447638595251087, 0.4505133473530442, 0.4443531816744951, 0.4414784379930712, 0.45338808911292217, 0.4529774114581349, 0.4542094457198462, 0.4513347038008594, 0.45174537883646926, 0.44804928281958345, 0.4562628351809797, 0.45872689971444053, 0.4492813121122012, 0.44599589472923434, 0.4509240259502458, 0.45420944611149894, 0.45338808989622753, 0.44845996043765324, 0.45831622111723896, 0.4501026675808846, 0.44845995726771426, 0.46324435449234025, 0.46242299628208794, 0.4652977401960557, 0.46036961077419886, 0.4599589337436081, 0.4624229992562006, 0.4665297732460915, 0.4488706380557231, 0.4694045165725802, 0.45749486881849455, 0.4792607814012367, 0.47926077862295036, 0.47392197037134814, 0.47885010397899325, 0.4751540047921684, 0.4784394271442288, 0.4845995890286424, 0.479671457061043, 0.46611909817376423, 0.48254620270073045, 0.48172484828461365, 0.4870636571236949, 0.49117043405098104, 0.4895277200538275, 0.4895277212287856, 0.49281314080255967, 0.4977412720235711, 0.5002053410610379, 0.496098563313729, 0.5047227934890215, 0.49609856233459726, 0.49404517718164337, 0.49856262823884245, 0.4985626315678904, 0.5104722815495008, 0.5055441471585503, 0.5039014366862711, 0.5141683822295015, 0.509240246504484, 0.5092402494418793, 0.5059548243849674, 0.513757700303252, 0.4993839834749821, 0.49240246200953175, 0.5026694058270424, 0.5034907592640275, 0.5047227954472849, 0.5174538002733822, 0.5039014374695764, 0.5170431235977266, 0.5108829563892843, 0.5141683789004535, 0.5149897364865094, 0.5133470258184037, 0.5129363456545915, 0.5174537967117905, 0.5137577018698628, 0.4989733042903015, 0.5096509268641227, 0.5166324455880041, 0.5178644796171717, 0.5133470222935295, 0.5170431212478105, 0.5100616027197554, 0.5186858338741795, 0.5186858269468225, 0.5137577026531681, 0.5100616013489709, 0.5092402480710948, 0.5174537986700539, 0.509650922555943, 0.5199178663367363, 0.5133470252309247, 0.5244353217021152, 0.5108829552143261, 0.5154004135171001, 0.5170431218352896, 0.5104722828835677, 0.5039014376654027, 0.5133470234684876, 0.5166324479379204, 0.5199178645742992, 0.5195071883270138, 0.5207392196146124, 0.5141683769421901, 0.5075975338781149, 0.5043121190041732, 0.51006160193645, 0.5137577022615155, 0.524845997166095, 0.5203285432082182, 0.5133470246434456, 0.5158110881977747, 0.5178644788338663, 0.5186858323075688, 0.51170431397534, 0.5227926092715723, 0.5125256664699108, 0.5059548257557518, 0.5129363456545915, 0.5166324446088725, 0.5190965099256386, 0.5158110889810801, 0.5195071867604031, 0.5190965091423332, 0.5215605785714528, 0.5158110909393436, 0.5149897343324196, 0.5203285429756744, 0.51786447863804, 0.5162217667949762, 0.5190965122755548, 0.5186858340700059, 0.5199178651617783, 0.5178644782463873, 0.5186858326992215, 0.5178644800088245, 0.5195071883270138, 0.5186858323075688, 0.5195071875437084, 0.5223819260969299, 0.52402464330074, 0.514989732374156, 0.5244353209188097, 0.517043119289547, 0.5227926041066524, 0.5273100618953822, 0.5117043104504658, 0.5162217665991499, 0.5186858328950479, 0.5236139585594867, 0.5129363452629387, 0.5240246397758657, 0.5186858336783533, 0.5256673522064084, 0.5252566741966859, 0.5252566747841649, 0.5264887097924642, 0.5211499007575566, 0.5178644807921298, 0.5240246431049136, 0.5207392231394867, 0.5145790561268706, 0.508418890485039, 0.5170431200728525, 0.5051334724778757, 0.514989732374156, 0.530595482448288, 0.5158110876102957, 0.5203285429756744, 0.517043123793553, 0.5182751539061936, 0.5211499011492092, 0.5186858321117425, 0.5059548277140153, 0.504312116066778, 0.5051334716945703, 0.5125256674490425, 0.5186858325033952, 0.5203285439548062, 0.5195071883270138, 0.5059548253640992, 0.523613965095191, 0.5203285453255906, 0.5248459977535741, 0.519507190089451, 0.5203285445422852, 0.5203285466963751, 0.520328542192369, 0.5215605783756264, 0.5256673510314502, 0.5223819316535026, 0.5178644757006448, 0.5211499007575566, 0.5293634517481685, 0.5195071881311875, 0.5199178645742992, 0.5211498986034667, 0.5166324428464353, 0.5137577014782101, 0.5244353211146361, 0.5190965112964231, 0.5219712546596292, 0.5207392233353131, 0.5219712526646483, 0.5244353224854205, 0.5289527725634878, 0.5207392203979179, 0.5199178637909938, 0.5211498987992931, 0.5281314198730908, 0.5207392198104388, 0.5367556480901198, 0.5305954846023779, 0.5285420965120288, 0.5195071857812713, 0.5199178657492572]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
