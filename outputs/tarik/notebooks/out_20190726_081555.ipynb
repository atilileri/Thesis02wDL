{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf35.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 08:15:55 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '3', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '04', '01', '02', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000018EE468BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000018EDDEB6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6111, Accuracy:0.1992, Validation Loss:1.6079, Validation Accuracy:0.2365\n",
    "Epoch #2: Loss:1.6062, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2200\n",
    "Epoch #3: Loss:1.6059, Accuracy:0.2251, Validation Loss:1.6053, Validation Accuracy:0.2381\n",
    "Epoch #4: Loss:1.6056, Accuracy:0.2345, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6041, Accuracy:0.2329, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6033, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6030, Accuracy:0.2341, Validation Loss:1.6043, Validation Accuracy:0.2381\n",
    "Epoch #14: Loss:1.6034, Accuracy:0.2398, Validation Loss:1.6044, Validation Accuracy:0.2397\n",
    "Epoch #15: Loss:1.6035, Accuracy:0.2407, Validation Loss:1.6040, Validation Accuracy:0.2348\n",
    "Epoch #16: Loss:1.6029, Accuracy:0.2402, Validation Loss:1.6037, Validation Accuracy:0.2348\n",
    "Epoch #17: Loss:1.6029, Accuracy:0.2407, Validation Loss:1.6035, Validation Accuracy:0.2365\n",
    "Epoch #18: Loss:1.6025, Accuracy:0.2398, Validation Loss:1.6034, Validation Accuracy:0.2348\n",
    "Epoch #19: Loss:1.6024, Accuracy:0.2452, Validation Loss:1.6029, Validation Accuracy:0.2447\n",
    "Epoch #20: Loss:1.6022, Accuracy:0.2407, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6025, Accuracy:0.2394, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6027, Accuracy:0.2402, Validation Loss:1.6037, Validation Accuracy:0.2365\n",
    "Epoch #23: Loss:1.6027, Accuracy:0.2411, Validation Loss:1.6034, Validation Accuracy:0.2397\n",
    "Epoch #24: Loss:1.6023, Accuracy:0.2460, Validation Loss:1.6027, Validation Accuracy:0.2463\n",
    "Epoch #25: Loss:1.6023, Accuracy:0.2435, Validation Loss:1.6028, Validation Accuracy:0.2447\n",
    "Epoch #26: Loss:1.6023, Accuracy:0.2444, Validation Loss:1.6032, Validation Accuracy:0.2365\n",
    "Epoch #27: Loss:1.6022, Accuracy:0.2411, Validation Loss:1.6025, Validation Accuracy:0.2447\n",
    "Epoch #28: Loss:1.6021, Accuracy:0.2448, Validation Loss:1.6024, Validation Accuracy:0.2447\n",
    "Epoch #29: Loss:1.6016, Accuracy:0.2452, Validation Loss:1.6016, Validation Accuracy:0.2397\n",
    "Epoch #30: Loss:1.6012, Accuracy:0.2423, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #31: Loss:1.6011, Accuracy:0.2419, Validation Loss:1.6023, Validation Accuracy:0.2397\n",
    "Epoch #32: Loss:1.6016, Accuracy:0.2398, Validation Loss:1.6021, Validation Accuracy:0.2496\n",
    "Epoch #33: Loss:1.6013, Accuracy:0.2431, Validation Loss:1.6025, Validation Accuracy:0.2348\n",
    "Epoch #34: Loss:1.6018, Accuracy:0.2439, Validation Loss:1.6027, Validation Accuracy:0.2315\n",
    "Epoch #35: Loss:1.6014, Accuracy:0.2419, Validation Loss:1.6020, Validation Accuracy:0.2397\n",
    "Epoch #36: Loss:1.6011, Accuracy:0.2439, Validation Loss:1.6020, Validation Accuracy:0.2479\n",
    "Epoch #37: Loss:1.6011, Accuracy:0.2402, Validation Loss:1.6020, Validation Accuracy:0.2397\n",
    "Epoch #38: Loss:1.6010, Accuracy:0.2398, Validation Loss:1.6020, Validation Accuracy:0.2397\n",
    "Epoch #39: Loss:1.6010, Accuracy:0.2398, Validation Loss:1.6017, Validation Accuracy:0.2397\n",
    "Epoch #40: Loss:1.6009, Accuracy:0.2419, Validation Loss:1.6016, Validation Accuracy:0.2496\n",
    "Epoch #41: Loss:1.6010, Accuracy:0.2435, Validation Loss:1.6015, Validation Accuracy:0.2496\n",
    "Epoch #42: Loss:1.6007, Accuracy:0.2411, Validation Loss:1.6021, Validation Accuracy:0.2397\n",
    "Epoch #43: Loss:1.6007, Accuracy:0.2398, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #44: Loss:1.6007, Accuracy:0.2411, Validation Loss:1.6018, Validation Accuracy:0.2496\n",
    "Epoch #45: Loss:1.6005, Accuracy:0.2407, Validation Loss:1.6018, Validation Accuracy:0.2397\n",
    "Epoch #46: Loss:1.6004, Accuracy:0.2411, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #47: Loss:1.6003, Accuracy:0.2411, Validation Loss:1.6013, Validation Accuracy:0.2397\n",
    "Epoch #48: Loss:1.6003, Accuracy:0.2419, Validation Loss:1.6013, Validation Accuracy:0.2463\n",
    "Epoch #49: Loss:1.6001, Accuracy:0.2452, Validation Loss:1.6012, Validation Accuracy:0.2463\n",
    "Epoch #50: Loss:1.5999, Accuracy:0.2402, Validation Loss:1.6014, Validation Accuracy:0.2397\n",
    "Epoch #51: Loss:1.5997, Accuracy:0.2402, Validation Loss:1.6012, Validation Accuracy:0.2397\n",
    "Epoch #52: Loss:1.6001, Accuracy:0.2423, Validation Loss:1.6013, Validation Accuracy:0.2529\n",
    "Epoch #53: Loss:1.5997, Accuracy:0.2423, Validation Loss:1.6011, Validation Accuracy:0.2463\n",
    "Epoch #54: Loss:1.5996, Accuracy:0.2427, Validation Loss:1.6011, Validation Accuracy:0.2430\n",
    "Epoch #55: Loss:1.5994, Accuracy:0.2419, Validation Loss:1.6016, Validation Accuracy:0.2397\n",
    "Epoch #56: Loss:1.5994, Accuracy:0.2415, Validation Loss:1.6012, Validation Accuracy:0.2512\n",
    "Epoch #57: Loss:1.5995, Accuracy:0.2423, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #58: Loss:1.5994, Accuracy:0.2439, Validation Loss:1.6007, Validation Accuracy:0.2430\n",
    "Epoch #59: Loss:1.5993, Accuracy:0.2435, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #60: Loss:1.5989, Accuracy:0.2452, Validation Loss:1.6010, Validation Accuracy:0.2397\n",
    "Epoch #61: Loss:1.5992, Accuracy:0.2431, Validation Loss:1.6008, Validation Accuracy:0.2479\n",
    "Epoch #62: Loss:1.5993, Accuracy:0.2419, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #63: Loss:1.5984, Accuracy:0.2439, Validation Loss:1.6010, Validation Accuracy:0.2397\n",
    "Epoch #64: Loss:1.5989, Accuracy:0.2407, Validation Loss:1.6011, Validation Accuracy:0.2397\n",
    "Epoch #65: Loss:1.5987, Accuracy:0.2415, Validation Loss:1.6009, Validation Accuracy:0.2512\n",
    "Epoch #66: Loss:1.5989, Accuracy:0.2444, Validation Loss:1.6005, Validation Accuracy:0.2496\n",
    "Epoch #67: Loss:1.5986, Accuracy:0.2427, Validation Loss:1.6005, Validation Accuracy:0.2397\n",
    "Epoch #68: Loss:1.5988, Accuracy:0.2419, Validation Loss:1.6005, Validation Accuracy:0.2496\n",
    "Epoch #69: Loss:1.5986, Accuracy:0.2435, Validation Loss:1.6011, Validation Accuracy:0.2512\n",
    "Epoch #70: Loss:1.5986, Accuracy:0.2427, Validation Loss:1.6008, Validation Accuracy:0.2496\n",
    "Epoch #71: Loss:1.5984, Accuracy:0.2427, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #72: Loss:1.5985, Accuracy:0.2427, Validation Loss:1.6015, Validation Accuracy:0.2479\n",
    "Epoch #73: Loss:1.5983, Accuracy:0.2444, Validation Loss:1.6014, Validation Accuracy:0.2479\n",
    "Epoch #74: Loss:1.5980, Accuracy:0.2439, Validation Loss:1.6017, Validation Accuracy:0.2397\n",
    "Epoch #75: Loss:1.5978, Accuracy:0.2419, Validation Loss:1.6016, Validation Accuracy:0.2545\n",
    "Epoch #76: Loss:1.5985, Accuracy:0.2415, Validation Loss:1.6018, Validation Accuracy:0.2414\n",
    "Epoch #77: Loss:1.5975, Accuracy:0.2402, Validation Loss:1.6022, Validation Accuracy:0.2463\n",
    "Epoch #78: Loss:1.5984, Accuracy:0.2468, Validation Loss:1.6021, Validation Accuracy:0.2496\n",
    "Epoch #79: Loss:1.5978, Accuracy:0.2407, Validation Loss:1.6023, Validation Accuracy:0.2447\n",
    "Epoch #80: Loss:1.5974, Accuracy:0.2423, Validation Loss:1.6018, Validation Accuracy:0.2381\n",
    "Epoch #81: Loss:1.5986, Accuracy:0.2448, Validation Loss:1.6028, Validation Accuracy:0.2315\n",
    "Epoch #82: Loss:1.5976, Accuracy:0.2497, Validation Loss:1.6025, Validation Accuracy:0.2479\n",
    "Epoch #83: Loss:1.5980, Accuracy:0.2407, Validation Loss:1.6023, Validation Accuracy:0.2479\n",
    "Epoch #84: Loss:1.5972, Accuracy:0.2431, Validation Loss:1.6021, Validation Accuracy:0.2414\n",
    "Epoch #85: Loss:1.5976, Accuracy:0.2489, Validation Loss:1.6023, Validation Accuracy:0.2512\n",
    "Epoch #86: Loss:1.5975, Accuracy:0.2439, Validation Loss:1.6023, Validation Accuracy:0.2496\n",
    "Epoch #87: Loss:1.5975, Accuracy:0.2460, Validation Loss:1.6022, Validation Accuracy:0.2496\n",
    "Epoch #88: Loss:1.5969, Accuracy:0.2460, Validation Loss:1.6019, Validation Accuracy:0.2479\n",
    "Epoch #89: Loss:1.5985, Accuracy:0.2382, Validation Loss:1.6027, Validation Accuracy:0.2348\n",
    "Epoch #90: Loss:1.5969, Accuracy:0.2370, Validation Loss:1.6019, Validation Accuracy:0.2479\n",
    "Epoch #91: Loss:1.5969, Accuracy:0.2472, Validation Loss:1.6025, Validation Accuracy:0.2348\n",
    "Epoch #92: Loss:1.5979, Accuracy:0.2505, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #93: Loss:1.5973, Accuracy:0.2419, Validation Loss:1.6028, Validation Accuracy:0.2233\n",
    "Epoch #94: Loss:1.5972, Accuracy:0.2505, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #95: Loss:1.5972, Accuracy:0.2460, Validation Loss:1.6012, Validation Accuracy:0.2447\n",
    "Epoch #96: Loss:1.5975, Accuracy:0.2423, Validation Loss:1.6006, Validation Accuracy:0.2447\n",
    "Epoch #97: Loss:1.5981, Accuracy:0.2427, Validation Loss:1.6009, Validation Accuracy:0.2365\n",
    "Epoch #98: Loss:1.5973, Accuracy:0.2472, Validation Loss:1.6025, Validation Accuracy:0.2496\n",
    "Epoch #99: Loss:1.5968, Accuracy:0.2464, Validation Loss:1.6024, Validation Accuracy:0.2365\n",
    "Epoch #100: Loss:1.5968, Accuracy:0.2542, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #101: Loss:1.5969, Accuracy:0.2431, Validation Loss:1.6000, Validation Accuracy:0.2479\n",
    "Epoch #102: Loss:1.5979, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2479\n",
    "Epoch #103: Loss:1.5983, Accuracy:0.2452, Validation Loss:1.6015, Validation Accuracy:0.2463\n",
    "Epoch #104: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6022, Validation Accuracy:0.2365\n",
    "Epoch #105: Loss:1.5981, Accuracy:0.2402, Validation Loss:1.6021, Validation Accuracy:0.2447\n",
    "Epoch #106: Loss:1.5983, Accuracy:0.2493, Validation Loss:1.6017, Validation Accuracy:0.2479\n",
    "Epoch #107: Loss:1.5975, Accuracy:0.2398, Validation Loss:1.6013, Validation Accuracy:0.2627\n",
    "Epoch #108: Loss:1.5979, Accuracy:0.2378, Validation Loss:1.6012, Validation Accuracy:0.2529\n",
    "Epoch #109: Loss:1.6001, Accuracy:0.2439, Validation Loss:1.6024, Validation Accuracy:0.2496\n",
    "Epoch #110: Loss:1.5991, Accuracy:0.2452, Validation Loss:1.6039, Validation Accuracy:0.2299\n",
    "Epoch #111: Loss:1.6003, Accuracy:0.2361, Validation Loss:1.6012, Validation Accuracy:0.2397\n",
    "Epoch #112: Loss:1.5978, Accuracy:0.2460, Validation Loss:1.6011, Validation Accuracy:0.2545\n",
    "Epoch #113: Loss:1.5985, Accuracy:0.2402, Validation Loss:1.6002, Validation Accuracy:0.2529\n",
    "Epoch #114: Loss:1.5978, Accuracy:0.2357, Validation Loss:1.6002, Validation Accuracy:0.2381\n",
    "Epoch #115: Loss:1.5980, Accuracy:0.2435, Validation Loss:1.6013, Validation Accuracy:0.2381\n",
    "Epoch #116: Loss:1.5983, Accuracy:0.2419, Validation Loss:1.6011, Validation Accuracy:0.2348\n",
    "Epoch #117: Loss:1.5983, Accuracy:0.2407, Validation Loss:1.6011, Validation Accuracy:0.2562\n",
    "Epoch #118: Loss:1.5977, Accuracy:0.2357, Validation Loss:1.6002, Validation Accuracy:0.2529\n",
    "Epoch #119: Loss:1.5980, Accuracy:0.2402, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #120: Loss:1.5982, Accuracy:0.2452, Validation Loss:1.6011, Validation Accuracy:0.2365\n",
    "Epoch #121: Loss:1.5980, Accuracy:0.2460, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #122: Loss:1.5980, Accuracy:0.2444, Validation Loss:1.6013, Validation Accuracy:0.2397\n",
    "Epoch #123: Loss:1.5979, Accuracy:0.2439, Validation Loss:1.6004, Validation Accuracy:0.2578\n",
    "Epoch #124: Loss:1.5976, Accuracy:0.2456, Validation Loss:1.6001, Validation Accuracy:0.2479\n",
    "Epoch #125: Loss:1.5976, Accuracy:0.2431, Validation Loss:1.5999, Validation Accuracy:0.2447\n",
    "Epoch #126: Loss:1.5979, Accuracy:0.2431, Validation Loss:1.5995, Validation Accuracy:0.2365\n",
    "Epoch #127: Loss:1.5981, Accuracy:0.2394, Validation Loss:1.5999, Validation Accuracy:0.2578\n",
    "Epoch #128: Loss:1.5977, Accuracy:0.2402, Validation Loss:1.6003, Validation Accuracy:0.2496\n",
    "Epoch #129: Loss:1.5980, Accuracy:0.2423, Validation Loss:1.5999, Validation Accuracy:0.2594\n",
    "Epoch #130: Loss:1.5980, Accuracy:0.2353, Validation Loss:1.5997, Validation Accuracy:0.2479\n",
    "Epoch #131: Loss:1.5979, Accuracy:0.2394, Validation Loss:1.6006, Validation Accuracy:0.2430\n",
    "Epoch #132: Loss:1.5980, Accuracy:0.2378, Validation Loss:1.6001, Validation Accuracy:0.2529\n",
    "Epoch #133: Loss:1.5984, Accuracy:0.2378, Validation Loss:1.6002, Validation Accuracy:0.2562\n",
    "Epoch #134: Loss:1.5979, Accuracy:0.2423, Validation Loss:1.6011, Validation Accuracy:0.2332\n",
    "Epoch #135: Loss:1.5985, Accuracy:0.2444, Validation Loss:1.6005, Validation Accuracy:0.2479\n",
    "Epoch #136: Loss:1.5981, Accuracy:0.2394, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #137: Loss:1.5989, Accuracy:0.2333, Validation Loss:1.6006, Validation Accuracy:0.2414\n",
    "Epoch #138: Loss:1.5982, Accuracy:0.2423, Validation Loss:1.6007, Validation Accuracy:0.2414\n",
    "Epoch #139: Loss:1.5994, Accuracy:0.2386, Validation Loss:1.6000, Validation Accuracy:0.2365\n",
    "Epoch #140: Loss:1.5986, Accuracy:0.2407, Validation Loss:1.6002, Validation Accuracy:0.2529\n",
    "Epoch #141: Loss:1.5994, Accuracy:0.2341, Validation Loss:1.5996, Validation Accuracy:0.2447\n",
    "Epoch #142: Loss:1.5987, Accuracy:0.2431, Validation Loss:1.5991, Validation Accuracy:0.2447\n",
    "Epoch #143: Loss:1.5980, Accuracy:0.2444, Validation Loss:1.5996, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5971, Accuracy:0.2444, Validation Loss:1.6001, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5973, Accuracy:0.2468, Validation Loss:1.5998, Validation Accuracy:0.2282\n",
    "Epoch #146: Loss:1.5970, Accuracy:0.2435, Validation Loss:1.6012, Validation Accuracy:0.2250\n",
    "Epoch #147: Loss:1.5983, Accuracy:0.2378, Validation Loss:1.6053, Validation Accuracy:0.2233\n",
    "Epoch #148: Loss:1.5992, Accuracy:0.2345, Validation Loss:1.6017, Validation Accuracy:0.2315\n",
    "Epoch #149: Loss:1.5995, Accuracy:0.2427, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #150: Loss:1.5980, Accuracy:0.2357, Validation Loss:1.6018, Validation Accuracy:0.2167\n",
    "Epoch #151: Loss:1.5980, Accuracy:0.2411, Validation Loss:1.6020, Validation Accuracy:0.2299\n",
    "Epoch #152: Loss:1.5968, Accuracy:0.2402, Validation Loss:1.6013, Validation Accuracy:0.2184\n",
    "Epoch #153: Loss:1.5959, Accuracy:0.2444, Validation Loss:1.6020, Validation Accuracy:0.2233\n",
    "Epoch #154: Loss:1.5961, Accuracy:0.2431, Validation Loss:1.6011, Validation Accuracy:0.2217\n",
    "Epoch #155: Loss:1.5958, Accuracy:0.2390, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #156: Loss:1.5958, Accuracy:0.2439, Validation Loss:1.6002, Validation Accuracy:0.2414\n",
    "Epoch #157: Loss:1.5963, Accuracy:0.2480, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #158: Loss:1.5959, Accuracy:0.2448, Validation Loss:1.6003, Validation Accuracy:0.2299\n",
    "Epoch #159: Loss:1.5963, Accuracy:0.2427, Validation Loss:1.6005, Validation Accuracy:0.2332\n",
    "Epoch #160: Loss:1.5963, Accuracy:0.2415, Validation Loss:1.6003, Validation Accuracy:0.2200\n",
    "Epoch #161: Loss:1.5959, Accuracy:0.2419, Validation Loss:1.6004, Validation Accuracy:0.2315\n",
    "Epoch #162: Loss:1.5958, Accuracy:0.2452, Validation Loss:1.6004, Validation Accuracy:0.2315\n",
    "Epoch #163: Loss:1.5958, Accuracy:0.2489, Validation Loss:1.6005, Validation Accuracy:0.2430\n",
    "Epoch #164: Loss:1.5960, Accuracy:0.2456, Validation Loss:1.6000, Validation Accuracy:0.2299\n",
    "Epoch #165: Loss:1.5967, Accuracy:0.2472, Validation Loss:1.5992, Validation Accuracy:0.2529\n",
    "Epoch #166: Loss:1.5955, Accuracy:0.2419, Validation Loss:1.5983, Validation Accuracy:0.2365\n",
    "Epoch #167: Loss:1.5972, Accuracy:0.2448, Validation Loss:1.5996, Validation Accuracy:0.2447\n",
    "Epoch #168: Loss:1.5954, Accuracy:0.2571, Validation Loss:1.5993, Validation Accuracy:0.2463\n",
    "Epoch #169: Loss:1.5951, Accuracy:0.2480, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #170: Loss:1.5950, Accuracy:0.2485, Validation Loss:1.6006, Validation Accuracy:0.2233\n",
    "Epoch #171: Loss:1.5946, Accuracy:0.2554, Validation Loss:1.5987, Validation Accuracy:0.2414\n",
    "Epoch #172: Loss:1.5941, Accuracy:0.2583, Validation Loss:1.5991, Validation Accuracy:0.2282\n",
    "Epoch #173: Loss:1.5943, Accuracy:0.2534, Validation Loss:1.5993, Validation Accuracy:0.2217\n",
    "Epoch #174: Loss:1.5945, Accuracy:0.2530, Validation Loss:1.5995, Validation Accuracy:0.2332\n",
    "Epoch #175: Loss:1.5943, Accuracy:0.2583, Validation Loss:1.5998, Validation Accuracy:0.2348\n",
    "Epoch #176: Loss:1.5942, Accuracy:0.2571, Validation Loss:1.6011, Validation Accuracy:0.2315\n",
    "Epoch #177: Loss:1.5945, Accuracy:0.2620, Validation Loss:1.6008, Validation Accuracy:0.2217\n",
    "Epoch #178: Loss:1.5946, Accuracy:0.2538, Validation Loss:1.6007, Validation Accuracy:0.2217\n",
    "Epoch #179: Loss:1.5952, Accuracy:0.2583, Validation Loss:1.6030, Validation Accuracy:0.2200\n",
    "Epoch #180: Loss:1.5957, Accuracy:0.2522, Validation Loss:1.6026, Validation Accuracy:0.2266\n",
    "Epoch #181: Loss:1.5962, Accuracy:0.2476, Validation Loss:1.6037, Validation Accuracy:0.2381\n",
    "Epoch #182: Loss:1.6034, Accuracy:0.2259, Validation Loss:1.6042, Validation Accuracy:0.2151\n",
    "Epoch #183: Loss:1.6008, Accuracy:0.2407, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #184: Loss:1.6000, Accuracy:0.2357, Validation Loss:1.6027, Validation Accuracy:0.2562\n",
    "Epoch #185: Loss:1.5992, Accuracy:0.2345, Validation Loss:1.5984, Validation Accuracy:0.2332\n",
    "Epoch #186: Loss:1.5982, Accuracy:0.2374, Validation Loss:1.5980, Validation Accuracy:0.2381\n",
    "Epoch #187: Loss:1.5979, Accuracy:0.2431, Validation Loss:1.5978, Validation Accuracy:0.2315\n",
    "Epoch #188: Loss:1.5973, Accuracy:0.2513, Validation Loss:1.5985, Validation Accuracy:0.2479\n",
    "Epoch #189: Loss:1.5969, Accuracy:0.2480, Validation Loss:1.5986, Validation Accuracy:0.2496\n",
    "Epoch #190: Loss:1.5964, Accuracy:0.2472, Validation Loss:1.5982, Validation Accuracy:0.2496\n",
    "Epoch #191: Loss:1.5967, Accuracy:0.2439, Validation Loss:1.5988, Validation Accuracy:0.2430\n",
    "Epoch #192: Loss:1.5961, Accuracy:0.2472, Validation Loss:1.5986, Validation Accuracy:0.2463\n",
    "Epoch #193: Loss:1.5963, Accuracy:0.2411, Validation Loss:1.5991, Validation Accuracy:0.2348\n",
    "Epoch #194: Loss:1.5963, Accuracy:0.2468, Validation Loss:1.5988, Validation Accuracy:0.2250\n",
    "Epoch #195: Loss:1.5970, Accuracy:0.2522, Validation Loss:1.5985, Validation Accuracy:0.2282\n",
    "Epoch #196: Loss:1.5963, Accuracy:0.2464, Validation Loss:1.5990, Validation Accuracy:0.2430\n",
    "Epoch #197: Loss:1.5962, Accuracy:0.2460, Validation Loss:1.5991, Validation Accuracy:0.2332\n",
    "Epoch #198: Loss:1.5965, Accuracy:0.2501, Validation Loss:1.5993, Validation Accuracy:0.2348\n",
    "Epoch #199: Loss:1.5964, Accuracy:0.2513, Validation Loss:1.5993, Validation Accuracy:0.2332\n",
    "Epoch #200: Loss:1.5958, Accuracy:0.2567, Validation Loss:1.5990, Validation Accuracy:0.2332\n",
    "Epoch #201: Loss:1.5956, Accuracy:0.2509, Validation Loss:1.5994, Validation Accuracy:0.2282\n",
    "Epoch #202: Loss:1.5961, Accuracy:0.2493, Validation Loss:1.5997, Validation Accuracy:0.2332\n",
    "Epoch #203: Loss:1.5961, Accuracy:0.2460, Validation Loss:1.6002, Validation Accuracy:0.2315\n",
    "Epoch #204: Loss:1.5963, Accuracy:0.2431, Validation Loss:1.5998, Validation Accuracy:0.2332\n",
    "Epoch #205: Loss:1.5961, Accuracy:0.2513, Validation Loss:1.6008, Validation Accuracy:0.2299\n",
    "Epoch #206: Loss:1.5961, Accuracy:0.2534, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #207: Loss:1.5952, Accuracy:0.2526, Validation Loss:1.5995, Validation Accuracy:0.2348\n",
    "Epoch #208: Loss:1.5955, Accuracy:0.2472, Validation Loss:1.6003, Validation Accuracy:0.2233\n",
    "Epoch #209: Loss:1.5954, Accuracy:0.2559, Validation Loss:1.6009, Validation Accuracy:0.2315\n",
    "Epoch #210: Loss:1.5951, Accuracy:0.2517, Validation Loss:1.6007, Validation Accuracy:0.2348\n",
    "Epoch #211: Loss:1.5955, Accuracy:0.2513, Validation Loss:1.6010, Validation Accuracy:0.2282\n",
    "Epoch #212: Loss:1.5950, Accuracy:0.2530, Validation Loss:1.6011, Validation Accuracy:0.2348\n",
    "Epoch #213: Loss:1.5951, Accuracy:0.2559, Validation Loss:1.6012, Validation Accuracy:0.2315\n",
    "Epoch #214: Loss:1.5953, Accuracy:0.2517, Validation Loss:1.6003, Validation Accuracy:0.2348\n",
    "Epoch #215: Loss:1.5950, Accuracy:0.2522, Validation Loss:1.6003, Validation Accuracy:0.2348\n",
    "Epoch #216: Loss:1.5949, Accuracy:0.2534, Validation Loss:1.6003, Validation Accuracy:0.2348\n",
    "Epoch #217: Loss:1.5946, Accuracy:0.2522, Validation Loss:1.6003, Validation Accuracy:0.2348\n",
    "Epoch #218: Loss:1.5951, Accuracy:0.2559, Validation Loss:1.6004, Validation Accuracy:0.2348\n",
    "Epoch #219: Loss:1.5949, Accuracy:0.2559, Validation Loss:1.6005, Validation Accuracy:0.2348\n",
    "Epoch #220: Loss:1.5955, Accuracy:0.2505, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #221: Loss:1.5945, Accuracy:0.2452, Validation Loss:1.6006, Validation Accuracy:0.2348\n",
    "Epoch #222: Loss:1.5945, Accuracy:0.2542, Validation Loss:1.6010, Validation Accuracy:0.2315\n",
    "Epoch #223: Loss:1.5952, Accuracy:0.2501, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #224: Loss:1.5944, Accuracy:0.2542, Validation Loss:1.6009, Validation Accuracy:0.2348\n",
    "Epoch #225: Loss:1.5940, Accuracy:0.2513, Validation Loss:1.6012, Validation Accuracy:0.2315\n",
    "Epoch #226: Loss:1.5942, Accuracy:0.2517, Validation Loss:1.6011, Validation Accuracy:0.2315\n",
    "Epoch #227: Loss:1.5943, Accuracy:0.2522, Validation Loss:1.6012, Validation Accuracy:0.2348\n",
    "Epoch #228: Loss:1.5937, Accuracy:0.2600, Validation Loss:1.6013, Validation Accuracy:0.2348\n",
    "Epoch #229: Loss:1.5946, Accuracy:0.2534, Validation Loss:1.6015, Validation Accuracy:0.2315\n",
    "Epoch #230: Loss:1.5937, Accuracy:0.2546, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #231: Loss:1.5945, Accuracy:0.2571, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #232: Loss:1.5934, Accuracy:0.2513, Validation Loss:1.6021, Validation Accuracy:0.2348\n",
    "Epoch #233: Loss:1.5940, Accuracy:0.2522, Validation Loss:1.6016, Validation Accuracy:0.2266\n",
    "Epoch #234: Loss:1.5937, Accuracy:0.2534, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #235: Loss:1.5937, Accuracy:0.2571, Validation Loss:1.6015, Validation Accuracy:0.2315\n",
    "Epoch #236: Loss:1.5934, Accuracy:0.2534, Validation Loss:1.6018, Validation Accuracy:0.2299\n",
    "Epoch #237: Loss:1.5936, Accuracy:0.2497, Validation Loss:1.6019, Validation Accuracy:0.2299\n",
    "Epoch #238: Loss:1.5933, Accuracy:0.2497, Validation Loss:1.6020, Validation Accuracy:0.2332\n",
    "Epoch #239: Loss:1.5928, Accuracy:0.2517, Validation Loss:1.6022, Validation Accuracy:0.2299\n",
    "Epoch #240: Loss:1.5933, Accuracy:0.2505, Validation Loss:1.6027, Validation Accuracy:0.2299\n",
    "Epoch #241: Loss:1.5936, Accuracy:0.2522, Validation Loss:1.6027, Validation Accuracy:0.2299\n",
    "Epoch #242: Loss:1.5931, Accuracy:0.2522, Validation Loss:1.6026, Validation Accuracy:0.2299\n",
    "Epoch #243: Loss:1.5925, Accuracy:0.2509, Validation Loss:1.6024, Validation Accuracy:0.2299\n",
    "Epoch #244: Loss:1.5927, Accuracy:0.2517, Validation Loss:1.6027, Validation Accuracy:0.2332\n",
    "Epoch #245: Loss:1.5928, Accuracy:0.2554, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #246: Loss:1.5925, Accuracy:0.2464, Validation Loss:1.6027, Validation Accuracy:0.2365\n",
    "Epoch #247: Loss:1.5924, Accuracy:0.2579, Validation Loss:1.6030, Validation Accuracy:0.2299\n",
    "Epoch #248: Loss:1.5930, Accuracy:0.2513, Validation Loss:1.6031, Validation Accuracy:0.2299\n",
    "Epoch #249: Loss:1.5924, Accuracy:0.2546, Validation Loss:1.6034, Validation Accuracy:0.2299\n",
    "Epoch #250: Loss:1.5926, Accuracy:0.2534, Validation Loss:1.6035, Validation Accuracy:0.2299\n",
    "Epoch #251: Loss:1.5921, Accuracy:0.2530, Validation Loss:1.6040, Validation Accuracy:0.2299\n",
    "Epoch #252: Loss:1.5921, Accuracy:0.2522, Validation Loss:1.6041, Validation Accuracy:0.2381\n",
    "Epoch #253: Loss:1.5921, Accuracy:0.2579, Validation Loss:1.6035, Validation Accuracy:0.2299\n",
    "Epoch #254: Loss:1.5913, Accuracy:0.2595, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #255: Loss:1.5927, Accuracy:0.2554, Validation Loss:1.6042, Validation Accuracy:0.2299\n",
    "Epoch #256: Loss:1.5910, Accuracy:0.2509, Validation Loss:1.6038, Validation Accuracy:0.2299\n",
    "Epoch #257: Loss:1.5909, Accuracy:0.2530, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #258: Loss:1.5912, Accuracy:0.2575, Validation Loss:1.6040, Validation Accuracy:0.2299\n",
    "Epoch #259: Loss:1.5923, Accuracy:0.2583, Validation Loss:1.6046, Validation Accuracy:0.2299\n",
    "Epoch #260: Loss:1.5931, Accuracy:0.2571, Validation Loss:1.6050, Validation Accuracy:0.2315\n",
    "Epoch #261: Loss:1.5922, Accuracy:0.2505, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #262: Loss:1.5913, Accuracy:0.2563, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #263: Loss:1.5913, Accuracy:0.2554, Validation Loss:1.6041, Validation Accuracy:0.2299\n",
    "Epoch #264: Loss:1.5904, Accuracy:0.2538, Validation Loss:1.6049, Validation Accuracy:0.2299\n",
    "Epoch #265: Loss:1.5903, Accuracy:0.2554, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #266: Loss:1.5902, Accuracy:0.2575, Validation Loss:1.6048, Validation Accuracy:0.2299\n",
    "Epoch #267: Loss:1.5899, Accuracy:0.2571, Validation Loss:1.6048, Validation Accuracy:0.2299\n",
    "Epoch #268: Loss:1.5908, Accuracy:0.2579, Validation Loss:1.6047, Validation Accuracy:0.2299\n",
    "Epoch #269: Loss:1.5906, Accuracy:0.2591, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #270: Loss:1.5888, Accuracy:0.2641, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #271: Loss:1.5902, Accuracy:0.2575, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #272: Loss:1.5886, Accuracy:0.2583, Validation Loss:1.6065, Validation Accuracy:0.2282\n",
    "Epoch #273: Loss:1.5893, Accuracy:0.2579, Validation Loss:1.6059, Validation Accuracy:0.2299\n",
    "Epoch #274: Loss:1.5890, Accuracy:0.2591, Validation Loss:1.6052, Validation Accuracy:0.2299\n",
    "Epoch #275: Loss:1.5889, Accuracy:0.2579, Validation Loss:1.6060, Validation Accuracy:0.2282\n",
    "Epoch #276: Loss:1.5892, Accuracy:0.2554, Validation Loss:1.6066, Validation Accuracy:0.2250\n",
    "Epoch #277: Loss:1.5884, Accuracy:0.2554, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #278: Loss:1.5888, Accuracy:0.2538, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #279: Loss:1.5894, Accuracy:0.2509, Validation Loss:1.6067, Validation Accuracy:0.2282\n",
    "Epoch #280: Loss:1.5897, Accuracy:0.2542, Validation Loss:1.6069, Validation Accuracy:0.2282\n",
    "Epoch #281: Loss:1.5896, Accuracy:0.2534, Validation Loss:1.6055, Validation Accuracy:0.2282\n",
    "Epoch #282: Loss:1.5890, Accuracy:0.2571, Validation Loss:1.6068, Validation Accuracy:0.2282\n",
    "Epoch #283: Loss:1.5891, Accuracy:0.2575, Validation Loss:1.6066, Validation Accuracy:0.2282\n",
    "Epoch #284: Loss:1.5893, Accuracy:0.2624, Validation Loss:1.6081, Validation Accuracy:0.2282\n",
    "Epoch #285: Loss:1.5887, Accuracy:0.2591, Validation Loss:1.6072, Validation Accuracy:0.2250\n",
    "Epoch #286: Loss:1.5893, Accuracy:0.2571, Validation Loss:1.6072, Validation Accuracy:0.2282\n",
    "Epoch #287: Loss:1.5882, Accuracy:0.2575, Validation Loss:1.6068, Validation Accuracy:0.2299\n",
    "Epoch #288: Loss:1.5879, Accuracy:0.2620, Validation Loss:1.6071, Validation Accuracy:0.2315\n",
    "Epoch #289: Loss:1.5875, Accuracy:0.2604, Validation Loss:1.6064, Validation Accuracy:0.2282\n",
    "Epoch #290: Loss:1.5881, Accuracy:0.2587, Validation Loss:1.6060, Validation Accuracy:0.2282\n",
    "Epoch #291: Loss:1.5883, Accuracy:0.2669, Validation Loss:1.6072, Validation Accuracy:0.2299\n",
    "Epoch #292: Loss:1.5877, Accuracy:0.2575, Validation Loss:1.6082, Validation Accuracy:0.2348\n",
    "Epoch #293: Loss:1.5875, Accuracy:0.2579, Validation Loss:1.6062, Validation Accuracy:0.2299\n",
    "Epoch #294: Loss:1.5871, Accuracy:0.2624, Validation Loss:1.6064, Validation Accuracy:0.2299\n",
    "Epoch #295: Loss:1.5871, Accuracy:0.2579, Validation Loss:1.6071, Validation Accuracy:0.2299\n",
    "Epoch #296: Loss:1.5870, Accuracy:0.2595, Validation Loss:1.6074, Validation Accuracy:0.2299\n",
    "Epoch #297: Loss:1.5870, Accuracy:0.2563, Validation Loss:1.6072, Validation Accuracy:0.2299\n",
    "Epoch #298: Loss:1.5867, Accuracy:0.2616, Validation Loss:1.6073, Validation Accuracy:0.2299\n",
    "Epoch #299: Loss:1.5884, Accuracy:0.2595, Validation Loss:1.6080, Validation Accuracy:0.2299\n",
    "Epoch #300: Loss:1.5889, Accuracy:0.2591, Validation Loss:1.6072, Validation Accuracy:0.2299\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60723615, Accuracy:0.2299\n",
    "Labels: ['03', '04', '01', '02', '05']\n",
    "Confusion Matrix:\n",
    "      03  04  01  02  05\n",
    "t:03  25  10  21   2  57\n",
    "t:04  25  16  23   0  48\n",
    "t:01  27  14  27   0  58\n",
    "t:02  17  21  24   0  52\n",
    "t:05  24  17  29   0  72\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.21      0.22      0.21       115\n",
    "          04       0.21      0.14      0.17       112\n",
    "          01       0.22      0.21      0.22       126\n",
    "          02       0.00      0.00      0.00       114\n",
    "          05       0.25      0.51      0.34       142\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.18      0.22      0.19       609\n",
    "weighted avg       0.18      0.23      0.19       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 08:31:41 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6078769052752917, 1.6057732543726078, 1.605346445771078, 1.6054681905580468, 1.6053262612306818, 1.604839953295703, 1.604866447707115, 1.6046410628727503, 1.6044590901858702, 1.6042388241083556, 1.604293854953033, 1.6032735295287885, 1.6043293726659564, 1.6043612751467475, 1.6039636978766405, 1.6037027835845947, 1.603522260983785, 1.6034298375713805, 1.602923655549098, 1.6038478820390498, 1.603778805638769, 1.6037228545923343, 1.6033542471370479, 1.6027002675192696, 1.6027523173487246, 1.6032130960956192, 1.6025356488862061, 1.6023826186097119, 1.6015535784863877, 1.6017580016688957, 1.6022876757510582, 1.6020527144370995, 1.6024866174594523, 1.6027158371529164, 1.6020361666608913, 1.6020206404828476, 1.6019937357879037, 1.6020452101242366, 1.601721230986083, 1.6015706447936435, 1.6015292772127099, 1.6020525670403918, 1.6019260474222243, 1.6017846720559257, 1.6017682548422727, 1.6013710242382606, 1.601303395966591, 1.601275213246275, 1.6011705981882531, 1.6014363288096412, 1.6012452261397012, 1.601284609835332, 1.6011391864425835, 1.601080113052343, 1.601562017290463, 1.6011623563046133, 1.6008953427642045, 1.6006574687503634, 1.6008002540748107, 1.6009858709642257, 1.6007768657602897, 1.6004959143245554, 1.6010346506616753, 1.60111964179573, 1.6008860755632273, 1.6005382602437963, 1.6004960429296509, 1.6004946400183566, 1.601059576952203, 1.6007876282646543, 1.6008983363071685, 1.601516882578532, 1.601410730914725, 1.6017258061564028, 1.60159888369305, 1.601801175397801, 1.6022019906975757, 1.6020547856251006, 1.6023146115696096, 1.6017936329144757, 1.602815368296869, 1.6025278321627914, 1.602329676961664, 1.6020996085137178, 1.602255867032582, 1.6023465829529786, 1.6022088314316347, 1.6018965136632934, 1.602692899445595, 1.6018803881111208, 1.6024800284546976, 1.602606358199284, 1.6027936299250436, 1.6018688708103348, 1.6011718096599985, 1.6005794397128628, 1.600944302939429, 1.6024985321245366, 1.6024372160728342, 1.6009129360195842, 1.5999737077550151, 1.600554249165289, 1.601482015329433, 1.6021655396679155, 1.6021060606920465, 1.6016588350039203, 1.601252282194316, 1.6011569204393084, 1.6023558824520392, 1.603893079389688, 1.6011640968776883, 1.6011479540998712, 1.6002440493682335, 1.600150289793907, 1.6012926236749283, 1.6011000546720031, 1.6011255496045442, 1.6001999589609983, 1.6003453721749568, 1.6011278588196327, 1.6012103179796968, 1.6012944080950984, 1.600380202819561, 1.6000629978618404, 1.599946278069407, 1.5994596866942783, 1.599868899104239, 1.6002649176492676, 1.5998965625105233, 1.5996569819834041, 1.6006416372086223, 1.6001109554262585, 1.6001723701339245, 1.601138891453422, 1.6005070434611028, 1.6007848197016223, 1.600629893038269, 1.600654115426325, 1.600007605474375, 1.600168068420711, 1.5996290504051547, 1.5991363047770484, 1.5996369898612863, 1.600098387752652, 1.5998449957820973, 1.6012114932384398, 1.605264302936485, 1.6016910532229445, 1.6043685099174236, 1.6017983646815634, 1.6020102859130634, 1.6013051292970655, 1.6019876122670416, 1.6010975653706316, 1.6006072017751107, 1.6002404063401747, 1.6004570276279169, 1.6002911795144794, 1.6005243337017365, 1.6002524290570288, 1.600442399336591, 1.600376127584423, 1.6004686234228325, 1.6000262193491894, 1.5992394548723068, 1.5983412706205997, 1.5995555795080751, 1.5993211929042548, 1.599940577555564, 1.600625143458299, 1.5986537763050623, 1.5991069368149455, 1.5992565926268378, 1.5995475333703955, 1.5998207115383178, 1.6011200620623058, 1.600781841622589, 1.600681703274669, 1.6029982376959915, 1.6025912511133404, 1.6036908843834412, 1.6041562148111403, 1.59993209412141, 1.602680605816332, 1.598383995699765, 1.598043534947538, 1.5977974460629993, 1.598450140412805, 1.5985891971682094, 1.5982037923606158, 1.5987696138704548, 1.5986400466834383, 1.5991409406286154, 1.598768032830337, 1.598516374386003, 1.59896599443871, 1.5990821516572549, 1.5993453689005184, 1.5992996661338117, 1.5989885191220563, 1.5993741273097022, 1.5996561257905757, 1.6001842718797756, 1.599757575049189, 1.6007959441402666, 1.6004443538600002, 1.599450789844657, 1.6002630298752307, 1.600947209571187, 1.6006745276192726, 1.6009917016491317, 1.601063146575527, 1.6011793957946727, 1.6003152648803636, 1.6003218268721757, 1.6002712416139926, 1.6003209951475923, 1.6004181884975464, 1.6004792320708727, 1.6006790229252406, 1.600572826435609, 1.6009586207776625, 1.6007894377403071, 1.6009132648727968, 1.6011719406140457, 1.601104604591094, 1.6012216503005505, 1.601347326057885, 1.6015048723894192, 1.6018407593415485, 1.6016629797288742, 1.6021228472783255, 1.6016488958070627, 1.6017659313377293, 1.6014604983462881, 1.6018334783748258, 1.601926950789829, 1.6019996234348841, 1.602196158446702, 1.6027000431943996, 1.602685921102126, 1.6025570069236317, 1.6024435776207835, 1.6027287278073565, 1.6026483592141438, 1.602724006219059, 1.603037573238116, 1.6030723112948815, 1.6034001742286244, 1.6035093799208968, 1.6040219933724364, 1.6040773888918372, 1.603458515137483, 1.6034319864705278, 1.6041922236506772, 1.6038042046557899, 1.604391566246797, 1.6039806559363805, 1.6045990007851512, 1.6050484928200006, 1.6048715059784637, 1.6057481215700924, 1.6041207133451314, 1.604875259015752, 1.6057273793494564, 1.6048424580610052, 1.6047526562742411, 1.6047309388472333, 1.6057121473775904, 1.605690535066163, 1.605514383081145, 1.6064830133676138, 1.6059085021073791, 1.6052333253553543, 1.606035897492971, 1.6065702825931494, 1.6052145082962337, 1.6057252952422219, 1.606699746034807, 1.6068790191891549, 1.6054958933092691, 1.6067728914063553, 1.60657091289514, 1.6081305476049288, 1.6071506699513527, 1.6072024817537205, 1.6067563992219998, 1.6070768130432405, 1.6064024464837436, 1.6060419961540962, 1.607197857451165, 1.6082373867285467, 1.606220416247551, 1.6063954683360209, 1.6070627884324549, 1.607365418537497, 1.6071831886404253, 1.607294762271574, 1.607966256454856, 1.6072360353516828], 'val_acc': [0.2364532014566102, 0.2200328399150438, 0.23809523758140494, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.23316912910914772, 0.238095237385659, 0.23973727380407267, 0.23481116503819652, 0.23481116503819652, 0.23645320116299126, 0.23481116503819652, 0.24466338188483797, 0.23316912901127476, 0.23316912891340177, 0.23645320116299126, 0.23973727341258075, 0.2463054181075057, 0.24466338198271095, 0.23645320116299126, 0.24466338198271095, 0.24466338198271095, 0.23973727360832672, 0.23316912901127476, 0.23973727341258075, 0.24958949035709518, 0.23481116503819652, 0.23152709269073404, 0.23973727341258075, 0.24794745423230044, 0.23973727341258075, 0.23973727341258075, 0.23973727341258075, 0.24958949035709518, 0.24958949035709518, 0.23973727341258075, 0.23973727341258075, 0.24958949035709518, 0.23973727341258075, 0.23973727341258075, 0.23973727341258075, 0.2463054181075057, 0.2463054181075057, 0.23973727341258075, 0.23973727341258075, 0.2528735626066847, 0.2463054180096327, 0.24302134576004322, 0.23973727341258075, 0.25123152648188996, 0.23973727341258075, 0.2430213458579162, 0.23973727351045374, 0.23973727341258075, 0.24794745423230044, 0.23809523748353198, 0.23973727341258075, 0.23973727341258075, 0.25123152648188996, 0.24958949035709518, 0.23973727341258075, 0.24958949035709518, 0.25123152648188996, 0.24958949035709518, 0.23973727341258075, 0.24794745423230044, 0.24794745423230044, 0.23973727341258075, 0.25451559873147944, 0.2413793095373755, 0.2463054181075057, 0.24958949035709518, 0.24466338208058394, 0.23809523767927793, 0.23152709298435298, 0.24794745433017343, 0.24794745433017343, 0.2413793099288674, 0.2512315266776359, 0.24958949035709518, 0.24958949035709518, 0.24794745452591938, 0.2348111652339425, 0.24794745452591938, 0.2348111651360695, 0.2331691292070207, 0.2233169121646333, 0.23316912910914772, 0.24466338208058394, 0.24466338198271095, 0.23645320126086425, 0.24958949035709518, 0.23645320135873724, 0.23973727370619968, 0.24794745442804642, 0.24794745423230044, 0.24630541820537868, 0.23645320126086425, 0.24466338198271095, 0.24794745413442745, 0.26272577935545316, 0.2528735626066847, 0.24958949055284116, 0.2298850566638123, 0.23973727360832672, 0.25451559873147944, 0.2528735626066847, 0.238095237287786, 0.23809523718991302, 0.2348111652339425, 0.2561576346605282, 0.2528735625088117, 0.238095237287786, 0.23645320126086425, 0.23645320126086425, 0.23973727370619968, 0.25779967078532295, 0.24794745413442745, 0.24466338188483797, 0.23645320106511827, 0.2577996710789419, 0.24958949016134924, 0.2594417071058637, 0.2479474540365545, 0.24302134566217024, 0.2528735626066847, 0.2561576349541472, 0.23316912910914772, 0.24794745442804642, 0.23973727341258075, 0.2413793099288674, 0.2413793095373755, 0.23645320116299126, 0.2528735626066847, 0.24466338168909202, 0.24466338168909202, 0.2298850565659393, 0.23973727351045374, 0.22824302044114456, 0.22495894819155507, 0.22331691187101435, 0.23152709288648002, 0.23152709259286106, 0.21674876737183538, 0.2298850564680663, 0.2183908036923761, 0.2233169121646333, 0.22167487603983857, 0.2348111651360695, 0.24137930973312147, 0.23809523748353198, 0.2298850565659393, 0.2331691288155288, 0.2200328399150438, 0.23152709288648002, 0.23152709288648002, 0.24302134576004322, 0.2298850565659393, 0.2528735628024306, 0.23645320126086425, 0.24466338198271095, 0.2463054180096327, 0.23809523748353198, 0.2233169121646333, 0.2413793100267404, 0.22824302063689053, 0.22167487613771153, 0.2331691293048937, 0.23481116552756143, 0.23152709318009895, 0.22167487613771153, 0.22167487623558452, 0.22003284001291679, 0.22660098451209576, 0.23809523767927793, 0.21510673163853256, 0.24137930983099445, 0.2561576350520201, 0.23316912891340177, 0.238095237287786, 0.23152709269073404, 0.24794745433017343, 0.24958949045496817, 0.24958949045496817, 0.2430213458579162, 0.24630541820537868, 0.23481116552756143, 0.224958948485174, 0.22824302093050947, 0.2430213459557892, 0.23316912901127476, 0.23481116503819652, 0.23316912940276668, 0.23316912901127476, 0.22824302063689053, 0.23316912901127476, 0.23152709308222597, 0.2331691288155288, 0.2298850566638123, 0.23316912891340177, 0.2348111651360695, 0.22331691245825225, 0.23152709298435298, 0.2348111651360695, 0.22824302044114456, 0.2348111651360695, 0.23152709298435298, 0.2348111651360695, 0.2348111651360695, 0.2348111651360695, 0.2348111651360695, 0.23481116533181545, 0.2348111651360695, 0.24466338188483797, 0.2348111651360695, 0.23152709288648002, 0.2348111651360695, 0.2348111651360695, 0.23152709288648002, 0.23152709288648002, 0.23481116533181545, 0.2348111651360695, 0.23152709288648002, 0.2331691292070207, 0.23316912901127476, 0.2348111651360695, 0.2266009843163498, 0.2331691292070207, 0.23152709288648002, 0.22988505676168527, 0.22988505676168527, 0.2331691292070207, 0.22988505676168527, 0.22988505676168527, 0.22988505676168527, 0.22988505676168527, 0.22988505676168527, 0.2331691292070207, 0.23316912901127476, 0.2364532014566102, 0.22988505676168527, 0.22988505676168527, 0.22988505695743122, 0.22988505676168527, 0.22988505676168527, 0.23809523758140494, 0.22988505676168527, 0.2331691292070207, 0.22988505676168527, 0.22988505676168527, 0.2331691292070207, 0.22988505695743122, 0.22988505695743122, 0.23152709288648002, 0.2331691292070207, 0.2331691293048937, 0.22988505676168527, 0.22988505695743122, 0.22988505695743122, 0.22988505695743122, 0.22988505695743122, 0.22988505695743122, 0.2298850570553042, 0.22988505695743122, 0.22824302083263648, 0.22824302093050947, 0.22988505695743122, 0.22988505695743122, 0.22824302083263648, 0.22495894868091998, 0.22824302093050947, 0.2331691292070207, 0.22824302093050947, 0.22824302083263648, 0.22824302083263648, 0.22824302083263648, 0.22824302093050947, 0.2282430207347635, 0.2249589489745389, 0.22824302083263648, 0.2298850570553042, 0.23152709308222597, 0.22824302083263648, 0.22824302083263648, 0.2298850570553042, 0.23481116542968844, 0.2298850570553042, 0.22988505695743122, 0.2298850570553042, 0.22988505695743122, 0.22988505695743122, 0.2298850570553042, 0.22988505695743122, 0.2298850570553042], 'loss': [1.6111308400880628, 1.6061518387628042, 1.6059011843170228, 1.6055680808100612, 1.6054033794931808, 1.6046546162031514, 1.6044738091972084, 1.6041451700903797, 1.6039044774288513, 1.603765662837567, 1.6035041888391703, 1.6033686059701124, 1.6029727850116988, 1.6033708989987383, 1.6034592205983658, 1.6029067061030644, 1.602923128100636, 1.6025234960677444, 1.6023879866825237, 1.6022028050138721, 1.6024748986751391, 1.6027412467423896, 1.6026986536304075, 1.6023451985764552, 1.602282320253658, 1.6022723692398542, 1.6022179227590072, 1.6021221742492926, 1.6016130801833386, 1.6011976607281568, 1.601066532174175, 1.6015899057995366, 1.6013067031543113, 1.6018462493434333, 1.6013829584728765, 1.601147486541795, 1.6010521184737188, 1.6010094254413425, 1.6009844393211223, 1.600929771997111, 1.6009620287579922, 1.6006670460808694, 1.6007163713110546, 1.6006906337072226, 1.6005186825072741, 1.6004207115153757, 1.6003174372522249, 1.6003003541938579, 1.600147531409528, 1.5999270292278187, 1.5996903080715046, 1.6000623339745053, 1.5997478716672078, 1.59962662842729, 1.5993640750340612, 1.5994159992225851, 1.5995377592971927, 1.5993544571698324, 1.5992823670287397, 1.598859460299999, 1.5992252365030057, 1.5992534199779282, 1.5984262716109259, 1.598886194170378, 1.598729473658411, 1.5988787516431398, 1.5986185060634261, 1.59875339614782, 1.598569191016211, 1.5985992538365985, 1.5984186578335458, 1.5984886268815466, 1.5982908813860381, 1.5979627467278827, 1.5977620723555954, 1.5984783807574356, 1.5974685553897332, 1.5983536745243738, 1.5978360481575529, 1.5973691030694228, 1.5986205834382858, 1.5975580896440227, 1.5979506253706601, 1.5971944139723415, 1.5975776268471438, 1.5975027983193524, 1.597496574517393, 1.5969466081634929, 1.5984650540400824, 1.596864022709261, 1.5968908354487017, 1.5978678625956697, 1.5972833972202434, 1.5972194125275347, 1.5972044302452761, 1.5975120528767486, 1.5980608903896638, 1.597327757811889, 1.596801276911945, 1.5968235312546057, 1.5969177536406312, 1.5978519698188045, 1.5983082539736613, 1.59840380920277, 1.5980615336302615, 1.598255945329059, 1.5974735007393777, 1.597865001278981, 1.600106577657821, 1.5990944251387516, 1.6002994770876435, 1.5977896342287319, 1.5984790043664419, 1.5977989733341538, 1.5979686764476235, 1.598323609843636, 1.5982533390272324, 1.5977360179536886, 1.598010206369404, 1.5981521069391553, 1.598027529412961, 1.5979562455868574, 1.5978832376566265, 1.5976385057829243, 1.5975765315903776, 1.5978768629704658, 1.5980910707058602, 1.5976910456494873, 1.5980157203008507, 1.5979820098230726, 1.5979057108841883, 1.5979989226104299, 1.5983776240867755, 1.5979096839070566, 1.5984636148143354, 1.598092788005022, 1.598903607440925, 1.5982219808645073, 1.5994370314130057, 1.598647508288311, 1.5993560651978918, 1.5987336334996154, 1.59799638592487, 1.5970577295311177, 1.597273729371339, 1.5970412017383615, 1.598295883965933, 1.5992422409860505, 1.599519111635259, 1.5980158395590969, 1.5979572724512716, 1.5968187037924233, 1.5959480452096928, 1.5960514457808384, 1.5957692210923964, 1.5958407380497677, 1.5963493660490127, 1.5959176466939875, 1.5962679851226493, 1.5963163820869868, 1.5959244527366372, 1.5958161479393804, 1.5958046075989334, 1.596045979826847, 1.596741908531659, 1.595543887385108, 1.5971637238712038, 1.5954022830027086, 1.5950547917422817, 1.5950392393605666, 1.5945709475256824, 1.5941263630649638, 1.5942517590963377, 1.5945210066664146, 1.59433852324006, 1.5942226827022232, 1.5945009758829827, 1.5946188311312477, 1.5951539904185144, 1.5957476683955418, 1.5961690751923672, 1.6033568620681762, 1.6007541381847687, 1.5999627554441134, 1.5991977495334477, 1.5981662673871864, 1.5978618229194343, 1.5972997463214569, 1.596878206558541, 1.5964491993984402, 1.5967091132483198, 1.596141666357522, 1.5962693680483213, 1.5963286848283647, 1.596961723558712, 1.5962712743688658, 1.596186235944838, 1.5964752230556105, 1.5964362670753527, 1.5957742544659843, 1.5955638356766906, 1.5961387717013975, 1.5961445034896569, 1.5962691319307019, 1.5961498635015938, 1.5961469696287747, 1.5951847948332831, 1.5954615450003309, 1.5954132186313918, 1.5950735646841219, 1.5954969073712704, 1.595033625459769, 1.595081537068502, 1.5952670511034235, 1.5950371846526066, 1.5949497410404119, 1.594648891503806, 1.5951080593974682, 1.594885476805591, 1.5954573484906425, 1.5945428594181914, 1.5944736502743355, 1.5951988042991998, 1.5943826835013513, 1.5939875045596208, 1.5942183747673426, 1.594331004536372, 1.5937364712388118, 1.594565909697045, 1.593676607711604, 1.594537309895306, 1.593355482559674, 1.5940131621683893, 1.593656246862862, 1.5936848257111818, 1.5933908667652514, 1.593572141551384, 1.5932998729192747, 1.5928316202007036, 1.59334409422943, 1.5936433767146398, 1.5931362940300662, 1.5924852051529306, 1.5927353218595595, 1.5927508467766294, 1.5924609175942517, 1.5924105195294171, 1.593032475369667, 1.5924402709859107, 1.592571558257148, 1.5921052075019853, 1.5920703513421561, 1.592083385984511, 1.591347951272185, 1.592732105411788, 1.5910140889870803, 1.5909473446605142, 1.5911879445982664, 1.5922979070910193, 1.5931110918644273, 1.5921661149794561, 1.5912713139943273, 1.59134701923668, 1.5904174804197935, 1.5903459206254085, 1.5902316430266143, 1.5899064859080854, 1.5907921117923587, 1.5905986813793926, 1.588819102878688, 1.5901523756540286, 1.5886112402596757, 1.5892743109188041, 1.5890293824844046, 1.5889414635037495, 1.589172038552208, 1.5884271336776765, 1.5888491847921447, 1.5893574556530867, 1.5896568263091102, 1.5895901506196792, 1.5890348117699125, 1.5891232407802918, 1.5892799733355794, 1.5886712822336435, 1.589289749035845, 1.5882319667745666, 1.5879286292152484, 1.5875339584918482, 1.5881197929382325, 1.5883431709767366, 1.5876533512706874, 1.58746089357615, 1.58707758714531, 1.5871116262197005, 1.5869826256616895, 1.586961890686709, 1.5866653761090195, 1.5884065246190378, 1.5888682363459217], 'acc': [0.19917864424369663, 0.23285421008202084, 0.22505133574870578, 0.2344969197709947, 0.23285421029620593, 0.2328542091028891, 0.23285421051039099, 0.23285421008202084, 0.23285420851541005, 0.2328542087112364, 0.23285420912124782, 0.23285421049203225, 0.2340862417612722, 0.23983572861007596, 0.24065708386457432, 0.24024640544484038, 0.2406570850211737, 0.23983572921591373, 0.24517453921159435, 0.24065708365038924, 0.23942505120619123, 0.24024640544484038, 0.24106776205176444, 0.24599589229364416, 0.24353182756435698, 0.2443531818397236, 0.24106776126845905, 0.24476386063275152, 0.2451745397990734, 0.2422997941226685, 0.2418891166637076, 0.23983572902008737, 0.24312115014211352, 0.2439425051824268, 0.24188911787538314, 0.2439425059657322, 0.24024640644233083, 0.2398357284142496, 0.23983572978503406, 0.24188911709207775, 0.2435318263893989, 0.24106776185593812, 0.23983572941174008, 0.24106776244341716, 0.24065708386457432, 0.2410677626576022, 0.24106776185593812, 0.24188911670042504, 0.24517453842828896, 0.24024640603231942, 0.2402464066381572, 0.24229979472850627, 0.2422997945326799, 0.24271047332570783, 0.24188911591711965, 0.24147843767485336, 0.24229979373101582, 0.24394250500495918, 0.24353182738688936, 0.2451745386241153, 0.24312115092541892, 0.24188911789374185, 0.24394250559243824, 0.2406570850211737, 0.24147843966983426, 0.24435318280049662, 0.2427104736990018, 0.2418891180895682, 0.24353182797436843, 0.242710471544912, 0.24271047312988148, 0.24271047234657608, 0.24435318242720266, 0.2439425051824268, 0.24188911709207775, 0.24147844045313965, 0.2402464058364931, 0.24681724792143647, 0.2406570850211737, 0.24229979431849485, 0.2447638614344156, 0.24969199124792518, 0.2406570850395324, 0.24312114955463449, 0.24887063679509094, 0.2439425053966119, 0.24599589427026636, 0.24599589346860223, 0.2381930173544913, 0.23696098647690406, 0.24722792730194343, 0.2505133480690343, 0.24188911632713106, 0.2505133468757175, 0.24599589386025494, 0.24229979510180025, 0.2427104719549234, 0.2472279255395063, 0.24640656971588762, 0.2542094464174776, 0.24312114935880813, 0.24271047234657608, 0.2451745401907261, 0.24435318417128107, 0.24024640603231942, 0.24928131558811886, 0.23983572782677057, 0.23778234189051134, 0.2439425071406903, 0.24517453842828896, 0.2361396306532854, 0.24599589327277588, 0.24024640742146258, 0.2357289516460724, 0.2435318271727043, 0.24188911810792693, 0.24065708521700002, 0.2357289516460724, 0.24024640605067815, 0.2451745374491572, 0.24599589346860223, 0.24435318299632297, 0.2439425059657322, 0.24558521604635877, 0.24312114976881954, 0.24312115094377765, 0.23942505179367027, 0.24024640642397213, 0.2422997953159853, 0.23531827422382895, 0.23942505216696425, 0.237782341321391, 0.23778234034225926, 0.24229979551181166, 0.24435318280049662, 0.23942505099200614, 0.23326488732679668, 0.24229979510180025, 0.238603696753357, 0.240657084256227, 0.234086243327883, 0.24312114755965356, 0.2443531818213649, 0.244353182623029, 0.2468172465139346, 0.24353182756435698, 0.2377823417130437, 0.23449691900604805, 0.24271047389482817, 0.2357289536226946, 0.24106776185593812, 0.2402464058364931, 0.2443531815888211, 0.2431211499462872, 0.23901437376558904, 0.2439425053966119, 0.24804928077564592, 0.24476385924360836, 0.24271047350317546, 0.24147843988401935, 0.24188911670042504, 0.24517453825082133, 0.24887063679509094, 0.24558521506722703, 0.24722792653699674, 0.24188911789374185, 0.24476386082857787, 0.2570841887648346, 0.2480492835172148, 0.24845995958703254, 0.2554414768850534, 0.2583162220106967, 0.253388090808044, 0.2529774127799628, 0.2583162212273913, 0.2570841877857028, 0.2620123181866914, 0.25379876782027605, 0.2583162196240631, 0.25215605814966086, 0.24763860413670785, 0.2258726898098873, 0.24065708462952098, 0.23572895203772512, 0.23449692114177914, 0.23737166211835167, 0.2431211499462872, 0.2513347015243781, 0.24804928077564592, 0.2472279237403517, 0.24394250500495918, 0.24722792653699674, 0.24106776148264414, 0.24681724890056822, 0.2521560573479968, 0.2464065703217254, 0.24599589346860223, 0.25010266745849313, 0.251334701096008, 0.25667351193007015, 0.2509240244937873, 0.24928131441316076, 0.2459958936644286, 0.2431211503562986, 0.2513347028951625, 0.253388091768817, 0.2525667349660666, 0.24722792614534406, 0.2558521559106251, 0.25174538013993836, 0.2513347031093476, 0.2529774133858005, 0.2558521559106251, 0.2517453787507952, 0.25215605715217043, 0.2533880905938589, 0.2521560579354758, 0.25585215512731974, 0.2558521566939305, 0.2505133468757175, 0.24517453903412673, 0.25420944624000996, 0.25010267141173753, 0.2542094450466931, 0.25133470252186857, 0.2517453789466216, 0.25215605715217043, 0.2599589326788023, 0.2533880913771643, 0.2546201244272001, 0.2570841893523136, 0.25133469992104984, 0.2521560573479968, 0.25338809235629606, 0.25708419033144536, 0.25338808941890084, 0.24969199163957786, 0.24969199144375154, 0.25174538053159107, 0.2505133466982499, 0.252156056368865, 0.2521560591104339, 0.2509240250812664, 0.25174538012157965, 0.25544147927168703, 0.24640656870003844, 0.25790554380514785, 0.25133470328681523, 0.2546201240355474, 0.25338808941890084, 0.2529774129757891, 0.25215605813130215, 0.25790554595923765, 0.2595482564315169, 0.25544148064247146, 0.2509240250812664, 0.2529774121924837, 0.2574948671662098, 0.25831622142321764, 0.25708418837318187, 0.25051334785484924, 0.25626283133788763, 0.2554414798591661, 0.2537987686035814, 0.2554414794858721, 0.2574948685369942, 0.257084190723098, 0.2579055449801059, 0.25913757920509983, 0.26406570925115314, 0.25749486540377264, 0.25831622181487035, 0.25790554239764596, 0.25913757724683634, 0.2579055436093215, 0.2554414798591661, 0.25544147966333974, 0.25379876938688684, 0.25092402269463276, 0.2542094470049566, 0.2533880913771643, 0.25708418798152916, 0.25749486599125165, 0.26242299701643673, 0.25913757724683634, 0.2570841887648346, 0.2574948677536888, 0.2620123211608041, 0.2603696114718302, 0.2587269004120719, 0.2669404519901628, 0.2574948671662098, 0.2579055445884532, 0.2624230001496583, 0.2579055457634113, 0.259548254436536, 0.25626283352869494, 0.261601644521866, 0.25954825525655884, 0.25913757665935727]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
