{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf16.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 03:14:04 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '1', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['yd', 'ib', 'ek', 'sg', 'eg', 'by', 'ce', 'mb', 'eb', 'my', 'sk', 'ds', 'ck', 'aa', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002188229D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000218F3F26EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6975, Accuracy:0.0830, Validation Loss:2.6927, Validation Accuracy:0.0837\n",
    "Epoch #2: Loss:2.6905, Accuracy:0.0830, Validation Loss:2.6865, Validation Accuracy:0.0837\n",
    "Epoch #3: Loss:2.6846, Accuracy:0.0830, Validation Loss:2.6819, Validation Accuracy:0.0837\n",
    "Epoch #4: Loss:2.6810, Accuracy:0.0830, Validation Loss:2.6784, Validation Accuracy:0.0837\n",
    "Epoch #5: Loss:2.6779, Accuracy:0.0858, Validation Loss:2.6760, Validation Accuracy:0.0887\n",
    "Epoch #6: Loss:2.6755, Accuracy:0.0809, Validation Loss:2.6737, Validation Accuracy:0.0837\n",
    "Epoch #7: Loss:2.6733, Accuracy:0.0830, Validation Loss:2.6715, Validation Accuracy:0.0837\n",
    "Epoch #8: Loss:2.6713, Accuracy:0.0830, Validation Loss:2.6695, Validation Accuracy:0.0837\n",
    "Epoch #9: Loss:2.6696, Accuracy:0.0797, Validation Loss:2.6679, Validation Accuracy:0.0821\n",
    "Epoch #10: Loss:2.6679, Accuracy:0.0809, Validation Loss:2.6665, Validation Accuracy:0.0837\n",
    "Epoch #11: Loss:2.6665, Accuracy:0.0834, Validation Loss:2.6653, Validation Accuracy:0.0821\n",
    "Epoch #12: Loss:2.6653, Accuracy:0.0813, Validation Loss:2.6643, Validation Accuracy:0.0821\n",
    "Epoch #13: Loss:2.6644, Accuracy:0.0817, Validation Loss:2.6634, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6637, Accuracy:0.1023, Validation Loss:2.6627, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6628, Accuracy:0.1023, Validation Loss:2.6621, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6622, Accuracy:0.1023, Validation Loss:2.6617, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6619, Accuracy:0.1023, Validation Loss:2.6612, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6614, Accuracy:0.1023, Validation Loss:2.6609, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6611, Accuracy:0.1023, Validation Loss:2.6607, Validation Accuracy:0.1018\n",
    "Epoch #20: Loss:2.6610, Accuracy:0.1023, Validation Loss:2.6605, Validation Accuracy:0.1018\n",
    "Epoch #21: Loss:2.6608, Accuracy:0.1023, Validation Loss:2.6602, Validation Accuracy:0.1018\n",
    "Epoch #22: Loss:2.6605, Accuracy:0.1023, Validation Loss:2.6600, Validation Accuracy:0.1018\n",
    "Epoch #23: Loss:2.6602, Accuracy:0.1023, Validation Loss:2.6598, Validation Accuracy:0.1018\n",
    "Epoch #24: Loss:2.6600, Accuracy:0.1023, Validation Loss:2.6596, Validation Accuracy:0.1018\n",
    "Epoch #25: Loss:2.6597, Accuracy:0.1023, Validation Loss:2.6592, Validation Accuracy:0.1018\n",
    "Epoch #26: Loss:2.6595, Accuracy:0.1023, Validation Loss:2.6588, Validation Accuracy:0.1018\n",
    "Epoch #27: Loss:2.6590, Accuracy:0.1023, Validation Loss:2.6582, Validation Accuracy:0.1018\n",
    "Epoch #28: Loss:2.6585, Accuracy:0.1023, Validation Loss:2.6574, Validation Accuracy:0.1018\n",
    "Epoch #29: Loss:2.6578, Accuracy:0.1023, Validation Loss:2.6562, Validation Accuracy:0.1018\n",
    "Epoch #30: Loss:2.6562, Accuracy:0.1023, Validation Loss:2.6542, Validation Accuracy:0.1018\n",
    "Epoch #31: Loss:2.6540, Accuracy:0.1023, Validation Loss:2.6509, Validation Accuracy:0.1018\n",
    "Epoch #32: Loss:2.6502, Accuracy:0.1027, Validation Loss:2.6449, Validation Accuracy:0.1067\n",
    "Epoch #33: Loss:2.6434, Accuracy:0.1068, Validation Loss:2.6344, Validation Accuracy:0.1100\n",
    "Epoch #34: Loss:2.6317, Accuracy:0.1064, Validation Loss:2.6194, Validation Accuracy:0.1149\n",
    "Epoch #35: Loss:2.6151, Accuracy:0.1080, Validation Loss:2.5946, Validation Accuracy:0.1100\n",
    "Epoch #36: Loss:2.5913, Accuracy:0.1076, Validation Loss:2.5768, Validation Accuracy:0.1117\n",
    "Epoch #37: Loss:2.5737, Accuracy:0.1191, Validation Loss:2.5563, Validation Accuracy:0.1330\n",
    "Epoch #38: Loss:2.5514, Accuracy:0.1409, Validation Loss:2.5379, Validation Accuracy:0.1675\n",
    "Epoch #39: Loss:2.5382, Accuracy:0.1446, Validation Loss:2.5238, Validation Accuracy:0.1741\n",
    "Epoch #40: Loss:2.5238, Accuracy:0.1614, Validation Loss:2.5119, Validation Accuracy:0.1609\n",
    "Epoch #41: Loss:2.5099, Accuracy:0.1643, Validation Loss:2.4986, Validation Accuracy:0.1954\n",
    "Epoch #42: Loss:2.4982, Accuracy:0.1688, Validation Loss:2.4835, Validation Accuracy:0.1954\n",
    "Epoch #43: Loss:2.4858, Accuracy:0.1733, Validation Loss:2.4720, Validation Accuracy:0.2167\n",
    "Epoch #44: Loss:2.4751, Accuracy:0.1770, Validation Loss:2.4596, Validation Accuracy:0.2036\n",
    "Epoch #45: Loss:2.4646, Accuracy:0.1667, Validation Loss:2.4482, Validation Accuracy:0.1938\n",
    "Epoch #46: Loss:2.4550, Accuracy:0.1667, Validation Loss:2.4400, Validation Accuracy:0.1724\n",
    "Epoch #47: Loss:2.4491, Accuracy:0.1688, Validation Loss:2.4325, Validation Accuracy:0.1790\n",
    "Epoch #48: Loss:2.4439, Accuracy:0.1754, Validation Loss:2.4298, Validation Accuracy:0.1970\n",
    "Epoch #49: Loss:2.4262, Accuracy:0.1708, Validation Loss:2.4067, Validation Accuracy:0.1806\n",
    "Epoch #50: Loss:2.4188, Accuracy:0.1758, Validation Loss:2.3989, Validation Accuracy:0.1806\n",
    "Epoch #51: Loss:2.4068, Accuracy:0.1811, Validation Loss:2.3909, Validation Accuracy:0.1954\n",
    "Epoch #52: Loss:2.3973, Accuracy:0.1754, Validation Loss:2.3772, Validation Accuracy:0.1888\n",
    "Epoch #53: Loss:2.3867, Accuracy:0.1782, Validation Loss:2.3686, Validation Accuracy:0.1872\n",
    "Epoch #54: Loss:2.3757, Accuracy:0.1828, Validation Loss:2.3569, Validation Accuracy:0.1921\n",
    "Epoch #55: Loss:2.3662, Accuracy:0.1885, Validation Loss:2.3515, Validation Accuracy:0.2003\n",
    "Epoch #56: Loss:2.3606, Accuracy:0.1881, Validation Loss:2.3355, Validation Accuracy:0.1921\n",
    "Epoch #57: Loss:2.3511, Accuracy:0.1832, Validation Loss:2.3373, Validation Accuracy:0.1987\n",
    "Epoch #58: Loss:2.3389, Accuracy:0.1910, Validation Loss:2.3175, Validation Accuracy:0.2020\n",
    "Epoch #59: Loss:2.3287, Accuracy:0.1984, Validation Loss:2.3072, Validation Accuracy:0.2020\n",
    "Epoch #60: Loss:2.3216, Accuracy:0.1918, Validation Loss:2.3095, Validation Accuracy:0.2085\n",
    "Epoch #61: Loss:2.3178, Accuracy:0.1963, Validation Loss:2.2903, Validation Accuracy:0.2053\n",
    "Epoch #62: Loss:2.2986, Accuracy:0.1992, Validation Loss:2.2992, Validation Accuracy:0.2118\n",
    "Epoch #63: Loss:2.3025, Accuracy:0.2021, Validation Loss:2.2726, Validation Accuracy:0.2233\n",
    "Epoch #64: Loss:2.2841, Accuracy:0.2066, Validation Loss:2.2627, Validation Accuracy:0.2020\n",
    "Epoch #65: Loss:2.2708, Accuracy:0.2119, Validation Loss:2.2486, Validation Accuracy:0.2151\n",
    "Epoch #66: Loss:2.2568, Accuracy:0.2107, Validation Loss:2.2411, Validation Accuracy:0.2184\n",
    "Epoch #67: Loss:2.2547, Accuracy:0.2177, Validation Loss:2.2424, Validation Accuracy:0.2250\n",
    "Epoch #68: Loss:2.2427, Accuracy:0.2152, Validation Loss:2.2285, Validation Accuracy:0.2167\n",
    "Epoch #69: Loss:2.2347, Accuracy:0.2148, Validation Loss:2.2138, Validation Accuracy:0.2167\n",
    "Epoch #70: Loss:2.2207, Accuracy:0.2222, Validation Loss:2.2045, Validation Accuracy:0.2266\n",
    "Epoch #71: Loss:2.2118, Accuracy:0.2271, Validation Loss:2.1971, Validation Accuracy:0.2217\n",
    "Epoch #72: Loss:2.2049, Accuracy:0.2267, Validation Loss:2.1871, Validation Accuracy:0.2200\n",
    "Epoch #73: Loss:2.1987, Accuracy:0.2242, Validation Loss:2.1819, Validation Accuracy:0.2332\n",
    "Epoch #74: Loss:2.1888, Accuracy:0.2255, Validation Loss:2.1725, Validation Accuracy:0.2250\n",
    "Epoch #75: Loss:2.1810, Accuracy:0.2320, Validation Loss:2.1685, Validation Accuracy:0.2332\n",
    "Epoch #76: Loss:2.1819, Accuracy:0.2345, Validation Loss:2.1637, Validation Accuracy:0.2151\n",
    "Epoch #77: Loss:2.1785, Accuracy:0.2275, Validation Loss:2.1641, Validation Accuracy:0.2414\n",
    "Epoch #78: Loss:2.1651, Accuracy:0.2353, Validation Loss:2.1550, Validation Accuracy:0.2233\n",
    "Epoch #79: Loss:2.1599, Accuracy:0.2316, Validation Loss:2.1410, Validation Accuracy:0.2414\n",
    "Epoch #80: Loss:2.1536, Accuracy:0.2386, Validation Loss:2.1441, Validation Accuracy:0.2348\n",
    "Epoch #81: Loss:2.1560, Accuracy:0.2390, Validation Loss:2.1385, Validation Accuracy:0.2282\n",
    "Epoch #82: Loss:2.1509, Accuracy:0.2390, Validation Loss:2.1303, Validation Accuracy:0.2627\n",
    "Epoch #83: Loss:2.1416, Accuracy:0.2493, Validation Loss:2.1223, Validation Accuracy:0.2430\n",
    "Epoch #84: Loss:2.1323, Accuracy:0.2444, Validation Loss:2.1218, Validation Accuracy:0.2496\n",
    "Epoch #85: Loss:2.1341, Accuracy:0.2460, Validation Loss:2.1487, Validation Accuracy:0.2611\n",
    "Epoch #86: Loss:2.1462, Accuracy:0.2513, Validation Loss:2.1102, Validation Accuracy:0.2496\n",
    "Epoch #87: Loss:2.1236, Accuracy:0.2583, Validation Loss:2.1246, Validation Accuracy:0.2397\n",
    "Epoch #88: Loss:2.1281, Accuracy:0.2472, Validation Loss:2.1157, Validation Accuracy:0.2644\n",
    "Epoch #89: Loss:2.1196, Accuracy:0.2509, Validation Loss:2.1028, Validation Accuracy:0.2644\n",
    "Epoch #90: Loss:2.1209, Accuracy:0.2559, Validation Loss:2.1289, Validation Accuracy:0.2397\n",
    "Epoch #91: Loss:2.1162, Accuracy:0.2505, Validation Loss:2.1398, Validation Accuracy:0.2578\n",
    "Epoch #92: Loss:2.1244, Accuracy:0.2616, Validation Loss:2.1083, Validation Accuracy:0.2447\n",
    "Epoch #93: Loss:2.1139, Accuracy:0.2575, Validation Loss:2.0970, Validation Accuracy:0.2578\n",
    "Epoch #94: Loss:2.1242, Accuracy:0.2534, Validation Loss:2.0908, Validation Accuracy:0.2627\n",
    "Epoch #95: Loss:2.0981, Accuracy:0.2715, Validation Loss:2.1083, Validation Accuracy:0.2479\n",
    "Epoch #96: Loss:2.1037, Accuracy:0.2686, Validation Loss:2.0856, Validation Accuracy:0.2644\n",
    "Epoch #97: Loss:2.0936, Accuracy:0.2698, Validation Loss:2.0795, Validation Accuracy:0.2611\n",
    "Epoch #98: Loss:2.0824, Accuracy:0.2760, Validation Loss:2.0774, Validation Accuracy:0.2709\n",
    "Epoch #99: Loss:2.0850, Accuracy:0.2674, Validation Loss:2.0800, Validation Accuracy:0.2775\n",
    "Epoch #100: Loss:2.0875, Accuracy:0.2628, Validation Loss:2.0816, Validation Accuracy:0.2627\n",
    "Epoch #101: Loss:2.0849, Accuracy:0.2661, Validation Loss:2.1083, Validation Accuracy:0.2594\n",
    "Epoch #102: Loss:2.0949, Accuracy:0.2735, Validation Loss:2.0864, Validation Accuracy:0.2545\n",
    "Epoch #103: Loss:2.0864, Accuracy:0.2719, Validation Loss:2.0697, Validation Accuracy:0.2709\n",
    "Epoch #104: Loss:2.0682, Accuracy:0.2752, Validation Loss:2.0660, Validation Accuracy:0.2709\n",
    "Epoch #105: Loss:2.0644, Accuracy:0.2805, Validation Loss:2.0591, Validation Accuracy:0.2791\n",
    "Epoch #106: Loss:2.0595, Accuracy:0.2797, Validation Loss:2.0584, Validation Accuracy:0.2791\n",
    "Epoch #107: Loss:2.0588, Accuracy:0.2821, Validation Loss:2.0806, Validation Accuracy:0.2759\n",
    "Epoch #108: Loss:2.0630, Accuracy:0.2772, Validation Loss:2.0543, Validation Accuracy:0.2890\n",
    "Epoch #109: Loss:2.0538, Accuracy:0.2891, Validation Loss:2.0556, Validation Accuracy:0.2923\n",
    "Epoch #110: Loss:2.0575, Accuracy:0.2793, Validation Loss:2.0587, Validation Accuracy:0.2759\n",
    "Epoch #111: Loss:2.0532, Accuracy:0.2850, Validation Loss:2.0459, Validation Accuracy:0.2824\n",
    "Epoch #112: Loss:2.0447, Accuracy:0.2817, Validation Loss:2.0454, Validation Accuracy:0.2906\n",
    "Epoch #113: Loss:2.0464, Accuracy:0.2805, Validation Loss:2.0493, Validation Accuracy:0.2890\n",
    "Epoch #114: Loss:2.0455, Accuracy:0.2842, Validation Loss:2.0474, Validation Accuracy:0.2726\n",
    "Epoch #115: Loss:2.0396, Accuracy:0.2854, Validation Loss:2.0387, Validation Accuracy:0.2956\n",
    "Epoch #116: Loss:2.0362, Accuracy:0.2875, Validation Loss:2.0494, Validation Accuracy:0.2890\n",
    "Epoch #117: Loss:2.0425, Accuracy:0.2838, Validation Loss:2.0402, Validation Accuracy:0.2808\n",
    "Epoch #118: Loss:2.0401, Accuracy:0.2862, Validation Loss:2.0403, Validation Accuracy:0.2808\n",
    "Epoch #119: Loss:2.0359, Accuracy:0.2867, Validation Loss:2.0354, Validation Accuracy:0.2956\n",
    "Epoch #120: Loss:2.0398, Accuracy:0.2834, Validation Loss:2.0365, Validation Accuracy:0.2972\n",
    "Epoch #121: Loss:2.0408, Accuracy:0.2953, Validation Loss:2.0330, Validation Accuracy:0.2874\n",
    "Epoch #122: Loss:2.0261, Accuracy:0.2903, Validation Loss:2.0276, Validation Accuracy:0.2841\n",
    "Epoch #123: Loss:2.0218, Accuracy:0.2916, Validation Loss:2.0284, Validation Accuracy:0.2923\n",
    "Epoch #124: Loss:2.0226, Accuracy:0.2891, Validation Loss:2.0308, Validation Accuracy:0.2841\n",
    "Epoch #125: Loss:2.0257, Accuracy:0.2912, Validation Loss:2.0363, Validation Accuracy:0.2890\n",
    "Epoch #126: Loss:2.0229, Accuracy:0.2887, Validation Loss:2.0302, Validation Accuracy:0.2972\n",
    "Epoch #127: Loss:2.0169, Accuracy:0.2920, Validation Loss:2.0226, Validation Accuracy:0.2972\n",
    "Epoch #128: Loss:2.0182, Accuracy:0.2949, Validation Loss:2.0188, Validation Accuracy:0.2956\n",
    "Epoch #129: Loss:2.0136, Accuracy:0.2953, Validation Loss:2.0171, Validation Accuracy:0.2939\n",
    "Epoch #130: Loss:2.0134, Accuracy:0.2977, Validation Loss:2.0179, Validation Accuracy:0.2956\n",
    "Epoch #131: Loss:2.0176, Accuracy:0.2928, Validation Loss:2.0260, Validation Accuracy:0.3005\n",
    "Epoch #132: Loss:2.0108, Accuracy:0.2953, Validation Loss:2.0193, Validation Accuracy:0.2857\n",
    "Epoch #133: Loss:2.0168, Accuracy:0.2961, Validation Loss:2.0251, Validation Accuracy:0.2874\n",
    "Epoch #134: Loss:2.0101, Accuracy:0.2990, Validation Loss:2.0192, Validation Accuracy:0.3005\n",
    "Epoch #135: Loss:2.0176, Accuracy:0.2920, Validation Loss:2.0258, Validation Accuracy:0.2989\n",
    "Epoch #136: Loss:2.0110, Accuracy:0.2994, Validation Loss:2.0132, Validation Accuracy:0.2956\n",
    "Epoch #137: Loss:2.0066, Accuracy:0.2977, Validation Loss:2.0117, Validation Accuracy:0.2874\n",
    "Epoch #138: Loss:2.0088, Accuracy:0.2940, Validation Loss:2.0133, Validation Accuracy:0.3021\n",
    "Epoch #139: Loss:2.0286, Accuracy:0.2883, Validation Loss:2.0071, Validation Accuracy:0.3005\n",
    "Epoch #140: Loss:2.0309, Accuracy:0.2834, Validation Loss:2.0215, Validation Accuracy:0.2956\n",
    "Epoch #141: Loss:2.0155, Accuracy:0.2973, Validation Loss:2.0145, Validation Accuracy:0.2956\n",
    "Epoch #142: Loss:1.9999, Accuracy:0.3006, Validation Loss:2.0111, Validation Accuracy:0.3071\n",
    "Epoch #143: Loss:1.9972, Accuracy:0.3002, Validation Loss:2.0062, Validation Accuracy:0.2956\n",
    "Epoch #144: Loss:1.9991, Accuracy:0.2994, Validation Loss:2.0078, Validation Accuracy:0.2989\n",
    "Epoch #145: Loss:1.9983, Accuracy:0.3018, Validation Loss:2.0274, Validation Accuracy:0.3005\n",
    "Epoch #146: Loss:2.0029, Accuracy:0.2969, Validation Loss:2.0045, Validation Accuracy:0.3005\n",
    "Epoch #147: Loss:2.0005, Accuracy:0.3002, Validation Loss:2.0068, Validation Accuracy:0.2890\n",
    "Epoch #148: Loss:2.0030, Accuracy:0.2953, Validation Loss:2.0058, Validation Accuracy:0.3071\n",
    "Epoch #149: Loss:1.9953, Accuracy:0.3006, Validation Loss:2.0133, Validation Accuracy:0.3021\n",
    "Epoch #150: Loss:1.9971, Accuracy:0.3006, Validation Loss:2.0017, Validation Accuracy:0.3054\n",
    "Epoch #151: Loss:1.9878, Accuracy:0.2969, Validation Loss:1.9966, Validation Accuracy:0.3071\n",
    "Epoch #152: Loss:1.9895, Accuracy:0.2982, Validation Loss:2.0003, Validation Accuracy:0.3087\n",
    "Epoch #153: Loss:1.9891, Accuracy:0.3031, Validation Loss:1.9971, Validation Accuracy:0.3021\n",
    "Epoch #154: Loss:1.9871, Accuracy:0.3002, Validation Loss:2.0134, Validation Accuracy:0.2956\n",
    "Epoch #155: Loss:2.0119, Accuracy:0.2977, Validation Loss:1.9942, Validation Accuracy:0.3038\n",
    "Epoch #156: Loss:2.0013, Accuracy:0.2994, Validation Loss:2.0863, Validation Accuracy:0.2857\n",
    "Epoch #157: Loss:2.0277, Accuracy:0.2916, Validation Loss:1.9962, Validation Accuracy:0.3120\n",
    "Epoch #158: Loss:2.0062, Accuracy:0.2973, Validation Loss:2.0347, Validation Accuracy:0.2874\n",
    "Epoch #159: Loss:2.0112, Accuracy:0.2899, Validation Loss:2.0057, Validation Accuracy:0.2923\n",
    "Epoch #160: Loss:2.0015, Accuracy:0.2879, Validation Loss:2.0404, Validation Accuracy:0.3021\n",
    "Epoch #161: Loss:1.9988, Accuracy:0.2965, Validation Loss:1.9927, Validation Accuracy:0.3120\n",
    "Epoch #162: Loss:1.9896, Accuracy:0.3002, Validation Loss:1.9976, Validation Accuracy:0.3136\n",
    "Epoch #163: Loss:1.9828, Accuracy:0.2986, Validation Loss:1.9899, Validation Accuracy:0.3153\n",
    "Epoch #164: Loss:1.9776, Accuracy:0.3002, Validation Loss:1.9908, Validation Accuracy:0.3186\n",
    "Epoch #165: Loss:1.9784, Accuracy:0.3047, Validation Loss:2.0059, Validation Accuracy:0.3071\n",
    "Epoch #166: Loss:1.9784, Accuracy:0.3006, Validation Loss:1.9943, Validation Accuracy:0.3071\n",
    "Epoch #167: Loss:1.9799, Accuracy:0.3043, Validation Loss:1.9862, Validation Accuracy:0.3103\n",
    "Epoch #168: Loss:1.9780, Accuracy:0.3043, Validation Loss:2.0053, Validation Accuracy:0.3071\n",
    "Epoch #169: Loss:1.9816, Accuracy:0.3006, Validation Loss:1.9849, Validation Accuracy:0.3103\n",
    "Epoch #170: Loss:1.9732, Accuracy:0.3047, Validation Loss:1.9854, Validation Accuracy:0.3186\n",
    "Epoch #171: Loss:1.9694, Accuracy:0.3076, Validation Loss:1.9848, Validation Accuracy:0.3120\n",
    "Epoch #172: Loss:1.9697, Accuracy:0.3121, Validation Loss:1.9841, Validation Accuracy:0.3218\n",
    "Epoch #173: Loss:1.9699, Accuracy:0.3068, Validation Loss:1.9802, Validation Accuracy:0.3186\n",
    "Epoch #174: Loss:1.9654, Accuracy:0.3117, Validation Loss:1.9870, Validation Accuracy:0.3005\n",
    "Epoch #175: Loss:1.9739, Accuracy:0.3051, Validation Loss:1.9809, Validation Accuracy:0.3153\n",
    "Epoch #176: Loss:1.9691, Accuracy:0.3117, Validation Loss:1.9948, Validation Accuracy:0.3136\n",
    "Epoch #177: Loss:1.9713, Accuracy:0.3051, Validation Loss:1.9911, Validation Accuracy:0.3218\n",
    "Epoch #178: Loss:1.9687, Accuracy:0.3068, Validation Loss:1.9777, Validation Accuracy:0.3153\n",
    "Epoch #179: Loss:1.9693, Accuracy:0.3113, Validation Loss:1.9859, Validation Accuracy:0.3136\n",
    "Epoch #180: Loss:1.9665, Accuracy:0.3109, Validation Loss:1.9790, Validation Accuracy:0.3186\n",
    "Epoch #181: Loss:1.9607, Accuracy:0.3113, Validation Loss:1.9775, Validation Accuracy:0.3153\n",
    "Epoch #182: Loss:1.9581, Accuracy:0.3125, Validation Loss:1.9803, Validation Accuracy:0.3153\n",
    "Epoch #183: Loss:1.9654, Accuracy:0.3092, Validation Loss:1.9866, Validation Accuracy:0.3005\n",
    "Epoch #184: Loss:1.9674, Accuracy:0.3084, Validation Loss:1.9750, Validation Accuracy:0.3103\n",
    "Epoch #185: Loss:1.9597, Accuracy:0.3097, Validation Loss:1.9759, Validation Accuracy:0.3202\n",
    "Epoch #186: Loss:1.9655, Accuracy:0.3105, Validation Loss:1.9801, Validation Accuracy:0.3218\n",
    "Epoch #187: Loss:1.9665, Accuracy:0.3060, Validation Loss:1.9901, Validation Accuracy:0.3186\n",
    "Epoch #188: Loss:1.9619, Accuracy:0.3039, Validation Loss:1.9763, Validation Accuracy:0.3136\n",
    "Epoch #189: Loss:1.9613, Accuracy:0.3113, Validation Loss:1.9838, Validation Accuracy:0.3054\n",
    "Epoch #190: Loss:1.9658, Accuracy:0.3084, Validation Loss:1.9833, Validation Accuracy:0.3136\n",
    "Epoch #191: Loss:1.9627, Accuracy:0.3080, Validation Loss:1.9733, Validation Accuracy:0.3202\n",
    "Epoch #192: Loss:1.9584, Accuracy:0.3113, Validation Loss:1.9734, Validation Accuracy:0.3103\n",
    "Epoch #193: Loss:1.9581, Accuracy:0.3105, Validation Loss:1.9855, Validation Accuracy:0.3005\n",
    "Epoch #194: Loss:1.9615, Accuracy:0.3092, Validation Loss:1.9690, Validation Accuracy:0.3136\n",
    "Epoch #195: Loss:1.9544, Accuracy:0.3142, Validation Loss:1.9971, Validation Accuracy:0.3218\n",
    "Epoch #196: Loss:1.9563, Accuracy:0.3187, Validation Loss:1.9712, Validation Accuracy:0.3087\n",
    "Epoch #197: Loss:1.9549, Accuracy:0.3133, Validation Loss:1.9822, Validation Accuracy:0.3021\n",
    "Epoch #198: Loss:1.9609, Accuracy:0.3088, Validation Loss:1.9699, Validation Accuracy:0.3169\n",
    "Epoch #199: Loss:1.9494, Accuracy:0.3175, Validation Loss:1.9833, Validation Accuracy:0.3153\n",
    "Epoch #200: Loss:1.9492, Accuracy:0.3129, Validation Loss:1.9704, Validation Accuracy:0.3103\n",
    "Epoch #201: Loss:1.9552, Accuracy:0.3129, Validation Loss:1.9767, Validation Accuracy:0.3087\n",
    "Epoch #202: Loss:1.9537, Accuracy:0.3117, Validation Loss:1.9700, Validation Accuracy:0.3021\n",
    "Epoch #203: Loss:1.9635, Accuracy:0.3072, Validation Loss:1.9696, Validation Accuracy:0.3120\n",
    "Epoch #204: Loss:1.9596, Accuracy:0.3158, Validation Loss:1.9680, Validation Accuracy:0.3202\n",
    "Epoch #205: Loss:1.9436, Accuracy:0.3203, Validation Loss:1.9675, Validation Accuracy:0.3251\n",
    "Epoch #206: Loss:1.9435, Accuracy:0.3175, Validation Loss:1.9668, Validation Accuracy:0.3186\n",
    "Epoch #207: Loss:1.9436, Accuracy:0.3175, Validation Loss:1.9709, Validation Accuracy:0.3202\n",
    "Epoch #208: Loss:1.9467, Accuracy:0.3150, Validation Loss:1.9628, Validation Accuracy:0.3087\n",
    "Epoch #209: Loss:1.9464, Accuracy:0.3191, Validation Loss:1.9673, Validation Accuracy:0.3169\n",
    "Epoch #210: Loss:1.9526, Accuracy:0.3166, Validation Loss:1.9647, Validation Accuracy:0.3186\n",
    "Epoch #211: Loss:1.9428, Accuracy:0.3166, Validation Loss:1.9785, Validation Accuracy:0.3153\n",
    "Epoch #212: Loss:1.9449, Accuracy:0.3109, Validation Loss:1.9618, Validation Accuracy:0.3169\n",
    "Epoch #213: Loss:1.9446, Accuracy:0.3162, Validation Loss:1.9760, Validation Accuracy:0.3021\n",
    "Epoch #214: Loss:1.9514, Accuracy:0.3150, Validation Loss:1.9691, Validation Accuracy:0.3120\n",
    "Epoch #215: Loss:1.9440, Accuracy:0.3133, Validation Loss:1.9656, Validation Accuracy:0.3268\n",
    "Epoch #216: Loss:1.9403, Accuracy:0.3166, Validation Loss:1.9702, Validation Accuracy:0.3218\n",
    "Epoch #217: Loss:1.9432, Accuracy:0.3162, Validation Loss:1.9599, Validation Accuracy:0.3103\n",
    "Epoch #218: Loss:1.9488, Accuracy:0.3133, Validation Loss:1.9814, Validation Accuracy:0.2989\n",
    "Epoch #219: Loss:1.9501, Accuracy:0.3175, Validation Loss:1.9596, Validation Accuracy:0.3054\n",
    "Epoch #220: Loss:1.9476, Accuracy:0.3162, Validation Loss:1.9789, Validation Accuracy:0.3153\n",
    "Epoch #221: Loss:1.9451, Accuracy:0.3129, Validation Loss:1.9632, Validation Accuracy:0.3218\n",
    "Epoch #222: Loss:1.9373, Accuracy:0.3183, Validation Loss:1.9614, Validation Accuracy:0.3218\n",
    "Epoch #223: Loss:1.9336, Accuracy:0.3158, Validation Loss:1.9605, Validation Accuracy:0.3284\n",
    "Epoch #224: Loss:1.9392, Accuracy:0.3170, Validation Loss:1.9629, Validation Accuracy:0.3218\n",
    "Epoch #225: Loss:1.9288, Accuracy:0.3207, Validation Loss:1.9585, Validation Accuracy:0.3071\n",
    "Epoch #226: Loss:1.9323, Accuracy:0.3187, Validation Loss:1.9573, Validation Accuracy:0.3087\n",
    "Epoch #227: Loss:1.9377, Accuracy:0.3146, Validation Loss:1.9612, Validation Accuracy:0.3186\n",
    "Epoch #228: Loss:1.9322, Accuracy:0.3175, Validation Loss:1.9614, Validation Accuracy:0.3268\n",
    "Epoch #229: Loss:1.9344, Accuracy:0.3207, Validation Loss:1.9737, Validation Accuracy:0.3268\n",
    "Epoch #230: Loss:1.9531, Accuracy:0.3109, Validation Loss:1.9542, Validation Accuracy:0.3087\n",
    "Epoch #231: Loss:1.9370, Accuracy:0.3129, Validation Loss:1.9686, Validation Accuracy:0.3120\n",
    "Epoch #232: Loss:1.9313, Accuracy:0.3183, Validation Loss:1.9508, Validation Accuracy:0.3136\n",
    "Epoch #233: Loss:1.9268, Accuracy:0.3199, Validation Loss:1.9628, Validation Accuracy:0.3268\n",
    "Epoch #234: Loss:1.9263, Accuracy:0.3199, Validation Loss:1.9566, Validation Accuracy:0.3202\n",
    "Epoch #235: Loss:1.9258, Accuracy:0.3195, Validation Loss:1.9499, Validation Accuracy:0.3054\n",
    "Epoch #236: Loss:1.9220, Accuracy:0.3232, Validation Loss:1.9523, Validation Accuracy:0.3136\n",
    "Epoch #237: Loss:1.9307, Accuracy:0.3175, Validation Loss:1.9595, Validation Accuracy:0.3317\n",
    "Epoch #238: Loss:1.9337, Accuracy:0.3138, Validation Loss:1.9703, Validation Accuracy:0.3136\n",
    "Epoch #239: Loss:1.9304, Accuracy:0.3150, Validation Loss:1.9488, Validation Accuracy:0.3103\n",
    "Epoch #240: Loss:1.9243, Accuracy:0.3224, Validation Loss:1.9530, Validation Accuracy:0.3103\n",
    "Epoch #241: Loss:1.9224, Accuracy:0.3228, Validation Loss:1.9589, Validation Accuracy:0.3153\n",
    "Epoch #242: Loss:1.9251, Accuracy:0.3183, Validation Loss:1.9543, Validation Accuracy:0.3333\n",
    "Epoch #243: Loss:1.9229, Accuracy:0.3248, Validation Loss:1.9451, Validation Accuracy:0.3153\n",
    "Epoch #244: Loss:1.9213, Accuracy:0.3216, Validation Loss:1.9492, Validation Accuracy:0.3054\n",
    "Epoch #245: Loss:1.9164, Accuracy:0.3224, Validation Loss:1.9464, Validation Accuracy:0.3235\n",
    "Epoch #246: Loss:1.9139, Accuracy:0.3240, Validation Loss:1.9443, Validation Accuracy:0.3383\n",
    "Epoch #247: Loss:1.9151, Accuracy:0.3220, Validation Loss:1.9495, Validation Accuracy:0.3218\n",
    "Epoch #248: Loss:1.9162, Accuracy:0.3216, Validation Loss:1.9483, Validation Accuracy:0.3235\n",
    "Epoch #249: Loss:1.9190, Accuracy:0.3203, Validation Loss:1.9403, Validation Accuracy:0.3284\n",
    "Epoch #250: Loss:1.9153, Accuracy:0.3273, Validation Loss:1.9546, Validation Accuracy:0.3021\n",
    "Epoch #251: Loss:1.9143, Accuracy:0.3150, Validation Loss:1.9431, Validation Accuracy:0.3284\n",
    "Epoch #252: Loss:1.9137, Accuracy:0.3187, Validation Loss:1.9394, Validation Accuracy:0.3251\n",
    "Epoch #253: Loss:1.9024, Accuracy:0.3273, Validation Loss:1.9393, Validation Accuracy:0.3317\n",
    "Epoch #254: Loss:1.9043, Accuracy:0.3355, Validation Loss:1.9336, Validation Accuracy:0.3218\n",
    "Epoch #255: Loss:1.9078, Accuracy:0.3261, Validation Loss:1.9428, Validation Accuracy:0.3169\n",
    "Epoch #256: Loss:1.9091, Accuracy:0.3277, Validation Loss:1.9349, Validation Accuracy:0.3218\n",
    "Epoch #257: Loss:1.9117, Accuracy:0.3257, Validation Loss:1.9302, Validation Accuracy:0.3202\n",
    "Epoch #258: Loss:1.9022, Accuracy:0.3298, Validation Loss:1.9317, Validation Accuracy:0.3284\n",
    "Epoch #259: Loss:1.9051, Accuracy:0.3331, Validation Loss:1.9268, Validation Accuracy:0.3399\n",
    "Epoch #260: Loss:1.8878, Accuracy:0.3347, Validation Loss:1.9191, Validation Accuracy:0.3284\n",
    "Epoch #261: Loss:1.8855, Accuracy:0.3335, Validation Loss:1.9177, Validation Accuracy:0.3300\n",
    "Epoch #262: Loss:1.8848, Accuracy:0.3359, Validation Loss:1.9121, Validation Accuracy:0.3432\n",
    "Epoch #263: Loss:1.8820, Accuracy:0.3409, Validation Loss:1.9124, Validation Accuracy:0.3465\n",
    "Epoch #264: Loss:1.8788, Accuracy:0.3376, Validation Loss:1.9140, Validation Accuracy:0.3432\n",
    "Epoch #265: Loss:1.8779, Accuracy:0.3351, Validation Loss:1.9022, Validation Accuracy:0.3448\n",
    "Epoch #266: Loss:1.8774, Accuracy:0.3372, Validation Loss:1.9026, Validation Accuracy:0.3580\n",
    "Epoch #267: Loss:1.8738, Accuracy:0.3413, Validation Loss:1.8978, Validation Accuracy:0.3498\n",
    "Epoch #268: Loss:1.8709, Accuracy:0.3483, Validation Loss:1.8959, Validation Accuracy:0.3432\n",
    "Epoch #269: Loss:1.8722, Accuracy:0.3474, Validation Loss:1.9003, Validation Accuracy:0.3448\n",
    "Epoch #270: Loss:1.8732, Accuracy:0.3454, Validation Loss:1.8919, Validation Accuracy:0.3432\n",
    "Epoch #271: Loss:1.8638, Accuracy:0.3491, Validation Loss:1.8829, Validation Accuracy:0.3432\n",
    "Epoch #272: Loss:1.8607, Accuracy:0.3511, Validation Loss:1.8880, Validation Accuracy:0.3514\n",
    "Epoch #273: Loss:1.8621, Accuracy:0.3499, Validation Loss:1.8810, Validation Accuracy:0.3629\n",
    "Epoch #274: Loss:1.8551, Accuracy:0.3606, Validation Loss:1.8932, Validation Accuracy:0.3498\n",
    "Epoch #275: Loss:1.8741, Accuracy:0.3483, Validation Loss:1.9013, Validation Accuracy:0.3563\n",
    "Epoch #276: Loss:1.8699, Accuracy:0.3495, Validation Loss:1.9088, Validation Accuracy:0.3268\n",
    "Epoch #277: Loss:1.8642, Accuracy:0.3577, Validation Loss:1.8712, Validation Accuracy:0.3711\n",
    "Epoch #278: Loss:1.8457, Accuracy:0.3614, Validation Loss:1.8771, Validation Accuracy:0.3629\n",
    "Epoch #279: Loss:1.8468, Accuracy:0.3630, Validation Loss:1.8665, Validation Accuracy:0.3711\n",
    "Epoch #280: Loss:1.8419, Accuracy:0.3606, Validation Loss:1.8572, Validation Accuracy:0.3695\n",
    "Epoch #281: Loss:1.8417, Accuracy:0.3639, Validation Loss:1.8621, Validation Accuracy:0.3530\n",
    "Epoch #282: Loss:1.8369, Accuracy:0.3639, Validation Loss:1.8524, Validation Accuracy:0.3826\n",
    "Epoch #283: Loss:1.8445, Accuracy:0.3610, Validation Loss:1.8477, Validation Accuracy:0.3760\n",
    "Epoch #284: Loss:1.8315, Accuracy:0.3663, Validation Loss:1.8439, Validation Accuracy:0.3711\n",
    "Epoch #285: Loss:1.8269, Accuracy:0.3700, Validation Loss:1.8383, Validation Accuracy:0.3744\n",
    "Epoch #286: Loss:1.8307, Accuracy:0.3692, Validation Loss:1.8422, Validation Accuracy:0.3908\n",
    "Epoch #287: Loss:1.8313, Accuracy:0.3622, Validation Loss:1.8386, Validation Accuracy:0.3859\n",
    "Epoch #288: Loss:1.8314, Accuracy:0.3700, Validation Loss:1.8325, Validation Accuracy:0.3826\n",
    "Epoch #289: Loss:1.8267, Accuracy:0.3663, Validation Loss:1.8295, Validation Accuracy:0.3810\n",
    "Epoch #290: Loss:1.8240, Accuracy:0.3630, Validation Loss:1.8402, Validation Accuracy:0.3760\n",
    "Epoch #291: Loss:1.8310, Accuracy:0.3655, Validation Loss:1.8387, Validation Accuracy:0.3612\n",
    "Epoch #292: Loss:1.8343, Accuracy:0.3585, Validation Loss:1.8854, Validation Accuracy:0.3580\n",
    "Epoch #293: Loss:1.8452, Accuracy:0.3626, Validation Loss:1.8413, Validation Accuracy:0.3711\n",
    "Epoch #294: Loss:1.8232, Accuracy:0.3721, Validation Loss:1.8381, Validation Accuracy:0.3842\n",
    "Epoch #295: Loss:1.8134, Accuracy:0.3762, Validation Loss:1.8296, Validation Accuracy:0.3662\n",
    "Epoch #296: Loss:1.8228, Accuracy:0.3671, Validation Loss:1.8370, Validation Accuracy:0.3662\n",
    "Epoch #297: Loss:1.8202, Accuracy:0.3803, Validation Loss:1.8456, Validation Accuracy:0.3777\n",
    "Epoch #298: Loss:1.8089, Accuracy:0.3803, Validation Loss:1.8241, Validation Accuracy:0.3777\n",
    "Epoch #299: Loss:1.8105, Accuracy:0.3745, Validation Loss:1.8221, Validation Accuracy:0.3793\n",
    "Epoch #300: Loss:1.8020, Accuracy:0.3819, Validation Loss:1.8188, Validation Accuracy:0.3892\n",
    "\n",
    "Test:\n",
    "Test Loss:1.81875169, Accuracy:0.3892\n",
    "Labels: ['yd', 'ib', 'ek', 'sg', 'eg', 'by', 'ce', 'mb', 'eb', 'my', 'sk', 'ds', 'ck', 'aa', 'eo']\n",
    "Confusion Matrix:\n",
    "      yd  ib  ek  sg  eg  by  ce  mb  eb  my  sk  ds  ck  aa  eo\n",
    "t:yd  40   5   2   2   0   4   0   7   0   0   0   1   0   0   1\n",
    "t:ib  22   7   0   4   1   6   0  10   0   0   0   0   0   2   2\n",
    "t:ek   8   0  13   0   4   0   0   6  12   0   0   2   0   3   0\n",
    "t:sg  19   3   0  19   0   6   0   0   0   0   0   0   0   2   2\n",
    "t:eg   0   0   8   2  25   1   0   3   1   0   0   1   0   8   1\n",
    "t:by   2   2   0   6   1  18   0   6   0   0   0   0   0   3   2\n",
    "t:ce   3   1   3   2   2   4   0   8   1   0   0   0   0   3   0\n",
    "t:mb   7   2   2   1   3   4   0  26   2   0   1   0   0   3   1\n",
    "t:eb   0   0   1   0   0   0   0   1  47   0   0   1   0   0   0\n",
    "t:my   0   0   2   0   0   0   0   4  12   2   0   0   0   0   0\n",
    "t:sk   4   1   6   0   1   0   0   2  16   0   0   3   0   0   0\n",
    "t:ds   2   0   3   0   1   0   0   2  14   0   0   6   0   3   0\n",
    "t:ck   1   0   6   0   1   0   0   3   7   0   1   1   0   2   1\n",
    "t:aa   5   0   0   4  10   0   0   1   1   0   0   0   0  13   0\n",
    "t:eo   0   0   0   1   4   4   0   3   0   0   0   0   0   1  21\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          yd       0.35      0.65      0.46        62\n",
    "          ib       0.33      0.13      0.19        54\n",
    "          ek       0.28      0.27      0.28        48\n",
    "          sg       0.46      0.37      0.41        51\n",
    "          eg       0.47      0.50      0.49        50\n",
    "          by       0.38      0.45      0.41        40\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          mb       0.32      0.50      0.39        52\n",
    "          eb       0.42      0.94      0.58        50\n",
    "          my       1.00      0.10      0.18        20\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ds       0.40      0.19      0.26        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          aa       0.30      0.38      0.34        34\n",
    "          eo       0.68      0.62      0.65        34\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.36      0.34      0.31       609\n",
    "weighted avg       0.36      0.39      0.34       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 03:29:49 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 45 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6927244678898203, 2.686494836274822, 2.681888621820409, 2.678412675466052, 2.6759688975580023, 2.673666030119597, 2.6714529102463245, 2.6695470304911946, 2.667881107486919, 2.6665295708942884, 2.6653448227786862, 2.6643143913820264, 2.6634259486237575, 2.6627216143365366, 2.6621351954580725, 2.661673664459454, 2.6612375428523922, 2.6609257054446367, 2.6606959928628453, 2.6604651907590418, 2.6602249971555763, 2.6600205882625234, 2.6598166961388046, 2.6595656272813017, 2.6592429122705568, 2.658825661748501, 2.658245285156325, 2.6574184961115392, 2.656241364471235, 2.654245407123284, 2.6508977965181098, 2.64486537817468, 2.634399963717155, 2.6194312145753056, 2.594611665493945, 2.576835778742197, 2.5562576691701104, 2.5379489118046754, 2.523764416893519, 2.5119219758044715, 2.4985999884863794, 2.4835250745657436, 2.471968212738413, 2.459571190459779, 2.448198955438799, 2.44000085273204, 2.432517605266352, 2.429815295098842, 2.4067456138936563, 2.3989240684728514, 2.390946846290175, 2.377232815831753, 2.3686290906959373, 2.3568520788684464, 2.3514967970855913, 2.3355418809724755, 2.3373346219117614, 2.3175236568075097, 2.3071988572432294, 2.3094968079346154, 2.2902731194676242, 2.299176329658145, 2.272633242489669, 2.262732420844593, 2.248563612231676, 2.2410635932521474, 2.242394678306893, 2.228475609827903, 2.213832999293636, 2.2044979061790677, 2.1971200592999387, 2.1871488615014085, 2.1819004350890863, 2.1724522951592755, 2.168495648404452, 2.16373270329192, 2.16412715293308, 2.1549794462514043, 2.1410265728366396, 2.1440730944447135, 2.1385441139609553, 2.1303330172458894, 2.1223486456377754, 2.121791143526976, 2.1486596051108076, 2.1101788891164346, 2.1245901236197438, 2.115674964704341, 2.1028211794071794, 2.128860053561983, 2.1398437297207185, 2.108263713581417, 2.0969754827433618, 2.0907889829676334, 2.1083247375801473, 2.0855713176414103, 2.0795316925189766, 2.0773913337679333, 2.0799645024958893, 2.081575168177412, 2.1083058915506245, 2.086388306077478, 2.0697489011855352, 2.0659691563185016, 2.0591146525099555, 2.058379267432615, 2.080622462020523, 2.054320669330791, 2.0556089835018163, 2.0587090218595683, 2.045949452812057, 2.0454243623172905, 2.0492987842199644, 2.047353216579982, 2.038702734585466, 2.0494293123238974, 2.040237844479691, 2.040310845390721, 2.035360160328093, 2.036532905888675, 2.0330029230791165, 2.0276014385943735, 2.0284399563455815, 2.030806951530657, 2.036279630582712, 2.030188574383803, 2.022635340690613, 2.018763356803869, 2.0171337245133123, 2.0178538838826574, 2.0259647424193634, 2.0193090767695985, 2.0251197294257155, 2.0191853780464584, 2.0257538063772795, 2.013171448300429, 2.011683980232389, 2.013301713321792, 2.007130241159148, 2.0215484152482257, 2.0145169480876577, 2.01110652534441, 2.0061527786192244, 2.0077613656743996, 2.0273618363394528, 2.004481124760482, 2.006840595079369, 2.0058187622154873, 2.0132973245016266, 2.0017067628541017, 1.996567778790917, 2.0002810882621604, 1.997088859821188, 2.013409945765152, 1.9941771308385288, 2.086302769399433, 1.9962126345470035, 2.0346854844899798, 2.005695984085597, 2.0404015754048266, 1.992652621175268, 1.9975970719248204, 1.9898613534733187, 1.99076871351264, 2.0059191530756957, 1.9942747376039502, 1.9861843145539608, 2.005293991961111, 1.9849326134902503, 1.9853713988083337, 1.9848038222402187, 1.9841254632461247, 1.9802207018941493, 1.9870068214601289, 1.9808749696499803, 1.9948333329559351, 1.991085781643935, 1.9776645941883082, 1.9858693673301409, 1.9789822019379715, 1.977495371023031, 1.9802937149414288, 1.9866237462252037, 1.9749879721546018, 1.9758754559534133, 1.9801009666352045, 1.990124981587352, 1.976345146230876, 1.9837599399641817, 1.9833190952028548, 1.973281267241304, 1.9734391494729053, 1.9854939539835763, 1.9690137645489671, 1.9970543558765905, 1.9712117787065178, 1.9821668579464866, 1.9699069341806748, 1.9832971972980717, 1.9703517488658135, 1.9767331098296568, 1.9699501819015528, 1.9696459143815566, 1.9679518133548681, 1.967546678528997, 1.9667583664845558, 1.9708583983294483, 1.9628042250822721, 1.9672974404834567, 1.9647497756923558, 1.978478896206823, 1.9617671373442476, 1.975963168543548, 1.9690610546196623, 1.9656400893905088, 1.970190021400577, 1.9599406623292244, 1.981405341174998, 1.9596472647781247, 1.9789412111680105, 1.9632365648773895, 1.961427082960633, 1.9605362354632474, 1.962876919063637, 1.9585231881227791, 1.9573293393860114, 1.9612347298655017, 1.9614017330758482, 1.9736702220976254, 1.9542296872350382, 1.968645935379617, 1.9508050844587128, 1.96277937943908, 1.9566435653392122, 1.9498832746483814, 1.9523494543113145, 1.9594817729223342, 1.9703280968814845, 1.9487888274717409, 1.9530165134783841, 1.9589318162310496, 1.9542880234459938, 1.94514826444178, 1.949173756812398, 1.9464153417421288, 1.9442615240861238, 1.949509868872381, 1.948333838498847, 1.9403057853968078, 1.9546093196900216, 1.9430544810929322, 1.9394107518720705, 1.9392852747968852, 1.9336038313084243, 1.9427778382215202, 1.934944764538156, 1.9302343774115902, 1.9316568926637396, 1.9267556624263769, 1.919128218503617, 1.9177115995112703, 1.9120839172591912, 1.9123515671697155, 1.914010606767313, 1.9021530593753058, 1.9025722988720597, 1.8977908753409174, 1.8959150400459277, 1.9003382267427367, 1.8919058605563661, 1.8829291352301787, 1.8879599005522203, 1.880986726147005, 1.8931812511876298, 1.9012838721471075, 1.9087596411383994, 1.871212104466944, 1.87708682966937, 1.8664643684240005, 1.8571768259180004, 1.8620689682576848, 1.8524372321240028, 1.8477290522288807, 1.8438807162158009, 1.8383137537732304, 1.842217382734828, 1.8386033325163993, 1.8324777663047678, 1.829514201051496, 1.8401980065359858, 1.8386660205515344, 1.88535694144238, 1.8413451810188481, 1.8381302742339511, 1.8296165757970073, 1.836962216593362, 1.8455966438957427, 1.824052100698349, 1.8221103381640806, 1.8187514660980901], 'val_acc': [0.08374384235841496, 0.08374384235841496, 0.08374384235841496, 0.08374384235841496, 0.0886699507327992, 0.08374384235841496, 0.08374384235841496, 0.08374384235841496, 0.08210180583601123, 0.08374384235841496, 0.08210180583601123, 0.08210180583601123, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10180623963328418, 0.10673234800766841, 0.11001642025725791, 0.11494252833190614, 0.11001641985964893, 0.11165845598444367, 0.13300492520304932, 0.16748768411735793, 0.17405582891015584, 0.16091953952030597, 0.19540229872823348, 0.19540229863036052, 0.21674876636863732, 0.2036124772724064, 0.19376026250556577, 0.17241379258961514, 0.17898193708879412, 0.1970443349509012, 0.1806239735072078, 0.18062397370295377, 0.19540229655055968, 0.18883415403330855, 0.18719211800638677, 0.192118226478644, 0.20032840492494391, 0.19211822657651698, 0.19868637087995, 0.20197044114761165, 0.20197044104973866, 0.20853858772659145, 0.20525451547700196, 0.21182265987830795, 0.22331691294761713, 0.20197044104973866, 0.21510673014596962, 0.21839080249343207, 0.22495894709048403, 0.21674876617289138, 0.2167487664665103, 0.2266009832152788, 0.2216748768228224, 0.22003283891184577, 0.23316912742083884, 0.22495894709048403, 0.23316912969638562, 0.21510673004809663, 0.24137930814268554, 0.22331691076994334, 0.2413793083384315, 0.23481116383925252, 0.2282430194379465, 0.2627257779607632, 0.2430213442674803, 0.24958948925602417, 0.2610837421295874, 0.24958948925602417, 0.23973727221363675, 0.2643678142813039, 0.2643678144770499, 0.23973727280087465, 0.257799669390633, 0.24466338078376695, 0.2577996698799979, 0.26272577825438215, 0.24794745293548345, 0.2643678142813039, 0.2610837421295874, 0.27093595887835586, 0.27750410337753484, 0.26272577845012807, 0.2594417056133007, 0.25451559743466246, 0.27093595897622885, 0.2709359587804829, 0.2791461393065836, 0.2791461395023296, 0.2758620671548671, 0.288998356544717, 0.2922824286964335, 0.2758620671548671, 0.28243021194766504, 0.29064039257163876, 0.28899835625109804, 0.27257799480740463, 0.295566501043896, 0.28899835625109804, 0.2807881756271243, 0.2807881756271243, 0.29556650084815, 0.29720853687507176, 0.2873563202241763, 0.2840722479745868, 0.2922824285985605, 0.28407224768096784, 0.28899835595747914, 0.29720853658145285, 0.29720853677719883, 0.295566500946023, 0.2939244645276093, 0.2955665006524041, 0.30049260892891533, 0.28571428390363557, 0.2873563198326844, 0.3004926090267883, 0.2988505728041206, 0.2955665005545311, 0.2873563201263033, 0.30213464515158306, 0.30049260892891533, 0.295566500750277, 0.2955665003587851, 0.3070607535259673, 0.2955665006524041, 0.2988505729998665, 0.30049260922253423, 0.30049260912466125, 0.28899835615322506, 0.3070607536238403, 0.3021346448579641, 0.30541871730329956, 0.3070607536238403, 0.308702789748635, 0.30213464515158306, 0.2955665005545311, 0.3037766813742508, 0.2857142835121437, 0.3119868619982245, 0.2873563198326844, 0.29228242840281454, 0.30213464515158306, 0.3119868619982245, 0.3136288983187652, 0.315270934247814, 0.31855500659527647, 0.3070607533302213, 0.3070607536238403, 0.31034482587342976, 0.30706075372171326, 0.3103448257755568, 0.3185550064974035, 0.3119868619003515, 0.32183907884486596, 0.31855500659527647, 0.3004926090267883, 0.315270934149941, 0.3136288979272733, 0.321839078551247, 0.315270934247814, 0.31362889822089224, 0.31855500659527647, 0.315270934247814, 0.315270934345687, 0.30049260922253423, 0.3103448257755568, 0.32019704252432524, 0.321839078551247, 0.3185550063016575, 0.3136288978294003, 0.30541871720542657, 0.31362889802514626, 0.3201970426221982, 0.3103448257755568, 0.30049260922253423, 0.3136288979272733, 0.321839078551247, 0.308702789748635, 0.302134645347329, 0.31691297027473575, 0.31527093395419503, 0.3103448257755568, 0.308702789748635, 0.30213464524945605, 0.3119868619003515, 0.32019704242645225, 0.32512315109445544, 0.3185550063995305, 0.32019704242645225, 0.30870278965076203, 0.3169129704704817, 0.3185550063995305, 0.315270934052068, 0.31691297037260874, 0.30213464544520197, 0.3119868619982245, 0.3267651870235042, 0.32183907864912, 0.3103448257755568, 0.2988505730977395, 0.30541871740117255, 0.315270934052068, 0.32183907884486596, 0.32183907884486596, 0.32840722334404493, 0.32183907864912, 0.3070607536238403, 0.308702789846508, 0.3185550063995305, 0.3267651869256312, 0.3267651870235042, 0.30870278965076203, 0.3119868619982245, 0.31362889812301925, 0.3267651870235042, 0.32019704242645225, 0.30541871749904553, 0.31362889812301925, 0.33169129530001545, 0.3136288979272733, 0.31034482597130275, 0.3103448260691757, 0.31527093395419503, 0.3333333315226832, 0.315270934247814, 0.30541871740117255, 0.3234811148717877, 0.3382594400928134, 0.321839078453374, 0.3234811144802958, 0.32840722324617194, 0.30213464524945605, 0.32840722324617194, 0.32512315089870947, 0.33169129549576143, 0.32183907874699297, 0.3169129704704817, 0.32183907874699297, 0.32019704242645225, 0.3284072234419179, 0.33990147611973515, 0.32840722334404493, 0.33004925956671266, 0.3431855484671976, 0.3464696207167871, 0.3431855485650706, 0.34482758478773834, 0.3579638738839693, 0.3497536935536145, 0.34318554885868957, 0.34482758478773834, 0.3431855484671976, 0.34318554885868957, 0.3513957295805363, 0.36288998264984546, 0.3497536927706307, 0.3563218381506665, 0.32676518751286915, 0.3711001631759462, 0.3628899823562265, 0.3711001629802002, 0.36945812695327845, 0.353037765901077, 0.3825944161473824, 0.3760262716482034, 0.37110016366531107, 0.3743842358170276, 0.3908045968692291, 0.38587848839697186, 0.38259441634312835, 0.3809523802183336, 0.3760262712567115, 0.36124794632930474, 0.35796387447120714, 0.3711001631759462, 0.3842364520764312, 0.3661740550951809, 0.36617405489943494, 0.37766830747937924, 0.37766830777299815, 0.3793103439956659, 0.38916256054868836], 'loss': [2.69754931099361, 2.6905131749303925, 2.684633916702114, 2.681018743867502, 2.6779456978705873, 2.6755397292378014, 2.6732772296459033, 2.67128285764424, 2.6696466390112343, 2.6679358638042787, 2.6664848903366183, 2.665326680980424, 2.6644311306657733, 2.6636579867016366, 2.662841555616939, 2.6622360510502996, 2.6618524024618724, 2.6613631581378914, 2.6611420211361176, 2.660999825847712, 2.6607689686134854, 2.6604646935355247, 2.66016779660689, 2.6599933576290122, 2.6597040622875676, 2.6595159133112163, 2.659032869338989, 2.6585104390091474, 2.657773236131766, 2.6561655647700815, 2.653970356300871, 2.650194658831649, 2.6433960165576034, 2.631689208733717, 2.6150670960208964, 2.5913452301182054, 2.5737174706782158, 2.551409633154742, 2.53816572508528, 2.523831836398867, 2.5099111405241414, 2.498150837250069, 2.4858089969877835, 2.4750598160393182, 2.4646022821109153, 2.455039744896076, 2.4490568380336253, 2.443928515299145, 2.4261684768253775, 2.4187603358125784, 2.4067932950397783, 2.397336876416843, 2.3867156109036363, 2.3756731269295943, 2.3661844647151, 2.3605515948066476, 2.3511261194883186, 2.3388935311374235, 2.3287495595473775, 2.3216351508115105, 2.3178066442634537, 2.298568743308222, 2.3024830842654564, 2.284145258780133, 2.270833616139218, 2.2568010975448014, 2.254736622060349, 2.2427232492630975, 2.2347069804918105, 2.220736648120919, 2.2117639386923162, 2.204882260367611, 2.198687201114161, 2.1887766934028643, 2.1809714047081417, 2.181946165507824, 2.1784609596832087, 2.1651329312725967, 2.1599090877744453, 2.1535683245139934, 2.1559579914844993, 2.1508655108465553, 2.1416080912525404, 2.1323052602137382, 2.1341136296916545, 2.146174602880615, 2.1236436791978086, 2.128095831763328, 2.119629996317368, 2.1208698118002265, 2.1162497796561928, 2.1243954391205335, 2.113895111260228, 2.1242065612785135, 2.0981443684203915, 2.1037236366428633, 2.093560311436898, 2.0823561459840936, 2.0849845416247232, 2.087521445824625, 2.084927857044541, 2.094945165410913, 2.086422102945786, 2.0682149889043226, 2.0644334600201866, 2.0595422026068277, 2.058788693073594, 2.063012066220356, 2.0537528652919637, 2.0575438221377267, 2.053153060838672, 2.044657785153242, 2.0464277398659707, 2.045544067985958, 2.0396380042148565, 2.036160798728833, 2.0425297121247716, 2.0400926995326363, 2.0358772791876194, 2.03984195995135, 2.040834876790918, 2.0261018587578494, 2.0217701517336177, 2.0226281803980988, 2.0257339118197715, 2.022901877534463, 2.0168715302704294, 2.0181754232187292, 2.013630032000845, 2.0134357835233088, 2.017629580625029, 2.010780547923376, 2.0167941947247705, 2.01007762488888, 2.017636897872361, 2.0109781903163118, 2.0066035780818554, 2.008815674615347, 2.028562036725775, 2.030887488613873, 2.0155276595199867, 1.9998607076903387, 1.9972111912478656, 1.999148219126206, 1.998283638503762, 2.002908205937066, 2.0005430749309627, 2.0029920695498737, 1.995277327629575, 1.9970611954127004, 1.9878321144370328, 1.9894583601726399, 1.9891253186447175, 1.987146908448707, 2.0119306377316892, 2.0012944648887587, 2.0276892891165166, 2.006193923068977, 2.0112465195587284, 2.001459196118114, 1.9988363480421063, 1.9895504150547285, 1.9828116131024685, 1.9776489595612952, 1.9783549331786452, 1.9783881959973908, 1.9799401883471919, 1.9780394241795158, 1.9816419259233886, 1.9731939698146843, 1.9694287031827766, 1.9697219274861613, 1.9698639159329863, 1.9653560615418137, 1.9738877124120566, 1.9691312585278458, 1.9713065343715817, 1.9687191834929543, 1.9693390790441931, 1.9664864413302536, 1.9606870600819832, 1.958147155332859, 1.9653841156734333, 1.9674441940730603, 1.9596839052450976, 1.9655253104850252, 1.966465879955331, 1.9618770629718318, 1.9612550173452012, 1.9657571267787926, 1.9626634852352574, 1.9584032476315507, 1.9580860667649724, 1.96150320873613, 1.9543700629436016, 1.9562572706406611, 1.9549409162337286, 1.9608799962782029, 1.9494174382525058, 1.94921916211655, 1.9551697344750594, 1.9536892145321354, 1.9634841557156133, 1.9596271523215198, 1.943618321320849, 1.9435498089271404, 1.9436018841467355, 1.946713719181946, 1.9464295363768904, 1.9526121985985758, 1.9427731168833111, 1.9448902472823062, 1.9446282792140326, 1.9513913713686275, 1.9439676451732002, 1.9402539847078264, 1.943210529448805, 1.9487566799598552, 1.9501164880376576, 1.947560459093881, 1.9451054023766174, 1.9373080039660788, 1.9336166278047973, 1.9392155011821333, 1.9287844806725973, 1.9322735124544932, 1.9376637255631433, 1.9322047785811844, 1.9343932387765184, 1.9531404718970862, 1.9370427045489238, 1.931314667930838, 1.926773144968726, 1.926289129648855, 1.9258107213269025, 1.9220375298474603, 1.9306974549068319, 1.9336888069000087, 1.9303790075823022, 1.9242657025002357, 1.922384369495713, 1.9251157758172288, 1.922889178685339, 1.9213400266008944, 1.9164017460919014, 1.9139397813554173, 1.915081016924347, 1.9161938207105444, 1.919013783721219, 1.9152761941572969, 1.9143405001266292, 1.9137019116286136, 1.9024200516314966, 1.9042741119004862, 1.9078228315533554, 1.9090506702477927, 1.9117226591345222, 1.902211912554637, 1.9050773308262443, 1.8878165808056904, 1.8854571080550522, 1.8848274530571345, 1.8819965278343498, 1.878833804531998, 1.877887523002938, 1.8773889106891484, 1.8737980145938098, 1.8708768303144638, 1.8721956930121357, 1.8731505463989853, 1.8638392328481654, 1.8607156663459918, 1.8620924609887282, 1.855147052202871, 1.874052572984715, 1.8699316506023524, 1.8641641906154718, 1.8456866909101515, 1.8468091197082395, 1.8418793458468616, 1.841732276195863, 1.8369179941055955, 1.8445332025845194, 1.8314971158636668, 1.8268542475768919, 1.8306842719260183, 1.8313172352632212, 1.8313834773441604, 1.8267057458478078, 1.8240217283765883, 1.830970121360168, 1.8342968983327095, 1.8451597147164158, 1.8232449744026764, 1.8133701669117264, 1.8228478477720853, 1.8202193105979623, 1.8089084588037134, 1.8105363018458873, 1.8019943312697833], 'acc': [0.08295687882256458, 0.08295687879502651, 0.08295687939168492, 0.08295687920503793, 0.08583162271817361, 0.0809034913105152, 0.08295687899085286, 0.08295687859920016, 0.07967145845630576, 0.0809034905272098, 0.08336755680474903, 0.08131416891940565, 0.08172484574499071, 0.10225872704013417, 0.10225872723596052, 0.10225872644347576, 0.10225872664848147, 0.10225872722678116, 0.10225872684430783, 0.10225872723596052, 0.10225872663930212, 0.10225872645265513, 0.10225872643429641, 0.10225872643429641, 0.10225872663930212, 0.10225872703095482, 0.10225872685348718, 0.10225872722678116, 0.10225872743178686, 0.10225872742260751, 0.10225872645265513, 0.10266940405236623, 0.10677618042889073, 0.10636550302500597, 0.10800821369311159, 0.10759753605668305, 0.11909650936263787, 0.1408624228877943, 0.14455852264373945, 0.16139630302633837, 0.16427104615700072, 0.16878850093490044, 0.17330595373617794, 0.17700205329629676, 0.16673511304037772, 0.16673511225707233, 0.1687885019140322, 0.1753593420407121, 0.1708418894169022, 0.1757700200320759, 0.18110882944027745, 0.17535934223653843, 0.17823408691545287, 0.18275153993091545, 0.1885010263880665, 0.1880903495716608, 0.18316221717569128, 0.1909650914722889, 0.19835728883008938, 0.19178644694097233, 0.19630390132721934, 0.1991786448495344, 0.20205338835349074, 0.20657084256227012, 0.2119096511871663, 0.21067761815548922, 0.21765913686101196, 0.21519507193589846, 0.21478439355288198, 0.22217659046395358, 0.22710472207661772, 0.2266940440668952, 0.2242299791417817, 0.22546201317094924, 0.23203285390346692, 0.2344969197709947, 0.2275153996763288, 0.2353182744196553, 0.23162217585702696, 0.23860369614751922, 0.2390143727864573, 0.23901437378394774, 0.24928131480481344, 0.2443531843671074, 0.24599589405608127, 0.2513347015243781, 0.2583162235773075, 0.24722792710611707, 0.2509240232821118, 0.25585215767306224, 0.2505133484423283, 0.2616016417802971, 0.2574948685369942, 0.2533880909855116, 0.2714579039897762, 0.26858316402905286, 0.26981519531665155, 0.27597535997935146, 0.2673511307831907, 0.2628336744386802, 0.26611909832063396, 0.2735112948584116, 0.271868581607846, 0.2751540051348645, 0.28049281158731215, 0.2796714555678671, 0.28213552245124407, 0.2772073898594482, 0.2891170417626046, 0.2792607785372763, 0.2850102671852347, 0.2817248436582162, 0.2804928143655985, 0.28418891273240043, 0.28542094515823974, 0.28747433485191703, 0.2837782339393725, 0.28624229984361776, 0.2866529764458384, 0.2833675586712189, 0.2952772054461728, 0.2903490779825794, 0.2915811086459816, 0.28911704352504175, 0.2911704304404327, 0.2887063649278402, 0.29199178865068504, 0.2948665295905401, 0.2952772062294782, 0.2977412709220479, 0.2928131397377539, 0.29527720642530453, 0.29609856205309687, 0.2989733081578719, 0.29199178528491965, 0.2993839829976554, 0.2977412711545917, 0.2940451747460532, 0.28829568554733326, 0.2833675584753926, 0.2973305949073063, 0.3006160144810804, 0.3002053368630106, 0.29938398221435, 0.3018480500768587, 0.29691991709341015, 0.3002053390171004, 0.2952772093994172, 0.3006160164393439, 0.3006160170268229, 0.2969199168975838, 0.2981519483810088, 0.3030800827352418, 0.300205338233795, 0.2977412711545917, 0.2993839829976554, 0.29158111005348347, 0.2973305933406955, 0.28993839977703056, 0.2878850114908551, 0.296509239866993, 0.3002053370588369, 0.2985626295606703, 0.3002053378421423, 0.30472279359917376, 0.3006160176510194, 0.30431211363118776, 0.30431211480614584, 0.3006160152643858, 0.3047227904659522, 0.307597538137338, 0.3121149907611479, 0.3067761787521276, 0.3117043121272289, 0.3051334698464591, 0.31170431232305523, 0.3051334688673274, 0.30677617836047494, 0.3112936366632489, 0.3108829590451791, 0.31129363333420096, 0.31252566834250023, 0.3092402457946135, 0.3084188890285805, 0.3096509257993169, 0.31047227927301946, 0.30595482332016166, 0.3039014395379922, 0.3112936362715962, 0.30841888961605957, 0.30800821238964243, 0.3112936358799435, 0.3104722771189296, 0.3092402468104627, 0.3141683788147795, 0.3186858310469367, 0.31334702439866274, 0.3088295700124157, 0.31745379858437994, 0.3129363432190012, 0.31293634517726465, 0.3117043125188816, 0.30718685676185015, 0.3158110867045988, 0.3203285440649585, 0.3174537979969009, 0.31745379897603265, 0.31498973189682933, 0.31909651062327, 0.31663244550233016, 0.3166324429565876, 0.3108829582618737, 0.31622176631764953, 0.3149897326801347, 0.31334702259950814, 0.316632443739893, 0.3162217672967813, 0.31334702259950814, 0.3174537987802063, 0.31622176631764953, 0.312936345373091, 0.3182751538205196, 0.3158110881121007, 0.31704312096631015, 0.3207392199205912, 0.3186858318302421, 0.3145790546704122, 0.3174537964302901, 0.32073922109554925, 0.31088295884935274, 0.3129363438064802, 0.3182751541754548, 0.31991786664271504, 0.319917862138709, 0.3195071870663817, 0.3232032862164891, 0.31745380113012245, 0.3137577011967097, 0.31498973483422454, 0.3223819287895422, 0.3227926101650301, 0.3182751561704358, 0.3248459980595528, 0.32156057652751524, 0.322381929609565, 0.3240246418442814, 0.3219712517956689, 0.32156057613586253, 0.3203285413233896, 0.32731006318049266, 0.3149897326801347, 0.3186858314385894, 0.3273100615771644, 0.3355236117844709, 0.3260780307179359, 0.32772073707786187, 0.32566735094577626, 0.32977412849725884, 0.33305954627187834, 0.33470225873913856, 0.33347022428160084, 0.3359342897941934, 0.34086242457679655, 0.33757700304475896, 0.33511293733634007, 0.3371663267974736, 0.34127309863327465, 0.34825461990289863, 0.34743326627008725, 0.34537987716388896, 0.3490759773298455, 0.35112936283773466, 0.3498973313910271, 0.36057494824916675, 0.3482546185321142, 0.3494866523654554, 0.35770020453102536, 0.36139630211452195, 0.36303901356593293, 0.3605749480533404, 0.36386036798204974, 0.3638603719720116, 0.36098562410479945, 0.36632443548962323, 0.37002053366059884, 0.3691991764661957, 0.36221765891727237, 0.3700205328772935, 0.36632443490214417, 0.36303901239097486, 0.36550307849104646, 0.3585215591796859, 0.3626283351645577, 0.37207392175094794, 0.3761807004773886, 0.3671457917048946, 0.3802874741123442, 0.38028747528730233, 0.3745379862476913, 0.3819301863470606]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
