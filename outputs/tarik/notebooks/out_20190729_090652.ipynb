{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf87.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 09:06:53 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '3Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000021E04AA9550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000021E765A7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0948, Accuracy:0.3729, Validation Loss:1.0858, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0821, Accuracy:0.3729, Validation Loss:1.0781, Validation Accuracy:0.3727\n",
    "Epoch #3: Loss:1.0768, Accuracy:0.3729, Validation Loss:1.0749, Validation Accuracy:0.3727\n",
    "Epoch #4: Loss:1.0739, Accuracy:0.3910, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0738, Validation Accuracy:0.3810\n",
    "Epoch #19: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0739, Validation Accuracy:0.3810\n",
    "Epoch #20: Loss:1.0737, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3826\n",
    "Epoch #21: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #22: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #23: Loss:1.0738, Accuracy:0.3975, Validation Loss:1.0740, Validation Accuracy:0.3810\n",
    "Epoch #24: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #25: Loss:1.0736, Accuracy:0.3971, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #26: Loss:1.0734, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #27: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0738, Validation Accuracy:0.3826\n",
    "Epoch #28: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #29: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.3875\n",
    "Epoch #30: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #31: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3810\n",
    "Epoch #32: Loss:1.0732, Accuracy:0.4025, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #33: Loss:1.0733, Accuracy:0.4033, Validation Loss:1.0739, Validation Accuracy:0.3842\n",
    "Epoch #34: Loss:1.0732, Accuracy:0.4008, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #35: Loss:1.0731, Accuracy:0.4037, Validation Loss:1.0739, Validation Accuracy:0.3875\n",
    "Epoch #36: Loss:1.0731, Accuracy:0.3971, Validation Loss:1.0738, Validation Accuracy:0.3842\n",
    "Epoch #37: Loss:1.0731, Accuracy:0.3984, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #38: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #39: Loss:1.0731, Accuracy:0.4008, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #40: Loss:1.0730, Accuracy:0.4025, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #41: Loss:1.0730, Accuracy:0.4021, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #42: Loss:1.0731, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #43: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #44: Loss:1.0729, Accuracy:0.4053, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #45: Loss:1.0727, Accuracy:0.4041, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #46: Loss:1.0726, Accuracy:0.4025, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #47: Loss:1.0726, Accuracy:0.4000, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #48: Loss:1.0727, Accuracy:0.4016, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #49: Loss:1.0729, Accuracy:0.4033, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #50: Loss:1.0722, Accuracy:0.4000, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #51: Loss:1.0728, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #52: Loss:1.0724, Accuracy:0.4012, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #53: Loss:1.0725, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #54: Loss:1.0723, Accuracy:0.3996, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #55: Loss:1.0726, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #56: Loss:1.0722, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #57: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #58: Loss:1.0724, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3859\n",
    "Epoch #59: Loss:1.0727, Accuracy:0.4008, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0719, Accuracy:0.4041, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #61: Loss:1.0730, Accuracy:0.4029, Validation Loss:1.0733, Validation Accuracy:0.3859\n",
    "Epoch #62: Loss:1.0721, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #63: Loss:1.0736, Accuracy:0.4012, Validation Loss:1.0737, Validation Accuracy:0.3826\n",
    "Epoch #64: Loss:1.0731, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #65: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #66: Loss:1.0734, Accuracy:0.3988, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #67: Loss:1.0730, Accuracy:0.3992, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #68: Loss:1.0729, Accuracy:0.4070, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #69: Loss:1.0729, Accuracy:0.4090, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #70: Loss:1.0727, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3826\n",
    "Epoch #71: Loss:1.0728, Accuracy:0.4029, Validation Loss:1.0740, Validation Accuracy:0.3826\n",
    "Epoch #72: Loss:1.0730, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #73: Loss:1.0729, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3875\n",
    "Epoch #74: Loss:1.0728, Accuracy:0.4037, Validation Loss:1.0743, Validation Accuracy:0.3744\n",
    "Epoch #75: Loss:1.0732, Accuracy:0.4000, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #76: Loss:1.0731, Accuracy:0.4012, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #77: Loss:1.0727, Accuracy:0.4016, Validation Loss:1.0732, Validation Accuracy:0.3941\n",
    "Epoch #78: Loss:1.0728, Accuracy:0.4041, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #79: Loss:1.0726, Accuracy:0.4057, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #80: Loss:1.0723, Accuracy:0.4033, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #81: Loss:1.0722, Accuracy:0.4025, Validation Loss:1.0728, Validation Accuracy:0.3842\n",
    "Epoch #82: Loss:1.0718, Accuracy:0.4066, Validation Loss:1.0732, Validation Accuracy:0.3826\n",
    "Epoch #83: Loss:1.0726, Accuracy:0.4066, Validation Loss:1.0731, Validation Accuracy:0.4007\n",
    "Epoch #84: Loss:1.0725, Accuracy:0.4086, Validation Loss:1.0728, Validation Accuracy:0.3941\n",
    "Epoch #85: Loss:1.0731, Accuracy:0.4021, Validation Loss:1.0728, Validation Accuracy:0.3859\n",
    "Epoch #86: Loss:1.0735, Accuracy:0.4012, Validation Loss:1.0728, Validation Accuracy:0.3859\n",
    "Epoch #87: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #88: Loss:1.0724, Accuracy:0.3996, Validation Loss:1.0728, Validation Accuracy:0.3908\n",
    "Epoch #89: Loss:1.0727, Accuracy:0.4004, Validation Loss:1.0727, Validation Accuracy:0.3908\n",
    "Epoch #90: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0728, Validation Accuracy:0.3875\n",
    "Epoch #91: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #92: Loss:1.0726, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #93: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #94: Loss:1.0727, Accuracy:0.3988, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #95: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #96: Loss:1.0726, Accuracy:0.3984, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #97: Loss:1.0725, Accuracy:0.4041, Validation Loss:1.0735, Validation Accuracy:0.3892\n",
    "Epoch #98: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0735, Validation Accuracy:0.3892\n",
    "Epoch #99: Loss:1.0719, Accuracy:0.3979, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #100: Loss:1.0724, Accuracy:0.4000, Validation Loss:1.0732, Validation Accuracy:0.3892\n",
    "Epoch #101: Loss:1.0720, Accuracy:0.4000, Validation Loss:1.0730, Validation Accuracy:0.3875\n",
    "Epoch #102: Loss:1.0722, Accuracy:0.3992, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #103: Loss:1.0721, Accuracy:0.4012, Validation Loss:1.0731, Validation Accuracy:0.3875\n",
    "Epoch #104: Loss:1.0719, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #105: Loss:1.0719, Accuracy:0.4025, Validation Loss:1.0729, Validation Accuracy:0.3908\n",
    "Epoch #106: Loss:1.0716, Accuracy:0.3959, Validation Loss:1.0732, Validation Accuracy:0.3875\n",
    "Epoch #107: Loss:1.0716, Accuracy:0.3996, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #108: Loss:1.0715, Accuracy:0.4033, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #109: Loss:1.0714, Accuracy:0.4016, Validation Loss:1.0729, Validation Accuracy:0.3859\n",
    "Epoch #110: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0729, Validation Accuracy:0.3875\n",
    "Epoch #111: Loss:1.0708, Accuracy:0.4033, Validation Loss:1.0731, Validation Accuracy:0.3859\n",
    "Epoch #112: Loss:1.0710, Accuracy:0.4107, Validation Loss:1.0726, Validation Accuracy:0.3908\n",
    "Epoch #113: Loss:1.0709, Accuracy:0.3992, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #114: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #115: Loss:1.0711, Accuracy:0.4136, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #116: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0740, Validation Accuracy:0.3892\n",
    "Epoch #117: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #118: Loss:1.0706, Accuracy:0.4029, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #119: Loss:1.0706, Accuracy:0.4094, Validation Loss:1.0735, Validation Accuracy:0.3892\n",
    "Epoch #120: Loss:1.0706, Accuracy:0.4033, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #121: Loss:1.0703, Accuracy:0.4016, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #122: Loss:1.0706, Accuracy:0.4062, Validation Loss:1.0736, Validation Accuracy:0.3875\n",
    "Epoch #123: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #124: Loss:1.0706, Accuracy:0.3988, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #125: Loss:1.0708, Accuracy:0.4029, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #126: Loss:1.0705, Accuracy:0.4045, Validation Loss:1.0736, Validation Accuracy:0.3859\n",
    "Epoch #127: Loss:1.0703, Accuracy:0.4066, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #128: Loss:1.0702, Accuracy:0.4070, Validation Loss:1.0732, Validation Accuracy:0.3908\n",
    "Epoch #129: Loss:1.0695, Accuracy:0.4127, Validation Loss:1.0737, Validation Accuracy:0.3908\n",
    "Epoch #130: Loss:1.0708, Accuracy:0.4131, Validation Loss:1.0734, Validation Accuracy:0.3908\n",
    "Epoch #131: Loss:1.0704, Accuracy:0.4172, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #132: Loss:1.0704, Accuracy:0.4168, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #133: Loss:1.0706, Accuracy:0.4090, Validation Loss:1.0733, Validation Accuracy:0.3892\n",
    "Epoch #134: Loss:1.0711, Accuracy:0.4127, Validation Loss:1.0731, Validation Accuracy:0.3793\n",
    "Epoch #135: Loss:1.0706, Accuracy:0.4127, Validation Loss:1.0725, Validation Accuracy:0.3908\n",
    "Epoch #136: Loss:1.0704, Accuracy:0.4140, Validation Loss:1.0725, Validation Accuracy:0.3810\n",
    "Epoch #137: Loss:1.0710, Accuracy:0.4053, Validation Loss:1.0725, Validation Accuracy:0.3924\n",
    "Epoch #138: Loss:1.0704, Accuracy:0.4123, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #139: Loss:1.0706, Accuracy:0.4103, Validation Loss:1.0735, Validation Accuracy:0.3810\n",
    "Epoch #140: Loss:1.0709, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3826\n",
    "Epoch #141: Loss:1.0705, Accuracy:0.4131, Validation Loss:1.0736, Validation Accuracy:0.3908\n",
    "Epoch #142: Loss:1.0705, Accuracy:0.4074, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #143: Loss:1.0706, Accuracy:0.4164, Validation Loss:1.0747, Validation Accuracy:0.3760\n",
    "Epoch #144: Loss:1.0700, Accuracy:0.4127, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #145: Loss:1.0706, Accuracy:0.4066, Validation Loss:1.0740, Validation Accuracy:0.4072\n",
    "Epoch #146: Loss:1.0710, Accuracy:0.4090, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #147: Loss:1.0702, Accuracy:0.4127, Validation Loss:1.0741, Validation Accuracy:0.3826\n",
    "Epoch #148: Loss:1.0710, Accuracy:0.4094, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #149: Loss:1.0709, Accuracy:0.4049, Validation Loss:1.0742, Validation Accuracy:0.3810\n",
    "Epoch #150: Loss:1.0702, Accuracy:0.4086, Validation Loss:1.0728, Validation Accuracy:0.4072\n",
    "Epoch #151: Loss:1.0711, Accuracy:0.4070, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #152: Loss:1.0704, Accuracy:0.4099, Validation Loss:1.0732, Validation Accuracy:0.3810\n",
    "Epoch #153: Loss:1.0700, Accuracy:0.4078, Validation Loss:1.0737, Validation Accuracy:0.3793\n",
    "Epoch #154: Loss:1.0701, Accuracy:0.4119, Validation Loss:1.0739, Validation Accuracy:0.3810\n",
    "Epoch #155: Loss:1.0702, Accuracy:0.4099, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #156: Loss:1.0710, Accuracy:0.4029, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #157: Loss:1.0708, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #158: Loss:1.0699, Accuracy:0.4148, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #159: Loss:1.0698, Accuracy:0.4066, Validation Loss:1.0733, Validation Accuracy:0.3892\n",
    "Epoch #160: Loss:1.0699, Accuracy:0.4086, Validation Loss:1.0735, Validation Accuracy:0.3842\n",
    "Epoch #161: Loss:1.0720, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.4154\n",
    "Epoch #162: Loss:1.0749, Accuracy:0.4053, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #163: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0780, Validation Accuracy:0.3908\n",
    "Epoch #164: Loss:1.0727, Accuracy:0.3988, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #165: Loss:1.0707, Accuracy:0.4037, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #166: Loss:1.0710, Accuracy:0.4045, Validation Loss:1.0748, Validation Accuracy:0.3777\n",
    "Epoch #167: Loss:1.0701, Accuracy:0.4016, Validation Loss:1.0753, Validation Accuracy:0.3875\n",
    "Epoch #168: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #169: Loss:1.0695, Accuracy:0.4008, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #170: Loss:1.0696, Accuracy:0.4029, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #171: Loss:1.0697, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #172: Loss:1.0699, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #173: Loss:1.0697, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #174: Loss:1.0695, Accuracy:0.4016, Validation Loss:1.0739, Validation Accuracy:0.4039\n",
    "Epoch #175: Loss:1.0695, Accuracy:0.4000, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #176: Loss:1.0692, Accuracy:0.4029, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #177: Loss:1.0692, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #178: Loss:1.0691, Accuracy:0.4000, Validation Loss:1.0737, Validation Accuracy:0.3957\n",
    "Epoch #179: Loss:1.0691, Accuracy:0.4107, Validation Loss:1.0737, Validation Accuracy:0.4023\n",
    "Epoch #180: Loss:1.0693, Accuracy:0.4053, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #181: Loss:1.0688, Accuracy:0.4004, Validation Loss:1.0737, Validation Accuracy:0.4023\n",
    "Epoch #182: Loss:1.0687, Accuracy:0.4111, Validation Loss:1.0735, Validation Accuracy:0.4039\n",
    "Epoch #183: Loss:1.0686, Accuracy:0.4119, Validation Loss:1.0737, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0693, Accuracy:0.4099, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #185: Loss:1.0684, Accuracy:0.4086, Validation Loss:1.0732, Validation Accuracy:0.3908\n",
    "Epoch #186: Loss:1.0688, Accuracy:0.4062, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #187: Loss:1.0684, Accuracy:0.4099, Validation Loss:1.0739, Validation Accuracy:0.4007\n",
    "Epoch #188: Loss:1.0684, Accuracy:0.4082, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #189: Loss:1.0686, Accuracy:0.4074, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #190: Loss:1.0686, Accuracy:0.4070, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #191: Loss:1.0711, Accuracy:0.4086, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #192: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0729, Validation Accuracy:0.3990\n",
    "Epoch #193: Loss:1.0719, Accuracy:0.4008, Validation Loss:1.0739, Validation Accuracy:0.4154\n",
    "Epoch #194: Loss:1.0702, Accuracy:0.4094, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #195: Loss:1.0715, Accuracy:0.3971, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #196: Loss:1.0693, Accuracy:0.4045, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #197: Loss:1.0700, Accuracy:0.4029, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #198: Loss:1.0692, Accuracy:0.3951, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #199: Loss:1.0691, Accuracy:0.3906, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #200: Loss:1.0684, Accuracy:0.4057, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #201: Loss:1.0702, Accuracy:0.3930, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #202: Loss:1.0683, Accuracy:0.4115, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #203: Loss:1.0693, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #204: Loss:1.0683, Accuracy:0.4016, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #205: Loss:1.0682, Accuracy:0.3992, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #206: Loss:1.0683, Accuracy:0.3984, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #207: Loss:1.0689, Accuracy:0.4008, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #208: Loss:1.0683, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #209: Loss:1.0682, Accuracy:0.4025, Validation Loss:1.0741, Validation Accuracy:0.4056\n",
    "Epoch #210: Loss:1.0678, Accuracy:0.4103, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #211: Loss:1.0688, Accuracy:0.4029, Validation Loss:1.0754, Validation Accuracy:0.3974\n",
    "Epoch #212: Loss:1.0686, Accuracy:0.4033, Validation Loss:1.0750, Validation Accuracy:0.4056\n",
    "Epoch #213: Loss:1.0681, Accuracy:0.4037, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #214: Loss:1.0683, Accuracy:0.4025, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #215: Loss:1.0679, Accuracy:0.4012, Validation Loss:1.0748, Validation Accuracy:0.4056\n",
    "Epoch #216: Loss:1.0676, Accuracy:0.4049, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #217: Loss:1.0680, Accuracy:0.4012, Validation Loss:1.0753, Validation Accuracy:0.4023\n",
    "Epoch #218: Loss:1.0677, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #219: Loss:1.0675, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #220: Loss:1.0674, Accuracy:0.4066, Validation Loss:1.0754, Validation Accuracy:0.4056\n",
    "Epoch #221: Loss:1.0674, Accuracy:0.4103, Validation Loss:1.0746, Validation Accuracy:0.4056\n",
    "Epoch #222: Loss:1.0673, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.4007\n",
    "Epoch #223: Loss:1.0676, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.3974\n",
    "Epoch #224: Loss:1.0671, Accuracy:0.4021, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #225: Loss:1.0674, Accuracy:0.4099, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #226: Loss:1.0675, Accuracy:0.4066, Validation Loss:1.0746, Validation Accuracy:0.4007\n",
    "Epoch #227: Loss:1.0681, Accuracy:0.4053, Validation Loss:1.0754, Validation Accuracy:0.3974\n",
    "Epoch #228: Loss:1.0671, Accuracy:0.4082, Validation Loss:1.0756, Validation Accuracy:0.4056\n",
    "Epoch #229: Loss:1.0674, Accuracy:0.4099, Validation Loss:1.0745, Validation Accuracy:0.4039\n",
    "Epoch #230: Loss:1.0671, Accuracy:0.4070, Validation Loss:1.0748, Validation Accuracy:0.4039\n",
    "Epoch #231: Loss:1.0674, Accuracy:0.4099, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #232: Loss:1.0673, Accuracy:0.4115, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #233: Loss:1.0671, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.4039\n",
    "Epoch #234: Loss:1.0673, Accuracy:0.4107, Validation Loss:1.0748, Validation Accuracy:0.4039\n",
    "Epoch #235: Loss:1.0682, Accuracy:0.4082, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #236: Loss:1.0671, Accuracy:0.4103, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #237: Loss:1.0679, Accuracy:0.4094, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #238: Loss:1.0677, Accuracy:0.4049, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #239: Loss:1.0670, Accuracy:0.4053, Validation Loss:1.0744, Validation Accuracy:0.4039\n",
    "Epoch #240: Loss:1.0680, Accuracy:0.4094, Validation Loss:1.0752, Validation Accuracy:0.4105\n",
    "Epoch #241: Loss:1.0673, Accuracy:0.4144, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #242: Loss:1.0676, Accuracy:0.4037, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #243: Loss:1.0672, Accuracy:0.4045, Validation Loss:1.0761, Validation Accuracy:0.4056\n",
    "Epoch #244: Loss:1.0670, Accuracy:0.4123, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #245: Loss:1.0670, Accuracy:0.4111, Validation Loss:1.0752, Validation Accuracy:0.3990\n",
    "Epoch #246: Loss:1.0667, Accuracy:0.4090, Validation Loss:1.0743, Validation Accuracy:0.4039\n",
    "Epoch #247: Loss:1.0669, Accuracy:0.4111, Validation Loss:1.0742, Validation Accuracy:0.4056\n",
    "Epoch #248: Loss:1.0667, Accuracy:0.4107, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #249: Loss:1.0667, Accuracy:0.4140, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #250: Loss:1.0668, Accuracy:0.4111, Validation Loss:1.0754, Validation Accuracy:0.3990\n",
    "Epoch #251: Loss:1.0670, Accuracy:0.4094, Validation Loss:1.0754, Validation Accuracy:0.4072\n",
    "Epoch #252: Loss:1.0668, Accuracy:0.4148, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #253: Loss:1.0670, Accuracy:0.4086, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #254: Loss:1.0667, Accuracy:0.4082, Validation Loss:1.0748, Validation Accuracy:0.4039\n",
    "Epoch #255: Loss:1.0668, Accuracy:0.4094, Validation Loss:1.0762, Validation Accuracy:0.4072\n",
    "Epoch #256: Loss:1.0668, Accuracy:0.4107, Validation Loss:1.0749, Validation Accuracy:0.4023\n",
    "Epoch #257: Loss:1.0667, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #258: Loss:1.0668, Accuracy:0.4037, Validation Loss:1.0751, Validation Accuracy:0.3990\n",
    "Epoch #259: Loss:1.0679, Accuracy:0.4074, Validation Loss:1.0784, Validation Accuracy:0.3957\n",
    "Epoch #260: Loss:1.0669, Accuracy:0.4140, Validation Loss:1.0764, Validation Accuracy:0.3990\n",
    "Epoch #261: Loss:1.0670, Accuracy:0.4021, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #262: Loss:1.0674, Accuracy:0.4078, Validation Loss:1.0744, Validation Accuracy:0.4056\n",
    "Epoch #263: Loss:1.0665, Accuracy:0.4111, Validation Loss:1.0755, Validation Accuracy:0.4056\n",
    "Epoch #264: Loss:1.0665, Accuracy:0.4119, Validation Loss:1.0763, Validation Accuracy:0.3941\n",
    "Epoch #265: Loss:1.0672, Accuracy:0.4053, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #266: Loss:1.0668, Accuracy:0.4127, Validation Loss:1.0746, Validation Accuracy:0.4056\n",
    "Epoch #267: Loss:1.0667, Accuracy:0.4119, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #268: Loss:1.0665, Accuracy:0.4115, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #269: Loss:1.0661, Accuracy:0.4094, Validation Loss:1.0742, Validation Accuracy:0.4056\n",
    "Epoch #270: Loss:1.0672, Accuracy:0.4057, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #271: Loss:1.0666, Accuracy:0.4115, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #272: Loss:1.0662, Accuracy:0.4111, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #273: Loss:1.0663, Accuracy:0.4078, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #274: Loss:1.0664, Accuracy:0.4090, Validation Loss:1.0751, Validation Accuracy:0.3990\n",
    "Epoch #275: Loss:1.0670, Accuracy:0.4111, Validation Loss:1.0750, Validation Accuracy:0.4072\n",
    "Epoch #276: Loss:1.0664, Accuracy:0.4086, Validation Loss:1.0766, Validation Accuracy:0.3941\n",
    "Epoch #277: Loss:1.0673, Accuracy:0.4037, Validation Loss:1.0748, Validation Accuracy:0.4023\n",
    "Epoch #278: Loss:1.0661, Accuracy:0.4119, Validation Loss:1.0754, Validation Accuracy:0.4023\n",
    "Epoch #279: Loss:1.0665, Accuracy:0.4086, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #280: Loss:1.0660, Accuracy:0.4111, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #281: Loss:1.0662, Accuracy:0.4094, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #282: Loss:1.0662, Accuracy:0.4107, Validation Loss:1.0753, Validation Accuracy:0.4122\n",
    "Epoch #283: Loss:1.0662, Accuracy:0.4107, Validation Loss:1.0760, Validation Accuracy:0.4039\n",
    "Epoch #284: Loss:1.0669, Accuracy:0.4057, Validation Loss:1.0755, Validation Accuracy:0.4039\n",
    "Epoch #285: Loss:1.0661, Accuracy:0.4123, Validation Loss:1.0753, Validation Accuracy:0.4072\n",
    "Epoch #286: Loss:1.0662, Accuracy:0.4131, Validation Loss:1.0751, Validation Accuracy:0.4072\n",
    "Epoch #287: Loss:1.0665, Accuracy:0.4115, Validation Loss:1.0763, Validation Accuracy:0.4007\n",
    "Epoch #288: Loss:1.0663, Accuracy:0.4090, Validation Loss:1.0755, Validation Accuracy:0.4056\n",
    "Epoch #289: Loss:1.0667, Accuracy:0.4111, Validation Loss:1.0748, Validation Accuracy:0.4105\n",
    "Epoch #290: Loss:1.0666, Accuracy:0.4107, Validation Loss:1.0754, Validation Accuracy:0.4039\n",
    "Epoch #291: Loss:1.0661, Accuracy:0.4103, Validation Loss:1.0773, Validation Accuracy:0.4089\n",
    "Epoch #292: Loss:1.0665, Accuracy:0.4152, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #293: Loss:1.0665, Accuracy:0.4119, Validation Loss:1.0745, Validation Accuracy:0.4122\n",
    "Epoch #294: Loss:1.0661, Accuracy:0.4140, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #295: Loss:1.0662, Accuracy:0.4082, Validation Loss:1.0756, Validation Accuracy:0.4039\n",
    "Epoch #296: Loss:1.0657, Accuracy:0.4127, Validation Loss:1.0763, Validation Accuracy:0.4089\n",
    "Epoch #297: Loss:1.0663, Accuracy:0.4168, Validation Loss:1.0750, Validation Accuracy:0.4138\n",
    "Epoch #298: Loss:1.0657, Accuracy:0.4160, Validation Loss:1.0755, Validation Accuracy:0.4056\n",
    "Epoch #299: Loss:1.0662, Accuracy:0.4127, Validation Loss:1.0753, Validation Accuracy:0.4056\n",
    "Epoch #300: Loss:1.0657, Accuracy:0.4144, Validation Loss:1.0753, Validation Accuracy:0.4187\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07528687, Accuracy:0.4187\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "       02  03   01\n",
    "t:02  114   0  113\n",
    "t:03   53   0   89\n",
    "t:01   99   0  141\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.43      0.50      0.46       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.41      0.59      0.48       240\n",
    "\n",
    "    accuracy                           0.42       609\n",
    "   macro avg       0.28      0.36      0.32       609\n",
    "weighted avg       0.32      0.42      0.36       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 09:47:17 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 24 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0857808574275627, 1.078101191223157, 1.0749056010410702, 1.0736575110988273, 1.074017057865124, 1.07396793874418, 1.0738218639088772, 1.0739378807775688, 1.0741117998884229, 1.0740897927573945, 1.0740119941128885, 1.0737724308114138, 1.0736817127378115, 1.0736954668276808, 1.073823348054745, 1.0737380928593903, 1.0736693114482712, 1.0737635838770123, 1.0738875856148982, 1.0743124860848112, 1.074379334504577, 1.0743729919439857, 1.074048053454883, 1.074042518933614, 1.0739222853054553, 1.073776457110062, 1.073813923669762, 1.0738976070250588, 1.0739118066327324, 1.0739367967364433, 1.0739183396541427, 1.0738386655675953, 1.0738820052890747, 1.0736302397717004, 1.0738575587718946, 1.0737627439310986, 1.0740288069291264, 1.0738792816797893, 1.0739723866796258, 1.073911224288502, 1.0740664181450905, 1.074064884866987, 1.0742249402702344, 1.0739015800808058, 1.0738559058930095, 1.0737398941137128, 1.073605116952229, 1.0736613261875847, 1.0738060227755843, 1.0738392651374704, 1.0738447603138013, 1.0737978525349658, 1.0742038273067505, 1.0743496598085551, 1.0742562627557464, 1.0742068073432434, 1.0740828443630575, 1.0740571098374616, 1.0731396436299792, 1.0745687705934146, 1.0732964903654527, 1.0743186910359925, 1.0736921659635597, 1.0742294310740454, 1.0743504739160021, 1.0746431458368286, 1.0746129541757268, 1.0744320287297315, 1.0745717463234963, 1.0741277046391529, 1.074015210610501, 1.073852954043935, 1.0743517002644405, 1.0743292564241758, 1.074220021957247, 1.073834093920703, 1.0732246768494154, 1.0734799235129395, 1.07317457038585, 1.073060754876223, 1.0728312973514176, 1.0732167105760873, 1.0731497623258819, 1.0728122432439393, 1.0727798586408492, 1.072777763767587, 1.0729847095282794, 1.0728329179322191, 1.0727169390382438, 1.0728085964967073, 1.0732822093274597, 1.0731471790664497, 1.0730426250811673, 1.073137811447795, 1.0732636524147197, 1.073612489528061, 1.073520141477851, 1.0735270644252133, 1.0735608390203641, 1.0731994344291624, 1.072957690126203, 1.0730259573126857, 1.0731037015398148, 1.0732605429901474, 1.072901028326188, 1.0731859461623068, 1.0730465985284063, 1.0729774393275846, 1.0729140753816502, 1.0729022349042845, 1.0731484884111753, 1.072644878295059, 1.0736759932366107, 1.0732406497197393, 1.0743014066677374, 1.0739762197770117, 1.0735458211945783, 1.0736821093191262, 1.0735169472952781, 1.073526149704343, 1.0736337007560166, 1.0735828263810507, 1.0740014829463365, 1.073972546995567, 1.0737360985995514, 1.0736177768221826, 1.0734427999001614, 1.0732300929443785, 1.073688527046166, 1.0733672766066928, 1.0729977716561805, 1.0735063184854041, 1.0733160886466993, 1.073094343708458, 1.0725476732003474, 1.0725333138639703, 1.072499633814118, 1.072960478918893, 1.0734572941055047, 1.0737735101546364, 1.0736466614875104, 1.0750414508904143, 1.0747052603363012, 1.0738814344938556, 1.0740384658177693, 1.0738698893971435, 1.0741005550660132, 1.074544072738422, 1.0742287132736106, 1.0728144081942554, 1.0730849937069395, 1.0732449774671657, 1.0736641944531344, 1.0738955548244158, 1.07344550784977, 1.073134968824966, 1.0746089235706673, 1.0731808070478768, 1.0732510150555514, 1.0734704159359234, 1.0745525450150564, 1.0733649908811196, 1.0779981063113033, 1.075238595259405, 1.0751956865705292, 1.074752586032761, 1.075348022340358, 1.0746987711619862, 1.0739958859821064, 1.0738616892073933, 1.0741125589912552, 1.0740904017230757, 1.074199291089877, 1.0738706105252596, 1.0738424589285513, 1.074229081080269, 1.073837462708672, 1.073719347639037, 1.0736588421713542, 1.0738758958619217, 1.0737236855652532, 1.0735392184876065, 1.073664947292096, 1.0740114693179703, 1.0732082692273144, 1.0733235994191788, 1.0738907005203573, 1.0738306773707198, 1.0738417614856963, 1.0729912507710198, 1.076547724860055, 1.072881382870165, 1.0738550495044352, 1.0732999061324522, 1.0746408781198837, 1.0746889447148014, 1.074554687454587, 1.0744334713774557, 1.0745824902319947, 1.0744034374875975, 1.074300048582268, 1.0745654951762684, 1.0741261739057468, 1.0743049995848306, 1.0746865797121146, 1.074577930134114, 1.0747383273098072, 1.0746042704934557, 1.0741488530326555, 1.0745147777895623, 1.0753880034526582, 1.0749570017769223, 1.0746289914464715, 1.0740983936391244, 1.0748477498886033, 1.0749646834356248, 1.0752999162047563, 1.0744562096196442, 1.0752780423767265, 1.0753552036723872, 1.0746229491601829, 1.0747539345266783, 1.074991514725834, 1.0751256042317607, 1.0749393458828354, 1.0746425548797758, 1.0754205311460447, 1.0756493934073863, 1.0745480129088478, 1.0747802003068094, 1.075063914696767, 1.074685942754761, 1.07465441140831, 1.0748147036641689, 1.0761088103496383, 1.0747096235137463, 1.0752936437212188, 1.075996394815116, 1.0743616453336768, 1.0752108061842143, 1.0751726412029297, 1.0746639205512938, 1.076060146729543, 1.0757104759341587, 1.0751704028478788, 1.0742519712213225, 1.0741773890744288, 1.0748812608139464, 1.0760348399088693, 1.0754157821533128, 1.0754286111477755, 1.0745583796148817, 1.0748290512557883, 1.0748390260784106, 1.0761870570566463, 1.0748833705639016, 1.0746976469929386, 1.0751348224962483, 1.0783595312600849, 1.076436079194393, 1.0751017347736702, 1.0744441416854733, 1.0754774049389342, 1.076278503500965, 1.074388973622878, 1.0746455272821762, 1.0756745855209275, 1.074833129622862, 1.0742410270647071, 1.0747271324026173, 1.0752841065865628, 1.0752695477850527, 1.0747475154294168, 1.0751475140770472, 1.074980118591797, 1.0766452392333834, 1.0748200706268962, 1.0754462764376687, 1.0749359624139194, 1.074980519871015, 1.0755848040917433, 1.0753324478130621, 1.076010003269991, 1.0755091104992895, 1.0752577055459736, 1.075112336570602, 1.0762701232249319, 1.0754685233574979, 1.0747815206133087, 1.0753875095856014, 1.0773442323963434, 1.0754899555826423, 1.0744534400100583, 1.0749526849912696, 1.0755967731742044, 1.0763004522996975, 1.0749703177873333, 1.075491400187826, 1.0753041225896875, 1.0752868484002225], 'val_acc': [0.37274219910499495, 0.37274219910499495, 0.37274219910499495, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.39408866882519966, 0.3809523797289687, 0.3809523797289687, 0.3825944159516364, 0.3858784882990989, 0.38587848820122594, 0.38095237982684166, 0.38587848839697186, 0.38423645217430413, 0.3842364520764312, 0.3825944159516364, 0.38423645217430413, 0.3875205244238936, 0.38423645217430413, 0.38095237953322275, 0.3875205240324017, 0.38423645168493925, 0.385878487907607, 0.3875205242281477, 0.38423645217430413, 0.3875205240324017, 0.38916256015719647, 0.385878487907607, 0.38916256025506946, 0.3842364519785582, 0.3875205240324017, 0.38916256015719647, 0.3908045961841182, 0.3908045961841182, 0.38916256035294244, 0.38587848810335296, 0.3875205240324017, 0.38916256015719647, 0.3908045961841182, 0.40065681312863266, 0.38587848800547997, 0.38587848810335296, 0.3908045962819912, 0.3957307049499944, 0.38423645178281224, 0.38587848810335296, 0.385878487907607, 0.3940886686294537, 0.38916256025506946, 0.38587848810335296, 0.38916256054868836, 0.3825944159516364, 0.3760262716482034, 0.38587848820122594, 0.3875205244238936, 0.385878487907607, 0.3875205241302747, 0.3875205240324017, 0.3825944155601445, 0.3825944155601445, 0.3908045964777372, 0.3875205243260207, 0.3743842352297897, 0.37602627135458444, 0.3875205243260207, 0.39408866892307265, 0.3908045965756102, 0.3924466327004049, 0.39408866882519966, 0.3842364518806852, 0.3825944161473824, 0.40065681332437864, 0.39408866882519966, 0.38587848810335296, 0.38587848810335296, 0.3908045963798642, 0.3908045963798642, 0.3908045963798642, 0.3875205241302747, 0.3908045965756102, 0.38916256054868836, 0.3908045965756102, 0.3908045967713561, 0.3908045965756102, 0.38587848810335296, 0.38916256064656135, 0.38916256064656135, 0.38587848810335296, 0.38916256054868836, 0.3875205244238936, 0.39244663289615084, 0.3875205243260207, 0.3908045967713561, 0.3908045967713561, 0.3875205241302747, 0.38916256064656135, 0.38916256064656135, 0.38587848810335296, 0.3875205244238936, 0.38587848820122594, 0.3908045965756102, 0.38916256035294244, 0.39244663230891297, 0.3924466327004049, 0.38916256025506946, 0.3924466327982779, 0.39244663260253193, 0.38916256045081543, 0.39080459667348316, 0.3908045964777372, 0.3875205242281477, 0.38587848800547997, 0.3842364519785582, 0.38752052393452874, 0.38587848810335296, 0.39573070426488355, 0.3908045963798642, 0.3908045965756102, 0.3908045961841182, 0.39244663230891297, 0.3842364514891933, 0.3891625597657046, 0.379310343604174, 0.3908045958904993, 0.3809523792396038, 0.39244663221104, 0.39080459598837225, 0.3809523792396038, 0.38259441536439853, 0.3908045967713561, 0.38423645158706626, 0.37602627135458444, 0.38916256045081543, 0.40722495723631974, 0.39244663230891297, 0.3825944159516364, 0.39244663221104, 0.38095237982684166, 0.4072249582150495, 0.3940886690209456, 0.38095237982684166, 0.3793103438977929, 0.3809523797289687, 0.3875205244238936, 0.38916256054868836, 0.3908045963798642, 0.3957307048521214, 0.38916256054868836, 0.3842364519785582, 0.41543513795816644, 0.39573070465637544, 0.39080459667348316, 0.3891625596678316, 0.3825944159516364, 0.37766830747937924, 0.38752052354303684, 0.38752052373878276, 0.39080459598837225, 0.39244663211316705, 0.39244663230891297, 0.3940886682379618, 0.399014776612346, 0.40394088528034916, 0.39901477651447304, 0.39573070426488355, 0.3973727403896783, 0.39573070426488355, 0.4022988489598085, 0.3973727403896783, 0.40229884905768143, 0.40394088508460324, 0.39573070416701056, 0.3973727401939323, 0.3908045964777372, 0.4022988489598085, 0.40065681283501375, 0.3973727403896783, 0.39573070436275654, 0.40394088498673025, 0.39408866882519966, 0.39901477641660005, 0.4154351385454043, 0.3957307040691376, 0.3940886685315807, 0.3940886686294537, 0.3957307048521214, 0.39244663260253193, 0.3973727408790432, 0.3957307039712646, 0.39408866784646984, 0.3990147771995839, 0.3973727401939323, 0.3973727401939323, 0.39901477651447304, 0.3957307039712646, 0.405582921111525, 0.39573070426488355, 0.405582921111525, 0.39901477631872706, 0.3973727402918053, 0.405582921111525, 0.3973727402918053, 0.4006568126392678, 0.405582921111525, 0.3973727402918053, 0.4022988487640625, 0.3973727402918053, 0.39408866794434283, 0.405582921111525, 0.405582921111525, 0.4006568126392678, 0.3973727402918053, 0.405582921111525, 0.405582921111525, 0.4006568126392678, 0.3973727402918053, 0.405582921111525, 0.4039408847909843, 0.4039408847909843, 0.405582921111525, 0.39901477631872706, 0.4039408847909843, 0.4039408847909843, 0.39901477641660005, 0.405582921111525, 0.39901477641660005, 0.39408866794434283, 0.4039408847909843, 0.4105090296816552, 0.39408866794434283, 0.39408866794434283, 0.405582921111525, 0.39901477641660005, 0.39901477641660005, 0.4039408847909843, 0.405582921111525, 0.40558292081790603, 0.39901477641660005, 0.39901477641660005, 0.40722495713844675, 0.39408866794434283, 0.39901477641660005, 0.4039408847909843, 0.40722495713844675, 0.4022988487640625, 0.39408866794434283, 0.39901477641660005, 0.3957307049499944, 0.39901477641660005, 0.3940886680422158, 0.405582921111525, 0.40558292081790603, 0.39408866794434283, 0.39901477641660005, 0.405582921111525, 0.39901477641660005, 0.39901477641660005, 0.405582921111525, 0.39901477641660005, 0.39901477641660005, 0.39901477641660005, 0.40558292081790603, 0.39901477641660005, 0.40722495713844675, 0.3940886680422158, 0.40229884847044356, 0.40229884847044356, 0.405582921111525, 0.39901477641660005, 0.39408866794434283, 0.41215106570857696, 0.4039408847909843, 0.40394088488885727, 0.40722495713844675, 0.4072249569427008, 0.4006568125413948, 0.405582921013652, 0.4105090296816552, 0.40394088488885727, 0.4088669932632415, 0.4088669930674955, 0.41215106561070397, 0.39901477641660005, 0.40394088488885727, 0.4088669932632415, 0.4137931017354987, 0.405582921013652, 0.405582921013652, 0.4187192103056289], 'loss': [1.0948350275321663, 1.0820892231175543, 1.0767848838770904, 1.0739124300543532, 1.073626626345656, 1.0742300843800852, 1.074261248136203, 1.073917489423889, 1.0740382502455976, 1.0742926396873207, 1.0741573703852032, 1.0740310134339381, 1.073905451097038, 1.073718260641705, 1.0736413294774552, 1.073738974712223, 1.0736431475782295, 1.073471101402502, 1.073631666132557, 1.073702118871638, 1.0738670637475392, 1.073864734246256, 1.0737643761311713, 1.0735614877951463, 1.0735668478560398, 1.0734333411868837, 1.0734010967630625, 1.0733746963359982, 1.073407915877121, 1.0733043298584235, 1.0732442606645933, 1.073160402485967, 1.073302402780286, 1.0732207270863119, 1.0731438858553124, 1.0730845298120864, 1.073115165963065, 1.0730866474292606, 1.0730704895524763, 1.0730017734993655, 1.0729847533991694, 1.0731320679310166, 1.0730658336341747, 1.0729037289746732, 1.0727039017961255, 1.072641355496902, 1.0726419211413092, 1.0726628773511069, 1.0728687937499561, 1.0721916015632833, 1.0727721927591907, 1.0723660341278483, 1.0724579752348287, 1.072318015108363, 1.0725692284914992, 1.0722409870100706, 1.0721363156238375, 1.0723816452085115, 1.0726505820511303, 1.0718791199905426, 1.0730291304891848, 1.0721260119757368, 1.0736443837320535, 1.0731408511343923, 1.0734464071614542, 1.0733808307432295, 1.0729675494180322, 1.0728626471525344, 1.0729007112906455, 1.0727107258547992, 1.0727802767645898, 1.073006076342761, 1.0728571318013467, 1.0728268191554953, 1.0731523300343226, 1.0731266972465436, 1.0727245056653658, 1.072755454795806, 1.072648930990231, 1.072347010577239, 1.0722224292813876, 1.0718326234230027, 1.072552776679366, 1.0724700577694777, 1.0730943961799513, 1.073525326648532, 1.0727536587744524, 1.072409179568046, 1.0726926975916053, 1.072800745954259, 1.072828522942639, 1.0725621591603243, 1.0723564600797648, 1.0726501095711083, 1.0725639753028353, 1.072628008364652, 1.0724572931716574, 1.072381494715963, 1.071937359627757, 1.0724082055767459, 1.0719651311819558, 1.0721688324910659, 1.0720535459459686, 1.0719049315677776, 1.071898180650245, 1.0716078957493056, 1.0716242627196733, 1.0714588037016946, 1.0714499647857227, 1.071354478438532, 1.0707830678266177, 1.0710118293762207, 1.0709208132060402, 1.0709649277418791, 1.0711005306831376, 1.0709614042383935, 1.0710642925278118, 1.0706298781126677, 1.0705814794348496, 1.0706132319428838, 1.0703326070577948, 1.070601637358538, 1.0709465269190575, 1.0706161249344843, 1.0708295852007073, 1.0705094736459564, 1.070319877366021, 1.0702370586826082, 1.069545520747222, 1.0708230027917474, 1.0703782189308495, 1.0704099875945574, 1.07063719521313, 1.071142542778344, 1.07055900503233, 1.0703844318644467, 1.0710237768886026, 1.0704323482709255, 1.0705643315090045, 1.0708711467973995, 1.0705204962215384, 1.070463735905516, 1.070626841188701, 1.0700126062428437, 1.0706229431673242, 1.070957007251481, 1.070156874646886, 1.0709883764294383, 1.0709145051497944, 1.0701952381545268, 1.0710683437833062, 1.0703959123799442, 1.0699834411883502, 1.070090365899417, 1.070245825142831, 1.0710128827261485, 1.0707670119264043, 1.0699447892774547, 1.0698340525617345, 1.069880899805308, 1.071962061799772, 1.074887741370857, 1.0708790101554604, 1.0726548950285393, 1.0706974430005898, 1.0709724538380117, 1.070108131412608, 1.0699811118339366, 1.06953008938619, 1.069621886364978, 1.0697303065039538, 1.0698920940226844, 1.0697238237216486, 1.0695190167280193, 1.0694617349264313, 1.069158812470015, 1.0691633665586153, 1.0691286108087465, 1.0691115036637386, 1.0693217607004686, 1.0688156231228085, 1.0687088502261184, 1.0685582253477657, 1.0692668797299112, 1.0684367498577987, 1.0687669707519563, 1.068393881756667, 1.0684273751119813, 1.0685917649670549, 1.068576473964558, 1.0711294476256479, 1.0734317185208049, 1.0718878996200876, 1.0702440606495194, 1.0715480188079929, 1.0693484768975197, 1.0699793598245546, 1.0692311435264727, 1.0690550916738333, 1.0683584248505578, 1.070236154062792, 1.0683266141820982, 1.069328424622146, 1.0683468587099894, 1.0682385001094434, 1.0683499945752184, 1.0688816226728153, 1.068290700951641, 1.06821935196432, 1.067837662716421, 1.0688247502462085, 1.0685802266338278, 1.068050367141896, 1.0682984373652715, 1.0679049737154827, 1.0676459606178488, 1.0679588781489973, 1.0677287145806533, 1.0675444921184125, 1.0673909846272556, 1.067353848118557, 1.0672753957262764, 1.067611113906641, 1.0671468339661554, 1.0673857573366263, 1.0674680566885633, 1.068066915251636, 1.0670782556768805, 1.0673592316296556, 1.0670919409522774, 1.0674096060974152, 1.0673268142911687, 1.0670562483691581, 1.0673146063787002, 1.0681732289355395, 1.0670752340763257, 1.0679180967734334, 1.0676931596145003, 1.0670202990087396, 1.0679768397822762, 1.0673055905336226, 1.0675842245011848, 1.0672383247704476, 1.06698205568952, 1.066980099629083, 1.0666790507412545, 1.0668509761410816, 1.0667268174384898, 1.0667055965693824, 1.0668446865904258, 1.067040581918595, 1.0667644054248349, 1.0669784446516566, 1.0666861964936618, 1.0667683146083136, 1.0668093711198967, 1.0667335131819489, 1.066845318032486, 1.0679330805733462, 1.066902722421368, 1.0670290663991375, 1.0673929941972422, 1.0665385322159566, 1.066471402503137, 1.0671548636786992, 1.0668037927126248, 1.0666640812856216, 1.0665125631943375, 1.0661070817794642, 1.06716226155753, 1.066577024185682, 1.0662111342075669, 1.0663213315196105, 1.0663970705420085, 1.0669857513733223, 1.0663889385591543, 1.0672584343739848, 1.0661188602447509, 1.0665290052151533, 1.0660189141972598, 1.0661829824565128, 1.0662390104799055, 1.066190713925528, 1.066878213040393, 1.0661276514770068, 1.0662199605416958, 1.0665149546257036, 1.0662700328494, 1.0667275398908453, 1.0666260794202895, 1.0661481021610864, 1.066533215187903, 1.0665010202102347, 1.0661251829390164, 1.0662370061482738, 1.0657454515139915, 1.0662684777434113, 1.0657478409381373, 1.0662252769333136, 1.065737275078556], 'acc': [0.3728952769870876, 0.37289527581212945, 0.37289527737874023, 0.3909650937487702, 0.3942505133225443, 0.3942505141058497, 0.39425051171921605, 0.3942505153175252, 0.39425051391002336, 0.3942505113275634, 0.3942505156724605, 0.3942505121475862, 0.3942505152808078, 0.394250514301676, 0.3942505119150424, 0.39425051113173704, 0.39425051312671794, 0.3958932247739553, 0.40041067876854963, 0.39917864591434016, 0.3946611893740033, 0.3975359328596009, 0.3975359322721219, 0.3979466138434361, 0.39712525484987843, 0.3987679668887685, 0.40041067935602864, 0.3983572878999142, 0.3983572873124352, 0.3983572906782005, 0.39794661364760975, 0.40246406803385676, 0.40328542107918913, 0.40082135677827213, 0.40369610030058717, 0.39712525762816475, 0.39835729048237417, 0.4016427126018908, 0.4008213530208541, 0.40246406705472504, 0.40205338763750065, 0.4004106775568741, 0.4016427122102381, 0.4053388113603455, 0.404106778506136, 0.4024640662714196, 0.4000000011504798, 0.40164271064362733, 0.40328542189921196, 0.40000000017134807, 0.3987679673171386, 0.40123203400468926, 0.39876796712131224, 0.3995893236915189, 0.4012320329888401, 0.40369610030058717, 0.3979466132559571, 0.3991786461101665, 0.40082135579914036, 0.40410677435706527, 0.4028747416986822, 0.3991786433318802, 0.40123203220553466, 0.39753593602953996, 0.39589322238732166, 0.39876796649711577, 0.3991786427444012, 0.40698152144097205, 0.4090349093354948, 0.40041067540278424, 0.40287474388948946, 0.40205338669508633, 0.39999999880056364, 0.40369610088806623, 0.39999999977969536, 0.4012320310305766, 0.4016427118185854, 0.40410677772283066, 0.405749486787608, 0.40328542049171007, 0.402464066467246, 0.40657084163209495, 0.40657084264794413, 0.40862423015081417, 0.4020533868541952, 0.4012320324380785, 0.3999999991554989, 0.3995893247073681, 0.4004106775935915, 0.3999999987638462, 0.4000000003671744, 0.40041067896437593, 0.40205339002413426, 0.39876796653383323, 0.3987679653221577, 0.3983572918531586, 0.4041067745528916, 0.3983572896990688, 0.3979466106734971, 0.3999999997429779, 0.4000000019337852, 0.3991786461101665, 0.4012320343963419, 0.4004106770061125, 0.4024640668588987, 0.3958932259489134, 0.3995893201666446, 0.4032854217033856, 0.4016427122102381, 0.4004106759902633, 0.40328542029588377, 0.4106776164420087, 0.3991786433318802, 0.4016427126018908, 0.4135523605518028, 0.40328542287834374, 0.40492812939737854, 0.40287474545610025, 0.4094455833919729, 0.40328542131173295, 0.4016427090402991, 0.406160162251588, 0.4000000003671744, 0.39876796947122845, 0.4028747452602739, 0.40451745475342143, 0.40657084401872856, 0.40698151905433844, 0.41273100727392664, 0.4131416845003437, 0.4172484601302803, 0.4168377834913422, 0.40903490972714746, 0.4127310051198368, 0.4127310071148177, 0.4139630413398116, 0.4053388109686928, 0.4123203265226352, 0.4102669388239389, 0.40164271123110634, 0.41314168512454025, 0.4073921953016238, 0.4164271060690988, 0.4127310044956403, 0.40657084006548416, 0.4090349055780767, 0.41273100672316504, 0.40944558476275733, 0.40492813213894746, 0.40862423132577225, 0.4069815200701876, 0.40985626081421633, 0.4078028751105009, 0.41190965129119905, 0.4098562625766535, 0.4028747436936631, 0.4045174545575951, 0.4147843928185332, 0.40657084401872856, 0.408624229134965, 0.4045174535784633, 0.40533881116451914, 0.40616016561735335, 0.398767968687923, 0.403696097950671, 0.4045174531500932, 0.4016427108027362, 0.40205338806587076, 0.4008213551749439, 0.40287474150285585, 0.40287474193122597, 0.40041067736104774, 0.4012320320097083, 0.4016427102519746, 0.3999999987638462, 0.40287474506444754, 0.40164271123110634, 0.399999999583869, 0.4106776196119477, 0.40533880881460294, 0.40041067974768135, 0.4110882942559048, 0.4119096487087391, 0.40985626061839, 0.4086242315215986, 0.40616016463822163, 0.40985626081421633, 0.4082135519452653, 0.4073921949099711, 0.4069815188585121, 0.4086242311299459, 0.401232034983821, 0.40082135619079307, 0.40944558714939094, 0.3971252579831, 0.40451745573255316, 0.402874743852772, 0.3950718687545103, 0.3905544167181794, 0.4057494878034572, 0.3930184810558139, 0.4114989728531064, 0.39917864591434016, 0.40164270845282, 0.39917864352770654, 0.3983572906782005, 0.4008213530208541, 0.3995893207541237, 0.402464063884786, 0.4102669394114179, 0.40287474545610025, 0.40328541990423106, 0.4036960965431691, 0.40246406564722315, 0.4012320326339048, 0.40492813374227565, 0.40123203517964734, 0.40492813315479664, 0.4020533866583689, 0.4065708408487896, 0.41026694058637597, 0.4024640668588987, 0.40451745334591954, 0.4020533892408289, 0.40985626218500076, 0.4065708432354232, 0.4053388105770401, 0.4082135539035288, 0.4098562649632871, 0.40698151807520666, 0.4098562633966763, 0.41149897383223816, 0.40287474349783675, 0.41067762000360036, 0.40821355034193707, 0.4102669421897042, 0.4094455861702592, 0.40492813315479664, 0.40533880881460294, 0.4094455861702592, 0.4143737161795951, 0.4036960965431691, 0.4045174515834824, 0.41232032828507237, 0.4110882976216702, 0.40903490675303483, 0.411088294684275, 0.4106776201994267, 0.4139630377782199, 0.4110882962508858, 0.4094455855460627, 0.41478439458097033, 0.40862422878002974, 0.4082135507335898, 0.409445584566931, 0.41067761980777406, 0.4028747433020104, 0.40369609853815003, 0.40739219651329933, 0.4139630393448307, 0.4020533902199606, 0.40780287389882536, 0.41108829347259945, 0.41190964988369716, 0.4053388076029274, 0.4127310045323577, 0.4119096506670026, 0.41149897543556635, 0.4094455843711046, 0.40574948858676263, 0.411498972069801, 0.4110882976216702, 0.40780287624874156, 0.40903490538225035, 0.4110882962508858, 0.40862423073829324, 0.40369609990893446, 0.4119096491003918, 0.40862423210907767, 0.4110882976216702, 0.4094455861702592, 0.4106776168336614, 0.4106776162461823, 0.4057494854168236, 0.4123203271101143, 0.41314168512454025, 0.41149897523974, 0.4090349063613821, 0.41108829406007846, 0.41067761745785786, 0.4102669404272671, 0.4151950700449503, 0.41190964890456544, 0.4139630413398116, 0.4082135546868342, 0.41273100629479487, 0.4168377834913422, 0.4160164280593762, 0.41273100672316504, 0.4143737165712478]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
