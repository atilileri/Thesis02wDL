{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf10.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 01:38:57 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '3', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ce', 'ek', 'mb', 'eg', 'yd', 'ib', 'sg', 'eo', 'eb', 'by', 'ds', 'ck', 'sk', 'my', 'aa'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001250326D240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000012573F56EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7211, Accuracy:0.0661, Validation Loss:2.7127, Validation Accuracy:0.0706\n",
    "Epoch #2: Loss:2.7084, Accuracy:0.0879, Validation Loss:2.7029, Validation Accuracy:0.1018\n",
    "Epoch #3: Loss:2.6999, Accuracy:0.1023, Validation Loss:2.6951, Validation Accuracy:0.1018\n",
    "Epoch #4: Loss:2.6928, Accuracy:0.1023, Validation Loss:2.6886, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6866, Accuracy:0.1027, Validation Loss:2.6829, Validation Accuracy:0.1067\n",
    "Epoch #6: Loss:2.6807, Accuracy:0.1064, Validation Loss:2.6763, Validation Accuracy:0.1051\n",
    "Epoch #7: Loss:2.6744, Accuracy:0.1097, Validation Loss:2.6696, Validation Accuracy:0.1100\n",
    "Epoch #8: Loss:2.6682, Accuracy:0.1109, Validation Loss:2.6634, Validation Accuracy:0.1182\n",
    "Epoch #9: Loss:2.6616, Accuracy:0.1191, Validation Loss:2.6561, Validation Accuracy:0.1297\n",
    "Epoch #10: Loss:2.6545, Accuracy:0.1257, Validation Loss:2.6484, Validation Accuracy:0.1346\n",
    "Epoch #11: Loss:2.6457, Accuracy:0.1359, Validation Loss:2.6388, Validation Accuracy:0.1511\n",
    "Epoch #12: Loss:2.6352, Accuracy:0.1515, Validation Loss:2.6279, Validation Accuracy:0.1609\n",
    "Epoch #13: Loss:2.6231, Accuracy:0.1552, Validation Loss:2.6138, Validation Accuracy:0.1626\n",
    "Epoch #14: Loss:2.6079, Accuracy:0.1573, Validation Loss:2.5992, Validation Accuracy:0.1658\n",
    "Epoch #15: Loss:2.5912, Accuracy:0.1577, Validation Loss:2.5807, Validation Accuracy:0.1691\n",
    "Epoch #16: Loss:2.5729, Accuracy:0.1593, Validation Loss:2.5644, Validation Accuracy:0.1609\n",
    "Epoch #17: Loss:2.5585, Accuracy:0.1622, Validation Loss:2.5457, Validation Accuracy:0.1658\n",
    "Epoch #18: Loss:2.5396, Accuracy:0.1610, Validation Loss:2.5339, Validation Accuracy:0.1658\n",
    "Epoch #19: Loss:2.5263, Accuracy:0.1573, Validation Loss:2.5248, Validation Accuracy:0.1658\n",
    "Epoch #20: Loss:2.5195, Accuracy:0.1630, Validation Loss:2.5110, Validation Accuracy:0.1642\n",
    "Epoch #21: Loss:2.5139, Accuracy:0.1598, Validation Loss:2.5065, Validation Accuracy:0.1642\n",
    "Epoch #22: Loss:2.5032, Accuracy:0.1659, Validation Loss:2.4969, Validation Accuracy:0.1691\n",
    "Epoch #23: Loss:2.4941, Accuracy:0.1659, Validation Loss:2.4933, Validation Accuracy:0.1708\n",
    "Epoch #24: Loss:2.4877, Accuracy:0.1651, Validation Loss:2.4868, Validation Accuracy:0.1626\n",
    "Epoch #25: Loss:2.4817, Accuracy:0.1610, Validation Loss:2.4805, Validation Accuracy:0.1626\n",
    "Epoch #26: Loss:2.4782, Accuracy:0.1663, Validation Loss:2.4775, Validation Accuracy:0.1675\n",
    "Epoch #27: Loss:2.4744, Accuracy:0.1630, Validation Loss:2.4768, Validation Accuracy:0.1806\n",
    "Epoch #28: Loss:2.4716, Accuracy:0.1667, Validation Loss:2.4707, Validation Accuracy:0.1888\n",
    "Epoch #29: Loss:2.4674, Accuracy:0.1721, Validation Loss:2.4695, Validation Accuracy:0.1806\n",
    "Epoch #30: Loss:2.4645, Accuracy:0.1725, Validation Loss:2.4680, Validation Accuracy:0.1839\n",
    "Epoch #31: Loss:2.4639, Accuracy:0.1692, Validation Loss:2.4663, Validation Accuracy:0.1856\n",
    "Epoch #32: Loss:2.4624, Accuracy:0.1762, Validation Loss:2.4646, Validation Accuracy:0.1888\n",
    "Epoch #33: Loss:2.4591, Accuracy:0.1717, Validation Loss:2.4640, Validation Accuracy:0.1888\n",
    "Epoch #34: Loss:2.4574, Accuracy:0.1758, Validation Loss:2.4642, Validation Accuracy:0.1888\n",
    "Epoch #35: Loss:2.4558, Accuracy:0.1737, Validation Loss:2.4677, Validation Accuracy:0.1856\n",
    "Epoch #36: Loss:2.4565, Accuracy:0.1721, Validation Loss:2.4648, Validation Accuracy:0.1839\n",
    "Epoch #37: Loss:2.4555, Accuracy:0.1737, Validation Loss:2.4656, Validation Accuracy:0.1806\n",
    "Epoch #38: Loss:2.4553, Accuracy:0.1721, Validation Loss:2.4646, Validation Accuracy:0.1724\n",
    "Epoch #39: Loss:2.4522, Accuracy:0.1721, Validation Loss:2.4612, Validation Accuracy:0.1708\n",
    "Epoch #40: Loss:2.4553, Accuracy:0.1749, Validation Loss:2.4650, Validation Accuracy:0.1839\n",
    "Epoch #41: Loss:2.4520, Accuracy:0.1766, Validation Loss:2.4601, Validation Accuracy:0.1888\n",
    "Epoch #42: Loss:2.4494, Accuracy:0.1745, Validation Loss:2.4596, Validation Accuracy:0.1823\n",
    "Epoch #43: Loss:2.4483, Accuracy:0.1754, Validation Loss:2.4572, Validation Accuracy:0.1872\n",
    "Epoch #44: Loss:2.4477, Accuracy:0.1758, Validation Loss:2.4550, Validation Accuracy:0.1872\n",
    "Epoch #45: Loss:2.4485, Accuracy:0.1762, Validation Loss:2.4563, Validation Accuracy:0.1888\n",
    "Epoch #46: Loss:2.4462, Accuracy:0.1774, Validation Loss:2.4556, Validation Accuracy:0.1888\n",
    "Epoch #47: Loss:2.4455, Accuracy:0.1758, Validation Loss:2.4552, Validation Accuracy:0.1888\n",
    "Epoch #48: Loss:2.4433, Accuracy:0.1766, Validation Loss:2.4561, Validation Accuracy:0.1888\n",
    "Epoch #49: Loss:2.4454, Accuracy:0.1758, Validation Loss:2.4580, Validation Accuracy:0.1888\n",
    "Epoch #50: Loss:2.4448, Accuracy:0.1778, Validation Loss:2.4582, Validation Accuracy:0.1888\n",
    "Epoch #51: Loss:2.4439, Accuracy:0.1774, Validation Loss:2.4584, Validation Accuracy:0.1888\n",
    "Epoch #52: Loss:2.4440, Accuracy:0.1762, Validation Loss:2.4582, Validation Accuracy:0.1888\n",
    "Epoch #53: Loss:2.4454, Accuracy:0.1725, Validation Loss:2.4572, Validation Accuracy:0.1888\n",
    "Epoch #54: Loss:2.4455, Accuracy:0.1762, Validation Loss:2.4584, Validation Accuracy:0.1806\n",
    "Epoch #55: Loss:2.4437, Accuracy:0.1762, Validation Loss:2.4562, Validation Accuracy:0.1856\n",
    "Epoch #56: Loss:2.4443, Accuracy:0.1762, Validation Loss:2.4585, Validation Accuracy:0.1888\n",
    "Epoch #57: Loss:2.4453, Accuracy:0.1807, Validation Loss:2.4603, Validation Accuracy:0.1773\n",
    "Epoch #58: Loss:2.4450, Accuracy:0.1766, Validation Loss:2.4617, Validation Accuracy:0.1790\n",
    "Epoch #59: Loss:2.4437, Accuracy:0.1758, Validation Loss:2.4569, Validation Accuracy:0.1856\n",
    "Epoch #60: Loss:2.4405, Accuracy:0.1811, Validation Loss:2.4589, Validation Accuracy:0.1806\n",
    "Epoch #61: Loss:2.4424, Accuracy:0.1754, Validation Loss:2.4570, Validation Accuracy:0.1806\n",
    "Epoch #62: Loss:2.4417, Accuracy:0.1733, Validation Loss:2.4573, Validation Accuracy:0.1773\n",
    "Epoch #63: Loss:2.4424, Accuracy:0.1774, Validation Loss:2.4602, Validation Accuracy:0.1806\n",
    "Epoch #64: Loss:2.4406, Accuracy:0.1778, Validation Loss:2.4570, Validation Accuracy:0.1806\n",
    "Epoch #65: Loss:2.4394, Accuracy:0.1807, Validation Loss:2.4590, Validation Accuracy:0.1806\n",
    "Epoch #66: Loss:2.4401, Accuracy:0.1729, Validation Loss:2.4549, Validation Accuracy:0.1790\n",
    "Epoch #67: Loss:2.4393, Accuracy:0.1758, Validation Loss:2.4548, Validation Accuracy:0.1806\n",
    "Epoch #68: Loss:2.4385, Accuracy:0.1774, Validation Loss:2.4562, Validation Accuracy:0.1888\n",
    "Epoch #69: Loss:2.4380, Accuracy:0.1807, Validation Loss:2.4554, Validation Accuracy:0.1806\n",
    "Epoch #70: Loss:2.4385, Accuracy:0.1774, Validation Loss:2.4558, Validation Accuracy:0.1773\n",
    "Epoch #71: Loss:2.4383, Accuracy:0.1782, Validation Loss:2.4561, Validation Accuracy:0.1823\n",
    "Epoch #72: Loss:2.4416, Accuracy:0.1778, Validation Loss:2.4577, Validation Accuracy:0.1806\n",
    "Epoch #73: Loss:2.4374, Accuracy:0.1791, Validation Loss:2.4553, Validation Accuracy:0.1839\n",
    "Epoch #74: Loss:2.4372, Accuracy:0.1749, Validation Loss:2.4581, Validation Accuracy:0.1806\n",
    "Epoch #75: Loss:2.4369, Accuracy:0.1786, Validation Loss:2.4562, Validation Accuracy:0.1806\n",
    "Epoch #76: Loss:2.4367, Accuracy:0.1807, Validation Loss:2.4572, Validation Accuracy:0.1823\n",
    "Epoch #77: Loss:2.4362, Accuracy:0.1819, Validation Loss:2.4575, Validation Accuracy:0.1806\n",
    "Epoch #78: Loss:2.4364, Accuracy:0.1766, Validation Loss:2.4556, Validation Accuracy:0.1806\n",
    "Epoch #79: Loss:2.4358, Accuracy:0.1770, Validation Loss:2.4581, Validation Accuracy:0.1790\n",
    "Epoch #80: Loss:2.4342, Accuracy:0.1803, Validation Loss:2.4557, Validation Accuracy:0.1806\n",
    "Epoch #81: Loss:2.4340, Accuracy:0.1774, Validation Loss:2.4564, Validation Accuracy:0.1823\n",
    "Epoch #82: Loss:2.4343, Accuracy:0.1795, Validation Loss:2.4555, Validation Accuracy:0.1856\n",
    "Epoch #83: Loss:2.4345, Accuracy:0.1766, Validation Loss:2.4557, Validation Accuracy:0.1790\n",
    "Epoch #84: Loss:2.4351, Accuracy:0.1782, Validation Loss:2.4554, Validation Accuracy:0.1757\n",
    "Epoch #85: Loss:2.4348, Accuracy:0.1766, Validation Loss:2.4569, Validation Accuracy:0.1773\n",
    "Epoch #86: Loss:2.4328, Accuracy:0.1795, Validation Loss:2.4569, Validation Accuracy:0.1806\n",
    "Epoch #87: Loss:2.4335, Accuracy:0.1795, Validation Loss:2.4576, Validation Accuracy:0.1773\n",
    "Epoch #88: Loss:2.4330, Accuracy:0.1786, Validation Loss:2.4578, Validation Accuracy:0.1741\n",
    "Epoch #89: Loss:2.4377, Accuracy:0.1733, Validation Loss:2.4695, Validation Accuracy:0.1626\n",
    "Epoch #90: Loss:2.4400, Accuracy:0.1795, Validation Loss:2.4672, Validation Accuracy:0.1773\n",
    "Epoch #91: Loss:2.4403, Accuracy:0.1840, Validation Loss:2.4653, Validation Accuracy:0.1806\n",
    "Epoch #92: Loss:2.4380, Accuracy:0.1799, Validation Loss:2.4605, Validation Accuracy:0.1839\n",
    "Epoch #93: Loss:2.4357, Accuracy:0.1795, Validation Loss:2.4611, Validation Accuracy:0.1691\n",
    "Epoch #94: Loss:2.4335, Accuracy:0.1778, Validation Loss:2.4592, Validation Accuracy:0.1757\n",
    "Epoch #95: Loss:2.4310, Accuracy:0.1770, Validation Loss:2.4581, Validation Accuracy:0.1691\n",
    "Epoch #96: Loss:2.4327, Accuracy:0.1786, Validation Loss:2.4560, Validation Accuracy:0.1708\n",
    "Epoch #97: Loss:2.4324, Accuracy:0.1786, Validation Loss:2.4591, Validation Accuracy:0.1806\n",
    "Epoch #98: Loss:2.4318, Accuracy:0.1766, Validation Loss:2.4621, Validation Accuracy:0.1642\n",
    "Epoch #99: Loss:2.4326, Accuracy:0.1758, Validation Loss:2.4613, Validation Accuracy:0.1642\n",
    "Epoch #100: Loss:2.4339, Accuracy:0.1749, Validation Loss:2.4607, Validation Accuracy:0.1806\n",
    "Epoch #101: Loss:2.4356, Accuracy:0.1737, Validation Loss:2.4605, Validation Accuracy:0.1708\n",
    "Epoch #102: Loss:2.4337, Accuracy:0.1741, Validation Loss:2.4614, Validation Accuracy:0.1626\n",
    "Epoch #103: Loss:2.4341, Accuracy:0.1741, Validation Loss:2.4610, Validation Accuracy:0.1839\n",
    "Epoch #104: Loss:2.4341, Accuracy:0.1762, Validation Loss:2.4612, Validation Accuracy:0.1872\n",
    "Epoch #105: Loss:2.4362, Accuracy:0.1758, Validation Loss:2.4637, Validation Accuracy:0.1856\n",
    "Epoch #106: Loss:2.4356, Accuracy:0.1766, Validation Loss:2.4619, Validation Accuracy:0.1773\n",
    "Epoch #107: Loss:2.4354, Accuracy:0.1770, Validation Loss:2.4624, Validation Accuracy:0.1790\n",
    "Epoch #108: Loss:2.4359, Accuracy:0.1770, Validation Loss:2.4817, Validation Accuracy:0.1823\n",
    "Epoch #109: Loss:2.4501, Accuracy:0.1713, Validation Loss:2.4698, Validation Accuracy:0.1461\n",
    "Epoch #110: Loss:2.4479, Accuracy:0.1688, Validation Loss:2.4644, Validation Accuracy:0.1724\n",
    "Epoch #111: Loss:2.4396, Accuracy:0.1741, Validation Loss:2.4695, Validation Accuracy:0.1757\n",
    "Epoch #112: Loss:2.4368, Accuracy:0.1725, Validation Loss:2.4619, Validation Accuracy:0.1773\n",
    "Epoch #113: Loss:2.4360, Accuracy:0.1749, Validation Loss:2.4577, Validation Accuracy:0.1806\n",
    "Epoch #114: Loss:2.4347, Accuracy:0.1807, Validation Loss:2.4549, Validation Accuracy:0.1806\n",
    "Epoch #115: Loss:2.4310, Accuracy:0.1774, Validation Loss:2.4560, Validation Accuracy:0.1773\n",
    "Epoch #116: Loss:2.4306, Accuracy:0.1717, Validation Loss:2.4593, Validation Accuracy:0.1741\n",
    "Epoch #117: Loss:2.4307, Accuracy:0.1758, Validation Loss:2.4597, Validation Accuracy:0.1773\n",
    "Epoch #118: Loss:2.4310, Accuracy:0.1758, Validation Loss:2.4584, Validation Accuracy:0.1708\n",
    "Epoch #119: Loss:2.4309, Accuracy:0.1778, Validation Loss:2.4585, Validation Accuracy:0.1806\n",
    "Epoch #120: Loss:2.4314, Accuracy:0.1754, Validation Loss:2.4566, Validation Accuracy:0.1708\n",
    "Epoch #121: Loss:2.4300, Accuracy:0.1745, Validation Loss:2.4555, Validation Accuracy:0.1741\n",
    "Epoch #122: Loss:2.4295, Accuracy:0.1762, Validation Loss:2.4562, Validation Accuracy:0.1708\n",
    "Epoch #123: Loss:2.4295, Accuracy:0.1754, Validation Loss:2.4570, Validation Accuracy:0.1708\n",
    "Epoch #124: Loss:2.4285, Accuracy:0.1745, Validation Loss:2.4565, Validation Accuracy:0.1724\n",
    "Epoch #125: Loss:2.4288, Accuracy:0.1786, Validation Loss:2.4558, Validation Accuracy:0.1724\n",
    "Epoch #126: Loss:2.4293, Accuracy:0.1733, Validation Loss:2.4558, Validation Accuracy:0.1741\n",
    "Epoch #127: Loss:2.4299, Accuracy:0.1741, Validation Loss:2.4559, Validation Accuracy:0.1790\n",
    "Epoch #128: Loss:2.4293, Accuracy:0.1725, Validation Loss:2.4560, Validation Accuracy:0.1741\n",
    "Epoch #129: Loss:2.4281, Accuracy:0.1766, Validation Loss:2.4568, Validation Accuracy:0.1741\n",
    "Epoch #130: Loss:2.4305, Accuracy:0.1729, Validation Loss:2.4565, Validation Accuracy:0.1790\n",
    "Epoch #131: Loss:2.4278, Accuracy:0.1754, Validation Loss:2.4570, Validation Accuracy:0.1708\n",
    "Epoch #132: Loss:2.4293, Accuracy:0.1741, Validation Loss:2.4574, Validation Accuracy:0.1724\n",
    "Epoch #133: Loss:2.4278, Accuracy:0.1778, Validation Loss:2.4574, Validation Accuracy:0.1708\n",
    "Epoch #134: Loss:2.4293, Accuracy:0.1811, Validation Loss:2.4560, Validation Accuracy:0.1691\n",
    "Epoch #135: Loss:2.4324, Accuracy:0.1823, Validation Loss:2.4568, Validation Accuracy:0.1658\n",
    "Epoch #136: Loss:2.4307, Accuracy:0.1774, Validation Loss:2.4604, Validation Accuracy:0.1839\n",
    "Epoch #137: Loss:2.4303, Accuracy:0.1836, Validation Loss:2.4573, Validation Accuracy:0.1675\n",
    "Epoch #138: Loss:2.4290, Accuracy:0.1819, Validation Loss:2.4562, Validation Accuracy:0.1741\n",
    "Epoch #139: Loss:2.4288, Accuracy:0.1828, Validation Loss:2.4573, Validation Accuracy:0.1691\n",
    "Epoch #140: Loss:2.4295, Accuracy:0.1774, Validation Loss:2.4580, Validation Accuracy:0.1757\n",
    "Epoch #141: Loss:2.4287, Accuracy:0.1807, Validation Loss:2.4587, Validation Accuracy:0.1773\n",
    "Epoch #142: Loss:2.4313, Accuracy:0.1799, Validation Loss:2.4587, Validation Accuracy:0.1609\n",
    "Epoch #143: Loss:2.4256, Accuracy:0.1815, Validation Loss:2.4602, Validation Accuracy:0.1708\n",
    "Epoch #144: Loss:2.4282, Accuracy:0.1749, Validation Loss:2.4590, Validation Accuracy:0.1675\n",
    "Epoch #145: Loss:2.4256, Accuracy:0.1811, Validation Loss:2.4574, Validation Accuracy:0.1741\n",
    "Epoch #146: Loss:2.4250, Accuracy:0.1864, Validation Loss:2.4577, Validation Accuracy:0.1724\n",
    "Epoch #147: Loss:2.4246, Accuracy:0.1856, Validation Loss:2.4593, Validation Accuracy:0.1675\n",
    "Epoch #148: Loss:2.4249, Accuracy:0.1819, Validation Loss:2.4591, Validation Accuracy:0.1708\n",
    "Epoch #149: Loss:2.4247, Accuracy:0.1786, Validation Loss:2.4592, Validation Accuracy:0.1708\n",
    "Epoch #150: Loss:2.4227, Accuracy:0.1823, Validation Loss:2.4572, Validation Accuracy:0.1691\n",
    "Epoch #151: Loss:2.4230, Accuracy:0.1803, Validation Loss:2.4576, Validation Accuracy:0.1691\n",
    "Epoch #152: Loss:2.4251, Accuracy:0.1803, Validation Loss:2.4596, Validation Accuracy:0.1724\n",
    "Epoch #153: Loss:2.4227, Accuracy:0.1741, Validation Loss:2.4594, Validation Accuracy:0.1741\n",
    "Epoch #154: Loss:2.4226, Accuracy:0.1836, Validation Loss:2.4590, Validation Accuracy:0.1741\n",
    "Epoch #155: Loss:2.4209, Accuracy:0.1803, Validation Loss:2.4605, Validation Accuracy:0.1691\n",
    "Epoch #156: Loss:2.4219, Accuracy:0.1799, Validation Loss:2.4628, Validation Accuracy:0.1642\n",
    "Epoch #157: Loss:2.4231, Accuracy:0.1795, Validation Loss:2.4647, Validation Accuracy:0.1626\n",
    "Epoch #158: Loss:2.4260, Accuracy:0.1786, Validation Loss:2.4629, Validation Accuracy:0.1691\n",
    "Epoch #159: Loss:2.4259, Accuracy:0.1836, Validation Loss:2.4623, Validation Accuracy:0.1691\n",
    "Epoch #160: Loss:2.4281, Accuracy:0.1778, Validation Loss:2.4645, Validation Accuracy:0.1658\n",
    "Epoch #161: Loss:2.4258, Accuracy:0.1828, Validation Loss:2.4669, Validation Accuracy:0.1773\n",
    "Epoch #162: Loss:2.4273, Accuracy:0.1832, Validation Loss:2.4655, Validation Accuracy:0.1708\n",
    "Epoch #163: Loss:2.4291, Accuracy:0.1795, Validation Loss:2.4628, Validation Accuracy:0.1757\n",
    "Epoch #164: Loss:2.4327, Accuracy:0.1778, Validation Loss:2.4620, Validation Accuracy:0.1642\n",
    "Epoch #165: Loss:2.4334, Accuracy:0.1762, Validation Loss:2.4632, Validation Accuracy:0.1708\n",
    "Epoch #166: Loss:2.4318, Accuracy:0.1803, Validation Loss:2.4646, Validation Accuracy:0.1708\n",
    "Epoch #167: Loss:2.4319, Accuracy:0.1799, Validation Loss:2.4618, Validation Accuracy:0.1741\n",
    "Epoch #168: Loss:2.4306, Accuracy:0.1807, Validation Loss:2.4609, Validation Accuracy:0.1724\n",
    "Epoch #169: Loss:2.4312, Accuracy:0.1729, Validation Loss:2.4623, Validation Accuracy:0.1724\n",
    "Epoch #170: Loss:2.4328, Accuracy:0.1815, Validation Loss:2.4639, Validation Accuracy:0.1724\n",
    "Epoch #171: Loss:2.4311, Accuracy:0.1729, Validation Loss:2.4614, Validation Accuracy:0.1576\n",
    "Epoch #172: Loss:2.4295, Accuracy:0.1823, Validation Loss:2.4600, Validation Accuracy:0.1691\n",
    "Epoch #173: Loss:2.4269, Accuracy:0.1840, Validation Loss:2.4632, Validation Accuracy:0.1626\n",
    "Epoch #174: Loss:2.4278, Accuracy:0.1803, Validation Loss:2.4611, Validation Accuracy:0.1708\n",
    "Epoch #175: Loss:2.4276, Accuracy:0.1840, Validation Loss:2.4606, Validation Accuracy:0.1708\n",
    "Epoch #176: Loss:2.4287, Accuracy:0.1811, Validation Loss:2.4714, Validation Accuracy:0.1724\n",
    "Epoch #177: Loss:2.4549, Accuracy:0.1622, Validation Loss:2.4788, Validation Accuracy:0.1478\n",
    "Epoch #178: Loss:2.4538, Accuracy:0.1692, Validation Loss:2.4846, Validation Accuracy:0.1576\n",
    "Epoch #179: Loss:2.4500, Accuracy:0.1741, Validation Loss:2.4688, Validation Accuracy:0.1658\n",
    "Epoch #180: Loss:2.4375, Accuracy:0.1807, Validation Loss:2.4642, Validation Accuracy:0.1790\n",
    "Epoch #181: Loss:2.4349, Accuracy:0.1799, Validation Loss:2.4627, Validation Accuracy:0.1724\n",
    "Epoch #182: Loss:2.4330, Accuracy:0.1795, Validation Loss:2.4608, Validation Accuracy:0.1708\n",
    "Epoch #183: Loss:2.4318, Accuracy:0.1828, Validation Loss:2.4640, Validation Accuracy:0.1691\n",
    "Epoch #184: Loss:2.4315, Accuracy:0.1836, Validation Loss:2.4617, Validation Accuracy:0.1724\n",
    "Epoch #185: Loss:2.4300, Accuracy:0.1803, Validation Loss:2.4601, Validation Accuracy:0.1741\n",
    "Epoch #186: Loss:2.4295, Accuracy:0.1836, Validation Loss:2.4588, Validation Accuracy:0.1790\n",
    "Epoch #187: Loss:2.4286, Accuracy:0.1856, Validation Loss:2.4616, Validation Accuracy:0.1790\n",
    "Epoch #188: Loss:2.4272, Accuracy:0.1844, Validation Loss:2.4599, Validation Accuracy:0.1790\n",
    "Epoch #189: Loss:2.4277, Accuracy:0.1823, Validation Loss:2.4582, Validation Accuracy:0.1790\n",
    "Epoch #190: Loss:2.4276, Accuracy:0.1815, Validation Loss:2.4577, Validation Accuracy:0.1790\n",
    "Epoch #191: Loss:2.4312, Accuracy:0.1811, Validation Loss:2.4587, Validation Accuracy:0.1790\n",
    "Epoch #192: Loss:2.4284, Accuracy:0.1803, Validation Loss:2.4572, Validation Accuracy:0.1790\n",
    "Epoch #193: Loss:2.4284, Accuracy:0.1799, Validation Loss:2.4566, Validation Accuracy:0.1757\n",
    "Epoch #194: Loss:2.4284, Accuracy:0.1828, Validation Loss:2.4583, Validation Accuracy:0.1773\n",
    "Epoch #195: Loss:2.4271, Accuracy:0.1815, Validation Loss:2.4601, Validation Accuracy:0.1741\n",
    "Epoch #196: Loss:2.4263, Accuracy:0.1819, Validation Loss:2.4609, Validation Accuracy:0.1741\n",
    "Epoch #197: Loss:2.4255, Accuracy:0.1832, Validation Loss:2.4586, Validation Accuracy:0.1757\n",
    "Epoch #198: Loss:2.4237, Accuracy:0.1848, Validation Loss:2.4599, Validation Accuracy:0.1741\n",
    "Epoch #199: Loss:2.4239, Accuracy:0.1836, Validation Loss:2.4586, Validation Accuracy:0.1724\n",
    "Epoch #200: Loss:2.4235, Accuracy:0.1852, Validation Loss:2.4605, Validation Accuracy:0.1741\n",
    "Epoch #201: Loss:2.4229, Accuracy:0.1840, Validation Loss:2.4609, Validation Accuracy:0.1724\n",
    "Epoch #202: Loss:2.4228, Accuracy:0.1848, Validation Loss:2.4607, Validation Accuracy:0.1708\n",
    "Epoch #203: Loss:2.4240, Accuracy:0.1836, Validation Loss:2.4599, Validation Accuracy:0.1741\n",
    "Epoch #204: Loss:2.4230, Accuracy:0.1799, Validation Loss:2.4626, Validation Accuracy:0.1708\n",
    "Epoch #205: Loss:2.4236, Accuracy:0.1848, Validation Loss:2.4628, Validation Accuracy:0.1790\n",
    "Epoch #206: Loss:2.4229, Accuracy:0.1836, Validation Loss:2.4628, Validation Accuracy:0.1626\n",
    "Epoch #207: Loss:2.4215, Accuracy:0.1803, Validation Loss:2.4601, Validation Accuracy:0.1757\n",
    "Epoch #208: Loss:2.4209, Accuracy:0.1815, Validation Loss:2.4595, Validation Accuracy:0.1757\n",
    "Epoch #209: Loss:2.4203, Accuracy:0.1836, Validation Loss:2.4609, Validation Accuracy:0.1708\n",
    "Epoch #210: Loss:2.4214, Accuracy:0.1844, Validation Loss:2.4597, Validation Accuracy:0.1757\n",
    "Epoch #211: Loss:2.4209, Accuracy:0.1823, Validation Loss:2.4596, Validation Accuracy:0.1724\n",
    "Epoch #212: Loss:2.4197, Accuracy:0.1856, Validation Loss:2.4591, Validation Accuracy:0.1724\n",
    "Epoch #213: Loss:2.4198, Accuracy:0.1844, Validation Loss:2.4601, Validation Accuracy:0.1708\n",
    "Epoch #214: Loss:2.4204, Accuracy:0.1852, Validation Loss:2.4611, Validation Accuracy:0.1708\n",
    "Epoch #215: Loss:2.4231, Accuracy:0.1823, Validation Loss:2.4616, Validation Accuracy:0.1724\n",
    "Epoch #216: Loss:2.4216, Accuracy:0.1832, Validation Loss:2.4649, Validation Accuracy:0.1691\n",
    "Epoch #217: Loss:2.4239, Accuracy:0.1791, Validation Loss:2.4682, Validation Accuracy:0.1576\n",
    "Epoch #218: Loss:2.4220, Accuracy:0.1758, Validation Loss:2.4645, Validation Accuracy:0.1642\n",
    "Epoch #219: Loss:2.4200, Accuracy:0.1803, Validation Loss:2.4597, Validation Accuracy:0.1790\n",
    "Epoch #220: Loss:2.4185, Accuracy:0.1852, Validation Loss:2.4646, Validation Accuracy:0.1757\n",
    "Epoch #221: Loss:2.4208, Accuracy:0.1828, Validation Loss:2.4645, Validation Accuracy:0.1773\n",
    "Epoch #222: Loss:2.4205, Accuracy:0.1832, Validation Loss:2.4671, Validation Accuracy:0.1757\n",
    "Epoch #223: Loss:2.4211, Accuracy:0.1803, Validation Loss:2.4681, Validation Accuracy:0.1741\n",
    "Epoch #224: Loss:2.4203, Accuracy:0.1754, Validation Loss:2.4657, Validation Accuracy:0.1773\n",
    "Epoch #225: Loss:2.4216, Accuracy:0.1844, Validation Loss:2.4626, Validation Accuracy:0.1757\n",
    "Epoch #226: Loss:2.4186, Accuracy:0.1803, Validation Loss:2.4626, Validation Accuracy:0.1609\n",
    "Epoch #227: Loss:2.4160, Accuracy:0.1877, Validation Loss:2.4630, Validation Accuracy:0.1708\n",
    "Epoch #228: Loss:2.4149, Accuracy:0.1860, Validation Loss:2.4628, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4151, Accuracy:0.1852, Validation Loss:2.4593, Validation Accuracy:0.1741\n",
    "Epoch #230: Loss:2.4146, Accuracy:0.1836, Validation Loss:2.4597, Validation Accuracy:0.1691\n",
    "Epoch #231: Loss:2.4154, Accuracy:0.1856, Validation Loss:2.4613, Validation Accuracy:0.1741\n",
    "Epoch #232: Loss:2.4132, Accuracy:0.1840, Validation Loss:2.4632, Validation Accuracy:0.1741\n",
    "Epoch #233: Loss:2.4133, Accuracy:0.1864, Validation Loss:2.4637, Validation Accuracy:0.1790\n",
    "Epoch #234: Loss:2.4135, Accuracy:0.1848, Validation Loss:2.4629, Validation Accuracy:0.1773\n",
    "Epoch #235: Loss:2.4136, Accuracy:0.1860, Validation Loss:2.4639, Validation Accuracy:0.1741\n",
    "Epoch #236: Loss:2.4163, Accuracy:0.1832, Validation Loss:2.4642, Validation Accuracy:0.1823\n",
    "Epoch #237: Loss:2.4268, Accuracy:0.1823, Validation Loss:2.4712, Validation Accuracy:0.1527\n",
    "Epoch #238: Loss:2.4290, Accuracy:0.1770, Validation Loss:2.4642, Validation Accuracy:0.1708\n",
    "Epoch #239: Loss:2.4276, Accuracy:0.1844, Validation Loss:2.4649, Validation Accuracy:0.1691\n",
    "Epoch #240: Loss:2.4196, Accuracy:0.1774, Validation Loss:2.4680, Validation Accuracy:0.1691\n",
    "Epoch #241: Loss:2.4241, Accuracy:0.1807, Validation Loss:2.4616, Validation Accuracy:0.1724\n",
    "Epoch #242: Loss:2.4236, Accuracy:0.1836, Validation Loss:2.4608, Validation Accuracy:0.1741\n",
    "Epoch #243: Loss:2.4210, Accuracy:0.1873, Validation Loss:2.4579, Validation Accuracy:0.1741\n",
    "Epoch #244: Loss:2.4479, Accuracy:0.1795, Validation Loss:2.4642, Validation Accuracy:0.1576\n",
    "Epoch #245: Loss:2.4258, Accuracy:0.1786, Validation Loss:2.4596, Validation Accuracy:0.1708\n",
    "Epoch #246: Loss:2.4296, Accuracy:0.1811, Validation Loss:2.4607, Validation Accuracy:0.1708\n",
    "Epoch #247: Loss:2.4255, Accuracy:0.1848, Validation Loss:2.4619, Validation Accuracy:0.1658\n",
    "Epoch #248: Loss:2.4279, Accuracy:0.1844, Validation Loss:2.4603, Validation Accuracy:0.1757\n",
    "Epoch #249: Loss:2.4260, Accuracy:0.1836, Validation Loss:2.4626, Validation Accuracy:0.1691\n",
    "Epoch #250: Loss:2.4279, Accuracy:0.1828, Validation Loss:2.4665, Validation Accuracy:0.1708\n",
    "Epoch #251: Loss:2.4269, Accuracy:0.1852, Validation Loss:2.4629, Validation Accuracy:0.1675\n",
    "Epoch #252: Loss:2.4242, Accuracy:0.1852, Validation Loss:2.4622, Validation Accuracy:0.1724\n",
    "Epoch #253: Loss:2.4240, Accuracy:0.1832, Validation Loss:2.4595, Validation Accuracy:0.1790\n",
    "Epoch #254: Loss:2.4242, Accuracy:0.1786, Validation Loss:2.4581, Validation Accuracy:0.1790\n",
    "Epoch #255: Loss:2.4245, Accuracy:0.1832, Validation Loss:2.4590, Validation Accuracy:0.1806\n",
    "Epoch #256: Loss:2.4227, Accuracy:0.1819, Validation Loss:2.4608, Validation Accuracy:0.1790\n",
    "Epoch #257: Loss:2.4233, Accuracy:0.1795, Validation Loss:2.4614, Validation Accuracy:0.1724\n",
    "Epoch #258: Loss:2.4221, Accuracy:0.1774, Validation Loss:2.4639, Validation Accuracy:0.1724\n",
    "Epoch #259: Loss:2.4221, Accuracy:0.1815, Validation Loss:2.4638, Validation Accuracy:0.1741\n",
    "Epoch #260: Loss:2.4223, Accuracy:0.1803, Validation Loss:2.4640, Validation Accuracy:0.1741\n",
    "Epoch #261: Loss:2.4200, Accuracy:0.1836, Validation Loss:2.4662, Validation Accuracy:0.1790\n",
    "Epoch #262: Loss:2.4201, Accuracy:0.1873, Validation Loss:2.4617, Validation Accuracy:0.1790\n",
    "Epoch #263: Loss:2.4189, Accuracy:0.1811, Validation Loss:2.4628, Validation Accuracy:0.1773\n",
    "Epoch #264: Loss:2.4198, Accuracy:0.1852, Validation Loss:2.4668, Validation Accuracy:0.1757\n",
    "Epoch #265: Loss:2.4198, Accuracy:0.1819, Validation Loss:2.4675, Validation Accuracy:0.1741\n",
    "Epoch #266: Loss:2.4170, Accuracy:0.1860, Validation Loss:2.4666, Validation Accuracy:0.1757\n",
    "Epoch #267: Loss:2.4166, Accuracy:0.1873, Validation Loss:2.4686, Validation Accuracy:0.1757\n",
    "Epoch #268: Loss:2.4170, Accuracy:0.1778, Validation Loss:2.4670, Validation Accuracy:0.1790\n",
    "Epoch #269: Loss:2.4153, Accuracy:0.1860, Validation Loss:2.4685, Validation Accuracy:0.1790\n",
    "Epoch #270: Loss:2.4157, Accuracy:0.1864, Validation Loss:2.4654, Validation Accuracy:0.1790\n",
    "Epoch #271: Loss:2.4148, Accuracy:0.1832, Validation Loss:2.4680, Validation Accuracy:0.1790\n",
    "Epoch #272: Loss:2.4139, Accuracy:0.1836, Validation Loss:2.4662, Validation Accuracy:0.1773\n",
    "Epoch #273: Loss:2.4128, Accuracy:0.1844, Validation Loss:2.4641, Validation Accuracy:0.1724\n",
    "Epoch #274: Loss:2.4112, Accuracy:0.1852, Validation Loss:2.4632, Validation Accuracy:0.1790\n",
    "Epoch #275: Loss:2.4127, Accuracy:0.1864, Validation Loss:2.4665, Validation Accuracy:0.1757\n",
    "Epoch #276: Loss:2.4137, Accuracy:0.1819, Validation Loss:2.4666, Validation Accuracy:0.1773\n",
    "Epoch #277: Loss:2.4136, Accuracy:0.1836, Validation Loss:2.4680, Validation Accuracy:0.1626\n",
    "Epoch #278: Loss:2.4144, Accuracy:0.1844, Validation Loss:2.4648, Validation Accuracy:0.1773\n",
    "Epoch #279: Loss:2.4121, Accuracy:0.1877, Validation Loss:2.4656, Validation Accuracy:0.1626\n",
    "Epoch #280: Loss:2.4146, Accuracy:0.1819, Validation Loss:2.4661, Validation Accuracy:0.1757\n",
    "Epoch #281: Loss:2.4114, Accuracy:0.1848, Validation Loss:2.4686, Validation Accuracy:0.1741\n",
    "Epoch #282: Loss:2.4106, Accuracy:0.1799, Validation Loss:2.4686, Validation Accuracy:0.1757\n",
    "Epoch #283: Loss:2.4118, Accuracy:0.1860, Validation Loss:2.4663, Validation Accuracy:0.1806\n",
    "Epoch #284: Loss:2.4115, Accuracy:0.1856, Validation Loss:2.4684, Validation Accuracy:0.1790\n",
    "Epoch #285: Loss:2.4122, Accuracy:0.1860, Validation Loss:2.4695, Validation Accuracy:0.1773\n",
    "Epoch #286: Loss:2.4111, Accuracy:0.1852, Validation Loss:2.4710, Validation Accuracy:0.1806\n",
    "Epoch #287: Loss:2.4107, Accuracy:0.1856, Validation Loss:2.4711, Validation Accuracy:0.1839\n",
    "Epoch #288: Loss:2.4117, Accuracy:0.1889, Validation Loss:2.4726, Validation Accuracy:0.1757\n",
    "Epoch #289: Loss:2.4165, Accuracy:0.1881, Validation Loss:2.4663, Validation Accuracy:0.1724\n",
    "Epoch #290: Loss:2.4166, Accuracy:0.1885, Validation Loss:2.4664, Validation Accuracy:0.1708\n",
    "Epoch #291: Loss:2.4159, Accuracy:0.1881, Validation Loss:2.4690, Validation Accuracy:0.1626\n",
    "Epoch #292: Loss:2.4159, Accuracy:0.1762, Validation Loss:2.4709, Validation Accuracy:0.1675\n",
    "Epoch #293: Loss:2.4139, Accuracy:0.1840, Validation Loss:2.4719, Validation Accuracy:0.1708\n",
    "Epoch #294: Loss:2.4132, Accuracy:0.1869, Validation Loss:2.4679, Validation Accuracy:0.1708\n",
    "Epoch #295: Loss:2.4114, Accuracy:0.1844, Validation Loss:2.4660, Validation Accuracy:0.1675\n",
    "Epoch #296: Loss:2.4126, Accuracy:0.1877, Validation Loss:2.4638, Validation Accuracy:0.1773\n",
    "Epoch #297: Loss:2.4118, Accuracy:0.1832, Validation Loss:2.4634, Validation Accuracy:0.1823\n",
    "Epoch #298: Loss:2.4112, Accuracy:0.1860, Validation Loss:2.4651, Validation Accuracy:0.1675\n",
    "Epoch #299: Loss:2.4105, Accuracy:0.1819, Validation Loss:2.4634, Validation Accuracy:0.1773\n",
    "Epoch #300: Loss:2.4104, Accuracy:0.1832, Validation Loss:2.4652, Validation Accuracy:0.1806\n",
    "\n",
    "Test:\n",
    "Test Loss:2.46518040, Accuracy:0.1806\n",
    "Labels: ['ce', 'ek', 'mb', 'eg', 'yd', 'ib', 'sg', 'eo', 'eb', 'by', 'ds', 'ck', 'sk', 'my', 'aa']\n",
    "Confusion Matrix:\n",
    "      ce  ek  mb  eg  yd  ib  sg  eo  eb  by  ds  ck  sk  my  aa\n",
    "t:ce   0   0   0  10   2   0  10   1   2   0   2   0   0   0   0\n",
    "t:ek   0   0   0  14   7   2  19   1   1   0   4   0   0   0   0\n",
    "t:mb   0   0   0   8   7   2  27   3   4   0   1   0   0   0   0\n",
    "t:eg   0   0   0  29   0   0   8   0   4   0   9   0   0   0   0\n",
    "t:yd   0   0   0   1  35   1  24   0   0   0   1   0   0   0   0\n",
    "t:ib   0   0   0   5  27   2  15   1   2   0   2   0   0   0   0\n",
    "t:sg   0   0   0   7   9   0  33   1   1   0   0   0   0   0   0\n",
    "t:eo   0   0   0   1   2   0  28   0   2   0   1   0   0   0   0\n",
    "t:eb   0   0   0  18   5   1  18   1   5   0   2   0   0   0   0\n",
    "t:by   0   0   0   6   2   1  23   2   5   0   1   0   0   0   0\n",
    "t:ds   0   0   0  13   0   0   9   1   2   1   5   0   0   0   0\n",
    "t:ck   0   0   0   7   0   0   9   1   5   0   1   0   0   0   0\n",
    "t:sk   0   0   0  14   2   0   9   0   5   0   3   0   0   0   0\n",
    "t:my   0   0   0   4   4   3   8   0   0   0   1   0   0   0   0\n",
    "t:aa   0   0   0  13   2   0   9   1   2   0   6   0   0   0   1\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          eg       0.19      0.58      0.29        50\n",
    "          yd       0.34      0.56      0.42        62\n",
    "          ib       0.17      0.04      0.06        54\n",
    "          sg       0.13      0.65      0.22        51\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          eb       0.12      0.10      0.11        50\n",
    "          by       0.00      0.00      0.00        40\n",
    "          ds       0.13      0.16      0.14        31\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          my       0.00      0.00      0.00        20\n",
    "          aa       1.00      0.03      0.06        34\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.14      0.14      0.09       609\n",
    "weighted avg       0.15      0.18      0.11       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 01:54:39 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 42 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7126788584078083, 2.702912332584901, 2.6951333360718976, 2.688594355763277, 2.682910241125448, 2.676258273508357, 2.6695960366667197, 2.6633826942475167, 2.6561048653325425, 2.6483902919468623, 2.6388460211761675, 2.627929165249779, 2.6137737718904743, 2.599168311590436, 2.5807103312074258, 2.5643616125892925, 2.545747585484547, 2.5339064746850424, 2.52475605848779, 2.5109886179612384, 2.506491457104487, 2.4969338012250577, 2.4933055116625256, 2.4867596865091808, 2.4804683334525977, 2.4775010725156035, 2.4768391066584092, 2.4707187983790053, 2.46946696773147, 2.468022425186458, 2.4663285935062103, 2.4645584248165386, 2.46403755734511, 2.4641886262470867, 2.4676862553814165, 2.464782971662449, 2.465556537380751, 2.4645521405882436, 2.4612284603181536, 2.464990238055024, 2.4600931772066064, 2.459571406171827, 2.45722017577912, 2.4550417577496106, 2.4563087865049615, 2.45556446484157, 2.4552133036364476, 2.4561117764176994, 2.458032780680163, 2.4581820608555587, 2.458393491547683, 2.458237882513914, 2.457237774515387, 2.458375426153048, 2.4562157562800815, 2.4584953174215234, 2.4603025913238525, 2.4617326314421906, 2.4568573285401945, 2.458908019590456, 2.4569664154146693, 2.4573215561351556, 2.460219740280377, 2.457027443132573, 2.4590356021091857, 2.4548703187400682, 2.4547985927224745, 2.456248284560706, 2.455435984631869, 2.4558070081795376, 2.456057680847218, 2.457679151118487, 2.4552930431021456, 2.4581451408185786, 2.4561533403318307, 2.457166581318296, 2.457459072369856, 2.4556298032770014, 2.458078452519008, 2.4557463406342004, 2.4564232763594203, 2.455512957815662, 2.4557378507404297, 2.4553948353076804, 2.456852960273354, 2.4568596109380865, 2.457593185365298, 2.457837617651778, 2.469469802132968, 2.4672274566049057, 2.465331830414645, 2.460455576187284, 2.461054246413884, 2.4591917776317627, 2.4581215917966244, 2.4559820191613557, 2.459093332682141, 2.4621004130452726, 2.4613177130375004, 2.4607409781031615, 2.4604735899049857, 2.461368690374841, 2.4609956984058, 2.461242384511262, 2.4636989074387574, 2.4618654356801453, 2.462377306667259, 2.481718088801467, 2.4697892544500544, 2.4643819707955044, 2.4695167772484137, 2.4618971845003577, 2.4577415964286318, 2.4548550251082246, 2.4560118849054344, 2.4592616530670517, 2.4596867208997604, 2.4583861620359624, 2.458489122453386, 2.4566317258405763, 2.455549453866893, 2.4561707178751626, 2.457028212022703, 2.456470079218421, 2.4558077563205964, 2.455824086623043, 2.455885623671934, 2.455982224303122, 2.456756322058942, 2.4564541341440234, 2.4570097700128413, 2.4574377082643055, 2.457431148425699, 2.456043936348901, 2.456771131415281, 2.4603989707621055, 2.4573034143995964, 2.4561833618896936, 2.4573227368747856, 2.457966548077187, 2.4586932373360066, 2.458723535287165, 2.46018131182503, 2.4590397601448646, 2.4573869254984486, 2.4577012982078763, 2.459275166193644, 2.459101596685074, 2.459248471925607, 2.4572013837754825, 2.4576271817406212, 2.459615754767983, 2.459365807926322, 2.459012751900308, 2.4604885644709142, 2.462778766362734, 2.4647311870687703, 2.4629370496778065, 2.4623180390970263, 2.4645111513842504, 2.4668774307263504, 2.4654849534747245, 2.462807609529918, 2.4620000337340757, 2.463184611550693, 2.4646062158011453, 2.4618436918274327, 2.4609102024429146, 2.4623296264748658, 2.46391153687914, 2.461376787993708, 2.460010052510279, 2.463195567452066, 2.4611135723164126, 2.4605625376521267, 2.471424664182616, 2.478848232620064, 2.4846374671447453, 2.4688390894672163, 2.4641636263560778, 2.462665225484688, 2.4608167282661975, 2.4639859148629975, 2.4617205774059827, 2.4601247067913437, 2.458822477431524, 2.4615851442997876, 2.459917775907344, 2.458237754496056, 2.4577092838600545, 2.4587174965243035, 2.457223521860558, 2.4565656713664237, 2.4582983531388156, 2.460148293788014, 2.460851252372629, 2.458623571348895, 2.459913636663277, 2.4585630596173416, 2.460533506568821, 2.4608989908973182, 2.460707172775895, 2.459874825328833, 2.4625972166828727, 2.4628118294213204, 2.462805354144969, 2.4600808847518194, 2.459478247146105, 2.4609357758695856, 2.459698634390369, 2.459553198274133, 2.4591166765623296, 2.4601132360780964, 2.4610590523686904, 2.461631178268658, 2.4649142706139724, 2.468214939772006, 2.4644850114687715, 2.459674680174278, 2.4645508044263216, 2.4644563307707337, 2.467102635670178, 2.4681473690496487, 2.465747283205806, 2.46263993197474, 2.462614259109121, 2.4629938809938228, 2.4628262245792083, 2.4593120545197786, 2.459710361922316, 2.461284436615817, 2.4632296080659764, 2.4636999073091204, 2.4628762353229994, 2.4638989147881567, 2.46422441722137, 2.4711693961827823, 2.4641972993590757, 2.4648518538827378, 2.4680143776785566, 2.461589651937751, 2.460804815950065, 2.4578611752865545, 2.4641616387516017, 2.4596438266960856, 2.460724589468419, 2.4619065885277607, 2.460292038267665, 2.4626120655798953, 2.466518531291943, 2.46293994006265, 2.4622192179236704, 2.4594524035900096, 2.458145389024456, 2.4590000879196894, 2.460755598368903, 2.4613572650746565, 2.463856476672569, 2.463803554011879, 2.4639870411852507, 2.466238991967563, 2.4617250545075766, 2.4628055009544383, 2.4667564432805005, 2.467526881369855, 2.466606359372194, 2.4686327736170224, 2.4670165242819952, 2.4684984605692093, 2.465443316351604, 2.4679532000192475, 2.4661924549315755, 2.464128704885348, 2.4632445520955355, 2.4664973684132394, 2.4665767931194336, 2.46795799696974, 2.4647606695422595, 2.465550899897107, 2.466135814663616, 2.4685532396845824, 2.4686401696823697, 2.466325996348815, 2.4684368487453616, 2.469475922717641, 2.4710115705217635, 2.47105201594348, 2.472607555452043, 2.4663438781337392, 2.4663876872540302, 2.4690305211861143, 2.4709251525954072, 2.471911575210897, 2.4679222929066627, 2.466047450239435, 2.4637548751235987, 2.463359602566423, 2.4650750442091467, 2.4634315869686834, 2.4651806201840856], 'val_acc': [0.07060755315819398, 0.10180623962716712, 0.10180623962716712, 0.10180623962716712, 0.10673234800155136, 0.1050903118767566, 0.11001642034901382, 0.11822660097298755, 0.12972085275771386, 0.1346469611320981, 0.15106732257579153, 0.16091953932456, 0.16256157544935473, 0.1658456477968172, 0.16912972024215267, 0.16091953952030597, 0.1658456477968172, 0.1658456478946902, 0.1658456477968172, 0.1642036118677684, 0.1642036118677684, 0.16912972014427968, 0.1707717563669474, 0.1625615756451007, 0.16256157574297367, 0.16748768401948494, 0.18062397311571587, 0.18883415393543557, 0.18062397311571587, 0.18390804556105134, 0.1855500815879731, 0.18883415383756258, 0.18883415393543557, 0.18883415403330855, 0.18555008168584608, 0.1839080456589243, 0.18062397331146185, 0.17241379258961514, 0.1707717563669474, 0.18390804556105134, 0.18883415383756258, 0.1822660094362566, 0.18719211771276784, 0.18719211781064082, 0.18883415393543557, 0.18883415393543557, 0.18883415393543557, 0.18883415393543557, 0.18883415393543557, 0.18883415393543557, 0.18883415393543557, 0.18883415393543557, 0.18883415393543557, 0.1806239734093348, 0.18555008168584608, 0.18883415393543557, 0.17733990106187233, 0.17898193728454007, 0.18555008188159203, 0.1806239734093348, 0.1806239734093348, 0.17733990115974532, 0.1806239734093348, 0.18062397291996993, 0.1806239734093348, 0.17898193748028604, 0.1806239734093348, 0.18883415393543557, 0.1806239734093348, 0.1773399008661264, 0.18226600914263763, 0.1806239734093348, 0.18390804536530536, 0.1806239734093348, 0.18062397311571587, 0.18226600924051062, 0.1806239734093348, 0.18062397360508078, 0.17898193728454007, 0.18062397291996993, 0.18226600904476467, 0.18555008139222715, 0.17898193748028604, 0.17569786513282357, 0.17733990057250745, 0.18062397291996993, 0.17733990057250745, 0.17405582900802882, 0.16256157603659263, 0.17733990057250745, 0.18062397311571587, 0.18390804536530536, 0.16912972014427968, 0.1756978649370776, 0.1691297200464067, 0.17077175617120144, 0.1806239734093348, 0.16420361157414948, 0.16420361157414948, 0.1806239734093348, 0.17077175607332848, 0.16256157544935473, 0.1839080456589243, 0.1871921179085138, 0.18555008178371907, 0.17733990096399938, 0.17898193708879412, 0.18226600924051062, 0.14614121499662525, 0.1724137926874881, 0.1756978649370776, 0.17733990106187233, 0.18062397321358886, 0.18062397311571587, 0.17733990106187233, 0.1740558287144099, 0.17733990096399938, 0.1707717559754555, 0.18062397311571587, 0.1707717558775825, 0.1740558286165369, 0.1707717558775825, 0.17077175646482037, 0.17241379249174216, 0.17241379200237725, 0.1740558286165369, 0.17898193689304814, 0.174055828127172, 0.1740558286165369, 0.17898193708879412, 0.17077175646482037, 0.17241379210025023, 0.17077175646482037, 0.1691297199485337, 0.16584564750319827, 0.18390804575679728, 0.16748768382373896, 0.17405582881228285, 0.16912972034002563, 0.1756978649370776, 0.1773399008661264, 0.16091954011977797, 0.17077175646482037, 0.16748768441097686, 0.17405582881228285, 0.17241379210025023, 0.167487683725866, 0.17077175646482037, 0.17077175646482037, 0.16912971985066075, 0.16912971985066075, 0.17241379219812322, 0.1740558287144099, 0.1740558287144099, 0.16912971985066075, 0.16420361137840353, 0.1625615752536088, 0.1691297200464067, 0.1691297200464067, 0.16584564859203518, 0.17733990096399938, 0.17077175626907443, 0.17569786464345866, 0.16420361176989545, 0.17077175626907443, 0.17077175626907443, 0.1740558286165369, 0.17241379239386917, 0.17241379249174216, 0.17241379258961514, 0.15763546796806144, 0.16912972024215267, 0.1625615752536088, 0.17077175626907443, 0.17077175626907443, 0.17241379258961514, 0.14778325092567404, 0.15763546687922456, 0.1658456478946902, 0.17898193699092113, 0.17241379239386917, 0.17077175617120144, 0.16912972024215267, 0.17241379239386917, 0.1740558286165369, 0.17898193708879412, 0.1789819371866671, 0.1789819371866671, 0.1789819371866671, 0.1789819371866671, 0.1789819371866671, 0.1789819371866671, 0.1756978649370776, 0.17733990106187233, 0.1740558287144099, 0.1740558287144099, 0.17569786483920463, 0.1740558287144099, 0.17241379258961514, 0.1740558287144099, 0.17241379258961514, 0.1707717563669474, 0.1740558286165369, 0.1707717563669474, 0.17898193708879412, 0.1625615752536088, 0.17569786474133164, 0.17569786474133164, 0.1707717563669474, 0.17569786474133164, 0.17241379249174216, 0.17241379249174216, 0.1707717563669474, 0.17077175626907443, 0.17241379249174216, 0.16912972014427968, 0.15763546796806144, 0.16420361157414948, 0.17898193699092113, 0.17569786474133164, 0.1773399008661264, 0.17569786483920463, 0.1740558286165369, 0.1773399008661264, 0.17569786474133164, 0.16091953912881404, 0.1707717563669474, 0.16912971975278776, 0.1740558287144099, 0.16912972034002563, 0.1740558285186639, 0.1740558287144099, 0.17898193679517518, 0.17733990067038044, 0.1740558286165369, 0.18226600924051062, 0.15270935939793126, 0.1707717563669474, 0.16912972014427968, 0.1691297200464067, 0.17241379229599618, 0.1740558285186639, 0.1740558286165369, 0.15763546727071645, 0.1707717563669474, 0.1707717563669474, 0.1658456477968172, 0.17569786464345866, 0.16912972014427968, 0.1707717563669474, 0.16748768401948494, 0.17241379239386917, 0.17898193689304814, 0.17898193689304814, 0.1806239730178429, 0.17898193689304814, 0.17241379239386917, 0.17241379249174216, 0.1740558286165369, 0.1740558286165369, 0.17898193699092113, 0.17898193699092113, 0.1773399008661264, 0.17569786464345866, 0.1740558285186639, 0.17569786464345866, 0.17569786464345866, 0.17898193689304814, 0.17898193689304814, 0.17898193689304814, 0.17898193689304814, 0.1773399007682534, 0.17241379239386917, 0.17898193689304814, 0.17569786464345866, 0.1773399007682534, 0.16256157634244567, 0.1773399007682534, 0.16256157634244567, 0.17569786464345866, 0.1740558285186639, 0.17569786464345866, 0.1806239730178429, 0.17898193689304814, 0.1773399007682534, 0.1806239730178429, 0.18390804516955941, 0.17569786454558567, 0.17241379239386917, 0.17077175617120144, 0.16256157544935473, 0.16748768392161195, 0.17077175607332848, 0.17077175607332848, 0.16748768461895694, 0.17733990057250745, 0.18226600914263763, 0.16748768382373896, 0.1773399007682534, 0.18062397311571587], 'loss': [2.7211099498325795, 2.708435332358985, 2.699884188493419, 2.6928360149845694, 2.686619799142011, 2.6807085913554354, 2.674376175486821, 2.668208371834099, 2.661556128946418, 2.6544540310297657, 2.645652234725639, 2.6352176892439196, 2.6231250387931997, 2.6078524939088608, 2.5911931483407775, 2.572942540190303, 2.5585213202960193, 2.5395774390907992, 2.5263394540829824, 2.5194871010476803, 2.5139425718319246, 2.503182502448926, 2.494052559443323, 2.487713081195369, 2.4817212916497575, 2.4782426979017944, 2.4744190935726285, 2.4715974709826085, 2.4674459844154497, 2.4644621031485054, 2.463906596965124, 2.462350274796848, 2.459084470658821, 2.457363311661832, 2.455807983312274, 2.4564984742620886, 2.455484251417908, 2.455347974393402, 2.4522383916059804, 2.45528895008001, 2.451973112016243, 2.449392074823869, 2.4482947611955646, 2.4476697963855596, 2.448542932808032, 2.446165397181893, 2.4455444227253875, 2.4433094586679824, 2.445417047770851, 2.4448110253414335, 2.443904165271861, 2.4440209344671984, 2.445409991657954, 2.4454611216237656, 2.443665766960786, 2.4443081927250545, 2.4452781032977406, 2.4449996469446766, 2.4437232475750745, 2.440532818369307, 2.4423782535646974, 2.441707985308136, 2.442432929064459, 2.4406406683598703, 2.43940419014964, 2.440089137059707, 2.439315353232977, 2.4384664457191922, 2.4380471451816126, 2.438493596846563, 2.4382981469743794, 2.4415647656521022, 2.4373583257075944, 2.4371902394343694, 2.4368615007498424, 2.4367148324939016, 2.4362093538719036, 2.4364285056840713, 2.435820203444307, 2.434240262855984, 2.434014346222613, 2.434280933834444, 2.434526303857259, 2.4350969058042677, 2.4347678054284754, 2.4328029765730275, 2.433456707881951, 2.433022350256448, 2.4376990178282503, 2.4399995729418995, 2.4402716890252836, 2.437995783455318, 2.4356601603466874, 2.433529186052953, 2.4310385513109836, 2.43269734617621, 2.432384619330968, 2.4317998081262107, 2.4326369037863165, 2.433943382474676, 2.435648663088037, 2.433681201641075, 2.4341209866917355, 2.4341007406951465, 2.436161781875015, 2.4356091855732567, 2.43541745140812, 2.4358832916439925, 2.450115309211997, 2.4479089450052878, 2.439557889942271, 2.436819070957035, 2.4359549306990917, 2.4347199447835495, 2.430992462796597, 2.430604115746594, 2.430663458669455, 2.4309736623901115, 2.4308571806678536, 2.4313525497546187, 2.429961608420652, 2.4294548601585246, 2.4295118799934152, 2.428501300204706, 2.4288143174604224, 2.429328173827342, 2.429943406459487, 2.429339670596426, 2.4281460961766803, 2.4304579244257245, 2.4277623984358394, 2.4292506675211065, 2.427809091270337, 2.4293072686792643, 2.432362638802499, 2.430730069491408, 2.4303477941352485, 2.429047386259514, 2.4287611108541, 2.429521276622827, 2.428730701274206, 2.431289632511335, 2.4255656513590096, 2.428162791547834, 2.425550508792885, 2.4250422290708005, 2.424613013453552, 2.4249150880308368, 2.4246717804511224, 2.4227153763389198, 2.422958584438849, 2.4250668332806846, 2.422672232661159, 2.4225786966954415, 2.4209101699460946, 2.421901817928839, 2.423094393732122, 2.4259959775074798, 2.4258911512715615, 2.4280607816864577, 2.425795551589872, 2.427346298288271, 2.4290962271132264, 2.432678888661661, 2.4333885439612293, 2.431778125351704, 2.431855083246251, 2.430639058804365, 2.4312102322705718, 2.432754235297013, 2.431136454595922, 2.429498493989635, 2.4269179899344944, 2.4278340122293396, 2.4275726165614824, 2.428737916006445, 2.454895786974709, 2.4537691316075882, 2.449951115672838, 2.437462564758207, 2.4349188197564784, 2.433023537649511, 2.4317785433430448, 2.431527462828086, 2.4299697275768803, 2.4294961876448173, 2.4286105104050844, 2.427179881728405, 2.4277128042381646, 2.4275652402725063, 2.4311561919335714, 2.4284433962138525, 2.428422468545745, 2.4283964771020092, 2.427064358869862, 2.426259022131103, 2.425496018544849, 2.423747906303014, 2.4238566650257463, 2.4235275406612264, 2.4228817806596386, 2.42280963451221, 2.424029942361726, 2.423026662489717, 2.423601422907146, 2.4228728309059533, 2.42145047922154, 2.4208848394162845, 2.4202508787354895, 2.421387879412767, 2.420945670717306, 2.419714425085017, 2.4198077995919105, 2.4204203121960775, 2.4231095820236987, 2.421590048380701, 2.4238888708228203, 2.4220419033841676, 2.420038298268093, 2.4184675816882564, 2.4208169592479414, 2.4205153982252554, 2.421112105489022, 2.4203097777200187, 2.421649029318557, 2.4186341289622093, 2.416046215131787, 2.4149147931555213, 2.4150960541358963, 2.414629748029141, 2.4154104238662875, 2.4131880215795625, 2.413284107889728, 2.413527416154834, 2.413624164651796, 2.4163205699019854, 2.4267504045850687, 2.42900161371094, 2.4275837437083343, 2.4195874457976165, 2.424118820891488, 2.4235870928245404, 2.4210325133384374, 2.4479115556642506, 2.4258082586637024, 2.4295593478106863, 2.42552285781876, 2.4278558918093265, 2.425956097079988, 2.4279077454514084, 2.4269092680247657, 2.42422675322703, 2.4240028356869363, 2.42417461984701, 2.424536239733686, 2.4226783983516498, 2.42331004475666, 2.4221412865288205, 2.4220967446509327, 2.422285110700791, 2.4200214744348547, 2.420088414930465, 2.4189478239729176, 2.419752238761228, 2.4197538433623267, 2.4169692190276035, 2.4165618215987816, 2.4170014304057283, 2.415286938806334, 2.415738699617327, 2.41477551734423, 2.4138717042347246, 2.4128315070816133, 2.411165308413809, 2.4126935163807333, 2.4136541938390086, 2.4135601017264614, 2.414417223470167, 2.4120611398371827, 2.4145780598603235, 2.411371297268407, 2.410611410845966, 2.4117505337912934, 2.411497918683156, 2.412178516192113, 2.4111256077549053, 2.4107186023704323, 2.411657475935605, 2.416487021416854, 2.416648599988871, 2.415891241880413, 2.415940561137895, 2.4138585691824095, 2.4131876647839556, 2.4114172270655385, 2.412581673197188, 2.4117733029124673, 2.411214360074586, 2.4104587050678794, 2.4104229244608164], 'acc': [0.06611909667293883, 0.08788501039851128, 0.10225872644347576, 0.10225872684430783, 0.10266940504985668, 0.10636550361248502, 0.10965092416539085, 0.11088295642294188, 0.11909650898016454, 0.12566735126093428, 0.13593429168514157, 0.15154004195509996, 0.1552361393427702, 0.15728952841225102, 0.1577002050511891, 0.1593429161109474, 0.16221765963326246, 0.16098562599574762, 0.15728952862643608, 0.16303901447774938, 0.1597535933373645, 0.16591375819589077, 0.16591375680674764, 0.1650924035472302, 0.16098562619157394, 0.1663244362056133, 0.1630390140860967, 0.16673511261200757, 0.17207392088196852, 0.17248459949752878, 0.16919917753712108, 0.1761806984334511, 0.17166324463468313, 0.17577002102956635, 0.17371663174590046, 0.17207392107779484, 0.1737166335266963, 0.17207392187945897, 0.17207392227111165, 0.17494866440428355, 0.17659137605152092, 0.17453798796117184, 0.17535934262819114, 0.17577002142121906, 0.1761806988251038, 0.17741273071854022, 0.17577002142121906, 0.1765913766757174, 0.17577001985460824, 0.17782340773077226, 0.17741273052271386, 0.17618069724013427, 0.17248459908751737, 0.17618069864763616, 0.1761806984518098, 0.1761806992351152, 0.18069815203639272, 0.17659137546404186, 0.17577002042372858, 0.18110882965446254, 0.17535934241400608, 0.17330595530278875, 0.17741273011270245, 0.17782340771241353, 0.18069815125308733, 0.17289527750725128, 0.17577002022790222, 0.17741273029017007, 0.18069815105726098, 0.17741273167931323, 0.17823408732546428, 0.1778234095115681, 0.1790554413682871, 0.17494866559760036, 0.1786447637502173, 0.18069815283805682, 0.18193018412565548, 0.1765913764431736, 0.17700205347376438, 0.18028747383084384, 0.17741273011270245, 0.17946611996548867, 0.1765913752498568, 0.1782340855263097, 0.17659137605152092, 0.17946611839887788, 0.17946611822141026, 0.17864476353603223, 0.17330595373617794, 0.1794661185947042, 0.18398357219764583, 0.17987679582112134, 0.17946611957383596, 0.17782340851407766, 0.17700205407960215, 0.17864476433769633, 0.17864476355439093, 0.17659137687154375, 0.17577002163540412, 0.1749486646001099, 0.17371663194172682, 0.17412731053892835, 0.17412730957815534, 0.17618069862927743, 0.17577002142121906, 0.17659137685318502, 0.17700205249463263, 0.177002053651232, 0.1712525668391457, 0.1687885019140322, 0.17412730936397028, 0.17248460008500782, 0.17494866577506799, 0.18069815303388317, 0.1774127311101929, 0.17166324386973644, 0.1757700196404232, 0.17577001985460824, 0.17782340911991543, 0.17535934300148512, 0.17453798797953055, 0.17618069806015713, 0.1753593436073229, 0.17453798874447723, 0.1786447633769233, 0.17330595412783065, 0.17412730975562296, 0.1724846004583018, 0.17659137663899996, 0.1728952783089154, 0.17535934243236478, 0.17412730996980805, 0.17782340970739446, 0.18110883006447395, 0.18234086272285704, 0.17741273048599643, 0.18357289436539095, 0.1819301850864285, 0.18275153952090403, 0.17741273148348688, 0.18069815244640414, 0.1798767972102645, 0.1815195066850533, 0.17494866520594768, 0.18110882865697206, 0.18644763751441204, 0.18562628286575145, 0.18193018410729678, 0.1786447629669119, 0.18234086250867199, 0.18028747424085528, 0.180287475219987, 0.17412730957815534, 0.18357289596871917, 0.18028747383084384, 0.17987679719190577, 0.17946611920054198, 0.1786447647109903, 0.18357289459793474, 0.17782340771241353, 0.18275153932507768, 0.1831622173531589, 0.1794661193780096, 0.17782340890573037, 0.17618069902093014, 0.18028747343919116, 0.17987679740609086, 0.18069815105726098, 0.172895277898904, 0.18151950609757425, 0.1728952767055872, 0.1823408629186834, 0.18398357298095122, 0.18028747343919116, 0.183983573586789, 0.18110882926280983, 0.16221765806665167, 0.16919917931791692, 0.17412730955979663, 0.18069815125308733, 0.1798767977793848, 0.17946611918218328, 0.18275154110587352, 0.18357289616454553, 0.18028747343919116, 0.18357289616454553, 0.18562628284739274, 0.18439425159651152, 0.18234086272285704, 0.18151950647086823, 0.1811088286753308, 0.18028747343919116, 0.17987679660442674, 0.18275154053675322, 0.181519507076706, 0.18193018449894946, 0.18316221815482303, 0.18480492704213278, 0.18357289477540237, 0.18521560681429242, 0.1839835735684303, 0.18480492819873215, 0.18357289418792333, 0.17987679640860038, 0.18480492821709085, 0.18357289514869635, 0.1802874748099756, 0.18151950688087964, 0.18357289516705508, 0.18439425159651152, 0.18234086152954024, 0.18562628366741557, 0.1843942511865001, 0.18521560642263973, 0.18234086193955165, 0.18316221676567987, 0.17905544234741885, 0.17577002140286033, 0.18028747482833432, 0.18521560622681338, 0.1827515389517837, 0.18316221754898526, 0.180287475219987, 0.17535934182652702, 0.18439425059902104, 0.18028747361665878, 0.1876796709377418, 0.18603696009216858, 0.18521560622681338, 0.18357289575453412, 0.18562628405906825, 0.1839835720018195, 0.18644763888519647, 0.18480492743378546, 0.18603696046546256, 0.183162218528117, 0.182340862135378, 0.17700205345540565, 0.18439425058066233, 0.1774127315018456, 0.1806981514672724, 0.18357289577289285, 0.18726899470881514, 0.17946611861306294, 0.1786447649435341, 0.18110882906698347, 0.18480492821709085, 0.18439424942406296, 0.18357289516705508, 0.1827515409284059, 0.18521560505185528, 0.18521560622681338, 0.18316221792227924, 0.1786447629669119, 0.18316221656985351, 0.1819301850864285, 0.17946611820305153, 0.1774127310918342, 0.1815195062934006, 0.18028747461414923, 0.1835728945612173, 0.18726899413969483, 0.18110883004611522, 0.18521560622681338, 0.18193018530061358, 0.186036961462953, 0.18726899275055167, 0.17782340931574178, 0.18603696069800632, 0.1864476377102384, 0.1831622173531589, 0.183572894579576, 0.18439425018900962, 0.18521560622681338, 0.18644763966850186, 0.18193018489060217, 0.18357289595036047, 0.18439425081320612, 0.18767967035026276, 0.18193018510478723, 0.1848049284129172, 0.17987679779774354, 0.18603696007380985, 0.18562628443236223, 0.18603696187296442, 0.18521560583516067, 0.1856262822782724, 0.18891170361448362, 0.18809034894746432, 0.18850102738555696, 0.18809034935747573, 0.17618069706266665, 0.1839835735684303, 0.18685831552413454, 0.1843942511865001, 0.1876796723452437, 0.18316221655149478, 0.1860369602879949, 0.1819301843031231, 0.1831622173531589]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
