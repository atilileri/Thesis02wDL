{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf12.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 15:39:20 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '3', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000027240605E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002723CDB7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0926, Accuracy:0.3684, Validation Loss:1.0803, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0771, Accuracy:0.3725, Validation Loss:1.0757, Validation Accuracy:0.3695\n",
    "Epoch #3: Loss:1.0741, Accuracy:0.3828, Validation Loss:1.0750, Validation Accuracy:0.4039\n",
    "Epoch #4: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #8: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.3930, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #10: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0735, Validation Accuracy:0.4072\n",
    "Epoch #11: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #12: Loss:1.0738, Accuracy:0.3930, Validation Loss:1.0735, Validation Accuracy:0.4007\n",
    "Epoch #13: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #14: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #15: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #16: Loss:1.0732, Accuracy:0.3951, Validation Loss:1.0736, Validation Accuracy:0.3793\n",
    "Epoch #17: Loss:1.0734, Accuracy:0.3971, Validation Loss:1.0733, Validation Accuracy:0.3892\n",
    "Epoch #18: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #19: Loss:1.0733, Accuracy:0.3992, Validation Loss:1.0731, Validation Accuracy:0.4056\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0738, Accuracy:0.3963, Validation Loss:1.0734, Validation Accuracy:0.3957\n",
    "Epoch #22: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0733, Validation Accuracy:0.4253\n",
    "Epoch #23: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0730, Validation Accuracy:0.4220\n",
    "Epoch #24: Loss:1.0734, Accuracy:0.3988, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #25: Loss:1.0736, Accuracy:0.3988, Validation Loss:1.0729, Validation Accuracy:0.4319\n",
    "Epoch #26: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0729, Validation Accuracy:0.4039\n",
    "Epoch #27: Loss:1.0733, Accuracy:0.3988, Validation Loss:1.0730, Validation Accuracy:0.4154\n",
    "Epoch #28: Loss:1.0732, Accuracy:0.3963, Validation Loss:1.0727, Validation Accuracy:0.4220\n",
    "Epoch #29: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0728, Validation Accuracy:0.4154\n",
    "Epoch #30: Loss:1.0732, Accuracy:0.3975, Validation Loss:1.0727, Validation Accuracy:0.4154\n",
    "Epoch #31: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #32: Loss:1.0734, Accuracy:0.3996, Validation Loss:1.0727, Validation Accuracy:0.4007\n",
    "Epoch #33: Loss:1.0739, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #34: Loss:1.0749, Accuracy:0.3947, Validation Loss:1.0735, Validation Accuracy:0.4089\n",
    "Epoch #35: Loss:1.0743, Accuracy:0.3864, Validation Loss:1.0740, Validation Accuracy:0.4089\n",
    "Epoch #36: Loss:1.0745, Accuracy:0.3889, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #37: Loss:1.0739, Accuracy:0.3873, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #38: Loss:1.0743, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #39: Loss:1.0743, Accuracy:0.3963, Validation Loss:1.0738, Validation Accuracy:0.4007\n",
    "Epoch #40: Loss:1.0739, Accuracy:0.3897, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #41: Loss:1.0742, Accuracy:0.3889, Validation Loss:1.0739, Validation Accuracy:0.4039\n",
    "Epoch #42: Loss:1.0741, Accuracy:0.3934, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #43: Loss:1.0740, Accuracy:0.3926, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0741, Accuracy:0.3926, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #45: Loss:1.0738, Accuracy:0.3930, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #46: Loss:1.0738, Accuracy:0.3930, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #47: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #48: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #49: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #50: Loss:1.0737, Accuracy:0.3922, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #51: Loss:1.0739, Accuracy:0.3885, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3924\n",
    "Epoch #53: Loss:1.0737, Accuracy:0.3951, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #54: Loss:1.0740, Accuracy:0.3959, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #55: Loss:1.0741, Accuracy:0.3922, Validation Loss:1.0740, Validation Accuracy:0.4105\n",
    "Epoch #56: Loss:1.0738, Accuracy:0.3988, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #57: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #58: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3974\n",
    "Epoch #59: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #60: Loss:1.0736, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #61: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #62: Loss:1.0735, Accuracy:0.3930, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #64: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3924\n",
    "Epoch #65: Loss:1.0735, Accuracy:0.3971, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #66: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #67: Loss:1.0736, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3957\n",
    "Epoch #68: Loss:1.0735, Accuracy:0.3955, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #69: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #70: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #71: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #72: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #73: Loss:1.0731, Accuracy:0.3943, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #74: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #75: Loss:1.0736, Accuracy:0.3955, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #76: Loss:1.0735, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #77: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.4023\n",
    "Epoch #78: Loss:1.0731, Accuracy:0.3955, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #79: Loss:1.0731, Accuracy:0.3979, Validation Loss:1.0731, Validation Accuracy:0.4007\n",
    "Epoch #80: Loss:1.0732, Accuracy:0.3975, Validation Loss:1.0731, Validation Accuracy:0.4039\n",
    "Epoch #81: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0729, Validation Accuracy:0.4007\n",
    "Epoch #82: Loss:1.0730, Accuracy:0.3988, Validation Loss:1.0728, Validation Accuracy:0.4138\n",
    "Epoch #83: Loss:1.0727, Accuracy:0.4012, Validation Loss:1.0728, Validation Accuracy:0.4154\n",
    "Epoch #84: Loss:1.0727, Accuracy:0.3922, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #85: Loss:1.0727, Accuracy:0.3959, Validation Loss:1.0728, Validation Accuracy:0.3974\n",
    "Epoch #86: Loss:1.0730, Accuracy:0.3975, Validation Loss:1.0727, Validation Accuracy:0.4089\n",
    "Epoch #87: Loss:1.0731, Accuracy:0.3930, Validation Loss:1.0728, Validation Accuracy:0.4204\n",
    "Epoch #88: Loss:1.0731, Accuracy:0.3947, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #89: Loss:1.0728, Accuracy:0.3984, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #90: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0726, Validation Accuracy:0.4023\n",
    "Epoch #91: Loss:1.0725, Accuracy:0.4004, Validation Loss:1.0731, Validation Accuracy:0.4171\n",
    "Epoch #92: Loss:1.0725, Accuracy:0.3992, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #93: Loss:1.0723, Accuracy:0.4021, Validation Loss:1.0733, Validation Accuracy:0.4056\n",
    "Epoch #94: Loss:1.0722, Accuracy:0.3971, Validation Loss:1.0731, Validation Accuracy:0.4039\n",
    "Epoch #95: Loss:1.0730, Accuracy:0.3934, Validation Loss:1.0731, Validation Accuracy:0.4056\n",
    "Epoch #96: Loss:1.0726, Accuracy:0.4000, Validation Loss:1.0738, Validation Accuracy:0.4187\n",
    "Epoch #97: Loss:1.0721, Accuracy:0.3910, Validation Loss:1.0733, Validation Accuracy:0.3957\n",
    "Epoch #98: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0737, Validation Accuracy:0.4056\n",
    "Epoch #99: Loss:1.0717, Accuracy:0.3975, Validation Loss:1.0733, Validation Accuracy:0.4072\n",
    "Epoch #100: Loss:1.0722, Accuracy:0.3938, Validation Loss:1.0733, Validation Accuracy:0.4072\n",
    "Epoch #101: Loss:1.0724, Accuracy:0.3897, Validation Loss:1.0732, Validation Accuracy:0.4056\n",
    "Epoch #102: Loss:1.0720, Accuracy:0.4008, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #103: Loss:1.0722, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.4302\n",
    "Epoch #104: Loss:1.0716, Accuracy:0.4025, Validation Loss:1.0741, Validation Accuracy:0.4105\n",
    "Epoch #105: Loss:1.0716, Accuracy:0.4062, Validation Loss:1.0734, Validation Accuracy:0.4138\n",
    "Epoch #106: Loss:1.0717, Accuracy:0.4021, Validation Loss:1.0735, Validation Accuracy:0.4105\n",
    "Epoch #107: Loss:1.0731, Accuracy:0.4041, Validation Loss:1.0734, Validation Accuracy:0.4105\n",
    "Epoch #108: Loss:1.0716, Accuracy:0.3926, Validation Loss:1.0747, Validation Accuracy:0.4122\n",
    "Epoch #109: Loss:1.0729, Accuracy:0.3889, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #110: Loss:1.0724, Accuracy:0.3914, Validation Loss:1.0734, Validation Accuracy:0.4039\n",
    "Epoch #111: Loss:1.0721, Accuracy:0.3992, Validation Loss:1.0823, Validation Accuracy:0.3924\n",
    "Epoch #112: Loss:1.0781, Accuracy:0.3955, Validation Loss:1.0759, Validation Accuracy:0.3924\n",
    "Epoch #113: Loss:1.0738, Accuracy:0.3906, Validation Loss:1.0757, Validation Accuracy:0.4023\n",
    "Epoch #114: Loss:1.0743, Accuracy:0.3934, Validation Loss:1.0749, Validation Accuracy:0.4007\n",
    "Epoch #115: Loss:1.0729, Accuracy:0.3930, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #116: Loss:1.0722, Accuracy:0.3938, Validation Loss:1.0740, Validation Accuracy:0.4089\n",
    "Epoch #117: Loss:1.0722, Accuracy:0.3877, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #118: Loss:1.0728, Accuracy:0.4004, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #119: Loss:1.0723, Accuracy:0.3996, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #120: Loss:1.0730, Accuracy:0.3914, Validation Loss:1.0746, Validation Accuracy:0.4089\n",
    "Epoch #121: Loss:1.0729, Accuracy:0.3926, Validation Loss:1.0748, Validation Accuracy:0.3892\n",
    "Epoch #122: Loss:1.0723, Accuracy:0.3959, Validation Loss:1.0745, Validation Accuracy:0.3842\n",
    "Epoch #123: Loss:1.0720, Accuracy:0.3901, Validation Loss:1.0739, Validation Accuracy:0.4089\n",
    "Epoch #124: Loss:1.0723, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.4122\n",
    "Epoch #125: Loss:1.0720, Accuracy:0.3992, Validation Loss:1.0734, Validation Accuracy:0.4089\n",
    "Epoch #126: Loss:1.0724, Accuracy:0.3996, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #127: Loss:1.0718, Accuracy:0.4016, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #128: Loss:1.0719, Accuracy:0.3988, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #129: Loss:1.0717, Accuracy:0.4000, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #130: Loss:1.0719, Accuracy:0.3992, Validation Loss:1.0746, Validation Accuracy:0.4039\n",
    "Epoch #131: Loss:1.0715, Accuracy:0.3967, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #132: Loss:1.0718, Accuracy:0.3947, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #133: Loss:1.0715, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.3990\n",
    "Epoch #134: Loss:1.0708, Accuracy:0.3979, Validation Loss:1.0755, Validation Accuracy:0.4039\n",
    "Epoch #135: Loss:1.0710, Accuracy:0.3955, Validation Loss:1.0751, Validation Accuracy:0.4072\n",
    "Epoch #136: Loss:1.0710, Accuracy:0.3963, Validation Loss:1.0748, Validation Accuracy:0.4089\n",
    "Epoch #137: Loss:1.0714, Accuracy:0.4008, Validation Loss:1.0747, Validation Accuracy:0.4023\n",
    "Epoch #138: Loss:1.0715, Accuracy:0.3910, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #139: Loss:1.0708, Accuracy:0.3885, Validation Loss:1.0752, Validation Accuracy:0.3990\n",
    "Epoch #140: Loss:1.0706, Accuracy:0.3984, Validation Loss:1.0754, Validation Accuracy:0.4056\n",
    "Epoch #141: Loss:1.0697, Accuracy:0.4029, Validation Loss:1.0762, Validation Accuracy:0.3957\n",
    "Epoch #142: Loss:1.0703, Accuracy:0.4053, Validation Loss:1.0762, Validation Accuracy:0.3990\n",
    "Epoch #143: Loss:1.0704, Accuracy:0.3975, Validation Loss:1.0762, Validation Accuracy:0.3990\n",
    "Epoch #144: Loss:1.0716, Accuracy:0.4021, Validation Loss:1.0761, Validation Accuracy:0.3957\n",
    "Epoch #145: Loss:1.0711, Accuracy:0.3918, Validation Loss:1.0774, Validation Accuracy:0.3974\n",
    "Epoch #146: Loss:1.0705, Accuracy:0.3943, Validation Loss:1.0777, Validation Accuracy:0.4039\n",
    "Epoch #147: Loss:1.0714, Accuracy:0.3992, Validation Loss:1.0777, Validation Accuracy:0.3842\n",
    "Epoch #148: Loss:1.0712, Accuracy:0.3922, Validation Loss:1.0775, Validation Accuracy:0.3842\n",
    "Epoch #149: Loss:1.0712, Accuracy:0.3971, Validation Loss:1.0773, Validation Accuracy:0.4007\n",
    "Epoch #150: Loss:1.0706, Accuracy:0.4004, Validation Loss:1.0772, Validation Accuracy:0.4039\n",
    "Epoch #151: Loss:1.0707, Accuracy:0.3959, Validation Loss:1.0768, Validation Accuracy:0.4056\n",
    "Epoch #152: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.4105\n",
    "Epoch #153: Loss:1.0709, Accuracy:0.3930, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #154: Loss:1.0714, Accuracy:0.3906, Validation Loss:1.0748, Validation Accuracy:0.4105\n",
    "Epoch #155: Loss:1.0717, Accuracy:0.3906, Validation Loss:1.0756, Validation Accuracy:0.4072\n",
    "Epoch #156: Loss:1.0715, Accuracy:0.3910, Validation Loss:1.0757, Validation Accuracy:0.3974\n",
    "Epoch #157: Loss:1.0710, Accuracy:0.3807, Validation Loss:1.0751, Validation Accuracy:0.4007\n",
    "Epoch #158: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0750, Validation Accuracy:0.4105\n",
    "Epoch #159: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0749, Validation Accuracy:0.4187\n",
    "Epoch #160: Loss:1.0703, Accuracy:0.3971, Validation Loss:1.0749, Validation Accuracy:0.4056\n",
    "Epoch #161: Loss:1.0705, Accuracy:0.3918, Validation Loss:1.0741, Validation Accuracy:0.4138\n",
    "Epoch #162: Loss:1.0700, Accuracy:0.3959, Validation Loss:1.0741, Validation Accuracy:0.4072\n",
    "Epoch #163: Loss:1.0702, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.4089\n",
    "Epoch #164: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.4089\n",
    "Epoch #165: Loss:1.0712, Accuracy:0.3959, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #166: Loss:1.0706, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.4089\n",
    "Epoch #167: Loss:1.0705, Accuracy:0.4029, Validation Loss:1.0755, Validation Accuracy:0.4105\n",
    "Epoch #168: Loss:1.0703, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.4072\n",
    "Epoch #169: Loss:1.0701, Accuracy:0.3955, Validation Loss:1.0754, Validation Accuracy:0.4138\n",
    "Epoch #170: Loss:1.0707, Accuracy:0.3967, Validation Loss:1.0749, Validation Accuracy:0.4138\n",
    "Epoch #171: Loss:1.0702, Accuracy:0.3934, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #172: Loss:1.0702, Accuracy:0.3819, Validation Loss:1.0753, Validation Accuracy:0.4056\n",
    "Epoch #173: Loss:1.0707, Accuracy:0.3971, Validation Loss:1.0752, Validation Accuracy:0.3990\n",
    "Epoch #174: Loss:1.0697, Accuracy:0.4045, Validation Loss:1.0762, Validation Accuracy:0.4089\n",
    "Epoch #175: Loss:1.0701, Accuracy:0.3840, Validation Loss:1.0748, Validation Accuracy:0.4089\n",
    "Epoch #176: Loss:1.0701, Accuracy:0.4016, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #177: Loss:1.0714, Accuracy:0.3906, Validation Loss:1.0746, Validation Accuracy:0.4138\n",
    "Epoch #178: Loss:1.0721, Accuracy:0.3947, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #179: Loss:1.0695, Accuracy:0.3955, Validation Loss:1.0758, Validation Accuracy:0.3924\n",
    "Epoch #180: Loss:1.0699, Accuracy:0.3889, Validation Loss:1.0749, Validation Accuracy:0.4105\n",
    "Epoch #181: Loss:1.0697, Accuracy:0.3988, Validation Loss:1.0769, Validation Accuracy:0.3957\n",
    "Epoch #182: Loss:1.0686, Accuracy:0.4021, Validation Loss:1.0752, Validation Accuracy:0.4138\n",
    "Epoch #183: Loss:1.0696, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.4105\n",
    "Epoch #184: Loss:1.0691, Accuracy:0.4025, Validation Loss:1.0753, Validation Accuracy:0.4056\n",
    "Epoch #185: Loss:1.0681, Accuracy:0.3967, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #186: Loss:1.0688, Accuracy:0.3893, Validation Loss:1.0762, Validation Accuracy:0.4039\n",
    "Epoch #187: Loss:1.0679, Accuracy:0.3971, Validation Loss:1.0770, Validation Accuracy:0.4039\n",
    "Epoch #188: Loss:1.0675, Accuracy:0.3992, Validation Loss:1.0775, Validation Accuracy:0.3974\n",
    "Epoch #189: Loss:1.0676, Accuracy:0.4012, Validation Loss:1.0775, Validation Accuracy:0.3892\n",
    "Epoch #190: Loss:1.0686, Accuracy:0.4041, Validation Loss:1.0778, Validation Accuracy:0.4056\n",
    "Epoch #191: Loss:1.0671, Accuracy:0.4004, Validation Loss:1.0788, Validation Accuracy:0.3842\n",
    "Epoch #192: Loss:1.0669, Accuracy:0.4136, Validation Loss:1.0793, Validation Accuracy:0.4072\n",
    "Epoch #193: Loss:1.0663, Accuracy:0.4016, Validation Loss:1.0767, Validation Accuracy:0.3941\n",
    "Epoch #194: Loss:1.0668, Accuracy:0.3988, Validation Loss:1.0779, Validation Accuracy:0.3924\n",
    "Epoch #195: Loss:1.0663, Accuracy:0.4086, Validation Loss:1.0788, Validation Accuracy:0.3924\n",
    "Epoch #196: Loss:1.0676, Accuracy:0.4016, Validation Loss:1.0784, Validation Accuracy:0.4007\n",
    "Epoch #197: Loss:1.0700, Accuracy:0.4037, Validation Loss:1.0782, Validation Accuracy:0.3892\n",
    "Epoch #198: Loss:1.0664, Accuracy:0.3934, Validation Loss:1.0807, Validation Accuracy:0.3810\n",
    "Epoch #199: Loss:1.0821, Accuracy:0.3963, Validation Loss:1.0912, Validation Accuracy:0.3580\n",
    "Epoch #200: Loss:1.0804, Accuracy:0.3721, Validation Loss:1.0797, Validation Accuracy:0.3415\n",
    "Epoch #201: Loss:1.0765, Accuracy:0.3778, Validation Loss:1.0761, Validation Accuracy:0.4007\n",
    "Epoch #202: Loss:1.0727, Accuracy:0.3967, Validation Loss:1.0761, Validation Accuracy:0.4007\n",
    "Epoch #203: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #204: Loss:1.0729, Accuracy:0.3975, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #205: Loss:1.0724, Accuracy:0.3971, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #206: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.4023\n",
    "Epoch #207: Loss:1.0719, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #208: Loss:1.0719, Accuracy:0.3967, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #209: Loss:1.0720, Accuracy:0.4021, Validation Loss:1.0747, Validation Accuracy:0.3875\n",
    "Epoch #210: Loss:1.0715, Accuracy:0.4029, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #211: Loss:1.0724, Accuracy:0.3971, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #212: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #213: Loss:1.0738, Accuracy:0.3971, Validation Loss:1.0737, Validation Accuracy:0.4039\n",
    "Epoch #214: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0730, Validation Accuracy:0.4056\n",
    "Epoch #215: Loss:1.0728, Accuracy:0.3947, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #216: Loss:1.0736, Accuracy:0.3959, Validation Loss:1.0794, Validation Accuracy:0.3711\n",
    "Epoch #217: Loss:1.0816, Accuracy:0.3943, Validation Loss:1.0800, Validation Accuracy:0.3924\n",
    "Epoch #218: Loss:1.0799, Accuracy:0.3947, Validation Loss:1.0803, Validation Accuracy:0.3941\n",
    "Epoch #219: Loss:1.0764, Accuracy:0.3996, Validation Loss:1.0751, Validation Accuracy:0.3662\n",
    "Epoch #220: Loss:1.0751, Accuracy:0.3782, Validation Loss:1.0760, Validation Accuracy:0.3695\n",
    "Epoch #221: Loss:1.0738, Accuracy:0.3881, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #222: Loss:1.0729, Accuracy:0.3984, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #223: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #224: Loss:1.0729, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #225: Loss:1.0727, Accuracy:0.3963, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #226: Loss:1.0728, Accuracy:0.3967, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #227: Loss:1.0726, Accuracy:0.3975, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #228: Loss:1.0725, Accuracy:0.3967, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #229: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #230: Loss:1.0727, Accuracy:0.3963, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #231: Loss:1.0728, Accuracy:0.3984, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #232: Loss:1.0726, Accuracy:0.3984, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #233: Loss:1.0724, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #234: Loss:1.0725, Accuracy:0.3984, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #235: Loss:1.0721, Accuracy:0.3988, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #236: Loss:1.0722, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #237: Loss:1.0726, Accuracy:0.4000, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #238: Loss:1.0719, Accuracy:0.3992, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #239: Loss:1.0722, Accuracy:0.3988, Validation Loss:1.0751, Validation Accuracy:0.3924\n",
    "Epoch #240: Loss:1.0723, Accuracy:0.3988, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #241: Loss:1.0724, Accuracy:0.4000, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #242: Loss:1.0717, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.3924\n",
    "Epoch #243: Loss:1.0717, Accuracy:0.3992, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #244: Loss:1.0715, Accuracy:0.3996, Validation Loss:1.0752, Validation Accuracy:0.3842\n",
    "Epoch #245: Loss:1.0712, Accuracy:0.3996, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #246: Loss:1.0707, Accuracy:0.4016, Validation Loss:1.0751, Validation Accuracy:0.3842\n",
    "Epoch #247: Loss:1.0709, Accuracy:0.3943, Validation Loss:1.0751, Validation Accuracy:0.3810\n",
    "Epoch #248: Loss:1.0708, Accuracy:0.4016, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #249: Loss:1.0704, Accuracy:0.3992, Validation Loss:1.0752, Validation Accuracy:0.3957\n",
    "Epoch #250: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #251: Loss:1.0700, Accuracy:0.4025, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #252: Loss:1.0708, Accuracy:0.3906, Validation Loss:1.0757, Validation Accuracy:0.3924\n",
    "Epoch #253: Loss:1.0693, Accuracy:0.3971, Validation Loss:1.0757, Validation Accuracy:0.3957\n",
    "Epoch #254: Loss:1.0698, Accuracy:0.3959, Validation Loss:1.0753, Validation Accuracy:0.4007\n",
    "Epoch #255: Loss:1.0691, Accuracy:0.3979, Validation Loss:1.0756, Validation Accuracy:0.4023\n",
    "Epoch #256: Loss:1.0701, Accuracy:0.3852, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #257: Loss:1.0701, Accuracy:0.4004, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #258: Loss:1.0692, Accuracy:0.4016, Validation Loss:1.0747, Validation Accuracy:0.4105\n",
    "Epoch #259: Loss:1.0693, Accuracy:0.3975, Validation Loss:1.0753, Validation Accuracy:0.3990\n",
    "Epoch #260: Loss:1.0695, Accuracy:0.3967, Validation Loss:1.0744, Validation Accuracy:0.4007\n",
    "Epoch #261: Loss:1.0702, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.4072\n",
    "Epoch #262: Loss:1.0713, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.4056\n",
    "Epoch #263: Loss:1.0714, Accuracy:0.3893, Validation Loss:1.0757, Validation Accuracy:0.4023\n",
    "Epoch #264: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.4154\n",
    "Epoch #265: Loss:1.0712, Accuracy:0.3885, Validation Loss:1.0748, Validation Accuracy:0.4039\n",
    "Epoch #266: Loss:1.0698, Accuracy:0.4000, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #267: Loss:1.0708, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #268: Loss:1.0716, Accuracy:0.3922, Validation Loss:1.0757, Validation Accuracy:0.3957\n",
    "Epoch #269: Loss:1.0700, Accuracy:0.3967, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #270: Loss:1.0706, Accuracy:0.4066, Validation Loss:1.0757, Validation Accuracy:0.3957\n",
    "Epoch #271: Loss:1.0698, Accuracy:0.3988, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #272: Loss:1.0693, Accuracy:0.4021, Validation Loss:1.0755, Validation Accuracy:0.3924\n",
    "Epoch #273: Loss:1.0690, Accuracy:0.3975, Validation Loss:1.0756, Validation Accuracy:0.3908\n",
    "Epoch #274: Loss:1.0691, Accuracy:0.4016, Validation Loss:1.0757, Validation Accuracy:0.3908\n",
    "Epoch #275: Loss:1.0701, Accuracy:0.3947, Validation Loss:1.0757, Validation Accuracy:0.3892\n",
    "Epoch #276: Loss:1.0688, Accuracy:0.4000, Validation Loss:1.0756, Validation Accuracy:0.4056\n",
    "Epoch #277: Loss:1.0687, Accuracy:0.3967, Validation Loss:1.0760, Validation Accuracy:0.3908\n",
    "Epoch #278: Loss:1.0685, Accuracy:0.4004, Validation Loss:1.0762, Validation Accuracy:0.3941\n",
    "Epoch #279: Loss:1.0697, Accuracy:0.4029, Validation Loss:1.0763, Validation Accuracy:0.3908\n",
    "Epoch #280: Loss:1.0696, Accuracy:0.3959, Validation Loss:1.0757, Validation Accuracy:0.4089\n",
    "Epoch #281: Loss:1.0688, Accuracy:0.4053, Validation Loss:1.0771, Validation Accuracy:0.3941\n",
    "Epoch #282: Loss:1.0700, Accuracy:0.3959, Validation Loss:1.0763, Validation Accuracy:0.3957\n",
    "Epoch #283: Loss:1.0690, Accuracy:0.4033, Validation Loss:1.0772, Validation Accuracy:0.3974\n",
    "Epoch #284: Loss:1.0688, Accuracy:0.3996, Validation Loss:1.0768, Validation Accuracy:0.4171\n",
    "Epoch #285: Loss:1.0680, Accuracy:0.3975, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #286: Loss:1.0678, Accuracy:0.4045, Validation Loss:1.0779, Validation Accuracy:0.3875\n",
    "Epoch #287: Loss:1.0677, Accuracy:0.4021, Validation Loss:1.0781, Validation Accuracy:0.3957\n",
    "Epoch #288: Loss:1.0675, Accuracy:0.4033, Validation Loss:1.0794, Validation Accuracy:0.3924\n",
    "Epoch #289: Loss:1.0677, Accuracy:0.3996, Validation Loss:1.0792, Validation Accuracy:0.3990\n",
    "Epoch #290: Loss:1.0674, Accuracy:0.3971, Validation Loss:1.0794, Validation Accuracy:0.3941\n",
    "Epoch #291: Loss:1.0690, Accuracy:0.4021, Validation Loss:1.0775, Validation Accuracy:0.4105\n",
    "Epoch #292: Loss:1.0691, Accuracy:0.3947, Validation Loss:1.0770, Validation Accuracy:0.3957\n",
    "Epoch #293: Loss:1.0678, Accuracy:0.4004, Validation Loss:1.0777, Validation Accuracy:0.3957\n",
    "Epoch #294: Loss:1.0682, Accuracy:0.3959, Validation Loss:1.0770, Validation Accuracy:0.3941\n",
    "Epoch #295: Loss:1.0684, Accuracy:0.4000, Validation Loss:1.0789, Validation Accuracy:0.4023\n",
    "Epoch #296: Loss:1.0671, Accuracy:0.3988, Validation Loss:1.0796, Validation Accuracy:0.3924\n",
    "Epoch #297: Loss:1.0690, Accuracy:0.3926, Validation Loss:1.0790, Validation Accuracy:0.3842\n",
    "Epoch #298: Loss:1.0684, Accuracy:0.3963, Validation Loss:1.0783, Validation Accuracy:0.3974\n",
    "Epoch #299: Loss:1.0677, Accuracy:0.3992, Validation Loss:1.0792, Validation Accuracy:0.3974\n",
    "Epoch #300: Loss:1.0690, Accuracy:0.3955, Validation Loss:1.0784, Validation Accuracy:0.3892\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07835805, Accuracy:0.3892\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02  03\n",
    "t:01  201  39   0\n",
    "t:02  191  36   0\n",
    "t:03  128  14   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.39      0.84      0.53       240\n",
    "          02       0.40      0.16      0.23       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.39       609\n",
    "   macro avg       0.26      0.33      0.25       609\n",
    "weighted avg       0.30      0.39      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 15:54:54 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 33 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0803028274639486, 1.0756577315980382, 1.0750205434601883, 1.074370758482584, 1.0741201983688304, 1.073841076766329, 1.0734115349639617, 1.0733970875419028, 1.0733153303268508, 1.0734948986661061, 1.0731722662601564, 1.0734906147657748, 1.0732991740425624, 1.0734448415305227, 1.0733757915559465, 1.0735707053997245, 1.0732775225819429, 1.0731580686099425, 1.0731093897216621, 1.073300248492136, 1.0733745462201498, 1.0732686500048207, 1.0730287085221515, 1.0730417052709018, 1.0729190751249567, 1.0729060204354022, 1.072950707672069, 1.0726790218713445, 1.0728438290077673, 1.0726811235957154, 1.0729915093514328, 1.0727105414730378, 1.073822002301271, 1.07348850207963, 1.0740182601368093, 1.0736451013922104, 1.0735687557699645, 1.0739175435553239, 1.0737628374976673, 1.073960313851806, 1.0738703251276502, 1.074059434124989, 1.0744315417137835, 1.0743341003536981, 1.074320683534118, 1.0745919590513107, 1.0745310380149553, 1.0745013543146194, 1.0741842329404232, 1.0742207139192153, 1.0741321809577629, 1.0741130702797024, 1.0738186106110246, 1.0737353937183498, 1.0739892353173743, 1.0737725155694144, 1.0739678678841427, 1.0738877821438417, 1.0738961504793716, 1.07380592607708, 1.073753584194653, 1.0738016823046705, 1.0737569944807657, 1.0737189641727016, 1.073493252051092, 1.0735822232877483, 1.0736835623413863, 1.073559615608115, 1.0735836011435598, 1.0736020856302948, 1.0734695376238017, 1.0736056742409767, 1.0729961426583026, 1.073472456587555, 1.0735904426606027, 1.073691151216504, 1.0733105823128486, 1.073165184562821, 1.073121531256314, 1.073099532346616, 1.072871695598358, 1.0727675397603578, 1.0728369657629229, 1.0728124427090724, 1.072841455000766, 1.072709804293753, 1.0727598902039928, 1.072847034152114, 1.0728421062475746, 1.0726119710502562, 1.0730627473743481, 1.0730439845368585, 1.073311643255951, 1.073098224567858, 1.0731175063278875, 1.0737683207334947, 1.0732741017255485, 1.073693417367481, 1.07328823458385, 1.0733470634873865, 1.0732057061297162, 1.0737020992880384, 1.0737615295231635, 1.0741052975991285, 1.0734231697123235, 1.0734664792889248, 1.0734206749300652, 1.074657467981473, 1.0743750942555945, 1.0734381384058735, 1.0823375487758218, 1.0759321983616144, 1.0756846912975968, 1.0749175448723027, 1.0755471552925548, 1.0739575090079472, 1.0734839192752181, 1.0735087572843178, 1.0731928225221306, 1.0746074061479867, 1.0747815583922788, 1.0745470359407623, 1.0739359127476884, 1.0735523865140717, 1.0733732987311477, 1.0738066307625356, 1.0747431279794728, 1.0745301195749117, 1.0745597212576905, 1.0745839612628831, 1.0750911815217368, 1.0756127258826946, 1.0752271437292615, 1.075457234883739, 1.0750856518941168, 1.074794576281593, 1.0747215861365909, 1.075706861680756, 1.075155584487226, 1.075412675077692, 1.0761599908712853, 1.0762304598083245, 1.0762455845114045, 1.0760569940451135, 1.0773724188358325, 1.0776937540333063, 1.0777124323085416, 1.0775034177087994, 1.0773143525585556, 1.0771635935224335, 1.0768425131861996, 1.0752153425968338, 1.0755226130556004, 1.074777393896983, 1.0756386994141076, 1.0757304706009738, 1.0751380053255555, 1.074968846560699, 1.0749289713469632, 1.074875011819924, 1.074110804911709, 1.0741315134640397, 1.0738591734803173, 1.0744581150108174, 1.074903822689025, 1.07527327615835, 1.07552769599094, 1.0753429861883028, 1.0754314107064935, 1.07492133236088, 1.0752323311929437, 1.075310059368904, 1.075172542546966, 1.0762438891556463, 1.0747523981166396, 1.0750594121481984, 1.0746059486235695, 1.0768725721315406, 1.075762211395602, 1.0749257466280207, 1.0769136224082734, 1.0751710807161379, 1.074640340601478, 1.0752516215657952, 1.0761338427344762, 1.076194136992268, 1.0769643069096582, 1.0775424690277902, 1.0775089275660774, 1.0777833099631449, 1.0788112393349458, 1.079309632233994, 1.0766704947686156, 1.0778525611645677, 1.078818615434205, 1.078442521283192, 1.0782077343788836, 1.0806516132918484, 1.0912040017900013, 1.0797127980512546, 1.0761032542963138, 1.0761019668751357, 1.0749026484090118, 1.074822377101541, 1.0748751858380823, 1.074902671898527, 1.074810091106371, 1.0753379946663266, 1.0747126372185443, 1.0743428809302193, 1.074697523281492, 1.0734368445250788, 1.0736816947291834, 1.0729649017988558, 1.0735318619629433, 1.0793874001463841, 1.079977473797665, 1.080256188444316, 1.0750916436779479, 1.0759564297539848, 1.074210265782862, 1.074675558040099, 1.0745327785880303, 1.0746689304733903, 1.074615483800766, 1.0744705374409216, 1.074306665970187, 1.0742840289286597, 1.0744822929645408, 1.074526329345891, 1.074821363528961, 1.0747536706611245, 1.0747538016151716, 1.074943282921326, 1.0747988611606543, 1.0746131608834604, 1.0749057556803787, 1.074953512800934, 1.075050701257239, 1.0748919970883524, 1.074853427304423, 1.075177930846003, 1.0751129114764861, 1.0751816971940165, 1.0752742676116367, 1.0751485897010966, 1.0751054776321687, 1.0749187473397341, 1.075153136488252, 1.0754970028286888, 1.0752416588794227, 1.0756625242421192, 1.0757256730632438, 1.0752962056443414, 1.075644572380141, 1.075642472221738, 1.0753409811624361, 1.0747293781960148, 1.075297757322565, 1.074425863906472, 1.0745145526817084, 1.0743858448194556, 1.0756796222602205, 1.0741573262880197, 1.0748062746473916, 1.074943512139845, 1.0745429491566123, 1.0756944196760556, 1.076129173018858, 1.0757147852814648, 1.0749332008299177, 1.0755154313320792, 1.0755723550401886, 1.0757440113277466, 1.0756801670212268, 1.0756048148097272, 1.0760221943283708, 1.0761749497775375, 1.0762640873982596, 1.0756713543423682, 1.077076348569397, 1.076270687364788, 1.0772278093547851, 1.076784698246735, 1.0764637375112824, 1.0778845947951519, 1.0781468791131708, 1.0793854481676726, 1.0791653012994475, 1.0794294342423112, 1.0774608740861389, 1.0770422594104885, 1.0777232034257285, 1.077002855944516, 1.0788930770015872, 1.0795899126525779, 1.0790428608313374, 1.0782837859906977, 1.0792384449092822, 1.0783580232332102], 'val_acc': [0.37274219890924903, 0.36945812665965955, 0.40394088655269794, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3957307055372323, 0.3940886695103105, 0.40065681420523547, 0.40722495870441444, 0.3924466333855158, 0.4006568140094895, 0.3973727417599, 0.39573070563510526, 0.3973727417599, 0.3793103438977929, 0.3891625612337993, 0.40394088635695197, 0.4055829223838737, 0.39408866941243753, 0.3957307052436133, 0.4252873557835377, 0.4220032835339482, 0.4072249583129225, 0.4318555006742086, 0.403940886063333, 0.41543513942626115, 0.4220032839254401, 0.41543513942626115, 0.41543513923051517, 0.39901477788469475, 0.4006568139116165, 0.39901477778682176, 0.4088669948292092, 0.4088669949270822, 0.397372741955646, 0.39573070573297825, 0.3973727417599, 0.4006568141073625, 0.4022988503300302, 0.40394088655269794, 0.3973727417599, 0.39408866941243753, 0.39408866941243753, 0.3924466332876428, 0.39408866941243753, 0.3924466332876428, 0.3924466332876428, 0.39080459716284804, 0.397372741857773, 0.3940886696081835, 0.3924466332876428, 0.3924466332876428, 0.3924466332876428, 0.4105090310518769, 0.3973727417599, 0.3924466332876428, 0.3973727417599, 0.39901477798256774, 0.397372741857773, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3924466332876428, 0.3924466332876428, 0.39408866941243753, 0.3957307055372323, 0.397372741662027, 0.397372741662027, 0.397372741662027, 0.40229885013428424, 0.39737274156415403, 0.3957307055372323, 0.39408866941243753, 0.3973727417599, 0.39408866941243753, 0.4022988502321572, 0.40065681420523547, 0.40065681430310846, 0.40394088655269794, 0.40065681420523547, 0.41379310300784744, 0.41543513952413413, 0.4055829225796197, 0.397372741955646, 0.4088669949270822, 0.42036124789851836, 0.4055829225796197, 0.4055829225796197, 0.4022988502321572, 0.4170771752574369, 0.4072249584107955, 0.40558292228600074, 0.403940886161206, 0.40558292218812775, 0.41871921157797765, 0.39573070563510526, 0.40558292209025476, 0.4072249583129225, 0.4072249582150495, 0.40558292209025476, 0.397372741662027, 0.43021346445154085, 0.410509030562512, 0.41379310290997445, 0.41050903066038497, 0.410509030562512, 0.41215106717667166, 0.3924466332876428, 0.40394088596546, 0.3924466332876428, 0.3924466332876428, 0.40229884993853826, 0.4006568138137435, 0.3809523792396038, 0.4088669943398443, 0.3990147776889488, 0.39408866941243753, 0.39901477778682176, 0.4088669942419713, 0.3891625609401803, 0.3842364522721771, 0.4088669946334632, 0.4121510667851797, 0.4088669946334632, 0.39408866941243753, 0.39573070563510526, 0.40229885013428424, 0.3973727417599, 0.403940886259079, 0.3957307055372323, 0.38752052393452874, 0.39901477778682176, 0.403940886259079, 0.4072249583129225, 0.40886699443771723, 0.40229885003641125, 0.3908045962819912, 0.39901477778682176, 0.40558292228600074, 0.39573070426488355, 0.3990147772974569, 0.39901477778682176, 0.3957307052436133, 0.39737274146628104, 0.403940886259079, 0.3842364514891933, 0.3842364513913203, 0.4006568140094895, 0.403940886259079, 0.4055829225796197, 0.41050903095400393, 0.3891625598635775, 0.41050903046463905, 0.40722495870441444, 0.3973727404875513, 0.4006568139116165, 0.41050903095400393, 0.41871921108861276, 0.4055829225796197, 0.41379310271422853, 0.40722495870441444, 0.4088669948292092, 0.4088669948292092, 0.39080459598837225, 0.4088669948292092, 0.41050903046463905, 0.40722495860654145, 0.41379310310572043, 0.41379310300784744, 0.3891625594720856, 0.4055829223838737, 0.3990147776889488, 0.40886699414409833, 0.4088669947313362, 0.38423645119557437, 0.41379310251848256, 0.39408866941243753, 0.39244663201529406, 0.41050903095400393, 0.39573070563510526, 0.41379310261635555, 0.41050903095400393, 0.4055829225796197, 0.39901477680809194, 0.40394088596546, 0.403940886259079, 0.3973727412705351, 0.3891625599614505, 0.4055829223838737, 0.3842364514891933, 0.4072249583129225, 0.3940886682379618, 0.39244663201529406, 0.39244663230891297, 0.40065681371587053, 0.38916256025506946, 0.3809523801204606, 0.3579638736882233, 0.3415435123424029, 0.4006568140094895, 0.4006568140094895, 0.3924466332876428, 0.3924466332876428, 0.3924466332876428, 0.40229885013428424, 0.39408866941243753, 0.3924466332876428, 0.38752052491325856, 0.3858784887884638, 0.39901477798256774, 0.397372741955646, 0.40394088655269794, 0.4055829225796197, 0.3924466332876428, 0.3711001627844543, 0.3924466332876428, 0.39408866941243753, 0.3661740542143241, 0.36945812665965955, 0.39408866941243753, 0.3924466332876428, 0.3924466332876428, 0.3924466332876428, 0.3924466332876428, 0.3924466332876428, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.39408866941243753, 0.3924466332876428, 0.39408866941243753, 0.3924466332876428, 0.3924466332876428, 0.3924466332876428, 0.3940886695103105, 0.3924466332876428, 0.3924466332876428, 0.3924466332876428, 0.39408866941243753, 0.39408866941243753, 0.3924466332876428, 0.39408866941243753, 0.3842364524679231, 0.3924466332876428, 0.3842364523700501, 0.3809523803162066, 0.38259441624525536, 0.3957307055372323, 0.38587848849484485, 0.38587848859271784, 0.39244663289615084, 0.3957307055372323, 0.40065681361799754, 0.4022988498406653, 0.3924466332876428, 0.39573070563510526, 0.41050903095400393, 0.3990147776889488, 0.4006568138137435, 0.40722495860654145, 0.40558292218812775, 0.4022988498406653, 0.41543513932838816, 0.40394088596546, 0.3924466333855158, 0.4006568140094895, 0.39573070563510526, 0.39244663348338876, 0.39573070563510526, 0.39737274146628104, 0.3924466330918968, 0.3908045968692291, 0.39080459726072103, 0.38916256064656135, 0.40558292218812775, 0.3908045967713561, 0.39408866921669156, 0.39080459726072103, 0.4088669947313362, 0.3940886696081835, 0.3957307052436133, 0.3973727417599, 0.4170771755510559, 0.39408866911881857, 0.38752052501113154, 0.3957307052436133, 0.3924466333855158, 0.39901477739532987, 0.39408866911881857, 0.41050903095400393, 0.39573070563510526, 0.3957307055372323, 0.39408866911881857, 0.40229885013428424, 0.3924466327982779, 0.3842364523700501, 0.3973727417599, 0.3973727417599, 0.38916256054868836], 'loss': [1.09262148628979, 1.0770994584907987, 1.0741497632169625, 1.0740785367190226, 1.0746293971670726, 1.0745307373070374, 1.074101174881326, 1.07383738122192, 1.0737821660736993, 1.0735451237132172, 1.0734529609308106, 1.073796346642888, 1.073572018259115, 1.0734572515350593, 1.0734614617036353, 1.0731619611657865, 1.073365063442097, 1.0734882874165717, 1.073335072881632, 1.073350413526108, 1.0737957864816183, 1.0734633258725583, 1.0733368695394214, 1.0733598463833944, 1.0735872701452989, 1.073452384418041, 1.0733184700873843, 1.0732250635628828, 1.0733851517984754, 1.0731653414222984, 1.0729358464540641, 1.0734166363181519, 1.0739035553511163, 1.0748635098674704, 1.0743330053725038, 1.0745151040000838, 1.0739450435129279, 1.074349671516575, 1.0742818322759389, 1.0739259634174605, 1.0741859150128688, 1.0740693019890442, 1.0739862939905092, 1.0740896178466828, 1.073814106428158, 1.073820057248188, 1.073937992197777, 1.0738825969382724, 1.0737915078717335, 1.0737070034172012, 1.073902326247041, 1.0737113644699785, 1.0736721068192312, 1.0739783345796243, 1.0740772422089468, 1.0738456299172778, 1.0741040319387918, 1.0737265609373057, 1.0736676133878422, 1.0735688490054935, 1.0734304766390603, 1.073538561376458, 1.0735430566192408, 1.0736603132263591, 1.0734905318802632, 1.073538132761538, 1.073561299801852, 1.0735029154000095, 1.0733881390804627, 1.0734297695101165, 1.0732241137561367, 1.073510839415282, 1.0731144162908473, 1.0733861684799195, 1.073575041622107, 1.073490148941839, 1.0734349758473265, 1.0731438048811175, 1.073085592414809, 1.0731955821509234, 1.0728551521927914, 1.0730171812143658, 1.0727080341726847, 1.0726804803773853, 1.0726787710091905, 1.0730384349823, 1.0730878544539153, 1.0730826370035598, 1.0728469574475925, 1.0731828372826078, 1.0725005682978541, 1.0725430862125185, 1.0722965582195494, 1.072213480653704, 1.0730336508956533, 1.0726360896284821, 1.0720692022135616, 1.072125161257123, 1.0717264061835756, 1.0721585347177556, 1.0724130817996893, 1.0719557054233746, 1.072201513020165, 1.0716367423901567, 1.0715515206237103, 1.071655692897538, 1.0730698312332498, 1.071642724346576, 1.072851791567871, 1.0724384454731088, 1.072104695347545, 1.0780690417887004, 1.073803663498567, 1.0743446893026207, 1.072933952078927, 1.0722093326599935, 1.072186666053913, 1.0727811320851226, 1.0722536208938034, 1.0730434742796349, 1.0729461024184492, 1.072334490076962, 1.0720429730855954, 1.0723291871484055, 1.0719716929801926, 1.0724079266710693, 1.0717902576164544, 1.0719257714567243, 1.071681520826273, 1.0718621156543677, 1.071469619435696, 1.0718152900496059, 1.071487462153425, 1.0708044098143215, 1.0709733296713546, 1.070975405185864, 1.0713585945615043, 1.071498330124105, 1.0707970285807302, 1.0705935412608623, 1.069653752354381, 1.0702965835771032, 1.0703954428862743, 1.071601068361584, 1.0710778074342857, 1.0704549457013484, 1.0714380584947871, 1.0712254535001406, 1.0711821281934422, 1.0705616936301794, 1.0706568283711615, 1.0707789729508042, 1.0708820014518878, 1.0713710958707994, 1.0717360810822285, 1.0714646778067525, 1.0710016261380801, 1.0709764417436822, 1.0713152528543493, 1.0703490104518631, 1.0705163069084684, 1.0699620475024902, 1.0702342933697866, 1.0700635519360615, 1.0711678919116574, 1.070626895971122, 1.0704622009696412, 1.0702897572664265, 1.070062309075185, 1.070738803043013, 1.0702224564992917, 1.070204163087222, 1.0707122776297817, 1.069681322697007, 1.070085731130361, 1.0701226559018207, 1.071414893853346, 1.0720582272237822, 1.0695436304844381, 1.069945344347239, 1.069689902632633, 1.0686386263101253, 1.0696288476489653, 1.06912405035579, 1.0680737065093964, 1.0687589785891147, 1.0678852269292123, 1.0674511885985702, 1.0675815135791316, 1.0686457055794873, 1.067057224171852, 1.0668596855668806, 1.0663396210151532, 1.0667883975305106, 1.0662951485577061, 1.0676041094919004, 1.069973802125919, 1.066432643720012, 1.0821297877133504, 1.0804258936974058, 1.0764794875464156, 1.072671408183276, 1.073414661752125, 1.0728636591831027, 1.0724039229034643, 1.0723396535771583, 1.0719080921071267, 1.0719188077738642, 1.0719883187840362, 1.0714916117137463, 1.0724184669263555, 1.073428293077363, 1.0737921749052326, 1.0731268880303635, 1.0727874799430737, 1.073600098631465, 1.0816078197295171, 1.0799370252621003, 1.0763628931750506, 1.0750918204289932, 1.0738415026811603, 1.0729197598580706, 1.0733038071978998, 1.0728801724357526, 1.0727345755457633, 1.0727995149408767, 1.0725835659175929, 1.072542046227739, 1.0727526947213395, 1.0726706687919414, 1.0727962600621843, 1.0726014877002097, 1.0723935591856313, 1.0724586602353952, 1.0721471180416475, 1.0721623983226518, 1.0725728063857531, 1.0718928309191913, 1.072236269700209, 1.072280114485253, 1.0724095830682367, 1.0716888756233074, 1.071696324661772, 1.071491033634366, 1.071234637853791, 1.0707484334890847, 1.0709250816819114, 1.0707904633555325, 1.0703817124239474, 1.0701389483113064, 1.0699895350105708, 1.0707891305614057, 1.069271061846363, 1.0698204063047374, 1.0690638955858454, 1.0700774525225285, 1.070081711598735, 1.0692265846890836, 1.0692769397700348, 1.0695463279434299, 1.0701957721729787, 1.0713002354702177, 1.0714451702223666, 1.0722727495052486, 1.0711611704170336, 1.0698193670053502, 1.0707611763012237, 1.0715963289233448, 1.069951935176732, 1.070641613789897, 1.0697506964818653, 1.0692978950986138, 1.0690401584950315, 1.0691025845568773, 1.0700726532103835, 1.0687810919368046, 1.0687140765865726, 1.0685146594683983, 1.0696945993317715, 1.0696375451783136, 1.068754457742037, 1.0699666589682106, 1.0690258288040788, 1.068795729221994, 1.0679938853398976, 1.0678117291883276, 1.067691007482932, 1.067515452392782, 1.0676688141891355, 1.0673635559649928, 1.068970079539493, 1.0691378605194406, 1.067804945567795, 1.0682045644803213, 1.0684314205905985, 1.0671298679140315, 1.0689662367411463, 1.0683794345209485, 1.0676508864827714, 1.0689933468918535], 'acc': [0.36837782220918786, 0.3724846015231076, 0.3827515421706793, 0.39507186992946836, 0.39425051508498143, 0.3942505126983478, 0.3942505148891551, 0.3934291586738837, 0.3930184814474666, 0.3958932216040163, 0.3958932259489134, 0.3930184794524857, 0.39876796790461766, 0.3946611901573087, 0.39630390141289334, 0.3950718707127738, 0.39712525524153114, 0.39753593423038536, 0.3991786433318802, 0.39466119191974586, 0.39630390180454605, 0.39630390297950413, 0.3983572886832196, 0.39876796590963676, 0.39876796829627037, 0.39466119231139857, 0.3987679692754021, 0.3963038992220861, 0.3963038996137388, 0.3975359334837974, 0.39958932294493094, 0.3995893245115417, 0.4016427104110835, 0.39466118992476495, 0.38644763916669683, 0.38891170448346307, 0.38726899377864, 0.3946611889456332, 0.3963038992220861, 0.38973305952377635, 0.3889117036634402, 0.3934291590655364, 0.3926078034377441, 0.3926078030460914, 0.39301848085998753, 0.39301848085998753, 0.39425051250252147, 0.3942505117559335, 0.39466118992476495, 0.3921971266029797, 0.38850102866454783, 0.3963039016087197, 0.3950718707127738, 0.39589322575308705, 0.3921971264071533, 0.3987679691162932, 0.39425051312671794, 0.39425051171921605, 0.39466119035313507, 0.39917864450683826, 0.39671457684015593, 0.39301848023579106, 0.3942505129308916, 0.3946611903164176, 0.3971252574323384, 0.39466119191974586, 0.3946611927030512, 0.39548254496507823, 0.396714580597574, 0.39630390282039524, 0.3983572878999142, 0.39466118953311224, 0.3942505119150424, 0.39425051312671794, 0.39548254594420995, 0.3946611917239195, 0.3942505129308916, 0.3954825465684064, 0.3979466124726517, 0.3975359334837974, 0.39876796907957573, 0.39876796829627037, 0.40123203361303655, 0.39219712699463233, 0.39589322496978163, 0.3975359362253663, 0.393018479060833, 0.39466118914145953, 0.3983572902865478, 0.39712525622066286, 0.40041067778941786, 0.39917864689347193, 0.4020533878700444, 0.3971252570406857, 0.3934291590655364, 0.4000000019337852, 0.3909650933571175, 0.40041067696939503, 0.39753593642119267, 0.3938398362919535, 0.38973305815299186, 0.40082135403670327, 0.3942505153175252, 0.40246406368895965, 0.40616016264324073, 0.4020533872825654, 0.4041067775270043, 0.3926078050043549, 0.3889117058542475, 0.39137576898020643, 0.3991786466976456, 0.39548254833084356, 0.39055441593487406, 0.3934291586738837, 0.3930184800399647, 0.39383983410114626, 0.3876796725716679, 0.40041067720193885, 0.3995893225165608, 0.39137577234597176, 0.3926078048085285, 0.39589322356227974, 0.39014373514686523, 0.3963039007886969, 0.39917864689347193, 0.39958932235745187, 0.40164270923612544, 0.398767964930505, 0.3999999999755217, 0.3991786429402275, 0.3967145804017476, 0.3946611907447878, 0.39999999778471446, 0.39794661364760975, 0.3954825465684064, 0.39630389941791244, 0.40082135364505056, 0.3909650931612912, 0.3885010266695669, 0.3983572906782005, 0.40287474306946663, 0.4053388097937347, 0.39753593207629556, 0.40205339002413426, 0.3917864472224727, 0.3942505133225443, 0.39917864391935926, 0.3921971256238479, 0.39712525524153114, 0.4004106779852442, 0.39589322379482356, 0.4004106791602023, 0.39301848168001036, 0.3905544133524141, 0.3905544169140058, 0.3909650937487702, 0.3806981534928512, 0.4016427127977171, 0.403285419316752, 0.39712525661231557, 0.39178644780995175, 0.3958932245781289, 0.40205339002413426, 0.40287474506444754, 0.39589322536143434, 0.4020533893999378, 0.40287474111120314, 0.39425051367747954, 0.3954825455892747, 0.3967145778560051, 0.39342915926136274, 0.38193018458462347, 0.3971252556331838, 0.4045174545575951, 0.3839835707167091, 0.4016427113902153, 0.39055441515156863, 0.3946611925072249, 0.3954825461767537, 0.38891170409181036, 0.39876796767207384, 0.40205338607088986, 0.39342915808640466, 0.4024640654513968, 0.39671457821094036, 0.3893223810856837, 0.3971252588031228, 0.3991786429402275, 0.40123203420051556, 0.404106778506136, 0.4004106765777423, 0.41355236074762913, 0.40164270845282, 0.39876796512633134, 0.408624229134965, 0.40164270845282, 0.4036960967389955, 0.3934291570705555, 0.39630390023793527, 0.372073922925906, 0.3778234078164463, 0.3967145778560051, 0.4004106779852442, 0.39753593661701897, 0.39712525860729647, 0.3991786427444012, 0.39876796829627037, 0.39671457824765777, 0.40205338920411143, 0.4028747448686212, 0.39712525801981746, 0.3983572916573323, 0.3971252560248365, 0.3987679663012894, 0.3946611917239195, 0.3958932247739553, 0.39425051508498143, 0.3946611889456332, 0.39958932098666744, 0.3782340866094742, 0.38809034842730056, 0.3983572896990688, 0.3967145770359823, 0.3967145772318086, 0.3963038992220861, 0.39671458075668287, 0.39753593465875553, 0.3967145790309632, 0.39507186754283474, 0.3963039025878515, 0.3983572893074161, 0.39835728966235134, 0.3987679666929421, 0.39835729149822335, 0.39876796947122845, 0.39958932333658365, 0.399999999583869, 0.3991786461101665, 0.39876796947122845, 0.39876796907957573, 0.39999999935132524, 0.40000000095465343, 0.39917864391935926, 0.39958932372823636, 0.3995893211824938, 0.4016427100194308, 0.3942505148891551, 0.4016427108394537, 0.39917864431101197, 0.40082135736575114, 0.40246406525557044, 0.3905544133524141, 0.3971252588031228, 0.3958932216040163, 0.3979466110651498, 0.3852156049049855, 0.40041067696939503, 0.40164270962777815, 0.3975359350136908, 0.3967145809892267, 0.3991786441151856, 0.39425051453421983, 0.3893223834723173, 0.3991786447393821, 0.3885010274528723, 0.39999999778471446, 0.39548254735171184, 0.3921971275821114, 0.3967145786393105, 0.40657083967383145, 0.39876796590963676, 0.40205339041578697, 0.39753593266377457, 0.40164271201441176, 0.39466119231139857, 0.39999999778471446, 0.39671457883513683, 0.40041067818107057, 0.4028747421270523, 0.3958932255939781, 0.40533880916953824, 0.395893225165608, 0.4032854211159066, 0.3995893207541237, 0.3975359324679482, 0.40451745416594237, 0.4020533878700444, 0.40328541951257835, 0.39958932059501473, 0.397125256649033, 0.40205338963248155, 0.3946611887498068, 0.4004106783768969, 0.3958932231706271, 0.4000000005630007, 0.39876796947122845, 0.39260780183441585, 0.39630389941791244, 0.39917864708929823, 0.3954825465684064]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
