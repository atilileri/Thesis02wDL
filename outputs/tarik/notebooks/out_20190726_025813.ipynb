{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf15.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 02:58:13 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '0', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '01', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000014E23935E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000014E1F117EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0885, Accuracy:0.3729, Validation Loss:1.0811, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0784, Accuracy:0.3856, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0748, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0749, Accuracy:0.3943, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0747, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0734, Accuracy:0.3955, Validation Loss:1.0730, Validation Accuracy:0.3957\n",
    "Epoch #20: Loss:1.0732, Accuracy:0.3943, Validation Loss:1.0727, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0731, Accuracy:0.3947, Validation Loss:1.0724, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0728, Accuracy:0.3955, Validation Loss:1.0720, Validation Accuracy:0.4039\n",
    "Epoch #23: Loss:1.0727, Accuracy:0.4025, Validation Loss:1.0715, Validation Accuracy:0.4072\n",
    "Epoch #24: Loss:1.0721, Accuracy:0.4078, Validation Loss:1.0709, Validation Accuracy:0.4039\n",
    "Epoch #25: Loss:1.0719, Accuracy:0.4057, Validation Loss:1.0701, Validation Accuracy:0.4072\n",
    "Epoch #26: Loss:1.0714, Accuracy:0.4053, Validation Loss:1.0692, Validation Accuracy:0.4056\n",
    "Epoch #27: Loss:1.0704, Accuracy:0.4086, Validation Loss:1.0681, Validation Accuracy:0.4072\n",
    "Epoch #28: Loss:1.0703, Accuracy:0.4045, Validation Loss:1.0670, Validation Accuracy:0.4105\n",
    "Epoch #29: Loss:1.0691, Accuracy:0.4066, Validation Loss:1.0657, Validation Accuracy:0.4056\n",
    "Epoch #30: Loss:1.0685, Accuracy:0.4082, Validation Loss:1.0642, Validation Accuracy:0.4089\n",
    "Epoch #31: Loss:1.0678, Accuracy:0.4078, Validation Loss:1.0630, Validation Accuracy:0.4089\n",
    "Epoch #32: Loss:1.0687, Accuracy:0.4070, Validation Loss:1.0620, Validation Accuracy:0.4056\n",
    "Epoch #33: Loss:1.0679, Accuracy:0.3988, Validation Loss:1.0623, Validation Accuracy:0.4204\n",
    "Epoch #34: Loss:1.0676, Accuracy:0.4045, Validation Loss:1.0619, Validation Accuracy:0.4105\n",
    "Epoch #35: Loss:1.0685, Accuracy:0.4074, Validation Loss:1.0620, Validation Accuracy:0.4138\n",
    "Epoch #36: Loss:1.0675, Accuracy:0.4090, Validation Loss:1.0617, Validation Accuracy:0.4253\n",
    "Epoch #37: Loss:1.0672, Accuracy:0.4045, Validation Loss:1.0610, Validation Accuracy:0.4056\n",
    "Epoch #38: Loss:1.0671, Accuracy:0.4057, Validation Loss:1.0612, Validation Accuracy:0.4089\n",
    "Epoch #39: Loss:1.0670, Accuracy:0.4099, Validation Loss:1.0606, Validation Accuracy:0.4105\n",
    "Epoch #40: Loss:1.0667, Accuracy:0.4111, Validation Loss:1.0601, Validation Accuracy:0.4105\n",
    "Epoch #41: Loss:1.0668, Accuracy:0.4053, Validation Loss:1.0599, Validation Accuracy:0.4072\n",
    "Epoch #42: Loss:1.0666, Accuracy:0.4111, Validation Loss:1.0601, Validation Accuracy:0.4056\n",
    "Epoch #43: Loss:1.0665, Accuracy:0.4111, Validation Loss:1.0597, Validation Accuracy:0.4089\n",
    "Epoch #44: Loss:1.0672, Accuracy:0.4090, Validation Loss:1.0597, Validation Accuracy:0.4138\n",
    "Epoch #45: Loss:1.0664, Accuracy:0.4103, Validation Loss:1.0598, Validation Accuracy:0.4105\n",
    "Epoch #46: Loss:1.0666, Accuracy:0.4123, Validation Loss:1.0596, Validation Accuracy:0.4105\n",
    "Epoch #47: Loss:1.0657, Accuracy:0.4119, Validation Loss:1.0591, Validation Accuracy:0.4204\n",
    "Epoch #48: Loss:1.0667, Accuracy:0.4045, Validation Loss:1.0589, Validation Accuracy:0.4187\n",
    "Epoch #49: Loss:1.0661, Accuracy:0.4131, Validation Loss:1.0594, Validation Accuracy:0.4072\n",
    "Epoch #50: Loss:1.0662, Accuracy:0.4111, Validation Loss:1.0586, Validation Accuracy:0.4105\n",
    "Epoch #51: Loss:1.0658, Accuracy:0.4115, Validation Loss:1.0587, Validation Accuracy:0.4220\n",
    "Epoch #52: Loss:1.0657, Accuracy:0.4078, Validation Loss:1.0587, Validation Accuracy:0.4089\n",
    "Epoch #53: Loss:1.0656, Accuracy:0.4103, Validation Loss:1.0585, Validation Accuracy:0.4056\n",
    "Epoch #54: Loss:1.0655, Accuracy:0.4111, Validation Loss:1.0580, Validation Accuracy:0.4171\n",
    "Epoch #55: Loss:1.0653, Accuracy:0.4152, Validation Loss:1.0578, Validation Accuracy:0.4105\n",
    "Epoch #56: Loss:1.0650, Accuracy:0.4136, Validation Loss:1.0576, Validation Accuracy:0.4154\n",
    "Epoch #57: Loss:1.0648, Accuracy:0.4140, Validation Loss:1.0575, Validation Accuracy:0.4089\n",
    "Epoch #58: Loss:1.0649, Accuracy:0.4119, Validation Loss:1.0574, Validation Accuracy:0.4122\n",
    "Epoch #59: Loss:1.0644, Accuracy:0.4144, Validation Loss:1.0570, Validation Accuracy:0.4204\n",
    "Epoch #60: Loss:1.0652, Accuracy:0.4094, Validation Loss:1.0567, Validation Accuracy:0.4204\n",
    "Epoch #61: Loss:1.0654, Accuracy:0.4140, Validation Loss:1.0585, Validation Accuracy:0.4138\n",
    "Epoch #62: Loss:1.0647, Accuracy:0.4168, Validation Loss:1.0568, Validation Accuracy:0.4253\n",
    "Epoch #63: Loss:1.0642, Accuracy:0.4148, Validation Loss:1.0565, Validation Accuracy:0.4236\n",
    "Epoch #64: Loss:1.0642, Accuracy:0.4140, Validation Loss:1.0566, Validation Accuracy:0.4154\n",
    "Epoch #65: Loss:1.0635, Accuracy:0.4127, Validation Loss:1.0562, Validation Accuracy:0.4204\n",
    "Epoch #66: Loss:1.0631, Accuracy:0.4148, Validation Loss:1.0559, Validation Accuracy:0.4204\n",
    "Epoch #67: Loss:1.0640, Accuracy:0.4201, Validation Loss:1.0560, Validation Accuracy:0.4089\n",
    "Epoch #68: Loss:1.0628, Accuracy:0.4168, Validation Loss:1.0556, Validation Accuracy:0.4187\n",
    "Epoch #69: Loss:1.0620, Accuracy:0.4197, Validation Loss:1.0551, Validation Accuracy:0.4138\n",
    "Epoch #70: Loss:1.0625, Accuracy:0.4222, Validation Loss:1.0550, Validation Accuracy:0.4105\n",
    "Epoch #71: Loss:1.0613, Accuracy:0.4238, Validation Loss:1.0543, Validation Accuracy:0.4286\n",
    "Epoch #72: Loss:1.0614, Accuracy:0.4197, Validation Loss:1.0539, Validation Accuracy:0.4319\n",
    "Epoch #73: Loss:1.0606, Accuracy:0.4201, Validation Loss:1.0530, Validation Accuracy:0.4384\n",
    "Epoch #74: Loss:1.0611, Accuracy:0.4214, Validation Loss:1.0531, Validation Accuracy:0.4253\n",
    "Epoch #75: Loss:1.0623, Accuracy:0.4148, Validation Loss:1.0525, Validation Accuracy:0.4368\n",
    "Epoch #76: Loss:1.0626, Accuracy:0.4267, Validation Loss:1.0547, Validation Accuracy:0.4154\n",
    "Epoch #77: Loss:1.0613, Accuracy:0.4177, Validation Loss:1.0536, Validation Accuracy:0.4319\n",
    "Epoch #78: Loss:1.0596, Accuracy:0.4230, Validation Loss:1.0543, Validation Accuracy:0.4220\n",
    "Epoch #79: Loss:1.0591, Accuracy:0.4246, Validation Loss:1.0516, Validation Accuracy:0.4401\n",
    "Epoch #80: Loss:1.0578, Accuracy:0.4246, Validation Loss:1.0509, Validation Accuracy:0.4368\n",
    "Epoch #81: Loss:1.0571, Accuracy:0.4287, Validation Loss:1.0505, Validation Accuracy:0.4319\n",
    "Epoch #82: Loss:1.0563, Accuracy:0.4255, Validation Loss:1.0493, Validation Accuracy:0.4335\n",
    "Epoch #83: Loss:1.0557, Accuracy:0.4222, Validation Loss:1.0490, Validation Accuracy:0.4401\n",
    "Epoch #84: Loss:1.0555, Accuracy:0.4271, Validation Loss:1.0488, Validation Accuracy:0.4417\n",
    "Epoch #85: Loss:1.0552, Accuracy:0.4279, Validation Loss:1.0479, Validation Accuracy:0.4417\n",
    "Epoch #86: Loss:1.0544, Accuracy:0.4251, Validation Loss:1.0486, Validation Accuracy:0.4417\n",
    "Epoch #87: Loss:1.0558, Accuracy:0.4259, Validation Loss:1.0483, Validation Accuracy:0.4417\n",
    "Epoch #88: Loss:1.0533, Accuracy:0.4218, Validation Loss:1.0478, Validation Accuracy:0.4417\n",
    "Epoch #89: Loss:1.0534, Accuracy:0.4279, Validation Loss:1.0487, Validation Accuracy:0.4401\n",
    "Epoch #90: Loss:1.0524, Accuracy:0.4296, Validation Loss:1.0474, Validation Accuracy:0.4483\n",
    "Epoch #91: Loss:1.0517, Accuracy:0.4255, Validation Loss:1.0481, Validation Accuracy:0.4565\n",
    "Epoch #92: Loss:1.0515, Accuracy:0.4357, Validation Loss:1.0473, Validation Accuracy:0.4450\n",
    "Epoch #93: Loss:1.0495, Accuracy:0.4341, Validation Loss:1.0498, Validation Accuracy:0.4499\n",
    "Epoch #94: Loss:1.0514, Accuracy:0.4361, Validation Loss:1.0511, Validation Accuracy:0.4417\n",
    "Epoch #95: Loss:1.0484, Accuracy:0.4415, Validation Loss:1.0552, Validation Accuracy:0.4433\n",
    "Epoch #96: Loss:1.0539, Accuracy:0.4394, Validation Loss:1.0494, Validation Accuracy:0.4417\n",
    "Epoch #97: Loss:1.0510, Accuracy:0.4390, Validation Loss:1.0467, Validation Accuracy:0.4565\n",
    "Epoch #98: Loss:1.0478, Accuracy:0.4382, Validation Loss:1.0467, Validation Accuracy:0.4466\n",
    "Epoch #99: Loss:1.0476, Accuracy:0.4341, Validation Loss:1.0487, Validation Accuracy:0.4548\n",
    "Epoch #100: Loss:1.0495, Accuracy:0.4435, Validation Loss:1.0523, Validation Accuracy:0.4286\n",
    "Epoch #101: Loss:1.0495, Accuracy:0.4448, Validation Loss:1.0531, Validation Accuracy:0.4401\n",
    "Epoch #102: Loss:1.0503, Accuracy:0.4312, Validation Loss:1.0483, Validation Accuracy:0.4516\n",
    "Epoch #103: Loss:1.0451, Accuracy:0.4423, Validation Loss:1.0476, Validation Accuracy:0.4647\n",
    "Epoch #104: Loss:1.0477, Accuracy:0.4345, Validation Loss:1.0461, Validation Accuracy:0.4548\n",
    "Epoch #105: Loss:1.0488, Accuracy:0.4480, Validation Loss:1.0462, Validation Accuracy:0.4516\n",
    "Epoch #106: Loss:1.0486, Accuracy:0.4304, Validation Loss:1.0473, Validation Accuracy:0.4565\n",
    "Epoch #107: Loss:1.0477, Accuracy:0.4517, Validation Loss:1.0476, Validation Accuracy:0.4548\n",
    "Epoch #108: Loss:1.0463, Accuracy:0.4370, Validation Loss:1.0468, Validation Accuracy:0.4548\n",
    "Epoch #109: Loss:1.0435, Accuracy:0.4480, Validation Loss:1.0474, Validation Accuracy:0.4466\n",
    "Epoch #110: Loss:1.0428, Accuracy:0.4444, Validation Loss:1.0458, Validation Accuracy:0.4663\n",
    "Epoch #111: Loss:1.0435, Accuracy:0.4505, Validation Loss:1.0472, Validation Accuracy:0.4483\n",
    "Epoch #112: Loss:1.0434, Accuracy:0.4415, Validation Loss:1.0471, Validation Accuracy:0.4614\n",
    "Epoch #113: Loss:1.0407, Accuracy:0.4559, Validation Loss:1.0458, Validation Accuracy:0.4598\n",
    "Epoch #114: Loss:1.0400, Accuracy:0.4534, Validation Loss:1.0469, Validation Accuracy:0.4581\n",
    "Epoch #115: Loss:1.0412, Accuracy:0.4419, Validation Loss:1.0469, Validation Accuracy:0.4548\n",
    "Epoch #116: Loss:1.0400, Accuracy:0.4600, Validation Loss:1.0461, Validation Accuracy:0.4565\n",
    "Epoch #117: Loss:1.0396, Accuracy:0.4575, Validation Loss:1.0484, Validation Accuracy:0.4548\n",
    "Epoch #118: Loss:1.0433, Accuracy:0.4452, Validation Loss:1.0491, Validation Accuracy:0.4499\n",
    "Epoch #119: Loss:1.0462, Accuracy:0.4407, Validation Loss:1.0475, Validation Accuracy:0.4483\n",
    "Epoch #120: Loss:1.0415, Accuracy:0.4600, Validation Loss:1.0532, Validation Accuracy:0.4499\n",
    "Epoch #121: Loss:1.0435, Accuracy:0.4374, Validation Loss:1.0504, Validation Accuracy:0.4368\n",
    "Epoch #122: Loss:1.0407, Accuracy:0.4591, Validation Loss:1.0525, Validation Accuracy:0.4483\n",
    "Epoch #123: Loss:1.0398, Accuracy:0.4513, Validation Loss:1.0486, Validation Accuracy:0.4516\n",
    "Epoch #124: Loss:1.0400, Accuracy:0.4452, Validation Loss:1.0455, Validation Accuracy:0.4548\n",
    "Epoch #125: Loss:1.0386, Accuracy:0.4595, Validation Loss:1.0477, Validation Accuracy:0.4548\n",
    "Epoch #126: Loss:1.0364, Accuracy:0.4538, Validation Loss:1.0466, Validation Accuracy:0.4548\n",
    "Epoch #127: Loss:1.0356, Accuracy:0.4600, Validation Loss:1.0481, Validation Accuracy:0.4565\n",
    "Epoch #128: Loss:1.0358, Accuracy:0.4542, Validation Loss:1.0459, Validation Accuracy:0.4548\n",
    "Epoch #129: Loss:1.0338, Accuracy:0.4620, Validation Loss:1.0450, Validation Accuracy:0.4614\n",
    "Epoch #130: Loss:1.0350, Accuracy:0.4608, Validation Loss:1.0506, Validation Accuracy:0.4499\n",
    "Epoch #131: Loss:1.0349, Accuracy:0.4649, Validation Loss:1.0463, Validation Accuracy:0.4466\n",
    "Epoch #132: Loss:1.0352, Accuracy:0.4637, Validation Loss:1.0449, Validation Accuracy:0.4516\n",
    "Epoch #133: Loss:1.0341, Accuracy:0.4591, Validation Loss:1.0464, Validation Accuracy:0.4516\n",
    "Epoch #134: Loss:1.0321, Accuracy:0.4657, Validation Loss:1.0448, Validation Accuracy:0.4499\n",
    "Epoch #135: Loss:1.0331, Accuracy:0.4628, Validation Loss:1.0441, Validation Accuracy:0.4499\n",
    "Epoch #136: Loss:1.0333, Accuracy:0.4665, Validation Loss:1.0462, Validation Accuracy:0.4532\n",
    "Epoch #137: Loss:1.0324, Accuracy:0.4628, Validation Loss:1.0427, Validation Accuracy:0.4532\n",
    "Epoch #138: Loss:1.0302, Accuracy:0.4628, Validation Loss:1.0431, Validation Accuracy:0.4598\n",
    "Epoch #139: Loss:1.0303, Accuracy:0.4686, Validation Loss:1.0452, Validation Accuracy:0.4516\n",
    "Epoch #140: Loss:1.0314, Accuracy:0.4698, Validation Loss:1.0442, Validation Accuracy:0.4483\n",
    "Epoch #141: Loss:1.0300, Accuracy:0.4624, Validation Loss:1.0443, Validation Accuracy:0.4548\n",
    "Epoch #142: Loss:1.0293, Accuracy:0.4665, Validation Loss:1.0477, Validation Accuracy:0.4483\n",
    "Epoch #143: Loss:1.0295, Accuracy:0.4632, Validation Loss:1.0419, Validation Accuracy:0.4548\n",
    "Epoch #144: Loss:1.0290, Accuracy:0.4649, Validation Loss:1.0420, Validation Accuracy:0.4516\n",
    "Epoch #145: Loss:1.0311, Accuracy:0.4657, Validation Loss:1.0531, Validation Accuracy:0.4466\n",
    "Epoch #146: Loss:1.0332, Accuracy:0.4493, Validation Loss:1.0422, Validation Accuracy:0.4631\n",
    "Epoch #147: Loss:1.0288, Accuracy:0.4653, Validation Loss:1.0419, Validation Accuracy:0.4516\n",
    "Epoch #148: Loss:1.0345, Accuracy:0.4505, Validation Loss:1.0539, Validation Accuracy:0.4401\n",
    "Epoch #149: Loss:1.0322, Accuracy:0.4595, Validation Loss:1.0416, Validation Accuracy:0.4433\n",
    "Epoch #150: Loss:1.0304, Accuracy:0.4591, Validation Loss:1.0412, Validation Accuracy:0.4565\n",
    "Epoch #151: Loss:1.0318, Accuracy:0.4624, Validation Loss:1.0444, Validation Accuracy:0.4483\n",
    "Epoch #152: Loss:1.0269, Accuracy:0.4657, Validation Loss:1.0418, Validation Accuracy:0.4516\n",
    "Epoch #153: Loss:1.0272, Accuracy:0.4657, Validation Loss:1.0394, Validation Accuracy:0.4433\n",
    "Epoch #154: Loss:1.0288, Accuracy:0.4620, Validation Loss:1.0466, Validation Accuracy:0.4499\n",
    "Epoch #155: Loss:1.0253, Accuracy:0.4678, Validation Loss:1.0435, Validation Accuracy:0.4483\n",
    "Epoch #156: Loss:1.0287, Accuracy:0.4661, Validation Loss:1.0404, Validation Accuracy:0.4499\n",
    "Epoch #157: Loss:1.0261, Accuracy:0.4727, Validation Loss:1.0441, Validation Accuracy:0.4516\n",
    "Epoch #158: Loss:1.0278, Accuracy:0.4632, Validation Loss:1.0445, Validation Accuracy:0.4532\n",
    "Epoch #159: Loss:1.0259, Accuracy:0.4694, Validation Loss:1.0456, Validation Accuracy:0.4532\n",
    "Epoch #160: Loss:1.0251, Accuracy:0.4682, Validation Loss:1.0414, Validation Accuracy:0.4499\n",
    "Epoch #161: Loss:1.0281, Accuracy:0.4612, Validation Loss:1.0402, Validation Accuracy:0.4466\n",
    "Epoch #162: Loss:1.0272, Accuracy:0.4789, Validation Loss:1.0464, Validation Accuracy:0.4450\n",
    "Epoch #163: Loss:1.0246, Accuracy:0.4715, Validation Loss:1.0408, Validation Accuracy:0.4532\n",
    "Epoch #164: Loss:1.0232, Accuracy:0.4669, Validation Loss:1.0460, Validation Accuracy:0.4483\n",
    "Epoch #165: Loss:1.0237, Accuracy:0.4649, Validation Loss:1.0431, Validation Accuracy:0.4532\n",
    "Epoch #166: Loss:1.0290, Accuracy:0.4616, Validation Loss:1.0404, Validation Accuracy:0.4532\n",
    "Epoch #167: Loss:1.0266, Accuracy:0.4637, Validation Loss:1.0509, Validation Accuracy:0.4548\n",
    "Epoch #168: Loss:1.0259, Accuracy:0.4571, Validation Loss:1.0393, Validation Accuracy:0.4548\n",
    "Epoch #169: Loss:1.0220, Accuracy:0.4715, Validation Loss:1.0458, Validation Accuracy:0.4532\n",
    "Epoch #170: Loss:1.0256, Accuracy:0.4686, Validation Loss:1.0416, Validation Accuracy:0.4466\n",
    "Epoch #171: Loss:1.0247, Accuracy:0.4739, Validation Loss:1.0443, Validation Accuracy:0.4516\n",
    "Epoch #172: Loss:1.0246, Accuracy:0.4661, Validation Loss:1.0429, Validation Accuracy:0.4548\n",
    "Epoch #173: Loss:1.0215, Accuracy:0.4723, Validation Loss:1.0397, Validation Accuracy:0.4516\n",
    "Epoch #174: Loss:1.0224, Accuracy:0.4682, Validation Loss:1.0471, Validation Accuracy:0.4466\n",
    "Epoch #175: Loss:1.0255, Accuracy:0.4694, Validation Loss:1.0408, Validation Accuracy:0.4548\n",
    "Epoch #176: Loss:1.0233, Accuracy:0.4694, Validation Loss:1.0408, Validation Accuracy:0.4499\n",
    "Epoch #177: Loss:1.0219, Accuracy:0.4715, Validation Loss:1.0437, Validation Accuracy:0.4598\n",
    "Epoch #178: Loss:1.0228, Accuracy:0.4764, Validation Loss:1.0413, Validation Accuracy:0.4450\n",
    "Epoch #179: Loss:1.0223, Accuracy:0.4735, Validation Loss:1.0413, Validation Accuracy:0.4499\n",
    "Epoch #180: Loss:1.0199, Accuracy:0.4682, Validation Loss:1.0482, Validation Accuracy:0.4532\n",
    "Epoch #181: Loss:1.0213, Accuracy:0.4665, Validation Loss:1.0403, Validation Accuracy:0.4433\n",
    "Epoch #182: Loss:1.0238, Accuracy:0.4657, Validation Loss:1.0402, Validation Accuracy:0.4433\n",
    "Epoch #183: Loss:1.0235, Accuracy:0.4657, Validation Loss:1.0578, Validation Accuracy:0.4384\n",
    "Epoch #184: Loss:1.0308, Accuracy:0.4554, Validation Loss:1.0467, Validation Accuracy:0.4548\n",
    "Epoch #185: Loss:1.0278, Accuracy:0.4702, Validation Loss:1.0407, Validation Accuracy:0.4516\n",
    "Epoch #186: Loss:1.0259, Accuracy:0.4653, Validation Loss:1.0407, Validation Accuracy:0.4598\n",
    "Epoch #187: Loss:1.0218, Accuracy:0.4686, Validation Loss:1.0398, Validation Accuracy:0.4548\n",
    "Epoch #188: Loss:1.0195, Accuracy:0.4702, Validation Loss:1.0417, Validation Accuracy:0.4565\n",
    "Epoch #189: Loss:1.0225, Accuracy:0.4702, Validation Loss:1.0412, Validation Accuracy:0.4565\n",
    "Epoch #190: Loss:1.0220, Accuracy:0.4710, Validation Loss:1.0416, Validation Accuracy:0.4532\n",
    "Epoch #191: Loss:1.0297, Accuracy:0.4604, Validation Loss:1.0521, Validation Accuracy:0.4450\n",
    "Epoch #192: Loss:1.0204, Accuracy:0.4645, Validation Loss:1.0387, Validation Accuracy:0.4532\n",
    "Epoch #193: Loss:1.0184, Accuracy:0.4817, Validation Loss:1.0460, Validation Accuracy:0.4548\n",
    "Epoch #194: Loss:1.0207, Accuracy:0.4612, Validation Loss:1.0405, Validation Accuracy:0.4565\n",
    "Epoch #195: Loss:1.0178, Accuracy:0.4747, Validation Loss:1.0417, Validation Accuracy:0.4516\n",
    "Epoch #196: Loss:1.0183, Accuracy:0.4715, Validation Loss:1.0415, Validation Accuracy:0.4532\n",
    "Epoch #197: Loss:1.0190, Accuracy:0.4702, Validation Loss:1.0396, Validation Accuracy:0.4450\n",
    "Epoch #198: Loss:1.0250, Accuracy:0.4587, Validation Loss:1.0502, Validation Accuracy:0.4516\n",
    "Epoch #199: Loss:1.0220, Accuracy:0.4723, Validation Loss:1.0408, Validation Accuracy:0.4548\n",
    "Epoch #200: Loss:1.0208, Accuracy:0.4637, Validation Loss:1.0393, Validation Accuracy:0.4532\n",
    "Epoch #201: Loss:1.0218, Accuracy:0.4665, Validation Loss:1.0485, Validation Accuracy:0.4516\n",
    "Epoch #202: Loss:1.0221, Accuracy:0.4641, Validation Loss:1.0387, Validation Accuracy:0.4516\n",
    "Epoch #203: Loss:1.0185, Accuracy:0.4686, Validation Loss:1.0382, Validation Accuracy:0.4532\n",
    "Epoch #204: Loss:1.0189, Accuracy:0.4719, Validation Loss:1.0447, Validation Accuracy:0.4516\n",
    "Epoch #205: Loss:1.0182, Accuracy:0.4768, Validation Loss:1.0409, Validation Accuracy:0.4516\n",
    "Epoch #206: Loss:1.0175, Accuracy:0.4706, Validation Loss:1.0406, Validation Accuracy:0.4516\n",
    "Epoch #207: Loss:1.0165, Accuracy:0.4719, Validation Loss:1.0405, Validation Accuracy:0.4532\n",
    "Epoch #208: Loss:1.0166, Accuracy:0.4756, Validation Loss:1.0495, Validation Accuracy:0.4516\n",
    "Epoch #209: Loss:1.0214, Accuracy:0.4678, Validation Loss:1.0401, Validation Accuracy:0.4532\n",
    "Epoch #210: Loss:1.0195, Accuracy:0.4784, Validation Loss:1.0478, Validation Accuracy:0.4516\n",
    "Epoch #211: Loss:1.0184, Accuracy:0.4682, Validation Loss:1.0407, Validation Accuracy:0.4499\n",
    "Epoch #212: Loss:1.0204, Accuracy:0.4686, Validation Loss:1.0405, Validation Accuracy:0.4499\n",
    "Epoch #213: Loss:1.0183, Accuracy:0.4727, Validation Loss:1.0454, Validation Accuracy:0.4466\n",
    "Epoch #214: Loss:1.0176, Accuracy:0.4731, Validation Loss:1.0419, Validation Accuracy:0.4532\n",
    "Epoch #215: Loss:1.0167, Accuracy:0.4739, Validation Loss:1.0431, Validation Accuracy:0.4565\n",
    "Epoch #216: Loss:1.0182, Accuracy:0.4690, Validation Loss:1.0401, Validation Accuracy:0.4565\n",
    "Epoch #217: Loss:1.0170, Accuracy:0.4752, Validation Loss:1.0529, Validation Accuracy:0.4483\n",
    "Epoch #218: Loss:1.0225, Accuracy:0.4678, Validation Loss:1.0387, Validation Accuracy:0.4516\n",
    "Epoch #219: Loss:1.0187, Accuracy:0.4702, Validation Loss:1.0387, Validation Accuracy:0.4516\n",
    "Epoch #220: Loss:1.0169, Accuracy:0.4731, Validation Loss:1.0473, Validation Accuracy:0.4532\n",
    "Epoch #221: Loss:1.0181, Accuracy:0.4715, Validation Loss:1.0404, Validation Accuracy:0.4516\n",
    "Epoch #222: Loss:1.0183, Accuracy:0.4739, Validation Loss:1.0394, Validation Accuracy:0.4548\n",
    "Epoch #223: Loss:1.0145, Accuracy:0.4690, Validation Loss:1.0476, Validation Accuracy:0.4499\n",
    "Epoch #224: Loss:1.0156, Accuracy:0.4735, Validation Loss:1.0399, Validation Accuracy:0.4483\n",
    "Epoch #225: Loss:1.0186, Accuracy:0.4686, Validation Loss:1.0479, Validation Accuracy:0.4417\n",
    "Epoch #226: Loss:1.0151, Accuracy:0.4752, Validation Loss:1.0450, Validation Accuracy:0.4548\n",
    "Epoch #227: Loss:1.0159, Accuracy:0.4719, Validation Loss:1.0422, Validation Accuracy:0.4483\n",
    "Epoch #228: Loss:1.0160, Accuracy:0.4727, Validation Loss:1.0393, Validation Accuracy:0.4516\n",
    "Epoch #229: Loss:1.0156, Accuracy:0.4735, Validation Loss:1.0437, Validation Accuracy:0.4532\n",
    "Epoch #230: Loss:1.0134, Accuracy:0.4784, Validation Loss:1.0410, Validation Accuracy:0.4532\n",
    "Epoch #231: Loss:1.0156, Accuracy:0.4731, Validation Loss:1.0401, Validation Accuracy:0.4516\n",
    "Epoch #232: Loss:1.0181, Accuracy:0.4694, Validation Loss:1.0540, Validation Accuracy:0.4565\n",
    "Epoch #233: Loss:1.0147, Accuracy:0.4694, Validation Loss:1.0408, Validation Accuracy:0.4466\n",
    "Epoch #234: Loss:1.0133, Accuracy:0.4752, Validation Loss:1.0542, Validation Accuracy:0.4581\n",
    "Epoch #235: Loss:1.0149, Accuracy:0.4764, Validation Loss:1.0385, Validation Accuracy:0.4499\n",
    "Epoch #236: Loss:1.0156, Accuracy:0.4780, Validation Loss:1.0469, Validation Accuracy:0.4532\n",
    "Epoch #237: Loss:1.0135, Accuracy:0.4805, Validation Loss:1.0413, Validation Accuracy:0.4499\n",
    "Epoch #238: Loss:1.0163, Accuracy:0.4715, Validation Loss:1.0432, Validation Accuracy:0.4499\n",
    "Epoch #239: Loss:1.0137, Accuracy:0.4739, Validation Loss:1.0421, Validation Accuracy:0.4499\n",
    "Epoch #240: Loss:1.0153, Accuracy:0.4772, Validation Loss:1.0447, Validation Accuracy:0.4483\n",
    "Epoch #241: Loss:1.0142, Accuracy:0.4739, Validation Loss:1.0433, Validation Accuracy:0.4516\n",
    "Epoch #242: Loss:1.0116, Accuracy:0.4789, Validation Loss:1.0500, Validation Accuracy:0.4483\n",
    "Epoch #243: Loss:1.0135, Accuracy:0.4801, Validation Loss:1.0427, Validation Accuracy:0.4663\n",
    "Epoch #244: Loss:1.0219, Accuracy:0.4616, Validation Loss:1.0534, Validation Accuracy:0.4532\n",
    "Epoch #245: Loss:1.0179, Accuracy:0.4735, Validation Loss:1.0422, Validation Accuracy:0.4499\n",
    "Epoch #246: Loss:1.0137, Accuracy:0.4735, Validation Loss:1.0379, Validation Accuracy:0.4581\n",
    "Epoch #247: Loss:1.0141, Accuracy:0.4805, Validation Loss:1.0416, Validation Accuracy:0.4516\n",
    "Epoch #248: Loss:1.0116, Accuracy:0.4747, Validation Loss:1.0459, Validation Accuracy:0.4516\n",
    "Epoch #249: Loss:1.0147, Accuracy:0.4756, Validation Loss:1.0387, Validation Accuracy:0.4466\n",
    "Epoch #250: Loss:1.0132, Accuracy:0.4772, Validation Loss:1.0460, Validation Accuracy:0.4516\n",
    "Epoch #251: Loss:1.0139, Accuracy:0.4764, Validation Loss:1.0423, Validation Accuracy:0.4499\n",
    "Epoch #252: Loss:1.0163, Accuracy:0.4678, Validation Loss:1.0379, Validation Accuracy:0.4663\n",
    "Epoch #253: Loss:1.0123, Accuracy:0.4678, Validation Loss:1.0459, Validation Accuracy:0.4532\n",
    "Epoch #254: Loss:1.0111, Accuracy:0.4735, Validation Loss:1.0400, Validation Accuracy:0.4516\n",
    "Epoch #255: Loss:1.0118, Accuracy:0.4752, Validation Loss:1.0393, Validation Accuracy:0.4532\n",
    "Epoch #256: Loss:1.0104, Accuracy:0.4756, Validation Loss:1.0513, Validation Accuracy:0.4450\n",
    "Epoch #257: Loss:1.0108, Accuracy:0.4739, Validation Loss:1.0396, Validation Accuracy:0.4516\n",
    "Epoch #258: Loss:1.0115, Accuracy:0.4805, Validation Loss:1.0397, Validation Accuracy:0.4532\n",
    "Epoch #259: Loss:1.0103, Accuracy:0.4809, Validation Loss:1.0438, Validation Accuracy:0.4499\n",
    "Epoch #260: Loss:1.0104, Accuracy:0.4739, Validation Loss:1.0517, Validation Accuracy:0.4466\n",
    "Epoch #261: Loss:1.0118, Accuracy:0.4735, Validation Loss:1.0457, Validation Accuracy:0.4483\n",
    "Epoch #262: Loss:1.0090, Accuracy:0.4756, Validation Loss:1.0444, Validation Accuracy:0.4466\n",
    "Epoch #263: Loss:1.0084, Accuracy:0.4760, Validation Loss:1.0394, Validation Accuracy:0.4565\n",
    "Epoch #264: Loss:1.0112, Accuracy:0.4784, Validation Loss:1.0439, Validation Accuracy:0.4433\n",
    "Epoch #265: Loss:1.0089, Accuracy:0.4805, Validation Loss:1.0426, Validation Accuracy:0.4466\n",
    "Epoch #266: Loss:1.0085, Accuracy:0.4797, Validation Loss:1.0400, Validation Accuracy:0.4614\n",
    "Epoch #267: Loss:1.0100, Accuracy:0.4784, Validation Loss:1.0623, Validation Accuracy:0.4532\n",
    "Epoch #268: Loss:1.0142, Accuracy:0.4776, Validation Loss:1.0395, Validation Accuracy:0.4466\n",
    "Epoch #269: Loss:1.0119, Accuracy:0.4694, Validation Loss:1.0382, Validation Accuracy:0.4631\n",
    "Epoch #270: Loss:1.0100, Accuracy:0.4772, Validation Loss:1.0427, Validation Accuracy:0.4548\n",
    "Epoch #271: Loss:1.0085, Accuracy:0.4805, Validation Loss:1.0448, Validation Accuracy:0.4499\n",
    "Epoch #272: Loss:1.0089, Accuracy:0.4789, Validation Loss:1.0389, Validation Accuracy:0.4532\n",
    "Epoch #273: Loss:1.0135, Accuracy:0.4772, Validation Loss:1.0388, Validation Accuracy:0.4631\n",
    "Epoch #274: Loss:1.0109, Accuracy:0.4830, Validation Loss:1.0526, Validation Accuracy:0.4483\n",
    "Epoch #275: Loss:1.0132, Accuracy:0.4702, Validation Loss:1.0425, Validation Accuracy:0.4581\n",
    "Epoch #276: Loss:1.0081, Accuracy:0.4776, Validation Loss:1.0440, Validation Accuracy:0.4516\n",
    "Epoch #277: Loss:1.0066, Accuracy:0.4797, Validation Loss:1.0459, Validation Accuracy:0.4466\n",
    "Epoch #278: Loss:1.0089, Accuracy:0.4760, Validation Loss:1.0389, Validation Accuracy:0.4598\n",
    "Epoch #279: Loss:1.0070, Accuracy:0.4834, Validation Loss:1.0425, Validation Accuracy:0.4532\n",
    "Epoch #280: Loss:1.0087, Accuracy:0.4838, Validation Loss:1.0482, Validation Accuracy:0.4483\n",
    "Epoch #281: Loss:1.0080, Accuracy:0.4797, Validation Loss:1.0404, Validation Accuracy:0.4614\n",
    "Epoch #282: Loss:1.0057, Accuracy:0.4813, Validation Loss:1.0445, Validation Accuracy:0.4483\n",
    "Epoch #283: Loss:1.0062, Accuracy:0.4805, Validation Loss:1.0401, Validation Accuracy:0.4581\n",
    "Epoch #284: Loss:1.0046, Accuracy:0.4752, Validation Loss:1.0493, Validation Accuracy:0.4516\n",
    "Epoch #285: Loss:1.0077, Accuracy:0.4805, Validation Loss:1.0478, Validation Accuracy:0.4499\n",
    "Epoch #286: Loss:1.0063, Accuracy:0.4875, Validation Loss:1.0385, Validation Accuracy:0.4729\n",
    "Epoch #287: Loss:1.0046, Accuracy:0.4821, Validation Loss:1.0412, Validation Accuracy:0.4614\n",
    "Epoch #288: Loss:1.0083, Accuracy:0.4776, Validation Loss:1.0536, Validation Accuracy:0.4433\n",
    "Epoch #289: Loss:1.0062, Accuracy:0.4801, Validation Loss:1.0397, Validation Accuracy:0.4565\n",
    "Epoch #290: Loss:1.0070, Accuracy:0.4825, Validation Loss:1.0436, Validation Accuracy:0.4532\n",
    "Epoch #291: Loss:1.0045, Accuracy:0.4797, Validation Loss:1.0480, Validation Accuracy:0.4433\n",
    "Epoch #292: Loss:1.0073, Accuracy:0.4715, Validation Loss:1.0416, Validation Accuracy:0.4565\n",
    "Epoch #293: Loss:1.0029, Accuracy:0.4789, Validation Loss:1.0430, Validation Accuracy:0.4631\n",
    "Epoch #294: Loss:1.0051, Accuracy:0.4797, Validation Loss:1.0427, Validation Accuracy:0.4516\n",
    "Epoch #295: Loss:1.0038, Accuracy:0.4817, Validation Loss:1.0421, Validation Accuracy:0.4565\n",
    "Epoch #296: Loss:1.0014, Accuracy:0.4825, Validation Loss:1.0454, Validation Accuracy:0.4581\n",
    "Epoch #297: Loss:1.0014, Accuracy:0.4825, Validation Loss:1.0415, Validation Accuracy:0.4696\n",
    "Epoch #298: Loss:1.0031, Accuracy:0.4752, Validation Loss:1.0470, Validation Accuracy:0.4450\n",
    "Epoch #299: Loss:1.0012, Accuracy:0.4899, Validation Loss:1.0404, Validation Accuracy:0.4614\n",
    "Epoch #300: Loss:1.0016, Accuracy:0.4821, Validation Loss:1.0506, Validation Accuracy:0.4450\n",
    "\n",
    "Test:\n",
    "Test Loss:1.05056846, Accuracy:0.4450\n",
    "Labels: ['03', '01', '02']\n",
    "Confusion Matrix:\n",
    "      03   01   02\n",
    "t:03   6   89   47\n",
    "t:01   4  139   97\n",
    "t:02   5   96  126\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.40      0.04      0.08       142\n",
    "          01       0.43      0.58      0.49       240\n",
    "          02       0.47      0.56      0.51       227\n",
    "\n",
    "    accuracy                           0.44       609\n",
    "   macro avg       0.43      0.39      0.36       609\n",
    "weighted avg       0.44      0.44      0.40       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 03:13:57 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 44 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0811133979772307, 1.0750676070528078, 1.0747393487122259, 1.0753147557059728, 1.07473075683481, 1.0743056619891589, 1.0742858951706409, 1.0743220788113197, 1.0742481541751054, 1.0742268628870522, 1.0740979169977123, 1.0740426044745985, 1.0739647823405776, 1.0738848909760148, 1.0737926265093298, 1.073646888161332, 1.0734516014215003, 1.0732561942979033, 1.0729945858906837, 1.0727015537972913, 1.0723975468151674, 1.0719916634567461, 1.0715419897696459, 1.0708756374412374, 1.0701405179911647, 1.0692142120918813, 1.068095814418323, 1.0669811558840898, 1.0656509969034806, 1.064233514475705, 1.0630145098383987, 1.062029642620306, 1.0623269053711288, 1.0618939258782147, 1.0620449853843852, 1.0616556511723936, 1.0610084459307942, 1.0611725397689393, 1.06057028856575, 1.0600663938350083, 1.0598845994726973, 1.0601322445376167, 1.0596723850137495, 1.0596566014297686, 1.0598061748326117, 1.0595965428501124, 1.059116035650908, 1.0589172164794847, 1.0594455941361551, 1.058599518241945, 1.0586694388945506, 1.058681627016741, 1.058488349217695, 1.0579694893168308, 1.0577629792866448, 1.0575921410214528, 1.0575099044245453, 1.0574453861646855, 1.0569743753849774, 1.0567139297087595, 1.0584658699474117, 1.0567749515542844, 1.0564703853259534, 1.0566362579076356, 1.0562137085424463, 1.0559051430284097, 1.056041264181654, 1.055552372791497, 1.0550566635695584, 1.0550022546097955, 1.0542848509520732, 1.0538844005227677, 1.0530172741080348, 1.053098752972332, 1.052488956545374, 1.0547073707596226, 1.0536162622260734, 1.0542751822761323, 1.0516008586914865, 1.0508886806679085, 1.0505103207574102, 1.0493332478408939, 1.0489778634166873, 1.048785237060196, 1.0478960665184485, 1.0485505102498973, 1.0483224008275174, 1.0478400438289923, 1.0486544824781872, 1.047446418082577, 1.0481387930746344, 1.0473005188314002, 1.0498152055176608, 1.0510835381368502, 1.0551672976200999, 1.049435136353441, 1.046717102500214, 1.0466871774451094, 1.048711267598157, 1.0522999941617592, 1.0531129095158944, 1.0482709073080805, 1.0476372788105104, 1.0460975336519565, 1.046180740170095, 1.0473116967086917, 1.0476445049683645, 1.0468182773229915, 1.04740845041322, 1.0458218935870969, 1.047213194405504, 1.0470575845887509, 1.0457758124434497, 1.0469131411002774, 1.0469121249829998, 1.0460603949667393, 1.048353345523327, 1.049076922812877, 1.0474658867799982, 1.0531748133927143, 1.0504315858599784, 1.0525296127854897, 1.0486465860861667, 1.0454551629440734, 1.0476530774669304, 1.046559770706252, 1.0480677629339283, 1.0459210680819107, 1.0449907043688795, 1.050595974295793, 1.0463051696129033, 1.0449020970239624, 1.0463630348590796, 1.0447841540150258, 1.044113625838056, 1.0461733123939025, 1.0427186884512063, 1.0431103986276586, 1.0452229526438346, 1.0441828320179078, 1.0443158417891203, 1.0477289229582487, 1.0419204728356724, 1.0420199033662016, 1.0530709525438757, 1.0421893485073972, 1.0418837728171513, 1.0539487228409214, 1.0416055868803378, 1.0411774295891447, 1.0444309840648633, 1.0418307076534028, 1.03937725147786, 1.0465998124997995, 1.043537866697327, 1.0404498514479212, 1.0441369137348995, 1.0444655719844775, 1.0455541407141975, 1.041402153389403, 1.0402221004363938, 1.0463541552351026, 1.0408413421931526, 1.0460479703834296, 1.043095802830162, 1.0403762032050021, 1.0509287535850638, 1.0393433637415443, 1.0457690684078949, 1.0416126652499922, 1.0442963711342397, 1.042852087561133, 1.0397178324181067, 1.0471011592053818, 1.040844348068112, 1.040777379851819, 1.0437344520158565, 1.0413318565130625, 1.0413300767907956, 1.0481791979769377, 1.0402910501890386, 1.0401986359766944, 1.05776453429255, 1.0466985050680602, 1.0407311278219489, 1.0406930059047754, 1.039846843686597, 1.0417158478390798, 1.0412080399508548, 1.0415817790822246, 1.0520833358780308, 1.0386527156203447, 1.0460333289771244, 1.0404508505353003, 1.0416938246568828, 1.0414920996366854, 1.039552766114033, 1.0502426134933196, 1.0407795628107632, 1.0392662073395327, 1.0485441563360405, 1.038717342910704, 1.038209680657473, 1.044675062246902, 1.040941391476661, 1.0406433267546404, 1.0405168488303624, 1.0494759767905049, 1.0400512482732387, 1.047771229728298, 1.0407061672758782, 1.040484251255668, 1.0453961660905033, 1.0419350361393394, 1.043070395200319, 1.0401100233466363, 1.0529441823708796, 1.0386969731946296, 1.0387151756114366, 1.0472876454026046, 1.0404221070028095, 1.0393975977044192, 1.0475975018612464, 1.0398608463738352, 1.0479373732223887, 1.0450101916621668, 1.042201408024492, 1.0392805995612309, 1.043657879719789, 1.040979125229596, 1.0401008953210364, 1.0539560734932059, 1.040837439997443, 1.0541764229584993, 1.038475006671962, 1.0469396911035422, 1.0413260808327711, 1.0431510014291272, 1.0420876271618997, 1.0447196621808708, 1.0433111928758168, 1.0500207456266155, 1.0426618128966032, 1.0534185789684551, 1.0422201659683328, 1.037911272009801, 1.0415712835753492, 1.0459317937860348, 1.0387294615430784, 1.0460123788742792, 1.042262656739584, 1.0379034787759014, 1.0459011352708187, 1.0400275776930434, 1.0392556229639915, 1.0513315243869776, 1.039607467714006, 1.0397495826085408, 1.0437991405747011, 1.0517279539985218, 1.045682917283282, 1.0444243766599883, 1.0393916675805654, 1.043855032505856, 1.0425734126509116, 1.0400142683379952, 1.0622917709287947, 1.0395237616521775, 1.0381515844310643, 1.042664621851127, 1.04484656350366, 1.038912584433219, 1.0387682194388754, 1.0526170180544674, 1.0425145142575594, 1.0440238019319983, 1.0459452133460585, 1.0388589196995952, 1.042459756674242, 1.0482403180869342, 1.040406579846036, 1.044473702097174, 1.0400964123470637, 1.0492933706696985, 1.0477808408549267, 1.0385489661509573, 1.0411757477398575, 1.0536495963928147, 1.0396556392287581, 1.0436304647151278, 1.0480063816988214, 1.0416153097779097, 1.0430465756574483, 1.0426985831879239, 1.042139782694173, 1.0454469396563, 1.0414982143489795, 1.0470369764540974, 1.040374779936128, 1.0505685320824434], 'val_acc': [0.37274219881137605, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3940886690209456, 0.3957307051457404, 0.3940886690209456, 0.3940886690209456, 0.4039408858675871, 0.40722495811717657, 0.4039408856718411, 0.4072249580193036, 0.40558292189450884, 0.40722495811717657, 0.41050903026889307, 0.40558292179663585, 0.40886699404622534, 0.40886699414409833, 0.4055829216008899, 0.4203612474091534, 0.41050903046463905, 0.41379310271422853, 0.4252873557835377, 0.40558292169876287, 0.4088669942419713, 0.41050903026889307, 0.4105090301710201, 0.4072249578235576, 0.40558292179663585, 0.40886699404622534, 0.4137931023227366, 0.41050903036676606, 0.41050903036676606, 0.42036124682191556, 0.4187192106971208, 0.4072249580193036, 0.41050903026889307, 0.4220032829467103, 0.40886699414409833, 0.40558292189450884, 0.41707717467019906, 0.41050903026889307, 0.4154351387411503, 0.4088669942419713, 0.4121510663936878, 0.42036124701766153, 0.42036124691978854, 0.41379310251848256, 0.42528735548991875, 0.423645319462997, 0.4154351388390233, 0.42036124701766153, 0.4203612471155345, 0.40886699404622534, 0.4187192108928668, 0.41379310261635555, 0.4105090301710201, 0.42857142773950824, 0.4318555000869707, 0.4384236443904037, 0.42528735548991875, 0.43678160846135494, 0.4154351390347692, 0.4318555003805897, 0.42200328343607524, 0.44006568061307144, 0.4367816086571009, 0.4318555002827167, 0.43349753640751143, 0.4400656810045634, 0.44170771742297704, 0.4417077172272311, 0.4417077172272311, 0.4417077172272311, 0.44170771712935814, 0.4400656809066904, 0.4482758618242831, 0.45648603809290916, 0.4449917894768206, 0.4499178934958572, 0.44170771703148515, 0.4433497532541529, 0.44170771712935814, 0.45648603809290916, 0.44663382579736133, 0.45484400651920803, 0.4285714281310002, 0.4400656811024364, 0.45155993407387257, 0.46469621861900995, 0.4548440020659874, 0.45155992962065195, 0.45648603809290916, 0.45484400642133505, 0.4548440019681144, 0.4466338212462677, 0.46633825493955067, 0.44827586192215607, 0.46141214646729345, 0.45977011044037164, 0.458128074119831, 0.45484400632346206, 0.45648603799503623, 0.45484400651920803, 0.44991789785120484, 0.4482758617264101, 0.44991789785120484, 0.4367816086571009, 0.44827585737106246, 0.45155993407387257, 0.45484400651920803, 0.45484400632346206, 0.45484400632346206, 0.4564860424482568, 0.4548440019681144, 0.46141214636942046, 0.44991789814482375, 0.44663382569948834, 0.45155993426961855, 0.45155993417174556, 0.4499178980469508, 0.4499178979490778, 0.45320196574544674, 0.4532019702965403, 0.4597701102446257, 0.45155993417174556, 0.448275862020029, 0.45484400632346206, 0.44827586192215607, 0.45484400632346206, 0.4515599339759996, 0.4466338212462677, 0.46305418704530876, 0.4515599339759996, 0.4400656812981823, 0.4433497532541529, 0.4564860424482568, 0.448275862020029, 0.4515599339759996, 0.4433497532541529, 0.44991789814482375, 0.4482758616285371, 0.4499178979490778, 0.45155993417174556, 0.4532019701007943, 0.45320196574544674, 0.44991789785120484, 0.44663382550374237, 0.4449917897704395, 0.4532019701986673, 0.448275862020029, 0.45320197000292134, 0.45320197000292134, 0.45484400651920803, 0.45484400632346206, 0.4532019702965403, 0.4466338254058694, 0.45155993417174556, 0.45484400632346206, 0.4515599339759996, 0.44663382569948834, 0.4548440061277161, 0.4499178979490778, 0.4597701146978463, 0.4449917895746936, 0.4499178979490778, 0.4532019702965403, 0.4433497532541529, 0.44334975335202587, 0.43842364527126054, 0.4548440061277161, 0.45155993407387257, 0.4597701147957193, 0.45484400622558907, 0.4564860424482568, 0.4564860424482568, 0.45320196990504835, 0.4449917897704395, 0.45320197000292134, 0.45484400632346206, 0.4564860424482568, 0.45155993407387257, 0.4532019701986673, 0.44499178928107463, 0.45155993407387257, 0.45484400632346206, 0.45320196990504835, 0.4515599339759996, 0.45155993407387257, 0.4532019701007943, 0.45155993407387257, 0.4515599339759996, 0.4515599338781266, 0.4532019701986673, 0.4515599339759996, 0.45320196990504835, 0.4515599339759996, 0.44991789775333185, 0.44991789765545886, 0.44663382560161535, 0.45320196990504835, 0.4564860424482568, 0.4564860422525108, 0.44827586192215607, 0.4515599338781266, 0.4515599338781266, 0.4532019701986673, 0.45155993407387257, 0.45484400622558907, 0.4499178979490778, 0.4482758615306641, 0.44170771712935814, 0.4548440061277161, 0.4482758617264101, 0.4515599338781266, 0.4532019701007943, 0.45320197000292134, 0.4515599337802536, 0.4564860424482568, 0.44663382560161535, 0.45812807857305154, 0.44991789765545886, 0.4532019701007943, 0.44991789775333185, 0.44991789775333185, 0.44991789775333185, 0.4482758615306641, 0.4515599338781266, 0.4482758616285371, 0.46633825890340636, 0.4532019701986673, 0.44991789775333185, 0.45812807837730557, 0.4515599339759996, 0.4515599339759996, 0.4466338253079964, 0.4515599338781266, 0.44991789765545886, 0.4663382590991523, 0.4532019701007943, 0.4515599339759996, 0.45320196990504835, 0.4449917893789476, 0.4515599337802536, 0.45320197000292134, 0.44991789775333185, 0.4466338254058694, 0.4482758616285371, 0.4466338254058694, 0.4564860423503838, 0.4433497532541529, 0.44663382550374237, 0.46141215082264103, 0.4532019701986673, 0.4466338253079964, 0.46305418704530876, 0.4548440061277161, 0.44991789775333185, 0.45320196990504835, 0.46305418704530876, 0.4482758617264101, 0.4581280787687975, 0.4515599338781266, 0.44663382560161535, 0.4597701145999733, 0.45320196990504835, 0.4482758616285371, 0.46141215082264103, 0.44827586143279113, 0.45812807837730557, 0.4515599338781266, 0.44991789775333185, 0.47290640369620424, 0.46141215062689506, 0.4433497531562799, 0.4564860424482568, 0.45320196990504835, 0.4433497532541529, 0.4564860425461298, 0.4630541869474358, 0.45155993417174556, 0.4564860425461298, 0.45812807857305154, 0.46962233144661475, 0.44499178928107463, 0.461412150920514, 0.4449917894768206], 'loss': [1.088511740451476, 1.0783644110759916, 1.0748290559839173, 1.0749110955232468, 1.0750141667144744, 1.0744964170260107, 1.0741119662349474, 1.0741942898203949, 1.0741882277220427, 1.07432248416133, 1.0742453881112946, 1.0740891257350695, 1.0740373677052022, 1.074008271238887, 1.0740690265103288, 1.0740776671031662, 1.0737383191345653, 1.0736315207804499, 1.0734062354422693, 1.0732475342447019, 1.073090847708606, 1.0728353216907571, 1.0726626764332734, 1.0721193765957497, 1.0718998473772523, 1.071438677061265, 1.0704242888417332, 1.0702694534031518, 1.069053661015489, 1.068466661549202, 1.0677819857607143, 1.068712833872566, 1.0679279933964692, 1.067647641836006, 1.0684646587841808, 1.0675449695430497, 1.067215364718584, 1.067122638690643, 1.0669574779651494, 1.0666773149854594, 1.0667826436628307, 1.0666062143059483, 1.0664543519029872, 1.0672366603444, 1.0663633088066837, 1.0665840254182444, 1.0657186868988757, 1.0666775668181434, 1.0661418288150608, 1.066185339128702, 1.0658214527968264, 1.0657107166196287, 1.0655846485122273, 1.0655191184558908, 1.065262151498814, 1.0650233264821265, 1.0648186019803465, 1.0648592139171624, 1.0643712652292585, 1.0651954682211122, 1.0654336302187408, 1.064700065501172, 1.0641889609840127, 1.0641856113743244, 1.0634898102503783, 1.0631075364608296, 1.064009650434067, 1.0627708949102759, 1.0620494693211706, 1.062541426231729, 1.0613086175135273, 1.061440816845982, 1.0606302732804473, 1.0611264365899244, 1.062347030933388, 1.0625623560539261, 1.061312079723366, 1.059600057298398, 1.059118897175642, 1.0578300233739113, 1.0570584806818248, 1.0562588813613327, 1.0556561013756347, 1.0555215100732918, 1.0552443781917344, 1.0544015423228363, 1.0557963507376167, 1.053310243158125, 1.0533590469027447, 1.0524347678836612, 1.0516984007931343, 1.051539835352183, 1.0495182600843833, 1.0514253068019233, 1.0484456394242554, 1.0539213723960108, 1.050962454435517, 1.0478132528935615, 1.0475670183463752, 1.0495284811916783, 1.0494946239420522, 1.0503154901998, 1.045114748189092, 1.0476981758826567, 1.0487527024819376, 1.048631010456986, 1.0476782164779288, 1.0463482735338152, 1.0435082723472642, 1.042822214514323, 1.0434535002071998, 1.0433839007324752, 1.0407207069945286, 1.0400251327843637, 1.0412277291687606, 1.040022073927846, 1.0395890513974293, 1.043285798830663, 1.0461547900519088, 1.041504032655908, 1.0434729794946784, 1.0407280007916555, 1.0397517656643533, 1.0399763230670405, 1.0385842042292412, 1.0364224067703653, 1.0356332102840196, 1.0358378349143622, 1.0338373864211097, 1.034968640574195, 1.0349348926935842, 1.035225548293801, 1.0341189237590689, 1.0321380877152115, 1.0330665510537933, 1.033333481459647, 1.0323669948617047, 1.0301597062567176, 1.0302563452377946, 1.0314468971757673, 1.0300245167048805, 1.0293313060697833, 1.029484406437962, 1.0290472935357378, 1.0310908066908193, 1.033204736259194, 1.028759923069384, 1.0344877365923026, 1.0322472760319954, 1.0304298046433216, 1.0317951565160888, 1.0269406371537664, 1.0271724761633902, 1.0288336898756714, 1.0253184846539272, 1.028681402578491, 1.0260642786290366, 1.0278109333598393, 1.0259194138358505, 1.025111250955711, 1.0281188435623043, 1.027227903783199, 1.0245931593054864, 1.0231691337464037, 1.0237284681390688, 1.029002694525513, 1.0265905283804546, 1.0259267355627104, 1.0219834138725328, 1.0256049386284924, 1.0247468285002503, 1.0245877709476854, 1.021458686083494, 1.0224043055481489, 1.02553341638381, 1.023316675192032, 1.0219070647041901, 1.0227833179477794, 1.022317562896368, 1.0199033787607903, 1.021330788933521, 1.023774791596117, 1.023464945258546, 1.0308016141092509, 1.027791865944128, 1.0258695161318143, 1.0217606825995005, 1.0194854883197886, 1.022476569338256, 1.0220330610901913, 1.0296724086424653, 1.0203905994642442, 1.0184412475484108, 1.0206616068278005, 1.0178207439074036, 1.0183094868669764, 1.0190023872641811, 1.0250052791846116, 1.0219965865235063, 1.0207538997857721, 1.0218430108847805, 1.0221397353393586, 1.018548502520614, 1.0189387520725477, 1.0182222933005503, 1.0175187903018459, 1.0164952040208193, 1.0165720300263204, 1.021352107422063, 1.019506756725742, 1.0183840908798594, 1.0203807581131952, 1.0183315489571196, 1.0175716953355918, 1.0166661675216235, 1.0182416441993791, 1.0169993277447913, 1.0224733487781312, 1.0186610656597286, 1.0168675509811183, 1.018099622315205, 1.018252227781245, 1.0145428170903263, 1.0155648863535887, 1.0186130846795116, 1.0151050322354451, 1.0159003461900433, 1.0160278589573728, 1.0155873233777541, 1.013376442509755, 1.015570856315644, 1.018110388700967, 1.0146924483457875, 1.013310406242308, 1.0148520104449388, 1.0155975823529693, 1.013540027028971, 1.0162554789128, 1.013724342068118, 1.015299643089639, 1.0142025608791219, 1.0116237302090842, 1.013452719075479, 1.0219348611772918, 1.0179210003886134, 1.0136808920689921, 1.0141030585496578, 1.0116329050651567, 1.0146665555985313, 1.01317883223234, 1.0138651465977977, 1.0163318600742723, 1.0123194075707782, 1.011092532782584, 1.0118058136111656, 1.0103522460808254, 1.0107625721906, 1.0114609155322003, 1.0103060994549697, 1.0104134190498681, 1.011799366469256, 1.0090398656269364, 1.0083781138093075, 1.0112489866769778, 1.0089277860320325, 1.0084834037620185, 1.0099718136219518, 1.0141954028875677, 1.0119249271905888, 1.010020312538382, 1.0085372726040944, 1.0089415706403446, 1.0134599906218371, 1.0108872763674852, 1.0132446993058222, 1.0081227989412187, 1.0065722457192516, 1.0089313611847175, 1.0070251930910459, 1.0087400430526576, 1.008033538696947, 1.0057060318805844, 1.0061639487376204, 1.004612614046132, 1.0077423081016148, 1.0063030238513828, 1.0045942789475286, 1.0082943936393, 1.0061709142073958, 1.0069554056230268, 1.0044782473076541, 1.0072689721226937, 1.002877559823422, 1.0050917047249952, 1.0037724648657766, 1.0013556574893927, 1.0013520412131747, 1.003148733859679, 1.0011716076970345, 1.0016225182545013], 'acc': [0.37289527737874023, 0.38562628510551533, 0.3942505126983478, 0.39425051547663414, 0.39425051211086876, 0.394250514301676, 0.3942505146933287, 0.3942505123066951, 0.39425051391002336, 0.3942505133225443, 0.39425051113173704, 0.394250513714197, 0.394250513714197, 0.3942505152808078, 0.3950718677386611, 0.3942505113642808, 0.394250514301676, 0.3942505115233897, 0.3954825477433645, 0.3942505115233897, 0.3946611928988776, 0.3954825441817728, 0.40246406447226507, 0.4078028770687644, 0.405749485453541, 0.40533880681962203, 0.4086242309341196, 0.4045174519751351, 0.40657084006548416, 0.40821355190854786, 0.40780287569797996, 0.406981518271033, 0.39876796790461766, 0.4045174555367268, 0.4073921956932765, 0.4090349075363402, 0.4045174535784633, 0.4057494858451937, 0.4098562614016954, 0.41108829585923307, 0.4053388105770401, 0.4110882944517312, 0.4110882976216702, 0.4090349063613821, 0.41026693921559154, 0.412320328089246, 0.41190965027534987, 0.4045174531500932, 0.4131416839495821, 0.41108829347259945, 0.41149897288982384, 0.4078028725280409, 0.4102669427771833, 0.4110882956266893, 0.41519506961658015, 0.4135523605518028, 0.41396303914900434, 0.41190965285780984, 0.41437371794203226, 0.4094455861702592, 0.41396303954065705, 0.4168377821205578, 0.41478439458097033, 0.41396303797404627, 0.4127310045323577, 0.414784392390163, 0.42012320302839884, 0.41683778227966667, 0.41971252462702363, 0.4221765903354425, 0.4238193002202428, 0.41971252740530995, 0.4201232056108588, 0.4213552345118239, 0.4147843948135141, 0.4266940449175159, 0.4176591353617165, 0.42299794752984565, 0.42464065721881955, 0.42464065565220876, 0.4287474322245596, 0.42546201268750294, 0.4221765919020533, 0.42710472155645396, 0.42792608035418533, 0.4250513338577576, 0.4258726881147655, 0.4217659124848289, 0.4279260761683971, 0.4295687886356573, 0.42546201069252204, 0.43572895290670455, 0.4340862404761618, 0.436139630328948, 0.44147843717304835, 0.43942505009854843, 0.43901437267630494, 0.4381930184192971, 0.434086243413557, 0.4435318292166418, 0.44476385850925954, 0.43121149793297847, 0.44229979342503717, 0.43449691868171064, 0.44804927929470917, 0.43039014547512516, 0.4517453815780381, 0.43696098674004574, 0.4480492796863619, 0.444353183081997, 0.45051334754887057, 0.44147843760141847, 0.45585215400131823, 0.45338809126701196, 0.4418891179610572, 0.4599589327644763, 0.45749486525690286, 0.4451745394930947, 0.4406570823285614, 0.45995892998619003, 0.43737166514142095, 0.45913757514170306, 0.4513347031766629, 0.44517453573567667, 0.45954825295559926, 0.45379876947256087, 0.4599589315528008, 0.4542094452914761, 0.4620123198756937, 0.46078028878392135, 0.4648870617946805, 0.4636550323062364, 0.4591375767450313, 0.4657084166391674, 0.4628336739001578, 0.46652977367446163, 0.4628336752709422, 0.46283367746174947, 0.4685831601614825, 0.46981519477812905, 0.4624229992562006, 0.46652977563272513, 0.4632443539048612, 0.46488706516044587, 0.46570841980910643, 0.4492813140704647, 0.46529774179938393, 0.45051334696139156, 0.4595482563213646, 0.45913757615755224, 0.46242299863200415, 0.46570841922162737, 0.46570842059241185, 0.46201232183395713, 0.46776180864604344, 0.46611909621550074, 0.4726899381046178, 0.46324435449234025, 0.46940451676840655, 0.4681724823475863, 0.46119096675692645, 0.4788501004174015, 0.4714579078328683, 0.4669404494933769, 0.4648870633980087, 0.4616016428451029, 0.46365502995632024, 0.4570841894379876, 0.47145790544623467, 0.46858315996565614, 0.47392196919639007, 0.466119098406308, 0.47227926048654795, 0.46817248332671807, 0.4694045183717348, 0.46940451719677667, 0.4714579034879712, 0.4763860390538797, 0.4735112949073682, 0.4681724823475863, 0.4665297720711334, 0.4657084176182992, 0.46570841781412553, 0.4554414797857312, 0.4702258714537846, 0.46529774078353475, 0.46858315996565614, 0.4702258734120481, 0.4702258743911798, 0.4710472278281648, 0.4603696090117617, 0.46447638495991606, 0.48172484707293817, 0.46119096640199114, 0.4747433256074878, 0.47145790685373656, 0.47022587360787443, 0.45872689713198056, 0.47227926087820066, 0.46365502894047106, 0.4665297728544388, 0.46406570655854085, 0.46858316313559517, 0.4718685836517835, 0.47679671526444767, 0.4706365512259442, 0.4718685832968483, 0.47556468166365023, 0.4677618059044746, 0.47843942456176886, 0.4681724831676092, 0.4685831645063796, 0.47268993790879144, 0.47310061474355586, 0.4739219695880428, 0.4689938417327967, 0.47515400459634205, 0.4677618082911082, 0.47022587302039537, 0.4731006151352085, 0.4714579078328683, 0.4739219699796955, 0.4689938385628577, 0.4735112945524329, 0.4685831605531352, 0.4751540032255576, 0.4718685854509381, 0.47268994029542505, 0.4735112951031945, 0.4784394263609234, 0.47310061673853676, 0.46940451598510113, 0.4694045183717348, 0.4751540042414068, 0.4763860390171623, 0.4780287473353517, 0.4804928144512725, 0.47145790662119275, 0.47392197217050275, 0.477207390924254, 0.47392197177885004, 0.4788501004174015, 0.48008213444656905, 0.46160164421588734, 0.4735112945524329, 0.47351129474825926, 0.4804928150020341, 0.47474332423670335, 0.47556468283860837, 0.47720739072842766, 0.47638603549228803, 0.46776180809528184, 0.4677618051211692, 0.47351129314493107, 0.47515400224642584, 0.47556468064780105, 0.4739219729538081, 0.48049281543040423, 0.48090349226516865, 0.4739219705671745, 0.47351129474825926, 0.4755646798644957, 0.4759753596733728, 0.4784394249534215, 0.48049281288466167, 0.4796714558493675, 0.47843942514924787, 0.47761807073313106, 0.4694045185675611, 0.4772073905326013, 0.4804928152345779, 0.4788501043706459, 0.4772073920992121, 0.48295687698975237, 0.47022587497865886, 0.4776180697172819, 0.47967146094085256, 0.4759753616316363, 0.4833675581694139, 0.483778232225892, 0.4796714601942646, 0.4813141710214791, 0.4804928150387515, 0.47515400459634205, 0.48049281543040423, 0.4874743313759994, 0.4821355241035289, 0.47761806912980287, 0.48008213444656905, 0.48254619917585617, 0.47967145643684655, 0.4714579046996467, 0.4788501049214075, 0.47967145643684655, 0.48172484628963275, 0.48254620211325144, 0.4825461999591616, 0.47515400205059954, 0.48993839669276557, 0.4821355229285708]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
