{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf1.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 10:55:22 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'All', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['my', 'ek', 'by', 'eb', 'sg', 'ck', 'eo', 'yd', 'eg', 'sk', 'mb', 'ds', 'ib', 'aa', 'ce'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002398091D278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000239E8E36EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7307, Accuracy:0.0493, Validation Loss:2.7186, Validation Accuracy:0.0476\n",
    "Epoch #2: Loss:2.7118, Accuracy:0.0546, Validation Loss:2.7081, Validation Accuracy:0.0706\n",
    "Epoch #3: Loss:2.7030, Accuracy:0.0990, Validation Loss:2.6992, Validation Accuracy:0.1018\n",
    "Epoch #4: Loss:2.6952, Accuracy:0.1101, Validation Loss:2.6918, Validation Accuracy:0.1018\n",
    "Epoch #5: Loss:2.6889, Accuracy:0.1018, Validation Loss:2.6855, Validation Accuracy:0.1034\n",
    "Epoch #6: Loss:2.6828, Accuracy:0.1051, Validation Loss:2.6795, Validation Accuracy:0.0952\n",
    "Epoch #7: Loss:2.6771, Accuracy:0.1072, Validation Loss:2.6733, Validation Accuracy:0.0969\n",
    "Epoch #8: Loss:2.6711, Accuracy:0.0965, Validation Loss:2.6683, Validation Accuracy:0.0936\n",
    "Epoch #9: Loss:2.6656, Accuracy:0.0928, Validation Loss:2.6626, Validation Accuracy:0.0985\n",
    "Epoch #10: Loss:2.6599, Accuracy:0.0953, Validation Loss:2.6564, Validation Accuracy:0.1051\n",
    "Epoch #11: Loss:2.6528, Accuracy:0.1257, Validation Loss:2.6495, Validation Accuracy:0.1379\n",
    "Epoch #12: Loss:2.6450, Accuracy:0.1499, Validation Loss:2.6430, Validation Accuracy:0.1215\n",
    "Epoch #13: Loss:2.6367, Accuracy:0.1396, Validation Loss:2.6334, Validation Accuracy:0.1297\n",
    "Epoch #14: Loss:2.6254, Accuracy:0.1478, Validation Loss:2.6213, Validation Accuracy:0.1346\n",
    "Epoch #15: Loss:2.6108, Accuracy:0.1503, Validation Loss:2.6062, Validation Accuracy:0.1281\n",
    "Epoch #16: Loss:2.5931, Accuracy:0.1503, Validation Loss:2.5883, Validation Accuracy:0.1281\n",
    "Epoch #17: Loss:2.5745, Accuracy:0.1548, Validation Loss:2.5685, Validation Accuracy:0.1544\n",
    "Epoch #18: Loss:2.5559, Accuracy:0.1507, Validation Loss:2.5508, Validation Accuracy:0.1445\n",
    "Epoch #19: Loss:2.5431, Accuracy:0.1536, Validation Loss:2.5342, Validation Accuracy:0.1544\n",
    "Epoch #20: Loss:2.5250, Accuracy:0.1561, Validation Loss:2.5226, Validation Accuracy:0.1593\n",
    "Epoch #21: Loss:2.5140, Accuracy:0.1634, Validation Loss:2.5152, Validation Accuracy:0.1461\n",
    "Epoch #22: Loss:2.5065, Accuracy:0.1589, Validation Loss:2.5030, Validation Accuracy:0.1527\n",
    "Epoch #23: Loss:2.4993, Accuracy:0.1577, Validation Loss:2.5021, Validation Accuracy:0.1494\n",
    "Epoch #24: Loss:2.4911, Accuracy:0.1643, Validation Loss:2.4949, Validation Accuracy:0.1626\n",
    "Epoch #25: Loss:2.4893, Accuracy:0.1647, Validation Loss:2.4939, Validation Accuracy:0.1691\n",
    "Epoch #26: Loss:2.4843, Accuracy:0.1602, Validation Loss:2.4862, Validation Accuracy:0.1691\n",
    "Epoch #27: Loss:2.4795, Accuracy:0.1717, Validation Loss:2.4854, Validation Accuracy:0.1708\n",
    "Epoch #28: Loss:2.4770, Accuracy:0.1708, Validation Loss:2.4832, Validation Accuracy:0.1724\n",
    "Epoch #29: Loss:2.4758, Accuracy:0.1717, Validation Loss:2.4855, Validation Accuracy:0.1724\n",
    "Epoch #30: Loss:2.4733, Accuracy:0.1717, Validation Loss:2.4819, Validation Accuracy:0.1757\n",
    "Epoch #31: Loss:2.4713, Accuracy:0.1729, Validation Loss:2.4804, Validation Accuracy:0.1790\n",
    "Epoch #32: Loss:2.4700, Accuracy:0.1721, Validation Loss:2.4801, Validation Accuracy:0.1790\n",
    "Epoch #33: Loss:2.4668, Accuracy:0.1741, Validation Loss:2.4831, Validation Accuracy:0.1757\n",
    "Epoch #34: Loss:2.4659, Accuracy:0.1741, Validation Loss:2.4777, Validation Accuracy:0.1790\n",
    "Epoch #35: Loss:2.4649, Accuracy:0.1745, Validation Loss:2.4810, Validation Accuracy:0.1839\n",
    "Epoch #36: Loss:2.4645, Accuracy:0.1741, Validation Loss:2.4780, Validation Accuracy:0.1790\n",
    "Epoch #37: Loss:2.4631, Accuracy:0.1766, Validation Loss:2.4755, Validation Accuracy:0.1806\n",
    "Epoch #38: Loss:2.4613, Accuracy:0.1758, Validation Loss:2.4739, Validation Accuracy:0.1790\n",
    "Epoch #39: Loss:2.4595, Accuracy:0.1754, Validation Loss:2.4730, Validation Accuracy:0.1823\n",
    "Epoch #40: Loss:2.4585, Accuracy:0.1745, Validation Loss:2.4709, Validation Accuracy:0.1806\n",
    "Epoch #41: Loss:2.4554, Accuracy:0.1762, Validation Loss:2.4684, Validation Accuracy:0.1790\n",
    "Epoch #42: Loss:2.4545, Accuracy:0.1762, Validation Loss:2.4653, Validation Accuracy:0.1741\n",
    "Epoch #43: Loss:2.4549, Accuracy:0.1754, Validation Loss:2.4676, Validation Accuracy:0.1790\n",
    "Epoch #44: Loss:2.4538, Accuracy:0.1741, Validation Loss:2.4672, Validation Accuracy:0.1806\n",
    "Epoch #45: Loss:2.4517, Accuracy:0.1766, Validation Loss:2.4657, Validation Accuracy:0.1790\n",
    "Epoch #46: Loss:2.4514, Accuracy:0.1733, Validation Loss:2.4634, Validation Accuracy:0.1773\n",
    "Epoch #47: Loss:2.4513, Accuracy:0.1745, Validation Loss:2.4641, Validation Accuracy:0.1757\n",
    "Epoch #48: Loss:2.4519, Accuracy:0.1741, Validation Loss:2.4603, Validation Accuracy:0.1741\n",
    "Epoch #49: Loss:2.4520, Accuracy:0.1745, Validation Loss:2.4645, Validation Accuracy:0.1757\n",
    "Epoch #50: Loss:2.4501, Accuracy:0.1758, Validation Loss:2.4594, Validation Accuracy:0.1790\n",
    "Epoch #51: Loss:2.4493, Accuracy:0.1713, Validation Loss:2.4594, Validation Accuracy:0.1773\n",
    "Epoch #52: Loss:2.4459, Accuracy:0.1770, Validation Loss:2.4628, Validation Accuracy:0.1806\n",
    "Epoch #53: Loss:2.4464, Accuracy:0.1762, Validation Loss:2.4577, Validation Accuracy:0.1773\n",
    "Epoch #54: Loss:2.4447, Accuracy:0.1741, Validation Loss:2.4560, Validation Accuracy:0.1773\n",
    "Epoch #55: Loss:2.4444, Accuracy:0.1741, Validation Loss:2.4547, Validation Accuracy:0.1773\n",
    "Epoch #56: Loss:2.4432, Accuracy:0.1745, Validation Loss:2.4577, Validation Accuracy:0.1773\n",
    "Epoch #57: Loss:2.4428, Accuracy:0.1778, Validation Loss:2.4612, Validation Accuracy:0.1806\n",
    "Epoch #58: Loss:2.4469, Accuracy:0.1770, Validation Loss:2.4599, Validation Accuracy:0.1773\n",
    "Epoch #59: Loss:2.4471, Accuracy:0.1754, Validation Loss:2.4602, Validation Accuracy:0.1773\n",
    "Epoch #60: Loss:2.4446, Accuracy:0.1754, Validation Loss:2.4616, Validation Accuracy:0.1839\n",
    "Epoch #61: Loss:2.4415, Accuracy:0.1754, Validation Loss:2.4568, Validation Accuracy:0.1773\n",
    "Epoch #62: Loss:2.4414, Accuracy:0.1745, Validation Loss:2.4557, Validation Accuracy:0.1757\n",
    "Epoch #63: Loss:2.4407, Accuracy:0.1749, Validation Loss:2.4513, Validation Accuracy:0.1757\n",
    "Epoch #64: Loss:2.4411, Accuracy:0.1745, Validation Loss:2.4519, Validation Accuracy:0.1757\n",
    "Epoch #65: Loss:2.4405, Accuracy:0.1737, Validation Loss:2.4554, Validation Accuracy:0.1790\n",
    "Epoch #66: Loss:2.4415, Accuracy:0.1749, Validation Loss:2.4507, Validation Accuracy:0.1741\n",
    "Epoch #67: Loss:2.4443, Accuracy:0.1721, Validation Loss:2.4522, Validation Accuracy:0.1757\n",
    "Epoch #68: Loss:2.4452, Accuracy:0.1745, Validation Loss:2.4499, Validation Accuracy:0.1790\n",
    "Epoch #69: Loss:2.4441, Accuracy:0.1749, Validation Loss:2.4508, Validation Accuracy:0.1905\n",
    "Epoch #70: Loss:2.4425, Accuracy:0.1741, Validation Loss:2.4545, Validation Accuracy:0.1806\n",
    "Epoch #71: Loss:2.4410, Accuracy:0.1758, Validation Loss:2.4499, Validation Accuracy:0.1773\n",
    "Epoch #72: Loss:2.4415, Accuracy:0.1745, Validation Loss:2.4568, Validation Accuracy:0.1839\n",
    "Epoch #73: Loss:2.4412, Accuracy:0.1721, Validation Loss:2.4515, Validation Accuracy:0.1806\n",
    "Epoch #74: Loss:2.4398, Accuracy:0.1745, Validation Loss:2.4535, Validation Accuracy:0.1790\n",
    "Epoch #75: Loss:2.4393, Accuracy:0.1745, Validation Loss:2.4542, Validation Accuracy:0.1823\n",
    "Epoch #76: Loss:2.4393, Accuracy:0.1766, Validation Loss:2.4533, Validation Accuracy:0.1856\n",
    "Epoch #77: Loss:2.4395, Accuracy:0.1762, Validation Loss:2.4518, Validation Accuracy:0.1856\n",
    "Epoch #78: Loss:2.4396, Accuracy:0.1762, Validation Loss:2.4522, Validation Accuracy:0.1839\n",
    "Epoch #79: Loss:2.4396, Accuracy:0.1754, Validation Loss:2.4516, Validation Accuracy:0.1823\n",
    "Epoch #80: Loss:2.4392, Accuracy:0.1778, Validation Loss:2.4519, Validation Accuracy:0.1856\n",
    "Epoch #81: Loss:2.4397, Accuracy:0.1733, Validation Loss:2.4520, Validation Accuracy:0.1806\n",
    "Epoch #82: Loss:2.4380, Accuracy:0.1758, Validation Loss:2.4561, Validation Accuracy:0.1823\n",
    "Epoch #83: Loss:2.4386, Accuracy:0.1762, Validation Loss:2.4526, Validation Accuracy:0.1790\n",
    "Epoch #84: Loss:2.4375, Accuracy:0.1766, Validation Loss:2.4563, Validation Accuracy:0.1823\n",
    "Epoch #85: Loss:2.4380, Accuracy:0.1819, Validation Loss:2.4493, Validation Accuracy:0.1806\n",
    "Epoch #86: Loss:2.4365, Accuracy:0.1778, Validation Loss:2.4493, Validation Accuracy:0.1790\n",
    "Epoch #87: Loss:2.4362, Accuracy:0.1811, Validation Loss:2.4485, Validation Accuracy:0.1806\n",
    "Epoch #88: Loss:2.4360, Accuracy:0.1778, Validation Loss:2.4475, Validation Accuracy:0.1790\n",
    "Epoch #89: Loss:2.4363, Accuracy:0.1778, Validation Loss:2.4509, Validation Accuracy:0.1790\n",
    "Epoch #90: Loss:2.4372, Accuracy:0.1725, Validation Loss:2.4471, Validation Accuracy:0.1790\n",
    "Epoch #91: Loss:2.4352, Accuracy:0.1799, Validation Loss:2.4505, Validation Accuracy:0.1790\n",
    "Epoch #92: Loss:2.4361, Accuracy:0.1803, Validation Loss:2.4469, Validation Accuracy:0.1773\n",
    "Epoch #93: Loss:2.4363, Accuracy:0.1762, Validation Loss:2.4477, Validation Accuracy:0.1773\n",
    "Epoch #94: Loss:2.4365, Accuracy:0.1791, Validation Loss:2.4504, Validation Accuracy:0.1806\n",
    "Epoch #95: Loss:2.4361, Accuracy:0.1745, Validation Loss:2.4450, Validation Accuracy:0.1773\n",
    "Epoch #96: Loss:2.4357, Accuracy:0.1782, Validation Loss:2.4491, Validation Accuracy:0.1790\n",
    "Epoch #97: Loss:2.4354, Accuracy:0.1766, Validation Loss:2.4482, Validation Accuracy:0.1773\n",
    "Epoch #98: Loss:2.4345, Accuracy:0.1713, Validation Loss:2.4476, Validation Accuracy:0.1790\n",
    "Epoch #99: Loss:2.4345, Accuracy:0.1786, Validation Loss:2.4486, Validation Accuracy:0.1790\n",
    "Epoch #100: Loss:2.4344, Accuracy:0.1795, Validation Loss:2.4490, Validation Accuracy:0.1806\n",
    "Epoch #101: Loss:2.4341, Accuracy:0.1795, Validation Loss:2.4466, Validation Accuracy:0.1773\n",
    "Epoch #102: Loss:2.4347, Accuracy:0.1778, Validation Loss:2.4485, Validation Accuracy:0.1806\n",
    "Epoch #103: Loss:2.4349, Accuracy:0.1795, Validation Loss:2.4458, Validation Accuracy:0.1790\n",
    "Epoch #104: Loss:2.4352, Accuracy:0.1795, Validation Loss:2.4488, Validation Accuracy:0.1806\n",
    "Epoch #105: Loss:2.4348, Accuracy:0.1782, Validation Loss:2.4510, Validation Accuracy:0.1806\n",
    "Epoch #106: Loss:2.5187, Accuracy:0.1577, Validation Loss:2.4739, Validation Accuracy:0.1724\n",
    "Epoch #107: Loss:2.5176, Accuracy:0.1647, Validation Loss:2.5279, Validation Accuracy:0.1691\n",
    "Epoch #108: Loss:2.4608, Accuracy:0.1713, Validation Loss:2.4744, Validation Accuracy:0.1576\n",
    "Epoch #109: Loss:2.4696, Accuracy:0.1643, Validation Loss:2.4574, Validation Accuracy:0.1839\n",
    "Epoch #110: Loss:2.4448, Accuracy:0.1815, Validation Loss:2.4780, Validation Accuracy:0.1839\n",
    "Epoch #111: Loss:2.4497, Accuracy:0.1725, Validation Loss:2.4637, Validation Accuracy:0.1806\n",
    "Epoch #112: Loss:2.4423, Accuracy:0.1749, Validation Loss:2.4579, Validation Accuracy:0.1757\n",
    "Epoch #113: Loss:2.4430, Accuracy:0.1766, Validation Loss:2.4570, Validation Accuracy:0.1905\n",
    "Epoch #114: Loss:2.4396, Accuracy:0.1791, Validation Loss:2.4576, Validation Accuracy:0.1806\n",
    "Epoch #115: Loss:2.4375, Accuracy:0.1799, Validation Loss:2.4549, Validation Accuracy:0.1823\n",
    "Epoch #116: Loss:2.4366, Accuracy:0.1782, Validation Loss:2.4558, Validation Accuracy:0.1823\n",
    "Epoch #117: Loss:2.4364, Accuracy:0.1774, Validation Loss:2.4552, Validation Accuracy:0.1806\n",
    "Epoch #118: Loss:2.4361, Accuracy:0.1791, Validation Loss:2.4537, Validation Accuracy:0.1790\n",
    "Epoch #119: Loss:2.4358, Accuracy:0.1782, Validation Loss:2.4528, Validation Accuracy:0.1790\n",
    "Epoch #120: Loss:2.4343, Accuracy:0.1803, Validation Loss:2.4503, Validation Accuracy:0.1773\n",
    "Epoch #121: Loss:2.4340, Accuracy:0.1770, Validation Loss:2.4514, Validation Accuracy:0.1806\n",
    "Epoch #122: Loss:2.4338, Accuracy:0.1782, Validation Loss:2.4483, Validation Accuracy:0.1790\n",
    "Epoch #123: Loss:2.4342, Accuracy:0.1791, Validation Loss:2.4507, Validation Accuracy:0.1806\n",
    "Epoch #124: Loss:2.4333, Accuracy:0.1770, Validation Loss:2.4496, Validation Accuracy:0.1806\n",
    "Epoch #125: Loss:2.4337, Accuracy:0.1774, Validation Loss:2.4485, Validation Accuracy:0.1790\n",
    "Epoch #126: Loss:2.4331, Accuracy:0.1786, Validation Loss:2.4514, Validation Accuracy:0.1790\n",
    "Epoch #127: Loss:2.4329, Accuracy:0.1786, Validation Loss:2.4471, Validation Accuracy:0.1790\n",
    "Epoch #128: Loss:2.4326, Accuracy:0.1778, Validation Loss:2.4485, Validation Accuracy:0.1790\n",
    "Epoch #129: Loss:2.4321, Accuracy:0.1791, Validation Loss:2.4500, Validation Accuracy:0.1806\n",
    "Epoch #130: Loss:2.4323, Accuracy:0.1799, Validation Loss:2.4480, Validation Accuracy:0.1790\n",
    "Epoch #131: Loss:2.4324, Accuracy:0.1811, Validation Loss:2.4468, Validation Accuracy:0.1773\n",
    "Epoch #132: Loss:2.4321, Accuracy:0.1795, Validation Loss:2.4489, Validation Accuracy:0.1773\n",
    "Epoch #133: Loss:2.4318, Accuracy:0.1803, Validation Loss:2.4500, Validation Accuracy:0.1773\n",
    "Epoch #134: Loss:2.4326, Accuracy:0.1795, Validation Loss:2.4476, Validation Accuracy:0.1806\n",
    "Epoch #135: Loss:2.4320, Accuracy:0.1786, Validation Loss:2.4499, Validation Accuracy:0.1790\n",
    "Epoch #136: Loss:2.4321, Accuracy:0.1815, Validation Loss:2.4488, Validation Accuracy:0.1773\n",
    "Epoch #137: Loss:2.4326, Accuracy:0.1791, Validation Loss:2.4469, Validation Accuracy:0.1773\n",
    "Epoch #138: Loss:2.4320, Accuracy:0.1799, Validation Loss:2.4517, Validation Accuracy:0.1806\n",
    "Epoch #139: Loss:2.4326, Accuracy:0.1803, Validation Loss:2.4457, Validation Accuracy:0.1790\n",
    "Epoch #140: Loss:2.4322, Accuracy:0.1791, Validation Loss:2.4455, Validation Accuracy:0.1823\n",
    "Epoch #141: Loss:2.4326, Accuracy:0.1766, Validation Loss:2.4502, Validation Accuracy:0.1806\n",
    "Epoch #142: Loss:2.4337, Accuracy:0.1749, Validation Loss:2.4513, Validation Accuracy:0.1790\n",
    "Epoch #143: Loss:2.4343, Accuracy:0.1782, Validation Loss:2.4467, Validation Accuracy:0.1938\n",
    "Epoch #144: Loss:2.4359, Accuracy:0.1795, Validation Loss:2.4503, Validation Accuracy:0.1839\n",
    "Epoch #145: Loss:2.4345, Accuracy:0.1815, Validation Loss:2.4520, Validation Accuracy:0.1856\n",
    "Epoch #146: Loss:2.4338, Accuracy:0.1807, Validation Loss:2.4488, Validation Accuracy:0.1872\n",
    "Epoch #147: Loss:2.4335, Accuracy:0.1807, Validation Loss:2.4500, Validation Accuracy:0.1823\n",
    "Epoch #148: Loss:2.4320, Accuracy:0.1795, Validation Loss:2.4496, Validation Accuracy:0.1823\n",
    "Epoch #149: Loss:2.4319, Accuracy:0.1823, Validation Loss:2.4505, Validation Accuracy:0.1806\n",
    "Epoch #150: Loss:2.4317, Accuracy:0.1803, Validation Loss:2.4519, Validation Accuracy:0.1790\n",
    "Epoch #151: Loss:2.4316, Accuracy:0.1791, Validation Loss:2.4479, Validation Accuracy:0.1790\n",
    "Epoch #152: Loss:2.4319, Accuracy:0.1762, Validation Loss:2.4461, Validation Accuracy:0.1806\n",
    "Epoch #153: Loss:2.4322, Accuracy:0.1782, Validation Loss:2.4445, Validation Accuracy:0.1790\n",
    "Epoch #154: Loss:2.4327, Accuracy:0.1799, Validation Loss:2.4488, Validation Accuracy:0.1790\n",
    "Epoch #155: Loss:2.4307, Accuracy:0.1807, Validation Loss:2.4452, Validation Accuracy:0.1790\n",
    "Epoch #156: Loss:2.4294, Accuracy:0.1795, Validation Loss:2.4421, Validation Accuracy:0.1773\n",
    "Epoch #157: Loss:2.4297, Accuracy:0.1770, Validation Loss:2.4486, Validation Accuracy:0.1790\n",
    "Epoch #158: Loss:2.4301, Accuracy:0.1758, Validation Loss:2.4477, Validation Accuracy:0.1773\n",
    "Epoch #159: Loss:2.4297, Accuracy:0.1713, Validation Loss:2.4785, Validation Accuracy:0.1675\n",
    "Epoch #160: Loss:2.4606, Accuracy:0.1774, Validation Loss:2.4832, Validation Accuracy:0.1888\n",
    "Epoch #161: Loss:2.4431, Accuracy:0.1745, Validation Loss:2.4508, Validation Accuracy:0.1773\n",
    "Epoch #162: Loss:2.4350, Accuracy:0.1803, Validation Loss:2.4537, Validation Accuracy:0.1773\n",
    "Epoch #163: Loss:2.4317, Accuracy:0.1778, Validation Loss:2.4526, Validation Accuracy:0.1806\n",
    "Epoch #164: Loss:2.4293, Accuracy:0.1799, Validation Loss:2.4501, Validation Accuracy:0.1806\n",
    "Epoch #165: Loss:2.4284, Accuracy:0.1799, Validation Loss:2.4513, Validation Accuracy:0.1823\n",
    "Epoch #166: Loss:2.4283, Accuracy:0.1815, Validation Loss:2.4513, Validation Accuracy:0.1823\n",
    "Epoch #167: Loss:2.4279, Accuracy:0.1803, Validation Loss:2.4545, Validation Accuracy:0.1806\n",
    "Epoch #168: Loss:2.4281, Accuracy:0.1828, Validation Loss:2.4493, Validation Accuracy:0.1790\n",
    "Epoch #169: Loss:2.4288, Accuracy:0.1803, Validation Loss:2.4484, Validation Accuracy:0.1790\n",
    "Epoch #170: Loss:2.4320, Accuracy:0.1778, Validation Loss:2.4478, Validation Accuracy:0.1773\n",
    "Epoch #171: Loss:2.4321, Accuracy:0.1762, Validation Loss:2.4448, Validation Accuracy:0.1790\n",
    "Epoch #172: Loss:2.4320, Accuracy:0.1725, Validation Loss:2.4491, Validation Accuracy:0.1823\n",
    "Epoch #173: Loss:2.4285, Accuracy:0.1758, Validation Loss:2.4447, Validation Accuracy:0.1773\n",
    "Epoch #174: Loss:2.4284, Accuracy:0.1778, Validation Loss:2.4519, Validation Accuracy:0.1724\n",
    "Epoch #175: Loss:2.4292, Accuracy:0.1782, Validation Loss:2.4497, Validation Accuracy:0.1724\n",
    "Epoch #176: Loss:2.4289, Accuracy:0.1770, Validation Loss:2.4463, Validation Accuracy:0.1708\n",
    "Epoch #177: Loss:2.4301, Accuracy:0.1770, Validation Loss:2.4503, Validation Accuracy:0.1757\n",
    "Epoch #178: Loss:2.4304, Accuracy:0.1766, Validation Loss:2.4458, Validation Accuracy:0.1757\n",
    "Epoch #179: Loss:2.4305, Accuracy:0.1823, Validation Loss:2.4510, Validation Accuracy:0.1757\n",
    "Epoch #180: Loss:2.4290, Accuracy:0.1778, Validation Loss:2.4452, Validation Accuracy:0.1741\n",
    "Epoch #181: Loss:2.4292, Accuracy:0.1803, Validation Loss:2.4488, Validation Accuracy:0.1773\n",
    "Epoch #182: Loss:2.4300, Accuracy:0.1782, Validation Loss:2.4489, Validation Accuracy:0.1790\n",
    "Epoch #183: Loss:2.4293, Accuracy:0.1799, Validation Loss:2.4477, Validation Accuracy:0.1790\n",
    "Epoch #184: Loss:2.4295, Accuracy:0.1754, Validation Loss:2.4465, Validation Accuracy:0.1741\n",
    "Epoch #185: Loss:2.4293, Accuracy:0.1795, Validation Loss:2.4471, Validation Accuracy:0.1790\n",
    "Epoch #186: Loss:2.4285, Accuracy:0.1799, Validation Loss:2.4451, Validation Accuracy:0.1806\n",
    "Epoch #187: Loss:2.4284, Accuracy:0.1815, Validation Loss:2.4502, Validation Accuracy:0.1806\n",
    "Epoch #188: Loss:2.4265, Accuracy:0.1811, Validation Loss:2.4448, Validation Accuracy:0.1773\n",
    "Epoch #189: Loss:2.4276, Accuracy:0.1770, Validation Loss:2.4469, Validation Accuracy:0.1773\n",
    "Epoch #190: Loss:2.4264, Accuracy:0.1786, Validation Loss:2.4447, Validation Accuracy:0.1757\n",
    "Epoch #191: Loss:2.4256, Accuracy:0.1766, Validation Loss:2.4481, Validation Accuracy:0.1823\n",
    "Epoch #192: Loss:2.4269, Accuracy:0.1762, Validation Loss:2.4441, Validation Accuracy:0.1839\n",
    "Epoch #193: Loss:2.4261, Accuracy:0.1774, Validation Loss:2.4463, Validation Accuracy:0.1806\n",
    "Epoch #194: Loss:2.4245, Accuracy:0.1782, Validation Loss:2.4452, Validation Accuracy:0.1741\n",
    "Epoch #195: Loss:2.4250, Accuracy:0.1729, Validation Loss:2.4457, Validation Accuracy:0.1724\n",
    "Epoch #196: Loss:2.4250, Accuracy:0.1737, Validation Loss:2.4425, Validation Accuracy:0.1823\n",
    "Epoch #197: Loss:2.4253, Accuracy:0.1717, Validation Loss:2.4427, Validation Accuracy:0.1839\n",
    "Epoch #198: Loss:2.4251, Accuracy:0.1815, Validation Loss:2.4448, Validation Accuracy:0.1790\n",
    "Epoch #199: Loss:2.4243, Accuracy:0.1737, Validation Loss:2.4459, Validation Accuracy:0.1839\n",
    "Epoch #200: Loss:2.4234, Accuracy:0.1762, Validation Loss:2.4452, Validation Accuracy:0.1773\n",
    "Epoch #201: Loss:2.4226, Accuracy:0.1803, Validation Loss:2.4463, Validation Accuracy:0.1773\n",
    "Epoch #202: Loss:2.4230, Accuracy:0.1737, Validation Loss:2.4457, Validation Accuracy:0.1757\n",
    "Epoch #203: Loss:2.4230, Accuracy:0.1795, Validation Loss:2.4463, Validation Accuracy:0.1806\n",
    "Epoch #204: Loss:2.4225, Accuracy:0.1774, Validation Loss:2.4471, Validation Accuracy:0.1856\n",
    "Epoch #205: Loss:2.4245, Accuracy:0.1791, Validation Loss:2.4468, Validation Accuracy:0.1806\n",
    "Epoch #206: Loss:2.4250, Accuracy:0.1795, Validation Loss:2.4522, Validation Accuracy:0.1872\n",
    "Epoch #207: Loss:2.4227, Accuracy:0.1832, Validation Loss:2.4449, Validation Accuracy:0.1724\n",
    "Epoch #208: Loss:2.4227, Accuracy:0.1828, Validation Loss:2.4552, Validation Accuracy:0.1905\n",
    "Epoch #209: Loss:2.4247, Accuracy:0.1803, Validation Loss:2.4436, Validation Accuracy:0.1905\n",
    "Epoch #210: Loss:2.4250, Accuracy:0.1819, Validation Loss:2.4466, Validation Accuracy:0.1839\n",
    "Epoch #211: Loss:2.4255, Accuracy:0.1791, Validation Loss:2.4483, Validation Accuracy:0.1823\n",
    "Epoch #212: Loss:3.0422, Accuracy:0.1318, Validation Loss:3.3971, Validation Accuracy:0.1018\n",
    "Epoch #213: Loss:3.3044, Accuracy:0.1023, Validation Loss:3.1350, Validation Accuracy:0.1018\n",
    "Epoch #214: Loss:2.9986, Accuracy:0.0920, Validation Loss:2.8184, Validation Accuracy:0.0887\n",
    "Epoch #215: Loss:2.7505, Accuracy:0.0891, Validation Loss:2.6857, Validation Accuracy:0.0887\n",
    "Epoch #216: Loss:2.6649, Accuracy:0.0903, Validation Loss:2.6538, Validation Accuracy:0.0870\n",
    "Epoch #217: Loss:2.6461, Accuracy:0.1170, Validation Loss:2.6415, Validation Accuracy:0.1511\n",
    "Epoch #218: Loss:2.6341, Accuracy:0.1199, Validation Loss:2.6270, Validation Accuracy:0.1379\n",
    "Epoch #219: Loss:2.6146, Accuracy:0.1326, Validation Loss:2.6016, Validation Accuracy:0.1478\n",
    "Epoch #220: Loss:2.5882, Accuracy:0.1384, Validation Loss:2.5782, Validation Accuracy:0.1494\n",
    "Epoch #221: Loss:2.5668, Accuracy:0.1400, Validation Loss:2.5556, Validation Accuracy:0.1741\n",
    "Epoch #222: Loss:2.5444, Accuracy:0.1762, Validation Loss:2.5390, Validation Accuracy:0.1724\n",
    "Epoch #223: Loss:2.5277, Accuracy:0.1754, Validation Loss:2.5236, Validation Accuracy:0.1741\n",
    "Epoch #224: Loss:2.5120, Accuracy:0.1762, Validation Loss:2.5117, Validation Accuracy:0.1708\n",
    "Epoch #225: Loss:2.5023, Accuracy:0.1741, Validation Loss:2.4998, Validation Accuracy:0.1773\n",
    "Epoch #226: Loss:2.4921, Accuracy:0.1741, Validation Loss:2.4934, Validation Accuracy:0.1790\n",
    "Epoch #227: Loss:2.4841, Accuracy:0.1799, Validation Loss:2.5037, Validation Accuracy:0.1724\n",
    "Epoch #228: Loss:2.5076, Accuracy:0.1618, Validation Loss:2.4959, Validation Accuracy:0.1823\n",
    "Epoch #229: Loss:2.4735, Accuracy:0.1791, Validation Loss:2.4820, Validation Accuracy:0.1757\n",
    "Epoch #230: Loss:2.4842, Accuracy:0.1692, Validation Loss:2.4730, Validation Accuracy:0.1823\n",
    "Epoch #231: Loss:2.4671, Accuracy:0.1762, Validation Loss:2.4862, Validation Accuracy:0.1773\n",
    "Epoch #232: Loss:2.4646, Accuracy:0.1819, Validation Loss:2.4733, Validation Accuracy:0.1675\n",
    "Epoch #233: Loss:2.4624, Accuracy:0.1733, Validation Loss:2.4662, Validation Accuracy:0.1658\n",
    "Epoch #234: Loss:2.4541, Accuracy:0.1766, Validation Loss:2.4731, Validation Accuracy:0.1757\n",
    "Epoch #235: Loss:2.4511, Accuracy:0.1786, Validation Loss:2.4566, Validation Accuracy:0.1823\n",
    "Epoch #236: Loss:2.4468, Accuracy:0.1819, Validation Loss:2.4604, Validation Accuracy:0.1806\n",
    "Epoch #237: Loss:2.4459, Accuracy:0.1782, Validation Loss:2.4656, Validation Accuracy:0.1773\n",
    "Epoch #238: Loss:2.4474, Accuracy:0.1733, Validation Loss:2.4641, Validation Accuracy:0.1790\n",
    "Epoch #239: Loss:2.4489, Accuracy:0.1807, Validation Loss:2.4737, Validation Accuracy:0.1856\n",
    "Epoch #240: Loss:2.4505, Accuracy:0.1725, Validation Loss:2.4697, Validation Accuracy:0.1773\n",
    "Epoch #241: Loss:2.4513, Accuracy:0.1762, Validation Loss:2.4724, Validation Accuracy:0.1773\n",
    "Epoch #242: Loss:2.4536, Accuracy:0.1791, Validation Loss:2.4687, Validation Accuracy:0.1741\n",
    "Epoch #243: Loss:2.4479, Accuracy:0.1774, Validation Loss:2.4740, Validation Accuracy:0.1773\n",
    "Epoch #244: Loss:2.4467, Accuracy:0.1795, Validation Loss:2.4662, Validation Accuracy:0.1773\n",
    "Epoch #245: Loss:2.4455, Accuracy:0.1778, Validation Loss:2.4663, Validation Accuracy:0.1806\n",
    "Epoch #246: Loss:2.4449, Accuracy:0.1770, Validation Loss:2.4653, Validation Accuracy:0.1839\n",
    "Epoch #247: Loss:2.4432, Accuracy:0.1791, Validation Loss:2.4607, Validation Accuracy:0.1839\n",
    "Epoch #248: Loss:2.4428, Accuracy:0.1782, Validation Loss:2.4608, Validation Accuracy:0.1790\n",
    "Epoch #249: Loss:2.4430, Accuracy:0.1791, Validation Loss:2.4544, Validation Accuracy:0.1806\n",
    "Epoch #250: Loss:2.4418, Accuracy:0.1762, Validation Loss:2.4544, Validation Accuracy:0.1888\n",
    "Epoch #251: Loss:2.4417, Accuracy:0.1754, Validation Loss:2.4558, Validation Accuracy:0.1905\n",
    "Epoch #252: Loss:2.4406, Accuracy:0.1786, Validation Loss:2.4572, Validation Accuracy:0.1905\n",
    "Epoch #253: Loss:2.4399, Accuracy:0.1799, Validation Loss:2.4558, Validation Accuracy:0.1905\n",
    "Epoch #254: Loss:2.4378, Accuracy:0.1791, Validation Loss:2.4473, Validation Accuracy:0.1872\n",
    "Epoch #255: Loss:2.4358, Accuracy:0.1758, Validation Loss:2.4467, Validation Accuracy:0.1888\n",
    "Epoch #256: Loss:2.4355, Accuracy:0.1770, Validation Loss:2.4494, Validation Accuracy:0.1856\n",
    "Epoch #257: Loss:2.4359, Accuracy:0.1762, Validation Loss:2.4442, Validation Accuracy:0.1872\n",
    "Epoch #258: Loss:2.4399, Accuracy:0.1770, Validation Loss:2.4712, Validation Accuracy:0.1790\n",
    "Epoch #259: Loss:2.4515, Accuracy:0.1762, Validation Loss:2.4686, Validation Accuracy:0.1823\n",
    "Epoch #260: Loss:2.4492, Accuracy:0.1688, Validation Loss:2.4405, Validation Accuracy:0.1888\n",
    "Epoch #261: Loss:2.4430, Accuracy:0.1684, Validation Loss:2.4412, Validation Accuracy:0.1888\n",
    "Epoch #262: Loss:2.4377, Accuracy:0.1749, Validation Loss:2.4452, Validation Accuracy:0.1823\n",
    "Epoch #263: Loss:2.4400, Accuracy:0.1770, Validation Loss:2.4440, Validation Accuracy:0.1823\n",
    "Epoch #264: Loss:2.4383, Accuracy:0.1758, Validation Loss:2.4454, Validation Accuracy:0.1823\n",
    "Epoch #265: Loss:2.4392, Accuracy:0.1791, Validation Loss:2.4542, Validation Accuracy:0.1888\n",
    "Epoch #266: Loss:2.4379, Accuracy:0.1745, Validation Loss:2.4503, Validation Accuracy:0.1856\n",
    "Epoch #267: Loss:2.4357, Accuracy:0.1708, Validation Loss:2.4490, Validation Accuracy:0.1888\n",
    "Epoch #268: Loss:2.4378, Accuracy:0.1733, Validation Loss:2.4580, Validation Accuracy:0.1724\n",
    "Epoch #269: Loss:2.4371, Accuracy:0.1766, Validation Loss:2.4493, Validation Accuracy:0.1823\n",
    "Epoch #270: Loss:2.4387, Accuracy:0.1782, Validation Loss:2.4521, Validation Accuracy:0.1856\n",
    "Epoch #271: Loss:2.4364, Accuracy:0.1733, Validation Loss:2.4464, Validation Accuracy:0.1856\n",
    "Epoch #272: Loss:2.4348, Accuracy:0.1774, Validation Loss:2.4482, Validation Accuracy:0.1806\n",
    "Epoch #273: Loss:2.4344, Accuracy:0.1819, Validation Loss:2.4435, Validation Accuracy:0.1856\n",
    "Epoch #274: Loss:2.4479, Accuracy:0.1762, Validation Loss:2.4546, Validation Accuracy:0.1642\n",
    "Epoch #275: Loss:2.4502, Accuracy:0.1696, Validation Loss:2.4614, Validation Accuracy:0.1888\n",
    "Epoch #276: Loss:2.4382, Accuracy:0.1778, Validation Loss:2.4535, Validation Accuracy:0.1856\n",
    "Epoch #277: Loss:2.4387, Accuracy:0.1786, Validation Loss:2.4613, Validation Accuracy:0.1823\n",
    "Epoch #278: Loss:2.4360, Accuracy:0.1799, Validation Loss:2.4495, Validation Accuracy:0.1839\n",
    "Epoch #279: Loss:2.4367, Accuracy:0.1770, Validation Loss:2.4445, Validation Accuracy:0.1888\n",
    "Epoch #280: Loss:2.4389, Accuracy:0.1754, Validation Loss:2.4511, Validation Accuracy:0.1970\n",
    "Epoch #281: Loss:2.4396, Accuracy:0.1741, Validation Loss:2.4492, Validation Accuracy:0.1856\n",
    "Epoch #282: Loss:2.4362, Accuracy:0.1795, Validation Loss:2.4609, Validation Accuracy:0.1839\n",
    "Epoch #283: Loss:2.4366, Accuracy:0.1778, Validation Loss:2.4484, Validation Accuracy:0.1839\n",
    "Epoch #284: Loss:2.4373, Accuracy:0.1766, Validation Loss:2.4504, Validation Accuracy:0.1806\n",
    "Epoch #285: Loss:2.4372, Accuracy:0.1749, Validation Loss:2.4533, Validation Accuracy:0.1872\n",
    "Epoch #286: Loss:2.4360, Accuracy:0.1754, Validation Loss:2.4457, Validation Accuracy:0.1856\n",
    "Epoch #287: Loss:2.4358, Accuracy:0.1749, Validation Loss:2.4490, Validation Accuracy:0.1806\n",
    "Epoch #288: Loss:2.4347, Accuracy:0.1807, Validation Loss:2.4506, Validation Accuracy:0.1790\n",
    "Epoch #289: Loss:2.4351, Accuracy:0.1807, Validation Loss:2.4489, Validation Accuracy:0.1773\n",
    "Epoch #290: Loss:2.4359, Accuracy:0.1786, Validation Loss:2.4486, Validation Accuracy:0.1773\n",
    "Epoch #291: Loss:2.4345, Accuracy:0.1807, Validation Loss:2.4496, Validation Accuracy:0.1806\n",
    "Epoch #292: Loss:2.4345, Accuracy:0.1795, Validation Loss:2.4483, Validation Accuracy:0.1823\n",
    "Epoch #293: Loss:2.4348, Accuracy:0.1807, Validation Loss:2.4473, Validation Accuracy:0.1806\n",
    "Epoch #294: Loss:2.4348, Accuracy:0.1819, Validation Loss:2.4470, Validation Accuracy:0.1823\n",
    "Epoch #295: Loss:2.4337, Accuracy:0.1799, Validation Loss:2.4481, Validation Accuracy:0.1806\n",
    "Epoch #296: Loss:2.4339, Accuracy:0.1791, Validation Loss:2.4483, Validation Accuracy:0.1806\n",
    "Epoch #297: Loss:2.4341, Accuracy:0.1791, Validation Loss:2.4485, Validation Accuracy:0.1806\n",
    "Epoch #298: Loss:2.4323, Accuracy:0.1791, Validation Loss:2.4506, Validation Accuracy:0.1806\n",
    "Epoch #299: Loss:2.4332, Accuracy:0.1803, Validation Loss:2.4504, Validation Accuracy:0.1773\n",
    "Epoch #300: Loss:2.4345, Accuracy:0.1811, Validation Loss:2.4462, Validation Accuracy:0.1773\n",
    "\n",
    "Test:\n",
    "Test Loss:2.44623184, Accuracy:0.1773\n",
    "Labels: ['my', 'ek', 'by', 'eb', 'sg', 'ck', 'eo', 'yd', 'eg', 'sk', 'mb', 'ds', 'ib', 'aa', 'ce']\n",
    "Confusion Matrix:\n",
    "      my  ek  by  eb  sg  ck  eo  yd  eg  sk  mb  ds  ib  aa  ce\n",
    "t:my   0   0   2   1   4   0   0   5   4   0   0   4   0   0   0\n",
    "t:ek   0   0   0   4  16   0   0   5  22   0   0   1   0   0   0\n",
    "t:by   0   0   0   8  18   0   0   1  13   0   0   0   0   0   0\n",
    "t:eb   0   0   1   8  11   0   0   6  23   0   0   0   1   0   0\n",
    "t:sg   0   0   0   4  26   0   0  16   4   0   0   0   1   0   0\n",
    "t:ck   0   0   0   5   9   0   0   0   8   0   0   1   0   0   0\n",
    "t:eo   0   0   0   2  22   0   0   6   4   0   0   0   0   0   0\n",
    "t:yd   0   0   1   0  18   0   0  36   5   0   0   0   2   0   0\n",
    "t:eg   0   0   1   3  10   0   0   0  32   0   0   4   0   0   0\n",
    "t:sk   0   0   0   4   7   0   0   4  12   0   0   6   0   0   0\n",
    "t:mb   0   0   0   2  23   0   0  10  12   0   0   4   1   0   0\n",
    "t:ds   0   0   0   3  11   0   0   2   9   0   0   6   0   0   0\n",
    "t:ib   0   0   0   0  22   0   0  27   5   0   0   0   0   0   0\n",
    "t:aa   0   0   0   2   3   0   0   3  15   0   0  10   1   0   0\n",
    "t:ce   0   0   0   2   9   0   0   3  12   0   0   1   0   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          my       0.00      0.00      0.00        20\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          by       0.00      0.00      0.00        40\n",
    "          eb       0.17      0.16      0.16        50\n",
    "          sg       0.12      0.51      0.20        51\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          eo       0.00      0.00      0.00        34\n",
    "          yd       0.29      0.58      0.39        62\n",
    "          eg       0.18      0.64      0.28        50\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          ds       0.16      0.19      0.18        31\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ce       0.00      0.00      0.00        27\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.06      0.14      0.08       609\n",
    "weighted avg       0.08      0.18      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 11:36:21 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 59 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7186460029119734, 2.708066737123311, 2.699203177234418, 2.6918169700453434, 2.6854773542563906, 2.679479225515732, 2.673289929313221, 2.668324827952142, 2.662566250376709, 2.6563830042903254, 2.6495282133224562, 2.6429917009788975, 2.633382808594477, 2.6212517913730666, 2.6061956792433665, 2.588318874096048, 2.5684700779531195, 2.5507937546434074, 2.534223793371166, 2.522558175871525, 2.5151665175489604, 2.5030161418351047, 2.5021138304755803, 2.494872457679661, 2.493871460602984, 2.4862086017339298, 2.4854242723367874, 2.483249496356607, 2.485530846616122, 2.4818850530583676, 2.4804357362693947, 2.4800937062218074, 2.4830580474120643, 2.4777026199942154, 2.4810151278679005, 2.4779775357990235, 2.47550643410393, 2.473943933086051, 2.472998476185039, 2.4709045139243844, 2.4684293794710257, 2.4652509704990733, 2.46761559735378, 2.4671995009499033, 2.4656735198642625, 2.4634149638302807, 2.46412983707998, 2.4603233834597082, 2.4645233772854107, 2.4593542240719097, 2.4593879276112776, 2.4628260855995765, 2.4576522203893303, 2.455974477069523, 2.454732144407451, 2.457683303281787, 2.46119666843383, 2.4599398333450844, 2.460222729321184, 2.461559538770779, 2.456810557392039, 2.4556898925887736, 2.4513403797776046, 2.451924268835284, 2.4553888670133643, 2.4507464101944847, 2.4521560422305404, 2.4499332231449573, 2.4507753449707783, 2.4544895078944062, 2.449853246435156, 2.4567803490925306, 2.451483594177196, 2.45346250126906, 2.454197791213864, 2.4533270157029476, 2.451848171810407, 2.4521779133181267, 2.451633858171786, 2.4519171515121836, 2.4519685896355137, 2.456077750680482, 2.4526183405533213, 2.4562596147283546, 2.4493495067352145, 2.449324063675353, 2.448473080429929, 2.447512646614037, 2.45090018508861, 2.447129610919796, 2.450513542187821, 2.446930816020872, 2.4477192208488976, 2.4504148960113525, 2.444966143183716, 2.4491490781405094, 2.4482295321322036, 2.4476168398395157, 2.448618155199123, 2.449026635127702, 2.4466492687344354, 2.44854291749901, 2.4458467424013737, 2.4488217353037816, 2.4509760828441, 2.473896373473169, 2.5279478008915444, 2.4743501721148813, 2.4573665514759635, 2.4779580892208957, 2.4637499712958126, 2.4578896105191586, 2.4569981755881476, 2.457554130131388, 2.454914437921959, 2.455844624289151, 2.455221173015526, 2.4537225791386197, 2.452754972407775, 2.450317045737957, 2.4513888261196843, 2.4482616062822014, 2.450738348984366, 2.4495761660715236, 2.4484937515947816, 2.4513709705647186, 2.447097093209453, 2.4485151078704934, 2.4499858322206194, 2.4480117361729565, 2.446782576822491, 2.4489260612450208, 2.4499813163613253, 2.4476484257990894, 2.4498704603348656, 2.44879096951978, 2.446932272370813, 2.4517368158487653, 2.4457193763776757, 2.4455056476279826, 2.450242284875002, 2.451348362297847, 2.4466688002663095, 2.450281835346191, 2.452040563858984, 2.448792500644678, 2.450038725910907, 2.4496039165847603, 2.4505337862349887, 2.4519181658677476, 2.447877376146113, 2.446127254974666, 2.4444826960759407, 2.4488413040273884, 2.445239829703896, 2.442081848780314, 2.4486339072680043, 2.447652236189944, 2.4784948724048284, 2.4831833205199594, 2.4507977872450755, 2.4537428877819543, 2.4525781229799017, 2.4501112016355266, 2.451308868984479, 2.4513340865449953, 2.4545316837104085, 2.4493341328475275, 2.448426785335948, 2.447800621414811, 2.444753221690361, 2.4490740729865967, 2.444729721604897, 2.4518848444244936, 2.4497173995220014, 2.446306592725181, 2.4503079549040896, 2.4458452210637738, 2.45095110918305, 2.4451905945055983, 2.4488220962593314, 2.4488938704304313, 2.4476963295333687, 2.4465359697983966, 2.447113173740055, 2.445063645420795, 2.450220958352676, 2.4447970922748836, 2.4469109799082838, 2.4447062027278204, 2.448136033682988, 2.4441217514877445, 2.4462594390894195, 2.4452305051493526, 2.4457159809682563, 2.4425201756613597, 2.442691826468031, 2.444804807797637, 2.445906225292162, 2.445217037435823, 2.446251440518008, 2.4457450253622874, 2.4463431944792298, 2.4471463507227904, 2.4468266462849084, 2.452165239941702, 2.444926585665673, 2.4552166735988923, 2.443628862769341, 2.446615291542216, 2.4483267102139727, 3.3970501148837737, 3.1349737299682667, 2.818431888307844, 2.6856675539502173, 2.653841968827647, 2.6415135210566527, 2.626983218200884, 2.6015501758343675, 2.5781611961684203, 2.5556433286964406, 2.5390095667690282, 2.5235912373109013, 2.511719263637399, 2.499804228984664, 2.4933617647449764, 2.5037109918390783, 2.4959311066394174, 2.482041874934104, 2.4730115668918504, 2.486163917237706, 2.473319611134396, 2.4661792405133176, 2.473120344487709, 2.4565920951135443, 2.4604332274795557, 2.4656187568000583, 2.464116715836799, 2.4736683400002213, 2.469675034333528, 2.472356877694968, 2.4686640225020535, 2.4740489930746397, 2.4661967120147104, 2.46630146624811, 2.465301781060856, 2.460734491473544, 2.460849340717585, 2.4544313619485236, 2.454396875033825, 2.455818026328126, 2.4571885287468067, 2.4558466400810453, 2.447253202569896, 2.4467042373318977, 2.4493624244025973, 2.444195051302855, 2.471210991416267, 2.4685635668499324, 2.4405217151140737, 2.4412292745117288, 2.445241279789967, 2.4439650419701886, 2.445385856189947, 2.4542193596781963, 2.450342341988349, 2.448983522080044, 2.458016715417746, 2.4492981206803095, 2.452122074042635, 2.446441806204409, 2.448190500387809, 2.443523568277093, 2.4545709538733824, 2.4614173097563494, 2.4535486071763564, 2.461348503094001, 2.449499020239794, 2.444526839530331, 2.451060822836088, 2.4491787900282636, 2.4608511196568683, 2.4484488173267134, 2.450414978224656, 2.453283925753313, 2.4456988845161227, 2.4489694573413368, 2.450611308681945, 2.4488728152120056, 2.448565867538327, 2.449604439617965, 2.448283321164512, 2.447286888100635, 2.4469907362081345, 2.4480671706458033, 2.4482997978849363, 2.4484605272415236, 2.4505597758175703, 2.4503635835569284, 2.446232045225322], 'val_acc': [0.04761904751505758, 0.07060755286457504, 0.10180623943142116, 0.10180623933354818, 0.1034482755562159, 0.09523809473649622, 0.0968801311549099, 0.09359605870957445, 0.09852216727970464, 0.1050903118767566, 0.13793103437265153, 0.1215106729289581, 0.12972085365080482, 0.13464696202518905, 0.1280788173302641, 0.1280788173302641, 0.154351395522726, 0.14449917877395752, 0.15435139542485302, 0.15927750409285618, 0.14614121489875226, 0.15270935949580422, 0.14942528724621473, 0.1625615756451007, 0.1691297200464067, 0.1691297199485337, 0.17077175607332848, 0.17241379219812322, 0.17241379219812322, 0.17569786454558567, 0.17898193679517518, 0.1789819366973022, 0.17569786425196673, 0.1789819365994292, 0.18390804497381344, 0.1789819365994292, 0.18062397272422395, 0.1789819365994292, 0.1822660088490187, 0.18062397272422395, 0.1789819365994292, 0.17405582822504498, 0.1789819365994292, 0.18062397282209694, 0.1789819365994292, 0.17733990047463447, 0.17569786434983972, 0.17405582822504498, 0.17569786434983972, 0.1789819366973022, 0.17733990057250745, 0.18062397272422395, 0.17733990057250745, 0.17733990057250745, 0.17733990057250745, 0.17733990057250745, 0.18062397272422395, 0.17733990047463447, 0.17733990047463447, 0.18390804516955941, 0.17733990057250745, 0.17569786425196673, 0.1756978644477127, 0.1756978644477127, 0.1789819365994292, 0.17405582822504498, 0.17569786434983972, 0.1789819365994292, 0.19047618947299244, 0.18062397282209694, 0.17733990047463447, 0.18390804516955941, 0.18062397272422395, 0.1789819365994292, 0.1822660088490187, 0.18555008119648117, 0.18555008119648117, 0.18390804507168643, 0.18226600894689168, 0.18555008129435416, 0.18062397282209694, 0.18226600894689168, 0.1789819366973022, 0.18226600894689168, 0.18062397282209694, 0.17898193650155622, 0.18062397272422395, 0.1789819366973022, 0.17898193650155622, 0.1789819366973022, 0.17898193650155622, 0.17733990037676148, 0.17733990047463447, 0.18062397291996993, 0.17733990047463447, 0.17898193650155622, 0.17733990047463447, 0.1789819365994292, 0.17898193650155622, 0.180623972626351, 0.17733990047463447, 0.180623972626351, 0.17898193650155622, 0.180623972626351, 0.180623972626351, 0.17241379210025023, 0.16912972084162467, 0.15763546697709752, 0.18390804497381344, 0.18390804546317835, 0.18062397272422395, 0.1756978644477127, 0.19047618937511945, 0.180623972626351, 0.1822660088490187, 0.1822660088490187, 0.18062397282209694, 0.1789819366973022, 0.1789819366973022, 0.17733990067038044, 0.18062397282209694, 0.1789819366973022, 0.18062397282209694, 0.18062397282209694, 0.1789819365994292, 0.1789819365994292, 0.1789819365994292, 0.1789819365994292, 0.18062397272422395, 0.17898193650155622, 0.17733990037676148, 0.17733990037676148, 0.17733990037676148, 0.18062397272422395, 0.17898193650155622, 0.17733990047463447, 0.17733990047463447, 0.180623972626351, 0.1789819365994292, 0.18226600894689168, 0.180623972626351, 0.17898193650155622, 0.19376026162470894, 0.18390804507168643, 0.18555008119648117, 0.18719211712552997, 0.18226600983998262, 0.18226600983998262, 0.18062397272422395, 0.1789819365994292, 0.17898193759039313, 0.18062397272422395, 0.1789819365994292, 0.1789819365994292, 0.17898193650155622, 0.17733990037676148, 0.1789819365994292, 0.17733990126985244, 0.167487683725866, 0.1888341533481977, 0.17733990047463447, 0.1773399014655984, 0.18062397272422395, 0.180623972626351, 0.1822660088490187, 0.1822660088490187, 0.180623972626351, 0.17898193650155622, 0.17898193650155622, 0.17733990037676148, 0.17898193650155622, 0.18226600875114574, 0.17733990047463447, 0.17241379299334117, 0.17241379299334117, 0.1707717558775825, 0.17569786534080364, 0.17569786534080364, 0.17569786534080364, 0.174055828127172, 0.1773399014655984, 0.17898193650155622, 0.17898193650155622, 0.17405582822504498, 0.1789819366973022, 0.180623972626351, 0.180623972626351, 0.17733990037676148, 0.17733990047463447, 0.17569786534080364, 0.18226600894689168, 0.18390804487594048, 0.180623972626351, 0.174055828127172, 0.17241379200237725, 0.18226600904476467, 0.18390804497381344, 0.17898193759039313, 0.18390804507168643, 0.17733990037676148, 0.17733990057250745, 0.17569786534080364, 0.18062397291996993, 0.18555008139222715, 0.18062397282209694, 0.18719211761489485, 0.17241379309121416, 0.19047618976661138, 0.19047618986448436, 0.18390804507168643, 0.18226600914263763, 0.10180623973115716, 0.10180623973115716, 0.08866995063492622, 0.08866995063492622, 0.08702791460800445, 0.15106732238004555, 0.1379310344705245, 0.14778325121929298, 0.1494252862552508, 0.17405582822504498, 0.17241379200237725, 0.174055828127172, 0.1707717558775825, 0.17733990047463447, 0.17898193650155622, 0.17241379200237725, 0.18226600875114574, 0.1756978644477127, 0.18226600894689168, 0.17733990067038044, 0.16748768362799302, 0.16584564750319827, 0.17569786434983972, 0.18226600904476467, 0.18062397291996993, 0.17733990067038044, 0.1789819366973022, 0.18555008139222715, 0.17733990037676148, 0.17733990047463447, 0.17405582832291797, 0.1773399008661264, 0.17733990047463447, 0.18062397291996993, 0.18390804516955941, 0.18390804516955941, 0.1789819366973022, 0.18062397291996993, 0.18883415373968962, 0.19047618986448436, 0.19047618986448436, 0.19047618986448436, 0.1871921175170219, 0.18883415364181663, 0.18555008119648117, 0.1871921175170219, 0.1789819365994292, 0.18226600875114574, 0.18883415354394364, 0.18883415354394364, 0.1822660088490187, 0.18226600875114574, 0.18226600875114574, 0.18883415364181663, 0.18555008129435416, 0.18883415364181663, 0.17241379229599618, 0.18226600894689168, 0.18555008139222715, 0.18555008119648117, 0.18062397291996993, 0.18555008119648117, 0.16420361137840353, 0.18883415393543557, 0.18555008129435416, 0.18226600914263763, 0.18390804497381344, 0.18883415364181663, 0.19704433436366334, 0.18555008109860818, 0.18390804507168643, 0.18390804497381344, 0.18062397272422395, 0.1871921175170219, 0.18555008100073522, 0.18062397272422395, 0.1789819365994292, 0.17733990047463447, 0.17733990047463447, 0.18062397272422395, 0.1822660088490187, 0.18062397272422395, 0.1822660088490187, 0.18062397272422395, 0.18062397272422395, 0.18062397272422395, 0.18062397272422395, 0.17733990047463447, 0.17733990047463447], 'loss': [2.7307356938199585, 2.711759060657979, 2.703015127906564, 2.6951580489195837, 2.688905139723353, 2.6827920600374133, 2.677051990379788, 2.6710682204127068, 2.665632300700006, 2.659931622883133, 2.652763654074385, 2.6450156299974883, 2.636679864664097, 2.625412094323787, 2.610762136623845, 2.59307245963408, 2.574482793142174, 2.5559106769013455, 2.5431312887086026, 2.524973449667866, 2.514006143820604, 2.506544914970163, 2.4992864245506774, 2.491084183804553, 2.4892666512201456, 2.4842872584380165, 2.4794507842289106, 2.4769842443524936, 2.4757935027567024, 2.4733207301192706, 2.471305097250968, 2.469966072615167, 2.4667676369512352, 2.465910453130577, 2.464872418585744, 2.464455635102133, 2.46313052911778, 2.4612791927932958, 2.4595170425438537, 2.4585025950868515, 2.4554218961962437, 2.454514826690392, 2.4549494039351445, 2.4537784542146404, 2.4517188001707106, 2.451385703860367, 2.4512504911520643, 2.4518699961276513, 2.4519900289649104, 2.4500970492862333, 2.4492669533410356, 2.4458671054800925, 2.4463911209263105, 2.4447362080247004, 2.444449664630929, 2.4431971398222374, 2.4428183387192366, 2.44686704367338, 2.447140953526115, 2.4445636901033, 2.441520990336455, 2.441406561461807, 2.4406701568705347, 2.441126129025062, 2.4405151872419477, 2.4415368763573118, 2.444259320441213, 2.4452189373039857, 2.4441321426838085, 2.442514380292481, 2.4410009040480034, 2.4414887779791985, 2.4412233793270417, 2.439812954740113, 2.439337944054261, 2.439323660284587, 2.4394844485993747, 2.4396243627066485, 2.4396109266692365, 2.439173833839213, 2.4396932885876916, 2.438029926954109, 2.4385687230793605, 2.437484967977849, 2.43796348992804, 2.4365129668609806, 2.436230835239011, 2.4360362488631107, 2.4363037253307365, 2.437183692078326, 2.4352203770584637, 2.4360865892570858, 2.436257856729339, 2.4365296557698652, 2.436106384559334, 2.435726407910764, 2.4354306004620185, 2.4344765310659544, 2.4344537599865173, 2.4343704978543386, 2.434070510100535, 2.4347216149864743, 2.4348572555753485, 2.4351507622603275, 2.4348171771674187, 2.5186735903213155, 2.5176357560089238, 2.4607662441794145, 2.469564728570425, 2.4448320423063556, 2.4496617748017675, 2.4422518656728696, 2.443027759626416, 2.4395814203383743, 2.4375475161863793, 2.4366444985235005, 2.436414159347879, 2.4361307658209204, 2.435824732418178, 2.4343371193511776, 2.434021286504225, 2.4337734929345225, 2.434159096065733, 2.4332645417238896, 2.4337318104150603, 2.433069055623832, 2.4329325772898396, 2.432563789962988, 2.432120498249908, 2.432281949534798, 2.4323837092280143, 2.4320582369269776, 2.431833135616608, 2.432598745945298, 2.4319835655987876, 2.432142551429952, 2.4325637562808557, 2.432004723264941, 2.4325808866802427, 2.432153262837467, 2.432567415883654, 2.433742744477133, 2.434288566264284, 2.435888179469647, 2.4345094215698557, 2.433774346541575, 2.4335424525047475, 2.431950796898875, 2.431878187720046, 2.4316522393628066, 2.431626670316504, 2.43187530613533, 2.4322097104677676, 2.432664852024838, 2.4307286005000557, 2.429405337927033, 2.4297002020802587, 2.4300826232291346, 2.4296624592931853, 2.4606117057604466, 2.443053687034936, 2.434977702585334, 2.431653015912191, 2.429295107079727, 2.4283767921479087, 2.4283431221572283, 2.4278887807466165, 2.428073184544056, 2.4288487044692775, 2.4320036014737045, 2.43208104394055, 2.432016826801966, 2.428450823319766, 2.4283921454231843, 2.429215371192603, 2.428930755709231, 2.430101613489265, 2.4304136894077244, 2.4304685356680618, 2.428999154719484, 2.429197160419253, 2.430007120813922, 2.4292592190619122, 2.429547528417693, 2.4292710518690104, 2.4285315579212665, 2.428447587485186, 2.4265219540566636, 2.427598625433763, 2.4263939207339433, 2.4255875573755534, 2.42687651185774, 2.426085957952104, 2.4244558469470765, 2.4250029883100757, 2.4250214972290416, 2.4253209912067075, 2.4250816717284907, 2.424304056363429, 2.423391289485798, 2.4226056550317723, 2.422994953895741, 2.4229751301986724, 2.4224856436374984, 2.4245231354261083, 2.4250427717056118, 2.4227302014705336, 2.42265443126279, 2.4247336537441435, 2.425045155303924, 2.425462732765464, 3.0421838316829297, 3.304427015903794, 2.9986247839134577, 2.750495486093008, 2.66489829527524, 2.646072812344749, 2.6340913748104713, 2.6146228868613743, 2.588212600723674, 2.5668210619039358, 2.5444393135438954, 2.527674901656791, 2.511976362498634, 2.5022812558395415, 2.4921477834791617, 2.4840873813237496, 2.507584142978676, 2.4735318447285364, 2.484152820027095, 2.467112090014824, 2.464575128437802, 2.462443668494724, 2.454136216518081, 2.4510619692244324, 2.446753875332936, 2.445873311314984, 2.447369798448786, 2.4488648543857208, 2.4505468393008565, 2.4513354725416683, 2.453649830279654, 2.44794685032823, 2.4467070282851893, 2.445471879540038, 2.4449127888532636, 2.4431948939877612, 2.4427884776489446, 2.443046674885055, 2.441771183072664, 2.4416672919075593, 2.4406045422172156, 2.4398592857854324, 2.4378357952869893, 2.4357770020956866, 2.435545590966634, 2.435912996542772, 2.439860656276131, 2.4515379437675713, 2.4491767714890122, 2.442950764231124, 2.4376869160536625, 2.4400291932436966, 2.438313717616902, 2.4391578489260506, 2.437931093689842, 2.435678798315217, 2.437780152307154, 2.437105609063495, 2.438706860160436, 2.436364936143221, 2.4347852641307353, 2.434356044107394, 2.4479432258762617, 2.4501675584233027, 2.4382068516537396, 2.4386891220139773, 2.436000924335613, 2.4366860670720283, 2.438887272918983, 2.4396323035629868, 2.4361884900431856, 2.436593536772522, 2.437287482392861, 2.4371754402497467, 2.4359706858100343, 2.43575877663536, 2.434732395618603, 2.435056023529178, 2.4358705881929494, 2.4344821127043614, 2.434539013476832, 2.4347896749234055, 2.434791728358494, 2.4336695426298607, 2.4339119459814116, 2.43409076246148, 2.4322741767953797, 2.433240508347811, 2.434499528716477], 'acc': [0.049281313945013394, 0.054620122961562274, 0.098973306291402, 0.1100616013826286, 0.1018480488162266, 0.10513347035744351, 0.10718685884862465, 0.09650923979049836, 0.09281314221618112, 0.09527720792460001, 0.12566735066427587, 0.14989733050368895, 0.13963039063942262, 0.147843943215004, 0.15030800851341145, 0.1503080091008905, 0.15482546092303626, 0.15071868613148126, 0.15359342965379633, 0.15605749477473618, 0.16344969286076586, 0.15893223751374583, 0.1577002050511891, 0.16427104615700072, 0.1646817239525382, 0.1601642719529248, 0.17166324502633581, 0.1708418886335968, 0.17166324504469455, 0.17166324345972503, 0.17289527829055668, 0.17207392146944755, 0.17412730916814392, 0.17412730897231757, 0.17453798758787786, 0.17412731014727567, 0.17659137685318502, 0.17577002061955493, 0.17535934300148512, 0.17453798678621374, 0.17618069786433077, 0.1761806984518098, 0.17535934280565876, 0.17412730936397028, 0.17659137585569457, 0.1733059543420157, 0.17453798797953055, 0.17412730897231757, 0.1745379869820401, 0.17577002022790222, 0.17125256664331934, 0.17700205407960215, 0.1761806992351152, 0.1741273097739817, 0.17412731055728708, 0.17453798874447723, 0.1778234085324364, 0.17700205427542848, 0.17535934301984385, 0.17535934200399467, 0.17535934280565876, 0.1745379869820401, 0.17494866501012132, 0.17453798815699817, 0.1737166325475646, 0.17494866557924166, 0.17207392187945897, 0.17453798837118326, 0.17494866598925307, 0.17412730897231757, 0.17577002102956635, 0.17453798796117184, 0.17207392227111165, 0.17453798876283594, 0.17453798797953055, 0.17659137505403044, 0.1761806988251038, 0.17618069884346252, 0.1753593420407121, 0.1778234091015567, 0.17330595473366842, 0.17577001983624954, 0.17618069823762475, 0.1765913768348263, 0.18193018449894946, 0.17782340890573037, 0.18110882906698347, 0.17782340851407766, 0.177823408709904, 0.17248460028083418, 0.17987679623113276, 0.18028747502416068, 0.17618069902093014, 0.17905544115410205, 0.17453798796117184, 0.17823408515301573, 0.1765913768348263, 0.17125256642913428, 0.17864476335856458, 0.1794661197696623, 0.1794661181663341, 0.1778234094932094, 0.17946611918218328, 0.17946611918218328, 0.17823408613214747, 0.15770020581613575, 0.1646817259475191, 0.17125256644749298, 0.16427104613864202, 0.18151950688087964, 0.17248460006664912, 0.17494866557924166, 0.17659137546404186, 0.17905544236577756, 0.17987679779774354, 0.17823408613214747, 0.1774127318751396, 0.1790554409582757, 0.17823408732546428, 0.180287475219987, 0.17700205366959074, 0.1782340869338116, 0.1790554417599398, 0.17700205308211167, 0.1774127318751396, 0.1786447649435341, 0.17864476314437952, 0.17782340773077226, 0.17905544115410205, 0.1798767968186118, 0.18110882967282124, 0.1794661197696623, 0.1802874732250061, 0.17946611861306294, 0.17864476474770774, 0.18151950607921552, 0.1790554417599398, 0.17987679701443815, 0.18028747461414923, 0.17905544199248358, 0.17659137487656282, 0.17494866559760036, 0.1782340857404948, 0.17946612017967373, 0.18151950727253235, 0.18069815105726098, 0.18069815244640414, 0.17946611822141026, 0.1823408625270307, 0.1802874740266702, 0.17905544236577756, 0.17618069902093014, 0.17823408593632112, 0.17987679640860038, 0.18069815184056637, 0.17946611838051915, 0.17700205366959074, 0.17577001965878192, 0.17125256742662473, 0.1774127299168761, 0.17453798796117184, 0.1802874744183229, 0.17782340872826272, 0.1798767968186118, 0.1798767960169477, 0.18151950803747902, 0.18028747342083243, 0.18275153994927415, 0.1802874748099756, 0.17782340755330464, 0.17618069745431936, 0.17248459946081135, 0.17577002120703397, 0.17782340911991543, 0.17823408533048335, 0.17700205368794944, 0.17700205407960215, 0.17659137585569457, 0.1823408617253666, 0.17782340970739446, 0.18028747381248514, 0.17823408691545287, 0.17987679621277403, 0.17535934241400608, 0.17946612017967373, 0.17987679582112134, 0.18151950647086823, 0.18110882985028887, 0.17700205366959074, 0.1786447645518814, 0.17659137526821553, 0.1761806984518098, 0.17741273130601926, 0.17823408593632112, 0.17289527752561, 0.17371663333086998, 0.17166324424303042, 0.18151950670341202, 0.17371663174590046, 0.1761806980417984, 0.18028747541581336, 0.17371663194172682, 0.17946611978802104, 0.17741273050435516, 0.1790554425432452, 0.17946611798886647, 0.18316221655149478, 0.18275153993091545, 0.1802874732433648, 0.18193018412565548, 0.17905544038915536, 0.13182751554116087, 0.10225872682594911, 0.09199178698004148, 0.08911704365355279, 0.09034907592028318, 0.11704312007897198, 0.11991786441213052, 0.13264887075894177, 0.1383983562186024, 0.14004106726000196, 0.1761806976501457, 0.1753593428240175, 0.17618069747267806, 0.17412730996980805, 0.17412730897231757, 0.179876795625295, 0.16180698221101897, 0.17905544119081948, 0.16919917814295884, 0.1761806976501457, 0.18193018489060217, 0.17330595473366842, 0.17659137468073646, 0.17864476396440235, 0.18193018449894946, 0.17823408712963792, 0.1733059559086265, 0.18069815283805682, 0.17248459910587607, 0.17618069784597204, 0.17905544216995123, 0.17741273089600784, 0.17946611918218328, 0.17782340952992684, 0.17700205229880628, 0.17905544236577756, 0.17823408712963792, 0.17905544117246075, 0.1761806992351152, 0.17535934243236478, 0.1786447649251754, 0.17987679701443815, 0.1790554409766344, 0.17577002163540412, 0.17700205249463263, 0.1761806976501457, 0.17700205249463263, 0.1761806992351152, 0.16878850072071538, 0.16837782351265698, 0.17494866520594768, 0.17700205407960215, 0.17577001944459683, 0.17905544138664584, 0.17453798797953055, 0.1708418886152381, 0.17330595412783065, 0.17659137646153233, 0.1782340865421589, 0.17330595373617794, 0.1774127315018456, 0.18193018490896087, 0.1761806992351152, 0.1696098555652023, 0.1778234082998926, 0.17864476335856458, 0.17987679639024168, 0.17700205386541706, 0.17535934241400608, 0.17412730936397028, 0.17946611900471565, 0.17782340911991543, 0.17659137526821553, 0.1749486646001099, 0.17535934399897557, 0.1749486649917626, 0.18069815203639272, 0.18069815184056637, 0.17864476373185856, 0.18069815303388317, 0.1794661185947042, 0.18069815164474, 0.18193018588809262, 0.17987679740609086, 0.17905544177829852, 0.1790554413682871, 0.17905544078080807, 0.18028747383084384, 0.18110883043776793]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
