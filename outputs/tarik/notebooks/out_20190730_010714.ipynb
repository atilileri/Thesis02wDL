{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf26.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 01:07:14 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['03', '05', '02', '04', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000293020CBE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002931C1A6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6083, Accuracy:0.2304, Validation Loss:1.6062, Validation Accuracy:0.2348\n",
    "Epoch #2: Loss:1.6063, Accuracy:0.2320, Validation Loss:1.6048, Validation Accuracy:0.2365\n",
    "Epoch #3: Loss:1.6055, Accuracy:0.2320, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6044, Accuracy:0.2333, Validation Loss:1.6039, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6042, Accuracy:0.2333, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6039, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6048, Accuracy:0.2329, Validation Loss:1.6042, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6039, Accuracy:0.2345, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6037, Accuracy:0.2361, Validation Loss:1.6038, Validation Accuracy:0.2348\n",
    "Epoch #17: Loss:1.6035, Accuracy:0.2333, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #19: Loss:1.6030, Accuracy:0.2320, Validation Loss:1.6028, Validation Accuracy:0.2381\n",
    "Epoch #20: Loss:1.6027, Accuracy:0.2402, Validation Loss:1.6028, Validation Accuracy:0.2397\n",
    "Epoch #21: Loss:1.6024, Accuracy:0.2415, Validation Loss:1.6029, Validation Accuracy:0.2397\n",
    "Epoch #22: Loss:1.6024, Accuracy:0.2394, Validation Loss:1.6030, Validation Accuracy:0.2381\n",
    "Epoch #23: Loss:1.6018, Accuracy:0.2435, Validation Loss:1.6023, Validation Accuracy:0.2365\n",
    "Epoch #24: Loss:1.6036, Accuracy:0.2382, Validation Loss:1.6029, Validation Accuracy:0.2381\n",
    "Epoch #25: Loss:1.6047, Accuracy:0.2366, Validation Loss:1.6031, Validation Accuracy:0.2348\n",
    "Epoch #26: Loss:1.6039, Accuracy:0.2394, Validation Loss:1.6037, Validation Accuracy:0.2447\n",
    "Epoch #27: Loss:1.6037, Accuracy:0.2415, Validation Loss:1.6024, Validation Accuracy:0.2381\n",
    "Epoch #28: Loss:1.6028, Accuracy:0.2378, Validation Loss:1.6025, Validation Accuracy:0.2348\n",
    "Epoch #29: Loss:1.6024, Accuracy:0.2402, Validation Loss:1.6010, Validation Accuracy:0.2397\n",
    "Epoch #30: Loss:1.6021, Accuracy:0.2407, Validation Loss:1.6011, Validation Accuracy:0.2496\n",
    "Epoch #31: Loss:1.6020, Accuracy:0.2423, Validation Loss:1.6018, Validation Accuracy:0.2479\n",
    "Epoch #32: Loss:1.6012, Accuracy:0.2431, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #33: Loss:1.6015, Accuracy:0.2444, Validation Loss:1.6015, Validation Accuracy:0.2397\n",
    "Epoch #34: Loss:1.6014, Accuracy:0.2448, Validation Loss:1.6014, Validation Accuracy:0.2496\n",
    "Epoch #35: Loss:1.6017, Accuracy:0.2439, Validation Loss:1.6020, Validation Accuracy:0.2463\n",
    "Epoch #36: Loss:1.6013, Accuracy:0.2444, Validation Loss:1.6022, Validation Accuracy:0.2414\n",
    "Epoch #37: Loss:1.6013, Accuracy:0.2431, Validation Loss:1.6020, Validation Accuracy:0.2414\n",
    "Epoch #38: Loss:1.6012, Accuracy:0.2431, Validation Loss:1.6015, Validation Accuracy:0.2463\n",
    "Epoch #39: Loss:1.6009, Accuracy:0.2427, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #40: Loss:1.6004, Accuracy:0.2431, Validation Loss:1.6014, Validation Accuracy:0.2496\n",
    "Epoch #41: Loss:1.6006, Accuracy:0.2448, Validation Loss:1.6017, Validation Accuracy:0.2414\n",
    "Epoch #42: Loss:1.6009, Accuracy:0.2398, Validation Loss:1.6012, Validation Accuracy:0.2447\n",
    "Epoch #43: Loss:1.5993, Accuracy:0.2448, Validation Loss:1.6022, Validation Accuracy:0.2430\n",
    "Epoch #44: Loss:1.6015, Accuracy:0.2407, Validation Loss:1.6015, Validation Accuracy:0.2381\n",
    "Epoch #45: Loss:1.6015, Accuracy:0.2390, Validation Loss:1.6025, Validation Accuracy:0.2348\n",
    "Epoch #46: Loss:1.6014, Accuracy:0.2411, Validation Loss:1.6020, Validation Accuracy:0.2381\n",
    "Epoch #47: Loss:1.6010, Accuracy:0.2460, Validation Loss:1.6034, Validation Accuracy:0.2365\n",
    "Epoch #48: Loss:1.6014, Accuracy:0.2423, Validation Loss:1.6032, Validation Accuracy:0.2381\n",
    "Epoch #49: Loss:1.6011, Accuracy:0.2423, Validation Loss:1.6028, Validation Accuracy:0.2397\n",
    "Epoch #50: Loss:1.6013, Accuracy:0.2435, Validation Loss:1.6026, Validation Accuracy:0.2397\n",
    "Epoch #51: Loss:1.6013, Accuracy:0.2435, Validation Loss:1.6031, Validation Accuracy:0.2397\n",
    "Epoch #52: Loss:1.6006, Accuracy:0.2448, Validation Loss:1.6016, Validation Accuracy:0.2397\n",
    "Epoch #53: Loss:1.5996, Accuracy:0.2460, Validation Loss:1.6023, Validation Accuracy:0.2414\n",
    "Epoch #54: Loss:1.6006, Accuracy:0.2448, Validation Loss:1.6028, Validation Accuracy:0.2381\n",
    "Epoch #55: Loss:1.6008, Accuracy:0.2427, Validation Loss:1.6022, Validation Accuracy:0.2397\n",
    "Epoch #56: Loss:1.6005, Accuracy:0.2435, Validation Loss:1.6021, Validation Accuracy:0.2414\n",
    "Epoch #57: Loss:1.6003, Accuracy:0.2444, Validation Loss:1.6022, Validation Accuracy:0.2365\n",
    "Epoch #58: Loss:1.6008, Accuracy:0.2407, Validation Loss:1.6019, Validation Accuracy:0.2397\n",
    "Epoch #59: Loss:1.6004, Accuracy:0.2452, Validation Loss:1.6019, Validation Accuracy:0.2414\n",
    "Epoch #60: Loss:1.6004, Accuracy:0.2439, Validation Loss:1.6016, Validation Accuracy:0.2430\n",
    "Epoch #61: Loss:1.5999, Accuracy:0.2452, Validation Loss:1.6018, Validation Accuracy:0.2430\n",
    "Epoch #62: Loss:1.5995, Accuracy:0.2452, Validation Loss:1.6016, Validation Accuracy:0.2430\n",
    "Epoch #63: Loss:1.5990, Accuracy:0.2448, Validation Loss:1.6008, Validation Accuracy:0.2430\n",
    "Epoch #64: Loss:1.5990, Accuracy:0.2456, Validation Loss:1.6008, Validation Accuracy:0.2414\n",
    "Epoch #65: Loss:1.5987, Accuracy:0.2452, Validation Loss:1.6012, Validation Accuracy:0.2381\n",
    "Epoch #66: Loss:1.5983, Accuracy:0.2472, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #67: Loss:1.5979, Accuracy:0.2460, Validation Loss:1.6003, Validation Accuracy:0.2397\n",
    "Epoch #68: Loss:1.5972, Accuracy:0.2460, Validation Loss:1.5996, Validation Accuracy:0.2397\n",
    "Epoch #69: Loss:1.5982, Accuracy:0.2353, Validation Loss:1.5996, Validation Accuracy:0.2397\n",
    "Epoch #70: Loss:1.5971, Accuracy:0.2456, Validation Loss:1.6000, Validation Accuracy:0.2397\n",
    "Epoch #71: Loss:1.5977, Accuracy:0.2460, Validation Loss:1.6007, Validation Accuracy:0.2381\n",
    "Epoch #72: Loss:1.5978, Accuracy:0.2452, Validation Loss:1.5996, Validation Accuracy:0.2381\n",
    "Epoch #73: Loss:1.5994, Accuracy:0.2349, Validation Loss:1.6007, Validation Accuracy:0.2463\n",
    "Epoch #74: Loss:1.5969, Accuracy:0.2476, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #75: Loss:1.5977, Accuracy:0.2452, Validation Loss:1.5992, Validation Accuracy:0.2414\n",
    "Epoch #76: Loss:1.5966, Accuracy:0.2464, Validation Loss:1.6002, Validation Accuracy:0.2496\n",
    "Epoch #77: Loss:1.5973, Accuracy:0.2390, Validation Loss:1.5996, Validation Accuracy:0.2414\n",
    "Epoch #78: Loss:1.5973, Accuracy:0.2452, Validation Loss:1.5991, Validation Accuracy:0.2397\n",
    "Epoch #79: Loss:1.5971, Accuracy:0.2431, Validation Loss:1.6000, Validation Accuracy:0.2545\n",
    "Epoch #80: Loss:1.5964, Accuracy:0.2427, Validation Loss:1.5994, Validation Accuracy:0.2365\n",
    "Epoch #81: Loss:1.5968, Accuracy:0.2448, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #82: Loss:1.5968, Accuracy:0.2439, Validation Loss:1.6001, Validation Accuracy:0.2397\n",
    "Epoch #83: Loss:1.5966, Accuracy:0.2419, Validation Loss:1.6006, Validation Accuracy:0.2365\n",
    "Epoch #84: Loss:1.5977, Accuracy:0.2407, Validation Loss:1.5986, Validation Accuracy:0.2381\n",
    "Epoch #85: Loss:1.5988, Accuracy:0.2435, Validation Loss:1.5992, Validation Accuracy:0.2512\n",
    "Epoch #86: Loss:1.5974, Accuracy:0.2427, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #87: Loss:1.5981, Accuracy:0.2419, Validation Loss:1.6008, Validation Accuracy:0.2447\n",
    "Epoch #88: Loss:1.5967, Accuracy:0.2427, Validation Loss:1.6007, Validation Accuracy:0.2397\n",
    "Epoch #89: Loss:1.5972, Accuracy:0.2394, Validation Loss:1.6021, Validation Accuracy:0.2381\n",
    "Epoch #90: Loss:1.5970, Accuracy:0.2452, Validation Loss:1.6019, Validation Accuracy:0.2381\n",
    "Epoch #91: Loss:1.5975, Accuracy:0.2439, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #92: Loss:1.5974, Accuracy:0.2435, Validation Loss:1.6011, Validation Accuracy:0.2430\n",
    "Epoch #93: Loss:1.5977, Accuracy:0.2402, Validation Loss:1.6011, Validation Accuracy:0.2397\n",
    "Epoch #94: Loss:1.5975, Accuracy:0.2415, Validation Loss:1.6022, Validation Accuracy:0.2365\n",
    "Epoch #95: Loss:1.5964, Accuracy:0.2423, Validation Loss:1.6034, Validation Accuracy:0.2365\n",
    "Epoch #96: Loss:1.5961, Accuracy:0.2489, Validation Loss:1.6021, Validation Accuracy:0.2365\n",
    "Epoch #97: Loss:1.5966, Accuracy:0.2435, Validation Loss:1.6016, Validation Accuracy:0.2332\n",
    "Epoch #98: Loss:1.5979, Accuracy:0.2439, Validation Loss:1.6001, Validation Accuracy:0.2430\n",
    "Epoch #99: Loss:1.5995, Accuracy:0.2431, Validation Loss:1.6007, Validation Accuracy:0.2414\n",
    "Epoch #100: Loss:1.5985, Accuracy:0.2419, Validation Loss:1.6021, Validation Accuracy:0.2381\n",
    "Epoch #101: Loss:1.5976, Accuracy:0.2386, Validation Loss:1.6009, Validation Accuracy:0.2348\n",
    "Epoch #102: Loss:1.5971, Accuracy:0.2435, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #103: Loss:1.5974, Accuracy:0.2419, Validation Loss:1.6019, Validation Accuracy:0.2348\n",
    "Epoch #104: Loss:1.5985, Accuracy:0.2353, Validation Loss:1.6013, Validation Accuracy:0.2365\n",
    "Epoch #105: Loss:1.5990, Accuracy:0.2411, Validation Loss:1.6008, Validation Accuracy:0.2397\n",
    "Epoch #106: Loss:1.5979, Accuracy:0.2390, Validation Loss:1.6005, Validation Accuracy:0.2332\n",
    "Epoch #107: Loss:1.5969, Accuracy:0.2452, Validation Loss:1.5999, Validation Accuracy:0.2332\n",
    "Epoch #108: Loss:1.5968, Accuracy:0.2427, Validation Loss:1.6006, Validation Accuracy:0.2250\n",
    "Epoch #109: Loss:1.5957, Accuracy:0.2411, Validation Loss:1.6026, Validation Accuracy:0.2348\n",
    "Epoch #110: Loss:1.5967, Accuracy:0.2489, Validation Loss:1.6077, Validation Accuracy:0.2299\n",
    "Epoch #111: Loss:1.5966, Accuracy:0.2480, Validation Loss:1.6041, Validation Accuracy:0.2266\n",
    "Epoch #112: Loss:1.5962, Accuracy:0.2513, Validation Loss:1.6025, Validation Accuracy:0.2299\n",
    "Epoch #113: Loss:1.5966, Accuracy:0.2390, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #114: Loss:1.5954, Accuracy:0.2431, Validation Loss:1.6026, Validation Accuracy:0.2348\n",
    "Epoch #115: Loss:1.5962, Accuracy:0.2476, Validation Loss:1.6001, Validation Accuracy:0.2365\n",
    "Epoch #116: Loss:1.5957, Accuracy:0.2448, Validation Loss:1.6009, Validation Accuracy:0.2365\n",
    "Epoch #117: Loss:1.5966, Accuracy:0.2431, Validation Loss:1.6008, Validation Accuracy:0.2315\n",
    "Epoch #118: Loss:1.5959, Accuracy:0.2489, Validation Loss:1.6003, Validation Accuracy:0.2282\n",
    "Epoch #119: Loss:1.5949, Accuracy:0.2509, Validation Loss:1.6003, Validation Accuracy:0.2381\n",
    "Epoch #120: Loss:1.5949, Accuracy:0.2489, Validation Loss:1.5992, Validation Accuracy:0.2299\n",
    "Epoch #121: Loss:1.5953, Accuracy:0.2464, Validation Loss:1.5996, Validation Accuracy:0.2315\n",
    "Epoch #122: Loss:1.5944, Accuracy:0.2480, Validation Loss:1.6004, Validation Accuracy:0.2315\n",
    "Epoch #123: Loss:1.5946, Accuracy:0.2505, Validation Loss:1.5996, Validation Accuracy:0.2282\n",
    "Epoch #124: Loss:1.5951, Accuracy:0.2460, Validation Loss:1.5998, Validation Accuracy:0.2332\n",
    "Epoch #125: Loss:1.5943, Accuracy:0.2485, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #126: Loss:1.5949, Accuracy:0.2464, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #127: Loss:1.5949, Accuracy:0.2435, Validation Loss:1.6033, Validation Accuracy:0.2299\n",
    "Epoch #128: Loss:1.5941, Accuracy:0.2480, Validation Loss:1.6038, Validation Accuracy:0.2348\n",
    "Epoch #129: Loss:1.5941, Accuracy:0.2460, Validation Loss:1.6018, Validation Accuracy:0.2299\n",
    "Epoch #130: Loss:1.5937, Accuracy:0.2444, Validation Loss:1.6023, Validation Accuracy:0.2299\n",
    "Epoch #131: Loss:1.5934, Accuracy:0.2526, Validation Loss:1.6031, Validation Accuracy:0.2397\n",
    "Epoch #132: Loss:1.5933, Accuracy:0.2497, Validation Loss:1.6024, Validation Accuracy:0.2299\n",
    "Epoch #133: Loss:1.5931, Accuracy:0.2526, Validation Loss:1.6036, Validation Accuracy:0.2365\n",
    "Epoch #134: Loss:1.5929, Accuracy:0.2517, Validation Loss:1.6028, Validation Accuracy:0.2365\n",
    "Epoch #135: Loss:1.5922, Accuracy:0.2538, Validation Loss:1.6038, Validation Accuracy:0.2332\n",
    "Epoch #136: Loss:1.5949, Accuracy:0.2509, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #137: Loss:1.5933, Accuracy:0.2485, Validation Loss:1.6031, Validation Accuracy:0.2332\n",
    "Epoch #138: Loss:1.5924, Accuracy:0.2489, Validation Loss:1.6046, Validation Accuracy:0.2233\n",
    "Epoch #139: Loss:1.5933, Accuracy:0.2485, Validation Loss:1.6021, Validation Accuracy:0.2282\n",
    "Epoch #140: Loss:1.5938, Accuracy:0.2427, Validation Loss:1.5998, Validation Accuracy:0.2315\n",
    "Epoch #141: Loss:1.5942, Accuracy:0.2439, Validation Loss:1.6002, Validation Accuracy:0.2282\n",
    "Epoch #142: Loss:1.5931, Accuracy:0.2493, Validation Loss:1.6000, Validation Accuracy:0.2381\n",
    "Epoch #143: Loss:1.5942, Accuracy:0.2550, Validation Loss:1.6013, Validation Accuracy:0.2512\n",
    "Epoch #144: Loss:1.5943, Accuracy:0.2616, Validation Loss:1.5989, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5936, Accuracy:0.2464, Validation Loss:1.6015, Validation Accuracy:0.2184\n",
    "Epoch #146: Loss:1.5955, Accuracy:0.2485, Validation Loss:1.6013, Validation Accuracy:0.2266\n",
    "Epoch #147: Loss:1.5948, Accuracy:0.2489, Validation Loss:1.6044, Validation Accuracy:0.2414\n",
    "Epoch #148: Loss:1.5952, Accuracy:0.2456, Validation Loss:1.6008, Validation Accuracy:0.2348\n",
    "Epoch #149: Loss:1.5935, Accuracy:0.2476, Validation Loss:1.6015, Validation Accuracy:0.2250\n",
    "Epoch #150: Loss:1.5932, Accuracy:0.2485, Validation Loss:1.6021, Validation Accuracy:0.2266\n",
    "Epoch #151: Loss:1.5910, Accuracy:0.2493, Validation Loss:1.6014, Validation Accuracy:0.2381\n",
    "Epoch #152: Loss:1.5908, Accuracy:0.2427, Validation Loss:1.6045, Validation Accuracy:0.2266\n",
    "Epoch #153: Loss:1.5917, Accuracy:0.2501, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #154: Loss:1.5932, Accuracy:0.2472, Validation Loss:1.6065, Validation Accuracy:0.2381\n",
    "Epoch #155: Loss:1.5921, Accuracy:0.2513, Validation Loss:1.6048, Validation Accuracy:0.2282\n",
    "Epoch #156: Loss:1.5893, Accuracy:0.2456, Validation Loss:1.6030, Validation Accuracy:0.2463\n",
    "Epoch #157: Loss:1.5900, Accuracy:0.2489, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #158: Loss:1.5895, Accuracy:0.2402, Validation Loss:1.5992, Validation Accuracy:0.2430\n",
    "Epoch #159: Loss:1.5878, Accuracy:0.2497, Validation Loss:1.6014, Validation Accuracy:0.2463\n",
    "Epoch #160: Loss:1.5887, Accuracy:0.2505, Validation Loss:1.5994, Validation Accuracy:0.2529\n",
    "Epoch #161: Loss:1.5882, Accuracy:0.2476, Validation Loss:1.6013, Validation Accuracy:0.2447\n",
    "Epoch #162: Loss:1.5900, Accuracy:0.2444, Validation Loss:1.6010, Validation Accuracy:0.2397\n",
    "Epoch #163: Loss:1.5897, Accuracy:0.2423, Validation Loss:1.6014, Validation Accuracy:0.2365\n",
    "Epoch #164: Loss:1.5898, Accuracy:0.2407, Validation Loss:1.6006, Validation Accuracy:0.2496\n",
    "Epoch #165: Loss:1.5887, Accuracy:0.2460, Validation Loss:1.6017, Validation Accuracy:0.2332\n",
    "Epoch #166: Loss:1.5885, Accuracy:0.2448, Validation Loss:1.6010, Validation Accuracy:0.2282\n",
    "Epoch #167: Loss:1.5875, Accuracy:0.2509, Validation Loss:1.6002, Validation Accuracy:0.2299\n",
    "Epoch #168: Loss:1.5870, Accuracy:0.2538, Validation Loss:1.6009, Validation Accuracy:0.2397\n",
    "Epoch #169: Loss:1.5862, Accuracy:0.2513, Validation Loss:1.6012, Validation Accuracy:0.2479\n",
    "Epoch #170: Loss:1.5853, Accuracy:0.2485, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #171: Loss:1.5855, Accuracy:0.2546, Validation Loss:1.6055, Validation Accuracy:0.2430\n",
    "Epoch #172: Loss:1.5858, Accuracy:0.2608, Validation Loss:1.6045, Validation Accuracy:0.2463\n",
    "Epoch #173: Loss:1.5855, Accuracy:0.2595, Validation Loss:1.6028, Validation Accuracy:0.2348\n",
    "Epoch #174: Loss:1.5838, Accuracy:0.2637, Validation Loss:1.6035, Validation Accuracy:0.2414\n",
    "Epoch #175: Loss:1.5842, Accuracy:0.2534, Validation Loss:1.6081, Validation Accuracy:0.2430\n",
    "Epoch #176: Loss:1.5864, Accuracy:0.2612, Validation Loss:1.6100, Validation Accuracy:0.2365\n",
    "Epoch #177: Loss:1.5844, Accuracy:0.2567, Validation Loss:1.6039, Validation Accuracy:0.2381\n",
    "Epoch #178: Loss:1.5863, Accuracy:0.2575, Validation Loss:1.6068, Validation Accuracy:0.2381\n",
    "Epoch #179: Loss:1.5882, Accuracy:0.2571, Validation Loss:1.6089, Validation Accuracy:0.2365\n",
    "Epoch #180: Loss:1.5891, Accuracy:0.2534, Validation Loss:1.6081, Validation Accuracy:0.2250\n",
    "Epoch #181: Loss:1.5903, Accuracy:0.2505, Validation Loss:1.6036, Validation Accuracy:0.2447\n",
    "Epoch #182: Loss:1.5899, Accuracy:0.2513, Validation Loss:1.6027, Validation Accuracy:0.2463\n",
    "Epoch #183: Loss:1.5904, Accuracy:0.2509, Validation Loss:1.6000, Validation Accuracy:0.2414\n",
    "Epoch #184: Loss:1.5883, Accuracy:0.2546, Validation Loss:1.5996, Validation Accuracy:0.2397\n",
    "Epoch #185: Loss:1.5879, Accuracy:0.2522, Validation Loss:1.5998, Validation Accuracy:0.2365\n",
    "Epoch #186: Loss:1.5903, Accuracy:0.2575, Validation Loss:1.6037, Validation Accuracy:0.2282\n",
    "Epoch #187: Loss:1.5891, Accuracy:0.2489, Validation Loss:1.6030, Validation Accuracy:0.2282\n",
    "Epoch #188: Loss:1.5883, Accuracy:0.2616, Validation Loss:1.6027, Validation Accuracy:0.2430\n",
    "Epoch #189: Loss:1.5868, Accuracy:0.2579, Validation Loss:1.6034, Validation Accuracy:0.2299\n",
    "Epoch #190: Loss:1.5870, Accuracy:0.2608, Validation Loss:1.6032, Validation Accuracy:0.2282\n",
    "Epoch #191: Loss:1.5891, Accuracy:0.2600, Validation Loss:1.6037, Validation Accuracy:0.2299\n",
    "Epoch #192: Loss:1.6294, Accuracy:0.2324, Validation Loss:1.6141, Validation Accuracy:0.2315\n",
    "Epoch #193: Loss:1.6053, Accuracy:0.2415, Validation Loss:1.6036, Validation Accuracy:0.2348\n",
    "Epoch #194: Loss:1.5967, Accuracy:0.2415, Validation Loss:1.6001, Validation Accuracy:0.2414\n",
    "Epoch #195: Loss:1.5967, Accuracy:0.2439, Validation Loss:1.6021, Validation Accuracy:0.2447\n",
    "Epoch #196: Loss:1.5966, Accuracy:0.2452, Validation Loss:1.6002, Validation Accuracy:0.2414\n",
    "Epoch #197: Loss:1.5947, Accuracy:0.2452, Validation Loss:1.5997, Validation Accuracy:0.2397\n",
    "Epoch #198: Loss:1.5946, Accuracy:0.2435, Validation Loss:1.6007, Validation Accuracy:0.2447\n",
    "Epoch #199: Loss:1.5932, Accuracy:0.2476, Validation Loss:1.6006, Validation Accuracy:0.2463\n",
    "Epoch #200: Loss:1.5926, Accuracy:0.2468, Validation Loss:1.6010, Validation Accuracy:0.2463\n",
    "Epoch #201: Loss:1.5920, Accuracy:0.2493, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #202: Loss:1.5922, Accuracy:0.2402, Validation Loss:1.6030, Validation Accuracy:0.2479\n",
    "Epoch #203: Loss:1.5914, Accuracy:0.2501, Validation Loss:1.6011, Validation Accuracy:0.2332\n",
    "Epoch #204: Loss:1.5914, Accuracy:0.2509, Validation Loss:1.6028, Validation Accuracy:0.2463\n",
    "Epoch #205: Loss:1.5908, Accuracy:0.2509, Validation Loss:1.6019, Validation Accuracy:0.2463\n",
    "Epoch #206: Loss:1.5894, Accuracy:0.2538, Validation Loss:1.6025, Validation Accuracy:0.2414\n",
    "Epoch #207: Loss:1.5900, Accuracy:0.2517, Validation Loss:1.6023, Validation Accuracy:0.2463\n",
    "Epoch #208: Loss:1.5906, Accuracy:0.2435, Validation Loss:1.6036, Validation Accuracy:0.2397\n",
    "Epoch #209: Loss:1.5910, Accuracy:0.2448, Validation Loss:1.6040, Validation Accuracy:0.2397\n",
    "Epoch #210: Loss:1.5895, Accuracy:0.2497, Validation Loss:1.6023, Validation Accuracy:0.2430\n",
    "Epoch #211: Loss:1.5887, Accuracy:0.2517, Validation Loss:1.6025, Validation Accuracy:0.2365\n",
    "Epoch #212: Loss:1.5878, Accuracy:0.2567, Validation Loss:1.6028, Validation Accuracy:0.2365\n",
    "Epoch #213: Loss:1.5881, Accuracy:0.2591, Validation Loss:1.6030, Validation Accuracy:0.2447\n",
    "Epoch #214: Loss:1.5874, Accuracy:0.2522, Validation Loss:1.6019, Validation Accuracy:0.2315\n",
    "Epoch #215: Loss:1.5877, Accuracy:0.2538, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #216: Loss:1.5851, Accuracy:0.2559, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #217: Loss:1.5852, Accuracy:0.2554, Validation Loss:1.6023, Validation Accuracy:0.2381\n",
    "Epoch #218: Loss:1.5855, Accuracy:0.2583, Validation Loss:1.6032, Validation Accuracy:0.2348\n",
    "Epoch #219: Loss:1.5853, Accuracy:0.2567, Validation Loss:1.6047, Validation Accuracy:0.2299\n",
    "Epoch #220: Loss:1.5861, Accuracy:0.2583, Validation Loss:1.6027, Validation Accuracy:0.2365\n",
    "Epoch #221: Loss:1.5853, Accuracy:0.2546, Validation Loss:1.6038, Validation Accuracy:0.2250\n",
    "Epoch #222: Loss:1.5860, Accuracy:0.2612, Validation Loss:1.6007, Validation Accuracy:0.2365\n",
    "Epoch #223: Loss:1.5854, Accuracy:0.2571, Validation Loss:1.6028, Validation Accuracy:0.2348\n",
    "Epoch #224: Loss:1.5858, Accuracy:0.2628, Validation Loss:1.6047, Validation Accuracy:0.2414\n",
    "Epoch #225: Loss:1.5820, Accuracy:0.2620, Validation Loss:1.6029, Validation Accuracy:0.2233\n",
    "Epoch #226: Loss:1.5828, Accuracy:0.2522, Validation Loss:1.6044, Validation Accuracy:0.2397\n",
    "Epoch #227: Loss:1.5803, Accuracy:0.2669, Validation Loss:1.6059, Validation Accuracy:0.2479\n",
    "Epoch #228: Loss:1.5806, Accuracy:0.2632, Validation Loss:1.6038, Validation Accuracy:0.2365\n",
    "Epoch #229: Loss:1.5812, Accuracy:0.2624, Validation Loss:1.6038, Validation Accuracy:0.2414\n",
    "Epoch #230: Loss:1.5822, Accuracy:0.2661, Validation Loss:1.6038, Validation Accuracy:0.2282\n",
    "Epoch #231: Loss:1.5827, Accuracy:0.2632, Validation Loss:1.6052, Validation Accuracy:0.2397\n",
    "Epoch #232: Loss:1.5832, Accuracy:0.2665, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #233: Loss:1.5840, Accuracy:0.2563, Validation Loss:1.6080, Validation Accuracy:0.2299\n",
    "Epoch #234: Loss:1.5843, Accuracy:0.2538, Validation Loss:1.6078, Validation Accuracy:0.2299\n",
    "Epoch #235: Loss:1.5830, Accuracy:0.2595, Validation Loss:1.6082, Validation Accuracy:0.2315\n",
    "Epoch #236: Loss:1.5830, Accuracy:0.2624, Validation Loss:1.6090, Validation Accuracy:0.2332\n",
    "Epoch #237: Loss:1.5914, Accuracy:0.2509, Validation Loss:1.6199, Validation Accuracy:0.2250\n",
    "Epoch #238: Loss:1.6186, Accuracy:0.2423, Validation Loss:1.6123, Validation Accuracy:0.2365\n",
    "Epoch #239: Loss:1.5929, Accuracy:0.2435, Validation Loss:1.6071, Validation Accuracy:0.2348\n",
    "Epoch #240: Loss:1.5912, Accuracy:0.2472, Validation Loss:1.6059, Validation Accuracy:0.2397\n",
    "Epoch #241: Loss:1.5926, Accuracy:0.2534, Validation Loss:1.6055, Validation Accuracy:0.2414\n",
    "Epoch #242: Loss:1.5909, Accuracy:0.2522, Validation Loss:1.6047, Validation Accuracy:0.2348\n",
    "Epoch #243: Loss:1.5901, Accuracy:0.2612, Validation Loss:1.6032, Validation Accuracy:0.2348\n",
    "Epoch #244: Loss:1.5883, Accuracy:0.2575, Validation Loss:1.6043, Validation Accuracy:0.2414\n",
    "Epoch #245: Loss:1.5876, Accuracy:0.2616, Validation Loss:1.6051, Validation Accuracy:0.2315\n",
    "Epoch #246: Loss:1.5881, Accuracy:0.2554, Validation Loss:1.6052, Validation Accuracy:0.2299\n",
    "Epoch #247: Loss:1.5875, Accuracy:0.2587, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #248: Loss:1.5870, Accuracy:0.2645, Validation Loss:1.6033, Validation Accuracy:0.2365\n",
    "Epoch #249: Loss:1.5858, Accuracy:0.2641, Validation Loss:1.6036, Validation Accuracy:0.2381\n",
    "Epoch #250: Loss:1.5864, Accuracy:0.2595, Validation Loss:1.6039, Validation Accuracy:0.2397\n",
    "Epoch #251: Loss:1.5863, Accuracy:0.2620, Validation Loss:1.6067, Validation Accuracy:0.2414\n",
    "Epoch #252: Loss:1.5853, Accuracy:0.2637, Validation Loss:1.6073, Validation Accuracy:0.2348\n",
    "Epoch #253: Loss:1.5851, Accuracy:0.2616, Validation Loss:1.6063, Validation Accuracy:0.2381\n",
    "Epoch #254: Loss:1.5845, Accuracy:0.2604, Validation Loss:1.6058, Validation Accuracy:0.2365\n",
    "Epoch #255: Loss:1.5861, Accuracy:0.2591, Validation Loss:1.6052, Validation Accuracy:0.2348\n",
    "Epoch #256: Loss:1.5868, Accuracy:0.2583, Validation Loss:1.6063, Validation Accuracy:0.2397\n",
    "Epoch #257: Loss:1.5831, Accuracy:0.2575, Validation Loss:1.6068, Validation Accuracy:0.2365\n",
    "Epoch #258: Loss:1.5834, Accuracy:0.2579, Validation Loss:1.6063, Validation Accuracy:0.2414\n",
    "Epoch #259: Loss:1.5823, Accuracy:0.2632, Validation Loss:1.6078, Validation Accuracy:0.2397\n",
    "Epoch #260: Loss:1.5824, Accuracy:0.2632, Validation Loss:1.6052, Validation Accuracy:0.2496\n",
    "Epoch #261: Loss:1.5830, Accuracy:0.2575, Validation Loss:1.6035, Validation Accuracy:0.2397\n",
    "Epoch #262: Loss:1.5831, Accuracy:0.2628, Validation Loss:1.6063, Validation Accuracy:0.2430\n",
    "Epoch #263: Loss:1.5822, Accuracy:0.2595, Validation Loss:1.6074, Validation Accuracy:0.2430\n",
    "Epoch #264: Loss:1.5814, Accuracy:0.2616, Validation Loss:1.6074, Validation Accuracy:0.2512\n",
    "Epoch #265: Loss:1.5834, Accuracy:0.2653, Validation Loss:1.6068, Validation Accuracy:0.2447\n",
    "Epoch #266: Loss:1.5831, Accuracy:0.2554, Validation Loss:1.6084, Validation Accuracy:0.2397\n",
    "Epoch #267: Loss:1.5815, Accuracy:0.2637, Validation Loss:1.6068, Validation Accuracy:0.2430\n",
    "Epoch #268: Loss:1.5822, Accuracy:0.2604, Validation Loss:1.6082, Validation Accuracy:0.2348\n",
    "Epoch #269: Loss:1.5823, Accuracy:0.2641, Validation Loss:1.6084, Validation Accuracy:0.2414\n",
    "Epoch #270: Loss:1.5817, Accuracy:0.2632, Validation Loss:1.6081, Validation Accuracy:0.2381\n",
    "Epoch #271: Loss:1.5818, Accuracy:0.2559, Validation Loss:1.6076, Validation Accuracy:0.2381\n",
    "Epoch #272: Loss:1.5814, Accuracy:0.2600, Validation Loss:1.6048, Validation Accuracy:0.2397\n",
    "Epoch #273: Loss:1.5810, Accuracy:0.2665, Validation Loss:1.6054, Validation Accuracy:0.2430\n",
    "Epoch #274: Loss:1.5811, Accuracy:0.2641, Validation Loss:1.6068, Validation Accuracy:0.2430\n",
    "Epoch #275: Loss:1.5804, Accuracy:0.2674, Validation Loss:1.6069, Validation Accuracy:0.2217\n",
    "Epoch #276: Loss:1.5809, Accuracy:0.2686, Validation Loss:1.6050, Validation Accuracy:0.2447\n",
    "Epoch #277: Loss:1.5809, Accuracy:0.2595, Validation Loss:1.6108, Validation Accuracy:0.2348\n",
    "Epoch #278: Loss:1.5815, Accuracy:0.2653, Validation Loss:1.6086, Validation Accuracy:0.2365\n",
    "Epoch #279: Loss:1.5833, Accuracy:0.2579, Validation Loss:1.6069, Validation Accuracy:0.2479\n",
    "Epoch #280: Loss:1.5814, Accuracy:0.2632, Validation Loss:1.6061, Validation Accuracy:0.2545\n",
    "Epoch #281: Loss:1.5816, Accuracy:0.2620, Validation Loss:1.6062, Validation Accuracy:0.2447\n",
    "Epoch #282: Loss:1.5796, Accuracy:0.2678, Validation Loss:1.6058, Validation Accuracy:0.2479\n",
    "Epoch #283: Loss:1.5796, Accuracy:0.2653, Validation Loss:1.6062, Validation Accuracy:0.2512\n",
    "Epoch #284: Loss:1.5794, Accuracy:0.2678, Validation Loss:1.6065, Validation Accuracy:0.2430\n",
    "Epoch #285: Loss:1.5803, Accuracy:0.2678, Validation Loss:1.6054, Validation Accuracy:0.2315\n",
    "Epoch #286: Loss:1.5803, Accuracy:0.2661, Validation Loss:1.6059, Validation Accuracy:0.2414\n",
    "Epoch #287: Loss:1.5794, Accuracy:0.2702, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #288: Loss:1.5788, Accuracy:0.2706, Validation Loss:1.6066, Validation Accuracy:0.2496\n",
    "Epoch #289: Loss:1.5786, Accuracy:0.2682, Validation Loss:1.6072, Validation Accuracy:0.2397\n",
    "Epoch #290: Loss:1.5784, Accuracy:0.2661, Validation Loss:1.6065, Validation Accuracy:0.2430\n",
    "Epoch #291: Loss:1.5776, Accuracy:0.2649, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #292: Loss:1.5786, Accuracy:0.2604, Validation Loss:1.6062, Validation Accuracy:0.2250\n",
    "Epoch #293: Loss:1.5781, Accuracy:0.2657, Validation Loss:1.6052, Validation Accuracy:0.2463\n",
    "Epoch #294: Loss:1.5768, Accuracy:0.2616, Validation Loss:1.6053, Validation Accuracy:0.2463\n",
    "Epoch #295: Loss:1.5759, Accuracy:0.2674, Validation Loss:1.6057, Validation Accuracy:0.2233\n",
    "Epoch #296: Loss:1.5758, Accuracy:0.2637, Validation Loss:1.6073, Validation Accuracy:0.2430\n",
    "Epoch #297: Loss:1.5757, Accuracy:0.2628, Validation Loss:1.6073, Validation Accuracy:0.2479\n",
    "Epoch #298: Loss:1.5754, Accuracy:0.2649, Validation Loss:1.6071, Validation Accuracy:0.2447\n",
    "Epoch #299: Loss:1.5739, Accuracy:0.2674, Validation Loss:1.6084, Validation Accuracy:0.2233\n",
    "Epoch #300: Loss:1.5741, Accuracy:0.2702, Validation Loss:1.6085, Validation Accuracy:0.2463\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60852456, Accuracy:0.2463\n",
    "Labels: ['03', '05', '02', '04', '01']\n",
    "Confusion Matrix:\n",
    "      03  05  02  04  01\n",
    "t:03  13  59   0  22  21\n",
    "t:05  12  77   0  22  31\n",
    "t:02   5  64   0  22  23\n",
    "t:04   4  53   0  27  28\n",
    "t:01   8  71   0  14  33\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.31      0.11      0.17       115\n",
    "          05       0.24      0.54      0.33       142\n",
    "          02       0.00      0.00      0.00       114\n",
    "          04       0.25      0.24      0.25       112\n",
    "          01       0.24      0.26      0.25       126\n",
    "\n",
    "    accuracy                           0.25       609\n",
    "   macro avg       0.21      0.23      0.20       609\n",
    "weighted avg       0.21      0.25      0.21       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 01:47:49 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 35 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6061915701441773, 1.604840489052395, 1.6053078213740257, 1.6044009434570037, 1.6048285299529779, 1.6047512126477872, 1.6038016562391384, 1.6038376345423055, 1.6038789087524163, 1.6041985926369728, 1.6042586212674972, 1.604249796648135, 1.604266208968139, 1.6038114222008215, 1.6037925975076084, 1.6038109455594092, 1.6033923575052096, 1.6032351960102325, 1.602750032601881, 1.6027819930234761, 1.6028862847091725, 1.6029530278176118, 1.6023188159970814, 1.6028842105849819, 1.6030952891301247, 1.6036776353181486, 1.602372310823212, 1.602485014300041, 1.6010406643690538, 1.6010559460603937, 1.6017953566533982, 1.6008761340174182, 1.6014633249179484, 1.6014194312354026, 1.6019714829956957, 1.6021654521694715, 1.6019515664315185, 1.6015454162713538, 1.6012496310110358, 1.601371603646302, 1.6017018525275495, 1.6011692704434073, 1.6021587664662127, 1.601452658329104, 1.602484663327535, 1.6020202470334683, 1.6034440720218353, 1.6032490436666704, 1.6027659555569853, 1.6025920887103027, 1.603099934964736, 1.6015616500710423, 1.602307047749975, 1.6027849315618254, 1.6021641788420027, 1.6020613061187694, 1.602197883751592, 1.6018564573845449, 1.6018505644524235, 1.601612934338048, 1.6017728184635807, 1.6016355704008456, 1.6008458789346254, 1.6007832737000314, 1.601153992275495, 1.6005380554935225, 1.600344034642813, 1.5996328578598198, 1.5995631286467629, 1.6000230388688337, 1.6007031212103582, 1.599614143371582, 1.6007462407176327, 1.6010121805914517, 1.5991873202848512, 1.6002457177110494, 1.5996138143226235, 1.5991093020133784, 1.5999855560622191, 1.5994446183660347, 1.5998897482021688, 1.6001330158001879, 1.6005816197356175, 1.59860142306937, 1.599151615830282, 1.6016821929778176, 1.6007922510012422, 1.6007366397697937, 1.6020531051460354, 1.6018899440373888, 1.6013571985053703, 1.6010545574385544, 1.6011319430590851, 1.6021673628457858, 1.603371882086317, 1.6020605368371472, 1.6016243093315212, 1.6000506181043552, 1.6007350405253018, 1.6021428298088913, 1.6009237423710438, 1.6024418215837777, 1.6019327734491509, 1.6013437607409724, 1.6007632852970868, 1.6005449432066117, 1.5998515405482652, 1.600603144744347, 1.6025798508686384, 1.6077016639004786, 1.604123552640279, 1.602480524670706, 1.6028525886081515, 1.6025829383696633, 1.6000500725603652, 1.6009217044598558, 1.6007585122276018, 1.600289572244403, 1.6003033827091087, 1.5991590972408676, 1.59962914123128, 1.6004303180916948, 1.5996240056402773, 1.5997874670232262, 1.60049383413224, 1.6017282856704762, 1.6032508828957093, 1.6038044305466275, 1.6017555971255248, 1.6022859727612073, 1.6030547975123615, 1.6024472926833555, 1.603595778859895, 1.6028265021313195, 1.6037912137794181, 1.6045423896833397, 1.6030955244167684, 1.604648081344141, 1.602135459582011, 1.5997923109527488, 1.6002283268569921, 1.6000255442213738, 1.6012609868214047, 1.598918115172676, 1.6014931746108583, 1.6013049983430183, 1.6043556635015703, 1.6008087214577962, 1.601506482008447, 1.6020522023656685, 1.6014279683039498, 1.6044688189558207, 1.6058741639596097, 1.6065167407879883, 1.6047573079812312, 1.6030499347912266, 1.603484873896945, 1.5991956897948567, 1.601428800811517, 1.599385511894727, 1.6013370008499948, 1.601026319322132, 1.6013968906966336, 1.6005853825602039, 1.6017332229708217, 1.6009603214185617, 1.6001507417713283, 1.6009301565746563, 1.6012457785348, 1.6036409587891427, 1.6055304814246292, 1.6044940854528267, 1.6028279604387206, 1.603452556238973, 1.608080731432622, 1.6099621030105942, 1.6038716432496245, 1.6067778244002895, 1.6089325438579316, 1.6080629208992268, 1.6035797960065268, 1.6026823859300912, 1.6000389338322656, 1.5996141852612173, 1.5998452807882149, 1.6036551533074215, 1.6030463861127204, 1.6026545377396206, 1.6033719999253848, 1.6032424290191951, 1.6037114764669258, 1.6140507600577594, 1.6035821246004653, 1.600137027613635, 1.6020526521898844, 1.6001765888508512, 1.5996556098042254, 1.6007152711620862, 1.6006125632569512, 1.6009698806725112, 1.5997531388585007, 1.6029798076266335, 1.6011167202872791, 1.6028470446910765, 1.6019204715985578, 1.6025075137321585, 1.6022620075833425, 1.6035531404961898, 1.6040471606262408, 1.6023140613277167, 1.60246016278447, 1.602775406171927, 1.603015471366043, 1.601910744590321, 1.6017484071806734, 1.6009751695326004, 1.6023355941662842, 1.6032076373280366, 1.604690281628388, 1.6027043838610595, 1.6037893968654189, 1.6007435290488508, 1.6027907148762093, 1.60468473301341, 1.6028552542766328, 1.6044051357482259, 1.6058864474100825, 1.6038231354433132, 1.6038191013148266, 1.603833876807114, 1.6051843182010994, 1.6059596035476584, 1.607951147019961, 1.6078184780424647, 1.6082284930108608, 1.6090278770340292, 1.619863778108055, 1.612345395025557, 1.6071437820425174, 1.605897444613853, 1.6055406899679274, 1.6047248805097758, 1.6031779532362087, 1.6042908410524892, 1.6050965202657264, 1.605164170069452, 1.6043571727029209, 1.6033217877590011, 1.6035792412624765, 1.6038688147205047, 1.6067442007252735, 1.6073379391324147, 1.6062625123949474, 1.6057553614301634, 1.6051608650946656, 1.6063004097915048, 1.6068394054920214, 1.6062864865771265, 1.6078155140571406, 1.6051837246993492, 1.603505736501346, 1.6063064276095487, 1.6074282856801851, 1.6074437413896834, 1.6067625887092503, 1.608427355246395, 1.6067686542893083, 1.6082240683691842, 1.6083906184276338, 1.6080815332080736, 1.6076099138541762, 1.6047791982519215, 1.605405218495524, 1.6068381771861235, 1.6069106018210475, 1.604978513835099, 1.6108480870038613, 1.6085525308728021, 1.6069176660969926, 1.6060939021102705, 1.6061808274101546, 1.6057902284835164, 1.6062057621177586, 1.6064800417481973, 1.605423132382786, 1.6059042095942255, 1.6070827421883644, 1.6065967429447643, 1.6072045139882756, 1.60649534849502, 1.6065756530793038, 1.606210421263095, 1.6052393960248073, 1.605261391998316, 1.605676707375813, 1.6072666036280114, 1.6073313215487501, 1.6071215124161569, 1.6083962633496238, 1.6085244507233694], 'val_acc': [0.23481116383925252, 0.23645319996404726, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.23316912771445777, 0.23481116374137953, 0.2331691276165848, 0.2331691276165848, 0.238095236186715, 0.23973727221363675, 0.23973727221363675, 0.238095236088842, 0.23645319996404726, 0.238095236088842, 0.23481116383925252, 0.24466338068589397, 0.238095236088842, 0.2348111639371255, 0.23973727221363675, 0.24958948925602417, 0.24794745303335644, 0.23973727231150974, 0.23973727231150974, 0.24958948915815118, 0.2463054169085617, 0.24137930853417747, 0.24137930853417747, 0.2463054169085617, 0.238095236088842, 0.24958948915815118, 0.24137930843630448, 0.24466338058802098, 0.24302134456109922, 0.238095236186715, 0.23481116364350654, 0.238095236186715, 0.23645319996404726, 0.238095236186715, 0.23973727240938272, 0.23973727231150974, 0.23973727231150974, 0.23973727231150974, 0.24137930853417747, 0.23809523628458798, 0.23973727231150974, 0.24137930843630448, 0.23645319996404726, 0.23973727231150974, 0.24137930853417747, 0.2430213446589722, 0.2430213446589722, 0.2430213446589722, 0.2430213446589722, 0.24137930843630448, 0.23809523628458798, 0.23809523628458798, 0.23973727231150974, 0.23973727231150974, 0.23973727231150974, 0.23973727231150974, 0.23809523628458798, 0.23809523628458798, 0.24630541671281572, 0.23645320006192025, 0.24137930843630448, 0.2495894890602782, 0.24137930843630448, 0.23973727231150974, 0.25451559753253544, 0.23645319996404726, 0.24137930843630448, 0.23973727231150974, 0.23645320006192025, 0.23809523638246094, 0.2512315254786919, 0.23645319996404726, 0.2446633809795129, 0.23973727240938272, 0.23809523628458798, 0.23809523638246094, 0.23645320006192025, 0.24302134485471816, 0.23973727240938272, 0.23645320006192025, 0.23645319996404726, 0.23645319986617427, 0.2331691276165848, 0.24302134446322624, 0.2413793082405585, 0.238095236088842, 0.23481116374137953, 0.23316912781233076, 0.23481116403499847, 0.23645319986617427, 0.23973727260512867, 0.2331691281059497, 0.23316912771445777, 0.22495894709048403, 0.23481116413287145, 0.2298850552691223, 0.2266009832152788, 0.2298850553669953, 0.23316912742083884, 0.23481116364350654, 0.23645320006192025, 0.23645319976830131, 0.23152709178540898, 0.22824301914432757, 0.23809523589309606, 0.22988505517124935, 0.2315270912960441, 0.23152709139391706, 0.22824301904645458, 0.2331691275187118, 0.23809523589309606, 0.2331691276165848, 0.22988505517124935, 0.23481116354563358, 0.22988505517124935, 0.22988505744679613, 0.2397372720178908, 0.22988505517124935, 0.23645319976830131, 0.23645319986617427, 0.2331691276165848, 0.23316912771445777, 0.2331691275187118, 0.22331691076994334, 0.22824301914432757, 0.2315270934737179, 0.22824301914432757, 0.23809523589309606, 0.25123152697125484, 0.2397372720178908, 0.21839080427961396, 0.22660098509933366, 0.24137930843630448, 0.23481116364350654, 0.22495894689473808, 0.2266009831174058, 0.23809523589309606, 0.22660098292165984, 0.2331691275187118, 0.238095236088842, 0.22824301934007354, 0.24630541671281572, 0.2331691275187118, 0.2430213442674803, 0.24630541651706978, 0.25287356101624875, 0.24466338029440204, 0.2397372720178908, 0.23645319967042833, 0.24958948886453225, 0.23316912742083884, 0.22824302102838243, 0.22988505734892314, 0.2397372741955646, 0.24794745283761047, 0.23481116572330737, 0.24302134634728112, 0.2463054164191968, 0.23481116354563358, 0.24137930804481256, 0.2430213441696073, 0.23645319967042833, 0.23809523579522307, 0.23809523579522307, 0.23645319996404726, 0.2249589467968651, 0.24466338058802098, 0.2463054169085617, 0.24137930843630448, 0.23973727211576376, 0.23645320006192025, 0.2282430194379465, 0.22824301904645458, 0.2430213446589722, 0.22988505744679613, 0.22824301924220056, 0.22988505744679613, 0.23152709357159088, 0.23481116403499847, 0.24137930872992341, 0.24466338088163994, 0.24137930853417747, 0.23973727240938272, 0.24466338078376695, 0.2463054168106887, 0.2463054168106887, 0.24794745313122943, 0.24794745313122943, 0.2331691275187118, 0.24630541671281572, 0.24630541651706978, 0.24137930804481256, 0.2463054164191968, 0.2397372740976916, 0.2397372741955646, 0.2430213441696073, 0.23645319986617427, 0.23645319967042833, 0.24466338256994882, 0.2315270912960441, 0.23645319967042833, 0.2364532019459751, 0.23809523807076985, 0.23481116354563358, 0.22988505744679613, 0.2364532019459751, 0.22495894907241187, 0.2364532019459751, 0.23481116354563358, 0.2413793083384315, 0.22331691294761713, 0.2397372720178908, 0.2479474548195383, 0.23645320184810212, 0.24137930843630448, 0.22824301904645458, 0.23973727231150974, 0.23481116383925252, 0.22988505744679613, 0.2298850554648683, 0.23152709158966303, 0.23316912969638562, 0.22495894907241187, 0.23645319986617427, 0.23481116374137953, 0.2397372720178908, 0.2413793083384315, 0.23481116383925252, 0.2348111639371255, 0.2413793083384315, 0.23152709158966303, 0.22988505517124935, 0.23316912771445777, 0.23645320006192025, 0.238095236088842, 0.23973727211576376, 0.2413793083384315, 0.23481116364350654, 0.238095236088842, 0.23645319986617427, 0.23481116374137953, 0.2397372720178908, 0.23645319976830131, 0.24137930804481256, 0.2397372741955646, 0.24958948886453225, 0.2397372720178908, 0.2430213442674803, 0.24302134436535328, 0.251231524989327, 0.24466338049014802, 0.23973727240938272, 0.24302134436535328, 0.23481116374137953, 0.2413793082405585, 0.238095236186715, 0.23809523589309606, 0.2397372720178908, 0.2430213442674803, 0.2430213442674803, 0.22167487484089454, 0.24466338039227503, 0.23481116572330737, 0.2364532019459751, 0.24794745254399153, 0.2545155970431705, 0.24466338039227503, 0.24794745254399153, 0.251231524989327, 0.2430213442674803, 0.2315270934737179, 0.24137931032035934, 0.22824301904645458, 0.24958948876665926, 0.2397372720178908, 0.2430213441696073, 0.2331691276165848, 0.2249589467968651, 0.2463054164191968, 0.24630541651706978, 0.22331691067207035, 0.2430213442674803, 0.2479474527397375, 0.24466338049014802, 0.22331691067207035, 0.24630541661494276], 'loss': [1.6083203610943084, 1.6063117284794364, 1.6055151989817376, 1.605263434543257, 1.6051038088005427, 1.6050059893292812, 1.604817274414783, 1.6045330132791884, 1.6043941864487572, 1.6041910508330108, 1.6038976693789817, 1.6047701862558448, 1.6043602622265198, 1.60398073700664, 1.6039403641737953, 1.6037392850774024, 1.6034832715009026, 1.6036059897538328, 1.6029813670524582, 1.602735147926597, 1.602442528579759, 1.6023785329697313, 1.601838644035543, 1.6036319531944008, 1.6047096104102947, 1.6039056148861957, 1.6036791765224763, 1.6028363971984363, 1.6024109805144324, 1.6021222739738605, 1.601974236598005, 1.6011855157738593, 1.6015052296052967, 1.6013970877355619, 1.6016510586474222, 1.6012616139907367, 1.6013053062270555, 1.601164286582132, 1.600926121793978, 1.6004350423323301, 1.6006347317959984, 1.6008708262100846, 1.5992989287484107, 1.6014580533245015, 1.6015071716151932, 1.601441586972262, 1.601014533816422, 1.601416467934908, 1.6010843359224605, 1.6012902429216451, 1.6012736257831175, 1.6006245439791826, 1.5996093563475404, 1.6006196709384175, 1.6008457424704299, 1.6004722672076686, 1.6003375823492876, 1.6007900990010286, 1.600354697621089, 1.6003709032305458, 1.5999478389595079, 1.5995113106968466, 1.5990183579603505, 1.5990132818476621, 1.5986998458662562, 1.5983398600525436, 1.5979189824764244, 1.597192785823125, 1.5981802277006898, 1.5970543549046134, 1.5977127905988595, 1.5978278915495354, 1.5993545245340963, 1.5969214246503136, 1.5977376860514803, 1.5966041826369581, 1.5973439224446824, 1.5972864612661593, 1.5971089959389375, 1.5964293268426977, 1.5967773352805104, 1.5968165935676935, 1.5965812832423059, 1.5977153588124613, 1.5988322133156307, 1.5974140405165342, 1.5980713806113178, 1.596725362918705, 1.5972390852914453, 1.59704186505116, 1.5974683026758307, 1.5974288338263667, 1.5976503152377306, 1.5974571795434187, 1.5964322423053718, 1.5961459709143981, 1.5965603324666895, 1.5978612695631305, 1.599503372484164, 1.5985171813005294, 1.5975957649199626, 1.5971067816325037, 1.5974316320380146, 1.59851925608069, 1.5989747377881278, 1.5979262813160797, 1.5969146836220116, 1.5967617593506769, 1.5957174593418286, 1.5967218706005653, 1.596613786989169, 1.596208294999673, 1.5966152763464612, 1.595388941745249, 1.5962323193677397, 1.5956738537097124, 1.5966039797119047, 1.595850333100227, 1.594944091009653, 1.5948522728326628, 1.595299570075785, 1.59435042889456, 1.594641489610535, 1.5950816212248753, 1.5943395237168736, 1.594944842003699, 1.5949118499148798, 1.594104554863681, 1.594149064919787, 1.5937152827300085, 1.593387790333319, 1.5932810367744805, 1.5931414739796759, 1.592861098528398, 1.5922073576239835, 1.5949272893537485, 1.5933307412223894, 1.592364614357449, 1.593305359728772, 1.5938105252733956, 1.5942043137011832, 1.5931104628701964, 1.5942123678920206, 1.5943288208278052, 1.5936482835354502, 1.5954673894866536, 1.5948200156311725, 1.5951801604069233, 1.5934542405287098, 1.5932166067236992, 1.5909600840456921, 1.5908227434882882, 1.5916895615736317, 1.5932374262467057, 1.5921030374033496, 1.5893168134121434, 1.589998263353195, 1.589526084659036, 1.5877782304673713, 1.5886604199419276, 1.5881699572353638, 1.5899561002269174, 1.589746886699841, 1.5898180627724963, 1.588697996961997, 1.5885234794577534, 1.5874733591471364, 1.5870035539662324, 1.5862492242143384, 1.5852970154133665, 1.5854608159290446, 1.5857667204780501, 1.5854546593934358, 1.5838098901008433, 1.5842123443341107, 1.5864112195048243, 1.5844354056234966, 1.5863411900443953, 1.5882148950741277, 1.5891236619538105, 1.5902769047132017, 1.5898825291001086, 1.5903841341300666, 1.5883124074407182, 1.5879130367380883, 1.5903089499816268, 1.5891235687894254, 1.5882521034021397, 1.5868285174242525, 1.5870041822261145, 1.58913137858898, 1.6293784206652788, 1.6052937818014157, 1.5967309281076985, 1.5967480479324623, 1.5966497622965787, 1.594728540737771, 1.5946101613113277, 1.5932222940104208, 1.5926248927870326, 1.5919712531248402, 1.592191428717157, 1.5914452063229538, 1.5913591192977874, 1.5907977557035442, 1.5894391139674726, 1.5900062949750458, 1.590611571648772, 1.5909853524495934, 1.5894808493110923, 1.5887234561007615, 1.587824533460566, 1.5880870372607723, 1.5873640265552904, 1.5877066481039999, 1.58505789861542, 1.5852392656357626, 1.5854572976639139, 1.58530377808048, 1.5861249413088852, 1.5852661791768161, 1.585983677174766, 1.5853522327156773, 1.5857643285081617, 1.582026813700948, 1.5827616140338185, 1.580287302152332, 1.580552525001385, 1.5812423538133593, 1.5821574400582596, 1.5826940853248141, 1.583212257214885, 1.5840157759507822, 1.5842913070988116, 1.5829740481699761, 1.582952673968838, 1.5914142146981962, 1.618564014023579, 1.592893058416535, 1.591154995003765, 1.5926034148468864, 1.590949018192487, 1.5901410054866782, 1.5883271752441688, 1.5876002710702728, 1.588059045940454, 1.5874565838298758, 1.58702716362305, 1.5857780318974959, 1.5863852299703955, 1.586260071280556, 1.5852524054368664, 1.5851056675157018, 1.5844863973848629, 1.5861161753382282, 1.586815676503113, 1.5831460294292692, 1.5834289688838825, 1.5822660362451229, 1.5823665960123896, 1.5829789886239618, 1.5830972539326005, 1.5821949707653977, 1.5814054795603978, 1.5834128449829696, 1.5831429434018458, 1.581463916737441, 1.5822018685037351, 1.5822762648917321, 1.5817131595200338, 1.5818238537414362, 1.5813520846180846, 1.5810490406514193, 1.5811191129488622, 1.580413072358901, 1.5809211102844019, 1.5808622344563383, 1.5814974992916568, 1.5833187321128297, 1.5814064135052095, 1.5816176188310314, 1.5796221207299517, 1.5795862858300336, 1.5793969559228884, 1.580252980010955, 1.5803021582245091, 1.5794040602580233, 1.5788426317473456, 1.5785722547488046, 1.5783699925675285, 1.5775560365320476, 1.5786110057478324, 1.5780801876859254, 1.5768153324753842, 1.5758964114120608, 1.5758392607651697, 1.575693948704604, 1.5753752814181288, 1.5738515928295849, 1.5740608376399203], 'acc': [0.23039014262952354, 0.2320328538851082, 0.23203285367092313, 0.2328542083195837, 0.23285420951290053, 0.23285420969036816, 0.23285421010037957, 0.23285420992291195, 0.2332648877184494, 0.23326488593765352, 0.2328542102778472, 0.23285420990455322, 0.2328542087112364, 0.23285420892542147, 0.23449691937934203, 0.2361396298516213, 0.2332648881284608, 0.23285421049203225, 0.23203285447258723, 0.24024640622814578, 0.24147843927818158, 0.2394250511878325, 0.24353182615685512, 0.23819301731777387, 0.23655030750640854, 0.23942505177531154, 0.24147843869070254, 0.2377823409113796, 0.2402464058364931, 0.24065708523535875, 0.24229979271516663, 0.24312114878968782, 0.24435318417128107, 0.24476386063275152, 0.24394250598409092, 0.24435318397545472, 0.24312115014211352, 0.2431211507295926, 0.2427104727382288, 0.2431211499646459, 0.24476386139769818, 0.23983572999921912, 0.2447638598310874, 0.24065708325873655, 0.23901437456725314, 0.2410677598609572, 0.2459958920978178, 0.24229979232351392, 0.24229979373101582, 0.24353182697687795, 0.24353182640775764, 0.24476386198517722, 0.24599589346860223, 0.24476385822775917, 0.24271047232821738, 0.24353182758271572, 0.24435318280049662, 0.24065708386457432, 0.24517453705750453, 0.24394250500495918, 0.2451745374491572, 0.24517453686167817, 0.2447638594394347, 0.2455852178087959, 0.24517453881994164, 0.24722792632281168, 0.24599589425190763, 0.24599589346860223, 0.23531827657374513, 0.24558521682966417, 0.24599589425190763, 0.24517453787752735, 0.23490759820908735, 0.24763860531166593, 0.2451745386241153, 0.24640657149668346, 0.2390143739797741, 0.24517453707586323, 0.24312115112124527, 0.24271047428648085, 0.24476386122023056, 0.24394250422165378, 0.24188911632713106, 0.24065708482534734, 0.24353182658522526, 0.24271047134908563, 0.24188911670042504, 0.2427104733073491, 0.23942505060035346, 0.24517453805499498, 0.24394250616155855, 0.2435318283660211, 0.2402464070298099, 0.24147844025731333, 0.24229979551181166, 0.24887063759675507, 0.24353182893514144, 0.2439425043991214, 0.24312115014211352, 0.241889116112946, 0.23860369751830365, 0.24353182758271572, 0.24188911591711965, 0.2353182744196553, 0.2410677630308962, 0.23901437458561187, 0.24517453883830037, 0.24271047115325928, 0.2410677614642854, 0.24887063738256998, 0.2480492815589513, 0.25133470211185716, 0.23901437478143822, 0.24312114896715545, 0.24763860257009707, 0.2447638598310874, 0.2431211507295926, 0.24887063679509094, 0.2509240260603981, 0.24887063640343823, 0.24640657226163015, 0.24804928216478908, 0.2505133470715439, 0.24599589387861365, 0.2484599597828589, 0.24640657108667205, 0.24353182697687795, 0.24804928018816688, 0.2459958936827873, 0.24435318456293376, 0.2525667347702402, 0.24969199067880485, 0.2525667329710857, 0.25174537812659875, 0.2537987676244497, 0.2509240224988064, 0.24845995996032652, 0.24887063603014428, 0.2484599578062367, 0.24271047115325928, 0.24394250400746872, 0.2492813157839452, 0.25503080263274897, 0.2616016425819612, 0.2464065701075403, 0.24845995839371574, 0.2488706358159592, 0.24558521547723844, 0.2476386039408815, 0.24845995819788938, 0.24928131202652715, 0.24271047271987006, 0.2501026672626668, 0.24722792653699674, 0.2513347001168762, 0.24558521761296956, 0.24887063816587537, 0.24024640761728894, 0.24969198964459696, 0.2505133460924121, 0.24763860354922881, 0.24435318321050806, 0.24229979433685359, 0.24065708482534734, 0.24599589483938666, 0.2447638594394347, 0.2509240241021346, 0.2537987686035814, 0.2513347005085289, 0.24845995919537986, 0.2546201212572611, 0.2607802873274629, 0.25954825564821155, 0.2636550324163887, 0.25338809000637985, 0.2611909647497064, 0.25667351038181807, 0.2574948685369942, 0.2570841887648346, 0.25338809039803256, 0.2505133464840648, 0.25133470367846794, 0.25092402429796096, 0.25462012366225345, 0.25215605656469137, 0.2574948685369942, 0.24887063562013284, 0.2616016425636025, 0.2579055451759323, 0.2607802892857264, 0.2599589322871496, 0.2324435326781361, 0.24147844162809776, 0.24147843929654028, 0.24394250694486394, 0.24517453864247402, 0.24517453903412673, 0.24353182756435698, 0.24763860492001324, 0.2468172485089155, 0.24928131403986678, 0.24024640761728894, 0.25010266986348545, 0.25092402347793813, 0.2509240254912778, 0.25379876901359283, 0.25174538012157965, 0.24353182738688936, 0.24476385984944612, 0.249691993402015, 0.25174537972992694, 0.2566735095434365, 0.25913757763848905, 0.252156056368865, 0.2537987686035814, 0.2558521563022778, 0.2554414782925553, 0.25831621942823674, 0.2566735123217228, 0.2583162224207081, 0.254620123252242, 0.26119096455388, 0.25708419033144536, 0.2628336764153024, 0.2620123207691514, 0.2521560579354758, 0.26694045375259995, 0.26324435381918715, 0.26242299600058755, 0.2661190977331549, 0.26324435421083986, 0.2665297751553984, 0.25626283372452124, 0.253798767232797, 0.25954825564821155, 0.26242299819139486, 0.2509240241021346, 0.24229979627675835, 0.24353182658522526, 0.2472279271244758, 0.25338809000637985, 0.2521560567605177, 0.26119096592466445, 0.2574948665787307, 0.26160164295525523, 0.25544147927168703, 0.25872689782961195, 0.2644763866733966, 0.26406571042611127, 0.2595482550607325, 0.2620123207875101, 0.2636550318472684, 0.26160164393438695, 0.26036961166765654, 0.25913757626770456, 0.2583162202115421, 0.2574948671662098, 0.25790554278929867, 0.2632443512367272, 0.2632443520200326, 0.2574948673620361, 0.2628336756136383, 0.25954825527491754, 0.2616016441302133, 0.26529774153624225, 0.2554414775092499, 0.2636550328080414, 0.2603696114718302, 0.2640657078803687, 0.26324435421083986, 0.2558521566939305, 0.25995893248297597, 0.26652977178963305, 0.2640657094469795, 0.2673511307831907, 0.268583161874963, 0.25954825267409887, 0.26529774190953626, 0.2579055440009742, 0.2632443547983189, 0.26201232233576216, 0.2677618070304761, 0.26529774011038165, 0.2677618048396688, 0.26776180761795515, 0.26611909734150224, 0.2702258731305477, 0.27063655094444383, 0.26817248422017576, 0.26611909792898125, 0.2648870642914664, 0.2603696108843512, 0.2657084201150851, 0.26160164336526665, 0.2673511270257727, 0.26365503065395157, 0.26283367522198564, 0.2648870633123347, 0.2673511270257727, 0.270225874660441]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
