{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf44.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 15:36:09 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'AllShfRnd', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '04', '05', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000013C837BBE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000013CD7906EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6075, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6059, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6058, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6056, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6054, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6043, Accuracy:0.2329, Validation Loss:1.6035, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6028, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6032, Accuracy:0.2329, Validation Loss:1.6018, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6020, Accuracy:0.2329, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6008, Accuracy:0.2329, Validation Loss:1.5986, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.5991, Accuracy:0.2324, Validation Loss:1.5962, Validation Accuracy:0.2365\n",
    "Epoch #19: Loss:1.5967, Accuracy:0.2374, Validation Loss:1.5929, Validation Accuracy:0.2430\n",
    "Epoch #20: Loss:1.5936, Accuracy:0.2456, Validation Loss:1.5884, Validation Accuracy:0.2529\n",
    "Epoch #21: Loss:1.5892, Accuracy:0.2550, Validation Loss:1.5828, Validation Accuracy:0.2529\n",
    "Epoch #22: Loss:1.5844, Accuracy:0.2522, Validation Loss:1.5761, Validation Accuracy:0.2512\n",
    "Epoch #23: Loss:1.5784, Accuracy:0.2390, Validation Loss:1.5697, Validation Accuracy:0.2430\n",
    "Epoch #24: Loss:1.5735, Accuracy:0.2353, Validation Loss:1.5660, Validation Accuracy:0.2709\n",
    "Epoch #25: Loss:1.5709, Accuracy:0.2464, Validation Loss:1.5628, Validation Accuracy:0.2742\n",
    "Epoch #26: Loss:1.5698, Accuracy:0.2567, Validation Loss:1.5614, Validation Accuracy:0.2726\n",
    "Epoch #27: Loss:1.5687, Accuracy:0.2641, Validation Loss:1.5619, Validation Accuracy:0.2660\n",
    "Epoch #28: Loss:1.5673, Accuracy:0.2727, Validation Loss:1.5595, Validation Accuracy:0.2677\n",
    "Epoch #29: Loss:1.5655, Accuracy:0.2632, Validation Loss:1.5601, Validation Accuracy:0.2512\n",
    "Epoch #30: Loss:1.5658, Accuracy:0.2702, Validation Loss:1.5598, Validation Accuracy:0.2545\n",
    "Epoch #31: Loss:1.5651, Accuracy:0.2641, Validation Loss:1.5585, Validation Accuracy:0.2627\n",
    "Epoch #32: Loss:1.5635, Accuracy:0.2669, Validation Loss:1.5604, Validation Accuracy:0.2709\n",
    "Epoch #33: Loss:1.5629, Accuracy:0.2624, Validation Loss:1.5580, Validation Accuracy:0.2709\n",
    "Epoch #34: Loss:1.5619, Accuracy:0.2669, Validation Loss:1.5579, Validation Accuracy:0.2496\n",
    "Epoch #35: Loss:1.5610, Accuracy:0.2735, Validation Loss:1.5584, Validation Accuracy:0.2726\n",
    "Epoch #36: Loss:1.5597, Accuracy:0.2760, Validation Loss:1.5575, Validation Accuracy:0.2660\n",
    "Epoch #37: Loss:1.5603, Accuracy:0.2723, Validation Loss:1.5572, Validation Accuracy:0.2644\n",
    "Epoch #38: Loss:1.5598, Accuracy:0.2723, Validation Loss:1.5609, Validation Accuracy:0.2562\n",
    "Epoch #39: Loss:1.5594, Accuracy:0.2649, Validation Loss:1.5575, Validation Accuracy:0.2496\n",
    "Epoch #40: Loss:1.5575, Accuracy:0.2665, Validation Loss:1.5583, Validation Accuracy:0.2479\n",
    "Epoch #41: Loss:1.5564, Accuracy:0.2789, Validation Loss:1.5565, Validation Accuracy:0.2660\n",
    "Epoch #42: Loss:1.5561, Accuracy:0.2727, Validation Loss:1.5563, Validation Accuracy:0.2611\n",
    "Epoch #43: Loss:1.5539, Accuracy:0.2719, Validation Loss:1.5568, Validation Accuracy:0.2545\n",
    "Epoch #44: Loss:1.5532, Accuracy:0.2780, Validation Loss:1.5561, Validation Accuracy:0.2479\n",
    "Epoch #45: Loss:1.5520, Accuracy:0.2784, Validation Loss:1.5564, Validation Accuracy:0.2512\n",
    "Epoch #46: Loss:1.5511, Accuracy:0.2834, Validation Loss:1.5563, Validation Accuracy:0.2463\n",
    "Epoch #47: Loss:1.5500, Accuracy:0.2875, Validation Loss:1.5565, Validation Accuracy:0.2397\n",
    "Epoch #48: Loss:1.5491, Accuracy:0.2862, Validation Loss:1.5566, Validation Accuracy:0.2496\n",
    "Epoch #49: Loss:1.5482, Accuracy:0.2871, Validation Loss:1.5564, Validation Accuracy:0.2512\n",
    "Epoch #50: Loss:1.5470, Accuracy:0.2916, Validation Loss:1.5567, Validation Accuracy:0.2496\n",
    "Epoch #51: Loss:1.5460, Accuracy:0.2862, Validation Loss:1.5564, Validation Accuracy:0.2512\n",
    "Epoch #52: Loss:1.5449, Accuracy:0.2924, Validation Loss:1.5566, Validation Accuracy:0.2430\n",
    "Epoch #53: Loss:1.5437, Accuracy:0.2924, Validation Loss:1.5576, Validation Accuracy:0.2496\n",
    "Epoch #54: Loss:1.5431, Accuracy:0.2825, Validation Loss:1.5580, Validation Accuracy:0.2299\n",
    "Epoch #55: Loss:1.5432, Accuracy:0.2883, Validation Loss:1.5592, Validation Accuracy:0.2562\n",
    "Epoch #56: Loss:1.5426, Accuracy:0.2850, Validation Loss:1.5567, Validation Accuracy:0.2447\n",
    "Epoch #57: Loss:1.5396, Accuracy:0.2908, Validation Loss:1.5571, Validation Accuracy:0.2529\n",
    "Epoch #58: Loss:1.5393, Accuracy:0.2986, Validation Loss:1.5594, Validation Accuracy:0.2578\n",
    "Epoch #59: Loss:1.5380, Accuracy:0.2887, Validation Loss:1.5588, Validation Accuracy:0.2447\n",
    "Epoch #60: Loss:1.5404, Accuracy:0.2912, Validation Loss:1.5580, Validation Accuracy:0.2562\n",
    "Epoch #61: Loss:1.5368, Accuracy:0.2916, Validation Loss:1.5562, Validation Accuracy:0.2562\n",
    "Epoch #62: Loss:1.5346, Accuracy:0.2936, Validation Loss:1.5561, Validation Accuracy:0.2545\n",
    "Epoch #63: Loss:1.5347, Accuracy:0.2986, Validation Loss:1.5636, Validation Accuracy:0.2397\n",
    "Epoch #64: Loss:1.5320, Accuracy:0.2908, Validation Loss:1.5580, Validation Accuracy:0.2496\n",
    "Epoch #65: Loss:1.5357, Accuracy:0.2990, Validation Loss:1.5559, Validation Accuracy:0.2545\n",
    "Epoch #66: Loss:1.5325, Accuracy:0.3010, Validation Loss:1.5547, Validation Accuracy:0.2578\n",
    "Epoch #67: Loss:1.5286, Accuracy:0.3010, Validation Loss:1.5544, Validation Accuracy:0.2578\n",
    "Epoch #68: Loss:1.5270, Accuracy:0.3010, Validation Loss:1.5537, Validation Accuracy:0.2627\n",
    "Epoch #69: Loss:1.5262, Accuracy:0.3060, Validation Loss:1.5544, Validation Accuracy:0.2529\n",
    "Epoch #70: Loss:1.5251, Accuracy:0.3088, Validation Loss:1.5581, Validation Accuracy:0.2545\n",
    "Epoch #71: Loss:1.5241, Accuracy:0.2969, Validation Loss:1.5566, Validation Accuracy:0.2627\n",
    "Epoch #72: Loss:1.5240, Accuracy:0.3080, Validation Loss:1.5553, Validation Accuracy:0.2644\n",
    "Epoch #73: Loss:1.5230, Accuracy:0.3072, Validation Loss:1.5540, Validation Accuracy:0.2529\n",
    "Epoch #74: Loss:1.5255, Accuracy:0.2830, Validation Loss:1.5565, Validation Accuracy:0.2578\n",
    "Epoch #75: Loss:1.5241, Accuracy:0.2990, Validation Loss:1.5565, Validation Accuracy:0.2545\n",
    "Epoch #76: Loss:1.5304, Accuracy:0.3018, Validation Loss:1.5479, Validation Accuracy:0.2627\n",
    "Epoch #77: Loss:1.5196, Accuracy:0.3047, Validation Loss:1.5456, Validation Accuracy:0.2693\n",
    "Epoch #78: Loss:1.5144, Accuracy:0.3084, Validation Loss:1.5501, Validation Accuracy:0.2611\n",
    "Epoch #79: Loss:1.5129, Accuracy:0.3105, Validation Loss:1.5460, Validation Accuracy:0.2677\n",
    "Epoch #80: Loss:1.5128, Accuracy:0.3101, Validation Loss:1.5507, Validation Accuracy:0.2644\n",
    "Epoch #81: Loss:1.5102, Accuracy:0.3158, Validation Loss:1.5447, Validation Accuracy:0.2726\n",
    "Epoch #82: Loss:1.5090, Accuracy:0.3158, Validation Loss:1.5456, Validation Accuracy:0.2693\n",
    "Epoch #83: Loss:1.5091, Accuracy:0.3125, Validation Loss:1.5439, Validation Accuracy:0.2644\n",
    "Epoch #84: Loss:1.5046, Accuracy:0.3179, Validation Loss:1.5445, Validation Accuracy:0.2677\n",
    "Epoch #85: Loss:1.5012, Accuracy:0.3035, Validation Loss:1.5429, Validation Accuracy:0.2709\n",
    "Epoch #86: Loss:1.5003, Accuracy:0.3121, Validation Loss:1.5417, Validation Accuracy:0.2627\n",
    "Epoch #87: Loss:1.4995, Accuracy:0.3179, Validation Loss:1.5774, Validation Accuracy:0.2841\n",
    "Epoch #88: Loss:1.5157, Accuracy:0.3068, Validation Loss:1.5370, Validation Accuracy:0.2759\n",
    "Epoch #89: Loss:1.5079, Accuracy:0.3125, Validation Loss:1.5447, Validation Accuracy:0.2594\n",
    "Epoch #90: Loss:1.5074, Accuracy:0.3187, Validation Loss:1.5641, Validation Accuracy:0.2841\n",
    "Epoch #91: Loss:1.5117, Accuracy:0.3142, Validation Loss:1.5382, Validation Accuracy:0.2644\n",
    "Epoch #92: Loss:1.4938, Accuracy:0.3203, Validation Loss:1.5358, Validation Accuracy:0.2923\n",
    "Epoch #93: Loss:1.4904, Accuracy:0.3183, Validation Loss:1.5319, Validation Accuracy:0.2841\n",
    "Epoch #94: Loss:1.4849, Accuracy:0.3265, Validation Loss:1.5300, Validation Accuracy:0.2956\n",
    "Epoch #95: Loss:1.4830, Accuracy:0.3265, Validation Loss:1.5317, Validation Accuracy:0.2923\n",
    "Epoch #96: Loss:1.4793, Accuracy:0.3290, Validation Loss:1.5323, Validation Accuracy:0.2709\n",
    "Epoch #97: Loss:1.4792, Accuracy:0.3257, Validation Loss:1.5266, Validation Accuracy:0.3005\n",
    "Epoch #98: Loss:1.4762, Accuracy:0.3310, Validation Loss:1.5241, Validation Accuracy:0.3021\n",
    "Epoch #99: Loss:1.4737, Accuracy:0.3298, Validation Loss:1.5202, Validation Accuracy:0.2989\n",
    "Epoch #100: Loss:1.4672, Accuracy:0.3429, Validation Loss:1.5167, Validation Accuracy:0.3136\n",
    "Epoch #101: Loss:1.4654, Accuracy:0.3376, Validation Loss:1.5188, Validation Accuracy:0.2939\n",
    "Epoch #102: Loss:1.4661, Accuracy:0.3347, Validation Loss:1.5111, Validation Accuracy:0.2972\n",
    "Epoch #103: Loss:1.4610, Accuracy:0.3503, Validation Loss:1.5121, Validation Accuracy:0.2939\n",
    "Epoch #104: Loss:1.4632, Accuracy:0.3429, Validation Loss:1.5436, Validation Accuracy:0.2578\n",
    "Epoch #105: Loss:1.4784, Accuracy:0.3310, Validation Loss:1.5003, Validation Accuracy:0.3054\n",
    "Epoch #106: Loss:1.4663, Accuracy:0.3310, Validation Loss:1.5166, Validation Accuracy:0.3120\n",
    "Epoch #107: Loss:1.4638, Accuracy:0.3413, Validation Loss:1.5094, Validation Accuracy:0.2956\n",
    "Epoch #108: Loss:1.4582, Accuracy:0.3421, Validation Loss:1.5085, Validation Accuracy:0.3005\n",
    "Epoch #109: Loss:1.4608, Accuracy:0.3413, Validation Loss:1.5271, Validation Accuracy:0.2759\n",
    "Epoch #110: Loss:1.4639, Accuracy:0.3310, Validation Loss:1.5096, Validation Accuracy:0.2923\n",
    "Epoch #111: Loss:1.4577, Accuracy:0.3372, Validation Loss:1.4965, Validation Accuracy:0.3087\n",
    "Epoch #112: Loss:1.4466, Accuracy:0.3458, Validation Loss:1.4946, Validation Accuracy:0.3038\n",
    "Epoch #113: Loss:1.4383, Accuracy:0.3528, Validation Loss:1.4894, Validation Accuracy:0.3087\n",
    "Epoch #114: Loss:1.4360, Accuracy:0.3556, Validation Loss:1.4885, Validation Accuracy:0.3153\n",
    "Epoch #115: Loss:1.4350, Accuracy:0.3532, Validation Loss:1.4927, Validation Accuracy:0.3103\n",
    "Epoch #116: Loss:1.4326, Accuracy:0.3593, Validation Loss:1.4869, Validation Accuracy:0.3120\n",
    "Epoch #117: Loss:1.4355, Accuracy:0.3536, Validation Loss:1.4874, Validation Accuracy:0.3120\n",
    "Epoch #118: Loss:1.4383, Accuracy:0.3548, Validation Loss:1.4958, Validation Accuracy:0.3038\n",
    "Epoch #119: Loss:1.4456, Accuracy:0.3581, Validation Loss:1.4846, Validation Accuracy:0.3103\n",
    "Epoch #120: Loss:1.4274, Accuracy:0.3655, Validation Loss:1.4801, Validation Accuracy:0.3153\n",
    "Epoch #121: Loss:1.4229, Accuracy:0.3618, Validation Loss:1.4763, Validation Accuracy:0.3054\n",
    "Epoch #122: Loss:1.4209, Accuracy:0.3708, Validation Loss:1.4778, Validation Accuracy:0.3103\n",
    "Epoch #123: Loss:1.4174, Accuracy:0.3639, Validation Loss:1.4763, Validation Accuracy:0.3136\n",
    "Epoch #124: Loss:1.4195, Accuracy:0.3610, Validation Loss:1.4810, Validation Accuracy:0.3153\n",
    "Epoch #125: Loss:1.4293, Accuracy:0.3684, Validation Loss:1.4718, Validation Accuracy:0.3186\n",
    "Epoch #126: Loss:1.4172, Accuracy:0.3717, Validation Loss:1.4728, Validation Accuracy:0.3153\n",
    "Epoch #127: Loss:1.4142, Accuracy:0.3733, Validation Loss:1.4687, Validation Accuracy:0.3218\n",
    "Epoch #128: Loss:1.4108, Accuracy:0.3713, Validation Loss:1.4731, Validation Accuracy:0.3218\n",
    "Epoch #129: Loss:1.4111, Accuracy:0.3663, Validation Loss:1.4710, Validation Accuracy:0.3136\n",
    "Epoch #130: Loss:1.4088, Accuracy:0.3778, Validation Loss:1.4698, Validation Accuracy:0.3169\n",
    "Epoch #131: Loss:1.4079, Accuracy:0.3758, Validation Loss:1.4925, Validation Accuracy:0.3005\n",
    "Epoch #132: Loss:1.4147, Accuracy:0.3589, Validation Loss:1.4635, Validation Accuracy:0.3120\n",
    "Epoch #133: Loss:1.4113, Accuracy:0.3647, Validation Loss:1.4665, Validation Accuracy:0.3136\n",
    "Epoch #134: Loss:1.4055, Accuracy:0.3708, Validation Loss:1.4594, Validation Accuracy:0.3103\n",
    "Epoch #135: Loss:1.4022, Accuracy:0.3762, Validation Loss:1.4778, Validation Accuracy:0.3169\n",
    "Epoch #136: Loss:1.4074, Accuracy:0.3741, Validation Loss:1.4751, Validation Accuracy:0.3054\n",
    "Epoch #137: Loss:1.4052, Accuracy:0.3754, Validation Loss:1.4628, Validation Accuracy:0.3103\n",
    "Epoch #138: Loss:1.4030, Accuracy:0.3684, Validation Loss:1.4531, Validation Accuracy:0.3153\n",
    "Epoch #139: Loss:1.4062, Accuracy:0.3741, Validation Loss:1.4740, Validation Accuracy:0.3038\n",
    "Epoch #140: Loss:1.4094, Accuracy:0.3622, Validation Loss:1.4537, Validation Accuracy:0.3136\n",
    "Epoch #141: Loss:1.3994, Accuracy:0.3708, Validation Loss:1.4537, Validation Accuracy:0.3136\n",
    "Epoch #142: Loss:1.3960, Accuracy:0.3799, Validation Loss:1.4520, Validation Accuracy:0.3087\n",
    "Epoch #143: Loss:1.3929, Accuracy:0.3791, Validation Loss:1.4749, Validation Accuracy:0.3136\n",
    "Epoch #144: Loss:1.3977, Accuracy:0.3700, Validation Loss:1.4565, Validation Accuracy:0.3087\n",
    "Epoch #145: Loss:1.3908, Accuracy:0.3799, Validation Loss:1.4537, Validation Accuracy:0.3136\n",
    "Epoch #146: Loss:1.3921, Accuracy:0.3713, Validation Loss:1.4525, Validation Accuracy:0.3202\n",
    "Epoch #147: Loss:1.4016, Accuracy:0.3754, Validation Loss:1.4553, Validation Accuracy:0.3169\n",
    "Epoch #148: Loss:1.4031, Accuracy:0.3721, Validation Loss:1.4716, Validation Accuracy:0.3071\n",
    "Epoch #149: Loss:1.4125, Accuracy:0.3536, Validation Loss:1.4510, Validation Accuracy:0.3071\n",
    "Epoch #150: Loss:1.3988, Accuracy:0.3807, Validation Loss:1.4787, Validation Accuracy:0.3120\n",
    "Epoch #151: Loss:1.3980, Accuracy:0.3828, Validation Loss:1.4460, Validation Accuracy:0.3136\n",
    "Epoch #152: Loss:1.3887, Accuracy:0.3840, Validation Loss:1.4480, Validation Accuracy:0.3103\n",
    "Epoch #153: Loss:1.3877, Accuracy:0.3799, Validation Loss:1.4467, Validation Accuracy:0.3251\n",
    "Epoch #154: Loss:1.3857, Accuracy:0.3832, Validation Loss:1.4606, Validation Accuracy:0.3021\n",
    "Epoch #155: Loss:1.3896, Accuracy:0.3766, Validation Loss:1.4715, Validation Accuracy:0.2906\n",
    "Epoch #156: Loss:1.3928, Accuracy:0.3704, Validation Loss:1.4451, Validation Accuracy:0.3186\n",
    "Epoch #157: Loss:1.3825, Accuracy:0.3737, Validation Loss:1.4436, Validation Accuracy:0.3153\n",
    "Epoch #158: Loss:1.3802, Accuracy:0.3803, Validation Loss:1.4429, Validation Accuracy:0.3186\n",
    "Epoch #159: Loss:1.3778, Accuracy:0.3860, Validation Loss:1.4470, Validation Accuracy:0.3153\n",
    "Epoch #160: Loss:1.3777, Accuracy:0.3860, Validation Loss:1.4417, Validation Accuracy:0.3136\n",
    "Epoch #161: Loss:1.3765, Accuracy:0.3828, Validation Loss:1.4489, Validation Accuracy:0.3153\n",
    "Epoch #162: Loss:1.3774, Accuracy:0.3823, Validation Loss:1.4463, Validation Accuracy:0.3136\n",
    "Epoch #163: Loss:1.3739, Accuracy:0.3889, Validation Loss:1.4409, Validation Accuracy:0.3300\n",
    "Epoch #164: Loss:1.3735, Accuracy:0.3881, Validation Loss:1.4387, Validation Accuracy:0.3202\n",
    "Epoch #165: Loss:1.3740, Accuracy:0.3823, Validation Loss:1.4425, Validation Accuracy:0.3317\n",
    "Epoch #166: Loss:1.3782, Accuracy:0.3906, Validation Loss:1.4387, Validation Accuracy:0.3153\n",
    "Epoch #167: Loss:1.3832, Accuracy:0.3815, Validation Loss:1.4436, Validation Accuracy:0.3268\n",
    "Epoch #168: Loss:1.3794, Accuracy:0.3811, Validation Loss:1.4360, Validation Accuracy:0.3235\n",
    "Epoch #169: Loss:1.3710, Accuracy:0.3947, Validation Loss:1.4377, Validation Accuracy:0.3300\n",
    "Epoch #170: Loss:1.3691, Accuracy:0.3959, Validation Loss:1.4423, Validation Accuracy:0.3218\n",
    "Epoch #171: Loss:1.3726, Accuracy:0.3901, Validation Loss:1.4466, Validation Accuracy:0.3071\n",
    "Epoch #172: Loss:1.3744, Accuracy:0.3823, Validation Loss:1.4364, Validation Accuracy:0.3169\n",
    "Epoch #173: Loss:1.3708, Accuracy:0.3918, Validation Loss:1.4376, Validation Accuracy:0.3350\n",
    "Epoch #174: Loss:1.3669, Accuracy:0.3959, Validation Loss:1.4343, Validation Accuracy:0.3251\n",
    "Epoch #175: Loss:1.3714, Accuracy:0.3848, Validation Loss:1.4584, Validation Accuracy:0.3251\n",
    "Epoch #176: Loss:1.3837, Accuracy:0.3774, Validation Loss:1.4430, Validation Accuracy:0.3186\n",
    "Epoch #177: Loss:1.3840, Accuracy:0.3819, Validation Loss:1.4359, Validation Accuracy:0.3350\n",
    "Epoch #178: Loss:1.3755, Accuracy:0.3889, Validation Loss:1.4446, Validation Accuracy:0.3136\n",
    "Epoch #179: Loss:1.3682, Accuracy:0.3926, Validation Loss:1.4428, Validation Accuracy:0.3218\n",
    "Epoch #180: Loss:1.3641, Accuracy:0.3951, Validation Loss:1.4398, Validation Accuracy:0.3202\n",
    "Epoch #181: Loss:1.3637, Accuracy:0.3963, Validation Loss:1.4323, Validation Accuracy:0.3284\n",
    "Epoch #182: Loss:1.3607, Accuracy:0.3992, Validation Loss:1.4362, Validation Accuracy:0.3300\n",
    "Epoch #183: Loss:1.3604, Accuracy:0.3979, Validation Loss:1.4352, Validation Accuracy:0.3383\n",
    "Epoch #184: Loss:1.3619, Accuracy:0.3971, Validation Loss:1.4308, Validation Accuracy:0.3251\n",
    "Epoch #185: Loss:1.3611, Accuracy:0.4029, Validation Loss:1.4305, Validation Accuracy:0.3251\n",
    "Epoch #186: Loss:1.3597, Accuracy:0.4004, Validation Loss:1.4335, Validation Accuracy:0.3284\n",
    "Epoch #187: Loss:1.3590, Accuracy:0.4033, Validation Loss:1.4352, Validation Accuracy:0.3284\n",
    "Epoch #188: Loss:1.3597, Accuracy:0.4004, Validation Loss:1.4361, Validation Accuracy:0.3317\n",
    "Epoch #189: Loss:1.3584, Accuracy:0.4004, Validation Loss:1.4330, Validation Accuracy:0.3333\n",
    "Epoch #190: Loss:1.3589, Accuracy:0.4029, Validation Loss:1.4361, Validation Accuracy:0.3153\n",
    "Epoch #191: Loss:1.3587, Accuracy:0.4033, Validation Loss:1.4597, Validation Accuracy:0.2972\n",
    "Epoch #192: Loss:1.3606, Accuracy:0.3938, Validation Loss:1.4522, Validation Accuracy:0.3087\n",
    "Epoch #193: Loss:1.3590, Accuracy:0.4041, Validation Loss:1.4301, Validation Accuracy:0.3300\n",
    "Epoch #194: Loss:1.3552, Accuracy:0.4057, Validation Loss:1.4354, Validation Accuracy:0.3333\n",
    "Epoch #195: Loss:1.3531, Accuracy:0.4074, Validation Loss:1.4341, Validation Accuracy:0.3268\n",
    "Epoch #196: Loss:1.3524, Accuracy:0.4021, Validation Loss:1.4337, Validation Accuracy:0.3333\n",
    "Epoch #197: Loss:1.3537, Accuracy:0.4000, Validation Loss:1.4512, Validation Accuracy:0.3054\n",
    "Epoch #198: Loss:1.3582, Accuracy:0.3934, Validation Loss:1.4546, Validation Accuracy:0.3120\n",
    "Epoch #199: Loss:1.3607, Accuracy:0.3918, Validation Loss:1.4533, Validation Accuracy:0.3038\n",
    "Epoch #200: Loss:1.3673, Accuracy:0.4012, Validation Loss:1.4532, Validation Accuracy:0.3054\n",
    "Epoch #201: Loss:1.3596, Accuracy:0.3963, Validation Loss:1.4428, Validation Accuracy:0.3169\n",
    "Epoch #202: Loss:1.3529, Accuracy:0.3955, Validation Loss:1.4292, Validation Accuracy:0.3300\n",
    "Epoch #203: Loss:1.3494, Accuracy:0.4066, Validation Loss:1.4334, Validation Accuracy:0.3317\n",
    "Epoch #204: Loss:1.3469, Accuracy:0.3996, Validation Loss:1.4523, Validation Accuracy:0.3054\n",
    "Epoch #205: Loss:1.3541, Accuracy:0.4090, Validation Loss:1.4495, Validation Accuracy:0.3153\n",
    "Epoch #206: Loss:1.3579, Accuracy:0.3988, Validation Loss:1.4398, Validation Accuracy:0.3120\n",
    "Epoch #207: Loss:1.3584, Accuracy:0.4057, Validation Loss:1.4267, Validation Accuracy:0.3251\n",
    "Epoch #208: Loss:1.3516, Accuracy:0.4074, Validation Loss:1.4340, Validation Accuracy:0.3268\n",
    "Epoch #209: Loss:1.3509, Accuracy:0.3947, Validation Loss:1.4271, Validation Accuracy:0.3300\n",
    "Epoch #210: Loss:1.3599, Accuracy:0.3918, Validation Loss:1.4269, Validation Accuracy:0.3383\n",
    "Epoch #211: Loss:1.3505, Accuracy:0.3943, Validation Loss:1.4531, Validation Accuracy:0.3103\n",
    "Epoch #212: Loss:1.3563, Accuracy:0.4029, Validation Loss:1.4311, Validation Accuracy:0.3366\n",
    "Epoch #213: Loss:1.3561, Accuracy:0.3910, Validation Loss:1.4269, Validation Accuracy:0.3350\n",
    "Epoch #214: Loss:1.3646, Accuracy:0.3943, Validation Loss:1.4522, Validation Accuracy:0.3317\n",
    "Epoch #215: Loss:1.3820, Accuracy:0.3807, Validation Loss:1.4352, Validation Accuracy:0.3120\n",
    "Epoch #216: Loss:1.3637, Accuracy:0.3860, Validation Loss:1.4403, Validation Accuracy:0.3563\n",
    "Epoch #217: Loss:1.3546, Accuracy:0.3984, Validation Loss:1.4482, Validation Accuracy:0.3300\n",
    "Epoch #218: Loss:1.3525, Accuracy:0.3938, Validation Loss:1.4401, Validation Accuracy:0.3481\n",
    "Epoch #219: Loss:1.3495, Accuracy:0.4086, Validation Loss:1.4246, Validation Accuracy:0.3350\n",
    "Epoch #220: Loss:1.3550, Accuracy:0.4012, Validation Loss:1.4374, Validation Accuracy:0.3235\n",
    "Epoch #221: Loss:1.3612, Accuracy:0.3922, Validation Loss:1.4325, Validation Accuracy:0.3399\n",
    "Epoch #222: Loss:1.3423, Accuracy:0.4000, Validation Loss:1.4349, Validation Accuracy:0.3186\n",
    "Epoch #223: Loss:1.3439, Accuracy:0.4066, Validation Loss:1.4299, Validation Accuracy:0.3350\n",
    "Epoch #224: Loss:1.3458, Accuracy:0.4078, Validation Loss:1.4284, Validation Accuracy:0.3333\n",
    "Epoch #225: Loss:1.3388, Accuracy:0.4222, Validation Loss:1.4248, Validation Accuracy:0.3333\n",
    "Epoch #226: Loss:1.3407, Accuracy:0.4078, Validation Loss:1.4270, Validation Accuracy:0.3448\n",
    "Epoch #227: Loss:1.3411, Accuracy:0.4049, Validation Loss:1.4474, Validation Accuracy:0.2989\n",
    "Epoch #228: Loss:1.3412, Accuracy:0.4078, Validation Loss:1.4405, Validation Accuracy:0.3284\n",
    "Epoch #229: Loss:1.3391, Accuracy:0.4111, Validation Loss:1.4247, Validation Accuracy:0.3366\n",
    "Epoch #230: Loss:1.3392, Accuracy:0.4172, Validation Loss:1.4239, Validation Accuracy:0.3448\n",
    "Epoch #231: Loss:1.3385, Accuracy:0.4111, Validation Loss:1.4288, Validation Accuracy:0.3432\n",
    "Epoch #232: Loss:1.3403, Accuracy:0.4049, Validation Loss:1.4524, Validation Accuracy:0.3103\n",
    "Epoch #233: Loss:1.3458, Accuracy:0.4152, Validation Loss:1.4519, Validation Accuracy:0.3120\n",
    "Epoch #234: Loss:1.3485, Accuracy:0.3988, Validation Loss:1.4238, Validation Accuracy:0.3350\n",
    "Epoch #235: Loss:1.3518, Accuracy:0.4049, Validation Loss:1.4302, Validation Accuracy:0.3366\n",
    "Epoch #236: Loss:1.3428, Accuracy:0.4094, Validation Loss:1.4225, Validation Accuracy:0.3350\n",
    "Epoch #237: Loss:1.3336, Accuracy:0.4197, Validation Loss:1.4240, Validation Accuracy:0.3399\n",
    "Epoch #238: Loss:1.3321, Accuracy:0.4172, Validation Loss:1.4386, Validation Accuracy:0.3268\n",
    "Epoch #239: Loss:1.3333, Accuracy:0.4119, Validation Loss:1.4351, Validation Accuracy:0.3399\n",
    "Epoch #240: Loss:1.3341, Accuracy:0.4177, Validation Loss:1.4272, Validation Accuracy:0.3415\n",
    "Epoch #241: Loss:1.3356, Accuracy:0.4181, Validation Loss:1.4237, Validation Accuracy:0.3366\n",
    "Epoch #242: Loss:1.3307, Accuracy:0.4148, Validation Loss:1.4243, Validation Accuracy:0.3448\n",
    "Epoch #243: Loss:1.3293, Accuracy:0.4189, Validation Loss:1.4250, Validation Accuracy:0.3448\n",
    "Epoch #244: Loss:1.3294, Accuracy:0.4193, Validation Loss:1.4356, Validation Accuracy:0.3268\n",
    "Epoch #245: Loss:1.3270, Accuracy:0.4172, Validation Loss:1.4253, Validation Accuracy:0.3415\n",
    "Epoch #246: Loss:1.3298, Accuracy:0.4193, Validation Loss:1.4245, Validation Accuracy:0.3415\n",
    "Epoch #247: Loss:1.3283, Accuracy:0.4168, Validation Loss:1.4266, Validation Accuracy:0.3465\n",
    "Epoch #248: Loss:1.3260, Accuracy:0.4226, Validation Loss:1.4305, Validation Accuracy:0.3333\n",
    "Epoch #249: Loss:1.3257, Accuracy:0.4251, Validation Loss:1.4354, Validation Accuracy:0.3235\n",
    "Epoch #250: Loss:1.3277, Accuracy:0.4185, Validation Loss:1.4441, Validation Accuracy:0.3218\n",
    "Epoch #251: Loss:1.3307, Accuracy:0.4053, Validation Loss:1.4296, Validation Accuracy:0.3300\n",
    "Epoch #252: Loss:1.3292, Accuracy:0.4136, Validation Loss:1.4254, Validation Accuracy:0.3415\n",
    "Epoch #253: Loss:1.3295, Accuracy:0.4136, Validation Loss:1.4241, Validation Accuracy:0.3481\n",
    "Epoch #254: Loss:1.3265, Accuracy:0.4193, Validation Loss:1.4254, Validation Accuracy:0.3498\n",
    "Epoch #255: Loss:1.3238, Accuracy:0.4168, Validation Loss:1.4310, Validation Accuracy:0.3465\n",
    "Epoch #256: Loss:1.3282, Accuracy:0.4156, Validation Loss:1.4266, Validation Accuracy:0.3465\n",
    "Epoch #257: Loss:1.3283, Accuracy:0.4201, Validation Loss:1.4257, Validation Accuracy:0.3481\n",
    "Epoch #258: Loss:1.3239, Accuracy:0.4193, Validation Loss:1.4287, Validation Accuracy:0.3399\n",
    "Epoch #259: Loss:1.3234, Accuracy:0.4144, Validation Loss:1.4272, Validation Accuracy:0.3498\n",
    "Epoch #260: Loss:1.3200, Accuracy:0.4263, Validation Loss:1.4456, Validation Accuracy:0.3136\n",
    "Epoch #261: Loss:1.3278, Accuracy:0.4197, Validation Loss:1.4344, Validation Accuracy:0.3415\n",
    "Epoch #262: Loss:1.3227, Accuracy:0.4230, Validation Loss:1.4307, Validation Accuracy:0.3399\n",
    "Epoch #263: Loss:1.3191, Accuracy:0.4205, Validation Loss:1.4286, Validation Accuracy:0.3530\n",
    "Epoch #264: Loss:1.3177, Accuracy:0.4193, Validation Loss:1.4314, Validation Accuracy:0.3432\n",
    "Epoch #265: Loss:1.3178, Accuracy:0.4226, Validation Loss:1.4314, Validation Accuracy:0.3432\n",
    "Epoch #266: Loss:1.3162, Accuracy:0.4181, Validation Loss:1.4348, Validation Accuracy:0.3317\n",
    "Epoch #267: Loss:1.3143, Accuracy:0.4230, Validation Loss:1.4303, Validation Accuracy:0.3448\n",
    "Epoch #268: Loss:1.3225, Accuracy:0.4205, Validation Loss:1.4344, Validation Accuracy:0.3268\n",
    "Epoch #269: Loss:1.3279, Accuracy:0.4201, Validation Loss:1.4424, Validation Accuracy:0.3530\n",
    "Epoch #270: Loss:1.3366, Accuracy:0.4082, Validation Loss:1.4371, Validation Accuracy:0.3415\n",
    "Epoch #271: Loss:1.3375, Accuracy:0.4033, Validation Loss:1.4320, Validation Accuracy:0.3612\n",
    "Epoch #272: Loss:1.3204, Accuracy:0.4189, Validation Loss:1.4263, Validation Accuracy:0.3498\n",
    "Epoch #273: Loss:1.3169, Accuracy:0.4230, Validation Loss:1.4291, Validation Accuracy:0.3448\n",
    "Epoch #274: Loss:1.3200, Accuracy:0.4255, Validation Loss:1.4348, Validation Accuracy:0.3399\n",
    "Epoch #275: Loss:1.3178, Accuracy:0.4267, Validation Loss:1.4309, Validation Accuracy:0.3448\n",
    "Epoch #276: Loss:1.3175, Accuracy:0.4230, Validation Loss:1.4315, Validation Accuracy:0.3448\n",
    "Epoch #277: Loss:1.3142, Accuracy:0.4230, Validation Loss:1.4406, Validation Accuracy:0.3333\n",
    "Epoch #278: Loss:1.3132, Accuracy:0.4226, Validation Loss:1.4306, Validation Accuracy:0.3448\n",
    "Epoch #279: Loss:1.3129, Accuracy:0.4230, Validation Loss:1.4310, Validation Accuracy:0.3448\n",
    "Epoch #280: Loss:1.3184, Accuracy:0.4201, Validation Loss:1.4320, Validation Accuracy:0.3514\n",
    "Epoch #281: Loss:1.3212, Accuracy:0.4214, Validation Loss:1.4320, Validation Accuracy:0.3465\n",
    "Epoch #282: Loss:1.3097, Accuracy:0.4251, Validation Loss:1.4476, Validation Accuracy:0.3268\n",
    "Epoch #283: Loss:1.3149, Accuracy:0.4205, Validation Loss:1.4442, Validation Accuracy:0.3350\n",
    "Epoch #284: Loss:1.3263, Accuracy:0.4234, Validation Loss:1.4307, Validation Accuracy:0.3530\n",
    "Epoch #285: Loss:1.3132, Accuracy:0.4156, Validation Loss:1.4333, Validation Accuracy:0.3498\n",
    "Epoch #286: Loss:1.3180, Accuracy:0.4127, Validation Loss:1.4371, Validation Accuracy:0.3448\n",
    "Epoch #287: Loss:1.3173, Accuracy:0.4189, Validation Loss:1.4313, Validation Accuracy:0.3580\n",
    "Epoch #288: Loss:1.3172, Accuracy:0.4177, Validation Loss:1.4344, Validation Accuracy:0.3366\n",
    "Epoch #289: Loss:1.3090, Accuracy:0.4193, Validation Loss:1.4278, Validation Accuracy:0.3448\n",
    "Epoch #290: Loss:1.3081, Accuracy:0.4242, Validation Loss:1.4286, Validation Accuracy:0.3465\n",
    "Epoch #291: Loss:1.3063, Accuracy:0.4296, Validation Loss:1.4305, Validation Accuracy:0.3547\n",
    "Epoch #292: Loss:1.3092, Accuracy:0.4148, Validation Loss:1.4439, Validation Accuracy:0.3251\n",
    "Epoch #293: Loss:1.3084, Accuracy:0.4230, Validation Loss:1.4299, Validation Accuracy:0.3514\n",
    "Epoch #294: Loss:1.3094, Accuracy:0.4246, Validation Loss:1.4312, Validation Accuracy:0.3465\n",
    "Epoch #295: Loss:1.3038, Accuracy:0.4255, Validation Loss:1.4350, Validation Accuracy:0.3481\n",
    "Epoch #296: Loss:1.3016, Accuracy:0.4279, Validation Loss:1.4311, Validation Accuracy:0.3465\n",
    "Epoch #297: Loss:1.3027, Accuracy:0.4349, Validation Loss:1.4282, Validation Accuracy:0.3448\n",
    "Epoch #298: Loss:1.3019, Accuracy:0.4259, Validation Loss:1.4306, Validation Accuracy:0.3481\n",
    "Epoch #299: Loss:1.3051, Accuracy:0.4296, Validation Loss:1.4371, Validation Accuracy:0.3415\n",
    "Epoch #300: Loss:1.3120, Accuracy:0.4255, Validation Loss:1.4409, Validation Accuracy:0.3547\n",
    "\n",
    "Test:\n",
    "Test Loss:1.44086587, Accuracy:0.3547\n",
    "Labels: ['01', '04', '05', '02', '03']\n",
    "Confusion Matrix:\n",
    "      01  04  05  02  03\n",
    "t:01  28   4  42  23  29\n",
    "t:04  14  41   6  14  37\n",
    "t:05  26   3  81  22  10\n",
    "t:02  22  15  30  28  19\n",
    "t:03  20  27  13  17  38\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.25      0.22      0.24       126\n",
    "          04       0.46      0.37      0.41       112\n",
    "          05       0.47      0.57      0.52       142\n",
    "          02       0.27      0.25      0.26       114\n",
    "          03       0.29      0.33      0.31       115\n",
    "\n",
    "    accuracy                           0.35       609\n",
    "   macro avg       0.35      0.35      0.34       609\n",
    "weighted avg       0.35      0.35      0.35       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 16:17:05 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 55 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6059404960015333, 1.605562136286781, 1.6055456632855294, 1.60536283832074, 1.6053501050460515, 1.605228538975144, 1.6051527715864635, 1.6050555089424396, 1.604935973148628, 1.6047143517260873, 1.6044387275166503, 1.6040252701598043, 1.603524187515522, 1.6028007186692337, 1.6017867784781996, 1.600425227326517, 1.5986286262769025, 1.596199679844485, 1.592888115857818, 1.588421975059071, 1.5827552084069338, 1.5760586664985945, 1.5697313406197309, 1.5660454230551257, 1.5627654366109562, 1.5614245715008188, 1.5618874425762783, 1.5595278090052613, 1.5600893387849304, 1.5597700720350143, 1.55852385087945, 1.560411927343785, 1.5580160660892481, 1.557932012774087, 1.558370272514268, 1.5575175473255476, 1.5572293205997236, 1.5609257661650333, 1.5575435453252056, 1.5583083175477528, 1.556479290201159, 1.556343007361752, 1.5568462351860084, 1.5561390845059173, 1.5563977447832356, 1.5563014416859067, 1.5565435320677232, 1.556561042326816, 1.5563916952543462, 1.5566647536257414, 1.5564398971097222, 1.556615651143204, 1.557609711961793, 1.5580206389106162, 1.559163217865579, 1.5567407950587657, 1.5570906888087983, 1.5594473121984447, 1.5588271386909172, 1.5579739840355609, 1.5562423741680451, 1.5561073462559867, 1.5635595660295785, 1.5579504586988677, 1.5559459777888407, 1.5547070882982026, 1.5543519910333192, 1.55371311145463, 1.5543818158664922, 1.5581425951032215, 1.5565553173447282, 1.5552928999726996, 1.554044968976176, 1.5565037140118076, 1.5565440785904432, 1.5479035072138745, 1.5455905377179726, 1.5501064060161072, 1.5460303786940175, 1.5507302358624186, 1.5447157194657475, 1.5455722732496966, 1.5439026150210151, 1.5444501503347763, 1.5428627426009656, 1.5417147491170071, 1.577373288926624, 1.5370318472679025, 1.544743899054128, 1.5641016273075723, 1.538155047177094, 1.535836929562448, 1.53188967509027, 1.5300315583280741, 1.531652789202034, 1.5323459796717602, 1.5265601609140782, 1.5241366969345043, 1.5201915807911914, 1.516727119243791, 1.518836581452531, 1.5110666366242032, 1.5121325105673378, 1.5436485074032311, 1.500319257158364, 1.5165601125100172, 1.5094169088576619, 1.508546467093607, 1.527147191694413, 1.5096354134172838, 1.4965442464073695, 1.4946367253223662, 1.4894090055049152, 1.4884760313237633, 1.4927387161207903, 1.4868502628627083, 1.487420209522905, 1.4957599915894382, 1.4846175885552844, 1.480052606617093, 1.4762550496506965, 1.4777849609237195, 1.4762963774952003, 1.4809632174095693, 1.471842764242138, 1.4727631646815584, 1.4686899664758266, 1.4730955563938284, 1.4710473552321761, 1.4698309262202096, 1.4925101061759911, 1.463516603549713, 1.466526644178995, 1.4594431450018546, 1.4778403885454576, 1.47509249775672, 1.4628127607805976, 1.45313518035588, 1.4740256106324972, 1.4536858245069757, 1.4536707645958085, 1.4520445318253365, 1.474928626285985, 1.456473814442827, 1.4536855130751536, 1.4525370889500835, 1.4552544447393057, 1.4715663086996094, 1.4509648853922126, 1.4786690688877075, 1.4460198485792564, 1.4480036518648145, 1.4467019606106386, 1.4605623783149155, 1.4714565005012725, 1.4450711101929739, 1.4435966461162848, 1.4429484848514176, 1.446964886975406, 1.4416835339394305, 1.4489321563827189, 1.4462898194496268, 1.4409362122734584, 1.4386935018749267, 1.4425128717923594, 1.438708918630979, 1.443594412654883, 1.4360159870438975, 1.4376858429759984, 1.4422897331428841, 1.4466355964663777, 1.4364278328242561, 1.437631763065194, 1.4343155166394213, 1.4584364276409931, 1.4430201059491763, 1.4359371438989499, 1.4445834886068585, 1.4428087795896483, 1.4398493614102819, 1.4322961725429166, 1.4362462271610503, 1.4352102242471354, 1.4307653312808384, 1.4304946711889432, 1.433535313567113, 1.4351685060851875, 1.436067220221208, 1.4330239957580817, 1.4361212459103814, 1.4597008899710644, 1.4522050088653815, 1.430086637952645, 1.435355445825799, 1.4341014498364553, 1.4336621209318414, 1.451156999285781, 1.4546172284140375, 1.4533164264337575, 1.453186752565193, 1.4428108186753121, 1.42923279329278, 1.4333661154573187, 1.4522567807355733, 1.4495498654486119, 1.4397969290932215, 1.42666004287394, 1.433997314356035, 1.427070473019517, 1.4268503613855648, 1.453140257223095, 1.431090035853519, 1.426874499798604, 1.45217108021816, 1.4351814073099096, 1.4403231214419963, 1.448244815976749, 1.4401367558242848, 1.4246152852752134, 1.43744299251262, 1.4324701452881636, 1.4348992406832566, 1.4298893273953341, 1.428368325891166, 1.424750799615982, 1.4269861879411394, 1.447418217588528, 1.4405011531754668, 1.4246953651431356, 1.4238754792753698, 1.4288050186849384, 1.4523848099465833, 1.451906671273493, 1.423792868020695, 1.4301677321761308, 1.422451982357232, 1.4239538769025128, 1.438569026040326, 1.4351429539948262, 1.4271959309115982, 1.4237278435617833, 1.4242824251428614, 1.4250343302004835, 1.4355683686893757, 1.4252967546726096, 1.424549097889554, 1.42656443800245, 1.4305151107863252, 1.4354034499777557, 1.4440794709476539, 1.4296297038521477, 1.4254081147449162, 1.4240511099888968, 1.425396500941372, 1.4309838757726359, 1.4265897060654238, 1.4257373265640685, 1.4286824719267721, 1.4272294406624655, 1.4455985253667596, 1.4344027897798761, 1.4307093845408148, 1.4285968610610085, 1.4314082328517646, 1.4313586419830573, 1.434807559343786, 1.4302828869796151, 1.434379059497163, 1.4423962418473217, 1.437147241899337, 1.4320489547914277, 1.426340859120311, 1.4290652899515062, 1.4347529186208063, 1.4309262969027992, 1.4315296996794702, 1.4406166098192212, 1.4305961063538475, 1.4309588987839046, 1.4319540426648896, 1.432015577364829, 1.4475852910716742, 1.444200843621553, 1.430709304676463, 1.433277257166081, 1.4371039922209992, 1.431315458271108, 1.434352622047825, 1.4278257482353298, 1.4285755421727748, 1.4304783031075263, 1.443852712172397, 1.4299336029782475, 1.4311621578651892, 1.4349817329243877, 1.4310743303721762, 1.4282061116057272, 1.4306174115398638, 1.4371288623324365, 1.4408658065623643], 'val_acc': [0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23316912742083884, 0.23645319967042833, 0.2430213442674803, 0.25287356091837576, 0.25287356111412174, 0.251231524891454, 0.24302134624940813, 0.270935958291118, 0.2742200305407075, 0.27257799441591274, 0.2660098500146067, 0.26765188613940144, 0.2512315266776359, 0.25451559892722536, 0.26272577974694505, 0.27093596056666475, 0.2709359603709188, 0.24958949065071412, 0.2725779963978406, 0.26600985199653454, 0.2643678156759939, 0.2561576350520201, 0.24958949055284116, 0.24794745413442745, 0.26600985170291563, 0.2610837433285314, 0.25451559882935243, 0.2479474540365545, 0.2512315265797629, 0.2463054181075057, 0.23973727370619968, 0.24958949035709518, 0.25123152648188996, 0.2495894902592222, 0.25123152638401697, 0.24302134576004322, 0.24958949016134924, 0.2298850566638123, 0.2561576347584012, 0.24466338198271095, 0.2528735626066847, 0.25779967098106893, 0.24466338208058394, 0.2561576350520201, 0.2561576349541472, 0.25451559882935243, 0.23973727360832672, 0.2495894907485871, 0.25451559873147944, 0.25779967098106893, 0.2577996710789419, 0.26272577955119913, 0.2528735626066847, 0.25451559912297134, 0.26272577955119913, 0.2643678156759939, 0.2528735626066847, 0.2577996711768149, 0.25451559902509835, 0.26272577955119913, 0.26929392424612403, 0.2610837436221503, 0.26765188792558336, 0.2643678158717398, 0.2725779965935865, 0.2692939241482511, 0.2643678155781209, 0.2676518860415285, 0.27093596056666475, 0.26272577994269103, 0.2840722474852219, 0.2758620667633752, 0.25944170730160965, 0.2840722474852219, 0.2643678156759939, 0.29228242850068753, 0.2840722474852219, 0.2955665005545311, 0.29228242840281454, 0.27093596017517285, 0.30049260892891533, 0.30213464524945605, 0.2988505727062476, 0.3136288976336543, 0.2939244641361174, 0.2972085362878339, 0.2939244640382444, 0.2577996714704338, 0.3054187171075536, 0.3119868621939704, 0.29556650026091214, 0.30049260873316935, 0.27586206874530306, 0.2922824282070686, 0.308702789846508, 0.3037766811785048, 0.30870278955288905, 0.315270934149941, 0.3103448256776838, 0.3119868619982245, 0.3119868620960975, 0.3037766813742508, 0.3103448257755568, 0.315270934247814, 0.30541871730329956, 0.3103448256776838, 0.31362889802514626, 0.315270934247814, 0.3185550063016575, 0.3152709337584491, 0.32183907874699297, 0.32183907864912, 0.3136288979272733, 0.31691297027473575, 0.3004926108129702, 0.31198686170460554, 0.31362889812301925, 0.31034482587342976, 0.31691297017686276, 0.3054187170096806, 0.3103448255798108, 0.31527093395419503, 0.3037766813742508, 0.3136288978294003, 0.3136288977315273, 0.3087027892592701, 0.3136288977315273, 0.3087027891613971, 0.31362889981132813, 0.3201970422307063, 0.3169129700789898, 0.30706075372171326, 0.30706075303660235, 0.31198686180247853, 0.31362889981132813, 0.3103448253840648, 0.32512315060509056, 0.30213464664414597, 0.2906403917886549, 0.31855500610591153, 0.3152709336605761, 0.3185550059101656, 0.3152709357403769, 0.3136288995177092, 0.3152709359361229, 0.3136288975357814, 0.3300492589794748, 0.3201970443105071, 0.3316912952021425, 0.3152709337584491, 0.3267651870235042, 0.3234811163643507, 0.3300492588816018, 0.32183908043530185, 0.30706075531214916, 0.31691297196304463, 0.334975367451732, 0.32512315268489134, 0.3251231508008365, 0.3185550062037845, 0.334975367353859, 0.31362889981132813, 0.3218390803374289, 0.3201970443105071, 0.3284072249344809, 0.3300492587837288, 0.33825944158537635, 0.3251231505072176, 0.3251231504093446, 0.32840722295255304, 0.32840722295255304, 0.33169129718407037, 0.3333333311311913, 0.3152709358382499, 0.2972085384655077, 0.3087027914369439, 0.3300492610592756, 0.3333333312290643, 0.32676518653413933, 0.3333333310333183, 0.3054187190894814, 0.31198686150885957, 0.3037766829646867, 0.3054187190894814, 0.31691296978537087, 0.3300492588816018, 0.33169129490852356, 0.3054187190894814, 0.3152709358382499, 0.31198686141098664, 0.32512315268489134, 0.3267651867298853, 0.3300492610592756, 0.33825943940770253, 0.3103448252861919, 0.33661740367439974, 0.33497536715811305, 0.33169129549576143, 0.31198686150885957, 0.3563218374655556, 0.3300492587837288, 0.34811165645008996, 0.334975367353859, 0.32348111457816875, 0.33990147572824325, 0.3185550059101656, 0.33497536725598603, 0.3333333311311913, 0.3333333311311913, 0.34482758420050047, 0.29885057468817544, 0.32840722275680706, 0.3366174033807808, 0.34482758420050047, 0.3431855481735787, 0.3103448252861919, 0.3119868635886604, 0.33497536725598603, 0.33661740367439974, 0.33497536715811305, 0.33990147563037026, 0.3267651867298853, 0.3399014755324973, 0.341543511950911, 0.33661740347865376, 0.34482758420050047, 0.3448275840047545, 0.3267651866320123, 0.341543511950911, 0.34154351204878397, 0.3464696203252952, 0.3333333310333183, 0.3234811144802958, 0.3218390782576281, 0.3300492589794748, 0.34154351204878397, 0.34811165645008996, 0.3497536926727577, 0.3464696202274222, 0.3464696202274222, 0.34811165635221697, 0.33990147602186216, 0.3497536926727577, 0.3136288977315273, 0.341543511950911, 0.33990147582611624, 0.3530377649223472, 0.3431855480757057, 0.34318554787995975, 0.3316912952021425, 0.34482758429837346, 0.32676518682775824, 0.3530377649223472, 0.3415435122445299, 0.36124794583993985, 0.3497536927706307, 0.3448275840047545, 0.33990147572824325, 0.3448275840047545, 0.34482758429837346, 0.33333333132693727, 0.3448275841026275, 0.34482758420050047, 0.35139572869967944, 0.3464696203252952, 0.32676518653413933, 0.334975367353859, 0.3530377645308552, 0.3497536929663766, 0.34482758420050047, 0.3579638734924774, 0.33661740347865376, 0.3448275841026275, 0.3464696204231682, 0.3546798011450149, 0.3251231508008365, 0.3513957288954254, 0.3464696203252952, 0.34811165645008996, 0.3464696206189142, 0.3448275841026275, 0.34811165664583593, 0.34154351214665696, 0.3546798013407609], 'loss': [1.607534863963509, 1.6058342258053884, 1.6059241149949341, 1.6058248639351533, 1.6055609597317737, 1.6055519115753487, 1.6054120759944406, 1.6053271097813788, 1.6052762893680674, 1.6052365639371304, 1.6049814459234781, 1.6046930700846522, 1.6042743139443212, 1.6037645846666497, 1.6031878921775113, 1.6020285000791294, 1.6007519573646405, 1.5991061137197444, 1.5967406276315144, 1.5935719756375104, 1.5891585180157264, 1.5844250605091668, 1.5783552736717084, 1.5734933467861074, 1.570929312559124, 1.5697864140328441, 1.5687453609227644, 1.5672930699354324, 1.5654742471001721, 1.5658225138818949, 1.5650514175270127, 1.5634569565128742, 1.562917273979657, 1.5619211636529566, 1.5609658735733503, 1.5596973955264082, 1.5603112306927753, 1.5598424063081369, 1.5594334998904313, 1.5575093538119809, 1.5563657051728736, 1.556087374540325, 1.55390518979615, 1.5532370439545085, 1.552009363340891, 1.5510749038974363, 1.549991975868507, 1.5491237329016965, 1.5481972722791792, 1.5469701766967774, 1.5460455242368474, 1.544924475573906, 1.543721244026748, 1.543098592611309, 1.5432375609262767, 1.542551676154871, 1.5395508299618041, 1.5392607615468927, 1.5379617033553075, 1.5403721829948973, 1.5367822482600595, 1.5346260153537414, 1.5346683665712266, 1.5320238295032258, 1.5357416646926065, 1.5324974652433296, 1.5285982730697067, 1.5270207835418732, 1.526166444441621, 1.5250992260429648, 1.5241279838511097, 1.5239713717290264, 1.5229548372037602, 1.5254803820557172, 1.524101386177956, 1.5303649706517402, 1.5195620611707779, 1.5143705669125003, 1.512859315793862, 1.512819789518321, 1.5101504101155965, 1.50903266793159, 1.5091251110394144, 1.5045826217721865, 1.5011505769263547, 1.5003116123974936, 1.4995158214588675, 1.5157252429691919, 1.5079062497591336, 1.5074414420176825, 1.5117262476523554, 1.4937930617733903, 1.4904398707148965, 1.484867235571452, 1.4830091481825654, 1.4793050879570493, 1.4792374997657918, 1.4762492524035413, 1.4737027149670423, 1.4671714099280888, 1.4653900081372113, 1.4660854256373412, 1.4610134232949916, 1.4632450565909947, 1.4783666062893563, 1.4663092014481154, 1.4637635748489193, 1.4582380194928366, 1.4608204844551165, 1.4639433284070213, 1.4577456463044185, 1.4466333367741329, 1.4383164117468457, 1.4359942723103862, 1.4349632699386785, 1.432589434353478, 1.4354558700898346, 1.4383166123709394, 1.4455906314281957, 1.4274391921393925, 1.4229332424531973, 1.4208543961542588, 1.417402092778952, 1.4194982043037179, 1.4293421986166701, 1.4171519545803815, 1.4142372064766697, 1.4107955824912697, 1.411125244741812, 1.4087969609599338, 1.4078624595117275, 1.4147073640960444, 1.411320964022094, 1.405528741205987, 1.4021975242626497, 1.4074077848536277, 1.4052368102866766, 1.403017838436965, 1.4061806660658036, 1.4093558053461188, 1.3994078954876816, 1.3959649568710484, 1.3929319233375408, 1.3977434812385199, 1.390771733368202, 1.3921243065926083, 1.40159295094821, 1.4030687695411197, 1.4124981768077405, 1.3987814458243901, 1.3980146696924918, 1.3887302267477988, 1.3877041828950571, 1.3856712214021467, 1.3895998661033426, 1.3927677889868955, 1.382460432228856, 1.3802040753668094, 1.3777997784056457, 1.3776864504177713, 1.376465738774325, 1.377435334409287, 1.3738913119940788, 1.3735084196870087, 1.373973410575052, 1.378150224538799, 1.383200275334979, 1.3793510968680254, 1.3709786755348379, 1.3691107875267827, 1.3726333595154467, 1.374350818618367, 1.3707627897145078, 1.3668805389678453, 1.3713585711602558, 1.383725037173324, 1.3839703648976476, 1.3755421438745894, 1.368229760526387, 1.3641060440936863, 1.3636652988085267, 1.3606573434825795, 1.3603869106735291, 1.3619099944523962, 1.3611250126386325, 1.3596859347893717, 1.3590101005605113, 1.3596644032417626, 1.3583956056551767, 1.3589347291041693, 1.3587088313190845, 1.3605622420320767, 1.3590089913511179, 1.3551895911199112, 1.3531137771919768, 1.3523608222389614, 1.3536871466548537, 1.3581553215853244, 1.3606893686788037, 1.3673108808803363, 1.359577075803549, 1.3528902640822487, 1.3493934994605532, 1.3468866313507424, 1.354099513373091, 1.3578730264483536, 1.3584299771447936, 1.3515873684285846, 1.3508759413901297, 1.3598531704418957, 1.3505346983120428, 1.3563342331371269, 1.3560997273642914, 1.364612133204325, 1.3819911692421538, 1.363722176032879, 1.3546459470686236, 1.3525364470922483, 1.3494836205084955, 1.3549908517077718, 1.3612121884582957, 1.34230881980802, 1.3438513456673593, 1.3458063650424965, 1.3387623689992227, 1.3407274720115583, 1.3411023463556655, 1.3412421399318217, 1.3390968704125719, 1.3391571235362998, 1.3385026595920508, 1.3402524445825534, 1.3458385480747574, 1.348539000076435, 1.3517910057514355, 1.342848653421265, 1.3336256786538345, 1.3321397670730184, 1.3332522884286648, 1.3341422192125105, 1.3356071558821128, 1.3306815170899065, 1.3293202671427011, 1.3293885111563994, 1.3270085144826274, 1.3298476504594148, 1.3282927453885087, 1.3259536119457143, 1.3257000098238245, 1.3276668138817351, 1.3307126124536721, 1.3291663387228088, 1.3295345954092133, 1.3265058440594213, 1.3237824094858504, 1.3282143961477573, 1.3283406445133124, 1.3239183943374446, 1.323405121092434, 1.3199717824708754, 1.3278097106201203, 1.3226712416329667, 1.3190749499342524, 1.3176589794961824, 1.3177502235592757, 1.316166311123043, 1.3143317740555907, 1.3224941085741015, 1.3279061475573624, 1.336615109639491, 1.3375339123747432, 1.3204345510236046, 1.3169455217874515, 1.3200237614907768, 1.3177553492160303, 1.3174792630471732, 1.3141673431748973, 1.3132073143424439, 1.3129354525885297, 1.3184271116276296, 1.3211897759956501, 1.3097498881988212, 1.3148919081541057, 1.3262726657934012, 1.3132193396468428, 1.3180334761891765, 1.3173391028840928, 1.317198870803786, 1.3090381224786967, 1.3081072704013612, 1.3063403532000784, 1.3092384465666034, 1.3084304346440998, 1.3093616634423728, 1.303766707277396, 1.3015747116331693, 1.302666211715714, 1.3019384912886414, 1.305073706374276, 1.31204196457011], 'acc': [0.23285420970872686, 0.2328542102778472, 0.2328542091028891, 0.2328542094945418, 0.23285420929871545, 0.23285420890706277, 0.23285421049203225, 0.23285420833794243, 0.23285420969036816, 0.23285421029620593, 0.2328542085337688, 0.2328542091028891, 0.23285421029620593, 0.2328542091028891, 0.23285420951290053, 0.23285420929871545, 0.2328542110611526, 0.2324435330514301, 0.23737166429080023, 0.2455852160647175, 0.2550308024369226, 0.25215605715217043, 0.23901437591967886, 0.23531827559461338, 0.2464065698749965, 0.256673510755112, 0.2640657086636741, 0.2726899398180983, 0.26324435323170814, 0.2702258745013321, 0.26406571003445856, 0.26694045375259995, 0.2624229979955685, 0.26694045296929453, 0.2735112944667589, 0.27597535935515494, 0.27227926122089674, 0.2722792604375914, 0.26488706507477183, 0.2665297751553984, 0.2788501046766246, 0.272689937039812, 0.2718685837986533, 0.2780287496363113, 0.27843942428026847, 0.2833675586712189, 0.28747433367695896, 0.28624229745698415, 0.2870636546513873, 0.2915811102493098, 0.2862422976528105, 0.2924024625113368, 0.2924024629029895, 0.28254619948183485, 0.28829568770142305, 0.28501026597355916, 0.290759753997321, 0.2985626297197792, 0.2887063677061265, 0.29117043024460637, 0.2915811102493098, 0.29363449497389355, 0.2985626305398021, 0.29075975145157845, 0.2989733081578719, 0.30102669307828195, 0.3010266964073299, 0.3010266952323718, 0.30595482488677245, 0.3088295693882192, 0.2969199184641946, 0.3080082133687742, 0.3071868591117663, 0.282956879878191, 0.29897330498793284, 0.30184804792276887, 0.3047227908576049, 0.3084188929451075, 0.3104722790771931, 0.3100616038457569, 0.3158110888954061, 0.3158110879162743, 0.3125256653683876, 0.31786447839325704, 0.3034907609407907, 0.31211498759120887, 0.31786447698575515, 0.30677618051456473, 0.31252566716754215, 0.31868583378850557, 0.31416837861895314, 0.3203285424983477, 0.31827515244973514, 0.32648870637774224, 0.3264887050069578, 0.3289527713028557, 0.32566735290403975, 0.3310061595890311, 0.32977412552314617, 0.342915810280512, 0.33757699987482, 0.3347022610890547, 0.35030800603498424, 0.3429158093380977, 0.33100615896483465, 0.331006161743121, 0.34127310215814893, 0.34209445465271965, 0.3412731000040591, 0.33100615935648736, 0.3371663248024927, 0.3457905539986534, 0.3527720735058403, 0.35564681644067625, 0.3531827509280837, 0.35934291738993823, 0.3535934283503272, 0.35482546081288396, 0.35811088254074785, 0.3655030812693328, 0.36180698153174634, 0.37084188948421754, 0.36386036880207256, 0.36098562704219467, 0.36837782577077954, 0.37166324472035717, 0.37330595578011544, 0.37125256870561557, 0.3663244372887778, 0.37782341016636245, 0.3757700205094026, 0.35893223993097734, 0.36468172325490683, 0.3708418878808893, 0.37618069675668797, 0.37412731101625507, 0.3753593430871591, 0.3683778255749532, 0.3741273102329497, 0.36221765891727237, 0.37084188948421754, 0.3798767972775798, 0.37905544106230843, 0.37002053503138327, 0.3798767970817535, 0.37125256671063467, 0.37535934152054834, 0.372073919755967, 0.3535934297578291, 0.38069815130204393, 0.3827515402124158, 0.3839835707167091, 0.3798767945360109, 0.3831622156763958, 0.3765913769572178, 0.37043121284527947, 0.3737166318315745, 0.3802874754831287, 0.3860369617444534, 0.3860369607653216, 0.3827515400533069, 0.38234086396513045, 0.38891170409181036, 0.3880903510097605, 0.382340862594346, 0.39055441515156863, 0.38151950912064353, 0.38110882915265754, 0.39466119035313507, 0.39589322199566895, 0.39014373753349885, 0.3823408647484358, 0.3917864499640416, 0.3958932253247169, 0.3848049304568547, 0.37741273215663995, 0.3819301857595816, 0.3889117035043313, 0.3926078026544387, 0.39507187090860013, 0.39630390020121786, 0.39917864630599287, 0.3979466102818444, 0.397125256649033, 0.40287474388948946, 0.40041067720193885, 0.4032854232699964, 0.400410679551855, 0.4004106785727233, 0.40287474388948946, 0.40328541912092564, 0.39383983511699544, 0.4041067761195024, 0.4057494854168236, 0.40739219765154, 0.4020533894366552, 0.3999999986047373, 0.39342915824551355, 0.3917864462066235, 0.4012320345921683, 0.39630390219619877, 0.3954825447692519, 0.40657083967383145, 0.3995893245115417, 0.40903490577390306, 0.3987679667296596, 0.4057494878034572, 0.40739219706406093, 0.3946611907080703, 0.39178644585168826, 0.39425051547663414, 0.4028747442444247, 0.39096509198633306, 0.39425051156010715, 0.3806981505187385, 0.38603696311523783, 0.3983572902865478, 0.39383983394203737, 0.4086242285474859, 0.4012320314222293, 0.3921971240572371, 0.39999999778471446, 0.406570842219574, 0.40780287687293804, 0.4221765909229216, 0.407802873702999, 0.40492813076816303, 0.4078028755021536, 0.411088294684275, 0.41724845891860474, 0.4110882968383648, 0.40492813315479664, 0.4151950700449503, 0.3987679653221577, 0.40492812962992236, 0.40944558519112745, 0.41971252681783094, 0.4172484583311257, 0.41190964949204445, 0.4176591369283273, 0.41806981395891807, 0.41478439262270683, 0.41889116860757863, 0.4193018499830665, 0.41724845970191016, 0.41930184955469635, 0.41683778169218766, 0.42258727069508123, 0.42505133327027855, 0.41848049196864057, 0.40533880681962203, 0.41355236195930467, 0.41355236133510814, 0.4193018499830665, 0.4168377834913422, 0.4156057482504992, 0.42012320126596175, 0.4193018503747192, 0.4143737148088107, 0.42628336909860065, 0.4197125261936344, 0.4229979481540421, 0.4205338820539706, 0.4193018475964329, 0.42258726795351237, 0.41806981474222343, 0.42299794498410315, 0.42053388244562323, 0.42012320244091983, 0.4082135527285707, 0.40328542229086467, 0.4188911691950577, 0.42299794557158216, 0.4254620110841747, 0.4266940429592524, 0.4229979447882768, 0.42299794596323487, 0.42258726955684056, 0.42299794893734755, 0.4201232010701354, 0.4213552378775892, 0.4250513334661049, 0.4205338806464687, 0.42340862553956815, 0.4156057476630201, 0.4127310084856022, 0.4188911713491475, 0.41765913872748184, 0.4193018472047802, 0.4242299799924024, 0.4295687870323291, 0.41478439262270683, 0.42299794713819294, 0.4246406566313405, 0.4254620114758274, 0.42792607738007266, 0.43490759904134935, 0.42587269226383623, 0.4295687864815675, 0.4254620134340909]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
