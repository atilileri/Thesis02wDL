{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf23.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 01:47:03 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '2Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '05', '03', '04', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001CA04FBBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001CA6BE07EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6124, Accuracy:0.2033, Validation Loss:1.6081, Validation Accuracy:0.2299\n",
    "Epoch #2: Loss:1.6073, Accuracy:0.2324, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #11: Loss:1.6032, Accuracy:0.2337, Validation Loss:1.6044, Validation Accuracy:0.2266\n",
    "Epoch #12: Loss:1.6028, Accuracy:0.2398, Validation Loss:1.6046, Validation Accuracy:0.2282\n",
    "Epoch #13: Loss:1.6023, Accuracy:0.2407, Validation Loss:1.6047, Validation Accuracy:0.2315\n",
    "Epoch #14: Loss:1.6019, Accuracy:0.2423, Validation Loss:1.6047, Validation Accuracy:0.2299\n",
    "Epoch #15: Loss:1.6018, Accuracy:0.2419, Validation Loss:1.6048, Validation Accuracy:0.2315\n",
    "Epoch #16: Loss:1.6019, Accuracy:0.2419, Validation Loss:1.6048, Validation Accuracy:0.2299\n",
    "Epoch #17: Loss:1.6025, Accuracy:0.2374, Validation Loss:1.6049, Validation Accuracy:0.2266\n",
    "Epoch #18: Loss:1.6020, Accuracy:0.2423, Validation Loss:1.6047, Validation Accuracy:0.2299\n",
    "Epoch #19: Loss:1.6018, Accuracy:0.2431, Validation Loss:1.6047, Validation Accuracy:0.2315\n",
    "Epoch #20: Loss:1.6013, Accuracy:0.2439, Validation Loss:1.6048, Validation Accuracy:0.2315\n",
    "Epoch #21: Loss:1.6006, Accuracy:0.2411, Validation Loss:1.6052, Validation Accuracy:0.2315\n",
    "Epoch #22: Loss:1.6000, Accuracy:0.2476, Validation Loss:1.6045, Validation Accuracy:0.2315\n",
    "Epoch #23: Loss:1.6001, Accuracy:0.2448, Validation Loss:1.6052, Validation Accuracy:0.2315\n",
    "Epoch #24: Loss:1.5997, Accuracy:0.2448, Validation Loss:1.6046, Validation Accuracy:0.2365\n",
    "Epoch #25: Loss:1.5992, Accuracy:0.2464, Validation Loss:1.6043, Validation Accuracy:0.2315\n",
    "Epoch #26: Loss:1.5995, Accuracy:0.2427, Validation Loss:1.6051, Validation Accuracy:0.2299\n",
    "Epoch #27: Loss:1.5989, Accuracy:0.2460, Validation Loss:1.6049, Validation Accuracy:0.2250\n",
    "Epoch #28: Loss:1.5986, Accuracy:0.2480, Validation Loss:1.6049, Validation Accuracy:0.2348\n",
    "Epoch #29: Loss:1.5991, Accuracy:0.2472, Validation Loss:1.6043, Validation Accuracy:0.2381\n",
    "Epoch #30: Loss:1.5988, Accuracy:0.2456, Validation Loss:1.6043, Validation Accuracy:0.2266\n",
    "Epoch #31: Loss:1.5989, Accuracy:0.2452, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #32: Loss:1.5980, Accuracy:0.2480, Validation Loss:1.6038, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.5979, Accuracy:0.2480, Validation Loss:1.6042, Validation Accuracy:0.2414\n",
    "Epoch #34: Loss:1.5984, Accuracy:0.2501, Validation Loss:1.6031, Validation Accuracy:0.2348\n",
    "Epoch #35: Loss:1.5983, Accuracy:0.2542, Validation Loss:1.6011, Validation Accuracy:0.2282\n",
    "Epoch #36: Loss:1.5998, Accuracy:0.2472, Validation Loss:1.6034, Validation Accuracy:0.2299\n",
    "Epoch #37: Loss:1.6021, Accuracy:0.2374, Validation Loss:1.6029, Validation Accuracy:0.2200\n",
    "Epoch #38: Loss:1.6025, Accuracy:0.2259, Validation Loss:1.6068, Validation Accuracy:0.2200\n",
    "Epoch #39: Loss:1.6005, Accuracy:0.2320, Validation Loss:1.6007, Validation Accuracy:0.2282\n",
    "Epoch #40: Loss:1.5993, Accuracy:0.2501, Validation Loss:1.6028, Validation Accuracy:0.2282\n",
    "Epoch #41: Loss:1.5998, Accuracy:0.2423, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #42: Loss:1.5985, Accuracy:0.2493, Validation Loss:1.6005, Validation Accuracy:0.2381\n",
    "Epoch #43: Loss:1.5980, Accuracy:0.2468, Validation Loss:1.6009, Validation Accuracy:0.2348\n",
    "Epoch #44: Loss:1.5974, Accuracy:0.2509, Validation Loss:1.5999, Validation Accuracy:0.2299\n",
    "Epoch #45: Loss:1.5969, Accuracy:0.2505, Validation Loss:1.6001, Validation Accuracy:0.2332\n",
    "Epoch #46: Loss:1.5966, Accuracy:0.2526, Validation Loss:1.6006, Validation Accuracy:0.2315\n",
    "Epoch #47: Loss:1.5966, Accuracy:0.2538, Validation Loss:1.6026, Validation Accuracy:0.2381\n",
    "Epoch #48: Loss:1.5977, Accuracy:0.2472, Validation Loss:1.6035, Validation Accuracy:0.2365\n",
    "Epoch #49: Loss:1.5968, Accuracy:0.2497, Validation Loss:1.6042, Validation Accuracy:0.2348\n",
    "Epoch #50: Loss:1.5960, Accuracy:0.2530, Validation Loss:1.6034, Validation Accuracy:0.2332\n",
    "Epoch #51: Loss:1.5960, Accuracy:0.2513, Validation Loss:1.6024, Validation Accuracy:0.2348\n",
    "Epoch #52: Loss:1.5958, Accuracy:0.2522, Validation Loss:1.6027, Validation Accuracy:0.2315\n",
    "Epoch #53: Loss:1.5952, Accuracy:0.2489, Validation Loss:1.6023, Validation Accuracy:0.2348\n",
    "Epoch #54: Loss:1.5957, Accuracy:0.2522, Validation Loss:1.6012, Validation Accuracy:0.2348\n",
    "Epoch #55: Loss:1.5942, Accuracy:0.2542, Validation Loss:1.5999, Validation Accuracy:0.2348\n",
    "Epoch #56: Loss:1.5947, Accuracy:0.2530, Validation Loss:1.5996, Validation Accuracy:0.2397\n",
    "Epoch #57: Loss:1.5946, Accuracy:0.2534, Validation Loss:1.5998, Validation Accuracy:0.2447\n",
    "Epoch #58: Loss:1.5947, Accuracy:0.2530, Validation Loss:1.6032, Validation Accuracy:0.2348\n",
    "Epoch #59: Loss:1.5961, Accuracy:0.2378, Validation Loss:1.6039, Validation Accuracy:0.2217\n",
    "Epoch #60: Loss:1.5953, Accuracy:0.2534, Validation Loss:1.6045, Validation Accuracy:0.2299\n",
    "Epoch #61: Loss:1.5932, Accuracy:0.2538, Validation Loss:1.6039, Validation Accuracy:0.2282\n",
    "Epoch #62: Loss:1.5930, Accuracy:0.2480, Validation Loss:1.6046, Validation Accuracy:0.2151\n",
    "Epoch #63: Loss:1.5940, Accuracy:0.2522, Validation Loss:1.6042, Validation Accuracy:0.2250\n",
    "Epoch #64: Loss:1.5931, Accuracy:0.2542, Validation Loss:1.6034, Validation Accuracy:0.2282\n",
    "Epoch #65: Loss:1.5921, Accuracy:0.2542, Validation Loss:1.6025, Validation Accuracy:0.2266\n",
    "Epoch #66: Loss:1.5913, Accuracy:0.2587, Validation Loss:1.6024, Validation Accuracy:0.2200\n",
    "Epoch #67: Loss:1.5912, Accuracy:0.2517, Validation Loss:1.6042, Validation Accuracy:0.2217\n",
    "Epoch #68: Loss:1.5920, Accuracy:0.2439, Validation Loss:1.6021, Validation Accuracy:0.2365\n",
    "Epoch #69: Loss:1.5942, Accuracy:0.2435, Validation Loss:1.6021, Validation Accuracy:0.2299\n",
    "Epoch #70: Loss:1.5976, Accuracy:0.2472, Validation Loss:1.6090, Validation Accuracy:0.2250\n",
    "Epoch #71: Loss:1.5978, Accuracy:0.2423, Validation Loss:1.6039, Validation Accuracy:0.2315\n",
    "Epoch #72: Loss:1.5936, Accuracy:0.2468, Validation Loss:1.6063, Validation Accuracy:0.2266\n",
    "Epoch #73: Loss:1.5964, Accuracy:0.2390, Validation Loss:1.6027, Validation Accuracy:0.2365\n",
    "Epoch #74: Loss:1.5955, Accuracy:0.2460, Validation Loss:1.5999, Validation Accuracy:0.2282\n",
    "Epoch #75: Loss:1.5951, Accuracy:0.2480, Validation Loss:1.6021, Validation Accuracy:0.2266\n",
    "Epoch #76: Loss:1.5942, Accuracy:0.2439, Validation Loss:1.5998, Validation Accuracy:0.2250\n",
    "Epoch #77: Loss:1.5938, Accuracy:0.2476, Validation Loss:1.5997, Validation Accuracy:0.2233\n",
    "Epoch #78: Loss:1.5948, Accuracy:0.2513, Validation Loss:1.6010, Validation Accuracy:0.2184\n",
    "Epoch #79: Loss:1.5938, Accuracy:0.2526, Validation Loss:1.6014, Validation Accuracy:0.2381\n",
    "Epoch #80: Loss:1.5930, Accuracy:0.2563, Validation Loss:1.6030, Validation Accuracy:0.2299\n",
    "Epoch #81: Loss:1.5926, Accuracy:0.2530, Validation Loss:1.6023, Validation Accuracy:0.2348\n",
    "Epoch #82: Loss:1.5913, Accuracy:0.2587, Validation Loss:1.5997, Validation Accuracy:0.2365\n",
    "Epoch #83: Loss:1.5926, Accuracy:0.2501, Validation Loss:1.5992, Validation Accuracy:0.2282\n",
    "Epoch #84: Loss:1.5927, Accuracy:0.2567, Validation Loss:1.6030, Validation Accuracy:0.2200\n",
    "Epoch #85: Loss:1.5925, Accuracy:0.2538, Validation Loss:1.6015, Validation Accuracy:0.2250\n",
    "Epoch #86: Loss:1.5926, Accuracy:0.2480, Validation Loss:1.6023, Validation Accuracy:0.2200\n",
    "Epoch #87: Loss:1.5910, Accuracy:0.2571, Validation Loss:1.6038, Validation Accuracy:0.2266\n",
    "Epoch #88: Loss:1.5924, Accuracy:0.2559, Validation Loss:1.6021, Validation Accuracy:0.2282\n",
    "Epoch #89: Loss:1.5914, Accuracy:0.2526, Validation Loss:1.6035, Validation Accuracy:0.2430\n",
    "Epoch #90: Loss:1.5921, Accuracy:0.2583, Validation Loss:1.6026, Validation Accuracy:0.2332\n",
    "Epoch #91: Loss:1.5923, Accuracy:0.2538, Validation Loss:1.6018, Validation Accuracy:0.2414\n",
    "Epoch #92: Loss:1.5909, Accuracy:0.2641, Validation Loss:1.6044, Validation Accuracy:0.2233\n",
    "Epoch #93: Loss:1.5904, Accuracy:0.2595, Validation Loss:1.6012, Validation Accuracy:0.2266\n",
    "Epoch #94: Loss:1.5900, Accuracy:0.2554, Validation Loss:1.6015, Validation Accuracy:0.2315\n",
    "Epoch #95: Loss:1.5904, Accuracy:0.2559, Validation Loss:1.6025, Validation Accuracy:0.2102\n",
    "Epoch #96: Loss:1.5897, Accuracy:0.2600, Validation Loss:1.6047, Validation Accuracy:0.2167\n",
    "Epoch #97: Loss:1.5888, Accuracy:0.2542, Validation Loss:1.6052, Validation Accuracy:0.2250\n",
    "Epoch #98: Loss:1.5887, Accuracy:0.2575, Validation Loss:1.6051, Validation Accuracy:0.2282\n",
    "Epoch #99: Loss:1.5889, Accuracy:0.2534, Validation Loss:1.6044, Validation Accuracy:0.2135\n",
    "Epoch #100: Loss:1.5889, Accuracy:0.2608, Validation Loss:1.6052, Validation Accuracy:0.2217\n",
    "Epoch #101: Loss:1.5900, Accuracy:0.2530, Validation Loss:1.6082, Validation Accuracy:0.2217\n",
    "Epoch #102: Loss:1.5910, Accuracy:0.2620, Validation Loss:1.6039, Validation Accuracy:0.2397\n",
    "Epoch #103: Loss:1.5913, Accuracy:0.2583, Validation Loss:1.6025, Validation Accuracy:0.2299\n",
    "Epoch #104: Loss:1.5911, Accuracy:0.2476, Validation Loss:1.6058, Validation Accuracy:0.2365\n",
    "Epoch #105: Loss:1.5903, Accuracy:0.2632, Validation Loss:1.6052, Validation Accuracy:0.2282\n",
    "Epoch #106: Loss:1.5924, Accuracy:0.2526, Validation Loss:1.6040, Validation Accuracy:0.2365\n",
    "Epoch #107: Loss:1.5931, Accuracy:0.2460, Validation Loss:1.6047, Validation Accuracy:0.2315\n",
    "Epoch #108: Loss:1.5912, Accuracy:0.2563, Validation Loss:1.6026, Validation Accuracy:0.2266\n",
    "Epoch #109: Loss:1.5917, Accuracy:0.2476, Validation Loss:1.6035, Validation Accuracy:0.2397\n",
    "Epoch #110: Loss:1.5913, Accuracy:0.2653, Validation Loss:1.6030, Validation Accuracy:0.2250\n",
    "Epoch #111: Loss:1.5917, Accuracy:0.2534, Validation Loss:1.5987, Validation Accuracy:0.2250\n",
    "Epoch #112: Loss:1.5917, Accuracy:0.2476, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #113: Loss:1.5904, Accuracy:0.2546, Validation Loss:1.5990, Validation Accuracy:0.2365\n",
    "Epoch #114: Loss:1.5889, Accuracy:0.2612, Validation Loss:1.6029, Validation Accuracy:0.2266\n",
    "Epoch #115: Loss:1.5897, Accuracy:0.2509, Validation Loss:1.6012, Validation Accuracy:0.2365\n",
    "Epoch #116: Loss:1.5885, Accuracy:0.2563, Validation Loss:1.6022, Validation Accuracy:0.2348\n",
    "Epoch #117: Loss:1.5873, Accuracy:0.2641, Validation Loss:1.6054, Validation Accuracy:0.2233\n",
    "Epoch #118: Loss:1.5853, Accuracy:0.2641, Validation Loss:1.6033, Validation Accuracy:0.2348\n",
    "Epoch #119: Loss:1.5862, Accuracy:0.2567, Validation Loss:1.6039, Validation Accuracy:0.2250\n",
    "Epoch #120: Loss:1.5862, Accuracy:0.2649, Validation Loss:1.6075, Validation Accuracy:0.2233\n",
    "Epoch #121: Loss:1.5861, Accuracy:0.2632, Validation Loss:1.6040, Validation Accuracy:0.2250\n",
    "Epoch #122: Loss:1.5825, Accuracy:0.2661, Validation Loss:1.6071, Validation Accuracy:0.2266\n",
    "Epoch #123: Loss:1.5852, Accuracy:0.2554, Validation Loss:1.6075, Validation Accuracy:0.2447\n",
    "Epoch #124: Loss:1.5848, Accuracy:0.2591, Validation Loss:1.6072, Validation Accuracy:0.2233\n",
    "Epoch #125: Loss:1.5864, Accuracy:0.2686, Validation Loss:1.6088, Validation Accuracy:0.2233\n",
    "Epoch #126: Loss:1.5855, Accuracy:0.2604, Validation Loss:1.6104, Validation Accuracy:0.2315\n",
    "Epoch #127: Loss:1.5849, Accuracy:0.2546, Validation Loss:1.6098, Validation Accuracy:0.2397\n",
    "Epoch #128: Loss:1.5857, Accuracy:0.2620, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #129: Loss:1.5847, Accuracy:0.2702, Validation Loss:1.6080, Validation Accuracy:0.2365\n",
    "Epoch #130: Loss:1.5838, Accuracy:0.2682, Validation Loss:1.6106, Validation Accuracy:0.2200\n",
    "Epoch #131: Loss:1.5855, Accuracy:0.2653, Validation Loss:1.6134, Validation Accuracy:0.2315\n",
    "Epoch #132: Loss:1.5844, Accuracy:0.2645, Validation Loss:1.6117, Validation Accuracy:0.2266\n",
    "Epoch #133: Loss:1.5855, Accuracy:0.2694, Validation Loss:1.6118, Validation Accuracy:0.2397\n",
    "Epoch #134: Loss:1.5875, Accuracy:0.2682, Validation Loss:1.6124, Validation Accuracy:0.2381\n",
    "Epoch #135: Loss:1.5859, Accuracy:0.2620, Validation Loss:1.6125, Validation Accuracy:0.2381\n",
    "Epoch #136: Loss:1.5856, Accuracy:0.2641, Validation Loss:1.6143, Validation Accuracy:0.2397\n",
    "Epoch #137: Loss:1.5834, Accuracy:0.2641, Validation Loss:1.6123, Validation Accuracy:0.2365\n",
    "Epoch #138: Loss:1.5828, Accuracy:0.2649, Validation Loss:1.6114, Validation Accuracy:0.2282\n",
    "Epoch #139: Loss:1.5825, Accuracy:0.2653, Validation Loss:1.6132, Validation Accuracy:0.2414\n",
    "Epoch #140: Loss:1.5846, Accuracy:0.2637, Validation Loss:1.6102, Validation Accuracy:0.2381\n",
    "Epoch #141: Loss:1.5843, Accuracy:0.2632, Validation Loss:1.6110, Validation Accuracy:0.2397\n",
    "Epoch #142: Loss:1.5829, Accuracy:0.2756, Validation Loss:1.6147, Validation Accuracy:0.2233\n",
    "Epoch #143: Loss:1.5829, Accuracy:0.2575, Validation Loss:1.6174, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5848, Accuracy:0.2637, Validation Loss:1.6160, Validation Accuracy:0.2365\n",
    "Epoch #145: Loss:1.5832, Accuracy:0.2583, Validation Loss:1.6143, Validation Accuracy:0.2414\n",
    "Epoch #146: Loss:1.5796, Accuracy:0.2678, Validation Loss:1.6146, Validation Accuracy:0.2315\n",
    "Epoch #147: Loss:1.5835, Accuracy:0.2665, Validation Loss:1.6087, Validation Accuracy:0.2414\n",
    "Epoch #148: Loss:1.5817, Accuracy:0.2727, Validation Loss:1.6106, Validation Accuracy:0.2397\n",
    "Epoch #149: Loss:1.5809, Accuracy:0.2661, Validation Loss:1.6086, Validation Accuracy:0.2332\n",
    "Epoch #150: Loss:1.5792, Accuracy:0.2674, Validation Loss:1.6110, Validation Accuracy:0.2381\n",
    "Epoch #151: Loss:1.5766, Accuracy:0.2665, Validation Loss:1.6183, Validation Accuracy:0.2266\n",
    "Epoch #152: Loss:1.5773, Accuracy:0.2817, Validation Loss:1.6145, Validation Accuracy:0.2332\n",
    "Epoch #153: Loss:1.5793, Accuracy:0.2653, Validation Loss:1.6143, Validation Accuracy:0.2381\n",
    "Epoch #154: Loss:1.5777, Accuracy:0.2715, Validation Loss:1.6104, Validation Accuracy:0.2430\n",
    "Epoch #155: Loss:1.5761, Accuracy:0.2780, Validation Loss:1.6180, Validation Accuracy:0.2397\n",
    "Epoch #156: Loss:1.5774, Accuracy:0.2789, Validation Loss:1.6145, Validation Accuracy:0.2414\n",
    "Epoch #157: Loss:1.5735, Accuracy:0.2842, Validation Loss:1.6145, Validation Accuracy:0.2348\n",
    "Epoch #158: Loss:1.5725, Accuracy:0.2821, Validation Loss:1.6115, Validation Accuracy:0.2430\n",
    "Epoch #159: Loss:1.5716, Accuracy:0.2760, Validation Loss:1.6149, Validation Accuracy:0.2348\n",
    "Epoch #160: Loss:1.5697, Accuracy:0.2743, Validation Loss:1.6120, Validation Accuracy:0.2430\n",
    "Epoch #161: Loss:1.5691, Accuracy:0.2731, Validation Loss:1.6114, Validation Accuracy:0.2381\n",
    "Epoch #162: Loss:1.5702, Accuracy:0.2747, Validation Loss:1.6194, Validation Accuracy:0.2365\n",
    "Epoch #163: Loss:1.5714, Accuracy:0.2768, Validation Loss:1.6225, Validation Accuracy:0.2348\n",
    "Epoch #164: Loss:1.5795, Accuracy:0.2534, Validation Loss:1.6172, Validation Accuracy:0.2447\n",
    "Epoch #165: Loss:1.5698, Accuracy:0.2686, Validation Loss:1.6249, Validation Accuracy:0.2348\n",
    "Epoch #166: Loss:1.5689, Accuracy:0.2735, Validation Loss:1.6198, Validation Accuracy:0.2397\n",
    "Epoch #167: Loss:1.5710, Accuracy:0.2723, Validation Loss:1.6188, Validation Accuracy:0.2299\n",
    "Epoch #168: Loss:1.5729, Accuracy:0.2583, Validation Loss:1.6241, Validation Accuracy:0.2512\n",
    "Epoch #169: Loss:1.5728, Accuracy:0.2661, Validation Loss:1.6236, Validation Accuracy:0.2315\n",
    "Epoch #170: Loss:1.5718, Accuracy:0.2702, Validation Loss:1.6227, Validation Accuracy:0.2365\n",
    "Epoch #171: Loss:1.5732, Accuracy:0.2715, Validation Loss:1.6262, Validation Accuracy:0.2463\n",
    "Epoch #172: Loss:1.5716, Accuracy:0.2678, Validation Loss:1.6283, Validation Accuracy:0.2381\n",
    "Epoch #173: Loss:1.5685, Accuracy:0.2756, Validation Loss:1.6297, Validation Accuracy:0.2282\n",
    "Epoch #174: Loss:1.5723, Accuracy:0.2702, Validation Loss:1.6243, Validation Accuracy:0.2315\n",
    "Epoch #175: Loss:1.5734, Accuracy:0.2727, Validation Loss:1.6304, Validation Accuracy:0.2381\n",
    "Epoch #176: Loss:1.5723, Accuracy:0.2768, Validation Loss:1.6227, Validation Accuracy:0.2250\n",
    "Epoch #177: Loss:1.5723, Accuracy:0.2600, Validation Loss:1.6237, Validation Accuracy:0.2348\n",
    "Epoch #178: Loss:1.5668, Accuracy:0.2764, Validation Loss:1.6262, Validation Accuracy:0.2233\n",
    "Epoch #179: Loss:1.5663, Accuracy:0.2706, Validation Loss:1.6263, Validation Accuracy:0.2282\n",
    "Epoch #180: Loss:1.5699, Accuracy:0.2649, Validation Loss:1.6319, Validation Accuracy:0.2217\n",
    "Epoch #181: Loss:1.5687, Accuracy:0.2698, Validation Loss:1.6282, Validation Accuracy:0.2348\n",
    "Epoch #182: Loss:1.5710, Accuracy:0.2632, Validation Loss:1.6195, Validation Accuracy:0.2463\n",
    "Epoch #183: Loss:1.5697, Accuracy:0.2735, Validation Loss:1.6216, Validation Accuracy:0.2118\n",
    "Epoch #184: Loss:1.5675, Accuracy:0.2723, Validation Loss:1.6309, Validation Accuracy:0.2365\n",
    "Epoch #185: Loss:1.5697, Accuracy:0.2789, Validation Loss:1.6261, Validation Accuracy:0.2266\n",
    "Epoch #186: Loss:1.5707, Accuracy:0.2735, Validation Loss:1.6249, Validation Accuracy:0.2529\n",
    "Epoch #187: Loss:1.5681, Accuracy:0.2686, Validation Loss:1.6288, Validation Accuracy:0.2397\n",
    "Epoch #188: Loss:1.5686, Accuracy:0.2559, Validation Loss:1.6312, Validation Accuracy:0.2348\n",
    "Epoch #189: Loss:1.5682, Accuracy:0.2756, Validation Loss:1.6300, Validation Accuracy:0.2463\n",
    "Epoch #190: Loss:1.5661, Accuracy:0.2694, Validation Loss:1.6362, Validation Accuracy:0.2217\n",
    "Epoch #191: Loss:1.5641, Accuracy:0.2813, Validation Loss:1.6400, Validation Accuracy:0.2299\n",
    "Epoch #192: Loss:1.5629, Accuracy:0.2780, Validation Loss:1.6295, Validation Accuracy:0.2348\n",
    "Epoch #193: Loss:1.5642, Accuracy:0.2760, Validation Loss:1.6321, Validation Accuracy:0.2332\n",
    "Epoch #194: Loss:1.5647, Accuracy:0.2743, Validation Loss:1.6329, Validation Accuracy:0.2447\n",
    "Epoch #195: Loss:1.5668, Accuracy:0.2723, Validation Loss:1.6247, Validation Accuracy:0.2463\n",
    "Epoch #196: Loss:1.5663, Accuracy:0.2735, Validation Loss:1.6282, Validation Accuracy:0.2496\n",
    "Epoch #197: Loss:1.5653, Accuracy:0.2727, Validation Loss:1.6267, Validation Accuracy:0.2479\n",
    "Epoch #198: Loss:1.5643, Accuracy:0.2723, Validation Loss:1.6295, Validation Accuracy:0.2430\n",
    "Epoch #199: Loss:1.5644, Accuracy:0.2686, Validation Loss:1.6365, Validation Accuracy:0.2447\n",
    "Epoch #200: Loss:1.5644, Accuracy:0.2739, Validation Loss:1.6290, Validation Accuracy:0.2430\n",
    "Epoch #201: Loss:1.5674, Accuracy:0.2747, Validation Loss:1.6299, Validation Accuracy:0.2365\n",
    "Epoch #202: Loss:1.5687, Accuracy:0.2739, Validation Loss:1.6342, Validation Accuracy:0.2381\n",
    "Epoch #203: Loss:1.5704, Accuracy:0.2727, Validation Loss:1.6327, Validation Accuracy:0.2299\n",
    "Epoch #204: Loss:1.5748, Accuracy:0.2694, Validation Loss:1.6333, Validation Accuracy:0.2315\n",
    "Epoch #205: Loss:1.5731, Accuracy:0.2739, Validation Loss:1.6316, Validation Accuracy:0.2266\n",
    "Epoch #206: Loss:1.5699, Accuracy:0.2752, Validation Loss:1.6258, Validation Accuracy:0.2348\n",
    "Epoch #207: Loss:1.5666, Accuracy:0.2764, Validation Loss:1.6288, Validation Accuracy:0.2266\n",
    "Epoch #208: Loss:1.5637, Accuracy:0.2715, Validation Loss:1.6230, Validation Accuracy:0.2332\n",
    "Epoch #209: Loss:1.5649, Accuracy:0.2768, Validation Loss:1.6181, Validation Accuracy:0.2397\n",
    "Epoch #210: Loss:1.5659, Accuracy:0.2739, Validation Loss:1.6261, Validation Accuracy:0.2250\n",
    "Epoch #211: Loss:1.5686, Accuracy:0.2637, Validation Loss:1.6252, Validation Accuracy:0.2282\n",
    "Epoch #212: Loss:1.5656, Accuracy:0.2719, Validation Loss:1.6259, Validation Accuracy:0.2250\n",
    "Epoch #213: Loss:1.5682, Accuracy:0.2595, Validation Loss:1.6355, Validation Accuracy:0.2217\n",
    "Epoch #214: Loss:1.5717, Accuracy:0.2731, Validation Loss:1.6278, Validation Accuracy:0.2233\n",
    "Epoch #215: Loss:1.5763, Accuracy:0.2632, Validation Loss:1.6231, Validation Accuracy:0.2430\n",
    "Epoch #216: Loss:1.5663, Accuracy:0.2678, Validation Loss:1.6389, Validation Accuracy:0.2118\n",
    "Epoch #217: Loss:1.5664, Accuracy:0.2674, Validation Loss:1.6316, Validation Accuracy:0.2282\n",
    "Epoch #218: Loss:1.5629, Accuracy:0.2743, Validation Loss:1.6307, Validation Accuracy:0.2315\n",
    "Epoch #219: Loss:1.5617, Accuracy:0.2813, Validation Loss:1.6300, Validation Accuracy:0.2315\n",
    "Epoch #220: Loss:1.5626, Accuracy:0.2682, Validation Loss:1.6318, Validation Accuracy:0.2282\n",
    "Epoch #221: Loss:1.5587, Accuracy:0.2727, Validation Loss:1.6281, Validation Accuracy:0.2266\n",
    "Epoch #222: Loss:1.5572, Accuracy:0.2801, Validation Loss:1.6251, Validation Accuracy:0.2299\n",
    "Epoch #223: Loss:1.5592, Accuracy:0.2756, Validation Loss:1.6263, Validation Accuracy:0.2414\n",
    "Epoch #224: Loss:1.5592, Accuracy:0.2649, Validation Loss:1.6272, Validation Accuracy:0.2282\n",
    "Epoch #225: Loss:1.5574, Accuracy:0.2739, Validation Loss:1.6236, Validation Accuracy:0.2282\n",
    "Epoch #226: Loss:1.5575, Accuracy:0.2842, Validation Loss:1.6181, Validation Accuracy:0.2529\n",
    "Epoch #227: Loss:1.5601, Accuracy:0.2694, Validation Loss:1.6171, Validation Accuracy:0.2348\n",
    "Epoch #228: Loss:1.5598, Accuracy:0.2817, Validation Loss:1.6158, Validation Accuracy:0.2562\n",
    "Epoch #229: Loss:1.5580, Accuracy:0.2813, Validation Loss:1.6201, Validation Accuracy:0.2479\n",
    "Epoch #230: Loss:1.5610, Accuracy:0.2821, Validation Loss:1.6287, Validation Accuracy:0.2282\n",
    "Epoch #231: Loss:1.5682, Accuracy:0.2657, Validation Loss:1.6282, Validation Accuracy:0.2414\n",
    "Epoch #232: Loss:1.5617, Accuracy:0.2805, Validation Loss:1.6448, Validation Accuracy:0.2282\n",
    "Epoch #233: Loss:1.5633, Accuracy:0.2752, Validation Loss:1.6329, Validation Accuracy:0.2184\n",
    "Epoch #234: Loss:1.5673, Accuracy:0.2694, Validation Loss:1.6338, Validation Accuracy:0.2332\n",
    "Epoch #235: Loss:1.5604, Accuracy:0.2657, Validation Loss:1.6429, Validation Accuracy:0.2118\n",
    "Epoch #236: Loss:1.5597, Accuracy:0.2891, Validation Loss:1.6362, Validation Accuracy:0.2299\n",
    "Epoch #237: Loss:1.5562, Accuracy:0.2817, Validation Loss:1.6305, Validation Accuracy:0.2217\n",
    "Epoch #238: Loss:1.5565, Accuracy:0.2739, Validation Loss:1.6325, Validation Accuracy:0.2315\n",
    "Epoch #239: Loss:1.5550, Accuracy:0.2825, Validation Loss:1.6329, Validation Accuracy:0.2479\n",
    "Epoch #240: Loss:1.5581, Accuracy:0.2858, Validation Loss:1.6278, Validation Accuracy:0.2332\n",
    "Epoch #241: Loss:1.5566, Accuracy:0.2825, Validation Loss:1.6372, Validation Accuracy:0.2250\n",
    "Epoch #242: Loss:1.5555, Accuracy:0.2813, Validation Loss:1.6350, Validation Accuracy:0.2348\n",
    "Epoch #243: Loss:1.5544, Accuracy:0.2879, Validation Loss:1.6440, Validation Accuracy:0.2299\n",
    "Epoch #244: Loss:1.5574, Accuracy:0.2760, Validation Loss:1.6445, Validation Accuracy:0.2414\n",
    "Epoch #245: Loss:1.5571, Accuracy:0.2895, Validation Loss:1.6481, Validation Accuracy:0.2299\n",
    "Epoch #246: Loss:1.5591, Accuracy:0.2862, Validation Loss:1.6408, Validation Accuracy:0.2397\n",
    "Epoch #247: Loss:1.5577, Accuracy:0.2867, Validation Loss:1.6381, Validation Accuracy:0.2315\n",
    "Epoch #248: Loss:1.5540, Accuracy:0.2871, Validation Loss:1.6336, Validation Accuracy:0.2512\n",
    "Epoch #249: Loss:1.5511, Accuracy:0.2854, Validation Loss:1.6227, Validation Accuracy:0.2529\n",
    "Epoch #250: Loss:1.5515, Accuracy:0.2838, Validation Loss:1.6469, Validation Accuracy:0.2135\n",
    "Epoch #251: Loss:1.5545, Accuracy:0.2850, Validation Loss:1.6241, Validation Accuracy:0.2693\n",
    "Epoch #252: Loss:1.5524, Accuracy:0.2797, Validation Loss:1.6268, Validation Accuracy:0.2463\n",
    "Epoch #253: Loss:1.5528, Accuracy:0.2838, Validation Loss:1.6341, Validation Accuracy:0.2496\n",
    "Epoch #254: Loss:1.5525, Accuracy:0.2879, Validation Loss:1.6425, Validation Accuracy:0.2332\n",
    "Epoch #255: Loss:1.5555, Accuracy:0.2768, Validation Loss:1.6374, Validation Accuracy:0.2479\n",
    "Epoch #256: Loss:1.5565, Accuracy:0.2887, Validation Loss:1.6412, Validation Accuracy:0.2447\n",
    "Epoch #257: Loss:1.5583, Accuracy:0.2784, Validation Loss:1.6406, Validation Accuracy:0.2200\n",
    "Epoch #258: Loss:1.5592, Accuracy:0.2768, Validation Loss:1.6343, Validation Accuracy:0.2430\n",
    "Epoch #259: Loss:1.5579, Accuracy:0.2772, Validation Loss:1.6399, Validation Accuracy:0.2299\n",
    "Epoch #260: Loss:1.5556, Accuracy:0.2756, Validation Loss:1.6348, Validation Accuracy:0.2315\n",
    "Epoch #261: Loss:1.5543, Accuracy:0.2858, Validation Loss:1.6328, Validation Accuracy:0.2332\n",
    "Epoch #262: Loss:1.5534, Accuracy:0.2817, Validation Loss:1.6313, Validation Accuracy:0.2315\n",
    "Epoch #263: Loss:1.5534, Accuracy:0.2908, Validation Loss:1.6271, Validation Accuracy:0.2315\n",
    "Epoch #264: Loss:1.5528, Accuracy:0.2719, Validation Loss:1.6312, Validation Accuracy:0.2447\n",
    "Epoch #265: Loss:1.5503, Accuracy:0.2920, Validation Loss:1.6200, Validation Accuracy:0.2266\n",
    "Epoch #266: Loss:1.5533, Accuracy:0.2871, Validation Loss:1.6212, Validation Accuracy:0.2463\n",
    "Epoch #267: Loss:1.5545, Accuracy:0.2780, Validation Loss:1.6170, Validation Accuracy:0.2299\n",
    "Epoch #268: Loss:1.5549, Accuracy:0.2862, Validation Loss:1.6237, Validation Accuracy:0.2381\n",
    "Epoch #269: Loss:1.5585, Accuracy:0.2846, Validation Loss:1.6262, Validation Accuracy:0.2430\n",
    "Epoch #270: Loss:1.5529, Accuracy:0.2838, Validation Loss:1.6367, Validation Accuracy:0.2250\n",
    "Epoch #271: Loss:1.5484, Accuracy:0.2891, Validation Loss:1.6307, Validation Accuracy:0.2447\n",
    "Epoch #272: Loss:1.5498, Accuracy:0.2801, Validation Loss:1.6375, Validation Accuracy:0.2397\n",
    "Epoch #273: Loss:1.5504, Accuracy:0.2793, Validation Loss:1.6332, Validation Accuracy:0.2430\n",
    "Epoch #274: Loss:1.5496, Accuracy:0.2813, Validation Loss:1.6297, Validation Accuracy:0.2266\n",
    "Epoch #275: Loss:1.5478, Accuracy:0.2809, Validation Loss:1.6366, Validation Accuracy:0.2348\n",
    "Epoch #276: Loss:1.5472, Accuracy:0.2813, Validation Loss:1.6327, Validation Accuracy:0.2332\n",
    "Epoch #277: Loss:1.5475, Accuracy:0.2862, Validation Loss:1.6299, Validation Accuracy:0.2512\n",
    "Epoch #278: Loss:1.5459, Accuracy:0.2842, Validation Loss:1.6351, Validation Accuracy:0.2282\n",
    "Epoch #279: Loss:1.5473, Accuracy:0.2920, Validation Loss:1.6340, Validation Accuracy:0.2447\n",
    "Epoch #280: Loss:1.5489, Accuracy:0.2891, Validation Loss:1.6318, Validation Accuracy:0.2282\n",
    "Epoch #281: Loss:1.5500, Accuracy:0.2817, Validation Loss:1.6354, Validation Accuracy:0.2381\n",
    "Epoch #282: Loss:1.5516, Accuracy:0.2862, Validation Loss:1.6363, Validation Accuracy:0.2233\n",
    "Epoch #283: Loss:1.5540, Accuracy:0.2825, Validation Loss:1.6327, Validation Accuracy:0.2299\n",
    "Epoch #284: Loss:1.5546, Accuracy:0.2871, Validation Loss:1.6270, Validation Accuracy:0.2381\n",
    "Epoch #285: Loss:1.5522, Accuracy:0.2764, Validation Loss:1.6254, Validation Accuracy:0.2217\n",
    "Epoch #286: Loss:1.5493, Accuracy:0.2846, Validation Loss:1.6360, Validation Accuracy:0.2250\n",
    "Epoch #287: Loss:1.5478, Accuracy:0.2916, Validation Loss:1.6366, Validation Accuracy:0.2282\n",
    "Epoch #288: Loss:1.5461, Accuracy:0.2842, Validation Loss:1.6420, Validation Accuracy:0.2414\n",
    "Epoch #289: Loss:1.5474, Accuracy:0.2846, Validation Loss:1.6520, Validation Accuracy:0.2463\n",
    "Epoch #290: Loss:1.5592, Accuracy:0.2772, Validation Loss:1.6410, Validation Accuracy:0.2381\n",
    "Epoch #291: Loss:1.5593, Accuracy:0.2932, Validation Loss:1.6351, Validation Accuracy:0.2414\n",
    "Epoch #292: Loss:1.5640, Accuracy:0.2887, Validation Loss:1.6285, Validation Accuracy:0.2529\n",
    "Epoch #293: Loss:1.5565, Accuracy:0.2887, Validation Loss:1.6285, Validation Accuracy:0.2381\n",
    "Epoch #294: Loss:1.5482, Accuracy:0.2965, Validation Loss:1.6403, Validation Accuracy:0.2430\n",
    "Epoch #295: Loss:1.5470, Accuracy:0.2986, Validation Loss:1.6550, Validation Accuracy:0.2397\n",
    "Epoch #296: Loss:1.5565, Accuracy:0.2789, Validation Loss:1.6586, Validation Accuracy:0.2479\n",
    "Epoch #297: Loss:1.5535, Accuracy:0.2899, Validation Loss:1.6388, Validation Accuracy:0.2479\n",
    "Epoch #298: Loss:1.5450, Accuracy:0.3006, Validation Loss:1.6456, Validation Accuracy:0.2414\n",
    "Epoch #299: Loss:1.5454, Accuracy:0.2830, Validation Loss:1.6550, Validation Accuracy:0.2299\n",
    "Epoch #300: Loss:1.5474, Accuracy:0.2879, Validation Loss:1.6519, Validation Accuracy:0.2463\n",
    "\n",
    "Test:\n",
    "Test Loss:1.65194106, Accuracy:0.2463\n",
    "Labels: ['02', '05', '03', '04', '01']\n",
    "Confusion Matrix:\n",
    "      02  05  03  04  01\n",
    "t:02   2  47  10  11  44\n",
    "t:05   0  54  11  18  59\n",
    "t:03   0  40  17   6  52\n",
    "t:04   0  30  18  14  50\n",
    "t:01   0  33  12  18  63\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       1.00      0.02      0.03       114\n",
    "          05       0.26      0.38      0.31       142\n",
    "          03       0.25      0.15      0.19       115\n",
    "          04       0.21      0.12      0.16       112\n",
    "          01       0.24      0.50      0.32       126\n",
    "\n",
    "    accuracy                           0.25       609\n",
    "   macro avg       0.39      0.23      0.20       609\n",
    "weighted avg       0.38      0.25      0.21       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 02:27:56 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 53 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6081266399283323, 1.6052919657555316, 1.6049462425689196, 1.6048382503058523, 1.6049307888168811, 1.6048617830808918, 1.604512607522786, 1.60509450408234, 1.6053324642244036, 1.6049742753478302, 1.6044186056149612, 1.6045923867249137, 1.6046587062390958, 1.6046510151845872, 1.6048031293699894, 1.6047506175800692, 1.6049351934924698, 1.60465369338081, 1.6046534995923096, 1.6048385607589446, 1.605186358265493, 1.6045237329401603, 1.6052254604784335, 1.604608250760484, 1.6042764611627864, 1.6050657061324722, 1.6048724107162902, 1.6048727049224678, 1.6042686587288266, 1.6043182460741066, 1.6044144726347649, 1.6038009863964637, 1.6041883461189583, 1.6031411193274512, 1.6011497118985907, 1.6033696764208414, 1.6029472850226416, 1.6068458183254124, 1.6007195746370138, 1.6028364673623898, 1.6003934297655604, 1.6004588282950014, 1.6009359408677701, 1.5998917939431954, 1.6000662815003168, 1.6005608144847827, 1.6025822397523326, 1.6034814779002875, 1.6042061576310833, 1.6034088545832141, 1.6023860320277599, 1.6027207423509244, 1.6022816921885574, 1.6012442483886318, 1.5999011143870738, 1.5996101290134375, 1.599827727073519, 1.6031866707825309, 1.6039218579607057, 1.6044757830098344, 1.6039243799516525, 1.6045975281882952, 1.6042387732144059, 1.6034250237867358, 1.6024642752113405, 1.6023963404014976, 1.6041647794798677, 1.6020538319507842, 1.602097812740282, 1.6089737540591136, 1.6038950131640255, 1.6062692413580633, 1.6027422438701386, 1.5998933593236362, 1.6021451172961783, 1.5998305812453597, 1.5997241858778328, 1.6009740526061536, 1.601365220370551, 1.602970419258907, 1.6022761786121062, 1.5997389691999588, 1.5992326382150008, 1.6029651619139171, 1.6014708730779061, 1.6022692740648643, 1.6038315188512817, 1.6020901984181897, 1.6034933056541656, 1.6025748225464218, 1.601767805213803, 1.6044240273865573, 1.6011555308387393, 1.6014517042632956, 1.6025398116197884, 1.6046853963964678, 1.6052393419989224, 1.6051193446361374, 1.6043614957524441, 1.6051863431930542, 1.6081842726283082, 1.6038804659115269, 1.6025423208872478, 1.6057599688985664, 1.6051637218112038, 1.6040058308242773, 1.604742769341555, 1.602631263544994, 1.6034842502503168, 1.6030239044934853, 1.5987005914960588, 1.6009331626453618, 1.598962634850801, 1.6029052186286312, 1.6011657624800608, 1.6022121976749064, 1.6054074443228334, 1.603344913969682, 1.6039043324334281, 1.6075287785240386, 1.604004730731983, 1.6071323598742682, 1.607507961528446, 1.6072498880975157, 1.608785589731777, 1.6103615541567748, 1.609849955648037, 1.609436386715994, 1.608009022444927, 1.610591714801068, 1.6134093381305439, 1.6116922226641175, 1.6118002695403075, 1.6123553168010243, 1.6124733864575966, 1.6143140113608199, 1.6122799095849099, 1.611382826208481, 1.6131804535541627, 1.6101944323243766, 1.6109649158267945, 1.6146744605159915, 1.6174223988709975, 1.615983759828389, 1.614267783211957, 1.6145868833820611, 1.6086827115276567, 1.6106372519666925, 1.608577052360685, 1.610976662933337, 1.618257087439739, 1.6144927280094041, 1.6142672658553852, 1.6104152969930363, 1.617952448198165, 1.61448766660612, 1.6145052651466407, 1.6114703596910624, 1.614862005503111, 1.6120027624719053, 1.6113945470850652, 1.6193656400702465, 1.6225268185040829, 1.617211127907576, 1.6248669238709073, 1.6198084980787706, 1.618812094964026, 1.624073088658463, 1.6235773477256787, 1.6227453733704165, 1.6261557602921535, 1.6282529836804995, 1.6297223268471328, 1.624274184159653, 1.6303926183672375, 1.6227106723096374, 1.6237153280740497, 1.6261886283877645, 1.626260738654677, 1.631881565883242, 1.6282309550174157, 1.619455815340302, 1.6215832525090434, 1.630943779287667, 1.6260726962770735, 1.624876881272139, 1.6288395564152884, 1.631175199557212, 1.6299529981926353, 1.6361982916376274, 1.6400309574036371, 1.6295057116275156, 1.632075977834379, 1.6328694695126638, 1.6246520823054322, 1.6282445654297502, 1.6267016740463833, 1.6295162666411627, 1.6364657589171712, 1.6289960936763994, 1.6299345534423302, 1.6341782827878428, 1.6327016314457985, 1.633331200955145, 1.631600860891671, 1.625766439782379, 1.6287933004704993, 1.6230073116095782, 1.6181092483461001, 1.626132456148395, 1.6251642159836241, 1.625865599009008, 1.6354712948619048, 1.6278249379645036, 1.6231065377813254, 1.6388547003562814, 1.6315501266708123, 1.630731244784075, 1.6299999163460066, 1.6318033348359107, 1.628089599225713, 1.6250782241962227, 1.6263156373708314, 1.627150457676604, 1.6236150568146228, 1.618053216065092, 1.6170753087903478, 1.6157583828238626, 1.620147408131504, 1.628698946219947, 1.6282101329324281, 1.6447608406320582, 1.6329489547043599, 1.6337846098666513, 1.642939056081725, 1.6362040293432025, 1.6305482358180832, 1.6324983794113685, 1.6328622118397103, 1.6278215456870193, 1.6371608802250452, 1.6350284070999948, 1.6439969727558454, 1.644492668667059, 1.6481349211803993, 1.6408241163137902, 1.6380794549418984, 1.6335603889377637, 1.622749842446426, 1.6469083582043451, 1.6240514082274413, 1.626760579878082, 1.634096890834752, 1.6425151335586272, 1.6373716547767125, 1.641169884717719, 1.6405915544538074, 1.6343023219132071, 1.6399261031440522, 1.634786347254548, 1.6328057628155537, 1.6313136577214709, 1.6271410111723275, 1.6312169580428275, 1.620045430358799, 1.6212052142091573, 1.616966583262915, 1.6237473249043932, 1.6262365924117992, 1.6367469068818492, 1.630710196025266, 1.6375376915892552, 1.6331558117921325, 1.6297477832177198, 1.6365712659895322, 1.6326566627264414, 1.629881217953411, 1.6351332224061337, 1.6340406980420568, 1.6317978449447206, 1.6353710398493926, 1.636340938765427, 1.632730644911968, 1.6270067398184038, 1.625358336860519, 1.636043760185367, 1.6365612432091499, 1.6419661520737145, 1.6520208083154337, 1.6410189953147876, 1.635135400667175, 1.628503865209119, 1.6285152282620885, 1.6403200998290615, 1.6550043903548142, 1.6585804161375575, 1.638768532397516, 1.6455989880319104, 1.6549764615170082, 1.651941160654591], 'val_acc': [0.22988505695743122, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.2331691292070207, 0.22660098460996875, 0.2282430207347635, 0.23152709298435298, 0.22988505685955823, 0.23152709308222597, 0.22988505685955823, 0.22660098460996875, 0.22988505676168527, 0.23152709288648002, 0.23152709298435298, 0.23152709298435298, 0.23152709278860703, 0.23152709259286106, 0.23645320135873724, 0.23152709278860703, 0.2298850564680663, 0.22495894809368208, 0.23481116503819652, 0.238095237287786, 0.22660098421847683, 0.23645320116299126, 0.23645320116299126, 0.24137930963524848, 0.23481116503819652, 0.22824302063689053, 0.22988505676168527, 0.22003283971929785, 0.2200328405022817, 0.22824302044114456, 0.22824302063689053, 0.23316912901127476, 0.23809523748353198, 0.2348111651360695, 0.2298850566638123, 0.23316912891340177, 0.23152709278860703, 0.238095237287786, 0.23645320116299126, 0.23481116503819652, 0.23316912891340177, 0.23481116503819652, 0.23152709288648002, 0.2348111651360695, 0.23481116503819652, 0.2348111651360695, 0.23973727360832672, 0.2446633821784569, 0.2348111651360695, 0.22167487683505652, 0.2298850565659393, 0.22824302053901754, 0.21510673134491362, 0.22495894809368208, 0.2282430202453986, 0.22660098421847683, 0.22003283962142486, 0.22167487613771153, 0.23645320116299126, 0.2298850570553042, 0.22495894828942806, 0.23152709288648002, 0.2266009844142228, 0.23645320126086425, 0.22824302063689053, 0.22660098460996875, 0.22495894819155507, 0.22331691196888734, 0.21839080379024906, 0.238095237385659, 0.2298850566638123, 0.23481116494032353, 0.23645320116299126, 0.22824302063689053, 0.22003283981717084, 0.22495894819155507, 0.22003283962142486, 0.2266009843163498, 0.22824302044114456, 0.24302134556429728, 0.23316912891340177, 0.24137930963524848, 0.2233169121646333, 0.2266009843163498, 0.23152709269073404, 0.2101806229705294, 0.21674876756758135, 0.22495894828942806, 0.22824302053901754, 0.21346469522011888, 0.2216748757462196, 0.2216748758440926, 0.23973727351045374, 0.22988505676168527, 0.2364532014566102, 0.22824302034327157, 0.23645320106511827, 0.2315270924949881, 0.22660098412060387, 0.23973727351045374, 0.224958949084646, 0.22495894819155507, 0.23316912861978284, 0.23645320096724531, 0.22660098421847683, 0.23645320126086425, 0.23481116503819652, 0.2233169121646333, 0.2348111651360695, 0.22495894828942806, 0.22331691226250627, 0.22495894828942806, 0.22660098451209576, 0.2446633822763299, 0.2233169121646333, 0.2233169121646333, 0.23152709288648002, 0.23973727390194566, 0.23316912891340177, 0.23645320116299126, 0.22003283981717084, 0.23152709327797194, 0.2266009844142228, 0.23973727390194566, 0.23809523767927793, 0.23809523797289686, 0.23973727399981865, 0.23645320175022916, 0.22824302083263648, 0.2413793101246134, 0.238095237287786, 0.23973727351045374, 0.2233169121646333, 0.2298850566638123, 0.23645320106511827, 0.24137930943950253, 0.23152709278860703, 0.24137930973312147, 0.23973727360832672, 0.23316912891340177, 0.23809523777715091, 0.22660098451209576, 0.23316912910914772, 0.23809523748353198, 0.24302134615153514, 0.23973727351045374, 0.2413793099288674, 0.2348111651360695, 0.24302134615153514, 0.2348111652339425, 0.24302134615153514, 0.23809523767927793, 0.23645320126086425, 0.23481116533181545, 0.24466338188483797, 0.2348111652339425, 0.23973727341258075, 0.2298850564680663, 0.2512315265797629, 0.23152709269073404, 0.23645320116299126, 0.24630541820537868, 0.23809523718991302, 0.2282430207347635, 0.23152709278860703, 0.238095237385659, 0.22495894868091998, 0.2348111651360695, 0.2233169121646333, 0.22824302034327157, 0.22167487663931057, 0.2348111651360695, 0.2463054181075057, 0.21182265938894307, 0.23645320165235617, 0.2266009844142228, 0.2528735626066847, 0.23973727341258075, 0.2348111651360695, 0.2463054180096327, 0.2216748763334575, 0.2298850565659393, 0.23481116484245057, 0.23316912910914772, 0.24466338208058394, 0.24630541820537868, 0.24958949045496817, 0.24794745433017343, 0.2430213459557892, 0.24466338198271095, 0.2430213459557892, 0.23645320155448318, 0.23809523758140494, 0.22988505695743122, 0.23152709298435298, 0.22660098460996875, 0.23481116552756143, 0.22660098470784173, 0.2331691293048937, 0.23973727380407267, 0.224958948485174, 0.22824302083263648, 0.22495894868091998, 0.2216748758440926, 0.2233169126539982, 0.2430213459557892, 0.21182265938894307, 0.2282430211262554, 0.23152709278860703, 0.23152709288648002, 0.22824302053901754, 0.2266009844142228, 0.22988505685955823, 0.24137930963524848, 0.22824302053901754, 0.22824302053901754, 0.25287356319392257, 0.23481116503819652, 0.25615763316796525, 0.24794745462379236, 0.22824302053901754, 0.24137931022248635, 0.22824302093050947, 0.21839080408386802, 0.23316912901127476, 0.21182265938894307, 0.22988505676168527, 0.22167487623558452, 0.23152709288648002, 0.24794745254399153, 0.2331691292070207, 0.224958948485174, 0.23481116572330737, 0.22988505695743122, 0.24137931032035934, 0.22988505676168527, 0.23973727380407267, 0.23152709278860703, 0.25123152677550886, 0.2528735627045577, 0.2134646955137378, 0.26929392424612403, 0.24630541791175975, 0.24958949065071412, 0.23316912861978284, 0.24794745452591938, 0.2446633822763299, 0.22003283981717084, 0.24302134624940813, 0.2298850566638123, 0.23152709298435298, 0.23316912891340177, 0.23152709259286106, 0.23152709288648002, 0.24466338208058394, 0.22660098412060387, 0.24630541820537868, 0.22988505745903023, 0.23809523748353198, 0.2430213458579162, 0.224958949084646, 0.2446633822763299, 0.23973727399981865, 0.24302134624940813, 0.2266009843163498, 0.2348111656254344, 0.2331691288155288, 0.25123152687338185, 0.2282430202453986, 0.2446633821784569, 0.22824302044114456, 0.23809523767927793, 0.22331691187101435, 0.22988505695743122, 0.23809523718991302, 0.2216748758440926, 0.22495894877879294, 0.22824302044114456, 0.2413793099288674, 0.2463054185968706, 0.23809523767927793, 0.24137930973312147, 0.2528735630960496, 0.23809523777715091, 0.24302134605366216, 0.23973727351045374, 0.24794745433017343, 0.24794745462379236, 0.2413793100267404, 0.22988505685955823, 0.24630541849899762], 'loss': [1.6123527715827894, 1.60726025329723, 1.6048783248454883, 1.6051754310635327, 1.6049318951013396, 1.6044591434192854, 1.6041988743893665, 1.6038172163268134, 1.6036759775032499, 1.6034252212277673, 1.603244156024784, 1.6027556635271107, 1.6022914536924577, 1.6018711428377908, 1.601799730647516, 1.601882847133848, 1.6025329524731489, 1.601993977362615, 1.601815243080656, 1.601323040946553, 1.6006333370228323, 1.6000183212683676, 1.6000735200161318, 1.5996807604110217, 1.599202128504336, 1.5995381713647863, 1.598887928751215, 1.5986323904942192, 1.599114699872857, 1.5987960245575015, 1.598933928556266, 1.597950975850867, 1.597926702783338, 1.5984270529090991, 1.5982799277902873, 1.5998200588402562, 1.6021314851557205, 1.6024636931977478, 1.6005123401324608, 1.5993076387127323, 1.5998224498799694, 1.598504426102374, 1.5980474978746575, 1.5974487277761378, 1.5968856084028553, 1.5965916546952799, 1.5965906017859615, 1.5976906478772173, 1.5967562443911418, 1.5960334625577046, 1.5959566927543656, 1.5957810016628164, 1.59523414772394, 1.5956760849061689, 1.594198675909571, 1.594719492434476, 1.59455650063266, 1.5947420775278394, 1.5960953592030174, 1.5952888163697794, 1.5932221210957553, 1.5929728090395918, 1.5940414826238425, 1.593084396914535, 1.592054057806669, 1.5913267606582484, 1.591213645827354, 1.591972451337309, 1.5941515012932999, 1.5976293267165855, 1.597757558117657, 1.5936341241155072, 1.5963847712569659, 1.5954635241193202, 1.5951060814044804, 1.5941703832614593, 1.5938423154780017, 1.5948212873764351, 1.5937507574073588, 1.5930469392016684, 1.5926476545647184, 1.5913040655104775, 1.5925518730582642, 1.592739366114262, 1.5924627465633885, 1.5925963679867847, 1.5910146381820742, 1.5924433509427174, 1.5914176368615465, 1.592066215293853, 1.5923163920702141, 1.5908755984884022, 1.59042277703295, 1.5899554939974994, 1.5903551648040082, 1.5897264672500642, 1.5888034545420622, 1.5887490989735973, 1.588859721914209, 1.5888772565481355, 1.5900411160819585, 1.591008189081901, 1.5912947237124433, 1.5911170856174257, 1.5902782927793153, 1.5923703068335688, 1.5930807899400683, 1.5911972016524485, 1.59170984472827, 1.591256447641267, 1.5917181222101011, 1.5917136752385133, 1.5904227408050757, 1.588871896242459, 1.5897489669141829, 1.5884841293769696, 1.587269025710574, 1.585270651067307, 1.5861502317432505, 1.5861568937066644, 1.5860500272539362, 1.582480011289859, 1.5852182380962176, 1.5848075135287807, 1.5863684792782982, 1.5854609238293627, 1.5849049087422584, 1.5857092045660626, 1.5846996717629247, 1.5838217165435853, 1.5854873031071814, 1.584366751794208, 1.5855263978793637, 1.5875162269545287, 1.5859113076873874, 1.5855651560750095, 1.5834037191813977, 1.582842471712179, 1.5824542684476723, 1.5846173063685518, 1.5843117324723355, 1.5828507596707198, 1.5829296326000832, 1.58480018321004, 1.5831855299536453, 1.579583510236329, 1.583502369543855, 1.581669908088825, 1.5809283009299997, 1.57915317948594, 1.5766270691854019, 1.5773107218791327, 1.579325472794519, 1.5776537353253217, 1.5760591075650476, 1.5774345320108245, 1.5735023453984662, 1.572509632120387, 1.5715797557478324, 1.5696712189876079, 1.5691158509107586, 1.5701660985574586, 1.5714228156166155, 1.5794605102871968, 1.5698185969671918, 1.5688866071877294, 1.5710160025825735, 1.5728655925276833, 1.5728486852724204, 1.571816418645808, 1.5731593269587052, 1.571561672751174, 1.5685012288651672, 1.5723013707010163, 1.5733978651877056, 1.5722880344860852, 1.572263221965923, 1.5668362704635401, 1.56630198079213, 1.569862058813812, 1.5687189203023422, 1.5709759731801873, 1.5696971847291354, 1.5674599990707647, 1.5696768838522126, 1.5707224515918834, 1.5681391204896649, 1.5686226588744647, 1.568225896187142, 1.5660769351453996, 1.5640698497545058, 1.562854956552478, 1.564180786359971, 1.564734631789049, 1.566841364298513, 1.5663185615069568, 1.565263169502086, 1.5642787759064158, 1.564436299453281, 1.564447690280311, 1.5674456264938417, 1.5686509373251663, 1.570418253620547, 1.5747517818787748, 1.5731258276306872, 1.569864193761618, 1.5666395821365733, 1.5637299356029752, 1.564874676512497, 1.5658959580153167, 1.5685736907336256, 1.5655917207807977, 1.5682210823838472, 1.5717453663843612, 1.5762645373843778, 1.5663289400586358, 1.5663548909663174, 1.562888671191566, 1.5616587614866253, 1.5626328398314835, 1.5587042446253971, 1.5572150270552116, 1.5591857819586563, 1.5591650149171112, 1.557431334098017, 1.5575309682920484, 1.5600582881629834, 1.5598291893024954, 1.5579953704281755, 1.5609856785690026, 1.5682031616293184, 1.5616882671320953, 1.5632666395919768, 1.5673412025831563, 1.5604101369023569, 1.559661886579447, 1.5562360430155446, 1.55652713643452, 1.5549985744135582, 1.5580622569246703, 1.5566048375879715, 1.5555115332104097, 1.5544096997631158, 1.5574069615996593, 1.5570802770356134, 1.5590612483954773, 1.5577144685956732, 1.5539852140865287, 1.5511112450574214, 1.5515022845238875, 1.554473858447535, 1.5524246701959221, 1.5527544304085952, 1.5524882107055162, 1.5554999539494758, 1.5564808560592682, 1.5583338596003256, 1.5591600041614666, 1.557853484202704, 1.5556249288562876, 1.5543001027077865, 1.5533574869990103, 1.5533762162715747, 1.552825279793945, 1.5503405889691269, 1.553279133596949, 1.554489883456142, 1.5548747954182556, 1.5585448478036836, 1.552928612853957, 1.5483815945639012, 1.5497803938707042, 1.5503609942214935, 1.5496241876476844, 1.5477845531224714, 1.5472111228065568, 1.5475024015262142, 1.5458597057898675, 1.5472657833256027, 1.5488590480855358, 1.5500277465863395, 1.5516285251543018, 1.5540118839216919, 1.554621229573197, 1.5521761802677256, 1.5493236931932046, 1.54779463876689, 1.5461488924966456, 1.5474261919331012, 1.5592263819990217, 1.5592638149398552, 1.5640221701510388, 1.5565181771343004, 1.548174215782839, 1.5469883036074943, 1.5565493360437161, 1.553486395126985, 1.5449814609433592, 1.5454239744914875, 1.5473711276690818], 'acc': [0.20328542159935287, 0.232443531503178, 0.2328542083195837, 0.2328542083195837, 0.23285421010037957, 0.23285420851541005, 0.2328542083195837, 0.2328542083195837, 0.23285421049203225, 0.2328542087112364, 0.23367556355572333, 0.23983572802259692, 0.24065708482534734, 0.24229979431849485, 0.24188911652295741, 0.24188911709207775, 0.23737166309748342, 0.24229979568927928, 0.24312114935880813, 0.24394250498660047, 0.24106776089516507, 0.2476386039408815, 0.24476385924360836, 0.24476385963526104, 0.24640657108667205, 0.242710471544912, 0.2459958936644286, 0.24804928097147227, 0.24722792710611707, 0.2455852144981067, 0.24517453784080992, 0.2480492813814837, 0.2480492817914951, 0.25010266867016867, 0.2542094464174776, 0.2472279263411704, 0.23737166211835167, 0.2258726898098873, 0.2320328538667495, 0.2501026696493004, 0.24229979353518946, 0.24928131362985537, 0.24681724870474187, 0.2509240260603981, 0.25051334863815466, 0.252566735749372, 0.25379876782027605, 0.24722792573533264, 0.24969199107045756, 0.2529774133674418, 0.25133470172020445, 0.25215605814966086, 0.248870637009276, 0.25215605574466854, 0.25420944584835725, 0.25297741238831006, 0.2533880898105535, 0.2529774121924837, 0.23778233993224784, 0.25338809041639127, 0.2537987695827132, 0.24804928116729863, 0.25215605754382314, 0.2542094460258249, 0.2542094460258249, 0.2587268972421329, 0.2517453807274174, 0.24394250537825315, 0.24353182736853066, 0.2472279251478536, 0.2422997945143212, 0.24681724831308918, 0.23901437356976268, 0.24599589427026636, 0.24804928097147227, 0.24394250420329508, 0.24763860335340246, 0.25133470172020445, 0.25256673518025163, 0.25626283411617395, 0.25297741317161543, 0.2587269011953773, 0.2501026700409531, 0.2566735113425911, 0.25379876742862334, 0.24804928077564592, 0.25708418995815135, 0.25585215673064793, 0.2525667353577193, 0.2583162226165345, 0.253798767232797, 0.2640657086636741, 0.25954825603986426, 0.25544147809672896, 0.25585215532314604, 0.2599589322871496, 0.2542094456341722, 0.25749486599125165, 0.25338809039803256, 0.2607802886982473, 0.25297741160500464, 0.2620123207691514, 0.2583162206399123, 0.2476386039408815, 0.2632443510409009, 0.2525667353577193, 0.24599589307694955, 0.256262834703653, 0.2476386043508929, 0.265297742888668, 0.25338809039803256, 0.24763860374505514, 0.25462012344806834, 0.26119096592466445, 0.25092402570546285, 0.25626283568278474, 0.26406570964280585, 0.26406570807619506, 0.2566735113425911, 0.2648870622964855, 0.26324435364171955, 0.2661190965581968, 0.25544147809672896, 0.2591375774426627, 0.2685831642248792, 0.2603696118634829, 0.2546201252105055, 0.2620123207875101, 0.27022587430550576, 0.26817248601933036, 0.2652977417137099, 0.26447638409093666, 0.26940451946101884, 0.2681724844527196, 0.26201231980837836, 0.2640657088595005, 0.26406570608121416, 0.26488706210065915, 0.26529774108951343, 0.26365503067231033, 0.26324435460249257, 0.2755646795829953, 0.25749486540377264, 0.26365502865897067, 0.2583162225981757, 0.2677618054271479, 0.2665297729645911, 0.2726899366481593, 0.2661190947590423, 0.26735112882492723, 0.2665297745679193, 0.28172484741563425, 0.2652977399145553, 0.2714579039897762, 0.2780287486571796, 0.27885010428497187, 0.2841889099541141, 0.28213552503370404, 0.2759753613501359, 0.2743326504862039, 0.2731006166528627, 0.27474332732096834, 0.2767967165862755, 0.2533880913771643, 0.2685831616791367, 0.27351129583754336, 0.2722792622000285, 0.2583162220290554, 0.26611909636237047, 0.2702258741096794, 0.2714579046139727, 0.2677618078137815, 0.275564682557108, 0.2702258746971585, 0.272689940209751, 0.2767967130246838, 0.2599589323055083, 0.276386038576553, 0.27063655133609654, 0.2648870646831191, 0.26981519390914965, 0.2632443549941453, 0.2735112952500643, 0.27227926063341773, 0.27885010213088207, 0.273511295641717, 0.2685831602716348, 0.2558521567122892, 0.27556468216545527, 0.2694045185002458, 0.2813141678025835, 0.2780287486571796, 0.27597535935515494, 0.2743326506820303, 0.27227926161254945, 0.27351129208012526, 0.27268993944480435, 0.27227925863843677, 0.26858316125076653, 0.2739219707140443, 0.2747433241510293, 0.273921971693176, 0.2726899378598348, 0.2694045172702116, 0.2739219720848287, 0.2751540045473855, 0.276386035994093, 0.27145790379394985, 0.27679671501966474, 0.2739219707140443, 0.2636550308497779, 0.2718685842086647, 0.25954825408160076, 0.2731006152820783, 0.2632443544066662, 0.2677618084012605, 0.2673511284332745, 0.27433265087785663, 0.2813141680351273, 0.26817248523602494, 0.27268993645233297, 0.2800821333817633, 0.275564682557108, 0.26488706350816105, 0.27392196911071603, 0.28418891112907224, 0.2694045172702116, 0.28172484702398154, 0.28131416799840986, 0.2821355230754406, 0.2657084187443007, 0.28049281514890384, 0.27515400337242735, 0.26940451830441947, 0.2657084191359534, 0.2891170453241963, 0.2817248480031133, 0.2739219726723078, 0.2825462020642948, 0.28583162003474066, 0.28254619948183485, 0.2813141705808698, 0.28788500812508977, 0.2759753605668305, 0.2895277227464398, 0.2862423016060549, 0.28665297625001207, 0.28706365488393104, 0.285420944179108, 0.283778235897636, 0.285010266793582, 0.2796714589519912, 0.2837782349185043, 0.2878850112950288, 0.2767967138447066, 0.2887063673144738, 0.27843942529611765, 0.2767967165862755, 0.2772073928335609, 0.27556468017047436, 0.2858316235963324, 0.28172484561647965, 0.2907597550131702, 0.27186858121619334, 0.29199178767155326, 0.28706365328060285, 0.2780287496363113, 0.28624230141022855, 0.28459958897968585, 0.2837782339026551, 0.2891170453241963, 0.2800821333817633, 0.27926077892892903, 0.2813141680351273, 0.28090348920538194, 0.28131416942427045, 0.28624229784863686, 0.28418891014994047, 0.2919917862640514, 0.28911704254591, 0.28172484819893967, 0.28624229843611587, 0.2825462000693139, 0.28706365425973457, 0.2763860362266368, 0.2845995903137528, 0.29158111021259236, 0.28418890975828776, 0.28459958815966296, 0.2772073942043453, 0.29322382150489446, 0.288706365747863, 0.2887063637528821, 0.29650924006281937, 0.298562626782384, 0.2788501011150329, 0.2899383960196125, 0.3006160180426721, 0.2829568770999047, 0.2878850101200707]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
