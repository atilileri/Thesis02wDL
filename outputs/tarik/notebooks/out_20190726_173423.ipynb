{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf11.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 17:34:23 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '02', '01', '05', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000027101DEBE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002714D8D6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6106, Accuracy:0.2131, Validation Loss:1.6099, Validation Accuracy:0.2184\n",
    "Epoch #2: Loss:1.6080, Accuracy:0.2382, Validation Loss:1.6083, Validation Accuracy:0.2348\n",
    "Epoch #3: Loss:1.6063, Accuracy:0.2349, Validation Loss:1.6077, Validation Accuracy:0.2348\n",
    "Epoch #4: Loss:1.6051, Accuracy:0.2329, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6049, Accuracy:0.2324, Validation Loss:1.6068, Validation Accuracy:0.2299\n",
    "Epoch #6: Loss:1.6047, Accuracy:0.2324, Validation Loss:1.6066, Validation Accuracy:0.2348\n",
    "Epoch #7: Loss:1.6045, Accuracy:0.2341, Validation Loss:1.6065, Validation Accuracy:0.2315\n",
    "Epoch #8: Loss:1.6043, Accuracy:0.2333, Validation Loss:1.6065, Validation Accuracy:0.2282\n",
    "Epoch #9: Loss:1.6043, Accuracy:0.2337, Validation Loss:1.6065, Validation Accuracy:0.2299\n",
    "Epoch #10: Loss:1.6037, Accuracy:0.2353, Validation Loss:1.6060, Validation Accuracy:0.2315\n",
    "Epoch #11: Loss:1.6033, Accuracy:0.2308, Validation Loss:1.6059, Validation Accuracy:0.2348\n",
    "Epoch #12: Loss:1.6030, Accuracy:0.2316, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6023, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6022, Accuracy:0.2329, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #16: Loss:1.6019, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #17: Loss:1.6017, Accuracy:0.2329, Validation Loss:1.6055, Validation Accuracy:0.2332\n",
    "Epoch #18: Loss:1.6014, Accuracy:0.2333, Validation Loss:1.6054, Validation Accuracy:0.2397\n",
    "Epoch #19: Loss:1.6011, Accuracy:0.2382, Validation Loss:1.6053, Validation Accuracy:0.2430\n",
    "Epoch #20: Loss:1.6009, Accuracy:0.2370, Validation Loss:1.6053, Validation Accuracy:0.2414\n",
    "Epoch #21: Loss:1.6006, Accuracy:0.2394, Validation Loss:1.6053, Validation Accuracy:0.2365\n",
    "Epoch #22: Loss:1.6010, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2299\n",
    "Epoch #23: Loss:1.6006, Accuracy:0.2324, Validation Loss:1.6056, Validation Accuracy:0.2381\n",
    "Epoch #24: Loss:1.6004, Accuracy:0.2398, Validation Loss:1.6056, Validation Accuracy:0.2447\n",
    "Epoch #25: Loss:1.6004, Accuracy:0.2402, Validation Loss:1.6054, Validation Accuracy:0.2381\n",
    "Epoch #26: Loss:1.6003, Accuracy:0.2407, Validation Loss:1.6052, Validation Accuracy:0.2381\n",
    "Epoch #27: Loss:1.6004, Accuracy:0.2378, Validation Loss:1.6054, Validation Accuracy:0.2479\n",
    "Epoch #28: Loss:1.6007, Accuracy:0.2444, Validation Loss:1.6061, Validation Accuracy:0.2447\n",
    "Epoch #29: Loss:1.6006, Accuracy:0.2382, Validation Loss:1.6058, Validation Accuracy:0.2381\n",
    "Epoch #30: Loss:1.6004, Accuracy:0.2423, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #31: Loss:1.6001, Accuracy:0.2407, Validation Loss:1.6055, Validation Accuracy:0.2381\n",
    "Epoch #32: Loss:1.5997, Accuracy:0.2448, Validation Loss:1.6048, Validation Accuracy:0.2430\n",
    "Epoch #33: Loss:1.5997, Accuracy:0.2444, Validation Loss:1.6052, Validation Accuracy:0.2397\n",
    "Epoch #34: Loss:1.5997, Accuracy:0.2402, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #35: Loss:1.5996, Accuracy:0.2382, Validation Loss:1.6051, Validation Accuracy:0.2282\n",
    "Epoch #36: Loss:1.5990, Accuracy:0.2398, Validation Loss:1.6044, Validation Accuracy:0.2282\n",
    "Epoch #37: Loss:1.5991, Accuracy:0.2390, Validation Loss:1.6053, Validation Accuracy:0.2479\n",
    "Epoch #38: Loss:1.5996, Accuracy:0.2431, Validation Loss:1.6060, Validation Accuracy:0.2414\n",
    "Epoch #39: Loss:1.5989, Accuracy:0.2398, Validation Loss:1.6060, Validation Accuracy:0.2365\n",
    "Epoch #40: Loss:1.5987, Accuracy:0.2431, Validation Loss:1.6055, Validation Accuracy:0.2348\n",
    "Epoch #41: Loss:1.5989, Accuracy:0.2398, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #42: Loss:1.5983, Accuracy:0.2415, Validation Loss:1.6057, Validation Accuracy:0.2430\n",
    "Epoch #43: Loss:1.5980, Accuracy:0.2415, Validation Loss:1.6059, Validation Accuracy:0.2381\n",
    "Epoch #44: Loss:1.5978, Accuracy:0.2448, Validation Loss:1.6057, Validation Accuracy:0.2430\n",
    "Epoch #45: Loss:1.5976, Accuracy:0.2390, Validation Loss:1.6056, Validation Accuracy:0.2430\n",
    "Epoch #46: Loss:1.5975, Accuracy:0.2386, Validation Loss:1.6051, Validation Accuracy:0.2332\n",
    "Epoch #47: Loss:1.5974, Accuracy:0.2382, Validation Loss:1.6057, Validation Accuracy:0.2315\n",
    "Epoch #48: Loss:1.5971, Accuracy:0.2407, Validation Loss:1.6062, Validation Accuracy:0.2365\n",
    "Epoch #49: Loss:1.5971, Accuracy:0.2444, Validation Loss:1.6050, Validation Accuracy:0.2365\n",
    "Epoch #50: Loss:1.5982, Accuracy:0.2444, Validation Loss:1.6051, Validation Accuracy:0.2282\n",
    "Epoch #51: Loss:1.5984, Accuracy:0.2366, Validation Loss:1.6054, Validation Accuracy:0.2282\n",
    "Epoch #52: Loss:1.6028, Accuracy:0.2193, Validation Loss:1.6061, Validation Accuracy:0.1987\n",
    "Epoch #53: Loss:1.6019, Accuracy:0.2238, Validation Loss:1.6033, Validation Accuracy:0.2479\n",
    "Epoch #54: Loss:1.6010, Accuracy:0.2378, Validation Loss:1.6057, Validation Accuracy:0.2282\n",
    "Epoch #55: Loss:1.6020, Accuracy:0.2357, Validation Loss:1.6071, Validation Accuracy:0.2381\n",
    "Epoch #56: Loss:1.6007, Accuracy:0.2398, Validation Loss:1.6053, Validation Accuracy:0.2430\n",
    "Epoch #57: Loss:1.5999, Accuracy:0.2407, Validation Loss:1.6047, Validation Accuracy:0.2479\n",
    "Epoch #58: Loss:1.6001, Accuracy:0.2390, Validation Loss:1.6063, Validation Accuracy:0.2479\n",
    "Epoch #59: Loss:1.6044, Accuracy:0.2324, Validation Loss:1.6080, Validation Accuracy:0.2217\n",
    "Epoch #60: Loss:1.6025, Accuracy:0.2259, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #61: Loss:1.6010, Accuracy:0.2415, Validation Loss:1.6055, Validation Accuracy:0.2479\n",
    "Epoch #62: Loss:1.5998, Accuracy:0.2415, Validation Loss:1.6043, Validation Accuracy:0.2381\n",
    "Epoch #63: Loss:1.5983, Accuracy:0.2370, Validation Loss:1.6050, Validation Accuracy:0.2299\n",
    "Epoch #64: Loss:1.5988, Accuracy:0.2341, Validation Loss:1.6061, Validation Accuracy:0.2299\n",
    "Epoch #65: Loss:1.5986, Accuracy:0.2357, Validation Loss:1.6061, Validation Accuracy:0.2348\n",
    "Epoch #66: Loss:1.5990, Accuracy:0.2370, Validation Loss:1.6067, Validation Accuracy:0.2332\n",
    "Epoch #67: Loss:1.5983, Accuracy:0.2423, Validation Loss:1.6068, Validation Accuracy:0.2299\n",
    "Epoch #68: Loss:1.5977, Accuracy:0.2394, Validation Loss:1.6068, Validation Accuracy:0.2315\n",
    "Epoch #69: Loss:1.5978, Accuracy:0.2382, Validation Loss:1.6072, Validation Accuracy:0.2397\n",
    "Epoch #70: Loss:1.5970, Accuracy:0.2411, Validation Loss:1.6069, Validation Accuracy:0.2282\n",
    "Epoch #71: Loss:1.5968, Accuracy:0.2452, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #72: Loss:1.5969, Accuracy:0.2435, Validation Loss:1.6078, Validation Accuracy:0.2233\n",
    "Epoch #73: Loss:1.5968, Accuracy:0.2411, Validation Loss:1.6074, Validation Accuracy:0.2397\n",
    "Epoch #74: Loss:1.5965, Accuracy:0.2390, Validation Loss:1.6077, Validation Accuracy:0.2315\n",
    "Epoch #75: Loss:1.5959, Accuracy:0.2448, Validation Loss:1.6072, Validation Accuracy:0.2414\n",
    "Epoch #76: Loss:1.5954, Accuracy:0.2448, Validation Loss:1.6075, Validation Accuracy:0.2430\n",
    "Epoch #77: Loss:1.5955, Accuracy:0.2452, Validation Loss:1.6076, Validation Accuracy:0.2414\n",
    "Epoch #78: Loss:1.5957, Accuracy:0.2411, Validation Loss:1.6077, Validation Accuracy:0.2381\n",
    "Epoch #79: Loss:1.5950, Accuracy:0.2419, Validation Loss:1.6080, Validation Accuracy:0.2381\n",
    "Epoch #80: Loss:1.5947, Accuracy:0.2423, Validation Loss:1.6078, Validation Accuracy:0.2414\n",
    "Epoch #81: Loss:1.5946, Accuracy:0.2431, Validation Loss:1.6084, Validation Accuracy:0.2397\n",
    "Epoch #82: Loss:1.5942, Accuracy:0.2452, Validation Loss:1.6091, Validation Accuracy:0.2430\n",
    "Epoch #83: Loss:1.5937, Accuracy:0.2415, Validation Loss:1.6084, Validation Accuracy:0.2332\n",
    "Epoch #84: Loss:1.5935, Accuracy:0.2444, Validation Loss:1.6092, Validation Accuracy:0.2332\n",
    "Epoch #85: Loss:1.5931, Accuracy:0.2431, Validation Loss:1.6101, Validation Accuracy:0.2332\n",
    "Epoch #86: Loss:1.5939, Accuracy:0.2402, Validation Loss:1.6093, Validation Accuracy:0.2348\n",
    "Epoch #87: Loss:1.5934, Accuracy:0.2407, Validation Loss:1.6097, Validation Accuracy:0.2315\n",
    "Epoch #88: Loss:1.5933, Accuracy:0.2439, Validation Loss:1.6095, Validation Accuracy:0.2315\n",
    "Epoch #89: Loss:1.5932, Accuracy:0.2476, Validation Loss:1.6081, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:1.5931, Accuracy:0.2468, Validation Loss:1.6105, Validation Accuracy:0.2233\n",
    "Epoch #91: Loss:1.5943, Accuracy:0.2411, Validation Loss:1.6088, Validation Accuracy:0.2414\n",
    "Epoch #92: Loss:1.5935, Accuracy:0.2431, Validation Loss:1.6093, Validation Accuracy:0.2315\n",
    "Epoch #93: Loss:1.5918, Accuracy:0.2448, Validation Loss:1.6085, Validation Accuracy:0.2430\n",
    "Epoch #94: Loss:1.5925, Accuracy:0.2444, Validation Loss:1.6086, Validation Accuracy:0.2397\n",
    "Epoch #95: Loss:1.5934, Accuracy:0.2468, Validation Loss:1.6089, Validation Accuracy:0.2397\n",
    "Epoch #96: Loss:1.5903, Accuracy:0.2415, Validation Loss:1.6106, Validation Accuracy:0.2250\n",
    "Epoch #97: Loss:1.5919, Accuracy:0.2452, Validation Loss:1.6109, Validation Accuracy:0.2348\n",
    "Epoch #98: Loss:1.5892, Accuracy:0.2464, Validation Loss:1.6115, Validation Accuracy:0.2299\n",
    "Epoch #99: Loss:1.5902, Accuracy:0.2448, Validation Loss:1.6111, Validation Accuracy:0.2332\n",
    "Epoch #100: Loss:1.5890, Accuracy:0.2448, Validation Loss:1.6130, Validation Accuracy:0.2365\n",
    "Epoch #101: Loss:1.5874, Accuracy:0.2501, Validation Loss:1.6115, Validation Accuracy:0.2299\n",
    "Epoch #102: Loss:1.5885, Accuracy:0.2505, Validation Loss:1.6123, Validation Accuracy:0.2250\n",
    "Epoch #103: Loss:1.5891, Accuracy:0.2419, Validation Loss:1.6135, Validation Accuracy:0.2299\n",
    "Epoch #104: Loss:1.5866, Accuracy:0.2517, Validation Loss:1.6138, Validation Accuracy:0.2282\n",
    "Epoch #105: Loss:1.5877, Accuracy:0.2505, Validation Loss:1.6127, Validation Accuracy:0.2332\n",
    "Epoch #106: Loss:1.5856, Accuracy:0.2464, Validation Loss:1.6138, Validation Accuracy:0.2348\n",
    "Epoch #107: Loss:1.5846, Accuracy:0.2522, Validation Loss:1.6132, Validation Accuracy:0.2299\n",
    "Epoch #108: Loss:1.5860, Accuracy:0.2522, Validation Loss:1.6134, Validation Accuracy:0.2266\n",
    "Epoch #109: Loss:1.5873, Accuracy:0.2411, Validation Loss:1.6140, Validation Accuracy:0.2282\n",
    "Epoch #110: Loss:1.5847, Accuracy:0.2571, Validation Loss:1.6128, Validation Accuracy:0.2299\n",
    "Epoch #111: Loss:1.5855, Accuracy:0.2501, Validation Loss:1.6133, Validation Accuracy:0.2282\n",
    "Epoch #112: Loss:1.5849, Accuracy:0.2398, Validation Loss:1.6142, Validation Accuracy:0.2250\n",
    "Epoch #113: Loss:1.5843, Accuracy:0.2522, Validation Loss:1.6140, Validation Accuracy:0.2250\n",
    "Epoch #114: Loss:1.5830, Accuracy:0.2522, Validation Loss:1.6100, Validation Accuracy:0.2381\n",
    "Epoch #115: Loss:1.5829, Accuracy:0.2497, Validation Loss:1.6114, Validation Accuracy:0.2282\n",
    "Epoch #116: Loss:1.5833, Accuracy:0.2513, Validation Loss:1.6132, Validation Accuracy:0.2233\n",
    "Epoch #117: Loss:1.5826, Accuracy:0.2526, Validation Loss:1.6151, Validation Accuracy:0.2250\n",
    "Epoch #118: Loss:1.5833, Accuracy:0.2497, Validation Loss:1.6144, Validation Accuracy:0.2332\n",
    "Epoch #119: Loss:1.5830, Accuracy:0.2505, Validation Loss:1.6162, Validation Accuracy:0.2365\n",
    "Epoch #120: Loss:1.5825, Accuracy:0.2468, Validation Loss:1.6188, Validation Accuracy:0.2167\n",
    "Epoch #121: Loss:1.5818, Accuracy:0.2538, Validation Loss:1.6152, Validation Accuracy:0.2217\n",
    "Epoch #122: Loss:1.5814, Accuracy:0.2452, Validation Loss:1.6119, Validation Accuracy:0.2250\n",
    "Epoch #123: Loss:1.5820, Accuracy:0.2480, Validation Loss:1.6143, Validation Accuracy:0.2315\n",
    "Epoch #124: Loss:1.5829, Accuracy:0.2493, Validation Loss:1.6184, Validation Accuracy:0.2266\n",
    "Epoch #125: Loss:1.5828, Accuracy:0.2493, Validation Loss:1.6181, Validation Accuracy:0.2250\n",
    "Epoch #126: Loss:1.5820, Accuracy:0.2571, Validation Loss:1.6139, Validation Accuracy:0.2332\n",
    "Epoch #127: Loss:1.5822, Accuracy:0.2546, Validation Loss:1.6139, Validation Accuracy:0.2200\n",
    "Epoch #128: Loss:1.5814, Accuracy:0.2550, Validation Loss:1.6126, Validation Accuracy:0.2299\n",
    "Epoch #129: Loss:1.5815, Accuracy:0.2538, Validation Loss:1.6109, Validation Accuracy:0.2381\n",
    "Epoch #130: Loss:1.5863, Accuracy:0.2456, Validation Loss:1.6027, Validation Accuracy:0.2496\n",
    "Epoch #131: Loss:1.6031, Accuracy:0.2390, Validation Loss:1.6343, Validation Accuracy:0.2184\n",
    "Epoch #132: Loss:1.6170, Accuracy:0.2156, Validation Loss:1.6235, Validation Accuracy:0.2069\n",
    "Epoch #133: Loss:1.6073, Accuracy:0.2366, Validation Loss:1.6239, Validation Accuracy:0.2299\n",
    "Epoch #134: Loss:1.6026, Accuracy:0.2374, Validation Loss:1.6066, Validation Accuracy:0.2315\n",
    "Epoch #135: Loss:1.5992, Accuracy:0.2448, Validation Loss:1.6062, Validation Accuracy:0.2250\n",
    "Epoch #136: Loss:1.5971, Accuracy:0.2341, Validation Loss:1.6060, Validation Accuracy:0.1938\n",
    "Epoch #137: Loss:1.5978, Accuracy:0.2333, Validation Loss:1.6043, Validation Accuracy:0.2365\n",
    "Epoch #138: Loss:1.5946, Accuracy:0.2407, Validation Loss:1.6050, Validation Accuracy:0.2496\n",
    "Epoch #139: Loss:1.5957, Accuracy:0.2452, Validation Loss:1.6049, Validation Accuracy:0.2529\n",
    "Epoch #140: Loss:1.5941, Accuracy:0.2497, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #141: Loss:1.5939, Accuracy:0.2526, Validation Loss:1.6048, Validation Accuracy:0.2414\n",
    "Epoch #142: Loss:1.5930, Accuracy:0.2538, Validation Loss:1.6063, Validation Accuracy:0.2430\n",
    "Epoch #143: Loss:1.5928, Accuracy:0.2509, Validation Loss:1.6077, Validation Accuracy:0.2496\n",
    "Epoch #144: Loss:1.5916, Accuracy:0.2554, Validation Loss:1.6071, Validation Accuracy:0.2447\n",
    "Epoch #145: Loss:1.5917, Accuracy:0.2472, Validation Loss:1.6061, Validation Accuracy:0.2463\n",
    "Epoch #146: Loss:1.5903, Accuracy:0.2530, Validation Loss:1.6073, Validation Accuracy:0.2463\n",
    "Epoch #147: Loss:1.5902, Accuracy:0.2530, Validation Loss:1.6078, Validation Accuracy:0.2479\n",
    "Epoch #148: Loss:1.5889, Accuracy:0.2579, Validation Loss:1.6087, Validation Accuracy:0.2496\n",
    "Epoch #149: Loss:1.5882, Accuracy:0.2575, Validation Loss:1.6101, Validation Accuracy:0.2447\n",
    "Epoch #150: Loss:1.5866, Accuracy:0.2554, Validation Loss:1.6128, Validation Accuracy:0.2430\n",
    "Epoch #151: Loss:1.5903, Accuracy:0.2472, Validation Loss:1.6180, Validation Accuracy:0.2118\n",
    "Epoch #152: Loss:1.5992, Accuracy:0.2411, Validation Loss:1.6135, Validation Accuracy:0.2085\n",
    "Epoch #153: Loss:1.5979, Accuracy:0.2304, Validation Loss:1.6129, Validation Accuracy:0.2381\n",
    "Epoch #154: Loss:1.6000, Accuracy:0.2267, Validation Loss:1.6078, Validation Accuracy:0.2266\n",
    "Epoch #155: Loss:1.6006, Accuracy:0.2308, Validation Loss:1.6099, Validation Accuracy:0.2414\n",
    "Epoch #156: Loss:1.5967, Accuracy:0.2394, Validation Loss:1.6053, Validation Accuracy:0.2266\n",
    "Epoch #157: Loss:1.5955, Accuracy:0.2324, Validation Loss:1.6044, Validation Accuracy:0.2250\n",
    "Epoch #158: Loss:1.5947, Accuracy:0.2407, Validation Loss:1.6033, Validation Accuracy:0.2348\n",
    "Epoch #159: Loss:1.5940, Accuracy:0.2386, Validation Loss:1.6041, Validation Accuracy:0.2250\n",
    "Epoch #160: Loss:1.5955, Accuracy:0.2439, Validation Loss:1.6926, Validation Accuracy:0.1921\n",
    "Epoch #161: Loss:1.6356, Accuracy:0.2164, Validation Loss:1.6395, Validation Accuracy:0.2036\n",
    "Epoch #162: Loss:1.6244, Accuracy:0.1947, Validation Loss:1.6149, Validation Accuracy:0.2332\n",
    "Epoch #163: Loss:1.6102, Accuracy:0.2349, Validation Loss:1.6160, Validation Accuracy:0.2381\n",
    "Epoch #164: Loss:1.6110, Accuracy:0.2337, Validation Loss:1.6072, Validation Accuracy:0.2348\n",
    "Epoch #165: Loss:1.6040, Accuracy:0.2345, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #166: Loss:1.6017, Accuracy:0.2349, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #167: Loss:1.6018, Accuracy:0.2337, Validation Loss:1.6051, Validation Accuracy:0.2282\n",
    "Epoch #168: Loss:1.6019, Accuracy:0.2390, Validation Loss:1.6049, Validation Accuracy:0.2381\n",
    "Epoch #169: Loss:1.6015, Accuracy:0.2407, Validation Loss:1.6048, Validation Accuracy:0.2365\n",
    "Epoch #170: Loss:1.6010, Accuracy:0.2361, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #171: Loss:1.6003, Accuracy:0.2361, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #172: Loss:1.6002, Accuracy:0.2361, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #173: Loss:1.5999, Accuracy:0.2357, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #174: Loss:1.5997, Accuracy:0.2361, Validation Loss:1.6045, Validation Accuracy:0.2299\n",
    "Epoch #175: Loss:1.5992, Accuracy:0.2370, Validation Loss:1.6049, Validation Accuracy:0.2381\n",
    "Epoch #176: Loss:1.5991, Accuracy:0.2439, Validation Loss:1.6057, Validation Accuracy:0.2430\n",
    "Epoch #177: Loss:1.5987, Accuracy:0.2456, Validation Loss:1.6055, Validation Accuracy:0.2397\n",
    "Epoch #178: Loss:1.5984, Accuracy:0.2431, Validation Loss:1.6053, Validation Accuracy:0.2381\n",
    "Epoch #179: Loss:1.5982, Accuracy:0.2423, Validation Loss:1.6053, Validation Accuracy:0.2381\n",
    "Epoch #180: Loss:1.5979, Accuracy:0.2435, Validation Loss:1.6054, Validation Accuracy:0.2365\n",
    "Epoch #181: Loss:1.5976, Accuracy:0.2435, Validation Loss:1.6060, Validation Accuracy:0.2365\n",
    "Epoch #182: Loss:1.5978, Accuracy:0.2427, Validation Loss:1.6057, Validation Accuracy:0.2282\n",
    "Epoch #183: Loss:1.5977, Accuracy:0.2427, Validation Loss:1.6054, Validation Accuracy:0.2447\n",
    "Epoch #184: Loss:1.5975, Accuracy:0.2427, Validation Loss:1.6064, Validation Accuracy:0.2479\n",
    "Epoch #185: Loss:1.5970, Accuracy:0.2427, Validation Loss:1.6061, Validation Accuracy:0.2397\n",
    "Epoch #186: Loss:1.5966, Accuracy:0.2448, Validation Loss:1.6057, Validation Accuracy:0.2315\n",
    "Epoch #187: Loss:1.5967, Accuracy:0.2444, Validation Loss:1.6060, Validation Accuracy:0.2365\n",
    "Epoch #188: Loss:1.5961, Accuracy:0.2431, Validation Loss:1.6061, Validation Accuracy:0.2282\n",
    "Epoch #189: Loss:1.5961, Accuracy:0.2411, Validation Loss:1.6058, Validation Accuracy:0.2315\n",
    "Epoch #190: Loss:1.5956, Accuracy:0.2394, Validation Loss:1.6057, Validation Accuracy:0.2348\n",
    "Epoch #191: Loss:1.5957, Accuracy:0.2394, Validation Loss:1.6063, Validation Accuracy:0.2348\n",
    "Epoch #192: Loss:1.5953, Accuracy:0.2456, Validation Loss:1.6061, Validation Accuracy:0.2315\n",
    "Epoch #193: Loss:1.5949, Accuracy:0.2439, Validation Loss:1.6059, Validation Accuracy:0.2315\n",
    "Epoch #194: Loss:1.5947, Accuracy:0.2431, Validation Loss:1.6063, Validation Accuracy:0.2299\n",
    "Epoch #195: Loss:1.5945, Accuracy:0.2431, Validation Loss:1.6063, Validation Accuracy:0.2348\n",
    "Epoch #196: Loss:1.5940, Accuracy:0.2448, Validation Loss:1.6071, Validation Accuracy:0.2348\n",
    "Epoch #197: Loss:1.5942, Accuracy:0.2448, Validation Loss:1.6070, Validation Accuracy:0.2397\n",
    "Epoch #198: Loss:1.5938, Accuracy:0.2423, Validation Loss:1.6061, Validation Accuracy:0.2348\n",
    "Epoch #199: Loss:1.5940, Accuracy:0.2444, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #200: Loss:1.5949, Accuracy:0.2431, Validation Loss:1.6075, Validation Accuracy:0.2414\n",
    "Epoch #201: Loss:1.5942, Accuracy:0.2398, Validation Loss:1.6073, Validation Accuracy:0.2397\n",
    "Epoch #202: Loss:1.5942, Accuracy:0.2411, Validation Loss:1.6085, Validation Accuracy:0.2447\n",
    "Epoch #203: Loss:1.5941, Accuracy:0.2460, Validation Loss:1.6071, Validation Accuracy:0.2430\n",
    "Epoch #204: Loss:1.5930, Accuracy:0.2452, Validation Loss:1.6066, Validation Accuracy:0.2381\n",
    "Epoch #205: Loss:1.5930, Accuracy:0.2370, Validation Loss:1.6077, Validation Accuracy:0.2332\n",
    "Epoch #206: Loss:1.5930, Accuracy:0.2419, Validation Loss:1.6078, Validation Accuracy:0.2365\n",
    "Epoch #207: Loss:1.5926, Accuracy:0.2476, Validation Loss:1.6084, Validation Accuracy:0.2299\n",
    "Epoch #208: Loss:1.5929, Accuracy:0.2423, Validation Loss:1.6082, Validation Accuracy:0.2397\n",
    "Epoch #209: Loss:1.5934, Accuracy:0.2398, Validation Loss:1.6073, Validation Accuracy:0.2315\n",
    "Epoch #210: Loss:1.5919, Accuracy:0.2444, Validation Loss:1.6078, Validation Accuracy:0.2381\n",
    "Epoch #211: Loss:1.5927, Accuracy:0.2435, Validation Loss:1.6082, Validation Accuracy:0.2414\n",
    "Epoch #212: Loss:1.5940, Accuracy:0.2411, Validation Loss:1.6090, Validation Accuracy:0.2348\n",
    "Epoch #213: Loss:1.5928, Accuracy:0.2415, Validation Loss:1.6099, Validation Accuracy:0.2430\n",
    "Epoch #214: Loss:1.5927, Accuracy:0.2452, Validation Loss:1.6141, Validation Accuracy:0.2200\n",
    "Epoch #215: Loss:1.5977, Accuracy:0.2341, Validation Loss:1.6097, Validation Accuracy:0.2397\n",
    "Epoch #216: Loss:1.5957, Accuracy:0.2398, Validation Loss:1.6099, Validation Accuracy:0.2348\n",
    "Epoch #217: Loss:1.5922, Accuracy:0.2419, Validation Loss:1.6099, Validation Accuracy:0.2332\n",
    "Epoch #218: Loss:1.5930, Accuracy:0.2419, Validation Loss:1.6074, Validation Accuracy:0.2348\n",
    "Epoch #219: Loss:1.5938, Accuracy:0.2472, Validation Loss:1.6082, Validation Accuracy:0.2315\n",
    "Epoch #220: Loss:1.5936, Accuracy:0.2407, Validation Loss:1.6095, Validation Accuracy:0.2315\n",
    "Epoch #221: Loss:1.5929, Accuracy:0.2407, Validation Loss:1.6097, Validation Accuracy:0.2282\n",
    "Epoch #222: Loss:1.5921, Accuracy:0.2448, Validation Loss:1.6090, Validation Accuracy:0.2332\n",
    "Epoch #223: Loss:1.5925, Accuracy:0.2411, Validation Loss:1.6082, Validation Accuracy:0.2315\n",
    "Epoch #224: Loss:1.5954, Accuracy:0.2370, Validation Loss:1.6092, Validation Accuracy:0.2430\n",
    "Epoch #225: Loss:1.5938, Accuracy:0.2448, Validation Loss:1.6113, Validation Accuracy:0.2266\n",
    "Epoch #226: Loss:1.5928, Accuracy:0.2444, Validation Loss:1.6099, Validation Accuracy:0.2430\n",
    "Epoch #227: Loss:1.5925, Accuracy:0.2489, Validation Loss:1.6086, Validation Accuracy:0.2348\n",
    "Epoch #228: Loss:1.5962, Accuracy:0.2394, Validation Loss:1.6080, Validation Accuracy:0.2397\n",
    "Epoch #229: Loss:1.5936, Accuracy:0.2431, Validation Loss:1.6092, Validation Accuracy:0.2397\n",
    "Epoch #230: Loss:1.5908, Accuracy:0.2431, Validation Loss:1.6101, Validation Accuracy:0.2332\n",
    "Epoch #231: Loss:1.5912, Accuracy:0.2394, Validation Loss:1.6109, Validation Accuracy:0.2332\n",
    "Epoch #232: Loss:1.5907, Accuracy:0.2452, Validation Loss:1.6082, Validation Accuracy:0.2348\n",
    "Epoch #233: Loss:1.5907, Accuracy:0.2452, Validation Loss:1.6074, Validation Accuracy:0.2348\n",
    "Epoch #234: Loss:1.5903, Accuracy:0.2476, Validation Loss:1.6097, Validation Accuracy:0.2365\n",
    "Epoch #235: Loss:1.5908, Accuracy:0.2468, Validation Loss:1.6091, Validation Accuracy:0.2348\n",
    "Epoch #236: Loss:1.5902, Accuracy:0.2476, Validation Loss:1.6093, Validation Accuracy:0.2348\n",
    "Epoch #237: Loss:1.5901, Accuracy:0.2480, Validation Loss:1.6099, Validation Accuracy:0.2348\n",
    "Epoch #238: Loss:1.5897, Accuracy:0.2456, Validation Loss:1.6097, Validation Accuracy:0.2348\n",
    "Epoch #239: Loss:1.5896, Accuracy:0.2476, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #240: Loss:1.5900, Accuracy:0.2550, Validation Loss:1.6081, Validation Accuracy:0.2348\n",
    "Epoch #241: Loss:1.5923, Accuracy:0.2468, Validation Loss:1.6082, Validation Accuracy:0.2348\n",
    "Epoch #242: Loss:1.5900, Accuracy:0.2493, Validation Loss:1.6093, Validation Accuracy:0.2365\n",
    "Epoch #243: Loss:1.5899, Accuracy:0.2460, Validation Loss:1.6123, Validation Accuracy:0.2266\n",
    "Epoch #244: Loss:1.5898, Accuracy:0.2468, Validation Loss:1.6099, Validation Accuracy:0.2397\n",
    "Epoch #245: Loss:1.5908, Accuracy:0.2472, Validation Loss:1.6105, Validation Accuracy:0.2299\n",
    "Epoch #246: Loss:1.5897, Accuracy:0.2464, Validation Loss:1.6116, Validation Accuracy:0.2315\n",
    "Epoch #247: Loss:1.5892, Accuracy:0.2480, Validation Loss:1.6122, Validation Accuracy:0.2365\n",
    "Epoch #248: Loss:1.5893, Accuracy:0.2480, Validation Loss:1.6131, Validation Accuracy:0.2365\n",
    "Epoch #249: Loss:1.5906, Accuracy:0.2431, Validation Loss:1.6138, Validation Accuracy:0.2282\n",
    "Epoch #250: Loss:1.5896, Accuracy:0.2489, Validation Loss:1.6130, Validation Accuracy:0.2332\n",
    "Epoch #251: Loss:1.5891, Accuracy:0.2452, Validation Loss:1.6131, Validation Accuracy:0.2332\n",
    "Epoch #252: Loss:1.5884, Accuracy:0.2480, Validation Loss:1.6116, Validation Accuracy:0.2315\n",
    "Epoch #253: Loss:1.5909, Accuracy:0.2427, Validation Loss:1.6127, Validation Accuracy:0.2365\n",
    "Epoch #254: Loss:1.5896, Accuracy:0.2427, Validation Loss:1.6116, Validation Accuracy:0.2414\n",
    "Epoch #255: Loss:1.5891, Accuracy:0.2439, Validation Loss:1.6109, Validation Accuracy:0.2397\n",
    "Epoch #256: Loss:1.5902, Accuracy:0.2419, Validation Loss:1.6116, Validation Accuracy:0.2299\n",
    "Epoch #257: Loss:1.5899, Accuracy:0.2554, Validation Loss:1.6109, Validation Accuracy:0.2414\n",
    "Epoch #258: Loss:1.5891, Accuracy:0.2493, Validation Loss:1.6137, Validation Accuracy:0.2299\n",
    "Epoch #259: Loss:1.5901, Accuracy:0.2415, Validation Loss:1.6116, Validation Accuracy:0.2397\n",
    "Epoch #260: Loss:1.5886, Accuracy:0.2452, Validation Loss:1.6130, Validation Accuracy:0.2266\n",
    "Epoch #261: Loss:1.5887, Accuracy:0.2431, Validation Loss:1.6102, Validation Accuracy:0.2365\n",
    "Epoch #262: Loss:1.5888, Accuracy:0.2452, Validation Loss:1.6095, Validation Accuracy:0.2430\n",
    "Epoch #263: Loss:1.5903, Accuracy:0.2439, Validation Loss:1.6109, Validation Accuracy:0.2414\n",
    "Epoch #264: Loss:1.5877, Accuracy:0.2439, Validation Loss:1.6091, Validation Accuracy:0.2447\n",
    "Epoch #265: Loss:1.5882, Accuracy:0.2468, Validation Loss:1.6110, Validation Accuracy:0.2332\n",
    "Epoch #266: Loss:1.5884, Accuracy:0.2439, Validation Loss:1.6109, Validation Accuracy:0.2414\n",
    "Epoch #267: Loss:1.5883, Accuracy:0.2444, Validation Loss:1.6112, Validation Accuracy:0.2348\n",
    "Epoch #268: Loss:1.5882, Accuracy:0.2398, Validation Loss:1.6122, Validation Accuracy:0.2332\n",
    "Epoch #269: Loss:1.5894, Accuracy:0.2452, Validation Loss:1.6128, Validation Accuracy:0.2315\n",
    "Epoch #270: Loss:1.5895, Accuracy:0.2452, Validation Loss:1.6119, Validation Accuracy:0.2365\n",
    "Epoch #271: Loss:1.5892, Accuracy:0.2390, Validation Loss:1.6120, Validation Accuracy:0.2332\n",
    "Epoch #272: Loss:1.5895, Accuracy:0.2452, Validation Loss:1.6092, Validation Accuracy:0.2397\n",
    "Epoch #273: Loss:1.5896, Accuracy:0.2485, Validation Loss:1.6107, Validation Accuracy:0.2348\n",
    "Epoch #274: Loss:1.5880, Accuracy:0.2509, Validation Loss:1.6106, Validation Accuracy:0.2414\n",
    "Epoch #275: Loss:1.5875, Accuracy:0.2493, Validation Loss:1.6127, Validation Accuracy:0.2282\n",
    "Epoch #276: Loss:1.5885, Accuracy:0.2427, Validation Loss:1.6099, Validation Accuracy:0.2397\n",
    "Epoch #277: Loss:1.5883, Accuracy:0.2460, Validation Loss:1.6123, Validation Accuracy:0.2463\n",
    "Epoch #278: Loss:1.5881, Accuracy:0.2448, Validation Loss:1.6112, Validation Accuracy:0.2512\n",
    "Epoch #279: Loss:1.5876, Accuracy:0.2468, Validation Loss:1.6123, Validation Accuracy:0.2447\n",
    "Epoch #280: Loss:1.5885, Accuracy:0.2456, Validation Loss:1.6127, Validation Accuracy:0.2496\n",
    "Epoch #281: Loss:1.5878, Accuracy:0.2435, Validation Loss:1.6149, Validation Accuracy:0.2315\n",
    "Epoch #282: Loss:1.5881, Accuracy:0.2439, Validation Loss:1.6121, Validation Accuracy:0.2463\n",
    "Epoch #283: Loss:1.5885, Accuracy:0.2402, Validation Loss:1.6107, Validation Accuracy:0.2282\n",
    "Epoch #284: Loss:1.5872, Accuracy:0.2460, Validation Loss:1.6117, Validation Accuracy:0.2430\n",
    "Epoch #285: Loss:1.5872, Accuracy:0.2493, Validation Loss:1.6106, Validation Accuracy:0.2463\n",
    "Epoch #286: Loss:1.5875, Accuracy:0.2485, Validation Loss:1.6102, Validation Accuracy:0.2381\n",
    "Epoch #287: Loss:1.5879, Accuracy:0.2452, Validation Loss:1.6103, Validation Accuracy:0.2332\n",
    "Epoch #288: Loss:1.5868, Accuracy:0.2497, Validation Loss:1.6097, Validation Accuracy:0.2479\n",
    "Epoch #289: Loss:1.5871, Accuracy:0.2509, Validation Loss:1.6104, Validation Accuracy:0.2414\n",
    "Epoch #290: Loss:1.5865, Accuracy:0.2497, Validation Loss:1.6097, Validation Accuracy:0.2479\n",
    "Epoch #291: Loss:1.5868, Accuracy:0.2472, Validation Loss:1.6100, Validation Accuracy:0.2479\n",
    "Epoch #292: Loss:1.5861, Accuracy:0.2435, Validation Loss:1.6127, Validation Accuracy:0.2365\n",
    "Epoch #293: Loss:1.5877, Accuracy:0.2452, Validation Loss:1.6125, Validation Accuracy:0.2397\n",
    "Epoch #294: Loss:1.5880, Accuracy:0.2402, Validation Loss:1.6122, Validation Accuracy:0.2348\n",
    "Epoch #295: Loss:1.5863, Accuracy:0.2464, Validation Loss:1.6083, Validation Accuracy:0.2381\n",
    "Epoch #296: Loss:1.5871, Accuracy:0.2472, Validation Loss:1.6067, Validation Accuracy:0.2397\n",
    "Epoch #297: Loss:1.5874, Accuracy:0.2513, Validation Loss:1.6087, Validation Accuracy:0.2397\n",
    "Epoch #298: Loss:1.5877, Accuracy:0.2452, Validation Loss:1.6167, Validation Accuracy:0.2365\n",
    "Epoch #299: Loss:1.5967, Accuracy:0.2333, Validation Loss:1.6147, Validation Accuracy:0.2348\n",
    "Epoch #300: Loss:1.5940, Accuracy:0.2283, Validation Loss:1.6125, Validation Accuracy:0.2348\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61253262, Accuracy:0.2348\n",
    "Labels: ['04', '02', '01', '05', '03']\n",
    "Confusion Matrix:\n",
    "      04  02  01  05  03\n",
    "t:04   1  13  21  73   4\n",
    "t:02   2   9  21  79   3\n",
    "t:01   1   6  30  84   5\n",
    "t:05   3   9  27  97   6\n",
    "t:03   2   7  19  81   6\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.11      0.01      0.02       112\n",
    "          02       0.20      0.08      0.11       114\n",
    "          01       0.25      0.24      0.25       126\n",
    "          05       0.23      0.68      0.35       142\n",
    "          03       0.25      0.05      0.09       115\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.21      0.21      0.16       609\n",
    "weighted avg       0.21      0.23      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 18:15:23 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 59 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6098769057560436, 1.6083080541519892, 1.6076734248053264, 1.6072262229982073, 1.606805468231978, 1.6065803361056474, 1.6064895387549314, 1.6064784201886657, 1.6065169946704982, 1.6060241090840306, 1.605902100627254, 1.6058476687652137, 1.6058394497838513, 1.6056348611960074, 1.6056023935966304, 1.6054551192300857, 1.6054541352151455, 1.6053672294898573, 1.6052635365910521, 1.6053197965246115, 1.6052642678979583, 1.6056769869010437, 1.6056146788088168, 1.6055984594943293, 1.6053944111653344, 1.605155801733922, 1.605442342891286, 1.6061029821781103, 1.6058430244965702, 1.60583325403273, 1.6054932521090328, 1.6048206666420246, 1.6051762082502368, 1.60541836420695, 1.6051100970097558, 1.604440031380489, 1.6053247594676778, 1.6059823785900873, 1.6060117072072522, 1.6054642588046972, 1.6059995793748176, 1.6057332693453885, 1.605878316318656, 1.6057408979569359, 1.6055678722306426, 1.605097243742794, 1.6057388668968564, 1.6061921599267543, 1.6049531635588221, 1.6050998097765818, 1.605369362337836, 1.6061199719486956, 1.6032886794830974, 1.605657389794273, 1.6071061825713109, 1.6053004861856721, 1.6046841919715769, 1.6063033569426763, 1.607951180688266, 1.6056355420004558, 1.6054748676484834, 1.6042673439032142, 1.6049994625677224, 1.6060715631898401, 1.6060836859328798, 1.606749405610346, 1.606787324538959, 1.6068251234752986, 1.6072012383753835, 1.6068830822880436, 1.6070537541691696, 1.6078285802957069, 1.6073872457779883, 1.6076696504317285, 1.6072210564793428, 1.607471612482431, 1.6075798308320821, 1.6076800637253008, 1.607978840962615, 1.607828998996315, 1.6084015206946136, 1.609145545998622, 1.6083852056603518, 1.6092056954044036, 1.6101073625639741, 1.6092613201423231, 1.6097110417871836, 1.6094944103206517, 1.6080659210975534, 1.6105497521524164, 1.6088297333819135, 1.6093357147645873, 1.608470268045936, 1.6085968456049076, 1.6089283695753376, 1.6105676249330267, 1.6109027151990993, 1.6114651853423596, 1.6111444763362115, 1.6130015531001225, 1.611475130803088, 1.6123077383965303, 1.6134865587372302, 1.6137820339359477, 1.6127244594257648, 1.6137820540977816, 1.6131878233895514, 1.6134445774927124, 1.6139628620962008, 1.6128424208348215, 1.6133034190129372, 1.6142086471830095, 1.6140458452681994, 1.6099967016962362, 1.611396952215674, 1.6132132793686464, 1.6151142778067753, 1.6144019324204018, 1.6161807687411756, 1.6187817517955512, 1.6152241220223689, 1.6118994657629229, 1.6142655131460606, 1.6183600954234307, 1.6180715609849576, 1.613942548754963, 1.6139353824953728, 1.6125816838886156, 1.61088724598313, 1.6027110012489783, 1.6342690180870896, 1.6235120364989357, 1.623916599355112, 1.6065826864273873, 1.6062370687478478, 1.6059825927361675, 1.6043015182116154, 1.605032342799583, 1.604930156361685, 1.6040768020454494, 1.6048372193118818, 1.6063266512991368, 1.6077255946270546, 1.6070521312394166, 1.6060662465338245, 1.6072970683547272, 1.6078223152505158, 1.6086819561439978, 1.6100943088531494, 1.6128339773328433, 1.617952292775873, 1.6135088060485514, 1.6128554794392953, 1.6077710951881847, 1.6099227336044186, 1.605271972263192, 1.6043887398708825, 1.6033289585207484, 1.6040553013092191, 1.692574493990743, 1.6394969602719511, 1.6148627792868904, 1.6159745431298693, 1.6072345682357136, 1.6059089787487912, 1.6048345260432202, 1.605135641074533, 1.6048677372814986, 1.6047621376212986, 1.6044316513002017, 1.6045617342778222, 1.6047644716961238, 1.604796441317779, 1.6045251567962722, 1.6049213916405864, 1.6057159702961863, 1.6055140522704727, 1.6053269835332735, 1.605268973826579, 1.605359976514807, 1.60596223867977, 1.605744134420636, 1.605443986178619, 1.6063547042398814, 1.6060858857259765, 1.605671002359813, 1.6060111461993314, 1.606103611697117, 1.605773145342108, 1.6057013173408696, 1.6062655396062164, 1.606124860899789, 1.6058875230341318, 1.6062520763948438, 1.606303808137114, 1.6071058008666892, 1.606997081603127, 1.6060802218166284, 1.607222791767277, 1.6074666190029951, 1.607306416398786, 1.6084560214592318, 1.6070568483255572, 1.6066185419978376, 1.6076897400353343, 1.6077825306671594, 1.6084098397021616, 1.608232153264564, 1.607292520393096, 1.6077821000260477, 1.6081616434166193, 1.6089664030153372, 1.6099425102102345, 1.6141302875305827, 1.6097184398099902, 1.6099386966874447, 1.6098630974445436, 1.6073615161460413, 1.6081637435750225, 1.6095166413850581, 1.6096857868391892, 1.6090459289221928, 1.6081608618030017, 1.6091719983246526, 1.6113124856807914, 1.6098555704251494, 1.6086371682939076, 1.6079750019928505, 1.6091805882446089, 1.6101140844802355, 1.6109068483750417, 1.6082018457218539, 1.6074014986285632, 1.6096832386182838, 1.6091286752415799, 1.6092676041748724, 1.6098539854701126, 1.609680473510855, 1.6094038365118217, 1.608111570817105, 1.608231173751781, 1.6093237448991422, 1.6122938942635197, 1.6099099586358407, 1.6104939338217423, 1.6115609593383589, 1.6122074638094221, 1.613138979291681, 1.6138413635576496, 1.6129945861099193, 1.6131088185584408, 1.611647961762151, 1.6126773091176851, 1.611642217988451, 1.6109170698375732, 1.6115981892411932, 1.610922532715821, 1.6137001761074723, 1.6115749990215833, 1.6129948372519858, 1.610167939870424, 1.6094549461734315, 1.6108975502462026, 1.6091271419634765, 1.6110288082868203, 1.610862452995601, 1.611236699109007, 1.6121895135134117, 1.6128407746112992, 1.611948978724738, 1.6119716008895724, 1.609152181395169, 1.6107093097736878, 1.610629624138129, 1.612746975496289, 1.6099062553180263, 1.6122984764806938, 1.6112261229547962, 1.6123424226231566, 1.6127337595120634, 1.6149427092134072, 1.6121123187451918, 1.6107401033536162, 1.61169080585486, 1.6105903408601758, 1.6101726959100107, 1.6103235447935282, 1.6096957622490493, 1.610392320136523, 1.6096675284390378, 1.6099533574726, 1.6126744005284677, 1.612543011338057, 1.6122291389553027, 1.6083363325920794, 1.6066507480806123, 1.6086645838858067, 1.6166559722035976, 1.614722344284183, 1.6125324682649134], 'val_acc': [0.21839080427961396, 0.23481116354563358, 0.23481116374137953, 0.2331691276165848, 0.22988505517124935, 0.23481116374137953, 0.23152709149179004, 0.2282430213220014, 0.22988505744679613, 0.23152709357159088, 0.23481116374137953, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2397372719200178, 0.2430213441696073, 0.24137930804481256, 0.2364532019459751, 0.2298850552691223, 0.23809523807076985, 0.24466338039227503, 0.23809523579522307, 0.23809523807076985, 0.24794745254399153, 0.24466338247207586, 0.23809523807076985, 0.23316912742083884, 0.2380952378750239, 0.24302134634728112, 0.2397372719200178, 0.2331691275187118, 0.22824301914432757, 0.22824302102838243, 0.24794745452591938, 0.2413793101246134, 0.23645319967042833, 0.23481116364350654, 0.23481116364350654, 0.2430213442674803, 0.23809523589309606, 0.2430213441696073, 0.2430213441696073, 0.23316912742083884, 0.2315270912960441, 0.23645319967042833, 0.23645320184810212, 0.22824301914432757, 0.2282430213220014, 0.198686370977823, 0.24794745254399153, 0.2282430213220014, 0.23809523807076985, 0.24302134644515408, 0.2479474527397375, 0.2479474527397375, 0.2216748745472756, 0.23316912969638562, 0.2479474548195383, 0.238095236088842, 0.2298850552691223, 0.22988505517124935, 0.23481116354563358, 0.23316912969638562, 0.2298850552691223, 0.2315270912960441, 0.2397372740976916, 0.22824301904645458, 0.2282430212241284, 0.22331691294761713, 0.23973727399981865, 0.2315270912960441, 0.24137930804481256, 0.2430213441696073, 0.24137930814268554, 0.23809523579522307, 0.23809523807076985, 0.24137930804481256, 0.2397372741955646, 0.24302134605366216, 0.23316912959851263, 0.23316912742083884, 0.23316912959851263, 0.23481116354563358, 0.2315270934737179, 0.23152709357159088, 0.23316912969638562, 0.22331691086781633, 0.2413793100267404, 0.2315270912960441, 0.24302134436535328, 0.2397372719200178, 0.2397372720178908, 0.22495894887666593, 0.23481116354563358, 0.22988505517124935, 0.23316912940276668, 0.23645320175022916, 0.22988505725105016, 0.22495894877879294, 0.2298850570553042, 0.2282430212241284, 0.23316912969638562, 0.23481116552756143, 0.22988505725105016, 0.22660098490358768, 0.2282430212241284, 0.22988505734892314, 0.2282430211262554, 0.22495894868091998, 0.22495894887666593, 0.23809523807076985, 0.2282430211262554, 0.2233169126539982, 0.22495894887666593, 0.23316912950063964, 0.2364532019459751, 0.21674876835056517, 0.2216748768228224, 0.22495894907241187, 0.2315270912960441, 0.22660098509933366, 0.22495894907241187, 0.23316912959851263, 0.22003284069802764, 0.2298850552691223, 0.23809523797289686, 0.2495894907485871, 0.2183908045732329, 0.2068965516996697, 0.22988505517124935, 0.2315270934737179, 0.22495894887666593, 0.19376026250556577, 0.23645319976830131, 0.24958948866878627, 0.25287356091837576, 0.24302134634728112, 0.24137930804481256, 0.24302134624940813, 0.24958949055284116, 0.24466338247207586, 0.24630541651706978, 0.2463054164191968, 0.24794745264186452, 0.24958948876665926, 0.24466338029440204, 0.2430213442674803, 0.21182265779850715, 0.20853858762871846, 0.23809523589309606, 0.2266009831174058, 0.2413793082405585, 0.2266009832152788, 0.22495894699261107, 0.23481116364350654, 0.2249589489745389, 0.19211822628289804, 0.2036124788628423, 0.23316912969638562, 0.23809523589309606, 0.23481116374137953, 0.2331691276165848, 0.2331691276165848, 0.2282430213220014, 0.23809523579522307, 0.23645319986617427, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2331691276165848, 0.2298850553669953, 0.23809523579522307, 0.2430213441696073, 0.2397372719200178, 0.23809523579522307, 0.23809523579522307, 0.2364532019459751, 0.2364532019459751, 0.22824301914432757, 0.24466338256994882, 0.24794745472166535, 0.2397372719200178, 0.23152709139391706, 0.23645320184810212, 0.22824301904645458, 0.2315270934737179, 0.23481116582118036, 0.23481116582118036, 0.23152709357159088, 0.23152709357159088, 0.22988505744679613, 0.23481116582118036, 0.23481116582118036, 0.23973727390194566, 0.23481116582118036, 0.23316912959851263, 0.24137931022248635, 0.2397372719200178, 0.24466338237420288, 0.2430213441696073, 0.2380952378750239, 0.23316912940276668, 0.23645320165235617, 0.22988505734892314, 0.23973727399981865, 0.23152709357159088, 0.23809523777715091, 0.24137930814268554, 0.23481116582118036, 0.2430213459557892, 0.22003283861822684, 0.23973727370619968, 0.23481116354563358, 0.23316912742083884, 0.23481116582118036, 0.23152709318009895, 0.2315270912960441, 0.2282430207347635, 0.23316912969638562, 0.23152709318009895, 0.24302134615153514, 0.22660098292165984, 0.2430213459557892, 0.23481116552756143, 0.2397372741955646, 0.2397372719200178, 0.2331691293048937, 0.2331691293048937, 0.2348111656254344, 0.23481116572330737, 0.23645320184810212, 0.23481116572330737, 0.23481116572330737, 0.23481116572330737, 0.23481116572330737, 0.23316912959851263, 0.2348111656254344, 0.23481116572330737, 0.23645320175022916, 0.22660098509933366, 0.2397372740976916, 0.22988505734892314, 0.23152709327797194, 0.23645320184810212, 0.23645320165235617, 0.2282430212241284, 0.23316912940276668, 0.23316912950063964, 0.2315270934737179, 0.23645319967042833, 0.2413793101246134, 0.23973727399981865, 0.22988505734892314, 0.2413793101246134, 0.22988505734892314, 0.23973727399981865, 0.22660098500146067, 0.23645320184810212, 0.24302134634728112, 0.24137931022248635, 0.24466338247207586, 0.23316912969638562, 0.24137931022248635, 0.2348111656254344, 0.2331691293048937, 0.23152709327797194, 0.23645320175022916, 0.23316912950063964, 0.2397372740976916, 0.23481116572330737, 0.2413793101246134, 0.22824302093050947, 0.2397372719200178, 0.24630541869474357, 0.25123152697125484, 0.24466338247207586, 0.2495894908464601, 0.23152709357159088, 0.2463054185968706, 0.2282430212241284, 0.24302134634728112, 0.2463054185968706, 0.23809523797289686, 0.23316912950063964, 0.24794745462379236, 0.2413793101246134, 0.24794745452591938, 0.24794745452591938, 0.2364532019459751, 0.23973727399981865, 0.23481116582118036, 0.2380952378750239, 0.23973727399981865, 0.23973727399981865, 0.23645320175022916, 0.23481116552756143, 0.23481116572330737], 'loss': [1.6106448617069629, 1.6080361058824606, 1.6063280611801931, 1.6050687085431703, 1.6048639081586802, 1.6046545050716987, 1.6044799533468008, 1.6043189782136764, 1.6042541966056432, 1.6036922857746696, 1.603299339840789, 1.6030467430424151, 1.602873815060641, 1.6023474384382275, 1.6022031592637362, 1.6018791367141128, 1.6017166958207711, 1.601374301773322, 1.6010674669022922, 1.6009047777500975, 1.6005505942221294, 1.6010456509658688, 1.6005871181860107, 1.600408815456367, 1.600403087535678, 1.6002843509219755, 1.6003779620850112, 1.600697928385568, 1.600585920071455, 1.6004173668992592, 1.6001346842709019, 1.5996982028596944, 1.5997123909681974, 1.5996511822118897, 1.5995537328524267, 1.5990335696042195, 1.5991446787816543, 1.599645016766182, 1.5989450041028752, 1.5987368863710878, 1.5988775042293009, 1.5982637119978604, 1.5980413361006938, 1.5978013338249566, 1.5975677618990198, 1.5974937466380532, 1.5973780355903893, 1.5971365067503536, 1.5971335807620133, 1.5981984501746647, 1.5983782152865211, 1.6028297672036738, 1.6019032724094586, 1.6009743028597665, 1.6019915947434349, 1.600721496282417, 1.5999470317633, 1.6001389204354257, 1.604398323965758, 1.6024598331666824, 1.6010495028701406, 1.599831406683403, 1.5983186452050964, 1.5988165460817623, 1.5985524530528263, 1.5989553673800991, 1.5983005418424978, 1.5977189936432261, 1.5978387036607495, 1.5969608700985292, 1.5968142714098983, 1.596861867003862, 1.596785120895511, 1.5965274963046001, 1.5959494129588228, 1.5953704002211961, 1.595499248181525, 1.5956683925534665, 1.5950394519300677, 1.5947266717711024, 1.5945501211977102, 1.5941762472325036, 1.5937242596056427, 1.5935120811207828, 1.5930675290203682, 1.5939238035213776, 1.5933978551222314, 1.5932855730428832, 1.5932446078842915, 1.5931082410244481, 1.5942517716781803, 1.5935371966822192, 1.591806050492508, 1.5924790112144893, 1.5934104758366423, 1.5903213535246175, 1.5918732285744355, 1.589196671695435, 1.5902023379073251, 1.5889860750958171, 1.5874158116581503, 1.5885273017922465, 1.589056229787196, 1.5865612143608578, 1.5876755529360604, 1.5856261673894017, 1.5846007467540137, 1.5859625752212085, 1.5873001667508355, 1.584687425076839, 1.5854836600517102, 1.5849424606475986, 1.5843372339095914, 1.583004583666212, 1.582924892574365, 1.5833124847137952, 1.5826313676775359, 1.5833013022459999, 1.5829649322576347, 1.5824637339590022, 1.5818046513034578, 1.581428500122603, 1.5819947033202624, 1.5828811746847948, 1.5827969858533792, 1.581985685663791, 1.5822060389195625, 1.5813588033711397, 1.5814703820422444, 1.5863256121073415, 1.6031081729845835, 1.617032172939371, 1.6072621633874318, 1.6025986415404803, 1.5991688027763757, 1.597117867606866, 1.5978274547588653, 1.5946418238861115, 1.5957328470825414, 1.5941167337448934, 1.5939028070692653, 1.5929981797138033, 1.5928446556753202, 1.5915530923455647, 1.5917002464956327, 1.5903134992724817, 1.5902445331980806, 1.5888536980509513, 1.5881994103504158, 1.5866269793108994, 1.59027636653344, 1.5991900801903414, 1.597942543176655, 1.5999975523175156, 1.6006171938819806, 1.5967320757480128, 1.5955499168783733, 1.594725969586774, 1.5939541132298338, 1.595470616019482, 1.635593394332353, 1.6244095667676515, 1.610191877764598, 1.6109964030968824, 1.6039900269106917, 1.601686701245866, 1.6017583622335163, 1.6019302696662763, 1.601537565822719, 1.600966983066692, 1.6003183833872268, 1.6001585245622012, 1.5998501595040855, 1.5996599153816333, 1.5992394728827035, 1.59910962087173, 1.5987453147371202, 1.5983538273178821, 1.598212165411493, 1.5979175864303872, 1.5975511442709263, 1.5978111229393273, 1.597727678884471, 1.5975122468917031, 1.5970175964876367, 1.596557083169048, 1.596717055426486, 1.5961320767902005, 1.5961249803370763, 1.5956173175659023, 1.5956715985245284, 1.5952675872270086, 1.5949478932719456, 1.5947446115697435, 1.594463895429576, 1.5940291126650707, 1.5941916042284798, 1.5938426721756953, 1.5940091739200224, 1.5949298139470314, 1.5941585982849467, 1.594213169800917, 1.5941104792961105, 1.5930029810331685, 1.5929525029732705, 1.5930229710847201, 1.5926305393908302, 1.5929126762021983, 1.5933753644171682, 1.5919422297507095, 1.5926804856842793, 1.5940129823508449, 1.5927925048177982, 1.5927345387010359, 1.5976730224777786, 1.595657698329714, 1.592168844211273, 1.5929538318508705, 1.5938196274778926, 1.5936467072802158, 1.5928964328961694, 1.5920598991597703, 1.5924810435982455, 1.5953729152189877, 1.5937585730327473, 1.5928066066158382, 1.592454334450943, 1.5962229696876948, 1.593616601329075, 1.5907882204780344, 1.5912163593441064, 1.5907450147233215, 1.5906708545998136, 1.5903023443672446, 1.5907894007234358, 1.5901538888042224, 1.5900685313301164, 1.5896845290303474, 1.5895949709831567, 1.5899638467745614, 1.5922718910710767, 1.58998575186093, 1.5898781685858536, 1.5898328749306148, 1.5907809407314482, 1.5897274474098941, 1.5892218982414543, 1.5892969948065598, 1.5906159840080527, 1.5895564002912392, 1.5891476814262187, 1.5884078869829432, 1.5908744584853154, 1.589595193001279, 1.5890805550914036, 1.590232877662784, 1.5899471160077951, 1.5891319398272943, 1.5901364977110093, 1.5885586466387802, 1.5887436408037032, 1.5887518557679727, 1.5903493828352473, 1.5876757683450435, 1.588200745592372, 1.5883840490905166, 1.5882706178042434, 1.5882118350916086, 1.5893923939620689, 1.5895296548671056, 1.5892284516681146, 1.589462771308006, 1.589574183967324, 1.5880060809348888, 1.587459238549767, 1.5885133301208152, 1.5883264257677772, 1.5880909030197583, 1.587625663138513, 1.5885328091635107, 1.5878362777051984, 1.588134190238232, 1.5884596625392686, 1.5871818200273926, 1.5872413789467155, 1.5874840130306611, 1.587938501898513, 1.5868078757605268, 1.5871061877303545, 1.5864561556790644, 1.5868212868300797, 1.586081255632749, 1.5877255886242374, 1.5880199828431836, 1.5863173386889071, 1.5871110977822995, 1.5874387155568086, 1.5877454281342838, 1.5966844264976299, 1.5940427769870484], 'acc': [0.21314168265223257, 0.23819301755031766, 0.23490759819072865, 0.23285420929871545, 0.23244353246395102, 0.2324435322864834, 0.2340862415838046, 0.23326488654349128, 0.23367556473068143, 0.23531827676957148, 0.23080082062088733, 0.23162217607121202, 0.23285420892542147, 0.2328542091028891, 0.2328542097270856, 0.23285420890706277, 0.2328542083195837, 0.2332648877368081, 0.2381930193311135, 0.23696098608525137, 0.23942505138365885, 0.23285421125697894, 0.23244353207229834, 0.23983572700674774, 0.2402464058364931, 0.24065708365038924, 0.2377823405380856, 0.2443531818213649, 0.2381930173544913, 0.24229979373101582, 0.24065708445205336, 0.2447638602227401, 0.24435318162553854, 0.24024640423316485, 0.23819301874363447, 0.2398357284142496, 0.23901437378394774, 0.24312114916298178, 0.23983572843260834, 0.2431211507479513, 0.2398357294300988, 0.24147843986566062, 0.2414784414322714, 0.2447638617893509, 0.23901437513637347, 0.238603696753357, 0.23819301794197034, 0.24065708365038924, 0.24435318321050806, 0.24435318340633438, 0.2365503082713552, 0.21930184695999724, 0.22381930111370046, 0.23778234034225926, 0.23572895301685687, 0.23983572980339277, 0.24065708325873655, 0.23901437300064235, 0.2324435322864834, 0.2258726896324197, 0.24147844007984567, 0.24147844047149838, 0.2369609846961082, 0.2340862417612722, 0.23572895184189877, 0.23696098348443267, 0.24229979392684217, 0.23942505120619123, 0.2381930175686764, 0.24106776244341716, 0.24517453842828896, 0.2435318267810516, 0.2410677630308962, 0.23901437356976268, 0.2447638595985436, 0.24476386122023056, 0.24517453746751594, 0.24106776087680637, 0.2418891168962514, 0.2422997949059739, 0.2431211499646459, 0.24517453921159435, 0.24147844027567203, 0.24435318321050806, 0.2431211487713291, 0.24024640720727752, 0.2406570850395324, 0.2439425046133065, 0.2476386043508929, 0.24681724911475328, 0.24106776126845905, 0.2431211477554799, 0.24476386082857787, 0.24435318456293376, 0.24681724911475328, 0.24147843869070254, 0.2451745386241153, 0.24640656991171397, 0.24476386082857787, 0.24476386041856643, 0.2501026716075639, 0.25051334589658575, 0.24188911693296883, 0.2517453787507952, 0.2505133472673702, 0.24640657051755172, 0.25215605756218185, 0.25215605773964944, 0.24106776226594953, 0.25708418837318187, 0.25010266947183274, 0.23983572782677057, 0.25215605754382314, 0.2521560593062602, 0.24969199281453597, 0.2513347011327254, 0.252566734788599, 0.2496919894487706, 0.2505133480506756, 0.24681724811726283, 0.2537987691910605, 0.24517453702078706, 0.24804928079400465, 0.24928131362985537, 0.2492813144315195, 0.2570841893706723, 0.25462012327060074, 0.2550307998911801, 0.25379876997436585, 0.24558521526305338, 0.23901437399813283, 0.21560574974979463, 0.2365503074880498, 0.23737166329330978, 0.24476385822775917, 0.2340862425445776, 0.23326488654349128, 0.24065708521700002, 0.24517453803663625, 0.24969199124792518, 0.252566733166912, 0.2537987682119287, 0.2509240254729191, 0.2554414787025667, 0.2472279245236571, 0.25297741317161543, 0.25297741160500464, 0.25790554537175864, 0.25749486599125165, 0.25544147927168703, 0.247227925931159, 0.24106776167847047, 0.23039014360865528, 0.22669404465437426, 0.2308008212267251, 0.23942505218532295, 0.23244353089734024, 0.24065708523535875, 0.23860369376088558, 0.2439425053966119, 0.21642710459428158, 0.1946611914240604, 0.23490759598156266, 0.23367556572817189, 0.23449692018100612, 0.2349075964282915, 0.23367556610146586, 0.23901437358812141, 0.24065708365038924, 0.2361396302799914, 0.23613962945996858, 0.23613963004744762, 0.23572895323104193, 0.23613963125912316, 0.23696098450028186, 0.2439425043991214, 0.24558521626054383, 0.24312114976881954, 0.24229979568927928, 0.24353182893514144, 0.24353182799272713, 0.24271047175909705, 0.24271047311152277, 0.24271047293405512, 0.24271047134908563, 0.2447638598310874, 0.2443531837979871, 0.24312114955463449, 0.24106776187429682, 0.23942505120619123, 0.23942505020870075, 0.24558521665219654, 0.24394250459494776, 0.24312115014211352, 0.24312115055212494, 0.2447638602227401, 0.24476386082857787, 0.24229979551181166, 0.24435318240884393, 0.2431211499462872, 0.23983572921591373, 0.24106776107263272, 0.24599589407444, 0.24517453864247402, 0.23696098606689264, 0.2418891180895682, 0.24763860257009707, 0.24229979549345296, 0.23983572921591373, 0.2443531837979871, 0.24353182736853066, 0.24106776167847047, 0.24147844027567203, 0.2451745384466477, 0.23408623996211764, 0.23983572921591373, 0.24188911630877236, 0.24188911787538314, 0.24722792691029072, 0.24065708325873655, 0.24065708365038924, 0.2447638606143928, 0.24106776267596094, 0.23696098608525137, 0.2447638610244042, 0.24435318340633438, 0.24887063679509094, 0.23942505275444328, 0.24312115016047225, 0.24312114916298178, 0.23942505120619123, 0.24517453805499498, 0.24517453825082133, 0.24763860452836053, 0.24681724948804726, 0.24763860257009707, 0.24804928018816688, 0.2455852148897594, 0.24763860413670785, 0.2550308000870064, 0.2468172500755263, 0.24928131402150805, 0.24599589407444, 0.24681724909639458, 0.24722792653699674, 0.24640657147832476, 0.24804928196896273, 0.24804928292973574, 0.2431211491813405, 0.24887063562013284, 0.2451745384466477, 0.2480492791723177, 0.24271047115325928, 0.24271047254240244, 0.2439425055740795, 0.24188911509709682, 0.2554414798591661, 0.24928131501899853, 0.24147843829904983, 0.24517453864247402, 0.2431211499462872, 0.2451745374491572, 0.24394250479077412, 0.2439425043991214, 0.24681724948804726, 0.24394250655321126, 0.24435318201719122, 0.23983572821842328, 0.24517453803663625, 0.2451745374491572, 0.23901437456725314, 0.24517453785916862, 0.24845995997868525, 0.2509240250996251, 0.24928131441316076, 0.24271047033323645, 0.24599589248947049, 0.2447638606143928, 0.24681724792143647, 0.24558521526305338, 0.24353182736853066, 0.24394250616155855, 0.2402464066381572, 0.2459958928811232, 0.24928131382568172, 0.24845995996032652, 0.24517453725333085, 0.24969199379366772, 0.2509240232821118, 0.24969199124792518, 0.24722792828107518, 0.2435318283476624, 0.2451745397990734, 0.2402464068156248, 0.24640657304493555, 0.2472279255395063, 0.25133470330517393, 0.24517453705750453, 0.23326488632930623, 0.2283367566932643]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
