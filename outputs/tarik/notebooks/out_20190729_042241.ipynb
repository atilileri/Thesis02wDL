{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf80.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 04:22:41 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '1Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '05', '04', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002710093BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000027129D46EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6088, Accuracy:0.2082, Validation Loss:1.6074, Validation Accuracy:0.2282\n",
    "Epoch #2: Loss:1.6067, Accuracy:0.2222, Validation Loss:1.6060, Validation Accuracy:0.2348\n",
    "Epoch #3: Loss:1.6059, Accuracy:0.2316, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6059, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6053, Accuracy:0.2329, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6052, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6040, Accuracy:0.2329, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6037, Accuracy:0.2329, Validation Loss:1.6047, Validation Accuracy:0.2365\n",
    "Epoch #10: Loss:1.6034, Accuracy:0.2349, Validation Loss:1.6044, Validation Accuracy:0.2365\n",
    "Epoch #11: Loss:1.6034, Accuracy:0.2324, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #12: Loss:1.6033, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6029, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2397\n",
    "Epoch #14: Loss:1.6027, Accuracy:0.2357, Validation Loss:1.6046, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6023, Accuracy:0.2349, Validation Loss:1.6044, Validation Accuracy:0.2397\n",
    "Epoch #16: Loss:1.6021, Accuracy:0.2370, Validation Loss:1.6042, Validation Accuracy:0.2397\n",
    "Epoch #17: Loss:1.6017, Accuracy:0.2394, Validation Loss:1.6042, Validation Accuracy:0.2430\n",
    "Epoch #18: Loss:1.6016, Accuracy:0.2423, Validation Loss:1.6043, Validation Accuracy:0.2463\n",
    "Epoch #19: Loss:1.6015, Accuracy:0.2419, Validation Loss:1.6042, Validation Accuracy:0.2463\n",
    "Epoch #20: Loss:1.6014, Accuracy:0.2423, Validation Loss:1.6036, Validation Accuracy:0.2463\n",
    "Epoch #21: Loss:1.6010, Accuracy:0.2415, Validation Loss:1.6036, Validation Accuracy:0.2430\n",
    "Epoch #22: Loss:1.6012, Accuracy:0.2390, Validation Loss:1.6036, Validation Accuracy:0.2430\n",
    "Epoch #23: Loss:1.6009, Accuracy:0.2402, Validation Loss:1.6036, Validation Accuracy:0.2430\n",
    "Epoch #24: Loss:1.6011, Accuracy:0.2382, Validation Loss:1.6038, Validation Accuracy:0.2414\n",
    "Epoch #25: Loss:1.6009, Accuracy:0.2382, Validation Loss:1.6038, Validation Accuracy:0.2430\n",
    "Epoch #26: Loss:1.6006, Accuracy:0.2353, Validation Loss:1.6039, Validation Accuracy:0.2430\n",
    "Epoch #27: Loss:1.6005, Accuracy:0.2402, Validation Loss:1.6041, Validation Accuracy:0.2430\n",
    "Epoch #28: Loss:1.6007, Accuracy:0.2386, Validation Loss:1.6038, Validation Accuracy:0.2447\n",
    "Epoch #29: Loss:1.6009, Accuracy:0.2402, Validation Loss:1.6041, Validation Accuracy:0.2463\n",
    "Epoch #30: Loss:1.6006, Accuracy:0.2411, Validation Loss:1.6042, Validation Accuracy:0.2447\n",
    "Epoch #31: Loss:1.6007, Accuracy:0.2398, Validation Loss:1.6038, Validation Accuracy:0.2479\n",
    "Epoch #32: Loss:1.6002, Accuracy:0.2345, Validation Loss:1.6036, Validation Accuracy:0.2463\n",
    "Epoch #33: Loss:1.6003, Accuracy:0.2386, Validation Loss:1.6038, Validation Accuracy:0.2447\n",
    "Epoch #34: Loss:1.6004, Accuracy:0.2398, Validation Loss:1.6038, Validation Accuracy:0.2447\n",
    "Epoch #35: Loss:1.5998, Accuracy:0.2402, Validation Loss:1.6034, Validation Accuracy:0.2447\n",
    "Epoch #36: Loss:1.5997, Accuracy:0.2398, Validation Loss:1.6031, Validation Accuracy:0.2479\n",
    "Epoch #37: Loss:1.5995, Accuracy:0.2394, Validation Loss:1.6036, Validation Accuracy:0.2479\n",
    "Epoch #38: Loss:1.5995, Accuracy:0.2402, Validation Loss:1.6037, Validation Accuracy:0.2512\n",
    "Epoch #39: Loss:1.5994, Accuracy:0.2398, Validation Loss:1.6038, Validation Accuracy:0.2529\n",
    "Epoch #40: Loss:1.5991, Accuracy:0.2419, Validation Loss:1.6041, Validation Accuracy:0.2496\n",
    "Epoch #41: Loss:1.5990, Accuracy:0.2419, Validation Loss:1.6035, Validation Accuracy:0.2479\n",
    "Epoch #42: Loss:1.5992, Accuracy:0.2407, Validation Loss:1.6034, Validation Accuracy:0.2496\n",
    "Epoch #43: Loss:1.5991, Accuracy:0.2390, Validation Loss:1.6033, Validation Accuracy:0.2545\n",
    "Epoch #44: Loss:1.5991, Accuracy:0.2407, Validation Loss:1.6034, Validation Accuracy:0.2479\n",
    "Epoch #45: Loss:1.5993, Accuracy:0.2407, Validation Loss:1.6031, Validation Accuracy:0.2512\n",
    "Epoch #46: Loss:1.5990, Accuracy:0.2394, Validation Loss:1.6027, Validation Accuracy:0.2529\n",
    "Epoch #47: Loss:1.5987, Accuracy:0.2394, Validation Loss:1.6029, Validation Accuracy:0.2447\n",
    "Epoch #48: Loss:1.5989, Accuracy:0.2407, Validation Loss:1.6033, Validation Accuracy:0.2479\n",
    "Epoch #49: Loss:1.5989, Accuracy:0.2419, Validation Loss:1.6029, Validation Accuracy:0.2479\n",
    "Epoch #50: Loss:1.5990, Accuracy:0.2394, Validation Loss:1.6026, Validation Accuracy:0.2562\n",
    "Epoch #51: Loss:1.5990, Accuracy:0.2349, Validation Loss:1.6027, Validation Accuracy:0.2512\n",
    "Epoch #52: Loss:1.5986, Accuracy:0.2423, Validation Loss:1.6037, Validation Accuracy:0.2479\n",
    "Epoch #53: Loss:1.5989, Accuracy:0.2390, Validation Loss:1.6032, Validation Accuracy:0.2381\n",
    "Epoch #54: Loss:1.5982, Accuracy:0.2353, Validation Loss:1.6027, Validation Accuracy:0.2545\n",
    "Epoch #55: Loss:1.5983, Accuracy:0.2378, Validation Loss:1.6023, Validation Accuracy:0.2529\n",
    "Epoch #56: Loss:1.5981, Accuracy:0.2390, Validation Loss:1.6022, Validation Accuracy:0.2447\n",
    "Epoch #57: Loss:1.5982, Accuracy:0.2407, Validation Loss:1.6020, Validation Accuracy:0.2463\n",
    "Epoch #58: Loss:1.5981, Accuracy:0.2382, Validation Loss:1.6016, Validation Accuracy:0.2529\n",
    "Epoch #59: Loss:1.5982, Accuracy:0.2398, Validation Loss:1.6029, Validation Accuracy:0.2365\n",
    "Epoch #60: Loss:1.5995, Accuracy:0.2337, Validation Loss:1.6013, Validation Accuracy:0.2414\n",
    "Epoch #61: Loss:1.5989, Accuracy:0.2370, Validation Loss:1.6019, Validation Accuracy:0.2479\n",
    "Epoch #62: Loss:1.5988, Accuracy:0.2427, Validation Loss:1.6022, Validation Accuracy:0.2414\n",
    "Epoch #63: Loss:1.5983, Accuracy:0.2382, Validation Loss:1.6017, Validation Accuracy:0.2365\n",
    "Epoch #64: Loss:1.5984, Accuracy:0.2382, Validation Loss:1.6019, Validation Accuracy:0.2315\n",
    "Epoch #65: Loss:1.5975, Accuracy:0.2390, Validation Loss:1.6014, Validation Accuracy:0.2348\n",
    "Epoch #66: Loss:1.5978, Accuracy:0.2386, Validation Loss:1.6015, Validation Accuracy:0.2348\n",
    "Epoch #67: Loss:1.5978, Accuracy:0.2345, Validation Loss:1.6020, Validation Accuracy:0.2315\n",
    "Epoch #68: Loss:1.5974, Accuracy:0.2386, Validation Loss:1.6014, Validation Accuracy:0.2348\n",
    "Epoch #69: Loss:1.5972, Accuracy:0.2382, Validation Loss:1.6010, Validation Accuracy:0.2365\n",
    "Epoch #70: Loss:1.5970, Accuracy:0.2382, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #71: Loss:1.5966, Accuracy:0.2382, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #72: Loss:1.5967, Accuracy:0.2382, Validation Loss:1.6011, Validation Accuracy:0.2332\n",
    "Epoch #73: Loss:1.5968, Accuracy:0.2386, Validation Loss:1.6037, Validation Accuracy:0.2299\n",
    "Epoch #74: Loss:1.5975, Accuracy:0.2402, Validation Loss:1.6019, Validation Accuracy:0.2266\n",
    "Epoch #75: Loss:1.5966, Accuracy:0.2370, Validation Loss:1.6016, Validation Accuracy:0.2250\n",
    "Epoch #76: Loss:1.5961, Accuracy:0.2390, Validation Loss:1.6017, Validation Accuracy:0.2266\n",
    "Epoch #77: Loss:1.5962, Accuracy:0.2386, Validation Loss:1.6020, Validation Accuracy:0.2282\n",
    "Epoch #78: Loss:1.5961, Accuracy:0.2402, Validation Loss:1.6021, Validation Accuracy:0.2266\n",
    "Epoch #79: Loss:1.5958, Accuracy:0.2394, Validation Loss:1.6033, Validation Accuracy:0.2233\n",
    "Epoch #80: Loss:1.5961, Accuracy:0.2448, Validation Loss:1.6035, Validation Accuracy:0.2135\n",
    "Epoch #81: Loss:1.5960, Accuracy:0.2435, Validation Loss:1.6027, Validation Accuracy:0.2233\n",
    "Epoch #82: Loss:1.5957, Accuracy:0.2394, Validation Loss:1.6046, Validation Accuracy:0.2118\n",
    "Epoch #83: Loss:1.5954, Accuracy:0.2398, Validation Loss:1.6045, Validation Accuracy:0.2200\n",
    "Epoch #84: Loss:1.5954, Accuracy:0.2402, Validation Loss:1.6047, Validation Accuracy:0.2167\n",
    "Epoch #85: Loss:1.5957, Accuracy:0.2394, Validation Loss:1.6033, Validation Accuracy:0.2282\n",
    "Epoch #86: Loss:1.5972, Accuracy:0.2386, Validation Loss:1.6045, Validation Accuracy:0.2233\n",
    "Epoch #87: Loss:1.5976, Accuracy:0.2357, Validation Loss:1.6082, Validation Accuracy:0.2118\n",
    "Epoch #88: Loss:1.5989, Accuracy:0.2415, Validation Loss:1.6063, Validation Accuracy:0.2184\n",
    "Epoch #89: Loss:1.5969, Accuracy:0.2415, Validation Loss:1.6042, Validation Accuracy:0.2233\n",
    "Epoch #90: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.6050, Validation Accuracy:0.2151\n",
    "Epoch #91: Loss:1.5968, Accuracy:0.2394, Validation Loss:1.6055, Validation Accuracy:0.2135\n",
    "Epoch #92: Loss:1.5959, Accuracy:0.2444, Validation Loss:1.6056, Validation Accuracy:0.2102\n",
    "Epoch #93: Loss:1.5955, Accuracy:0.2431, Validation Loss:1.6053, Validation Accuracy:0.2151\n",
    "Epoch #94: Loss:1.5958, Accuracy:0.2411, Validation Loss:1.6053, Validation Accuracy:0.2135\n",
    "Epoch #95: Loss:1.5952, Accuracy:0.2411, Validation Loss:1.6058, Validation Accuracy:0.2217\n",
    "Epoch #96: Loss:1.5952, Accuracy:0.2407, Validation Loss:1.6055, Validation Accuracy:0.2118\n",
    "Epoch #97: Loss:1.5961, Accuracy:0.2402, Validation Loss:1.6060, Validation Accuracy:0.2118\n",
    "Epoch #98: Loss:1.5945, Accuracy:0.2407, Validation Loss:1.6065, Validation Accuracy:0.2233\n",
    "Epoch #99: Loss:1.5956, Accuracy:0.2427, Validation Loss:1.6049, Validation Accuracy:0.2102\n",
    "Epoch #100: Loss:1.5950, Accuracy:0.2452, Validation Loss:1.6049, Validation Accuracy:0.2069\n",
    "Epoch #101: Loss:1.5950, Accuracy:0.2378, Validation Loss:1.6051, Validation Accuracy:0.2151\n",
    "Epoch #102: Loss:1.5943, Accuracy:0.2439, Validation Loss:1.6044, Validation Accuracy:0.2135\n",
    "Epoch #103: Loss:1.5958, Accuracy:0.2431, Validation Loss:1.6045, Validation Accuracy:0.2217\n",
    "Epoch #104: Loss:1.5950, Accuracy:0.2456, Validation Loss:1.6060, Validation Accuracy:0.2217\n",
    "Epoch #105: Loss:1.5951, Accuracy:0.2386, Validation Loss:1.6051, Validation Accuracy:0.2118\n",
    "Epoch #106: Loss:1.5951, Accuracy:0.2398, Validation Loss:1.6050, Validation Accuracy:0.2135\n",
    "Epoch #107: Loss:1.5944, Accuracy:0.2452, Validation Loss:1.6053, Validation Accuracy:0.2151\n",
    "Epoch #108: Loss:1.5946, Accuracy:0.2456, Validation Loss:1.6046, Validation Accuracy:0.2200\n",
    "Epoch #109: Loss:1.5952, Accuracy:0.2423, Validation Loss:1.6048, Validation Accuracy:0.2151\n",
    "Epoch #110: Loss:1.5942, Accuracy:0.2444, Validation Loss:1.6059, Validation Accuracy:0.2184\n",
    "Epoch #111: Loss:1.5944, Accuracy:0.2439, Validation Loss:1.6055, Validation Accuracy:0.2151\n",
    "Epoch #112: Loss:1.5941, Accuracy:0.2407, Validation Loss:1.6056, Validation Accuracy:0.2085\n",
    "Epoch #113: Loss:1.5943, Accuracy:0.2431, Validation Loss:1.6054, Validation Accuracy:0.2184\n",
    "Epoch #114: Loss:1.5948, Accuracy:0.2423, Validation Loss:1.6054, Validation Accuracy:0.2167\n",
    "Epoch #115: Loss:1.5937, Accuracy:0.2501, Validation Loss:1.6053, Validation Accuracy:0.2135\n",
    "Epoch #116: Loss:1.5944, Accuracy:0.2472, Validation Loss:1.6054, Validation Accuracy:0.2167\n",
    "Epoch #117: Loss:1.5942, Accuracy:0.2431, Validation Loss:1.6067, Validation Accuracy:0.2217\n",
    "Epoch #118: Loss:1.5939, Accuracy:0.2485, Validation Loss:1.6070, Validation Accuracy:0.2069\n",
    "Epoch #119: Loss:1.5934, Accuracy:0.2509, Validation Loss:1.6078, Validation Accuracy:0.2135\n",
    "Epoch #120: Loss:1.5942, Accuracy:0.2448, Validation Loss:1.6070, Validation Accuracy:0.2151\n",
    "Epoch #121: Loss:1.5950, Accuracy:0.2513, Validation Loss:1.6086, Validation Accuracy:0.2020\n",
    "Epoch #122: Loss:1.5943, Accuracy:0.2480, Validation Loss:1.6089, Validation Accuracy:0.2233\n",
    "Epoch #123: Loss:1.5948, Accuracy:0.2444, Validation Loss:1.6056, Validation Accuracy:0.2102\n",
    "Epoch #124: Loss:1.5953, Accuracy:0.2493, Validation Loss:1.6061, Validation Accuracy:0.2151\n",
    "Epoch #125: Loss:1.5946, Accuracy:0.2538, Validation Loss:1.6055, Validation Accuracy:0.2217\n",
    "Epoch #126: Loss:1.5964, Accuracy:0.2423, Validation Loss:1.6058, Validation Accuracy:0.2332\n",
    "Epoch #127: Loss:1.5950, Accuracy:0.2501, Validation Loss:1.6052, Validation Accuracy:0.2069\n",
    "Epoch #128: Loss:1.5946, Accuracy:0.2444, Validation Loss:1.6043, Validation Accuracy:0.2102\n",
    "Epoch #129: Loss:1.5941, Accuracy:0.2501, Validation Loss:1.6044, Validation Accuracy:0.2118\n",
    "Epoch #130: Loss:1.5943, Accuracy:0.2534, Validation Loss:1.6046, Validation Accuracy:0.2102\n",
    "Epoch #131: Loss:1.5945, Accuracy:0.2526, Validation Loss:1.6047, Validation Accuracy:0.2118\n",
    "Epoch #132: Loss:1.5939, Accuracy:0.2542, Validation Loss:1.6046, Validation Accuracy:0.2151\n",
    "Epoch #133: Loss:1.5948, Accuracy:0.2448, Validation Loss:1.6051, Validation Accuracy:0.2102\n",
    "Epoch #134: Loss:1.5955, Accuracy:0.2427, Validation Loss:1.6052, Validation Accuracy:0.2085\n",
    "Epoch #135: Loss:1.5938, Accuracy:0.2493, Validation Loss:1.6060, Validation Accuracy:0.2135\n",
    "Epoch #136: Loss:1.5937, Accuracy:0.2534, Validation Loss:1.6049, Validation Accuracy:0.2135\n",
    "Epoch #137: Loss:1.5930, Accuracy:0.2538, Validation Loss:1.6044, Validation Accuracy:0.2118\n",
    "Epoch #138: Loss:1.5937, Accuracy:0.2485, Validation Loss:1.6043, Validation Accuracy:0.2085\n",
    "Epoch #139: Loss:1.5934, Accuracy:0.2513, Validation Loss:1.6044, Validation Accuracy:0.2053\n",
    "Epoch #140: Loss:1.5930, Accuracy:0.2522, Validation Loss:1.6048, Validation Accuracy:0.2118\n",
    "Epoch #141: Loss:1.5939, Accuracy:0.2509, Validation Loss:1.6054, Validation Accuracy:0.2151\n",
    "Epoch #142: Loss:1.5938, Accuracy:0.2517, Validation Loss:1.6043, Validation Accuracy:0.2085\n",
    "Epoch #143: Loss:1.5934, Accuracy:0.2517, Validation Loss:1.6038, Validation Accuracy:0.2036\n",
    "Epoch #144: Loss:1.5937, Accuracy:0.2509, Validation Loss:1.6034, Validation Accuracy:0.2053\n",
    "Epoch #145: Loss:1.5935, Accuracy:0.2480, Validation Loss:1.6040, Validation Accuracy:0.2020\n",
    "Epoch #146: Loss:1.5939, Accuracy:0.2464, Validation Loss:1.6040, Validation Accuracy:0.2151\n",
    "Epoch #147: Loss:1.5929, Accuracy:0.2530, Validation Loss:1.6036, Validation Accuracy:0.2151\n",
    "Epoch #148: Loss:1.5928, Accuracy:0.2534, Validation Loss:1.6040, Validation Accuracy:0.2085\n",
    "Epoch #149: Loss:1.5932, Accuracy:0.2513, Validation Loss:1.6039, Validation Accuracy:0.2135\n",
    "Epoch #150: Loss:1.5926, Accuracy:0.2546, Validation Loss:1.6040, Validation Accuracy:0.2118\n",
    "Epoch #151: Loss:1.5936, Accuracy:0.2534, Validation Loss:1.6036, Validation Accuracy:0.2102\n",
    "Epoch #152: Loss:1.5925, Accuracy:0.2542, Validation Loss:1.6028, Validation Accuracy:0.2118\n",
    "Epoch #153: Loss:1.5926, Accuracy:0.2530, Validation Loss:1.6026, Validation Accuracy:0.2102\n",
    "Epoch #154: Loss:1.5923, Accuracy:0.2485, Validation Loss:1.6034, Validation Accuracy:0.2102\n",
    "Epoch #155: Loss:1.5933, Accuracy:0.2513, Validation Loss:1.6040, Validation Accuracy:0.2020\n",
    "Epoch #156: Loss:1.5929, Accuracy:0.2522, Validation Loss:1.6056, Validation Accuracy:0.2118\n",
    "Epoch #157: Loss:1.5947, Accuracy:0.2493, Validation Loss:1.6058, Validation Accuracy:0.2069\n",
    "Epoch #158: Loss:1.5937, Accuracy:0.2489, Validation Loss:1.6052, Validation Accuracy:0.2200\n",
    "Epoch #159: Loss:1.5936, Accuracy:0.2480, Validation Loss:1.6042, Validation Accuracy:0.2118\n",
    "Epoch #160: Loss:1.5933, Accuracy:0.2522, Validation Loss:1.6051, Validation Accuracy:0.2118\n",
    "Epoch #161: Loss:1.5932, Accuracy:0.2505, Validation Loss:1.6053, Validation Accuracy:0.2151\n",
    "Epoch #162: Loss:1.5924, Accuracy:0.2579, Validation Loss:1.6044, Validation Accuracy:0.2085\n",
    "Epoch #163: Loss:1.5935, Accuracy:0.2501, Validation Loss:1.6042, Validation Accuracy:0.2036\n",
    "Epoch #164: Loss:1.5925, Accuracy:0.2480, Validation Loss:1.6044, Validation Accuracy:0.2020\n",
    "Epoch #165: Loss:1.5924, Accuracy:0.2497, Validation Loss:1.6046, Validation Accuracy:0.2020\n",
    "Epoch #166: Loss:1.5928, Accuracy:0.2497, Validation Loss:1.6046, Validation Accuracy:0.2102\n",
    "Epoch #167: Loss:1.5921, Accuracy:0.2505, Validation Loss:1.6046, Validation Accuracy:0.2069\n",
    "Epoch #168: Loss:1.5927, Accuracy:0.2460, Validation Loss:1.6046, Validation Accuracy:0.2069\n",
    "Epoch #169: Loss:1.5928, Accuracy:0.2497, Validation Loss:1.6044, Validation Accuracy:0.2053\n",
    "Epoch #170: Loss:1.5923, Accuracy:0.2460, Validation Loss:1.6052, Validation Accuracy:0.2053\n",
    "Epoch #171: Loss:1.5927, Accuracy:0.2468, Validation Loss:1.6042, Validation Accuracy:0.2118\n",
    "Epoch #172: Loss:1.5917, Accuracy:0.2534, Validation Loss:1.6036, Validation Accuracy:0.2184\n",
    "Epoch #173: Loss:1.5926, Accuracy:0.2505, Validation Loss:1.6017, Validation Accuracy:0.2217\n",
    "Epoch #174: Loss:1.5924, Accuracy:0.2464, Validation Loss:1.6017, Validation Accuracy:0.2135\n",
    "Epoch #175: Loss:1.5917, Accuracy:0.2600, Validation Loss:1.6023, Validation Accuracy:0.2348\n",
    "Epoch #176: Loss:1.5915, Accuracy:0.2493, Validation Loss:1.6042, Validation Accuracy:0.2102\n",
    "Epoch #177: Loss:1.5912, Accuracy:0.2493, Validation Loss:1.6033, Validation Accuracy:0.2200\n",
    "Epoch #178: Loss:1.5908, Accuracy:0.2505, Validation Loss:1.6025, Validation Accuracy:0.2151\n",
    "Epoch #179: Loss:1.5906, Accuracy:0.2542, Validation Loss:1.6027, Validation Accuracy:0.2135\n",
    "Epoch #180: Loss:1.5913, Accuracy:0.2522, Validation Loss:1.6020, Validation Accuracy:0.2250\n",
    "Epoch #181: Loss:1.5900, Accuracy:0.2567, Validation Loss:1.6024, Validation Accuracy:0.2135\n",
    "Epoch #182: Loss:1.5904, Accuracy:0.2546, Validation Loss:1.6028, Validation Accuracy:0.2282\n",
    "Epoch #183: Loss:1.5902, Accuracy:0.2575, Validation Loss:1.6025, Validation Accuracy:0.2266\n",
    "Epoch #184: Loss:1.5904, Accuracy:0.2538, Validation Loss:1.6029, Validation Accuracy:0.2233\n",
    "Epoch #185: Loss:1.5902, Accuracy:0.2600, Validation Loss:1.6027, Validation Accuracy:0.2282\n",
    "Epoch #186: Loss:1.5899, Accuracy:0.2530, Validation Loss:1.6026, Validation Accuracy:0.2085\n",
    "Epoch #187: Loss:1.5916, Accuracy:0.2448, Validation Loss:1.6030, Validation Accuracy:0.2282\n",
    "Epoch #188: Loss:1.5906, Accuracy:0.2505, Validation Loss:1.6041, Validation Accuracy:0.2118\n",
    "Epoch #189: Loss:1.5900, Accuracy:0.2542, Validation Loss:1.6048, Validation Accuracy:0.2315\n",
    "Epoch #190: Loss:1.5911, Accuracy:0.2452, Validation Loss:1.6030, Validation Accuracy:0.2250\n",
    "Epoch #191: Loss:1.5915, Accuracy:0.2550, Validation Loss:1.6033, Validation Accuracy:0.2085\n",
    "Epoch #192: Loss:1.5899, Accuracy:0.2575, Validation Loss:1.6031, Validation Accuracy:0.2250\n",
    "Epoch #193: Loss:1.5904, Accuracy:0.2550, Validation Loss:1.6027, Validation Accuracy:0.2299\n",
    "Epoch #194: Loss:1.5905, Accuracy:0.2513, Validation Loss:1.6031, Validation Accuracy:0.2102\n",
    "Epoch #195: Loss:1.5890, Accuracy:0.2600, Validation Loss:1.6037, Validation Accuracy:0.2200\n",
    "Epoch #196: Loss:1.5901, Accuracy:0.2608, Validation Loss:1.6023, Validation Accuracy:0.2118\n",
    "Epoch #197: Loss:1.5906, Accuracy:0.2538, Validation Loss:1.6028, Validation Accuracy:0.2266\n",
    "Epoch #198: Loss:1.5901, Accuracy:0.2530, Validation Loss:1.6038, Validation Accuracy:0.2167\n",
    "Epoch #199: Loss:1.5903, Accuracy:0.2587, Validation Loss:1.6033, Validation Accuracy:0.2167\n",
    "Epoch #200: Loss:1.5898, Accuracy:0.2472, Validation Loss:1.6034, Validation Accuracy:0.2299\n",
    "Epoch #201: Loss:1.5895, Accuracy:0.2530, Validation Loss:1.6033, Validation Accuracy:0.2397\n",
    "Epoch #202: Loss:1.5895, Accuracy:0.2595, Validation Loss:1.6038, Validation Accuracy:0.2135\n",
    "Epoch #203: Loss:1.5893, Accuracy:0.2637, Validation Loss:1.6031, Validation Accuracy:0.2102\n",
    "Epoch #204: Loss:1.5887, Accuracy:0.2583, Validation Loss:1.6029, Validation Accuracy:0.2118\n",
    "Epoch #205: Loss:1.5890, Accuracy:0.2571, Validation Loss:1.6033, Validation Accuracy:0.2217\n",
    "Epoch #206: Loss:1.5887, Accuracy:0.2608, Validation Loss:1.6030, Validation Accuracy:0.2118\n",
    "Epoch #207: Loss:1.5889, Accuracy:0.2587, Validation Loss:1.6039, Validation Accuracy:0.2135\n",
    "Epoch #208: Loss:1.5885, Accuracy:0.2571, Validation Loss:1.6042, Validation Accuracy:0.2102\n",
    "Epoch #209: Loss:1.5880, Accuracy:0.2612, Validation Loss:1.6043, Validation Accuracy:0.2282\n",
    "Epoch #210: Loss:1.5884, Accuracy:0.2554, Validation Loss:1.6041, Validation Accuracy:0.2282\n",
    "Epoch #211: Loss:1.5883, Accuracy:0.2563, Validation Loss:1.6042, Validation Accuracy:0.2102\n",
    "Epoch #212: Loss:1.5893, Accuracy:0.2530, Validation Loss:1.6044, Validation Accuracy:0.2085\n",
    "Epoch #213: Loss:1.5894, Accuracy:0.2513, Validation Loss:1.6056, Validation Accuracy:0.2430\n",
    "Epoch #214: Loss:1.5895, Accuracy:0.2575, Validation Loss:1.6066, Validation Accuracy:0.2118\n",
    "Epoch #215: Loss:1.5899, Accuracy:0.2616, Validation Loss:1.6049, Validation Accuracy:0.2250\n",
    "Epoch #216: Loss:1.5891, Accuracy:0.2575, Validation Loss:1.6042, Validation Accuracy:0.2118\n",
    "Epoch #217: Loss:1.5887, Accuracy:0.2583, Validation Loss:1.6039, Validation Accuracy:0.2118\n",
    "Epoch #218: Loss:1.5883, Accuracy:0.2559, Validation Loss:1.6045, Validation Accuracy:0.2085\n",
    "Epoch #219: Loss:1.5887, Accuracy:0.2575, Validation Loss:1.6043, Validation Accuracy:0.2085\n",
    "Epoch #220: Loss:1.5878, Accuracy:0.2583, Validation Loss:1.6038, Validation Accuracy:0.2118\n",
    "Epoch #221: Loss:1.5889, Accuracy:0.2600, Validation Loss:1.6035, Validation Accuracy:0.2102\n",
    "Epoch #222: Loss:1.5882, Accuracy:0.2563, Validation Loss:1.6042, Validation Accuracy:0.2233\n",
    "Epoch #223: Loss:1.5886, Accuracy:0.2575, Validation Loss:1.6055, Validation Accuracy:0.2167\n",
    "Epoch #224: Loss:1.5909, Accuracy:0.2509, Validation Loss:1.6055, Validation Accuracy:0.2496\n",
    "Epoch #225: Loss:1.5887, Accuracy:0.2526, Validation Loss:1.6084, Validation Accuracy:0.2036\n",
    "Epoch #226: Loss:1.5909, Accuracy:0.2517, Validation Loss:1.6033, Validation Accuracy:0.2381\n",
    "Epoch #227: Loss:1.5902, Accuracy:0.2505, Validation Loss:1.6035, Validation Accuracy:0.2381\n",
    "Epoch #228: Loss:1.5906, Accuracy:0.2567, Validation Loss:1.6043, Validation Accuracy:0.2250\n",
    "Epoch #229: Loss:1.5898, Accuracy:0.2559, Validation Loss:1.6052, Validation Accuracy:0.2414\n",
    "Epoch #230: Loss:1.5893, Accuracy:0.2554, Validation Loss:1.6039, Validation Accuracy:0.2200\n",
    "Epoch #231: Loss:1.5891, Accuracy:0.2587, Validation Loss:1.6032, Validation Accuracy:0.2118\n",
    "Epoch #232: Loss:1.5882, Accuracy:0.2571, Validation Loss:1.6031, Validation Accuracy:0.2118\n",
    "Epoch #233: Loss:1.5889, Accuracy:0.2542, Validation Loss:1.6039, Validation Accuracy:0.2151\n",
    "Epoch #234: Loss:1.5878, Accuracy:0.2600, Validation Loss:1.6037, Validation Accuracy:0.2200\n",
    "Epoch #235: Loss:1.5879, Accuracy:0.2612, Validation Loss:1.6041, Validation Accuracy:0.2266\n",
    "Epoch #236: Loss:1.5884, Accuracy:0.2595, Validation Loss:1.6037, Validation Accuracy:0.2381\n",
    "Epoch #237: Loss:1.5880, Accuracy:0.2559, Validation Loss:1.6038, Validation Accuracy:0.2118\n",
    "Epoch #238: Loss:1.5879, Accuracy:0.2563, Validation Loss:1.6044, Validation Accuracy:0.2167\n",
    "Epoch #239: Loss:1.5883, Accuracy:0.2550, Validation Loss:1.6049, Validation Accuracy:0.2118\n",
    "Epoch #240: Loss:1.5879, Accuracy:0.2542, Validation Loss:1.6036, Validation Accuracy:0.2151\n",
    "Epoch #241: Loss:1.5876, Accuracy:0.2571, Validation Loss:1.6036, Validation Accuracy:0.2184\n",
    "Epoch #242: Loss:1.5890, Accuracy:0.2612, Validation Loss:1.6044, Validation Accuracy:0.2200\n",
    "Epoch #243: Loss:1.5872, Accuracy:0.2559, Validation Loss:1.6042, Validation Accuracy:0.2184\n",
    "Epoch #244: Loss:1.5873, Accuracy:0.2600, Validation Loss:1.6036, Validation Accuracy:0.2167\n",
    "Epoch #245: Loss:1.5871, Accuracy:0.2587, Validation Loss:1.6042, Validation Accuracy:0.2184\n",
    "Epoch #246: Loss:1.5873, Accuracy:0.2595, Validation Loss:1.6038, Validation Accuracy:0.2118\n",
    "Epoch #247: Loss:1.5874, Accuracy:0.2575, Validation Loss:1.6036, Validation Accuracy:0.2118\n",
    "Epoch #248: Loss:1.5886, Accuracy:0.2608, Validation Loss:1.6052, Validation Accuracy:0.2200\n",
    "Epoch #249: Loss:1.5872, Accuracy:0.2579, Validation Loss:1.6041, Validation Accuracy:0.2118\n",
    "Epoch #250: Loss:1.5878, Accuracy:0.2587, Validation Loss:1.6037, Validation Accuracy:0.2118\n",
    "Epoch #251: Loss:1.5881, Accuracy:0.2604, Validation Loss:1.6056, Validation Accuracy:0.2266\n",
    "Epoch #252: Loss:1.5873, Accuracy:0.2567, Validation Loss:1.6033, Validation Accuracy:0.2118\n",
    "Epoch #253: Loss:1.5873, Accuracy:0.2571, Validation Loss:1.6038, Validation Accuracy:0.2167\n",
    "Epoch #254: Loss:1.5875, Accuracy:0.2571, Validation Loss:1.6046, Validation Accuracy:0.2118\n",
    "Epoch #255: Loss:1.5866, Accuracy:0.2632, Validation Loss:1.6035, Validation Accuracy:0.2430\n",
    "Epoch #256: Loss:1.5872, Accuracy:0.2591, Validation Loss:1.6031, Validation Accuracy:0.2365\n",
    "Epoch #257: Loss:1.5868, Accuracy:0.2559, Validation Loss:1.6040, Validation Accuracy:0.2135\n",
    "Epoch #258: Loss:1.5865, Accuracy:0.2575, Validation Loss:1.6044, Validation Accuracy:0.2135\n",
    "Epoch #259: Loss:1.5871, Accuracy:0.2559, Validation Loss:1.6048, Validation Accuracy:0.2200\n",
    "Epoch #260: Loss:1.5871, Accuracy:0.2563, Validation Loss:1.6038, Validation Accuracy:0.2118\n",
    "Epoch #261: Loss:1.5869, Accuracy:0.2575, Validation Loss:1.6046, Validation Accuracy:0.2135\n",
    "Epoch #262: Loss:1.5865, Accuracy:0.2595, Validation Loss:1.6047, Validation Accuracy:0.2200\n",
    "Epoch #263: Loss:1.5875, Accuracy:0.2505, Validation Loss:1.6047, Validation Accuracy:0.2135\n",
    "Epoch #264: Loss:1.5906, Accuracy:0.2526, Validation Loss:1.6049, Validation Accuracy:0.2414\n",
    "Epoch #265: Loss:1.5878, Accuracy:0.2645, Validation Loss:1.6051, Validation Accuracy:0.2167\n",
    "Epoch #266: Loss:1.5878, Accuracy:0.2554, Validation Loss:1.6048, Validation Accuracy:0.2200\n",
    "Epoch #267: Loss:1.5867, Accuracy:0.2616, Validation Loss:1.6030, Validation Accuracy:0.2135\n",
    "Epoch #268: Loss:1.5863, Accuracy:0.2583, Validation Loss:1.6030, Validation Accuracy:0.2118\n",
    "Epoch #269: Loss:1.5866, Accuracy:0.2571, Validation Loss:1.6046, Validation Accuracy:0.2135\n",
    "Epoch #270: Loss:1.5871, Accuracy:0.2538, Validation Loss:1.6048, Validation Accuracy:0.2430\n",
    "Epoch #271: Loss:1.5871, Accuracy:0.2579, Validation Loss:1.6032, Validation Accuracy:0.2118\n",
    "Epoch #272: Loss:1.5863, Accuracy:0.2571, Validation Loss:1.6039, Validation Accuracy:0.2118\n",
    "Epoch #273: Loss:1.5861, Accuracy:0.2583, Validation Loss:1.6044, Validation Accuracy:0.2200\n",
    "Epoch #274: Loss:1.5873, Accuracy:0.2608, Validation Loss:1.6043, Validation Accuracy:0.2496\n",
    "Epoch #275: Loss:1.5860, Accuracy:0.2591, Validation Loss:1.6050, Validation Accuracy:0.2167\n",
    "Epoch #276: Loss:1.5869, Accuracy:0.2571, Validation Loss:1.6034, Validation Accuracy:0.2135\n",
    "Epoch #277: Loss:1.5871, Accuracy:0.2591, Validation Loss:1.6050, Validation Accuracy:0.2200\n",
    "Epoch #278: Loss:1.5856, Accuracy:0.2637, Validation Loss:1.6059, Validation Accuracy:0.2118\n",
    "Epoch #279: Loss:1.5861, Accuracy:0.2571, Validation Loss:1.6033, Validation Accuracy:0.2135\n",
    "Epoch #280: Loss:1.5861, Accuracy:0.2575, Validation Loss:1.6043, Validation Accuracy:0.2430\n",
    "Epoch #281: Loss:1.5857, Accuracy:0.2595, Validation Loss:1.6040, Validation Accuracy:0.2135\n",
    "Epoch #282: Loss:1.5857, Accuracy:0.2583, Validation Loss:1.6047, Validation Accuracy:0.2135\n",
    "Epoch #283: Loss:1.5858, Accuracy:0.2571, Validation Loss:1.6047, Validation Accuracy:0.2200\n",
    "Epoch #284: Loss:1.5857, Accuracy:0.2608, Validation Loss:1.6047, Validation Accuracy:0.2135\n",
    "Epoch #285: Loss:1.5865, Accuracy:0.2546, Validation Loss:1.6046, Validation Accuracy:0.2200\n",
    "Epoch #286: Loss:1.5856, Accuracy:0.2550, Validation Loss:1.6049, Validation Accuracy:0.2135\n",
    "Epoch #287: Loss:1.5867, Accuracy:0.2595, Validation Loss:1.6061, Validation Accuracy:0.2135\n",
    "Epoch #288: Loss:1.5860, Accuracy:0.2595, Validation Loss:1.6050, Validation Accuracy:0.2135\n",
    "Epoch #289: Loss:1.5859, Accuracy:0.2587, Validation Loss:1.6041, Validation Accuracy:0.2200\n",
    "Epoch #290: Loss:1.5859, Accuracy:0.2600, Validation Loss:1.6048, Validation Accuracy:0.2118\n",
    "Epoch #291: Loss:1.5854, Accuracy:0.2579, Validation Loss:1.6052, Validation Accuracy:0.2135\n",
    "Epoch #292: Loss:1.5857, Accuracy:0.2612, Validation Loss:1.6057, Validation Accuracy:0.2135\n",
    "Epoch #293: Loss:1.5858, Accuracy:0.2591, Validation Loss:1.6046, Validation Accuracy:0.2200\n",
    "Epoch #294: Loss:1.5864, Accuracy:0.2628, Validation Loss:1.6031, Validation Accuracy:0.2365\n",
    "Epoch #295: Loss:1.5854, Accuracy:0.2534, Validation Loss:1.6052, Validation Accuracy:0.2167\n",
    "Epoch #296: Loss:1.5854, Accuracy:0.2579, Validation Loss:1.6048, Validation Accuracy:0.2200\n",
    "Epoch #297: Loss:1.5858, Accuracy:0.2591, Validation Loss:1.6051, Validation Accuracy:0.2135\n",
    "Epoch #298: Loss:1.5852, Accuracy:0.2595, Validation Loss:1.6057, Validation Accuracy:0.2200\n",
    "Epoch #299: Loss:1.5856, Accuracy:0.2612, Validation Loss:1.6050, Validation Accuracy:0.2200\n",
    "Epoch #300: Loss:1.5851, Accuracy:0.2591, Validation Loss:1.6041, Validation Accuracy:0.2135\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60413826, Accuracy:0.2135\n",
    "Labels: ['01', '05', '04', '02', '03']\n",
    "Confusion Matrix:\n",
    "      01  05  04  02  03\n",
    "t:01  41  48  20   4  13\n",
    "t:05  41  50  19   7  25\n",
    "t:04  32  40  23   3  14\n",
    "t:02  29  45  21   5  14\n",
    "t:03  38  39  18   9  11\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.23      0.33      0.27       126\n",
    "          05       0.23      0.35      0.27       142\n",
    "          04       0.23      0.21      0.22       112\n",
    "          02       0.18      0.04      0.07       114\n",
    "          03       0.14      0.10      0.11       115\n",
    "\n",
    "    accuracy                           0.21       609\n",
    "   macro avg       0.20      0.20      0.19       609\n",
    "weighted avg       0.20      0.21      0.19       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 05:03:29 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.607396642562791, 1.6060340946726808, 1.6056915662558795, 1.6059394235094193, 1.605686000219511, 1.6050427787996866, 1.6049808894080677, 1.6048806017059802, 1.6047045164703344, 1.6044284629900076, 1.6044665639623632, 1.604469646765485, 1.604345806126524, 1.6045525124899076, 1.6043506244133259, 1.6042046094762867, 1.604159629403664, 1.6043478411015226, 1.6042268961325459, 1.6035903151986635, 1.6035918496512427, 1.6036293107300557, 1.6035813210633, 1.603768469273359, 1.603797343564151, 1.6038694103754605, 1.604056248523919, 1.603848607669323, 1.6041276619352143, 1.604243844209242, 1.6037907296996594, 1.60358783470586, 1.603793569386299, 1.6038040921018628, 1.6033690727403012, 1.6031406121496692, 1.6036088102556803, 1.6036998241014278, 1.6037925959416406, 1.6040649480616127, 1.6034848010794478, 1.6033930236287108, 1.6032932245085392, 1.6033752726020876, 1.6031194138409468, 1.6026590197348634, 1.6029419024002376, 1.603331471115889, 1.6029005438236181, 1.6025717340666672, 1.6026700187003475, 1.6036873039940895, 1.6032408503280289, 1.6027365676287948, 1.6022912328466405, 1.602151334579355, 1.6020370345984773, 1.6016422316358594, 1.6029361576478078, 1.6012691298533348, 1.6019398171717703, 1.6021533908906633, 1.601730144278365, 1.6019328711263847, 1.6014399986548964, 1.6014520793125546, 1.6019612914822958, 1.6014400865448324, 1.600995269314996, 1.6009685828767974, 1.6010297787404804, 1.6010598077366895, 1.6036579319213216, 1.6018873764376336, 1.6015589491682882, 1.6017191292617121, 1.6020225529208756, 1.6021133931399567, 1.6032954438762321, 1.6034562192331199, 1.6026899722605112, 1.604586968672491, 1.60453595257745, 1.6047178569489904, 1.6032838189151684, 1.604535011039383, 1.608200294630868, 1.6062930843904493, 1.6041641599439047, 1.6050081033816284, 1.6055345760386175, 1.6056181358782138, 1.605265341368802, 1.6053465111502285, 1.6058260646751166, 1.605538361765481, 1.6059646062271544, 1.6065210000243288, 1.6048947310408543, 1.6049226996150903, 1.605127776197612, 1.604444009329885, 1.604539116615145, 1.605972784689103, 1.6051389603387742, 1.6050030010674388, 1.605333792947979, 1.60461651123999, 1.6048004364928197, 1.6059316508288455, 1.6054904402183194, 1.6055996241827903, 1.6054056610771392, 1.6053693507888243, 1.6053192705552175, 1.6053508318508005, 1.6067302720300083, 1.6070103126597914, 1.6077969132973056, 1.6070144650188378, 1.6086309187126473, 1.6089491793283297, 1.6055885394805758, 1.6061352207547142, 1.60545592668217, 1.605761741378233, 1.6051852444709815, 1.6043053702963592, 1.6044427702579591, 1.6046399571038232, 1.6046517165423615, 1.6046444690481978, 1.6050647395389226, 1.605219524090709, 1.6060467619809806, 1.6049060768681793, 1.604411210136852, 1.604312694718685, 1.6044061504952818, 1.6047854544885445, 1.6054079646155948, 1.6042743379063598, 1.6037593203029414, 1.603375943814984, 1.6040258597466355, 1.6039690209922728, 1.603554606633429, 1.6039788296265751, 1.603866306823267, 1.6040297840616387, 1.6036289047529348, 1.6028023684161834, 1.6025759197025269, 1.6034090082437926, 1.6039627068148459, 1.6056024292223952, 1.605751848768914, 1.6052481116136699, 1.6042215802595141, 1.6051046883531392, 1.6053381449679045, 1.6043764092456336, 1.6042302230308796, 1.6043904792694819, 1.6046413173424983, 1.6046248646988266, 1.604625900977938, 1.604619879636467, 1.6044173277853353, 1.6052108846470248, 1.6042363234537185, 1.6035555103925256, 1.6017142405063647, 1.601715657707114, 1.6022637045050685, 1.6041563205139586, 1.6033469824172397, 1.6025442724744674, 1.6027299371258965, 1.6020466306526673, 1.6024440740325376, 1.6027872879516902, 1.6025016217787669, 1.6028850006156758, 1.6027368463710416, 1.6026273306171686, 1.6030200854898087, 1.6040831776871078, 1.6047627346464761, 1.6029964740248932, 1.6032990692871545, 1.6030854564190693, 1.6027134664735967, 1.6031085659913438, 1.6036807376958662, 1.602263761075651, 1.6028415041016828, 1.6038118800506216, 1.6032963811078877, 1.6034052841769064, 1.6032636936857978, 1.6038097920284677, 1.603072811230063, 1.602886911096244, 1.6033471439076565, 1.6029712926773798, 1.6038616727334134, 1.6041504027221003, 1.6042565187601425, 1.6041323625786943, 1.604218157641406, 1.6044150986303445, 1.605610288814175, 1.6065743149599223, 1.604908229682246, 1.6041832808007552, 1.6038935386097097, 1.6045422031374401, 1.6042534801956077, 1.6038173883419318, 1.6034843058421695, 1.6041982424474506, 1.6055372530603644, 1.605537668824783, 1.6083589316588904, 1.6033045323611481, 1.6035008195585805, 1.6043429621334733, 1.6051656311172962, 1.6039028341938513, 1.6031632638721436, 1.603144636490858, 1.603858388116207, 1.603707482270615, 1.6041193711150847, 1.603712494150171, 1.6038020618247673, 1.6043687418763861, 1.6048964326604833, 1.603645867510578, 1.6036480894229683, 1.6043968098895696, 1.6041938600869015, 1.6035660090313364, 1.6042379219152265, 1.603814764171594, 1.60364952345787, 1.6052278914475089, 1.604142091544391, 1.603676175053288, 1.605560546242349, 1.6032754889458467, 1.6037568118184657, 1.6045974500856572, 1.6034582493144696, 1.603137668130433, 1.6039860878867664, 1.6044113573378138, 1.6047725896725709, 1.6038322509411715, 1.6046097654427214, 1.604704235966374, 1.6046512393137113, 1.6049222928549856, 1.6050818710295829, 1.6047967787838138, 1.603046452079109, 1.6029752297158704, 1.6046007442944155, 1.6048097702474233, 1.6032082529490805, 1.603858754552644, 1.6044289913083531, 1.604331680706569, 1.6049717326078117, 1.6033962391475933, 1.6049994377079855, 1.6058880558546345, 1.6032925397891717, 1.6043424514322642, 1.604004730536237, 1.604741881633627, 1.6047135477974301, 1.6046891960212946, 1.6045748880148325, 1.6049444700892532, 1.606063284114468, 1.6049817923841805, 1.604133689736302, 1.60477933253365, 1.6052304709877678, 1.6057329042791733, 1.6045816643484707, 1.60314702498306, 1.605155133652961, 1.6048015475468878, 1.6050663383919226, 1.6056918361895582, 1.6050409599282276, 1.6041383302857724], 'val_acc': [0.2282430211262554, 0.23481116582118036, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.2364532019459751, 0.2364532019459751, 0.23316912969638562, 0.23316912969638562, 0.2397372741955646, 0.23316912969638562, 0.2397372741955646, 0.2397372741955646, 0.2430213441696073, 0.24630541651706978, 0.24630541651706978, 0.24630541651706978, 0.2430213442674803, 0.2430213442674803, 0.2430213442674803, 0.24137930814268554, 0.2430213442674803, 0.2430213442674803, 0.2430213442674803, 0.24466338039227503, 0.24630541651706978, 0.24466338039227503, 0.2479474527397375, 0.24630541651706978, 0.24466338039227503, 0.24466338039227503, 0.24466338039227503, 0.24794745264186452, 0.24794745264186452, 0.251231524891454, 0.25287356101624875, 0.24958948876665926, 0.24794745264186452, 0.24958948876665926, 0.2545155972389165, 0.24794745264186452, 0.251231524891454, 0.25287356101624875, 0.24466338039227503, 0.24794745264186452, 0.24794745264186452, 0.2561576333637112, 0.251231524891454, 0.2479474527397375, 0.23809523579522307, 0.2545155972389165, 0.25287356111412174, 0.24466338039227503, 0.24630541651706978, 0.25287356111412174, 0.23645319967042833, 0.24137930814268554, 0.24794745264186452, 0.24137931032035934, 0.23645320184810212, 0.2315270934737179, 0.23481116572330737, 0.2348111656254344, 0.2315270933758449, 0.23481116582118036, 0.2364532019459751, 0.23316912940276668, 0.23316912950063964, 0.23316912950063964, 0.22988505725105016, 0.22660098490358768, 0.22495894887666593, 0.22660098500146067, 0.2282430211262554, 0.22660098490358768, 0.2233169126539982, 0.21346469600310272, 0.2233169126539982, 0.21182265987830795, 0.2200328405022817, 0.21674876815481922, 0.2282430211262554, 0.22331691275187118, 0.21182265987830795, 0.2183908044753599, 0.22331691294761713, 0.21510673203002448, 0.21346469590522973, 0.21018062365564025, 0.2151067319321515, 0.21346469580735675, 0.22167487662707644, 0.211822659682562, 0.21182265958468902, 0.22331691275187118, 0.21018062355776726, 0.20689655121030479, 0.21510673212789747, 0.21346469580735675, 0.22167487643133046, 0.22167487662707644, 0.211822659682562, 0.21346469570948376, 0.2151067319321515, 0.22003284020866276, 0.2151067319321515, 0.21839080437748695, 0.21510673173640554, 0.20853858733509953, 0.21839080427961396, 0.21674876795907325, 0.2134646956116108, 0.21674876795907325, 0.22167487652920345, 0.20689655091668585, 0.2134646956116108, 0.21510673163853256, 0.20197044244442863, 0.2233169126539982, 0.21018062345989427, 0.21510673163853256, 0.22167487652920345, 0.23316912950063964, 0.20689655121030479, 0.21018062345989427, 0.211822659780435, 0.21018062326414833, 0.21182265938894307, 0.21510673173640554, 0.21018062355776726, 0.2085385870414806, 0.21346469580735675, 0.2134646956116108, 0.21182265948681606, 0.20853858713935358, 0.20525451508551004, 0.21182265958468902, 0.21510673163853256, 0.20853858723722654, 0.20361247905858829, 0.20525451498763705, 0.20197044234655565, 0.2151067318342785, 0.21510673163853256, 0.2085385870414806, 0.2134646956116108, 0.21182265948681606, 0.21018062316627534, 0.21182265938894307, 0.21018062336202128, 0.21018062336202128, 0.20197044283592056, 0.21182265938894307, 0.20689655130817777, 0.2200328405022817, 0.211822659780435, 0.21182265958468902, 0.2151067318342785, 0.20853858733509953, 0.20361247905858829, 0.20197044293379354, 0.20197044293379354, 0.21018062345989427, 0.20689655121030479, 0.2068965511124318, 0.20525451508551004, 0.20525451518338303, 0.211822659780435, 0.21839080418174098, 0.22167487662707644, 0.2134646956116108, 0.23481116533181545, 0.21018062336202128, 0.22003284020866276, 0.2151067319321515, 0.21346469580735675, 0.22495894868091998, 0.2134646956116108, 0.22824302083263648, 0.22660098480571472, 0.22331691245825225, 0.22824302093050947, 0.20853858723722654, 0.22824302093050947, 0.21182265938894307, 0.23152709327797194, 0.22495894868091998, 0.20853858723722654, 0.224958948583047, 0.2298850571531772, 0.21018062336202128, 0.22003284020866276, 0.21182265958468902, 0.22660098480571472, 0.2167487676654543, 0.21674876795907325, 0.2298850570553042, 0.2397372741955646, 0.2134646956116108, 0.21018062336202128, 0.21182265958468902, 0.22167487643133046, 0.21182265958468902, 0.2134646956116108, 0.21018062316627534, 0.22824302083263648, 0.22824302083263648, 0.21018062326414833, 0.20853858713935358, 0.24302134644515408, 0.21182265987830795, 0.22495894877879294, 0.21182265958468902, 0.21182265958468902, 0.20853858713935358, 0.20853858713935358, 0.21182265958468902, 0.21018062345989427, 0.22331691245825225, 0.21674876805694623, 0.24958949094433308, 0.20361247856922338, 0.23809523797289686, 0.23809523797289686, 0.22495894907241187, 0.24137931022248635, 0.22003284069802764, 0.21182265958468902, 0.21182265958468902, 0.2151067318342785, 0.22003284020866276, 0.22660098470784173, 0.23809523797289686, 0.21182265958468902, 0.21674876795907325, 0.21182265958468902, 0.2151067318342785, 0.21839080408386802, 0.22003284020866276, 0.2183908045732329, 0.21674876795907325, 0.21839080408386802, 0.21182265958468902, 0.21182265958468902, 0.22003284020866276, 0.21182265948681606, 0.21182265958468902, 0.22660098470784173, 0.21182265958468902, 0.21674876795907325, 0.21182265958468902, 0.24302134634728112, 0.23645320184810212, 0.21346469570948376, 0.21346469570948376, 0.22003284020866276, 0.21182265958468902, 0.21346469570948376, 0.22003284020866276, 0.21346469570948376, 0.24137931022248635, 0.2167487678612003, 0.22003284020866276, 0.21346469570948376, 0.21182265958468902, 0.21346469570948376, 0.24302134634728112, 0.21182265958468902, 0.21182265958468902, 0.22003284020866276, 0.24958949094433308, 0.21674876835056517, 0.21346469570948376, 0.22003284020866276, 0.21182265958468902, 0.21346469570948376, 0.24302134634728112, 0.21346469570948376, 0.21346469570948376, 0.22003284020866276, 0.21346469570948376, 0.22003284020866276, 0.21346469570948376, 0.21346469570948376, 0.21346469570948376, 0.22003284020866276, 0.21182265948681606, 0.21346469570948376, 0.21346469570948376, 0.22003284020866276, 0.23645320184810212, 0.2167487678612003, 0.22003284020866276, 0.21346469570948376, 0.22003284020866276, 0.22003284020866276, 0.21346469570948376], 'loss': [1.6087818248560786, 1.6066592444629395, 1.605866672811567, 1.6052412717494142, 1.6053411172400756, 1.6051728774879503, 1.6043993271841406, 1.6040142300192581, 1.6036877193490093, 1.6034183143345482, 1.6034313211695614, 1.6032817443048686, 1.6028871676760288, 1.6026560009382589, 1.602324225476635, 1.6020731842248592, 1.6016890277607974, 1.6016417316832336, 1.6015277392565592, 1.6014462229652326, 1.6010470472077325, 1.6011757289114918, 1.600916639588452, 1.6011286854499174, 1.6008798525318717, 1.6005852408477659, 1.6004546235474228, 1.6007439825813874, 1.6008507530302483, 1.6006307003679217, 1.6007196648654507, 1.6001991249452627, 1.6002510051218146, 1.6003701769106198, 1.5997743788196321, 1.5997467287267257, 1.5995414053879724, 1.599495760371308, 1.5993577610051117, 1.5990778980313876, 1.5990147472162266, 1.5991764643353847, 1.599058822929492, 1.5990846908068022, 1.599312423827467, 1.5989789643571606, 1.598689411993634, 1.5988971891834016, 1.5988567547631705, 1.599038122860558, 1.5989549790564503, 1.59861837848256, 1.5989430292431088, 1.598214883921817, 1.5982865839768239, 1.5980917254022995, 1.5981692940302699, 1.5980560442750213, 1.5982179495833004, 1.5994875773267334, 1.5989463532485022, 1.598768452599308, 1.5982797695625979, 1.5984127090696927, 1.5975259832288204, 1.597830909625216, 1.597815939826887, 1.5973514969588796, 1.5972318832879193, 1.5969524508873785, 1.5965867391601969, 1.5966848065476154, 1.5968153398874114, 1.5975145035455849, 1.5965503430219645, 1.5961105986052715, 1.5962369289731098, 1.5960894160202153, 1.5957636316698924, 1.5961393390103287, 1.5959668419933906, 1.595675194581676, 1.595410438190985, 1.5953685933804365, 1.595673226086266, 1.5971658539723077, 1.597564282407506, 1.5988768774381164, 1.5969224310508743, 1.5959507580410528, 1.5968362686325637, 1.5959297621764197, 1.5955454632976462, 1.5957962373443697, 1.5952328422965456, 1.5951689053854659, 1.596052741955438, 1.594453954011263, 1.5955581446692684, 1.595006994496136, 1.5949658472679968, 1.5943130700250425, 1.5957561133578573, 1.5950396923558667, 1.5951115447637727, 1.5951203487247412, 1.5943953671249766, 1.5945635267841254, 1.5952372352200612, 1.594231367600772, 1.5944258141566596, 1.5940983405592994, 1.59427815237574, 1.5948447437501785, 1.5936931954761795, 1.594356134637915, 1.5942029683252135, 1.5939480950944966, 1.5933924598615516, 1.5941549299678763, 1.5950391878092802, 1.594286414338333, 1.5947788768235662, 1.5952705233004059, 1.5945841545931367, 1.5964422230847808, 1.5949562453146588, 1.5945788727648693, 1.5941273409238341, 1.5942776423949725, 1.5944718172907584, 1.5939484982519914, 1.5947598158211678, 1.595538601346574, 1.593760059012035, 1.5936951080631672, 1.593045304492269, 1.593749630426724, 1.5934487841702094, 1.5929909638555633, 1.593898473725916, 1.5938226099131778, 1.593436267635416, 1.593731005235864, 1.593485432483822, 1.5939332585070412, 1.5928847835783597, 1.592798780171044, 1.5931716390703738, 1.5926047267365504, 1.5936080935065018, 1.5925194980182686, 1.5925506233924223, 1.5923446585265517, 1.5932855547331197, 1.5929023292764746, 1.5947496233534764, 1.5937418608205274, 1.5935676404827674, 1.5933497445539282, 1.5931952128909697, 1.5923683423036423, 1.59347486089632, 1.5925433902035504, 1.5924049685378339, 1.5928239976600944, 1.5921172452903136, 1.5926590114648338, 1.5927789253865423, 1.592315984702453, 1.5927153567269108, 1.5916545223651235, 1.5926063479339319, 1.5924444418423473, 1.5917182668278593, 1.5914626550870266, 1.5912487129411168, 1.5908346794469155, 1.5905564901030773, 1.5913070188165934, 1.5900444488995373, 1.5904180875793865, 1.5901616779930539, 1.5903625666483228, 1.5902271972299846, 1.589941076772169, 1.5915956000283025, 1.5906435791716684, 1.5900035607007006, 1.5910879211993678, 1.591475207311172, 1.5898930871755925, 1.5904136251864738, 1.5904906050625278, 1.5890107147013137, 1.5900903439864487, 1.5906110664167934, 1.5901233788143683, 1.59027555032922, 1.5897837235452703, 1.5894692160020865, 1.5894775133602919, 1.589303734170338, 1.5887362177611866, 1.5889586268998759, 1.5886531946350662, 1.5889139821641989, 1.5885202433783905, 1.5880245815801914, 1.5883983566041355, 1.5882829048795133, 1.5893313834309823, 1.5893757723195352, 1.5894881351282955, 1.589862293873969, 1.5890766394456555, 1.5886863887187637, 1.5883451508790316, 1.5886864886881145, 1.5878061758174544, 1.5889451388705682, 1.588151004867632, 1.5886256469103834, 1.590916842748497, 1.588747853371152, 1.5909226583014768, 1.5902489431584885, 1.5906364578485979, 1.5898482607130642, 1.5892979419696502, 1.5890737300535982, 1.5881898578921871, 1.5889476320337221, 1.5877627630253348, 1.5878606219066487, 1.5883532131477058, 1.5879814540091481, 1.587887798197705, 1.588270202162819, 1.5878641892262797, 1.5876468935541548, 1.5889844014169745, 1.5872324731560459, 1.5872949115059949, 1.5870823381862602, 1.5872610893582417, 1.5873904055883263, 1.5885869768855507, 1.58717181476969, 1.5878064967278827, 1.5881090485829348, 1.5872684264329915, 1.5873213615750386, 1.5874921294942774, 1.586612242056359, 1.587206357460492, 1.5867738532334628, 1.5864682504039036, 1.5870790000813697, 1.5870540282564731, 1.5868878258327195, 1.58654990210915, 1.5874935094335976, 1.5905705363843474, 1.5877913356072115, 1.5877607378871534, 1.5867475421032131, 1.586299800236367, 1.5865646376012532, 1.5870927396006653, 1.5870528498224654, 1.5863210096496332, 1.5861312274815365, 1.5872559393211068, 1.5860119116134956, 1.5869103293154518, 1.5870532686950245, 1.5855687527196363, 1.5860566090754171, 1.586087494072728, 1.5857028611142043, 1.5857050828620394, 1.5857537439471643, 1.5856870270852435, 1.586474832176428, 1.5856245727754472, 1.5867412788912012, 1.58604705573597, 1.5859271469057463, 1.5859187001320372, 1.5854082421355669, 1.5856757551737635, 1.5858192980411852, 1.5864440076405018, 1.5853761223552163, 1.58544789202649, 1.5858010117278207, 1.585163898389687, 1.5856376359105355, 1.5851431717373263], 'acc': [0.20821355283872303, 0.2221765908556063, 0.23162217705034377, 0.23285420833794243, 0.23285421049203225, 0.2328542098861945, 0.2328542094945418, 0.2328542091028891, 0.23285420833794243, 0.2349075979949023, 0.23244353189483072, 0.23285421029620593, 0.23285420929871545, 0.23572895381852096, 0.2349075972115969, 0.23696098508776092, 0.23942505198949662, 0.24229979492433262, 0.2418891147054441, 0.24229979492433262, 0.24147843849487619, 0.23901437337393633, 0.2402464062465045, 0.23819301911692844, 0.23819301774614401, 0.23531827539878705, 0.24024640720727752, 0.23860369516838748, 0.24024640544484038, 0.24106776087680637, 0.23983572843260834, 0.23449692059101754, 0.23860369790995636, 0.23983572802259692, 0.2402464068156248, 0.23983572880590232, 0.23942505060035346, 0.24024640722563625, 0.23983572821842328, 0.2418891168962514, 0.24188911630877236, 0.24065708365038924, 0.23901437435306808, 0.2406570834545629, 0.24065708560865273, 0.2394250507961798, 0.23942505255861693, 0.24065708521700002, 0.24188911728790408, 0.23942505179367027, 0.2349075968199442, 0.24229979431849485, 0.2390143739797741, 0.23531827618209245, 0.23778234149885863, 0.23901437259063094, 0.24065708404204195, 0.23819301952693986, 0.23983572939338135, 0.23367556473068143, 0.2369609846961082, 0.24271047072488913, 0.2381930193311135, 0.23819301874363447, 0.23901437458561187, 0.2386036973224773, 0.23449691758018745, 0.23860369673499826, 0.23819301952693986, 0.23819301872527573, 0.2381930189211021, 0.23819301874363447, 0.2386036951867462, 0.24024640644233083, 0.23696098628107773, 0.23901437376558904, 0.23860369614751922, 0.2402464062465045, 0.23942505218532295, 0.24476386120187182, 0.24353182817019475, 0.23942505157948518, 0.23983572782677057, 0.24024640564066674, 0.23942505099200614, 0.23860369614751922, 0.2357289518786162, 0.24147843988401935, 0.24147843829904983, 0.24394250299161954, 0.23942505120619123, 0.24435318340633438, 0.24312115053376623, 0.24106776128681778, 0.24106776068098001, 0.24065708560865273, 0.24024640661979849, 0.24065708404204195, 0.24271047213239103, 0.2451745394074207, 0.23778234130303227, 0.24394250576990587, 0.2431211507479513, 0.24558521526305338, 0.23860369693082462, 0.2398357286284347, 0.24517453705750453, 0.24558521526305338, 0.2422997941226685, 0.2443531818213649, 0.24394250616155855, 0.24065708406040065, 0.2431211503562986, 0.24229979549345296, 0.25010267082425847, 0.247227925931159, 0.243121148930438, 0.24845995976450017, 0.25092402625622445, 0.2447638604552839, 0.25133470252186857, 0.24804928273390942, 0.24435318321050806, 0.2492813150006398, 0.25379876938688684, 0.24229979431849485, 0.25010266984512675, 0.2443531826046703, 0.25010267043260576, 0.2533880913771643, 0.25256673516189293, 0.25420944403084395, 0.2447638614344156, 0.24271047193656467, 0.24928131382568172, 0.2533880896330859, 0.25379876823028746, 0.24845995800206302, 0.2513347023076835, 0.25215605756218185, 0.25092402371048195, 0.2517453811007114, 0.25174537955245935, 0.2509240256687454, 0.24804928057981956, 0.24640657188833617, 0.2529774125841364, 0.2533880892230745, 0.2513347025402273, 0.2546201228605893, 0.2533880902205649, 0.2542094464174776, 0.25297741356326814, 0.24845995819788938, 0.25133470426594695, 0.25215605715217043, 0.2492813142173344, 0.24887063720510236, 0.24804928253808306, 0.25215605754382314, 0.25051334728572894, 0.2579055447842796, 0.2501026706284321, 0.24804928097147227, 0.24969199163957786, 0.24969199105209883, 0.2505133464840648, 0.24599589427026636, 0.24969199203123058, 0.24599589127779498, 0.24681724852727424, 0.2533880905938589, 0.2505133464840648, 0.24640657108667205, 0.2599589342454131, 0.2492813153922925, 0.24928131304237633, 0.25051334628823846, 0.2542094466133039, 0.25215605773964944, 0.25667351114676473, 0.2546201224872953, 0.2574948677536888, 0.2537987691910605, 0.25995893326628133, 0.252977411800831, 0.24476386104276293, 0.25051334589658575, 0.2542094432475386, 0.2451745374491572, 0.25503080088867053, 0.25749486873282057, 0.2550308000870064, 0.2513347019160308, 0.2599589322871496, 0.26078028791494196, 0.25379876783863475, 0.25297741434657356, 0.25872689943294014, 0.2472279271428345, 0.2529774129757891, 0.25954825564821155, 0.263655031437257, 0.2583162226165345, 0.257084190723098, 0.26078028713163653, 0.2587268976337856, 0.25708419052727166, 0.26119096709962253, 0.2554414793084045, 0.25626283352869494, 0.2529774133674418, 0.2513347027176949, 0.257494866187078, 0.26160164236777617, 0.25749486738039484, 0.25831622083573863, 0.25585215710394194, 0.25749486640126307, 0.2583162202115421, 0.2599589334621077, 0.25626283133788763, 0.2574948671294923, 0.25092402527709273, 0.252566734788599, 0.2517453775391197, 0.2505133470715439, 0.2566735103267419, 0.2558521555189724, 0.2554414804466451, 0.25872690021624556, 0.25708418837318187, 0.25420944582999855, 0.2599589326788023, 0.26119096592466445, 0.2595482548832649, 0.25585215430729685, 0.2562628327453895, 0.25503080184944354, 0.25420944582999855, 0.25708418956649864, 0.2611909647680651, 0.2558521555189724, 0.25995893169967055, 0.25872689982459285, 0.25954825564821155, 0.2574948677536888, 0.2607802888940737, 0.257905545567585, 0.25872690023460426, 0.26036961166765654, 0.25667351212589645, 0.25708418993979265, 0.25708419033144536, 0.2632443508450745, 0.2591375768551836, 0.2558521570855832, 0.2574948673620361, 0.2558521567122892, 0.25626283374288, 0.25749486638290436, 0.25954825564821155, 0.2505133470715439, 0.252566735749372, 0.2644763862817439, 0.25544147727670613, 0.26160164336526665, 0.25831622181487035, 0.2570841885690082, 0.2537987689952341, 0.2579055441968005, 0.2570841885690082, 0.2583162206399123, 0.2607802886982473, 0.2591375748602027, 0.2570841893523136, 0.2591375778526741, 0.2636550308497779, 0.25708418696568, 0.2574948671845685, 0.2595482540448833, 0.25831622181487035, 0.2570841897439663, 0.26078028634833117, 0.25462012366225345, 0.2550308000870064, 0.2595482554707439, 0.2595482550790912, 0.2587268996287665, 0.25995893111219154, 0.2579055441968005, 0.2611909629505518, 0.25913757804850046, 0.26283367776772815, 0.25338809157299064, 0.257905545194291, 0.25913757665935727, 0.25954825623569056, 0.26119096553301174, 0.25913757783431535]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
