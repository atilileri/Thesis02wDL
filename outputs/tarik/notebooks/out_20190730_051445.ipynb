{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf2.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 05:14:45 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': '0', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['01', '02', '05', '04', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001FE0220BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001FE686E6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6131, Accuracy:0.2066, Validation Loss:1.6069, Validation Accuracy:0.2069\n",
    "Epoch #2: Loss:1.6066, Accuracy:0.2181, Validation Loss:1.6040, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6032, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6029, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6044, Accuracy:0.2329, Validation Loss:1.6024, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6038, Accuracy:0.2329, Validation Loss:1.6019, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6034, Accuracy:0.2329, Validation Loss:1.6015, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6031, Accuracy:0.2329, Validation Loss:1.6012, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6030, Accuracy:0.2329, Validation Loss:1.6009, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6027, Accuracy:0.2337, Validation Loss:1.6006, Validation Accuracy:0.2463\n",
    "Epoch #11: Loss:1.6024, Accuracy:0.2386, Validation Loss:1.6004, Validation Accuracy:0.2463\n",
    "Epoch #12: Loss:1.6021, Accuracy:0.2382, Validation Loss:1.6003, Validation Accuracy:0.2463\n",
    "Epoch #13: Loss:1.6015, Accuracy:0.2386, Validation Loss:1.6003, Validation Accuracy:0.2479\n",
    "Epoch #14: Loss:1.6016, Accuracy:0.2402, Validation Loss:1.6003, Validation Accuracy:0.2479\n",
    "Epoch #15: Loss:1.6013, Accuracy:0.2402, Validation Loss:1.6000, Validation Accuracy:0.2496\n",
    "Epoch #16: Loss:1.6007, Accuracy:0.2419, Validation Loss:1.5999, Validation Accuracy:0.2479\n",
    "Epoch #17: Loss:1.6001, Accuracy:0.2398, Validation Loss:1.5998, Validation Accuracy:0.2479\n",
    "Epoch #18: Loss:1.5998, Accuracy:0.2378, Validation Loss:1.5999, Validation Accuracy:0.2414\n",
    "Epoch #19: Loss:1.5993, Accuracy:0.2390, Validation Loss:1.5998, Validation Accuracy:0.2463\n",
    "Epoch #20: Loss:1.5988, Accuracy:0.2378, Validation Loss:1.6000, Validation Accuracy:0.2447\n",
    "Epoch #21: Loss:1.5985, Accuracy:0.2382, Validation Loss:1.6000, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.5978, Accuracy:0.2374, Validation Loss:1.6001, Validation Accuracy:0.2348\n",
    "Epoch #23: Loss:1.5976, Accuracy:0.2390, Validation Loss:1.6004, Validation Accuracy:0.2332\n",
    "Epoch #24: Loss:1.5970, Accuracy:0.2386, Validation Loss:1.6001, Validation Accuracy:0.2315\n",
    "Epoch #25: Loss:1.5970, Accuracy:0.2423, Validation Loss:1.6003, Validation Accuracy:0.2299\n",
    "Epoch #26: Loss:1.5967, Accuracy:0.2402, Validation Loss:1.6001, Validation Accuracy:0.2299\n",
    "Epoch #27: Loss:1.5956, Accuracy:0.2435, Validation Loss:1.6002, Validation Accuracy:0.2315\n",
    "Epoch #28: Loss:1.5954, Accuracy:0.2407, Validation Loss:1.6000, Validation Accuracy:0.2348\n",
    "Epoch #29: Loss:1.5949, Accuracy:0.2415, Validation Loss:1.6000, Validation Accuracy:0.2299\n",
    "Epoch #30: Loss:1.5941, Accuracy:0.2480, Validation Loss:1.5997, Validation Accuracy:0.2282\n",
    "Epoch #31: Loss:1.5937, Accuracy:0.2452, Validation Loss:1.5997, Validation Accuracy:0.2348\n",
    "Epoch #32: Loss:1.5935, Accuracy:0.2579, Validation Loss:1.6000, Validation Accuracy:0.2365\n",
    "Epoch #33: Loss:1.5922, Accuracy:0.2534, Validation Loss:1.5999, Validation Accuracy:0.2381\n",
    "Epoch #34: Loss:1.5921, Accuracy:0.2575, Validation Loss:1.6009, Validation Accuracy:0.2250\n",
    "Epoch #35: Loss:1.5911, Accuracy:0.2550, Validation Loss:1.5998, Validation Accuracy:0.2430\n",
    "Epoch #36: Loss:1.5905, Accuracy:0.2559, Validation Loss:1.6003, Validation Accuracy:0.2250\n",
    "Epoch #37: Loss:1.5887, Accuracy:0.2563, Validation Loss:1.6000, Validation Accuracy:0.2332\n",
    "Epoch #38: Loss:1.5903, Accuracy:0.2600, Validation Loss:1.6038, Validation Accuracy:0.2085\n",
    "Epoch #39: Loss:1.5890, Accuracy:0.2587, Validation Loss:1.6006, Validation Accuracy:0.2381\n",
    "Epoch #40: Loss:1.5881, Accuracy:0.2563, Validation Loss:1.6002, Validation Accuracy:0.2414\n",
    "Epoch #41: Loss:1.5867, Accuracy:0.2583, Validation Loss:1.6020, Validation Accuracy:0.2184\n",
    "Epoch #42: Loss:1.5863, Accuracy:0.2595, Validation Loss:1.6028, Validation Accuracy:0.2217\n",
    "Epoch #43: Loss:1.5852, Accuracy:0.2674, Validation Loss:1.6030, Validation Accuracy:0.2299\n",
    "Epoch #44: Loss:1.5848, Accuracy:0.2632, Validation Loss:1.6033, Validation Accuracy:0.2266\n",
    "Epoch #45: Loss:1.5837, Accuracy:0.2637, Validation Loss:1.6035, Validation Accuracy:0.2266\n",
    "Epoch #46: Loss:1.5826, Accuracy:0.2645, Validation Loss:1.6034, Validation Accuracy:0.2200\n",
    "Epoch #47: Loss:1.5839, Accuracy:0.2682, Validation Loss:1.6059, Validation Accuracy:0.2135\n",
    "Epoch #48: Loss:1.5853, Accuracy:0.2571, Validation Loss:1.6022, Validation Accuracy:0.2250\n",
    "Epoch #49: Loss:1.5847, Accuracy:0.2686, Validation Loss:1.6032, Validation Accuracy:0.2266\n",
    "Epoch #50: Loss:1.5816, Accuracy:0.2719, Validation Loss:1.6057, Validation Accuracy:0.2217\n",
    "Epoch #51: Loss:1.5807, Accuracy:0.2723, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #52: Loss:1.5797, Accuracy:0.2665, Validation Loss:1.6048, Validation Accuracy:0.2233\n",
    "Epoch #53: Loss:1.5793, Accuracy:0.2686, Validation Loss:1.6031, Validation Accuracy:0.2315\n",
    "Epoch #54: Loss:1.5778, Accuracy:0.2752, Validation Loss:1.6032, Validation Accuracy:0.2250\n",
    "Epoch #55: Loss:1.5769, Accuracy:0.2719, Validation Loss:1.6053, Validation Accuracy:0.2167\n",
    "Epoch #56: Loss:1.5756, Accuracy:0.2735, Validation Loss:1.6081, Validation Accuracy:0.2217\n",
    "Epoch #57: Loss:1.5751, Accuracy:0.2813, Validation Loss:1.6061, Validation Accuracy:0.2233\n",
    "Epoch #58: Loss:1.5735, Accuracy:0.2760, Validation Loss:1.6059, Validation Accuracy:0.2266\n",
    "Epoch #59: Loss:1.5723, Accuracy:0.2867, Validation Loss:1.6071, Validation Accuracy:0.2282\n",
    "Epoch #60: Loss:1.5726, Accuracy:0.2780, Validation Loss:1.6073, Validation Accuracy:0.2167\n",
    "Epoch #61: Loss:1.5762, Accuracy:0.2784, Validation Loss:1.6142, Validation Accuracy:0.2003\n",
    "Epoch #62: Loss:1.5737, Accuracy:0.2801, Validation Loss:1.6069, Validation Accuracy:0.2167\n",
    "Epoch #63: Loss:1.5696, Accuracy:0.2805, Validation Loss:1.6076, Validation Accuracy:0.2282\n",
    "Epoch #64: Loss:1.5690, Accuracy:0.2858, Validation Loss:1.6083, Validation Accuracy:0.2184\n",
    "Epoch #65: Loss:1.5687, Accuracy:0.2801, Validation Loss:1.6111, Validation Accuracy:0.2315\n",
    "Epoch #66: Loss:1.5687, Accuracy:0.2867, Validation Loss:1.6150, Validation Accuracy:0.2053\n",
    "Epoch #67: Loss:1.5716, Accuracy:0.2813, Validation Loss:1.6114, Validation Accuracy:0.2217\n",
    "Epoch #68: Loss:1.5708, Accuracy:0.2776, Validation Loss:1.6162, Validation Accuracy:0.2151\n",
    "Epoch #69: Loss:1.5699, Accuracy:0.2871, Validation Loss:1.6097, Validation Accuracy:0.2315\n",
    "Epoch #70: Loss:1.5656, Accuracy:0.2830, Validation Loss:1.6107, Validation Accuracy:0.2151\n",
    "Epoch #71: Loss:1.5622, Accuracy:0.2875, Validation Loss:1.6132, Validation Accuracy:0.2118\n",
    "Epoch #72: Loss:1.5627, Accuracy:0.2809, Validation Loss:1.6126, Validation Accuracy:0.2233\n",
    "Epoch #73: Loss:1.5605, Accuracy:0.2912, Validation Loss:1.6173, Validation Accuracy:0.2102\n",
    "Epoch #74: Loss:1.5594, Accuracy:0.2867, Validation Loss:1.6133, Validation Accuracy:0.2217\n",
    "Epoch #75: Loss:1.5573, Accuracy:0.2936, Validation Loss:1.6158, Validation Accuracy:0.2233\n",
    "Epoch #76: Loss:1.5562, Accuracy:0.2969, Validation Loss:1.6185, Validation Accuracy:0.2102\n",
    "Epoch #77: Loss:1.5528, Accuracy:0.2928, Validation Loss:1.6198, Validation Accuracy:0.2315\n",
    "Epoch #78: Loss:1.5517, Accuracy:0.3010, Validation Loss:1.6258, Validation Accuracy:0.2085\n",
    "Epoch #79: Loss:1.5505, Accuracy:0.3043, Validation Loss:1.6198, Validation Accuracy:0.2102\n",
    "Epoch #80: Loss:1.5462, Accuracy:0.3027, Validation Loss:1.6228, Validation Accuracy:0.2365\n",
    "Epoch #81: Loss:1.5465, Accuracy:0.3002, Validation Loss:1.6214, Validation Accuracy:0.2233\n",
    "Epoch #82: Loss:1.5429, Accuracy:0.3125, Validation Loss:1.6383, Validation Accuracy:0.2085\n",
    "Epoch #83: Loss:1.5506, Accuracy:0.3047, Validation Loss:1.6266, Validation Accuracy:0.2151\n",
    "Epoch #84: Loss:1.5463, Accuracy:0.3014, Validation Loss:1.6421, Validation Accuracy:0.2003\n",
    "Epoch #85: Loss:1.5516, Accuracy:0.2994, Validation Loss:1.6238, Validation Accuracy:0.2217\n",
    "Epoch #86: Loss:1.5484, Accuracy:0.3039, Validation Loss:1.6258, Validation Accuracy:0.2332\n",
    "Epoch #87: Loss:1.5375, Accuracy:0.3092, Validation Loss:1.6325, Validation Accuracy:0.2118\n",
    "Epoch #88: Loss:1.5416, Accuracy:0.3084, Validation Loss:1.6269, Validation Accuracy:0.2250\n",
    "Epoch #89: Loss:1.5344, Accuracy:0.3162, Validation Loss:1.6291, Validation Accuracy:0.2299\n",
    "Epoch #90: Loss:1.5377, Accuracy:0.3138, Validation Loss:1.6390, Validation Accuracy:0.2118\n",
    "Epoch #91: Loss:1.5335, Accuracy:0.3125, Validation Loss:1.6316, Validation Accuracy:0.2217\n",
    "Epoch #92: Loss:1.5309, Accuracy:0.3211, Validation Loss:1.6346, Validation Accuracy:0.2102\n",
    "Epoch #93: Loss:1.5264, Accuracy:0.3248, Validation Loss:1.6406, Validation Accuracy:0.2299\n",
    "Epoch #94: Loss:1.5240, Accuracy:0.3191, Validation Loss:1.6428, Validation Accuracy:0.2135\n",
    "Epoch #95: Loss:1.5195, Accuracy:0.3232, Validation Loss:1.6461, Validation Accuracy:0.2217\n",
    "Epoch #96: Loss:1.5192, Accuracy:0.3294, Validation Loss:1.6479, Validation Accuracy:0.2184\n",
    "Epoch #97: Loss:1.5144, Accuracy:0.3281, Validation Loss:1.6597, Validation Accuracy:0.2118\n",
    "Epoch #98: Loss:1.5154, Accuracy:0.3302, Validation Loss:1.6546, Validation Accuracy:0.2233\n",
    "Epoch #99: Loss:1.5144, Accuracy:0.3368, Validation Loss:1.6543, Validation Accuracy:0.2217\n",
    "Epoch #100: Loss:1.5231, Accuracy:0.3306, Validation Loss:1.6536, Validation Accuracy:0.2299\n",
    "Epoch #101: Loss:1.5193, Accuracy:0.3314, Validation Loss:1.6657, Validation Accuracy:0.2233\n",
    "Epoch #102: Loss:1.5126, Accuracy:0.3314, Validation Loss:1.6512, Validation Accuracy:0.2332\n",
    "Epoch #103: Loss:1.5097, Accuracy:0.3458, Validation Loss:1.6549, Validation Accuracy:0.2430\n",
    "Epoch #104: Loss:1.5074, Accuracy:0.3368, Validation Loss:1.6554, Validation Accuracy:0.2447\n",
    "Epoch #105: Loss:1.5079, Accuracy:0.3446, Validation Loss:1.6813, Validation Accuracy:0.2365\n",
    "Epoch #106: Loss:1.5173, Accuracy:0.3355, Validation Loss:1.6707, Validation Accuracy:0.2315\n",
    "Epoch #107: Loss:1.5138, Accuracy:0.3294, Validation Loss:1.6594, Validation Accuracy:0.2578\n",
    "Epoch #108: Loss:1.5107, Accuracy:0.3314, Validation Loss:1.6541, Validation Accuracy:0.2397\n",
    "Epoch #109: Loss:1.5099, Accuracy:0.3363, Validation Loss:1.6823, Validation Accuracy:0.2053\n",
    "Epoch #110: Loss:1.5099, Accuracy:0.3281, Validation Loss:1.6595, Validation Accuracy:0.2644\n",
    "Epoch #111: Loss:1.5118, Accuracy:0.3253, Validation Loss:1.7051, Validation Accuracy:0.2266\n",
    "Epoch #112: Loss:1.5105, Accuracy:0.3355, Validation Loss:1.6623, Validation Accuracy:0.2397\n",
    "Epoch #113: Loss:1.5022, Accuracy:0.3417, Validation Loss:1.6655, Validation Accuracy:0.2299\n",
    "Epoch #114: Loss:1.4974, Accuracy:0.3499, Validation Loss:1.6718, Validation Accuracy:0.2348\n",
    "Epoch #115: Loss:1.4856, Accuracy:0.3499, Validation Loss:1.6720, Validation Accuracy:0.2430\n",
    "Epoch #116: Loss:1.4815, Accuracy:0.3556, Validation Loss:1.6827, Validation Accuracy:0.2381\n",
    "Epoch #117: Loss:1.4784, Accuracy:0.3606, Validation Loss:1.6888, Validation Accuracy:0.2479\n",
    "Epoch #118: Loss:1.4730, Accuracy:0.3622, Validation Loss:1.6993, Validation Accuracy:0.2430\n",
    "Epoch #119: Loss:1.4755, Accuracy:0.3602, Validation Loss:1.7093, Validation Accuracy:0.2496\n",
    "Epoch #120: Loss:1.4780, Accuracy:0.3602, Validation Loss:1.6982, Validation Accuracy:0.2496\n",
    "Epoch #121: Loss:1.4682, Accuracy:0.3630, Validation Loss:1.7068, Validation Accuracy:0.2479\n",
    "Epoch #122: Loss:1.4656, Accuracy:0.3610, Validation Loss:1.7122, Validation Accuracy:0.2463\n",
    "Epoch #123: Loss:1.4593, Accuracy:0.3647, Validation Loss:1.7169, Validation Accuracy:0.2332\n",
    "Epoch #124: Loss:1.4628, Accuracy:0.3721, Validation Loss:1.7046, Validation Accuracy:0.2611\n",
    "Epoch #125: Loss:1.4604, Accuracy:0.3713, Validation Loss:1.7099, Validation Accuracy:0.2430\n",
    "Epoch #126: Loss:1.4655, Accuracy:0.3655, Validation Loss:1.7063, Validation Accuracy:0.2594\n",
    "Epoch #127: Loss:1.4659, Accuracy:0.3634, Validation Loss:1.7394, Validation Accuracy:0.2365\n",
    "Epoch #128: Loss:1.4613, Accuracy:0.3659, Validation Loss:1.7194, Validation Accuracy:0.2512\n",
    "Epoch #129: Loss:1.4561, Accuracy:0.3749, Validation Loss:1.7169, Validation Accuracy:0.2594\n",
    "Epoch #130: Loss:1.4503, Accuracy:0.3807, Validation Loss:1.7138, Validation Accuracy:0.2529\n",
    "Epoch #131: Loss:1.4428, Accuracy:0.3770, Validation Loss:1.7276, Validation Accuracy:0.2299\n",
    "Epoch #132: Loss:1.4457, Accuracy:0.3688, Validation Loss:1.7318, Validation Accuracy:0.2512\n",
    "Epoch #133: Loss:1.4453, Accuracy:0.3786, Validation Loss:1.7424, Validation Accuracy:0.2332\n",
    "Epoch #134: Loss:1.4513, Accuracy:0.3791, Validation Loss:1.7409, Validation Accuracy:0.2233\n",
    "Epoch #135: Loss:1.4458, Accuracy:0.3873, Validation Loss:1.7426, Validation Accuracy:0.2053\n",
    "Epoch #136: Loss:1.4493, Accuracy:0.3799, Validation Loss:1.7386, Validation Accuracy:0.2414\n",
    "Epoch #137: Loss:1.4344, Accuracy:0.3951, Validation Loss:1.7541, Validation Accuracy:0.2545\n",
    "Epoch #138: Loss:1.4370, Accuracy:0.3852, Validation Loss:1.7357, Validation Accuracy:0.2479\n",
    "Epoch #139: Loss:1.4327, Accuracy:0.3844, Validation Loss:1.7643, Validation Accuracy:0.2299\n",
    "Epoch #140: Loss:1.4403, Accuracy:0.3754, Validation Loss:1.7585, Validation Accuracy:0.2200\n",
    "Epoch #141: Loss:1.4349, Accuracy:0.3840, Validation Loss:1.7410, Validation Accuracy:0.2545\n",
    "Epoch #142: Loss:1.4270, Accuracy:0.3910, Validation Loss:1.7510, Validation Accuracy:0.2512\n",
    "Epoch #143: Loss:1.4262, Accuracy:0.3922, Validation Loss:1.7493, Validation Accuracy:0.2463\n",
    "Epoch #144: Loss:1.4335, Accuracy:0.3963, Validation Loss:1.7541, Validation Accuracy:0.2529\n",
    "Epoch #145: Loss:1.4232, Accuracy:0.3819, Validation Loss:1.7670, Validation Accuracy:0.2266\n",
    "Epoch #146: Loss:1.4236, Accuracy:0.4049, Validation Loss:1.7646, Validation Accuracy:0.2512\n",
    "Epoch #147: Loss:1.4194, Accuracy:0.3869, Validation Loss:1.7653, Validation Accuracy:0.2594\n",
    "Epoch #148: Loss:1.4139, Accuracy:0.3947, Validation Loss:1.7937, Validation Accuracy:0.2463\n",
    "Epoch #149: Loss:1.4097, Accuracy:0.3971, Validation Loss:1.7775, Validation Accuracy:0.2611\n",
    "Epoch #150: Loss:1.4073, Accuracy:0.4045, Validation Loss:1.7915, Validation Accuracy:0.2397\n",
    "Epoch #151: Loss:1.4008, Accuracy:0.4148, Validation Loss:1.7816, Validation Accuracy:0.2578\n",
    "Epoch #152: Loss:1.3971, Accuracy:0.4082, Validation Loss:1.7979, Validation Accuracy:0.2496\n",
    "Epoch #153: Loss:1.4024, Accuracy:0.4127, Validation Loss:1.7926, Validation Accuracy:0.2479\n",
    "Epoch #154: Loss:1.4107, Accuracy:0.4082, Validation Loss:1.7910, Validation Accuracy:0.2578\n",
    "Epoch #155: Loss:1.3995, Accuracy:0.4148, Validation Loss:1.7903, Validation Accuracy:0.2677\n",
    "Epoch #156: Loss:1.3991, Accuracy:0.4115, Validation Loss:1.8125, Validation Accuracy:0.2135\n",
    "Epoch #157: Loss:1.4010, Accuracy:0.3988, Validation Loss:1.8172, Validation Accuracy:0.2233\n",
    "Epoch #158: Loss:1.3991, Accuracy:0.4041, Validation Loss:1.8260, Validation Accuracy:0.2512\n",
    "Epoch #159: Loss:1.4029, Accuracy:0.3955, Validation Loss:1.8482, Validation Accuracy:0.2512\n",
    "Epoch #160: Loss:1.3934, Accuracy:0.4222, Validation Loss:1.8206, Validation Accuracy:0.2447\n",
    "Epoch #161: Loss:1.3892, Accuracy:0.4123, Validation Loss:1.8152, Validation Accuracy:0.2414\n",
    "Epoch #162: Loss:1.3796, Accuracy:0.4214, Validation Loss:1.8060, Validation Accuracy:0.2545\n",
    "Epoch #163: Loss:1.3808, Accuracy:0.4189, Validation Loss:1.8286, Validation Accuracy:0.2545\n",
    "Epoch #164: Loss:1.3679, Accuracy:0.4242, Validation Loss:1.8304, Validation Accuracy:0.2430\n",
    "Epoch #165: Loss:1.3684, Accuracy:0.4292, Validation Loss:1.8343, Validation Accuracy:0.2562\n",
    "Epoch #166: Loss:1.3694, Accuracy:0.4209, Validation Loss:1.8108, Validation Accuracy:0.2397\n",
    "Epoch #167: Loss:1.3700, Accuracy:0.4181, Validation Loss:1.8424, Validation Accuracy:0.2529\n",
    "Epoch #168: Loss:1.3750, Accuracy:0.4189, Validation Loss:1.8371, Validation Accuracy:0.2414\n",
    "Epoch #169: Loss:1.3736, Accuracy:0.4177, Validation Loss:1.8257, Validation Accuracy:0.2315\n",
    "Epoch #170: Loss:1.3695, Accuracy:0.4292, Validation Loss:1.8345, Validation Accuracy:0.2545\n",
    "Epoch #171: Loss:1.3598, Accuracy:0.4312, Validation Loss:1.8315, Validation Accuracy:0.2512\n",
    "Epoch #172: Loss:1.3665, Accuracy:0.4238, Validation Loss:1.8353, Validation Accuracy:0.2529\n",
    "Epoch #173: Loss:1.3578, Accuracy:0.4300, Validation Loss:1.8492, Validation Accuracy:0.2479\n",
    "Epoch #174: Loss:1.3515, Accuracy:0.4316, Validation Loss:1.8736, Validation Accuracy:0.2578\n",
    "Epoch #175: Loss:1.3509, Accuracy:0.4431, Validation Loss:1.8749, Validation Accuracy:0.2611\n",
    "Epoch #176: Loss:1.3603, Accuracy:0.4296, Validation Loss:1.8439, Validation Accuracy:0.2266\n",
    "Epoch #177: Loss:1.3485, Accuracy:0.4312, Validation Loss:1.8625, Validation Accuracy:0.2381\n",
    "Epoch #178: Loss:1.3585, Accuracy:0.4353, Validation Loss:1.8428, Validation Accuracy:0.2447\n",
    "Epoch #179: Loss:1.3469, Accuracy:0.4419, Validation Loss:1.8404, Validation Accuracy:0.2332\n",
    "Epoch #180: Loss:1.3525, Accuracy:0.4349, Validation Loss:1.8796, Validation Accuracy:0.2545\n",
    "Epoch #181: Loss:1.3459, Accuracy:0.4402, Validation Loss:1.8501, Validation Accuracy:0.2397\n",
    "Epoch #182: Loss:1.3520, Accuracy:0.4435, Validation Loss:1.8835, Validation Accuracy:0.2430\n",
    "Epoch #183: Loss:1.3397, Accuracy:0.4402, Validation Loss:1.8836, Validation Accuracy:0.2479\n",
    "Epoch #184: Loss:1.3283, Accuracy:0.4485, Validation Loss:1.8993, Validation Accuracy:0.2414\n",
    "Epoch #185: Loss:1.3229, Accuracy:0.4522, Validation Loss:1.9051, Validation Accuracy:0.2463\n",
    "Epoch #186: Loss:1.3333, Accuracy:0.4444, Validation Loss:1.9121, Validation Accuracy:0.2463\n",
    "Epoch #187: Loss:1.3246, Accuracy:0.4530, Validation Loss:1.9117, Validation Accuracy:0.2414\n",
    "Epoch #188: Loss:1.3310, Accuracy:0.4435, Validation Loss:1.8937, Validation Accuracy:0.2414\n",
    "Epoch #189: Loss:1.3197, Accuracy:0.4583, Validation Loss:1.9084, Validation Accuracy:0.2414\n",
    "Epoch #190: Loss:1.3126, Accuracy:0.4567, Validation Loss:1.9137, Validation Accuracy:0.2381\n",
    "Epoch #191: Loss:1.3133, Accuracy:0.4616, Validation Loss:1.8945, Validation Accuracy:0.2397\n",
    "Epoch #192: Loss:1.3155, Accuracy:0.4579, Validation Loss:1.9000, Validation Accuracy:0.2299\n",
    "Epoch #193: Loss:1.3186, Accuracy:0.4567, Validation Loss:1.9277, Validation Accuracy:0.2447\n",
    "Epoch #194: Loss:1.3050, Accuracy:0.4665, Validation Loss:1.9158, Validation Accuracy:0.2479\n",
    "Epoch #195: Loss:1.3067, Accuracy:0.4616, Validation Loss:1.8996, Validation Accuracy:0.2381\n",
    "Epoch #196: Loss:1.3058, Accuracy:0.4661, Validation Loss:1.9057, Validation Accuracy:0.2463\n",
    "Epoch #197: Loss:1.3003, Accuracy:0.4649, Validation Loss:1.9317, Validation Accuracy:0.2397\n",
    "Epoch #198: Loss:1.2972, Accuracy:0.4645, Validation Loss:1.9589, Validation Accuracy:0.2332\n",
    "Epoch #199: Loss:1.3004, Accuracy:0.4563, Validation Loss:1.9107, Validation Accuracy:0.2381\n",
    "Epoch #200: Loss:1.3089, Accuracy:0.4600, Validation Loss:1.9451, Validation Accuracy:0.2299\n",
    "Epoch #201: Loss:1.3276, Accuracy:0.4423, Validation Loss:1.9426, Validation Accuracy:0.2299\n",
    "Epoch #202: Loss:1.3084, Accuracy:0.4522, Validation Loss:1.9794, Validation Accuracy:0.2381\n",
    "Epoch #203: Loss:1.2948, Accuracy:0.4715, Validation Loss:2.0109, Validation Accuracy:0.2479\n",
    "Epoch #204: Loss:1.2994, Accuracy:0.4595, Validation Loss:1.9386, Validation Accuracy:0.2381\n",
    "Epoch #205: Loss:1.2787, Accuracy:0.4813, Validation Loss:1.9365, Validation Accuracy:0.2250\n",
    "Epoch #206: Loss:1.3066, Accuracy:0.4612, Validation Loss:1.9288, Validation Accuracy:0.2463\n",
    "Epoch #207: Loss:1.3008, Accuracy:0.4608, Validation Loss:1.9458, Validation Accuracy:0.2381\n",
    "Epoch #208: Loss:1.3053, Accuracy:0.4579, Validation Loss:1.9418, Validation Accuracy:0.2365\n",
    "Epoch #209: Loss:1.2853, Accuracy:0.4743, Validation Loss:1.9578, Validation Accuracy:0.2414\n",
    "Epoch #210: Loss:1.2840, Accuracy:0.4624, Validation Loss:1.9524, Validation Accuracy:0.2151\n",
    "Epoch #211: Loss:1.2957, Accuracy:0.4682, Validation Loss:1.9434, Validation Accuracy:0.2479\n",
    "Epoch #212: Loss:1.2839, Accuracy:0.4760, Validation Loss:1.9324, Validation Accuracy:0.2430\n",
    "Epoch #213: Loss:1.2871, Accuracy:0.4702, Validation Loss:1.9609, Validation Accuracy:0.2397\n",
    "Epoch #214: Loss:1.2759, Accuracy:0.4723, Validation Loss:1.9723, Validation Accuracy:0.2332\n",
    "Epoch #215: Loss:1.2666, Accuracy:0.4793, Validation Loss:1.9999, Validation Accuracy:0.2250\n",
    "Epoch #216: Loss:1.2689, Accuracy:0.4752, Validation Loss:1.9812, Validation Accuracy:0.2332\n",
    "Epoch #217: Loss:1.2661, Accuracy:0.4867, Validation Loss:1.9623, Validation Accuracy:0.2447\n",
    "Epoch #218: Loss:1.2623, Accuracy:0.4936, Validation Loss:1.9947, Validation Accuracy:0.2266\n",
    "Epoch #219: Loss:1.2677, Accuracy:0.4834, Validation Loss:2.0726, Validation Accuracy:0.2332\n",
    "Epoch #220: Loss:1.2814, Accuracy:0.4628, Validation Loss:2.0583, Validation Accuracy:0.2365\n",
    "Epoch #221: Loss:1.3015, Accuracy:0.4632, Validation Loss:2.0348, Validation Accuracy:0.2414\n",
    "Epoch #222: Loss:1.2871, Accuracy:0.4686, Validation Loss:2.0318, Validation Accuracy:0.2397\n",
    "Epoch #223: Loss:1.2651, Accuracy:0.4768, Validation Loss:2.0064, Validation Accuracy:0.2463\n",
    "Epoch #224: Loss:1.2639, Accuracy:0.4789, Validation Loss:2.0163, Validation Accuracy:0.2332\n",
    "Epoch #225: Loss:1.2583, Accuracy:0.4764, Validation Loss:2.0188, Validation Accuracy:0.2397\n",
    "Epoch #226: Loss:1.2471, Accuracy:0.4945, Validation Loss:2.0488, Validation Accuracy:0.2381\n",
    "Epoch #227: Loss:1.2468, Accuracy:0.4916, Validation Loss:2.0280, Validation Accuracy:0.2348\n",
    "Epoch #228: Loss:1.2408, Accuracy:0.4957, Validation Loss:2.0303, Validation Accuracy:0.2348\n",
    "Epoch #229: Loss:1.2330, Accuracy:0.5023, Validation Loss:2.0160, Validation Accuracy:0.2233\n",
    "Epoch #230: Loss:1.2300, Accuracy:0.5018, Validation Loss:2.0201, Validation Accuracy:0.2332\n",
    "Epoch #231: Loss:1.2340, Accuracy:0.4982, Validation Loss:2.0289, Validation Accuracy:0.2282\n",
    "Epoch #232: Loss:1.2329, Accuracy:0.5035, Validation Loss:2.0720, Validation Accuracy:0.2233\n",
    "Epoch #233: Loss:1.2542, Accuracy:0.4858, Validation Loss:2.0395, Validation Accuracy:0.2233\n",
    "Epoch #234: Loss:1.2393, Accuracy:0.4883, Validation Loss:2.0389, Validation Accuracy:0.2299\n",
    "Epoch #235: Loss:1.2346, Accuracy:0.4965, Validation Loss:2.0703, Validation Accuracy:0.2315\n",
    "Epoch #236: Loss:1.2535, Accuracy:0.4789, Validation Loss:2.0344, Validation Accuracy:0.2266\n",
    "Epoch #237: Loss:1.2568, Accuracy:0.4862, Validation Loss:2.0402, Validation Accuracy:0.2233\n",
    "Epoch #238: Loss:1.2348, Accuracy:0.4932, Validation Loss:2.0674, Validation Accuracy:0.2282\n",
    "Epoch #239: Loss:1.2333, Accuracy:0.5018, Validation Loss:2.0346, Validation Accuracy:0.2381\n",
    "Epoch #240: Loss:1.2271, Accuracy:0.4990, Validation Loss:2.0468, Validation Accuracy:0.2250\n",
    "Epoch #241: Loss:1.2247, Accuracy:0.5039, Validation Loss:2.0111, Validation Accuracy:0.2447\n",
    "Epoch #242: Loss:1.2266, Accuracy:0.4998, Validation Loss:2.0838, Validation Accuracy:0.2233\n",
    "Epoch #243: Loss:1.2117, Accuracy:0.5121, Validation Loss:2.0430, Validation Accuracy:0.2348\n",
    "Epoch #244: Loss:1.2088, Accuracy:0.5138, Validation Loss:2.1045, Validation Accuracy:0.2332\n",
    "Epoch #245: Loss:1.2072, Accuracy:0.5051, Validation Loss:2.0401, Validation Accuracy:0.2299\n",
    "Epoch #246: Loss:1.2166, Accuracy:0.5072, Validation Loss:2.0926, Validation Accuracy:0.2315\n",
    "Epoch #247: Loss:1.2067, Accuracy:0.5150, Validation Loss:2.0723, Validation Accuracy:0.2250\n",
    "Epoch #248: Loss:1.2158, Accuracy:0.5039, Validation Loss:2.1315, Validation Accuracy:0.2332\n",
    "Epoch #249: Loss:1.2042, Accuracy:0.5166, Validation Loss:2.0725, Validation Accuracy:0.2299\n",
    "Epoch #250: Loss:1.2010, Accuracy:0.5162, Validation Loss:2.0932, Validation Accuracy:0.2266\n",
    "Epoch #251: Loss:1.2069, Accuracy:0.5183, Validation Loss:2.1249, Validation Accuracy:0.2167\n",
    "Epoch #252: Loss:1.1932, Accuracy:0.5240, Validation Loss:2.0997, Validation Accuracy:0.2233\n",
    "Epoch #253: Loss:1.1866, Accuracy:0.5158, Validation Loss:2.1085, Validation Accuracy:0.2315\n",
    "Epoch #254: Loss:1.1881, Accuracy:0.5244, Validation Loss:2.0920, Validation Accuracy:0.2332\n",
    "Epoch #255: Loss:1.1889, Accuracy:0.5187, Validation Loss:2.1379, Validation Accuracy:0.2282\n",
    "Epoch #256: Loss:1.1804, Accuracy:0.5265, Validation Loss:2.1259, Validation Accuracy:0.2299\n",
    "Epoch #257: Loss:1.1870, Accuracy:0.5203, Validation Loss:2.1133, Validation Accuracy:0.2282\n",
    "Epoch #258: Loss:1.1791, Accuracy:0.5240, Validation Loss:2.1097, Validation Accuracy:0.2282\n",
    "Epoch #259: Loss:1.1779, Accuracy:0.5232, Validation Loss:2.1181, Validation Accuracy:0.2118\n",
    "Epoch #260: Loss:1.2052, Accuracy:0.5088, Validation Loss:2.1604, Validation Accuracy:0.2233\n",
    "Epoch #261: Loss:1.1985, Accuracy:0.5088, Validation Loss:2.1898, Validation Accuracy:0.2299\n",
    "Epoch #262: Loss:1.1891, Accuracy:0.5154, Validation Loss:2.1447, Validation Accuracy:0.2266\n",
    "Epoch #263: Loss:1.1825, Accuracy:0.5244, Validation Loss:2.1034, Validation Accuracy:0.2299\n",
    "Epoch #264: Loss:1.1742, Accuracy:0.5191, Validation Loss:2.1229, Validation Accuracy:0.2250\n",
    "Epoch #265: Loss:1.1681, Accuracy:0.5298, Validation Loss:2.1530, Validation Accuracy:0.2151\n",
    "Epoch #266: Loss:1.1794, Accuracy:0.5232, Validation Loss:2.1486, Validation Accuracy:0.2282\n",
    "Epoch #267: Loss:1.1779, Accuracy:0.5154, Validation Loss:2.1614, Validation Accuracy:0.2200\n",
    "Epoch #268: Loss:1.1681, Accuracy:0.5331, Validation Loss:2.1127, Validation Accuracy:0.2266\n",
    "Epoch #269: Loss:1.1671, Accuracy:0.5318, Validation Loss:2.1585, Validation Accuracy:0.2332\n",
    "Epoch #270: Loss:1.1579, Accuracy:0.5376, Validation Loss:2.1977, Validation Accuracy:0.2135\n",
    "Epoch #271: Loss:1.1548, Accuracy:0.5400, Validation Loss:2.1732, Validation Accuracy:0.2250\n",
    "Epoch #272: Loss:1.1596, Accuracy:0.5355, Validation Loss:2.2007, Validation Accuracy:0.2135\n",
    "Epoch #273: Loss:1.1554, Accuracy:0.5347, Validation Loss:2.2121, Validation Accuracy:0.2167\n",
    "Epoch #274: Loss:1.1571, Accuracy:0.5409, Validation Loss:2.2087, Validation Accuracy:0.2085\n",
    "Epoch #275: Loss:1.1537, Accuracy:0.5372, Validation Loss:2.1913, Validation Accuracy:0.2266\n",
    "Epoch #276: Loss:1.1474, Accuracy:0.5335, Validation Loss:2.1519, Validation Accuracy:0.2151\n",
    "Epoch #277: Loss:1.1569, Accuracy:0.5318, Validation Loss:2.1656, Validation Accuracy:0.2233\n",
    "Epoch #278: Loss:1.1480, Accuracy:0.5372, Validation Loss:2.2129, Validation Accuracy:0.2282\n",
    "Epoch #279: Loss:1.1386, Accuracy:0.5474, Validation Loss:2.2076, Validation Accuracy:0.2217\n",
    "Epoch #280: Loss:1.1346, Accuracy:0.5462, Validation Loss:2.2169, Validation Accuracy:0.2266\n",
    "Epoch #281: Loss:1.1291, Accuracy:0.5503, Validation Loss:2.2171, Validation Accuracy:0.2233\n",
    "Epoch #282: Loss:1.1286, Accuracy:0.5524, Validation Loss:2.1861, Validation Accuracy:0.2381\n",
    "Epoch #283: Loss:1.1317, Accuracy:0.5433, Validation Loss:2.2289, Validation Accuracy:0.2102\n",
    "Epoch #284: Loss:1.1279, Accuracy:0.5515, Validation Loss:2.2234, Validation Accuracy:0.2217\n",
    "Epoch #285: Loss:1.1240, Accuracy:0.5437, Validation Loss:2.2020, Validation Accuracy:0.2200\n",
    "Epoch #286: Loss:1.1220, Accuracy:0.5569, Validation Loss:2.2530, Validation Accuracy:0.2282\n",
    "Epoch #287: Loss:1.1292, Accuracy:0.5454, Validation Loss:2.2331, Validation Accuracy:0.2299\n",
    "Epoch #288: Loss:1.1275, Accuracy:0.5515, Validation Loss:2.2325, Validation Accuracy:0.2200\n",
    "Epoch #289: Loss:1.1299, Accuracy:0.5483, Validation Loss:2.2844, Validation Accuracy:0.2463\n",
    "Epoch #290: Loss:1.1336, Accuracy:0.5483, Validation Loss:2.2274, Validation Accuracy:0.2184\n",
    "Epoch #291: Loss:1.1155, Accuracy:0.5573, Validation Loss:2.2372, Validation Accuracy:0.2167\n",
    "Epoch #292: Loss:1.1251, Accuracy:0.5433, Validation Loss:2.2952, Validation Accuracy:0.2233\n",
    "Epoch #293: Loss:1.1231, Accuracy:0.5491, Validation Loss:2.2645, Validation Accuracy:0.2167\n",
    "Epoch #294: Loss:1.1034, Accuracy:0.5626, Validation Loss:2.2100, Validation Accuracy:0.2233\n",
    "Epoch #295: Loss:1.1084, Accuracy:0.5602, Validation Loss:2.2465, Validation Accuracy:0.2184\n",
    "Epoch #296: Loss:1.1037, Accuracy:0.5622, Validation Loss:2.3022, Validation Accuracy:0.2315\n",
    "Epoch #297: Loss:1.1020, Accuracy:0.5581, Validation Loss:2.2694, Validation Accuracy:0.2299\n",
    "Epoch #298: Loss:1.1136, Accuracy:0.5593, Validation Loss:2.2443, Validation Accuracy:0.2315\n",
    "Epoch #299: Loss:1.1010, Accuracy:0.5667, Validation Loss:2.2728, Validation Accuracy:0.2167\n",
    "Epoch #300: Loss:1.0980, Accuracy:0.5647, Validation Loss:2.3122, Validation Accuracy:0.2118\n",
    "\n",
    "Test:\n",
    "Test Loss:2.31216526, Accuracy:0.2118\n",
    "Labels: ['01', '02', '05', '04', '03']\n",
    "Confusion Matrix:\n",
    "      01  02  05  04  03\n",
    "t:01  32  14  33  29  18\n",
    "t:02  25  16  29  21  23\n",
    "t:05  29  14  34  30  35\n",
    "t:04  21   9  29  32  21\n",
    "t:03  30  15  29  26  15\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.23      0.25      0.24       126\n",
    "          02       0.24      0.14      0.18       114\n",
    "          05       0.22      0.24      0.23       142\n",
    "          04       0.23      0.29      0.26       112\n",
    "          03       0.13      0.13      0.13       115\n",
    "\n",
    "    accuracy                           0.21       609\n",
    "   macro avg       0.21      0.21      0.21       609\n",
    "weighted avg       0.21      0.21      0.21       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 05:30:22 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 37 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6069216469825782, 1.6040077321047854, 1.6032224216288926, 1.6028573237029202, 1.6023540776742895, 1.6018950179684142, 1.6015094250489534, 1.6012404816491264, 1.6009241847569131, 1.6006470008436682, 1.600437951792637, 1.6003437840879844, 1.6002625906212968, 1.6003316556683118, 1.600041877068518, 1.5998839347429072, 1.5997742995839987, 1.5998582513070068, 1.5998476962933594, 1.5999857647274123, 1.6000031506878205, 1.6001095368552873, 1.6004428796971764, 1.6001175714439555, 1.6002794314292068, 1.6001296687400204, 1.600192056305107, 1.6000143890506138, 1.5999557114587042, 1.5997392371761778, 1.5997103298043187, 1.599970925226196, 1.5998931990076952, 1.6009125537277247, 1.599834095081085, 1.600324610183979, 1.5999865573028038, 1.603811960697957, 1.600561763461196, 1.6002074023968675, 1.6019572722305022, 1.602779419747088, 1.6029577437292766, 1.6032638473463763, 1.6034693755148275, 1.6034137043851153, 1.6059440765866309, 1.602231087551524, 1.6031892266375287, 1.6057160714968477, 1.605423429133661, 1.6048208978180032, 1.6031407667889774, 1.6032340254493926, 1.6053004542790805, 1.6080597343507463, 1.6061111001545572, 1.6059412259382175, 1.6071151377532282, 1.607273529120071, 1.614215164544743, 1.6068717750226726, 1.6075703627957498, 1.6083387647356306, 1.611058400769539, 1.6149669171162622, 1.6114095578639966, 1.6161988676083694, 1.6096612896238054, 1.6106892323063315, 1.6132186334121403, 1.612591681809261, 1.6172529513808502, 1.6132662611446162, 1.6158473129538675, 1.6184959728729549, 1.619840305231279, 1.625798522936691, 1.6198382031154164, 1.6228370879867002, 1.6214391132097918, 1.638318082969177, 1.6266106140045893, 1.6421207095601875, 1.6237511174823656, 1.6257561447193665, 1.6325462686604466, 1.626869939426679, 1.6291405134796118, 1.6389979699562336, 1.6315601273319014, 1.63457885379666, 1.640616652609288, 1.6428257798522172, 1.6461117580802178, 1.6478817147770146, 1.6596954054824629, 1.6546073212412191, 1.6543000317950947, 1.6536096442117676, 1.6657007237764807, 1.6512483411234589, 1.654898835911931, 1.6554281355320721, 1.6812810234248345, 1.6707469551825562, 1.6594194207089679, 1.6541481468282113, 1.6822652646473475, 1.659532377676815, 1.7050724949547027, 1.6622669741829432, 1.6654802952298193, 1.6717628097690775, 1.671995949275388, 1.682744694656535, 1.6888395318844047, 1.699306703944903, 1.7093295336552636, 1.6982374551456745, 1.7067662810261417, 1.7122334230122307, 1.7168831331976528, 1.7046095446021294, 1.709944266795329, 1.7062921095364199, 1.7393693632288716, 1.719433333290426, 1.7169142176560777, 1.7138069850470632, 1.7275833921087982, 1.731770430292402, 1.742397363354224, 1.7409441754931496, 1.7426337432391539, 1.7386446504170083, 1.754102583393479, 1.7357140673792422, 1.7642984562515234, 1.7585484766216308, 1.7409842349038336, 1.7509618977999257, 1.7492793984405317, 1.754108302698934, 1.7669975851556938, 1.764555315078773, 1.7653346819243407, 1.7937040315277275, 1.777481096327207, 1.791485558981183, 1.7816213230389875, 1.7978680542928636, 1.7926249960177443, 1.7909504529486344, 1.7903423916138648, 1.8125147633560381, 1.8172082270698986, 1.8260021170567604, 1.8482427514832596, 1.8206320503858118, 1.8152239077979904, 1.8059673054856424, 1.8285588785541078, 1.8304261294100281, 1.834291671884471, 1.8107968703866593, 1.842388128607927, 1.8371185494956908, 1.8256972233454387, 1.8345300133396643, 1.8314589626096152, 1.835292540356052, 1.8492242706624549, 1.87359431929189, 1.8749033817516758, 1.843918941682587, 1.8624596411762957, 1.8428246669581372, 1.8404326055241726, 1.8795875774815751, 1.8501339894405922, 1.8834782738990972, 1.8836437669293633, 1.8992776835493266, 1.905074753001797, 1.9120933069971395, 1.9117446157145384, 1.8937032418493762, 1.9083871798366552, 1.9137075722511179, 1.8944943068649969, 1.900037545484471, 1.9277149503454198, 1.9157826430691873, 1.8996449918386775, 1.9056714356239206, 1.9316523259104963, 1.9588774992718876, 1.910694737348259, 1.9451416674114408, 1.9425919717560065, 1.9793917635587244, 2.010854389084188, 1.9386179713388578, 1.9365133119529887, 1.9287996765819482, 1.9458459703793078, 1.941789818906236, 1.957834922229911, 1.9524397979228956, 1.943393393298871, 1.9323608374165, 1.9609161116219507, 1.9723106201842109, 1.9999233975590547, 1.9811948144377158, 1.962330065337308, 1.9946639956707632, 2.0726147050340775, 2.0582527785465636, 2.0348257305978357, 2.0317503525118523, 2.0064036200199222, 2.0163229529689293, 2.0187808007050814, 2.04876731416862, 2.0279709611620222, 2.0302837435247865, 2.0160015451497046, 2.020053227742513, 2.028933785036084, 2.072026083230581, 2.0394824193224728, 2.038917063883764, 2.070258599001003, 2.0343878140003224, 2.0401701050243157, 2.067389072455796, 2.0346089732666517, 2.0468110760248743, 2.011061370470645, 2.0837962071492364, 2.042968688144277, 2.1044644941445836, 2.0400522142795507, 2.0925530929283553, 2.072281771692736, 2.1314969215487025, 2.0724586806273813, 2.0931964887578305, 2.1248711788008365, 2.099703006164977, 2.108513158726183, 2.0920411549961235, 2.137855862553288, 2.125896346588636, 2.1132988264212273, 2.109732448956845, 2.118074977339195, 2.1604337289024067, 2.1897662985696775, 2.1447173989269337, 2.1033940201713923, 2.122899767213267, 2.152982249440035, 2.1485584518200853, 2.161414516383204, 2.1126570286617685, 2.158546800488126, 2.197698901635281, 2.173237866760279, 2.2006981599898565, 2.2121228322215463, 2.208689740921672, 2.1913074464437803, 2.151853874203411, 2.165555609075111, 2.2128620844560696, 2.2076363132896484, 2.216915726857428, 2.2170900636901605, 2.186120525760995, 2.228906421630058, 2.2233511986599375, 2.2020021126971066, 2.2530122574522773, 2.2331245477955135, 2.232468936243668, 2.284388167517526, 2.227432624459854, 2.2372355598142777, 2.2952008051629527, 2.2644930950722277, 2.2099913117920824, 2.246542434582765, 2.302165207604469, 2.2693609807683135, 2.244308826762859, 2.272802096869558, 2.312165603653355], 'val_acc': [0.20689655101455884, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.23316912969638562, 0.24630541651706978, 0.24630541651706978, 0.24630541651706978, 0.24794745264186452, 0.24794745254399153, 0.24958948876665926, 0.2479474548195383, 0.2479474548195383, 0.24137931032035934, 0.24630541869474357, 0.24466338256994882, 0.23316912940276668, 0.23481116582118036, 0.23316912742083884, 0.23152709357159088, 0.22988505725105016, 0.22988505725105016, 0.2315270933758449, 0.23481116354563358, 0.22988505725105016, 0.2282430212241284, 0.23481116354563358, 0.23645320184810212, 0.23809523599096902, 0.22495894877879294, 0.24302134436535328, 0.2249589489745389, 0.2331691275187118, 0.2085385875308455, 0.23809523807076985, 0.2413793082405585, 0.2183908045732329, 0.22167487672494943, 0.2298850553669953, 0.22660098509933366, 0.22660098509933366, 0.22003284069802764, 0.21346469600310272, 0.22495894887666593, 0.22660098519720664, 0.22167487672494943, 0.23316912969638562, 0.22331691284974417, 0.23152709357159088, 0.22495894887666593, 0.21674876844843816, 0.22167487662707644, 0.22331691067207035, 0.22660098500146067, 0.2282430211262554, 0.2167487682526922, 0.20032840700474475, 0.21674876617289138, 0.22824302102838243, 0.21839080437748695, 0.2315270912960441, 0.20525451518338303, 0.22167487662707644, 0.21510673203002448, 0.23152709357159088, 0.21510673212789747, 0.211822659780435, 0.22331691076994334, 0.21018062365564025, 0.22167487662707644, 0.22331691294761713, 0.21018062355776726, 0.2315270912960441, 0.20853858733509953, 0.21018062355776726, 0.23645319976830131, 0.22331691294761713, 0.20853858733509953, 0.21510673203002448, 0.2003284067111258, 0.2216748768228224, 0.2331691275187118, 0.21182265958468902, 0.2249589489745389, 0.2298850552691223, 0.21182265938894307, 0.22167487672494943, 0.21018062365564025, 0.22988505517124935, 0.21346469590522973, 0.22167487672494943, 0.21839080398599503, 0.21182265929107008, 0.22331691245825225, 0.22167487613771153, 0.22988505734892314, 0.2233169125561252, 0.23316912969638562, 0.2430213441696073, 0.24466338039227503, 0.23645320165235617, 0.23152709288648002, 0.257799669390633, 0.23973727211576376, 0.20525451469401812, 0.2643678141834309, 0.22660098490358768, 0.2397372741955646, 0.22988505734892314, 0.2348111656254344, 0.2430213459557892, 0.23809523758140494, 0.24794745433017343, 0.24302134624940813, 0.24958948886453225, 0.2495894907485871, 0.24794745462379236, 0.2463054185968706, 0.2331691275187118, 0.2610837415423495, 0.24302134436535328, 0.2594417055154277, 0.23645319967042833, 0.25123152697125484, 0.25944170769310154, 0.25287356091837576, 0.2298850570553042, 0.25123152697125484, 0.23316912959851263, 0.22331691076994334, 0.20525451518338303, 0.24137930804481256, 0.25451559733678947, 0.24794745472166535, 0.22988505695743122, 0.22003284011078977, 0.2545155970431705, 0.25123152697125484, 0.2463054185968706, 0.2528735630960496, 0.22660098490358768, 0.251231524891454, 0.25944170541755474, 0.24630541830325164, 0.2610837437200233, 0.23973727380407267, 0.2577996714704338, 0.24958949065071412, 0.24794745452591938, 0.2577996713725608, 0.26765188831707526, 0.21346469541586482, 0.2233169121646333, 0.251231524891454, 0.25123152697125484, 0.2446633822763299, 0.24137930983099445, 0.25451559902509835, 0.25451559912297134, 0.2430213458579162, 0.2561576351498931, 0.23973727390194566, 0.2528735629981766, 0.24137930983099445, 0.23152709327797194, 0.25451559902509835, 0.251231524793581, 0.2528735630960496, 0.24794745442804642, 0.2577996710789419, 0.2610837436221503, 0.22660098460996875, 0.23809523777715091, 0.2446633822763299, 0.2331691292070207, 0.25451559892722536, 0.23973727399981865, 0.2430213458579162, 0.24794745462379236, 0.24137930983099445, 0.24630541830325164, 0.2463054181075057, 0.24137930973312147, 0.24137930973312147, 0.24137930973312147, 0.23809523748353198, 0.23973727380407267, 0.22988505685955823, 0.2446633821784569, 0.24794745442804642, 0.2380952378750239, 0.24630541830325164, 0.23973727351045374, 0.2331691293048937, 0.23809523767927793, 0.22988505685955823, 0.2298850570553042, 0.238095237385659, 0.24794745442804642, 0.23809523758140494, 0.224958948583047, 0.24630541820537868, 0.23809523758140494, 0.23645320126086425, 0.24137930983099445, 0.2151067319321515, 0.24794745433017343, 0.24302134615153514, 0.23973727370619968, 0.23316912910914772, 0.224958948485174, 0.23316912940276668, 0.24466338208058394, 0.22660098470784173, 0.23316912891340177, 0.2364532014566102, 0.24137930973312147, 0.23973727351045374, 0.24630541830325164, 0.23316912891340177, 0.23973727341258075, 0.238095237287786, 0.2348111651360695, 0.2348111652339425, 0.2233169125561252, 0.23316912940276668, 0.22824302083263648, 0.22331691226250627, 0.22331691236037926, 0.22988505695743122, 0.23152709288648002, 0.22660098470784173, 0.22331691245825225, 0.22824302063689053, 0.23809523748353198, 0.22495894868091998, 0.24466338188483797, 0.22331691226250627, 0.2348111652339425, 0.23316912901127476, 0.22988505695743122, 0.23152709308222597, 0.224958948485174, 0.23316912901127476, 0.22988505695743122, 0.22660098460996875, 0.2167487677633273, 0.22331691236037926, 0.23152709308222597, 0.23316912901127476, 0.22824302083263648, 0.22988505685955823, 0.2282430207347635, 0.22824302083263648, 0.211822659780435, 0.22331691236037926, 0.2298850566638123, 0.22660098470784173, 0.22988505685955823, 0.224958948583047, 0.21510673154065957, 0.22824302063689053, 0.22003284020866276, 0.22660098451209576, 0.2331691292070207, 0.21346469541586482, 0.224958948485174, 0.21346469531799184, 0.2167487677633273, 0.2085385870414806, 0.2266009844142228, 0.21510673144278658, 0.2233169121646333, 0.22824302063689053, 0.22167487603983857, 0.2266009844142228, 0.22331691226250627, 0.23809523748353198, 0.21018062316627534, 0.22167487603983857, 0.2200328399150438, 0.22824302063689053, 0.22988505676168527, 0.22003284011078977, 0.2463054181075057, 0.21839080408386802, 0.2167487677633273, 0.22331691187101435, 0.21674876756758135, 0.2233169121646333, 0.21839080398599503, 0.23152709278860703, 0.22988505676168527, 0.23152709288648002, 0.2167487677633273, 0.21182265899745115], 'loss': [1.6131409809085133, 1.6065625274450628, 1.6047046882660727, 1.6042141809111015, 1.6044455479792257, 1.6038396958697747, 1.6033644984634994, 1.6031431033136418, 1.602985939206039, 1.6027299679280307, 1.602404535575569, 1.6020629363872676, 1.6015472329372742, 1.6015666545538931, 1.6012523897864246, 1.6006815297892452, 1.6001234936273563, 1.599773600458854, 1.5993315262471381, 1.5988099276407544, 1.59849529731445, 1.5978028351276563, 1.597578411817061, 1.597045175789318, 1.5969748356993438, 1.5967358072190803, 1.5955738878348036, 1.5953677217084035, 1.594938615655997, 1.5941144350862602, 1.5936641956501674, 1.593524668104105, 1.5921862520965953, 1.5921111033437678, 1.5910867406602267, 1.5905430153410047, 1.5887196087004958, 1.5902672291781135, 1.5890335982340318, 1.5881336077527588, 1.5867119509581422, 1.5863016560826704, 1.5852334744631633, 1.5848462414202993, 1.583672037898148, 1.5825851096754446, 1.5838523012411914, 1.5853200773928444, 1.5846824902039045, 1.5816331624005608, 1.580735929350099, 1.5797165735056757, 1.5793177793158153, 1.5777884969476312, 1.5768613376656597, 1.5756092507736394, 1.575089575818432, 1.5734705593551699, 1.5723361124003448, 1.57258283754149, 1.5762294599407751, 1.5737204427836613, 1.5695610217245208, 1.5690183287528505, 1.5687282721364768, 1.5686839861057134, 1.571643266198082, 1.5708339659829893, 1.5698972340726753, 1.5656421505695006, 1.5621877734421215, 1.5626920511100817, 1.5605267762648252, 1.55941937229227, 1.5572977195774995, 1.556166604656948, 1.5527957110434343, 1.5517462829789586, 1.5504966499869095, 1.5462296506462645, 1.546520964663621, 1.542862394996737, 1.550641251393657, 1.5463411770317343, 1.55159848549038, 1.5483781925706648, 1.5375470986846047, 1.5416404146433367, 1.534352187601203, 1.5377175198443371, 1.53354963605164, 1.5309368218729384, 1.5263569594898263, 1.5240326024178852, 1.5195468194186075, 1.5192330709473063, 1.51444251953454, 1.515434490826585, 1.5143647953714923, 1.5230530169955026, 1.5192658945765094, 1.5125888770121079, 1.5097009355282636, 1.5073723202613345, 1.5078616335161903, 1.5173343791609182, 1.5137770851045174, 1.5106520548003899, 1.5099408371982144, 1.509903229237582, 1.5118436608715957, 1.510516863926725, 1.5022306989595386, 1.4974296305458648, 1.4855723343835474, 1.48145029535039, 1.4784291247812384, 1.4730091352971917, 1.4755275571125979, 1.4779667578683497, 1.4681966536833275, 1.4656440799975543, 1.4592705172924536, 1.4628464550453044, 1.4603644154155033, 1.46554120570972, 1.4659042526809096, 1.46132356861044, 1.4560802832276425, 1.4502703916365607, 1.442759041913481, 1.4456526686768267, 1.4452751358431712, 1.4513447628373728, 1.4458323940359348, 1.449344532396759, 1.434445228419999, 1.4369986397040209, 1.4327250417497859, 1.4403123773343753, 1.4349227900867345, 1.4269618508262556, 1.4261750117954042, 1.433451582518936, 1.4231886137192744, 1.4235710966513633, 1.4193778485488109, 1.4139213254564351, 1.4096621266135934, 1.4073111637906617, 1.400771067323626, 1.3971160631160227, 1.4023588630453028, 1.4106654360064246, 1.3994857338664468, 1.3990920739986568, 1.4010456454337745, 1.3990640974142714, 1.402936196278253, 1.3933934578415794, 1.389192335023038, 1.37963193167896, 1.3807971377637107, 1.3679311356750112, 1.3683930156167283, 1.3694460797848398, 1.3700366327650004, 1.3749797758870057, 1.3736131073268287, 1.3695309482805538, 1.3598401402545905, 1.3665187188976844, 1.3577873850749993, 1.3515158537232166, 1.350854249118045, 1.3603017763435474, 1.3485089422496193, 1.3584880533649202, 1.3469470208185654, 1.3524950520458652, 1.3458665273517554, 1.3520010697523426, 1.3397014184654126, 1.32831762419589, 1.3229240188853206, 1.3333167490283566, 1.3246153205327185, 1.3310193743304306, 1.3197002135263087, 1.3126163334817122, 1.3132740195526968, 1.3154665702667079, 1.3186393546862278, 1.3049544930213286, 1.3067446666086968, 1.3057837946459008, 1.3003494384107648, 1.2971571149767303, 1.3004092508272957, 1.3089041055839898, 1.3276496434848166, 1.3084048371540202, 1.2948289278841116, 1.299401389744737, 1.2787133655019365, 1.3065748459504616, 1.3008371396720777, 1.3053386169782164, 1.2853023495272689, 1.2840423081689791, 1.2956856881813348, 1.283860759323872, 1.2871290622061038, 1.2759154456352062, 1.2666016401451472, 1.2689300668802594, 1.2660643421893736, 1.2623066314192033, 1.2676765882503815, 1.2813856697180432, 1.3015228886868675, 1.287068468442443, 1.2650521980418807, 1.2638626084435403, 1.2582784038304793, 1.2470880625429095, 1.2467590489181894, 1.240827338896248, 1.2330214448043697, 1.2299661047404797, 1.2340345655868186, 1.232870775473436, 1.254188839121276, 1.239256845264709, 1.234600868509046, 1.253489046811568, 1.2567541168455716, 1.2347748032340768, 1.2333055661689085, 1.227113135051923, 1.224703141649156, 1.2265680437949649, 1.2116580809411082, 1.2087686383014342, 1.207244300744372, 1.216561419714158, 1.2066834998571407, 1.215760204385683, 1.204156247888992, 1.2010120129927961, 1.206877451804629, 1.1931999850322088, 1.1866174393366005, 1.1880620323412228, 1.1888571412656341, 1.1803879563568553, 1.1869519415332552, 1.1790908605411068, 1.177880516238281, 1.2052256394215923, 1.1985364204559483, 1.1891236392869107, 1.1824955725327164, 1.174219979985294, 1.1681320493960528, 1.179407407224056, 1.1778990111067065, 1.1681211388331418, 1.1671206473325066, 1.1578568070331394, 1.1547783259248832, 1.1595951597303826, 1.155447984818805, 1.1571138653667066, 1.1537251589479387, 1.1473908385701737, 1.1569255794587812, 1.1479727490971465, 1.138627585248536, 1.1346383200044259, 1.1291435314154967, 1.1286494078332638, 1.1316642044016467, 1.1278720674083953, 1.1239887566047528, 1.1219640667678394, 1.1292369570820238, 1.1274506264888287, 1.1299476636753434, 1.1335995340738942, 1.1154850877530766, 1.1251028940662955, 1.1231000572748988, 1.1033773582329252, 1.1084344602952994, 1.103683989983075, 1.1020074952064842, 1.1136318179860987, 1.1010361157893156, 1.0979872269796884], 'acc': [0.20657084275809645, 0.21806981604569256, 0.23285420851541005, 0.23285420929871545, 0.23285420970872686, 0.23285421164863163, 0.23285420851541005, 0.23285420892542147, 0.23285420990455322, 0.23367556435738746, 0.23860369614751922, 0.2381930189211021, 0.23860369636170428, 0.2402464058364931, 0.2402464070298099, 0.2418891168962514, 0.23983572960756644, 0.23778233995060657, 0.23901437454889443, 0.23778234247799038, 0.23819301815615543, 0.23737166329330978, 0.23901437478143822, 0.23860369556004016, 0.24229979570763802, 0.24024640564066674, 0.24353182952262048, 0.2406570834729216, 0.24147843947400793, 0.24804928077564592, 0.24517453686167817, 0.25790554302184243, 0.25338808840305166, 0.2574948641920971, 0.25503080028283276, 0.25585215571479875, 0.2562628323170194, 0.2599589323055083, 0.2587269008037246, 0.2562628354869584, 0.25831622338148114, 0.2595482548649061, 0.26735113019571166, 0.2632443526442291, 0.2636550310456043, 0.26447638765252834, 0.26817248523602494, 0.2570841911147507, 0.2685831610549402, 0.271868581020367, 0.27227926004593866, 0.2665297747637457, 0.2685831598799821, 0.27515400216075186, 0.27186858458195867, 0.27351129583754336, 0.2813141664317991, 0.2759753605668305, 0.2866529760541857, 0.27802874728639515, 0.2784394227136577, 0.28008213416506866, 0.2804928109998331, 0.2858316216013515, 0.280082136943355, 0.28665297883247204, 0.28131416940591175, 0.27761806988251037, 0.28706365484721363, 0.28295687690407834, 0.28747433369531766, 0.28090349159201555, 0.2911704306362591, 0.28665297781662286, 0.2936344983396589, 0.2969199178767155, 0.29281314169601735, 0.3010266938615873, 0.3043121157852776, 0.30266940374638757, 0.3002053403878848, 0.31252566912580565, 0.3047227943824792, 0.3014373732420943, 0.2993839857759417, 0.3039014369922497, 0.3092402444238291, 0.3084188929451075, 0.31622176827591303, 0.3137577011967097, 0.31252566755919486, 0.3211498979303137, 0.3248459976679001, 0.31909650886083285, 0.32320328719562086, 0.3293634493125782, 0.3281314178291532, 0.33018480689863405, 0.33675564561781207, 0.3305954815793086, 0.33141683779458003, 0.3314168385778854, 0.34579055599363434, 0.33675564679277015, 0.34455852075279125, 0.33552361593354163, 0.32936345048753635, 0.33141683642379555, 0.3363449676080896, 0.32813141665419515, 0.3252566748943172, 0.3355236121761236, 0.3416837788338044, 0.34989733256598515, 0.34989733237015885, 0.35564681781146074, 0.3605749510274531, 0.3622176611080796, 0.36016426945613883, 0.36016427066781437, 0.36303901552419643, 0.36098562864552286, 0.3646817230590805, 0.3720739209676425, 0.3712525665148083, 0.36550307966600454, 0.36344969278733097, 0.3659137594381642, 0.3749486637066522, 0.38069815212206676, 0.3770020535594384, 0.3687885010022158, 0.37864476442337036, 0.37905543949569764, 0.3872689926036819, 0.37987679512349, 0.39507186797120486, 0.38521560533335564, 0.3843942499013897, 0.3753593432829855, 0.3839835724791462, 0.3909650921821594, 0.3921971256238479, 0.3963039031753305, 0.3819301863470606, 0.4049281323714912, 0.3868583161972876, 0.39466119191974586, 0.3971252578239911, 0.404517453970116, 0.4147843947767967, 0.4082135535118761, 0.4127310047281841, 0.4082135540626377, 0.414784394972623, 0.4114989742606083, 0.3987679666929421, 0.4041067755687408, 0.3954825445734255, 0.42217658955213716, 0.41232032828507237, 0.42135523568678196, 0.41889116958671035, 0.42422997725083356, 0.4291581086676713, 0.42094455650210133, 0.41806981517059355, 0.4188911684117523, 0.41765913673250094, 0.42915811121341385, 0.43121149672130293, 0.42381930061189543, 0.42997946605790077, 0.43162217535522196, 0.4431211478778714, 0.4295687874606992, 0.4312114967580204, 0.435318274309503, 0.4418891153785972, 0.4349075980622176, 0.4402464082720833, 0.4435318278458574, 0.4402464057263408, 0.4484599600460005, 0.4521560586086289, 0.4443531805362545, 0.4529774110664822, 0.4435318278458574, 0.45831622268384975, 0.45667351279904955, 0.46160164323675557, 0.45790554428247454, 0.45667351002076323, 0.46652977265861245, 0.4616016444117137, 0.46611909621550074, 0.4648870631654649, 0.46447638495991606, 0.4562628326352372, 0.45995893397615184, 0.4422997938166898, 0.45215605641782164, 0.47145790685373656, 0.45954825612553823, 0.4813141700790648, 0.46119096702618767, 0.46078028721731057, 0.4579055464365644, 0.47433264701028627, 0.46242299768958983, 0.4681724857133517, 0.4759753574825655, 0.47022587497865886, 0.4722792598990689, 0.47926078081375767, 0.4751540046330595, 0.48665297809812325, 0.49363449682200466, 0.4833675559786066, 0.46283367429181044, 0.4632443533173822, 0.46858316391890054, 0.47679671389366324, 0.47885010456647226, 0.4763860390538797, 0.4944558502957072, 0.49158110716504483, 0.495687883345743, 0.5022587276214936, 0.5018480517291435, 0.49815195261575357, 0.503490760047333, 0.4858316234494626, 0.4882956885704025, 0.4965092389735353, 0.478850101041598, 0.48624230208338165, 0.4932238176373241, 0.5018480486326394, 0.4989733046819542, 0.5039014396236663, 0.49979465913478843, 0.5121149912017572, 0.5137577001074256, 0.5051334709479823, 0.5071868588057876, 0.5149897347240722, 0.5039014353154866, 0.5166324450005251, 0.5162217646408864, 0.5182751544936727, 0.5240246434965662, 0.5158110895685591, 0.5244353224854205, 0.518685830740958, 0.5264887092049851, 0.5203285439548062, 0.5240246438882189, 0.5232032886520793, 0.5088295681031088, 0.508829571627983, 0.5154004113630103, 0.5244353143831053, 0.5190965099256386, 0.5297741272121484, 0.5232032864979895, 0.5154004109713576, 0.5330595522690602, 0.5318275154983239, 0.5375770041095648, 0.5400410713845945, 0.5355236166066947, 0.5347022623496869, 0.5408624242708179, 0.5371663209349223, 0.5334702215889886, 0.5318275158899766, 0.5371663260998423, 0.5474332649849768, 0.5462012354598154, 0.5503080128154716, 0.5523613943701162, 0.543326489000105, 0.5515400433197648, 0.5437371687722646, 0.5568788529421514, 0.545379875915496, 0.5515400381548449, 0.5482546225710326, 0.548254623354338, 0.557289531539353, 0.5433264903708894, 0.5490759799612621, 0.5626283330594245, 0.5601642754533208, 0.5622176560288337, 0.5581108856005346, 0.559342919042223, 0.5667351173424378, 0.5646817270979989]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
