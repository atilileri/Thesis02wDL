{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf30.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 03:49:56 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '0Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['03', '02', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000181005C8550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000018158096EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0952, Accuracy:0.3187, Validation Loss:1.0817, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0804, Accuracy:0.3729, Validation Loss:1.0762, Validation Accuracy:0.3727\n",
    "Epoch #3: Loss:1.0759, Accuracy:0.3762, Validation Loss:1.0738, Validation Accuracy:0.4105\n",
    "Epoch #4: Loss:1.0742, Accuracy:0.3947, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #14: Loss:1.0744, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0742, Accuracy:0.3938, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0739, Accuracy:0.3988, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #30: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #31: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0735, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0744, Validation Accuracy:0.3974\n",
    "Epoch #35: Loss:1.0735, Accuracy:0.3984, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0738, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0747, Validation Accuracy:0.3892\n",
    "Epoch #39: Loss:1.0735, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0736, Accuracy:0.3955, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0746, Validation Accuracy:0.4007\n",
    "Epoch #43: Loss:1.0734, Accuracy:0.4012, Validation Loss:1.0744, Validation Accuracy:0.3974\n",
    "Epoch #44: Loss:1.0736, Accuracy:0.3992, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #45: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #46: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0737, Validation Accuracy:0.4007\n",
    "Epoch #47: Loss:1.0736, Accuracy:0.3926, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #48: Loss:1.0735, Accuracy:0.3955, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #49: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #50: Loss:1.0735, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3826\n",
    "Epoch #51: Loss:1.0735, Accuracy:0.3951, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #52: Loss:1.0735, Accuracy:0.3979, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #53: Loss:1.0734, Accuracy:0.3955, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #54: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0744, Validation Accuracy:0.3957\n",
    "Epoch #55: Loss:1.0731, Accuracy:0.4016, Validation Loss:1.0745, Validation Accuracy:0.3777\n",
    "Epoch #56: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #57: Loss:1.0736, Accuracy:0.3910, Validation Loss:1.0741, Validation Accuracy:0.3744\n",
    "Epoch #58: Loss:1.0732, Accuracy:0.3934, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #59: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3957\n",
    "Epoch #60: Loss:1.0734, Accuracy:0.3930, Validation Loss:1.0745, Validation Accuracy:0.3859\n",
    "Epoch #61: Loss:1.0730, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #62: Loss:1.0725, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3859\n",
    "Epoch #63: Loss:1.0722, Accuracy:0.3922, Validation Loss:1.0738, Validation Accuracy:0.3760\n",
    "Epoch #64: Loss:1.0722, Accuracy:0.4074, Validation Loss:1.0744, Validation Accuracy:0.3678\n",
    "Epoch #65: Loss:1.0726, Accuracy:0.4094, Validation Loss:1.0743, Validation Accuracy:0.3678\n",
    "Epoch #66: Loss:1.0722, Accuracy:0.3992, Validation Loss:1.0740, Validation Accuracy:0.3810\n",
    "Epoch #67: Loss:1.0728, Accuracy:0.4008, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #68: Loss:1.0720, Accuracy:0.3959, Validation Loss:1.0736, Validation Accuracy:0.4105\n",
    "Epoch #69: Loss:1.0755, Accuracy:0.3901, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #70: Loss:1.0738, Accuracy:0.3967, Validation Loss:1.0755, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0743, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3908\n",
    "Epoch #72: Loss:1.0733, Accuracy:0.4012, Validation Loss:1.0750, Validation Accuracy:0.3727\n",
    "Epoch #73: Loss:1.0742, Accuracy:0.3852, Validation Loss:1.0757, Validation Accuracy:0.3481\n",
    "Epoch #74: Loss:1.0737, Accuracy:0.3930, Validation Loss:1.0759, Validation Accuracy:0.3777\n",
    "Epoch #75: Loss:1.0741, Accuracy:0.3971, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0740, Accuracy:0.3975, Validation Loss:1.0757, Validation Accuracy:0.3826\n",
    "Epoch #77: Loss:1.0735, Accuracy:0.4004, Validation Loss:1.0758, Validation Accuracy:0.3793\n",
    "Epoch #78: Loss:1.0735, Accuracy:0.4049, Validation Loss:1.0754, Validation Accuracy:0.3957\n",
    "Epoch #79: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0755, Validation Accuracy:0.3793\n",
    "Epoch #80: Loss:1.0734, Accuracy:0.4066, Validation Loss:1.0753, Validation Accuracy:0.3777\n",
    "Epoch #81: Loss:1.0733, Accuracy:0.4078, Validation Loss:1.0747, Validation Accuracy:0.3842\n",
    "Epoch #82: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #83: Loss:1.0731, Accuracy:0.4033, Validation Loss:1.0752, Validation Accuracy:0.3760\n",
    "Epoch #84: Loss:1.0731, Accuracy:0.3881, Validation Loss:1.0754, Validation Accuracy:0.3711\n",
    "Epoch #85: Loss:1.0729, Accuracy:0.4062, Validation Loss:1.0757, Validation Accuracy:0.3941\n",
    "Epoch #86: Loss:1.0737, Accuracy:0.3988, Validation Loss:1.0763, Validation Accuracy:0.3892\n",
    "Epoch #87: Loss:1.0733, Accuracy:0.4053, Validation Loss:1.0759, Validation Accuracy:0.3695\n",
    "Epoch #88: Loss:1.0740, Accuracy:0.3840, Validation Loss:1.0757, Validation Accuracy:0.3580\n",
    "Epoch #89: Loss:1.0743, Accuracy:0.3840, Validation Loss:1.0765, Validation Accuracy:0.3678\n",
    "Epoch #90: Loss:1.0743, Accuracy:0.4016, Validation Loss:1.0756, Validation Accuracy:0.3859\n",
    "Epoch #91: Loss:1.0742, Accuracy:0.3934, Validation Loss:1.0743, Validation Accuracy:0.3842\n",
    "Epoch #92: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0736, Validation Accuracy:0.4023\n",
    "Epoch #93: Loss:1.0738, Accuracy:0.3926, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #94: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #95: Loss:1.0736, Accuracy:0.4074, Validation Loss:1.0742, Validation Accuracy:0.4204\n",
    "Epoch #96: Loss:1.0733, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.4138\n",
    "Epoch #97: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #98: Loss:1.0732, Accuracy:0.3959, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #99: Loss:1.0732, Accuracy:0.3984, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #100: Loss:1.0730, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #101: Loss:1.0728, Accuracy:0.3996, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #102: Loss:1.0730, Accuracy:0.4004, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #103: Loss:1.0729, Accuracy:0.4021, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #104: Loss:1.0728, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #105: Loss:1.0730, Accuracy:0.3914, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #106: Loss:1.0724, Accuracy:0.4070, Validation Loss:1.0741, Validation Accuracy:0.3990\n",
    "Epoch #107: Loss:1.0727, Accuracy:0.3996, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #108: Loss:1.0729, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #109: Loss:1.0725, Accuracy:0.4082, Validation Loss:1.0744, Validation Accuracy:0.4039\n",
    "Epoch #110: Loss:1.0723, Accuracy:0.3996, Validation Loss:1.0744, Validation Accuracy:0.3974\n",
    "Epoch #111: Loss:1.0724, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.4007\n",
    "Epoch #112: Loss:1.0723, Accuracy:0.4057, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #113: Loss:1.0726, Accuracy:0.4037, Validation Loss:1.0755, Validation Accuracy:0.3793\n",
    "Epoch #114: Loss:1.0722, Accuracy:0.3930, Validation Loss:1.0757, Validation Accuracy:0.3432\n",
    "Epoch #115: Loss:1.0719, Accuracy:0.4029, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #116: Loss:1.0721, Accuracy:0.4021, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #117: Loss:1.0720, Accuracy:0.4082, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #118: Loss:1.0722, Accuracy:0.4074, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #119: Loss:1.0724, Accuracy:0.4008, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #120: Loss:1.0722, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #121: Loss:1.0721, Accuracy:0.3918, Validation Loss:1.0741, Validation Accuracy:0.4072\n",
    "Epoch #122: Loss:1.0722, Accuracy:0.3979, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #123: Loss:1.0716, Accuracy:0.4037, Validation Loss:1.0749, Validation Accuracy:0.3711\n",
    "Epoch #124: Loss:1.0720, Accuracy:0.3938, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #125: Loss:1.0714, Accuracy:0.3984, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #126: Loss:1.0714, Accuracy:0.4082, Validation Loss:1.0748, Validation Accuracy:0.3875\n",
    "Epoch #127: Loss:1.0715, Accuracy:0.3996, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #128: Loss:1.0712, Accuracy:0.3918, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #129: Loss:1.0714, Accuracy:0.4037, Validation Loss:1.0752, Validation Accuracy:0.3826\n",
    "Epoch #130: Loss:1.0723, Accuracy:0.3906, Validation Loss:1.0754, Validation Accuracy:0.3875\n",
    "Epoch #131: Loss:1.0719, Accuracy:0.3910, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #132: Loss:1.0725, Accuracy:0.3848, Validation Loss:1.0778, Validation Accuracy:0.3563\n",
    "Epoch #133: Loss:1.0712, Accuracy:0.3967, Validation Loss:1.0761, Validation Accuracy:0.3974\n",
    "Epoch #134: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0761, Validation Accuracy:0.3530\n",
    "Epoch #135: Loss:1.0730, Accuracy:0.3893, Validation Loss:1.0765, Validation Accuracy:0.3563\n",
    "Epoch #136: Loss:1.0729, Accuracy:0.3881, Validation Loss:1.0770, Validation Accuracy:0.3990\n",
    "Epoch #137: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0757, Validation Accuracy:0.3695\n",
    "Epoch #138: Loss:1.0720, Accuracy:0.3914, Validation Loss:1.0748, Validation Accuracy:0.3629\n",
    "Epoch #139: Loss:1.0715, Accuracy:0.4066, Validation Loss:1.0759, Validation Accuracy:0.4007\n",
    "Epoch #140: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0763, Validation Accuracy:0.3547\n",
    "Epoch #141: Loss:1.0711, Accuracy:0.3918, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #142: Loss:1.0708, Accuracy:0.3975, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #143: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0752, Validation Accuracy:0.3974\n",
    "Epoch #144: Loss:1.0714, Accuracy:0.3852, Validation Loss:1.0756, Validation Accuracy:0.3695\n",
    "Epoch #145: Loss:1.0711, Accuracy:0.3988, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #146: Loss:1.0713, Accuracy:0.4025, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #147: Loss:1.0704, Accuracy:0.4045, Validation Loss:1.0755, Validation Accuracy:0.3678\n",
    "Epoch #148: Loss:1.0709, Accuracy:0.3823, Validation Loss:1.0753, Validation Accuracy:0.3908\n",
    "Epoch #149: Loss:1.0705, Accuracy:0.4008, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #150: Loss:1.0728, Accuracy:0.3889, Validation Loss:1.0741, Validation Accuracy:0.3842\n",
    "Epoch #151: Loss:1.0712, Accuracy:0.3930, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #152: Loss:1.0707, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.3859\n",
    "Epoch #153: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #154: Loss:1.0700, Accuracy:0.3959, Validation Loss:1.0727, Validation Accuracy:0.3859\n",
    "Epoch #155: Loss:1.0700, Accuracy:0.3922, Validation Loss:1.0731, Validation Accuracy:0.4072\n",
    "Epoch #156: Loss:1.0696, Accuracy:0.3984, Validation Loss:1.0740, Validation Accuracy:0.3810\n",
    "Epoch #157: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #158: Loss:1.0701, Accuracy:0.4070, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #159: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #160: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #161: Loss:1.0717, Accuracy:0.3992, Validation Loss:1.0756, Validation Accuracy:0.4056\n",
    "Epoch #162: Loss:1.0711, Accuracy:0.4012, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #163: Loss:1.0701, Accuracy:0.4066, Validation Loss:1.0736, Validation Accuracy:0.3924\n",
    "Epoch #164: Loss:1.0701, Accuracy:0.4012, Validation Loss:1.0730, Validation Accuracy:0.4056\n",
    "Epoch #165: Loss:1.0695, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.3974\n",
    "Epoch #166: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0759, Validation Accuracy:0.3826\n",
    "Epoch #167: Loss:1.0738, Accuracy:0.3807, Validation Loss:1.0760, Validation Accuracy:0.3711\n",
    "Epoch #168: Loss:1.0708, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #169: Loss:1.0703, Accuracy:0.4021, Validation Loss:1.0737, Validation Accuracy:0.4056\n",
    "Epoch #170: Loss:1.0703, Accuracy:0.4119, Validation Loss:1.0744, Validation Accuracy:0.3612\n",
    "Epoch #171: Loss:1.0697, Accuracy:0.4074, Validation Loss:1.0738, Validation Accuracy:0.4089\n",
    "Epoch #172: Loss:1.0697, Accuracy:0.4016, Validation Loss:1.0739, Validation Accuracy:0.4039\n",
    "Epoch #173: Loss:1.0685, Accuracy:0.4008, Validation Loss:1.0751, Validation Accuracy:0.3563\n",
    "Epoch #174: Loss:1.0699, Accuracy:0.3938, Validation Loss:1.0746, Validation Accuracy:0.4039\n",
    "Epoch #175: Loss:1.0683, Accuracy:0.4008, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #176: Loss:1.0681, Accuracy:0.4062, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "Epoch #177: Loss:1.0690, Accuracy:0.3860, Validation Loss:1.0757, Validation Accuracy:0.3612\n",
    "Epoch #178: Loss:1.0702, Accuracy:0.3930, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #179: Loss:1.0686, Accuracy:0.3844, Validation Loss:1.0752, Validation Accuracy:0.3859\n",
    "Epoch #180: Loss:1.0671, Accuracy:0.4000, Validation Loss:1.0738, Validation Accuracy:0.4072\n",
    "Epoch #181: Loss:1.0679, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #182: Loss:1.0681, Accuracy:0.3844, Validation Loss:1.0760, Validation Accuracy:0.4056\n",
    "Epoch #183: Loss:1.0702, Accuracy:0.3955, Validation Loss:1.0794, Validation Accuracy:0.3711\n",
    "Epoch #184: Loss:1.0697, Accuracy:0.3943, Validation Loss:1.0760, Validation Accuracy:0.4056\n",
    "Epoch #185: Loss:1.0705, Accuracy:0.4004, Validation Loss:1.0758, Validation Accuracy:0.3990\n",
    "Epoch #186: Loss:1.0693, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #187: Loss:1.0699, Accuracy:0.4025, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #188: Loss:1.0702, Accuracy:0.3979, Validation Loss:1.0735, Validation Accuracy:0.4039\n",
    "Epoch #189: Loss:1.0708, Accuracy:0.3971, Validation Loss:1.0725, Validation Accuracy:0.4039\n",
    "Epoch #190: Loss:1.0714, Accuracy:0.3984, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #191: Loss:1.0708, Accuracy:0.3910, Validation Loss:1.0755, Validation Accuracy:0.3662\n",
    "Epoch #192: Loss:1.0696, Accuracy:0.3984, Validation Loss:1.0732, Validation Accuracy:0.4023\n",
    "Epoch #193: Loss:1.0694, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #194: Loss:1.0701, Accuracy:0.3959, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #195: Loss:1.0703, Accuracy:0.3988, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #196: Loss:1.0694, Accuracy:0.3992, Validation Loss:1.0752, Validation Accuracy:0.3563\n",
    "Epoch #197: Loss:1.0687, Accuracy:0.3975, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #198: Loss:1.0682, Accuracy:0.4000, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #199: Loss:1.0682, Accuracy:0.4008, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #200: Loss:1.0690, Accuracy:0.4000, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #201: Loss:1.0685, Accuracy:0.4016, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #202: Loss:1.0695, Accuracy:0.3852, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #203: Loss:1.0702, Accuracy:0.3971, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #204: Loss:1.0690, Accuracy:0.3910, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "Epoch #205: Loss:1.0710, Accuracy:0.4012, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #206: Loss:1.0701, Accuracy:0.4041, Validation Loss:1.0744, Validation Accuracy:0.3580\n",
    "Epoch #207: Loss:1.0705, Accuracy:0.3889, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #208: Loss:1.0689, Accuracy:0.3992, Validation Loss:1.0733, Validation Accuracy:0.4072\n",
    "Epoch #209: Loss:1.0689, Accuracy:0.3967, Validation Loss:1.0743, Validation Accuracy:0.4023\n",
    "Epoch #210: Loss:1.0687, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.4089\n",
    "Epoch #211: Loss:1.0698, Accuracy:0.3943, Validation Loss:1.0754, Validation Accuracy:0.4105\n",
    "Epoch #212: Loss:1.0696, Accuracy:0.3971, Validation Loss:1.0729, Validation Accuracy:0.3793\n",
    "Epoch #213: Loss:1.0699, Accuracy:0.4074, Validation Loss:1.0759, Validation Accuracy:0.4056\n",
    "Epoch #214: Loss:1.0712, Accuracy:0.3926, Validation Loss:1.0748, Validation Accuracy:0.4039\n",
    "Epoch #215: Loss:1.0729, Accuracy:0.3918, Validation Loss:1.0764, Validation Accuracy:0.3695\n",
    "Epoch #216: Loss:1.0725, Accuracy:0.3930, Validation Loss:1.0785, Validation Accuracy:0.3875\n",
    "Epoch #217: Loss:1.0713, Accuracy:0.3951, Validation Loss:1.0768, Validation Accuracy:0.3629\n",
    "Epoch #218: Loss:1.0709, Accuracy:0.3930, Validation Loss:1.0744, Validation Accuracy:0.4089\n",
    "Epoch #219: Loss:1.0695, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3957\n",
    "Epoch #220: Loss:1.0693, Accuracy:0.4025, Validation Loss:1.0747, Validation Accuracy:0.4154\n",
    "Epoch #221: Loss:1.0688, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.4007\n",
    "Epoch #222: Loss:1.0690, Accuracy:0.4004, Validation Loss:1.0752, Validation Accuracy:0.3662\n",
    "Epoch #223: Loss:1.0702, Accuracy:0.3938, Validation Loss:1.0763, Validation Accuracy:0.3744\n",
    "Epoch #224: Loss:1.0703, Accuracy:0.4053, Validation Loss:1.0769, Validation Accuracy:0.3974\n",
    "Epoch #225: Loss:1.0719, Accuracy:0.3889, Validation Loss:1.0774, Validation Accuracy:0.3612\n",
    "Epoch #226: Loss:1.0691, Accuracy:0.4045, Validation Loss:1.0780, Validation Accuracy:0.3941\n",
    "Epoch #227: Loss:1.0839, Accuracy:0.4062, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #228: Loss:1.0726, Accuracy:0.4070, Validation Loss:1.0804, Validation Accuracy:0.3695\n",
    "Epoch #229: Loss:1.0753, Accuracy:0.3885, Validation Loss:1.0820, Validation Accuracy:0.3695\n",
    "Epoch #230: Loss:1.0701, Accuracy:0.3914, Validation Loss:1.0766, Validation Accuracy:0.3744\n",
    "Epoch #231: Loss:1.0697, Accuracy:0.4008, Validation Loss:1.0753, Validation Accuracy:0.3974\n",
    "Epoch #232: Loss:1.0695, Accuracy:0.4004, Validation Loss:1.0759, Validation Accuracy:0.3530\n",
    "Epoch #233: Loss:1.0692, Accuracy:0.3992, Validation Loss:1.0748, Validation Accuracy:0.3957\n",
    "Epoch #234: Loss:1.0699, Accuracy:0.4012, Validation Loss:1.0760, Validation Accuracy:0.3908\n",
    "Epoch #235: Loss:1.0695, Accuracy:0.3930, Validation Loss:1.0762, Validation Accuracy:0.3727\n",
    "Epoch #236: Loss:1.0693, Accuracy:0.3943, Validation Loss:1.0770, Validation Accuracy:0.3793\n",
    "Epoch #237: Loss:1.0691, Accuracy:0.3963, Validation Loss:1.0769, Validation Accuracy:0.3760\n",
    "Epoch #238: Loss:1.0695, Accuracy:0.3881, Validation Loss:1.0774, Validation Accuracy:0.3612\n",
    "Epoch #239: Loss:1.0676, Accuracy:0.3955, Validation Loss:1.0767, Validation Accuracy:0.4007\n",
    "Epoch #240: Loss:1.0680, Accuracy:0.4029, Validation Loss:1.0762, Validation Accuracy:0.3826\n",
    "Epoch #241: Loss:1.0674, Accuracy:0.4004, Validation Loss:1.0754, Validation Accuracy:0.3810\n",
    "Epoch #242: Loss:1.0673, Accuracy:0.3984, Validation Loss:1.0730, Validation Accuracy:0.3842\n",
    "Epoch #243: Loss:1.0666, Accuracy:0.4025, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #244: Loss:1.0680, Accuracy:0.3947, Validation Loss:1.0773, Validation Accuracy:0.3793\n",
    "Epoch #245: Loss:1.0711, Accuracy:0.4037, Validation Loss:1.0754, Validation Accuracy:0.3777\n",
    "Epoch #246: Loss:1.0703, Accuracy:0.3885, Validation Loss:1.0752, Validation Accuracy:0.3793\n",
    "Epoch #247: Loss:1.0667, Accuracy:0.4094, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #248: Loss:1.0668, Accuracy:0.3992, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #249: Loss:1.0686, Accuracy:0.4033, Validation Loss:1.0727, Validation Accuracy:0.3892\n",
    "Epoch #250: Loss:1.0709, Accuracy:0.3967, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #251: Loss:1.0710, Accuracy:0.4012, Validation Loss:1.0825, Validation Accuracy:0.3580\n",
    "Epoch #252: Loss:1.0703, Accuracy:0.3914, Validation Loss:1.0768, Validation Accuracy:0.3859\n",
    "Epoch #253: Loss:1.0686, Accuracy:0.4008, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #254: Loss:1.0680, Accuracy:0.4021, Validation Loss:1.0753, Validation Accuracy:0.3711\n",
    "Epoch #255: Loss:1.0680, Accuracy:0.3914, Validation Loss:1.0754, Validation Accuracy:0.3990\n",
    "Epoch #256: Loss:1.0670, Accuracy:0.4029, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #257: Loss:1.0668, Accuracy:0.4021, Validation Loss:1.0754, Validation Accuracy:0.3892\n",
    "Epoch #258: Loss:1.0679, Accuracy:0.3910, Validation Loss:1.0771, Validation Accuracy:0.3695\n",
    "Epoch #259: Loss:1.0663, Accuracy:0.3992, Validation Loss:1.0754, Validation Accuracy:0.3957\n",
    "Epoch #260: Loss:1.0664, Accuracy:0.4057, Validation Loss:1.0766, Validation Accuracy:0.4039\n",
    "Epoch #261: Loss:1.0659, Accuracy:0.4012, Validation Loss:1.0752, Validation Accuracy:0.4007\n",
    "Epoch #262: Loss:1.0651, Accuracy:0.4074, Validation Loss:1.0751, Validation Accuracy:0.4154\n",
    "Epoch #263: Loss:1.0651, Accuracy:0.3963, Validation Loss:1.0754, Validation Accuracy:0.3695\n",
    "Epoch #264: Loss:1.0655, Accuracy:0.4041, Validation Loss:1.0754, Validation Accuracy:0.4023\n",
    "Epoch #265: Loss:1.0672, Accuracy:0.4099, Validation Loss:1.0766, Validation Accuracy:0.4007\n",
    "Epoch #266: Loss:1.0660, Accuracy:0.4062, Validation Loss:1.0757, Validation Accuracy:0.4138\n",
    "Epoch #267: Loss:1.0660, Accuracy:0.4094, Validation Loss:1.0783, Validation Accuracy:0.3760\n",
    "Epoch #268: Loss:1.0654, Accuracy:0.4025, Validation Loss:1.0769, Validation Accuracy:0.3990\n",
    "Epoch #269: Loss:1.0663, Accuracy:0.4057, Validation Loss:1.0765, Validation Accuracy:0.3678\n",
    "Epoch #270: Loss:1.0645, Accuracy:0.3975, Validation Loss:1.0756, Validation Accuracy:0.3990\n",
    "Epoch #271: Loss:1.0646, Accuracy:0.4053, Validation Loss:1.0743, Validation Accuracy:0.4089\n",
    "Epoch #272: Loss:1.0645, Accuracy:0.3988, Validation Loss:1.0734, Validation Accuracy:0.4056\n",
    "Epoch #273: Loss:1.0657, Accuracy:0.4103, Validation Loss:1.0752, Validation Accuracy:0.3662\n",
    "Epoch #274: Loss:1.0668, Accuracy:0.3914, Validation Loss:1.0777, Validation Accuracy:0.3580\n",
    "Epoch #275: Loss:1.0671, Accuracy:0.4000, Validation Loss:1.0750, Validation Accuracy:0.3908\n",
    "Epoch #276: Loss:1.0685, Accuracy:0.3955, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #277: Loss:1.0668, Accuracy:0.3877, Validation Loss:1.0750, Validation Accuracy:0.3777\n",
    "Epoch #278: Loss:1.0675, Accuracy:0.3918, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #279: Loss:1.0675, Accuracy:0.3971, Validation Loss:1.0740, Validation Accuracy:0.3727\n",
    "Epoch #280: Loss:1.0657, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.4039\n",
    "Epoch #281: Loss:1.0660, Accuracy:0.3984, Validation Loss:1.0755, Validation Accuracy:0.4089\n",
    "Epoch #282: Loss:1.0671, Accuracy:0.4025, Validation Loss:1.0767, Validation Accuracy:0.3974\n",
    "Epoch #283: Loss:1.0654, Accuracy:0.4025, Validation Loss:1.0780, Validation Accuracy:0.3645\n",
    "Epoch #284: Loss:1.0656, Accuracy:0.3959, Validation Loss:1.0781, Validation Accuracy:0.3941\n",
    "Epoch #285: Loss:1.0656, Accuracy:0.3971, Validation Loss:1.0770, Validation Accuracy:0.3924\n",
    "Epoch #286: Loss:1.0644, Accuracy:0.4004, Validation Loss:1.0756, Validation Accuracy:0.3629\n",
    "Epoch #287: Loss:1.0654, Accuracy:0.3996, Validation Loss:1.0742, Validation Accuracy:0.4072\n",
    "Epoch #288: Loss:1.0651, Accuracy:0.4049, Validation Loss:1.0743, Validation Accuracy:0.4039\n",
    "Epoch #289: Loss:1.0653, Accuracy:0.4066, Validation Loss:1.0758, Validation Accuracy:0.4072\n",
    "Epoch #290: Loss:1.0644, Accuracy:0.3938, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #291: Loss:1.0653, Accuracy:0.4062, Validation Loss:1.0741, Validation Accuracy:0.4089\n",
    "Epoch #292: Loss:1.0663, Accuracy:0.4066, Validation Loss:1.0744, Validation Accuracy:0.4056\n",
    "Epoch #293: Loss:1.0663, Accuracy:0.3996, Validation Loss:1.0727, Validation Accuracy:0.4122\n",
    "Epoch #294: Loss:1.0654, Accuracy:0.4099, Validation Loss:1.0735, Validation Accuracy:0.4089\n",
    "Epoch #295: Loss:1.0661, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.4105\n",
    "Epoch #296: Loss:1.0651, Accuracy:0.4123, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #297: Loss:1.0652, Accuracy:0.4070, Validation Loss:1.0739, Validation Accuracy:0.4072\n",
    "Epoch #298: Loss:1.0650, Accuracy:0.4037, Validation Loss:1.0738, Validation Accuracy:0.4122\n",
    "Epoch #299: Loss:1.0648, Accuracy:0.4016, Validation Loss:1.0756, Validation Accuracy:0.4154\n",
    "Epoch #300: Loss:1.0648, Accuracy:0.4008, Validation Loss:1.0745, Validation Accuracy:0.4023\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07452488, Accuracy:0.4023\n",
    "Labels: ['03', '02', '01']\n",
    "Confusion Matrix:\n",
    "      03  02   01\n",
    "t:03   0  18  124\n",
    "t:02   1  25  201\n",
    "t:01   0  20  220\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          03       0.00      0.00      0.00       142\n",
    "          02       0.40      0.11      0.17       227\n",
    "          01       0.40      0.92      0.56       240\n",
    "\n",
    "    accuracy                           0.40       609\n",
    "   macro avg       0.27      0.34      0.24       609\n",
    "weighted avg       0.31      0.40      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 04:30:52 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 55 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0817156337164893, 1.0762300219246124, 1.073813475020022, 1.0744349335997758, 1.0745725062093123, 1.0743919650126366, 1.073888196146547, 1.0740100440916365, 1.074011786817917, 1.0739562758084, 1.0737412163776716, 1.0738586954686833, 1.074016926128093, 1.073869985116918, 1.0739210064970997, 1.0740220065187351, 1.074051691002055, 1.0740149674940187, 1.0740126359638908, 1.074102681062883, 1.0741085604885332, 1.074145140319035, 1.0740777348062676, 1.0741901201959119, 1.074188330686347, 1.0742246918686114, 1.0742881366576271, 1.0742795005416244, 1.0743715982327515, 1.0743703900886874, 1.0743368176991128, 1.074407614119143, 1.0745082629725264, 1.0744463248401637, 1.074444508904894, 1.0744613510830257, 1.0742528616696938, 1.0746876974215454, 1.074603483398951, 1.074605736630695, 1.0743866640162978, 1.0746057387839005, 1.0743663569389306, 1.0741801179688553, 1.074166852656648, 1.0736555702776354, 1.073668185713256, 1.074460652857187, 1.0740426034958688, 1.0746266994570277, 1.074510502697799, 1.0743755438840643, 1.074576432873267, 1.0743919583572739, 1.0744910886135008, 1.074016580049236, 1.074099058979642, 1.0745605610078584, 1.0744516291641837, 1.0745242015873073, 1.0739401001452618, 1.0739744482760751, 1.0738454315266976, 1.0743733491803624, 1.0742606881804067, 1.0740206304246374, 1.0732693104516893, 1.0736064818888071, 1.0739681889075168, 1.075479322857849, 1.0739311204950994, 1.0750469754286391, 1.0757074299312772, 1.0758918660810624, 1.0756548361238, 1.0756813076329348, 1.07577816469133, 1.0753837868889369, 1.0754737609321456, 1.0753157647763958, 1.074661550654958, 1.0747892259572722, 1.0752010237798706, 1.075375356110446, 1.0756780907438306, 1.0763080687749953, 1.0758702784336258, 1.0756861666348962, 1.0764806407621537, 1.0755992235221299, 1.074330877983707, 1.0736206998966011, 1.073633844825043, 1.0734378081824392, 1.0741907681150389, 1.0740718563593472, 1.0741996383432098, 1.0743031730792794, 1.0742355992249863, 1.0740362752247326, 1.0738357700933572, 1.0738467976377515, 1.0740305848896796, 1.074133389297573, 1.0742529718746692, 1.0741290482394215, 1.0743683257517949, 1.0743072173865558, 1.0743528015312107, 1.0744090358220493, 1.0746127644978916, 1.0748989632955717, 1.0754822651153715, 1.075719703007214, 1.0751314080994705, 1.0749567578774564, 1.0744544003397374, 1.0740738570788029, 1.0747493513307744, 1.0741580669907318, 1.0740949518379124, 1.0737155944060026, 1.074935373414326, 1.0749088102960822, 1.074502282541961, 1.0747533541399075, 1.0754523615923226, 1.0754657210583365, 1.0751930915663395, 1.0754168567986324, 1.0759489360114036, 1.0778369562966483, 1.076072623381278, 1.0760628746452394, 1.0764539971923202, 1.0769527355829875, 1.075661100973245, 1.074778910732426, 1.0759433842644903, 1.076300269668717, 1.074876889415171, 1.0758738830954766, 1.075171284879174, 1.07564604047484, 1.0745595897164055, 1.0749748786681979, 1.075464704353821, 1.0753019433499165, 1.0748420730600217, 1.0740697150942924, 1.0742357462302021, 1.074186394171566, 1.073176320746223, 1.0726960229951956, 1.0730936437209055, 1.0740323176329163, 1.0740261696438091, 1.0746666880076743, 1.0740537798071925, 1.0734001294341189, 1.0755946775179583, 1.0742137482992338, 1.073593044907393, 1.072959703960638, 1.0745296182695085, 1.07593713468323, 1.0760334536359815, 1.0746264400936307, 1.073722970309516, 1.0743944609693705, 1.0737883265578296, 1.073920873977085, 1.0751152798068544, 1.074632241220897, 1.0752050263932578, 1.0757505341703668, 1.0757332546957608, 1.075625825985312, 1.0751962563870183, 1.0738236984400131, 1.073826117468585, 1.0760259305315065, 1.0794235773274463, 1.0759965206797684, 1.0757982016392724, 1.074336066034627, 1.072789743811822, 1.073465454950317, 1.0725314176728573, 1.0735861417303727, 1.0754757622388391, 1.0732415523043604, 1.074542566277515, 1.0749461433570373, 1.0739460623714527, 1.0752269865452557, 1.0736316325042048, 1.074410289966414, 1.0743022266475635, 1.0734574184041892, 1.0740015021294405, 1.0732780121425884, 1.0737402875630921, 1.0744787740394204, 1.0738111421196723, 1.0744239295448967, 1.0733769930446482, 1.073266327479007, 1.074296690168835, 1.0738243201291815, 1.0753868123384924, 1.072928123090459, 1.0758862730317515, 1.0747696895317491, 1.076377935010224, 1.0784689675410981, 1.0768275873610147, 1.074421632270312, 1.0743396264578908, 1.074651939332583, 1.0742311563789355, 1.0752086345785357, 1.0763337890111362, 1.07694991468796, 1.0773700201648406, 1.0779842416249668, 1.0757135150859314, 1.0803917497641151, 1.0819560297212774, 1.0765871751093121, 1.0753430308183818, 1.0758889253895074, 1.0747598198247073, 1.0760460373607568, 1.0762134727781825, 1.076965990520659, 1.0768674978090234, 1.0774047885622298, 1.0767447709645739, 1.0762364715582435, 1.0754491157328163, 1.0730127953543451, 1.073755418725789, 1.0772647857666016, 1.0753946948325497, 1.0751664317495913, 1.0742421698296207, 1.0739553336830954, 1.07267160290372, 1.0729810753087887, 1.0825295749752002, 1.0768442778360277, 1.0749901920703833, 1.075292014723341, 1.0753837142671858, 1.0747235666942128, 1.07539294721262, 1.0770747191800272, 1.075442100784853, 1.0766162214608028, 1.0752009082897542, 1.0750529396122899, 1.0754082655084545, 1.0754158068173036, 1.0765760496919379, 1.0757461770610466, 1.0783305164236936, 1.0769016067382737, 1.0765268943579913, 1.0755933853988773, 1.0742693689264884, 1.0733798217695139, 1.0752215720162603, 1.0777264400851747, 1.0749508561367667, 1.0739580015047823, 1.0749823246487646, 1.0738286795874534, 1.0739946719656632, 1.0737072595430321, 1.075463227059062, 1.0766845943501038, 1.078010263505632, 1.0781014654632468, 1.0769757488482496, 1.0756274870855271, 1.074211299908768, 1.0743242834980655, 1.07582655191813, 1.0755329776084286, 1.0740677006726194, 1.0744272809114754, 1.0727102723223432, 1.073494042081786, 1.0735747628219805, 1.0732074318260982, 1.0739430320282484, 1.0738002678443646, 1.0755533608309742, 1.0745248986386704], 'val_acc': [0.3727421994964869, 0.3727421994964869, 0.41050903036676606, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3990147771017109, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.39573070475424843, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3973727408790432, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.38916256035294244, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.40065681322650565, 0.3973727408790432, 0.39573070475424843, 0.39573070475424843, 0.40065681312863266, 0.3990147771017109, 0.3940886686294537, 0.39244663250465894, 0.3825944159516364, 0.3990147771017109, 0.39573070475424843, 0.39573070475424843, 0.39573070475424843, 0.37766830757725217, 0.3990147770038379, 0.37438423591490055, 0.39573070475424843, 0.39573070475424843, 0.38587848849484485, 0.39573070475424843, 0.38587848800547997, 0.37602627145245743, 0.3678160910242297, 0.3678160913178486, 0.38095237963109574, 0.3924466327982779, 0.41050903085613094, 0.3875205246196396, 0.3940886686294537, 0.3908045962819912, 0.3727421995943599, 0.3481116573309468, 0.37766830738150625, 0.3940886686294537, 0.38259441585376347, 0.379310343604174, 0.39573070475424843, 0.379310343604174, 0.37766830747937924, 0.3842364519785582, 0.39573070475424843, 0.3760262711588385, 0.3711001625887083, 0.3940886686294537, 0.38916256015719647, 0.36945812646391357, 0.35796387417758824, 0.3678160908284837, 0.38587848820122594, 0.38423645266366907, 0.40229884964491935, 0.3940886686294537, 0.40229884954704637, 0.42036124691978854, 0.4137931022248636, 0.3990147771995839, 0.3940886686294537, 0.3990147771995839, 0.40065681332437864, 0.40229884954704637, 0.3990147770038379, 0.3990147770038379, 0.4039408855739681, 0.3990147774932028, 0.3990147770038379, 0.3990147770038379, 0.3924466329940238, 0.4039408855739681, 0.3973727408790432, 0.40065681332437864, 0.3875205245217666, 0.3793103437020469, 0.34318554925018147, 0.3973727408790432, 0.39080459667348316, 0.38916256064656135, 0.3973727412705351, 0.3990147772974569, 0.4006568134222516, 0.40722495811717657, 0.3973727408790432, 0.37110016346956515, 0.38916256064656135, 0.39737274117266214, 0.3875205243260207, 0.38095237992471465, 0.3842364522721771, 0.3825944161473824, 0.3875205246196396, 0.3990147770038379, 0.3563218380527935, 0.3973727408790432, 0.353037765705331, 0.3563218382485394, 0.3990147770038379, 0.36945812705115144, 0.3628899823562265, 0.40065681332437864, 0.35467980202587174, 0.3924466327004049, 0.3990147772974569, 0.39737274107478915, 0.3694581274426434, 0.39408866882519966, 0.3940886686294537, 0.3678160909263567, 0.3908045963798642, 0.3990147771017109, 0.3842364520764312, 0.39573070465637544, 0.38587848820122594, 0.40065681332437864, 0.38587848839697186, 0.4072249578235576, 0.3809523801204606, 0.3842364522721771, 0.3957307054393593, 0.38916256045081543, 0.39901477739532987, 0.40558292189450884, 0.40065681322650565, 0.3924466329940238, 0.40558292218812775, 0.3973727407811702, 0.3825944156580175, 0.3711001629802002, 0.3973727408790432, 0.40558292179663585, 0.3612479465250507, 0.40886699414409833, 0.4039408856718411, 0.3563218379549205, 0.4039408856718411, 0.3875205241302747, 0.40065681332437864, 0.36124794623143175, 0.3940886686294537, 0.38587848820122594, 0.4072249578235576, 0.3990147771017109, 0.40558292169876287, 0.3711001630780732, 0.4055829215030169, 0.39901477690596493, 0.3990147770038379, 0.40558292169876287, 0.4039408856718411, 0.40394088547609514, 0.39737274097691616, 0.3661740550951809, 0.4022988493513004, 0.3908045961841182, 0.39244663230891297, 0.4022988491555544, 0.3563218382485394, 0.3973727405854242, 0.3990147770038379, 0.39244663221104, 0.39573070446062947, 0.3908045965756102, 0.3908045964777372, 0.3940886685315807, 0.40229884964491935, 0.3957307048521214, 0.35796387407971525, 0.3875205246196396, 0.4072249578235576, 0.40229884954704637, 0.40886699414409833, 0.41050903046463905, 0.37931034419141185, 0.40558292199238183, 0.4039408858675871, 0.3694581274426434, 0.3875205240324017, 0.3628899824540995, 0.4088669937526064, 0.39573070465637544, 0.4154351387411503, 0.40065681322650565, 0.3661740550951809, 0.37438423591490055, 0.3973727408790432, 0.3612479464271777, 0.3940886687273267, 0.3990147771017109, 0.3694581273447704, 0.36945812714902443, 0.3743842352297897, 0.3973727408790432, 0.353037765705331, 0.39573070475424843, 0.3908045963798642, 0.3727421999858518, 0.379310343408428, 0.3760262711588385, 0.3612479464271777, 0.40065681312863266, 0.38259441585376347, 0.38095237963109574, 0.38423645217430413, 0.38916256064656135, 0.379310343604174, 0.37766830787087113, 0.3793103438977929, 0.3957307049499944, 0.3875205244238936, 0.38916256074443434, 0.4072249579214306, 0.35796387417758824, 0.3858784882990989, 0.3908045963798642, 0.37110016337169216, 0.3990147772974569, 0.39573070475424843, 0.38916256035294244, 0.3694581273447704, 0.39573070455850246, 0.4039408858675871, 0.40065681361799754, 0.4154351387411503, 0.3694581273447704, 0.40229884964491935, 0.40065681371587053, 0.41379310271422853, 0.3760262718439494, 0.3990147771995839, 0.3678160913178486, 0.3990147776889488, 0.4088669943398443, 0.40558292218812775, 0.36617405529092684, 0.3579638746669531, 0.3908045962819912, 0.3842364520764312, 0.37766830826236303, 0.3973727407811702, 0.3727421993986139, 0.4039408857697141, 0.40886699414409833, 0.3973727407811702, 0.3645320187746402, 0.3940886684337077, 0.3924466327982779, 0.36288998264984546, 0.4072249578235576, 0.40394088547609514, 0.4072249578235576, 0.39080459667348316, 0.40886699385047937, 0.40558292130727097, 0.41215106619794184, 0.40886699385047937, 0.4105090298774011, 0.3973727405854242, 0.4072249577256846, 0.41215106610006885, 0.4154351384475313, 0.4022988494491734], 'loss': [1.0951541597593493, 1.0804273871180947, 1.0759208365877062, 1.0742187290465808, 1.0749839017523388, 1.074668435981876, 1.0745122664273397, 1.0741210582564744, 1.0741252655366118, 1.0741728742998973, 1.074140271954468, 1.0742803196153112, 1.0741168647820944, 1.074409750452766, 1.0740693032619155, 1.074012152174415, 1.0741081475232417, 1.0741363782412707, 1.0740685845302604, 1.0740495795341978, 1.0741330591804927, 1.0740737236500766, 1.0740683341173176, 1.0740328841140871, 1.0742151899749004, 1.0740105895780685, 1.0742011487361587, 1.0739783356077128, 1.0739413304005805, 1.073903160809981, 1.0738113613344071, 1.0738266895439101, 1.0734618044487014, 1.0735314000068994, 1.0735181480952112, 1.0738412340074104, 1.0736441955429328, 1.0739374793285708, 1.0735384889696657, 1.0735085113337397, 1.073563834431235, 1.07350726235329, 1.0734379703748886, 1.0735743250934984, 1.0737754399281998, 1.0734854003976748, 1.07358511830747, 1.0734658019009067, 1.0736760724496548, 1.0735123166313407, 1.073529519633346, 1.0735088477634063, 1.073425754776236, 1.073335609494783, 1.0731274548007723, 1.0734531845645003, 1.0736265128642872, 1.0731574949053033, 1.0729105927371392, 1.0734452885034393, 1.0729823289221072, 1.0725227563043394, 1.0721617992408956, 1.0721817601143213, 1.0726352004789474, 1.0721602557865746, 1.0728341087423556, 1.0719766685850076, 1.0755263332958338, 1.0738452894731714, 1.0742597083536263, 1.073317027679459, 1.07415361776489, 1.0736600704506438, 1.0740660445646093, 1.0739795507102041, 1.0734818072779222, 1.0734521799263768, 1.073416503156235, 1.0734226929333666, 1.0733200160874479, 1.0733929118581376, 1.073131725822386, 1.0730779746229888, 1.0729472878532489, 1.0736607297000453, 1.073297589123861, 1.074049472221359, 1.074287200463626, 1.0742912590626084, 1.0741810920057355, 1.073653846550771, 1.0738452684707955, 1.073436383838771, 1.0735818061495708, 1.0733132701145305, 1.073080596150314, 1.0732192431142442, 1.0731917986879604, 1.0729911209866252, 1.0727970082167482, 1.0729861602156558, 1.0729333293511392, 1.0728109532068397, 1.0729975953973538, 1.0724482081019657, 1.0726819716439844, 1.072882916403502, 1.072541342574713, 1.0722706302235503, 1.0724292158835722, 1.0722739643140005, 1.0726024676152568, 1.072154840845347, 1.0718543537343552, 1.0721088836814834, 1.0719992066800472, 1.0721856731653703, 1.072414751806788, 1.0722002067115517, 1.0720703408458638, 1.0722120641438133, 1.0715836389843199, 1.0720455153522062, 1.0713527329893329, 1.0713979292699198, 1.0714819719169664, 1.0711561682287918, 1.0713853138428204, 1.072264452538696, 1.0718975393189543, 1.0725353631150796, 1.071243370729795, 1.0711888587450344, 1.0729903417446285, 1.0729316191996392, 1.071262616641223, 1.0719853895156046, 1.0715240964654535, 1.071600294847508, 1.0710573172422404, 1.07079083806925, 1.0715327935541925, 1.0713710657624982, 1.0710993394714605, 1.0712819363302275, 1.0703718415031198, 1.070908543950968, 1.0704987078476735, 1.072772549603754, 1.0711854814259179, 1.0707280105144337, 1.0713222245660896, 1.070024278863989, 1.069964514963436, 1.0695866502530766, 1.0700280087194893, 1.0700847852890987, 1.0712038309422363, 1.0722039574225581, 1.0716933767408805, 1.0711047067289725, 1.0701247002309842, 1.0701138238887278, 1.0695275389927859, 1.0735375966869096, 1.0738336802997628, 1.0707583257549842, 1.0703368467961494, 1.0702579764125284, 1.069749121401589, 1.0697067590709584, 1.0685106876694446, 1.0698809087154066, 1.0683027925432584, 1.068083447009876, 1.0690469253724115, 1.070190309694905, 1.068551745111203, 1.0671048244656478, 1.0678701819335656, 1.0680575471149578, 1.070153713275275, 1.0696812301690573, 1.0704935565866238, 1.0692846933184708, 1.0699244438010809, 1.0702422163569707, 1.0707804321508387, 1.0713729253295021, 1.0707599658985647, 1.069641694249069, 1.0693554206550488, 1.0701437825784545, 1.070334975430608, 1.0694313859058358, 1.0686656378622663, 1.0682427352948356, 1.0681652336884329, 1.068963272909364, 1.068454814348867, 1.0694740663074125, 1.0701833032729444, 1.069015742621138, 1.071000910735473, 1.070065756793874, 1.0704631040718031, 1.0689392242098736, 1.0689337215384418, 1.0686979738838618, 1.0697566777039358, 1.0695515987075086, 1.0699202720634258, 1.0712116409376171, 1.0729401452830196, 1.0725012221620314, 1.0712615876716756, 1.0709283650043808, 1.0695175249718543, 1.0693355058497718, 1.068833126669302, 1.069032655751191, 1.0702417414291194, 1.0703312950702173, 1.0718888095761716, 1.0690714668199512, 1.0839249955065686, 1.0725940063503978, 1.0752971253111132, 1.0700760501610915, 1.0697239277054398, 1.0695275608763803, 1.069151793417255, 1.069886089203539, 1.069496864410886, 1.069331616346841, 1.0691211030713341, 1.0695255028393724, 1.0675974988349899, 1.0680091102999583, 1.0673658775842654, 1.0672587130838351, 1.0666181107566097, 1.0680188039000273, 1.0710988202868545, 1.0702916502218227, 1.0667301713074013, 1.0667740577545008, 1.0686170147184963, 1.0708738493478762, 1.0710453180806592, 1.0703184150817213, 1.06859643904825, 1.0679887603685352, 1.0680072905346598, 1.067013313099589, 1.0668411076191269, 1.0679302003594149, 1.066293398457631, 1.066354548025425, 1.0658712224549092, 1.0651043558022815, 1.0650815446273991, 1.065457528621509, 1.067208802137042, 1.0660471010991435, 1.0659808054596982, 1.0653586667665955, 1.066258233430694, 1.0645166590962811, 1.0645582317082054, 1.0644901719181445, 1.0657497881863887, 1.0667536126514725, 1.0670632453424975, 1.0685107519004868, 1.066771580012672, 1.067506885479608, 1.0674970432473403, 1.0657241992637116, 1.0660031248656632, 1.0670542723344336, 1.0654111169936475, 1.0655954414324593, 1.065582805590463, 1.0643520148138246, 1.0653750755948452, 1.0650800922323302, 1.06526663249523, 1.0644110095084816, 1.0652897209602215, 1.0663470912518198, 1.0662647585114904, 1.0654381030883633, 1.0660673024473248, 1.065092297843839, 1.0652020889630798, 1.064993395991394, 1.0648233386770165, 1.0647723349702432], 'acc': [0.3186858302269139, 0.37289527538375933, 0.37618069675668797, 0.3946611932905303, 0.39425051171921605, 0.3942505141058497, 0.3942505156724605, 0.3942505125392389, 0.3942505152808078, 0.3942505148891551, 0.3942505123066951, 0.3942505133225443, 0.39466118914145953, 0.394661189178177, 0.3942505129308916, 0.3942505133225443, 0.3942505148891551, 0.3942505121475862, 0.39425051508498143, 0.39425051171921605, 0.3942505152808078, 0.39425051391002336, 0.3942505113275634, 0.3942505129308916, 0.39383983805439066, 0.3942505141058497, 0.3942505133225443, 0.39425051113173704, 0.3987679663012894, 0.3946611927030512, 0.394250514301676, 0.3942505133225443, 0.3954825481350172, 0.39589322536143434, 0.398357288879046, 0.39548254516090453, 0.3942505146933287, 0.39507186992946836, 0.39958932431571537, 0.3958932220323864, 0.39548254578510106, 0.39671458138087934, 0.4012320334172102, 0.399178644898491, 0.39507186636787667, 0.39753593661701897, 0.3926078032052003, 0.3954825471558855, 0.39630389941791244, 0.39342915808640466, 0.3950718685586839, 0.3979466134517834, 0.39548254778008196, 0.39835728911158974, 0.4016427098603219, 0.39589322418647627, 0.3909650933571175, 0.3934291590655364, 0.3991786451310348, 0.39301848223077196, 0.3946611927030512, 0.39507186699207314, 0.39219712699463233, 0.40739219886321554, 0.40944558675773823, 0.39917864532686115, 0.40082135736575114, 0.39589322575308705, 0.3901437377293252, 0.3967145776234613, 0.3946611932905303, 0.40123203083475023, 0.385215604941703, 0.393018479060833, 0.39712525582901015, 0.39753593344708, 0.40041067618608966, 0.4049281299848576, 0.3991786466976456, 0.40657084343124955, 0.40780287291969364, 0.4004106754395017, 0.40328542287834374, 0.388090348464018, 0.40616016463822163, 0.3987679695079459, 0.4053388083862328, 0.38398357189166715, 0.38398357424158336, 0.40164271217352066, 0.39342915609142376, 0.39466119231139857, 0.39260780422104946, 0.3946611909406141, 0.4073921984715628, 0.39753593364290635, 0.39876796692548594, 0.3958932261447397, 0.3983572878999142, 0.3999999981763671, 0.39958932372823636, 0.40041067876854963, 0.4020533862667162, 0.4016427086486464, 0.39137576823361847, 0.40698151905433844, 0.39958932192908175, 0.4024640676422041, 0.40821355409935517, 0.3995893213416027, 0.402053387833327, 0.40574948799928356, 0.4036960981464974, 0.39301848007668216, 0.4028747436936631, 0.40205338802915336, 0.4082135511252425, 0.4073921966724082, 0.4008213532166804, 0.4036960977181272, 0.3917864469899289, 0.3979466109060409, 0.4036960967389955, 0.39383983410114626, 0.3983572902865478, 0.40821355292439704, 0.3995893225165608, 0.39178644780995175, 0.40369610030058717, 0.390554415347395, 0.39096509433624926, 0.38480492614867506, 0.3967145800100949, 0.40328542029588377, 0.38932238030237826, 0.3880903486231269, 0.4032854232699964, 0.3913757715626664, 0.40657084264794413, 0.40123203322138384, 0.39178644976821525, 0.3975359324679482, 0.40410677439378273, 0.3852156068999664, 0.3987679692754021, 0.40246406587976696, 0.40451745177930876, 0.38234086161521424, 0.4008213555665966, 0.38891170667427033, 0.39301848223077196, 0.3954825485266699, 0.40123203361303655, 0.3958932239906499, 0.392197123628867, 0.39835729048237417, 0.40328542052842753, 0.40698152183262476, 0.4016427086486464, 0.40041067739776515, 0.39917864552268745, 0.40123203377214545, 0.40657084124044224, 0.40123203122640294, 0.4041067755687408, 0.3958932221914953, 0.3806981529053721, 0.39589322536143434, 0.40205338963248155, 0.4119096530536362, 0.40739219549745015, 0.4016427086486464, 0.4008213532166804, 0.3938398366836062, 0.4008213532166804, 0.4061601658131797, 0.38603696115697433, 0.39301848203494566, 0.3843942508805214, 0.4000000003671744, 0.40082135482000864, 0.3843942499013897, 0.3954825481350172, 0.39425051211086876, 0.4004106783768969, 0.4012320327930137, 0.40246406666307233, 0.3979466101227355, 0.3971252558657276, 0.3983572893074161, 0.39096509198633306, 0.39835728989489516, 0.3991786433318802, 0.39589322340317085, 0.3987679688837494, 0.39917864552268745, 0.3975359328596009, 0.3999999985680198, 0.40082135736575114, 0.39999999938804265, 0.40164270884447273, 0.385215605529182, 0.39712525524153114, 0.3909650937487702, 0.401232034983821, 0.40410677592367605, 0.38891170526676844, 0.3991786462692754, 0.3967145794226159, 0.3991786445435557, 0.39425051547663414, 0.3971252578239911, 0.40739219592582027, 0.39260780382939675, 0.3917864464391673, 0.3930184818391193, 0.3950718705169474, 0.3930184814474666, 0.40246406447226507, 0.4024640672505514, 0.39753593367962375, 0.4004106785727233, 0.3938398359003008, 0.40533881120123655, 0.38891170370015765, 0.4045174557692706, 0.4061601658131797, 0.40698152183262476, 0.38850102784452495, 0.39137577136684004, 0.4008213563866194, 0.4004106755986106, 0.3991786457185138, 0.401232032401361, 0.3930184806641612, 0.39425051508498143, 0.3963038992220861, 0.38809034826819166, 0.3954825471558855, 0.4028747440853158, 0.40041067638191596, 0.39835728907487233, 0.4024640672505514, 0.3946611893372859, 0.4036960981464974, 0.38850102608208786, 0.40944558358779926, 0.39917864552268745, 0.40328541994094846, 0.39671457781928765, 0.40123203322138384, 0.3913757721501454, 0.40082135759829496, 0.40205338959576414, 0.39137576901692384, 0.40287474545610025, 0.4020533882616971, 0.3909650905788312, 0.39917864708929823, 0.4057494846335182, 0.4012320330255575, 0.40739219749243105, 0.3963039025878515, 0.40410677533619704, 0.40985626061839, 0.40616016205576166, 0.4094455861702592, 0.40246406705472504, 0.40574948760763085, 0.3975359330554273, 0.405338809402082, 0.39876796594635416, 0.4102669388239389, 0.3913757678052483, 0.4000000011504798, 0.39548254637258007, 0.387679672412559, 0.39178644921745365, 0.39712525484987843, 0.3987679678679002, 0.3983572882915669, 0.40246406408061236, 0.402464066467246, 0.39589322340317085, 0.397125256649033, 0.4004106767735687, 0.39958932094995003, 0.4049281335464493, 0.4065708404571369, 0.39383983527610433, 0.40616016463822163, 0.4065708408487896, 0.39958932392406266, 0.40985626081421633, 0.40369609932145545, 0.4123203267184616, 0.40698151807520666, 0.4036960981097799, 0.4016427124060645, 0.40082135380415945]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
