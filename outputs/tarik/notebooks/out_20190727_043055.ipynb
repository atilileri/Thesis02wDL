{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf27.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 04:30:55 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '3Ov', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x00000176869E9550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000017682EF6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0877, Accuracy:0.3934, Validation Loss:1.0796, Validation Accuracy:0.3924\n",
    "Epoch #2: Loss:1.0770, Accuracy:0.3938, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #3: Loss:1.0743, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.3892\n",
    "Epoch #4: Loss:1.0741, Accuracy:0.3959, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #5: Loss:1.0744, Accuracy:0.3971, Validation Loss:1.0747, Validation Accuracy:0.3727\n",
    "Epoch #6: Loss:1.0742, Accuracy:0.3963, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #7: Loss:1.0735, Accuracy:0.3971, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #8: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0740, Accuracy:0.4016, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #12: Loss:1.0740, Accuracy:0.3881, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #13: Loss:1.0734, Accuracy:0.3975, Validation Loss:1.0732, Validation Accuracy:0.3908\n",
    "Epoch #14: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.4056\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3971, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #16: Loss:1.0739, Accuracy:0.3959, Validation Loss:1.0734, Validation Accuracy:0.3974\n",
    "Epoch #17: Loss:1.0740, Accuracy:0.3963, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #18: Loss:1.0739, Accuracy:0.3988, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #19: Loss:1.0740, Accuracy:0.3988, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #20: Loss:1.0740, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.4039\n",
    "Epoch #21: Loss:1.0739, Accuracy:0.3963, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #22: Loss:1.0735, Accuracy:0.3996, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #23: Loss:1.0733, Accuracy:0.4021, Validation Loss:1.0744, Validation Accuracy:0.3990\n",
    "Epoch #24: Loss:1.0735, Accuracy:0.3914, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #25: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #26: Loss:1.0730, Accuracy:0.4049, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0734, Accuracy:0.4070, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #28: Loss:1.0738, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3990\n",
    "Epoch #29: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #30: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0735, Validation Accuracy:0.3924\n",
    "Epoch #31: Loss:1.0732, Accuracy:0.3930, Validation Loss:1.0739, Validation Accuracy:0.3957\n",
    "Epoch #32: Loss:1.0732, Accuracy:0.4016, Validation Loss:1.0740, Validation Accuracy:0.3974\n",
    "Epoch #33: Loss:1.0734, Accuracy:0.3910, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #34: Loss:1.0732, Accuracy:0.3934, Validation Loss:1.0738, Validation Accuracy:0.3957\n",
    "Epoch #35: Loss:1.0731, Accuracy:0.3959, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #36: Loss:1.0731, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #37: Loss:1.0728, Accuracy:0.3951, Validation Loss:1.0741, Validation Accuracy:0.3990\n",
    "Epoch #38: Loss:1.0729, Accuracy:0.3922, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0727, Accuracy:0.4008, Validation Loss:1.0741, Validation Accuracy:0.3892\n",
    "Epoch #40: Loss:1.0729, Accuracy:0.4025, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #41: Loss:1.0726, Accuracy:0.4021, Validation Loss:1.0737, Validation Accuracy:0.4007\n",
    "Epoch #42: Loss:1.0730, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #43: Loss:1.0730, Accuracy:0.3918, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #44: Loss:1.0725, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.4089\n",
    "Epoch #45: Loss:1.0729, Accuracy:0.4053, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #46: Loss:1.0724, Accuracy:0.4086, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #47: Loss:1.0729, Accuracy:0.3959, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #48: Loss:1.0724, Accuracy:0.3922, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #49: Loss:1.0726, Accuracy:0.3938, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #50: Loss:1.0725, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3990\n",
    "Epoch #51: Loss:1.0723, Accuracy:0.3979, Validation Loss:1.0749, Validation Accuracy:0.3974\n",
    "Epoch #52: Loss:1.0724, Accuracy:0.3963, Validation Loss:1.0750, Validation Accuracy:0.4039\n",
    "Epoch #53: Loss:1.0723, Accuracy:0.3996, Validation Loss:1.0755, Validation Accuracy:0.4007\n",
    "Epoch #54: Loss:1.0721, Accuracy:0.4057, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #55: Loss:1.0721, Accuracy:0.4123, Validation Loss:1.0762, Validation Accuracy:0.3908\n",
    "Epoch #56: Loss:1.0716, Accuracy:0.4053, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #57: Loss:1.0716, Accuracy:0.3996, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #58: Loss:1.0715, Accuracy:0.4103, Validation Loss:1.0747, Validation Accuracy:0.3810\n",
    "Epoch #59: Loss:1.0712, Accuracy:0.4086, Validation Loss:1.0748, Validation Accuracy:0.3842\n",
    "Epoch #60: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0746, Validation Accuracy:0.3974\n",
    "Epoch #61: Loss:1.0711, Accuracy:0.4078, Validation Loss:1.0754, Validation Accuracy:0.3908\n",
    "Epoch #62: Loss:1.0709, Accuracy:0.4086, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #63: Loss:1.0708, Accuracy:0.4094, Validation Loss:1.0743, Validation Accuracy:0.4072\n",
    "Epoch #64: Loss:1.0723, Accuracy:0.4025, Validation Loss:1.0756, Validation Accuracy:0.3875\n",
    "Epoch #65: Loss:1.0717, Accuracy:0.3979, Validation Loss:1.0757, Validation Accuracy:0.3826\n",
    "Epoch #66: Loss:1.0708, Accuracy:0.4099, Validation Loss:1.0761, Validation Accuracy:0.3875\n",
    "Epoch #67: Loss:1.0723, Accuracy:0.3926, Validation Loss:1.0785, Validation Accuracy:0.3727\n",
    "Epoch #68: Loss:1.0729, Accuracy:0.3885, Validation Loss:1.0786, Validation Accuracy:0.3859\n",
    "Epoch #69: Loss:1.0728, Accuracy:0.3885, Validation Loss:1.0765, Validation Accuracy:0.3777\n",
    "Epoch #70: Loss:1.0722, Accuracy:0.3996, Validation Loss:1.0759, Validation Accuracy:0.3826\n",
    "Epoch #71: Loss:1.0730, Accuracy:0.3873, Validation Loss:1.0759, Validation Accuracy:0.3744\n",
    "Epoch #72: Loss:1.0716, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #73: Loss:1.0723, Accuracy:0.4049, Validation Loss:1.0730, Validation Accuracy:0.3990\n",
    "Epoch #74: Loss:1.0725, Accuracy:0.3992, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #75: Loss:1.0715, Accuracy:0.3971, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #76: Loss:1.0724, Accuracy:0.4012, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #77: Loss:1.0711, Accuracy:0.3963, Validation Loss:1.0756, Validation Accuracy:0.3974\n",
    "Epoch #78: Loss:1.0707, Accuracy:0.4000, Validation Loss:1.0752, Validation Accuracy:0.4023\n",
    "Epoch #79: Loss:1.0701, Accuracy:0.3971, Validation Loss:1.0756, Validation Accuracy:0.3842\n",
    "Epoch #80: Loss:1.0704, Accuracy:0.4021, Validation Loss:1.0751, Validation Accuracy:0.3892\n",
    "Epoch #81: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0751, Validation Accuracy:0.4007\n",
    "Epoch #82: Loss:1.0704, Accuracy:0.4021, Validation Loss:1.0760, Validation Accuracy:0.3793\n",
    "Epoch #83: Loss:1.0701, Accuracy:0.3988, Validation Loss:1.0768, Validation Accuracy:0.3908\n",
    "Epoch #84: Loss:1.0702, Accuracy:0.4045, Validation Loss:1.0773, Validation Accuracy:0.4089\n",
    "Epoch #85: Loss:1.0709, Accuracy:0.4037, Validation Loss:1.0785, Validation Accuracy:0.3842\n",
    "Epoch #86: Loss:1.0703, Accuracy:0.4008, Validation Loss:1.0789, Validation Accuracy:0.3810\n",
    "Epoch #87: Loss:1.0700, Accuracy:0.3975, Validation Loss:1.0778, Validation Accuracy:0.3990\n",
    "Epoch #88: Loss:1.0700, Accuracy:0.4021, Validation Loss:1.0781, Validation Accuracy:0.3810\n",
    "Epoch #89: Loss:1.0703, Accuracy:0.4016, Validation Loss:1.0769, Validation Accuracy:0.3908\n",
    "Epoch #90: Loss:1.0716, Accuracy:0.4033, Validation Loss:1.0771, Validation Accuracy:0.3908\n",
    "Epoch #91: Loss:1.0710, Accuracy:0.4057, Validation Loss:1.0750, Validation Accuracy:0.3924\n",
    "Epoch #92: Loss:1.0706, Accuracy:0.3938, Validation Loss:1.0750, Validation Accuracy:0.4023\n",
    "Epoch #93: Loss:1.0704, Accuracy:0.4008, Validation Loss:1.0767, Validation Accuracy:0.3892\n",
    "Epoch #94: Loss:1.0692, Accuracy:0.4025, Validation Loss:1.0776, Validation Accuracy:0.3974\n",
    "Epoch #95: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0773, Validation Accuracy:0.4056\n",
    "Epoch #96: Loss:1.0719, Accuracy:0.4012, Validation Loss:1.0776, Validation Accuracy:0.4039\n",
    "Epoch #97: Loss:1.0721, Accuracy:0.3984, Validation Loss:1.0779, Validation Accuracy:0.3974\n",
    "Epoch #98: Loss:1.0711, Accuracy:0.3988, Validation Loss:1.0772, Validation Accuracy:0.4072\n",
    "Epoch #99: Loss:1.0707, Accuracy:0.4045, Validation Loss:1.0769, Validation Accuracy:0.4105\n",
    "Epoch #100: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0769, Validation Accuracy:0.4023\n",
    "Epoch #101: Loss:1.0695, Accuracy:0.4111, Validation Loss:1.0767, Validation Accuracy:0.4089\n",
    "Epoch #102: Loss:1.0699, Accuracy:0.4049, Validation Loss:1.0769, Validation Accuracy:0.4105\n",
    "Epoch #103: Loss:1.0693, Accuracy:0.4025, Validation Loss:1.0773, Validation Accuracy:0.4023\n",
    "Epoch #104: Loss:1.0700, Accuracy:0.3971, Validation Loss:1.0745, Validation Accuracy:0.4072\n",
    "Epoch #105: Loss:1.0702, Accuracy:0.4094, Validation Loss:1.0753, Validation Accuracy:0.3957\n",
    "Epoch #106: Loss:1.0702, Accuracy:0.4074, Validation Loss:1.0743, Validation Accuracy:0.4089\n",
    "Epoch #107: Loss:1.0695, Accuracy:0.4127, Validation Loss:1.0761, Validation Accuracy:0.4039\n",
    "Epoch #108: Loss:1.0699, Accuracy:0.4033, Validation Loss:1.0749, Validation Accuracy:0.4039\n",
    "Epoch #109: Loss:1.0707, Accuracy:0.4025, Validation Loss:1.0749, Validation Accuracy:0.3990\n",
    "Epoch #110: Loss:1.0714, Accuracy:0.4123, Validation Loss:1.0746, Validation Accuracy:0.4039\n",
    "Epoch #111: Loss:1.0712, Accuracy:0.4070, Validation Loss:1.0742, Validation Accuracy:0.3924\n",
    "Epoch #112: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0737, Validation Accuracy:0.3957\n",
    "Epoch #113: Loss:1.0706, Accuracy:0.4012, Validation Loss:1.0741, Validation Accuracy:0.3875\n",
    "Epoch #114: Loss:1.0708, Accuracy:0.3992, Validation Loss:1.0761, Validation Accuracy:0.3859\n",
    "Epoch #115: Loss:1.0711, Accuracy:0.4021, Validation Loss:1.0773, Validation Accuracy:0.3957\n",
    "Epoch #116: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0799, Validation Accuracy:0.3760\n",
    "Epoch #117: Loss:1.0712, Accuracy:0.3992, Validation Loss:1.0791, Validation Accuracy:0.3826\n",
    "Epoch #118: Loss:1.0706, Accuracy:0.4025, Validation Loss:1.0776, Validation Accuracy:0.3974\n",
    "Epoch #119: Loss:1.0696, Accuracy:0.4115, Validation Loss:1.0763, Validation Accuracy:0.3941\n",
    "Epoch #120: Loss:1.0708, Accuracy:0.3979, Validation Loss:1.0767, Validation Accuracy:0.3974\n",
    "Epoch #121: Loss:1.0695, Accuracy:0.4062, Validation Loss:1.0770, Validation Accuracy:0.3908\n",
    "Epoch #122: Loss:1.0695, Accuracy:0.4070, Validation Loss:1.0765, Validation Accuracy:0.3924\n",
    "Epoch #123: Loss:1.0697, Accuracy:0.4041, Validation Loss:1.0761, Validation Accuracy:0.3908\n",
    "Epoch #124: Loss:1.0695, Accuracy:0.4074, Validation Loss:1.0759, Validation Accuracy:0.3974\n",
    "Epoch #125: Loss:1.0701, Accuracy:0.4045, Validation Loss:1.0767, Validation Accuracy:0.3957\n",
    "Epoch #126: Loss:1.0701, Accuracy:0.4029, Validation Loss:1.0765, Validation Accuracy:0.4089\n",
    "Epoch #127: Loss:1.0713, Accuracy:0.4000, Validation Loss:1.0768, Validation Accuracy:0.4105\n",
    "Epoch #128: Loss:1.0723, Accuracy:0.3947, Validation Loss:1.0760, Validation Accuracy:0.3974\n",
    "Epoch #129: Loss:1.0713, Accuracy:0.4000, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #130: Loss:1.0710, Accuracy:0.3914, Validation Loss:1.0762, Validation Accuracy:0.3974\n",
    "Epoch #131: Loss:1.0702, Accuracy:0.4004, Validation Loss:1.0760, Validation Accuracy:0.4056\n",
    "Epoch #132: Loss:1.0699, Accuracy:0.3988, Validation Loss:1.0756, Validation Accuracy:0.4072\n",
    "Epoch #133: Loss:1.0694, Accuracy:0.4037, Validation Loss:1.0755, Validation Accuracy:0.3990\n",
    "Epoch #134: Loss:1.0696, Accuracy:0.4053, Validation Loss:1.0759, Validation Accuracy:0.3990\n",
    "Epoch #135: Loss:1.0696, Accuracy:0.4000, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #136: Loss:1.0693, Accuracy:0.3992, Validation Loss:1.0754, Validation Accuracy:0.3974\n",
    "Epoch #137: Loss:1.0689, Accuracy:0.4070, Validation Loss:1.0758, Validation Accuracy:0.4056\n",
    "Epoch #138: Loss:1.0687, Accuracy:0.4078, Validation Loss:1.0752, Validation Accuracy:0.3957\n",
    "Epoch #139: Loss:1.0694, Accuracy:0.4016, Validation Loss:1.0754, Validation Accuracy:0.4039\n",
    "Epoch #140: Loss:1.0694, Accuracy:0.4119, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #141: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0733, Validation Accuracy:0.3974\n",
    "Epoch #142: Loss:1.0707, Accuracy:0.4012, Validation Loss:1.0743, Validation Accuracy:0.4122\n",
    "Epoch #143: Loss:1.0702, Accuracy:0.4000, Validation Loss:1.0763, Validation Accuracy:0.3990\n",
    "Epoch #144: Loss:1.0702, Accuracy:0.4099, Validation Loss:1.0770, Validation Accuracy:0.3908\n",
    "Epoch #145: Loss:1.0697, Accuracy:0.4119, Validation Loss:1.0763, Validation Accuracy:0.4007\n",
    "Epoch #146: Loss:1.0697, Accuracy:0.3996, Validation Loss:1.0762, Validation Accuracy:0.3875\n",
    "Epoch #147: Loss:1.0691, Accuracy:0.4053, Validation Loss:1.0780, Validation Accuracy:0.3859\n",
    "Epoch #148: Loss:1.0690, Accuracy:0.4000, Validation Loss:1.0774, Validation Accuracy:0.3990\n",
    "Epoch #149: Loss:1.0689, Accuracy:0.4099, Validation Loss:1.0776, Validation Accuracy:0.3892\n",
    "Epoch #150: Loss:1.0694, Accuracy:0.4078, Validation Loss:1.0772, Validation Accuracy:0.3892\n",
    "Epoch #151: Loss:1.0692, Accuracy:0.4066, Validation Loss:1.0780, Validation Accuracy:0.3875\n",
    "Epoch #152: Loss:1.0694, Accuracy:0.4070, Validation Loss:1.0783, Validation Accuracy:0.3842\n",
    "Epoch #153: Loss:1.0694, Accuracy:0.4049, Validation Loss:1.0786, Validation Accuracy:0.3859\n",
    "Epoch #154: Loss:1.0689, Accuracy:0.4099, Validation Loss:1.0790, Validation Accuracy:0.3826\n",
    "Epoch #155: Loss:1.0691, Accuracy:0.4000, Validation Loss:1.0794, Validation Accuracy:0.3760\n",
    "Epoch #156: Loss:1.0695, Accuracy:0.4078, Validation Loss:1.0790, Validation Accuracy:0.3924\n",
    "Epoch #157: Loss:1.0702, Accuracy:0.4045, Validation Loss:1.0776, Validation Accuracy:0.3957\n",
    "Epoch #158: Loss:1.0723, Accuracy:0.3869, Validation Loss:1.0779, Validation Accuracy:0.3842\n",
    "Epoch #159: Loss:1.0734, Accuracy:0.4000, Validation Loss:1.0786, Validation Accuracy:0.3859\n",
    "Epoch #160: Loss:1.0722, Accuracy:0.4004, Validation Loss:1.0779, Validation Accuracy:0.3842\n",
    "Epoch #161: Loss:1.0742, Accuracy:0.3889, Validation Loss:1.0768, Validation Accuracy:0.3826\n",
    "Epoch #162: Loss:1.0722, Accuracy:0.3959, Validation Loss:1.0775, Validation Accuracy:0.3793\n",
    "Epoch #163: Loss:1.0721, Accuracy:0.3971, Validation Loss:1.0782, Validation Accuracy:0.3875\n",
    "Epoch #164: Loss:1.0714, Accuracy:0.3984, Validation Loss:1.0776, Validation Accuracy:0.3760\n",
    "Epoch #165: Loss:1.0708, Accuracy:0.3934, Validation Loss:1.0777, Validation Accuracy:0.3842\n",
    "Epoch #166: Loss:1.0706, Accuracy:0.3959, Validation Loss:1.0778, Validation Accuracy:0.3842\n",
    "Epoch #167: Loss:1.0713, Accuracy:0.3996, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #168: Loss:1.0718, Accuracy:0.3992, Validation Loss:1.0774, Validation Accuracy:0.3744\n",
    "Epoch #169: Loss:1.0708, Accuracy:0.3967, Validation Loss:1.0765, Validation Accuracy:0.3875\n",
    "Epoch #170: Loss:1.0711, Accuracy:0.3910, Validation Loss:1.0766, Validation Accuracy:0.3875\n",
    "Epoch #171: Loss:1.0709, Accuracy:0.3918, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #172: Loss:1.0706, Accuracy:0.3930, Validation Loss:1.0778, Validation Accuracy:0.3777\n",
    "Epoch #173: Loss:1.0699, Accuracy:0.3930, Validation Loss:1.0783, Validation Accuracy:0.3875\n",
    "Epoch #174: Loss:1.0701, Accuracy:0.3947, Validation Loss:1.0769, Validation Accuracy:0.3908\n",
    "Epoch #175: Loss:1.0720, Accuracy:0.3893, Validation Loss:1.0778, Validation Accuracy:0.3859\n",
    "Epoch #176: Loss:1.0704, Accuracy:0.3922, Validation Loss:1.0783, Validation Accuracy:0.3892\n",
    "Epoch #177: Loss:1.0695, Accuracy:0.3984, Validation Loss:1.0787, Validation Accuracy:0.3908\n",
    "Epoch #178: Loss:1.0687, Accuracy:0.3984, Validation Loss:1.0791, Validation Accuracy:0.3892\n",
    "Epoch #179: Loss:1.0697, Accuracy:0.3988, Validation Loss:1.0799, Validation Accuracy:0.3859\n",
    "Epoch #180: Loss:1.0693, Accuracy:0.3988, Validation Loss:1.0789, Validation Accuracy:0.3859\n",
    "Epoch #181: Loss:1.0690, Accuracy:0.4045, Validation Loss:1.0797, Validation Accuracy:0.3842\n",
    "Epoch #182: Loss:1.0694, Accuracy:0.3988, Validation Loss:1.0784, Validation Accuracy:0.3974\n",
    "Epoch #183: Loss:1.0704, Accuracy:0.4000, Validation Loss:1.0785, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0701, Accuracy:0.3951, Validation Loss:1.0795, Validation Accuracy:0.3957\n",
    "Epoch #185: Loss:1.0702, Accuracy:0.3988, Validation Loss:1.0786, Validation Accuracy:0.3892\n",
    "Epoch #186: Loss:1.0717, Accuracy:0.3938, Validation Loss:1.0791, Validation Accuracy:0.3941\n",
    "Epoch #187: Loss:1.0690, Accuracy:0.4012, Validation Loss:1.0801, Validation Accuracy:0.3859\n",
    "Epoch #188: Loss:1.0695, Accuracy:0.3967, Validation Loss:1.0785, Validation Accuracy:0.3908\n",
    "Epoch #189: Loss:1.0700, Accuracy:0.4057, Validation Loss:1.0788, Validation Accuracy:0.3908\n",
    "Epoch #190: Loss:1.0684, Accuracy:0.4025, Validation Loss:1.0791, Validation Accuracy:0.3859\n",
    "Epoch #191: Loss:1.0683, Accuracy:0.3947, Validation Loss:1.0800, Validation Accuracy:0.3859\n",
    "Epoch #192: Loss:1.0677, Accuracy:0.4045, Validation Loss:1.0805, Validation Accuracy:0.3793\n",
    "Epoch #193: Loss:1.0682, Accuracy:0.4008, Validation Loss:1.0787, Validation Accuracy:0.3842\n",
    "Epoch #194: Loss:1.0675, Accuracy:0.4004, Validation Loss:1.0787, Validation Accuracy:0.3875\n",
    "Epoch #195: Loss:1.0679, Accuracy:0.3975, Validation Loss:1.0808, Validation Accuracy:0.3875\n",
    "Epoch #196: Loss:1.0681, Accuracy:0.3996, Validation Loss:1.0818, Validation Accuracy:0.3892\n",
    "Epoch #197: Loss:1.0689, Accuracy:0.4123, Validation Loss:1.0818, Validation Accuracy:0.3793\n",
    "Epoch #198: Loss:1.0687, Accuracy:0.4041, Validation Loss:1.0821, Validation Accuracy:0.3777\n",
    "Epoch #199: Loss:1.0678, Accuracy:0.4066, Validation Loss:1.0803, Validation Accuracy:0.3727\n",
    "Epoch #200: Loss:1.0698, Accuracy:0.4004, Validation Loss:1.0810, Validation Accuracy:0.3760\n",
    "Epoch #201: Loss:1.0713, Accuracy:0.3914, Validation Loss:1.0812, Validation Accuracy:0.3810\n",
    "Epoch #202: Loss:1.0690, Accuracy:0.4041, Validation Loss:1.0814, Validation Accuracy:0.3892\n",
    "Epoch #203: Loss:1.0689, Accuracy:0.3996, Validation Loss:1.0794, Validation Accuracy:0.3842\n",
    "Epoch #204: Loss:1.0685, Accuracy:0.4021, Validation Loss:1.0786, Validation Accuracy:0.3842\n",
    "Epoch #205: Loss:1.0677, Accuracy:0.4012, Validation Loss:1.0789, Validation Accuracy:0.3678\n",
    "Epoch #206: Loss:1.0679, Accuracy:0.4021, Validation Loss:1.0781, Validation Accuracy:0.3777\n",
    "Epoch #207: Loss:1.0677, Accuracy:0.4016, Validation Loss:1.0794, Validation Accuracy:0.3842\n",
    "Epoch #208: Loss:1.0693, Accuracy:0.4025, Validation Loss:1.0777, Validation Accuracy:0.3711\n",
    "Epoch #209: Loss:1.0692, Accuracy:0.3992, Validation Loss:1.0784, Validation Accuracy:0.3760\n",
    "Epoch #210: Loss:1.0693, Accuracy:0.4029, Validation Loss:1.0779, Validation Accuracy:0.3744\n",
    "Epoch #211: Loss:1.0689, Accuracy:0.3959, Validation Loss:1.0793, Validation Accuracy:0.3908\n",
    "Epoch #212: Loss:1.0693, Accuracy:0.3971, Validation Loss:1.0784, Validation Accuracy:0.3760\n",
    "Epoch #213: Loss:1.0701, Accuracy:0.3979, Validation Loss:1.0788, Validation Accuracy:0.3744\n",
    "Epoch #214: Loss:1.0689, Accuracy:0.3988, Validation Loss:1.0786, Validation Accuracy:0.3826\n",
    "Epoch #215: Loss:1.0691, Accuracy:0.4012, Validation Loss:1.0781, Validation Accuracy:0.3842\n",
    "Epoch #216: Loss:1.0682, Accuracy:0.3996, Validation Loss:1.0788, Validation Accuracy:0.3793\n",
    "Epoch #217: Loss:1.0683, Accuracy:0.3996, Validation Loss:1.0770, Validation Accuracy:0.3826\n",
    "Epoch #218: Loss:1.0689, Accuracy:0.3967, Validation Loss:1.0778, Validation Accuracy:0.3826\n",
    "Epoch #219: Loss:1.0677, Accuracy:0.4070, Validation Loss:1.0766, Validation Accuracy:0.3793\n",
    "Epoch #220: Loss:1.0685, Accuracy:0.3947, Validation Loss:1.0785, Validation Accuracy:0.3760\n",
    "Epoch #221: Loss:1.0680, Accuracy:0.4033, Validation Loss:1.0786, Validation Accuracy:0.3760\n",
    "Epoch #222: Loss:1.0679, Accuracy:0.4049, Validation Loss:1.0791, Validation Accuracy:0.3842\n",
    "Epoch #223: Loss:1.0682, Accuracy:0.4103, Validation Loss:1.0791, Validation Accuracy:0.3744\n",
    "Epoch #224: Loss:1.0669, Accuracy:0.4041, Validation Loss:1.0793, Validation Accuracy:0.3645\n",
    "Epoch #225: Loss:1.0666, Accuracy:0.4070, Validation Loss:1.0790, Validation Accuracy:0.3760\n",
    "Epoch #226: Loss:1.0657, Accuracy:0.4041, Validation Loss:1.0806, Validation Accuracy:0.3727\n",
    "Epoch #227: Loss:1.0665, Accuracy:0.4000, Validation Loss:1.0797, Validation Accuracy:0.3859\n",
    "Epoch #228: Loss:1.0656, Accuracy:0.4033, Validation Loss:1.0804, Validation Accuracy:0.3760\n",
    "Epoch #229: Loss:1.0660, Accuracy:0.3938, Validation Loss:1.0804, Validation Accuracy:0.3744\n",
    "Epoch #230: Loss:1.0662, Accuracy:0.4025, Validation Loss:1.0813, Validation Accuracy:0.3810\n",
    "Epoch #231: Loss:1.0658, Accuracy:0.4127, Validation Loss:1.0812, Validation Accuracy:0.3777\n",
    "Epoch #232: Loss:1.0652, Accuracy:0.4136, Validation Loss:1.0811, Validation Accuracy:0.3990\n",
    "Epoch #233: Loss:1.0654, Accuracy:0.4111, Validation Loss:1.0804, Validation Accuracy:0.3908\n",
    "Epoch #234: Loss:1.0663, Accuracy:0.4127, Validation Loss:1.0806, Validation Accuracy:0.3908\n",
    "Epoch #235: Loss:1.0652, Accuracy:0.4062, Validation Loss:1.0802, Validation Accuracy:0.3826\n",
    "Epoch #236: Loss:1.0654, Accuracy:0.4041, Validation Loss:1.0802, Validation Accuracy:0.3908\n",
    "Epoch #237: Loss:1.0649, Accuracy:0.4066, Validation Loss:1.0789, Validation Accuracy:0.3760\n",
    "Epoch #238: Loss:1.0667, Accuracy:0.4074, Validation Loss:1.0777, Validation Accuracy:0.3842\n",
    "Epoch #239: Loss:1.0674, Accuracy:0.4074, Validation Loss:1.0794, Validation Accuracy:0.3859\n",
    "Epoch #240: Loss:1.0671, Accuracy:0.4029, Validation Loss:1.0788, Validation Accuracy:0.3810\n",
    "Epoch #241: Loss:1.0656, Accuracy:0.4099, Validation Loss:1.0769, Validation Accuracy:0.3826\n",
    "Epoch #242: Loss:1.0653, Accuracy:0.4029, Validation Loss:1.0759, Validation Accuracy:0.3875\n",
    "Epoch #243: Loss:1.0648, Accuracy:0.4078, Validation Loss:1.0757, Validation Accuracy:0.3990\n",
    "Epoch #244: Loss:1.0667, Accuracy:0.4057, Validation Loss:1.0780, Validation Accuracy:0.3810\n",
    "Epoch #245: Loss:1.0664, Accuracy:0.4094, Validation Loss:1.0763, Validation Accuracy:0.3875\n",
    "Epoch #246: Loss:1.0654, Accuracy:0.4004, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #247: Loss:1.0647, Accuracy:0.4094, Validation Loss:1.0772, Validation Accuracy:0.3875\n",
    "Epoch #248: Loss:1.0660, Accuracy:0.4029, Validation Loss:1.0780, Validation Accuracy:0.3908\n",
    "Epoch #249: Loss:1.0639, Accuracy:0.4033, Validation Loss:1.0793, Validation Accuracy:0.3859\n",
    "Epoch #250: Loss:1.0649, Accuracy:0.4041, Validation Loss:1.0771, Validation Accuracy:0.3777\n",
    "Epoch #251: Loss:1.0640, Accuracy:0.4094, Validation Loss:1.0813, Validation Accuracy:0.3924\n",
    "Epoch #252: Loss:1.0640, Accuracy:0.4029, Validation Loss:1.0793, Validation Accuracy:0.3826\n",
    "Epoch #253: Loss:1.0629, Accuracy:0.4119, Validation Loss:1.0762, Validation Accuracy:0.3924\n",
    "Epoch #254: Loss:1.0642, Accuracy:0.4049, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #255: Loss:1.0643, Accuracy:0.4053, Validation Loss:1.0791, Validation Accuracy:0.3678\n",
    "Epoch #256: Loss:1.0639, Accuracy:0.3967, Validation Loss:1.0766, Validation Accuracy:0.3941\n",
    "Epoch #257: Loss:1.0644, Accuracy:0.4111, Validation Loss:1.0767, Validation Accuracy:0.3957\n",
    "Epoch #258: Loss:1.0652, Accuracy:0.3926, Validation Loss:1.0775, Validation Accuracy:0.3777\n",
    "Epoch #259: Loss:1.0665, Accuracy:0.4066, Validation Loss:1.0770, Validation Accuracy:0.3974\n",
    "Epoch #260: Loss:1.0641, Accuracy:0.4045, Validation Loss:1.0789, Validation Accuracy:0.3941\n",
    "Epoch #261: Loss:1.0656, Accuracy:0.4066, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #262: Loss:1.0668, Accuracy:0.3832, Validation Loss:1.0767, Validation Accuracy:0.3842\n",
    "Epoch #263: Loss:1.0657, Accuracy:0.4057, Validation Loss:1.0793, Validation Accuracy:0.3859\n",
    "Epoch #264: Loss:1.0661, Accuracy:0.4025, Validation Loss:1.0790, Validation Accuracy:0.3777\n",
    "Epoch #265: Loss:1.0667, Accuracy:0.4012, Validation Loss:1.0795, Validation Accuracy:0.3924\n",
    "Epoch #266: Loss:1.0648, Accuracy:0.4090, Validation Loss:1.0807, Validation Accuracy:0.4007\n",
    "Epoch #267: Loss:1.0656, Accuracy:0.4090, Validation Loss:1.0800, Validation Accuracy:0.3990\n",
    "Epoch #268: Loss:1.0652, Accuracy:0.4164, Validation Loss:1.0787, Validation Accuracy:0.3892\n",
    "Epoch #269: Loss:1.0648, Accuracy:0.4078, Validation Loss:1.0788, Validation Accuracy:0.4023\n",
    "Epoch #270: Loss:1.0659, Accuracy:0.4070, Validation Loss:1.0792, Validation Accuracy:0.3859\n",
    "Epoch #271: Loss:1.0646, Accuracy:0.4094, Validation Loss:1.0794, Validation Accuracy:0.3974\n",
    "Epoch #272: Loss:1.0645, Accuracy:0.4004, Validation Loss:1.0821, Validation Accuracy:0.3826\n",
    "Epoch #273: Loss:1.0640, Accuracy:0.4082, Validation Loss:1.0805, Validation Accuracy:0.3924\n",
    "Epoch #274: Loss:1.0641, Accuracy:0.4037, Validation Loss:1.0806, Validation Accuracy:0.3826\n",
    "Epoch #275: Loss:1.0641, Accuracy:0.4160, Validation Loss:1.0811, Validation Accuracy:0.3941\n",
    "Epoch #276: Loss:1.0631, Accuracy:0.4119, Validation Loss:1.0826, Validation Accuracy:0.3826\n",
    "Epoch #277: Loss:1.0630, Accuracy:0.4144, Validation Loss:1.0815, Validation Accuracy:0.3859\n",
    "Epoch #278: Loss:1.0630, Accuracy:0.4164, Validation Loss:1.0798, Validation Accuracy:0.3842\n",
    "Epoch #279: Loss:1.0628, Accuracy:0.4103, Validation Loss:1.0794, Validation Accuracy:0.3826\n",
    "Epoch #280: Loss:1.0636, Accuracy:0.4078, Validation Loss:1.0800, Validation Accuracy:0.3924\n",
    "Epoch #281: Loss:1.0625, Accuracy:0.4197, Validation Loss:1.0802, Validation Accuracy:0.3826\n",
    "Epoch #282: Loss:1.0630, Accuracy:0.4164, Validation Loss:1.0788, Validation Accuracy:0.3941\n",
    "Epoch #283: Loss:1.0617, Accuracy:0.4152, Validation Loss:1.0797, Validation Accuracy:0.3842\n",
    "Epoch #284: Loss:1.0627, Accuracy:0.4012, Validation Loss:1.0800, Validation Accuracy:0.3826\n",
    "Epoch #285: Loss:1.0637, Accuracy:0.4119, Validation Loss:1.0798, Validation Accuracy:0.3842\n",
    "Epoch #286: Loss:1.0636, Accuracy:0.4078, Validation Loss:1.0794, Validation Accuracy:0.3859\n",
    "Epoch #287: Loss:1.0609, Accuracy:0.4074, Validation Loss:1.0821, Validation Accuracy:0.3826\n",
    "Epoch #288: Loss:1.0634, Accuracy:0.3988, Validation Loss:1.0785, Validation Accuracy:0.3924\n",
    "Epoch #289: Loss:1.0616, Accuracy:0.4144, Validation Loss:1.0815, Validation Accuracy:0.3859\n",
    "Epoch #290: Loss:1.0626, Accuracy:0.4070, Validation Loss:1.0820, Validation Accuracy:0.4007\n",
    "Epoch #291: Loss:1.0610, Accuracy:0.4070, Validation Loss:1.0842, Validation Accuracy:0.4056\n",
    "Epoch #292: Loss:1.0616, Accuracy:0.4025, Validation Loss:1.0841, Validation Accuracy:0.4122\n",
    "Epoch #293: Loss:1.0602, Accuracy:0.4107, Validation Loss:1.0846, Validation Accuracy:0.3941\n",
    "Epoch #294: Loss:1.0632, Accuracy:0.4025, Validation Loss:1.0846, Validation Accuracy:0.4089\n",
    "Epoch #295: Loss:1.0626, Accuracy:0.4049, Validation Loss:1.0859, Validation Accuracy:0.3908\n",
    "Epoch #296: Loss:1.0627, Accuracy:0.4115, Validation Loss:1.0820, Validation Accuracy:0.3924\n",
    "Epoch #297: Loss:1.0611, Accuracy:0.4103, Validation Loss:1.0790, Validation Accuracy:0.3957\n",
    "Epoch #298: Loss:1.0620, Accuracy:0.4111, Validation Loss:1.0788, Validation Accuracy:0.4072\n",
    "Epoch #299: Loss:1.0624, Accuracy:0.4119, Validation Loss:1.0837, Validation Accuracy:0.4056\n",
    "Epoch #300: Loss:1.0625, Accuracy:0.4082, Validation Loss:1.0817, Validation Accuracy:0.3826\n",
    "\n",
    "Test:\n",
    "Test Loss:1.08167768, Accuracy:0.3826\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02  03\n",
    "t:01  185  52   3\n",
    "t:02  179  46   2\n",
    "t:03  107  33   2\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.39      0.77      0.52       240\n",
    "          02       0.35      0.20      0.26       227\n",
    "          03       0.29      0.01      0.03       142\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.34      0.33      0.27       609\n",
    "weighted avg       0.35      0.38      0.31       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 05:11:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 52 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0795678520829024, 1.0744183836703622, 1.0743273214753626, 1.0746477435179336, 1.0746801424104788, 1.0743199017247542, 1.0736090455736433, 1.0742023400289475, 1.0739060861528018, 1.0741086241059703, 1.0732525728019, 1.0734772190867583, 1.0731562785131394, 1.0734274538084008, 1.0738336807009818, 1.0734256693882307, 1.073806248470676, 1.074036176177277, 1.0741794637858575, 1.074222236822783, 1.0745712119370259, 1.0742365333247068, 1.0744330000211844, 1.0742062527949392, 1.073237023525833, 1.073003527370384, 1.0746126378502556, 1.0742407271818966, 1.073883388038535, 1.0735319373251377, 1.07386797578464, 1.0739930764403445, 1.0739059158538167, 1.0738142376462814, 1.0739272235845305, 1.073976545302543, 1.0740875990324223, 1.0739900605823411, 1.074073202308567, 1.0738566510978786, 1.073683680767692, 1.073151698057679, 1.0735239777071723, 1.0739199066005514, 1.074547637076605, 1.0744660540754571, 1.074281521031422, 1.073977690612154, 1.0741484404001722, 1.0747064837485503, 1.074899068802644, 1.0749516416653035, 1.075513350944018, 1.0755780398943546, 1.0761847055604306, 1.0761126428597863, 1.0751116704471007, 1.074711005480223, 1.074838970095066, 1.0746064640226818, 1.0754420050650786, 1.0750840745731722, 1.0743386490983133, 1.0756120486016736, 1.075705860635917, 1.0761260925646878, 1.0785418230128798, 1.0785521564421003, 1.0764794404479279, 1.0758641666575215, 1.0759489260283597, 1.0741611981431056, 1.07296950339488, 1.0732023122862642, 1.0742342100159092, 1.0751027127204857, 1.07558843752825, 1.0751528595077189, 1.075625230721848, 1.0751337922852615, 1.0751118581674761, 1.0760140706752908, 1.0768064464058587, 1.0773010328289714, 1.0785029554993453, 1.0789018910506676, 1.0778120601509984, 1.0781015261444944, 1.0769444449586039, 1.07713921258015, 1.0749698333160826, 1.0750018796701541, 1.0766898053033012, 1.0775756372019576, 1.077288085212457, 1.0776434365555962, 1.0779290851113832, 1.0772144663314318, 1.0768790425142436, 1.0768735365718847, 1.0767177912989274, 1.0768683872786649, 1.077281464105365, 1.0744849402329018, 1.0753171794324476, 1.0743485693078128, 1.0760875539043657, 1.0749035717427045, 1.0748865606358093, 1.0745508477018384, 1.0741658751013243, 1.0737241087680185, 1.074142161457018, 1.0761411058883166, 1.0773481044471753, 1.0798678049704515, 1.0791139256190785, 1.0775530532075854, 1.0762534155242744, 1.0766538888558574, 1.07697300605586, 1.0765119195963166, 1.0760856434237978, 1.0759268039944527, 1.0766599881042205, 1.076484316479788, 1.0768448640951773, 1.0759797432935492, 1.0755776395938668, 1.0761857631758516, 1.0759863174216109, 1.0756141879092689, 1.07548116052092, 1.0759124865477112, 1.0764754166939772, 1.0754147004611387, 1.0757576434678828, 1.0752262867534494, 1.0754052318375686, 1.0743080514600907, 1.073345889794611, 1.0743138024763912, 1.0762819932599372, 1.076954923044089, 1.0762752096836985, 1.076239341585507, 1.0779829328674793, 1.0774024039849468, 1.0776330271769432, 1.077212241678598, 1.0779781449213013, 1.0783413503753336, 1.0786021871519793, 1.078979351054663, 1.0794155010448887, 1.0790060782080213, 1.0776071331183898, 1.0779166742302906, 1.0786480903625488, 1.077905735358816, 1.0768391256066183, 1.0775487119536877, 1.0782287312650132, 1.0776282809246545, 1.0776516596476238, 1.0777767742012914, 1.0770780858147908, 1.0774084020326486, 1.076501961803593, 1.0766432165903803, 1.076833378700983, 1.0777637018946005, 1.0783272521640672, 1.0768683612444523, 1.0777596340978086, 1.0782540970052208, 1.0786593418403212, 1.079095355395613, 1.079883259113982, 1.0789396504463233, 1.0797314064451822, 1.0783604414787982, 1.0785287428763504, 1.0794749682760003, 1.0785742984420952, 1.0791429367362964, 1.0801459824901887, 1.0785150592550268, 1.0787910514668682, 1.0790707679413418, 1.0799634474251658, 1.0804558788810066, 1.0786731615050869, 1.0787018705862887, 1.080774075292014, 1.081772939912204, 1.0817745622547192, 1.0821158907487867, 1.0803005961557524, 1.0809749644769628, 1.0812432881450809, 1.0814171232809182, 1.0793759509651923, 1.0786070992010959, 1.0789441878376727, 1.0781055230812486, 1.0794478923033415, 1.077655934543641, 1.0784242458531421, 1.0778904908592086, 1.0792727601547742, 1.0784172919779185, 1.078821243323716, 1.0786106705861334, 1.0780814089407083, 1.0788341322164425, 1.0769770497758988, 1.0778113157291132, 1.0765914249498465, 1.078473600456476, 1.07861027831123, 1.0790922001664862, 1.0790876157961065, 1.079290986648334, 1.0790149599852037, 1.0805957423055113, 1.0797074950974563, 1.0803824048519917, 1.080363700542544, 1.0813156311539398, 1.081164489825958, 1.0811365754733533, 1.0804094853268076, 1.0806044486943136, 1.0801759997416405, 1.0801662005031443, 1.078921109584752, 1.0776603051594325, 1.079431677491011, 1.0788365618152962, 1.0768747650735289, 1.0758993983855976, 1.0756960245971805, 1.077970563680276, 1.0762876736119462, 1.0742094262284403, 1.0772050202186472, 1.0779680794682995, 1.0792622998821715, 1.0771078662136309, 1.081328399662901, 1.0793492938888876, 1.0761774707897542, 1.0760947826069172, 1.0791207759446895, 1.0765880663406673, 1.0766633909519865, 1.0775439913441198, 1.0769739313470124, 1.0788760606095513, 1.0751553396090303, 1.0767481663739935, 1.0792844129117642, 1.0790271935204567, 1.0795012522605056, 1.0807384424804662, 1.0799535655818746, 1.078684389884836, 1.0787986172439625, 1.0791583936202702, 1.0794482516929238, 1.0820595253081549, 1.0805304784492906, 1.0806058048223235, 1.0810800849510531, 1.0825562195237635, 1.0815329404887308, 1.0798237268952118, 1.0794019135348316, 1.07997610435893, 1.0802041634745982, 1.0788021535904733, 1.0796578137941157, 1.080032662805078, 1.079754225334706, 1.0794498078733046, 1.0821369485119097, 1.0785476266848435, 1.081490685004123, 1.0820106733804462, 1.0841851946951329, 1.084100399307038, 1.0846226048978482, 1.084631460249326, 1.0859288032027496, 1.0820434582840242, 1.0790247523725913, 1.0788214335887891, 1.0837102807409853, 1.0816775525144755], 'val_acc': [0.39244663230891297, 0.399014776612346, 0.38916256025506946, 0.3973727408790432, 0.37274219890924903, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3940886685315807, 0.3924466327004049, 0.40229884905768143, 0.3908045962819912, 0.4055829216008899, 0.3973727408790432, 0.3973727408790432, 0.39901477680809194, 0.39901477680809194, 0.39573070475424843, 0.40394088537822215, 0.3908045961841182, 0.4022988493513004, 0.39901477690596493, 0.3973727408790432, 0.40065681322650565, 0.39408866882519966, 0.38916256054868836, 0.39901477690596493, 0.39573070465637544, 0.39244663240678596, 0.39573070465637544, 0.3973727406832972, 0.3990147770038379, 0.39573070465637544, 0.39901477690596493, 0.39901477690596493, 0.39901477690596493, 0.3940886685315807, 0.3891625600593235, 0.39244663240678596, 0.40065681312863266, 0.3990147770038379, 0.4022988491555544, 0.40886699355686046, 0.3908045964777372, 0.39244663240678596, 0.3973727408790432, 0.39901477690596493, 0.40065681322650565, 0.39901477680809194, 0.3973727407811702, 0.40394088537822215, 0.4006568130307597, 0.39737274107478915, 0.3908045963798642, 0.39244663240678596, 0.38752052393452874, 0.3809523793374768, 0.3842364518806852, 0.3973727408790432, 0.3908045962819912, 0.38916256025506946, 0.40722495762781163, 0.3875205241302747, 0.38259441585376347, 0.3875205245217666, 0.372742199007122, 0.3858784891799557, 0.37766830738150625, 0.3825944157558905, 0.37438423542553567, 0.3908045963798642, 0.3990147770038379, 0.39573070465637544, 0.3940886690209456, 0.38916256064656135, 0.39737274097691616, 0.4022988493513004, 0.3842364524679231, 0.38916256064656135, 0.40065681312863266, 0.3793103438977929, 0.3908045964777372, 0.40886699385047937, 0.3842364520764312, 0.38095237992471465, 0.39901477690596493, 0.3809523802183336, 0.3908045961841182, 0.3908045963798642, 0.3924466330918968, 0.4022988492534274, 0.38916256025506946, 0.39737274107478915, 0.40558292179663585, 0.40394088537822215, 0.39737274136840806, 0.40722495762781163, 0.4105090300731471, 0.4022988498406653, 0.40886699394835235, 0.4105090298774011, 0.4022988492534274, 0.40722495762781163, 0.39573070446062947, 0.4088669937526064, 0.40394088537822215, 0.40394088537822215, 0.39901477690596493, 0.40394088547609514, 0.39244663240678596, 0.39573070455850246, 0.38752052393452874, 0.38587848810335296, 0.39573070475424843, 0.3760262716482034, 0.38259441634312835, 0.39737274097691616, 0.3940886686294537, 0.3973727407811702, 0.39080459667348316, 0.39244663289615084, 0.3908045967713561, 0.39737274117266214, 0.39573070475424843, 0.4088669937526064, 0.4105090298774011, 0.39737274156415403, 0.39408866882519966, 0.3973727408790432, 0.40558292209025476, 0.4072249582150495, 0.3990147774932028, 0.39901477739532987, 0.3940886687273267, 0.39737274097691616, 0.4055829216008899, 0.39573070465637544, 0.40394088547609514, 0.39901477690596493, 0.3973727406832972, 0.41215106590432293, 0.39901477690596493, 0.3908045964777372, 0.40065681332437864, 0.3875205240324017, 0.38587848800547997, 0.3990147770038379, 0.38916256045081543, 0.38916256035294244, 0.3875205240324017, 0.38423645178281224, 0.385878487907607, 0.3825944155601445, 0.37602627096309255, 0.39244663260253193, 0.39573070475424843, 0.3842364519785582, 0.385878487907607, 0.3842364518806852, 0.3825944157558905, 0.379310343408428, 0.3875205240324017, 0.3760262712567115, 0.3842364520764312, 0.3842364519785582, 0.38916256025506946, 0.37438423513191676, 0.3875205243260207, 0.3875205241302747, 0.379310343604174, 0.37766830738150625, 0.3875205241302747, 0.39080459667348316, 0.385878487907607, 0.38916256045081543, 0.3908045965756102, 0.38916256035294244, 0.385878487907607, 0.385878487907607, 0.3842364519785582, 0.3973727412705351, 0.3957307050478674, 0.3957307050478674, 0.3891625600593235, 0.39408866882519966, 0.385878487809734, 0.3908045964777372, 0.3908045964777372, 0.385878487809734, 0.38587848810335296, 0.379310343506301, 0.38423645168493925, 0.3875205241302747, 0.3875205241302747, 0.38916256054868836, 0.379310343310555, 0.3776683071857603, 0.37274219890924903, 0.37602627106096553, 0.38095237943534976, 0.38916256035294244, 0.38423645168493925, 0.38423645168493925, 0.3678160905348648, 0.3776683071857603, 0.38423645178281224, 0.3711001629802002, 0.37602627106096553, 0.3743842349361708, 0.3908045963798642, 0.37602627135458444, 0.3743842352297897, 0.3825944157558905, 0.3842364518806852, 0.379310343310555, 0.3825944156580175, 0.3825944156580175, 0.379310343310555, 0.37602627145245743, 0.37602627096309255, 0.38423645158706626, 0.3743842349361708, 0.36453201808952934, 0.37602627096309255, 0.37274219861563007, 0.385878487711861, 0.37602627135458444, 0.3743842348382978, 0.3809523793374768, 0.3776683069900143, 0.399014776710219, 0.39080459598837225, 0.39080459598837225, 0.3825944157558905, 0.39080459598837225, 0.37602627086521956, 0.38423645158706626, 0.3858784874182421, 0.38095237943534976, 0.38259441634312835, 0.3875205240324017, 0.39901477680809194, 0.38095238002258763, 0.38752052373878276, 0.3973727403896783, 0.3875205246196396, 0.3908045963798642, 0.3858784886905908, 0.3776683071857603, 0.39244663240678596, 0.38259441585376347, 0.39244663221104, 0.399014776710219, 0.36781609004549987, 0.3940886683358347, 0.39573070455850246, 0.3776683070878873, 0.3973727407811702, 0.3940886683358347, 0.38095237943534976, 0.38423645276154206, 0.385878487809734, 0.37766830728363326, 0.3924466332876428, 0.40065681312863266, 0.39901477690596493, 0.38916256074443434, 0.40229884905768143, 0.38587848800547997, 0.3973727407811702, 0.3825944155601445, 0.39244663221104, 0.3825944155601445, 0.3940886683358347, 0.3825944155601445, 0.385878487809734, 0.38423645168493925, 0.3825944154622715, 0.39244663221104, 0.3825944159516364, 0.3940886683358347, 0.38423645217430413, 0.3825944155601445, 0.38423645168493925, 0.38587848800547997, 0.38259441634312835, 0.39244663250465894, 0.38587848810335296, 0.4006568130307597, 0.40558292199238183, 0.41215106600219586, 0.39408866892307265, 0.4088669937526064, 0.3908045963798642, 0.3924466330918968, 0.3957307052436133, 0.40722495811717657, 0.4055829214051439, 0.3825944156580175], 'loss': [1.0876929242997688, 1.076963768896381, 1.0742768337104844, 1.0741350320330392, 1.07442091343584, 1.0741610913306046, 1.0735329307325077, 1.0737621263801684, 1.0738336101449735, 1.073619103970224, 1.0740186836685244, 1.0740014945701897, 1.073410899046755, 1.0734792375466662, 1.0741485025848452, 1.073860626152164, 1.0740157149410836, 1.073942131477215, 1.0740401569088383, 1.0740217195644026, 1.0739025344594058, 1.073538087133999, 1.073325760212767, 1.073464781156066, 1.0738325908688304, 1.0730008443033425, 1.0734185562486276, 1.0737911071620683, 1.073749560399222, 1.0736063866155103, 1.073199678446478, 1.0731825281217602, 1.0733879825172972, 1.073201626918644, 1.0731378980730593, 1.0731344794345832, 1.072841479646107, 1.072923597613889, 1.072723097967661, 1.0728692014604133, 1.0726373576530441, 1.0729738423466928, 1.0729699255259864, 1.0725026794038024, 1.0728827727159191, 1.0724079663259047, 1.0729078002534118, 1.0724453839922832, 1.0726234953995848, 1.0725391697345084, 1.0722637986255623, 1.0723913172187256, 1.072309264314248, 1.0721007204643265, 1.0720775667402045, 1.0716365448013714, 1.0715501613930265, 1.0714980870546502, 1.0711650963436652, 1.0715125595030108, 1.071135122869049, 1.0709415185622855, 1.070781337456047, 1.0722743031425397, 1.0717224506871657, 1.0707858607509542, 1.0723026936548692, 1.0729070597360757, 1.0728265641406332, 1.0722152976284771, 1.073021430254472, 1.0716326646001921, 1.0722883965690033, 1.0725110235645052, 1.071478865474646, 1.0723917851947415, 1.0710814802553619, 1.0707461935293994, 1.0701430545940047, 1.0703590253050568, 1.070596614804356, 1.0703847401440756, 1.0700636611582073, 1.0702179139644459, 1.07087845058167, 1.0702717174984346, 1.0699786959242772, 1.0699510241436028, 1.0702537646773416, 1.0716342524581377, 1.0709774425142355, 1.070633941117743, 1.070436780457624, 1.0692367084706833, 1.0714694236093478, 1.0719046045867324, 1.0720924747063638, 1.0711146361529216, 1.0706577510070017, 1.0708379094850355, 1.0695027221154874, 1.0699067450646746, 1.0693374109953582, 1.0699855981176638, 1.0701614608510075, 1.070177049117901, 1.0695026841251758, 1.0699486218438745, 1.0707402575432152, 1.0714082912253158, 1.0712374904072506, 1.071060776269901, 1.07057104781423, 1.0707931941050035, 1.0710810591797564, 1.072812503759866, 1.0711684995118598, 1.0705719655544117, 1.0695984804654757, 1.0708152855690989, 1.0695431039563439, 1.0695376749645757, 1.0696984230860058, 1.069537484768235, 1.0700874818668717, 1.0700593267867697, 1.071283817046477, 1.0722590003904622, 1.0713440745762977, 1.0709877232506535, 1.0701699602530477, 1.069888066462178, 1.0694420793462829, 1.0696204373479135, 1.0695663651401748, 1.069324012899301, 1.068871817304858, 1.0686792982187605, 1.0694231619825108, 1.0693676021309604, 1.0700079455267968, 1.0706973642783977, 1.0702422047542595, 1.0701658810922987, 1.0697339599871782, 1.0697375705354757, 1.069082748229009, 1.0690374935921703, 1.0688967529018802, 1.069396296712652, 1.0691519837604655, 1.0694317211115874, 1.069367412717925, 1.0689387170196314, 1.0690520853966903, 1.069473802676191, 1.0702174414844239, 1.0722642360526677, 1.0733669428365187, 1.0721501950121024, 1.074160405987342, 1.0722048679661211, 1.0720987925539271, 1.0714335847929028, 1.0707970251048124, 1.0705623690352548, 1.0713234487255496, 1.0718049474810183, 1.0707955993910834, 1.071097253186502, 1.0709130998999186, 1.070643262207141, 1.0699017797896995, 1.070101237688711, 1.0720477074323493, 1.0704159898190038, 1.069466279027888, 1.0686670119267958, 1.069654242507731, 1.0692632658036092, 1.0689507569130932, 1.0694408037824064, 1.0703693238616725, 1.0700647159768326, 1.0702285066522368, 1.0717059257338915, 1.069013106602663, 1.0695429174317472, 1.0700364887347211, 1.068376814364408, 1.0682716379420223, 1.0677126362583231, 1.0681827200511642, 1.0674987885986265, 1.067852866918889, 1.0680769608985228, 1.0688742245492016, 1.0686938373459927, 1.067829235965956, 1.069756407120879, 1.0713200922129824, 1.0690222667228024, 1.0689092495113428, 1.068514707396897, 1.0677407688673517, 1.067905958575145, 1.0676699825870428, 1.069328862245078, 1.069181245944828, 1.069263376347583, 1.0688777351281482, 1.0692722829705146, 1.0700530277385358, 1.0689323893807507, 1.069091524774289, 1.068239709728797, 1.068308325714644, 1.0689319853420376, 1.0676832540813657, 1.0685107633073716, 1.0680101243867033, 1.0678975096963024, 1.06823899231897, 1.0669317996477445, 1.0665534166829542, 1.065653779913023, 1.066541961967578, 1.0655504475873598, 1.0660354337653095, 1.0662217982740618, 1.0657872472210832, 1.065207214668791, 1.0653668883400043, 1.0662774169224734, 1.0652305076254467, 1.0654304724209607, 1.0648514072018727, 1.0666726230351098, 1.0673978220510776, 1.0671004012869614, 1.0655896906490443, 1.0653395304689661, 1.0648053347452466, 1.066675486897541, 1.0663629031524031, 1.065448943006919, 1.06473468427051, 1.0660207141840972, 1.0638525819876357, 1.0648560878432507, 1.0639544824799962, 1.0639759034346752, 1.0628884734559108, 1.0642434572537087, 1.0642798753734486, 1.0639076266690202, 1.0644466017305974, 1.0651999043243376, 1.0664599481794135, 1.0640754358969184, 1.065560000437241, 1.066755647727841, 1.0657216863220966, 1.0661117053374618, 1.066736465606846, 1.0647759899221652, 1.0656169173653856, 1.0652019472827168, 1.0648301177445867, 1.0659337407509648, 1.0646413341929535, 1.0644876864901314, 1.063952892810657, 1.0640969368956172, 1.06407878144321, 1.0630970758579106, 1.0629600264942867, 1.0630420055232743, 1.0627888244280335, 1.0636117360430333, 1.0624848521465637, 1.0630039401612488, 1.0616636980730405, 1.0626793737528994, 1.063680627213856, 1.0635622449968876, 1.0609095449565127, 1.0633511664686262, 1.0616405180592312, 1.0625537519827026, 1.0610091175631575, 1.0615695892173407, 1.060163233950887, 1.063181933974828, 1.0625658072485327, 1.0627000789133185, 1.0611257232924507, 1.0620026380864012, 1.0623740116918357, 1.0625402138218498], 'acc': [0.3934291588697101, 0.3938398368794326, 0.4012320326339048, 0.39589322575308705, 0.3971252570406857, 0.39630389980956515, 0.39712525860729647, 0.39425051449750237, 0.39425051351837065, 0.39425051351837065, 0.4016427082569937, 0.38809034944314974, 0.3975359328596009, 0.3958932216040163, 0.39712525524153114, 0.395893225165608, 0.39630390376280955, 0.39876796907957573, 0.3987679673171386, 0.4036960973264745, 0.39630390297950413, 0.39958932392406266, 0.4020533882616971, 0.3913757705835346, 0.3946611893372859, 0.404928133350623, 0.40698152046184033, 0.39712525484987843, 0.3942505129308916, 0.39507186699207314, 0.3930184824633158, 0.40164270845282, 0.3909650907746575, 0.3934291563239675, 0.39589322536143434, 0.395071866563703, 0.39507186992946836, 0.3921971260155006, 0.4008213545874649, 0.4024640649006352, 0.4020533868541952, 0.3942505146933287, 0.39178644898490983, 0.3979466138434361, 0.4053388074071011, 0.4086242311299459, 0.39589322379482356, 0.3921971256238479, 0.39383983805439066, 0.3967145804017476, 0.3979466122401079, 0.39630389941791244, 0.3995893201666446, 0.4057494850251709, 0.4123203298884006, 0.4053388070154484, 0.39958932153742904, 0.41026694140639886, 0.40862423226818656, 0.4041067757278497, 0.4078028770687644, 0.40862422776418056, 0.4094455849953011, 0.4024640672505514, 0.39794661306013074, 0.4098562643758081, 0.39260780324191774, 0.38850102725704594, 0.3885010290194831, 0.3995893201666446, 0.3872689947944892, 0.3971252554373575, 0.40492813158818586, 0.3991786465018192, 0.3971252578239911, 0.40123203502053845, 0.39630390360370066, 0.3999999987638462, 0.39712525684485933, 0.40205338904500254, 0.39794661165262885, 0.4020533888491762, 0.39876796829627037, 0.40451745217096147, 0.4036960973264745, 0.4008213538408769, 0.3975359338387327, 0.4020533866583689, 0.4016427108394537, 0.40328542049171007, 0.40574948482934453, 0.39383983550864815, 0.4008213536083331, 0.40246406368895965, 0.4020533864625426, 0.40123203083475023, 0.39835728911158974, 0.39876796512633134, 0.40451745318681065, 0.40041067896437593, 0.41108829742584385, 0.4049281299848576, 0.40246406705472504, 0.39712525582901015, 0.4094455855460627, 0.40739219592582027, 0.4127310051198368, 0.403285418729273, 0.4024640668588987, 0.4123203285176162, 0.40698151964181745, 0.40492812939737854, 0.40123203126312035, 0.3991786437235329, 0.4020533894366552, 0.39917864532686115, 0.39917864591434016, 0.4024640654513968, 0.4114989742238908, 0.39794661208099896, 0.4061601634265461, 0.4069815172919013, 0.4041067759603935, 0.40739219549745015, 0.4045174555367268, 0.40287474506444754, 0.4000000003671744, 0.3946611887498068, 0.40000000134630614, 0.39137577097518733, 0.40041067834017946, 0.39876796829627037, 0.4036960971306482, 0.4053388070154484, 0.40000000232543786, 0.39917864352770654, 0.4069815188585121, 0.4078028764812853, 0.40164271201441176, 0.4119096510953727, 0.40328542150755925, 0.4012320326339048, 0.39999999798054076, 0.4098562610100427, 0.4119096500795235, 0.39958932392406266, 0.40533880998956107, 0.40000000017134807, 0.40985626061839, 0.4078028725280409, 0.40657084202374766, 0.40698152003347016, 0.40492813315479664, 0.4098562647307433, 0.4000000005630007, 0.4078028764812853, 0.4045174543617687, 0.38685831698059303, 0.4000000003671744, 0.400410679551855, 0.3889117024884821, 0.39589322536143434, 0.39712525723651204, 0.3983572896990688, 0.39342915609142376, 0.39589322297480073, 0.39958932094995003, 0.3991786466976456, 0.3967145776234613, 0.3909650921821594, 0.3917864464391673, 0.39301848085998753, 0.3930184814474666, 0.39466119113644044, 0.3893223826890119, 0.3921971260155006, 0.3983572906782005, 0.3983572902865478, 0.39876796712131224, 0.398767968100444, 0.4045174519751351, 0.39876796512633134, 0.39999999935132524, 0.3950718677753785, 0.398767964930505, 0.3938398357044745, 0.4012320334172102, 0.39671457860259307, 0.40574948858676263, 0.40246406803385676, 0.3946611925072249, 0.4045174531500932, 0.40082135419581216, 0.40041067618608966, 0.3975359344629292, 0.3995893219657992, 0.4123203263268089, 0.4041067765111551, 0.406570842219574, 0.40041067974768135, 0.3913757711710137, 0.4041067759603935, 0.39958932036247097, 0.40205339002413426, 0.40123203204642577, 0.40205339041578697, 0.40164270845282, 0.4024640658430495, 0.39917864532686115, 0.4028747433020104, 0.3958932224240391, 0.39712525622066286, 0.3979466106734971, 0.3987679667296596, 0.4012320335763191, 0.39958932094995003, 0.39958932392406266, 0.39671458138087934, 0.40698151807520666, 0.39466119012059125, 0.4032854226825174, 0.40492813374227565, 0.4102669417980515, 0.40410677733117795, 0.4069815198376438, 0.404106774748718, 0.40000000232543786, 0.40328542131173295, 0.39383983707525894, 0.4024640678380304, 0.4127310047281841, 0.4135523637217418, 0.41108829464755753, 0.4127310049607279, 0.406160162251588, 0.4041067751403707, 0.4065708428437705, 0.40739219706406093, 0.4073921982757365, 0.40287474506444754, 0.40985626359250266, 0.4028747446727948, 0.4078028725280409, 0.4057494883909363, 0.409445584566931, 0.40041067638191596, 0.40944558675773823, 0.40287474506444754, 0.4032854220950383, 0.40410677831030967, 0.4094455857786065, 0.4028747444769685, 0.4119096500795235, 0.4049281325673176, 0.40533881077286643, 0.3967145772318086, 0.4110882944517312, 0.3926078012469368, 0.40657084104461594, 0.4045174527584405, 0.40657083967383145, 0.3831622195929228, 0.4057494850251709, 0.40246406744637775, 0.4012320310305766, 0.40903490675303483, 0.40903490734051384, 0.4164271060690988, 0.40780287291969364, 0.406981520853493, 0.40944558479947474, 0.40041067958857246, 0.40821355135778625, 0.4036960995172818, 0.4160164270802445, 0.4119096522703308, 0.41437371676707413, 0.41642710681568673, 0.41026694258135693, 0.4078028748779571, 0.41971252501867634, 0.416427105089967, 0.4151950727865192, 0.4012320344330594, 0.4119096530536362, 0.4078028733113463, 0.40739219905904184, 0.39876796767207384, 0.41437371755037955, 0.40698151846685937, 0.4069815192501648, 0.40246406349313335, 0.41067761863281593, 0.40246406666307233, 0.4049281303765103, 0.4114989726939975, 0.4102669427771833, 0.4110882948433839, 0.41190965187867806, 0.40821355331604975]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
