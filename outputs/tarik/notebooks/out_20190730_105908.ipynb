{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf18.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.30 10:59:08 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nMags', 'channelMode': 'Split', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 3 Label(s): ['01', '03', '02'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nMags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002378034CE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000237D1D06EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0797, Accuracy:0.3822, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0746, Accuracy:0.3895, Validation Loss:1.0741, Validation Accuracy:0.4031\n",
    "Epoch #3: Loss:1.0745, Accuracy:0.3881, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0751, Accuracy:0.3931, Validation Loss:1.0742, Validation Accuracy:0.3970\n",
    "Epoch #5: Loss:1.0737, Accuracy:0.4033, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0738, Accuracy:0.4035, Validation Loss:1.0734, Validation Accuracy:0.3978\n",
    "Epoch #7: Loss:1.0735, Accuracy:0.3972, Validation Loss:1.0732, Validation Accuracy:0.4015\n",
    "Epoch #8: Loss:1.0733, Accuracy:0.3998, Validation Loss:1.0731, Validation Accuracy:0.4011\n",
    "Epoch #9: Loss:1.0732, Accuracy:0.3996, Validation Loss:1.0729, Validation Accuracy:0.3986\n",
    "Epoch #10: Loss:1.0731, Accuracy:0.4041, Validation Loss:1.0726, Validation Accuracy:0.4011\n",
    "Epoch #11: Loss:1.0731, Accuracy:0.4031, Validation Loss:1.0724, Validation Accuracy:0.4002\n",
    "Epoch #12: Loss:1.0727, Accuracy:0.4054, Validation Loss:1.0722, Validation Accuracy:0.4048\n",
    "Epoch #13: Loss:1.0731, Accuracy:0.4072, Validation Loss:1.0723, Validation Accuracy:0.4085\n",
    "Epoch #14: Loss:1.0717, Accuracy:0.4151, Validation Loss:1.0715, Validation Accuracy:0.4023\n",
    "Epoch #15: Loss:1.0717, Accuracy:0.4034, Validation Loss:1.0710, Validation Accuracy:0.4044\n",
    "Epoch #16: Loss:1.0725, Accuracy:0.4050, Validation Loss:1.0717, Validation Accuracy:0.4064\n",
    "Epoch #17: Loss:1.0706, Accuracy:0.4104, Validation Loss:1.0704, Validation Accuracy:0.4011\n",
    "Epoch #18: Loss:1.0695, Accuracy:0.4164, Validation Loss:1.0701, Validation Accuracy:0.4122\n",
    "Epoch #19: Loss:1.0697, Accuracy:0.4146, Validation Loss:1.0691, Validation Accuracy:0.4175\n",
    "Epoch #20: Loss:1.0693, Accuracy:0.4162, Validation Loss:1.0686, Validation Accuracy:0.4195\n",
    "Epoch #21: Loss:1.0685, Accuracy:0.4163, Validation Loss:1.0686, Validation Accuracy:0.4097\n",
    "Epoch #22: Loss:1.0674, Accuracy:0.4229, Validation Loss:1.0682, Validation Accuracy:0.4130\n",
    "Epoch #23: Loss:1.0660, Accuracy:0.4210, Validation Loss:1.0680, Validation Accuracy:0.4253\n",
    "Epoch #24: Loss:1.0645, Accuracy:0.4296, Validation Loss:1.0666, Validation Accuracy:0.4195\n",
    "Epoch #25: Loss:1.0643, Accuracy:0.4241, Validation Loss:1.0663, Validation Accuracy:0.4245\n",
    "Epoch #26: Loss:1.0626, Accuracy:0.4321, Validation Loss:1.0663, Validation Accuracy:0.4212\n",
    "Epoch #27: Loss:1.0622, Accuracy:0.4260, Validation Loss:1.0705, Validation Accuracy:0.4027\n",
    "Epoch #28: Loss:1.0610, Accuracy:0.4273, Validation Loss:1.0656, Validation Accuracy:0.4146\n",
    "Epoch #29: Loss:1.0629, Accuracy:0.4299, Validation Loss:1.0641, Validation Accuracy:0.4187\n",
    "Epoch #30: Loss:1.0617, Accuracy:0.4276, Validation Loss:1.0650, Validation Accuracy:0.4236\n",
    "Epoch #31: Loss:1.0605, Accuracy:0.4260, Validation Loss:1.0631, Validation Accuracy:0.4249\n",
    "Epoch #32: Loss:1.0582, Accuracy:0.4376, Validation Loss:1.0644, Validation Accuracy:0.4191\n",
    "Epoch #33: Loss:1.0581, Accuracy:0.4349, Validation Loss:1.0617, Validation Accuracy:0.4298\n",
    "Epoch #34: Loss:1.0551, Accuracy:0.4376, Validation Loss:1.0623, Validation Accuracy:0.4228\n",
    "Epoch #35: Loss:1.0588, Accuracy:0.4305, Validation Loss:1.0646, Validation Accuracy:0.4282\n",
    "Epoch #36: Loss:1.0544, Accuracy:0.4370, Validation Loss:1.0615, Validation Accuracy:0.4236\n",
    "Epoch #37: Loss:1.0529, Accuracy:0.4411, Validation Loss:1.0616, Validation Accuracy:0.4232\n",
    "Epoch #38: Loss:1.0552, Accuracy:0.4387, Validation Loss:1.0613, Validation Accuracy:0.4298\n",
    "Epoch #39: Loss:1.0529, Accuracy:0.4409, Validation Loss:1.0589, Validation Accuracy:0.4319\n",
    "Epoch #40: Loss:1.0529, Accuracy:0.4407, Validation Loss:1.0595, Validation Accuracy:0.4273\n",
    "Epoch #41: Loss:1.0528, Accuracy:0.4400, Validation Loss:1.0646, Validation Accuracy:0.4224\n",
    "Epoch #42: Loss:1.0561, Accuracy:0.4369, Validation Loss:1.0615, Validation Accuracy:0.4261\n",
    "Epoch #43: Loss:1.0527, Accuracy:0.4464, Validation Loss:1.0577, Validation Accuracy:0.4306\n",
    "Epoch #44: Loss:1.0495, Accuracy:0.4438, Validation Loss:1.0591, Validation Accuracy:0.4294\n",
    "Epoch #45: Loss:1.0488, Accuracy:0.4446, Validation Loss:1.0596, Validation Accuracy:0.4294\n",
    "Epoch #46: Loss:1.0518, Accuracy:0.4454, Validation Loss:1.0591, Validation Accuracy:0.4376\n",
    "Epoch #47: Loss:1.0500, Accuracy:0.4437, Validation Loss:1.0588, Validation Accuracy:0.4339\n",
    "Epoch #48: Loss:1.0481, Accuracy:0.4472, Validation Loss:1.0601, Validation Accuracy:0.4298\n",
    "Epoch #49: Loss:1.0488, Accuracy:0.4502, Validation Loss:1.0602, Validation Accuracy:0.4310\n",
    "Epoch #50: Loss:1.0496, Accuracy:0.4455, Validation Loss:1.0569, Validation Accuracy:0.4314\n",
    "Epoch #51: Loss:1.0474, Accuracy:0.4448, Validation Loss:1.0567, Validation Accuracy:0.4335\n",
    "Epoch #52: Loss:1.0509, Accuracy:0.4422, Validation Loss:1.0616, Validation Accuracy:0.4253\n",
    "Epoch #53: Loss:1.0478, Accuracy:0.4486, Validation Loss:1.0592, Validation Accuracy:0.4302\n",
    "Epoch #54: Loss:1.0489, Accuracy:0.4475, Validation Loss:1.0579, Validation Accuracy:0.4261\n",
    "Epoch #55: Loss:1.0468, Accuracy:0.4440, Validation Loss:1.0569, Validation Accuracy:0.4339\n",
    "Epoch #56: Loss:1.0460, Accuracy:0.4431, Validation Loss:1.0588, Validation Accuracy:0.4282\n",
    "Epoch #57: Loss:1.0470, Accuracy:0.4457, Validation Loss:1.0580, Validation Accuracy:0.4327\n",
    "Epoch #58: Loss:1.0438, Accuracy:0.4464, Validation Loss:1.0569, Validation Accuracy:0.4249\n",
    "Epoch #59: Loss:1.0443, Accuracy:0.4473, Validation Loss:1.0562, Validation Accuracy:0.4335\n",
    "Epoch #60: Loss:1.0446, Accuracy:0.4496, Validation Loss:1.0551, Validation Accuracy:0.4343\n",
    "Epoch #61: Loss:1.0436, Accuracy:0.4467, Validation Loss:1.0561, Validation Accuracy:0.4331\n",
    "Epoch #62: Loss:1.0429, Accuracy:0.4475, Validation Loss:1.0556, Validation Accuracy:0.4298\n",
    "Epoch #63: Loss:1.0410, Accuracy:0.4483, Validation Loss:1.0562, Validation Accuracy:0.4376\n",
    "Epoch #64: Loss:1.0408, Accuracy:0.4508, Validation Loss:1.0565, Validation Accuracy:0.4302\n",
    "Epoch #65: Loss:1.0453, Accuracy:0.4512, Validation Loss:1.0553, Validation Accuracy:0.4372\n",
    "Epoch #66: Loss:1.0415, Accuracy:0.4470, Validation Loss:1.0566, Validation Accuracy:0.4327\n",
    "Epoch #67: Loss:1.0414, Accuracy:0.4497, Validation Loss:1.0555, Validation Accuracy:0.4372\n",
    "Epoch #68: Loss:1.0386, Accuracy:0.4516, Validation Loss:1.0586, Validation Accuracy:0.4372\n",
    "Epoch #69: Loss:1.0428, Accuracy:0.4510, Validation Loss:1.0570, Validation Accuracy:0.4364\n",
    "Epoch #70: Loss:1.0391, Accuracy:0.4549, Validation Loss:1.0535, Validation Accuracy:0.4351\n",
    "Epoch #71: Loss:1.0377, Accuracy:0.4517, Validation Loss:1.0568, Validation Accuracy:0.4319\n",
    "Epoch #72: Loss:1.0393, Accuracy:0.4495, Validation Loss:1.0556, Validation Accuracy:0.4339\n",
    "Epoch #73: Loss:1.0370, Accuracy:0.4502, Validation Loss:1.0543, Validation Accuracy:0.4401\n",
    "Epoch #74: Loss:1.0345, Accuracy:0.4543, Validation Loss:1.0540, Validation Accuracy:0.4343\n",
    "Epoch #75: Loss:1.0351, Accuracy:0.4584, Validation Loss:1.0531, Validation Accuracy:0.4306\n",
    "Epoch #76: Loss:1.0370, Accuracy:0.4496, Validation Loss:1.0554, Validation Accuracy:0.4339\n",
    "Epoch #77: Loss:1.0383, Accuracy:0.4527, Validation Loss:1.0531, Validation Accuracy:0.4323\n",
    "Epoch #78: Loss:1.0374, Accuracy:0.4500, Validation Loss:1.0514, Validation Accuracy:0.4351\n",
    "Epoch #79: Loss:1.0341, Accuracy:0.4531, Validation Loss:1.0527, Validation Accuracy:0.4323\n",
    "Epoch #80: Loss:1.0377, Accuracy:0.4551, Validation Loss:1.0529, Validation Accuracy:0.4310\n",
    "Epoch #81: Loss:1.0338, Accuracy:0.4505, Validation Loss:1.0515, Validation Accuracy:0.4368\n",
    "Epoch #82: Loss:1.0332, Accuracy:0.4573, Validation Loss:1.0558, Validation Accuracy:0.4327\n",
    "Epoch #83: Loss:1.0355, Accuracy:0.4532, Validation Loss:1.0500, Validation Accuracy:0.4392\n",
    "Epoch #84: Loss:1.0315, Accuracy:0.4608, Validation Loss:1.0500, Validation Accuracy:0.4368\n",
    "Epoch #85: Loss:1.0327, Accuracy:0.4578, Validation Loss:1.0497, Validation Accuracy:0.4306\n",
    "Epoch #86: Loss:1.0284, Accuracy:0.4602, Validation Loss:1.0513, Validation Accuracy:0.4360\n",
    "Epoch #87: Loss:1.0315, Accuracy:0.4535, Validation Loss:1.0497, Validation Accuracy:0.4351\n",
    "Epoch #88: Loss:1.0270, Accuracy:0.4603, Validation Loss:1.0483, Validation Accuracy:0.4323\n",
    "Epoch #89: Loss:1.0261, Accuracy:0.4639, Validation Loss:1.0499, Validation Accuracy:0.4360\n",
    "Epoch #90: Loss:1.0275, Accuracy:0.4620, Validation Loss:1.0485, Validation Accuracy:0.4331\n",
    "Epoch #91: Loss:1.0273, Accuracy:0.4586, Validation Loss:1.0502, Validation Accuracy:0.4356\n",
    "Epoch #92: Loss:1.0299, Accuracy:0.4594, Validation Loss:1.0501, Validation Accuracy:0.4405\n",
    "Epoch #93: Loss:1.0248, Accuracy:0.4672, Validation Loss:1.0474, Validation Accuracy:0.4372\n",
    "Epoch #94: Loss:1.0234, Accuracy:0.4660, Validation Loss:1.0488, Validation Accuracy:0.4265\n",
    "Epoch #95: Loss:1.0249, Accuracy:0.4654, Validation Loss:1.0527, Validation Accuracy:0.4314\n",
    "Epoch #96: Loss:1.0227, Accuracy:0.4689, Validation Loss:1.0531, Validation Accuracy:0.4335\n",
    "Epoch #97: Loss:1.0279, Accuracy:0.4605, Validation Loss:1.0494, Validation Accuracy:0.4236\n",
    "Epoch #98: Loss:1.0247, Accuracy:0.4696, Validation Loss:1.0500, Validation Accuracy:0.4327\n",
    "Epoch #99: Loss:1.0242, Accuracy:0.4645, Validation Loss:1.0458, Validation Accuracy:0.4306\n",
    "Epoch #100: Loss:1.0230, Accuracy:0.4678, Validation Loss:1.0472, Validation Accuracy:0.4372\n",
    "Epoch #101: Loss:1.0201, Accuracy:0.4667, Validation Loss:1.0459, Validation Accuracy:0.4454\n",
    "Epoch #102: Loss:1.0192, Accuracy:0.4717, Validation Loss:1.0484, Validation Accuracy:0.4384\n",
    "Epoch #103: Loss:1.0241, Accuracy:0.4679, Validation Loss:1.0457, Validation Accuracy:0.4487\n",
    "Epoch #104: Loss:1.0182, Accuracy:0.4713, Validation Loss:1.0434, Validation Accuracy:0.4388\n",
    "Epoch #105: Loss:1.0185, Accuracy:0.4675, Validation Loss:1.0426, Validation Accuracy:0.4384\n",
    "Epoch #106: Loss:1.0159, Accuracy:0.4776, Validation Loss:1.0438, Validation Accuracy:0.4323\n",
    "Epoch #107: Loss:1.0163, Accuracy:0.4741, Validation Loss:1.0425, Validation Accuracy:0.4475\n",
    "Epoch #108: Loss:1.0181, Accuracy:0.4777, Validation Loss:1.0399, Validation Accuracy:0.4479\n",
    "Epoch #109: Loss:1.0137, Accuracy:0.4782, Validation Loss:1.0425, Validation Accuracy:0.4376\n",
    "Epoch #110: Loss:1.0149, Accuracy:0.4693, Validation Loss:1.0426, Validation Accuracy:0.4516\n",
    "Epoch #111: Loss:1.0105, Accuracy:0.4786, Validation Loss:1.0421, Validation Accuracy:0.4462\n",
    "Epoch #112: Loss:1.0169, Accuracy:0.4772, Validation Loss:1.0438, Validation Accuracy:0.4409\n",
    "Epoch #113: Loss:1.0209, Accuracy:0.4636, Validation Loss:1.0364, Validation Accuracy:0.4438\n",
    "Epoch #114: Loss:1.0136, Accuracy:0.4742, Validation Loss:1.0373, Validation Accuracy:0.4499\n",
    "Epoch #115: Loss:1.0109, Accuracy:0.4771, Validation Loss:1.0380, Validation Accuracy:0.4466\n",
    "Epoch #116: Loss:1.0100, Accuracy:0.4799, Validation Loss:1.0387, Validation Accuracy:0.4409\n",
    "Epoch #117: Loss:1.0117, Accuracy:0.4794, Validation Loss:1.0412, Validation Accuracy:0.4462\n",
    "Epoch #118: Loss:1.0103, Accuracy:0.4869, Validation Loss:1.0378, Validation Accuracy:0.4495\n",
    "Epoch #119: Loss:1.0058, Accuracy:0.4845, Validation Loss:1.0375, Validation Accuracy:0.4495\n",
    "Epoch #120: Loss:1.0043, Accuracy:0.4839, Validation Loss:1.0417, Validation Accuracy:0.4487\n",
    "Epoch #121: Loss:1.0050, Accuracy:0.4866, Validation Loss:1.0386, Validation Accuracy:0.4450\n",
    "Epoch #122: Loss:1.0028, Accuracy:0.4803, Validation Loss:1.0433, Validation Accuracy:0.4392\n",
    "Epoch #123: Loss:1.0078, Accuracy:0.4824, Validation Loss:1.0379, Validation Accuracy:0.4503\n",
    "Epoch #124: Loss:1.0065, Accuracy:0.4815, Validation Loss:1.0333, Validation Accuracy:0.4487\n",
    "Epoch #125: Loss:1.0002, Accuracy:0.4932, Validation Loss:1.0384, Validation Accuracy:0.4462\n",
    "Epoch #126: Loss:1.0106, Accuracy:0.4758, Validation Loss:1.0344, Validation Accuracy:0.4503\n",
    "Epoch #127: Loss:1.0008, Accuracy:0.4866, Validation Loss:1.0367, Validation Accuracy:0.4524\n",
    "Epoch #128: Loss:0.9980, Accuracy:0.4931, Validation Loss:1.0349, Validation Accuracy:0.4528\n",
    "Epoch #129: Loss:0.9978, Accuracy:0.4879, Validation Loss:1.0379, Validation Accuracy:0.4511\n",
    "Epoch #130: Loss:0.9972, Accuracy:0.4893, Validation Loss:1.0401, Validation Accuracy:0.4581\n",
    "Epoch #131: Loss:0.9998, Accuracy:0.4879, Validation Loss:1.0358, Validation Accuracy:0.4507\n",
    "Epoch #132: Loss:1.0002, Accuracy:0.4933, Validation Loss:1.0377, Validation Accuracy:0.4569\n",
    "Epoch #133: Loss:0.9973, Accuracy:0.4933, Validation Loss:1.0414, Validation Accuracy:0.4503\n",
    "Epoch #134: Loss:0.9936, Accuracy:0.4943, Validation Loss:1.0387, Validation Accuracy:0.4475\n",
    "Epoch #135: Loss:0.9963, Accuracy:0.4980, Validation Loss:1.0471, Validation Accuracy:0.4413\n",
    "Epoch #136: Loss:1.0039, Accuracy:0.4847, Validation Loss:1.0440, Validation Accuracy:0.4413\n",
    "Epoch #137: Loss:0.9943, Accuracy:0.4922, Validation Loss:1.0359, Validation Accuracy:0.4569\n",
    "Epoch #138: Loss:0.9941, Accuracy:0.4924, Validation Loss:1.0376, Validation Accuracy:0.4540\n",
    "Epoch #139: Loss:0.9893, Accuracy:0.4996, Validation Loss:1.0341, Validation Accuracy:0.4475\n",
    "Epoch #140: Loss:0.9861, Accuracy:0.5046, Validation Loss:1.0386, Validation Accuracy:0.4585\n",
    "Epoch #141: Loss:0.9932, Accuracy:0.4938, Validation Loss:1.0350, Validation Accuracy:0.4561\n",
    "Epoch #142: Loss:0.9851, Accuracy:0.5018, Validation Loss:1.0358, Validation Accuracy:0.4499\n",
    "Epoch #143: Loss:0.9938, Accuracy:0.5022, Validation Loss:1.0377, Validation Accuracy:0.4536\n",
    "Epoch #144: Loss:0.9868, Accuracy:0.5038, Validation Loss:1.0306, Validation Accuracy:0.4573\n",
    "Epoch #145: Loss:0.9858, Accuracy:0.5070, Validation Loss:1.0279, Validation Accuracy:0.4676\n",
    "Epoch #146: Loss:0.9911, Accuracy:0.4995, Validation Loss:1.0355, Validation Accuracy:0.4573\n",
    "Epoch #147: Loss:0.9863, Accuracy:0.5062, Validation Loss:1.0385, Validation Accuracy:0.4553\n",
    "Epoch #148: Loss:0.9792, Accuracy:0.5138, Validation Loss:1.0305, Validation Accuracy:0.4499\n",
    "Epoch #149: Loss:0.9847, Accuracy:0.5055, Validation Loss:1.0329, Validation Accuracy:0.4626\n",
    "Epoch #150: Loss:0.9783, Accuracy:0.5107, Validation Loss:1.0323, Validation Accuracy:0.4581\n",
    "Epoch #151: Loss:0.9814, Accuracy:0.5047, Validation Loss:1.0344, Validation Accuracy:0.4544\n",
    "Epoch #152: Loss:0.9768, Accuracy:0.5142, Validation Loss:1.0337, Validation Accuracy:0.4631\n",
    "Epoch #153: Loss:0.9765, Accuracy:0.5093, Validation Loss:1.0328, Validation Accuracy:0.4676\n",
    "Epoch #154: Loss:0.9749, Accuracy:0.5154, Validation Loss:1.0358, Validation Accuracy:0.4573\n",
    "Epoch #155: Loss:0.9753, Accuracy:0.5112, Validation Loss:1.0336, Validation Accuracy:0.4643\n",
    "Epoch #156: Loss:0.9706, Accuracy:0.5121, Validation Loss:1.0365, Validation Accuracy:0.4626\n",
    "Epoch #157: Loss:0.9738, Accuracy:0.5155, Validation Loss:1.0377, Validation Accuracy:0.4598\n",
    "Epoch #158: Loss:0.9701, Accuracy:0.5189, Validation Loss:1.0389, Validation Accuracy:0.4692\n",
    "Epoch #159: Loss:0.9779, Accuracy:0.5103, Validation Loss:1.0311, Validation Accuracy:0.4635\n",
    "Epoch #160: Loss:0.9669, Accuracy:0.5298, Validation Loss:1.0311, Validation Accuracy:0.4647\n",
    "Epoch #161: Loss:0.9682, Accuracy:0.5234, Validation Loss:1.0318, Validation Accuracy:0.4663\n",
    "Epoch #162: Loss:0.9692, Accuracy:0.5216, Validation Loss:1.0342, Validation Accuracy:0.4647\n",
    "Epoch #163: Loss:0.9655, Accuracy:0.5243, Validation Loss:1.0321, Validation Accuracy:0.4692\n",
    "Epoch #164: Loss:0.9676, Accuracy:0.5219, Validation Loss:1.0315, Validation Accuracy:0.4524\n",
    "Epoch #165: Loss:0.9612, Accuracy:0.5251, Validation Loss:1.0338, Validation Accuracy:0.4631\n",
    "Epoch #166: Loss:0.9618, Accuracy:0.5283, Validation Loss:1.0355, Validation Accuracy:0.4647\n",
    "Epoch #167: Loss:0.9589, Accuracy:0.5300, Validation Loss:1.0324, Validation Accuracy:0.4672\n",
    "Epoch #168: Loss:0.9661, Accuracy:0.5269, Validation Loss:1.0438, Validation Accuracy:0.4507\n",
    "Epoch #169: Loss:0.9649, Accuracy:0.5241, Validation Loss:1.0381, Validation Accuracy:0.4639\n",
    "Epoch #170: Loss:0.9609, Accuracy:0.5291, Validation Loss:1.0408, Validation Accuracy:0.4614\n",
    "Epoch #171: Loss:0.9594, Accuracy:0.5342, Validation Loss:1.0306, Validation Accuracy:0.4729\n",
    "Epoch #172: Loss:0.9529, Accuracy:0.5367, Validation Loss:1.0382, Validation Accuracy:0.4553\n",
    "Epoch #173: Loss:0.9548, Accuracy:0.5309, Validation Loss:1.0418, Validation Accuracy:0.4692\n",
    "Epoch #174: Loss:0.9578, Accuracy:0.5366, Validation Loss:1.0390, Validation Accuracy:0.4688\n",
    "Epoch #175: Loss:0.9560, Accuracy:0.5306, Validation Loss:1.0321, Validation Accuracy:0.4581\n",
    "Epoch #176: Loss:0.9542, Accuracy:0.5292, Validation Loss:1.0409, Validation Accuracy:0.4553\n",
    "Epoch #177: Loss:0.9709, Accuracy:0.5128, Validation Loss:1.0294, Validation Accuracy:0.4676\n",
    "Epoch #178: Loss:0.9509, Accuracy:0.5376, Validation Loss:1.0238, Validation Accuracy:0.4737\n",
    "Epoch #179: Loss:0.9502, Accuracy:0.5332, Validation Loss:1.0260, Validation Accuracy:0.4733\n",
    "Epoch #180: Loss:0.9453, Accuracy:0.5414, Validation Loss:1.0353, Validation Accuracy:0.4536\n",
    "Epoch #181: Loss:0.9503, Accuracy:0.5390, Validation Loss:1.0272, Validation Accuracy:0.4704\n",
    "Epoch #182: Loss:0.9463, Accuracy:0.5424, Validation Loss:1.0399, Validation Accuracy:0.4577\n",
    "Epoch #183: Loss:0.9499, Accuracy:0.5381, Validation Loss:1.0353, Validation Accuracy:0.4758\n",
    "Epoch #184: Loss:0.9454, Accuracy:0.5408, Validation Loss:1.0351, Validation Accuracy:0.4704\n",
    "Epoch #185: Loss:0.9509, Accuracy:0.5366, Validation Loss:1.0395, Validation Accuracy:0.4589\n",
    "Epoch #186: Loss:0.9456, Accuracy:0.5378, Validation Loss:1.0350, Validation Accuracy:0.4713\n",
    "Epoch #187: Loss:0.9433, Accuracy:0.5408, Validation Loss:1.0305, Validation Accuracy:0.4659\n",
    "Epoch #188: Loss:0.9412, Accuracy:0.5450, Validation Loss:1.0340, Validation Accuracy:0.4692\n",
    "Epoch #189: Loss:0.9429, Accuracy:0.5438, Validation Loss:1.0297, Validation Accuracy:0.4680\n",
    "Epoch #190: Loss:0.9392, Accuracy:0.5429, Validation Loss:1.0529, Validation Accuracy:0.4573\n",
    "Epoch #191: Loss:0.9523, Accuracy:0.5334, Validation Loss:1.0391, Validation Accuracy:0.4626\n",
    "Epoch #192: Loss:0.9421, Accuracy:0.5471, Validation Loss:1.0302, Validation Accuracy:0.4667\n",
    "Epoch #193: Loss:0.9385, Accuracy:0.5438, Validation Loss:1.0320, Validation Accuracy:0.4692\n",
    "Epoch #194: Loss:0.9416, Accuracy:0.5445, Validation Loss:1.0275, Validation Accuracy:0.4741\n",
    "Epoch #195: Loss:0.9365, Accuracy:0.5474, Validation Loss:1.0396, Validation Accuracy:0.4483\n",
    "Epoch #196: Loss:0.9423, Accuracy:0.5470, Validation Loss:1.0295, Validation Accuracy:0.4709\n",
    "Epoch #197: Loss:0.9363, Accuracy:0.5448, Validation Loss:1.0345, Validation Accuracy:0.4700\n",
    "Epoch #198: Loss:0.9331, Accuracy:0.5455, Validation Loss:1.0342, Validation Accuracy:0.4762\n",
    "Epoch #199: Loss:0.9332, Accuracy:0.5497, Validation Loss:1.0351, Validation Accuracy:0.4725\n",
    "Epoch #200: Loss:0.9338, Accuracy:0.5495, Validation Loss:1.0414, Validation Accuracy:0.4667\n",
    "Epoch #201: Loss:0.9284, Accuracy:0.5508, Validation Loss:1.0334, Validation Accuracy:0.4733\n",
    "Epoch #202: Loss:0.9255, Accuracy:0.5574, Validation Loss:1.0389, Validation Accuracy:0.4787\n",
    "Epoch #203: Loss:0.9330, Accuracy:0.5520, Validation Loss:1.0455, Validation Accuracy:0.4709\n",
    "Epoch #204: Loss:0.9299, Accuracy:0.5505, Validation Loss:1.0474, Validation Accuracy:0.4618\n",
    "Epoch #205: Loss:0.9297, Accuracy:0.5508, Validation Loss:1.0486, Validation Accuracy:0.4577\n",
    "Epoch #206: Loss:0.9282, Accuracy:0.5498, Validation Loss:1.0431, Validation Accuracy:0.4713\n",
    "Epoch #207: Loss:0.9232, Accuracy:0.5544, Validation Loss:1.0541, Validation Accuracy:0.4667\n",
    "Epoch #208: Loss:0.9281, Accuracy:0.5506, Validation Loss:1.0401, Validation Accuracy:0.4704\n",
    "Epoch #209: Loss:0.9236, Accuracy:0.5559, Validation Loss:1.0511, Validation Accuracy:0.4540\n",
    "Epoch #210: Loss:0.9284, Accuracy:0.5516, Validation Loss:1.0420, Validation Accuracy:0.4635\n",
    "Epoch #211: Loss:0.9224, Accuracy:0.5539, Validation Loss:1.0430, Validation Accuracy:0.4770\n",
    "Epoch #212: Loss:0.9210, Accuracy:0.5568, Validation Loss:1.0486, Validation Accuracy:0.4631\n",
    "Epoch #213: Loss:0.9277, Accuracy:0.5500, Validation Loss:1.0435, Validation Accuracy:0.4594\n",
    "Epoch #214: Loss:0.9152, Accuracy:0.5608, Validation Loss:1.0489, Validation Accuracy:0.4626\n",
    "Epoch #215: Loss:0.9153, Accuracy:0.5670, Validation Loss:1.0479, Validation Accuracy:0.4606\n",
    "Epoch #216: Loss:0.9193, Accuracy:0.5574, Validation Loss:1.0501, Validation Accuracy:0.4544\n",
    "Epoch #217: Loss:0.9207, Accuracy:0.5549, Validation Loss:1.0475, Validation Accuracy:0.4766\n",
    "Epoch #218: Loss:0.9120, Accuracy:0.5672, Validation Loss:1.0571, Validation Accuracy:0.4585\n",
    "Epoch #219: Loss:0.9086, Accuracy:0.5694, Validation Loss:1.0493, Validation Accuracy:0.4643\n",
    "Epoch #220: Loss:0.9063, Accuracy:0.5687, Validation Loss:1.0477, Validation Accuracy:0.4663\n",
    "Epoch #221: Loss:0.9106, Accuracy:0.5584, Validation Loss:1.0534, Validation Accuracy:0.4663\n",
    "Epoch #222: Loss:0.9040, Accuracy:0.5722, Validation Loss:1.0512, Validation Accuracy:0.4561\n",
    "Epoch #223: Loss:0.9111, Accuracy:0.5671, Validation Loss:1.0539, Validation Accuracy:0.4663\n",
    "Epoch #224: Loss:0.9033, Accuracy:0.5708, Validation Loss:1.0523, Validation Accuracy:0.4651\n",
    "Epoch #225: Loss:0.9011, Accuracy:0.5720, Validation Loss:1.0547, Validation Accuracy:0.4713\n",
    "Epoch #226: Loss:0.9260, Accuracy:0.5537, Validation Loss:1.0480, Validation Accuracy:0.4758\n",
    "Epoch #227: Loss:0.9044, Accuracy:0.5644, Validation Loss:1.0599, Validation Accuracy:0.4540\n",
    "Epoch #228: Loss:0.8996, Accuracy:0.5739, Validation Loss:1.0604, Validation Accuracy:0.4585\n",
    "Epoch #229: Loss:0.9025, Accuracy:0.5689, Validation Loss:1.0485, Validation Accuracy:0.4594\n",
    "Epoch #230: Loss:0.8949, Accuracy:0.5749, Validation Loss:1.0490, Validation Accuracy:0.4626\n",
    "Epoch #231: Loss:0.8953, Accuracy:0.5746, Validation Loss:1.0613, Validation Accuracy:0.4618\n",
    "Epoch #232: Loss:0.9033, Accuracy:0.5703, Validation Loss:1.0559, Validation Accuracy:0.4704\n",
    "Epoch #233: Loss:0.9097, Accuracy:0.5637, Validation Loss:1.0498, Validation Accuracy:0.4672\n",
    "Epoch #234: Loss:0.8965, Accuracy:0.5732, Validation Loss:1.0582, Validation Accuracy:0.4569\n",
    "Epoch #235: Loss:0.8960, Accuracy:0.5731, Validation Loss:1.0681, Validation Accuracy:0.4614\n",
    "Epoch #236: Loss:0.8958, Accuracy:0.5763, Validation Loss:1.0720, Validation Accuracy:0.4540\n",
    "Epoch #237: Loss:0.8990, Accuracy:0.5725, Validation Loss:1.0525, Validation Accuracy:0.4622\n",
    "Epoch #238: Loss:0.8955, Accuracy:0.5767, Validation Loss:1.0557, Validation Accuracy:0.4729\n",
    "Epoch #239: Loss:0.8972, Accuracy:0.5746, Validation Loss:1.0776, Validation Accuracy:0.4688\n",
    "Epoch #240: Loss:0.9019, Accuracy:0.5728, Validation Loss:1.0550, Validation Accuracy:0.4610\n",
    "Epoch #241: Loss:0.8912, Accuracy:0.5769, Validation Loss:1.0572, Validation Accuracy:0.4635\n",
    "Epoch #242: Loss:0.8939, Accuracy:0.5737, Validation Loss:1.0546, Validation Accuracy:0.4721\n",
    "Epoch #243: Loss:0.8935, Accuracy:0.5781, Validation Loss:1.0536, Validation Accuracy:0.4729\n",
    "Epoch #244: Loss:0.8926, Accuracy:0.5791, Validation Loss:1.0618, Validation Accuracy:0.4651\n",
    "Epoch #245: Loss:0.8849, Accuracy:0.5834, Validation Loss:1.0628, Validation Accuracy:0.4717\n",
    "Epoch #246: Loss:0.8846, Accuracy:0.5799, Validation Loss:1.0779, Validation Accuracy:0.4811\n",
    "Epoch #247: Loss:0.8968, Accuracy:0.5687, Validation Loss:1.0683, Validation Accuracy:0.4672\n",
    "Epoch #248: Loss:0.8881, Accuracy:0.5796, Validation Loss:1.0599, Validation Accuracy:0.4651\n",
    "Epoch #249: Loss:0.8819, Accuracy:0.5841, Validation Loss:1.0679, Validation Accuracy:0.4655\n",
    "Epoch #250: Loss:0.8792, Accuracy:0.5831, Validation Loss:1.0657, Validation Accuracy:0.4610\n",
    "Epoch #251: Loss:0.8815, Accuracy:0.5855, Validation Loss:1.0719, Validation Accuracy:0.4602\n",
    "Epoch #252: Loss:0.8805, Accuracy:0.5835, Validation Loss:1.0720, Validation Accuracy:0.4573\n",
    "Epoch #253: Loss:0.8800, Accuracy:0.5818, Validation Loss:1.0749, Validation Accuracy:0.4639\n",
    "Epoch #254: Loss:0.8883, Accuracy:0.5780, Validation Loss:1.0653, Validation Accuracy:0.4626\n",
    "Epoch #255: Loss:0.8790, Accuracy:0.5872, Validation Loss:1.0742, Validation Accuracy:0.4729\n",
    "Epoch #256: Loss:0.8948, Accuracy:0.5745, Validation Loss:1.0692, Validation Accuracy:0.4680\n",
    "Epoch #257: Loss:0.8773, Accuracy:0.5858, Validation Loss:1.0620, Validation Accuracy:0.4692\n",
    "Epoch #258: Loss:0.8792, Accuracy:0.5808, Validation Loss:1.0737, Validation Accuracy:0.4667\n",
    "Epoch #259: Loss:0.8813, Accuracy:0.5825, Validation Loss:1.0719, Validation Accuracy:0.4700\n",
    "Epoch #260: Loss:0.8878, Accuracy:0.5820, Validation Loss:1.0744, Validation Accuracy:0.4639\n",
    "Epoch #261: Loss:0.8785, Accuracy:0.5853, Validation Loss:1.0856, Validation Accuracy:0.4655\n",
    "Epoch #262: Loss:0.8829, Accuracy:0.5828, Validation Loss:1.0728, Validation Accuracy:0.4651\n",
    "Epoch #263: Loss:0.8739, Accuracy:0.5896, Validation Loss:1.0655, Validation Accuracy:0.4680\n",
    "Epoch #264: Loss:0.8733, Accuracy:0.5890, Validation Loss:1.0797, Validation Accuracy:0.4700\n",
    "Epoch #265: Loss:0.8728, Accuracy:0.5898, Validation Loss:1.0861, Validation Accuracy:0.4676\n",
    "Epoch #266: Loss:0.8724, Accuracy:0.5905, Validation Loss:1.0764, Validation Accuracy:0.4676\n",
    "Epoch #267: Loss:0.8708, Accuracy:0.5900, Validation Loss:1.0816, Validation Accuracy:0.4667\n",
    "Epoch #268: Loss:0.8619, Accuracy:0.5935, Validation Loss:1.0752, Validation Accuracy:0.4688\n",
    "Epoch #269: Loss:0.8646, Accuracy:0.5979, Validation Loss:1.0963, Validation Accuracy:0.4688\n",
    "Epoch #270: Loss:0.8893, Accuracy:0.5745, Validation Loss:1.0733, Validation Accuracy:0.4631\n",
    "Epoch #271: Loss:0.8679, Accuracy:0.5912, Validation Loss:1.0906, Validation Accuracy:0.4540\n",
    "Epoch #272: Loss:0.8929, Accuracy:0.5732, Validation Loss:1.0717, Validation Accuracy:0.4561\n",
    "Epoch #273: Loss:0.8637, Accuracy:0.5957, Validation Loss:1.0815, Validation Accuracy:0.4704\n",
    "Epoch #274: Loss:0.8639, Accuracy:0.5944, Validation Loss:1.0734, Validation Accuracy:0.4639\n",
    "Epoch #275: Loss:0.8621, Accuracy:0.5953, Validation Loss:1.0808, Validation Accuracy:0.4643\n",
    "Epoch #276: Loss:0.8603, Accuracy:0.6002, Validation Loss:1.0887, Validation Accuracy:0.4672\n",
    "Epoch #277: Loss:0.8558, Accuracy:0.5998, Validation Loss:1.0858, Validation Accuracy:0.4721\n",
    "Epoch #278: Loss:0.8639, Accuracy:0.5923, Validation Loss:1.0769, Validation Accuracy:0.4635\n",
    "Epoch #279: Loss:0.8725, Accuracy:0.5894, Validation Loss:1.0744, Validation Accuracy:0.4639\n",
    "Epoch #280: Loss:0.8624, Accuracy:0.5977, Validation Loss:1.0749, Validation Accuracy:0.4709\n",
    "Epoch #281: Loss:0.8739, Accuracy:0.5915, Validation Loss:1.0926, Validation Accuracy:0.4717\n",
    "Epoch #282: Loss:0.8603, Accuracy:0.5945, Validation Loss:1.0878, Validation Accuracy:0.4721\n",
    "Epoch #283: Loss:0.8599, Accuracy:0.5995, Validation Loss:1.0928, Validation Accuracy:0.4635\n",
    "Epoch #284: Loss:0.8570, Accuracy:0.6008, Validation Loss:1.0808, Validation Accuracy:0.4663\n",
    "Epoch #285: Loss:0.8574, Accuracy:0.6012, Validation Loss:1.1001, Validation Accuracy:0.4745\n",
    "Epoch #286: Loss:0.8615, Accuracy:0.5951, Validation Loss:1.0827, Validation Accuracy:0.4676\n",
    "Epoch #287: Loss:0.8669, Accuracy:0.5972, Validation Loss:1.0876, Validation Accuracy:0.4651\n",
    "Epoch #288: Loss:0.8590, Accuracy:0.6008, Validation Loss:1.0806, Validation Accuracy:0.4614\n",
    "Epoch #289: Loss:0.8576, Accuracy:0.6024, Validation Loss:1.0950, Validation Accuracy:0.4635\n",
    "Epoch #290: Loss:0.8507, Accuracy:0.6042, Validation Loss:1.1093, Validation Accuracy:0.4635\n",
    "Epoch #291: Loss:0.8662, Accuracy:0.5939, Validation Loss:1.1033, Validation Accuracy:0.4643\n",
    "Epoch #292: Loss:0.8510, Accuracy:0.5989, Validation Loss:1.0886, Validation Accuracy:0.4598\n",
    "Epoch #293: Loss:0.8488, Accuracy:0.6059, Validation Loss:1.0849, Validation Accuracy:0.4626\n",
    "Epoch #294: Loss:0.8568, Accuracy:0.6006, Validation Loss:1.0964, Validation Accuracy:0.4631\n",
    "Epoch #295: Loss:0.8684, Accuracy:0.5899, Validation Loss:1.0831, Validation Accuracy:0.4585\n",
    "Epoch #296: Loss:0.8705, Accuracy:0.5857, Validation Loss:1.0770, Validation Accuracy:0.4762\n",
    "Epoch #297: Loss:0.8440, Accuracy:0.6104, Validation Loss:1.0987, Validation Accuracy:0.4676\n",
    "Epoch #298: Loss:0.8394, Accuracy:0.6139, Validation Loss:1.0997, Validation Accuracy:0.4655\n",
    "Epoch #299: Loss:0.8413, Accuracy:0.6051, Validation Loss:1.0913, Validation Accuracy:0.4594\n",
    "Epoch #300: Loss:0.8437, Accuracy:0.6046, Validation Loss:1.0922, Validation Accuracy:0.4618\n",
    "\n",
    "Test:\n",
    "Test Loss:1.09215021, Accuracy:0.4618\n",
    "Labels: ['01', '03', '02']\n",
    "Confusion Matrix:\n",
    "       01   03   02\n",
    "t:01  447  135  378\n",
    "t:03  174  180  214\n",
    "t:02  268  142  498\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.50      0.47      0.48       960\n",
    "          03       0.39      0.32      0.35       568\n",
    "          02       0.46      0.55      0.50       908\n",
    "\n",
    "    accuracy                           0.46      2436\n",
    "   macro avg       0.45      0.44      0.44      2436\n",
    "weighted avg       0.46      0.46      0.46      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.30 12:01:22 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 14 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.07500932134431, 1.074116957990211, 1.074417072172431, 1.0742492358672795, 1.073841505449981, 1.0734468966673552, 1.073218627907764, 1.0731065146050038, 1.0728514086828247, 1.0725857140786934, 1.0724132769605013, 1.0722146866161053, 1.072281052326334, 1.071484888519951, 1.0710059801737468, 1.0717068190253622, 1.0704460647110086, 1.0700696041235587, 1.0691061303729104, 1.0686455873041514, 1.0686184813823607, 1.0682008356492116, 1.0679870806695597, 1.066592934292134, 1.0662914628074283, 1.0663298374326358, 1.070511749029551, 1.065591353892497, 1.0641123864842557, 1.0650087336601295, 1.063098581945172, 1.064356804285535, 1.0616584540587928, 1.0623295587076147, 1.0645909266323095, 1.0614715967272303, 1.0615887771099073, 1.0613078519041315, 1.0588531650737394, 1.0595422489889736, 1.0645752460107036, 1.0615159987620337, 1.0577419381619282, 1.059090471228551, 1.0596100086061826, 1.0591362677575724, 1.0587825232930175, 1.0601137167910246, 1.0602253669588437, 1.0568647709581849, 1.0566899907608533, 1.0616456123408426, 1.0592168115434193, 1.0579416074580552, 1.0568741933857082, 1.058754910389191, 1.0579555800004155, 1.0568930529217022, 1.0562396088648704, 1.055064926006524, 1.0560806054004113, 1.0556336003179816, 1.056193285191979, 1.0565294796610114, 1.0553078549640325, 1.0566383647214015, 1.0555419491233888, 1.058640041179062, 1.057003310552763, 1.0534634560787033, 1.0567947712242114, 1.055588200370275, 1.0543333389880427, 1.0539913854771255, 1.0531212614087635, 1.055401325813068, 1.0530606435829, 1.051422249898926, 1.052658127054988, 1.0528852802583542, 1.0514987942033214, 1.0558240869753859, 1.050009247313188, 1.0499869708357186, 1.0496576276710272, 1.051255879143776, 1.0497348543458385, 1.0483437066007717, 1.0499453415424365, 1.0484625353601766, 1.0502342208852908, 1.0500654316888067, 1.047363085895532, 1.0487829579899854, 1.0527321260746672, 1.0530573848041602, 1.049364719680573, 1.050031982031949, 1.0457705799581969, 1.0471807313082842, 1.0458960247352989, 1.048403126265615, 1.045737623776904, 1.0433748680578272, 1.0426031462664676, 1.0437615894527466, 1.0425174798088512, 1.0398843141612162, 1.0425353615937758, 1.0425880894872355, 1.0421098276899365, 1.0438075367061572, 1.0364043432699244, 1.0372561713549109, 1.0379679473162873, 1.0387475424016441, 1.041158534623132, 1.0378485923917422, 1.0374519511788154, 1.041662008891552, 1.0386052687571359, 1.0433448378871424, 1.0378996198400487, 1.0332522126058443, 1.0383804320114587, 1.0344135148576132, 1.0367071761677809, 1.034910495058069, 1.0378935981070858, 1.0401331006208272, 1.0358443148617673, 1.0376994829068238, 1.0414040292229363, 1.038681578362125, 1.0471331787422569, 1.0440350876653135, 1.0358594260583762, 1.0376405919518181, 1.0341274305713197, 1.0386228561401367, 1.0350133871601523, 1.035750145591147, 1.0377129324159795, 1.0306336640920153, 1.0278702651338625, 1.035507980042882, 1.0384804642650685, 1.030477094141329, 1.0328548780607276, 1.032310478792989, 1.0343759641271506, 1.0337097120206735, 1.0327953789230246, 1.0358202582705394, 1.0335892027822033, 1.0365062552719868, 1.0376697068143947, 1.038943507205481, 1.0310505912417458, 1.0310969282253621, 1.031786069494163, 1.0342123800114849, 1.0320808468585336, 1.031487636573992, 1.033816617502172, 1.0355042702654509, 1.03243926475788, 1.0437620126555118, 1.038060461554817, 1.0407532144258371, 1.030634919410856, 1.038240090183828, 1.0417787140030383, 1.0389775176745135, 1.0320894906086286, 1.0409390110100432, 1.0293898453266161, 1.0238186481159504, 1.0259945872186245, 1.0353388754996564, 1.0271502703868698, 1.0399352866049079, 1.0353320658892051, 1.0351349078179972, 1.0394826456048023, 1.0349713166554768, 1.0305239183366397, 1.0339880522053033, 1.0297266411272372, 1.0528901880010595, 1.0390897627143045, 1.0301917464470824, 1.0320306160962835, 1.0275030048022717, 1.0395919090421328, 1.029513998571875, 1.0345462619377475, 1.0341772128795754, 1.0351039349347695, 1.0414472581522023, 1.033429289294777, 1.0389458628123618, 1.04553603910656, 1.0473555046545069, 1.0485976764133997, 1.0431142671550633, 1.0541383948036407, 1.0401337916040656, 1.0511491308462835, 1.042004108820447, 1.0429970707212175, 1.0486172365241841, 1.0435442656327547, 1.0488700453675244, 1.0479480184749235, 1.0501057467437143, 1.0475320142673936, 1.0571441159068267, 1.0492553045401236, 1.0477199027886728, 1.0533759907157159, 1.0511909813325002, 1.0539222505488028, 1.0523396765657247, 1.0546515960802978, 1.048040216387982, 1.059900891604682, 1.0604300596835383, 1.0485301634361004, 1.048971751053345, 1.0612949193600558, 1.0559082041037298, 1.0497735543008313, 1.0581877699430744, 1.0681306250968394, 1.0719656388356376, 1.0524519746526708, 1.055663496402684, 1.0775990012439796, 1.0549889591527102, 1.0572145873885632, 1.0545628031682106, 1.0536333614186504, 1.0618462999074525, 1.062814493288939, 1.0779237040549468, 1.06825108344136, 1.059919989755001, 1.0678954128365603, 1.0656580036301135, 1.0719134237965926, 1.0720483232993014, 1.074869872705494, 1.065307358019849, 1.074166340780963, 1.0692492619719607, 1.0620308906965459, 1.0736606792471874, 1.0719248941183481, 1.0744012345625653, 1.0856310244655765, 1.0728251973200706, 1.0654832083603432, 1.0796812328407526, 1.0860896552920538, 1.076402465306675, 1.0815549902923784, 1.0751819356125956, 1.096341561018344, 1.0732652972680203, 1.090613813431588, 1.0716682741011696, 1.0814836291452543, 1.0733590728935154, 1.0808461980866682, 1.0887499509382326, 1.0857787363243416, 1.0769126565977074, 1.074359280526736, 1.0748721384649793, 1.0926045075621706, 1.0877939840451445, 1.0928273557246417, 1.0807997487448706, 1.1000853696675919, 1.0826643928518436, 1.087616137487352, 1.080574650482591, 1.0949858213684633, 1.1093116667861813, 1.1032653771010525, 1.0886078798907928, 1.0849351712635584, 1.0964108210283352, 1.0831065745580764, 1.077006292656333, 1.0987368060645994, 1.0996880969781986, 1.091335190928041, 1.092150131078385], 'val_acc': [0.394088669363501, 0.4031198702520142, 0.394088669363501, 0.39696223209252696, 0.394088669363501, 0.39778325059535274, 0.4014778342740289, 0.40106732284494223, 0.3986042710067016, 0.4010673227960057, 0.40024630478254486, 0.40476190632787246, 0.4084564876575971, 0.40229884974279234, 0.4043513949477222, 0.4064039423058577, 0.40106732494921127, 0.41215106903625826, 0.4174876865397142, 0.41954022915101014, 0.4096880146043837, 0.4129720868050367, 0.4252873558324742, 0.41954023154889813, 0.4244663374275214, 0.42118226532474135, 0.402709361269752, 0.4146141231255774, 0.4187192131928818, 0.423645321763012, 0.4248768487587351, 0.419129719875129, 0.4298029572309923, 0.4228243011559172, 0.42816092115513404, 0.423645319365124, 0.4232348103339253, 0.4298029548331043, 0.43185549994016126, 0.4273399030438002, 0.422413792173655, 0.4261083759502041, 0.4306239753912627, 0.42939244575296914, 0.4293924481997936, 0.4376026264258793, 0.4339080453408371, 0.4298029570841828, 0.43103448192670035, 0.43144499100683553, 0.43349753611389247, 0.42528735793674327, 0.43021346645793695, 0.42610837335657015, 0.4339080475429792, 0.42816091865937306, 0.4326765204983196, 0.4248768487587351, 0.4334975384139075, 0.43431855422522636, 0.4330870294316453, 0.42980295473523134, 0.43760262637694286, 0.43021346606644506, 0.4371921196457592, 0.4326765204493831, 0.43719211734574415, 0.4371921196946957, 0.4363710992833468, 0.4351395722386872, 0.43185550224017627, 0.43390804509615466, 0.4400656830109595, 0.4343185540294804, 0.43062397534232616, 0.4339080475429792, 0.432266011271375, 0.43513957218975075, 0.43226600902029644, 0.4310344842756519, 0.4367816082166725, 0.4326765179536221, 0.4392446649974986, 0.4367816106145605, 0.4306239728465652, 0.4359605902521481, 0.4351395722386872, 0.432266011271375, 0.43596059010533866, 0.4330870270826938, 0.43555008352096447, 0.44047619194428517, 0.4371921196457592, 0.42651888488352985, 0.43144499335578707, 0.43349753851178047, 0.42364531916937803, 0.4326765204983196, 0.4306239728465652, 0.4371921197436322, 0.44540229816546384, 0.4384236470329742, 0.44868637276400486, 0.4388341559173634, 0.4384236445372132, 0.43226600892242345, 0.44745484567040883, 0.4478653547994805, 0.43760262657268884, 0.45155993603133215, 0.44622331627679773, 0.4408866989690878, 0.44376026429174764, 0.4499179000044104, 0.44663382755907494, 0.4408867009754839, 0.4462233163746707, 0.44950739082640223, 0.4495073885263872, 0.44868637266613187, 0.4449917913364072, 0.4392446648017526, 0.45032840903560906, 0.44868637036611686, 0.44622331867468573, 0.45032840653984807, 0.452380951744778, 0.4527914630759917, 0.451149424553309, 0.45812807813262313, 0.45073891566891977, 0.456896553632661, 0.4503284065887846, 0.4474548432725208, 0.4412972077066675, 0.44129721034923797, 0.45689655113690003, 0.45402298786957274, 0.4474548433214573, 0.45853858965958283, 0.45607553297662967, 0.4499178975086494, 0.45361248138307153, 0.45730706007022576, 0.46756978604593885, 0.45730706237024077, 0.4552545149142323, 0.4499178999065374, 0.4626436800694427, 0.4581280807262571, 0.45443349685183493, 0.4630541891495778, 0.46756978604593885, 0.45730705997235277, 0.4642857138452859, 0.4626436798736967, 0.4597701143063544, 0.46921182451968513, 0.46346469803396706, 0.4646962251764996, 0.4663382587076604, 0.46469622532330906, 0.4692118222686066, 0.452380951744778, 0.46305418900276835, 0.46469622498075364, 0.4671592793147552, 0.45073891772425234, 0.46387520452046827, 0.4614121504800856, 0.4729064035493948, 0.45525451486529583, 0.46921182207286066, 0.46880131309059847, 0.45812807808368666, 0.45525451721424737, 0.4675697858991294, 0.4737274239607437, 0.473316912335911, 0.45361248099157964, 0.4704433516132812, 0.4577175690035515, 0.4757799667188491, 0.4704433517111542, 0.4589490961950205, 0.47126436972461505, 0.46592775217222265, 0.4692118219749877, 0.4679802973771526, 0.4573070625170503, 0.4626436774758087, 0.46674877023462, 0.4692118219749877, 0.4741379328451329, 0.44827586148172766, 0.4708538583934013, 0.47003284013525803, 0.4761904780500628, 0.47249589681821114, 0.466748767934605, 0.47331691228697453, 0.47865353248193737, 0.4708538581976554, 0.4618226616644898, 0.4577175716950584, 0.4712643672777906, 0.4667487679835415, 0.47044335136859877, 0.4540229903653337, 0.46346469558714254, 0.47701149386138164, 0.46305418650700736, 0.4593596074772977, 0.46264368002050615, 0.4605911345219573, 0.45443349920078646, 0.476600984634437, 0.4585385872616948, 0.4642857160474279, 0.46633825875659685, 0.46633826115448485, 0.4560755331234392, 0.4663382590012793, 0.4651067340119523, 0.47126436742460004, 0.47577996686565854, 0.4540229879674457, 0.4585385897085193, 0.4593596052751557, 0.4626436774758087, 0.4618226619581088, 0.4704433515154082, 0.4671592769658037, 0.4568965533879786, 0.46141215043114914, 0.4540229902674608, 0.462233168542483, 0.47290640580047333, 0.4688013156352959, 0.4610016415467599, 0.46346469558714254, 0.4720853853891244, 0.47290640580047333, 0.46510673420769827, 0.47167487640686223, 0.481116586718066, 0.4671592769658037, 0.4651067342566348, 0.465517243238897, 0.46100164369996544, 0.4601806258822505, 0.45730706016809874, 0.46387520711410224, 0.46264367772049114, 0.47290640364726777, 0.46798029747502556, 0.46921182466649464, 0.46674876788566855, 0.470032840233131, 0.4638752070651657, 0.46551724098781844, 0.46510673420769827, 0.46798029517501055, 0.470032840233131, 0.4675697860948754, 0.46756978854169984, 0.4667487679835415, 0.46880131524380403, 0.4688013131884714, 0.4630541891495778, 0.4540229877716998, 0.4560755354723907, 0.4704433516132812, 0.46387520701622925, 0.4642857136495399, 0.46715927706367666, 0.4720853853891244, 0.46346469558714254, 0.4638752046672777, 0.4708538583934013, 0.47167487640686223, 0.47208538783594894, 0.4634646957828885, 0.4663382589523428, 0.474548439723126, 0.46756978604593885, 0.4651067343545077, 0.46141215043114914, 0.46346469813184005, 0.46346469563607906, 0.46428571629211035, 0.4597701167042424, 0.4626436777694276, 0.46305418910064133, 0.45853858716382184, 0.4761904757011113, 0.4675697859970024, 0.4655172409388819, 0.4593596075262342, 0.46182265941341133], 'loss': [1.0796993845542109, 1.0745798513874625, 1.0745038340958237, 1.0750614954950384, 1.0737124308423585, 1.0738498908538348, 1.0735018309626492, 1.0733463462128532, 1.0731574276389528, 1.0731159912732102, 1.073056179248332, 1.0727400059572725, 1.0730580637831952, 1.0717085456701274, 1.0717152765399376, 1.0725415225880837, 1.070595768736618, 1.0695369341045435, 1.0697421605092543, 1.069349130272131, 1.0684577358332012, 1.0673608173335112, 1.0660231120042978, 1.0644774289346572, 1.064306682972448, 1.0626471035779135, 1.0621858344675335, 1.060992573810554, 1.0628657643555126, 1.0616753096942784, 1.060481068632686, 1.0581910447663105, 1.0580534044477239, 1.0550783702235447, 1.0588282438274281, 1.0544255548923658, 1.0528580428638497, 1.055230382237836, 1.0529248062834848, 1.0528913665356332, 1.052774144198126, 1.0560773356004907, 1.0526663911905623, 1.0495300717911926, 1.0487877576013365, 1.0518331236907834, 1.0500227236405046, 1.0481494789495605, 1.0487682207409117, 1.0495629663095336, 1.0474446643304531, 1.0508818451628792, 1.0477970271629475, 1.048865362506138, 1.046771374155119, 1.0459794324526308, 1.0469652211152063, 1.0438187923029953, 1.0443074864283723, 1.0446218179481475, 1.0436303170065127, 1.0429395761822773, 1.041041537476761, 1.0408378108570953, 1.0453042196786868, 1.0415336265211477, 1.0414448374595486, 1.0386324870757744, 1.042794802595213, 1.0391354239207273, 1.0377475944143055, 1.039277249709292, 1.0369513335903566, 1.0345181010831797, 1.0351448730766406, 1.0370184602433896, 1.038347700880783, 1.0374213564567252, 1.0341063289671708, 1.0377366894079674, 1.0337720392910608, 1.03324940625647, 1.0354848468083375, 1.0314907036278038, 1.0327208239439822, 1.028371670162898, 1.0315254670149003, 1.0270469635419042, 1.0261357929672303, 1.0275445874956355, 1.0272838335017649, 1.0299433506000213, 1.0248102924906988, 1.0233626855472275, 1.0248989935528325, 1.0226754225744603, 1.0278808614801331, 1.0246900331068334, 1.0242476766359145, 1.0229698660926898, 1.020095075400703, 1.0192161097418846, 1.02409538165255, 1.0182279704043018, 1.0184548327320655, 1.0159120109536564, 1.0162709163444488, 1.0180548424593476, 1.013724232503276, 1.0149223122019053, 1.0105021604522297, 1.0168905281677874, 1.0208878088780742, 1.0136367152603745, 1.0108757545815845, 1.0100124790438392, 1.011736074634646, 1.010256955217287, 1.0057667279635123, 1.0043195140680004, 1.004995439894635, 1.0027893277653923, 1.0078214436341115, 1.0065294159266494, 1.000190010697445, 1.0106450803470808, 1.0007801450253513, 0.9980268873962779, 0.9978379892862308, 0.9971537917546424, 0.9998383598161185, 1.000226037482706, 0.9973444564631343, 0.9936197804719271, 0.9963342590743267, 1.0039226255622489, 0.9943164194633829, 0.9940706186960365, 0.9892763711833367, 0.9861395630258799, 0.9932331306488852, 0.9850712749257959, 0.9938267482379624, 0.9867660725875557, 0.9857978073233696, 0.9911101425697671, 0.9863454607722696, 0.979221705685406, 0.9846555463342451, 0.978262159785206, 0.9813726417337845, 0.9767972266159998, 0.9764770214072978, 0.9748543810305899, 0.9752804085459308, 0.9705720576662302, 0.9737618342806916, 0.9700813828307745, 0.9778622292395245, 0.9669271579268532, 0.968231943916736, 0.9691832105481893, 0.9654909509897721, 0.9676224003582274, 0.9611756274097999, 0.9617582124606295, 0.9589049057304492, 0.966097733054073, 0.9648547337285303, 0.9608780981823649, 0.9594252518070308, 0.9529407336726571, 0.9548279356907525, 0.9577513199322523, 0.9559652422243076, 0.9542055642825132, 0.970877187061114, 0.950937037032243, 0.9502499590664184, 0.9452534005382467, 0.9502776315569633, 0.9463266034390647, 0.9498577121591666, 0.9453928120816758, 0.9509043487924814, 0.9456052621532025, 0.9433419914461014, 0.9412491333313302, 0.9428708319546506, 0.9392034905157539, 0.9523259587601225, 0.9420599953105073, 0.93852696034453, 0.9415716451296327, 0.9365405468480543, 0.9423391638105655, 0.9363014307599783, 0.9331408876413193, 0.9331909588964568, 0.9337620414747595, 0.9283924138276729, 0.9255209060175463, 0.932969497679685, 0.9299257176367899, 0.9296629894195886, 0.9282177264195938, 0.9232287910439885, 0.9281163538261116, 0.9235896486031691, 0.9283633552292778, 0.9223929497985135, 0.9210489207714245, 0.9277097852812656, 0.9151625619776684, 0.9152703189507158, 0.9192710795931258, 0.920686982911715, 0.9120441509223327, 0.9086071848379758, 0.9062620844194776, 0.9106045981207422, 0.9040009194575785, 0.9111054219014836, 0.9032677940274656, 0.9010503857287538, 0.9259569851524776, 0.9043917272125181, 0.8995686593486543, 0.902547801789317, 0.8948595454315875, 0.8952744873642187, 0.9033018582655419, 0.9097449332781641, 0.8964602101265282, 0.8960048073371089, 0.8958353628123321, 0.8989937376192708, 0.8954891565399248, 0.8972321266511137, 0.9018738619600722, 0.891231936522333, 0.893889237918893, 0.8935174839942117, 0.892570316253012, 0.8849221452550476, 0.8845881761466698, 0.8968146608350703, 0.8881301700701704, 0.8819210607413149, 0.8792300349633062, 0.8814654191416637, 0.8804554035776205, 0.8799899186197003, 0.8882561466532322, 0.8790371989322638, 0.8948043174812191, 0.8772616511742437, 0.8791979631359328, 0.8812908948079762, 0.8878379096975072, 0.8784511064112309, 0.8828799272954342, 0.8739044167177877, 0.8733439226904444, 0.8727836583429294, 0.8724076057116843, 0.8708252209412733, 0.8618844533358266, 0.8646272062276178, 0.889292330345334, 0.867920191381012, 0.892925870369592, 0.8637170797255984, 0.8638555250618247, 0.8620870780161519, 0.8602959320775293, 0.8558018032285467, 0.8639450858505844, 0.8725137449387897, 0.8623798936789041, 0.8738901824677014, 0.8603452269056739, 0.8599441260282998, 0.8569731113847031, 0.8574491320449469, 0.8615498470085112, 0.8669387314109097, 0.858953352165418, 0.8576221822468407, 0.850700862182484, 0.8662474979120603, 0.8510131463622655, 0.8488306191422856, 0.8567907644737918, 0.8683918115539473, 0.87054639907833, 0.8440299803716201, 0.8394481631275075, 0.8412776385244648, 0.8437166062223838], 'acc': [0.38223819303071965, 0.3895277207453393, 0.38809034910045365, 0.39312114990956976, 0.40328542095679765, 0.4034907597658326, 0.3972279260657896, 0.3997946611909651, 0.399589322369691, 0.4041067761929373, 0.40308008212328444, 0.4054414784394251, 0.40718685829174345, 0.4150924024395874, 0.40338809035213574, 0.4050308008091161, 0.4103696098623824, 0.41642710471055344, 0.41457905546595675, 0.4162217659137577, 0.41632443534275343, 0.4228952772196313, 0.42104722791383886, 0.4295687884765484, 0.42412731006160165, 0.4321355236017239, 0.42597535935515496, 0.42731006158940354, 0.4298767967023399, 0.4276180698029559, 0.42597535933067665, 0.4375770020656272, 0.4349075975359343, 0.43757700205338806, 0.43049281312944465, 0.4369609856018051, 0.4410677618314598, 0.4387063654908409, 0.44086242301018574, 0.44065708420115085, 0.4400410677373287, 0.4368583161972876, 0.44640657082964996, 0.44383983571671365, 0.44455852156057496, 0.44537987680895375, 0.4437371663121962, 0.44722792605355044, 0.4502053387845566, 0.4454825462073516, 0.44476386036960985, 0.44219712526891264, 0.4485626283489948, 0.4475359343160594, 0.44404517456246595, 0.44312114990956974, 0.44568788501026696, 0.44640657082964996, 0.4473305954825462, 0.44958932238193017, 0.44671457904320233, 0.44753593427934196, 0.4482546201109641, 0.45082135523613964, 0.4512320328297312, 0.4470225872934721, 0.44969199178644764, 0.4516427104722793, 0.45102669404517454, 0.45492813142907695, 0.45174537988903585, 0.449486653001891, 0.4502053388090349, 0.45431211498973306, 0.4584188911459529, 0.4495893223574519, 0.45266940452357335, 0.4499999999755217, 0.45308008211104533, 0.455133470250351, 0.4505133470225873, 0.4572895277207392, 0.4531827515278019, 0.4607802874865718, 0.4578028747433265, 0.4601642710472279, 0.4534907597413543, 0.4602669404395062, 0.46386036960985627, 0.46201232034078127, 0.4586242299549878, 0.45944558519112744, 0.4672484599466931, 0.4660164271292011, 0.46540041065313975, 0.4688911704312115, 0.460472279236302, 0.4696098562383554, 0.46447638602472185, 0.4677618070059978, 0.4667351129118667, 0.4716632443654219, 0.46786447638603695, 0.4712525667595912, 0.4674537987924454, 0.47761806981519506, 0.4741273100493625, 0.4777207392074734, 0.4782340862178215, 0.46930184803704217, 0.47864476387260874, 0.4772073922093644, 0.46355236138406475, 0.47422997947835827, 0.47710472278036864, 0.47987679671457906, 0.4793634497164701, 0.48685831623400505, 0.4844969199178645, 0.4838809034662815, 0.48655030800821353, 0.48028747433264884, 0.4824435318275154, 0.48151950716238, 0.49322381927736975, 0.4757700205338809, 0.48655030800821353, 0.4931211498973306, 0.4878850102669405, 0.4893223819301848, 0.4878850102547013, 0.4933264886818872, 0.49332648870636553, 0.4942505133531422, 0.4980492813264075, 0.4847022587146602, 0.49219712525667353, 0.4924024640657084, 0.49958932240640846, 0.5046201232277637, 0.4938398357044745, 0.5018480492813142, 0.5021560575193448, 0.5037987679671458, 0.506981519519426, 0.49948665300189105, 0.5061601642771668, 0.5137577001808605, 0.5055441478194642, 0.510677618057576, 0.5047227926078028, 0.5141683778356477, 0.5093429158110883, 0.5154004106776181, 0.5111909651046416, 0.5121149897575379, 0.5155030800576572, 0.5188911704556898, 0.5102669404395063, 0.5297741273100616, 0.5234086242055012, 0.5215605749731436, 0.5243326488706366, 0.5218685831377393, 0.5250513347022587, 0.528336755622339, 0.5299794661190965, 0.5268993840080511, 0.5241273100616016, 0.5290554414662003, 0.5341889116798338, 0.5366529774249702, 0.5309034907597536, 0.5365503079837353, 0.5305954825462013, 0.5291581108707177, 0.5128336755402034, 0.5375770020778664, 0.5331622176468984, 0.5413757700205338, 0.5390143737288716, 0.5424024640657085, 0.5380903490514971, 0.54075975358119, 0.5365503079837353, 0.5377823408869012, 0.5407597536056683, 0.5449691991664056, 0.5438398357289528, 0.5429158110638174, 0.5333675564681725, 0.5471252566735113, 0.5438398357289528, 0.5444558521315792, 0.5474332648870637, 0.5470225872445156, 0.5447638603696099, 0.5454825462257104, 0.5496919917619694, 0.549486653001891, 0.5508213552606179, 0.5573921971007784, 0.5519507186613534, 0.5505133470225873, 0.5508213552116613, 0.5497946611909651, 0.5544147843697722, 0.5506160164393439, 0.5558521560636145, 0.551642710447801, 0.553901437347185, 0.5567761806859128, 0.5499999999755217, 0.5607802874498543, 0.5670431211498973, 0.557392197149735, 0.554928131441316, 0.5672484599344539, 0.569404517478277, 0.5686858316221766, 0.5584188911459529, 0.57217659137577, 0.567145790578893, 0.5708418891415215, 0.5719712525422568, 0.5536960985871067, 0.5643737166569218, 0.5739219712280884, 0.5688911704434506, 0.5749486652977412, 0.5746406570841889, 0.5703285420822167, 0.563655030776343, 0.5732032854454229, 0.5731006159919488, 0.5762833675564681, 0.5724845995648441, 0.576694045174538, 0.5746406570841889, 0.5727926078028748, 0.5768993840080512, 0.5737166324680101, 0.5781314168377824, 0.5790554414539612, 0.5833675564681725, 0.5798767966901007, 0.5686858315976983, 0.5795687885010267, 0.5840862422753164, 0.5830595482546201, 0.585523613963039, 0.5834702258726899, 0.5818275153759324, 0.5780287474087866, 0.5871663244353182, 0.5745379876551931, 0.5858316222010697, 0.5808008213307578, 0.5825462012565111, 0.5820328542094456, 0.5853182751784825, 0.5827515400655461, 0.5896303901192589, 0.5890143737411108, 0.589835728952772, 0.5904517454043551, 0.5900410677373287, 0.5935318275154005, 0.5979466119096509, 0.5745379876551931, 0.5911704312114989, 0.5732032854454229, 0.5956878850347452, 0.5943531827637792, 0.5952772073921971, 0.6002053388090349, 0.5997946611787259, 0.592299794661191, 0.5894250513347022, 0.5977412730883769, 0.591478439400573, 0.5944558521315793, 0.5994866529529345, 0.6008213552116614, 0.6012320328786878, 0.5950718685586839, 0.5972279260780288, 0.600821355260618, 0.6023613963283797, 0.6042094456096939, 0.5939425051089919, 0.5988706365380688, 0.6058521560574949, 0.6006160164393439, 0.5899383983328113, 0.5857289527720739, 0.6103696098317846, 0.6138603696343345, 0.6051334702013944, 0.6046201231910463]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
