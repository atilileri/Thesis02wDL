{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf18.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 18:35:22 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': 'Front', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001DB261E5E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001DB209F7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1262, Accuracy:0.2542, Validation Loss:1.0954, Validation Accuracy:0.3678\n",
    "Epoch #2: Loss:1.0833, Accuracy:0.3901, Validation Loss:1.0807, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0790, Accuracy:0.3943, Validation Loss:1.0797, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0771, Accuracy:0.3943, Validation Loss:1.0770, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0751, Accuracy:0.3943, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #6: Loss:1.0740, Accuracy:0.3959, Validation Loss:1.0761, Validation Accuracy:0.3678\n",
    "Epoch #7: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0770, Validation Accuracy:0.3580\n",
    "Epoch #8: Loss:1.0740, Accuracy:0.3823, Validation Loss:1.0764, Validation Accuracy:0.3678\n",
    "Epoch #9: Loss:1.0735, Accuracy:0.3979, Validation Loss:1.0762, Validation Accuracy:0.3695\n",
    "Epoch #10: Loss:1.0736, Accuracy:0.3984, Validation Loss:1.0756, Validation Accuracy:0.3957\n",
    "Epoch #11: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0756, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0745, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0741, Accuracy:0.3934, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #16: Loss:1.0739, Accuracy:0.3901, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #17: Loss:1.0743, Accuracy:0.3914, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #18: Loss:1.0742, Accuracy:0.3918, Validation Loss:1.0745, Validation Accuracy:0.3908\n",
    "Epoch #19: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0746, Validation Accuracy:0.3990\n",
    "Epoch #20: Loss:1.0737, Accuracy:0.3967, Validation Loss:1.0748, Validation Accuracy:0.3990\n",
    "Epoch #21: Loss:1.0734, Accuracy:0.3938, Validation Loss:1.0751, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0734, Accuracy:0.3938, Validation Loss:1.0751, Validation Accuracy:0.3924\n",
    "Epoch #23: Loss:1.0733, Accuracy:0.3926, Validation Loss:1.0749, Validation Accuracy:0.3957\n",
    "Epoch #24: Loss:1.0730, Accuracy:0.3971, Validation Loss:1.0749, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0731, Accuracy:0.3996, Validation Loss:1.0747, Validation Accuracy:0.3957\n",
    "Epoch #26: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #27: Loss:1.0725, Accuracy:0.3971, Validation Loss:1.0755, Validation Accuracy:0.3957\n",
    "Epoch #28: Loss:1.0729, Accuracy:0.4004, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #29: Loss:1.0730, Accuracy:0.4053, Validation Loss:1.0755, Validation Accuracy:0.3924\n",
    "Epoch #30: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0751, Validation Accuracy:0.3957\n",
    "Epoch #31: Loss:1.0724, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #32: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0752, Validation Accuracy:0.3892\n",
    "Epoch #33: Loss:1.0726, Accuracy:0.3971, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0726, Accuracy:0.3979, Validation Loss:1.0755, Validation Accuracy:0.3908\n",
    "Epoch #35: Loss:1.0732, Accuracy:0.3963, Validation Loss:1.0754, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0730, Accuracy:0.3971, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #37: Loss:1.0728, Accuracy:0.4029, Validation Loss:1.0752, Validation Accuracy:0.3810\n",
    "Epoch #38: Loss:1.0727, Accuracy:0.4016, Validation Loss:1.0756, Validation Accuracy:0.3711\n",
    "Epoch #39: Loss:1.0726, Accuracy:0.4045, Validation Loss:1.0753, Validation Accuracy:0.3760\n",
    "Epoch #40: Loss:1.0724, Accuracy:0.4033, Validation Loss:1.0751, Validation Accuracy:0.3826\n",
    "Epoch #41: Loss:1.0728, Accuracy:0.4000, Validation Loss:1.0753, Validation Accuracy:0.3924\n",
    "Epoch #42: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0749, Validation Accuracy:0.3924\n",
    "Epoch #43: Loss:1.0726, Accuracy:0.4029, Validation Loss:1.0751, Validation Accuracy:0.3875\n",
    "Epoch #44: Loss:1.0729, Accuracy:0.4041, Validation Loss:1.0755, Validation Accuracy:0.3826\n",
    "Epoch #45: Loss:1.0727, Accuracy:0.4016, Validation Loss:1.0754, Validation Accuracy:0.3760\n",
    "Epoch #46: Loss:1.0726, Accuracy:0.4049, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #47: Loss:1.0725, Accuracy:0.4033, Validation Loss:1.0749, Validation Accuracy:0.3859\n",
    "Epoch #48: Loss:1.0723, Accuracy:0.4000, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #49: Loss:1.0719, Accuracy:0.4070, Validation Loss:1.0750, Validation Accuracy:0.3678\n",
    "Epoch #50: Loss:1.0720, Accuracy:0.4070, Validation Loss:1.0752, Validation Accuracy:0.3875\n",
    "Epoch #51: Loss:1.0721, Accuracy:0.4086, Validation Loss:1.0761, Validation Accuracy:0.3530\n",
    "Epoch #52: Loss:1.0720, Accuracy:0.3996, Validation Loss:1.0756, Validation Accuracy:0.3924\n",
    "Epoch #53: Loss:1.0719, Accuracy:0.4025, Validation Loss:1.0762, Validation Accuracy:0.3842\n",
    "Epoch #54: Loss:1.0718, Accuracy:0.4062, Validation Loss:1.0762, Validation Accuracy:0.3842\n",
    "Epoch #55: Loss:1.0714, Accuracy:0.4111, Validation Loss:1.0756, Validation Accuracy:0.3612\n",
    "Epoch #56: Loss:1.0715, Accuracy:0.4082, Validation Loss:1.0763, Validation Accuracy:0.3612\n",
    "Epoch #57: Loss:1.0711, Accuracy:0.4094, Validation Loss:1.0763, Validation Accuracy:0.3678\n",
    "Epoch #58: Loss:1.0714, Accuracy:0.4074, Validation Loss:1.0762, Validation Accuracy:0.3793\n",
    "Epoch #59: Loss:1.0714, Accuracy:0.4074, Validation Loss:1.0766, Validation Accuracy:0.3629\n",
    "Epoch #60: Loss:1.0713, Accuracy:0.4086, Validation Loss:1.0774, Validation Accuracy:0.3662\n",
    "Epoch #61: Loss:1.0714, Accuracy:0.4078, Validation Loss:1.0764, Validation Accuracy:0.3826\n",
    "Epoch #62: Loss:1.0710, Accuracy:0.4127, Validation Loss:1.0767, Validation Accuracy:0.3645\n",
    "Epoch #63: Loss:1.0708, Accuracy:0.4160, Validation Loss:1.0765, Validation Accuracy:0.3727\n",
    "Epoch #64: Loss:1.0705, Accuracy:0.4115, Validation Loss:1.0770, Validation Accuracy:0.3727\n",
    "Epoch #65: Loss:1.0708, Accuracy:0.4103, Validation Loss:1.0765, Validation Accuracy:0.3777\n",
    "Epoch #66: Loss:1.0705, Accuracy:0.4107, Validation Loss:1.0765, Validation Accuracy:0.3727\n",
    "Epoch #67: Loss:1.0702, Accuracy:0.4148, Validation Loss:1.0766, Validation Accuracy:0.3777\n",
    "Epoch #68: Loss:1.0700, Accuracy:0.4086, Validation Loss:1.0769, Validation Accuracy:0.3711\n",
    "Epoch #69: Loss:1.0697, Accuracy:0.4123, Validation Loss:1.0772, Validation Accuracy:0.3711\n",
    "Epoch #70: Loss:1.0692, Accuracy:0.4082, Validation Loss:1.0773, Validation Accuracy:0.3645\n",
    "Epoch #71: Loss:1.0698, Accuracy:0.4103, Validation Loss:1.0776, Validation Accuracy:0.3810\n",
    "Epoch #72: Loss:1.0705, Accuracy:0.4099, Validation Loss:1.0777, Validation Accuracy:0.3990\n",
    "Epoch #73: Loss:1.0708, Accuracy:0.4057, Validation Loss:1.0762, Validation Accuracy:0.3744\n",
    "Epoch #74: Loss:1.0711, Accuracy:0.4078, Validation Loss:1.0772, Validation Accuracy:0.4023\n",
    "Epoch #75: Loss:1.0703, Accuracy:0.4160, Validation Loss:1.0770, Validation Accuracy:0.4039\n",
    "Epoch #76: Loss:1.0703, Accuracy:0.4140, Validation Loss:1.0776, Validation Accuracy:0.4007\n",
    "Epoch #77: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0765, Validation Accuracy:0.3859\n",
    "Epoch #78: Loss:1.0707, Accuracy:0.4086, Validation Loss:1.0767, Validation Accuracy:0.4056\n",
    "Epoch #79: Loss:1.0704, Accuracy:0.4156, Validation Loss:1.0765, Validation Accuracy:0.3695\n",
    "Epoch #80: Loss:1.0699, Accuracy:0.4094, Validation Loss:1.0768, Validation Accuracy:0.3711\n",
    "Epoch #81: Loss:1.0704, Accuracy:0.4033, Validation Loss:1.0769, Validation Accuracy:0.3941\n",
    "Epoch #82: Loss:1.0701, Accuracy:0.4090, Validation Loss:1.0766, Validation Accuracy:0.3645\n",
    "Epoch #83: Loss:1.0703, Accuracy:0.4115, Validation Loss:1.0790, Validation Accuracy:0.3629\n",
    "Epoch #84: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0776, Validation Accuracy:0.3711\n",
    "Epoch #85: Loss:1.0691, Accuracy:0.4115, Validation Loss:1.0769, Validation Accuracy:0.3645\n",
    "Epoch #86: Loss:1.0698, Accuracy:0.4107, Validation Loss:1.0775, Validation Accuracy:0.3629\n",
    "Epoch #87: Loss:1.0696, Accuracy:0.4070, Validation Loss:1.0799, Validation Accuracy:0.3744\n",
    "Epoch #88: Loss:1.0725, Accuracy:0.3959, Validation Loss:1.0768, Validation Accuracy:0.3974\n",
    "Epoch #89: Loss:1.0736, Accuracy:0.3959, Validation Loss:1.0762, Validation Accuracy:0.3941\n",
    "Epoch #90: Loss:1.0744, Accuracy:0.3774, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #91: Loss:1.0737, Accuracy:0.3922, Validation Loss:1.0755, Validation Accuracy:0.3990\n",
    "Epoch #92: Loss:1.0735, Accuracy:0.3967, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #93: Loss:1.0719, Accuracy:0.4021, Validation Loss:1.0729, Validation Accuracy:0.3990\n",
    "Epoch #94: Loss:1.0718, Accuracy:0.4090, Validation Loss:1.0744, Validation Accuracy:0.3859\n",
    "Epoch #95: Loss:1.0705, Accuracy:0.4012, Validation Loss:1.0738, Validation Accuracy:0.3990\n",
    "Epoch #96: Loss:1.0708, Accuracy:0.4049, Validation Loss:1.0742, Validation Accuracy:0.3760\n",
    "Epoch #97: Loss:1.0708, Accuracy:0.4111, Validation Loss:1.0778, Validation Accuracy:0.3629\n",
    "Epoch #98: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0764, Validation Accuracy:0.3974\n",
    "Epoch #99: Loss:1.0711, Accuracy:0.4037, Validation Loss:1.0777, Validation Accuracy:0.3957\n",
    "Epoch #100: Loss:1.0698, Accuracy:0.4045, Validation Loss:1.0772, Validation Accuracy:0.3990\n",
    "Epoch #101: Loss:1.0700, Accuracy:0.4086, Validation Loss:1.0766, Validation Accuracy:0.3810\n",
    "Epoch #102: Loss:1.0696, Accuracy:0.4099, Validation Loss:1.0774, Validation Accuracy:0.4072\n",
    "Epoch #103: Loss:1.0703, Accuracy:0.4062, Validation Loss:1.0770, Validation Accuracy:0.3842\n",
    "Epoch #104: Loss:1.0698, Accuracy:0.4066, Validation Loss:1.0781, Validation Accuracy:0.3760\n",
    "Epoch #105: Loss:1.0708, Accuracy:0.4008, Validation Loss:1.0765, Validation Accuracy:0.3744\n",
    "Epoch #106: Loss:1.0724, Accuracy:0.3951, Validation Loss:1.0794, Validation Accuracy:0.3760\n",
    "Epoch #107: Loss:1.0721, Accuracy:0.4008, Validation Loss:1.0779, Validation Accuracy:0.3744\n",
    "Epoch #108: Loss:1.0732, Accuracy:0.4029, Validation Loss:1.0785, Validation Accuracy:0.3892\n",
    "Epoch #109: Loss:1.0780, Accuracy:0.3975, Validation Loss:1.0715, Validation Accuracy:0.3957\n",
    "Epoch #110: Loss:1.0738, Accuracy:0.3897, Validation Loss:1.0757, Validation Accuracy:0.3727\n",
    "Epoch #111: Loss:1.0738, Accuracy:0.3918, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #112: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #113: Loss:1.0724, Accuracy:0.3992, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #114: Loss:1.0737, Accuracy:0.3951, Validation Loss:1.0752, Validation Accuracy:0.3793\n",
    "Epoch #115: Loss:1.0719, Accuracy:0.4131, Validation Loss:1.0760, Validation Accuracy:0.3744\n",
    "Epoch #116: Loss:1.0717, Accuracy:0.4074, Validation Loss:1.0762, Validation Accuracy:0.3760\n",
    "Epoch #117: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0760, Validation Accuracy:0.3727\n",
    "Epoch #118: Loss:1.0715, Accuracy:0.4008, Validation Loss:1.0755, Validation Accuracy:0.3859\n",
    "Epoch #119: Loss:1.0714, Accuracy:0.4037, Validation Loss:1.0756, Validation Accuracy:0.3711\n",
    "Epoch #120: Loss:1.0707, Accuracy:0.4070, Validation Loss:1.0758, Validation Accuracy:0.3695\n",
    "Epoch #121: Loss:1.0706, Accuracy:0.4074, Validation Loss:1.0761, Validation Accuracy:0.3744\n",
    "Epoch #122: Loss:1.0707, Accuracy:0.4082, Validation Loss:1.0756, Validation Accuracy:0.3662\n",
    "Epoch #123: Loss:1.0707, Accuracy:0.4053, Validation Loss:1.0757, Validation Accuracy:0.3695\n",
    "Epoch #124: Loss:1.0709, Accuracy:0.4053, Validation Loss:1.0765, Validation Accuracy:0.3629\n",
    "Epoch #125: Loss:1.0715, Accuracy:0.4021, Validation Loss:1.0761, Validation Accuracy:0.3793\n",
    "Epoch #126: Loss:1.0711, Accuracy:0.4021, Validation Loss:1.0769, Validation Accuracy:0.3662\n",
    "Epoch #127: Loss:1.0712, Accuracy:0.4029, Validation Loss:1.0773, Validation Accuracy:0.3793\n",
    "Epoch #128: Loss:1.0710, Accuracy:0.4123, Validation Loss:1.0773, Validation Accuracy:0.3612\n",
    "Epoch #129: Loss:1.0711, Accuracy:0.4037, Validation Loss:1.0776, Validation Accuracy:0.3596\n",
    "Epoch #130: Loss:1.0721, Accuracy:0.4099, Validation Loss:1.0776, Validation Accuracy:0.3793\n",
    "Epoch #131: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0774, Validation Accuracy:0.3645\n",
    "Epoch #132: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0773, Validation Accuracy:0.3612\n",
    "Epoch #133: Loss:1.0709, Accuracy:0.4016, Validation Loss:1.0768, Validation Accuracy:0.3793\n",
    "Epoch #134: Loss:1.0707, Accuracy:0.4070, Validation Loss:1.0766, Validation Accuracy:0.3760\n",
    "Epoch #135: Loss:1.0709, Accuracy:0.4029, Validation Loss:1.0772, Validation Accuracy:0.3563\n",
    "Epoch #136: Loss:1.0704, Accuracy:0.4053, Validation Loss:1.0767, Validation Accuracy:0.3612\n",
    "Epoch #137: Loss:1.0704, Accuracy:0.4057, Validation Loss:1.0772, Validation Accuracy:0.3514\n",
    "Epoch #138: Loss:1.0714, Accuracy:0.4074, Validation Loss:1.0773, Validation Accuracy:0.3760\n",
    "Epoch #139: Loss:1.0706, Accuracy:0.4029, Validation Loss:1.0788, Validation Accuracy:0.3530\n",
    "Epoch #140: Loss:1.0712, Accuracy:0.4086, Validation Loss:1.0771, Validation Accuracy:0.3580\n",
    "Epoch #141: Loss:1.0704, Accuracy:0.4082, Validation Loss:1.0770, Validation Accuracy:0.3580\n",
    "Epoch #142: Loss:1.0702, Accuracy:0.4033, Validation Loss:1.0779, Validation Accuracy:0.3580\n",
    "Epoch #143: Loss:1.0713, Accuracy:0.4103, Validation Loss:1.0767, Validation Accuracy:0.3924\n",
    "Epoch #144: Loss:1.0715, Accuracy:0.4045, Validation Loss:1.0769, Validation Accuracy:0.3678\n",
    "Epoch #145: Loss:1.0709, Accuracy:0.4033, Validation Loss:1.0758, Validation Accuracy:0.3793\n",
    "Epoch #146: Loss:1.0709, Accuracy:0.4045, Validation Loss:1.0760, Validation Accuracy:0.3777\n",
    "Epoch #147: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0756, Validation Accuracy:0.3711\n",
    "Epoch #148: Loss:1.0708, Accuracy:0.4049, Validation Loss:1.0758, Validation Accuracy:0.3744\n",
    "Epoch #149: Loss:1.0709, Accuracy:0.4045, Validation Loss:1.0759, Validation Accuracy:0.3793\n",
    "Epoch #150: Loss:1.0716, Accuracy:0.4021, Validation Loss:1.0760, Validation Accuracy:0.3744\n",
    "Epoch #151: Loss:1.0710, Accuracy:0.4021, Validation Loss:1.0765, Validation Accuracy:0.3711\n",
    "Epoch #152: Loss:1.0710, Accuracy:0.3975, Validation Loss:1.0769, Validation Accuracy:0.3711\n",
    "Epoch #153: Loss:1.0710, Accuracy:0.4016, Validation Loss:1.0765, Validation Accuracy:0.3842\n",
    "Epoch #154: Loss:1.0705, Accuracy:0.4004, Validation Loss:1.0768, Validation Accuracy:0.3727\n",
    "Epoch #155: Loss:1.0707, Accuracy:0.4037, Validation Loss:1.0749, Validation Accuracy:0.3727\n",
    "Epoch #156: Loss:1.0705, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.3727\n",
    "Epoch #157: Loss:1.0708, Accuracy:0.4041, Validation Loss:1.0741, Validation Accuracy:0.3711\n",
    "Epoch #158: Loss:1.0703, Accuracy:0.4037, Validation Loss:1.0753, Validation Accuracy:0.3678\n",
    "Epoch #159: Loss:1.0711, Accuracy:0.4057, Validation Loss:1.0770, Validation Accuracy:0.3662\n",
    "Epoch #160: Loss:1.0748, Accuracy:0.4074, Validation Loss:1.0796, Validation Accuracy:0.3547\n",
    "Epoch #161: Loss:1.0708, Accuracy:0.4086, Validation Loss:1.0771, Validation Accuracy:0.3875\n",
    "Epoch #162: Loss:1.0719, Accuracy:0.4057, Validation Loss:1.0753, Validation Accuracy:0.3875\n",
    "Epoch #163: Loss:1.0703, Accuracy:0.3992, Validation Loss:1.0766, Validation Accuracy:0.3760\n",
    "Epoch #164: Loss:1.0703, Accuracy:0.4107, Validation Loss:1.0756, Validation Accuracy:0.3760\n",
    "Epoch #165: Loss:1.0698, Accuracy:0.4131, Validation Loss:1.0750, Validation Accuracy:0.3793\n",
    "Epoch #166: Loss:1.0697, Accuracy:0.4140, Validation Loss:1.0753, Validation Accuracy:0.3727\n",
    "Epoch #167: Loss:1.0703, Accuracy:0.4078, Validation Loss:1.0765, Validation Accuracy:0.3727\n",
    "Epoch #168: Loss:1.0706, Accuracy:0.4094, Validation Loss:1.0756, Validation Accuracy:0.3727\n",
    "Epoch #169: Loss:1.0713, Accuracy:0.4086, Validation Loss:1.0770, Validation Accuracy:0.3727\n",
    "Epoch #170: Loss:1.0720, Accuracy:0.4156, Validation Loss:1.0773, Validation Accuracy:0.3629\n",
    "Epoch #171: Loss:1.0734, Accuracy:0.3951, Validation Loss:1.0742, Validation Accuracy:0.3842\n",
    "Epoch #172: Loss:1.0722, Accuracy:0.4041, Validation Loss:1.0983, Validation Accuracy:0.3810\n",
    "Epoch #173: Loss:1.1068, Accuracy:0.3918, Validation Loss:1.0853, Validation Accuracy:0.3727\n",
    "Epoch #174: Loss:1.0781, Accuracy:0.3918, Validation Loss:1.0888, Validation Accuracy:0.3941\n",
    "Epoch #175: Loss:1.0811, Accuracy:0.3947, Validation Loss:1.0835, Validation Accuracy:0.3941\n",
    "Epoch #176: Loss:1.0756, Accuracy:0.3971, Validation Loss:1.0786, Validation Accuracy:0.3892\n",
    "Epoch #177: Loss:1.0724, Accuracy:0.3979, Validation Loss:1.0782, Validation Accuracy:0.3530\n",
    "Epoch #178: Loss:1.0731, Accuracy:0.4037, Validation Loss:1.0785, Validation Accuracy:0.3530\n",
    "Epoch #179: Loss:1.0734, Accuracy:0.4070, Validation Loss:1.0782, Validation Accuracy:0.3744\n",
    "Epoch #180: Loss:1.0729, Accuracy:0.4029, Validation Loss:1.0773, Validation Accuracy:0.3760\n",
    "Epoch #181: Loss:1.0726, Accuracy:0.4049, Validation Loss:1.0773, Validation Accuracy:0.3744\n",
    "Epoch #182: Loss:1.0728, Accuracy:0.3988, Validation Loss:1.0775, Validation Accuracy:0.3842\n",
    "Epoch #183: Loss:1.0729, Accuracy:0.4037, Validation Loss:1.0773, Validation Accuracy:0.3711\n",
    "Epoch #184: Loss:1.0727, Accuracy:0.4037, Validation Loss:1.0772, Validation Accuracy:0.3711\n",
    "Epoch #185: Loss:1.0726, Accuracy:0.4045, Validation Loss:1.0772, Validation Accuracy:0.3744\n",
    "Epoch #186: Loss:1.0725, Accuracy:0.4029, Validation Loss:1.0773, Validation Accuracy:0.3727\n",
    "Epoch #187: Loss:1.0725, Accuracy:0.4041, Validation Loss:1.0774, Validation Accuracy:0.3711\n",
    "Epoch #188: Loss:1.0725, Accuracy:0.4057, Validation Loss:1.0774, Validation Accuracy:0.3711\n",
    "Epoch #189: Loss:1.0724, Accuracy:0.4057, Validation Loss:1.0775, Validation Accuracy:0.3711\n",
    "Epoch #190: Loss:1.0726, Accuracy:0.4049, Validation Loss:1.0775, Validation Accuracy:0.3695\n",
    "Epoch #191: Loss:1.0725, Accuracy:0.4049, Validation Loss:1.0773, Validation Accuracy:0.3727\n",
    "Epoch #192: Loss:1.0724, Accuracy:0.4057, Validation Loss:1.0774, Validation Accuracy:0.3727\n",
    "Epoch #193: Loss:1.0723, Accuracy:0.4045, Validation Loss:1.0775, Validation Accuracy:0.3695\n",
    "Epoch #194: Loss:1.0722, Accuracy:0.4053, Validation Loss:1.0776, Validation Accuracy:0.3662\n",
    "Epoch #195: Loss:1.0724, Accuracy:0.4074, Validation Loss:1.0775, Validation Accuracy:0.3662\n",
    "Epoch #196: Loss:1.0722, Accuracy:0.4053, Validation Loss:1.0774, Validation Accuracy:0.3695\n",
    "Epoch #197: Loss:1.0721, Accuracy:0.4074, Validation Loss:1.0775, Validation Accuracy:0.3711\n",
    "Epoch #198: Loss:1.0722, Accuracy:0.4041, Validation Loss:1.0776, Validation Accuracy:0.3645\n",
    "Epoch #199: Loss:1.0722, Accuracy:0.4094, Validation Loss:1.0776, Validation Accuracy:0.3695\n",
    "Epoch #200: Loss:1.0721, Accuracy:0.4070, Validation Loss:1.0775, Validation Accuracy:0.3695\n",
    "Epoch #201: Loss:1.0721, Accuracy:0.4057, Validation Loss:1.0775, Validation Accuracy:0.3678\n",
    "Epoch #202: Loss:1.0720, Accuracy:0.4057, Validation Loss:1.0777, Validation Accuracy:0.3727\n",
    "Epoch #203: Loss:1.0721, Accuracy:0.4086, Validation Loss:1.0777, Validation Accuracy:0.3727\n",
    "Epoch #204: Loss:1.0721, Accuracy:0.4094, Validation Loss:1.0776, Validation Accuracy:0.3645\n",
    "Epoch #205: Loss:1.0721, Accuracy:0.4025, Validation Loss:1.0776, Validation Accuracy:0.3547\n",
    "Epoch #206: Loss:1.0720, Accuracy:0.4025, Validation Loss:1.0777, Validation Accuracy:0.3711\n",
    "Epoch #207: Loss:1.0720, Accuracy:0.4074, Validation Loss:1.0777, Validation Accuracy:0.3662\n",
    "Epoch #208: Loss:1.0721, Accuracy:0.4062, Validation Loss:1.0776, Validation Accuracy:0.3727\n",
    "Epoch #209: Loss:1.0719, Accuracy:0.4049, Validation Loss:1.0776, Validation Accuracy:0.3727\n",
    "Epoch #210: Loss:1.0719, Accuracy:0.4082, Validation Loss:1.0778, Validation Accuracy:0.3727\n",
    "Epoch #211: Loss:1.0720, Accuracy:0.4062, Validation Loss:1.0779, Validation Accuracy:0.3695\n",
    "Epoch #212: Loss:1.0719, Accuracy:0.4041, Validation Loss:1.0778, Validation Accuracy:0.3678\n",
    "Epoch #213: Loss:1.0718, Accuracy:0.4037, Validation Loss:1.0777, Validation Accuracy:0.3678\n",
    "Epoch #214: Loss:1.0717, Accuracy:0.4066, Validation Loss:1.0777, Validation Accuracy:0.3711\n",
    "Epoch #215: Loss:1.0717, Accuracy:0.4066, Validation Loss:1.0778, Validation Accuracy:0.3547\n",
    "Epoch #216: Loss:1.0719, Accuracy:0.3984, Validation Loss:1.0777, Validation Accuracy:0.3695\n",
    "Epoch #217: Loss:1.0717, Accuracy:0.4115, Validation Loss:1.0777, Validation Accuracy:0.3711\n",
    "Epoch #218: Loss:1.0717, Accuracy:0.4094, Validation Loss:1.0778, Validation Accuracy:0.3727\n",
    "Epoch #219: Loss:1.0718, Accuracy:0.4074, Validation Loss:1.0775, Validation Accuracy:0.3744\n",
    "Epoch #220: Loss:1.0718, Accuracy:0.4066, Validation Loss:1.0775, Validation Accuracy:0.3777\n",
    "Epoch #221: Loss:1.0716, Accuracy:0.4074, Validation Loss:1.0775, Validation Accuracy:0.3711\n",
    "Epoch #222: Loss:1.0716, Accuracy:0.4099, Validation Loss:1.0775, Validation Accuracy:0.3547\n",
    "Epoch #223: Loss:1.0718, Accuracy:0.4107, Validation Loss:1.0773, Validation Accuracy:0.3760\n",
    "Epoch #224: Loss:1.0717, Accuracy:0.4103, Validation Loss:1.0774, Validation Accuracy:0.3711\n",
    "Epoch #225: Loss:1.0716, Accuracy:0.4115, Validation Loss:1.0774, Validation Accuracy:0.3711\n",
    "Epoch #226: Loss:1.0722, Accuracy:0.4074, Validation Loss:1.0777, Validation Accuracy:0.3744\n",
    "Epoch #227: Loss:1.0716, Accuracy:0.4094, Validation Loss:1.0774, Validation Accuracy:0.3744\n",
    "Epoch #228: Loss:1.0713, Accuracy:0.4057, Validation Loss:1.0775, Validation Accuracy:0.3678\n",
    "Epoch #229: Loss:1.0714, Accuracy:0.4103, Validation Loss:1.0775, Validation Accuracy:0.3530\n",
    "Epoch #230: Loss:1.0714, Accuracy:0.4037, Validation Loss:1.0775, Validation Accuracy:0.3678\n",
    "Epoch #231: Loss:1.0713, Accuracy:0.4111, Validation Loss:1.0774, Validation Accuracy:0.3678\n",
    "Epoch #232: Loss:1.0713, Accuracy:0.4111, Validation Loss:1.0772, Validation Accuracy:0.3695\n",
    "Epoch #233: Loss:1.0711, Accuracy:0.4111, Validation Loss:1.0771, Validation Accuracy:0.3711\n",
    "Epoch #234: Loss:1.0714, Accuracy:0.4094, Validation Loss:1.0773, Validation Accuracy:0.3727\n",
    "Epoch #235: Loss:1.0712, Accuracy:0.4082, Validation Loss:1.0774, Validation Accuracy:0.3678\n",
    "Epoch #236: Loss:1.0715, Accuracy:0.4107, Validation Loss:1.0771, Validation Accuracy:0.3695\n",
    "Epoch #237: Loss:1.0710, Accuracy:0.4078, Validation Loss:1.0769, Validation Accuracy:0.3711\n",
    "Epoch #238: Loss:1.0713, Accuracy:0.4107, Validation Loss:1.0773, Validation Accuracy:0.3711\n",
    "Epoch #239: Loss:1.0711, Accuracy:0.4090, Validation Loss:1.0768, Validation Accuracy:0.3744\n",
    "Epoch #240: Loss:1.0708, Accuracy:0.4074, Validation Loss:1.0768, Validation Accuracy:0.3744\n",
    "Epoch #241: Loss:1.0708, Accuracy:0.4078, Validation Loss:1.0768, Validation Accuracy:0.3744\n",
    "Epoch #242: Loss:1.0706, Accuracy:0.4074, Validation Loss:1.0767, Validation Accuracy:0.3711\n",
    "Epoch #243: Loss:1.0707, Accuracy:0.4099, Validation Loss:1.0767, Validation Accuracy:0.3760\n",
    "Epoch #244: Loss:1.0705, Accuracy:0.4078, Validation Loss:1.0771, Validation Accuracy:0.3744\n",
    "Epoch #245: Loss:1.0707, Accuracy:0.4082, Validation Loss:1.0771, Validation Accuracy:0.3727\n",
    "Epoch #246: Loss:1.0707, Accuracy:0.4066, Validation Loss:1.0767, Validation Accuracy:0.3777\n",
    "Epoch #247: Loss:1.0707, Accuracy:0.4090, Validation Loss:1.0763, Validation Accuracy:0.3777\n",
    "Epoch #248: Loss:1.0709, Accuracy:0.4049, Validation Loss:1.0765, Validation Accuracy:0.3744\n",
    "Epoch #249: Loss:1.0703, Accuracy:0.4086, Validation Loss:1.0768, Validation Accuracy:0.3777\n",
    "Epoch #250: Loss:1.0710, Accuracy:0.4070, Validation Loss:1.0755, Validation Accuracy:0.3810\n",
    "Epoch #251: Loss:1.0711, Accuracy:0.4049, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #252: Loss:1.0714, Accuracy:0.4049, Validation Loss:1.0748, Validation Accuracy:0.3826\n",
    "Epoch #253: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0755, Validation Accuracy:0.3777\n",
    "Epoch #254: Loss:1.0713, Accuracy:0.4115, Validation Loss:1.0755, Validation Accuracy:0.3793\n",
    "Epoch #255: Loss:1.0721, Accuracy:0.4045, Validation Loss:1.0753, Validation Accuracy:0.3793\n",
    "Epoch #256: Loss:1.0722, Accuracy:0.3934, Validation Loss:1.0761, Validation Accuracy:0.3629\n",
    "Epoch #257: Loss:1.0713, Accuracy:0.4025, Validation Loss:1.0754, Validation Accuracy:0.3793\n",
    "Epoch #258: Loss:1.0714, Accuracy:0.4057, Validation Loss:1.0749, Validation Accuracy:0.3826\n",
    "Epoch #259: Loss:1.0714, Accuracy:0.4025, Validation Loss:1.0743, Validation Accuracy:0.3826\n",
    "Epoch #260: Loss:1.0710, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.3793\n",
    "Epoch #261: Loss:1.0712, Accuracy:0.4037, Validation Loss:1.0757, Validation Accuracy:0.3629\n",
    "Epoch #262: Loss:1.0711, Accuracy:0.4025, Validation Loss:1.0757, Validation Accuracy:0.3793\n",
    "Epoch #263: Loss:1.0712, Accuracy:0.4041, Validation Loss:1.0756, Validation Accuracy:0.3777\n",
    "Epoch #264: Loss:1.0712, Accuracy:0.4078, Validation Loss:1.0744, Validation Accuracy:0.3760\n",
    "Epoch #265: Loss:1.0707, Accuracy:0.4041, Validation Loss:1.0739, Validation Accuracy:0.3810\n",
    "Epoch #266: Loss:1.0713, Accuracy:0.4033, Validation Loss:1.0738, Validation Accuracy:0.3810\n",
    "Epoch #267: Loss:1.0717, Accuracy:0.4012, Validation Loss:1.0741, Validation Accuracy:0.3777\n",
    "Epoch #268: Loss:1.0708, Accuracy:0.4062, Validation Loss:1.0744, Validation Accuracy:0.3777\n",
    "Epoch #269: Loss:1.0714, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3727\n",
    "Epoch #270: Loss:1.0712, Accuracy:0.4070, Validation Loss:1.0753, Validation Accuracy:0.4007\n",
    "Epoch #271: Loss:1.0718, Accuracy:0.4111, Validation Loss:1.0745, Validation Accuracy:0.3793\n",
    "Epoch #272: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0745, Validation Accuracy:0.3826\n",
    "Epoch #273: Loss:1.0701, Accuracy:0.4045, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #274: Loss:1.0702, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.3826\n",
    "Epoch #275: Loss:1.0701, Accuracy:0.4037, Validation Loss:1.0742, Validation Accuracy:0.3793\n",
    "Epoch #276: Loss:1.0702, Accuracy:0.4078, Validation Loss:1.0740, Validation Accuracy:0.3842\n",
    "Epoch #277: Loss:1.0700, Accuracy:0.4057, Validation Loss:1.0743, Validation Accuracy:0.3810\n",
    "Epoch #278: Loss:1.0703, Accuracy:0.4049, Validation Loss:1.0742, Validation Accuracy:0.3810\n",
    "Epoch #279: Loss:1.0704, Accuracy:0.4041, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #280: Loss:1.0697, Accuracy:0.4082, Validation Loss:1.0742, Validation Accuracy:0.3777\n",
    "Epoch #281: Loss:1.0697, Accuracy:0.4074, Validation Loss:1.0746, Validation Accuracy:0.3810\n",
    "Epoch #282: Loss:1.0698, Accuracy:0.4066, Validation Loss:1.0748, Validation Accuracy:0.3793\n",
    "Epoch #283: Loss:1.0696, Accuracy:0.4074, Validation Loss:1.0749, Validation Accuracy:0.3744\n",
    "Epoch #284: Loss:1.0697, Accuracy:0.4057, Validation Loss:1.0745, Validation Accuracy:0.3810\n",
    "Epoch #285: Loss:1.0699, Accuracy:0.4057, Validation Loss:1.0743, Validation Accuracy:0.3810\n",
    "Epoch #286: Loss:1.0702, Accuracy:0.4070, Validation Loss:1.0740, Validation Accuracy:0.3760\n",
    "Epoch #287: Loss:1.0705, Accuracy:0.4049, Validation Loss:1.0738, Validation Accuracy:0.3777\n",
    "Epoch #288: Loss:1.0702, Accuracy:0.4078, Validation Loss:1.0739, Validation Accuracy:0.3760\n",
    "Epoch #289: Loss:1.0698, Accuracy:0.4086, Validation Loss:1.0743, Validation Accuracy:0.3810\n",
    "Epoch #290: Loss:1.0693, Accuracy:0.4066, Validation Loss:1.0748, Validation Accuracy:0.3760\n",
    "Epoch #291: Loss:1.0694, Accuracy:0.4103, Validation Loss:1.0752, Validation Accuracy:0.3760\n",
    "Epoch #292: Loss:1.0694, Accuracy:0.4099, Validation Loss:1.0757, Validation Accuracy:0.3777\n",
    "Epoch #293: Loss:1.0702, Accuracy:0.4127, Validation Loss:1.0754, Validation Accuracy:0.3760\n",
    "Epoch #294: Loss:1.0717, Accuracy:0.3832, Validation Loss:1.0760, Validation Accuracy:0.3530\n",
    "Epoch #295: Loss:1.0701, Accuracy:0.4057, Validation Loss:1.0767, Validation Accuracy:0.3990\n",
    "Epoch #296: Loss:1.0712, Accuracy:0.4099, Validation Loss:1.0757, Validation Accuracy:0.3727\n",
    "Epoch #297: Loss:1.0687, Accuracy:0.4111, Validation Loss:1.0749, Validation Accuracy:0.3793\n",
    "Epoch #298: Loss:1.0703, Accuracy:0.4090, Validation Loss:1.0747, Validation Accuracy:0.3810\n",
    "Epoch #299: Loss:1.0692, Accuracy:0.4070, Validation Loss:1.0746, Validation Accuracy:0.3612\n",
    "Epoch #300: Loss:1.0698, Accuracy:0.4049, Validation Loss:1.0741, Validation Accuracy:0.3777\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07414770, Accuracy:0.3777\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02  03\n",
    "t:01  193  47   0\n",
    "t:02  190  37   0\n",
    "t:03  122  20   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.38      0.80      0.52       240\n",
    "          02       0.36      0.16      0.22       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.25      0.32      0.25       609\n",
    "weighted avg       0.28      0.38      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 19:04:23 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 29 minutes, 1 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0953858900931472, 1.080739530436511, 1.0797159918423356, 1.0770310063667485, 1.0764981608085444, 1.0760835313248909, 1.0769685782822482, 1.076362088787536, 1.0761636235248084, 1.075618809275635, 1.0755697122739845, 1.075615167422052, 1.0748610153965565, 1.074323438658503, 1.0749252692036244, 1.0745944033311114, 1.0743472186606897, 1.0745045625909013, 1.0745733013294014, 1.0747974007000476, 1.0750802655525395, 1.0751137038561316, 1.0749134878415387, 1.0749163480815043, 1.0747060652436882, 1.0749077838042687, 1.0755044650561705, 1.0755313165082132, 1.0755258144807738, 1.075090682173793, 1.0748513799974289, 1.0752345662203133, 1.075435127335033, 1.0754706906567653, 1.0753741090129358, 1.0752404675695109, 1.0752400222474523, 1.0755674275271412, 1.0753459803185048, 1.0751236583211738, 1.0753101828846046, 1.0749461439442751, 1.0751031470807706, 1.07546116996477, 1.075410485854877, 1.0752948109543774, 1.074887898167953, 1.0748872482913665, 1.0749746792030648, 1.075226644968556, 1.0760641567812765, 1.0756052425146494, 1.0761645261094293, 1.0762004773996539, 1.0756409679140364, 1.0762571166888835, 1.0762769770739702, 1.0761803727236092, 1.076586200490178, 1.0773624692644392, 1.0764075008714924, 1.0767297453089497, 1.0765195360716144, 1.0770250715449918, 1.076473188909208, 1.0765240255052038, 1.0765525635044366, 1.0769022354742968, 1.0772494038533302, 1.0773269531174834, 1.0776204927801498, 1.0776641501973219, 1.0762257207986365, 1.0771671085326346, 1.0769680014189045, 1.0775776514278843, 1.0764834516741373, 1.0766814383379932, 1.0764532437661207, 1.0767684644470465, 1.0769029517087638, 1.0766115609452447, 1.0789535570222952, 1.077641935967068, 1.076922975737473, 1.0775363380685816, 1.079876209323238, 1.0767825994585536, 1.07616570195541, 1.0740816988577004, 1.0755330893793718, 1.0731742930138248, 1.0728819706952826, 1.0743825102870297, 1.0738487462887818, 1.0741688570952768, 1.077795328373588, 1.0763732635328922, 1.0777383727588872, 1.077183982617358, 1.0765548551023887, 1.077435835828922, 1.0769909590923141, 1.0781043499757113, 1.076454490667885, 1.0793991092782107, 1.0779174219798573, 1.0785157373190317, 1.071488243997195, 1.0757343739711593, 1.0737546887891045, 1.075539128142233, 1.0760702131612743, 1.075180943180579, 1.0760428942678792, 1.0761943301935306, 1.075998647851114, 1.0755462086650929, 1.0756349348278076, 1.075793854708742, 1.0760951394518021, 1.075597462199983, 1.0756581831839676, 1.076546540792744, 1.076107500221929, 1.076903156459038, 1.0773038860220823, 1.0773410783416923, 1.0776366833199813, 1.0776204886694847, 1.0774258429976715, 1.0773142609494464, 1.076841628218715, 1.0766198259269075, 1.0772189134838936, 1.0767362329172971, 1.0772186226053975, 1.077257382458654, 1.078828025333987, 1.0771406522916847, 1.0769524167128188, 1.0778992812230277, 1.0767035932572213, 1.0769405503969867, 1.075754536588008, 1.0759979588253352, 1.075630150600803, 1.0758125846609106, 1.075888391003037, 1.0759969930147695, 1.076537656862356, 1.0769283331086483, 1.0764915246290134, 1.076754961303498, 1.0749200764548015, 1.0744532963325237, 1.074077205704938, 1.0753316599355738, 1.0769753880884456, 1.0796332881955677, 1.0771185173385445, 1.0752858432447185, 1.0766321019390337, 1.0755651541335634, 1.0750192607369133, 1.075288103523317, 1.0764885802182853, 1.0756303040656354, 1.077044364071049, 1.0772854635868165, 1.0742264604333587, 1.0982668959644237, 1.0852540433896194, 1.08881018001262, 1.083491441847264, 1.0785568643281809, 1.0781594168376454, 1.078463389759972, 1.0782378418692227, 1.0772661951375124, 1.077326867380753, 1.0775240231030092, 1.0772613234120636, 1.077226528197478, 1.0771793089868205, 1.0772521961694477, 1.077403543226433, 1.07740860932762, 1.077495214387114, 1.077481190559312, 1.0773299705414545, 1.077403130006712, 1.0775222194997351, 1.0775689043239225, 1.0775124513848466, 1.0774483312722694, 1.0775412483559845, 1.0776007518000987, 1.0776301966903636, 1.0775346237254653, 1.0775384685676086, 1.0776661857595584, 1.0777099199091469, 1.0776369751772075, 1.0776476736726432, 1.077678236663831, 1.0776535967496423, 1.0776039275825513, 1.0776370910588156, 1.077754142249159, 1.0779073212925827, 1.0777813211841927, 1.077717111419966, 1.0776630019515214, 1.0777669287667486, 1.0776920868649662, 1.0776631561993377, 1.077795519030153, 1.0775261878575793, 1.0775015115346422, 1.077532907620635, 1.077488851468943, 1.0773479558759917, 1.0773965691893754, 1.0773566307496947, 1.0776898443992502, 1.0774496106678628, 1.0774613597318652, 1.0774742629140468, 1.077533980895733, 1.07735757913887, 1.0772026436669486, 1.0770705327611838, 1.0773140998505215, 1.0774078425906954, 1.0770929182691527, 1.0769110142891043, 1.0772610458442926, 1.0768456842707492, 1.0768082071407674, 1.0767650120755525, 1.0766895062034745, 1.0766802971390472, 1.077084482009775, 1.0770843005532702, 1.0767470095153708, 1.0763367903839387, 1.076459161362233, 1.0767502109405442, 1.0755115682855616, 1.0748685185349438, 1.074833791635698, 1.075487178338964, 1.0754659547790126, 1.0753270835907784, 1.076090737712403, 1.07543608825195, 1.0749422012291519, 1.0743460878362796, 1.0744770050831811, 1.0757238855111384, 1.075705842040051, 1.075567452191132, 1.0743613346848386, 1.0738523186525493, 1.0737868496545626, 1.0741288090378585, 1.074407579080616, 1.0747573589065, 1.0753259382811673, 1.0745228712977644, 1.0744567365677682, 1.074972469426924, 1.074568193338579, 1.0741647920389286, 1.074002084865163, 1.074335213365226, 1.074190656344096, 1.0745179255803425, 1.0742405390700291, 1.0746428809925448, 1.0747953252056353, 1.074862534972443, 1.0745317767602078, 1.0742602193688329, 1.0740136714600186, 1.0737859654700619, 1.073855113904856, 1.0742642538888114, 1.0748319888154079, 1.0751879140857015, 1.0756870275256278, 1.0753750020060047, 1.0759652263816746, 1.076664807565498, 1.0757062644598323, 1.074929894876402, 1.0746540038652217, 1.0745616992706148, 1.0741476730760096], 'val_acc': [0.36781609004549987, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3678160909263567, 0.3579638748626991, 0.36781608725612, 0.3694581267575325, 0.3957307048521214, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.38916256015719647, 0.3908045963798642, 0.39244663289615084, 0.3908045968692291, 0.3990147771995839, 0.3990147772974569, 0.3940886686294537, 0.39244663240678596, 0.3957307049499944, 0.39408866882519966, 0.3957307049499944, 0.39244663240678596, 0.3957307049499944, 0.3908045962819912, 0.39244663240678596, 0.39573070465637544, 0.39244663230891297, 0.3891625600593235, 0.3940886684337077, 0.3908045961841182, 0.3940886685315807, 0.38752052383665575, 0.38095237943534976, 0.3711001625887083, 0.37602627076734657, 0.38259441536439853, 0.39244663250465894, 0.39244663250465894, 0.3875205242281477, 0.38259441536439853, 0.37602627096309255, 0.3825944155601445, 0.385878487907607, 0.3825944157558905, 0.3678160904369918, 0.3875205240324017, 0.35303776502022016, 0.39244663240678596, 0.3842364518806852, 0.38423645168493925, 0.3612479455463209, 0.3612479456441939, 0.3678160904369918, 0.379310343310555, 0.3628899818668616, 0.3661740542143241, 0.3825944156580175, 0.36453201808952934, 0.37274219871350306, 0.37274219871350306, 0.37766830728363326, 0.37274219871350306, 0.3776683071857603, 0.37110016239296234, 0.37110016249083533, 0.36453201789378337, 0.38095237963109574, 0.399014776612346, 0.3743842346425518, 0.40229884905768143, 0.4039408851824762, 0.40065681312863266, 0.385878487907607, 0.40558292169876287, 0.3694581262681676, 0.37110016239296234, 0.3940886686294537, 0.3645320177959104, 0.3628899819647346, 0.3711001625887083, 0.36453201828527526, 0.3628899821604805, 0.3743842349361708, 0.39737274097691616, 0.3940886696081835, 0.40394088635695197, 0.3990147771995839, 0.40065681361799754, 0.399014776710219, 0.385878487809734, 0.3990147771995839, 0.3760262712567115, 0.36288998157324265, 0.3973727404875513, 0.39573070446062947, 0.39901477651447304, 0.3809523793374768, 0.40722495752993865, 0.38423645168493925, 0.37602627106096553, 0.3743842348382978, 0.3760262712567115, 0.37438423513191676, 0.38916256025506946, 0.3957307049499944, 0.3727421996922329, 0.39080459696710207, 0.38423645178281224, 0.39244663260253193, 0.379310343310555, 0.3743842348382978, 0.3760262711588385, 0.37274219890924903, 0.385878487907607, 0.3711001628823273, 0.3694581267575325, 0.37438423552340866, 0.36617405489943494, 0.36945812714902443, 0.36288998255197247, 0.379310343408428, 0.36617405470368897, 0.379310343408428, 0.36124794623143175, 0.359605910302383, 0.379310343506301, 0.3645320185788942, 0.36124794613355876, 0.379310343408428, 0.37602627106096553, 0.3563218379549205, 0.36124794632930474, 0.3513957294826633, 0.37602627106096553, 0.353037765705331, 0.35796387407971525, 0.35796387407971525, 0.35796387417758824, 0.3924466327004049, 0.3678160907306107, 0.379310343604174, 0.37766830738150625, 0.3711001629802002, 0.3743842352297897, 0.3793103437020469, 0.3743842350340438, 0.3711001628823273, 0.3711001627844543, 0.38423645168493925, 0.372742199007122, 0.37274219910499495, 0.372742199007122, 0.3711001627844543, 0.3678160904369918, 0.3661740541164511, 0.35467980183012576, 0.38752052383665575, 0.3875205241302747, 0.37602627135458444, 0.37602627145245743, 0.3793103437020469, 0.372742199007122, 0.37274219890924903, 0.37274219920286794, 0.37274219910499495, 0.3628899833349563, 0.38423645217430413, 0.3809523805119525, 0.37274219881137605, 0.3940886686294537, 0.3940886686294537, 0.38916256025506946, 0.35303776550958504, 0.35303776541171206, 0.37438423513191676, 0.37602627135458444, 0.3743842352297897, 0.38423645217430413, 0.3711001628823273, 0.3711001628823273, 0.37438423513191676, 0.37274219890924903, 0.3711001627844543, 0.3711001627844543, 0.3711001627844543, 0.3694581267575325, 0.37274219890924903, 0.37274219890924903, 0.36945812665965955, 0.36617405441007006, 0.36617405441007006, 0.36945812665965955, 0.3711001627844543, 0.36453201818740233, 0.36945812665965955, 0.36945812665965955, 0.3678160905348648, 0.372742199007122, 0.372742199007122, 0.36453201828527526, 0.3546798016343798, 0.3711001628823273, 0.36617405441007006, 0.37274219890924903, 0.37274219890924903, 0.372742199007122, 0.36945812665965955, 0.3678160905348648, 0.3678160905348648, 0.3711001628823273, 0.3546798015365068, 0.3694581267575325, 0.3711001628823273, 0.372742199007122, 0.37438423513191676, 0.37766830757725217, 0.3711001628823273, 0.3546798015365068, 0.3760262712567115, 0.3711001628823273, 0.3711001628823273, 0.3743842352297897, 0.3743842352297897, 0.36781609063273774, 0.35303776541171206, 0.36781609063273774, 0.36781609063273774, 0.3694581267575325, 0.3711001628823273, 0.37274219910499495, 0.36781609063273774, 0.3694581267575325, 0.3711001628823273, 0.3711001628823273, 0.3743842352297897, 0.3743842352297897, 0.3743842352297897, 0.3711001628823273, 0.37602627135458444, 0.3743842352297897, 0.37274219910499495, 0.37766830747937924, 0.37766830757725217, 0.3743842352297897, 0.37766830747937924, 0.38095237992471465, 0.3825944160495094, 0.3825944160495094, 0.37766830757725217, 0.3793103437020469, 0.379310343604174, 0.3628899823562265, 0.379310343604174, 0.3825944160495094, 0.3825944160495094, 0.3793103437020469, 0.3628899823562265, 0.379310343604174, 0.37766830757725217, 0.37602627145245743, 0.38095237992471465, 0.38095237992471465, 0.37766830767512516, 0.37766830757725217, 0.37274219920286794, 0.4006568135201246, 0.3793103437999199, 0.3825944161473824, 0.3842364522721771, 0.3825944161473824, 0.3793103437999199, 0.3842364522721771, 0.38095237982684166, 0.38095237982684166, 0.38095237982684166, 0.37766830747937924, 0.38095237982684166, 0.379310343604174, 0.37438423513191676, 0.38095237982684166, 0.3809523797289687, 0.37602627135458444, 0.37766830757725217, 0.3760262712567115, 0.38095237982684166, 0.37602627135458444, 0.37602627135458444, 0.37766830757725217, 0.37602627135458444, 0.35303776531383907, 0.3990147772974569, 0.372742199007122, 0.3793103437020469, 0.38095237982684166, 0.36124794613355876, 0.37766830738150625], 'loss': [1.1261581958441764, 1.0832703244269997, 1.0789515577057793, 1.0770989239338242, 1.075131458961988, 1.073958237313147, 1.073811084386994, 1.0740314430279898, 1.0734979674556662, 1.0735906568151234, 1.0734263451926762, 1.073398824100377, 1.074110469191471, 1.0745496404244426, 1.0741188814997429, 1.0739176873553704, 1.0743185910845685, 1.0741993859073709, 1.0738700285094964, 1.073695833824988, 1.0734077808058973, 1.0733920111058919, 1.0732885623614645, 1.073006309327159, 1.0730753153011785, 1.0736883454254276, 1.072479883307549, 1.072942954803639, 1.0729630629874354, 1.072478573767801, 1.0723517187811755, 1.0728077403818557, 1.0726255750754041, 1.0726458051121455, 1.0731791282826135, 1.073034961757229, 1.072792056698574, 1.0726532603680965, 1.0726275422000298, 1.0724172474667277, 1.072840665938673, 1.072805497386862, 1.072575577916061, 1.0728573094648013, 1.0727147460718174, 1.072568459383516, 1.0724788963427534, 1.0722720003225965, 1.0719044115998662, 1.0719950188356748, 1.0721334090712624, 1.0719992117715322, 1.071855682514042, 1.071831465354935, 1.0713766852933033, 1.071547252147839, 1.071130905454898, 1.0713828654749438, 1.071426314690764, 1.071310138408653, 1.0714035704395364, 1.0710270054776077, 1.070751927765488, 1.0705089621475345, 1.0708177708012856, 1.0704936414283894, 1.070190377157082, 1.0700061770680014, 1.0697310025197524, 1.0692487800880135, 1.0697731188924895, 1.070489757702336, 1.0708477751185517, 1.071052295079711, 1.070316579993011, 1.070318039878438, 1.07170442345206, 1.0707258936316082, 1.0704211806369759, 1.0698726366188003, 1.0704178268659776, 1.0700919969370724, 1.0702889707298984, 1.070013948338722, 1.069121983065987, 1.0698413826356923, 1.0696297873216978, 1.0724691170197003, 1.0736003032210426, 1.0743793033231699, 1.0737099577024487, 1.0735225019024137, 1.0718916089627777, 1.0717623217639491, 1.0705213227066417, 1.0708343601814285, 1.0708236496551327, 1.0709313114565746, 1.071135400550811, 1.0698016437416937, 1.0699623504948077, 1.069553696986831, 1.0703054995996997, 1.0698210761777185, 1.0708156984689543, 1.0723607504392307, 1.0720630858713107, 1.0732355920685879, 1.0780166942725682, 1.0737838987452293, 1.0738295455732874, 1.0732945752094902, 1.0723811487397619, 1.0736834469761936, 1.071897964506913, 1.0717195254331742, 1.0716554535977405, 1.071529785661482, 1.071410088274758, 1.070650564131061, 1.0705549453073457, 1.0707175384067167, 1.0706858214411648, 1.0709127784999244, 1.0715231083746564, 1.0711487898346825, 1.0712355769390443, 1.071012799059341, 1.0711452905157508, 1.0720856012015372, 1.0711641649935524, 1.0708966255677554, 1.070920157677339, 1.0707334706915477, 1.0709159130433257, 1.0704137872131942, 1.0704121336065524, 1.071379683688436, 1.0706107648735907, 1.071178473486303, 1.0703885792706782, 1.070182616607854, 1.0713465717538917, 1.071486784545303, 1.070930931308676, 1.0708579080060767, 1.0712141537813191, 1.0707780631415897, 1.0709285398773098, 1.0716344446616985, 1.0710456222968914, 1.0710294194290035, 1.0710317929422586, 1.070499398771987, 1.0707095953962886, 1.0705281933230297, 1.0707918525476476, 1.070311567866582, 1.0711026988235097, 1.0748310861646273, 1.0708080677036387, 1.0718785539055262, 1.070323019154997, 1.0703098487560265, 1.069755005346921, 1.0696695551490392, 1.0702584897713006, 1.0706304297065343, 1.0713013751306084, 1.0719601075996854, 1.073393485678295, 1.0722182725734044, 1.106780063053421, 1.078137685435019, 1.0810752477489214, 1.0755565988944051, 1.0724019484843073, 1.0731084547003682, 1.0734182474304763, 1.072893629426584, 1.0726297631645594, 1.0727998332076494, 1.0728810185524473, 1.0726873569175204, 1.0726405292565817, 1.0724736678282094, 1.0724721399420831, 1.0724576851670502, 1.0723913762114132, 1.07264538819785, 1.072474719170917, 1.0724433332987635, 1.0722538430587956, 1.0722281607759072, 1.0723580422097898, 1.0721961939359348, 1.0721444280730137, 1.0721562124620474, 1.072213108730512, 1.0721394331303464, 1.072134092505218, 1.0720105041468657, 1.0721464677513013, 1.0721383191232075, 1.072103806736533, 1.0719637545716836, 1.0719570137881644, 1.0720644483331292, 1.071881091423348, 1.0719277843557589, 1.0719775272345886, 1.0718683282942254, 1.0718360656585537, 1.071694367034724, 1.0717363062825291, 1.0719297732660658, 1.0716942976142836, 1.0717093702214455, 1.0718229067154244, 1.0717925068289347, 1.0716040443345995, 1.0716426690256327, 1.071760560305946, 1.0717193621139995, 1.0716286861920994, 1.0721850057402187, 1.0715775360072173, 1.0713449140348963, 1.071364318614623, 1.0714347512815032, 1.0713224558859635, 1.0713399952686788, 1.071093490579045, 1.0714020319298307, 1.0711995837135238, 1.0715290954225607, 1.0710209269787987, 1.0712822055424998, 1.0710992091000693, 1.0708316377545775, 1.0707732647106634, 1.070634109724229, 1.0706721090438185, 1.0705400838499441, 1.0706926053064805, 1.0706749974335, 1.0707163948787557, 1.0708636017061113, 1.070307398821539, 1.0710355775312232, 1.0710977935693102, 1.0713941023335074, 1.0710783156526162, 1.0713390078143172, 1.0721073446822118, 1.0721871876373918, 1.0712804571559051, 1.0714272380609533, 1.071359666661805, 1.0709687849824188, 1.0711831922648625, 1.0711118499356374, 1.0712468034188116, 1.071222797164682, 1.070731014490617, 1.0712635016784042, 1.0717359581522383, 1.0708223404091242, 1.0713852835876496, 1.0712169666799431, 1.0718239329923593, 1.0717363312503885, 1.0701367707223128, 1.0702065301382078, 1.0700955324349217, 1.0702200079845452, 1.0700434530050602, 1.0703159928077055, 1.0704432679397615, 1.0697369337571474, 1.0697336658070464, 1.0698260792961356, 1.0695842767398216, 1.0696787934528484, 1.0699306815067111, 1.0701770752607185, 1.070451573816413, 1.0702156509951644, 1.069814601277424, 1.0692647544755094, 1.0693993954687881, 1.0693628325844202, 1.070247612498869, 1.0717429586014953, 1.0701455443301975, 1.0712133470257206, 1.068721688160906, 1.070331104335354, 1.0691926309460242, 1.0698084786687299], 'acc': [0.25420944540162843, 0.3901437377293252, 0.39425051211086876, 0.3942505121475862, 0.3942505119150424, 0.3958932237581061, 0.395071866563703, 0.38234086239851967, 0.39794661266847803, 0.39835728911158974, 0.39466119309470393, 0.39425051250252147, 0.3942505121475862, 0.3942505141058497, 0.3934291569114466, 0.39014373596688806, 0.391375770779361, 0.39178644581497085, 0.39507186953781565, 0.39671457742763494, 0.39383983394203737, 0.39383983370949355, 0.3926078034377441, 0.3971252574323384, 0.39958932353241, 0.39630390121706704, 0.39712525821564376, 0.4004106767735687, 0.4053388109319753, 0.40205338904500254, 0.40041067638191596, 0.3991786437235329, 0.3971252588031228, 0.3979466122768253, 0.3963039025878515, 0.39712525762816475, 0.4028747422861612, 0.4016427126018908, 0.40451745217096147, 0.4032854220950383, 0.3999999991922163, 0.39917864552268745, 0.40287474506444754, 0.40410677831030967, 0.4016427118185854, 0.4049281319798385, 0.4032854197084047, 0.40000000232543786, 0.4069815172919013, 0.4069815196785349, 0.40862422815583327, 0.3995893221616255, 0.40246406408061236, 0.406160164834048, 0.4110882968383648, 0.40821355429518147, 0.40944558378362556, 0.40739219905904184, 0.40739219647658187, 0.4086242283516596, 0.40780287334806375, 0.4127310051198368, 0.4160164256727426, 0.41149897445643463, 0.4102669427771833, 0.4106776164420087, 0.4147843952051668, 0.4086242315215986, 0.4123203276975933, 0.4082135501461107, 0.41026693999889696, 0.40985626414326426, 0.4057494844376918, 0.4078028743271955, 0.41601642900179053, 0.41396303679908814, 0.4049281303765103, 0.40862423187653385, 0.4156057492296309, 0.4094455865619119, 0.40328542307417004, 0.40903490812381926, 0.41149897347730285, 0.403285422486691, 0.41149897148232195, 0.4106776203952531, 0.40698152046184033, 0.3958932239906499, 0.39589322301151814, 0.37741273215663995, 0.3921971260155006, 0.39671458138087934, 0.40205338904500254, 0.40903490734051384, 0.40123203220553466, 0.40492813217566487, 0.4110882948433839, 0.4061601634265461, 0.40369609948056434, 0.40451745279515794, 0.40862422972244405, 0.409856263788329, 0.4061601644056778, 0.40657084401872856, 0.40082135380415945, 0.3950718701252947, 0.40082135560331406, 0.4028747452602739, 0.3975359320395781, 0.3897330614820398, 0.3917864471857553, 0.39671457977755115, 0.3991786457185138, 0.3950718693419893, 0.4131416859078456, 0.4073921966724082, 0.40492813018068397, 0.40082135619079307, 0.40369609693482184, 0.40698151846685937, 0.4073921982757365, 0.40821355409935517, 0.40533880721127474, 0.4053388082271239, 0.40205338607088986, 0.40205338865334983, 0.4028747456519266, 0.41232032730594065, 0.40369610030058717, 0.40985626218500076, 0.4016427104110835, 0.4061601658131797, 0.40164271103528, 0.40698152144097205, 0.4028747433020104, 0.4053388085820592, 0.4057494889784153, 0.40739219749243105, 0.40287474545610025, 0.4086242314848812, 0.4082135544542904, 0.40328542029588377, 0.4102669423855306, 0.40451745295426683, 0.4032854220950383, 0.4045174559283795, 0.40328542189921196, 0.40492813276314393, 0.4045174559283795, 0.4020533888491762, 0.40205338802915336, 0.3975359358337136, 0.4016427124060645, 0.40041067935602864, 0.40369609791395356, 0.4049281323714912, 0.4041067755320234, 0.40369610088806623, 0.40574948815839246, 0.40739219690495204, 0.4086242301140967, 0.4057494883909363, 0.39917864689347193, 0.4106776201627093, 0.41314168512454025, 0.4139630413398116, 0.4078028740946517, 0.4094455853869538, 0.40862422933079134, 0.4156057492296309, 0.3950718687545103, 0.4041067741612389, 0.391786448164887, 0.3917864485932571, 0.3946611887498068, 0.39712525566990126, 0.3979466134517834, 0.40369610088806623, 0.40698151787938036, 0.4028747416986822, 0.4049281317840122, 0.39876796551798405, 0.4036960973264745, 0.40369609990893446, 0.4045174531500932, 0.40287474189450856, 0.4041067767436989, 0.4057494860043026, 0.4057494854168236, 0.4049281337789931, 0.40492813076816303, 0.40574948858676263, 0.4045174555367268, 0.4053388105770401, 0.4073921982757365, 0.4053388097937347, 0.40739219588910286, 0.4041067751403707, 0.40944558515441004, 0.40698151748772765, 0.40574948564936736, 0.4057494844376918, 0.4086242303466405, 0.40944558358779926, 0.40246406447226507, 0.40246406505974414, 0.40739219647658187, 0.40616016205576166, 0.40492813276314393, 0.4082135501461107, 0.40616016323071974, 0.40410677533619704, 0.4036960987339764, 0.40657084401872856, 0.4065708408487896, 0.3983572878999142, 0.41149897304893274, 0.4094455833919729, 0.40739219588910286, 0.4065708428437705, 0.40739219549745015, 0.40985626515911344, 0.4106776172620315, 0.4102669413696814, 0.41149897504391364, 0.40739219929158565, 0.4094455855460627, 0.4057494878034572, 0.41026694258135693, 0.4036960991256291, 0.41108829406007846, 0.4110882958225156, 0.41108829363170835, 0.409445583820343, 0.40821355409935517, 0.4106776178127931, 0.4078028744863044, 0.4106776168703788, 0.4090349055780767, 0.40739219905904184, 0.4078028755021536, 0.40739219549745015, 0.40985626457163443, 0.407802873702999, 0.40821355034193707, 0.4065708408487896, 0.40903490675303483, 0.4049281292015522, 0.4086242295266177, 0.406981520266014, 0.4049281292015522, 0.4049281295932049, 0.4053388074071011, 0.41149897109066924, 0.4045174523667878, 0.3934291590655364, 0.4024640672505514, 0.4057494850251709, 0.4024640662714196, 0.40410677788193955, 0.4036960971306482, 0.4024640660755933, 0.40410677576456716, 0.4078028733113463, 0.40410677615621987, 0.4032854220950383, 0.40123203517964734, 0.40616016600900606, 0.4004106759902633, 0.4069815188585121, 0.41108829347259945, 0.404928133350623, 0.4045174515834824, 0.4045174555367268, 0.403696098697259, 0.40780287491467454, 0.4057494844376918, 0.40492813315479664, 0.40410677733117795, 0.4082135505377634, 0.40739219729660475, 0.40657084006548416, 0.4073921956932765, 0.4057494844376918, 0.40574948721597814, 0.40698151787938036, 0.40492813080488044, 0.40780287291969364, 0.40862422972244405, 0.4065708426112267, 0.4102669384322862, 0.4098562628091973, 0.4127310078981231, 0.38316221783048565, 0.4057494858084763, 0.40985626120586904, 0.4110882952350366, 0.40903490913966845, 0.40698152183262476, 0.4049281303765103]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
