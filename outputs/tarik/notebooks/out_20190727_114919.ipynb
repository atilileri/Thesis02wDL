{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf39.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 11:49:20 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'Split', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000026F03280A90>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000026F309E7EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0847, Accuracy:0.3852, Validation Loss:1.0774, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0755, Accuracy:0.3919, Validation Loss:1.0758, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0755, Accuracy:0.3945, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0746, Accuracy:0.3943, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0738, Accuracy:0.3958, Validation Loss:1.0739, Validation Accuracy:0.4068\n",
    "Epoch #6: Loss:1.0729, Accuracy:0.4125, Validation Loss:1.0733, Validation Accuracy:0.3933\n",
    "Epoch #7: Loss:1.0717, Accuracy:0.4007, Validation Loss:1.0724, Validation Accuracy:0.4134\n",
    "Epoch #8: Loss:1.0695, Accuracy:0.4256, Validation Loss:1.0706, Validation Accuracy:0.4130\n",
    "Epoch #9: Loss:1.0640, Accuracy:0.4286, Validation Loss:1.0676, Validation Accuracy:0.4245\n",
    "Epoch #10: Loss:1.0566, Accuracy:0.4421, Validation Loss:1.0701, Validation Accuracy:0.4245\n",
    "Epoch #11: Loss:1.0543, Accuracy:0.4380, Validation Loss:1.0691, Validation Accuracy:0.4220\n",
    "Epoch #12: Loss:1.0542, Accuracy:0.4413, Validation Loss:1.0649, Validation Accuracy:0.4298\n",
    "Epoch #13: Loss:1.0523, Accuracy:0.4430, Validation Loss:1.0669, Validation Accuracy:0.4249\n",
    "Epoch #14: Loss:1.0508, Accuracy:0.4424, Validation Loss:1.0631, Validation Accuracy:0.4282\n",
    "Epoch #15: Loss:1.0499, Accuracy:0.4465, Validation Loss:1.0623, Validation Accuracy:0.4298\n",
    "Epoch #16: Loss:1.0495, Accuracy:0.4433, Validation Loss:1.0637, Validation Accuracy:0.4314\n",
    "Epoch #17: Loss:1.0486, Accuracy:0.4425, Validation Loss:1.0694, Validation Accuracy:0.4290\n",
    "Epoch #18: Loss:1.0568, Accuracy:0.4378, Validation Loss:1.0620, Validation Accuracy:0.4278\n",
    "Epoch #19: Loss:1.0501, Accuracy:0.4420, Validation Loss:1.0638, Validation Accuracy:0.4253\n",
    "Epoch #20: Loss:1.0540, Accuracy:0.4398, Validation Loss:1.0660, Validation Accuracy:0.4265\n",
    "Epoch #21: Loss:1.0503, Accuracy:0.4434, Validation Loss:1.0625, Validation Accuracy:0.4364\n",
    "Epoch #22: Loss:1.0489, Accuracy:0.4437, Validation Loss:1.0621, Validation Accuracy:0.4278\n",
    "Epoch #23: Loss:1.0486, Accuracy:0.4424, Validation Loss:1.0618, Validation Accuracy:0.4343\n",
    "Epoch #24: Loss:1.0486, Accuracy:0.4440, Validation Loss:1.0644, Validation Accuracy:0.4364\n",
    "Epoch #25: Loss:1.0482, Accuracy:0.4452, Validation Loss:1.0609, Validation Accuracy:0.4339\n",
    "Epoch #26: Loss:1.0475, Accuracy:0.4414, Validation Loss:1.0615, Validation Accuracy:0.4306\n",
    "Epoch #27: Loss:1.0480, Accuracy:0.4449, Validation Loss:1.0612, Validation Accuracy:0.4364\n",
    "Epoch #28: Loss:1.0481, Accuracy:0.4382, Validation Loss:1.0618, Validation Accuracy:0.4343\n",
    "Epoch #29: Loss:1.0473, Accuracy:0.4447, Validation Loss:1.0607, Validation Accuracy:0.4356\n",
    "Epoch #30: Loss:1.0463, Accuracy:0.4453, Validation Loss:1.0607, Validation Accuracy:0.4286\n",
    "Epoch #31: Loss:1.0462, Accuracy:0.4435, Validation Loss:1.0590, Validation Accuracy:0.4331\n",
    "Epoch #32: Loss:1.0461, Accuracy:0.4426, Validation Loss:1.0600, Validation Accuracy:0.4347\n",
    "Epoch #33: Loss:1.0457, Accuracy:0.4434, Validation Loss:1.0584, Validation Accuracy:0.4339\n",
    "Epoch #34: Loss:1.0516, Accuracy:0.4412, Validation Loss:1.0652, Validation Accuracy:0.4290\n",
    "Epoch #35: Loss:1.0480, Accuracy:0.4407, Validation Loss:1.0588, Validation Accuracy:0.4372\n",
    "Epoch #36: Loss:1.0463, Accuracy:0.4439, Validation Loss:1.0580, Validation Accuracy:0.4376\n",
    "Epoch #37: Loss:1.0464, Accuracy:0.4444, Validation Loss:1.0589, Validation Accuracy:0.4372\n",
    "Epoch #38: Loss:1.0460, Accuracy:0.4467, Validation Loss:1.0591, Validation Accuracy:0.4360\n",
    "Epoch #39: Loss:1.0458, Accuracy:0.4458, Validation Loss:1.0602, Validation Accuracy:0.4376\n",
    "Epoch #40: Loss:1.0444, Accuracy:0.4429, Validation Loss:1.0591, Validation Accuracy:0.4368\n",
    "Epoch #41: Loss:1.0440, Accuracy:0.4430, Validation Loss:1.0598, Validation Accuracy:0.4413\n",
    "Epoch #42: Loss:1.0444, Accuracy:0.4438, Validation Loss:1.0582, Validation Accuracy:0.4356\n",
    "Epoch #43: Loss:1.0438, Accuracy:0.4426, Validation Loss:1.0631, Validation Accuracy:0.4491\n",
    "Epoch #44: Loss:1.0494, Accuracy:0.4440, Validation Loss:1.0562, Validation Accuracy:0.4409\n",
    "Epoch #45: Loss:1.0452, Accuracy:0.4465, Validation Loss:1.0565, Validation Accuracy:0.4356\n",
    "Epoch #46: Loss:1.0433, Accuracy:0.4468, Validation Loss:1.0571, Validation Accuracy:0.4356\n",
    "Epoch #47: Loss:1.0447, Accuracy:0.4437, Validation Loss:1.0590, Validation Accuracy:0.4405\n",
    "Epoch #48: Loss:1.0430, Accuracy:0.4474, Validation Loss:1.0559, Validation Accuracy:0.4356\n",
    "Epoch #49: Loss:1.0440, Accuracy:0.4460, Validation Loss:1.0614, Validation Accuracy:0.4335\n",
    "Epoch #50: Loss:1.0448, Accuracy:0.4458, Validation Loss:1.0560, Validation Accuracy:0.4409\n",
    "Epoch #51: Loss:1.0429, Accuracy:0.4539, Validation Loss:1.0572, Validation Accuracy:0.4376\n",
    "Epoch #52: Loss:1.0456, Accuracy:0.4460, Validation Loss:1.0567, Validation Accuracy:0.4380\n",
    "Epoch #53: Loss:1.0420, Accuracy:0.4479, Validation Loss:1.0611, Validation Accuracy:0.4425\n",
    "Epoch #54: Loss:1.0480, Accuracy:0.4414, Validation Loss:1.0550, Validation Accuracy:0.4364\n",
    "Epoch #55: Loss:1.0443, Accuracy:0.4511, Validation Loss:1.0546, Validation Accuracy:0.4401\n",
    "Epoch #56: Loss:1.0422, Accuracy:0.4470, Validation Loss:1.0554, Validation Accuracy:0.4397\n",
    "Epoch #57: Loss:1.0414, Accuracy:0.4487, Validation Loss:1.0570, Validation Accuracy:0.4413\n",
    "Epoch #58: Loss:1.0411, Accuracy:0.4492, Validation Loss:1.0558, Validation Accuracy:0.4458\n",
    "Epoch #59: Loss:1.0416, Accuracy:0.4457, Validation Loss:1.0550, Validation Accuracy:0.4433\n",
    "Epoch #60: Loss:1.0421, Accuracy:0.4490, Validation Loss:1.0564, Validation Accuracy:0.4553\n",
    "Epoch #61: Loss:1.0404, Accuracy:0.4540, Validation Loss:1.0541, Validation Accuracy:0.4520\n",
    "Epoch #62: Loss:1.0411, Accuracy:0.4498, Validation Loss:1.0624, Validation Accuracy:0.4401\n",
    "Epoch #63: Loss:1.0429, Accuracy:0.4496, Validation Loss:1.0577, Validation Accuracy:0.4495\n",
    "Epoch #64: Loss:1.0410, Accuracy:0.4569, Validation Loss:1.0547, Validation Accuracy:0.4479\n",
    "Epoch #65: Loss:1.0408, Accuracy:0.4493, Validation Loss:1.0527, Validation Accuracy:0.4446\n",
    "Epoch #66: Loss:1.0402, Accuracy:0.4542, Validation Loss:1.0546, Validation Accuracy:0.4581\n",
    "Epoch #67: Loss:1.0404, Accuracy:0.4485, Validation Loss:1.0531, Validation Accuracy:0.4462\n",
    "Epoch #68: Loss:1.0396, Accuracy:0.4545, Validation Loss:1.0534, Validation Accuracy:0.4614\n",
    "Epoch #69: Loss:1.0391, Accuracy:0.4588, Validation Loss:1.0531, Validation Accuracy:0.4569\n",
    "Epoch #70: Loss:1.0394, Accuracy:0.4534, Validation Loss:1.0526, Validation Accuracy:0.4585\n",
    "Epoch #71: Loss:1.0390, Accuracy:0.4535, Validation Loss:1.0553, Validation Accuracy:0.4528\n",
    "Epoch #72: Loss:1.0389, Accuracy:0.4578, Validation Loss:1.0522, Validation Accuracy:0.4462\n",
    "Epoch #73: Loss:1.0388, Accuracy:0.4529, Validation Loss:1.0550, Validation Accuracy:0.4557\n",
    "Epoch #74: Loss:1.0377, Accuracy:0.4548, Validation Loss:1.0533, Validation Accuracy:0.4491\n",
    "Epoch #75: Loss:1.0377, Accuracy:0.4534, Validation Loss:1.0518, Validation Accuracy:0.4425\n",
    "Epoch #76: Loss:1.0390, Accuracy:0.4548, Validation Loss:1.0544, Validation Accuracy:0.4388\n",
    "Epoch #77: Loss:1.0422, Accuracy:0.4541, Validation Loss:1.0510, Validation Accuracy:0.4413\n",
    "Epoch #78: Loss:1.0379, Accuracy:0.4598, Validation Loss:1.0508, Validation Accuracy:0.4462\n",
    "Epoch #79: Loss:1.0381, Accuracy:0.4502, Validation Loss:1.0509, Validation Accuracy:0.4499\n",
    "Epoch #80: Loss:1.0372, Accuracy:0.4571, Validation Loss:1.0546, Validation Accuracy:0.4659\n",
    "Epoch #81: Loss:1.0374, Accuracy:0.4573, Validation Loss:1.0498, Validation Accuracy:0.4450\n",
    "Epoch #82: Loss:1.0422, Accuracy:0.4500, Validation Loss:1.0501, Validation Accuracy:0.4450\n",
    "Epoch #83: Loss:1.0370, Accuracy:0.4592, Validation Loss:1.0512, Validation Accuracy:0.4622\n",
    "Epoch #84: Loss:1.0355, Accuracy:0.4589, Validation Loss:1.0504, Validation Accuracy:0.4495\n",
    "Epoch #85: Loss:1.0353, Accuracy:0.4567, Validation Loss:1.0515, Validation Accuracy:0.4503\n",
    "Epoch #86: Loss:1.0338, Accuracy:0.4591, Validation Loss:1.0598, Validation Accuracy:0.4347\n",
    "Epoch #87: Loss:1.0421, Accuracy:0.4424, Validation Loss:1.0533, Validation Accuracy:0.4491\n",
    "Epoch #88: Loss:1.0378, Accuracy:0.4554, Validation Loss:1.0490, Validation Accuracy:0.4479\n",
    "Epoch #89: Loss:1.0390, Accuracy:0.4592, Validation Loss:1.0502, Validation Accuracy:0.4438\n",
    "Epoch #90: Loss:1.0363, Accuracy:0.4557, Validation Loss:1.0567, Validation Accuracy:0.4339\n",
    "Epoch #91: Loss:1.0431, Accuracy:0.4400, Validation Loss:1.0468, Validation Accuracy:0.4487\n",
    "Epoch #92: Loss:1.0358, Accuracy:0.4560, Validation Loss:1.0483, Validation Accuracy:0.4532\n",
    "Epoch #93: Loss:1.0343, Accuracy:0.4552, Validation Loss:1.0484, Validation Accuracy:0.4520\n",
    "Epoch #94: Loss:1.0326, Accuracy:0.4594, Validation Loss:1.0476, Validation Accuracy:0.4610\n",
    "Epoch #95: Loss:1.0313, Accuracy:0.4601, Validation Loss:1.0480, Validation Accuracy:0.4606\n",
    "Epoch #96: Loss:1.0319, Accuracy:0.4588, Validation Loss:1.0459, Validation Accuracy:0.4413\n",
    "Epoch #97: Loss:1.0365, Accuracy:0.4562, Validation Loss:1.0493, Validation Accuracy:0.4540\n",
    "Epoch #98: Loss:1.0347, Accuracy:0.4598, Validation Loss:1.0458, Validation Accuracy:0.4495\n",
    "Epoch #99: Loss:1.0325, Accuracy:0.4585, Validation Loss:1.0454, Validation Accuracy:0.4479\n",
    "Epoch #100: Loss:1.0367, Accuracy:0.4565, Validation Loss:1.0434, Validation Accuracy:0.4507\n",
    "Epoch #101: Loss:1.0355, Accuracy:0.4588, Validation Loss:1.0458, Validation Accuracy:0.4450\n",
    "Epoch #102: Loss:1.0328, Accuracy:0.4607, Validation Loss:1.0446, Validation Accuracy:0.4491\n",
    "Epoch #103: Loss:1.0304, Accuracy:0.4590, Validation Loss:1.0474, Validation Accuracy:0.4536\n",
    "Epoch #104: Loss:1.0306, Accuracy:0.4615, Validation Loss:1.0414, Validation Accuracy:0.4458\n",
    "Epoch #105: Loss:1.0334, Accuracy:0.4574, Validation Loss:1.0424, Validation Accuracy:0.4536\n",
    "Epoch #106: Loss:1.0298, Accuracy:0.4617, Validation Loss:1.0443, Validation Accuracy:0.4520\n",
    "Epoch #107: Loss:1.0300, Accuracy:0.4668, Validation Loss:1.0432, Validation Accuracy:0.4540\n",
    "Epoch #108: Loss:1.0308, Accuracy:0.4631, Validation Loss:1.0448, Validation Accuracy:0.4553\n",
    "Epoch #109: Loss:1.0310, Accuracy:0.4616, Validation Loss:1.0408, Validation Accuracy:0.4532\n",
    "Epoch #110: Loss:1.0292, Accuracy:0.4666, Validation Loss:1.0426, Validation Accuracy:0.4524\n",
    "Epoch #111: Loss:1.0270, Accuracy:0.4595, Validation Loss:1.0371, Validation Accuracy:0.4618\n",
    "Epoch #112: Loss:1.0293, Accuracy:0.4609, Validation Loss:1.0413, Validation Accuracy:0.4495\n",
    "Epoch #113: Loss:1.0278, Accuracy:0.4580, Validation Loss:1.0398, Validation Accuracy:0.4581\n",
    "Epoch #114: Loss:1.0312, Accuracy:0.4564, Validation Loss:1.0415, Validation Accuracy:0.4544\n",
    "Epoch #115: Loss:1.0341, Accuracy:0.4560, Validation Loss:1.0506, Validation Accuracy:0.4470\n",
    "Epoch #116: Loss:1.0322, Accuracy:0.4587, Validation Loss:1.0421, Validation Accuracy:0.4516\n",
    "Epoch #117: Loss:1.0281, Accuracy:0.4649, Validation Loss:1.0408, Validation Accuracy:0.4561\n",
    "Epoch #118: Loss:1.0270, Accuracy:0.4638, Validation Loss:1.0420, Validation Accuracy:0.4511\n",
    "Epoch #119: Loss:1.0278, Accuracy:0.4637, Validation Loss:1.0445, Validation Accuracy:0.4392\n",
    "Epoch #120: Loss:1.0365, Accuracy:0.4590, Validation Loss:1.0374, Validation Accuracy:0.4548\n",
    "Epoch #121: Loss:1.0296, Accuracy:0.4638, Validation Loss:1.0445, Validation Accuracy:0.4475\n",
    "Epoch #122: Loss:1.0297, Accuracy:0.4670, Validation Loss:1.0438, Validation Accuracy:0.4503\n",
    "Epoch #123: Loss:1.0282, Accuracy:0.4640, Validation Loss:1.0370, Validation Accuracy:0.4548\n",
    "Epoch #124: Loss:1.0243, Accuracy:0.4671, Validation Loss:1.0378, Validation Accuracy:0.4573\n",
    "Epoch #125: Loss:1.0241, Accuracy:0.4639, Validation Loss:1.0351, Validation Accuracy:0.4659\n",
    "Epoch #126: Loss:1.0252, Accuracy:0.4624, Validation Loss:1.0342, Validation Accuracy:0.4581\n",
    "Epoch #127: Loss:1.0259, Accuracy:0.4679, Validation Loss:1.0374, Validation Accuracy:0.4614\n",
    "Epoch #128: Loss:1.0242, Accuracy:0.4651, Validation Loss:1.0432, Validation Accuracy:0.4421\n",
    "Epoch #129: Loss:1.0296, Accuracy:0.4625, Validation Loss:1.0398, Validation Accuracy:0.4606\n",
    "Epoch #130: Loss:1.0257, Accuracy:0.4691, Validation Loss:1.0400, Validation Accuracy:0.4536\n",
    "Epoch #131: Loss:1.0243, Accuracy:0.4679, Validation Loss:1.0350, Validation Accuracy:0.4626\n",
    "Epoch #132: Loss:1.0225, Accuracy:0.4669, Validation Loss:1.0378, Validation Accuracy:0.4614\n",
    "Epoch #133: Loss:1.0215, Accuracy:0.4689, Validation Loss:1.0344, Validation Accuracy:0.4618\n",
    "Epoch #134: Loss:1.0221, Accuracy:0.4656, Validation Loss:1.0401, Validation Accuracy:0.4548\n",
    "Epoch #135: Loss:1.0241, Accuracy:0.4696, Validation Loss:1.0472, Validation Accuracy:0.4491\n",
    "Epoch #136: Loss:1.0279, Accuracy:0.4656, Validation Loss:1.0334, Validation Accuracy:0.4618\n",
    "Epoch #137: Loss:1.0211, Accuracy:0.4702, Validation Loss:1.0371, Validation Accuracy:0.4553\n",
    "Epoch #138: Loss:1.0256, Accuracy:0.4667, Validation Loss:1.0346, Validation Accuracy:0.4635\n",
    "Epoch #139: Loss:1.0205, Accuracy:0.4707, Validation Loss:1.0348, Validation Accuracy:0.4659\n",
    "Epoch #140: Loss:1.0211, Accuracy:0.4706, Validation Loss:1.0331, Validation Accuracy:0.4754\n",
    "Epoch #141: Loss:1.0227, Accuracy:0.4697, Validation Loss:1.0391, Validation Accuracy:0.4635\n",
    "Epoch #142: Loss:1.0220, Accuracy:0.4686, Validation Loss:1.0379, Validation Accuracy:0.4589\n",
    "Epoch #143: Loss:1.0238, Accuracy:0.4679, Validation Loss:1.0337, Validation Accuracy:0.4635\n",
    "Epoch #144: Loss:1.0215, Accuracy:0.4713, Validation Loss:1.0337, Validation Accuracy:0.4594\n",
    "Epoch #145: Loss:1.0268, Accuracy:0.4668, Validation Loss:1.0340, Validation Accuracy:0.4635\n",
    "Epoch #146: Loss:1.0184, Accuracy:0.4744, Validation Loss:1.0338, Validation Accuracy:0.4602\n",
    "Epoch #147: Loss:1.0239, Accuracy:0.4689, Validation Loss:1.0314, Validation Accuracy:0.4733\n",
    "Epoch #148: Loss:1.0207, Accuracy:0.4739, Validation Loss:1.0304, Validation Accuracy:0.4733\n",
    "Epoch #149: Loss:1.0188, Accuracy:0.4722, Validation Loss:1.0412, Validation Accuracy:0.4589\n",
    "Epoch #150: Loss:1.0270, Accuracy:0.4659, Validation Loss:1.0343, Validation Accuracy:0.4614\n",
    "Epoch #151: Loss:1.0190, Accuracy:0.4760, Validation Loss:1.0350, Validation Accuracy:0.4696\n",
    "Epoch #152: Loss:1.0182, Accuracy:0.4759, Validation Loss:1.0337, Validation Accuracy:0.4622\n",
    "Epoch #153: Loss:1.0204, Accuracy:0.4718, Validation Loss:1.0350, Validation Accuracy:0.4631\n",
    "Epoch #154: Loss:1.0235, Accuracy:0.4668, Validation Loss:1.0385, Validation Accuracy:0.4598\n",
    "Epoch #155: Loss:1.0195, Accuracy:0.4715, Validation Loss:1.0352, Validation Accuracy:0.4713\n",
    "Epoch #156: Loss:1.0186, Accuracy:0.4730, Validation Loss:1.0387, Validation Accuracy:0.4548\n",
    "Epoch #157: Loss:1.0189, Accuracy:0.4732, Validation Loss:1.0372, Validation Accuracy:0.4758\n",
    "Epoch #158: Loss:1.0190, Accuracy:0.4754, Validation Loss:1.0327, Validation Accuracy:0.4647\n",
    "Epoch #159: Loss:1.0181, Accuracy:0.4726, Validation Loss:1.0310, Validation Accuracy:0.4729\n",
    "Epoch #160: Loss:1.0171, Accuracy:0.4743, Validation Loss:1.0326, Validation Accuracy:0.4622\n",
    "Epoch #161: Loss:1.0192, Accuracy:0.4783, Validation Loss:1.0306, Validation Accuracy:0.4713\n",
    "Epoch #162: Loss:1.0171, Accuracy:0.4763, Validation Loss:1.0337, Validation Accuracy:0.4741\n",
    "Epoch #163: Loss:1.0170, Accuracy:0.4829, Validation Loss:1.0313, Validation Accuracy:0.4803\n",
    "Epoch #164: Loss:1.0146, Accuracy:0.4790, Validation Loss:1.0298, Validation Accuracy:0.4877\n",
    "Epoch #165: Loss:1.0140, Accuracy:0.4793, Validation Loss:1.0397, Validation Accuracy:0.4532\n",
    "Epoch #166: Loss:1.0250, Accuracy:0.4703, Validation Loss:1.0390, Validation Accuracy:0.4544\n",
    "Epoch #167: Loss:1.0204, Accuracy:0.4758, Validation Loss:1.0308, Validation Accuracy:0.4778\n",
    "Epoch #168: Loss:1.0156, Accuracy:0.4794, Validation Loss:1.0286, Validation Accuracy:0.4836\n",
    "Epoch #169: Loss:1.0140, Accuracy:0.4830, Validation Loss:1.0296, Validation Accuracy:0.4766\n",
    "Epoch #170: Loss:1.0159, Accuracy:0.4809, Validation Loss:1.0320, Validation Accuracy:0.4750\n",
    "Epoch #171: Loss:1.0165, Accuracy:0.4778, Validation Loss:1.0288, Validation Accuracy:0.4791\n",
    "Epoch #172: Loss:1.0173, Accuracy:0.4795, Validation Loss:1.0476, Validation Accuracy:0.4544\n",
    "Epoch #173: Loss:1.0404, Accuracy:0.4573, Validation Loss:1.0422, Validation Accuracy:0.4647\n",
    "Epoch #174: Loss:1.0219, Accuracy:0.4771, Validation Loss:1.0310, Validation Accuracy:0.4766\n",
    "Epoch #175: Loss:1.0177, Accuracy:0.4785, Validation Loss:1.0310, Validation Accuracy:0.4684\n",
    "Epoch #176: Loss:1.0143, Accuracy:0.4771, Validation Loss:1.0317, Validation Accuracy:0.4778\n",
    "Epoch #177: Loss:1.0161, Accuracy:0.4802, Validation Loss:1.0304, Validation Accuracy:0.4754\n",
    "Epoch #178: Loss:1.0171, Accuracy:0.4803, Validation Loss:1.0289, Validation Accuracy:0.4791\n",
    "Epoch #179: Loss:1.0134, Accuracy:0.4796, Validation Loss:1.0309, Validation Accuracy:0.4856\n",
    "Epoch #180: Loss:1.0138, Accuracy:0.4800, Validation Loss:1.0349, Validation Accuracy:0.4700\n",
    "Epoch #181: Loss:1.0135, Accuracy:0.4832, Validation Loss:1.0288, Validation Accuracy:0.4807\n",
    "Epoch #182: Loss:1.0129, Accuracy:0.4846, Validation Loss:1.0364, Validation Accuracy:0.4655\n",
    "Epoch #183: Loss:1.0167, Accuracy:0.4792, Validation Loss:1.0273, Validation Accuracy:0.4877\n",
    "Epoch #184: Loss:1.0145, Accuracy:0.4840, Validation Loss:1.0287, Validation Accuracy:0.4778\n",
    "Epoch #185: Loss:1.0139, Accuracy:0.4791, Validation Loss:1.0315, Validation Accuracy:0.4663\n",
    "Epoch #186: Loss:1.0136, Accuracy:0.4809, Validation Loss:1.0351, Validation Accuracy:0.4692\n",
    "Epoch #187: Loss:1.0120, Accuracy:0.4876, Validation Loss:1.0285, Validation Accuracy:0.4819\n",
    "Epoch #188: Loss:1.0161, Accuracy:0.4755, Validation Loss:1.0299, Validation Accuracy:0.4709\n",
    "Epoch #189: Loss:1.0115, Accuracy:0.4804, Validation Loss:1.0300, Validation Accuracy:0.4782\n",
    "Epoch #190: Loss:1.0131, Accuracy:0.4886, Validation Loss:1.0310, Validation Accuracy:0.4762\n",
    "Epoch #191: Loss:1.0166, Accuracy:0.4800, Validation Loss:1.0325, Validation Accuracy:0.4823\n",
    "Epoch #192: Loss:1.0130, Accuracy:0.4807, Validation Loss:1.0337, Validation Accuracy:0.4762\n",
    "Epoch #193: Loss:1.0121, Accuracy:0.4836, Validation Loss:1.0305, Validation Accuracy:0.4869\n",
    "Epoch #194: Loss:1.0189, Accuracy:0.4766, Validation Loss:1.0409, Validation Accuracy:0.4540\n",
    "Epoch #195: Loss:1.0159, Accuracy:0.4809, Validation Loss:1.0310, Validation Accuracy:0.4745\n",
    "Epoch #196: Loss:1.0120, Accuracy:0.4837, Validation Loss:1.0296, Validation Accuracy:0.4762\n",
    "Epoch #197: Loss:1.0099, Accuracy:0.4881, Validation Loss:1.0270, Validation Accuracy:0.4836\n",
    "Epoch #198: Loss:1.0093, Accuracy:0.4874, Validation Loss:1.0372, Validation Accuracy:0.4684\n",
    "Epoch #199: Loss:1.0197, Accuracy:0.4741, Validation Loss:1.0307, Validation Accuracy:0.4807\n",
    "Epoch #200: Loss:1.0122, Accuracy:0.4857, Validation Loss:1.0442, Validation Accuracy:0.4540\n",
    "Epoch #201: Loss:1.0205, Accuracy:0.4698, Validation Loss:1.0302, Validation Accuracy:0.4737\n",
    "Epoch #202: Loss:1.0118, Accuracy:0.4858, Validation Loss:1.0265, Validation Accuracy:0.4799\n",
    "Epoch #203: Loss:1.0135, Accuracy:0.4860, Validation Loss:1.0277, Validation Accuracy:0.4832\n",
    "Epoch #204: Loss:1.0119, Accuracy:0.4817, Validation Loss:1.0329, Validation Accuracy:0.4774\n",
    "Epoch #205: Loss:1.0117, Accuracy:0.4848, Validation Loss:1.0362, Validation Accuracy:0.4680\n",
    "Epoch #206: Loss:1.0109, Accuracy:0.4880, Validation Loss:1.0261, Validation Accuracy:0.4795\n",
    "Epoch #207: Loss:1.0092, Accuracy:0.4899, Validation Loss:1.0259, Validation Accuracy:0.4811\n",
    "Epoch #208: Loss:1.0090, Accuracy:0.4830, Validation Loss:1.0264, Validation Accuracy:0.4852\n",
    "Epoch #209: Loss:1.0092, Accuracy:0.4848, Validation Loss:1.0288, Validation Accuracy:0.4787\n",
    "Epoch #210: Loss:1.0135, Accuracy:0.4834, Validation Loss:1.0293, Validation Accuracy:0.4787\n",
    "Epoch #211: Loss:1.0089, Accuracy:0.4871, Validation Loss:1.0381, Validation Accuracy:0.4700\n",
    "Epoch #212: Loss:1.0111, Accuracy:0.4885, Validation Loss:1.0268, Validation Accuracy:0.4918\n",
    "Epoch #213: Loss:1.0092, Accuracy:0.4873, Validation Loss:1.0245, Validation Accuracy:0.4873\n",
    "Epoch #214: Loss:1.0087, Accuracy:0.4832, Validation Loss:1.0274, Validation Accuracy:0.4721\n",
    "Epoch #215: Loss:1.0063, Accuracy:0.4897, Validation Loss:1.0295, Validation Accuracy:0.4836\n",
    "Epoch #216: Loss:1.0098, Accuracy:0.4923, Validation Loss:1.0275, Validation Accuracy:0.4865\n",
    "Epoch #217: Loss:1.0066, Accuracy:0.4918, Validation Loss:1.0275, Validation Accuracy:0.4787\n",
    "Epoch #218: Loss:1.0081, Accuracy:0.4881, Validation Loss:1.0292, Validation Accuracy:0.4704\n",
    "Epoch #219: Loss:1.0077, Accuracy:0.4922, Validation Loss:1.0286, Validation Accuracy:0.4799\n",
    "Epoch #220: Loss:1.0051, Accuracy:0.4976, Validation Loss:1.0246, Validation Accuracy:0.4852\n",
    "Epoch #221: Loss:1.0101, Accuracy:0.4875, Validation Loss:1.0360, Validation Accuracy:0.4667\n",
    "Epoch #222: Loss:1.0065, Accuracy:0.4914, Validation Loss:1.0290, Validation Accuracy:0.4828\n",
    "Epoch #223: Loss:1.0073, Accuracy:0.4877, Validation Loss:1.0283, Validation Accuracy:0.4811\n",
    "Epoch #224: Loss:1.0150, Accuracy:0.4803, Validation Loss:1.0334, Validation Accuracy:0.4692\n",
    "Epoch #225: Loss:1.0134, Accuracy:0.4881, Validation Loss:1.0267, Validation Accuracy:0.4848\n",
    "Epoch #226: Loss:1.0117, Accuracy:0.4844, Validation Loss:1.0311, Validation Accuracy:0.4696\n",
    "Epoch #227: Loss:1.0095, Accuracy:0.4864, Validation Loss:1.0270, Validation Accuracy:0.4803\n",
    "Epoch #228: Loss:1.0097, Accuracy:0.4874, Validation Loss:1.0267, Validation Accuracy:0.4869\n",
    "Epoch #229: Loss:1.0049, Accuracy:0.4940, Validation Loss:1.0236, Validation Accuracy:0.4877\n",
    "Epoch #230: Loss:1.0069, Accuracy:0.4919, Validation Loss:1.0257, Validation Accuracy:0.4865\n",
    "Epoch #231: Loss:1.0045, Accuracy:0.4952, Validation Loss:1.0236, Validation Accuracy:0.4877\n",
    "Epoch #232: Loss:1.0043, Accuracy:0.4957, Validation Loss:1.0233, Validation Accuracy:0.4897\n",
    "Epoch #233: Loss:1.0052, Accuracy:0.4950, Validation Loss:1.0247, Validation Accuracy:0.4856\n",
    "Epoch #234: Loss:1.0047, Accuracy:0.4976, Validation Loss:1.0225, Validation Accuracy:0.4848\n",
    "Epoch #235: Loss:1.0112, Accuracy:0.4825, Validation Loss:1.0251, Validation Accuracy:0.4910\n",
    "Epoch #236: Loss:1.0086, Accuracy:0.4812, Validation Loss:1.0276, Validation Accuracy:0.4828\n",
    "Epoch #237: Loss:1.0047, Accuracy:0.4953, Validation Loss:1.0272, Validation Accuracy:0.4856\n",
    "Epoch #238: Loss:1.0048, Accuracy:0.4945, Validation Loss:1.0218, Validation Accuracy:0.4959\n",
    "Epoch #239: Loss:1.0038, Accuracy:0.4932, Validation Loss:1.0230, Validation Accuracy:0.4815\n",
    "Epoch #240: Loss:1.0053, Accuracy:0.4924, Validation Loss:1.0270, Validation Accuracy:0.4774\n",
    "Epoch #241: Loss:1.0082, Accuracy:0.4888, Validation Loss:1.0310, Validation Accuracy:0.4782\n",
    "Epoch #242: Loss:1.0039, Accuracy:0.4940, Validation Loss:1.0232, Validation Accuracy:0.4860\n",
    "Epoch #243: Loss:1.0023, Accuracy:0.4958, Validation Loss:1.0223, Validation Accuracy:0.4873\n",
    "Epoch #244: Loss:1.0029, Accuracy:0.4969, Validation Loss:1.0286, Validation Accuracy:0.4860\n",
    "Epoch #245: Loss:1.0036, Accuracy:0.4960, Validation Loss:1.0372, Validation Accuracy:0.4741\n",
    "Epoch #246: Loss:1.0119, Accuracy:0.4876, Validation Loss:1.0249, Validation Accuracy:0.4836\n",
    "Epoch #247: Loss:1.0073, Accuracy:0.4899, Validation Loss:1.0275, Validation Accuracy:0.4778\n",
    "Epoch #248: Loss:1.0053, Accuracy:0.4922, Validation Loss:1.0293, Validation Accuracy:0.4750\n",
    "Epoch #249: Loss:1.0112, Accuracy:0.4867, Validation Loss:1.0233, Validation Accuracy:0.4910\n",
    "Epoch #250: Loss:1.0047, Accuracy:0.4925, Validation Loss:1.0325, Validation Accuracy:0.4787\n",
    "Epoch #251: Loss:1.0037, Accuracy:0.5002, Validation Loss:1.0246, Validation Accuracy:0.4910\n",
    "Epoch #252: Loss:1.0030, Accuracy:0.4956, Validation Loss:1.0282, Validation Accuracy:0.4811\n",
    "Epoch #253: Loss:1.0068, Accuracy:0.4949, Validation Loss:1.0228, Validation Accuracy:0.4836\n",
    "Epoch #254: Loss:1.0048, Accuracy:0.4965, Validation Loss:1.0289, Validation Accuracy:0.4799\n",
    "Epoch #255: Loss:1.0023, Accuracy:0.4931, Validation Loss:1.0198, Validation Accuracy:0.4947\n",
    "Epoch #256: Loss:1.0003, Accuracy:0.4982, Validation Loss:1.0243, Validation Accuracy:0.4799\n",
    "Epoch #257: Loss:1.0005, Accuracy:0.4968, Validation Loss:1.0219, Validation Accuracy:0.4852\n",
    "Epoch #258: Loss:1.0037, Accuracy:0.4933, Validation Loss:1.0278, Validation Accuracy:0.4799\n",
    "Epoch #259: Loss:1.0048, Accuracy:0.4943, Validation Loss:1.0247, Validation Accuracy:0.4828\n",
    "Epoch #260: Loss:1.0132, Accuracy:0.4843, Validation Loss:1.0329, Validation Accuracy:0.4622\n",
    "Epoch #261: Loss:1.0019, Accuracy:0.4962, Validation Loss:1.0319, Validation Accuracy:0.4766\n",
    "Epoch #262: Loss:1.0068, Accuracy:0.4902, Validation Loss:1.0279, Validation Accuracy:0.4778\n",
    "Epoch #263: Loss:1.0011, Accuracy:0.4958, Validation Loss:1.0313, Validation Accuracy:0.4840\n",
    "Epoch #264: Loss:1.0110, Accuracy:0.4868, Validation Loss:1.0250, Validation Accuracy:0.4877\n",
    "Epoch #265: Loss:1.0038, Accuracy:0.4959, Validation Loss:1.0236, Validation Accuracy:0.4815\n",
    "Epoch #266: Loss:0.9995, Accuracy:0.4992, Validation Loss:1.0205, Validation Accuracy:0.4975\n",
    "Epoch #267: Loss:1.0000, Accuracy:0.5015, Validation Loss:1.0296, Validation Accuracy:0.4754\n",
    "Epoch #268: Loss:1.0115, Accuracy:0.4849, Validation Loss:1.0283, Validation Accuracy:0.4787\n",
    "Epoch #269: Loss:1.0064, Accuracy:0.4902, Validation Loss:1.0256, Validation Accuracy:0.4885\n",
    "Epoch #270: Loss:1.0018, Accuracy:0.4971, Validation Loss:1.0257, Validation Accuracy:0.4926\n",
    "Epoch #271: Loss:1.0004, Accuracy:0.4992, Validation Loss:1.0251, Validation Accuracy:0.4881\n",
    "Epoch #272: Loss:1.0019, Accuracy:0.4976, Validation Loss:1.0219, Validation Accuracy:0.4836\n",
    "Epoch #273: Loss:1.0024, Accuracy:0.4951, Validation Loss:1.0250, Validation Accuracy:0.4721\n",
    "Epoch #274: Loss:0.9989, Accuracy:0.4996, Validation Loss:1.0265, Validation Accuracy:0.4865\n",
    "Epoch #275: Loss:1.0003, Accuracy:0.5009, Validation Loss:1.0256, Validation Accuracy:0.4885\n",
    "Epoch #276: Loss:0.9974, Accuracy:0.5016, Validation Loss:1.0199, Validation Accuracy:0.5029\n",
    "Epoch #277: Loss:0.9982, Accuracy:0.4975, Validation Loss:1.0216, Validation Accuracy:0.4910\n",
    "Epoch #278: Loss:0.9977, Accuracy:0.4993, Validation Loss:1.0206, Validation Accuracy:0.5000\n",
    "Epoch #279: Loss:0.9985, Accuracy:0.4990, Validation Loss:1.0202, Validation Accuracy:0.4988\n",
    "Epoch #280: Loss:0.9982, Accuracy:0.5012, Validation Loss:1.0262, Validation Accuracy:0.4901\n",
    "Epoch #281: Loss:0.9997, Accuracy:0.4971, Validation Loss:1.0218, Validation Accuracy:0.4877\n",
    "Epoch #282: Loss:1.0025, Accuracy:0.4980, Validation Loss:1.0316, Validation Accuracy:0.4795\n",
    "Epoch #283: Loss:1.0062, Accuracy:0.4903, Validation Loss:1.0187, Validation Accuracy:0.4947\n",
    "Epoch #284: Loss:1.0054, Accuracy:0.4915, Validation Loss:1.0188, Validation Accuracy:0.4959\n",
    "Epoch #285: Loss:1.0030, Accuracy:0.4953, Validation Loss:1.0227, Validation Accuracy:0.4906\n",
    "Epoch #286: Loss:0.9981, Accuracy:0.4994, Validation Loss:1.0207, Validation Accuracy:0.4926\n",
    "Epoch #287: Loss:0.9977, Accuracy:0.5002, Validation Loss:1.0279, Validation Accuracy:0.4811\n",
    "Epoch #288: Loss:1.0001, Accuracy:0.4987, Validation Loss:1.0216, Validation Accuracy:0.4893\n",
    "Epoch #289: Loss:0.9960, Accuracy:0.5032, Validation Loss:1.0225, Validation Accuracy:0.4721\n",
    "Epoch #290: Loss:0.9972, Accuracy:0.5014, Validation Loss:1.0187, Validation Accuracy:0.4971\n",
    "Epoch #291: Loss:0.9985, Accuracy:0.4991, Validation Loss:1.0314, Validation Accuracy:0.4815\n",
    "Epoch #292: Loss:1.0009, Accuracy:0.4965, Validation Loss:1.0266, Validation Accuracy:0.4914\n",
    "Epoch #293: Loss:1.0002, Accuracy:0.4979, Validation Loss:1.0174, Validation Accuracy:0.4988\n",
    "Epoch #294: Loss:0.9972, Accuracy:0.5025, Validation Loss:1.0188, Validation Accuracy:0.4984\n",
    "Epoch #295: Loss:0.9992, Accuracy:0.5008, Validation Loss:1.0191, Validation Accuracy:0.4979\n",
    "Epoch #296: Loss:0.9993, Accuracy:0.5010, Validation Loss:1.0208, Validation Accuracy:0.4885\n",
    "Epoch #297: Loss:0.9961, Accuracy:0.4996, Validation Loss:1.0178, Validation Accuracy:0.4984\n",
    "Epoch #298: Loss:0.9956, Accuracy:0.5050, Validation Loss:1.0194, Validation Accuracy:0.4947\n",
    "Epoch #299: Loss:0.9925, Accuracy:0.5080, Validation Loss:1.0222, Validation Accuracy:0.4922\n",
    "Epoch #300: Loss:0.9950, Accuracy:0.5042, Validation Loss:1.0206, Validation Accuracy:0.4901\n",
    "\n",
    "Test:\n",
    "Test Loss:1.02060437, Accuracy:0.4901\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "       02   01   03\n",
    "t:02  535  299   74\n",
    "t:01  313  536  111\n",
    "t:03  152  293  123\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.54      0.59      0.56       908\n",
    "          01       0.48      0.56      0.51       960\n",
    "          03       0.40      0.22      0.28       568\n",
    "\n",
    "    accuracy                           0.49      2436\n",
    "   macro avg       0.47      0.45      0.45      2436\n",
    "weighted avg       0.48      0.49      0.48      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 12:51:48 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 28 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.077376083200201, 1.0757861538669355, 1.0743725589539226, 1.0745423399951854, 1.0739492823924925, 1.0732931712969576, 1.0724142016644156, 1.0705831976751192, 1.0675812243240808, 1.0701047317147843, 1.0690619509012633, 1.064899382724355, 1.0669121129563681, 1.0630949555555196, 1.0623144690430615, 1.0636591985699382, 1.0694353805582708, 1.061966622012785, 1.0637900729484746, 1.065976000380242, 1.062475679738964, 1.0621246741518795, 1.0618239541359136, 1.0644302170460642, 1.0608976427557433, 1.061525431955585, 1.0611683944567476, 1.0618327262953584, 1.0606887538249075, 1.0607252263866231, 1.0589509478147785, 1.06001651521974, 1.0584231305787912, 1.0652285264239132, 1.0588380785411216, 1.0580282927733924, 1.058881290831981, 1.0590535719406429, 1.0601679407708555, 1.0590509203658707, 1.0597951881991232, 1.0582341176927188, 1.063093789105345, 1.056216757285771, 1.0565073713293216, 1.0570643049938533, 1.0590204685583882, 1.0558552528641298, 1.0614434932840282, 1.0560226751665764, 1.0571952731347045, 1.0567064948857123, 1.0610510504304482, 1.0550451679965742, 1.0545879788390913, 1.0554188117996617, 1.0569871746260544, 1.0557720690525223, 1.0550084718929722, 1.0563917314673488, 1.0540777010283446, 1.0624424117343572, 1.057726795254474, 1.05474449828732, 1.0526594704595105, 1.0546219716909875, 1.0530528010210185, 1.0533865874232526, 1.0531179717021624, 1.0526254529436234, 1.0553379471861866, 1.0522085005426642, 1.054993661166412, 1.0532739818194035, 1.0518024832940063, 1.0544238094430056, 1.0510365080167898, 1.0507707903146353, 1.0509150740744053, 1.0546283927457085, 1.0498290198972855, 1.0501357526419002, 1.051198056178727, 1.0504315944728007, 1.0515409817640808, 1.059754012840722, 1.053338519653859, 1.0490284463259192, 1.050159240786861, 1.0567154479144243, 1.0467638335204477, 1.0482648877283232, 1.0483588033121796, 1.04764195185381, 1.0480295372713964, 1.0459069884665102, 1.0492522326987757, 1.0458435425030186, 1.0454237472834846, 1.0434298022040005, 1.0458314982541088, 1.0446209936893631, 1.0474031274933338, 1.0414190225804771, 1.0423523514533082, 1.044340885331478, 1.0432079002774994, 1.044789511581947, 1.0407744013812938, 1.042567175401647, 1.037141793662887, 1.0413369161546329, 1.039797023207879, 1.0414658246564943, 1.0505738573512813, 1.0421065023575706, 1.0408256098946131, 1.041972718019595, 1.0444680135238347, 1.0373647712134375, 1.044535030285126, 1.0437757632219538, 1.0370068689089496, 1.037842781085686, 1.0350761327445996, 1.034235627193169, 1.0373907688216035, 1.0431971886670843, 1.0397814222548787, 1.03997985052162, 1.035009732387336, 1.037769118553312, 1.0343581972451046, 1.0400516335012877, 1.0471631427508075, 1.0334027849003207, 1.0371466489456753, 1.0346109260283471, 1.0348188863403496, 1.0330862333426138, 1.0390977577622889, 1.0378702986612305, 1.0336889077485685, 1.0337192240998467, 1.0339912886690037, 1.0337855847206805, 1.0313559478922627, 1.030410906168432, 1.0412072657755835, 1.0342767677087894, 1.0349775462706492, 1.033692565849066, 1.0350274344774695, 1.0385177219638293, 1.0351871351890376, 1.0386667194820585, 1.0371644213086082, 1.0327153139317955, 1.0310331146509581, 1.0326360638309973, 1.030632437156339, 1.0337389417861287, 1.0313079153571418, 1.0298254873560764, 1.0396765405908595, 1.0389526317076534, 1.0307750032453113, 1.0286025801315684, 1.0296081234081624, 1.0320394217282876, 1.0288409554508129, 1.0476260063879204, 1.0422165059103754, 1.0309968707205235, 1.031001636351662, 1.0316546399800843, 1.0303753716213557, 1.028939552495045, 1.0308902120746806, 1.0348979776911744, 1.0288212191686645, 1.0363818710464954, 1.027300325324774, 1.0286917345864433, 1.0315205857084302, 1.0350763768398117, 1.0284501973827094, 1.0298727468904016, 1.0299502174646789, 1.0309619711733413, 1.032467845429732, 1.0337047111028912, 1.0304757164812637, 1.0408554539108903, 1.030992436291549, 1.0295831881133206, 1.0269626116713475, 1.0371678106499032, 1.0306771486655049, 1.0441826683742854, 1.030180222295188, 1.026501238052481, 1.0277307826309956, 1.0329211294553158, 1.036202410171772, 1.026123178416285, 1.0259467277229322, 1.0263785159059347, 1.0288023854711372, 1.0293018649560086, 1.0380583960434486, 1.026841904337966, 1.024453220304793, 1.027404507392733, 1.029504774043517, 1.027478951342979, 1.0274538925324364, 1.029194581880554, 1.0285517255269443, 1.0246390697404082, 1.0359871254374438, 1.0290050990084318, 1.0282577008057894, 1.0333893696467082, 1.0267009966087655, 1.0311122664872845, 1.0269756395437055, 1.026672329025707, 1.0235677477957188, 1.0256884389714458, 1.0235904339694821, 1.0232885414352166, 1.0246700492789984, 1.0224890569943708, 1.0250571811531957, 1.0276369782308443, 1.0271866043604458, 1.02179292485436, 1.0230465072324906, 1.0270281270611266, 1.0309745353235205, 1.0232102025318615, 1.0223238808768136, 1.0286273764467788, 1.0371561207011808, 1.0249216100460985, 1.0274581574454096, 1.0293499162827415, 1.0232546877586979, 1.0325469152485012, 1.024632264044876, 1.0282389767259996, 1.0228326058348607, 1.0289137326242106, 1.019775307237221, 1.0243214163286933, 1.021858645190159, 1.0278175709087078, 1.0246707514197564, 1.0328887162733156, 1.0319404699923762, 1.0279014169288974, 1.0313093266855125, 1.025030045869511, 1.0236494656658328, 1.0205270750769253, 1.0295774533439348, 1.028277791388125, 1.0256113671316889, 1.0256778955068102, 1.0250973390240974, 1.0219322574158216, 1.0249717597695212, 1.0264723230465291, 1.0255513610119498, 1.0198743911016555, 1.0216141230562834, 1.0206454342417726, 1.020195084252381, 1.0262124808551056, 1.0217746913139456, 1.0315946044984514, 1.0187233058102612, 1.0187554412287445, 1.0227454652144208, 1.0206697830817186, 1.0279250256533694, 1.0215704574177809, 1.0225007322621462, 1.0187491198087169, 1.0314255293171197, 1.0265602711190536, 1.01743353842123, 1.0187667488856074, 1.019126452248672, 1.0208150763034038, 1.0178317575423392, 1.0193926342602433, 1.0221650385112793, 1.020604285700568], 'val_acc': [0.3727422013560735, 0.3940886711252147, 0.3940886711252147, 0.3940886711252147, 0.40681445119024695, 0.3932676506649293, 0.4133825934383474, 0.41297208670716373, 0.4244663372317754, 0.4244663396785999, 0.4220032853935348, 0.4298029545884218, 0.42487684861192565, 0.42816091831681763, 0.42980295683950037, 0.43144499301323164, 0.428981936574961, 0.42775040928561897, 0.4252873573005689, 0.42651888238776886, 0.43637110148548885, 0.42775041168350697, 0.4343185538337344, 0.43637110143655233, 0.43390804480253575, 0.43062397255294627, 0.43637109908760086, 0.4343185562316224, 0.4355500833252185, 0.4285714297948408, 0.43308702678907485, 0.4347290628649331, 0.4339080448514722, 0.428981936574961, 0.43719211945001324, 0.4376026261322604, 0.43719211705212524, 0.43596059235641715, 0.4376026260833239, 0.4367816104188145, 0.44129720741304856, 0.435550083276282, 0.4490968791526331, 0.4408866983818499, 0.435550083276282, 0.4355500833252185, 0.4404761918464122, 0.43555008097626696, 0.433497538267098, 0.4408866984307864, 0.4376026261322604, 0.4380131352123956, 0.4425287370513421, 0.43637109908760086, 0.44006568271734053, 0.4396551712882538, 0.44129720990880955, 0.4458128068051706, 0.443349755064803, 0.45525451476742285, 0.45197044241996037, 0.44006568071094443, 0.44950739063065626, 0.4478653544079885, 0.44458128210946257, 0.4581280802858287, 0.4462233182342573, 0.46141215263329116, 0.4568965532411691, 0.45853858701701233, 0.45279146062916725, 0.44622331598317877, 0.455665023847558, 0.4490968815505211, 0.44252873460451764, 0.43883415562374445, 0.4412972098598731, 0.4462233158853058, 0.44991789721503045, 0.46592774967646167, 0.4449917888406462, 0.44499179114066123, 0.46223317069568853, 0.44950739053278327, 0.4503284064909116, 0.434729063354298, 0.44909688174626705, 0.44786535455479803, 0.4437602641449382, 0.43390804490040874, 0.4486863724703859, 0.45320196946461994, 0.4519704448667849, 0.46100164365102897, 0.46059113231981524, 0.4412972075598581, 0.45402298782063627, 0.44950738833064124, 0.44786535205903705, 0.4507389152774278, 0.4449917889385192, 0.4490968792994426, 0.4536124810894526, 0.4458128069519801, 0.4536124786426281, 0.4519704425667699, 0.4540229901695878, 0.4552545171163744, 0.45320196961142944, 0.45238095159796854, 0.46182265931553834, 0.4495073906795928, 0.4581280779858137, 0.454433496753962, 0.4470443342413221, 0.45155993353557117, 0.4560755352277082, 0.4511494269022605, 0.4392446625017376, 0.4548440057362242, 0.44745484557253584, 0.45032840653984807, 0.4548440058340972, 0.4573070623213043, 0.46592774967646167, 0.4581280779858137, 0.46141215043114914, 0.44211822821588936, 0.46059113481557623, 0.4536124810894526, 0.4626436798247602, 0.46141215033327615, 0.46182265931553834, 0.4548440058340972, 0.4490968793973155, 0.46182266161555335, 0.45525451706743786, 0.4634646976424751, 0.4659277495785887, 0.475369457345095, 0.46346469539139656, 0.45894909844609905, 0.4634646953424601, 0.4593596074772977, 0.4634646977403481, 0.4601806230928706, 0.47331691448911656, 0.47331691458698955, 0.4589490983971625, 0.4614121525843547, 0.4696223308593769, 0.462233168248864, 0.4630541863601979, 0.4597701141595449, 0.4712643693820596, 0.45484400568728767, 0.47577996877418166, 0.46469622243605613, 0.4729064054089814, 0.462233168248864, 0.4712643669841716, 0.47413793020256245, 0.48029556801949425, 0.48768472823211906, 0.4532019695624929, 0.454433496656089, 0.47783251143441413, 0.4835796380180052, 0.47660098678764257, 0.47495894846070574, 0.4790640408769617, 0.454433496753962, 0.46469622488288065, 0.47660098678764257, 0.4683908062615418, 0.4778325114833506, 0.4753694596940465, 0.4790640409748347, 0.48563218297825267, 0.47003283984163907, 0.4807060745549319, 0.465517240645263, 0.48768473072788004, 0.4778325114833506, 0.46633825856085087, 0.4692118243239392, 0.481937601844274, 0.47085386044873395, 0.4782430230103103, 0.47619047785431684, 0.48234811332229716, 0.47619047555430183, 0.48686371246973675, 0.4540229877227633, 0.4745484415337761, 0.47619047775644385, 0.48357964026908373, 0.46839080396152677, 0.4807060747017414, 0.4540229902674608, 0.4737274213181732, 0.47988505654147107, 0.48316913128682154, 0.4774220025010884, 0.4679802949303281, 0.47947454751027235, 0.4811165861308281, 0.4852216739959905, 0.4786535320904455, 0.47865352954574797, 0.4700328425820825, 0.49178982094199397, 0.48727421934772985, 0.472085385046569, 0.4835796403180202, 0.48645320113852303, 0.4786535294968115, 0.47044334892177425, 0.4798850590861685, 0.48522167414279993, 0.46674877023462, 0.48275861990667135, 0.48111658632657406, 0.46921182192605115, 0.48481116516053774, 0.4696223311040593, 0.4802955683131132, 0.4868637102675947, 0.4876847306300071, 0.486453203536411, 0.4876847306789436, 0.4897372734859855, 0.4856321856208231, 0.48481116755842574, 0.49096880057958153, 0.48275862240243234, 0.4856321856208231, 0.4958949114007903, 0.48152709521096326, 0.4774220049479129, 0.4782430206124223, 0.4860426922051973, 0.48727422174561785, 0.4860426923520068, 0.47413793049618136, 0.4835796379690687, 0.47783251177696956, 0.47495895095646673, 0.49096880302640605, 0.47865352983936693, 0.49096880312427904, 0.4811165861308281, 0.4835796379201322, 0.4798850592819145, 0.4946633843561307, 0.47988505923297803, 0.4852216742406729, 0.4798850591840415, 0.4827586224513688, 0.462233170842498, 0.47660098473231, 0.47783251407698457, 0.4839901472449498, 0.48768473082575303, 0.48152709530883625, 0.4975369475745215, 0.47536945744296794, 0.4786535297904304, 0.4885057465881354, 0.49261083920013726, 0.48809523985695175, 0.4835796380669417, 0.47208538754233, 0.48645320378109347, 0.4885057489860234, 0.5028735625822164, 0.49096880062851805, 0.49999999931488914, 0.4987684746681176, 0.4901477827618666, 0.487684728525738, 0.4794745503975253, 0.4946633820071792, 0.4958949114007903, 0.49055829169519233, 0.49261083680224926, 0.4811165839776225, 0.4893267669505478, 0.472085385144442, 0.4971264361943713, 0.4815270931066942, 0.4913793098554627, 0.4987684746191811, 0.4983579656369189, 0.4979474567035932, 0.4885057489860234, 0.49835796333690385, 0.4946633820071792, 0.4922003279667965, 0.49014778501294515], 'loss': [1.0847192531248873, 1.0754667979246293, 1.075486313415504, 1.0746417796097743, 1.0738187537545785, 1.0728577294144053, 1.071682932930071, 1.0694570870859668, 1.0639669435959331, 1.0566240857514022, 1.0543415988495217, 1.0541595610260228, 1.0523171644680798, 1.0507893414957568, 1.0499105408695935, 1.0494903966386704, 1.0486136605852194, 1.0567891026424432, 1.0501242107434439, 1.0540353804887932, 1.0503321200670404, 1.048944856890418, 1.0486445639167723, 1.0485699912116269, 1.0481757245758965, 1.0475165278025476, 1.0479796091878684, 1.0480613216482393, 1.0473038846707197, 1.0462783953981967, 1.0461590329724415, 1.0461286153636673, 1.0457363792023864, 1.0515997084993602, 1.047970677254381, 1.046321294978414, 1.0464306356970534, 1.0459956196788889, 1.045775200307247, 1.0444233853714178, 1.0440329257712473, 1.0443535015078786, 1.0437960798001142, 1.049401856888491, 1.0451901012867137, 1.043349873946188, 1.0447117015811207, 1.0429785386248047, 1.0440318659345718, 1.044752007000745, 1.042931009905539, 1.0455997741442686, 1.0420398058587765, 1.0480235681396735, 1.0443485408348225, 1.0422258924899406, 1.0413648445013857, 1.0410651305862522, 1.041636876306005, 1.042100910925033, 1.0403935223389456, 1.041052332477648, 1.042907313497649, 1.0409998126098507, 1.0408166223972486, 1.0402317023130412, 1.0403709456171588, 1.039569113655991, 1.039121965852851, 1.0393720608717119, 1.0389602452088185, 1.038940410520996, 1.038783255715145, 1.0377342143342725, 1.037741704814488, 1.039037138554105, 1.0422024464215587, 1.037934773560667, 1.038104573094135, 1.0372414931134766, 1.037361356389596, 1.0422156864612744, 1.0370362796088264, 1.0354794453301714, 1.0352909243327146, 1.0338476334753957, 1.0420944141901005, 1.0377702718642703, 1.0389562442072608, 1.0363189108073099, 1.0430653251416875, 1.0358352201185677, 1.0342545759996105, 1.0326282040049652, 1.0313339489930953, 1.0318896835589555, 1.0365303931539798, 1.0346744324881927, 1.032471441953334, 1.0367202625627145, 1.0354944170378073, 1.0328475442510365, 1.0303663944561623, 1.0306319855566632, 1.0333726577200684, 1.0298276154167598, 1.030000290048196, 1.030776993596823, 1.0309545983034483, 1.0291528778154502, 1.0270410737952167, 1.0293434489189477, 1.027821185040523, 1.0311571576756864, 1.0340765237808227, 1.032232818123741, 1.0281257544454854, 1.0269548628119718, 1.0278154801783865, 1.0365358227821835, 1.0295544578798987, 1.0297073141994908, 1.0281500867260065, 1.0243413888208674, 1.0240624781996317, 1.0251813142451418, 1.025853933397015, 1.0242159541627465, 1.0295899520909273, 1.0257007221421666, 1.024260199755369, 1.0224774322960166, 1.0215074629019907, 1.0221490938315891, 1.0241183107393723, 1.0278729597156298, 1.0210531498861999, 1.025570951864215, 1.0204707781147417, 1.0211398843622306, 1.0227126943502094, 1.022010470317864, 1.023765977770396, 1.0215088142996207, 1.0267889202742606, 1.0184326980148253, 1.023882260777867, 1.020693370938546, 1.0188407116112523, 1.026990607754161, 1.018967058428504, 1.018200366379544, 1.0203916800585127, 1.023460368600959, 1.0194961706470904, 1.0185996466838358, 1.0189178774733807, 1.0189719849788188, 1.01813975516286, 1.017146770812158, 1.0191668328073724, 1.0171118048427041, 1.0170238165884782, 1.0145583924082027, 1.0139886928290067, 1.0250412342484727, 1.0204429467845502, 1.015569846292296, 1.0139601349096279, 1.0158519856249282, 1.0165360986819258, 1.0172780113298545, 1.040392120516031, 1.021911287454609, 1.0177408893495126, 1.0143411436854446, 1.0161133938501503, 1.017144850857204, 1.0133849302356492, 1.0137543459447746, 1.0134870845923922, 1.012925731401424, 1.0166514366803963, 1.0144901351517475, 1.0138954809314171, 1.0136439491346387, 1.0119635449297864, 1.0161378808090085, 1.011531084469946, 1.0130872769522226, 1.016600923983713, 1.0129618850821587, 1.0120886286915696, 1.0188975364520565, 1.0159281983023063, 1.0119798738364076, 1.0099472323237504, 1.0093482522259503, 1.019673580090368, 1.0122249137204775, 1.0204858845019487, 1.0117655073103229, 1.013538912458831, 1.0118564872036724, 1.0117027149552926, 1.010934658168033, 1.0092351811132882, 1.009013253315763, 1.009210172962604, 1.0135407574857285, 1.0088565104306355, 1.011104417508143, 1.0091684861839185, 1.0087178961697056, 1.0063393953154953, 1.0098312219310346, 1.0066090553448186, 1.0080699416891015, 1.0076824686120913, 1.005114760340117, 1.010062484036236, 1.006491547639365, 1.007345230280741, 1.0149720420582828, 1.0133813942727121, 1.0116505317374667, 1.0095261144931802, 1.0096796870721194, 1.0048629678005556, 1.0069261915140328, 1.0045257971027304, 1.004347581882986, 1.0051991804914062, 1.004703854241655, 1.0111699130990421, 1.0086487531172421, 1.0046796510351756, 1.0047627068642964, 1.003809054725224, 1.0052580132866298, 1.008169390435581, 1.003871225478468, 1.0023428852798024, 1.0028767974224913, 1.0035642511790783, 1.0118736355456484, 1.0072675785734424, 1.0053273966180225, 1.0111630460320067, 1.0046573622270776, 1.0037327502052886, 1.0030464916258621, 1.0067925889634008, 1.0048484765528654, 1.002265743357445, 1.0003329471396225, 1.0005369156293067, 1.0036680946604672, 1.0048205906850356, 1.0132232268243355, 1.0019213533009836, 1.006829077972279, 1.0010860001037254, 1.0110096410069866, 1.0038262855100926, 0.9995139661510867, 1.0000090829890367, 1.0114949738465295, 1.0063792615945335, 1.001835391433332, 1.0004313172256187, 1.001939168040023, 1.00239654911617, 0.9988656311798879, 1.000253254162947, 0.9973690060619456, 0.9981588012139165, 0.9977441382848752, 0.9984842591706732, 0.9981643542861547, 0.999728472644054, 1.0025350272533096, 1.0062302024947054, 1.0054488228576628, 1.0029549545820733, 0.9981377630018355, 0.9976554635613851, 1.0000607018108485, 0.9959896011274209, 0.9972359057569405, 0.9984834047068806, 1.0009431705337775, 1.0001772108998386, 0.9971633938548501, 0.9991887086960325, 0.9993412058456232, 0.9961395486913912, 0.9956484393172685, 0.9925190195165865, 0.9950388075145118], 'acc': [0.38521560574948666, 0.3918891170431211, 0.3944558521805358, 0.39425051333478345, 0.3957905543903061, 0.4125256673511294, 0.40071868584386133, 0.425564681724846, 0.4286447638603696, 0.44209445585215607, 0.4379876796469796, 0.4412731006282556, 0.4430184804928131, 0.44240246406570843, 0.44650924027088484, 0.44332648869412633, 0.442505133482465, 0.43778234087466217, 0.44199178645987774, 0.4398357289650112, 0.4434291581170025, 0.443737166299957, 0.44240246406570843, 0.44404517452574854, 0.4451745379876797, 0.441375770032773, 0.44486652977412733, 0.4381930185049711, 0.4446611909650924, 0.44527720737995796, 0.44353182752763953, 0.442607802850265, 0.44342915811088296, 0.4411704311870207, 0.44065708417667254, 0.4439425051334702, 0.44435318276377916, 0.4467145790309632, 0.4457905544025452, 0.44291581107605654, 0.4430184805172914, 0.44383983571671365, 0.4426078028625042, 0.44404517455022685, 0.4465092402464066, 0.4468172484477198, 0.4437371663121962, 0.4474332648748245, 0.4459958932115802, 0.4457905544025452, 0.4539014373594241, 0.44599589322381933, 0.4479466119157705, 0.44137577002665346, 0.451129363449692, 0.44702258726899385, 0.4486652977412731, 0.44917864473938207, 0.44568788501026696, 0.4489733059548255, 0.4540041067761807, 0.44979466117872596, 0.44958932238193017, 0.4568788500904303, 0.44928131418061695, 0.4542094455852156, 0.44845995893223817, 0.4545174537865288, 0.45882956877626185, 0.4533880903368368, 0.4534907597658326, 0.4578028747433265, 0.4528747433020104, 0.45482546203679863, 0.4533880903368368, 0.45482546203679863, 0.45410677615621986, 0.45975359344139727, 0.4502053388090349, 0.45708418889946517, 0.45728952770850007, 0.45, 0.45924024640657085, 0.4589322381930185, 0.45667351130587364, 0.4591375770020534, 0.4424024640779476, 0.45544147841494675, 0.45924024643104916, 0.4557494866407383, 0.44004106776180696, 0.4559548254620123, 0.455236139618151, 0.45944558521560575, 0.4600616016427105, 0.45882956880074016, 0.4561601642710472, 0.45975359341691896, 0.45852156056270954, 0.4564681724845996, 0.45882956880074016, 0.4606776180698152, 0.45903490759753596, 0.46149897328147654, 0.4573921971252567, 0.46170431209051144, 0.4668377823531016, 0.46314168380271237, 0.46160164268599396, 0.4666324435440667, 0.45954825460788407, 0.46088295686661096, 0.4580082135278831, 0.4563655030800821, 0.4559548254620123, 0.4587268993839836, 0.4648870636550308, 0.46375770019309964, 0.46365503077634307, 0.4590349075852968, 0.46375770021757795, 0.46704312113765817, 0.4639630390143737, 0.46714579054217564, 0.4638603695976171, 0.4624229979466119, 0.46786447636155865, 0.4650924024640657, 0.4625256673388902, 0.4690965092157681, 0.46786447638603695, 0.4669404517453799, 0.4688911704312115, 0.46560574948665295, 0.4696098562628337, 0.46560574948665295, 0.4702258726776993, 0.46673511296082326, 0.4707392197125257, 0.47063655029576906, 0.46971252564287286, 0.4685831622176591, 0.46786447636155865, 0.4712525667595912, 0.4668377823163841, 0.47443531828127355, 0.4688911704312115, 0.47392197125256674, 0.47217659138800916, 0.4659137577002053, 0.47597535931843754, 0.47587268991392007, 0.4717659137821785, 0.4668377823163841, 0.47145790554414785, 0.4729979466241488, 0.47320328540870543, 0.47535934290357196, 0.472587269006079, 0.4743326488583974, 0.4783367556712955, 0.47628336755646816, 0.4828542094455852, 0.47895277209840026, 0.47926078026299607, 0.47032854209445585, 0.4757700205583592, 0.47936344967975264, 0.4829568788501027, 0.4809034907842319, 0.47782340862423, 0.4794661190965092, 0.4572895277329784, 0.4771047227926078, 0.478542094443613, 0.4771047227681295, 0.4801848049036531, 0.48028747433264884, 0.479568788525505, 0.47997946611909653, 0.48316221764689843, 0.48459958929790364, 0.4791581108829569, 0.4839835728707989, 0.4790554414906786, 0.48090349074751443, 0.4875770020564479, 0.4754620123080894, 0.4803901437371663, 0.48860369609856263, 0.4799794660946182, 0.48069815193847953, 0.48357289527720737, 0.47659137579449884, 0.4809034907842319, 0.48367556470620315, 0.48809034907597537, 0.48737166324435316, 0.47412731008607994, 0.48572895274759564, 0.46981519508410774, 0.48583162217659137, 0.486036960961148, 0.48172484600813237, 0.48480492810693854, 0.4879876796714579, 0.48993839835728953, 0.48295687885622224, 0.4848049281344766, 0.4833675564681725, 0.4870636550185617, 0.488501026681806, 0.48726899381535743, 0.48316221764689843, 0.4897330595237763, 0.49229979468566926, 0.491786447663082, 0.48809034907597537, 0.4921971252444344, 0.4976386036960986, 0.4874743326243923, 0.49137577003277305, 0.4876796714701447, 0.48028747432040975, 0.48809034907597537, 0.484394250513347, 0.48644763859145695, 0.4873716632198749, 0.4940451745379877, 0.49188911704312116, 0.4951745379876797, 0.49568788498578864, 0.4949691991541665, 0.4976386036838594, 0.4825462012197937, 0.48121149897330595, 0.49527720737995795, 0.4944558521438184, 0.49322381930184805, 0.49240246405346927, 0.48880903493207584, 0.4940451745135094, 0.49579055441478437, 0.4969199178522372, 0.495995893199341, 0.4875770020533881, 0.4899383983328112, 0.4921971252321952, 0.4866529774372093, 0.49250513349470415, 0.500205338821274, 0.4955852156057495, 0.4948665297741273, 0.4965092402341674, 0.4931211498973306, 0.498151950730925, 0.4968172484721981, 0.4933264886818872, 0.49425051335926173, 0.48429158109659043, 0.49620123202061506, 0.4902464065708419, 0.49579055441478437, 0.4867556468172485, 0.49589322381930184, 0.49917864476386037, 0.5015400410922402, 0.4849075975481734, 0.4902464065463636, 0.49712525669798957, 0.4991786447883387, 0.4976386036960986, 0.4950718685586839, 0.4995893223819302, 0.5009240246406571, 0.5016427104600402, 0.4975359342671028, 0.4992813141561387, 0.4989733059548255, 0.5012320328542095, 0.4971252566735113, 0.4980492813019292, 0.4903490759631202, 0.4914784394250513, 0.49527720736771885, 0.49938398359737357, 0.5002053387967957, 0.4986652977412731, 0.5031827515400411, 0.501437371638766, 0.4990759753471038, 0.49650924025864573, 0.49794661190965095, 0.5024640657084188, 0.5008213552361396, 0.5010266940329354, 0.49958932240640846, 0.5050308008091161, 0.5080082135523614, 0.5042094455607373]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
