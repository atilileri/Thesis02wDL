{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf33.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 07:44:09 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '2', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '03', '01'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000014F15709550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000014F0DF26EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1015, Accuracy:0.3437, Validation Loss:1.0911, Validation Accuracy:0.3727\n",
    "Epoch #2: Loss:1.0849, Accuracy:0.3749, Validation Loss:1.0797, Validation Accuracy:0.3859\n",
    "Epoch #3: Loss:1.0766, Accuracy:0.3869, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #4: Loss:1.0735, Accuracy:0.3996, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #5: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0754, Validation Accuracy:0.3924\n",
    "Epoch #6: Loss:1.0745, Accuracy:0.3922, Validation Loss:1.0761, Validation Accuracy:0.4023\n",
    "Epoch #7: Loss:1.0741, Accuracy:0.3914, Validation Loss:1.0750, Validation Accuracy:0.4007\n",
    "Epoch #8: Loss:1.0738, Accuracy:0.3922, Validation Loss:1.0747, Validation Accuracy:0.4105\n",
    "Epoch #9: Loss:1.0734, Accuracy:0.3967, Validation Loss:1.0753, Validation Accuracy:0.4122\n",
    "Epoch #10: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0757, Validation Accuracy:0.4023\n",
    "Epoch #11: Loss:1.0738, Accuracy:0.3934, Validation Loss:1.0758, Validation Accuracy:0.4056\n",
    "Epoch #12: Loss:1.0738, Accuracy:0.3926, Validation Loss:1.0758, Validation Accuracy:0.4056\n",
    "Epoch #13: Loss:1.0734, Accuracy:0.3926, Validation Loss:1.0757, Validation Accuracy:0.4039\n",
    "Epoch #14: Loss:1.0735, Accuracy:0.3901, Validation Loss:1.0760, Validation Accuracy:0.3908\n",
    "Epoch #15: Loss:1.0733, Accuracy:0.3930, Validation Loss:1.0761, Validation Accuracy:0.3924\n",
    "Epoch #16: Loss:1.0735, Accuracy:0.3930, Validation Loss:1.0758, Validation Accuracy:0.4007\n",
    "Epoch #17: Loss:1.0734, Accuracy:0.3910, Validation Loss:1.0761, Validation Accuracy:0.4105\n",
    "Epoch #18: Loss:1.0735, Accuracy:0.3930, Validation Loss:1.0763, Validation Accuracy:0.4105\n",
    "Epoch #19: Loss:1.0735, Accuracy:0.3910, Validation Loss:1.0763, Validation Accuracy:0.4072\n",
    "Epoch #20: Loss:1.0736, Accuracy:0.3963, Validation Loss:1.0764, Validation Accuracy:0.4072\n",
    "Epoch #21: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0766, Validation Accuracy:0.4089\n",
    "Epoch #22: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0756, Validation Accuracy:0.4072\n",
    "Epoch #23: Loss:1.0738, Accuracy:0.3922, Validation Loss:1.0759, Validation Accuracy:0.4089\n",
    "Epoch #24: Loss:1.0739, Accuracy:0.3996, Validation Loss:1.0760, Validation Accuracy:0.4105\n",
    "Epoch #25: Loss:1.0739, Accuracy:0.3951, Validation Loss:1.0760, Validation Accuracy:0.3990\n",
    "Epoch #26: Loss:1.0737, Accuracy:0.3951, Validation Loss:1.0763, Validation Accuracy:0.4056\n",
    "Epoch #27: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0763, Validation Accuracy:0.4072\n",
    "Epoch #28: Loss:1.0736, Accuracy:0.4000, Validation Loss:1.0765, Validation Accuracy:0.4105\n",
    "Epoch #29: Loss:1.0736, Accuracy:0.4012, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #30: Loss:1.0734, Accuracy:0.4049, Validation Loss:1.0766, Validation Accuracy:0.4023\n",
    "Epoch #31: Loss:1.0736, Accuracy:0.4029, Validation Loss:1.0766, Validation Accuracy:0.4039\n",
    "Epoch #32: Loss:1.0732, Accuracy:0.3992, Validation Loss:1.0769, Validation Accuracy:0.3990\n",
    "Epoch #33: Loss:1.0733, Accuracy:0.4000, Validation Loss:1.0771, Validation Accuracy:0.4105\n",
    "Epoch #34: Loss:1.0736, Accuracy:0.3992, Validation Loss:1.0773, Validation Accuracy:0.4056\n",
    "Epoch #35: Loss:1.0736, Accuracy:0.3930, Validation Loss:1.0771, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0766, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0734, Accuracy:0.3979, Validation Loss:1.0766, Validation Accuracy:0.3842\n",
    "Epoch #38: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0767, Validation Accuracy:0.3826\n",
    "Epoch #39: Loss:1.0734, Accuracy:0.3984, Validation Loss:1.0770, Validation Accuracy:0.3793\n",
    "Epoch #40: Loss:1.0734, Accuracy:0.3959, Validation Loss:1.0768, Validation Accuracy:0.3859\n",
    "Epoch #41: Loss:1.0734, Accuracy:0.3951, Validation Loss:1.0769, Validation Accuracy:0.3842\n",
    "Epoch #42: Loss:1.0733, Accuracy:0.3967, Validation Loss:1.0768, Validation Accuracy:0.3924\n",
    "Epoch #43: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0767, Validation Accuracy:0.3908\n",
    "Epoch #44: Loss:1.0732, Accuracy:0.3951, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #45: Loss:1.0734, Accuracy:0.3992, Validation Loss:1.0769, Validation Accuracy:0.3990\n",
    "Epoch #46: Loss:1.0731, Accuracy:0.4021, Validation Loss:1.0768, Validation Accuracy:0.3924\n",
    "Epoch #47: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0769, Validation Accuracy:0.4056\n",
    "Epoch #48: Loss:1.0731, Accuracy:0.4021, Validation Loss:1.0770, Validation Accuracy:0.3908\n",
    "Epoch #49: Loss:1.0730, Accuracy:0.3996, Validation Loss:1.0769, Validation Accuracy:0.3924\n",
    "Epoch #50: Loss:1.0729, Accuracy:0.4041, Validation Loss:1.0769, Validation Accuracy:0.4023\n",
    "Epoch #51: Loss:1.0729, Accuracy:0.4053, Validation Loss:1.0769, Validation Accuracy:0.3990\n",
    "Epoch #52: Loss:1.0732, Accuracy:0.3889, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #53: Loss:1.0729, Accuracy:0.4053, Validation Loss:1.0775, Validation Accuracy:0.3974\n",
    "Epoch #54: Loss:1.0728, Accuracy:0.4012, Validation Loss:1.0775, Validation Accuracy:0.3924\n",
    "Epoch #55: Loss:1.0730, Accuracy:0.4008, Validation Loss:1.0773, Validation Accuracy:0.3875\n",
    "Epoch #56: Loss:1.0732, Accuracy:0.4107, Validation Loss:1.0773, Validation Accuracy:0.3892\n",
    "Epoch #57: Loss:1.0727, Accuracy:0.4033, Validation Loss:1.0772, Validation Accuracy:0.3924\n",
    "Epoch #58: Loss:1.0725, Accuracy:0.4078, Validation Loss:1.0771, Validation Accuracy:0.3941\n",
    "Epoch #59: Loss:1.0725, Accuracy:0.4033, Validation Loss:1.0770, Validation Accuracy:0.3957\n",
    "Epoch #60: Loss:1.0727, Accuracy:0.4066, Validation Loss:1.0769, Validation Accuracy:0.3924\n",
    "Epoch #61: Loss:1.0728, Accuracy:0.4057, Validation Loss:1.0772, Validation Accuracy:0.3957\n",
    "Epoch #62: Loss:1.0728, Accuracy:0.4025, Validation Loss:1.0771, Validation Accuracy:0.3892\n",
    "Epoch #63: Loss:1.0724, Accuracy:0.4078, Validation Loss:1.0770, Validation Accuracy:0.3924\n",
    "Epoch #64: Loss:1.0725, Accuracy:0.4070, Validation Loss:1.0770, Validation Accuracy:0.3908\n",
    "Epoch #65: Loss:1.0725, Accuracy:0.4082, Validation Loss:1.0768, Validation Accuracy:0.3908\n",
    "Epoch #66: Loss:1.0722, Accuracy:0.4041, Validation Loss:1.0770, Validation Accuracy:0.3924\n",
    "Epoch #67: Loss:1.0722, Accuracy:0.4074, Validation Loss:1.0768, Validation Accuracy:0.3892\n",
    "Epoch #68: Loss:1.0721, Accuracy:0.4078, Validation Loss:1.0765, Validation Accuracy:0.3941\n",
    "Epoch #69: Loss:1.0720, Accuracy:0.4057, Validation Loss:1.0765, Validation Accuracy:0.3908\n",
    "Epoch #70: Loss:1.0718, Accuracy:0.4041, Validation Loss:1.0766, Validation Accuracy:0.4286\n",
    "Epoch #71: Loss:1.0718, Accuracy:0.4127, Validation Loss:1.0764, Validation Accuracy:0.4023\n",
    "Epoch #72: Loss:1.0716, Accuracy:0.4074, Validation Loss:1.0764, Validation Accuracy:0.3859\n",
    "Epoch #73: Loss:1.0712, Accuracy:0.4057, Validation Loss:1.0763, Validation Accuracy:0.3974\n",
    "Epoch #74: Loss:1.0714, Accuracy:0.4185, Validation Loss:1.0769, Validation Accuracy:0.4204\n",
    "Epoch #75: Loss:1.0716, Accuracy:0.4160, Validation Loss:1.0776, Validation Accuracy:0.3826\n",
    "Epoch #76: Loss:1.0713, Accuracy:0.4078, Validation Loss:1.0769, Validation Accuracy:0.4154\n",
    "Epoch #77: Loss:1.0717, Accuracy:0.4111, Validation Loss:1.0765, Validation Accuracy:0.4122\n",
    "Epoch #78: Loss:1.0724, Accuracy:0.4016, Validation Loss:1.0755, Validation Accuracy:0.4171\n",
    "Epoch #79: Loss:1.0722, Accuracy:0.4090, Validation Loss:1.0766, Validation Accuracy:0.4253\n",
    "Epoch #80: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0771, Validation Accuracy:0.4138\n",
    "Epoch #81: Loss:1.0714, Accuracy:0.4107, Validation Loss:1.0784, Validation Accuracy:0.4023\n",
    "Epoch #82: Loss:1.0720, Accuracy:0.3984, Validation Loss:1.0771, Validation Accuracy:0.4138\n",
    "Epoch #83: Loss:1.0715, Accuracy:0.4057, Validation Loss:1.0771, Validation Accuracy:0.3974\n",
    "Epoch #84: Loss:1.0713, Accuracy:0.4053, Validation Loss:1.0782, Validation Accuracy:0.3941\n",
    "Epoch #85: Loss:1.0709, Accuracy:0.4103, Validation Loss:1.0776, Validation Accuracy:0.3941\n",
    "Epoch #86: Loss:1.0706, Accuracy:0.4136, Validation Loss:1.0775, Validation Accuracy:0.3892\n",
    "Epoch #87: Loss:1.0714, Accuracy:0.4123, Validation Loss:1.0775, Validation Accuracy:0.3908\n",
    "Epoch #88: Loss:1.0722, Accuracy:0.4140, Validation Loss:1.0778, Validation Accuracy:0.3908\n",
    "Epoch #89: Loss:1.0710, Accuracy:0.4164, Validation Loss:1.0772, Validation Accuracy:0.3892\n",
    "Epoch #90: Loss:1.0713, Accuracy:0.4029, Validation Loss:1.0777, Validation Accuracy:0.3990\n",
    "Epoch #91: Loss:1.0709, Accuracy:0.4041, Validation Loss:1.0776, Validation Accuracy:0.3875\n",
    "Epoch #92: Loss:1.0708, Accuracy:0.4131, Validation Loss:1.0777, Validation Accuracy:0.3941\n",
    "Epoch #93: Loss:1.0711, Accuracy:0.4131, Validation Loss:1.0782, Validation Accuracy:0.3892\n",
    "Epoch #94: Loss:1.0707, Accuracy:0.4103, Validation Loss:1.0784, Validation Accuracy:0.3875\n",
    "Epoch #95: Loss:1.0710, Accuracy:0.4152, Validation Loss:1.0785, Validation Accuracy:0.3957\n",
    "Epoch #96: Loss:1.0713, Accuracy:0.4123, Validation Loss:1.0776, Validation Accuracy:0.3990\n",
    "Epoch #97: Loss:1.0707, Accuracy:0.4099, Validation Loss:1.0773, Validation Accuracy:0.3924\n",
    "Epoch #98: Loss:1.0714, Accuracy:0.4057, Validation Loss:1.0781, Validation Accuracy:0.3826\n",
    "Epoch #99: Loss:1.0708, Accuracy:0.4099, Validation Loss:1.0785, Validation Accuracy:0.3974\n",
    "Epoch #100: Loss:1.0715, Accuracy:0.4025, Validation Loss:1.0786, Validation Accuracy:0.3941\n",
    "Epoch #101: Loss:1.0706, Accuracy:0.4201, Validation Loss:1.0788, Validation Accuracy:0.3810\n",
    "Epoch #102: Loss:1.0706, Accuracy:0.4111, Validation Loss:1.0783, Validation Accuracy:0.3908\n",
    "Epoch #103: Loss:1.0706, Accuracy:0.4111, Validation Loss:1.0787, Validation Accuracy:0.3875\n",
    "Epoch #104: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0789, Validation Accuracy:0.3941\n",
    "Epoch #105: Loss:1.0708, Accuracy:0.4131, Validation Loss:1.0785, Validation Accuracy:0.3826\n",
    "Epoch #106: Loss:1.0707, Accuracy:0.4090, Validation Loss:1.0779, Validation Accuracy:0.3892\n",
    "Epoch #107: Loss:1.0707, Accuracy:0.4103, Validation Loss:1.0788, Validation Accuracy:0.3793\n",
    "Epoch #108: Loss:1.0708, Accuracy:0.4045, Validation Loss:1.0792, Validation Accuracy:0.3859\n",
    "Epoch #109: Loss:1.0704, Accuracy:0.4049, Validation Loss:1.0795, Validation Accuracy:0.3793\n",
    "Epoch #110: Loss:1.0700, Accuracy:0.4041, Validation Loss:1.0796, Validation Accuracy:0.3957\n",
    "Epoch #111: Loss:1.0705, Accuracy:0.4119, Validation Loss:1.0797, Validation Accuracy:0.3974\n",
    "Epoch #112: Loss:1.0701, Accuracy:0.4090, Validation Loss:1.0801, Validation Accuracy:0.3711\n",
    "Epoch #113: Loss:1.0705, Accuracy:0.4021, Validation Loss:1.0803, Validation Accuracy:0.3695\n",
    "Epoch #114: Loss:1.0702, Accuracy:0.4127, Validation Loss:1.0802, Validation Accuracy:0.3908\n",
    "Epoch #115: Loss:1.0709, Accuracy:0.4053, Validation Loss:1.0806, Validation Accuracy:0.3842\n",
    "Epoch #116: Loss:1.0701, Accuracy:0.4086, Validation Loss:1.0801, Validation Accuracy:0.3678\n",
    "Epoch #117: Loss:1.0701, Accuracy:0.4099, Validation Loss:1.0796, Validation Accuracy:0.3678\n",
    "Epoch #118: Loss:1.0709, Accuracy:0.4062, Validation Loss:1.0794, Validation Accuracy:0.3842\n",
    "Epoch #119: Loss:1.0706, Accuracy:0.4115, Validation Loss:1.0805, Validation Accuracy:0.3826\n",
    "Epoch #120: Loss:1.0703, Accuracy:0.4078, Validation Loss:1.0793, Validation Accuracy:0.3842\n",
    "Epoch #121: Loss:1.0692, Accuracy:0.4099, Validation Loss:1.0797, Validation Accuracy:0.3924\n",
    "Epoch #122: Loss:1.0699, Accuracy:0.4099, Validation Loss:1.0797, Validation Accuracy:0.3875\n",
    "Epoch #123: Loss:1.0710, Accuracy:0.4041, Validation Loss:1.0801, Validation Accuracy:0.3777\n",
    "Epoch #124: Loss:1.0700, Accuracy:0.4136, Validation Loss:1.0804, Validation Accuracy:0.3826\n",
    "Epoch #125: Loss:1.0701, Accuracy:0.4008, Validation Loss:1.0807, Validation Accuracy:0.3727\n",
    "Epoch #126: Loss:1.0698, Accuracy:0.4119, Validation Loss:1.0809, Validation Accuracy:0.3810\n",
    "Epoch #127: Loss:1.0697, Accuracy:0.4115, Validation Loss:1.0810, Validation Accuracy:0.3810\n",
    "Epoch #128: Loss:1.0703, Accuracy:0.4049, Validation Loss:1.0811, Validation Accuracy:0.3842\n",
    "Epoch #129: Loss:1.0699, Accuracy:0.4119, Validation Loss:1.0809, Validation Accuracy:0.3744\n",
    "Epoch #130: Loss:1.0706, Accuracy:0.4090, Validation Loss:1.0802, Validation Accuracy:0.3711\n",
    "Epoch #131: Loss:1.0700, Accuracy:0.4107, Validation Loss:1.0808, Validation Accuracy:0.3810\n",
    "Epoch #132: Loss:1.0696, Accuracy:0.4131, Validation Loss:1.0805, Validation Accuracy:0.3744\n",
    "Epoch #133: Loss:1.0690, Accuracy:0.4148, Validation Loss:1.0800, Validation Accuracy:0.3727\n",
    "Epoch #134: Loss:1.0692, Accuracy:0.4123, Validation Loss:1.0801, Validation Accuracy:0.3859\n",
    "Epoch #135: Loss:1.0692, Accuracy:0.4115, Validation Loss:1.0805, Validation Accuracy:0.3859\n",
    "Epoch #136: Loss:1.0692, Accuracy:0.4164, Validation Loss:1.0808, Validation Accuracy:0.3941\n",
    "Epoch #137: Loss:1.0694, Accuracy:0.4094, Validation Loss:1.0804, Validation Accuracy:0.3842\n",
    "Epoch #138: Loss:1.0690, Accuracy:0.4094, Validation Loss:1.0809, Validation Accuracy:0.3842\n",
    "Epoch #139: Loss:1.0693, Accuracy:0.4144, Validation Loss:1.0807, Validation Accuracy:0.3842\n",
    "Epoch #140: Loss:1.0695, Accuracy:0.4152, Validation Loss:1.0805, Validation Accuracy:0.3842\n",
    "Epoch #141: Loss:1.0692, Accuracy:0.4156, Validation Loss:1.0795, Validation Accuracy:0.3924\n",
    "Epoch #142: Loss:1.0692, Accuracy:0.4140, Validation Loss:1.0800, Validation Accuracy:0.3957\n",
    "Epoch #143: Loss:1.0688, Accuracy:0.4144, Validation Loss:1.0799, Validation Accuracy:0.3760\n",
    "Epoch #144: Loss:1.0690, Accuracy:0.4090, Validation Loss:1.0801, Validation Accuracy:0.3727\n",
    "Epoch #145: Loss:1.0683, Accuracy:0.4168, Validation Loss:1.0802, Validation Accuracy:0.3974\n",
    "Epoch #146: Loss:1.0693, Accuracy:0.4107, Validation Loss:1.0804, Validation Accuracy:0.3875\n",
    "Epoch #147: Loss:1.0687, Accuracy:0.4127, Validation Loss:1.0802, Validation Accuracy:0.3793\n",
    "Epoch #148: Loss:1.0689, Accuracy:0.4037, Validation Loss:1.0797, Validation Accuracy:0.3957\n",
    "Epoch #149: Loss:1.0685, Accuracy:0.4144, Validation Loss:1.0800, Validation Accuracy:0.3990\n",
    "Epoch #150: Loss:1.0685, Accuracy:0.4115, Validation Loss:1.0804, Validation Accuracy:0.3711\n",
    "Epoch #151: Loss:1.0681, Accuracy:0.4123, Validation Loss:1.0813, Validation Accuracy:0.3875\n",
    "Epoch #152: Loss:1.0700, Accuracy:0.4177, Validation Loss:1.0819, Validation Accuracy:0.3859\n",
    "Epoch #153: Loss:1.0693, Accuracy:0.4136, Validation Loss:1.0818, Validation Accuracy:0.3842\n",
    "Epoch #154: Loss:1.0702, Accuracy:0.4103, Validation Loss:1.0812, Validation Accuracy:0.3859\n",
    "Epoch #155: Loss:1.0684, Accuracy:0.4172, Validation Loss:1.0821, Validation Accuracy:0.3777\n",
    "Epoch #156: Loss:1.0688, Accuracy:0.4094, Validation Loss:1.0807, Validation Accuracy:0.3859\n",
    "Epoch #157: Loss:1.0693, Accuracy:0.4115, Validation Loss:1.0801, Validation Accuracy:0.3744\n",
    "Epoch #158: Loss:1.0686, Accuracy:0.4156, Validation Loss:1.0801, Validation Accuracy:0.3859\n",
    "Epoch #159: Loss:1.0688, Accuracy:0.4136, Validation Loss:1.0792, Validation Accuracy:0.3875\n",
    "Epoch #160: Loss:1.0688, Accuracy:0.4164, Validation Loss:1.0794, Validation Accuracy:0.3859\n",
    "Epoch #161: Loss:1.0689, Accuracy:0.4185, Validation Loss:1.0808, Validation Accuracy:0.3826\n",
    "Epoch #162: Loss:1.0696, Accuracy:0.4144, Validation Loss:1.0802, Validation Accuracy:0.3842\n",
    "Epoch #163: Loss:1.0680, Accuracy:0.4172, Validation Loss:1.0805, Validation Accuracy:0.3793\n",
    "Epoch #164: Loss:1.0693, Accuracy:0.4160, Validation Loss:1.0810, Validation Accuracy:0.3826\n",
    "Epoch #165: Loss:1.0684, Accuracy:0.4230, Validation Loss:1.0801, Validation Accuracy:0.3727\n",
    "Epoch #166: Loss:1.0687, Accuracy:0.4144, Validation Loss:1.0796, Validation Accuracy:0.3810\n",
    "Epoch #167: Loss:1.0688, Accuracy:0.4148, Validation Loss:1.0807, Validation Accuracy:0.3760\n",
    "Epoch #168: Loss:1.0691, Accuracy:0.4115, Validation Loss:1.0804, Validation Accuracy:0.3711\n",
    "Epoch #169: Loss:1.0682, Accuracy:0.4160, Validation Loss:1.0805, Validation Accuracy:0.3826\n",
    "Epoch #170: Loss:1.0682, Accuracy:0.4172, Validation Loss:1.0806, Validation Accuracy:0.3826\n",
    "Epoch #171: Loss:1.0682, Accuracy:0.4144, Validation Loss:1.0801, Validation Accuracy:0.3842\n",
    "Epoch #172: Loss:1.0678, Accuracy:0.4172, Validation Loss:1.0802, Validation Accuracy:0.3826\n",
    "Epoch #173: Loss:1.0677, Accuracy:0.4164, Validation Loss:1.0803, Validation Accuracy:0.3760\n",
    "Epoch #174: Loss:1.0676, Accuracy:0.4164, Validation Loss:1.0806, Validation Accuracy:0.3760\n",
    "Epoch #175: Loss:1.0679, Accuracy:0.4123, Validation Loss:1.0805, Validation Accuracy:0.3859\n",
    "Epoch #176: Loss:1.0677, Accuracy:0.4144, Validation Loss:1.0807, Validation Accuracy:0.3744\n",
    "Epoch #177: Loss:1.0674, Accuracy:0.4156, Validation Loss:1.0811, Validation Accuracy:0.3760\n",
    "Epoch #178: Loss:1.0680, Accuracy:0.4177, Validation Loss:1.0813, Validation Accuracy:0.3777\n",
    "Epoch #179: Loss:1.0682, Accuracy:0.4201, Validation Loss:1.0810, Validation Accuracy:0.3760\n",
    "Epoch #180: Loss:1.0680, Accuracy:0.4152, Validation Loss:1.0814, Validation Accuracy:0.3760\n",
    "Epoch #181: Loss:1.0688, Accuracy:0.4111, Validation Loss:1.0810, Validation Accuracy:0.3695\n",
    "Epoch #182: Loss:1.0678, Accuracy:0.4119, Validation Loss:1.0818, Validation Accuracy:0.3793\n",
    "Epoch #183: Loss:1.0681, Accuracy:0.4185, Validation Loss:1.0806, Validation Accuracy:0.3859\n",
    "Epoch #184: Loss:1.0681, Accuracy:0.4160, Validation Loss:1.0805, Validation Accuracy:0.3810\n",
    "Epoch #185: Loss:1.0670, Accuracy:0.4205, Validation Loss:1.0812, Validation Accuracy:0.3760\n",
    "Epoch #186: Loss:1.0688, Accuracy:0.4218, Validation Loss:1.0810, Validation Accuracy:0.3777\n",
    "Epoch #187: Loss:1.0669, Accuracy:0.4152, Validation Loss:1.0808, Validation Accuracy:0.3678\n",
    "Epoch #188: Loss:1.0683, Accuracy:0.4144, Validation Loss:1.0805, Validation Accuracy:0.3760\n",
    "Epoch #189: Loss:1.0682, Accuracy:0.4115, Validation Loss:1.0814, Validation Accuracy:0.3793\n",
    "Epoch #190: Loss:1.0687, Accuracy:0.4136, Validation Loss:1.0810, Validation Accuracy:0.3695\n",
    "Epoch #191: Loss:1.0672, Accuracy:0.4152, Validation Loss:1.0807, Validation Accuracy:0.3760\n",
    "Epoch #192: Loss:1.0671, Accuracy:0.4160, Validation Loss:1.0815, Validation Accuracy:0.3777\n",
    "Epoch #193: Loss:1.0671, Accuracy:0.4172, Validation Loss:1.0801, Validation Accuracy:0.3744\n",
    "Epoch #194: Loss:1.0674, Accuracy:0.4168, Validation Loss:1.0803, Validation Accuracy:0.3842\n",
    "Epoch #195: Loss:1.0676, Accuracy:0.4123, Validation Loss:1.0813, Validation Accuracy:0.3744\n",
    "Epoch #196: Loss:1.0670, Accuracy:0.4156, Validation Loss:1.0803, Validation Accuracy:0.3826\n",
    "Epoch #197: Loss:1.0672, Accuracy:0.4226, Validation Loss:1.0809, Validation Accuracy:0.3744\n",
    "Epoch #198: Loss:1.0670, Accuracy:0.4168, Validation Loss:1.0808, Validation Accuracy:0.3760\n",
    "Epoch #199: Loss:1.0669, Accuracy:0.4181, Validation Loss:1.0808, Validation Accuracy:0.3760\n",
    "Epoch #200: Loss:1.0670, Accuracy:0.4177, Validation Loss:1.0814, Validation Accuracy:0.3744\n",
    "Epoch #201: Loss:1.0665, Accuracy:0.4193, Validation Loss:1.0822, Validation Accuracy:0.3760\n",
    "Epoch #202: Loss:1.0672, Accuracy:0.4172, Validation Loss:1.0813, Validation Accuracy:0.3744\n",
    "Epoch #203: Loss:1.0665, Accuracy:0.4136, Validation Loss:1.0813, Validation Accuracy:0.3842\n",
    "Epoch #204: Loss:1.0668, Accuracy:0.4152, Validation Loss:1.0814, Validation Accuracy:0.3793\n",
    "Epoch #205: Loss:1.0682, Accuracy:0.4193, Validation Loss:1.0817, Validation Accuracy:0.3760\n",
    "Epoch #206: Loss:1.0676, Accuracy:0.4123, Validation Loss:1.0816, Validation Accuracy:0.3678\n",
    "Epoch #207: Loss:1.0677, Accuracy:0.4251, Validation Loss:1.0829, Validation Accuracy:0.3760\n",
    "Epoch #208: Loss:1.0671, Accuracy:0.4197, Validation Loss:1.0802, Validation Accuracy:0.3842\n",
    "Epoch #209: Loss:1.0667, Accuracy:0.4201, Validation Loss:1.0805, Validation Accuracy:0.3760\n",
    "Epoch #210: Loss:1.0672, Accuracy:0.4189, Validation Loss:1.0822, Validation Accuracy:0.3711\n",
    "Epoch #211: Loss:1.0673, Accuracy:0.4082, Validation Loss:1.0804, Validation Accuracy:0.3826\n",
    "Epoch #212: Loss:1.0666, Accuracy:0.4214, Validation Loss:1.0812, Validation Accuracy:0.3711\n",
    "Epoch #213: Loss:1.0672, Accuracy:0.4197, Validation Loss:1.0805, Validation Accuracy:0.3793\n",
    "Epoch #214: Loss:1.0663, Accuracy:0.4136, Validation Loss:1.0805, Validation Accuracy:0.3711\n",
    "Epoch #215: Loss:1.0667, Accuracy:0.4156, Validation Loss:1.0804, Validation Accuracy:0.3793\n",
    "Epoch #216: Loss:1.0665, Accuracy:0.4168, Validation Loss:1.0813, Validation Accuracy:0.3711\n",
    "Epoch #217: Loss:1.0672, Accuracy:0.4136, Validation Loss:1.0807, Validation Accuracy:0.3760\n",
    "Epoch #218: Loss:1.0668, Accuracy:0.4222, Validation Loss:1.0825, Validation Accuracy:0.3678\n",
    "Epoch #219: Loss:1.0674, Accuracy:0.4164, Validation Loss:1.0806, Validation Accuracy:0.3760\n",
    "Epoch #220: Loss:1.0665, Accuracy:0.4168, Validation Loss:1.0817, Validation Accuracy:0.3760\n",
    "Epoch #221: Loss:1.0667, Accuracy:0.4136, Validation Loss:1.0812, Validation Accuracy:0.3760\n",
    "Epoch #222: Loss:1.0664, Accuracy:0.4185, Validation Loss:1.0820, Validation Accuracy:0.3678\n",
    "Epoch #223: Loss:1.0670, Accuracy:0.4185, Validation Loss:1.0809, Validation Accuracy:0.3760\n",
    "Epoch #224: Loss:1.0663, Accuracy:0.4193, Validation Loss:1.0823, Validation Accuracy:0.3711\n",
    "Epoch #225: Loss:1.0661, Accuracy:0.4201, Validation Loss:1.0826, Validation Accuracy:0.3678\n",
    "Epoch #226: Loss:1.0668, Accuracy:0.4160, Validation Loss:1.0807, Validation Accuracy:0.3760\n",
    "Epoch #227: Loss:1.0660, Accuracy:0.4193, Validation Loss:1.0813, Validation Accuracy:0.3744\n",
    "Epoch #228: Loss:1.0668, Accuracy:0.4205, Validation Loss:1.0817, Validation Accuracy:0.3793\n",
    "Epoch #229: Loss:1.0659, Accuracy:0.4172, Validation Loss:1.0812, Validation Accuracy:0.3744\n",
    "Epoch #230: Loss:1.0666, Accuracy:0.4131, Validation Loss:1.0811, Validation Accuracy:0.3810\n",
    "Epoch #231: Loss:1.0662, Accuracy:0.4164, Validation Loss:1.0804, Validation Accuracy:0.3760\n",
    "Epoch #232: Loss:1.0664, Accuracy:0.4160, Validation Loss:1.0822, Validation Accuracy:0.3711\n",
    "Epoch #233: Loss:1.0664, Accuracy:0.4189, Validation Loss:1.0817, Validation Accuracy:0.3760\n",
    "Epoch #234: Loss:1.0659, Accuracy:0.4164, Validation Loss:1.0808, Validation Accuracy:0.3744\n",
    "Epoch #235: Loss:1.0659, Accuracy:0.4152, Validation Loss:1.0813, Validation Accuracy:0.3695\n",
    "Epoch #236: Loss:1.0657, Accuracy:0.4152, Validation Loss:1.0812, Validation Accuracy:0.3760\n",
    "Epoch #237: Loss:1.0659, Accuracy:0.4168, Validation Loss:1.0822, Validation Accuracy:0.3711\n",
    "Epoch #238: Loss:1.0659, Accuracy:0.4172, Validation Loss:1.0815, Validation Accuracy:0.3711\n",
    "Epoch #239: Loss:1.0657, Accuracy:0.4181, Validation Loss:1.0808, Validation Accuracy:0.3760\n",
    "Epoch #240: Loss:1.0661, Accuracy:0.4136, Validation Loss:1.0815, Validation Accuracy:0.3711\n",
    "Epoch #241: Loss:1.0662, Accuracy:0.4185, Validation Loss:1.0818, Validation Accuracy:0.3662\n",
    "Epoch #242: Loss:1.0657, Accuracy:0.4156, Validation Loss:1.0813, Validation Accuracy:0.3760\n",
    "Epoch #243: Loss:1.0655, Accuracy:0.4160, Validation Loss:1.0805, Validation Accuracy:0.3760\n",
    "Epoch #244: Loss:1.0659, Accuracy:0.4189, Validation Loss:1.0813, Validation Accuracy:0.3662\n",
    "Epoch #245: Loss:1.0653, Accuracy:0.4197, Validation Loss:1.0811, Validation Accuracy:0.3760\n",
    "Epoch #246: Loss:1.0655, Accuracy:0.4160, Validation Loss:1.0818, Validation Accuracy:0.3760\n",
    "Epoch #247: Loss:1.0655, Accuracy:0.4160, Validation Loss:1.0816, Validation Accuracy:0.3711\n",
    "Epoch #248: Loss:1.0653, Accuracy:0.4177, Validation Loss:1.0820, Validation Accuracy:0.3810\n",
    "Epoch #249: Loss:1.0654, Accuracy:0.4152, Validation Loss:1.0811, Validation Accuracy:0.3760\n",
    "Epoch #250: Loss:1.0658, Accuracy:0.4148, Validation Loss:1.0816, Validation Accuracy:0.3760\n",
    "Epoch #251: Loss:1.0655, Accuracy:0.4148, Validation Loss:1.0807, Validation Accuracy:0.3760\n",
    "Epoch #252: Loss:1.0659, Accuracy:0.4177, Validation Loss:1.0816, Validation Accuracy:0.3711\n",
    "Epoch #253: Loss:1.0654, Accuracy:0.4115, Validation Loss:1.0813, Validation Accuracy:0.3760\n",
    "Epoch #254: Loss:1.0648, Accuracy:0.4197, Validation Loss:1.0832, Validation Accuracy:0.3744\n",
    "Epoch #255: Loss:1.0658, Accuracy:0.4214, Validation Loss:1.0812, Validation Accuracy:0.3711\n",
    "Epoch #256: Loss:1.0651, Accuracy:0.4181, Validation Loss:1.0812, Validation Accuracy:0.3760\n",
    "Epoch #257: Loss:1.0662, Accuracy:0.4107, Validation Loss:1.0809, Validation Accuracy:0.3727\n",
    "Epoch #258: Loss:1.0654, Accuracy:0.4140, Validation Loss:1.0816, Validation Accuracy:0.3826\n",
    "Epoch #259: Loss:1.0650, Accuracy:0.4156, Validation Loss:1.0810, Validation Accuracy:0.3760\n",
    "Epoch #260: Loss:1.0652, Accuracy:0.4127, Validation Loss:1.0809, Validation Accuracy:0.3760\n",
    "Epoch #261: Loss:1.0649, Accuracy:0.4177, Validation Loss:1.0816, Validation Accuracy:0.3810\n",
    "Epoch #262: Loss:1.0656, Accuracy:0.4197, Validation Loss:1.0803, Validation Accuracy:0.3727\n",
    "Epoch #263: Loss:1.0665, Accuracy:0.4078, Validation Loss:1.0814, Validation Accuracy:0.3760\n",
    "Epoch #264: Loss:1.0655, Accuracy:0.4168, Validation Loss:1.0824, Validation Accuracy:0.3793\n",
    "Epoch #265: Loss:1.0656, Accuracy:0.4107, Validation Loss:1.0796, Validation Accuracy:0.3629\n",
    "Epoch #266: Loss:1.0655, Accuracy:0.4082, Validation Loss:1.0809, Validation Accuracy:0.3793\n",
    "Epoch #267: Loss:1.0646, Accuracy:0.4172, Validation Loss:1.0804, Validation Accuracy:0.3760\n",
    "Epoch #268: Loss:1.0659, Accuracy:0.4107, Validation Loss:1.0804, Validation Accuracy:0.3629\n",
    "Epoch #269: Loss:1.0665, Accuracy:0.4148, Validation Loss:1.0817, Validation Accuracy:0.3875\n",
    "Epoch #270: Loss:1.0648, Accuracy:0.4234, Validation Loss:1.0796, Validation Accuracy:0.3695\n",
    "Epoch #271: Loss:1.0650, Accuracy:0.4131, Validation Loss:1.0798, Validation Accuracy:0.3695\n",
    "Epoch #272: Loss:1.0649, Accuracy:0.4214, Validation Loss:1.0801, Validation Accuracy:0.3711\n",
    "Epoch #273: Loss:1.0650, Accuracy:0.4078, Validation Loss:1.0799, Validation Accuracy:0.3727\n",
    "Epoch #274: Loss:1.0643, Accuracy:0.4177, Validation Loss:1.0802, Validation Accuracy:0.3711\n",
    "Epoch #275: Loss:1.0645, Accuracy:0.4201, Validation Loss:1.0798, Validation Accuracy:0.3711\n",
    "Epoch #276: Loss:1.0641, Accuracy:0.4189, Validation Loss:1.0800, Validation Accuracy:0.3727\n",
    "Epoch #277: Loss:1.0648, Accuracy:0.4189, Validation Loss:1.0806, Validation Accuracy:0.3727\n",
    "Epoch #278: Loss:1.0650, Accuracy:0.4168, Validation Loss:1.0798, Validation Accuracy:0.3760\n",
    "Epoch #279: Loss:1.0646, Accuracy:0.4193, Validation Loss:1.0803, Validation Accuracy:0.3711\n",
    "Epoch #280: Loss:1.0644, Accuracy:0.4197, Validation Loss:1.0804, Validation Accuracy:0.3727\n",
    "Epoch #281: Loss:1.0639, Accuracy:0.4164, Validation Loss:1.0806, Validation Accuracy:0.3727\n",
    "Epoch #282: Loss:1.0648, Accuracy:0.4152, Validation Loss:1.0795, Validation Accuracy:0.3711\n",
    "Epoch #283: Loss:1.0643, Accuracy:0.4201, Validation Loss:1.0794, Validation Accuracy:0.3727\n",
    "Epoch #284: Loss:1.0640, Accuracy:0.4193, Validation Loss:1.0808, Validation Accuracy:0.3727\n",
    "Epoch #285: Loss:1.0642, Accuracy:0.4172, Validation Loss:1.0810, Validation Accuracy:0.3842\n",
    "Epoch #286: Loss:1.0640, Accuracy:0.4185, Validation Loss:1.0803, Validation Accuracy:0.3727\n",
    "Epoch #287: Loss:1.0644, Accuracy:0.4246, Validation Loss:1.0807, Validation Accuracy:0.3777\n",
    "Epoch #288: Loss:1.0636, Accuracy:0.4189, Validation Loss:1.0804, Validation Accuracy:0.3810\n",
    "Epoch #289: Loss:1.0636, Accuracy:0.4218, Validation Loss:1.0798, Validation Accuracy:0.3810\n",
    "Epoch #290: Loss:1.0637, Accuracy:0.4214, Validation Loss:1.0789, Validation Accuracy:0.3727\n",
    "Epoch #291: Loss:1.0636, Accuracy:0.4234, Validation Loss:1.0784, Validation Accuracy:0.3777\n",
    "Epoch #292: Loss:1.0636, Accuracy:0.4197, Validation Loss:1.0793, Validation Accuracy:0.3924\n",
    "Epoch #293: Loss:1.0640, Accuracy:0.4238, Validation Loss:1.0798, Validation Accuracy:0.3777\n",
    "Epoch #294: Loss:1.0636, Accuracy:0.4193, Validation Loss:1.0792, Validation Accuracy:0.3777\n",
    "Epoch #295: Loss:1.0635, Accuracy:0.4234, Validation Loss:1.0801, Validation Accuracy:0.3760\n",
    "Epoch #296: Loss:1.0634, Accuracy:0.4238, Validation Loss:1.0795, Validation Accuracy:0.3908\n",
    "Epoch #297: Loss:1.0632, Accuracy:0.4205, Validation Loss:1.0800, Validation Accuracy:0.3908\n",
    "Epoch #298: Loss:1.0635, Accuracy:0.4177, Validation Loss:1.0803, Validation Accuracy:0.3859\n",
    "Epoch #299: Loss:1.0634, Accuracy:0.4218, Validation Loss:1.0794, Validation Accuracy:0.3859\n",
    "Epoch #300: Loss:1.0632, Accuracy:0.4197, Validation Loss:1.0791, Validation Accuracy:0.3760\n",
    "\n",
    "Test:\n",
    "Test Loss:1.07910025, Accuracy:0.3760\n",
    "Labels: ['02', '03', '01']\n",
    "Confusion Matrix:\n",
    "      02  03   01\n",
    "t:02  83   0  144\n",
    "t:03  54   0   88\n",
    "t:01  94   0  146\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.36      0.37      0.36       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "          01       0.39      0.61      0.47       240\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.25      0.32      0.28       609\n",
    "weighted avg       0.29      0.38      0.32       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 07:59:56 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 47 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0910702605161369, 1.079677515624975, 1.074710658618382, 1.074548348417423, 1.0753528897593958, 1.076067660242466, 1.0750372407863098, 1.0746551544599736, 1.0752861854086564, 1.0756740311683692, 1.0757853374105368, 1.075787667178951, 1.075748295815316, 1.0760444490780383, 1.076062964688381, 1.0758474086501524, 1.0761451564594637, 1.0762943407193388, 1.0763432803412376, 1.0763829768389122, 1.076561499698996, 1.0755849094030696, 1.0758663055736248, 1.075984230769679, 1.0759912102876235, 1.0762997125757152, 1.0763494894030843, 1.0765060779496367, 1.0764277917019447, 1.0766425138623843, 1.0766216729857847, 1.076913609293294, 1.0770599785305204, 1.077261624665096, 1.077089469616832, 1.0766416490567337, 1.0765930760670177, 1.0767207954121731, 1.0769965695630153, 1.0768095074811788, 1.0768990849430728, 1.0767525598920624, 1.0766985069942006, 1.0766949759328306, 1.0768980394639014, 1.0768411825051645, 1.0768816447610339, 1.0769562272994193, 1.0769433141341938, 1.076920952702978, 1.0768766070430111, 1.0770870593968283, 1.0775242102361469, 1.077509713486106, 1.077319777853579, 1.0772549875068351, 1.0771867117075302, 1.0770860303603174, 1.0769754317397946, 1.0769058205615516, 1.0771569271980248, 1.0771455134468517, 1.0769529884867677, 1.0769575969339005, 1.0768086790842768, 1.0769713240108272, 1.0767693942403558, 1.0764653717943014, 1.0764978395894242, 1.0765866045098391, 1.0763908726436946, 1.0764165094920568, 1.0762645810695703, 1.0768718715567502, 1.0775513004982609, 1.0769440428964023, 1.0764663751880914, 1.0755391757085013, 1.0766146292631653, 1.0770688282053655, 1.0783754728110553, 1.0770699782128796, 1.0771461016634611, 1.078245787784971, 1.077582806984975, 1.0774931046371585, 1.077477109451795, 1.077758744823913, 1.077223495701068, 1.0776817767295148, 1.0776177799368922, 1.0777002089520784, 1.078169559806047, 1.07839507990087, 1.0784810415433936, 1.0776313639235222, 1.0772688684400862, 1.0781409411594784, 1.0784620641683318, 1.0786434985538227, 1.0787947761209924, 1.078334293341989, 1.0786604276431606, 1.078921421408066, 1.0784635185608136, 1.0778589166443924, 1.0788483394582087, 1.0792369498016408, 1.0795048510499776, 1.079611360733145, 1.0796517634822427, 1.080090758835741, 1.080313772012056, 1.0801581191311915, 1.08060217510499, 1.0801106677658256, 1.0796447475555495, 1.0794314377022103, 1.0804836417262387, 1.0792790768768987, 1.0796582438479896, 1.0797259005028235, 1.0801438539486212, 1.0804137285119795, 1.0806845606645732, 1.0808675719795164, 1.0809649796712966, 1.0810543044251566, 1.0808502895687209, 1.0802283911477952, 1.0807814369060722, 1.0804553546733262, 1.079982822751764, 1.0800504862577065, 1.0805141733980728, 1.0807984121914567, 1.0804497416579273, 1.0808892214826762, 1.080731758343175, 1.0804959433810857, 1.07954209994017, 1.0799573252745254, 1.079895624582012, 1.08006096199424, 1.0801700842987336, 1.080351636914784, 1.0802294726442234, 1.0796786570196668, 1.0799762339427554, 1.0804480204637024, 1.0812721367931521, 1.0819020519898639, 1.0818095958878842, 1.0812164141822527, 1.0820883534029004, 1.0806796356962232, 1.0800834332389393, 1.0800881387760681, 1.0791704981589356, 1.0794185730819827, 1.0807632037571497, 1.0802050114460962, 1.080534631199829, 1.0809831903094338, 1.0801216471567139, 1.0796186234954934, 1.0806836747183588, 1.0804368396502215, 1.0804584441318106, 1.080566830431495, 1.080149228349695, 1.0802006654942955, 1.080306389257434, 1.0806302276542425, 1.0804507796987524, 1.0807256336478372, 1.0811121800458685, 1.0813369302718314, 1.0809750829032685, 1.081380555782412, 1.0810306624238715, 1.0818354169331943, 1.0805813951053838, 1.0805487047471045, 1.0811833295916102, 1.0809696059313119, 1.0807535272513704, 1.0804623349742546, 1.081386962351932, 1.0809705778100025, 1.0807116245009825, 1.0814678690507886, 1.0800569317806725, 1.0803491681667383, 1.0813406502280525, 1.080327161622948, 1.0809128685733564, 1.080837202581083, 1.0808145620161285, 1.0814209154673986, 1.0821604548612447, 1.081301915234533, 1.0813352337415973, 1.081400893396149, 1.0817059685639756, 1.0816125509578411, 1.0828799702263818, 1.0802332497582647, 1.0804855482918876, 1.0822423479239929, 1.080433633527145, 1.0811572660170556, 1.0804509067378805, 1.0805243050132087, 1.0804207002000856, 1.081339404892256, 1.0806513098856108, 1.0824723852483313, 1.0806450931896716, 1.081724285492169, 1.081234048348538, 1.0819542494117724, 1.0808535839732254, 1.0822530650152948, 1.0826310271700028, 1.0807339857364524, 1.0813019600603577, 1.0816822574643665, 1.0812199702991054, 1.0811041904787713, 1.0803651788160327, 1.082215516633784, 1.0817051795120114, 1.0808195488402017, 1.081313918768283, 1.0812379458463446, 1.0821599212577582, 1.0815172021220667, 1.08079805495508, 1.0815291966515026, 1.0818167359175157, 1.081256277455485, 1.0804629231908638, 1.0812969307594111, 1.0811124967628316, 1.0817726827020129, 1.0816235242806045, 1.0819900120028918, 1.081053868498904, 1.0815849188709104, 1.0806892892997253, 1.081616402651093, 1.0813239511402173, 1.0831545532630582, 1.081162969271342, 1.0811980179769456, 1.0808692931737414, 1.0816136075945324, 1.0810481807085486, 1.08093978540455, 1.0816110752290498, 1.080317170357665, 1.0813949470253805, 1.0823682403721049, 1.0795500495750916, 1.0809255104346815, 1.0804062355523822, 1.0804345226052947, 1.0817169847551042, 1.0795698504534066, 1.0798445141374184, 1.080147102352825, 1.0799154407285116, 1.080189829193704, 1.0797860338574363, 1.0799737364200537, 1.0806190965602356, 1.079799624695175, 1.0803011082271832, 1.0804393645773576, 1.0806143434568383, 1.0795342434803252, 1.079384192065848, 1.0808140614937092, 1.081010137285505, 1.08030049984874, 1.0806512748470838, 1.080365030244849, 1.0797726477699718, 1.0788838233070812, 1.0783769039097677, 1.0792868051231397, 1.0797660728589262, 1.0791686769385251, 1.0800731352397375, 1.0795378248484069, 1.0799922516389042, 1.0803350370701506, 1.0794056107845214, 1.07910021752951], 'val_acc': [0.372742199007122, 0.385878487907607, 0.3973727412705351, 0.3924466327982779, 0.3924466327982779, 0.4022988492534274, 0.40065681312863266, 0.41050903036676606, 0.4121510662958148, 0.4022988493513004, 0.4055829216008899, 0.4055829216008899, 0.40394088547609514, 0.3908045965756102, 0.3924466327004049, 0.40065681332437864, 0.41050903036676606, 0.41050903046463905, 0.40722495811717657, 0.4072249577256846, 0.4088669943398443, 0.40722495811717657, 0.4088669942419713, 0.41050903036676606, 0.3990147771017109, 0.40558292179663585, 0.4072249584107955, 0.41050903036676606, 0.40229884974279234, 0.40229884954704637, 0.4039408857697141, 0.3990147771995839, 0.41050903026889307, 0.40558292189450884, 0.39408866892307265, 0.39408866892307265, 0.3842364518806852, 0.3825944159516364, 0.379310343604174, 0.3858784882990989, 0.3842364520764312, 0.3924466327982779, 0.3908045965756102, 0.3875205242281477, 0.3990147776889488, 0.39244663260253193, 0.4055829215030169, 0.3908045963798642, 0.39244663250465894, 0.40229884954704637, 0.3990147774932028, 0.38916256035294244, 0.39737274146628104, 0.3924466329940238, 0.3875205242281477, 0.38916256074443434, 0.3924466330918968, 0.39408866892307265, 0.3957307049499944, 0.3924466327982779, 0.3957307053414863, 0.38916256045081543, 0.3924466327982779, 0.39080459667348316, 0.3908045967713561, 0.3924466327982779, 0.38916256045081543, 0.39408866892307265, 0.3908045965756102, 0.4285714274458893, 0.40229884954704637, 0.3858784882990989, 0.39737274107478915, 0.42036124652829665, 0.3825944157558905, 0.4154351381539124, 0.41215106610006885, 0.41707717427870716, 0.4252873552941728, 0.4137931022248636, 0.40229884905768143, 0.4137931023227366, 0.3973727412705351, 0.3940886685315807, 0.3940886685315807, 0.38916256015719647, 0.3908045964777372, 0.39080459667348316, 0.38916256035294244, 0.39901477690596493, 0.3875205241302747, 0.3940886686294537, 0.38916256025506946, 0.3875205244238936, 0.3957307048521214, 0.3990147771017109, 0.39244663240678596, 0.38259441585376347, 0.3973727408790432, 0.3940886687273267, 0.3809523797289687, 0.3908045962819912, 0.3875205241302747, 0.3940886687273267, 0.38259441585376347, 0.38916256025506946, 0.379310343604174, 0.385878487907607, 0.379310343604174, 0.39573070475424843, 0.3973727407811702, 0.3711001629802002, 0.3694581267575325, 0.3908045964777372, 0.38423645178281224, 0.3678160908284837, 0.3678160905348648, 0.3842364520764312, 0.3825944159516364, 0.38423645217430413, 0.3924466327982779, 0.3875205245217666, 0.37766830757725217, 0.3825944159516364, 0.3727421994964869, 0.38095237982684166, 0.3809523797289687, 0.3842364518806852, 0.3743842353276627, 0.3711001629802002, 0.38095237963109574, 0.3743842352297897, 0.37274219910499495, 0.38587848800547997, 0.38587848800547997, 0.3940886685315807, 0.3842364518806852, 0.3842364518806852, 0.38423645168493925, 0.38423645168493925, 0.39244663240678596, 0.39573070475424843, 0.37602627145245743, 0.37274219910499495, 0.3973727408790432, 0.3875205242281477, 0.3793103437020469, 0.39573070475424843, 0.3990147770038379, 0.3711001630780732, 0.38752052393452874, 0.385878487809734, 0.38423645168493925, 0.385878487809734, 0.37766830747937924, 0.385878487809734, 0.3743842352297897, 0.38587848800547997, 0.38752052393452874, 0.385878487809734, 0.3825944155601445, 0.38423645178281224, 0.3793103437020469, 0.3825944155601445, 0.37274219910499495, 0.3809523797289687, 0.3760262711588385, 0.3711001629802002, 0.3825944155601445, 0.3825944155601445, 0.38423645158706626, 0.3825944155601445, 0.37602627086521956, 0.37602627086521956, 0.385878487711861, 0.3743842350340438, 0.3760262711588385, 0.37766830728363326, 0.37602627096309255, 0.3760262711588385, 0.3694581267575325, 0.37931034321268203, 0.385878487711861, 0.3809523793374768, 0.3760262711588385, 0.37766830728363326, 0.36781609063273774, 0.37602627106096553, 0.37931034321268203, 0.3694581267575325, 0.37602627106096553, 0.3776683070878873, 0.3743842346425518, 0.38423645158706626, 0.3743842350340438, 0.3825944154622715, 0.3743842346425518, 0.37602627076734657, 0.37602627076734657, 0.3743842346425518, 0.37602627106096553, 0.3743842346425518, 0.38423645158706626, 0.379310343310555, 0.37602627106096553, 0.36781609063273774, 0.37602627106096553, 0.38423645158706626, 0.37602627096309255, 0.3711001625887083, 0.3825944154622715, 0.3711001625887083, 0.37931034311480905, 0.3711001628823273, 0.379310343310555, 0.3711001625887083, 0.37602627096309255, 0.3678160904369918, 0.37602627096309255, 0.37602627096309255, 0.37602627096309255, 0.3678160904369918, 0.37602627096309255, 0.3711001625887083, 0.3678160904369918, 0.37602627096309255, 0.3743842349361708, 0.379310343506301, 0.3743842346425518, 0.38095237943534976, 0.37602627096309255, 0.3711001625887083, 0.37602627096309255, 0.3743842346425518, 0.36945812646391357, 0.37602627096309255, 0.37110016239296234, 0.37110016239296234, 0.37602627096309255, 0.3711001627844543, 0.3661740543121971, 0.37602627096309255, 0.37602627096309255, 0.3661740543121971, 0.37602627096309255, 0.37602627096309255, 0.37110016239296234, 0.38095237943534976, 0.3760262712567115, 0.3760262712567115, 0.37602627096309255, 0.37110016239296234, 0.37602627096309255, 0.3743842348382978, 0.3711001627844543, 0.37602627096309255, 0.37274219871350306, 0.38259441536439853, 0.37602627096309255, 0.37602627096309255, 0.38095237943534976, 0.37274219871350306, 0.37602627096309255, 0.379310343310555, 0.3628899822583535, 0.379310343310555, 0.37602627096309255, 0.3628899822583535, 0.38752052383665575, 0.36945812665965955, 0.36945812646391357, 0.3711001627844543, 0.37274219871350306, 0.3711001627844543, 0.3711001627844543, 0.37274219871350306, 0.37274219871350306, 0.37602627096309255, 0.3711001627844543, 0.37274219871350306, 0.37274219871350306, 0.3711001627844543, 0.37274219871350306, 0.37274219871350306, 0.3842364513913203, 0.37274219871350306, 0.3776683069900143, 0.38095237904385787, 0.38095237904385787, 0.3727421985177571, 0.3776683069900143, 0.39244663211316705, 0.3776683069900143, 0.3776683069900143, 0.37602627076734657, 0.39080459598837225, 0.39080459598837225, 0.3858784875161151, 0.3858784875161151, 0.37602627086521956], 'loss': [1.1014768666065693, 1.0848583128907596, 1.0766427949713486, 1.0734709320127107, 1.0740468773264171, 1.0745120271764987, 1.0740747258403707, 1.0738225557965664, 1.0733613072969095, 1.073705683107004, 1.0738320063761373, 1.073789398821962, 1.073440792869004, 1.0734749559504295, 1.0732595129424296, 1.073528882952931, 1.0734369448323025, 1.0734714003803794, 1.0734576660994386, 1.0736458654521182, 1.0737683205633928, 1.073526149363978, 1.073827571056217, 1.0738600913993632, 1.0739146266385025, 1.0737336448575436, 1.0735785293872842, 1.0735965903045215, 1.073593639446235, 1.0734493238969995, 1.0736443012891608, 1.0731624385904237, 1.0732627593516324, 1.073635045997416, 1.0736325561143534, 1.073611320313487, 1.0734421550860396, 1.0734421845089483, 1.0733662681168354, 1.0733710557773128, 1.0733709347566296, 1.0733115715657415, 1.0733287103856612, 1.0732012295380264, 1.0733708294510107, 1.0730927905507646, 1.0729582118792211, 1.073122191331225, 1.0729828788024933, 1.0729454978045987, 1.072917341353712, 1.073158599953387, 1.0728623439643907, 1.0728287910289098, 1.0730218310620505, 1.073240386044465, 1.0726894816823564, 1.072531224374164, 1.072541278049931, 1.0726831286839635, 1.0727815001407444, 1.072836262734274, 1.0724495391336555, 1.072495094608722, 1.0724981051940448, 1.0721941435361544, 1.0721804384333398, 1.0721170440102015, 1.071971210268244, 1.0718331820176612, 1.071827176464167, 1.071614518645363, 1.0711580883061371, 1.0713757614335484, 1.071607764447739, 1.0712957253446325, 1.0717373202713607, 1.0724407814366617, 1.0721868378915336, 1.071699075189704, 1.071419670204852, 1.0719939208862963, 1.071548396165366, 1.0713206060613205, 1.070850360956525, 1.0706144587459996, 1.0713720124360226, 1.0722335756192216, 1.0710468554643635, 1.0712796205367885, 1.070904838377935, 1.0707971402996619, 1.0710758780551888, 1.0706926414853983, 1.0709970232397623, 1.0712642418040876, 1.0706979664933756, 1.071440785229818, 1.070828462332426, 1.0715268988873679, 1.0705930928674812, 1.0705571950583488, 1.0705781474495326, 1.0708646683722307, 1.0707862022721057, 1.0706768492164063, 1.070708571909879, 1.0707724884550185, 1.0704055245162525, 1.0700467085201881, 1.0704824662551253, 1.0701045875921387, 1.0705135754736053, 1.0701828663843613, 1.0709021459614716, 1.0700852240869887, 1.0700758159038222, 1.0709048756828543, 1.0706153960688158, 1.0703119625056303, 1.0692010018370235, 1.0699321133400135, 1.0709540213402782, 1.0700061366788172, 1.0701242699515403, 1.0697807949426483, 1.069743529824995, 1.0702994975710796, 1.0698810877986022, 1.070591136219565, 1.069995285549203, 1.0696318898602433, 1.0690100072100912, 1.0691514822002308, 1.0692045233822456, 1.0692085615173748, 1.0693745825569732, 1.0689730421963168, 1.0692658553622831, 1.069494417511707, 1.069207934530364, 1.0691924970497586, 1.0687652719583844, 1.0690183311517234, 1.0683426550526394, 1.069259349472469, 1.0686628136546705, 1.0688615238397272, 1.0684750076681682, 1.0684896998826483, 1.0681025339103087, 1.070001434447584, 1.0692619885262522, 1.0702492634128986, 1.0684068672955649, 1.0688273186066801, 1.0693148147888496, 1.0685723952444182, 1.0687513718125268, 1.0687964285668405, 1.0688650539523523, 1.0696333811268424, 1.068047211204466, 1.0693439034710674, 1.0683911150730612, 1.0686616227856895, 1.0688203146815056, 1.0691289787174985, 1.06821345249975, 1.0682008683559097, 1.0681767492078902, 1.0677521134793635, 1.0677155330195809, 1.0675622664927458, 1.0679183043004061, 1.067658116391552, 1.0673993582108672, 1.0680099014873623, 1.0682045954208843, 1.068045682241295, 1.0687866236394925, 1.067770571826175, 1.0681035711045628, 1.0680689496915687, 1.0669773911059026, 1.0688346648852682, 1.0669104015557918, 1.068327260360091, 1.0682466116284444, 1.0686779423171244, 1.067175689223366, 1.0671190577121241, 1.0670900130418781, 1.0674020829386779, 1.0676480959573076, 1.0670015950956873, 1.0671811855304412, 1.0669604698000992, 1.0668791250526537, 1.0669816190946764, 1.0665140124072285, 1.067152686383445, 1.0664521522835295, 1.0667722681954166, 1.0682113409531924, 1.0675913062183764, 1.0676512848425206, 1.067061742962753, 1.066741261247247, 1.0672046906159889, 1.0672990814126737, 1.0665612910072906, 1.0672036261039592, 1.0663427386685318, 1.066742750066017, 1.066456160946793, 1.0671675395671836, 1.0668019597290477, 1.0673640054354188, 1.0665031675440575, 1.0667368867314082, 1.0663678252966253, 1.0669875334910055, 1.0662539700952645, 1.0660718006764593, 1.0668115324063467, 1.0660130248667035, 1.0667854402099546, 1.0659490429155636, 1.0666131116526327, 1.0662179165552284, 1.0664132261667898, 1.0664159091835883, 1.0659449352131243, 1.0658950348899106, 1.0657119773007027, 1.0659365075814407, 1.0658612982203584, 1.065727003252237, 1.0660702586418793, 1.0662194692623443, 1.065682760890749, 1.0655484714057657, 1.0658800038468912, 1.06529811681908, 1.0654964179718518, 1.0655385284208418, 1.0653390085917478, 1.0653602111510918, 1.0658131379611193, 1.0655057701486828, 1.0658582688357061, 1.0653537719401491, 1.0648093683763695, 1.065781867430685, 1.0650782130826915, 1.0661511106902324, 1.0654025740202449, 1.0650100561627618, 1.0651981702820232, 1.064854998559188, 1.06558635087963, 1.066518801830143, 1.0655372433104309, 1.0655554207443456, 1.065529164886083, 1.0646360641142671, 1.0659045436299068, 1.0664665484575275, 1.064819351946304, 1.0649716029666532, 1.0648908489783442, 1.064972262216055, 1.064269033104977, 1.0644536245040581, 1.0641100051711472, 1.0647747276255237, 1.064959488659179, 1.0646048390645022, 1.0643520227937482, 1.0638984741371516, 1.0647862711971057, 1.0643269015043912, 1.0640181751466629, 1.064151649455515, 1.0639952478957126, 1.0644095731711731, 1.0635841709387621, 1.0635792283796432, 1.0636766483161972, 1.063624550723442, 1.0635822355869615, 1.0640252762996196, 1.0635592103249238, 1.0635209471293299, 1.063394266134415, 1.0631545958332946, 1.0634752538904273, 1.0634153157533806, 1.063170736183621], 'acc': [0.3437371682582205, 0.3749486646857839, 0.3868583173722457, 0.39958932333658365, 0.39425051328582683, 0.39219712343304064, 0.39137577097518733, 0.39219712542802154, 0.3967145786393105, 0.39260780167530696, 0.3934291598488418, 0.39260780523689864, 0.39260780340102663, 0.39014373635854077, 0.3930184804683349, 0.39301848203494566, 0.3909650937487702, 0.3930184794892031, 0.39096509116631023, 0.39630390278367783, 0.39466118956982965, 0.39753593563788725, 0.3921971266029797, 0.3995893227123872, 0.39507186738372585, 0.3950718669553557, 0.39671457805183147, 0.40000000095465343, 0.4012320320097083, 0.40492812939737854, 0.4028747421270523, 0.3991786429402275, 0.3999999995471516, 0.3991786447393821, 0.3930184794892031, 0.3942505156724605, 0.3979466106734971, 0.3983572896990688, 0.39835728750826155, 0.3958932245781289, 0.3950718667595293, 0.39671457922678954, 0.39425051195175986, 0.39507187032112107, 0.3991786427444012, 0.40205338865334983, 0.4008213545874649, 0.4020533898283079, 0.39958932431571537, 0.4041067775270043, 0.40533880959790836, 0.388911706478444, 0.4053388070154484, 0.4012320306389239, 0.40082135619079307, 0.4106776164420087, 0.40328541951257835, 0.4078028766771117, 0.403285418729273, 0.406570842219574, 0.4057494864326728, 0.40246406392150347, 0.4078028744863044, 0.40698151944599115, 0.4082135515168952, 0.40410677831030967, 0.40739219945069455, 0.40780287687293804, 0.4057494881951099, 0.40410677693952524, 0.4127310045323577, 0.4073921992548682, 0.4057494850251709, 0.4184804907936825, 0.41601642488943724, 0.40780287350717265, 0.4110882936684258, 0.40164271064362733, 0.40903490538225035, 0.4045174545575951, 0.4106776172253141, 0.3983572893074161, 0.405749484866062, 0.4053388079945801, 0.41026694258135693, 0.4135523619225872, 0.4123203298884006, 0.41396303957737446, 0.41642710289915974, 0.4028747444769685, 0.404106774748718, 0.4131416815629485, 0.41314168512454025, 0.41026694258135693, 0.4151950716115611, 0.4123203286767251, 0.40985626316413254, 0.4057494860043026, 0.40985626398415537, 0.4024640648639178, 0.42012320244091983, 0.4110882962508858, 0.4110882962508858, 0.40123203083475023, 0.4131416850878228, 0.40903490773216655, 0.4102669388239389, 0.4045174523667878, 0.40492813315479664, 0.40410677435706527, 0.41190964890456544, 0.40903490874801574, 0.4020533870500216, 0.4127310084856022, 0.4053388101853874, 0.4086242287433123, 0.4098562639474379, 0.4061601662048324, 0.41149897109066924, 0.40780287389882536, 0.40985626277247983, 0.40985626081421633, 0.40410677615621987, 0.41355235976849736, 0.4008213530208541, 0.41190964890456544, 0.4114989756313927, 0.40492813374227565, 0.4119096530536362, 0.4090349093354948, 0.41067762000360036, 0.4131416849287139, 0.4147843934060122, 0.41232032691428794, 0.41149897386895556, 0.41642710426994417, 0.4094455863660855, 0.40944558519112745, 0.414373718174576, 0.41519507102408204, 0.41560575040458897, 0.41396303973648335, 0.414373714417158, 0.40903490718140495, 0.4168377842746476, 0.4106776158545296, 0.4127310078981231, 0.40369609908891163, 0.41437371715872684, 0.4114989730856502, 0.41232032828507237, 0.4176591390824171, 0.41355236074762913, 0.4102669423488131, 0.4172484591144311, 0.40944558675773823, 0.411498974652261, 0.41560574864215183, 0.41355236290171893, 0.41642710685240414, 0.4184804909895088, 0.41437371520046334, 0.41724846071775934, 0.4160164276310061, 0.4229979451799295, 0.4143737165712478, 0.41478439657595123, 0.4114989742606083, 0.41601642488943724, 0.4172484606810419, 0.4143737153962897, 0.417248460521933, 0.41642710544490225, 0.4164271046983143, 0.4123203267184616, 0.41437371598376876, 0.4156057496212836, 0.4176591389233082, 0.42012320146178805, 0.4151950732148893, 0.41108829644671213, 0.411909652625266, 0.41848049396362147, 0.4160164286468553, 0.42053388244562323, 0.4217659152998327, 0.4151950702407766, 0.414373715004637, 0.41149897543556635, 0.4135523599643237, 0.41519507200321376, 0.41601642547691625, 0.4172484618927174, 0.41683778365045115, 0.4123203267184616, 0.4156057512246118, 0.4225872707317987, 0.4168377826713194, 0.4180698145463971, 0.4176591359491955, 0.41930184583399577, 0.4172484610726946, 0.41355235976849736, 0.4151950725906928, 0.4193018475964329, 0.41232033008422697, 0.425051334053584, 0.4197125240395446, 0.4201232056108588, 0.418891169390884, 0.40821355409935517, 0.42135523748593656, 0.4197125256061554, 0.4135523605518028, 0.41560574785884646, 0.4168377842746476, 0.41355236113928184, 0.4221765909229216, 0.4164271044657705, 0.41683778466630034, 0.41355236195930467, 0.4184804913811615, 0.41848049494275325, 0.41930184700895384, 0.42012320381170426, 0.41601642488943724, 0.4193018468131275, 0.42053387927568425, 0.4172484583311257, 0.4131416837537558, 0.41642710685240414, 0.4160164286101378, 0.41889116860757863, 0.41642710250750703, 0.4151950696532976, 0.4151950739981947, 0.4168377820838403, 0.4172484604852155, 0.418069813371439, 0.41355235976849736, 0.418480493339425, 0.41560574864215183, 0.4160164252810899, 0.41889116958671035, 0.41971252638946077, 0.41601642586856896, 0.4160164280593762, 0.41765913771163266, 0.415195070436603, 0.41478439500934045, 0.41478439516844934, 0.41765913575336916, 0.41149897484808734, 0.41971252541032905, 0.42135523823252446, 0.41806981415474437, 0.4106776164420087, 0.413963038953178, 0.41560574785884646, 0.4127310060989685, 0.4176591382991117, 0.4197125275644189, 0.40780287311552, 0.4168377801255769, 0.4106776203952531, 0.4082135505377634, 0.4172484618927174, 0.4106776196119477, 0.41478439458097033, 0.4234086238138485, 0.41314168512454025, 0.421355236274261, 0.40780287291969364, 0.4176591363408482, 0.4201232020492671, 0.4188911707616685, 0.41889117197334397, 0.41683778169218766, 0.4193018475964329, 0.419712524235371, 0.4164271028624423, 0.4151950712199084, 0.4201232020492671, 0.4193018481839119, 0.41724845872277844, 0.4184804943552742, 0.4246406562396878, 0.4188911691950577, 0.4217659156914854, 0.4213552382692419, 0.4234086243646101, 0.41971252462702363, 0.4238193035860081, 0.4193018481839119, 0.42340862260217293, 0.4238193023743326, 0.42053388123394775, 0.4176591399024399, 0.42176591232572, 0.4197125256428728]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
