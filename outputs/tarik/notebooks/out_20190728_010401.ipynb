{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf55.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 01:04:02 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '3Ov', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['ce', 'by', 'my', 'eg', 'mb', 'ib', 'sk', 'yd', 'sg', 'ck', 'ek', 'aa', 'ds', 'eb', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001408219E278>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x00000140EDE76EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 19,755\n",
    "Trainable params: 19,755\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7131, Accuracy:0.0665, Validation Loss:2.7063, Validation Accuracy:0.0657\n",
    "Epoch #2: Loss:2.7025, Accuracy:0.0665, Validation Loss:2.6971, Validation Accuracy:0.0887\n",
    "Epoch #3: Loss:2.6939, Accuracy:0.0891, Validation Loss:2.6894, Validation Accuracy:0.0887\n",
    "Epoch #4: Loss:2.6869, Accuracy:0.0891, Validation Loss:2.6831, Validation Accuracy:0.0887\n",
    "Epoch #5: Loss:2.6813, Accuracy:0.0891, Validation Loss:2.6784, Validation Accuracy:0.0887\n",
    "Epoch #6: Loss:2.6770, Accuracy:0.0977, Validation Loss:2.6750, Validation Accuracy:0.1018\n",
    "Epoch #7: Loss:2.6740, Accuracy:0.1023, Validation Loss:2.6725, Validation Accuracy:0.1018\n",
    "Epoch #8: Loss:2.6715, Accuracy:0.1023, Validation Loss:2.6705, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6697, Accuracy:0.1023, Validation Loss:2.6688, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6681, Accuracy:0.1023, Validation Loss:2.6673, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6668, Accuracy:0.1023, Validation Loss:2.6659, Validation Accuracy:0.1018\n",
    "Epoch #12: Loss:2.6657, Accuracy:0.1023, Validation Loss:2.6648, Validation Accuracy:0.1018\n",
    "Epoch #13: Loss:2.6645, Accuracy:0.1023, Validation Loss:2.6639, Validation Accuracy:0.1018\n",
    "Epoch #14: Loss:2.6635, Accuracy:0.1023, Validation Loss:2.6630, Validation Accuracy:0.1018\n",
    "Epoch #15: Loss:2.6627, Accuracy:0.1023, Validation Loss:2.6623, Validation Accuracy:0.1018\n",
    "Epoch #16: Loss:2.6623, Accuracy:0.1023, Validation Loss:2.6617, Validation Accuracy:0.1018\n",
    "Epoch #17: Loss:2.6615, Accuracy:0.1023, Validation Loss:2.6612, Validation Accuracy:0.1018\n",
    "Epoch #18: Loss:2.6612, Accuracy:0.1023, Validation Loss:2.6607, Validation Accuracy:0.1018\n",
    "Epoch #19: Loss:2.6606, Accuracy:0.1023, Validation Loss:2.6603, Validation Accuracy:0.1018\n",
    "Epoch #20: Loss:2.6602, Accuracy:0.1023, Validation Loss:2.6599, Validation Accuracy:0.1018\n",
    "Epoch #21: Loss:2.6598, Accuracy:0.1023, Validation Loss:2.6595, Validation Accuracy:0.1018\n",
    "Epoch #22: Loss:2.6594, Accuracy:0.1023, Validation Loss:2.6591, Validation Accuracy:0.1018\n",
    "Epoch #23: Loss:2.6590, Accuracy:0.1023, Validation Loss:2.6586, Validation Accuracy:0.1018\n",
    "Epoch #24: Loss:2.6584, Accuracy:0.1023, Validation Loss:2.6578, Validation Accuracy:0.1018\n",
    "Epoch #25: Loss:2.6575, Accuracy:0.1023, Validation Loss:2.6568, Validation Accuracy:0.1018\n",
    "Epoch #26: Loss:2.6563, Accuracy:0.1023, Validation Loss:2.6554, Validation Accuracy:0.1018\n",
    "Epoch #27: Loss:2.6549, Accuracy:0.1023, Validation Loss:2.6533, Validation Accuracy:0.1018\n",
    "Epoch #28: Loss:2.6520, Accuracy:0.1027, Validation Loss:2.6499, Validation Accuracy:0.1018\n",
    "Epoch #29: Loss:2.6475, Accuracy:0.1035, Validation Loss:2.6439, Validation Accuracy:0.1034\n",
    "Epoch #30: Loss:2.6395, Accuracy:0.1097, Validation Loss:2.6344, Validation Accuracy:0.1149\n",
    "Epoch #31: Loss:2.6261, Accuracy:0.1211, Validation Loss:2.6184, Validation Accuracy:0.1346\n",
    "Epoch #32: Loss:2.6064, Accuracy:0.1396, Validation Loss:2.6020, Validation Accuracy:0.1346\n",
    "Epoch #33: Loss:2.5865, Accuracy:0.1478, Validation Loss:2.5815, Validation Accuracy:0.1445\n",
    "Epoch #34: Loss:2.5674, Accuracy:0.1532, Validation Loss:2.5681, Validation Accuracy:0.1593\n",
    "Epoch #35: Loss:2.5531, Accuracy:0.1589, Validation Loss:2.5589, Validation Accuracy:0.1494\n",
    "Epoch #36: Loss:2.5410, Accuracy:0.1544, Validation Loss:2.5388, Validation Accuracy:0.1544\n",
    "Epoch #37: Loss:2.5311, Accuracy:0.1565, Validation Loss:2.5321, Validation Accuracy:0.1593\n",
    "Epoch #38: Loss:2.5239, Accuracy:0.1598, Validation Loss:2.5187, Validation Accuracy:0.1560\n",
    "Epoch #39: Loss:2.5077, Accuracy:0.1602, Validation Loss:2.5081, Validation Accuracy:0.1642\n",
    "Epoch #40: Loss:2.5009, Accuracy:0.1614, Validation Loss:2.5016, Validation Accuracy:0.1626\n",
    "Epoch #41: Loss:2.4967, Accuracy:0.1663, Validation Loss:2.4967, Validation Accuracy:0.1658\n",
    "Epoch #42: Loss:2.4894, Accuracy:0.1778, Validation Loss:2.4821, Validation Accuracy:0.1872\n",
    "Epoch #43: Loss:2.4849, Accuracy:0.1745, Validation Loss:2.4768, Validation Accuracy:0.1757\n",
    "Epoch #44: Loss:2.4808, Accuracy:0.1749, Validation Loss:2.4744, Validation Accuracy:0.1773\n",
    "Epoch #45: Loss:2.4746, Accuracy:0.1696, Validation Loss:2.4755, Validation Accuracy:0.1757\n",
    "Epoch #46: Loss:2.4676, Accuracy:0.1803, Validation Loss:2.4664, Validation Accuracy:0.1806\n",
    "Epoch #47: Loss:2.4667, Accuracy:0.1823, Validation Loss:2.4573, Validation Accuracy:0.1856\n",
    "Epoch #48: Loss:2.4552, Accuracy:0.1947, Validation Loss:2.4455, Validation Accuracy:0.1970\n",
    "Epoch #49: Loss:2.4460, Accuracy:0.1897, Validation Loss:2.4330, Validation Accuracy:0.1905\n",
    "Epoch #50: Loss:2.4368, Accuracy:0.1959, Validation Loss:2.4256, Validation Accuracy:0.2069\n",
    "Epoch #51: Loss:2.4293, Accuracy:0.2053, Validation Loss:2.4259, Validation Accuracy:0.2036\n",
    "Epoch #52: Loss:2.4307, Accuracy:0.2082, Validation Loss:2.4037, Validation Accuracy:0.2053\n",
    "Epoch #53: Loss:2.4195, Accuracy:0.2049, Validation Loss:2.3950, Validation Accuracy:0.2085\n",
    "Epoch #54: Loss:2.4066, Accuracy:0.2078, Validation Loss:2.3855, Validation Accuracy:0.2102\n",
    "Epoch #55: Loss:2.3967, Accuracy:0.2164, Validation Loss:2.3782, Validation Accuracy:0.2151\n",
    "Epoch #56: Loss:2.3894, Accuracy:0.2164, Validation Loss:2.3726, Validation Accuracy:0.2184\n",
    "Epoch #57: Loss:2.3854, Accuracy:0.2222, Validation Loss:2.3554, Validation Accuracy:0.2200\n",
    "Epoch #58: Loss:2.3699, Accuracy:0.2259, Validation Loss:2.3518, Validation Accuracy:0.2184\n",
    "Epoch #59: Loss:2.3627, Accuracy:0.2300, Validation Loss:2.3429, Validation Accuracy:0.2184\n",
    "Epoch #60: Loss:2.3569, Accuracy:0.2230, Validation Loss:2.3251, Validation Accuracy:0.2348\n",
    "Epoch #61: Loss:2.3426, Accuracy:0.2394, Validation Loss:2.3136, Validation Accuracy:0.2381\n",
    "Epoch #62: Loss:2.3375, Accuracy:0.2370, Validation Loss:2.3045, Validation Accuracy:0.2414\n",
    "Epoch #63: Loss:2.3305, Accuracy:0.2423, Validation Loss:2.2995, Validation Accuracy:0.2332\n",
    "Epoch #64: Loss:2.3193, Accuracy:0.2398, Validation Loss:2.2937, Validation Accuracy:0.2381\n",
    "Epoch #65: Loss:2.3081, Accuracy:0.2448, Validation Loss:2.2848, Validation Accuracy:0.2430\n",
    "Epoch #66: Loss:2.2993, Accuracy:0.2464, Validation Loss:2.2697, Validation Accuracy:0.2611\n",
    "Epoch #67: Loss:2.2877, Accuracy:0.2530, Validation Loss:2.2606, Validation Accuracy:0.2660\n",
    "Epoch #68: Loss:2.2757, Accuracy:0.2534, Validation Loss:2.2523, Validation Accuracy:0.2611\n",
    "Epoch #69: Loss:2.2704, Accuracy:0.2637, Validation Loss:2.2583, Validation Accuracy:0.2512\n",
    "Epoch #70: Loss:2.2659, Accuracy:0.2546, Validation Loss:2.2379, Validation Accuracy:0.2726\n",
    "Epoch #71: Loss:2.2494, Accuracy:0.2628, Validation Loss:2.2316, Validation Accuracy:0.2709\n",
    "Epoch #72: Loss:2.2417, Accuracy:0.2702, Validation Loss:2.2230, Validation Accuracy:0.2693\n",
    "Epoch #73: Loss:2.2344, Accuracy:0.2772, Validation Loss:2.2241, Validation Accuracy:0.2545\n",
    "Epoch #74: Loss:2.2287, Accuracy:0.2706, Validation Loss:2.2183, Validation Accuracy:0.2824\n",
    "Epoch #75: Loss:2.2266, Accuracy:0.2641, Validation Loss:2.2100, Validation Accuracy:0.2759\n",
    "Epoch #76: Loss:2.2288, Accuracy:0.2649, Validation Loss:2.2100, Validation Accuracy:0.2841\n",
    "Epoch #77: Loss:2.2183, Accuracy:0.2743, Validation Loss:2.1978, Validation Accuracy:0.2791\n",
    "Epoch #78: Loss:2.2074, Accuracy:0.2772, Validation Loss:2.1962, Validation Accuracy:0.2644\n",
    "Epoch #79: Loss:2.1947, Accuracy:0.2760, Validation Loss:2.1859, Validation Accuracy:0.2824\n",
    "Epoch #80: Loss:2.1910, Accuracy:0.2899, Validation Loss:2.1885, Validation Accuracy:0.2693\n",
    "Epoch #81: Loss:2.1897, Accuracy:0.2793, Validation Loss:2.1673, Validation Accuracy:0.2890\n",
    "Epoch #82: Loss:2.1856, Accuracy:0.2809, Validation Loss:2.1619, Validation Accuracy:0.2874\n",
    "Epoch #83: Loss:2.1766, Accuracy:0.2821, Validation Loss:2.1586, Validation Accuracy:0.2808\n",
    "Epoch #84: Loss:2.1701, Accuracy:0.2883, Validation Loss:2.1471, Validation Accuracy:0.2906\n",
    "Epoch #85: Loss:2.1616, Accuracy:0.2908, Validation Loss:2.1490, Validation Accuracy:0.2923\n",
    "Epoch #86: Loss:2.1546, Accuracy:0.2916, Validation Loss:2.1369, Validation Accuracy:0.2874\n",
    "Epoch #87: Loss:2.1471, Accuracy:0.2908, Validation Loss:2.1356, Validation Accuracy:0.2939\n",
    "Epoch #88: Loss:2.1414, Accuracy:0.2895, Validation Loss:2.1273, Validation Accuracy:0.2956\n",
    "Epoch #89: Loss:2.1416, Accuracy:0.2965, Validation Loss:2.1257, Validation Accuracy:0.2857\n",
    "Epoch #90: Loss:2.1345, Accuracy:0.2945, Validation Loss:2.1164, Validation Accuracy:0.2972\n",
    "Epoch #91: Loss:2.1298, Accuracy:0.2961, Validation Loss:2.1317, Validation Accuracy:0.2923\n",
    "Epoch #92: Loss:2.1333, Accuracy:0.2920, Validation Loss:2.1109, Validation Accuracy:0.2759\n",
    "Epoch #93: Loss:2.1347, Accuracy:0.2945, Validation Loss:2.1208, Validation Accuracy:0.2841\n",
    "Epoch #94: Loss:2.1191, Accuracy:0.2932, Validation Loss:2.1084, Validation Accuracy:0.3021\n",
    "Epoch #95: Loss:2.1225, Accuracy:0.2990, Validation Loss:2.1213, Validation Accuracy:0.2808\n",
    "Epoch #96: Loss:2.1178, Accuracy:0.2940, Validation Loss:2.0846, Validation Accuracy:0.2890\n",
    "Epoch #97: Loss:2.1016, Accuracy:0.2998, Validation Loss:2.0914, Validation Accuracy:0.2841\n",
    "Epoch #98: Loss:2.0976, Accuracy:0.2969, Validation Loss:2.0816, Validation Accuracy:0.2923\n",
    "Epoch #99: Loss:2.1007, Accuracy:0.3060, Validation Loss:2.0880, Validation Accuracy:0.2972\n",
    "Epoch #100: Loss:2.0955, Accuracy:0.3014, Validation Loss:2.0973, Validation Accuracy:0.3054\n",
    "Epoch #101: Loss:2.1032, Accuracy:0.2986, Validation Loss:2.0947, Validation Accuracy:0.2906\n",
    "Epoch #102: Loss:2.0947, Accuracy:0.2973, Validation Loss:2.0610, Validation Accuracy:0.3038\n",
    "Epoch #103: Loss:2.0815, Accuracy:0.3166, Validation Loss:2.0696, Validation Accuracy:0.2923\n",
    "Epoch #104: Loss:2.0796, Accuracy:0.2969, Validation Loss:2.0553, Validation Accuracy:0.3038\n",
    "Epoch #105: Loss:2.0721, Accuracy:0.3162, Validation Loss:2.0570, Validation Accuracy:0.3071\n",
    "Epoch #106: Loss:2.0671, Accuracy:0.3105, Validation Loss:2.0451, Validation Accuracy:0.3071\n",
    "Epoch #107: Loss:2.0662, Accuracy:0.3133, Validation Loss:2.0480, Validation Accuracy:0.3005\n",
    "Epoch #108: Loss:2.0620, Accuracy:0.3039, Validation Loss:2.0412, Validation Accuracy:0.3005\n",
    "Epoch #109: Loss:2.0594, Accuracy:0.3158, Validation Loss:2.0399, Validation Accuracy:0.3120\n",
    "Epoch #110: Loss:2.0586, Accuracy:0.3080, Validation Loss:2.0363, Validation Accuracy:0.3103\n",
    "Epoch #111: Loss:2.0528, Accuracy:0.3076, Validation Loss:2.0387, Validation Accuracy:0.3071\n",
    "Epoch #112: Loss:2.0519, Accuracy:0.3121, Validation Loss:2.0269, Validation Accuracy:0.3038\n",
    "Epoch #113: Loss:2.0478, Accuracy:0.3211, Validation Loss:2.0277, Validation Accuracy:0.3054\n",
    "Epoch #114: Loss:2.0457, Accuracy:0.3121, Validation Loss:2.0247, Validation Accuracy:0.2989\n",
    "Epoch #115: Loss:2.0416, Accuracy:0.3203, Validation Loss:2.0245, Validation Accuracy:0.3103\n",
    "Epoch #116: Loss:2.0416, Accuracy:0.3092, Validation Loss:2.0152, Validation Accuracy:0.3054\n",
    "Epoch #117: Loss:2.0345, Accuracy:0.3158, Validation Loss:2.0161, Validation Accuracy:0.2956\n",
    "Epoch #118: Loss:2.0319, Accuracy:0.3232, Validation Loss:2.0120, Validation Accuracy:0.3087\n",
    "Epoch #119: Loss:2.0298, Accuracy:0.3154, Validation Loss:2.0165, Validation Accuracy:0.3120\n",
    "Epoch #120: Loss:2.0283, Accuracy:0.3179, Validation Loss:2.0094, Validation Accuracy:0.3005\n",
    "Epoch #121: Loss:2.0227, Accuracy:0.3203, Validation Loss:2.0028, Validation Accuracy:0.3136\n",
    "Epoch #122: Loss:2.0232, Accuracy:0.3191, Validation Loss:2.0050, Validation Accuracy:0.3136\n",
    "Epoch #123: Loss:2.0191, Accuracy:0.3195, Validation Loss:1.9981, Validation Accuracy:0.3120\n",
    "Epoch #124: Loss:2.0170, Accuracy:0.3179, Validation Loss:1.9987, Validation Accuracy:0.3087\n",
    "Epoch #125: Loss:2.0190, Accuracy:0.3253, Validation Loss:2.0031, Validation Accuracy:0.3169\n",
    "Epoch #126: Loss:2.0142, Accuracy:0.3248, Validation Loss:2.0104, Validation Accuracy:0.3136\n",
    "Epoch #127: Loss:2.0184, Accuracy:0.3203, Validation Loss:1.9912, Validation Accuracy:0.3120\n",
    "Epoch #128: Loss:2.0168, Accuracy:0.3248, Validation Loss:2.0054, Validation Accuracy:0.3120\n",
    "Epoch #129: Loss:2.0211, Accuracy:0.3211, Validation Loss:2.0078, Validation Accuracy:0.3054\n",
    "Epoch #130: Loss:2.0110, Accuracy:0.3191, Validation Loss:1.9969, Validation Accuracy:0.3186\n",
    "Epoch #131: Loss:2.0125, Accuracy:0.3248, Validation Loss:1.9822, Validation Accuracy:0.3186\n",
    "Epoch #132: Loss:2.0027, Accuracy:0.3261, Validation Loss:1.9807, Validation Accuracy:0.3218\n",
    "Epoch #133: Loss:2.0044, Accuracy:0.3228, Validation Loss:1.9825, Validation Accuracy:0.3153\n",
    "Epoch #134: Loss:1.9929, Accuracy:0.3298, Validation Loss:1.9805, Validation Accuracy:0.3153\n",
    "Epoch #135: Loss:1.9925, Accuracy:0.3306, Validation Loss:1.9813, Validation Accuracy:0.3202\n",
    "Epoch #136: Loss:1.9907, Accuracy:0.3322, Validation Loss:1.9764, Validation Accuracy:0.3202\n",
    "Epoch #137: Loss:1.9895, Accuracy:0.3322, Validation Loss:1.9695, Validation Accuracy:0.3202\n",
    "Epoch #138: Loss:1.9816, Accuracy:0.3335, Validation Loss:1.9676, Validation Accuracy:0.3218\n",
    "Epoch #139: Loss:1.9809, Accuracy:0.3359, Validation Loss:1.9698, Validation Accuracy:0.3186\n",
    "Epoch #140: Loss:1.9809, Accuracy:0.3339, Validation Loss:1.9679, Validation Accuracy:0.3186\n",
    "Epoch #141: Loss:1.9792, Accuracy:0.3298, Validation Loss:1.9701, Validation Accuracy:0.3251\n",
    "Epoch #142: Loss:1.9782, Accuracy:0.3326, Validation Loss:1.9643, Validation Accuracy:0.3218\n",
    "Epoch #143: Loss:1.9743, Accuracy:0.3380, Validation Loss:1.9600, Validation Accuracy:0.3251\n",
    "Epoch #144: Loss:1.9693, Accuracy:0.3351, Validation Loss:1.9552, Validation Accuracy:0.3251\n",
    "Epoch #145: Loss:1.9663, Accuracy:0.3347, Validation Loss:1.9604, Validation Accuracy:0.3268\n",
    "Epoch #146: Loss:1.9648, Accuracy:0.3388, Validation Loss:1.9527, Validation Accuracy:0.3333\n",
    "Epoch #147: Loss:1.9634, Accuracy:0.3380, Validation Loss:1.9500, Validation Accuracy:0.3268\n",
    "Epoch #148: Loss:1.9600, Accuracy:0.3425, Validation Loss:1.9488, Validation Accuracy:0.3317\n",
    "Epoch #149: Loss:1.9567, Accuracy:0.3388, Validation Loss:1.9529, Validation Accuracy:0.3235\n",
    "Epoch #150: Loss:1.9545, Accuracy:0.3372, Validation Loss:1.9506, Validation Accuracy:0.3350\n",
    "Epoch #151: Loss:1.9545, Accuracy:0.3433, Validation Loss:1.9445, Validation Accuracy:0.3333\n",
    "Epoch #152: Loss:1.9506, Accuracy:0.3405, Validation Loss:1.9469, Validation Accuracy:0.3317\n",
    "Epoch #153: Loss:1.9511, Accuracy:0.3405, Validation Loss:1.9432, Validation Accuracy:0.3202\n",
    "Epoch #154: Loss:1.9478, Accuracy:0.3409, Validation Loss:1.9428, Validation Accuracy:0.3383\n",
    "Epoch #155: Loss:1.9402, Accuracy:0.3384, Validation Loss:1.9389, Validation Accuracy:0.3366\n",
    "Epoch #156: Loss:1.9400, Accuracy:0.3425, Validation Loss:1.9340, Validation Accuracy:0.3415\n",
    "Epoch #157: Loss:1.9384, Accuracy:0.3450, Validation Loss:1.9393, Validation Accuracy:0.3383\n",
    "Epoch #158: Loss:1.9367, Accuracy:0.3462, Validation Loss:1.9303, Validation Accuracy:0.3383\n",
    "Epoch #159: Loss:1.9301, Accuracy:0.3462, Validation Loss:1.9285, Validation Accuracy:0.3415\n",
    "Epoch #160: Loss:1.9430, Accuracy:0.3437, Validation Loss:1.9242, Validation Accuracy:0.3481\n",
    "Epoch #161: Loss:1.9396, Accuracy:0.3400, Validation Loss:1.9249, Validation Accuracy:0.3383\n",
    "Epoch #162: Loss:1.9298, Accuracy:0.3405, Validation Loss:1.9199, Validation Accuracy:0.3333\n",
    "Epoch #163: Loss:1.9251, Accuracy:0.3478, Validation Loss:1.9173, Validation Accuracy:0.3383\n",
    "Epoch #164: Loss:1.9234, Accuracy:0.3417, Validation Loss:1.9254, Validation Accuracy:0.3399\n",
    "Epoch #165: Loss:1.9192, Accuracy:0.3520, Validation Loss:1.9210, Validation Accuracy:0.3415\n",
    "Epoch #166: Loss:1.9233, Accuracy:0.3487, Validation Loss:1.9143, Validation Accuracy:0.3415\n",
    "Epoch #167: Loss:1.9166, Accuracy:0.3503, Validation Loss:1.9093, Validation Accuracy:0.3465\n",
    "Epoch #168: Loss:1.9099, Accuracy:0.3495, Validation Loss:1.9351, Validation Accuracy:0.3284\n",
    "Epoch #169: Loss:1.9279, Accuracy:0.3433, Validation Loss:1.9158, Validation Accuracy:0.3383\n",
    "Epoch #170: Loss:1.9123, Accuracy:0.3458, Validation Loss:1.9079, Validation Accuracy:0.3481\n",
    "Epoch #171: Loss:1.9147, Accuracy:0.3503, Validation Loss:1.9019, Validation Accuracy:0.3465\n",
    "Epoch #172: Loss:1.8991, Accuracy:0.3593, Validation Loss:1.9080, Validation Accuracy:0.3432\n",
    "Epoch #173: Loss:1.8963, Accuracy:0.3548, Validation Loss:1.9026, Validation Accuracy:0.3612\n",
    "Epoch #174: Loss:1.8955, Accuracy:0.3544, Validation Loss:1.9112, Validation Accuracy:0.3366\n",
    "Epoch #175: Loss:1.8998, Accuracy:0.3552, Validation Loss:1.8975, Validation Accuracy:0.3465\n",
    "Epoch #176: Loss:1.8945, Accuracy:0.3622, Validation Loss:1.8986, Validation Accuracy:0.3448\n",
    "Epoch #177: Loss:1.9009, Accuracy:0.3478, Validation Loss:1.9018, Validation Accuracy:0.3465\n",
    "Epoch #178: Loss:1.8976, Accuracy:0.3593, Validation Loss:1.9032, Validation Accuracy:0.3645\n",
    "Epoch #179: Loss:1.8977, Accuracy:0.3499, Validation Loss:1.8916, Validation Accuracy:0.3596\n",
    "Epoch #180: Loss:1.8954, Accuracy:0.3581, Validation Loss:1.8795, Validation Accuracy:0.3662\n",
    "Epoch #181: Loss:1.8784, Accuracy:0.3626, Validation Loss:1.8977, Validation Accuracy:0.3333\n",
    "Epoch #182: Loss:1.8847, Accuracy:0.3589, Validation Loss:1.9123, Validation Accuracy:0.3333\n",
    "Epoch #183: Loss:1.8779, Accuracy:0.3700, Validation Loss:1.8744, Validation Accuracy:0.3580\n",
    "Epoch #184: Loss:1.8678, Accuracy:0.3663, Validation Loss:1.8718, Validation Accuracy:0.3612\n",
    "Epoch #185: Loss:1.8735, Accuracy:0.3643, Validation Loss:1.8699, Validation Accuracy:0.3580\n",
    "Epoch #186: Loss:1.8582, Accuracy:0.3667, Validation Loss:1.8656, Validation Accuracy:0.3580\n",
    "Epoch #187: Loss:1.8564, Accuracy:0.3737, Validation Loss:1.8586, Validation Accuracy:0.3596\n",
    "Epoch #188: Loss:1.8496, Accuracy:0.3717, Validation Loss:1.8842, Validation Accuracy:0.3448\n",
    "Epoch #189: Loss:1.8503, Accuracy:0.3770, Validation Loss:1.8568, Validation Accuracy:0.3580\n",
    "Epoch #190: Loss:1.8479, Accuracy:0.3762, Validation Loss:1.8561, Validation Accuracy:0.3645\n",
    "Epoch #191: Loss:1.8372, Accuracy:0.3762, Validation Loss:1.8605, Validation Accuracy:0.3695\n",
    "Epoch #192: Loss:1.8486, Accuracy:0.3733, Validation Loss:1.8460, Validation Accuracy:0.3678\n",
    "Epoch #193: Loss:1.8440, Accuracy:0.3733, Validation Loss:1.8530, Validation Accuracy:0.3547\n",
    "Epoch #194: Loss:1.8335, Accuracy:0.3778, Validation Loss:1.8537, Validation Accuracy:0.3498\n",
    "Epoch #195: Loss:1.8370, Accuracy:0.3791, Validation Loss:1.8503, Validation Accuracy:0.3465\n",
    "Epoch #196: Loss:1.8246, Accuracy:0.3791, Validation Loss:1.8411, Validation Accuracy:0.3465\n",
    "Epoch #197: Loss:1.8258, Accuracy:0.3786, Validation Loss:1.8304, Validation Accuracy:0.3612\n",
    "Epoch #198: Loss:1.8135, Accuracy:0.3828, Validation Loss:1.8206, Validation Accuracy:0.3662\n",
    "Epoch #199: Loss:1.8104, Accuracy:0.3906, Validation Loss:1.8224, Validation Accuracy:0.3645\n",
    "Epoch #200: Loss:1.8045, Accuracy:0.3848, Validation Loss:1.8125, Validation Accuracy:0.3629\n",
    "Epoch #201: Loss:1.8028, Accuracy:0.3852, Validation Loss:1.8223, Validation Accuracy:0.3695\n",
    "Epoch #202: Loss:1.8066, Accuracy:0.3836, Validation Loss:1.8165, Validation Accuracy:0.3662\n",
    "Epoch #203: Loss:1.8013, Accuracy:0.3885, Validation Loss:1.8191, Validation Accuracy:0.3514\n",
    "Epoch #204: Loss:1.8054, Accuracy:0.3828, Validation Loss:1.8155, Validation Accuracy:0.3530\n",
    "Epoch #205: Loss:1.7996, Accuracy:0.3864, Validation Loss:1.8210, Validation Accuracy:0.3563\n",
    "Epoch #206: Loss:1.7966, Accuracy:0.3869, Validation Loss:1.7965, Validation Accuracy:0.3662\n",
    "Epoch #207: Loss:1.7901, Accuracy:0.3869, Validation Loss:1.8082, Validation Accuracy:0.3448\n",
    "Epoch #208: Loss:1.7911, Accuracy:0.3832, Validation Loss:1.7981, Validation Accuracy:0.3596\n",
    "Epoch #209: Loss:1.7928, Accuracy:0.3828, Validation Loss:1.8082, Validation Accuracy:0.3711\n",
    "Epoch #210: Loss:1.7903, Accuracy:0.3807, Validation Loss:1.7975, Validation Accuracy:0.3842\n",
    "Epoch #211: Loss:1.7951, Accuracy:0.3828, Validation Loss:1.7886, Validation Accuracy:0.3760\n",
    "Epoch #212: Loss:1.7851, Accuracy:0.3860, Validation Loss:1.8084, Validation Accuracy:0.3612\n",
    "Epoch #213: Loss:1.7931, Accuracy:0.3832, Validation Loss:1.8424, Validation Accuracy:0.3350\n",
    "Epoch #214: Loss:1.8013, Accuracy:0.3844, Validation Loss:1.8277, Validation Accuracy:0.3448\n",
    "Epoch #215: Loss:1.7875, Accuracy:0.3914, Validation Loss:1.7797, Validation Accuracy:0.3777\n",
    "Epoch #216: Loss:1.7675, Accuracy:0.3979, Validation Loss:1.7809, Validation Accuracy:0.3777\n",
    "Epoch #217: Loss:1.7794, Accuracy:0.3877, Validation Loss:1.7767, Validation Accuracy:0.3727\n",
    "Epoch #218: Loss:1.7645, Accuracy:0.4012, Validation Loss:1.7954, Validation Accuracy:0.3678\n",
    "Epoch #219: Loss:1.7801, Accuracy:0.3906, Validation Loss:1.8014, Validation Accuracy:0.3727\n",
    "Epoch #220: Loss:1.7787, Accuracy:0.3922, Validation Loss:1.7968, Validation Accuracy:0.3678\n",
    "Epoch #221: Loss:1.7880, Accuracy:0.3860, Validation Loss:1.7796, Validation Accuracy:0.3875\n",
    "Epoch #222: Loss:1.7639, Accuracy:0.3975, Validation Loss:1.7664, Validation Accuracy:0.3875\n",
    "Epoch #223: Loss:1.7565, Accuracy:0.3934, Validation Loss:1.8083, Validation Accuracy:0.3514\n",
    "Epoch #224: Loss:1.7675, Accuracy:0.3844, Validation Loss:1.8027, Validation Accuracy:0.3695\n",
    "Epoch #225: Loss:1.7605, Accuracy:0.4016, Validation Loss:1.7611, Validation Accuracy:0.3859\n",
    "Epoch #226: Loss:1.7650, Accuracy:0.3934, Validation Loss:1.7572, Validation Accuracy:0.3957\n",
    "Epoch #227: Loss:1.7709, Accuracy:0.3910, Validation Loss:1.7769, Validation Accuracy:0.3760\n",
    "Epoch #228: Loss:1.7554, Accuracy:0.4012, Validation Loss:1.7919, Validation Accuracy:0.3580\n",
    "Epoch #229: Loss:1.7523, Accuracy:0.3947, Validation Loss:1.7607, Validation Accuracy:0.3892\n",
    "Epoch #230: Loss:1.7470, Accuracy:0.4057, Validation Loss:1.7582, Validation Accuracy:0.4007\n",
    "Epoch #231: Loss:1.7471, Accuracy:0.3967, Validation Loss:1.7620, Validation Accuracy:0.3875\n",
    "Epoch #232: Loss:1.7435, Accuracy:0.4008, Validation Loss:1.7476, Validation Accuracy:0.3941\n",
    "Epoch #233: Loss:1.7387, Accuracy:0.4029, Validation Loss:1.7696, Validation Accuracy:0.3678\n",
    "Epoch #234: Loss:1.7379, Accuracy:0.4066, Validation Loss:1.7515, Validation Accuracy:0.3695\n",
    "Epoch #235: Loss:1.7362, Accuracy:0.4021, Validation Loss:1.7389, Validation Accuracy:0.3859\n",
    "Epoch #236: Loss:1.7283, Accuracy:0.4082, Validation Loss:1.7445, Validation Accuracy:0.4105\n",
    "Epoch #237: Loss:1.7281, Accuracy:0.4111, Validation Loss:1.7445, Validation Accuracy:0.3826\n",
    "Epoch #238: Loss:1.7235, Accuracy:0.4123, Validation Loss:1.7450, Validation Accuracy:0.3859\n",
    "Epoch #239: Loss:1.7202, Accuracy:0.4148, Validation Loss:1.7415, Validation Accuracy:0.3941\n",
    "Epoch #240: Loss:1.7331, Accuracy:0.4115, Validation Loss:1.7497, Validation Accuracy:0.4023\n",
    "Epoch #241: Loss:1.7361, Accuracy:0.4041, Validation Loss:1.7447, Validation Accuracy:0.3727\n",
    "Epoch #242: Loss:1.7353, Accuracy:0.4148, Validation Loss:1.7436, Validation Accuracy:0.3793\n",
    "Epoch #243: Loss:1.7195, Accuracy:0.4177, Validation Loss:1.7404, Validation Accuracy:0.3924\n",
    "Epoch #244: Loss:1.7230, Accuracy:0.4078, Validation Loss:1.7252, Validation Accuracy:0.3941\n",
    "Epoch #245: Loss:1.7146, Accuracy:0.4115, Validation Loss:1.7414, Validation Accuracy:0.3777\n",
    "Epoch #246: Loss:1.7150, Accuracy:0.4156, Validation Loss:1.7347, Validation Accuracy:0.3957\n",
    "Epoch #247: Loss:1.7121, Accuracy:0.4222, Validation Loss:1.7261, Validation Accuracy:0.3974\n",
    "Epoch #248: Loss:1.7172, Accuracy:0.4127, Validation Loss:1.7260, Validation Accuracy:0.3908\n",
    "Epoch #249: Loss:1.7162, Accuracy:0.4205, Validation Loss:1.7268, Validation Accuracy:0.3908\n",
    "Epoch #250: Loss:1.7038, Accuracy:0.4177, Validation Loss:1.7299, Validation Accuracy:0.3941\n",
    "Epoch #251: Loss:1.7072, Accuracy:0.4168, Validation Loss:1.7195, Validation Accuracy:0.3941\n",
    "Epoch #252: Loss:1.7059, Accuracy:0.4189, Validation Loss:1.7217, Validation Accuracy:0.4039\n",
    "Epoch #253: Loss:1.7017, Accuracy:0.4251, Validation Loss:1.7156, Validation Accuracy:0.3990\n",
    "Epoch #254: Loss:1.7097, Accuracy:0.4197, Validation Loss:1.7167, Validation Accuracy:0.4023\n",
    "Epoch #255: Loss:1.7015, Accuracy:0.4218, Validation Loss:1.7197, Validation Accuracy:0.4007\n",
    "Epoch #256: Loss:1.7030, Accuracy:0.4201, Validation Loss:1.7116, Validation Accuracy:0.3974\n",
    "Epoch #257: Loss:1.6982, Accuracy:0.4185, Validation Loss:1.7138, Validation Accuracy:0.4023\n",
    "Epoch #258: Loss:1.7019, Accuracy:0.4234, Validation Loss:1.7089, Validation Accuracy:0.4007\n",
    "Epoch #259: Loss:1.7020, Accuracy:0.4156, Validation Loss:1.7135, Validation Accuracy:0.3924\n",
    "Epoch #260: Loss:1.7002, Accuracy:0.4189, Validation Loss:1.7475, Validation Accuracy:0.3793\n",
    "Epoch #261: Loss:1.7076, Accuracy:0.4152, Validation Loss:1.7098, Validation Accuracy:0.3974\n",
    "Epoch #262: Loss:1.6936, Accuracy:0.4271, Validation Loss:1.7180, Validation Accuracy:0.4105\n",
    "Epoch #263: Loss:1.7049, Accuracy:0.4214, Validation Loss:1.7092, Validation Accuracy:0.4072\n",
    "Epoch #264: Loss:1.7053, Accuracy:0.4177, Validation Loss:1.7094, Validation Accuracy:0.4007\n",
    "Epoch #265: Loss:1.6968, Accuracy:0.4238, Validation Loss:1.7061, Validation Accuracy:0.3957\n",
    "Epoch #266: Loss:1.6911, Accuracy:0.4197, Validation Loss:1.7040, Validation Accuracy:0.4056\n",
    "Epoch #267: Loss:1.6882, Accuracy:0.4205, Validation Loss:1.7050, Validation Accuracy:0.4122\n",
    "Epoch #268: Loss:1.6857, Accuracy:0.4234, Validation Loss:1.6999, Validation Accuracy:0.4056\n",
    "Epoch #269: Loss:1.6843, Accuracy:0.4255, Validation Loss:1.7162, Validation Accuracy:0.4122\n",
    "Epoch #270: Loss:1.6873, Accuracy:0.4275, Validation Loss:1.7002, Validation Accuracy:0.4204\n",
    "Epoch #271: Loss:1.6917, Accuracy:0.4201, Validation Loss:1.7024, Validation Accuracy:0.4039\n",
    "Epoch #272: Loss:1.6872, Accuracy:0.4267, Validation Loss:1.7080, Validation Accuracy:0.3957\n",
    "Epoch #273: Loss:1.6889, Accuracy:0.4275, Validation Loss:1.7133, Validation Accuracy:0.4105\n",
    "Epoch #274: Loss:1.6909, Accuracy:0.4308, Validation Loss:1.6985, Validation Accuracy:0.4236\n",
    "Epoch #275: Loss:1.6806, Accuracy:0.4251, Validation Loss:1.6911, Validation Accuracy:0.4039\n",
    "Epoch #276: Loss:1.6814, Accuracy:0.4308, Validation Loss:1.6911, Validation Accuracy:0.4023\n",
    "Epoch #277: Loss:1.6906, Accuracy:0.4242, Validation Loss:1.7229, Validation Accuracy:0.4122\n",
    "Epoch #278: Loss:1.6902, Accuracy:0.4230, Validation Loss:1.6948, Validation Accuracy:0.4154\n",
    "Epoch #279: Loss:1.6759, Accuracy:0.4296, Validation Loss:1.6951, Validation Accuracy:0.3974\n",
    "Epoch #280: Loss:1.6805, Accuracy:0.4292, Validation Loss:1.6943, Validation Accuracy:0.4204\n",
    "Epoch #281: Loss:1.6893, Accuracy:0.4218, Validation Loss:1.6920, Validation Accuracy:0.4039\n",
    "Epoch #282: Loss:1.6791, Accuracy:0.4316, Validation Loss:1.6886, Validation Accuracy:0.4154\n",
    "Epoch #283: Loss:1.6758, Accuracy:0.4279, Validation Loss:1.6837, Validation Accuracy:0.4122\n",
    "Epoch #284: Loss:1.6766, Accuracy:0.4374, Validation Loss:1.6940, Validation Accuracy:0.4187\n",
    "Epoch #285: Loss:1.6828, Accuracy:0.4255, Validation Loss:1.6921, Validation Accuracy:0.4089\n",
    "Epoch #286: Loss:1.6766, Accuracy:0.4300, Validation Loss:1.6847, Validation Accuracy:0.4138\n",
    "Epoch #287: Loss:1.6835, Accuracy:0.4329, Validation Loss:1.6888, Validation Accuracy:0.4138\n",
    "Epoch #288: Loss:1.6790, Accuracy:0.4390, Validation Loss:1.7024, Validation Accuracy:0.3957\n",
    "Epoch #289: Loss:1.6794, Accuracy:0.4333, Validation Loss:1.6861, Validation Accuracy:0.4154\n",
    "Epoch #290: Loss:1.6712, Accuracy:0.4337, Validation Loss:1.6813, Validation Accuracy:0.4220\n",
    "Epoch #291: Loss:1.6662, Accuracy:0.4394, Validation Loss:1.6850, Validation Accuracy:0.4171\n",
    "Epoch #292: Loss:1.6625, Accuracy:0.4402, Validation Loss:1.6813, Validation Accuracy:0.4138\n",
    "Epoch #293: Loss:1.6629, Accuracy:0.4386, Validation Loss:1.6979, Validation Accuracy:0.4072\n",
    "Epoch #294: Loss:1.6712, Accuracy:0.4304, Validation Loss:1.6933, Validation Accuracy:0.4007\n",
    "Epoch #295: Loss:1.6695, Accuracy:0.4320, Validation Loss:1.6821, Validation Accuracy:0.4220\n",
    "Epoch #296: Loss:1.6667, Accuracy:0.4345, Validation Loss:1.6763, Validation Accuracy:0.4204\n",
    "Epoch #297: Loss:1.6658, Accuracy:0.4386, Validation Loss:1.6754, Validation Accuracy:0.4204\n",
    "Epoch #298: Loss:1.6644, Accuracy:0.4333, Validation Loss:1.6758, Validation Accuracy:0.4204\n",
    "Epoch #299: Loss:1.6599, Accuracy:0.4452, Validation Loss:1.6770, Validation Accuracy:0.4105\n",
    "Epoch #300: Loss:1.6596, Accuracy:0.4312, Validation Loss:1.6985, Validation Accuracy:0.4072\n",
    "\n",
    "Test:\n",
    "Test Loss:1.69850862, Accuracy:0.4072\n",
    "Labels: ['ce', 'by', 'my', 'eg', 'mb', 'ib', 'sk', 'yd', 'sg', 'ck', 'ek', 'aa', 'ds', 'eb', 'eo']\n",
    "Confusion Matrix:\n",
    "      ce  by  my  eg  mb  ib  sk  yd  sg  ck  ek  aa  ds  eb  eo\n",
    "t:ce   0   4   0   2  11   2   1   1   0   0   2   0   0   0   4\n",
    "t:by   0  17   0   5   2   4   0   0   2   0   0   0   1   0   9\n",
    "t:my   0   1   1   0   6   0   9   0   0   0   2   0   0   0   1\n",
    "t:eg   0   8   0  23   1   0   1   0   3   0   2   9   1   1   1\n",
    "t:mb   0   4   1   1  42   2   1   0   0   0   0   0   1   0   0\n",
    "t:ib   0   5   0   2   2  27   0   7   4   0   1   5   0   0   1\n",
    "t:sk   0   0   0   1   6   0  15   0   0   0   4   0   2   5   0\n",
    "t:yd   0   3   0   0   1  19   0  27   8   0   1   1   2   0   0\n",
    "t:sg   0   7   0   0   1  12   0  16  11   0   1   3   0   0   0\n",
    "t:ck   0   0   1   1   5   2   1   2   0   0   5   3   2   1   0\n",
    "t:ek   0   0   0   1   0   0   3  13   0   0  14   1   2  14   0\n",
    "t:aa   0   2   0   4   3   0   0   1   3   0   2  18   0   0   1\n",
    "t:ds   0   1   0   2  10   0   2   2   0   0   4   0   3   7   0\n",
    "t:eb   0   0   1   0   0   0   1   6   0   0  18   0   2  22   0\n",
    "t:eo   0   2   0   4   0   0   0   0   0   0   0   0   0   0  28\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          by       0.31      0.42      0.36        40\n",
    "          my       0.25      0.05      0.08        20\n",
    "          eg       0.50      0.46      0.48        50\n",
    "          mb       0.47      0.81      0.59        52\n",
    "          ib       0.40      0.50      0.44        54\n",
    "          sk       0.44      0.45      0.45        33\n",
    "          yd       0.36      0.44      0.39        62\n",
    "          sg       0.35      0.22      0.27        51\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          ek       0.25      0.29      0.27        48\n",
    "          aa       0.45      0.53      0.49        34\n",
    "          ds       0.19      0.10      0.13        31\n",
    "          eb       0.44      0.44      0.44        50\n",
    "          eo       0.62      0.82      0.71        34\n",
    "\n",
    "    accuracy                           0.41       609\n",
    "   macro avg       0.34      0.37      0.34       609\n",
    "weighted avg       0.36      0.41      0.37       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 01:44:30 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 28 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.7063182551285316, 2.6970893812101266, 2.6893730296681473, 2.683128067621065, 2.678351295796913, 2.6749753486150984, 2.6724714763058817, 2.6704761512173807, 2.6687884749645865, 2.6672854971611635, 2.665941742644913, 2.6648268464750844, 2.6638617852247015, 2.663035633920253, 2.6623362402610593, 2.661694141835806, 2.6611669705614864, 2.660717233648441, 2.6602844459865675, 2.6598690503532274, 2.65945419380426, 2.6591213360208594, 2.658594567591725, 2.657823694946339, 2.6568415979250704, 2.6554245995770533, 2.6532706360903084, 2.649893672986962, 2.643949221311923, 2.6344499693715515, 2.6184391607400426, 2.602018269412036, 2.581512271085592, 2.5681367836562283, 2.55887265510747, 2.5388329260063482, 2.532133581994594, 2.5186707597647984, 2.50809035277719, 2.501558141755353, 2.4967487410371527, 2.4821043566529974, 2.47680430146078, 2.4743569002950134, 2.4754567921455273, 2.4664292914918295, 2.457293416478951, 2.4455010025763553, 2.43295134425359, 2.4256478330771913, 2.4259023086973794, 2.403681486502461, 2.3950499576105075, 2.3855404650245005, 2.378203877870281, 2.3726057990626943, 2.35538762543589, 2.351759416912185, 2.342870343494885, 2.3251131430439567, 2.313574775686405, 2.3044692174162966, 2.299547934179823, 2.293685773714814, 2.2847552679246674, 2.2696743575222973, 2.2605687142984423, 2.252343230646819, 2.258271666387423, 2.2379061126552386, 2.2316377573999864, 2.223028575649794, 2.224098360988698, 2.2182543136803385, 2.2099833183100657, 2.209996622379973, 2.197767299580065, 2.19615552499768, 2.1858944673647827, 2.1885308299354342, 2.1673176648777304, 2.161917106662869, 2.158585567975475, 2.1471033245080404, 2.1490176667525067, 2.136875400793768, 2.1356136454345753, 2.12730958387378, 2.125683430576168, 2.116410946415367, 2.1316782469036935, 2.110868687308676, 2.1208244000358145, 2.1084476163234616, 2.1213096390021065, 2.084575592786416, 2.091357224093282, 2.0816217584563006, 2.087960802275559, 2.0973183987371637, 2.0947337185808004, 2.0610157190676786, 2.069559722502635, 2.055333645668719, 2.0570235784809383, 2.045143476260707, 2.0480339350958765, 2.0411620915229687, 2.0398864589496983, 2.0363052619501874, 2.038722212482947, 2.0268513091483533, 2.0276602849193, 2.024682288882376, 2.024496949952224, 2.0152125601306534, 2.0161002837182656, 2.0120139979376583, 2.0165091633600944, 2.0093804212235074, 2.002807148571672, 2.0050131252833774, 1.9980862426444619, 1.9986626737810709, 2.003105697373451, 2.0103958437986953, 1.9912338125686144, 2.0053907540827156, 2.0077729841758467, 1.9968703225719908, 1.9821587158932865, 1.9806937188742, 1.9825070665779176, 1.9805172260954658, 1.9812702703945742, 1.9763823998189716, 1.969485001219513, 1.967614086390716, 1.9697743662080938, 1.9679177380939228, 1.970069842972779, 1.9642991767141031, 1.959963994660401, 1.9552476819121387, 1.96036544202388, 1.9526766783302445, 1.9499869031467656, 1.9488292609529543, 1.9529337730313756, 1.9505981531832215, 1.9445122441243263, 1.9469438141398439, 1.9431504216687432, 1.942757488471534, 1.9389486258057342, 1.9339828195634539, 1.9393184257454081, 1.9302743072384487, 1.9284956065696253, 1.9242158259077025, 1.9249000253740007, 1.919937214240652, 1.9172873232752232, 1.9253556399509824, 1.9209569357886103, 1.9143022541537857, 1.909327613309099, 1.9350947959865452, 1.9158122028623308, 1.9078822073286585, 1.9019176621350944, 1.908036397008473, 1.9026206557582361, 1.9112187443891377, 1.8975282330035381, 1.8985944528297838, 1.901801825352686, 1.9032402044446597, 1.8915956073206635, 1.879528854672349, 1.8977188829130727, 1.9122719574835891, 1.874442595370689, 1.8718128717200118, 1.8699496884651372, 1.8656126902804195, 1.8585820507337698, 1.8842381730259736, 1.8567658692157913, 1.8560567005906004, 1.8604509525111157, 1.8459563691823548, 1.8530289058027596, 1.8536836950258277, 1.8503489676367473, 1.8411153329808527, 1.8303647129406482, 1.8206486104940154, 1.8223553224541675, 1.8125380101462303, 1.822278307380739, 1.816479957162453, 1.819111734579741, 1.8154514883147868, 1.821019933533003, 1.7965166234030512, 1.8081503994946408, 1.7981083735652355, 1.8081833167224879, 1.7974594492826164, 1.7886035405160563, 1.808433965117669, 1.8424432113252838, 1.8277291468603074, 1.7797376559481441, 1.7808685249882965, 1.7767165341400748, 1.795439104532765, 1.8013845363078251, 1.7968223592134924, 1.7796483294325705, 1.7664169515490729, 1.8082822347900942, 1.8027195407839245, 1.7610896598725092, 1.7571586746300383, 1.7769097058448102, 1.7919288908906759, 1.7606635164157511, 1.7582474326461015, 1.7620489497490117, 1.7475769102867014, 1.7695745198401716, 1.7515193801403828, 1.7389379448099873, 1.7444741123024075, 1.7445242058467396, 1.7450089979250052, 1.7414845159684105, 1.7497475664016648, 1.7446681726938007, 1.7436427508277454, 1.7404195782782017, 1.725243134843109, 1.74142233256636, 1.7346911471465538, 1.7260823189135648, 1.726005036255409, 1.7267833940305537, 1.7299253461004673, 1.719538249797226, 1.7216671525159688, 1.7156121437185503, 1.7167461041746468, 1.7196907762236195, 1.7115682651256692, 1.7137793480664834, 1.7088834470128778, 1.7135462496668248, 1.7475425764453432, 1.7098499961283016, 1.7179515371573186, 1.7091952646502917, 1.7093652112926365, 1.706056413979366, 1.7040250138891937, 1.7049830406170172, 1.699887538386879, 1.7161539015903067, 1.700163873545642, 1.7024250562946588, 1.7079913782564486, 1.713284126056239, 1.6985476226446468, 1.6910906296058241, 1.691149777966767, 1.7228516316766223, 1.6948138471503171, 1.6951406168428744, 1.694262552143905, 1.6919596437945938, 1.6885708762311387, 1.6837152522577246, 1.693980083089744, 1.6920887689872328, 1.6846577318626867, 1.688818424010316, 1.7024124310717403, 1.6861309230033987, 1.681322586947474, 1.6850074537477666, 1.6812800987209202, 1.697901930910809, 1.6933228977403814, 1.6821462710698445, 1.6762588495887167, 1.675432577704757, 1.6758081039967403, 1.677015581741709, 1.698508825208166], 'val_acc': [0.06568144498873128, 0.08866995033519022, 0.08866995033519022, 0.08866995033519022, 0.08866995033519022, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10180623892982213, 0.10344827515248986, 0.11494252802605308, 0.13464696181720898, 0.13464696181720898, 0.14449917846810445, 0.15927750368913016, 0.1494252868424887, 0.15435139541261889, 0.15927750349338418, 0.15599343143954067, 0.1642036119656414, 0.16256157593871964, 0.16584564818830913, 0.1871921179085138, 0.17569786483920463, 0.17733990145336426, 0.1756978653285695, 0.18062397291996993, 0.18555008197946501, 0.19704433485302825, 0.19047619035384925, 0.2068965516996697, 0.2036124771745334, 0.20525451329932815, 0.20853858564679062, 0.21018062394925918, 0.21510673004809663, 0.2183908023955591, 0.22003283842248086, 0.21839080249343207, 0.21839080408386802, 0.23481116383925252, 0.238095236088842, 0.2413793101246134, 0.2331691292070207, 0.23809523638246094, 0.24302134475684517, 0.2610837438178963, 0.2660098500146067, 0.26108374164022247, 0.2512315250872, 0.2725779945137857, 0.2709359584868639, 0.2692939222641962, 0.2545155971410435, 0.28243021184979206, 0.275862068941049, 0.2840722474852219, 0.2791461394044566, 0.2643678160674858, 0.28243021145830016, 0.2692939222641962, 0.28899835595747914, 0.2873563198326844, 0.28078817552925134, 0.29064039218014687, 0.2922824282070686, 0.2873563198326844, 0.29392446442973635, 0.2955665004566581, 0.28571428390363557, 0.29720853658145285, 0.29228242810919564, 0.27586206686124815, 0.28407224768096784, 0.30213464515158306, 0.28078817503988646, 0.28899835595747914, 0.2840722475830949, 0.29228242810919564, 0.2972085363857069, 0.30541871749904553, 0.29064039218014687, 0.3037766813742508, 0.2922824282070686, 0.3037766813742508, 0.3070607534280943, 0.3070607536238403, 0.30049260892891533, 0.30049260922253423, 0.3119868619982245, 0.3103448257755568, 0.3070607539174592, 0.3037766813742508, 0.30541871749904553, 0.2988505730977395, 0.3103448260691757, 0.30541871759691847, 0.2955665006524041, 0.308702789846508, 0.3119868621939704, 0.3004926093204072, 0.31362889812301925, 0.3136288983187652, 0.3119868619982245, 0.30870278994438094, 0.31691297076410063, 0.31362889822089224, 0.3119868622918434, 0.3119868620960975, 0.30541871769479145, 0.3185550067910224, 0.31855500669314946, 0.32183907884486596, 0.3152709345414329, 0.31527093444355997, 0.3201970429158172, 0.3201970429158172, 0.3201970428179442, 0.32183907884486596, 0.3185550067910224, 0.3185550068888954, 0.3251231512902014, 0.32183907923635785, 0.3251231512902014, 0.3251231512902014, 0.32676518751286915, 0.33333333191417513, 0.32676518741499616, 0.33169129598512637, 0.3234811154590256, 0.33497536823471585, 0.3333333320120481, 0.3316912958872534, 0.3201970430136901, 0.33825944038643235, 0.3366174042616376, 0.3415435127338948, 0.33825944038643235, 0.33825944048430534, 0.34154351292964075, 0.3481116571352008, 0.33825944038643235, 0.33333333191417513, 0.33825944028855937, 0.3399014766091001, 0.34154351244027586, 0.34154351263602184, 0.34646962101040607, 0.32840722334404493, 0.3382594401906864, 0.3481116572330738, 0.3464696209125331, 0.3431855486629436, 0.36124794623143175, 0.33661740406589163, 0.3464696209125331, 0.3448275851792303, 0.34646962101040607, 0.3645320186767672, 0.359605910106637, 0.36617405470368897, 0.33333333171842916, 0.33333333191417513, 0.3579638738839693, 0.36124794623143175, 0.3579638737860963, 0.3579638742754612, 0.35960591020451, 0.3448275849834843, 0.35796387417758824, 0.3645320186767672, 0.3694581273447704, 0.3678160909263567, 0.35467980222161766, 0.34975369345574153, 0.34646962140189796, 0.34646962140189796, 0.3612479465250507, 0.36617405480156195, 0.3645320186767672, 0.36288998255197247, 0.36945812705115144, 0.36617405480156195, 0.35139572967840926, 0.353037765901077, 0.3563218379549205, 0.36617405489943494, 0.3448275851792303, 0.35960591040025597, 0.3711001631759462, 0.38423645217430413, 0.37602627145245743, 0.3612479464271777, 0.3349753680389699, 0.3448275851792303, 0.37766830767512516, 0.37766830787087113, 0.37274219930074093, 0.3678160905348648, 0.37274219910499495, 0.3678160909263567, 0.3875205245217666, 0.3875205245217666, 0.3513957294826633, 0.36945812695327845, 0.3858784882990989, 0.3957307052436133, 0.3760262715503304, 0.35796387398184226, 0.3891625609401803, 0.40065681371587053, 0.3875205244238936, 0.39408866921669156, 0.3678160908284837, 0.36945812695327845, 0.38587848839697186, 0.41050903066038497, 0.3825944161473824, 0.38587848859271784, 0.3940886690209456, 0.40229885003641125, 0.37274219920286794, 0.3793103438977929, 0.3924466331897698, 0.39408866921669156, 0.37766830767512516, 0.39573070573297825, 0.39737274146628104, 0.39080459667348316, 0.3908045968692291, 0.39408866931456454, 0.3940886690209456, 0.403940886259079, 0.3990147774932028, 0.40229885003641125, 0.40065681371587053, 0.39737274136840806, 0.40229885003641125, 0.4006568140094895, 0.3924466327004049, 0.3793103437020469, 0.39737274146628104, 0.41050903066038497, 0.4072249582150495, 0.4006568135201246, 0.3957307051457404, 0.40558292228600074, 0.4121510665894338, 0.40558292209025476, 0.4121510666873067, 0.4203612474091534, 0.4039408858675871, 0.3957307051457404, 0.41050903066038497, 0.42364531956087, 0.4039408858675871, 0.40229884974279234, 0.4121510667851797, 0.4154351388390233, 0.39737274117266214, 0.4203612472134075, 0.403940886063333, 0.4154351390347692, 0.4121510665894338, 0.41871921108861276, 0.4088669945355902, 0.41379310271422853, 0.41379310290997445, 0.3957307051457404, 0.41543513893689626, 0.42200328333820225, 0.417077175061691, 0.41379310261635555, 0.4072249578235576, 0.40065681332437864, 0.42200328343607524, 0.4203612474091534, 0.4203612474091534, 0.4203612474091534, 0.41050903046463905, 0.4072249580193036], 'loss': [2.7131312685580715, 2.7024658387201765, 2.6938995556175342, 2.686886324187324, 2.6812960058756676, 2.677031120480453, 2.6739574540077538, 2.6714794198100815, 2.6697486730571645, 2.6681430487662126, 2.666774475794798, 2.6656979523645044, 2.6644630853155555, 2.663547094352926, 2.662701817949205, 2.6622570571468596, 2.6614615698859434, 2.6611773685752977, 2.660639863533161, 2.660240229835745, 2.659789745029238, 2.6594076955587713, 2.6590461463654065, 2.658379920454241, 2.6574525434623264, 2.6563275987362713, 2.6549160444271394, 2.6519559204211225, 2.6475294784843553, 2.6395248996648455, 2.626099846054641, 2.6063816927296917, 2.5865157084298573, 2.567399406628932, 2.5530811976603167, 2.540974097868745, 2.5310946297107044, 2.523945536114107, 2.5077490876097945, 2.5008912208878282, 2.4966763378903116, 2.489388791148912, 2.4848754352123095, 2.4807771889336054, 2.474603700441991, 2.467619369945487, 2.4667129903358602, 2.4551739768081133, 2.445983217775944, 2.436762524140689, 2.429318028260061, 2.4306719147449156, 2.4194898034512873, 2.406602472642119, 2.3966859863524075, 2.3893886784508487, 2.3854017820691182, 2.3698789587745432, 2.362680924256969, 2.3568500799809637, 2.342620650062326, 2.3375062669327127, 2.3305167228534236, 2.319276155240727, 2.308063800427948, 2.2993339345685264, 2.2877247081889753, 2.2756855598465373, 2.2704259568905685, 2.265915662894748, 2.249396980371808, 2.2417301127063665, 2.2343901138775646, 2.2287425563075947, 2.22664515418928, 2.2287675316084092, 2.2182814068373222, 2.207376860201481, 2.194732956621926, 2.191006864120828, 2.1897447903298253, 2.185611524229422, 2.1766213475801126, 2.170093480174791, 2.1616458965278014, 2.1546421395679762, 2.1470957201853915, 2.1414041364462224, 2.141596828985508, 2.134541669763334, 2.129776755691309, 2.133296365463758, 2.134725635203493, 2.119126300106793, 2.1225093814137046, 2.117782842060379, 2.1015920857384462, 2.0975668840584567, 2.100659929506588, 2.095500703807729, 2.103156051498664, 2.094730589277201, 2.0814537343058497, 2.079633082305626, 2.0721144355053287, 2.0670984587385424, 2.066175722341518, 2.062026450472446, 2.059352298928482, 2.05860321135002, 2.0527932297277744, 2.0518640987192582, 2.0477691013954993, 2.045703926223504, 2.041599508771172, 2.0416184594254227, 2.0345185359155864, 2.0318781652979294, 2.0297797848311783, 2.028329801657361, 2.022677349016162, 2.0232066764479057, 2.019056529430883, 2.0170061787051097, 2.0189948076095425, 2.014152099758203, 2.018409452643972, 2.016820959241973, 2.0211209285430596, 2.010950177993618, 2.0124991181939533, 2.0027132083258343, 2.004350433702097, 1.9929202548287488, 1.9925403214578021, 1.9907109154812854, 1.9895213004744763, 1.981594730255785, 1.9809461003701054, 1.9808619249038384, 1.9791830325763085, 1.978247065563711, 1.9743311284259115, 1.9693323521153883, 1.9663217767308134, 1.964812186317522, 1.9634381580646523, 1.9599884559976002, 1.9567476375881407, 1.9544920836141222, 1.9544661472465468, 1.9505922465843342, 1.9511222115287545, 1.9478090742530274, 1.940207019136182, 1.9399730723007014, 1.9383683717226345, 1.9366539782322407, 1.9300574875465408, 1.9429791595411987, 1.9395809633775902, 1.9298014976650293, 1.9250591255066576, 1.9234372285357246, 1.9192278839969048, 1.9232815510438452, 1.916557329048611, 1.9098825626549534, 1.9278852788329859, 1.9122885633543043, 1.914677656113, 1.8990806917390295, 1.8962840008295048, 1.8955412911683382, 1.8997897984310832, 1.894459453696343, 1.9009459813272684, 1.8975934028625487, 1.8976910978861659, 1.8954066978098185, 1.8783554990188787, 1.8846539028371383, 1.8778954965622763, 1.8677713567471357, 1.8734588372878715, 1.8582046727135442, 1.856419184956952, 1.8495992463716981, 1.8503266635127136, 1.8478646944190933, 1.8371579437530017, 1.8485603787326226, 1.8440198607023737, 1.8335057110757065, 1.8370066888033731, 1.8245970914496044, 1.8257649619966072, 1.813487087236048, 1.8103922242746215, 1.804453720400221, 1.8028071002059405, 1.8066393748445921, 1.8013066036745264, 1.8054451696681781, 1.7996338089388744, 1.7966419643445182, 1.7901087967522091, 1.7911478943893306, 1.7928369575946972, 1.7902705197951143, 1.7950912966620507, 1.7851189757764216, 1.7930724002986962, 1.801316738863011, 1.7874941660393435, 1.7674538539420408, 1.7793811968954192, 1.764526475430514, 1.7801176757048778, 1.7787262797600434, 1.7879518019834828, 1.763899222438585, 1.7565472248888114, 1.767481691244936, 1.760503263297267, 1.765001386098059, 1.7709165804684774, 1.7554156169264714, 1.7522555504001875, 1.746989550189071, 1.7470762826089252, 1.7434667390474794, 1.7386604174451417, 1.7378527691721672, 1.736215252357342, 1.7282715571244884, 1.7281214346386324, 1.723471356564234, 1.7202387331937128, 1.733137707984423, 1.7360998979094582, 1.735256186354087, 1.7194679030157947, 1.7229828764525772, 1.7145645459329812, 1.715004638626835, 1.712139258492409, 1.7172190132571932, 1.7162422680512102, 1.7038359989620577, 1.7071860048070826, 1.7059352998616024, 1.7017358728992376, 1.709748808706076, 1.7014711608142579, 1.7030388658785967, 1.6982401292182092, 1.70188536800643, 1.702014204021352, 1.700188715404064, 1.7076164697474767, 1.693585371285738, 1.7048711508451302, 1.7052629506073937, 1.6968046316131185, 1.6911176366238134, 1.688197235992557, 1.6856806291447037, 1.68425329877121, 1.6873254477365796, 1.691652739855788, 1.6872256725475774, 1.6889278732041313, 1.6909452666981755, 1.6805667755295364, 1.6813568739920426, 1.6905692725700519, 1.6902415518398402, 1.675923074246432, 1.6805066675620892, 1.6893062466713438, 1.6790893820521768, 1.6758330952215488, 1.676602899954794, 1.6828245510066069, 1.6766159351846275, 1.6834580629023683, 1.6790091480807356, 1.679399387498656, 1.6711950600759204, 1.666214290340823, 1.6625272187364175, 1.6628866426264237, 1.6712059117930136, 1.6694783121653407, 1.6667094031888112, 1.665832662631354, 1.6644242743936652, 1.6598520357261204, 1.6595893691452621], 'acc': [0.06652977428641897, 0.06652977389476628, 0.08911704326190009, 0.08911704287942675, 0.08911704345772643, 0.09774127364219826, 0.1022587274501456, 0.10225872704931353, 0.10225872664848147, 0.10225872643429641, 0.10225872704013417, 0.10225872684430783, 0.10225872683512846, 0.10225872644347576, 0.10225872683512846, 0.10225872723596052, 0.10225872742260751, 0.10225872644347576, 0.10225872703095482, 0.10225872644347576, 0.10225872682594911, 0.10225872704013417, 0.10225872723596052, 0.10225872742260751, 0.10225872644347576, 0.10225872704013417, 0.10225872703095482, 0.10266940485403034, 0.10349075949351157, 0.10965092414703212, 0.12114989687468726, 0.13963038987447593, 0.14784394360665667, 0.15318275183990016, 0.15893223731791947, 0.1544147850857623, 0.15646817221533835, 0.1597535933373645, 0.1601642717387397, 0.16139630380964376, 0.16632443481647013, 0.17782340814078368, 0.17453798796117184, 0.17494866440428355, 0.16960985734599818, 0.18028747383084384, 0.18234086272285704, 0.19466118985744962, 0.18973305885062325, 0.19589322490246633, 0.20533880968970195, 0.20821355125375351, 0.2049281318941645, 0.2078028740273364, 0.21642710420262887, 0.21642710518176062, 0.22217659222639072, 0.22587268900822322, 0.22997946518892134, 0.22299794685669258, 0.23942505060035346, 0.23696098512447836, 0.24229979431849485, 0.23983573056833946, 0.2447638592619671, 0.2464065701075403, 0.25297741356326814, 0.2533880909855116, 0.2636550298706462, 0.2546201230564157, 0.26283367776772815, 0.27022587430550576, 0.27720739263773453, 0.27063655016113847, 0.26406570907368554, 0.2648870642914664, 0.27433264751209124, 0.27720739005527456, 0.27597536076265683, 0.2899383962154388, 0.27926078209886807, 0.28090349100453654, 0.28213552581700946, 0.28829568554733326, 0.2907597554048229, 0.2915811110326152, 0.2907597536056683, 0.2895277187931954, 0.2965092388878613, 0.29445585236412297, 0.29609856166144416, 0.29199178567657236, 0.29445585040585953, 0.2932238213090681, 0.2989733081578719, 0.2940451722003106, 0.29979466261070614, 0.29691991552679936, 0.30595482449511974, 0.30143737187130987, 0.2985626279573421, 0.2973305945156536, 0.3166324429565876, 0.2969199172892365, 0.31622176357608067, 0.31047227927301946, 0.3133470216203764, 0.30390143522981256, 0.3158110900703642, 0.3080082141520796, 0.3075975365340098, 0.312114989353646, 0.3211498979303137, 0.31211498817868794, 0.32032854426078483, 0.30924024641881, 0.31581108674131625, 0.3232032856290101, 0.3154004120973591, 0.3178644742441863, 0.32032854014843154, 0.3190965082733538, 0.31950718608725, 0.31786447444001265, 0.32525667450266454, 0.3248459972762474, 0.3203285428900004, 0.3248459968845947, 0.32114989655952925, 0.31909651121074906, 0.32484599512215756, 0.32607803052210954, 0.32279260801094023, 0.32977412513149346, 0.33059548295009306, 0.3322381924432406, 0.33223819303071966, 0.33347022529745, 0.3359342935516115, 0.33388090248714974, 0.3297741290847379, 0.3326488700613104, 0.3379876778845425, 0.33511293831547184, 0.3347022567441576, 0.33880903531148937, 0.3379876778845425, 0.34250513524490217, 0.33880903331650847, 0.33716632264840285, 0.3433264910685208, 0.3404517435929614, 0.34045174437626674, 0.3408624210152048, 0.3383983553435034, 0.3425051330908123, 0.3449691989583401, 0.3462012318125496, 0.3462012334158778, 0.3437371684907643, 0.3400410679698725, 0.34045174378878773, 0.34784394189317613, 0.34168377805049904, 0.35195072104798697, 0.34866529657855416, 0.3503080066224633, 0.3494866527571081, 0.34332649067686816, 0.3457905524320426, 0.35030800701411596, 0.3593429150033046, 0.3548254621836684, 0.35441478613220934, 0.355236140621761, 0.36221766009223044, 0.347843943300678, 0.3593429150033046, 0.34989732900439346, 0.3581108827365742, 0.36262833692699487, 0.3589322387560192, 0.3700205352272096, 0.3663244372887778, 0.36427104821929696, 0.3667351113452559, 0.3737166322232272, 0.3716632451120099, 0.37700205516276664, 0.3761806985191251, 0.3761806981274724, 0.37330595460515736, 0.373305953821852, 0.37782340722896723, 0.37905544204144015, 0.3790554406706557, 0.37864476364006494, 0.38275154197485295, 0.3905544169140058, 0.3848049288902439, 0.38521560611666106, 0.38357289443270626, 0.38850102682867577, 0.38275154001658945, 0.38644763995000225, 0.3868583152181559, 0.3868583156098086, 0.3831622160680485, 0.38275153844997867, 0.38069815329702483, 0.3827515391965666, 0.38603696154862704, 0.38316221587222216, 0.3843942492771932, 0.39137577234597176, 0.3979466103185618, 0.38767967143342724, 0.4012320338088629, 0.390554412764935, 0.39219712581967425, 0.3860369623319324, 0.39753593266377457, 0.3934291590655364, 0.3843942516638268, 0.4016427108394537, 0.3934291594571891, 0.3909650917537893, 0.40123203165477306, 0.3946611901573087, 0.4057494846335182, 0.39671457981426855, 0.40082135497911753, 0.40287474111120314, 0.4065708436270759, 0.4020533894366552, 0.40821355429518147, 0.4110882958225156, 0.41232032985168315, 0.4147843957559284, 0.41149897148232195, 0.404106774748718, 0.41478439461768773, 0.41765913911913455, 0.40780287569797996, 0.41149897383223816, 0.4156057482504992, 0.42217659111874795, 0.4127310070781003, 0.42053387986316326, 0.4176591363408482, 0.41683778227966667, 0.41889116997836306, 0.4250513324869732, 0.41971252638946077, 0.42176591510400635, 0.42012320342005155, 0.4184804921644669, 0.42340862495208914, 0.4156057510287855, 0.4188911682159259, 0.4151950732148893, 0.42710472390637017, 0.42135523431599753, 0.41765913555754286, 0.42381930335346435, 0.41971252462702363, 0.42053388142977405, 0.42340862299382565, 0.42546201206330647, 0.4275154001536555, 0.4201232056108588, 0.4266940473041495, 0.4275154007411346, 0.4308008232890213, 0.4250513365993265, 0.4308008197274296, 0.42422998120407795, 0.4229979457674085, 0.4295687866406764, 0.4291581100751732, 0.42176591490818, 0.43162217848844353, 0.42792607937505356, 0.4373716647864857, 0.42546201268750294, 0.4299794680528817, 0.4328542089927368, 0.4390143738512631, 0.4332648863782628, 0.43367556422887643, 0.43942505244846464, 0.4402464055305144, 0.43860369662484594, 0.43039014602588677, 0.43203285493155524, 0.4344969216558233, 0.438603697212325, 0.4332648880183085, 0.4451745379264839, 0.4312115002828947]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
