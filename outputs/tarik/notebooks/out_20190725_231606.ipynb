{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf1.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.25 23:16:06 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': '0', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['my', 'by', 'ek', 'sg', 'eo', 'eb', 'ds', 'yd', 'eg', 'mb', 'ib', 'ce', 'sk', 'aa', 'ck'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002E282173DD8>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000002E2DB576EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7155, Accuracy:0.0727, Validation Loss:2.7086, Validation Accuracy:0.1117\n",
    "Epoch #2: Loss:2.7069, Accuracy:0.1068, Validation Loss:2.7010, Validation Accuracy:0.1084\n",
    "Epoch #3: Loss:2.6999, Accuracy:0.1097, Validation Loss:2.6956, Validation Accuracy:0.1117\n",
    "Epoch #4: Loss:2.6941, Accuracy:0.1027, Validation Loss:2.6903, Validation Accuracy:0.1149\n",
    "Epoch #5: Loss:2.6887, Accuracy:0.1055, Validation Loss:2.6853, Validation Accuracy:0.1117\n",
    "Epoch #6: Loss:2.6834, Accuracy:0.1101, Validation Loss:2.6811, Validation Accuracy:0.1182\n",
    "Epoch #7: Loss:2.6792, Accuracy:0.1072, Validation Loss:2.6771, Validation Accuracy:0.1084\n",
    "Epoch #8: Loss:2.6757, Accuracy:0.1047, Validation Loss:2.6734, Validation Accuracy:0.1018\n",
    "Epoch #9: Loss:2.6700, Accuracy:0.1027, Validation Loss:2.6690, Validation Accuracy:0.1018\n",
    "Epoch #10: Loss:2.6661, Accuracy:0.1023, Validation Loss:2.6643, Validation Accuracy:0.1018\n",
    "Epoch #11: Loss:2.6600, Accuracy:0.1039, Validation Loss:2.6601, Validation Accuracy:0.1002\n",
    "Epoch #12: Loss:2.6540, Accuracy:0.1084, Validation Loss:2.6519, Validation Accuracy:0.1100\n",
    "Epoch #13: Loss:2.6452, Accuracy:0.1109, Validation Loss:2.6437, Validation Accuracy:0.1149\n",
    "Epoch #14: Loss:2.6343, Accuracy:0.1265, Validation Loss:2.6302, Validation Accuracy:0.1297\n",
    "Epoch #15: Loss:2.6173, Accuracy:0.1265, Validation Loss:2.6110, Validation Accuracy:0.1248\n",
    "Epoch #16: Loss:2.5958, Accuracy:0.1425, Validation Loss:2.6002, Validation Accuracy:0.1429\n",
    "Epoch #17: Loss:2.5761, Accuracy:0.1507, Validation Loss:2.5815, Validation Accuracy:0.1445\n",
    "Epoch #18: Loss:2.5510, Accuracy:0.1643, Validation Loss:2.5415, Validation Accuracy:0.1905\n",
    "Epoch #19: Loss:2.5160, Accuracy:0.1877, Validation Loss:2.5376, Validation Accuracy:0.2200\n",
    "Epoch #20: Loss:2.4970, Accuracy:0.2037, Validation Loss:2.4926, Validation Accuracy:0.2167\n",
    "Epoch #21: Loss:2.4611, Accuracy:0.2119, Validation Loss:2.4836, Validation Accuracy:0.2053\n",
    "Epoch #22: Loss:2.4505, Accuracy:0.2049, Validation Loss:2.4818, Validation Accuracy:0.1954\n",
    "Epoch #23: Loss:2.4517, Accuracy:0.2021, Validation Loss:2.4995, Validation Accuracy:0.1888\n",
    "Epoch #24: Loss:2.4474, Accuracy:0.1938, Validation Loss:2.4854, Validation Accuracy:0.1938\n",
    "Epoch #25: Loss:2.4342, Accuracy:0.2053, Validation Loss:2.5148, Validation Accuracy:0.1905\n",
    "Epoch #26: Loss:2.4455, Accuracy:0.1951, Validation Loss:2.4981, Validation Accuracy:0.1658\n",
    "Epoch #27: Loss:2.4107, Accuracy:0.1975, Validation Loss:2.4355, Validation Accuracy:0.2020\n",
    "Epoch #28: Loss:2.3682, Accuracy:0.2099, Validation Loss:2.4385, Validation Accuracy:0.1987\n",
    "Epoch #29: Loss:2.3535, Accuracy:0.2127, Validation Loss:2.4106, Validation Accuracy:0.2135\n",
    "Epoch #30: Loss:2.3415, Accuracy:0.2152, Validation Loss:2.3850, Validation Accuracy:0.2348\n",
    "Epoch #31: Loss:2.3576, Accuracy:0.2119, Validation Loss:2.4651, Validation Accuracy:0.1905\n",
    "Epoch #32: Loss:2.3713, Accuracy:0.1984, Validation Loss:2.4568, Validation Accuracy:0.2036\n",
    "Epoch #33: Loss:2.3419, Accuracy:0.2131, Validation Loss:2.4129, Validation Accuracy:0.2036\n",
    "Epoch #34: Loss:2.3326, Accuracy:0.2152, Validation Loss:2.4048, Validation Accuracy:0.2036\n",
    "Epoch #35: Loss:2.3426, Accuracy:0.2119, Validation Loss:2.3836, Validation Accuracy:0.2118\n",
    "Epoch #36: Loss:2.3045, Accuracy:0.2251, Validation Loss:2.3720, Validation Accuracy:0.2151\n",
    "Epoch #37: Loss:2.2819, Accuracy:0.2292, Validation Loss:2.3585, Validation Accuracy:0.2250\n",
    "Epoch #38: Loss:2.2669, Accuracy:0.2378, Validation Loss:2.3650, Validation Accuracy:0.2184\n",
    "Epoch #39: Loss:2.2891, Accuracy:0.2251, Validation Loss:2.3516, Validation Accuracy:0.2151\n",
    "Epoch #40: Loss:2.2662, Accuracy:0.2407, Validation Loss:2.3435, Validation Accuracy:0.2266\n",
    "Epoch #41: Loss:2.2522, Accuracy:0.2456, Validation Loss:2.3499, Validation Accuracy:0.2217\n",
    "Epoch #42: Loss:2.2610, Accuracy:0.2468, Validation Loss:2.3291, Validation Accuracy:0.2217\n",
    "Epoch #43: Loss:2.2510, Accuracy:0.2472, Validation Loss:2.4643, Validation Accuracy:0.2118\n",
    "Epoch #44: Loss:2.3572, Accuracy:0.2271, Validation Loss:2.4709, Validation Accuracy:0.1872\n",
    "Epoch #45: Loss:2.3663, Accuracy:0.2111, Validation Loss:2.3557, Validation Accuracy:0.2233\n",
    "Epoch #46: Loss:2.2545, Accuracy:0.2419, Validation Loss:2.3272, Validation Accuracy:0.2217\n",
    "Epoch #47: Loss:2.2421, Accuracy:0.2452, Validation Loss:2.3218, Validation Accuracy:0.2381\n",
    "Epoch #48: Loss:2.2441, Accuracy:0.2374, Validation Loss:2.3185, Validation Accuracy:0.2233\n",
    "Epoch #49: Loss:2.2419, Accuracy:0.2439, Validation Loss:2.3180, Validation Accuracy:0.2266\n",
    "Epoch #50: Loss:2.2420, Accuracy:0.2423, Validation Loss:2.3273, Validation Accuracy:0.2397\n",
    "Epoch #51: Loss:2.2276, Accuracy:0.2460, Validation Loss:2.3125, Validation Accuracy:0.2447\n",
    "Epoch #52: Loss:2.2258, Accuracy:0.2563, Validation Loss:2.3466, Validation Accuracy:0.2184\n",
    "Epoch #53: Loss:2.2506, Accuracy:0.2480, Validation Loss:2.4086, Validation Accuracy:0.2102\n",
    "Epoch #54: Loss:2.3196, Accuracy:0.2230, Validation Loss:2.3918, Validation Accuracy:0.2184\n",
    "Epoch #55: Loss:2.2705, Accuracy:0.2308, Validation Loss:2.3224, Validation Accuracy:0.2250\n",
    "Epoch #56: Loss:2.2248, Accuracy:0.2485, Validation Loss:2.2869, Validation Accuracy:0.2365\n",
    "Epoch #57: Loss:2.2101, Accuracy:0.2538, Validation Loss:2.2939, Validation Accuracy:0.2332\n",
    "Epoch #58: Loss:2.2077, Accuracy:0.2505, Validation Loss:2.3591, Validation Accuracy:0.2217\n",
    "Epoch #59: Loss:2.2693, Accuracy:0.2349, Validation Loss:2.3556, Validation Accuracy:0.2167\n",
    "Epoch #60: Loss:2.2630, Accuracy:0.2439, Validation Loss:2.3658, Validation Accuracy:0.2118\n",
    "Epoch #61: Loss:2.2918, Accuracy:0.2267, Validation Loss:2.3081, Validation Accuracy:0.2365\n",
    "Epoch #62: Loss:2.2657, Accuracy:0.2242, Validation Loss:2.3049, Validation Accuracy:0.2348\n",
    "Epoch #63: Loss:2.2335, Accuracy:0.2476, Validation Loss:2.2973, Validation Accuracy:0.2365\n",
    "Epoch #64: Loss:2.2255, Accuracy:0.2472, Validation Loss:2.3117, Validation Accuracy:0.2332\n",
    "Epoch #65: Loss:2.2101, Accuracy:0.2595, Validation Loss:2.3279, Validation Accuracy:0.2332\n",
    "Epoch #66: Loss:2.2129, Accuracy:0.2550, Validation Loss:2.3102, Validation Accuracy:0.2447\n",
    "Epoch #67: Loss:2.1969, Accuracy:0.2628, Validation Loss:2.3025, Validation Accuracy:0.2266\n",
    "Epoch #68: Loss:2.1935, Accuracy:0.2678, Validation Loss:2.3597, Validation Accuracy:0.2266\n",
    "Epoch #69: Loss:2.2033, Accuracy:0.2559, Validation Loss:2.3145, Validation Accuracy:0.2430\n",
    "Epoch #70: Loss:2.1944, Accuracy:0.2579, Validation Loss:2.3085, Validation Accuracy:0.2365\n",
    "Epoch #71: Loss:2.1812, Accuracy:0.2645, Validation Loss:2.3346, Validation Accuracy:0.2266\n",
    "Epoch #72: Loss:2.1858, Accuracy:0.2690, Validation Loss:2.3132, Validation Accuracy:0.2348\n",
    "Epoch #73: Loss:2.1772, Accuracy:0.2719, Validation Loss:2.3199, Validation Accuracy:0.2332\n",
    "Epoch #74: Loss:2.1891, Accuracy:0.2669, Validation Loss:2.3252, Validation Accuracy:0.2315\n",
    "Epoch #75: Loss:2.1921, Accuracy:0.2649, Validation Loss:2.3242, Validation Accuracy:0.2365\n",
    "Epoch #76: Loss:2.1929, Accuracy:0.2600, Validation Loss:2.3794, Validation Accuracy:0.2217\n",
    "Epoch #77: Loss:2.2165, Accuracy:0.2612, Validation Loss:2.3729, Validation Accuracy:0.2282\n",
    "Epoch #78: Loss:2.2273, Accuracy:0.2460, Validation Loss:2.3490, Validation Accuracy:0.2299\n",
    "Epoch #79: Loss:2.2313, Accuracy:0.2439, Validation Loss:2.2883, Validation Accuracy:0.2315\n",
    "Epoch #80: Loss:2.2190, Accuracy:0.2489, Validation Loss:2.3501, Validation Accuracy:0.2151\n",
    "Epoch #81: Loss:2.2138, Accuracy:0.2604, Validation Loss:2.3506, Validation Accuracy:0.2233\n",
    "Epoch #82: Loss:2.2083, Accuracy:0.2538, Validation Loss:2.3551, Validation Accuracy:0.2282\n",
    "Epoch #83: Loss:2.2081, Accuracy:0.2501, Validation Loss:2.3130, Validation Accuracy:0.2053\n",
    "Epoch #84: Loss:2.1764, Accuracy:0.2690, Validation Loss:2.2941, Validation Accuracy:0.2414\n",
    "Epoch #85: Loss:2.1557, Accuracy:0.2801, Validation Loss:2.3053, Validation Accuracy:0.2381\n",
    "Epoch #86: Loss:2.1806, Accuracy:0.2665, Validation Loss:2.3231, Validation Accuracy:0.2200\n",
    "Epoch #87: Loss:2.1897, Accuracy:0.2657, Validation Loss:2.3411, Validation Accuracy:0.2315\n",
    "Epoch #88: Loss:2.2043, Accuracy:0.2608, Validation Loss:2.3411, Validation Accuracy:0.2085\n",
    "Epoch #89: Loss:2.1774, Accuracy:0.2624, Validation Loss:2.3063, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:2.1564, Accuracy:0.2682, Validation Loss:2.2980, Validation Accuracy:0.2332\n",
    "Epoch #91: Loss:2.1559, Accuracy:0.2723, Validation Loss:2.2914, Validation Accuracy:0.2348\n",
    "Epoch #92: Loss:2.1666, Accuracy:0.2657, Validation Loss:2.3108, Validation Accuracy:0.2118\n",
    "Epoch #93: Loss:2.1832, Accuracy:0.2591, Validation Loss:2.2930, Validation Accuracy:0.2447\n",
    "Epoch #94: Loss:2.1559, Accuracy:0.2764, Validation Loss:2.2848, Validation Accuracy:0.2332\n",
    "Epoch #95: Loss:2.1506, Accuracy:0.2723, Validation Loss:2.2852, Validation Accuracy:0.2282\n",
    "Epoch #96: Loss:2.1391, Accuracy:0.2764, Validation Loss:2.2806, Validation Accuracy:0.2266\n",
    "Epoch #97: Loss:2.1506, Accuracy:0.2747, Validation Loss:2.3526, Validation Accuracy:0.2118\n",
    "Epoch #98: Loss:2.1728, Accuracy:0.2637, Validation Loss:2.2906, Validation Accuracy:0.2299\n",
    "Epoch #99: Loss:2.1590, Accuracy:0.2665, Validation Loss:2.2869, Validation Accuracy:0.2365\n",
    "Epoch #100: Loss:2.1667, Accuracy:0.2645, Validation Loss:2.3059, Validation Accuracy:0.2365\n",
    "Epoch #101: Loss:2.1674, Accuracy:0.2649, Validation Loss:2.3333, Validation Accuracy:0.2135\n",
    "Epoch #102: Loss:2.1833, Accuracy:0.2715, Validation Loss:2.3866, Validation Accuracy:0.2069\n",
    "Epoch #103: Loss:2.2043, Accuracy:0.2583, Validation Loss:2.3717, Validation Accuracy:0.2085\n",
    "Epoch #104: Loss:2.2118, Accuracy:0.2522, Validation Loss:2.4186, Validation Accuracy:0.2036\n",
    "Epoch #105: Loss:2.2637, Accuracy:0.2402, Validation Loss:2.3038, Validation Accuracy:0.2200\n",
    "Epoch #106: Loss:2.2159, Accuracy:0.2513, Validation Loss:2.2994, Validation Accuracy:0.2282\n",
    "Epoch #107: Loss:2.1664, Accuracy:0.2739, Validation Loss:2.3107, Validation Accuracy:0.2102\n",
    "Epoch #108: Loss:2.1538, Accuracy:0.2772, Validation Loss:2.3054, Validation Accuracy:0.2184\n",
    "Epoch #109: Loss:2.1429, Accuracy:0.2784, Validation Loss:2.3079, Validation Accuracy:0.2217\n",
    "Epoch #110: Loss:2.1527, Accuracy:0.2797, Validation Loss:2.3061, Validation Accuracy:0.2332\n",
    "Epoch #111: Loss:2.1428, Accuracy:0.2719, Validation Loss:2.3163, Validation Accuracy:0.2233\n",
    "Epoch #112: Loss:2.1627, Accuracy:0.2674, Validation Loss:2.3195, Validation Accuracy:0.2299\n",
    "Epoch #113: Loss:2.1521, Accuracy:0.2690, Validation Loss:2.3583, Validation Accuracy:0.2118\n",
    "Epoch #114: Loss:2.1627, Accuracy:0.2604, Validation Loss:2.3179, Validation Accuracy:0.2135\n",
    "Epoch #115: Loss:2.1605, Accuracy:0.2764, Validation Loss:2.3319, Validation Accuracy:0.2348\n",
    "Epoch #116: Loss:2.1470, Accuracy:0.2756, Validation Loss:2.3035, Validation Accuracy:0.2217\n",
    "Epoch #117: Loss:2.1316, Accuracy:0.2842, Validation Loss:2.2905, Validation Accuracy:0.2381\n",
    "Epoch #118: Loss:2.1289, Accuracy:0.2858, Validation Loss:2.2977, Validation Accuracy:0.2233\n",
    "Epoch #119: Loss:2.1331, Accuracy:0.2797, Validation Loss:2.3288, Validation Accuracy:0.2447\n",
    "Epoch #120: Loss:2.1446, Accuracy:0.2723, Validation Loss:2.3115, Validation Accuracy:0.2151\n",
    "Epoch #121: Loss:2.1432, Accuracy:0.2694, Validation Loss:2.3110, Validation Accuracy:0.2479\n",
    "Epoch #122: Loss:2.1301, Accuracy:0.2834, Validation Loss:2.2829, Validation Accuracy:0.2299\n",
    "Epoch #123: Loss:2.1239, Accuracy:0.2916, Validation Loss:2.3018, Validation Accuracy:0.2348\n",
    "Epoch #124: Loss:2.1280, Accuracy:0.2797, Validation Loss:2.2904, Validation Accuracy:0.2365\n",
    "Epoch #125: Loss:2.1037, Accuracy:0.2871, Validation Loss:2.2968, Validation Accuracy:0.2381\n",
    "Epoch #126: Loss:2.1056, Accuracy:0.2908, Validation Loss:2.2855, Validation Accuracy:0.2348\n",
    "Epoch #127: Loss:2.1228, Accuracy:0.2838, Validation Loss:2.2934, Validation Accuracy:0.2184\n",
    "Epoch #128: Loss:2.1140, Accuracy:0.2842, Validation Loss:2.2942, Validation Accuracy:0.2315\n",
    "Epoch #129: Loss:2.1305, Accuracy:0.2850, Validation Loss:2.2624, Validation Accuracy:0.2430\n",
    "Epoch #130: Loss:2.0988, Accuracy:0.2916, Validation Loss:2.2665, Validation Accuracy:0.2315\n",
    "Epoch #131: Loss:2.0996, Accuracy:0.2875, Validation Loss:2.2780, Validation Accuracy:0.2315\n",
    "Epoch #132: Loss:2.1154, Accuracy:0.2809, Validation Loss:2.3076, Validation Accuracy:0.2282\n",
    "Epoch #133: Loss:2.1308, Accuracy:0.2899, Validation Loss:2.2916, Validation Accuracy:0.2332\n",
    "Epoch #134: Loss:2.1114, Accuracy:0.2854, Validation Loss:2.3050, Validation Accuracy:0.2348\n",
    "Epoch #135: Loss:2.1022, Accuracy:0.2924, Validation Loss:2.3385, Validation Accuracy:0.2151\n",
    "Epoch #136: Loss:2.0962, Accuracy:0.2940, Validation Loss:2.2820, Validation Accuracy:0.2365\n",
    "Epoch #137: Loss:2.0827, Accuracy:0.2932, Validation Loss:2.3011, Validation Accuracy:0.2233\n",
    "Epoch #138: Loss:2.1159, Accuracy:0.2801, Validation Loss:2.3510, Validation Accuracy:0.2266\n",
    "Epoch #139: Loss:2.1944, Accuracy:0.2637, Validation Loss:2.3628, Validation Accuracy:0.2217\n",
    "Epoch #140: Loss:2.2057, Accuracy:0.2595, Validation Loss:2.5207, Validation Accuracy:0.1938\n",
    "Epoch #141: Loss:2.2565, Accuracy:0.2468, Validation Loss:2.3646, Validation Accuracy:0.2102\n",
    "Epoch #142: Loss:2.2271, Accuracy:0.2316, Validation Loss:2.3448, Validation Accuracy:0.2069\n",
    "Epoch #143: Loss:2.2229, Accuracy:0.2509, Validation Loss:2.3129, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:2.1750, Accuracy:0.2665, Validation Loss:2.3084, Validation Accuracy:0.2233\n",
    "Epoch #145: Loss:2.1174, Accuracy:0.2768, Validation Loss:2.3040, Validation Accuracy:0.2250\n",
    "Epoch #146: Loss:2.1311, Accuracy:0.2706, Validation Loss:2.2846, Validation Accuracy:0.2397\n",
    "Epoch #147: Loss:2.1063, Accuracy:0.2842, Validation Loss:2.2970, Validation Accuracy:0.2529\n",
    "Epoch #148: Loss:2.1210, Accuracy:0.2825, Validation Loss:2.3288, Validation Accuracy:0.2250\n",
    "Epoch #149: Loss:2.1312, Accuracy:0.2694, Validation Loss:2.2864, Validation Accuracy:0.2594\n",
    "Epoch #150: Loss:2.1153, Accuracy:0.2825, Validation Loss:2.3241, Validation Accuracy:0.2266\n",
    "Epoch #151: Loss:2.1194, Accuracy:0.2940, Validation Loss:2.3011, Validation Accuracy:0.2365\n",
    "Epoch #152: Loss:2.0954, Accuracy:0.2940, Validation Loss:2.2819, Validation Accuracy:0.2332\n",
    "Epoch #153: Loss:2.0815, Accuracy:0.3035, Validation Loss:2.2964, Validation Accuracy:0.2348\n",
    "Epoch #154: Loss:2.0900, Accuracy:0.2912, Validation Loss:2.2973, Validation Accuracy:0.2250\n",
    "Epoch #155: Loss:2.0869, Accuracy:0.2891, Validation Loss:2.2988, Validation Accuracy:0.2332\n",
    "Epoch #156: Loss:2.0776, Accuracy:0.3055, Validation Loss:2.2944, Validation Accuracy:0.2381\n",
    "Epoch #157: Loss:2.0748, Accuracy:0.3105, Validation Loss:2.3002, Validation Accuracy:0.2250\n",
    "Epoch #158: Loss:2.0812, Accuracy:0.2982, Validation Loss:2.3719, Validation Accuracy:0.2167\n",
    "Epoch #159: Loss:2.1227, Accuracy:0.2924, Validation Loss:2.3187, Validation Accuracy:0.2266\n",
    "Epoch #160: Loss:2.0921, Accuracy:0.2887, Validation Loss:2.2866, Validation Accuracy:0.2529\n",
    "Epoch #161: Loss:2.0834, Accuracy:0.2994, Validation Loss:2.2789, Validation Accuracy:0.2447\n",
    "Epoch #162: Loss:2.0794, Accuracy:0.2969, Validation Loss:2.2856, Validation Accuracy:0.2463\n",
    "Epoch #163: Loss:2.0665, Accuracy:0.3097, Validation Loss:2.3009, Validation Accuracy:0.2397\n",
    "Epoch #164: Loss:2.0645, Accuracy:0.3002, Validation Loss:2.2903, Validation Accuracy:0.2348\n",
    "Epoch #165: Loss:2.0782, Accuracy:0.2895, Validation Loss:2.2916, Validation Accuracy:0.2332\n",
    "Epoch #166: Loss:2.0615, Accuracy:0.2998, Validation Loss:2.2945, Validation Accuracy:0.2365\n",
    "Epoch #167: Loss:2.0676, Accuracy:0.3084, Validation Loss:2.3035, Validation Accuracy:0.2414\n",
    "Epoch #168: Loss:2.0739, Accuracy:0.3064, Validation Loss:2.3002, Validation Accuracy:0.2250\n",
    "Epoch #169: Loss:2.0812, Accuracy:0.3010, Validation Loss:2.3008, Validation Accuracy:0.2430\n",
    "Epoch #170: Loss:2.0601, Accuracy:0.3097, Validation Loss:2.3065, Validation Accuracy:0.2414\n",
    "Epoch #171: Loss:2.0629, Accuracy:0.3060, Validation Loss:2.3168, Validation Accuracy:0.2200\n",
    "Epoch #172: Loss:2.0547, Accuracy:0.3068, Validation Loss:2.3045, Validation Accuracy:0.2512\n",
    "Epoch #173: Loss:2.0503, Accuracy:0.3084, Validation Loss:2.2999, Validation Accuracy:0.2430\n",
    "Epoch #174: Loss:2.0507, Accuracy:0.3039, Validation Loss:2.3298, Validation Accuracy:0.2348\n",
    "Epoch #175: Loss:2.0725, Accuracy:0.3088, Validation Loss:2.3135, Validation Accuracy:0.2217\n",
    "Epoch #176: Loss:2.0522, Accuracy:0.3133, Validation Loss:2.2979, Validation Accuracy:0.2315\n",
    "Epoch #177: Loss:2.0674, Accuracy:0.3105, Validation Loss:2.3076, Validation Accuracy:0.2463\n",
    "Epoch #178: Loss:2.0676, Accuracy:0.3035, Validation Loss:2.2763, Validation Accuracy:0.2299\n",
    "Epoch #179: Loss:2.0925, Accuracy:0.2990, Validation Loss:2.2976, Validation Accuracy:0.2315\n",
    "Epoch #180: Loss:2.0893, Accuracy:0.2899, Validation Loss:2.2939, Validation Accuracy:0.2315\n",
    "Epoch #181: Loss:2.0498, Accuracy:0.3138, Validation Loss:2.2795, Validation Accuracy:0.2447\n",
    "Epoch #182: Loss:2.0443, Accuracy:0.3105, Validation Loss:2.2827, Validation Accuracy:0.2447\n",
    "Epoch #183: Loss:2.0777, Accuracy:0.3014, Validation Loss:2.4062, Validation Accuracy:0.2135\n",
    "Epoch #184: Loss:2.0794, Accuracy:0.3088, Validation Loss:2.3057, Validation Accuracy:0.2365\n",
    "Epoch #185: Loss:2.0624, Accuracy:0.3113, Validation Loss:2.3225, Validation Accuracy:0.2348\n",
    "Epoch #186: Loss:2.0439, Accuracy:0.3187, Validation Loss:2.3221, Validation Accuracy:0.2282\n",
    "Epoch #187: Loss:2.0463, Accuracy:0.3187, Validation Loss:2.3258, Validation Accuracy:0.2282\n",
    "Epoch #188: Loss:2.0774, Accuracy:0.3080, Validation Loss:2.3209, Validation Accuracy:0.2233\n",
    "Epoch #189: Loss:2.0728, Accuracy:0.3051, Validation Loss:2.3123, Validation Accuracy:0.2266\n",
    "Epoch #190: Loss:2.1083, Accuracy:0.2871, Validation Loss:2.3005, Validation Accuracy:0.2447\n",
    "Epoch #191: Loss:2.0656, Accuracy:0.3088, Validation Loss:2.2882, Validation Accuracy:0.2365\n",
    "Epoch #192: Loss:2.0672, Accuracy:0.3031, Validation Loss:2.3317, Validation Accuracy:0.2053\n",
    "Epoch #193: Loss:2.1133, Accuracy:0.2953, Validation Loss:2.3150, Validation Accuracy:0.2397\n",
    "Epoch #194: Loss:2.1081, Accuracy:0.2912, Validation Loss:2.3000, Validation Accuracy:0.2200\n",
    "Epoch #195: Loss:2.1135, Accuracy:0.2969, Validation Loss:2.3305, Validation Accuracy:0.2118\n",
    "Epoch #196: Loss:2.1394, Accuracy:0.2801, Validation Loss:2.3312, Validation Accuracy:0.2282\n",
    "Epoch #197: Loss:2.0934, Accuracy:0.3051, Validation Loss:2.3769, Validation Accuracy:0.2085\n",
    "Epoch #198: Loss:2.0587, Accuracy:0.3195, Validation Loss:2.2954, Validation Accuracy:0.2348\n",
    "Epoch #199: Loss:2.0724, Accuracy:0.3129, Validation Loss:2.2920, Validation Accuracy:0.2315\n",
    "Epoch #200: Loss:2.0516, Accuracy:0.3154, Validation Loss:2.3138, Validation Accuracy:0.2348\n",
    "Epoch #201: Loss:2.0367, Accuracy:0.3162, Validation Loss:2.3079, Validation Accuracy:0.2266\n",
    "Epoch #202: Loss:2.0290, Accuracy:0.3211, Validation Loss:2.3527, Validation Accuracy:0.2167\n",
    "Epoch #203: Loss:2.0175, Accuracy:0.3220, Validation Loss:2.3160, Validation Accuracy:0.2250\n",
    "Epoch #204: Loss:2.0274, Accuracy:0.3207, Validation Loss:2.3381, Validation Accuracy:0.2233\n",
    "Epoch #205: Loss:2.0221, Accuracy:0.3285, Validation Loss:2.3478, Validation Accuracy:0.2200\n",
    "Epoch #206: Loss:2.0319, Accuracy:0.3125, Validation Loss:2.3094, Validation Accuracy:0.2266\n",
    "Epoch #207: Loss:2.0087, Accuracy:0.3281, Validation Loss:2.3207, Validation Accuracy:0.2282\n",
    "Epoch #208: Loss:2.0213, Accuracy:0.3203, Validation Loss:2.3398, Validation Accuracy:0.2184\n",
    "Epoch #209: Loss:2.0007, Accuracy:0.3331, Validation Loss:2.3259, Validation Accuracy:0.2250\n",
    "Epoch #210: Loss:2.0409, Accuracy:0.3203, Validation Loss:2.4017, Validation Accuracy:0.2167\n",
    "Epoch #211: Loss:2.1165, Accuracy:0.2924, Validation Loss:2.4787, Validation Accuracy:0.2085\n",
    "Epoch #212: Loss:2.0826, Accuracy:0.3035, Validation Loss:2.3512, Validation Accuracy:0.2332\n",
    "Epoch #213: Loss:2.0790, Accuracy:0.3088, Validation Loss:2.3477, Validation Accuracy:0.2233\n",
    "Epoch #214: Loss:2.0842, Accuracy:0.2961, Validation Loss:2.3428, Validation Accuracy:0.2266\n",
    "Epoch #215: Loss:2.0681, Accuracy:0.3097, Validation Loss:2.3229, Validation Accuracy:0.2184\n",
    "Epoch #216: Loss:2.0487, Accuracy:0.3203, Validation Loss:2.3510, Validation Accuracy:0.2184\n",
    "Epoch #217: Loss:2.0264, Accuracy:0.3253, Validation Loss:2.3264, Validation Accuracy:0.2332\n",
    "Epoch #218: Loss:2.0041, Accuracy:0.3314, Validation Loss:2.3507, Validation Accuracy:0.2184\n",
    "Epoch #219: Loss:2.0159, Accuracy:0.3224, Validation Loss:2.3317, Validation Accuracy:0.2365\n",
    "Epoch #220: Loss:1.9904, Accuracy:0.3281, Validation Loss:2.3475, Validation Accuracy:0.2250\n",
    "Epoch #221: Loss:2.0213, Accuracy:0.3216, Validation Loss:2.3198, Validation Accuracy:0.2332\n",
    "Epoch #222: Loss:2.0209, Accuracy:0.3154, Validation Loss:2.3421, Validation Accuracy:0.2397\n",
    "Epoch #223: Loss:2.0166, Accuracy:0.3088, Validation Loss:2.3375, Validation Accuracy:0.2381\n",
    "Epoch #224: Loss:2.0004, Accuracy:0.3207, Validation Loss:2.3419, Validation Accuracy:0.2315\n",
    "Epoch #225: Loss:1.9980, Accuracy:0.3244, Validation Loss:2.3459, Validation Accuracy:0.2250\n",
    "Epoch #226: Loss:2.0091, Accuracy:0.3228, Validation Loss:2.3637, Validation Accuracy:0.2217\n",
    "Epoch #227: Loss:2.0198, Accuracy:0.3179, Validation Loss:2.3618, Validation Accuracy:0.2217\n",
    "Epoch #228: Loss:2.0304, Accuracy:0.3158, Validation Loss:2.3503, Validation Accuracy:0.2200\n",
    "Epoch #229: Loss:2.0207, Accuracy:0.3211, Validation Loss:2.3659, Validation Accuracy:0.2135\n",
    "Epoch #230: Loss:2.0214, Accuracy:0.3199, Validation Loss:2.3183, Validation Accuracy:0.2250\n",
    "Epoch #231: Loss:2.0142, Accuracy:0.3166, Validation Loss:2.3220, Validation Accuracy:0.2250\n",
    "Epoch #232: Loss:2.0072, Accuracy:0.3294, Validation Loss:2.3116, Validation Accuracy:0.2266\n",
    "Epoch #233: Loss:2.0049, Accuracy:0.3269, Validation Loss:2.3020, Validation Accuracy:0.2266\n",
    "Epoch #234: Loss:1.9981, Accuracy:0.3203, Validation Loss:2.3120, Validation Accuracy:0.2332\n",
    "Epoch #235: Loss:1.9897, Accuracy:0.3277, Validation Loss:2.3196, Validation Accuracy:0.2282\n",
    "Epoch #236: Loss:1.9751, Accuracy:0.3343, Validation Loss:2.3250, Validation Accuracy:0.2200\n",
    "Epoch #237: Loss:1.9692, Accuracy:0.3405, Validation Loss:2.3451, Validation Accuracy:0.2200\n",
    "Epoch #238: Loss:1.9751, Accuracy:0.3368, Validation Loss:2.3302, Validation Accuracy:0.2299\n",
    "Epoch #239: Loss:1.9962, Accuracy:0.3302, Validation Loss:2.3693, Validation Accuracy:0.2299\n",
    "Epoch #240: Loss:2.0370, Accuracy:0.3146, Validation Loss:2.4575, Validation Accuracy:0.2200\n",
    "Epoch #241: Loss:2.0753, Accuracy:0.2998, Validation Loss:2.5575, Validation Accuracy:0.1888\n",
    "Epoch #242: Loss:2.1004, Accuracy:0.2801, Validation Loss:2.3383, Validation Accuracy:0.2217\n",
    "Epoch #243: Loss:1.9879, Accuracy:0.3216, Validation Loss:2.3210, Validation Accuracy:0.2151\n",
    "Epoch #244: Loss:1.9935, Accuracy:0.3265, Validation Loss:2.3510, Validation Accuracy:0.2348\n",
    "Epoch #245: Loss:1.9714, Accuracy:0.3355, Validation Loss:2.3312, Validation Accuracy:0.2348\n",
    "Epoch #246: Loss:1.9938, Accuracy:0.3203, Validation Loss:2.3500, Validation Accuracy:0.2496\n",
    "Epoch #247: Loss:2.0107, Accuracy:0.3166, Validation Loss:2.3554, Validation Accuracy:0.2397\n",
    "Epoch #248: Loss:1.9915, Accuracy:0.3265, Validation Loss:2.3538, Validation Accuracy:0.2348\n",
    "Epoch #249: Loss:1.9947, Accuracy:0.3216, Validation Loss:2.3562, Validation Accuracy:0.2315\n",
    "Epoch #250: Loss:2.0235, Accuracy:0.3154, Validation Loss:2.3916, Validation Accuracy:0.2200\n",
    "Epoch #251: Loss:1.9989, Accuracy:0.3326, Validation Loss:2.3394, Validation Accuracy:0.2348\n",
    "Epoch #252: Loss:1.9876, Accuracy:0.3298, Validation Loss:2.3461, Validation Accuracy:0.2250\n",
    "Epoch #253: Loss:2.0040, Accuracy:0.3331, Validation Loss:2.3645, Validation Accuracy:0.2217\n",
    "Epoch #254: Loss:1.9864, Accuracy:0.3302, Validation Loss:2.3781, Validation Accuracy:0.2184\n",
    "Epoch #255: Loss:1.9809, Accuracy:0.3384, Validation Loss:2.3661, Validation Accuracy:0.2282\n",
    "Epoch #256: Loss:1.9714, Accuracy:0.3306, Validation Loss:2.3788, Validation Accuracy:0.2266\n",
    "Epoch #257: Loss:1.9670, Accuracy:0.3396, Validation Loss:2.3952, Validation Accuracy:0.2135\n",
    "Epoch #258: Loss:1.9904, Accuracy:0.3339, Validation Loss:2.3691, Validation Accuracy:0.2167\n",
    "Epoch #259: Loss:1.9880, Accuracy:0.3326, Validation Loss:2.3523, Validation Accuracy:0.2348\n",
    "Epoch #260: Loss:1.9815, Accuracy:0.3376, Validation Loss:2.3507, Validation Accuracy:0.2414\n",
    "Epoch #261: Loss:1.9795, Accuracy:0.3544, Validation Loss:2.3340, Validation Accuracy:0.2414\n",
    "Epoch #262: Loss:1.9624, Accuracy:0.3458, Validation Loss:2.3381, Validation Accuracy:0.2332\n",
    "Epoch #263: Loss:1.9630, Accuracy:0.3466, Validation Loss:2.3509, Validation Accuracy:0.2332\n",
    "Epoch #264: Loss:1.9537, Accuracy:0.3466, Validation Loss:2.3417, Validation Accuracy:0.2332\n",
    "Epoch #265: Loss:1.9483, Accuracy:0.3446, Validation Loss:2.3399, Validation Accuracy:0.2299\n",
    "Epoch #266: Loss:1.9873, Accuracy:0.3343, Validation Loss:2.3677, Validation Accuracy:0.2233\n",
    "Epoch #267: Loss:1.9486, Accuracy:0.3515, Validation Loss:2.3940, Validation Accuracy:0.2266\n",
    "Epoch #268: Loss:2.0283, Accuracy:0.3162, Validation Loss:2.3985, Validation Accuracy:0.2102\n",
    "Epoch #269: Loss:2.0152, Accuracy:0.3179, Validation Loss:2.3782, Validation Accuracy:0.2053\n",
    "Epoch #270: Loss:1.9613, Accuracy:0.3405, Validation Loss:2.3487, Validation Accuracy:0.2217\n",
    "Epoch #271: Loss:1.9545, Accuracy:0.3437, Validation Loss:2.3574, Validation Accuracy:0.2233\n",
    "Epoch #272: Loss:1.9481, Accuracy:0.3466, Validation Loss:2.3627, Validation Accuracy:0.2332\n",
    "Epoch #273: Loss:1.9859, Accuracy:0.3318, Validation Loss:2.3556, Validation Accuracy:0.2250\n",
    "Epoch #274: Loss:1.9793, Accuracy:0.3359, Validation Loss:2.3736, Validation Accuracy:0.2250\n",
    "Epoch #275: Loss:1.9506, Accuracy:0.3437, Validation Loss:2.3676, Validation Accuracy:0.2233\n",
    "Epoch #276: Loss:1.9437, Accuracy:0.3487, Validation Loss:2.3792, Validation Accuracy:0.2118\n",
    "Epoch #277: Loss:1.9474, Accuracy:0.3405, Validation Loss:2.3780, Validation Accuracy:0.2315\n",
    "Epoch #278: Loss:1.9639, Accuracy:0.3429, Validation Loss:2.3855, Validation Accuracy:0.2085\n",
    "Epoch #279: Loss:1.9673, Accuracy:0.3425, Validation Loss:2.3773, Validation Accuracy:0.2233\n",
    "Epoch #280: Loss:1.9728, Accuracy:0.3384, Validation Loss:2.3741, Validation Accuracy:0.2299\n",
    "Epoch #281: Loss:1.9858, Accuracy:0.3343, Validation Loss:2.4246, Validation Accuracy:0.2250\n",
    "Epoch #282: Loss:1.9802, Accuracy:0.3294, Validation Loss:2.3426, Validation Accuracy:0.2250\n",
    "Epoch #283: Loss:1.9398, Accuracy:0.3433, Validation Loss:2.3665, Validation Accuracy:0.2282\n",
    "Epoch #284: Loss:1.9653, Accuracy:0.3335, Validation Loss:2.3680, Validation Accuracy:0.2348\n",
    "Epoch #285: Loss:1.9491, Accuracy:0.3396, Validation Loss:2.3228, Validation Accuracy:0.2414\n",
    "Epoch #286: Loss:1.9337, Accuracy:0.3487, Validation Loss:2.3376, Validation Accuracy:0.2315\n",
    "Epoch #287: Loss:1.9273, Accuracy:0.3503, Validation Loss:2.3155, Validation Accuracy:0.2332\n",
    "Epoch #288: Loss:1.9703, Accuracy:0.3376, Validation Loss:2.3593, Validation Accuracy:0.2167\n",
    "Epoch #289: Loss:2.0211, Accuracy:0.3183, Validation Loss:2.3411, Validation Accuracy:0.2217\n",
    "Epoch #290: Loss:2.0152, Accuracy:0.3158, Validation Loss:2.4152, Validation Accuracy:0.2167\n",
    "Epoch #291: Loss:1.9948, Accuracy:0.3310, Validation Loss:2.3726, Validation Accuracy:0.2184\n",
    "Epoch #292: Loss:2.0507, Accuracy:0.3158, Validation Loss:2.4226, Validation Accuracy:0.2085\n",
    "Epoch #293: Loss:2.0161, Accuracy:0.3158, Validation Loss:2.4798, Validation Accuracy:0.2053\n",
    "Epoch #294: Loss:1.9680, Accuracy:0.3409, Validation Loss:2.4309, Validation Accuracy:0.2167\n",
    "Epoch #295: Loss:1.9430, Accuracy:0.3499, Validation Loss:2.4009, Validation Accuracy:0.2217\n",
    "Epoch #296: Loss:1.9233, Accuracy:0.3462, Validation Loss:2.3833, Validation Accuracy:0.2348\n",
    "Epoch #297: Loss:1.9308, Accuracy:0.3483, Validation Loss:2.3888, Validation Accuracy:0.2299\n",
    "Epoch #298: Loss:1.9160, Accuracy:0.3520, Validation Loss:2.3796, Validation Accuracy:0.2381\n",
    "Epoch #299: Loss:1.9199, Accuracy:0.3565, Validation Loss:2.3586, Validation Accuracy:0.2266\n",
    "Epoch #300: Loss:1.9049, Accuracy:0.3598, Validation Loss:2.3491, Validation Accuracy:0.2348\n",
    "\n",
    "Test:\n",
    "Test Loss:2.34913635, Accuracy:0.2348\n",
    "Labels: ['my', 'by', 'ek', 'sg', 'eo', 'eb', 'ds', 'yd', 'eg', 'mb', 'ib', 'ce', 'sk', 'aa', 'ck']\n",
    "Confusion Matrix:\n",
    "      my  by  ek  sg  eo  eb  ds  yd  eg  mb  ib  ce  sk  aa  ck\n",
    "t:my   0   0   7   1   0   3   4   1   2   0   1   0   0   1   0\n",
    "t:by   0   4   0  10  10   0   3   0   9   4   0   0   0   0   0\n",
    "t:ek   1   2   6   5   0  25   1   1   1   3   3   0   0   0   0\n",
    "t:sg   0   1   1  14   5   3   1   8   2  12   3   0   0   0   1\n",
    "t:eo   0   5   0   0  12   0   0   4   8   2   1   0   0   1   1\n",
    "t:eb   0   1   7   1   1  27   1   2   1   7   2   0   0   0   0\n",
    "t:ds   0   1   1   3   1   6   9   1   4   4   1   0   0   0   0\n",
    "t:yd   0   1   1  10   3   7   0  19   1   9  11   0   0   0   0\n",
    "t:eg   0   5   1   3   3   1   2   1  32   1   0   0   0   1   0\n",
    "t:mb   0   1   3   9   3   7   1   7   3  15   2   0   1   0   0\n",
    "t:ib   1   1   6   7   5   2   1  13   3   9   5   0   0   1   0\n",
    "t:ce   0   1   3   2   2   5   3   2   4   3   2   0   0   0   0\n",
    "t:sk   0   1   5   1   0  15   3   1   3   3   0   0   0   1   0\n",
    "t:aa   0   0   3   0   0   5   7   0  15   3   0   0   0   0   1\n",
    "t:ck   0   0   4   2   0   6   4   0   1   4   1   0   0   1   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          my       0.00      0.00      0.00        20\n",
    "          by       0.17      0.10      0.12        40\n",
    "          ek       0.12      0.12      0.12        48\n",
    "          sg       0.21      0.27      0.24        51\n",
    "          eo       0.27      0.35      0.30        34\n",
    "          eb       0.24      0.54      0.33        50\n",
    "          ds       0.23      0.29      0.25        31\n",
    "          yd       0.32      0.31      0.31        62\n",
    "          eg       0.36      0.64      0.46        50\n",
    "          mb       0.19      0.29      0.23        52\n",
    "          ib       0.16      0.09      0.12        54\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ck       0.00      0.00      0.00        23\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.15      0.20      0.17       609\n",
    "weighted avg       0.18      0.23      0.19       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.25 23:32:00 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 53 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.708594478801358, 2.7009942668607865, 2.6955752208315094, 2.69031512600252, 2.685337806178627, 2.681069450425397, 2.6771331968761625, 2.6734234758198556, 2.6689773294921775, 2.664299638009032, 2.660107425476726, 2.65192037654432, 2.6436800674851892, 2.630200115526447, 2.611008276493091, 2.6002364233013835, 2.581471042288544, 2.5415383700666756, 2.5375634484690397, 2.4926231784382087, 2.483610545473146, 2.4818247253280163, 2.4994569834817217, 2.485356451842585, 2.5148439184198237, 2.4981360803488246, 2.4355140241300335, 2.438532468329118, 2.410594134495176, 2.384970896349752, 2.4651120280593095, 2.456776212197415, 2.4128543610251794, 2.404809179368669, 2.383639233452933, 2.3719517663977614, 2.358454953273529, 2.365039442560356, 2.3516344190231098, 2.343482640380734, 2.3498720355417535, 2.3290775515175803, 2.4643170712225153, 2.4708959018851346, 2.35573471551654, 2.327221792906963, 2.321845078898964, 2.318518218148518, 2.318018843583481, 2.327250233620454, 2.3124632001510395, 2.3466300396692183, 2.4085851733516197, 2.3917882148855427, 2.322401645735567, 2.2868810342058956, 2.2938819577541256, 2.359100408350501, 2.3556427352729883, 2.3658427311281853, 2.3080674548845965, 2.304940675475523, 2.2973330706015402, 2.311738401798192, 2.3278613274516338, 2.310208582134278, 2.3024854389904754, 2.3596628499148515, 2.3144573946108764, 2.3084503054031598, 2.334609544140169, 2.3131840749718675, 2.3199377545386506, 2.325151160824279, 2.3242268883340267, 2.3794158029634573, 2.372855116385349, 2.3489594236383295, 2.2882513956874853, 2.350052695751973, 2.3505731494164426, 2.3550788357927295, 2.3130447433891357, 2.2940976842870855, 2.3053130218743885, 2.323109006255327, 2.341113419759841, 2.341080021975663, 2.30625246467653, 2.2980118506451936, 2.2914366714277095, 2.3108489262842387, 2.2929767822397165, 2.28477478888626, 2.2851781340068196, 2.2805711800241704, 2.3525620227181068, 2.2905903512425416, 2.2868652257621775, 2.305886862508965, 2.333332390620791, 2.3866040996338542, 2.3716622679104358, 2.4185759351758533, 2.303752636087352, 2.299378305037425, 2.310706514443083, 2.3054466920924694, 2.3078628893947757, 2.3061325162502344, 2.316342650180184, 2.3195319136571024, 2.358332058870538, 2.317856681366468, 2.331931769554251, 2.303463520870616, 2.2904664608841068, 2.297671822687284, 2.3287833022758098, 2.311501073132595, 2.310976940227064, 2.2829377772577093, 2.3018116939244013, 2.2904075871547454, 2.2968226403046907, 2.2854870446209836, 2.293362631586385, 2.2941798784071197, 2.2624122207779407, 2.2664999781766744, 2.277963113315, 2.307628953006663, 2.2916281646108394, 2.304959345725174, 2.3385443115860762, 2.282020667895112, 2.3011149800273976, 2.351022114307422, 2.3627668715071404, 2.5206641669343846, 2.364575479613932, 2.3448322872418683, 2.3128663154658424, 2.3083622345979187, 2.304021793437513, 2.284599497204735, 2.297021214402172, 2.328844764158252, 2.2864201354667277, 2.324074563917464, 2.301050632066523, 2.281926959606227, 2.2963607475675385, 2.2973338923430795, 2.2987767812262225, 2.29443762141887, 2.300192860742704, 2.371903603495831, 2.3187293924134353, 2.2865734601451453, 2.278883275923079, 2.2856127452380552, 2.300852081850049, 2.2902581163227853, 2.291593810803393, 2.2944570582096997, 2.3034831275689385, 2.3001572020926893, 2.30077024986004, 2.306526164507435, 2.316826485647944, 2.3044545239415664, 2.2998624826691225, 2.329815963218952, 2.313500941093332, 2.2979342130996128, 2.3075784568128914, 2.2763289990292, 2.2975953768431063, 2.2939318917655007, 2.2795209500981475, 2.2827198293995976, 2.4062104980738095, 2.3056954942117573, 2.3224927897523777, 2.3221323556696447, 2.3257945543048026, 2.320935112697933, 2.312271058265799, 2.3004829315912154, 2.2882343689209135, 2.331651099209715, 2.3150456472374925, 2.3000053932709843, 2.3304554750570916, 2.3312380861961977, 2.37692536156753, 2.2953603882311993, 2.291996186198468, 2.3138425209252116, 2.3078913504658467, 2.3527355769584917, 2.3160458276620246, 2.33805409988942, 2.347818447842778, 2.309366696378085, 2.320660854991042, 2.339774177187965, 2.325944628817303, 2.4016861304860986, 2.47874307162656, 2.3512340453262204, 2.347682965408601, 2.342795881340265, 2.3229086606569087, 2.3509611253472187, 2.3264340931558842, 2.3507451110676985, 2.3317420208591155, 2.3475143040342283, 2.319795143428107, 2.3420767087263035, 2.3375144274951203, 2.3418889707336676, 2.3459369607746896, 2.3637476221876974, 2.3618424220625402, 2.3503307603262917, 2.3659180745311166, 2.31826577985228, 2.3219939942038903, 2.311616852561437, 2.301965164237813, 2.3120139678710787, 2.319614364204344, 2.324972757173485, 2.345075059994101, 2.3301809867614596, 2.3693363780067083, 2.457494883701719, 2.557540397142933, 2.3383036556306536, 2.3209522937123217, 2.3510142637199563, 2.3311974242794493, 2.3500470794088932, 2.3553573110420714, 2.353837681129844, 2.356193301712938, 2.391551143430137, 2.3393851454034813, 2.3461306146017242, 2.364496634315779, 2.378059493692833, 2.366132867355848, 2.3787518722083183, 2.3952061640609466, 2.3690795961076208, 2.352324595396546, 2.3507139941154445, 2.3339921789999276, 2.338148095924866, 2.3509157909744087, 2.3416854716678364, 2.339903971244549, 2.3676838573368117, 2.3939738281450444, 2.3985009052483317, 2.3781872692170793, 2.348650190043332, 2.3574359001980234, 2.3627388352048024, 2.355626515371263, 2.373630902254327, 2.3676189115677757, 2.379246419286493, 2.3780489915305956, 2.3855453735501895, 2.37733639089149, 2.3741118727842183, 2.4245622620011003, 2.3425727270311127, 2.3665481983930214, 2.36796000086028, 2.322779231079302, 2.3375511803650504, 2.315493516734081, 2.359295338441194, 2.3410619354404645, 2.415183966187225, 2.37259656611726, 2.42261672528898, 2.4797846949942204, 2.430850260754916, 2.4009047177037584, 2.3832884715695686, 2.3888153599205078, 2.379627169059415, 2.3585969426949034, 2.3491362949897505], 'val_acc': [0.11165845647992563, 0.10837438423033614, 0.11165845628417967, 0.11494252872951512, 0.11165845647992563, 0.11822660097910462, 0.10837438373485418, 0.1018062392356752, 0.1018062392356752, 0.1018062392356752, 0.10016420311088045, 0.11001641985964893, 0.11494252872951512, 0.12972085355293184, 0.12479474547216654, 0.14285714284490875, 0.14449917877395752, 0.19047618986448436, 0.2200328404044087, 0.2167487678612003, 0.2052545148897641, 0.19540229804312262, 0.18883415364181663, 0.19376026211407385, 0.1904761900602303, 0.16584564760107126, 0.20197044273804757, 0.19868637068420403, 0.21346469590522973, 0.23481116542968844, 0.19047618986448436, 0.20361247905858829, 0.2036124789607153, 0.2036124789607153, 0.211822659682562, 0.21510673203002448, 0.22495894868091998, 0.21839080398599503, 0.2151067319321515, 0.22660098480571472, 0.22167487613771153, 0.2216748763334575, 0.21182265779850715, 0.18719211781064082, 0.22331691067207035, 0.22167487643133046, 0.23809523767927793, 0.22331691226250627, 0.22660098480571472, 0.23973727380407267, 0.24466338208058394, 0.21839080427961396, 0.21018062394925918, 0.21839080437748695, 0.224958948485174, 0.23645320155448318, 0.2331691293048937, 0.22167487652920345, 0.21674876835056517, 0.211822659682562, 0.23645320165235617, 0.23481116582118036, 0.23645320165235617, 0.23316912940276668, 0.23316912742083884, 0.24466338247207586, 0.22660098490358768, 0.22660098451209576, 0.2430213442674803, 0.23645319967042833, 0.22660098451209576, 0.23481116572330737, 0.23316912959851263, 0.2315270934737179, 0.23645320165235617, 0.2216748745472756, 0.22824302093050947, 0.22988505744679613, 0.2315270933758449, 0.2151067318342785, 0.2233169126539982, 0.22824302102838243, 0.20525451518338303, 0.24137931032035934, 0.23809523767927793, 0.22003284001291679, 0.23152709318009895, 0.20853858762871846, 0.23316912950063964, 0.23316912969638562, 0.23481116572330737, 0.21182265987830795, 0.2446633822763299, 0.23316912910914772, 0.2282430207347635, 0.22660098451209576, 0.21182265997618094, 0.2298850570553042, 0.23645319967042833, 0.23645320155448318, 0.21346469610097568, 0.20689655121030479, 0.20853858762871846, 0.2036124789607153, 0.22003284060015468, 0.2282430211262554, 0.21018062326414833, 0.21839080408386802, 0.22167487623558452, 0.23316912910914772, 0.22331691236037926, 0.22988505676168527, 0.211822659780435, 0.21346469590522973, 0.23481116364350654, 0.22167487672494943, 0.23809523807076985, 0.22331691245825225, 0.24466338256994882, 0.2151067318342785, 0.24794745472166535, 0.22988505676168527, 0.23481116582118036, 0.23645320155448318, 0.23809523589309606, 0.2348111651360695, 0.21839080437748695, 0.23152709318009895, 0.24302134624940813, 0.23152709327797194, 0.23152709318009895, 0.2282430207347635, 0.23316912969638562, 0.23481116542968844, 0.21510673222577043, 0.23645320175022916, 0.2233169125561252, 0.22660098470784173, 0.22167487652920345, 0.19376026250556577, 0.21018062365564025, 0.2068965511124318, 0.2298850571531772, 0.2233169126539982, 0.22495894887666593, 0.2397372719200178, 0.2528735613098677, 0.22495894868091998, 0.2594417056133007, 0.22660098470784173, 0.23645319967042833, 0.23316912742083884, 0.23481116572330737, 0.22495894887666593, 0.23316912950063964, 0.23809523797289686, 0.22495894689473808, 0.21674876805694623, 0.22660098292165984, 0.2528735628024306, 0.2446633822763299, 0.24630541830325164, 0.23973727380407267, 0.23481116364350654, 0.23316912959851263, 0.2364532019459751, 0.2413793099288674, 0.22495894887666593, 0.24302134615153514, 0.2413793100267404, 0.2200328405022817, 0.25123152687338185, 0.2430213441696073, 0.23481116542968844, 0.22167487652920345, 0.2315270933758449, 0.24630541849899762, 0.2298850571531772, 0.23152709318009895, 0.23152709327797194, 0.24466338237420288, 0.24466338039227503, 0.2134646956116108, 0.23645319986617427, 0.2348111656254344, 0.2282430213220014, 0.22824302063689053, 0.22331691086781633, 0.22660098490358768, 0.24466338256994882, 0.23645320184810212, 0.2052545147918911, 0.23973727231150974, 0.22003284060015468, 0.21182265958468902, 0.22824301924220056, 0.2085385875308455, 0.23481116364350654, 0.23152709357159088, 0.2348111656254344, 0.22660098509933366, 0.21674876795907325, 0.22495894907241187, 0.22331691275187118, 0.22003283842248086, 0.22660098480571472, 0.22824302093050947, 0.21839080418174098, 0.224958948583047, 0.2167487678612003, 0.2085385857446636, 0.2331691293048937, 0.22331691067207035, 0.22660098490358768, 0.21839080418174098, 0.21839080398599503, 0.23316912959851263, 0.21839080379024906, 0.23645320175022916, 0.224958948583047, 0.23316912940276668, 0.23973727399981865, 0.23809523807076985, 0.2315270933758449, 0.2249589489745389, 0.22167487623558452, 0.22167487484089454, 0.22003283842248086, 0.21346469570948376, 0.22495894907241187, 0.2249589467968651, 0.22660098292165984, 0.22660098292165984, 0.23316912940276668, 0.2282430212241284, 0.22003284030653572, 0.2200328405022817, 0.2298850571531772, 0.22988505744679613, 0.22003284011078977, 0.18883415393543557, 0.22167487662707644, 0.2151067319321515, 0.23481116552756143, 0.23481116354563358, 0.24958948866878627, 0.23973727211576376, 0.2348111656254344, 0.23152709198115495, 0.22003284020866276, 0.23481116572330737, 0.22495894907241187, 0.2216748763334575, 0.2183908023955591, 0.2282430207347635, 0.22660098500146067, 0.2134646956116108, 0.2167487682526922, 0.2348111656254344, 0.2413793100267404, 0.2413793100267404, 0.2331691293048937, 0.2331691276165848, 0.23316912950063964, 0.22988505725105016, 0.2233169125561252, 0.22660098480571472, 0.21018062365564025, 0.20525451508551004, 0.22167487672494943, 0.22331691284974417, 0.2331691293048937, 0.224958948485174, 0.2249589467968651, 0.22331691275187118, 0.21182265987830795, 0.23152709149179004, 0.20853858743297252, 0.22331691294761713, 0.2298850571531772, 0.22495894718835702, 0.224958948485174, 0.22824301914432757, 0.2348111656254344, 0.2413793083384315, 0.23152709139391706, 0.23316912771445777, 0.21674876835056517, 0.2216748746451486, 0.2167487682526922, 0.21839080249343207, 0.20853858733509953, 0.20525451349507412, 0.21674876795907325, 0.22167487662707644, 0.23481116542968844, 0.22988505517124935, 0.23809523767927793, 0.22660098490358768, 0.23481116552756143], 'loss': [2.7155265340080494, 2.7069024530524346, 2.699896711882135, 2.6940811047563806, 2.6887281089837547, 2.6833619567158284, 2.6791919108044198, 2.675656310146105, 2.6700092019976043, 2.666053435151337, 2.659964148758373, 2.653985003132595, 2.6452363118988287, 2.6342864259802097, 2.6173134664735267, 2.5957942623866903, 2.576081290822744, 2.5509809054388404, 2.5159539979586123, 2.497029380093365, 2.4611191806362394, 2.450472377800599, 2.4516792945548493, 2.4474244797254245, 2.434242991108669, 2.4454591034374196, 2.41069089871902, 2.368151519332823, 2.3535187295330133, 2.3414593612388908, 2.357638532180316, 2.3712658697085214, 2.341948422856889, 2.3325558643811046, 2.342597064639019, 2.304492116610862, 2.281900447986454, 2.2669426007437266, 2.289108729313531, 2.266181305546535, 2.2522466875934013, 2.2610213764394334, 2.2510272329592853, 2.357182076628448, 2.366349002712806, 2.2545185710859985, 2.2420618710331848, 2.244132153552171, 2.2419295059337263, 2.2420162701264053, 2.2275840882647944, 2.225793644241239, 2.2505546107674035, 2.3195686152338735, 2.270510214603902, 2.2247592103554727, 2.2100594377615614, 2.2077432552157488, 2.269265484760919, 2.263039201289966, 2.291774355949073, 2.265704251706478, 2.23345554800249, 2.225452983746538, 2.2101295527001916, 2.2129422626456194, 2.196910255400797, 2.1935437198537087, 2.2032774320128516, 2.194351725020203, 2.1811733937116617, 2.1857663494360766, 2.177199076920809, 2.1891394438929628, 2.192065367904287, 2.1928894012615667, 2.2165361728511552, 2.2273053881079266, 2.2313448816843837, 2.2190395668546765, 2.213793353819015, 2.2083341088383106, 2.208126763984163, 2.176364822994757, 2.15574311377821, 2.1806423054093944, 2.1897276672739268, 2.2043116532312035, 2.17736444855128, 2.156411769992272, 2.1558597743878374, 2.1666211303499443, 2.18317435967604, 2.1558970814612857, 2.150638540569517, 2.139128491129474, 2.1505819283471705, 2.1728115409796245, 2.1589786831113593, 2.166746322330263, 2.1674071101437358, 2.183348405924176, 2.2043062009361, 2.2118263798817472, 2.263707235167893, 2.215907081784164, 2.166395991440916, 2.1537975297571452, 2.1429208418671846, 2.1526889394685718, 2.1427977442496613, 2.162687186098197, 2.1521300576305977, 2.1626529671083485, 2.1604656189129336, 2.146952745116467, 2.1315658725997015, 2.128861779990382, 2.133116905008743, 2.1446257600059746, 2.143228722010305, 2.1300870837616968, 2.1239249533941122, 2.128015720281268, 2.103682898104313, 2.1056470295242216, 2.122805608812054, 2.113960118068562, 2.130535741410461, 2.0988208587654316, 2.0996164191674893, 2.1153556580905795, 2.1307973988981463, 2.111419499336572, 2.1021989047894487, 2.096221734561959, 2.0826727759421972, 2.1158980595747305, 2.194449970022119, 2.2056647329115036, 2.256495222369748, 2.22706205301461, 2.2228707973472392, 2.1749901472910227, 2.1173515126935265, 2.1310785650962187, 2.1062863553573954, 2.1210469904866307, 2.1311530855402077, 2.1153480815691625, 2.1194454355651104, 2.0953878309692446, 2.081537706798107, 2.09000202498152, 2.086941800518937, 2.077563722814133, 2.0747944203245567, 2.0811873095236275, 2.1227398741171837, 2.092064995589442, 2.083406179690508, 2.0793724099223865, 2.0664528691058774, 2.064452314866397, 2.0782437659386983, 2.0615073063045557, 2.0675624678022317, 2.0739424848458605, 2.081184461425217, 2.0601380432410896, 2.06291268513188, 2.054697186746147, 2.050304225729721, 2.0506662423605793, 2.072474553991392, 2.0522303048590125, 2.0674361439456197, 2.0675519700412632, 2.09252379934401, 2.08933103432156, 2.049847934378246, 2.044256767648936, 2.0776948584178636, 2.079388411382875, 2.062365695142648, 2.043898582948062, 2.0463486076135653, 2.0774361552643823, 2.0728386675307884, 2.108292987850902, 2.065562494775353, 2.0671645396054403, 2.1133354560060913, 2.108078335638653, 2.1134659776942195, 2.1394393722134692, 2.0934085146847203, 2.0586712639434626, 2.0724374440171633, 2.051622089174494, 2.036743938261968, 2.0290443099254944, 2.0174505219077674, 2.0273892963691416, 2.022066301976386, 2.0318771906701936, 2.0086583941379366, 2.021299234503838, 2.0007473305755084, 2.040930990223033, 2.1164986782739783, 2.082588463103747, 2.0789637005549437, 2.0842181889183466, 2.068063137222854, 2.0486738091376773, 2.0264355149357227, 2.004123028592652, 2.015936912207633, 1.990428655886797, 2.021281603768131, 2.0208967131510898, 2.0165647825910815, 2.0004143135258796, 1.9979775924212635, 2.00905807219002, 2.019796326468857, 2.03040899853442, 2.020667714553692, 2.0213835641833544, 2.0141637166177957, 2.007172449658294, 2.0049012341783277, 1.998088504793218, 1.989710998437243, 1.9751109888911003, 1.9691695922209251, 1.975091267709125, 1.996199669289638, 2.0370104783859095, 2.0753202506893715, 2.1004135704628006, 1.987885430020718, 1.9935422288318925, 1.9714299829099213, 1.9938038749616493, 2.0106529951585146, 1.991476379674563, 1.9946691123856657, 2.023456031928562, 1.9989246673897307, 1.9876430486996315, 2.0039669021199127, 1.9864107851619839, 1.980889198275807, 1.9714288536283269, 1.9669552605254939, 1.9904384657098038, 1.9879970513330103, 1.9814634991867097, 1.9795352551481806, 1.9624385978651733, 1.9629625056558566, 1.9537072088684144, 1.9483415026929098, 1.987332666287432, 1.9485919092225343, 2.028279270773306, 2.015160704785059, 1.961347667784172, 1.9544500488030592, 1.9481421058917192, 1.9859209781309908, 1.9792773348594839, 1.9505509175803872, 1.9437351085322103, 1.9473615340383637, 1.9638716829875655, 1.9672513919689327, 1.9728032146391192, 1.985796562112577, 1.980229321201724, 1.9397932984256157, 1.9652718381470478, 1.949075845183778, 1.9337118312808277, 1.9273109216709645, 1.9703386194162544, 2.021071611635494, 2.0151523275786603, 1.994773002865378, 2.0506627260047554, 2.0160525499183293, 1.968002216135452, 1.9430064821634938, 1.923261233619596, 1.9307810458314492, 1.9159714673334078, 1.919878167391313, 1.9049066438812006], 'acc': [0.07268993853910748, 0.10677618023306437, 0.10965092416539085, 0.10266940426655129, 0.10554414738803429, 0.11006160137344924, 0.1071868578694929, 0.10472279313102639, 0.10266940406154558, 0.10225872684430783, 0.10390143770823979, 0.10841889091034934, 0.11088295701960028, 0.1264887062920682, 0.12648870650625327, 0.14250513337843226, 0.15071868534817587, 0.16427104633446837, 0.1876796709193831, 0.2036960980424646, 0.21190965042221963, 0.20492813248164354, 0.20205338696434758, 0.19383983681211725, 0.20533880812309116, 0.195071869647968, 0.19753593337976466, 0.2098562621360442, 0.21273100524834784, 0.2151950729150302, 0.2119096507955136, 0.19835728922174206, 0.21314168288477636, 0.21519507076094038, 0.21190965138299264, 0.22505133437792132, 0.2291581109502722, 0.23778234071555324, 0.22505133516122672, 0.24065708445205336, 0.24558521626054383, 0.2468172500755263, 0.2472279267144644, 0.2271047232515758, 0.211088296520147, 0.24188911671878377, 0.24517453725333085, 0.23737166407661517, 0.2439425043991214, 0.24229979510180025, 0.24599589309530825, 0.25626283372452124, 0.24804927936814405, 0.22299794707087767, 0.23080082022923465, 0.24845995679038752, 0.2537987695827132, 0.2505133476590229, 0.23490759699741184, 0.24394250537825315, 0.22669404639845267, 0.22422997855430266, 0.24763860570331864, 0.24722792730194343, 0.2595482558440379, 0.2550308024369226, 0.2628336756136383, 0.26776180800960786, 0.25585215728140953, 0.2579055449801059, 0.2644763866917553, 0.26899384105964363, 0.2718685830153479, 0.26694045179433645, 0.26488706546642454, 0.2599589315038442, 0.26119096592466445, 0.2459958936644286, 0.24394250576990587, 0.24887063661762332, 0.2603696093177404, 0.2537987691910605, 0.25010266886599497, 0.2689938402763382, 0.28008213318593694, 0.26652977633035646, 0.2657084193317797, 0.26078028811076825, 0.2624230001496583, 0.26817248564603635, 0.2722792590300895, 0.26570841835264797, 0.25913757861762077, 0.27638603759742125, 0.2722792618083758, 0.2763860372241273, 0.2747433271435007, 0.26365502924644973, 0.2665297731971349, 0.2644763878483547, 0.26488706254738803, 0.27145790520145174, 0.2583162220290554, 0.25215605754382314, 0.24024640701145117, 0.2513347040701206, 0.2739219696981951, 0.27720739342103995, 0.27843942349696305, 0.27967145674282523, 0.2718685822504012, 0.2673511290207536, 0.26899384090053474, 0.2603696108843512, 0.27638603720576854, 0.2755646839278924, 0.28418890956246146, 0.285831620230567, 0.2796714565469989, 0.2722792610250704, 0.2694045186777134, 0.28336755769208716, 0.2915811104451362, 0.27967145971693785, 0.2870636558630628, 0.2907597550131702, 0.28377823370682875, 0.2841889139073585, 0.2850102673443436, 0.29158110825432887, 0.28747433129032535, 0.280903489792861, 0.2899383968029179, 0.28542094398328166, 0.2924024637230123, 0.29404517396274776, 0.29322381794330277, 0.2800821361600496, 0.26365503008483127, 0.25954825568492895, 0.2468172496838736, 0.2316221784027695, 0.25092402625622445, 0.26652977380297266, 0.2767967136488803, 0.27063654937783305, 0.2841889139073585, 0.2825461990901822, 0.26940451789440806, 0.28254620343507925, 0.29404517337526875, 0.29404517259196333, 0.30349075780756907, 0.29117043165210826, 0.28911704254591, 0.305544148052008, 0.31047227986049847, 0.2981519503392723, 0.2924024638821212, 0.28870636551531925, 0.29938398221435, 0.29691991552679936, 0.3096509240368798, 0.3002053384296214, 0.28952771898902174, 0.2997946625739887, 0.3084188905951913, 0.3063655046956495, 0.3010266960523946, 0.30965092207861633, 0.3059548231243353, 0.3067761801229121, 0.3084188900077123, 0.3039014373839024, 0.3088295672341294, 0.3133470227953345, 0.31047227692310325, 0.3034907599616589, 0.2989733045962802, 0.2899383964112652, 0.31375769806348813, 0.31047227692310325, 0.30143737347463806, 0.3088295670015856, 0.3112936323183518, 0.31868583085111035, 0.31868583006780493, 0.3080082139562532, 0.3051334718414401, 0.28706365684219454, 0.3088295688007402, 0.3030800805444345, 0.29527720564199916, 0.2911704320070435, 0.2969199178767155, 0.28008213557257056, 0.3051334692589801, 0.3195071854630535, 0.31293634419813293, 0.3154004104940309, 0.3162217661218232, 0.32114989933781557, 0.32197125492889045, 0.32073921854980675, 0.3285420934889596, 0.31252566677588944, 0.32813141626254244, 0.3203285417150423, 0.333059549245991, 0.3203285399526052, 0.29240246413302373, 0.303490760549138, 0.3088295701715246, 0.2960985626405759, 0.3096509248201852, 0.3203285432816531, 0.32525667450266454, 0.33141683799040633, 0.32238193137200216, 0.3281314150508669, 0.3215605751567308, 0.3154004091232464, 0.308829570367351, 0.3207392216830283, 0.3244353188748722, 0.3227926082067666, 0.31786447855236594, 0.31581108967871147, 0.32114989949692446, 0.3199178635094934, 0.31663244233239113, 0.32936345025499253, 0.3268993859540755, 0.3203285434774794, 0.32772073825281994, 0.3342915834709849, 0.34045174460881056, 0.33675564898357746, 0.330184806311155, 0.31457905408293313, 0.299794661435748, 0.28008213573167945, 0.321560576760059, 0.32648870418693493, 0.33552361593354163, 0.32032854230252134, 0.31663244550233016, 0.3264887081401794, 0.32156057417759903, 0.3154004099065518, 0.3326488708446158, 0.3297741271264744, 0.33305954865851195, 0.33018480454871785, 0.3383983553435034, 0.3305954831459194, 0.33963039231006614, 0.3338809036621078, 0.33264887022041933, 0.3375770026531063, 0.3544147841739459, 0.3457905538028271, 0.3466119110339476, 0.3466119116214266, 0.34455852036113854, 0.3342915826876795, 0.35154004084745716, 0.316221764518495, 0.3178644787481923, 0.34045174774203213, 0.34373716532082527, 0.3466119082556613, 0.33182751678343425, 0.3359342910058689, 0.343737165712478, 0.3486652975209685, 0.3404517469587267, 0.34291581086799106, 0.34250513285826856, 0.33839835886837766, 0.334291580729416, 0.3293634484925554, 0.34332648848606084, 0.33347022408577454, 0.3396303915267608, 0.348665298732644, 0.35030800760159503, 0.33757700343641167, 0.3182751522539088, 0.3158110869371426, 0.3310061587690083, 0.3158110879162743, 0.31581108674131625, 0.34086242457679655, 0.34989732959187253, 0.3462012310659616, 0.34825461872794056, 0.3519507200688552, 0.35646817465092856, 0.359753595008008]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
