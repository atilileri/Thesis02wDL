{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf37.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 09:44:18 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': 'Split', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "12176 Files with 15 Label(s): ['ds', 'eo', 'eb', 'by', 'ib', 'ek', 'eg', 'yd', 'mb', 'aa', 'ce', 'sk', 'ck', 'my', 'sg'].\n",
    "Padding:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 12176 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "9740 steps for training, 2436 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (9740, 7991, 7)\n",
    "Test Batch: (2436, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000002090C0DFF98>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000020949E46EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.6972, Accuracy:0.0890, Validation Loss:2.6863, Validation Accuracy:0.0891\n",
    "Epoch #2: Loss:2.6813, Accuracy:0.0810, Validation Loss:2.6752, Validation Accuracy:0.0813\n",
    "Epoch #3: Loss:2.6721, Accuracy:0.0815, Validation Loss:2.6682, Validation Accuracy:0.0813\n",
    "Epoch #4: Loss:2.6669, Accuracy:0.0955, Validation Loss:2.6644, Validation Accuracy:0.1022\n",
    "Epoch #5: Loss:2.6649, Accuracy:0.1022, Validation Loss:2.6627, Validation Accuracy:0.1022\n",
    "Epoch #6: Loss:2.6622, Accuracy:0.1022, Validation Loss:2.6616, Validation Accuracy:0.1022\n",
    "Epoch #7: Loss:2.6622, Accuracy:0.1022, Validation Loss:2.6612, Validation Accuracy:0.1022\n",
    "Epoch #8: Loss:2.6618, Accuracy:0.1022, Validation Loss:2.6610, Validation Accuracy:0.1022\n",
    "Epoch #9: Loss:2.6613, Accuracy:0.1022, Validation Loss:2.6606, Validation Accuracy:0.1022\n",
    "Epoch #10: Loss:2.6612, Accuracy:0.1022, Validation Loss:2.6605, Validation Accuracy:0.1022\n",
    "Epoch #11: Loss:2.6602, Accuracy:0.1022, Validation Loss:2.6585, Validation Accuracy:0.1022\n",
    "Epoch #12: Loss:2.6578, Accuracy:0.1028, Validation Loss:2.6553, Validation Accuracy:0.1039\n",
    "Epoch #13: Loss:2.6512, Accuracy:0.1141, Validation Loss:2.6424, Validation Accuracy:0.1096\n",
    "Epoch #14: Loss:2.6303, Accuracy:0.1209, Validation Loss:2.6099, Validation Accuracy:0.1359\n",
    "Epoch #15: Loss:2.5947, Accuracy:0.1302, Validation Loss:2.5769, Validation Accuracy:0.1379\n",
    "Epoch #16: Loss:2.5629, Accuracy:0.1351, Validation Loss:2.5507, Validation Accuracy:0.1457\n",
    "Epoch #17: Loss:2.5357, Accuracy:0.1452, Validation Loss:2.5206, Validation Accuracy:0.1474\n",
    "Epoch #18: Loss:2.5085, Accuracy:0.1482, Validation Loss:2.5009, Validation Accuracy:0.1470\n",
    "Epoch #19: Loss:2.4872, Accuracy:0.1492, Validation Loss:2.4816, Validation Accuracy:0.1466\n",
    "Epoch #20: Loss:2.4621, Accuracy:0.1532, Validation Loss:2.4613, Validation Accuracy:0.1736\n",
    "Epoch #21: Loss:2.4310, Accuracy:0.1803, Validation Loss:2.4197, Validation Accuracy:0.2020\n",
    "Epoch #22: Loss:2.3830, Accuracy:0.2059, Validation Loss:2.3807, Validation Accuracy:0.2081\n",
    "Epoch #23: Loss:2.3458, Accuracy:0.2204, Validation Loss:2.3306, Validation Accuracy:0.2319\n",
    "Epoch #24: Loss:2.3199, Accuracy:0.2224, Validation Loss:2.3108, Validation Accuracy:0.2389\n",
    "Epoch #25: Loss:2.2958, Accuracy:0.2318, Validation Loss:2.4036, Validation Accuracy:0.1925\n",
    "Epoch #26: Loss:2.2973, Accuracy:0.2278, Validation Loss:2.2815, Validation Accuracy:0.2373\n",
    "Epoch #27: Loss:2.2781, Accuracy:0.2313, Validation Loss:2.2564, Validation Accuracy:0.2451\n",
    "Epoch #28: Loss:2.2487, Accuracy:0.2395, Validation Loss:2.2471, Validation Accuracy:0.2496\n",
    "Epoch #29: Loss:2.2337, Accuracy:0.2424, Validation Loss:2.2462, Validation Accuracy:0.2401\n",
    "Epoch #30: Loss:2.2264, Accuracy:0.2405, Validation Loss:2.2696, Validation Accuracy:0.2352\n",
    "Epoch #31: Loss:2.2454, Accuracy:0.2358, Validation Loss:2.2580, Validation Accuracy:0.2328\n",
    "Epoch #32: Loss:2.2376, Accuracy:0.2383, Validation Loss:2.2119, Validation Accuracy:0.2512\n",
    "Epoch #33: Loss:2.1970, Accuracy:0.2472, Validation Loss:2.1968, Validation Accuracy:0.2631\n",
    "Epoch #34: Loss:2.1817, Accuracy:0.2604, Validation Loss:2.2343, Validation Accuracy:0.2525\n",
    "Epoch #35: Loss:2.1933, Accuracy:0.2603, Validation Loss:2.1827, Validation Accuracy:0.2594\n",
    "Epoch #36: Loss:2.1598, Accuracy:0.2661, Validation Loss:2.1906, Validation Accuracy:0.2586\n",
    "Epoch #37: Loss:2.1630, Accuracy:0.2658, Validation Loss:2.1642, Validation Accuracy:0.2619\n",
    "Epoch #38: Loss:2.1325, Accuracy:0.2732, Validation Loss:2.1530, Validation Accuracy:0.2656\n",
    "Epoch #39: Loss:2.1443, Accuracy:0.2665, Validation Loss:2.1273, Validation Accuracy:0.2693\n",
    "Epoch #40: Loss:2.1189, Accuracy:0.2763, Validation Loss:2.1381, Validation Accuracy:0.2709\n",
    "Epoch #41: Loss:2.1017, Accuracy:0.2821, Validation Loss:2.1142, Validation Accuracy:0.2828\n",
    "Epoch #42: Loss:2.1011, Accuracy:0.2783, Validation Loss:2.0999, Validation Accuracy:0.2828\n",
    "Epoch #43: Loss:2.0791, Accuracy:0.2871, Validation Loss:2.0854, Validation Accuracy:0.2943\n",
    "Epoch #44: Loss:2.0864, Accuracy:0.2824, Validation Loss:2.1366, Validation Accuracy:0.2783\n",
    "Epoch #45: Loss:2.0851, Accuracy:0.2811, Validation Loss:2.1453, Validation Accuracy:0.2767\n",
    "Epoch #46: Loss:2.0795, Accuracy:0.2892, Validation Loss:2.0842, Validation Accuracy:0.2890\n",
    "Epoch #47: Loss:2.0597, Accuracy:0.2970, Validation Loss:2.0723, Validation Accuracy:0.2993\n",
    "Epoch #48: Loss:2.0674, Accuracy:0.2921, Validation Loss:2.1040, Validation Accuracy:0.2800\n",
    "Epoch #49: Loss:2.0584, Accuracy:0.2958, Validation Loss:2.0521, Validation Accuracy:0.2984\n",
    "Epoch #50: Loss:2.0495, Accuracy:0.2957, Validation Loss:2.0650, Validation Accuracy:0.2980\n",
    "Epoch #51: Loss:2.0430, Accuracy:0.3018, Validation Loss:2.0560, Validation Accuracy:0.2931\n",
    "Epoch #52: Loss:2.0418, Accuracy:0.3049, Validation Loss:2.0397, Validation Accuracy:0.3038\n",
    "Epoch #53: Loss:2.0316, Accuracy:0.3051, Validation Loss:2.0350, Validation Accuracy:0.3058\n",
    "Epoch #54: Loss:2.0239, Accuracy:0.3071, Validation Loss:2.0292, Validation Accuracy:0.3136\n",
    "Epoch #55: Loss:2.0240, Accuracy:0.3107, Validation Loss:2.0307, Validation Accuracy:0.3103\n",
    "Epoch #56: Loss:2.0191, Accuracy:0.3123, Validation Loss:2.0362, Validation Accuracy:0.3124\n",
    "Epoch #57: Loss:2.0265, Accuracy:0.3071, Validation Loss:2.0489, Validation Accuracy:0.3025\n",
    "Epoch #58: Loss:2.0308, Accuracy:0.3100, Validation Loss:2.0561, Validation Accuracy:0.3161\n",
    "Epoch #59: Loss:2.0264, Accuracy:0.3077, Validation Loss:2.0184, Validation Accuracy:0.3136\n",
    "Epoch #60: Loss:2.0056, Accuracy:0.3217, Validation Loss:2.0125, Validation Accuracy:0.3210\n",
    "Epoch #61: Loss:2.0064, Accuracy:0.3196, Validation Loss:2.0132, Validation Accuracy:0.3206\n",
    "Epoch #62: Loss:2.0053, Accuracy:0.3186, Validation Loss:2.0342, Validation Accuracy:0.3083\n",
    "Epoch #63: Loss:2.0225, Accuracy:0.3146, Validation Loss:2.0900, Validation Accuracy:0.2939\n",
    "Epoch #64: Loss:2.0261, Accuracy:0.3145, Validation Loss:2.0266, Validation Accuracy:0.3202\n",
    "Epoch #65: Loss:2.0039, Accuracy:0.3230, Validation Loss:2.0095, Validation Accuracy:0.3231\n",
    "Epoch #66: Loss:1.9989, Accuracy:0.3189, Validation Loss:2.1121, Validation Accuracy:0.2808\n",
    "Epoch #67: Loss:2.0746, Accuracy:0.2945, Validation Loss:2.0126, Validation Accuracy:0.3190\n",
    "Epoch #68: Loss:2.0103, Accuracy:0.3157, Validation Loss:2.0034, Validation Accuracy:0.3136\n",
    "Epoch #69: Loss:2.0003, Accuracy:0.3187, Validation Loss:2.0047, Validation Accuracy:0.3161\n",
    "Epoch #70: Loss:1.9918, Accuracy:0.3226, Validation Loss:2.0116, Validation Accuracy:0.3243\n",
    "Epoch #71: Loss:1.9872, Accuracy:0.3273, Validation Loss:2.0149, Validation Accuracy:0.3268\n",
    "Epoch #72: Loss:1.9898, Accuracy:0.3255, Validation Loss:1.9970, Validation Accuracy:0.3264\n",
    "Epoch #73: Loss:1.9896, Accuracy:0.3251, Validation Loss:1.9918, Validation Accuracy:0.3231\n",
    "Epoch #74: Loss:1.9809, Accuracy:0.3233, Validation Loss:2.0133, Validation Accuracy:0.3194\n",
    "Epoch #75: Loss:1.9890, Accuracy:0.3259, Validation Loss:1.9890, Validation Accuracy:0.3268\n",
    "Epoch #76: Loss:1.9777, Accuracy:0.3304, Validation Loss:1.9934, Validation Accuracy:0.3231\n",
    "Epoch #77: Loss:1.9766, Accuracy:0.3295, Validation Loss:1.9927, Validation Accuracy:0.3276\n",
    "Epoch #78: Loss:1.9778, Accuracy:0.3304, Validation Loss:1.9864, Validation Accuracy:0.3243\n",
    "Epoch #79: Loss:1.9755, Accuracy:0.3284, Validation Loss:1.9921, Validation Accuracy:0.3235\n",
    "Epoch #80: Loss:1.9790, Accuracy:0.3306, Validation Loss:2.0297, Validation Accuracy:0.3181\n",
    "Epoch #81: Loss:2.0012, Accuracy:0.3245, Validation Loss:1.9884, Validation Accuracy:0.3268\n",
    "Epoch #82: Loss:1.9749, Accuracy:0.3313, Validation Loss:1.9938, Validation Accuracy:0.3161\n",
    "Epoch #83: Loss:1.9813, Accuracy:0.3285, Validation Loss:2.0405, Validation Accuracy:0.3149\n",
    "Epoch #84: Loss:1.9847, Accuracy:0.3284, Validation Loss:1.9987, Validation Accuracy:0.3255\n",
    "Epoch #85: Loss:1.9808, Accuracy:0.3311, Validation Loss:2.0248, Validation Accuracy:0.3095\n",
    "Epoch #86: Loss:2.0060, Accuracy:0.3201, Validation Loss:1.9749, Validation Accuracy:0.3222\n",
    "Epoch #87: Loss:1.9579, Accuracy:0.3359, Validation Loss:1.9726, Validation Accuracy:0.3317\n",
    "Epoch #88: Loss:1.9594, Accuracy:0.3393, Validation Loss:1.9843, Validation Accuracy:0.3210\n",
    "Epoch #89: Loss:1.9633, Accuracy:0.3338, Validation Loss:2.0000, Validation Accuracy:0.3288\n",
    "Epoch #90: Loss:1.9759, Accuracy:0.3307, Validation Loss:1.9696, Validation Accuracy:0.3309\n",
    "Epoch #91: Loss:1.9559, Accuracy:0.3392, Validation Loss:2.0026, Validation Accuracy:0.3255\n",
    "Epoch #92: Loss:1.9767, Accuracy:0.3306, Validation Loss:1.9835, Validation Accuracy:0.3292\n",
    "Epoch #93: Loss:1.9696, Accuracy:0.3299, Validation Loss:1.9683, Validation Accuracy:0.3288\n",
    "Epoch #94: Loss:1.9514, Accuracy:0.3386, Validation Loss:2.0344, Validation Accuracy:0.3194\n",
    "Epoch #95: Loss:1.9655, Accuracy:0.3297, Validation Loss:1.9804, Validation Accuracy:0.3227\n",
    "Epoch #96: Loss:1.9678, Accuracy:0.3370, Validation Loss:1.9650, Validation Accuracy:0.3300\n",
    "Epoch #97: Loss:1.9559, Accuracy:0.3340, Validation Loss:2.1298, Validation Accuracy:0.2874\n",
    "Epoch #98: Loss:2.0073, Accuracy:0.3225, Validation Loss:2.1112, Validation Accuracy:0.2902\n",
    "Epoch #99: Loss:1.9824, Accuracy:0.3258, Validation Loss:2.0188, Validation Accuracy:0.3198\n",
    "Epoch #100: Loss:1.9541, Accuracy:0.3357, Validation Loss:1.9866, Validation Accuracy:0.3272\n",
    "Epoch #101: Loss:1.9473, Accuracy:0.3407, Validation Loss:1.9686, Validation Accuracy:0.3288\n",
    "Epoch #102: Loss:1.9426, Accuracy:0.3399, Validation Loss:1.9619, Validation Accuracy:0.3366\n",
    "Epoch #103: Loss:1.9392, Accuracy:0.3441, Validation Loss:1.9644, Validation Accuracy:0.3325\n",
    "Epoch #104: Loss:1.9404, Accuracy:0.3445, Validation Loss:1.9566, Validation Accuracy:0.3292\n",
    "Epoch #105: Loss:1.9453, Accuracy:0.3465, Validation Loss:1.9568, Validation Accuracy:0.3350\n",
    "Epoch #106: Loss:1.9422, Accuracy:0.3426, Validation Loss:1.9654, Validation Accuracy:0.3354\n",
    "Epoch #107: Loss:1.9424, Accuracy:0.3409, Validation Loss:1.9847, Validation Accuracy:0.3268\n",
    "Epoch #108: Loss:1.9468, Accuracy:0.3392, Validation Loss:1.9550, Validation Accuracy:0.3329\n",
    "Epoch #109: Loss:1.9343, Accuracy:0.3448, Validation Loss:1.9659, Validation Accuracy:0.3313\n",
    "Epoch #110: Loss:1.9306, Accuracy:0.3440, Validation Loss:1.9556, Validation Accuracy:0.3325\n",
    "Epoch #111: Loss:1.9318, Accuracy:0.3485, Validation Loss:1.9719, Validation Accuracy:0.3259\n",
    "Epoch #112: Loss:1.9326, Accuracy:0.3472, Validation Loss:1.9572, Validation Accuracy:0.3296\n",
    "Epoch #113: Loss:1.9288, Accuracy:0.3470, Validation Loss:1.9624, Validation Accuracy:0.3378\n",
    "Epoch #114: Loss:1.9477, Accuracy:0.3383, Validation Loss:1.9635, Validation Accuracy:0.3313\n",
    "Epoch #115: Loss:1.9317, Accuracy:0.3456, Validation Loss:1.9559, Validation Accuracy:0.3354\n",
    "Epoch #116: Loss:1.9266, Accuracy:0.3492, Validation Loss:1.9664, Validation Accuracy:0.3333\n",
    "Epoch #117: Loss:1.9308, Accuracy:0.3445, Validation Loss:1.9476, Validation Accuracy:0.3374\n",
    "Epoch #118: Loss:1.9247, Accuracy:0.3463, Validation Loss:1.9603, Validation Accuracy:0.3366\n",
    "Epoch #119: Loss:1.9423, Accuracy:0.3440, Validation Loss:1.9528, Validation Accuracy:0.3362\n",
    "Epoch #120: Loss:1.9378, Accuracy:0.3406, Validation Loss:1.9497, Validation Accuracy:0.3391\n",
    "Epoch #121: Loss:1.9222, Accuracy:0.3454, Validation Loss:1.9486, Validation Accuracy:0.3399\n",
    "Epoch #122: Loss:1.9213, Accuracy:0.3445, Validation Loss:1.9433, Validation Accuracy:0.3292\n",
    "Epoch #123: Loss:1.9174, Accuracy:0.3478, Validation Loss:1.9375, Validation Accuracy:0.3391\n",
    "Epoch #124: Loss:1.9142, Accuracy:0.3490, Validation Loss:1.9880, Validation Accuracy:0.3259\n",
    "Epoch #125: Loss:1.9450, Accuracy:0.3444, Validation Loss:1.9443, Validation Accuracy:0.3321\n",
    "Epoch #126: Loss:1.9250, Accuracy:0.3475, Validation Loss:1.9413, Validation Accuracy:0.3313\n",
    "Epoch #127: Loss:1.9144, Accuracy:0.3485, Validation Loss:2.0001, Validation Accuracy:0.3206\n",
    "Epoch #128: Loss:1.9499, Accuracy:0.3414, Validation Loss:1.9763, Validation Accuracy:0.3276\n",
    "Epoch #129: Loss:1.9249, Accuracy:0.3466, Validation Loss:1.9563, Validation Accuracy:0.3387\n",
    "Epoch #130: Loss:1.9276, Accuracy:0.3484, Validation Loss:1.9432, Validation Accuracy:0.3309\n",
    "Epoch #131: Loss:1.9152, Accuracy:0.3512, Validation Loss:1.9444, Validation Accuracy:0.3399\n",
    "Epoch #132: Loss:1.9198, Accuracy:0.3496, Validation Loss:1.9361, Validation Accuracy:0.3346\n",
    "Epoch #133: Loss:1.9083, Accuracy:0.3522, Validation Loss:1.9290, Validation Accuracy:0.3407\n",
    "Epoch #134: Loss:1.9082, Accuracy:0.3530, Validation Loss:1.9307, Validation Accuracy:0.3395\n",
    "Epoch #135: Loss:1.9054, Accuracy:0.3529, Validation Loss:1.9321, Validation Accuracy:0.3448\n",
    "Epoch #136: Loss:1.9046, Accuracy:0.3555, Validation Loss:1.9340, Validation Accuracy:0.3456\n",
    "Epoch #137: Loss:1.9053, Accuracy:0.3509, Validation Loss:1.9375, Validation Accuracy:0.3329\n",
    "Epoch #138: Loss:1.9089, Accuracy:0.3509, Validation Loss:2.0775, Validation Accuracy:0.3030\n",
    "Epoch #139: Loss:1.9974, Accuracy:0.3283, Validation Loss:2.0097, Validation Accuracy:0.3177\n",
    "Epoch #140: Loss:1.9260, Accuracy:0.3480, Validation Loss:1.9365, Validation Accuracy:0.3411\n",
    "Epoch #141: Loss:1.9093, Accuracy:0.3561, Validation Loss:1.9347, Validation Accuracy:0.3432\n",
    "Epoch #142: Loss:1.9022, Accuracy:0.3579, Validation Loss:1.9292, Validation Accuracy:0.3489\n",
    "Epoch #143: Loss:1.8993, Accuracy:0.3602, Validation Loss:1.9541, Validation Accuracy:0.3346\n",
    "Epoch #144: Loss:1.9157, Accuracy:0.3487, Validation Loss:1.9325, Validation Accuracy:0.3362\n",
    "Epoch #145: Loss:1.9046, Accuracy:0.3544, Validation Loss:1.9207, Validation Accuracy:0.3444\n",
    "Epoch #146: Loss:1.8999, Accuracy:0.3524, Validation Loss:1.9228, Validation Accuracy:0.3444\n",
    "Epoch #147: Loss:1.8934, Accuracy:0.3600, Validation Loss:1.9286, Validation Accuracy:0.3436\n",
    "Epoch #148: Loss:1.9001, Accuracy:0.3524, Validation Loss:2.0122, Validation Accuracy:0.3186\n",
    "Epoch #149: Loss:1.9320, Accuracy:0.3476, Validation Loss:1.9689, Validation Accuracy:0.3317\n",
    "Epoch #150: Loss:1.9051, Accuracy:0.3567, Validation Loss:1.9255, Validation Accuracy:0.3481\n",
    "Epoch #151: Loss:1.8975, Accuracy:0.3525, Validation Loss:1.9166, Validation Accuracy:0.3514\n",
    "Epoch #152: Loss:1.8883, Accuracy:0.3609, Validation Loss:1.9173, Validation Accuracy:0.3477\n",
    "Epoch #153: Loss:1.8904, Accuracy:0.3616, Validation Loss:1.9275, Validation Accuracy:0.3407\n",
    "Epoch #154: Loss:1.8981, Accuracy:0.3566, Validation Loss:1.9205, Validation Accuracy:0.3473\n",
    "Epoch #155: Loss:1.8897, Accuracy:0.3574, Validation Loss:1.9111, Validation Accuracy:0.3506\n",
    "Epoch #156: Loss:1.8835, Accuracy:0.3566, Validation Loss:1.9312, Validation Accuracy:0.3391\n",
    "Epoch #157: Loss:1.8940, Accuracy:0.3545, Validation Loss:1.9299, Validation Accuracy:0.3362\n",
    "Epoch #158: Loss:1.8893, Accuracy:0.3567, Validation Loss:1.9098, Validation Accuracy:0.3514\n",
    "Epoch #159: Loss:1.8917, Accuracy:0.3611, Validation Loss:1.9280, Validation Accuracy:0.3469\n",
    "Epoch #160: Loss:1.8925, Accuracy:0.3623, Validation Loss:1.9099, Validation Accuracy:0.3498\n",
    "Epoch #161: Loss:1.8807, Accuracy:0.3610, Validation Loss:1.9633, Validation Accuracy:0.3276\n",
    "Epoch #162: Loss:1.8960, Accuracy:0.3548, Validation Loss:1.9024, Validation Accuracy:0.3551\n",
    "Epoch #163: Loss:1.8806, Accuracy:0.3619, Validation Loss:1.9340, Validation Accuracy:0.3374\n",
    "Epoch #164: Loss:1.8770, Accuracy:0.3657, Validation Loss:1.9172, Validation Accuracy:0.3465\n",
    "Epoch #165: Loss:1.8788, Accuracy:0.3581, Validation Loss:1.9008, Validation Accuracy:0.3563\n",
    "Epoch #166: Loss:1.8750, Accuracy:0.3617, Validation Loss:1.9103, Validation Accuracy:0.3432\n",
    "Epoch #167: Loss:1.8873, Accuracy:0.3606, Validation Loss:1.8987, Validation Accuracy:0.3555\n",
    "Epoch #168: Loss:1.8699, Accuracy:0.3648, Validation Loss:1.9243, Validation Accuracy:0.3485\n",
    "Epoch #169: Loss:1.8824, Accuracy:0.3600, Validation Loss:1.9467, Validation Accuracy:0.3407\n",
    "Epoch #170: Loss:1.8904, Accuracy:0.3602, Validation Loss:1.8978, Validation Accuracy:0.3522\n",
    "Epoch #171: Loss:1.8670, Accuracy:0.3698, Validation Loss:1.9189, Validation Accuracy:0.3456\n",
    "Epoch #172: Loss:1.8738, Accuracy:0.3659, Validation Loss:1.8955, Validation Accuracy:0.3625\n",
    "Epoch #173: Loss:1.8703, Accuracy:0.3651, Validation Loss:1.9014, Validation Accuracy:0.3547\n",
    "Epoch #174: Loss:1.8680, Accuracy:0.3664, Validation Loss:1.9084, Validation Accuracy:0.3567\n",
    "Epoch #175: Loss:1.8726, Accuracy:0.3638, Validation Loss:1.9059, Validation Accuracy:0.3592\n",
    "Epoch #176: Loss:1.8787, Accuracy:0.3607, Validation Loss:1.9210, Validation Accuracy:0.3448\n",
    "Epoch #177: Loss:1.8751, Accuracy:0.3638, Validation Loss:1.9499, Validation Accuracy:0.3428\n",
    "Epoch #178: Loss:1.8983, Accuracy:0.3562, Validation Loss:1.9243, Validation Accuracy:0.3411\n",
    "Epoch #179: Loss:1.8695, Accuracy:0.3677, Validation Loss:1.8952, Validation Accuracy:0.3645\n",
    "Epoch #180: Loss:1.8733, Accuracy:0.3630, Validation Loss:1.8976, Validation Accuracy:0.3604\n",
    "Epoch #181: Loss:1.8629, Accuracy:0.3700, Validation Loss:1.9133, Validation Accuracy:0.3530\n",
    "Epoch #182: Loss:1.8717, Accuracy:0.3641, Validation Loss:1.8937, Validation Accuracy:0.3608\n",
    "Epoch #183: Loss:1.8647, Accuracy:0.3658, Validation Loss:1.8983, Validation Accuracy:0.3539\n",
    "Epoch #184: Loss:1.8591, Accuracy:0.3671, Validation Loss:1.8843, Validation Accuracy:0.3608\n",
    "Epoch #185: Loss:1.8609, Accuracy:0.3684, Validation Loss:1.8926, Validation Accuracy:0.3571\n",
    "Epoch #186: Loss:1.8592, Accuracy:0.3695, Validation Loss:1.9053, Validation Accuracy:0.3534\n",
    "Epoch #187: Loss:1.8731, Accuracy:0.3637, Validation Loss:1.9393, Validation Accuracy:0.3461\n",
    "Epoch #188: Loss:1.8741, Accuracy:0.3656, Validation Loss:1.9035, Validation Accuracy:0.3576\n",
    "Epoch #189: Loss:1.8751, Accuracy:0.3665, Validation Loss:1.9012, Validation Accuracy:0.3637\n",
    "Epoch #190: Loss:1.8637, Accuracy:0.3694, Validation Loss:1.9361, Validation Accuracy:0.3403\n",
    "Epoch #191: Loss:1.8765, Accuracy:0.3633, Validation Loss:1.8740, Validation Accuracy:0.3666\n",
    "Epoch #192: Loss:1.8495, Accuracy:0.3698, Validation Loss:1.8775, Validation Accuracy:0.3592\n",
    "Epoch #193: Loss:1.8482, Accuracy:0.3764, Validation Loss:1.8866, Validation Accuracy:0.3617\n",
    "Epoch #194: Loss:1.8508, Accuracy:0.3694, Validation Loss:1.9210, Validation Accuracy:0.3576\n",
    "Epoch #195: Loss:1.8730, Accuracy:0.3613, Validation Loss:1.9019, Validation Accuracy:0.3543\n",
    "Epoch #196: Loss:1.8535, Accuracy:0.3723, Validation Loss:1.9099, Validation Accuracy:0.3666\n",
    "Epoch #197: Loss:1.8755, Accuracy:0.3671, Validation Loss:1.8825, Validation Accuracy:0.3678\n",
    "Epoch #198: Loss:1.8448, Accuracy:0.3740, Validation Loss:1.8788, Validation Accuracy:0.3682\n",
    "Epoch #199: Loss:1.8484, Accuracy:0.3738, Validation Loss:1.9223, Validation Accuracy:0.3489\n",
    "Epoch #200: Loss:1.8710, Accuracy:0.3672, Validation Loss:1.8652, Validation Accuracy:0.3666\n",
    "Epoch #201: Loss:1.8462, Accuracy:0.3743, Validation Loss:1.8699, Validation Accuracy:0.3690\n",
    "Epoch #202: Loss:1.8346, Accuracy:0.3779, Validation Loss:1.9155, Validation Accuracy:0.3493\n",
    "Epoch #203: Loss:1.8647, Accuracy:0.3672, Validation Loss:1.8727, Validation Accuracy:0.3645\n",
    "Epoch #204: Loss:1.8518, Accuracy:0.3726, Validation Loss:1.8847, Validation Accuracy:0.3621\n",
    "Epoch #205: Loss:1.8414, Accuracy:0.3742, Validation Loss:1.8667, Validation Accuracy:0.3662\n",
    "Epoch #206: Loss:1.8393, Accuracy:0.3779, Validation Loss:1.8591, Validation Accuracy:0.3690\n",
    "Epoch #207: Loss:1.8314, Accuracy:0.3787, Validation Loss:1.8786, Validation Accuracy:0.3654\n",
    "Epoch #208: Loss:1.8325, Accuracy:0.3763, Validation Loss:1.8564, Validation Accuracy:0.3736\n",
    "Epoch #209: Loss:1.8402, Accuracy:0.3758, Validation Loss:1.8530, Validation Accuracy:0.3723\n",
    "Epoch #210: Loss:1.8333, Accuracy:0.3812, Validation Loss:1.8590, Validation Accuracy:0.3666\n",
    "Epoch #211: Loss:1.8331, Accuracy:0.3753, Validation Loss:1.8535, Validation Accuracy:0.3748\n",
    "Epoch #212: Loss:1.8273, Accuracy:0.3836, Validation Loss:1.8506, Validation Accuracy:0.3715\n",
    "Epoch #213: Loss:1.8243, Accuracy:0.3843, Validation Loss:1.8732, Validation Accuracy:0.3658\n",
    "Epoch #214: Loss:1.8350, Accuracy:0.3804, Validation Loss:1.8605, Validation Accuracy:0.3760\n",
    "Epoch #215: Loss:1.8190, Accuracy:0.3841, Validation Loss:1.8661, Validation Accuracy:0.3756\n",
    "Epoch #216: Loss:1.8347, Accuracy:0.3823, Validation Loss:1.8462, Validation Accuracy:0.3781\n",
    "Epoch #217: Loss:1.8227, Accuracy:0.3832, Validation Loss:1.8702, Validation Accuracy:0.3703\n",
    "Epoch #218: Loss:1.8325, Accuracy:0.3829, Validation Loss:1.8969, Validation Accuracy:0.3682\n",
    "Epoch #219: Loss:1.8307, Accuracy:0.3813, Validation Loss:1.8645, Validation Accuracy:0.3736\n",
    "Epoch #220: Loss:1.8275, Accuracy:0.3819, Validation Loss:1.9085, Validation Accuracy:0.3633\n",
    "Epoch #221: Loss:1.8439, Accuracy:0.3748, Validation Loss:1.8962, Validation Accuracy:0.3637\n",
    "Epoch #222: Loss:1.8358, Accuracy:0.3808, Validation Loss:1.8496, Validation Accuracy:0.3805\n",
    "Epoch #223: Loss:1.8109, Accuracy:0.3902, Validation Loss:1.9080, Validation Accuracy:0.3551\n",
    "Epoch #224: Loss:1.8309, Accuracy:0.3807, Validation Loss:1.8387, Validation Accuracy:0.3789\n",
    "Epoch #225: Loss:1.8096, Accuracy:0.3897, Validation Loss:1.8392, Validation Accuracy:0.3859\n",
    "Epoch #226: Loss:1.8192, Accuracy:0.3890, Validation Loss:1.8971, Validation Accuracy:0.3654\n",
    "Epoch #227: Loss:1.8283, Accuracy:0.3819, Validation Loss:1.8473, Validation Accuracy:0.3789\n",
    "Epoch #228: Loss:1.8145, Accuracy:0.3884, Validation Loss:1.8454, Validation Accuracy:0.3777\n",
    "Epoch #229: Loss:1.8147, Accuracy:0.3875, Validation Loss:1.8329, Validation Accuracy:0.3838\n",
    "Epoch #230: Loss:1.8077, Accuracy:0.3902, Validation Loss:1.8691, Validation Accuracy:0.3662\n",
    "Epoch #231: Loss:1.8302, Accuracy:0.3848, Validation Loss:1.9354, Validation Accuracy:0.3526\n",
    "Epoch #232: Loss:1.8624, Accuracy:0.3683, Validation Loss:1.8875, Validation Accuracy:0.3756\n",
    "Epoch #233: Loss:1.8111, Accuracy:0.3910, Validation Loss:1.8248, Validation Accuracy:0.3883\n",
    "Epoch #234: Loss:1.8008, Accuracy:0.3917, Validation Loss:1.8506, Validation Accuracy:0.3814\n",
    "Epoch #235: Loss:1.8238, Accuracy:0.3848, Validation Loss:1.8368, Validation Accuracy:0.3805\n",
    "Epoch #236: Loss:1.8022, Accuracy:0.3960, Validation Loss:1.8357, Validation Accuracy:0.3818\n",
    "Epoch #237: Loss:1.8009, Accuracy:0.3951, Validation Loss:1.8568, Validation Accuracy:0.3686\n",
    "Epoch #238: Loss:1.8115, Accuracy:0.3884, Validation Loss:1.8484, Validation Accuracy:0.3801\n",
    "Epoch #239: Loss:1.8007, Accuracy:0.3874, Validation Loss:1.8324, Validation Accuracy:0.3846\n",
    "Epoch #240: Loss:1.8105, Accuracy:0.3885, Validation Loss:1.8263, Validation Accuracy:0.3851\n",
    "Epoch #241: Loss:1.8003, Accuracy:0.3938, Validation Loss:1.8211, Validation Accuracy:0.3822\n",
    "Epoch #242: Loss:1.7990, Accuracy:0.3929, Validation Loss:1.8263, Validation Accuracy:0.3818\n",
    "Epoch #243: Loss:1.7943, Accuracy:0.3955, Validation Loss:1.8279, Validation Accuracy:0.3842\n",
    "Epoch #244: Loss:1.8005, Accuracy:0.3931, Validation Loss:1.8551, Validation Accuracy:0.3666\n",
    "Epoch #245: Loss:1.8116, Accuracy:0.3883, Validation Loss:1.8665, Validation Accuracy:0.3703\n",
    "Epoch #246: Loss:1.8027, Accuracy:0.3903, Validation Loss:1.8260, Validation Accuracy:0.3855\n",
    "Epoch #247: Loss:1.7977, Accuracy:0.3954, Validation Loss:1.8428, Validation Accuracy:0.3744\n",
    "Epoch #248: Loss:1.7989, Accuracy:0.3923, Validation Loss:1.8320, Validation Accuracy:0.3781\n",
    "Epoch #249: Loss:1.8037, Accuracy:0.3913, Validation Loss:1.8567, Validation Accuracy:0.3797\n",
    "Epoch #250: Loss:1.7995, Accuracy:0.3956, Validation Loss:1.8369, Validation Accuracy:0.3822\n",
    "Epoch #251: Loss:1.7935, Accuracy:0.3959, Validation Loss:1.8430, Validation Accuracy:0.3810\n",
    "Epoch #252: Loss:1.7897, Accuracy:0.3960, Validation Loss:1.8340, Validation Accuracy:0.3818\n",
    "Epoch #253: Loss:1.7926, Accuracy:0.3988, Validation Loss:1.8604, Validation Accuracy:0.3744\n",
    "Epoch #254: Loss:1.8131, Accuracy:0.3876, Validation Loss:1.8517, Validation Accuracy:0.3670\n",
    "Epoch #255: Loss:1.8301, Accuracy:0.3805, Validation Loss:1.8222, Validation Accuracy:0.3818\n",
    "Epoch #256: Loss:1.7891, Accuracy:0.3991, Validation Loss:1.8135, Validation Accuracy:0.3978\n",
    "Epoch #257: Loss:1.7790, Accuracy:0.4037, Validation Loss:1.8389, Validation Accuracy:0.3756\n",
    "Epoch #258: Loss:1.7946, Accuracy:0.3960, Validation Loss:1.8058, Validation Accuracy:0.3937\n",
    "Epoch #259: Loss:1.7800, Accuracy:0.3976, Validation Loss:1.8916, Validation Accuracy:0.3612\n",
    "Epoch #260: Loss:1.8157, Accuracy:0.3880, Validation Loss:1.8207, Validation Accuracy:0.3851\n",
    "Epoch #261: Loss:1.7946, Accuracy:0.3921, Validation Loss:1.8907, Validation Accuracy:0.3637\n",
    "Epoch #262: Loss:1.8122, Accuracy:0.3863, Validation Loss:1.8107, Validation Accuracy:0.3912\n",
    "Epoch #263: Loss:1.7989, Accuracy:0.3908, Validation Loss:1.8050, Validation Accuracy:0.3900\n",
    "Epoch #264: Loss:1.7723, Accuracy:0.4057, Validation Loss:1.8335, Validation Accuracy:0.3871\n",
    "Epoch #265: Loss:1.7796, Accuracy:0.4034, Validation Loss:1.8442, Validation Accuracy:0.3727\n",
    "Epoch #266: Loss:1.8036, Accuracy:0.3881, Validation Loss:1.8375, Validation Accuracy:0.3863\n",
    "Epoch #267: Loss:1.7826, Accuracy:0.4023, Validation Loss:1.8057, Validation Accuracy:0.3949\n",
    "Epoch #268: Loss:1.7800, Accuracy:0.4002, Validation Loss:1.8208, Validation Accuracy:0.3904\n",
    "Epoch #269: Loss:1.7784, Accuracy:0.4011, Validation Loss:1.7987, Validation Accuracy:0.3900\n",
    "Epoch #270: Loss:1.7658, Accuracy:0.4073, Validation Loss:1.8173, Validation Accuracy:0.3908\n",
    "Epoch #271: Loss:1.7726, Accuracy:0.4036, Validation Loss:1.8426, Validation Accuracy:0.3826\n",
    "Epoch #272: Loss:1.7861, Accuracy:0.4000, Validation Loss:1.8079, Validation Accuracy:0.3822\n",
    "Epoch #273: Loss:1.7783, Accuracy:0.4012, Validation Loss:1.8137, Validation Accuracy:0.3904\n",
    "Epoch #274: Loss:1.7931, Accuracy:0.3977, Validation Loss:1.8750, Validation Accuracy:0.3662\n",
    "Epoch #275: Loss:1.8309, Accuracy:0.3834, Validation Loss:1.8326, Validation Accuracy:0.3892\n",
    "Epoch #276: Loss:1.7923, Accuracy:0.3959, Validation Loss:1.8279, Validation Accuracy:0.3871\n",
    "Epoch #277: Loss:1.7991, Accuracy:0.3979, Validation Loss:1.8111, Validation Accuracy:0.3916\n",
    "Epoch #278: Loss:1.7693, Accuracy:0.4062, Validation Loss:1.8082, Validation Accuracy:0.3785\n",
    "Epoch #279: Loss:1.7875, Accuracy:0.3910, Validation Loss:1.8085, Validation Accuracy:0.3904\n",
    "Epoch #280: Loss:1.7889, Accuracy:0.4017, Validation Loss:1.8276, Validation Accuracy:0.3896\n",
    "Epoch #281: Loss:1.7781, Accuracy:0.4040, Validation Loss:1.8411, Validation Accuracy:0.3744\n",
    "Epoch #282: Loss:1.7826, Accuracy:0.3970, Validation Loss:1.8332, Validation Accuracy:0.3818\n",
    "Epoch #283: Loss:1.7758, Accuracy:0.4027, Validation Loss:1.7984, Validation Accuracy:0.3924\n",
    "Epoch #284: Loss:1.7662, Accuracy:0.4081, Validation Loss:1.8097, Validation Accuracy:0.3953\n",
    "Epoch #285: Loss:1.7633, Accuracy:0.4052, Validation Loss:1.8041, Validation Accuracy:0.3957\n",
    "Epoch #286: Loss:1.7630, Accuracy:0.4055, Validation Loss:1.8124, Validation Accuracy:0.4011\n",
    "Epoch #287: Loss:1.7776, Accuracy:0.3992, Validation Loss:1.7900, Validation Accuracy:0.3986\n",
    "Epoch #288: Loss:1.7585, Accuracy:0.4076, Validation Loss:1.8392, Validation Accuracy:0.3785\n",
    "Epoch #289: Loss:1.7863, Accuracy:0.3963, Validation Loss:1.8239, Validation Accuracy:0.3916\n",
    "Epoch #290: Loss:1.7663, Accuracy:0.4068, Validation Loss:1.8016, Validation Accuracy:0.3994\n",
    "Epoch #291: Loss:1.7648, Accuracy:0.4055, Validation Loss:1.7875, Validation Accuracy:0.4039\n",
    "Epoch #292: Loss:1.7779, Accuracy:0.4006, Validation Loss:1.8442, Validation Accuracy:0.3777\n",
    "Epoch #293: Loss:1.7718, Accuracy:0.4024, Validation Loss:1.8028, Validation Accuracy:0.3970\n",
    "Epoch #294: Loss:1.7710, Accuracy:0.4025, Validation Loss:1.7831, Validation Accuracy:0.4007\n",
    "Epoch #295: Loss:1.7650, Accuracy:0.4075, Validation Loss:1.7953, Validation Accuracy:0.3957\n",
    "Epoch #296: Loss:1.7618, Accuracy:0.4114, Validation Loss:1.7872, Validation Accuracy:0.3957\n",
    "Epoch #297: Loss:1.7521, Accuracy:0.4112, Validation Loss:1.7820, Validation Accuracy:0.3961\n",
    "Epoch #298: Loss:1.7487, Accuracy:0.4157, Validation Loss:1.8049, Validation Accuracy:0.3990\n",
    "Epoch #299: Loss:1.7655, Accuracy:0.4068, Validation Loss:1.7924, Validation Accuracy:0.4019\n",
    "Epoch #300: Loss:1.7603, Accuracy:0.4075, Validation Loss:1.8381, Validation Accuracy:0.3851\n",
    "\n",
    "Test:\n",
    "Test Loss:1.83805859, Accuracy:0.3851\n",
    "Labels: ['ds', 'eo', 'eb', 'by', 'ib', 'ek', 'eg', 'yd', 'mb', 'aa', 'ce', 'sk', 'ck', 'my', 'sg']\n",
    "Confusion Matrix:\n",
    "      ds  eo   eb  by  ib  ek   eg   yd   mb  aa  ce  sk  ck  my  sg\n",
    "t:ds  49   0   21   0   0   2   13    1   18   6   5   1   3   7   0\n",
    "t:eo   0  61    0  21   5   2   12    0   20   2   4   0   1   0   7\n",
    "t:eb  17   0  149   0   2   9    5    2    2   0   0   3   1  11   0\n",
    "t:by   0  16    0  94   1   1    7    3    5   4  14   0   0   0  17\n",
    "t:ib   2  11    4   5  48  11   15   57   42   2   6   0   0   0  14\n",
    "t:ek  33   0   54   2   2  32   25    9   12   1   8   1   2   1   9\n",
    "t:eg  18  10   10   7   3   4  107    4    8   9  11   1   1   0   5\n",
    "t:yd   2   5    1  19  12   4    7  128   24   0   8   1   1   2  35\n",
    "t:mb  18   7   10   8   1   7    4   16  107   0  17   1   4   5   2\n",
    "t:aa   4   0    4   9   3   7   42    7    5  37   7   0   0   0  12\n",
    "t:ce   9   6    2  18   2   1   10    6   26   0  18   0   6   1   4\n",
    "t:sk  30   3   48   0   0   4   11    0   13   3   5   2   5   5   1\n",
    "t:ck  29   0    8   0   0   4    2    0   19   0  14   0  10   1   4\n",
    "t:my  17   0   21   0   0   2    1    1   22   0   3   0   0  13   0\n",
    "t:sg   0  17    0  42   4   3    1   30    9   6   7   0   1   0  83\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          ds       0.21      0.39      0.28       126\n",
    "          eo       0.45      0.45      0.45       135\n",
    "          eb       0.45      0.74      0.56       201\n",
    "          by       0.42      0.58      0.49       162\n",
    "          ib       0.58      0.22      0.32       217\n",
    "          ek       0.34      0.17      0.23       191\n",
    "          eg       0.41      0.54      0.47       198\n",
    "          yd       0.48      0.51      0.50       249\n",
    "          mb       0.32      0.52      0.40       207\n",
    "          aa       0.53      0.27      0.36       137\n",
    "          ce       0.14      0.17      0.15       109\n",
    "          sk       0.20      0.02      0.03       130\n",
    "          ck       0.29      0.11      0.16        91\n",
    "          my       0.28      0.16      0.21        80\n",
    "          sg       0.43      0.41      0.42       203\n",
    "\n",
    "    accuracy                           0.39      2436\n",
    "   macro avg       0.37      0.35      0.33      2436\n",
    "weighted avg       0.39      0.39      0.36      2436\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 10:46:44 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 1 hours, 2 minutes, 25 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.6862868190007454, 2.6751746117383584, 2.668152620443961, 2.664427492223154, 2.6626738773778156, 2.6615511329694725, 2.6612074093278406, 2.6609751559634907, 2.6606177402834588, 2.6604928833314743, 2.658472173123916, 2.6553098673890965, 2.642383369906195, 2.609855850341872, 2.5768797593359487, 2.550669558138291, 2.520606333007562, 2.500931612963747, 2.48157465712386, 2.461258946185433, 2.4196643175554198, 2.3806866781269194, 2.330557814959822, 2.31080811951548, 2.403625317199281, 2.2815169274121865, 2.256387375845698, 2.247148534151525, 2.2461854221394106, 2.269577087831419, 2.2579766987579797, 2.2118673175818033, 2.1967939907694096, 2.2343337931264995, 2.182737985072269, 2.1905948626388274, 2.1642098532521667, 2.152973902832307, 2.127267529420273, 2.1381123363482346, 2.1141696767071, 2.0998687567969263, 2.0854074888432947, 2.136566938829344, 2.145333135460789, 2.0842080558657843, 2.0723432782052575, 2.1039644043238095, 2.0521138172431534, 2.064972071420579, 2.0560170096912604, 2.0397129415095536, 2.035016460763214, 2.0291764603068283, 2.0307437007258873, 2.0361810090702352, 2.0489255066575676, 2.0561249436220317, 2.0183881072966727, 2.0125339461860596, 2.013238651216128, 2.034177124402402, 2.0899598390989507, 2.026618353838991, 2.009466499334877, 2.1120700773542933, 2.01258067152966, 2.0034317257760583, 2.004745154936717, 2.011633199228246, 2.014893596395483, 1.9970221018360557, 1.9918463371070148, 2.0132878899378532, 1.9890071500111095, 1.993353175412258, 1.9926531972556278, 1.9864207680393713, 1.99211176941156, 2.0296578853588385, 1.9884221495078702, 1.9938447863010351, 2.0405179510758624, 1.9987315019754746, 2.024816814510302, 1.9748909849251433, 1.9726070981894808, 1.9842795213846542, 2.0000372790350704, 1.9695528782842977, 2.0025735574794323, 1.9835019863297787, 1.9682511073615163, 2.034410258623571, 1.9803872116289312, 1.9649841953772433, 2.129770505212994, 2.111172391863292, 2.018834572511745, 1.9865931986979468, 1.968642371824418, 1.961910659260742, 1.9643986084191083, 1.9565922231314021, 1.956829211590521, 1.9653948794053302, 1.984723985293033, 1.9550030129687932, 1.965898339971533, 1.955570131687108, 1.971900784519114, 1.957170814520424, 1.9624022693665353, 1.9634795725247738, 1.9559436490383055, 1.966419863974911, 1.9476154856689654, 1.9602993779581757, 1.9528111475833336, 1.9496625362358657, 1.9486236008517261, 1.943331095972672, 1.9375441403224551, 1.9879820311598002, 1.9443355295654197, 1.941279508014422, 2.0000970250084285, 1.9762646365048262, 1.9562755327898098, 1.9431671391566987, 1.9444259802500408, 1.9361341665139535, 1.9289602544311624, 1.9306639450524241, 1.9321188300309706, 1.933962452392077, 1.937526217039387, 2.0775210301472833, 2.0097198357135793, 1.9364806787525295, 1.9346847886522416, 1.9292307291516333, 1.9540544194345209, 1.932535371169668, 1.9207041733370627, 1.9228355181824006, 1.9285648967244942, 2.0122231791172123, 1.9688723803741004, 1.9255187904893472, 1.9165555626300756, 1.9173491774325693, 1.9275098911842885, 1.920535912458924, 1.9111316004410166, 1.931225221536821, 1.9298762750547311, 1.90981985919777, 1.9280188567141203, 1.909931075592542, 1.9632581122011583, 1.9023605103563206, 1.9340318803521017, 1.9172421566567006, 1.9008435500274934, 1.9102647482663735, 1.8986600905607878, 1.924255401238628, 1.946727928073927, 1.8977849211403106, 1.9189052693362307, 1.8954744808779562, 1.901417364432111, 1.9083611548240549, 1.9059250617066432, 1.9210257126975725, 1.9498958932159374, 1.9242972438950061, 1.895161621872036, 1.8976233784592602, 1.9133259756811734, 1.893699361381468, 1.898269870794074, 1.884294184949402, 1.8925758743129537, 1.90529449056522, 1.9392563357141805, 1.90347636626859, 1.9011874396616995, 1.9360836393923204, 1.873982802204702, 1.8775167150058965, 1.886611019645027, 1.920979321883817, 1.9018971105710234, 1.909913518158673, 1.8825436767881922, 1.878827433476503, 1.922287584721357, 1.865195948110621, 1.8698816984549336, 1.9155384630992496, 1.8726762474464078, 1.8847341067685282, 1.8666981131768188, 1.8590524593989055, 1.878610664792053, 1.8564038200331439, 1.8529958662336878, 1.8589967440306063, 1.8534826860443516, 1.8505682669249661, 1.8731990201132638, 1.8605415274944213, 1.8660505107666667, 1.8461676700949081, 1.8701552649828406, 1.8968967312858218, 1.8644524190226213, 1.9084584129659217, 1.8961776365787524, 1.8496004290181427, 1.908025079955804, 1.8387336390359061, 1.8392195907132378, 1.8971344821754543, 1.8473293495491416, 1.845375247581056, 1.8328943859375952, 1.869115100509819, 1.9354383291673583, 1.8874686087293577, 1.8248009597530896, 1.8506297524926698, 1.8367978023190803, 1.8357159674461252, 1.856785560280623, 1.8483902703365083, 1.8324230654877787, 1.8262931625244065, 1.8210707871588971, 1.8263425055787286, 1.8278615002953165, 1.855123050890141, 1.8664903260999908, 1.8259555283438396, 1.8428224235136912, 1.8320450271878923, 1.8566659277883069, 1.83689022984215, 1.842978986809015, 1.8339551573707944, 1.8603714393277473, 1.8516967243748932, 1.8221513187552516, 1.8134991402305014, 1.8388611485413926, 1.8057979593919025, 1.8915792590095883, 1.820677135574015, 1.890673455933632, 1.810714924276756, 1.8049966819180643, 1.833453504127039, 1.84419635872927, 1.837547593711828, 1.8057106422086067, 1.8207765459427105, 1.7987062584590443, 1.8172903857599143, 1.8425888496470961, 1.8079159287200577, 1.8136738750147703, 1.8749742627339605, 1.832640324515858, 1.8279345720663838, 1.8110834088035797, 1.8081933404816, 1.8084977107682252, 1.8275979383434178, 1.8411379183454466, 1.8331801685793647, 1.7983575297889647, 1.809738151545595, 1.8040653813648693, 1.8123688410068381, 1.7900304152264774, 1.8391942418071827, 1.8238708364160974, 1.801612441762915, 1.787472836294002, 1.844213907550317, 1.8028394518227413, 1.7831194363595622, 1.7952739887049634, 1.787208094776949, 1.7819600299074145, 1.8049410156820012, 1.7923867179842419, 1.8380586811278645], 'val_acc': [0.0890804596355396, 0.08128078770020912, 0.08128078770020912, 0.10221674851155634, 0.10221674851155634, 0.10221674851155634, 0.10221674851155634, 0.10221674851155634, 0.10221674851155634, 0.10221674851155634, 0.10221674851155634, 0.10385878466081933, 0.10960591109760093, 0.13587848820122592, 0.13793103432371503, 0.14573070492552614, 0.14737274112372561, 0.14696223221486818, 0.14655172323260596, 0.17364531934065577, 0.2019704428114523, 0.20812807842624206, 0.23193760226023413, 0.2389162559618895, 0.19252873489813657, 0.2372742199104995, 0.24507389140540156, 0.24958949096880131, 0.24014778300654907, 0.23522167470556957, 0.23275862049390922, 0.2512315269957231, 0.26313628890048496, 0.2524630541137874, 0.2594417076196968, 0.2586206895817677, 0.2619047617334842, 0.26559934306320887, 0.269293924343997, 0.27093596049326, 0.28284072019587986, 0.2828407223735537, 0.2943349755162676, 0.27832512298143164, 0.2766830869545099, 0.288998357841534, 0.2992610839885248, 0.27996715925303584, 0.29844006338143, 0.2980295543991678, 0.2931034480801161, 0.30377668088488585, 0.3058292283408943, 0.3136288999336694, 0.3103448276840799, 0.3123973729868828, 0.3025451561891778, 0.3160919521144654, 0.31362889998260585, 0.3210180625931187, 0.3206075536108565, 0.30829228252808644, 0.2939244640871809, 0.32019704472646726, 0.3230706077491121, 0.28078817733990147, 0.31896551743712526, 0.3136288975357814, 0.31609195177190996, 0.32430213259162965, 0.32676518677882177, 0.3263546801455111, 0.3230706078959216, 0.31937602407043597, 0.3267651890788368, 0.3230706077491121, 0.3275862071901707, 0.32430213489164467, 0.32348111678031083, 0.31814449702577635, 0.3267651891277733, 0.31609195416979796, 0.3148604272230114, 0.32553366208311374, 0.309523809768492, 0.32224958968671474, 0.33169129750215753, 0.3210180602441672, 0.3288177319348152, 0.33087027948869663, 0.3255336621320502, 0.3292282410638869, 0.32881773183694224, 0.3193760242172454, 0.32266009881578644, 0.3300492589794748, 0.28735632174120745, 0.29022988488619356, 0.31978653545058616, 0.3271756982079085, 0.3288177319348152, 0.33661740367439974, 0.3325123132645399, 0.3292282409170774, 0.33497536984962, 0.33538587667867664, 0.3267651890788368, 0.33292282229573855, 0.3312807885688318, 0.33251231331347636, 0.32594417096750294, 0.3296387499972126, 0.3378489308169323, 0.33128078621988033, 0.3353858766297401, 0.3333333314248102, 0.3374384241346851, 0.33661740597441475, 0.336206894643201, 0.33908046021054333, 0.3399014782729407, 0.3292282410638869, 0.33908046021054333, 0.32594416886323385, 0.3321018066312292, 0.33128078861776833, 0.320607553464047, 0.32758620723910714, 0.33866994887932966, 0.33087027963550614, 0.3399014759729257, 0.33456485856734275, 0.34072249403532306, 0.3394909693885515, 0.3448275844451829, 0.3456486025075803, 0.33292282479149954, 0.30295566292036147, 0.31773398804351416, 0.3411330054644098, 0.3431855484671976, 0.3489326770571848, 0.3345648609162943, 0.3362068946921375, 0.34441707531611127, 0.34441707531611127, 0.3435960573515869, 0.3185550083080536, 0.331691295153206, 0.34811165928840637, 0.3513957313911864, 0.3477011501103982, 0.34072249403532306, 0.347290638730248, 0.35057471332878903, 0.3390804603084163, 0.336206894643201, 0.3513957290422349, 0.34688013204800083, 0.34975369291744013, 0.32758620474334615, 0.35509031027408655, 0.3374384240368121, 0.3464696229189292, 0.3563218375144921, 0.34318554836932463, 0.35550082180104625, 0.3485221681238591, 0.34072249413319605, 0.3522167494535838, 0.3456486046118494, 0.3624794753314239, 0.3546798037386489, 0.35673234899251527, 0.3591954007328829, 0.34482758659838847, 0.3427750417849505, 0.3411330053665368, 0.36453201818740233, 0.36042693012649396, 0.3530377675649176, 0.3608374391576927, 0.353858785529442, 0.36083743935343865, 0.35714285557688946, 0.3534482743450378, 0.34605911364304803, 0.35755336695703965, 0.36371100237608345, 0.3403119848573149, 0.36658456329445926, 0.3591954031797074, 0.36165845731796303, 0.35755336465702464, 0.3542692946095772, 0.3665845658391567, 0.36781609268807036, 0.368226601817142, 0.3489326749039792, 0.36658456339233225, 0.36904761743271486, 0.34934318623519295, 0.3645320182363388, 0.3620689639023372, 0.36617405666114855, 0.36904761992847585, 0.3653530361029902, 0.3735632193695344, 0.37233168982911385, 0.3665845634412687, 0.3747947463652575, 0.3715106716688435, 0.3657635452320619, 0.3760262735567265, 0.3756157622255128, 0.378078818614847, 0.37027914462418393, 0.368226599615, 0.3735632168248369, 0.36330049119167923, 0.36371100007606844, 0.380541870599897, 0.3550903126230381, 0.3788998368240538, 0.38587848795654345, 0.3653530361519267, 0.3788998368240538, 0.37766830973045773, 0.383825942996296, 0.36617405675902154, 0.35262725598902145, 0.37561576198083035, 0.3883415421926721, 0.3813628887601674, 0.38054187045308757, 0.38177339989563513, 0.36863710845045267, 0.3801313613729524, 0.3846469633587084, 0.3850574722430976, 0.3821839065778823, 0.3817734001403176, 0.38423645418070024, 0.36658456329445926, 0.37027914462418393, 0.3854679789253448, 0.37438423498510726, 0.37807881621695894, 0.3797208547885782, 0.3821839091715163, 0.38095238202898374, 0.3817734000913811, 0.3743842349361708, 0.36699507477248244, 0.3817734000424446, 0.39778324995917835, 0.3756157621276398, 0.3936781596471915, 0.36124794574206687, 0.3850574722430976, 0.36371100237608345, 0.39121510790682384, 0.38998357821959384, 0.38711001495226655, 0.3727422012582005, 0.38628899698774216, 0.39490968678972405, 0.39039408979549, 0.38998358061748184, 0.39080459642880067, 0.3825944180559055, 0.3821839066268188, 0.3903940874465385, 0.36617405666114855, 0.3891625600593235, 0.38711001749696405, 0.3916256168890861, 0.37848932534603064, 0.390394087299729, 0.3895730692862681, 0.37438423513191676, 0.3817734000424446, 0.39244663490254694, 0.3953201980230648, 0.3957307048031849, 0.4010673247045288, 0.39860426792370274, 0.3784893274992362, 0.3916256166444036, 0.3994252885307976, 0.40394088552503166, 0.3776683070389508, 0.3969622342457325, 0.40065681562439365, 0.3957307049010579, 0.39573070470531196, 0.3961412138833201, 0.39901477695490145, 0.40188834271798973, 0.3850574698941461], 'loss': [2.697203948316633, 2.6812725338358163, 2.6721026333450535, 2.6668820484952516, 2.6649116348681754, 2.662195152521623, 2.6621722522947087, 2.6617808697404803, 2.6613412211807845, 2.66121171632097, 2.6601826747095316, 2.6578386170663384, 2.6512387058328555, 2.6303267521045535, 2.5946824005252282, 2.5629436348987555, 2.535749479289907, 2.508535510950265, 2.487158240234093, 2.46207594656112, 2.4310354543172847, 2.3830037071964334, 2.3457808987071136, 2.319854735055254, 2.2958100879951178, 2.2972675185428755, 2.278089171709221, 2.248710894437786, 2.233713445526374, 2.2264341813582904, 2.2454010627597754, 2.237637346334281, 2.197037812033228, 2.1817268762255595, 2.193300413742692, 2.1597564322258167, 2.163029885683706, 2.1325058774536885, 2.1443322443129835, 2.1189333545109084, 2.101660870722432, 2.1010631318944193, 2.0790691159344306, 2.0863555586068783, 2.085055214962186, 2.079451511185272, 2.0596798443451556, 2.067352307944327, 2.0584214088118786, 2.049538983652479, 2.042987210060292, 2.041767132796301, 2.0316167891147936, 2.0239133815256234, 2.024008040653362, 2.01914948976505, 2.02645269825718, 2.030778389544947, 2.026378262606, 2.005581935279423, 2.006403163130523, 2.0053267408445388, 2.022548921592916, 2.02612066195486, 2.00394260242, 1.9988936773805404, 2.074555240813222, 2.010292416333663, 2.000341877653369, 1.9917969310063357, 1.9872372020686186, 1.9897664256164425, 1.9895690346155812, 1.9809082345061724, 1.9889578625406819, 1.977716322454339, 1.9766208282486368, 1.977760540827099, 1.975488115972562, 1.979044791707268, 2.0011729589967513, 1.9749267194305358, 1.9813366371503356, 1.9846594125583186, 1.980773944776406, 2.005996761184943, 1.9578518721112481, 1.9593696958475288, 1.9633336564109065, 1.9759499779961682, 1.9559123259550248, 1.9767494087591309, 1.9696023748151086, 1.9514023366650026, 1.965473894514832, 1.9678347917552845, 1.9558710308779925, 2.0073355197416927, 1.9823785488610395, 1.9540861076398062, 1.947311925349539, 1.9425776463514481, 1.9392051628238123, 1.9404344128876985, 1.9453318693799404, 1.9422470118230863, 1.9423563721733172, 1.9467712150217327, 1.9343406015842601, 1.9306445227511364, 1.9317669398975568, 1.9325928260658312, 1.9288223060004765, 1.9476569767605352, 1.9316966114592504, 1.926615752184905, 1.9308202616732713, 1.924667807232428, 1.942344953000423, 1.9378278762163323, 1.9222185115305062, 1.92131765970704, 1.9173712280986246, 1.914239690338072, 1.9449644626288443, 1.9249593390576403, 1.9143735065107719, 1.9498990685053674, 1.924910121385077, 1.9276313030744234, 1.9151621253583466, 1.9198023249236464, 1.9082632440805924, 1.9082155614907736, 1.9054174526026606, 1.904639152530772, 1.9053194375008773, 1.908883738077152, 1.9973527465757648, 1.9260463225523305, 1.909253598973002, 1.9021500900785537, 1.8992572866181328, 1.9156519703796513, 1.9045938411532486, 1.8999249860736134, 1.8934363018071136, 1.9001256042437387, 1.9319979598634787, 1.9050500955425005, 1.8974962546840095, 1.888339246371933, 1.8904227503516102, 1.8980619482435974, 1.8896583855764086, 1.8834661348644468, 1.8940016235414228, 1.8893428677161372, 1.8916859937644348, 1.8924640545365257, 1.8806998491287232, 1.8959940744376524, 1.8806362461015673, 1.8769823863521005, 1.8788008464190504, 1.8749691649384077, 1.887332610330053, 1.869949341603618, 1.882429063981074, 1.8903523830417734, 1.867029532172107, 1.8738374385500836, 1.8703473308492735, 1.8679805013433375, 1.872559331232028, 1.8787220424205615, 1.8751474594923014, 1.8983307848720825, 1.8695006127230196, 1.8732809373240695, 1.862949950592229, 1.8716912983379326, 1.8647130217640306, 1.859079485556428, 1.8609390424262326, 1.8592089224644999, 1.873139759304587, 1.874109653232034, 1.8751331231922095, 1.863671033925834, 1.876451606280505, 1.8495125783787125, 1.8482403921150818, 1.8508107330275267, 1.8730309065362512, 1.8534858481350376, 1.875472877745266, 1.844759040642568, 1.848359688008835, 1.8710421270413564, 1.8461541514132302, 1.834592692122567, 1.8647199263562901, 1.851848146753879, 1.841377618523349, 1.8393475749409418, 1.8313558778723653, 1.8324777520902347, 1.840222829910764, 1.8332982663501214, 1.8331165060125583, 1.827286430844536, 1.8243082579156455, 1.8349964416981723, 1.819026467736497, 1.8346834818195759, 1.8227344156045933, 1.83246095875695, 1.8307243969895757, 1.8275404946759986, 1.8439469397679982, 1.835788416372922, 1.8109267835010003, 1.830876858327423, 1.8096073013556322, 1.8191785075581295, 1.8282808817387606, 1.8144628827331981, 1.8146964467281679, 1.8076935948287682, 1.8302217074243439, 1.8623573944064382, 1.8110794701370616, 1.8008083911891835, 1.8238314195824845, 1.8022438506081364, 1.8008555245840085, 1.8114774390657336, 1.8007188162030137, 1.8105363500681257, 1.8002612165357053, 1.7990169771887683, 1.7943373042699982, 1.8004715455386184, 1.8116480168375881, 1.8026828301760696, 1.7976536259269322, 1.7988608911052133, 1.8036632319984984, 1.7995097032073097, 1.793515194808678, 1.789701564013346, 1.7925993631507826, 1.8130588836983244, 1.830086625625955, 1.789098886934394, 1.7790359986636184, 1.794589116294281, 1.7799575072783953, 1.815661101076882, 1.7946310359594513, 1.812223847543924, 1.798876770761713, 1.7722700030896699, 1.7795903266088184, 1.8036017905515322, 1.7825764505770172, 1.77995746145503, 1.7783607312541234, 1.7658070825698196, 1.7725749534747928, 1.7860549119953257, 1.778294934286474, 1.7931354004744386, 1.830870848955315, 1.792292732771417, 1.7991359796856952, 1.7693319214932484, 1.7874713485001048, 1.7888615541144808, 1.7781329928482337, 1.782584842862045, 1.7758414457466078, 1.7661570758545424, 1.7633120775712343, 1.7630383561034466, 1.7776432506847186, 1.7584643858903732, 1.7862732419732659, 1.7663010594781174, 1.764827852817042, 1.7778688125297029, 1.771839558368346, 1.7709568646898994, 1.765041280233395, 1.7618084541336467, 1.752071842128981, 1.748738616643745, 1.7654707030838765, 1.7602760276265703], 'acc': [0.08901437371969223, 0.08100616016427105, 0.08151950718685831, 0.09548254620429182, 0.10215605749792632, 0.10215605749792632, 0.10215605749792632, 0.10215605749792632, 0.10215605749486653, 0.10215605749486653, 0.10215605749486653, 0.10277207392197125, 0.11406570841889117, 0.12094455852768014, 0.1301848049311912, 0.13511293635720834, 0.14517453799073946, 0.14815195072480541, 0.14917864476386036, 0.15318275154310085, 0.18028747433876843, 0.20585215605749488, 0.2204312115020331, 0.2223819301878647, 0.2318275154004107, 0.22782340863034956, 0.2313141683808832, 0.2395277207392197, 0.2424024640687682, 0.2404517453798768, 0.23583162217659137, 0.23829568789724942, 0.24722792608414831, 0.2603696098623824, 0.26026694045480514, 0.2661190964970011, 0.2658110882956879, 0.27320328542400435, 0.26652977413342965, 0.27628336755646815, 0.2821355236262022, 0.2783367556345781, 0.2870636550338606, 0.2824435318275154, 0.2811088295718483, 0.2892197125134282, 0.2970225872720536, 0.2920944558643952, 0.29579055442090396, 0.2956878850163865, 0.30184804928743375, 0.3049281314168378, 0.30513347022587267, 0.30708418892394346, 0.31067761808205435, 0.31232032854515424, 0.3070841889117043, 0.30995893225043214, 0.30770020533880904, 0.3216632443593023, 0.31960985626589344, 0.3185831622237787, 0.31457905544147846, 0.314476386036961, 0.32299794661190967, 0.31889117041897236, 0.2944558521682966, 0.3157084188911704, 0.3186858316221766, 0.3225872689938398, 0.3273100616138819, 0.32546201233256766, 0.3250513346777804, 0.3233059548132229, 0.3258726899506375, 0.33039014374940545, 0.32946611909650925, 0.33039014374940545, 0.32843942505745427, 0.3305954825584404, 0.324537987685791, 0.33131416839006256, 0.3285420944680913, 0.3284394250635739, 0.33110882955654936, 0.3201232032884807, 0.33593429156886967, 0.33932238191794567, 0.33377823409848145, 0.33069815195683827, 0.3392197125379065, 0.3305954825584404, 0.32987679672069864, 0.33860369610162244, 0.32967145790554414, 0.3369609856385225, 0.3339835728952772, 0.3224845995893224, 0.32577002054612003, 0.3357289527475956, 0.3406570842011509, 0.3399383983572895, 0.34414784394250514, 0.3444558521560575, 0.3465092402341674, 0.3426078028808629, 0.34086242298570746, 0.33921971252566735, 0.34476386034513157, 0.34404517453798766, 0.34845995891999904, 0.34722792607802877, 0.3470225872751134, 0.33829568787277114, 0.3455852156057495, 0.3491786447760995, 0.34445585214381835, 0.3463039014496108, 0.3440451745257485, 0.34055441478439424, 0.34537987678447546, 0.34445585216829666, 0.3478439425173726, 0.3489733059425863, 0.3443531827637792, 0.34753593426710283, 0.3484599589567165, 0.34137577003277303, 0.3466119096631632, 0.34835728953995987, 0.3512320328419703, 0.34958932236969104, 0.3521560574948665, 0.35297741271876704, 0.35287474333872787, 0.35554414785618166, 0.35092402462841793, 0.3509240246406571, 0.32833675562233894, 0.3480492813141684, 0.35605749486652977, 0.3579055441478439, 0.3601642710594671, 0.3486652977412731, 0.3544147843942505, 0.35236139630390145, 0.3599589322259539, 0.35236139627942314, 0.34763860369609856, 0.35667351128139535, 0.352464065720658, 0.36088295689108923, 0.36160164271659184, 0.3565708418921768, 0.35739219712831644, 0.3565708418646388, 0.35451745380182775, 0.3566735112936345, 0.36108829568788503, 0.36232032854209445, 0.3609856262711284, 0.3548254620000812, 0.36190965089954635, 0.36570841889117045, 0.3581108829446397, 0.36170431212722887, 0.36057494866529777, 0.36478439425051334, 0.3599589322443126, 0.3601642710472279, 0.3698151950841077, 0.3659137577124445, 0.36509240247630487, 0.36642710472891216, 0.3637577002083986, 0.36067761805757603, 0.3637577002053388, 0.3561601642832864, 0.36765913758312163, 0.3630390143737166, 0.3700205338809035, 0.36406570842195096, 0.3658110882834487, 0.36714579055441476, 0.3683778234147438, 0.36950718685831624, 0.3636550307885822, 0.365605749486653, 0.3665297741150709, 0.3694045174660379, 0.3633470225750298, 0.3698151950718686, 0.37638603697322476, 0.36940451742932046, 0.3612936345091591, 0.3722792607925266, 0.3671457905666539, 0.37402464066932334, 0.3738193018480493, 0.36724845995893224, 0.3743326488828757, 0.3779260780165083, 0.36724845995893224, 0.3725872689816007, 0.3742299794661191, 0.37792607800426914, 0.3787474332648871, 0.3762833675564682, 0.3757700205461201, 0.3812114989610668, 0.37525667351129366, 0.38357289528944655, 0.3842915811210687, 0.3803901437432859, 0.38408624229979466, 0.3823408624291175, 0.38316221765913755, 0.3828542094700635, 0.381314168383943, 0.3819301848110477, 0.37484599590546297, 0.3808008213307578, 0.390246406583081, 0.38069815197519696, 0.38973305956049376, 0.38901437374111075, 0.3819301847804498, 0.3883983572895277, 0.3874743326611098, 0.39024640657084186, 0.38480492813447664, 0.3682751539918676, 0.3909650923902249, 0.39168377825856454, 0.3848049281191777, 0.39599589323605844, 0.3950718685586839, 0.388398357314006, 0.3873716632443532, 0.3885010266940452, 0.39383983575343107, 0.3929158110882957, 0.3954825462073516, 0.3931211498973306, 0.3882956878850103, 0.39034907597535934, 0.39537987678447545, 0.3922997946489518, 0.39127310061601644, 0.39558521559351034, 0.39589322381930186, 0.39599589321158013, 0.3987679671335514, 0.3875770020411489, 0.38049281314168376, 0.39907597536546247, 0.40369609856262834, 0.39599589323605844, 0.3976386036838594, 0.3879876796714579, 0.39209445586439523, 0.3863449691869395, 0.39075975358119003, 0.4057494866529774, 0.40338809032459766, 0.38809034906373624, 0.4022587268871448, 0.4002053388090349, 0.4011293634252137, 0.4072895277268588, 0.40359342915811086, 0.4, 0.4012320328419703, 0.397741273100616, 0.38336755648041165, 0.3958932238254214, 0.39794661192189007, 0.40616016427104723, 0.3909650923902249, 0.40174537988903586, 0.404004106800659, 0.39702258728123296, 0.4026694045205136, 0.4081108829446397, 0.40523613960591187, 0.4055441478439425, 0.3991786447760995, 0.40759753592205245, 0.39630390141289334, 0.40677618067367366, 0.40554414783170334, 0.400616016451583, 0.4023613963161406, 0.4024640656839406, 0.4074948665420133, 0.41139630388919823, 0.41119096511688075, 0.4157084188789313, 0.4067761806859128, 0.4074948665420133]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
