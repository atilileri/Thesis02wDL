{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf47.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.27 19:35:31 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Mags', 'channelMode': '0Ov', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '03', '02', '01', '05'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Mags------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7988, 28)\n",
    "Test Batch: (609, 7988, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001EC0C0ECE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001EC095C6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6113, Accuracy:0.1881, Validation Loss:1.6082, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6070, Accuracy:0.2329, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6055, Accuracy:0.2329, Validation Loss:1.6052, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6049, Accuracy:0.2329, Validation Loss:1.6048, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6045, Accuracy:0.2329, Validation Loss:1.6043, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6036, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6023, Accuracy:0.2329, Validation Loss:1.6025, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.5998, Accuracy:0.2329, Validation Loss:1.6010, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.5971, Accuracy:0.2341, Validation Loss:1.5992, Validation Accuracy:0.2381\n",
    "Epoch #10: Loss:1.5940, Accuracy:0.2411, Validation Loss:1.5968, Validation Accuracy:0.2381\n",
    "Epoch #11: Loss:1.5898, Accuracy:0.2398, Validation Loss:1.5946, Validation Accuracy:0.2167\n",
    "Epoch #12: Loss:1.5860, Accuracy:0.2366, Validation Loss:1.5926, Validation Accuracy:0.2200\n",
    "Epoch #13: Loss:1.5831, Accuracy:0.2431, Validation Loss:1.5914, Validation Accuracy:0.2299\n",
    "Epoch #14: Loss:1.5806, Accuracy:0.2448, Validation Loss:1.5913, Validation Accuracy:0.2266\n",
    "Epoch #15: Loss:1.5812, Accuracy:0.2452, Validation Loss:1.5924, Validation Accuracy:0.2250\n",
    "Epoch #16: Loss:1.5788, Accuracy:0.2444, Validation Loss:1.5896, Validation Accuracy:0.2447\n",
    "Epoch #17: Loss:1.5781, Accuracy:0.2464, Validation Loss:1.5882, Validation Accuracy:0.2414\n",
    "Epoch #18: Loss:1.5779, Accuracy:0.2448, Validation Loss:1.5886, Validation Accuracy:0.2315\n",
    "Epoch #19: Loss:1.5766, Accuracy:0.2575, Validation Loss:1.5861, Validation Accuracy:0.2529\n",
    "Epoch #20: Loss:1.5769, Accuracy:0.2649, Validation Loss:1.5854, Validation Accuracy:0.2529\n",
    "Epoch #21: Loss:1.5760, Accuracy:0.2604, Validation Loss:1.5861, Validation Accuracy:0.2381\n",
    "Epoch #22: Loss:1.5761, Accuracy:0.2575, Validation Loss:1.5852, Validation Accuracy:0.2397\n",
    "Epoch #23: Loss:1.5753, Accuracy:0.2641, Validation Loss:1.5836, Validation Accuracy:0.2545\n",
    "Epoch #24: Loss:1.5749, Accuracy:0.2653, Validation Loss:1.5832, Validation Accuracy:0.2562\n",
    "Epoch #25: Loss:1.5748, Accuracy:0.2591, Validation Loss:1.5842, Validation Accuracy:0.2430\n",
    "Epoch #26: Loss:1.5742, Accuracy:0.2600, Validation Loss:1.5821, Validation Accuracy:0.2545\n",
    "Epoch #27: Loss:1.5740, Accuracy:0.2682, Validation Loss:1.5819, Validation Accuracy:0.2562\n",
    "Epoch #28: Loss:1.5730, Accuracy:0.2632, Validation Loss:1.5827, Validation Accuracy:0.2545\n",
    "Epoch #29: Loss:1.5732, Accuracy:0.2608, Validation Loss:1.5816, Validation Accuracy:0.2627\n",
    "Epoch #30: Loss:1.5730, Accuracy:0.2628, Validation Loss:1.5805, Validation Accuracy:0.2644\n",
    "Epoch #31: Loss:1.5721, Accuracy:0.2674, Validation Loss:1.5807, Validation Accuracy:0.2611\n",
    "Epoch #32: Loss:1.5720, Accuracy:0.2645, Validation Loss:1.5797, Validation Accuracy:0.2627\n",
    "Epoch #33: Loss:1.5714, Accuracy:0.2624, Validation Loss:1.5794, Validation Accuracy:0.2627\n",
    "Epoch #34: Loss:1.5708, Accuracy:0.2624, Validation Loss:1.5787, Validation Accuracy:0.2611\n",
    "Epoch #35: Loss:1.5702, Accuracy:0.2661, Validation Loss:1.5781, Validation Accuracy:0.2627\n",
    "Epoch #36: Loss:1.5698, Accuracy:0.2661, Validation Loss:1.5781, Validation Accuracy:0.2562\n",
    "Epoch #37: Loss:1.5695, Accuracy:0.2637, Validation Loss:1.5779, Validation Accuracy:0.2660\n",
    "Epoch #38: Loss:1.5686, Accuracy:0.2657, Validation Loss:1.5771, Validation Accuracy:0.2677\n",
    "Epoch #39: Loss:1.5681, Accuracy:0.2641, Validation Loss:1.5761, Validation Accuracy:0.2677\n",
    "Epoch #40: Loss:1.5674, Accuracy:0.2645, Validation Loss:1.5757, Validation Accuracy:0.2644\n",
    "Epoch #41: Loss:1.5662, Accuracy:0.2674, Validation Loss:1.5763, Validation Accuracy:0.2562\n",
    "Epoch #42: Loss:1.5657, Accuracy:0.2682, Validation Loss:1.5751, Validation Accuracy:0.2562\n",
    "Epoch #43: Loss:1.5642, Accuracy:0.2682, Validation Loss:1.5738, Validation Accuracy:0.2578\n",
    "Epoch #44: Loss:1.5630, Accuracy:0.2637, Validation Loss:1.5746, Validation Accuracy:0.2447\n",
    "Epoch #45: Loss:1.5627, Accuracy:0.2690, Validation Loss:1.5730, Validation Accuracy:0.2594\n",
    "Epoch #46: Loss:1.5603, Accuracy:0.2669, Validation Loss:1.5734, Validation Accuracy:0.2512\n",
    "Epoch #47: Loss:1.5593, Accuracy:0.2702, Validation Loss:1.5717, Validation Accuracy:0.2447\n",
    "Epoch #48: Loss:1.5585, Accuracy:0.2731, Validation Loss:1.5706, Validation Accuracy:0.2529\n",
    "Epoch #49: Loss:1.5581, Accuracy:0.2678, Validation Loss:1.5728, Validation Accuracy:0.2496\n",
    "Epoch #50: Loss:1.5552, Accuracy:0.2682, Validation Loss:1.5688, Validation Accuracy:0.2496\n",
    "Epoch #51: Loss:1.5549, Accuracy:0.2694, Validation Loss:1.5676, Validation Accuracy:0.2578\n",
    "Epoch #52: Loss:1.5522, Accuracy:0.2776, Validation Loss:1.5671, Validation Accuracy:0.2660\n",
    "Epoch #53: Loss:1.5543, Accuracy:0.2739, Validation Loss:1.5651, Validation Accuracy:0.2726\n",
    "Epoch #54: Loss:1.5576, Accuracy:0.2735, Validation Loss:1.5680, Validation Accuracy:0.2414\n",
    "Epoch #55: Loss:1.5517, Accuracy:0.2710, Validation Loss:1.5634, Validation Accuracy:0.2644\n",
    "Epoch #56: Loss:1.5515, Accuracy:0.2715, Validation Loss:1.5669, Validation Accuracy:0.2611\n",
    "Epoch #57: Loss:1.5464, Accuracy:0.2756, Validation Loss:1.5609, Validation Accuracy:0.2775\n",
    "Epoch #58: Loss:1.5474, Accuracy:0.2735, Validation Loss:1.5637, Validation Accuracy:0.2512\n",
    "Epoch #59: Loss:1.5438, Accuracy:0.2793, Validation Loss:1.5587, Validation Accuracy:0.2775\n",
    "Epoch #60: Loss:1.5437, Accuracy:0.2834, Validation Loss:1.5658, Validation Accuracy:0.2562\n",
    "Epoch #61: Loss:1.5438, Accuracy:0.2752, Validation Loss:1.5578, Validation Accuracy:0.2775\n",
    "Epoch #62: Loss:1.5404, Accuracy:0.2706, Validation Loss:1.5576, Validation Accuracy:0.2709\n",
    "Epoch #63: Loss:1.5384, Accuracy:0.2735, Validation Loss:1.5547, Validation Accuracy:0.2611\n",
    "Epoch #64: Loss:1.5349, Accuracy:0.2731, Validation Loss:1.5520, Validation Accuracy:0.2644\n",
    "Epoch #65: Loss:1.5338, Accuracy:0.2735, Validation Loss:1.5529, Validation Accuracy:0.2611\n",
    "Epoch #66: Loss:1.5342, Accuracy:0.2760, Validation Loss:1.5502, Validation Accuracy:0.2709\n",
    "Epoch #67: Loss:1.5345, Accuracy:0.2813, Validation Loss:1.5483, Validation Accuracy:0.2611\n",
    "Epoch #68: Loss:1.5283, Accuracy:0.2780, Validation Loss:1.5473, Validation Accuracy:0.2660\n",
    "Epoch #69: Loss:1.5269, Accuracy:0.2821, Validation Loss:1.5453, Validation Accuracy:0.2611\n",
    "Epoch #70: Loss:1.5256, Accuracy:0.2780, Validation Loss:1.5456, Validation Accuracy:0.2791\n",
    "Epoch #71: Loss:1.5259, Accuracy:0.2821, Validation Loss:1.5430, Validation Accuracy:0.2677\n",
    "Epoch #72: Loss:1.5228, Accuracy:0.2916, Validation Loss:1.5432, Validation Accuracy:0.2709\n",
    "Epoch #73: Loss:1.5219, Accuracy:0.2842, Validation Loss:1.5417, Validation Accuracy:0.2775\n",
    "Epoch #74: Loss:1.5243, Accuracy:0.2908, Validation Loss:1.5409, Validation Accuracy:0.2709\n",
    "Epoch #75: Loss:1.5187, Accuracy:0.2817, Validation Loss:1.5382, Validation Accuracy:0.2693\n",
    "Epoch #76: Loss:1.5199, Accuracy:0.2916, Validation Loss:1.5396, Validation Accuracy:0.2759\n",
    "Epoch #77: Loss:1.5231, Accuracy:0.2871, Validation Loss:1.5442, Validation Accuracy:0.2775\n",
    "Epoch #78: Loss:1.5227, Accuracy:0.2883, Validation Loss:1.5356, Validation Accuracy:0.2726\n",
    "Epoch #79: Loss:1.5153, Accuracy:0.2891, Validation Loss:1.5367, Validation Accuracy:0.2693\n",
    "Epoch #80: Loss:1.5195, Accuracy:0.2875, Validation Loss:1.5412, Validation Accuracy:0.2791\n",
    "Epoch #81: Loss:1.5175, Accuracy:0.2912, Validation Loss:1.5357, Validation Accuracy:0.2660\n",
    "Epoch #82: Loss:1.5137, Accuracy:0.2858, Validation Loss:1.5343, Validation Accuracy:0.2857\n",
    "Epoch #83: Loss:1.5142, Accuracy:0.2887, Validation Loss:1.5327, Validation Accuracy:0.2759\n",
    "Epoch #84: Loss:1.5204, Accuracy:0.2891, Validation Loss:1.5336, Validation Accuracy:0.2923\n",
    "Epoch #85: Loss:1.5215, Accuracy:0.2830, Validation Loss:1.5372, Validation Accuracy:0.2693\n",
    "Epoch #86: Loss:1.5162, Accuracy:0.2936, Validation Loss:1.5409, Validation Accuracy:0.2890\n",
    "Epoch #87: Loss:1.5158, Accuracy:0.2887, Validation Loss:1.5428, Validation Accuracy:0.2677\n",
    "Epoch #88: Loss:1.5183, Accuracy:0.2797, Validation Loss:1.5381, Validation Accuracy:0.2939\n",
    "Epoch #89: Loss:1.5163, Accuracy:0.2903, Validation Loss:1.5327, Validation Accuracy:0.2874\n",
    "Epoch #90: Loss:1.5095, Accuracy:0.2920, Validation Loss:1.5307, Validation Accuracy:0.2808\n",
    "Epoch #91: Loss:1.5080, Accuracy:0.2936, Validation Loss:1.5302, Validation Accuracy:0.2824\n",
    "Epoch #92: Loss:1.5078, Accuracy:0.2834, Validation Loss:1.5315, Validation Accuracy:0.2956\n",
    "Epoch #93: Loss:1.5066, Accuracy:0.2936, Validation Loss:1.5324, Validation Accuracy:0.2890\n",
    "Epoch #94: Loss:1.5051, Accuracy:0.2891, Validation Loss:1.5320, Validation Accuracy:0.2857\n",
    "Epoch #95: Loss:1.5061, Accuracy:0.2940, Validation Loss:1.5333, Validation Accuracy:0.2857\n",
    "Epoch #96: Loss:1.5046, Accuracy:0.2899, Validation Loss:1.5291, Validation Accuracy:0.2972\n",
    "Epoch #97: Loss:1.5060, Accuracy:0.2936, Validation Loss:1.5287, Validation Accuracy:0.2923\n",
    "Epoch #98: Loss:1.5036, Accuracy:0.2936, Validation Loss:1.5279, Validation Accuracy:0.2906\n",
    "Epoch #99: Loss:1.5033, Accuracy:0.2953, Validation Loss:1.5362, Validation Accuracy:0.2857\n",
    "Epoch #100: Loss:1.5039, Accuracy:0.2940, Validation Loss:1.5311, Validation Accuracy:0.2956\n",
    "Epoch #101: Loss:1.5041, Accuracy:0.2936, Validation Loss:1.5299, Validation Accuracy:0.2923\n",
    "Epoch #102: Loss:1.5029, Accuracy:0.2965, Validation Loss:1.5307, Validation Accuracy:0.2923\n",
    "Epoch #103: Loss:1.5018, Accuracy:0.2998, Validation Loss:1.5292, Validation Accuracy:0.2923\n",
    "Epoch #104: Loss:1.5019, Accuracy:0.2990, Validation Loss:1.5313, Validation Accuracy:0.2906\n",
    "Epoch #105: Loss:1.5034, Accuracy:0.2982, Validation Loss:1.5291, Validation Accuracy:0.2923\n",
    "Epoch #106: Loss:1.5016, Accuracy:0.2920, Validation Loss:1.5288, Validation Accuracy:0.2923\n",
    "Epoch #107: Loss:1.5060, Accuracy:0.2949, Validation Loss:1.5340, Validation Accuracy:0.2857\n",
    "Epoch #108: Loss:1.5050, Accuracy:0.3080, Validation Loss:1.5277, Validation Accuracy:0.2890\n",
    "Epoch #109: Loss:1.4997, Accuracy:0.2936, Validation Loss:1.5268, Validation Accuracy:0.3038\n",
    "Epoch #110: Loss:1.4977, Accuracy:0.3027, Validation Loss:1.5279, Validation Accuracy:0.2857\n",
    "Epoch #111: Loss:1.4996, Accuracy:0.3014, Validation Loss:1.5289, Validation Accuracy:0.3038\n",
    "Epoch #112: Loss:1.5000, Accuracy:0.2957, Validation Loss:1.5266, Validation Accuracy:0.2890\n",
    "Epoch #113: Loss:1.4976, Accuracy:0.3002, Validation Loss:1.5309, Validation Accuracy:0.2857\n",
    "Epoch #114: Loss:1.4992, Accuracy:0.2990, Validation Loss:1.5248, Validation Accuracy:0.2923\n",
    "Epoch #115: Loss:1.4965, Accuracy:0.2982, Validation Loss:1.5240, Validation Accuracy:0.3071\n",
    "Epoch #116: Loss:1.4970, Accuracy:0.3027, Validation Loss:1.5239, Validation Accuracy:0.3038\n",
    "Epoch #117: Loss:1.4942, Accuracy:0.2990, Validation Loss:1.5281, Validation Accuracy:0.2808\n",
    "Epoch #118: Loss:1.4985, Accuracy:0.3035, Validation Loss:1.5236, Validation Accuracy:0.2972\n",
    "Epoch #119: Loss:1.4926, Accuracy:0.3002, Validation Loss:1.5248, Validation Accuracy:0.2939\n",
    "Epoch #120: Loss:1.4926, Accuracy:0.3035, Validation Loss:1.5251, Validation Accuracy:0.3021\n",
    "Epoch #121: Loss:1.4940, Accuracy:0.3010, Validation Loss:1.5245, Validation Accuracy:0.2956\n",
    "Epoch #122: Loss:1.4950, Accuracy:0.3060, Validation Loss:1.5306, Validation Accuracy:0.2874\n",
    "Epoch #123: Loss:1.4986, Accuracy:0.3031, Validation Loss:1.5264, Validation Accuracy:0.2742\n",
    "Epoch #124: Loss:1.4942, Accuracy:0.3014, Validation Loss:1.5276, Validation Accuracy:0.3021\n",
    "Epoch #125: Loss:1.4986, Accuracy:0.3076, Validation Loss:1.5309, Validation Accuracy:0.2841\n",
    "Epoch #126: Loss:1.4926, Accuracy:0.3154, Validation Loss:1.5243, Validation Accuracy:0.2923\n",
    "Epoch #127: Loss:1.4934, Accuracy:0.3018, Validation Loss:1.5244, Validation Accuracy:0.2939\n",
    "Epoch #128: Loss:1.4908, Accuracy:0.3060, Validation Loss:1.5290, Validation Accuracy:0.2841\n",
    "Epoch #129: Loss:1.4920, Accuracy:0.3072, Validation Loss:1.5260, Validation Accuracy:0.3054\n",
    "Epoch #130: Loss:1.4986, Accuracy:0.2990, Validation Loss:1.5229, Validation Accuracy:0.2857\n",
    "Epoch #131: Loss:1.4921, Accuracy:0.3039, Validation Loss:1.5297, Validation Accuracy:0.2890\n",
    "Epoch #132: Loss:1.4933, Accuracy:0.2977, Validation Loss:1.5239, Validation Accuracy:0.3021\n",
    "Epoch #133: Loss:1.4891, Accuracy:0.3072, Validation Loss:1.5236, Validation Accuracy:0.2841\n",
    "Epoch #134: Loss:1.4881, Accuracy:0.3138, Validation Loss:1.5258, Validation Accuracy:0.2939\n",
    "Epoch #135: Loss:1.4948, Accuracy:0.3068, Validation Loss:1.5348, Validation Accuracy:0.2857\n",
    "Epoch #136: Loss:1.4966, Accuracy:0.3068, Validation Loss:1.5325, Validation Accuracy:0.2791\n",
    "Epoch #137: Loss:1.4964, Accuracy:0.2994, Validation Loss:1.5254, Validation Accuracy:0.2923\n",
    "Epoch #138: Loss:1.4895, Accuracy:0.3027, Validation Loss:1.5236, Validation Accuracy:0.2989\n",
    "Epoch #139: Loss:1.4873, Accuracy:0.3138, Validation Loss:1.5251, Validation Accuracy:0.2956\n",
    "Epoch #140: Loss:1.4868, Accuracy:0.3035, Validation Loss:1.5239, Validation Accuracy:0.3038\n",
    "Epoch #141: Loss:1.4878, Accuracy:0.3109, Validation Loss:1.5285, Validation Accuracy:0.2939\n",
    "Epoch #142: Loss:1.4897, Accuracy:0.3097, Validation Loss:1.5244, Validation Accuracy:0.3071\n",
    "Epoch #143: Loss:1.4858, Accuracy:0.3129, Validation Loss:1.5244, Validation Accuracy:0.2989\n",
    "Epoch #144: Loss:1.4864, Accuracy:0.3142, Validation Loss:1.5251, Validation Accuracy:0.3120\n",
    "Epoch #145: Loss:1.4856, Accuracy:0.3084, Validation Loss:1.5221, Validation Accuracy:0.2989\n",
    "Epoch #146: Loss:1.4844, Accuracy:0.3133, Validation Loss:1.5218, Validation Accuracy:0.2972\n",
    "Epoch #147: Loss:1.4843, Accuracy:0.3097, Validation Loss:1.5223, Validation Accuracy:0.3087\n",
    "Epoch #148: Loss:1.4852, Accuracy:0.3133, Validation Loss:1.5300, Validation Accuracy:0.3005\n",
    "Epoch #149: Loss:1.4833, Accuracy:0.3220, Validation Loss:1.5258, Validation Accuracy:0.2972\n",
    "Epoch #150: Loss:1.4880, Accuracy:0.3097, Validation Loss:1.5225, Validation Accuracy:0.3038\n",
    "Epoch #151: Loss:1.4850, Accuracy:0.3150, Validation Loss:1.5242, Validation Accuracy:0.3021\n",
    "Epoch #152: Loss:1.4854, Accuracy:0.3076, Validation Loss:1.5275, Validation Accuracy:0.2939\n",
    "Epoch #153: Loss:1.4864, Accuracy:0.3203, Validation Loss:1.5279, Validation Accuracy:0.2989\n",
    "Epoch #154: Loss:1.4889, Accuracy:0.3146, Validation Loss:1.5260, Validation Accuracy:0.2989\n",
    "Epoch #155: Loss:1.4818, Accuracy:0.3068, Validation Loss:1.5218, Validation Accuracy:0.3054\n",
    "Epoch #156: Loss:1.4813, Accuracy:0.3113, Validation Loss:1.5222, Validation Accuracy:0.3087\n",
    "Epoch #157: Loss:1.4794, Accuracy:0.3199, Validation Loss:1.5229, Validation Accuracy:0.2972\n",
    "Epoch #158: Loss:1.4785, Accuracy:0.3170, Validation Loss:1.5227, Validation Accuracy:0.3021\n",
    "Epoch #159: Loss:1.4830, Accuracy:0.3170, Validation Loss:1.5240, Validation Accuracy:0.3087\n",
    "Epoch #160: Loss:1.4803, Accuracy:0.3228, Validation Loss:1.5260, Validation Accuracy:0.3054\n",
    "Epoch #161: Loss:1.4797, Accuracy:0.3211, Validation Loss:1.5293, Validation Accuracy:0.2906\n",
    "Epoch #162: Loss:1.4811, Accuracy:0.3224, Validation Loss:1.5269, Validation Accuracy:0.3202\n",
    "Epoch #163: Loss:1.4792, Accuracy:0.3175, Validation Loss:1.5229, Validation Accuracy:0.3153\n",
    "Epoch #164: Loss:1.4784, Accuracy:0.3142, Validation Loss:1.5223, Validation Accuracy:0.3120\n",
    "Epoch #165: Loss:1.4778, Accuracy:0.3236, Validation Loss:1.5272, Validation Accuracy:0.3054\n",
    "Epoch #166: Loss:1.4845, Accuracy:0.3113, Validation Loss:1.5306, Validation Accuracy:0.2874\n",
    "Epoch #167: Loss:1.4768, Accuracy:0.3142, Validation Loss:1.5263, Validation Accuracy:0.3136\n",
    "Epoch #168: Loss:1.4777, Accuracy:0.3158, Validation Loss:1.5236, Validation Accuracy:0.3071\n",
    "Epoch #169: Loss:1.4747, Accuracy:0.3257, Validation Loss:1.5280, Validation Accuracy:0.2890\n",
    "Epoch #170: Loss:1.4848, Accuracy:0.3175, Validation Loss:1.5225, Validation Accuracy:0.3005\n",
    "Epoch #171: Loss:1.4837, Accuracy:0.3129, Validation Loss:1.5322, Validation Accuracy:0.3005\n",
    "Epoch #172: Loss:1.4848, Accuracy:0.3240, Validation Loss:1.5420, Validation Accuracy:0.2775\n",
    "Epoch #173: Loss:1.4918, Accuracy:0.3088, Validation Loss:1.5298, Validation Accuracy:0.3120\n",
    "Epoch #174: Loss:1.4929, Accuracy:0.3236, Validation Loss:1.5227, Validation Accuracy:0.2906\n",
    "Epoch #175: Loss:1.4780, Accuracy:0.3138, Validation Loss:1.5243, Validation Accuracy:0.2923\n",
    "Epoch #176: Loss:1.4738, Accuracy:0.3150, Validation Loss:1.5238, Validation Accuracy:0.3103\n",
    "Epoch #177: Loss:1.4736, Accuracy:0.3216, Validation Loss:1.5230, Validation Accuracy:0.3021\n",
    "Epoch #178: Loss:1.4724, Accuracy:0.3203, Validation Loss:1.5217, Validation Accuracy:0.3038\n",
    "Epoch #179: Loss:1.4710, Accuracy:0.3224, Validation Loss:1.5220, Validation Accuracy:0.3071\n",
    "Epoch #180: Loss:1.4701, Accuracy:0.3331, Validation Loss:1.5232, Validation Accuracy:0.3021\n",
    "Epoch #181: Loss:1.4718, Accuracy:0.3220, Validation Loss:1.5233, Validation Accuracy:0.3071\n",
    "Epoch #182: Loss:1.4707, Accuracy:0.3261, Validation Loss:1.5238, Validation Accuracy:0.2972\n",
    "Epoch #183: Loss:1.4716, Accuracy:0.3220, Validation Loss:1.5241, Validation Accuracy:0.3087\n",
    "Epoch #184: Loss:1.4709, Accuracy:0.3302, Validation Loss:1.5240, Validation Accuracy:0.2956\n",
    "Epoch #185: Loss:1.4715, Accuracy:0.3253, Validation Loss:1.5262, Validation Accuracy:0.3120\n",
    "Epoch #186: Loss:1.4693, Accuracy:0.3310, Validation Loss:1.5246, Validation Accuracy:0.2890\n",
    "Epoch #187: Loss:1.4692, Accuracy:0.3285, Validation Loss:1.5224, Validation Accuracy:0.3103\n",
    "Epoch #188: Loss:1.4668, Accuracy:0.3302, Validation Loss:1.5239, Validation Accuracy:0.2956\n",
    "Epoch #189: Loss:1.4666, Accuracy:0.3363, Validation Loss:1.5241, Validation Accuracy:0.3021\n",
    "Epoch #190: Loss:1.4677, Accuracy:0.3248, Validation Loss:1.5245, Validation Accuracy:0.3120\n",
    "Epoch #191: Loss:1.4670, Accuracy:0.3351, Validation Loss:1.5266, Validation Accuracy:0.3005\n",
    "Epoch #192: Loss:1.4654, Accuracy:0.3290, Validation Loss:1.5232, Validation Accuracy:0.3021\n",
    "Epoch #193: Loss:1.4673, Accuracy:0.3265, Validation Loss:1.5287, Validation Accuracy:0.3005\n",
    "Epoch #194: Loss:1.4644, Accuracy:0.3294, Validation Loss:1.5262, Validation Accuracy:0.2874\n",
    "Epoch #195: Loss:1.4665, Accuracy:0.3281, Validation Loss:1.5291, Validation Accuracy:0.3087\n",
    "Epoch #196: Loss:1.4655, Accuracy:0.3195, Validation Loss:1.5383, Validation Accuracy:0.2956\n",
    "Epoch #197: Loss:1.4761, Accuracy:0.3211, Validation Loss:1.5349, Validation Accuracy:0.2775\n",
    "Epoch #198: Loss:1.4714, Accuracy:0.3248, Validation Loss:1.5234, Validation Accuracy:0.2989\n",
    "Epoch #199: Loss:1.4628, Accuracy:0.3326, Validation Loss:1.5263, Validation Accuracy:0.3120\n",
    "Epoch #200: Loss:1.4666, Accuracy:0.3248, Validation Loss:1.5264, Validation Accuracy:0.2923\n",
    "Epoch #201: Loss:1.4605, Accuracy:0.3339, Validation Loss:1.5259, Validation Accuracy:0.3038\n",
    "Epoch #202: Loss:1.4663, Accuracy:0.3294, Validation Loss:1.5255, Validation Accuracy:0.2989\n",
    "Epoch #203: Loss:1.4613, Accuracy:0.3326, Validation Loss:1.5281, Validation Accuracy:0.3038\n",
    "Epoch #204: Loss:1.4614, Accuracy:0.3355, Validation Loss:1.5233, Validation Accuracy:0.2874\n",
    "Epoch #205: Loss:1.4595, Accuracy:0.3351, Validation Loss:1.5257, Validation Accuracy:0.2989\n",
    "Epoch #206: Loss:1.4598, Accuracy:0.3388, Validation Loss:1.5283, Validation Accuracy:0.2906\n",
    "Epoch #207: Loss:1.4606, Accuracy:0.3331, Validation Loss:1.5327, Validation Accuracy:0.3021\n",
    "Epoch #208: Loss:1.4653, Accuracy:0.3281, Validation Loss:1.5311, Validation Accuracy:0.3054\n",
    "Epoch #209: Loss:1.4628, Accuracy:0.3211, Validation Loss:1.5277, Validation Accuracy:0.3005\n",
    "Epoch #210: Loss:1.4576, Accuracy:0.3343, Validation Loss:1.5240, Validation Accuracy:0.2972\n",
    "Epoch #211: Loss:1.4578, Accuracy:0.3248, Validation Loss:1.5303, Validation Accuracy:0.2923\n",
    "Epoch #212: Loss:1.4617, Accuracy:0.3376, Validation Loss:1.5282, Validation Accuracy:0.3071\n",
    "Epoch #213: Loss:1.4611, Accuracy:0.3368, Validation Loss:1.5276, Validation Accuracy:0.3103\n",
    "Epoch #214: Loss:1.4593, Accuracy:0.3331, Validation Loss:1.5336, Validation Accuracy:0.2906\n",
    "Epoch #215: Loss:1.4579, Accuracy:0.3368, Validation Loss:1.5304, Validation Accuracy:0.2906\n",
    "Epoch #216: Loss:1.4604, Accuracy:0.3257, Validation Loss:1.5299, Validation Accuracy:0.2972\n",
    "Epoch #217: Loss:1.4568, Accuracy:0.3318, Validation Loss:1.5367, Validation Accuracy:0.3153\n",
    "Epoch #218: Loss:1.4612, Accuracy:0.3392, Validation Loss:1.5342, Validation Accuracy:0.2939\n",
    "Epoch #219: Loss:1.4598, Accuracy:0.3310, Validation Loss:1.5283, Validation Accuracy:0.2972\n",
    "Epoch #220: Loss:1.4574, Accuracy:0.3392, Validation Loss:1.5274, Validation Accuracy:0.2972\n",
    "Epoch #221: Loss:1.4565, Accuracy:0.3405, Validation Loss:1.5388, Validation Accuracy:0.3169\n",
    "Epoch #222: Loss:1.4596, Accuracy:0.3384, Validation Loss:1.5301, Validation Accuracy:0.2956\n",
    "Epoch #223: Loss:1.4562, Accuracy:0.3359, Validation Loss:1.5340, Validation Accuracy:0.2923\n",
    "Epoch #224: Loss:1.4531, Accuracy:0.3384, Validation Loss:1.5278, Validation Accuracy:0.2972\n",
    "Epoch #225: Loss:1.4504, Accuracy:0.3433, Validation Loss:1.5252, Validation Accuracy:0.3071\n",
    "Epoch #226: Loss:1.4482, Accuracy:0.3462, Validation Loss:1.5273, Validation Accuracy:0.2939\n",
    "Epoch #227: Loss:1.4509, Accuracy:0.3462, Validation Loss:1.5350, Validation Accuracy:0.2923\n",
    "Epoch #228: Loss:1.4601, Accuracy:0.3450, Validation Loss:1.5323, Validation Accuracy:0.3054\n",
    "Epoch #229: Loss:1.4577, Accuracy:0.3405, Validation Loss:1.5351, Validation Accuracy:0.3038\n",
    "Epoch #230: Loss:1.4484, Accuracy:0.3433, Validation Loss:1.5277, Validation Accuracy:0.2956\n",
    "Epoch #231: Loss:1.4509, Accuracy:0.3433, Validation Loss:1.5355, Validation Accuracy:0.3169\n",
    "Epoch #232: Loss:1.4551, Accuracy:0.3462, Validation Loss:1.5342, Validation Accuracy:0.2956\n",
    "Epoch #233: Loss:1.4554, Accuracy:0.3351, Validation Loss:1.5349, Validation Accuracy:0.2956\n",
    "Epoch #234: Loss:1.4524, Accuracy:0.3441, Validation Loss:1.5322, Validation Accuracy:0.2989\n",
    "Epoch #235: Loss:1.4540, Accuracy:0.3384, Validation Loss:1.5326, Validation Accuracy:0.3005\n",
    "Epoch #236: Loss:1.4549, Accuracy:0.3446, Validation Loss:1.5304, Validation Accuracy:0.3169\n",
    "Epoch #237: Loss:1.4510, Accuracy:0.3441, Validation Loss:1.5332, Validation Accuracy:0.2874\n",
    "Epoch #238: Loss:1.4495, Accuracy:0.3466, Validation Loss:1.5317, Validation Accuracy:0.2939\n",
    "Epoch #239: Loss:1.4500, Accuracy:0.3474, Validation Loss:1.5292, Validation Accuracy:0.2906\n",
    "Epoch #240: Loss:1.4441, Accuracy:0.3520, Validation Loss:1.5305, Validation Accuracy:0.2906\n",
    "Epoch #241: Loss:1.4449, Accuracy:0.3474, Validation Loss:1.5345, Validation Accuracy:0.2890\n",
    "Epoch #242: Loss:1.4444, Accuracy:0.3507, Validation Loss:1.5271, Validation Accuracy:0.2923\n",
    "Epoch #243: Loss:1.4423, Accuracy:0.3474, Validation Loss:1.5356, Validation Accuracy:0.2989\n",
    "Epoch #244: Loss:1.4445, Accuracy:0.3495, Validation Loss:1.5315, Validation Accuracy:0.2923\n",
    "Epoch #245: Loss:1.4505, Accuracy:0.3437, Validation Loss:1.5668, Validation Accuracy:0.2857\n",
    "Epoch #246: Loss:1.4609, Accuracy:0.3392, Validation Loss:1.5340, Validation Accuracy:0.3005\n",
    "Epoch #247: Loss:1.4492, Accuracy:0.3466, Validation Loss:1.5282, Validation Accuracy:0.3005\n",
    "Epoch #248: Loss:1.4531, Accuracy:0.3409, Validation Loss:1.5361, Validation Accuracy:0.2972\n",
    "Epoch #249: Loss:1.4573, Accuracy:0.3277, Validation Loss:1.5304, Validation Accuracy:0.3071\n",
    "Epoch #250: Loss:1.4561, Accuracy:0.3384, Validation Loss:1.5396, Validation Accuracy:0.2956\n",
    "Epoch #251: Loss:1.4475, Accuracy:0.3376, Validation Loss:1.5333, Validation Accuracy:0.2726\n",
    "Epoch #252: Loss:1.4574, Accuracy:0.3483, Validation Loss:1.5349, Validation Accuracy:0.3038\n",
    "Epoch #253: Loss:1.4479, Accuracy:0.3491, Validation Loss:1.5314, Validation Accuracy:0.3103\n",
    "Epoch #254: Loss:1.4425, Accuracy:0.3483, Validation Loss:1.5329, Validation Accuracy:0.2906\n",
    "Epoch #255: Loss:1.4414, Accuracy:0.3536, Validation Loss:1.5296, Validation Accuracy:0.2956\n",
    "Epoch #256: Loss:1.4432, Accuracy:0.3520, Validation Loss:1.5347, Validation Accuracy:0.2956\n",
    "Epoch #257: Loss:1.4415, Accuracy:0.3528, Validation Loss:1.5304, Validation Accuracy:0.3071\n",
    "Epoch #258: Loss:1.4399, Accuracy:0.3589, Validation Loss:1.5286, Validation Accuracy:0.2890\n",
    "Epoch #259: Loss:1.4341, Accuracy:0.3589, Validation Loss:1.5318, Validation Accuracy:0.2972\n",
    "Epoch #260: Loss:1.4344, Accuracy:0.3626, Validation Loss:1.5304, Validation Accuracy:0.2791\n",
    "Epoch #261: Loss:1.4332, Accuracy:0.3585, Validation Loss:1.5298, Validation Accuracy:0.2874\n",
    "Epoch #262: Loss:1.4305, Accuracy:0.3581, Validation Loss:1.5403, Validation Accuracy:0.2841\n",
    "Epoch #263: Loss:1.4344, Accuracy:0.3524, Validation Loss:1.5291, Validation Accuracy:0.2841\n",
    "Epoch #264: Loss:1.4339, Accuracy:0.3561, Validation Loss:1.5346, Validation Accuracy:0.3038\n",
    "Epoch #265: Loss:1.4358, Accuracy:0.3544, Validation Loss:1.5329, Validation Accuracy:0.2923\n",
    "Epoch #266: Loss:1.4372, Accuracy:0.3581, Validation Loss:1.5465, Validation Accuracy:0.2939\n",
    "Epoch #267: Loss:1.4347, Accuracy:0.3528, Validation Loss:1.5312, Validation Accuracy:0.3038\n",
    "Epoch #268: Loss:1.4322, Accuracy:0.3598, Validation Loss:1.5310, Validation Accuracy:0.2972\n",
    "Epoch #269: Loss:1.4289, Accuracy:0.3598, Validation Loss:1.5300, Validation Accuracy:0.2857\n",
    "Epoch #270: Loss:1.4277, Accuracy:0.3647, Validation Loss:1.5345, Validation Accuracy:0.2956\n",
    "Epoch #271: Loss:1.4328, Accuracy:0.3622, Validation Loss:1.5286, Validation Accuracy:0.2890\n",
    "Epoch #272: Loss:1.4337, Accuracy:0.3556, Validation Loss:1.5495, Validation Accuracy:0.2906\n",
    "Epoch #273: Loss:1.4356, Accuracy:0.3524, Validation Loss:1.5419, Validation Accuracy:0.2857\n",
    "Epoch #274: Loss:1.4338, Accuracy:0.3589, Validation Loss:1.5302, Validation Accuracy:0.2726\n",
    "Epoch #275: Loss:1.4283, Accuracy:0.3536, Validation Loss:1.5318, Validation Accuracy:0.2956\n",
    "Epoch #276: Loss:1.4294, Accuracy:0.3573, Validation Loss:1.5337, Validation Accuracy:0.2989\n",
    "Epoch #277: Loss:1.4317, Accuracy:0.3643, Validation Loss:1.5339, Validation Accuracy:0.2791\n",
    "Epoch #278: Loss:1.4263, Accuracy:0.3626, Validation Loss:1.5377, Validation Accuracy:0.2857\n",
    "Epoch #279: Loss:1.4282, Accuracy:0.3548, Validation Loss:1.5338, Validation Accuracy:0.2989\n",
    "Epoch #280: Loss:1.4295, Accuracy:0.3548, Validation Loss:1.5378, Validation Accuracy:0.3120\n",
    "Epoch #281: Loss:1.4357, Accuracy:0.3540, Validation Loss:1.5543, Validation Accuracy:0.2956\n",
    "Epoch #282: Loss:1.4316, Accuracy:0.3684, Validation Loss:1.5322, Validation Accuracy:0.2857\n",
    "Epoch #283: Loss:1.4338, Accuracy:0.3688, Validation Loss:1.5531, Validation Accuracy:0.3120\n",
    "Epoch #284: Loss:1.4433, Accuracy:0.3433, Validation Loss:1.5631, Validation Accuracy:0.2890\n",
    "Epoch #285: Loss:1.4355, Accuracy:0.3556, Validation Loss:1.5444, Validation Accuracy:0.2808\n",
    "Epoch #286: Loss:1.4315, Accuracy:0.3524, Validation Loss:1.5349, Validation Accuracy:0.3054\n",
    "Epoch #287: Loss:1.4293, Accuracy:0.3552, Validation Loss:1.5381, Validation Accuracy:0.2808\n",
    "Epoch #288: Loss:1.4225, Accuracy:0.3585, Validation Loss:1.5287, Validation Accuracy:0.2841\n",
    "Epoch #289: Loss:1.4216, Accuracy:0.3684, Validation Loss:1.5310, Validation Accuracy:0.2972\n",
    "Epoch #290: Loss:1.4193, Accuracy:0.3614, Validation Loss:1.5392, Validation Accuracy:0.2791\n",
    "Epoch #291: Loss:1.4196, Accuracy:0.3688, Validation Loss:1.5317, Validation Accuracy:0.2923\n",
    "Epoch #292: Loss:1.4170, Accuracy:0.3659, Validation Loss:1.5334, Validation Accuracy:0.2890\n",
    "Epoch #293: Loss:1.4151, Accuracy:0.3684, Validation Loss:1.5344, Validation Accuracy:0.2939\n",
    "Epoch #294: Loss:1.4159, Accuracy:0.3696, Validation Loss:1.5357, Validation Accuracy:0.2972\n",
    "Epoch #295: Loss:1.4200, Accuracy:0.3671, Validation Loss:1.5371, Validation Accuracy:0.2906\n",
    "Epoch #296: Loss:1.4260, Accuracy:0.3626, Validation Loss:1.5412, Validation Accuracy:0.2874\n",
    "Epoch #297: Loss:1.4243, Accuracy:0.3663, Validation Loss:1.5506, Validation Accuracy:0.2923\n",
    "Epoch #298: Loss:1.4202, Accuracy:0.3749, Validation Loss:1.5391, Validation Accuracy:0.2874\n",
    "Epoch #299: Loss:1.4156, Accuracy:0.3680, Validation Loss:1.5348, Validation Accuracy:0.2939\n",
    "Epoch #300: Loss:1.4137, Accuracy:0.3655, Validation Loss:1.5374, Validation Accuracy:0.3038\n",
    "\n",
    "Test:\n",
    "Test Loss:1.53735244, Accuracy:0.3038\n",
    "Labels: ['04', '03', '02', '01', '05']\n",
    "Confusion Matrix:\n",
    "      04  03  02  01  05\n",
    "t:04  50  22  13  18   9\n",
    "t:03  34  29  11  23  18\n",
    "t:02  23  29  27  22  13\n",
    "t:01  23  28  23  28  24\n",
    "t:05  36  24  13  18  51\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.30      0.45      0.36       112\n",
    "          03       0.22      0.25      0.23       115\n",
    "          02       0.31      0.24      0.27       114\n",
    "          01       0.26      0.22      0.24       126\n",
    "          05       0.44      0.36      0.40       142\n",
    "\n",
    "    accuracy                           0.30       609\n",
    "   macro avg       0.31      0.30      0.30       609\n",
    "weighted avg       0.31      0.30      0.30       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.27 20:19:34 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 44 minutes, 2 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.6081638250053418, 1.6059811183775978, 1.6051952190978578, 1.6047896027369257, 1.6043476232362694, 1.6035949211010987, 1.6024606077150367, 1.6009877252657034, 1.599153215466266, 1.5967832429851414, 1.5945574387736705, 1.5926230353087627, 1.5914219273330739, 1.5913236728442715, 1.5923681214133703, 1.58955329920858, 1.5881876512897035, 1.588601783970111, 1.5860816757080003, 1.5854297520100384, 1.5860668587175693, 1.5852298857934761, 1.5836325999355472, 1.583196807377444, 1.584203355809542, 1.582126820028709, 1.5819277984559634, 1.5827088694658578, 1.581552881325407, 1.5805409087727613, 1.580743490377279, 1.5796804516186267, 1.5793933375128384, 1.5787345991150303, 1.578072535776348, 1.5780685071287484, 1.5779369145582853, 1.5771120651602157, 1.5761116946663567, 1.5756925078252657, 1.5762752089007148, 1.5751396466554288, 1.573813231120556, 1.5745701308320896, 1.5729664973241746, 1.5734114139929585, 1.5717154582733004, 1.5706227285717116, 1.5727512180707333, 1.568836280669289, 1.5675556426760795, 1.567105101247139, 1.5651068779439565, 1.5679556278172384, 1.5633529127133499, 1.566916614330461, 1.560885303713418, 1.5637491324852253, 1.5586535492162594, 1.5658484417425196, 1.5577745797794635, 1.5576027524099365, 1.554721532784072, 1.5519724677153213, 1.552857732537932, 1.5501551385387802, 1.5482819738059208, 1.547308189724075, 1.5453172419067283, 1.5456293198862687, 1.543012193662584, 1.5432018591656864, 1.5417445866736676, 1.5409058119080141, 1.5381929422246998, 1.5396046203932738, 1.5441949598503426, 1.5355922735383358, 1.5366607861370092, 1.5412025359659556, 1.5357250055460312, 1.5343031423236742, 1.532694228372746, 1.5335893564427818, 1.5371628905752022, 1.540935339794566, 1.5427861145172996, 1.5381286625791653, 1.5326681745854895, 1.5306968739858793, 1.5302296391457368, 1.531494389026623, 1.5323625701205876, 1.5319625313450353, 1.5332851909064307, 1.5290764611342857, 1.5286996519232814, 1.5278676632785642, 1.536177367020906, 1.5310676953279718, 1.5298971851862515, 1.5306569594272057, 1.529170415867334, 1.531280848976035, 1.529062436327754, 1.5287735158782483, 1.5340450698714734, 1.5276709438740521, 1.5268216857377728, 1.5278868718296046, 1.528869847945979, 1.5266369518583827, 1.5309224334256402, 1.5247754095418895, 1.5240123176026619, 1.5238637980960665, 1.5281279106641246, 1.5236266281804427, 1.5248464659125542, 1.5250953980071595, 1.5244698825923877, 1.5306253977401307, 1.526388801768887, 1.5276139011523995, 1.530921688612263, 1.5242539548325813, 1.5243846919931998, 1.5289663148826762, 1.5260252600232955, 1.5229267674713887, 1.529715281010457, 1.5239402224081882, 1.5235656006582852, 1.5257748289061297, 1.5347631314313666, 1.5324715796754083, 1.525429884788438, 1.5235764373503686, 1.5251309468436907, 1.5238573463092298, 1.528525851630225, 1.5244318458247068, 1.524360359987406, 1.5250857821826278, 1.5221118562914469, 1.521791560113528, 1.5223436334058764, 1.5299578881616076, 1.5258453393413125, 1.5224601447288626, 1.5241616001270089, 1.5274533469884461, 1.5279089531483516, 1.5260279585770982, 1.521809946140045, 1.522195909215116, 1.5229135346530107, 1.5226554099366387, 1.5240185168772105, 1.5260041672216456, 1.5293226809728713, 1.5269495319263102, 1.5228527469196538, 1.522294209116981, 1.5272194922263986, 1.5305643330262408, 1.5263124134740218, 1.5235891894166693, 1.5279817847391264, 1.5225127207234574, 1.5322170709741527, 1.5419561761157659, 1.529827522526821, 1.5227421261798377, 1.5242536285240662, 1.5238100756173847, 1.5230400458541018, 1.5216781801386616, 1.5219606380353028, 1.5232113981481843, 1.5233138968008884, 1.5237687828114075, 1.5240754110276797, 1.524008441245419, 1.5262168534283567, 1.5245841316793156, 1.5224133577252843, 1.5239116822557497, 1.5241300126014672, 1.5244640517117354, 1.526613590361058, 1.5231754149513683, 1.5287171110926787, 1.5262373174939836, 1.529078282941934, 1.5383275271636512, 1.5349231525790712, 1.5233715708032618, 1.5262924753777891, 1.5264067432563293, 1.525942204425292, 1.5255172606955216, 1.528070340602856, 1.5233373246560935, 1.5257396887871628, 1.5282860422760787, 1.5326655768408564, 1.5310649219992125, 1.527704105588603, 1.5239902217987136, 1.5302600324251774, 1.5281682668256837, 1.527644439088104, 1.5336129289542513, 1.530420768828619, 1.5299377513832255, 1.536742496764523, 1.5341917951706008, 1.528329138293838, 1.52738254117261, 1.5387722271416575, 1.5301284645187052, 1.5339502013962845, 1.5277825437351595, 1.5251520414070543, 1.5273287887447964, 1.5350181600338915, 1.5323070921921378, 1.535057227404051, 1.5277372684776294, 1.5354753515403259, 1.5341693131598737, 1.5349031553675585, 1.5321679825853245, 1.5325970050736601, 1.53041878474757, 1.5332064004171462, 1.531679844817113, 1.5291604443723932, 1.5305010168423205, 1.5345185063351159, 1.527127167078466, 1.5356231088121537, 1.5315081378313513, 1.5667664742430638, 1.5340253404404338, 1.5282446484651864, 1.5361468245830443, 1.530378666613098, 1.539632685078776, 1.5332542168487273, 1.5349078423088212, 1.5313758568223474, 1.5329067245101302, 1.5296201707890076, 1.534719530193285, 1.530353264268396, 1.5286350054498181, 1.5317888498697767, 1.5303685698407428, 1.529814718392095, 1.540263354093179, 1.5290702305403836, 1.5345615874761822, 1.5329463879267375, 1.5465338676433844, 1.531172528838485, 1.5310256236488204, 1.5300120303196272, 1.534465222914622, 1.5286261980561004, 1.5495175459897772, 1.5418908451186808, 1.5301756655249885, 1.5318419571189066, 1.533691723162709, 1.5338707753198684, 1.5377445363841817, 1.5337954272190337, 1.5378075898770236, 1.5543106823719193, 1.5322025072789935, 1.5530784607716577, 1.5631091271715212, 1.544383704955942, 1.5349145219439553, 1.538062011471327, 1.5286809620990347, 1.531024293946515, 1.5392285235018175, 1.5316794748572489, 1.5334328246625577, 1.5343869644628565, 1.5356663068135579, 1.5370813950724986, 1.541177852008926, 1.550566057657765, 1.5390694689476627, 1.5348430746686086, 1.537352530827076], 'val_acc': [0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23316912950063964, 0.23809523807076985, 0.2380952378750239, 0.21674876844843816, 0.22003284069802764, 0.22988505744679613, 0.22660098490358768, 0.2249589467968651, 0.2446633822763299, 0.24137930983099445, 0.23152709298435298, 0.2528735625088117, 0.2528735625088117, 0.238095237385659, 0.23973727351045374, 0.25451559863360645, 0.2561576347584012, 0.2430213458579162, 0.25451559863360645, 0.2561576347584012, 0.25451559863360645, 0.26272577935545316, 0.2643678154802479, 0.2610837432306584, 0.26272577935545316, 0.26272577935545316, 0.26108374313278543, 0.26272577935545316, 0.2561576347584012, 0.26600985150716966, 0.2676518877298374, 0.2676518876319644, 0.2643678154802479, 0.2561576349541472, 0.2561576348562742, 0.25779967078532295, 0.24466338188483797, 0.25944170730160965, 0.25123152677550886, 0.2446633822763299, 0.2528735627045577, 0.24958949055284116, 0.24958949055284116, 0.2577996714704338, 0.2660098521922805, 0.2725779965935865, 0.2413793100267404, 0.2643678158717398, 0.2610837437200233, 0.2775041050658437, 0.25123152677550886, 0.2775041050658437, 0.25615763544351206, 0.2775041050658437, 0.27093596046879176, 0.2610837438178963, 0.2643678160674858, 0.2610837437200233, 0.2709359603709188, 0.2610837437200233, 0.26600985209440753, 0.2610837437200233, 0.27914614089701956, 0.26765188831707526, 0.27093596027304584, 0.27750410288816996, 0.2709359603709188, 0.26929392245994216, 0.2758620685495571, 0.2775041049679708, 0.27257799461165866, 0.26929392236206917, 0.27914614119063846, 0.2660098521922805, 0.28571428370788965, 0.2758620666655022, 0.29228242791344966, 0.26929392255781515, 0.288998357645788, 0.2676518863351474, 0.2939244663137912, 0.28735631953906543, 0.28078817523563243, 0.2824302112625542, 0.29556650026091214, 0.2889983560553521, 0.2857142835121437, 0.28571428361001666, 0.29720853658145285, 0.29228242840281454, 0.2906403920822739, 0.2857142856898175, 0.2955665005545311, 0.2922824283049416, 0.2922824282070686, 0.2922824283049416, 0.2906403922780198, 0.29228242840281454, 0.2922824283049416, 0.28571428370788965, 0.28899835615322506, 0.3037766811785048, 0.2857142838057626, 0.3037766810806318, 0.28899835615322506, 0.2857142838057626, 0.29228242840281454, 0.3070607535259673, 0.3037766811785048, 0.28078817543137835, 0.29720853677719883, 0.29392446472335526, 0.30213464505371007, 0.2955665003587851, 0.2873563200284303, 0.27422003112794535, 0.30213464524945605, 0.28407224738734893, 0.29228242840281454, 0.2939244645276093, 0.2840722474852219, 0.30541871749904553, 0.28571428400150856, 0.28899835615322506, 0.30213464515158306, 0.2840722474852219, 0.2939244646254823, 0.28571428370788965, 0.2791461392087106, 0.2922824287943065, 0.29885057290199357, 0.295566500750277, 0.3037766814721237, 0.2939244646254823, 0.3070607535259673, 0.2988505729998665, 0.3119868619003515, 0.2988505729998665, 0.29720853697294475, 0.30870278955288905, 0.3004926090267883, 0.2972085372665637, 0.3037766817657427, 0.30213464544520197, 0.2939244645276093, 0.2988505728041206, 0.29885057329348547, 0.30541871740117255, 0.30870278965076203, 0.29720853677719883, 0.302134645347329, 0.308702789748635, 0.30541871720542657, 0.2906403924737658, 0.32019704252432524, 0.315270934345687, 0.3119868619003515, 0.30541871740117255, 0.2873563200284303, 0.3136288978294003, 0.3070607533302213, 0.288998356446844, 0.30049260892891533, 0.30049260892891533, 0.27750410318178886, 0.3119868620960975, 0.2906403920822739, 0.2922824288921795, 0.31034482597130275, 0.3021346447600911, 0.3037766812763778, 0.3070607535259673, 0.30213464544520197, 0.3070607536238403, 0.29720853648357987, 0.30870278994438094, 0.29556650026091214, 0.3119868624875894, 0.28899835595747914, 0.31034482587342976, 0.2955665006524041, 0.30213464515158306, 0.3119868620960975, 0.30049260912466125, 0.30213464515158306, 0.30049260922253423, 0.2873563197348114, 0.308702789846508, 0.2955665003587851, 0.27750410337753484, 0.2988505726083746, 0.3119868620960975, 0.29228242850068753, 0.3037766813742508, 0.2988505729998665, 0.3037766813742508, 0.2873563199305573, 0.29885057290199357, 0.2906403920822739, 0.30213464544520197, 0.3054187171075536, 0.30049260912466125, 0.29720853667932584, 0.29228242840281454, 0.3070607539174592, 0.3103448254819378, 0.2906403923758928, 0.2906403920822739, 0.29720853677719883, 0.315270934149941, 0.29392446472335526, 0.29720853687507176, 0.29720853677719883, 0.3169129704704817, 0.2955665006524041, 0.2922824285985605, 0.29720853667932584, 0.3070607534280943, 0.2939244646254823, 0.29228242850068753, 0.30541871720542657, 0.3037766815699967, 0.2955665004566581, 0.31691297037260874, 0.29556650084815, 0.2955665004566581, 0.2988505729998665, 0.3004926090267883, 0.31691297037260874, 0.2873563201263033, 0.2939244642339904, 0.29064039218014687, 0.29064039218014687, 0.28899835585960615, 0.2922824282070686, 0.2988505726083746, 0.29228242810919564, 0.28571428419725453, 0.3004926090267883, 0.3004926090267883, 0.2972085371686907, 0.3070607532323483, 0.2955665006524041, 0.27257799649571357, 0.3037766812763778, 0.31034482587342976, 0.2906403924737658, 0.29556650016303915, 0.29556650084815, 0.3070607535259673, 0.2889983560553521, 0.2972085362878339, 0.2791461390129647, 0.2873563197348114, 0.2840722474852219, 0.28407224728947594, 0.3037766812763778, 0.29228242810919564, 0.29392446491910124, 0.3037766812763778, 0.29720853687507176, 0.28571428361001666, 0.29556650084815, 0.28899835595747914, 0.2906403922780198, 0.2857142856898175, 0.2725779966914595, 0.295566500750277, 0.2988505727062476, 0.2791461389150917, 0.28571428361001666, 0.29885057290199357, 0.3119868621939704, 0.295566500750277, 0.28571428390363557, 0.31198686170460554, 0.28899835615322506, 0.28078817552925134, 0.30541871740117255, 0.28078817533350536, 0.2840722475830949, 0.29720853667932584, 0.2791461391108377, 0.29228242840281454, 0.28899835615322506, 0.2939244645276093, 0.29720853677719883, 0.29064039218014687, 0.2873563197348114, 0.29228242801132265, 0.2873563197348114, 0.2939244646254823, 0.3037766814721237], 'loss': [1.6112786504032675, 1.6070124931159206, 1.6055251718301793, 1.604947554551111, 1.6044631925206898, 1.60347836830288, 1.602255816038629, 1.5998373234786047, 1.5971409271385146, 1.5939800203703267, 1.589758072545641, 1.585952920443713, 1.5831242377263564, 1.580579122625582, 1.5812194522156608, 1.5788086001143564, 1.5781436398778363, 1.577889305806013, 1.5766026013685692, 1.5768618196922162, 1.576018918515231, 1.576112513718419, 1.5752884518684058, 1.574936238255589, 1.5748475308780552, 1.5742187820665645, 1.5740358361962885, 1.5729871212334603, 1.57321689985616, 1.572964264675822, 1.5720842812340363, 1.5720364898626809, 1.5714031651769085, 1.5708214825428486, 1.5702207865411495, 1.5698089522747534, 1.5694882439881623, 1.568617919289356, 1.5681418365521598, 1.567354787105897, 1.5661539200150256, 1.5656544233494472, 1.5642016975297086, 1.5629719691110098, 1.5627140284074161, 1.5603285945171692, 1.5592747781800538, 1.5584621205711757, 1.5580692402880785, 1.5551517771499603, 1.5549038614825301, 1.5522291063528042, 1.5543140623848541, 1.5576409911228157, 1.5516904386406807, 1.5514691275003265, 1.5464227119755207, 1.5473744813421668, 1.543841860279655, 1.5436573735008006, 1.5438178600471857, 1.5404122826989426, 1.5384076829318882, 1.5349308114766584, 1.533829839469471, 1.5342384997824134, 1.5344893011958691, 1.5282587498854807, 1.5268624203405832, 1.525622767638377, 1.5258892595890365, 1.5227695764702203, 1.5219112727186763, 1.5243004199170969, 1.5187181201069262, 1.5199152414803632, 1.5231172591509026, 1.5226980387062996, 1.5153098433414278, 1.5194803830289743, 1.5175225548186098, 1.513715235604398, 1.5141950746336512, 1.52043163600154, 1.5215082309574073, 1.5162121735559106, 1.5158280169449792, 1.5182687568468725, 1.5163363974686765, 1.509480582811015, 1.5079698785374542, 1.5077778145028335, 1.5065825708103375, 1.505141639905299, 1.5060521028369849, 1.5046387538283268, 1.5060489485151225, 1.5036422762782666, 1.5032612020719713, 1.5039345827925132, 1.504144844724902, 1.5029209078704553, 1.5017542056724031, 1.5019278514067005, 1.5033821448653142, 1.5015543565123477, 1.5060207893716235, 1.504959946050781, 1.499665465834694, 1.4977245857093857, 1.499628120428238, 1.4999573084363214, 1.4975744904923487, 1.4992218574704086, 1.4965375971745172, 1.4969774410220387, 1.4941704340294402, 1.4984838801977327, 1.4925902093949994, 1.4926356218189185, 1.4939630874618124, 1.4949883996583597, 1.498591827562947, 1.4941912788140455, 1.4986024844328236, 1.4925944141293943, 1.4934085594799973, 1.4908230247438812, 1.4920145193898946, 1.4985770600042794, 1.4921351356428016, 1.493322619814158, 1.4890793493885768, 1.488062976077352, 1.4947614566501406, 1.4966469520416104, 1.4963601067325663, 1.489498521415115, 1.487332892368951, 1.4868371422041124, 1.487794807559411, 1.4897064663791069, 1.4858024697039407, 1.4863718849432788, 1.4855984065077388, 1.484445113763672, 1.4842867390086274, 1.4852353770140505, 1.4832764689192879, 1.4880435820722482, 1.4849778860746223, 1.4853546546959535, 1.4863715075859054, 1.488901400810884, 1.4817820435921514, 1.4813473240795567, 1.4794003514538556, 1.4784999508632526, 1.4829860133067294, 1.4802783786883345, 1.4796671417949137, 1.481083493360014, 1.479245314216222, 1.478408997798113, 1.4777660238669394, 1.484526205454519, 1.4768039374380875, 1.4776983543587905, 1.4747329051979268, 1.484849969317536, 1.48365896830079, 1.4848075439797779, 1.4918074445313252, 1.4929157856308704, 1.4780337081552777, 1.4738447517340187, 1.4735889426002267, 1.4724480026311697, 1.4710427086945677, 1.4701017531526162, 1.4717811545796953, 1.470719321699358, 1.4716151645296163, 1.4709099416615292, 1.471545237535324, 1.4693096104588597, 1.4692412929613243, 1.4667975355222729, 1.4666402045706215, 1.4677068623184424, 1.4670281897824893, 1.4654007582204298, 1.4673163069836657, 1.4643724445934414, 1.466467208343365, 1.465515094860868, 1.4760752698479247, 1.47137127789139, 1.462809586476007, 1.4666425869450188, 1.4604968731408245, 1.4662555848793328, 1.461260051942704, 1.4613757915810148, 1.4594647949481157, 1.4597740416654081, 1.4606407049989798, 1.4653096986747132, 1.4628392320394026, 1.4575772312877115, 1.4577712646010474, 1.4616705229150195, 1.4610630957742492, 1.4592645917829792, 1.4579055487008066, 1.460403802507467, 1.4568269657158508, 1.461232421138693, 1.459771690476357, 1.4574163038872596, 1.4564740131523086, 1.4596401699759387, 1.4562378873570498, 1.4531167793567665, 1.4504175261550372, 1.4481622460931234, 1.4509030941820242, 1.4600904339882383, 1.4576771271547009, 1.4484102459169266, 1.4508960742970023, 1.4551059004217692, 1.455403547463231, 1.4523870073549556, 1.4540427132553633, 1.454904786664113, 1.4510080006577886, 1.4494708948311619, 1.4500255754596154, 1.4441162173018562, 1.4449472493459556, 1.444438507963255, 1.4423405079381422, 1.4444986345342052, 1.4505102144864062, 1.4609315326326437, 1.449207462079716, 1.453091294026228, 1.4572635106727083, 1.4560526772935778, 1.4474886019371862, 1.4574418365588178, 1.4479386849080267, 1.4424540192194788, 1.441397698752934, 1.4431953178049357, 1.4414694251465847, 1.439885788238024, 1.4341049658444383, 1.4344295390577533, 1.433157659556097, 1.430519828218699, 1.4344022231914668, 1.4339003451306227, 1.4358430982859962, 1.437154262315566, 1.4346743797619486, 1.432206955680612, 1.4288675745409862, 1.4277340711754205, 1.4327730478936886, 1.4336766715901588, 1.4356154246496713, 1.4337608330058855, 1.428327221890005, 1.4294061421858457, 1.4316852668472384, 1.4262897269192172, 1.4282180394969681, 1.4294673411018795, 1.435698522581457, 1.431598918589723, 1.4338158545308044, 1.4432769889459474, 1.4354692985389756, 1.4314972439340987, 1.4292785072718313, 1.422494217845204, 1.4216132140012736, 1.4192784452340441, 1.4196189253726779, 1.4170106869213879, 1.41511148647606, 1.4159263850237556, 1.4199844475399541, 1.4260254480021202, 1.4243003410969917, 1.420230579229351, 1.4155637161932442, 1.4136694131201053], 'acc': [0.18809034835998528, 0.23285421029620593, 0.23285420892542147, 0.23285421049203225, 0.23285420970872686, 0.23285420970872686, 0.23285421008202084, 0.23285420970872686, 0.2340862425629363, 0.24106776322672255, 0.23983572843260834, 0.23655030787970252, 0.24312114955463449, 0.24476386002691375, 0.24517453803663625, 0.24435318240884393, 0.24640657169250982, 0.24476385963526104, 0.2574948671845685, 0.26488706448729277, 0.26036960970939305, 0.257494866187078, 0.26406570768454235, 0.265297742888668, 0.25913757822596806, 0.2599589320546058, 0.26817248406106686, 0.26324435421083986, 0.2607802892857264, 0.2628336769844227, 0.2673511294124063, 0.264476386869223, 0.2624229979955685, 0.2624229991705266, 0.26611909792898125, 0.2661190959707178, 0.26365502865897067, 0.26570841991925875, 0.26406571062193757, 0.26447638663667916, 0.26735112878820977, 0.2681724862151567, 0.2681724822619123, 0.26365502967481985, 0.26899384068634963, 0.26694044979935555, 0.2702258725430686, 0.2731006176319945, 0.26776180800960786, 0.26817248265356497, 0.26940451550777444, 0.27761807002326055, 0.27392197345561314, 0.27351129149264625, 0.27104723012912446, 0.27145790774719425, 0.2755646795829953, 0.27351129348762715, 0.27926078131556265, 0.2833675586712189, 0.2751540055265172, 0.2706365515319229, 0.2735112940751062, 0.27310061367875005, 0.2735112948584116, 0.2759753605668305, 0.28131416721510444, 0.2780287486571796, 0.2821355246420514, 0.2780287472496777, 0.28213552147211235, 0.2915811066877181, 0.28418891112907224, 0.2907597526265366, 0.28172484385404256, 0.29158110727519715, 0.2870636544555609, 0.2882956859389859, 0.2891170423500837, 0.2874743330527625, 0.2911704306362591, 0.28583162300885334, 0.28870636594368937, 0.2891170413709519, 0.2829568786665155, 0.29363449556137255, 0.28870636512366654, 0.27967145873780613, 0.2903490759875985, 0.2919917845016143, 0.29363449853548523, 0.2833675545221481, 0.29363449791128876, 0.28911704512836994, 0.29404517497859695, 0.28993839739039695, 0.2936344953655462, 0.2936344957571989, 0.2952772085793944, 0.2940451763493814, 0.29363449794800617, 0.29650923908368765, 0.2997946630023588, 0.29897330479210654, 0.29815194896848785, 0.2919917882590323, 0.2948665307654982, 0.3080082153637551, 0.2936344973238097, 0.3026694065246739, 0.3014373732420943, 0.29568788404337437, 0.3002053368630106, 0.2989733036171484, 0.29815195253007953, 0.30266940492134564, 0.29897330635871733, 0.3034907585908745, 0.30020533803796867, 0.3034907617608135, 0.301026693665761, 0.30595482586590417, 0.30308008038532563, 0.30143737108800445, 0.307597535554878, 0.3154004106898572, 0.30184804933027076, 0.3059548252784251, 0.30718685832846093, 0.29897330717874016, 0.303901436013118, 0.2977412721337234, 0.3071868600908981, 0.31375770240838524, 0.306776179535433, 0.30677617973125937, 0.2993839820185236, 0.3026694041380402, 0.31375770021757793, 0.3034907585908745, 0.31088295571613117, 0.30965092384105347, 0.3129363443572418, 0.314168376269037, 0.30841889141521417, 0.31334702357863986, 0.30965092266609534, 0.31334702341953097, 0.3219712531664533, 0.3096509240368798, 0.3149897342467455, 0.3075975359465307, 0.3203285432816531, 0.3145790560411966, 0.3067761793028892, 0.3112936360757699, 0.31991786370531977, 0.31704311939969937, 0.31704312272874724, 0.32279260840259294, 0.32114989515202735, 0.32238192843460695, 0.3174537976052482, 0.3141683756448405, 0.323613965009517, 0.3112936358799435, 0.31416837701562494, 0.31581108967871147, 0.3256673527082134, 0.317453799759338, 0.312936344785612, 0.324024640473497, 0.3088295704040684, 0.32361396324707986, 0.31375769943427256, 0.31498973130935026, 0.32156057711499425, 0.3203285440649585, 0.3223819319594812, 0.3330595478384891, 0.32197125199149523, 0.3260780291513251, 0.32197125336227966, 0.33018480729028676, 0.32525667548179626, 0.3310061581815293, 0.3285420938806123, 0.3301848055278496, 0.33634497116968126, 0.3248459973129648, 0.3351129351455328, 0.32895277306529286, 0.3264887087276584, 0.3293634493125782, 0.3281314178291532, 0.3195071888655363, 0.32114989871361904, 0.32484599469378744, 0.33264886966965773, 0.32484599708042106, 0.33388090487378336, 0.3293634512708417, 0.3326488728028793, 0.33552361237194994, 0.3351129351455328, 0.3388090327290294, 0.333059549245991, 0.32813141763332687, 0.32114989671863814, 0.3342915822960268, 0.324845996492942, 0.33757700245727995, 0.3367556474169667, 0.3330595472877275, 0.33675564780861933, 0.3256673497708182, 0.3318275136134953, 0.3392197130886681, 0.3310061590015521, 0.33921971195042744, 0.34045174656707405, 0.3383983594558567, 0.3359342913975216, 0.3383983594558567, 0.34332648793529924, 0.3462012298542861, 0.34620123439500955, 0.3449691979792084, 0.34045174656707405, 0.34332649087269446, 0.34332648911025737, 0.3462012324000286, 0.3351129375321664, 0.34414784415057065, 0.33839835632263515, 0.3445585223561195, 0.34414784219230715, 0.34661190805983494, 0.34743326486258536, 0.3519507194813762, 0.34743326724921897, 0.35071868404470674, 0.3474332650951292, 0.3494866533813046, 0.3437371677074589, 0.33921971488782265, 0.34661191201307934, 0.3408624232060121, 0.32772074040690974, 0.3383983594558567, 0.33757700104977806, 0.34825461833628785, 0.34907597474738555, 0.3482546189237669, 0.35359342913363256, 0.35195071826970065, 0.3527720762841266, 0.3589322371894084, 0.3589322377768875, 0.36262833555621043, 0.35852155957133863, 0.35811088492738147, 0.35236139432115965, 0.3560574960170096, 0.3544147865605795, 0.35811088371570593, 0.3527720762841266, 0.3597535926213744, 0.35975359242554805, 0.3646817254089966, 0.36221765950475143, 0.3556468160490236, 0.3523613949086387, 0.3589322385969103, 0.35359342721208653, 0.3572895263254765, 0.36427104837840585, 0.3626283351645577, 0.3548254612045366, 0.3548254606170576, 0.3540041057725707, 0.3683778249874742, 0.3687884990072348, 0.34332648691945006, 0.35564681585319724, 0.3523613956919441, 0.3552361388593239, 0.35852156055047035, 0.3683778216217088, 0.3613963028978273, 0.36878850319302303, 0.365913759670708, 0.36837782142588243, 0.36960985604252905, 0.3671457913132419, 0.3626283363395158, 0.3663244362729286, 0.3749486670357001, 0.36796714478694437, 0.36550307809939375]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
