{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf6.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 13:30:27 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'Front', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '03', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000024D01225E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000024D613A6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.1078, Accuracy:0.2862, Validation Loss:1.0919, Validation Accuracy:0.3908\n",
    "Epoch #2: Loss:1.0866, Accuracy:0.3885, Validation Loss:1.0789, Validation Accuracy:0.3924\n",
    "Epoch #3: Loss:1.0763, Accuracy:0.3971, Validation Loss:1.0746, Validation Accuracy:0.3859\n",
    "Epoch #4: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0749, Validation Accuracy:0.3908\n",
    "Epoch #5: Loss:1.0743, Accuracy:0.3959, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #6: Loss:1.0744, Accuracy:0.3963, Validation Loss:1.0747, Validation Accuracy:0.3924\n",
    "Epoch #7: Loss:1.0744, Accuracy:0.3947, Validation Loss:1.0740, Validation Accuracy:0.3875\n",
    "Epoch #8: Loss:1.0737, Accuracy:0.3959, Validation Loss:1.0737, Validation Accuracy:0.3974\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.4004, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #10: Loss:1.0737, Accuracy:0.3926, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #11: Loss:1.0734, Accuracy:0.3926, Validation Loss:1.0738, Validation Accuracy:0.3924\n",
    "Epoch #12: Loss:1.0734, Accuracy:0.4000, Validation Loss:1.0741, Validation Accuracy:0.4039\n",
    "Epoch #13: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3908\n",
    "Epoch #14: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #15: Loss:1.0739, Accuracy:0.3955, Validation Loss:1.0750, Validation Accuracy:0.3842\n",
    "Epoch #16: Loss:1.0734, Accuracy:0.3951, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #17: Loss:1.0734, Accuracy:0.3930, Validation Loss:1.0747, Validation Accuracy:0.3760\n",
    "Epoch #18: Loss:1.0734, Accuracy:0.3963, Validation Loss:1.0749, Validation Accuracy:0.3875\n",
    "Epoch #19: Loss:1.0733, Accuracy:0.4008, Validation Loss:1.0746, Validation Accuracy:0.3875\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0737, Validation Accuracy:0.3892\n",
    "Epoch #21: Loss:1.0727, Accuracy:0.4021, Validation Loss:1.0739, Validation Accuracy:0.3859\n",
    "Epoch #22: Loss:1.0726, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3826\n",
    "Epoch #23: Loss:1.0723, Accuracy:0.3979, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0738, Validation Accuracy:0.3908\n",
    "Epoch #25: Loss:1.0727, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0734, Accuracy:0.3947, Validation Loss:1.0728, Validation Accuracy:0.4007\n",
    "Epoch #27: Loss:1.0729, Accuracy:0.4004, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #28: Loss:1.0723, Accuracy:0.3992, Validation Loss:1.0732, Validation Accuracy:0.3974\n",
    "Epoch #29: Loss:1.0728, Accuracy:0.3926, Validation Loss:1.0739, Validation Accuracy:0.3892\n",
    "Epoch #30: Loss:1.0728, Accuracy:0.3910, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #31: Loss:1.0725, Accuracy:0.4029, Validation Loss:1.0731, Validation Accuracy:0.3892\n",
    "Epoch #32: Loss:1.0730, Accuracy:0.3996, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #33: Loss:1.0728, Accuracy:0.3971, Validation Loss:1.0734, Validation Accuracy:0.3924\n",
    "Epoch #34: Loss:1.0723, Accuracy:0.3975, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #35: Loss:1.0719, Accuracy:0.3996, Validation Loss:1.0739, Validation Accuracy:0.3924\n",
    "Epoch #36: Loss:1.0719, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3875\n",
    "Epoch #37: Loss:1.0714, Accuracy:0.3971, Validation Loss:1.0730, Validation Accuracy:0.3826\n",
    "Epoch #38: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0729, Validation Accuracy:0.3957\n",
    "Epoch #39: Loss:1.0732, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #40: Loss:1.0726, Accuracy:0.3988, Validation Loss:1.0754, Validation Accuracy:0.4039\n",
    "Epoch #41: Loss:1.0712, Accuracy:0.3967, Validation Loss:1.0763, Validation Accuracy:0.4122\n",
    "Epoch #42: Loss:1.0724, Accuracy:0.3934, Validation Loss:1.0744, Validation Accuracy:0.3908\n",
    "Epoch #43: Loss:1.0720, Accuracy:0.3934, Validation Loss:1.0764, Validation Accuracy:0.3924\n",
    "Epoch #44: Loss:1.0700, Accuracy:0.3992, Validation Loss:1.0767, Validation Accuracy:0.3875\n",
    "Epoch #45: Loss:1.0720, Accuracy:0.4057, Validation Loss:1.0757, Validation Accuracy:0.3859\n",
    "Epoch #46: Loss:1.0708, Accuracy:0.3934, Validation Loss:1.0773, Validation Accuracy:0.4007\n",
    "Epoch #47: Loss:1.0708, Accuracy:0.3897, Validation Loss:1.0738, Validation Accuracy:0.3892\n",
    "Epoch #48: Loss:1.0684, Accuracy:0.3947, Validation Loss:1.0801, Validation Accuracy:0.3941\n",
    "Epoch #49: Loss:1.0723, Accuracy:0.4000, Validation Loss:1.0740, Validation Accuracy:0.4023\n",
    "Epoch #50: Loss:1.0666, Accuracy:0.3992, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #51: Loss:1.0674, Accuracy:0.4000, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0655, Accuracy:0.4004, Validation Loss:1.0730, Validation Accuracy:0.3941\n",
    "Epoch #53: Loss:1.0652, Accuracy:0.3996, Validation Loss:1.0767, Validation Accuracy:0.3892\n",
    "Epoch #54: Loss:1.0680, Accuracy:0.4004, Validation Loss:1.0745, Validation Accuracy:0.3990\n",
    "Epoch #55: Loss:1.0748, Accuracy:0.3992, Validation Loss:1.0859, Validation Accuracy:0.3875\n",
    "Epoch #56: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0766, Validation Accuracy:0.4007\n",
    "Epoch #57: Loss:1.0691, Accuracy:0.4123, Validation Loss:1.0791, Validation Accuracy:0.3957\n",
    "Epoch #58: Loss:1.0781, Accuracy:0.3971, Validation Loss:1.0789, Validation Accuracy:0.3842\n",
    "Epoch #59: Loss:1.0758, Accuracy:0.3906, Validation Loss:1.0797, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0767, Accuracy:0.3914, Validation Loss:1.0783, Validation Accuracy:0.3990\n",
    "Epoch #61: Loss:1.0742, Accuracy:0.4062, Validation Loss:1.0786, Validation Accuracy:0.4007\n",
    "Epoch #62: Loss:1.0744, Accuracy:0.3951, Validation Loss:1.0762, Validation Accuracy:0.4122\n",
    "Epoch #63: Loss:1.0729, Accuracy:0.4152, Validation Loss:1.0737, Validation Accuracy:0.3957\n",
    "Epoch #64: Loss:1.0720, Accuracy:0.3963, Validation Loss:1.0750, Validation Accuracy:0.3957\n",
    "Epoch #65: Loss:1.0724, Accuracy:0.3951, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #66: Loss:1.0716, Accuracy:0.3984, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #67: Loss:1.0712, Accuracy:0.4070, Validation Loss:1.0742, Validation Accuracy:0.3892\n",
    "Epoch #68: Loss:1.0710, Accuracy:0.4049, Validation Loss:1.0743, Validation Accuracy:0.3924\n",
    "Epoch #69: Loss:1.0703, Accuracy:0.3984, Validation Loss:1.0722, Validation Accuracy:0.4138\n",
    "Epoch #70: Loss:1.0698, Accuracy:0.3979, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0714, Accuracy:0.3967, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #72: Loss:1.0726, Accuracy:0.3934, Validation Loss:1.0753, Validation Accuracy:0.3826\n",
    "Epoch #73: Loss:1.0729, Accuracy:0.3864, Validation Loss:1.0748, Validation Accuracy:0.4072\n",
    "Epoch #74: Loss:1.0728, Accuracy:0.3992, Validation Loss:1.0748, Validation Accuracy:0.3974\n",
    "Epoch #75: Loss:1.0723, Accuracy:0.3955, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #76: Loss:1.0713, Accuracy:0.3914, Validation Loss:1.0746, Validation Accuracy:0.3908\n",
    "Epoch #77: Loss:1.0701, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.3957\n",
    "Epoch #78: Loss:1.0696, Accuracy:0.3938, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #79: Loss:1.0712, Accuracy:0.3926, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #80: Loss:1.0715, Accuracy:0.3918, Validation Loss:1.0735, Validation Accuracy:0.3793\n",
    "Epoch #81: Loss:1.0716, Accuracy:0.3840, Validation Loss:1.0736, Validation Accuracy:0.3957\n",
    "Epoch #82: Loss:1.0702, Accuracy:0.4004, Validation Loss:1.0726, Validation Accuracy:0.3974\n",
    "Epoch #83: Loss:1.0699, Accuracy:0.4033, Validation Loss:1.0731, Validation Accuracy:0.3990\n",
    "Epoch #84: Loss:1.0701, Accuracy:0.4066, Validation Loss:1.0732, Validation Accuracy:0.4007\n",
    "Epoch #85: Loss:1.0700, Accuracy:0.4099, Validation Loss:1.0716, Validation Accuracy:0.3990\n",
    "Epoch #86: Loss:1.0694, Accuracy:0.4049, Validation Loss:1.0716, Validation Accuracy:0.4039\n",
    "Epoch #87: Loss:1.0682, Accuracy:0.4037, Validation Loss:1.0713, Validation Accuracy:0.4056\n",
    "Epoch #88: Loss:1.0675, Accuracy:0.4062, Validation Loss:1.0702, Validation Accuracy:0.3908\n",
    "Epoch #89: Loss:1.0673, Accuracy:0.3938, Validation Loss:1.0720, Validation Accuracy:0.3875\n",
    "Epoch #90: Loss:1.0675, Accuracy:0.4053, Validation Loss:1.0733, Validation Accuracy:0.3908\n",
    "Epoch #91: Loss:1.0656, Accuracy:0.4041, Validation Loss:1.0720, Validation Accuracy:0.3859\n",
    "Epoch #92: Loss:1.0661, Accuracy:0.4041, Validation Loss:1.0733, Validation Accuracy:0.4072\n",
    "Epoch #93: Loss:1.0662, Accuracy:0.4000, Validation Loss:1.0735, Validation Accuracy:0.3908\n",
    "Epoch #94: Loss:1.0657, Accuracy:0.4033, Validation Loss:1.0773, Validation Accuracy:0.3859\n",
    "Epoch #95: Loss:1.0664, Accuracy:0.3992, Validation Loss:1.0776, Validation Accuracy:0.4023\n",
    "Epoch #96: Loss:1.0675, Accuracy:0.3967, Validation Loss:1.0771, Validation Accuracy:0.3875\n",
    "Epoch #97: Loss:1.0659, Accuracy:0.4062, Validation Loss:1.0758, Validation Accuracy:0.3777\n",
    "Epoch #98: Loss:1.0656, Accuracy:0.4053, Validation Loss:1.0776, Validation Accuracy:0.3892\n",
    "Epoch #99: Loss:1.0683, Accuracy:0.4037, Validation Loss:1.0773, Validation Accuracy:0.3875\n",
    "Epoch #100: Loss:1.0706, Accuracy:0.4107, Validation Loss:1.0784, Validation Accuracy:0.3842\n",
    "Epoch #101: Loss:1.0737, Accuracy:0.3963, Validation Loss:1.0802, Validation Accuracy:0.3777\n",
    "Epoch #102: Loss:1.0693, Accuracy:0.4115, Validation Loss:1.0789, Validation Accuracy:0.3760\n",
    "Epoch #103: Loss:1.0684, Accuracy:0.4078, Validation Loss:1.0792, Validation Accuracy:0.3777\n",
    "Epoch #104: Loss:1.0686, Accuracy:0.4062, Validation Loss:1.0794, Validation Accuracy:0.3695\n",
    "Epoch #105: Loss:1.0677, Accuracy:0.4156, Validation Loss:1.0791, Validation Accuracy:0.3727\n",
    "Epoch #106: Loss:1.0681, Accuracy:0.4078, Validation Loss:1.0792, Validation Accuracy:0.3859\n",
    "Epoch #107: Loss:1.0677, Accuracy:0.4123, Validation Loss:1.0805, Validation Accuracy:0.3892\n",
    "Epoch #108: Loss:1.0667, Accuracy:0.4140, Validation Loss:1.0807, Validation Accuracy:0.3875\n",
    "Epoch #109: Loss:1.0677, Accuracy:0.4041, Validation Loss:1.0806, Validation Accuracy:0.3842\n",
    "Epoch #110: Loss:1.0668, Accuracy:0.4164, Validation Loss:1.0810, Validation Accuracy:0.3908\n",
    "Epoch #111: Loss:1.0662, Accuracy:0.4156, Validation Loss:1.0810, Validation Accuracy:0.3810\n",
    "Epoch #112: Loss:1.0658, Accuracy:0.4115, Validation Loss:1.0813, Validation Accuracy:0.3875\n",
    "Epoch #113: Loss:1.0644, Accuracy:0.4168, Validation Loss:1.0827, Validation Accuracy:0.3826\n",
    "Epoch #114: Loss:1.0668, Accuracy:0.4074, Validation Loss:1.0809, Validation Accuracy:0.3859\n",
    "Epoch #115: Loss:1.0653, Accuracy:0.4144, Validation Loss:1.0800, Validation Accuracy:0.3875\n",
    "Epoch #116: Loss:1.0652, Accuracy:0.4152, Validation Loss:1.0813, Validation Accuracy:0.3760\n",
    "Epoch #117: Loss:1.0650, Accuracy:0.4070, Validation Loss:1.0820, Validation Accuracy:0.3826\n",
    "Epoch #118: Loss:1.0639, Accuracy:0.4177, Validation Loss:1.0863, Validation Accuracy:0.3842\n",
    "Epoch #119: Loss:1.0662, Accuracy:0.4082, Validation Loss:1.0867, Validation Accuracy:0.3908\n",
    "Epoch #120: Loss:1.0644, Accuracy:0.4123, Validation Loss:1.0863, Validation Accuracy:0.3875\n",
    "Epoch #121: Loss:1.0645, Accuracy:0.4070, Validation Loss:1.0858, Validation Accuracy:0.3859\n",
    "Epoch #122: Loss:1.0642, Accuracy:0.4136, Validation Loss:1.0851, Validation Accuracy:0.3793\n",
    "Epoch #123: Loss:1.0638, Accuracy:0.4164, Validation Loss:1.0849, Validation Accuracy:0.3842\n",
    "Epoch #124: Loss:1.0638, Accuracy:0.4156, Validation Loss:1.0864, Validation Accuracy:0.3810\n",
    "Epoch #125: Loss:1.0630, Accuracy:0.4070, Validation Loss:1.0850, Validation Accuracy:0.3908\n",
    "Epoch #126: Loss:1.0617, Accuracy:0.4115, Validation Loss:1.0864, Validation Accuracy:0.3826\n",
    "Epoch #127: Loss:1.0614, Accuracy:0.4189, Validation Loss:1.0851, Validation Accuracy:0.3842\n",
    "Epoch #128: Loss:1.0609, Accuracy:0.4168, Validation Loss:1.0867, Validation Accuracy:0.3842\n",
    "Epoch #129: Loss:1.0600, Accuracy:0.4168, Validation Loss:1.0859, Validation Accuracy:0.3908\n",
    "Epoch #130: Loss:1.0608, Accuracy:0.4127, Validation Loss:1.0884, Validation Accuracy:0.3826\n",
    "Epoch #131: Loss:1.0609, Accuracy:0.4205, Validation Loss:1.0941, Validation Accuracy:0.3875\n",
    "Epoch #132: Loss:1.0639, Accuracy:0.4127, Validation Loss:1.0940, Validation Accuracy:0.3842\n",
    "Epoch #133: Loss:1.0654, Accuracy:0.4070, Validation Loss:1.0901, Validation Accuracy:0.3793\n",
    "Epoch #134: Loss:1.0636, Accuracy:0.4127, Validation Loss:1.0891, Validation Accuracy:0.3859\n",
    "Epoch #135: Loss:1.0614, Accuracy:0.4041, Validation Loss:1.0872, Validation Accuracy:0.3924\n",
    "Epoch #136: Loss:1.0639, Accuracy:0.4094, Validation Loss:1.0874, Validation Accuracy:0.3859\n",
    "Epoch #137: Loss:1.0646, Accuracy:0.4045, Validation Loss:1.0891, Validation Accuracy:0.3810\n",
    "Epoch #138: Loss:1.0628, Accuracy:0.4111, Validation Loss:1.0923, Validation Accuracy:0.3596\n",
    "Epoch #139: Loss:1.0644, Accuracy:0.4099, Validation Loss:1.0904, Validation Accuracy:0.3629\n",
    "Epoch #140: Loss:1.0630, Accuracy:0.4115, Validation Loss:1.0930, Validation Accuracy:0.3760\n",
    "Epoch #141: Loss:1.0619, Accuracy:0.4119, Validation Loss:1.0872, Validation Accuracy:0.3777\n",
    "Epoch #142: Loss:1.0626, Accuracy:0.4033, Validation Loss:1.0905, Validation Accuracy:0.3810\n",
    "Epoch #143: Loss:1.0622, Accuracy:0.4107, Validation Loss:1.0879, Validation Accuracy:0.3924\n",
    "Epoch #144: Loss:1.0589, Accuracy:0.4111, Validation Loss:1.0904, Validation Accuracy:0.3892\n",
    "Epoch #145: Loss:1.0602, Accuracy:0.4168, Validation Loss:1.0887, Validation Accuracy:0.3892\n",
    "Epoch #146: Loss:1.0588, Accuracy:0.4144, Validation Loss:1.0898, Validation Accuracy:0.3777\n",
    "Epoch #147: Loss:1.0622, Accuracy:0.4115, Validation Loss:1.0880, Validation Accuracy:0.3908\n",
    "Epoch #148: Loss:1.0602, Accuracy:0.4094, Validation Loss:1.0896, Validation Accuracy:0.3744\n",
    "Epoch #149: Loss:1.0611, Accuracy:0.4127, Validation Loss:1.0889, Validation Accuracy:0.3957\n",
    "Epoch #150: Loss:1.0592, Accuracy:0.4107, Validation Loss:1.0904, Validation Accuracy:0.3842\n",
    "Epoch #151: Loss:1.0592, Accuracy:0.4156, Validation Loss:1.0897, Validation Accuracy:0.3875\n",
    "Epoch #152: Loss:1.0589, Accuracy:0.4209, Validation Loss:1.0901, Validation Accuracy:0.3924\n",
    "Epoch #153: Loss:1.0621, Accuracy:0.4127, Validation Loss:1.0910, Validation Accuracy:0.3859\n",
    "Epoch #154: Loss:1.0608, Accuracy:0.4209, Validation Loss:1.0917, Validation Accuracy:0.3777\n",
    "Epoch #155: Loss:1.0607, Accuracy:0.4119, Validation Loss:1.0958, Validation Accuracy:0.3859\n",
    "Epoch #156: Loss:1.0594, Accuracy:0.4082, Validation Loss:1.0945, Validation Accuracy:0.3957\n",
    "Epoch #157: Loss:1.0592, Accuracy:0.4177, Validation Loss:1.0939, Validation Accuracy:0.3842\n",
    "Epoch #158: Loss:1.0560, Accuracy:0.4242, Validation Loss:1.0950, Validation Accuracy:0.3842\n",
    "Epoch #159: Loss:1.0576, Accuracy:0.4209, Validation Loss:1.0936, Validation Accuracy:0.3842\n",
    "Epoch #160: Loss:1.0550, Accuracy:0.4230, Validation Loss:1.0887, Validation Accuracy:0.3826\n",
    "Epoch #161: Loss:1.0555, Accuracy:0.4209, Validation Loss:1.0898, Validation Accuracy:0.3842\n",
    "Epoch #162: Loss:1.0545, Accuracy:0.4234, Validation Loss:1.0971, Validation Accuracy:0.3842\n",
    "Epoch #163: Loss:1.0543, Accuracy:0.4197, Validation Loss:1.0983, Validation Accuracy:0.3908\n",
    "Epoch #164: Loss:1.0529, Accuracy:0.4238, Validation Loss:1.1003, Validation Accuracy:0.3810\n",
    "Epoch #165: Loss:1.0544, Accuracy:0.4148, Validation Loss:1.1016, Validation Accuracy:0.3793\n",
    "Epoch #166: Loss:1.0578, Accuracy:0.4123, Validation Loss:1.1014, Validation Accuracy:0.3859\n",
    "Epoch #167: Loss:1.0555, Accuracy:0.4127, Validation Loss:1.0957, Validation Accuracy:0.3892\n",
    "Epoch #168: Loss:1.0548, Accuracy:0.4218, Validation Loss:1.0923, Validation Accuracy:0.3908\n",
    "Epoch #169: Loss:1.0589, Accuracy:0.4152, Validation Loss:1.0950, Validation Accuracy:0.3777\n",
    "Epoch #170: Loss:1.0569, Accuracy:0.4037, Validation Loss:1.0904, Validation Accuracy:0.3892\n",
    "Epoch #171: Loss:1.0608, Accuracy:0.4177, Validation Loss:1.0955, Validation Accuracy:0.3859\n",
    "Epoch #172: Loss:1.0589, Accuracy:0.4160, Validation Loss:1.0986, Validation Accuracy:0.3777\n",
    "Epoch #173: Loss:1.0560, Accuracy:0.4156, Validation Loss:1.0970, Validation Accuracy:0.3810\n",
    "Epoch #174: Loss:1.0587, Accuracy:0.4189, Validation Loss:1.0961, Validation Accuracy:0.3793\n",
    "Epoch #175: Loss:1.0577, Accuracy:0.4214, Validation Loss:1.0876, Validation Accuracy:0.3826\n",
    "Epoch #176: Loss:1.0583, Accuracy:0.4148, Validation Loss:1.0844, Validation Accuracy:0.3810\n",
    "Epoch #177: Loss:1.0573, Accuracy:0.4312, Validation Loss:1.0874, Validation Accuracy:0.3727\n",
    "Epoch #178: Loss:1.0569, Accuracy:0.4255, Validation Loss:1.0852, Validation Accuracy:0.3826\n",
    "Epoch #179: Loss:1.0528, Accuracy:0.4324, Validation Loss:1.0947, Validation Accuracy:0.3727\n",
    "Epoch #180: Loss:1.0556, Accuracy:0.4209, Validation Loss:1.0941, Validation Accuracy:0.3760\n",
    "Epoch #181: Loss:1.0546, Accuracy:0.4222, Validation Loss:1.0918, Validation Accuracy:0.3777\n",
    "Epoch #182: Loss:1.0506, Accuracy:0.4193, Validation Loss:1.0939, Validation Accuracy:0.3760\n",
    "Epoch #183: Loss:1.0494, Accuracy:0.4263, Validation Loss:1.0879, Validation Accuracy:0.3842\n",
    "Epoch #184: Loss:1.0502, Accuracy:0.4329, Validation Loss:1.0930, Validation Accuracy:0.3695\n",
    "Epoch #185: Loss:1.0509, Accuracy:0.4300, Validation Loss:1.0938, Validation Accuracy:0.3826\n",
    "Epoch #186: Loss:1.0498, Accuracy:0.4275, Validation Loss:1.0943, Validation Accuracy:0.3744\n",
    "Epoch #187: Loss:1.0506, Accuracy:0.4312, Validation Loss:1.0871, Validation Accuracy:0.3810\n",
    "Epoch #188: Loss:1.0504, Accuracy:0.4263, Validation Loss:1.0926, Validation Accuracy:0.3842\n",
    "Epoch #189: Loss:1.0517, Accuracy:0.4209, Validation Loss:1.0988, Validation Accuracy:0.3826\n",
    "Epoch #190: Loss:1.0529, Accuracy:0.4279, Validation Loss:1.0968, Validation Accuracy:0.3842\n",
    "Epoch #191: Loss:1.0552, Accuracy:0.4234, Validation Loss:1.0966, Validation Accuracy:0.3859\n",
    "Epoch #192: Loss:1.0537, Accuracy:0.4316, Validation Loss:1.0911, Validation Accuracy:0.3826\n",
    "Epoch #193: Loss:1.0544, Accuracy:0.4197, Validation Loss:1.0925, Validation Accuracy:0.3793\n",
    "Epoch #194: Loss:1.0533, Accuracy:0.4349, Validation Loss:1.0936, Validation Accuracy:0.3810\n",
    "Epoch #195: Loss:1.0550, Accuracy:0.4296, Validation Loss:1.0921, Validation Accuracy:0.3793\n",
    "Epoch #196: Loss:1.0566, Accuracy:0.4238, Validation Loss:1.0949, Validation Accuracy:0.3810\n",
    "Epoch #197: Loss:1.0577, Accuracy:0.4246, Validation Loss:1.0952, Validation Accuracy:0.3842\n",
    "Epoch #198: Loss:1.0584, Accuracy:0.4172, Validation Loss:1.0969, Validation Accuracy:0.3826\n",
    "Epoch #199: Loss:1.0558, Accuracy:0.4271, Validation Loss:1.0971, Validation Accuracy:0.3711\n",
    "Epoch #200: Loss:1.0577, Accuracy:0.4189, Validation Loss:1.0943, Validation Accuracy:0.3612\n",
    "Epoch #201: Loss:1.0577, Accuracy:0.4246, Validation Loss:1.0957, Validation Accuracy:0.3629\n",
    "Epoch #202: Loss:1.0558, Accuracy:0.4226, Validation Loss:1.0974, Validation Accuracy:0.3662\n",
    "Epoch #203: Loss:1.0540, Accuracy:0.4255, Validation Loss:1.0971, Validation Accuracy:0.3645\n",
    "Epoch #204: Loss:1.0538, Accuracy:0.4226, Validation Loss:1.1001, Validation Accuracy:0.3662\n",
    "Epoch #205: Loss:1.0545, Accuracy:0.4263, Validation Loss:1.1006, Validation Accuracy:0.3612\n",
    "Epoch #206: Loss:1.0556, Accuracy:0.4218, Validation Loss:1.1037, Validation Accuracy:0.3563\n",
    "Epoch #207: Loss:1.0551, Accuracy:0.4201, Validation Loss:1.1001, Validation Accuracy:0.3678\n",
    "Epoch #208: Loss:1.0524, Accuracy:0.4136, Validation Loss:1.1008, Validation Accuracy:0.3727\n",
    "Epoch #209: Loss:1.0524, Accuracy:0.4226, Validation Loss:1.0999, Validation Accuracy:0.3727\n",
    "Epoch #210: Loss:1.0520, Accuracy:0.4226, Validation Loss:1.1000, Validation Accuracy:0.3678\n",
    "Epoch #211: Loss:1.0537, Accuracy:0.4271, Validation Loss:1.0982, Validation Accuracy:0.3810\n",
    "Epoch #212: Loss:1.0563, Accuracy:0.4119, Validation Loss:1.1006, Validation Accuracy:0.3678\n",
    "Epoch #213: Loss:1.0525, Accuracy:0.4177, Validation Loss:1.0988, Validation Accuracy:0.3678\n",
    "Epoch #214: Loss:1.0517, Accuracy:0.4172, Validation Loss:1.1031, Validation Accuracy:0.3612\n",
    "Epoch #215: Loss:1.0516, Accuracy:0.4172, Validation Loss:1.1013, Validation Accuracy:0.3678\n",
    "Epoch #216: Loss:1.0517, Accuracy:0.4275, Validation Loss:1.1036, Validation Accuracy:0.3629\n",
    "Epoch #217: Loss:1.0515, Accuracy:0.4255, Validation Loss:1.1090, Validation Accuracy:0.3678\n",
    "Epoch #218: Loss:1.0514, Accuracy:0.4296, Validation Loss:1.1052, Validation Accuracy:0.3711\n",
    "Epoch #219: Loss:1.0494, Accuracy:0.4296, Validation Loss:1.1069, Validation Accuracy:0.3727\n",
    "Epoch #220: Loss:1.0479, Accuracy:0.4283, Validation Loss:1.1084, Validation Accuracy:0.3629\n",
    "Epoch #221: Loss:1.0524, Accuracy:0.4292, Validation Loss:1.1047, Validation Accuracy:0.3695\n",
    "Epoch #222: Loss:1.0499, Accuracy:0.4296, Validation Loss:1.1083, Validation Accuracy:0.3662\n",
    "Epoch #223: Loss:1.0481, Accuracy:0.4263, Validation Loss:1.1059, Validation Accuracy:0.3744\n",
    "Epoch #224: Loss:1.0461, Accuracy:0.4287, Validation Loss:1.1111, Validation Accuracy:0.3514\n",
    "Epoch #225: Loss:1.0489, Accuracy:0.4259, Validation Loss:1.1119, Validation Accuracy:0.3727\n",
    "Epoch #226: Loss:1.0489, Accuracy:0.4279, Validation Loss:1.1104, Validation Accuracy:0.3629\n",
    "Epoch #227: Loss:1.0466, Accuracy:0.4386, Validation Loss:1.1031, Validation Accuracy:0.3859\n",
    "Epoch #228: Loss:1.0449, Accuracy:0.4402, Validation Loss:1.0954, Validation Accuracy:0.3662\n",
    "Epoch #229: Loss:1.0446, Accuracy:0.4357, Validation Loss:1.1006, Validation Accuracy:0.3727\n",
    "Epoch #230: Loss:1.0440, Accuracy:0.4300, Validation Loss:1.1030, Validation Accuracy:0.3760\n",
    "Epoch #231: Loss:1.0429, Accuracy:0.4308, Validation Loss:1.1072, Validation Accuracy:0.3744\n",
    "Epoch #232: Loss:1.0442, Accuracy:0.4304, Validation Loss:1.1117, Validation Accuracy:0.3711\n",
    "Epoch #233: Loss:1.0461, Accuracy:0.4349, Validation Loss:1.1064, Validation Accuracy:0.3875\n",
    "Epoch #234: Loss:1.0462, Accuracy:0.4374, Validation Loss:1.1027, Validation Accuracy:0.3810\n",
    "Epoch #235: Loss:1.0448, Accuracy:0.4386, Validation Loss:1.1020, Validation Accuracy:0.3941\n",
    "Epoch #236: Loss:1.0462, Accuracy:0.4226, Validation Loss:1.0963, Validation Accuracy:0.3941\n",
    "Epoch #237: Loss:1.0442, Accuracy:0.4366, Validation Loss:1.1039, Validation Accuracy:0.3941\n",
    "Epoch #238: Loss:1.0428, Accuracy:0.4407, Validation Loss:1.1003, Validation Accuracy:0.3941\n",
    "Epoch #239: Loss:1.0416, Accuracy:0.4431, Validation Loss:1.1092, Validation Accuracy:0.3941\n",
    "Epoch #240: Loss:1.0417, Accuracy:0.4452, Validation Loss:1.1024, Validation Accuracy:0.3941\n",
    "Epoch #241: Loss:1.0428, Accuracy:0.4456, Validation Loss:1.1074, Validation Accuracy:0.3875\n",
    "Epoch #242: Loss:1.0447, Accuracy:0.4366, Validation Loss:1.1025, Validation Accuracy:0.3810\n",
    "Epoch #243: Loss:1.0446, Accuracy:0.4423, Validation Loss:1.0986, Validation Accuracy:0.3842\n",
    "Epoch #244: Loss:1.0430, Accuracy:0.4423, Validation Loss:1.1021, Validation Accuracy:0.3711\n",
    "Epoch #245: Loss:1.0450, Accuracy:0.4378, Validation Loss:1.0985, Validation Accuracy:0.3859\n",
    "Epoch #246: Loss:1.0444, Accuracy:0.4279, Validation Loss:1.1012, Validation Accuracy:0.3924\n",
    "Epoch #247: Loss:1.0429, Accuracy:0.4209, Validation Loss:1.1068, Validation Accuracy:0.3892\n",
    "Epoch #248: Loss:1.0405, Accuracy:0.4366, Validation Loss:1.1060, Validation Accuracy:0.3793\n",
    "Epoch #249: Loss:1.0403, Accuracy:0.4361, Validation Loss:1.1127, Validation Accuracy:0.3744\n",
    "Epoch #250: Loss:1.0401, Accuracy:0.4361, Validation Loss:1.1119, Validation Accuracy:0.3793\n",
    "Epoch #251: Loss:1.0360, Accuracy:0.4448, Validation Loss:1.1052, Validation Accuracy:0.3727\n",
    "Epoch #252: Loss:1.0378, Accuracy:0.4407, Validation Loss:1.1059, Validation Accuracy:0.3842\n",
    "Epoch #253: Loss:1.0370, Accuracy:0.4431, Validation Loss:1.1127, Validation Accuracy:0.3695\n",
    "Epoch #254: Loss:1.0352, Accuracy:0.4444, Validation Loss:1.1173, Validation Accuracy:0.3875\n",
    "Epoch #255: Loss:1.0365, Accuracy:0.4320, Validation Loss:1.1215, Validation Accuracy:0.3859\n",
    "Epoch #256: Loss:1.0380, Accuracy:0.4329, Validation Loss:1.1222, Validation Accuracy:0.3727\n",
    "Epoch #257: Loss:1.0374, Accuracy:0.4337, Validation Loss:1.1159, Validation Accuracy:0.3645\n",
    "Epoch #258: Loss:1.0411, Accuracy:0.4320, Validation Loss:1.1167, Validation Accuracy:0.3760\n",
    "Epoch #259: Loss:1.0433, Accuracy:0.4242, Validation Loss:1.1305, Validation Accuracy:0.3727\n",
    "Epoch #260: Loss:1.0880, Accuracy:0.3704, Validation Loss:1.1395, Validation Accuracy:0.3300\n",
    "Epoch #261: Loss:1.0704, Accuracy:0.4090, Validation Loss:1.1199, Validation Accuracy:0.3727\n",
    "Epoch #262: Loss:1.0725, Accuracy:0.4209, Validation Loss:1.0988, Validation Accuracy:0.3875\n",
    "Epoch #263: Loss:1.0716, Accuracy:0.4004, Validation Loss:1.0778, Validation Accuracy:0.3777\n",
    "Epoch #264: Loss:1.0671, Accuracy:0.4131, Validation Loss:1.0777, Validation Accuracy:0.3678\n",
    "Epoch #265: Loss:1.0684, Accuracy:0.4086, Validation Loss:1.0772, Validation Accuracy:0.3842\n",
    "Epoch #266: Loss:1.0662, Accuracy:0.4119, Validation Loss:1.0785, Validation Accuracy:0.4007\n",
    "Epoch #267: Loss:1.0657, Accuracy:0.4099, Validation Loss:1.0820, Validation Accuracy:0.4023\n",
    "Epoch #268: Loss:1.0625, Accuracy:0.4222, Validation Loss:1.0875, Validation Accuracy:0.3957\n",
    "Epoch #269: Loss:1.0625, Accuracy:0.4201, Validation Loss:1.0920, Validation Accuracy:0.4023\n",
    "Epoch #270: Loss:1.0624, Accuracy:0.4234, Validation Loss:1.0919, Validation Accuracy:0.3957\n",
    "Epoch #271: Loss:1.0625, Accuracy:0.4209, Validation Loss:1.0861, Validation Accuracy:0.3924\n",
    "Epoch #272: Loss:1.0607, Accuracy:0.4230, Validation Loss:1.0852, Validation Accuracy:0.3941\n",
    "Epoch #273: Loss:1.0618, Accuracy:0.4259, Validation Loss:1.0842, Validation Accuracy:0.3990\n",
    "Epoch #274: Loss:1.0603, Accuracy:0.4214, Validation Loss:1.0839, Validation Accuracy:0.3892\n",
    "Epoch #275: Loss:1.0594, Accuracy:0.4283, Validation Loss:1.0868, Validation Accuracy:0.3892\n",
    "Epoch #276: Loss:1.0597, Accuracy:0.4222, Validation Loss:1.0893, Validation Accuracy:0.3990\n",
    "Epoch #277: Loss:1.0589, Accuracy:0.4222, Validation Loss:1.0867, Validation Accuracy:0.4023\n",
    "Epoch #278: Loss:1.0590, Accuracy:0.4209, Validation Loss:1.0886, Validation Accuracy:0.3957\n",
    "Epoch #279: Loss:1.0582, Accuracy:0.4292, Validation Loss:1.0863, Validation Accuracy:0.3892\n",
    "Epoch #280: Loss:1.0571, Accuracy:0.4300, Validation Loss:1.0914, Validation Accuracy:0.4039\n",
    "Epoch #281: Loss:1.0581, Accuracy:0.4251, Validation Loss:1.0901, Validation Accuracy:0.3875\n",
    "Epoch #282: Loss:1.0568, Accuracy:0.4320, Validation Loss:1.0896, Validation Accuracy:0.4056\n",
    "Epoch #283: Loss:1.0558, Accuracy:0.4222, Validation Loss:1.0907, Validation Accuracy:0.3990\n",
    "Epoch #284: Loss:1.0550, Accuracy:0.4263, Validation Loss:1.0955, Validation Accuracy:0.3974\n",
    "Epoch #285: Loss:1.0545, Accuracy:0.4312, Validation Loss:1.0980, Validation Accuracy:0.3990\n",
    "Epoch #286: Loss:1.0524, Accuracy:0.4333, Validation Loss:1.0968, Validation Accuracy:0.3990\n",
    "Epoch #287: Loss:1.0522, Accuracy:0.4349, Validation Loss:1.0971, Validation Accuracy:0.3908\n",
    "Epoch #288: Loss:1.0520, Accuracy:0.4296, Validation Loss:1.0947, Validation Accuracy:0.3859\n",
    "Epoch #289: Loss:1.0510, Accuracy:0.4279, Validation Loss:1.0989, Validation Accuracy:0.4056\n",
    "Epoch #290: Loss:1.0518, Accuracy:0.4333, Validation Loss:1.0987, Validation Accuracy:0.3941\n",
    "Epoch #291: Loss:1.0509, Accuracy:0.4238, Validation Loss:1.0992, Validation Accuracy:0.3908\n",
    "Epoch #292: Loss:1.0496, Accuracy:0.4296, Validation Loss:1.0967, Validation Accuracy:0.3810\n",
    "Epoch #293: Loss:1.0468, Accuracy:0.4427, Validation Loss:1.0876, Validation Accuracy:0.3924\n",
    "Epoch #294: Loss:1.0467, Accuracy:0.4382, Validation Loss:1.0872, Validation Accuracy:0.3974\n",
    "Epoch #295: Loss:1.0456, Accuracy:0.4287, Validation Loss:1.0920, Validation Accuracy:0.3924\n",
    "Epoch #296: Loss:1.0446, Accuracy:0.4251, Validation Loss:1.1002, Validation Accuracy:0.3957\n",
    "Epoch #297: Loss:1.0453, Accuracy:0.4324, Validation Loss:1.1099, Validation Accuracy:0.3875\n",
    "Epoch #298: Loss:1.0415, Accuracy:0.4349, Validation Loss:1.1010, Validation Accuracy:0.3842\n",
    "Epoch #299: Loss:1.0420, Accuracy:0.4460, Validation Loss:1.1126, Validation Accuracy:0.3908\n",
    "Epoch #300: Loss:1.0430, Accuracy:0.4394, Validation Loss:1.1028, Validation Accuracy:0.3842\n",
    "\n",
    "Test:\n",
    "Test Loss:1.10282707, Accuracy:0.3842\n",
    "Labels: ['01', '03', '02']\n",
    "Confusion Matrix:\n",
    "       01  03  02\n",
    "t:01  195   0  45\n",
    "t:03  125   0  17\n",
    "t:02  188   0  39\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.38      0.81      0.52       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "          02       0.39      0.17      0.24       227\n",
    "\n",
    "    accuracy                           0.38       609\n",
    "   macro avg       0.26      0.33      0.25       609\n",
    "weighted avg       0.30      0.38      0.29       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 13:46:08 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 40 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0919248650618179, 1.0788520083247344, 1.0746091962447895, 1.074866767587333, 1.0748279405932122, 1.0746654689018362, 1.073956509724822, 1.0737015269268517, 1.0741096905299596, 1.0735699515820332, 1.0737596532981384, 1.0740921924071163, 1.0739936235503023, 1.0741619533310187, 1.0749673553679768, 1.0748458427357164, 1.0747074661975229, 1.0749375957182083, 1.0745918412122428, 1.0736996058760018, 1.0739366912293709, 1.0742521481756702, 1.0734685553705752, 1.073816399268916, 1.0733371160691987, 1.0727927154312384, 1.072785174513881, 1.073192947212307, 1.0739137116324138, 1.0740690064939176, 1.0731318157490446, 1.0742374421731984, 1.0733683765032414, 1.0744380952885193, 1.0738788792260958, 1.0734237921844758, 1.072992770934144, 1.0729353079459154, 1.074496591228178, 1.0754218428397218, 1.0762936657872693, 1.0743877096912153, 1.0763633924556288, 1.0767060917586528, 1.0756651711189884, 1.0773078790439174, 1.0737606298747322, 1.0800700633983893, 1.0740480753784305, 1.0743326716039372, 1.0731122803022513, 1.07297715901937, 1.0767256688993356, 1.074511253383555, 1.0859174740138313, 1.076646587140063, 1.0791209276478082, 1.0788643170264358, 1.0797014614239897, 1.078265642297679, 1.0786292433542963, 1.0762014974318506, 1.0737475818405402, 1.0749642547519727, 1.074186799169957, 1.0742618581540087, 1.074248663701839, 1.0742898104813299, 1.0722179305181518, 1.0737885365932447, 1.0746536096328585, 1.0752835784639632, 1.0747963871274675, 1.0747881777376573, 1.0740883698800123, 1.0746079213513529, 1.0732333444805175, 1.0741523817450738, 1.0735596402721062, 1.0735058864740707, 1.073629115406907, 1.0725587160129266, 1.0731285164508912, 1.073158680511813, 1.0716198081844936, 1.0716220578928104, 1.071276941910166, 1.0702454245149209, 1.0719840070492723, 1.0732822073700001, 1.0719504260468757, 1.0732654059070281, 1.0734585231943867, 1.0772538543334735, 1.0775618915291647, 1.0771221985370654, 1.075750864981039, 1.0776354626481757, 1.0773005301533465, 1.0784100133601473, 1.0802027916869115, 1.0789178218356104, 1.0791977661583811, 1.0793615875181501, 1.0790822353269078, 1.079177092448831, 1.0804905159328566, 1.080678500565402, 1.0805799056743752, 1.0810440085791602, 1.08104621404889, 1.0812512052861731, 1.0826926519130837, 1.0809177473456597, 1.0799930496951826, 1.08127135831147, 1.082010162287745, 1.0863026756371184, 1.086740831240449, 1.0863385838632318, 1.0858361721038818, 1.0851346506861044, 1.0848844153148982, 1.0864034617084197, 1.084957446370806, 1.0864055880967816, 1.0851497513124313, 1.0867061444691248, 1.0859032460229934, 1.0884239379995562, 1.0940671726596376, 1.0940429512503111, 1.0900553276973406, 1.0891343408030243, 1.087154214801068, 1.087392591882026, 1.0890819096604396, 1.0923369249882564, 1.0904234268003692, 1.0930455529631065, 1.0871735337528297, 1.090462404714625, 1.0878867999281985, 1.0904065752264314, 1.088698621258164, 1.0897721993707867, 1.0880319093444273, 1.089610989262122, 1.0888645331847844, 1.0903777389103555, 1.0896732251240897, 1.0901322934427873, 1.0910425538499955, 1.0916680731796866, 1.0957638082050143, 1.0945157515395842, 1.0939359813683922, 1.0949941472271196, 1.0936353511998218, 1.08871086047005, 1.089801121815085, 1.0971258870878047, 1.0982714298323457, 1.1002506467900643, 1.1016188033891625, 1.101443515231065, 1.0957497556025564, 1.0922712346015893, 1.0949567803021134, 1.0903986263744936, 1.095546323677589, 1.0986241887159927, 1.0970134083273375, 1.0960636043000496, 1.0875946269638237, 1.084428076673611, 1.0873985721168455, 1.085174541168025, 1.0946855282744359, 1.0940993932276133, 1.091811541070296, 1.0938934289371633, 1.087856402342347, 1.0930011603241092, 1.093830084370079, 1.0943189084236258, 1.0871225012151282, 1.0925532050907905, 1.0987625938330965, 1.0968484303046917, 1.0966272013528007, 1.0911141522412229, 1.0924602241938925, 1.0935934572580022, 1.0920904085945418, 1.0949087937672932, 1.0952096386691816, 1.096888166930288, 1.0971344360968553, 1.0943081531618617, 1.0956753717463201, 1.0973632108597529, 1.0971462027779941, 1.1000504665969824, 1.1005555780846106, 1.103726121983896, 1.100073920682146, 1.1007865420703231, 1.0998536369874952, 1.0999577763828345, 1.0982393531376504, 1.1005624875255016, 1.0988122962770008, 1.1031120007457014, 1.1012872999720582, 1.1035996456255859, 1.1089714765548706, 1.1052144053338588, 1.1069133520517835, 1.1083716220652138, 1.1046849553808202, 1.1083451779605134, 1.1059083862257708, 1.1110879266986315, 1.111867544099028, 1.1104436842678802, 1.1031459055119155, 1.0954421100945309, 1.1005667724045627, 1.103002833419637, 1.1071521184714557, 1.111684872207579, 1.1063530875739989, 1.1026902723390677, 1.1019909101949732, 1.0963450783774966, 1.103883093996784, 1.1002962011813335, 1.1092177674492396, 1.1023694785749187, 1.1074169488571743, 1.102501128498948, 1.0985960451448689, 1.1021387667100027, 1.0984757351757857, 1.1011647156306676, 1.1068145165889722, 1.1060101537673148, 1.112685701138476, 1.1118616366817056, 1.1052251145953225, 1.1059269364831483, 1.1127109874058239, 1.1172698219421462, 1.1214547677971851, 1.1222325512536837, 1.1158573535471323, 1.116661893714629, 1.130511940797953, 1.1395490692166859, 1.1198940737102614, 1.0987559093043135, 1.0777876316424466, 1.0776889363337425, 1.0771640779936842, 1.0784815696659933, 1.0819754745376913, 1.0875197071551495, 1.0920014814007262, 1.0918629046144157, 1.0860697839451932, 1.08524268033665, 1.0842315310915116, 1.0838831594620628, 1.0867560158417926, 1.089259283687485, 1.0867041337666252, 1.0886166172074567, 1.0862680215553697, 1.091403383731059, 1.0901029436850587, 1.0896138248380964, 1.0907043980064455, 1.0954545403544735, 1.0980499681385083, 1.0968291861278865, 1.0970777192922256, 1.0946674215774035, 1.0989448376281312, 1.098672369626551, 1.0992329795959548, 1.0967369884105738, 1.0875810989605381, 1.0872407431281454, 1.0920131014681411, 1.1002371201570007, 1.1098615757154517, 1.1009688986150306, 1.1126093574736897, 1.1028270300581733], 'val_acc': [0.3908045968692291, 0.3924466335812617, 0.38587848820122594, 0.39080459608624524, 0.39080459608624524, 0.3924466327982779, 0.3875205243260207, 0.39737274136840806, 0.4006568138137435, 0.39737274136840806, 0.39244663289615084, 0.4039408855739681, 0.3908045965756102, 0.3957307049499944, 0.3842364523700501, 0.3924466329940238, 0.37602627145245743, 0.3875205246196396, 0.3875205247175126, 0.38916256064656135, 0.38587848859271784, 0.3825944159516364, 0.3940886690209456, 0.39080459696710207, 0.3940886695103105, 0.4006568138137435, 0.403940886063333, 0.3973727417599, 0.3891625610380533, 0.39080459706497506, 0.38916256054868836, 0.39737274136840806, 0.3924466333855158, 0.39080459716284804, 0.3924466331897698, 0.38752052481538557, 0.3825944161473824, 0.3957307051457404, 0.3924466329940238, 0.403940886161206, 0.4121510667851797, 0.39080459716284804, 0.3924466331897698, 0.3875205247175126, 0.38587848839697186, 0.4006568139116165, 0.3891625608423073, 0.39408866941243753, 0.40229884993853826, 0.39901477778682176, 0.39408866941243753, 0.39408866941243753, 0.3891625610380533, 0.3990147776889488, 0.3875205241302747, 0.4006568134222516, 0.39573070446062947, 0.3842364514891933, 0.3940886686294537, 0.3990147771017109, 0.4006568134222516, 0.41215106717667166, 0.39573070475424843, 0.3957307049499944, 0.3957307051457404, 0.39408866882519966, 0.38916256025506946, 0.39244663260253193, 0.4137931021269906, 0.3940886683358347, 0.3973727405854242, 0.3825944159516364, 0.4072249579214306, 0.3973727406832972, 0.3973727406832972, 0.39080459726072103, 0.3957307052436133, 0.3973727406832972, 0.3957307052436133, 0.37931034409353886, 0.39573070455850246, 0.39737274097691616, 0.3990147774932028, 0.40065681312863266, 0.3990147776889488, 0.40394088596546, 0.40558292199238183, 0.39080459696710207, 0.3875205245217666, 0.3908045967713561, 0.38587848820122594, 0.4072249579214306, 0.3908045968692291, 0.38587848839697186, 0.4022988493513004, 0.3875205246196396, 0.37766830787087113, 0.3891625608423073, 0.38752052501113154, 0.384236452859415, 0.37766830787087113, 0.37602627135458444, 0.3776683070878873, 0.36945812714902443, 0.3727421994964869, 0.38587848849484485, 0.3891625609401803, 0.3875205243260207, 0.38423645178281224, 0.3908045967713561, 0.38095238002258763, 0.3875205245217666, 0.38259441644100134, 0.385878487711861, 0.38752052481538557, 0.3760262716482034, 0.38259441634312835, 0.38423645217430413, 0.39080459696710207, 0.3875205245217666, 0.38587848839697186, 0.3793103438977929, 0.3842364520764312, 0.38095237982684166, 0.3908045961841182, 0.3825944160495094, 0.3842364520764312, 0.3842364520764312, 0.3908045964777372, 0.3825944160495094, 0.38752052364090983, 0.3842364525657961, 0.37931034311480905, 0.38587848849484485, 0.3924466327004049, 0.38587848859271784, 0.38095237953322275, 0.35960591020451, 0.3628899822583535, 0.3760262717460764, 0.3776683069900143, 0.38095238002258763, 0.3924466327004049, 0.38916256064656135, 0.38916256054868836, 0.37766830777299815, 0.3908045965756102, 0.37438423552340866, 0.3957307051457404, 0.38423645217430413, 0.3875205245217666, 0.39244663289615084, 0.38587848839697186, 0.3776683071857603, 0.3858784882990989, 0.3957307052436133, 0.3842364523700501, 0.3842364513913203, 0.38423645217430413, 0.3825944161473824, 0.3842364523700501, 0.3842364514891933, 0.3908045968692291, 0.3809523802183336, 0.37931034321268203, 0.38587848859271784, 0.38916256035294244, 0.39080459667348316, 0.3776683079687441, 0.3891625598635775, 0.385878487809734, 0.37766830757725217, 0.3809523792396038, 0.3793103437020469, 0.3825944156580175, 0.3809523797289687, 0.3727421995943599, 0.3825944159516364, 0.3727421993986139, 0.3760262716482034, 0.3776683069900143, 0.3760262718439494, 0.3842364522721771, 0.3694581273447704, 0.38259441536439853, 0.37438423552340866, 0.3809523801204606, 0.3842364520764312, 0.3825944160495094, 0.3842364514891933, 0.38587848800547997, 0.38259441585376347, 0.3793103438977929, 0.3809523801204606, 0.3793103438977929, 0.3809523801204606, 0.3842364519785582, 0.38259441536439853, 0.37110016337169216, 0.3612479465250507, 0.36288998255197247, 0.36617405499730793, 0.3645320188725132, 0.36617405489943494, 0.3612479466229237, 0.3563218378570475, 0.36781609063273774, 0.37274219930074093, 0.37274219910499495, 0.36781609063273774, 0.38095238002258763, 0.3678160910242297, 0.3678160905348648, 0.36124794623143175, 0.36781609063273774, 0.3628899824540995, 0.3678160907306107, 0.3711001630780732, 0.37274219910499495, 0.36288998264984546, 0.36945812714902443, 0.36617405480156195, 0.3743842352297897, 0.35139572967840926, 0.372742199007122, 0.3628899821604805, 0.38587848810335296, 0.3661740550951809, 0.37274219930074093, 0.3760262717460764, 0.37438423552340866, 0.3711001630780732, 0.3875205241302747, 0.38095238002258763, 0.39408866892307265, 0.39408866882519966, 0.39408866911881857, 0.39408866892307265, 0.39408866911881857, 0.39408866921669156, 0.3875205245217666, 0.38095237982684166, 0.3842364522721771, 0.37110016337169216, 0.38587848810335296, 0.3924466329940238, 0.38916256025506946, 0.3793103438977929, 0.3743842352297897, 0.3793103437999199, 0.3727421994964869, 0.38423645217430413, 0.3694581272468974, 0.3875205241302747, 0.385878487711861, 0.37274219861563007, 0.3645320187746402, 0.3760262715503304, 0.3727421995943599, 0.3300492591752207, 0.3727421993986139, 0.3875205243260207, 0.3776683080666171, 0.3678160908284837, 0.3842364522721771, 0.4006568139116165, 0.4022988503300302, 0.3957307049499944, 0.4022988503300302, 0.3957307013776306, 0.39244663289615084, 0.3940886697060565, 0.3990147780804407, 0.38916256054868836, 0.3891625613316722, 0.3990147774932028, 0.4022988498406653, 0.39573070602659716, 0.38916256054868836, 0.4039408820994773, 0.3875205244238936, 0.4055829223838737, 0.39901477778682176, 0.39737274146628104, 0.39901477778682176, 0.3990147775910758, 0.39080459706497506, 0.3858784887884638, 0.40558292179663585, 0.39408866931456454, 0.3908045967713561, 0.3809523803162066, 0.3924466330918968, 0.3973727417599, 0.3924466327982779, 0.3957307049499944, 0.38752052481538557, 0.3842364522721771, 0.39080459706497506, 0.3842364520764312], 'loss': [1.1077925847541135, 1.0865732478410066, 1.0762922073047019, 1.0737889132215748, 1.0742569132262432, 1.0744239669071332, 1.0744056993930982, 1.073721879218883, 1.073807383905446, 1.0737161011176921, 1.0734071827522293, 1.0734488602291632, 1.0735545626411203, 1.0735440103425138, 1.0738695154444637, 1.073408429627546, 1.0734395016390195, 1.07335348609047, 1.0732851236997443, 1.0734064456128976, 1.072707822729185, 1.072629288383578, 1.072343203859897, 1.0725492640931993, 1.0727370463357568, 1.0733848652555713, 1.0729128530627647, 1.0722878206926694, 1.0728050233891857, 1.072770949115009, 1.07254511536514, 1.0729522714869442, 1.072785204930472, 1.0722582515994625, 1.071944226229705, 1.07189037315165, 1.071446493469959, 1.0720664454681428, 1.0732088929573858, 1.072609784617806, 1.071203174826056, 1.0724028251009556, 1.0719773317019798, 1.0700113388547172, 1.0719877968578613, 1.0708348276678787, 1.0708328248049443, 1.0684388813786438, 1.0723408391588278, 1.0666119676840624, 1.0674262483017156, 1.0654926218291327, 1.0651935262112158, 1.067974948736187, 1.0748442828043285, 1.0713409314654936, 1.0690591127231135, 1.0780988337322917, 1.0758468473716438, 1.0766816086837643, 1.07423132410774, 1.0744269808215037, 1.0729213561365492, 1.0720266019049611, 1.0724254410369685, 1.0716348201587216, 1.071180321401639, 1.0709539655787255, 1.0703268179413719, 1.0698238188236402, 1.0713798029466821, 1.0725921866830124, 1.0729277816396474, 1.0728355274552928, 1.0723058508651702, 1.0712823193176082, 1.0701136880831552, 1.069569673871113, 1.0711817691458325, 1.0714676822725018, 1.0715940460287325, 1.0701825514955932, 1.069926179899572, 1.0701233341463783, 1.070031819500228, 1.0694309374145414, 1.0681974800705176, 1.0674748968539542, 1.0673425824245633, 1.0674930183304898, 1.0655631234758445, 1.066121141279013, 1.0661916867418701, 1.0657138368677064, 1.0663869707980929, 1.067478495163105, 1.0659423120702316, 1.0656248719295682, 1.0682609398017429, 1.070587771873944, 1.073732637379938, 1.069282352263433, 1.068369482625926, 1.068561242738054, 1.0677326428082445, 1.0680951208549359, 1.0676883293617923, 1.0666559408822343, 1.0676542649768461, 1.06683081395817, 1.0662493340533372, 1.0658107076582233, 1.064388997354057, 1.0667903963300482, 1.0653147522673714, 1.0652421918493031, 1.0649742509305355, 1.0638941007962706, 1.066187346397729, 1.0644441495930634, 1.0645049379102014, 1.0642106055723812, 1.0638040881871687, 1.0638175017045508, 1.0629635972898355, 1.0616836275652939, 1.0613565316680031, 1.0608771123435707, 1.0599564982145964, 1.0607963811200747, 1.0608781126245581, 1.063880639790999, 1.0654490566351575, 1.0636166194136383, 1.0614006694092644, 1.063940082879037, 1.0646301988703515, 1.0628158448902734, 1.0644220188657851, 1.063047790625257, 1.0618697238409054, 1.062574637205449, 1.0622264386692086, 1.058877399080343, 1.0602284340887833, 1.0587761980796986, 1.0622140876076793, 1.0602172464805462, 1.061131855937245, 1.0591897532190875, 1.0591596922100937, 1.0589123025811917, 1.0620854940257767, 1.0607553153556966, 1.0607484993748597, 1.0594347805947493, 1.0591985209521817, 1.0560009039402987, 1.057566719035593, 1.0550061039366516, 1.0554624945231288, 1.0544514823498423, 1.0543376742936748, 1.0529145418985668, 1.0543742662582554, 1.057773693190463, 1.055516368652516, 1.0548369890855323, 1.0589367746572476, 1.056932765698286, 1.060800122480373, 1.0589242166562247, 1.055999735101782, 1.0587167216032682, 1.0577096629191718, 1.0582686974037845, 1.0573205158206227, 1.0569138647349707, 1.052791330505935, 1.0555528408692847, 1.0546049670761861, 1.0506124573321802, 1.04942571583225, 1.050201812025458, 1.050948396992145, 1.0497669608196438, 1.0506460972145597, 1.0504197590160174, 1.051694295930177, 1.0528984804173025, 1.0552265951520854, 1.0536971947985263, 1.0544169569407156, 1.0533118058523847, 1.0549547468612326, 1.0566178220498244, 1.0577161874124892, 1.0583555952969028, 1.0557929071312813, 1.0577207247089802, 1.0577379499862327, 1.0558265627776817, 1.0540165703889035, 1.0538191492797413, 1.0544841948475925, 1.0556207059100424, 1.0550754641605353, 1.0524308263398783, 1.0523730874795933, 1.0519829092574071, 1.0536603542323963, 1.056300382839336, 1.052531373916955, 1.0517100044834051, 1.0515720957358514, 1.0516600509443812, 1.051501656312962, 1.051435516306507, 1.0494356007546615, 1.0478796774846573, 1.0523864665804947, 1.049945108552733, 1.0480574010578758, 1.046136370625584, 1.0488863127921886, 1.0489248869600236, 1.0466433584812485, 1.0449472971275846, 1.044569621683391, 1.0439552839287007, 1.0429344937052325, 1.044186018575633, 1.0461177704025832, 1.0462053213276168, 1.0447538136456782, 1.0462228948820298, 1.0441791369929696, 1.042836622095206, 1.0415976456792937, 1.0417196010417273, 1.0427584857666516, 1.0446932187560158, 1.0446496963011411, 1.042981797517937, 1.0449521038321743, 1.04439761643537, 1.0429173400514669, 1.0404931178572732, 1.040340404784655, 1.040097070621514, 1.0360220128261088, 1.037845591842761, 1.0369692100881307, 1.0352166740311735, 1.0365457263080982, 1.0379559433190975, 1.0373758147139813, 1.0411218199152232, 1.0433459444946822, 1.087974076545214, 1.0704347900786195, 1.072481262561477, 1.0716211136361657, 1.0671075179101994, 1.0683529669254468, 1.0662085851359906, 1.0657227503445603, 1.0625271776128844, 1.0625362656689277, 1.0623872750593653, 1.0624589065261936, 1.0606857841754107, 1.0617872528471741, 1.0602560316512717, 1.0593708389348808, 1.0597227836291647, 1.0588820492707238, 1.059045444523774, 1.0581786065130998, 1.0570598889180522, 1.0580912990002171, 1.0567890352292226, 1.055761049464498, 1.0550047120029677, 1.054532020243776, 1.0524386438256172, 1.0522392299385777, 1.0520271352184383, 1.0510139668012302, 1.051771315314197, 1.0508934186469359, 1.0496137244500663, 1.046808421195655, 1.046728048481246, 1.0456486476765032, 1.044552672717116, 1.0452589574046203, 1.0415015295056103, 1.0419802100751434, 1.0430428228828696], 'acc': [0.2862423001618356, 0.3885010251029561, 0.39712525762816475, 0.3946611928988776, 0.39589322199566895, 0.3963038992220861, 0.39466119293559504, 0.395893222583148, 0.40041067778941786, 0.39260780382939675, 0.3926078016385895, 0.39999999880056364, 0.3950718669553557, 0.3975359344262117, 0.39548254437759917, 0.39507186797120486, 0.3930184818391193, 0.39630390297950413, 0.4008213530208541, 0.4020533882616971, 0.40205338920411143, 0.39917864591434016, 0.3979466109060409, 0.40082135497911753, 0.40041067638191596, 0.39466119309470393, 0.40041067618608966, 0.3991786427444012, 0.39260780144276314, 0.390965094727902, 0.4028747413070295, 0.39958932052157986, 0.39712525606155397, 0.3975359348545819, 0.39958932353241, 0.39466118992476495, 0.39712525661231557, 0.39794661204428156, 0.394661189178177, 0.3987679667296596, 0.39671457781928765, 0.39342915808640466, 0.39342915965301545, 0.3991786461101665, 0.4057494873750871, 0.3934291598488418, 0.3897330585446446, 0.3946611925072249, 0.3999999985680198, 0.39917864708929823, 0.40000000095465343, 0.40041067638191596, 0.39958932294493094, 0.40041067896437593, 0.3991786429402275, 0.4012320334172102, 0.4123203267184616, 0.3971252589989492, 0.39055441710983213, 0.3913757680377921, 0.4061601662048324, 0.3950718681670312, 0.4151950712199084, 0.39630390376280955, 0.395071869733642, 0.39835728750826155, 0.4069815192501648, 0.40492813315479664, 0.3983572908740269, 0.3979466108693234, 0.3967145770359823, 0.3934291573030993, 0.3864476385792178, 0.3991786461101665, 0.39548254500179564, 0.3913757705835346, 0.4012320343963419, 0.39383983550864815, 0.39260780144276314, 0.3917864469899289, 0.3839835736541043, 0.40041067540278424, 0.40328541994094846, 0.40657084303959684, 0.4098562649632871, 0.40492812939737854, 0.40369609892980274, 0.4061601658131797, 0.3938398378585643, 0.4053388084229503, 0.40410677831030967, 0.4041067775270043, 0.3999999985680198, 0.40328542287834374, 0.39917864591434016, 0.3967145784067667, 0.40616016561735335, 0.40533880936536454, 0.40369609990893446, 0.4106776158545296, 0.39630390180454605, 0.4114989740647819, 0.40780287530632725, 0.406160165421527, 0.4156057500496537, 0.4078028725280409, 0.41232032828507237, 0.4139630369949145, 0.40410677478543544, 0.4164271036824651, 0.4156057512246118, 0.4114989716781483, 0.41683778130053495, 0.4073921986673892, 0.41437371797874967, 0.41519507341071565, 0.4069815206209492, 0.4176591369283273, 0.4082135535118761, 0.4123203284808987, 0.40698151948270855, 0.41355235957267106, 0.41642710583655496, 0.41560574785884646, 0.4069815192868822, 0.4114989742606083, 0.4188911709574948, 0.4168377820838403, 0.41683778169218766, 0.412731006135686, 0.4205338804873598, 0.4127310044956403, 0.4069815188585121, 0.4127310066864476, 0.4041067771353516, 0.4094455865619119, 0.4045174549492478, 0.41108829664253843, 0.40985626320084995, 0.41149897246145367, 0.4119096520745044, 0.4032854220950383, 0.41067761980777406, 0.41108829546758036, 0.416837784470474, 0.41437371559211605, 0.41149897523974, 0.4094455859377154, 0.41273100570731586, 0.41067761941612135, 0.41560575181209086, 0.42094456065117214, 0.4127310075064704, 0.4209445572486893, 0.41190965050789363, 0.4082135501461107, 0.4176591399024399, 0.4242299811673605, 0.4209445604553458, 0.4229979457674085, 0.42094455689375404, 0.42340862256545553, 0.4197125267811135, 0.4238193027659853, 0.4147843928185332, 0.41232033086753234, 0.4127310059031422, 0.4217659148714626, 0.41519507141573475, 0.40369610030058717, 0.4176591359491955, 0.41601642786354986, 0.41560575181209086, 0.4188911713491475, 0.42135523490347654, 0.41478439262270683, 0.4312114981655223, 0.4254620144499401, 0.43244353098301425, 0.42094455630627503, 0.4221765923304235, 0.41930184939558746, 0.42628336949025336, 0.4328542083685403, 0.42997946723285885, 0.42751539937035016, 0.4312114969171293, 0.42628336867023053, 0.4209445606144547, 0.4279260799625326, 0.42340862397295737, 0.4316221743760902, 0.4197125266220046, 0.4349075957123014, 0.4295687874606992, 0.4238193015910272, 0.4246406566313405, 0.41724845872277844, 0.4271047241021965, 0.41889116997836306, 0.42464065682716684, 0.4225872707317987, 0.4254620110841747, 0.42258726795351237, 0.42628336569611786, 0.42176591154241466, 0.420123204790836, 0.41355236329337164, 0.42258726795351237, 0.4225872689326441, 0.42710472312306474, 0.4119096492962181, 0.4176591394740698, 0.4172484595060838, 0.4172484618927174, 0.42751539995782917, 0.4254620126507855, 0.42956879000644177, 0.42956878824400463, 0.42833675382318437, 0.42915811180089286, 0.42956878902731, 0.42628336749527246, 0.42874743242038593, 0.4258726908563344, 0.42792607738007266, 0.438603697212325, 0.4402464074887779, 0.43572895251505184, 0.4299794639038109, 0.430800823680674, 0.4303901458667779, 0.4349075968872595, 0.4373716629873311, 0.43860369407910343, 0.42258727092762505, 0.4365503069678861, 0.440657084127716, 0.4431211504236139, 0.4451745389056157, 0.4455852173069909, 0.4365503099419987, 0.44229979616660603, 0.4422997944041689, 0.4377823427962082, 0.4279260771842463, 0.42094455630627503, 0.43655030833867053, 0.43613962974146897, 0.4361396287623372, 0.4447638583134332, 0.440657084127716, 0.443121152186051, 0.44435318327782336, 0.4320328547357289, 0.4328542071935822, 0.43367556677461894, 0.4320328561432308, 0.4242299815957306, 0.37043120951623154, 0.40903490694886113, 0.42094455650210133, 0.4004106759902633, 0.41314168273790663, 0.40862422893913863, 0.411909652037787, 0.4098562614016954, 0.42217659151040066, 0.42012320521920615, 0.42340862655541733, 0.4209445598678667, 0.4229979483498685, 0.4258726877231128, 0.42135523768176286, 0.42833675480231614, 0.42217658955213716, 0.42217658955213716, 0.42094456065117214, 0.42915811043010843, 0.4299794668412062, 0.4250513350327157, 0.4320328559474044, 0.42217659072709524, 0.426283369294427, 0.43121149989124197, 0.4332648893890929, 0.4349075998246547, 0.42956879078974713, 0.4279260779675517, 0.4332648885690701, 0.42381930139520085, 0.4295687878523519, 0.4427104712389333, 0.4381930166201425, 0.42874743163708057, 0.4250513350327157, 0.4324435301997089, 0.43490759943300206, 0.445995893199341, 0.4394250491194167]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
