{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf26.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 05:53:00 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '0', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['04', '03', '01', '05', '02'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001FD4C36BE48>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001FD43BF6EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 11,561\n",
    "Trainable params: 11,561\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6077, Accuracy:0.2115, Validation Loss:1.6061, Validation Accuracy:0.2332\n",
    "Epoch #2: Loss:1.6050, Accuracy:0.2337, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #3: Loss:1.6042, Accuracy:0.2329, Validation Loss:1.6053, Validation Accuracy:0.2332\n",
    "Epoch #4: Loss:1.6035, Accuracy:0.2329, Validation Loss:1.6050, Validation Accuracy:0.2332\n",
    "Epoch #5: Loss:1.6024, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #6: Loss:1.6021, Accuracy:0.2329, Validation Loss:1.6054, Validation Accuracy:0.2332\n",
    "Epoch #7: Loss:1.6015, Accuracy:0.2349, Validation Loss:1.6057, Validation Accuracy:0.2365\n",
    "Epoch #8: Loss:1.6009, Accuracy:0.2398, Validation Loss:1.6056, Validation Accuracy:0.2282\n",
    "Epoch #9: Loss:1.6006, Accuracy:0.2402, Validation Loss:1.6065, Validation Accuracy:0.2282\n",
    "Epoch #10: Loss:1.6000, Accuracy:0.2497, Validation Loss:1.6090, Validation Accuracy:0.2233\n",
    "Epoch #11: Loss:1.6001, Accuracy:0.2460, Validation Loss:1.6092, Validation Accuracy:0.2266\n",
    "Epoch #12: Loss:1.6030, Accuracy:0.2386, Validation Loss:1.6110, Validation Accuracy:0.2332\n",
    "Epoch #13: Loss:1.6024, Accuracy:0.2320, Validation Loss:1.6075, Validation Accuracy:0.2299\n",
    "Epoch #14: Loss:1.5999, Accuracy:0.2431, Validation Loss:1.6128, Validation Accuracy:0.2003\n",
    "Epoch #15: Loss:1.6017, Accuracy:0.2427, Validation Loss:1.6090, Validation Accuracy:0.2299\n",
    "Epoch #16: Loss:1.6003, Accuracy:0.2402, Validation Loss:1.6070, Validation Accuracy:0.2365\n",
    "Epoch #17: Loss:1.5997, Accuracy:0.2394, Validation Loss:1.6100, Validation Accuracy:0.2250\n",
    "Epoch #18: Loss:1.5990, Accuracy:0.2444, Validation Loss:1.6091, Validation Accuracy:0.2299\n",
    "Epoch #19: Loss:1.5999, Accuracy:0.2444, Validation Loss:1.6082, Validation Accuracy:0.2266\n",
    "Epoch #20: Loss:1.5991, Accuracy:0.2468, Validation Loss:1.6101, Validation Accuracy:0.2184\n",
    "Epoch #21: Loss:1.5987, Accuracy:0.2444, Validation Loss:1.6094, Validation Accuracy:0.2266\n",
    "Epoch #22: Loss:1.5986, Accuracy:0.2435, Validation Loss:1.6099, Validation Accuracy:0.2233\n",
    "Epoch #23: Loss:1.5988, Accuracy:0.2452, Validation Loss:1.6101, Validation Accuracy:0.2250\n",
    "Epoch #24: Loss:1.5978, Accuracy:0.2431, Validation Loss:1.6105, Validation Accuracy:0.2217\n",
    "Epoch #25: Loss:1.5978, Accuracy:0.2448, Validation Loss:1.6110, Validation Accuracy:0.2151\n",
    "Epoch #26: Loss:1.5984, Accuracy:0.2456, Validation Loss:1.6117, Validation Accuracy:0.2151\n",
    "Epoch #27: Loss:1.5992, Accuracy:0.2427, Validation Loss:1.6114, Validation Accuracy:0.2233\n",
    "Epoch #28: Loss:1.5981, Accuracy:0.2468, Validation Loss:1.6113, Validation Accuracy:0.2151\n",
    "Epoch #29: Loss:1.5980, Accuracy:0.2452, Validation Loss:1.6109, Validation Accuracy:0.2200\n",
    "Epoch #30: Loss:1.5977, Accuracy:0.2444, Validation Loss:1.6096, Validation Accuracy:0.2167\n",
    "Epoch #31: Loss:1.5976, Accuracy:0.2452, Validation Loss:1.6089, Validation Accuracy:0.2332\n",
    "Epoch #32: Loss:1.5975, Accuracy:0.2435, Validation Loss:1.6093, Validation Accuracy:0.2233\n",
    "Epoch #33: Loss:1.5977, Accuracy:0.2448, Validation Loss:1.6088, Validation Accuracy:0.2266\n",
    "Epoch #34: Loss:1.5981, Accuracy:0.2439, Validation Loss:1.6087, Validation Accuracy:0.2282\n",
    "Epoch #35: Loss:1.5980, Accuracy:0.2468, Validation Loss:1.6096, Validation Accuracy:0.2315\n",
    "Epoch #36: Loss:1.5978, Accuracy:0.2460, Validation Loss:1.6098, Validation Accuracy:0.2315\n",
    "Epoch #37: Loss:1.5981, Accuracy:0.2427, Validation Loss:1.6107, Validation Accuracy:0.2266\n",
    "Epoch #38: Loss:1.5982, Accuracy:0.2444, Validation Loss:1.6102, Validation Accuracy:0.2299\n",
    "Epoch #39: Loss:1.5978, Accuracy:0.2419, Validation Loss:1.6110, Validation Accuracy:0.2250\n",
    "Epoch #40: Loss:1.5974, Accuracy:0.2423, Validation Loss:1.6108, Validation Accuracy:0.2282\n",
    "Epoch #41: Loss:1.5971, Accuracy:0.2439, Validation Loss:1.6100, Validation Accuracy:0.2250\n",
    "Epoch #42: Loss:1.5968, Accuracy:0.2431, Validation Loss:1.6103, Validation Accuracy:0.2233\n",
    "Epoch #43: Loss:1.5968, Accuracy:0.2444, Validation Loss:1.6103, Validation Accuracy:0.2315\n",
    "Epoch #44: Loss:1.5968, Accuracy:0.2439, Validation Loss:1.6098, Validation Accuracy:0.2315\n",
    "Epoch #45: Loss:1.5968, Accuracy:0.2431, Validation Loss:1.6099, Validation Accuracy:0.2266\n",
    "Epoch #46: Loss:1.5968, Accuracy:0.2444, Validation Loss:1.6093, Validation Accuracy:0.2282\n",
    "Epoch #47: Loss:1.5967, Accuracy:0.2431, Validation Loss:1.6095, Validation Accuracy:0.2250\n",
    "Epoch #48: Loss:1.5968, Accuracy:0.2427, Validation Loss:1.6083, Validation Accuracy:0.2266\n",
    "Epoch #49: Loss:1.5968, Accuracy:0.2448, Validation Loss:1.6085, Validation Accuracy:0.2250\n",
    "Epoch #50: Loss:1.5967, Accuracy:0.2435, Validation Loss:1.6079, Validation Accuracy:0.2282\n",
    "Epoch #51: Loss:1.5964, Accuracy:0.2444, Validation Loss:1.6087, Validation Accuracy:0.2250\n",
    "Epoch #52: Loss:1.5970, Accuracy:0.2435, Validation Loss:1.6092, Validation Accuracy:0.2200\n",
    "Epoch #53: Loss:1.5964, Accuracy:0.2444, Validation Loss:1.6081, Validation Accuracy:0.2332\n",
    "Epoch #54: Loss:1.5966, Accuracy:0.2419, Validation Loss:1.6094, Validation Accuracy:0.2282\n",
    "Epoch #55: Loss:1.5965, Accuracy:0.2431, Validation Loss:1.6104, Validation Accuracy:0.2200\n",
    "Epoch #56: Loss:1.5958, Accuracy:0.2431, Validation Loss:1.6094, Validation Accuracy:0.2266\n",
    "Epoch #57: Loss:1.5957, Accuracy:0.2419, Validation Loss:1.6087, Validation Accuracy:0.2299\n",
    "Epoch #58: Loss:1.5950, Accuracy:0.2411, Validation Loss:1.6100, Validation Accuracy:0.2184\n",
    "Epoch #59: Loss:1.5952, Accuracy:0.2423, Validation Loss:1.6086, Validation Accuracy:0.2315\n",
    "Epoch #60: Loss:1.5949, Accuracy:0.2435, Validation Loss:1.6091, Validation Accuracy:0.2299\n",
    "Epoch #61: Loss:1.5951, Accuracy:0.2423, Validation Loss:1.6094, Validation Accuracy:0.2299\n",
    "Epoch #62: Loss:1.5951, Accuracy:0.2439, Validation Loss:1.6100, Validation Accuracy:0.2282\n",
    "Epoch #63: Loss:1.5956, Accuracy:0.2427, Validation Loss:1.6079, Validation Accuracy:0.2282\n",
    "Epoch #64: Loss:1.5955, Accuracy:0.2415, Validation Loss:1.6114, Validation Accuracy:0.2233\n",
    "Epoch #65: Loss:1.5958, Accuracy:0.2444, Validation Loss:1.6087, Validation Accuracy:0.2299\n",
    "Epoch #66: Loss:1.5950, Accuracy:0.2402, Validation Loss:1.6074, Validation Accuracy:0.2332\n",
    "Epoch #67: Loss:1.5955, Accuracy:0.2398, Validation Loss:1.6093, Validation Accuracy:0.2233\n",
    "Epoch #68: Loss:1.5960, Accuracy:0.2427, Validation Loss:1.6094, Validation Accuracy:0.2365\n",
    "Epoch #69: Loss:1.5949, Accuracy:0.2472, Validation Loss:1.6067, Validation Accuracy:0.2365\n",
    "Epoch #70: Loss:1.5952, Accuracy:0.2431, Validation Loss:1.6076, Validation Accuracy:0.2348\n",
    "Epoch #71: Loss:1.5984, Accuracy:0.2402, Validation Loss:1.6064, Validation Accuracy:0.2348\n",
    "Epoch #72: Loss:1.6115, Accuracy:0.2296, Validation Loss:1.6055, Validation Accuracy:0.2200\n",
    "Epoch #73: Loss:1.6015, Accuracy:0.2427, Validation Loss:1.6044, Validation Accuracy:0.2315\n",
    "Epoch #74: Loss:1.6004, Accuracy:0.2435, Validation Loss:1.6074, Validation Accuracy:0.2315\n",
    "Epoch #75: Loss:1.5983, Accuracy:0.2456, Validation Loss:1.6055, Validation Accuracy:0.2299\n",
    "Epoch #76: Loss:1.5973, Accuracy:0.2439, Validation Loss:1.6062, Validation Accuracy:0.2299\n",
    "Epoch #77: Loss:1.5959, Accuracy:0.2464, Validation Loss:1.6081, Validation Accuracy:0.2315\n",
    "Epoch #78: Loss:1.5962, Accuracy:0.2423, Validation Loss:1.6093, Validation Accuracy:0.2184\n",
    "Epoch #79: Loss:1.5960, Accuracy:0.2439, Validation Loss:1.6078, Validation Accuracy:0.2332\n",
    "Epoch #80: Loss:1.5958, Accuracy:0.2444, Validation Loss:1.6073, Validation Accuracy:0.2348\n",
    "Epoch #81: Loss:1.5957, Accuracy:0.2435, Validation Loss:1.6077, Validation Accuracy:0.2332\n",
    "Epoch #82: Loss:1.5952, Accuracy:0.2448, Validation Loss:1.6072, Validation Accuracy:0.2332\n",
    "Epoch #83: Loss:1.5952, Accuracy:0.2452, Validation Loss:1.6074, Validation Accuracy:0.2315\n",
    "Epoch #84: Loss:1.5950, Accuracy:0.2444, Validation Loss:1.6071, Validation Accuracy:0.2332\n",
    "Epoch #85: Loss:1.5950, Accuracy:0.2456, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #86: Loss:1.5949, Accuracy:0.2460, Validation Loss:1.6059, Validation Accuracy:0.2282\n",
    "Epoch #87: Loss:1.5948, Accuracy:0.2431, Validation Loss:1.6070, Validation Accuracy:0.2315\n",
    "Epoch #88: Loss:1.5945, Accuracy:0.2452, Validation Loss:1.6073, Validation Accuracy:0.2365\n",
    "Epoch #89: Loss:1.5948, Accuracy:0.2435, Validation Loss:1.6076, Validation Accuracy:0.2365\n",
    "Epoch #90: Loss:1.5941, Accuracy:0.2439, Validation Loss:1.6081, Validation Accuracy:0.2299\n",
    "Epoch #91: Loss:1.5945, Accuracy:0.2427, Validation Loss:1.6097, Validation Accuracy:0.2151\n",
    "Epoch #92: Loss:1.5940, Accuracy:0.2402, Validation Loss:1.6077, Validation Accuracy:0.2151\n",
    "Epoch #93: Loss:1.5950, Accuracy:0.2419, Validation Loss:1.6076, Validation Accuracy:0.2282\n",
    "Epoch #94: Loss:1.5943, Accuracy:0.2460, Validation Loss:1.6088, Validation Accuracy:0.2332\n",
    "Epoch #95: Loss:1.5942, Accuracy:0.2431, Validation Loss:1.6067, Validation Accuracy:0.2315\n",
    "Epoch #96: Loss:1.5935, Accuracy:0.2415, Validation Loss:1.6062, Validation Accuracy:0.2200\n",
    "Epoch #97: Loss:1.5936, Accuracy:0.2402, Validation Loss:1.6071, Validation Accuracy:0.2250\n",
    "Epoch #98: Loss:1.5934, Accuracy:0.2402, Validation Loss:1.6088, Validation Accuracy:0.2348\n",
    "Epoch #99: Loss:1.5935, Accuracy:0.2444, Validation Loss:1.6083, Validation Accuracy:0.2430\n",
    "Epoch #100: Loss:1.5931, Accuracy:0.2435, Validation Loss:1.6075, Validation Accuracy:0.2381\n",
    "Epoch #101: Loss:1.5932, Accuracy:0.2411, Validation Loss:1.6073, Validation Accuracy:0.2299\n",
    "Epoch #102: Loss:1.5931, Accuracy:0.2452, Validation Loss:1.6071, Validation Accuracy:0.2266\n",
    "Epoch #103: Loss:1.5931, Accuracy:0.2419, Validation Loss:1.6088, Validation Accuracy:0.2282\n",
    "Epoch #104: Loss:1.5935, Accuracy:0.2411, Validation Loss:1.6083, Validation Accuracy:0.2332\n",
    "Epoch #105: Loss:1.5932, Accuracy:0.2476, Validation Loss:1.6091, Validation Accuracy:0.2282\n",
    "Epoch #106: Loss:1.5926, Accuracy:0.2419, Validation Loss:1.6074, Validation Accuracy:0.2184\n",
    "Epoch #107: Loss:1.5925, Accuracy:0.2402, Validation Loss:1.6081, Validation Accuracy:0.2332\n",
    "Epoch #108: Loss:1.5924, Accuracy:0.2460, Validation Loss:1.6096, Validation Accuracy:0.2315\n",
    "Epoch #109: Loss:1.5930, Accuracy:0.2448, Validation Loss:1.6098, Validation Accuracy:0.2332\n",
    "Epoch #110: Loss:1.5924, Accuracy:0.2464, Validation Loss:1.6092, Validation Accuracy:0.2282\n",
    "Epoch #111: Loss:1.5922, Accuracy:0.2472, Validation Loss:1.6104, Validation Accuracy:0.2282\n",
    "Epoch #112: Loss:1.5935, Accuracy:0.2468, Validation Loss:1.6106, Validation Accuracy:0.2315\n",
    "Epoch #113: Loss:1.5938, Accuracy:0.2464, Validation Loss:1.6131, Validation Accuracy:0.2299\n",
    "Epoch #114: Loss:1.5946, Accuracy:0.2439, Validation Loss:1.6130, Validation Accuracy:0.2069\n",
    "Epoch #115: Loss:1.5944, Accuracy:0.2464, Validation Loss:1.6097, Validation Accuracy:0.2184\n",
    "Epoch #116: Loss:1.5943, Accuracy:0.2411, Validation Loss:1.6103, Validation Accuracy:0.2135\n",
    "Epoch #117: Loss:1.5969, Accuracy:0.2427, Validation Loss:1.6119, Validation Accuracy:0.2085\n",
    "Epoch #118: Loss:1.5944, Accuracy:0.2480, Validation Loss:1.6126, Validation Accuracy:0.2085\n",
    "Epoch #119: Loss:1.5975, Accuracy:0.2386, Validation Loss:1.6136, Validation Accuracy:0.2266\n",
    "Epoch #120: Loss:1.5968, Accuracy:0.2431, Validation Loss:1.6124, Validation Accuracy:0.2085\n",
    "Epoch #121: Loss:1.5958, Accuracy:0.2501, Validation Loss:1.6100, Validation Accuracy:0.1987\n",
    "Epoch #122: Loss:1.5944, Accuracy:0.2480, Validation Loss:1.6119, Validation Accuracy:0.2020\n",
    "Epoch #123: Loss:1.5932, Accuracy:0.2480, Validation Loss:1.6117, Validation Accuracy:0.2217\n",
    "Epoch #124: Loss:1.5947, Accuracy:0.2472, Validation Loss:1.6145, Validation Accuracy:0.2069\n",
    "Epoch #125: Loss:1.5936, Accuracy:0.2444, Validation Loss:1.6142, Validation Accuracy:0.2085\n",
    "Epoch #126: Loss:1.5940, Accuracy:0.2464, Validation Loss:1.6134, Validation Accuracy:0.2135\n",
    "Epoch #127: Loss:1.5931, Accuracy:0.2448, Validation Loss:1.6120, Validation Accuracy:0.2151\n",
    "Epoch #128: Loss:1.5932, Accuracy:0.2448, Validation Loss:1.6124, Validation Accuracy:0.2135\n",
    "Epoch #129: Loss:1.5930, Accuracy:0.2476, Validation Loss:1.6122, Validation Accuracy:0.2167\n",
    "Epoch #130: Loss:1.5925, Accuracy:0.2497, Validation Loss:1.6111, Validation Accuracy:0.2167\n",
    "Epoch #131: Loss:1.5924, Accuracy:0.2522, Validation Loss:1.6120, Validation Accuracy:0.2266\n",
    "Epoch #132: Loss:1.5921, Accuracy:0.2480, Validation Loss:1.6123, Validation Accuracy:0.2184\n",
    "Epoch #133: Loss:1.5924, Accuracy:0.2526, Validation Loss:1.6110, Validation Accuracy:0.2053\n",
    "Epoch #134: Loss:1.5929, Accuracy:0.2419, Validation Loss:1.6110, Validation Accuracy:0.2151\n",
    "Epoch #135: Loss:1.5922, Accuracy:0.2476, Validation Loss:1.6120, Validation Accuracy:0.2200\n",
    "Epoch #136: Loss:1.5927, Accuracy:0.2497, Validation Loss:1.6134, Validation Accuracy:0.2200\n",
    "Epoch #137: Loss:1.5925, Accuracy:0.2526, Validation Loss:1.6109, Validation Accuracy:0.2167\n",
    "Epoch #138: Loss:1.5924, Accuracy:0.2411, Validation Loss:1.6116, Validation Accuracy:0.2200\n",
    "Epoch #139: Loss:1.5927, Accuracy:0.2439, Validation Loss:1.6122, Validation Accuracy:0.2217\n",
    "Epoch #140: Loss:1.5921, Accuracy:0.2493, Validation Loss:1.6115, Validation Accuracy:0.2167\n",
    "Epoch #141: Loss:1.5922, Accuracy:0.2493, Validation Loss:1.6103, Validation Accuracy:0.2118\n",
    "Epoch #142: Loss:1.5925, Accuracy:0.2534, Validation Loss:1.6087, Validation Accuracy:0.2151\n",
    "Epoch #143: Loss:1.5927, Accuracy:0.2522, Validation Loss:1.6108, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5934, Accuracy:0.2497, Validation Loss:1.6114, Validation Accuracy:0.2151\n",
    "Epoch #145: Loss:1.5932, Accuracy:0.2460, Validation Loss:1.6127, Validation Accuracy:0.2135\n",
    "Epoch #146: Loss:1.5927, Accuracy:0.2407, Validation Loss:1.6120, Validation Accuracy:0.2167\n",
    "Epoch #147: Loss:1.5942, Accuracy:0.2472, Validation Loss:1.6122, Validation Accuracy:0.2233\n",
    "Epoch #148: Loss:1.5940, Accuracy:0.2444, Validation Loss:1.6114, Validation Accuracy:0.2085\n",
    "Epoch #149: Loss:1.5938, Accuracy:0.2501, Validation Loss:1.6116, Validation Accuracy:0.2200\n",
    "Epoch #150: Loss:1.5935, Accuracy:0.2402, Validation Loss:1.6133, Validation Accuracy:0.2167\n",
    "Epoch #151: Loss:1.5934, Accuracy:0.2472, Validation Loss:1.6107, Validation Accuracy:0.2102\n",
    "Epoch #152: Loss:1.5942, Accuracy:0.2476, Validation Loss:1.6108, Validation Accuracy:0.2085\n",
    "Epoch #153: Loss:1.5942, Accuracy:0.2407, Validation Loss:1.6120, Validation Accuracy:0.2414\n",
    "Epoch #154: Loss:1.5942, Accuracy:0.2439, Validation Loss:1.6121, Validation Accuracy:0.2250\n",
    "Epoch #155: Loss:1.5940, Accuracy:0.2480, Validation Loss:1.6115, Validation Accuracy:0.2085\n",
    "Epoch #156: Loss:1.5937, Accuracy:0.2423, Validation Loss:1.6114, Validation Accuracy:0.2414\n",
    "Epoch #157: Loss:1.5936, Accuracy:0.2444, Validation Loss:1.6108, Validation Accuracy:0.2266\n",
    "Epoch #158: Loss:1.5943, Accuracy:0.2431, Validation Loss:1.6116, Validation Accuracy:0.2266\n",
    "Epoch #159: Loss:1.5939, Accuracy:0.2464, Validation Loss:1.6086, Validation Accuracy:0.2266\n",
    "Epoch #160: Loss:1.5938, Accuracy:0.2468, Validation Loss:1.6080, Validation Accuracy:0.2282\n",
    "Epoch #161: Loss:1.5944, Accuracy:0.2489, Validation Loss:1.6092, Validation Accuracy:0.2299\n",
    "Epoch #162: Loss:1.5943, Accuracy:0.2464, Validation Loss:1.6092, Validation Accuracy:0.2348\n",
    "Epoch #163: Loss:1.5941, Accuracy:0.2452, Validation Loss:1.6090, Validation Accuracy:0.2282\n",
    "Epoch #164: Loss:1.5938, Accuracy:0.2472, Validation Loss:1.6083, Validation Accuracy:0.2299\n",
    "Epoch #165: Loss:1.5938, Accuracy:0.2464, Validation Loss:1.6083, Validation Accuracy:0.2381\n",
    "Epoch #166: Loss:1.5944, Accuracy:0.2431, Validation Loss:1.6094, Validation Accuracy:0.2381\n",
    "Epoch #167: Loss:1.5937, Accuracy:0.2423, Validation Loss:1.6089, Validation Accuracy:0.2282\n",
    "Epoch #168: Loss:1.5940, Accuracy:0.2456, Validation Loss:1.6088, Validation Accuracy:0.2266\n",
    "Epoch #169: Loss:1.5937, Accuracy:0.2439, Validation Loss:1.6095, Validation Accuracy:0.2397\n",
    "Epoch #170: Loss:1.5942, Accuracy:0.2468, Validation Loss:1.6088, Validation Accuracy:0.2266\n",
    "Epoch #171: Loss:1.5936, Accuracy:0.2485, Validation Loss:1.6086, Validation Accuracy:0.2381\n",
    "Epoch #172: Loss:1.5940, Accuracy:0.2476, Validation Loss:1.6092, Validation Accuracy:0.2167\n",
    "Epoch #173: Loss:1.5945, Accuracy:0.2456, Validation Loss:1.6092, Validation Accuracy:0.2463\n",
    "Epoch #174: Loss:1.5946, Accuracy:0.2460, Validation Loss:1.6085, Validation Accuracy:0.2365\n",
    "Epoch #175: Loss:1.5944, Accuracy:0.2464, Validation Loss:1.6076, Validation Accuracy:0.2447\n",
    "Epoch #176: Loss:1.5940, Accuracy:0.2472, Validation Loss:1.6086, Validation Accuracy:0.2315\n",
    "Epoch #177: Loss:1.5938, Accuracy:0.2517, Validation Loss:1.6074, Validation Accuracy:0.2381\n",
    "Epoch #178: Loss:1.5935, Accuracy:0.2526, Validation Loss:1.6068, Validation Accuracy:0.2414\n",
    "Epoch #179: Loss:1.5935, Accuracy:0.2456, Validation Loss:1.6071, Validation Accuracy:0.2447\n",
    "Epoch #180: Loss:1.5935, Accuracy:0.2464, Validation Loss:1.6075, Validation Accuracy:0.2282\n",
    "Epoch #181: Loss:1.5934, Accuracy:0.2493, Validation Loss:1.6079, Validation Accuracy:0.2447\n",
    "Epoch #182: Loss:1.5935, Accuracy:0.2480, Validation Loss:1.6085, Validation Accuracy:0.2479\n",
    "Epoch #183: Loss:1.5933, Accuracy:0.2472, Validation Loss:1.6090, Validation Accuracy:0.2299\n",
    "Epoch #184: Loss:1.5930, Accuracy:0.2497, Validation Loss:1.6085, Validation Accuracy:0.2397\n",
    "Epoch #185: Loss:1.5937, Accuracy:0.2517, Validation Loss:1.6084, Validation Accuracy:0.2299\n",
    "Epoch #186: Loss:1.5940, Accuracy:0.2427, Validation Loss:1.6087, Validation Accuracy:0.2365\n",
    "Epoch #187: Loss:1.5934, Accuracy:0.2460, Validation Loss:1.6072, Validation Accuracy:0.2266\n",
    "Epoch #188: Loss:1.5941, Accuracy:0.2493, Validation Loss:1.6072, Validation Accuracy:0.2397\n",
    "Epoch #189: Loss:1.5928, Accuracy:0.2513, Validation Loss:1.6091, Validation Accuracy:0.2348\n",
    "Epoch #190: Loss:1.5939, Accuracy:0.2468, Validation Loss:1.6082, Validation Accuracy:0.2365\n",
    "Epoch #191: Loss:1.5930, Accuracy:0.2505, Validation Loss:1.6083, Validation Accuracy:0.2365\n",
    "Epoch #192: Loss:1.5936, Accuracy:0.2464, Validation Loss:1.6076, Validation Accuracy:0.2529\n",
    "Epoch #193: Loss:1.5925, Accuracy:0.2489, Validation Loss:1.6087, Validation Accuracy:0.2381\n",
    "Epoch #194: Loss:1.5928, Accuracy:0.2509, Validation Loss:1.6087, Validation Accuracy:0.2365\n",
    "Epoch #195: Loss:1.5924, Accuracy:0.2526, Validation Loss:1.6088, Validation Accuracy:0.2562\n",
    "Epoch #196: Loss:1.5933, Accuracy:0.2456, Validation Loss:1.6085, Validation Accuracy:0.2463\n",
    "Epoch #197: Loss:1.5926, Accuracy:0.2522, Validation Loss:1.6082, Validation Accuracy:0.2397\n",
    "Epoch #198: Loss:1.5928, Accuracy:0.2534, Validation Loss:1.6085, Validation Accuracy:0.2479\n",
    "Epoch #199: Loss:1.5928, Accuracy:0.2526, Validation Loss:1.6088, Validation Accuracy:0.2529\n",
    "Epoch #200: Loss:1.5926, Accuracy:0.2505, Validation Loss:1.6081, Validation Accuracy:0.2463\n",
    "Epoch #201: Loss:1.5932, Accuracy:0.2497, Validation Loss:1.6086, Validation Accuracy:0.2397\n",
    "Epoch #202: Loss:1.5924, Accuracy:0.2501, Validation Loss:1.6082, Validation Accuracy:0.2545\n",
    "Epoch #203: Loss:1.5922, Accuracy:0.2489, Validation Loss:1.6084, Validation Accuracy:0.2644\n",
    "Epoch #204: Loss:1.5923, Accuracy:0.2530, Validation Loss:1.6080, Validation Accuracy:0.2512\n",
    "Epoch #205: Loss:1.5928, Accuracy:0.2513, Validation Loss:1.6091, Validation Accuracy:0.2447\n",
    "Epoch #206: Loss:1.5927, Accuracy:0.2517, Validation Loss:1.6088, Validation Accuracy:0.2512\n",
    "Epoch #207: Loss:1.5928, Accuracy:0.2493, Validation Loss:1.6085, Validation Accuracy:0.2529\n",
    "Epoch #208: Loss:1.5925, Accuracy:0.2501, Validation Loss:1.6092, Validation Accuracy:0.2397\n",
    "Epoch #209: Loss:1.5928, Accuracy:0.2513, Validation Loss:1.6086, Validation Accuracy:0.2545\n",
    "Epoch #210: Loss:1.5938, Accuracy:0.2485, Validation Loss:1.6087, Validation Accuracy:0.2479\n",
    "Epoch #211: Loss:1.5928, Accuracy:0.2513, Validation Loss:1.6087, Validation Accuracy:0.2512\n",
    "Epoch #212: Loss:1.5933, Accuracy:0.2509, Validation Loss:1.6097, Validation Accuracy:0.2447\n",
    "Epoch #213: Loss:1.5916, Accuracy:0.2517, Validation Loss:1.6093, Validation Accuracy:0.2496\n",
    "Epoch #214: Loss:1.5924, Accuracy:0.2456, Validation Loss:1.6084, Validation Accuracy:0.2496\n",
    "Epoch #215: Loss:1.5916, Accuracy:0.2530, Validation Loss:1.6075, Validation Accuracy:0.2512\n",
    "Epoch #216: Loss:1.5919, Accuracy:0.2522, Validation Loss:1.6072, Validation Accuracy:0.2496\n",
    "Epoch #217: Loss:1.5921, Accuracy:0.2538, Validation Loss:1.6072, Validation Accuracy:0.2545\n",
    "Epoch #218: Loss:1.5924, Accuracy:0.2476, Validation Loss:1.6082, Validation Accuracy:0.2594\n",
    "Epoch #219: Loss:1.5917, Accuracy:0.2505, Validation Loss:1.6092, Validation Accuracy:0.2332\n",
    "Epoch #220: Loss:1.5919, Accuracy:0.2497, Validation Loss:1.6087, Validation Accuracy:0.2496\n",
    "Epoch #221: Loss:1.5921, Accuracy:0.2472, Validation Loss:1.6076, Validation Accuracy:0.2594\n",
    "Epoch #222: Loss:1.5920, Accuracy:0.2517, Validation Loss:1.6075, Validation Accuracy:0.2578\n",
    "Epoch #223: Loss:1.5915, Accuracy:0.2522, Validation Loss:1.6074, Validation Accuracy:0.2529\n",
    "Epoch #224: Loss:1.5917, Accuracy:0.2509, Validation Loss:1.6079, Validation Accuracy:0.2496\n",
    "Epoch #225: Loss:1.5921, Accuracy:0.2485, Validation Loss:1.6088, Validation Accuracy:0.2414\n",
    "Epoch #226: Loss:1.5915, Accuracy:0.2435, Validation Loss:1.6086, Validation Accuracy:0.2365\n",
    "Epoch #227: Loss:1.5921, Accuracy:0.2444, Validation Loss:1.6064, Validation Accuracy:0.2512\n",
    "Epoch #228: Loss:1.5913, Accuracy:0.2509, Validation Loss:1.6068, Validation Accuracy:0.2578\n",
    "Epoch #229: Loss:1.5913, Accuracy:0.2509, Validation Loss:1.6076, Validation Accuracy:0.2496\n",
    "Epoch #230: Loss:1.5913, Accuracy:0.2517, Validation Loss:1.6086, Validation Accuracy:0.2529\n",
    "Epoch #231: Loss:1.5911, Accuracy:0.2546, Validation Loss:1.6082, Validation Accuracy:0.2594\n",
    "Epoch #232: Loss:1.5915, Accuracy:0.2493, Validation Loss:1.6070, Validation Accuracy:0.2512\n",
    "Epoch #233: Loss:1.5908, Accuracy:0.2546, Validation Loss:1.6073, Validation Accuracy:0.2594\n",
    "Epoch #234: Loss:1.5911, Accuracy:0.2522, Validation Loss:1.6073, Validation Accuracy:0.2578\n",
    "Epoch #235: Loss:1.5918, Accuracy:0.2509, Validation Loss:1.6086, Validation Accuracy:0.2545\n",
    "Epoch #236: Loss:1.5912, Accuracy:0.2505, Validation Loss:1.6088, Validation Accuracy:0.2496\n",
    "Epoch #237: Loss:1.5911, Accuracy:0.2522, Validation Loss:1.6082, Validation Accuracy:0.2594\n",
    "Epoch #238: Loss:1.5914, Accuracy:0.2526, Validation Loss:1.6082, Validation Accuracy:0.2562\n",
    "Epoch #239: Loss:1.5912, Accuracy:0.2595, Validation Loss:1.6065, Validation Accuracy:0.2397\n",
    "Epoch #240: Loss:1.5920, Accuracy:0.2530, Validation Loss:1.6065, Validation Accuracy:0.2496\n",
    "Epoch #241: Loss:1.5916, Accuracy:0.2419, Validation Loss:1.6070, Validation Accuracy:0.2594\n",
    "Epoch #242: Loss:1.5915, Accuracy:0.2554, Validation Loss:1.6089, Validation Accuracy:0.2594\n",
    "Epoch #243: Loss:1.5915, Accuracy:0.2444, Validation Loss:1.6089, Validation Accuracy:0.2496\n",
    "Epoch #244: Loss:1.5906, Accuracy:0.2517, Validation Loss:1.6082, Validation Accuracy:0.2430\n",
    "Epoch #245: Loss:1.5910, Accuracy:0.2468, Validation Loss:1.6081, Validation Accuracy:0.2578\n",
    "Epoch #246: Loss:1.5908, Accuracy:0.2530, Validation Loss:1.6073, Validation Accuracy:0.2447\n",
    "Epoch #247: Loss:1.5907, Accuracy:0.2509, Validation Loss:1.6071, Validation Accuracy:0.2496\n",
    "Epoch #248: Loss:1.5904, Accuracy:0.2480, Validation Loss:1.6069, Validation Accuracy:0.2529\n",
    "Epoch #249: Loss:1.5907, Accuracy:0.2513, Validation Loss:1.6062, Validation Accuracy:0.2529\n",
    "Epoch #250: Loss:1.5908, Accuracy:0.2489, Validation Loss:1.6065, Validation Accuracy:0.2463\n",
    "Epoch #251: Loss:1.5910, Accuracy:0.2493, Validation Loss:1.6088, Validation Accuracy:0.2365\n",
    "Epoch #252: Loss:1.5915, Accuracy:0.2476, Validation Loss:1.6077, Validation Accuracy:0.2463\n",
    "Epoch #253: Loss:1.5915, Accuracy:0.2546, Validation Loss:1.6070, Validation Accuracy:0.2430\n",
    "Epoch #254: Loss:1.5899, Accuracy:0.2559, Validation Loss:1.6063, Validation Accuracy:0.2463\n",
    "Epoch #255: Loss:1.5910, Accuracy:0.2456, Validation Loss:1.6066, Validation Accuracy:0.2414\n",
    "Epoch #256: Loss:1.5902, Accuracy:0.2501, Validation Loss:1.6062, Validation Accuracy:0.2365\n",
    "Epoch #257: Loss:1.5907, Accuracy:0.2460, Validation Loss:1.6076, Validation Accuracy:0.2447\n",
    "Epoch #258: Loss:1.5909, Accuracy:0.2505, Validation Loss:1.6077, Validation Accuracy:0.2447\n",
    "Epoch #259: Loss:1.5908, Accuracy:0.2505, Validation Loss:1.6067, Validation Accuracy:0.2479\n",
    "Epoch #260: Loss:1.5905, Accuracy:0.2517, Validation Loss:1.6076, Validation Accuracy:0.2414\n",
    "Epoch #261: Loss:1.5912, Accuracy:0.2480, Validation Loss:1.6087, Validation Accuracy:0.2397\n",
    "Epoch #262: Loss:1.5904, Accuracy:0.2509, Validation Loss:1.6063, Validation Accuracy:0.2381\n",
    "Epoch #263: Loss:1.5901, Accuracy:0.2542, Validation Loss:1.6067, Validation Accuracy:0.2529\n",
    "Epoch #264: Loss:1.5898, Accuracy:0.2489, Validation Loss:1.6066, Validation Accuracy:0.2545\n",
    "Epoch #265: Loss:1.5898, Accuracy:0.2534, Validation Loss:1.6071, Validation Accuracy:0.2545\n",
    "Epoch #266: Loss:1.5903, Accuracy:0.2530, Validation Loss:1.6079, Validation Accuracy:0.2496\n",
    "Epoch #267: Loss:1.5899, Accuracy:0.2526, Validation Loss:1.6077, Validation Accuracy:0.2562\n",
    "Epoch #268: Loss:1.5894, Accuracy:0.2554, Validation Loss:1.6066, Validation Accuracy:0.2512\n",
    "Epoch #269: Loss:1.5901, Accuracy:0.2559, Validation Loss:1.6073, Validation Accuracy:0.2512\n",
    "Epoch #270: Loss:1.5900, Accuracy:0.2460, Validation Loss:1.6061, Validation Accuracy:0.2479\n",
    "Epoch #271: Loss:1.5900, Accuracy:0.2489, Validation Loss:1.6066, Validation Accuracy:0.2447\n",
    "Epoch #272: Loss:1.5900, Accuracy:0.2550, Validation Loss:1.6068, Validation Accuracy:0.2529\n",
    "Epoch #273: Loss:1.5898, Accuracy:0.2542, Validation Loss:1.6077, Validation Accuracy:0.2447\n",
    "Epoch #274: Loss:1.5903, Accuracy:0.2534, Validation Loss:1.6071, Validation Accuracy:0.2578\n",
    "Epoch #275: Loss:1.5897, Accuracy:0.2571, Validation Loss:1.6057, Validation Accuracy:0.2578\n",
    "Epoch #276: Loss:1.5898, Accuracy:0.2530, Validation Loss:1.6062, Validation Accuracy:0.2578\n",
    "Epoch #277: Loss:1.5902, Accuracy:0.2509, Validation Loss:1.6069, Validation Accuracy:0.2496\n",
    "Epoch #278: Loss:1.5887, Accuracy:0.2563, Validation Loss:1.6073, Validation Accuracy:0.2545\n",
    "Epoch #279: Loss:1.5909, Accuracy:0.2530, Validation Loss:1.6085, Validation Accuracy:0.2545\n",
    "Epoch #280: Loss:1.5900, Accuracy:0.2637, Validation Loss:1.6054, Validation Accuracy:0.2348\n",
    "Epoch #281: Loss:1.5899, Accuracy:0.2530, Validation Loss:1.6053, Validation Accuracy:0.2512\n",
    "Epoch #282: Loss:1.5893, Accuracy:0.2530, Validation Loss:1.6057, Validation Accuracy:0.2332\n",
    "Epoch #283: Loss:1.5901, Accuracy:0.2550, Validation Loss:1.6078, Validation Accuracy:0.2578\n",
    "Epoch #284: Loss:1.5895, Accuracy:0.2517, Validation Loss:1.6064, Validation Accuracy:0.2414\n",
    "Epoch #285: Loss:1.5893, Accuracy:0.2559, Validation Loss:1.6070, Validation Accuracy:0.2479\n",
    "Epoch #286: Loss:1.5891, Accuracy:0.2534, Validation Loss:1.6069, Validation Accuracy:0.2463\n",
    "Epoch #287: Loss:1.5892, Accuracy:0.2563, Validation Loss:1.6060, Validation Accuracy:0.2479\n",
    "Epoch #288: Loss:1.5886, Accuracy:0.2567, Validation Loss:1.6050, Validation Accuracy:0.2479\n",
    "Epoch #289: Loss:1.5888, Accuracy:0.2554, Validation Loss:1.6044, Validation Accuracy:0.2545\n",
    "Epoch #290: Loss:1.5888, Accuracy:0.2522, Validation Loss:1.6052, Validation Accuracy:0.2496\n",
    "Epoch #291: Loss:1.5890, Accuracy:0.2567, Validation Loss:1.6064, Validation Accuracy:0.2299\n",
    "Epoch #292: Loss:1.5886, Accuracy:0.2591, Validation Loss:1.6065, Validation Accuracy:0.2414\n",
    "Epoch #293: Loss:1.5895, Accuracy:0.2546, Validation Loss:1.6071, Validation Accuracy:0.2463\n",
    "Epoch #294: Loss:1.5887, Accuracy:0.2513, Validation Loss:1.6067, Validation Accuracy:0.2660\n",
    "Epoch #295: Loss:1.5889, Accuracy:0.2534, Validation Loss:1.6048, Validation Accuracy:0.2348\n",
    "Epoch #296: Loss:1.5887, Accuracy:0.2538, Validation Loss:1.6056, Validation Accuracy:0.2332\n",
    "Epoch #297: Loss:1.5892, Accuracy:0.2517, Validation Loss:1.6076, Validation Accuracy:0.2479\n",
    "Epoch #298: Loss:1.5880, Accuracy:0.2571, Validation Loss:1.6061, Validation Accuracy:0.2315\n",
    "Epoch #299: Loss:1.5888, Accuracy:0.2485, Validation Loss:1.6055, Validation Accuracy:0.2414\n",
    "Epoch #300: Loss:1.5880, Accuracy:0.2526, Validation Loss:1.6065, Validation Accuracy:0.2315\n",
    "\n",
    "Test:\n",
    "Test Loss:1.60653150, Accuracy:0.2315\n",
    "Labels: ['04', '03', '01', '05', '02']\n",
    "Confusion Matrix:\n",
    "      04  03  01  05  02\n",
    "t:04  35   0  23  54   0\n",
    "t:03  39   1  13  61   1\n",
    "t:01  39   2  15  69   1\n",
    "t:05  37   1  17  87   0\n",
    "t:02  33   0  14  64   3\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          04       0.19      0.31      0.24       112\n",
    "          03       0.25      0.01      0.02       115\n",
    "          01       0.18      0.12      0.14       126\n",
    "          05       0.26      0.61      0.36       142\n",
    "          02       0.60      0.03      0.05       114\n",
    "\n",
    "    accuracy                           0.23       609\n",
    "   macro avg       0.30      0.22      0.16       609\n",
    "weighted avg       0.29      0.23      0.17       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 06:08:43 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 43 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.606064898039907, 1.6054308715908008, 1.6053480624369605, 1.604954017794191, 1.6053579243141638, 1.605396261943385, 1.6056645554666253, 1.605571507820355, 1.606528790713531, 1.608976872488, 1.609244241307326, 1.6109540525132604, 1.607549957258165, 1.612772281533979, 1.6089582014553652, 1.607004609014013, 1.610007043346787, 1.6091361683969232, 1.608225736516254, 1.610069716896721, 1.6093749680934086, 1.6098627991277008, 1.6101071703414416, 1.6104999105331346, 1.6110087719261157, 1.6117363089606875, 1.611351094809659, 1.6113306589314502, 1.6108973922792131, 1.6095931796213285, 1.608928728573428, 1.6092671122652753, 1.6088330833782702, 1.6086704637029487, 1.609581699512275, 1.6098016989837922, 1.6107205996176683, 1.6101871065318292, 1.6110144473844756, 1.6107685777354124, 1.6099927441044197, 1.6102786424320514, 1.6102795305314714, 1.6098394711029353, 1.6099284540843495, 1.609292808620409, 1.6094695762264708, 1.6082648827720354, 1.608530898203795, 1.6079101300200414, 1.608693086650767, 1.6092039487632037, 1.6080777210555053, 1.6093863728402675, 1.6103664446738357, 1.609416726187532, 1.6086856300999182, 1.609955000564187, 1.6086414841008303, 1.6091338135730262, 1.6094442954400099, 1.6099985520827946, 1.6079217050658854, 1.611404499396902, 1.608657356749223, 1.6073739916233007, 1.6093199499722184, 1.609412811268335, 1.6066552719654903, 1.6076391655431788, 1.6064038881527378, 1.6055357035353461, 1.6043587887815653, 1.6073627107836344, 1.6055072187790143, 1.6062339479700098, 1.608075470172713, 1.6092611564987007, 1.607843634921733, 1.6072802676747389, 1.6076567909008959, 1.6072174563196493, 1.607403337661856, 1.6071123977012822, 1.6068164515377852, 1.6058511900392856, 1.607041841070053, 1.6073206026957345, 1.6076411061686249, 1.6080785132394049, 1.6096636313327233, 1.6077133297724482, 1.6075581731075919, 1.6087717663478383, 1.6066834926605225, 1.6062219334744858, 1.6070954488416023, 1.608772162929153, 1.6083469185335884, 1.6075029713766915, 1.6072914343945106, 1.6071458338516686, 1.6087516661739505, 1.6083273206438338, 1.609075065512571, 1.6074113250757478, 1.60806053553896, 1.609632373639124, 1.6098356154947642, 1.6091973258943981, 1.6104004257809743, 1.6105632331766715, 1.6130795363330686, 1.612961515612986, 1.6096774637209763, 1.610319870054624, 1.6118804721409463, 1.6126229465497146, 1.6136215234233438, 1.6123775772273248, 1.6099808094732475, 1.6119356987315838, 1.6116533537803612, 1.6145497129859987, 1.6141722039831878, 1.6134321094537996, 1.6120145125146375, 1.6123661313738142, 1.6122130331734719, 1.6110897535956747, 1.6119676497573727, 1.6123180831790167, 1.6110190174654004, 1.6110114700884264, 1.611981164058441, 1.61340638236655, 1.6108830683728548, 1.6116425118031368, 1.6122089743809942, 1.6115248132809041, 1.6102768206244031, 1.608656259593118, 1.6107977697218971, 1.6113567420806008, 1.612701499403404, 1.6120024643508084, 1.612247513432808, 1.6113688583640238, 1.6116145902079315, 1.6133277856657657, 1.6107057000224423, 1.6108252424716167, 1.6119897731615014, 1.6121145574917346, 1.6114535323896235, 1.6114191819098587, 1.6107561161561161, 1.611630887429311, 1.6085504808253648, 1.6079696941454031, 1.6092358706228447, 1.6092325360904187, 1.6089509516122502, 1.6083250235649948, 1.6082500415091052, 1.609406599270299, 1.6089281650208096, 1.6087651839984463, 1.6095307024045922, 1.6087541204367952, 1.6085862561399713, 1.6091886795996053, 1.609198077167392, 1.6084615612656417, 1.607576447754658, 1.6085567425428744, 1.607402181194725, 1.6068142086805772, 1.6071240937181295, 1.6074979970803598, 1.607933033080328, 1.6085245593623771, 1.609011397181669, 1.608498913509701, 1.608399914599013, 1.6086765126446, 1.6072284259232394, 1.607209909921405, 1.6091043483251812, 1.6081656252809347, 1.6083401662767032, 1.607603659966505, 1.6086705973974393, 1.608731829669871, 1.6088270131003093, 1.6085175669251992, 1.6081786098934354, 1.6084827329529134, 1.6088368332836231, 1.6080576594435718, 1.6086178250696468, 1.6081962698981875, 1.6084158070177477, 1.607998725032963, 1.6090713475138096, 1.6087959016289421, 1.6084713002143822, 1.6091980548523526, 1.608645659362154, 1.608717597764114, 1.6086889892963354, 1.6096819766440806, 1.609309600883321, 1.6084113821803252, 1.6075176339235604, 1.6071754241811818, 1.6071586736121592, 1.6082304397044316, 1.609196561310679, 1.6086758326231356, 1.607601620489349, 1.6074928800852233, 1.6073529050855213, 1.6078587335906005, 1.6088203481461223, 1.608614088475019, 1.606397841560038, 1.6067876019109841, 1.6075857429473075, 1.6086327992440836, 1.6081978589638897, 1.6070190047591386, 1.6072771310414782, 1.6072583971743906, 1.608578377952325, 1.608792343358884, 1.6082446271758557, 1.608221427168948, 1.6064768305357258, 1.6065339122108246, 1.6069775954843155, 1.6088879928604527, 1.6089360531914998, 1.6082351066796063, 1.6080594111741666, 1.607318889331348, 1.6070677713416088, 1.6069315206045391, 1.606242405369951, 1.6065410908024103, 1.6087712816808415, 1.6076925926412071, 1.6070358764949104, 1.6063271228511542, 1.6065549603823959, 1.6062144600894848, 1.6075575723632411, 1.607663914292121, 1.6066807046508162, 1.6076327507523285, 1.6087009272551889, 1.6063189870618246, 1.606732765442045, 1.6065521535810774, 1.6071187114872172, 1.607917688750281, 1.6077337705443058, 1.6066427166238795, 1.6073005228794266, 1.6060762693142068, 1.6066440952626746, 1.6068464449082298, 1.607713154188322, 1.6071303877337226, 1.6056673374082067, 1.60618988007356, 1.606929926840934, 1.6072796573388362, 1.6085351852360616, 1.6054499517324914, 1.60533257971452, 1.6056891908786568, 1.6078338168916249, 1.6064238620704814, 1.6070221854352402, 1.606926484452484, 1.605970484478329, 1.6050170949722942, 1.604366543062019, 1.6052186156337094, 1.6064064537950338, 1.606521506614873, 1.6071121187632895, 1.60669581075803, 1.6048211675559358, 1.6055726251382938, 1.6075611302418074, 1.6061472514971529, 1.605527507847753, 1.6065313937433052], 'val_acc': [0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23316912959851263, 0.23645320184810212, 0.2282430211262554, 0.2282430211262554, 0.2233169125561252, 0.22660098500146067, 0.23316912959851263, 0.22988505734892314, 0.20032840651537984, 0.2298850570553042, 0.23645320175022916, 0.22495894868091998, 0.22988505725105016, 0.22660098490358768, 0.21839080408386802, 0.22660098490358768, 0.2233169125561252, 0.22495894887666593, 0.2216748763334575, 0.2151067318342785, 0.2151067318342785, 0.2233169126539982, 0.2151067318342785, 0.22003284030653572, 0.21674876795907325, 0.23316912940276668, 0.2233169125561252, 0.22660098480571472, 0.22824302102838243, 0.23152709327797194, 0.23152709327797194, 0.22660098490358768, 0.2298850571531772, 0.22495894877879294, 0.22824302102838243, 0.22495894877879294, 0.2233169126539982, 0.23152709327797194, 0.23152709327797194, 0.22660098490358768, 0.22824302093050947, 0.22495894868091998, 0.22660098480571472, 0.22495894868091998, 0.22824302093050947, 0.22495894868091998, 0.22003284030653572, 0.23316912940276668, 0.22824302093050947, 0.22003284030653572, 0.22660098480571472, 0.2298850570553042, 0.21839080418174098, 0.23152709318009895, 0.2298850570553042, 0.2298850570553042, 0.22824302093050947, 0.22824302093050947, 0.2233169126539982, 0.2298850570553042, 0.2331691293048937, 0.2233169125561252, 0.23645320165235617, 0.23645320165235617, 0.23481116542968844, 0.23481116542968844, 0.22003284020866276, 0.23152709318009895, 0.23152709327797194, 0.2298850571531772, 0.2298850571531772, 0.23152709327797194, 0.21839080418174098, 0.23316912950063964, 0.2348111656254344, 0.23316912950063964, 0.23316912950063964, 0.23152709327797194, 0.23316912950063964, 0.23316912950063964, 0.22824302102838243, 0.23152709327797194, 0.23645320175022916, 0.23645320175022916, 0.2298850571531772, 0.21510673212789747, 0.21510673212789747, 0.22824302102838243, 0.23316912940276668, 0.2315270933758449, 0.2200328404044087, 0.2249589489745389, 0.23481116552756143, 0.24302134624940813, 0.2380952378750239, 0.22988505725105016, 0.22660098500146067, 0.2282430212241284, 0.23316912959851263, 0.2282430211262554, 0.21839080427961396, 0.23316912950063964, 0.2315270933758449, 0.23316912959851263, 0.2282430212241284, 0.2282430211262554, 0.23152709357159088, 0.22988505725105016, 0.20689655150392372, 0.21839080437748695, 0.2134646939233019, 0.2085385875308455, 0.2085385875308455, 0.22660098490358768, 0.20853858723722654, 0.19868637087995, 0.2019704431295395, 0.22167487662707644, 0.20689655130817777, 0.20853858782446444, 0.21346469610097568, 0.21510673222577043, 0.21346469590522973, 0.21674876835056517, 0.21674876835056517, 0.22660098509933366, 0.21839080437748695, 0.205254515281256, 0.2151067319321515, 0.22003284060015468, 0.22003284060015468, 0.2167487682526922, 0.22003284060015468, 0.22167487662707644, 0.21674876617289138, 0.211822659682562, 0.2151067323236434, 0.2298850570553042, 0.21510673203002448, 0.21346469600310272, 0.21674876844843816, 0.22331691275187118, 0.20853858762871846, 0.22003284060015468, 0.21674876844843816, 0.21018062365564025, 0.20853858772659145, 0.24137931022248635, 0.22495894887666593, 0.2085385875308455, 0.2413793101246134, 0.22660098509933366, 0.22660098500146067, 0.22660098509933366, 0.22824302102838243, 0.22988505725105016, 0.23481116572330737, 0.22824302102838243, 0.22988505725105016, 0.23809523797289686, 0.23809523589309606, 0.22824302102838243, 0.22660098490358768, 0.23973727399981865, 0.22660098500146067, 0.23809523797289686, 0.21674876835056517, 0.2463054185968706, 0.23645320175022916, 0.24466338247207586, 0.23152709327797194, 0.2380952378750239, 0.2413793101246134, 0.24466338237420288, 0.22824302102838243, 0.24466338237420288, 0.24794745472166535, 0.2298850571531772, 0.23973727390194566, 0.2298850571531772, 0.23645320175022916, 0.22660098519720664, 0.23973727390194566, 0.2348111656254344, 0.23645320165235617, 0.23645320165235617, 0.2528735630960496, 0.23809523777715091, 0.23645320165235617, 0.25615763534563907, 0.24630541849899762, 0.23973727370619968, 0.24794745462379236, 0.2528735629003036, 0.24630541849899762, 0.23973727390194566, 0.25451559892722536, 0.2643678159696128, 0.25123152697125484, 0.24466338237420288, 0.25123152687338185, 0.2528735630960496, 0.23973727370619968, 0.25451559892722536, 0.24794745472166535, 0.2512315270691278, 0.24466338208058394, 0.2495894908464601, 0.2495894907485871, 0.2512315266776359, 0.2495894907485871, 0.2545155992208443, 0.25944170759522855, 0.23316912940276668, 0.2495894907485871, 0.25944170759522855, 0.2577996714704338, 0.2528735628024306, 0.2495894907485871, 0.24137930983099445, 0.23645320155448318, 0.2512315266776359, 0.2577996714704338, 0.2495894907485871, 0.2528735628024306, 0.25944170759522855, 0.25123152687338185, 0.25944170759522855, 0.2577996714704338, 0.2545155993187173, 0.2495894907485871, 0.25944170759522855, 0.25615763534563907, 0.23973727390194566, 0.2495894907485871, 0.25944170759522855, 0.25944170759522855, 0.2495894907485871, 0.24302134634728112, 0.2577996714704338, 0.24466338237420288, 0.2495894907485871, 0.2528735630960496, 0.2528735629003036, 0.2463054185968706, 0.23645320155448318, 0.24630541830325164, 0.24302134615153514, 0.24630541849899762, 0.2413793099288674, 0.2364532014566102, 0.24466338247207586, 0.24466338237420288, 0.24794745462379236, 0.2413793101246134, 0.23973727370619968, 0.23809523767927793, 0.2528735630960496, 0.25451559902509835, 0.25451559902509835, 0.2495894907485871, 0.25615763534563907, 0.25123152677550886, 0.25123152677550886, 0.24794745452591938, 0.24466338237420288, 0.2528735630960496, 0.24466338237420288, 0.2577996714704338, 0.2577996713725608, 0.2577996714704338, 0.2495894907485871, 0.2545155992208443, 0.25451559912297134, 0.23481116552756143, 0.2512315266776359, 0.2331691293048937, 0.2577996714704338, 0.2413793100267404, 0.24794745452591938, 0.24630541830325164, 0.24794745452591938, 0.24794745452591938, 0.25451559902509835, 0.24958949055284116, 0.22988505695743122, 0.2413793100267404, 0.24630541849899762, 0.2660098521922805, 0.23481116533181545, 0.2331691292070207, 0.24794745462379236, 0.23152709308222597, 0.24137930983099445, 0.23152709318009895], 'loss': [1.6077427649644855, 1.6049739304998818, 1.604159736731214, 1.6034607878945446, 1.6023694410461176, 1.6021354123552232, 1.60152339558337, 1.6009032499618843, 1.6006239563043112, 1.5999519448015969, 1.6001156847090203, 1.602999470610883, 1.6024058542212423, 1.599851611850198, 1.601692056019448, 1.6003290653718325, 1.5997264989837239, 1.5990004689296902, 1.5998934935250566, 1.5991255463515954, 1.598661168791675, 1.5986422042827098, 1.5987937834718144, 1.5978444344209204, 1.597818308738223, 1.5983517779951468, 1.5991917744799071, 1.598110815533867, 1.5979669877880653, 1.5976541029109603, 1.597642584649934, 1.5974964232904956, 1.597707216646637, 1.5981171257442028, 1.5980309905457546, 1.597758212716183, 1.5980511371604715, 1.5981502217188999, 1.5977961343906253, 1.59738762559832, 1.5970562698415172, 1.5968266992843128, 1.5967508947090447, 1.5967791777616653, 1.5967968488375999, 1.5967631377233862, 1.5966874160805766, 1.596779049984973, 1.5968287975146784, 1.5967430702714704, 1.5963720330957025, 1.59702106192861, 1.596431126486839, 1.5966373427937408, 1.5964659577277651, 1.5958348663435824, 1.595707646287687, 1.5950243025100206, 1.595179658455036, 1.5949383241684774, 1.5950827129567673, 1.59513002612018, 1.5956228456947592, 1.5954611131542762, 1.5957713010129988, 1.59496350161104, 1.5955047458104283, 1.5960463566946543, 1.5948970692358468, 1.5952193977896438, 1.5984333622871727, 1.611455987709014, 1.6015183063992728, 1.6003755290895028, 1.59834941569295, 1.5972847788240876, 1.5959240732741307, 1.5961811345705506, 1.5960428208541086, 1.5958212108337904, 1.5956729645601777, 1.595249203734819, 1.5952219868587518, 1.5950367274470398, 1.594996288473846, 1.5948861767868732, 1.5948372246548381, 1.594475801181989, 1.5947599978907152, 1.5940631003350447, 1.594484524609372, 1.5939665937325793, 1.5950304398056907, 1.5943374919205966, 1.5942033078391449, 1.5935210964273379, 1.5935516248248687, 1.5934392555048824, 1.593455406576701, 1.5930987646447559, 1.5932204153013916, 1.5930845122562542, 1.5931127109566754, 1.5935031051263673, 1.5932146493414345, 1.592575487317001, 1.5924725284321841, 1.5924005657250877, 1.5929996255976464, 1.5924020054893573, 1.5921864306412683, 1.5934772569296052, 1.5938280012573305, 1.5946430609701105, 1.5943807631792228, 1.594256407524281, 1.5968881904711714, 1.5944419568079453, 1.5975002641305787, 1.5967587764258258, 1.5957565588627998, 1.5943644451164856, 1.5931571501725998, 1.5946527470798217, 1.5935503011856236, 1.5940031380624007, 1.5931366145977983, 1.5931924306391692, 1.5930208396128316, 1.592511595152242, 1.5923875233475921, 1.592113638658543, 1.5924071252713212, 1.5928775942545896, 1.5922189646433023, 1.5927026793207721, 1.5925052253617398, 1.5924076281044273, 1.5926988341725092, 1.59212422566737, 1.5922036988044912, 1.5924794646993554, 1.592724256887573, 1.593405364622081, 1.5932054906410358, 1.5927379001582183, 1.594159572472073, 1.5940169930213286, 1.593817452728381, 1.5935383540648944, 1.5933858899365216, 1.594185516476876, 1.5942119650282653, 1.5941736668776683, 1.5940310540385314, 1.593670525296268, 1.5936317844802104, 1.594346022018417, 1.5938633458080722, 1.593777453679079, 1.594399960574673, 1.5943136702817569, 1.5940508079724636, 1.5938039100635224, 1.5937564114036011, 1.5943780231769569, 1.5936954895818503, 1.593970150918197, 1.5937303667440552, 1.5942322129831177, 1.5936231141707247, 1.593990791113225, 1.5945182700911098, 1.5946384227740935, 1.5943830702094326, 1.5939997727866047, 1.5938103015417926, 1.5935428382434884, 1.5935087891329975, 1.5934587824760766, 1.5933688174527774, 1.593468743036415, 1.593296007454028, 1.5929724404944041, 1.5937068152966196, 1.594038660766163, 1.593400707332995, 1.5941365352156716, 1.5927897144881606, 1.593879268644282, 1.5929724078893173, 1.5936423996880313, 1.5925199425440795, 1.592839931903189, 1.5923885943218912, 1.5932997456811047, 1.5926142213770496, 1.5927782678506213, 1.592826999873841, 1.5926254505004727, 1.5931602519150876, 1.5923824396466328, 1.5921702604763805, 1.5923205765855386, 1.592781056956344, 1.592695617479955, 1.5928386383232884, 1.5925442630505415, 1.5928385073154614, 1.5938072529661582, 1.592794561679848, 1.5932531554596134, 1.5915753925605476, 1.5923700746324763, 1.5916196850535806, 1.591882828814293, 1.5921233221735553, 1.5924014537486209, 1.591661197889512, 1.5919451324357143, 1.5921377211870353, 1.5920039728681654, 1.5914519612549267, 1.5917419834548199, 1.5921111215066617, 1.5915481238394547, 1.592122548806349, 1.5912852916384626, 1.5913414756864983, 1.591313657574585, 1.5910551316439494, 1.5915242809534562, 1.5907922126918848, 1.5911371897867819, 1.5918419618136583, 1.5911831598262278, 1.591092708419236, 1.5913554219984176, 1.591224814734175, 1.5920342456143983, 1.5916158994854843, 1.5914584679280463, 1.5915480282761967, 1.5906154183636456, 1.5909795870771153, 1.5908060995215507, 1.5907368491562486, 1.5904335692188334, 1.590657355800057, 1.590779197730078, 1.5910391619073292, 1.591523124060347, 1.5915418808954698, 1.5898639272125838, 1.5909902293089724, 1.59018902269477, 1.5907066965494803, 1.5909327008151422, 1.5908385116706394, 1.5904680385237113, 1.59121158960174, 1.5903610304395766, 1.5901057121934832, 1.589829338159894, 1.5898423418127292, 1.5902636725310182, 1.5898909955543659, 1.5894289885703055, 1.5901408985165355, 1.5900140246326673, 1.590040388293335, 1.5900075850790287, 1.5897504995001415, 1.5902839977883216, 1.5897142127798813, 1.5897623365175062, 1.590205167549102, 1.5886888619565867, 1.590880546530659, 1.5900492405744548, 1.5898553898202321, 1.5893170906043395, 1.5900983932816273, 1.589521254014675, 1.5892856044690957, 1.5891252207805, 1.589187044970064, 1.588638722137749, 1.588802948526778, 1.588799716706638, 1.5890128234083893, 1.588604948799713, 1.5894706390232032, 1.58868107585202, 1.5889340185776384, 1.5887147101043921, 1.5891673677511038, 1.587958296025803, 1.5888426688172734, 1.58800039403982], 'acc': [0.21149897257160602, 0.2336755641615611, 0.23285421011873828, 0.2328542091028891, 0.23285420912124782, 0.23285420851541005, 0.234907596177389, 0.23983573076416578, 0.2402464058364931, 0.24969199183540422, 0.24599589327277588, 0.23860369673499826, 0.23203285406257582, 0.24312115092541892, 0.242710471544912, 0.24024640761728894, 0.23942505236279057, 0.2443531830146817, 0.24435318338797568, 0.24681724772561012, 0.24435318377962836, 0.24353182776018334, 0.2451745397990734, 0.24312114814713262, 0.2447638606143928, 0.24558521586889115, 0.24271047312988148, 0.24681724733395743, 0.24517453686167817, 0.2443531830146817, 0.24517453864247402, 0.2435318267810516, 0.24476386159352453, 0.24394250459494776, 0.24681724792143647, 0.24599589307694955, 0.242710471544912, 0.24435318299632297, 0.24188911787538314, 0.24229979392684217, 0.24394250498660047, 0.24312114898551415, 0.24435318201719122, 0.24394250498660047, 0.2431211487713291, 0.24435318162553854, 0.24312114975046084, 0.2427104719549234, 0.24476385963526104, 0.2435318267810516, 0.24435318456293376, 0.2435318283660211, 0.24435318240884393, 0.24188911709207775, 0.24312114976881954, 0.24312115094377765, 0.24188911650459868, 0.24106776244341716, 0.2422997952976266, 0.24353182856184746, 0.2422997949059739, 0.2439425055740795, 0.24271047312988148, 0.24147843988401935, 0.2443531826046703, 0.2402464058364931, 0.23983572939338135, 0.24271047312988148, 0.2472279259495177, 0.24312115053376623, 0.2402464070298099, 0.22956878933328867, 0.2427104727382288, 0.24353182738688936, 0.24558521549559717, 0.24394250459494776, 0.24640657030336666, 0.24229979392684217, 0.24394250420329508, 0.24435318139299475, 0.24353182776018334, 0.2447638598310874, 0.24517453705750453, 0.24435318340633438, 0.24558521526305338, 0.2459958922569267, 0.2431211487713291, 0.24517453864247402, 0.24353182758271572, 0.24394250480913285, 0.24271047254240244, 0.24024640701145117, 0.24188911752044787, 0.24599589387861365, 0.2431211507295926, 0.241478440648966, 0.24024640564066674, 0.24024640722563625, 0.2443531837979871, 0.24353182738688936, 0.24106776361837523, 0.24517453921159435, 0.24188911593547838, 0.2410677622475908, 0.2476386031575761, 0.2418891180895682, 0.2402464070298099, 0.24599589386025494, 0.24476386041856643, 0.24640657265328283, 0.24722792573533264, 0.2468172496838736, 0.24640657069501937, 0.24394250479077412, 0.24640657186997744, 0.24106776148264414, 0.2427104731482402, 0.2480492823422567, 0.23860369694918332, 0.24312114935880813, 0.2501026684743423, 0.24804928253808306, 0.24804928097147227, 0.2472279267328231, 0.24435318340633438, 0.24640657186997744, 0.24476385963526104, 0.24476386004527248, 0.24763860354922881, 0.24969199398949407, 0.2521560567605177, 0.24804928077564592, 0.2525667341827612, 0.24188911591711965, 0.2476386031575761, 0.24969199242288326, 0.2525667371201564, 0.24106776285342857, 0.2439425055740795, 0.24928131519646615, 0.24928131382568172, 0.2533880882072253, 0.25215605854131357, 0.24969199146211024, 0.2459958936644286, 0.24065708464787972, 0.2472279267144644, 0.24435318280049662, 0.25010267025513816, 0.24024640683398354, 0.247227925931159, 0.2476386043325342, 0.24065708325873655, 0.24394250559243824, 0.24804928332138845, 0.24229979392684217, 0.24435318360216074, 0.2431211507479513, 0.2464065728491092, 0.24681724948804726, 0.24887063601178555, 0.2464065712824984, 0.24517453727168959, 0.24722792495202725, 0.24640657108667205, 0.24312115055212494, 0.2422997941226685, 0.24558521604635877, 0.24394250520078553, 0.2468172492922209, 0.24845996015615288, 0.24763860395924023, 0.24558521643801146, 0.24599589088614227, 0.24640657186997744, 0.24722792749776978, 0.2517453799257533, 0.2525667373159828, 0.24558521643801146, 0.24640657071337807, 0.24928131304237633, 0.24804928214643035, 0.24722792495202725, 0.2496919926187096, 0.25174538053159107, 0.2427104719549234, 0.2459958920978178, 0.2492813142173344, 0.2513347025035098, 0.2468172492922209, 0.2505133472673702, 0.2464065720658038, 0.24887063757839634, 0.2509240254912778, 0.2525667353577193, 0.24558521526305338, 0.2521560555488422, 0.25338809000637985, 0.252566735749372, 0.2505133480506756, 0.24969199144375154, 0.2501026684743423, 0.24887063759675507, 0.25297741493405257, 0.2513347044617733, 0.2517453795341006, 0.24928131382568172, 0.2501026704509645, 0.25133470328681523, 0.24845995999704396, 0.2513347025035098, 0.250924024512146, 0.2517453775391197, 0.24558521487140067, 0.25297741493405257, 0.2521560583454872, 0.2537987676244497, 0.24763860175007424, 0.25051334824650195, 0.24969199124792518, 0.24722792573533264, 0.2517453809232438, 0.25215605754382314, 0.2509240268437035, 0.2484599582162481, 0.2435318263893989, 0.24435318338797568, 0.25092402371048195, 0.2509240254912778, 0.2517453809232438, 0.25462012266476297, 0.24928131441316076, 0.2546201236438947, 0.2521560573479968, 0.2509240256687454, 0.2505133470899026, 0.2521560583454872, 0.2525667345927726, 0.25954825408160076, 0.2529774121924837, 0.2418891154887495, 0.2554414794858721, 0.24435318340633438, 0.25174538168819044, 0.2468172487231006, 0.2529774125841364, 0.25092402529545144, 0.2480492813814837, 0.25133470211185716, 0.24887063855752808, 0.24928131460898711, 0.24763860276592342, 0.25462012383972105, 0.25585215430729685, 0.24558521626054383, 0.2501026704509645, 0.24599589346860223, 0.25051334630659716, 0.2505133468757175, 0.2517453811007114, 0.2480492813814837, 0.2509240239063083, 0.25420944543834584, 0.24887063421263098, 0.25338808899053067, 0.25297741238831006, 0.2525667349844253, 0.25544147927168703, 0.2558521567122892, 0.24599589483938666, 0.24887063601178555, 0.25503080167197595, 0.2542094460441836, 0.2533880892230745, 0.257084190135619, 0.2529774147382262, 0.2509240235146556, 0.25626283352869494, 0.25297741358162684, 0.2636550308497779, 0.2529774121924837, 0.2529774107849818, 0.2550308018678023, 0.2517453811007114, 0.25585215528642863, 0.253388090808044, 0.2562628348994794, 0.25667351212589645, 0.25544148064247146, 0.2521560579354758, 0.2566735113425911, 0.25913757920509983, 0.2546201220772839, 0.2513347009001816, 0.2533880892230745, 0.25379876938688684, 0.251745380904885, 0.2570841887648346, 0.24845996015615288, 0.2525667349660666]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
