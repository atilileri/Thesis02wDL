{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf28.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 06:24:42 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': '1', 'classificationMode': 'Speaker', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 15 Label(s): ['mb', 'by', 'eg', 'eb', 'yd', 'ds', 'sk', 'ck', 'aa', 'ib', 'ek', 'my', 'sg', 'ce', 'eo'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 15 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000016D0CA7E240>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000016D08236EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 15)                195       \n",
    "=================================================================\n",
    "Total params: 11,691\n",
    "Trainable params: 11,691\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:2.7078, Accuracy:0.0821, Validation Loss:2.6999, Validation Accuracy:0.1034\n",
    "Epoch #2: Loss:2.6971, Accuracy:0.0982, Validation Loss:2.6908, Validation Accuracy:0.0985\n",
    "Epoch #3: Loss:2.6883, Accuracy:0.1121, Validation Loss:2.6845, Validation Accuracy:0.1182\n",
    "Epoch #4: Loss:2.6814, Accuracy:0.1195, Validation Loss:2.6781, Validation Accuracy:0.1084\n",
    "Epoch #5: Loss:2.6755, Accuracy:0.1179, Validation Loss:2.6733, Validation Accuracy:0.0920\n",
    "Epoch #6: Loss:2.6698, Accuracy:0.1228, Validation Loss:2.6675, Validation Accuracy:0.1100\n",
    "Epoch #7: Loss:2.6637, Accuracy:0.1253, Validation Loss:2.6616, Validation Accuracy:0.1199\n",
    "Epoch #8: Loss:2.6573, Accuracy:0.1179, Validation Loss:2.6556, Validation Accuracy:0.1051\n",
    "Epoch #9: Loss:2.6481, Accuracy:0.1142, Validation Loss:2.6469, Validation Accuracy:0.1084\n",
    "Epoch #10: Loss:2.6385, Accuracy:0.1183, Validation Loss:2.6432, Validation Accuracy:0.1100\n",
    "Epoch #11: Loss:2.6319, Accuracy:0.1290, Validation Loss:2.6275, Validation Accuracy:0.1527\n",
    "Epoch #12: Loss:2.6130, Accuracy:0.1483, Validation Loss:2.6156, Validation Accuracy:0.1560\n",
    "Epoch #13: Loss:2.5913, Accuracy:0.1602, Validation Loss:2.5935, Validation Accuracy:0.1560\n",
    "Epoch #14: Loss:2.5699, Accuracy:0.1618, Validation Loss:2.5768, Validation Accuracy:0.1626\n",
    "Epoch #15: Loss:2.5488, Accuracy:0.1552, Validation Loss:2.5586, Validation Accuracy:0.1478\n",
    "Epoch #16: Loss:2.5285, Accuracy:0.1487, Validation Loss:2.5460, Validation Accuracy:0.1297\n",
    "Epoch #17: Loss:2.5115, Accuracy:0.1446, Validation Loss:2.5275, Validation Accuracy:0.1330\n",
    "Epoch #18: Loss:2.4929, Accuracy:0.1511, Validation Loss:2.5240, Validation Accuracy:0.1576\n",
    "Epoch #19: Loss:2.4923, Accuracy:0.1569, Validation Loss:2.5225, Validation Accuracy:0.1396\n",
    "Epoch #20: Loss:2.4824, Accuracy:0.1548, Validation Loss:2.5021, Validation Accuracy:0.1741\n",
    "Epoch #21: Loss:2.4703, Accuracy:0.1754, Validation Loss:2.4919, Validation Accuracy:0.1823\n",
    "Epoch #22: Loss:2.4531, Accuracy:0.1774, Validation Loss:2.4837, Validation Accuracy:0.1642\n",
    "Epoch #23: Loss:2.4545, Accuracy:0.1914, Validation Loss:2.4718, Validation Accuracy:0.1856\n",
    "Epoch #24: Loss:2.4504, Accuracy:0.1647, Validation Loss:2.4678, Validation Accuracy:0.1724\n",
    "Epoch #25: Loss:2.4445, Accuracy:0.1856, Validation Loss:2.4774, Validation Accuracy:0.1921\n",
    "Epoch #26: Loss:2.4416, Accuracy:0.1918, Validation Loss:2.4718, Validation Accuracy:0.1954\n",
    "Epoch #27: Loss:2.4452, Accuracy:0.1782, Validation Loss:2.4851, Validation Accuracy:0.1839\n",
    "Epoch #28: Loss:2.4531, Accuracy:0.1774, Validation Loss:2.4700, Validation Accuracy:0.1642\n",
    "Epoch #29: Loss:2.4466, Accuracy:0.1708, Validation Loss:2.4545, Validation Accuracy:0.1790\n",
    "Epoch #30: Loss:2.4350, Accuracy:0.1807, Validation Loss:2.4688, Validation Accuracy:0.1658\n",
    "Epoch #31: Loss:2.4385, Accuracy:0.1877, Validation Loss:2.4503, Validation Accuracy:0.1987\n",
    "Epoch #32: Loss:2.4448, Accuracy:0.1758, Validation Loss:2.4903, Validation Accuracy:0.1708\n",
    "Epoch #33: Loss:2.4588, Accuracy:0.1758, Validation Loss:2.4722, Validation Accuracy:0.1790\n",
    "Epoch #34: Loss:2.4638, Accuracy:0.1745, Validation Loss:2.4698, Validation Accuracy:0.1560\n",
    "Epoch #35: Loss:2.4550, Accuracy:0.1832, Validation Loss:2.4718, Validation Accuracy:0.1609\n",
    "Epoch #36: Loss:2.4515, Accuracy:0.1786, Validation Loss:2.4587, Validation Accuracy:0.1609\n",
    "Epoch #37: Loss:2.4502, Accuracy:0.1774, Validation Loss:2.4592, Validation Accuracy:0.1675\n",
    "Epoch #38: Loss:2.4459, Accuracy:0.1778, Validation Loss:2.4618, Validation Accuracy:0.1691\n",
    "Epoch #39: Loss:2.4462, Accuracy:0.1733, Validation Loss:2.4624, Validation Accuracy:0.1708\n",
    "Epoch #40: Loss:2.4453, Accuracy:0.1836, Validation Loss:2.4542, Validation Accuracy:0.1790\n",
    "Epoch #41: Loss:2.4441, Accuracy:0.1828, Validation Loss:2.4592, Validation Accuracy:0.1773\n",
    "Epoch #42: Loss:2.4448, Accuracy:0.1819, Validation Loss:2.4728, Validation Accuracy:0.1626\n",
    "Epoch #43: Loss:2.4470, Accuracy:0.1770, Validation Loss:2.4587, Validation Accuracy:0.1757\n",
    "Epoch #44: Loss:2.4482, Accuracy:0.1782, Validation Loss:2.4566, Validation Accuracy:0.1757\n",
    "Epoch #45: Loss:2.4418, Accuracy:0.1848, Validation Loss:2.4688, Validation Accuracy:0.1856\n",
    "Epoch #46: Loss:2.4328, Accuracy:0.1819, Validation Loss:2.4658, Validation Accuracy:0.1675\n",
    "Epoch #47: Loss:2.4505, Accuracy:0.1815, Validation Loss:2.4634, Validation Accuracy:0.1757\n",
    "Epoch #48: Loss:2.4546, Accuracy:0.1778, Validation Loss:2.4715, Validation Accuracy:0.1741\n",
    "Epoch #49: Loss:2.4591, Accuracy:0.1733, Validation Loss:2.4819, Validation Accuracy:0.1576\n",
    "Epoch #50: Loss:2.4579, Accuracy:0.1749, Validation Loss:2.4689, Validation Accuracy:0.1658\n",
    "Epoch #51: Loss:2.4508, Accuracy:0.1844, Validation Loss:2.4678, Validation Accuracy:0.1691\n",
    "Epoch #52: Loss:2.4502, Accuracy:0.1811, Validation Loss:2.4683, Validation Accuracy:0.1642\n",
    "Epoch #53: Loss:2.4492, Accuracy:0.1774, Validation Loss:2.4711, Validation Accuracy:0.1675\n",
    "Epoch #54: Loss:2.4496, Accuracy:0.1758, Validation Loss:2.4703, Validation Accuracy:0.1593\n",
    "Epoch #55: Loss:2.4490, Accuracy:0.1799, Validation Loss:2.4693, Validation Accuracy:0.1593\n",
    "Epoch #56: Loss:2.4473, Accuracy:0.1791, Validation Loss:2.4709, Validation Accuracy:0.1527\n",
    "Epoch #57: Loss:2.4468, Accuracy:0.1803, Validation Loss:2.4688, Validation Accuracy:0.1675\n",
    "Epoch #58: Loss:2.4463, Accuracy:0.1811, Validation Loss:2.4694, Validation Accuracy:0.1576\n",
    "Epoch #59: Loss:2.4463, Accuracy:0.1786, Validation Loss:2.4693, Validation Accuracy:0.1724\n",
    "Epoch #60: Loss:2.4450, Accuracy:0.1791, Validation Loss:2.4697, Validation Accuracy:0.1724\n",
    "Epoch #61: Loss:2.4445, Accuracy:0.1762, Validation Loss:2.4695, Validation Accuracy:0.1544\n",
    "Epoch #62: Loss:2.4431, Accuracy:0.1832, Validation Loss:2.4661, Validation Accuracy:0.1724\n",
    "Epoch #63: Loss:2.4428, Accuracy:0.1807, Validation Loss:2.4625, Validation Accuracy:0.1741\n",
    "Epoch #64: Loss:2.4407, Accuracy:0.1786, Validation Loss:2.4624, Validation Accuracy:0.1527\n",
    "Epoch #65: Loss:2.4412, Accuracy:0.1799, Validation Loss:2.4609, Validation Accuracy:0.1544\n",
    "Epoch #66: Loss:2.4442, Accuracy:0.1749, Validation Loss:2.4666, Validation Accuracy:0.1494\n",
    "Epoch #67: Loss:2.4449, Accuracy:0.1762, Validation Loss:2.4653, Validation Accuracy:0.1642\n",
    "Epoch #68: Loss:2.4440, Accuracy:0.1774, Validation Loss:2.4615, Validation Accuracy:0.1642\n",
    "Epoch #69: Loss:2.4445, Accuracy:0.1762, Validation Loss:2.4666, Validation Accuracy:0.1642\n",
    "Epoch #70: Loss:2.4458, Accuracy:0.1774, Validation Loss:2.4603, Validation Accuracy:0.1675\n",
    "Epoch #71: Loss:2.4425, Accuracy:0.1782, Validation Loss:2.4613, Validation Accuracy:0.1708\n",
    "Epoch #72: Loss:2.4401, Accuracy:0.1791, Validation Loss:2.4573, Validation Accuracy:0.1741\n",
    "Epoch #73: Loss:2.4402, Accuracy:0.1799, Validation Loss:2.4579, Validation Accuracy:0.1757\n",
    "Epoch #74: Loss:2.4425, Accuracy:0.1758, Validation Loss:2.5101, Validation Accuracy:0.1445\n",
    "Epoch #75: Loss:2.4710, Accuracy:0.1733, Validation Loss:2.4939, Validation Accuracy:0.1593\n",
    "Epoch #76: Loss:2.4839, Accuracy:0.1696, Validation Loss:2.4731, Validation Accuracy:0.1691\n",
    "Epoch #77: Loss:2.4611, Accuracy:0.1778, Validation Loss:2.4723, Validation Accuracy:0.1708\n",
    "Epoch #78: Loss:2.4522, Accuracy:0.1795, Validation Loss:2.4803, Validation Accuracy:0.1741\n",
    "Epoch #79: Loss:2.4486, Accuracy:0.1782, Validation Loss:2.4663, Validation Accuracy:0.1724\n",
    "Epoch #80: Loss:2.4476, Accuracy:0.1795, Validation Loss:2.4655, Validation Accuracy:0.1724\n",
    "Epoch #81: Loss:2.4432, Accuracy:0.1803, Validation Loss:2.4660, Validation Accuracy:0.1724\n",
    "Epoch #82: Loss:2.4414, Accuracy:0.1791, Validation Loss:2.4622, Validation Accuracy:0.1724\n",
    "Epoch #83: Loss:2.4467, Accuracy:0.1758, Validation Loss:2.4646, Validation Accuracy:0.1773\n",
    "Epoch #84: Loss:2.4468, Accuracy:0.1786, Validation Loss:2.4696, Validation Accuracy:0.1773\n",
    "Epoch #85: Loss:2.4459, Accuracy:0.1795, Validation Loss:2.4639, Validation Accuracy:0.1691\n",
    "Epoch #86: Loss:2.4432, Accuracy:0.1807, Validation Loss:2.4668, Validation Accuracy:0.1724\n",
    "Epoch #87: Loss:2.4432, Accuracy:0.1782, Validation Loss:2.4616, Validation Accuracy:0.1741\n",
    "Epoch #88: Loss:2.4409, Accuracy:0.1819, Validation Loss:2.4566, Validation Accuracy:0.1773\n",
    "Epoch #89: Loss:2.4412, Accuracy:0.1786, Validation Loss:2.4656, Validation Accuracy:0.1741\n",
    "Epoch #90: Loss:2.4424, Accuracy:0.1786, Validation Loss:2.4610, Validation Accuracy:0.1708\n",
    "Epoch #91: Loss:2.4454, Accuracy:0.1762, Validation Loss:2.4616, Validation Accuracy:0.1691\n",
    "Epoch #92: Loss:2.4455, Accuracy:0.1729, Validation Loss:2.4643, Validation Accuracy:0.1675\n",
    "Epoch #93: Loss:2.4440, Accuracy:0.1733, Validation Loss:2.4627, Validation Accuracy:0.1658\n",
    "Epoch #94: Loss:2.4441, Accuracy:0.1745, Validation Loss:2.4645, Validation Accuracy:0.1642\n",
    "Epoch #95: Loss:2.4443, Accuracy:0.1737, Validation Loss:2.4599, Validation Accuracy:0.1675\n",
    "Epoch #96: Loss:2.4428, Accuracy:0.1770, Validation Loss:2.4554, Validation Accuracy:0.1757\n",
    "Epoch #97: Loss:2.4400, Accuracy:0.1786, Validation Loss:2.4579, Validation Accuracy:0.1708\n",
    "Epoch #98: Loss:2.4405, Accuracy:0.1791, Validation Loss:2.4573, Validation Accuracy:0.1708\n",
    "Epoch #99: Loss:2.4364, Accuracy:0.1782, Validation Loss:2.4545, Validation Accuracy:0.1773\n",
    "Epoch #100: Loss:2.4396, Accuracy:0.1782, Validation Loss:2.4573, Validation Accuracy:0.1642\n",
    "Epoch #101: Loss:2.4395, Accuracy:0.1770, Validation Loss:2.4634, Validation Accuracy:0.1724\n",
    "Epoch #102: Loss:2.4415, Accuracy:0.1782, Validation Loss:2.4640, Validation Accuracy:0.1708\n",
    "Epoch #103: Loss:2.4423, Accuracy:0.1791, Validation Loss:2.4644, Validation Accuracy:0.1691\n",
    "Epoch #104: Loss:2.4431, Accuracy:0.1786, Validation Loss:2.4610, Validation Accuracy:0.1708\n",
    "Epoch #105: Loss:2.4449, Accuracy:0.1774, Validation Loss:2.4580, Validation Accuracy:0.1708\n",
    "Epoch #106: Loss:2.4443, Accuracy:0.1770, Validation Loss:2.4626, Validation Accuracy:0.1675\n",
    "Epoch #107: Loss:2.4437, Accuracy:0.1762, Validation Loss:2.4620, Validation Accuracy:0.1675\n",
    "Epoch #108: Loss:2.4435, Accuracy:0.1762, Validation Loss:2.4624, Validation Accuracy:0.1708\n",
    "Epoch #109: Loss:2.4444, Accuracy:0.1770, Validation Loss:2.4620, Validation Accuracy:0.1708\n",
    "Epoch #110: Loss:2.4434, Accuracy:0.1786, Validation Loss:2.4630, Validation Accuracy:0.1675\n",
    "Epoch #111: Loss:2.4434, Accuracy:0.1778, Validation Loss:2.4644, Validation Accuracy:0.1675\n",
    "Epoch #112: Loss:2.4439, Accuracy:0.1795, Validation Loss:2.4639, Validation Accuracy:0.1691\n",
    "Epoch #113: Loss:2.4440, Accuracy:0.1766, Validation Loss:2.4613, Validation Accuracy:0.1658\n",
    "Epoch #114: Loss:2.4434, Accuracy:0.1791, Validation Loss:2.4650, Validation Accuracy:0.1576\n",
    "Epoch #115: Loss:2.4413, Accuracy:0.1786, Validation Loss:2.4640, Validation Accuracy:0.1675\n",
    "Epoch #116: Loss:2.4415, Accuracy:0.1778, Validation Loss:2.4657, Validation Accuracy:0.1658\n",
    "Epoch #117: Loss:2.4407, Accuracy:0.1782, Validation Loss:2.4660, Validation Accuracy:0.1658\n",
    "Epoch #118: Loss:2.4404, Accuracy:0.1778, Validation Loss:2.4671, Validation Accuracy:0.1658\n",
    "Epoch #119: Loss:2.4420, Accuracy:0.1786, Validation Loss:2.4678, Validation Accuracy:0.1658\n",
    "Epoch #120: Loss:2.4412, Accuracy:0.1770, Validation Loss:2.4659, Validation Accuracy:0.1658\n",
    "Epoch #121: Loss:2.4409, Accuracy:0.1766, Validation Loss:2.4666, Validation Accuracy:0.1691\n",
    "Epoch #122: Loss:2.4415, Accuracy:0.1782, Validation Loss:2.4657, Validation Accuracy:0.1658\n",
    "Epoch #123: Loss:2.4413, Accuracy:0.1807, Validation Loss:2.4680, Validation Accuracy:0.1658\n",
    "Epoch #124: Loss:2.4411, Accuracy:0.1778, Validation Loss:2.4683, Validation Accuracy:0.1658\n",
    "Epoch #125: Loss:2.4418, Accuracy:0.1799, Validation Loss:2.4670, Validation Accuracy:0.1658\n",
    "Epoch #126: Loss:2.4400, Accuracy:0.1774, Validation Loss:2.4650, Validation Accuracy:0.1658\n",
    "Epoch #127: Loss:2.4415, Accuracy:0.1786, Validation Loss:2.4651, Validation Accuracy:0.1658\n",
    "Epoch #128: Loss:2.4403, Accuracy:0.1778, Validation Loss:2.4682, Validation Accuracy:0.1658\n",
    "Epoch #129: Loss:2.4398, Accuracy:0.1766, Validation Loss:2.4667, Validation Accuracy:0.1658\n",
    "Epoch #130: Loss:2.4391, Accuracy:0.1766, Validation Loss:2.4675, Validation Accuracy:0.1658\n",
    "Epoch #131: Loss:2.4402, Accuracy:0.1770, Validation Loss:2.4653, Validation Accuracy:0.1658\n",
    "Epoch #132: Loss:2.4396, Accuracy:0.1770, Validation Loss:2.4659, Validation Accuracy:0.1658\n",
    "Epoch #133: Loss:2.4403, Accuracy:0.1766, Validation Loss:2.4649, Validation Accuracy:0.1658\n",
    "Epoch #134: Loss:2.4403, Accuracy:0.1766, Validation Loss:2.4665, Validation Accuracy:0.1658\n",
    "Epoch #135: Loss:2.4393, Accuracy:0.1766, Validation Loss:2.4663, Validation Accuracy:0.1658\n",
    "Epoch #136: Loss:2.4403, Accuracy:0.1770, Validation Loss:2.4660, Validation Accuracy:0.1658\n",
    "Epoch #137: Loss:2.4389, Accuracy:0.1770, Validation Loss:2.4673, Validation Accuracy:0.1658\n",
    "Epoch #138: Loss:2.4393, Accuracy:0.1766, Validation Loss:2.4651, Validation Accuracy:0.1658\n",
    "Epoch #139: Loss:2.4392, Accuracy:0.1766, Validation Loss:2.4662, Validation Accuracy:0.1658\n",
    "Epoch #140: Loss:2.4401, Accuracy:0.1766, Validation Loss:2.4665, Validation Accuracy:0.1658\n",
    "Epoch #141: Loss:2.4389, Accuracy:0.1766, Validation Loss:2.4664, Validation Accuracy:0.1658\n",
    "Epoch #142: Loss:2.4390, Accuracy:0.1774, Validation Loss:2.4682, Validation Accuracy:0.1658\n",
    "Epoch #143: Loss:2.4398, Accuracy:0.1766, Validation Loss:2.4681, Validation Accuracy:0.1642\n",
    "Epoch #144: Loss:2.4394, Accuracy:0.1770, Validation Loss:2.4674, Validation Accuracy:0.1642\n",
    "Epoch #145: Loss:2.4383, Accuracy:0.1758, Validation Loss:2.4674, Validation Accuracy:0.1642\n",
    "Epoch #146: Loss:2.4390, Accuracy:0.1782, Validation Loss:2.4716, Validation Accuracy:0.1576\n",
    "Epoch #147: Loss:2.4376, Accuracy:0.1778, Validation Loss:2.4678, Validation Accuracy:0.1642\n",
    "Epoch #148: Loss:2.4393, Accuracy:0.1782, Validation Loss:2.4681, Validation Accuracy:0.1642\n",
    "Epoch #149: Loss:2.4411, Accuracy:0.1762, Validation Loss:2.4689, Validation Accuracy:0.1642\n",
    "Epoch #150: Loss:2.4425, Accuracy:0.1766, Validation Loss:2.4667, Validation Accuracy:0.1642\n",
    "Epoch #151: Loss:2.4380, Accuracy:0.1754, Validation Loss:2.4703, Validation Accuracy:0.1642\n",
    "Epoch #152: Loss:2.4386, Accuracy:0.1754, Validation Loss:2.4656, Validation Accuracy:0.1642\n",
    "Epoch #153: Loss:2.4375, Accuracy:0.1758, Validation Loss:2.4683, Validation Accuracy:0.1642\n",
    "Epoch #154: Loss:2.4379, Accuracy:0.1762, Validation Loss:2.4662, Validation Accuracy:0.1642\n",
    "Epoch #155: Loss:2.4371, Accuracy:0.1762, Validation Loss:2.4671, Validation Accuracy:0.1642\n",
    "Epoch #156: Loss:2.4373, Accuracy:0.1762, Validation Loss:2.4661, Validation Accuracy:0.1642\n",
    "Epoch #157: Loss:2.4379, Accuracy:0.1766, Validation Loss:2.4638, Validation Accuracy:0.1642\n",
    "Epoch #158: Loss:2.4379, Accuracy:0.1778, Validation Loss:2.4645, Validation Accuracy:0.1642\n",
    "Epoch #159: Loss:2.4377, Accuracy:0.1766, Validation Loss:2.4661, Validation Accuracy:0.1642\n",
    "Epoch #160: Loss:2.4388, Accuracy:0.1815, Validation Loss:2.4649, Validation Accuracy:0.1691\n",
    "Epoch #161: Loss:2.4424, Accuracy:0.1778, Validation Loss:2.4636, Validation Accuracy:0.1675\n",
    "Epoch #162: Loss:2.4413, Accuracy:0.1774, Validation Loss:2.4651, Validation Accuracy:0.1675\n",
    "Epoch #163: Loss:2.4405, Accuracy:0.1803, Validation Loss:2.4635, Validation Accuracy:0.1708\n",
    "Epoch #164: Loss:2.4396, Accuracy:0.1770, Validation Loss:2.4684, Validation Accuracy:0.1675\n",
    "Epoch #165: Loss:2.4398, Accuracy:0.1791, Validation Loss:2.4646, Validation Accuracy:0.1708\n",
    "Epoch #166: Loss:2.4375, Accuracy:0.1774, Validation Loss:2.4687, Validation Accuracy:0.1675\n",
    "Epoch #167: Loss:2.4391, Accuracy:0.1778, Validation Loss:2.4652, Validation Accuracy:0.1708\n",
    "Epoch #168: Loss:2.4421, Accuracy:0.1786, Validation Loss:2.4649, Validation Accuracy:0.1675\n",
    "Epoch #169: Loss:2.4416, Accuracy:0.1766, Validation Loss:2.4632, Validation Accuracy:0.1708\n",
    "Epoch #170: Loss:2.4415, Accuracy:0.1770, Validation Loss:2.4637, Validation Accuracy:0.1708\n",
    "Epoch #171: Loss:2.4408, Accuracy:0.1762, Validation Loss:2.4664, Validation Accuracy:0.1691\n",
    "Epoch #172: Loss:2.4400, Accuracy:0.1754, Validation Loss:2.4651, Validation Accuracy:0.1724\n",
    "Epoch #173: Loss:2.4405, Accuracy:0.1762, Validation Loss:2.4665, Validation Accuracy:0.1724\n",
    "Epoch #174: Loss:2.4403, Accuracy:0.1754, Validation Loss:2.4627, Validation Accuracy:0.1708\n",
    "Epoch #175: Loss:2.4423, Accuracy:0.1762, Validation Loss:2.4631, Validation Accuracy:0.1708\n",
    "Epoch #176: Loss:2.4416, Accuracy:0.1749, Validation Loss:2.4662, Validation Accuracy:0.1708\n",
    "Epoch #177: Loss:2.4395, Accuracy:0.1786, Validation Loss:2.4653, Validation Accuracy:0.1691\n",
    "Epoch #178: Loss:2.4378, Accuracy:0.1782, Validation Loss:2.4647, Validation Accuracy:0.1691\n",
    "Epoch #179: Loss:2.4381, Accuracy:0.1786, Validation Loss:2.4666, Validation Accuracy:0.1691\n",
    "Epoch #180: Loss:2.4379, Accuracy:0.1791, Validation Loss:2.4667, Validation Accuracy:0.1691\n",
    "Epoch #181: Loss:2.4381, Accuracy:0.1791, Validation Loss:2.4647, Validation Accuracy:0.1691\n",
    "Epoch #182: Loss:2.4389, Accuracy:0.1766, Validation Loss:2.4667, Validation Accuracy:0.1675\n",
    "Epoch #183: Loss:2.4374, Accuracy:0.1774, Validation Loss:2.4655, Validation Accuracy:0.1691\n",
    "Epoch #184: Loss:2.4382, Accuracy:0.1778, Validation Loss:2.4654, Validation Accuracy:0.1691\n",
    "Epoch #185: Loss:2.4371, Accuracy:0.1791, Validation Loss:2.4653, Validation Accuracy:0.1691\n",
    "Epoch #186: Loss:2.4374, Accuracy:0.1791, Validation Loss:2.4651, Validation Accuracy:0.1691\n",
    "Epoch #187: Loss:2.4376, Accuracy:0.1791, Validation Loss:2.4655, Validation Accuracy:0.1691\n",
    "Epoch #188: Loss:2.4378, Accuracy:0.1778, Validation Loss:2.4645, Validation Accuracy:0.1691\n",
    "Epoch #189: Loss:2.4373, Accuracy:0.1791, Validation Loss:2.4646, Validation Accuracy:0.1691\n",
    "Epoch #190: Loss:2.4372, Accuracy:0.1791, Validation Loss:2.4677, Validation Accuracy:0.1691\n",
    "Epoch #191: Loss:2.4364, Accuracy:0.1791, Validation Loss:2.4650, Validation Accuracy:0.1691\n",
    "Epoch #192: Loss:2.4364, Accuracy:0.1791, Validation Loss:2.4660, Validation Accuracy:0.1691\n",
    "Epoch #193: Loss:2.4377, Accuracy:0.1795, Validation Loss:2.4665, Validation Accuracy:0.1691\n",
    "Epoch #194: Loss:2.4366, Accuracy:0.1774, Validation Loss:2.4660, Validation Accuracy:0.1691\n",
    "Epoch #195: Loss:2.4363, Accuracy:0.1778, Validation Loss:2.4646, Validation Accuracy:0.1691\n",
    "Epoch #196: Loss:2.4372, Accuracy:0.1791, Validation Loss:2.4648, Validation Accuracy:0.1691\n",
    "Epoch #197: Loss:2.4366, Accuracy:0.1791, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #198: Loss:2.4370, Accuracy:0.1791, Validation Loss:2.4646, Validation Accuracy:0.1691\n",
    "Epoch #199: Loss:2.4372, Accuracy:0.1791, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #200: Loss:2.4360, Accuracy:0.1791, Validation Loss:2.4676, Validation Accuracy:0.1691\n",
    "Epoch #201: Loss:2.4357, Accuracy:0.1791, Validation Loss:2.4663, Validation Accuracy:0.1691\n",
    "Epoch #202: Loss:2.4359, Accuracy:0.1791, Validation Loss:2.4668, Validation Accuracy:0.1691\n",
    "Epoch #203: Loss:2.4358, Accuracy:0.1791, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #204: Loss:2.4361, Accuracy:0.1791, Validation Loss:2.4668, Validation Accuracy:0.1691\n",
    "Epoch #205: Loss:2.4377, Accuracy:0.1774, Validation Loss:2.4662, Validation Accuracy:0.1691\n",
    "Epoch #206: Loss:2.4372, Accuracy:0.1795, Validation Loss:2.4662, Validation Accuracy:0.1691\n",
    "Epoch #207: Loss:2.4364, Accuracy:0.1778, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #208: Loss:2.4359, Accuracy:0.1786, Validation Loss:2.4659, Validation Accuracy:0.1691\n",
    "Epoch #209: Loss:2.4364, Accuracy:0.1791, Validation Loss:2.4658, Validation Accuracy:0.1691\n",
    "Epoch #210: Loss:2.4367, Accuracy:0.1782, Validation Loss:2.4642, Validation Accuracy:0.1691\n",
    "Epoch #211: Loss:2.4366, Accuracy:0.1791, Validation Loss:2.4672, Validation Accuracy:0.1691\n",
    "Epoch #212: Loss:2.4357, Accuracy:0.1795, Validation Loss:2.4658, Validation Accuracy:0.1691\n",
    "Epoch #213: Loss:2.4360, Accuracy:0.1791, Validation Loss:2.4672, Validation Accuracy:0.1691\n",
    "Epoch #214: Loss:2.4372, Accuracy:0.1774, Validation Loss:2.4669, Validation Accuracy:0.1691\n",
    "Epoch #215: Loss:2.4355, Accuracy:0.1795, Validation Loss:2.4653, Validation Accuracy:0.1691\n",
    "Epoch #216: Loss:2.4365, Accuracy:0.1795, Validation Loss:2.4658, Validation Accuracy:0.1691\n",
    "Epoch #217: Loss:2.4371, Accuracy:0.1786, Validation Loss:2.4684, Validation Accuracy:0.1691\n",
    "Epoch #218: Loss:2.4360, Accuracy:0.1791, Validation Loss:2.4664, Validation Accuracy:0.1691\n",
    "Epoch #219: Loss:2.4347, Accuracy:0.1791, Validation Loss:2.4643, Validation Accuracy:0.1675\n",
    "Epoch #220: Loss:2.4366, Accuracy:0.1786, Validation Loss:2.4677, Validation Accuracy:0.1691\n",
    "Epoch #221: Loss:2.4372, Accuracy:0.1791, Validation Loss:2.4657, Validation Accuracy:0.1675\n",
    "Epoch #222: Loss:2.4370, Accuracy:0.1803, Validation Loss:2.4672, Validation Accuracy:0.1691\n",
    "Epoch #223: Loss:2.4349, Accuracy:0.1791, Validation Loss:2.4664, Validation Accuracy:0.1691\n",
    "Epoch #224: Loss:2.4355, Accuracy:0.1778, Validation Loss:2.4646, Validation Accuracy:0.1691\n",
    "Epoch #225: Loss:2.4351, Accuracy:0.1799, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #226: Loss:2.4350, Accuracy:0.1795, Validation Loss:2.4673, Validation Accuracy:0.1691\n",
    "Epoch #227: Loss:2.4340, Accuracy:0.1795, Validation Loss:2.4670, Validation Accuracy:0.1691\n",
    "Epoch #228: Loss:2.4343, Accuracy:0.1795, Validation Loss:2.4640, Validation Accuracy:0.1691\n",
    "Epoch #229: Loss:2.4343, Accuracy:0.1786, Validation Loss:2.4672, Validation Accuracy:0.1691\n",
    "Epoch #230: Loss:2.4362, Accuracy:0.1778, Validation Loss:2.4671, Validation Accuracy:0.1724\n",
    "Epoch #231: Loss:2.4391, Accuracy:0.1786, Validation Loss:2.4700, Validation Accuracy:0.1675\n",
    "Epoch #232: Loss:2.4398, Accuracy:0.1770, Validation Loss:2.4660, Validation Accuracy:0.1675\n",
    "Epoch #233: Loss:2.4347, Accuracy:0.1786, Validation Loss:2.4658, Validation Accuracy:0.1675\n",
    "Epoch #234: Loss:2.4346, Accuracy:0.1786, Validation Loss:2.4657, Validation Accuracy:0.1691\n",
    "Epoch #235: Loss:2.4339, Accuracy:0.1782, Validation Loss:2.4653, Validation Accuracy:0.1724\n",
    "Epoch #236: Loss:2.4348, Accuracy:0.1791, Validation Loss:2.4668, Validation Accuracy:0.1691\n",
    "Epoch #237: Loss:2.4347, Accuracy:0.1795, Validation Loss:2.4640, Validation Accuracy:0.1691\n",
    "Epoch #238: Loss:2.4382, Accuracy:0.1795, Validation Loss:2.4664, Validation Accuracy:0.1675\n",
    "Epoch #239: Loss:2.4361, Accuracy:0.1786, Validation Loss:2.4668, Validation Accuracy:0.1708\n",
    "Epoch #240: Loss:2.4348, Accuracy:0.1795, Validation Loss:2.4664, Validation Accuracy:0.1741\n",
    "Epoch #241: Loss:2.4339, Accuracy:0.1819, Validation Loss:2.4647, Validation Accuracy:0.1675\n",
    "Epoch #242: Loss:2.4363, Accuracy:0.1795, Validation Loss:2.4685, Validation Accuracy:0.1675\n",
    "Epoch #243: Loss:2.4346, Accuracy:0.1791, Validation Loss:2.4653, Validation Accuracy:0.1691\n",
    "Epoch #244: Loss:2.4341, Accuracy:0.1782, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #245: Loss:2.4348, Accuracy:0.1795, Validation Loss:2.4657, Validation Accuracy:0.1691\n",
    "Epoch #246: Loss:2.4344, Accuracy:0.1786, Validation Loss:2.4661, Validation Accuracy:0.1691\n",
    "Epoch #247: Loss:2.4340, Accuracy:0.1786, Validation Loss:2.4665, Validation Accuracy:0.1675\n",
    "Epoch #248: Loss:2.4345, Accuracy:0.1791, Validation Loss:2.4691, Validation Accuracy:0.1675\n",
    "Epoch #249: Loss:2.4351, Accuracy:0.1799, Validation Loss:2.4643, Validation Accuracy:0.1691\n",
    "Epoch #250: Loss:2.4343, Accuracy:0.1770, Validation Loss:2.4665, Validation Accuracy:0.1691\n",
    "Epoch #251: Loss:2.4342, Accuracy:0.1791, Validation Loss:2.4673, Validation Accuracy:0.1675\n",
    "Epoch #252: Loss:2.4345, Accuracy:0.1770, Validation Loss:2.4643, Validation Accuracy:0.1691\n",
    "Epoch #253: Loss:2.4334, Accuracy:0.1782, Validation Loss:2.4705, Validation Accuracy:0.1675\n",
    "Epoch #254: Loss:2.4349, Accuracy:0.1786, Validation Loss:2.4658, Validation Accuracy:0.1691\n",
    "Epoch #255: Loss:2.4337, Accuracy:0.1791, Validation Loss:2.4659, Validation Accuracy:0.1691\n",
    "Epoch #256: Loss:2.4328, Accuracy:0.1786, Validation Loss:2.4646, Validation Accuracy:0.1691\n",
    "Epoch #257: Loss:2.4333, Accuracy:0.1786, Validation Loss:2.4661, Validation Accuracy:0.1675\n",
    "Epoch #258: Loss:2.4337, Accuracy:0.1786, Validation Loss:2.4663, Validation Accuracy:0.1675\n",
    "Epoch #259: Loss:2.4330, Accuracy:0.1791, Validation Loss:2.4674, Validation Accuracy:0.1675\n",
    "Epoch #260: Loss:2.4326, Accuracy:0.1782, Validation Loss:2.4663, Validation Accuracy:0.1691\n",
    "Epoch #261: Loss:2.4330, Accuracy:0.1774, Validation Loss:2.4669, Validation Accuracy:0.1675\n",
    "Epoch #262: Loss:2.4334, Accuracy:0.1786, Validation Loss:2.4671, Validation Accuracy:0.1691\n",
    "Epoch #263: Loss:2.4339, Accuracy:0.1778, Validation Loss:2.4657, Validation Accuracy:0.1675\n",
    "Epoch #264: Loss:2.4374, Accuracy:0.1823, Validation Loss:2.4640, Validation Accuracy:0.1691\n",
    "Epoch #265: Loss:2.4361, Accuracy:0.1774, Validation Loss:2.4696, Validation Accuracy:0.1675\n",
    "Epoch #266: Loss:2.4361, Accuracy:0.1758, Validation Loss:2.4670, Validation Accuracy:0.1691\n",
    "Epoch #267: Loss:2.4332, Accuracy:0.1774, Validation Loss:2.4658, Validation Accuracy:0.1691\n",
    "Epoch #268: Loss:2.4351, Accuracy:0.1786, Validation Loss:2.4656, Validation Accuracy:0.1691\n",
    "Epoch #269: Loss:2.4353, Accuracy:0.1778, Validation Loss:2.4668, Validation Accuracy:0.1675\n",
    "Epoch #270: Loss:2.4351, Accuracy:0.1786, Validation Loss:2.4670, Validation Accuracy:0.1691\n",
    "Epoch #271: Loss:2.4337, Accuracy:0.1786, Validation Loss:2.4662, Validation Accuracy:0.1691\n",
    "Epoch #272: Loss:2.4350, Accuracy:0.1791, Validation Loss:2.4638, Validation Accuracy:0.1691\n",
    "Epoch #273: Loss:2.4326, Accuracy:0.1774, Validation Loss:2.4646, Validation Accuracy:0.1691\n",
    "Epoch #274: Loss:2.4321, Accuracy:0.1774, Validation Loss:2.4680, Validation Accuracy:0.1691\n",
    "Epoch #275: Loss:2.4338, Accuracy:0.1799, Validation Loss:2.4642, Validation Accuracy:0.1773\n",
    "Epoch #276: Loss:2.4336, Accuracy:0.1795, Validation Loss:2.4660, Validation Accuracy:0.1757\n",
    "Epoch #277: Loss:2.4343, Accuracy:0.1786, Validation Loss:2.4666, Validation Accuracy:0.1724\n",
    "Epoch #278: Loss:2.4323, Accuracy:0.1778, Validation Loss:2.4677, Validation Accuracy:0.1691\n",
    "Epoch #279: Loss:2.4323, Accuracy:0.1754, Validation Loss:2.4645, Validation Accuracy:0.1691\n",
    "Epoch #280: Loss:2.4343, Accuracy:0.1778, Validation Loss:2.4685, Validation Accuracy:0.1806\n",
    "Epoch #281: Loss:2.4357, Accuracy:0.1807, Validation Loss:2.4671, Validation Accuracy:0.1675\n",
    "Epoch #282: Loss:2.4316, Accuracy:0.1786, Validation Loss:2.4661, Validation Accuracy:0.1675\n",
    "Epoch #283: Loss:2.4322, Accuracy:0.1786, Validation Loss:2.4656, Validation Accuracy:0.1675\n",
    "Epoch #284: Loss:2.4326, Accuracy:0.1778, Validation Loss:2.4658, Validation Accuracy:0.1675\n",
    "Epoch #285: Loss:2.4336, Accuracy:0.1815, Validation Loss:2.4678, Validation Accuracy:0.1675\n",
    "Epoch #286: Loss:2.4337, Accuracy:0.1762, Validation Loss:2.4665, Validation Accuracy:0.1757\n",
    "Epoch #287: Loss:2.4321, Accuracy:0.1811, Validation Loss:2.4650, Validation Accuracy:0.1658\n",
    "Epoch #288: Loss:2.4322, Accuracy:0.1786, Validation Loss:2.4663, Validation Accuracy:0.1691\n",
    "Epoch #289: Loss:2.4332, Accuracy:0.1778, Validation Loss:2.4703, Validation Accuracy:0.1773\n",
    "Epoch #290: Loss:2.4318, Accuracy:0.1791, Validation Loss:2.4635, Validation Accuracy:0.1691\n",
    "Epoch #291: Loss:2.4337, Accuracy:0.1828, Validation Loss:2.4653, Validation Accuracy:0.1675\n",
    "Epoch #292: Loss:2.4332, Accuracy:0.1803, Validation Loss:2.4677, Validation Accuracy:0.1691\n",
    "Epoch #293: Loss:2.4318, Accuracy:0.1774, Validation Loss:2.4681, Validation Accuracy:0.1658\n",
    "Epoch #294: Loss:2.4313, Accuracy:0.1807, Validation Loss:2.4637, Validation Accuracy:0.1691\n",
    "Epoch #295: Loss:2.4335, Accuracy:0.1766, Validation Loss:2.4696, Validation Accuracy:0.1806\n",
    "Epoch #296: Loss:2.4335, Accuracy:0.1819, Validation Loss:2.4663, Validation Accuracy:0.1691\n",
    "Epoch #297: Loss:2.4341, Accuracy:0.1778, Validation Loss:2.4677, Validation Accuracy:0.1773\n",
    "Epoch #298: Loss:2.4342, Accuracy:0.1811, Validation Loss:2.4665, Validation Accuracy:0.1773\n",
    "Epoch #299: Loss:2.4310, Accuracy:0.1778, Validation Loss:2.4647, Validation Accuracy:0.1691\n",
    "Epoch #300: Loss:2.4330, Accuracy:0.1811, Validation Loss:2.4683, Validation Accuracy:0.1757\n",
    "\n",
    "Test:\n",
    "Test Loss:2.46834874, Accuracy:0.1757\n",
    "Labels: ['mb', 'by', 'eg', 'eb', 'yd', 'ds', 'sk', 'ck', 'aa', 'ib', 'ek', 'my', 'sg', 'ce', 'eo']\n",
    "Confusion Matrix:\n",
    "      mb  by  eg  eb  yd  ds  sk  ck  aa  ib  ek  my  sg  ce  eo\n",
    "t:mb   0   1  14   0   8   3   0   0   0   0   0   0  26   0   0\n",
    "t:by   0   5  11   1   6   1   0   0   0   0   0   0  16   0   0\n",
    "t:eg   0   4  31   1   2   4   0   0   0   0   0   0   8   0   0\n",
    "t:eb   0   4  15   0  11   0   0   0   0   0   0   0  20   0   0\n",
    "t:yd   0   1   2   0  31   0   0   0   0   0   0   0  28   0   0\n",
    "t:ds   0   1  14   1   1   9   0   0   0   0   0   0   5   0   0\n",
    "t:sk   0   4  13   1   2   3   0   0   0   0   0   0  10   0   0\n",
    "t:ck   0   2  11   0   0   2   0   0   0   0   0   0   8   0   0\n",
    "t:aa   0   4  18   1   1   5   0   0   0   0   0   0   5   0   0\n",
    "t:ib   0   2   7   1  27   2   0   0   0   0   0   0  15   0   0\n",
    "t:ek   0   2  22   1   2   2   0   0   0   0   0   0  19   0   0\n",
    "t:my   0   3   4   0   4   3   0   0   0   0   0   0   6   0   0\n",
    "t:sg   0   3   1   1  15   0   0   0   0   0   0   0  31   0   0\n",
    "t:ce   0   0  10   0   2   2   0   0   0   0   0   0  13   0   0\n",
    "t:eo   0   4   3   0   2   0   0   0   0   0   0   0  25   0   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          mb       0.00      0.00      0.00        52\n",
    "          by       0.12      0.12      0.12        40\n",
    "          eg       0.18      0.62      0.27        50\n",
    "          eb       0.00      0.00      0.00        50\n",
    "          yd       0.27      0.50      0.35        62\n",
    "          ds       0.25      0.29      0.27        31\n",
    "          sk       0.00      0.00      0.00        33\n",
    "          ck       0.00      0.00      0.00        23\n",
    "          aa       0.00      0.00      0.00        34\n",
    "          ib       0.00      0.00      0.00        54\n",
    "          ek       0.00      0.00      0.00        48\n",
    "          my       0.00      0.00      0.00        20\n",
    "          sg       0.13      0.61      0.22        51\n",
    "          ce       0.00      0.00      0.00        27\n",
    "          eo       0.00      0.00      0.00        34\n",
    "\n",
    "    accuracy                           0.18       609\n",
    "   macro avg       0.06      0.14      0.08       609\n",
    "weighted avg       0.07      0.18      0.10       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 06:40:30 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 48 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [2.699903515955106, 2.6908239449186278, 2.684523878426387, 2.678107081962924, 2.6733274827841274, 2.6674804170730666, 2.661639299690234, 2.655564044692442, 2.6468638261942243, 2.6432268235875274, 2.62750355285181, 2.6156373247137212, 2.593547158249102, 2.5768357360695773, 2.558582608139965, 2.546008493708468, 2.527532927116933, 2.524004115651198, 2.5224782120809572, 2.5020589949853704, 2.491902812556876, 2.4837018218142255, 2.4718220190853124, 2.467843630435236, 2.4774045905064677, 2.471844344695018, 2.485062165800574, 2.4700442519289716, 2.4545491912290576, 2.4688319803654464, 2.4502981566443234, 2.4903343365893185, 2.472195862157787, 2.46983313795381, 2.471782108050066, 2.458652123637583, 2.459237023527399, 2.461768447080465, 2.4624061835027486, 2.4542013729734373, 2.4592090052336895, 2.4727751531428694, 2.458661894492915, 2.456591931861414, 2.468757109884754, 2.4658411202955324, 2.4634441718679345, 2.4714854726650444, 2.4818979213977683, 2.4688566756757413, 2.467776110606828, 2.4682767485162893, 2.4711123468057665, 2.4703424419284064, 2.4692923689906427, 2.4709208716312654, 2.4688054845838123, 2.4693930203887238, 2.4693306608153094, 2.4696762757544057, 2.4694548667162315, 2.466129540614111, 2.4624959576893324, 2.462421140060049, 2.4609196276108816, 2.466557254932197, 2.4653165614467927, 2.461545190591922, 2.46659713269063, 2.4603185704580475, 2.4612749660347872, 2.457305381255001, 2.4579250640274073, 2.510131021242815, 2.49386020873372, 2.4730691408680383, 2.4723073649288985, 2.4803178968100714, 2.466264934571114, 2.46549781672473, 2.4660376202688234, 2.4621915175213993, 2.4646430731994178, 2.46961803741643, 2.463862969566057, 2.4668369007423787, 2.461633869384114, 2.4566258589426675, 2.4655771216343974, 2.4609556597441875, 2.4616128141656883, 2.4643422539402504, 2.4627279464051446, 2.464505500981373, 2.4598822738540975, 2.4553751221235554, 2.4579221814724024, 2.457337093274973, 2.454488290746028, 2.457282748715631, 2.463394787511215, 2.4639642179893158, 2.4644481677727157, 2.4610324199563762, 2.4580419502039064, 2.4625994176504453, 2.4620132978718074, 2.462399807665344, 2.461965462257122, 2.463019486522831, 2.4644104745391946, 2.46391058242184, 2.461292199900585, 2.4649785528042045, 2.4640061154545627, 2.4656712390323383, 2.4660226507922896, 2.467099535837158, 2.4677802428040403, 2.46586211874763, 2.4665839957877727, 2.465735395945156, 2.4680232657196095, 2.468282225488246, 2.467035847148676, 2.465011166038576, 2.465140265197002, 2.4682354946637584, 2.466742007015961, 2.4675163702033034, 2.4652577194282768, 2.465876325597904, 2.4648718661666895, 2.4664940372084945, 2.4662658811985763, 2.4660285952055983, 2.4672564125217633, 2.465060489322556, 2.46615712043687, 2.466500755209837, 2.4664290421114767, 2.4682097407593124, 2.4681293341913833, 2.467372686013408, 2.467444094139563, 2.471573839438177, 2.467817914114014, 2.4681141215983673, 2.4688884988794184, 2.4666881216766408, 2.4702547399085537, 2.4655933896896287, 2.4683164606736407, 2.4662052089553357, 2.4671328490590816, 2.4661436194465276, 2.4638491150585105, 2.4645110237578844, 2.466053369205769, 2.4648842212601836, 2.463583104128908, 2.465118870555082, 2.4635168957984312, 2.4683866046723866, 2.4645615062494386, 2.4686768802711723, 2.4652441565822105, 2.464873165528371, 2.4632233551570346, 2.4636994719701057, 2.4663634586021037, 2.465146927606492, 2.4664931062407094, 2.462747169049894, 2.463136755578428, 2.4662329281492186, 2.4652555466481227, 2.4647258760893873, 2.4666381900142174, 2.4667328926925784, 2.464671151391391, 2.4666973178218345, 2.46553972084534, 2.465350314314142, 2.4653307099647708, 2.4651131845264405, 2.4655194368659963, 2.464507636961287, 2.464580281810416, 2.467724255936095, 2.464967263743208, 2.4659614696095535, 2.466535572543716, 2.4660443269169, 2.464620601563227, 2.464836676915487, 2.466125195445294, 2.464646517545327, 2.4660626777091443, 2.4675645511138615, 2.466280368748557, 2.4667975409277556, 2.4661115738754398, 2.4667774945840066, 2.466222745052895, 2.466204906723574, 2.466081473627701, 2.4659455562460013, 2.4658095789660375, 2.4642307539095825, 2.467159978666133, 2.4658256293517615, 2.4671964328277287, 2.466878929357419, 2.465331441271677, 2.465837464935478, 2.46842232831006, 2.466435710001853, 2.4642954749622565, 2.4676725641260004, 2.4657410894121443, 2.467200208767294, 2.4664107958475747, 2.46457649960698, 2.4660646613987014, 2.4672859732936363, 2.466970370125105, 2.464042505802975, 2.4671701648943922, 2.46708542411942, 2.4700322581825196, 2.465991320476939, 2.465761724168248, 2.465680704132481, 2.46525702257266, 2.466797587515294, 2.4640466011999473, 2.466389038684137, 2.466836651753518, 2.4664053572418263, 2.4647380256496234, 2.4684594985103763, 2.4652998870229488, 2.466101650338259, 2.465724441609751, 2.4660687998597846, 2.4664690126534947, 2.469092924215132, 2.4643056647139425, 2.466500776741892, 2.46730579844445, 2.4642879113383676, 2.4705175018467145, 2.46582713816162, 2.465881987354047, 2.4645736581586264, 2.4660744177688323, 2.466267665618746, 2.4673542580972554, 2.4662522854671884, 2.466923918042864, 2.4670988980968205, 2.465679375800397, 2.464044315278628, 2.4695732605281133, 2.4669587831387574, 2.465793319523628, 2.46560207141444, 2.4667632928231273, 2.466979634781385, 2.4662163230194443, 2.4638479989150475, 2.4645715022126247, 2.467952686773341, 2.464169592301442, 2.4660313407384313, 2.466558029694706, 2.4676965383081795, 2.4644983308068635, 2.468511090490031, 2.467092021541251, 2.466093352667021, 2.4656325858606296, 2.4658089964260608, 2.4678189233801833, 2.4664505213156516, 2.4650439382186664, 2.4662955535456463, 2.470311243545833, 2.4635065968204994, 2.4653406303700165, 2.467671318398712, 2.468110540230286, 2.463670677347919, 2.469565744666239, 2.466260528329558, 2.467685829046716, 2.466498555025248, 2.464668172529374, 2.468348629173191], 'val_acc': [0.1034482755562159, 0.09852216727970464, 0.11822660037351555, 0.10837438352687409, 0.09195402188743472, 0.11001641965166885, 0.11986863659618327, 0.10509031088579268, 0.10837438323325516, 0.11001641945592289, 0.1527093589942052, 0.15599343143954067, 0.15599343134166768, 0.16256157603659263, 0.14778325061982098, 0.12972085324707877, 0.13300492549666826, 0.15763546766220837, 0.13957306980010128, 0.17405582900802882, 0.18226600963200254, 0.16420361216138737, 0.18555007989966418, 0.17241379080343325, 0.19211822657651698, 0.19540229655055968, 0.18390804377486944, 0.1642036118677684, 0.1789819354004852, 0.16584564809043614, 0.1986863690937681, 0.17077175695418528, 0.17898193530261222, 0.1559934312437947, 0.1609195398139249, 0.1609195397160519, 0.16748768441097686, 0.1691297205357716, 0.17077175675843934, 0.17898193738241305, 0.1773399013554913, 0.16256157593871964, 0.17569786523069655, 0.17569786523069655, 0.18555008178371907, 0.16748768441097686, 0.1756978653285695, 0.17405582920377477, 0.1576354675643354, 0.16584564838405508, 0.16912972073151755, 0.16420361235713332, 0.16748768441097686, 0.15927750378700312, 0.15927750368913016, 0.15270935918995115, 0.16748768441097686, 0.15763546736858944, 0.1724137927853611, 0.1724137927853611, 0.15435139502112696, 0.17241379307898003, 0.17405582920377477, 0.1527093589942052, 0.15435139511899995, 0.14942528664674273, 0.16420361235713332, 0.16420361225926033, 0.16420361225926033, 0.1674876846067228, 0.17077175695418528, 0.17405582920377477, 0.17569786523069655, 0.1444991782723585, 0.15927750359125717, 0.16912972073151755, 0.17077175695418528, 0.17405582920377477, 0.17241379307898003, 0.17241379307898003, 0.17241379307898003, 0.17241379298110704, 0.1773399013554913, 0.1773399013554913, 0.16912972082939054, 0.17241379307898003, 0.17405582920377477, 0.17733990145336426, 0.174055826928228, 0.1707717546786385, 0.16912971855384376, 0.16748768242904902, 0.16584564857980105, 0.1642036124550063, 0.16748768242904902, 0.1756978653285695, 0.1707717546786385, 0.1707717546786385, 0.17733990145336426, 0.1642036120635144, 0.17241379307898003, 0.1707717568563123, 0.16912972073151755, 0.1707717568563123, 0.1707717568563123, 0.1674876846067228, 0.1674876846067228, 0.1707717568563123, 0.1707717568563123, 0.1674876847045958, 0.1674876847045958, 0.16912972082939054, 0.16584564857980105, 0.1576354675643354, 0.1674876847045958, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16912972073151755, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16584564857980105, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.15763546785795435, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16420361235713332, 0.16912972073151755, 0.1674876846067228, 0.1674876846067228, 0.17077175695418528, 0.1674876846067228, 0.17077175695418528, 0.1674876846067228, 0.17077175695418528, 0.1674876846067228, 0.1707717568563123, 0.17077175695418528, 0.16912972073151755, 0.17241379080343325, 0.17241379080343325, 0.17077175695418528, 0.17077175695418528, 0.17077175695418528, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.1674876846067228, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.1674876847045958, 0.16912972082939054, 0.1674876847045958, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.17241379307898003, 0.1674876846067228, 0.1674876847045958, 0.1674876847045958, 0.16912971855384376, 0.17241379307898003, 0.16912972082939054, 0.16912972082939054, 0.1674876847045958, 0.17077175695418528, 0.17405582920377477, 0.1674876847045958, 0.1674876847045958, 0.16912971855384376, 0.16912972082939054, 0.16912972082939054, 0.16912972082939054, 0.1674876847045958, 0.1674876847045958, 0.16912971855384376, 0.16912971855384376, 0.1674876847045958, 0.16912971855384376, 0.1674876847045958, 0.16912971855384376, 0.16912972082939054, 0.16912971855384376, 0.1674876847045958, 0.1674876847045958, 0.1674876847045958, 0.16912971855384376, 0.1674876847045958, 0.16912971855384376, 0.1674876847045958, 0.16912971855384376, 0.1674876847045958, 0.16912971855384376, 0.16912971855384376, 0.16912971855384376, 0.1674876847045958, 0.16912971855384376, 0.16912971855384376, 0.16912971855384376, 0.16912971855384376, 0.16912971855384376, 0.17733989917781748, 0.1756978653285695, 0.17241379307898003, 0.16912971855384376, 0.16912971855384376, 0.18062397370295377, 0.1674876847045958, 0.1674876847045958, 0.1674876847045958, 0.1674876847045958, 0.1674876847045958, 0.1756978653285695, 0.16584564857980105, 0.16912971855384376, 0.17733989917781748, 0.16912971855384376, 0.1674876847045958, 0.16912971855384376, 0.16584564857980105, 0.16912971855384376, 0.18062397370295377, 0.16912971855384376, 0.17733989917781748, 0.17733989917781748, 0.16912971855384376, 0.1756978653285695], 'loss': [2.707756306846039, 2.697102882534082, 2.6882560906224184, 2.6813638128049564, 2.6754821577600874, 2.6697884820080393, 2.6637281139773266, 2.657319350311154, 2.6480500655007804, 2.6385278286630367, 2.6319359542408027, 2.612954104656556, 2.5913317846811283, 2.569880964623829, 2.5487765267154763, 2.528491145139847, 2.511466823711043, 2.4928788408361666, 2.4922907580585205, 2.4824230539725303, 2.470270596098851, 2.453059454375469, 2.454509704754338, 2.4504498453355668, 2.44446591665123, 2.441603413842297, 2.4451869634632213, 2.4531450855168964, 2.4466291804578026, 2.4350308635641174, 2.4385424615910902, 2.4448275652264666, 2.4588135422622397, 2.4638291313907694, 2.4550445962001164, 2.451522747545027, 2.4502446339115713, 2.4459015133444533, 2.4462226834385303, 2.4453456709272317, 2.44407674762013, 2.44479108365899, 2.4469885250381376, 2.4481657249482014, 2.4418400217130687, 2.4327861300239326, 2.4504568960632387, 2.4545844174998006, 2.4591107136415014, 2.457905322668244, 2.4508426777880783, 2.45017673651051, 2.4491605125168756, 2.449569812104932, 2.4490252361650096, 2.4473294565565045, 2.4467836690389646, 2.4462641961765486, 2.446343197910693, 2.445013823499425, 2.444453307098921, 2.443098979711043, 2.4427670700104573, 2.4406779535986804, 2.441172579818193, 2.4441935486372492, 2.4449392378452623, 2.4440298983203803, 2.44452500196453, 2.445797806158203, 2.442518440751814, 2.440067420603069, 2.440198618577491, 2.442521799663254, 2.471036206917107, 2.483896257647254, 2.461056702141889, 2.452175171125596, 2.448619460863744, 2.4476195066126953, 2.443220654943885, 2.441437027733429, 2.4467239642290117, 2.446781219545086, 2.445891201324776, 2.4431562453080007, 2.4432491010708977, 2.4408935560582843, 2.4411720997499, 2.442425276268679, 2.4454150509295767, 2.4454886729222793, 2.443999616713005, 2.44409226020014, 2.444326468219013, 2.442824628563632, 2.43998254887622, 2.4404635243347292, 2.4363875384203464, 2.439601499393001, 2.4395070106341854, 2.44151866617144, 2.442291830697344, 2.443084136269665, 2.444852221917812, 2.444261635989869, 2.4436634530766543, 2.4435182577285923, 2.4443530246217637, 2.4433577521870515, 2.443407038447793, 2.4438605171454273, 2.444047479609934, 2.443422590290986, 2.4412526553661182, 2.4415008585066276, 2.440668834846857, 2.4404488394147807, 2.4419961959674374, 2.441176765802215, 2.4409294344806085, 2.4414711214923273, 2.4413336996669885, 2.4411015282421387, 2.441821373953222, 2.4400357553846295, 2.4415372856343796, 2.4403250650213972, 2.439828949248766, 2.4391047967288038, 2.4402405098478406, 2.4396327154837105, 2.440345154846473, 2.4403473041385597, 2.439332590945203, 2.440279795942365, 2.4389154868938596, 2.4392893349610314, 2.43915994093893, 2.440119059717386, 2.4389057240691763, 2.4390341159499402, 2.439750905301292, 2.439383032091834, 2.4382928132521298, 2.4390476267930175, 2.437555919878292, 2.4392625604077285, 2.441137853344363, 2.4424993715736654, 2.437965543558955, 2.438607043652074, 2.4375048753906814, 2.4378594735319856, 2.4370811324344768, 2.43734734131815, 2.4378537620607097, 2.4378571978339916, 2.4377306784447703, 2.4388167428284944, 2.4423639469812537, 2.441251305339273, 2.4404855312018423, 2.439593327657398, 2.4398350355316727, 2.4374525861328875, 2.439094793918931, 2.4421116253188995, 2.441598736823707, 2.441547854186573, 2.440802274641315, 2.439980401444484, 2.440451506667558, 2.440255841484305, 2.442313136799869, 2.441611420790028, 2.4395099109203175, 2.437823752260306, 2.438083903549633, 2.4379397676221153, 2.43811552304262, 2.4389147353123346, 2.4374398279483804, 2.438223010950265, 2.4371048743230364, 2.437395382319143, 2.4376391293331827, 2.437836667007979, 2.4372707206365756, 2.4371705817980445, 2.4364209032156627, 2.436437649207928, 2.4376838000648076, 2.4366431992156796, 2.43627318826789, 2.4372156851100724, 2.4365596716408855, 2.437019825275429, 2.4371595013557763, 2.436042457537485, 2.4356931019612653, 2.435922749771964, 2.435833383830421, 2.4360757720054296, 2.4376619392841503, 2.4371860120330746, 2.436360942950239, 2.4359147283330835, 2.436356512379108, 2.43667616805012, 2.436643069089071, 2.435660556111737, 2.4360215075451737, 2.437178770864279, 2.4354643750239693, 2.4365298461130758, 2.4371089469236025, 2.4359877888916452, 2.434714006055797, 2.436648880725524, 2.437246732496383, 2.4370393009890767, 2.434886324136409, 2.4355227314716004, 2.435145770646708, 2.435011577410375, 2.4340301401071724, 2.4343265849706817, 2.434330370293995, 2.4361741346989816, 2.439095794102005, 2.4397991261687855, 2.4347496208958557, 2.4346065076714423, 2.4338850532469074, 2.434792655596253, 2.4347378444867456, 2.438209252093117, 2.436109679141818, 2.434775738549673, 2.4339220244781683, 2.4362772031975966, 2.4346113460509438, 2.434057074063123, 2.4347714808932075, 2.4343741233833516, 2.4339804539690273, 2.4344812223798686, 2.435091602826755, 2.4342967305584855, 2.434227897401218, 2.4345026130793763, 2.4334214875340705, 2.4348688838418258, 2.433704134962642, 2.4327528763600688, 2.433251337153221, 2.4336534653845754, 2.432981718247431, 2.4325646185042675, 2.4329798489870234, 2.433437128086599, 2.433861594131595, 2.4374341516279343, 2.4360521083494966, 2.4360685794015686, 2.4332288798855073, 2.435097845868653, 2.435331810573288, 2.4351381173613627, 2.4336655035156, 2.4349802945428807, 2.4325607979322115, 2.4320815922543253, 2.4337924847612635, 2.433619856197976, 2.4342949523573294, 2.4323474885991465, 2.432309221291199, 2.4342970476013432, 2.435701298958467, 2.431602188693914, 2.4321874951435065, 2.432611797379762, 2.4335699029526916, 2.433731549868104, 2.4321080273426534, 2.4321748016796074, 2.433200960531372, 2.431835201095017, 2.4337277690487964, 2.433178989696307, 2.431790501968572, 2.4313002203524237, 2.43346138793585, 2.433486726396627, 2.4341454577397026, 2.434193330330036, 2.4310272805255053, 2.433006919140199], 'acc': [0.0821355233722399, 0.09815195028113634, 0.11211498965962467, 0.11950718617904357, 0.11786447612595509, 0.12279260773861922, 0.1252566732420324, 0.1178644759393081, 0.1141683781508058, 0.11827515393985126, 0.12895277317544518, 0.14825461944393065, 0.16016427036795527, 0.16180698121352852, 0.155236140536087, 0.1486652988244376, 0.14455852246627182, 0.15112936355372475, 0.15687885001087581, 0.15482546289965846, 0.17535934380314924, 0.1774127298985174, 0.19137576971455522, 0.16468172414836452, 0.18562628225991368, 0.1917864483301155, 0.1782340869338116, 0.17741273167931323, 0.1708418902002076, 0.18069815281969812, 0.18767967134775324, 0.17577001962206448, 0.17577001983624954, 0.17453798876283594, 0.18316221733480018, 0.17864476277108554, 0.17741273128766055, 0.1778234095115681, 0.17330595549861508, 0.18357289436539095, 0.1827515393434364, 0.18193018528225485, 0.17700205406124342, 0.17823408593632112, 0.1848049286271023, 0.18193018490896087, 0.18151950609757425, 0.17782340892408907, 0.17330595473366842, 0.17494866618507943, 0.1843942504031947, 0.1811088296361038, 0.17741273050435516, 0.17577001983624954, 0.179876795625295, 0.17905544117246075, 0.18028747461414923, 0.18110882965446254, 0.17864476357274967, 0.17905544236577756, 0.1761806980417984, 0.18316221774481162, 0.1806981526422305, 0.17864476355439093, 0.17987679738773213, 0.17494866479593627, 0.17618069766850442, 0.17741273050435516, 0.17618069884346252, 0.17741273052271386, 0.1782340867196265, 0.17905544138664584, 0.17987679623113276, 0.17577002142121906, 0.1733059555169738, 0.16960985595685502, 0.17782340794495732, 0.17946611998384737, 0.1782340857404948, 0.17946611918218328, 0.18028747363501751, 0.1790554405849817, 0.1757700208153813, 0.17864476414187, 0.17946611798886647, 0.18069815164474, 0.17823408711127922, 0.18193018490896087, 0.17864476355439093, 0.17864476316273825, 0.1761806976501457, 0.17289527732978366, 0.1733059551253211, 0.17453798678621374, 0.17371663293921727, 0.17700205445289613, 0.1786447633769233, 0.17905544156411346, 0.1782340861505062, 0.1782340855263097, 0.17700205347376438, 0.1782340861505062, 0.17905544115410205, 0.1786447641602287, 0.1774127307001815, 0.17700205407960215, 0.17618069706266665, 0.1761806988251038, 0.177002052690459, 0.1786447637502173, 0.17782340931574178, 0.1794661200022061, 0.17659137487656282, 0.17905544117246075, 0.17864476453352268, 0.17782340931574178, 0.17823408534884208, 0.1778234075349459, 0.1786447637502173, 0.17700205288628534, 0.17659137626570598, 0.1782340861505062, 0.18069815125308733, 0.17782340812242498, 0.17987679738773213, 0.1774127303085288, 0.1786447645518814, 0.1778234091015567, 0.17659137646153233, 0.17659137567822694, 0.1770020531004704, 0.177002052690459, 0.17659137564150948, 0.17659137646153233, 0.17659137646153233, 0.17700205249463263, 0.17700205407960215, 0.17659137507238917, 0.17659137685318502, 0.17659137526821553, 0.17659137663899996, 0.17741273029017007, 0.17659137624734725, 0.17700205229880628, 0.17577002159868668, 0.17823408593632112, 0.17782340773077226, 0.1782340857588535, 0.17618069704430794, 0.17659137626570598, 0.17535934280565876, 0.17535934321567018, 0.17577002042372858, 0.17618069745431936, 0.1761806992167565, 0.1761806976501457, 0.17659137466237776, 0.1778234095115681, 0.17659137606987962, 0.1815195074867174, 0.1778234085324364, 0.17741273050435516, 0.1802874744366816, 0.17700205368794944, 0.17905544078080807, 0.1774127318934983, 0.17782340773077226, 0.1786447649435341, 0.17659137626570598, 0.17700205407960215, 0.17618069864763616, 0.1753593418081683, 0.17618069862927743, 0.1753593433747791, 0.1761806984334511, 0.17494866481429497, 0.17864476394604364, 0.17823408732546428, 0.17864476316273825, 0.1790554413682871, 0.17905544216995123, 0.17659137507238917, 0.17741273009434372, 0.17782340872826272, 0.17905544177829852, 0.1790554409766344, 0.17905544038915536, 0.17782340792659862, 0.17905544119081948, 0.17905544056662298, 0.1790554404075141, 0.1790554413682871, 0.17946611898635692, 0.17741273169767197, 0.17782340931574178, 0.17905544193740744, 0.17905544078080807, 0.1790554413682871, 0.17905544099499313, 0.17905544234741885, 0.17905544195576614, 0.1790554405849817, 0.17905544134992837, 0.1790554409766344, 0.17741273208932465, 0.1794661185947042, 0.17782340833661003, 0.17864476335856458, 0.1790554413682871, 0.1782340865421589, 0.17905544156411346, 0.17946611820305153, 0.17905544078080807, 0.1774127318751396, 0.17946611839887788, 0.1794661197696623, 0.17864476298527063, 0.17905544216995123, 0.1790554421515925, 0.1786447637502173, 0.1790554405849817, 0.1802874732433648, 0.1790554409766344, 0.17782340968903576, 0.1798767960353064, 0.17946611879053057, 0.17946611939636833, 0.1794661185947042, 0.17864476314437952, 0.1778234095115681, 0.17864476355439093, 0.17700205288628534, 0.17864476396440235, 0.17864476414187, 0.17823408730710558, 0.1790554413682871, 0.17946611839887788, 0.17946611800722517, 0.17864476318109695, 0.1794661185947042, 0.1819301854964399, 0.17946611998384737, 0.1790554425616039, 0.17823408691545287, 0.17946611918218328, 0.17864476316273825, 0.17864476275272684, 0.17905544158247216, 0.1798767972102645, 0.17700205407960215, 0.1790554413682871, 0.17700205386541706, 0.17823408593632112, 0.17864476277108554, 0.17905544156411346, 0.1786447649251754, 0.17864476355439093, 0.17864476394604364, 0.17905544117246075, 0.17823408515301573, 0.1774127307001815, 0.17864476453352268, 0.17782340851407766, 0.18234086231284563, 0.17741273089600784, 0.17577002042372858, 0.1774127318934983, 0.17864476474770774, 0.17782340812242498, 0.1786447641602287, 0.1786447641602287, 0.17905544113574332, 0.1774127303085288, 0.1774127299168761, 0.1798767968002531, 0.17946611877217186, 0.17864476435605506, 0.17782340970739446, 0.1753593433931378, 0.17782340970739446, 0.18069815301552444, 0.17864476355439093, 0.17864476394604364, 0.17782340970739446, 0.18151950805583775, 0.17618069782761334, 0.18110882967282124, 0.17864476275272684, 0.17782340890573037, 0.1790554405849817, 0.18275154051839448, 0.18028747363501751, 0.17741273169767197, 0.18069815127144603, 0.17659137468073646, 0.18193018471313452, 0.17782340888737164, 0.18110882847950444, 0.1778234091015567, 0.18110882985028887]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
