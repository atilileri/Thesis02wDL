{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf9.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.29 14:52:25 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'nFreqs', 'channelMode': '2', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['01', '02', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for nFreqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 7)\n",
    "Test Batch: (609, 7991, 7)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x0000020EBFBC6E80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x0000020EBC377EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            2696      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 11,535\n",
    "Trainable params: 11,535\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0958, Accuracy:0.3844, Validation Loss:1.0837, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0809, Accuracy:0.3943, Validation Loss:1.0773, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0770, Accuracy:0.3943, Validation Loss:1.0760, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0751, Accuracy:0.3943, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0741, Accuracy:0.3979, Validation Loss:1.0748, Validation Accuracy:0.3908\n",
    "Epoch #6: Loss:1.0743, Accuracy:0.3938, Validation Loss:1.0749, Validation Accuracy:0.3727\n",
    "Epoch #7: Loss:1.0745, Accuracy:0.3881, Validation Loss:1.0743, Validation Accuracy:0.4154\n",
    "Epoch #8: Loss:1.0742, Accuracy:0.4033, Validation Loss:1.0743, Validation Accuracy:0.3974\n",
    "Epoch #9: Loss:1.0738, Accuracy:0.3967, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0737, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #12: Loss:1.0734, Accuracy:0.4029, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #13: Loss:1.0735, Accuracy:0.4000, Validation Loss:1.0738, Validation Accuracy:0.4039\n",
    "Epoch #14: Loss:1.0733, Accuracy:0.4012, Validation Loss:1.0740, Validation Accuracy:0.4072\n",
    "Epoch #15: Loss:1.0732, Accuracy:0.4029, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #16: Loss:1.0733, Accuracy:0.3992, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #17: Loss:1.0731, Accuracy:0.4000, Validation Loss:1.0743, Validation Accuracy:0.3908\n",
    "Epoch #18: Loss:1.0732, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0735, Accuracy:0.3930, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0734, Accuracy:0.4021, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0732, Accuracy:0.4049, Validation Loss:1.0743, Validation Accuracy:0.3859\n",
    "Epoch #22: Loss:1.0735, Accuracy:0.3963, Validation Loss:1.0742, Validation Accuracy:0.3908\n",
    "Epoch #23: Loss:1.0733, Accuracy:0.3951, Validation Loss:1.0743, Validation Accuracy:0.3875\n",
    "Epoch #24: Loss:1.0732, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3908\n",
    "Epoch #25: Loss:1.0731, Accuracy:0.3996, Validation Loss:1.0741, Validation Accuracy:0.4056\n",
    "Epoch #26: Loss:1.0733, Accuracy:0.4078, Validation Loss:1.0742, Validation Accuracy:0.4023\n",
    "Epoch #27: Loss:1.0731, Accuracy:0.3988, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #28: Loss:1.0729, Accuracy:0.4045, Validation Loss:1.0746, Validation Accuracy:0.4007\n",
    "Epoch #29: Loss:1.0727, Accuracy:0.4189, Validation Loss:1.0742, Validation Accuracy:0.4105\n",
    "Epoch #30: Loss:1.0722, Accuracy:0.4066, Validation Loss:1.0752, Validation Accuracy:0.4023\n",
    "Epoch #31: Loss:1.0731, Accuracy:0.3959, Validation Loss:1.0745, Validation Accuracy:0.3924\n",
    "Epoch #32: Loss:1.0727, Accuracy:0.3975, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #33: Loss:1.0723, Accuracy:0.4016, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0722, Accuracy:0.4045, Validation Loss:1.0741, Validation Accuracy:0.3957\n",
    "Epoch #35: Loss:1.0715, Accuracy:0.4070, Validation Loss:1.0745, Validation Accuracy:0.4007\n",
    "Epoch #36: Loss:1.0716, Accuracy:0.4103, Validation Loss:1.0752, Validation Accuracy:0.3974\n",
    "Epoch #37: Loss:1.0719, Accuracy:0.4172, Validation Loss:1.0755, Validation Accuracy:0.3842\n",
    "Epoch #38: Loss:1.0726, Accuracy:0.4123, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #39: Loss:1.0729, Accuracy:0.4049, Validation Loss:1.0750, Validation Accuracy:0.3990\n",
    "Epoch #40: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0749, Validation Accuracy:0.4072\n",
    "Epoch #41: Loss:1.0721, Accuracy:0.4053, Validation Loss:1.0750, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0723, Accuracy:0.4090, Validation Loss:1.0746, Validation Accuracy:0.3957\n",
    "Epoch #43: Loss:1.0728, Accuracy:0.4074, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #44: Loss:1.0717, Accuracy:0.4140, Validation Loss:1.0761, Validation Accuracy:0.3974\n",
    "Epoch #45: Loss:1.0736, Accuracy:0.4008, Validation Loss:1.0751, Validation Accuracy:0.4056\n",
    "Epoch #46: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0761, Validation Accuracy:0.3990\n",
    "Epoch #47: Loss:1.0725, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #48: Loss:1.0713, Accuracy:0.4193, Validation Loss:1.0735, Validation Accuracy:0.3810\n",
    "Epoch #49: Loss:1.0719, Accuracy:0.4119, Validation Loss:1.0742, Validation Accuracy:0.4039\n",
    "Epoch #50: Loss:1.0730, Accuracy:0.4021, Validation Loss:1.0746, Validation Accuracy:0.4039\n",
    "Epoch #51: Loss:1.0732, Accuracy:0.4029, Validation Loss:1.0749, Validation Accuracy:0.3892\n",
    "Epoch #52: Loss:1.0726, Accuracy:0.4029, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #53: Loss:1.0728, Accuracy:0.4004, Validation Loss:1.0747, Validation Accuracy:0.3908\n",
    "Epoch #54: Loss:1.0726, Accuracy:0.4008, Validation Loss:1.0749, Validation Accuracy:0.4007\n",
    "Epoch #55: Loss:1.0729, Accuracy:0.4025, Validation Loss:1.0748, Validation Accuracy:0.4007\n",
    "Epoch #56: Loss:1.0728, Accuracy:0.4025, Validation Loss:1.0747, Validation Accuracy:0.3974\n",
    "Epoch #57: Loss:1.0726, Accuracy:0.3992, Validation Loss:1.0755, Validation Accuracy:0.3892\n",
    "Epoch #58: Loss:1.0737, Accuracy:0.3955, Validation Loss:1.0753, Validation Accuracy:0.3908\n",
    "Epoch #59: Loss:1.0731, Accuracy:0.3947, Validation Loss:1.0745, Validation Accuracy:0.4056\n",
    "Epoch #60: Loss:1.0726, Accuracy:0.4066, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #61: Loss:1.0737, Accuracy:0.4004, Validation Loss:1.0748, Validation Accuracy:0.3924\n",
    "Epoch #62: Loss:1.0722, Accuracy:0.4066, Validation Loss:1.0742, Validation Accuracy:0.4056\n",
    "Epoch #63: Loss:1.0728, Accuracy:0.4164, Validation Loss:1.0743, Validation Accuracy:0.3990\n",
    "Epoch #64: Loss:1.0724, Accuracy:0.4172, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #65: Loss:1.0724, Accuracy:0.4057, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #66: Loss:1.0725, Accuracy:0.4164, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #67: Loss:1.0719, Accuracy:0.4164, Validation Loss:1.0733, Validation Accuracy:0.3924\n",
    "Epoch #68: Loss:1.0719, Accuracy:0.4172, Validation Loss:1.0734, Validation Accuracy:0.4056\n",
    "Epoch #69: Loss:1.0716, Accuracy:0.4090, Validation Loss:1.0734, Validation Accuracy:0.3908\n",
    "Epoch #70: Loss:1.0717, Accuracy:0.4033, Validation Loss:1.0732, Validation Accuracy:0.3826\n",
    "Epoch #71: Loss:1.0718, Accuracy:0.4119, Validation Loss:1.0736, Validation Accuracy:0.4089\n",
    "Epoch #72: Loss:1.0717, Accuracy:0.4181, Validation Loss:1.0730, Validation Accuracy:0.4089\n",
    "Epoch #73: Loss:1.0716, Accuracy:0.4189, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #74: Loss:1.0714, Accuracy:0.4107, Validation Loss:1.0738, Validation Accuracy:0.3875\n",
    "Epoch #75: Loss:1.0716, Accuracy:0.4127, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #76: Loss:1.0711, Accuracy:0.4177, Validation Loss:1.0730, Validation Accuracy:0.4056\n",
    "Epoch #77: Loss:1.0719, Accuracy:0.4103, Validation Loss:1.0730, Validation Accuracy:0.4072\n",
    "Epoch #78: Loss:1.0710, Accuracy:0.4148, Validation Loss:1.0730, Validation Accuracy:0.4023\n",
    "Epoch #79: Loss:1.0715, Accuracy:0.4127, Validation Loss:1.0730, Validation Accuracy:0.4105\n",
    "Epoch #80: Loss:1.0709, Accuracy:0.4164, Validation Loss:1.0726, Validation Accuracy:0.4023\n",
    "Epoch #81: Loss:1.0718, Accuracy:0.4103, Validation Loss:1.0729, Validation Accuracy:0.4105\n",
    "Epoch #82: Loss:1.0713, Accuracy:0.4127, Validation Loss:1.0725, Validation Accuracy:0.4089\n",
    "Epoch #83: Loss:1.0714, Accuracy:0.4164, Validation Loss:1.0733, Validation Accuracy:0.4007\n",
    "Epoch #84: Loss:1.0714, Accuracy:0.4111, Validation Loss:1.0724, Validation Accuracy:0.4171\n",
    "Epoch #85: Loss:1.0713, Accuracy:0.4152, Validation Loss:1.0721, Validation Accuracy:0.4204\n",
    "Epoch #86: Loss:1.0712, Accuracy:0.4103, Validation Loss:1.0723, Validation Accuracy:0.4154\n",
    "Epoch #87: Loss:1.0713, Accuracy:0.4172, Validation Loss:1.0719, Validation Accuracy:0.4187\n",
    "Epoch #88: Loss:1.0714, Accuracy:0.4086, Validation Loss:1.0723, Validation Accuracy:0.4154\n",
    "Epoch #89: Loss:1.0718, Accuracy:0.4033, Validation Loss:1.0729, Validation Accuracy:0.3941\n",
    "Epoch #90: Loss:1.0715, Accuracy:0.4164, Validation Loss:1.0725, Validation Accuracy:0.4122\n",
    "Epoch #91: Loss:1.0711, Accuracy:0.4099, Validation Loss:1.0719, Validation Accuracy:0.4171\n",
    "Epoch #92: Loss:1.0709, Accuracy:0.4107, Validation Loss:1.0716, Validation Accuracy:0.4089\n",
    "Epoch #93: Loss:1.0712, Accuracy:0.4127, Validation Loss:1.0718, Validation Accuracy:0.4105\n",
    "Epoch #94: Loss:1.0703, Accuracy:0.4127, Validation Loss:1.0721, Validation Accuracy:0.4023\n",
    "Epoch #95: Loss:1.0715, Accuracy:0.4041, Validation Loss:1.0721, Validation Accuracy:0.3941\n",
    "Epoch #96: Loss:1.0710, Accuracy:0.4136, Validation Loss:1.0728, Validation Accuracy:0.3974\n",
    "Epoch #97: Loss:1.0718, Accuracy:0.4082, Validation Loss:1.0717, Validation Accuracy:0.4007\n",
    "Epoch #98: Loss:1.0727, Accuracy:0.3963, Validation Loss:1.0730, Validation Accuracy:0.3892\n",
    "Epoch #99: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0736, Validation Accuracy:0.4007\n",
    "Epoch #100: Loss:1.0725, Accuracy:0.4004, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #101: Loss:1.0720, Accuracy:0.3984, Validation Loss:1.0738, Validation Accuracy:0.3859\n",
    "Epoch #102: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0730, Validation Accuracy:0.3908\n",
    "Epoch #103: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #104: Loss:1.0713, Accuracy:0.4029, Validation Loss:1.0727, Validation Accuracy:0.3924\n",
    "Epoch #105: Loss:1.0715, Accuracy:0.4029, Validation Loss:1.0727, Validation Accuracy:0.3974\n",
    "Epoch #106: Loss:1.0715, Accuracy:0.4082, Validation Loss:1.0753, Validation Accuracy:0.3810\n",
    "Epoch #107: Loss:1.0703, Accuracy:0.4053, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #108: Loss:1.0710, Accuracy:0.4094, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #109: Loss:1.0720, Accuracy:0.3988, Validation Loss:1.0749, Validation Accuracy:0.3727\n",
    "Epoch #110: Loss:1.0723, Accuracy:0.3996, Validation Loss:1.0746, Validation Accuracy:0.3924\n",
    "Epoch #111: Loss:1.0727, Accuracy:0.4037, Validation Loss:1.0736, Validation Accuracy:0.3974\n",
    "Epoch #112: Loss:1.0712, Accuracy:0.4070, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #113: Loss:1.0703, Accuracy:0.4172, Validation Loss:1.0726, Validation Accuracy:0.3924\n",
    "Epoch #114: Loss:1.0704, Accuracy:0.4074, Validation Loss:1.0730, Validation Accuracy:0.3924\n",
    "Epoch #115: Loss:1.0708, Accuracy:0.4070, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #116: Loss:1.0712, Accuracy:0.4016, Validation Loss:1.0741, Validation Accuracy:0.3974\n",
    "Epoch #117: Loss:1.0716, Accuracy:0.4074, Validation Loss:1.0742, Validation Accuracy:0.3957\n",
    "Epoch #118: Loss:1.0730, Accuracy:0.4016, Validation Loss:1.0740, Validation Accuracy:0.3924\n",
    "Epoch #119: Loss:1.0733, Accuracy:0.4057, Validation Loss:1.0727, Validation Accuracy:0.3957\n",
    "Epoch #120: Loss:1.0715, Accuracy:0.3992, Validation Loss:1.0739, Validation Accuracy:0.3875\n",
    "Epoch #121: Loss:1.0709, Accuracy:0.4033, Validation Loss:1.0736, Validation Accuracy:0.3990\n",
    "Epoch #122: Loss:1.0710, Accuracy:0.4111, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #123: Loss:1.0705, Accuracy:0.4090, Validation Loss:1.0751, Validation Accuracy:0.3810\n",
    "Epoch #124: Loss:1.0699, Accuracy:0.3988, Validation Loss:1.0719, Validation Accuracy:0.4039\n",
    "Epoch #125: Loss:1.0707, Accuracy:0.4037, Validation Loss:1.0728, Validation Accuracy:0.3990\n",
    "Epoch #126: Loss:1.0688, Accuracy:0.4230, Validation Loss:1.0740, Validation Accuracy:0.3957\n",
    "Epoch #127: Loss:1.0701, Accuracy:0.4094, Validation Loss:1.0724, Validation Accuracy:0.4056\n",
    "Epoch #128: Loss:1.0706, Accuracy:0.4094, Validation Loss:1.0724, Validation Accuracy:0.3908\n",
    "Epoch #129: Loss:1.0695, Accuracy:0.4172, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #130: Loss:1.0706, Accuracy:0.4070, Validation Loss:1.0724, Validation Accuracy:0.4089\n",
    "Epoch #131: Loss:1.0700, Accuracy:0.4131, Validation Loss:1.0725, Validation Accuracy:0.3990\n",
    "Epoch #132: Loss:1.0707, Accuracy:0.4090, Validation Loss:1.0725, Validation Accuracy:0.3990\n",
    "Epoch #133: Loss:1.0717, Accuracy:0.4049, Validation Loss:1.0721, Validation Accuracy:0.4039\n",
    "Epoch #134: Loss:1.0717, Accuracy:0.3971, Validation Loss:1.0731, Validation Accuracy:0.3941\n",
    "Epoch #135: Loss:1.0718, Accuracy:0.3947, Validation Loss:1.0735, Validation Accuracy:0.4122\n",
    "Epoch #136: Loss:1.0719, Accuracy:0.3975, Validation Loss:1.0718, Validation Accuracy:0.4056\n",
    "Epoch #137: Loss:1.0706, Accuracy:0.4078, Validation Loss:1.0727, Validation Accuracy:0.3990\n",
    "Epoch #138: Loss:1.0724, Accuracy:0.4012, Validation Loss:1.0718, Validation Accuracy:0.4089\n",
    "Epoch #139: Loss:1.0700, Accuracy:0.4152, Validation Loss:1.0711, Validation Accuracy:0.3974\n",
    "Epoch #140: Loss:1.0694, Accuracy:0.4111, Validation Loss:1.0712, Validation Accuracy:0.4039\n",
    "Epoch #141: Loss:1.0697, Accuracy:0.4119, Validation Loss:1.0719, Validation Accuracy:0.4039\n",
    "Epoch #142: Loss:1.0701, Accuracy:0.4168, Validation Loss:1.0717, Validation Accuracy:0.4105\n",
    "Epoch #143: Loss:1.0691, Accuracy:0.4115, Validation Loss:1.0711, Validation Accuracy:0.3924\n",
    "Epoch #144: Loss:1.0695, Accuracy:0.4053, Validation Loss:1.0712, Validation Accuracy:0.3924\n",
    "Epoch #145: Loss:1.0690, Accuracy:0.4094, Validation Loss:1.0710, Validation Accuracy:0.4039\n",
    "Epoch #146: Loss:1.0691, Accuracy:0.4177, Validation Loss:1.0715, Validation Accuracy:0.4023\n",
    "Epoch #147: Loss:1.0690, Accuracy:0.4152, Validation Loss:1.0703, Validation Accuracy:0.4007\n",
    "Epoch #148: Loss:1.0697, Accuracy:0.4070, Validation Loss:1.0710, Validation Accuracy:0.4023\n",
    "Epoch #149: Loss:1.0690, Accuracy:0.4111, Validation Loss:1.0705, Validation Accuracy:0.4089\n",
    "Epoch #150: Loss:1.0691, Accuracy:0.4115, Validation Loss:1.0706, Validation Accuracy:0.4138\n",
    "Epoch #151: Loss:1.0693, Accuracy:0.4008, Validation Loss:1.0708, Validation Accuracy:0.4023\n",
    "Epoch #152: Loss:1.0688, Accuracy:0.4119, Validation Loss:1.0704, Validation Accuracy:0.4154\n",
    "Epoch #153: Loss:1.0685, Accuracy:0.4164, Validation Loss:1.0698, Validation Accuracy:0.4105\n",
    "Epoch #154: Loss:1.0679, Accuracy:0.4144, Validation Loss:1.0707, Validation Accuracy:0.4056\n",
    "Epoch #155: Loss:1.0683, Accuracy:0.4148, Validation Loss:1.0706, Validation Accuracy:0.4105\n",
    "Epoch #156: Loss:1.0688, Accuracy:0.4115, Validation Loss:1.0707, Validation Accuracy:0.4171\n",
    "Epoch #157: Loss:1.0684, Accuracy:0.4156, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #158: Loss:1.0692, Accuracy:0.4148, Validation Loss:1.0713, Validation Accuracy:0.3974\n",
    "Epoch #159: Loss:1.0694, Accuracy:0.4136, Validation Loss:1.0708, Validation Accuracy:0.3990\n",
    "Epoch #160: Loss:1.0692, Accuracy:0.4156, Validation Loss:1.0710, Validation Accuracy:0.3990\n",
    "Epoch #161: Loss:1.0688, Accuracy:0.4222, Validation Loss:1.0704, Validation Accuracy:0.3990\n",
    "Epoch #162: Loss:1.0683, Accuracy:0.4156, Validation Loss:1.0707, Validation Accuracy:0.4007\n",
    "Epoch #163: Loss:1.0679, Accuracy:0.4185, Validation Loss:1.0708, Validation Accuracy:0.3908\n",
    "Epoch #164: Loss:1.0684, Accuracy:0.4222, Validation Loss:1.0710, Validation Accuracy:0.4007\n",
    "Epoch #165: Loss:1.0686, Accuracy:0.4177, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #166: Loss:1.0678, Accuracy:0.4140, Validation Loss:1.0718, Validation Accuracy:0.4105\n",
    "Epoch #167: Loss:1.0679, Accuracy:0.4234, Validation Loss:1.0719, Validation Accuracy:0.4138\n",
    "Epoch #168: Loss:1.0678, Accuracy:0.4197, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #169: Loss:1.0672, Accuracy:0.4136, Validation Loss:1.0725, Validation Accuracy:0.4023\n",
    "Epoch #170: Loss:1.0671, Accuracy:0.4185, Validation Loss:1.0734, Validation Accuracy:0.4007\n",
    "Epoch #171: Loss:1.0671, Accuracy:0.4246, Validation Loss:1.0746, Validation Accuracy:0.4023\n",
    "Epoch #172: Loss:1.0671, Accuracy:0.4242, Validation Loss:1.0756, Validation Accuracy:0.4023\n",
    "Epoch #173: Loss:1.0673, Accuracy:0.4251, Validation Loss:1.0747, Validation Accuracy:0.4056\n",
    "Epoch #174: Loss:1.0666, Accuracy:0.4156, Validation Loss:1.0732, Validation Accuracy:0.3924\n",
    "Epoch #175: Loss:1.0667, Accuracy:0.4181, Validation Loss:1.0700, Validation Accuracy:0.4039\n",
    "Epoch #176: Loss:1.0662, Accuracy:0.4329, Validation Loss:1.0719, Validation Accuracy:0.3892\n",
    "Epoch #177: Loss:1.0661, Accuracy:0.4226, Validation Loss:1.0768, Validation Accuracy:0.4105\n",
    "Epoch #178: Loss:1.0679, Accuracy:0.4193, Validation Loss:1.0714, Validation Accuracy:0.3974\n",
    "Epoch #179: Loss:1.0658, Accuracy:0.4185, Validation Loss:1.0714, Validation Accuracy:0.3957\n",
    "Epoch #180: Loss:1.0658, Accuracy:0.4185, Validation Loss:1.0750, Validation Accuracy:0.3859\n",
    "Epoch #181: Loss:1.0672, Accuracy:0.4164, Validation Loss:1.0761, Validation Accuracy:0.3859\n",
    "Epoch #182: Loss:1.0684, Accuracy:0.4115, Validation Loss:1.0748, Validation Accuracy:0.3859\n",
    "Epoch #183: Loss:1.0675, Accuracy:0.4115, Validation Loss:1.0735, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0660, Accuracy:0.4230, Validation Loss:1.0745, Validation Accuracy:0.3892\n",
    "Epoch #185: Loss:1.0661, Accuracy:0.4152, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #186: Loss:1.0658, Accuracy:0.4148, Validation Loss:1.0727, Validation Accuracy:0.3974\n",
    "Epoch #187: Loss:1.0658, Accuracy:0.4251, Validation Loss:1.0716, Validation Accuracy:0.4056\n",
    "Epoch #188: Loss:1.0669, Accuracy:0.4168, Validation Loss:1.0719, Validation Accuracy:0.3957\n",
    "Epoch #189: Loss:1.0648, Accuracy:0.4123, Validation Loss:1.0717, Validation Accuracy:0.4023\n",
    "Epoch #190: Loss:1.0623, Accuracy:0.4222, Validation Loss:1.0757, Validation Accuracy:0.3810\n",
    "Epoch #191: Loss:1.0633, Accuracy:0.4160, Validation Loss:1.0770, Validation Accuracy:0.3777\n",
    "Epoch #192: Loss:1.0620, Accuracy:0.4370, Validation Loss:1.0756, Validation Accuracy:0.3793\n",
    "Epoch #193: Loss:1.0602, Accuracy:0.4370, Validation Loss:1.0792, Validation Accuracy:0.3859\n",
    "Epoch #194: Loss:1.0602, Accuracy:0.4271, Validation Loss:1.0800, Validation Accuracy:0.3908\n",
    "Epoch #195: Loss:1.0611, Accuracy:0.4255, Validation Loss:1.0797, Validation Accuracy:0.3941\n",
    "Epoch #196: Loss:1.0593, Accuracy:0.4374, Validation Loss:1.0818, Validation Accuracy:0.3842\n",
    "Epoch #197: Loss:1.0604, Accuracy:0.4324, Validation Loss:1.0835, Validation Accuracy:0.3875\n",
    "Epoch #198: Loss:1.0610, Accuracy:0.4312, Validation Loss:1.0874, Validation Accuracy:0.3793\n",
    "Epoch #199: Loss:1.0649, Accuracy:0.4136, Validation Loss:1.0834, Validation Accuracy:0.3645\n",
    "Epoch #200: Loss:1.0647, Accuracy:0.4209, Validation Loss:1.0797, Validation Accuracy:0.3760\n",
    "Epoch #201: Loss:1.0638, Accuracy:0.4156, Validation Loss:1.0751, Validation Accuracy:0.3974\n",
    "Epoch #202: Loss:1.0610, Accuracy:0.4275, Validation Loss:1.0754, Validation Accuracy:0.3777\n",
    "Epoch #203: Loss:1.0591, Accuracy:0.4444, Validation Loss:1.0737, Validation Accuracy:0.3875\n",
    "Epoch #204: Loss:1.0615, Accuracy:0.4415, Validation Loss:1.0757, Validation Accuracy:0.4023\n",
    "Epoch #205: Loss:1.0685, Accuracy:0.4111, Validation Loss:1.0750, Validation Accuracy:0.3810\n",
    "Epoch #206: Loss:1.0676, Accuracy:0.4209, Validation Loss:1.0780, Validation Accuracy:0.3793\n",
    "Epoch #207: Loss:1.0780, Accuracy:0.3885, Validation Loss:1.0766, Validation Accuracy:0.4023\n",
    "Epoch #208: Loss:1.0786, Accuracy:0.3988, Validation Loss:1.0744, Validation Accuracy:0.4007\n",
    "Epoch #209: Loss:1.0680, Accuracy:0.4197, Validation Loss:1.0744, Validation Accuracy:0.4089\n",
    "Epoch #210: Loss:1.0694, Accuracy:0.4016, Validation Loss:1.0723, Validation Accuracy:0.4007\n",
    "Epoch #211: Loss:1.0686, Accuracy:0.4197, Validation Loss:1.0731, Validation Accuracy:0.3793\n",
    "Epoch #212: Loss:1.0690, Accuracy:0.4074, Validation Loss:1.0720, Validation Accuracy:0.3990\n",
    "Epoch #213: Loss:1.0686, Accuracy:0.4177, Validation Loss:1.0722, Validation Accuracy:0.3990\n",
    "Epoch #214: Loss:1.0683, Accuracy:0.4201, Validation Loss:1.0729, Validation Accuracy:0.4039\n",
    "Epoch #215: Loss:1.0678, Accuracy:0.4267, Validation Loss:1.0733, Validation Accuracy:0.3941\n",
    "Epoch #216: Loss:1.0677, Accuracy:0.4197, Validation Loss:1.0727, Validation Accuracy:0.4007\n",
    "Epoch #217: Loss:1.0665, Accuracy:0.4275, Validation Loss:1.0728, Validation Accuracy:0.4138\n",
    "Epoch #218: Loss:1.0659, Accuracy:0.4259, Validation Loss:1.0716, Validation Accuracy:0.4105\n",
    "Epoch #219: Loss:1.0658, Accuracy:0.4287, Validation Loss:1.0721, Validation Accuracy:0.4072\n",
    "Epoch #220: Loss:1.0664, Accuracy:0.4218, Validation Loss:1.0713, Validation Accuracy:0.4105\n",
    "Epoch #221: Loss:1.0650, Accuracy:0.4255, Validation Loss:1.0730, Validation Accuracy:0.4007\n",
    "Epoch #222: Loss:1.0646, Accuracy:0.4177, Validation Loss:1.0694, Validation Accuracy:0.4138\n",
    "Epoch #223: Loss:1.0648, Accuracy:0.4242, Validation Loss:1.0731, Validation Accuracy:0.3908\n",
    "Epoch #224: Loss:1.0648, Accuracy:0.4234, Validation Loss:1.0753, Validation Accuracy:0.3941\n",
    "Epoch #225: Loss:1.0657, Accuracy:0.4148, Validation Loss:1.0743, Validation Accuracy:0.4072\n",
    "Epoch #226: Loss:1.0650, Accuracy:0.4251, Validation Loss:1.0752, Validation Accuracy:0.4039\n",
    "Epoch #227: Loss:1.0652, Accuracy:0.4172, Validation Loss:1.0743, Validation Accuracy:0.4089\n",
    "Epoch #228: Loss:1.0655, Accuracy:0.4230, Validation Loss:1.0747, Validation Accuracy:0.4122\n",
    "Epoch #229: Loss:1.0654, Accuracy:0.4214, Validation Loss:1.0741, Validation Accuracy:0.4220\n",
    "Epoch #230: Loss:1.0648, Accuracy:0.4279, Validation Loss:1.0768, Validation Accuracy:0.4023\n",
    "Epoch #231: Loss:1.0645, Accuracy:0.4181, Validation Loss:1.0762, Validation Accuracy:0.3990\n",
    "Epoch #232: Loss:1.0637, Accuracy:0.4324, Validation Loss:1.0754, Validation Accuracy:0.4236\n",
    "Epoch #233: Loss:1.0645, Accuracy:0.4189, Validation Loss:1.0772, Validation Accuracy:0.4007\n",
    "Epoch #234: Loss:1.0632, Accuracy:0.4201, Validation Loss:1.0764, Validation Accuracy:0.3990\n",
    "Epoch #235: Loss:1.0625, Accuracy:0.4271, Validation Loss:1.0775, Validation Accuracy:0.4039\n",
    "Epoch #236: Loss:1.0622, Accuracy:0.4304, Validation Loss:1.0788, Validation Accuracy:0.4138\n",
    "Epoch #237: Loss:1.0620, Accuracy:0.4329, Validation Loss:1.0782, Validation Accuracy:0.3924\n",
    "Epoch #238: Loss:1.0606, Accuracy:0.4324, Validation Loss:1.0800, Validation Accuracy:0.4056\n",
    "Epoch #239: Loss:1.0604, Accuracy:0.4407, Validation Loss:1.0780, Validation Accuracy:0.4072\n",
    "Epoch #240: Loss:1.0608, Accuracy:0.4337, Validation Loss:1.0785, Validation Accuracy:0.4039\n",
    "Epoch #241: Loss:1.0605, Accuracy:0.4267, Validation Loss:1.0779, Validation Accuracy:0.4105\n",
    "Epoch #242: Loss:1.0606, Accuracy:0.4296, Validation Loss:1.0813, Validation Accuracy:0.4236\n",
    "Epoch #243: Loss:1.0613, Accuracy:0.4205, Validation Loss:1.0781, Validation Accuracy:0.4220\n",
    "Epoch #244: Loss:1.0614, Accuracy:0.4222, Validation Loss:1.0766, Validation Accuracy:0.4187\n",
    "Epoch #245: Loss:1.0609, Accuracy:0.4353, Validation Loss:1.0761, Validation Accuracy:0.4220\n",
    "Epoch #246: Loss:1.0621, Accuracy:0.4222, Validation Loss:1.0752, Validation Accuracy:0.4204\n",
    "Epoch #247: Loss:1.0636, Accuracy:0.4238, Validation Loss:1.0740, Validation Accuracy:0.4187\n",
    "Epoch #248: Loss:1.0603, Accuracy:0.4341, Validation Loss:1.0738, Validation Accuracy:0.4138\n",
    "Epoch #249: Loss:1.0584, Accuracy:0.4394, Validation Loss:1.0790, Validation Accuracy:0.4023\n",
    "Epoch #250: Loss:1.0587, Accuracy:0.4357, Validation Loss:1.0758, Validation Accuracy:0.4072\n",
    "Epoch #251: Loss:1.0561, Accuracy:0.4386, Validation Loss:1.0833, Validation Accuracy:0.4089\n",
    "Epoch #252: Loss:1.0572, Accuracy:0.4378, Validation Loss:1.0801, Validation Accuracy:0.4122\n",
    "Epoch #253: Loss:1.0559, Accuracy:0.4419, Validation Loss:1.0900, Validation Accuracy:0.4105\n",
    "Epoch #254: Loss:1.0587, Accuracy:0.4234, Validation Loss:1.0853, Validation Accuracy:0.3990\n",
    "Epoch #255: Loss:1.0579, Accuracy:0.4275, Validation Loss:1.0860, Validation Accuracy:0.4105\n",
    "Epoch #256: Loss:1.0565, Accuracy:0.4349, Validation Loss:1.0799, Validation Accuracy:0.4138\n",
    "Epoch #257: Loss:1.0558, Accuracy:0.4251, Validation Loss:1.0815, Validation Accuracy:0.4039\n",
    "Epoch #258: Loss:1.0553, Accuracy:0.4341, Validation Loss:1.0813, Validation Accuracy:0.3974\n",
    "Epoch #259: Loss:1.0549, Accuracy:0.4378, Validation Loss:1.0787, Validation Accuracy:0.4023\n",
    "Epoch #260: Loss:1.0557, Accuracy:0.4337, Validation Loss:1.0784, Validation Accuracy:0.3941\n",
    "Epoch #261: Loss:1.0544, Accuracy:0.4349, Validation Loss:1.0786, Validation Accuracy:0.4122\n",
    "Epoch #262: Loss:1.0566, Accuracy:0.4407, Validation Loss:1.0780, Validation Accuracy:0.4089\n",
    "Epoch #263: Loss:1.0553, Accuracy:0.4382, Validation Loss:1.0775, Validation Accuracy:0.4253\n",
    "Epoch #264: Loss:1.0619, Accuracy:0.4304, Validation Loss:1.0843, Validation Accuracy:0.3974\n",
    "Epoch #265: Loss:1.0668, Accuracy:0.4136, Validation Loss:1.0821, Validation Accuracy:0.4138\n",
    "Epoch #266: Loss:1.0619, Accuracy:0.4255, Validation Loss:1.0831, Validation Accuracy:0.4007\n",
    "Epoch #267: Loss:1.0605, Accuracy:0.4189, Validation Loss:1.0789, Validation Accuracy:0.4089\n",
    "Epoch #268: Loss:1.0602, Accuracy:0.4263, Validation Loss:1.0829, Validation Accuracy:0.3810\n",
    "Epoch #269: Loss:1.0567, Accuracy:0.4189, Validation Loss:1.0779, Validation Accuracy:0.4056\n",
    "Epoch #270: Loss:1.0572, Accuracy:0.4242, Validation Loss:1.0762, Validation Accuracy:0.4023\n",
    "Epoch #271: Loss:1.0563, Accuracy:0.4230, Validation Loss:1.0827, Validation Accuracy:0.3957\n",
    "Epoch #272: Loss:1.0551, Accuracy:0.4255, Validation Loss:1.0804, Validation Accuracy:0.3974\n",
    "Epoch #273: Loss:1.0526, Accuracy:0.4333, Validation Loss:1.0797, Validation Accuracy:0.3875\n",
    "Epoch #274: Loss:1.0521, Accuracy:0.4333, Validation Loss:1.0787, Validation Accuracy:0.4253\n",
    "Epoch #275: Loss:1.0558, Accuracy:0.4316, Validation Loss:1.0851, Validation Accuracy:0.4122\n",
    "Epoch #276: Loss:1.0606, Accuracy:0.4168, Validation Loss:1.0840, Validation Accuracy:0.4105\n",
    "Epoch #277: Loss:1.0596, Accuracy:0.4214, Validation Loss:1.0824, Validation Accuracy:0.4089\n",
    "Epoch #278: Loss:1.0586, Accuracy:0.4185, Validation Loss:1.0778, Validation Accuracy:0.4039\n",
    "Epoch #279: Loss:1.0578, Accuracy:0.4251, Validation Loss:1.0787, Validation Accuracy:0.4138\n",
    "Epoch #280: Loss:1.0550, Accuracy:0.4361, Validation Loss:1.0819, Validation Accuracy:0.4105\n",
    "Epoch #281: Loss:1.0531, Accuracy:0.4480, Validation Loss:1.0764, Validation Accuracy:0.3957\n",
    "Epoch #282: Loss:1.0538, Accuracy:0.4296, Validation Loss:1.0726, Validation Accuracy:0.4236\n",
    "Epoch #283: Loss:1.0527, Accuracy:0.4411, Validation Loss:1.0782, Validation Accuracy:0.4023\n",
    "Epoch #284: Loss:1.0546, Accuracy:0.4448, Validation Loss:1.0789, Validation Accuracy:0.4236\n",
    "Epoch #285: Loss:1.0542, Accuracy:0.4341, Validation Loss:1.0771, Validation Accuracy:0.4269\n",
    "Epoch #286: Loss:1.0540, Accuracy:0.4353, Validation Loss:1.0786, Validation Accuracy:0.4269\n",
    "Epoch #287: Loss:1.0540, Accuracy:0.4444, Validation Loss:1.0817, Validation Accuracy:0.4187\n",
    "Epoch #288: Loss:1.0514, Accuracy:0.4394, Validation Loss:1.0838, Validation Accuracy:0.4089\n",
    "Epoch #289: Loss:1.0499, Accuracy:0.4444, Validation Loss:1.0879, Validation Accuracy:0.4105\n",
    "Epoch #290: Loss:1.0511, Accuracy:0.4415, Validation Loss:1.0839, Validation Accuracy:0.4204\n",
    "Epoch #291: Loss:1.0548, Accuracy:0.4357, Validation Loss:1.0856, Validation Accuracy:0.3974\n",
    "Epoch #292: Loss:1.0549, Accuracy:0.4267, Validation Loss:1.0832, Validation Accuracy:0.4236\n",
    "Epoch #293: Loss:1.0525, Accuracy:0.4374, Validation Loss:1.0845, Validation Accuracy:0.4089\n",
    "Epoch #294: Loss:1.0494, Accuracy:0.4333, Validation Loss:1.0855, Validation Accuracy:0.3941\n",
    "Epoch #295: Loss:1.0463, Accuracy:0.4452, Validation Loss:1.0879, Validation Accuracy:0.4220\n",
    "Epoch #296: Loss:1.0512, Accuracy:0.4407, Validation Loss:1.0863, Validation Accuracy:0.3941\n",
    "Epoch #297: Loss:1.0533, Accuracy:0.4333, Validation Loss:1.0825, Validation Accuracy:0.3974\n",
    "Epoch #298: Loss:1.0527, Accuracy:0.4444, Validation Loss:1.0809, Validation Accuracy:0.4056\n",
    "Epoch #299: Loss:1.0499, Accuracy:0.4472, Validation Loss:1.0962, Validation Accuracy:0.3957\n",
    "Epoch #300: Loss:1.0558, Accuracy:0.4419, Validation Loss:1.0810, Validation Accuracy:0.4154\n",
    "\n",
    "Test:\n",
    "Test Loss:1.08101583, Accuracy:0.4154\n",
    "Labels: ['01', '02', '03']\n",
    "Confusion Matrix:\n",
    "       01  02  03\n",
    "t:01  189  51   0\n",
    "t:02  163  64   0\n",
    "t:03  106  36   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          01       0.41      0.79      0.54       240\n",
    "          02       0.42      0.28      0.34       227\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.42       609\n",
    "   macro avg       0.28      0.36      0.29       609\n",
    "weighted avg       0.32      0.42      0.34       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.29 15:07:59 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 15 minutes, 33 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0837447028637714, 1.0772934735114938, 1.0760255081117251, 1.0752103675175182, 1.0747764243672437, 1.0749063029860824, 1.0743204846562227, 1.0742526183574659, 1.0740291234503434, 1.074222216269457, 1.074341287753852, 1.0738068110445647, 1.073803571448929, 1.0740349613778502, 1.074058078192725, 1.0740784246150301, 1.0742977417161312, 1.074524134837935, 1.0744444509640898, 1.0744078999082443, 1.0743213110956653, 1.0742011078081304, 1.074319055319224, 1.074136212345806, 1.074075952539303, 1.0741700239369434, 1.0739295921106449, 1.0745929481556458, 1.0741525348184144, 1.075247381904051, 1.0745168282284916, 1.074058604357865, 1.0738405374862094, 1.074127611855568, 1.074467628851704, 1.0752293045689123, 1.0755397594229537, 1.0750737049309491, 1.0750358118408028, 1.0748997098706625, 1.07497094828507, 1.074623036658627, 1.0755495549422767, 1.0761210953660787, 1.0750928259835455, 1.0761446351879727, 1.07405287585235, 1.0734856314651289, 1.0742176264181904, 1.0746174665115933, 1.0748643865334773, 1.0745467894965988, 1.0747104967364733, 1.0749209165964613, 1.0748221889896736, 1.0746545112387496, 1.0755090155624991, 1.0752510703451723, 1.074452627077087, 1.0743653275109277, 1.0747617990121074, 1.0741910417678908, 1.0743184019192098, 1.0739138284927519, 1.0741437775356624, 1.0739677326236843, 1.073349883795176, 1.073394638955691, 1.0734106956052858, 1.0731724306867627, 1.0735664267845342, 1.0730208538240205, 1.0727886914815417, 1.0738497422442257, 1.0733419823137607, 1.0730041623702777, 1.0730065929478612, 1.0729926871548732, 1.0729516521463254, 1.0725918432761883, 1.0728591142225343, 1.0725372342640542, 1.073306947310374, 1.0724100328627086, 1.0720874257079878, 1.0723083424450728, 1.07185489849504, 1.0723306230332073, 1.0728854432286103, 1.0724859922781758, 1.071937566516043, 1.0716157681836282, 1.0718046982691598, 1.0721339277054485, 1.0720579197449833, 1.0727875606571315, 1.0717272365034507, 1.073041855603799, 1.0735798639616942, 1.074010568690809, 1.0737780933505405, 1.072987315689989, 1.0734067082600836, 1.0727091642044644, 1.0726966540801701, 1.075331874473146, 1.0732421322996393, 1.0739576795026782, 1.0749060802271801, 1.074564536999795, 1.0735582917781885, 1.0744283197352844, 1.0726369319878188, 1.0729863152323882, 1.0741146618901019, 1.0740820220342802, 1.0741620697998648, 1.0739571163415518, 1.0727180031132815, 1.0739497613828561, 1.073569436770159, 1.074185707103247, 1.0750640423231328, 1.0719453963544372, 1.0728151095520295, 1.0739538274179343, 1.0723801806251012, 1.0723805893426654, 1.0731545214974039, 1.0723538788277136, 1.0725259299348728, 1.0724647080369771, 1.0721374611157697, 1.073123583652703, 1.0734720388656767, 1.0717611297206535, 1.0727245478794492, 1.0717680933831752, 1.0711285938770312, 1.071196364260268, 1.0718741465867643, 1.0717310461113214, 1.0711027127377113, 1.0711555713894723, 1.070969659510896, 1.0715044862139598, 1.0702557054842243, 1.0709635330538445, 1.0704892587974937, 1.070553597558308, 1.0708295212590635, 1.0704083843967207, 1.0697887210031645, 1.070746615993957, 1.0706360929313747, 1.0706769198619674, 1.0728199403665728, 1.0712826416410248, 1.0707511539725443, 1.0709935491308202, 1.0703697817274698, 1.070670958027268, 1.0708392668631668, 1.0710141744911181, 1.072321257372012, 1.0718373657251619, 1.071927580731647, 1.0741745304004313, 1.0724542225131457, 1.0733608494838471, 1.0745873919065754, 1.0756270110313528, 1.074692632960177, 1.0732008958684986, 1.070028865474394, 1.0718859500681435, 1.0767877336793346, 1.0714032082330613, 1.0713976966140697, 1.075019554943091, 1.0761006215131537, 1.0747609473214361, 1.073547602874305, 1.0744724680833238, 1.074406277761475, 1.0727047154860347, 1.0716028671546522, 1.0719491433236008, 1.071706834489293, 1.0756827718127147, 1.0770139169614694, 1.0756380571716133, 1.0791682756593075, 1.0800371160256648, 1.0797378541213538, 1.0817837721021304, 1.083533598480162, 1.0874484562130005, 1.0833729233452056, 1.079652264004662, 1.0750994118563648, 1.0753755669288447, 1.0736708057729285, 1.0756537808573305, 1.074966984820875, 1.0779876000384, 1.076560675804251, 1.0744017395871417, 1.074413606294466, 1.072322808658744, 1.0731299330644029, 1.071962194293982, 1.0722286262731442, 1.072875810961418, 1.0732756185609915, 1.0727391401535185, 1.0728258789075027, 1.0716451895843782, 1.072050426785386, 1.0712941607035245, 1.073031125201772, 1.069362475562761, 1.0730674822733712, 1.0753443429035505, 1.0743034704173924, 1.075157953796324, 1.074330869175139, 1.074672110758, 1.0741451070422219, 1.0768394662045884, 1.0762489816825378, 1.0753881768835785, 1.0772038686451653, 1.0763542724556132, 1.0774997709615672, 1.0788202679215981, 1.0782254666138948, 1.080000014336434, 1.0780022007295456, 1.0784940122579314, 1.0779463007728063, 1.0812819250698746, 1.078081172088097, 1.0766097827889454, 1.0760841095584563, 1.075198174110187, 1.0740073283121896, 1.0737709662401422, 1.07900262329183, 1.0758043185047719, 1.0833142017104551, 1.0801099699314787, 1.0899766254894838, 1.085279443581116, 1.0860282352992467, 1.0799204725741558, 1.0814761919732556, 1.0813065424732777, 1.0787061192523475, 1.0784222566826982, 1.0785715092579131, 1.0779914742424375, 1.0775047691389061, 1.0843320615185892, 1.0821274843904969, 1.083144510125096, 1.0788868209607103, 1.0829411216557319, 1.0778836299633157, 1.0761801808925684, 1.082707541525266, 1.080363724227805, 1.079701933367499, 1.078668987026747, 1.085137348261177, 1.0839582202078282, 1.0823536884217035, 1.0777871461924662, 1.0787049663086439, 1.0819076348603849, 1.0763965159997173, 1.0725942810963722, 1.0781817316813227, 1.078905716122469, 1.0770552763210728, 1.078645402574774, 1.0816516872305784, 1.0837655615532535, 1.0878851830665701, 1.0838892262165964, 1.0856369931513845, 1.0832219403757055, 1.0845032576073959, 1.0855493770640081, 1.0878913060001942, 1.0862828941376534, 1.0824877180293668, 1.0808603800772054, 1.0962452585082532, 1.0810158047182807], 'val_acc': [0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.39408866911881857, 0.3908045965756102, 0.372742199007122, 0.4154351386432773, 0.39737274136840806, 0.39408866911881857, 0.39408866911881857, 0.3875205245217666, 0.39737274107478915, 0.40394088596546, 0.4072249583129225, 0.4006568138137435, 0.3908045967713561, 0.39080459696710207, 0.39408866911881857, 0.39408866921669156, 0.3940886690209456, 0.38587848839697186, 0.3908045968692291, 0.3875205247175126, 0.3908045968692291, 0.40558292228600074, 0.40229884993853826, 0.40229885003641125, 0.4006568139116165, 0.410509030562512, 0.40229885013428424, 0.3924466332876428, 0.4006568139116165, 0.3940886690209456, 0.3957307052436133, 0.40065681361799754, 0.3973727412705351, 0.3842364519785582, 0.39737274117266214, 0.3990147774932028, 0.4072249582150495, 0.3940886690209456, 0.3957307054393593, 0.38916256054868836, 0.3973727407811702, 0.40558292209025476, 0.3990147776889488, 0.39408866941243753, 0.38095238002258763, 0.40394088635695197, 0.403940886161206, 0.38916256064656135, 0.3891625609401803, 0.3908045968692291, 0.40065681361799754, 0.4006568135201246, 0.39737274136840806, 0.38916256064656135, 0.39080459696710207, 0.4055829224817467, 0.3924466332876428, 0.39244663289615084, 0.40558292228600074, 0.39901477778682176, 0.3940886690209456, 0.39737274136840806, 0.3924466335812617, 0.3924466332876428, 0.4055829225796197, 0.39080459706497506, 0.38259441624525536, 0.4088669947313362, 0.4088669947313362, 0.4055829225796197, 0.3875205246196396, 0.4006568139116165, 0.4055829224817467, 0.40722495880228743, 0.40229885003641125, 0.41050903075825795, 0.40229885013428424, 0.41050903075825795, 0.4088669946334632, 0.4006568139116165, 0.4170771753553099, 0.4203612475070264, 0.4154351390347692, 0.41871921148010466, 0.4154351391326422, 0.39408866911881857, 0.4121510668830527, 0.41707717515956394, 0.4088669946334632, 0.41050903046463905, 0.4022988502321572, 0.3940886696081835, 0.3973727412705351, 0.4006568134222516, 0.38916256074443434, 0.40065681371587053, 0.3957307053414863, 0.3858784886905908, 0.39080459716284804, 0.3990147776889488, 0.3924466331897698, 0.3973727417599, 0.38095237992471465, 0.39901477778682176, 0.39408866911881857, 0.372742199007122, 0.3924466330918968, 0.39737274156415403, 0.3924466327004049, 0.3924466331897698, 0.3924466330918968, 0.39408866911881857, 0.3973727412705351, 0.3957307050478674, 0.39244663250465894, 0.3957307048521214, 0.3875205243260207, 0.3990147776889488, 0.39737274156415403, 0.38095238002258763, 0.4039408858675871, 0.3990147776889488, 0.3957307055372323, 0.40558292218812775, 0.3908045968692291, 0.3924466332876428, 0.4088669946334632, 0.3990147775910758, 0.3990147775910758, 0.403940886161206, 0.39408866921669156, 0.4121510667851797, 0.40558292228600074, 0.39901477778682176, 0.4088669946334632, 0.397372741857773, 0.403940886259079, 0.403940886259079, 0.41050903075825795, 0.3924466332876428, 0.3924466332876428, 0.403940886161206, 0.4022988498406653, 0.4006568138137435, 0.40229884993853826, 0.40886699404622534, 0.41379310261635555, 0.40229885003641125, 0.4154351390347692, 0.41050903075825795, 0.4055829223838737, 0.41050903066038497, 0.417077175061691, 0.40558292218812775, 0.39737274136840806, 0.3990147774932028, 0.3990147774932028, 0.39901477778682176, 0.4006568140094895, 0.39080459706497506, 0.4006568139116165, 0.39408866931456454, 0.41050903085613094, 0.41379310300784744, 0.39408866941243753, 0.4022988502321572, 0.4006568140094895, 0.4022988502321572, 0.40229884993853826, 0.40558292228600074, 0.3924466332876428, 0.40394088635695197, 0.38916256054868836, 0.410509030562512, 0.397372741857773, 0.3957307054393593, 0.38587848849484485, 0.38587848859271784, 0.38587848849484485, 0.3957307053414863, 0.38916256064656135, 0.3924466329940238, 0.397372741662027, 0.40558292199238183, 0.3957307051457404, 0.4022988502321572, 0.38095238002258763, 0.3776683079687441, 0.37931034409353886, 0.38587848849484485, 0.39080459726072103, 0.39408866941243753, 0.3842364525657961, 0.38752052491325856, 0.37931034428928484, 0.3645320188725132, 0.3760262711588385, 0.3973727417599, 0.37766830787087113, 0.3875205241302747, 0.40229885013428424, 0.3809523797289687, 0.379310343310555, 0.40229884964491935, 0.40065681332437864, 0.40886699414409833, 0.4006568135201246, 0.3793103437999199, 0.3990147772974569, 0.3990147775910758, 0.403940886161206, 0.39408866882519966, 0.4006568139116165, 0.41379310261635555, 0.41050903066038497, 0.40722495850866847, 0.41050903036676606, 0.4006568134222516, 0.41379310310572043, 0.39080459726072103, 0.39408866941243753, 0.4072249583129225, 0.403940886161206, 0.4088669945355902, 0.41215106707879867, 0.4220032840233131, 0.4022988458768096, 0.3990147738229661, 0.42364532005023486, 0.40065680975201484, 0.3990147736272201, 0.4039408866505709, 0.4137931032035934, 0.3924466336791347, 0.4055829225796197, 0.40722495880228743, 0.4039408866505709, 0.41050903095400393, 0.42364532014810785, 0.42200327947221955, 0.4187192071247571, 0.4220032839254401, 0.4203612478006454, 0.41871921138223167, 0.4137931032035934, 0.40229885013428424, 0.40722495880228743, 0.4088669948292092, 0.41215106707879867, 0.4105090310518769, 0.39901477798256774, 0.41050903095400393, 0.41379309875037285, 0.40394088655269794, 0.39737273750242535, 0.4022988502321572, 0.39408866941243753, 0.41215106717667166, 0.4088669948292092, 0.42528735597928363, 0.39737274156415403, 0.41379310261635555, 0.40065681420523547, 0.40886699443771723, 0.38095237992471465, 0.4055829181263991, 0.4022988502321572, 0.3957307051457404, 0.39737274117266214, 0.3875205247175126, 0.4252873560771566, 0.4121510669809257, 0.4105090300731471, 0.4088669947313362, 0.403940886259079, 0.41379310261635555, 0.41050903066038497, 0.3957307054393593, 0.4236453199523619, 0.40229885003641125, 0.42364532014810785, 0.4269293921040784, 0.42692939181045947, 0.41871921148010466, 0.4088669942419713, 0.41050903075825795, 0.4203612473112805, 0.39737274136840806, 0.423645319462997, 0.4088669942419713, 0.39408866931456454, 0.42200328324032926, 0.39408866941243753, 0.39737274146628104, 0.40558292189450884, 0.3957307050478674, 0.4154351391326422], 'loss': [1.0958156452041876, 1.0808942881452963, 1.076995240540475, 1.0750909303982399, 1.0740516527967041, 1.0742546735602971, 1.0744962982083737, 1.0742037333012606, 1.0737607295018692, 1.0737348090941412, 1.0735010827591287, 1.073353254917466, 1.0734807584809571, 1.0733172352553884, 1.073210654317476, 1.0733320139761577, 1.07310475637291, 1.0732340876816235, 1.073529859783713, 1.0733885848791447, 1.073205481172832, 1.0735454352239808, 1.0733068738385148, 1.0732042537332804, 1.073102343498559, 1.0733239345726782, 1.073080529177703, 1.0728968762274396, 1.0727091928282313, 1.0721662718167784, 1.073130049108235, 1.0726613695861378, 1.0723453378285714, 1.072224387887567, 1.0715365021625338, 1.071581754302587, 1.0719177184898017, 1.0726475960420143, 1.0729405765415951, 1.072129947302033, 1.0720871122465976, 1.072318656194871, 1.072754037845306, 1.0717483582682679, 1.0736185340176372, 1.073568407661861, 1.0724714127409385, 1.0712517803944113, 1.0719223877732513, 1.0729608952876724, 1.0732380652084976, 1.0725775739740298, 1.0727580113577402, 1.0725759581129164, 1.0728844808112425, 1.0727675106001586, 1.0726097336539988, 1.0737179805121138, 1.0730779295829287, 1.0725905615201476, 1.0737068378949801, 1.072152410297668, 1.0727793804184367, 1.0724010754904463, 1.0724451524276264, 1.072539064477846, 1.071867081614735, 1.071896869935539, 1.0716487368519056, 1.0716856836538295, 1.0717651774016739, 1.071653224653287, 1.071605729322414, 1.0713938087898114, 1.0716142357252463, 1.0711287631636037, 1.0719472191416997, 1.0709603405586259, 1.0714561661655653, 1.0709244634091732, 1.0718372715572067, 1.071298290278143, 1.071438758867722, 1.0713920389602316, 1.0712637860182619, 1.071226144669237, 1.0713484519316185, 1.0713520242937782, 1.0717829896194488, 1.0715470442292137, 1.0710962861470372, 1.0708981032244234, 1.0711672405442663, 1.0703207136913981, 1.071483052193017, 1.0709567627622851, 1.0717666965735277, 1.07272523932878, 1.0724666230242845, 1.0725437336633827, 1.07195198237284, 1.0715595497977317, 1.0710685607099435, 1.0713443020775577, 1.0715199359388567, 1.0714916743781777, 1.0702887643778838, 1.070997634609622, 1.0720157830866945, 1.0723085302102247, 1.0727278876353583, 1.071153801814242, 1.0702581797292345, 1.0704051825055352, 1.070774570725537, 1.0712382919244943, 1.0715894052380164, 1.073028011586387, 1.0733071757537873, 1.0714966659917968, 1.0709432838389026, 1.0710042940272932, 1.0704525633759077, 1.069949654827862, 1.0707182215469329, 1.068750304216232, 1.0701419715763851, 1.0706025620995117, 1.0695197507341294, 1.0705994428305656, 1.0700372822720412, 1.0707051556702756, 1.071677209415475, 1.071716274324139, 1.0717640856208253, 1.071866631801613, 1.0706131344213623, 1.0723936257665896, 1.0699508222466376, 1.0694454316485833, 1.0697056900059663, 1.0700946475935669, 1.069134097863027, 1.0695368838750852, 1.0690190796489834, 1.0690966567954, 1.0690443733144834, 1.069689456001689, 1.068954109656003, 1.0691099013636, 1.0692915435689185, 1.0688237280816268, 1.068490190280781, 1.0678695704168364, 1.0682915671405362, 1.0687540886583269, 1.068367455676351, 1.0691876282682165, 1.0693612254865361, 1.069183710860031, 1.0688163458689037, 1.0683245536483044, 1.0678519764475265, 1.0684351710568218, 1.068615911873459, 1.0678288246327112, 1.067853232918334, 1.067843388727803, 1.0672342535406658, 1.0670817391338778, 1.0670565155252538, 1.0670549220862575, 1.0673056247053205, 1.0666427355282606, 1.0666916996546594, 1.06619565873665, 1.0661205284404558, 1.0679453691662704, 1.0657723447380614, 1.0657795142833701, 1.0672036916078729, 1.0684400525181201, 1.0675206486938915, 1.066041270712318, 1.0661123660555611, 1.0658458917782292, 1.065753653994331, 1.0669386052008283, 1.064844473725227, 1.0623365705262953, 1.0632878950244347, 1.0619944398652845, 1.0602436957173278, 1.06016440513932, 1.0611483550903977, 1.0592610961847482, 1.0603623734852128, 1.0609924380049813, 1.064875714392143, 1.0647026944698983, 1.063759675456758, 1.061044033303153, 1.0590865001541387, 1.0615450872777668, 1.068475221216801, 1.0676096851576036, 1.0780122729052752, 1.078618037774088, 1.0680358559199183, 1.0694470627351953, 1.0685510746996996, 1.069032517399876, 1.068622232119895, 1.0683270232143833, 1.067817799707213, 1.0677068029830588, 1.0665495348172511, 1.0658747510498798, 1.0657722824652827, 1.0663681380313035, 1.0649768901801453, 1.0645653037319927, 1.064756562871365, 1.064781517708326, 1.065704868804258, 1.065002988202371, 1.065210557767253, 1.0655465480972854, 1.0653555235089218, 1.0648370086290018, 1.0645143748308843, 1.06374211316236, 1.0645211447925294, 1.0632470909330145, 1.062535915286634, 1.062162430966904, 1.062020483203003, 1.0605590822270763, 1.0604477383517632, 1.0607780552497879, 1.0605175698317542, 1.060640149390673, 1.0613406262603384, 1.06138974455103, 1.0608659033902617, 1.0621036572622813, 1.0636156953580571, 1.060257942035213, 1.05836450471036, 1.0586665892747884, 1.0560886560279485, 1.0571824772402, 1.0558767901308972, 1.0586710961692387, 1.0579272264327846, 1.0564878301209248, 1.0558239890319856, 1.0553215315209767, 1.0548555483318698, 1.0557270147472437, 1.0543725426436938, 1.0565840446483918, 1.055343082847047, 1.0619271325379671, 1.0668221155476032, 1.0619038293983412, 1.060454366535132, 1.0602083697700893, 1.0566969410349947, 1.0571808245637333, 1.0562522846570495, 1.0551033754857904, 1.0526107205992117, 1.05206512704767, 1.0557954782333216, 1.0606061122255894, 1.059564699578334, 1.058574000570074, 1.057783592848807, 1.0549756583247096, 1.053064890465942, 1.053750520418312, 1.052749102561136, 1.0546102619269055, 1.0541837459716954, 1.0540482511755378, 1.0539531500677308, 1.051429372352741, 1.049931735669318, 1.0510865794070203, 1.0548117250387674, 1.0548795899816117, 1.0525105696684036, 1.0493602167164766, 1.0463209522823045, 1.0511591380626515, 1.053343173023122, 1.0527052904790921, 1.0498737388567758, 1.055789931696788], 'acc': [0.3843942496688459, 0.3942505134816532, 0.39425051508498143, 0.394250513714197, 0.39794661208099896, 0.3938398349211691, 0.38809035022645516, 0.40328542131173295, 0.39671457742763494, 0.39425051351837065, 0.3963039023920251, 0.4028747422861612, 0.3999999987638462, 0.40123203478799463, 0.4028747442444247, 0.39917864587762275, 0.4000000001346306, 0.3946611901573087, 0.39301847925665934, 0.4020533884575235, 0.4049281317840122, 0.39630390337115684, 0.3950718669553557, 0.39630390219619877, 0.39958932036247097, 0.4078028748779571, 0.39876796790461766, 0.40451745177930876, 0.4188911682159259, 0.4065708436270759, 0.39589322575308705, 0.3975359328596009, 0.40164271123110634, 0.4045174559283795, 0.4069815202292965, 0.4102669427771833, 0.41724845970191016, 0.41232032832178983, 0.4049281295932049, 0.4004106799435077, 0.4053388070154484, 0.4090349060064469, 0.40739219945069455, 0.4139630397732008, 0.4008213569740985, 0.40041067579443695, 0.40164271119438893, 0.41930184740060655, 0.4119096514870254, 0.40205339041578697, 0.40287474189450856, 0.40287474189450856, 0.4004106765777423, 0.4008213530208541, 0.402464063884786, 0.40246406408061236, 0.3991786466976456, 0.3954825471558855, 0.3946611928988776, 0.40657084124044224, 0.4004106775935915, 0.4065708442145549, 0.41642710524907595, 0.41724846087686823, 0.4057494850251709, 0.41642710528579335, 0.41642710646075143, 0.4172484610726946, 0.40903490812381926, 0.40328542307417004, 0.41190965184196066, 0.41806981415474437, 0.4188911711900386, 0.4106776172253141, 0.41273100786140565, 0.4176591393149609, 0.41026693862811253, 0.4147843928185332, 0.4127310084856022, 0.41642710646075143, 0.4102669427404658, 0.4127310055114895, 0.41642710646075143, 0.4110882960550594, 0.4151950712199084, 0.4102669398030706, 0.41724846169689106, 0.40862423015081417, 0.40328542131173295, 0.41642710567744606, 0.4098562643758081, 0.41067761745785786, 0.4127310082897758, 0.41273100770229676, 0.40410677811448337, 0.4135523609434555, 0.4082135535118761, 0.3963039020003724, 0.4008213530208541, 0.40041067739776515, 0.3983572896990688, 0.4028747440853158, 0.40533881116451914, 0.4028747416986822, 0.4028747422861612, 0.4082135507335898, 0.40533881054032267, 0.40944558358779926, 0.3987679663012894, 0.39958932372823636, 0.40369609932145545, 0.40698152065766663, 0.41724845970191016, 0.4073921984715628, 0.4069815186626857, 0.4016427090402991, 0.4073921949099711, 0.4016427108394537, 0.40574948858676263, 0.39917864336859765, 0.4032854212750155, 0.4110882942559048, 0.4090349055780767, 0.39876796790461766, 0.4036960965431691, 0.4229979443599066, 0.40944558476275733, 0.40944558675773823, 0.41724846009356287, 0.40698151846685937, 0.41314168610367197, 0.40903490616555577, 0.40492812962992236, 0.3971252550457048, 0.3946611909406141, 0.39753593642119267, 0.4078028746821308, 0.40123203282973113, 0.4151950741940211, 0.4110882956266893, 0.41190964890456544, 0.4168377814963613, 0.4114989712864956, 0.40533880959790836, 0.40944558417527827, 0.4176591355208254, 0.41519507141573475, 0.40698152065766663, 0.41108829742584385, 0.41149897265728, 0.40082135736575114, 0.41190965145030795, 0.41642710250750703, 0.4143737148088107, 0.41478439203522777, 0.41149897132321306, 0.41560574942545725, 0.41478439516844934, 0.4135523603559764, 0.41560574844632553, 0.42217658994378987, 0.4156057472713674, 0.41848049196864057, 0.42217659311372885, 0.4176591365366746, 0.41396304114398524, 0.42340862577211197, 0.41971252580198176, 0.41355235976849736, 0.4184804925561196, 0.42464065565220876, 0.424229978621618, 0.4250513358527385, 0.4156057516162645, 0.41806981317561265, 0.4328542105593476, 0.42258726795351237, 0.4193018472047802, 0.4184804921644669, 0.4184804937677951, 0.4164271046615969, 0.41149897187397466, 0.41149897109066924, 0.42299794870480373, 0.41519507301906294, 0.4147843922310541, 0.4250513365993265, 0.4168377842746476, 0.412320327501767, 0.4221765937012079, 0.4160164294301607, 0.4369609853692613, 0.4369609853692613, 0.42710472351471745, 0.4254620130424382, 0.4373716641622892, 0.43244353098301425, 0.43121149910793655, 0.41355236117599925, 0.4209445604553458, 0.41560574864215183, 0.4275153995661765, 0.44435318386530237, 0.44147843818889754, 0.4110882968383648, 0.42094455767705946, 0.38850102529878244, 0.39876796907957573, 0.41971252462702363, 0.4016427113902153, 0.419712527992789, 0.4073921949099711, 0.41765913614502187, 0.42012320283257254, 0.42669404554171236, 0.4197125254470465, 0.4275154015611574, 0.4258726898772026, 0.42874743519867226, 0.42176591408815717, 0.42546201402156997, 0.41765913731997995, 0.42422998136318685, 0.4234086251479155, 0.414784394972623, 0.4250513358527385, 0.417248460521933, 0.42299794459245044, 0.42135523768176286, 0.42792607738007266, 0.41806981673720434, 0.4324435311788406, 0.418891168803405, 0.42012320302839884, 0.4271047237105438, 0.4303901452792988, 0.4328542075852349, 0.4324435315704933, 0.4406570855352179, 0.43367556677461894, 0.4266940441342105, 0.4295687904348119, 0.4205338826414496, 0.42217659013961617, 0.43531827450532934, 0.42217659252624984, 0.4238192996327637, 0.4340862440377535, 0.4394250520568119, 0.435728954277489, 0.4386036960373669, 0.4377823390387901, 0.44188911577024986, 0.4234086247562628, 0.4275153995661765, 0.4349075953206487, 0.4250513360485649, 0.43408624263025164, 0.4377823394304428, 0.4336755654038345, 0.4349075972789122, 0.4406570829527579, 0.43819302057338694, 0.43039014606260423, 0.41355236313426275, 0.4254620140582874, 0.4188911723649967, 0.426283368706948, 0.4188911711900386, 0.42422997725083356, 0.42299794655071393, 0.425462013862461, 0.43326488758993836, 0.43326488719828565, 0.4316221755510483, 0.41683778368716856, 0.42135523607843467, 0.4184804943552742, 0.42505133366193126, 0.43613963232392894, 0.44804928203627803, 0.4295687892598538, 0.4410677638998756, 0.4447638614833722, 0.4340862408678145, 0.4353182768552455, 0.44435318327782336, 0.4394250522526383, 0.444353183081997, 0.44147844073464004, 0.4357289521233991, 0.4266940443300369, 0.4373716609923502, 0.43326488915654915, 0.4451745365556995, 0.4406570860859795, 0.4332648854358485, 0.4443531840611287, 0.44722792817092283, 0.4418891158069673]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
