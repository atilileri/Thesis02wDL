{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf2.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.26 11:36:29 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Freqs', 'channelMode': 'All', 'classificationMode': 'Posture5', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 5 Label(s): ['02', '04', '01', '05', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 5 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Freqs------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001F101F4BE80>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001F150A27EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 5)                 65        \n",
    "=================================================================\n",
    "Total params: 19,625\n",
    "Trainable params: 19,625\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.6122, Accuracy:0.1893, Validation Loss:1.6097, Validation Accuracy:0.2151\n",
    "Epoch #2: Loss:1.6064, Accuracy:0.2238, Validation Loss:1.6071, Validation Accuracy:0.2365\n",
    "Epoch #3: Loss:1.6060, Accuracy:0.2333, Validation Loss:1.6064, Validation Accuracy:0.2348\n",
    "Epoch #4: Loss:1.6049, Accuracy:0.2316, Validation Loss:1.6065, Validation Accuracy:0.2348\n",
    "Epoch #5: Loss:1.6045, Accuracy:0.2316, Validation Loss:1.6062, Validation Accuracy:0.2299\n",
    "Epoch #6: Loss:1.6042, Accuracy:0.2300, Validation Loss:1.6053, Validation Accuracy:0.2348\n",
    "Epoch #7: Loss:1.6041, Accuracy:0.2312, Validation Loss:1.6049, Validation Accuracy:0.2332\n",
    "Epoch #8: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6045, Validation Accuracy:0.2332\n",
    "Epoch #9: Loss:1.6050, Accuracy:0.2329, Validation Loss:1.6044, Validation Accuracy:0.2332\n",
    "Epoch #10: Loss:1.6047, Accuracy:0.2329, Validation Loss:1.6058, Validation Accuracy:0.2348\n",
    "Epoch #11: Loss:1.6050, Accuracy:0.2312, Validation Loss:1.6061, Validation Accuracy:0.2365\n",
    "Epoch #12: Loss:1.6044, Accuracy:0.2312, Validation Loss:1.6058, Validation Accuracy:0.2365\n",
    "Epoch #13: Loss:1.6034, Accuracy:0.2337, Validation Loss:1.6060, Validation Accuracy:0.2332\n",
    "Epoch #14: Loss:1.6036, Accuracy:0.2329, Validation Loss:1.6068, Validation Accuracy:0.2332\n",
    "Epoch #15: Loss:1.6025, Accuracy:0.2333, Validation Loss:1.6064, Validation Accuracy:0.2381\n",
    "Epoch #16: Loss:1.6022, Accuracy:0.2341, Validation Loss:1.6058, Validation Accuracy:0.2315\n",
    "Epoch #17: Loss:1.6023, Accuracy:0.2316, Validation Loss:1.6073, Validation Accuracy:0.2381\n",
    "Epoch #18: Loss:1.6026, Accuracy:0.2370, Validation Loss:1.6070, Validation Accuracy:0.2447\n",
    "Epoch #19: Loss:1.6020, Accuracy:0.2366, Validation Loss:1.6070, Validation Accuracy:0.2397\n",
    "Epoch #20: Loss:1.6008, Accuracy:0.2411, Validation Loss:1.6066, Validation Accuracy:0.2332\n",
    "Epoch #21: Loss:1.6021, Accuracy:0.2374, Validation Loss:1.6085, Validation Accuracy:0.2332\n",
    "Epoch #22: Loss:1.6023, Accuracy:0.2320, Validation Loss:1.6080, Validation Accuracy:0.2299\n",
    "Epoch #23: Loss:1.6016, Accuracy:0.2444, Validation Loss:1.6081, Validation Accuracy:0.2414\n",
    "Epoch #24: Loss:1.6015, Accuracy:0.2398, Validation Loss:1.6076, Validation Accuracy:0.2397\n",
    "Epoch #25: Loss:1.6012, Accuracy:0.2415, Validation Loss:1.6077, Validation Accuracy:0.2414\n",
    "Epoch #26: Loss:1.6016, Accuracy:0.2337, Validation Loss:1.6092, Validation Accuracy:0.2381\n",
    "Epoch #27: Loss:1.6007, Accuracy:0.2398, Validation Loss:1.6089, Validation Accuracy:0.2397\n",
    "Epoch #28: Loss:1.6003, Accuracy:0.2444, Validation Loss:1.6086, Validation Accuracy:0.2397\n",
    "Epoch #29: Loss:1.6001, Accuracy:0.2435, Validation Loss:1.6088, Validation Accuracy:0.2381\n",
    "Epoch #30: Loss:1.5998, Accuracy:0.2407, Validation Loss:1.6091, Validation Accuracy:0.2414\n",
    "Epoch #31: Loss:1.5997, Accuracy:0.2402, Validation Loss:1.6090, Validation Accuracy:0.2381\n",
    "Epoch #32: Loss:1.5994, Accuracy:0.2398, Validation Loss:1.6087, Validation Accuracy:0.2381\n",
    "Epoch #33: Loss:1.5989, Accuracy:0.2411, Validation Loss:1.6086, Validation Accuracy:0.2381\n",
    "Epoch #34: Loss:1.5996, Accuracy:0.2419, Validation Loss:1.6089, Validation Accuracy:0.2414\n",
    "Epoch #35: Loss:1.5987, Accuracy:0.2427, Validation Loss:1.6090, Validation Accuracy:0.2381\n",
    "Epoch #36: Loss:1.5991, Accuracy:0.2394, Validation Loss:1.6095, Validation Accuracy:0.2414\n",
    "Epoch #37: Loss:1.5988, Accuracy:0.2415, Validation Loss:1.6089, Validation Accuracy:0.2381\n",
    "Epoch #38: Loss:1.5983, Accuracy:0.2431, Validation Loss:1.6094, Validation Accuracy:0.2381\n",
    "Epoch #39: Loss:1.5983, Accuracy:0.2423, Validation Loss:1.6091, Validation Accuracy:0.2381\n",
    "Epoch #40: Loss:1.5981, Accuracy:0.2431, Validation Loss:1.6093, Validation Accuracy:0.2381\n",
    "Epoch #41: Loss:1.5979, Accuracy:0.2423, Validation Loss:1.6090, Validation Accuracy:0.2397\n",
    "Epoch #42: Loss:1.5975, Accuracy:0.2411, Validation Loss:1.6089, Validation Accuracy:0.2414\n",
    "Epoch #43: Loss:1.5975, Accuracy:0.2435, Validation Loss:1.6093, Validation Accuracy:0.2381\n",
    "Epoch #44: Loss:1.5969, Accuracy:0.2444, Validation Loss:1.6096, Validation Accuracy:0.2414\n",
    "Epoch #45: Loss:1.5970, Accuracy:0.2427, Validation Loss:1.6093, Validation Accuracy:0.2447\n",
    "Epoch #46: Loss:1.5973, Accuracy:0.2415, Validation Loss:1.6097, Validation Accuracy:0.2430\n",
    "Epoch #47: Loss:1.5974, Accuracy:0.2419, Validation Loss:1.6098, Validation Accuracy:0.2414\n",
    "Epoch #48: Loss:1.5976, Accuracy:0.2415, Validation Loss:1.6106, Validation Accuracy:0.2365\n",
    "Epoch #49: Loss:1.5972, Accuracy:0.2394, Validation Loss:1.6113, Validation Accuracy:0.2315\n",
    "Epoch #50: Loss:1.6035, Accuracy:0.2296, Validation Loss:1.6177, Validation Accuracy:0.2036\n",
    "Epoch #51: Loss:1.6045, Accuracy:0.2131, Validation Loss:1.6122, Validation Accuracy:0.2020\n",
    "Epoch #52: Loss:1.6011, Accuracy:0.2304, Validation Loss:1.6103, Validation Accuracy:0.2365\n",
    "Epoch #53: Loss:1.6014, Accuracy:0.2444, Validation Loss:1.6091, Validation Accuracy:0.2315\n",
    "Epoch #54: Loss:1.6016, Accuracy:0.2316, Validation Loss:1.6093, Validation Accuracy:0.2348\n",
    "Epoch #55: Loss:1.6011, Accuracy:0.2320, Validation Loss:1.6094, Validation Accuracy:0.2332\n",
    "Epoch #56: Loss:1.6008, Accuracy:0.2345, Validation Loss:1.6097, Validation Accuracy:0.2397\n",
    "Epoch #57: Loss:1.5997, Accuracy:0.2439, Validation Loss:1.6098, Validation Accuracy:0.2414\n",
    "Epoch #58: Loss:1.5991, Accuracy:0.2460, Validation Loss:1.6098, Validation Accuracy:0.2447\n",
    "Epoch #59: Loss:1.5987, Accuracy:0.2444, Validation Loss:1.6100, Validation Accuracy:0.2447\n",
    "Epoch #60: Loss:1.5984, Accuracy:0.2468, Validation Loss:1.6103, Validation Accuracy:0.2381\n",
    "Epoch #61: Loss:1.5984, Accuracy:0.2444, Validation Loss:1.6101, Validation Accuracy:0.2397\n",
    "Epoch #62: Loss:1.5985, Accuracy:0.2456, Validation Loss:1.6107, Validation Accuracy:0.2463\n",
    "Epoch #63: Loss:1.5978, Accuracy:0.2415, Validation Loss:1.6106, Validation Accuracy:0.2447\n",
    "Epoch #64: Loss:1.5981, Accuracy:0.2402, Validation Loss:1.6106, Validation Accuracy:0.2447\n",
    "Epoch #65: Loss:1.5983, Accuracy:0.2452, Validation Loss:1.6104, Validation Accuracy:0.2414\n",
    "Epoch #66: Loss:1.5975, Accuracy:0.2423, Validation Loss:1.6105, Validation Accuracy:0.2430\n",
    "Epoch #67: Loss:1.5978, Accuracy:0.2423, Validation Loss:1.6109, Validation Accuracy:0.2414\n",
    "Epoch #68: Loss:1.5979, Accuracy:0.2411, Validation Loss:1.6115, Validation Accuracy:0.2397\n",
    "Epoch #69: Loss:1.5975, Accuracy:0.2427, Validation Loss:1.6112, Validation Accuracy:0.2430\n",
    "Epoch #70: Loss:1.5974, Accuracy:0.2452, Validation Loss:1.6116, Validation Accuracy:0.2430\n",
    "Epoch #71: Loss:1.5973, Accuracy:0.2427, Validation Loss:1.6113, Validation Accuracy:0.2381\n",
    "Epoch #72: Loss:1.5974, Accuracy:0.2415, Validation Loss:1.6116, Validation Accuracy:0.2414\n",
    "Epoch #73: Loss:1.5976, Accuracy:0.2402, Validation Loss:1.6109, Validation Accuracy:0.2299\n",
    "Epoch #74: Loss:1.5978, Accuracy:0.2394, Validation Loss:1.6103, Validation Accuracy:0.2282\n",
    "Epoch #75: Loss:1.5978, Accuracy:0.2374, Validation Loss:1.6101, Validation Accuracy:0.2332\n",
    "Epoch #76: Loss:1.5966, Accuracy:0.2444, Validation Loss:1.6139, Validation Accuracy:0.2151\n",
    "Epoch #77: Loss:1.6009, Accuracy:0.2361, Validation Loss:1.6123, Validation Accuracy:0.2397\n",
    "Epoch #78: Loss:1.5994, Accuracy:0.2361, Validation Loss:1.6117, Validation Accuracy:0.2282\n",
    "Epoch #79: Loss:1.5987, Accuracy:0.2382, Validation Loss:1.6105, Validation Accuracy:0.2266\n",
    "Epoch #80: Loss:1.5980, Accuracy:0.2394, Validation Loss:1.6102, Validation Accuracy:0.2463\n",
    "Epoch #81: Loss:1.5977, Accuracy:0.2361, Validation Loss:1.6105, Validation Accuracy:0.2397\n",
    "Epoch #82: Loss:1.5976, Accuracy:0.2411, Validation Loss:1.6098, Validation Accuracy:0.2348\n",
    "Epoch #83: Loss:1.5971, Accuracy:0.2378, Validation Loss:1.6095, Validation Accuracy:0.2430\n",
    "Epoch #84: Loss:1.5968, Accuracy:0.2386, Validation Loss:1.6101, Validation Accuracy:0.2348\n",
    "Epoch #85: Loss:1.5971, Accuracy:0.2398, Validation Loss:1.6098, Validation Accuracy:0.2332\n",
    "Epoch #86: Loss:1.5966, Accuracy:0.2398, Validation Loss:1.6104, Validation Accuracy:0.2315\n",
    "Epoch #87: Loss:1.5965, Accuracy:0.2366, Validation Loss:1.6112, Validation Accuracy:0.2315\n",
    "Epoch #88: Loss:1.5964, Accuracy:0.2382, Validation Loss:1.6113, Validation Accuracy:0.2315\n",
    "Epoch #89: Loss:1.5964, Accuracy:0.2394, Validation Loss:1.6111, Validation Accuracy:0.2332\n",
    "Epoch #90: Loss:1.5965, Accuracy:0.2411, Validation Loss:1.6110, Validation Accuracy:0.2447\n",
    "Epoch #91: Loss:1.5962, Accuracy:0.2448, Validation Loss:1.6123, Validation Accuracy:0.2365\n",
    "Epoch #92: Loss:1.5958, Accuracy:0.2394, Validation Loss:1.6135, Validation Accuracy:0.2282\n",
    "Epoch #93: Loss:1.5960, Accuracy:0.2390, Validation Loss:1.6141, Validation Accuracy:0.2315\n",
    "Epoch #94: Loss:1.5961, Accuracy:0.2402, Validation Loss:1.6123, Validation Accuracy:0.2365\n",
    "Epoch #95: Loss:1.5960, Accuracy:0.2361, Validation Loss:1.6119, Validation Accuracy:0.2233\n",
    "Epoch #96: Loss:1.5956, Accuracy:0.2390, Validation Loss:1.6121, Validation Accuracy:0.2250\n",
    "Epoch #97: Loss:1.5955, Accuracy:0.2411, Validation Loss:1.6116, Validation Accuracy:0.2167\n",
    "Epoch #98: Loss:1.5964, Accuracy:0.2370, Validation Loss:1.6119, Validation Accuracy:0.2167\n",
    "Epoch #99: Loss:1.6009, Accuracy:0.2349, Validation Loss:1.6126, Validation Accuracy:0.2217\n",
    "Epoch #100: Loss:1.5978, Accuracy:0.2427, Validation Loss:1.6154, Validation Accuracy:0.2036\n",
    "Epoch #101: Loss:1.5990, Accuracy:0.2370, Validation Loss:1.6116, Validation Accuracy:0.2332\n",
    "Epoch #102: Loss:1.5956, Accuracy:0.2411, Validation Loss:1.6115, Validation Accuracy:0.2282\n",
    "Epoch #103: Loss:1.5963, Accuracy:0.2370, Validation Loss:1.6112, Validation Accuracy:0.2233\n",
    "Epoch #104: Loss:1.5957, Accuracy:0.2353, Validation Loss:1.6111, Validation Accuracy:0.2348\n",
    "Epoch #105: Loss:1.5955, Accuracy:0.2382, Validation Loss:1.6118, Validation Accuracy:0.2315\n",
    "Epoch #106: Loss:1.5959, Accuracy:0.2366, Validation Loss:1.6135, Validation Accuracy:0.2348\n",
    "Epoch #107: Loss:1.5952, Accuracy:0.2427, Validation Loss:1.6132, Validation Accuracy:0.2233\n",
    "Epoch #108: Loss:1.5958, Accuracy:0.2402, Validation Loss:1.6128, Validation Accuracy:0.2069\n",
    "Epoch #109: Loss:1.5955, Accuracy:0.2398, Validation Loss:1.6121, Validation Accuracy:0.2381\n",
    "Epoch #110: Loss:1.5966, Accuracy:0.2431, Validation Loss:1.6126, Validation Accuracy:0.2414\n",
    "Epoch #111: Loss:1.5954, Accuracy:0.2423, Validation Loss:1.6128, Validation Accuracy:0.2217\n",
    "Epoch #112: Loss:1.5957, Accuracy:0.2460, Validation Loss:1.6122, Validation Accuracy:0.2200\n",
    "Epoch #113: Loss:1.5953, Accuracy:0.2411, Validation Loss:1.6128, Validation Accuracy:0.2332\n",
    "Epoch #114: Loss:1.5953, Accuracy:0.2431, Validation Loss:1.6120, Validation Accuracy:0.2233\n",
    "Epoch #115: Loss:1.5946, Accuracy:0.2468, Validation Loss:1.6125, Validation Accuracy:0.2167\n",
    "Epoch #116: Loss:1.5942, Accuracy:0.2386, Validation Loss:1.6120, Validation Accuracy:0.2381\n",
    "Epoch #117: Loss:1.5942, Accuracy:0.2423, Validation Loss:1.6133, Validation Accuracy:0.2381\n",
    "Epoch #118: Loss:1.5941, Accuracy:0.2435, Validation Loss:1.6133, Validation Accuracy:0.2102\n",
    "Epoch #119: Loss:1.5939, Accuracy:0.2505, Validation Loss:1.6137, Validation Accuracy:0.2102\n",
    "Epoch #120: Loss:1.5940, Accuracy:0.2493, Validation Loss:1.6148, Validation Accuracy:0.2085\n",
    "Epoch #121: Loss:1.5939, Accuracy:0.2480, Validation Loss:1.6153, Validation Accuracy:0.2118\n",
    "Epoch #122: Loss:1.5938, Accuracy:0.2530, Validation Loss:1.6147, Validation Accuracy:0.2299\n",
    "Epoch #123: Loss:1.5943, Accuracy:0.2468, Validation Loss:1.6141, Validation Accuracy:0.2348\n",
    "Epoch #124: Loss:1.5944, Accuracy:0.2394, Validation Loss:1.6129, Validation Accuracy:0.2414\n",
    "Epoch #125: Loss:1.5949, Accuracy:0.2411, Validation Loss:1.6148, Validation Accuracy:0.2167\n",
    "Epoch #126: Loss:1.5942, Accuracy:0.2419, Validation Loss:1.6146, Validation Accuracy:0.2397\n",
    "Epoch #127: Loss:1.5950, Accuracy:0.2444, Validation Loss:1.6137, Validation Accuracy:0.2414\n",
    "Epoch #128: Loss:1.5947, Accuracy:0.2485, Validation Loss:1.6137, Validation Accuracy:0.2167\n",
    "Epoch #129: Loss:1.5936, Accuracy:0.2423, Validation Loss:1.6138, Validation Accuracy:0.2365\n",
    "Epoch #130: Loss:1.5935, Accuracy:0.2464, Validation Loss:1.6140, Validation Accuracy:0.2299\n",
    "Epoch #131: Loss:1.5942, Accuracy:0.2431, Validation Loss:1.6145, Validation Accuracy:0.2200\n",
    "Epoch #132: Loss:1.5941, Accuracy:0.2489, Validation Loss:1.6136, Validation Accuracy:0.2348\n",
    "Epoch #133: Loss:1.5936, Accuracy:0.2464, Validation Loss:1.6126, Validation Accuracy:0.2299\n",
    "Epoch #134: Loss:1.5924, Accuracy:0.2489, Validation Loss:1.6127, Validation Accuracy:0.2167\n",
    "Epoch #135: Loss:1.5932, Accuracy:0.2456, Validation Loss:1.6122, Validation Accuracy:0.2348\n",
    "Epoch #136: Loss:1.5948, Accuracy:0.2439, Validation Loss:1.6108, Validation Accuracy:0.2381\n",
    "Epoch #137: Loss:1.5952, Accuracy:0.2468, Validation Loss:1.6098, Validation Accuracy:0.2332\n",
    "Epoch #138: Loss:1.5955, Accuracy:0.2460, Validation Loss:1.6107, Validation Accuracy:0.2332\n",
    "Epoch #139: Loss:1.5954, Accuracy:0.2394, Validation Loss:1.6111, Validation Accuracy:0.2381\n",
    "Epoch #140: Loss:1.5960, Accuracy:0.2444, Validation Loss:1.6105, Validation Accuracy:0.2332\n",
    "Epoch #141: Loss:1.5952, Accuracy:0.2435, Validation Loss:1.6108, Validation Accuracy:0.2167\n",
    "Epoch #142: Loss:1.5956, Accuracy:0.2456, Validation Loss:1.6100, Validation Accuracy:0.2397\n",
    "Epoch #143: Loss:1.5948, Accuracy:0.2464, Validation Loss:1.6106, Validation Accuracy:0.2299\n",
    "Epoch #144: Loss:1.5957, Accuracy:0.2427, Validation Loss:1.6100, Validation Accuracy:0.2397\n",
    "Epoch #145: Loss:1.5946, Accuracy:0.2431, Validation Loss:1.6103, Validation Accuracy:0.2365\n",
    "Epoch #146: Loss:1.5945, Accuracy:0.2435, Validation Loss:1.6105, Validation Accuracy:0.2381\n",
    "Epoch #147: Loss:1.5943, Accuracy:0.2489, Validation Loss:1.6111, Validation Accuracy:0.2167\n",
    "Epoch #148: Loss:1.5961, Accuracy:0.2435, Validation Loss:1.6106, Validation Accuracy:0.2430\n",
    "Epoch #149: Loss:1.5938, Accuracy:0.2456, Validation Loss:1.6115, Validation Accuracy:0.2414\n",
    "Epoch #150: Loss:1.5946, Accuracy:0.2423, Validation Loss:1.6116, Validation Accuracy:0.2266\n",
    "Epoch #151: Loss:1.5934, Accuracy:0.2431, Validation Loss:1.6111, Validation Accuracy:0.2332\n",
    "Epoch #152: Loss:1.5933, Accuracy:0.2485, Validation Loss:1.6120, Validation Accuracy:0.2233\n",
    "Epoch #153: Loss:1.5925, Accuracy:0.2468, Validation Loss:1.6123, Validation Accuracy:0.2365\n",
    "Epoch #154: Loss:1.5928, Accuracy:0.2472, Validation Loss:1.6116, Validation Accuracy:0.2348\n",
    "Epoch #155: Loss:1.5922, Accuracy:0.2485, Validation Loss:1.6128, Validation Accuracy:0.2151\n",
    "Epoch #156: Loss:1.5928, Accuracy:0.2468, Validation Loss:1.6119, Validation Accuracy:0.2299\n",
    "Epoch #157: Loss:1.5929, Accuracy:0.2431, Validation Loss:1.6122, Validation Accuracy:0.2151\n",
    "Epoch #158: Loss:1.5925, Accuracy:0.2464, Validation Loss:1.6126, Validation Accuracy:0.2397\n",
    "Epoch #159: Loss:1.5927, Accuracy:0.2444, Validation Loss:1.6112, Validation Accuracy:0.2217\n",
    "Epoch #160: Loss:1.5920, Accuracy:0.2476, Validation Loss:1.6107, Validation Accuracy:0.2266\n",
    "Epoch #161: Loss:1.5923, Accuracy:0.2517, Validation Loss:1.6103, Validation Accuracy:0.2200\n",
    "Epoch #162: Loss:1.5924, Accuracy:0.2452, Validation Loss:1.6105, Validation Accuracy:0.2414\n",
    "Epoch #163: Loss:1.5902, Accuracy:0.2501, Validation Loss:1.6133, Validation Accuracy:0.2151\n",
    "Epoch #164: Loss:1.5897, Accuracy:0.2587, Validation Loss:1.6138, Validation Accuracy:0.2102\n",
    "Epoch #165: Loss:1.5905, Accuracy:0.2538, Validation Loss:1.6134, Validation Accuracy:0.2184\n",
    "Epoch #166: Loss:1.5897, Accuracy:0.2489, Validation Loss:1.6140, Validation Accuracy:0.2184\n",
    "Epoch #167: Loss:1.5903, Accuracy:0.2546, Validation Loss:1.6147, Validation Accuracy:0.2250\n",
    "Epoch #168: Loss:1.5921, Accuracy:0.2444, Validation Loss:1.6135, Validation Accuracy:0.2151\n",
    "Epoch #169: Loss:1.5915, Accuracy:0.2489, Validation Loss:1.6143, Validation Accuracy:0.2250\n",
    "Epoch #170: Loss:1.5904, Accuracy:0.2530, Validation Loss:1.6124, Validation Accuracy:0.2200\n",
    "Epoch #171: Loss:1.5920, Accuracy:0.2452, Validation Loss:1.6140, Validation Accuracy:0.2135\n",
    "Epoch #172: Loss:1.5932, Accuracy:0.2448, Validation Loss:1.6126, Validation Accuracy:0.2069\n",
    "Epoch #173: Loss:1.5932, Accuracy:0.2542, Validation Loss:1.6124, Validation Accuracy:0.2102\n",
    "Epoch #174: Loss:1.5894, Accuracy:0.2554, Validation Loss:1.6147, Validation Accuracy:0.2135\n",
    "Epoch #175: Loss:1.5911, Accuracy:0.2517, Validation Loss:1.6123, Validation Accuracy:0.2151\n",
    "Epoch #176: Loss:1.5897, Accuracy:0.2550, Validation Loss:1.6115, Validation Accuracy:0.2184\n",
    "Epoch #177: Loss:1.5888, Accuracy:0.2579, Validation Loss:1.6120, Validation Accuracy:0.2217\n",
    "Epoch #178: Loss:1.5891, Accuracy:0.2522, Validation Loss:1.6122, Validation Accuracy:0.2167\n",
    "Epoch #179: Loss:1.5894, Accuracy:0.2480, Validation Loss:1.6122, Validation Accuracy:0.2381\n",
    "Epoch #180: Loss:1.5904, Accuracy:0.2439, Validation Loss:1.6129, Validation Accuracy:0.2118\n",
    "Epoch #181: Loss:1.5901, Accuracy:0.2468, Validation Loss:1.6142, Validation Accuracy:0.2282\n",
    "Epoch #182: Loss:1.5874, Accuracy:0.2530, Validation Loss:1.6136, Validation Accuracy:0.2151\n",
    "Epoch #183: Loss:1.5871, Accuracy:0.2649, Validation Loss:1.6137, Validation Accuracy:0.2332\n",
    "Epoch #184: Loss:1.5886, Accuracy:0.2501, Validation Loss:1.6145, Validation Accuracy:0.2151\n",
    "Epoch #185: Loss:1.5868, Accuracy:0.2468, Validation Loss:1.6146, Validation Accuracy:0.2135\n",
    "Epoch #186: Loss:1.5873, Accuracy:0.2575, Validation Loss:1.6146, Validation Accuracy:0.2266\n",
    "Epoch #187: Loss:1.5868, Accuracy:0.2575, Validation Loss:1.6160, Validation Accuracy:0.2167\n",
    "Epoch #188: Loss:1.5865, Accuracy:0.2559, Validation Loss:1.6151, Validation Accuracy:0.2151\n",
    "Epoch #189: Loss:1.5876, Accuracy:0.2517, Validation Loss:1.6170, Validation Accuracy:0.2151\n",
    "Epoch #190: Loss:1.5867, Accuracy:0.2534, Validation Loss:1.6162, Validation Accuracy:0.2151\n",
    "Epoch #191: Loss:1.5867, Accuracy:0.2559, Validation Loss:1.6155, Validation Accuracy:0.2250\n",
    "Epoch #192: Loss:1.5870, Accuracy:0.2493, Validation Loss:1.6160, Validation Accuracy:0.2200\n",
    "Epoch #193: Loss:1.5894, Accuracy:0.2480, Validation Loss:1.6142, Validation Accuracy:0.2233\n",
    "Epoch #194: Loss:1.5880, Accuracy:0.2583, Validation Loss:1.6154, Validation Accuracy:0.2233\n",
    "Epoch #195: Loss:1.5890, Accuracy:0.2505, Validation Loss:1.6142, Validation Accuracy:0.2102\n",
    "Epoch #196: Loss:1.5874, Accuracy:0.2513, Validation Loss:1.6163, Validation Accuracy:0.2135\n",
    "Epoch #197: Loss:1.5876, Accuracy:0.2526, Validation Loss:1.6134, Validation Accuracy:0.2151\n",
    "Epoch #198: Loss:1.5878, Accuracy:0.2472, Validation Loss:1.6119, Validation Accuracy:0.2184\n",
    "Epoch #199: Loss:1.5871, Accuracy:0.2522, Validation Loss:1.6143, Validation Accuracy:0.2200\n",
    "Epoch #200: Loss:1.5884, Accuracy:0.2497, Validation Loss:1.6135, Validation Accuracy:0.2200\n",
    "Epoch #201: Loss:1.5897, Accuracy:0.2419, Validation Loss:1.6140, Validation Accuracy:0.2167\n",
    "Epoch #202: Loss:1.5872, Accuracy:0.2517, Validation Loss:1.6199, Validation Accuracy:0.2053\n",
    "Epoch #203: Loss:1.5868, Accuracy:0.2575, Validation Loss:1.6178, Validation Accuracy:0.2053\n",
    "Epoch #204: Loss:1.5882, Accuracy:0.2460, Validation Loss:1.6172, Validation Accuracy:0.2118\n",
    "Epoch #205: Loss:1.5865, Accuracy:0.2595, Validation Loss:1.6169, Validation Accuracy:0.2069\n",
    "Epoch #206: Loss:1.5869, Accuracy:0.2587, Validation Loss:1.6162, Validation Accuracy:0.2003\n",
    "Epoch #207: Loss:1.5869, Accuracy:0.2538, Validation Loss:1.6163, Validation Accuracy:0.2036\n",
    "Epoch #208: Loss:1.5871, Accuracy:0.2505, Validation Loss:1.6174, Validation Accuracy:0.2118\n",
    "Epoch #209: Loss:1.5856, Accuracy:0.2628, Validation Loss:1.6168, Validation Accuracy:0.2135\n",
    "Epoch #210: Loss:1.5880, Accuracy:0.2497, Validation Loss:1.6180, Validation Accuracy:0.2020\n",
    "Epoch #211: Loss:1.5863, Accuracy:0.2538, Validation Loss:1.6181, Validation Accuracy:0.2151\n",
    "Epoch #212: Loss:1.5863, Accuracy:0.2554, Validation Loss:1.6171, Validation Accuracy:0.2102\n",
    "Epoch #213: Loss:1.5866, Accuracy:0.2559, Validation Loss:1.6171, Validation Accuracy:0.2266\n",
    "Epoch #214: Loss:1.5874, Accuracy:0.2489, Validation Loss:1.6185, Validation Accuracy:0.2217\n",
    "Epoch #215: Loss:1.5896, Accuracy:0.2542, Validation Loss:1.6200, Validation Accuracy:0.2085\n",
    "Epoch #216: Loss:1.5872, Accuracy:0.2604, Validation Loss:1.6157, Validation Accuracy:0.2217\n",
    "Epoch #217: Loss:1.5869, Accuracy:0.2542, Validation Loss:1.6135, Validation Accuracy:0.2118\n",
    "Epoch #218: Loss:1.5871, Accuracy:0.2579, Validation Loss:1.6112, Validation Accuracy:0.2102\n",
    "Epoch #219: Loss:1.5890, Accuracy:0.2505, Validation Loss:1.6089, Validation Accuracy:0.2200\n",
    "Epoch #220: Loss:1.5877, Accuracy:0.2452, Validation Loss:1.6099, Validation Accuracy:0.2135\n",
    "Epoch #221: Loss:1.5866, Accuracy:0.2575, Validation Loss:1.6119, Validation Accuracy:0.2151\n",
    "Epoch #222: Loss:1.5857, Accuracy:0.2538, Validation Loss:1.6118, Validation Accuracy:0.2069\n",
    "Epoch #223: Loss:1.5844, Accuracy:0.2563, Validation Loss:1.6127, Validation Accuracy:0.2118\n",
    "Epoch #224: Loss:1.5826, Accuracy:0.2546, Validation Loss:1.6149, Validation Accuracy:0.2233\n",
    "Epoch #225: Loss:1.5829, Accuracy:0.2669, Validation Loss:1.6147, Validation Accuracy:0.2151\n",
    "Epoch #226: Loss:1.5821, Accuracy:0.2575, Validation Loss:1.6170, Validation Accuracy:0.2167\n",
    "Epoch #227: Loss:1.5837, Accuracy:0.2579, Validation Loss:1.6193, Validation Accuracy:0.2184\n",
    "Epoch #228: Loss:1.5852, Accuracy:0.2517, Validation Loss:1.6186, Validation Accuracy:0.2167\n",
    "Epoch #229: Loss:1.5862, Accuracy:0.2563, Validation Loss:1.6192, Validation Accuracy:0.2299\n",
    "Epoch #230: Loss:1.5853, Accuracy:0.2608, Validation Loss:1.6182, Validation Accuracy:0.2315\n",
    "Epoch #231: Loss:1.5830, Accuracy:0.2600, Validation Loss:1.6130, Validation Accuracy:0.2217\n",
    "Epoch #232: Loss:1.5836, Accuracy:0.2554, Validation Loss:1.6116, Validation Accuracy:0.2250\n",
    "Epoch #233: Loss:1.5825, Accuracy:0.2563, Validation Loss:1.6096, Validation Accuracy:0.2430\n",
    "Epoch #234: Loss:1.5824, Accuracy:0.2682, Validation Loss:1.6090, Validation Accuracy:0.2282\n",
    "Epoch #235: Loss:1.5836, Accuracy:0.2628, Validation Loss:1.6083, Validation Accuracy:0.2200\n",
    "Epoch #236: Loss:1.5843, Accuracy:0.2612, Validation Loss:1.6070, Validation Accuracy:0.2151\n",
    "Epoch #237: Loss:1.5831, Accuracy:0.2600, Validation Loss:1.6089, Validation Accuracy:0.2414\n",
    "Epoch #238: Loss:1.5832, Accuracy:0.2567, Validation Loss:1.6094, Validation Accuracy:0.2266\n",
    "Epoch #239: Loss:1.5838, Accuracy:0.2661, Validation Loss:1.6096, Validation Accuracy:0.2282\n",
    "Epoch #240: Loss:1.5822, Accuracy:0.2710, Validation Loss:1.6114, Validation Accuracy:0.2217\n",
    "Epoch #241: Loss:1.5821, Accuracy:0.2632, Validation Loss:1.6127, Validation Accuracy:0.2348\n",
    "Epoch #242: Loss:1.5839, Accuracy:0.2550, Validation Loss:1.6159, Validation Accuracy:0.2315\n",
    "Epoch #243: Loss:1.5845, Accuracy:0.2493, Validation Loss:1.6122, Validation Accuracy:0.2332\n",
    "Epoch #244: Loss:1.5831, Accuracy:0.2530, Validation Loss:1.6127, Validation Accuracy:0.2381\n",
    "Epoch #245: Loss:1.5806, Accuracy:0.2637, Validation Loss:1.6124, Validation Accuracy:0.2233\n",
    "Epoch #246: Loss:1.5799, Accuracy:0.2620, Validation Loss:1.6120, Validation Accuracy:0.2282\n",
    "Epoch #247: Loss:1.5821, Accuracy:0.2493, Validation Loss:1.6116, Validation Accuracy:0.2184\n",
    "Epoch #248: Loss:1.5830, Accuracy:0.2612, Validation Loss:1.6122, Validation Accuracy:0.2233\n",
    "Epoch #249: Loss:1.5808, Accuracy:0.2620, Validation Loss:1.6114, Validation Accuracy:0.2167\n",
    "Epoch #250: Loss:1.5794, Accuracy:0.2587, Validation Loss:1.6087, Validation Accuracy:0.2266\n",
    "Epoch #251: Loss:1.5804, Accuracy:0.2595, Validation Loss:1.6131, Validation Accuracy:0.2381\n",
    "Epoch #252: Loss:1.5803, Accuracy:0.2710, Validation Loss:1.6128, Validation Accuracy:0.2250\n",
    "Epoch #253: Loss:1.5805, Accuracy:0.2715, Validation Loss:1.6129, Validation Accuracy:0.2233\n",
    "Epoch #254: Loss:1.5834, Accuracy:0.2439, Validation Loss:1.6116, Validation Accuracy:0.2315\n",
    "Epoch #255: Loss:1.5804, Accuracy:0.2534, Validation Loss:1.6148, Validation Accuracy:0.2332\n",
    "Epoch #256: Loss:1.5783, Accuracy:0.2559, Validation Loss:1.6141, Validation Accuracy:0.2529\n",
    "Epoch #257: Loss:1.5784, Accuracy:0.2657, Validation Loss:1.6139, Validation Accuracy:0.2414\n",
    "Epoch #258: Loss:1.5780, Accuracy:0.2608, Validation Loss:1.6145, Validation Accuracy:0.2315\n",
    "Epoch #259: Loss:1.5773, Accuracy:0.2653, Validation Loss:1.6192, Validation Accuracy:0.2299\n",
    "Epoch #260: Loss:1.5762, Accuracy:0.2591, Validation Loss:1.6195, Validation Accuracy:0.2266\n",
    "Epoch #261: Loss:1.5769, Accuracy:0.2624, Validation Loss:1.6213, Validation Accuracy:0.2282\n",
    "Epoch #262: Loss:1.5760, Accuracy:0.2600, Validation Loss:1.6191, Validation Accuracy:0.2430\n",
    "Epoch #263: Loss:1.5786, Accuracy:0.2550, Validation Loss:1.6157, Validation Accuracy:0.2184\n",
    "Epoch #264: Loss:1.5801, Accuracy:0.2624, Validation Loss:1.6198, Validation Accuracy:0.2381\n",
    "Epoch #265: Loss:1.5824, Accuracy:0.2579, Validation Loss:1.6148, Validation Accuracy:0.2118\n",
    "Epoch #266: Loss:1.5846, Accuracy:0.2538, Validation Loss:1.6158, Validation Accuracy:0.2167\n",
    "Epoch #267: Loss:1.5832, Accuracy:0.2538, Validation Loss:1.6140, Validation Accuracy:0.2184\n",
    "Epoch #268: Loss:1.5823, Accuracy:0.2604, Validation Loss:1.6138, Validation Accuracy:0.2233\n",
    "Epoch #269: Loss:1.5849, Accuracy:0.2522, Validation Loss:1.6118, Validation Accuracy:0.2184\n",
    "Epoch #270: Loss:1.5843, Accuracy:0.2608, Validation Loss:1.6118, Validation Accuracy:0.2414\n",
    "Epoch #271: Loss:1.5802, Accuracy:0.2641, Validation Loss:1.6118, Validation Accuracy:0.2332\n",
    "Epoch #272: Loss:1.5807, Accuracy:0.2505, Validation Loss:1.6138, Validation Accuracy:0.2233\n",
    "Epoch #273: Loss:1.5805, Accuracy:0.2637, Validation Loss:1.6125, Validation Accuracy:0.2397\n",
    "Epoch #274: Loss:1.5797, Accuracy:0.2637, Validation Loss:1.6122, Validation Accuracy:0.2348\n",
    "Epoch #275: Loss:1.5843, Accuracy:0.2526, Validation Loss:1.6087, Validation Accuracy:0.2479\n",
    "Epoch #276: Loss:1.5840, Accuracy:0.2563, Validation Loss:1.6108, Validation Accuracy:0.2332\n",
    "Epoch #277: Loss:1.5873, Accuracy:0.2530, Validation Loss:1.6094, Validation Accuracy:0.2282\n",
    "Epoch #278: Loss:1.5802, Accuracy:0.2649, Validation Loss:1.6143, Validation Accuracy:0.2381\n",
    "Epoch #279: Loss:1.5811, Accuracy:0.2641, Validation Loss:1.6143, Validation Accuracy:0.2430\n",
    "Epoch #280: Loss:1.5845, Accuracy:0.2480, Validation Loss:1.6203, Validation Accuracy:0.2118\n",
    "Epoch #281: Loss:1.5847, Accuracy:0.2489, Validation Loss:1.6196, Validation Accuracy:0.2053\n",
    "Epoch #282: Loss:1.6004, Accuracy:0.2230, Validation Loss:1.6246, Validation Accuracy:0.2315\n",
    "Epoch #283: Loss:1.5916, Accuracy:0.2489, Validation Loss:1.6138, Validation Accuracy:0.2118\n",
    "Epoch #284: Loss:1.5842, Accuracy:0.2604, Validation Loss:1.6123, Validation Accuracy:0.1970\n",
    "Epoch #285: Loss:1.5827, Accuracy:0.2439, Validation Loss:1.6156, Validation Accuracy:0.2151\n",
    "Epoch #286: Loss:1.5789, Accuracy:0.2612, Validation Loss:1.6186, Validation Accuracy:0.2053\n",
    "Epoch #287: Loss:1.5800, Accuracy:0.2608, Validation Loss:1.6224, Validation Accuracy:0.2184\n",
    "Epoch #288: Loss:1.5791, Accuracy:0.2620, Validation Loss:1.6187, Validation Accuracy:0.2036\n",
    "Epoch #289: Loss:1.5785, Accuracy:0.2583, Validation Loss:1.6116, Validation Accuracy:0.2118\n",
    "Epoch #290: Loss:1.5762, Accuracy:0.2649, Validation Loss:1.6180, Validation Accuracy:0.2217\n",
    "Epoch #291: Loss:1.5762, Accuracy:0.2669, Validation Loss:1.6182, Validation Accuracy:0.2184\n",
    "Epoch #292: Loss:1.5752, Accuracy:0.2653, Validation Loss:1.6177, Validation Accuracy:0.2085\n",
    "Epoch #293: Loss:1.5746, Accuracy:0.2604, Validation Loss:1.6176, Validation Accuracy:0.2167\n",
    "Epoch #294: Loss:1.5736, Accuracy:0.2669, Validation Loss:1.6179, Validation Accuracy:0.2200\n",
    "Epoch #295: Loss:1.5744, Accuracy:0.2628, Validation Loss:1.6166, Validation Accuracy:0.2167\n",
    "Epoch #296: Loss:1.5751, Accuracy:0.2637, Validation Loss:1.6194, Validation Accuracy:0.2365\n",
    "Epoch #297: Loss:1.5747, Accuracy:0.2657, Validation Loss:1.6176, Validation Accuracy:0.2085\n",
    "Epoch #298: Loss:1.5765, Accuracy:0.2550, Validation Loss:1.6190, Validation Accuracy:0.2167\n",
    "Epoch #299: Loss:1.5744, Accuracy:0.2739, Validation Loss:1.6188, Validation Accuracy:0.2332\n",
    "Epoch #300: Loss:1.5726, Accuracy:0.2645, Validation Loss:1.6193, Validation Accuracy:0.2135\n",
    "\n",
    "Test:\n",
    "Test Loss:1.61934328, Accuracy:0.2135\n",
    "Labels: ['02', '04', '01', '05', '03']\n",
    "Confusion Matrix:\n",
    "      02  04  01  05  03\n",
    "t:02   4  13  20  77   0\n",
    "t:04   4  12  27  68   1\n",
    "t:01   5  15  23  83   0\n",
    "t:05   6  12  35  89   0\n",
    "t:03   8  13  29  63   2\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.15      0.04      0.06       114\n",
    "          04       0.18      0.11      0.14       112\n",
    "          01       0.17      0.18      0.18       126\n",
    "          05       0.23      0.63      0.34       142\n",
    "          03       0.67      0.02      0.03       115\n",
    "\n",
    "    accuracy                           0.21       609\n",
    "   macro avg       0.28      0.19      0.15       609\n",
    "weighted avg       0.28      0.21      0.16       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.26 12:17:28 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 58 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.609733477406118, 1.607138566978655, 1.6064317005216977, 1.6064622188828066, 1.6062006124330468, 1.6052617652858616, 1.6048670206555395, 1.6045324328693458, 1.6044000584895193, 1.6058201560833183, 1.6060787429559016, 1.6058325141130019, 1.6060174678151047, 1.6068418384185565, 1.60641282082387, 1.6058390261895943, 1.6072587399255662, 1.60702361085732, 1.6069558441932565, 1.6065880479092276, 1.608500940459115, 1.6080083917514445, 1.60812334591532, 1.6076174011371407, 1.6077057618421482, 1.6092471113345894, 1.6088831857311705, 1.6086417099916681, 1.608784585163511, 1.6091187019849253, 1.609022260690949, 1.6087272840571913, 1.6086154496924239, 1.608857576287243, 1.6089602354516341, 1.6094843364505738, 1.6089037780104012, 1.609386755523619, 1.6090681364970842, 1.6093181224879374, 1.6090273054558264, 1.608893535015814, 1.6092734446470764, 1.6095715271819793, 1.6093313981746804, 1.6096787791338265, 1.6098201388404483, 1.610635372609732, 1.611265052324054, 1.6176539592946495, 1.6121921568668534, 1.61032372155213, 1.6091304081805626, 1.6092834676232048, 1.6094332774871676, 1.6097423871749728, 1.6098437158540748, 1.6098458473318316, 1.6099560059154367, 1.6103423835804505, 1.6101248360228264, 1.610654675118833, 1.610618486780251, 1.610633154220769, 1.6103672799218465, 1.6104843638017652, 1.6109016714816415, 1.6115082283129638, 1.6112207366132187, 1.6115558572199153, 1.6112952494660426, 1.6116360144074915, 1.6108507597191972, 1.6103196905555788, 1.6101106869176103, 1.613863268313541, 1.612337662277159, 1.6117122089138562, 1.6104949826285953, 1.6102163644847025, 1.610453967977627, 1.6097794160467063, 1.6095387882787018, 1.610076050452998, 1.6097988976633608, 1.6103845795582863, 1.6112114582547217, 1.611305019538391, 1.6111394284393987, 1.6109916251672705, 1.6122744363321264, 1.6135275166218699, 1.614122972504063, 1.612346609433492, 1.6119158960915552, 1.6120721688998745, 1.6116243792676377, 1.6118731077864448, 1.612608386182237, 1.6154149050391562, 1.6115707070956677, 1.6115112365368747, 1.6111837130266262, 1.611060434765808, 1.6117644080974785, 1.6135332077399067, 1.613171301256064, 1.6127761725721688, 1.612070214767957, 1.6125896468342622, 1.6128148372928888, 1.612158569992078, 1.6127884767717133, 1.612015981979558, 1.6124983702974367, 1.6119907653977719, 1.6132550650629505, 1.6132563648161238, 1.6136866512361223, 1.614817284793885, 1.615282455297135, 1.6146505292021778, 1.6140656160016364, 1.6128711984271096, 1.6147601189480234, 1.61455526532015, 1.6137253040163388, 1.6137396177439072, 1.613770682823482, 1.6139671025409292, 1.614475351640548, 1.6136289110716144, 1.6125972169177678, 1.6126939044601616, 1.612221179728829, 1.6107513401504416, 1.6097972148353439, 1.6106665864562362, 1.6111206092270725, 1.610547145403469, 1.6107743264027612, 1.609973571962128, 1.610554605086253, 1.6099574442567497, 1.6103100148327831, 1.6105007144617918, 1.6111212675207354, 1.6106332528767326, 1.6114527075161487, 1.6115819575947101, 1.6111008486724252, 1.6120359468929872, 1.612280401298761, 1.611577375769028, 1.6127790860550353, 1.6118788985391752, 1.6122419714731928, 1.612570355091189, 1.6112016533395928, 1.6106546821656877, 1.6103363608687578, 1.6105333112535023, 1.6133265473768237, 1.6137853643577087, 1.6134252886858285, 1.6140407772095529, 1.614667940413815, 1.613499962833323, 1.6143455885118256, 1.6124355021760186, 1.614007104402301, 1.6126137971878052, 1.6123907205897992, 1.6147041808208222, 1.6123139098751524, 1.6115156398422417, 1.6120350309976412, 1.6121764216321246, 1.6121633910193232, 1.61294770612701, 1.614241248868369, 1.6135772120189198, 1.6136573161593408, 1.614539179895899, 1.6145837843320248, 1.6145501062396321, 1.6160046003134967, 1.6150971729375654, 1.6170445396786643, 1.6162044512618743, 1.6154623554257923, 1.6160132213570606, 1.61416420619476, 1.6154476778064846, 1.6141673199257436, 1.6162638353009529, 1.6134259526561243, 1.6119067723723663, 1.6142646080167422, 1.6135361186780757, 1.6139902156366308, 1.6198773020006754, 1.6178486304134374, 1.61718088848446, 1.6168665054004963, 1.6161921981520253, 1.6163225759230615, 1.617411995756215, 1.6167785690727297, 1.6180305576872551, 1.6180601000590082, 1.6171236165442882, 1.6171432332256548, 1.6184806365684923, 1.6199826708763887, 1.6156654602592606, 1.6135406701631343, 1.6112378745634959, 1.6088784839132149, 1.6099215804649691, 1.6118903663162332, 1.6118296153831169, 1.612696901330807, 1.6149239299332567, 1.6146700718915716, 1.6170489468989506, 1.6193157655656436, 1.618645571135535, 1.6192060757936124, 1.6182396556747762, 1.613011258184812, 1.6116109076391887, 1.6095823165035403, 1.609040076900977, 1.6083088598423598, 1.6070469206777112, 1.6089285798064985, 1.6094340087940735, 1.6096485802301241, 1.6114144683471454, 1.6127492261833354, 1.6159353784739678, 1.6122492919805993, 1.612669552684026, 1.6123615959399245, 1.6120269220255083, 1.611604128956599, 1.6122316715165312, 1.6113819003300909, 1.608662271734529, 1.6130871530041122, 1.6127985336118926, 1.6129388355073475, 1.6116165390547077, 1.6147806648354617, 1.614124143064903, 1.6139422833234414, 1.6144729948591912, 1.6191545468441566, 1.6195002670945793, 1.621285207753111, 1.6191183563523692, 1.6156842430628384, 1.619777394241496, 1.6148166533174186, 1.6157560820258505, 1.6139548975845863, 1.6137648151425892, 1.6118363461079463, 1.611809848368853, 1.611832984758324, 1.6137675035176018, 1.6125261104361372, 1.6121705864450615, 1.6086681924625765, 1.6108341990237558, 1.609445254790959, 1.614284068492833, 1.6142687562651235, 1.6202903439845946, 1.619617760670792, 1.6246103095303615, 1.613750914830488, 1.612323722032882, 1.6156402781287633, 1.618579431316144, 1.6224420013881864, 1.6186631420758752, 1.611558413662151, 1.618047706795052, 1.6181702083359015, 1.6177114320701762, 1.6176172240418558, 1.6179017327689185, 1.6165962403239484, 1.6194110813203508, 1.6175767083473394, 1.6190167667439026, 1.6187840846959005, 1.61934338962699], 'val_acc': [0.21510673203002448, 0.23645319976830131, 0.23481116364350654, 0.23481116364350654, 0.2298850553669953, 0.23481116374137953, 0.23316912742083884, 0.2331691275187118, 0.2331691275187118, 0.23481116364350654, 0.23645319976830131, 0.23645319976830131, 0.2331691275187118, 0.2331691275187118, 0.23809523589309606, 0.23152709139391706, 0.23809523589309606, 0.24466338049014802, 0.2397372720178908, 0.23316912742083884, 0.23316912771445777, 0.2298850553669953, 0.24137930814268554, 0.2397372720178908, 0.24137930814268554, 0.23809523589309606, 0.2397372720178908, 0.2397372720178908, 0.23809523589309606, 0.24137930814268554, 0.23809523589309606, 0.23809523589309606, 0.23809523589309606, 0.24137930814268554, 0.23809523589309606, 0.24137930814268554, 0.23809523589309606, 0.23809523589309606, 0.23809523589309606, 0.23809523589309606, 0.2397372720178908, 0.24137930814268554, 0.23809523589309606, 0.24137930804481256, 0.24466338039227503, 0.2430213442674803, 0.2413793082405585, 0.23645319986617427, 0.2315270934737179, 0.20361247905858829, 0.20197044293379354, 0.23645319976830131, 0.23152709139391706, 0.23481116364350654, 0.2331691275187118, 0.2397372719200178, 0.2413793082405585, 0.24466338049014802, 0.24466338039227503, 0.23809523599096902, 0.2397372719200178, 0.24630541651706978, 0.24466338049014802, 0.24466338039227503, 0.24137930814268554, 0.2430213441696073, 0.24137930814268554, 0.2397372719200178, 0.2430213442674803, 0.2430213442674803, 0.23809523579522307, 0.24137930814268554, 0.22988505725105016, 0.2282430211262554, 0.23316912959851263, 0.2151067319321515, 0.2397372719200178, 0.2282430211262554, 0.22660098500146067, 0.24630541651706978, 0.2397372719200178, 0.23481116572330737, 0.2430213442674803, 0.23481116572330737, 0.23316912959851263, 0.23152709327797194, 0.2315270934737179, 0.2315270934737179, 0.23316912959851263, 0.24466338049014802, 0.2364532019459751, 0.2282430211262554, 0.2315270934737179, 0.2364532019459751, 0.22331691294761713, 0.2249589489745389, 0.2167487677633273, 0.2167487678612003, 0.22167487652920345, 0.20361247876496932, 0.23316912959851263, 0.2282430213220014, 0.22331691275187118, 0.23481116582118036, 0.2315270934737179, 0.23481116572330737, 0.2233169125561252, 0.20689655130817777, 0.23809523807076985, 0.24137930804481256, 0.22167487643133046, 0.2200328404044087, 0.23316912959851263, 0.2233169126539982, 0.21674876795907325, 0.23809523807076985, 0.23809523807076985, 0.21018062336202128, 0.21018062326414833, 0.20853858733509953, 0.21182265948681606, 0.22988505725105016, 0.2348111656254344, 0.24137931032035934, 0.2167487677633273, 0.2397372719200178, 0.24137931032035934, 0.2167487677633273, 0.2364532019459751, 0.22988505725105016, 0.2200328399150438, 0.23481116582118036, 0.22988505734892314, 0.2167487677633273, 0.23481116582118036, 0.23809523797289686, 0.23316912940276668, 0.23316912940276668, 0.2380952378750239, 0.23316912959851263, 0.21674876756758135, 0.23973727399981865, 0.22988505744679613, 0.2397372740976916, 0.23645320184810212, 0.23809523807076985, 0.2167487677633273, 0.2430213441696073, 0.24137930804481256, 0.22660098509933366, 0.23316912969638562, 0.2233169125561252, 0.2364532019459751, 0.23481116572330737, 0.21510673163853256, 0.22988505725105016, 0.21510673163853256, 0.2397372741955646, 0.22167487643133046, 0.22660098500146067, 0.2200328405022817, 0.2413793082405585, 0.21510673222577043, 0.21018062336202128, 0.21839080398599503, 0.21839080398599503, 0.224958948583047, 0.21510673173640554, 0.224958948485174, 0.22003284030653572, 0.2134646956116108, 0.20689655130817777, 0.21018062365564025, 0.21346469541586482, 0.21510673212789747, 0.21839080427961396, 0.22167487643133046, 0.21674876805694623, 0.23809523589309606, 0.211822659682562, 0.22824302102838243, 0.2151067319321515, 0.23316912940276668, 0.21510673203002448, 0.21346469570948376, 0.22660098490358768, 0.21674876805694623, 0.2151067319321515, 0.2151067319321515, 0.21510673203002448, 0.22495894877879294, 0.22003284020866276, 0.2233169125561252, 0.2233169125561252, 0.21018062345989427, 0.21346469580735675, 0.2151067323236434, 0.2183908044753599, 0.2200328405022817, 0.2200328404044087, 0.21674876835056517, 0.2052545148897641, 0.20525451518338303, 0.211822659780435, 0.2068965511124318, 0.2003284067111258, 0.2036124788628423, 0.21182265958468902, 0.21346469590522973, 0.20197044264017458, 0.2151067319321515, 0.21018062345989427, 0.22660098490358768, 0.22167487643133046, 0.2085385869436076, 0.22167487643133046, 0.21182265958468902, 0.21018062345989427, 0.22003284030653572, 0.2134646956116108, 0.2151067319321515, 0.2068965511124318, 0.211822659780435, 0.22331691284974417, 0.21510673203002448, 0.21674876815481922, 0.21839080398599503, 0.2167487676654543, 0.22988505676168527, 0.23152709308222597, 0.22167487643133046, 0.22495894868091998, 0.24302134644515408, 0.22824302093050947, 0.22003284011078977, 0.21510673163853256, 0.2413793101246134, 0.22660098460996875, 0.22824302093050947, 0.22167487623558452, 0.23481116354563358, 0.23152709327797194, 0.2331691275187118, 0.2380952378750239, 0.22331691275187118, 0.2282430213220014, 0.21839080427961396, 0.22331691284974417, 0.21674876815481922, 0.22660098509933366, 0.2380952378750239, 0.22495894887666593, 0.2233169126539982, 0.2315270934737179, 0.23316912940276668, 0.2528735612119947, 0.24137931022248635, 0.2315270934737179, 0.2298850571531772, 0.22660098500146067, 0.2282430211262554, 0.2430213442674803, 0.21839080437748695, 0.23809523807076985, 0.211822659682562, 0.21674876815481922, 0.21839080437748695, 0.22331691275187118, 0.21839080418174098, 0.24137931032035934, 0.23316912969638562, 0.22331691275187118, 0.2397372741955646, 0.23481116364350654, 0.24794745462379236, 0.23316912910914772, 0.22824302093050947, 0.23809523767927793, 0.2430213442674803, 0.21182265997618094, 0.2052545148897641, 0.2315270912960441, 0.21182265958468902, 0.19704433446153632, 0.2151067319321515, 0.20525451537912898, 0.2183908045732329, 0.20361247905858829, 0.211822659780435, 0.2216748768228224, 0.2183908045732329, 0.2085385875308455, 0.21674876835056517, 0.2200328405022817, 0.21674876815481922, 0.2364532019459751, 0.20853858733509953, 0.21674876835056517, 0.23316912969638562, 0.21346469619884867], 'loss': [1.612207691282707, 1.60643313415731, 1.6060151012036834, 1.604858498504764, 1.6044959131941903, 1.6042017247887363, 1.6040693999315925, 1.6046639641207592, 1.6049592839618974, 1.604731470946169, 1.6049937513085117, 1.6044447066602765, 1.6033722293939923, 1.6036046620022344, 1.6025321371011911, 1.6022487998253512, 1.6023252070562062, 1.602552651037181, 1.6019972365005306, 1.600785534386762, 1.602066739873475, 1.6023176795403327, 1.6016291946845866, 1.6014500077500236, 1.6012353857929458, 1.601554781994046, 1.6006929495496183, 1.6002671335756902, 1.6001188882322528, 1.5998192434682983, 1.599661064294819, 1.5994315372600203, 1.5989346486097489, 1.5996288330892763, 1.598698536718161, 1.5990534405443948, 1.5987771720122508, 1.5983125709655104, 1.5983239805429132, 1.5981230242296411, 1.5979440013975579, 1.5975290489392604, 1.59746060743469, 1.5969366707106636, 1.5970157267376628, 1.5972864045254749, 1.5974381193732823, 1.5975694414526531, 1.5972224133704966, 1.6035403624207576, 1.6045285664544702, 1.6011085823576063, 1.601370290417446, 1.6015518476830861, 1.6010859912915396, 1.6007889934144226, 1.5996548013765464, 1.5991397187450338, 1.5987344004535087, 1.5983904690223554, 1.598423103826002, 1.5985495753356809, 1.5978470510036304, 1.5981181126110853, 1.5982662952900912, 1.5975169159793265, 1.5978040885141989, 1.5979152357798583, 1.5975221472354395, 1.5973810354052629, 1.5973188708695054, 1.5974498786475868, 1.597603336888417, 1.5978127196584149, 1.5977558368530116, 1.596609759869272, 1.6008830194845336, 1.5993578084440447, 1.5987208598448266, 1.5979526250514162, 1.5977318197794763, 1.597636670596301, 1.5970791057884326, 1.5967566157758113, 1.5970955339545343, 1.5965822761308486, 1.5965207140058952, 1.5963972051040838, 1.596387641630623, 1.5965186823075312, 1.5962092064734112, 1.595771829058747, 1.595994493848734, 1.59611116953699, 1.5960372477341482, 1.5955961220073505, 1.5954894694949078, 1.596400848404338, 1.6009196798414664, 1.5977687936054363, 1.5989991725103077, 1.595593612541653, 1.596304310322787, 1.5957404636014902, 1.5954552835017994, 1.5959348587529616, 1.595163789715855, 1.5958319083376342, 1.5954747489345638, 1.5966407618238696, 1.5954407301282, 1.5957243514991148, 1.5953004424332105, 1.595334663724018, 1.594556261626602, 1.5941958906714186, 1.5941518114822357, 1.5941014545409342, 1.5939437759485577, 1.5939584241021096, 1.593945459271848, 1.593752050203954, 1.5942546428351432, 1.594444214049306, 1.594874567319725, 1.5941712546887095, 1.5950184299226169, 1.594747800846609, 1.593620088164077, 1.593473570351728, 1.5941894691338039, 1.5940992180082099, 1.5936078701175949, 1.5924005074177925, 1.5932439755610128, 1.59475733935221, 1.5952073155487343, 1.5954740830760228, 1.5953992363853375, 1.5959862975369243, 1.5951562333645517, 1.595611903163197, 1.5947570181480424, 1.5957402222944725, 1.5945500222564477, 1.5945434485128038, 1.5942946963241702, 1.596050534551883, 1.5938239376158196, 1.594569151700155, 1.5933733598407533, 1.5932939326738675, 1.5924683168438671, 1.59282634835958, 1.5921689168138915, 1.5928315180772628, 1.5928978074992217, 1.5925008948578727, 1.5926557876735743, 1.5920424895609673, 1.5922772654272936, 1.5924370834225257, 1.5902483100029476, 1.5897061294598747, 1.5905489510824058, 1.5896990973357057, 1.5903288702210852, 1.5920989675933086, 1.591509022017524, 1.5904045720364768, 1.5920266118627309, 1.593200362046886, 1.5931992711962126, 1.5893553947765968, 1.591110339448682, 1.5897402231698163, 1.588767524323669, 1.589094626830099, 1.5894416484499858, 1.5903790385816132, 1.590051417967622, 1.5874450586659707, 1.587102259306937, 1.5886430228760111, 1.5867853826076344, 1.5873067856324528, 1.5868102315514974, 1.5864922626307367, 1.5876246761247608, 1.5866510034831398, 1.5866891931948965, 1.5869807643322484, 1.5894214975270893, 1.5879726726661227, 1.5890240229620336, 1.5873826758327916, 1.587591158633849, 1.5877891253641743, 1.5870762190045273, 1.5883511078675914, 1.5896777164764717, 1.5872080098432193, 1.5868068590301263, 1.5882320674293096, 1.5865175410707384, 1.5868518999714627, 1.586884484888347, 1.5871246860257409, 1.5855942983157336, 1.5880063802065056, 1.5862592512087657, 1.5863173023631196, 1.5866095950716086, 1.587429886334241, 1.5896082622559409, 1.5872148834459592, 1.5869317534523089, 1.5871130008227528, 1.5890424755319679, 1.5877369384256477, 1.5866022885457691, 1.585744964709272, 1.5844386860085709, 1.5826430034343713, 1.5828835368890781, 1.582134847327669, 1.5836521759659847, 1.5851868971172545, 1.5861919907818585, 1.5852815022458775, 1.5830422939461115, 1.5835836456541652, 1.5825016006551973, 1.5824437801842817, 1.5835802802314993, 1.584256610586413, 1.583124552566168, 1.5831707865795315, 1.583830201552389, 1.5822121223629868, 1.5821310542692149, 1.583892535233155, 1.5844567727259298, 1.5831076675861524, 1.580565122216634, 1.5799120671940046, 1.582090732106438, 1.5829749971934168, 1.5808370589720395, 1.5794251330824114, 1.580359253550457, 1.5802538434093247, 1.5805432520852687, 1.5834382448353073, 1.5804346413582993, 1.5783395696714428, 1.5783699871333472, 1.5780229811795683, 1.5773487180655008, 1.5762047200232316, 1.5768702610807008, 1.5759812029969766, 1.5785669420289308, 1.58007696438619, 1.582435317597595, 1.5846126029623608, 1.5832489281464406, 1.5823005384488271, 1.5848661698844644, 1.584257061966146, 1.580219873363722, 1.5807189222723552, 1.5805224188544178, 1.579733664837706, 1.584272073622357, 1.583993505207665, 1.5872750534903588, 1.5802498414531136, 1.5810868495788417, 1.5845345657708954, 1.5847411642329159, 1.6004312584777143, 1.5915569471872317, 1.5841675540505005, 1.5827499429303273, 1.5789057144149372, 1.5799967913656998, 1.5791480862874026, 1.5785399797271165, 1.576158516607735, 1.5762321527488912, 1.575227815414601, 1.5746299376967507, 1.5736291353217875, 1.574375453817771, 1.5750552947026748, 1.5747357994623987, 1.576530523613493, 1.5743515573732663, 1.572554004755353], 'acc': [0.18932238084090075, 0.2238193023070173, 0.23326488755934047, 0.23162217605285332, 0.2316221766770498, 0.22997946716554357, 0.23121149784730446, 0.23285420931707418, 0.23285421029620593, 0.23285421047367355, 0.2312114984531422, 0.2312114992364476, 0.23367556512233412, 0.23285420929871545, 0.23326488713097035, 0.23408624197545727, 0.23162217648122346, 0.23696098724185075, 0.23655030688221204, 0.24106776185593812, 0.23737166331166848, 0.2320328548458812, 0.2443531837979871, 0.23983572880590232, 0.24147843849487619, 0.23367556496322522, 0.23983572962592514, 0.2443531826046703, 0.2435318271727043, 0.24065708464787972, 0.24024640818640927, 0.23983572782677057, 0.24106776005678354, 0.24188911591711965, 0.2427104725240437, 0.23942505255861693, 0.24147843869070254, 0.24312115053376623, 0.24229979492433262, 0.24312115055212494, 0.24229979570763802, 0.2410677626392435, 0.24353182856184746, 0.24435318299632297, 0.24271047291569642, 0.24147843988401935, 0.24188911632713106, 0.2414784385132349, 0.23942505101036488, 0.22956878897835342, 0.21314168402301703, 0.23039014319864387, 0.24435318417128107, 0.23162217646286473, 0.23203285447258723, 0.2344969199851798, 0.24394250559243824, 0.2459958920978178, 0.24435318321050806, 0.24681724931057963, 0.2443531826046703, 0.24558521569142353, 0.24147843927818158, 0.24024640642397213, 0.24517453881994164, 0.2422997941226685, 0.24229979472850627, 0.24106776226594953, 0.24271047293405512, 0.24517453921159435, 0.24271047193656467, 0.2414784414322714, 0.24024640642397213, 0.2394250519711379, 0.23737166270583074, 0.2443531818213649, 0.23613963259319015, 0.2361396306532854, 0.23819301774614401, 0.2394250507961798, 0.23613963141823208, 0.2410677630308962, 0.2377823401280742, 0.23860369655753064, 0.23983572882426105, 0.23983572980339277, 0.23655030805717014, 0.23819302068353923, 0.23942505157948518, 0.24106776322672255, 0.2447638606143928, 0.23942505353774868, 0.23901437456725314, 0.24024640661979849, 0.23613963004744762, 0.2390143731964687, 0.24106776244341716, 0.23696098530194598, 0.23490759779907594, 0.24271047174073831, 0.23696098588942502, 0.2410677634225489, 0.23696098489193457, 0.23531827422382895, 0.23819301911692844, 0.23655030807552885, 0.24271047215074973, 0.24024640765400637, 0.23983572939338135, 0.24312114955463449, 0.2422997949059739, 0.2459958932911346, 0.24106776207012318, 0.24312114916298178, 0.2468172485089155, 0.23860369556004016, 0.2422997945143212, 0.24353182658522526, 0.2505133476590229, 0.2492813153922925, 0.24804928097147227, 0.2529774127799628, 0.246817250467179, 0.23942505101036488, 0.24106776246177586, 0.2418891168962514, 0.24435318338797568, 0.24845995880372715, 0.24229979373101582, 0.246406570499193, 0.24312115055212494, 0.24887063601178555, 0.24640656973424632, 0.24887063640343823, 0.24558521665219654, 0.2439425053966119, 0.246817250467179, 0.24599589405608127, 0.23942505216696425, 0.24435318456293376, 0.24353182952262048, 0.2455852156730648, 0.2464065701075403, 0.2427104733073491, 0.2431211491813405, 0.2435318283476624, 0.24887063757839634, 0.24353182756435698, 0.2455852174171432, 0.24229979551181166, 0.24312114896715545, 0.2484599578062367, 0.24681724909639458, 0.24722792675118183, 0.24845995819788938, 0.24681724772561012, 0.24312114955463449, 0.24640657090920443, 0.24435318242720266, 0.24763860317593483, 0.2517453787507952, 0.24517453903412673, 0.2501026696493004, 0.2587268976337856, 0.25379876840775506, 0.24887063759675507, 0.25462012383972105, 0.24435318164389727, 0.24887063601178555, 0.25297741434657356, 0.24517453842828896, 0.2447638610244042, 0.2542094448508668, 0.2554414804466451, 0.251745380317406, 0.2550308008703118, 0.2579055446068119, 0.25215605715217043, 0.24804928195060402, 0.2439425053966119, 0.2468172487231006, 0.25297741238831006, 0.26488706546642454, 0.2501026696676591, 0.24681724772561012, 0.25749486794951515, 0.2574948677536888, 0.2558521566939305, 0.25174538168819044, 0.25338809000637985, 0.2558521539156442, 0.2492813144315195, 0.24804928098983098, 0.2583162225981757, 0.2505133470899026, 0.25133470269933617, 0.2525667341827612, 0.2472279237403517, 0.25215605754382314, 0.24969199242288326, 0.24188911728790408, 0.251745379944112, 0.2574948667745571, 0.24599589307694955, 0.2595482558440379, 0.25872690021624556, 0.2537987676244497, 0.2505133484423283, 0.26283367483033293, 0.2496919916579366, 0.25379876782027605, 0.25544148064247146, 0.2558521563022778, 0.24887063562013284, 0.2542094466133039, 0.2603696075185858, 0.2542094456341722, 0.25790554341349514, 0.25051334746319653, 0.24517453999489974, 0.2574948665787307, 0.2537987676244497, 0.25626283133788763, 0.254620123252242, 0.26694045375259995, 0.25749486755786244, 0.25790554302184243, 0.25174538053159107, 0.25626283352869494, 0.2607802886982473, 0.25995893029216866, 0.2554414798591661, 0.2562628327453895, 0.26817248564603635, 0.2628336760236497, 0.26119096496389144, 0.2599589322871496, 0.2566735113425911, 0.26611909694984953, 0.27104722637170636, 0.26324435421083986, 0.25503080028283276, 0.2492813153922925, 0.2529774141507472, 0.26365503261221507, 0.2620123207691514, 0.24928131402150805, 0.261190965300468, 0.2620123201816723, 0.2587269004120719, 0.2595482558440379, 0.27104722895416633, 0.27145790715971524, 0.24394250420329508, 0.25338809078968527, 0.25585215806471495, 0.2657084193317797, 0.2607802883065946, 0.26529774230118897, 0.25913757783431535, 0.2624229985830475, 0.25995893209132326, 0.25503079867950457, 0.26242299678389297, 0.2579055440193329, 0.25379876782027605, 0.2537987691910605, 0.26036960951356675, 0.25215605717052914, 0.2607802886982473, 0.2640657078803687, 0.25051334824650195, 0.2636550328080414, 0.26365503261221507, 0.2525667341827612, 0.2562628333328686, 0.2529774137774532, 0.2648870646831191, 0.26406571042611127, 0.24804928038399324, 0.248870637009276, 0.22299794628757227, 0.24887063797004905, 0.26036961127600383, 0.2439425046133065, 0.2611909627547254, 0.26078028948155274, 0.262012319985846, 0.25831622142321764, 0.26488706350816105, 0.2669404499951819, 0.26529774231954767, 0.26036960912191404, 0.26694045218598916, 0.26283367659277007, 0.26365503124143064, 0.2657084181201042, 0.2550308016536172, 0.27392196911071603, 0.26447638369928395]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
