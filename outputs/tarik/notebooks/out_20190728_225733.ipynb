{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======= Running File: classifierLSTMnSVM.py =======\n",
    "Reading Configuration from command line argument: C:\\Users\\ATIL\\PycharmProjects\\Thesis02wDL\\confFiles\\conf72.txt\n",
    "Total of 1 configuration(s) will be run\n",
    "============ Config: 1/1 === Start Time: 2019.07.28 22:57:33 =======================================\n",
    "Parameters: {'inputFolder': 'C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/', 'featureMode': 'Phases', 'channelMode': 'AllShfUni', 'classificationMode': 'Posture3', 'trainingEpoch': 300, 'stepSize': 1, 'sampRate': 8, 'batchSize': 512, 'learningRate': 0.001, 'lossFunction': 'CatCrosEnt', 'optimizer': 'Adam', 'clsModel': 'LSTM'}\n",
    "Initial Scan.\n",
    "Shuffling...\n",
    "Reading:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "Generating Labels...\n",
    "3044 Files with 3 Label(s): ['02', '01', '03'].\n",
    "Padding:....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
    "\n",
    "Total of 3044 inputs loaded @ C:/Users/ATIL/Desktop/Dataset/inputsFrom_max_sample_set/\n",
    "Total of 3 classes\n",
    "2435 steps for training, 609 steps for test\n",
    "Splitting Train and Test Data...\n",
    "------Model for Phases------\n",
    "---LSTM Classifier---\n",
    "Train Batch: (2435, 7991, 28)\n",
    "Test Batch: (609, 7991, 28)\n",
    "Optimizer: <keras.optimizers.Adam object at 0x000001B580788550>\n",
    "Learning Rate: 0.001\n",
    "Loss func: <function categorical_crossentropy at 0x000001B5C7D86EA0>\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv1d_1 (Conv1D)            (None, 166, 8)            10760     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 166, 8)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 6, 16)             3088      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 6, 16)             0         \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 6, 24)             3936      \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 12)                1776      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 39        \n",
    "=================================================================\n",
    "Total params: 19,599\n",
    "Trainable params: 19,599\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Training:\n",
    "Epoch #1: Loss:1.0866, Accuracy:0.3926, Validation Loss:1.0788, Validation Accuracy:0.3941\n",
    "Epoch #2: Loss:1.0775, Accuracy:0.3951, Validation Loss:1.0748, Validation Accuracy:0.3941\n",
    "Epoch #3: Loss:1.0747, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #4: Loss:1.0751, Accuracy:0.3943, Validation Loss:1.0752, Validation Accuracy:0.3941\n",
    "Epoch #5: Loss:1.0750, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3892\n",
    "Epoch #6: Loss:1.0747, Accuracy:0.3992, Validation Loss:1.0745, Validation Accuracy:0.3859\n",
    "Epoch #7: Loss:1.0746, Accuracy:0.3901, Validation Loss:1.0744, Validation Accuracy:0.3924\n",
    "Epoch #8: Loss:1.0745, Accuracy:0.3951, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #9: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #10: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #11: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #12: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #13: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #14: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #15: Loss:1.0744, Accuracy:0.3951, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #16: Loss:1.0742, Accuracy:0.3934, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #17: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #18: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #19: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #20: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #21: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #22: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #23: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #24: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #25: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #26: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #27: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #28: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #29: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #30: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #31: Loss:1.0741, Accuracy:0.3926, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #32: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #33: Loss:1.0744, Accuracy:0.3943, Validation Loss:1.0744, Validation Accuracy:0.3941\n",
    "Epoch #34: Loss:1.0745, Accuracy:0.3737, Validation Loss:1.0743, Validation Accuracy:0.4007\n",
    "Epoch #35: Loss:1.0742, Accuracy:0.3963, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #36: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #37: Loss:1.0742, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #38: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0741, Validation Accuracy:0.3941\n",
    "Epoch #39: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #40: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #41: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #42: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #43: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #44: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0743, Validation Accuracy:0.3941\n",
    "Epoch #45: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #46: Loss:1.0738, Accuracy:0.3959, Validation Loss:1.0740, Validation Accuracy:0.3990\n",
    "Epoch #47: Loss:1.0739, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.4023\n",
    "Epoch #48: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3974\n",
    "Epoch #49: Loss:1.0738, Accuracy:0.3951, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #50: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #51: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #52: Loss:1.0740, Accuracy:0.3955, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #53: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.4007\n",
    "Epoch #54: Loss:1.0739, Accuracy:0.3984, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #55: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #56: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0740, Validation Accuracy:0.3941\n",
    "Epoch #57: Loss:1.0739, Accuracy:0.3984, Validation Loss:1.0741, Validation Accuracy:0.4007\n",
    "Epoch #58: Loss:1.0741, Accuracy:0.4012, Validation Loss:1.0739, Validation Accuracy:0.4056\n",
    "Epoch #59: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #60: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0742, Validation Accuracy:0.3941\n",
    "Epoch #61: Loss:1.0744, Accuracy:0.3930, Validation Loss:1.0738, Validation Accuracy:0.4072\n",
    "Epoch #62: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0739, Validation Accuracy:0.3941\n",
    "Epoch #63: Loss:1.0741, Accuracy:0.3951, Validation Loss:1.0739, Validation Accuracy:0.4056\n",
    "Epoch #64: Loss:1.0741, Accuracy:0.3979, Validation Loss:1.0739, Validation Accuracy:0.3990\n",
    "Epoch #65: Loss:1.0741, Accuracy:0.3947, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #66: Loss:1.0742, Accuracy:0.3955, Validation Loss:1.0742, Validation Accuracy:0.3974\n",
    "Epoch #67: Loss:1.0742, Accuracy:0.3955, Validation Loss:1.0739, Validation Accuracy:0.4056\n",
    "Epoch #68: Loss:1.0741, Accuracy:0.4025, Validation Loss:1.0739, Validation Accuracy:0.4072\n",
    "Epoch #69: Loss:1.0741, Accuracy:0.3914, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #70: Loss:1.0743, Accuracy:0.3943, Validation Loss:1.0738, Validation Accuracy:0.3941\n",
    "Epoch #71: Loss:1.0741, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #72: Loss:1.0740, Accuracy:0.3967, Validation Loss:1.0738, Validation Accuracy:0.4023\n",
    "Epoch #73: Loss:1.0740, Accuracy:0.3992, Validation Loss:1.0738, Validation Accuracy:0.4089\n",
    "Epoch #74: Loss:1.0742, Accuracy:0.3951, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #75: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0738, Validation Accuracy:0.4072\n",
    "Epoch #76: Loss:1.0738, Accuracy:0.3988, Validation Loss:1.0737, Validation Accuracy:0.3990\n",
    "Epoch #77: Loss:1.0739, Accuracy:0.3947, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #78: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0737, Validation Accuracy:0.3941\n",
    "Epoch #79: Loss:1.0740, Accuracy:0.3984, Validation Loss:1.0737, Validation Accuracy:0.4089\n",
    "Epoch #80: Loss:1.0739, Accuracy:0.3963, Validation Loss:1.0736, Validation Accuracy:0.4089\n",
    "Epoch #81: Loss:1.0740, Accuracy:0.3975, Validation Loss:1.0736, Validation Accuracy:0.4089\n",
    "Epoch #82: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #83: Loss:1.0738, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #84: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #85: Loss:1.0740, Accuracy:0.3971, Validation Loss:1.0735, Validation Accuracy:0.4089\n",
    "Epoch #86: Loss:1.0740, Accuracy:0.3963, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #87: Loss:1.0738, Accuracy:0.4012, Validation Loss:1.0735, Validation Accuracy:0.4089\n",
    "Epoch #88: Loss:1.0739, Accuracy:0.3881, Validation Loss:1.0736, Validation Accuracy:0.3941\n",
    "Epoch #89: Loss:1.0740, Accuracy:0.3938, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #90: Loss:1.0737, Accuracy:0.4000, Validation Loss:1.0735, Validation Accuracy:0.3990\n",
    "Epoch #91: Loss:1.0738, Accuracy:0.3947, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #92: Loss:1.0739, Accuracy:0.3975, Validation Loss:1.0735, Validation Accuracy:0.4023\n",
    "Epoch #93: Loss:1.0738, Accuracy:0.3984, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #94: Loss:1.0737, Accuracy:0.4000, Validation Loss:1.0734, Validation Accuracy:0.4105\n",
    "Epoch #95: Loss:1.0738, Accuracy:0.3922, Validation Loss:1.0735, Validation Accuracy:0.3941\n",
    "Epoch #96: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0734, Validation Accuracy:0.3990\n",
    "Epoch #97: Loss:1.0736, Accuracy:0.3967, Validation Loss:1.0734, Validation Accuracy:0.4039\n",
    "Epoch #98: Loss:1.0738, Accuracy:0.4012, Validation Loss:1.0734, Validation Accuracy:0.4023\n",
    "Epoch #99: Loss:1.0737, Accuracy:0.3979, Validation Loss:1.0733, Validation Accuracy:0.4105\n",
    "Epoch #100: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0733, Validation Accuracy:0.4105\n",
    "Epoch #101: Loss:1.0739, Accuracy:0.3943, Validation Loss:1.0734, Validation Accuracy:0.3941\n",
    "Epoch #102: Loss:1.0736, Accuracy:0.3943, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #103: Loss:1.0737, Accuracy:0.3947, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #104: Loss:1.0736, Accuracy:0.4004, Validation Loss:1.0733, Validation Accuracy:0.4039\n",
    "Epoch #105: Loss:1.0736, Accuracy:0.4012, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #106: Loss:1.0735, Accuracy:0.4016, Validation Loss:1.0732, Validation Accuracy:0.4089\n",
    "Epoch #107: Loss:1.0736, Accuracy:0.3979, Validation Loss:1.0732, Validation Accuracy:0.4039\n",
    "Epoch #108: Loss:1.0737, Accuracy:0.3996, Validation Loss:1.0732, Validation Accuracy:0.3990\n",
    "Epoch #109: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0731, Validation Accuracy:0.4007\n",
    "Epoch #110: Loss:1.0735, Accuracy:0.3943, Validation Loss:1.0731, Validation Accuracy:0.4039\n",
    "Epoch #111: Loss:1.0735, Accuracy:0.3959, Validation Loss:1.0731, Validation Accuracy:0.4039\n",
    "Epoch #112: Loss:1.0735, Accuracy:0.4029, Validation Loss:1.0731, Validation Accuracy:0.4039\n",
    "Epoch #113: Loss:1.0735, Accuracy:0.3975, Validation Loss:1.0730, Validation Accuracy:0.4105\n",
    "Epoch #114: Loss:1.0736, Accuracy:0.3951, Validation Loss:1.0730, Validation Accuracy:0.4039\n",
    "Epoch #115: Loss:1.0734, Accuracy:0.3943, Validation Loss:1.0729, Validation Accuracy:0.4105\n",
    "Epoch #116: Loss:1.0734, Accuracy:0.4004, Validation Loss:1.0729, Validation Accuracy:0.4039\n",
    "Epoch #117: Loss:1.0734, Accuracy:0.4045, Validation Loss:1.0728, Validation Accuracy:0.4056\n",
    "Epoch #118: Loss:1.0732, Accuracy:0.4012, Validation Loss:1.0728, Validation Accuracy:0.4039\n",
    "Epoch #119: Loss:1.0734, Accuracy:0.3934, Validation Loss:1.0729, Validation Accuracy:0.4007\n",
    "Epoch #120: Loss:1.0734, Accuracy:0.3934, Validation Loss:1.0729, Validation Accuracy:0.3990\n",
    "Epoch #121: Loss:1.0733, Accuracy:0.3943, Validation Loss:1.0727, Validation Accuracy:0.4039\n",
    "Epoch #122: Loss:1.0731, Accuracy:0.4037, Validation Loss:1.0727, Validation Accuracy:0.4023\n",
    "Epoch #123: Loss:1.0733, Accuracy:0.3996, Validation Loss:1.0727, Validation Accuracy:0.4023\n",
    "Epoch #124: Loss:1.0731, Accuracy:0.3992, Validation Loss:1.0726, Validation Accuracy:0.4056\n",
    "Epoch #125: Loss:1.0732, Accuracy:0.3947, Validation Loss:1.0728, Validation Accuracy:0.3990\n",
    "Epoch #126: Loss:1.0731, Accuracy:0.4000, Validation Loss:1.0725, Validation Accuracy:0.4056\n",
    "Epoch #127: Loss:1.0733, Accuracy:0.3984, Validation Loss:1.0726, Validation Accuracy:0.4023\n",
    "Epoch #128: Loss:1.0731, Accuracy:0.3996, Validation Loss:1.0725, Validation Accuracy:0.4138\n",
    "Epoch #129: Loss:1.0731, Accuracy:0.4029, Validation Loss:1.0726, Validation Accuracy:0.3941\n",
    "Epoch #130: Loss:1.0730, Accuracy:0.3959, Validation Loss:1.0724, Validation Accuracy:0.4007\n",
    "Epoch #131: Loss:1.0729, Accuracy:0.3988, Validation Loss:1.0723, Validation Accuracy:0.4056\n",
    "Epoch #132: Loss:1.0731, Accuracy:0.3926, Validation Loss:1.0723, Validation Accuracy:0.4023\n",
    "Epoch #133: Loss:1.0729, Accuracy:0.4008, Validation Loss:1.0721, Validation Accuracy:0.4056\n",
    "Epoch #134: Loss:1.0729, Accuracy:0.3992, Validation Loss:1.0721, Validation Accuracy:0.3957\n",
    "Epoch #135: Loss:1.0727, Accuracy:0.3984, Validation Loss:1.0719, Validation Accuracy:0.4138\n",
    "Epoch #136: Loss:1.0728, Accuracy:0.3975, Validation Loss:1.0719, Validation Accuracy:0.4023\n",
    "Epoch #137: Loss:1.0732, Accuracy:0.3979, Validation Loss:1.0719, Validation Accuracy:0.3957\n",
    "Epoch #138: Loss:1.0727, Accuracy:0.4000, Validation Loss:1.0717, Validation Accuracy:0.4138\n",
    "Epoch #139: Loss:1.0725, Accuracy:0.4012, Validation Loss:1.0716, Validation Accuracy:0.4072\n",
    "Epoch #140: Loss:1.0724, Accuracy:0.4045, Validation Loss:1.0715, Validation Accuracy:0.4072\n",
    "Epoch #141: Loss:1.0725, Accuracy:0.4053, Validation Loss:1.0715, Validation Accuracy:0.4072\n",
    "Epoch #142: Loss:1.0723, Accuracy:0.4099, Validation Loss:1.0715, Validation Accuracy:0.4089\n",
    "Epoch #143: Loss:1.0725, Accuracy:0.3979, Validation Loss:1.0713, Validation Accuracy:0.4138\n",
    "Epoch #144: Loss:1.0723, Accuracy:0.4012, Validation Loss:1.0712, Validation Accuracy:0.4138\n",
    "Epoch #145: Loss:1.0729, Accuracy:0.3996, Validation Loss:1.0716, Validation Accuracy:0.4007\n",
    "Epoch #146: Loss:1.0718, Accuracy:0.3984, Validation Loss:1.0711, Validation Accuracy:0.4072\n",
    "Epoch #147: Loss:1.0723, Accuracy:0.4037, Validation Loss:1.0712, Validation Accuracy:0.3990\n",
    "Epoch #148: Loss:1.0722, Accuracy:0.4012, Validation Loss:1.0709, Validation Accuracy:0.4138\n",
    "Epoch #149: Loss:1.0718, Accuracy:0.4041, Validation Loss:1.0713, Validation Accuracy:0.4007\n",
    "Epoch #150: Loss:1.0725, Accuracy:0.3947, Validation Loss:1.0711, Validation Accuracy:0.3957\n",
    "Epoch #151: Loss:1.0722, Accuracy:0.3906, Validation Loss:1.0709, Validation Accuracy:0.3990\n",
    "Epoch #152: Loss:1.0720, Accuracy:0.3984, Validation Loss:1.0708, Validation Accuracy:0.3990\n",
    "Epoch #153: Loss:1.0719, Accuracy:0.4000, Validation Loss:1.0710, Validation Accuracy:0.3957\n",
    "Epoch #154: Loss:1.0720, Accuracy:0.4049, Validation Loss:1.0707, Validation Accuracy:0.4138\n",
    "Epoch #155: Loss:1.0718, Accuracy:0.4025, Validation Loss:1.0707, Validation Accuracy:0.3990\n",
    "Epoch #156: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0708, Validation Accuracy:0.3957\n",
    "Epoch #157: Loss:1.0718, Accuracy:0.3934, Validation Loss:1.0706, Validation Accuracy:0.3990\n",
    "Epoch #158: Loss:1.0718, Accuracy:0.4025, Validation Loss:1.0705, Validation Accuracy:0.4154\n",
    "Epoch #159: Loss:1.0715, Accuracy:0.4033, Validation Loss:1.0708, Validation Accuracy:0.3957\n",
    "Epoch #160: Loss:1.0716, Accuracy:0.4041, Validation Loss:1.0705, Validation Accuracy:0.4056\n",
    "Epoch #161: Loss:1.0715, Accuracy:0.3955, Validation Loss:1.0703, Validation Accuracy:0.4171\n",
    "Epoch #162: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0704, Validation Accuracy:0.4056\n",
    "Epoch #163: Loss:1.0719, Accuracy:0.4053, Validation Loss:1.0701, Validation Accuracy:0.4138\n",
    "Epoch #164: Loss:1.0713, Accuracy:0.4004, Validation Loss:1.0706, Validation Accuracy:0.3957\n",
    "Epoch #165: Loss:1.0715, Accuracy:0.3992, Validation Loss:1.0700, Validation Accuracy:0.4138\n",
    "Epoch #166: Loss:1.0721, Accuracy:0.3979, Validation Loss:1.0703, Validation Accuracy:0.3974\n",
    "Epoch #167: Loss:1.0713, Accuracy:0.4012, Validation Loss:1.0705, Validation Accuracy:0.3957\n",
    "Epoch #168: Loss:1.0717, Accuracy:0.3934, Validation Loss:1.0703, Validation Accuracy:0.4056\n",
    "Epoch #169: Loss:1.0715, Accuracy:0.3988, Validation Loss:1.0700, Validation Accuracy:0.4171\n",
    "Epoch #170: Loss:1.0713, Accuracy:0.4066, Validation Loss:1.0703, Validation Accuracy:0.3990\n",
    "Epoch #171: Loss:1.0718, Accuracy:0.4062, Validation Loss:1.0700, Validation Accuracy:0.4138\n",
    "Epoch #172: Loss:1.0717, Accuracy:0.3889, Validation Loss:1.0703, Validation Accuracy:0.3990\n",
    "Epoch #173: Loss:1.0714, Accuracy:0.4021, Validation Loss:1.0703, Validation Accuracy:0.3957\n",
    "Epoch #174: Loss:1.0720, Accuracy:0.3885, Validation Loss:1.0701, Validation Accuracy:0.4072\n",
    "Epoch #175: Loss:1.0709, Accuracy:0.3967, Validation Loss:1.0700, Validation Accuracy:0.4138\n",
    "Epoch #176: Loss:1.0713, Accuracy:0.4008, Validation Loss:1.0700, Validation Accuracy:0.4072\n",
    "Epoch #177: Loss:1.0716, Accuracy:0.3955, Validation Loss:1.0698, Validation Accuracy:0.4138\n",
    "Epoch #178: Loss:1.0717, Accuracy:0.3926, Validation Loss:1.0701, Validation Accuracy:0.4056\n",
    "Epoch #179: Loss:1.0708, Accuracy:0.4070, Validation Loss:1.0699, Validation Accuracy:0.3957\n",
    "Epoch #180: Loss:1.0709, Accuracy:0.4074, Validation Loss:1.0697, Validation Accuracy:0.4138\n",
    "Epoch #181: Loss:1.0708, Accuracy:0.4004, Validation Loss:1.0698, Validation Accuracy:0.4056\n",
    "Epoch #182: Loss:1.0710, Accuracy:0.3979, Validation Loss:1.0697, Validation Accuracy:0.4089\n",
    "Epoch #183: Loss:1.0707, Accuracy:0.4070, Validation Loss:1.0698, Validation Accuracy:0.3957\n",
    "Epoch #184: Loss:1.0708, Accuracy:0.4066, Validation Loss:1.0701, Validation Accuracy:0.4023\n",
    "Epoch #185: Loss:1.0717, Accuracy:0.3938, Validation Loss:1.0700, Validation Accuracy:0.3941\n",
    "Epoch #186: Loss:1.0723, Accuracy:0.3971, Validation Loss:1.0707, Validation Accuracy:0.4072\n",
    "Epoch #187: Loss:1.0730, Accuracy:0.3971, Validation Loss:1.0746, Validation Accuracy:0.3941\n",
    "Epoch #188: Loss:1.0738, Accuracy:0.3873, Validation Loss:1.0718, Validation Accuracy:0.3760\n",
    "Epoch #189: Loss:1.0725, Accuracy:0.3955, Validation Loss:1.0720, Validation Accuracy:0.3974\n",
    "Epoch #190: Loss:1.0733, Accuracy:0.3901, Validation Loss:1.0718, Validation Accuracy:0.3957\n",
    "Epoch #191: Loss:1.0732, Accuracy:0.3938, Validation Loss:1.0714, Validation Accuracy:0.3957\n",
    "Epoch #192: Loss:1.0734, Accuracy:0.3930, Validation Loss:1.0714, Validation Accuracy:0.4039\n",
    "Epoch #193: Loss:1.0739, Accuracy:0.3807, Validation Loss:1.0723, Validation Accuracy:0.3941\n",
    "Epoch #194: Loss:1.0740, Accuracy:0.3943, Validation Loss:1.0720, Validation Accuracy:0.3941\n",
    "Epoch #195: Loss:1.0745, Accuracy:0.3696, Validation Loss:1.0722, Validation Accuracy:0.3810\n",
    "Epoch #196: Loss:1.0733, Accuracy:0.3823, Validation Loss:1.0720, Validation Accuracy:0.3842\n",
    "Epoch #197: Loss:1.0732, Accuracy:0.3971, Validation Loss:1.0720, Validation Accuracy:0.3941\n",
    "Epoch #198: Loss:1.0729, Accuracy:0.3951, Validation Loss:1.0713, Validation Accuracy:0.3941\n",
    "Epoch #199: Loss:1.0728, Accuracy:0.3885, Validation Loss:1.0712, Validation Accuracy:0.4072\n",
    "Epoch #200: Loss:1.0727, Accuracy:0.3852, Validation Loss:1.0707, Validation Accuracy:0.3957\n",
    "Epoch #201: Loss:1.0724, Accuracy:0.3967, Validation Loss:1.0711, Validation Accuracy:0.3892\n",
    "Epoch #202: Loss:1.0727, Accuracy:0.3930, Validation Loss:1.0706, Validation Accuracy:0.3941\n",
    "Epoch #203: Loss:1.0724, Accuracy:0.4033, Validation Loss:1.0706, Validation Accuracy:0.3941\n",
    "Epoch #204: Loss:1.0724, Accuracy:0.3984, Validation Loss:1.0706, Validation Accuracy:0.3957\n",
    "Epoch #205: Loss:1.0721, Accuracy:0.4062, Validation Loss:1.0704, Validation Accuracy:0.4023\n",
    "Epoch #206: Loss:1.0722, Accuracy:0.4062, Validation Loss:1.0705, Validation Accuracy:0.4023\n",
    "Epoch #207: Loss:1.0723, Accuracy:0.3988, Validation Loss:1.0706, Validation Accuracy:0.3892\n",
    "Epoch #208: Loss:1.0725, Accuracy:0.3988, Validation Loss:1.0708, Validation Accuracy:0.3859\n",
    "Epoch #209: Loss:1.0722, Accuracy:0.3979, Validation Loss:1.0703, Validation Accuracy:0.3892\n",
    "Epoch #210: Loss:1.0725, Accuracy:0.4021, Validation Loss:1.0704, Validation Accuracy:0.3957\n",
    "Epoch #211: Loss:1.0722, Accuracy:0.4037, Validation Loss:1.0703, Validation Accuracy:0.3990\n",
    "Epoch #212: Loss:1.0723, Accuracy:0.4045, Validation Loss:1.0702, Validation Accuracy:0.3957\n",
    "Epoch #213: Loss:1.0719, Accuracy:0.3996, Validation Loss:1.0702, Validation Accuracy:0.3892\n",
    "Epoch #214: Loss:1.0721, Accuracy:0.4000, Validation Loss:1.0703, Validation Accuracy:0.3957\n",
    "Epoch #215: Loss:1.0725, Accuracy:0.3947, Validation Loss:1.0704, Validation Accuracy:0.3892\n",
    "Epoch #216: Loss:1.0722, Accuracy:0.3938, Validation Loss:1.0703, Validation Accuracy:0.3859\n",
    "Epoch #217: Loss:1.0719, Accuracy:0.3996, Validation Loss:1.0702, Validation Accuracy:0.3826\n",
    "Epoch #218: Loss:1.0718, Accuracy:0.4021, Validation Loss:1.0700, Validation Accuracy:0.3892\n",
    "Epoch #219: Loss:1.0717, Accuracy:0.4021, Validation Loss:1.0701, Validation Accuracy:0.3892\n",
    "Epoch #220: Loss:1.0720, Accuracy:0.4033, Validation Loss:1.0701, Validation Accuracy:0.3826\n",
    "Epoch #221: Loss:1.0719, Accuracy:0.4070, Validation Loss:1.0702, Validation Accuracy:0.3892\n",
    "Epoch #222: Loss:1.0717, Accuracy:0.4045, Validation Loss:1.0701, Validation Accuracy:0.3892\n",
    "Epoch #223: Loss:1.0717, Accuracy:0.4008, Validation Loss:1.0700, Validation Accuracy:0.3892\n",
    "Epoch #224: Loss:1.0716, Accuracy:0.4033, Validation Loss:1.0701, Validation Accuracy:0.3892\n",
    "Epoch #225: Loss:1.0719, Accuracy:0.3984, Validation Loss:1.0701, Validation Accuracy:0.3810\n",
    "Epoch #226: Loss:1.0718, Accuracy:0.4004, Validation Loss:1.0698, Validation Accuracy:0.3875\n",
    "Epoch #227: Loss:1.0717, Accuracy:0.4041, Validation Loss:1.0698, Validation Accuracy:0.3892\n",
    "Epoch #228: Loss:1.0716, Accuracy:0.4074, Validation Loss:1.0697, Validation Accuracy:0.3875\n",
    "Epoch #229: Loss:1.0721, Accuracy:0.4004, Validation Loss:1.0701, Validation Accuracy:0.3892\n",
    "Epoch #230: Loss:1.0716, Accuracy:0.3984, Validation Loss:1.0700, Validation Accuracy:0.3941\n",
    "Epoch #231: Loss:1.0716, Accuracy:0.4012, Validation Loss:1.0698, Validation Accuracy:0.3941\n",
    "Epoch #232: Loss:1.0725, Accuracy:0.4008, Validation Loss:1.0699, Validation Accuracy:0.3892\n",
    "Epoch #233: Loss:1.0712, Accuracy:0.4021, Validation Loss:1.0697, Validation Accuracy:0.3875\n",
    "Epoch #234: Loss:1.0716, Accuracy:0.4029, Validation Loss:1.0701, Validation Accuracy:0.3990\n",
    "Epoch #235: Loss:1.0712, Accuracy:0.4049, Validation Loss:1.0698, Validation Accuracy:0.3826\n",
    "Epoch #236: Loss:1.0717, Accuracy:0.3996, Validation Loss:1.0699, Validation Accuracy:0.3826\n",
    "Epoch #237: Loss:1.0714, Accuracy:0.4033, Validation Loss:1.0699, Validation Accuracy:0.3810\n",
    "Epoch #238: Loss:1.0712, Accuracy:0.4008, Validation Loss:1.0696, Validation Accuracy:0.3875\n",
    "Epoch #239: Loss:1.0711, Accuracy:0.4053, Validation Loss:1.0695, Validation Accuracy:0.3777\n",
    "Epoch #240: Loss:1.0714, Accuracy:0.4053, Validation Loss:1.0696, Validation Accuracy:0.3777\n",
    "Epoch #241: Loss:1.0711, Accuracy:0.4057, Validation Loss:1.0694, Validation Accuracy:0.3826\n",
    "Epoch #242: Loss:1.0710, Accuracy:0.4025, Validation Loss:1.0693, Validation Accuracy:0.3826\n",
    "Epoch #243: Loss:1.0712, Accuracy:0.4012, Validation Loss:1.0693, Validation Accuracy:0.3842\n",
    "Epoch #244: Loss:1.0710, Accuracy:0.4033, Validation Loss:1.0694, Validation Accuracy:0.3777\n",
    "Epoch #245: Loss:1.0709, Accuracy:0.4057, Validation Loss:1.0694, Validation Accuracy:0.3777\n",
    "Epoch #246: Loss:1.0712, Accuracy:0.4008, Validation Loss:1.0694, Validation Accuracy:0.3842\n",
    "Epoch #247: Loss:1.0710, Accuracy:0.4041, Validation Loss:1.0693, Validation Accuracy:0.3842\n",
    "Epoch #248: Loss:1.0708, Accuracy:0.3959, Validation Loss:1.0694, Validation Accuracy:0.3842\n",
    "Epoch #249: Loss:1.0711, Accuracy:0.4004, Validation Loss:1.0697, Validation Accuracy:0.3875\n",
    "Epoch #250: Loss:1.0707, Accuracy:0.4033, Validation Loss:1.0696, Validation Accuracy:0.3810\n",
    "Epoch #251: Loss:1.0710, Accuracy:0.4000, Validation Loss:1.0693, Validation Accuracy:0.3842\n",
    "Epoch #252: Loss:1.0709, Accuracy:0.4000, Validation Loss:1.0695, Validation Accuracy:0.3744\n",
    "Epoch #253: Loss:1.0706, Accuracy:0.4094, Validation Loss:1.0692, Validation Accuracy:0.3842\n",
    "Epoch #254: Loss:1.0711, Accuracy:0.4016, Validation Loss:1.0692, Validation Accuracy:0.3842\n",
    "Epoch #255: Loss:1.0708, Accuracy:0.3984, Validation Loss:1.0694, Validation Accuracy:0.3744\n",
    "Epoch #256: Loss:1.0711, Accuracy:0.4008, Validation Loss:1.0692, Validation Accuracy:0.3842\n",
    "Epoch #257: Loss:1.0709, Accuracy:0.4012, Validation Loss:1.0686, Validation Accuracy:0.3842\n",
    "Epoch #258: Loss:1.0709, Accuracy:0.3996, Validation Loss:1.0690, Validation Accuracy:0.3810\n",
    "Epoch #259: Loss:1.0704, Accuracy:0.4025, Validation Loss:1.0690, Validation Accuracy:0.3777\n",
    "Epoch #260: Loss:1.0707, Accuracy:0.3996, Validation Loss:1.0692, Validation Accuracy:0.3842\n",
    "Epoch #261: Loss:1.0702, Accuracy:0.4033, Validation Loss:1.0693, Validation Accuracy:0.3744\n",
    "Epoch #262: Loss:1.0709, Accuracy:0.4066, Validation Loss:1.0691, Validation Accuracy:0.3793\n",
    "Epoch #263: Loss:1.0712, Accuracy:0.4033, Validation Loss:1.0693, Validation Accuracy:0.3842\n",
    "Epoch #264: Loss:1.0704, Accuracy:0.4000, Validation Loss:1.0698, Validation Accuracy:0.3826\n",
    "Epoch #265: Loss:1.0710, Accuracy:0.4045, Validation Loss:1.0692, Validation Accuracy:0.3760\n",
    "Epoch #266: Loss:1.0704, Accuracy:0.3992, Validation Loss:1.0690, Validation Accuracy:0.3842\n",
    "Epoch #267: Loss:1.0703, Accuracy:0.4029, Validation Loss:1.0686, Validation Accuracy:0.3744\n",
    "Epoch #268: Loss:1.0701, Accuracy:0.4086, Validation Loss:1.0687, Validation Accuracy:0.3760\n",
    "Epoch #269: Loss:1.0700, Accuracy:0.4057, Validation Loss:1.0687, Validation Accuracy:0.3793\n",
    "Epoch #270: Loss:1.0702, Accuracy:0.4008, Validation Loss:1.0685, Validation Accuracy:0.3744\n",
    "Epoch #271: Loss:1.0703, Accuracy:0.4000, Validation Loss:1.0689, Validation Accuracy:0.3711\n",
    "Epoch #272: Loss:1.0707, Accuracy:0.3959, Validation Loss:1.0690, Validation Accuracy:0.3695\n",
    "Epoch #273: Loss:1.0702, Accuracy:0.4045, Validation Loss:1.0688, Validation Accuracy:0.3744\n",
    "Epoch #274: Loss:1.0706, Accuracy:0.3979, Validation Loss:1.0686, Validation Accuracy:0.3842\n",
    "Epoch #275: Loss:1.0698, Accuracy:0.4008, Validation Loss:1.0687, Validation Accuracy:0.3711\n",
    "Epoch #276: Loss:1.0703, Accuracy:0.4066, Validation Loss:1.0690, Validation Accuracy:0.3744\n",
    "Epoch #277: Loss:1.0706, Accuracy:0.4041, Validation Loss:1.0689, Validation Accuracy:0.3793\n",
    "Epoch #278: Loss:1.0696, Accuracy:0.3996, Validation Loss:1.0690, Validation Accuracy:0.3711\n",
    "Epoch #279: Loss:1.0700, Accuracy:0.3988, Validation Loss:1.0688, Validation Accuracy:0.3793\n",
    "Epoch #280: Loss:1.0701, Accuracy:0.4004, Validation Loss:1.0685, Validation Accuracy:0.3842\n",
    "Epoch #281: Loss:1.0695, Accuracy:0.4016, Validation Loss:1.0686, Validation Accuracy:0.3744\n",
    "Epoch #282: Loss:1.0700, Accuracy:0.4033, Validation Loss:1.0690, Validation Accuracy:0.3695\n",
    "Epoch #283: Loss:1.0699, Accuracy:0.4066, Validation Loss:1.0687, Validation Accuracy:0.3695\n",
    "Epoch #284: Loss:1.0702, Accuracy:0.4041, Validation Loss:1.0685, Validation Accuracy:0.3842\n",
    "Epoch #285: Loss:1.0696, Accuracy:0.4008, Validation Loss:1.0684, Validation Accuracy:0.3793\n",
    "Epoch #286: Loss:1.0696, Accuracy:0.4004, Validation Loss:1.0692, Validation Accuracy:0.3695\n",
    "Epoch #287: Loss:1.0696, Accuracy:0.4070, Validation Loss:1.0689, Validation Accuracy:0.3711\n",
    "Epoch #288: Loss:1.0699, Accuracy:0.4049, Validation Loss:1.0685, Validation Accuracy:0.3662\n",
    "Epoch #289: Loss:1.0692, Accuracy:0.4021, Validation Loss:1.0685, Validation Accuracy:0.3842\n",
    "Epoch #290: Loss:1.0696, Accuracy:0.4004, Validation Loss:1.0689, Validation Accuracy:0.3793\n",
    "Epoch #291: Loss:1.0702, Accuracy:0.3975, Validation Loss:1.0692, Validation Accuracy:0.3711\n",
    "Epoch #292: Loss:1.0708, Accuracy:0.4008, Validation Loss:1.0687, Validation Accuracy:0.3842\n",
    "Epoch #293: Loss:1.0700, Accuracy:0.4004, Validation Loss:1.0699, Validation Accuracy:0.3892\n",
    "Epoch #294: Loss:1.0697, Accuracy:0.4045, Validation Loss:1.0690, Validation Accuracy:0.3711\n",
    "Epoch #295: Loss:1.0692, Accuracy:0.4062, Validation Loss:1.0685, Validation Accuracy:0.3744\n",
    "Epoch #296: Loss:1.0691, Accuracy:0.4041, Validation Loss:1.0682, Validation Accuracy:0.3777\n",
    "Epoch #297: Loss:1.0691, Accuracy:0.4037, Validation Loss:1.0684, Validation Accuracy:0.3711\n",
    "Epoch #298: Loss:1.0691, Accuracy:0.4025, Validation Loss:1.0684, Validation Accuracy:0.3793\n",
    "Epoch #299: Loss:1.0691, Accuracy:0.4021, Validation Loss:1.0692, Validation Accuracy:0.3711\n",
    "Epoch #300: Loss:1.0690, Accuracy:0.4037, Validation Loss:1.0691, Validation Accuracy:0.3711\n",
    "\n",
    "Test:\n",
    "Test Loss:1.06913245, Accuracy:0.3711\n",
    "Labels: ['02', '01', '03']\n",
    "Confusion Matrix:\n",
    "      02   01  03\n",
    "t:02  71  156   0\n",
    "t:01  85  155   0\n",
    "t:03  47   95   0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          02       0.35      0.31      0.33       227\n",
    "          01       0.38      0.65      0.48       240\n",
    "          03       0.00      0.00      0.00       142\n",
    "\n",
    "    accuracy                           0.37       609\n",
    "   macro avg       0.24      0.32      0.27       609\n",
    "weighted avg       0.28      0.37      0.31       609\n",
    "\n",
    "============ Config: 1/1 === End Time: 2019.07.28 23:38:08 =========================================\n",
    "============ Config: 1/1 === Duration: 0 days, 0 hours, 40 minutes, 34 seconds =====================\n",
    "\n",
    "Ending script after plotting results...\n",
    "\"\"\"\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'val_loss': [1.0787698443495777, 1.0747595354058277, 1.0743815160932995, 1.0751745988582742, 1.0746023901577653, 1.0744584957367094, 1.0743514988418479, 1.0741250644176465, 1.0743355046352143, 1.0743766696190795, 1.0743582732180266, 1.0743211423626478, 1.0742417806866524, 1.0742186740505675, 1.0741708219932218, 1.074146065610187, 1.0741139219703737, 1.0740969566680332, 1.0740520386468797, 1.0740456659414106, 1.0740639527247262, 1.0739895720004253, 1.0740558666548705, 1.0739089346480095, 1.073914992593975, 1.0739101496431824, 1.0738644973789333, 1.0739938919180132, 1.0738247196466857, 1.073852616773646, 1.0743111503143812, 1.0745622808318616, 1.074369269834559, 1.0743253283899994, 1.074099673034718, 1.0742249686533987, 1.0740582226532434, 1.074068038725892, 1.0739522776971702, 1.0740406201978034, 1.0741676395554065, 1.0740377720940877, 1.073979075710566, 1.0743204470729983, 1.0740418242312026, 1.0740106694999783, 1.0739177886292657, 1.0738029779471787, 1.0739806567506838, 1.074012680006732, 1.0738868445206942, 1.0738955514967343, 1.0739805706224614, 1.074035863962471, 1.0740023626286799, 1.0740482149453, 1.0740525037392803, 1.0738674402236938, 1.0738979536911537, 1.074171263987599, 1.0738185661766917, 1.0738800401953836, 1.0739048304424692, 1.0739010625284882, 1.0738383782125263, 1.0742002670792328, 1.07389489085412, 1.073925490645548, 1.0738043538455306, 1.0737802584966023, 1.0736941960449093, 1.0737989034950244, 1.0737744695055857, 1.0737399301309696, 1.0737975262264507, 1.073742911146192, 1.0737026647981165, 1.0737248880326846, 1.0736500484798537, 1.0735582291394814, 1.073559007229672, 1.0735454672858828, 1.0735375070806794, 1.0734985054811625, 1.0735432218839773, 1.0735324797371926, 1.073515061087209, 1.0735969363370748, 1.07349731808617, 1.073470369152639, 1.0734791283928506, 1.0734848580728416, 1.0734446577250665, 1.0734036238909943, 1.0734902696656476, 1.0734079870684394, 1.0733662193827638, 1.0733655918212164, 1.0733164220020688, 1.0733007687848972, 1.0733708158893929, 1.0732387028304227, 1.0732018027595307, 1.0732592921734638, 1.0731971150352841, 1.073185253025863, 1.073157675747801, 1.0732052496501379, 1.0731421722762886, 1.073076912922225, 1.0730670956751005, 1.0730645956077012, 1.07304267969429, 1.072954292367832, 1.0729276335298135, 1.0728696298912437, 1.0728302093953725, 1.0728021922761388, 1.072867386055306, 1.0728933817060122, 1.0727150925666045, 1.0727358369404458, 1.0726791346210174, 1.0726191396587978, 1.0727914822316913, 1.0724795663298057, 1.0725719482440668, 1.0724598875969698, 1.0726425011561227, 1.0724155472221437, 1.0722992547431407, 1.0722945601677856, 1.0721492874994263, 1.0720960518409466, 1.0718881150184594, 1.071859763760872, 1.0718856030105566, 1.0716621589973838, 1.071583886256163, 1.0714830494866583, 1.0714703950975917, 1.0714690943656884, 1.0712854447231699, 1.0712232088612021, 1.0716150204340618, 1.0711006949883572, 1.071193857537506, 1.070859953883442, 1.071330897522286, 1.0711440750335042, 1.070932890198305, 1.0708441239076687, 1.0709880124563458, 1.0707114439683987, 1.0706772745536466, 1.0707961326749453, 1.0706128038600553, 1.0704590642002025, 1.070766056112468, 1.0704520490564933, 1.0703245812448963, 1.0703836898694092, 1.0701346628380135, 1.0705783566817861, 1.0700065917373682, 1.0703252622450905, 1.0704659348833934, 1.0702956313961636, 1.0699991351865195, 1.070319950091232, 1.069957605332185, 1.0702888542795417, 1.070277652521243, 1.0701348519286107, 1.0699602610176224, 1.0700380620110799, 1.0697827574067515, 1.0701373292894787, 1.0698914300827753, 1.0697322482937466, 1.0698191833809287, 1.0696515418430073, 1.069834277939131, 1.0700678690313705, 1.0700242078950253, 1.070682611567242, 1.0745860860852772, 1.0717535118751338, 1.0720270652880615, 1.0717891127800903, 1.0714235736427244, 1.0714127453677174, 1.0723217439964683, 1.0719613253776663, 1.072224185384553, 1.0720253597535132, 1.072020177574972, 1.071296273781161, 1.0711675347953007, 1.07074438762195, 1.0710828257311742, 1.0706406244503452, 1.0706351429762315, 1.0705637904419296, 1.0704215191463726, 1.0705083863097067, 1.0705529226262385, 1.0708056884054675, 1.0702519504894763, 1.0704310277021185, 1.0702856279946313, 1.0701635874355173, 1.0701545091293911, 1.0703264478783694, 1.0703748210114603, 1.0702994161442974, 1.070156771952687, 1.069958697986133, 1.0700658851460674, 1.0701025315301955, 1.07022460888955, 1.0700984809590481, 1.0700495605202536, 1.070061314086413, 1.0701454096827014, 1.0697821245600634, 1.069759609859761, 1.069672434983778, 1.0700688831911886, 1.0699895330642049, 1.0697743518794895, 1.069940745536917, 1.069736449197791, 1.0701103143895592, 1.0697970137807535, 1.0699236712040767, 1.0698595843683127, 1.069591711307394, 1.0694608997633108, 1.0695807373973927, 1.0693747899410955, 1.069312641381826, 1.0693062074078714, 1.0693894906584265, 1.0693686899097485, 1.0694285212283456, 1.0692520245346921, 1.0694344942205645, 1.069687900872066, 1.0695761269928004, 1.069295882787219, 1.0695473302174083, 1.0691630750258372, 1.069205402544958, 1.0693542344621054, 1.0691784371687665, 1.0686269162715167, 1.0689846128470009, 1.0689847651373576, 1.0692388181420187, 1.0693223425515963, 1.0691357162002664, 1.0693398510489753, 1.0697526796697983, 1.069237261570146, 1.0690429091257807, 1.068646779005555, 1.0686529277776458, 1.0687146382574573, 1.0685474355819777, 1.068866657310323, 1.0690272862492327, 1.0688436757559063, 1.0686042637660587, 1.068653058535947, 1.0689917534638704, 1.0688910546952672, 1.069032647926819, 1.0687700521769783, 1.068479485112458, 1.0686214786444979, 1.0689655219392824, 1.0686632009171109, 1.06851984695065, 1.0683568911795154, 1.0692344269729013, 1.0688619570583349, 1.0685313211873246, 1.0685314127964338, 1.0688838453715659, 1.069223438773445, 1.068731929281075, 1.06988048592616, 1.0690122899555026, 1.0685198017333333, 1.0681666873750233, 1.0683766554533358, 1.0684068639485902, 1.0692362217676072, 1.0691324918728156], 'val_acc': [0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.38916256025506946, 0.3858784887884638, 0.39244663260253193, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3990147770038379, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.40065681361799754, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3990147770038379, 0.4022988493513004, 0.3973727407811702, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.3990147770038379, 0.40065681312863266, 0.3940886686294537, 0.3940886686294537, 0.3940886686294537, 0.40065681361799754, 0.40558292218812775, 0.3940886686294537, 0.3940886686294537, 0.4072249583129225, 0.3940886686294537, 0.40558292218812775, 0.3990147770038379, 0.3940886686294537, 0.39737274107478915, 0.40558292209025476, 0.4072249583129225, 0.3940886686294537, 0.3940886686294537, 0.3990147770038379, 0.40229884974279234, 0.40886699443771723, 0.3940886686294537, 0.4072249583129225, 0.3990147770038379, 0.3940886686294537, 0.3940886686294537, 0.40886699443771723, 0.40886699443771723, 0.40886699443771723, 0.3940886686294537, 0.3940886686294537, 0.3990147770038379, 0.4088669945355902, 0.4022988498406653, 0.40886699443771723, 0.3940886686294537, 0.3990147772974569, 0.3990147770038379, 0.3940886686294537, 0.40229884974279234, 0.4022988498406653, 0.41050903066038497, 0.3940886686294537, 0.3990147770038379, 0.40394088596546, 0.40229884974279234, 0.41050903066038497, 0.41050903066038497, 0.3940886686294537, 0.40394088596546, 0.40394088596546, 0.40394088596546, 0.40394088596546, 0.40886699443771723, 0.40394088596546, 0.3990147770038379, 0.4006568135201246, 0.40394088596546, 0.40394088596546, 0.40394088596546, 0.41050903066038497, 0.40394088596546, 0.41050903066038497, 0.40394088596546, 0.40558292209025476, 0.40394088596546, 0.4006568135201246, 0.3990147770038379, 0.40394088596546, 0.4022988498406653, 0.4022988498406653, 0.40558292218812775, 0.3990147770038379, 0.40558292218812775, 0.4022988498406653, 0.41379310271422853, 0.3940886686294537, 0.4006568135201246, 0.40558292218812775, 0.40229884993853826, 0.40558292218812775, 0.3957307050478674, 0.41379310271422853, 0.4022988498406653, 0.3957307050478674, 0.41379310281210147, 0.4072249583129225, 0.4072249583129225, 0.4072249583129225, 0.4088669945355902, 0.41379310290997445, 0.41379310281210147, 0.40065681322650565, 0.4072249583129225, 0.3990147774932028, 0.41379310281210147, 0.40065681322650565, 0.3957307050478674, 0.3990147774932028, 0.39901477739532987, 0.3957307050478674, 0.41379310281210147, 0.39901477739532987, 0.3957307050478674, 0.3990147775910758, 0.41543513923051517, 0.3957307050478674, 0.40558292209025476, 0.4170771754531829, 0.40558292189450884, 0.41379310281210147, 0.3957307050478674, 0.41379310290997445, 0.39737274136840806, 0.3957307050478674, 0.40558292189450884, 0.4170771754531829, 0.3990147775910758, 0.41379310290997445, 0.39901477739532987, 0.3957307053414863, 0.4072249583129225, 0.41379310281210147, 0.4072249583129225, 0.41379310281210147, 0.40558292189450884, 0.3957307053414863, 0.41379310281210147, 0.40558292189450884, 0.40886699443771723, 0.3957307054393593, 0.40229885013428424, 0.3940886684337077, 0.4072249582150495, 0.3940886686294537, 0.3760262712567115, 0.39737274107478915, 0.3957307048521214, 0.3957307051457404, 0.40394088596546, 0.3940886686294537, 0.3940886686294537, 0.3809523801204606, 0.3842364519785582, 0.3940886686294537, 0.39408866892307265, 0.40722495425119387, 0.3957307055372323, 0.38916256025506946, 0.3940886686294537, 0.3940886697060565, 0.3957307055372323, 0.4022988493513004, 0.4022988493513004, 0.3891625611359263, 0.3858784887884638, 0.3891625610380533, 0.3957307053414863, 0.3990147770038379, 0.3957307053414863, 0.3891625611359263, 0.3957307055372323, 0.38916256064656135, 0.3858784887884638, 0.3825944165388743, 0.3891625611359263, 0.3891625611359263, 0.3825944165388743, 0.3891625611359263, 0.3891625611359263, 0.3891625611359263, 0.3891625611359263, 0.38095237982684166, 0.38752052501113154, 0.3891625611359263, 0.38752052491325856, 0.3891625610380533, 0.3940886690209456, 0.39408866931456454, 0.3891625610380533, 0.38752052491325856, 0.3990147771995839, 0.3825944165388743, 0.3825944165388743, 0.3809523797289687, 0.38752052491325856, 0.3776683080666171, 0.3776683080666171, 0.3825944165388743, 0.3825944165388743, 0.38423645266366907, 0.3776683080666171, 0.3776683080666171, 0.38423645178281224, 0.3842364525657961, 0.38423645266366907, 0.3875205244238936, 0.3809523804140796, 0.38423645266366907, 0.3743842349361708, 0.38423645266366907, 0.3842364525657961, 0.3743842352297897, 0.38423645178281224, 0.3842364525657961, 0.3809523804140796, 0.3776683080666171, 0.38423645266366907, 0.3743842349361708, 0.37931034419141185, 0.3842364525657961, 0.3825944159516364, 0.37602627135458444, 0.3842364525657961, 0.3743842349361708, 0.37602627106096553, 0.37931034419141185, 0.37438423571915463, 0.3711001628823273, 0.3694581267575325, 0.3743842349361708, 0.3842364525657961, 0.3711001628823273, 0.3743842349361708, 0.37931034419141185, 0.3711001628823273, 0.37931034419141185, 0.3842364525657961, 0.3743842349361708, 0.3694581267575325, 0.3694581267575325, 0.3842364525657961, 0.37931034419141185, 0.3694581267575325, 0.3711001628823273, 0.36617405441007006, 0.3842364525657961, 0.37931034419141185, 0.3711001628823273, 0.3842364525657961, 0.38916256025506946, 0.3711001628823273, 0.37438423571915463, 0.37766830787087113, 0.3711001628823273, 0.37931034419141185, 0.3711001628823273, 0.3711001628823273], 'loss': [1.0866230445720821, 1.0775180109717273, 1.0747157273106507, 1.0751292347663237, 1.0750136667698071, 1.0747077134600411, 1.0746253380785242, 1.0744924922743373, 1.074324477086077, 1.0743114843995174, 1.074239925045742, 1.074100833898697, 1.0740718464587014, 1.0741966929034286, 1.0743796264856014, 1.0742284611754838, 1.074136603588441, 1.074201908346564, 1.0741150452615789, 1.0741037675732215, 1.0741817346098976, 1.074150754000372, 1.0742485856618236, 1.0740545738404292, 1.0740008089331876, 1.0740723258905587, 1.0740522414996638, 1.0741557935914465, 1.0739540877038694, 1.0739624235419523, 1.0740823626273466, 1.0742588126439088, 1.074431484778559, 1.0745128381913203, 1.0742467641830444, 1.0742275057387303, 1.0741797085415412, 1.0740028367150245, 1.0741135016603882, 1.0740983175790775, 1.0739050090190567, 1.073829853412307, 1.0739417420275648, 1.0742596308553487, 1.0739848174108861, 1.0737647655808216, 1.0739039526827772, 1.0739620873081122, 1.0737959054950816, 1.0738691090068777, 1.0738098455895144, 1.073997702050258, 1.073910666197477, 1.073871594190108, 1.0741431837453979, 1.0740397611927448, 1.0739488741210843, 1.0741067970068303, 1.074091423414571, 1.0743357980520574, 1.0744267792672346, 1.0741452093241886, 1.0740749996056058, 1.0741170915000493, 1.074087134866499, 1.074202235572392, 1.0742155076052373, 1.0740670480767314, 1.0740948462633138, 1.0743128781935518, 1.0741109599812564, 1.0740402723484705, 1.0739546993185118, 1.0741644640967587, 1.0739970380031107, 1.0738321030164402, 1.0739147046263458, 1.0738599292551467, 1.074007952335679, 1.073861045856985, 1.073978432786538, 1.073882042798663, 1.0738303487060985, 1.073861976570662, 1.073960542336137, 1.0739644393294254, 1.0738015892079724, 1.073920037810073, 1.0740110189762937, 1.0737461911089856, 1.0737846733363503, 1.0738812924900094, 1.07380541692769, 1.0737128131933036, 1.073761406620425, 1.0737160224444569, 1.0736202845583216, 1.0737884044647217, 1.0736641864267462, 1.0736016355745601, 1.0739141208680014, 1.0735966118943765, 1.0737273257371092, 1.0735506881189052, 1.0736317635561652, 1.0735482583545317, 1.0736020022593975, 1.073664635701346, 1.0734783104557766, 1.07345982249023, 1.0735044744225253, 1.0734504052990517, 1.0735002875572848, 1.073557218144317, 1.073435035672276, 1.073364474641224, 1.073356124752601, 1.0732246757777564, 1.0733944691182162, 1.0733627840234023, 1.073344972784759, 1.0730685260995947, 1.0733126637871995, 1.0730690167914672, 1.073223809735731, 1.0731161485707246, 1.0732800679040395, 1.0730989367565336, 1.0731269559331498, 1.0729999612733814, 1.0728983117814426, 1.0731169644812049, 1.0728922302473252, 1.0728865369879, 1.072675032537331, 1.0727757542040315, 1.0731801715474842, 1.0727017349286245, 1.072500503822029, 1.072440881601839, 1.072454123183687, 1.0723425751104492, 1.072454239504538, 1.0722719195931845, 1.0729110664410757, 1.0717840416475488, 1.072344902163903, 1.0722199658838385, 1.0718380818376796, 1.0724993646022476, 1.0722126520634676, 1.0720206055063486, 1.0719415783147792, 1.0720191138480968, 1.0717762855533701, 1.0721057801765583, 1.071834550843836, 1.0717783500526474, 1.0714888354346492, 1.0715597780822974, 1.0714863367883576, 1.071579696265579, 1.0719471071290285, 1.0713384546538398, 1.0715011868878312, 1.0721220665643838, 1.0713397403517297, 1.0717345628895065, 1.0715250489158552, 1.0712952130629052, 1.0717574438764819, 1.0717180849835124, 1.0713762509015061, 1.0720438035361821, 1.0709103086890626, 1.0712887567661136, 1.0716247200721098, 1.0717295973697483, 1.0708431251239972, 1.0709366010689392, 1.0707900226972922, 1.0709922032679375, 1.0707428242881196, 1.0707688606250458, 1.0716632180634955, 1.0722900142415104, 1.072963689729663, 1.073788542424384, 1.0725430709380634, 1.0732667113721248, 1.0732050699375004, 1.0734109588227478, 1.0738698518251737, 1.0739901750239504, 1.0745214014817068, 1.0732735510969065, 1.073161059434409, 1.0729176397441105, 1.0728153962619007, 1.0726512674433495, 1.0723624779213625, 1.0726906467022592, 1.0724244441829422, 1.0724070247438655, 1.072089248467275, 1.072183187737357, 1.0722560697022894, 1.0725260759525965, 1.072188548728426, 1.0724804724511179, 1.0722221417593516, 1.0722553536142903, 1.0718648900241579, 1.072050614425534, 1.0725457012775743, 1.072156948769117, 1.0718838777385453, 1.0717948685926089, 1.0716827346069366, 1.0719938744754518, 1.0718761088177409, 1.071735793756019, 1.071679340691537, 1.0715747097924015, 1.0718661605455058, 1.0718221271796882, 1.0716832971181223, 1.0715919540158532, 1.0721052817005885, 1.0715956361876375, 1.0715626105635563, 1.0725031188381282, 1.0711999862835393, 1.0715582857876098, 1.0711998046056446, 1.0717211283697485, 1.071363468973054, 1.0712039770776487, 1.0710511489080943, 1.071402906881955, 1.0710605874933012, 1.0710002070334903, 1.0712133970593525, 1.070996363011229, 1.070931454850418, 1.071153189073598, 1.070954658510259, 1.0708423723675142, 1.0711392602881367, 1.0706854798221002, 1.0710182804346575, 1.0709462223111725, 1.0706420806399117, 1.0710892940204002, 1.0707827873543303, 1.071086560431447, 1.0708705348890175, 1.0709050385614196, 1.070405319828762, 1.0707038314435515, 1.070187102450972, 1.070931065155985, 1.071210120982458, 1.070381354649209, 1.0709967506984421, 1.0704469205417673, 1.070293933361218, 1.0701334123493955, 1.0699952479995007, 1.0702161664101133, 1.0702808245496338, 1.070650131746484, 1.070205471794708, 1.0706109292208537, 1.0698303159502254, 1.0702696865344195, 1.070551709472766, 1.0696309056859732, 1.0700358373183734, 1.0700635219256736, 1.0694660626887296, 1.0700040109838058, 1.069932733619972, 1.0702337938657287, 1.0696063355988301, 1.069583249434798, 1.0695902372532557, 1.0698502771173903, 1.0692270553087553, 1.0695755508646094, 1.0701788471464748, 1.0707563994111955, 1.0700234955096393, 1.069653737079926, 1.0691691683547941, 1.0691138005109784, 1.0691391139059832, 1.0690835587542649, 1.0690812084953887, 1.069029642375343], 'acc': [0.3926078010511105, 0.39507186989275095, 0.39425051113173704, 0.39425051449750237, 0.39425051547663414, 0.3991786461101665, 0.3901437383168042, 0.39507187055366483, 0.39425051211086876, 0.3942505129308916, 0.3942505123066951, 0.39425051171921605, 0.39425051547663414, 0.39425051391002336, 0.39507186793448745, 0.3934291569114466, 0.3942505129308916, 0.39425051250252147, 0.39425051391002336, 0.3942505113275634, 0.3942505152808078, 0.39425051250252147, 0.39425051391002336, 0.39425051250252147, 0.394250514301676, 0.39425051113173704, 0.3942505123066951, 0.39425051156010715, 0.39425051351837065, 0.3942505141058497, 0.39260780523689864, 0.3942505137509144, 0.3942505156724605, 0.3737166302649637, 0.39630390219619877, 0.3942505141058497, 0.39425051171921605, 0.39425051508498143, 0.3942505133225443, 0.39425051312671794, 0.39425051113173704, 0.39425051547663414, 0.3942505138733059, 0.39425051273506523, 0.3942505117559335, 0.39589322536143434, 0.39794661071021453, 0.3942505157091779, 0.39507187090860013, 0.39425051312671794, 0.394250514301676, 0.3954825465684064, 0.39425051508498143, 0.3983572908740269, 0.3946611887865243, 0.39425051351837065, 0.398357289466525, 0.4012320338088629, 0.395071869733642, 0.3942505113642808, 0.393018479648312, 0.39466119293559504, 0.39507187032112107, 0.39794661306013074, 0.3946611925072249, 0.39548254539344835, 0.39548254696005913, 0.40246406666307233, 0.3913757701918819, 0.39425051156010715, 0.394250513714197, 0.3967145770359823, 0.399178644151903, 0.395071869733642, 0.39383983410114626, 0.3987679666929421, 0.3946611901573087, 0.3942505129308916, 0.39835729106985324, 0.39630390141289334, 0.3975359344262117, 0.3942505141058497, 0.39425051547663414, 0.39425051547663414, 0.3971252562573803, 0.3963039005928705, 0.401232032401361, 0.3880903486598444, 0.3938398362552361, 0.40000000134630614, 0.3946611887865243, 0.3975359338387327, 0.3983572886832196, 0.40000000095465343, 0.3921971256238479, 0.39466119035313507, 0.3967145770359823, 0.4012320330255575, 0.3979466105143882, 0.3942505156724605, 0.3942505123434126, 0.3942505152808078, 0.394661189765656, 0.4004106755986106, 0.40123203517964734, 0.4016427102519746, 0.39794661008601806, 0.3995893231407573, 0.395893225165608, 0.3942505115233897, 0.3958932245781289, 0.40287474404859835, 0.39753593309214474, 0.3950718677386611, 0.394250514301676, 0.40041067818107057, 0.4045174523667878, 0.4012320306389239, 0.39342915667890277, 0.3934291588697101, 0.3942505129308916, 0.40369609892980274, 0.3995893211457764, 0.3991786465018192, 0.39466119152809315, 0.40000000173795885, 0.3983572878999142, 0.3995893217332554, 0.4028747444769685, 0.3958932245781289, 0.39876796692548594, 0.3926078006594578, 0.4008213569740985, 0.3991786461101665, 0.3983572873124352, 0.39753593602953996, 0.3979466124726517, 0.3999999985680198, 0.40123203361303655, 0.4045174523667878, 0.405338809402082, 0.40985626515911344, 0.3979466142350888, 0.401232032401361, 0.3995893202033621, 0.3983572908740269, 0.4036960967389955, 0.4012320338088629, 0.40410677811448337, 0.3946611927030512, 0.39055441593487406, 0.3983572893074161, 0.40000000232543786, 0.40492813213894746, 0.40246406744637775, 0.3979466112976936, 0.39342915926136274, 0.40246406349313335, 0.403285419316752, 0.4041067745528916, 0.39548254735171184, 0.40287474111120314, 0.40533881116451914, 0.4004106759902633, 0.39917864395607666, 0.39794660969436535, 0.4012320310305766, 0.3934291594571891, 0.3987679651630488, 0.4065708408487896, 0.40616016502987434, 0.3889117054625948, 0.4020533894366552, 0.3885010256537177, 0.39671458079340033, 0.4008213546241823, 0.39548254793919085, 0.3926078018711333, 0.40698151964181745, 0.4073921992548682, 0.40041067896437593, 0.3979466128643044, 0.406981520266014, 0.40657084303959684, 0.39383983609612716, 0.3971252584114701, 0.3971252578239911, 0.38726899381535745, 0.3954825467642328, 0.3901437383168042, 0.3938398359003008, 0.3930184818391193, 0.38069815368867754, 0.3942505146933287, 0.3696098558834201, 0.38234086102773523, 0.39712525903566664, 0.395071869146163, 0.38850102862783037, 0.3852156047458766, 0.3967145770359823, 0.3930184794524857, 0.4032854226825174, 0.3983572873124352, 0.4061601624474144, 0.40616016502987434, 0.39876796512633134, 0.39876796551798405, 0.3979466122768253, 0.4020533862667162, 0.4036960988930853, 0.40451745573255316, 0.39958932431571537, 0.3999999985680198, 0.3946611889456332, 0.39383983511699544, 0.39958932036247097, 0.40205338963248155, 0.40205339041578697, 0.40328542229086467, 0.406981520853493, 0.4045174549492478, 0.40082135619079307, 0.403285422486691, 0.3983572912656796, 0.4004106779852442, 0.4041067771353516, 0.4073921967091257, 0.4004106767735687, 0.3983572886832196, 0.401232034983821, 0.40082135619079307, 0.4020533890082851, 0.40287474388948946, 0.4049281305723367, 0.39958932176997286, 0.4032854201000574, 0.4008213575615775, 0.4053388113603455, 0.4053388086187766, 0.40574948878258893, 0.40246406368895965, 0.401232032401361, 0.403285422486691, 0.4057494854168236, 0.4008213565824458, 0.4041067771353516, 0.39589322575308705, 0.40041067540278424, 0.4032854201000574, 0.39999999977969536, 0.40000000072210967, 0.4094455830003202, 0.40164270884447273, 0.39835728750826155, 0.400821355015835, 0.4012320330255575, 0.3995893221249081, 0.40246406564722315, 0.3995893225532782, 0.403285422486691, 0.4065708438229022, 0.4032854189250993, 0.4000000005630007, 0.4045174555367268, 0.39917864630599287, 0.40287474506444754, 0.4086242279600069, 0.40574948799928356, 0.4008213545874649, 0.400000000917936, 0.3958932259489134, 0.40451745534090044, 0.39794661403926246, 0.40082135419581216, 0.40657084401872856, 0.4041067741979564, 0.3995893219657992, 0.3987679684920967, 0.40041067579443695, 0.4016427126018908, 0.4032854189250993, 0.40657084343124955, 0.4041067775270043, 0.40082135341250674, 0.40041067876854963, 0.40698152144097205, 0.4049281299848576, 0.4020533894366552, 0.40041067563532806, 0.39753593465875553, 0.4008213554074877, 0.4004106755986106, 0.4045174523667878, 0.406160162251588, 0.4041067741612389, 0.40369610030058717, 0.4024640668588987, 0.4020533872825654, 0.4036960987339764]}\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# ACCURACIES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Accuracies')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(r['acc'], label='Train Acc')\n",
    "plt.plot(r['val_acc'], label='Test Acc')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# LOSSES\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch(s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(r['loss'], label='Train Loss')\n",
    "plt.plot(r['val_loss'], label='Test Loss')\n",
    "plt.grid(linestyle='dashed', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
